{"hands_on_practices": [{"introduction": "The concept of infinite divisibility allows us to think of a random variable as being composed of numerous smaller, identical building blocks. This exercise provides a concrete starting point by exploring this idea with the well-known Gamma distribution. Using the powerful tool of moment generating functions, we will \"divide\" a Gamma-distributed variable to identify the precise distribution of its independent and identically distributed components, making the abstract definition tangible [@problem_id:1308915].", "problem": "A random variable $X$ is said to be infinitely divisible if for any positive integer $n$, it can be expressed as the sum of $n$ independent and identically distributed (i.i.d.) random variables, $Y_1, Y_2, \\dots, Y_n$. That is, $X = \\sum_{i=1}^n Y_i$.\n\nConsider a random variable $X$ that follows a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$. We denote this as $X \\sim \\Gamma(\\alpha, \\beta)$. The probability density function for such a variable is given by $f(x; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x)$ for $x > 0$. Given that the Gamma distribution is infinitely divisible, what is the distribution of the i.i.d. components $Y_i$?\n\nA. Normal distribution with mean $\\frac{\\alpha}{n\\beta}$ and variance $\\frac{\\alpha}{n\\beta^2}$.\n\nB. Gamma distribution $\\Gamma(\\frac{\\alpha}{n}, \\beta)$.\n\nC. Gamma distribution $\\Gamma(\\alpha, \\frac{\\beta}{n})$.\n\nD. Gamma distribution $\\Gamma(\\frac{\\alpha}{n}, \\frac{\\beta}{n})$.\n\nE. Exponential distribution with rate $\\frac{\\beta}{\\alpha}$.", "solution": "Let $X \\sim \\Gamma(\\alpha, \\beta)$ with density $f(x;\\alpha,\\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} \\exp(-\\beta x)$ for $x>0$. The moment generating function (mgf) of $X$ is computed as follows for $t\\beta$:\n$$\nM_{X}(t) = \\mathbb{E}[\\exp(tX)] = \\int_{0}^{\\infty} \\exp(tx) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} \\exp(-\\beta x)\\,dx \\\\\n= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty} x^{\\alpha - 1} \\exp\\big(-( \\beta - t ) x\\big)\\,dx \\\\\n= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\cdot \\frac{\\Gamma(\\alpha)}{(\\beta - t)^{\\alpha}} = \\left(\\frac{\\beta}{\\beta - t}\\right)^{\\alpha}.\n$$\nIf $X$ is infinitely divisible so that $X = \\sum_{i=1}^{n} Y_{i}$ with $Y_{1},\\dots,Y_{n}$ i.i.d., then by independence,\n$$\nM_{X}(t) = \\prod_{i=1}^{n} M_{Y}(t) = \\big(M_{Y}(t)\\big)^{n}.\n$$\nEquating with the mgf of $X$ gives\n$$\n\\big(M_{Y}(t)\\big)^{n} = \\left(\\frac{\\beta}{\\beta - t}\\right)^{\\alpha} \\quad \\Longrightarrow \\quad M_{Y}(t) = \\left(\\frac{\\beta}{\\beta - t}\\right)^{\\alpha/n}, \\quad t\\beta.\n$$\nBut the mgf of a $\\Gamma(k,\\beta)$ distribution (shape $k$, rate $\\beta$) is $M(t) = \\left(\\frac{\\beta}{\\beta - t}\\right)^{k}$ for $t\\beta$. Hence $M_{Y}(t)$ matches that of $\\Gamma\\!\\left(\\frac{\\alpha}{n}, \\beta\\right)$. By the uniqueness of the mgf, this identifies the distribution:\n$$\nY_{i} \\sim \\Gamma\\!\\left(\\frac{\\alpha}{n}, \\beta\\right) \\quad \\text{i.i.d.}\n$$\nThis corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1308915"}, {"introduction": "Understanding a concept fully requires knowing not only what it is, but also what it is not. While some distributions, like the Gamma, can be infinitely divided, others cannot. This practice investigates a fundamental counterexample: the Bernoulli distribution [@problem_id:1308940]. By attempting to decompose a Bernoulli variable into just two identical parts, we will uncover a logical contradiction related to the possible values the components can take, thus revealing a key structural property that a distribution must have to be infinitely divisible.", "problem": "A random variable $X$ is said to be infinitely divisible if for any positive integer $n$, it can be represented as the sum of $n$ independent and identically distributed (i.i.d.) random variables, $X = Y_1 + Y_2 + \\dots + Y_n$.\n\nConsider a Bernoulli random variable $X$ with success probability $p$, where $0  p  1$. The probability mass function is $P(X=1) = p$ and $P(X=0) = 1-p$. Let us hypothesize that this Bernoulli variable is infinitely divisible. This implies that for any integer $n \\ge 2$, we can find i.i.d. random variables $Y_1, Y_2, \\dots, Y_n$ such that their sum equals $X$.\n\nFocusing on the case where $n=2$, we assume $X = Y_1 + Y_2$, where $Y_1$ and $Y_2$ are i.i.d. By analyzing the properties that $Y_1$ and $Y_2$ must possess, a contradiction can be found.\n\nWhich of the following statements correctly identifies the fundamental reason why a Bernoulli random variable with $p \\in (0,1)$ cannot be infinitely divisible?\n\nA. The sum $Y_1 + Y_2$ must be able to take on a value other than 0 or 1 with non-zero probability, which contradicts the support of the Bernoulli variable $X$.\n\nB. The expectation $E[X] = p$ cannot be decomposed into the sum $E[Y_1] + E[Y_2]$ because the expectation of the $Y_i$ variables would be imaginary.\n\nC. The variance $\\text{Var}(X) = p(1-p)$ cannot be satisfied by the sum of variances $\\text{Var}(Y_1) + \\text{Var}(Y_2)$ because the variance of each $Y_i$ would have to be negative.\n\nD. The random variables $Y_1$ and $Y_2$ must be able to take negative values, which is impossible since their sum must be non-negative.\n\nE. The characteristic function of $X$ cannot be expressed as the square of another valid characteristic function for any $p \\in (0,1)$.", "solution": "To determine why a Bernoulli random variable $X$ with parameter $p \\in (0,1)$ is not infinitely divisible, we analyze the consequences of assuming that it is. An infinitely divisible random variable can be decomposed into a sum of $n$ independent and identically distributed (i.i.d.) random variables for any positive integer $n$. We will investigate the case for $n=2$.\n\nLet's assume $X$ is infinitely divisible. Then there exist two i.i.d. random variables, $Y_1$ and $Y_2$, such that $X = Y_1 + Y_2$. The random variable $X$ can only take values in the set $\\{0, 1\\}$.\n\nLet's analyze the possible values (the support) of $Y_1$ and $Y_2$. Let $y_{max}$ be the maximum possible value that $Y_1$ (and $Y_2$) can take. Since $Y_1$ and $Y_2$ are i.i.d., the maximum value of their sum is $y_{max} + y_{max} = 2y_{max}$. This sum must be less than or equal to the maximum value of $X$, which is 1. Thus, $2y_{max} \\le 1$, which implies $y_{max} \\le \\frac{1}{2}$.\n\nSimilarly, let $y_{min}$ be the minimum possible value of $Y_1$. The minimum value of the sum is $2y_{min}$. This must be greater than or equal to the minimum value of $X$, which is 0. Thus, $2y_{min} \\ge 0$, which implies $y_{min} \\ge 0$.\nSo, any value $y$ in the support of $Y_1$ and $Y_2$ must satisfy $0 \\le y \\le \\frac{1}{2}$.\n\nNow, let's consider the events $X=1$ and $X=0$.\nThe event $X=1$ happens with probability $P(X=1)=p$. For the sum $Y_1+Y_2$ to be 1, given that each variable is constrained to be in $[0, \\frac{1}{2}]$, the only possibility is that both $Y_1$ and $Y_2$ must be exactly equal to $\\frac{1}{2}$.\nTherefore, the value $\\frac{1}{2}$ must be in the support of $Y_1$ and $Y_2$. Let $P(Y_1 = \\frac{1}{2}) = q$. Since $Y_1$ and $Y_2$ are i.i.d., $P(Y_2 = \\frac{1}{2}) = q$.\nBecause of their independence, we have:\n$P(X=1) = P(Y_1 = \\frac{1}{2} \\text{ and } Y_2 = \\frac{1}{2}) = P(Y_1 = \\frac{1}{2}) P(Y_2 = \\frac{1}{2}) = q \\cdot q = q^2$.\nSo, $p = q^2$. Since we are given $p \\in (0,1)$, it follows that $q = \\sqrt{p} \\in (0,1)$. This is a valid probability.\n\nNext, consider the event $X=0$, which occurs with probability $P(X=0)=1-p$. For the sum $Y_1+Y_2$ to be 0, given that each variable is non-negative, the only possibility is that both $Y_1$ and $Y_2$ must be exactly equal to 0.\nTherefore, the value 0 must also be in the support of $Y_1$ and $Y_2$. Let $P(Y_1 = 0) = r$. Since they are i.i.d., $P(Y_2 = 0) = r$.\nBecause of their independence, we have:\n$P(X=0) = P(Y_1 = 0 \\text{ and } Y_2 = 0) = P(Y_1 = 0) P(Y_2 = 0) = r \\cdot r = r^2$.\nSo, $1-p = r^2$. Since $p \\in (0,1)$, it follows that $r = \\sqrt{1-p} \\in (0,1)$. This is also a valid probability.\n\nNow, we have established that the component random variables $Y_i$ must be able to take at least two values, 0 and $\\frac{1}{2}$, with non-zero probabilities $r$ and $q$ respectively. Let's consider the outcome where one variable is 0 and the other is $\\frac{1}{2}$. The sum $Y_1+Y_2$ would be $\\frac{1}{2}$. The probability of this event is:\n$P(Y_1 + Y_2 = \\frac{1}{2}) = P(Y_1 = \\frac{1}{2}, Y_2 = 0) + P(Y_1 = 0, Y_2 = \\frac{1}{2})$\nSince $Y_1$ and $Y_2$ are independent:\n$P(Y_1 + Y_2 = \\frac{1}{2}) = P(Y_1 = \\frac{1}{2})P(Y_2 = 0) + P(Y_1 = 0)P(Y_2 = \\frac{1}{2}) = q \\cdot r + r \\cdot q = 2qr$.\n\nSubstituting our expressions for $q$ and $r$:\n$P(Y_1 + Y_2 = \\frac{1}{2}) = 2\\sqrt{p}\\sqrt{1-p} = 2\\sqrt{p(1-p)}$.\nSince we are given $p \\in (0,1)$, both $p$ and $1-p$ are positive. Therefore, the probability $2\\sqrt{p(1-p)}$ is strictly greater than 0.\n\nThis leads to a contradiction. Our assumption that $X = Y_1 + Y_2$ implies that the sum must be able to take the value $\\frac{1}{2}$ with a non-zero probability. However, the original Bernoulli variable $X$ can only take values 0 or 1. Its support is $\\{0, 1\\}$. The value $\\frac{1}{2}$ is not in its support, so $P(X=\\frac{1}{2})$ must be 0.\nThe assumption has led to an impossible conclusion. Therefore, the initial assumption must be false: a Bernoulli random variable with $p \\in (0,1)$ is not infinitely divisible.\n\nThis reasoning directly corresponds to option A. The other options are incorrect:\nB: $E[X] = p$. $E[Y_1+Y_2] = 2E[Y_1]$. So $E[Y_1]=p/2$, which is a valid real number.\nC: $\\text{Var}(X) = p(1-p)$. $\\text{Var}(Y_1+Y_2) = 2\\text{Var}(Y_1)$. So $\\text{Var}(Y_1) = p(1-p)/2$, which is positive for $p \\in (0,1)$.\nD: We showed the values of $Y_i$ must be in $[0, 1/2]$, so no negative values are required.\nE: While true, this is a more advanced argument using characteristic functions. The question asks for the fundamental reason based on the decomposition itself, which is a contradiction in the support of the random variable. The reasoning based on the sum of values is more direct and elementary.", "answer": "$$\\boxed{A}$$", "id": "1308940"}, {"introduction": "The cornerstone of the theory of infinitely divisible distributions is the Lévy-Khintchine representation, which provides a universal recipe for constructing any such distribution from three key ingredients: a drift, a Gaussian component, and a jump process. This advanced practice challenges you to work backwards from this powerful formula [@problem_id:1308904]. By interpreting a given Lévy-Khintchine triplet, you will identify the specific building blocks of a compound Poisson process and use this insight to calculate a concrete probability, bridging the gap between abstract theory and practical application.", "problem": "An infinitely divisible probability distribution is one whose characteristic function $\\phi_X(u) = E[\\exp(iuX)]$ can be expressed using the Lévy-Khintchine representation. For any such random variable $X$, its characteristic function is given by:\n$$ \\phi_X(u) = \\exp\\left(i\\gamma u - \\frac{1}{2}\\sigma^2 u^2 + \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left(e^{iux} - 1 - iux \\mathbf{1}_{|x|1}\\right) \\nu(dx)\\right) $$\nHere, the triplet $(\\gamma, \\sigma^2, \\nu)$ uniquely characterizes the distribution. The parameter $\\gamma \\in \\mathbb{R}$ is a drift coefficient, $\\sigma^2 \\ge 0$ is the variance of a continuous Gaussian component, and $\\nu$ is the Lévy measure, which satisfies $\\int_{\\mathbb{R}\\setminus\\{0\\}} \\min(1, x^2) \\nu(dx)  \\infty$. The term $\\mathbf{1}_{|x|1}$ is an indicator function that equals 1 if $|x|1$ and 0 otherwise.\n\nConsider a random variable $X$ whose distribution is characterized by the Lévy-Khintchine triplet $(\\gamma, \\sigma^2, \\nu)$ with the following specific components:\n- Drift: $\\gamma = -1$\n- Gaussian variance: $\\sigma^2 = 0$\n- Lévy measure: $\\nu(dx) = 3 \\delta_{1}(dx) + 1.5 \\delta_{2}(dx)$, where $\\delta_{a}(dx)$ denotes the Dirac delta measure centered at the point $x=a$.\n\nCalculate the probability $P(X=2)$. Express your answer as a real number rounded to four significant figures.", "solution": "The problem asks for the probability $P(X=2)$ for a random variable $X$ defined by a specific Lévy-Khintchine triplet $(\\gamma, \\sigma^2, \\nu)$. The given parameters are $\\gamma = -1$, $\\sigma^2 = 0$, and the Lévy measure $\\nu(dx) = 3 \\delta_1(dx) + 1.5 \\delta_2(dx)$.\n\nFirst, we substitute these parameters into the general Lévy-Khintchine formula for the characteristic function $\\phi_X(u)$:\n$$ \\phi_X(u) = \\exp\\left(i(-1)u - \\frac{1}{2}(0)u^2 + \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left(e^{iux} - 1 - iux \\mathbf{1}_{|x|1}\\right) \\nu(dx)\\right) $$\n$$ \\phi_X(u) = \\exp\\left(-iu + \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left(e^{iux} - 1 - iux \\mathbf{1}_{|x|1}\\right) (3 \\delta_1(dx) + 1.5 \\delta_2(dx))\\right) $$\n\nNext, we evaluate the integral over the Lévy measure. The integral of a function $f(x)$ with respect to a Dirac delta measure $\\delta_a(dx)$ is simply $f(a)$. Applying this property, the integral becomes a sum:\n$$ \\int_{\\mathbb{R}\\setminus\\{0\\}} (\\dots) (3 \\delta_1(dx) + 1.5 \\delta_2(dx)) = 3 \\left(e^{iu(1)} - 1 - iu(1) \\mathbf{1}_{|1|1}\\right) + 1.5 \\left(e^{iu(2)} - 1 - iu(2) \\mathbf{1}_{|2|1}\\right) $$\nThe indicator function $\\mathbf{1}_{|x|1}$ evaluates to 0 for both $x=1$ and $x=2$, since neither $|1|1$ nor $|2|1$ is true. Therefore, the truncation terms $iux \\mathbf{1}_{|x|1}$ vanish. The integral simplifies to:\n$$ 3(e^{iu} - 1) + 1.5(e^{i2u} - 1) $$\n\nNow we substitute this result back into the expression for $\\phi_X(u)$:\n$$ \\phi_X(u) = \\exp(-iu + 3(e^{iu} - 1) + 1.5(e^{i2u} - 1)) $$\nWe can separate the terms in the exponent to see the structure more clearly:\n$$ \\phi_X(u) = \\exp(-iu) \\cdot \\exp(3(e^{iu} - 1)) \\cdot \\exp(1.5(e^{i2u} - 1)) $$\n\nThis expression is a product of three characteristic functions. We can interpret each term:\n1. $\\phi_C(u) = \\exp(-iu)$ is the characteristic function of a degenerate random variable (a constant) $C = -1$.\n2. $\\phi_{Y_1}(u) = \\exp(3(e^{iu} - 1))$ is the characteristic function of a Poisson random variable $Y_1$ with rate $\\lambda_1 = 3$. Let's call this variable $N_1 \\sim \\text{Poisson}(3)$.\n3. $\\phi_{Y_2}(u) = \\exp(1.5(e^{i2u} - 1))$ is the characteristic function of a scaled Poisson random variable. A random variable $Z = a N$, with $N \\sim \\text{Poisson}(\\lambda)$, has characteristic function $\\phi_Z(u) = E[\\exp(iuaN)] = E[(e^{iua})^N] = \\exp(\\lambda(e^{iua}-1))$. Thus, our third term corresponds to a random variable $Y_2 = 2 N_2$, where $N_2 \\sim \\text{Poisson}(1.5)$.\n\nSince the characteristic function of $X$ is the product of the characteristic functions of $C$, $N_1$, and $2N_2$, the random variable $X$ is the sum of these three independent random variables:\n$$ X = C + N_1 + 2N_2 = -1 + N_1 + 2N_2 $$\nwhere $N_1 \\sim \\text{Poisson}(3)$ and $N_2 \\sim \\text{Poisson}(1.5)$ are independent.\n\nWe need to calculate $P(X=2)$. This is equivalent to finding $P(N_1 + 2N_2 - 1 = 2)$, which simplifies to $P(N_1 + 2N_2 = 3)$. We can find this probability by summing over all possible non-negative integer values for $k_1$ (from $N_1$) and $k_2$ (from $N_2$) that satisfy the equation $k_1 + 2k_2 = 3$.\nThe possible pairs $(k_1, k_2)$ are:\n- If $k_2=0$, then $k_1=3$. This gives the pair $(3, 0)$.\n- If $k_2=1$, then $k_1=1$. This gives the pair $(1, 1)$.\n- If $k_2 \\ge 2$, then $2k_2 \\ge 4$, so there are no non-negative integer solutions for $k_1$.\n\nThe probability is the sum of the probabilities of these two mutually exclusive events:\n$$ P(N_1 + 2N_2 = 3) = P(N_1=3, N_2=0) + P(N_1=1, N_2=1) $$\nDue to independence, we can write this as:\n$$ P(N_1 + 2N_2 = 3) = P(N_1=3)P(N_2=0) + P(N_1=1)P(N_2=1) $$\nUsing the probability mass function for a Poisson distribution, $P(N=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}$:\n- $P(N_1=3) = \\frac{e^{-3}3^3}{3!} = \\frac{27}{6}e^{-3} = 4.5 e^{-3}$\n- $P(N_2=0) = \\frac{e^{-1.5}(1.5)^0}{0!} = e^{-1.5}$\n- $P(N_1=1) = \\frac{e^{-3}3^1}{1!} = 3e^{-3}$\n- $P(N_2=1) = \\frac{e^{-1.5}(1.5)^1}{1!} = 1.5e^{-1.5}$\n\nSubstituting these into the sum:\n$$ P(X=2) = (4.5 e^{-3})(e^{-1.5}) + (3 e^{-3})(1.5 e^{-1.5}) $$\n$$ P(X=2) = 4.5 e^{-4.5} + 4.5 e^{-4.5} $$\n$$ P(X=2) = 9 e^{-4.5} $$\n\nFinally, we compute the numerical value and round to four significant figures:\n$$ 9 e^{-4.5} \\approx 9 \\times 0.0111089965 \\approx 0.0999809685 $$\nRounding to four significant figures gives $0.09998$.", "answer": "$$\\boxed{0.09998}$$", "id": "1308904"}]}