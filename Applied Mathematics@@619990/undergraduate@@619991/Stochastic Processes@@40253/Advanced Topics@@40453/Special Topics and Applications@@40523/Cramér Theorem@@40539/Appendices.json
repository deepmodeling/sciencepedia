{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a cornerstone example: determining the rate function for the sample mean of standard normal random variables. This exercise is fundamental because the Gaussian distribution's mathematical properties simplify the calculations, allowing you to focus on the core procedure of Cramér's theorem. By working through this problem [@problem_id:1370561], you will see how the Legendre-Fenchel transform converts the familiar quadratic cumulant generating function into a corresponding quadratic rate function, providing a clear and tangible first step into the world of large deviations.", "problem": "In a digital communication system, a signal is transmitted over a noisy channel. An analyst models the noise affecting each of a series of $n$ transmissions as an independent and identically distributed (i.i.d.) random variable, $X_i$, drawn from a standard normal distribution, $X_i \\sim \\mathcal{N}(0, 1)$. The performance of the system is related to the behavior of the sample mean of the noise, $M_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nA critical error is associated with the rare event that this average noise level deviates significantly from its expected value of zero. According to Large Deviation Theory, and specifically Cramér's theorem, the probability that the sample mean $M_n$ is close to a non-zero value $x$ decays exponentially for large $n$. This relationship is described by $P(M_n \\approx x) \\asymp \\exp(-nI(x))$, where $I(x)$ is known as the rate function.\n\nYour task is to find the analytical expression for this rate function $I(x)$ for the sample mean of i.i.d. standard normal random variables. The expression should be valid for any real number $x$.", "solution": "The problem asks for the rate function $I(x)$ for the sample mean of i.i.d. standard normal random variables. According to Cramér's theorem, the rate function $I(x)$ is the Legendre-Fenchel transform of the cumulant generating function (CGF) of a single random variable $X$.\n\nThe formula for the rate function is:\n$$I(x) = \\sup_{\\lambda \\in \\mathbb{R}} (\\lambda x - \\Lambda(\\lambda))$$\nwhere $\\Lambda(\\lambda)$ is the CGF of $X$.\n\nThe CGF is defined as the natural logarithm of the moment generating function (MGF), $M(\\lambda)$:\n$$\\Lambda(\\lambda) = \\ln(M(\\lambda))$$\nAnd the MGF for a random variable $X$ is defined as:\n$$M(\\lambda) = E[\\exp(\\lambda X)]$$\n\nLet's first compute the MGF for a single standard normal random variable $X \\sim \\mathcal{N}(0, 1)$. The probability density function (PDF) of $X$ is $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$.\n\nThe MGF is calculated by the integral:\n$$M(\\lambda) = \\int_{-\\infty}^{\\infty} \\exp(\\lambda x) f(x) \\,dx = \\int_{-\\infty}^{\\infty} \\exp(\\lambda x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) \\,dx$$\n$$M(\\lambda) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(\\lambda x - \\frac{x^2}{2}\\right) \\,dx$$\n\nTo solve this integral, we complete the square for the term in the exponent:\n$$\\lambda x - \\frac{x^2}{2} = -\\frac{1}{2}(x^2 - 2\\lambda x) = -\\frac{1}{2}((x - \\lambda)^2 - \\lambda^2) = -\\frac{(x - \\lambda)^2}{2} + \\frac{\\lambda^2}{2}$$\n\nSubstituting this back into the integral:\n$$M(\\lambda) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\lambda)^2}{2} + \\frac{\\lambda^2}{2}\\right) \\,dx$$\n$$M(\\lambda) = \\exp\\left(\\frac{\\lambda^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\lambda)^2}{2}\\right) \\,dx$$\n\nThe integral on the right is the integral of the PDF of a normal distribution with mean $\\lambda$ and variance 1, $\\mathcal{N}(\\lambda, 1)$, over its entire domain. Therefore, the integral evaluates to 1.\nSo, the MGF of a standard normal random variable is:\n$$M(\\lambda) = \\exp\\left(\\frac{\\lambda^2}{2}\\right)$$\n\nNext, we find the CGF, $\\Lambda(\\lambda)$:\n$$\\Lambda(\\lambda) = \\ln(M(\\lambda)) = \\ln\\left(\\exp\\left(\\frac{\\lambda^2}{2}\\right)\\right) = \\frac{\\lambda^2}{2}$$\n\nNow we can compute the rate function $I(x)$ by finding the supremum of $\\lambda x - \\Lambda(\\lambda)$:\n$$I(x) = \\sup_{\\lambda \\in \\mathbb{R}} \\left(\\lambda x - \\frac{\\lambda^2}{2}\\right)$$\n\nLet $g(\\lambda) = \\lambda x - \\frac{\\lambda^2}{2}$. To find the value of $\\lambda$ that maximizes this expression for a fixed $x$, we can use calculus. We take the derivative of $g(\\lambda)$ with respect to $\\lambda$ and set it to zero.\n$$\\frac{dg}{d\\lambda} = x - \\lambda$$\nSetting the derivative to zero gives:\n$$x - \\lambda = 0 \\implies \\lambda = x$$\n\nTo confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2g}{d\\lambda^2} = -1$$\nSince the second derivative is negative, the function $g(\\lambda)$ has a global maximum at $\\lambda = x$.\n\nFinally, we substitute this value of $\\lambda$ back into the expression for $g(\\lambda)$ to find the supremum, which is the rate function $I(x)$:\n$$I(x) = (x)x - \\frac{x^2}{2} = x^2 - \\frac{x^2}{2} = \\frac{x^2}{2}$$\n\nThus, the analytical expression for the rate function is $I(x) = \\frac{x^2}{2}$.", "answer": "$$\\boxed{\\frac{x^{2}}{2}}$$", "id": "1370561"}, {"introduction": "Having mastered the continuous case, we now turn our attention to discrete random variables with the Bernoulli distribution, a building block for modeling binary outcomes. This practice [@problem_id:2972664] moves beyond a simple calculation to reveal a deeper insight: the rate function you will derive is precisely the relative entropy, also known as the Kullback-Leibler divergence. This exercise is crucial for understanding the profound link between the probability of rare events and the informational 'distance' between statistical models.", "problem": "Let $\\{X_{i}\\}_{i \\geq 1}$ be independent and identically distributed random variables with the $\\mathrm{Bernoulli}(p)$ law for a fixed parameter $p \\in (0,1)$. Define the empirical mean $\\,\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\,$ and fix a value $a \\in (0,1)$. Starting only from the definitions of the cumulant generating function $\\Lambda(\\theta) = \\ln \\mathbb{E}[\\exp(\\theta X_{1})]$ and the Legendre–Fenchel transform that yields the Cramér rate function $I(a) = \\sup_{\\theta \\in \\mathbb{R}}\\{\\theta a - \\Lambda(\\theta)\\}$ for the sums of independent and identically distributed random variables, derive the explicit closed-form expression of $I(a)$ for the chosen $a \\in (0,1)$. Interpret $I(a)$ as the specific relative entropy (also called the entropy rate for this independent process) of the empirical law concentrating at $a$ with respect to the reference law parameter $p$, but report only the final explicit formula. Your final answer must be a single closed-form analytic expression in terms of $a$ and $p$, with no numerical approximation.", "solution": "The derivation of the Cramér rate function $I(a)$ begins with the calculation of the cumulant generating function (CGF), $\\Lambda(\\theta)$, for a single random variable $X_1$ following the $\\mathrm{Bernoulli}(p)$ distribution.\n\nBy definition, the CGF is $\\Lambda(\\theta) = \\ln \\mathbb{E}[\\exp(\\theta X_1)]$.\nFor a $\\mathrm{Bernoulli}(p)$ random variable, $X_1$ takes the value $1$ with probability $p$ and the value $0$ with probability $1-p$. The expectation of $\\exp(\\theta X_1)$ is therefore:\n$$\n\\mathbb{E}[\\exp(\\theta X_1)] = \\exp(\\theta \\cdot 1) P(X_1=1) + \\exp(\\theta \\cdot 0) P(X_1=0) = \\exp(\\theta) p + 1 \\cdot (1-p) = p\\exp(\\theta) + 1-p\n$$\nThus, the cumulant generating function is:\n$$\n\\Lambda(\\theta) = \\ln(p\\exp(\\theta) + 1-p)\n$$\nThe Cramér rate function, $I(a)$, is the Legendre–Fenchel transform of $\\Lambda(\\theta)$, defined as:\n$$\nI(a) = \\sup_{\\theta \\in \\mathbb{R}}\\{\\theta a - \\Lambda(\\theta)\\}\n$$\nTo find the supremum, we define a function $f(\\theta) = \\theta a - \\Lambda(\\theta)$ and find the value of $\\theta$ that maximizes it by using calculus. We differentiate $f(\\theta)$ with respect to $\\theta$ and set the derivative to zero.\n\nFirst, we find the derivative of $\\Lambda(\\theta)$:\n$$\n\\Lambda'(\\theta) = \\frac{d}{d\\theta}\\ln(p\\exp(\\theta) + 1-p) = \\frac{p\\exp(\\theta)}{p\\exp(\\theta) + 1-p}\n$$\nThe derivative of $f(\\theta)$ is:\n$$\nf'(\\theta) = \\frac{d}{d\\theta}(\\theta a - \\Lambda(\\theta)) = a - \\Lambda'(\\theta) = a - \\frac{p\\exp(\\theta)}{p\\exp(\\theta) + 1-p}\n$$\nSetting $f'(\\theta) = 0$ to find the critical point, which we denote $\\theta^*$:\n$$\na = \\frac{p\\exp(\\theta^*)}{p\\exp(\\theta^*) + 1-p}\n$$\nWe now solve this equation for $\\exp(\\theta^*)$:\n$$\na(p\\exp(\\theta^*) + 1-p) = p\\exp(\\theta^*)\n$$\n$$\nap\\exp(\\theta^*) + a(1-p) = p\\exp(\\theta^*)\n$$\n$$\na(1-p) = p\\exp(\\theta^*) - ap\\exp(\\theta^*)\n$$\n$$\na(1-p) = (p - ap)\\exp(\\theta^*) = p(1-a)\\exp(\\theta^*)\n$$\n$$\n\\exp(\\theta^*) = \\frac{a(1-p)}{p(1-a)}\n$$\nTaking the natural logarithm of both sides gives the value of $\\theta^*$:\n$$\n\\theta^* = \\ln\\left(\\frac{a(1-p)}{p(1-a)}\\right)\n$$\nTo confirm that this point corresponds to a maximum, we examine the second derivative, $f''(\\theta) = -\\Lambda''(\\theta)$. The second derivative of the CGF is:\n$$\n\\Lambda''(\\theta) = \\frac{d}{d\\theta} \\left( \\frac{p\\exp(\\theta)}{p\\exp(\\theta) + 1-p} \\right) = \\frac{p\\exp(\\theta)(p\\exp(\\theta)+1-p) - p\\exp(\\theta)(p\\exp(\\theta))}{(p\\exp(\\theta) + 1-p)^2} = \\frac{p(1-p)\\exp(\\theta)}{(p\\exp(\\theta) + 1-p)^2}\n$$\nSince $p \\in (0,1)$ and $\\exp(\\theta)>0$, we have $\\Lambda''(\\theta) > 0$ for all $\\theta \\in \\mathbb{R}$. This means $\\Lambda(\\theta)$ is a strictly convex function. Consequently, $f(\\theta) = \\theta a - \\Lambda(\\theta)$ is a strictly concave function, and the critical point $\\theta^*$ is indeed the unique global maximum.\n\nThe value of the rate function $I(a)$ is the value of $f(\\theta)$ at this maximum, $I(a) = f(\\theta^*) = a\\theta^* - \\Lambda(\\theta^*)$.\n$$\nI(a) = a \\ln\\left(\\frac{a(1-p)}{p(1-a)}\\right) - \\ln(p\\exp(\\theta^*) + 1-p)\n$$\nWe substitute the expression for $\\exp(\\theta^*)$ into the argument of the logarithm in the second term:\n$$\np\\exp(\\theta^*) + 1-p = p\\left(\\frac{a(1-p)}{p(1-a)}\\right) + 1-p = \\frac{a(1-p)}{1-a} + (1-p)\n$$\nFactoring out $(1-p)$:\n$$\n(1-p) \\left( \\frac{a}{1-a} + 1 \\right) = (1-p) \\left( \\frac{a + 1-a}{1-a} \\right) = \\frac{1-p}{1-a}\n$$\nTherefore, $\\Lambda(\\theta^*) = \\ln\\left(\\frac{1-p}{1-a}\\right)$.\nSubstituting this back into the expression for $I(a)$:\n$$\nI(a) = a \\ln\\left(\\frac{a(1-p)}{p(1-a)}\\right) - \\ln\\left(\\frac{1-p}{1-a}\\right)\n$$\nUsing the properties of logarithms, we can expand and simplify this expression:\n$$\nI(a) = a \\left[ \\ln(a) + \\ln(1-p) - \\ln(p) - \\ln(1-a) \\right] - \\left[ \\ln(1-p) - \\ln(1-a) \\right]\n$$\n$$\nI(a) = a\\ln(a) + a\\ln(1-p) - a\\ln(p) - a\\ln(1-a) - \\ln(1-p) + \\ln(1-a)\n$$\nGrouping terms with common logarithms:\n$$\nI(a) = a(\\ln(a) - \\ln(p)) + (1-a)(\\ln(1-a) - \\ln(1-p))\n$$\n$$\nI(a) = a\\ln\\left(\\frac{a}{p}\\right) + (1-a)\\ln\\left(\\frac{1-a}{1-p}\\right)\n$$\nThis is the final closed-form expression for the Cramér rate function $I(a)$ for a sequence of i.i.d. $\\mathrm{Bernoulli}(p)$ random variables. This expression is also known as the Kullback–Leibler divergence, or relative entropy, between a $\\mathrm{Bernoulli}(a)$ and a $\\mathrm{Bernoulli}(p)$ distribution.", "answer": "$$\n\\boxed{a\\ln\\left(\\frac{a}{p}\\right) + (1-a)\\ln\\left(\\frac{1-a}{1-p}\\right)}\n$$", "id": "2972664"}, {"introduction": "In our final practice, we shift from deriving rate functions to actively using them to solve a practical problem. This exercise places you in the role of an analyst modeling a system where you must balance operational parameters against the risk of a rare failure event. By combining the large deviation approximation with optimization techniques [@problem_id:1294716], you will determine a critical system parameter, showcasing how Cramér's theorem provides a powerful quantitative tool for risk assessment and engineering design.", "problem": "In the modeling of digital communication systems, bit errors are sometimes assumed to occur according to a Poisson process. Consider a simplified model where data is transmitted in large blocks of $n$ segments. For each segment $i=1, \\dots, n$, the number of bit errors, $X_i$, is an independent random variable following a Poisson distribution with parameter $\\lambda$. The parameter $\\lambda$ represents the average error rate per segment and can be adjusted.\n\nWhile a higher $\\lambda$ might correspond to a higher data throughput, it also increases the system's instability. The probability of the system maintaining operational stability over the transmission of one block is found to decay exponentially with both $n$ and $\\lambda$, given by the relationship $P_{\\text{stable}} = C \\exp(-n \\beta \\lambda)$, where $\\beta$ is a positive dimensionless constant representing the instability cost, and $C$ is a normalization constant.\n\nA system alert is triggered if the sample mean of the errors, $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$, exceeds a predefined critical threshold $a$. It is given that $a > \\frac{a}{1+\\beta}$.\n\nWe are interested in identifying the most vulnerable operating rate, $\\lambda_v$. This is defined as the error rate $\\lambda$ (where $\\lambda < a$) that maximizes the joint probability of maintaining system stability while simultaneously triggering a system alert. Using a large deviation approximation for the probability of the alert event, determine the value of this vulnerable rate, $\\lambda_v$. Your answer should be a closed-form analytic expression in terms of $a$ and $\\beta$.", "solution": "We model the alert event by the large deviation principle for the sample mean of independent and identically distributed Poisson random variables. Let $X_{1},\\dots,X_{n}$ be independent with $X_{i} \\sim \\text{Poisson}(\\lambda)$ and $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. The moment generating function of $X_{1}$ is\n$$\nM(t)=\\mathbb{E}[\\exp(tX_{1})]=\\exp\\!\\big(\\lambda(\\exp(t)-1)\\big),\n$$\nso the cumulant generating function is\n$$\n\\Lambda(t)=\\ln M(t)=\\lambda(\\exp(t)-1).\n$$\nBy Cramér’s theorem, the good rate function for the empirical mean is the Legendre transform\n$$\nI_{\\lambda}(x)=\\sup_{t \\in \\mathbb{R}}\\big(tx-\\Lambda(t)\\big).\n$$\nOptimizing, set the derivative with respect to $t$ to zero:\n$$\n\\frac{\\partial}{\\partial t}\\big(tx-\\Lambda(t)\\big)=x-\\lambda\\exp(t)=0 \\quad \\Rightarrow \\quad \\exp(t^{\\ast})=\\frac{x}{\\lambda}, \\quad t^{\\ast}=\\ln\\!\\left(\\frac{x}{\\lambda}\\right).\n$$\nEvaluating at $t^{\\ast}$ gives\n$$\nI_{\\lambda}(x)=t^{\\ast}x-\\Lambda(t^{\\ast})=x\\ln\\!\\left(\\frac{x}{\\lambda}\\right)-\\lambda\\left(\\frac{x}{\\lambda}-1\\right)=x\\ln\\!\\left(\\frac{x}{\\lambda}\\right)-x+\\lambda.\n$$\nFor the alert threshold $a$ with $a>\\lambda$, the large deviation approximation is\n$$\n\\mathbb{P}\\!\\left(\\bar{X}_{n}\\ge a\\right)\\approx \\exp\\!\\big(-n\\,I_{\\lambda}(a)\\big)=\\exp\\!\\left(-n\\left[a\\ln\\!\\left(\\frac{a}{\\lambda}\\right)-a+\\lambda\\right]\\right).\n$$\nThe stability probability is given as $P_{\\text{stable}}=C\\exp(-n\\beta\\lambda)$ with $C$ independent of $\\lambda$. Hence, the joint probability of stability and alert is approximated (up to the common prefactor $C$) by\n$$\nJ_{n}(\\lambda)\\approx C\\exp\\!\\left(-n\\left[\\beta\\lambda+I_{\\lambda}(a)\\right]\\right)\n=C\\exp\\!\\left(-n\\left[a\\ln\\!\\left(\\frac{a}{\\lambda}\\right)-a+(1+\\beta)\\lambda\\right]\\right).\n$$\nFor large $n$, maximizing $J_{n}(\\lambda)$ over $\\lambda<a$ is equivalent to minimizing the exponent\n$$\nF(\\lambda)=a\\ln\\!\\left(\\frac{a}{\\lambda}\\right)-a+(1+\\beta)\\lambda,\n$$\nover $\\lambda \\in (0,a)$. Differentiate:\n$$\nF'(\\lambda)=-\\frac{a}{\\lambda}+(1+\\beta),\n$$\nand set $F'(\\lambda)=0$ to obtain\n$$\n-\\frac{a}{\\lambda}+(1+\\beta)=0 \\quad \\Rightarrow \\quad \\lambda=\\frac{a}{1+\\beta}.\n$$\nThe second derivative is $F''(\\lambda)=\\frac{a}{\\lambda^{2}}>0$, so this critical point is the unique minimizer. Since $\\beta>0$, we have $\\lambda=\\frac{a}{1+\\beta}<a$, so the constraint $\\lambda<a$ is satisfied. The provided inequality $a>\\frac{a}{1+\\beta}$ is consistent with $\\beta>0$ and confirms feasibility.\n\nTherefore, the most vulnerable operating rate is\n$$\n\\lambda_{v}=\\frac{a}{1+\\beta}.\n$$", "answer": "$$\\boxed{\\frac{a}{1+\\beta}}$$", "id": "1294716"}]}