{"hands_on_practices": [{"introduction": "Understanding the long-term behavior of a system is a central goal in the study of Markov chains. This first exercise provides hands-on practice with the foundational method for finding the stationary distribution of a continuous-time Markov chain. By modeling a simple computer server [@problem_id:1333676], you will learn to construct the generator matrix $Q$ from a system's description and solve the global balance equations, $\\pi Q = \\mathbf{0}$, to determine the proportion of time the system spends in each state.", "problem": "Consider a simplified model for a computer server that can be in one of three states: {1: Idle, 2: Computing, 3: Updating}. The server transitions between these states according to a continuous-time Markov chain.\nThe transitions are as follows:\n- From 'Idle' (state 1), a new task arrives, causing a transition to 'Computing' (state 2) at a rate of $3\\lambda$.\n- From 'Idle' (state 1), the system can initiate a self-update, transitioning to 'Updating' (state 3) at a rate of $\\lambda$.\n- From 'Computing' (state 2), the current task can complete, returning the server to 'Idle' (state 1) at a rate of $4\\lambda$.\n- From 'Computing' (state 2), a mandatory system patch can interrupt the task, forcing a transition to 'Updating' (state 3) at a rate of $\\lambda$.\n- From 'Updating' (state 3), the update process completes, and the server always returns to 'Idle' (state 1) at a rate of $4\\lambda$.\nNo other transitions are possible. The parameter $\\lambda$ is a positive constant representing the base rate of transitions.\n\nAssuming the system has been running for a very long time, it reaches a stationary state. Determine the stationary probability distribution vector $\\pi = (\\pi_1, \\pi_2, \\pi_3)$, where $\\pi_i$ is the long-term probability of finding the server in state $i$.", "solution": "Let the state space of the continuous-time Markov chain be $S = \\{1, 2, 3\\}$, corresponding to the states {Idle, Computing, Updating}. The stationary distribution is a probability vector $\\pi = (\\pi_1, \\pi_2, \\pi_3)$ that satisfies the equation $\\boldsymbol{\\pi} Q = \\mathbf{0}$, where $Q$ is the generator matrix of the chain, and the normalization condition $\\sum_{i=1}^3 \\pi_i = 1$.\n\nFirst, we construct the generator matrix $Q$. The off-diagonal elements $q_{ij}$ for $i \\neq j$ are the transition rates $\\lambda_{ij}$ from state $i$ to state $j$. The diagonal elements $q_{ii}$ are given by $q_{ii} = -\\sum_{j \\neq i} q_{ij}$. From the problem description, the transition rates are:\n$q_{12} = 3\\lambda$\n$q_{13} = \\lambda$\n$q_{21} = 4\\lambda$\n$q_{23} = \\lambda$\n$q_{31} = 4\\lambda$\n$q_{32} = 0$ (no transition from Updating to Computing)\nAll other off-diagonal rates not listed are zero.\n\nNow, we calculate the diagonal elements:\n$q_{11} = -(q_{12} + q_{13}) = -(3\\lambda + \\lambda) = -4\\lambda$\n$q_{22} = -(q_{21} + q_{23}) = -(4\\lambda + \\lambda) = -5\\lambda$\n$q_{33} = -(q_{31} + q_{32}) = -(4\\lambda + 0) = -4\\lambda$\n\nSo, the generator matrix $Q$ is:\n$$\nQ = \\begin{pmatrix}\n-4\\lambda & 3\\lambda & \\lambda \\\\\n4\\lambda & -5\\lambda & \\lambda \\\\\n4\\lambda & 0 & -4\\lambda\n\\end{pmatrix}\n$$\n\nThe condition $\\boldsymbol{\\pi} Q = \\mathbf{0}$ yields a system of linear equations. We can divide by the common factor $\\lambda$ since $\\lambda > 0$.\n$$\n(\\pi_1, \\pi_2, \\pi_3) \\begin{pmatrix}\n-4 & 3 & 1 \\\\\n4 & -5 & 1 \\\\\n4 & 0 & -4\n\\end{pmatrix} = (0, 0, 0)\n$$\nThis results in the following equations, corresponding to each column:\n1.  $-4\\pi_1 + 4\\pi_2 + 4\\pi_3 = 0$\n2.  $3\\pi_1 - 5\\pi_2 + 0\\pi_3 = 0$\n3.  $\\pi_1 + \\pi_2 - 4\\pi_3 = 0$\n\nThis system of three equations with three unknowns is linearly dependent. We can use any two of these equations along with the normalization condition $\\pi_1 + \\pi_2 + \\pi_3 = 1$.\n\nLet's use equation (2):\n$3\\pi_1 - 5\\pi_2 = 0 \\implies 5\\pi_2 = 3\\pi_1 \\implies \\pi_2 = \\frac{3}{5}\\pi_1$.\n\nNext, let's simplify equation (1) by dividing by 4:\n$-\\pi_1 + \\pi_2 + \\pi_3 = 0 \\implies \\pi_3 = \\pi_1 - \\pi_2$.\nNow, substitute the expression for $\\pi_2$ in terms of $\\pi_1$:\n$\\pi_3 = \\pi_1 - \\frac{3}{5}\\pi_1 = \\frac{2}{5}\\pi_1$.\n\nWe now have both $\\pi_2$ and $\\pi_3$ expressed in terms of $\\pi_1$. We can use the normalization condition to solve for $\\pi_1$:\n$\\pi_1 + \\pi_2 + \\pi_3 = 1$\n$\\pi_1 + \\frac{3}{5}\\pi_1 + \\frac{2}{5}\\pi_1 = 1$\nMultiplying everything by 5 to clear the denominators:\n$5\\pi_1 + 3\\pi_1 + 2\\pi_1 = 5$\n$10\\pi_1 = 5$\n$\\pi_1 = \\frac{5}{10} = \\frac{1}{2}$.\n\nNow we can find the values of $\\pi_2$ and $\\pi_3$:\n$\\pi_2 = \\frac{3}{5}\\pi_1 = \\frac{3}{5} \\left(\\frac{1}{2}\\right) = \\frac{3}{10}$.\n$\\pi_3 = \\frac{2}{5}\\pi_1 = \\frac{2}{5} \\left(\\frac{1}{2}\\right) = \\frac{2}{10} = \\frac{1}{5}$.\n\nThe stationary distribution vector is $\\pi = (\\pi_1, \\pi_2, \\pi_3) = \\left(\\frac{1}{2}, \\frac{3}{10}, \\frac{1}{5}\\right)$.\nWe can verify this result with the third equation from the system: $\\pi_1 + \\pi_2 - 4\\pi_3 = \\frac{1}{2} + \\frac{3}{10} - 4\\left(\\frac{1}{5}\\right) = \\frac{5}{10} + \\frac{3}{10} - \\frac{8}{10} = 0$, which holds true. The sum of probabilities is $\\frac{1}{2} + \\frac{3}{10} + \\frac{1}{5} = \\frac{5}{10} + \\frac{3}{10} + \\frac{2}{10} = \\frac{10}{10} = 1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{3}{10} & \\frac{1}{5} \\end{pmatrix}}$$", "id": "1333676"}, {"introduction": "While the general method of solving $\\pi Q = \\mathbf{0}$ is always applicable, many real-world systems, especially in queuing theory, exhibit a special structure known as a birth-death process. In this practice [@problem_id:1333674], you will analyze a simple web server model that fits this pattern, where states represent the number of users. This will allow you to use the more direct and intuitive detailed balance equations, a powerful shortcut for this important class of stochastic models.", "problem": "Consider a simplified model for a small web server that can handle a maximum of two concurrent user sessions. Let $N(t)$ be the number of active sessions at time $t$. The state space for the number of sessions is therefore $\\{0, 1, 2\\}$.\n\nNew session requests arrive according to a Poisson process with a constant rate $\\lambda > 0$. If the server is not at full capacity (i.e., if $N(t) < 2$), an incoming request is accepted, and the number of active sessions increases by one. If the server is at full capacity ($N(t) = 2$), any new incoming requests are rejected and have no effect on the system.\n\nEach active session terminates independently of all other sessions and new session arrivals. The time until a single session terminates is exponentially distributed with rate $\\mu > 0$. This means that if there are $n$ active sessions, the total rate at which sessions terminate is $n\\mu$.\n\nAssuming the system has been running for a long time and has reached a statistical equilibrium, determine the long-run probabilities for the number of active sessions. Let these probabilities be $p_0$, $p_1$, and $p_2$ for having 0, 1, and 2 sessions, respectively.\n\nYour answer should be in the form of a row vector $(p_0, p_1, p_2)$, where each component is an expression in terms of the parameters $\\lambda$ and $\\mu$.", "solution": "We model $\\{N(t)\\}$ as a continuous-time birth–death Markov chain on the state space $\\{0,1,2\\}$. The birth (arrival) rates are $\\lambda_{0}=\\lambda$, $\\lambda_{1}=\\lambda$, and $\\lambda_{2}=0$ (blocked at capacity). The death (service completion) rates are $\\mu_{1}=\\mu$ and $\\mu_{2}=2\\mu$.\n\nIn steady state, for a birth–death chain, detailed balance holds:\n$$\np_{n}\\lambda_{n}=p_{n+1}\\mu_{n+1}.\n$$\nApplying this,\n$$\np_{0}\\lambda=p_{1}\\mu \\quad\\Rightarrow\\quad p_{1}=\\frac{\\lambda}{\\mu}\\,p_{0},\n$$\n$$\np_{1}\\lambda=p_{2}\\cdot 2\\mu \\quad\\Rightarrow\\quad p_{2}=\\frac{\\lambda}{2\\mu}\\,p_{1}=\\frac{\\lambda^{2}}{2\\mu^{2}}\\,p_{0}.\n$$\n\nUse normalization $\\sum_{n=0}^{2}p_{n}=1$:\n$$\np_{0}+p_{1}+p_{2}=p_{0}\\left(1+\\frac{\\lambda}{\\mu}+\\frac{\\lambda^{2}}{2\\mu^{2}}\\right)=1,\n$$\nhence\n$$\np_{0}=\\frac{1}{1+\\frac{\\lambda}{\\mu}+\\frac{\\lambda^{2}}{2\\mu^{2}}}=\\frac{2\\mu^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}}.\n$$\nThen\n$$\np_{1}=\\frac{\\lambda}{\\mu}p_{0}=\\frac{2\\lambda\\mu}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}},\\qquad\np_{2}=\\frac{\\lambda^{2}}{2\\mu^{2}}p_{0}=\\frac{\\lambda^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}}.\n$$\n\nTherefore, the long-run probability vector is\n$$\n\\begin{pmatrix}\n\\frac{2\\mu^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}} &\n\\frac{2\\lambda\\mu}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}} &\n\\frac{\\lambda^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{2\\mu^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}} & \\frac{2\\lambda\\mu}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}} & \\frac{\\lambda^{2}}{2\\mu^{2}+2\\lambda\\mu+\\lambda^{2}}\\end{pmatrix}}$$", "id": "1333674"}, {"introduction": "Many practical applications involve systems with a large or even arbitrary number of states, making direct matrix solutions impractical. This final exercise challenges you to scale up your analysis for a linear communication network of $N$ nodes [@problem_id:1333689]. Instead of solving for a small, fixed set of probabilities, you will need to identify and solve a recurrence relation to find a general, closed-form expression for the stationary distribution, a crucial skill for analyzing more complex and scalable systems.", "problem": "Consider a simplified model of a data packet traversing a linear communication network. The network consists of a source node, labeled 0, and a sequence of $N$ subsequent nodes, labeled $1, 2, \\dots, N$. The location of the packet in this network can be modeled as a Continuous-Time Markov Chain (CTMC) on the state space $S = \\{0, 1, \\dots, N\\}$.\n\nThe dynamics of the packet's movement are as follows:\n- From any node $i$ where $0 \\le i < N$, the packet attempts to move forward to the next node, $i+1$. This transition occurs at a constant rate $\\lambda > 0$.\n- From any node $i$ where $1 \\le i \\le N$, there is a risk of the packet being dropped. If dropped, the packet is instantaneously retransmitted from the source node, 0. This \"drop and return\" event occurs at a constant rate $\\mu > 0$.\n- The packet cannot be dropped from the source node 0. Once the packet reaches node $N$, it can only be dropped and returned to node 0.\n\nAssuming the process has been running for a long time and has reached a stationary state, determine the probability of finding the packet at the final destination, node $N$. Your answer should be a closed-form analytic expression in terms of $N$, $\\lambda$, and $\\mu$.", "solution": "Model the packet location as a CTMC on $S=\\{0,1,\\dots,N\\}$ with generator entries:\n- For $i=0$: $q_{0,1}=\\lambda$, $q_{0,0}=-\\lambda$.\n- For $1 \\leq i \\leq N-1$: $q_{i,i+1}=\\lambda$, $q_{i,0}=\\mu$, $q_{i,i}=-(\\lambda+\\mu)$.\n- For $i=N$: $q_{N,0}=\\mu$, $q_{N,N}=-\\mu$.\n\nLet $\\pi_i$ denote the stationary probability of state $i$. Stationarity requires $\\boldsymbol{\\pi} Q=\\mathbf{0}$ and $\\sum_{i=0}^{N}\\pi_i=1$. Writing the global balance at each state gives:\n- For $i=0$: inflow equals outflow,\n$$\n\\mu\\sum_{i=1}^{N}\\pi_i=\\lambda \\pi_0.\n$$\n- For $1 \\leq i \\leq N-1$: the only inflow to $i$ is from $i-1$ and outflows are to $0$ and $i+1$,\n$$\n\\lambda \\pi_{i-1}=(\\lambda+\\mu)\\pi_i.\n$$\n- For $i=N$: the only inflow is from $N-1$ and outflow is to $0$,\n$$\n\\lambda \\pi_{N-1}=\\mu \\pi_N.\n$$\n\nDefine $r=\\frac{\\lambda}{\\lambda+\\mu}$. The recursion for $1 \\leq i \\leq N-1$ yields\n$$\n\\pi_i=r^{i}\\pi_0.\n$$\nUsing the $i=N$ balance,\n$$\n\\pi_N=\\frac{\\lambda}{\\mu}\\pi_{N-1}=\\frac{\\lambda}{\\mu}r^{N-1}\\pi_0.\n$$\n\nNormalize using $\\sum_{i=0}^{N}\\pi_i=1$:\n$$\n1=\\pi_0\\left(\\sum_{i=0}^{N-1}r^{i}+\\frac{\\lambda}{\\mu}r^{N-1}\\right)\n=\\pi_0\\left(\\frac{1-r^{N}}{1-r}+\\frac{\\lambda}{\\mu}r^{N-1}\\right).\n$$\nSince $1-r=\\frac{\\mu}{\\lambda+\\mu}$ and $r=\\frac{\\lambda}{\\lambda+\\mu}$,\n$$\n\\frac{1-r^{N}}{1-r}=\\frac{\\lambda+\\mu}{\\mu}(1-r^{N}),\n$$\nso\n$$\n1=\\pi_0\\left(\\frac{\\lambda+\\mu}{\\mu}(1-r^{N})+\\frac{\\lambda}{\\mu}r^{N-1}\\right)\n=\\pi_0\\cdot\\frac{\\lambda+\\mu}{\\mu},\n$$\nbecause $-(\\lambda+\\mu)r^{N}+\\lambda r^{N-1}=r^{N-1}\\big(-(\\lambda+\\mu)r+\\lambda\\big)=0$. Hence\n$$\n\\pi_0=\\frac{\\mu}{\\lambda+\\mu}.\n$$\nTherefore\n$$\n\\pi_N=\\frac{\\lambda}{\\mu}r^{N-1}\\pi_0=\\frac{\\lambda}{\\lambda+\\mu}r^{N-1}=r^{N}=\\left(\\frac{\\lambda}{\\lambda+\\mu}\\right)^{N}.\n$$\nThis is the stationary probability of being at node $N$.", "answer": "$$\\boxed{\\left(\\frac{\\lambda}{\\lambda+\\mu}\\right)^{N}}$$", "id": "1333689"}]}