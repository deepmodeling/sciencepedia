{"hands_on_practices": [{"introduction": "To truly grasp the Markov property, it's best to start with a concrete calculation. This exercise models a 'self-correcting' particle whose movement probabilities change depending on its position, a classic example of a Markov chain. By tracing the possible paths the particle can take and calculating the overall probability of reaching a target, you'll get hands-on practice applying the core principle: the next step depends only on the current state, not the past history. [@problem_id:1342491]", "problem": "Consider a simplified one-dimensional model for a self-correcting alignment system. A particle moves along the integer line $\\mathbb{Z}$. Let $X_n$ denote the position of the particle at a discrete time step $n$. The system's objective is to keep the particle as close to the origin (position 0) as possible.\n\nAt each time step, the particle's position changes by exactly one unit, i.e., $X_{n+1} = X_n \\pm 1$. The direction of movement is determined by a probabilistic rule designed to favor returning to the origin. The probability of moving to the right ($X_{n+1} = X_n + 1$) depends on the current position $X_n = x$ and is given by the function $p(x)$. The probability of moving to the left ($X_{n+1} = X_n - 1$) is therefore $1-p(x)$.\n\nThe function $p(x)$ is defined as follows:\n$$\np(x) = \\begin{cases}\nq & \\text{if } x < 0 \\\\\n1/2 & \\text{if } x = 0 \\\\\n1-q & \\text{if } x > 0\n\\end{cases}\n$$\nwhere $q$ is a given constant satisfying $1/2 < q < 1$.\n\nSuppose the particle starts at the position $X_0 = 1$. Calculate the probability that the particle is at the origin at time step $n=3$, i.e., find $P(X_3 = 0)$. Express your answer as a function of $q$.", "solution": "We model the motion as a Markov chain on $\\mathbb{Z}$ with nearest-neighbor steps. From a current position $x$, the probability of moving to the right is $p(x)$ and to the left is $1-p(x)$, where $p(x)=1-q$ for $x>0$, $p(0)=\\frac{1}{2}$, and $p(x)=q$ for $x<0$, with $X_{0}=1$ and $\\frac{1}{2}<q<1$. The probability of any particular length-$3$ path is the product of its stepwise transition probabilities, conditioned on the current state at each step.\n\nTo have $X_{3}=0$ starting from $X_{0}=1$, the net displacement after $3$ steps must be $-1$. With steps of $\\pm 1$, this requires exactly two left moves and one right move. The possible sequences are RLL, LRL, and LLR, where R denotes a right move and L denotes a left move. We compute each path probability by following the states and using the appropriate $p(x)$ at each state:\n\n1) RLL: $1 \\to 2 \\to 1 \\to 0$.\n- From $1>0$, $\\Pr(\\text{R})=1-q$.\n- From $2>0$, $\\Pr(\\text{L})=q$.\n- From $1>0$, $\\Pr(\\text{L})=q$.\nThus the path probability is $(1-q)\\,q\\,q=(1-q)q^{2}$.\n\n2) LRL: $1 \\to 0 \\to 1 \\to 0$.\n- From $1>0$, $\\Pr(\\text{L})=q$.\n- From $0$, $\\Pr(\\text{R})=\\frac{1}{2}$.\n- From $1>0$, $\\Pr(\\text{L})=q$.\nThus the path probability is $q\\cdot \\frac{1}{2}\\cdot q=\\frac{q^{2}}{2}$.\n\n3) LLR: $1 \\to 0 \\to -1 \\to 0$.\n- From $1>0$, $\\Pr(\\text{L})=q$.\n- From $0$, $\\Pr(\\text{L})=\\frac{1}{2}$.\n- From $-1<0$, $\\Pr(\\text{R})=q$.\nThus the path probability is $q\\cdot \\frac{1}{2}\\cdot q=\\frac{q^{2}}{2}$.\n\nSumming over the disjoint paths yields\n$$\nP(X_{3}=0)=(1-q)q^{2}+\\frac{q^{2}}{2}+\\frac{q^{2}}{2}=(1-q)q^{2}+q^{2}=q^{2}(2-q).\n$$", "answer": "$$\\boxed{q^{2}(2-q)}$$", "id": "1342491"}, {"introduction": "What happens when a process seems to have 'memory' of more than just its immediate past? This problem explores a common scenario in modeling, where a system's future depends on its last two states. You will practice the essential technique of state augmentation, learning how to redefine the state as a vector to restore the Markov property, a crucial step in applying Markov chain analysis to a vast range of real-world phenomena. [@problem_id:1342473]", "problem": "A biologist is studying a species whose population size, denoted by $P_t$ for year $t$, exhibits complex dynamics. The population in the next year, $P_{t+1}$, is found to depend on the population of both the current year, $P_t$, and the preceding year, $P_{t-1}$. This 'memory' effect is due to how resource availability recovers over a two-year cycle. The model is described by the following linear recurrence relation:\n$$P_{t+1} = \\alpha P_t + \\beta P_{t-1} + W_t$$\nHere, $\\alpha$ and $\\beta$ are constant coefficients representing growth and delayed-regulation effects, and $\\{W_t\\}$ is a sequence of independent and identically distributed random variables representing unpredictable environmental fluctuations, with a mean of zero.\n\nThis process $\\{P_t\\}$ is not a Markov process because predicting the future distribution of the population requires knowledge of more than just the current state $P_t$. To analyze this system using the powerful framework of Markov processes, one must first define an appropriate state vector $S_t$ at time $t$ such that the resulting vector process $\\{S_t\\}$ possesses the Markov property.\n\nWhich of the following vectors, where vectors are written in column form, constitutes a valid state vector $S_t$ for this purpose?\n\nA. $S_t = P_t$\n\nB. $S_t = \\begin{pmatrix} P_t \\\\ W_t \\end{pmatrix}$\n\nC. $S_t = \\begin{pmatrix} P_t \\\\ P_{t-1} \\end{pmatrix}$\n\nD. $S_t = \\begin{pmatrix} P_{t+1} \\\\ P_t \\end{pmatrix}$\n\nE. $S_t = P_t + P_{t-1}$", "solution": "We are given the linear recurrence\n$$P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t},$$\nwhere $\\{W_{t}\\}$ is an independent and identically distributed sequence with mean zero and independent of the past. A vector process $\\{S_{t}\\}$ is Markov if, for every $t$, the conditional distribution of $S_{t+1}$ given the entire past depends only on $S_{t}$; that is,\n$$\\Pr(S_{t+1}\\in A\\mid S_{t},S_{t-1},\\dots)=\\Pr(S_{t+1}\\in A\\mid S_{t})\\quad\\text{for all measurable }A.$$\n\nConsider $S_{t}=\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}$. Then\n$$S_{t+1}=\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}=\\begin{pmatrix}\\alpha & \\beta\\\\ 1 & 0\\end{pmatrix}\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}+\\begin{pmatrix}1\\\\0\\end{pmatrix}W_{t}.$$\nLet $A=\\begin{pmatrix}\\alpha & \\beta\\\\ 1 & 0\\end{pmatrix}$ and $B=\\begin{pmatrix}1\\\\0\\end{pmatrix}$. Then $S_{t+1}=A S_{t}+B W_{t}$. Because $W_{t}$ is independent of $(S_{t},S_{t-1},\\dots)$ and has a fixed distribution, the conditional distribution of $S_{t+1}$ given the past depends only on $S_{t}$ through the deterministic term $A S_{t}$ and the independent innovation $B W_{t}$. Hence $\\{S_{t}\\}$ is a first-order Markov process. Therefore option C is valid.\n\nWe now justify why the other options are invalid as state vectors at time $t$.\n\nA. $S_{t}=P_{t}$ is insufficient because\n$$P_{t+1}\\mid(P_{t},P_{t-1})=\\alpha P_{t}+\\beta P_{t-1}+W_{t}$$\nhas a conditional mean that depends on $P_{t-1}$, which is not determined by $P_{t}$. Thus the conditional distribution of $P_{t+1}$ given $P_{t}$ alone does not match the one given the full past, violating the Markov property.\n\nB. $S_{t}=\\begin{pmatrix}P_{t}\\\\ W_{t}\\end{pmatrix}$ is also insufficient. Although $P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t}$ includes $W_{t}$, the term $\\beta P_{t-1}$ is still unknown given $(P_{t},W_{t})$. Therefore the conditional distribution of $P_{t+1}$ given $(P_{t},W_{t})$ depends on $P_{t-1}$, so the process is not Markov in this state.\n\nD. $S_{t}=\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}$ uses a future value $P_{t+1}$ as part of the state at time $t$, which is not determined by the information available at time $t$. While the sequence $\\left(\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}\\right)_{t}$ would satisfy a Markov recursion in $t$ if one allowed future information, it is not a valid state vector at time $t$ constructed from present and past information, which is required for Markov modeling of the systemâ€™s evolution.\n\nE. $S_{t}=P_{t}+P_{t-1}$ aggregates the two needed components into a single scalar, losing information. Since\n$$P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t},$$\nknowing only $P_{t}+P_{t-1}$ does not determine the conditional distribution of $P_{t+1}$ because different pairs $(P_{t},P_{t-1})$ can yield the same sum but different values of $\\alpha P_{t}+\\beta P_{t-1}$.\n\nTherefore, the only valid choice that yields a Markov state vector at time $t$ is $S_{t}=\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}$.", "answer": "$$\\boxed{C}$$", "id": "1342473"}, {"introduction": "Once we have a Markov process, we often want to study a function of that process. But does a transformed process automatically inherit the 'memoryless' property? This exercise challenges you to investigate this crucial question by examining several different transformations applied to a base Markov chain. By determining which new processes remain Markovian, you will gain a deeper insight into the structural requirements of the Markov property, including the important concepts of state relabeling and lumpability. [@problem_id:1342456]", "problem": "Let $\\{X_n\\}_{n \\ge 0}$ be a time-homogeneous Markov chain with a discrete state space $S_X$ and transition probabilities $P_{ij} = P(X_{n+1}=j | X_n=i)$. Based on this chain, four new stochastic processes are constructed. Your task is to determine which of these new processes are also necessarily Markov processes under the conditions given for each case.\n\nI. A process $\\{Y_n\\}$ is defined as $Y_n = X_n^2$. For this specific case, the base process $\\{X_n\\}$ is defined on the state space $S_X = \\{-1, 1, 2\\}$. Its transition probabilities are such that moving from state 1 to state 2 is possible ($P_{1,2} > 0$), but moving from state -1 to state 2 is impossible ($P_{-1,2} = 0$).\n\nII. A process $\\{Z_n\\}$ is defined for $n \\ge 1$ by taking pairs of consecutive states from the original chain, $Z_n = (X_n, X_{n-1})$. The state space for $\\{Z_n\\}$ is $S_X \\times S_X$.\n\nIII. A process $\\{U_n\\}$ is defined via a bijective (one-to-one and onto) function $f: S_X \\to S_U$, where $U_n = f(X_n)$.\n\nIV. A process $\\{V_n\\}$ is defined by reversing time. It is assumed that the original chain $\\{X_n\\}$ is stationary, possessing a stationary distribution $\\pi = (\\pi_i)_{i \\in S_X}$ with all $\\pi_i > 0$. The time-reversed process is defined as $V_n = X_{-n}$ for $n \\in \\mathbb{Z}$.\n\nWhich of the processes described above are guaranteed to be Markov processes under the specified conditions?\n\nA. I and II only\n\nB. II and III only\n\nC. I, II, and III only\n\nD. II, III, and IV only\n\nE. I, III, and IV only", "solution": "We analyze each construction and test the Markov property, using explicit conditional probabilities and the lumpability criterion when appropriate.\n\nI. Define $Y_{n}=X_{n}^{2}$ with $S_{X}=\\{-1,1,2\\}$ and transitions satisfying $P_{1,2}>0$ and $P_{-1,2}=0$. The image state space is $S_{Y}=\\{1,4\\}$ with the preimage classes $f^{-1}(1)=\\{-1,1\\}$ and $f^{-1}(4)=\\{2\\}$. A function of a Markov chain is itself Markov if and only if the partition induced by the function is strongly lumpable, i.e., for any $y\\in S_{Y}$, any $x,x'\\in f^{-1}(y)$, and any $y'\\in S_{Y}$,\n$$\n\\sum_{j\\in f^{-1}(y')}P_{xj}=\\sum_{j\\in f^{-1}(y')}P_{x'j}.\n$$\nHere, take $y=1$, so $x=1$, $x'=-1$, and take $y'=4$, so $f^{-1}(4)=\\{2\\}$. Then\n$$\n\\sum_{j\\in f^{-1}(4)}P_{1j}=P_{1,2}>0,\\qquad \\sum_{j\\in f^{-1}(4)}P_{-1,j}=P_{-1,2}=0,\n$$\nwhich are not equal. Therefore the lumpability condition fails, and $\\{Y_{n}\\}$ is not necessarily Markov under the given assumptions.\n\nII. Define $Z_{n}=(X_{n},X_{n-1})$ on $S_{X}\\times S_{X}$. For any $(i,j)\\in S_{X}\\times S_{X}$,\n$$\n\\mathbb{P}\\big(Z_{n+1}=(k,i)\\mid Z_{n}=(i,j), Z_{n-1}, Z_{n-2},\\dots\\big)=\\mathbb{P}(X_{n+1}=k\\mid X_{n}=i)=P_{ik},\n$$\nwhich depends only on the current state $(i,j)$ through $i$, and not on earlier history. Hence $\\{Z_{n}\\}$ is a first-order Markov chain on $S_{X}\\times S_{X}$.\n\nIII. Let $f:S_{X}\\to S_{U}$ be bijective and define $U_{n}=f(X_{n})$. Since $f$ is invertible, for $u,u'\\in S_{U}$,\n$$\n\\mathbb{P}(U_{n+1}=u'\\mid U_{n}=u)=\\mathbb{P}\\big(X_{n+1}=f^{-1}(u')\\mid X_{n}=f^{-1}(u)\\big)=P_{f^{-1}(u),\\,f^{-1}(u')},\n$$\nwhich depends only on the current state $u$. Therefore $\\{U_{n}\\}$ is Markov.\n\nIV. Assume $\\{X_{n}\\}_{n\\in\\mathbb{Z}}$ is stationary with stationary distribution $\\pi=(\\pi_{i})_{i\\in S_{X}}$ satisfying $\\pi_{i}>0$ for all $i$, and define $V_{n}=X_{-n}$. For $i,j\\in S_{X}$, the transition probabilities of the time-reversed chain are\n$$\nP^{*}_{ij}=\\mathbb{P}(V_{n+1}=j\\mid V_{n}=i)=\\frac{\\pi_{j}P_{ji}}{\\pi_{i}},\n$$\nwhich are well-defined because $\\pi_{i}>0$. This shows that $\\{V_{n}\\}$ is Markov (the standard time-reversal of a stationary Markov chain).\n\nCombining the conclusions: I is not necessarily Markov, while II, III, and IV are Markov under the stated conditions. The correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1342456"}]}