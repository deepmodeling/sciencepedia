{"hands_on_practices": [{"introduction": "Many systems, from weather patterns to financial markets, can be modeled as evolving through a series of states where the future depends only on the present. A key question is whether such a system settles into a stable, long-term behavior. This first practice explores this fundamental concept by calculating the stationary distribution for a simple two-state system, providing a direct method to determine the long-run probabilities of occupying each state [@problem_id:1314745].", "problem": "Consider a simplified climate model for a region that can be characterized by one of two states each year: 'Dry' or 'Wet'. We label the 'Dry' state as state 1 and the 'Wet' state as state 2. The climate's evolution is modeled as a discrete-time Markov chain, where the state of any given year depends only on the state of the immediately preceding year.\n\nThe transition probabilities are as follows:\n- If a year is Dry (state 1), the probability that the next year will also be Dry is $2/3$.\n- If a year is Wet (state 2), the probability that the next year will be Dry is $1/4$.\n\nThe one-step transition probability matrix $P$ is a $2 \\times 2$ matrix where the entry $P_{ij}$ is the probability of moving from state $i$ to state $j$ in one year.\n\nLet $P^n$ be the $n$-step transition matrix. Determine the limiting matrix $L = \\lim_{n \\to \\infty} P^n$. In your final answer, present the four entries of this limiting matrix, $L_{11}, L_{12}, L_{21}, L_{22}$, as a single row matrix of the form $\\begin{pmatrix} L_{11} & L_{12} & L_{21} & L_{22} \\end{pmatrix}$. The answer should be in exact fractional form.", "solution": "The Markov chain has two states with transition probabilities:\n- From state 1 (Dry) to state 1 with probability $\\frac{2}{3}$, hence to state 2 with probability $\\frac{1}{3}$.\n- From state 2 (Wet) to state 1 with probability $\\frac{1}{4}$, hence to state 2 with probability $\\frac{3}{4}$.\n\nTherefore, the one-step transition matrix is\n$$\nP=\\begin{pmatrix}\n\\frac{2}{3} & \\frac{1}{3} \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$\nEach row sums to $1$, so $P$ is stochastic. Because $P_{11}>0$ and $P_{22}>0$, the chain is aperiodic, and since both off-diagonal entries are positive, the chain is irreducible. For a finite, irreducible, aperiodic Markov chain, there is a unique stationary distribution $\\boldsymbol{\\pi}$ satisfying $\\boldsymbol{\\pi}=\\boldsymbol{\\pi}P$ and $\\pi_{1}+\\pi_{2}=1$, and\n$$\n\\lim_{n\\to\\infty}P^{n}=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\boldsymbol{\\pi}.\n$$\nThus each row of the limiting matrix equals $\\boldsymbol{\\pi}$.\n\nCompute $\\boldsymbol{\\pi}=(\\pi_{1},\\pi_{2})$ from $\\boldsymbol{\\pi}=\\boldsymbol{\\pi}P$ and $\\pi_{1}+\\pi_{2}=1$. From the first component,\n$$\n\\pi_{1}=\\frac{2}{3}\\pi_{1}+\\frac{1}{4}\\pi_{2}\n\\;\\Rightarrow\\;\n\\pi_{1}-\\frac{2}{3}\\pi_{1}=\\frac{1}{4}\\pi_{2}\n\\;\\Rightarrow\\;\n\\frac{1}{3}\\pi_{1}=\\frac{1}{4}\\pi_{2}\n\\;\\Rightarrow\\;\n4\\pi_{1}=3\\pi_{2}\n\\;\\Rightarrow\\;\n\\pi_{1}=\\frac{3}{4}\\pi_{2}.\n$$\nUsing $\\pi_{1}+\\pi_{2}=1$ gives\n$$\n\\frac{3}{4}\\pi_{2}+\\pi_{2}=\\frac{7}{4}\\pi_{2}=1\n\\;\\Rightarrow\\;\n\\pi_{2}=\\frac{4}{7},\\quad \\pi_{1}=\\frac{3}{7}.\n$$\nHence the limiting matrix $L=\\lim_{n\\to\\infty}P^{n}$ has identical rows equal to $(\\frac{3}{7},\\frac{4}{7})$, so\n$$\nL=\\begin{pmatrix}\n\\frac{3}{7} & \\frac{4}{7} \\\\\n\\frac{3}{7} & \\frac{4}{7}\n\\end{pmatrix}.\n$$\nTherefore, the entries are $L_{11}=\\frac{3}{7}$, $L_{12}=\\frac{4}{7}$, $L_{21}=\\frac{3}{7}$, $L_{22}=\\frac{4}{7}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{7} & \\frac{4}{7} & \\frac{3}{7} & \\frac{4}{7} \\end{pmatrix}}$$", "id": "1314745"}, {"introduction": "While some Markov chains settle into a predictable steady state, others do not. The long-term behavior of a chain with an infinite number of states, like a random walk on the integers, can be fundamentally different. This exercise introduces the critical distinction between recurrent and transient states, which respectively determine if a process is guaranteed to return to its starting point or tends to drift away forever [@problem_id:1314739]. Understanding this distinction is crucial for correctly characterizing the limiting behavior of unbounded systems.", "problem": "Consider a simplified model for the daily fluctuation of a highly volatile asset's price, represented as a discrete-time random walk on the set of all integers, $\\mathbb{Z}$. Let $X_n$ be the asset's price level at the end of day $n$, starting from some initial price level $X_0 = i$. Each day, the price level increases by one unit (i.e., $X_{n+1} = X_n + 1$) with probability $p$, and it decreases by one unit (i.e., $X_{n+1} = X_n - 1$) with probability $1-p$. The market is assumed to have a positive drift, which means $p > 1/2$.\n\nA state in a Markov chain is defined as **recurrent** if, starting from that state, the probability of ever returning to it is exactly 1. If this probability is less than 1, the state is defined as **transient**.\n\nBased on this model, which of the following statements about the state $i$ is correct?\n\nA. The state $i$ is recurrent.\n\nB. The state $i$ is transient.\n\nC. The state $i$ is recurrent if $1/2 < p < 3/4$ and transient if $p \\ge 3/4$.\n\nD. The classification of the state $i$ as recurrent or transient depends on the specific numerical value of $i$.", "solution": "Let $X_{n}$ be the nearest-neighbor random walk on $\\mathbb{Z}$ with transition probabilities $\\mathbb{P}(X_{n+1}=x+1 \\mid X_{n}=x)=p$ and $\\mathbb{P}(X_{n+1}=x-1 \\mid X_{n}=x)=1-p$, with $p> \\frac{1}{2}$. A state $i$ is recurrent if the return probability\n$$\nR:=\\mathbb{P}_{i}\\big(T_{i}^{+}<\\infty\\big)=1,\\quad T_{i}^{+}:=\\inf\\{n\\geq 1: X_{n}=i\\},\n$$\nand transient if $R<1$.\n\nCondition on the first step from $i$. Let\n$$\nh_{+}:=\\mathbb{P}_{i+1}(\\text{hit } i),\\qquad h_{-}:=\\mathbb{P}_{i-1}(\\text{hit } i).\n$$\nBy the Markov property,\n$$\nR=p\\,h_{+}+(1-p)\\,h_{-}.\n$$\n\nCompute $h_{+}$. For $k\\geq 1$, define $h(k):=\\mathbb{P}_{i+k}(\\text{hit } i)$. Then $h(0)=1$, and for $k\\geq 1$ the function is harmonic:\n$$\nh(k)=p\\,h(k+1)+(1-p)\\,h(k-1).\n$$\nSeek solutions of the form $h(k)=r^{k}$. The characteristic equation is\n$$\np r^{2}-r+(1-p)=0,\n$$\nwith roots\n$$\nr=\\frac{1\\pm\\sqrt{1-4p(1-p)}}{2p}=\\frac{1\\pm(2p-1)}{2p},\n$$\nsince $p>\\frac{1}{2}$ implies $|2p-1|=2p-1$. Thus $r_{1}=\\frac{1-p}{p}$ and $r_{2}=1$. The bounded solution as $k\\to\\infty$ requires the coefficient of $r_{2}^{k}$ to be zero, so\n$$\nh(k)=\\left(\\frac{1-p}{p}\\right)^{k},\\quad \\text{hence } h_{+}=h(1)=\\frac{1-p}{p}.\n$$\n\nCompute $h_{-}$. Since $p>\\frac{1}{2}$, the increments have positive mean $2p-1>0$, and by the strong law of large numbers $X_{n}\\to +\\infty$ almost surely. Therefore, starting from $i-1$, the walk crosses level $i$ almost surely, so\n$$\nh_{-}=1.\n$$\n\nCombine these to get\n$$\nR=p\\,\\frac{1-p}{p}+(1-p)\\cdot 1=(1-p)+(1-p)=2(1-p).\n$$\nBecause $p>\\frac{1}{2}$, we have $2(1-p)<1$, hence $R<1$ and the state $i$ is transient.\n\nTherefore, the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1314739"}, {"introduction": "The theoretical concept of a limiting distribution has a powerful computational counterpart. This practice guides you through implementing the power iteration method, a numerical technique to approximate the stationary distribution of a Markov chain. By applying this algorithm to chains with different properties—ergodic, absorbing, and periodic—you will gain a concrete, hands-on understanding of the conditions under which a system converges to a steady state and what happens when it does not [@problem_id:2393833].", "problem": "Given a finite-state time-homogeneous Markov chain with a row-stochastic transition matrix $P \\in \\mathbb{R}^{n \\times n}$, a stationary distribution is any row vector $\\pi \\in \\mathbb{R}^{1 \\times n}$ with nonnegative components summing to $1$ that satisfies the fixed-point equation $\\pi = \\pi P$. Define the mapping $T(\\pi) = \\pi P$. For each test case below, let $\\pi_0$ be the specified initial row probability vector. Construct a sequence $(\\pi_n)_{n \\ge 0}$ by $\\pi_{n+1} = T(\\pi_n)$, and check convergence using the $\\ell_1$-norm: stop at the first index $n$ such that $\\|\\pi_{n+1} - \\pi_n\\|_1 < \\varepsilon$ or after a maximum of $N_{\\max}$ iterations, whichever occurs first. If convergence occurs, report the approximate stationary distribution $\\pi_{n+1}$ with each component rounded to $8$ decimal places. If the stopping criterion is not met within $N_{\\max}$ iterations, report the integer $-1$ for that test case. All arithmetic is real-valued. Angles are not involved. There are no physical units. The norm to be used is the $\\ell_1$-norm.\n\nUse the following test suite, where each test case specifies the transition matrix $P$, the initial distribution $\\pi_0$, the tolerance $\\varepsilon$, and the maximum iteration count $N_{\\max}$.\n\n- Test case $1$ (strictly positive, ergodic):\n  - $$P = \\begin{bmatrix}\n  0.6 & 0.3 & 0.1 \\\\\n  0.2 & 0.5 & 0.3 \\\\\n  0.25 & 0.25 & 0.5\n  \\end{bmatrix}$$\n  - $$\\pi_0 = \\left[\\dfrac{1}{3}, \\dfrac{1}{3}, \\dfrac{1}{3}\\right]$$\n  - $\\varepsilon = 10^{-10}$\n  - $N_{\\max} = 10^6$\n\n- Test case $2$ (absorbing state):\n  - $$P = \\begin{bmatrix}\n  0.5 & 0.5 & 0.0 \\\\\n  0.0 & 1.0 & 0.0 \\\\\n  0.1 & 0.0 & 0.9\n  \\end{bmatrix}$$\n  - $$\\pi_0 = \\left[\\dfrac{1}{3}, \\dfrac{1}{3}, \\dfrac{1}{3}\\right]$$\n  - $\\varepsilon = 10^{-10}$\n  - $N_{\\max} = 10^6$\n\n- Test case $3$ (multiple closed classes; limit depends on $\\pi_0$):\n  - $$P = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 & 0.0 \\\\\n  1.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0 & 1.0 \\\\\n  0.0 & 0.0 & 0.0 & 1.0\n  \\end{bmatrix}$$\n  - $$\\pi_0 = \\left[\\dfrac{1}{4}, \\dfrac{1}{4}, \\dfrac{1}{4}, \\dfrac{1}{4}\\right]$$\n  - $\\varepsilon = 10^{-12}$\n  - $N_{\\max} = 10^6$\n\n- Test case $4$ (period-$2$ chain that does not converge from the given $\\pi_0$):\n  - $$P = \\begin{bmatrix}\n  0.0 & 1.0 \\\\\n  1.0 & 0.0\n  \\end{bmatrix}$$\n  - $$\\pi_0 = \\left[1.0, 0.0\\right]$$\n  - $\\varepsilon = 10^{-12}$\n  - $N_{\\max} = 10^5$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. For a converged case, output the list of rounded components; for a non-converged case, output the integer $-1$. For example, an output with four results must look like $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$ with no spaces anywhere in the line. Each list of components must be written with each entry rounded to $8$ decimal places.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: Finite-state, time-homogeneous Markov chain.\n- **Transition Matrix**: A row-stochastic matrix $P \\in \\mathbb{R}^{n \\times n}$.\n- **Stationary Distribution**: A row vector $\\pi \\in \\mathbb{R}^{1 \\times n}$ with non-negative components summing to $1$ that is a fixed point of the equation $\\pi = \\pi P$.\n- **Iterative Mapping**: $T(\\pi) = \\pi P$.\n- **Iterative Sequence**: The sequence is defined by $\\pi_{n+1} = T(\\pi_n)$, starting from a given initial probability vector $\\pi_0$.\n- **Stopping Criterion**: The iteration stops at the first index $n$ for which the $\\ell_1$-norm of the difference between consecutive iterates is less than a tolerance $\\varepsilon$, i.e., $\\|\\pi_{n+1} - \\pi_n\\|_1 < \\varepsilon$.\n- **Maximum Iterations**: A hard limit $N_{\\max}$ on the number of iterations.\n- **Output on Convergence**: The vector $\\pi_{n+1}$, with each component rounded to $8$ decimal places.\n- **Output on Non-convergence**: The integer $-1$.\n- **Test Case 1**: $$P = \\begin{bmatrix} 0.6 & 0.3 & 0.1 \\\\ 0.2 & 0.5 & 0.3 \\\\ 0.25 & 0.25 & 0.5 \\end{bmatrix}$$, $$\\pi_0 = \\left[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right]$$, $\\varepsilon = 10^{-10}$, $N_{\\max} = 10^6$.\n- **Test Case 2**: $$P = \\begin{bmatrix} 0.5 & 0.5 & 0.0 \\\\ 0.0 & 1.0 & 0.0 \\\\ 0.1 & 0.0 & 0.9 \\end{bmatrix}$$, $$\\pi_0 = \\left[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right]$$, $\\varepsilon = 10^{-10}$, $N_{\\max} = 10^6$.\n- **Test Case 3**: $$P = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 0.0 & 0.0 & 1.0 \\end{bmatrix}$$, $$\\pi_0 = \\left[\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}\\right]$$, $\\varepsilon = 10^{-12}$, $N_{\\max} = 10^6$.\n- **Test Case 4**: $$P = \\begin{bmatrix} 0.0 & 1.0 \\\\ 1.0 & 0.0 \\end{bmatrix}$$, $$\\pi_0 = \\left[1.0, 0.0\\right]$$, $\\varepsilon = 10^{-12}$, $N_{\\max} = 10^5$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the required criteria.\n\n- **Scientifically Grounded**: The problem addresses the computation of a stationary distribution for a Markov chain. This is a fundamental concept in the theory of stochastic processes. The proposed iterative method, $\\pi_{k+1} = \\pi_k P$, is the standard power iteration method for finding the left eigenvector associated with the eigenvalue $\\lambda=1$. The existence and uniqueness of such distributions under various conditions (ergodicity, reducibility) are well-established mathematical facts. The problem is scientifically and mathematically sound.\n\n- **Well-Posed**: The problem is completely specified. For each test case, all necessary inputs—the transition matrix $P$, the initial vector $\\pi_0$, the convergence tolerance $\\varepsilon$, and the maximum iteration count $N_{\\max}$—are provided. The algorithm is deterministically defined, and the conditions for reporting either a converged result or a failure are unambiguous.\n\n- **Objective**: The problem is stated using precise, formal mathematical language. All quantities are defined, and there are no subjective or opinion-based assertions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is scientifically grounded, well-posed, and objective. A solution will be constructed.\n\n### Solution\nThe problem requires the implementation of a fixed-point iteration to find the stationary distribution $\\pi$ of a finite-state Markov chain. A stationary distribution satisfies the equation $\\pi = \\pi P$, where $P$ is the row-stochastic transition matrix. This equation identifies $\\pi$ as a left eigenvector of $P$ corresponding to the eigenvalue $\\lambda=1$. The Perron-Frobenius theorem for stochastic matrices guarantees that such an eigenvector exists and that $\\lambda=1$ is the largest eigenvalue in magnitude.\n\nThe iterative procedure is given by $\\pi_{k+1} = \\pi_k P$ for $k=0, 1, 2, \\ldots$, starting with an initial probability distribution $\\pi_0$. This is known as the power method. The iteration is terminated under one of two conditions:\n$1$. Convergence: The $\\ell_1$-norm of the difference between successive iterates falls below a given tolerance $\\varepsilon$. The $\\ell_1$-norm of a vector $\\mathbf{x} = [x_1, \\ldots, x_n]$ is $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n |x_i|$. The condition is $\\|\\pi_{k+1} - \\pi_k\\|_1 < \\varepsilon$.\n$2$. Non-convergence: The number of iterations exceeds a specified maximum, $N_{\\max}$.\n\nThe behavior of the sequence $(\\pi_k)_{k \\ge 0}$ depends on the properties of the matrix $P$.\n- **Case 1**: The matrix $P$ is strictly positive, which implies the corresponding Markov chain is ergodic (irreducible and aperiodic). For such a chain, the power method is guaranteed to converge to the unique stationary distribution, regardless of the initial probability vector $\\pi_0$.\n- **Case 2**: The matrix $P$ is reducible. State 2 (with index 1) is an absorbing state since $P_{22}=1.0$. From any initial state, there is a non-zero probability of eventually reaching the absorbing state. The iteration will converge to a distribution with all probability mass concentrated on the absorbing states. In this specific case, the limit will be $[0, 1, 0]$.\n- **Case 3**: The matrix $P$ is reducible, with two closed recurrent classes corresponding to states $\\{1\\}$ and $\\{4\\}$. The limiting distribution depends on the initial distribution $\\pi_0$. The total probability mass initially in states that can reach state 1 will eventually collect in state 1. Similarly for state 4. Here, state 2 (index 1) leads to state 1, and state 3 (index 2) leads to state 4. Thus, the final mass in state 1 will be $\\pi_{0,0} + \\pi_{0,1}$, and the final mass in state 4 will be $\\pi_{0,2} + \\pi_{0,3}$. For $\\pi_0 = [0.25, 0.25, 0.25, 0.25]$, the limit is expected to be $[0.5, 0, 0, 0.5]$. The iteration will converge rapidly.\n- **Case 4**: The matrix $P$ corresponds to a periodic chain with period $2$. For the initial distribution $\\pi_0 = [1.0, 0.0]$, the sequence of distributions will be $\\pi_0 = [1.0, 0.0]$, $\\pi_1 = [0.0, 1.0]$, $\\pi_2 = [1.0, 0.0]$, and so on. The sequence oscillates and does not converge to a single limit. The difference $\\|\\pi_{k+1} - \\pi_k\\|_1$ will always be $2$, so the convergence criterion will never be met. The procedure will terminate after $N_{\\max}$ iterations, and the result must be $-1$.\n\nThe algorithm to be implemented will iterate for each test case, performing the vector-matrix multiplication $\\pi_{k+1} = \\pi_k P$ and checking the stopping criteria at each step. If convergence is achieved, the resulting vector $\\pi_{k+1}$ is rounded to $8$ decimal places. Otherwise, $-1$ is reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the stationary distribution of Markov chains using fixed-point iteration.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (strictly positive, ergodic)\n        {\n            \"P\": np.array([\n                [0.6, 0.3, 0.1],\n                [0.2, 0.5, 0.3],\n                [0.25, 0.25, 0.5]\n            ]),\n            \"pi0\": np.array([1/3, 1/3, 1/3]),\n            \"epsilon\": 1e-10,\n            \"N_max\": 10**6\n        },\n        # Test case 2 (absorbing state)\n        {\n            \"P\": np.array([\n                [0.5, 0.5, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.1, 0.0, 0.9]\n            ]),\n            \"pi0\": np.array([1/3, 1/3, 1/3]),\n            \"epsilon\": 1e-10,\n            \"N_max\": 10**6\n        },\n        # Test case 3 (multiple closed classes)\n        {\n            \"P\": np.array([\n                [1.0, 0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0, 1.0]\n            ]),\n            \"pi0\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"epsilon\": 1e-12,\n            \"N_max\": 10**6\n        },\n        # Test case 4 (period-2 chain)\n        {\n            \"P\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"pi0\": np.array([1.0, 0.0]),\n            \"epsilon\": 1e-12,\n            \"N_max\": 10**5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        P = case[\"P\"]\n        pi_current = case[\"pi0\"]\n        epsilon = case[\"epsilon\"]\n        N_max = case[\"N_max\"]\n        \n        converged = False\n        pi_next = None\n\n        for _ in range(N_max):\n            pi_next = np.dot(pi_current, P)\n            \n            # Check for convergence using the l1-norm\n            if np.linalg.norm(pi_next - pi_current, ord=1)  epsilon:\n                converged = True\n                break\n            \n            pi_current = pi_next\n\n        if converged:\n            # Round to 8 decimal places as per problem statement\n            final_pi = np.round(pi_next, 8)\n            results.append(final_pi)\n        else:\n            results.append(-1)\n\n    # Format the final output string to have no spaces\n    def format_result(res):\n        if isinstance(res, int) and res == -1:\n            return str(res)\n        # Convert list of floats to a string representation '[f1,f2,...]'\n        # without spaces.\n        return '[' + ','.join(map(str, res)) + ']'\n\n    formatted_results = [format_result(res) for res in results]\n    final_output_string = '[' + ','.join(formatted_results) + ']'\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```", "id": "2393833"}]}