{"hands_on_practices": [{"introduction": "The concept of expected return time, or mean recurrence time, is fundamental to understanding the long-term behavior of stochastic systems. This first exercise provides a direct application of first-step analysis, a powerful recursive technique for calculating such expected values. By analyzing a simple random walk with reflecting boundaries, you will build a foundational skill for solving a wide range of problems in Markov chains [@problem_id:1301617].", "problem": "A particle performs a discrete-time random walk on the set of integer positions $S = \\{1, 2, 3, 4\\}$. The movement of the particle is governed by the following rules:\n- If the particle is at an interior position $i \\in \\{2, 3\\}$, it moves to position $i-1$ with a probability of $0.5$ or to position $i+1$ with a probability of $0.5$ at the next time step.\n- The positions 1 and 4 act as reflecting boundaries. If the particle is at position 1, it must move to position 2 at the next time step. If the particle is at position 4, it must move to position 3 at the next time step.\n\nAssuming the particle starts at position 2, calculate the expected number of time steps until the particle first returns to position 2.", "solution": "We model the process as a Markov chain on states $\\{1,2,3,4\\}$ with transitions: $P_{1,2}=1$, $P_{2,1}=P_{2,3}=\\frac{1}{2}$, $P_{3,2}=P_{3,4}=\\frac{1}{2}$, and $P_{4,3}=1$. Let $f_{i}$ denote the expected number of steps to hit state $2$ starting from $i$, with $f_{2}=0$. By first-step analysis (law of total expectation for Markov chains), for $i \\neq 2$ we have\n$$\nf_{i}=1+\\sum_{j}P_{i,j}f_{j}.\n$$\nThus,\n$$\nf_{1}=1+P_{1,2}f_{2}=1,\n$$\n$$\nf_{3}=1+P_{3,2}f_{2}+P_{3,4}f_{4}=1+\\frac{1}{2}f_{4},\n$$\n$$\nf_{4}=1+P_{4,3}f_{3}=1+f_{3}.\n$$\nSolving, substitute $f_{4}=1+f_{3}$ into the $f_{3}$ equation:\n$$\nf_{3}=1+\\frac{1}{2}(1+f_{3})=1+\\frac{1}{2}+\\frac{1}{2}f_{3},\n$$\n$$\nf_{3}-\\frac{1}{2}f_{3}=\\frac{3}{2} \\quad \\Rightarrow \\quad \\frac{1}{2}f_{3}=\\frac{3}{2} \\quad \\Rightarrow \\quad f_{3}=3,\n$$\nand hence $f_{4}=1+f_{3}=4$, while $f_{1}=1$ as above.\n\nLet $r$ be the expected number of steps for the first return to $2$ starting from $2$. By first-step analysis,\n$$\nr=1+\\sum_{j}P_{2,j}f_{j}=1+\\frac{1}{2}f_{1}+\\frac{1}{2}f_{3}=1+\\frac{1}{2}\\cdot 1+\\frac{1}{2}\\cdot 3=1+\\frac{1}{2}+\\frac{3}{2}=3.\n$$\nTherefore, the expected number of time steps until the particle first returns to position $2$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "1301617"}, {"introduction": "Building on the first-step analysis method, we now move from a simple numerical example to a more general and practical scenario. This problem models a faulty traffic light, where the probabilities of state transitions are represented by symbolic variables $p$ and $q$ [@problem_id:1301641]. Tackling this challenge will enhance your ability to derive closed-form solutions and analyze how system behavior changes with varying parameters.", "problem": "A simplified model of a faulty traffic light considers its state at discrete time steps. The light is supposed to cycle through Green (G), then Yellow (Y), then Red (R), and back to Green. However, at each time step, there is a probability that it fails to transition and remains stuck on its current color.\n\nSpecifically, the behavior is as follows:\n- If the current state is Green, it transitions to Yellow with probability $1-p$ or remains Green with probability $p$.\n- If the current state is Yellow, it transitions to Red with probability $1-p$ or remains Yellow with probability $p$.\n- If the current state is Red, it transitions to Green with probability $1-q$ or remains Red with probability $q$.\n\nThe probabilities $p$ and $q$ are constants satisfying $0 \\le p < 1$ and $0 \\le q < 1$.\n\nSuppose the light is currently Green. Calculate the expected number of time steps until the light is Green again for the first time. Express your answer as a closed-form analytic expression in terms of $p$ and $q$.", "solution": "Let $E_{G}$ denote the expected number of steps to return to Green starting from Green (first return time with $n \\geq 1$). Let $E_{Y}$ be the expected number of steps to hit Green starting from Yellow, and $E_{R}$ the same starting from Red.\n\nFirst-step recursions:\n\n- From Red: with probability $q$ it stays Red, and with probability $1-q$ it goes to Green (and stops):\n$$\nE_{R} = q \\left(1 + E_{R}\\right) + (1 - q)\\cdot 1.\n$$\nSolving,\n$$\nE_{R} = q + q E_{R} + 1 - q \\;\\Rightarrow\\; E_{R} = q E_{R} + 1 \\;\\Rightarrow\\; E_{R} (1 - q) = 1 \\;\\Rightarrow\\; E_{R} = \\frac{1}{1 - q}.\n$$\n\n- From Yellow: with probability $p$ it stays Yellow, and with probability $1-p$ it goes to Red:\n$$\nE_{Y} = p \\left(1 + E_{Y}\\right) + (1 - p)\\left(1 + E_{R}\\right).\n$$\nSolving,\n$$\nE_{Y} = p + p E_{Y} + (1 - p) + (1 - p) E_{R}\n= 1 + p E_{Y} + (1 - p) E_{R},\n$$\nso\n$$\nE_{Y} (1 - p) = 1 + (1 - p) E_{R} \\;\\Rightarrow\\; E_{Y} = \\frac{1}{1 - p} + E_{R}.\n$$\n\n- From Green: with probability $p$ it stays Green (return occurs in one step), and with probability $1-p$ it goes to Yellow, after which the expected remaining time is $E_{Y}$:\n$$\nE_{G} = p \\cdot 1 + (1 - p)\\left(1 + E_{Y}\\right) = 1 + (1-p)E_Y.\n$$\nSubstitute the expressions for $E_Y$ and $E_R$:\n$$\nE_{G} = 1 + (1-p) \\left( \\frac{1}{1-p} + E_R \\right) = 1 + 1 + (1-p)E_R = 2 + (1-p)\\frac{1}{1-q}.\n$$\nTherefore, the expected number of steps until the light is Green again for the first time, starting from Green, is\n$$\n2 + \\frac{1 - p}{1 - q}.\n$$", "answer": "$$\\boxed{2 + \\frac{1 - p}{1 - q}}$$", "id": "1301641"}, {"introduction": "While first-step analysis is versatile, there is an alternative and often more elegant method for finding mean recurrence times in many common systems. For any irreducible, positive recurrent Markov chain, the expected return time to a state $i$ is simply the reciprocal of its stationary probability, $\\mu_i = \\frac{1}{\\pi_i}$. This problem demonstrates this powerful theorem through a practical application in system reliability, calculating the average time between server reboots by first determining the system's steady-state behavior [@problem_id:1301645].", "problem": "A specialized computing server used for scientific simulations can be in one of three states: ONLINE, DEGRADED, or OFFLINE. The state of the server is monitored at discrete time intervals of exactly one hour. The transitions between states are modeled as a discrete-time Markov chain.\n\n- If the server is ONLINE, there is a probability $p$ that a minor fault will cause it to transition to the DEGRADED state in the next hour. Otherwise, it remains ONLINE.\n- If the server is DEGRADED, it cannot repair itself to the ONLINE state. There is a probability $q$ that a critical failure will occur, causing it to transition to the OFFLINE state in the next hour. Otherwise, it remains DEGRADED.\n- If the server is OFFLINE, an automated recovery protocol is initiated. This protocol guarantees that the server will be back in the ONLINE state in the next hour.\n\nThe probabilities $p$ and $q$ are constants such that $0 < p < 1$ and $0 < q < 1$.\n\nCalculate the mean recurrence time for the ONLINE state. Express your answer in hours, as a symbolic expression in terms of $p$ and $q$.", "solution": "Let's model the server's behavior as a discrete-time Markov chain with three states. We can label the states as follows:\n- State 0: ONLINE\n- State 1: DEGRADED\n- State 2: OFFLINE\n\nThe problem describes the transition probabilities between these states over one-hour intervals. We can summarize these probabilities in a transition matrix $P$, where the entry $P_{ij}$ is the probability of transitioning from state $i$ to state $j$.\n\nThe transitions are:\n- From State 0 (ONLINE):\n  - To State 1 (DEGRADED) with probability $p$. So, $P_{01} = p$.\n  - Stays in State 0 (ONLINE) with probability $1-p$. So, $P_{00} = 1-p$.\n  - Cannot go to State 2 (OFFLINE) directly. So, $P_{02} = 0$.\n- From State 1 (DEGRADED):\n  - To State 2 (OFFLINE) with probability $q$. So, $P_{12} = q$.\n  - Stays in State 1 (DEGRADED) with probability $1-q$. So, $P_{11} = 1-q$.\n  - Cannot go back to State 0 (ONLINE). So, $P_{10} = 0$.\n- From State 2 (OFFLINE):\n  - To State 0 (ONLINE) with probability 1. So, $P_{20} = 1$.\n  - Cannot go to State 1 or stay in State 2. So, $P_{21} = 0$ and $P_{22} = 0$.\n\nThe transition matrix $P$ is:\n$$\nP = \\begin{pmatrix}\n1-p & p & 0 \\\\\n0 & 1-q & q \\\\\n1 & 0 & 0\n\\end{pmatrix}\n$$\n\nThe question asks for the mean recurrence time of the ONLINE state (State 0). For an irreducible, positive recurrent Markov chain, the mean recurrence time $\\mu_i$ of a state $i$ is the reciprocal of its stationary probability $\\pi_i$.\n$$\n\\mu_i = \\frac{1}{\\pi_i}\n$$\nSo, our goal is to find the stationary distribution vector $\\pi = (\\pi_0, \\pi_1, \\pi_2)$, which is the unique probability vector that satisfies the equation $\\pi P = \\pi$, subject to the condition $\\pi_0 + \\pi_1 + \\pi_2 = 1$.\n\nThe equation $\\pi P = \\pi$ gives us a system of linear equations:\n1. $\\pi_0 = \\pi_0(1-p) + \\pi_1(0) + \\pi_2(1) \\implies \\pi_0 = \\pi_0 - p\\pi_0 + \\pi_2 \\implies p\\pi_0 = \\pi_2$\n2. $\\pi_1 = \\pi_0(p) + \\pi_1(1-q) + \\pi_2(0) \\implies \\pi_1 = p\\pi_0 + \\pi_1 - q\\pi_1 \\implies q\\pi_1 = p\\pi_0$\n3. $\\pi_2 = \\pi_0(0) + \\pi_1(q) + \\pi_2(0) \\implies \\pi_2 = q\\pi_1$\n\nNotice that the third equation is redundant if we use the first two. From (2), we have $\\pi_1 = \\frac{p}{q}\\pi_0$. Substituting this into (3) gives $\\pi_2 = q\\left(\\frac{p}{q}\\pi_0\\right) = p\\pi_0$, which is identical to equation (1).\n\nSo, we have expressions for $\\pi_1$ and $\\pi_2$ in terms of $\\pi_0$:\n- $\\pi_1 = \\frac{p}{q}\\pi_0$\n- $\\pi_2 = p\\pi_0$\n\nNow, we use the normalization condition $\\pi_0 + \\pi_1 + \\pi_2 = 1$:\n$$\n\\pi_0 + \\frac{p}{q}\\pi_0 + p\\pi_0 = 1\n$$\nFactor out $\\pi_0$:\n$$\n\\pi_0 \\left(1 + \\frac{p}{q} + p\\right) = 1\n$$\nTo simplify the expression in the parenthesis, find a common denominator:\n$$\n\\pi_0 \\left(\\frac{q + p + pq}{q}\\right) = 1\n$$\nNow, solve for $\\pi_0$:\n$$\n\\pi_0 = \\frac{q}{q + p + pq}\n$$\nThe mean recurrence time for the ONLINE state (State 0), denoted $\\mu_0$, is the reciprocal of $\\pi_0$:\n$$\n\\mu_0 = \\frac{1}{\\pi_0} = \\frac{q + p + pq}{q}\n$$\nWe can simplify this expression by dividing each term in the numerator by the denominator:\n$$\n\\mu_0 = \\frac{q}{q} + \\frac{p}{q} + \\frac{pq}{q} = 1 + \\frac{p}{q} + p\n$$\nThe units are in hours, as the time step of the Markov chain is one hour.", "answer": "$$\\boxed{1 + p + \\frac{p}{q}}$$", "id": "1301645"}]}