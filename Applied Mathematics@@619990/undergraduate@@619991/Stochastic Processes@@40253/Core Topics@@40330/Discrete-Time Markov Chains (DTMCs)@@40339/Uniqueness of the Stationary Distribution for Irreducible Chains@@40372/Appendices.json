{"hands_on_practices": [{"introduction": "A core application of Markov chains is predicting the long-term behavior of a system. This practice provides a direct, computational exercise in finding the unique stationary distribution for an irreducible chain [@problem_id:1348565]. By setting up and solving the fundamental equation $\\pi = \\pi P$ along with the normalization constraint $\\sum_{i} \\pi_i = 1$, you will master the essential skill of determining the long-run proportion of time the system spends in each state.", "problem": "The mood of a newly released virtual pet is modeled as a discrete-time Markov chain. The pet's mood can be in one of three states at the end of each hour: State 1 (Happy), State 2 (Neutral), or State 3 (Sad). The transition probabilities from one state to another over a one-hour period are given by the following transition matrix $P$:\n\n$$\nP = \\begin{pmatrix} 0.5 & 0.5 & 0 \\\\ 0.2 & 0.2 & 0.6 \\\\ 0.1 & 0.8 & 0.1 \\end{pmatrix}\n$$\n\nwhere the entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$.\n\nFor an irreducible and aperiodic Markov chain like this one, it is a known property that after many time steps, the probability of finding the pet in any given mood becomes independent of its initial mood. This implies that for a very large integer $n$, all rows of the matrix $P^n$ (the $n$-step transition matrix) converge to the same limiting row vector $\\pi = (\\pi_1, \\pi_2, \\pi_3)$. This vector $\\pi$ represents the long-term proportions of time the pet spends in each mood.\n\nCalculate this limiting vector $\\pi$. Express your answer as a row vector of three exact fractions in their simplest form.", "solution": "Let $\\pi = (\\pi_{1},\\pi_{2},\\pi_{3})$ denote the limiting row vector. For an irreducible and aperiodic Markov chain, $\\pi$ is the unique stationary distribution satisfying $\\pi = \\pi P$ and $\\pi_{1}+\\pi_{2}+\\pi_{3}=1$.\n\nWrite $P$ using exact fractions:\n$$\nP=\\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n\\frac{1}{5} & \\frac{1}{5} & \\frac{3}{5} \\\\\n\\frac{1}{10} & \\frac{4}{5} & \\frac{1}{10}\n\\end{pmatrix}.\n$$\nThe stationarity equations $\\pi = \\pi P$ componentwise are:\n$$\n\\pi_{1}=\\frac{1}{2}\\pi_{1}+\\frac{1}{5}\\pi_{2}+\\frac{1}{10}\\pi_{3},\\quad\n\\pi_{2}=\\frac{1}{2}\\pi_{1}+\\frac{1}{5}\\pi_{2}+\\frac{4}{5}\\pi_{3},\\quad\n\\pi_{3}=0\\cdot\\pi_{1}+\\frac{3}{5}\\pi_{2}+\\frac{1}{10}\\pi_{3}.\n$$\nRewriting each by moving all terms to the left and clearing denominators:\n$$\n5\\pi_{1}-2\\pi_{2}-\\pi_{3}=0,\\qquad 8\\pi_{2}-5\\pi_{1}-8\\pi_{3}=0,\\qquad 9\\pi_{3}-6\\pi_{2}=0.\n$$\nFrom $9\\pi_{3}-6\\pi_{2}=0$ we get $\\pi_{3}=\\frac{2}{3}\\pi_{2}$. Substitute this into $5\\pi_{1}-2\\pi_{2}-\\pi_{3}=0$:\n$$\n5\\pi_{1}-2\\pi_{2}-\\frac{2}{3}\\pi_{2}=0 \\;\\Longrightarrow\\; 5\\pi_{1}-\\frac{8}{3}\\pi_{2}=0 \\;\\Longrightarrow\\; \\pi_{1}=\\frac{8}{15}\\pi_{2}.\n$$\nUse the normalization $\\pi_{1}+\\pi_{2}+\\pi_{3}=1$ with $\\pi_{1}=\\frac{8}{15}\\pi_{2}$ and $\\pi_{3}=\\frac{2}{3}\\pi_{2}$:\n$$\n\\frac{8}{15}\\pi_{2}+\\pi_{2}+\\frac{2}{3}\\pi_{2}=1 \\;\\Longrightarrow\\; \\left(\\frac{8}{15}+\\frac{15}{15}+\\frac{10}{15}\\right)\\pi_{2}=1 \\;\\Longrightarrow\\; \\frac{33}{15}\\pi_{2}=1 \\;\\Longrightarrow\\; \\pi_{2}=\\frac{5}{11}.\n$$\nThen\n$$\n\\pi_{1}=\\frac{8}{15}\\cdot\\frac{5}{11}=\\frac{8}{33},\\qquad \\pi_{3}=\\frac{2}{3}\\cdot\\frac{5}{11}=\\frac{10}{33}.\n$$\nTherefore, the limiting vector is\n$$\n\\pi=\\left(\\frac{8}{33},\\frac{5}{11},\\frac{10}{33}\\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{8}{33} & \\frac{5}{11} & \\frac{10}{33} \\end{pmatrix}}$$", "id": "1348565"}, {"introduction": "While the algebraic method is always reliable, deep understanding often comes from recognizing and exploiting a problem's underlying structure. This exercise [@problem_id:1348583] presents a highly symmetric random walk, allowing you to deduce the stationary distribution through logical reasoning rather than extensive calculation. This practice highlights how symmetry can provide a powerful shortcut to the solution, reinforcing the connection between the structure of a chain and its long-term behavior.", "problem": "A particle performs a random walk on a set of four distinct sites, which we can label as $\\{1, 2, 3, 4\\}$. At each discrete time step, the particle, currently at one site, jumps to one of the *other three* sites. The choice of the destination site is made with equal probability among the available options. For example, if the particle is at site 1, it will jump to site 2, 3, or 4, each with a probability of $1/3$. Assume the process continues for a very large number of steps. What is the limiting probability of finding the particle at site 2? Express your answer as a fraction.", "solution": "Model the process as a time-homogeneous Markov chain on the state space $\\{1,2,3,4\\}$ with transition probabilities\n$$\nP_{ij}=\\begin{cases}\n\\frac{1}{3}, & i\\neq j,\\\\\n0, & i=j.\n\\end{cases}\n$$\nSince $P_{ij}>0$ for all $i\\neq j$, every state can be reached from any other in one step, so the chain is irreducible.\n\nTo show aperiodicity, compute return probabilities. For any state $i$,\n$$\nP^{2}_{ii}=\\sum_{j\\neq i}P_{ij}P_{ji}=3\\cdot \\frac{1}{3}\\cdot \\frac{1}{3}=\\frac{1}{3}>0.\n$$\nFor three steps, count the return paths $i\\to j\\to k\\to i$ with $j\\neq i$, $k\\neq j$, and $k\\neq i$. There are $3\\cdot 2=6$ such paths, each with probability $\\left(\\frac{1}{3}\\right)^{3}$, hence\n$$\nP^{3}_{ii}=6\\left(\\frac{1}{3}\\right)^{3}>0.\n$$\nTherefore the set $\\{n\\geq 1:P^{n}_{ii}>0\\}$ contains $2$ and $3$, and $\\gcd(2,3)=1$, so the period is $1$. The chain is thus ergodic (irreducible and aperiodic), implying convergence to a unique stationary distribution.\n\nA stationary distribution $\\pi=(\\pi_{1},\\pi_{2},\\pi_{3},\\pi_{4})$ satisfies $\\pi=\\pi P$ and $\\sum_{j=1}^{4}\\pi_{j}=1$. By symmetry (or because $P$ is doubly stochastic, since each column has three entries $\\frac{1}{3}$ and one $0$), all components are equal: let $\\pi_{j}=c$ for all $j$. Then\n$$\n\\sum_{j=1}^{4}\\pi_{j}=4c=1 \\quad \\Rightarrow \\quad c=\\frac{1}{4}.\n$$\nOne can also verify stationarity directly:\n$$\n(\\pi P)_{j}=\\sum_{i=1}^{4}\\pi_{i}P_{ij}=c\\sum_{i\\neq j}\\frac{1}{3}=c\\cdot \\frac{3}{3}=c=\\pi_{j}.\n$$\nBy ergodicity, for any initial distribution,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(X_{n}=j)=\\pi_{j}.\n$$\nTherefore, the limiting probability of being at site $2$ is $\\pi_{2}=\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1348583"}, {"introduction": "A crucial aspect of scientific reasoning is understanding not just a theorem, but also its limitations and converses. This problem [@problem_id:1348575] challenges you to investigate whether the uniqueness of a stationary distribution is a sufficient condition for a Markov chain to be irreducible. By considering the role of transient states and recurrent classes, you will uncover a key subtlety about the relationship between a chain's topology and its equilibrium states.", "problem": "In the study of discrete-time Markov chains, a fundamental theorem states that any finite-state, irreducible Markov chain has a unique stationary distribution. A student of probability theory wonders if the converse of this statement holds true.\n\nConsider the following claim regarding a Markov chain with a finite number of states:\n\n\"If a finite-state Markov chain is known to have exactly one stationary distribution, then the chain must be irreducible.\"\n\nWhich one of the following options correctly evaluates this claim?\n\nA. The claim is always true. The uniqueness of the stationary distribution is a sufficient condition for irreducibility.\n\nB. The claim is always false. No reducible chain can have a unique stationary distribution.\n\nC. The claim is false. A counterexample can be constructed using a reducible chain that contains multiple distinct recurrent communicating classes.\n\nD. The claim is false. A counterexample can be constructed using a reducible chain that contains exactly one recurrent communicating class and at least one transient state.\n\nE. The claim is true only if all states in the chain are also aperiodic.", "solution": "The problem asks us to evaluate the claim: \"If a finite-state Markov chain has exactly one stationary distribution, then the chain must be irreducible.\" To do this, we need to determine if a reducible chain can have a unique stationary distribution.\n\nLet's begin by recalling the relevant definitions for a finite-state Markov chain with transition matrix $P$.\n- A **stationary distribution** is a probability vector $\\pi$ (i.e., its entries are non-negative and sum to 1) such that $\\pi P = \\pi$.\n- A chain is **irreducible** if it consists of a single communicating class, meaning it is possible to go from any state $i$ to any state $j$ in a finite number of steps.\n- A chain is **reducible** if it is not irreducible, meaning it has more than one communicating class.\n- A communicating class can be either **recurrent** or **transient**. In a finite chain, if we start in a recurrent class, we stay in it forever. If we start in a transient state, there is a non-zero probability that we will eventually leave that state and never return.\n\nThe known theorem states that if a chain is finite and irreducible, its stationary distribution is unique. We are investigating the converse. Let's analyze the properties of stationary distributions in reducible chains.\n\nAny stationary distribution $\\pi$ must assign zero probability to all transient states. That is, if state $i$ is transient, then $\\pi_i = 0$. This is because in the long run ($n \\to \\infty$), the probability of being in a transient state tends to zero, and the stationary distribution describes this long-run behavior. Therefore, the support of any stationary distribution must be contained within the set of recurrent states.\n\nLet's consider the structure of a reducible chain. It must have at least two communicating classes. We can break this down into two main cases based on the number of recurrent classes.\n\n**Case 1: The chain has multiple (say, $k \\ge 2$) recurrent classes.**\nLet the recurrent classes be $C_1, C_2, \\dots, C_k$. For each recurrent class $C_j$, we can consider it as a smaller, self-contained irreducible Markov chain. As such, each class $C_j$ has its own unique stationary distribution, let's call it $\\pi^{(j)}$, where the probabilities are non-zero only for states within $C_j$. We can extend each $\\pi^{(j)}$ to a stationary distribution for the entire chain by setting the probabilities for all states outside $C_j$ to zero. This gives us at least $k$ distinct stationary distributions: $\\pi^{(1)}, \\pi^{(2)}, \\dots, \\pi^{(k)}$.\nFurthermore, any convex combination of these distributions, $\\pi = \\alpha_1 \\pi^{(1)} + \\alpha_2 \\pi^{(2)} + \\dots + \\alpha_k \\pi^{(k)}$ where $\\alpha_j \\ge 0$ and $\\sum \\alpha_j = 1$, is also a valid stationary distribution for the full chain. Since there are infinitely many ways to choose the coefficients $\\alpha_j$, a chain with multiple recurrent classes has infinitely many stationary distributions, not a unique one.\nThis analysis shows that if a chain has a unique stationary distribution, it cannot have multiple recurrent classes. This directly refutes option C, which suggests such a structure as a counterexample. It also refutes option B, as we haven't yet ruled out all reducible chains.\n\n**Case 2: The chain has exactly one recurrent class and at least one transient state.**\nLet the single recurrent class be $C_R$ and the set of transient states be $T$. Since the chain is reducible, both $C_R$ and $T$ are non-empty, and there must be at least one path from a state in $T$ to a state in $C_R$. (If not, $T$ would contain another recurrent class).\nAs established, any stationary distribution $\\pi$ must have $\\pi_i = 0$ for all $i \\in T$. The entire probability mass must be on the recurrent class $C_R$.\nWithin the recurrent class $C_R$, the states form an irreducible sub-chain. By the fundamental theorem, this irreducible sub-chain has a unique stationary distribution, let's call it $\\pi_{R}$.\nTherefore, the only possible stationary distribution for the entire chain is one where the probabilities for states in $C_R$ are given by $\\pi_R$, and the probabilities for all states in $T$ are zero. This distribution is unique.\n\nLet's construct a concrete counterexample. Consider a 3-state chain with states $\\{1, 2, 3\\}$ and transition matrix:\n$$ P = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & 0.5 & 0 \\\\ 0 & 0.5 & 0.5 \\end{pmatrix} $$\nThe communicating classes are $C_1 = \\{1, 2\\}$ and $C_2 = \\{3\\}$.\nFrom state 3, we can reach state 2 ($P_{32} = 0.5 > 0$), but from states 1 or 2, we can never reach state 3. Therefore, class $C_1$ is recurrent and class $C_2$ is transient. This chain is reducible.\nLet's find the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$ by solving $\\pi = \\pi P$.\n1. $\\pi_1 = 0.5 \\pi_2$\n2. $\\pi_2 = \\pi_1 + 0.5 \\pi_2 + 0.5 \\pi_3$\n3. $\\pi_3 = 0.5 \\pi_3 \\implies 0.5 \\pi_3 = 0 \\implies \\pi_3 = 0$.\nAs expected, the stationary probability for the transient state is zero.\nSubstituting $\\pi_3 = 0$ into equation (2) gives $\\pi_2 = \\pi_1 + 0.5 \\pi_2$, which simplifies to $0.5 \\pi_2 = \\pi_1$. This is the same as equation (1).\nWe also have the normalization condition: $\\pi_1 + \\pi_2 + \\pi_3 = 1$.\nSubstituting $\\pi_1 = 0.5 \\pi_2$ and $\\pi_3 = 0$ into the normalization equation gives $0.5 \\pi_2 + \\pi_2 + 0 = 1$, which means $1.5 \\pi_2 = 1$, so $\\pi_2 = 2/3$.\nThen, $\\pi_1 = 0.5 \\pi_2 = 0.5 (2/3) = 1/3$.\nThe unique stationary distribution is $\\pi = (1/3, 2/3, 0)$.\n\nWe have successfully constructed a **reducible** chain that has a **unique** stationary distribution. Therefore, the original claim is **false**.\n\nNow let's evaluate the options:\n- A: False. Our counterexample disproves this.\n- B: False. Our counterexample is a reducible chain with a unique stationary distribution.\n- C: False. As shown in Case 1, a chain with multiple recurrent classes has a non-unique (in fact, infinite) set of stationary distributions. This structure cannot be a counterexample.\n- D: True. This option correctly describes the structure of our counterexample and the general principle for constructing such a counterexample: a single recurrent class that acts as a \"sink\" for all transient states.\n- E: False. Aperiodicity relates to the convergence to the stationary distribution, not its existence or uniqueness. Our counterexample's recurrent class is aperiodic ($P_{22}=0.5>0$), but we could have built a periodic one (e.g., a simple 2-cycle) and the uniqueness of the overall stationary distribution would still hold.\n\nThe correct evaluation is that the claim is false, and a counterexample is a reducible chain with exactly one recurrent class.", "answer": "$$\\boxed{D}$$", "id": "1348575"}]}