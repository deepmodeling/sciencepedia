{"hands_on_practices": [{"introduction": "Understanding a stochastic process begins with its finite-dimensional distributions, which describe the joint behavior of the process at any finite set of time points. This first practice provides a concrete entry point by focusing on one of the simplest and most fundamental models: the normalized random walk. By directly calculating the joint probability mass function for the process at two distinct times, you will build a foundational understanding of how the behavior of underlying independent steps dictates the properties of the process itself [@problem_id:1302889].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{Z_i\\}_{i \\ge 1}$, which represent the steps of a random walk. Each step is either forward or backward with equal probability, such that the probability mass function for each $Z_i$ is given by $P(Z_i = 1) = 1/2$ and $P(Z_i = -1) = 1/2$.\n\nThe position of the random walk at time $n$ is given by the sum of the steps, $S_n = \\sum_{i=1}^n Z_i$, with the starting position being $S_0=0$. A normalized stochastic process $\\{X_n\\}_{n \\ge 1}$ is constructed from this random walk, where $X_n = S_n / n$.\n\nDetermine the two-dimensional finite-dimensional distribution for this process at times $n=1$ and $n=2$. That is, find the joint probability mass function $P(X_1=x_1, X_2=x_2)$. Which of the following tables correctly represents this joint distribution?\n\nA.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2) & x_2 = -1 & x_2 = 0 & x_2 = 1 \\\\\n\\hline\nx_1 = -1 & 1/4 & 1/4 & 0 \\\\\nx_1 = 1 & 0 & 1/4 & 1/4 \\\\\n\\end{array}\n$$\n\nB.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2) & x_2 = -1 & x_2 = 0 & x_2 = 1 \\\\\n\\hline\nx_1 = -1 & 1/8 & 1/4 & 1/8 \\\\\nx_1 = 1 & 1/8 & 1/4 & 1/8 \\\\\n\\end{array}\n$$\n\nC.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2) & x_2 = -1 & x_2 = 0 & x_2 = 1 \\\\\n\\hline\nx_1 = -1 & 1/4 & 0 & 1/4 \\\\\nx_1 = 1 & 1/4 & 1/4 & 0 \\\\\n\\end{array}\n$$\n\nD.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2) & x_2 = -1 & x_2 = 0 & x_2 = 1 \\\\\n\\hline\nx_1 = -1 & 1/2 & 0 & 0 \\\\\nx_1 = 1 & 0 & 0 & 1/2 \\\\\n\\end{array}\n$$", "solution": "We are given i.i.d. steps with $P(Z_{i}=1)=\\frac{1}{2}$ and $P(Z_{i}=-1)=\\frac{1}{2}$. The random walk positions are $S_{n}=\\sum_{i=1}^{n}Z_{i}$ with $S_{0}=0$, and the normalized process is $X_{n}=S_{n}/n$.\n\nAt the specified times,\n$$\nX_{1}=\\frac{S_{1}}{1}=Z_{1}, \\qquad X_{2}=\\frac{S_{2}}{2}=\\frac{Z_{1}+Z_{2}}{2}.\n$$\nBy independence and identical distribution, for each pair $(z_{1},z_{2})\\in\\{-1,1\\}^{2}$,\n$$\nP(Z_{1}=z_{1},Z_{2}=z_{2})=P(Z_{1}=z_{1})P(Z_{2}=z_{2})=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\n\nEnumerate the four possible outcomes and map them to $(X_{1},X_{2})$:\n- If $(Z_{1},Z_{2})=(1,1)$, then $X_{1}=1$ and $X_{2}=\\frac{1+1}{2}=1$, giving $(X_{1},X_{2})=(1,1)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(1,-1)$, then $X_{1}=1$ and $X_{2}=\\frac{1+(-1)}{2}=0$, giving $(X_{1},X_{2})=(1,0)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(-1,1)$, then $X_{1}=-1$ and $X_{2}=\\frac{-1+1}{2}=0$, giving $(X_{1},X_{2})=(-1,0)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(-1,-1)$, then $X_{1}=-1$ and $X_{2}=\\frac{-1+(-1)}{2}=-1$, giving $(X_{1},X_{2})=(-1,-1)$ with probability $\\frac{1}{4}$.\n\nTherefore, the joint pmf has nonzero entries only at $(-1,-1)$, $(-1,0)$, $(1,0)$, and $(1,1)$ with probabilities $\\frac{1}{4}$ each, and zeros elsewhere. Comparing with the provided tables, this matches option A.", "answer": "$$\\boxed{A}$$", "id": "1302889"}, {"introduction": "While a full probability distribution offers a complete picture, we often characterize a process by its moments, particularly its mean and covariance structure. This exercise introduces the \"running average\" process and asks you to compute its covariance matrix, capturing the relationship between the process at two consecutive times. This practice is essential for learning how the definition of a process, in this case through overlapping sums, directly translates into statistical dependencies and correlations over time [@problem_id:1302890].", "problem": "Consider a sequence of Independent and Identically Distributed (IID) random variables $\\{Z_k\\}_{k=1, 2, \\dots}$. These random variables have a mean of $E[Z_k] = 0$ and a finite, non-zero variance of $\\text{Var}(Z_k) = \\sigma^2$ for all $k$.\n\nA new stochastic process, called the running average process, is defined as $\\{X_n\\}_{n=1, 2, \\dots}$ where\n$$ X_n = \\frac{1}{n} \\sum_{k=1}^{n} Z_k $$\n\nDetermine the $2 \\times 2$ covariance matrix of the random vector $(X_n, X_{n+1})$. Express your answer as a matrix whose entries are in terms of $n$ and $\\sigma$.", "solution": "The covariance matrix for the random vector $(X_n, X_{n+1})$ is a $2 \\times 2$ matrix given by:\n$$ \\mathbf{C} = \\begin{pmatrix} \\text{Var}(X_n) & \\text{Cov}(X_n, X_{n+1}) \\\\ \\text{Cov}(X_{n+1}, X_n) & \\text{Var}(X_{n+1}) \\end{pmatrix} $$\nWe need to compute its three unique components: $\\text{Var}(X_n)$, $\\text{Var}(X_{n+1})$, and $\\text{Cov}(X_n, X_{n+1})$.\n\nFirst, let's find the expected value of the process $X_n$. Using the linearity of expectation:\n$$ E[X_n] = E\\left[\\frac{1}{n} \\sum_{k=1}^{n} Z_k\\right] = \\frac{1}{n} \\sum_{k=1}^{n} E[Z_k] $$\nSince we are given that $E[Z_k] = 0$ for all $k$, we have:\n$$ E[X_n] = \\frac{1}{n} \\sum_{k=1}^{n} 0 = 0 $$\nSimilarly, $E[X_{n+1}] = 0$.\n\nNext, we compute the variance of $X_n$, which is a diagonal element of the covariance matrix. The variance of a random variable $Y$ is given by $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$. Since $E[X_n]=0$, this simplifies to $\\text{Var}(X_n) = E[X_n^2]$.\n$$ \\text{Var}(X_n) = \\text{Var}\\left(\\frac{1}{n} \\sum_{k=1}^{n} Z_k\\right) $$\nUsing the property that $\\text{Var}(aY) = a^2 \\text{Var}(Y)$, we get:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{k=1}^{n} Z_k\\right) $$\nSince the random variables $\\{Z_k\\}$ are independent, the variance of their sum is the sum of their variances:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\text{Var}(Z_k) $$\nGiven that $\\text{Var}(Z_k) = \\sigma^2$ for all $k$:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\sigma^2 = \\frac{1}{n^2} (n \\sigma^2) = \\frac{\\sigma^2}{n} $$\nBy the same logic, the other diagonal element is:\n$$ \\text{Var}(X_{n+1}) = \\frac{\\sigma^2}{n+1} $$\n\nNow, we compute the off-diagonal term, the covariance $\\text{Cov}(X_n, X_{n+1})$. The covariance is defined as $\\text{Cov}(Y, W) = E[YW] - E[Y]E[W]$. Since $E[X_n] = 0$ and $E[X_{n+1}]=0$, this simplifies to $\\text{Cov}(X_n, X_{n+1}) = E[X_n X_{n+1}]$.\nLet's express $X_{n+1}$ in terms of $X_n$:\n$$ X_{n+1} = \\frac{1}{n+1} \\sum_{k=1}^{n+1} Z_k = \\frac{1}{n+1} \\left( \\left(\\sum_{k=1}^{n} Z_k\\right) + Z_{n+1} \\right) $$\nRecognizing that $\\sum_{k=1}^{n} Z_k = n X_n$, we can write:\n$$ X_{n+1} = \\frac{1}{n+1} (n X_n + Z_{n+1}) $$\nNow we can compute the covariance:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\text{Cov}\\left(X_n, \\frac{n X_n + Z_{n+1}}{n+1}\\right) $$\nUsing the bilinearity property of covariance, $\\text{Cov}(A, bB+cC) = b \\text{Cov}(A,B) + c \\text{Cov}(A,C)$:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\frac{n}{n+1} \\text{Cov}(X_n, X_n) + \\frac{1}{n+1} \\text{Cov}(X_n, Z_{n+1}) $$\nWe know that $\\text{Cov}(X_n, X_n) = \\text{Var}(X_n) = \\frac{\\sigma^2}{n}$.\nThe second term is $\\text{Cov}(X_n, Z_{n+1}) = \\text{Cov}\\left(\\frac{1}{n} \\sum_{k=1}^{n} Z_k, Z_{n+1}\\right)$. Since $Z_{n+1}$ is independent of all $Z_k$ for $k=1, \\dots, n$, it is also independent of any function of them, including their average $X_n$. The covariance of independent random variables is zero. Thus, $\\text{Cov}(X_n, Z_{n+1}) = 0$.\nSubstituting these results back:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\frac{n}{n+1} \\left(\\frac{\\sigma^2}{n}\\right) + \\frac{1}{n+1} (0) = \\frac{\\sigma^2}{n+1} $$\nSince the covariance matrix is symmetric, $\\text{Cov}(X_{n+1}, X_n) = \\text{Cov}(X_n, X_{n+1})$.\n\nFinally, we can assemble the covariance matrix:\n$$ \\mathbf{C} = \\begin{pmatrix} \\text{Var}(X_n) & \\text{Cov}(X_n, X_{n+1}) \\\\ \\text{Cov}(X_n, X_{n+1}) & \\text{Var}(X_{n+1}) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sigma^2}{n} & \\frac{\\sigma^2}{n+1} \\\\ \\frac{\\sigma^2}{n+1} & \\frac{\\sigma^2}{n+1} \\end{pmatrix} $$\nThis can also be written by factoring out $\\sigma^2$:\n$$ \\mathbf{C} = \\sigma^2 \\begin{pmatrix} \\frac{1}{n} & \\frac{1}{n+1} \\\\ \\frac{1}{n+1} & \\frac{1}{n+1} \\end{pmatrix} $$", "answer": "$$\\boxed{\\sigma^{2} \\begin{pmatrix} \\frac{1}{n} & \\frac{1}{n+1} \\\\ \\frac{1}{n+1} & \\frac{1}{n+1} \\end{pmatrix}}$$", "id": "1302890"}, {"introduction": "Stochastic processes can be constructed from one another, and the properties of the derived process are often inherited from the original. This practice explores this idea by examining the \"sign process,\" derived from a standard Brownian motion. You will determine the correlation between the sign of the Brownian motion at two different times, a task which requires linking the discrete outcomes of the sign function back to the joint Gaussian distribution of the underlying continuous process [@problem_id:1302876].", "problem": "A standard one-dimensional Brownian motion, denoted by $\\{W_t\\}_{t \\ge 0}$, is a stochastic process characterized by the following properties:\n1.  $W_0 = 0$.\n2.  The process has independent increments: for any time points $0 \\le s < t < u < v$, the random variables $W_t - W_s$ and $W_v - W_u$ are independent.\n3.  The process has stationary, normally distributed increments: for any $t > s \\ge 0$, the increment $W_t - W_s$ is a normal random variable with mean 0 and variance $t-s$, i.e., $W_t - W_s \\sim N(0, t-s)$.\n\nFrom this process, we can construct a new stochastic process called the sign process, $\\{X_t\\}_{t > 0}$, defined as $X_t = \\text{sgn}(W_t)$, where the sign function is given by\n$$\n\\text{sgn}(x) = \\begin{cases} \n      1 & \\text{if } x > 0 \\\\\n      0 & \\text{if } x = 0 \\\\\n      -1 & \\text{if } x < 0\n   \\end{cases}\n$$\nConsider two time points $t_1$ and $t_2$ such that $0 < t_1 < t_2$. For the purposes of this problem, you may assume that the Brownian motion is never exactly zero at times $t_1$ and $t_2$, so that $X_{t_1}$ and $X_{t_2}$ only take values in $\\{-1, 1\\}$.\n\nDetermine the correlation coefficient, $\\text{Corr}(X_{t_1}, X_{t_2})$, as a function of $t_1$ and $t_2$. Your final answer should be a single closed-form analytic expression.", "solution": "We are given a standard Brownian motion $\\{W_{t}\\}_{t \\ge 0}$ with $W_{0}=0$, independent increments, and Gaussian increments $W_{t}-W_{s} \\sim N(0,t-s)$. Define $X_{t}=\\operatorname{sgn}(W_{t})$ for $t>0$, and fix $0<t_{1}<t_{2}$ with the understanding that $W_{t_{1}},W_{t_{2}} \\neq 0$ almost surely so that $X_{t_{1}},X_{t_{2}} \\in \\{-1,1\\}$.\n\nFirst, by symmetry of $W_{t}$ about $0$, we have $\\mathbb{P}(W_{t}>0)=\\mathbb{P}(W_{t}<0)=\\frac{1}{2}$, hence\n$$\n\\mathbb{E}[X_{t}]=\\mathbb{E}[\\operatorname{sgn}(W_{t})]=0,\\qquad \\operatorname{Var}(X_{t})=1.\n$$\nTherefore, the correlation coefficient equals the covariance:\n$$\n\\operatorname{Corr}(X_{t_{1}},X_{t_{2}})=\\frac{\\operatorname{Cov}(X_{t_{1}},X_{t_{2}})}{\\sqrt{\\operatorname{Var}(X_{t_{1}})\\operatorname{Var}(X_{t_{2}})}}=\\mathbb{E}[X_{t_{1}}X_{t_{2}}]=\\mathbb{E}[\\operatorname{sgn}(W_{t_{1}})\\operatorname{sgn}(W_{t_{2}})].\n$$\nSince $\\operatorname{sgn}(W_{t_{1}})\\operatorname{sgn}(W_{t_{2}})=1$ if $W_{t_{1}}$ and $W_{t_{2}}$ have the same sign and $-1$ otherwise, we get\n$$\n\\mathbb{E}[\\operatorname{sgn}(W_{t_{1}})\\operatorname{sgn}(W_{t_{2}})]=\\mathbb{P}\\big(W_{t_{1}}W_{t_{2}}>0\\big)-\\mathbb{P}\\big(W_{t_{1}}W_{t_{2}}<0\\big)=2\\mathbb{P}(W_{t_{1}}W_{t_{2}}>0)-1.\n$$\nBy symmetry,\n$$\n\\mathbb{P}(W_{t_{1}}W_{t_{2}}>0)=\\mathbb{P}(W_{t_{1}}>0,W_{t_{2}}>0)+\\mathbb{P}(W_{t_{1}}<0,W_{t_{2}}<0)=2\\,\\mathbb{P}(W_{t_{1}}>0,W_{t_{2}}>0),\n$$\nso\n$$\n\\mathbb{E}[\\operatorname{sgn}(W_{t_{1}})\\operatorname{sgn}(W_{t_{2}})]=4\\,\\mathbb{P}(W_{t_{1}}>0,W_{t_{2}}>0)-1.\n$$\n\nThe pair $(W_{t_{1}},W_{t_{2}})$ is jointly Gaussian with mean zero, variances $\\operatorname{Var}(W_{t_{1}})=t_{1}$ and $\\operatorname{Var}(W_{t_{2}})=t_{2}$, and covariance\n$$\n\\operatorname{Cov}(W_{t_{1}},W_{t_{2}})=\\min(t_{1},t_{2})=t_{1},\n$$\nso their correlation is\n$$\n\\rho=\\frac{\\operatorname{Cov}(W_{t_{1}},W_{t_{2}})}{\\sqrt{\\operatorname{Var}(W_{t_{1}})\\operatorname{Var}(W_{t_{2}})}}=\\frac{t_{1}}{\\sqrt{t_{1}t_{2}}}=\\sqrt{\\frac{t_{1}}{t_{2}}}.\n$$\nLet $Z_{1}=W_{t_{1}}/\\sqrt{t_{1}}$ and $Z_{2}=W_{t_{2}}/\\sqrt{t_{2}}$. Then $(Z_{1},Z_{2})$ is standard bivariate normal with correlation $\\rho$, and\n$$\n\\mathbb{P}(W_{t_{1}}>0,W_{t_{2}}>0)=\\mathbb{P}(Z_{1}>0,Z_{2}>0).\n$$\nFor a standard bivariate normal pair with correlation $\\rho$, the quadrant probability satisfies the classical identity\n$$\n\\mathbb{P}(Z_{1}>0,Z_{2}>0)=\\frac{1}{4}+\\frac{1}{2\\pi}\\arcsin(\\rho).\n$$\nSubstituting this into the expression for the expectation yields\n$$\n\\mathbb{E}[\\operatorname{sgn}(W_{t_{1}})\\operatorname{sgn}(W_{t_{2}})]=4\\left(\\frac{1}{4}+\\frac{1}{2\\pi}\\arcsin(\\rho)\\right)-1=\\frac{2}{\\pi}\\arcsin(\\rho).\n$$\nFinally, replacing $\\rho$ by $\\sqrt{t_{1}/t_{2}}$ gives\n$$\n\\operatorname{Corr}(X_{t_{1}},X_{t_{2}})=\\frac{2}{\\pi}\\arcsin\\!\\left(\\sqrt{\\frac{t_{1}}{t_{2}}}\\right).\n$$\nThis is a closed-form analytic expression in terms of $t_{1}$ and $t_{2}$.", "answer": "$$\\boxed{\\frac{2}{\\pi}\\arcsin\\!\\left(\\sqrt{\\frac{t_{1}}{t_{2}}}\\right)}$$", "id": "1302876"}]}