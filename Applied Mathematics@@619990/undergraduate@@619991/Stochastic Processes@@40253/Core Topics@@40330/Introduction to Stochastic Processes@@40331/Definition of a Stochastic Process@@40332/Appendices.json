{"hands_on_practices": [{"introduction": "A cornerstone of analyzing stochastic processes is understanding their \"memory,\" or how values at different times relate to each other. The autocovariance function is our primary tool for quantifying this relationship. This first practice exercise [@problem_id:731518] provides a hands-on opportunity to calculate the autocovariance for a fundamental and widely used type of discrete-time process, a moving average process, helping you build intuition for how a process's structure dictates its correlation over time.", "problem": "Consider a discrete-time stochastic process $\\{X_n\\}_{n \\in \\mathbb{Z}}$ defined by a linear filter applied to a white noise process. The process is given by:\n$$\nX_n = a Z_n + b Z_{n-1}\n$$\nfor all integers $n$, where $a$ and $b$ are non-zero real constants. The sequence $\\{Z_n\\}_{n \\in \\mathbb{Z}}$ consists of independent and identically distributed (i.i.d.) random variables, each with zero mean ($E[Z_n] = 0$) and unit variance ($Var(Z_n) = 1$).\n\nThe autocovariance function of a wide-sense stationary process $\\{Y_t\\}$ at lag $k$ is defined as $\\gamma_Y(k) = \\text{Cov}(Y_t, Y_{t-k}) = E[(Y_t - E[Y_t])(Y_{t-k} - E[Y_{t-k}])]$. The process $\\{X_n\\}$ as defined above is wide-sense stationary.\n\nYour task is to derive the value of the quantity $S$, defined as a linear combination of the autocovariance values of the process $\\{X_n\\}$ at different lags:\n$$\nS = \\gamma_X(0) + 2\\gamma_X(1) - 3\\gamma_X(2)\n$$\nExpress your final answer in terms of the constants $a$ and $b$.", "solution": "1. The autocovariance at lag $k$ is \n$$\n\\gamma_X(k)=\\mathrm{Cov}(X_n,X_{n-k})=E[X_nX_{n-k}],\n$$\nwith $X_n=aZ_n+bZ_{n-1}$ and $X_{n-k}=aZ_{n-k}+bZ_{n-k-1}$.\n2. Expand the product:\n$$\nE[X_nX_{n-k}]\n=E\\bigl[(aZ_n+bZ_{n-1})(aZ_{n-k}+bZ_{n-k-1})\\bigr]\n=a^2E[Z_nZ_{n-k}]+abE[Z_nZ_{n-k-1}]+baE[Z_{n-1}Z_{n-k}]+b^2E[Z_{n-1}Z_{n-k-1}].\n$$\n3. Use $E[Z_iZ_j]=\\delta_{ij}$ to get\n$$\n\\gamma_X(k)\n=(a^2+b^2)\\delta_{k,0}+ab\\,\\delta_{k,1}+ab\\,\\delta_{k,-1}.\n$$\n4. Hence\n$$\n\\gamma_X(0)=a^2+b^2,\\quad\n\\gamma_X(1)=ab,\\quad\n\\gamma_X(2)=0.\n$$\n5. Form the combination\n$$\nS=\\gamma_X(0)+2\\gamma_X(1)-3\\gamma_X(2)\n=(a^2+b^2)+2(ab)-3\\cdot0\n=a^2+2ab+b^2\n=(a+b)^2.\n$$", "answer": "$$\\boxed{(a+b)^2}$$", "id": "731518"}, {"introduction": "Some of the most important stochastic processes, like the Wiener process, are often defined not by a single formula but by a set of fundamental properties. Understanding these defining characteristics is crucial for grasping the essence of such processes. This thought experiment [@problem_id:1296396] challenges you to critically examine a simple, constructed process and determine which key property of a standard Wiener process it fails to satisfy, thereby sharpening your understanding of what truly defines Brownian motion.", "problem": "A stochastic process, denoted as $X_t$, is constructed for a time interval $t \\in [0, T]$, where $T$ is a positive constant. The process is defined by connecting the origin $(0,0)$ to the random point $(T, \\sqrt{T}Z)$ in the time-value plane with a straight line. Here, $Z$ is a random variable following the standard normal distribution, i.e., $Z \\sim N(0,1)$. The value of the process $X_t$ at any time $t$ is the vertical coordinate of the point on this line corresponding to the horizontal coordinate $t$.\n\nA standard Wiener process (or standard Brownian motion) $W_t$ is defined by a set of fundamental properties. Which of the following properties of a standard Wiener process is violated by the process $X_t$?\n\nA. The process starts at the origin, i.e., $X_0 = 0$ almost surely.\n\nB. The sample paths of the process are continuous functions of time.\n\nC. The process has independent increments, meaning that for any choice of non-overlapping time intervals $[t_1, t_2]$ and $[t_3, t_4]$ with $0 \\le t_1 < t_2 \\le t_3 < t_4 \\le T$, the random variables $(X_{t_2} - X_{t_1})$ and $(X_{t_4} - X_{t_3})$ are statistically independent.\n\nD. The process $X_t$ is a Gaussian process, meaning that for any finite set of times $t_1, t_2, \\dots, t_k$ in $[0, T]$, the random vector $(X_{t_1}, X_{t_2}, \\dots, X_{t_k})$ has a multivariate normal distribution.", "solution": "By construction, the straight line through the points $(0,0)$ and $(T,\\sqrt{T}Z)$ has slope\n$$\nm=\\frac{\\sqrt{T}Z-0}{T-0}=\\frac{Z}{\\sqrt{T}}.\n$$\nHence, for any $t\\in[0,T]$,\n$$\nX_{t}=mt=\\frac{t}{\\sqrt{T}}\\,Z.\n$$\n\nCheck A: At $t=0$,\n$$\nX_{0}=\\frac{0}{\\sqrt{T}}\\,Z=0\n$$\nalmost surely, so the process starts at the origin. Thus A holds.\n\nCheck B: For each outcome of $Z$, the sample path $t\\mapsto X_{t}=\\frac{t}{\\sqrt{T}}Z$ is a linear function of $t$, hence continuous on $[0,T]$. Thus B holds.\n\nCheck C: For $0\\le t_{1}<t_{2}\\le t_{3}<t_{4}\\le T$,\n$$\nX_{t_{2}}-X_{t_{1}}=\\frac{t_{2}-t_{1}}{\\sqrt{T}}\\,Z,\\qquad\nX_{t_{4}}-X_{t_{3}}=\\frac{t_{4}-t_{3}}{\\sqrt{T}}\\,Z.\n$$\nBoth increments are scalar multiples of the same random variable $Z$, so they are perfectly correlated unless one interval has zero length. In particular, their covariance is\n$$\n\\operatorname{Cov}\\!\\left(X_{t_{2}}-X_{t_{1}},\\,X_{t_{4}}-X_{t_{3}}\\right)\n=\\frac{(t_{2}-t_{1})(t_{4}-t_{3})}{T}\\,\\operatorname{Var}(Z)\n=\\frac{(t_{2}-t_{1})(t_{4}-t_{3})}{T}\\neq 0,\n$$\nso they are not independent. Therefore the independent increments property fails. Thus C is violated.\n\nCheck D: For any finite set $t_{1},\\dots,t_{k}$,\n$$\n\\begin{pmatrix}X_{t_{1}}\\\\ \\vdots \\\\ X_{t_{k}}\\end{pmatrix}\n=\\frac{1}{\\sqrt{T}}\\begin{pmatrix}t_{1}\\\\ \\vdots \\\\ t_{k}\\end{pmatrix}Z,\n$$\nwhich is a linear transformation of the Gaussian scalar $Z$. Therefore the vector is multivariate normal (possibly singular), and the process is Gaussian. Thus D holds.\n\nConsequently, the violated Wiener process property is independent increments.", "answer": "$$\\boxed{C}$$", "id": "1296396"}, {"introduction": "Gaussian processes form a cornerstone of stochastic modeling due to their mathematical tractability and wide-ranging applications, from finance to machine learning. A process earns this title only if it meets a specific and powerful criterion related to its finite-dimensional distributions. In this final practice [@problem_id:1304187], you will act as a signal processing engineer, applying the formal definition to determine whether two different models of a fluctuating signal qualify as Gaussian processes, reinforcing a key classification skill.", "problem": "In a signal processing laboratory, two engineers are tasked with creating simplified mathematical models for a fluctuating voltage signal, which they represent as a stochastic process where $t \\geq 0$ denotes time.\n\nThe first model, Process A, is defined as:\n$$A_t = Z \\sin(\\omega t + \\phi)$$\nHere, $Z$ is a random variable with a standard normal distribution, $Z \\sim N(0, 1)$, while $\\omega > 0$ and $\\phi$ are fixed, known real constants.\n\nThe second model, Process B, is defined as:\n$$B_t = U \\cos(\\omega t) + W \\sin(\\omega t)$$\nIn this model, $U$ and $W$ are independent random variables, both drawn from a standard normal distribution, i.e., $U \\sim N(0, 1)$ and $W \\sim N(0, 1)$. The parameter $\\omega$ is the same fixed, positive real constant as in Process A.\n\nA stochastic process $\\{X_t\\}$ is defined as a Gaussian process if, for any finite collection of time indices $t_1, t_2, \\dots, t_n$, the random vector $(X_{t_1}, X_{t_2}, \\dots, X_{t_n})$ has a multivariate normal distribution.\n\nBased on this definition, determine which of the proposed processes are Gaussian processes.\n\nA. Only Process A is a Gaussian process.\n\nB. Only Process B is a Gaussian process.\n\nC. Both Process A and Process B are Gaussian processes.\n\nD. Neither Process A nor Process B is a Gaussian process.", "solution": "We recall the definition: a stochastic process $\\{X_{t}\\}$ is Gaussian if for any finite set of times $t_{1},\\dots,t_{n}$, the random vector $(X_{t_{1}},\\dots,X_{t_{n}})$ is multivariate normal.\n\nProcess A: $A_{t}=Z\\sin(\\omega t+\\phi)$ with $Z\\sim N(0,1)$ independent of $t$, and $\\omega>0$, $\\phi\\in\\mathbb{R}$ fixed. For any $t_{1},\\dots,t_{n}$, define the deterministic vector $v\\in\\mathbb{R}^{n}$ by $v_{i}=\\sin(\\omega t_{i}+\\phi)$ for $i=1,\\dots,n$. Then\n$$\n\\begin{pmatrix}\nA_{t_{1}}\\\\\n\\vdots\\\\\nA_{t_{n}}\n\\end{pmatrix}\n=Z\n\\begin{pmatrix}\nv_{1}\\\\\n\\vdots\\\\\nv_{n}\n\\end{pmatrix}.\n$$\nFor any $a\\in\\mathbb{R}^{n}$, the linear functional $a^{\\top}(A_{t_{1}},\\dots,A_{t_{n}})^{\\top}=Z\\,a^{\\top}v$ is a scalar multiple of $Z$, hence is univariate normal. Therefore $(A_{t_{1}},\\dots,A_{t_{n}})$ is multivariate normal (possibly degenerate). Its mean is $0$, and its covariance matrix is\n$$\n\\Sigma_{A}=\\mathbb{E}[Z^{2}]\\,v v^{\\top}=v v^{\\top},\n$$\nso Process A is Gaussian.\n\nProcess B: $B_{t}=U\\cos(\\omega t)+W\\sin(\\omega t)$ with $U,W$ independent and $U\\sim N(0,1)$, $W\\sim N(0,1)$. Let $G=(U,W)^{\\top}\\sim N(0,I_{2})$. For any $t_{1},\\dots,t_{n}$, define the $n\\times 2$ deterministic matrix $M$ with $i$-th row $r_{i}=(\\cos(\\omega t_{i}),\\sin(\\omega t_{i}))$. Then\n$$\n\\begin{pmatrix}\nB_{t_{1}}\\\\\n\\vdots\\\\\nB_{t_{n}}\n\\end{pmatrix}\n= M G.\n$$\nSince $G$ is multivariate normal and $M$ is deterministic, $MG$ is multivariate normal. Hence Process B is Gaussian. Its mean is $0$, and its covariance matrix is\n$$\n\\Sigma_{B}=M M^{\\top},\\quad\n(\\Sigma_{B})_{ij}=\\cos(\\omega t_{i})\\cos(\\omega t_{j})+\\sin(\\omega t_{i})\\sin(\\omega t_{j})\n=\\cos\\big(\\omega(t_{i}-t_{j})\\big),\n$$\nusing the trigonometric identity $\\cos x\\cos y+\\sin x\\sin y=\\cos(x-y)$.\n\nTherefore, both proposed processes are Gaussian processes.", "answer": "$$\\boxed{C}$$", "id": "1304187"}]}