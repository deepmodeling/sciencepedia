{"hands_on_practices": [{"introduction": "The concept of an adapted process is fundamental to modeling phenomena that evolve over time. It formalizes the intuitive idea that any quantity we measure at a time $n$ should only depend on information available up to that time. This first practice [@problem_id:1302339] asks you to examine several processes derived from a simple random walk and determine which ones are \"adapted\" to the natural flow of information, reinforcing the crucial rule against \"peeking into the future.\"", "problem": "Let $\\{X_i\\}_{i \\ge 1}$ be a sequence of independent and identically distributed random variables with the probability distribution $P(X_i = 1) = P(X_i = -1) = 1/2$. A simple symmetric random walk $\\{S_n\\}_{n \\ge 0}$ is defined by $S_0 = 0$ and $S_n = \\sum_{i=1}^n X_i$ for $n \\ge 1$.\n\nLet $\\{\\mathcal{F}_n\\}_{n \\ge 0}$ be the natural filtration generated by this random walk, where $\\mathcal{F}_n = \\sigma(S_0, S_1, \\dots, S_n)$ represents the information available up to time $n$. A stochastic process $\\{Y_n\\}_{n \\ge 0}$ is said to be adapted to the filtration $\\{\\mathcal{F}_n\\}$ if the random variable $Y_n$ is $\\mathcal{F}_n$-measurable for every $n \\ge 0$.\n\nConsider the following stochastic processes, defined for $n \\ge 1$ and for a fixed real constant $a$. Which of these processes is **not** adapted to the filtration $\\{\\mathcal{F}_n\\}$?\n\nA. $Y_n = S_n^2 - n$\n\nB. $Y_n = \\mathbf{1}_{\\{S_n > a\\}}$\n\nC. $Y_n = S_{n-1}$\n\nD. $Y_n = S_n - S_{n+1}$\n\nE. $Y_n = \\max_{0 \\le k \\le n} S_k$", "solution": "By definition, a process $\\{Y_{n}\\}_{n\\geq 0}$ is adapted to the natural filtration $\\{\\mathcal{F}_{n}\\}$ of $\\{S_{n}\\}$ if, for each $n$, the random variable $Y_{n}$ is $\\mathcal{F}_{n}$-measurable, where $\\mathcal{F}_{n}=\\sigma(S_{0},\\dots,S_{n})$.\n\nFor option A, $Y_{n}=S_{n}^{2}-n$ is a measurable function of $S_{n}$ and the deterministic constant $n$, hence $Y_{n}$ is $\\mathcal{F}_{n}$-measurable.\n\nFor option B, $Y_{n}=\\mathbf{1}_{\\{S_{n}>a\\}}$ is the indicator of the event $\\{S_{n}>a\\}$, which lies in $\\sigma(S_{n})\\subseteq\\mathcal{F}_{n}$, so $Y_{n}$ is $\\mathcal{F}_{n}$-measurable.\n\nFor option C, $Y_{n}=S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable, and since $\\{\\mathcal{F}_{n}\\}$ is a filtration with $\\mathcal{F}_{n-1}\\subseteq\\mathcal{F}_{n}$, it follows that $Y_{n}$ is $\\mathcal{F}_{n}$-measurable.\n\nFor option D, $Y_{n}=S_{n}-S_{n+1}$. Using $S_{n+1}=S_{n}+X_{n+1}$, we have\n$$\nY_{n}=S_{n}-(S_{n}+X_{n+1})=-X_{n+1}.\n$$\nThe increment $X_{n+1}$ is independent of $\\mathcal{F}_{n}$ and not $\\mathcal{F}_{n}$-measurable, so $Y_{n}$ is not $\\mathcal{F}_{n}$-measurable; hence this process is not adapted.\n\nFor option E, $Y_{n}=\\max_{0\\leq k\\leq n}S_{k}$ is a measurable function of $(S_{0},\\dots,S_{n})$, thus $\\mathcal{F}_{n}$-measurable.\n\nTherefore, the only process that is not adapted is the one in option D.", "answer": "$$\\boxed{D}$$", "id": "1302339"}, {"introduction": "A filtration represents the information available to an observer, which may not be the complete history of the process. This exercise [@problem_id:1302361] explores the consequences of information loss by constructing a filtration based only on the squared values of a random walk, effectively hiding its direction. Your task is to investigate whether you can still perfectly track the random walk's position given this partial information, providing a deeper insight into what \"measurability\" with respect to a filtration truly means.", "problem": "Let $\\{X_i\\}_{i \\ge 1}$ be a sequence of independent and identically distributed random variables with $P(X_i = 1) = P(X_i = -1) = 1/2$. A simple symmetric random walk $\\{S_n\\}_{n \\ge 0}$ is defined by $S_0 = 0$ and $S_n = \\sum_{i=1}^n X_i$ for $n \\ge 1$.\n\nConsider the filtration $\\{\\mathcal{G}_n\\}_{n \\ge 0}$ generated by the squares of the random walk's positions, where $\\mathcal{G}_0 = \\{\\emptyset, \\Omega\\}$ (the trivial sigma-algebra) and for $n \\ge 1$, $\\mathcal{G}_n = \\sigma(S_1^2, S_2^2, \\dots, S_n^2)$. This is the smallest sigma-algebra with respect to which the random variables $S_1^2, S_2^2, \\dots, S_n^2$ are all measurable.\n\nIs the random walk process $\\{S_n\\}_{n \\ge 0}$ adapted to the filtration $\\{\\mathcal{G}_n\\}_{n \\ge 0}$?\n\nA. Yes, the process is adapted.\n\nB. No, the process is not adapted.\n\nC. The process is adapted if and only if $n$ is an even number.\n\nD. The process is adapted if and only if $n$ is an odd number.\n\nE. Whether the process is adapted or not depends on the specific path of the random walk.", "solution": "We recall the definition: a process $\\{S_{n}\\}_{n \\ge 0}$ is adapted to a filtration $\\{\\mathcal{G}_{n}\\}_{n \\ge 0}$ if for every $n \\ge 0$, the random variable $S_{n}$ is $\\mathcal{G}_{n}$-measurable.\n\nBy construction, $S_{0}=0$ is $\\mathcal{G}_{0}$-measurable since $\\mathcal{G}_{0}$ is the trivial sigma-algebra and $S_{0}$ is constant. We now examine $n=1$.\n\nFor $n=1$, $S_{1}=X_{1}$ with $P(S_{1}=1)=P(S_{1}=-1)=\\frac{1}{2}$. Then\n$$\nS_{1}^{2}=1 \\quad \\text{almost surely},\n$$\nso\n$$\n\\mathcal{G}_{1}=\\sigma(S_{1}^{2})=\\{\\emptyset,\\Omega\\},\n$$\nthe trivial sigma-algebra. A random variable that is $\\mathcal{G}_{1}$-measurable must be almost surely constant. However, $S_{1}$ is not almost surely constant because it takes two distinct values $1$ and $-1$ with positive probability $\\frac{1}{2}$ each. Equivalently, the event $\\{S_{1}=1\\}$ would have to belong to $\\mathcal{G}_{1}$ if $S_{1}$ were $\\mathcal{G}_{1}$-measurable, but $\\{S_{1}=1\\}$ has probability $\\frac{1}{2}$ and thus cannot be an element of the trivial sigma-algebra.\n\nTherefore, $S_{1}$ is not $\\mathcal{G}_{1}$-measurable, and hence the process $\\{S_{n}\\}_{n \\ge 0}$ is not adapted to $\\{\\mathcal{G}_{n}\\}_{n \\ge 0}$. This already settles the question; a failure at any single time index implies the process is not adapted. Consequently, the correct choice is that the process is not adapted.\n\nFor intuition, the filtration generated by the squares $\\{S_{k}^{2}\\}$ discards the sign information, so in general $S_{n}$ cannot be reconstructed as a function of $(S_{1}^{2},\\dots,S_{n}^{2})$, except in degenerate cases such as $S_{n}=0$. This sign loss prevents measurability of $S_{n}$ with respect to $\\mathcal{G}_{n}$.", "answer": "$$\\boxed{B}$$", "id": "1302361"}, {"introduction": "Filtrations are not just abstract definitions; they are the foundation for prediction and estimation in stochastic modeling. This final practice [@problem_id:1302344] puts this idea into action by asking for the best possible estimate of a random walk's position at an unobserved time, given a filtration that only includes observations at even time steps. By calculating a conditional expectation, you will see how this tool provides the optimal prediction based on the specific information you are given.", "problem": "Consider a simple symmetric random walk on the integers, starting at the origin. The position of the walk at time $n$ is given by $S_n = \\sum_{i=1}^n X_i$, where $n$ is a positive integer and $S_0 = 0$. The steps $X_i$ are independent and identically distributed random variables with the probability distribution $P(X_i = 1) = P(X_i = -1) = \\frac{1}{2}$.\n\nLet us define a filtration $\\{\\mathcal{G}_k\\}_{k \\ge 1}$ by observing the position of the random walk only at even time steps. Specifically, for any positive integer $k$, the sigma-algebra $\\mathcal{G}_k$ is the one generated by the random variables $S_2, S_4, \\ldots, S_{2k}$. That is, $\\mathcal{G}_k = \\sigma(S_2, S_4, \\ldots, S_{2k})$.\n\nYour task is to determine the best estimate of the walk's position at time $n=3$, given the information available in the filtration up to time $k=2$. Compute the conditional expectation $E[S_3 | \\mathcal{G}_2]$ and express your answer as an analytic expression in terms of the observable random variables.", "solution": "The problem asks for the computation of the conditional expectation $E[S_3 | \\mathcal{G}_2]$.\n\nFirst, we identify the conditioning sigma-algebra. From the problem definition, $\\mathcal{G}_k = \\sigma(S_2, S_4, \\ldots, S_{2k})$. For $k=2$, this gives $\\mathcal{G}_2 = \\sigma(S_2, S_4)$. Therefore, we need to compute $E[S_3 | S_2, S_4]$.\n\nWe can express the random variable $S_3$ in terms of an earlier state and the subsequent step. A convenient choice is to relate it to $S_2$:\n$S_3 = S_2 + X_3$\n\nUsing the linearity property of conditional expectation, we have:\n$E[S_3 | S_2, S_4] = E[S_2 + X_3 | S_2, S_4] = E[S_2 | S_2, S_4] + E[X_3 | S_2, S_4]$\n\nLet's evaluate each term separately.\n\nFor the first term, $E[S_2 | S_2, S_4]$, the random variable $S_2$ is, by definition, measurable with respect to the sigma-algebra generated by $S_2$ and $S_4$. A random variable that is measurable with respect to the conditioning sigma-algebra is its own conditional expectation. Thus:\n$E[S_2 | S_2, S_4] = S_2$\n\nFor the second term, we need to compute $E[X_3 | S_2, S_4]$. We can relate $S_4$ to the random variables we are interested in:\n$S_4 = S_2 + X_3 + X_4$\nThis means that conditioning on the pair $(S_2, S_4)$ is equivalent to conditioning on the pair $(S_2, S_2 + X_3 + X_4)$, which is in turn equivalent to conditioning on the pair $(S_2, X_3 + X_4)$. So we have:\n$E[X_3 | S_2, S_4] = E[X_3 | S_2, X_3 + X_4]$\n\nThe random walk steps $X_i$ are independent. Therefore, $X_3$ is independent of $S_2 = X_1 + X_2$. The random variable $X_3$ is also independent of the sigma-algebra generated by $S_2$. Because of this independence, conditioning on $S_2$ provides no information about $X_3$. We can write:\n$E[X_3 | S_2, X_3 + X_4] = E[X_3 | X_3 + X_4]$\n\nNow we compute $E[X_3 | X_3 + X_4]$. The random variables $X_3$ and $X_4$ are independent and identically distributed. By symmetry, their conditional expectations given their sum must be equal:\n$E[X_3 | X_3 + X_4] = E[X_4 | X_3 + X_4]$\n\nLet's sum these two expectations:\n$E[X_3 | X_3 + X_4] + E[X_4 | X_3 + X_4] = E[X_3 + X_4 | X_3 + X_4]$\nThe conditional expectation of a random variable given itself is just the random variable. So:\n$E[X_3 + X_4 | X_3 + X_4] = X_3 + X_4$\n\nCombining the above, we get:\n$2 E[X_3 | X_3 + X_4] = X_3 + X_4$\n$E[X_3 | X_3 + X_4] = \\frac{X_3 + X_4}{2}$\n\nSo, we have found that $E[X_3 | S_2, S_4] = \\frac{X_3 + X_4}{2}$. The final answer must be expressed in terms of the conditioning variables $S_2$ and $S_4$. We can express the sum $X_3+X_4$ using the definition of the random walk:\n$S_4 = S_2 + X_3 + X_4 \\implies X_3 + X_4 = S_4 - S_2$\n\nSubstituting this back gives:\n$E[X_3 | S_2, S_4] = \\frac{S_4 - S_2}{2}$\n\nFinally, we combine the two parts of our original expectation:\n$E[S_3 | S_2, S_4] = E[S_2 | S_2, S_4] + E[X_3 | S_2, S_4] = S_2 + \\frac{S_4 - S_2}{2}$\n\nSimplifying this expression:\n$E[S_3 | S_2, S_4] = \\frac{2S_2 + S_4 - S_2}{2} = \\frac{S_2 + S_4}{2}$\n\nThis is the best estimate for $S_3$ given the values of $S_2$ and $S_4$.", "answer": "$$\\boxed{\\frac{S_2 + S_4}{2}}$$", "id": "1302344"}]}