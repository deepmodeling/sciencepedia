{"hands_on_practices": [{"introduction": "Understanding a probability distribution begins with its fundamental properties. This first practice focuses on the direct relationship between a chi-square distribution's degrees of freedom, its mean, and its variance. Mastering this connection, specifically that for a random variable $X \\sim \\chi^2(k)$, the mean is $E[X] = k$ and the variance is $\\text{Var}(X) = 2k$, is a crucial first step for any further analysis involving this important statistical tool. [@problem_id:2324]", "problem": "A random variable $X$ is known to follow a chi-squared distribution, denoted as $X \\sim \\chi^2(k)$, where $k$ is the number of degrees of freedom. The chi-squared distribution is a continuous probability distribution widely used in statistical inference.\n\nThe fundamental properties of a chi-squared distribution with $k$ degrees of freedom are its mean (expected value) and variance:\n- Mean: $E[X] = k$\n- Variance: $\\text{Var}(X) = 2k$\n\nGiven that a specific chi-squared random variable $X$ has a mean value of $\\mu = 6$, derive the variance of $X$.", "solution": "For $X\\sim\\chi^2(k)$ we have the standard results  \n$$E[X]=k,\\qquad \\mathrm{Var}(X)=2k.$$  \nWe are given $E[X]=\\mu=6$, hence  \n$$k=6.$$  \nSubstituting into the variance formula gives  \n$$\\mathrm{Var}(X)=2k=2\\cdot6=12.$$", "answer": "$$\\boxed{12}$$", "id": "2324"}, {"introduction": "The chi-square distribution is not just an abstract mathematical function; it naturally emerges from summing the squares of independent standard normal variables. This exercise demonstrates this fundamental origin within a hypothetical materials science experiment, challenging you to apply your knowledge to determine a critical threshold for a system metric. This is a core skill in statistical quality control and hypothesis testing, connecting the theoretical definition to a practical application. [@problem_id:1288599]", "problem": "In a materials science experiment designed to study thermal fluctuations, a device measures microscopic displacements in a crystal lattice. The measurement system consists of 10 independent sensors. The displacement measurement from each sensor, $X_i$ for $i=1, 2, ..., 10$, after being properly calibrated and normalized, is accurately modeled as an independent random variable from a standard normal distribution, $N(0, 1)$.\n\nTo quantify the total thermal agitation within the observed region, an aggregate \"agitation metric\" $A$ is computed. This metric is defined as the sum of the squares of the individual normalized displacement measurements:\n$$\nA = \\sum_{i=1}^{10} X_i^2\n$$\nA critical system state is flagged if this agitation metric $A$ surpasses a certain threshold, $a_{crit}$. This threshold is carefully selected so that the probability of it being exceeded due to random thermal fluctuations alone is precisely 0.05.\n\nYou are provided with the following data points for the probability distribution that governs the sum of squares of 10 independent standard normal variables:\n- The value that is exceeded with a probability of 0.950 is 3.940.\n- The value that is exceeded with a probability of 0.050 is 18.307.\n- The value that is exceeded with a probability of 0.010 is 23.209.\n\nDetermine the value of the threshold $a_{crit}$.", "solution": "Each sensor reading $X_{i}$ is modeled as independent $N(0,1)$. A fundamental result in probability states that the sum of squares of $k$ independent standard normal variables follows a chi-square distribution with $k$ degrees of freedom. Therefore,\n$$\nA=\\sum_{i=1}^{10}X_{i}^{2}\\sim \\chi^{2}(10).\n$$\nThe critical threshold $a_{crit}$ is defined by the requirement that the probability of exceeding it due to random fluctuations is $0.05$, i.e.,\n$$\n\\mathbb{P}(A>a_{crit})=0.05.\n$$\nFor a continuous distribution, this is equivalent to $a_{crit}$ being the upper-tail critical value (the $0.95$ quantile) of the $\\chi^{2}(10)$ distribution. From the provided tabulated values for the distribution governing $A$, the value that is exceeded with probability $0.050$ is $18.307$. Hence,\n$$\na_{crit}=18.307.\n$$", "answer": "$$\\boxed{18.307}$$", "id": "1288599"}, {"introduction": "Beyond its basic definition, the chi-square distribution possesses elegant mathematical properties that are key to its role in multivariate statistics. This problem explores the effect of a specific linear transformation—a rotation—on a set of standard normal variables. By working through this exercise, you will uncover the remarkable fact that the sum of squares is invariant under such transformations, reinforcing the core definition of the $\\chi^2(2)$ distribution from a different perspective and offering a deeper insight into its geometric nature. [@problem_id:1288558]", "problem": "Let $X_1$ and $X_2$ be two independent random variables, each following a standard normal distribution, denoted as $N(0, 1)$. Consider two new random variables, $Y_1$ and $Y_2$, which are linear transformations of $X_1$ and $X_2$ defined as follows:\n\n$$Y_1 = \\frac{1}{\\sqrt{2}} (X_1 + X_2)$$\n$$Y_2 = \\frac{1}{\\sqrt{2}} (X_1 - X_2)$$\n\nNow, let a third random variable, $Z$, be defined as the sum of the squares of $Y_1$ and $Y_2$:\n\n$$Z = Y_1^2 + Y_2^2$$\n\nWhich of the following describes the probability distribution of the random variable $Z$?\n\nA. A standard normal distribution, $N(0, 1)$.\n\nB. A chi-square distribution with 1 degree of freedom, $\\chi^2(1)$.\n\nC. A chi-square distribution with 2 degrees of freedom, $\\chi^2(2)$.\n\nD. An F-distribution with $(1, 1)$ degrees of freedom.\n\nE. A Student's t-distribution with 2 degrees of freedom.", "solution": "Let $X=(X_{1},X_{2})^{\\top}$ with $X_{1}$ and $X_{2}$ independent and $X_{i}\\sim N(0,1)$. Then $X\\sim N_{2}(0,I_{2})$, where $I_{2}$ is the $2\\times 2$ identity matrix.\n\nDefine the linear transformation $Y=AX$ with\n$$\nA=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix},\\quad Y=\\begin{pmatrix}Y_{1}\\\\ Y_{2}\\end{pmatrix}.\n$$\nCompute $AA^{\\top}$:\n$$\nAA^{\\top}=\\frac{1}{2}\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix}\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix}^{\\top}\n=\\frac{1}{2}\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix}\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix}\n=\\frac{1}{2}\\begin{pmatrix}2 & 0\\\\ 0 & 2\\end{pmatrix}\n=I_{2}.\n$$\nThus $A$ is orthogonal. Since $X\\sim N_{2}(0,I_{2})$, it follows that $Y\\sim N_{2}(0,AI_{2}A^{\\top})=N_{2}(0,I_{2})$. Therefore $Y_{1}$ and $Y_{2}$ are independent and each has distribution $N(0,1)$.\n\nNow compute $Z$:\n$$\nZ=Y_{1}^{2}+Y_{2}^{2}=\\frac{1}{2}\\left((X_{1}+X_{2})^{2}+(X_{1}-X_{2})^{2}\\right)\n=\\frac{1}{2}\\left(2X_{1}^{2}+2X_{2}^{2}\\right)=X_{1}^{2}+X_{2}^{2}.\n$$\nSince $X_{1}$ and $X_{2}$ are independent $N(0,1)$, by the definition of the chi-square distribution, the sum of squares of two independent standard normal variables has a chi-square distribution with $2$ degrees of freedom. Hence $Z\\sim \\chi^{2}(2)$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1288558"}]}