{"hands_on_practices": [{"introduction": "The Beta distribution is a powerful tool for modeling quantities bounded between 0 and 1. A frequent challenge in practice is determining the appropriate shape parameters, $\\alpha$ and $\\beta$, based on prior knowledge or data. This exercise provides a concrete walkthrough of this process, showing how to derive the specific Beta parameters from more intuitive statistical properties like the expected value (mean) and variance [@problem_id:1900165]. Mastering this skill is essential for applying the Beta distribution to real-world scenarios, such as modeling market penetration or success rates.", "problem": "The market share of a new software product one year after its launch is a value that can range from 0 (no market share) to 1 (complete market dominance). A market analyst models the uncertainty in this market share, represented by the random variable $X$, using a Beta distribution with shape parameters $\\alpha > 0$ and $\\beta > 0$.\n\nBased on preliminary analysis and data from similar product launches in the past, the expected market share is estimated to be $0.6$. The variance associated with this estimate is $0.024$.\n\nDetermine the exact numerical values of the parameters $\\alpha$ and $\\beta$ that define this particular Beta distribution. Present your answer as an ordered pair $(\\alpha, \\beta)$.", "solution": "For a Beta distribution with parameters $\\alpha>0$ and $\\beta>0$, the mean and variance are\n$$\n\\mathbb{E}[X]=\\frac{\\alpha}{\\alpha+\\beta}, \\quad \\operatorname{Var}(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}.\n$$\nGiven $\\mathbb{E}[X]=0.6=\\frac{3}{5}$ and $\\operatorname{Var}(X)=0.024=\\frac{3}{125}$, let $S=\\alpha+\\beta$. Then\n$$\n\\alpha=\\frac{3}{5}S, \\quad \\beta=\\frac{2}{5}S.\n$$\nSubstitute into the variance formula:\n$$\n\\frac{\\alpha\\beta}{S^{2}(S+1)}=\\frac{\\left(\\frac{3}{5}S\\right)\\left(\\frac{2}{5}S\\right)}{S^{2}(S+1)}=\\frac{6}{25(S+1)}=\\frac{3}{125}.\n$$\nSolve for $S$:\n$$\n\\frac{6}{25(S+1)}=\\frac{3}{125}\\;\\Rightarrow\\;6\\cdot 125=3\\cdot 25\\,(S+1)\\;\\Rightarrow\\;750=75(S+1)\\;\\Rightarrow\\;S+1=10\\;\\Rightarrow\\;S=9.\n$$\nTherefore,\n$$\n\\alpha=\\frac{3}{5}\\cdot 9=\\frac{27}{5}, \\quad \\beta=\\frac{2}{5}\\cdot 9=\\frac{18}{5}.\n$$", "answer": "$$\\boxed{\\left(\\frac{27}{5}, \\frac{18}{5}\\right)}$$", "id": "1900165"}, {"introduction": "Once we have defined a Beta distribution, understanding the role of its parameters is crucial for interpreting the model. The parameters $\\alpha$ and $\\beta$ don't just determine the mean; they also dictate the shape and spread of the distribution. This practice explores the relationship between the parameter values and the variance, giving you a tangible sense of how they quantify uncertainty [@problem_id:1900194]. By comparing two distributions, you will see how higher parameter values lead to a more concentrated distribution, reflecting greater confidence in the estimate.", "problem": "A random variable $X$ is said to follow a Beta distribution with positive shape parameters $\\alpha$ and $\\beta$, denoted as $X \\sim \\text{Beta}(\\alpha, \\beta)$, if its Probability Density Function (PDF) is defined on the interval $(0, 1)$. The variance of this distribution is given by the formula:\n$$\n\\text{Var}(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\n$$\nVariance is a measure of the spread or dispersion of a probability distribution. A smaller variance indicates that the probability mass is more tightly concentrated around the mean of the distribution.\n\nConsider two independent random variables, $X_1 \\sim \\text{Beta}(1, 1)$ and $X_2 \\sim \\text{Beta}(10, 10)$. Let $V_1$ be the variance of $X_1$ and $V_2$ be the variance of $X_2$.\n\nWhich of the following statements correctly compares their variances and accurately describes the implication of this comparison for the concentration of their respective probability masses?\n\nA. $V_1 < V_2$, which implies the probability mass of the Beta(10,10) distribution is more concentrated around its mean than that of the Beta(1,1) distribution.\n\nB. $V_1 > V_2$, which implies the probability mass of the Beta(10,10) distribution is more concentrated around its mean than that of the Beta(1,1) distribution.\n\nC. $V_1 < V_2$, which implies the probability mass of the Beta(1,1) distribution is more concentrated around its mean than that of the Beta(10,10) distribution.\n\nD. $V_1 > V_2$, which implies the probability mass of the Beta(1,1) distribution is more concentrated around its mean than that of the Beta(10,10) distribution.\n\nE. $V_1 = V_2$, which implies both distributions have the same concentration of probability mass around their respective means.", "solution": "The problem asks us to compare the variances of two Beta distributions, $X_1 \\sim \\text{Beta}(1, 1)$ and $X_2 \\sim \\text{Beta}(10, 10)$, and interpret the result in terms of the concentration of probability mass. We are given the general formula for the variance of a Beta($\\alpha, \\beta$) distribution:\n$$\n\\text{Var}(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\n$$\n\nFirst, we calculate the variance $V_1$ for the random variable $X_1 \\sim \\text{Beta}(1, 1)$. For this distribution, the parameters are $\\alpha = 1$ and $\\beta = 1$. Substituting these values into the variance formula:\n$$\nV_1 = \\text{Var}(X_1) = \\frac{(1)(1)}{(1 + 1)^2 (1 + 1 + 1)} = \\frac{1}{(2)^2 (3)} = \\frac{1}{4 \\cdot 3} = \\frac{1}{12}\n$$\nIt is worth noting that a Beta(1, 1) distribution is a special case, corresponding to the continuous Uniform distribution on the interval $(0, 1)$, denoted as $U(0, 1)$.\n\nNext, we calculate the variance $V_2$ for the random variable $X_2 \\sim \\text{Beta}(10, 10)$. For this distribution, the parameters are $\\alpha = 10$ and $\\beta = 10$. Substituting these values into the variance formula:\n$$\nV_2 = \\text{Var}(X_2) = \\frac{(10)(10)}{(10 + 10)^2 (10 + 10 + 1)} = \\frac{100}{(20)^2 (21)} = \\frac{100}{400 \\cdot 21} = \\frac{1}{4 \\cdot 21} = \\frac{1}{84}\n$$\n\nNow, we compare the two variances, $V_1$ and $V_2$. We have $V_1 = \\frac{1}{12}$ and $V_2 = \\frac{1}{84}$. Since the numerators are the same, the fraction with the smaller denominator is larger. Because $12 < 84$, it follows that:\n$$\n\\frac{1}{12} > \\frac{1}{84}\n$$\nTherefore, $V_1 > V_2$.\n\nThe problem statement mentions that a smaller variance indicates a higher concentration of probability mass around the mean. Since $V_2 < V_1$, the probability mass of the Beta(10, 10) distribution is more concentrated around its mean than that of the Beta(1, 1) distribution.\n\nLet's check this conclusion against the given options.\n- A. $V_1 < V_2$, ... (Incorrect inequality)\n- B. $V_1 > V_2$, which implies the probability mass of the Beta(10,10) distribution is more concentrated around its mean than that of the Beta(1,1) distribution. (Correct inequality and correct implication)\n- C. $V_1 < V_2$, ... (Incorrect inequality)\n- D. $V_1 > V_2$, which implies the probability mass of the Beta(1,1) distribution is more concentrated around its mean than that of the Beta(10,10) distribution. (Correct inequality, but incorrect implication)\n- E. $V_1 = V_2$, ... (Incorrect inequality)\n\nThe correct statement is given in option B.", "answer": "$$\\boxed{B}$$", "id": "1900194"}, {"introduction": "One of the most elegant applications of the Beta distribution lies in Bayesian inference, where it serves as a conjugate prior for the parameter of a Binomial or Bernoulli distribution. This means that if we model our initial belief about a probability (like a coin's bias) with a Beta distribution, observing new data will update our belief into a new Beta distribution. This exercise [@problem_id:876] guides you through a complete Bayesian update, demonstrating how to combine a prior belief with observed data to arrive at a posterior belief, a cornerstone of modern data analysis.", "problem": "In Bayesian statistics, we often model an unknown probability parameter, $p$, using a probability distribution. Let the prior belief about the parameter $p$, which represents the probability of success in a single trial, be described by a Beta distribution. The probability density function (PDF) of the Beta distribution, denoted as $\\text{Beta}(p | \\alpha, \\beta)$, is given by:\n$$\nf(p | \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1}\n$$\nfor $0 < p < 1$, where $\\alpha > 0$ and $\\beta > 0$ are the shape parameters, and $\\Gamma(z)$ is the gamma function. For the purpose of finding the shape of the posterior distribution, we can work with the kernel of the distribution, which is the part of the PDF that depends on the parameter $p$:\n$$\nf(p | \\alpha, \\beta) \\propto p^{\\alpha-1} (1-p)^{\\beta-1}\n$$\n\nSuppose our prior distribution for $p$ is a $\\text{Beta}(\\alpha, \\beta)$ with parameters $\\alpha=2$ and $\\beta=2$.\n\nWe then perform an experiment consisting of $n=10$ independent Bernoulli trials and observe $k=7$ successes. The probability of observing $k$ successes in $n$ trials for a given $p$ is described by the Binomial distribution, which serves as our likelihood function:\n$$\nP(D | p) = \\binom{n}{k} p^k (1-p)^{n-k}\n$$\nwhere $D$ represents the observed data (i.e., $k$ successes in $n$ trials).\n\nAccording to Bayes' theorem, the posterior distribution of $p$ given the data $D$ is proportional to the product of the likelihood and the prior:\n$$\n\\text{Posterior}(p | D) \\propto \\text{Likelihood}(D | p) \\times \\text{Prior}(p)\n$$\n\nBy finding the kernel of the posterior distribution, you will find that it is also a Beta distribution, with updated parameters $\\alpha'$ and $\\beta'$. The expected value (mean) of a random variable $p$ following a $\\text{Beta}(\\alpha', \\beta')$ distribution is given by $E[p] = \\frac{\\alpha'}{\\alpha'+\\beta'}$.\n\nDerive the posterior mean of the probability parameter $p$.", "solution": "We have prior kernel\n$$\nf(p|\\alpha,\\beta)\\propto p^{\\alpha-1}(1-p)^{\\beta-1}\\,,\n$$\nand likelihood\n$$\nP(D|p)\\propto p^k(1-p)^{n-k}\\,.\n$$\nBy Bayes’ theorem, the posterior kernel is\n$$\nf(p|D)\\propto p^{\\alpha-1}(1-p)^{\\beta-1}\\;p^k(1-p)^{n-k}\n= p^{\\alpha-1+k}(1-p)^{\\beta-1+n-k}\\,.\n$$\nThus the posterior is $\\mathrm{Beta}(\\alpha',\\beta')$ with\n$$\n\\alpha'=\\alpha+k,\\quad \\beta'=\\beta+n-k.\n$$\nSubstituting $\\alpha=2,\\;\\beta=2,\\;k=7,\\;n=10$ gives\n$$\n\\alpha'=2+7=9,\\quad \\beta'=2+(10-7)=5,\n$$\nso the posterior mean is\n$$\nE[p|D]=\\frac{\\alpha'}{\\alpha'+\\beta'}=\\frac{9}{9+5}=\\frac{9}{14}\\,.\n$$", "answer": "$$\\boxed{\\frac{9}{14}}$$", "id": "876"}]}