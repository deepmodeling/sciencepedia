{"hands_on_practices": [{"introduction": "Understanding the fundamental properties of a distribution is the first step toward mastering it. This exercise [@problem_id:1392758] challenges you to work backward from the variance of a Bernoulli trial, a measure of its unpredictability, to determine the underlying probability of success, $p$. This practice is key for developing skills in statistical inference and highlights an interesting symmetry in the nature of Bernoulli variance.", "problem": "A data analytics firm is studying consumer behavior for a new online subscription service. The action of a single, randomly selected consumer either purchasing the subscription or not is modeled as a discrete random event. A random variable $X$ is defined to represent this outcome: $X=1$ if the consumer makes a purchase, and $X=0$ if they do not. After analyzing a large sample of consumers, the firm determines that the variance of this random variable, $\\mathrm{Var}(X)$, is $0.21$.\n\nLet $p$ represent the probability that a consumer makes a purchase. Based on the given variance, determine the two possible values for $p$. Present your two answers as decimal numbers in ascending order.", "solution": "Let $X$ be a Bernoulli random variable with success probability $p$. The variance of a Bernoulli random variable is given by\n$$\n\\mathrm{Var}(X)=p(1-p).\n$$\nWe are given $\\mathrm{Var}(X)=0.21$, so\n$$\np(1-p)=0.21.\n$$\nRewriting,\n$$\np-p^{2}=0.21 \\;\\;\\Longrightarrow\\;\\; p^{2}-p+0.21=0.\n$$\nSolving the quadratic equation using the quadratic formula,\n$$\np=\\frac{1\\pm \\sqrt{1-4\\cdot 0.21}}{2}=\\frac{1\\pm \\sqrt{0.16}}{2}=\\frac{1\\pm 0.4}{2}.\n$$\nThus the two solutions are\n$$\np=\\frac{1-0.4}{2}=0.3 \\quad \\text{and} \\quad p=\\frac{1+0.4}{2}=0.7,\n$$\nboth of which lie in the interval $[0,1]$. In ascending order, these are $0.3$ and $0.7$.", "answer": "$$\\boxed{\\begin{pmatrix}0.3 & 0.7\\end{pmatrix}}$$", "id": "1392758"}, {"introduction": "Real-world systems often involve multiple independent events, and the Bernoulli process is the model for the simplest of these. This practice [@problem_id:1392801] extends our focus from a single Bernoulli trial to a pair, exploring the probability distribution of their sum. By analyzing the outcomes of two independent trials, you will build the foundational understanding needed to transition to the more general Binomial distribution.", "problem": "A factory manufactures a specific type of microchip. Due to minor variations in the fabrication process, each microchip has a probability $p$ of being defective, independently of all other microchips. A quality control procedure involves randomly selecting two microchips from a large batch for testing.\n\nLet $X_1$ and $X_2$ be two independent and identically distributed (i.i.d.) Bernoulli random variables that model the state of the two selected microchips. For each microchip $i$ (where $i=1, 2$), let $X_i = 1$ if the microchip is defective and $X_i = 0$ if it is not.\n\nDetermine the probability that exactly one of the two selected microchips is defective. Express your answer as a function of $p$.", "solution": "Let $X_{1}$ and $X_{2}$ be i.i.d. Bernoulli random variables with parameter $p$, so that for each $i \\in \\{1,2\\}$, $\\Pr(X_{i}=1)=p$ and $\\Pr(X_{i}=0)=1-p$. The event that exactly one microchip is defective is the event $\\{X_{1}+X_{2}=1\\}$, which is the disjoint union of the two cases:\n$$\n\\{X_{1}=1, X_{2}=0\\} \\quad \\text{and} \\quad \\{X_{1}=0, X_{2}=1\\}.\n$$\nBy independence, $\\Pr(X_{1}=1, X_{2}=0)=\\Pr(X_{1}=1)\\Pr(X_{2}=0)=p(1-p)$ and $\\Pr(X_{1}=0, X_{2}=1)=\\Pr(X_{1}=0)\\Pr(X_{2}=1)=(1-p)p$. Summing these disjoint probabilities gives\n$$\n\\Pr(X_{1}+X_{2}=1)=p(1-p)+(1-p)p=2p(1-p).\n$$\nEquivalently, using the binomial distribution for $X_{1}+X_{2} \\sim \\text{Binomial}(2,p)$, the probability of exactly one success is\n$$\n\\binom{2}{1}p^{1}(1-p)^{1}=2p(1-p).\n$$", "answer": "$$\\boxed{2p(1-p)}$$", "id": "1392801"}, {"introduction": "How does knowing the collective outcome of a series of events change our expectation about a single one? This thought-provoking exercise [@problem_id:1392772] delves into the concept of conditional expectation, using an elegant argument based on symmetry to find a surprisingly simple answer. This problem illustrates a profound principle in statistics: when observing a set of identical and independent trials, knowing the total number of successes informs our expectation for any individual trial in a clear, intuitive way.", "problem": "In a simplified model for spontaneous neural activity, we consider a network of $n$ neurons over a brief time interval. The state of each neuron is represented by a random variable. Let $X_i$ for $i=1, 2, \\ldots, n$ be the random variable for the $i$-th neuron, where $X_i=1$ if the neuron fires and $X_i=0$ if it is quiescent. The neurons are assumed to behave independently and identically, with each neuron having a probability $p$ of firing, where $0 < p < 1$.\n\nAn experimental device can measure the total number of firing neurons in the network, but it cannot identify which specific neurons fired. Suppose a measurement is taken and it is found that exactly $k$ neurons have fired, where $k$ is an integer such that $0 \\le k \\le n$.\n\nGiven this measurement, what is the conditional expectation of the state of the first neuron, $X_1$? Express your answer as a function of $n$ and $k$.", "solution": "Let $S=\\sum_{i=1}^{n} X_{i}$ denote the total number of firing neurons. The measurement gives $S=k$. We seek $\\mathbb{E}[X_{1} \\mid S=k]$.\n\nFirst, apply linearity of expectation conditioned on $S=k$:\n$$\n\\mathbb{E}[S \\mid S=k]=\\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i} \\,\\middle|\\, S=k\\right]=\\sum_{i=1}^{n} \\mathbb{E}[X_{i} \\mid S=k].\n$$\nBy definition of conditioning on $S=k$, the left-hand side equals\n$$\n\\mathbb{E}[S \\mid S=k]=k.\n$$\nBecause the neurons are independent and identically distributed, the vector $(X_{1},\\ldots,X_{n})$ is exchangeable, so the conditional expectations are equal:\n$$\n\\mathbb{E}[X_{1} \\mid S=k]=\\mathbb{E}[X_{2} \\mid S=k]=\\cdots=\\mathbb{E}[X_{n} \\mid S=k].\n$$\nHence,\n$$\nk=\\sum_{i=1}^{n} \\mathbb{E}[X_{i} \\mid S=k]=n \\,\\mathbb{E}[X_{1} \\mid S=k],\n$$\nwhich implies\n$$\n\\mathbb{E}[X_{1} \\mid S=k]=\\frac{k}{n}.\n$$\nThis result does not depend on $p$ and is valid for all integers $k$ with $0 \\leq k \\leq n$.", "answer": "$$\\boxed{\\frac{k}{n}}$$", "id": "1392772"}]}