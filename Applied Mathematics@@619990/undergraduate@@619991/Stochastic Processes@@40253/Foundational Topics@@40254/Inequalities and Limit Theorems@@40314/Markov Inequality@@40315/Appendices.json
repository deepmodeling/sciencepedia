{"hands_on_practices": [{"introduction": "This first exercise establishes the fundamental use of Markov's inequality. It presents a common scenario where we only know the average of a non-negative quantity, such as system errors, but need to assess the risk of an extreme outcome. By applying the inequality directly, you will see how to calculate a guaranteed upper bound on this probability, demonstrating the power of this tool even with limited information [@problem_id:1316852].", "id": "1316852", "problem": "A company's internal server monitoring system tracks the number of critical errors logged per hour. Over a long-term observation period, it has been determined that the system logs an average of 4.8 critical errors per hour. Assume that the number of errors is a non-negative random variable. Without making any other assumptions about the probability distribution of the errors, determine the best possible upper bound for the probability that the server logs 20 or more critical errors in any given hour.\n\nExpress your answer as a decimal.\n\n", "solution": "Let $X$ denote the number of critical errors logged in an hour. We are given that $X \\ge 0$ and $\\mathbb{E}[X] = 4.8$.\n\nBy Markov's inequality, for any $a > 0$ and any non-negative random variable $X$,\n$$\n\\Pr(X \\ge a) \\le \\frac{\\mathbb{E}[X]}{a}.\n$$\nSetting $a = 20$ yields\n$$\n\\Pr(X \\ge 20) \\le \\frac{\\mathbb{E}[X]}{20} = \\frac{4.8}{20} = 0.24.\n$$\nThis bound is best possible under the given information. To see tightness, consider a distribution supported on $\\{0,20\\}$ with\n$$\n\\Pr(X=20) = \\frac{4.8}{20}, \\quad \\Pr(X=0) = 1 - \\frac{4.8}{20}.\n$$\nThen $\\mathbb{E}[X] = 20 \\cdot \\frac{4.8}{20} = 4.8$ and $\\Pr(X \\ge 20) = \\frac{4.8}{20} = 0.24$, which attains the bound.", "answer": "$$\\boxed{0.24}$$"}, {"introduction": "Markov's inequality provides an upper bound, but when is this bound achieved? This problem explores the concept of \"sharpness\" by reverse-engineering the inequality. By assuming that the probability of an event exactly equals its Markov bound, we can uncover surprising constraints on the underlying probability distribution, deepening our understanding of the conditions under which the inequality is most precise [@problem_id:1316841].", "id": "1316841", "problem": "A specialized electronic component's power consumption, denoted by the non-negative random variable $X$, can take one of three discrete values. It can be in an idle state with $0$ Watts, a nominal state with $A$ Watts, or a high-performance state with $B$ Watts. The parameters satisfy $0 < \\mu < A < B$, where $\\mu$ is the long-term average power consumption, i.e., $E[X] = \\mu$. A performance analysis of the component reveals that the probability of it consuming power at or above the nominal level, $P(X \\ge A)$, is exactly equal to the theoretical upper bound given by Markov's inequality. Based on this information, determine the probability that the component is in its high-performance state, $P(X = B)$.\n\n", "solution": "Let $p = P(X = B)$, $q = P(X = A)$, and $r = P(X = 0) = 1 - p - q$. Since $X$ is non-negative and takes values in $\\{0, A, B\\}$ with $0 < \\mu < A < B$, we have:\n$$\nE[X] = \\mu = A q + B p.\n$$\nMarkov's inequality for non-negative $X$ states that for any $t > 0$,\n$$\nP(X \\ge t) \\le \\frac{E[X]}{t}.\n$$\nTaking $t = A$ and using the given equality case,\n$$\nP(X \\ge A) = \\frac{\\mu}{A}.\n$$\nBut $P(X \\ge A) = P(X = A) + P(X = B) = q + p$, hence\n$$\nq + p = \\frac{\\mu}{A}.\n$$\nWe now solve the system\n$$\n\\begin{cases}\nq + p = \\frac{\\mu}{A}, \\\\\nA q + B p = \\mu.\n\\end{cases}\n$$\nFrom the first equation, $q = \\frac{\\mu}{A} - p$. Substitute into the second:\n$$\nA\\left(\\frac{\\mu}{A} - p\\right) + B p = \\mu \\;\\;\\Rightarrow\\;\\; \\mu - A p + B p = \\mu \\;\\;\\Rightarrow\\;\\; (B - A) p = 0.\n$$\nSince $B > A$, it follows that $p = 0$. Therefore,\n$$\nP(X = B) = 0.\n$$\nThis is also consistent with the equality condition in Markov's inequality: equality requires $X = A \\mathbf{1}_{\\{X \\ge A\\}}$ almost surely, which precludes any positive probability at values strictly greater than $A$.", "answer": "$$\\boxed{0}$$"}, {"introduction": "The true power of a foundational result like Markov's inequality lies in its versatility. This advanced practice challenges you to derive a new, more powerful inequality by applying Markov's principle to a clever transformation of a random variable that incorporates its variance, $\\sigma^2$. This technique leads to a result known as the Cantelli inequality and showcases a key method for creating tighter bounds in probability theory [@problem_id:1933078].", "id": "1933078", "problem": "Let $X$ be a random variable with a well-defined mean $E[X] = \\mu$ and a finite, non-zero variance $\\text{Var}(X) = \\sigma^2$. We want to find a tight upper bound for the tail probability $P(X \\ge a)$, where $a$ is a constant strictly greater than the mean, i.e., $a > \\mu$.\n\nA general method for deriving such bounds involves transforming the random variable. Consider the function $B(b) = \\frac{E[(X-b)^2]}{(a-b)^2}$, where $b$ is a real-valued parameter. This function provides an upper bound for $P(X \\ge a)$ for any choice of $b < a$.\n\nYour task is to find the tightest possible bound that can be obtained from this family of functions. To do this, you must find the minimum value of $B(b)$ by optimizing the choice of the parameter $b$ over its valid domain ($b<a$).\n\nExpress this optimal (minimum) value of $B(b)$ as a closed-form analytic expression in terms of $\\mu$, $\\sigma$, and $a$.\n\n", "solution": "We are given that for any $b<a$,\n$$\nP(X\\ge a)\\le B(b):=\\frac{E[(X-b)^{2}]}{(a-b)^{2}}.\n$$\nUsing $E[(X-b)^{2}]=\\operatorname{Var}(X)+\\left(E[X]-b\\right)^{2}$, we have\n$$\nE[(X-b)^{2}]=\\sigma^{2}+(\\mu-b)^{2},\n$$\nso the bound is\n$$\nB(b)=\\frac{\\sigma^{2}+(\\mu-b)^{2}}{(a-b)^{2}},\\quad b<a.\n$$\n\nTo minimize $B(b)$ over $b<a$, define\n$$\nf(b)=\\frac{\\sigma^{2}+(b-\\mu)^{2}}{(a-b)^{2}},\n$$\nwith numerator $N(b)=\\sigma^{2}+(b-\\mu)^{2}$ and denominator $D(b)=(a-b)^{2}$. Then\n$$\nf'(b)=\\frac{N'(b)D(b)-N(b)D'(b)}{D(b)^{2}},\n$$\nwith $N'(b)=2(b-\\mu)$ and $D'(b)=-2(a-b)$. Hence\n$$\nf'(b)=\\frac{2(b-\\mu)(a-b)^{2}+2\\left[\\sigma^{2}+(b-\\mu)^{2}\\right](a-b)}{(a-b)^{4}}\n=\\frac{2(a-b)\\left((b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}\\right)}{(a-b)^{4}}.\n$$\nFor $b<a$, we have $a-b>0$, so critical points satisfy\n$$\n(b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}=0.\n$$\nLet $t=b-\\mu$ and write $a-b=(a-\\mu)-t$. Then the equation becomes\n$$\nt\\big((a-\\mu)-t\\big)+\\sigma^{2}+t^{2}=t(a-\\mu)-t^{2}+\\sigma^{2}+t^{2}=t(a-\\mu)+\\sigma^{2}=0,\n$$\nso\n$$\nt^{*}=-\\frac{\\sigma^{2}}{a-\\mu},\\qquad b^{*}=\\mu+t^{*}=\\mu-\\frac{\\sigma^{2}}{a-\\mu}.\n$$\nSince $a>\\mu$, we have $a-\\mu>0$ and thus\n$$\nb^{*}<a \\;\\;\\Longleftrightarrow\\;\\; \\mu-\\frac{\\sigma^{2}}{a-\\mu}<a \\;\\;\\Longleftrightarrow\\;\\; -\\sigma^{2}<(a-\\mu)^{2},\n$$\nwhich is always true. Therefore $b^{*}$ is feasible. Moreover, $f(b)\\to+\\infty$ as $b\\to a^{-}$ and $f(b)\\to 1$ as $b\\to -\\infty$, and there is a unique critical point, so $b^{*}$ yields the global minimum.\n\nEvaluate $B(b)$ at $b=b^{*}$. Set $d=a-\\mu>0$. Then\n$$\n\\mu-b^{*}=\\frac{\\sigma^{2}}{d},\\qquad a-b^{*}=d+\\frac{\\sigma^{2}}{d}=\\frac{d^{2}+\\sigma^{2}}{d}.\n$$\nThus\n$$\nB(b^{*})=\\frac{\\sigma^{2}+(\\mu-b^{*})^{2}}{(a-b^{*})^{2}}\n=\\frac{\\sigma^{2}+\\left(\\frac{\\sigma^{2}}{d}\\right)^{2}}{\\left(\\frac{d^{2}+\\sigma^{2}}{d}\\right)^{2}}\n=\\frac{\\sigma^{2}\\left(1+\\frac{\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}\\left(\\frac{d^{2}+\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}}{d^{2}+\\sigma^{2}}\n=\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}.\n$$\n\nTherefore, the optimal (minimum) value of $B(b)$ over $b<a$ is $\\sigma^{2}/\\big((a-\\mu)^{2}+\\sigma^{2}\\big)$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}}$$"}]}