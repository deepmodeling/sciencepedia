{"hands_on_practices": [{"introduction": "When we say a sequence of random variables 'converges,' what do we actually mean? This practice explores two fundamental modes: convergence in probability and convergence in mean square. By analyzing a hypothetical sequence of energy surges, you will build concrete intuition for how these concepts differ and why one represents a stricter form of convergence than the other [@problem_id:1319233].", "problem": "An experimental data transmission protocol is being tested. On the $n$-th trial (for $n=1, 2, 3, \\dots$), a packet of data is transmitted. Due to noise in the channel, a surge of energy can corrupt the packet. The magnitude of this energy surge, if it occurs, is precisely $E_n = n^2$ joules. The probability of such a surge occurring on the $n$-th trial is $p_n = 1/n^3$. If no surge occurs, the energy level is 0 joules. Let the random variable $X_n$ represent the energy of the surge on the $n$-th trial.\n\nWe are interested in whether the sequence of energy surges $\\{X_n\\}$ eventually settles down to zero. Analyze the convergence of the sequence of random variables $\\{X_n\\}$ to the constant random variable $X=0$ in two different ways: convergence in probability and convergence in mean square.\n\nWhich of the following statements correctly describes the convergence properties of the sequence $\\{X_n\\}$?\n\nA. The sequence $\\{X_n\\}$ converges to 0 in probability and in mean square.\n\nB. The sequence $\\{X_n\\}$ converges to 0 in probability, but does not converge to 0 in mean square.\n\nC. The sequence $\\{X_n\\}$ does not converge to 0 in probability, but it converges to 0 in mean square.\n\nD. The sequence $\\{X_n\\}$ does not converge to 0 in probability and does not converge to 0 in mean square.", "solution": "Define the random variables by $P(X_{n}=n^{2})=p_{n}=\\frac{1}{n^{3}}$ and $P(X_{n}=0)=1-\\frac{1}{n^{3}}$. We analyze convergence to the constant random variable $X=0$.\n\nConvergence in probability: By definition, $X_{n}\\to 0$ in probability if for every $\\varepsilon>0$,\n$$\n\\lim_{n\\to\\infty}P\\big(|X_{n}-0|>\\varepsilon\\big)=0.\n$$\nSince $X_{n}\\in\\{0,n^{2}\\}$, for any fixed $\\varepsilon>0$ and all $n$ such that $n^{2}>\\varepsilon$, we have\n$$\n\\{|X_{n}|>\\varepsilon\\}=\\{X_{n}=n^{2}\\},\n$$\nhence\n$$\nP(|X_{n}|>\\varepsilon)=P(X_{n}=n^{2})=\\frac{1}{n^{3}}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty}P(|X_{n}|>\\varepsilon)=\\lim_{n\\to\\infty}\\frac{1}{n^{3}}=0,\n$$\nwhich proves $X_{n}\\to 0$ in probability.\n\nConvergence in mean square: By definition, $X_{n}\\to 0$ in mean square if\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=0.\n$$\nCompute\n$$\n\\mathbb{E}[X_{n}^{2}]=n^{4}\\cdot\\frac{1}{n^{3}}+0\\cdot\\Big(1-\\frac{1}{n^{3}}\\Big)=n.\n$$\nThus\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[X_{n}^{2}]=\\lim_{n\\to\\infty}n=+\\infty\\neq 0,\n$$\nso $X_{n}$ does not converge to $0$ in mean square.\n\nTherefore, the sequence converges to $0$ in probability but not in mean square, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1319233"}, {"introduction": "The Law of Large Numbers is a pillar of probability theory, assuring us that sample averages tend toward the true expected value. But what happens when that expected value is not well-defined? This exercise [@problem_id:1319185] investigates this question using the infamous Cauchy distribution, providing a classic counterexample that highlights the crucial assumptions behind our most trusted theorems.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random variables, where $n$ is a positive integer. Each random variable $X_i$ follows a standard Cauchy distribution, which is defined by the probability density function (PDF):\n$$f(x) = \\frac{1}{\\pi(1 + x^2)}, \\quad \\text{for } -\\infty  x  \\infty$$\nThe sample mean of these random variables is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nIdentify the distribution of the sample mean $\\bar{X}_n$.\n\nA. A Normal distribution with mean 0 and variance $\\frac{1}{n}$.\n\nB. A Normal distribution with mean 0 and variance 1.\n\nC. A Cauchy distribution with location parameter 0 and scale parameter $n$.\n\nD. A standard Cauchy distribution (location parameter 0, scale parameter 1).\n\nE. A Student's t-distribution with $n-1$ degrees of freedom.\n\nF. The distribution is undefined because the mean of the Cauchy distribution does not exist.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. standard Cauchy with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$. The characteristic function of a standard Cauchy random variable is\n$$\n\\phi_{X}(t)=\\int_{-\\infty}^{\\infty}\\exp(itx)\\frac{1}{\\pi(1+x^{2})}\\,dx=\\exp(-|t|).\n$$\nBy independence, the sum $S_{n}=\\sum_{i=1}^{n}X_{i}$ has characteristic function\n$$\n\\phi_{S_{n}}(t)=\\prod_{i=1}^{n}\\phi_{X}(t)=[\\exp(-|t|)]^{n}=\\exp(-n|t|).\n$$\nThis is the characteristic function of a Cauchy distribution with location $0$ and scale parameter $n$. For the sample mean $\\bar{X}_{n}=\\frac{S_{n}}{n}$, use the scaling property of characteristic functions: if $Y=aZ$, then $\\phi_{Y}(t)=\\phi_{Z}(at)$. Hence\n$$\n\\phi_{\\bar{X}_{n}}(t)=\\phi_{S_{n}}\\!\\left(\\frac{t}{n}\\right)=\\exp\\!\\left(-n\\left|\\frac{t}{n}\\right|\\right)=\\exp(-|t|),\n$$\nwhich is exactly the characteristic function of the standard Cauchy distribution (location $0$, scale $1$). Therefore, $\\bar{X}_{n}$ is standard Cauchy for every $n$. Although the mean of a Cauchy distribution does not exist, the distribution of the sample mean is well defined and equals the original standard Cauchy distribution, so option F is incorrect.\n\nThe correct choice is the standard Cauchy distribution, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1319185"}, {"introduction": "In statistics, we often construct estimators by combining different random quantities. This practice moves beyond single sequences to analyze the limiting behavior of a ratio of two distinct sample means. You will apply powerful tools like the Delta Method or Slutsky's Theorem to see how convergence in probability and convergence in distribution work in tandem to solve more complex, applied problems [@problem_id:798678].", "problem": "Let $\\{X_i\\}_{i=1}^n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a Normal distribution with mean $\\mu_X = 0$ and variance $\\sigma_X^2$.\nLet $\\{Y_i\\}_{i=1}^n$ be another sequence of i.i.d. random variables, independent of the $\\{X_i\\}$ sequence, drawn from an Exponential distribution with rate parameter $\\lambda$. The probability density function for the Exponential distribution is given by $f(y; \\lambda) = \\lambda e^{-\\lambda y}$ for $y \\ge 0$.\n\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ and $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i$ denote the respective sample means.\nConsider the statistic $T_n$ defined as:\n$$\nT_n = \\sqrt{n} \\frac{\\bar{X}_n}{\\bar{Y}_n}\n$$\nThe statistic $T_n$ converges in distribution to a random variable $T$ as $n \\to \\infty$. Determine the variance of this limiting distribution, $\\text{Var}(T)$.", "solution": "We apply the multivariate Delta method to $(\\bar X_n,\\bar Y_n)$.\n\n1. CLT for the sample means:\n$$\n\\sqrt{n}\\begin{pmatrix}\\bar X_n-0\\\\\\bar Y_n-\\tfrac1\\lambda\\end{pmatrix}\n\\Rightarrow N\\!\\Bigl(\\begin{pmatrix}0\\\\0\\end{pmatrix},\n\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}\\Bigr).\n$$\n\n2. Define $g(a,b)=a/b$.  Then\n$$\nT_n = \\sqrt{n}\\bigl(g(\\bar X_n,\\bar Y_n)-g(0,1/\\lambda)\\bigr),\n\\quad g(0,1/\\lambda)=0.\n$$\n\n3. Compute the gradient at $(0,1/\\lambda)$:\n$$\n\\nabla g(a,b)=\\Bigl(\\frac{\\partial}{\\partial a},\\frac{\\partial}{\\partial b}\\Bigr)\n=\\Bigl(\\tfrac1b,\\,-\\tfrac{a}{b^2}\\Bigr),\n\\quad\\nabla g(0,1/\\lambda)=(\\lambda,0).\n$$\n\n4. By the Delta method,\n$$\nT_n\\Rightarrow N\\bigl(0,\\;(\\nabla g)^T\\Sigma\\,(\\nabla g)\\bigr),\n\\quad\\Sigma=\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}.\n$$\n\n5. Hence\n$$\n\\mathrm{Var}(T)\n=(\\lambda,0)\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}\n\\begin{pmatrix}\\lambda\\\\0\\end{pmatrix}\n=\\lambda^2\\,\\sigma_X^2.\n$$", "answer": "$$\\boxed{\\lambda^2\\sigma_X^2}$$", "id": "798678"}]}