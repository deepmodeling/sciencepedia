{"hands_on_practices": [{"introduction": "The Cauchy-Schwarz inequality is far more than a static relationship; it is a dynamic tool for solving optimization problems. This first exercise demonstrates how to find the maximum value of a linear function on a sphere, a common task in fields from physics to economics. By reframing the problem in terms of vectors and their dot product, we can bypass more complex methods like Lagrange multipliers and arrive at an elegant, geometrically intuitive solution [@problem_id:945977].", "problem": "Find the maximum value of the linear form $ x_1 + 2x_2 + 3x_3 $ over real numbers $ x_1, x_2, x_3 $ subject to the constraint $ x_1^2 + x_2^2 + x_3^2 = 1 $.", "solution": "To maximize the linear form $ x_1 + 2x_2 + 3x_3 $ subject to the constraint $ x_1^2 + x_2^2 + x_3^2 = 1 $, we use the Cauchy-Schwarz inequality. \n\n### Step 1: Define vectors for the dot product\nLet $ \\mathbf{a} = (1, 2, 3) $ and $ \\mathbf{x} = (x_1, x_2, x_3) $. The linear form to maximize is the dot product $ \\mathbf{a} \\cdot \\mathbf{x} $, and the constraint is $ \\|\\mathbf{x}\\| = 1 $ (since $ \\|\\mathbf{x}\\|^2 = x_1^2 + x_2^2 + x_3^2 = 1 $).\n\n### Step 2: Apply the Cauchy-Schwarz inequality\nThe Cauchy-Schwarz inequality states $ |\\mathbf{a} \\cdot \\mathbf{x}| \\leq \\|\\mathbf{a}\\| \\|\\mathbf{x}\\| $. For the constraint $ \\|\\mathbf{x}\\| = 1 $, this simplifies to $ |\\mathbf{a} \\cdot \\mathbf{x}| \\leq \\|\\mathbf{a}\\| $. Equality holds when $ \\mathbf{x} $ is parallel to $ \\mathbf{a} $, so the maximum value of $ \\mathbf{a} \\cdot \\mathbf{x} $ is $ \\|\\mathbf{a}\\| $.\n\n### Step 3: Compute $ \\|\\mathbf{a}\\| $\nThe norm of $ \\mathbf{a} $ is:\n$$\n\\|\\mathbf{a}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n$$\n\nThus, the maximum value of the linear form is $ \\sqrt{14} $.", "answer": "$$\\boxed{\\sqrt{14}}$$", "id": "945977"}, {"introduction": "Let's now apply this powerful optimization principle to a scenario from digital signal processing. Abstract mathematical concepts often have direct, practical counterparts; here, the squared norm of a vector corresponds to signal 'energy', and the dot product represents 'cross-correlation'. This problem challenges you to determine the maximum possible correlation between two signals given their energies, providing a tangible link between vector algebra and real-world engineering constraints [@problem_id:1351138].", "problem": "In a digital signal processing application, a signal is represented as a vector of real numbers $s = (s_1, s_2, \\dots, s_n)$. The \"energy\" of a signal $s$ is defined as the sum of the squares of its components, $E_s = \\sum_{i=1}^n s_i^2$. The \"unnormalized cross-correlation\" between two signals $x = (x_1, x_2, \\dots, x_n)$ and $y = (y_1, y_2, \\dots, y_n)$ is given by $C_{xy} = \\sum_{i=1}^n x_i y_i$.\n\nAn engineer is analyzing two signals, a reference signal $r$ and a test signal $t$, of the same unknown length $n$. Through measurement, it is determined that the energy of the reference signal is $E_r = 81$ and the energy of the test signal is $E_t = 121$. What is the greatest possible value for the unnormalized cross-correlation, $C_{rt}$, between these two signals?", "solution": "We model the signals as vectors in $\\mathbb{R}^{n}$ with the standard inner product. The unnormalized cross-correlation between $r$ and $t$ is the inner product\n$$\nC_{rt}=\\sum_{i=1}^{n} r_{i} t_{i}.\n$$\nThe energy of a signal is the squared Euclidean norm:\n$$\nE_{r}=\\sum_{i=1}^{n} r_{i}^{2}=\\|r\\|^{2}, \\quad E_{t}=\\sum_{i=1}^{n} t_{i}^{2}=\\|t\\|^{2}.\n$$\nBy the Cauchy–Schwarz inequality,\n$$\n|C_{rt}|=\\left|\\sum_{i=1}^{n} r_{i} t_{i}\\right|\\leq \\|r\\|\\,\\|t\\|=\\sqrt{E_{r}}\\,\\sqrt{E_{t}}.\n$$\nTherefore, the greatest possible value of $C_{rt}$ is\n$$\n\\sqrt{E_{r}E_{t}}=\\sqrt{81\\cdot 121}=9\\cdot 11=99,\n$$\nwith equality achieved when $t$ is a nonnegative scalar multiple of $r$, for example $t=\\alpha r$ with $\\alpha=\\frac{11}{9}$.", "answer": "$$\\boxed{99}$$", "id": "1351138"}, {"introduction": "The power of the Cauchy-Schwarz inequality extends beyond the finite-dimensional vectors we have considered so far, into the realm of probability and stochastic processes. In this context, the inequality applies to random variables and their expectations, allowing us to find bounds on their statistical relationships. This practice explores this generalization by asking for an upper bound on a quantity involving a Poisson process, a cornerstone model for random events over time [@problem_id:1287492].", "problem": "A stochastic process that models the number of times a certain event occurs over time is known as a counting process. One of the most fundamental counting processes is the Poisson process, denoted by $\\{N(t) : t \\ge 0\\}$, which counts the cumulative number of events that have occurred up to time $t$. This process is characterized by a constant positive rate, $\\lambda$, which represents the average number of events per unit time. For a Poisson process, the random variable $N(t)$ follows a Poisson distribution with a mean of $\\lambda t$.\n\nGiven two distinct positive time instances, $t_1 > 0$ and $t_2 > 0$, we are interested in the statistical relationship between the number of counts at these two times. Determine a tight upper bound for the expected value of the geometric mean of the counts, i.e., find an upper bound for the quantity $E[\\sqrt{N(t_1) N(t_2)}]$. Your final answer should be a closed-form analytic expression in terms of $\\lambda$, $t_1$, and $t_2$.", "solution": "Let $N(t)$ be a Poisson process with rate $\\lambda>0$. For each $t>0$, the marginal distribution is $N(t)\\sim \\text{Poisson}(\\lambda t)$, so its mean is\n$$\n\\mathbb{E}[N(t)] = \\lambda t.\n$$\nWe seek an upper bound for $\\mathbb{E}\\big[\\sqrt{N(t_{1})N(t_{2})}\\big]$ when $t_{1}>0$ and $t_{2}>0$.\n\nUse the Cauchy–Schwarz inequality for square-integrable random variables $U$ and $V$:\n$$\n\\mathbb{E}[UV] \\le \\sqrt{\\mathbb{E}[U^{2}]\\,\\mathbb{E}[V^{2}]}.\n$$\nSet $U=\\sqrt{N(t_{1})}$ and $V=\\sqrt{N(t_{2})}$. Then\n$$\n\\mathbb{E}\\big[\\sqrt{N(t_{1})N(t_{2})}\\big]\n= \\mathbb{E}[UV]\n\\le \\sqrt{\\mathbb{E}[U^{2}]\\,\\mathbb{E}[V^{2}]}\n= \\sqrt{\\mathbb{E}[N(t_{1})]\\;\\mathbb{E}[N(t_{2})]}.\n$$\nUsing $\\mathbb{E}[N(t)]=\\lambda t$, we obtain\n$$\n\\mathbb{E}\\big[\\sqrt{N(t_{1})N(t_{2})}\\big] \\le \\sqrt{(\\lambda t_{1})(\\lambda t_{2})} = \\lambda \\sqrt{t_{1}t_{2}}.\n$$\n\nThis bound is tight in the sense that it cannot be improved using only the marginal means: equality is achieved when $t_{1}=t_{2}$, because then $N(t_{1})=N(t_{2})$ almost surely and\n$$\n\\mathbb{E}\\big[\\sqrt{N(t_{1})N(t_{2})}\\big] = \\mathbb{E}[N(t_{1})] = \\lambda t_{1} = \\lambda \\sqrt{t_{1}t_{2}}.\n$$\nFor $t_{1}\\neq t_{2}$, the inequality remains valid and is the sharpest general upper bound expressible solely in terms of $\\lambda$, $t_{1}$, and $t_{2}$.", "answer": "$$\\boxed{\\lambda \\sqrt{t_{1} t_{2}}}$$", "id": "1287492"}]}