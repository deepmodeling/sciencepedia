{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability theory, providing a bridge between theoretical expectation and long-term empirical averages. This first practice provides a direct application of this powerful theorem. By analyzing a sequence of independent and identically distributed random variables from a given probability distribution, you will calculate the precise value to which their sample mean converges, solidifying your understanding of the SLLN's fundamental promise. [@problem_id:1460774]", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2},  0\\le x\\le 1,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "After seeing the SLLN in action, a critical next step is to understand its limitations. The theorem's guarantees are not unconditional, and this exercise explores the crucial requirement of a finite mean ($E[|X|] \\lt \\infty$). You will investigate a scenario involving the Pareto distribution, often used to model \"heavy-tailed\" phenomena like insurance claims or financial market returns, and determine the conditions under which the sample average reliably converges. [@problem_id:1406772]", "problem": "An insurance company analyzes risk associated with catastrophic events. The financial size of a claim, denoted by a random variable $X$, is modeled using a Pareto distribution, which is suitable for phenomena with heavy tails. A sequence of claims $X_1, X_2, \\dots, X_n$ are assumed to be independent and identically distributed (i.i.d.).\n\nThe probability density function (PDF) for a single claim size $X$ is given by:\n$$f(x) = \\begin{cases} \\frac{\\alpha x_m^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x \\ge x_m \\\\ 0  \\text{for } x  x_m \\end{cases}$$\nHere, $\\alpha  0$ is the shape parameter that determines the heaviness of the tail, and $x_m  0$ is the minimum possible claim size (the scale parameter).\n\nFor the company's long-term financial models to be stable, it is essential that the sample mean of the claim sizes, $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, converges almost surely to a finite, non-zero constant as the number of claims $n$ approaches infinity.\n\nWhich of the following conditions on the shape parameter $\\alpha$ ensures this convergence?\n\nA. $\\alpha  0$\nB. $\\alpha  1$\nC. $\\alpha  2$\nD. $0  \\alpha \\le 1$\nE. $1  \\alpha \\le 2$", "solution": "We are given i.i.d. claim sizes $X_{1},X_{2},\\dots,X_{n}$ with common Pareto distribution having density\n$$\nf(x)=\\begin{cases}\n\\dfrac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x\\geq x_{m},\\\\\n0  \\text{for } xx_{m},\n\\end{cases}\n$$\nwhere $\\alpha0$ and $x_{m}0$. The sample mean is $\\bar{X}_{n}=\\dfrac{1}{n}\\sum_{i=1}^{n}X_{i}$. By the strong law of large numbers (SLLN), if the $X_{i}$ are i.i.d. and $\\mathbb{E}[|X|]\\infty$, then\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X].\n$$\nTherefore, for $\\bar{X}_{n}$ to converge almost surely to a finite, non-zero constant, it is necessary and sufficient that $\\mathbb{E}[X]$ exist, be finite, and be positive.\n\nWe compute $\\mathbb{E}[X]$ for the Pareto distribution:\n$$\n\\mathbb{E}[X]=\\int_{x_{m}}^{\\infty}x\\,f(x)\\,dx=\\int_{x_{m}}^{\\infty}x\\cdot\\frac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}\\,dx=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx.\n$$\nFor $\\alpha\\neq 1$, we evaluate the integral\n$$\n\\int x^{-\\alpha}\\,dx=\\frac{x^{1-\\alpha}}{1-\\alpha},\n$$\nso\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\lim_{b\\to\\infty}\\frac{b^{1-\\alpha}-x_{m}^{1-\\alpha}}{1-\\alpha}.\n$$\nThis converges if and only if $\\alpha1$. When $\\alpha1$, $1-\\alpha0$ and $b^{1-\\alpha}\\to 0$, giving\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\frac{0-x_{m}^{1-\\alpha}}{1-\\alpha}=\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}.\n$$\nHence, for $\\alpha1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\cdot\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}=\\frac{\\alpha x_{m}}{\\alpha-1},\n$$\nwhich is finite and strictly positive since $x_{m}0$.\n\nFor $\\alpha=1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-1}\\,dx=x_{m}\\int_{x_{m}}^{\\infty}\\frac{dx}{x},\n$$\nwhich diverges to $+\\infty$. For $0\\alpha1$, the integral $\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx$ diverges because $1-\\alpha0$ implies $b^{1-\\alpha}\\to\\infty$. Thus $\\mathbb{E}[X]=\\infty$ for $\\alpha\\leq 1$.\n\nTherefore, the SLLN yields almost sure convergence of $\\bar{X}_{n}$ to the finite, non-zero constant $\\mathbb{E}[X]=\\dfrac{\\alpha x_{m}}{\\alpha-1}$ if and only if $\\alpha1$. Among the options, this corresponds to $\\alpha1$.\n\nOption C, $\\alpha2$, is sufficient but not necessary; the minimal and correct condition is $\\alpha1$, which is option B.", "answer": "$$\\boxed{B}$$", "id": "1406772"}, {"introduction": "Our exploration culminates in a practice that showcases the extensibility of the SLLN. Often, we are interested not just in the average of our measurements, but in a function of that average. This problem combines the SLLN with the Continuous Mapping Theorem to analyze the long-term behavior of a variance-related quantity in a Monte Carlo simulation setting. This illustrates a sophisticated but common technique for deriving the convergence properties of more complex estimators. [@problem_id:1957105]", "problem": "A company specializes in running large-scale Monte Carlo simulations to price complex financial derivatives. A key component of their simulation involves repeatedly modeling a binary event, such as a stock price moving up or down. For a particular simulation, this is modeled as a sequence of experiments. In each experiment $i$, a total of $m$ independent Bernoulli trials are run, each with a success probability of $p$. Let the random variable $X_i$ be the total number of successes in experiment $i$.\n\nThe experiments are independent of each other, so the sequence of counts $X_1, X_2, \\ldots, X_n, \\ldots$ can be modeled as a sequence of independent and identically distributed (i.i.d.) random variables, with each $X_i$ following a binomial distribution with parameters $m$ and $p$.\n\nAnalysts at the company are studying the convergence properties of their estimators. They define the average proportion of successes over the first $n$ experiments as:\n$$ \\bar{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{m} $$\nThey are interested in a quantity related to the variance of the underlying process. Determine the value to which the quantity $V_n = \\bar{p}_n (1 - \\bar{p}_n)$ converges almost surely as the number of experiments $n$ approaches infinity. Your answer should be a symbolic expression in terms of the given parameters.", "solution": "Let $X_{i} \\sim \\text{Binomial}(m,p)$ be i.i.d. and define $Y_{i} = \\frac{X_{i}}{m}$. Then the average proportion is the sample mean\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}.\n$$\nWe compute the first two moments of $Y_{i}$ using linearity of expectation and the variance scaling rule:\n$$\n\\mathbb{E}[Y_{i}] = \\frac{1}{m}\\mathbb{E}[X_{i}] = \\frac{1}{m}\\cdot m p = p,\n$$\n$$\n\\operatorname{Var}(Y_{i}) = \\frac{1}{m^{2}}\\operatorname{Var}(X_{i}) = \\frac{1}{m^{2}}\\cdot m p(1-p) = \\frac{p(1-p)}{m}.\n$$\nThus $\\{Y_{i}\\}$ are i.i.d. with finite mean $\\mathbb{E}[Y_{i}]=p$. By the Strong Law of Large Numbers,\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i} \\xrightarrow{\\text{a.s.}} \\mathbb{E}[Y_{1}] = p.\n$$\n\nDefine the continuous function $g(x) = x(1-x)$. By the continuous mapping theorem applied to almost sure convergence,\n$$\nV_{n} = g(\\bar{p}_{n}) = \\bar{p}_{n}\\bigl(1-\\bar{p}_{n}\\bigr) \\xrightarrow{\\text{a.s.}} g(p) = p(1-p).\n$$\n\nTherefore, $V_{n}$ converges almost surely to $p(1-p)$.", "answer": "$$\\boxed{p(1-p)}$$", "id": "1957105"}]}