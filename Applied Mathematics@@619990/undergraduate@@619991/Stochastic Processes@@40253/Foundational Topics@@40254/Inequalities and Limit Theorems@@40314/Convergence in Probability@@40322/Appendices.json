{"hands_on_practices": [{"introduction": "To truly master a mathematical concept, it is often useful to explore cases where it does not apply. This practice challenges our intuition by examining a deterministic sequence of random variables that oscillates between two values. By rigorously applying the definition of convergence in probability, you will develop a deeper understanding of why this mode of convergence requires the sequence to 'settle down' near a specific value, a condition that this oscillating sequence fails to meet [@problem_id:1910711].", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ that models the state of a simple digital switch at discrete time steps $n=1, 2, 3, \\ldots$. The switch is designed to be in one of two states, represented by the values $-1$ and $1$. The state of the switch at time $n$ is deterministic and follows the rule $X_n = (-1)^n$. This means that for each $n$, the random variable $X_n$ takes the value $(-1)^n$ with a probability of 1.\n\nThe sequence of states is therefore $X_1 = -1$, $X_2 = 1$, $X_3 = -1$, $X_4 = 1$, and so on.\n\nWhich of the following statements correctly describes the convergence behavior of this sequence?\n\nA. The sequence $\\{X_n\\}$ converges in probability to 0.\n\nB. The sequence $\\{X_n\\}$ converges in probability to 1.\n\nC. The sequence $\\{X_n\\}$ converges in probability to -1.\n\nD. The sequence $\\{X_n\\}$ does not converge in probability.\n\nE. The sequence $\\{X_n\\}$ converges in probability, but the limit is not a constant.", "solution": "We recall the definition: a sequence of random variables $\\{X_n\\}$ converges in probability to a random variable $X$ if for every $\\varepsilon > 0$,\n$$\n\\lim_{n\\to\\infty} P\\left(|X_n - X| > \\varepsilon\\right) = 0.\n$$\nHere each $X_n$ is deterministic with $X_n = (-1)^n$ almost surely.\n\nFirst, check convergence in probability to a constant $c \\in \\mathbb{R}$. If $c \\notin \\{-1,1\\}$, set $\\delta = \\min\\{|c-1|,|c+1|\\} > 0$ and choose $\\varepsilon = \\delta/2$. Then for every $n$,\n$$\n|X_n - c| = \\left|(-1)^n - c\\right| \\geq \\delta > \\varepsilon,\n$$\nso $P(|X_n - c| > \\varepsilon) = 1$ for all $n$, which cannot tend to $0$. Hence no limit $c \\notin \\{-1,1\\}$ is possible.\n\nIf $c=1$, choose $\\varepsilon = \\frac{1}{2}$. Then for odd $n$,\n$$\n|X_n - 1| = |(-1) - 1| = 2 > \\frac{1}{2},\n$$\nso $P(|X_n - 1| > \\frac{1}{2}) = 1$ for all odd $n$, which does not tend to $0$. Similarly, if $c=-1$, for even $n$,\n$$\n|X_n - (-1)| = |1 - (-1)| = 2 > \\frac{1}{2},\n$$\nso $P(|X_n + 1| > \\frac{1}{2}) = 1$ for all even $n$, which also does not tend to $0$. Therefore $\\{X_n\\}$ does not converge in probability to any constant, ruling out A, B, and C.\n\nNext, consider whether $\\{X_n\\}$ could converge in probability to a non-constant random variable $X$. Take $\\varepsilon = \\frac{1}{2}$. Then\n$$\nP\\left(|X_n - X| > \\frac{1}{2}\\right) = P\\left(|(-1)^n - X| > \\frac{1}{2}\\right).\n$$\nDecompose $X$ by its mass on $\\{-1,1\\}$: let $p = P(X=1)$, $q = P(X=-1)$, and $r = 1 - p - q = P(X \\notin \\{-1,1\\})$. For even $n$ (so $X_n=1$),\n$$\nP\\left(|X_n - X| > \\frac{1}{2}\\right) = P\\left(|1-X| > \\frac{1}{2}\\right) = q+r.\n$$\nFor odd $n$ (so $X_n = -1$),\n$$\nP\\left(|X_n - X| > \\frac{1}{2}\\right) = P\\left(|-1-X| > \\frac{1}{2}\\right) = p+r.\n$$\nThus the sequence of probabilities alternates between the constants $q+r$ and $p+r$. For convergence in probability, we would need both $q+r \\to 0$ and $p+r \\to 0$, which forces $p=q=r=0$, contradicting $p+q+r=1$. Hence no random limit (constant or not) exists. Therefore the sequence does not converge in probability, and option D is correct.", "answer": "$$\\boxed{D}$$", "id": "1910711"}, {"introduction": "Once we understand the definition of convergence in probability, we can explore how it behaves under algebraic operations. This exercise demonstrates a fundamental property: the stability of convergence under addition. You will practice combining a sequence known to converge with another whose convergence must first be established using its mean and variance, providing an excellent opportunity to apply the versatile Chebyshev's inequality [@problem_id:1910723].", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables. The sequence $\\{X_n\\}$ is known to converge in probability to the constant 5. The sequence $\\{Y_n\\}$ is characterized by having a mean of $E[Y_n] = 0$ and a variance of $Var(Y_n) = \\frac{1}{\\sqrt{n}}$ for all positive integers $n$.\n\nA new sequence of random variables, $\\{Z_n\\}_{n=1}^{\\infty}$, is defined by the sum $Z_n = X_n + Y_n$.\n\nDetermine the numerical value to which the sequence $\\{Z_n\\}$ converges in probability.", "solution": "We are given that $X_n \\xrightarrow{p} 5$, that is, for every $\\varepsilon > 0$,\n$$\n\\lim_{n\\to\\infty} P\\big(|X_n - 5| > \\varepsilon\\big) = 0.\n$$\nFor $Y_n$, we have $E[Y_n] = 0$ and $\\text{Var}(Y_n) = n^{-1/2}$ for all $n$. By Chebyshev’s inequality, for any $\\varepsilon > 0$,\n$$\nP\\big(|Y_n| > \\varepsilon\\big) = P\\big(|Y_n - E[Y_n]| > \\varepsilon\\big) \\leq \\frac{\\text{Var}(Y_n)}{\\varepsilon^2} = \\frac{n^{-1/2}}{\\varepsilon^2} \\xrightarrow[n\\to\\infty]{} 0.\n$$\nHence $Y_n \\xrightarrow{p} 0$.\n\nDefine $Z_n = X_n + Y_n$. For any $\\varepsilon > 0$, by the triangle inequality,\n$$\n|Z_n - 5| = \\big|(X_n - 5) + Y_n\\big| \\leq |X_n - 5| + |Y_n|.\n$$\nTherefore, using the union bound,\n$$\nP\\big(|Z_n - 5| > \\varepsilon\\big) \\leq P\\big(|X_n - 5| > \\varepsilon/2\\big) + P\\big(|Y_n| > \\varepsilon/2\\big) \\xrightarrow[n\\to\\infty]{} 0,\n$$\nbecause the first term tends to zero by $X_n \\xrightarrow{p} 5$ and the second term tends to zero by Chebyshev’s inequality shown above. Thus $Z_n \\xrightarrow{p} 5$.\n\nTherefore, the numerical value to which $Z_n$ converges in probability is $5$.", "answer": "$$\\boxed{5}$$", "id": "1910723"}, {"introduction": "Convergence in probability is not just a theoretical curiosity; it is the cornerstone of modern statistical inference, particularly in the concept of consistency. This hands-on problem places you in the role of an engineer analyzing the efficiency of a device, where you will use convergence in probability to show that your estimator becomes more accurate as you collect more data [@problem_id:1910693]. This exercise connects the abstract definition to a tangible goal: proving that a statistical method works as intended by applying core results like the Weak Law of Large Numbers and the Continuous Mapping Theorem.", "problem": "An engineer is characterizing a new type of thermoelectric generator. In a series of $n$ independent trials, two physical quantities are measured for each trial $i$: the heat flow across the generator, $X_i$, and the resulting electrical power output, $Y_i$.\n\nThe sequences of measurements, $\\{X_1, X_2, \\dots, X_n\\}$ and $\\{Y_1, Y_2, \\dots, Y_n\\}$, can be modeled as follows:\n- The heat flow measurements $\\{X_i\\}$ are independent and identically distributed (i.i.d.) random variables with a true mean heat flow $\\mathbb{E}[X_i] = \\mu_{Q}$ and a finite variance.\n- The power output measurements $\\{Y_i\\}$ are i.i.d. random variables with a true mean power output $\\mathbb{E}[Y_i] = \\mu_{P}$ and a finite variance.\n- It is known that the true mean heat flow is non-zero, i.e., $\\mu_{Q} \\neq 0$.\n\nTo estimate the generator's conversion efficiency, the engineer computes the sample means of the two sets of measurements after $n$ trials:\n$$\n\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\quad \\text{and} \\quad \\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe engineer then defines an estimator for the efficiency, $\\eta_n$, as the ratio of the sample mean power output to the sample mean heat flow:\n$$\n\\eta_n = \\frac{\\bar{Y}_n}{\\bar{X}_n}\n$$\nDetermine the value to which the sequence of random variables $\\eta_n$ converges in probability as the number of trials $n$ approaches infinity. Express your answer as an analytic expression in terms of $\\mu_{Q}$ and $\\mu_{P}$.", "solution": "By assumption, the heat flow measurements $\\{X_i\\}$ are i.i.d. with finite variance and mean $E[X_i] = \\mu_{Q}$, and the power measurements $\\{Y_i\\}$ are i.i.d. with finite variance and mean $E[Y_i] = \\mu_{P}$. Define the sample means\n$$\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n}X_i,\\qquad \\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^{n}Y_i.\n$$\nBy the Weak Law of Large Numbers (WLLN), finite variance implies\n$$\n\\bar{X}_n \\xrightarrow{p} \\mu_{Q}\\quad\\text{and}\\quad \\bar{Y}_n \\xrightarrow{p} \\mu_{P}\\quad\\text{as }n\\to\\infty.\n$$\nFrom these, joint convergence in probability of the pair follows: for any $\\varepsilon > 0$,\n$$\nP\\left(\\left\\|(\\bar{X}_n,\\bar{Y}_n)-(\\mu_{Q},\\mu_{P})\\right\\| > \\varepsilon\\right)\n\\leq P\\left(|\\bar{X}_n-\\mu_{Q}| > \\frac{\\varepsilon}{2}\\right) + P\\left(|\\bar{Y}_n-\\mu_{P}| > \\frac{\\varepsilon}{2}\\right)\\to 0,\n$$\nso $(\\bar{X}_n,\\bar{Y}_n) \\xrightarrow{p} (\\mu_{Q},\\mu_{P})$. Because $\\mu_{Q} \\neq 0$, the mapping $g(x,y)=y/x$ is continuous at $(\\mu_{Q},\\mu_{P})$. By the Continuous Mapping Theorem,\n$$\n\\eta_n = \\frac{\\bar{Y}_n}{\\bar{X}_n} = g(\\bar{X}_n,\\bar{Y}_n) \\xrightarrow{p} g(\\mu_{Q},\\mu_{P}) = \\frac{\\mu_{P}}{\\mu_{Q}}.\n$$\nFinally, the fact that $\\mu_{Q} \\neq 0$ also ensures that $P(|\\bar{X}_n| > \\tfrac{|\\mu_{Q}|}{2}) \\to 1$, so the ratio is well-defined with probability tending to one.\nTherefore, $\\eta_n$ converges in probability to $\\mu_{P}/\\mu_{Q}$.", "answer": "$$\\boxed{\\frac{\\mu_{P}}{\\mu_{Q}}}$$", "id": "1910693"}]}