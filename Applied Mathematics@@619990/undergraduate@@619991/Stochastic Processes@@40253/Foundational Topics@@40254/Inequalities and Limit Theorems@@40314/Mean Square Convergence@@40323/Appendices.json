{"hands_on_practices": [{"introduction": "This first exercise provides a direct application of the definition of mean square convergence. We will examine a hypothetical model of signal degradation to see if the signal strength, represented by a sequence of random variables $X_n$, fades to zero. This practice is designed to solidify your grasp of the fundamental calculation involved in proving mean square convergence: evaluating the limit of the mean squared error $E[(X_n - X)^2]$ [@problem_id:1318343].", "problem": "A research team is studying a simple model for signal degradation over a noisy channel. A sequence of random variables, $\\{X_n\\}_{n=1}^{\\infty}$, represents the signal's amplitude at discrete time steps $n=1, 2, 3, \\ldots$. The model proposes that the signal at step $n$ is related to an initial random disturbance $Y$ by the formula $X_n = \\frac{Y}{n}$. The only information known about the initial disturbance $Y$ is that it has a well-defined and finite second moment, that is, $E[Y^2] < \\infty$. The team needs to determine if this signal model predicts that the signal will eventually fade to zero. Specifically, does the sequence $\\{X_n\\}$ converge in mean square to the zero random variable, $X=0$?\n\nSelect the correct statement from the options below.\n\nA. Yes, the sequence converges in mean square to 0.\n\nB. No, the sequence does not converge in mean square to 0 because the limit of the mean square error is a non-zero finite value.\n\nC. No, the sequence does not converge in mean square to 0 because the limit of the mean square error diverges.\n\nD. It is impossible to determine convergence in mean square without more information about the probability distribution of $Y$.", "solution": "Convergence in mean square to a random variable $X$ means that $E\\!\\left[(X_{n}-X)^{2}\\right] \\to 0$ as $n \\to \\infty$. Here $X_{n}=\\frac{Y}{n}$ and $X=0$, so we need to evaluate\n$$\nE\\!\\left[\\left(X_{n}-0\\right)^{2}\\right]=E\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right].\n$$\nUsing the scaling property of expectation for deterministic constants, $E\\!\\left[c^{2}Z^{2}\\right]=c^{2}E\\!\\left[Z^{2}\\right]$, we obtain\n$$\nE\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right]=\\frac{1}{n^{2}}E\\!\\left[Y^{2}\\right].\n$$\nBy assumption, $E\\!\\left[Y^{2}\\right]<\\infty$, so it is a finite constant independent of $n$. Therefore,\n$$\n\\lim_{n\\to\\infty}E\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right]=\\lim_{n\\to\\infty}\\frac{E\\!\\left[Y^{2}\\right]}{n^{2}}=0,\n$$\nsince $\\frac{1}{n^{2}}\\to 0$ as $n\\to\\infty$. Hence $X_{n}\\to 0$ in mean square. No additional distributional information about $Y$ is required beyond the finiteness of $E[Y^{2}]$.\n\nTherefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1318343"}, {"introduction": "While mean square convergence implies convergence in probability, the reverse is not always true. This exercise presents a classic and fundamental counterexample, where a sequence of random variables converges to zero in probability but fails to converge in mean square [@problem_id:1318373]. Understanding this scenario is crucial for appreciating the distinct nature of mean square convergence, which is sensitive to the magnitude of rare but large deviations.", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\geq 1$, the random variable $X_n$ can take one of two values: it is equal to $n$ with a probability of $p_n = \\frac{1}{n^2}$, and it is equal to $0$ with a probability of $1 - p_n$.\n\nWhich of the following statements accurately describes the convergence properties of the sequence $\\{X_n\\}$ as $n \\to \\infty$?\n\nA. The sequence $\\{X_n\\}$ converges to 0 in mean square.\n\nB. The sequence $\\{X_n\\}$ converges to 1 in mean square.\n\nC. The sequence $\\{X_n\\}$ converges to 0 in probability, but does not converge to 0 in mean square.\n\nD. The sequence $\\{X_n\\}$ does not converge to 0 in probability.", "solution": "We analyze convergence in mean square and in probability for the sequence defined by $P(X_{n}=n)=p_{n}=\\frac{1}{n^{2}}$ and $P(X_{n}=0)=1-p_{n}$.\n\nFirst, check mean square convergence to $0$. By definition, convergence in mean square to $0$ requires $\\lim_{n\\to\\infty}\\mathbb{E}[X_{n}^{2}]=0$. Compute\n$$\n\\mathbb{E}[X_{n}^{2}]=n^{2}\\cdot \\frac{1}{n^{2}}+0^{2}\\cdot\\left(1-\\frac{1}{n^{2}}\\right)=1.\n$$\nThus $\\mathbb{E}[X_{n}^{2}]=1$ for all $n$, so it does not tend to $0$. Therefore, $\\{X_{n}\\}$ does not converge to $0$ in mean square, and option A is false.\n\nNext, check mean square convergence to $1$. Convergence in mean square to $1$ requires $\\lim_{n\\to\\infty}\\mathbb{E}[(X_{n}-1)^{2}]=0$. Compute\n$$\n\\mathbb{E}[(X_{n}-1)^{2}]=(n-1)^{2}\\cdot \\frac{1}{n^{2}}+(0-1)^{2}\\cdot\\left(1-\\frac{1}{n^{2}}\\right)\n=\\frac{(n-1)^{2}}{n^{2}}+1-\\frac{1}{n^{2}}.\n$$\nSimplify:\n$$\n\\frac{(n-1)^{2}}{n^{2}}+1-\\frac{1}{n^{2}}=\\frac{n^{2}-2n+1}{n^{2}}+\\frac{n^{2}-1}{n^{2}}=\\frac{2n^{2}-2n}{n^{2}}=2-\\frac{2}{n}\\to 2.\n$$\nSince the limit is $2\\neq 0$, $\\{X_{n}\\}$ does not converge to $1$ in mean square, so option B is false.\n\nNow check convergence in probability to $0$. For any fixed $\\epsilon>0$,\n$$\nP(|X_{n}-0|>\\epsilon)=P(|X_{n}|>\\epsilon)=\n\\begin{cases}\n\\frac{1}{n^{2}}, & n>\\epsilon,\\\\\n0, & n\\leq \\epsilon.\n\\end{cases}\n$$\nIn either case, $\\lim_{n\\to\\infty}P(|X_{n}|>\\epsilon)=0$, because for all sufficiently large $n$ we have $n>\\epsilon$ and then $P(|X_{n}|>\\epsilon)=\\frac{1}{n^{2}}\\to 0$. Therefore, $X_{n}\\to 0$ in probability. Hence option D is false, and the precise statement is that $X_{n}$ converges to $0$ in probability but not in mean square.\n\nConsequently, the correct choice is that the sequence converges to $0$ in probability, but does not converge to $0$ in mean square.", "answer": "$$\\boxed{C}$$", "id": "1318373"}, {"introduction": "This practice builds on the concepts from the previous exercise by introducing a tunable parameter, $\\alpha$, which controls the growth rate of rare events in a sequence of random variables. Your task is to determine the precise \"tipping point\" for this parameter that separates convergent and divergent behavior in the mean square sense [@problem_id:1318384]. Solving this problem will provide deeper insight into the delicate balance between the probability of an event and its squared magnitude, which lies at the heart of mean square convergence.", "problem": "A sequence of discrete random variables, denoted $\\{X_n\\}_{n=1}^{\\infty}$, is defined for each integer $n \\geq 1$. The probability mass function of the random variable $X_n$ is given by:\n$$ P(X_n = n^{\\alpha}) = \\frac{1}{n} $$\n$$ P(X_n = 0) = 1 - \\frac{1}{n} $$\nHere, $\\alpha$ is a constant real number.\n\nWe are interested in the mean square convergence of this sequence to the constant random variable $X=0$. A sequence of random variables $\\{Y_n\\}$ converges in mean square to a random variable $Y$ if the limit of the mean squared error is zero, i.e., $\\lim_{n \\to \\infty} E[(Y_n - Y)^2] = 0$.\n\nDetermine the condition on $\\alpha$ that ensures the sequence $\\{X_n\\}$ converges in mean square to $X=0$.\n\nA. $\\alpha < 0$\n\nB. $\\alpha < 1/2$\n\nC. $\\alpha < 1$\n\nD. $\\alpha < 3/2$\n\nE. For all real $\\alpha$", "solution": "We are asked for mean square convergence of $X_{n}$ to $0$, which by definition requires\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=\\lim_{n\\to\\infty}\\mathbb{E}\\big[X_{n}^{2}\\big]=0.\n$$\nGiven the probability mass function\n$$\nP(X_{n}=n^{\\alpha})=\\frac{1}{n},\\qquad P(X_{n}=0)=1-\\frac{1}{n},\n$$\nwe compute the second moment using the law of the unconscious statistician:\n$$\n\\mathbb{E}[X_{n}^{2}]=\\sum_{x}x^{2}P(X_{n}=x)=(n^{\\alpha})^{2}\\cdot\\frac{1}{n}+0^{2}\\cdot\\Big(1-\\frac{1}{n}\\Big)=n^{2\\alpha-1}.\n$$\nThus\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[X_{n}^{2}]=\\lim_{n\\to\\infty}n^{2\\alpha-1}.\n$$\nFor a power $n^{\\beta}$, the limit as $n\\to\\infty$ is $0$ if and only if $\\beta<0$, equals $1$ if $\\beta=0$, and diverges to $\\infty$ if $\\beta>0$. Therefore the condition for mean square convergence is\n$$\n2\\alpha-1<0\\quad\\Longleftrightarrow\\quad \\alpha<\\frac{1}{2}.\n$$\nAmong the options, this corresponds to B.", "answer": "$$\\boxed{B}$$", "id": "1318384"}]}