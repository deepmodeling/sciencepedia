{"hands_on_practices": [{"introduction": "The Central Limit Theorem (CLT) is a cornerstone of probability theory and statistics, stating that the sum of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the underlying distribution. This exercise provides a concrete demonstration of this powerful theorem using chi-squared random variables, which are themselves sums of squared normal variables. By working through this problem [@problem_id:1353072], you will gain hands-on appreciation for how the CLT works in practice and why it is so fundamental for statistical approximation.", "problem": "Let $Z_1, Z_2, Z_3, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables, each following a standard normal distribution, $\\mathcal{N}(0, 1)$. For any positive integer $n$, define a new random variable $X_n$ as the sum of the squares of the first $n$ variables in the sequence:\n$$X_n = \\sum_{i=1}^{n} Z_i^2$$\nNow, consider the standardized random variable $Y_n$ defined as:\n$$Y_n = \\frac{X_n - n}{\\sqrt{2n}}$$\nAs $n$ approaches infinity, the distribution of $Y_n$ converges to a specific well-known probability distribution. This phenomenon is known as convergence in distribution.\n\nWhich of the following distributions is the limiting distribution of $Y_n$ as $n \\to \\infty$?\n\nA. A Chi-squared distribution with 1 degree of freedom, $\\chi_1^2$.\n\nB. A standard Normal distribution, $\\mathcal{N}(0, 1)$.\n\nC. A Normal distribution with mean 0 and variance 2, $\\mathcal{N}(0, 2)$.\n\nD. A Uniform distribution on the interval $[0, 1]$, $U(0, 1)$.\n\nE. The distribution of a constant random variable that is always equal to 0.", "solution": "Let $Z_{1},Z_{2},\\dots$ be i.i.d. with $Z_{i}\\sim \\mathcal{N}(0,1)$. Define $X_{n}=\\sum_{i=1}^{n}Z_{i}^{2}$ and $Y_{n}=\\frac{X_{n}-n}{\\sqrt{2n}}$.\n\nStep 1: Identify the distribution and moments of the summands. For each $i$, $X_{i}^{(1)}:=Z_{i}^{2}\\sim \\chi^{2}_{1}$ because the square of a standard normal is chi-squared with one degree of freedom. For a chi-squared random variable with $k$ degrees of freedom, the mean and variance are\n$$\n\\mathbb{E}[\\chi^{2}_{k}]=k,\\qquad \\operatorname{Var}(\\chi^{2}_{k})=2k.\n$$\nHence, for $X_{i}^{(1)}\\sim \\chi^{2}_{1}$,\n$$\n\\mu:=\\mathbb{E}[X_{i}^{(1)}]=1,\\qquad \\sigma^{2}:=\\operatorname{Var}(X_{i}^{(1)})=2.\n$$\n\nStep 2: Express $Y_{n}$ as a standardized sum. Since $X_{n}=\\sum_{i=1}^{n}X_{i}^{(1)}$, we have\n$$\n\\mathbb{E}[X_{n}]=n\\mu=n,\\qquad \\operatorname{Var}(X_{n})=n\\sigma^{2}=2n.\n$$\nTherefore,\n$$\nY_{n}=\\frac{X_{n}-\\mathbb{E}[X_{n}]}{\\sqrt{\\operatorname{Var}(X_{n})}}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}^{(1)}-\\mu\\right)}{\\sigma\\sqrt{n}}.\n$$\n\nStep 3: Apply the Central Limit Theorem (CLT). The sequence $\\{X_{i}^{(1)}\\}$ is i.i.d. with finite mean $\\mu$ and finite variance $\\sigma^{2}$. By the classical Central Limit Theorem,\n$$\n\\frac{\\sum_{i=1}^{n}\\left(X_{i}^{(1)}-\\mu\\right)}{\\sigma\\sqrt{n}}\\xrightarrow{d}\\mathcal{N}(0,1)\\quad \\text{as }n\\to\\infty.\n$$\nHence,\n$$\nY_{n}\\xrightarrow{d}\\mathcal{N}(0,1)\\quad \\text{as }n\\to\\infty.\n$$\n\nTherefore, the limiting distribution is a standard Normal distribution, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1353072"}, {"introduction": "Building upon the Central Limit Theorem, we often need to understand the behavior of functions of random variables that converge in distribution. The Continuous Mapping Theorem (CMT) provides the theoretical tool for this, stating that continuous functions preserve convergence. This practice [@problem_id:1910230] challenges you to apply the CMT to find the limiting distribution of a squared statistic, a crucial step in constructing many common hypothesis tests, such as the Wald test.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a population with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean.\n\nThe Central Limit Theorem states that the sequence of random variables $Y_n = \\sqrt{n}(\\bar{X}_n - \\mu)$ converges in distribution to a random variable $Y$ that follows a Normal distribution with mean 0 and variance $\\sigma^2$. This is denoted as $\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)$.\n\nConsider the statistic $T_n = n(\\bar{X}_n - \\mu)^2$. Using the information provided, determine the limiting distribution of $T_n$ as $n \\to \\infty$.\n\nWhich of the following describes the limiting distribution of $T_n$?\n\nA. A Normal distribution with mean $0$ and variance $\\sigma^4$.\n\nB. A Chi-squared distribution with 1 degree of freedom, denoted $\\chi_1^2$.\n\nC. A distribution equivalent to $\\sigma^2 W$, where $W$ follows a Chi-squared distribution with 1 degree of freedom.\n\nD. A Student's t-distribution with $n-1$ degrees of freedom.\n\nE. An F-distribution with $(1, n-1)$ degrees of freedom.", "solution": "Let $Y_{n}=\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)$. By the Central Limit Theorem, $Y_{n}\\xrightarrow{d}Y$ where $Y\\sim N\\left(0,\\sigma^{2}\\right)$. Define the continuous function $g(x)=x^{2}$. Then\n$$\nT_{n}=n\\left(\\bar{X}_{n}-\\mu\\right)^{2}=Y_{n}^{2}=g(Y_{n}).\n$$\nBy the Continuous Mapping Theorem, $T_{n}\\xrightarrow{d}g(Y)=Y^{2}$. If $Y\\sim N(0,\\sigma^{2})$, write $Y=\\sigma Z$ with $Z\\sim N(0,1)$. Then\n$$\nY^{2}=\\sigma^{2}Z^{2}.\n$$\nSince $Z^{2}\\sim\\chi_{1}^{2}$, letting $W\\sim\\chi_{1}^{2}$ gives $Y^{2}\\overset{d}{=}\\sigma^{2}W$. Therefore, the limiting distribution of $T_{n}$ is $\\sigma^{2}W$ with $W\\sim\\chi_{1}^{2}$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1910230"}, {"introduction": "In applied statistics, we frequently work with estimators that are non-linear transformations of simple sample averages. To perform inference, such as calculating confidence intervals, we need to know the variance of these complex estimators. The Delta method offers a powerful technique to approximate this variance. This problem [@problem_id:1910243] guides you through applying the Delta method to find the asymptotic variance of the sample odds ratio, a key metric in fields like epidemiology, demonstrating how abstract convergence concepts translate into practical tools for data analysis.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a Bernoulli distribution with a parameter $p$ representing the probability of success, where $0 < p < 1$. The sample proportion of successes is given by $\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\nIn many fields, such as epidemiology and social sciences, it is common to analyze the \"odds\" of an event occurring. The sample odds ratio is defined as the ratio of the sample proportion of successes to the sample proportion of failures, given by the statistic $O_n = \\frac{\\hat{p}_n}{1-\\hat{p}_n}$.\n\nFor a large sample size $n$, the sampling distribution of $O_n$ can be approximated by a normal distribution. Determine the variance of this asymptotic normal distribution for $O_n$. Express your answer as a formula in terms of $n$ and $p$.", "solution": "The problem asks for the variance of the asymptotic distribution of the sample odds ratio, $O_n = \\frac{\\hat{p}_n}{1-\\hat{p}_n}$. We can find this using the Delta method.\n\nFirst, we establish the limiting distribution of the sample proportion, $\\hat{p}_n$. The random variables $X_i$ are i.i.d. Bernoulli($p$). The mean and variance of each $X_i$ are:\n$E[X_i] = \\mu = p$\n$Var(X_i) = \\sigma^2 = p(1-p)$\n\nThe sample proportion $\\hat{p}_n$ is the sample mean of these variables. According to the Central Limit Theorem (CLT), for a large sample size $n$, the distribution of the sample mean is approximately normal. More formally, the CLT states:\n$$\n\\sqrt{n}(\\hat{p}_n - p) \\xrightarrow{d} N(0, p(1-p))\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. This tells us that the random variable $\\sqrt{n}(\\hat{p}_n - p)$ converges to a normal distribution with a mean of 0 and a variance of $p(1-p)$.\n\nNext, we apply the Delta method. The Delta method provides a way to find the limiting distribution of a function of a random variable whose limiting distribution is known. Our statistic of interest, $O_n$, is a function of $\\hat{p}_n$. Let's define this function as $g(x) = \\frac{x}{1-x}$. Then, $O_n = g(\\hat{p}_n)$.\n\nThe Delta method states that if $\\sqrt{n}(Y_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)$, then for a differentiable function $g$ such that $g'(\\mu) \\neq 0$:\n$$\n\\sqrt{n}(g(Y_n) - g(\\mu)) \\xrightarrow{d} N(0, [g'(\\mu)]^2 \\sigma^2)\n$$\n\nIn our case, $Y_n = \\hat{p}_n$, $\\mu = p$, and $\\sigma^2 = p(1-p)$. We need to find the derivative of our function $g(x)$. Using the quotient rule:\n$$\ng'(x) = \\frac{d}{dx} \\left( \\frac{x}{1-x} \\right) = \\frac{(1)(1-x) - (x)(-1)}{(1-x)^2} = \\frac{1-x+x}{(1-x)^2} = \\frac{1}{(1-x)^2}\n$$\n\nNow, we evaluate this derivative at the mean $\\mu=p$:\n$$\ng'(p) = \\frac{1}{(1-p)^2}\n$$\n\nWe can now find the variance of the limiting distribution of $\\sqrt{n}(O_n - g(p))$. Let's call this asymptotic variance $\\sigma_{asymp}^2$:\n$$\n\\sigma_{asymp}^2 = [g'(p)]^2 \\times (\\text{asymptotic variance of } \\sqrt{n}(\\hat{p}_n - p))\n$$\n$$\n\\sigma_{asymp}^2 = \\left( \\frac{1}{(1-p)^2} \\right)^2 \\times p(1-p) = \\frac{1}{(1-p)^4} \\times p(1-p) = \\frac{p}{(1-p)^3}\n$$\n\nSo, we have the limiting distribution:\n$$\n\\sqrt{n}(O_n - \\frac{p}{1-p}) \\xrightarrow{d} N\\left(0, \\frac{p}{(1-p)^3}\\right)\n$$\nThis result implies that for large $n$, the random variable $O_n$ itself is approximately normally distributed. We can see this by dividing the term inside the parenthesis by $\\sqrt{n}$:\n$$\nO_n - \\frac{p}{1-p} \\approx N\\left(0, \\frac{1}{n} \\frac{p}{(1-p)^3}\\right)\n$$\nOr, equivalently:\n$$\nO_n \\approx N\\left(\\frac{p}{1-p}, \\frac{p}{n(1-p)^3}\\right)\n$$\nThe question asks for the variance of this asymptotic normal distribution for $O_n$. From the expression above, this variance is $\\frac{p}{n(1-p)^3}$.", "answer": "$$\\boxed{\\frac{p}{n(1-p)^{3}}}$$", "id": "1910243"}]}