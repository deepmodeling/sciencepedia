{"hands_on_practices": [{"introduction": "The Weak Law of Large Numbers tells us that sample averages converge to the true mean, but in practice, we often need to know *how many* samples are required to achieve a desired accuracy. This exercise provides a concrete application of this principle, using Chebyshev's inequality as a tool to determine the minimum sample size needed for a system to meet its reliability specifications. It’s a practical demonstration of how abstract probability theory informs tangible engineering design choices. [@problem_id:1345681]", "problem": "A cloud computing platform uses a load balancer to manage incoming tasks. Each task is assigned an integer priority level, chosen randomly and uniformly from the set $\\{1, 2, \\dots, M\\}$. The priority levels of successive tasks are independent of each other.\n\nTo make resource allocation decisions, the load balancer computes the running average of the priority levels of the last $n$ tasks. The system is designed with the requirement that this running average must be a reliable estimator of the true mean priority level.\n\nThe maximum priority level is $M = 50$. The system's reliability specification demands that the probability of the running average deviating from the true mean priority level by $0.5$ or more must be no greater than $0.05$.\n\nWhat is the minimum number of tasks, $n$, that the load balancer must average to satisfy this stringent requirement? The final answer must be an integer.", "solution": "Let $X_{1}, X_{2}, \\dots, X_{n}$ be independent and identically distributed with the discrete uniform distribution on $\\{1,2,\\dots,M\\}$, where $M=50$. Then the true mean is $\\mu = \\frac{M+1}{2}$ and the variance is $\\operatorname{Var}(X_{1}) = \\frac{M^{2}-1}{12}$.\n\nThe running average is the sample mean $\\overline{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. Using independence,\n$$\n\\operatorname{Var}(\\overline{X}_{n}) = \\frac{\\operatorname{Var}(X_{1})}{n}.\n$$\nBy Chebyshev's inequality, for any $\\epsilon>0$,\n$$\n\\mathbb{P}\\big(|\\overline{X}_{n}-\\mu|\\geq \\epsilon\\big) \\leq \\frac{\\operatorname{Var}(\\overline{X}_{n})}{\\epsilon^{2}} = \\frac{\\operatorname{Var}(X_{1})}{n\\,\\epsilon^{2}}.\n$$\nSet $\\epsilon = 0.5$ and $M=50$. Then\n$$\n\\operatorname{Var}(X_{1}) = \\frac{50^{2}-1}{12} = \\frac{2499}{12},\n$$\nso\n$$\n\\mathbb{P}\\big(|\\overline{X}_{n}-\\mu|\\geq 0.5\\big) \\leq \\frac{\\frac{2499}{12}}{n \\cdot \\left(\\frac{1}{4}\\right)} = \\frac{2499}{12}\\cdot \\frac{4}{n} = \\frac{833}{n}.\n$$\nTo meet the requirement that this probability be no greater than $0.05$, we need\n$$\n\\frac{833}{n} \\leq 0.05 \\quad \\Longleftrightarrow \\quad n \\geq \\frac{833}{0.05} = 833 \\cdot 20 = 16660.\n$$\nTherefore, the minimum integer $n$ that guarantees the specification is $16660$.", "answer": "$$\\boxed{16660}$$", "id": "1345681"}, {"introduction": "A deep understanding of any scientific law includes knowing its limits. The Weak Law of Large Numbers is powerful, but it is not universally applicable. This problem explores a famous counterexample—the Cauchy distribution—to illustrate the critical importance of the theorem's underlying assumptions. By identifying why the sample mean of Cauchy variables fails to converge, you will gain a more robust and nuanced understanding of when and why the law of large numbers holds. [@problem_id:1345655]", "problem": "Consider a sequence of random variables $X_1, X_2, \\dots, X_n$ that are independent and identically distributed (i.i.d.), drawn from a standard Cauchy distribution. The Probability Density Function (PDF) for a standard Cauchy random variable $X$ is given by\n$$\nf(x) = \\frac{1}{\\pi(1+x^2)} \\quad \\text{for} \\quad x \\in (-\\infty, \\infty)\n$$\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean of the first $n$ variables. A well-known result in probability, the Weak Law of Large Numbers (WLLN), describes the convergence of the sample mean to a constant for many distributions. However, for a sequence of i.i.d. Cauchy random variables, the sample mean $\\bar{X}_n$ does not converge in probability to a constant value as $n \\to \\infty$.\n\nWhat is the fundamental mathematical property of the Cauchy distribution that causes this failure of the Weak Law of Large Numbers?\n\nA. The random variables $X_i$ are not truly independent.\n\nB. The variance of the Cauchy distribution is infinite.\n\nC. The standard Cauchy distribution is symmetric about zero.\n\nD. The expected value $E[X_i]$ of the Cauchy distribution is undefined.\n\nE. The PDF of the Cauchy distribution is non-zero over an infinite domain.", "solution": "We recall a standard form of the Weak Law of Large Numbers: if $\\{X_{i}\\}_{i=1}^{\\infty}$ are i.i.d. with finite expectation $E[X_{1}]=\\mu$ (equivalently $E[|X_{1}|]<\\infty$), then the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ converges in probability to $\\mu$.\n\nTo identify why the WLLN fails for i.i.d. standard Cauchy variables, we examine whether the expectation exists. For a standard Cauchy random variable $X$ with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$, the expectation, if it existed, would be\n$$\nE[X]=\\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx=\\int_{-\\infty}^{\\infty} \\frac{x}{\\pi(1+x^{2})}\\,dx.\n$$\nThis improper integral does not converge in the Lebesgue sense because the integral of the absolute value diverges:\n$$\nE[|X|]=\\int_{-\\infty}^{\\infty} |x|\\,f(x)\\,dx=\\frac{2}{\\pi}\\int_{0}^{\\infty} \\frac{x}{1+x^{2}}\\,dx.\n$$\nEvaluating the antiderivative,\n$$\n\\int \\frac{x}{1+x^{2}}\\,dx=\\frac{1}{2}\\ln\\!\\bigl(1+x^{2}\\bigr),\n$$\nhence\n$$\n\\int_{0}^{R} \\frac{x}{1+x^{2}}\\,dx=\\frac{1}{2}\\ln\\!\\bigl(1+R^{2}\\bigr)\\xrightarrow[R\\to\\infty]{}\\infty,\n$$\nso $E[|X|]=\\infty$. Consequently, $E[X]$ is undefined (the positive and negative parts both diverge), and the condition required by the WLLN fails.\n\nTherefore, the fundamental property causing the failure of the Weak Law of Large Numbers for the sample mean of i.i.d. Cauchy random variables is that the expected value does not exist. While it is also true that the variance is infinite, a finite variance is not necessary for the WLLN in general; what is essential is the existence of a finite mean. Hence the correct choice is that $E[X_{i}]$ is undefined.\n\nFor completeness, this is consistent with the stability property of the Cauchy distribution: if $X_{1},\\dots,X_{n}$ are i.i.d. standard Cauchy, then $S_{n}=\\sum_{i=1}^{n}X_{i}$ is also Cauchy with the same scale up to a linear factor, and $\\bar{X}_{n}=S_{n}/n$ is again standard Cauchy, so it does not converge in probability to any constant.", "answer": "$$\\boxed{D}$$", "id": "1345655"}, {"introduction": "The simplest version of the Law of Large Numbers applies to independent and identically distributed (i.i.d.) variables, but the principle of averaging is more general. This problem challenges you to extend your thinking to a case where the variables are independent but not identically distributed, with variances that grow over time. By determining the precise conditions under which a sample mean still converges, you will see how the proof technique behind the WLLN can be adapted to analyze more complex, non-i.i.d. systems. [@problem_id:1407182]", "problem": "In a data acquisition system, a sequence of measurements, represented by independent random variables $X_1, X_2, \\dots, X_k, \\dots$, is recorded. Ideally, each measurement should have a mean of zero, so we have $E[X_k] = 0$ for all $k \\ge 1$. However, the measurement apparatus degrades over time, causing the uncertainty of the measurements to increase. This degradation is modeled by the variance of each measurement, which follows a power law: $\\operatorname{Var}(X_k) = c k^{\\alpha}$, where $c$ is a known positive constant and $\\alpha$ is a real-valued parameter describing the rate of degradation.\n\nThe system is considered reliable if the sample mean of the measurements, defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{k=1}^{n} X_k$, converges in probability to the true mean of zero. Convergence in probability to zero means that for any arbitrarily small positive tolerance $\\epsilon$, the probability $P(|\\bar{X}_n| \\ge \\epsilon)$ tends to zero as the number of measurements $n$ approaches infinity.\n\nDetermine the supremum of the set of all possible values of the parameter $\\alpha$ for which the system remains reliable.", "solution": "Let $S_{n}=\\sum_{k=1}^{n}X_{k}$ and $\\bar{X}_{n}=S_{n}/n$. Since the $X_{k}$ are independent with $E[X_{k}]=0$ and $\\operatorname{Var}(X_{k})=c k^{\\alpha}$, we have\n$$\nE[\\bar{X}_{n}]=\\frac{1}{n}\\sum_{k=1}^{n}E[X_{k}]=0,\n$$\nand by independence,\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\right)=\\frac{1}{n^{2}}\\sum_{k=1}^{n}\\operatorname{Var}(X_{k})=\\frac{c}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nBy Chebyshev’s inequality, for any $\\epsilon>0$,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)\\le \\frac{\\operatorname{Var}(\\bar{X}_{n})}{\\epsilon^{2}}=\\frac{c}{\\epsilon^{2}n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nTherefore $\\bar{X}_{n}\\to 0$ in probability whenever\n$$\n\\frac{1}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}\\to 0.\n$$\nWe now evaluate the asymptotics of the partial sums. Using the integral comparison test:\n- If $\\alpha>-1$, then\n$$\n\\sum_{k=1}^{n}k^{\\alpha}\\sim \\frac{n^{\\alpha+1}}{\\alpha+1},\n$$\nso\n$$\n\\operatorname{Var}(\\bar{X}_{n})\\sim \\frac{c}{\\alpha+1}\\,n^{\\alpha-1}\\to 0 \\quad \\text{iff} \\quad \\alpha<1.\n$$\n- If $\\alpha=-1$, then $\\sum_{k=1}^{n}k^{-1}\\sim \\ln n$, so\n$$\n\\operatorname{Var}(\\bar{X}_{n})\\sim \\frac{c\\ln n}{n^{2}}\\to 0.\n$$\n- If $\\alpha<-1$, then $\\sum_{k=1}^{n}k^{\\alpha}$ converges to a finite limit as $n\\to\\infty$, hence\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right)\\to 0.\n$$\nThus for all $\\alpha<1$, $\\operatorname{Var}(\\bar{X}_{n})\\to 0$, and Chebyshev’s inequality gives $\\bar{X}_{n}\\to 0$ in probability; the system is reliable for all $\\alpha<1$.\n\nTo show sharpness at and beyond $\\alpha\\ge 1$, consider the valid choice $X_{k}\\sim \\mathcal{N}(0, c k^{\\alpha})$ independent. Then $\\bar{X}_{n}$ is Gaussian with mean $0$ and variance\n$$\nv_{n}=\\operatorname{Var}(\\bar{X}_{n})=\\frac{c}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nFor $\\alpha=1$, $\\sum_{k=1}^{n}k\\sim \\frac{n^{2}}{2}$, so $v_{n}\\to \\frac{c}{2}$ and hence $\\bar{X}_{n}$ converges in distribution to $\\mathcal{N}\\!\\left(0,\\frac{c}{2}\\right)$. Consequently, for any fixed $\\epsilon>0$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)=2\\int_{\\epsilon}^{\\infty}\\frac{1}{\\sqrt{\\pi c}}\\exp\\!\\left(-\\frac{x^{2}}{c}\\right)\\,dx>0,\n$$\nso $\\bar{X}_{n}$ does not converge to $0$ in probability.\n\nFor $\\alpha>1$, we have $v_{n}\\sim \\frac{c}{\\alpha+1}n^{\\alpha-1}\\to \\infty$. Then for any fixed $\\epsilon>0$,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n}|<\\epsilon\\right)=\\int_{-\\epsilon}^{\\epsilon}\\frac{1}{\\sqrt{2\\pi v_{n}}}\\exp\\!\\left(-\\frac{x^{2}}{2v_{n}}\\right)\\,dx\\le \\frac{2\\epsilon}{\\sqrt{2\\pi v_{n}}}\\to 0,\n$$\nso $\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)\\to 1$, and $\\bar{X}_{n}$ certainly does not converge to $0$ in probability.\n\nTherefore, the set of $\\alpha$ for which the system is reliable is $(-\\infty,1)$, and its supremum is $1$.", "answer": "$$\\boxed{1}$$", "id": "1407182"}]}