{"hands_on_practices": [{"introduction": "Let's begin with a fundamental example in a discrete setting. This practice problem explores how an expectation changes when we gain partial information about an outcome. By focusing only on the non-trivial results of a coin-toss game, we can directly apply the definition of conditional expectation to refine our prediction of the average score. [@problem_id:1291494]", "problem": "A player participates in a simple carnival game. The game consists of tossing three distinct, fair coins simultaneously. The player's score for a round is determined by the number of heads that land face up. However, there's a special rule: if a round results in all three coins landing as tails, the round is considered a 'dud' and is immediately replayed. We are interested in the expected score for any round that is not a dud.\n\nLet $X$ be the random variable representing the number of heads in a single toss of the three coins. Calculate the expected value of $X$ given that the round is not a dud (i.e., given that at least one head is observed). Express your final answer as an exact fraction.", "solution": "Let $X$ denote the number of heads when three independent fair coins are tossed. Then $X$ has a binomial distribution with parameters $n=3$ and $p=\\frac{1}{2}$.\n\nDefine the event $A=\\{X>0\\}$ (the round is not a dud). We seek $E[X \\mid A]$.\n\nFirst compute the unconditional expectation using indicator variables. Let $H_{i}$ be the indicator that the $i$-th coin shows heads. Then $X=\\sum_{i=1}^{3} H_{i}$, and by linearity of expectation,\n$$\nE[X]=\\sum_{i=1}^{3} E[H_{i}]=3 \\cdot \\frac{1}{2}=\\frac{3}{2}.\n$$\n\nNext compute $P(A)$. The complement $A^c$ is the event of all tails, which has probability\n$$\nP(A^c)=\\left(\\frac{1}{2}\\right)^{3}=\\frac{1}{8},\n$$\nso\n$$\nP(A)=1-P(A^c)=1-\\frac{1}{8}=\\frac{7}{8}.\n$$\n\nUse the conditional expectation identity\n$$\nE[X \\mid A]=\\frac{E[X \\mathbf{1}_{A}]}{P(A)}.\n$$\nSince $X=0$ on $A^c$, we have $X \\mathbf{1}_{A}=X$, hence\n$$\nE[X \\mid A]=\\frac{E[X]}{P(A)}=\\frac{\\frac{3}{2}}{\\frac{7}{8}}=\\frac{12}{7}.\n$$\n\nThis yields the expected score given the round is not a dud.", "answer": "$$\\boxed{\\frac{12}{7}}$$", "id": "1291494"}, {"introduction": "Building on the discrete case, we now turn to continuous random variables, where sums are replaced by integrals and probability mass functions by probability density functions. This exercise provides a geometric context for understanding conditional expectation, which is a powerful way to make the concept more intuitive. By considering a point chosen uniformly from a defined area, we can explore how fixing the value of one coordinate, say $X=x$, influences the expected value of the other coordinate, $Y$. [@problem_id:1905673]", "problem": "A point $(X, Y)$ is chosen uniformly at random from the region $R$ in the first quadrant of the Cartesian plane. The region $R$ is bounded by the curves $y = \\sqrt{x}$ and $y = x^3$. Find the conditional expectation of the random variable $Y$ given that the random variable $X$ takes on the value $x$, for $0 < x < 1$. Your answer should be a closed-form analytic expression in terms of $x$.", "solution": "The region $R$ in the first quadrant bounded by $y=\\sqrt{x}$ and $y=x^3$ has intersection points where $\\sqrt{x}=x^3$, which gives $x=x^6$, so $x=0$ or $x=1$. For $0<x<1$, the vertical cross-section at a fixed $x$ runs from $y_{\\ell}=x^3$ to $y_{u}=\\sqrt{x}$, with $y_{u}>y_{\\ell}$.\n\nSince $(X,Y)$ is uniformly distributed over $R$, the joint density is\n$$\nf_{X,Y}(x,y)=\\frac{1}{|R|} \\quad \\text{for } (x,y)\\in R,\n$$\nwhere $|R|$ is the area of $R$. The marginal density of $X$ for $0<x<1$ is\n$$\nf_{X}(x)=\\int_{y=x^3}^{\\sqrt{x}} \\frac{1}{|R|}\\,dy=\\frac{\\sqrt{x}-x^3}{|R|}.\n$$\nHence, for $0<x<1$, the conditional density of $Y$ given $X=x$ is\n$$\nf_{Y\\mid X}(y\\mid x)=\\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\\frac{1/|R|}{(\\sqrt{x}-x^3)/|R|}=\\frac{1}{\\sqrt{x}-x^3}, \\quad x^3\\leq y\\leq \\sqrt{x}.\n$$\nTherefore, the conditional expectation is\n$$\nE[Y\\mid X=x]=\\int_{x^3}^{\\sqrt{x}} y\\,f_{Y\\mid X}(y\\mid x)\\,dy=\\int_{x^3}^{\\sqrt{x}} \\frac{y}{\\sqrt{x}-x^3}\\,dy.\n$$\nEvaluating the integral,\n$$\nE[Y\\mid X=x]=\\left.\\frac{y^{2}}{2(\\sqrt{x}-x^{3})}\\right|_{y=x^{3}}^{y=\\sqrt{x}}=\\frac{(\\sqrt{x})^{2}-(x^{3})^{2}}{2(\\sqrt{x}-x^{3})}=\\frac{x-x^{6}}{2(\\sqrt{x}-x^{3})}.\n$$\nSimplify by writing $x-x^6=x(1-x^5)$ and $\\sqrt{x}-x^3=x^{1/2}(1-x^{5/2})$, so with $z=x^{5/2}$,\n$$\n\\frac{x-x^{6}}{2(\\sqrt{x}-x^{3})}=\\frac{x(1-z^{2})}{2x^{1/2}(1-z)}=\\frac{x^{1/2}}{2}(1+z)=\\frac{1}{2}\\left(x^{1/2}+x^{3}\\right).\n$$\nThus, for $0<x<1$,\n$$\nE[Y\\mid X=x]=\\frac{x^{1/2}+x^{3}}{2}.\n$$", "answer": "$$\\boxed{\\frac{x^{1/2}+x^{3}}{2}}$$", "id": "1905673"}, {"introduction": "Conditional expectation is not just a calculation; it's a tool for uncovering deep structural relationships between random variables. This problem showcases a classic and surprising result: conditioning the sum of two independent Poisson variables reveals an underlying binomial distribution. This practice is key to understanding how observing one aggregate quantity provides predictive power over its individual components, a common scenario in fields like physics and data analytics. [@problem_id:1905661]", "problem": "In a data center, two independent servers, Server A and Server B, process incoming requests. The number of requests arriving at Server A in a given minute, denoted by the random variable $X_1$, follows a Poisson distribution with mean $\\lambda_1$. Similarly, the number of requests arriving at Server B in the same minute, denoted by $X_2$, is also a Poisson random variable, independent of $X_1$, with mean $\\lambda_2$.\n\nSuppose that at the end of a particular minute, a monitoring system reports that a total of $k$ requests arrived at the two servers combined, where $k$ is a non-negative integer. Given this information, what is the expected number of requests that arrived at Server A?\n\nYour task is to find a closed-form expression for the conditional expectation $E[X_1 | X_1 + X_2 = k]$.", "solution": "Let $X_{1} \\sim \\mathrm{Poisson}(\\lambda_{1})$ and $X_{2} \\sim \\mathrm{Poisson}(\\lambda_{2})$ be independent. The joint probability mass function is\n$$\nP(X_{1}=i, X_{2}=j)=P(X_{1}=i)P(X_{2}=j)=\\exp(-\\lambda_{1})\\frac{\\lambda_{1}^{i}}{i!}\\,\\exp(-\\lambda_{2})\\frac{\\lambda_{2}^{j}}{j!}=\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{\\lambda_{1}^{i}\\lambda_{2}^{j}}{i!\\,j!}.\n$$\nFor $k \\in \\{0,1,2,\\dots\\}$ and $i \\in \\{0,1,\\dots,k\\}$, the conditional probability is\n$$\nP(X_{1}=i \\mid X_{1}+X_{2}=k)=\\frac{P(X_{1}=i, X_{2}=k-i)}{P(X_{1}+X_{2}=k)}=\\frac{\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{\\lambda_{1}^{i}\\lambda_{2}^{k-i}}{i!\\,(k-i)!}}{P(X_{1}+X_{2}=k)}.\n$$\nCompute the denominator by convolution:\n$$\nP(X_{1}+X_{2}=k)=\\sum_{i=0}^{k}P(X_{1}=i)P(X_{2}=k-i)=\\exp(-(\\lambda_{1}+\\lambda_{2}))\\sum_{i=0}^{k}\\frac{\\lambda_{1}^{i}}{i!}\\frac{\\lambda_{2}^{k-i}}{(k-i)!}.\n$$\nRewrite the sum using binomial coefficients:\n$$\n\\sum_{i=0}^{k}\\frac{\\lambda_{1}^{i}}{i!}\\frac{\\lambda_{2}^{k-i}}{(k-i)!}=\\frac{1}{k!}\\sum_{i=0}^{k}\\binom{k}{i}\\lambda_{1}^{i}\\lambda_{2}^{k-i}=\\frac{(\\lambda_{1}+\\lambda_{2})^{k}}{k!}.\n$$\nHence\n$$\nP(X_{1}+X_{2}=k)=\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{(\\lambda_{1}+\\lambda_{2})^{k}}{k!}.\n$$\nSubstituting back gives\n$$\nP(X_{1}=i \\mid X_{1}+X_{2}=k)=\\frac{\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{\\lambda_{1}^{i}\\lambda_{2}^{k-i}}{i!\\,(k-i)!}}{\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{(\\lambda_{1}+\\lambda_{2})^{k}}{k!}}=\\binom{k}{i}\\left(\\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}\\right)^{i}\\left(\\frac{\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}}\\right)^{k-i}.\n$$\nThus $X_{1}\\mid (X_{1}+X_{2}=k)$ has a $\\mathrm{Binomial}\\left(k,\\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}\\right)$ distribution. The expectation of a binomial random variable $Y \\sim \\mathrm{Binomial}(k,p)$ is $E[Y]=kp$, so\n$$\nE[X_{1} \\mid X_{1}+X_{2}=k]=k\\cdot \\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}=\\frac{k\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}.\n$$", "answer": "$$\\boxed{\\frac{k\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}}$$", "id": "1905661"}]}