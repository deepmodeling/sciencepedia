{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that anchors the abstract concept of conditional expectation in the familiar setting of rolling dice. This problem [@problem_id:1381964] requires you to apply the fundamental definition of expectation to a new, restricted sample space defined by the conditioning event. Mastering this skill of re-evaluating probabilities based on new information is the first step toward understanding more complex stochastic systems.", "problem": "Let $X_1$ and $X_2$ be the outcomes of two independent rolls of a fair six-sided die. The set of possible outcomes for each die is $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome having a probability of $1/6$.\n\nDefine two new random variables: the sum $S = X_1 + X_2$ and the maximum value $M = \\max(X_1, X_2)$.\n\nDetermine the conditional expectation of the sum $S$ given that the maximum value observed is $k$, where $k$ is an integer such that $1 \\le k \\le 6$. Express your answer as a function of $k$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and uniformly distributed on $\\{1,2,3,4,5,6\\}$. Define $S = X_{1} + X_{2}$ and $M = \\max(X_{1}, X_{2})$. We seek $\\mathbb{E}[S \\mid M = k]$ for $1 \\leq k \\leq 6$.\n\nFirst, characterize the event $\\{M = k\\}$. It consists of all ordered pairs $(i,j)$ with $1 \\leq i,j \\leq k$ such that at least one coordinate equals $k$. The total number of such pairs is\n$$\n|\\{(i,j) : \\max(i,j) = k\\}| = k^{2} - (k-1)^{2} = 2k - 1.\n$$\nSince each ordered pair has probability $\\frac{1}{36}$ and conditioning on $\\{M=k\\}$ restricts to these $2k-1$ equally likely pairs, the conditional distribution on this set is uniform.\n\nTherefore,\n$$\n\\mathbb{E}[S \\mid M = k] = \\frac{1}{2k - 1} \\sum_{\\max(i,j)=k} (i + j).\n$$\nPartition the set into the disjoint union\n$$\nB = \\{(k,j) : 1 \\leq j \\leq k\\}, \\quad C = \\{(i,k) : 1 \\leq i \\leq k-1\\},\n$$\nso that $|B| = k$, $|C| = k-1$, and $B \\cap C = \\varnothing$. Then\n$$\n\\sum_{(i,j)\\in B} (i + j) = \\sum_{j=1}^{k} (k + j) = k^{2} + \\frac{k(k+1)}{2},\n$$\nand\n$$\n\\sum_{(i,j)\\in C} (i + j) = \\sum_{i=1}^{k-1} (i + k) = \\frac{(k-1)k}{2} + k(k-1) = \\frac{3k(k-1)}{2}.\n$$\nHence the total sum over $\\{(i,j): \\max(i,j)=k\\}$ is\n$$\nk^{2} + \\frac{k(k+1)}{2} + \\frac{3k(k-1)}{2} = k^{2} + \\frac{k(4k - 2)}{2} = k^{2} + k(2k - 1) = 3k^{2} - k.\n$$\nDividing by the number of equally likely pairs $2k - 1$ gives\n$$\n\\mathbb{E}[S \\mid M = k] = \\frac{3k^{2} - k}{2k - 1}.\n$$\nThis holds for every integer $k$ with $1 \\leq k \\leq 6$.", "answer": "$$\\boxed{\\frac{3k^{2} - k}{2k - 1}}$$", "id": "1381964"}, {"introduction": "Moving from discrete outcomes to a continuous domain, this practice explores conditional expectation in a classic geometric scenario. The problem [@problem_id:1381953] involves order statistics, which are essential for analyzing the minimum or maximum values in a set of random variables. You will practice deriving a conditional probability density function to find the expected length, a technique that is broadly applicable in fields from reliability engineering to financial modeling.", "problem": "A straight, rigid rod of length $L$ is placed along the positive x-axis, with one end at the origin ($x=0$) and the other at $x=L$. Two points along the rod are selected for breaking it. The locations of these two breaks are determined by two random variables, $X_1$ and $X_2$, which are drawn independently from a uniform distribution on the interval $[0, L]$.\n\nThese two breaks divide the rod into three smaller segments. Let $U$ be the random variable representing the location of the break point closer to the origin, and let $V$ be the random variable for the location of the break point farther from the origin. Consequently, we have $U = \\min(X_1, X_2)$ and $V = \\max(X_1, X_2)$. The length of the middle segment is thus given by the random variable $M = V - U$.\n\nYour task is to determine the conditional expectation of the length of the middle segment, $M$, given that the first break (the one closer to the origin) occurs at a specific position $u$, where $0 < u < L$.\n\nExpress your answer as a symbolic expression in terms of $L$ and $u$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and identically distributed with uniform density on $[0,L]$, so $f_{X_{1},X_{2}}(x_{1},x_{2}) = L^{-2}$ on $[0,L]^{2}$. Define the order statistics $U=\\min(X_{1},X_{2})$ and $V=\\max(X_{1},X_{2})$. The joint density of $(U,V)$ for $0<u<v<L$ is the standard order-statistics result\n$$\nf_{U,V}(u,v) = \\frac{2}{L^{2}}, \\quad 0<u<v<L.\n$$\nThe marginal density of $U$ is obtained by integrating over $v$:\n$$\nf_{U}(u) = \\int_{v=u}^{L} \\frac{2}{L^{2}} \\, dv = \\frac{2(L-u)}{L^{2}}, \\quad 0<u<L.\n$$\nHence, the conditional density of $V$ given $U=u$ is\n$$\nf_{V\\mid U}(v\\mid u) = \\frac{f_{U,V}(u,v)}{f_{U}(u)} = \\frac{1}{L-u}, \\quad u<v<L,\n$$\nso $V\\mid U=u$ is uniform on $(u,L)$. The middle segment length is $M=V-U$, so conditioned on $U=u$ we have $M=V-u$. Therefore,\n$$\n\\mathbb{E}[M\\mid U=u] = \\int_{u}^{L} (v-u)\\,\\frac{1}{L-u}\\,dv\n= \\frac{1}{L-u}\\left[\\frac{(v-u)^{2}}{2}\\right]_{v=u}^{v=L}\n= \\frac{1}{L-u}\\cdot \\frac{(L-u)^{2}}{2}\n= \\frac{L-u}{2}.\n$$\nThis holds for $0<u<L$.", "answer": "$$\\boxed{\\frac{L-u}{2}}$$", "id": "1381953"}, {"introduction": "This final exercise tackles a more advanced and powerful application of conditional expectation within the framework of multivariate normal distributions. The problem [@problem_id:1381938] demonstrates how to find an expectation under a linear constraint, a scenario central to statistical regression, signal filtering, and portfolio theory. Successfully solving this requires leveraging the elegant properties of Gaussian random vectors and matrix algebra, showcasing how these tools simplify seemingly complex conditioning problems.", "problem": "Consider a set of $n$ random variables, $X_1, X_2, \\dots, X_n$, which are independent and identically distributed (i.i.d.) as standard normal random variables, i.e., $X_i \\sim \\mathcal{N}(0, 1)$ for all $i = 1, \\dots, n$. Let $a_1, a_2, \\dots, a_n$ be a set of given non-zero real constants, and let $s$ be a given real number.\n\nYour task is to compute the conditional expectation of the sum of the squares of these random variables, given that their weighted sum is equal to $s$. Specifically, find an expression for:\n$$ \\mathbb{E}\\left[ \\sum_{i=1}^n X_i^2 \\bigg| \\sum_{i=1}^n a_i X_i = s \\right] $$\nExpress your final answer as an analytic expression in terms of $n$, $s$, and the constants $a_i$.", "solution": "Let $X = (X_{1},\\dots,X_{n})^{\\top}$ with $X \\sim \\mathcal{N}(0, I_{n})$, and let $a = (a_{1},\\dots,a_{n})^{\\top}$ with all $a_{i} \\neq 0$. Define the linear statistic $Y = a^{\\top}X$. Since $X$ is multivariate normal, the conditional distribution $X \\mid Y=s$ is also normal with mean and covariance given by the standard Gaussian conditioning formulas:\n$$\n\\mu = \\operatorname{Cov}(X,Y)\\operatorname{Var}(Y)^{-1}s, \n\\qquad \n\\Sigma = \\operatorname{Cov}(X) - \\operatorname{Cov}(X,Y)\\operatorname{Var}(Y)^{-1}\\operatorname{Cov}(Y,X).\n$$\nHere, $\\operatorname{Cov}(X) = I_{n}$, $\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, a^{\\top}X) = a$, and $\\operatorname{Var}(Y) = a^{\\top}a$. Therefore,\n$$\n\\mu = \\frac{a}{a^{\\top}a}\\,s, \n\\qquad \n\\Sigma = I_{n} - \\frac{a a^{\\top}}{a^{\\top}a}.\n$$\n\nWe need $\\mathbb{E}\\!\\left[\\sum_{i=1}^{n} X_{i}^{2} \\mid a^{\\top}X = s\\right] = \\mathbb{E}\\!\\left[X^{\\top}X \\mid Y=s\\right]$. For any random vector with mean $\\mu$ and covariance $\\Sigma$, $\\mathbb{E}[X^{\\top}X] = \\operatorname{tr}(\\Sigma) + \\mu^{\\top}\\mu$. Hence,\n$$\n\\mathbb{E}\\!\\left[X^{\\top}X \\mid Y=s\\right] = \\operatorname{tr}\\!\\left(I_{n} - \\frac{a a^{\\top}}{a^{\\top}a}\\right) + \\left\\|\\frac{a}{a^{\\top}a}\\,s\\right\\|^{2}.\n$$\nCompute each term:\n$$\n\\operatorname{tr}\\!\\left(I_{n} - \\frac{a a^{\\top}}{a^{\\top}a}\\right) = \\operatorname{tr}(I_{n}) - \\frac{\\operatorname{tr}(a a^{\\top})}{a^{\\top}a} = n - \\frac{a^{\\top}a}{a^{\\top}a} = n - 1,\n$$\nand\n$$\n\\left\\|\\frac{a}{a^{\\top}a}\\,s\\right\\|^{2} = \\frac{s^{2} \\, a^{\\top}a}{(a^{\\top}a)^{2}} = \\frac{s^{2}}{a^{\\top}a}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\sum_{i=1}^{n} X_{i}^{2} \\bigg| \\sum_{i=1}^{n} a_{i} X_{i} = s\\right] = n - 1 + \\frac{s^{2}}{a^{\\top}a} = n - 1 + \\frac{s^{2}}{\\sum_{i=1}^{n} a_{i}^{2}}.\n$$", "answer": "$$\\boxed{n - 1 + \\frac{s^{2}}{\\sum_{i=1}^{n} a_{i}^{2}}}$$", "id": "1381938"}]}