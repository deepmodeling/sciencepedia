{"hands_on_practices": [{"introduction": "This first practice problem serves as a fundamental building block for understanding expectation as an integral. We will directly apply the core definition, $\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx$, to calculate the expected value of a function of a random variable. This exercise [@problem_id:1300766] is designed to solidify your grasp of this formula while sharpening essential calculus skills, such as handling symmetrical integrands and performing integration by parts within a probabilistic context.", "problem": "A continuous random variable $X$ is described by a Laplace distribution. Its Probability Density Function (PDF) is given by\n$$f_X(x) = \\frac{\\lambda}{2} \\exp(-\\lambda |x|)$$\nfor all real numbers $x$, where $\\lambda$ is a positive constant.\n\nDetermine the expected absolute value of this random variable, which is denoted as $\\mathbb{E}[|X|]$. Your answer should be an expression in terms of $\\lambda$.", "solution": "We use the definition of expectation for a function of a continuous random variable: for any measurable function $g$, $\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_{X}(x) \\, dx$. Here, $g(x) = |x|$ and $f_{X}(x) = \\frac{\\lambda}{2} \\exp(-\\lambda |x|)$ with $\\lambda > 0$. Therefore,\n$$\n\\mathbb{E}[|X|] = \\int_{-\\infty}^{\\infty} |x| \\cdot \\frac{\\lambda}{2} \\exp(-\\lambda |x|) \\, dx.\n$$\nThe integrand is an even function because both $|x|$ and $\\exp(-\\lambda |x|)$ are even. Using symmetry,\n$$\n\\mathbb{E}[|X|] = 2 \\int_{0}^{\\infty} x \\cdot \\frac{\\lambda}{2} \\exp(-\\lambda x) \\, dx = \\lambda \\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx.\n$$\nEvaluate the remaining integral by integration by parts. Let $u = x$ and $dv = \\exp(-\\lambda x) \\, dx$, so $du = dx$ and $v = -\\frac{1}{\\lambda} \\exp(-\\lambda x)$. Then\n$$\n\\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx = \\left[ -\\frac{x}{\\lambda} \\exp(-\\lambda x) \\right]_{0}^{\\infty} + \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-\\lambda x) \\, dx.\n$$\nSince $\\lim_{x \\to \\infty} x \\exp(-\\lambda x) = 0$ for $\\lambda > 0$, the boundary term vanishes, and\n$$\n\\int_{0}^{\\infty} \\exp(-\\lambda x) \\, dx = \\frac{1}{\\lambda}.\n$$\nThus,\n$$\n\\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx = \\frac{1}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{1}{\\lambda^{2}}.\n$$\nSubstituting back,\n$$\n\\mathbb{E}[|X|] = \\lambda \\cdot \\frac{1}{\\lambda^{2}} = \\frac{1}{\\lambda}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\lambda}}$$", "id": "1300766"}, {"introduction": "Having practiced a direct calculation, we now explore a more abstract and profound concept that reveals a universal property of continuous random variables. This problem investigates what happens when we apply a special transformation: passing a random variable $X$ through its own Cumulative Distribution Function (CDF) to create a new variable $Y = F_X(X)$. Solving this problem [@problem_id:1300774] uncovers the elegant and surprising result of the Probability Integral Transform, a cornerstone of statistical theory and simulation methods.", "problem": "Let $X$ be a continuous random variable with a probability density function (PDF) denoted by $f_X(x)$ and a corresponding cumulative distribution function (CDF) denoted by $F_X(x)$. Assume that the CDF $F_X(x)$ is continuous and strictly increasing over the support of $X$. A new random variable $Y$ is created by applying the CDF transformation to $X$, such that $Y = F_X(X)$.\n\nDetermine the expected value of the random variable $Y$.", "solution": "Let $Y=F_{X}(X)$ where $F_{X}$ is continuous and strictly increasing on the support of $X$. Define the CDF of $Y$:\n$$\nF_{Y}(y)=\\mathbb{P}(Y\\leq y)=\\mathbb{P}\\left(F_{X}(X)\\leq y\\right).\n$$\nBecause $F_{X}$ is strictly increasing, it is invertible on its range with inverse $F_{X}^{-1}$. For $0\\leq y\\leq 1$,\n$$\n\\{F_{X}(X)\\leq y\\}=\\{X\\leq F_{X}^{-1}(y)\\},\n$$\nso\n$$\nF_{Y}(y)=\\mathbb{P}\\left(X\\leq F_{X}^{-1}(y)\\right)=F_{X}\\!\\left(F_{X}^{-1}(y)\\right)=y.\n$$\nFor $y<0$, $F_{Y}(y)=0$, and for $y>1$, $F_{Y}(y)=1$. Hence the PDF of $Y$ is\n$$\nf_{Y}(y)=\\frac{d}{dy}F_{Y}(y)=\\begin{cases}\n1, & 0<y<1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nTherefore,\n$$\n\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y\\,f_{Y}(y)\\,dy=\\int_{0}^{1} y\\,dy=\\left.\\frac{y^{2}}{2}\\right|_{0}^{1}=\\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1300774"}, {"introduction": "Real-world systems often depend on the interplay of multiple random events, and this practice extends our analysis to such scenarios. The problem [@problem_id:1300750] models a common situation in engineering and technology where a system's response time is determined by the first of two independent signals. To find the expected outcome, you will employ a powerful, general strategy: first, derive the probability distribution of the resulting variable, $Y = \\min(T_A, T_B)$, and then use that distribution to compute its expectation.", "problem": "A prototype self-driving car is designed to execute a critical lane-change maneuver within a specific time window. Two independent sensor systems, System A and System B, are used to decide the exact moment for the maneuver. The time at which each system determines it is optimal to act is a random variable, independent of the other and uniformly distributed over the interval from $t=0$ to a maximum time $t=T$. For safety reasons, the car is programmed to initiate the maneuver as soon as the first signal is received from either system. Let the random variables for the decision times be $T_A$ and $T_B$. Both are independent and identically distributed according to a uniform distribution on $[0, T]$.\n\nFind the expected time at which the car initiates the maneuver. Express your answer as a closed-form analytic expression in terms of $T$.", "solution": "Let $T_{A}$ and $T_{B}$ be independent and identically distributed as $\\text{Uniform}(0, T)$. Define $X = \\min(T_{A}, T_{B})$. We seek $\\mathbb{E}[X]$.\n\nFor $x \\in [0, T]$, the survival function of $X$ is\n$$\n\\mathbb{P}(X > x) = \\mathbb{P}(T_{A} > x,\\, T_{B} > x) = \\mathbb{P}(T_{A} > x)\\,\\mathbb{P}(T_{B} > x) = \\left(\\frac{T - x}{T}\\right)^{2},\n$$\nusing independence and the fact that for a uniform distribution on $[0, T]$, $\\mathbb{P}(T_{A} > x) = \\frac{T - x}{T}$ for $x \\in [0, T]$. Hence the cumulative distribution function of $X$ is\n$$\nF_{X}(x) = \\begin{cases}\n0, & x < 0, \\\\\n1 - \\left(\\frac{T - x}{T}\\right)^{2}, & 0 \\leq x \\leq T, \\\\\n1, & x > T.\n\\end{cases}\n$$\nDifferentiating on $(0, T)$ gives the probability density function\n$$\nf_{X}(x) = \\frac{d}{dx}F_{X}(x) = \\frac{2\\,(T - x)}{T^{2}}, \\quad 0 \\leq x \\leq T.\n$$\nTherefore,\n$$\n\\mathbb{E}[X] = \\int_{0}^{T} x\\, f_{X}(x)\\, dx = \\int_{0}^{T} x \\cdot \\frac{2\\,(T - x)}{T^{2}}\\, dx = \\frac{2}{T^{2}} \\int_{0}^{T} \\left(Tx - x^{2}\\right) dx.\n$$\nEvaluate the integral explicitly:\n$$\n\\int_{0}^{T} \\left(Tx - x^{2}\\right) dx = \\left[\\frac{T x^{2}}{2} - \\frac{x^{3}}{3}\\right]_{0}^{T} = \\frac{T^{3}}{2} - \\frac{T^{3}}{3} = T^{3}\\left(\\frac{1}{2} - \\frac{1}{3}\\right) = \\frac{T^{3}}{6}.\n$$\nHence,\n$$\n\\mathbb{E}[X] = \\frac{2}{T^{2}} \\cdot \\frac{T^{3}}{6} = \\frac{T}{3}.\n$$", "answer": "$$\\boxed{\\frac{T}{3}}$$", "id": "1300750"}]}