{"hands_on_practices": [{"introduction": "Understanding the covariance matrix begins with mastering its construction. This first exercise provides a direct, hands-on opportunity to build a covariance matrix from the ground up, starting with a given joint probability mass function for two discrete random variables. By working through the necessary calculations for expectations, variances, and covariance, you will solidify your understanding of the essential components that define the relationships between variables [@problem_id:1294505].", "problem": "Consider a simple game involving two discrete random variables, $X$ and $Y$. The variable $X$ can take values from the set $\\{0, 1\\}$, and the variable $Y$ can take values from the set $\\{0, 1, 2\\}$. The joint probability mass function, $p(x, y) = P(X=x, Y=y)$, for these two variables is defined as follows:\n\n$p(0, 0) = \\frac{1}{10}$\n$p(0, 1) = \\frac{2}{10}$\n$p(0, 2) = 0$\n$p(1, 0) = \\frac{3}{10}$\n$p(1, 1) = \\frac{1}{10}$\n$p(1, 2) = \\frac{3}{10}$\n\nYour task is to compute the covariance matrix for the random vector $(X, Y)$. The covariance matrix, $K$, is a $2 \\times 2$ matrix where the element $K_{ij}$ is the covariance of the $i$-th and $j$-th random variables. For the vector $(X, Y)$, this is given by:\n$$\nK = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X,Y) \\\\ \\text{Cov}(Y,X) & \\text{Var}(Y) \\end{pmatrix}\n$$\nProvide the resulting $2 \\times 2$ matrix.", "solution": "To find the covariance matrix, we need to calculate three quantities: the variance of $X$, $\\text{Var}(X)$; the variance of $Y$, $\\text{Var}(Y)$; and the covariance between $X$ and $Y$, $\\text{Cov}(X,Y)$. Recall that $\\text{Cov}(Y,X) = \\text{Cov}(X,Y)$.\n\nFirst, let's find the marginal probability mass functions for $X$ and $Y$.\nThe marginal probability for $X$ is $P(X=x) = \\sum_{y} p(x,y)$.\n$P(X=0) = p(0,0) + p(0,1) + p(0,2) = \\frac{1}{10} + \\frac{2}{10} + 0 = \\frac{3}{10}$.\n$P(X=1) = p(1,0) + p(1,1) + p(1,2) = \\frac{3}{10} + \\frac{1}{10} + \\frac{3}{10} = \\frac{7}{10}$.\n\nThe marginal probability for $Y$ is $P(Y=y) = \\sum_{x} p(x,y)$.\n$P(Y=0) = p(0,0) + p(1,0) = \\frac{1}{10} + \\frac{3}{10} = \\frac{4}{10} = \\frac{2}{5}$.\n$P(Y=1) = p(0,1) + p(1,1) = \\frac{2}{10} + \\frac{1}{10} = \\frac{3}{10}$.\n$P(Y=2) = p(0,2) + p(1,2) = 0 + \\frac{3}{10} = \\frac{3}{10}$.\n\nNext, we compute the expected values (means) of $X$ and $Y$.\n$\\text{E}[X] = \\sum_{x} x P(X=x) = (0)P(X=0) + (1)P(X=1) = 0 \\cdot \\frac{3}{10} + 1 \\cdot \\frac{7}{10} = \\frac{7}{10}$.\n$\\text{E}[Y] = \\sum_{y} y P(Y=y) = (0)P(Y=0) + (1)P(Y=1) + (2)P(Y=2) = 0 \\cdot \\frac{4}{10} + 1 \\cdot \\frac{3}{10} + 2 \\cdot \\frac{3}{10} = \\frac{3}{10} + \\frac{6}{10} = \\frac{9}{10}$.\n\nNow we calculate the variances. The variance of a random variable $Z$ is given by $\\text{Var}(Z) = \\text{E}[Z^2] - (\\text{E}[Z])^2$. We need to compute $\\text{E}[X^2]$ and $\\text{E}[Y^2]$.\n$\\text{E}[X^2] = \\sum_{x} x^2 P(X=x) = (0^2)P(X=0) + (1^2)P(X=1) = 0 \\cdot \\frac{3}{10} + 1 \\cdot \\frac{7}{10} = \\frac{7}{10}$.\n$\\text{Var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2 = \\frac{7}{10} - \\left(\\frac{7}{10}\\right)^2 = \\frac{7}{10} - \\frac{49}{100} = \\frac{70}{100} - \\frac{49}{100} = \\frac{21}{100}$.\n\n$\\text{E}[Y^2] = \\sum_{y} y^2 P(Y=y) = (0^2)P(Y=0) + (1^2)P(Y=1) + (2^2)P(Y=2) = 0 \\cdot \\frac{4}{10} + 1 \\cdot \\frac{3}{10} + 4 \\cdot \\frac{3}{10} = \\frac{3}{10} + \\frac{12}{10} = \\frac{15}{10} = \\frac{3}{2}$.\n$\\text{Var}(Y) = \\text{E}[Y^2] - (\\text{E}[Y])^2 = \\frac{15}{10} - \\left(\\frac{9}{10}\\right)^2 = \\frac{15}{10} - \\frac{81}{100} = \\frac{150}{100} - \\frac{81}{100} = \\frac{69}{100}$.\n\nFinally, we compute the covariance, $\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y]$. We first need to find $\\text{E}[XY]$.\n$\\text{E}[XY] = \\sum_{x,y} xy \\cdot p(x,y)$. The non-zero terms are:\n$(1)(1)p(1,1) = 1 \\cdot \\frac{1}{10} = \\frac{1}{10}$.\n$(1)(2)p(1,2) = 2 \\cdot \\frac{3}{10} = \\frac{6}{10}$.\nSo, $\\text{E}[XY] = \\frac{1}{10} + \\frac{6}{10} = \\frac{7}{10}$.\n\nNow we can calculate the covariance:\n$\\text{Cov}(X,Y) = \\text{E}[XY] - \\text{E}[X]\\text{E}[Y] = \\frac{7}{10} - \\left(\\frac{7}{10}\\right)\\left(\\frac{9}{10}\\right) = \\frac{7}{10} - \\frac{63}{100} = \\frac{70}{100} - \\frac{63}{100} = \\frac{7}{100}$.\n\nWe now assemble the covariance matrix:\n$$\nK = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X,Y) \\\\ \\text{Cov}(Y,X) & \\text{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} \\frac{21}{100} & \\frac{7}{100} \\\\ \\frac{7}{100} & \\frac{69}{100} \\end{pmatrix}\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{21}{100} & \\frac{7}{100} \\\\ \\frac{7}{100} & \\frac{69}{100} \\end{pmatrix}}$$", "id": "1294505"}, {"introduction": "The covariance matrix does more than just quantify relationships; it reveals the underlying structure and dimensionality of our data. This problem explores a critical concept: the effect of linear dependence on the covariance matrix's rank. By examining a vector where all components are simple multiples of a single random variable, you will discover why the resulting covariance matrix is singular and what this implies about redundancy within the system [@problem_id:1294493].", "problem": "Let $X$ be a real-valued random variable with a finite, non-zero variance. Consider the 3-dimensional random vector $\\mathbf{Y}$ defined by its transpose, $\\mathbf{Y}^T = (X, 2X, 3X)$. What is the rank of the covariance matrix of the random vector $\\mathbf{Y}$?", "solution": "Let $\\mathbf{a}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}$ so that $\\mathbf{Y}=\\mathbf{a}X$. Then $\\mathbb{E}[\\mathbf{Y}]=\\mathbf{a}\\,\\mathbb{E}[X]$ and\n$$\n\\operatorname{Cov}(\\mathbf{Y})=\\mathbb{E}\\!\\left[(\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}])(\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}])^{T}\\right]\n=\\mathbb{E}\\!\\left[\\mathbf{a}(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])\\mathbf{a}^{T}\\right]\n=\\operatorname{Var}(X)\\,\\mathbf{a}\\mathbf{a}^{T}.\n$$\nSince $\\operatorname{Var}(X)\\neq 0$ and $\\mathbf{a}\\neq \\mathbf{0}$, the matrix $\\mathbf{a}\\mathbf{a}^{T}$ is an outer product of a nonzero vector with itself, which has rank $1$. Multiplying by the nonzero scalar $\\operatorname{Var}(X)$ does not change the rank. Therefore, the rank of the covariance matrix is $1$.", "answer": "$$\\boxed{1}$$", "id": "1294493"}, {"introduction": "We now move from theory to a practical application common in signal processing and statistics: signal filtering and decorrelation. This exercise challenges you to use the properties of the covariance matrix to solve an optimization problemâ€”finding the best way to subtract a portion of one signal from another to minimize the resulting signal's power. This process of creating uncorrelated variables is a cornerstone of many advanced data analysis techniques, such as Principal Component Analysis (PCA) [@problem_id:1294483].", "problem": "In a signal processing application, we are working with a pair of correlated random signals represented by a two-dimensional random vector $\\mathbf{W} = (X, Y)^T$. Both signals $X$ and $Y$ have been centered, meaning their expected values are zero, i.e., $E[X] = E[Y] = 0$. The statistical relationship between these signals is captured by their covariance matrix, $K_W$, which is given by:\n$$\nK_W = \\begin{pmatrix} \\mathrm{Var}(X) & \\mathrm{Cov}(X, Y) \\\\ \\mathrm{Cov}(Y, X) & \\mathrm{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 5 \\end{pmatrix}\n$$\nTo filter the signal $Y$, we construct a new signal $Z = Y - \\alpha X$, where $\\alpha$ is a real-valued scalar coefficient. The goal is to choose $\\alpha$ such that the power of the filtered signal $Z$, which is equivalent to its variance, is minimized.\n\nLet $\\alpha_0$ be the specific value of $\\alpha$ that minimizes $\\mathrm{Var}(Y - \\alpha X)$. Now, consider a new random vector $\\mathbf{V} = (X, Y - \\alpha_0 X)^T$. What is the determinant of the covariance matrix of this new vector $\\mathbf{V}$? Your final answer should be a single real number.", "solution": "We are given a centered random vector $\\mathbf{W}=(X,Y)^{T}$ with covariance matrix\n$$\nK_{W}=\\begin{pmatrix} \\mathrm{Var}(X) & \\mathrm{Cov}(X,Y) \\\\ \\mathrm{Cov}(Y,X) & \\mathrm{Var}(Y) \\end{pmatrix}=\\begin{pmatrix} 9 & 3 \\\\ 3 & 5 \\end{pmatrix}.\n$$\nDefine $Z=Y-\\alpha X$. Using the variance formula for linear combinations, namely $\\mathrm{Var}(aU+bV)=a^{2}\\mathrm{Var}(U)+b^{2}\\mathrm{Var}(V)+2ab\\,\\mathrm{Cov}(U,V)$, we obtain\n$$\n\\mathrm{Var}(Y-\\alpha X)=\\mathrm{Var}(Y)-2\\alpha\\,\\mathrm{Cov}(X,Y)+\\alpha^{2}\\mathrm{Var}(X).\n$$\nTo minimize this quadratic in $\\alpha$, differentiate with respect to $\\alpha$ and set the derivative to zero:\n$$\n\\frac{d}{d\\alpha}\\mathrm{Var}(Y-\\alpha X)=-2\\,\\mathrm{Cov}(X,Y)+2\\alpha\\,\\mathrm{Var}(X)=0,\n$$\nwhich yields\n$$\n\\alpha_{0}=\\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}=\\frac{3}{9}=\\frac{1}{3}.\n$$\nThe second derivative equals $2\\,\\mathrm{Var}(X)>0$, so this is indeed the minimizer. Substituting $\\alpha_{0}$ back into the variance gives the well-known residual variance formula\n$$\n\\mathrm{Var}(Y-\\alpha_{0}X)=\\mathrm{Var}(Y)-\\frac{\\mathrm{Cov}(X,Y)^{2}}{\\mathrm{Var}(X)}=5-\\frac{3^{2}}{9}=5-1=4.\n$$\nMoreover, the minimizing choice makes the residual uncorrelated with $X$:\n$$\n\\mathrm{Cov}\\bigl(X,\\,Y-\\alpha_{0}X\\bigr)=\\mathrm{Cov}(X,Y)-\\alpha_{0}\\mathrm{Var}(X)=3-\\frac{1}{3}\\cdot 9=0.\n$$\nTherefore, for $\\mathbf{V}=(X,\\,Y-\\alpha_{0}X)^{T}$, the covariance matrix is diagonal,\n$$\nK_{V}=\\begin{pmatrix} \\mathrm{Var}(X) & 0 \\\\ 0 & \\mathrm{Var}(Y-\\alpha_{0}X) \\end{pmatrix}=\\begin{pmatrix} 9 & 0 \\\\ 0 & 4 \\end{pmatrix},\n$$\nand its determinant is the product of the diagonal entries:\n$$\n\\det(K_{V})=9\\cdot 4=36.\n$$\nAs a consistency check, note that $\\mathbf{V}=M \\mathbf{W}$ with $M=\\begin{pmatrix}1&0\\\\ -\\alpha_{0}&1\\end{pmatrix}$, so $K_{V}=M K_{W} M^{T}$ and thus $\\det(K_{V})=\\det(M)^{2}\\det(K_{W})=\\det(K_{W})=9\\cdot 5-3^{2}=36$, in agreement with the direct computation.", "answer": "$$\\boxed{36}$$", "id": "1294483"}]}