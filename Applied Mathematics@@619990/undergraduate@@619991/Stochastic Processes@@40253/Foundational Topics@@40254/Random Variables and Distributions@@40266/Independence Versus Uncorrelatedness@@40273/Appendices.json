{"hands_on_practices": [{"introduction": "Let's begin with a foundational scenario commonly found in digital communications. This problem explores the relationship between the sum and difference of two independent binary signals. By working through the calculations for covariance and joint probabilities, you will gain hands-on experience in distinguishing between uncorrelatedness, which is a measure of linear relationship, and the much stronger condition of statistical independence [@problem_id:1308437].", "id": "1308437", "problem": "In a simplified model of a digital communication system, two independent signal bits, $B_1$ and $B_2$, are transmitted. Each bit is a random variable that can take the value 0 or 1 with equal probability. Let $X_1$ and $X_2$ be the random variables representing the values of bits $B_1$ and $B_2$, respectively.\n\nTwo new quantities are derived from these bits for error-checking purposes:\n1.  The sum, $S$, is defined as $S = X_1 + X_2$.\n2.  The difference, $D$, is defined as $D = X_1 - X_2$.\n\nAnalyze the statistical relationship between the sum $S$ and the difference $D$. Which of the following statements is true?\n\nA. $S$ and $D$ are independent and uncorrelated.\nB. $S$ and $D$ are independent but correlated.\nC. $S$ and $D$ are dependent but uncorrelated.\nD. $S$ and $D$ are dependent and correlated.\n\n", "solution": "The problem asks us to determine whether the random variables $S = X_1 + X_2$ and $D = X_1 - X_2$ are independent and/or uncorrelated, given that $X_1$ and $X_2$ are independent and identically distributed Bernoulli(1/2) random variables.\n\n**Part 1: Checking for Correlation**\n\nTwo random variables are uncorrelated if their covariance is zero. The covariance of $S$ and $D$ is given by the formula:\n$$ \\text{Cov}(S, D) = E[SD] - E[S]E[D] $$\nWe will calculate each term separately.\n\nFirst, let's find the expectations of $X_1$ and $X_2$. Since they are Bernoulli(1/2) variables:\n$$ E[X_1] = E[X_2] = (0 \\times P(X_i=0)) + (1 \\times P(X_i=1)) = (0 \\times \\frac{1}{2}) + (1 \\times \\frac{1}{2}) = \\frac{1}{2} $$\n\nNow, using the linearity of expectation, we find the expectations of $S$ and $D$:\n$$ E[S] = E[X_1 + X_2] = E[X_1] + E[X_2] = \\frac{1}{2} + \\frac{1}{2} = 1 $$\n$$ E[D] = E[X_1 - X_2] = E[X_1] - E[X_2] = \\frac{1}{2} - \\frac{1}{2} = 0 $$\n\nThe product of their expectations is:\n$$ E[S]E[D] = 1 \\times 0 = 0 $$\n\nNext, we calculate the expectation of the product, $E[SD]$.\n$$ SD = (X_1 + X_2)(X_1 - X_2) = X_1^2 - X_2^2 $$\nTherefore, using the linearity of expectation again:\n$$ E[SD] = E[X_1^2 - X_2^2] = E[X_1^2] - E[X_2^2] $$\nTo find $E[X_i^2]$, we use the definition of expectation. For a Bernoulli random variable $X$, its value is either 0 or 1. Thus, $X^2$ is also either $0^2=0$ or $1^2=1$. This means $X^2 = X$ for any Bernoulli variable.\nSo, we have:\n$$ E[X_1^2] = E[X_1] = \\frac{1}{2} $$\n$$ E[X_2^2] = E[X_2] = \\frac{1}{2} $$\nSubstituting these back into the expression for $E[SD]$:\n$$ E[SD] = \\frac{1}{2} - \\frac{1}{2} = 0 $$\n\nFinally, we can calculate the covariance:\n$$ \\text{Cov}(S, D) = E[SD] - E[S]E[D] = 0 - 0 = 0 $$\nSince the covariance is zero, the random variables $S$ and $D$ are **uncorrelated**. This eliminates options B and D.\n\n**Part 2: Checking for Independence**\n\nTwo random variables $S$ and $D$ are independent if and only if $P(S=s, D=d) = P(S=s)P(D=d)$ for all possible values $s$ and $d$. To prove they are dependent, we only need to find one counterexample.\n\nLet's list all possible outcomes for the pair $(X_1, X_2)$ and the corresponding values of $S$ and $D$. Since $X_1$ and $X_2$ are independent, each of the four outcomes has a probability of $\\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\n\n- If $(X_1, X_2) = (0, 0)$, then $S = 0+0=0$ and $D = 0-0=0$.\n- If $(X_1, X_2) = (0, 1)$, then $S = 0+1=1$ and $D = 0-1=-1$.\n- If $(X_1, X_2) = (1, 0)$, then $S = 1+0=1$ and $D = 1-0=1$.\n- If $(X_1, X_2) = (1, 1)$, then $S = 1+1=2$ and $D = 1-1=0$.\n\nFrom this, we can find the marginal probability distributions for $S$ and $D$.\nThe possible values for $S$ are $\\{0, 1, 2\\}$.\n$$ P(S=0) = P(X_1=0, X_2=0) = \\frac{1}{4} $$\n$$ P(S=1) = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n$$ P(S=2) = P(X_1=1, X_2=1) = \\frac{1}{4} $$\n\nThe possible values for $D$ are $\\{-1, 0, 1\\}$.\n$$ P(D=-1) = P(X_1=0, X_2=1) = \\frac{1}{4} $$\n$$ P(D=0) = P(X_1=0, X_2=0) + P(X_1=1, X_2=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n$$ P(D=1) = P(X_1=1, X_2=0) = \\frac{1}{4} $$\n\nNow let's test the independence condition. Consider the event $(S=0, D=1)$.\nThe joint probability is $P(S=0, D=1) = 0$, because there is no outcome for $(X_1, X_2)$ that results in $S=0$ and $D=1$ simultaneously.\n\nThe product of the marginal probabilities is:\n$$ P(S=0) \\times P(D=1) = \\frac{1}{4} \\times \\frac{1}{4} = \\frac{1}{16} $$\n\nSince $P(S=0, D=1) \\neq P(S=0)P(D=1)$, because $0 \\neq \\frac{1}{16}$, the random variables $S$ and $D$ are not independent. They are **dependent**.\n\n**Conclusion**\n\nWe have shown that $S$ and $D$ are dependent but uncorrelated. This corresponds to option C.", "answer": "$$\\boxed{C}$$"}, {"introduction": "Building on our computational foundation, this next exercise offers powerful geometric intuition. We will analyze a point chosen uniformly from a specific shapeâ€”a square with its center removed. This problem wonderfully illustrates how the geometry of the underlying probability space can reveal dependence at a glance, even when the variables have zero covariance due to symmetry [@problem_id:1308387].", "id": "1308387", "problem": "A point with coordinates $(X, Y)$ is selected from a two-dimensional region $S$ according to a uniform probability distribution. The region $S$ is defined as the area between two concentric squares centered at the origin of a Cartesian coordinate system. The outer square has vertices at $(\\pm 2, \\pm 2)$, and the inner square has vertices at $(\\pm 1, \\pm 1)$. Formally, the region is $S = \\{(x, y) \\in \\mathbb{R}^2 \\mid x \\in [-2, 2], y \\in [-2, 2]\\} \\setminus \\{(x, y) \\in \\mathbb{R}^2 \\mid x \\in (-1, 1), y \\in (-1, 1)\\}$.\n\nLet $\\text{Cov}(X,Y)$ denote the covariance between the random variables $X$ and $Y$. Which of the following statements is true?\n\nA. $\\text{Cov}(X,Y) = 0$ and $X$ and $Y$ are independent.\nB. $\\text{Cov}(X,Y) = 0$ and $X$ and $Y$ are not independent.\nC. $\\text{Cov}(X,Y) = 1$ and $X$ and $Y$ are independent.\nD. $\\text{Cov}(X,Y) = 1$ and $X$ and $Y$ are not independent.\nE. The covariance cannot be determined from the information given.\n\n", "solution": "The problem asks us to determine the covariance of the random variables $X$ and $Y$ and to establish whether they are statistically independent. The point $(X, Y)$ is chosen uniformly from the region $S$.\n\nFirst, we determine the joint probability density function (PDF), $f_{X,Y}(x, y)$. Since the distribution is uniform over the region $S$, the PDF is constant on $S$ and zero elsewhere. The value of the constant is the reciprocal of the area of $S$.\n\nThe area of the outer square with vertices at $(\\pm 2, \\pm 2)$ is $A_{\\text{outer}} = (2 - (-2)) \\times (2 - (-2)) = 4 \\times 4 = 16$.\nThe area of the inner square with vertices at $(\\pm 1, \\pm 1)$ is $A_{\\text{inner}} = (1 - (-1)) \\times (1 - (-1)) = 2 \\times 2 = 4$.\nThe area of the region $S$ is the difference between these two areas: $A_S = A_{\\text{outer}} - A_{\\text{inner}} = 16 - 4 = 12$.\n\nThe joint PDF is therefore:\n$$\nf_{X,Y}(x, y) = \\begin{cases} \\frac{1}{12} & \\text{if } (x, y) \\in S \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\nNext, we calculate the covariance, which is defined as $\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$. We need to find the expected values $E[X]$, $E[Y]$, and $E[XY]$.\n\nTo find $E[X]$, we compute the integral:\n$$\nE[X] = \\iint_{\\mathbb{R}^2} x f_{X,Y}(x, y) \\,dx\\,dy = \\frac{1}{12} \\iint_S x \\,dx\\,dy\n$$\nThe region of integration $S$ is symmetric with respect to the y-axis. This means for any point $(x, y) \\in S$, the point $(-x, y)$ is also in $S$. The integrand function is $g(x) = x$, which is an odd function of $x$. The integral of an odd function over a symmetric domain is zero. Thus, $E[X] = 0$.\nA similar argument applies to $E[Y]$. The region $S$ is symmetric with respect to the x-axis, and the integrand $h(y) = y$ is an odd function of $y$. Therefore, $E[Y] = 0$.\n\nNow, we calculate $E[XY]$:\n$$\nE[XY] = \\iint_{\\mathbb{R}^2} xy f_{X,Y}(x, y) \\,dx\\,dy = \\frac{1}{12} \\iint_S xy \\,dx\\,dy\n$$\nWe can evaluate this integral explicitly. The integral over $S$ is the integral over the outer square minus the integral over the inner square.\n$$\n\\iint_S xy \\,dx\\,dy = \\int_{-2}^{2} \\int_{-2}^{2} xy \\,dx\\,dy - \\int_{-1}^{1} \\int_{-1}^{1} xy \\,dx\\,dy\n$$\nFor the first term:\n$$\n\\int_{-2}^{2} \\int_{-2}^{2} xy \\,dx\\,dy = \\left( \\int_{-2}^{2} x \\,dx \\right) \\left( \\int_{-2}^{2} y \\,dy \\right) = \\left[ \\frac{x^2}{2} \\right]_{-2}^{2} \\left[ \\frac{y^2}{2} \\right]_{-2}^{2} = (2-2)(2-2) = 0\n$$\nFor the second term:\n$$\n\\int_{-1}^{1} \\int_{-1}^{1} xy \\,dx\\,dy = \\left( \\int_{-1}^{1} x \\,dx \\right) \\left( \\int_{-1}^{1} y \\,dy \\right) = \\left[ \\frac{x^2}{2} \\right]_{-1}^{1} \\left[ \\frac{y^2}{2} \\right]_{-1}^{1} = \\left(\\frac{1}{2}-\\frac{1}{2}\\right)\\left(\\frac{1}{2}-\\frac{1}{2}\\right) = 0\n$$\nSo, $E[XY] = \\frac{1}{12} (0 - 0) = 0$.\nNow we can compute the covariance:\n$$\n\\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = 0 - (0)(0) = 0\n$$\nSince the covariance is zero, the random variables $X$ and $Y$ are uncorrelated.\n\nNext, we address the question of independence. Two random variables $X$ and $Y$ are independent if and only if their joint PDF is the product of their marginal PDFs for all $x$ and $y$, i.e., $f_{X,Y}(x, y) = f_X(x) f_Y(y)$.\nA necessary condition for independence of continuous random variables with a uniform joint distribution is that the support of the distribution (the region where the PDF is non-zero) must be a rectangle with sides parallel to the coordinate axes. The support region $S$ is a square with a square hole, which is not a rectangle. This suggests $X$ and $Y$ are not independent.\n\nTo prove this formally, let's consider a point $(x,y)$ and check the independence condition. Let's pick the point $(0.5, 0.5)$. This point lies within the inner square, so it is not in the region $S$. Therefore, the joint PDF at this point is $f_{X,Y}(0.5, 0.5) = 0$.\n\nNow let's compute the marginal PDFs, $f_X(x)$ and $f_Y(y)$.\nThe marginal PDF of $X$ is given by $f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) \\,dy$.\nFor $x=0.5$, which is in the interval $(-1, 1)$, the point $(0.5, y)$ is in $S$ if $y \\in [-2, -1] \\cup [1, 2]$.\nSo, for $x=0.5$:\n$$\nf_X(0.5) = \\int_{-2}^{-1} \\frac{1}{12} \\,dy + \\int_{1}^{2} \\frac{1}{12} \\,dy = \\frac{1}{12}[y]_{-2}^{-1} + \\frac{1}{12}[y]_{1}^{2} = \\frac{1}{12}((-1) - (-2)) + \\frac{1}{12}(2 - 1) = \\frac{1}{12} + \\frac{1}{12} = \\frac{2}{12} = \\frac{1}{6}\n$$\nSince $f_X(0.5) = 1/6$, it is non-zero. By symmetry of the region $S$ and the PDF, $f_Y(y)$ has the same functional form as $f_X(x)$, so $f_Y(0.5) = 1/6$.\nThe product of the marginals at $(0.5, 0.5)$ is $f_X(0.5)f_Y(0.5) = (1/6)(1/6) = 1/36$.\nWe have found a point $(0.5, 0.5)$ where $f_{X,Y}(0.5, 0.5) = 0$ but $f_X(0.5)f_Y(0.5) = 1/36$.\nSince $f_{X,Y}(x, y) \\neq f_X(x)f_Y(y)$, the random variables $X$ and $Y$ are not independent.\n\nIn conclusion, $\\text{Cov}(X, Y) = 0$, but $X$ and $Y$ are not independent. This corresponds to option B.", "answer": "$$\\boxed{B}$$"}, {"introduction": "Our final practice problem takes us into the realm of dynamic systems, specifically a simple Markov chain modeling a particle's movement. You will analyze the relationship between the particle's position at one moment and the next after the system has settled into a steady state. This exercise reveals how dependence and uncorrelatedness are crucial concepts for describing the structure and evolution of stochastic processes over time [@problem_id:1308452].", "id": "1308452", "problem": "A particle's position is modeled by a discrete-time Markov chain, where its state at time $n$ is a random variable $X_n$. The state space for the particle's position is the set of integers $S = \\{-1, 0, 1\\}$. The transitions between states are governed by the following rules:\n\n- If the particle is at state 0, it will move to state -1 or state 1 with equal probability of 1/2 in the next time step.\n- If the particle is at state -1 or state 1, it will deterministically move to state 0 in the next time step.\n\nAssume that the Markov chain has been running for a long time and has reached its unique stationary distribution. Let the process be in this stationary regime. Calculate the following three quantities: the expectation of the particle's position, $E[X_n]$; the variance of its position, $Var(X_n)$; and the covariance of its positions at consecutive time steps, $Cov(X_n, X_{n+1})$.\n\nPresent your three results as a single row matrix in the order $[E[X_n], Var(X_n), Cov(X_n, X_{n+1})]$.\n\n", "solution": "Let the state space be ordered as $(-1,0,1)$. The transition matrix is\n$$\nP=\\begin{pmatrix}\n0 & 1 & 0\\\\\n\\frac{1}{2} & 0 & \\frac{1}{2}\\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nLet the stationary distribution be $\\pi=(\\pi_{-1},\\pi_{0},\\pi_{1})$ with $\\pi=\\pi P$ and $\\pi_{-1}+\\pi_{0}+\\pi_{1}=1$. From $\\pi=\\pi P$,\n$$\n\\pi_{-1}=\\pi_{0}\\cdot \\frac{1}{2},\\quad \\pi_{0}=\\pi_{-1}\\cdot 1+\\pi_{1}\\cdot 1,\\quad \\pi_{1}=\\pi_{0}\\cdot \\frac{1}{2}.\n$$\nBy symmetry $\\pi_{-1}=\\pi_{1}=\\frac{1}{2}\\pi_{0}$, and normalization gives\n$$\n\\pi_{-1}+\\pi_{0}+\\pi_{1}=\\frac{1}{2}\\pi_{0}+\\pi_{0}+\\frac{1}{2}\\pi_{0}=2\\pi_{0}=1 \\implies \\pi_{0}=\\frac{1}{2},\\quad \\pi_{-1}=\\pi_{1}=\\frac{1}{4}.\n$$\nExpectation in stationarity:\n$$\nE[X_{n}]=(-1)\\cdot \\frac{1}{4}+0\\cdot \\frac{1}{2}+1\\cdot \\frac{1}{4}=0.\n$$\nVariance in stationarity:\n$$\n\\operatorname{Var}(X_{n})=E[X_{n}^{2}]-(E[X_{n}])^{2}=\\left[(-1)^{2}\\cdot \\frac{1}{4}+0^{2}\\cdot \\frac{1}{2}+1^{2}\\cdot \\frac{1}{4}\\right]-0^{2}=\\frac{1}{2}.\n$$\nCovariance at lag one:\n$$\n\\operatorname{Cov}(X_{n},X_{n+1})=E[X_{n}X_{n+1}]-E[X_{n}]E[X_{n+1}].\n$$\nSince $E[X_{n}]=0$ in stationarity and, for each state $i\\in\\{-1,0,1\\}$,\n$$\nE[X_{n+1}\\mid X_{n}=i]=0,\n$$\nwe have\n$$\nE[X_{n}X_{n+1}]=E\\!\\left[X_{n}\\,E[X_{n+1}\\mid X_{n}]\\right]=E[X_{n}\\cdot 0]=0,\n$$\nhence $\\operatorname{Cov}(X_{n},X_{n+1})=0$.\n\nTherefore, the requested row matrix is $\\begin{pmatrix}0 & \\frac{1}{2} & 0\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}0 & \\frac{1}{2} & 0\\end{pmatrix}}$$"}]}