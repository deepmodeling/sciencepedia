{"hands_on_practices": [{"introduction": "The theory of Riemann-Stieltjes integration is built on the assumption that the integrator function has bounded variation. This practice helps demystify this concept by having you compute the total variation of a simple, yet highly oscillatory, \"sawtooth\" function [@problem_id:3067244]. By analyzing how the variation grows with the number of oscillations, you will gain a tangible understanding of why the \"rough\" paths of Brownian motion are unsuitable for classical integration.", "problem": "Let $T0$ and let $g:[0,T]\\to\\mathbb{R}$ be a function. The total variation $V(g,[0,T])$ over $[0,T]$ is defined in terms of partitions of the interval. Using only the core definition of total variation and elementary properties of monotone functions, perform the following tasks.\n\n1. State the definition of the total variation $V(g,[0,T])$ as a supremum over partitions.\n\n2. For a fixed positive integer $N$, consider the piecewise linear \"sawtooth\" function $g_N:[0,1]\\to\\mathbb{R}$ with $N$ identical teeth and unit amplitude, defined for each $k\\in\\{0,1,\\dots,N-1\\}$ by\n$$\ng_N(t)=\n\\begin{cases}\n2N\\left(t-\\frac{k}{N}\\right),  t\\in\\left[\\frac{k}{N},\\,\\frac{k}{N}+\\frac{1}{2N}\\right] \\\\\n2-2N\\left(t-\\frac{k}{N}\\right),  t\\in\\left[\\frac{k}{N}+\\frac{1}{2N},\\,\\frac{k+1}{N}\\right]\n\\end{cases}\n$$\nThis function increases linearly from $0$ to $1$ on each interval $\\left[\\frac{k}{N},\\,\\frac{k}{N}+\\frac{1}{2N}\\right]$ and then decreases linearly from $1$ back to $0$ on $\\left[\\frac{k}{N}+\\frac{1}{2N},\\,\\frac{k+1}{N}\\right]$. Starting from the definition from part 1, compute the exact value of the total variation $V(g_N,[0,1])$ as a closed-form expression in $N$.\n\n3. Briefly explain, using only fundamental definitions and well-tested facts about the Riemann–Stieltjes integral, how your computation in part 2 illustrates the limitation of the Riemann–Stieltjes integral with highly oscillatory integrators and why this motivates the development of stochastic integration when the integrator has unbounded variation. \n\nYour final numerical answer to part 2 must be given as a simplified analytic expression in terms of $N$. No rounding is required. Express your final answer without units.", "solution": "This problem is divided into three parts: stating the definition of total variation, calculating the total variation for a specific piecewise linear function, and explaining the implications of this calculation for the theory of integration.\n\n### Part 1: Definition of Total Variation\n\nThe total variation of a real-valued function $g$ defined on a closed interval $[0, T]$ is a measure of the total \"up-and-down\" movement of the function. To define it formally, we first need the concept of a partition of the interval.\n\nA partition $P$ of the interval $[0, T]$ is a finite set of points $\\{t_0, t_1, \\dots, t_n\\}$ such that $0 = t_0  t_1  \\dots  t_n = T$.\n\nFor a given partition $P$, the variational sum of $g$ with respect to $P$ is defined as the sum of the absolute differences in the function's values at consecutive points of the partition:\n$$\nS(P; g) = \\sum_{i=1}^{n} |g(t_i) - g(t_{i-1})|\n$$\nThe total variation of $g$ on $[0, T]$, denoted $V(g, [0, T])$, is the supremum of all such variational sums over the set of all possible partitions of $[0, T]$. Formally,\n$$\nV(g, [0, T]) = \\sup_{P} \\left\\{ \\sum_{i=1}^{n} |g(t_i) - g(t_{i-1})| \\right\\}\n$$\nwhere the supremum is taken over all partitions $P$ of the interval $[0, T]$. If $V(g, [0, T])$ is finite, the function $g$ is said to be of bounded variation on $[0, T]$.\n\n### Part 2: Total Variation of the Sawtooth Function $g_N$\n\nWe are given the function $g_N:[0, 1] \\to \\mathbb{R}$ defined for $k \\in \\{0, 1, \\dots, N-1\\}$ by\n$$\ng_N(t)=\n\\begin{cases}\n2N\\left(t-\\frac{k}{N}\\right),  t\\in\\left[\\frac{k}{N},\\,\\frac{k}{N}+\\frac{1}{2N}\\right] \\\\\n2-2N\\left(t-\\frac{k}{N}\\right),  t\\in\\left[\\frac{k}{N}+\\frac{1}{2N},\\,\\frac{k+1}{N}\\right]\n\\end{cases}\n$$\nWe want to compute its total variation $V(g_N, [0, 1])$.\n\nA fundamental property of total variation is its additivity over subintervals. If $a  c  b$, then $V(g, [a,b]) = V(g, [a,c]) + V(g, [c,b])$. This property extends to any partition of the interval. We can partition the interval $[0, 1]$ into $N$ subintervals $I_k = \\left[\\frac{k}{N}, \\frac{k+1}{N}\\right]$ for $k = 0, 1, \\dots, N-1$. The total variation over $[0, 1]$ is the sum of the total variations over these subintervals:\n$$\nV(g_N, [0, 1]) = \\sum_{k=0}^{N-1} V\\left(g_N, \\left[\\frac{k}{N}, \\frac{k+1}{N}\\right]\\right)\n$$\nLet's compute the total variation over a single such subinterval, $I_k$. Within $I_k$, the function $g_N(t)$ has a single turning point at $t_k^* = \\frac{k}{N} + \\frac{1}{2N}$.\nThe values of the function at the endpoints and the turning point are:\n$$\ng_N\\left(\\frac{k}{N}\\right) = 0\n$$\n$$\ng_N\\left(\\frac{k}{N} + \\frac{1}{2N}\\right) = 2N\\left(\\left(\\frac{k}{N} + \\frac{1}{2N}\\right) - \\frac{k}{N}\\right) = 2N\\left(\\frac{1}{2N}\\right) = 1\n$$\n$$\ng_N\\left(\\frac{k+1}{N}\\right) = 0\n$$\nThe function $g_N$ is monotonic on each of the two subintervals that form $I_k$:\n1.  On $\\left[\\frac{k}{N}, \\frac{k}{N} + \\frac{1}{2N}\\right]$, $g_N(t)$ is monotonically increasing from $0$ to $1$.\n2.  On $\\left[\\frac{k}{N} + \\frac{1}{2N}, \\frac{k+1}{N}\\right]$, $g_N(t)$ is monotonically decreasing from $1$ to $0$.\n\nFor a monotonic function $f$ on an interval $[a, b]$, the total variation is simply the absolute difference of its values at the endpoints, i.e., $V(f, [a, b]) = |f(b) - f(a)|$. Applying this and the additivity property to the interval $I_k$:\n$$\nV\\left(g_N, I_k\\right) = V\\left(g_N, \\left[\\frac{k}{N}, \\frac{k}{N} + \\frac{1}{2N}\\right]\\right) + V\\left(g_N, \\left[\\frac{k}{N} + \\frac{1}{2N}, \\frac{k+1}{N}\\right]\\right)\n$$\nFor the first subinterval (monotonically increasing):\n$$\nV\\left(g_N, \\left[\\frac{k}{N}, \\frac{k}{N} + \\frac{1}{2N}\\right]\\right) = \\left|g_N\\left(\\frac{k}{N} + \\frac{1}{2N}\\right) - g_N\\left(\\frac{k}{N}\\right)\\right| = |1 - 0| = 1\n$$\nFor the second subinterval (monotonically decreasing):\n$$\nV\\left(g_N, \\left[\\frac{k}{N} + \\frac{1}{2N}, \\frac{k+1}{N}\\right]\\right) = \\left|g_N\\left(\\frac{k+1}{N}\\right) - g_N\\left(\\frac{k}{N} + \\frac{1}{2N}\\right)\\right| = |0 - 1| = 1\n$$\nTherefore, the total variation over a single tooth-like segment $I_k$ is:\n$$\nV(g_N, I_k) = 1 + 1 = 2\n$$\nSince there are $N$ identical teeth over the interval $[0, 1]$, the total variation $V(g_N, [0, 1])$ is the sum of the variations of each tooth:\n$$\nV(g_N, [0, 1]) = \\sum_{k=0}^{N-1} V(g_N, I_k) = \\sum_{k=0}^{N-1} 2 = N \\times 2 = 2N\n$$\nThe exact value of the total variation of $g_N$ on $[0, 1]$ is $2N$.\n\n### Part 3: Limitation of Riemann-Stieltjes Integration\n\nThe result from part 2 provides a critical insight into the limitations of the Riemann-Stieltjes (R-S) integral and motivates the need for stochastic integration.\n\n1.  **Existence Condition for the Riemann-Stieltjes Integral**: A fundamental theorem in analysis states that the R-S integral $\\int_a^b f(t) \\, dg(t)$ is guaranteed to exist if the integrand $f$ is continuous and the integrator $g$ is of bounded variation on $[a, b]$.\n\n2.  **Unbounded Variation**: Our calculation in part 2 shows that $V(g_N, [0, 1]) = 2N$. As we let the number of oscillations $N$ tend to infinity, the total variation also tends to infinity:\n    $$\n    \\lim_{N \\to \\infty} V(g_N, [0, 1]) = \\lim_{N \\to \\infty} 2N = \\infty\n    $$\n    This demonstrates how a function can become so \"rough\" or \"oscillatory\" that its total variation becomes infinite. Although for any finite $N$, $g_N$ is of bounded variation, the sequence illustrates the concept of a function with unbounded variation in the limit.\n\n3.  **Connection to Stochastic Processes**: The central motivation for stochastic integration arises from the desire to define integrals with respect to stochastic processes, most notably Brownian motion, $B(t)$. A path of a standard Brownian motion is, with probability one, a continuous function that is nowhere differentiable. A crucial related property is that, with probability one, a Brownian motion path has **unbounded variation** on any finite time interval.\n\n4.  **Failure of Riemann-Stieltjes Integration**: Because sample paths of Brownian motion $B(t)$ have unbounded variation, the sufficient condition for the existence of the R-S integral is not met. Consequently, the integral $\\int_a^b f(t) \\, dB(t)$ cannot be defined in the pathwise sense of Riemann-Stieltjes for a general continuous function $f(t)$. The highly oscillatory nature of Brownian paths, qualitatively similar to our $g_N(t)$ as $N \\to \\infty$, is the fundamental obstacle. The machinery of R-S integration, which relies on the \"length\" of the integrator's path being finite, breaks down.\n\n5.  **Motivation for Stochastic Integration**: The failure of the R-S theory necessitates a new approach. Stochastic integration, particularly the Itô integral, is constructed precisely to handle such integrators of infinite variation. The Itô integral is not defined as a pathwise deterministic limit for each sample path. Instead, it is defined as a limit in the $L^2$ (mean-square) sense of approximating sums. This construction leverages the statistical properties of Brownian motion increments, specifically that while the first variation diverges, the quadratic variation converges. The quadratic variation of Brownian motion on $[0, T]$ is $\\int_0^T (dB_s)^2 = T$. This finite quadratic variation is the property that the Itô calculus successfully exploits, enabling a consistent and powerful theory of integration with respect to processes like Brownian motion.\n\nIn summary, the sawtooth function $g_N$ provides a clear, deterministic example of how increasing oscillations lead to unbounded total variation. This directly parallels the problematic nature of Brownian motion paths, explaining from first principles why the standard R-S integral is inadequate for stochastic calculus and why a more sophisticated theory is essential.", "answer": "$$\\boxed{2N}$$", "id": "3067244"}, {"introduction": "Since classical integration rules fail for Brownian motion, what happens if we attempt to compute an integral like $\\int_0^T B_t \\, dB_t$ from first principles? This practice guides you through a direct calculation of the limit of the approximating sums for this integral [@problem_id:3067261]. The surprising result you'll derive is a cornerstone of stochastic calculus, revealing a non-classical correction term that arises directly from the random nature of the integrator.", "problem": "Let $\\{B_{t}\\}_{t \\in [0,T]}$ be a standard Brownian motion (also called a Wiener process) with $B_{0}=0$. Consider a sequence of tagged partitions $\\pi_{n}=\\{0=t_{0}^{(n)}t_{1}^{(n)}\\cdotst_{N_{n}}^{(n)}=T\\}$ of $[0,T]$ whose mesh $\\|\\pi_{n}\\|=\\max_{0 \\leq i  N_{n}}(t_{i+1}^{(n)}-t_{i}^{(n)})$ tends to $0$ as $n \\to \\infty$. Define the left-point Riemann sums\n$$\nS(\\pi_{n})=\\sum_{i=0}^{N_{n}-1} B_{t_{i}^{(n)}}\\big(B_{t_{i+1}^{(n)}}-B_{t_{i}^{(n)}}\\big).\n$$\nStarting only from fundamental properties of Brownian motion, namely $B_{0}=0$, independent and stationary increments with $B_{t}-B_{s} \\sim \\mathcal{N}(0,t-s)$ for $0 \\leq s  t$, and the well-tested fact that the quadratic variation satisfies $\\sum_{i=0}^{N_{n}-1}\\big(B_{t_{i+1}^{(n)}}-B_{t_{i}^{(n)}}\\big)^{2} \\to T$ in probability as $n \\to \\infty$, do the following:\n\n1. Derive an identity that relates $S(\\pi_{n})$ to the telescoping sum of $\\big(B_{t_{i+1}^{(n)}}\\big)^{2}-\\big(B_{t_{i}^{(n)}}\\big)^{2}$ and $\\sum_{i=0}^{N_{n}-1}\\big(B_{t_{i+1}^{(n)}}-B_{t_{i}^{(n)}}\\big)^{2}$.\n\n2. Use this identity, together with the quadratic variation property, to compute the limit of $S(\\pi_{n})$ in probability as $n \\to \\infty$. Interpret the limiting expression to explain why classical Riemann–Stieltjes integration fails for $t \\mapsto B_{t}$ and how the resulting extra term motivates stochastic integration.\n\nExpress your final answer as a closed-form analytic expression in terms of $B_{T}$ and $T$. No rounding is required, and no physical units are involved.", "solution": "The problem as stated is mathematically well-defined, scientifically grounded in the theory of stochastic processes, and internally consistent. It presents a standard, fundamental exercise in motivating the Itô stochastic integral. All necessary conditions and definitions are provided. Therefore, the problem is valid and a solution will be provided.\n\nThe problem requires a two-part analysis. First, to derive an algebraic identity, and second, to use this identity to compute a limit and interpret its meaning in the context of stochastic calculus.\n\n**Part 1: Derivation of the Identity**\n\nWe begin with the left-point Riemann sum given by\n$$\nS(\\pi_{n})=\\sum_{i=0}^{N_{n}-1} B_{t_{i}^{(n)}}\\big(B_{t_{i+1}^{(n)}}-B_{t_{i}^{(n)}}\\big).\n$$\nThe core of the derivation lies in a simple algebraic identity. For any two real numbers $a$ and $b$, we have the identity:\n$$\na(b-a) = \\frac{1}{2} \\left[ b^2 - a^2 - (b-a)^2 \\right]\n$$\nThis can be verified by expanding the right-hand side:\n$$\n\\frac{1}{2} \\left[ b^2 - a^2 - (b^2 - 2ab + a^2) \\right] = \\frac{1}{2} \\left[ b^2 - a^2 - b^2 + 2ab - a^2 \\right] = \\frac{1}{2} \\left[ 2ab - 2a^2 \\right] = ab - a^2 = a(b-a).\n$$\nWe apply this identity to each term in the sum $S(\\pi_{n})$ by setting $a = B_{t_{i}^{(n)}}$ and $b = B_{t_{i+1}^{(n)}}$. This yields:\n$$\nB_{t_{i}^{(n)}}\\big(B_{t_{i+1}^{(n)}}-B_{t_{i}^{(n)}}\\big) = \\frac{1}{2} \\left[ \\big(B_{t_{i+1}^{(n)}}\\big)^2 - \\big(B_{t_{i}^{(n)}}\\big)^2 - \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2 \\right].\n$$\nNow, we sum both sides from $i=0$ to $N_{n}-1$:\n$$\nS(\\pi_{n}) = \\sum_{i=0}^{N_{n}-1} \\frac{1}{2} \\left[ \\big(B_{t_{i+1}^{(n)}}\\big)^2 - \\big(B_{t_{i}^{(n)}}\\big)^2 - \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2 \\right].\n$$\nUsing the linearity of summation, we can separate the terms:\n$$\nS(\\pi_{n}) = \\frac{1}{2} \\sum_{i=0}^{N_{n}-1} \\left[ \\big(B_{t_{i+1}^{(n)}}\\big)^2 - \\big(B_{t_{i}^{(n)}}\\big)^2 \\right] - \\frac{1}{2} \\sum_{i=0}^{N_{n}-1} \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2.\n$$\nThe first summation is a telescoping series:\n\\begin{align*}\n\\sum_{i=0}^{N_{n}-1} \\left[ \\big(B_{t_{i+1}^{(n)}}\\big)^2 - \\big(B_{t_{i}^{(n)}}\\big)^2 \\right] = \\left( \\big(B_{t_{1}^{(n)}}\\big)^2 - \\big(B_{t_{0}^{(n)}}\\big)^2 \\right) + \\left( \\big(B_{t_{2}^{(n)}}\\big)^2 - \\big(B_{t_{1}^{(n)}}\\big)^2 \\right) + \\dots + \\left( \\big(B_{t_{N_{n}}^{(n)}}\\big)^2 - \\big(B_{t_{N_{n}-1}^{(n)}}\\big)^2 \\right) \\\\\n= \\big(B_{t_{N_{n}}^{(n)}}\\big)^2 - \\big(B_{t_{0}^{(n)}}\\big)^2.\n\\end{align*}\nGiven the partition definition $t_{0}^{(n)}=0$ and $t_{N_{n}}^{(n)}=T$, and the initial condition $B_{0}=0$, this sum simplifies to:\n$$\n\\big(B_{T}\\big)^2 - \\big(B_{0}\\big)^2 = (B_{T})^2 - 0^2 = (B_{T})^2.\n$$\nSubstituting this result back into the expression for $S(\\pi_{n})$, we obtain the desired identity:\n$$\nS(\\pi_{n}) = \\frac{1}{2} (B_{T})^2 - \\frac{1}{2} \\sum_{i=0}^{N_{n}-1} \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2.\n$$\nThis identity relates the Riemann-like sum $S(\\pi_{n})$ to the squared value of the process at time $T$ and the quadratic variation sum over the partition.\n\n**Part 2: Limit Computation and Interpretation**\n\nWe are asked to compute the limit of $S(\\pi_{n})$ in probability as $n \\to \\infty$. We take the limit of the identity derived in Part 1. By the properties of convergence in probability (specifically, that if $X_n \\to X$ and $Y_n \\to Y$ in probability, then $X_n+Y_n \\to X+Y$ in probability), we can analyze the limit of each term separately.\n$$\n\\lim_{n\\to\\infty} S(\\pi_{n}) = \\lim_{n\\to\\infty} \\left( \\frac{1}{2} (B_{T})^2 - \\frac{1}{2} \\sum_{i=0}^{N_{n}-1} \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2 \\right).\n$$\nThe term $\\frac{1}{2}(B_{T})^2$ does not depend on the partition index $n$, so its limit is itself. The second term involves the quadratic variation sum. The problem statement provides the crucial fact that\n$$\n\\sum_{i=0}^{N_{n}-1} \\big(B_{t_{i+1}^{(n)}} - B_{t_{i}^{(n)}}\\big)^2 \\to T \\quad \\text{in probability as } n \\to \\infty.\n$$\nTherefore, we can substitute this limit into our expression:\n$$\n\\lim_{n\\to\\infty} S(\\pi_{n}) = \\frac{1}{2} (B_{T})^2 - \\frac{1}{2} T.\n$$\nThis limit is the definition of the Itô integral of $B_t$ with respect to itself, denoted $\\int_{0}^{T} B_{t} \\,dB_{t}$. Thus, we have shown that\n$$\n\\int_{0}^{T} B_{t} \\,dB_{t} = \\frac{1}{2} (B_{T})^2 - \\frac{1}{2} T.\n$$\n**Interpretation:**\n\nThis result highlights a fundamental departure from classical calculus and explains the failure of the standard Riemann-Stieltjes integration framework for paths of a Brownian motion.\n\n1.  **Failure of Riemann-Stieltjes Integration:** For a \"well-behaved\" function $f(t)$ (e.g., continuously differentiable), the Riemann-Stieltjes integral $\\int_0^T f(t) \\,df(t)$ is defined as the limit of sums analogous to $S(\\pi_n)$, and by the rules of ordinary calculus, it evaluates to $\\frac{1}{2}[f(T)]^2 - \\frac{1}{2}[f(0)]^2$. If this classical rule applied to Brownian motion, we would expect the limit to be $\\frac{1}{2}(B_T)^2 - \\frac{1}{2}(B_0)^2 = \\frac{1}{2}(B_T)^2$. Our calculation yields an additional term, $-\\frac{1}{2}T$.\n\n    The reason for this discrepancy is that the theory of Riemann-Stieltjes integration requires the integrator function (in this case, $t \\mapsto B_t$) to be of bounded variation. A function of bounded variation has a quadratic variation of zero. However, with probability $1$, a sample path of Brownian motion is of unbounded variation on any interval. The given fact that its quadratic variation over $[0,T]$ is $T \\neq 0$ is a precise statement of this property. This non-zero quadratic variation is the feature that breaks the classical integration rules.\n\n2.  **Motivation for Stochastic Integration:** The calculation demonstrates that while the limit of the Riemann-like sums exists, it incorporates a \"correction\" term, $-\\frac{1}{2}T$, that arises directly from the non-vanishing quadratic variation of the Brownian motion process. This discovery necessitates a new theory of integration, namely stochastic integration (specifically, Itô calculus), to consistently handle integrals involving such processes. The result $\\int_{0}^{T} B_{t} \\,dB_{t} = \\frac{1}{2}(B_T)^2 - \\frac{1}{2}T$ is a prototypical example of Itô's lemma, the chain rule of stochastic calculus, which systematically includes terms related to the quadratic variation of the underlying stochastic processes.\n\nThe final answer is the limiting value of $S(\\pi_{n})$.", "answer": "$$\n\\boxed{\\frac{1}{2}(B_T)^2 - \\frac{1}{2}T}\n$$", "id": "3067261"}, {"introduction": "The non-classical term discovered when evaluating $\\int B_t \\, dB_t$ is a specific example of a more general phenomenon captured by quadratic covariation. This exercise asks you to compute the quadratic covariation $[B, f(B)]_t$ between a Brownian motion $B_t$ and a function of that process, $f(B_t)$ [@problem_id:3067249]. Completing this calculation reveals a fundamental relationship that forms a critical building block for Itô's lemma, the chain rule of stochastic calculus.", "problem": "Let $B=\\{B_{t}\\}_{t\\geq 0}$ be a standard one-dimensional Brownian motion with $B_{0}=0$, continuous paths, and independent, stationary increments. Let $f:\\mathbb{R}\\to\\mathbb{R}$ be twice continuously differentiable with bounded first and second derivatives. The quadratic covariation $[X,Y]_{t}$ of two continuous processes $X=\\{X_{t}\\}_{t\\geq 0}$ and $Y=\\{Y_{t}\\}_{t\\geq 0}$ is defined, when the limit exists in probability, by\n$$\n[X,Y]_{t}=\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{i=0}^{n-1}\\big(X_{t_{i+1}}-X_{t_{i}}\\big)\\big(Y_{t_{i+1}}-Y_{t_{i}}\\big),\n$$\nwhere $\\mathcal{P}=\\{0=t_{0}t_{1}\\cdotst_{n}=t\\}$ is a partition of $[0,t]$ and $\\|\\mathcal{P}\\|=\\max_{i}(t_{i+1}-t_{i})$. Starting from these definitions and the well-tested facts that Brownian motion has independent Gaussian increments and quadratic variation $[B]_{t}=t$, compute the quadratic covariation $[B,f(B)]_{t}$ as a limit of partition sums. Your derivation must:\n- Use a second-order Taylor expansion of $f$ around $B_{t_{i}}$ to control the increment $f(B_{t_{i+1}})-f(B_{t_{i}})$.\n- Justify the replacement of $\\sum f'(B_{t_{i}})(B_{t_{i+1}}-B_{t_{i}})^{2}$ by a time integral, carefully explaining the role of the Brownian motion quadratic variation.\n- Explain why a pathwise Riemann–Stieltjes approach based on bounded variation would predict a vanishing quadratic covariation and why this fails for Brownian motion, thereby motivating stochastic integration.\n\nExpress your final result for $[B,f(B)]_{t}$ as a single closed-form analytic expression in terms of $f$ and $B$. No numerical rounding is required.", "solution": "The user wants to compute the quadratic covariation $[B,f(B)]_{t}$ of a standard one-dimensional Brownian motion $B_{t}$ and a process $f(B_{t})$, where $f$ is a twice continuously differentiable function with bounded first and second derivatives. The calculation will proceed from the definition of quadratic covariation as a limit of partition sums.\n\nLet $\\mathcal{P}=\\{0=t_{0}t_{1}\\cdotst_{n}=t\\}$ be a partition of the interval $[0,t]$ with mesh $\\|\\mathcal{P}\\|=\\max_{i}(t_{i+1}-t_{i})$. The quadratic covariation $[B,f(B)]_{t}$ is defined as the limit in probability:\n$$ [B,f(B)]_{t} = \\lim_{\\|\\mathcal{P}\\|\\to 0} \\sum_{i=0}^{n-1} (B_{t_{i+1}}-B_{t_{i}})\\big(f(B_{t_{i+1}})-f(B_{t_{i}})\\big) $$\nLet $\\Delta B_{i} = B_{t_{i+1}}-B_{t_{i}}$. The sum can be written as $S_{\\mathcal{P}} = \\sum_{i=0}^{n-1} \\Delta B_{i} \\big(f(B_{t_{i+1}})-f(B_{t_{i}})\\big)$.\n\nThe first step is to apply a second-order Taylor expansion to the increment $f(B_{t_{i+1}})-f(B_{t_{i}})$. Since $f \\in C^2(\\mathbb{R})$, we can expand $f(B_{t_{i+1}})$ around $B_{t_i}$:\n$$ f(B_{t_{i+1}}) = f(B_{t_i} + \\Delta B_i) = f(B_{t_i}) + f'(B_{t_i})\\Delta B_i + \\frac{1}{2}f''(\\xi_i)(\\Delta B_i)^2 $$\nwhere $\\xi_i$ is a random variable that lies strictly between $B_{t_i}$ and $B_{t_{i+1}}$. This gives the increment:\n$$ f(B_{t_{i+1}})-f(B_{t_{i}}) = f'(B_{t_i})\\Delta B_i + \\frac{1}{2}f''(\\xi_i)(\\Delta B_i)^2 $$\nSubstituting this expression into the sum $S_{\\mathcal{P}}$ yields:\n$$ S_{\\mathcal{P}} = \\sum_{i=0}^{n-1} \\Delta B_i \\left( f'(B_{t_i})\\Delta B_i + \\frac{1}{2}f''(\\xi_i)(\\Delta B_i)^2 \\right) $$\n$$ S_{\\mathcal{P}} = \\sum_{i=0}^{n-1} f'(B_{t_i})(\\Delta B_i)^2 + \\frac{1}{2}\\sum_{i=0}^{n-1} f''(\\xi_i)(\\Delta B_i)^3 $$\nWe analyze the limit of each sum as $\\|\\mathcal{P}\\| \\to 0$. Let's denote the two sums as $S_{1,\\mathcal{P}}$ and $S_{2,\\mathcal{P}}$ respectively.\n\nFirst, consider the second sum, $S_{2,\\mathcal{P}} = \\frac{1}{2}\\sum_{i=0}^{n-1} f''(\\xi_i)(\\Delta B_i)^3$. We will show that this term converges to $0$ in probability. We can show this by demonstrating convergence in $L^1$.\nThe second derivative $f''$ is bounded, so there exists a constant $M''  0$ such that $|f''(x)| \\le M''$ for all $x \\in \\mathbb{R}$.\n$$ E\\left[|S_{2,\\mathcal{P}}|\\right] \\le \\frac{1}{2} E\\left[\\sum_{i=0}^{n-1} |f''(\\xi_i)(\\Delta B_i)^3|\\right] \\le \\frac{M''}{2} \\sum_{i=0}^{n-1} E\\left[|\\Delta B_i|^3\\right] $$\nThe increment $\\Delta B_i$ is a Gaussian random variable with mean $0$ and variance $\\Delta t_i = t_{i+1}-t_i$. So, $\\Delta B_i$ has the same distribution as $\\sqrt{\\Delta t_i} Z$, where $Z \\sim N(0,1)$.\n$$ E\\left[|\\Delta B_i|^3\\right] = E\\left[|\\sqrt{\\Delta t_i}Z|^3\\right] = (\\Delta t_i)^{3/2} E\\left[|Z|^3\\right] $$\nThe third absolute moment of a standard normal distribution, $E[|Z|^3]$, is a finite constant. Let $C = E[|Z|^3]$.\n$$ E\\left[|S_{2,\\mathcal{P}}|\\right] \\le \\frac{M''C}{2} \\sum_{i=0}^{n-1} (\\Delta t_i)^{3/2} = \\frac{M''C}{2} \\sum_{i=0}^{n-1} \\sqrt{\\Delta t_i}\\Delta t_i $$\nLet $\\|\\mathcal{P}\\| = \\max_i \\Delta t_i$. Then $\\sqrt{\\Delta t_i} \\le \\sqrt{\\|\\mathcal{P}\\|}$.\n$$ \\sum_{i=0}^{n-1} \\sqrt{\\Delta t_i}\\Delta t_i \\le \\sqrt{\\|\\mathcal{P}\\|} \\sum_{i=0}^{n-1} \\Delta t_i = \\sqrt{\\|\\mathcal{P}\\|} t $$\nThus, $E[|S_{2,\\mathcal{P}}|] \\le \\frac{M''Ct}{2} \\sqrt{\\|\\mathcal{P}\\|}$. As $\\|\\mathcal{P}\\| \\to 0$, $E[|S_{2,\\mathcal{P}}|] \\to 0$. Convergence in $L^1$ implies convergence in probability, so $S_{2,\\mathcal{P}} \\to 0$ as $\\|\\mathcal{P}\\| \\to 0$.\n\nNext, we analyze the first sum, $S_{1,\\mathcal{P}} = \\sum_{i=0}^{n-1} f'(B_{t_i})(\\Delta B_i)^2$. To justify replacing this with a time integral, we rewrite it as:\n$$ S_{1,\\mathcal{P}} = \\sum_{i=0}^{n-1} f'(B_{t_i})\\Delta t_i + \\sum_{i=0}^{n-1} f'(B_{t_i})\\left((\\Delta B_i)^2 - \\Delta t_i\\right) $$\nThe first term, $\\sum_{i=0}^{n-1} f'(B_{t_i})\\Delta t_i$, is a Riemann sum for the integral of the process $s \\mapsto f'(B_s)$. Since $f'$ is continuous and Brownian motion $B_t$ has continuous paths, the composite function $s \\mapsto f'(B_s)$ is also a continuous function of time (for a given path). Therefore, as $\\|\\mathcal{P}\\| \\to 0$, this Riemann sum converges almost surely to the Riemann integral $\\int_0^t f'(B_s)ds$.\nThe second term is the remainder $R_{\\mathcal{P}} = \\sum_{i=0}^{n-1} f'(B_{t_i})\\left((\\Delta B_i)^2 - \\Delta t_i\\right)$. We show this converges to $0$ in probability by showing convergence in $L^2$.\nThe mean of $R_{\\mathcal{P}}$ is $0$. To see this, we use the tower property of conditional expectation. Let $\\mathcal{F}_{t_i}$ be the natural filtration of the Brownian motion. $B_{t_i}$ is $\\mathcal{F}_{t_i}$-measurable, while $\\Delta B_i$ is independent of $\\mathcal{F}_{t_i}$.\n$$ E\\left[f'(B_{t_i})\\left((\\Delta B_i)^2 - \\Delta t_i\\right)\\right] = E\\left[E\\left[f'(B_{t_i})\\left((\\Delta B_i)^2 - \\Delta t_i\\right) | \\mathcal{F}_{t_i}\\right]\\right] $$\n$$ = E\\left[f'(B_{t_i})E\\left[(\\Delta B_i)^2 - \\Delta t_i | \\mathcal{F}_{t_i}\\right]\\right] = E\\left[f'(B_{t_i})\\left(E[(\\Delta B_i)^2] - \\Delta t_i\\right)\\right] = E\\left[f'(B_{t_i})(\\Delta t_i - \\Delta t_i)\\right] = 0 $$\nThe variance is $Var(R_{\\mathcal{P}}) = E[R_{\\mathcal{P}}^2]$. Let $Z_i = f'(B_{t_i})((\\Delta B_i)^2 - \\Delta t_i)$. For $i  j$, the term $E[Z_i Z_j]$ is $0$ because conditioning on $\\mathcal{F}_{t_j}$ makes the expectation of the $Z_j$ factor zero due to the independence of the increment $\\Delta B_j$.\n$$ Var(R_{\\mathcal{P}}) = \\sum_{i=0}^{n-1} E[Z_i^2] = \\sum_{i=0}^{n-1} E\\left[ (f'(B_{t_i}))^2 \\left((\\Delta B_i)^2 - \\Delta t_i\\right)^2 \\right] $$\nBy conditioning on $\\mathcal{F}_{t_i}$ again:\n$$ E[Z_i^2] = E\\left[ (f'(B_{t_i}))^2 E\\left[\\left((\\Delta B_i)^2 - \\Delta t_i\\right)^2\\right] \\right] $$\nThe inner expectation is $E[(\\Delta B_i)^4] - 2\\Delta t_i E[(\\Delta B_i)^2] + (\\Delta t_i)^2 = 3(\\Delta t_i)^2 - 2\\Delta t_i(\\Delta t_i) + (\\Delta t_i)^2 = 2(\\Delta t_i)^2$.\nSince $f'$ is bounded, $|f'(x)| \\le M'$ for some constant $M'$. So $(f'(x))^2 \\le (M')^2$.\n$$ Var(R_{\\mathcal{P}}) = \\sum_{i=0}^{n-1} 2(\\Delta t_i)^2 E[(f'(B_{t_i}))^2] \\le 2(M')^2 \\sum_{i=0}^{n-1} (\\Delta t_i)^2 $$\n$$ \\le 2(M')^2 \\left(\\max_j \\Delta t_j\\right) \\sum_{i=0}^{n-1} \\Delta t_i = 2(M')^2 t \\|\\mathcal{P}\\| $$\nAs $\\|\\mathcal{P}\\| \\to 0$, $Var(R_{\\mathcal{P}}) \\to 0$. Since its mean is $0$ and its variance tends to $0$, $R_{\\mathcal{P}}$ converges to $0$ in $L^2$ and therefore in probability.\n\nCombining the results, as $\\|\\mathcal{P}\\| \\to 0$:\n$$ S_{\\mathcal{P}} = S_{1,\\mathcal{P}} + S_{2,\\mathcal{P}} \\xrightarrow{p} \\int_0^t f'(B_s)ds + 0 $$\nTherefore, the quadratic covariation is given by:\n$$ [B,f(B)]_{t} = \\int_0^t f'(B_s)ds $$\nThis result can be expressed notationally as $\\int_0^t f'(B_s) d[B,B]_s$ or simply $d[B,f(B)]_t = f'(B_t)dt$.\n\nFinally, we explain why a classical Riemann-Stieltjes approach fails and predicts a vanishing quadratic covariation. The Riemann-Stieltjes integral $\\int g(s)dh(s)$ is well-defined for any continuous integrand $g$ only if the integrator $h$ is a function of bounded variation. A function $h$ has bounded variation on $[0,t]$ if its total variation, $V_t(h) = \\sup_{\\mathcal{P}} \\sum_{i=0}^{n-1} |h(t_{i+1}) - h(t_i)|$, is finite.\nWith probability $1$, a path of Brownian motion is nowhere differentiable and does not have bounded variation on any time interval. This can be seen from its quadratic variation. If a path $B_s(\\omega)$ had bounded variation, $V_t(B)  \\infty$, then its quadratic variation sum would satisfy:\n$$ \\sum_{i=0}^{n-1} (B_{t_{i+1}} - B_{t_i})^2 \\le \\max_{i}|B_{t_{i+1}} - B_{t_i}| \\sum_{i=0}^{n-1} |B_{t_{i+1}} - B_{t_i}| \\le \\max_{i}|B_{t_{i+1}} - B_{t_i}| \\cdot V_t(B) $$\nAs $\\|\\mathcal{P}\\| \\to 0$, the continuity of Brownian paths implies $\\max_{i}|B_{t_{i+1}} - B_{t_i}| \\to 0$. This would force the quadratic variation $[B]_t$ to be $0$. However, it is a fundamental fact that $[B]_t = t$. This contradiction demonstrates that paths of $B_t$ are almost surely not of bounded variation.\nIf we were to incorrectly assume $B_t$ is of bounded variation, we would predict $[B,f(B)]_t=0$. This is because for two continuous functions $X$ and $Y$, where at least one has bounded variation, their quadratic covariation is zero. The argument is similar to the one above: $\\sum |\\Delta X_i \\Delta Y_i| \\le (\\max_i |\\Delta X_i|) \\sum |\\Delta Y_i|$. If $Y$ has bounded variation, $\\sum|\\Delta Y_i|$ is bounded by $V_t(Y)$, and continuity of $X$ makes $\\max_i|\\Delta X_i| \\to 0$. Since $f$ is $C^1$, if $B_t$ had bounded variation, so would $f(B_t)$. Thus, classical calculus would predict $[B, f(B)]_t = 0$.\nThe non-zero result we derived, $[B,f(B)]_t = \\int_0^t f'(B_s)ds$, shows the failure of the classical framework. The \"roughness\" of Brownian motion, captured by its non-vanishing quadratic variation, gives rise to non-classical results and motivates the development of a new theory of integration, namely stochastic integration (Itô calculus), which can handle such integrator processes. The non-zero covariation is a key component of Itô's formula, the chain rule of stochastic calculus.", "answer": "$$\\boxed{\\int_{0}^{t} f'(B_{s})ds}$$", "id": "3067249"}]}