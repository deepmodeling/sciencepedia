{"hands_on_practices": [{"introduction": "The construction of the Itô integral begins with simple, piecewise-constant processes. This first practice provides a foundational exercise in applying the definition directly [@problem_id:3045439]. By representing a basic indicator function as a simple process, you will compute its integral and discover a fundamental and intuitive relationship between the integral and the Brownian motion itself.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions that carries a standard Brownian motion $(W_t)_{t\\in[0,T]}$. The Itô integral for simple predictable processes is constructed as follows: given a partition $0=t_0<t_1<\\cdots<t_n=T$ and a process of the form $X_t=\\sum_{k=0}^{n-1}\\xi_k\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t)$, where each $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable and square-integrable, define\n$$\n\\int_0^T X_t\\,\\mathrm{d}W_t:=\\sum_{k=0}^{n-1}\\xi_k\\bigl(W_{t_{k+1}}-W_{t_k}\\bigr).\n$$\nFix $T>0$ and deterministic times $0\\le a<b\\le T$. Consider the deterministic simple predictable process $X_t=\\mathbf{1}_{(a,b]}(t)$. Using only the above construction and the measurability conditions that make $X_t$ an admissible integrand, compute the Itô integral\n$$\n\\int_0^T \\mathbf{1}_{(a,b]}(t)\\,\\mathrm{d}W_t\n$$\nin closed form in terms of the Brownian motion $(W_t)_{t\\in[0,T]}$. Provide your answer as a single analytic expression. No rounding is required.", "solution": "The problem is to compute the Itô integral $\\int_0^T \\mathbf{1}_{(a,b]}(t)\\,\\mathrm{d}W_t$ using the provided definition for simple predictable processes. The integrand is the process $X_t = \\mathbf{1}_{(a,b]}(t)$, where $t \\in [0,T]$ and $0 \\le a < b \\le T$ are deterministic constants.\n\nThe definition of the Itô integral is given for a simple predictable process of the form $Y_t = \\sum_{k=0}^{n-1} \\xi_k \\mathbf{1}_{(t_k, t_{k+1}]}(t)$, where $0=t_0 < t_1 < \\dots < t_n = T$ is a partition of the interval $[0,T]$, and each coefficient $\\xi_k$ is an $\\mathcal{F}_{t_k}$-measurable and square-integrable random variable. The integral is defined as:\n$$\n\\int_0^T Y_t\\,\\mathrm{d}W_t = \\sum_{k=0}^{n-1} \\xi_k (W_{t_{k+1}} - W_{t_k}).\n$$\nTo compute the integral of $X_t = \\mathbf{1}_{(a,b]}(t)$, we must first express $X_t$ in the required form of a simple predictable process. This requires us to select a suitable partition of $[0,T]$ and determine the corresponding coefficients $\\xi_k$.\n\nLet us construct a partition of $[0,T]$ that includes the points $a$ and $b$. We can choose any such partition. The result of the integration is independent of the specific choice of partition, as long as it correctly represents the process. Let $\\Pi = \\{s_0, s_1, \\dots, s_m\\}$ be a partition of $[0,T]$ such that $0=s_0 < s_1 < \\dots < s_m = T$, and for some indices $i, j$ with $0 \\le i < j \\le m$, we have $s_i = a$ and $s_j = b$. For instance, if $0 < a < b < T$, we could use the partition $\\{0, a, b, T\\}$. If $a=0$ and $b=T$, the partition is simply $\\{0, T\\}$. The general approach covers all cases.\n\nThe process is $X_t = \\mathbf{1}_{(a,b]}(t)$. Using our partition points, this is $X_t = \\mathbf{1}_{(s_i, s_j]}(t)$. The interval $(s_i, s_j]$ can be expressed as a disjoint union of the partition intervals:\n$$\n(s_i, s_j] = \\bigcup_{k=i}^{j-1} (s_k, s_{k+1}].\n$$\nTherefore, the indicator function can be written as a sum:\n$$\nX_t = \\mathbf{1}_{(s_i, s_j]}(t) = \\sum_{k=i}^{j-1} \\mathbf{1}_{(s_k, s_{k+1}]}(t).\n$$\nWe can write this sum over the entire partition from $k=0$ to $k=m-1$ by defining appropriate coefficients $\\xi_k$:\n$$\nX_t = \\sum_{k=0}^{m-1} \\xi_k \\mathbf{1}_{(s_k, s_{k+1}]}(t),\n$$\nwhere a comparison with the previous expression yields:\n$$\n\\xi_k = \\begin{cases} 1 & \\text{if } i \\le k \\le j-1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nNow, we must verify that these coefficients $\\xi_k$ satisfy the required conditions.\n1.  **Measurability**: Each $\\xi_k$ is either $0$ or $1$. These are deterministic constants. A constant random variable is measurable with respect to any sigma-algebra. Therefore, each $\\xi_k$ is measurable with respect to $\\mathcal{F}_{s_k}$ for any $k \\in \\{0, 1, \\dots, m-1\\}$. The assumption that the filtration satisfies the usual conditions guarantees that $\\mathcal{F}_0$ contains all null sets, making any constant $\\mathcal{F}_0$-measurable, and thus $\\mathcal{F}_t$-measurable for all $t \\ge 0$.\n2.  **Square-integrability**: We check if $\\mathbb{E}[|\\xi_k|^2] < \\infty$.\n    -   If $\\xi_k = 0$, then $\\mathbb{E}[|0|^2] = 0 < \\infty$.\n    -   If $\\xi_k = 1$, then $\\mathbb{E}[|1|^2] = \\mathbb{E}[1] = 1 < \\infty$.\nBoth conditions are satisfied. Thus, $X_t = \\mathbf{1}_{(a,b]}(t)$ is an admissible simple predictable process with respect to our chosen partition.\n\nWe can now apply the definition of the Itô integral:\n$$\n\\int_0^T X_t\\,\\mathrm{d}W_t = \\sum_{k=0}^{m-1} \\xi_k (W_{s_{k+1}} - W_{s_k}).\n$$\nSubstituting the values for $\\xi_k$:\n$$\n\\int_0^T X_t\\,\\mathrm{d}W_t = \\sum_{k=0}^{i-1} 0 \\cdot (W_{s_{k+1}} - W_{s_k}) + \\sum_{k=i}^{j-1} 1 \\cdot (W_{s_{k+1}} - W_{s_k}) + \\sum_{k=j}^{m-1} 0 \\cdot (W_{s_{k+1}} - W_{s_k}).\n$$\nThe first and third sums are equal to $0$. We are left with the middle sum:\n$$\n\\int_0^T X_t\\,\\mathrm{d}W_t = \\sum_{k=i}^{j-1} (W_{s_{k+1}} - W_{s_k}).\n$$\nThis is a telescoping sum:\n$$\n\\sum_{k=i}^{j-1} (W_{s_{k+1}} - W_{s_k}) = (W_{s_{i+1}} - W_{s_i}) + (W_{s_{i+2}} - W_{s_{i+1}}) + \\dots + (W_{s_j} - W_{s_{j-1}}).\n$$\nAll intermediate terms cancel out, leaving only the first and last terms:\n$$\n\\sum_{k=i}^{j-1} (W_{s_{k+1}} - W_{s_k}) = W_{s_j} - W_{s_i}.\n$$\nBy our construction of the partition, we have $s_i = a$ and $s_j = b$. Therefore, the value of the integral is:\n$$\n\\int_0^T \\mathbf{1}_{(a,b]}(t)\\,\\mathrm{d}W_t = W_b - W_a.\n$$\nThis result holds for any choice of $a$ and $b$ such that $0 \\le a < b \\le T$. Since $(W_t)_{t \\in [0,T]}$ is a standard Brownian motion, we have $W_0=0$. If $a=0$, the integral becomes $W_b - W_0 = W_b$. The general expression $W_b - W_a$ is the final closed-form answer.", "answer": "$$\n\\boxed{W_b - W_a}\n$$", "id": "3045439"}, {"introduction": "With the basic definition in hand, we can now explore the statistical properties of the Itô integral. This exercise [@problem_id:3045393] involves an integrand whose value depends on the past of the Brownian motion, making it a more realistic simple process. By calculating the integral's expectation and variance, you will directly verify the Itô isometry, a crucial property that forms the bedrock for extending the integral from simple processes to a much broader class of functions.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space supporting a standard one-dimensional Brownian motion $W=(W_t)_{t\\ge 0}$ adapted to $(\\mathcal{F}_t)_{t\\ge 0}$. Consider the time horizon $T=2$ and the partition $0=t_0<t_1<t_2=T$ with $t_1=1$, $t_2=2$. Define the simple (piecewise constant, adapted) process $X=(X_t)_{t\\in[0,2]}$ by\n$$\nX_t = c\\,\\mathbf{1}_{(t_0,t_1]}(t) + \\big(\\alpha\\,W_{t_1}+\\beta\\big)\\,\\mathbf{1}_{(t_1,t_2]}(t),\n$$\nwhere $c,\\alpha,\\beta\\in\\mathbb{R}$ are deterministic constants and $\\mathbf{1}_A$ denotes the indicator of the set $A$. The Itô integral of a simple process is defined by\n$$\n\\int_0^T X_t\\,dW_t = \\sum_{k=1}^{2} Y_k\\big(W_{t_k}-W_{t_{k-1}}\\big),\n$$\nwhere $Y_1=c$ and $Y_2=\\alpha W_{t_1}+\\beta$, which are $\\mathcal{F}_{t_{k-1}}$-measurable.\n\nStarting from the definition of the Itô integral for simple processes and the defining properties of Brownian motion (independent increments and Gaussian moments), compute the expectation $\\mathbb{E}\\!\\left[\\int_0^2 X_t\\,dW_t\\right]$ and the variance $\\mathrm{Var}\\!\\left(\\int_0^2 X_t\\,dW_t\\right)$. Then verify that your variance agrees with the statement of the Itô isometry for this simple process by evaluating $\\mathbb{E}\\!\\left[\\int_0^2 X_t^2\\,dt\\right]$ directly from the definition of $X$.\n\nExpress your final answer as two entries in a single row, corresponding to $\\mathbb{E}\\!\\left[\\int_0^2 X_t\\,dW_t\\right]$ and $\\mathrm{Var}\\!\\left(\\int_0^2 X_t\\,dW_t\\right)$, respectively. No rounding is required.", "solution": "The problem is well-posed and provides all necessary information to compute the requested quantities. We proceed with the solution.\n\nLet the Itô integral be denoted by $I = \\int_0^2 X_t\\,dW_t$. According to the provided definition for this simple process, with the partition $t_0=0$, $t_1=1$, and $t_2=2$, we have:\n$$\nI = c(W_{t_1} - W_{t_0}) + (\\alpha W_{t_1} + \\beta)(W_{t_2} - W_{t_1})\n$$\nSince $W$ is a standard Brownian motion, $W_{t_0} = W_0 = 0$. Substituting the time points, we get:\n$$\nI = c W_1 + (\\alpha W_1 + \\beta)(W_2 - W_1)\n$$\n\nFirst, we compute the expectation $\\mathbb{E}[I]$. By the linearity of expectation:\n$$\n\\mathbb{E}[I] = \\mathbb{E}[c W_1] + \\mathbb{E}[(\\alpha W_1 + \\beta)(W_2 - W_1)]\n$$\nFor a standard Brownian motion, $\\mathbb{E}[W_t] = 0$ for any $t \\ge 0$. Therefore, the first term is $\\mathbb{E}[c W_1] = c\\,\\mathbb{E}[W_1] = c \\cdot 0 = 0$.\n\nFor the second term, we use the key properties of Brownian motion. The increment $(W_2 - W_1)$ is independent of the filtration $\\mathcal{F}_1$. The random variable $(\\alpha W_1 + \\beta)$ is $\\mathcal{F}_1$-measurable since $W_1$ is. Therefore, $(\\alpha W_1 + \\beta)$ and $(W_2 - W_1)$ are independent. This allows us to separate the expectation:\n$$\n\\mathbb{E}[(\\alpha W_1 + \\beta)(W_2 - W_1)] = \\mathbb{E}[\\alpha W_1 + \\beta] \\cdot \\mathbb{E}[W_2 - W_1]\n$$\nThe expectation of the increment is $\\mathbb{E}[W_2 - W_1] = \\mathbb{E}[W_2] - \\mathbb{E}[W_1] = 0 - 0 = 0$.\nThus, the second term is also zero:\n$$\n\\mathbb{E}[(\\alpha W_1 + \\beta)(W_2 - W_1)] = (\\alpha\\,\\mathbb{E}[W_1] + \\beta) \\cdot 0 = (\\alpha \\cdot 0 + \\beta) \\cdot 0 = 0\n$$\nCombining the terms, the total expectation is:\n$$\n\\mathbb{E}[I] = 0 + 0 = 0\n$$\n\nNext, we compute the variance $\\mathrm{Var}(I)$. Since $\\mathbb{E}[I]=0$, the variance is simply the second moment:\n$$\n\\mathrm{Var}(I) = \\mathbb{E}[I^2] - (\\mathbb{E}[I])^2 = \\mathbb{E}[I^2]\n$$\nWe expand the expression for $I^2$:\n$$\nI^2 = [c W_1 + (\\alpha W_1 + \\beta)(W_2 - W_1)]^2 = c^2 W_1^2 + 2cW_1(\\alpha W_1 + \\beta)(W_2 - W_1) + (\\alpha W_1 + \\beta)^2 (W_2 - W_1)^2\n$$\nWe take the expectation term by term using linearity:\n$$\n\\mathbb{E}[I^2] = \\mathbb{E}[c^2 W_1^2] + \\mathbb{E}[2cW_1(\\alpha W_1 + \\beta)(W_2 - W_1)] + \\mathbb{E}[(\\alpha W_1 + \\beta)^2 (W_2 - W_1)^2]\n$$\nFor the first term, we use $\\mathbb{E}[W_t^2] = \\mathrm{Var}(W_t) = t$:\n$$\n\\mathbb{E}[c^2 W_1^2] = c^2 \\mathbb{E}[W_1^2] = c^2 \\cdot \\mathrm{Var}(W_1) = c^2 \\cdot 1 = c^2\n$$\nFor the second term (the cross-term), we use the tower property of conditional expectation and the independence of increments. The term $2cW_1(\\alpha W_1 + \\beta)$ is $\\mathcal{F}_1$-measurable, while $(W_2 - W_1)$ is independent of $\\mathcal{F}_1$ with mean $0$.\n$$\n\\mathbb{E}[2cW_1(\\alpha W_1 + \\beta)(W_2 - W_1)] = \\mathbb{E}[\\mathbb{E}[2cW_1(\\alpha W_1 + \\beta)(W_2 - W_1) | \\mathcal{F}_1]]\n$$\n$$\n= \\mathbb{E}[2cW_1(\\alpha W_1 + \\beta) \\mathbb{E}[(W_2 - W_1) | \\mathcal{F}_1]] = \\mathbb{E}[2cW_1(\\alpha W_1 + \\beta) \\mathbb{E}[W_2 - W_1]] = \\mathbb{E}[... \\cdot 0] = 0\n$$\nFor the third term, we again use the independence between the $\\mathcal{F}_1$-measurable term $(\\alpha W_1 + \\beta)^2$ and the increment term $(W_2 - W_1)^2$:\n$$\n\\mathbb{E}[(\\alpha W_1 + \\beta)^2 (W_2 - W_1)^2] = \\mathbb{E}[(\\alpha W_1 + \\beta)^2] \\cdot \\mathbb{E}[(W_2 - W_1)^2]\n$$\nThe second factor is the variance of the increment: $\\mathbb{E}[(W_2 - W_1)^2] = \\mathrm{Var}(W_2 - W_1) = 2 - 1 = 1$.\nThe first factor is:\n$$\n\\mathbb{E}[(\\alpha W_1 + \\beta)^2] = \\mathbb{E}[\\alpha^2 W_1^2 + 2\\alpha\\beta W_1 + \\beta^2] = \\alpha^2 \\mathbb{E}[W_1^2] + 2\\alpha\\beta \\mathbb{E}[W_1] + \\beta^2\n$$\nUsing $\\mathbb{E}[W_1] = 0$ and $\\mathbb{E}[W_1^2] = 1$, this becomes:\n$$\n\\mathbb{E}[(\\alpha W_1 + \\beta)^2] = \\alpha^2(1) + 2\\alpha\\beta(0) + \\beta^2 = \\alpha^2 + \\beta^2\n$$\nSo, the expectation of the third term is $(\\alpha^2 + \\beta^2) \\cdot 1 = \\alpha^2 + \\beta^2$.\nCombining all terms for the variance:\n$$\n\\mathrm{Var}(I) = \\mathbb{E}[I^2] = c^2 + 0 + (\\alpha^2 + \\beta^2) = c^2 + \\alpha^2 + \\beta^2\n$$\n\nFinally, we verify this result using the Itô isometry, which states that for a suitable process $X$, $\\mathbb{E}[(\\int_0^T X_t dW_t)^2] = \\mathbb{E}[\\int_0^T X_t^2 dt]$. Since the expectation of our integral is zero, the left side is the variance. We now compute the right side.\nFirst, we find the integral $\\int_0^2 X_t^2 dt$.\n$$\n\\int_0^2 X_t^2 dt = \\int_0^1 X_t^2 dt + \\int_1^2 X_t^2 dt\n$$\nFrom the definition of $X_t$:\n$$\nX_t^2 = \\begin{cases} c^2 & \\text{if } t \\in (0,1] \\\\ (\\alpha W_1 + \\beta)^2 & \\text{if } t \\in (1,2] \\end{cases}\n$$\nThe integral becomes:\n$$\n\\int_0^2 X_t^2 dt = \\int_0^1 c^2 dt + \\int_1^2 (\\alpha W_1 + \\beta)^2 dt\n$$\nThe integrand $(\\alpha W_1 + \\beta)^2$ is a random variable that is constant with respect to the integration variable $t$. Thus, we can evaluate the Riemann integrals:\n$$\n\\int_0^2 X_t^2 dt = c^2[t]_0^1 + (\\alpha W_1 + \\beta)^2[t]_1^2 = c^2(1-0) + (\\alpha W_1 + \\beta)^2(2-1) = c^2 + (\\alpha W_1 + \\beta)^2\n$$\nNow, we take the expectation:\n$$\n\\mathbb{E}\\left[\\int_0^2 X_t^2 dt\\right] = \\mathbb{E}[c^2 + (\\alpha W_1 + \\beta)^2] = \\mathbb{E}[c^2] + \\mathbb{E}[(\\alpha W_1 + \\beta)^2]\n$$\nAs calculated before, $\\mathbb{E}[c^2] = c^2$ and $\\mathbb{E}[(\\alpha W_1 + \\beta)^2] = \\alpha^2 + \\beta^2$.\nTherefore,\n$$\n\\mathbb{E}\\left[\\int_0^2 X_t^2 dt\\right] = c^2 + \\alpha^2 + \\beta^2\n$$\nThis result matches the variance calculated directly, thus verifying the Itô isometry for this simple process.\n\nThe requested quantities are the expectation and the variance of the Itô integral.\nExpectation: $\\mathbb{E}\\!\\left[\\int_0^2 X_t\\,dW_t\\right] = 0$.\nVariance: $\\mathrm{Var}\\!\\left(\\int_0^2 X_t\\,dW_t\\right) = c^2 + \\alpha^2 + \\beta^2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & c^{2} + \\alpha^{2} + \\beta^{2} \\end{pmatrix}}\n$$", "id": "3045393"}, {"introduction": "A key requirement in defining the Itô integral is that the integrand must be *predictable*, meaning its value over an interval cannot depend on future information. This \"no-peeking\" rule is not arbitrary, and this practice is designed to show you why [@problem_id:3045379]. You will explore what happens when this condition is violated, demonstrating through a direct calculation how the essential martingale property of the Itô integral breaks down.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ be a filtered probability space supporting a standard Brownian motion $W=(W_{t})_{t \\geq 0}$ adapted to $(\\mathcal{F}_{t})_{t \\geq 0}$, with the usual augmentation of the natural filtration. A simple process $H=(H_{t})_{t \\geq 0}$ is any process of the form\n$$\nH_{t}=\\sum_{i=0}^{n-1} \\xi_{i}\\,\\mathbf{1}_{(t_{i},t_{i+1}]}(t),\n$$\nwhere $0=t_{0}<t_{1}<\\cdots<t_{n}=T$ is a partition of $[0,T]$ and each $\\xi_{i}$ is a random variable. The Itô integral of $H$ with respect to $W$ is defined by\n$$\n\\int_{0}^{T} H_{t}\\,\\mathrm{d}W_{t}=\\sum_{i=0}^{n-1} \\xi_{i}\\,\\big(W_{t_{i+1}}-W_{t_{i}}\\big).\n$$\nAssume familiarity with the fundamental properties of Brownian motion: independent increments, Gaussian increments with $\\mathbb{E}[W_{t}-W_{s}]=0$ and $\\mathrm{Var}(W_{t}-W_{s})=t-s$ for $0 \\leq s<t$, and the tower property of conditional expectation.\n\n1. Starting from these properties, explain conceptually why, when $H$ is adapted (that is, each $\\xi_{i}$ is $\\mathcal{F}_{t_{i}}$-measurable), the process $\\left(\\int_{0}^{t} H_{s}\\,\\mathrm{d}W_{s}\\right)_{t \\geq 0}$ has the martingale property with respect to $(\\mathcal{F}_{t})_{t \\geq 0}$.\n\n2. Now deliberately violate adaptedness as follows. Take $T=2$ and the partition $0<t_{1}=1<t_{2}=2$. Define a simple process $H$ by\n$$\nH_{t}=\\xi_{0}\\,\\mathbf{1}_{(0,1]}(t)+\\xi_{1}\\,\\mathbf{1}_{(1,2]}(t),\n$$\nwith $\\xi_{0}=0$ and $\\xi_{1}=W_{2}-W_{1}$, which is $\\mathcal{F}_{2}$-measurable. Using only the fundamental properties stated above, compute the conditional expectation\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{2} H_{t}\\,\\mathrm{d}W_{t}\\,\\bigg|\\,\\mathcal{F}_{1}\\right].\n$$\nProvide your final numeric value exactly; no rounding is required.", "solution": "This problem consists of two parts. The first part requires a conceptual explanation of why the Itô integral of an adapted simple process is a martingale. The second part requires the calculation of a conditional expectation for a specific Itô integral where the integrand is deliberately chosen to be non-adapted.\n\nPart 1: Conceptual explanation of the martingale property.\n\nThe process $M_{t} = \\int_{0}^{t} H_{s}\\,\\mathrm{d}W_{s}$ is a martingale with respect to the filtration $(\\mathcal{F}_{t})_{t \\geq 0}$ if it satisfies three conditions:\n1. $M_{t}$ is $\\mathcal{F}_{t}$-measurable for all $t \\geq 0$.\n2. $\\mathbb{E}[|M_{t}|] < \\infty$ for all $t \\geq 0$.\n3. $\\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = M_{s}$ for all $0 \\leq s < t$.\n\nWe will focus on the conceptual justification for the third property, which is the core of the martingale definition. Let $H$ be a simple process of the form $H_{u}=\\sum_{i=0}^{n-1} \\xi_{i}\\,\\mathbf{1}_{(t_{i},t_{i+1}]}(u)$ on the interval $[0,T]$, where $0=t_0 < t_1 < \\dots < t_n = T$. The Itô integral up to time $t \\in [0,T]$ is $M_t = \\int_0^t H_u \\mathrm{d}W_u$.\n\nLet $s$ and $t$ be two time points such that $0 \\leq s < t \\leq T$. We can split the integral $M_{t}$ at time $s$:\n$$\nM_{t} = \\int_{0}^{t} H_{u}\\,\\mathrm{d}W_{u} = \\int_{0}^{s} H_{u}\\,\\mathrm{d}W_{u} + \\int_{s}^{t} H_{u}\\,\\mathrm{d}W_{u} = M_{s} + \\int_{s}^{t} H_{u}\\,\\mathrm{d}W_{u}.\n$$\nNow, we take the conditional expectation with respect to $\\mathcal{F}_{s}$:\n$$\n\\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[M_{s} + \\int_{s}^{t} H_{u}\\,\\mathrm{d}W_{u} \\,\\bigg|\\, \\mathcal{F}_{s}\\right].\n$$\nBy the linearity of conditional expectation, this becomes:\n$$\n\\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}[M_{s} | \\mathcal{F}_{s}] + \\mathbb{E}\\left[\\int_{s}^{t} H_{u}\\,\\mathrm{d}W_{u} \\,\\bigg|\\, \\mathcal{F}_{s}\\right].\n$$\nThe process $M_s = \\int_0^s H_u \\mathrm{d}W_u$ is defined based on the history of the Brownian motion up to time $s$, and because $H$ is adapted, $M_s$ is $\\mathcal{F}_s$-measurable. Therefore, conditioning on the information at time $s$ does not change its value: $\\mathbb{E}[M_{s} | \\mathcal{F}_{s}] = M_{s}$.\nThe martingale property thus holds if and only if the second term is zero:\n$$\n\\mathbb{E}\\left[\\int_{s}^{t} H_{u}\\,\\mathrm{d}W_{u} \\,\\bigg|\\, \\mathcal{F}_{s}\\right] = 0.\n$$\nLet us analyze this term. The integral from $s$ to $t$ comprises a sum of terms of the form $\\xi_j (W_{t_{j+1}} - W_{t_j})$ for time intervals $(t_j, t_{j+1}]$ that are fully or partially after time $s$.\nConsider a representative term in this sum, $\\xi_{j}(W_{t_{j+1}}-W_{t_{j}})$, where $s \\leq t_{j} < t_{j+1}$. The condition that $H$ is an adapted process means that each random coefficient $\\xi_{j}$ must be $\\mathcal{F}_{t_{j}}$-measurable. This is the crucial property of non-anticipation: the value of the integrand over the interval $(t_j, t_{j+1}]$ is determined by information available at the beginning of the interval, time $t_j$.\n\nWe now compute the conditional expectation of this term using the tower property of expectation, since $\\mathcal{F}_{s} \\subseteq \\mathcal{F}_{t_j}$:\n$$\n\\mathbb{E}[\\xi_{j}(W_{t_{j+1}}-W_{t_{j}}) | \\mathcal{F}_{s}] = \\mathbb{E}\\big[\\mathbb{E}[\\xi_{j}(W_{t_{j+1}}-W_{t_{j}}) | \\mathcal{F}_{t_j}] \\big| \\mathcal{F}_{s}\\big].\n$$\nInside the inner expectation, since $\\xi_{j}$ is $\\mathcal{F}_{t_{j}}$-measurable, it can be treated as a constant:\n$$\n\\mathbb{E}[\\xi_{j}(W_{t_{j+1}}-W_{t_{j}}) | \\mathcal{F}_{t_j}] = \\xi_{j} \\mathbb{E}[W_{t_{j+1}}-W_{t_{j}} | \\mathcal{F}_{t_j}].\n$$\nA fundamental property of Brownian motion is that its increments are independent of the past. The increment $W_{t_{j+1}}-W_{t_{j}}$ is independent of the entire history up to time $t_{j}$, which is captured by the filtration $\\mathcal{F}_{t_j}$. Therefore, conditioning on $\\mathcal{F}_{t_j}$ does not alter its expectation:\n$$\n\\mathbb{E}[W_{t_{j+1}}-W_{t_{j}} | \\mathcal{F}_{t_j}] = \\mathbb{E}[W_{t_{j+1}}-W_{t_{j}}].\n$$\nFurthermore, Brownian increments have zero mean: $\\mathbb{E}[W_{t_{j+1}}-W_{t_{j}}] = 0$.\nSubstituting this back, we get:\n$$\n\\mathbb{E}[\\xi_{j}(W_{t_{j+1}}-W_{t_{j}}) | \\mathcal{F}_{t_j}] = \\xi_{j} \\cdot 0 = 0.\n$$\nThe outer expectation is then $\\mathbb{E}[0 | \\mathcal{F}_{s}] = 0$. This argument applies to every term in the sum for the integral $\\int_s^t H_u \\mathrm{d}W_u$. (A slight modification is needed for the interval in which $s$ lies, but the principle is identical).\n\nConceptually, the adaptedness of $H$ ensures that at each step, we are multiplying a quantity $\\xi_j$ (known at time $t_j$) by a future, unpredictable noise increment $(W_{t_{j+1}}-W_{t_{j}})$ which has a mean of zero. The product's conditional expectation at any prior time $s$ is therefore zero. Summing these zero-expectation terms results in a total conditional expectation of zero for the future part of the integral. This is precisely why adaptedness is a cornerstone requirement for the Itô integral to be a martingale.\n\nPart 2: Calculation with violated adaptedness.\n\nWe are given the simple process $H_{t}=\\xi_{0}\\,\\mathbf{1}_{(0,1]}(t)+\\xi_{1}\\,\\mathbf{1}_{(1,2]}(t)$ on the interval $[0,2]$, with the partition $t_0=0 < t_1=1 < t_2=2$. The coefficients are specified as $\\xi_0=0$ and $\\xi_1=W_2-W_1$.\nThe Itô integral from $0$ to $2$ is defined as:\n$$\n\\int_{0}^{2} H_{t}\\,\\mathrm{d}W_{t} = \\sum_{i=0}^{1} \\xi_{i}\\,(W_{t_{i+1}}-W_{t_{i}}) = \\xi_0(W_{t_1}-W_{t_0}) + \\xi_1(W_{t_2}-W_{t_1}).\n$$\nSubstituting the given values for the partition times and coefficients, and using $W_{t_0}=W_0=0$:\n$$\n\\int_{0}^{2} H_{t}\\,\\mathrm{d}W_{t} = (0)(W_{1}-W_{0}) + (W_{2}-W_{1})(W_{2}-W_{1}) = 0 + (W_{2}-W_{1})^{2}.\n$$\nSo, the integral simplifies to the random variable $(W_2-W_1)^2$.\n\nThe problem asks for the computation of the conditional expectation $\\mathbb{E}\\!\\left[\\int_{0}^{2} H_{t}\\,\\mathrm{d}W_{t}\\,\\bigg|\\,\\mathcal{F}_{1}\\right]$. Substituting the expression for the integral, we need to compute:\n$$\n\\mathbb{E}\\big[(W_{2}-W_{1})^{2} \\,\\big|\\, \\mathcal{F}_{1}\\big].\n$$\nHere, the integrand $H$ is not adapted because the coefficient $\\xi_1 = W_2 - W_1$ for the interval $(1,2]$ is not $\\mathcal{F}_1$-measurable; it depends on the future value $W_2$. This violates the non-anticipation principle discussed in Part 1.\n\nTo compute the conditional expectation, we use the property of independent increments of Brownian motion. The increment $W_2 - W_1$ is a random variable that depends only on the behavior of the Brownian path between times $t=1$ and $t=2$. This increment is independent of the history of the process up to time $t=1$, which is represented by the sigma-algebra $\\mathcal{F}_1$.\nSince the random variable $(W_2-W_1)^2$ is a function of an increment that is independent of $\\mathcal{F}_1$, the conditional expectation with respect to $\\mathcal{F}_1$ is equal to its unconditional expectation:\n$$\n\\mathbb{E}\\big[(W_{2}-W_{1})^{2} \\,\\big|\\, \\mathcal{F}_{1}\\big] = \\mathbb{E}\\big[(W_{2}-W_{1})^{2}\\big].\n$$\nThe expression $\\mathbb{E}\\big[(W_{2}-W_{1})^{2}\\big]$ is the second moment of the random variable $W_2-W_1$. According to the properties of Brownian motion, the increment $W_t - W_s$ is normally distributed with mean $0$ and variance $t-s$. For $t=2$ and $s=1$, the increment $W_2-W_1$ follows a normal distribution with mean $\\mathbb{E}[W_2-W_1]=0$ and variance $\\mathrm{Var}(W_2-W_1)=2-1=1$.\nThe second moment of a random variable $X$ is related to its variance and mean by $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2$.\nFor $X = W_2 - W_1$, we have:\n$$\n\\mathbb{E}\\big[(W_{2}-W_{1})^{2}\\big] = \\mathrm{Var}(W_2-W_1) + \\big(\\mathbb{E}[W_2-W_1]\\big)^{2} = 1 + (0)^2 = 1.\n$$\nThus, the value of the conditional expectation is $1$.\n\nThis result demonstrates the consequence of violating the adaptedness condition. If the process were a martingale, we would expect $\\mathbb{E}[\\int_0^2 H_t \\mathrm{d}W_t | \\mathcal{F}_1] = \\int_0^1 H_t \\mathrm{d}W_t$. The integral up to time $1$ is $\\int_0^1 H_t \\mathrm{d}W_t = \\xi_0(W_1-W_0) = 0 \\cdot W_1 = 0$. Since our calculated result is $1$, not $0$, the process $M_t = \\int_0^t H_s \\mathrm{d}W_s$ is not a martingale.", "answer": "$$\\boxed{1}$$", "id": "3045379"}]}