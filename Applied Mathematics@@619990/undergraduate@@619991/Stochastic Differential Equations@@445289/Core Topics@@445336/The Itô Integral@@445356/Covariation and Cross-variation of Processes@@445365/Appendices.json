{"hands_on_practices": [{"introduction": "The abstract concept of quadratic variation finds a concrete anchor in the statistical estimation of volatility. This first practice invites you to explore this connection by treating the sum of squared increments of a Brownian motion path as a statistical estimator, known as \"realized variance\" [@problem_id:3047531]. By analyzing its statistical properties, such as its expectation and error, you will gain a deeper, more tangible understanding of why the quadratic variation of a standard Brownian motion, $[W]_t$, is the deterministic process $t$.", "problem": "Let $\\{W_{s}: s \\geq 0\\}$ be a standard Brownian motion with $W_{0} = 0$, independent increments, and for $0 \\leq s < u$, the increment $W_{u} - W_{s}$ is Gaussian with mean $0$ and variance $u - s$. Fix $t > 0$ and consider a finite partition $\\Pi$ of $[0,t]$ given by $0 = t_{0} < t_{1} < \\cdots < t_{n} = t$, with mesh size $|\\Pi| := \\max_{0 \\leq k \\leq n-1} (t_{k+1} - t_{k})$. Define the realized variance estimator\n$$\n\\operatorname{RV}(\\Pi) := \\sum_{k=0}^{n-1} \\big(W_{t_{k+1}} - W_{t_{k}}\\big)^{2}.\n$$\nStarting only from the defining properties of Brownian motion and basic moment identities for Gaussian random variables, derive the expectation $\\mathbb{E}[\\operatorname{RV}(\\Pi)]$ and the root mean squared error (RMSE) $\\sqrt{\\mathbb{E}\\big[(\\operatorname{RV}(\\Pi) - t)^{2}\\big]}$ in terms of the partition lengths $h_{k} := t_{k+1} - t_{k}$. Use these to determine the convergence rate, in mean square, of $\\operatorname{RV}(\\Pi)$ to $t$ as $|\\Pi| \\to 0$, and provide an explicit upper bound on the RMSE in terms of $t$ and $|\\Pi|$.\n\nFinally, specialize to an equally spaced partition of $[0,t]$ with $n$ intervals, so that $h_{k} = \\Delta := t/n$ for all $k$, and determine the exact RMSE as a closed-form function of $\\Delta$. Your final answer must be this exact RMSE as a symbolic expression in $t$ and $\\Delta$.", "solution": "The problem statement is a standard exercise in stochastic calculus, concerning the properties of the realized variance of a standard Brownian motion. All definitions and conditions are standard and self-consistent. The problem is mathematically well-posed, scientifically grounded in the theory of stochastic processes, and objectively stated. Therefore, the problem is valid and we may proceed with the solution.\n\nLet $\\{W_{s}: s \\geq 0\\}$ be a standard one-dimensional Brownian motion. We are given a partition of the interval $[0,t]$, $0 = t_{0} < t_{1} < \\cdots < t_{n} = t$. Let $h_{k} := t_{k+1} - t_{k}$ for $k = 0, 1, \\ldots, n-1$. The realized variance estimator is defined as:\n$$\n\\operatorname{RV}(\\Pi) := \\sum_{k=0}^{n-1} \\big(W_{t_{k+1}} - W_{t_{k}}\\big)^{2}\n$$\nLet's denote the increment $\\Delta W_{k} := W_{t_{k+1}} - W_{t_{k}}$. According to the definition of standard Brownian motion, the increment $\\Delta W_{k}$ over the interval $[t_k, t_{k+1}]$ is a Gaussian random variable with mean $0$ and variance $t_{k+1} - t_{k} = h_k$. That is, $\\Delta W_{k} \\sim \\mathcal{N}(0, h_{k})$.\n\nFirst, we compute the expectation of the realized variance, $\\mathbb{E}[\\operatorname{RV}(\\Pi)]$.\nBy the linearity of expectation, we have:\n$$\n\\mathbb{E}[\\operatorname{RV}(\\Pi)] = \\mathbb{E}\\left[\\sum_{k=0}^{n-1} (\\Delta W_{k})^{2}\\right] = \\sum_{k=0}^{n-1} \\mathbb{E}\\left[(\\Delta W_{k})^{2}\\right]\n$$\nThe quantity $\\mathbb{E}[(\\Delta W_{k})^{2}]$ is the second moment of the random variable $\\Delta W_{k}$. For any random variable $X$ with mean $\\mathbb{E}[X]$ and variance $\\operatorname{Var}(X)$, the second moment is given by $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + (\\mathbb{E}[X])^2$.\nFor $\\Delta W_{k} \\sim \\mathcal{N}(0, h_{k})$, we have $\\mathbb{E}[\\Delta W_{k}] = 0$ and $\\operatorname{Var}(\\Delta W_{k}) = h_{k}$.\nTherefore, the second moment is:\n$$\n\\mathbb{E}\\left[(\\Delta W_{k})^{2}\\right] = h_{k} + (0)^2 = h_{k}\n$$\nSubstituting this back into the expression for the expectation of $\\operatorname{RV}(\\Pi)$:\n$$\n\\mathbb{E}[\\operatorname{RV}(\\Pi)] = \\sum_{k=0}^{n-1} h_{k} = \\sum_{k=0}^{n-1} (t_{k+1} - t_{k}) = t_{n} - t_{0} = t - 0 = t\n$$\nThis shows that $\\operatorname{RV}(\\Pi)$ is an unbiased estimator of $t$.\n\nNext, we compute the root mean squared error (RMSE), which is defined as $\\sqrt{\\mathbb{E}\\big[(\\operatorname{RV}(\\Pi) - t)^{2}\\big]}$. The term inside the square root is the mean squared error (MSE). Since we have shown that $\\mathbb{E}[\\operatorname{RV}(\\Pi)] = t$, the MSE is equal to the variance of $\\operatorname{RV}(\\Pi)$:\n$$\n\\text{MSE} = \\mathbb{E}\\big[(\\operatorname{RV}(\\Pi) - \\mathbb{E}[\\operatorname{RV}(\\Pi)])^{2}\\big] = \\operatorname{Var}(\\operatorname{RV}(\\Pi))\n$$\n$$\n\\operatorname{Var}(\\operatorname{RV}(\\Pi)) = \\operatorname{Var}\\left(\\sum_{k=0}^{n-1} (\\Delta W_{k})^{2}\\right)\n$$\nA fundamental property of Brownian motion is that its increments over non-overlapping time intervals are independent. The intervals $[t_k, t_{k+1}]$ for $k=0, 1, \\ldots, n-1$ are non-overlapping. Thus, the random variables $\\Delta W_{0}, \\Delta W_{1}, \\ldots, \\Delta W_{n-1}$ are mutually independent. Consequently, any functions of these increments, such as $(\\Delta W_{k})^{2}$, are also mutually independent.\nThe variance of a sum of independent random variables is the sum of their variances:\n$$\n\\operatorname{Var}(\\operatorname{RV}(\\Pi)) = \\sum_{k=0}^{n-1} \\operatorname{Var}\\left((\\Delta W_{k})^{2}\\right)\n$$\nTo calculate $\\operatorname{Var}((\\Delta W_{k})^{2})$, we use the formula $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, with $Y = (\\Delta W_{k})^{2}$.\n$$\n\\operatorname{Var}\\left((\\Delta W_{k})^{2}\\right) = \\mathbb{E}\\left[\\left((\\Delta W_{k})^{2}\\right)^{2}\\right] - \\left(\\mathbb{E}\\left[(\\Delta W_{k})^{2}\\right]\\right)^{2} = \\mathbb{E}\\left[(\\Delta W_{k})^{4}\\right] - (h_{k})^2\n$$\nWe need the fourth moment of $\\Delta W_{k} \\sim \\mathcal{N}(0, h_{k})$. A general centered Gaussian random variable $X \\sim \\mathcal{N}(0, \\sigma^2)$ can be written as $X = \\sigma Z$, where $Z \\sim \\mathcal{N}(0, 1)$ is a standard normal variable. The moments of $X$ are $\\mathbb{E}[X^p] = \\sigma^p \\mathbb{E}[Z^p]$. For a standard normal variable, the fourth moment is $\\mathbb{E}[Z^4] = 3$.\nThus, for $\\Delta W_k \\sim \\mathcal{N}(0, h_k)$, we have $\\sigma^2 = h_k$, so:\n$$\n\\mathbb{E}\\left[(\\Delta W_{k})^{4}\\right] = (h_k)^2 \\mathbb{E}[Z^4] = 3h_k^2\n$$\nNow, we can compute the variance of $(\\Delta W_{k})^{2}$:\n$$\n\\operatorname{Var}\\left((\\Delta W_{k})^{2}\\right) = 3h_k^2 - (h_k)^2 = 2h_k^2\n$$\nSubstituting this result back into the expression for the MSE:\n$$\n\\text{MSE} = \\mathbb{E}\\big[(\\operatorname{RV}(\\Pi) - t)^{2}\\big] = \\sum_{k=0}^{n-1} 2h_k^2 = 2 \\sum_{k=0}^{n-1} h_k^2\n$$\nThe RMSE is the square root of the MSE:\n$$\n\\text{RMSE} = \\sqrt{2 \\sum_{k=0}^{n-1} h_k^2}\n$$\nTo analyze the convergence, let $|\\Pi| = \\max_{k} h_k$ be the mesh of the partition. We can bound the sum:\n$$\n\\sum_{k=0}^{n-1} h_k^2 \\leq \\sum_{k=0}^{n-1} \\left(h_k \\cdot \\max_{j} h_j\\right) = |\\Pi| \\sum_{k=0}^{n-1} h_k = |\\Pi| \\cdot t\n$$\nThus, the MSE is bounded by:\n$$\n\\mathbb{E}\\big[(\\operatorname{RV}(\\Pi) - t)^{2}\\big] \\leq 2t|\\Pi|\n$$\nAs $|\\Pi| \\to 0$, the MSE approaches $0$, which proves that $\\operatorname{RV}(\\Pi)$ converges to $t$ in mean square. The rate of convergence of the MSE is of order $O(|\\Pi|)$. An explicit upper bound on the RMSE is:\n$$\n\\text{RMSE} \\leq \\sqrt{2t|\\Pi|}\n$$\nThis shows that the RMSE converges to $0$ at a rate of $O(\\sqrt{|\\Pi|})$.\n\nFinally, we specialize to an equally spaced partition where $h_k = \\Delta$ for all $k=0, \\ldots, n-1$, with $\\Delta = t/n$.\nWe use the exact expression for the MSE:\n$$\n\\text{MSE} = 2 \\sum_{k=0}^{n-1} h_k^2 = 2 \\sum_{k=0}^{n-1} \\Delta^2\n$$\nThe sum has $n$ identical terms, so:\n$$\n\\text{MSE} = 2 n \\Delta^2\n$$\nSince $\\Delta = t/n$, we have $n = t/\\Delta$. Substituting for $n$:\n$$\n\\text{MSE} = 2 \\left(\\frac{t}{\\Delta}\\right) \\Delta^2 = 2t\\Delta\n$$\nThe exact RMSE for this specific case is the square root of this MSE:\n$$\n\\text{RMSE} = \\sqrt{2t\\Delta}\n$$\nThis is the required closed-form expression as a function of $t$ and $\\Delta$.", "answer": "$$\n\\boxed{\\sqrt{2t\\Delta}}\n$$", "id": "3047531"}, {"introduction": "Having examined the foundational case of Brownian motion, we now generalize to the broader class of continuous martingales defined by Itô integrals. This problem shifts our perspective from the limit of partition sums to a more powerful and elegant characterization: the quadratic variation $[M]_t$ is the unique process that makes $M_t^2 - [M]_t$ a martingale [@problem_id:3047526]. This exercise will guide you through deriving one of the most important formulas in stochastic calculus, revealing the central role of the Itô isometry in linking the integrand to the resulting process's variation.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\geq 0},\\mathbb{P})$ be a filtered probability space supporting a standard Brownian motion $W=(W_t)_{t\\geq 0}$. Let $\\phi=(\\phi_t)_{t\\in[0,T]}$ be an $(\\mathcal{F}_t)$-progressively measurable process such that $\\mathbb{E}\\!\\left[\\int_0^T \\phi_s^2\\,ds\\right]<\\infty$ for some fixed $T>0$. Define the continuous local martingale $M=(M_t)_{t\\in[0,T]}$ by\n$$\nM_t \\;=\\; \\int_0^t \\phi_s\\,dW_s,\\qquad t\\in[0,T].\n$$\nStarting from the definition of quadratic variation for continuous local martingales as the unique adapted, increasing process $[M]=( [M]_t )_{t\\in[0,T]}$ with $[M]_0=0$ such that $M_t^2 - [M]_t$ is a martingale, and using only foundational results about Itô integrals, derive an explicit expression for $[M]_t$ in terms of $\\phi$ and $t$. In your derivation, explain the role of the Itô isometry in identifying and verifying your expression. Provide your final result as a single closed-form analytic expression in terms of $\\phi$ and $t$. No numerical rounding is required.", "solution": "The problem requires the derivation of the quadratic variation, $[M]_t$, for a continuous local martingale $M_t$ defined by an Itô integral, $M_t = \\int_0^t \\phi_s\\,dW_s$. The derivation must be based on the definition of quadratic variation as the unique, adapted, increasing process $[M]_t$ with $[M]_0=0$ such that $M_t^2 - [M]_t$ is a local martingale.\n\nFirst, we analyze the properties of the process $M_t$. The problem states that $\\phi_t$ is a progressively measurable process satisfying the condition $\\mathbb{E}\\left[\\int_0^T \\phi_s^2\\,ds\\right] < \\infty$. This condition implies that the Itô integral $M_t = \\int_0^t \\phi_s\\,dW_s$ is not just a continuous local martingale, but a true square-integrable martingale on the interval $[0,T]$. This simplifies our analysis as we do not need to employ localization arguments with stopping times. Consequently, we need to find the unique adapted, increasing process $[M]_t$ with $[M]_0=0$ such that $M_t^2 - [M]_t$ is a true martingale.\n\nLet us propose a candidate for the quadratic variation process. A foundational result of Itô calculus suggests that the quadratic variation of $M_t$ is related to the integral of the square of the integrand $\\phi_s$. We therefore conjecture that the quadratic variation is given by the process $A_t$ defined as:\n$$\nA_t = \\int_0^t \\phi_s^2\\,ds\n$$\nTo validate this conjecture, we must verify that $A_t$ satisfies all the properties stipulated in the definition of $[M]_t$.\n\n1.  **Initial Condition:** At $t=0$, we have $A_0 = \\int_0^0 \\phi_s^2\\,ds = 0$. This condition is satisfied.\n\n2.  **Adapted and Increasing:** The process $\\phi_s$ is given as progressively measurable, which implies it is adapted to the filtration $(\\mathcal{F}_s)$. The process $\\phi_s^2$ is therefore also adapted. The integral $A_t = \\int_0^t \\phi_s^2\\,ds$ is a process whose value at time $t$ depends on the path of $\\phi$ up to time $t$, and is thus adapted. Furthermore, since $\\phi_s^2 \\ge 0$ for all $s$, the integral $A_t$ is a non-decreasing function of $t$ for any sample path. In the language of stochastic processes, it is an increasing process.\n\n3.  **Martingale Property:** The central task is to demonstrate that the process $X_t = M_t^2 - A_t$ is a martingale. To do this, we must show that for any $s < t$ with $s,t \\in [0,T]$, we have $\\mathbb{E}[X_t | \\mathcal{F}_s] = X_s$.\n\nLet's compute the conditional expectation of $X_t$:\n$$\n\\mathbb{E}[X_t | \\mathcal{F}_s] = \\mathbb{E}[M_t^2 - A_t | \\mathcal{F}_s] = \\mathbb{E}[M_t^2 | \\mathcal{F}_s] - \\mathbb{E}[A_t | \\mathcal{F}_s]\n$$\nWe analyze each term separately. For the first term, we express $M_t$ in terms of $M_s$ and the increment from $s$ to $t$:\n$$\nM_t = M_s + (M_t - M_s) = M_s + \\int_s^t \\phi_u\\,dW_u\n$$\nSquaring this expression gives:\n$$\nM_t^2 = M_s^2 + 2M_s(M_t - M_s) + (M_t - M_s)^2\n$$\nNow, we take the conditional expectation with respect to $\\mathcal{F}_s$:\n$$\n\\mathbb{E}[M_t^2 | \\mathcal{F}_s] = \\mathbb{E}[M_s^2 | \\mathcal{F}_s] + 2\\mathbb{E}[M_s(M_t - M_s) | \\mathcal{F}_s] + \\mathbb{E}[(M_t - M_s)^2 | \\mathcal{F}_s]\n$$\nSince $M_s$ is $\\mathcal{F}_s$-measurable, we can pull it out of the conditional expectations:\n- $\\mathbb{E}[M_s^2 | \\mathcal{F}_s] = M_s^2$.\n- $2\\mathbb{E}[M_s(M_t - M_s) | \\mathcal{F}_s] = 2M_s \\mathbb{E}[M_t - M_s | \\mathcal{F}_s]$. Since $M_t$ is a martingale, $\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s$, which implies $\\mathbb{E}[M_t - M_s | \\mathcal{F}_s] = 0$. Thus, this term is zero.\n\nThe third term requires evaluating the conditional second moment of a stochastic integral. This is precisely where the **Itô isometry** plays its crucial role. The conditional form of the Itô isometry states that for an adapted integrand $\\phi_u$:\n$$\n\\mathbb{E}\\left[ \\left(\\int_s^t \\phi_u\\,dW_u\\right)^2 \\bigg| \\mathcal{F}_s \\right] = \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right]\n$$\nApplying this to our term $(M_t - M_s)^2 = (\\int_s^t \\phi_u\\,dW_u)^2$, we get:\n$$\n\\mathbb{E}[(M_t - M_s)^2 | \\mathcal{F}_s] = \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right]\n$$\nCombining these results, the conditional expectation of $M_t^2$ becomes:\n$$\n\\mathbb{E}[M_t^2 | \\mathcal{F}_s] = M_s^2 + \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right]\n$$\nNow we analyze the second part of our original expression, $\\mathbb{E}[A_t | \\mathcal{F}_s]$. We split the integral for $A_t$:\n$$\nA_t = \\int_0^t \\phi_u^2\\,du = \\int_0^s \\phi_u^2\\,du + \\int_s^t \\phi_u^2\\,du = A_s + \\int_s^t \\phi_u^2\\,du\n$$\nTaking the conditional expectation:\n$$\n\\mathbb{E}[A_t | \\mathcal{F}_s] = \\mathbb{E}[A_s | \\mathcal{F}_s] + \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right]\n$$\nSince $A_s = \\int_0^s \\phi_u^2\\,du$ is determined by the history of the process $\\phi$ up to time $s$, it is $\\mathcal{F}_s$-measurable. Therefore, $\\mathbb{E}[A_s | \\mathcal{F}_s] = A_s$. This gives:\n$$\n\\mathbb{E}[A_t | \\mathcal{F}_s] = A_s + \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right]\n$$\nFinally, we substitute our findings for $\\mathbb{E}[M_t^2 | \\mathcal{F}_s]$ and $\\mathbb{E}[A_t | \\mathcal{F}_s]$ back into the expression for $\\mathbb{E}[X_t | \\mathcal{F}_s]$:\n$$\n\\mathbb{E}[X_t | \\mathcal{F}_s] = \\left( M_s^2 + \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right] \\right) - \\left( A_s + \\mathbb{E}\\left[ \\int_s^t \\phi_u^2\\,du \\bigg| \\mathcal{F}_s \\right] \\right)\n$$\nThe conditional expectation terms cancel, leaving:\n$$\n\\mathbb{E}[X_t | \\mathcal{F}_s] = M_s^2 - A_s = X_s\n$$\nThis confirms that $X_t = M_t^2 - A_t$ is a martingale.\n\nSince our candidate process $A_t = \\int_0^t \\phi_s^2\\,ds$ satisfies all the required properties—it is adapted, increasing, starts at zero, and makes $M_t^2 - A_t$ a martingale—it must be the quadratic variation of $M_t$, by the uniqueness property stated in the problem.\n\nThe role of the Itô isometry is central and indispensable in this derivation. It provides the quantitative link between the variance of the martingale increments and the integral of the squared integrand. Specifically, the conditional isometry allows us to precisely calculate $\\mathbb{E}[(M_t-M_s)^2|\\mathcal{F}_s]$ and show that it is compensated by the increment of the process $\\int_0^t \\phi_s^2\\,ds$ in expectation. This confirms that $\\int_0^t \\phi_s^2\\,ds$ is the correct compensator process needed to make $M_t^2$ a martingale, thereby identifying it as the quadratic variation.\n\nThus, the explicit expression for $[M]_t$ is:\n$$\n[M]_t = \\int_0^t \\phi_s^2\\,ds\n$$", "answer": "$$\\boxed{\\int_0^t \\phi_s^2\\,ds}$$", "id": "3047526"}, {"introduction": "Building on the concept of quadratic variation for a single process, we now turn to the interaction between two processes through their covariation, $[X, Y]_t$. This practice returns to first principles to derive the essential polarization identity, a formula that computes covariation from the more readily calculated quadratic variations of sums and differences [@problem_id:3047541]. Mastering this relationship is crucial, as it forms the basis for applying Itô's formula to multiple dimensions and is fundamental to applications like hedging in finance.", "problem": "Let $W_t$ and $\\widetilde{W}_t$ be two independent standard Brownian motions on a filtered probability space satisfying the usual conditions, and let $X_t$ and $Y_t$ be continuous semimartingales defined as solutions to constant-coefficient Stochastic Differential Equations (SDE):\n$$\n\\mathrm{d}X_t = \\alpha\\,\\mathrm{d}t + \\sigma_1\\,\\mathrm{d}W_t + \\sigma_2\\,\\mathrm{d}\\widetilde{W}_t,\\qquad X_0 = 0,\n$$\n$$\n\\mathrm{d}Y_t = \\beta\\,\\mathrm{d}t + \\tau_1\\,\\mathrm{d}W_t + \\tau_2\\,\\mathrm{d}\\widetilde{W}_t,\\qquad Y_0 = 0,\n$$\nwhere $\\alpha,\\beta,\\sigma_1,\\sigma_2,\\tau_1,\\tau_2 \\in \\mathbb{R}$ are fixed constants. Using only foundational definitions of quadratic variation and covariation for continuous semimartingales (as limits of partition sums), perform the following:\n\n1. Starting from the partition-based definitions, justify why finite variation terms do not contribute to quadratic variation and compute $[X]_t$ and $[Y]_t$.\n\n2. Compute $[X+Y]_t$ and $[X-Y]_t$ directly from the dynamics of $X_t$ and $Y_t$, again using the partition-based definitions.\n\n3. Verify the identity $[X,X]_t = [X]_t$ by appealing to the definitions and your computations.\n\n4. Using only algebraic manipulations of the quantities found above and the identity in step $3$, derive two distinct expressions for the covariation $[X,Y]_t$ in terms of quadratic variations of sums and differences, and check they are consistent.\n\nFinally, for a fixed time $T>0$, provide the closed-form expression of $[X,Y]_T$ in terms of $\\sigma_1,\\sigma_2,\\tau_1,\\tau_2$, and $T$. Your final answer must be a single analytic expression. No rounding is required, and no units are involved.", "solution": "The problem is well-defined, scientifically sound, and all necessary information is provided. We shall proceed with a rigorous solution based on the foundational definitions of quadratic variation and covariation for continuous semimartingales.\n\nFor a continuous semimartingale $Z_t$, its quadratic variation $[Z]_t$ over the interval $[0,t]$ is defined as the limit in probability of the sum of squared increments over a partition $\\Pi = \\{0 = t_0 < t_1 < \\dots < t_n = t\\}$ as the mesh of the partition $|\\Pi| = \\max_i(t_{i+1}-t_i)$ approaches zero:\n$$[Z]_t = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (Z_{t_{i+1}} - Z_{t_i})^2$$\nFor two continuous semimartingales $X_t$ and $Y_t$, their covariation $[X,Y]_t$ is defined as:\n$$[X,Y]_t = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (X_{t_{i+1}} - X_{t_i})(Y_{t_{i+1}} - Y_{t_i})$$\n\nThe processes $X_t$ and $Y_t$ are given in integral form as:\n$$X_t = X_0 + \\int_0^t \\alpha\\,\\mathrm{d}s + \\int_0^t \\sigma_1\\,\\mathrm{d}W_s + \\int_0^t \\sigma_2\\,\\mathrm{d}\\widetilde{W}_s = \\alpha t + \\sigma_1 W_t + \\sigma_2 \\widetilde{W}_t$$\n$$Y_t = Y_0 + \\int_0^t \\beta\\,\\mathrm{d}s + \\int_0^t \\tau_1\\,\\mathrm{d}W_s + \\int_0^t \\tau_2\\,\\mathrm{d}\\widetilde{W}_s = \\beta t + \\tau_1 W_t + \\tau_2 \\widetilde{W}_t$$\nWe can decompose $X_t$ into a finite-variation process $A_t = \\alpha t$ and a continuous martingale $M_t = \\sigma_1 W_t + \\sigma_2 \\widetilde{W}_t$. Thus, $X_t = A_t + M_t$. Similarly, $Y_t = B_t + N_t$ where $B_t = \\beta t$ and $N_t = \\tau_1 W_t + \\tau_2 \\widetilde{W}_t$.\n\n1. We first justify why finite variation terms do not contribute to the quadratic variation and then compute $[X]_t$ and $[Y]_t$.\nThe increment of $X_t$ over a subinterval $[t_i, t_{i+1}]$ is $\\Delta X_{t_i} = X_{t_{i+1}} - X_{t_i} = \\Delta A_{t_i} + \\Delta M_{t_i}$. The sum of squared increments is:\n$$\\sum_{i=0}^{n-1} (\\Delta X_{t_i})^2 = \\sum_{i=0}^{n-1} (\\Delta A_{t_i} + \\Delta M_{t_i})^2 = \\sum_{i=0}^{n-1} (\\Delta A_{t_i})^2 + 2\\sum_{i=0}^{n-1} \\Delta A_{t_i} \\Delta M_{t_i} + \\sum_{i=0}^{n-1} (\\Delta M_{t_i})^2$$\nLet us analyze each term in the limit $|\\Pi| \\to 0$.\nThe first term involves the finite variation part $A_t = \\alpha t$. The increment is $\\Delta A_{t_i} = \\alpha (t_{i+1}-t_i)$. The sum of squares is:\n$$\\sum_{i=0}^{n-1} (\\Delta A_{t_i})^2 = \\sum_{i=0}^{n-1} \\alpha^2 (t_{i+1}-t_i)^2 \\leq \\alpha^2 \\max_i(t_{i+1}-t_i) \\sum_{i=0}^{n-1} (t_{i+1}-t_i) = \\alpha^2 t |\\Pi|$$\nAs $|\\Pi| \\to 0$, this term converges to $0$. A process whose quadratic variation is zero is a finite variation process. This demonstrates that the drift term, which is of finite variation, does not contribute to the quadratic variation.\n\nThe second term is the cross-term. It converges in probability to $2[A,M]_t$. Since $A_t$ is a continuous process of finite variation and $M_t$ is a continuous martingale, their covariation $[A,M]_t$ is zero. This can be shown rigorously by examining the $L^2$-limit of the sum $S_n = \\sum_{i=0}^{n-1} \\Delta A_{t_i} \\Delta M_{t_i}$. The expectation is $\\mathbb{E}[S_n]=0$ and its variance is $\\mathbb{E}[S_n^2] = \\alpha^2 \\mathbb{E}\\left[\\left(\\sum_{i=0}^{n-1} (t_{i+1}-t_i) \\Delta M_{t_i}\\right)^2\\right] = \\alpha^2 \\sum_{i=0}^{n-1} (t_{i+1}-t_i)^2 \\mathbb{E}[(\\Delta M_{t_i})^2]$. We have $\\mathbb{E}[(\\Delta M_{t_i})^2] = (\\sigma_1^2 + \\sigma_2^2)(t_{i+1}-t_i)$. Thus, $\\mathbb{E}[S_n^2] = \\alpha^2(\\sigma_1^2+\\sigma_2^2) \\sum (t_{i+1}-t_i)^3 \\leq \\alpha^2(\\sigma_1^2+\\sigma_2^2) |\\Pi| \\sum (t_{i+1}-t_i)^2 \\leq \\alpha^2(\\sigma_1^2+\\sigma_2^2) |\\Pi|^2 t \\to 0$. As $S_n \\to 0$ in $L^2$, it also converges to $0$ in probability.\n\nTherefore, $[X]_t = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (\\Delta M_{t_i})^2 = [M]_t$. We now compute this.\n$\\Delta M_{t_i} = \\sigma_1 \\Delta W_{t_i} + \\sigma_2 \\Delta \\widetilde{W}_{t_i}$, where $\\Delta W_{t_i} = W_{t_{i+1}} - W_{t_i}$ and $\\Delta \\widetilde{W}_{t_i} = \\widetilde{W}_{t_{i+1}} - \\widetilde{W}_{t_i}$.\n$$[M]_t = \\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} (\\sigma_1 \\Delta W_{t_i} + \\sigma_2 \\Delta \\widetilde{W}_{t_i})^2$$\n$$= \\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} \\left( \\sigma_1^2 (\\Delta W_{t_i})^2 + \\sigma_2^2 (\\Delta \\widetilde{W}_{t_i})^2 + 2\\sigma_1\\sigma_2 \\Delta W_{t_i} \\Delta \\widetilde{W}_{t_i} \\right)$$\nBy linearity of limits, we can separate the terms:\n$$[M]_t = \\sigma_1^2 \\lim_{|\\Pi|\\to 0} \\sum (\\Delta W_{t_i})^2 + \\sigma_2^2 \\lim_{|\\Pi|\\to 0} \\sum (\\Delta \\widetilde W_{t_i})^2 + 2\\sigma_1\\sigma_2 \\lim_{|\\Pi|\\to 0} \\sum \\Delta W_{t_i} \\Delta \\widetilde W_{t_i}$$\nFrom the definition of quadratic variation, $\\lim \\sum (\\Delta W_{t_i})^2 = [W]_t = t$ and $\\lim \\sum (\\Delta \\widetilde{W}_{t_i})^2 = [\\widetilde{W}]_t = t$. The cross-term sum converges to the covariation $[W, \\widetilde{W}]_t$. Since $W_t$ and $\\widetilde{W}_t$ are independent Brownian motions, their covariation is zero, i.e., $[W, \\widetilde{W}]_t=0$.\nThus, we find $[X]_t = [M]_t = \\sigma_1^2 t + \\sigma_2^2 t + 0 = (\\sigma_1^2 + \\sigma_2^2)t$.\nBy an identical argument for $Y_t$, its finite variation part $B_t=\\beta t$ does not contribute, and its quadratic variation is determined by its martingale part $N_t = \\tau_1 W_t + \\tau_2 \\widetilde{W}_t$.\n$$[Y]_t = [N]_t = (\\tau_1^2 + \\tau_2^2)t$$\n\n2. We compute $[X+Y]_t$ and $[X-Y]_t$ from their dynamics.\nThe SDE for $Z_t = X_t+Y_t$ is:\n$$\\mathrm{d}Z_t = \\mathrm{d}X_t + \\mathrm{d}Y_t = (\\alpha+\\beta)\\mathrm{d}t + (\\sigma_1+\\tau_1)\\mathrm{d}W_t + (\\sigma_2+\\tau_2)\\mathrm{d}\\widetilde{W}_t$$\nThis is a process of the same form. The drift term $(\\alpha+\\beta)t$ is of finite variation and does not contribute to $[Z]_t$. The martingale part is $(\\sigma_1+\\tau_1)W_t + (\\sigma_2+\\tau_2)\\widetilde{W}_t$. Applying the result from part 1, we replace $\\sigma_1$ with $(\\sigma_1+\\tau_1)$ and $\\sigma_2$ with $(\\sigma_2+\\tau_2)$:\n$$[X+Y]_t = [Z]_t = ((\\sigma_1+\\tau_1)^2 + (\\sigma_2+\\tau_2)^2)t$$\nSimilarly, for $U_t = X_t-Y_t$, the SDE is:\n$$\\mathrm{d}U_t = \\mathrm{d}X_t - \\mathrm{d}Y_t = (\\alpha-\\beta)\\mathrm{d}t + (\\sigma_1-\\tau_1)\\mathrm{d}W_t + (\\sigma_2-\\tau_2)\\mathrm{d}\\widetilde{W}_t$$\nThe finite variation term $(\\alpha-\\beta)t$ does not contribute. The martingale part is $(\\sigma_1-\\tau_1)W_t + (\\sigma_2-\\tau_2)\\widetilde{W}_t$. The quadratic variation is:\n$$[X-Y]_t = [U]_t = ((\\sigma_1-\\tau_1)^2 + (\\sigma_2-\\tau_2)^2)t$$\n\n3. We verify the identity $[X,X]_t = [X]_t$.\nBy the definition of covariation, setting $Y_t = X_t$:\n$$[X,X]_t = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (X_{t_{i+1}} - X_{t_i})(X_{t_{i+1}} - X_{t_i}) = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (X_{t_{i+1}} - X_{t_i})^2$$\nThis is precisely the definition of the quadratic variation $[X]_t$. Therefore, the identity $[X,X]_t = [X]_t$ is true by definition. Our computation of $[X]_t$ in part 1 is also a computation of $[X,X]_t$, confirming their equality.\n\n4. We derive two distinct expressions for the covariation $[X,Y]_t$ and check their consistency.\nThe definition of covariation is a bilinear form. This property stems directly from algebraic expansion of the summands. For $[X+Y]_t$, we have:\n$$[X+Y]_t = [X+Y, X+Y]_t = \\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} (\\Delta X_{t_i} + \\Delta Y_{t_i})^2$$\n$$= \\lim_{|\\Pi|\\to 0} \\left( \\sum (\\Delta X_{t_i})^2 + \\sum (\\Delta Y_{t_i})^2 + 2\\sum \\Delta X_{t_i} \\Delta Y_{t_i} \\right)$$\n$$= [X]_t + [Y]_t + 2[X,Y]_t$$\nRearranging this equation gives the first expression for the covariation:\n$$[X,Y]_t = \\frac{1}{2}\\left([X+Y]_t - [X]_t - [Y]_t\\right) \\quad (\\text{Expression 1})$$\nSimilarly, for $[X-Y]_t$:\n$$[X-Y]_t = [X-Y, X-Y]_t = [X]_t + [Y]_t - 2[X,Y]_t$$\nRearranging gives the second expression:\n$$[X,Y]_t = \\frac{1}{2}\\left([X]_t + [Y]_t - [X-Y]_t\\right) \\quad (\\text{Expression 2})$$\nThese are two distinct expressions for $[X,Y]_t$. We now check their consistency by substituting the results from parts 1 and 2.\n\nUsing Expression 1:\n$$[X,Y]_t = \\frac{1}{2} \\left[ ((\\sigma_1+\\tau_1)^2 + (\\sigma_2+\\tau_2)^2)t - (\\sigma_1^2 + \\sigma_2^2)t - (\\tau_1^2 + \\tau_2^2)t \\right]$$\n$$= \\frac{t}{2} \\left[ (\\sigma_1^2+2\\sigma_1\\tau_1+\\tau_1^2 + \\sigma_2^2+2\\sigma_2\\tau_2+\\tau_2^2) - \\sigma_1^2 - \\sigma_2^2 - \\tau_1^2 - \\tau_2^2 \\right]$$\n$$= \\frac{t}{2} [2\\sigma_1\\tau_1 + 2\\sigma_2\\tau_2] = (\\sigma_1\\tau_1 + \\sigma_2\\tau_2)t$$\n\nUsing Expression 2:\n$$[X,Y]_t = \\frac{1}{2} \\left[ (\\sigma_1^2 + \\sigma_2^2)t + (\\tau_1^2 + \\tau_2^2)t - ((\\sigma_1-\\tau_1)^2 + (\\sigma_2-\\tau_2)^2)t \\right]$$\n$$= \\frac{t}{2} \\left[ \\sigma_1^2 + \\sigma_2^2 + \\tau_1^2 + \\tau_2^2 - (\\sigma_1^2-2\\sigma_1\\tau_1+\\tau_1^2 + \\sigma_2^2-2\\sigma_2\\tau_2+\\tau_2^2) \\right]$$\n$$= \\frac{t}{2} [2\\sigma_1\\tau_1 + 2\\sigma_2\\tau_2] = (\\sigma_1\\tau_1 + \\sigma_2\\tau_2)t$$\nBoth expressions yield the same result, confirming their consistency. This is the desired polarization identity. A direct computation of $[X,Y]_t$ from its definition also gives this result.\nFinally, we provide the closed-form expression for $[X,Y]_T$ for a fixed time $T>0$.\nBased on our derivation, $[X,Y]_t = (\\sigma_1\\tau_1 + \\sigma_2\\tau_2)t$. Setting $t=T$:\n$$[X,Y]_T = (\\sigma_1\\tau_1 + \\sigma_2\\tau_2)T$$", "answer": "$$\n\\boxed{(\\sigma_1\\tau_1 + \\sigma_2\\tau_2)T}\n$$", "id": "3047541"}]}