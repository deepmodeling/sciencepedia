{"hands_on_practices": [{"introduction": "To build a solid foundation in stochastic calculus, we begin with the most fundamental concept: the Itô integral. This first exercise [@problem_id:3061832] strips the integral down to its basics by considering a simple, deterministic integrand. By working through this problem, you will see how the integral is constructed from first principles and directly connect its value to the increments of Brownian motion, while also uncovering a foundational result known as the Itô isometry.", "problem": "Let $\\{W_{s}\\}_{s\\geq 0}$ be a standard Brownian motion (also known as a Wiener process) defined on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{s}\\}_{s\\geq 0},\\mathbb{P})$ satisfying the usual conditions. Fix $t0$ and numbers $0\\leq ab\\leq t$. Consider the process $H_{s}=\\mathbf{1}_{[a,b]}(s)$, where $\\mathbf{1}_{[a,b]}$ denotes the indicator function of the interval $[a,b]$. Starting from the definition of the Itô integral for simple processes and the basic properties of Brownian motion, evaluate the Itô integral $\\int_{0}^{t}H_{s}\\,dW_{s}$ explicitly in terms of $\\{W_{s}\\}$, and compute its variance $\\mathrm{Var}\\!\\left(\\int_{0}^{t}H_{s}\\,dW_{s}\\right)$. Express your final answer as a row matrix with two entries: the value of the integral and its variance. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded within the mathematical framework of stochastic calculus. All provided information is clear, consistent, and sufficient for deriving a unique solution. We may therefore proceed with the evaluation.\n\nThe problem asks for the evaluation of the Itô integral $I = \\int_{0}^{t} H_{s} \\, dW_{s}$ and its variance, where $H_{s} = \\mathbf{1}_{[a,b]}(s)$ for $0 \\leq a  b \\leq t$. The process $H_s$ is a deterministic, elementary process.\n\nFirst, we evaluate the integral $I$. By the definition of the Itô integral, we can split the integration interval $[0,t]$ into subintervals where the integrand $H_s$ is constant. The process $H_s$ is defined as:\n$$\nH_s = \\begin{cases}\n1  \\text{if } s \\in [a,b] \\\\\n0  \\text{if } s \\notin [a,b]\n\\end{cases}\n$$\nThe integral can be decomposed as follows:\n$$\nI = \\int_{0}^{t} H_{s} \\, dW_{s} = \\int_{0}^{a} H_{s} \\, dW_{s} + \\int_{a}^{b} H_{s} \\, dW_{s} + \\int_{b}^{t} H_{s} \\, dW_{s}\n$$\nWe evaluate each term separately.\nFor the interval $[0, a)$, $H_s = 0$. The integral of a zero process is zero:\n$$\n\\int_{0}^{a} 0 \\, dW_{s} = 0\n$$\nFor the interval $[a, b]$, $H_s = 1$. The Itô integral of a constant $c$ over an interval $[t_1, t_2]$ is given by $\\int_{t_1}^{t_2} c \\, dW_s = c(W_{t_2} - W_{t_1})$. In our case, $c=1$, $t_1=a$, and $t_2=b$. Thus,\n$$\n\\int_{a}^{b} 1 \\, dW_{s} = W_{b} - W_{a}\n$$\nFor the interval $(b, t]$, $H_s = 0$. Similar to the first interval, the integral is zero:\n$$\n\\int_{b}^{t} 0 \\, dW_{s} = 0\n$$\nCombining these results, we find the value of the Itô integral:\n$$\nI = 0 + (W_{b} - W_{a}) + 0 = W_{b} - W_{a}\n$$\nThis is the first part of our answer.\n\nNext, we compute the variance of this integral, $\\mathrm{Var}(I) = \\mathrm{Var}(W_{b} - W_{a})$. We will use two methods to confirm the result.\n\nMethod 1: Using the Itô Isometry.\nFor a suitable (predictable and square-integrable) process $\\phi_s$, the Itô isometry property states:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{t} \\phi_{s} \\, dW_{s}\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\phi_{s}^2 \\, ds\\right]\n$$\nThe Itô integral of such a process has a mean of zero, i.e., $\\mathbb{E}\\left[\\int_{0}^{t} \\phi_{s} \\, dW_{s}\\right] = 0$. Therefore, its variance is equal to its second moment:\n$$\n\\mathrm{Var}\\left(\\int_{0}^{t} \\phi_{s} \\, dW_{s}\\right) = \\mathbb{E}\\left[\\left(\\int_{0}^{t} \\phi_{s} \\, dW_{s}\\right)^2\\right]\n$$\nOur process $H_s = \\mathbf{1}_{[a,b]}(s)$ is deterministic and bounded, thus it satisfies the conditions for the isometry to hold. We have:\n$$\n\\mathrm{Var}(I) = \\mathbb{E}\\left[\\left(\\int_{0}^{t} H_{s} \\, dW_{s}\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{t} H_{s}^2 \\, ds\\right]\n$$\nThe square of the integrand is $H_s^2 = (\\mathbf{1}_{[a,b]}(s))^2 = \\mathbf{1}_{[a,b]}(s)$, since the indicator function only takes values $0$ and $1$. The integral inside the expectation is a deterministic Riemann integral:\n$$\n\\int_{0}^{t} H_{s}^2 \\, ds = \\int_{0}^{t} \\mathbf{1}_{[a,b]}(s) \\, ds = \\int_{a}^{b} 1 \\, ds = b - a\n$$\nSince the result of this integral, $b-a$, is a constant, its expectation is the constant itself:\n$$\n\\mathrm{Var}(I) = \\mathbb{E}[b - a] = b - a\n$$\n\nMethod 2: Using the properties of Brownian motion.\nWe found that $I = W_b - W_a$. A fundamental property of standard Brownian motion $\\{W_s\\}_{s \\geq 0}$ is that for any $0 \\leq s_1  s_2$, the increment $W_{s_2} - W_{s_1}$ is a normally distributed random variable with mean $0$ and variance $s_2 - s_1$. That is,\n$$\nW_{s_2} - W_{s_1} \\sim \\mathcal{N}(0, s_2 - s_1)\n$$\nIn our case, since $0 \\leq a  b$, we can apply this property directly to the increment $W_b - W_a$. We have:\n$$\nW_b - W_a \\sim \\mathcal{N}(0, b-a)\n$$\nThe variance of a random variable with this distribution is, by definition, $b-a$.\n$$\n\\mathrm{Var}(I) = \\mathrm{Var}(W_b - W_a) = b - a\n$$\nBoth methods yield the same result, which confirms our calculation.\n\nThe two required quantities are the value of the integral, which is the random variable $W_b - W_a$, and its variance, which is the constant $b-a$.", "answer": "$$\n\\boxed{\\begin{pmatrix} W_b - W_a  b - a \\end{pmatrix}}\n$$", "id": "3061832"}, {"introduction": "Having mastered the integral for a single process, we now expand our toolkit to handle multiple random sources, a common scenario in real-world modeling. The concept of quadratic covariation measures how two Itô processes vary together, which is essential for applying Itô's formula in multiple dimensions. This practice [@problem_id:3061836] guides you through computing the covariation of two independent Brownian motions from its definition, revealing a fundamental principle that greatly simplifies the analysis of complex systems.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions and supporting two independent standard Brownian motions $W^{1}=(W^{1}_{t})_{t\\geq 0}$ and $W^{2}=(W^{2}_{t})_{t\\geq 0}$. Each Brownian motion is an Itô process, which is a special case of a solution to a stochastic differential equation (SDE). Consider the continuous semimartingales $X=(X_{t})_{t\\geq 0}$ and $Y=(Y_{t})_{t\\geq 0}$ defined by $X_{t}=W^{1}_{t}$ and $Y_{t}=W^{2}_{t}$.\n\nStarting from the foundational properties of Brownian motion and the definition of quadratic covariation via refinement limits over partitions of the time interval, perform the following:\n\n- Define the quadratic covariation process $\\langle X,Y\\rangle_{t}$ for general continuous semimartingales $X$ and $Y$ using a partition-based construction.\n- Using only these foundational definitions and properties, compute the quadratic covariation $\\langle W^{1},W^{2}\\rangle_{t}$ for $t\\geq 0$.\n\nYour final answer must be a single closed-form analytic expression in $t$ (no units). Do not provide an inequality or an equation; provide only the value of $\\langle W^{1},W^{2}\\rangle_{t}$ as an expression.", "solution": "The problem requires the definition of the quadratic covariation process for two continuous semimartingales and the computation of this quantity for two independent standard Brownian motions, based on foundational principles.\n\nFirst, we define the quadratic covariation process. Let $X=(X_{t})_{t\\geq 0}$ and $Y=(Y_{t})_{t\\geq 0}$ be two continuous semimartingales defined on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$. For any $t \\geq 0$, let $\\Pi_n = \\{0 = t_{0}^{(n)}  t_{1}^{(n)}  \\dots  t_{k_n}^{(n)} = t\\}$ be a sequence of partitions of the time interval $[0, t]$ such that the mesh of the partition, $\\|\\Pi_n\\| = \\max_{j} (t_{j+1}^{(n)} - t_{j}^{(n)})$, tends to $0$ as $n \\to \\infty$. The quadratic covariation process, denoted $[X, Y]$, is defined for each $t \\ge 0$ as the limit in probability of the sum of products of increments:\n$$\n[X, Y]_t := \\lim_{\\|\\Pi_n\\| \\to 0} \\sum_{j=0}^{k_n-1} (X_{t_{j+1}^{(n)}} - X_{t_{j}^{(n)}})(Y_{t_{j+1}^{(n)}} - Y_{t_{j}^{(n)}})\n$$\nThis limit exists and defines a continuous process of finite variation. The notation $\\langle X, Y \\rangle_t$ used in the problem statement typically refers to the predictable quadratic covariation, which is the compensator of $[X, Y]_t$. For continuous local martingales, which include standard Brownian motions, these two processes are identical, i.e., $[X, Y]_t = \\langle X, Y \\rangle_t$. Since the problem specifies $X_t=W^1_t$ and $Y_t=W^2_t$, which are continuous martingales, we can compute $[W^1, W^2]_t$ using the partition-based definition to find $\\langle W^1, W^2 \\rangle_t$.\n\nNow, we compute $\\langle W^1, W^2 \\rangle_t$. Let $\\Pi = \\{0 = t_0  t_1  \\dots  t_m = t\\}$ be an arbitrary partition of $[0, t]$. For notational simplicity, we drop the sequence index $n$. We are interested in the limit of the sum:\n$$\nS_{\\Pi} = \\sum_{j=0}^{m-1} (W^1_{t_{j+1}} - W^1_{t_j})(W^2_{t_{j+1}} - W^2_{t_j})\n$$\nWe will show that $S_{\\Pi}$ converges to $0$ in $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ as the mesh $\\|\\Pi\\| \\to 0$. Convergence in $L^2$ implies convergence in probability.\n\nLet $\\Delta W^1_j = W^1_{t_{j+1}} - W^1_{t_j}$ and $\\Delta W^2_j = W^2_{t_{j+1}} - W^2_{t_j}$. The sum is $S_{\\Pi} = \\sum_{j=0}^{m-1} \\Delta W^1_j \\Delta W^2_j$.\n\nFirst, we compute the expectation of $S_{\\Pi}$. Using the linearity of expectation:\n$$\n\\mathbb{E}[S_{\\Pi}] = \\mathbb{E}\\left[\\sum_{j=0}^{m-1} \\Delta W^1_j \\Delta W^2_j\\right] = \\sum_{j=0}^{m-1} \\mathbb{E}[\\Delta W^1_j \\Delta W^2_j]\n$$\nThe problem states that $W^1$ and $W^2$ are independent standard Brownian motions. This implies that for any set of time points, the collection of random variables derived from $W^1$ is independent of the collection derived from $W^2$. In particular, for any $j$, the increment $\\Delta W^1_j$ is independent of the increment $\\Delta W^2_j$. Therefore, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[\\Delta W^1_j \\Delta W^2_j] = \\mathbb{E}[\\Delta W^1_j] \\mathbb{E}[\\Delta W^2_j]\n$$\nA fundamental property of a standard Brownian motion $W$ is that its increments $W_u - W_s$ are normally distributed with mean $0$ and variance $u-s$. Thus, $\\mathbb{E}[\\Delta W^1_j] = 0$ and $\\mathbb{E}[\\Delta W^2_j] = 0$ for all $j$. This leads to:\n$$\n\\mathbb{E}[\\Delta W^1_j \\Delta W^2_j] = 0 \\cdot 0 = 0\n$$\nSumming over $j$, we find that the expectation of $S_{\\Pi}$ is $0$:\n$$\n\\mathbb{E}[S_{\\Pi}] = \\sum_{j=0}^{m-1} 0 = 0\n$$\n\nNext, we compute the variance of $S_{\\Pi}$. Since $\\mathbb{E}[S_{\\Pi}] = 0$, the variance is given by $\\text{Var}(S_{\\Pi}) = \\mathbb{E}[S_{\\Pi}^2] - (\\mathbb{E}[S_{\\Pi}])^2 = \\mathbb{E}[S_{\\Pi}^2]$.\n$$\nS_{\\Pi}^2 = \\left(\\sum_{j=0}^{m-1} \\Delta W^1_j \\Delta W^2_j\\right)^2 = \\sum_{j=0}^{m-1} (\\Delta W^1_j \\Delta W^2_j)^2 + \\sum_{i=0, i \\neq j}^{m-1} (\\Delta W^1_i \\Delta W^2_i)(\\Delta W^1_j \\Delta W^2_j)\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[S_{\\Pi}^2] = \\sum_{j=0}^{m-1} \\mathbb{E}[(\\Delta W^1_j)^2 (\\Delta W^2_j)^2] + \\sum_{i \\neq j} \\mathbb{E}[\\Delta W^1_i \\Delta W^2_i \\Delta W^1_j \\Delta W^2_j]\n$$\nFor the diagonal terms, due to the independence of $\\Delta W^1_j$ and $\\Delta W^2_j$, we have:\n$$\n\\mathbb{E}[(\\Delta W^1_j)^2 (\\Delta W^2_j)^2] = \\mathbb{E}[(\\Delta W^1_j)^2] \\mathbb{E}[(\\Delta W^2_j)^2]\n$$\nThe expected squared increment is the variance, so $\\mathbb{E}[(\\Delta W^k_j)^2] = \\text{Var}(\\Delta W^k_j) = t_{j+1}-t_j$ for $k=1,2$.\n$$\n\\mathbb{E}[(\\Delta W^1_j)^2 (\\Delta W^2_j)^2] = (t_{j+1}-t_j) \\cdot (t_{j+1}-t_j) = (t_{j+1}-t_j)^2\n$$\nFor the off-diagonal terms where $i \\neq j$, the time intervals $[t_i, t_{i+1}]$ and $[t_j, t_{j+1}]$ are disjoint. The increments of a single Brownian motion over disjoint intervals are independent. Also, the two Brownian motions $W^1$ and $W^2$ are independent. Thus, the four random variables $\\Delta W^1_i$, $\\Delta W^2_i$, $\\Delta W^1_j$, and $\\Delta W^2_j$ are mutually independent. The expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[\\Delta W^1_i \\Delta W^2_i \\Delta W^1_j \\Delta W^2_j] = \\mathbb{E}[\\Delta W^1_i] \\mathbb{E}[\\Delta W^2_i] \\mathbb{E}[\\Delta W^1_j] \\mathbb{E}[\\Delta W^2_j] = 0 \\cdot 0 \\cdot 0 \\cdot 0 = 0\n$$\nTherefore, all off-diagonal terms in the expectation are zero. The variance of $S_{\\Pi}$ is:\n$$\n\\text{Var}(S_{\\Pi}) = \\sum_{j=0}^{m-1} (t_{j+1}-t_j)^2\n$$\nLet $\\|\\Pi\\| = \\max_{j} (t_{j+1}-t_j)$ be the mesh of the partition. We can bound this sum:\n$$\n\\sum_{j=0}^{m-1} (t_{j+1}-t_j)^2 \\leq \\sum_{j=0}^{m-1} \\|\\Pi\\|(t_{j+1}-t_j) = \\|\\Pi\\| \\sum_{j=0}^{m-1} (t_{j+1}-t_j)\n$$\nThe latter sum is a telescoping sum: $\\sum_{j=0}^{m-1} (t_{j+1}-t_j) = t_m - t_0 = t-0 = t$.\nSo, we have the inequality $0 \\le \\text{Var}(S_{\\Pi}) \\le \\|\\Pi\\|t$.\nAs we take the limit of refining partitions where $\\|\\Pi\\| \\to 0$, we get:\n$$\n\\lim_{\\|\\Pi\\| \\to 0} \\text{Var}(S_{\\Pi}) = 0\n$$\nSince $\\mathbb{E}[S_{\\Pi}]=0$ and $\\text{Var}(S_{\\Pi}) \\to 0$, $S_{\\Pi}$ converges to $0$ in $L^2$. This implies that $S_{\\Pi}$ converges to $0$ in probability.\nBy definition, this limit is the quadratic covariation:\n$$\n\\langle W^1, W^2 \\rangle_t = [W^1, W^2]_t = \\lim_{\\|\\Pi\\| \\to 0} S_{\\Pi} = 0\n$$\nThis result holds for any $t \\ge 0$. The quadratic covariation process of two independent Brownian motions is the zero process.", "answer": "$$\n\\boxed{0}\n$$", "id": "3061836"}, {"introduction": "We now apply our theoretical tools to one of the most important stochastic processes in science and finance: the mean-reverting Ornstein-Uhlenbeck process. This problem [@problem_id:3061787] offers a chance to engage with a stochastic differential equation in two ways: first by finding its exact analytical solution, and second by applying the widely used Euler-Maruyama numerical scheme. Comparing the statistical properties of the exact and approximated solutions provides crucial insights into the relationship between continuous-time models and their discrete-time simulations.", "problem": "Consider the Ornstein–Uhlenbeck process defined by the linear Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t}=\\theta\\left(\\mu - X_{t}\\right)\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}, \\quad X_{0}=x_{0},\n$$\nwhere $\\theta0$, $\\mu\\in\\mathbb{R}$, $\\sigma0$, and $\\{W_{t}\\}_{t\\geq 0}$ is a standard Wiener process (Brownian motion). Let $h0$ be a given time step.\n\nTasks:\n- Using the Euler–Maruyama discretization, compute the one-step approximation $X_{h}^{\\mathrm{EM}}$ from $X_{0}=x_{0}$ and determine its conditional mean and variance given $X_{0}=x_{0}$.\n- Using fundamental properties of linear SDEs and Itô isometry, derive the exact conditional mean and variance of $X_{h}$ given $X_{0}=x_{0}$.\n- Define the discrepancies\n$$\n\\Delta_{\\mathrm{mean}} \\equiv \\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right]-\\mathbb{E}\\!\\left[X_{h}\\mid X_{0}=x_{0}\\right], \n\\qquad\n\\Delta_{\\mathrm{var}} \\equiv \\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right)-\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right).\n$$\nProvide a single, closed-form analytic expression for the pair $\\left(\\Delta_{\\mathrm{mean}},\\Delta_{\\mathrm{var}}\\right)$ in terms of $\\theta$, $\\mu$, $\\sigma$, $h$, and $x_{0}$.\n\nAnswer format:\n- Your final answer must be a single row matrix using the $\\mathrm{pmatrix}$ environment that contains $\\Delta_{\\mathrm{mean}}$ and $\\Delta_{\\mathrm{var}}$ in closed form.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- The process is the Ornstein–Uhlenbeck process, defined by the Stochastic Differential Equation (SDE):\n$$\n\\mathrm{d}X_{t}=\\theta\\left(\\mu - X_{t}\\right)\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}\n$$\n- Initial condition: $X_{0}=x_{0}$\n- Parameters: $\\theta0$, $\\mu\\in\\mathbb{R}$, $\\sigma0$\n- $\\{W_{t}\\}_{t\\geq 0}$ is a standard Wiener process.\n- Time step: $h0$\n- Task 1: Compute the one-step Euler–Maruyama approximation $X_{h}^{\\mathrm{EM}}$ from $X_{0}=x_{0}$ and find its conditional mean $\\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right]$ and variance $\\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right)$.\n- Task 2: Derive the exact conditional mean $\\mathbb{E}\\!\\left[X_{h}\\mid X_{0}=x_{0}\\right]$ and variance $\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right)$.\n- Task 3: Define and compute the discrepancies:\n$$\n\\Delta_{\\mathrm{mean}} \\equiv \\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right]-\\mathbb{E}\\!\\left[X_{h}\\mid X_{0}=x_{0}\\right]\n$$\n$$\n\\Delta_{\\mathrm{var}} \\equiv \\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right)-\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right)\n$$\n- The final answer is the pair $(\\Delta_{\\mathrm{mean}}, \\Delta_{\\mathrm{var}})$ in a single row matrix.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The Ornstein–Uhlenbeck process, Euler–Maruyama discretization, and Itô calculus are standard, well-established topics in the theory of stochastic differential equations. The SDE is linear and its properties are well-known.\n- **Well-Posedness**: The problem asks for the calculation of well-defined statistical moments (mean and variance) for both the exact solution and a numerical approximation. All parameters are clearly defined, and sufficient information is provided to derive a unique analytical solution for the requested quantities.\n- **Objectivity**: The problem is stated in precise mathematical language, free from ambiguity or subjective interpretation.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a standard, formalizable exercise in stochastic calculus and numerical methods for SDEs.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution Derivation\n\n**Part 1: Euler–Maruyama Approximation**\n\nThe Euler–Maruyama discretization scheme for a general SDE $\\mathrm{d}X_t = a(t, X_t)\\mathrm{d}t + b(t, X_t)\\mathrm{d}W_t$ is given by\n$X_{t+h} \\approx X_t + a(t, X_t)h + b(t,X_t)(W_{t+h}-W_t)$.\nFor the given Ornstein–Uhlenbeck process, the drift and diffusion coefficients are $a(t, X_t) = \\theta(\\mu - X_t)$ and $b(t, X_t) = \\sigma$.\nA single step of length $h$ from the initial condition $X_{0}=x_{0}$ yields the approximation $X_{h}^{\\mathrm{EM}}$:\n$$\nX_{h}^{\\mathrm{EM}} = X_{0} + \\theta(\\mu - X_{0})h + \\sigma(W_{h} - W_{0})\n$$\nGiven $X_{0}=x_{0}$ and the property that $W_{0}=0$ for a standard Wiener process, this becomes:\n$$\nX_{h}^{\\mathrm{EM}} = x_{0} + \\theta(\\mu - x_{0})h + \\sigma W_{h}\n$$\nNow, we compute the conditional mean and variance of $X_{h}^{\\mathrm{EM}}$ given $X_{0}=x_{0}$. The Wiener process increment $W_{h}$ is a random variable with distribution $W_{h} \\sim \\mathcal{N}(0, h)$, so its mean is $\\mathbb{E}[W_{h}]=0$ and its variance is $\\mathrm{Var}(W_{h})=h$.\n\nThe conditional mean is:\n$$\n\\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right] = \\mathbb{E}\\!\\left[x_{0} + \\theta(\\mu - x_{0})h + \\sigma W_{h}\\right] = x_{0} + \\theta(\\mu - x_{0})h + \\sigma \\mathbb{E}[W_{h}]\n$$\n$$\n\\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right] = x_{0} + \\theta(\\mu - x_{0})h\n$$\nThe conditional variance is:\n$$\n\\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right) = \\mathrm{Var}\\!\\left(x_{0} + \\theta(\\mu - x_{0})h + \\sigma W_{h}\\right)\n$$\nSince $x_{0}$ is fixed by the condition, the first two terms are constants, so they do not contribute to the variance.\n$$\n\\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right) = \\mathrm{Var}\\!\\left(\\sigma W_{h}\\right) = \\sigma^{2}\\mathrm{Var}\\!\\left(W_{h}\\right) = \\sigma^{2}h\n$$\n\n**Part 2: Exact Solution**\n\nTo find the exact solution for $X_{h}$, we solve the linear SDE. The SDE can be written as:\n$$\n\\mathrm{d}X_{t} + \\theta X_{t}\\mathrm{d}t = \\theta\\mu\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nWe use an integrating factor $I_t = \\exp(\\theta t)$. Multiplying the SDE by $I_t$:\n$$\n\\exp(\\theta t)\\mathrm{d}X_{t} + \\theta\\exp(\\theta t) X_{t}\\mathrm{d}t = \\theta\\mu\\exp(\\theta t)\\,\\mathrm{d}t + \\sigma\\exp(\\theta t)\\,\\mathrm{d}W_{t}\n$$\nBy Itô's product rule, the left-hand side is the differential of $\\exp(\\theta t)X_{t}$:\n$$\n\\mathrm{d}(\\exp(\\theta t)X_{t}) = \\theta\\mu\\exp(\\theta t)\\,\\mathrm{d}t + \\sigma\\exp(\\theta t)\\,\\mathrm{d}W_{t}\n$$\nIntegrating from $t=0$ to $t=h$:\n$$\n\\int_{0}^{h}\\mathrm{d}(\\exp(\\theta s)X_{s}) = \\int_{0}^{h}\\theta\\mu\\exp(\\theta s)\\,\\mathrm{d}s + \\int_{0}^{h}\\sigma\\exp(\\theta s)\\,\\mathrm{d}W_{s}\n$$\n$$\n\\exp(\\theta h)X_{h} - X_{0} = \\theta\\mu\\left[\\frac{\\exp(\\theta s)}{\\theta}\\right]_{0}^{h} + \\sigma\\int_{0}^{h}\\exp(\\theta s)\\,\\mathrm{d}W_{s}\n$$\n$$\n\\exp(\\theta h)X_{h} - X_{0} = \\mu(\\exp(\\theta h) - 1) + \\sigma\\int_{0}^{h}\\exp(\\theta s)\\,\\mathrm{d}W_{s}\n$$\nSolving for $X_{h}$:\n$$\nX_{h} = X_{0}\\exp(-\\theta h) + \\mu(1 - \\exp(-\\theta h)) + \\sigma\\exp(-\\theta h)\\int_{0}^{h}\\exp(\\theta s)\\,\\mathrm{d}W_{s}\n$$\nThe stochastic integral can be rewritten as $\\sigma\\int_{0}^{h}\\exp(-\\theta(h-s))\\,\\mathrm{d}W_{s}$. Given $X_0 = x_0$, the solution is:\n$$\nX_{h} = x_{0}\\exp(-\\theta h) + \\mu(1 - \\exp(-\\theta h)) + \\sigma\\int_{0}^{h}\\exp(-\\theta(h-s))\\,\\mathrm{d}W_{s}\n$$\nThe conditional mean is found by taking the expectation. The Itô integral has zero mean.\n$$\n\\mathbb{E}\\!\\left[X_{h}\\mid X_{0}=x_{0}\\right] = x_{0}\\exp(-\\theta h) + \\mu(1 - \\exp(-\\theta h))\n$$\nThe conditional variance is found using the Itô isometry property, which states that for a deterministic function $f(t)$, $\\mathrm{Var}(\\int_{0}^{T} f(t)\\,\\mathrm{d}W_t) = \\int_{0}^{T} f(t)^2\\,\\mathrm{d}t$. The non-stochastic terms vanish when taking the variance.\n$$\n\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right) = \\mathrm{Var}\\!\\left(\\sigma\\int_{0}^{h}\\exp(-\\theta(h-s))\\,\\mathrm{d}W_{s}\\right) = \\sigma^{2}\\int_{0}^{h}\\left(\\exp(-\\theta(h-s))\\right)^{2}\\mathrm{d}s\n$$\n$$\n\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right) = \\sigma^{2}\\int_{0}^{h}\\exp(-2\\theta(h-s))\\,\\mathrm{d}s\n$$\nEvaluating the integral:\n$$\n\\int_{0}^{h}\\exp(-2\\theta h)\\exp(2\\theta s)\\,\\mathrm{d}s = \\exp(-2\\theta h)\\left[\\frac{\\exp(2\\theta s)}{2\\theta}\\right]_{0}^{h} = \\exp(-2\\theta h)\\left(\\frac{\\exp(2\\theta h) - 1}{2\\theta}\\right) = \\frac{1 - \\exp(-2\\theta h)}{2\\theta}\n$$\nSo, the exact conditional variance is:\n$$\n\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right) = \\frac{\\sigma^{2}}{2\\theta}(1 - \\exp(-2\\theta h))\n$$\n\n**Part 3: Discrepancies**\n\nNow we compute the discrepancies $\\Delta_{\\mathrm{mean}}$ and $\\Delta_{\\mathrm{var}}$.\n\nFor the mean:\n$$\n\\Delta_{\\mathrm{mean}} = \\mathbb{E}\\!\\left[X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right]-\\mathbb{E}\\!\\left[X_{h}\\mid X_{0}=x_{0}\\right]\n$$\n$$\n\\Delta_{\\mathrm{mean}} = \\left(x_{0} + \\theta(\\mu - x_{0})h\\right) - \\left(x_{0}\\exp(-\\theta h) + \\mu(1 - \\exp(-\\theta h))\\right)\n$$\n$$\n\\Delta_{\\mathrm{mean}} = x_{0} + \\theta\\mu h - \\theta x_{0}h - x_{0}\\exp(-\\theta h) - \\mu + \\mu\\exp(-\\theta h)\n$$\nRe-grouping terms with respect to $\\mu$ and $x_0$:\n$$\n\\Delta_{\\mathrm{mean}} = \\mu(\\theta h - 1 + \\exp(-\\theta h)) + x_{0}(1 - \\theta h - \\exp(-\\theta h))\n$$\n$$\n\\Delta_{\\mathrm{mean}} = \\mu(\\theta h - 1 + \\exp(-\\theta h)) - x_{0}(\\theta h - 1 + \\exp(-\\theta h))\n$$\n$$\n\\Delta_{\\mathrm{mean}} = (\\mu - x_{0})(\\theta h - 1 + \\exp(-\\theta h))\n$$\n\nFor the variance:\n$$\n\\Delta_{\\mathrm{var}} = \\mathrm{Var}\\!\\left(X_{h}^{\\mathrm{EM}}\\mid X_{0}=x_{0}\\right)-\\mathrm{Var}\\!\\left(X_{h}\\mid X_{0}=x_{0}\\right)\n$$\n$$\n\\Delta_{\\mathrm{var}} = \\sigma^{2}h - \\frac{\\sigma^{2}}{2\\theta}(1 - \\exp(-2\\theta h))\n$$\n$$\n\\Delta_{\\mathrm{var}} = \\sigma^{2}\\left(h - \\frac{1 - \\exp(-2\\theta h)}{2\\theta}\\right)\n$$\n\nThe pair $(\\Delta_{\\mathrm{mean}}, \\Delta_{\\mathrm{var}})$ is thus determined.", "answer": "$$\n\\boxed{\\begin{pmatrix} (\\mu - x_{0})(\\exp(-\\theta h) + \\theta h - 1)  \\sigma^{2}\\left(h - \\frac{1 - \\exp(-2\\theta h)}{2\\theta}\\right) \\end{pmatrix}}\n$$", "id": "3061787"}]}