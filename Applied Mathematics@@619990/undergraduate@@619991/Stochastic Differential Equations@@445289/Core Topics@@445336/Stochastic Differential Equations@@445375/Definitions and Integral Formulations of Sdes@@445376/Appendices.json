{"hands_on_practices": [{"introduction": "The gateway to understanding stochastic differential equations (SDEs) is to recognize that they are fundamentally defined through their integral form. This practice moves beyond a symbolic representation like $dX_t = \\dots$ and challenges you to engage with the precise, rigorous definition of an Itô solution. By evaluating different verification procedures, you will solidify your understanding of the crucial conditions—such as adaptedness, integrability, and the nature of almost sure convergence—that underpin the entire theory [@problem_id:3048330].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions and carrying a standard $1$-dimensional Wiener process $W=(W_t)_{t\\in[0,T]}$. Consider the stochastic differential equation (in the Itô sense)\n$$\ndX_t = b(t,X_t)\\,dt + \\sigma(t,X_t)\\,dW_t,\\quad t\\in[0,T],\\qquad X_0=\\xi,\n$$\nwhere $b:[0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ and $\\sigma:[0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ are Borel measurable and $\\xi$ is $\\mathcal{F}_0$-measurable. You are given a candidate process $X=(X_t)_{t\\in[0,T]}$ taking values in $\\mathbb{R}$.\n\nYour task is to identify which verification procedures correctly establish that $X$ solves the stochastic differential equation above, using only the integral formulation. Select all options that are valid and sufficient procedures, grounded in the definition of an Itô solution and the well-posedness of the relevant integrals.\n\nA. Verify that $X$ is $(\\mathcal{F}_t)$-adapted with almost surely continuous sample paths, that $b(\\cdot,X_{\\cdot})$ is Lebesgue integrable on $[0,T]$ almost surely, and that $\\sigma(\\cdot,X_{\\cdot})$ is progressively measurable and square-integrable on $[0,T]$ almost surely, so that the integrals\n$$\n\\int_0^t b(s,X_s)\\,ds \\quad\\text{and}\\quad \\int_0^t \\sigma(s,X_s)\\,dW_s\n$$\nare well-defined for all $t\\in[0,T]$, and moreover there exists a null set $N\\in\\mathcal{F}$, $\\mathbb{P}(N)=0$, such that for all $\\omega\\in N^{c}$ and all $t\\in[0,T]$,\n$$\nX_t(\\omega) = X_0(\\omega) + \\int_0^t b(s,X_s(\\omega))\\,ds + \\int_0^t \\sigma(s,X_s(\\omega))\\,dW_s(\\omega).\n$$\n\nB. For each fixed $t\\in[0,T]$, verify that there exists a null set $N_t$ (possibly depending on $t$) such that\n$$\nX_t = X_0 + \\int_0^t b(s,X_s)\\,ds + \\int_0^t \\sigma(s,X_s)\\,dW_s\n$$\nholds almost surely (i.e., off $N_t$). It is not necessary to ensure that the exceptional set is the same for different times $t$, nor to verify any measurability or integrability properties of the integrands.\n\nC. Replace the stochastic integral $\\int_0^t \\sigma(s,X_s)\\,dW_s$ by a pathwise Riemann–Stieltjes integral $\\int_0^t \\sigma(s,X_s)\\,dW_s$ defined as a limit of Riemann sums along refining deterministic partitions, and check the integral equation path by path for all $\\omega\\in\\Omega$.\n\nD. Verify the integral equation on the countable dense subset $\\mathbb{Q}\\cap[0,T]$ as follows: show that there exists a null set $N\\in\\mathcal{F}$, $\\mathbb{P}(N)=0$, such that for all $\\omega\\in N^{c}$ and all $q\\in\\mathbb{Q}\\cap[0,T]$,\n$$\nX_q(\\omega) = X_0(\\omega) + \\int_0^q b(s,X_s(\\omega))\\,ds + \\int_0^q \\sigma(s,X_s(\\omega))\\,dW_s(\\omega),\n$$\nand additionally verify that $X$ has almost surely continuous sample paths and that the two integrals define almost surely continuous processes of $t$. Conclude from continuity that the equality holds for all $t\\in[0,T]$ for $\\omega\\in N^{c}$.\n\nE. It is sufficient to verify that for each $t\\in[0,T]$,\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[X_0] + \\int_0^t \\mathbb{E}[b(s,X_s)]\\,ds,\n$$\nand\n$$\n\\operatorname{Var}(X_t) = \\int_0^t \\mathbb{E}[\\sigma(s,X_s)^2]\\,ds,\n$$\nsince these conditions imply that the integral equation holds almost surely.", "solution": "The problem asks for valid and sufficient procedures to verify if a given process $X = (X_t)_{t\\in[0,T]}$ is a solution to the Itô stochastic differential equation (SDE)\n$$\ndX_t = b(t,X_t)\\,dt + \\sigma(t,X_t)\\,dW_t, \\quad t\\in[0,T], \\quad X_0=\\xi.\n$$\nThe starting point for this analysis is the formal definition of a strong solution to this SDE, which is given in its integral form.\n\nA process $X = (X_t)_{t\\in[0,T]}$ is a strong solution to the SDE if it satisfies the following three conditions:\n1.  $X$ is $(\\mathcal{F}_t)$-adapted and its sample paths are continuous almost surely.\n2.  The coefficient processes satisfy the integrability conditions:\n    $$\n    \\mathbb{P}\\left(\\int_0^T |b(s,X_s)|\\,ds < \\infty\\right) = 1\n    $$\n    and\n    $$\n    \\mathbb{P}\\left(\\int_0^T \\sigma(s,X_s)^2\\,ds < \\infty\\right) = 1.\n    $$\n    These conditions ensure that the Lebesgue integral and the Itô integral, respectively, are well-defined.\n3.  The integral equation\n    $$\n    X_t = X_0 + \\int_0^t b(s,X_s)\\,ds + \\int_0^t \\sigma(s,X_s)\\,dW_s\n    $$\n    holds almost surely for all $t\\in[0,T]$. This means there exists a single null set $N \\subset \\Omega$ such that for every $\\omega \\in N^c$, the equality holds for all $t \\in [0,T]$.\n\nWith this definition as our benchmark, we evaluate each option.\n\n**A. Verify that $X$ is $(\\mathcal{F}_t)$-adapted with almost surely continuous sample paths, that $b(\\cdot,X_{\\cdot})$ is Lebesgue integrable on $[0,T]$ almost surely, and that $\\sigma(\\cdot,X_{\\cdot})$ is progressively measurable and square-integrable on $[0,T]$ almost surely, so that the integrals are well-defined for all $t\\in[0,T]$, and moreover there exists a null set $N\\in\\mathcal{F}$, $\\mathbb{P}(N)=0$, such that for all $\\omega\\in N^{c}$ and all $t\\in[0,T]$, the integral equation holds.**\n\nThis option is a meticulous restatement of the formal definition of a strong solution.\n- It correctly requires $X$ to be adapted and to have a.s. continuous paths.\n- It correctly requires the integrability conditions on the coefficients $b$ and $\\sigma$ that ensure the integrals are well-defined. The requirement that $\\sigma(\\cdot, X_\\cdot)$ be progressively measurable is a standard sufficient condition for the Itô integral to be well-defined.\n- Crucially, it captures the strong requirement that the integral equation must hold for all $t \\in [0,T]$ simultaneously, outside of a single null set $N$. This is the meaning of \"almost surely as processes\".\n\nThis procedure is both valid and sufficient.\nVerdict: **Correct**.\n\n**B. For each fixed $t\\in[0,T]$, verify that there exists a null set $N_t$ (possibly depending on $t$) such that the integral equation holds almost surely (i.e., off $N_t$). It is not necessary to ensure that the exceptional set is the same for different times $t$, nor to verify any measurability or integrability properties of the integrands.**\n\nThis procedure is flawed for two fundamental reasons.\n1.  It fails to verify the well-posedness of the integrals. Without checking the integrability conditions for $b(\\cdot, X_\\cdot)$ and $\\sigma(\\cdot, X_\\cdot)$, the expressions $\\int_0^t b(s,X_s)\\,ds$ and $\\int_0^t \\sigma(s,X_s)\\,dW_s$ may not be defined. A verification procedure cannot omit the prerequisites for its own terms.\n2.  The condition that the equality holds on $\\Omega \\setminus N_t$ for each $t$ is significantly weaker than the definition requires. The union of uncountably many null sets, $\\cup_{t \\in [0,T]} N_t$, is not in general a null set. Therefore, this condition does not guarantee that there exists even a single sample path $\\omega$ for which the equality $X_t(\\omega) = \\dots$ holds for all $t \\in [0,T]$. This weaker property is sometimes called a \"wide-sense\" solution, but it is not what is meant by a solution to an SDE in the standard Itô sense.\n\nVerdict: **Incorrect**.\n\n**C. Replace the stochastic integral $\\int_0^t \\sigma(s,X_s)\\,dW_s$ by a pathwise Riemann–Stieltjes integral $\\int_0^t \\sigma(s,X_s)\\,dW_s$ defined as a limit of Riemann sums along refining deterministic partitions, and check the integral equation path by path for all $\\omega\\in\\Omega$.**\n\nThis procedure fundamentally misunderstands the nature of the Itô integral. The Itô integral is a stochastic integral, not a pathwise Stieltjes-type integral. A key property of the Wiener process $W_t$ is that its sample paths are almost surely of unbounded variation on any interval. Pathwise Riemann–Stieltjes integrals with respect to functions of unbounded variation are generally not well-defined. The theory of stochastic integration (Itô calculus) was specifically developed to define integration with respect to processes like the Wiener process. Attempting to use a Riemann-Stieltjes framework is a categorical error.\n\nVerdict: **Incorrect**.\n\n**D. Verify the integral equation on the countable dense subset $\\mathbb{Q}\\cap[0,T]$ as follows: show that there exists a null set $N\\in\\mathcal{F}$, $\\mathbb{P}(N)=0$, such that for all $\\omega\\in N^{c}$ and all $q\\in\\mathbb{Q}\\cap[0,T]$, the integral equation holds, and additionally verify that $X$ has almost surely continuous sample paths and that the two integrals define almost surely continuous processes of $t$. Conclude from continuity that the equality holds for all $t\\in[0,T]$ for $\\omega\\in N^{c}$.**\n\nThis describes a valid and common technique for verifying the process equality. The argument relies on a standard result from real analysis: if two continuous functions on a closed interval agree on a dense subset of that interval, they must be identical over the entire interval.\nThe procedure requires verifying:\n1.  Both the left-hand side, $L_t(\\omega) = X_t(\\omega)$, and the right-hand side, $R_t(\\omega) = X_0(\\omega) + \\int_0^t b(s,X_s(\\omega))\\,ds + \\int_0^t \\sigma(s,X_s(\\omega))\\,dW_s(\\omega)$, are continuous functions of $t$ for almost all $\\omega$. The continuity of $X_t$ is a direct assumption. The continuity of $R_t$ follows from the fact that the Lebesgue integral of an $L^1$ function is continuous in its upper limit, and a fundamental property of the Itô integral is that it is a process with continuous sample paths (this holds if the integrand is square-integrable a.s., which must be verified for the integral to be well-defined).\n2.  The equality $L_t(\\omega) = R_t(\\omega)$ holds for all $t \\in \\mathbb{Q}\\cap[0,T]$ for almost all $\\omega$ (i.e., on a set of probability $1$).\n\nSince $\\mathbb{Q}\\cap[0,T]$ is dense in $[0,T]$, for any $\\omega$ where both processes are continuous and agree on this dense set, they must agree for all $t \\in [0,T]$. Since the premises hold almost surely, the conclusion (equality for all $t$) holds almost surely. This establishes the existence of a single null set outside of which the equality holds for all $t$, which matches the requirement of the formal definition.\n\nVerdict: **Correct**.\n\n**E. It is sufficient to verify that for each $t\\in[0,T]$, $\\mathbb{E}[X_t] = \\mathbb{E}[X_0] + \\int_0^t \\mathbb{E}[b(s,X_s)]\\,ds$, and $\\operatorname{Var}(X_t) = \\int_0^t \\mathbb{E}[\\sigma(s,X_s)^2]\\,ds$, since these conditions imply that the integral equation holds almost surely.**\n\nThis is incorrect. The SDE defines a relationship between sample paths, which is a much stronger condition than matching moments.\n- The first equation for the mean is a *consequence* of the SDE holding (under conditions that allow interchange of expectation and integration and assuming the Itô integral has zero mean), but it is not sufficient. Many different stochastic processes can have the same mean function.\n- The second equation for the variance is not even generally correct. For a general Itô process $X_t$, the evolution of its variance is more complex. Even if it were correct, it only concerns a second-order moment.\n- The fundamental error is that equality of moments (even all moments) does not, in general, imply almost sure pathwise equality of the processes. The SDE solution is a pathwise statement, and this procedure only checks properties of the marginal distributions of $X_t$ at each time $t$.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AD}$$", "id": "3048330"}, {"introduction": "Once we have a rigorous definition of a solution, a natural question arises: how can we prove a solution exists and construct it? The Picard iteration method, adapted from the theory of ordinary differential equations, provides a powerful answer by building a sequence of processes that converges to the solution. This exercise explores a special case with constant coefficients, where the iteration astonishingly converges in a single step, providing a crystal-clear insight into how the integral formulation serves as the engine of this constructive proof [@problem_id:3069748].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions, and let $(W_{t})_{t \\geq 0}$ denote a standard Wiener process (Brownian motion) adapted to $(\\mathcal{F}_{t})_{t \\geq 0}$. Fix constants $\\alpha \\in \\mathbb{R}$, $\\beta \\in \\mathbb{R}$, and an initial condition $x_{0} \\in \\mathbb{R}$. Consider the stochastic differential equation (SDE)\n$$\ndX_{t}=\\alpha\\,dt+\\beta\\,dW_{t},\\quad X_{0}=x_{0},\\quad t \\in [0,T],\n$$\nwith the requirement that $(X_{t})_{t \\in [0,T]}$ be $(\\mathcal{F}_{t})$-adapted and have square-integrable paths. Starting from the fundamental integral formulation of SDEs and the definition of the Itô integral, construct the Picard iteration associated with this SDE by defining an iteration map acting on adapted processes with square-integrable paths, and explain, using only the structural properties of the drift and diffusion coefficients, why the iteration converges in a single step from an arbitrary initial guess. As your final output, provide the explicit closed-form expression for the solution process $X_{t}$ on $[0,T]$. The final answer must be a single analytic expression in $t$, $\\alpha$, $\\beta$, $x_{0}$, and $W_{t}$.", "solution": "The problem statement is a valid exercise in stochastic calculus. It is well-posed, scientifically grounded, and provides all necessary information to construct a rigorous solution. We may therefore proceed with the derivation.\n\nThe stochastic differential equation (SDE) under consideration is:\n$$\ndX_{t} = \\alpha\\,dt + \\beta\\,dW_{t}, \\quad X_{0}=x_{0}\n$$\nfor $t \\in [0,T]$, where $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are constants, $x_{0} \\in \\mathbb{R}$ is the initial condition, and $(W_{t})_{t \\geq 0}$ is a standard Wiener process on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$.\n\nThe fundamental integral formulation of this SDE expresses the process $(X_{t})_{t \\in [0,T]}$ as the solution to the integral equation:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThis equation is an instance of the general form for an Itô process:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} a(s, X_{s}) \\, ds + \\int_{0}^{t} b(s, X_{s}) \\, dW_{s}\n$$\nIn our specific case, the drift coefficient is $a(t,x) = \\alpha$ and the diffusion coefficient is $b(t,x) = \\beta$.\n\nThe Picard iteration method provides a constructive proof of the existence and uniqueness of solutions to SDEs. It defines a sequence of stochastic processes $(X_{t}^{(n)})_{t \\in [0,T]}$ that converges to the true solution $(X_{t})_{t \\in [0,T]}$. The iteration is defined by an operator or map, $\\Phi$, which acts on the space of $(\\mathcal{F}_{t})$-adapted processes with square-integrable paths on $[0,T]$. For a process $Y = (Y_{t})_{t \\in [0,T]}$, the map is defined as:\n$$\n(\\Phi(Y))_{t} = x_{0} + \\int_{0}^{t} a(s, Y_{s}) \\, ds + \\int_{0}^{t} b(s, Y_{s}) \\, dW_{s}\n$$\nThe Picard iterates are then generated by the relation $X^{(n+1)} = \\Phi(X^{(n)})$ for $n \\geq 0$.\n\nFor the given SDE, the drift and diffusion coefficients are $a(t,x) = \\alpha$ and $b(t,x) = \\beta$, respectively. Notice that these coefficients are constant and, crucially, do not depend on the state variable $x$. This property is central to the behavior of the iteration. The iteration map for this specific problem is:\n$$\n(\\Phi(Y))_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nObserve that the right-hand side of this definition is independent of the input process $Y$. Therefore, $\\Phi$ is a constant map: it maps any adapted process $Y$ (with square-integrable paths) to the same, single, fixed process.\n\nLet us construct the Picard sequence. We begin with an arbitrary initial guess for the solution, denoted by $(X_{t}^{(0)})_{t \\in [0,T]}$, which must be an $(\\mathcal{F}_{t})$-adapted process with square-integrable paths. A common but not mandatory choice is $X_{t}^{(0)} = x_{0}$ for all $t$.\n\nThe first iterate, $(X_{t}^{(1)})_{t \\in [0,T]}$, is obtained by applying the map $\\Phi$ to the initial guess $X^{(0)}$:\n$$\nX_{t}^{(1)} = (\\Phi(X^{(0)}))_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThe integrals can be evaluated directly. The first integral is a standard Riemann integral of a constant, and the second is an Itô integral of a constant process.\n$$\n\\int_{0}^{t} \\alpha \\, ds = \\alpha t\n$$\n$$\n\\int_{0}^{t} \\beta \\, dW_{s} = \\beta \\int_{0}^{t} dW_{s} = \\beta (W_{t} - W_{0})\n$$\nBy convention, a standard Wiener process starts at zero, so $W_{0}=0$. Thus, the first iterate is:\n$$\nX_{t}^{(1)} = x_{0} + \\alpha t + \\beta W_{t}\n$$\nThis expression holds for any $t \\in [0,T]$.\n\nNow, we compute the second iterate, $(X_{t}^{(2)})_{t \\in [0,T]}$, by applying the map $\\Phi$ to the first iterate $X^{(1)}$:\n$$\nX_{t}^{(2)} = (\\Phi(X^{(1)}))_{t} = x_{0} + \\int_{0}^{t} a(s, X_{s}^{(1)}) \\, ds + \\int_{0}^{t} b(s, X_{s}^{(1)}) \\, dW_{s}\n$$\nSubstituting the specific forms of the coefficients $a(s,x) = \\alpha$ and $b(s,x) = \\beta$:\n$$\nX_{t}^{(2)} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThis is precisely the same expression we obtained for $X_{t}^{(1)}$. Therefore,\n$$\nX_{t}^{(2)} = x_{0} + \\alpha t + \\beta W_{t} = X_{t}^{(1)}\n$$\nSince $X^{(2)} = X^{(1)}$, the sequence of iterates becomes stationary. For any $n \\geq 1$, we have $X^{(n+1)} = \\Phi(X^{(n)}) = \\Phi(X^{(1)}) = X^{(2)} = X^{(1)}$. The sequence of processes is $(X^{(0)}, X^{(1)}, X^{(1)}, X^{(1)}, \\dots)$. This sequence converges immediately to the process $X^{(1)}$.\n\nThe convergence in a single step is a direct consequence of the fact that the drift and diffusion coefficients, $\\alpha$ and $\\beta$, are state-independent. This makes the Picard iteration map $\\Phi$ a constant map, meaning its output does not depend on its input. The first application of the map yields the fixed point, and all subsequent applications simply return the same fixed point. For a general SDE where coefficients depend on $X_t$, $\\Phi$ is not a constant map, and one must use its contractive property on a suitable function space to prove convergence, which does not occur in a single step.\n\nThe unique solution to the SDE is the fixed point of the iteration map, which we have found to be $X^{(1)}$. Therefore, the explicit closed-form solution for the process $(X_{t})_{t \\in [0,T]}$ is:\n$$\nX_{t} = x_{0} + \\alpha t + \\beta W_{t}\n$$\nThis process is known as an arithmetic Brownian motion or a drifted Wiener process.", "answer": "$$\\boxed{x_{0} + \\alpha t + \\beta W_{t}}$$", "id": "3069748"}, {"introduction": "With the foundational concepts established, we can now tackle the task of finding explicit solutions to important SDEs. This practice focuses on the homogeneous linear SDE, better known as Geometric Brownian Motion, which is a cornerstone model in mathematical finance and population dynamics. You will use the single most important tool in applied stochastic calculus, Itô's formula, to transform the SDE and derive its solution, ultimately revealing its connection to the log-normal distribution and calculating its key statistical properties [@problem_id:3048360].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\in [0,T]},\\mathbb{P})$ be a filtered probability space supporting a standard one-dimensional Brownian motion $W$. Let $a:[0,T]\\to\\mathbb{R}$ and $b:[0,T]\\to\\mathbb{R}$ be deterministic Borel functions satisfying $\\int_{0}^{T}\\big(|a(s)|+b(s)^{2}\\big)\\,ds<\\infty$. Consider the homogeneous linear Stochastic Differential Equation (SDE)\n$$\ndX_t \\;=\\; a(t)\\,X_t\\,dt \\;+\\; b(t)\\,X_t\\,dW_t,\\qquad X_0 = x_0 > 0,\\quad t\\in[0,T].\n$$\nStarting from the integral formulation of the SDE and using only core definitions and properties of the Itô integral and Itô’s formula, derive the distribution of $X_t$ for a fixed $t\\in(0,T]$, show that $X_t$ is log-normal, and compute $\\mathbb{E}[X_t]$ and $\\operatorname{Var}(X_t)$ explicitly in terms of $a$ and $b$. Express your final answer as a $1\\times 2$ row vector whose first entry is $\\mathbb{E}[X_t]$ and whose second entry is $\\operatorname{Var}(X_t)$. No rounding is required.", "solution": "The problem statement is a well-posed exercise in stochastic differential equations. It provides all necessary information: a valid SDE, an initial condition, and standard assumptions on the coefficients and the underlying Brownian motion. The problem is scientifically grounded, objective, and contains no contradictions or ambiguities. Therefore, it is valid, and we may proceed to the solution.\n\nThe stochastic differential equation (SDE) is given by\n$$\ndX_t = a(t)X_t dt + b(t)X_t dW_t\n$$\nwith initial condition $X_0 = x_0 > 0$, where $t \\in [0, T]$. The functions $a(t)$ and $b(t)$ are deterministic. The integral form of this SDE is\n$$\nX_t = X_0 + \\int_0^t a(s)X_s ds + \\int_0^t b(s)X_s dW_s.\n$$\nTo solve this SDE, we introduce a new process $Y_t$ by the transformation $Y_t = f(X_t) = \\ln(X_t)$. This transformation is well-defined because the solution $X_t$ to this SDE with a positive initial condition $x_0 > 0$ remains positive for all $t \\ge 0$ almost surely. We apply Itô's formula to $Y_t$. For a twice-differentiable function $f(x)$, Itô's lemma for the process $X_t$ states\n$$\nd f(X_t) = f'(X_t) dX_t + \\frac{1}{2} f''(X_t) (dX_t)^2.\n$$\nIn our case, $f(x) = \\ln(x)$, so we have the derivatives $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. The quadratic variation term $(dX_t)^2$ is computed using the rules of Itô calculus, $(dt)^2=0$, $dt dW_t = 0$, and $(dW_t)^2=dt$:\n$$\n(dX_t)^2 = (a(t)X_t dt + b(t)X_t dW_t)^2 = b(t)^2 X_t^2 (dW_t)^2 = b(t)^2 X_t^2 dt.\n$$\nSubstituting these into Itô's formula for $Y_t = \\ln(X_t)$:\n$$\ndY_t = \\frac{1}{X_t} dX_t + \\frac{1}{2} \\left( -\\frac{1}{X_t^2} \\right) (dX_t)^2\n$$\n$$\ndY_t = \\frac{1}{X_t} (a(t)X_t dt + b(t)X_t dW_t) - \\frac{1}{2X_t^2} (b(t)^2 X_t^2 dt)\n$$\n$$\ndY_t = (a(t) dt + b(t) dW_t) - \\frac{1}{2} b(t)^2 dt\n$$\n$$\ndY_t = \\left( a(t) - \\frac{1}{2} b(t)^2 \\right) dt + b(t) dW_t.\n$$\nThis is a simple SDE for $Y_t$ with an additive noise term. We can solve for $Y_t$ by direct integration from $0$ to $t$:\n$$\nY_t - Y_0 = \\int_0^t \\left( a(s) - \\frac{1}{2} b(s)^2 \\right) ds + \\int_0^t b(s) dW_s.\n$$\nThe initial condition is $Y_0 = \\ln(X_0) = \\ln(x_0)$. Therefore,\n$$\nY_t = \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2} b(s)^2 \\right) ds + \\int_0^t b(s) dW_s.\n$$\nNow we can determine the distribution of $Y_t$. The expression for $Y_t$ consists of two parts: a deterministic part and a stochastic integral.\nThe first two terms are deterministic for a fixed $t$:\n$$\n\\mu_Y(t) = \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2} b(s)^2 \\right) ds.\n$$\nThe third term is an Itô integral with a deterministic integrand $b(s)$. A fundamental property of the Itô integral is that for a deterministic, square-integrable function $g(s)$, the random variable $Z_t = \\int_0^t g(s) dW_s$ is normally distributed with mean $0$ and variance $\\int_0^t g(s)^2 ds$. In our case, $g(s) = b(s)$, so the stochastic integral $\\int_0^t b(s) dW_s$ is a normal random variable with mean $\\mathbb{E}[\\int_0^t b(s) dW_s] = 0$ and variance $\\operatorname{Var}(\\int_0^t b(s) dW_s) = \\int_0^t b(s)^2 ds$.\n\nThus, $Y_t$ is the sum of a constant and a normal random variable, which means $Y_t$ itself is a normal random variable. Its distribution is\n$$\nY_t \\sim \\mathcal{N}\\left( \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2} b(s)^2 \\right) ds, \\int_0^t b(s)^2 ds \\right).\n$$\nSince $X_t = \\exp(Y_t)$ and $Y_t$ is normally distributed, the random variable $X_t$ has a log-normal distribution by definition. This proves the first part of the problem.\n\nTo compute the expectation and variance of $X_t$, we use the properties of the log-normal distribution. If $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then for any real number $k$, the $k$-th moment of $X = \\exp(Y)$ is given by\n$$\n\\mathbb{E}[X^k] = \\mathbb{E}[\\exp(kY)] = \\exp\\left( k\\mu + \\frac{1}{2}k^2\\sigma^2 \\right).\n$$\nFor our process $X_t = \\exp(Y_t)$, we have:\n$$\n\\mu_t = \\mathbb{E}[Y_t] = \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2}b(s)^2 \\right) ds\n$$\n$$\n\\sigma_t^2 = \\operatorname{Var}(Y_t) = \\int_0^t b(s)^2 ds\n$$\nThe expectation of $X_t$ corresponds to the first moment ($k=1$):\n$$\n\\mathbb{E}[X_t] = \\exp\\left( \\mu_t + \\frac{1}{2}\\sigma_t^2 \\right)\n$$\n$$\n\\mathbb{E}[X_t] = \\exp\\left( \\left[ \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2}b(s)^2 \\right) ds \\right] + \\frac{1}{2} \\int_0^t b(s)^2 ds \\right)\n$$\n$$\n\\mathbb{E}[X_t] = \\exp\\left( \\ln(x_0) + \\int_0^t a(s) ds - \\frac{1}{2}\\int_0^t b(s)^2 ds + \\frac{1}{2}\\int_0^t b(s)^2 ds \\right)\n$$\n$$\n\\mathbb{E}[X_t] = \\exp\\left( \\ln(x_0) + \\int_0^t a(s) ds \\right) = x_0 \\exp\\left( \\int_0^t a(s) ds \\right).\n$$\nTo find the variance, we first compute the second moment $\\mathbb{E}[X_t^2]$ (using $k=2$):\n$$\n\\mathbb{E}[X_t^2] = \\exp\\left( 2\\mu_t + \\frac{1}{2}(2^2)\\sigma_t^2 \\right) = \\exp\\left( 2\\mu_t + 2\\sigma_t^2 \\right)\n$$\n$$\n\\mathbb{E}[X_t^2] = \\exp\\left( 2\\left[ \\ln(x_0) + \\int_0^t \\left( a(s) - \\frac{1}{2}b(s)^2 \\right) ds \\right] + 2 \\int_0^t b(s)^2 ds \\right)\n$$\n$$\n\\mathbb{E}[X_t^2] = \\exp\\left( 2\\ln(x_0) + 2\\int_0^t a(s) ds - \\int_0^t b(s)^2 ds + 2\\int_0^t b(s)^2 ds \\right)\n$$\n$$\n\\mathbb{E}[X_t^2] = \\exp\\left( 2\\ln(x_0) + 2\\int_0^t a(s) ds + \\int_0^t b(s)^2 ds \\right)\n$$\n$$\n\\mathbb{E}[X_t^2] = x_0^2 \\exp\\left( 2\\int_0^t a(s) ds + \\int_0^t b(s)^2 ds \\right).\n$$\nThe variance is given by $\\operatorname{Var}(X_t) = \\mathbb{E}[X_t^2] - (\\mathbb{E}[X_t])^2$:\n$$\n\\operatorname{Var}(X_t) = x_0^2 \\exp\\left( 2\\int_0^t a(s) ds + \\int_0^t b(s)^2 ds \\right) - \\left( x_0 \\exp\\left( \\int_0^t a(s) ds \\right) \\right)^2\n$$\n$$\n\\operatorname{Var}(X_t) = x_0^2 \\exp\\left( 2\\int_0^t a(s) ds \\right) \\exp\\left( \\int_0^t b(s)^2 ds \\right) - x_0^2 \\exp\\left( 2\\int_0^t a(s) ds \\right)\n$$\nFactoring out the common term $x_0^2 \\exp\\left( 2\\int_0^t a(s) ds \\right)$, we obtain:\n$$\n\\operatorname{Var}(X_t) = x_0^2 \\exp\\left( 2\\int_0^t a(s) ds \\right) \\left[ \\exp\\left( \\int_0^t b(s)^2 ds \\right) - 1 \\right].\n$$\nThe two required quantities are the expectation $\\mathbb{E}[X_t]$ and the variance $\\operatorname{Var}(X_t)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} x_0 \\exp\\left( \\int_0^t a(s) \\, ds \\right) & x_0^2 \\exp\\left( 2\\int_0^t a(s) \\, ds \\right) \\left[ \\exp\\left( \\int_0^t b(s)^2 \\, ds \\right) - 1 \\right] \\end{pmatrix}}\n$$", "id": "3048360"}]}