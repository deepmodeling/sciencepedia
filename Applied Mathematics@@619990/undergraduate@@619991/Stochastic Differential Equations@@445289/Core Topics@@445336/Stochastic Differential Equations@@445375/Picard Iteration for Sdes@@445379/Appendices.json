{"hands_on_practices": [{"introduction": "We begin our hands-on exploration of Picard iteration with the most fundamental type of stochastic differential equation: one with constant coefficients. This exercise [@problem_id:3069748] is designed to build your intuition by revealing the core mechanism of the iteration in its simplest form. You will discover how the structure of the coefficients, being independent of the process state $X_t$, leads to a remarkable result where the iteration converges to the exact solution in a single step, providing a clear foundation for more complex cases.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions, and let $(W_{t})_{t \\geq 0}$ denote a standard Wiener process (Brownian motion) adapted to $(\\mathcal{F}_{t})_{t \\geq 0}$. Fix constants $\\alpha \\in \\mathbb{R}$, $\\beta \\in \\mathbb{R}$, and an initial condition $x_{0} \\in \\mathbb{R}$. Consider the stochastic differential equation (SDE)\n$$\ndX_{t}=\\alpha\\,dt+\\beta\\,dW_{t},\\quad X_{0}=x_{0},\\quad t \\in [0,T],\n$$\nwith the requirement that $(X_{t})_{t \\in [0,T]}$ be $(\\mathcal{F}_{t})$-adapted and have square-integrable paths. Starting from the fundamental integral formulation of SDEs and the definition of the Itô integral, construct the Picard iteration associated with this SDE by defining an iteration map acting on adapted processes with square-integrable paths, and explain, using only the structural properties of the drift and diffusion coefficients, why the iteration converges in a single step from an arbitrary initial guess. As your final output, provide the explicit closed-form expression for the solution process $X_{t}$ on $[0,T]$. The final answer must be a single analytic expression in $t$, $\\alpha$, $\\beta$, $x_{0}$, and $W_{t}$.", "solution": "The problem statement is a valid exercise in stochastic calculus. It is well-posed, scientifically grounded, and provides all necessary information to construct a rigorous solution. We may therefore proceed with the derivation.\n\nThe stochastic differential equation (SDE) under consideration is:\n$$\ndX_{t} = \\alpha\\,dt + \\beta\\,dW_{t}, \\quad X_{0}=x_{0}\n$$\nfor $t \\in [0,T]$, where $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are constants, $x_{0} \\in \\mathbb{R}$ is the initial condition, and $(W_{t})_{t \\geq 0}$ is a standard Wiener process on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$.\n\nThe fundamental integral formulation of this SDE expresses the process $(X_{t})_{t \\in [0,T]}$ as the solution to the integral equation:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThis equation is an instance of the general form for an Itô process:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} a(s, X_{s}) \\, ds + \\int_{0}^{t} b(s, X_{s}) \\, dW_{s}\n$$\nIn our specific case, the drift coefficient is $a(t,x) = \\alpha$ and the diffusion coefficient is $b(t,x) = \\beta$.\n\nThe Picard iteration method provides a constructive proof of the existence and uniqueness of solutions to SDEs. It defines a sequence of stochastic processes $(X_{t}^{(n)})_{t \\in [0,T]}$ that converges to the true solution $(X_{t})_{t \\in [0,T]}$. The iteration is defined by an operator or map, $\\Phi$, which acts on the space of $(\\mathcal{F}_{t})$-adapted processes with square-integrable paths on $[0,T]$. For a process $Y = (Y_{t})_{t \\in [0,T]}$, the map is defined as:\n$$\n(\\Phi(Y))_{t} = x_{0} + \\int_{0}^{t} a(s, Y_{s}) \\, ds + \\int_{0}^{t} b(s, Y_{s}) \\, dW_{s}\n$$\nThe Picard iterates are then generated by the relation $X^{(n+1)} = \\Phi(X^{(n)})$ for $n \\geq 0$.\n\nFor the given SDE, the drift and diffusion coefficients are $a(t,x) = \\alpha$ and $b(t,x) = \\beta$, respectively. Notice that these coefficients are constant and, crucially, do not depend on the state variable $x$. This property is central to the behavior of the iteration. The iteration map for this specific problem is:\n$$\n(\\Phi(Y))_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nObserve that the right-hand side of this definition is independent of the input process $Y$. Therefore, $\\Phi$ is a constant map: it maps any adapted process $Y$ (with square-integrable paths) to the same, single, fixed process.\n\nLet us construct the Picard sequence. We begin with an arbitrary initial guess for the solution, denoted by $(X_{t}^{(0)})_{t \\in [0,T]}$, which must be an $(\\mathcal{F}_{t})$-adapted process with square-integrable paths. A common but not mandatory choice is $X_{t}^{(0)} = x_{0}$ for all $t$.\n\nThe first iterate, $(X_{t}^{(1)})_{t \\in [0,T]}$, is obtained by applying the map $\\Phi$ to the initial guess $X^{(0)}$:\n$$\nX_{t}^{(1)} = (\\Phi(X^{(0)}))_{t} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThe integrals can be evaluated directly. The first integral is a standard Riemann integral of a constant, and the second is an Itô integral of a constant process.\n$$\n\\int_{0}^{t} \\alpha \\, ds = \\alpha t\n$$\n$$\n\\int_{0}^{t} \\beta \\, dW_{s} = \\beta \\int_{0}^{t} dW_{s} = \\beta (W_{t} - W_{0})\n$$\nBy convention, a standard Wiener process starts at zero, so $W_{0}=0$. Thus, the first iterate is:\n$$\nX_{t}^{(1)} = x_{0} + \\alpha t + \\beta W_{t}\n$$\nThis expression holds for any $t \\in [0,T]$.\n\nNow, we compute the second iterate, $(X_{t}^{(2)})_{t \\in [0,T]}$, by applying the map $\\Phi$ to the first iterate $X^{(1)}$:\n$$\nX_{t}^{(2)} = (\\Phi(X^{(1)}))_{t} = x_{0} + \\int_{0}^{t} a(s, X_{s}^{(1)}) \\, ds + \\int_{0}^{t} b(s, X_{s}^{(1)}) \\, dW_{s}\n$$\nSubstituting the specific forms of the coefficients $a(s,x) = \\alpha$ and $b(s,x) = \\beta$:\n$$\nX_{t}^{(2)} = x_{0} + \\int_{0}^{t} \\alpha \\, ds + \\int_{0}^{t} \\beta \\, dW_{s}\n$$\nThis is precisely the same expression we obtained for $X_{t}^{(1)}$. Therefore,\n$$\nX_{t}^{(2)} = x_{0} + \\alpha t + \\beta W_{t} = X_{t}^{(1)}\n$$\nSince $X^{(2)} = X^{(1)}$, the sequence of iterates becomes stationary. For any $n \\geq 1$, we have $X^{(n+1)} = \\Phi(X^{(n)}) = \\Phi(X^{(1)}) = X^{(2)} = X^{(1)}$. The sequence of processes is $(X^{(0)}, X^{(1)}, X^{(1)}, X^{(1)}, \\dots)$. This sequence converges immediately to the process $X^{(1)}$.\n\nThe convergence in a single step is a direct consequence of the fact that the drift and diffusion coefficients, $\\alpha$ and $\\beta$, are state-independent. This makes the Picard iteration map $\\Phi$ a constant map, meaning its output does not depend on its input. The first application of the map yields the fixed point, and all subsequent applications simply return the same fixed point. For a general SDE where coefficients depend on $X_t$, $\\Phi$ is not a constant map, and one must use its contractive property on a suitable function space to prove convergence, which does not occur in a single step.\n\nThe unique solution to the SDE is the fixed point of the iteration map, which we have found to be $X^{(1)}$. Therefore, the explicit closed-form solution for the process $(X_{t})_{t \\in [0,T]}$ is:\n$$\nX_{t} = x_{0} + \\alpha t + \\beta W_{t}\n$$\nThis process is known as an arithmetic Brownian motion or a drifted Wiener process.", "answer": "$$\\boxed{x_{0} + \\alpha t + \\beta W_{t}}$$", "id": "3069748"}, {"introduction": "Having mastered the simplest case, we now advance to one of the most important models in financial mathematics: Geometric Brownian Motion. This practice [@problem_id:3069769] is essential for understanding how Picard iteration handles state-dependent coefficients, where the process is no longer trivial. By calculating the first two iterates, you will gain firsthand experience with the resulting Itô integrals and witness how the famous Itô correction term, a cornerstone of stochastic calculus, naturally emerges from the iterative procedure.", "problem": "Consider the stochastic differential equation (SDE) $dX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t}$, where $W_{t}$ is a standard Brownian motion, $\\mu$ and $\\sigma$ are real constants, and the initial condition is the deterministic constant $X_{0}=x_{0}0$. Write the integral form $X_{t}=x_{0}+\\int_{0}^{t}\\mu X_{s}\\,ds+\\int_{0}^{t}\\sigma X_{s}\\,dW_{s}$ and define the Picard iteration by $X_{t}^{(0)}\\equiv x_{0}$ and, for each integer $n\\geq 0$, $X_{t}^{(n+1)}:=x_{0}+\\int_{0}^{t}\\mu X_{s}^{(n)}\\,ds+\\int_{0}^{t}\\sigma X_{s}^{(n)}\\,dW_{s}$. Compute $X_{t}^{(1)}$ and $X_{t}^{(2)}$ explicitly, simplifying all Itô integrals using identities derived from Itô calculus so that the final expressions depend only on $t$ and $W_{t}$. Briefly discuss the pattern you observe in the coefficients of the resulting polynomials in $t$ and $W_{t}$ and how it relates to the exact solution. Provide your final answer as the ordered pair $(X_{t}^{(1)},X_{t}^{(2)})$. Since this is a symbolic computation, no rounding is required.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. We may proceed with the solution.\n\nThe problem asks for the first two Picard iterates, $X_{t}^{(1)}$ and $X_{t}^{(2)}$, for the stochastic differential equation (SDE) representing geometric Brownian motion:\n$$dX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t}$$\nwith a deterministic initial condition $X_{0}=x_{0}  0$. Here, $\\mu$ and $\\sigma$ are real constants, and $W_{t}$ is a standard one-dimensional Brownian motion, which implies $W_{0}=0$.\n\nThe Picard iteration scheme is defined by the integral form of the SDE:\n$$X_{t}^{(0)} \\equiv x_{0}$$\n$$X_{t}^{(n+1)} := x_{0}+\\int_{0}^{t}\\mu X_{s}^{(n)}\\,ds+\\int_{0}^{t}\\sigma X_{s}^{(n)}\\,dW_{s} \\quad \\text{for } n \\geq 0$$\n\n**Computation of $X_{t}^{(1)}$**\n\nFor $n=0$, we substitute $X_{s}^{(0)} = x_{0}$ into the iteration formula:\n$$X_{t}^{(1)} = x_{0} + \\int_{0}^{t}\\mu X_{s}^{(0)}\\,ds + \\int_{0}^{t}\\sigma X_{s}^{(0)}\\,dW_{s}$$\n$$X_{t}^{(1)} = x_{0} + \\int_{0}^{t}\\mu x_{0}\\,ds + \\int_{0}^{t}\\sigma x_{0}\\,dW_{s}$$\nSince $\\mu$, $\\sigma$, and $x_{0}$ are constants, we can factor them out of the integrals:\n$$X_{t}^{(1)} = x_{0} + \\mu x_{0}\\int_{0}^{t}ds + \\sigma x_{0}\\int_{0}^{t}dW_{s}$$\nThe integrals are straightforward:\n$$\\int_{0}^{t}ds = t$$\n$$\\int_{0}^{t}dW_{s} = W_{t} - W_{0} = W_{t}$$\nSubstituting these back, we get the expression for $X_{t}^{(1)}$:\n$$X_{t}^{(1)} = x_{0} + \\mu x_{0} t + \\sigma x_{0} W_{t} = x_{0}(1 + \\mu t + \\sigma W_{t})$$\n\n**Computation of $X_{t}^{(2)}$**\n\nFor $n=1$, we substitute the expression for $X_{s}^{(1)}$ into the iteration formula:\n$$X_{t}^{(2)} = x_{0} + \\int_{0}^{t}\\mu X_{s}^{(1)}\\,ds + \\int_{0}^{t}\\sigma X_{s}^{(1)}\\,dW_{s}$$\n$$X_{t}^{(2)} = x_{0} + \\int_{0}^{t}\\mu \\left[x_{0}(1 + \\mu s + \\sigma W_{s})\\right]\\,ds + \\int_{0}^{t}\\sigma \\left[x_{0}(1 + \\mu s + \\sigma W_{s})\\right]\\,dW_{s}$$\nFactoring out the constant $x_{0}$:\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\mu\\int_{0}^{t}(1 + \\mu s + \\sigma W_{s})\\,ds + \\sigma\\int_{0}^{t}(1 + \\mu s + \\sigma W_{s})\\,dW_{s} \\right)$$\nWe evaluate the integrals term by term. The first is a Lebesgue integral, and the second is an Itô integral.\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\mu\\left[\\int_{0}^{t}ds + \\mu\\int_{0}^{t}s\\,ds + \\sigma\\int_{0}^{t}W_{s}\\,ds\\right] + \\sigma\\left[\\int_{0}^{t}dW_{s} + \\mu\\int_{0}^{t}s\\,dW_{s} + \\sigma\\int_{0}^{t}W_{s}\\,dW_{s}\\right] \\right)$$\nThe simpler integrals evaluate to:\n$$\\int_{0}^{t}ds = t, \\quad \\int_{0}^{t}s\\,ds = \\frac{t^2}{2}, \\quad \\int_{0}^{t}dW_{s} = W_{t}$$\nThe remaining integrals, $\\int_{0}^{t}W_{s}\\,dW_{s}$ and the pair $\\int_{0}^{t}W_{s}\\,ds, \\int_{0}^{t}s\\,dW_{s}$, require Itô calculus.\n\n1. To evaluate $\\int_{0}^{t}W_{s}\\,dW_{s}$, we apply Itô's lemma to the function $f(W_{t}) = \\frac{1}{2}W_{t}^{2}$.\nThe differential is $df(W_{t}) = f'(W_{t})dW_{t} + \\frac{1}{2}f''(W_{t})(dW_{t})^2$. With $f'(W_t) = W_t$ and $f''(W_t)=1$, and using the identity $(dW_{t})^2 = dt$, we get:\n$$d\\left(\\frac{1}{2}W_{t}^{2}\\right) = W_{t}\\,dW_{t} + \\frac{1}{2}dt$$\nIntegrating from $0$ to $t$:\n$$\\int_{0}^{t}d\\left(\\frac{1}{2}W_{s}^{2}\\right) = \\int_{0}^{t}W_{s}\\,dW_{s} + \\frac{1}{2}\\int_{0}^{t}ds$$\n$$\\frac{1}{2}W_{t}^{2} - \\frac{1}{2}W_{0}^{2} = \\int_{0}^{t}W_{s}\\,dW_{s} + \\frac{1}{2}t$$\nSince $W_{0}=0$, we obtain the identity:\n$$\\int_{0}^{t}W_{s}\\,dW_{s} = \\frac{1}{2}(W_{t}^{2} - t)$$\n\n2. To handle the terms $\\int_{0}^{t}W_{s}\\,ds$ and $\\int_{0}^{t}s\\,dW_{s}$, we use integration by parts for Itô processes, which is an application of Itô's lemma to the product $f(t, W_t) = tW_t$.\n$$d(tW_t) = \\frac{\\partial f}{\\partial t}dt + \\frac{\\partial f}{\\partial W}dW_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial W^2}(dW_t)^2$$\nWith $\\frac{\\partial f}{\\partial t} = W_t$, $\\frac{\\partial f}{\\partial W} = t$, and $\\frac{\\partial^2 f}{\\partial W^2} = 0$, this simplifies to:\n$$d(tW_t) = W_t dt + t dW_t$$\nIntegrating from $0$ to $t$ gives:\n$$tW_t - 0 \\cdot W_0 = \\int_0^t W_s ds + \\int_0^t s dW_s$$\n$$tW_t = \\int_0^t W_s ds + \\int_0^t s dW_s$$\n\nNow, we substitute these results back into the expression for $X_{t}^{(2)}$:\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\mu\\left[t + \\frac{\\mu t^2}{2} + \\sigma\\int_{0}^{t}W_{s}\\,ds\\right] + \\sigma\\left[W_{t} + \\mu\\int_{0}^{t}s\\,dW_{s} + \\sigma\\frac{1}{2}(W_{t}^{2} - t)\\right] \\right)$$\nLet's expand and collect terms:\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\mu t + \\frac{\\mu^2 t^2}{2} + \\sigma W_{t} + \\frac{\\sigma^2}{2}(W_{t}^{2} - t) + \\mu\\sigma\\left(\\int_{0}^{t}W_{s}\\,ds + \\int_{0}^{t}s\\,dW_{s}\\right) \\right)$$\nUsing the identity from integration by parts, $\\int_{0}^{t}W_{s}\\,ds + \\int_{0}^{t}s\\,dW_{s} = tW_{t}$:\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\mu t + \\frac{\\mu^2 t^2}{2} + \\sigma W_{t} + \\frac{\\sigma^2 W_{t}^{2}}{2} - \\frac{\\sigma^2 t}{2} + \\mu\\sigma tW_{t} \\right)$$\nFinally, grouping terms by their dependence on $t$ and $W_{t}$:\n$$X_{t}^{(2)} = x_{0} \\left( 1 + \\left(\\mu - \\frac{\\sigma^2}{2}\\right)t + \\sigma W_{t} + \\frac{1}{2}\\mu^2 t^2 + \\mu\\sigma tW_t + \\frac{1}{2}\\sigma^2 W_t^2 \\right)$$\n\n**Discussion of the Pattern**\n\nThe exact solution to the SDE is $X_t = x_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)$. The Picard iteration method constructs this solution iteratively.\n- $X_t^{(0)} = x_0$ is the zeroth-order term.\n- $X_t^{(1)} = x_0(1 + \\mu t + \\sigma W_t)$ includes terms that arise from a single integration with respect to $dt$ or $dW_t$.\n- $X_t^{(2)} = x_0 \\left( 1 + \\mu t + \\sigma W_t + \\frac{\\mu^2 t^2}{2} + \\mu\\sigma tW_t + \\frac{\\sigma^2}{2}(W_t^2 - t) \\right)$. This can be rewritten as $X_t^{(2)} = x_0 \\left( 1 + (\\mu t + \\sigma W_t) + \\frac{1}{2}(\\mu t + \\sigma W_t)^2 - \\frac{\\sigma^2}{2}t \\right)$.\nThis expression resembles the second-order Taylor expansion of $\\exp(\\mu t + \\sigma W_t)$, with a crucial difference: the Itô correction term $-\\frac{\\sigma^2}{2}t$. This term originates from the quadratic variation of Brownian motion, $(dW_t)^2=dt$, which manifests in the result $\\int_0^t W_s dW_s = \\frac{1}{2}(W_t^2-t)$. The Picard iteration process naturally generates the Itô-Taylor expansion of the solution, correctly incorporating the stochastic correction terms at each step. As $n \\to \\infty$, the iterates converge to the exact solution, with the accumulated corrections forming the $-\\frac{1}{2}\\sigma^2 t$ term in the exponent.\n\nThe final expressions for the first two non-trivial iterates are:\n$X_{t}^{(1)} = x_{0}(1 + \\mu t + \\sigma W_{t})$\n$X_{t}^{(2)} = x_{0} \\left( 1 + \\left(\\mu - \\frac{\\sigma^2}{2}\\right)t + \\sigma W_{t} + \\frac{1}{2}\\mu^2 t^2 + \\mu\\sigma tW_t + \\frac{1}{2}\\sigma^2 W_t^2 \\right)$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_{0}(1 + \\mu t + \\sigma W_{t})  x_{0} \\left( 1 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_{t} + \\frac{1}{2}\\mu^2 t^2 + \\mu\\sigma tW_t + \\frac{1}{2}\\sigma^2 W_t^2 \\right)\n\\end{pmatrix}\n}\n$$", "id": "3069769"}, {"introduction": "To broaden our perspective, we now turn to another key model, the mean-reverting Ornstein-Uhlenbeck process. This exercise [@problem_id:3069760] shifts our focus from simply observing the path structure to quantitatively assessing the approximation's accuracy. By computing the first two iterates and comparing their expected values to the true solution's mean, you will learn how to analyze the bias of the Picard approximation, gaining a deeper, statistical insight into the nature of its convergence.", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation driven by a standard Brownian motion (Wiener process) $W_{t}$ on a filtered probability space, given by\n$$\ndX_{t}=\\theta\\left(\\mu-X_{t}\\right)\\,dt+\\sigma\\,dW_{t}, \\quad X_{0}=x_{0},\n$$\nwhere $\\theta0$, $\\mu\\in\\mathbb{R}$, $\\sigma0$, and $x_{0}\\in\\mathbb{R}$ are constants. Write the equation in its integral form and define the Picard iteration scheme $\\{X^{(n)}_{t}\\}_{n\\geq 0}$ by choosing the seed $X^{(0)}_{t}\\equiv x_{0}$ and, for $n\\geq 0$,\n$$\nX^{(n+1)}_{t}=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-X^{(n)}_{s}\\right)\\,ds+\\sigma\\,W_{t}.\n$$\nPerform the first two Picard iterates to obtain explicit expressions for $X^{(1)}_{t}$ and $X^{(2)}_{t}$. Then derive the exact strong solution $X_{t}$ of the stochastic differential equation and its expectation $\\mathbb{E}[X_{t}]$. Compute $\\mathbb{E}[X^{(1)}_{t}]$ and $\\mathbb{E}[X^{(2)}_{t}]$, and compare them to $\\mathbb{E}[X_{t}]$. Finally, provide the closed-form expression for the bias in the second Picard iterate, defined as\n$$\n\\mathbb{E}\\!\\left[X^{(2)}_{t}\\right]-\\mathbb{E}\\!\\left[X_{t}\\right],\n$$\nexpressed in terms of $\\theta$, $\\mu$, $x_{0}$, and $t$. Your final answer must be this single analytic expression.", "solution": "The given stochastic differential equation is\n$$\ndX_{t}=\\theta\\left(\\mu-X_{t}\\right)\\,dt+\\sigma\\,dW_{t}, \\quad X_{0}=x_{0}.\n$$\nThe integral (mild) form of this equation follows from the fundamental definition of an Itô stochastic differential equation with constant diffusion coefficient:\n$$\nX_{t}=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-X_{s}\\right)\\,ds+\\int_{0}^{t}\\sigma\\,dW_{s}=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-X_{s}\\right)\\,ds+\\sigma\\,W_{t}.\n$$\nDefine the Picard iteration mapping on adapted processes by\n$$\n\\Phi(Y)(t):=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-Y_{s}\\right)\\,ds+\\sigma\\,W_{t},\n$$\nand choose $X^{(0)}_{t}\\equiv x_{0}$. Then\n$$\nX^{(1)}_{t}=\\Phi\\!\\left(X^{(0)}\\right)(t)=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-x_{0}\\right)\\,ds+\\sigma\\,W_{t}\n= x_{0}+\\theta\\left(\\mu-x_{0}\\right)t+\\sigma\\,W_{t}.\n$$\nFor the second iterate, substitute $X^{(1)}_{s}$ into the drift integral:\n$$\nX^{(2)}_{t}=\\Phi\\!\\left(X^{(1)}\\right)(t)=x_{0}+\\int_{0}^{t}\\theta\\left(\\mu-X^{(1)}_{s}\\right)\\,ds+\\sigma\\,W_{t}.\n$$\nCompute the integrand using the explicit form of $X^{(1)}_{s}$:\n$$\n\\mu-X^{(1)}_{s}=\\mu-\\left[x_{0}+\\theta\\left(\\mu-x_{0}\\right)s+\\sigma\\,W_{s}\\right]\n=\\left(\\mu-x_{0}\\right)\\left(1-\\theta s\\right)-\\sigma\\,W_{s}.\n$$\nHence\n$$\n\\int_{0}^{t}\\theta\\left(\\mu-X^{(1)}_{s}\\right)\\,ds\n=\\theta\\left(\\mu-x_{0}\\right)\\int_{0}^{t}\\left(1-\\theta s\\right)\\,ds-\\theta\\sigma\\int_{0}^{t}W_{s}\\,ds\n=\\theta\\left(\\mu-x_{0}\\right)\\left(t-\\frac{\\theta}{2}t^{2}\\right)-\\theta\\sigma\\int_{0}^{t}W_{s}\\,ds.\n$$\nTherefore\n$$\nX^{(2)}_{t}\n=x_{0}+\\theta\\left(\\mu-x_{0}\\right)t-\\frac{\\theta^{2}}{2}\\left(\\mu-x_{0}\\right)t^{2}-\\theta\\sigma\\int_{0}^{t}W_{s}\\,ds+\\sigma\\,W_{t}.\n$$\n\nNext, derive the exact solution. Rewrite the stochastic differential equation as\n$$\ndX_{t}+\\theta X_{t}\\,dt=\\theta\\mu\\,dt+\\sigma\\,dW_{t}.\n$$\nMultiply both sides by the integrating factor $\\exp\\!\\left(\\theta t\\right)$ to obtain\n$$\nd\\!\\left(\\exp\\!\\left(\\theta t\\right)X_{t}\\right)=\\theta\\mu\\,\\exp\\!\\left(\\theta t\\right)\\,dt+\\sigma\\,\\exp\\!\\left(\\theta t\\right)\\,dW_{t}.\n$$\nIntegrating from $0$ to $t$ and using $X_{0}=x_{0}$,\n$$\n\\exp\\!\\left(\\theta t\\right)X_{t}-x_{0}\n=\\theta\\mu\\int_{0}^{t}\\exp\\!\\left(\\theta s\\right)\\,ds+\\sigma\\int_{0}^{t}\\exp\\!\\left(\\theta s\\right)\\,dW_{s}.\n$$\nSince $\\int_{0}^{t}\\exp\\!\\left(\\theta s\\right)\\,ds=\\frac{\\exp\\!\\left(\\theta t\\right)-1}{\\theta}$, it follows that\n$$\nX_{t}=\\mu+\\left(x_{0}-\\mu\\right)\\exp\\!\\left(-\\theta t\\right)+\\sigma\\int_{0}^{t}\\exp\\!\\left(-\\theta\\left(t-s\\right)\\right)\\,dW_{s}.\n$$\nTaking expectations and using that stochastic Itô integrals with deterministic integrands have zero mean,\n$$\n\\mathbb{E}\\!\\left[X_{t}\\right]=\\mu+\\left(x_{0}-\\mu\\right)\\exp\\!\\left(-\\theta t\\right).\n$$\n\nNow compute the expectations of the first two Picard iterates. Using $\\mathbb{E}\\!\\left[W_{t}\\right]=0$ and $\\mathbb{E}\\!\\left[\\int_{0}^{t}W_{s}\\,ds\\right]=\\int_{0}^{t}\\mathbb{E}\\!\\left[W_{s}\\right]\\,ds=0$, we obtain\n$$\n\\mathbb{E}\\!\\left[X^{(1)}_{t}\\right]=x_{0}+\\theta\\left(\\mu-x_{0}\\right)t,\n$$\nand\n$$\n\\mathbb{E}\\!\\left[X^{(2)}_{t}\\right]=x_{0}+\\theta\\left(\\mu-x_{0}\\right)t-\\frac{\\theta^{2}}{2}\\left(\\mu-x_{0}\\right)t^{2}.\n$$\nTherefore, the bias in the second Picard iterate relative to the exact mean is\n$$\n\\mathbb{E}\\!\\left[X^{(2)}_{t}\\right]-\\mathbb{E}\\!\\left[X_{t}\\right]\n=\\left[x_{0}+\\theta\\left(\\mu-x_{0}\\right)t-\\frac{\\theta^{2}}{2}\\left(\\mu-x_{0}\\right)t^{2}\\right]-\\left[\\mu+\\left(x_{0}-\\mu\\right)\\exp\\!\\left(-\\theta t\\right)\\right].\n$$\nThis simplifies to\n$$\n\\mathbb{E}\\!\\left[X^{(2)}_{t}\\right]-\\mathbb{E}\\!\\left[X_{t}\\right]\n=\\left(x_{0}-\\mu\\right)\\left[1-\\exp\\!\\left(-\\theta t\\right)-\\theta t+\\frac{\\theta^{2}}{2}t^{2}\\right].\n$$\nThis expression shows that the second Picard iterate matches the Taylor expansion of the exact mean up to order $t^{2}$, and the leading bias term is of order $t^{3}$.", "answer": "$$\\boxed{\\left(x_{0}-\\mu\\right)\\left[1-\\exp\\!\\left(-\\theta t\\right)-\\theta t+\\frac{\\theta^{2}}{2}t^{2}\\right]}$$", "id": "3069760"}]}