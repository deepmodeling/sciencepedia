{"hands_on_practices": [{"introduction": "This first exercise is a cornerstone of stochastic calculus, particularly in mathematical finance where it is used to model stock prices. We will solve the Geometric Brownian Motion SDE by employing one of the most powerful tools in our arsenal: Itô's lemma with a logarithmic transformation. This practice reinforces the mechanics of Itô calculus and reveals how a multiplicative random process can be understood by transforming it into an additive one. [@problem_id:3064037]", "problem": "Consider a probability space equipped with a filtration satisfying the usual conditions, and let $W_{t}$ denote a standard one-dimensional Brownian motion (also called Wiener process). Let $S_{t}$ be an Itô process satisfying the linear stochastic differential equation (SDE)\n$$\ndS_{t} = \\mu\\,S_{t}\\,dt + \\sigma\\,S_{t}\\,dW_{t}, \\quad S_{0} > 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants. Using only foundational tools from Itô calculus and properties of standard Brownian motion, derive the explicit strong solution for $S_{t}$ as a function of $t$, $S_{0}$, $\\mu$, $\\sigma$, and $W_{t}$. Then, establish the distribution of $\\ln S_{t}$ by computing its mean and variance from first principles. Your derivation must begin from the Itô differential of a smooth transformation of $S_{t}$ and the defining property that $W_{t}$ has stationary independent increments with $W_{t} \\sim \\mathcal{N}(0,t)$. Do not invoke any pre-memorized closed-form solution; derive it explicitly.\n\nProvide your final answer as a single analytic expression in the row-matrix format, with three entries ordered as follows:\n1. The explicit solution for $S_{t}$.\n2. The mean of $\\ln S_{t}$.\n3. The variance of $\\ln S_{t}$.\n\nNo numerical rounding is required for this problem.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem central to the theory of stochastic differential equations. All necessary definitions and conditions are provided, and there are no contradictions or ambiguities.\n\nWe are tasked with finding the explicit strong solution to the linear stochastic differential equation (SDE) for a process $S_t$, and then determining the mean and variance of $\\ln S_t$. The SDE is given by:\n$$\ndS_{t} = \\mu S_{t} dt + \\sigma S_{t} dW_{t}\n$$\nwith an initial condition $S_{0} > 0$, where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants, and $W_t$ is a standard one-dimensional Brownian motion.\n\nThe structure of the SDE suggests that a logarithmic transformation may simplify the equation. Let us define a new process $Y_t = f(S_t)$, where $f(x) = \\ln(x)$. The function $f(x)$ is twice continuously differentiable for $x > 0$. Since $S_0 > 0$, and the solution to this SDE is known to remain almost surely positive, this transformation is well-defined.\n\nWe apply Itô's lemma to find the differential $dY_t$. For a general Itô process $X_t$ and a twice-differentiable function $g(t, x)$, Itô's lemma states:\n$$\ndg(t, X_t) = \\frac{\\partial g}{\\partial t} dt + \\frac{\\partial g}{\\partial x} dX_t + \\frac{1}{2} \\frac{\\partial^2 g}{\\partial x^2} (dX_t)^2\n$$\nIn our case, the function $f$ does not explicitly depend on time $t$, so $\\frac{\\partial f}{\\partial t} = 0$. We have $X_t = S_t$ and $f(S_t) = \\ln(S_t)$. The required partial derivatives of $f(x)=\\ln(x)$ with respect to its argument $x$ are:\n$$\n\\frac{df}{dx} = f'(x) = \\frac{1}{x} \\implies \\frac{\\partial f}{\\partial S_t} = \\frac{1}{S_t}\n$$\n$$\n\\frac{d^2f}{dx^2} = f''(x) = -\\frac{1}{x^2} \\implies \\frac{\\partial^2 f}{\\partial S_t^2} = -\\frac{1}{S_t^2}\n$$\nNext, we determine the quadratic variation term $(dS_t)^2$. Using the Itô multiplication rules ($dt \\cdot dt = 0$, $dt \\cdot dW_t = 0$, and $dW_t \\cdot dW_t = dt$):\n$$\n(dS_t)^2 = (\\mu S_t dt + \\sigma S_t dW_t)^2 = (\\mu S_t dt)^2 + 2(\\mu S_t dt)(\\sigma S_t dW_t) + (\\sigma S_t dW_t)^2\n$$\n$$\n(dS_t)^2 = \\mu^2 S_t^2 (dt)^2 + 2\\mu\\sigma S_t^2 (dt \\cdot dW_t) + \\sigma^2 S_t^2 (dW_t)^2\n$$\n$$\n(dS_t)^2 = 0 + 0 + \\sigma^2 S_t^2 dt = \\sigma^2 S_t^2 dt\n$$\nNow, we substitute these components into Itô's lemma for $Y_t = f(S_t)$:\n$$\ndY_t = d(\\ln S_t) = \\left(\\frac{\\partial f}{\\partial S_t}\\right) dS_t + \\frac{1}{2} \\left(\\frac{\\partial^2 f}{\\partial S_t^2}\\right) (dS_t)^2\n$$\n$$\ndY_t = \\left(\\frac{1}{S_t}\\right) (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right) (\\sigma^2 S_t^2 dt)\n$$\nSimplifying the expression:\n$$\ndY_t = (\\mu dt + \\sigma dW_t) - \\frac{1}{2}\\sigma^2 dt\n$$\n$$\ndY_t = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)dt + \\sigma dW_t\n$$\nThis is a standard arithmetic Brownian motion, which can be solved by direct integration from $0$ to $t$:\n$$\n\\int_0^t dY_u = \\int_0^t \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)du + \\int_0^t \\sigma dW_u\n$$\n$$\nY_t - Y_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma (W_t - W_0)\n$$\nBy definition, $W_0=0$ and $Y_t = \\ln(S_t)$, so $Y_0 = \\ln(S_0)$. Substituting these back gives:\n$$\n\\ln(S_t) - \\ln(S_0) = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\n$$\n\\ln(S_t) = \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nTo obtain the explicit solution for $S_t$, we exponentiate both sides:\n$$\nS_t = \\exp\\left( \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\n$$\nS_t = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\nThis is the first required result: the explicit strong solution for $S_t$.\n\nNext, we establish the distribution of $\\ln S_t$ by computing its mean and variance. We have the expression for $\\ln S_t$:\n$$\n\\ln S_t = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nThe problem states that $W_t$ is a standard Brownian motion with the property $W_t \\sim \\mathcal{N}(0, t)$. This means its expected value is $\\mathbb{E}[W_t] = 0$ and its variance is $\\text{Var}(W_t) = \\mathbb{E}[(W_t - \\mathbb{E}[W_t])^2] = \\mathbb{E}[W_t^2] = t$.\nThe expression for $\\ln S_t$ is a linear function of the normally distributed random variable $W_t$. The terms $\\ln S_0$, $\\mu$, $\\sigma$, and $t$ are deterministic constants with respect to the probability measure at time $t$. Therefore, $\\ln S_t$ is also a normally distributed random variable.\n\nTo find the mean of $\\ln S_t$, we take the expectation of the expression:\n$$\n\\mathbb{E}[\\ln S_t] = \\mathbb{E}\\left[ \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\ln S_t] = \\mathbb{E}[\\ln S_0] + \\mathbb{E}\\left[\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t\\right] + \\mathbb{E}[\\sigma W_t]\n$$\nSince $\\ln S_0$, $\\mu$, $\\sigma$, and $t$ are non-random, and $\\mathbb{E}[W_t]=0$:\n$$\n\\mathbb{E}[\\ln S_t] = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma \\mathbb{E}[W_t]\n$$\n$$\n\\mathbb{E}[\\ln S_t] = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t\n$$\nThis is the second required result.\n\nTo find the variance of $\\ln S_t$, we use the property that for a random variable $X$ and constants $a$ and $b$, $\\text{Var}(aX+b) = a^2\\text{Var}(X)$. In our expression for $\\ln S_t$, the term $\\ln S_0 + (\\mu - \\frac{1}{2}\\sigma^2)t$ is a deterministic constant (analogous to $b$). The random part is $\\sigma W_t$ (analogous to $aX$).\n$$\n\\text{Var}(\\ln S_t) = \\text{Var}\\left( \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\n$$\n\\text{Var}(\\ln S_t) = \\text{Var}(\\sigma W_t)\n$$\n$$\n\\text{Var}(\\ln S_t) = \\sigma^2 \\text{Var}(W_t)\n$$\nUsing the property that $\\text{Var}(W_t) = t$:\n$$\n\\text{Var}(\\ln S_t) = \\sigma^2 t\n$$\nThis is the third and final required result.\n\nIn summary, $\\ln S_t$ follows a normal distribution:\n$$\n\\ln S_t \\sim \\mathcal{N}\\left(\\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t, \\sigma^2 t\\right)\n$$\nThis implies that $S_t$ follows a log-normal distribution.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right) & \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t & \\sigma^2 t\n\\end{pmatrix}\n}\n$$", "id": "3064037"}, {"introduction": "Often, we are less interested in a single path of a stochastic process and more in its statistical properties, like its mean and variance. This practice demonstrates a powerful technique for finding how these moments evolve over time. By applying Itô's formula and taking expectations, we can derive a system of ordinary differential equations (ODEs) that govern the moments, effectively translating a stochastic problem into a deterministic one. [@problem_id:3063970]", "problem": "Consider a one-dimensional linear stochastic differential equation (SDE) driven by a standard Wiener process (standard Brownian motion) $W_t$,\n$$\ndX_t \\;=\\; a\\,X_t\\,dt \\;+\\; b\\,X_t\\,dW_t \\;+\\; c\\,dt \\;+\\; d\\,dW_t,\\qquad X_0 = x_0,\n$$\nwhere $a,b,c,d,x_0 \\in \\mathbb{R}$ are constants. Assume $a \\neq 0$, $a+b^2 \\neq 0$, and $2a+b^2 \\neq 0$. Let $m_1(t) = \\mathbb{E}[X_t]$ and $m_2(t) = \\mathbb{E}[X_t^2]$.\n\nStarting from the fundamental Itô formula for twice continuously differentiable functions $f$,\n$$\ndf(X_t) \\;=\\; f'(X_t)\\,dX_t \\;+\\; \\tfrac{1}{2}\\,f''(X_t)\\,(dX_t)^2,\n$$\ntogether with the basic quadratic variation rules $dW_t^2 = dt$, $dt\\,dW_t = 0$, $dt^2=0$, and the fact that the Itô integral has zero mean for adapted square-integrable integrands, derive the ordinary differential equation (ODE) satisfied by $m_2(t) = \\mathbb{E}[X_t^2]$. Next, compute $m_1(t)$ from first principles and solve the ODE for $m_2(t)$ to obtain a closed-form analytic expression for $m_2(t)$ in terms of $t$, $a$, $b$, $c$, $d$, and $x_0$.\n\nProvide your final result as a single explicit expression for $m_2(t)$ with no units. No numerical rounding is required.", "solution": "The solution process involves three main parts:\n1.  Deriving and solving the ordinary differential equation (ODE) for the first moment, $m_1(t)$.\n2.  Deriving the ODE for the second moment, $m_2(t)$, using Itô's formula.\n3.  Solving the ODE for $m_2(t)$ to find its closed-form expression.\n\n**Part 1: Derivation of $m_1(t)$**\nWe start with the SDE in its integral form:\n$$\nX_t - X_0 = \\int_0^t (a X_s + c) ds + \\int_0^t (b X_s + d) dW_s\n$$\nTaking the expectation of both sides, and noting that $X_0 = x_0$ is a constant:\n$$\n\\mathbb{E}[X_t] - \\mathbb{E}[x_0] = \\mathbb{E}\\left[\\int_0^t (a X_s + c) ds\\right] + \\mathbb{E}\\left[\\int_0^t (b X_s + d) dW_s\\right]\n$$\nUsing the linearity of expectation and Fubini's theorem, we can swap expectation and the time integral. The expectation of the Itô integral term is zero under standard assumptions on the process $X_t$ (specifically, that $\\mathbb{E}[\\int_0^t (b X_s + d)^2 ds] < \\infty$, which holds for the solution to this SDE).\n$$\nm_1(t) - x_0 = \\int_0^t (a \\mathbb{E}[X_s] + c) ds + 0\n$$\n$$\nm_1(t) - x_0 = \\int_0^t (a m_1(s) + c) ds\n$$\nDifferentiating with respect to $t$ gives the ODE for $m_1(t)$:\n$$\n\\frac{dm_1(t)}{dt} = a m_1(t) + c\n$$\nThis is a first-order linear ODE with the initial condition $m_1(0) = \\mathbb{E}[X_0] = x_0$.\nThe solution to this ODE, given $a \\neq 0$, is:\n$$\nm_1(t) = \\left(x_0 - \\left(-\\frac{c}{a}\\right)\\right) e^{at} + \\left(-\\frac{c}{a}\\right) = \\left(x_0+\\frac{c}{a}\\right)e^{at} - \\frac{c}{a}\n$$\n\n**Part 2: Derivation of the ODE for $m_2(t)$**\nWe apply Itô's formula to the function $f(x) = x^2$. The derivatives are $f'(x) = 2x$ and $f''(x) = 2$.\n$$\nd(X_t^2) = f'(X_t) dX_t + \\frac{1}{2} f''(X_t) (dX_t)^2 = 2X_t dX_t + (dX_t)^2\n$$\nThe differential $dX_t$ is given by $dX_t = (aX_t+c)dt + (bX_t+d)dW_t$.\nThe quadratic variation term $(dX_t)^2$ is calculated using the rules $dt^2=0$, $dt dW_t=0$, and $dW_t^2=dt$:\n$$\n(dX_t)^2 = \\left((aX_t+c)dt + (bX_t+d)dW_t\\right)^2 = (bX_t+d)^2 dW_t^2 = (b^2X_t^2 + 2bdX_t + d^2)dt\n$$\nSubstituting these into the expression for $d(X_t^2)$:\n$$\nd(X_t^2) = 2X_t \\left( (aX_t+c)dt + (bX_t+d)dW_t \\right) + (b^2X_t^2 + 2bdX_t + d^2)dt\n$$\nGroup the $dt$ and $dW_t$ terms:\n$$\nd(X_t^2) = \\left(2aX_t^2 + 2cX_t + b^2X_t^2 + 2bdX_t + d^2\\right)dt + \\left(2bX_t^2 + 2dX_t\\right)dW_t\n$$\n$$\nd(X_t^2) = \\left((2a+b^2)X_t^2 + (2c+2bd)X_t + d^2\\right)dt + \\left(2(bX_t+d)X_t\\right)dW_t\n$$\nTaking the expectation of the integral form of this equation, and again using the zero-mean property of the Itô integral:\n$$\n\\mathbb{E}[X_t^2] - \\mathbb{E}[X_0^2] = \\int_0^t \\mathbb{E}\\left[(2a+b^2)X_s^2 + (2c+2bd)X_s + d^2\\right]ds\n$$\n$$\nm_2(t) - x_0^2 = \\int_0^t \\left( (2a+b^2)m_2(s) + (2c+2bd)m_1(s) + d^2 \\right)ds\n$$\nDifferentiating with respect to $t$ gives the ODE for $m_2(t)$:\n$$\n\\frac{dm_2(t)}{dt} = (2a+b^2)m_2(t) + (2c+2bd)m_1(t) + d^2\n$$\n\n**Part 3: Solving the ODE for $m_2(t)$**\nThe ODE for $m_2(t)$ is a first-order linear non-homogeneous ODE:\n$$\n\\frac{dm_2}{dt} - (2a+b^2)m_2(t) = (2c+2bd)m_1(t) + d^2\n$$\nSubstitute the expression for $m_1(t)$:\n$$\n\\frac{dm_2}{dt} - (2a+b^2)m_2(t) = (2c+2bd)\\left(\\left(x_0+\\frac{c}{a}\\right)e^{at} - \\frac{c}{a}\\right) + d^2\n$$\n$$\n\\frac{dm_2}{dt} - (2a+b^2)m_2(t) = (2c+2bd)\\left(x_0+\\frac{c}{a}\\right)e^{at} + \\left(d^2 - \\frac{c}{a}(2c+2bd)\\right)\n$$\nThis ODE is of the form $y' - P y = Q_1 e^{at} + Q_0$, where $P=(2a+b^2)$, $Q_1=(2c+2bd)(x_0+c/a)$, and $Q_0=d^2 - \\frac{c}{a}(2c+2bd)$.\nThe general solution can be found using an integrating factor or the method of undetermined coefficients. The solution is the sum of the homogeneous solution $m_{2,h}(t) = C e^{(2a+b^2)t}$ and a particular solution of the form $m_{2,p}(t) = K_a e^{at} + K_c$.\nThe general solution is of the form:\n$$\nm_2(t) = C_A e^{(2a+b^2)t} + C_a e^{at} + C_0\n$$\nwhere $C_A, C_a, C_0$ are constants. By substituting into the ODE and matching coefficients, we find:\n$$\n(a - (2a+b^2))C_a = (2c+2bd)\\left(x_0+\\frac{c}{a}\\right) \\implies -(a+b^2)C_a = \\frac{(2c+2bd)(ax_0+c)}{a}\n$$\n$$\nC_a = -\\frac{2(c+bd)(ax_0+c)}{a(a+b^2)}\n$$\nAnd for the constant term:\n$$\n-(2a+b^2)C_0 = d^2 - \\frac{c}{a}(2c+2bd) = \\frac{ad^2 - 2c^2 - 2bcd}{a}\n$$\n$$\nC_0 = -\\frac{ad^2 - 2c^2 - 2bcd}{a(2a+b^2)} = \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)}\n$$\nThe constant $C_A$ is determined by the initial condition $m_2(0)=x_0^2$:\n$$\nm_2(0) = C_A + C_a + C_0 = x_0^2 \\implies C_A = x_0^2 - C_a - C_0\n$$\nSubstituting the expressions for $C_a$ and $C_0$:\n$$\nC_A = x_0^2 - \\left(-\\frac{2(c+bd)(ax_0+c)}{a(a+b^2)}\\right) - \\left(\\frac{2c^2+2bcd-ad^2}{a(2a+b^2)}\\right)\n$$\n$$\nC_A = x_0^2 + \\frac{2(c+bd)(ax_0+c)}{a(a+b^2)} - \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)}\n$$\nCombining these gives the final expression for $m_2(t)$:\n$$\nm_2(t) = \\left( x_0^2 + \\frac{2(c+bd)(ax_0+c)}{a(a+b^2)} - \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)} \\right) e^{(2a+b^2)t} - \\left( \\frac{2(c+bd)(ax_0+c)}{a(a+b^2)} \\right) e^{at} + \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)}\n$$\nThis is the closed-form analytic expression for $m_2(t)$ under the given conditions.", "answer": "$$\n\\boxed{\n\\left( x_0^2 + \\frac{2(c+bd)(ax_0+c)}{a(a+b^2)} - \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)} \\right) \\exp\\left((2a+b^2)t\\right) - \\frac{2(c+bd)(ax_0+c)}{a(a+b^2)} \\exp(at) + \\frac{2c^2+2bcd-ad^2}{a(2a+b^2)}\n}\n$$", "id": "3063970"}, {"introduction": "Many real-world systems, from engineering to biology, can be modeled as multi-dimensional processes that eventually settle into a statistical equilibrium. This exercise extends our analysis to vector-valued SDEs and introduces the concept of a stationary distribution. You will use the Lyapunov equation, a fundamental tool in control theory, to find the stationary covariance matrix of a system, providing a complete statistical picture of its long-term behavior. [@problem_id:3063929]", "problem": "Consider the two-dimensional stochastic differential equation (SDE) for a state vector $X_t \\in \\mathbb{R}^{2}$ driven by a standard two-dimensional Wiener process (Brownian motion) $W_t \\in \\mathbb{R}^{2}$:\n$$\ndX_t = A X_t\\,dt + G\\,dW_t,\n$$\nwhere\n$$\nA = \\begin{pmatrix} -3 & 2 \\\\ -1 & -4 \\end{pmatrix}, \\qquad G = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}.\n$$\nAssume the initial condition $X_0$ is independent of $W_t$, has finite second moment, and the matrix $A$ is stable in the sense that all its eigenvalues have strictly negative real parts. Starting from the foundational definitions of Itô stochastic calculus, the Itô integral representation of linear SDEs, and the product rule for Itô processes, derive the dynamical equation satisfied by the second-moment matrix $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$ and use the stability of $A$ to justify the existence of a unique stationary covariance matrix $P$.\n\nFor the given matrices $A$ and $G$, compute the stationary covariance matrix $P$ and then determine the stationary variance of the scalar projection $Y_t = u^{\\top} X_t$ with $u = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Express your final answer as an exact value. No rounding is required. The final answer must be a single real number.", "solution": "The solution proceeds in four steps. First, we derive the general differential equation for the second-moment matrix $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$. Second, we establish the existence of a unique stationary solution. Third, we compute this stationary solution for the given matrices. Fourth, we compute the required stationary variance.\n\nThe SDE is given by $dX_t = A X_t\\,dt + G\\,dW_t$. To find the dynamics of $P(t)$, we consider the differential of the matrix process $X_t X_t^{\\top}$. Using the matrix version of Itô's product rule, for a process $Z_t = X_t Y_t^{\\top}$, its differential is $dZ_t = dX_t Y_t^{\\top} + X_t dY_t^{\\top} + d\\langle X, Y \\rangle_t$. Here, we apply it with $Y_t = X_t$.\n$$\nd(X_t X_t^{\\top}) = dX_t X_t^{\\top} + X_t (dX_t)^{\\top} + d\\langle X_t, X_t \\rangle_t.\n$$\nSubstituting the SDE for $dX_t$:\n$$\ndX_t X_t^{\\top} = (A X_t\\,dt + G\\,dW_t) X_t^{\\top} = A X_t X_t^{\\top}\\,dt + G\\,dW_t X_t^{\\top}.\n$$\n$$\nX_t (dX_t)^{\\top} = X_t (A X_t\\,dt + G\\,dW_t)^{\\top} = X_t (X_t^{\\top} A^{\\top}\\,dt + dW_t^{\\top} G^{\\top}) = X_t X_t^{\\top} A^{\\top}\\,dt + X_t dW_t^{\\top} G^{\\top}.\n$$\nThe quadratic covariation term $d\\langle X_t, X_t \\rangle_t$ is given by the product of the diffusion terms:\n$$\nd\\langle X_t, X_t \\rangle_t = (G\\,dW_t)(G\\,dW_t)^{\\top} = G (dW_t dW_t^{\\top}) G^{\\top}.\n$$\nSince $W_t$ is a standard $2$-dimensional Wiener process, its components are independent standard Brownian motions, meaning $\\mathbb{E}[dW_i(t) dW_j(t)] = \\delta_{ij}\\,dt$. This implies that the matrix $dW_t dW_t^{\\top}$ behaves as $I\\,dt$, where $I$ is the identity matrix.\n$$\nd\\langle X_t, X_t \\rangle_t = G I G^{\\top}\\,dt = GG^{\\top}\\,dt.\n$$\nCombining these terms, the stochastic differential for $X_t X_t^{\\top}$ is:\n$$\nd(X_t X_t^{\\top}) = (A X_t X_t^{\\top} + X_t X_t^{\\top} A^{\\top} + GG^{\\top})\\,dt + G\\,dW_t X_t^{\\top} + X_t dW_t^{\\top} G^{\\top}.\n$$\nTo obtain the dynamic equation for $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$, we take the expectation of the integral form of the above equation. The expectation of the stochastic integral terms (the Itô integrals) is zero because the integrand is non-anticipating with respect to the increment $dW_t$.\n$$\n\\mathbb{E}\\left[\\int_0^t (G\\,dW_s X_s^{\\top} + X_s dW_s^{\\top} G^{\\top})\\right] = 0.\n$$\nTherefore, taking the expectation of the entire equation yields:\n$$\n\\mathbb{E}[d(X_t X_t^{\\top})] = \\mathbb{E}[(A X_t X_t^{\\top} + X_t X_t^{\\top} A^{\\top} + GG^{\\top})\\,dt].\n$$\nUsing the linearity of expectation and that $d\\mathbb{E}[\\cdot] = \\mathbb{E}[d\\cdot]$, we have:\n$$\nd\\mathbb{E}[X_t X_t^{\\top}] = (A \\mathbb{E}[X_t X_t^{\\top}] + \\mathbb{E}[X_t X_t^{\\top}] A^{\\top} + GG^{\\top})\\,dt.\n$$\nThis gives the differential Lyapunov equation for the second-moment matrix $P(t)$:\n$$\n\\frac{d}{dt}P(t) = A P(t) + P(t) A^{\\top} + GG^{\\top}.\n$$\nA stationary solution $P$ exists if $\\lim_{t\\to\\infty} P(t)$ exists. In the stationary state, the time derivative is zero, so $\\frac{d}{dt}P(t) = 0$. The stationary second-moment matrix $P$ must therefore satisfy the continuous-time algebraic Lyapunov equation:\n$$\nA P + P A^{\\top} + GG^{\\top} = 0.\n$$\nAccording to a fundamental theorem of linear system theory, if the matrix $A$ is stable (all its eigenvalues have strictly negative real parts), then for any symmetric positive semi-definite matrix $Q$, the Lyapunov equation $AX + XA^{\\top} = -Q$ has a unique, symmetric, positive definite solution $X$. In our case, $Q = GG^{\\top}$, which is symmetric and positive semi-definite (and in this case positive definite, as $G$ is invertible). Since we have established that $A$ is stable, a unique symmetric positive definite stationary covariance matrix $P$ exists.\n\nWe now compute this stationary matrix $P$.\nFirst, calculate $GG^{\\top}$:\n$$\nGG^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}.\n$$\nThe Lyapunov equation is $A P + P A^{\\top} = -GG^{\\top}$. Let $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$ due to symmetry.\n$$\n\\begin{pmatrix} -3 & 2 \\\\ -1 & -4 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} -3 & -1 \\\\ 2 & -4 \\end{pmatrix} = \\begin{pmatrix} -1 & -2 \\\\ -2 & -5 \\end{pmatrix}.\n$$\nThe sum $AP + PA^{\\top}$ is:\n$$\n\\begin{pmatrix} -3p_{11}+2p_{12} & -3p_{12}+2p_{22} \\\\ -p_{11}-4p_{12} & -p_{12}-4p_{22} \\end{pmatrix} + \\begin{pmatrix} -3p_{11}+2p_{12} & -p_{11}-4p_{12} \\\\ -3p_{12}+2p_{22} & -p_{12}-4p_{22} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -6p_{11}+4p_{12} & -p_{11}-7p_{12}+2p_{22} \\\\ -p_{11}-7p_{12}+2p_{22} & -2p_{12}-8p_{22} \\end{pmatrix}.\n$$\nEquating this matrix with $-GG^{\\top}$ yields a system of three linear equations:\n1.  $-6p_{11} + 4p_{12} = -1$\n2.  $-p_{11} - 7p_{12} + 2p_{22} = -2$\n3.  $-2p_{12} - 8p_{22} = -5$\n\nFrom equation (1), $p_{11} = \\frac{4p_{12}+1}{6} = \\frac{2}{3}p_{12} + \\frac{1}{6}$.\nFrom equation (3), $p_{22} = \\frac{5-2p_{12}}{8} = \\frac{5}{8}-\\frac{1}{4}p_{12}$.\nSubstituting these into equation (2):\n$$\n-\\left(\\frac{2}{3}p_{12} + \\frac{1}{6}\\right) - 7p_{12} + 2\\left(\\frac{5}{8}-\\frac{1}{4}p_{12}\\right) = -2\n$$\n$$\n-\\frac{2}{3}p_{12} - \\frac{1}{6} - 7p_{12} + \\frac{5}{4} - \\frac{1}{2}p_{12} = -2\n$$\n$$\n\\left(-\\frac{2}{3} - 7 - \\frac{1}{2}\\right)p_{12} = -2 + \\frac{1}{6} - \\frac{5}{4}\n$$\n$$\n\\left(-\\frac{4}{6} - \\frac{42}{6} - \\frac{3}{6}\\right)p_{12} = -\\frac{24}{12} + \\frac{2}{12} - \\frac{15}{12}\n$$\n$$\n-\\frac{49}{6}p_{12} = -\\frac{37}{12} \\implies p_{12} = \\frac{37}{12} \\cdot \\frac{6}{49} = \\frac{37}{98}.\n$$\nNow we find $p_{11}$ and $p_{22}$:\n$$\np_{11} = \\frac{2}{3}\\left(\\frac{37}{98}\\right) + \\frac{1}{6} = \\frac{37}{147} + \\frac{1}{6} = \\frac{74}{294} + \\frac{49}{294} = \\frac{123}{294} = \\frac{41}{98}.\n$$\n$$\np_{22} = \\frac{5}{8} - \\frac{1}{4}\\left(\\frac{37}{98}\\right) = \\frac{5}{8} - \\frac{37}{392} = \\frac{5 \\cdot 49}{392} - \\frac{37}{392} = \\frac{245-37}{392} = \\frac{208}{392} = \\frac{26}{49} = \\frac{52}{98}.\n$$\nThus, the stationary covariance matrix is $P = \\frac{1}{98}\\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix}$.\n\nFinally, we compute the stationary variance of $Y_t = u^{\\top} X_t$ with $u = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe mean of the stationary process $X_t$ is $\\mathbb{E}[X] = 0$ since $A$ is stable. Thus, $\\mathbb{E}[Y_t] = u^{\\top}\\mathbb{E}[X_t] = 0$. The variance of $Y_t$ is its second moment:\n$$\n\\text{Var}(Y_t) = \\mathbb{E}[Y_t^2] = \\mathbb{E}[(u^{\\top}X_t)^2] = \\mathbb{E}[u^{\\top}X_t X_t^{\\top}u] = u^{\\top}\\mathbb{E}[X_t X_t^{\\top}]u = u^{\\top}Pu.\n$$\nSubstituting the values of $u$ and $P$:\n$$\n\\text{Var}(Y) = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\left( \\frac{1}{98}\\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 41-37 & 37-52 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 4 & -15 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} (4(1) + (-15)(-1)) = \\frac{1}{98}(4+15) = \\frac{19}{98}.\n$$\nThe stationary variance of $Y_t$ is an exact value.", "answer": "$$\\boxed{\\frac{19}{98}}$$", "id": "3063929"}]}