{"hands_on_practices": [{"introduction": "Before tackling the complexities of stochastic processes, we must first master the engine that drives many proofs in this field: the deterministic Grönwall inequality. This exercise focuses on this foundational tool, which provides bounds for functions satisfying certain integral inequalities. By working through this problem, you will practice the fundamental technique of converting an integral inequality into a solvable first-order differential inequality, a skill that is indispensable when analyzing the moments of solutions to stochastic differential equations. [@problem_id:3077522]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ and an $(\\mathcal{F}_t)$-adapted process $(X_t)_{t\\ge 0}$ such that the function $u:[0,\\infty)\\to[0,\\infty)$ defined by $u(t)=\\mathbb{E}[|X_t|]$ is finite for all $t\\ge 0$ and satisfies the deterministic Volterra-type integral inequality\n$$\nu(t)\\le 3+\\int_{0}^{t}(1+s)\\,u(s)\\,ds,\\qquad t\\ge 0.\n$$\nThis type of inequality arises, for example, when bounding moments of solutions to a stochastic differential equation (SDE) under linear growth conditions. Using only foundational properties of absolutely continuous functions, the Fundamental Theorem of Calculus, and standard comparison arguments, derive an explicit bound for $u(t)$ as a function of $t$. Express your final bound as a single closed-form analytic expression in $t$ without any inequality symbols. No rounding is required.", "solution": "We are given the integral inequality for a non-negative function $u(t)$:\n$$\nu(t) \\le 3 + \\int_{0}^{t}(1+s)u(s)ds, \\quad t \\ge 0.\n$$\nThe function $u(t)$ is defined as $u(t) = \\mathbb{E}[|X_t|]$, which guarantees its non-negativity, i.e., $u(t) \\ge 0$ for all $t \\ge 0$.\n\nTo find an explicit bound for $u(t)$, we introduce an auxiliary function, $v(t)$, defined as the right-hand side of the given inequality:\n$$\nv(t) = 3 + \\int_{0}^{t}(1+s)u(s)ds.\n$$\nFrom this definition, it is immediately evident that $u(t) \\le v(t)$ for all $t \\ge 0$.\n\nSince $u(s) \\ge 0$ and $(1+s)  0$ for $s \\ge 0$, the integrand $(1+s)u(s)$ is non-negative. This implies that the integral term is a non-decreasing function of $t$. Consequently, $v(t)$ is non-decreasing.\nLet us evaluate $v(t)$ at $t=0$:\n$$\nv(0) = 3 + \\int_{0}^{0}(1+s)u(s)ds = 3+0 = 3.\n$$\nSince $v(t)$ is non-decreasing, we have $v(t) \\ge v(0) = 3$ for all $t \\ge 0$.\n\nThe function $u(t)$ is given to be finite for all $t$, which implies it is locally integrable on $[0, \\infty)$. Therefore, the function $v(t)$, being the sum of a constant and an integral of a locally integrable function, is an absolutely continuous function of $t$. According to the Fundamental Theorem of Calculus, which applies to absolutely continuous functions, the derivative of $v(t)$ exists for almost every $t \\ge 0$ and is given by:\n$$\nv'(t) = \\frac{d}{dt}\\left(3 + \\int_{0}^{t}(1+s)u(s)ds\\right) = (1+t)u(t) \\quad \\text{a.e.}\n$$\nWe now combine the relations we have established. Using $u(t) \\le v(t)$ and the fact that $(1+t)0$ for $t \\ge 0$, we can write:\n$$\nv'(t) = (1+t)u(t) \\le (1+t)v(t) \\quad \\text{a.e.}\n$$\nThis gives us a first-order linear differential inequality for $v(t)$:\n$$\nv'(t) - (1+t)v(t) \\le 0 \\quad \\text{a.e.}\n$$\nWe can solve this inequality using the method of integrating factors. The appropriate integrating factor, $I(t)$, is\n$$\nI(t) = \\exp\\left(-\\int_{0}^{t}(1+s)ds\\right).\n$$\nThe integral in the exponent evaluates to:\n$$\n\\int_{0}^{t}(1+s)ds = \\left[s + \\frac{s^2}{2}\\right]_{0}^{t} = t + \\frac{t^2}{2}.\n$$\nThus, the integrating factor is:\n$$\nI(t) = \\exp\\left(-\\left(t + \\frac{t^2}{2}\\right)\\right).\n$$\nNote that $I(t)  0$ for all $t$. Multiplying the differential inequality by $I(t)$ does not alter its direction:\n$$\nI(t)v'(t) - (1+t)I(t)v(t) \\le 0 \\quad \\text{a.e.}\n$$\nThe left-hand side of this inequality is, by the product rule for differentiation (which is valid for the product of an absolutely continuous function and a $C^1$ function), the derivative of the product $I(t)v(t)$:\n$$\n\\frac{d}{dt}\\left( I(t)v(t) \\right) \\le 0 \\quad \\text{a.e.}\n$$\nAn absolutely continuous function whose derivative is non-positive almost everywhere on an interval is a non-increasing function on that interval. Therefore, for any $t \\ge 0$:\n$$\nI(t)v(t) \\le I(0)v(0).\n$$\nWe previously found $v(0)=3$, and $I(0) = \\exp(0) = 1$. Substituting these values, we get:\n$$\nI(t)v(t) \\le 3.\n$$\nSubstituting back the expression for $I(t)$:\n$$\n\\exp\\left(-\\left(t + \\frac{t^2}{2}\\right)\\right) v(t) \\le 3.\n$$\nTo isolate $v(t)$, we multiply by the inverse of the exponential term, which is $\\exp\\left(t + \\frac{t^2}{2}\\right)$ and is always positive:\n$$\nv(t) \\le 3 \\exp\\left(t + \\frac{t^2}{2}\\right).\n$$\nFinally, since we established that $u(t) \\le v(t)$, we obtain the explicit upper bound for $u(t)$:\n$$\nu(t) \\le 3 \\exp\\left(t + \\frac{t^2}{2}\\right).\n$$\nThe expression on the right-hand side is the closed-form analytic bound requested in the problem statement.", "answer": "$$\\boxed{3 \\exp\\left(t + \\frac{t^2}{2}\\right)}$$", "id": "3077522"}, {"introduction": "Now we venture into the stochastic world, where processes are influenced by random events. This problem introduces a system driven by random jumps, modeled by a Poisson random measure—a common scenario in areas like finance for modeling market shocks or in biology for population dynamics. The key skill you will develop here is reducing a complex stochastic inequality into a manageable deterministic one for its expectation. This is achieved by taking the expected value and applying the compensation formula for Poisson integrals, which then allows you to use the classical Grönwall method you practiced earlier. [@problem_id:3077518]", "problem": "Consider a filtered probability space carrying a Poisson random measure with jumps (Poisson Random Measure (PRM)) $N(\\mathrm{d}s,\\mathrm{d}z)$ on $[0,\\infty) \\times [0,1]$ with intensity measure $\\lambda\\,\\mathrm{d}s\\,\\nu(\\mathrm{d}z)$, where $\\lambda = 3$ and $\\nu$ is the uniform probability measure on $[0,1]$. Let $(X_t)_{t \\ge 0}$ be a nonnegative, adapted, right-continuous with left limits (càdlàg) process that satisfies, for all $t \\ge 0$, the integral inequality\n$$\nX_t \\le x_0 + \\int_0^t \\big(\\alpha + \\beta X_{s-}\\big)\\,\\mathrm{d}s \\;+\\; \\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,N(\\mathrm{d}s,\\mathrm{d}z),\n$$\nwhere $x_0 = 2$, $\\alpha = 1$, and $\\beta = 2$. This describes a linear growth bound driven by compound Poisson noise with jump sizes $z \\in [0,1]$.\n\nUsing only foundational properties of Poisson Random Measures and deterministic integral inequalities, derive a closed-form analytic upper bound for the expectation $\\mathbb{E}[X_t]$ as an explicit function of $t$. Your final answer must be a single analytic expression and must not contain an inequality sign. No rounding is required.", "solution": "The problem requires the derivation of a closed-form analytic upper bound for the expectation $\\mathbb{E}[X_t]$ of a nonnegative, adapted, càdlàg process $(X_t)_{t \\ge 0}$ satisfying the stochastic integral inequality:\n$$\nX_t \\le x_0 + \\int_0^t \\big(\\alpha + \\beta X_{s-}\\big)\\,\\mathrm{d}s \\;+\\; \\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,N(\\mathrm{d}s,\\mathrm{d}z)\n$$\nThe provided constants are $x_0 = 2$, $\\alpha = 1$, $\\beta = 2$, $\\lambda = 3$. The measure $\\nu$ is the uniform probability measure on $[0,1]$.\n\nLet $u(t) = \\mathbb{E}[X_t]$. Our goal is to find an upper bound for $u(t)$. We begin by taking the expectation of both sides of the inequality. By the linearity of expectation, we have:\n$$\n\\mathbb{E}[X_t] \\le \\mathbb{E}\\left[x_0 + \\int_0^t \\big(\\alpha + \\beta X_{s-}\\big)\\,\\mathrm{d}s \\;+\\; \\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,N(\\mathrm{d}s,\\mathrm{d}z)\\right]\n$$\n$$\nu(t) \\le x_0 + \\mathbb{E}\\left[\\int_0^t \\big(\\alpha + \\beta X_{s-}\\big)\\,\\mathrm{d}s\\right] \\;+\\; \\mathbb{E}\\left[\\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,N(\\mathrm{d}s,\\mathrm{d}z)\\right]\n$$\nsince $x_0$ is a deterministic constant.\n\nWe evaluate the expectation of each integral term separately.\n\nFor the first integral, which is a standard Lebesgue integral, we can interchange the expectation and integration operators. Since the process $X_t$ is nonnegative and $\\alpha, \\beta$ are positive constants, the integrand is nonnegative. Thus, by the Fubini-Tonelli theorem:\n$$\n\\mathbb{E}\\left[\\int_0^t \\big(\\alpha + \\beta X_{s-}\\big)\\,\\mathrm{d}s\\right] = \\int_0^t \\mathbb{E}\\big[\\alpha + \\beta X_{s-}\\big]\\,\\mathrm{d}s = \\int_0^t \\big(\\alpha + \\beta \\mathbb{E}[X_{s-}]\\big)\\,\\mathrm{d}s\n$$\nFor any fixed time $s$, the event $\\{X_s \\neq X_{s-}\\}$ corresponds to a jump of the Poisson process at exactly time $s$. The probability of a jump at any specific time is zero. Therefore, $X_s = X_{s-}$ almost surely for any fixed $s$, which implies $\\mathbb{E}[X_s] = \\mathbb{E}[X_{s-}]$. Let's denote $\\mathbb{E}[X_s]$ by $u(s)$. So, the first term becomes:\n$$\n\\int_0^t \\big(\\alpha + \\beta u(s)\\big)\\,\\mathrm{d}s\n$$\n\nFor the second integral, which is a stochastic integral with respect to the Poisson random measure $N$, we use a fundamental property of such integrals. For a predictable process integrand $H(s,z,\\omega)$, the expectation of the stochastic integral is given by the integral with respect to the intensity measure:\n$$\n\\mathbb{E}\\left[\\int_0^t \\int_{E} H(s,z)\\,N(\\mathrm{d}s,\\mathrm{d}z)\\right] = \\mathbb{E}\\left[\\int_0^t \\int_{E} H(s,z)\\,\\lambda\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}s\\right]\n$$\nIn our case, the integrand is $H(s,z) = z\\,X_{s-}$. The process $X_{s-}$ is left-continuous and adapted, hence it is predictable. Applying this property:\n$$\n\\mathbb{E}\\left[\\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,N(\\mathrm{d}s,\\mathrm{d}z)\\right] = \\mathbb{E}\\left[\\int_0^t \\int_{0}^{1} z\\,X_{s-}\\,\\lambda\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}s\\right]\n$$\nAgain, by the Fubini-Tonelli theorem, we can swap the expectation and the outer time integral:\n$$\n\\int_0^t \\mathbb{E}\\left[X_{s-} \\int_{0}^{1} z\\,\\lambda\\,\\nu(\\mathrm{d}z)\\right]\\,\\mathrm{d}s\n$$\nThe inner integral over the jump-size space $[0,1]$ is deterministic. Since $\\nu$ is the uniform probability measure on $[0,1]$, its density is $1$ on this interval.\n$$\n\\int_{0}^{1} z\\,\\lambda\\,\\nu(\\mathrm{d}z) = \\lambda \\int_0^1 z \\cdot 1 \\,\\mathrm{d}z = \\lambda \\left[\\frac{z^2}{2}\\right]_0^1 = \\frac{\\lambda}{2}\n$$\nSubstituting this back, the second term becomes:\n$$\n\\int_0^t \\mathbb{E}\\left[X_{s-} \\cdot \\frac{\\lambda}{2}\\right]\\,\\mathrm{d}s = \\int_0^t \\frac{\\lambda}{2} \\mathbb{E}[X_{s-}]\\,\\mathrm{d}s = \\int_0^t \\frac{\\lambda}{2} u(s)\\,\\mathrm{d}s\n$$\n\nCombining these results, we obtain a deterministic integral inequality for $u(t) = \\mathbb{E}[X_t]$:\n$$\nu(t) \\le x_0 + \\int_0^t \\big(\\alpha + \\beta u(s)\\big)\\,\\mathrm{d}s + \\int_0^t \\frac{\\lambda}{2} u(s)\\,\\mathrm{d}s\n$$\n$$\nu(t) \\le x_0 + \\int_0^t \\alpha \\,\\mathrm{d}s + \\int_0^t \\left(\\beta + \\frac{\\lambda}{2}\\right) u(s)\\,\\mathrm{d}s\n$$\nThis is an integral inequality of the Grönwall type. Let $C = \\beta + \\frac{\\lambda}{2}$. The inequality is:\n$$\nu(t) \\le x_0 + \\alpha t + C \\int_0^t u(s)\\,\\mathrm{d}s\n$$\nLet $v(t)$ be the right-hand side, $v(t) = x_0 + \\alpha t + C \\int_0^t u(s)\\,\\mathrm{d}s$. Then $u(t) \\le v(t)$. Also, $v(0) = x_0$ and $v(t)$ is differentiable with respect to $t$:\n$$\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} = \\alpha + C u(t)\n$$\nSince $u(t) \\le v(t)$ and $C  0$, we have $\\alpha + C u(t) \\le \\alpha + C v(t)$. This gives a differential inequality for $v(t)$:\n$$\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} \\le \\alpha + C v(t)\n$$\nRearranging gives:\n$$\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} - C v(t) \\le \\alpha\n$$\nWe multiply by the integrating factor $e^{-Ct}$:\n$$\ne^{-Ct}\\frac{\\mathrm{d}v}{\\mathrm{d}t} - C e^{-Ct}v(t) \\le \\alpha e^{-Ct}\n$$\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(v(t)e^{-Ct}\\right) \\le \\alpha e^{-Ct}\n$$\nIntegrating from $0$ to $t$:\n$$\n\\int_0^t \\frac{\\mathrm{d}}{\\mathrm{d}s}\\left(v(s)e^{-Cs}\\right)\\,\\mathrm{d}s \\le \\int_0^t \\alpha e^{-Cs}\\,\\mathrm{d}s\n$$\n$$\nv(t)e^{-Ct} - v(0)e^0 \\le \\alpha \\left[-\\frac{1}{C}e^{-Cs}\\right]_0^t\n$$\n$$\nv(t)e^{-Ct} - x_0 \\le -\\frac{\\alpha}{C}(e^{-Ct} - 1)\n$$\n$$\nv(t)e^{-Ct} \\le x_0 - \\frac{\\alpha}{C}e^{-Ct} + \\frac{\\alpha}{C}\n$$\nMultiplying by $e^{Ct}$:\n$$\nv(t) \\le x_0 e^{Ct} - \\frac{\\alpha}{C} + \\frac{\\alpha}{C} e^{Ct}\n$$\n$$\nv(t) \\le \\left(x_0 + \\frac{\\alpha}{C}\\right)e^{Ct} - \\frac{\\alpha}{C}\n$$\nSince $u(t) \\le v(t)$, we have found an upper bound for $u(t)$:\n$$\nu(t) = \\mathbb{E}[X_t] \\le \\left(x_0 + \\frac{\\alpha}{C}\\right)e^{Ct} - \\frac{\\alpha}{C}\n$$\nNow, we substitute the given values:\n$x_0 = 2$, $\\alpha = 1$, $\\beta = 2$, $\\lambda = 3$.\nThe constant $C$ is:\n$$\nC = \\beta + \\frac{\\lambda}{2} = 2 + \\frac{3}{2} = \\frac{7}{2}\n$$\nThe term $\\frac{\\alpha}{C}$ is:\n$$\n\\frac{\\alpha}{C} = \\frac{1}{7/2} = \\frac{2}{7}\n$$\nThe term $x_0 + \\frac{\\alpha}{C}$ is:\n$$\nx_0 + \\frac{\\alpha}{C} = 2 + \\frac{2}{7} = \\frac{14}{7} + \\frac{2}{7} = \\frac{16}{7}\n$$\nSubstituting these values back into the inequality for $u(t)$:\n$$\n\\mathbb{E}[X_t] \\le \\frac{16}{7} \\exp\\left(\\frac{7}{2}t\\right) - \\frac{2}{7}\n$$\nThis expression provides the required closed-form analytic upper bound for $\\mathbb{E}[X_t]$.", "answer": "$$\\boxed{\\frac{16}{7} \\exp\\left(\\frac{7}{2}t\\right) - \\frac{2}{7}}$$", "id": "3077518"}, {"introduction": "This final practice explores a more subtle but powerful application of Grönwall's inequality where the coefficients of the inequality are themselves random processes. Instead of taking expectations from the start, this problem guides you through a two-step approach: first, apply the deterministic Grönwall bound on a path-by-path basis, and second, take the expectation of the resulting bound. This technique is particularly effective when you can leverage statistical properties like independence to simplify the final expression, as demonstrated in this exercise. [@problem_id:3077548]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P})$ and let $(X_{t})_{t\\in[0,T]}$, $(A_{t})_{t\\in[0,T]}$, and $(B_{t})_{t\\in[0,T]}$ be nonnegative, $(\\mathcal{F}_{t})$-adapted processes such that, for each fixed $t\\in[0,T]$, the following pathwise integral inequality holds almost surely:\n$$\nX_{t} \\le A_{t} + \\int_{0}^{t} B_{s}\\,X_{s}\\,ds.\n$$\nAssume that $A_{t}$ is almost surely nondecreasing in $t$, and that for each $t\\in[0,T]$, $A_{t}$ is independent of the $\\sigma$-algebra $\\sigma(B_{s}:0\\le s\\le t)$. Starting only from the deterministic Grönwall inequality and the definitions of independence and conditional expectation, derive an upper bound for $\\mathbb{E}[X_{t}]$ that factors into the product of two expectations, one involving $A_{t}$ and the other involving $B_{s}$.\n\nNext, compute this bound explicitly in the following simple example. Let $A_{t}\\equiv \\xi$ be a time-constant random variable with the exponential distribution of rate $\\alpha0$, and let $B_{s}\\equiv Y$ be a time-constant process with $Y$ exponentially distributed with rate $\\lambda0$. Assume that $\\xi$ and $Y$ are independent and that the fixed time $t$ satisfies $0t\\lambda$, so all required expectations are finite. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\lambda$, and $t$. No rounding is required, and no physical units are involved.", "solution": "### Derivation of the General Bound\n\nThe starting point is the given integral inequality, which holds almost surely for each sample path:\n$$\nX_{t} \\le A_{t} + \\int_{0}^{t} B_{s}\\,X_{s}\\,ds\n$$\nLet us fix a sample path $\\omega \\in \\Omega$. For this path, $X_t(\\omega)$, $A_t(\\omega)$, and $B_s(\\omega)$ are deterministic, nonnegative functions of time. Let's denote them by $x(t)$, $a(t)$, and $b(s)$ respectively. The inequality is:\n$$\nx(t) \\le a(t) + \\int_{0}^{t} b(s)\\,x(s)\\,ds\n$$\nThe standard integral form of the deterministic Grönwall inequality states that if a function $u(t)$ satisfies $u(t) \\le \\phi(t) + \\int_0^t \\psi(s)u(s)ds$ for nonnegative functions $\\psi$, then $u(t) \\le \\phi(t) + \\int_0^t \\phi(s)\\psi(s)\\exp\\left(\\int_s^t \\psi(r)dr\\right)ds$. Applying this to our pathwise inequality with $u(t)=x(t)$, $\\phi(t)=a(t)$, and $\\psi(s)=b(s)$, we get:\n$$\nx(t) \\le a(t) + \\int_{0}^{t} a(s)b(s)\\exp\\left(\\int_{s}^{t} b(r)\\,dr\\right)ds\n$$\nThe problem states that $A_t$ is almost surely nondecreasing. This means for our deterministic path, $a(t)$ is a nondecreasing function of $t$. Thus, for any $s \\in [0, t]$, we have $a(s) \\le a(t)$. Since all terms in the integral are nonnegative ($a(s)\\ge0$, $b(s)\\ge0$), we can bound the integral:\n$$\n\\int_{0}^{t} a(s)b(s)\\exp\\left(\\int_{s}^{t} b(r)\\,dr\\right)ds \\le \\int_{0}^{t} a(t)b(s)\\exp\\left(\\int_{s}^{t} b(r)\\,dr\\right)ds = a(t) \\int_{0}^{t} b(s)\\exp\\left(\\int_{s}^{t} b(r)\\,dr\\right)ds\n$$\nLet's evaluate the integral $I = \\int_{0}^{t} b(s)\\exp\\left(\\int_{s}^{t} b(r)\\,dr\\right)ds$. Let $u(s) = \\int_{s}^{t} b(r)\\,dr$. Then $\\frac{du}{ds} = -b(s)$, so $b(s)ds = -du$. The limits of integration for $u$ are $u(0) = \\int_{0}^{t} b(r)\\,dr$ and $u(t) = \\int_{t}^{t} b(r)\\,dr = 0$.\n$$\nI = \\int_{u(0)}^{u(t)} \\exp(u) (-du) = \\int_0^{u(0)} \\exp(u)du = [\\exp(u)]_{0}^{u(0)} = \\exp(u(0)) - \\exp(0) = \\exp\\left(\\int_{0}^{t} b(r)\\,dr\\right) - 1\n$$\nSubstituting this back, we get:\n$$\nx(t) \\le a(t) + a(t) \\left(\\exp\\left(\\int_{0}^{t} b(r)\\,dr\\right) - 1\\right) = a(t) \\exp\\left(\\int_{0}^{t} b(r)\\,dr\\right)\n$$\nReverting to the stochastic process notation, this inequality holds almost surely:\n$$\nX_{t} \\le A_{t} \\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)\n$$\nSince both sides are nonnegative random variables, we can take the expectation:\n$$\n\\mathbb{E}[X_{t}] \\le \\mathbb{E}\\left[A_{t} \\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)\\right]\n$$\nThe problem states that for each $t \\in [0, T]$, the random variable $A_t$ is independent of the $\\sigma$-algebra $\\sigma(B_s : 0 \\le s \\le t)$. The random variable $V_t = \\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)$ is a function of the path $\\{B_s\\}_{s \\in [0,t]}$ and is therefore measurable with respect to $\\sigma(B_s : 0 \\le s \\le t)$. Consequently, $A_t$ and $V_t$ are independent random variables. For independent random variables, the expectation of their product is the product of their expectations.\n$$\n\\mathbb{E}[A_{t} V_t] = \\mathbb{E}[A_{t}] \\mathbb{E}[V_t] = \\mathbb{E}[A_{t}] \\mathbb{E}\\left[\\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)\\right]\n$$\nThus, we arrive at the desired upper bound for $\\mathbb{E}[X_t]$:\n$$\n\\mathbb{E}[X_{t}] \\le \\mathbb{E}[A_{t}] \\mathbb{E}\\left[\\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)\\right]\n$$\n\n### Explicit Computation for the Example\n\nWe are given $A_{t} \\equiv \\xi \\sim \\text{Exp}(\\alpha)$ and $B_{s} \\equiv Y \\sim \\text{Exp}(\\lambda)$, where $\\xi$ and $Y$ are independent. The fixed time $t$ satisfies $0  t  \\lambda$.\n\nFirst, we compute $\\mathbb{E}[A_{t}]$:\n$$\n\\mathbb{E}[A_{t}] = \\mathbb{E}[\\xi]\n$$\nThe expected value of an exponential random variable with rate $\\alpha$ is $1/\\alpha$.\n$$\n\\mathbb{E}[A_{t}] = \\frac{1}{\\alpha}\n$$\nNote that $A_t = \\xi$ (constant in time) is trivially a nondecreasing process. Also, the independence of $\\xi$ and $Y$ implies that $A_t = \\xi$ is independent of $\\sigma(B_s: 0 \\le s \\le t) = \\sigma(Y)$, so the condition for factoring the expectation is satisfied.\n\nNext, we compute the second expectation factor:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\int_{0}^{t} B_{s}\\,ds\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(\\int_{0}^{t} Y\\,ds\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(Y \\int_{0}^{t} ds\\right)\\right] = \\mathbb{E}[\\exp(Yt)]\n$$\nThis is the moment generating function (MGF) of the random variable $Y$, evaluated at $t$. Let $M_Y(k) = \\mathbb{E}[\\exp(kY)]$ be the MGF of $Y$.\nThe probability density function of $Y \\sim \\text{Exp}(\\lambda)$ is $f_Y(y) = \\lambda \\exp(-\\lambda y)$ for $y \\ge 0$.\nThe MGF is calculated as:\n$$\nM_Y(k) = \\int_{0}^{\\infty} \\exp(ky) f_Y(y)\\,dy = \\int_{0}^{\\infty} \\exp(ky) \\lambda \\exp(-\\lambda y)\\,dy = \\lambda \\int_{0}^{\\infty} \\exp((k-\\lambda)y)\\,dy\n$$\nThis integral converges for $k - \\lambda  0$, i.e., $k  \\lambda$. The problem states that $0  t  \\lambda$, so we can evaluate the MGF at $k=t$.\n$$\nM_Y(k) = \\lambda \\left[ \\frac{\\exp((k-\\lambda)y)}{k-\\lambda} \\right]_{y=0}^{y=\\infty} = \\lambda \\left( 0 - \\frac{1}{k-\\lambda} \\right) = \\frac{\\lambda}{\\lambda-k}\n$$\nEvaluating at $k=t$:\n$$\n\\mathbb{E}[\\exp(Yt)] = M_Y(t) = \\frac{\\lambda}{\\lambda - t}\n$$\nFinally, we multiply the two expectations to obtain the upper bound for $\\mathbb{E}[X_t]$:\n$$\n\\mathbb{E}[X_{t}] \\le \\left(\\frac{1}{\\alpha}\\right) \\left(\\frac{\\lambda}{\\lambda - t}\\right) = \\frac{\\lambda}{\\alpha(\\lambda - t)}\n$$\nThe problem asks for this bound.", "answer": "$$\\boxed{\\frac{\\lambda}{\\alpha(\\lambda - t)}}$$", "id": "3077548"}]}