{"hands_on_practices": [{"introduction": "Multidimensional Brownian motion can be projected onto different directions to create one-dimensional processes, a common technique in financial modeling and physics. This exercise explores the fundamental link between the geometric relationship of these projection vectors (their angle) and the statistical relationship (correlation) of the resulting processes. Understanding this connection by deriving the joint distribution is crucial for dissecting the behavior of complex systems modeled by multidimensional diffusions [@problem_id:3067398].", "problem": "Let $d \\geq 2$ and let $\\{W_{t}\\}_{t \\geq 0}$ be a $d$-dimensional standard Brownian motion, meaning $W_{0}=\\mathbf{0}$ almost surely, increments are independent and stationary, and for each $t>0$ the random vector $W_{t}$ is centered Gaussian with covariance matrix $\\mathbb{E}[W_{t}W_{t}^{\\top}]=t I_{d}$, where $I_{d}$ is the $d \\times d$ identity matrix. Fix a time $t>0$ and fix two nonzero, linearly independent, non-orthogonal vectors $u, v \\in \\mathbb{R}^{d}$, with $u^{\\top}v \\neq 0$. Define the scalar random variables $X := u^{\\top}W_{t}$ and $Y := v^{\\top}W_{t}$.\n\nStarting from the defining properties of $d$-dimensional Brownian motion and standard facts about linear transformations of Gaussian random vectors, derive the joint law of $(X,Y)$. In particular, compute the joint probability density function (PDF) of $(X,Y)$ in closed form. Express your final result in terms of the quantities $a := u^{\\top}u$, $b := v^{\\top}v$, $c := u^{\\top}v$, and the time $t>0$.\n\nAdditionally, explain how the covariance $\\mathbb{E}[XY]=t\\,u^{\\top}v$ determines the dependence or independence of $X$ and $Y$, and characterize the sign of their correlation in terms of $u^{\\top}v$.\n\nYour final answer must be a single closed-form analytic expression for the joint PDF of $(X,Y)$ in terms of $a$, $b$, $c$, and $t$. No numerical approximation is required.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   $d \\geq 2$ is an integer.\n-   $\\{W_{t}\\}_{t \\geq 0}$ is a $d$-dimensional standard Brownian motion.\n-   $W_{0}=\\mathbf{0}$ almost surely.\n-   Increments are independent and stationary.\n-   For each $t>0$, the random vector $W_{t}$ is centered Gaussian.\n-   The covariance matrix of $W_t$ is $\\mathbb{E}[W_{t}W_{t}^{\\top}]=t I_{d}$, where $I_{d}$ is the $d \\times d$ identity matrix.\n-   A fixed time $t>0$ is considered.\n-   Two nonzero, linearly independent, non-orthogonal vectors $u, v \\in \\mathbb{R}^{d}$ are fixed.\n-   $u^{\\top}v \\neq 0$.\n-   Scalar random variables are defined as $X := u^{\\top}W_{t}$ and $Y := v^{\\top}W_{t}$.\n-   Constants are defined as $a := u^{\\top}u$, $b := v^{\\top}v$, and $c := u^{\\top}v$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of stochastic processes, specifically the properties of multidimensional Brownian motion and multivariate Gaussian distributions. These are standard and well-established mathematical concepts.\n-   **Well-Posed:** The problem provides all necessary information to determine the joint distribution of the random variables $X$ and $Y$. The conditions on the vectors $u$ and $v$ (nonzero, linearly independent) are crucial. The linear independence ensures that the resulting bivariate Gaussian distribution is non-degenerate. A unique solution exists and can be derived methodically.\n-   **Objective:** The problem is stated using precise, unambiguous mathematical language. There are no subjective or opinion-based elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically grounded question in the field of stochastic processes. I will now proceed to derive the solution.\n\nThe core principle for this problem is that a linear transformation of a Gaussian random vector results in another Gaussian random vector. The random vector $W_{t}$ is, by definition, a $d$-dimensional Gaussian vector with mean $\\mathbf{0}$ and covariance matrix $tI_{d}$. We can write $W_{t} \\sim \\mathcal{N}(\\mathbf{0}, tI_{d})$.\n\nThe scalar random variables $X$ and $Y$ are defined as linear combinations of the components of $W_{t}$:\n$$X = u^{\\top}W_{t}$$\n$$Y = v^{\\top}W_{t}$$\nWe can express the pair $(X, Y)$ as a single $2$-dimensional random vector $Z = \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$. This vector is obtained by a linear transformation of $W_{t}$:\n$$Z = \\begin{pmatrix} X \\\\ Y \\end{pmatrix} = \\begin{pmatrix} u^{\\top} \\\\ v^{\\top} \\end{pmatrix} W_{t}$$\nLet the transformation matrix be $M = \\begin{pmatrix} u^{\\top} \\\\ v^{\\top} \\end{pmatrix}$. $M$ is a $2 \\times d$ matrix. The random vector $Z$ is thus given by $Z = MW_{t}$.\n\nSince $W_{t}$ is a Gaussian vector, $Z$ must also be a Gaussian vector. We need to determine its mean vector $\\mu_{Z}$ and its covariance matrix $\\Sigma_{Z}$.\n\nThe mean of $Z$ is given by:\n$$\\mu_{Z} = \\mathbb{E}[Z] = \\mathbb{E}[MW_{t}] = M\\mathbb{E}[W_{t}]$$\nGiven that $W_{t}$ is centered, $\\mathbb{E}[W_{t}] = \\mathbf{0}$. Therefore,\n$$\\mu_{Z} = M\\mathbf{0} = \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThis shows that both $X$ and $Y$ are zero-mean random variables, i.e., $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$.\n\nThe covariance matrix of $Z$ is given by:\n$$\\Sigma_{Z} = \\mathbb{E}[(Z-\\mu_{Z})(Z-\\mu_{Z})^{\\top}] = \\mathbb{E}[ZZ^{\\top}] = \\mathbb{E}[(MW_{t})(MW_{t})^{\\top}]$$\nUsing properties of the transpose, this becomes:\n$$\\Sigma_{Z} = \\mathbb{E}[M W_{t} W_{t}^{\\top} M^{\\top}] = M \\mathbb{E}[W_{t} W_{t}^{\\top}] M^{\\top}$$\nWe are given that $\\mathbb{E}[W_{t}W_{t}^{\\top}] = tI_{d}$. Substituting this into the equation:\n$$\\Sigma_{Z} = M (tI_{d}) M^{\\top} = t(MM^{\\top})$$\nNow we compute the matrix product $MM^{\\top}$:\n$$M^{\\top} = \\begin{pmatrix} u & v \\end{pmatrix}$$\n$$MM^{\\top} = \\begin{pmatrix} u^{\\top} \\\\ v^{\\top} \\end{pmatrix} \\begin{pmatrix} u & v \\end{pmatrix} = \\begin{pmatrix} u^{\\top}u & u^{\\top}v \\\\ v^{\\top}u & v^{\\top}v \\end{pmatrix}$$\nUsing the provided definitions $a := u^{\\top}u$, $b := v^{\\top}v$, and $c := u^{\\top}v$ (and noting $v^{\\top}u = u^{\\top}v = c$), we get:\n$$MM^{\\top} = \\begin{pmatrix} a & c \\\\ c & b \\end{pmatrix}$$\nTherefore, the covariance matrix of $Z = (X,Y)^{\\top}$ is:\n$$\\Sigma_{Z} = t \\begin{pmatrix} a & c \\\\ c & b \\end{pmatrix} = \\begin{pmatrix} ta & tc \\\\ tc & tb \\end{pmatrix}$$\nThe components of this matrix are $\\text{Var}(X) = ta$, $\\text{Var}(Y) = tb$, and $\\text{Cov}(X,Y) = tc$.\n\nThe joint law of $(X,Y)$ is a bivariate normal distribution with mean $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and covariance matrix $\\Sigma_{Z}$. The joint probability density function (PDF) for a bivariate normal vector $\\mathbf{z} = (x,y)^{\\top}$ with mean $\\mathbf{0}$ and covariance matrix $\\Sigma$ is given by:\n$$f(x,y) = \\frac{1}{2\\pi \\sqrt{\\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} \\mathbf{z}^{\\top} \\Sigma^{-1} \\mathbf{z}\\right)$$\nWe must compute the determinant and the inverse of $\\Sigma_{Z}$.\nThe determinant is:\n$$\\det(\\Sigma_{Z}) = (ta)(tb) - (tc)^2 = t^2ab - t^2c^2 = t^2(ab-c^2)$$\nBy the Cauchy-Schwarz inequality, $(u^{\\top}v)^2 \\leq (u^{\\top}u)(v^{\\top}v)$, which translates to $c^2 \\leq ab$. Since the vectors $u$ and $v$ are given to be linearly independent, the inequality is strict: $c^2 < ab$, which implies $ab - c^2 > 0$. As $t>0$, we have $\\det(\\Sigma_{Z}) > 0$, confirming the distribution is non-degenerate.\n\nThe inverse of $\\Sigma_{Z}$ is:\n$$\\Sigma_{Z}^{-1} = \\frac{1}{\\det(\\Sigma_{Z})} \\begin{pmatrix} tb & -tc \\\\ -tc & ta \\end{pmatrix} = \\frac{1}{t^2(ab-c^2)} \\begin{pmatrix} tb & -tc \\\\ -tc & ta \\end{pmatrix} = \\frac{1}{t(ab-c^2)} \\begin{pmatrix} b & -c \\\\ -c & a \\end{pmatrix}$$\nThe quadratic form in the exponent is $\\mathbf{z}^{\\top} \\Sigma_{Z}^{-1} \\mathbf{z}$ with $\\mathbf{z} = (x,y)^{\\top}$:\n$$\\begin{pmatrix} x & y \\end{pmatrix} \\frac{1}{t(ab-c^2)} \\begin{pmatrix} b & -c \\\\ -c & a \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\frac{1}{t(ab-c^2)} \\begin{pmatrix} x & y \\end{pmatrix} \\begin{pmatrix} bx - cy \\\\ -cx + ay \\end{pmatrix}$$\n$$= \\frac{1}{t(ab-c^2)} (x(bx-cy) + y(-cx+ay)) = \\frac{bx^2 - 2cxy + ay^2}{t(ab-c^2)}$$\nSubstituting all parts back into the PDF formula:\n$$f_{X,Y}(x,y) = \\frac{1}{2\\pi \\sqrt{t^2(ab-c^2)}} \\exp\\left( -\\frac{1}{2} \\frac{bx^2 - 2cxy + ay^2}{t(ab-c^2)} \\right)$$\nSimplifying the term in the denominator:\n$$f_{X,Y}(x,y) = \\frac{1}{2\\pi t \\sqrt{ab-c^2}} \\exp\\left( -\\frac{bx^2 - 2cxy + ay^2}{2t(ab-c^2)} \\right)$$\nThis is the closed-form expression for the joint PDF of $(X,Y)$.\n\nRegarding the second part of the question, the covariance between $X$ and $Y$ is the $(1,2)$-entry of the covariance matrix $\\Sigma_{Z}$, which we found to be $\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. Since $\\mathbb{E}[X]=\\mathbb{E}[Y]=0$, we have $\\text{Cov}(X,Y) = \\mathbb{E}[XY] = tc = t u^{\\top}v$.\nFor jointly Gaussian random variables, independence is equivalent to being uncorrelated. Thus, $X$ and $Y$ are independent if and only if $\\text{Cov}(X,Y) = 0$.\nGiven $t > 0$, this condition becomes $u^{\\top}v = 0$. This means $X$ and $Y$ are independent if and only if the vectors $u$ and $v$ are orthogonal. The problem states that $u$ and $v$ are non-orthogonal, so $u^{\\top}v \\neq 0$, which implies that $X$ and $Y$ are dependent.\n\nThe correlation coefficient $\\rho_{XY}$ is defined as:\n$$\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} = \\frac{tc}{\\sqrt{(ta)(tb)}} = \\frac{tc}{t\\sqrt{ab}} = \\frac{c}{\\sqrt{ab}}$$\nSince $a = \\|u\\|^2 > 0$ and $b = \\|v\\|^2 > 0$, the denominator $\\sqrt{ab}$ is a positive real number. Therefore, the sign of the correlation $\\rho_{XY}$ is determined entirely by the sign of $c = u^{\\top}v$.\n- If $u^{\\top}v > 0$ (the angle between $u$ and $v$ is acute), then $\\rho_{XY} > 0$, and $X$ and $Y$ are positively correlated.\n- If $u^{\\top}v < 0$ (the angle between $u$ and $v$ is obtuse), then $\\rho_{XY} < 0$, and $X$ and $Y$ are negatively correlated.\nThis completes the required analysis.", "answer": "$$\\boxed{\\frac{1}{2\\pi t \\sqrt{ab-c^2}} \\exp\\left(-\\frac{bx^2 - 2cxy + ay^2}{2t(ab-c^2)}\\right)}$$", "id": "3067398"}, {"introduction": "The Itô isometry is a cornerstone of stochastic calculus, relating the average squared size of an Itô integral to the average squared size of its integrand. This practice generalizes that idea to the covariance between two different Itô integrals, a result sometimes called the polarized Itô isometry. Deriving this provides a powerful tool for calculating the second-moment properties of stochastic systems and is essential for a deeper understanding of martingale theory [@problem_id:3067394].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_s)_{s\\geq 0},\\mathbb{P})$ satisfying the usual conditions, and let $W=(W^{1},\\dots,W^{d})$ be a $d$-dimensional standard Brownian motion, meaning each $W^{i}$ is a one-dimensional standard Brownian motion and the components are independent. Fix a time $t>0$. Let $H=(H_{s})_{0\\leq s\\leq t}$ and $G=(G_{s})_{0\\leq s\\leq t}$ be $\\mathbb{R}^{d}$-valued, $(\\mathcal{F}_{s})$-predictable processes that are square-integrable in the sense that\n$$\n\\mathbb{E}\\left[\\int_{0}^{t}\\|H_{s}\\|^{2}\\,ds\\right]<\\infty\n\\quad\\text{and}\\quad\n\\mathbb{E}\\left[\\int_{0}^{t}\\|G_{s}\\|^{2}\\,ds\\right]<\\infty,\n$$\nwhere $\\|\\cdot\\|$ denotes the Euclidean norm on $\\mathbb{R}^{d}$. Define the Itô integrals\n$$\nX_{t}=\\int_{0}^{t}H_{s}\\cdot dW_{s}\n\\quad\\text{and}\\quad\nY_{t}=\\int_{0}^{t}G_{s}\\cdot dW_{s},\n$$\nwhere $H_{s}\\cdot dW_{s}=\\sum_{i=1}^{d}H_{s}^{i}\\,dW_{s}^{i}$. Using only fundamental properties of continuous martingales and the Itô integral (specifically, that the Itô integral of a square-integrable predictable integrand is a martingale with zero mean, the independence of Brownian motion components, the definition of quadratic covariation, and the Itô isometry), derive a closed-form expression for the covariance $\\operatorname{Cov}(X_{t},Y_{t})$ in terms of $H$ and $G$. Express your final answer as a single symbolic mathematical expression. No numerical rounding is required in this problem.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem within the mathematical theory of stochastic processes. All necessary conditions and definitions are provided, and there are no contradictions or ambiguities.\n\nWe are asked to find the covariance of two Itô integrals, $X_{t} = \\int_{0}^{t} H_{s} \\cdot dW_{s}$ and $Y_{t} = \\int_{0}^{t} G_{s} \\cdot dW_{s}$. The definition of covariance between two random variables $A$ and $B$ is $\\operatorname{Cov}(A, B) = \\mathbb{E}[AB] - \\mathbb{E}[A]\\mathbb{E}[B]$.\n\nIn our case, $A = X_{t}$ and $B = Y_{t}$, so we have:\n$$\n\\operatorname{Cov}(X_{t}, Y_{t}) = \\mathbb{E}[X_{t}Y_{t}] - \\mathbb{E}[X_{t}]\\mathbb{E}[Y_{t}]\n$$\nThe problem states that the integrands $H=(H_{s})_{0\\leq s\\leq t}$ and $G=(G_{s})_{0\\leq s\\leq t}$ are $\\mathbb{R}^{d}$-valued, $(\\mathcal{F}_{s})$-predictable processes that satisfy the square-integrability conditions:\n$$\n\\mathbb{E}\\left[\\int_{0}^{t}\\|H_{s}\\|^{2}\\,ds\\right]<\\infty \\quad \\text{and} \\quad \\mathbb{E}\\left[\\int_{0}^{t}\\|G_{s}\\|^{2}\\,ds\\right]<\\infty\n$$\nA fundamental property of the Itô integral is that if the integrand is a predictable process satisfying this square-integrability condition, then the resulting stochastic process is a martingale with zero mean. Specifically, the processes $(X_{s})_{s\\geq 0}$ and $(Y_{s})_{s\\geq 0}$ are continuous martingales. Since $X_{0} = \\int_{0}^{0} H_{u} \\cdot dW_{u} = 0$ and $Y_{0} = \\int_{0}^{0} G_{u} \\cdot dW_{u} = 0$, the martingale property implies that for any $s \\geq 0$:\n$$\n\\mathbb{E}[X_{s}] = \\mathbb{E}[X_{0}] = 0 \\quad \\text{and} \\quad \\mathbb{E}[Y_{s}] = \\mathbb{E}[Y_{0}] = 0\n$$\nIn particular, for the fixed time $t > 0$, we have $\\mathbb{E}[X_{t}] = 0$ and $\\mathbb{E}[Y_{t}] = 0$. Substituting these into the covariance formula yields:\n$$\n\\operatorname{Cov}(X_{t}, Y_{t}) = \\mathbb{E}[X_{t}Y_{t}] - (0)(0) = \\mathbb{E}[X_{t}Y_{t}]\n$$\nOur task thus reduces to calculating the expectation of the product $X_{t}Y_{t}$. We can achieve this using the Itô isometry property, which relates the second moment of an Itô integral to the integral of the squared norm of its integrand. The Itô isometry for an $\\mathbb{R}^d$-valued, square-integrable, predictable process $\\Phi = (\\Phi_s)_{0 \\le s \\le t}$ is:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{t} \\Phi_{s} \\cdot dW_{s}\\right)^{2}\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\|\\Phi_{s}\\|^{2} \\,ds\\right]\n$$\nTo find an expression for $\\mathbb{E}[X_{t}Y_{t}]$, we use a polarization argument. Consider the sum of the two processes, $X_{t} + Y_{t}$. By the linearity of the Itô integral:\n$$\nX_{t} + Y_{t} = \\int_{0}^{t} H_{s} \\cdot dW_{s} + \\int_{0}^{t} G_{s} \\cdot dW_{s} = \\int_{0}^{t} (H_{s} + G_{s}) \\cdot dW_{s}\n$$\nThe sum of two predictable processes, $H_{s} + G_{s}$, is also predictable. Furthermore, its square-integrability is guaranteed by the triangle inequality and the given conditions on $H$ and $G$. We can therefore apply the Itô isometry to the process $X_{t} + Y_{t}$:\n$$\n\\mathbb{E}\\left[(X_{t} + Y_{t})^{2}\\right] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} (H_{s} + G_{s}) \\cdot dW_{s}\\right)^{2}\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s} + G_{s}\\|^{2} \\,ds\\right]\n$$\nLet us expand both sides of this equation.\nThe left-hand side (LHS) expands as:\n$$\n\\mathbb{E}\\left[(X_{t} + Y_{t})^{2}\\right] = \\mathbb{E}[X_{t}^{2} + 2X_{t}Y_{t} + Y_{t}^{2}]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\text{LHS} = \\mathbb{E}[X_{t}^{2}] + 2\\mathbb{E}[X_{t}Y_{t}] + \\mathbb{E}[Y_{t}^{2}]\n$$\nThe right-hand side (RHS) expansion involves the Euclidean norm property $\\|v\\|^{2} = v \\cdot v$:\n$$\n\\|H_{s} + G_{s}\\|^{2} = (H_{s} + G_{s}) \\cdot (H_{s} + G_{s}) = H_{s} \\cdot H_{s} + 2(H_{s} \\cdot G_{s}) + G_{s} \\cdot G_{s} = \\|H_{s}\\|^{2} + 2(H_{s} \\cdot G_{s}) + \\|G_{s}\\|^{2}\n$$\nSubstituting this into the RHS expectation gives:\n$$\n\\mathbb{E}\\left[\\int_{0}^{t} (\\|H_{s}\\|^{2} + 2(H_{s} \\cdot G_{s}) + \\|G_{s}\\|^{2}) \\,ds\\right]\n$$\nBy linearity of expectation and integration, this becomes:\n$$\n\\text{RHS} = \\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right] + 2\\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right] + \\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right]\n$$\nNow, we apply the Itô isometry directly to $X_{t}$ and $Y_{t}$:\n$$\n\\mathbb{E}[X_{t}^{2}] = \\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right]\n$$\n$$\n\\mathbb{E}[Y_{t}^{2}] = \\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right]\n$$\nEquating the expanded LHS and RHS:\n$$\n\\mathbb{E}[X_{t}^{2}] + 2\\mathbb{E}[X_{t}Y_{t}] + \\mathbb{E}[Y_{t}^{2}] = \\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right] + 2\\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right] + \\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right]\n$$\nSubstituting the isometry results for $\\mathbb{E}[X_{t}^{2}]$ and $\\mathbb{E}[Y_{t}^{2}]$ into the LHS:\n$$\n\\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right] + 2\\mathbb{E}[X_{t}Y_{t}] + \\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right] + 2\\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right] + \\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right]\n$$\nWe can cancel the terms $\\mathbb{E}\\left[\\int_{0}^{t} \\|H_{s}\\|^{2} \\,ds\\right]$ and $\\mathbb{E}\\left[\\int_{0}^{t} \\|G_{s}\\|^{2} \\,ds\\right]$ from both sides, which leaves:\n$$\n2\\mathbb{E}[X_{t}Y_{t}] = 2\\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right]\n$$\nDividing by $2$, we obtain:\n$$\n\\mathbb{E}[X_{t}Y_{t}] = \\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right]\n$$\nSince we established that $\\operatorname{Cov}(X_{t}, Y_{t}) = \\mathbb{E}[X_{t}Y_{t}]$, the final expression for the covariance is:\n$$\n\\operatorname{Cov}(X_{t}, Y_{t}) = \\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right]\n$$\nThis result is often referred to as the Itô isometry in its generalized (or polarized) form for covariation. The derivation relies only on the specified fundamental properties. The dot product $H_{s} \\cdot G_{s}$ is explicitly $\\sum_{i=1}^{d} H_{s}^{i} G_{s}^{i}$.", "answer": "$$\n\\boxed{\\mathbb{E}\\left[\\int_{0}^{t} H_{s} \\cdot G_{s} \\,ds\\right]}\n$$", "id": "3067394"}, {"introduction": "Just as the fundamental theorem of calculus connects a function's change to the integral of its derivative, Itô's formula provides the analogous rule for functions of a stochastic process. This exercise provides hands-on practice applying the multidimensional Itô's formula to calculate the expected change in a potential field for a particle undergoing Brownian motion. It is a classic application that demonstrates how a \"correction\" term, involving the Laplacian, creates an effective drift even when the underlying process has none [@problem_id:548895].", "problem": "In the study of stochastic processes, Itô's formula serves as a fundamental tool, acting as the chain rule for functions of a stochastic process. It provides a stochastic counterpart to the fundamental theorem of calculus, revealing how the evolution of a function depends not only on the first but also the second moments of the underlying process's increments.\n\n**Background:**\n\nAn $n$-dimensional standard Brownian motion $\\mathbf{B}_t = (B_{1,t}, \\dots, B_{n,t})$ is a stochastic process where each component $B_{i,t}$ is an independent one-dimensional standard Brownian motion. Key properties include:\n1.  $\\mathbf{B}_0 = \\mathbf{0}$.\n2.  For any $t, s \\geq 0$, the increment $\\mathbf{B}_{t+s} - \\mathbf{B}_t$ is independent of the process's history $\\{\\mathbf{B}_u\\}_{u \\le t}$.\n3.  Each component's increment $B_{i,t+s} - B_{i,t}$ follows a normal distribution $N(0, s)$.\n4.  The quadratic covariation between components is $d\\langle B_i, B_j \\rangle_t = \\delta_{ij} dt$, where $\\delta_{ij}$ is the Kronecker delta.\n\nFor a twice continuously differentiable function $\\phi: \\mathbb{R}^n \\to \\mathbb{R}$, Itô's formula for $\\phi(\\mathbf{B}_t)$ is given by:\n$$\nd\\phi(\\mathbf{B}_t) = \\nabla \\phi(\\mathbf{B}_t) \\cdot d\\mathbf{B}_t + \\frac{1}{2} \\Delta\\phi(\\mathbf{B}_t) dt\n$$\nwhere $\\nabla \\phi$ is the gradient of $\\phi$, and $\\Delta\\phi = \\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial x_i^2}$ is the Laplacian of $\\phi$.\n\n**Problem Statement:**\n\nConsider a point particle in an $n$-dimensional space whose position at time $t$ is described by a standard Brownian motion $\\mathbf{B}_t$, starting from the origin. The particle is influenced by a potential field $\\phi(\\mathbf{x}) = \\|\\mathbf{x}\\|^4$, where $\\|\\mathbf{x}\\|$ is the Euclidean norm.\n\nUsing Itô's formula, derive a closed-form expression for the expected total change in the potential energy of the particle from time $t=0$ to a final time $t=T$. This quantity is defined as $\\mathbb{E}[\\phi(\\mathbf{B}_T) - \\phi(\\mathbf{B}_0)]$. Your final answer should be in terms of the dimension $n$ and the final time $T$.", "solution": "We apply Itô’s formula to $\\phi(\\mathbf{B}_t)$ with $\\phi(\\mathbf{x})=(\\|\\mathbf{x}\\|^2)^2$.\n\n1. Itô’s formula:  \n$$\nd\\phi(\\mathbf{B}_t)=\\nabla\\phi(\\mathbf{B}_t)\\cdot d\\mathbf{B}_t+\\tfrac12\\,\\Delta\\phi(\\mathbf{B}_t)\\,dt.\n$$\n\n2. Compute gradient and Laplacian. Let $r^2=\\|\\mathbf{x}\\|^2$. Then\n$$\n\\frac{\\partial\\phi}{\\partial x_i}\n=\\frac{\\partial (r^4)}{\\partial x_i}\n=4x_i\\,r^2,\n$$\n$$\n\\frac{\\partial^2\\phi}{\\partial x_i^2}\n=\\frac{\\partial(4x_i\\,r^2)}{\\partial x_i}\n=4r^2+8x_i^2.\n$$\nHence\n$$\n\\Delta\\phi(\\mathbf{x})\n=\\sum_{i=1}^n\\bigl(4r^2+8x_i^2\\bigr)\n=4n\\,r^2+8r^2\n=4(n+2)\\,r^2.\n$$\n\n3. Integrate and take expectation. Since the martingale term has zero mean,\n$$\n\\mathbb{E}[\\phi(\\mathbf{B}_T)-\\phi(\\mathbf{B}_0)]\n=\\tfrac12\\int_0^T\\mathbb{E}[\\Delta\\phi(\\mathbf{B}_t)]\\,dt\n=2(n+2)\\int_0^T\\mathbb{E}[\\|\\mathbf{B}_t\\|^2]\\,dt.\n$$\nUsing $\\mathbb{E}[\\|\\mathbf{B}_t\\|^2]=nt$, we get\n$$\n\\mathbb{E}[\\phi(\\mathbf{B}_T)-\\phi(0)]\n=2(n+2)\\int_0^Tnt\\,dt\n=2n(n+2)\\,\\frac{T^2}{2}\n=n(n+2)T^2.\n$$", "answer": "$$\\boxed{n(n+2)T^2}$$", "id": "548895"}]}