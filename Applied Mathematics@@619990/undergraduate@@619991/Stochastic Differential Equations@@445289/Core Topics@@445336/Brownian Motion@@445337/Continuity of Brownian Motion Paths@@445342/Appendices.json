{"hands_on_practices": [{"introduction": "The concept of continuity for a stochastic process can be defined in several ways, and we begin by exploring one of the most fundamental: mean-square continuity. This exercise guides you through a direct calculation to show that the expected squared distance between the process at two points, $\\mathbb{E}[|B_t - B_s|^2]$, vanishes as the points get closer. This demonstrates that continuity is deeply embedded in the defining covariance structure of Brownian motion [@problem_id:3045665].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (SBM), that is, a mean-zero Gaussian process with covariance structure $\\mathbb{E}[B_{t} B_{s}] = \\min(t,s)$ for all $s,t \\geq 0$ and $B_{0} = 0$ almost surely. Using only these foundational properties and basic identities for variance and covariance of random variables, do the following:\n\n- Derive a closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ in terms of $s$ and $t$.\n- Based on your expression, determine whether $\\{B_{t}\\}_{t \\geq 0}$ is mean-square (MS) continuous at an arbitrary fixed time $t_{0} \\geq 0$, and justify your conclusion from first principles.\n\nReport only the closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ as your final answer. No rounding is required and no units are involved.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The solution proceeds in two parts as requested.\n\nFirst, we derive a closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ using the provided properties of a standard Brownian motion (SBM) $\\{B_{t}\\}_{t \\geq 0}$. The SBM is defined as a mean-zero Gaussian process, so $\\mathbb{E}[B_{t}] = 0$ for all $t \\geq 0$. The covariance structure is given by $\\mathbb{E}[B_{t} B_{s}] = \\min(t,s)$.\n\nThe expression we seek is the second moment of the random variable $B_{t} - B_{s}$. We can expand the square inside the expectation:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = \\mathbb{E}\\big[(B_{t} - B_{s})^{2}\\big]\n$$\nUsing the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}\\big[(B_{t} - B_{s})^{2}\\big] = \\mathbb{E}\\big[B_{t}^{2} - 2B_{t}B_{s} + B_{s}^{2}\\big] = \\mathbb{E}[B_{t}^{2}] - 2\\mathbb{E}[B_{t}B_{s}] + \\mathbb{E}[B_{s}^{2}]\n$$\nWe now evaluate each term using the given properties.\n\nThe term $\\mathbb{E}[B_{t}B_{s}]$ is the covariance, which is given as $\\min(t,s)$.\n$$\n\\mathbb{E}[B_{t}B_{s}] = \\min(t,s)\n$$\nThe terms $\\mathbb{E}[B_{t}^{2}]$ and $\\mathbb{E}[B_{s}^{2}]$ are the second moments of $B_{t}$ and $B_{s}$, respectively. Since the process is mean-zero, the second moment is equal to the variance. The variance of $B_{t}$ can be found from the covariance function:\n$$\n\\mathbb{E}[B_{t}^{2}] = \\text{Var}(B_{t}) = \\text{Cov}(B_t, B_t) = \\mathbb{E}[B_t B_t] = \\min(t,t) = t\n$$\nSimilarly, for $B_{s}$:\n$$\n\\mathbb{E}[B_{s}^{2}] = \\text{Var}(B_{s}) = \\text{Cov}(B_s, B_s) = \\mathbb{E}[B_s B_s] = \\min(s,s) = s\n$$\nSubstituting these results back into the expanded expression:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = t - 2\\min(t,s) + s\n$$\nTo simplify this expression, we consider two cases for the non-negative times $s$ and $t$:\n\nCase 1: $t \\geq s$. In this case, $\\min(t,s) = s$. The expression becomes:\n$$\nt - 2s + s = t - s\n$$\nSince $t \\geq s$, we can write this as $|t-s|$.\n\nCase 2: $s  t$. In this case, $\\min(t,s) = t$. The expression becomes:\n$$\nt - 2t + s = s - t\n$$\nSince $s  t$, we can write this as $|s-t|$, which is equivalent to $|t-s|$.\n\nIn both cases, the result is the same. Therefore, the closed-form expression is:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = |t - s|\n$$\n\nSecond, we use this result to determine if the process $\\{B_{t}\\}_{t \\geq 0}$ is mean-square (MS) continuous at an arbitrary fixed time $t_{0} \\geq 0$. A process $\\{X_{t}\\}$ is defined as MS continuous at $t_{0}$ if $\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|X_{t} - X_{t_{0}}|^{2}\\big] = 0$.\n\nFor the Brownian motion process $\\{B_{t}\\}$, we need to evaluate the limit:\n$$\n\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|B_{t} - B_{t_{0}}|^{2}\\big]\n$$\nUsing the expression derived in the first part, we substitute $|t - t_{0}|$ for the expectation:\n$$\n\\lim_{t \\to t_{0}} |t - t_{0}|\n$$\nThe function $f(t) = |t - t_{0}|$ is continuous for all $t \\in \\mathbb{R}$. Therefore, the limit can be evaluated by direct substitution of $t = t_{0}$:\n$$\n|t_{0} - t_{0}| = |0| = 0\n$$\nSince $\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|B_{t} - B_{t_{0}}|^{2}\\big] = 0$ for any arbitrary $t_{0} \\geq 0$, we conclude that the standard Brownian motion process $\\{B_{t}\\}_{t \\geq 0}$ is mean-square continuous everywhere on its domain $[0, \\infty)$.", "answer": "$$\n\\boxed{|t-s|}\n$$", "id": "3045665"}, {"introduction": "While mean-square continuity describes average behavior, the almost sure continuity of Brownian paths is a stronger, path-by-path property with tangible geometric meaning. This practice explores this by relating the error of a piecewise linear approximation to the path's modulus of continuity, $\\omega_B(\\delta)$. By completing this exercise, you will gain a concrete understanding of how the \"roughness\" of an individual Brownian path is rigorously controlled over small time scales [@problem_id:3045678].", "problem": "Let $T0$ and let $\\{B_t\\}_{t\\in[0,T]}$ be a standard Brownian motion with almost surely continuous sample paths. For an integer $n\\ge 1$, consider the dyadic partition of $[0,T]$ given by $t_k^{(n)} := k\\,T/2^n$ for $k=0,1,\\dots,2^n$. Define the piecewise linear interpolation $L^{(n)}:[0,T]\\to\\mathbb{R}$ of the path $t\\mapsto B_t$ on this partition by\n$$\nL^{(n)}(t) := B_{t_k^{(n)}} + \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}\\big(B_{t_{k+1}^{(n)}}-B_{t_k^{(n)}}\\big),\\quad t\\in[t_k^{(n)},t_{k+1}^{(n)}],\\;k=0,\\dots,2^n-1.\n$$\nDefine the uniform interpolation error\n$$\nE_n := \\sup_{t\\in[0,T]} \\big|B_t - L^{(n)}(t)\\big|.\n$$\nLet the modulus of continuity of the path $t\\mapsto B_t$ on $[0,T]$ be the function $\\omega_B:[0,T]\\to[0,\\infty)$ given by\n$$\n\\omega_B(\\delta) := \\sup\\big\\{|B_s-B_t|:\\, s,t\\in[0,T],\\,|s-t|\\le \\delta\\big}.\n$$\nUsing only the definitions above and basic properties of convex combinations, derive an explicit pathwise bound on $E_n$ in terms of $\\omega_B$ evaluated at the mesh size $T/2^n$. Provide your final bound as a single closed-form analytic expression in terms of $\\omega_B$, $T$, and $n$ (do not include inequality symbols in your final answer).", "solution": "The problem is to derive a pathwise bound on the uniform error $E_n$ of the piecewise linear interpolation of a Brownian motion path in terms of its modulus of continuity.\n\nFirst, let us establish the framework based on the provided definitions.\nLet $\\{B_t\\}_{t\\in[0,T]}$ be a standard Brownian motion on the interval $[0,T]$, with $T0$. We are given that its sample paths are almost surely continuous. We analyze a single such continuous path, which we denote by $t \\mapsto B_t$.\nThe interval $[0,T]$ is partitioned by the dyadic points $t_k^{(n)} = k\\,T/2^n$ for $k=0,1,\\dots,2^n$, where $n \\ge 1$ is an integer. The mesh size of this partition is uniform and given by $\\delta_n := t_{k+1}^{(n)} - t_k^{(n)} = T/2^n$.\n\nThe piecewise linear interpolation of the path, $L^{(n)}(t)$, is defined on each subinterval $[t_k^{(n)},t_{k+1}^{(n)}]$ as\n$$\nL^{(n)}(t) = B_{t_k^{(n)}} + \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}\\left(B_{t_{k+1}^{(n)}}-B_{t_k^{(n)}}\\right).\n$$\nThis expression represents a convex combination of the values of the Brownian path at the endpoints of the subinterval. To see this, let $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$ and define a parameter $\\lambda \\in [0,1]$ as\n$$\n\\lambda := \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}.\n$$\nThen, $1-\\lambda = 1 - \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}} = \\frac{(t_{k+1}^{(n)}-t_k^{(n)}) - (t-t_k^{(n)})}{t_{k+1}^{(n)}-t_k^{(n)}} = \\frac{t_{k+1}^{(n)}-t}{t_{k+1}^{(n)}-t_k^{(n)}}$.\nSubstituting this into the definition of $L^{(n)}(t)$:\n$$\nL^{(n)}(t) = (1-\\lambda)B_{t_k^{(n)}} + \\lambda B_{t_{k+1}^{(n)}}.\n$$\nThis formulation explicitly uses the property of convex combinations as required by the problem statement.\n\nThe quantity to be bounded is the uniform interpolation error, defined as\n$$\nE_n = \\sup_{t\\in[0,T]} |B_t - L^{(n)}(t)|.\n$$\nSince the supremum is over the entire interval $[0,T]$, we can express it as the maximum of the suprema over the subintervals of the partition:\n$$\nE_n = \\sup_{k \\in \\{0,1,\\dots,2^n-1\\}} \\sup_{t \\in [t_k^{(n)}, t_{k+1}^{(n)}]} |B_t - L^{(n)}(t)|.\n$$\nLet us analyze the error $|B_t - L^{(n)}(t)|$ for an arbitrary $t \\in [t_k^{(n)}, t_{k+1}^{(n)}]$. Using the convex combination form of $L^{(n)}(t)$, we can rewrite the error term by introducing $B_t = 1 \\cdot B_t = ((1-\\lambda)+\\lambda)B_t$:\n$$\nB_t - L^{(n)}(t) = B_t - \\left( (1-\\lambda)B_{t_k^{(n)}} + \\lambda B_{t_{k+1}^{(n)}} \\right)\n$$\n$$\nB_t - L^{(n)}(t) = (1-\\lambda)B_t + \\lambda B_t - (1-\\lambda)B_{t_k^{(n)}} - \\lambda B_{t_{k+1}^{(n)}}\n$$\n$$\nB_t - L^{(n)}(t) = (1-\\lambda)(B_t - B_{t_k^{(n)}}) + \\lambda(B_t - B_{t_{k+1}^{(n)}}).\n$$\nThis expression for the error is a convex combination of the increments of the Brownian path from the point $t$ to the endpoints of the subinterval $[t_k^{(n)}, t_{k+1}^{(n)}]$.\n\nNow, we take the absolute value and apply the triangle inequality:\n$$\n|B_t - L^{(n)}(t)| = |(1-\\lambda)(B_t - B_{t_k^{(n)}}) + \\lambda(B_t - B_{t_{k+1}^{(n)}})| \\le |(1-\\lambda)(B_t - B_{t_k^{(n)}})| + |\\lambda(B_t - B_{t_{k+1}^{(n)}})|.\n$$\nSince $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$, we have $0 \\le \\lambda \\le 1$, which implies $1-\\lambda \\ge 0$. Thus, we can write:\n$$\n|B_t - L^{(n)}(t)| \\le (1-\\lambda)|B_t - B_{t_k^{(n)}}| + \\lambda|B_t - B_{t_{k+1}^{(n)}}|.\n$$\nThe problem provides the definition for the path's modulus of continuity:\n$$\n\\omega_B(\\delta) := \\sup\\{|B_s - B_t| : s, t \\in [0,T], |s-t| \\le \\delta\\}.\n$$\nWe use this to bound the two difference terms.\nFor the first term, $|B_t - B_{t_k^{(n)}}|$, the time difference is $|t - t_k^{(n)}| = t - t_k^{(n)}$. Since $t \\le t_{k+1}^{(n)}$, we have $t - t_k^{(n)} \\le t_{k+1}^{(n)} - t_k^{(n)} = \\delta_n = T/2^n$. By the definition of the modulus of continuity, it follows that\n$$\n|B_t - B_{t_k^{(n)}}| \\le \\omega_B(t - t_k^{(n)}).\n$$\nThe modulus of continuity $\\omega_B(\\delta)$ is a non-decreasing function of $\\delta$, because as $\\delta$ increases, the supremum is taken over a larger set. Therefore, since $t-t_k^{(n)} \\le \\delta_n$, we have $\\omega_B(t-t_k^{(n)}) \\le \\omega_B(\\delta_n)$. This yields the bound:\n$$\n|B_t - B_{t_k^{(n)}}| \\le \\omega_B(T/2^n).\n$$\nFor the second term, $|B_t - B_{t_{k+1}^{(n)}}| = |B_{t_{k+1}^{(n)}} - B_t|$, the time difference is $|t_{k+1}^{(n)} - t| = t_{k+1}^{(n)} - t$. Since $t \\ge t_k^{(n)}$, we have $t_{k+1}^{(n)} - t \\le t_{k+1}^{(n)} - t_k^{(n)} = \\delta_n = T/2^n$. Applying the same reasoning:\n$$\n|B_t - B_{t_{k+1}^{(n)}}| \\le \\omega_B(t_{k+1}^{(n)} - t) \\le \\omega_B(T/2^n).\n$$\nSubstituting these two bounds back into the inequality for the error:\n$$\n|B_t - L^{(n)}(t)| \\le (1-\\lambda)\\omega_B(T/2^n) + \\lambda\\omega_B(T/2^n)\n$$\n$$\n|B_t - L^{(n)}(t)| \\le ((1-\\lambda) + \\lambda)\\omega_B(T/2^n) = \\omega_B(T/2^n).\n$$\nThis inequality holds for any $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$. The derived bound, $\\omega_B(T/2^n)$, is independent of the specific point $t$ within the subinterval and is also independent of the subinterval index $k$.\nTherefore, this same bound must hold for the supremum of the error over the entire interval $[0,T]$.\n$$\nE_n = \\sup_{t\\in[0,T]} |B_t - L^{(n)}(t)| \\le \\omega_B(T/2^n).\n$$\nThe problem asks for an explicit pathwise bound on $E_n$. The expression $\\omega_B(T/2^n)$ is such a bound, derived as requested.", "answer": "$$\\boxed{\\omega_B\\left(\\frac{T}{2^n}\\right)}$$", "id": "3045678"}, {"introduction": "Bridging the gap between abstract theory and practical application is a cornerstone of applied mathematics. This comprehensive exercise challenges you to combine theoretical derivation, using the powerful reflection principle, with computational simulation to investigate path properties. You will develop a Monte Carlo test to empirically measure the probability of large increments and check its consistency with a theoretically derived bound, providing hands-on experience in model validation [@problem_id:3045650].", "problem": "Let $\\{B_t\\}_{t \\ge 0}$ be a standard Brownian motion, that is, a continuous-time stochastic process with $B_0 = 0$, stationary independent increments, and for any $0 \\le s  t$ the increment $B_t - B_s$ is Gaussian with mean $0$ and variance $t - s$. Consider the event that a Brownian sample path, restricted to a finite time horizon $[0,1]$, has a maximal one-sided increment of size at least $\\varepsilon$ over a uniform coarse grid of $M$ subintervals. More precisely, let the coarse grid points be $t_i = i/M$ for $i = 0,1,\\dots,M$, and define the event\n$$\nE_{M,\\varepsilon} := \\left\\{\\max_{0 \\le i \\le M-1}\\ \\sup_{0 \\le u \\le \\Delta}\\ \\big(B_{t_i+u} - B_{t_i}\\big) \\ge \\varepsilon \\right\\},\\quad \\Delta := \\frac{1}{M}.\n$$\nYou will derive a theoretical tail-probability upper bound for $\\mathbb{P}(E_{M,\\varepsilon})$ using the reflection principle, and design a Monte Carlo test that estimates $\\mathbb{P}(E_{M,\\varepsilon})$ from simulated paths sampled on a fine grid within each coarse interval. Your test should then compare the empirical estimate to the theoretical upper bound, concluding whether the empirical estimate is consistent with the bound up to a statistically justified tolerance.\n\nTasks:\n- Starting from the fundamental properties of Brownian motion and the reflection principle, derive a one-interval tail probability expression for the event $\\sup_{0 \\le u \\le \\Delta} \\big(B_{u} - B_{0}\\big) \\ge \\varepsilon$ as a function of $\\varepsilon$ and $\\Delta$. Then, invoke a union bound to produce an explicit upper bound for $\\mathbb{P}(E_{M,\\varepsilon})$ in terms of $M$ and the one-interval tail probability. Do not assume or use any \"shortcut\" formula that is not derived from the core definitions and the reflection principle.\n- Construct a Monte Carlo estimator for $\\mathbb{P}(E_{M,\\varepsilon})$ by simulating independent Brownian motion segments on a fine grid of $K$ equally spaced points within each coarse interval of length $\\Delta$. Within each coarse interval, approximate $\\sup_{0 \\le u \\le \\Delta} \\big(B_{t_i+u} - B_{t_i}\\big)$ by the maximum of the cumulative sums over the $K$ fine steps. Aggregate across the $M$ intervals to approximate the event $E_{M,\\varepsilon}$ for each simulated path. Use $N$ independent replications to estimate the probability.\n- Justify a deterministic tolerance for Monte Carlo sampling error using a concentration inequality for averages of independent Bernoulli random variables. For a chosen significance level $\\alpha$, an acceptable tolerance $\\eta(N,\\alpha)$ must satisfy a non-asymptotic tail bound that depends only on $N$ and $\\alpha$. Use this tolerance to decide whether the empirical estimate is less than or equal to the theoretical upper bound plus $\\eta(N,\\alpha)$.\n- Implement a complete program that, for each test case below, computes a boolean indicating whether the empirical estimate is consistent with the theoretical upper bound within the prescribed tolerance.\n\nUse the following test suite, where each case is given as a tuple $(M,K,\\varepsilon,N)$, with time horizon fixed to $[0,1]$, and significance level $\\alpha = 10^{-3}$:\n- Case A (happy path): $(M,K,\\varepsilon,N) = (\\,16,\\,64,\\,0.6,\\,3000\\,)$.\n- Case B (boundary of one-interval reflection principle): $(M,K,\\varepsilon,N) = (\\,1,\\,2048,\\,1.0,\\,3000\\,)$.\n- Case C (many intervals, small upper bound): $(M,K,\\varepsilon,N) = (\\,64,\\,32,\\,0.4,\\,2000\\,)$.\n- Case D (very small threshold, bound saturates): $(M,K,\\varepsilon,N) = (\\,32,\\,32,\\,0.05,\\,2000\\,)$.\n\nImplementation details and requirements:\n- Time horizon is $[0,1]$ with coarse mesh size $\\Delta = 1/M$ and fine step $\\delta = \\Delta/K$.\n- In the simulation, for each coarse interval, generate $K$ independent Gaussian increments with mean $0$ and variance $\\delta$, accumulate them to form the fine-grid Brownian segment, and approximate the interval’s supremum by the maximum partial sum within the interval.\n- Across intervals, the Brownian segments can be simulated independently because Brownian motion has independent increments, and only the maxima relative to each interval’s start are needed.\n- Use a fixed base pseudorandom seed $123456$ to ensure deterministic output. For the $j$-th test case in the order listed above (starting from $j=0$), use the seed $123456 + j$.\n- The theoretical bound must be computed from your derivation that uses the reflection principle and the union bound across $M$ intervals, and it must be clipped to the interval $[0,1]$.\n- The concentration tolerance must be based on a valid concentration inequality for independent Bernoulli trials; for example, for a given $\\alpha$ you may use a form $\\eta(N,\\alpha) = \\sqrt{(\\log(c/\\alpha))/(2N)}$ with a universal constant $c$ derived from the bound you use.\n\nFinal output specification:\n- Your program should produce a single line of output containing the four boolean results, in the order of the test suite, aggregated as a comma-separated list enclosed in square brackets, for example, $[{\\rm True},{\\rm False},{\\rm True},{\\rm True}]$.", "solution": "The problem requires a three-part analysis: first, the derivation of a theoretical upper bound for a probability related to Brownian motion paths; second, the design of a Monte Carlo simulation to estimate this probability; and third, a statistical comparison between the theoretical bound and the empirical estimate.\n\n### Part 1: Theoretical Upper Bound Derivation\n\nThe problem considers the event $E_{M,\\varepsilon}$ defined as:\n$$\nE_{M,\\varepsilon} := \\left\\{\\max_{0 \\le i \\le M-1}\\ \\sup_{0 \\le u \\le \\Delta}\\ \\big(B_{t_i+u} - B_{t_i}\\big) \\ge \\varepsilon \\right\\}\n$$\nwhere $\\{B_t\\}_{t \\ge 0}$ is a standard Brownian motion, $t_i = i/M$, and $\\Delta = 1/M$. Let $A_i$ be the event for the $i$-th interval:\n$$\nA_i := \\left\\{\\sup_{0 \\le u \\le \\Delta}\\ \\big(B_{t_i+u} - B_{t_i}\\big) \\ge \\varepsilon \\right\\}\n$$\nThen $E_{M,\\varepsilon} = \\bigcup_{i=0}^{M-1} A_i$.\n\nDue to the stationary and independent increments property of Brownian motion, the process $W_u^{(i)} := B_{t_i+u} - B_{t_i}$ for $u \\in [0, \\Delta]$ is itself a standard Brownian motion starting at $0$. Therefore, the probability $\\mathbb{P}(A_i)$ is the same for all $i$ and is equal to the probability that a standard Brownian motion reaches a level $\\varepsilon$ within the time interval $[0, \\Delta]$. Let's denote this one-interval probability as $p_{1}(\\varepsilon, \\Delta) = \\mathbb{P}(\\sup_{0 \\le u \\le \\Delta} B_u \\ge \\varepsilon)$.\n\nTo calculate $p_{1}(\\varepsilon, \\Delta)$, we use the **reflection principle**. For a standard Brownian motion $\\{B_t\\}_{t \\ge 0}$ and any level $\\varepsilon  0$, the principle states:\n$$\n\\mathbb{P}\\left(\\sup_{0 \\le u \\le t} B_u \\ge \\varepsilon\\right) = 2 \\mathbb{P}(B_t \\ge \\varepsilon)\n$$\nThis can be shown by considering the first hitting time $\\tau_\\varepsilon = \\inf\\{u \\ge 0 : B_u = \\varepsilon\\}$. The event $\\{\\sup_{0 \\le u \\le t} B_u \\ge \\varepsilon\\}$ is equivalent to $\\{\\tau_\\varepsilon \\le t\\}$. By the strong Markov property, the process $B_u' = B_{u+\\tau_\\varepsilon} - B_{\\tau_\\varepsilon}$ is a standard Brownian motion independent of the path up to $\\tau_\\varepsilon$. By symmetry of Brownian increments, the probability that the path is above $\\varepsilon$ at time $t$, given that it has hit $\\varepsilon$, is $1/2$. Formally, $\\mathbb{P}(B_t \\ge \\varepsilon | \\tau_\\varepsilon \\le t) = 1/2$. This leads to $\\mathbb{P}(B_t \\ge \\varepsilon) = \\mathbb{P}(B_t \\ge \\varepsilon | \\tau_\\varepsilon \\le t)\\mathbb{P}(\\tau_\\varepsilon \\le t) = \\frac{1}{2}\\mathbb{P}(\\sup_{0 \\le u \\le t} B_u \\ge \\varepsilon)$, which proves the principle.\n\nApplying this to our case with time $t=\\Delta$:\n$$\np_{1}(\\varepsilon, \\Delta) = 2 \\mathbb{P}(B_\\Delta \\ge \\varepsilon)\n$$\nSince $B_\\Delta$ follows a normal distribution with mean $0$ and variance $\\Delta$, i.e., $B_\\Delta \\sim \\mathcal{N}(0, \\Delta)$, its probability density function is $f(x) = \\frac{1}{\\sqrt{2\\pi\\Delta}} e^{-x^2/(2\\Delta)}$. The probability $\\mathbb{P}(B_\\Delta \\ge \\varepsilon)$ can be expressed using the standard normal distribution $Z \\sim \\mathcal{N}(0, 1)$ by writing $B_\\Delta = \\sqrt{\\Delta}Z$:\n$$\n\\mathbb{P}(B_\\Delta \\ge \\varepsilon) = \\mathbb{P}\\left(Z \\ge \\frac{\\varepsilon}{\\sqrt{\\Delta}}\\right) = \\frac{1}{2} \\text{erfc}\\left(\\frac{\\varepsilon/\\sqrt{\\Delta}}{\\sqrt{2}}\\right) = \\frac{1}{2} \\text{erfc}\\left(\\frac{\\varepsilon}{\\sqrt{2\\Delta}}\\right)\n$$\nwhere $\\text{erfc}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_x^\\infty e^{-t^2}dt$ is the complementary error function.\nSubstituting this into the expression for $p_1$ gives:\n$$\np_1(\\varepsilon, \\Delta) = 2 \\cdot \\frac{1}{2} \\text{erfc}\\left(\\frac{\\varepsilon}{\\sqrt{2\\Delta}}\\right) = \\text{erfc}\\left(\\frac{\\varepsilon}{\\sqrt{2\\Delta}}\\right)\n$$\nNext, we invoke the **union bound** (or Boole's inequality) to find an upper bound for $\\mathbb{P}(E_{M,\\varepsilon})$:\n$$\n\\mathbb{P}(E_{M,\\varepsilon}) = \\mathbb{P}\\left(\\bigcup_{i=0}^{M-1} A_i\\right) \\le \\sum_{i=0}^{M-1} \\mathbb{P}(A_i)\n$$\nAs established, $\\mathbb{P}(A_i) = p_1(\\varepsilon, \\Delta)$ for all $i=0, \\dots, M-1$. There are $M$ such terms in the sum.\n$$\n\\mathbb{P}(E_{M,\\varepsilon}) \\le M \\cdot p_1(\\varepsilon, \\Delta) = M \\cdot \\text{erfc}\\left(\\frac{\\varepsilon}{\\sqrt{2\\Delta}}\\right)\n$$\nSubstituting $\\Delta = 1/M$, the argument of erfc becomes $\\varepsilon/\\sqrt{2/M} = \\varepsilon \\sqrt{M/2}$. Since a probability cannot exceed $1$, the final theoretical upper bound, $p_{bound}$, is:\n$$\np_{bound}(M, \\varepsilon) = \\min\\left(1, M \\cdot \\text{erfc}\\left(\\varepsilon\\sqrt{\\frac{M}{2}}\\right)\\right)\n$$\n\n### Part 2: Monte Carlo Estimator Design\n\nTo estimate $p = \\mathbb{P}(E_{M,\\varepsilon})$, we perform $N$ independent Monte Carlo simulations. The event $E_{M,\\varepsilon}$ concerns a single Brownian path over $[0,1]$. For each of the $N$ simulations, we generate a discretized version of such a path and check if the event occurs.\n\nFor a single simulation run:\n1.  The goal is to evaluate $\\max_{0 \\le i \\le M-1} \\sup_{0 \\le u \\le \\Delta} (B_{t_i+u} - B_{t_i})$.\n2.  The processes $W_u^{(i)} = B_{t_i+u} - B_{t_i}$ for different coarse intervals $i$ are independent because Brownian motion has independent increments over disjoint time intervals ($[t_i, t_{i+1}]$). Thus, we can simulate these $M$ processes independently.\n3.  For each coarse interval $i \\in \\{0, \\dots, M-1\\}$, we approximate the continuous supremum by discretizing the interval $[0, \\Delta]$ into $K$ fine steps of size $\\delta = \\Delta/K = 1/(MK)$.\n4.  We generate $K$ independent Gaussian increments $\\{\\xi_j\\}_{j=1}^K$, where $\\xi_j \\sim \\mathcal{N}(0, \\delta)$.\n5.  The path of the Brownian segment is approximated by the cumulative sums: $S_k = \\sum_{j=1}^k \\xi_j$ for $k=1, \\dots, K$.\n6.  The supremum over this interval is approximated by $\\max_{1 \\le k \\le K} S_k$.\n7.  The maximum over all $M$ intervals for one simulated path is calculated. If this overall maximum is greater than or equal to $\\varepsilon$, the event $E_{M,\\varepsilon}$ is considered to have occurred for this simulation.\n8.  This procedure is repeated $N$ times. The estimator $\\hat{p}$ is the fraction of the $N$ simulations for which the event occurred.\n\n### Part 3: Statistical Tolerance\n\nThe Monte Carlo estimator $\\hat{p}$ is the sample mean of $N$ independent and identically distributed Bernoulli random variables, each with parameter $p = \\mathbb{P}(E_{M,\\varepsilon})$. For a large number of trials $N$, $\\hat{p}$ will be close to $p$. To quantify this, we use a concentration inequality.\n\nWe use the one-sided **Hoeffding's inequality**, which states that for an average $\\hat{p}$ of $N$ i.i.d. random variables bounded in $[0,1]$ with true mean $p$, for any $\\eta  0$:\n$$\n\\mathbb{P}(\\hat{p} - p \\ge \\eta) \\le e^{-2N\\eta^2}\n$$\nWe want to find a tolerance $\\eta$ such that the probability of the empirical estimate exceeding the true probability by more than $\\eta$ is less than a given significance level $\\alpha$. Setting the right-hand side to $\\alpha$:\n$$\ne^{-2N\\eta^2} = \\alpha \\implies -2N\\eta^2 = \\ln(\\alpha) \\implies \\eta^2 = \\frac{-\\ln(\\alpha)}{2N} = \\frac{\\ln(1/\\alpha)}{2N}\n$$\nThis gives the tolerance:\n$$\n\\eta(N, \\alpha) = \\sqrt{\\frac{\\ln(1/\\alpha)}{2N}}\n$$\nThis matches the problem description form $\\sqrt{(\\ln(c/\\alpha))/(2N)}$ with the constant $c=1$. For the given $\\alpha=10^{-3}$, the tolerance is $\\eta(N) = \\sqrt{\\frac{\\ln(1000)}{2N}}$.\n\n### Part 4: Consistency Check\n\nThe theoretical bound $p_{bound}$ should be greater than or equal to the true probability $p$, i.e., $p \\le p_{bound}$. Our empirical estimate $\\hat{p}$ is an estimate of $p$. Due to statistical fluctuations, $\\hat{p}$ may be slightly larger than $p$. The tolerance $\\eta$ provides a margin for this statistical error.\n\nThe empirical estimate $\\hat{p}$ is considered consistent with the theoretical bound $p_{bound}$ if it does not statistically contradict the inequality $p \\le p_{bound}$. We accept consistency if the estimated value is not \"too large\" compared to the bound. The threshold for \"too large\" is determined by our tolerance $\\eta$. An observation $\\hat{p}  p_{bound} + \\eta$ would suggest that $p$ is likely larger than $p_{bound}$, which would contradict the theory (with confidence $1-\\alpha$). Therefore, the consistency check is:\n$$\n\\hat{p} \\le p_{bound} + \\eta(N, \\alpha)\n$$\nThe program will compute this boolean value for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Global parameters from the problem statement\n    ALPHA = 1e-3\n\n    def theoretical_bound(M, eps):\n        \"\"\"\n        Computes the theoretical upper bound for P(E_{M,eps}) using the\n        reflection principle and a union bound.\n\n        The one-interval probability is p1 = erfc(eps / sqrt(2*Delta)).\n        With Delta = 1/M, this is p1 = erfc(eps * sqrt(M/2)).\n        The union bound is M * p1, which must be clipped at 1.0.\n        \"\"\"\n        if eps  0:\n            return 1.0 # The sup will surely be = a negative number.\n        \n        arg = eps * np.sqrt(M / 2.0)\n        bound = M * erfc(arg)\n        \n        return min(1.0, bound)\n\n    def monte_carlo_estimator(M, K, eps, N, seed):\n        \"\"\"\n        Estimates P(E_{M,eps}) via Monte Carlo simulation.\n        N: number of simulated paths.\n        M: number of coarse intervals.\n        K: number of fine steps within each coarse interval.\n        eps: threshold for the maximum increment.\n        seed: pseudorandom number generator seed.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        event_count = 0\n        \n        delta = 1.0 / (M * K)\n        # Standard deviation of the fine-grid increments\n        std_dev = np.sqrt(delta)\n        \n        for _ in range(N):\n            max_of_path = -np.inf\n            \n            # For each path, check the M intervals. The increment processes\n            # a_i(u) = B(t_i+u) - B(t_i) are independent across i due to\n            # independent increments of B_t over disjoint time intervals.\n            for i in range(M):\n                # Generate K Gaussian increments for the i-th interval process.\n                increments = rng.normal(loc=0.0, scale=std_dev, size=K)\n                \n                # Form the path segment by taking cumulative sums.\n                path_segment = np.cumsum(increments)\n                \n                # Find the maximum of this segment, approximating the supremum.\n                interval_max = np.max(path_segment) if K  0 else 0.0\n                \n                # Update the maximum seen so far across all intervals for this path.\n                if interval_max  max_of_path:\n                    max_of_path = interval_max\n            \n            # Check if the event E_M,eps occurred for this simulated path.\n            if max_of_path = eps:\n                event_count += 1\n                \n        return event_count / N\n\n    def statistical_tolerance(N, alpha):\n        \"\"\"\n        Computes the one-sided tolerance for the Monte Carlo estimate based on\n        Hoeffding's inequality: eta = sqrt(log(1/alpha) / (2N)).\n        \"\"\"\n        return np.sqrt(np.log(1.0 / alpha) / (2.0 * N))\n    \n    # Test suite provided in the problem statement.\n    test_cases = [\n        # (M, K, ε, N)\n        (16, 64, 0.6, 3000),   # Case A\n        (1, 2048, 1.0, 3000),  # Case B\n        (64, 32, 0.4, 2000),   # Case C\n        (32, 32, 0.05, 2000),  # Case D\n    ]\n    \n    base_seed = 123456\n    results = []\n    \n    for j, case in enumerate(test_cases):\n        M, K, eps, N = case\n        # Use a unique seed for each test case for reproducibility.\n        seed = base_seed + j\n        \n        # 1. Compute the theoretical upper bound.\n        p_bound = theoretical_bound(M, eps)\n        \n        # 2. Compute the Monte Carlo empirical estimate.\n        p_hat = monte_carlo_estimator(M, K, eps, N, seed)\n        \n        # 3. Compute the statistical tolerance for the given confidence.\n        eta = statistical_tolerance(N, ALPHA)\n        \n        # 4. Perform the consistency check.\n        # The estimate is consistent if it's within the bound plus tolerance.\n        is_consistent = (p_hat = p_bound + eta)\n        results.append(is_consistent)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3045650"}]}