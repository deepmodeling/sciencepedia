{"hands_on_practices": [{"introduction": "Understanding the covariance structure of Brownian motion begins with mastering the fundamental formula $\\operatorname{Cov}(B_s, B_t) = \\min(s, t)$. This first exercise provides essential practice in applying this rule, combined with the bilinearity of the covariance operator. By calculating the covariance between an increment and the process value, you will gain insight into how the relationships between different points in a Brownian path are structured, particularly how the independent increments property manifests in these calculations [@problem_id:3047248].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (also called a standard Wiener process) with $B_{0}=0$, almost surely continuous paths, stationary and independent increments, Gaussian increments, zero mean $E[B_{t}]=0$, and variance $\\operatorname{Var}(B_{t})=t$. For fixed times $s,t,u \\geq 0$ with $t \\geq s$, derive from these defining properties an explicit expression for the covariance $\\operatorname{Cov}(B_{t}-B_{s},B_{u})$ in terms of the minimum function of pairs of times. Then, simplify your expression for the three regimes $u \\leq s$, $s  u \\leq t$, and $t  u$. Provide your final answer as a single closed-form analytic expression built from minimum functions together with the three simplified values corresponding to the listed regimes.", "solution": "The problem requires the derivation of the covariance $\\operatorname{Cov}(B_{t}-B_{s},B_{u})$ for a standard Brownian motion $\\{B_x\\}_{x \\geq 0}$, where $s, t, u \\geq 0$ and it is given that $t \\geq s$. The solution must be expressed generally in terms of the minimum function and then simplified for three specific regimes of $u$.\n\nThe fundamental properties of a standard Brownian motion $\\{B_x\\}_{x \\geq 0}$ given in the problem statement are:\n1.  $B_{0}=0$ almost surely.\n2.  Stationary and independent increments: For any sequence of times $0 \\leq \\tau_1  \\tau_2  \\dots  \\tau_n$, the random variables $B_{\\tau_2}-B_{\\tau_1}, B_{\\tau_3}-B_{\\tau_2}, \\dots, B_{\\tau_n}-B_{\\tau_{n-1}}$ are independent. The distribution of an increment $B_{b}-B_{a}$ with $ab$ depends only on the length of the time interval, $b-a$.\n3.  Gaussian increments: $B_b - B_a$ is a Gaussian random variable for $ab$.\n4.  Zero mean: $E[B_x]=0$ for all $x \\geq 0$.\n5.  Variance: $\\operatorname{Var}(B_x)=E[B_x^2] - (E[B_x])^2 = E[B_x^2] = x$.\n\nThe covariance between two random variables $X$ and $Y$ is defined as $\\operatorname{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]$.\nThe random variables in question are $B_t - B_s$ and $B_u$. Let us first determine their means.\nUsing the linearity of the expectation operator and the zero-mean property of Brownian motion:\n$E[B_t - B_s] = E[B_t] - E[B_s] = 0 - 0 = 0$.\n$E[B_u] = 0$.\nSince both variables have zero mean, their covariance simplifies to the expectation of their product:\n$\\operatorname{Cov}(B_{t}-B_{s},B_{u}) = E[(B_{t}-B_{s})B_{u}]$.\n\nWe can use the bilinearity property of the covariance operator:\n$\\operatorname{Cov}(B_{t}-B_{s},B_{u}) = \\operatorname{Cov}(B_t, B_u) - \\operatorname{Cov}(B_s, B_u)$.\n\nTo proceed, we must first establish the general formula for the covariance of a standard Brownian motion at two time points, say $a$ and $b$. Let $\\operatorname{Cov}(B_a, B_b)$. Since $E[B_a]=E[B_b]=0$, we have $\\operatorname{Cov}(B_a, B_b) = E[B_a B_b]$.\nWithout loss of generality, assume $a \\leq b$. We can write $B_b$ as $B_b = B_a + (B_b - B_a)$.\nSubstituting this into the expectation:\n$E[B_a B_b] = E[B_a (B_a + (B_b - B_a))] = E[B_a^2] + E[B_a (B_b - B_a)]$.\nThe increment $B_b - B_a$ is independent of the value of the process at time $a$, $B_a$, due to the property of independent increments. Therefore, the expectation of the product is the product of the expectations:\n$E[B_a (B_b - B_a)] = E[B_a] E[B_b - B_a] = 0 \\cdot (E[B_b] - E[B_a]) = 0$.\nThe term $E[B_a^2]$ is the variance of $B_a$, since its mean is zero. From the given properties, $\\operatorname{Var}(B_a) = a$.\nThus, for $a \\leq b$, we have $E[B_a B_b] = a$.\nIf we had assumed $b \\leq a$, a symmetric argument would yield $E[B_a B_b] = b$.\nCombining these two cases, we arrive at the general formula for the covariance function of a standard Brownian motion:\n$\\operatorname{Cov}(B_a, B_b) = \\min(a, b)$.\n\nNow, we substitute this fundamental result back into our expression for the desired covariance:\n$\\operatorname{Cov}(B_t - B_s, B_u) = \\operatorname{Cov}(B_t, B_u) - \\operatorname{Cov}(B_s, B_u) = \\min(t, u) - \\min(s, u)$.\nThis is the required explicit expression in terms of the minimum function.\n\nNext, we simplify this expression for the three specified regimes, keeping in mind the given condition $s \\leq t$.\n\nRegime 1: $u \\leq s$.\nThe ordering of the time points is $u \\leq s \\leq t$.\nIn this case:\n$\\min(t, u) = u$.\n$\\min(s, u) = u$.\nTherefore, the covariance is:\n$\\operatorname{Cov}(B_{t}-B_{s},B_{u}) = u - u = 0$.\nThis result is consistent with the property of independent increments. The interval $[s, t]$ over which the increment $B_t - B_s$ is defined is disjoint from and subsequent to the interval $[0, u]$. Thus, the increment $B_t - B_s$ is independent of $B_u$, and their covariance is zero.\n\nRegime 2: $s  u \\leq t$.\nThe ordering of the time points is $s  u \\leq t$.\nIn this case:\n$\\min(t, u) = u$.\n$\\min(s, u) = s$.\nTherefore, the covariance is:\n$\\operatorname{Cov}(B_{t}-B_{s},B_{u}) = u - s$.\n\nRegime 3: $t  u$.\nThe ordering of the time points is $s \\leq t  u$.\nIn this case:\n$\\min(t, u) = t$.\n$\\min(s, u) = s$.\nTherefore, the covariance is:\n$\\operatorname{Cov}(B_{t}-B_{s},B_{u}) = t - s$.\n\nIn summary, the general expression for the covariance is $\\min(t, u) - \\min(s, u)$. The simplified values for the three regimes $u \\leq s$, $s  u \\leq t$, and $t  u$ are $0$, $u-s$, and $t-s$, respectively. We present these four results together in the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\min(t, u) - \\min(s, u)  0  u - s  t - s\n\\end{pmatrix}\n}\n$$", "id": "3047248"}, {"introduction": "While covariance measures the direction of a linear relationship, correlation provides a normalized, unitless measure of its strength. This practice problem moves beyond basic covariance to compute the correlation between two increments of a Brownian motion that occur over partially overlapping time intervals. Solving this will require you to cleverly decompose the random variables into sums of independent increments, revealing how the shared portion of their time intervals is the sole source of their correlation [@problem_id:3047212].", "problem": "Let $\\{B_t\\}_{t \\ge 0}$ be a standard one-dimensional Brownian motion (also called a Wiener process), characterized by $B_0=0$, almost surely continuous paths, independent and stationary increments, $\\mathbb{E}[B_t]=0$ for all $t \\ge 0$, and $\\operatorname{Var}(B_t)=t$. Consider two time intervals $[s,t]$ and $[v,u]$ with $0 \\le s  v  t  u$, so that the intervals partially overlap but neither contains the other. Using only these defining properties, derive a closed-form expression for the correlation coefficient $\\rho=\\operatorname{Corr}(B_t - B_s, B_u - B_v)$ in terms of $s$, $t$, $u$, and $v$. Your final answer must be a single closed-form symbolic expression. No numerical rounding is required.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains sufficient information for a unique solution. We proceed with the derivation.\n\nLet the two random variables of interest be $X = B_t - B_s$ and $Y = B_u - B_v$. The correlation coefficient $\\rho$ between $X$ and $Y$ is defined as:\n$$ \\rho = \\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}} $$\nWe must compute the two variances, $\\operatorname{Var}(X)$ and $\\operatorname{Var}(Y)$, and the covariance, $\\operatorname{Cov}(X, Y)$, using the defining properties of a standard one-dimensional Brownian motion $\\{B_t\\}_{t \\ge 0}$.\n\nA key property of a standard Brownian motion is that for any $t_1  t_2$, the increment $B_{t_2} - B_{t_1}$ is a normally distributed random variable with mean $0$ and variance $t_2 - t_1$. This follows from the given properties of stationary increments, $\\mathbb{E}[B_t]=0$, and $\\operatorname{Var}(B_t)=t$.\nSpecifically, $\\mathbb{E}[B_{t_2} - B_{t_1}] = 0$ and $\\operatorname{Var}(B_{t_2} - B_{t_1}) = t_2 - t_1$.\n\nFirst, we calculate the variances of $X$ and $Y$.\nFor $X = B_t - B_s$, we have $s  t$. Using the property of the variance of an increment:\n$$ \\operatorname{Var}(X) = \\operatorname{Var}(B_t - B_s) = t - s $$\nFor $Y = B_u - B_v$, we have $v  u$. Similarly:\n$$ \\operatorname{Var}(Y) = \\operatorname{Var}(B_u - B_v) = u - v $$\n\nNext, we calculate the covariance, $\\operatorname{Cov}(X, Y)$.\n$$ \\operatorname{Cov}(X, Y) = \\operatorname{Cov}(B_t - B_s, B_u - B_v) $$\nSince the means of the increments are zero, $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$, the covariance is equal to the expectation of the product:\n$$ \\operatorname{Cov}(X, Y) = \\mathbb{E}[XY] = \\mathbb{E}[(B_t - B_s)(B_u - B_v)] $$\nTo evaluate this expectation, we use the property that increments over disjoint time intervals are independent. The given time ordering is $0 \\le s  v  t  u$. We can decompose the increments $X$ and $Y$ into sums of increments over non-overlapping intervals.\nLet us define the following increments over the disjoint intervals $[s, v]$, $[v, t]$, and $[t, u]$:\n$I_1 = B_v - B_s$\n$I_2 = B_t - B_v$\n$I_3 = B_u - B_t$\nBy the property of independent increments, $I_1$, $I_2$, and $I_3$ are mutually independent random variables.\n\nWe can express $X$ and $Y$ in terms of these independent increments:\n$X = B_t - B_s = (B_t - B_v) + (B_v - B_s) = I_2 + I_1$\n$Y = B_u - B_v = (B_u - B_t) + (B_t - B_v) = I_3 + I_2$\n\nNow we can compute the covariance using the bilinearity property:\n$$ \\operatorname{Cov}(X, Y) = \\operatorname{Cov}(I_1 + I_2, I_2 + I_3) $$\n$$ = \\operatorname{Cov}(I_1, I_2) + \\operatorname{Cov}(I_1, I_3) + \\operatorname{Cov}(I_2, I_2) + \\operatorname{Cov}(I_2, I_3) $$\nSince $I_1, I_2, I_3$ are independent, the covariance between any two distinct increments is zero:\n$\\operatorname{Cov}(I_1, I_2) = 0$\n$\\operatorname{Cov}(I_1, I_3) = 0$\n$\\operatorname{Cov}(I_2, I_3) = 0$\nThe covariance of an increment with itself is its variance:\n$\\operatorname{Cov}(I_2, I_2) = \\operatorname{Var}(I_2)$\n\nTherefore, the expression for the covariance simplifies to:\n$$ \\operatorname{Cov}(X, Y) = \\operatorname{Var}(I_2) = \\operatorname{Var}(B_t - B_v) $$\nUsing the variance property of an increment, and noting that $v  t$:\n$$ \\operatorname{Cov}(X, Y) = t - v $$\nThe covariance is the length of the overlapping time interval $[v, t]$.\n\nFinally, we substitute the calculated variances and covariance into the formula for the correlation coefficient:\n$$ \\rho = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}} = \\frac{t - v}{\\sqrt{(t - s)(u - v)}} $$\nThis is the closed-form expression for the correlation coefficient in terms of $s$, $t$, $u$, and $v$.", "answer": "$$\\boxed{\\frac{t - v}{\\sqrt{(t - s)(u - v)}}}$$", "id": "3047212"}, {"introduction": "One of the most profound features of Brownian motion is its Markov property: the future is independent of the past, given the present. This exercise offers a hands-on way to prove and understand this concept by calculating a conditional covariance. You will discover that conditioning on the value of the process at an intermediate time completely removes the statistical dependence between an earlier and a later point, providing a concrete illustration of this foundational principle in stochastic processes [@problem_id:3047221].", "problem": "Let $\\{B_{r}\\}_{r \\geq 0}$ be a standard Brownian motion (also called a Wiener process) with $B_{0} = 0$, continuous paths, independent and stationary increments, and such that for any $r \\geq 0$ the random variable $B_{r}$ is Gaussian with mean $0$ and variance $r$. Fix times $s$, $t$, and $u$ with $0 \\leq s  t  u$. Using only the core properties of Brownian motion and basic facts about the multivariate normal distribution, derive an explicit closed-form analytic expression for the conditional covariance $\\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t})$. Then briefly explain, in terms of the structural properties of Brownian motion, why conditioning on $B_{t}$ reduces the covariance relative to the unconditional covariance $\\operatorname{Cov}(B_{u}, B_{s})$.\n\nExpress your final answer as a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement is evaluated as valid. It is self-contained, scientifically grounded in the theory of stochastic processes, and well-posed. The problem requests the derivation of a conditional covariance for a standard Brownian motion, which is a standard and meaningful calculation in this field.\n\nLet $\\{B_{r}\\}_{r \\geq 0}$ be a standard one-dimensional Brownian motion (Wiener process) with $B_{0}=0$. The core properties are:\n1.  Continuity of paths.\n2.  Stationary and independent increments: for any $0 \\leq q  r \\leq v  w$, the increment $B_{w}-B_{v}$ is independent of $B_{r}-B_{q}$.\n3.  The increment $B_{r}-B_{q}$ is normally distributed with mean $0$ and variance $r-q$. Thus, $B_{r} = B_{r}-B_{0} \\sim \\mathcal{N}(0, r)$.\n\nWe are asked to compute the conditional covariance $\\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t})$ for fixed times $0 \\leq s  t  u$.\n\nThe definition of conditional covariance for two random variables $X$ and $Y$ given a third random variable $Z$ is:\n$$ \\operatorname{Cov}(X, Y \\mid Z) = E[(X - E[X \\mid Z])(Y - E[Y \\mid Z]) \\mid Z] $$\nIn our case, $X=B_{u}$, $Y=B_{s}$, and $Z=B_{t}$.\n\nFirst, we compute the required conditional expectations, $E[B_{u} \\mid B_{t}]$ and $E[B_{s} \\mid B_{t}]$.\n\nFor $E[B_{u} \\mid B_{t}]$, given $u > t$:\nWe can write $B_{u} = B_{t} + (B_{u} - B_{t})$. By linearity of conditional expectation:\n$$ E[B_{u} \\mid B_{t}] = E[B_{t} \\mid B_{t}] + E[B_{u} - B_{t} \\mid B_{t}] $$\nSince $B_{t}$ is given (it's the conditioning variable), $E[B_{t} \\mid B_{t}] = B_{t}$.\nThe increment $B_{u} - B_{t}$ is independent of the history of the process up to time $t$, denoted $\\mathcal{F}_{t}$. Since $B_{t}$ is $\\mathcal{F}_{t}$-measurable, the increment $B_{u} - B_{t}$ is independent of $B_{t}$. Therefore:\n$$ E[B_{u} - B_{t} \\mid B_{t}] = E[B_{u} - B_{t}] = 0 $$\nCombining these results, we get:\n$$ E[B_{u} \\mid B_{t}] = B_{t} + 0 = B_{t} $$\n\nFor $E[B_{s} \\mid B_{t}]$, given $s  t$:\nThe vector $(B_{s}, B_{t})$ has a bivariate normal distribution with mean vector $(0, 0)^{T}$. The covariance matrix is determined by $\\operatorname{Cov}(B_r, B_q) = \\min(r, q)$.\nThus, $\\operatorname{Var}(B_{s}) = s$, $\\operatorname{Var}(B_{t}) = t$, and $\\operatorname{Cov}(B_{s}, B_{t}) = s$.\nThe general formula for conditional expectation in a bivariate normal distribution $(X, Y)$ is $E[X \\mid Y=y] = E[X] + \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(Y)}(y - E[Y])$.\nApplying this with $X=B_s$ and $Y=B_t$:\n$$ E[B_{s} \\mid B_{t}] = E[B_s] + \\frac{\\operatorname{Cov}(B_{s}, B_{t})}{\\operatorname{Var}(B_{t})}(B_{t} - E[B_t]) = 0 + \\frac{s}{t}(B_{t} - 0) = \\frac{s}{t} B_{t} $$\n\nNow, we substitute these conditional expectations into the definition of conditional covariance:\n$$ \\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t}) = E\\left[ \\left(B_{u} - B_{t}\\right) \\left(B_{s} - \\frac{s}{t} B_{t}\\right) \\mid B_{t} \\right] $$\nLet's analyze the expression inside the expectation. Let $I_1 = B_{u} - B_{t}$ and $I_2 = B_{s} - \\frac{s}{t} B_{t}$.\nThe term $I_1 = B_{u} - B_{t}$ is an increment of the process over the interval $[t, u]$. Due to the independent increments property, this increment is independent of the entire history of the process up to time $t$, in particular, it is independent of the sigma-algebra $\\mathcal{F}_{t} = \\sigma(\\{B_{r} : r \\leq t\\})$.\nThe term $I_2 = B_{s} - \\frac{s}{t} B_{t}$ is a function of $B_s$ and $B_t$. Since both $s  t$ and $t \\le t$, both $B_s$ and $B_t$ are $\\mathcal{F}_{t}$-measurable. Therefore, $I_2$ is $\\mathcal{F}_{t}$-measurable.\nSince $I_1$ is independent of $\\mathcal{F}_t$ and $I_2$ is $\\mathcal{F}_t$-measurable, they are independent.\nMore specifically, for conditioning on just $B_t$, the increment $B_u - B_t$ is independent of the pair $(B_s, B_t)$ because $s  t  u$. Therefore, $B_u - B_t$ is independent of any function of $(B_s, B_t)$, which includes $I_2$.\nThe conditional expectation of a product of conditionally independent terms (given $B_t$) is the product of their conditional expectations:\n$$ \\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t}) = E[I_1 \\mid B_t] E[I_2 \\mid B_t] $$\nWe evaluate the first factor:\n$$ E[I_1 \\mid B_t] = E[B_{u} - B_{t} \\mid B_{t}] $$\nAs shown before, the increment $B_u-B_t$ is independent of $B_t$ and has mean $0$.\n$$ E[B_{u} - B_{t} \\mid B_{t}] = E[B_{u} - B_{t}] = 0 $$\nSince one factor is zero, the entire product is zero:\n$$ \\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t}) = 0 \\cdot E[I_2 \\mid B_t] = 0 $$\n\nAlternatively, using the properties of multivariate normal distributions, the vector $(B_s, B_t, B_u)^T$ is a zero-mean Gaussian vector with covariance matrix $\\Sigma$ where $\\Sigma_{ij} = \\min(t_i, t_j)$. For times $(s,t,u)$, this is:\n$$ \\Sigma = \\begin{pmatrix} s  s  s \\\\ s  t  t \\\\ s  t  u \\end{pmatrix} $$\nWe want the conditional covariance of $(B_s, B_u)$ given $B_t$. Let the vector be partitioned as $X_1 = (B_s, B_u)^T$ and $X_2 = B_t$. We rearrange the vector to $(B_s, B_u, B_t)^T$ to apply the standard block matrix formula. The corresponding covariance matrix is:\n$$ \\Sigma' = \\begin{pmatrix} s  s  s \\\\ s  u  t \\\\ s  t  t \\end{pmatrix} = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix} $$\nwhere $\\Sigma_{11} = \\begin{pmatrix} s  s \\\\ s  u \\end{pmatrix}$, $\\Sigma_{12} = \\begin{pmatrix} s \\\\ t \\end{pmatrix}$, $\\Sigma_{21} = \\begin{pmatrix} s  t \\end{pmatrix}$, and $\\Sigma_{22} = t$.\nThe conditional covariance matrix of $X_1$ given $X_2$ is $\\Sigma_{11.2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$.\n$$ \\Sigma_{11.2} = \\begin{pmatrix} s  s \\\\ s  u \\end{pmatrix} - \\begin{pmatrix} s \\\\ t \\end{pmatrix} (t^{-1}) \\begin{pmatrix} s  t \\end{pmatrix} = \\begin{pmatrix} s  s \\\\ s  u \\end{pmatrix} - \\frac{1}{t} \\begin{pmatrix} s^2  st \\\\ st  t^2 \\end{pmatrix} = \\begin{pmatrix} s - \\frac{s^2}{t}  0 \\\\ 0  u-t \\end{pmatrix} $$\nThe conditional covariance $\\operatorname{Cov}(B_{u}, B_{s} \\mid B_{t})$ is the off-diagonal element of this matrix, which is $0$.\n\nNow, for the explanation of the reduction in covariance:\nThe unconditional covariance is $\\operatorname{Cov}(B_{u}, B_{s}) = \\min(u, s) = s$. This positive covariance arises because both $B_s$ and $B_u$ trajectories share the common path segment from time $0$ to time $s$. A large fluctuation on this early segment tends to affect the values of the process at both later times $s$ and $u$ in a similar way.\nConditioning on $B_t$, where $s  t  u$, fundamentally alters the probabilistic structure due to the Markov property of Brownian motion. The Markov property implies that, given the state of the process at the present time $t$ (i.e., the value of $B_t$), the future evolution of the process (for times greater than $t$) is independent of its past (for times less than $t$).\nThe random variable $B_s$ is determined by the path of the process in the \"past\" interval $[0, s]$, which is part of the history before time $t$. The random variable $B_u$ is determined by the path up to time $u$. We can write $B_u = B_t + (B_u - B_t)$. So, given $B_t$, the value of $B_u$ is determined by the \"future\" increment $B_u - B_t$.\nThe Markov property ensures that the past (which determines $B_s$) and the future (which determines the increment $B_u - B_t$) are independent, conditional on the present ($B_t$). Consequently, $B_s$ and $B_u$ are conditionally independent given $B_t$.\nWhen two random variables are conditionally independent, their conditional covariance is zero. Thus, conditioning on the intermediate value $B_t$ \"breaks\" the dependence between $B_s$ and $B_u$ that was caused by their shared initial path. All the information from the past relevant to the future is encapsulated in the present state $B_t$, rendering the specific path taken to reach $B_s$ irrelevant for predicting $B_u$. This reduces the covariance from $s$ to $0$.", "answer": "$$\\boxed{0}$$", "id": "3047221"}]}