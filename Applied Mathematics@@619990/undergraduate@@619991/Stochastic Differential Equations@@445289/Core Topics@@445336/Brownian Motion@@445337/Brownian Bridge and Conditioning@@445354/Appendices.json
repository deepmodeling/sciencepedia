{"hands_on_practices": [{"introduction": "The first step in understanding any new stochastic process is to characterize its fundamental statistical properties. This exercise invites you to compute the mean and variance for a standard Brownian bridge, which is a Brownian motion path starting at $0$ and conditioned to return to $0$ at a future time $T$. By deriving these first two moments directly from the definition of a Brownian motion, you will uncover the basic structure of the bridge's uncertainty, which grows from zero and then shrinks back to zero as it approaches its terminal point. [@problem_id:3042151]", "problem": "Let $\\{B_{t}\\}_{t \\ge 0}$ be a one-dimensional standard Brownian motion with $B_{0} = 0$ almost surely, Gaussian increments of mean $0$, and covariance $\\mathbb{E}[B_{s} B_{t}] = \\min\\{s,t\\}$ for all $s,t \\ge 0$. Fix a deterministic time horizon $T > 0$ and define the process $\\{X_{t}\\}_{0 \\le t \\le T}$ by\n$$\nX_{t} := B_{t} - \\frac{t}{T} B_{T}, \\quad 0 \\le t \\le T.\n$$\nThis process is known as the Brownian bridge from $0$ to $0$ over $[0,T]$ and can be interpreted as Brownian motion conditioned to be at $0$ at time $T$.\n\nUsing only the foundational properties of standard Brownian motion stated above, compute the mean $\\mathbb{E}[X_{t}]$ and the variance $\\operatorname{Var}(X_{t})$ for $0 \\le t \\le T$. Provide your final expressions in exact symbolic form as functions of $t$ and $T$. Your final answer must be a single analytical expression; report the pair $\\big(\\mathbb{E}[X_{t}], \\operatorname{Var}(X_{t})\\big)$ as a row vector using the $\\text{pmatrix}$ convention.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides a clear definition of a one-dimensional standard Brownian bridge and asks for the computation of its first two moments using the fundamental properties of the underlying Brownian motion. The given information is complete and consistent, allowing for a unique and meaningful solution.\n\nWe proceed to compute the mean and variance of the process $\\{X_{t}\\}_{0 \\le t \\le T}$ defined by\n$$\nX_{t} := B_{t} - \\frac{t}{T} B_{T}\n$$\nwhere $\\{B_{t}\\}_{t \\ge 0}$ is a standard Brownian motion.\n\nFirst, we compute the mean, $\\mathbb{E}[X_{t}]$. By the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}[X_{t}] = \\mathbb{E}\\left[B_{t} - \\frac{t}{T} B_{T}\\right] = \\mathbb{E}[B_{t}] - \\frac{t}{T} \\mathbb{E}[B_{T}]\n$$\nFor a standard Brownian motion, the increment $B_{s} - B_{0}$ from time $0$ to time $s$ has a mean of $0$. Given that $B_{0} = 0$ almost surely, we have $\\mathbb{E}[B_{0}] = 0$. Thus, for any $s \\ge 0$:\n$$\n\\mathbb{E}[B_{s}] = \\mathbb{E}[B_{s} - B_{0}] + \\mathbb{E}[B_{0}] = 0 + 0 = 0\n$$\nApplying this result for $s=t$ and $s=T$, we find $\\mathbb{E}[B_{t}] = 0$ and $\\mathbb{E}[B_{T}] = 0$. Substituting these into the expression for the mean of $X_{t}$:\n$$\n\\mathbb{E}[X_{t}] = 0 - \\frac{t}{T}(0) = 0\n$$\nTherefore, the mean of the Brownian bridge process $X_{t}$ is $0$ for all $t \\in [0, T]$.\n\nNext, we compute the variance, $\\operatorname{Var}(X_{t})$. The variance of a random variable $Y$ is defined as $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. Since we have just shown that $\\mathbb{E}[X_{t}] = 0$, the variance simplifies to:\n$$\n\\operatorname{Var}(X_{t}) = \\mathbb{E}[X_{t}^2] - (\\mathbb{E}[X_{t}])^2 = \\mathbb{E}[X_{t}^2] - 0^2 = \\mathbb{E}[X_{t}^2]\n$$\nWe substitute the definition of $X_{t}$ and expand the square:\n$$\n\\operatorname{Var}(X_{t}) = \\mathbb{E}\\left[\\left(B_{t} - \\frac{t}{T} B_{T}\\right)^2\\right] = \\mathbb{E}\\left[B_{t}^2 - 2 \\frac{t}{T} B_{t} B_{T} + \\left(\\frac{t}{T}\\right)^2 B_{T}^2\\right]\n$$\nUsing the linearity of expectation again, we get:\n$$\n\\operatorname{Var}(X_{t}) = \\mathbb{E}[B_{t}^2] - 2 \\frac{t}{T} \\mathbb{E}[B_{t} B_{T}] + \\frac{t^2}{T^2} \\mathbb{E}[B_{T}^2]\n$$\nThe problem provides the covariance function of a standard Brownian motion: $\\mathbb{E}[B_{s} B_{u}] = \\min\\{s, u\\}$. We apply this to evaluate the three expectation terms:\n\\begin{enumerate}\n    \\item $\\mathbb{E}[B_{t}^2] = \\mathbb{E}[B_{t} B_{t}] = \\min\\{t, t\\} = t$\n    \\item $\\mathbb{E}[B_{T}^2] = \\mathbb{E}[B_{T} B_{T}] = \\min\\{T, T\\} = T$\n    \\item $\\mathbb{E}[B_{t} B_{T}] = \\min\\{t, T\\}$. Since the process $X_{t}$ is defined for $0 \\le t \\le T$, we have $t \\le T$, which implies $\\min\\{t, T\\} = t$.\n\\end{enumerate}\nSubstituting these results back into the expression for the variance:\n$$\n\\operatorname{Var}(X_{t}) = t - 2 \\frac{t}{T} (t) + \\frac{t^2}{T^2} (T)\n$$\nSimplifying the algebraic expression:\n$$\n\\operatorname{Var}(X_{t}) = t - \\frac{2t^2}{T} + \\frac{t^2}{T} = t - \\frac{t^2}{T}\n$$\nThis can be written in factored form as:\n$$\n\\operatorname{Var}(X_{t}) = t \\left(1 - \\frac{t}{T}\\right)\n$$\nThis expression is valid for all $t$ in the interval $[0, T]$. Note that the variance is $0$ at both $t=0$ and $t=T$, which is consistent with the fact that the Brownian bridge is fixed at $X_{0}=0$ and $X_{T}=0$.\n\nIn summary, for the Brownian bridge process $X_{t}$ on the interval $[0, T]$, the mean is $\\mathbb{E}[X_{t}] = 0$ and the variance is $\\operatorname{Var}(X_{t}) = t(1 - \\frac{t}{T})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & t \\left( 1 - \\frac{t}{T} \\right) \\end{pmatrix}}\n$$", "id": "3042151"}, {"introduction": "While a standard bridge pinned at zero is a crucial theoretical object, practical applications often require modeling paths between arbitrary points. This practice generalizes our analysis to a Brownian bridge connecting a starting value $a$ at time $0$ to an ending value $b$ at time $T$. Using the powerful tool of conditioning on jointly normal random variables, you will discover how the expected path of the bridge becomes a simple straight line between the endpoints, while the variance structure remarkably remains identical to that of the standard bridge. [@problem_id:3042166]", "problem": "Let $W_t$ be a standard Brownian motion, defined as a continuous-time stochastic process with $W_0=0$, independent increments, and for each $t \\geq 0$ the increment $W_t-W_s$ is normally distributed with mean $0$ and variance $t-s$ for $0 \\leq s < t$. Consider the process $X_t=a+W_t$ for fixed $a \\in \\mathbb{R}$, and fix a terminal time $T>0$ and a terminal value $b \\in \\mathbb{R}$. Define the Brownian bridge from $a$ to $b$ over the interval $[0,T]$ as the conditioned process $\\{X_t:0 \\leq t \\leq T\\}$ given the event $X_T=b$. Using only the fundamental properties of Brownian motion and conditioning of jointly normal random variables, derive the expectation $\\mathbb{E}[X_t \\mid X_T=b]$ and the variance $\\operatorname{Var}(X_t \\mid X_T=b)$ for an arbitrary $t \\in [0,T]$. The final answer should be written as a single closed-form analytic expression that contains both $\\mathbb{E}[X_t \\mid X_T=b]$ and $\\operatorname{Var}(X_t \\mid X_T=b)$.", "solution": "### Step 1: Extract Givens\n-   $W_t$ is a standard Brownian motion:\n    -   $W_0 = 0$\n    -   Continuous-time stochastic process\n    -   Independent increments\n    -   For $0 \\leq s < t$, the increment $W_t - W_s$ is normally distributed with mean $0$ and variance $t-s$.\n-   A stochastic process $X_t$ is defined as $X_t = a + W_t$ for a fixed constant $a \\in \\mathbb{R}$.\n-   $T > 0$ is a fixed terminal time.\n-   $b \\in \\mathbb{R}$ is a fixed terminal value.\n-   The Brownian bridge is the process $\\{X_t : 0 \\leq t \\leq T\\}$ conditioned on the event $X_T = b$.\n-   The task is to derive the conditional expectation $\\mathbb{E}[X_t \\mid X_T=b]$ and the conditional variance $\\operatorname{Var}(X_t \\mid X_T=b)$ for an arbitrary $t \\in [0,T]$.\n-   The derivation must use only fundamental properties of Brownian motion and the theory of conditioning for jointly normal random variables.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n1.  **Scientifically Grounded:** The problem is firmly based on the standard mathematical theory of stochastic processes, specifically the definition of Brownian motion and Brownian bridges. These are well-established concepts in probability theory and financial mathematics. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** The problem is well-posed. It asks for the first two conditional moments of a stochastic process at a specific time, given a condition at another time. The framework of Gaussian processes guarantees that a unique and meaningful solution exists.\n3.  **Objective:** The language is precise, formal, and free of any subjectivity or ambiguity.\n4.  **Complete and Consistent:** All necessary information and definitions (standard Brownian motion, the process $X_t$, the conditioning event) are provided. There are no contradictions.\n5.  **Topic Relevance:** The problem is directly on the topic of *Brownian bridge and conditioning* within the field of *stochastic differential equations*.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe core of the problem is to find the conditional distribution of the random variable $X_t$ given the value of the random variable $X_T$. Since $X_t$ and $X_T$ are linear transformations of a Gaussian process ($W_t$), they are jointly normally distributed. We can, therefore, use the standard formulas for conditional Gaussian distributions.\n\nLet the random vector be $(Y_1, Y_2)^T$, where $Y_1 = X_t$ and $Y_2 = X_T$. This vector follows a bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\Sigma$:\n$$\n\\begin{pmatrix} Y_1 \\\\ Y_2 \\end{pmatrix} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_2^2 \\end{pmatrix} \\right)\n$$\nThe conditional distribution of $Y_1$ given $Y_2 = y_2$ is also normal, with the following expectation and variance:\n$$\n\\mathbb{E}[Y_1 \\mid Y_2 = y_2] = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_2^2} (y_2 - \\mu_2)\n$$\n$$\n\\operatorname{Var}(Y_1 \\mid Y_2 = y_2) = \\sigma_1^2 - \\frac{\\sigma_{12}^2}{\\sigma_2^2}\n$$\n\nOur task is to compute the parameters $\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2,$ and $\\sigma_{12}$ for the specific case where $Y_1 = X_t$ and $Y_2 = X_T$. The conditioning event is $X_T = b$.\n\n**1. Compute the Mean Vector**\nThe expectation of $X_t$ is:\n$$\n\\mu_1 = \\mathbb{E}[X_t] = \\mathbb{E}[a + W_t] = a + \\mathbb{E}[W_t] = a + 0 = a\n$$\nSimilarly, the expectation of $X_T$ is:\n$$\n\\mu_2 = \\mathbb{E}[X_T] = \\mathbb{E}[a + W_T] = a + \\mathbb{E}[W_T] = a + 0 = a\n$$\nSo, the mean vector is $\\boldsymbol{\\mu} = (a, a)^T$.\n\n**2. Compute the Covariance Matrix**\nThe components of the covariance matrix $\\Sigma$ are the variances and the covariance of $X_t$ and $X_T$.\n\nThe variance of $X_t$ is:\n$$\n\\sigma_1^2 = \\operatorname{Var}(X_t) = \\operatorname{Var}(a + W_t) = \\operatorname{Var}(W_t) = t\n$$\nThe variance of $X_T$ is:\n$$\n\\sigma_2^2 = \\operatorname{Var}(X_T) = \\operatorname{Var}(a + W_T) = \\operatorname{Var}(W_T) = T\n$$\nThe covariance between $X_t$ and $X_T$ for $t \\in [0,T]$ (which implies $t \\leq T$) is:\n$$\n\\sigma_{12} = \\operatorname{Cov}(X_t, X_T) = \\operatorname{Cov}(a + W_t, a + W_T) = \\operatorname{Cov}(W_t, W_T)\n$$\nBy definition of covariance:\n$$\n\\operatorname{Cov}(W_t, W_T) = \\mathbb{E}[W_t W_T] - \\mathbb{E}[W_t]\\mathbb{E}[W_T] = \\mathbb{E}[W_t W_T]\n$$\nSince $t \\leq T$, we can write $W_T = W_t + (W_T - W_t)$. The term $(W_T - W_t)$ is the increment of the Brownian motion over $[t, T]$ and is independent of $W_t$ (which depends on the process up to time $t$).\n$$\n\\mathbb{E}[W_t W_T] = \\mathbb{E}[W_t (W_t + W_T - W_t)] = \\mathbb{E}[W_t^2] + \\mathbb{E}[W_t (W_T - W_t)]\n$$\nDue to the independence of increments:\n$$\n\\mathbb{E}[W_t (W_T - W_t)] = \\mathbb{E}[W_t] \\mathbb{E}[W_T - W_t] = 0 \\cdot 0 = 0\n$$\nTherefore, the covariance is:\n$$\n\\sigma_{12} = \\mathbb{E}[W_t^2] = \\operatorname{Var}(W_t) = t\n$$\nSo, the covariance matrix is:\n$$\n\\Sigma = \\begin{pmatrix} t & t \\\\ t & T \\end{pmatrix}\n$$\n\n**3. Compute the Conditional Expectation and Variance**\nNow we substitute these parameters into the general formulas for conditional Gaussians, with $y_2 = b$.\n\nThe conditional expectation $\\mathbb{E}[X_t \\mid X_T=b]$ is:\n$$\n\\mathbb{E}[X_t \\mid X_T=b] = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_2^2} (b - \\mu_2) = a + \\frac{t}{T} (b - a)\n$$\nThis expression can be rewritten as a weighted average of the start and end points:\n$$\n\\mathbb{E}[X_t \\mid X_T=b] = a \\left( 1 - \\frac{t}{T} \\right) + b \\frac{t}{T}\n$$\n\nThe conditional variance $\\operatorname{Var}(X_t \\mid X_T=b)$ is:\n$$\n\\operatorname{Var}(X_t \\mid X_T=b) = \\sigma_1^2 - \\frac{\\sigma_{12}^2}{\\sigma_2^2} = t - \\frac{t^2}{T}\n$$\nThis can be factored as:\n$$\n\\operatorname{Var}(X_t \\mid X_T=b) = t \\left( 1 - \\frac{t}{T} \\right) = \\frac{t(T-t)}{T}\n$$\nThese are the desired quantities. The expectation describes the mean path of the bridge, which is a straight line from $(0, a)$ to $(T, b)$. The variance is a quadratic function of $t$ that is zero at $t=0$ and $t=T$ and maximal at $t=T/2$.\n\nThe final answer combines these two results into a single matrix expression.", "answer": "$$\n\\boxed{\\begin{pmatrix} a \\left( 1 - \\frac{t}{T} \\right) + b \\frac{t}{T} & \\frac{t(T-t)}{T} \\end{pmatrix}}\n$$", "id": "3042166"}, {"introduction": "Theoretical formulas for mean and variance provide a static picture, but simulating sample paths brings the dynamic nature of a stochastic process to life. This capstone practice guides you through the development of a recursive algorithm to generate paths of a Brownian bridge, connecting theory directly to computational practice. You will derive the conditional distribution for a point on the bridge given its neighbors—a key result for simulation—and then implement a midpoint displacement method, using Monte Carlo verification to confirm that your simulated paths have the correct statistical properties at every scale. [@problem_id:3042164]", "problem": "Consider a standard Brownian motion $W_t$ with $W_0=0$ on the interval $[0,T]$. A Brownian bridge $X_t$ from $a$ to $b$ over $[0,T]$ is the process $W_t$ conditioned on the event $X_0=a$ and $X_T=b$. You may use the following facts as the fundamental base: (i) for $0 \\le s \\le t$, $W_t - W_s$ is Gaussian with mean $0$ and variance $t-s$, and the vector $(W_{t_1},\\dots,W_{t_n})$ is jointly Gaussian with mean $0$ and covariance $\\operatorname{Cov}(W_s,W_t)=\\min\\{s,t\\}$; (ii) any linear conditioning of a jointly Gaussian vector is again Gaussian, with conditional mean and covariance obtained from the standard block-matrix formulas for Gaussian conditioning.\n\nYour tasks are:\n\n- Derive, from the fundamental base above, the conditional distribution of $X_m$ for any times $0 \\le \\ell < m < r \\le T$ given the two endpoints $X_\\ell=x_\\ell$ and $X_r=x_r$. In particular, express the conditional mean as a linear function of $x_\\ell$ and $x_r$, and express the conditional variance in terms of the times $\\ell$, $m$, and $r$. Do not assume any formula beyond the fundamental base; derive it explicitly from first principles.\n- Using your derived conditional distribution, present a recursive midpoint simulation scheme that generates a path of the Brownian bridge on a dyadic grid of depth $L$. Starting from the known endpoints $(0,a)$ and $(T,b)$, recursively insert midpoints: at each step, for any current interval $[\\ell,r]$, sample the midpoint at $m=(\\ell+r)/2$ from the conditional Gaussian you have derived, then recurse on the left interval $[\\ell,m]$ and the right interval $[m,r]$ until the desired depth $L$ is reached.\n- Verify, via Monte Carlo, that the conditional variances used at each recursion level are correct. For each level $k \\in \\{0,1,\\dots,L-1\\}$, aggregate the midpoints sampled at that level across many independent realizations. For every sampled midpoint at time $m$ within its parent interval $[\\ell,r]$, compute the squared residual $(X_m - \\mathbb{E}[X_m \\mid X_\\ell, X_r])^2$. Average these squared residuals across all midpoints at level $k$ and across all realizations. Compare this empirical average to the theoretical conditional variance at that level obtained from your derivation, and compute the relative error defined as the absolute difference divided by the theoretical value.\n\nImplementation requirements:\n\n- Implement a program that carries out the recursive midpoint simulation for a Brownian bridge on a dyadic grid using the conditional Gaussian you derived, and performs the variance verification described above for each recursion level.\n- Use a fixed random seed so that the results are reproducible.\n- For each test case, return a boolean indicating whether the maximum relative error across all levels is less than or equal to a specified tolerance.\n\nTest suite:\n\n- Test case $1$: $T=1.0$, $a=0.0$, $b=0.0$, $L=5$, number of independent realizations $M=3500$, tolerance $\\varepsilon=0.06$.\n- Test case $2$: $T=2.0$, $a=1.5$, $b=-0.7$, $L=4$, number of independent realizations $M=3200$, tolerance $\\varepsilon=0.06$.\n- Test case $3$: $T=0.5$, $a=-2.0$, $b=3.0$, $L=4$, number of independent realizations $M=4000$, tolerance $\\varepsilon=0.06$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite, where each entry is a boolean indicating whether all recursion levels for that test case satisfied the tolerance. For example, the output should look like $[{\\rm True},{\\rm False},{\\rm True}]$ if the first and third test cases passed and the second failed.", "solution": "The problem is assessed to be valid. It is a well-posed and scientifically grounded problem in the field of stochastic processes. It requests a rigorous derivation from first principles, the design of a numerical algorithm based on this derivation, and a Monte Carlo verification of the algorithm's properties. The problem statement is clear, objective, and contains all necessary information to proceed.\n\n### 1. Derivation of the Conditional Distribution\n\nWe are asked to find the conditional distribution of a Brownian process $X_m$ at time $m$ given its values $X_\\ell=x_\\ell$ and $X_r=x_r$ at times $\\ell$ and $r$, where $0 \\le \\ell < m < r \\le T$.\n\nThe problem states that a Brownian bridge is derived from a standard Brownian motion $W_t$, which is a Gaussian process. Any finite set of points $(X_{t_1}, X_{t_2}, \\dots, X_{t_n})$ from a Gaussian process forms a jointly Gaussian random vector. Therefore, the vector $(X_\\ell, X_m, X_r)$ is jointly Gaussian. The conditional distribution of one component given others will also be Gaussian.\n\nThe conditional mean and variance for a Gaussian process are independent of any constant shift. We can therefore perform the derivation for a standard Brownian motion $W_t$ with $W_0=0$, and the results will apply directly to $X_t$. We want to find the distribution of $W_m$ conditional on $W_\\ell = w_\\ell$ and $W_r = w_r$.\n\nLet the random vector be partitioned as $Y = (Y_1, Y_2^T)^T$, where $Y_1 = W_m$ and $Y_2 = (W_\\ell, W_r)^T$. This is a jointly Gaussian vector with zero mean, since $\\mathbb{E}[W_t] = 0$ for all $t$. The covariance matrix $\\Sigma$ is determined by $\\operatorname{Cov}(W_s, W_t) = \\min\\{s, t\\}$.\n\nThe covariance matrix is partitioned as:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix} $$\nFor our chosen partitioning, the blocks are:\n- $\\Sigma_{11} = \\operatorname{Var}(W_m) = m$\n- $\\Sigma_{12} = (\\operatorname{Cov}(W_m, W_\\ell), \\operatorname{Cov}(W_m, W_r))$. Since $\\ell < m < r$, this is $(\\ell, m)$.\n- $\\Sigma_{21} = \\Sigma_{12}^T = \\begin{pmatrix} \\ell \\\\ m \\end{pmatrix}$\n- $\\Sigma_{22} = \\begin{pmatrix} \\operatorname{Var}(W_\\ell) & \\operatorname{Cov}(W_\\ell, W_r) \\\\ \\operatorname{Cov}(W_r, W_\\ell) & \\operatorname{Var}(W_r) \\end{pmatrix} = \\begin{pmatrix} \\ell & \\ell \\\\ \\ell & r \\end{pmatrix}$\n\nThe distribution of $Y_1$ conditional on $Y_2 = y_2 = (w_\\ell, w_r)^T$ is Gaussian, $\\mathcal{N}(\\mu_{1|2}, \\Sigma_{1|2})$, with:\n- Conditional mean: $\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (y_2 - \\mu_2)$\n- Conditional variance: $\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$\n\nSince the base process is a standard Brownian motion, the mean vector is zero: $\\mu_1 = 0$ and $\\mu_2 = (0, 0)^T$.\n\nFirst, we find the inverse of $\\Sigma_{22}$:\n$$ \\det(\\Sigma_{22}) = \\ell r - \\ell^2 = \\ell(r-\\ell) $$\n$$ \\Sigma_{22}^{-1} = \\frac{1}{\\ell(r-\\ell)} \\begin{pmatrix} r & -\\ell \\\\ -\\ell & \\ell \\end{pmatrix} $$\n\nNow, we compute the conditional mean $\\mu_{1|2}$:\n$$ \\mu_{1|2} = \\Sigma_{12} \\Sigma_{22}^{-1} y_2 = (\\ell, m) \\frac{1}{\\ell(r-\\ell)} \\begin{pmatrix} r & -\\ell \\\\ -\\ell & \\ell \\end{pmatrix} \\begin{pmatrix} w_\\ell \\\\ w_r \\end{pmatrix} $$\n$$ \\mu_{1|2} = \\frac{1}{\\ell(r-\\ell)} (\\ell r - m\\ell, -\\ell^2 + m\\ell) \\begin{pmatrix} w_\\ell \\\\ w_r \\end{pmatrix} $$\n$$ \\mu_{1|2} = \\frac{1}{\\ell(r-\\ell)} [(\\ell r - m\\ell)w_\\ell + (-\\ell^2 + m\\ell)w_r] $$\n$$ \\mu_{1|2} = \\frac{\\ell(r-m)}{\\ell(r-\\ell)}w_\\ell + \\frac{\\ell(m-\\ell)}{\\ell(r-\\ell)}w_r = \\frac{r-m}{r-\\ell}w_\\ell + \\frac{m-\\ell}{r-\\ell}w_r $$\nThis is the linear interpolation of the values $w_\\ell$ and $w_r$ at time $m$. As this result is independent of any constant shift of the process, we can state for our process $X_t$:\n$$ \\mathbb{E}[X_m \\mid X_\\ell=x_\\ell, X_r=x_r] = \\frac{r-m}{r-\\ell}x_\\ell + \\frac{m-\\ell}{r-\\ell}x_r $$\n\nNext, we compute the conditional variance $\\Sigma_{1|2}$:\n$$ \\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} = m - (\\ell, m) \\frac{1}{\\ell(r-\\ell)} \\begin{pmatrix} r & -\\ell \\\\ -\\ell & \\ell \\end{pmatrix} \\begin{pmatrix} \\ell \\\\ m \\end{pmatrix} $$\n$$ \\Sigma_{1|2} = m - \\frac{1}{\\ell(r-\\ell)} (\\ell r - m\\ell, -\\ell^2 + m\\ell) \\begin{pmatrix} \\ell \\\\ m \\end{pmatrix} $$\n$$ \\Sigma_{1|2} = m - \\frac{1}{\\ell(r-\\ell)} [ \\ell(\\ell r - m\\ell) + m(-\\ell^2 + m\\ell) ] $$\n$$ \\Sigma_{1|2} = m - \\frac{1}{\\ell(r-\\ell)} [ \\ell^2 r - m\\ell^2 - m\\ell^2 + m^2\\ell ] $$\n$$ \\Sigma_{1|2} = m - \\frac{\\ell(\\ell r - 2m\\ell + m^2)}{\\ell(r-\\ell)} = m - \\frac{\\ell r - 2m\\ell + m^2}{r-\\ell} $$\n$$ \\Sigma_{1|2} = \\frac{m(r-\\ell) - (\\ell r - 2m\\ell + m^2)}{r-\\ell} = \\frac{mr - m\\ell - \\ell r + 2m\\ell - m^2}{r-\\ell} $$\n$$ \\Sigma_{1|2} = \\frac{mr + m\\ell - \\ell r - m^2}{r-\\ell} = \\frac{m(r-\\ell) - \\ell(r-m)}{r-\\ell} $$\nThe numerator can be factored as $(m-\\ell)(r-m)$:\n$$ \\operatorname{Var}(X_m \\mid X_\\ell=x_\\ell, X_r=x_r) = \\frac{(m-\\ell)(r-m)}{r-\\ell} $$\nThis variance depends only on the time points $\\ell, m, r$.\n\n### 2. Recursive Midpoint Simulation Scheme\n\nThe simulation generates a path on a dyadic grid of depth $L$. This means we start with the interval $[0, T]$ and recursively subdivide each interval at its midpoint.\nFor any interval $[\\ell, r]$ with known values $X_\\ell=x_\\ell$ and $X_r=x_r$, we sample the value $X_m$ at the midpoint time $m = (\\ell+r)/2$. Using the formulas derived above:\n\nThe conditional mean at the midpoint is:\n$$ \\mathbb{E}[X_m] = \\frac{r - (\\ell+r)/2}{r-\\ell}x_\\ell + \\frac{(\\ell+r)/2 - \\ell}{r-\\ell}x_r = \\frac{(r-\\ell)/2}{r-\\ell}x_\\ell + \\frac{(r-\\ell)/2}{r-\\ell}x_r = \\frac{1}{2}x_\\ell + \\frac{1}{2}x_r = \\frac{x_\\ell+x_r}{2} $$\n\nThe conditional variance at the midpoint is:\n$$ \\operatorname{Var}(X_m) = \\frac{(m-\\ell)(r-m)}{r-\\ell} = \\frac{((\\ell+r)/2 - \\ell)(r - (\\ell+r)/2)}{r-\\ell} = \\frac{((r-\\ell)/2)((r-\\ell)/2)}{r-\\ell} = \\frac{r-\\ell}{4} $$\n\nThe simulation algorithm proceeds as follows:\n1. Initialize a structure (e.g., a dictionary) `points` with the given endpoints: `points` $= \\{0: a, T: b\\}$.\n2. Initialize a queue of intervals to be processed: `queue` $= [(0, T)]$.\n3. For each recursion level $k$ from $0$ to $L-1$:\n    a. Create an empty queue `next_queue` for the next level.\n    b. For each interval $[\\ell, r]$ in the current `queue`:\n        i. Calculate the midpoint time $m = (\\ell+r)/2$.\n        ii. Get the endpoint values $x_\\ell = \\text{points}[\\ell]$ and $x_r = \\text{points}[r]$.\n        iii. Compute the conditional mean $\\mu = (x_\\ell+x_r)/2$.\n        iv. Compute the conditional variance $\\sigma^2 = (r-\\ell)/4$.\n        v. Sample a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n        vi. Generate the new point value $x_m = \\mu + \\sqrt{\\sigma^2} Z$.\n        vii. Store the new point: `points`$[m] = x_m$.\n        viii. Add the two new sub-intervals to the next level's queue: `next_queue.extend([[\\ell, m], [m, r]])`\n    c. Replace `queue` with `next_queue`.\n4. After $L$ levels, `points` contains the simulated path on the dyadic grid.\n\n### 3. Monte Carlo Variance Verification\n\nTo verify the derived conditional variance, we perform $M$ independent simulations of the Brownian bridge path. For each simulation and at each level of recursion, we compute the squared error between the sampled midpoint and its conditional mean.\n\n1.  Initialize a list of lists, `squared_residuals`, with $L$ empty lists, one for each recursion level.\n2.  Repeat the simulation process $M$ times. In each simulation:\n    - At level $k$, for each interval $[\\ell,r]$ being subdivided, a midpoint $x_m$ is generated from $\\mathcal{N}(\\mu, \\sigma^2)$.\n    - The squared residual, $(x_m - \\mu)^2$, is calculated.\n    - This squared residual is appended to `squared_residuals[k]`.\n3.  After $M$ simulations, for each level $k \\in \\{0, 1, \\dots, L-1\\}$:\n    a.  The empirical variance is the average of all values in `squared_residuals[k]`.\n        $$ \\sigma^2_{\\text{emp}, k} = \\frac{1}{|\\text{squared\\_residuals}[k]|} \\sum_{v \\in \\text{squared\\_residuals}[k]} v $$\n    b.  The theoretical variance for level $k$ needs to be determined. At level $k$, all intervals have a length of $T/2^k$. The conditional variance for sampling a midpoint is (interval length)/$4$. Thus, the theoretical variance is:\n        $$ \\sigma^2_{\\text{th}, k} = \\frac{T/2^k}{4} = \\frac{T}{2^{k+2}} $$\n    c.  The relative error is computed as:\n        $$ \\text{err}_{\\text{rel}, k} = \\frac{|\\sigma^2_{\\text{emp}, k} - \\sigma^2_{\\text{th}, k}|}{\\sigma^2_{\\text{th}, k}} $$\n4.  For each test case, the maximum relative error across all levels, $\\max_{k} \\{\\text{err}_{\\text{rel}, k}\\}$, is compared against the given tolerance $\\varepsilon$. The test case passes if this maximum error is less than or equal to $\\varepsilon$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Brownian bridge simulation and verification problem\n    for the given test cases.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, a, b, L, M, tolerance)\n        (1.0, 0.0, 0.0, 5, 3500, 0.06),\n        (2.0, 1.5, -0.7, 4, 3200, 0.06),\n        (0.5, -2.0, 3.0, 4, 4000, 0.06),\n    ]\n\n    results = []\n\n    for T, a, b, L, M, tol in test_cases:\n        # A list of lists to store squared residuals for each level across all realizations.\n        all_squared_residuals = [[] for _ in range(L)]\n\n        # Run M independent Monte Carlo simulations.\n        for _ in range(M):\n            # Dictionary to store the (time, value) points of the bridge path.\n            points = {0.0: a, T: b}\n            # Queue of intervals [l, r] to be subdivided at the current level.\n            # Start with the full interval [0, T].\n            queue = [(0.0, T)]\n\n            # Perform L levels of recursive midpoint subdivision.\n            for k in range(L):\n                next_queue = []\n                # Process each interval in the current level's queue.\n                for l, r in queue:\n                    m = (l + r) / 2.0\n                    \n                    x_l = points[l]\n                    x_r = points[r]\n                    \n                    # Derived conditional mean and variance for the midpoint.\n                    mean = (x_l + x_r) / 2.0\n                    variance = (r - l) / 4.0\n                    std_dev = np.sqrt(variance)\n                    \n                    # Sample a new point from the conditional Gaussian distribution.\n                    Z = rng.normal()\n                    x_m = mean + std_dev * Z\n                    \n                    points[m] = x_m\n                    \n                    # Calculate the squared residual for variance verification.\n                    # The residual is (x_m - E[X_m | ...]).\n                    squared_residual = (x_m - mean)**2\n                    all_squared_residuals[k].append(squared_residual)\n                    \n                    # Add new sub-intervals to the queue for the next level.\n                    next_queue.append((l, m))\n                    next_queue.append((m, r))\n                \n                # Move to the next level of subdivision.\n                queue = next_queue\n        \n        # After all simulations, verify the variance for each level.\n        max_rel_error = 0.0\n        for k in range(L):\n            # Calculate the empirical variance from the collected squared residuals.\n            empirical_variance = np.mean(all_squared_residuals[k])\n            \n            # The theoretical variance for level k, derived as T / 2^(k+2).\n            # This is because intervals at level k have length T / 2^k,\n            # and the midpoint variance is (interval_length) / 4.\n            theoretical_variance = (T / (2**k)) / 4.0\n\n            # Compute the relative error.\n            relative_error = np.abs(empirical_variance - theoretical_variance) / theoretical_variance\n            \n            # Keep track of the maximum relative error found across all levels.\n            if relative_error > max_rel_error:\n                max_rel_error = relative_error\n        \n        # The test case passes if the maximum relative error is within the tolerance.\n        results.append(max_rel_error <= tol)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3042164"}]}