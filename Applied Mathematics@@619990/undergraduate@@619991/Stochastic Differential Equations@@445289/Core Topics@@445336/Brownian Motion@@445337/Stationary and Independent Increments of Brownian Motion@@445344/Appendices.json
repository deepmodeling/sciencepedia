{"hands_on_practices": [{"introduction": "A core application of stochastic processes is modeling phenomena with both random fluctuations and a deterministic trend. This exercise explores the Arithmetic Brownian Motion, $X_t = \\sigma B_t + \\mu t$, which incorporates a volatility parameter $\\sigma$ and a drift rate $\\mu$. By computing the characteristic function of an increment, you will verify that this process still possesses stationary and independent increments and see precisely how these parameters shape its distribution [@problem_id:3076070].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion, meaning a continuous-time stochastic process with $B_{0}=0$, stationary and independent increments, and Gaussian increments satisfying $B_{t}-B_{s} \\sim \\mathcal{N}(0, t-s)$ for all $0 \\leq s  t$. Define the process $X_{t} = \\sigma B_{t} + \\mu t$ for fixed constants $\\sigma  0$ and $\\mu \\in \\mathbb{R}$. For fixed times $0 \\leq s  t$ and frequency $\\xi \\in \\mathbb{R}$, compute the characteristic function\n$$\n\\varphi_{s,t}(\\xi) = \\mathbb{E}\\big[\\exp\\big(i \\xi (X_{t} - X_{s})\\big)\\big].\n$$\nExpress your final answer as a closed-form analytic expression in terms of $\\sigma$, $\\mu$, $t-s$, and $\\xi$. Then, using only the defining properties of Brownian motion and basic properties of Gaussian random variables, explain how the functional form you obtain reflects the stationarity and independence of increments of the process $\\{X_{t}\\}_{t \\geq 0}$. The final answer must be the closed-form expression for $\\varphi_{s,t}(\\xi)$.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   Stochastic process: $\\{B_{t}\\}_{t \\geq 0}$ is a standard Brownian motion.\n-   Properties of $B_t$: It is a continuous-time stochastic process with $B_{0}=0$.\n-   Increments of $B_t$: Stationary and independent.\n-   Distribution of increments of $B_t$: $B_{t}-B_{s} \\sim \\mathcal{N}(0, t-s)$ for all $0 \\leq s  t$.\n-   New process definition: $X_{t} = \\sigma B_{t} + \\mu t$.\n-   Constants: $\\sigma  0$ and $\\mu \\in \\mathbb{R}$.\n-   Time points: $0 \\leq s  t$.\n-   Frequency variable: $\\xi \\in \\mathbb{R}$.\n-   Task 1: Compute the characteristic function $\\varphi_{s,t}(\\xi) = \\mathbb{E}\\big[\\exp\\big(i \\xi (X_{t} - X_{s})\\big)\\big]$.\n-   Task 2: Explain how the resulting functional form reflects the stationarity and independence of increments of the process $\\{X_{t}\\}_{t \\geq 0}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the standard mathematical theory of stochastic processes, specifically Brownian motion and related processes (Itô processes). The definitions and properties provided are standard and form the foundation of stochastic calculus.\n2.  **Well-Posed**: The problem is well-posed. The quantity to be computed is clearly defined, and sufficient information is provided to derive a unique, meaningful solution.\n3.  **Objective**: The language is precise, mathematical, and free of any subjective or ambiguous terms.\n4.  **Completeness**: The problem is self-contained. All necessary definitions and properties of the processes involved are given.\n5.  **No other flaws detected**: The problem does not violate any of the other criteria for validity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n***\n\nThe first step is to compute the characteristic function $\\varphi_{s,t}(\\xi)$. We begin by analyzing the increment of the process $X_t$. For $0 \\leq s  t$, the increment is given by:\n$$\nX_{t} - X_{s} = (\\sigma B_{t} + \\mu t) - (\\sigma B_{s} + \\mu s) = \\sigma(B_{t} - B_{s}) + \\mu(t-s)\n$$\nThe increment $X_{t} - X_{s}$ is composed of a random part, $\\sigma(B_{t} - B_{s})$, and a deterministic part, $\\mu(t-s)$.\n\nNow, we compute the characteristic function by substituting this expression into its definition:\n$$\n\\varphi_{s,t}(\\xi) = \\mathbb{E}\\big[\\exp\\big(i \\xi (X_{t} - X_{s})\\big)\\big] = \\mathbb{E}\\big[\\exp\\big(i \\xi (\\sigma(B_{t} - B_{s}) + \\mu(t-s))\\big)\\big]\n$$\nUsing the property of the exponential function, $\\exp(a+b) = \\exp(a)\\exp(b)$, we can separate the terms:\n$$\n\\varphi_{s,t}(\\xi) = \\mathbb{E}\\big[\\exp\\big(i \\xi \\sigma(B_{t} - B_{s})\\big) \\exp\\big(i \\xi \\mu(t-s)\\big)\\big]\n$$\nThe term $\\exp\\big(i \\xi \\mu(t-s)\\big)$ is deterministic, as $\\mu$, $t$, $s$, and $\\xi$ are fixed constants with respect to the expectation. Therefore, it can be factored out of the expectation:\n$$\n\\varphi_{s,t}(\\xi) = \\exp\\big(i \\xi \\mu(t-s)\\big) \\mathbb{E}\\big[\\exp\\big(i \\xi \\sigma(B_{t} - B_{s})\\big)\\big]\n$$\nThe remaining expectation term is the characteristic function of the random variable $Z = \\sigma(B_{t} - B_{s})$. We must determine the distribution of $Z$.\n\nBy the definition of standard Brownian motion, the increment $B_{t} - B_{s}$ is a Gaussian random variable with mean $0$ and variance $t-s$. That is, $B_{t} - B_{s} \\sim \\mathcal{N}(0, t-s)$.\nThe random variable $Z$ is a scaled version of this increment. For a general random variable $Y$ and a constant $c$, if $Y \\sim \\mathcal{N}(m, v)$, then $cY \\sim \\mathcal{N}(cm, c^2v)$.\nIn our case, $Y = B_t - B_s$ with mean $m=0$ and variance $v=t-s$, and the scaling factor is $c=\\sigma$. Thus, the random variable $Z = \\sigma(B_t - B_s)$ is also Gaussian, with:\n-   Mean: $\\mathbb{E}[Z] = \\sigma \\mathbb{E}[B_t - B_s] = \\sigma \\cdot 0 = 0$.\n-   Variance: $\\text{Var}(Z) = \\sigma^2 \\text{Var}(B_t - B_s) = \\sigma^2(t-s)$.\nSo, $Z \\sim \\mathcal{N}(0, \\sigma^2(t-s))$.\n\nThe characteristic function of a general Gaussian random variable $W \\sim \\mathcal{N}(m, v)$ is given by the formula $\\mathbb{E}[\\exp(i\\theta W)] = \\exp(i\\theta m - \\frac{1}{2}\\theta^2 v)$.\nFor our random variable $Z = \\sigma(B_t - B_s)$, we have $m=0$, $v=\\sigma^2(t-s)$, and the frequency variable is $\\theta=\\xi$. Applying the formula, we get:\n$$\n\\mathbb{E}\\big[\\exp\\big(i \\xi \\sigma(B_{t} - B_{s})\\big)\\big] = \\exp\\left(i\\xi \\cdot 0 - \\frac{1}{2}\\xi^2 \\sigma^2(t-s)\\right) = \\exp\\left(-\\frac{1}{2}\\xi^2 \\sigma^2(t-s)\\right)\n$$\nSubstituting this result back into our expression for $\\varphi_{s,t}(\\xi)$:\n$$\n\\varphi_{s,t}(\\xi) = \\exp\\big(i \\xi \\mu(t-s)\\big) \\exp\\left(-\\frac{1}{2}\\xi^2 \\sigma^2(t-s)\\right)\n$$\nCombining the exponents, we arrive at the final closed-form expression for the characteristic function:\n$$\n\\varphi_{s,t}(\\xi) = \\exp\\left(i \\xi \\mu(t-s) - \\frac{1}{2}\\xi^2 \\sigma^2(t-s)\\right)\n$$\n\nNext, we explain how this functional form reflects the stationarity and independence of the increments of $\\{X_{t}\\}_{t \\geq 0}$.\n\nThe process $\\{X_t\\}$ is an example of a Lévy process, which is defined as a stochastic process with stationary and independent increments. The characteristic function of the increment of any Lévy process over a time interval of duration $\\tau$ must have the form $\\exp(\\tau \\psi(\\xi))$ for some function $\\psi(\\xi)$ called the characteristic exponent.\n\nOur derived characteristic function for the increment $X_t - X_s$ can be written as:\n$$\n\\varphi_{s,t}(\\xi) = \\exp\\left((t-s) \\left[i \\xi \\mu - \\frac{1}{2}\\xi^2 \\sigma^2\\right]\\right)\n$$\nThis expression perfectly matches the required structure for a Lévy process, with time duration $\\tau = t-s$ and characteristic exponent $\\psi(\\xi) = i \\xi \\mu - \\frac{1}{2}\\xi^2 \\sigma^2$. The log-characteristic function, $\\ln(\\varphi_{s,t}(\\xi))$, is linear in the time duration $t-s$. This mathematical structure is a direct consequence of, and thus reflects, the properties of stationary and independent increments.\n\n1.  **Stationary Increments**: A process has stationary increments if the distribution of an increment $X_t - X_s$ depends only on the time difference $t-s$. Since a characteristic function uniquely determines a probability distribution, the fact that $\\varphi_{s,t}(\\xi)$ depends on $s$ and $t$ only through their difference, $t-s$, proves that the distribution of $X_t - X_s$ depends only on $t-s$. For any $h0$, the increment $X_{s+h}-X_s$ has the same distribution as $X_{t+h}-X_t$ for any $s,t \\geq 0$, because their characteristic functions are identical.\n\n2.  **Independent Increments**: The exponential structure $\\varphi_{s,t}(\\xi) = \\exp((t-s)\\psi(\\xi))$ is the key to demonstrating independence. Consider two non-overlapping time intervals, $[t_1, t_2]$ and $[t_3, t_4]$ with $0 \\le t_1  t_2 \\le t_3  t_4$. The increments are independent if the joint characteristic function factorizes:\n    $$\n    \\mathbb{E}\\left[\\exp\\left(i\\xi_1 (X_{t_2}-X_{t_1}) + i\\xi_2 (X_{t_4}-X_{t_3})\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(i\\xi_1 (X_{t_2}-X_{t_1})\\right)\\right] \\mathbb{E}\\left[\\exp\\left(i\\xi_2 (X_{t_4}-X_{t_3})\\right)\\right]\n    $$\n    This factorization holds for $\\{X_t\\}$ because the underlying Brownian increments $B_{t_2}-B_{t_1}$ and $B_{t_4}-B_{t_3}$ are independent. More generally, for any process with stationary and independent increments, the characteristic function must satisfy the functional equation $\\varphi_{t+s}(\\xi) = \\varphi_t(\\xi)\\varphi_s(\\xi)$ for the increment from time $0$. The only continuous solutions to this equation are of the form $\\varphi_t(\\xi) = \\exp(t\\psi(\\xi))$. Our result for the increment $X_t-X_s$ is of the form $\\exp((t-s)\\psi(\\xi))$, which is precisely the form required for a process with stationary and independent increments. Thus, the exponential dependence on the time duration $t-s$ is the signature of these two fundamental properties.\n\nThe characteristic function also tells us the distribution of the increment: $X_t - X_s \\sim \\mathcal{N}(\\mu(t-s), \\sigma^2(t-s))$. This is a drifted Brownian motion.", "answer": "$$\n\\boxed{\\exp\\left(i \\xi \\mu(t-s) - \\frac{1}{2}\\xi^{2} \\sigma^{2} (t-s)\\right)}\n$$", "id": "3076070"}, {"introduction": "To truly appreciate the unique properties of Brownian motion, it is instructive to analyze a process that seems similar but fails to satisfy the key axioms. This problem guides you through the construction of a continuous-path process built from linearly interpolating discrete, independent random steps. By performing direct calculations on its increments, you will discover that this seemingly reasonable construction violates both the stationary and independent increment properties, offering a powerful lesson on the subtleties of stochastic process definitions [@problem_id:3076083].", "problem": "Let $\\{Z_{k}\\}_{k \\geq 1}$ be independent and identically distributed standard normal random variables, each with distribution $\\mathcal{N}(0,1)$. Define a stochastic process $\\{X_{t}\\}_{t \\geq 0}$ by setting $X_{0}=0$, $X_{n}=\\sum_{k=1}^{n} Z_{k}$ for each integer $n \\geq 1$, and for non-integer times by linear interpolation on each unit interval: for $t \\in [n,n+1)$ with integer $n \\geq 0$, define\n$$\nX_{t} \\equiv X_{n} + (t-n)\\,Z_{n+1}.\n$$\nThis construction yields continuous, piecewise linear sample paths whose slopes on $[n,n+1)$ are given by $Z_{n+1}$.\n\nUsing only the independence and normality of the $\\{Z_{k}\\}$ and the linear definition of $X_{t}$, perform the following calculations:\n\n1. Express the increments $X_{1.5}-X_{0.5}$ and $X_{2.5}-X_{1.5}$ as linear combinations of $Z_{1}$, $Z_{2}$, and $Z_{3}$.\n\n2. Compute the covariance $\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big)$.\n\n3. Compute the variances $\\operatorname{Var}\\!\\big(X_{1.5}-X_{0.5}\\big)$ and $\\operatorname{Var}\\!\\big(X_{1}-X_{0}\\big)$.\n\nThen, based on these computed quantities, determine whether the process $\\{X_{t}\\}$ has independent increments across adjacent disjoint intervals and whether its increments are stationary with respect to time shifts. In particular, identify which of the standard axioms for Brownian motion (zero start, almost sure continuity, independent increments, stationary increments with Gaussian law depending only on length) is violated by this process at non-integer times. Do not provide this identification in your final numerical answer; include it only in your reasoning.\n\nProvide your final numerical results for the three quantities from parts $2$ and $3$ in the following order as a single row matrix:\n$$\n\\bigg(\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big),\\ \\operatorname{Var}\\!\\big(X_{1.5}-X_{0.5}\\big),\\ \\operatorname{Var}\\!\\big(X_{1}-X_{0}\\big)\\bigg).\n$$\nExpress your final answer in exact form. No rounding is required.", "solution": "The problem requires the calculation of several statistical properties of a stochastic process $\\{X_{t}\\}_{t \\geq 0}$ defined via linear interpolation of a discrete-time random walk. The process is built from a sequence of independent and identically distributed (i.i.d.) standard normal random variables, $\\{Z_{k}\\}_{k \\geq 1}$, where each $Z_{k} \\sim \\mathcal{N}(0,1)$. By definition, for any $k \\geq 1$, the expectation is $\\mathbb{E}[Z_k]=0$ and the variance is $\\operatorname{Var}(Z_k)=1$. Due to independence, the covariance is $\\operatorname{Cov}(Z_i, Z_j) = \\delta_{ij}$ for any $i,j \\geq 1$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe process $X_t$ is defined by:\n$X_0 = 0$.\nFor an integer $n \\geq 1$, $X_n = \\sum_{k=1}^{n} Z_k$.\nFor a non-integer time $t$ in the interval $[n, n+1)$, where $n \\geq 0$ is an integer, the process is defined by linear interpolation:\n$$X_{t} = X_{n} + (t-n)Z_{n+1}$$\n\n**1. Expressing Increments as Linear Combinations**\n\nWe first compute the values of the process at the specified times $t=0.5$, $t=1.5$, and $t=2.5$.\n\nFor $t=0.5$, we are in the interval $[0,1)$, which corresponds to $n=0$:\n$$X_{0.5} = X_0 + (0.5 - 0)Z_{0+1} = 0 + 0.5 Z_1 = 0.5 Z_1$$\n\nFor $t=1.5$, we are in the interval $[1,2)$, which corresponds to $n=1$:\n$$X_{1.5} = X_1 + (1.5 - 1)Z_{1+1} = X_1 + 0.5 Z_2$$\nSince $X_1 = \\sum_{k=1}^{1} Z_k = Z_1$, this becomes:\n$$X_{1.5} = Z_1 + 0.5 Z_2$$\n\nFor $t=2.5$, we are in the interval $[2,3)$, which corresponds to $n=2$:\n$$X_{2.5} = X_2 + (2.5 - 2)Z_{2+1} = X_2 + 0.5 Z_3$$\nSince $X_2 = \\sum_{k=1}^{2} Z_k = Z_1 + Z_2$, this becomes:\n$$X_{2.5} = (Z_1 + Z_2) + 0.5 Z_3$$\n\nNow we can compute the required increments.\nThe first increment is $X_{1.5} - X_{0.5}$:\n$$X_{1.5} - X_{0.5} = (Z_1 + 0.5 Z_2) - (0.5 Z_1) = 0.5 Z_1 + 0.5 Z_2$$\n\nThe second increment is $X_{2.5} - X_{1.5}$:\n$$X_{2.5} - X_{1.5} = ((Z_1 + Z_2) + 0.5 Z_3) - (Z_1 + 0.5 Z_2) = 0.5 Z_2 + 0.5 Z_3$$\n\n**2. Computing the Covariance**\n\nWe need to compute $\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big)$. Using the expressions derived above and the bilinearity of the covariance operator:\n$$\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big) = \\operatorname{Cov}\\!\\big(0.5 Z_1 + 0.5 Z_2,\\, 0.5 Z_2 + 0.5 Z_3\\big)$$\n$$= \\operatorname{Cov}(0.5 Z_1, 0.5 Z_2) + \\operatorname{Cov}(0.5 Z_1, 0.5 Z_3) + \\operatorname{Cov}(0.5 Z_2, 0.5 Z_2) + \\operatorname{Cov}(0.5 Z_2, 0.5 Z_3)$$\n$$= (0.5)^2 \\operatorname{Cov}(Z_1, Z_2) + (0.5)^2 \\operatorname{Cov}(Z_1, Z_3) + (0.5)^2 \\operatorname{Var}(Z_2) + (0.5)^2 \\operatorname{Cov}(Z_2, Z_3)$$\nSince $\\{Z_k\\}$ are independent, $\\operatorname{Cov}(Z_i, Z_j) = 0$ for $i \\neq j$. Also, $\\operatorname{Var}(Z_k)=1$ for all $k$.\n$$= (0.25)(0) + (0.25)(0) + (0.25)(1) + (0.25)(0) = 0.25$$\nThus, $\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big) = \\frac{1}{4}$.\n\n**3. Computing the Variances**\n\nFirst, we compute $\\operatorname{Var}\\!\\big(X_{1.5}-X_{0.5}\\big)$. Using the property $\\operatorname{Var}(aX+bY) = a^2\\operatorname{Var}(X) + b^2\\operatorname{Var}(Y) + 2ab\\operatorname{Cov}(X,Y)$:\n$$\\operatorname{Var}\\!\\big(X_{1.5}-X_{0.5}\\big) = \\operatorname{Var}\\!\\big(0.5 Z_1 + 0.5 Z_2\\big)$$\n$$= (0.5)^2 \\operatorname{Var}(Z_1) + (0.5)^2 \\operatorname{Var}(Z_2) + 2(0.5)(0.5)\\operatorname{Cov}(Z_1, Z_2)$$\nDue to the independence of $Z_1$ and $Z_2$, $\\operatorname{Cov}(Z_1, Z_2)=0$.\n$$= (0.25)(1) + (0.25)(1) + 0 = 0.5$$\nThus, $\\operatorname{Var}\\!\\big(X_{1.5}-X_{0.5}\\big) = \\frac{1}{2}$.\n\nSecond, we compute $\\operatorname{Var}\\!\\big(X_{1}-X_{0}\\big)$.\nBy definition, $X_1 = Z_1$ and $X_0 = 0$.\n$$X_1 - X_0 = Z_1 - 0 = Z_1$$\nTherefore, the variance is:\n$$\\operatorname{Var}\\!\\big(X_{1}-X_{0}\\big) = \\operatorname{Var}(Z_1) = 1$$\n\n**Analysis of Brownian Motion Axioms**\n\nThe axioms for a standard Brownian motion $\\{B_t\\}_{t \\geq 0}$ include properties that this process $\\{X_t\\}$ violates.\nThe axiom of **independent increments** states that for any $0 \\leq t_1  t_2  t_3  t_4$, the increments $B_{t_2}-B_{t_1}$ and $B_{t_4}-B_{t_3}$ are independent. A necessary condition for independence is zero covariance. We consider the adjacent disjoint intervals $[0.5, 1.5]$ and $[1.5, 2.5]$. We calculated the covariance of the corresponding increments to be $\\operatorname{Cov}\\!\\big(X_{1.5}-X_{0.5},\\,X_{2.5}-X_{1.5}\\big) = \\frac{1}{4}$. Since the covariance is non-zero, the increments are not independent, and this axiom is violated.\n\nThe axiom of **stationary increments** implies that the distribution of an increment $B_{t+h}-B_t$ depends only on the time difference $h$, not on the start time $t$. A consequence is that $\\operatorname{Var}(B_{t+h}-B_t)$ depends only on $h$. We examine two intervals of the same length $h=1$: $[0,1]$ and $[0.5, 1.5]$. We found $\\operatorname{Var}(X_1 - X_0) = 1$ and $\\operatorname{Var}(X_{1.5} - X_{0.5}) = \\frac{1}{2}$. Since these variances are not equal for intervals of the same length, the process $\\{X_t\\}$ does not have stationary increments, and this axiom is also violated.\n\nThe final numerical results are $\\frac{1}{4}$, $\\frac{1}{2}$, and $1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4}  \\frac{1}{2}  1 \\end{pmatrix}}\n$$", "id": "3076083"}, {"introduction": "Many real-world phenomena, from the diffusion of particles in a fluid to the movement of asset prices, evolve in multiple dimensions. This practice extends the concepts of stationary and independent increments to a multi-dimensional Brownian motion, where each component follows its own independent random path. By deriving the joint characteristic function for a vector of increments, you will provide a rigorous confirmation of how independence manifests both across different time intervals and between spatial components, a key insight captured by the process's block-diagonal covariance matrix [@problem_id:3076105].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a $d$-dimensional Brownian motion, meaning $B_{t} = (B_{t}^{(1)}, \\dots, B_{t}^{(d)})$ where each $B_{t}^{(j)}$ is a standard one-dimensional Brownian motion and the components are mutually independent. Let $0 = t_{0}  t_{1}  \\dots  t_{n}$ be fixed times, and define the $\\mathbb{R}^{nd}$-valued random vector $X = (X_{1}, \\dots, X_{n})$ with $X_{k} := B_{t_{k}} - B_{t_{k-1}} \\in \\mathbb{R}^{d}$ for $k \\in \\{1, \\dots, n\\}$. Starting only from the defining properties of Brownian motion (stationary and independent increments, Gaussian marginals with mean $0$, variance proportional to elapsed time, and independence across components), compute the joint characteristic function $\\varphi(u_{1}, \\dots, u_{n})$ of the vector $(X_{1}, \\dots, X_{n})$, where each $u_{k} \\in \\mathbb{R}^{d}$, defined by\n$$\n\\varphi(u_{1}, \\dots, u_{n}) := \\mathbb{E}\\!\\left[\\exp\\!\\left(i \\sum_{k=1}^{n} \\langle u_{k}, X_{k} \\rangle \\right)\\right],\n$$\nwith $\\langle \\cdot, \\cdot \\rangle$ denoting the Euclidean inner product in $\\mathbb{R}^{d}$. In your derivation, identify the covariance matrix of the $\\mathbb{R}^{nd}$-valued Gaussian vector $(X_{1}, \\dots, X_{n})$ and explain how independence across disjoint intervals and across components arises from the structure of this covariance. Express your final answer as the closed-form expression for the characteristic function $\\varphi(u_{1}, \\dots, u_{n})$. No rounding is required. Use the standard Euclidean norm $|\\cdot|$ on $\\mathbb{R}^{d}$.", "solution": "The problem is to compute the joint characteristic function of the increments of a $d$-dimensional Brownian motion over a sequence of time intervals. Let $\\{B_{t}\\}_{t \\geq 0}$ be a $d$-dimensional Brownian motion, where $B_{t} = (B_{t}^{(1)}, \\dots, B_{t}^{(d)})$. The components $B_{t}^{(j)}$ are independent, standard one-dimensional Brownian motions. We are given times $0 = t_{0}  t_{1}  \\dots  t_{n}$ and define the increments $X_{k} = B_{t_{k}} - B_{t_{k-1}}$ for $k \\in \\{1, \\dots, n\\}$. Each $X_{k}$ is a vector in $\\mathbb{R}^{d}$. We wish to compute the joint characteristic function of the $\\mathbb{R}^{nd}$-valued random vector $(X_{1}, \\dots, X_{n})$, defined for $u_{1}, \\dots, u_{n} \\in \\mathbb{R}^{d}$ as\n$$\n\\varphi(u_{1}, \\dots, u_{n}) := \\mathbb{E}\\!\\left[\\exp\\!\\left(i \\sum_{k=1}^{n} \\langle u_{k}, X_{k} \\rangle \\right)\\right]\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ is the Euclidean inner product.\n\nThe derivation will proceed by applying the defining properties of Brownian motion directly. The fundamental properties we use are:\n$1$. **Independent increments**: For any choice of disjoint time intervals $[s_{1}, t_{1}]$ and $[s_{2}, t_{2}]$, the increments $B_{t_{1}} - B_{s_{1}}$ and $B_{t_{2}} - B_{s_{2}}$ are independent random vectors.\n$2$. **Stationary increments**: The distribution of an increment $B_{t} - B_{s}$ depends only on the time difference $t - s$.\n$3$. **Gaussian marginals**: For any time $t  0$, the vector $B_{t}$ is a multivariate Gaussian random vector. For a standard $1$-D Brownian motion $B_{t}^{(j)}$, $B_{t}^{(j)} \\sim \\mathcal{N}(0, t)$.\n$4$. **Component independence**: The component processes $\\{B_{t}^{(j)}\\}_{t \\geq 0}$ for $j \\in \\{1, \\dots, d\\}$ are mutually independent.\n\nThe summation in the exponent can be written as a product of exponentials:\n$$\n\\varphi(u_{1}, \\dots, u_{n}) = \\mathbb{E}\\!\\left[\\prod_{k=1}^{n} \\exp\\!\\left(i \\langle u_{k}, X_{k} \\rangle \\right)\\right]\n$$\nThe increments $X_{k} = B_{t_{k}} - B_{t_{k-1}}$ are defined over the time intervals $[t_{k-1}, t_{k}]$. Since $t_{0}  t_{1}  \\dots  t_{n}$, these intervals are disjoint (they only meet at endpoints). By the property of independent increments, the random vectors $X_{1}, X_{2}, \\dots, X_{n}$ are mutually independent. Therefore, the expectation of the product is the product of the expectations:\n$$\n\\varphi(u_{1}, \\dots, u_{n}) = \\prod_{k=1}^{n} \\mathbb{E}\\!\\left[\\exp\\!\\left(i \\langle u_{k}, X_{k} \\rangle \\right)\\right]\n$$\nThis reduces the problem to finding the characteristic function of each individual increment vector $X_{k}$. Let $\\varphi_{X_{k}}(u_{k}) = \\mathbb{E}\\!\\left[\\exp\\!\\left(i \\langle u_{k}, X_{k} \\rangle \\right)\\right]$.\n\nNow, we analyze $\\varphi_{X_{k}}(u_{k})$. The inner product $\\langle u_{k}, X_{k} \\rangle$ can be expanded in terms of components:\n$$\n\\langle u_{k}, X_{k} \\rangle = \\sum_{j=1}^{d} u_{k}^{(j)} X_{k}^{(j)}\n$$\nwhere $u_{k} = (u_{k}^{(1)}, \\dots, u_{k}^{(d)})$ and $X_{k} = (X_{k}^{(1)}, \\dots, X_{k}^{(d)})$. The components of the increment are $X_{k}^{(j)} = B_{t_{k}}^{(j)} - B_{t_{k-1}}^{(j)}$.\nThe characteristic function for $X_{k}$ is then:\n$$\n\\varphi_{X_{k}}(u_{k}) = \\mathbb{E}\\!\\left[\\exp\\!\\left(i \\sum_{j=1}^{d} u_{k}^{(j)} X_{k}^{(j)} \\right)\\right] = \\mathbb{E}\\!\\left[\\prod_{j=1}^{d} \\exp\\!\\left(i u_{k}^{(j)} X_{k}^{(j)} \\right)\\right]\n$$\nBy the component independence property of the $d$-dimensional Brownian motion, the scalar processes $\\{B_{t}^{(j)}\\}_{t \\geq 0}$ are independent. This implies that their increments, the random variables $\\{X_{k}^{(j)}\\}_{j=1}^{d}$, are also mutually independent for a fixed $k$. Thus, we can again separate the expectation:\n$$\n\\varphi_{X_{k}}(u_{k}) = \\prod_{j=1}^{d} \\mathbb{E}\\!\\left[\\exp\\!\\left(i u_{k}^{(j)} X_{k}^{(j)} \\right)\\right]\n$$\nThis further reduces our problem to finding the characteristic function of a single scalar increment $X_{k}^{(j)}$.\n\nLet $\\Delta t_{k} = t_{k} - t_{k-1}$. The random variable $X_{k}^{(j)} = B_{t_{k}}^{(j)} - B_{t_{k-1}}^{(j)}$ is an increment of a standard $1$-D Brownian motion over an interval of length $\\Delta t_{k}$. By the stationary increments property, the distribution of $X_{k}^{(j)}$ is the same as the distribution of $B_{\\Delta t_{k}}^{(j)} - B_{0}^{(j)}$. Since $B_{0}^{(j)}=0$ a.s., this is the distribution of $B_{\\Delta t_{k}}^{(j)}$. From the Gaussian marginals property of standard $1$-D Brownian motion, $B_{\\Delta t_{k}}^{(j)}$ follows a Gaussian distribution with mean $0$ and variance $\\Delta t_{k}$. That is, $X_{k}^{(j)} \\sim \\mathcal{N}(0, \\Delta t_{k})$.\n\nThe characteristic function of a a general Gaussian random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[e^{i \\tau Z}] = \\exp(i\\tau\\mu - \\frac{1}{2}\\tau^2\\sigma^2)$. For $X_{k}^{(j)}$, we have $\\mu=0$ and $\\sigma^2 = \\Delta t_k$. Setting the parameter $\\tau = u_{k}^{(j)}$, we get:\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(i u_{k}^{(j)} X_{k}^{(j)} \\right)\\right] = \\exp\\!\\left(i u_{k}^{(j)} \\cdot 0 - \\frac{1}{2} (u_{k}^{(j)})^2 \\Delta t_{k}\\right) = \\exp\\!\\left(-\\frac{1}{2} (u_{k}^{(j)})^2 \\Delta t_{k}\\right)\n$$\nNow, we substitute this result back to find $\\varphi_{X_{k}}(u_{k})$:\n$$\n\\varphi_{X_{k}}(u_{k}) = \\prod_{j=1}^{d} \\exp\\!\\left(-\\frac{1}{2} (u_{k}^{(j)})^2 \\Delta t_{k}\\right) = \\exp\\!\\left(-\\frac{1}{2} \\Delta t_{k} \\sum_{j=1}^{d} (u_{k}^{(j)})^2\\right)\n$$\nThe sum $\\sum_{j=1}^{d} (u_{k}^{(j)})^2$ is the squared Euclidean norm of the vector $u_{k}$, denoted by $|u_{k}|^2$. So,\n$$\n\\varphi_{X_{k}}(u_{k}) = \\exp\\!\\left(-\\frac{1}{2} \\Delta t_{k} |u_{k}|^2\\right)\n$$\nFinally, we substitute this into the expression for the joint characteristic function $\\varphi(u_{1}, \\dots, u_{n})$:\n$$\n\\varphi(u_{1}, \\dots, u_{n}) = \\prod_{k=1}^{n} \\varphi_{X_{k}}(u_{k}) = \\prod_{k=1}^{n} \\exp\\!\\left(-\\frac{1}{2} \\Delta t_{k} |u_{k}|^2\\right) = \\exp\\!\\left(-\\frac{1}{2} \\sum_{k=1}^{n} \\Delta t_{k} |u_{k}|^2\\right)\n$$\nSubstituting $\\Delta t_{k} = t_{k} - t_{k-1}$, we arrive at the final expression:\n$$\n\\varphi(u_{1}, \\dots, u_{n}) = \\exp\\!\\left(-\\frac{1}{2} \\sum_{k=1}^{n} (t_{k} - t_{k-1}) |u_{k}|^2\\right)\n$$\n\nThe problem also asks to identify the covariance matrix of the $\\mathbb{R}^{nd}$-valued vector $X = (X_{1}, \\dots, X_{n})$ and explain the independence structure. The vector $X$ is composed of linear operations on a Gaussian process, so it is a multivariate Gaussian vector. A Gaussian vector is fully characterized by its mean vector and covariance matrix.\nThe mean of each component is $\\mathbb{E}[X_{k}^{(j)}] = 0$, so the mean of the entire vector $X$ is the zero vector in $\\mathbb{R}^{nd}$.\nThe covariance matrix $\\Sigma$ of $X$ is an $nd \\times nd$ matrix, which can be viewed as an $n \\times n$ block matrix where each block $(k, l)$ is a $d \\times d$ matrix given by $\\text{Cov}(X_{k}, X_{l}) = \\mathbb{E}[X_{k}X_{l}^T]$. The entries of this block are $\\text{Cov}(X_{k}^{(i)}, X_{l}^{(j)}) = \\mathbb{E}[X_{k}^{(i)} X_{l}^{(j)}]$ for $i, j \\in \\{1, \\dots, d\\}$.\n\nLet's compute these entries:\n$1$. Case $i \\neq j$: The scalar processes $B_{t}^{(i)}$ and $B_{t}^{(j)}$ are independent. Thus, any increments $X_{k}^{(i)}$ and $X_{l}^{(j)}$ derived from them are independent for all $k,l$. Since their means are zero, their covariance is zero: $\\text{Cov}(X_{k}^{(i)}, X_{l}^{(j)}) = \\mathbb{E}[X_{k}^{(i)}]\\mathbb{E}[X_{l}^{(j)}] = 0$.\n\n$2$. Case $i = j$: We compute $\\text{Cov}(X_{k}^{(i)}, X_{l}^{(i)})$. For simplicity, let's drop the superscript $i$. We are considering increments of a single standard $1$-D Brownian motion.\nIf $k \\neq l$, assume $k  l$. The time intervals $[t_{k-1}, t_{k}]$ and $[t_{l-1}, t_{l}]$ are disjoint. By the independent increments property, $X_{k}$ and $X_{l}$ are independent. As their means are zero, their covariance is zero: $\\text{Cov}(X_{k}, X_{l}) = \\mathbb{E}[X_k X_l] - \\mathbb{E}[X_k]\\mathbb{E}[X_l] = 0 - 0 = 0$.\nIf $k = l$, we compute the variance: $\\text{Cov}(X_{k}, X_{k}) = \\text{Var}(X_{k}) = \\text{Var}(B_{t_{k}} - B_{t_{k-1}})$. By stationary increments, this is $\\text{Var}(B_{t_{k}-t_{k-1}}) = t_{k}-t_{k-1} = \\Delta t_{k}$.\n\nCombining these results, the covariance is:\n$$\n\\text{Cov}(X_{k}^{(i)}, X_{l}^{(j)}) = \\delta_{ij} \\delta_{kl} \\Delta t_{k}\n$$\nwhere $\\delta$ is the Kronecker delta. This shows that the full $nd \\times nd$ covariance matrix $\\Sigma$ is diagonal.\nWe can also describe it in block form. The $(k, l)$-th block is $\\text{Cov}(X_{k}, X_{l})$. For $k \\neq l$, this is the $d \\times d$ zero matrix. For $k=l$, the block is $\\text{Cov}(X_{k}, X_{k})$, whose $(i,j)$ entry is $\\delta_{ij}\\Delta t_k$. Thus, $\\text{Cov}(X_{k}, X_{k}) = (\\Delta t_k) I_{d}$, where $I_{d}$ is the $d\\times d$ identity matrix.\nThe full covariance matrix $\\Sigma$ is block-diagonal:\n$$\n\\Sigma = \\begin{pmatrix}\n(\\Delta t_{1})I_{d}  0  \\cdots  0 \\\\\n0  (\\Delta t_{2})I_{d}  \\cdots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\cdots  (\\Delta t_{n})I_{d}\n\\end{pmatrix}\n$$\n\nThe structure of this covariance matrix explains the independence properties observed. For a multivariate Gaussian vector, sub-vectors are independent if and only if the corresponding off-diagonal blocks in the covariance matrix are zero.\n- **Independence across disjoint intervals**: The vectors $X_{k}$ and $X_{l}$ ($k \\neq l$) correspond to increments over disjoint time intervals. The fact that the block-covariance $\\text{Cov}(X_k, X_l)$ is the zero matrix for $k \\neq l$ confirms their mutual independence. This is a manifestation of the independent increments property of Brownian motion.\n- **Independence across components**: For any fixed increment $X_k$, its covariance matrix is $\\text{Cov}(X_k,X_k) = (\\Delta t_k)I_d$, which is a diagonal matrix. The zero off-diagonal entries, $\\text{Cov}(X_k^{(i)}, X_k^{(j)}) = 0$ for $i \\neq j$, confirm that the components of the increment vector $X_k$ are mutually independent. This reflects the independence of the component Brownian motion processes.\n\nThe characteristic function of a general multivariate normal vector $Z \\sim \\mathcal{N}(\\mu, \\Sigma_Z)$ is $\\varphi_{Z}(u) = \\exp(i u^{T}\\mu - \\frac{1}{2} u^{T}\\Sigma_Z u)$. For our vector $X=(X_1, \\dots, X_n)$ with mean $\\mu=0$ and parameter vector $u=(u_1, \\dots, u_n)$, the term $u^T \\Sigma u$ becomes $\\sum_{k=1}^n u_k^T ((\\Delta t_k)I_d) u_k = \\sum_{k=1}^n (\\Delta t_k) u_k^T u_k = \\sum_{k=1}^n (t_k-t_{k-1})|u_k|^2$. This confirms our derived characteristic function.", "answer": "$$\n\\boxed{\\exp\\!\\left(-\\frac{1}{2} \\sum_{k=1}^{n} (t_{k} - t_{k-1}) |u_{k}|^{2}\\right)}\n$$", "id": "3076105"}]}