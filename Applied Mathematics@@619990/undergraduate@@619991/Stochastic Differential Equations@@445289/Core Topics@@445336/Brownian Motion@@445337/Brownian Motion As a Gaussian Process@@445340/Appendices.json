{"hands_on_practices": [{"introduction": "To truly understand Brownian motion, we begin with its most defining characteristic: how it spreads out over time. This exercise grounds us in the fundamental axioms of the process, guiding you to derive a key result directly from first principles. By calculating the variance of the process at time $t$, you will uncover the linear relationship that signifies diffusive scaling, a cornerstone concept in physics and finance [@problem_id:3042310].", "problem": "Consider a standard Brownian motion $\\{B_t\\}_{t \\ge 0}$, defined by the following fundamental properties: $B_0 = 0$, independent and stationary increments, and for all $0 \\le s  t$, the increment $B_t - B_s$ is Gaussian with mean $0$ and variance $t - s$. Let $K(s,t) = \\mathbb{E}[B_s B_t]$ denote the covariance function of the process, so that for each fixed $t \\ge 0$, $K(t,t) = \\mathbb{E}[B_t^2]$.\n\nUsing only these base properties, deduce the correct description of $K(t,t)$ and interpret what it implies about how $\\operatorname{Var}(B_t)$ changes with time. Select the single correct option.\n\nA. $K(t,t)=t$, and for $0 \\le s \\le t$, $\\operatorname{Cov}(B_s, B_t) = t$.\n\nB. $K(t,t)=t$, and therefore $\\operatorname{Var}(B_t)$ increases linearly in $t$, a signature of diffusive scaling.\n\nC. $K(t,t)=\\sqrt{t}$, which implies sublinear growth of $\\operatorname{Var}(B_t)$.\n\nD. $K(t,t)=t$, but $\\operatorname{Var}(B_t)$ is constant in $t$ because of stationary increments.\n\nE. $K(t,t)=t^2$, corresponding to quadratic growth of $\\operatorname{Var}(B_t)$.", "solution": "The problem statement defines a standard Brownian motion $\\{B_t\\}_{t \\ge 0}$ and its covariance function $K(s,t) = \\mathbb{E}[B_s B_t]$. The task is to determine the form of $K(t,t)$ and its physical interpretation regarding the variance of the process, $\\operatorname{Var}(B_t)$.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- The process is a standard Brownian motion, $\\{B_t\\}_{t \\ge 0}$.\n- Property 1: $B_0 = 0$.\n- Property 2: Increments are independent and stationary.\n- Property 3: For any $0 \\le s  t$, the increment $B_t - B_s$ follows a Gaussian distribution with mean $0$ and variance $t-s$. This can be written as $B_t - B_s \\sim \\mathcal{N}(0, t-s)$.\n- Definition 1: The covariance function is $K(s,t) = \\mathbb{E}[B_s B_t]$.\n- Definition 2: $K(t,t) = \\mathbb{E}[B_t^2]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective. It provides the standard axiomatic definition of a Wiener process (standard Brownian motion). The definitions and properties listed are textbook-standard and are sufficient to uniquely determine the quantities in question. There are no internal contradictions, ambiguities, or reliance on unstated assumptions. The problem is a standard exercise in the theory of stochastic processes.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed to the solution.\n\nOur objective is to compute $K(t,t) = \\mathbb{E}[B_t^2]$ and interpret the result in terms of $\\operatorname{Var}(B_t)$. To do this, we must first determine the mean and variance of the random variable $B_t$ for an arbitrary time $t \\ge 0$.\n\n1.  **Determine the mean of $B_t$:**\n    The process starts at $B_0 = 0$. We can express the position at time $t$, $B_t$, as the increment from time $s=0$ to time $t$.\n    Using Property 3 with $s=0$, the increment $B_t - B_0$ is a Gaussian random variable with mean $0$ and variance $t-0=t$.\n    So, $\\mathbb{E}[B_t - B_0] = 0$.\n    By the linearity of the expectation operator and the fact that $B_0=0$ (a constant), we have:\n    $$ \\mathbb{E}[B_t] - \\mathbb{E}[B_0] = \\mathbb{E}[B_t] - 0 = \\mathbb{E}[B_t] $$\n    Therefore, for all $t \\ge 0$, the mean of the process is $\\mathbb{E}[B_t] = 0$.\n\n2.  **Determine the variance of $B_t$:**\n    The variance of a random variable $X$ is defined as $\\operatorname{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$.\n    For $B_t$, we have $\\mathbb{E}[B_t] = 0$, so its variance is:\n    $$ \\operatorname{Var}(B_t) = \\mathbb{E}[(B_t - 0)^2] = \\mathbb{E}[B_t^2] $$\n    From Property 3, with $s=0$, the variance of the increment $B_t - B_0$ is given as $t-0=t$.\n    $$ \\operatorname{Var}(B_t - B_0) = t $$\n    Since $B_0=0$ is a constant, it has zero variance, and thus $\\operatorname{Var}(B_t - B_0) = \\operatorname{Var}(B_t)$.\n    Therefore, we conclude that $\\operatorname{Var}(B_t) = t$.\n\n3.  **Calculate $K(t,t)$:**\n    The problem defines $K(t,t) = \\mathbb{E}[B_t^2]$.\n    From our analysis of the variance in step 2, we found that $\\operatorname{Var}(B_t) = \\mathbb{E}[B_t^2]$.\n    Combining the results from step 2, we have:\n    $$ K(t,t) = \\mathbb{E}[B_t^2] = \\operatorname{Var}(B_t) = t $$\n    So, we have established that $K(t,t) = t$.\n\n4.  **Interpret the result:**\n    We found that $\\operatorname{Var}(B_t) = t$. This means that the variance of the position of the particle undergoing Brownian motion increases linearly with time $t$. The variance is the mean squared displacement (since the mean is zero), so $\\mathbb{E}[B_t^2] = t$. The root mean square displacement is therefore $\\sqrt{\\mathbb{E}[B_t^2]} = \\sqrt{t}$. This scaling, where the characteristic displacement grows with the square root of time, is the fundamental signature of a diffusion process, and this behavior is known as diffusive scaling.\n\nNow, we evaluate each of the given options.\n\n**A. $K(t,t)=t$, and for $0 \\le s \\le t$, $\\operatorname{Cov}(B_s, B_t) = t$.**\nThe first part, $K(t,t)=t$, is correct.\nThe second part claims that the covariance $\\operatorname{Cov}(B_s, B_t) = t$ for $0 \\le s \\le t$. Let's compute this value from first principles.\n$\\operatorname{Cov}(B_s, B_t) = \\mathbb{E}[B_s B_t] - \\mathbb{E}[B_s]\\mathbb{E}[B_t]$. Since $\\mathbb{E}[B_s]=\\mathbb{E}[B_t]=0$, this simplifies to $\\operatorname{Cov}(B_s, B_t) = \\mathbb{E}[B_s B_t]$.\nFor $s \\le t$, we can write $B_t = B_s + (B_t - B_s)$.\n$$ \\mathbb{E}[B_s B_t] = \\mathbb{E}[B_s (B_s + B_t - B_s)] = \\mathbb{E}[B_s^2] + \\mathbb{E}[B_s (B_t - B_s)] $$\nWe already know $\\mathbb{E}[B_s^2] = K(s,s) = s$.\nBy Property 2 (independent increments), the increment $B_s - B_0 = B_s$ is independent of the increment $B_t - B_s$.\nTherefore, $\\mathbb{E}[B_s (B_t - B_s)] = \\mathbb{E}[B_s] \\mathbb{E}[B_t - B_s]$.\nWe know $\\mathbb{E}[B_s]=0$ and $\\mathbb{E}[B_t-B_s]=0$. Thus, $\\mathbb{E}[B_s (B_t - B_s)] = 0 \\cdot 0 = 0$.\nSubstituting these back, we get $\\operatorname{Cov}(B_s, B_t) = s + 0 = s$.\nThe statement claims $\\operatorname{Cov}(B_s, B_t) = t$, which is incorrect for any $s  t$.\nThe correct general form is $\\operatorname{Cov}(B_s, B_t) = \\min(s,t)$.\nTherefore, this option is **Incorrect**.\n\n**B. $K(t,t)=t$, and therefore $\\operatorname{Var}(B_t)$ increases linearly in $t$, a signature of diffusive scaling.**\nThe first part, $K(t,t)=t$, is correct as derived above.\nThe implication is that $\\operatorname{Var}(B_t) = K(t,t) = t$, because $\\mathbb{E}[B_t]=0$. This shows that the variance increases linearly with $t$. As explained in our interpretation, this linear growth of the mean squared displacement corresponds precisely to diffusive scaling. This statement is entirely correct.\nTherefore, this option is **Correct**.\n\n**C. $K(t,t)=\\sqrt{t}$, which implies sublinear growth of $\\operatorname{Var}(B_t)$.**\nThe first part, $K(t,t)=\\sqrt{t}$, is incorrect. We rigorously derived $K(t,t)=t$.\nTherefore, this option is **Incorrect**.\n\n**D. $K(t,t)=t$, but $\\operatorname{Var}(B_t)$ is constant in $t$ because of stationary increments.**\nThe first part, $K(t,t)=t$, is correct.\nThe second part, \"$\\operatorname{Var}(B_t)$ is constant in $t$\", is factually incorrect and contradicts the first part. We have $\\operatorname{Var}(B_t) = K(t,t) = t$, which is a function of $t$, not a constant.\nThe reasoning \"because of stationary increments\" is a misinterpretation of the property. Stationary increments means that for any $h0$, the distribution of the increment $B_{t+h} - B_t$ is independent of $t$. It does not imply that the variance of the position $B_t$ itself is constant.\nTherefore, this option is **Incorrect**.\n\n**E. $K(t,t)=t^2$, corresponding to quadratic growth of $\\operatorname{Var}(B_t)$.**\nThe first part, $K(t,t)=t^2$, is incorrect. We derived $K(t,t)=t$. Quadratic growth of variance ($\\operatorname{Var}(X_t) \\propto t^2$) is characteristic of ballistic motion, not diffusive motion.\nTherefore, this option is **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "3042310"}, {"introduction": "The 'Gaussian' in 'Gaussian process' is not just a label; it is a property with profound implications. One of the most critical is that for Gaussian random variables, being uncorrelated implies they are also independent. This exercise challenges you to explore what happens when we relax the Gaussian assumption by constructing a process with uncorrelated but dependent increments, revealing just how special and powerful the Gaussian framework truly is [@problem_id:3042328].", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Brownian motion and let $U$ be a real-valued random variable independent of $\\{W_{t}\\}_{t \\geq 0}$ with distribution $\\mathbb{P}(U=\\sqrt{3}) = \\frac{1}{6}$, $\\mathbb{P}(U=-\\sqrt{3}) = \\frac{1}{6}$, and $\\mathbb{P}(U=0) = \\frac{2}{3}$. Define the continuous-time process $\\{X_{t}\\}_{t \\geq 0}$ by $X_{t} = U W_{t}$.\n\nUsing only the defining properties of Brownian motion and basic properties of independence and expectation, do the following:\n\n1) Prove that the increments over disjoint intervals are uncorrelated by computing $\\operatorname{Cov}(X_{2} - X_{1}, X_{4} - X_{2})$.\n\n2) Show that these increments are not independent by computing the covariance of their squares, $\\operatorname{Cov}\\big((X_{2} - X_{1})^{2}, (X_{4} - X_{2})^{2}\\big)$, and interpreting its sign. Your computation must explicitly use the distribution of $U$ and properties of moments of Gaussian random variables.\n\nProvide as your final answer the exact value of $\\operatorname{Cov}\\big((X_{2} - X_{1})^{2}, (X_{4} - X_{2})^{2}\\big)$. No rounding is required. Express the final answer as a pure number with no units.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded within the mathematical theory of stochastic processes, well-posed with sufficient and consistent information, and objectively formulated. All terms are standard and clearly defined. The problem constitutes a non-trivial exercise in applying the properties of Brownian motion and statistical independence.\n\nWe are given a standard Brownian motion $\\{W_{t}\\}_{t \\geq 0}$, an independent random variable $U$ with distribution $\\mathbb{P}(U=\\sqrt{3}) = \\frac{1}{6}$, $\\mathbb{P}(U=-\\sqrt{3}) = \\frac{1}{6}$, and $\\mathbb{P}(U=0) = \\frac{2}{3}$, and a process $X_{t} = U W_{t}$.\n\nFirst, we compute some moments of $U$ which will be necessary for the calculations.\nThe expectation of $U$ is:\n$$ \\mathbb{E}[U] = (\\sqrt{3})\\mathbb{P}(U=\\sqrt{3}) + (-\\sqrt{3})\\mathbb{P}(U=-\\sqrt{3}) + (0)\\mathbb{P}(U=0) = \\sqrt{3} \\cdot \\frac{1}{6} - \\sqrt{3} \\cdot \\frac{1}{6} + 0 = 0 $$\nThe second moment of $U$ (or expectation of $U^2$) is:\n$$ \\mathbb{E}[U^2] = (\\sqrt{3})^{2}\\mathbb{P}(U=\\sqrt{3}) + (-\\sqrt{3})^{2}\\mathbb{P}(U=-\\sqrt{3}) + (0)^{2}\\mathbb{P}(U=0) = 3 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 0 = \\frac{6}{6} = 1 $$\nThe fourth moment of $U$ (or expectation of $U^4$) is:\n$$ \\mathbb{E}[U^4] = (\\sqrt{3})^{4}\\mathbb{P}(U=\\sqrt{3}) + (-\\sqrt{3})^{4}\\mathbb{P}(U=-\\sqrt{3}) + (0)^{4}\\mathbb{P}(U=0) = 9 \\cdot \\frac{1}{6} + 9 \\cdot \\frac{1}{6} + 0 = \\frac{18}{6} = 3 $$\n\nThe defining properties of a standard Brownian motion $\\{W_{t}\\}_{t \\geq 0}$ that we will use are:\n1. $W_0 = 0$.\n2. For any $s  t$, the increment $W_t - W_s$ is a Gaussian random variable with mean $0$ and variance $t-s$, i.e., $W_t - W_s \\sim \\mathcal{N}(0, t-s)$.\n3. For any non-overlapping time intervals $[t_1, t_2]$ and $[t_3, t_4]$, the increments $W_{t_2} - W_{t_1}$ and $W_{t_4} - W_{t_3}$ are independent random variables.\n\n**1) Uncorrelated Increments**\n\nWe need to compute $\\operatorname{Cov}(X_{2} - X_{1}, X_{4} - X_{2})$. The covariance is defined as:\n$$ \\operatorname{Cov}(X_{2} - X_{1}, X_{4} - X_{2}) = \\mathbb{E}[(X_{2} - X_{1})(X_{4} - X_{2})] - \\mathbb{E}[X_{2} - X_{1}]\\mathbb{E}[X_{4} - X_{2}] $$\nLet's compute the expectations of the increments first.\n$$ \\mathbb{E}[X_{2} - X_{1}] = \\mathbb{E}[U W_{2} - U W_{1}] = \\mathbb{E}[U (W_{2} - W_{1})] $$\nSince $U$ and the process $\\{W_{t}\\}_{t \\geq 0}$ are independent, we can separate the expectations:\n$$ \\mathbb{E}[U (W_{2} - W_{1})] = \\mathbb{E}[U] \\mathbb{E}[W_{2} - W_{1}] $$\nFrom the properties of Brownian motion, $\\mathbb{E}[W_2 - W_1] = 0$. Thus,\n$$ \\mathbb{E}[X_{2} - X_{1}] = \\mathbb{E}[U] \\cdot 0 = 0 $$\nSimilarly, for the other increment:\n$$ \\mathbb{E}[X_{4} - X_{2}] = \\mathbb{E}[U(W_4 - W_2)] = \\mathbb{E}[U]\\mathbb{E}[W_4 - W_2] = \\mathbb{E}[U] \\cdot 0 = 0 $$\nNow we compute the expectation of the product:\n$$ \\mathbb{E}[(X_{2} - X_{1})(X_{4} - X_{2})] = \\mathbb{E}[U(W_{2} - W_{1}) \\cdot U(W_{4} - W_{2})] = \\mathbb{E}[U^2 (W_{2} - W_{1})(W_{4} - W_{2})] $$\nUsing the independence of $U$ and $\\{W_t\\}_{t \\ge 0}$:\n$$ \\mathbb{E}[U^2 (W_{2} - W_{1})(W_{4} - W_{2})] = \\mathbb{E}[U^2] \\mathbb{E}[(W_{2} - W_{1})(W_{4} - W_{2})] $$\nThe increments $W_{2} - W_{1}$ and $W_{4} - W_{2}$ correspond to disjoint time intervals $[1, 2]$ and $[2, 4]$, so they are independent. Therefore, the expectation of their product is the product of their expectations:\n$$ \\mathbb{E}[(W_{2} - W_{1})(W_{4} - W_{2})] = \\mathbb{E}[W_{2} - W_{1}] \\mathbb{E}[W_{4} - W_{2}] = 0 \\cdot 0 = 0 $$\nThis implies:\n$$ \\mathbb{E}[(X_{2} - X_{1})(X_{4} - X_{2})] = \\mathbb{E}[U^2] \\cdot 0 = 0 $$\nFinally, the covariance is:\n$$ \\operatorname{Cov}(X_{2} - X_{1}, X_{4} - X_{2}) = 0 - (0)(0) = 0 $$\nThis proves that the increments over disjoint intervals are uncorrelated.\n\n**2) Non-Independent Increments**\n\nWe now compute $\\operatorname{Cov}\\big((X_{2} - X_{1})^{2}, (X_{4} - X_{2})^{2}\\big)$. Let $Z_1 = (X_{2} - X_{1})^{2}$ and $Z_2 = (X_{4} - X_{2})^{2}$. The covariance is:\n$$ \\operatorname{Cov}(Z_1, Z_2) = \\mathbb{E}[Z_1 Z_2] - \\mathbb{E}[Z_1]\\mathbb{E}[Z_2] $$\nFirst, we compute $\\mathbb{E}[Z_1]$ and $\\mathbb{E}[Z_2]$.\n$$ \\mathbb{E}[Z_1] = \\mathbb{E}[(X_{2} - X_{1})^2] = \\mathbb{E}[(U(W_2 - W_1))^2] = \\mathbb{E}[U^2 (W_2 - W_1)^2] $$\nBy independence of $U$ and $W_t$:\n$$ \\mathbb{E}[Z_1] = \\mathbb{E}[U^2] \\mathbb{E}[(W_2 - W_1)^2] $$\nWe know $\\mathbb{E}[U^2] = 1$. The increment $W_2 - W_1 \\sim \\mathcal{N}(0, 2-1) = \\mathcal{N}(0, 1)$. The expectation of the square of a centered normal variable is its variance. So, $\\mathbb{E}[(W_2 - W_1)^2] = \\operatorname{Var}(W_2-W_1) = 1$.\n$$ \\mathbb{E}[Z_1] = 1 \\cdot 1 = 1 $$\nSimilarly for $Z_2$:\n$$ \\mathbb{E}[Z_2] = \\mathbb{E}[(X_{4} - X_{2})^2] = \\mathbb{E}[U^2 (W_4 - W_2)^2] = \\mathbb{E}[U^2] \\mathbb{E}[(W_4 - W_2)^2] $$\nThe increment $W_4 - W_2 \\sim \\mathcal{N}(0, 4-2) = \\mathcal{N}(0, 2)$. Thus, $\\mathbb{E}[(W_4 - W_2)^2] = \\operatorname{Var}(W_4-W_2) = 2$.\n$$ \\mathbb{E}[Z_2] = 1 \\cdot 2 = 2 $$\nThe product of the expectations is $\\mathbb{E}[Z_1]\\mathbb{E}[Z_2] = 1 \\cdot 2 = 2$.\n\nNext, we compute $\\mathbb{E}[Z_1 Z_2]$:\n$$ \\mathbb{E}[Z_1 Z_2] = \\mathbb{E}[ (X_2-X_1)^2 (X_4-X_2)^2 ] = \\mathbb{E}[ U^2(W_2-W_1)^2 \\cdot U^2(W_4-W_2)^2 ] = \\mathbb{E}[ U^4 (W_2-W_1)^2 (W_4-W_2)^2 ] $$\nUsing independence of $U$ and $W_t$:\n$$ \\mathbb{E}[Z_1 Z_2] = \\mathbb{E}[U^4] \\mathbb{E}[ (W_2-W_1)^2 (W_4-W_2)^2 ] $$\nSince the Brownian increments are independent:\n$$ \\mathbb{E}[ (W_2-W_1)^2 (W_4-W_2)^2 ] = \\mathbb{E}[ (W_2-W_1)^2 ] \\mathbb{E}[ (W_4-W_2)^2 ] = 1 \\cdot 2 = 2 $$\nWe calculated earlier that $\\mathbb{E}[U^4] = 3$.\nSo, we have:\n$$ \\mathbb{E}[Z_1 Z_2] = 3 \\cdot 2 = 6 $$\nFinally, we compute the covariance:\n$$ \\operatorname{Cov}\\big((X_{2} - X_{1})^{2}, (X_{4} - X_{2})^{2}\\big) = \\mathbb{E}[Z_1 Z_2] - \\mathbb{E}[Z_1]\\mathbb{E}[Z_2] = 6 - 2 = 4 $$\nSince the covariance is non-zero, the random variables $(X_{2} - X_{1})^{2}$ and $(X_{4} - X_{2})^{2}$ are not independent. If the increments $X_2 - X_1$ and $X_4 - X_2$ were independent, then any function of them, including their squares, would also be independent. As their squares are not independent, the increments themselves are not independent.\n\nThe positive sign of the covariance, $4  0$, indicates a positive correlation. This arises because the magnitude of both squared increments is scaled by the same random factor $U^2$. If a realization of $U^2$ is large (i.e., $U^2=3$), both $(X_2-X_1)^2$ and $(X_4-X_2)^2$ tend to be larger than their respective means. Conversely, if $U^2$ is small (i.e., $U^2=0$), both are smaller. This shared randomness induces the positive correlation and is the fundamental reason for the lack of independence between the increments of $X_t$.\n\nThe final answer required is the value of this covariance.", "answer": "$$\n\\boxed{4}\n$$", "id": "3042328"}, {"introduction": "What is the most likely path a Brownian particle takes, given that we know its location at a starting time $s$ and a future time $u$? This practice explores the elegant structure of the Brownian bridge, a concept that arises from conditioning the process on both past and future information. By applying the rules of conditional expectation for Gaussian variables, you will discover that the expected path is a simple linear interpolation, providing a beautiful and intuitive window into the process's underlying structure [@problem_id:3042318].", "problem": "Consider a standard Brownian motion (also called Wiener process) $\\{B_{r}\\}_{r \\geq 0}$ on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, with natural filtration $\\{\\mathcal{F}_{r}\\}_{r \\geq 0}$, where $\\mathcal{F}_{r}$ denotes the $\\sigma$-algebra generated by $\\{B_{q}: 0 \\leq q \\leq r\\}$ and completed appropriately. Brownian motion is a centered Gaussian process (Gaussian process (GP)) characterized by $\\mathbb{E}[B_{r}] = 0$ for all $r \\geq 0$, $\\operatorname{Var}(B_{r}) = r$, and $\\operatorname{Cov}(B_{r},B_{q}) = \\min\\{r,q\\}$, with independent increments.\n\nFix times $0 \\leq s  t  u$. Using only the defining properties above and properties of jointly Gaussian random variables, compute the conditional expectation $\\mathbb{E}[B_{t} \\mid \\mathcal{F}_{s} \\vee \\sigma(B_{u})]$ explicitly as a function of $B_{s}$ and $B_{u}$. In your derivation, start from the defining properties of Brownian motion as a centered Gaussian process with independent increments and the covariance structure $\\operatorname{Cov}(B_{r},B_{q}) = \\min\\{r,q\\}$, and justify any Gaussian conditioning steps you use. Conclude by interpreting the resulting expression as a linear interpolation in expectation between $B_{s}$ and $B_{u}$, arising from the Gaussian structure.\n\nYour final answer must be a single closed-form analytic expression in terms of $s$, $t$, $u$, $B_{s}$, and $B_{u}$. No numerical approximation is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   The process is a standard Brownian motion (Wiener process) $\\{B_{r}\\}_{r \\geq 0}$ on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$.\n-   The filtration is the natural filtration $\\{\\mathcal{F}_{r}\\}_{r \\geq 0}$, where $\\mathcal{F}_{r} = \\sigma(\\{B_{q}: 0 \\leq q \\leq r\\})$.\n-   Brownian motion is a centered Gaussian process with the following properties:\n    1.  $\\mathbb{E}[B_{r}] = 0$ for all $r \\geq 0$.\n    2.  $\\operatorname{Var}(B_{r}) = r$.\n    3.  $\\operatorname{Cov}(B_{r},B_{q}) = \\min\\{r,q\\}$.\n    4.  It has independent increments.\n-   The times are fixed and ordered as $0 \\leq s  t  u$.\n-   The task is to compute the conditional expectation $\\mathbb{E}[B_{t} \\mid \\mathcal{F}_{s} \\vee \\sigma(B_{u})]$ as an explicit function of $B_{s}$ and $B_{u}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a archetypal question in the theory of stochastic processes, specifically regarding the properties of Brownian motion. All definitions and properties provided are standard and mathematically correct. The problem is firmly rooted in established probability theory.\n-   **Well-Posed**: The problem is clearly stated. It asks for the conditional expectation of a random variable with respect to a well-defined $\\sigma$-algebra. For a Gaussian process like Brownian motion, this quantity is well-defined and admits a unique solution.\n-   **Objective**: The problem is stated in precise, formal mathematical language, devoid of any subjectivity or ambiguity.\n\nThe problem does not violate any of the invalidity criteria. It is a scientifically sound, well-posed, and objective problem within its specified domain.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n***\n\nThe goal is to compute the conditional expectation $\\mathbb{E}[B_{t} \\mid \\mathcal{F}_{s} \\vee \\sigma(B_{u})]$, where $0 \\leq s  t  u$. The conditioning is on the information available up to time $s$ (the filtration $\\mathcal{F}_s$) and the value of the process at a future time $u$ (the $\\sigma$-algebra $\\sigma(B_u)$).\n\nA key property of Brownian motion is that its increments are stationary and independent. This allows us to define a new process starting from time $s$. Let's define a process $\\{Y_{r}\\}_{r \\geq 0}$ as $Y_{r} = B_{s+r} - B_{s}$. Due to the stationary and independent increments property of $\\{B_r\\}$, the process $\\{Y_r\\}$ is itself a standard Brownian motion. Crucially, the process $\\{Y_r\\}$ is independent of the filtration $\\mathcal{F}_s$, since $Y_r$ is constructed from increments of $B$ over the interval $[s, s+r]$, which are independent of the behavior of $B$ up to time $s$.\n\nWe can express $B_t$ and $B_u$ in terms of $B_s$ and the process $Y$:\nSince $s  t  u$, we have $t-s  0$ and $u-s  0$.\n$$B_t = B_s + (B_t - B_s) = B_s + Y_{t-s}$$\n$$B_u = B_s + (B_u - B_s) = B_s + Y_{u-s}$$\n\nNow, we can rewrite the conditional expectation in terms of these new variables. The sigma-algebra of conditioning is $\\mathcal{G} = \\mathcal{F}_{s} \\vee \\sigma(B_{u})$. Substituting $B_u = B_s + Y_{u-s}$ and noting that $B_s$ is $\\mathcal{F}_s$-measurable, we have $\\sigma(B_u) = \\sigma(B_s + Y_{u-s})$. Thus, $\\mathcal{G} = \\mathcal{F}_{s} \\vee \\sigma(B_s + Y_{u-s})$. Since $B_s$ is $\\mathcal{F}_s$-measurable, $\\mathcal{F}_{s} \\vee \\sigma(B_s + Y_{u-s}) = \\mathcal{F}_{s} \\vee \\sigma(Y_{u-s})$.\n\nThe conditional expectation becomes:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = \\mathbb{E}[B_s + Y_{t-s} \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})]$$\nBy the linearity of conditional expectation:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = \\mathbb{E}[B_s \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})] + \\mathbb{E}[Y_{t-s} \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})]$$\nSince $B_s$ is $\\mathcal{F}_s$-measurable, it is measurable with respect to the larger sigma-algebra $\\mathcal{F}_s \\vee \\sigma(Y_{u-s})$. Therefore, $\\mathbb{E}[B_s \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})] = B_s$. The expression simplifies to:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s + \\mathbb{E}[Y_{t-s} \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})]$$\nThe process $\\{Y_r\\}$ is independent of $\\mathcal{F}_s$. Therefore, conditioning on $\\mathcal{F}_s$ provides no information about $Y_{t-s}$, and we can remove $\\mathcal{F}_s$ from the conditioning for the term involving $Y_{t-s}$:\n$$\\mathbb{E}[Y_{t-s} \\mid \\mathcal{F}_s \\vee \\sigma(Y_{u-s})] = \\mathbb{E}[Y_{t-s} \\mid \\sigma(Y_{u-s})] = \\mathbb{E}[Y_{t-s} \\mid Y_{u-s}]$$\nSo, the problem is reduced to:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s + \\mathbb{E}[Y_{t-s} \\mid Y_{u-s}]$$\nLet $r_1 = t-s$ and $r_2 = u-s$. Since $s  t  u$, we have $0  r_1  r_2$. We need to compute $\\mathbb{E}[Y_{r_1} \\mid Y_{r_2}]$, where $\\{Y_r\\}$ is a standard Brownian motion.\n\nSince $\\{Y_r\\}$ is a Gaussian process, the random vector $(Y_{r_1}, Y_{r_2})^T$ has a bivariate normal distribution.\nThe mean vector is $(\\mathbb{E}[Y_{r_1}], \\mathbb{E}[Y_{r_2}])^T = (0, 0)^T$.\nThe covariance matrix $\\Sigma$ is given by:\n$$\n\\Sigma = \\begin{pmatrix}\n\\operatorname{Var}(Y_{r_1})  \\operatorname{Cov}(Y_{r_1}, Y_{r_2}) \\\\\n\\operatorname{Cov}(Y_{r_2}, Y_{r_1})  \\operatorname{Var}(Y_{r_2})\n\\end{pmatrix}\n$$\nUsing the properties of Brownian motion, $\\operatorname{Var}(Y_r) = r$ and $\\operatorname{Cov}(Y_{r_a}, Y_{r_b}) = \\min(r_a, r_b)$.\nSince $r_1  r_2$, the covariance matrix is:\n$$\n\\Sigma = \\begin{pmatrix}\nr_1  r_1 \\\\\nr_1  r_2\n\\end{pmatrix}\n$$\nFor a bivariate normal vector $(X_1, X_2)^T$ with mean $(\\mu_1, \\mu_2)^T$, the conditional expectation of $X_1$ given $X_2 = x_2$ is:\n$$\\mathbb{E}[X_1 \\mid X_2=x_2] = \\mu_1 + \\frac{\\operatorname{Cov}(X_1, X_2)}{\\operatorname{Var}(X_2)} (x_2 - \\mu_2)$$\nApplying this formula to our case with $Y_{r_1}$ and $Y_{r_2}$:\n$$\\mathbb{E}[Y_{r_1} \\mid Y_{r_2}] = \\mathbb{E}[Y_{r_1}] + \\frac{\\operatorname{Cov}(Y_{r_1}, Y_{r_2})}{\\operatorname{Var}(Y_{r_2})} (Y_{r_2} - \\mathbb{E}[Y_{r_2}])$$\n$$\\mathbb{E}[Y_{r_1} \\mid Y_{r_2}] = 0 + \\frac{r_1}{r_2} (Y_{r_2} - 0) = \\frac{r_1}{r_2} Y_{r_2}$$\nSubstituting back $r_1 = t-s$ and $r_2 = u-s$:\n$$\\mathbb{E}[Y_{t-s} \\mid Y_{u-s}] = \\frac{t-s}{u-s} Y_{u-s}$$\nNow we substitute this result back into our main expression for $\\mathbb{E}[B_t \\mid \\mathcal{G}]$:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s + \\frac{t-s}{u-s} Y_{u-s}$$\nFinally, we replace $Y_{u-s}$ with its definition in terms of the original process, $Y_{u-s} = B_u - B_s$:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s + \\frac{t-s}{u-s} (B_u - B_s)$$\nWe can simplify this expression by collecting the terms with $B_s$ and $B_u$:\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s \\left(1 - \\frac{t-s}{u-s}\\right) + B_u \\left(\\frac{t-s}{u-s}\\right)$$\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s \\left(\\frac{u-s - (t-s)}{u-s}\\right) + B_u \\left(\\frac{t-s}{u-s}\\right)$$\n$$\\mathbb{E}[B_t \\mid \\mathcal{G}] = B_s \\frac{u-t}{u-s} + B_u \\frac{t-s}{u-s}$$\n\nThis is the final expression for the conditional expectation.\n\n**Interpretation**:\nThe result, $\\frac{u-t}{u-s} B_s + \\frac{t-s}{u-s} B_u$, is a linear combination of $B_s$ and $B_u$.\nLet's define a parameter $\\alpha = \\frac{t-s}{u-s}$. Since $stu$, we have $0  t-s  u-s$, which implies $0  \\alpha  1$.\nThe expression can be written as $(1-\\alpha)B_s + \\alpha B_u$. This is a weighted average of $B_s$ and $B_u$. The weights, $1-\\alpha = \\frac{u-t}{u-s}$ and $\\alpha = \\frac{t-s}{u-s}$, sum to $1$.\nThis expression represents the value at time $t$ of a straight line connecting the points $(s, B_s)$ and $(u, B_u)$ in the time-value plane.\nThis is the expected path of a Brownian bridge, which is a Brownian motion pinned at two points in time. Given that the process must start at $B_s$ at time $s$ and end at $B_u$ at time $u$, its expected value at an intermediate time $t$ is simply the linear interpolation between these two points. The Gaussian nature of the process is fundamental to this linear result.", "answer": "$$\n\\boxed{\\frac{u-t}{u-s} B_s + \\frac{t-s}{u-s} B_u}\n$$", "id": "3042318"}]}