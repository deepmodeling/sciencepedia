{"hands_on_practices": [{"introduction": "The arcsine distribution can seem abstract and its U-shaped density counter-intuitive. This first practice reveals a surprisingly simple and elegant origin for this law through a geometric interpretation. By performing a change of variables from a uniformly distributed angle, you will derive the arcsine density yourself and see how this representation simplifies probability calculations [@problem_id:3039568].", "problem": "Consider a standard one-dimensional Brownian motion $B_{t}$ with $B_{0}=0$. Define the random variable $T^{+}=\\int_{0}^{1}\\mathbf{1}_{\\{B_{t}0\\}}\\,dt$, the fraction of time the process spends above zero on the unit interval. It is a well-tested fact in the theory of stochastic processes that $T^{+}$ has the same distribution as a $\\mathrm{Beta}(1/2,1/2)$ random variable $X$, known as the arcsine law representation. In this problem, you will justify the angle parametrization behind this representation and then use it to compute a probability of interest. All angles in this problem are measured in radians.\n\nStarting from the fundamental definition of the probability density function (PDF) for the $\\mathrm{Unif}(0,\\pi/2)$ distribution and the change-of-variables formula for transformations of random variables, and using only the standard definition of the $\\mathrm{Beta}(a,b)$ distribution, do the following:\n\n1. Let $\\Theta\\sim\\mathrm{Unif}(0,\\pi/2)$ and define the transformation $X=\\sin^{2}\\Theta$. Using the change-of-variables formula, derive the PDF of $X$ and show that it coincides with the $\\mathrm{Beta}(1/2,1/2)$ PDF on $(0,1)$.\n\n2. Interpret the angle parametrization $X=\\sin^{2}\\Theta$ as a representation of the arcsine law for $T^{+}$ and use it to compute the probability $\\mathbb{P}\\!\\left(T^{+}\\leq \\frac{1}{4}\\right)$ as a single exact number. No rounding is required.\n\nYour final answer must be a single real number.", "solution": "The user-provided problem is valid. It is scientifically grounded in the theory of stochastic processes and probability theory, well-posed with a unique and meaningful solution, and stated objectively. The premises, including the distributional equivalence of the occupation time $T^{+}$ and a $\\mathrm{Beta}(1/2, 1/2)$ random variable, are standard results in the study of Brownian motion, often referred to as Lévy's arcsine law. The tasks are standard mathematical exercises involving a change of variables and probability calculation. We may, therefore, proceed with the solution.\n\nThe solution is presented in two parts, as requested by the problem statement.\n\n### Part 1: Derivation of the PDF for $X = \\sin^{2}\\Theta$\n\nLet $\\Theta$ be a random variable following a uniform distribution on the interval $(0, \\pi/2)$. Its probability density function (PDF), denoted $f_{\\Theta}(\\theta)$, is given by:\n$$\nf_{\\Theta}(\\theta) = \\begin{cases} \\frac{1}{\\pi/2 - 0} = \\frac{2}{\\pi}  \\text{for } \\theta \\in (0, \\pi/2) \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nWe are given the transformation $X = g(\\Theta) = \\sin^{2}\\Theta$. The support of $\\Theta$ is the interval $(0, \\pi/2)$. On this interval, the function $g(\\theta) = \\sin^{2}\\theta$ is strictly increasing. The range of the transformation is from $g(0) = \\sin^{2}(0) = 0$ to $g(\\pi/2) = \\sin^{2}(\\pi/2) = 1$. Thus, the support of the random variable $X$ is the interval $(0, 1)$.\n\nTo find the PDF of $X$, we use the change-of-variables formula. First, we find the inverse transformation. For $x \\in (0, 1)$, we solve $x = \\sin^{2}\\theta$ for $\\theta$:\n$$\n\\sin\\theta = \\sqrt{x}\n$$\nWe take the positive square root because for $\\theta \\in (0, \\pi/2)$, $\\sin\\theta$ is positive. This gives the inverse function $h(x)$:\n$$\n\\theta = h(x) = \\arcsin(\\sqrt{x})\n$$\nNext, we compute the derivative of $h(x)$ with respect to $x$:\n$$\n\\frac{dh}{dx} = \\frac{d}{dx}\\arcsin(\\sqrt{x}) = \\frac{1}{\\sqrt{1 - (\\sqrt{x})^{2}}} \\cdot \\frac{d}{dx}(\\sqrt{x})\n$$\nThe derivative of $\\sqrt{x} = x^{1/2}$ is $\\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}$. Substituting this in, we get:\n$$\n\\frac{dh}{dx} = \\frac{1}{\\sqrt{1-x}} \\cdot \\frac{1}{2\\sqrt{x}} = \\frac{1}{2\\sqrt{x(1-x)}}\n$$\nFor $x \\in (0, 1)$, this derivative is positive, so its absolute value is the derivative itself.\n\nThe change-of-variables formula for the PDF of $X$, denoted $f_{X}(x)$, is:\n$$\nf_{X}(x) = f_{\\Theta}(h(x)) \\left| \\frac{dh}{dx} \\right|\n$$\nFor $x \\in (0, 1)$, we have $\\theta = h(x) = \\arcsin(\\sqrt{x}) \\in (0, \\pi/2)$, so $f_{\\Theta}(h(x)) = 2/\\pi$. Substituting the components, we find the PDF of $X$:\n$$\nf_{X}(x) = \\frac{2}{\\pi} \\cdot \\frac{1}{2\\sqrt{x(1-x)}} = \\frac{1}{\\pi\\sqrt{x(1-x)}} \\quad \\text{for } x \\in (0, 1)\n$$\nNow, we compare this result with the standard PDF of a $\\mathrm{Beta}(a,b)$ distribution, which is given by:\n$$\nf(x; a, b) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a, b)} \\quad \\text{for } x \\in (0, 1)\n$$\nwhere $B(a,b)$ is the Beta function, $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$, and $\\Gamma(z)$ is the Gamma function.\n\nFor a $\\mathrm{Beta}(1/2, 1/2)$ distribution, we set $a=1/2$ and $b=1/2$. The term in the numerator becomes:\n$$\nx^{1/2 - 1}(1-x)^{1/2 - 1} = x^{-1/2}(1-x)^{-1/2} = \\frac{1}{\\sqrt{x}\\sqrt{1-x}} = \\frac{1}{\\sqrt{x(1-x)}}\n$$\nThe Beta function is evaluated as:\n$$\nB(1/2, 1/2) = \\frac{\\Gamma(1/2)\\Gamma(1/2)}{\\Gamma(1/2 + 1/2)} = \\frac{(\\sqrt{\\pi})(\\sqrt{\\pi})}{\\Gamma(1)} = \\frac{\\pi}{1} = \\pi\n$$\nHere we have used the known values $\\Gamma(1/2) = \\sqrt{\\pi}$ and $\\Gamma(1) = 0! = 1$.\n\nAssembling the $\\mathrm{Beta}(1/2, 1/2)$ PDF, we obtain:\n$$\nf(x; 1/2, 1/2) = \\frac{1}{\\pi\\sqrt{x(1-x)}} \\quad \\text{for } x \\in (0, 1)\n$$\nThis is identical to the PDF $f_{X}(x)$ that we derived. This concludes the first part of the problem, verifying that $X = \\sin^{2}\\Theta$ with $\\Theta \\sim \\mathrm{Unif}(0, \\pi/2)$ follows a $\\mathrm{Beta}(1/2, 1/2)$ distribution.\n\n### Part 2: Computation of $\\mathbb{P}\\!\\left(T^{+}\\leq \\frac{1}{4}\\right)$\n\nThe problem states that the random variable $T^{+}$, representing the fraction of time a standard Brownian motion spends above zero on the unit time interval, has the same distribution as a $\\mathrm{Beta}(1/2, 1/2)$ random variable. We denote this distributional equivalence as $T^{+} \\stackrel{d}{=} X$, where $X \\sim \\mathrm{Beta}(1/2, 1/2)$.\n\nWe are asked to compute the probability $\\mathbb{P}\\!\\left(T^{+}\\leq \\frac{1}{4}\\right)$. Due to the distributional equivalence, this is the same as computing $\\mathbb{P}\\!\\left(X\\leq \\frac{1}{4}\\right)$.\n\nFrom Part 1, we know that if $X \\sim \\mathrm{Beta}(1/2, 1/2)$, we can represent it as $X = \\sin^{2}\\Theta$ where $\\Theta \\sim \\mathrm{Unif}(0, \\pi/2)$. This is the angle parametrization mentioned in the problem. Using this representation, we can translate the inequality on $X$ into an inequality on $\\Theta$:\n$$\nX \\leq \\frac{1}{4} \\quad \\implies \\quad \\sin^{2}\\Theta \\leq \\frac{1}{4}\n$$\nSince $\\Theta$ is in the interval $(0, \\pi/2)$, $\\sin\\Theta$ is positive. We can take the square root of both sides of the inequality without changing the direction:\n$$\n\\sin\\Theta \\leq \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\n$$\nThe sine function is strictly increasing on $(0, \\pi/2)$. Therefore, we can apply the inverse sine function, $\\arcsin$, to both sides:\n$$\n\\Theta \\leq \\arcsin\\left(\\frac{1}{2}\\right)\n$$\nThe angle is measured in radians, and we know that $\\arcsin(1/2) = \\pi/6$. So, the condition on $\\Theta$ is $0  \\Theta \\leq \\pi/6$.\n\nWe now need to calculate the probability $\\mathbb{P}(0  \\Theta \\leq \\pi/6)$. Since $\\Theta$ is uniformly distributed on $(0, \\pi/2)$, the probability of it falling into a subinterval is the ratio of the length of the subinterval to the length of the total interval of support:\n$$\n\\mathbb{P}(0  \\Theta \\leq \\pi/6) = \\frac{\\text{length of } (0, \\pi/6]}{\\text{length of } (0, \\pi/2)} = \\frac{\\pi/6 - 0}{\\pi/2 - 0} = \\frac{\\pi/6}{\\pi/2}\n$$\n$$\n\\mathbb{P}(0  \\Theta \\leq \\pi/6) = \\frac{\\pi}{6} \\cdot \\frac{2}{\\pi} = \\frac{2}{6} = \\frac{1}{3}\n$$\nThus, the required probability is $\\frac{1}{3}$.\n$$\n\\mathbb{P}\\!\\left(T^{+}\\leq \\frac{1}{4}\\right) = \\mathbb{P}\\!\\left(X\\leq \\frac{1}{4}\\right) = \\mathbb{P}(\\sin^{2}\\Theta \\leq \\frac{1}{4}) = \\mathbb{P}(0  \\Theta \\leq \\pi/6) = \\frac{1}{3}\n$$", "answer": "$$\n\\boxed{\\frac{1}{3}}\n$$", "id": "3039568"}, {"introduction": "After understanding the geometric origin of the arcsine law, it is essential to connect it back to the fundamental tools of probability theory. This exercise asks you to work directly with the probability density function (PDF) to derive its most important statistical properties: the mean and variance. This practice reinforces your skills with the Beta and Gamma functions and solidifies the identification of the arcsine law as a $\\mathrm{Beta}(1/2, 1/2)$ distribution [@problem_id:3039537].", "problem": "Let $\\{B_{t}\\}_{t \\ge 0}$ be a standard one-dimensional Brownian motion. Consider the random variable $X := \\lambda\\{t \\in [0,1] : B_{t} \\ge 0\\}$, the proportion of time in $[0,1]$ that the Brownian motion is nonnegative, where $\\lambda$ denotes the Lebesgue measure on $\\mathbb{R}$. It is a classical result (the first arcsine law) that $X$ has a probability density function (pdf) on $(0,1)$ given by $f(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}$. Starting from this pdf and only using foundational definitions and facts about the Euler Beta function $B(a,b) = \\int_{0}^{1} x^{a-1}(1-x)^{b-1} \\, dx$ and the Gamma function $\\Gamma(z)$, do the following:\n\n1. Identify the law of $X$ on $[0,1]$ by comparing $f(x)$ to a Beta distribution’s pdf. Justify your identification by referencing the normalizing constant through $B(a,b)$.\n\n2. Without using pre-memorized formulas for the mean or variance of the Beta distribution, compute $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ directly from the integral definitions $\\mathbb{E}[X] = \\int_{0}^{1} x f(x) \\, dx$ and $\\mathbb{E}[X^{2}] = \\int_{0}^{1} x^{2} f(x) \\, dx$, and standard Beta–Gamma identities. Your reasoning should be explicit and begin from these integral definitions.\n\nProvide your final answer as the pair $\\big(\\mathbb{E}[X], \\operatorname{Var}(X)\\big)$ in exact form, with no rounding.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to determine the law of the random variable $X$ and compute its first two moments. We proceed with the solution.\n\nThe problem asks for two tasks. First, to identify the probability distribution of the random variable $X$, which represents the proportion of time a standard one-dimensional Brownian motion $\\{B_{t}\\}_{t \\ge 0}$ is nonnegative on the time interval $[0,1]$. Second, to compute the expectation $\\mathbb{E}[X]$ and variance $\\operatorname{Var}(X)$ using fundamental integral definitions.\n\nThe given probability density function (pdf) for $X$ is $f(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}$ for $x \\in (0,1)$.\n\nPart 1: Identification of the Law of $X$\n\nThe general form of the probability density function for a Beta-distributed random variable with parameters $\\alpha  0$ and $\\beta  0$ is given by\n$$f_{\\text{Beta}}(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}$$\nwhere $x \\in (0,1)$ and $B(\\alpha, \\beta)$ is the Euler Beta function, which serves as the normalizing constant. The Beta function is defined as $B(\\alpha, \\beta) = \\int_{0}^{1} t^{\\alpha-1}(1-t)^{\\beta-1} \\, dt$.\n\nWe are given the pdf of $X$ as $f(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}$. We can rewrite this function to match the form of a Beta pdf:\n$$f(x) = \\frac{1}{\\pi} (x(1-x))^{-1/2} = \\frac{1}{\\pi} x^{-1/2} (1-x)^{-1/2}$$\nBy comparing the exponents with the general form $x^{\\alpha-1}(1-x)^{\\beta-1}$, we can identify the parameters $\\alpha$ and $\\beta$:\n_  $\\alpha - 1 = -1/2 \\implies \\alpha = 1 - 1/2 = 1/2$\n_  $\\beta - 1 = -1/2 \\implies \\beta = 1 - 1/2 = 1/2$\nThis suggests that $X$ follows a Beta distribution with parameters $\\alpha = 1/2$ and $\\beta = 1/2$, i.e., $X \\sim \\text{Beta}(1/2, 1/2)$. This specific distribution is also known as the arcsine distribution.\n\nTo confirm this identification, we must verify that the normalizing constant in the given pdf, $\\frac{1}{\\pi}$, is indeed equal to $\\frac{1}{B(1/2, 1/2)}$. We use the fundamental relationship between the Beta function and the Gamma function, $\\Gamma(z)$:\n$$B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$$\nFor our parameters $a=1/2$ and $b=1/2$, we have:\n$$B(1/2, 1/2) = \\frac{\\Gamma(1/2)\\Gamma(1/2)}{\\Gamma(1/2 + 1/2)} = \\frac{(\\Gamma(1/2))^2}{\\Gamma(1)}$$\nUsing the well-known values of the Gamma function, $\\Gamma(1/2) = \\sqrt{\\pi}$ and $\\Gamma(1) = 0! = 1$, we find:\n$$B(1/2, 1/2) = \\frac{(\\sqrt{\\pi})^2}{1} = \\pi$$\nTherefore, the normalizing constant is $\\frac{1}{B(1/2, 1/2)} = \\frac{1}{\\pi}$, which exactly matches the constant in the given pdf $f(x)$. This confirms that the law of $X$ is the Beta distribution with parameters $(\\alpha, \\beta) = (1/2, 1/2)$.\n\nPart 2: Computation of Expectation and Variance\n\nWe are required to compute $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ directly from their integral definitions, without using the standard formulas for the mean and variance of a Beta distribution.\n\nFirst, we compute the expectation, $\\mathbb{E}[X]$:\n$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx = \\int_{0}^{1} x f(x) \\, dx$$\nSubstituting the expression for $f(x)$:\n$$\\mathbb{E}[X] = \\int_{0}^{1} x \\left( \\frac{1}{\\pi \\sqrt{x(1-x)}} \\right) \\, dx = \\frac{1}{\\pi} \\int_{0}^{1} \\frac{x}{\\sqrt{x}\\sqrt{1-x}} \\, dx = \\frac{1}{\\pi} \\int_{0}^{1} \\frac{\\sqrt{x}}{\\sqrt{1-x}} \\, dx$$\nWe rewrite the integrand in the power form $x^{a-1}(1-x)^{b-1}$:\n$$\\mathbb{E}[X] = \\frac{1}{\\pi} \\int_{0}^{1} x^{1/2} (1-x)^{-1/2} \\, dx$$\nThe exponents correspond to $a-1 = 1/2 \\implies a=3/2$ and $b-1=-1/2 \\implies b=1/2$. The integral is therefore the Beta function $B(3/2, 1/2)$.\n$$\\mathbb{E}[X] = \\frac{1}{\\pi} B(3/2, 1/2)$$\nUsing the Beta-Gamma identity:\n$$B(3/2, 1/2) = \\frac{\\Gamma(3/2)\\Gamma(1/2)}{\\Gamma(3/2 + 1/2)} = \\frac{\\Gamma(3/2)\\Gamma(1/2)}{\\Gamma(2)}$$\nUsing the recurrence relation $\\Gamma(z+1)=z\\Gamma(z)$, we have $\\Gamma(3/2) = \\Gamma(1/2+1) = \\frac{1}{2}\\Gamma(1/2)$. Also, $\\Gamma(2) = 1! = 1$.\n$$B(3/2, 1/2) = \\frac{(\\frac{1}{2}\\Gamma(1/2))\\Gamma(1/2)}{1} = \\frac{1}{2}(\\Gamma(1/2))^2 = \\frac{1}{2}(\\sqrt{\\pi})^2 = \\frac{\\pi}{2}$$\nSubstituting this result back into the expression for $\\mathbb{E}[X]$:\n$$\\mathbb{E}[X] = \\frac{1}{\\pi} \\left( \\frac{\\pi}{2} \\right) = \\frac{1}{2}$$\n\nNext, we compute the second moment, $\\mathbb{E}[X^2]$, which is necessary for the variance.\n$$\\mathbb{E}[X^2] = \\int_{0}^{1} x^2 f(x) \\, dx = \\int_{0}^{1} x^2 \\left( \\frac{1}{\\pi \\sqrt{x(1-x)}} \\right) \\, dx = \\frac{1}{\\pi} \\int_{0}^{1} x^{2 - 1/2} (1-x)^{-1/2} \\, dx$$\n$$\\mathbb{E}[X^2] = \\frac{1}{\\pi} \\int_{0}^{1} x^{3/2} (1-x)^{-1/2} \\, dx$$\nThe exponents correspond to $a-1 = 3/2 \\implies a=5/2$ and $b-1=-1/2 \\implies b=1/2$. The integral is the Beta function $B(5/2, 1/2)$.\n$$\\mathbb{E}[X^2] = \\frac{1}{\\pi} B(5/2, 1/2)$$\nUsing the Beta-Gamma identity:\n$$B(5/2, 1/2) = \\frac{\\Gamma(5/2)\\Gamma(1/2)}{\\Gamma(5/2 + 1/2)} = \\frac{\\Gamma(5/2)\\Gamma(1/2)}{\\Gamma(3)}$$\nUsing $\\Gamma(z+1)=z\\Gamma(z)$ repeatedly, $\\Gamma(5/2) = \\frac{3}{2}\\Gamma(3/2) = \\frac{3}{2} \\cdot \\frac{1}{2}\\Gamma(1/2) = \\frac{3}{4}\\Gamma(1/2)$. Also, $\\Gamma(3) = 2! = 2$.\n$$B(5/2, 1/2) = \\frac{(\\frac{3}{4}\\Gamma(1/2))\\Gamma(1/2)}{2} = \\frac{3}{8}(\\Gamma(1/2))^2 = \\frac{3}{8}(\\sqrt{\\pi})^2 = \\frac{3\\pi}{8}$$\nSubstituting this result back into the expression for $\\mathbb{E}[X^2]$:\n$$\\mathbb{E}[X^2] = \\frac{1}{\\pi} \\left( \\frac{3\\pi}{8} \\right) = \\frac{3}{8}$$\n\nFinally, we compute the variance using the formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n$$\\operatorname{Var}(X) = \\frac{3}{8} - \\left(\\frac{1}{2}\\right)^2 = \\frac{3}{8} - \\frac{1}{4} = \\frac{3}{8} - \\frac{2}{8} = \\frac{1}{8}$$\nThe expectation and variance are $\\mathbb{E}[X] = 1/2$ and $\\operatorname{Var}(X) = 1/8$, respectively.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{8} \\end{pmatrix}}\n$$", "id": "3039537"}, {"introduction": "Theoretical laws in science are validated through empirical evidence and simulation. This final practice provides a complete, hands-on experience in computational verification, bridging the gap between abstract theory and data. You will simulate Brownian motion, apply the arcsine law as a diagnostic tool using the probability integral transform, and use a standard statistical test to determine if the simulation results match the theoretical predictions [@problem_id:3039578].", "problem": "You will implement and apply a principled diagnostic based on the arcsine cumulative distribution function to test whether simulated time proportions from discretized Brownian motion behave like draws from the Beta distribution with parameters $\\tfrac{1}{2}$ and $\\tfrac{1}{2}$. Your program must be fully deterministic.\n\nStarting point and core facts:\n- Let $\\{B_t\\}_{t \\in [0,1]}$ be a standard Brownian motion with $B_0 = 0$, characterized by independent Gaussian increments with mean $0$ and variance equal to the time increment. Consider also a Brownian motion with drift $\\mu \\in \\mathbb{R}$, defined as $X_t = \\mu t + B_t$.\n- The arcsine law states that, for standard Brownian motion (i.e., $\\mu = 0$), the proportion of time in $[0,1]$ that the process spends above zero has the Beta distribution with parameters $(\\tfrac{1}{2}, \\tfrac{1}{2})$. Use this as the null model for the simulated proportions.\n- The Beta distribution with parameters $(\\tfrac{1}{2}, \\tfrac{1}{2})$ has density $f(x) = \\dfrac{1}{\\pi \\sqrt{x(1-x)}}$ on $x \\in (0,1)$ and cumulative distribution function $F(x) = \\dfrac{2}{\\pi} \\arcsin(\\sqrt{x})$ for $x \\in [0,1]$.\n- The probability integral transform states that if $X$ has a continuous cumulative distribution function $F$, then $U = F(X)$ is uniformly distributed on $[0,1]$.\n- The Kolmogorov–Smirnov (KS) statistic for a one-sample goodness-of-fit test to the Uniform distribution on $[0,1]$ is defined as\n$$\nD_n = \\sup_{u \\in [0,1]} \\left| F_n(u) - u \\right|,\n$$\nwhere $F_n$ is the empirical cumulative distribution function of an independent and identically distributed sample $\\{U_i\\}_{i=1}^{n}$.\n\nTasks to implement:\n1. Simulation of sample paths. For given integers $N \\ge 1$ and $M \\ge 1$ and a drift $\\mu \\in \\mathbb{R}$, simulate $N$ independent discrete-time paths of a Brownian motion with drift on $[0,1]$ using an equally spaced grid with $M$ steps and time step $\\Delta t = 1/M$. For each path,\n   - Generate increments $\\Delta X_k = \\mu \\Delta t + \\sqrt{\\Delta t} Z_k$ for $k = 1, \\dots, M$, where $Z_k$ are independent standard normal random variables.\n   - Form the partial sums $X_{t_k} = \\sum_{j=1}^{k} \\Delta X_j$ for $k = 1, \\dots, M$.\n   - Compute the sample proportion $P$ of grid times with $X_{t_k}  0$, i.e., $P = \\frac{1}{M} \\sum_{k=1}^{M} \\mathbf{1}\\{X_{t_k}  0\\}$.\n   - Repeat to obtain $N$ proportions $\\{P_i\\}_{i=1}^{N}$. Use a fixed random seed specified below to ensure determinism.\n2. Arcsine-transform diagnostic. Using the arcsine cumulative distribution function $F(x) = \\dfrac{2}{\\pi} \\arcsin(\\sqrt{x})$, transform the proportions by $U_i = F(P_i)$, $i = 1, \\dots, N$. Under the null hypothesis that the proportions follow the Beta distribution with parameters $(\\tfrac{1}{2}, \\tfrac{1}{2})$, the transformed values $\\{U_i\\}$ should be approximately independent and identically distributed Uniform on $[0,1]$.\n3. Goodness-of-fit metric. Compute the one-sample Kolmogorov–Smirnov statistic $D_n$ comparing the empirical cumulative distribution function of $\\{U_i\\}_{i=1}^{N}$ to the uniform cumulative distribution function $u \\mapsto u$ on $[0,1]$.\n4. Determinism requirement. Use the base random seed $314159$ and, for each test case indexed by $j = 0, 1, 2, \\dots$, use the derived seed $314159 + 1000 \\cdot j$ to initialize the random number generator for that test case, so that the results are reproducible and independent of execution order.\n\nTest suite:\nYou must evaluate and report the Kolmogorov–Smirnov statistic $D_n$ for each of the following five parameter sets $(N, M, \\mu)$:\n- Case 1 (happy path, null model, moderately large sample): $(N, M, \\mu) = (4000, 1024, 0.0)$.\n- Case 2 (boundary condition: coarse time grid, smaller sample): $(N, M, \\mu) = (800, 32, 0.0)$.\n- Case 3 (boundary condition: non-power-of-two grid, null model): $(N, M, \\mu) = (1500, 127, 0.0)$.\n- Case 4 (alternative model with positive drift): $(N, M, \\mu) = (4000, 1024, 0.25)$.\n- Case 5 (alternative model with negative drift): $(N, M, \\mu) = (4000, 1024, -0.25)$.\n\nImplementation constraints and output specification:\n- Your program must simulate the processes, compute the transformed values $\\{U_i\\}$ via the arcsine cumulative distribution function, compute the Kolmogorov–Smirnov statistic $D_n$ for each test case, and return the results for the five cases in order.\n- Your program must not require any user input and must not access any external files or networks.\n- To avoid excessive memory use, you may simulate in batches, but the final results must be identical for the stated seeds.\n- Final output format: Print a single line containing a list of the five results as decimal numbers rounded to six digits after the decimal point, comma-separated and enclosed in square brackets, for example, \"[0.012345,0.067890,0.000123,0.045678,0.098765]\".", "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in the theory of stochastic processes and statistics, well-posed with all necessary parameters and methods specified, and objective in its formulation. The task is to implement a numerical experiment to test the arcsine law for Brownian motion.\n\nThe solution proceeds by implementing a four-step diagnostic procedure for each specified test case. The theoretical foundation and algorithmic implementation are detailed below.\n\n**1. Theoretical Framework**\n\nThe problem concerns a Brownian motion with drift $\\mu \\in \\mathbb{R}$ on the time interval $[0, 1]$, defined by the stochastic differential equation $dX_t = \\mu dt + dB_t$, with initial condition $X_0 = 0$. Here, $\\{B_t\\}_{t \\in [0,1]}$ is a standard Brownian motion.\n\nA cornerstone of this problem is Lévy's arcsine law. For a standard Brownian motion (i.e., $\\mu=0$), this law states that the proportion of time the process spends above zero, denoted by $P = \\text{Leb}\\{t \\in [0,1] : B_t  0\\}$, is a random variable that follows the arcsine distribution. This distribution is equivalent to the Beta distribution with shape parameters $(\\frac{1}{2}, \\frac{1}{2})$. The probability density function on $x \\in (0,1)$ is $f(x) = \\frac{1}{\\pi\\sqrt{x(1-x)}}$, and its cumulative distribution function (CDF) is $F(x) = \\frac{2}{\\pi} \\arcsin(\\sqrt{x})$ for $x \\in [0,1]$.\n\nThe diagnostic test is built upon the probability integral transform (PIT). The PIT states that if a random variable $X$ has a continuous CDF $F_X$, then the transformed random variable $U = F_X(X)$ is uniformly distributed on $[0,1]$. Therefore, if the simulated proportions $\\{P_i\\}$ are indeed draws from the arcsine distribution, then the transformed values $\\{U_i = F(P_i)\\}$ should form a sample from the standard uniform distribution $U(0,1)$.\n\nTo quantify the agreement of the transformed sample $\\{U_i\\}$ with the uniform distribution, the one-sample Kolmogorov-Smirnov (KS) test is used. The KS statistic, $D_n$, measures the maximum absolute difference between the empirical cumulative distribution function (ECDF) of the sample, $F_n(u)$, and the CDF of the reference distribution, which in this case is $G(u) = u$. The statistic is formally defined as:\n$$D_n = \\sup_{u \\in [0,1]} |F_n(u) - u|$$\nA small value of $D_n$ indicates a good fit to the reference distribution, while a large value suggests that the sample was not drawn from it.\n\n**2. Algorithmic Implementation**\n\nThe overall algorithm simulates paths of the process, calculates the relevant statistics, and performs the goodness-of-fit test. This is done for each of the five parameter sets $(N, M, \\mu)$.\n\n**Step 2.1: Simulation of Sample Paths**\nFor each test case with parameters $N$, $M$, and $\\mu$, we simulate $N$ independent paths of the process $X_t$ on a discretized time grid. The time interval $[0,1]$ is divided into $M$ steps of size $\\Delta t = 1/M$. A path is approximated using the Euler-Maruyama scheme. The position at time $t_k = k\\Delta t$ is given by the sum of its increments:\n$$X_{t_k} = \\sum_{j=1}^{k} \\Delta X_j$$\nwhere the increments $\\Delta X_j$ are generated as independent random variables:\n$$\\Delta X_j = \\mu \\Delta t + \\sqrt{\\Delta t} Z_j$$\nHere, $\\{Z_j\\}_{j=1}^M$ are independent draws from a standard normal distribution, $Z_j \\sim \\mathcal{N}(0,1)$. To ensure reproducibility, the random number generator is seeded with $314159 + 1000 \\cdot j$ for the $j$-th test case (0-indexed).\n\n**Step 2.2: Calculation of Time-Positive Proportions**\nFor each of the $N$ simulated paths, we compute the proportion of time steps where the path's value is positive. For the $i$-th path, this proportion $P_i$ is calculated as:\n$$P_i = \\frac{1}{M} \\sum_{k=1}^{M} \\mathbf{1}\\{X_{t_k}^{(i)}  0\\}$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This procedure yields a sample of $N$ proportions, $\\{P_i\\}_{i=1}^N$.\n\n**Step 2.3: Arcsine Transform Diagnostic**\nEach proportion $P_i$ in the sample is transformed using the arcsine CDF:\n$$U_i = F(P_i) = \\frac{2}{\\pi} \\arcsin(\\sqrt{P_i})$$\nUnder the null hypothesis that $\\mu=0$ and the discretization is sufficiently fine, the resulting sample $\\{U_i\\}_{i=1}^N$ should be approximately independent and uniformly distributed on $[0,1]$.\n\n**Step 2.4: Goodness-of-Fit Metric Calculation**\nThe final step is to compute the KS statistic $D_N$ for the transformed sample $\\{U_i\\}_{i=1}^N$ against the standard uniform distribution. This is accomplished using the `scipy.stats.kstest` function, which provides a robust and standard implementation for calculating $D_N$.\n\nThis entire process is repeated for each of the five test cases. Cases 1, 2, and 3 use $\\mu=0$ and are expected to yield small KS statistics, consistent with the arcsine law (the null model). Cases 4 and 5 use non-zero drift ($\\mu=0.25$ and $\\mu=-0.25$, respectively), for which the arcsine law does not hold. Thus, for these alternative models, the distribution of $P_i$ will be skewed, the transformed $U_i$ values will not be uniform, and we expect significantly larger KS statistics, indicating a rejection of the null model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef compute_ks_for_case(N, M, mu, seed):\n    \"\"\"\n    Simulates Brownian motion paths and computes the KS statistic.\n\n    Args:\n        N (int): Number of independent paths to simulate.\n        M (int): Number of time steps for discretization.\n        mu (float): The drift of the Brownian motion.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The calculated Kolmogorov-Smirnov statistic.\n    \"\"\"\n    # Initialize the random number generator for reproducibility.\n    # The default_rng is the modern, recommended approach in NumPy.\n    rng = np.random.default_rng(seed)\n\n    # Define the time step for the discretization.\n    dt = 1.0 / M\n\n    # Step 1: Simulate N paths with M steps each.\n    # We generate all random numbers at once for efficiency.\n    # Z is an N x M array of standard normal random variables.\n    Z = rng.standard_normal(size=(N, M))\n    \n    # Calculate the increments for each step of each path.\n    # dX = mu*dt + sqrt(dt)*Z\n    dX = mu * dt + np.sqrt(dt) * Z\n\n    # Form the paths by taking the cumulative sum of increments along the time axis.\n    # X is an N x M array where each row represents a full path.\n    X = np.cumsum(dX, axis=1)\n\n    # Step 2: Compute the proportion of time each path is positive.\n    # Count the number of time points k where X_{t_k}  0 for each path.\n    positive_counts = np.sum(X  0, axis=1)\n    \n    # The proportion P is the count divided by the total number of steps.\n    P = positive_counts / M\n\n    # Step 3: Apply the arcsine transform (Probability Integral Transform).\n    # The CDF of the Beta(1/2, 1/2) distribution is F(x) = (2/pi) * arcsin(sqrt(x)).\n    # We transform the proportions P into a sample U, which should be uniform on [0,1]\n    # under the null hypothesis (mu=0).\n    # The input to np.sqrt is guaranteed to be in [0, 1].\n    U = (2.0 / np.pi) * np.arcsin(np.sqrt(P))\n\n    # Step 4: Compute the one-sample Kolmogorov-Smirnov statistic.\n    # We test the sample U against a standard uniform distribution ('uniform').\n    # kstest returns a tuple (statistic, pvalue); we only need the statistic.\n    ks_statistic, _ = kstest(U, 'uniform')\n    \n    return ks_statistic\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement as a list of tuples.\n    # Each tuple is (N, M, mu).\n    test_cases = [\n        (4000, 1024, 0.0),   # Case 1: Null model, large sample\n        (800, 32, 0.0),      # Case 2: Null model, coarse grid\n        (1500, 127, 0.0),    # Case 3: Null model, non-power-of-two grid\n        (4000, 1024, 0.25),  # Case 4: Alternative model, positive drift\n        (4000, 1024, -0.25), # Case 5: Alternative model, negative drift\n    ]\n\n    base_seed = 314159\n    results = []\n\n    # Iterate through the test cases, computing the KS statistic for each.\n    for j, (N, M, mu) in enumerate(test_cases):\n        # The seed for each case is derived deterministically as specified.\n        seed = base_seed + 1000 * j\n        result = compute_ks_for_case(N, M, mu, seed)\n        results.append(result)\n\n    # Format the results to six decimal places and print in the required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n\n```", "id": "3039578"}]}