{"hands_on_practices": [{"introduction": "The definition of the scaled random walk involves a crucial factor of $n^{-1/2}$. This exercise demonstrates why this specific scaling is essential, showing that other scaling choices lead to either a degenerate limit or explosive behavior. By analyzing the variance of the process for a general exponent $\\alpha$, you will gain a foundational understanding of the delicate balance required to capture non-trivial diffusive limits [@problem_id:3050198].", "problem": "Consider a simple symmetric random walk with steps $\\{X_k\\}_{k \\geq 1}$ that are independent and identically distributed with $\\mathbb{P}(X_k = 1) = \\mathbb{P}(X_k = -1) = \\frac{1}{2}$, so $\\mathbb{E}[X_k] = 0$ and $\\operatorname{Var}(X_k) = 1$. Let the partial sums be $S_n = \\sum_{k=1}^{n} X_k$. For a fixed time horizon $T  0$ and a scaling exponent $\\alpha \\in \\mathbb{R}$, define the scaled process on $[0,T]$ by\n$$\nW_n^{(\\alpha)}(t) = n^{-\\alpha} S_{\\lfloor n t \\rfloor}, \\quad t \\in [0,T].\n$$\nUsing only the growth of $\\operatorname{Var}(S_m)$ with $m$ and standard limit theorems as foundational inputs, analyze the limiting behavior of the finite-dimensional distributions of $\\{W_n^{(\\alpha)}\\}_{n \\geq 1}$ as $n \\to \\infty$ for different values of $\\alpha$. In particular, explain why scaling by $n^{-\\alpha}$ with $\\alpha \\neq \\frac{1}{2}$ yields either degenerate limits or explosive behavior.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. When $\\alpha = \\frac{1}{2}$, the sequence of processes $\\{W_n^{(1/2)}\\}_{n \\geq 1}$ converges in distribution (in the sense of finite-dimensional distributions, and in fact in the Skorokhod sense on $D([0,T])$) to a standard Brownian motion $B(t)$, because the variance of $W_n^{(1/2)}(t)$ is of order $t$ for each fixed $t  0$.\n\nB. When $\\alpha  \\frac{1}{2}$, for each fixed $t  0$ we have $\\operatorname{Var}(W_n^{(\\alpha)}(t)) \\to 0$ as $n \\to \\infty$, and consequently $W_n^{(\\alpha)}(t) \\to 0$ in probability; the process-level limit is the zero process, which is degenerate.\n\nC. When $\\alpha  \\frac{1}{2}$, for each fixed $t  0$ we have $\\operatorname{Var}(W_n^{(\\alpha)}(t)) \\to \\infty$ as $n \\to \\infty$, so the finite-dimensional distributions are not tight and the probability mass escapes to infinity; no non-degenerate finite limit in distribution exists for $W_n^{(\\alpha)}(t)$ under this scaling.\n\nD. When $\\alpha \\neq \\frac{1}{2}$, re-centering $W_n^{(\\alpha)}(t)$ at its mean (which is zero) yields a non-degenerate limit for all $\\alpha$, so variance growth is not relevant.\n\nE. The choice of $\\alpha$ affects only the time parameterization; for any $\\alpha$ one can obtain a Brownian limit by a deterministic time change of $W_n^{(\\alpha)}$, so the spatial scaling does not affect the existence of a non-degenerate limit.", "solution": "The problem statement is a standard formulation in the study of stochastic processes, specifically concerning the scaling limit of a random walk. All terms are well-defined, and the premises are consistent with established probability theory. The problem is scientifically grounded, well-posed, and objective. There are no flaws that would invalidate the problem.\n\nWe begin by analyzing the properties of the random walk and the scaled process. The steps $\\{X_k\\}_{k \\geq 1}$ are independent and identically distributed (i.i.d.) with $\\mathbb{E}[X_k] = 0$ and $\\operatorname{Var}(X_k) = \\mathbb{E}[X_k^2] - (\\mathbb{E}[X_k])^2 = (1^2 \\cdot \\frac{1}{2} + (-1)^2 \\cdot \\frac{1}{2}) - 0^2 = 1$.\n\nThe partial sum is $S_m = \\sum_{k=1}^{m} X_k$. Its mean and variance are:\n$$ \\mathbb{E}[S_m] = \\sum_{k=1}^{m} \\mathbb{E}[X_k] = 0 $$\nDue to the independence of the steps, the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(S_m) = \\sum_{k=1}^{m} \\operatorname{Var}(X_k) = \\sum_{k=1}^{m} 1 = m $$\nThis linear growth of the variance, $\\operatorname{Var}(S_m) = m$, is the fundamental property we need.\n\nThe scaled process is defined as $W_n^{(\\alpha)}(t) = n^{-\\alpha} S_{\\lfloor n t \\rfloor}$ for $t \\in [0,T]$. Let's compute its mean and variance for a fixed $t  0$.\nThe mean is:\n$$ \\mathbb{E}[W_n^{(\\alpha)}(t)] = \\mathbb{E}[n^{-\\alpha} S_{\\lfloor n t \\rfloor}] = n^{-\\alpha} \\mathbb{E}[S_{\\lfloor n t \\rfloor}] = 0 $$\nThe variance is:\n$$ \\operatorname{Var}(W_n^{(\\alpha)}(t)) = \\operatorname{Var}(n^{-\\alpha} S_{\\lfloor n t \\rfloor}) = (n^{-\\alpha})^2 \\operatorname{Var}(S_{\\lfloor n t \\rfloor}) = n^{-2\\alpha} \\lfloor n t \\rfloor $$\nTo understand the limiting behavior as $n \\to \\infty$, we analyze the limit of this variance. Since $\\lfloor nt \\rfloor = nt - \\{nt\\}$, where $0 \\leq \\{nt\\}  1$ is the fractional part, we have $\\lfloor nt \\rfloor \\sim nt$ for large $n$.\n$$ \\lim_{n \\to \\infty} \\operatorname{Var}(W_n^{(\\alpha)}(t)) = \\lim_{n \\to \\infty} n^{-2\\alpha} (nt) = \\lim_{n \\to \\infty} t \\, n^{1-2\\alpha} $$\nThe behavior of this limit depends critically on the value of the exponent $1-2\\alpha$.\n\nCase 1: $\\alpha = \\frac{1}{2}$ (Critical scaling)\nThe exponent is $1-2(\\frac{1}{2}) = 0$.\n$$ \\lim_{n \\to \\infty} \\operatorname{Var}(W_n^{(1/2)}(t)) = \\lim_{n \\to \\infty} t \\, n^0 = t $$\nThe variance converges to a finite, non-zero value. By the Central Limit Theorem (a standard limit theorem), for large $m$, the distribution of $S_m/\\sqrt{m}$ is approximately a standard normal distribution $N(0,1)$. We can write:\n$$ W_n^{(1/2)}(t) = n^{-1/2} S_{\\lfloor n t \\rfloor} = \\frac{S_{\\lfloor n t \\rfloor}}{\\sqrt{\\lfloor n t \\rfloor}} \\frac{\\sqrt{\\lfloor n t \\rfloor}}{\\sqrt{n}} $$\nAs $n \\to \\infty$, $\\lfloor n t \\rfloor \\to \\infty$. The term $\\frac{S_{\\lfloor n t \\rfloor}}{\\sqrt{\\lfloor n t \\rfloor}}$ converges in distribution to a standard normal random variable $Z \\sim N(0,1)$. The term $\\frac{\\sqrt{\\lfloor n t \\rfloor}}{\\sqrt{n}}$ converges to $\\sqrt{t}$. By Slutsky's Theorem, the product converges in distribution to $Z \\cdot \\sqrt{t}$, which has a $N(0,t)$ distribution. This is the distribution of a standard Brownian motion $B(t)$ at time $t$. A more detailed analysis shows that the finite-dimensional distributions of the process $\\{W_n^{(1/2)}(t)\\}_{t \\in [0,T]}$ converge to those of a standard Brownian motion.\n\nCase 2: $\\alpha  \\frac{1}{2}$ (Over-scaling)\nThe exponent is $1-2\\alpha  0$.\n$$ \\lim_{n \\to \\infty} \\operatorname{Var}(W_n^{(\\alpha)}(t)) = \\lim_{n \\to \\infty} t \\, n^{1-2\\alpha} = 0 $$\nA sequence of random variables with mean $0$ and variance approaching $0$ converges in mean square (and thus in probability) to $0$. This means for any fixed $t  0$, $W_n^{(\\alpha)}(t) \\xrightarrow{p} 0$. The limiting process is the zero process, $W(t) = 0$ for all $t$, which is a deterministic and therefore degenerate process.\n\nCase 3: $\\alpha  \\frac{1}{2}$ (Under-scaling)\nThe exponent is $1-2\\alpha  0$.\n$$ \\lim_{n \\to \\infty} \\operatorname{Var}(W_n^{(\\alpha)}(t)) = \\lim_{n \\to \\infty} t \\, n^{1-2\\alpha} = \\infty $$\nThe variance explodes. This indicates that the distribution of $W_n^{(\\alpha)}(t)$ becomes infinitely spread out as $n \\to \\infty$. A sequence of probability distributions whose variance diverges to infinity is not tight. For a sequence of random variables $\\{Y_n\\}$ to converge in distribution to a proper random variable $Y$, the sequence of distributions must be tight. That is, for any $\\epsilon  0$, there must exist a compact set $K$ such that $\\mathbb{P}(Y_n \\in K)  1-\\epsilon$ for all $n$. Here, the probability mass escapes to $\\pm\\infty$, and no such convergence is possible. The process exhibits explosive behavior.\n\nNow we evaluate each option:\n\nA. When $\\alpha = \\frac{1}{2}$, the sequence of processes $\\{W_n^{(1/2)}\\}_{n \\geq 1}$ converges in distribution (in the sense of finite-dimensional distributions, and in fact in the Skorokhod sense on $D([0,T])$) to a standard Brownian motion $B(t)$, because the variance of $W_n^{(1/2)}(t)$ is of order $t$ for each fixed $t  0$.\nOur analysis for $\\alpha = \\frac{1}{2}$ shows that $\\operatorname{Var}(W_n^{(1/2)}(t)) = n^{-1} \\lfloor nt \\rfloor$, which converges to $t$ as $n \\to \\infty$. This is the precise condition needed to obtain a non-degenerate Gaussian limit via the Central Limit Theorem, specifically $N(0,t)$, which is the distribution of $B(t)$. The extension to finite-dimensional distributions and the full weak convergence on the space of càdlàg functions $D([0,T])$ (Donsker's Invariance Principle) are correct consequences. The reason provided—that the variance is of order $t$—is the correct underlying principle.\n**Verdict: Correct**\n\nB. When $\\alpha  \\frac{1}{2}$, for each fixed $t  0$ we have $\\operatorname{Var}(W_n^{(\\alpha)}(t)) \\to 0$ as $n \\to \\infty$, and consequently $W_n^{(\\alpha)}(t) \\to 0$ in probability; the process-level limit is the zero process, which is degenerate.\nOur analysis for $\\alpha  \\frac{1}{2}$ shows that the exponent $1-2\\alpha$ is negative, causing $\\operatorname{Var}(W_n^{(\\alpha)}(t))$ to converge to $0$. As the mean is also $0$, this implies convergence in $L^2$ and therefore in probability to $0$. The limit of the process is thus the constant zero function, which is a degenerate limit (i.e., non-random). The statement is entirely accurate.\n**Verdict: Correct**\n\nC. When $\\alpha  \\frac{1}{2}$, for each fixed $t  0$ we have $\\operatorname{Var}(W_n^{(\\alpha)}(t)) \\to \\infty$ as $n \\to \\infty$, so the finite-dimensional distributions are not tight and the probability mass escapes to infinity; no non-degenerate finite limit in distribution exists for $W_n^{(\\alpha)}(t)$ under this scaling.\nOur analysis for $\\alpha  \\frac{1}{2}$ shows that the exponent $1-2\\alpha$ is positive, causing $\\operatorname{Var}(W_n^{(\\alpha)}(t))$ to diverge to $\\infty$. A sequence of distributions with diverging variance cannot be tight. By Prokhorov's theorem, tightness is a necessary condition for weak convergence (convergence in distribution) on spaces like $\\mathbb{R}$. The probability mass spreads out over the entire real line, meaning no limit in distribution to a proper (finite-valued) random variable exists. The statement accurately describes this \"explosive\" behavior.\n**Verdict: Correct**\n\nD. When $\\alpha \\neq \\frac{1}{2}$, re-centering $W_n^{(\\alpha)}(t)$ at its mean (which is zero) yields a non-degenerate limit for all $\\alpha$, so variance growth is not relevant.\nThe mean of $W_n^{(\\alpha)}(t)$ is already $0$, so \"re-centering\" the process (i.e., considering $W_n^{(\\alpha)}(t) - \\mathbb{E}[W_n^{(\\alpha)}(t)]$) changes nothing. The issues of a degenerate limit (for $\\alpha  1/2$) or an explosive, non-convergent behavior (for $\\alpha  1/2$) remain. The statement that variance growth is not relevant is a direct contradiction of our analysis, which shows that the behavior of the variance is precisely what determines the nature of the limit.\n**Verdict: Incorrect**\n\nE. The choice of $\\alpha$ affects only the time parameterization; for any $\\alpha$ one can obtain a Brownian limit by a deterministic time change of $W_n^{(\\alpha)}$, so the spatial scaling does not affect the existence of a non-degenerate limit.\nThe parameter $\\alpha$ controls the spatial (or value) scaling $n^{-\\alpha}$, whereas the time scaling is embedded in the index $\\lfloor nt \\rfloor$. A diffusive limit like Brownian motion arises only when space is scaled as the square root of time. We can write $W_n^{(\\alpha)}(t) = n^{1/2 - \\alpha} W_n^{(1/2)}(t)$. As $n \\to \\infty$, $W_n^{(1/2)}(t)$ converges to $B(t)$. The pre-factor $n^{1/2 - \\alpha}$ either vanishes or explodes if $\\alpha \\neq 1/2$. A deterministic time change means replacing $t$ with a function $f(t)$, which would transform the limit to $B(f(t))$, but it cannot eliminate the problematic $n^{1/2 - \\alpha}$ factor. The spatial scaling is coupled to the time scaling, and its choice is critical for the existence of a non-degenerate limit. The statement is fundamentally incorrect.\n**Verdict: Incorrect**", "answer": "$$\\boxed{ABC}$$", "id": "3050198"}, {"introduction": "Having established the correct scaling, we now verify that the resulting process shares key features with Brownian motion, whose covariance is $\\mathrm{Cov}(B(s), B(t)) = \\min(s,t)$. This practice asks you to compute the covariance of the scaled random walk and prove that it converges to this exact form as $n \\to \\infty$. This calculation provides concrete evidence that the discrete random walk's correlation structure morphs into that of its continuous limit [@problem_id:3050159].", "problem": "Let $\\{X_{k}\\}_{k \\geq 1}$ be independent and identically distributed (i.i.d.) real-valued random variables with $\\mathbb{E}[X_{1}] = 0$ and $\\operatorname{Var}(X_{1}) = 1$. For $n \\in \\mathbb{N}$ and $t \\in [0,1]$, define the scaled partial-sum (random walk) process\n$$\nW_{n}(t) \\coloneqq \\frac{1}{\\sqrt{n}} \\sum_{k=1}^{\\lfloor n t \\rfloor} X_{k}.\n$$\nUsing only the linearity of covariance, independence of the increments, and the fact that $\\operatorname{Cov}(X_{i},X_{j}) = 0$ for $i \\neq j$ and $\\operatorname{Var}(X_{1}) = 1$, compute the covariance $\\operatorname{Cov}(W_{n}(s), W_{n}(t))$ for fixed $s,t \\in [0,1]$ as an explicit function of $n,s,t$. Then, justify rigorously that this covariance converges as $n \\to \\infty$ to the function $\\min(s,t)$, the covariance function of a standard Brownian motion.\n\nProvide your reasoning from first principles and avoid using any results that assume the conclusion. As your final answer, report the limiting covariance as a function of $s$ and $t$ (no units are required).", "solution": "The problem is to compute the covariance of the scaled random walk process $W_{n}(t)$ at two time points, $s$ and $t$, and then find the limit of this covariance as $n \\to \\infty$.\n\nLet $\\{X_{k}\\}_{k \\geq 1}$ be independent and identically distributed (i.i.d.) real-valued random variables with mean $\\mathbb{E}[X_{1}] = 0$ and variance $\\operatorname{Var}(X_{1}) = 1$. The process is defined as\n$$\nW_{n}(t) \\coloneqq \\frac{1}{\\sqrt{n}} \\sum_{k=1}^{\\lfloor n t \\rfloor} X_{k}\n$$\nfor $n \\in \\mathbb{N}$ and $t \\in [0,1]$.\n\nFirst, we will compute the covariance $\\operatorname{Cov}(W_{n}(s), W_{n}(t))$ for fixed $s,t \\in [0,1]$. Let us assume, without loss of generality, that $s \\le t$. This implies that for any $n \\in \\mathbb{N}$, we have $ns \\le nt$ and therefore $\\lfloor ns \\rfloor \\le \\lfloor nt \\rfloor$.\n\nWe can express $W_{n}(t)$ in terms of $W_{n}(s)$ and an increment:\n$$\nW_{n}(t) = \\frac{1}{\\sqrt{n}} \\sum_{k=1}^{\\lfloor n t \\rfloor} X_{k} = \\frac{1}{\\sqrt{n}} \\left( \\sum_{k=1}^{\\lfloor n s \\rfloor} X_{k} + \\sum_{k=\\lfloor n s \\rfloor + 1}^{\\lfloor n t \\rfloor} X_{k} \\right) = W_{n}(s) + \\frac{1}{\\sqrt{n}} \\sum_{k=\\lfloor n s \\rfloor + 1}^{\\lfloor n t \\rfloor} X_{k}\n$$\nThe increment is the random variable $W_{n}(t) - W_{n}(s) = \\frac{1}{\\sqrt{n}} \\sum_{k=\\lfloor n s \\rfloor + 1}^{\\lfloor n t \\rfloor} X_{k}$.\n\nUsing the bilinearity of the covariance operator, we have:\n$$\n\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\operatorname{Cov}(W_{n}(s), W_{n}(s) + (W_{n}(t) - W_{n}(s)))\n$$\n$$\n\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\operatorname{Cov}(W_{n}(s), W_{n}(s)) + \\operatorname{Cov}(W_{n}(s), W_{n}(t) - W_{n}(s))\n$$\nThe first term is the variance of $W_{n}(s)$: $\\operatorname{Cov}(W_{n}(s), W_{n}(s)) = \\operatorname{Var}(W_{n}(s))$.\n\nFor the second term, we examine the random variables $W_{n}(s)$ and $W_{n}(t) - W_{n}(s)$.\n$W_{n}(s)$ is a function of the set of random variables $\\{X_1, X_2, \\dots, X_{\\lfloor ns \\rfloor}\\}$.\nThe increment $W_{n}(t) - W_{n}(s)$ is a function of the set of random variables $\\{X_{\\lfloor ns \\rfloor+1}, \\dots, X_{\\lfloor nt \\rfloor}\\}$.\nSince $s \\le t$, we have $\\lfloor ns \\rfloor \\le \\lfloor nt \\rfloor$. The index sets $\\{1, \\dots, \\lfloor ns \\rfloor\\}$ and $\\{\\lfloor ns \\rfloor+1, \\dots, \\lfloor nt \\rfloor\\}$ are disjoint. Because the random variables $\\{X_k\\}_{k \\ge 1}$ are independent, the random variable $W_{n}(s)$ and the increment $W_{n}(t) - W_{n}(s)$ are independent.\n\nThe covariance of two independent random variables is $0$. Therefore,\n$$\n\\operatorname{Cov}(W_{n}(s), W_{n}(t) - W_{n}(s)) = 0\n$$\nThis simplifies our main expression to:\n$$\n\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\operatorname{Var}(W_{n}(s)) \\quad (\\text{for } s \\le t)\n$$\nNow we compute the variance of $W_{n}(s)$:\n$$\n\\operatorname{Var}(W_{n}(s)) = \\operatorname{Var}\\left(\\frac{1}{\\sqrt{n}} \\sum_{k=1}^{\\lfloor n s \\rfloor} X_{k}\\right) = \\frac{1}{(\\sqrt{n})^2} \\operatorname{Var}\\left(\\sum_{k=1}^{\\lfloor n s \\rfloor} X_{k}\\right) = \\frac{1}{n} \\operatorname{Var}\\left(\\sum_{k=1}^{\\lfloor n s \\rfloor} X_{k}\\right)\n$$\nSince the $X_k$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}\\left(\\sum_{k=1}^{\\lfloor n s \\rfloor} X_{k}\\right) = \\sum_{k=1}^{\\lfloor n s \\rfloor} \\operatorname{Var}(X_k)\n$$\nAs the $X_k$ are identically distributed with $\\operatorname{Var}(X_1) = 1$, we have $\\operatorname{Var}(X_k) = 1$ for all $k$.\n$$\n\\sum_{k=1}^{\\lfloor n s \\rfloor} \\operatorname{Var}(X_k) = \\sum_{k=1}^{\\lfloor n s \\rfloor} 1 = \\lfloor n s \\rfloor\n$$\nSubstituting this back, we get:\n$$\n\\operatorname{Var}(W_{n}(s)) = \\frac{\\lfloor n s \\rfloor}{n}\n$$\nSo, for $s \\le t$, we have found that $\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\frac{\\lfloor n s \\rfloor}{n}$.\n\nBy a symmetric argument, if we had assumed $t \\le s$, we would find $\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\operatorname{Var}(W_{n}(t)) = \\frac{\\lfloor n t \\rfloor}{n}$.\nWe can combine both cases into a single expression:\n$$\n\\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\frac{\\min(\\lfloor n s \\rfloor, \\lfloor n t \\rfloor)}{n}\n$$\nThis is the explicit function of $n, s, t$ for the covariance.\n\nNext, we must justify rigorously that this covariance converges to $\\min(s,t)$ as $n \\to \\infty$.\nLet us again assume, without loss of generality, that $s \\le t$. Then $\\min(s,t) = s$, and as shown before, $\\min(\\lfloor ns \\rfloor, \\lfloor nt \\rfloor) = \\lfloor ns \\rfloor$.\nWe need to find the limit:\n$$\n\\lim_{n \\to \\infty} \\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\lim_{n \\to \\infty} \\frac{\\lfloor n s \\rfloor}{n}\n$$\nThe floor function $\\lfloor x \\rfloor$ satisfies the inequality $x - 1  \\lfloor x \\rfloor \\le x$ for any real number $x$. Let $x = ns$.\n$$\nns - 1  \\lfloor ns \\rfloor \\le ns\n$$\nSince $n \\in \\mathbb{N}$, $n  0$, we can divide the inequality by $n$ without changing the direction of the inequalities:\n$$\n\\frac{ns - 1}{n}  \\frac{\\lfloor ns \\rfloor}{n} \\le \\frac{ns}{n}\n$$\n$$\ns - \\frac{1}{n}  \\frac{\\lfloor ns \\rfloor}{n} \\le s\n$$\nNow we take the limit as $n \\to \\infty$. We have:\n$$\n\\lim_{n \\to \\infty} \\left(s - \\frac{1}{n}\\right) = s - 0 = s\n$$\nand\n$$\n\\lim_{n \\to \\infty} s = s\n$$\nBy the Squeeze Theorem (or Sandwich Theorem), since $\\frac{\\lfloor ns \\rfloor}{n}$ is bounded between two functions that both converge to $s$, it must also converge to $s$.\n$$\n\\lim_{n \\to \\infty} \\frac{\\lfloor ns \\rfloor}{n} = s\n$$\nSince we assumed $s \\le t$, we have $s = \\min(s,t)$. Therefore, the limit of the covariance is $\\min(s,t)$. This result holds for any $s,t \\in [0,1]$. This limiting function is the covariance function of a standard one-dimensional Brownian motion.\n$$\n\\lim_{n \\to \\infty} \\operatorname{Cov}(W_{n}(s), W_{n}(t)) = \\min(s, t)\n$$", "answer": "$$\n\\boxed{\\min(s,t)}\n$$", "id": "3050159"}, {"introduction": "To establish a stronger form of convergence, we must show that the entire joint distribution converges, not just the second moments. This advanced exercise guides you through proving the convergence of finite-dimensional distributions using the powerful tool of characteristic functions. Completing this practice will solidify your understanding of the core mathematical machinery behind the functional central limit theorem [@problem_id:3050151].", "problem": "Let $\\{X_{m}\\}_{m \\geq 1}$ be independent and identically distributed real-valued random variables with $\\mathbb{E}[X_{1}] = 0$, $\\mathrm{Var}(X_{1}) = 1$, and $\\mathbb{E}[|X_{1}|^{3}]  \\infty$. For each integer $n \\geq 1$, define the partial-sum process $W_{n}(t)$ on the interval $[0,1]$ by\n$$\nW_{n}(t) = \\frac{1}{\\sqrt{n}} \\sum_{m=1}^{\\lfloor n t \\rfloor} X_{m}, \\quad t \\in [0,1].\n$$\nFix integers $k \\geq 1$ and times $0 \\leq t_{1} \\leq \\cdots \\leq t_{k} \\leq 1$. For a vector $u = (u_{1},\\dots,u_{k}) \\in \\mathbb{R}^{k}$, let $\\psi_{n}(u)$ denote the joint characteristic function of the random vector $(W_{n}(t_{1}),\\dots,W_{n}(t_{k}))$, that is,\n$$\n\\psi_{n}(u) = \\mathbb{E}\\left[\\exp\\!\\left(i \\sum_{j=1}^{k} u_{j} W_{n}(t_{j})\\right)\\right].\n$$\nStarting only from the definition of characteristic functions and independence of $\\{X_{m}\\}_{m \\geq 1}$, do the following:\n\n- Derive an explicit representation of $\\psi_{n}(u)$ in terms of the common characteristic function $\\varphi(\\theta) = \\mathbb{E}[\\exp(i \\theta X_{1})]$ and suitable coefficients that depend on $u$ and $(t_{1},\\dots,t_{k})$.\n\n- Using your representation and assuming only the given moment conditions, show that $\\psi_{n}(u)$ converges, as $n \\to \\infty$, to the joint characteristic function of a centered Gaussian vector with covariance matrix whose $(i,j)$-entry is $\\min(t_{i},t_{j})$.\n\nProvide as your final answer the closed-form analytic expression for the limiting joint characteristic function $\\psi(u)$ as a function of $u$ and $(t_{1},\\dots,t_{k})$. Your final answer must be a single expression. No numerical approximation or rounding is required.", "solution": "The user has provided a problem statement that must be validated before attempting a solution.\n\n### Step 1: Extract Givens\n-   $\\{X_{m}\\}_{m \\geq 1}$ is a sequence of independent and identically distributed (i.i.d.) real-valued random variables.\n-   $\\mathbb{E}[X_{1}] = 0$.\n-   $\\mathrm{Var}(X_{1}) = 1$.\n-   $\\mathbb{E}[|X_{1}|^{3}]  \\infty$.\n-   The partial-sum process is defined as $W_{n}(t) = \\frac{1}{\\sqrt{n}} \\sum_{m=1}^{\\lfloor n t \\rfloor} X_{m}$ for $t \\in [0,1]$ and integer $n \\geq 1$.\n-   The times are ordered: $0 \\leq t_{1} \\leq \\cdots \\leq t_{k} \\leq 1$ for a fixed integer $k \\geq 1$.\n-   The joint characteristic function of the vector $(W_{n}(t_{1}),\\dots,W_{n}(t_{k}))$ is $\\psi_{n}(u) = \\mathbb{E}\\left[\\exp\\!\\left(i \\sum_{j=1}^{k} u_{j} W_{n}(t_{j})\\right)\\right]$ for $u = (u_{1},\\dots,u_{k}) \\in \\mathbb{R}^{k}$.\n-   The common characteristic function of the $X_m$ is $\\varphi(\\theta) = \\mathbb{E}[\\exp(i \\theta X_{1})]$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in the theory of stochastic processes, specifically related to the proof of Donsker's invariance principle. All concepts—i.i.d. random variables, moments, characteristic functions, stochastic processes, and convergence—are fundamental to modern probability theory. The assumptions and the goal are standard for establishing the convergence of finite-dimensional distributions of a random walk to those of a Brownian motion.\n-   **Well-Posed**: The problem is well-posed. It asks for a derivation and a proof of convergence. The given moment conditions ($\\mathbb{E}[X_{1}]=0$, $\\mathrm{Var}(X_{1})=1$, $\\mathbb{E}[|X_{1}|^3]  \\infty$) are sufficient to carry out the necessary Taylor expansions of the characteristic function to prove the central-limit-theorem-type convergence. The structure is clear, and a unique, meaningful solution exists.\n-   **Objective**: The problem is stated in precise, objective mathematical language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is scientifically sound, well-posed, completely formalizable, and its premises are the standard assumptions for the theorem it asks to prove a part of.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n---\n\nThe solution proceeds in two main parts as requested. First, we derive an explicit representation for $\\psi_{n}(u)$. Second, we compute the limit of this representation as $n \\to \\infty$.\n\n**Part 1: Representation of $\\psi_{n}(u)$**\n\nWe begin with the definition of $\\psi_{n}(u)$:\n$$\n\\psi_{n}(u) = \\mathbb{E}\\left[\\exp\\!\\left(i \\sum_{j=1}^{k} u_{j} W_{n}(t_{j})\\right)\\right]\n$$\nThe linear combination of the $W_{n}(t_{j})$ in the exponent is not a sum of independent random variables, which complicates taking the expectation. To address this, we re-express the sum in terms of the independent increments of the process $W_n(t)$. Let $t_0 = 0$ and $W_n(t_0) = 0$. The increments are $W_n(t_l) - W_n(t_{l-1})$ for $l=1, \\dots, k$. We can write $W_n(t_j) = \\sum_{l=1}^{j} (W_n(t_l) - W_n(t_{l-1}))$. Substituting this into the sum in the exponent:\n$$\n\\sum_{j=1}^{k} u_{j} W_{n}(t_{j}) = \\sum_{j=1}^{k} u_{j} \\sum_{l=1}^{j} (W_{n}(t_{l}) - W_{n}(t_{l-1}))\n$$\nWe can change the order of summation. The term for a fixed $l$ (where $1 \\leq l \\leq k$) appears in the inner sum for all $j$ such that $l \\leq j \\leq k$.\n$$\n\\sum_{j=1}^{k} u_{j} W_{n}(t_{j}) = \\sum_{l=1}^{k} (W_{n}(t_{l}) - W_{n}(t_{l-1})) \\sum_{j=l}^{k} u_{j}\n$$\nLet us define new coefficients $c_l = \\sum_{j=l}^{k} u_j$ for $l=1, \\dots, k$. The sum becomes:\n$$\n\\sum_{j=1}^{k} u_{j} W_{n}(t_{j}) = \\sum_{l=1}^{k} c_{l} (W_{n}(t_{l}) - W_{n}(t_{l-1}))\n$$\nThe increment $W_n(t_l) - W_n(t_{l-1})$ is given by:\n$$\nW_{n}(t_{l}) - W_{n}(t_{l-1}) = \\frac{1}{\\sqrt{n}} \\sum_{m=1}^{\\lfloor n t_l \\rfloor} X_{m} - \\frac{1}{\\sqrt{n}} \\sum_{m=1}^{\\lfloor n t_{l-1} \\rfloor} X_{m} = \\frac{1}{\\sqrt{n}} \\sum_{m = \\lfloor n t_{l-1} \\rfloor + 1}^{\\lfloor n t_l \\rfloor} X_{m}\n$$\nSince the time points are ordered $t_1 \\leq \\cdots \\leq t_k$, we have $\\lfloor n t_{l-1} \\rfloor \\leq \\lfloor n t_l \\rfloor$. The sums for different increments $l$ are over disjoint sets of an i.i.d. sequence $\\{X_m\\}$. Therefore, the random variables $\\{W_n(t_l) - W_n(t_{l-1})\\}_{l=1}^k$ are independent.\n\nNow we can compute the expectation. Let $\\Delta W_{n,l} = W_n(t_l) - W_n(t_{l-1})$.\n$$\n\\psi_{n}(u) = \\mathbb{E}\\left[\\exp\\left(i \\sum_{l=1}^{k} c_{l} \\Delta W_{n,l}\\right)\\right] = \\mathbb{E}\\left[\\prod_{l=1}^{k} \\exp\\left(i c_{l} \\Delta W_{n,l}\\right)\\right]\n$$\nBy the independence of the increments, the expectation of the product is the product of the expectations:\n$$\n\\psi_{n}(u) = \\prod_{l=1}^{k} \\mathbb{E}\\left[\\exp\\left(i c_{l} \\Delta W_{n,l}\\right)\\right]\n$$\nLet's analyze each term in the product. Let $N_{l} = \\lfloor n t_l \\rfloor - \\lfloor n t_{l-1} \\rfloor$ be the number of random variables in the $l$-th increment. The characteristic function of a sum of $N_l$ i.i.d. random variables is the $N_l$-th power of their common characteristic function.\n$$\n\\mathbb{E}\\left[\\exp\\left(i c_{l} \\Delta W_{n,l}\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(i \\frac{c_l}{\\sqrt{n}} \\sum_{m = \\lfloor n t_{l-1} \\rfloor + 1}^{\\lfloor n t_l \\rfloor} X_{m}\\right)\\right] = \\left(\\varphi\\left(\\frac{c_l}{\\sqrt{n}}\\right)\\right)^{N_l}\n$$\nSubstituting this back, we get the desired representation:\n$$\n\\psi_{n}(u) = \\prod_{l=1}^{k} \\left(\\varphi\\left(\\frac{c_{l}}{\\sqrt{n}}\\right)\\right)^{\\lfloor n t_{l} \\rfloor - \\lfloor n t_{l-1} \\rfloor}, \\quad \\text{where } c_{l} = \\sum_{j=l}^{k} u_{j} \\text{ and } t_0 = 0.\n$$\n\n**Part 2: Convergence of $\\psi_{n}(u)$ as $n \\to \\infty$**\n\nTo find the limit of $\\psi_n(u)$, we first analyze the limit of its logarithm, $\\ln \\psi_n(u)$.\n$$\n\\ln \\psi_{n}(u) = \\sum_{l=1}^{k} (\\lfloor n t_{l} \\rfloor - \\lfloor n t_{l-1} \\rfloor) \\ln\\left(\\varphi\\left(\\frac{c_{l}}{\\sqrt{n}}\\right)\\right)\n$$\nThe moment condition $\\mathbb{E}[|X_1|^3]  \\infty$ implies that $\\varphi(\\theta)$ is three times continuously differentiable. We can use a Taylor expansion of $\\varphi(\\theta)$ around $\\theta=0$. The derivatives are:\n$\\varphi(0) = \\mathbb{E}[\\exp(0)] = 1$.\n$\\varphi'(0) = \\mathbb{E}[i X_1] = i \\mathbb{E}[X_1] = 0$.\n$\\varphi''(0) = \\mathbb{E}[i^2 X_1^2] = - \\mathbb{E}[X_1^2] = -(\\mathrm{Var}(X_1) + (\\mathbb{E}[X_1])^2) = -(1 + 0^2) = -1$.\nThe Taylor expansion is $\\varphi(\\theta) = 1 + 0 \\cdot \\theta - \\frac{1}{2}\\theta^2 + O(|\\theta|^3) = 1 - \\frac{1}{2}\\theta^2 + O(|\\theta|^3)$.\nLet $\\theta_n = c_l/\\sqrt{n}$. As $n \\to \\infty$, $\\theta_n \\to 0$.\n$$\n\\varphi\\left(\\frac{c_{l}}{\\sqrt{n}}\\right) = 1 - \\frac{1}{2}\\left(\\frac{c_l}{\\sqrt{n}}\\right)^2 + O\\left(n^{-3/2}\\right) = 1 - \\frac{c_l^2}{2n} + O\\left(n^{-3/2}\\right)\n$$\nNow, we use the Taylor expansion for the natural logarithm, $\\ln(1+x) = x - x^2/2 + \\dots = x + O(x^2)$ for small $x$. Let $x = - \\frac{c_l^2}{2n} + O(n^{-3/2})$.\n$$\n\\ln\\left(\\varphi\\left(\\frac{c_{l}}{\\sqrt{n}}\\right)\\right) = \\ln\\left(1 - \\frac{c_l^2}{2n} + O(n^{-3/2})\\right) = \\left(- \\frac{c_l^2}{2n} + O(n^{-3/2})\\right) + O\\left(\\frac{1}{n^2}\\right) = -\\frac{c_l^2}{2n} + O(n^{-3/2})\n$$\nSubstituting this into the expression for $\\ln \\psi_n(u)$:\n$$\n\\ln \\psi_{n}(u) = \\sum_{l=1}^{k} (\\lfloor n t_{l} \\rfloor - \\lfloor n t_{l-1} \\rfloor) \\left(-\\frac{c_l^2}{2n} + O(n^{-3/2})\\right)\n$$\nWe use the approximation $\\lfloor nt \\rfloor = nt - \\{nt\\}$, where $0 \\leq \\{nt\\}  1$, so $\\lfloor nt \\rfloor = nt + O(1)$.\n$$\n\\ln \\psi_{n}(u) = \\sum_{l=1}^{k} (n(t_l - t_{l-1}) + O(1)) \\left(-\\frac{c_l^2}{2n} + O(n^{-3/2})\\right)\n$$\nExpanding this product:\n$$\n\\ln \\psi_{n}(u) = \\sum_{l=1}^{k} \\left[ n(t_l - t_{l-1})\\left(-\\frac{c_l^2}{2n}\\right) + n(t_l - t_{l-1}) O(n^{-3/2}) + O(1)\\left(-\\frac{c_l^2}{2n}\\right) + O(1)O(n^{-3/2}) \\right]\n$$\n$$\n\\ln \\psi_{n}(u) = \\sum_{l=1}^{k} \\left[ -\\frac{c_l^2}{2}(t_l - t_{l-1}) + O(n^{-1/2}) + O(n^{-1}) + O(n^{-3/2}) \\right]\n$$\nTaking the limit as $n \\to \\infty$, all terms with $n$ in the denominator vanish.\n$$\n\\lim_{n \\to \\infty} \\ln \\psi_{n}(u) = -\\frac{1}{2} \\sum_{l=1}^{k} (t_l - t_{l-1}) c_l^2\n$$\nThe final step is to show that this quadratic form in $u$ is the one specified in the problem. Recall $c_l = \\sum_{j=l}^k u_j$.\n$$\n\\sum_{l=1}^{k} (t_l - t_{l-1}) c_l^2 = \\sum_{l=1}^{k} (t_l - t_{l-1}) \\left(\\sum_{i=l}^{k} u_i\\right) \\left(\\sum_{j=l}^{k} u_j\\right) = \\sum_{l=1}^{k} (t_l - t_{l-1}) \\sum_{i=l}^{k} \\sum_{j=l}^{k} u_i u_j\n$$\nWe change the order of summation to find the coefficient of a generic term $u_i u_j$.\n$$\n= \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i u_j \\left( \\sum_{l=1}^{k} (t_l - t_{l-1}) \\mathbb{I}(l \\le i \\text{ and } l \\le j) \\right)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The condition $l \\le i$ and $l \\le j$ is equivalent to $l \\le \\min(i,j)$. The inner sum becomes a telescoping sum:\n$$\n\\sum_{l=1}^{\\min(i,j)} (t_l - t_{l-1}) = (t_1 - t_0) + (t_2 - t_1) + \\cdots + (t_{\\min(i,j)} - t_{\\min(i,j)-1}) = t_{\\min(i,j)} - t_0 = t_{\\min(i,j)}\n$$\nSince the times are ordered $t_1 \\leq t_2 \\leq \\dots \\leq t_k$, if $i \\leq j$, then $\\min(i,j)=i$ and $t_i \\leq t_j$, so $t_{\\min(i,j)} = t_i = \\min(t_i, t_j)$. Similarly, if $j  i$, then $\\min(i,j)=j$ and $t_j  t_i$, so $t_{\\min(i,j)} = t_j = \\min(t_i, t_j)$. In all cases, $t_{\\min(i,j)} = \\min(t_i, t_j)$.\nTherefore, the quadratic form is:\n$$\n\\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i u_j \\min(t_i, t_j)\n$$\nSo we have shown that:\n$$\n\\lim_{n \\to \\infty} \\ln \\psi_{n}(u) = -\\frac{1}{2} \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i u_j \\min(t_i, t_j)\n$$\nBy the continuity of the exponential function (and Lévy's continuity theorem), a sequence of characteristic functions converging pointwise to a continuous function implies that the limit is also a characteristic function and the corresponding distributions converge weakly.\n$$\n\\psi(u) = \\lim_{n \\to \\infty} \\psi_{n}(u) = \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i u_j \\min(t_i, t_j)\\right)\n$$\nThis is a characteristic function of a $k$-dimensional centered Gaussian vector with a covariance matrix $\\Sigma$ whose entries are $\\Sigma_{ij} = \\min(t_i, t_j)$. This completes the proof.\n\nThe final answer is the limiting joint characteristic function $\\psi(u)$.", "answer": "$$\n\\boxed{\\exp\\left(-\\frac{1}{2} \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i u_j \\min(t_i, t_j)\\right)}\n$$", "id": "3050151"}]}