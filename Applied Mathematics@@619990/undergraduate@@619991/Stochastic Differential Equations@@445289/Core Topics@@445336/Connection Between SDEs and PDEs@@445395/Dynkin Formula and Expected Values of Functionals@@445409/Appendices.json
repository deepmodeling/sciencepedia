{"hands_on_practices": [{"introduction": "The infinitesimal generator, denoted $L$, is the central operator linking a stochastic differential equation (SDE) to its analytical properties. This first practice provides a hands-on derivation of the generator for a general linear SDE, starting from Itô's formula. By applying it to simple polynomial functions, we will uncover the direct physical interpretation of its components as the instantaneous mean (drift) and covariance (diffusion) of the process [@problem_id:3051749].", "problem": "Consider the time-homogeneous Markov process in $\\mathbb{R}^{d}$ defined as the solution to the Stochastic Differential Equation (SDE)\n$$\ndX_{t}=\\mu\\,dt+\\Sigma\\,dW_{t},\n$$\nwhere $X_{t}\\in\\mathbb{R}^{d}$, $\\mu\\in\\mathbb{R}^{d}$ is a constant drift vector, $\\Sigma\\in\\mathbb{R}^{d\\times m}$ is a constant diffusion matrix, and $W_{t}\\in\\mathbb{R}^{m}$ is a standard $m$-dimensional Wiener process. Let $b:=\\mu$ and $\\sigma:=\\Sigma$, and define $a:=\\sigma\\sigma^{\\top}\\in\\mathbb{R}^{d\\times d}$. Using only fundamental principles (the definition of the infinitesimal generator for a time-homogeneous Markov process and Itô’s formula), derive the general form of the infinitesimal generator $L$ acting on twice continuously differentiable functions $f:\\mathbb{R}^{d}\\to\\mathbb{R}$. Then, explicitly compute $L f$ for the test functions $f(x)=x_{i}$ and $f(x)=x_{i}x_{j}$, where $x=(x_{1},\\dots,x_{d})$ and $i,j\\in\\{1,\\dots,d\\}$. Finally, interpret the roles of the terms $b$ and $a$ in the generator in terms of drift and instantaneous covariance. Your final answer must be the analytic expressions for $L f$ evaluated on $f(x)=x_{i}$ and $f(x)=x_{i}x_{j}$. No rounding is required, and no units are involved.", "solution": "The problem asks for the derivation of the infinitesimal generator of a time-homogeneous Markov process defined by a stochastic differential equation (SDE), and its application to specific functions. The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution.\n\nFirst, we derive the general form of the infinitesimal generator $L$ for the process $X_t \\in \\mathbb{R}^d$ governed by the SDE:\n$$\ndX_{t}=\\mu\\,dt+\\Sigma\\,dW_{t}\n$$\nHere, $\\mu \\in \\mathbb{R}^d$ is a constant vector, $\\Sigma \\in \\mathbb{R}^{d \\times m}$ is a constant matrix, and $W_t$ is a standard $m$-dimensional Wiener process. We are given the notation $b := \\mu$ and $a := \\Sigma\\Sigma^\\top$. The matrix $a \\in \\mathbb{R}^{d \\times d}$ is symmetric and positive semi-definite. The generator $L$ acts on functions $f \\in C^2(\\mathbb{R}^d, \\mathbb{R})$.\n\nThe derivation proceeds from two fundamental principles: the definition of the generator and Itô's formula for a multivariate process.\n\nThe infinitesimal generator $L$ is defined by the limit:\n$$\nLf(x) = \\lim_{t \\to 0^+} \\frac{\\mathbb{E}_x[f(X_t)] - f(x)}{t}\n$$\nwhere $\\mathbb{E}_x[\\cdot]$ denotes the expectation conditional on the process starting at $x$.\n\nWe apply Itô's formula to the function $f(X_t)$. The formula for a function of a $d$-dimensional Itô process $X_t$ is:\n$$\ndf(X_t) = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}(X_t) dX_{t,i} + \\frac{1}{2}\\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) d\\langle X_i, X_j \\rangle_t\n$$\nwhere $X_{t,i}$ is the $i$-th component of $X_t$, and $\\langle X_i, X_j \\rangle_t$ is the quadratic covariation process between $X_{t,i}$ and $X_{t,j}$.\n\nFrom the SDE, the $i$-th component is $dX_{t,i} = \\mu_i dt + \\sum_{k=1}^m \\Sigma_{ik} dW_{t,k}$. The differential of the quadratic covariation is found using the Itô multiplication rules $dt \\cdot dt = 0$, $dt \\cdot dW_t = 0$, and $dW_{t,k} \\cdot dW_{t,l} = \\delta_{kl} dt$, where $\\delta_{kl}$ is the Kronecker delta.\n$$\nd\\langle X_i, X_j \\rangle_t = (dX_{t,i} - \\mu_i dt)(dX_{t,j} - \\mu_j dt) = \\left(\\sum_{k=1}^m \\Sigma_{ik} dW_{t,k}\\right) \\left(\\sum_{l=1}^m \\Sigma_{jl} dW_{t,l}\\right)\n$$\n$$\nd\\langle X_i, X_j \\rangle_t = \\sum_{k=1}^m \\sum_{l=1}^m \\Sigma_{ik} \\Sigma_{jl} d\\langle W_k, W_l \\rangle_t = \\sum_{k=1}^m \\sum_{l=1}^m \\Sigma_{ik} \\Sigma_{jl} \\delta_{kl} dt = \\sum_{k=1}^m \\Sigma_{ik} \\Sigma_{jk} dt\n$$\nThe sum is the $(i,j)$-th entry of the matrix product $\\Sigma\\Sigma^\\top$. Using the definition $a := \\Sigma\\Sigma^\\top$, we have $d\\langle X_i, X_j \\rangle_t = a_{ij} dt$.\n\nSubstituting $dX_{t,i}$ and $d\\langle X_i, X_j \\rangle_t$ back into Itô's formula:\n$$\ndf(X_t) = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}(X_t) \\left(\\mu_i dt + \\sum_{k=1}^m \\Sigma_{ik} dW_{t,k}\\right) + \\frac{1}{2}\\sum_{i=1}^d \\sum_{j=1}^d a_{ij} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) dt\n$$\nWe group the terms by $dt$ (the drift part) and $dW_t$ (the martingale part):\n$$\ndf(X_t) = \\left( \\sum_{i=1}^d \\mu_i \\frac{\\partial f}{\\partial x_i}(X_t) + \\frac{1}{2}\\sum_{i,j=1}^d a_{ij} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) \\right) dt + \\sum_{i=1}^d \\sum_{k=1}^m \\Sigma_{ik} \\frac{\\partial f}{\\partial x_i}(X_t) dW_{t,k}\n$$\nLet's define the operator $\\mathcal{A}$ acting on $f$ as the expression in the parenthesis:\n$$\n\\mathcal{A}f(x) := \\sum_{i=1}^d \\mu_i \\frac{\\partial f}{\\partial x_i}(x) + \\frac{1}{2}\\sum_{i,j=1}^d a_{ij} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x)\n$$\nIntegrating $df(X_t)$ from $0$ to $t$ gives:\n$$\nf(X_t) - f(X_0) = \\int_0^t \\mathcal{A}f(X_s) ds + \\int_0^t \\sum_{i=1}^d \\sum_{k=1}^m \\Sigma_{ik} \\frac{\\partial f}{\\partial x_i}(X_s) dW_{s,k}\n$$\nTaking the expectation conditional on $X_0=x$ and noting that the expectation of the Itô integral (the last term) is zero:\n$$\n\\mathbb{E}_x[f(X_t)] - f(x) = \\mathbb{E}_x\\left[\\int_0^t \\mathcal{A}f(X_s) ds \\right]\n$$\nDividing by $t$ and taking the limit as $t \\to 0^+$:\n$$\n\\lim_{t \\to 0^+} \\frac{\\mathbb{E}_x[f(X_t)] - f(x)}{t} = \\lim_{t \\to 0^+} \\frac{1}{t} \\mathbb{E}_x\\left[\\int_0^t \\mathcal{A}f(X_s) ds \\right]\n$$\nSince $f \\in C^2$ and $X_s$ has continuous sample paths, the integrand $\\mathcal{A}f(X_s)$ is continuous in $s$. By the properties of expectation and the fundamental theorem of calculus, the limit is $\\mathcal{A}f(x)$.\n$$\n\\lim_{t \\to 0^+} \\frac{\\mathbb{E}_x[f(X_t)] - f(x)}{t} = \\mathcal{A}f(x)\n$$\nComparing this with the definition of the infinitesimal generator, we find that $L = \\mathcal{A}$. Using the notation $b=\\mu$, the general form of the generator is:\n$$\nLf(x) = \\sum_{i=1}^d b_i \\frac{\\partial f}{\\partial x_i}(x) + \\frac{1}{2}\\sum_{i=1}^d \\sum_{j=1}^d a_{ij} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x)\n$$\nNow, we compute $Lf$ for the two specified test functions. Let $x = (x_1, \\dots, x_d)$.\n\nCase 1: $f(x) = x_i$ for a fixed index $i \\in \\{1, \\dots, d\\}$.\nThe partial derivatives with respect to the variable $x_k$ are:\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial x_i}{\\partial x_k} = \\delta_{ik}\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x_k \\partial x_l} = 0\n$$\nSubstituting these into the generator formula:\n$$\nLf(x) = L(x_i) = \\sum_{k=1}^d b_k \\delta_{ik} + \\frac{1}{2}\\sum_{k,l=1}^d a_{kl} (0) = b_i\n$$\n\nCase 2: $f(x) = x_i x_j$ for fixed indices $i, j \\in \\{1, \\dots, d\\}$.\nThe partial derivatives with respect to $x_k$ and $x_l$ are:\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial(x_i x_j)}{\\partial x_k} = \\delta_{ik}x_j + x_i\\delta_{jk}\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x_k \\partial x_l} = \\frac{\\partial}{\\partial x_l}(\\delta_{ik}x_j + x_i\\delta_{jk}) = \\delta_{ik}\\delta_{jl} + \\delta_{il}\\delta_{jk}\n$$\nSubstituting these into the generator formula:\n$$\nLf(x) = L(x_i x_j) = \\sum_{k=1}^d b_k (\\delta_{ik}x_j + x_i\\delta_{jk}) + \\frac{1}{2}\\sum_{k,l=1}^d a_{kl} (\\delta_{ik}\\delta_{jl} + \\delta_{il}\\delta_{jk})\n$$\nThe first sum evaluates to:\n$$\n\\sum_{k=1}^d b_k \\delta_{ik}x_j + \\sum_{k=1}^d b_k x_i\\delta_{jk} = b_i x_j + b_j x_i\n$$\nThe second sum evaluates to:\n$$\n\\frac{1}{2} \\left( \\sum_{k,l=1}^d a_{kl} \\delta_{ik}\\delta_{jl} + \\sum_{k,l=1}^d a_{kl} \\delta_{il}\\delta_{jk} \\right)\n$$\nThe first term inside the parenthesis collapses to $a_{ij}$ (by setting $k=i, l=j$). The second term collapses to $a_{ji}$ (by setting $k=j, l=i$). Thus, the expression becomes:\n$$\n\\frac{1}{2}(a_{ij} + a_{ji})\n$$\nSince $a = \\Sigma\\Sigma^\\top$, $a$ is symmetric, so $a_{ij} = a_{ji}$. The expression simplifies to $\\frac{1}{2}(a_{ij} + a_{ij}) = a_{ij}$.\nCombining the parts, we get:\n$$\nLf(x) = L(x_i x_j) = b_j x_i + b_i x_j + a_{ij}\n$$\n\nFinally, we interpret the roles of $b$ and $a$.\nThe generator gives the instantaneous expected rate of change of a function of the process. For $f(x)=x_i$, $L(x_i)=b_i$ shows that $b_i$ is the instantaneous expected rate of change of the $i$-th component of the process, $X_{t,i}$. This is the definition of the drift of the process. The vector $b$ is the drift vector.\n\nFor the second moment, we can relate $L(x_i x_j)$ to the instantaneous covariance. From the definition of $L$:\n$$\nL(x_i x_j) = \\lim_{t \\to 0^+} \\frac{\\mathbb{E}_x[X_{t,i}X_{t,j}] - x_i x_j}{t} = b_j x_i + b_i x_j + a_{ij}\n$$\nThe instantaneous rate of change of the covariance is:\n$$\n\\lim_{t \\to 0^+} \\frac{\\mathrm{Cov}_x(X_{t,i}, X_{t,j})}{t} = \\lim_{t \\to 0^+} \\frac{\\mathbb{E}_x[X_{t,i}X_{t,j}] - \\mathbb{E}_x[X_{t,i}]\\mathbb{E}_x[X_{t,j}]}{t}\n$$\nWe know $\\mathbb{E}_x[X_{t,i}] = x_i + b_i t$. Thus $\\mathbb{E}_x[X_{t,i}]\\mathbb{E}_x[X_{t,j}] = (x_i + b_i t)(x_j + b_j t) = x_i x_j + (b_j x_i + b_i x_j) t + O(t^2)$.\nSubstituting this and the expression for $\\mathbb{E}_x[X_{t,i}X_{t,j}]$ from the generator:\n$$\n\\mathbb{E}_x[X_{t,i}X_{t,j}] = x_i x_j + (b_j x_i + b_i x_j + a_{ij})t + o(t)\n$$\n$$\n\\lim_{t \\to 0^+} \\frac{\\mathrm{Cov}_x(X_{t,i}, X_{t,j})}{t} = \\lim_{t \\to 0^+} \\frac{(x_i x_j + (b_j x_i + b_i x_j + a_{ij})t) - (x_i x_j + (b_j x_i + b_i x_j)t) + o(t)}{t} = a_{ij}\n$$\nThis demonstrates that $a_{ij}$ is the instantaneous rate of increase of the covariance between $X_{t,i}$ and $X_{t,j}$. The matrix $a = \\Sigma\\Sigma^\\top$ is the instantaneous covariance matrix, representing the magnitude and correlation structure of the random fluctuations.\n\nThe final answer requires the expressions for $L f$ on the two test functions.\nFor $f(x)=x_i$: $Lf(x) = b_i$.\nFor $f(x)=x_i x_j$: $Lf(x) = b_j x_i + b_i x_j + a_{ij}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} b_i  b_j x_i + b_i x_j + a_{ij} \\end{pmatrix}}\n$$", "id": "3051749"}, {"introduction": "One of the most powerful applications of the generator is in solving exit problems, which are often difficult to tackle directly. In this exercise, we will determine the probability that a standard Brownian motion exits an interval through a specific endpoint before the other. By leveraging the connection between martingales and the generator, we transform this stochastic question into a simple boundary value problem, demonstrating the profound link between random processes and deterministic differential equations [@problem_id:3051722].", "problem": "Consider a one-dimensional standard Brownian motion process $X_{t}$ starting at $X_{0}=x$ with $x \\in (a,b)$, where $ab$ are fixed real numbers. The process satisfies the Stochastic Differential Equation (SDE) $dX_{t}=dW_{t}$, where $W_{t}$ is a standard Brownian motion. Define the first hitting times $\\tau_{\\{a\\}}=\\inf\\{t\\geq 0: X_{t}=a\\}$ and $\\tau_{\\{b\\}}=\\inf\\{t\\geq 0: X_{t}=b\\}$, and let $\\tau=\\tau_{\\{a\\}} \\wedge \\tau_{\\{b\\}}$ denote the first exit time from the open interval $(a,b)$. For a function $f$ that is twice continuously differentiable and bounded on $(a,b)$ with continuous extension to $[a,b]$, Dynkin's formula states that for the generator $L$ of $X_{t}$, the identity $\\mathbb{E}_{x}\\big[f(X_{t \\wedge \\tau})\\big]=f(x)+\\mathbb{E}_{x}\\Big[\\displaystyle\\int_{0}^{t \\wedge \\tau} Lf(X_{s})\\,ds\\Big]$ holds.\n\nLet $u(x)=\\mathbb{P}_{x}\\big(\\tau_{\\{b\\}}\\tau_{\\{a\\}}\\big)$ denote the probability that the process exits $(a,b)$ through $b$ before $a$, starting from $x$. Using the fundamental characterization of exit probabilities via Dynkin's formula for the stopped process at $\\tau$, derive the boundary value problem that $u$ satisfies on $(a,b)$ with appropriate boundary conditions at $a$ and $b$. Solve this boundary value problem to obtain a closed-form analytic expression for $u(x)$ in terms of $a$, $b$, and $x$. Finally, interpret the resulting functional form by explaining why the solution profile is linear in $x$ for this driftless process.\n\nYour final answer should be the explicit analytic expression for $u(x)$ (no units required). Do not round or approximate.", "solution": "The problem statement is first validated to ensure it is self-contained, scientifically grounded, and well-posed.\n\n### Step 1: Extract Givens\n- **Process**: A one-dimensional standard Brownian motion process $X_{t}$.\n- **SDE**: $dX_{t}=dW_{t}$, where $W_{t}$ is a standard Brownian motion.\n- **Starting Condition**: $X_{0}=x$, with $x \\in (a,b)$ for fixed real numbers $ab$.\n- **Hitting Times**: $\\tau_{\\{a\\}}=\\inf\\{t\\geq 0: X_{t}=a\\}$ and $\\tau_{\\{b\\}}=\\inf\\{t\\geq 0: X_{t}=b\\}$.\n- **Exit Time**: $\\tau=\\tau_{\\{a\\}} \\wedge \\tau_{\\{b\\}}$.\n- **Function to Find**: $u(x)=\\mathbb{P}_{x}\\big(\\tau_{\\{b\\}}\\tau_{\\{a\\}}\\big)$, the probability of exiting $(a,b)$ at $b$.\n- **Provided Tool**: Dynkin's formula, $\\mathbb{E}_{x}\\big[f(X_{t \\wedge \\tau})\\big]=f(x)+\\mathbb{E}_{x}\\Big[\\displaystyle\\int_{0}^{t \\wedge \\tau} Lf(X_{s})\\,ds\\Big]$, for a $C^2$ bounded function $f$ and generator $L$.\n- **Tasks**:\n    1. Derive the boundary value problem (BVP) for $u(x)$.\n    2. Solve the BVP to find a closed-form expression for $u(x)$.\n    3. Interpret the linearity of the solution $u(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in stochastic calculus, specifically concerning the application of Dynkin's formula to determine exit probabilities for an Itô diffusion.\n\n- **Scientifically Grounded**: The problem is based on the well-established mathematical theory of stochastic differential equations and Brownian motion. All concepts like hitting times, exit times, the generator of a diffusion, and Dynkin's formula are core principles in this field.\n- **Well-Posed**: The problem is clearly stated. It asks for the derivation and solution of a well-posed Dirichlet problem for a second-order ordinary differential equation. The existence and uniqueness of the solution are guaranteed under the given conditions.\n- **Objective**: The problem is expressed in precise, objective mathematical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a complete, consistent, and formalizable problem rooted in established mathematical theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\n### Derivation of the Boundary Value Problem\n\nThe stochastic process is given by the SDE $dX_{t}=dW_{t}$. This is an Itô diffusion with drift coefficient $\\mu(x)=0$ and diffusion coefficient $\\sigma(x)=1$. The infinitesimal generator $L$ of this process is given by the operator:\n$$\nL = \\mu(x)\\frac{d}{dx} + \\frac{1}{2}\\sigma(x)^{2}\\frac{d^{2}}{dx^{2}} = 0 \\cdot \\frac{d}{dx} + \\frac{1}{2}(1)^{2}\\frac{d^{2}}{dx^{2}} = \\frac{1}{2}\\frac{d^{2}}{dx^{2}}\n$$\nWe wish to find the function $u(x) = \\mathbb{P}_{x}(\\tau_{\\{b\\}}  \\tau_{\\{a\\}})$. Let us assume that $u(x)$ is a twice continuously differentiable function, i.e., $u \\in C^{2}(a,b)$. The key insight is to show that $u(x)$ satisfies the homogeneous differential equation $Lu(x) = 0$ on the interval $(a,b)$ with specific boundary conditions.\n\nWe use Dynkin's formula for the process stopped at the first exit time $\\tau$. Since for a standard one-dimensional Brownian motion starting in $(a,b)$, the process must eventually exit the interval, we have $\\tau  \\infty$ almost surely. We can thus take the limit as $t \\to \\infty$ in Dynkin's formula. As $t \\to \\infty$, $t \\wedge \\tau \\to \\tau$. For a suitable function $f$, applying the Bounded or Dominated Convergence Theorem, Dynkin's formula becomes:\n$$\n\\mathbb{E}_{x}\\big[f(X_{\\tau})\\big] = f(x) + \\mathbb{E}_{x}\\Big[\\int_{0}^{\\tau} Lf(X_{s})\\,ds\\Big]\n$$\nLet us now hypothesize that the function $u(x)$ is the solution to the Dirichlet problem:\n$$\n\\begin{cases}\nLv(x) = 0,  x \\in (a,b) \\\\\nv(a) = 0 \\\\\nv(b) = 1\n\\end{cases}\n$$\nLet $v(x)$ be such a solution, which we assume to be in $C^2(a,b)$ and continuous on $[a,b]$. We apply the stopped Dynkin's formula to this function $v(x)$. Since $Lv(x)=0$ for all $x \\in (a,b)$, the integral term vanishes:\n$$\n\\mathbb{E}_{x}\\big[v(X_{\\tau})\\big] = v(x) + \\mathbb{E}_{x}\\Big[\\int_{0}^{\\tau} \\underbrace{Lv(X_{s})}_{=0}\\,ds\\Big] = v(x)\n$$\nNow, we evaluate the left-hand side. By definition of $\\tau$, the process stops when $X_t$ first hits either $a$ or $b$. Thus, the random variable $X_{\\tau}$ can only take the values $a$ and $b$. We can expand the expectation:\n$$\n\\mathbb{E}_{x}\\big[v(X_{\\tau})\\big] = v(a) \\cdot \\mathbb{P}_{x}(X_{\\tau}=a) + v(b) \\cdot \\mathbb{P}_{x}(X_{\\tau}=b)\n$$\nBy definition of $\\tau$, the event $\\{X_{\\tau}=a\\}$ is the same as $\\{\\tau_{\\{a\\}}  \\tau_{\\{b\\}}\\}$, and $\\{X_{\\tau}=b\\}$ is the same as $\\{\\tau_{\\{b\\}}  \\tau_{\\{a\\}}\\}$. The probability of the latter event is, by definition, $u(x)$. So, $\\mathbb{P}_{x}(X_{\\tau}=b) = u(x)$ and $\\mathbb{P}_{x}(X_{\\tau}=a) = 1 - u(x)$. Substituting these and the boundary conditions $v(a)=0$ and $v(b)=1$ into the expectation:\n$$\n\\mathbb{E}_{x}\\big[v(X_{\\tau})\\big] = 0 \\cdot \\mathbb{P}_{x}(X_{\\tau}=a) + 1 \\cdot \\mathbb{P}_{x}(X_{\\tau}=b) = \\mathbb{P}_{x}(X_{\\tau}=b) = u(x)\n$$\nBy equating the two expressions for $\\mathbb{E}_{x}[v(X_{\\tau})]$, we find that $v(x) = u(x)$. This confirms that the exit probability $u(x)$ is indeed the solution to the formulated Dirichlet problem.\n\nThe boundary conditions for $u(x)$ are determined by its definition.\n- If the process starts at $x=a$, then $X_0=a$, so $\\tau_{\\{a\\}}=0$. Since $ba$, the process must travel some distance to reach $b$, so $\\tau_{\\{b\\}}0$. Thus, the event $\\tau_{\\{b\\}}\\tau_{\\{a\\}}$ is impossible. Hence, $u(a) = \\mathbb{P}_{a}(\\tau_{\\{b\\}}  \\tau_{\\{a\\}}) = 0$.\n- If the process starts at $x=b$, then $X_0=b$, so $\\tau_{\\{b\\}}=0$. For similar reasons, $\\tau_{\\{a\\}}0$. The event $\\tau_{\\{b\\}}\\tau_{\\{a\\}}$ is certain. Hence, $u(b) = \\mathbb{P}_{b}(\\tau_{\\{b\\}}  \\tau_{\\{a\\}}) = 1$.\n\nSo, the boundary value problem for $u(x)$ is:\n$$\n\\begin{cases}\n\\frac{1}{2} u''(x) = 0,  x \\in (a,b) \\\\\nu(a) = 0 \\\\\nu(b) = 1\n\\end{cases}\n$$\n\n### Solving the Boundary Value Problem\n\nThe differential equation is $u''(x) = 0$. Integrating once with respect to $x$ gives:\n$$\nu'(x) = C_1\n$$\nwhere $C_1$ is a constant of integration. Integrating a second time gives the general solution:\n$$\nu(x) = C_1 x + C_2\n$$\nwhere $C_2$ is another constant of integration. We determine the constants $C_1$ and $C_2$ using the boundary conditions.\n1. $u(a) = 0 \\implies C_1 a + C_2 = 0$\n2. $u(b) = 1 \\implies C_1 b + C_2 = 1$\n\nThis is a system of two linear equations in $C_1$ and $C_2$. Subtracting the first equation from the second yields:\n$$\n(C_1 b + C_2) - (C_1 a + C_2) = 1 - 0\n$$\n$$\nC_1(b - a) = 1\n$$\nSince $ab$, $b-a \\neq 0$, so we can solve for $C_1$:\n$$\nC_1 = \\frac{1}{b-a}\n$$\nSubstituting $C_1$ back into the first equation ($C_2 = -C_1 a$):\n$$\nC_2 = -\\frac{a}{b-a}\n$$\nSubstituting the constants back into the general solution for $u(x)$:\n$$\nu(x) = \\frac{1}{b-a}x - \\frac{a}{b-a} = \\frac{x-a}{b-a}\n$$\nThis is the closed-form analytic expression for the probability $u(x)$.\n\n### Interpretation of the Linear Functional Form\n\nThe solution $u(x) = \\frac{x-a}{b-a}$ is a linear function of the starting position $x$. This linearity is a direct and fundamental consequence of the fact that the underlying process $X_t$ is a martingale.\n\nA process $X_t$ is a martingale if $\\mathbb{E}[|X_t|]  \\infty$ and $\\mathbb{E}[X_t | \\mathcal{F}_s] = X_s$ for all $st$. For a diffusion process, having $X_t$ be a martingale is equivalent to its generator $L$ annihilating the identity function, i.e., $L(f)(x) = 0$ for $f(x)=x$. For our process $X_t$, $L = \\frac{1}{2}\\frac{d^2}{dx^2}$, so $L(x) = \\frac{1}{2}\\frac{d^2}{dx^2}(x) = 0$. Thus, $X_t$ is a martingale.\n\nThe Optional Stopping Theorem states that for a martingale $X_t$ and a suitable stopping time $\\tau$ (which our $\\tau$ is), the stopped process is also a martingale, leading to $\\mathbb{E}_{x}[X_{\\tau}] = X_0 = x$.\nThe expected value of the stopped position can also be expressed in terms of the exit probabilities:\n$$\n\\mathbb{E}_{x}[X_{\\tau}] = a \\cdot \\mathbb{P}_{x}(X_{\\tau}=a) + b \\cdot \\mathbb{P}_{x}(X_{\\tau}=b)\n$$\nSubstituting $\\mathbb{P}_{x}(X_{\\tau}=b) = u(x)$ and $\\mathbb{P}_{x}(X_{\\tau}=a) = 1 - u(x)$, we have:\n$$\n\\mathbb{E}_{x}[X_{\\tau}] = a(1 - u(x)) + b u(x)\n$$\nEquating the two expressions for $\\mathbb{E}_{x}[X_{\\tau}]$ gives a single algebraic equation for $u(x)$:\n$$\nx = a(1 - u(x)) + b u(x)\n$$\nThis is a linear equation in $u(x)$. Solving for $u(x)$:\n$$\nx = a - a u(x) + b u(x)\n$$\n$$\nx - a = u(x)(b - a)\n$$\n$$\nu(x) = \\frac{x-a}{b-a}\n$$\nThis alternative derivation shows that the linearity of $u(x)$ is a direct reflection of the martingale property of the driftless Brownian motion. The expected exit position is simply the starting position, and since this expectation is a linear interpolation between the boundary points $a$ and $b$, the weighting probability $u(x)$ must be a linear function of $x$. Had there been a drift term in the SDE, $X_t$ would not be a martingale, and the resulting exit probability function would not be linear.", "answer": "$$\n\\boxed{\\frac{x-a}{b-a}}\n$$", "id": "3051722"}, {"introduction": "Beyond hitting times, the generator framework is invaluable for computing expectations of functionals integrated over time, a common task in fields like quantitative finance and optimal control. This practice challenges you to find the expected total discounted cost for an Ornstein-Uhlenbeck process, a ubiquitous model for mean-reverting systems [@problem_id:3051716]. By setting up and solving the associated resolvent equation, $(\\alpha I - L)u=g$, we will see how to handle expectations over an infinite horizon and analyze the stability conditions required for the result to be finite.", "problem": "Consider the Ornstein–Uhlenbeck process governed by the Stochastic Differential Equation (SDE)\n$$dX_t=-\\kappa X_t\\,dt+\\sigma\\,dW_t,\\qquad X_0=x,$$\nwhere $\\kappa\\in\\mathbb{R}$ and $\\sigma0$ are constants, and $W_t$ is a standard one-dimensional Brownian motion (BM). Let $\\alpha0$ be a discount rate and define the running cost $g(x)=x^2$. Define\n$$u(x)=\\mathbb{E}_x\\left[\\int_0^\\infty \\exp(-\\alpha t)\\,X_t^2\\,dt\\right],$$\nwhere $\\mathbb{E}_x$ denotes expectation conditioned on $X_0=x$. Starting from the infinitesimal generator of the process and the resolvent characterization of discounted expectations, derive $u(x)$ by solving the resolvent equation\n$$(\\alpha I-L)u=g,$$\nsubject to the condition that $u$ has at most polynomial growth as $|x|\\to\\infty$. Then, verify your expression for $u(x)$ by directly computing $\\mathbb{E}_x[X_t^2]$ from the SDE solution and integrating in $t$. Finally, discuss the parameter regimes for $\\alpha$ and $\\kappa$ under which $u(x)$ is finite. Express your final answer for $u(x)$ as a closed-form analytic expression in terms of $\\alpha$, $\\kappa$, $\\sigma$, and $x$. No rounding is required, and no units are involved.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, complete, and directly addresses standard concepts within the theory of stochastic differential equations without any ambiguities or logical flaws.\n\nWe are asked to find the function $u(x)$ defined as\n$$u(x) = \\mathbb{E}_x\\left[\\int_0^\\infty \\exp(-\\alpha t)\\,X_t^2\\,dt\\right],$$\nfor the Ornstein–Uhlenbeck process $X_t$ given by the SDE\n$$dX_t = -\\kappa X_t\\,dt + \\sigma\\,dW_t, \\quad X_0=x.$$\nThe constants are $\\kappa \\in \\mathbb{R}$, $\\sigma  0$, and $\\alpha  0$. The running cost is $g(x) = x^2$. We will employ two methods to derive and verify $u(x)$.\n\n**Method 1: Solving the Resolvent Equation**\n\nA key result from the theory of Itô processes, often related to Dynkin's formula, states that the function $u(x)$ defined above is the solution to the resolvent equation $(\\alpha I - L)u = g$, where $L$ is the infinitesimal generator of the process $X_t$ and $I$ is the identity operator.\n\nFirst, we determine the infinitesimal generator $L$. For a general Itô process $dY_t = b(Y_t)dt + a(Y_t)dW_t$, the generator is given by $L = b(y)\\frac{d}{dy} + \\frac{1}{2}a(y)^2\\frac{d^2}{dy^2}$. For the Ornstein–Uhlenbeck process, the drift is $b(x) = -\\kappa x$ and the diffusion coefficient is $a(x) = \\sigma$. Thus, the generator is:\n$$L = -\\kappa x \\frac{d}{dx} + \\frac{\\sigma^2}{2} \\frac{d^2}{dx^2}.$$\n\nThe resolvent equation is $(\\alpha I - L)u(x) = g(x)$, which translates to:\n$$\\alpha u(x) - \\left(-\\kappa x \\frac{du}{dx} + \\frac{\\sigma^2}{2} \\frac{d^2u}{dx^2}\\right) = x^2.$$\nRearranging the terms, we obtain a second-order linear ordinary differential equation (ODE) for $u(x)$:\n$$\\frac{\\sigma^2}{2} u''(x) - \\kappa x u'(x) - \\alpha u(x) = -x^2.$$\n\nWe are given that $u(x)$ has at most polynomial growth. Since the forcing term is a polynomial of degree $2$, we seek a polynomial solution of the form $u(x) = Ax^2 + Bx + C$. The derivatives are $u'(x) = 2Ax + B$ and $u''(x) = 2A$. Substituting these into the ODE yields:\n$$\\frac{\\sigma^2}{2}(2A) - \\kappa x(2Ax + B) - \\alpha(Ax^2 + Bx + C) = -x^2.$$\nExpanding and collecting terms by powers of $x$:\n$$\\sigma^2 A - 2\\kappa Ax^2 - \\kappa Bx - \\alpha Ax^2 - \\alpha Bx - \\alpha C = -x^2.$$\n$$(-2\\kappa A - \\alpha A)x^2 + (-\\kappa B - \\alpha B)x + (\\sigma^2 A - \\alpha C) = -1 \\cdot x^2 + 0 \\cdot x + 0 \\cdot 1.$$\nFor this equality to hold for all $x \\in \\mathbb{R}$, we equate the coefficients of the powers of $x$:\n\\begin{itemize}\n    \\item Coefficient of $x^2$: $-A(2\\kappa + \\alpha) = -1 \\implies A = \\frac{1}{2\\kappa + \\alpha}$.\n    \\item Coefficient of $x$: $-B(\\kappa + \\alpha) = 0$. Since $\\alpha  0$, this implies $B=0$ unless $\\kappa = -\\alpha$. However, both the SDE dynamics and the cost function $g(x)=x^2$ are symmetric with respect to $x=0$, implying that $u(x)$ should be an even function of $x$. Thus, we must have $B=0$.\n    \\item Constant term: $\\sigma^2 A - \\alpha C = 0 \\implies C = \\frac{\\sigma^2 A}{\\alpha} = \\frac{\\sigma^2}{\\alpha(2\\kappa + \\alpha)}$.\n\\end{itemize}\nThis requires $2\\kappa + \\alpha \\neq 0$ and $\\alpha \\neq 0$. The latter is given.\nThe resulting solution is:\n$$u(x) = \\frac{1}{2\\kappa + \\alpha}x^2 + \\frac{\\sigma^2}{\\alpha(2\\kappa + \\alpha)}.$$\nThis solution has quadratic growth, which is a form of polynomial growth, and thus satisfies the given condition.\n\n**Method 2: Direct Computation and Verification**\n\nFirst, we solve the linear SDE for $X_t$. Using an integrating factor $e^{\\kappa t}$, we can write:\n$d(e^{\\kappa t} X_t) = \\kappa e^{\\kappa t} X_t dt + e^{\\kappa t} dX_t = \\kappa e^{\\kappa t} X_t dt + e^{\\kappa t}(-\\kappa X_t dt + \\sigma dW_t) = \\sigma e^{\\kappa t} dW_t$.\nIntegrating from $0$ to $t$:\n$$e^{\\kappa t} X_t - X_0 = \\int_0^t \\sigma e^{\\kappa s} dW_s.$$\nSolving for $X_t$ with $X_0 = x$:\n$$X_t = x e^{-\\kappa t} + \\sigma \\int_0^t e^{-\\kappa(t-s)} dW_s.$$\nNext, we compute the second moment $\\mathbb{E}_x[X_t^2]$.\n$$X_t^2 = \\left(x e^{-\\kappa t} + \\sigma \\int_0^t e^{-\\kappa(t-s)} dW_s\\right)^2 = x^2 e^{-2\\kappa t} + 2x\\sigma e^{-\\kappa t}\\int_0^t e^{-\\kappa(t-s)} dW_s + \\sigma^2 \\left(\\int_0^t e^{-\\kappa(t-s)} dW_s\\right)^2.$$\nTaking the expectation $\\mathbb{E}_x[\\cdot]$ and using the properties of Itô integrals (zero mean of the integral and Itô isometry):\n$\\mathbb{E}_x[X_t^2] = x^2 e^{-2\\kappa t} + 0 + \\sigma^2 \\mathbb{E}\\left[\\left(\\int_0^t e^{-\\kappa(t-s)} dW_s\\right)^2\\right] = x^2 e^{-2\\kappa t} + \\sigma^2 \\int_0^t \\left(e^{-\\kappa(t-s)}\\right)^2 ds$.\nThe integral term evaluates to (for $\\kappa \\neq 0$):\n$$\\int_0^t e^{-2\\kappa(t-s)} ds = e^{-2\\kappa t} \\int_0^t e^{2\\kappa s} ds = e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa}\\right]_0^t = e^{-2\\kappa t} \\left(\\frac{e^{2\\kappa t} - 1}{2\\kappa}\\right) = \\frac{1 - e^{-2\\kappa t}}{2\\kappa}.$$\nIf $\\kappa=0$, the integral is $\\int_0^t 1 \\, ds = t$, which matches the limit $\\lim_{\\kappa \\to 0} \\frac{1 - e^{-2\\kappa t}}{2\\kappa} = t$.\nSo, the second moment is:\n$$\\mathbb{E}_x[X_t^2] = x^2 e^{-2\\kappa t} + \\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa t}).$$\nNow we compute $u(x)$ by integrating this expectation against the discounted time measure. By Fubini's theorem (as the integrand is non-negative), we can swap expectation and integration:\n$$u(x) = \\int_0^\\infty e^{-\\alpha t} \\mathbb{E}_x[X_t^2] dt = \\int_0^\\infty e^{-\\alpha t} \\left(x^2 e^{-2\\kappa t} + \\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa t})\\right) dt.$$\nThis can be split into:\n$$u(x) = x^2 \\int_0^\\infty e^{-(\\alpha+2\\kappa) t} dt + \\frac{\\sigma^2}{2\\kappa} \\left(\\int_0^\\infty e^{-\\alpha t} dt - \\int_0^\\infty e^{-(\\alpha+2\\kappa) t} dt\\right).$$\nFor these integrals to converge, the exponents in the exponentials must be positive. Since $\\alpha  0$ is given, the first integral converges. For the second and third integrals to converge, we require $\\alpha+2\\kappa  0$. Under this condition:\n$$\\int_0^\\infty e^{-(\\alpha+2\\kappa) t} dt = \\frac{1}{\\alpha+2\\kappa} \\quad \\text{and} \\quad \\int_0^\\infty e^{-\\alpha t} dt = \\frac{1}{\\alpha}.$$\nSubstituting these values back:\n$$u(x) = x^2 \\frac{1}{\\alpha+2\\kappa} + \\frac{\\sigma^2}{2\\kappa}\\left(\\frac{1}{\\alpha} - \\frac{1}{\\alpha+2\\kappa}\\right).$$\nSimplifying the term in parentheses:\n$$\\frac{1}{\\alpha} - \\frac{1}{\\alpha+2\\kappa} = \\frac{\\alpha+2\\kappa - \\alpha}{\\alpha(\\alpha+2\\kappa)} = \\frac{2\\kappa}{\\alpha(\\alpha+2\\kappa)}.$$\nThis yields:\n$$u(x) = \\frac{x^2}{2\\kappa+\\alpha} + \\frac{\\sigma^2}{2\\kappa} \\frac{2\\kappa}{\\alpha(2\\kappa+\\alpha)} = \\frac{x^2}{2\\kappa+\\alpha} + \\frac{\\sigma^2}{\\alpha(2\\kappa+\\alpha)}.$$\nThis expression matches the one derived from the resolvent equation, thereby verifying the result. The case $\\kappa=0$ can be verified by taking the limit of the final expression as $\\kappa \\to 0$, which yields $u(x) = \\frac{x^2}{\\alpha} + \\frac{\\sigma^2}{\\alpha^2}$, or by direct integration using $\\mathbb{E}_x[X_t^2] = x^2 + \\sigma^2 t$.\n\n**Discussion of Parameter Regimes**\n\nThe function $u(x)$ represents the expected total discounted cost. For this value to be finite, the integrals involved in its direct calculation must converge.\nThe integral $\\int_0^\\infty e^{-\\alpha t}\\mathbb{E}_x[X_t^2]dt$ converges if and only if the term $\\mathbb{E}_x[X_t^2]$ does not grow faster than $e^{\\alpha t}$. The dominant term in $\\mathbb{E}_x[X_t^2]$ for large $t$ is determined by the sign of $\\kappa$.\nThe behavior of $\\mathbb{E}_x[X_t^2]$ is dominated by the term $e^{-2\\kappa t}$. The discount factor is $e^{-\\alpha t}$. To ensure convergence of the integral $\\int_0^\\infty e^{-\\alpha t} e^{-2\\kappa t} dt = \\int_0^\\infty e^{-(\\alpha+2\\kappa)t}dt$, the rate in the exponent must be positive.\nTherefore, the condition for $u(x)$ to be finite is $\\alpha+2\\kappa  0$.\nGiven that $\\alpha0$, this condition is always met for $\\kappa \\ge 0$. For $\\kappa  0$, the process is unstable, and the discounting must be strong enough to counteract the exponential growth of the variance of $X_t$. This means we require $\\alpha  -2\\kappa$.\nThis condition is also consistent with the resolvent equation approach. Since $u(x)$ is an expectation of a non-negative quantity, it must be that $u(x) \\ge 0$ for all $x$. From the derived expression, $u(x) = \\frac{1}{2\\kappa+\\alpha} x^2 + \\frac{\\sigma^2}{\\alpha(2\\kappa+\\alpha)}$, and knowing $\\alpha  0$ and $\\sigma^2  0$, positivity of $u(x)$ for all $x$ requires the denominator $2\\kappa+\\alpha$ to be positive.\n\nThe final expression for $u(x)$ under the condition $\\alpha + 2\\kappa  0$ is:\n$$u(x) = \\frac{1}{2\\kappa + \\alpha}x^2 + \\frac{\\sigma^2}{\\alpha(2\\kappa + \\alpha)} = \\frac{\\alpha x^2 + \\sigma^2}{\\alpha(2\\kappa + \\alpha)}.$$", "answer": "$$\\boxed{\\frac{x^2}{2\\kappa + \\alpha} + \\frac{\\sigma^2}{\\alpha(2\\kappa + \\alpha)}}$$", "id": "3051716"}]}