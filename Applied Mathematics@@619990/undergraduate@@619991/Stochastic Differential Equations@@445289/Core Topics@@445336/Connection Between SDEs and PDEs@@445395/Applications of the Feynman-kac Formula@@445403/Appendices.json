{"hands_on_practices": [{"introduction": "This first exercise serves as a fundamental check of the Feynman-Kac theorem in action. You will begin with the probabilistic representation of a solution, which involves an expectation over the path of an arithmetic Brownian motion. By explicitly calculating this expectation for an exponential payoff function, you will derive a closed-form analytical solution to the associated partial differential equation (PDE), and then confirm its validity by direct differentiation and substitution back into the PDE.", "problem": "Consider the Itô diffusion defined on a filtered probability space by the stochastic differential equation (SDE) $dX_{s}=\\mu\\,ds+\\sigma\\,dW_{s}$ for $s\\in[t,T]$, with initial condition $X_{t}=x$, where $\\mu\\in\\mathbb{R}$, $\\sigma0$, and $\\{W_{s}\\}_{s\\ge 0}$ is a standard Brownian motion. Let $\\lambda\\in\\mathbb{R}$ be a constant and let the terminal payoff be $\\phi(x)=\\exp(\\alpha x)$ for a fixed $\\alpha\\in\\mathbb{R}$. Define\n$$\nu(t,x)\\equiv \\exp\\!\\big(-\\lambda\\,(T-t)\\big)\\,\\mathbb{E}_{x}\\!\\left[\\phi\\!\\left(X_{T}\\right)\\right],\n$$\nwhere $\\mathbb{E}_{x}[\\cdot]$ denotes conditional expectation given $X_{t}=x$. \n\n1) Compute explicitly the function $u(t,x)$ in closed form as a function of $t$, $x$, and the parameters $\\mu$, $\\sigma$, $\\lambda$, and $\\alpha$.\n\n2) Using only fundamental properties of the SDE solution and standard differentiation, verify directly that the function $u(t,x)$ you obtained solves the linear parabolic partial differential equation (PDE)\n$$\nu_{t}(t,x)+\\mu\\,u_{x}(t,x)+\\frac{1}{2}\\sigma^{2}\\,u_{xx}(t,x)-\\lambda\\,u(t,x)=0,\\quad t\\in[0,T),\\ x\\in\\mathbb{R},\n$$\nwith terminal condition $u(T,x)=\\phi(x)=\\exp(\\alpha x)$.\n\nProvide your final answer as a single simplified analytic expression for $u(t,x)$. No numerical rounding is required.", "solution": "The problem statement is self-contained, scientifically grounded in the theory of stochastic calculus and its connection to partial differential equations via the Feynman-Kac formula, and is well-posed. All parameters and functions are clearly defined, and the objectives are unambiguous. Therefore, the problem is valid, and we proceed with the solution. The solution is presented in two parts as requested.\n\nFirst, we compute the explicit form of the function $u(t,x)$.\nThe stochastic differential equation (SDE) is given by $dX_{s}=\\mu\\,ds+\\sigma\\,dW_{s}$ for $s \\in [t,T]$, with the initial condition $X_{t}=x$. This is an arithmetic Brownian motion.\nTo find the value of the process at time $T$, we integrate the SDE from $t$ to $T$:\n$$\n\\int_{t}^{T} dX_{s} = \\int_{t}^{T} \\mu\\,ds + \\int_{t}^{T} \\sigma\\,dW_{s}\n$$\nThis gives:\n$$\nX_{T} - X_{t} = \\mu(T-t) + \\sigma(W_{T}-W_{t})\n$$\nSubstituting the initial condition $X_{t}=x$, we obtain the solution for $X_{T}$:\n$$\nX_{T} = x + \\mu(T-t) + \\sigma(W_{T}-W_{t})\n$$\nThe term $W_{T}-W_{t}$ represents the increment of a standard Brownian motion over the interval $[t,T]$. This increment is a normally distributed random variable with mean $0$ and variance $T-t$. We denote this as $W_{T}-W_{t} \\sim N(0, T-t)$.\nConsequently, $X_{T}$, being a linear transformation of a normal random variable, is also normally distributed. We can find its mean and variance as follows:\nThe mean of $X_{T}$ conditional on $X_t=x$ is:\n$$\n\\mathbb{E}_{x}[X_{T}] = \\mathbb{E}_{x}[x + \\mu(T-t) + \\sigma(W_{T}-W_{t})] = x + \\mu(T-t) + \\sigma\\mathbb{E}[W_{T}-W_{t}] = x + \\mu(T-t)\n$$\nThe variance of $X_{T}$ is:\n$$\n\\text{Var}(X_{T}) = \\text{Var}(x + \\mu(T-t) + \\sigma(W_{T}-W_{t})) = \\sigma^{2}\\text{Var}(W_{T}-W_{t}) = \\sigma^{2}(T-t)\n$$\nThus, the distribution of $X_{T}$ given $X_{t}=x$ is $X_{T} \\sim N(x+\\mu(T-t), \\sigma^{2}(T-t))$.\n\nNext, we compute the expectation required for $u(t,x)$. The terminal payoff is $\\phi(x)=\\exp(\\alpha x)$. We need to calculate $\\mathbb{E}_{x}[\\phi(X_{T})] = \\mathbb{E}_{x}[\\exp(\\alpha X_{T})]$.\nThis expectation is precisely the moment-generating function (MGF) of the normal random variable $X_{T}$, evaluated at $\\alpha$. The MGF of a normal random variable $Y \\sim N(m,v)$ is given by $M_{Y}(k) = \\mathbb{E}[\\exp(kY)] = \\exp(km + \\frac{1}{2}k^{2}v)$.\nFor our case, the random variable is $X_T$, its mean is $m = x + \\mu(T-t)$, its variance is $v = \\sigma^{2}(T-t)$, and we are evaluating at $k=\\alpha$.\nSubstituting these values into the MGF formula yields:\n$$\n\\mathbb{E}_{x}[\\exp(\\alpha X_{T})] = \\exp\\left(\\alpha(x + \\mu(T-t)) + \\frac{1}{2}\\alpha^{2}\\sigma^{2}(T-t)\\right)\n$$\nNow we can assemble the function $u(t,x)$ using its definition:\n$$\nu(t,x) = \\exp(-\\lambda(T-t))\\,\\mathbb{E}_{x}[\\phi(X_{T})] = \\exp(-\\lambda(T-t)) \\exp\\left(\\alpha x + \\alpha\\mu(T-t) + \\frac{1}{2}\\alpha^{2}\\sigma^{2}(T-t)\\right)\n$$\nCombining the exponents, we get the closed-form solution for $u(t,x)$:\n$$\nu(t,x) = \\exp\\left(\\alpha x - \\lambda(T-t) + \\mu\\alpha(T-t) + \\frac{1}{2}\\sigma^{2}\\alpha^{2}(T-t)\\right)\n$$\nFactoring out the $(T-t)$ term in the exponent provides the final simplified expression:\n$$\nu(t,x) = \\exp\\left(\\alpha x + (T-t)\\left(\\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\\right)\n$$\nThis completes the first part of the problem.\n\nSecond, we verify that this function $u(t,x)$ solves the given partial differential equation (PDE) and satisfies the terminal condition.\nThe PDE is $u_{t}(t,x)+\\mu\\,u_{x}(t,x)+\\frac{1}{2}\\sigma^{2}\\,u_{xx}(t,x)-\\lambda\\,u(t,x)=0$.\nOur function is $u(t,x) = \\exp\\left(\\alpha x + (T-t)K\\right)$, where we define the constant $K = \\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda$ for notational convenience.\n\nWe compute the necessary partial derivatives of $u(t,x)$:\nThe partial derivative with respect to $t$ is:\n$$\nu_{t}(t,x) = \\frac{\\partial}{\\partial t} \\exp\\left(\\alpha x + (T-t)K\\right) = \\exp\\left(\\alpha x + (T-t)K\\right) \\cdot (-K) = -K\\,u(t,x)\n$$\nThe first partial derivative with respect to $x$ is:\n$$\nu_{x}(t,x) = \\frac{\\partial}{\\partial x} \\exp\\left(\\alpha x + (T-t)K\\right) = \\exp\\left(\\alpha x + (T-t)K\\right) \\cdot \\alpha = \\alpha\\,u(t,x)\n$$\nThe second partial derivative with respect to $x$ is:\n$$\nu_{xx}(t,x) = \\frac{\\partial}{\\partial x}(\\alpha\\,u(t,x)) = \\alpha\\,u_{x}(t,x) = \\alpha(\\alpha\\,u(t,x)) = \\alpha^{2}\\,u(t,x)\n$$\nNow, substitute these derivatives into the left-hand side of the PDE:\n$$\nu_{t} + \\mu u_{x} + \\frac{1}{2}\\sigma^{2}u_{xx} - \\lambda u = (-K\\,u) + \\mu(\\alpha\\,u) + \\frac{1}{2}\\sigma^{2}(\\alpha^{2}\\,u) - \\lambda u\n$$\nFactoring out $u(t,x)$, which is strictly positive, we get:\n$$\nu(t,x) \\left(-K + \\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\n$$\nSubstitute back the expression for $K$:\n$$\nu(t,x) \\left(-\\left(\\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right) + \\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\n$$\n$$\n= u(t,x) \\left(-\\mu\\alpha - \\frac{1}{2}\\sigma^{2}\\alpha^{2} + \\lambda + \\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\n$$\nAll terms inside the parenthesis cancel out, leaving:\n$$\n= u(t,x) \\cdot 0 = 0\n$$\nThis confirms that our function $u(t,x)$ solves the PDE for all $(t,x) \\in [0,T) \\times \\mathbb{R}$.\n\nFinally, we check the terminal condition $u(T,x)=\\phi(x)=\\exp(\\alpha x)$.\nWe evaluate our solution at $t=T$:\n$$\nu(T,x) = \\exp\\left(\\alpha x + (T-T)\\left(\\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\\right) = \\exp\\left(\\alpha x + 0\\right) = \\exp(\\alpha x)\n$$\nThis matches the given terminal condition. The direct verification is complete and successful. The explicit function $u(t,x)$ is the final answer.", "answer": "$$\n\\boxed{\\exp\\left(\\alpha x + (T-t)\\left(\\mu\\alpha + \\frac{1}{2}\\sigma^{2}\\alpha^{2} - \\lambda\\right)\\right)}\n$$", "id": "3039047"}, {"introduction": "One of the most powerful uses of the Feynman-Kac formula is in designing and validating numerical methods. This practice bridges theory and computation by tasking you with creating a Monte Carlo simulation to estimate a PDE's solution. You will first derive the exact analytical solution for a test case, which will then serve as a \"ground truth\" to verify the accuracy of your numerical estimator, providing a tangible sense of how theoretical results guide practical implementations.", "problem": "Consider the backward partial differential equation for a function $u(t,x)$ associated with a one-dimensional diffusion and a constant killing rate, posed on the time interval $[t,T]$:\n$$\nu_t(t,x) + \\tfrac{1}{2}\\sigma^2 u_{xx}(t,x) - c\\,u(t,x) = 0,\\quad t \\in [0,T),\\ x \\in \\mathbb{R},\n$$\nwith the terminal condition\n$$\nu(T,x) = \\exp(\\beta x),\n$$\nwhere $\\sigma \\ge 0$ is a constant volatility, $c \\ge 0$ is a constant killing rate, and $\\beta \\in \\mathbb{R}$ is a constant parameter in the terminal function. Assume the diffusion $X_s$ satisfies the stochastic differential equation (SDE)\n$$\ndX_s = \\sigma\\, dW_s,\\quad s \\in [t,T], \\quad X_t = x,\n$$\nwhere $W_s$ is a standard Brownian motion. The classical Feynman-Kac formula states that the unique bounded solution to the above terminal value problem can be represented as an expectation with respect to the law of $X_s$, namely,\n$$\nu(t,x) = \\mathbb{E}\\!\\left[\\exp\\!\\left(-\\int_t^T c(X_s)\\, ds\\right) f(X_T)\\,\\middle|\\, X_t=x\\right],\n$$\nfor a suitable terminal function $f$ and killing rate $c(\\cdot)$. In this problem, take $c(X_s) \\equiv c$ (constant) and $f(y) = \\exp(\\beta y)$.\n\nYour tasks are:\n- Starting from the Feynman-Kac representation, the defining properties of standard Brownian motion, and the moment generating function of a Gaussian random variable, derive an explicit formula for $u(t,x)$ that depends only on $(\\sigma,\\beta,c,x,t,T)$.\n- Design a Monte Carlo estimator that simulates $X_T$ under the given SDE and estimates $u(t,x)$ via the expectation representation. Your estimator must use antithetic variates: simulate $N/2$ independent standard normal draws and use both $+Z$ and $-Z$ to reduce variance. Use a total of $N=300000$ samples (that is, $N/2=150000$ independent draws and their antithetic counterparts), and fix the pseudorandom generator seed to a constant value to ensure reproducibility.\n- Implement a program that, for each test case below, computes the explicit analytic solution and the Monte Carlo estimate, then reports whether the absolute error between the two is within a prescribed tolerance.\n\nUse the following test suite, each case described by $(\\sigma,\\beta,c,x,t,T,\\text{tol})$:\n- Case $1$: $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.7,\\,0.3,\\,0.1,\\,0.2,\\,0.0,\\,0.8,\\,5\\times 10^{-3})$.\n- Case $2$: $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.6,\\,0.5,\\,0.0,\\,-0.1,\\,0.0,\\,1.0,\\,5\\times 10^{-3})$.\n- Case $3$: $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (1.5,\\,-0.4,\\,0.3,\\,1.1,\\,0.5,\\,0.5,\\,5\\times 10^{-3})$ (note that $t=T$ here).\n- Case $4$: $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.0,\\,0.9,\\,0.2,\\,0.4,\\,0.1,\\,0.6,\\,5\\times 10^{-3})$ (note that $\\sigma=0$ here).\n- Case $5$: $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (1.3,\\,0.8,\\,0.4,\\,-0.5,\\,0.0,\\,0.3,\\,5\\times 10^{-3})$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the absolute error for the corresponding test case is less than or equal to the specified tolerance (for example, $[{\\tt True},{\\tt False},{\\tt True},{\\tt True},{\\tt True}]$). No other text should be printed.\n\nNo physical units are involved in this problem. All angles, if any appear, must be in radians, but none are required here.", "solution": "The problem is assessed to be valid. It is scientifically grounded, well-posed, objective, and self-contained. It presents a standard application of the Feynman-Kac formula to a linear parabolic partial differential equation (PDE) with a constant-coefficient diffusion process. All parameters and conditions are clearly defined.\n\nThe validation procedure is as follows:\n\n### Step 1: Extract Givens\n- **PDE**: $u_t(t,x) + \\tfrac{1}{2}\\sigma^2 u_{xx}(t,x) - c\\,u(t,x) = 0$, for $t \\in [0,T),\\ x \\in \\mathbb{R}$.\n- **Terminal Condition**: $u(T,x) = \\exp(\\beta x)$.\n- **Parameters**: $\\sigma \\ge 0$ (volatility), $c \\ge 0$ (killing rate), $\\beta \\in \\mathbb{R}$.\n- **SDE**: $dX_s = \\sigma\\, dW_s$, for $s \\in [t,T]$, with initial condition $X_t = x$. $W_s$ is a standard Brownian motion.\n- **Feynman-Kac Representation**: $u(t,x) = \\mathbb{E}\\!\\left[\\exp\\!\\left(-\\int_t^T c(X_s)\\, ds\\right) f(X_T)\\,\\middle|\\, X_t=x\\right]$ with $c(X_s) = c$ and $f(y) = \\exp(\\beta y)$.\n- **Monte Carlo Specification**: Use antithetic variates, total samples $N=300000$, and a fixed seed.\n- **Test Cases**:\n    1. $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.7,\\,0.3,\\,0.1,\\,0.2,\\,0.0,\\,0.8,\\,5\\times 10^{-3})$.\n    2. $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.6,\\,0.5,\\,0.0,\\,-0.1,\\,0.0,\\,1.0,\\,5\\times 10^{-3})$.\n    3. $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (1.5,\\,-0.4,\\,0.3,\\,1.1,\\,0.5,\\,0.5,\\,5\\times 10^{-3})$.\n    4. $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (0.0,\\,0.9,\\,0.2,\\,0.4,\\,0.1,\\,0.6,\\,5\\times 10^{-3})$.\n    5. $(\\sigma,\\beta,c,x,t,T,\\text{tol}) = (1.3,\\,0.8,\\,0.4,\\,-0.5,\\,0.0,\\,0.3,\\,5\\times 10^{-3})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-posed and scientifically sound, based on the established connection between stochastic differential equations and partial differential equations embodied by the Feynman-Kac theorem. The provided parameters and conditions are complete and consistent. The special cases given ($\\sigma=0$ and $t=T$) are valid and well-defined limits of the general problem, not sources of contradiction. All terms are mathematically and scientifically formalizable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Analytic Solution Derivation\nThe solution $u(t,x)$ is given by the Feynman-Kac formula. We substitute the specific forms for the killing rate $c(X_s)=c$ and the terminal function $f(y) = \\exp(\\beta y)$:\n$$\nu(t,x) = \\mathbb{E}\\!\\left[\\exp\\!\\left(-\\int_t^T c\\, ds\\right) \\exp(\\beta X_T)\\,\\middle|\\, X_t=x\\right].\n$$\nSince $c$ is a constant, the integral in the exponent is deterministic:\n$$\n\\int_t^T c\\, ds = c(T-t).\n$$\nThis deterministic term can be factored out of the expectation:\n$$\nu(t,x) = \\exp(-c(T-t)) \\, \\mathbb{E}\\!\\left[\\exp(\\beta X_T)\\,\\middle|\\, X_t=x\\right].\n$$\nThe remaining task is to evaluate the expectation term, which is the moment generating function of the random variable $X_T$ (conditional on $X_t=x$) evaluated at $\\beta$. The stochastic process is defined by the SDE $dX_s = \\sigma\\, dW_s$ with the initial condition $X_t = x$. Integrating this SDE from $t$ to $T$ yields:\n$$\nX_T - X_t = \\int_t^T \\sigma\\, dW_s = \\sigma (W_T - W_t).\n$$\n$$\nX_T = x + \\sigma (W_T - W_t).\n$$\nThe increment of a standard Brownian motion $W_T - W_t$ is a normally distributed random variable with mean $0$ and variance $T-t$. Thus, $W_T - W_t \\sim \\mathcal{N}(0, T-t)$.\nConsequently, the conditional distribution of $X_T$ given $X_t=x$ is also normal:\n$$\nX_T \\,\\middle|\\, X_t=x \\sim \\mathcal{N}\\big(x, \\sigma^2(T-t)\\big).\n$$\nThe moment generating function of a generic normal random variable $Y \\sim \\mathcal{N}(\\mu, \\nu^2)$ is given by $M_Y(k) = \\mathbb{E}[\\exp(kY)] = \\exp(\\mu k + \\tfrac{1}{2}\\nu^2 k^2)$.\nApplying this to $X_T$ with parameter $\\beta$, mean $\\mu=x$, and variance $\\nu^2=\\sigma^2(T-t)$, we get:\n$$\n\\mathbb{E}\\!\\left[\\exp(\\beta X_T)\\,\\middle|\\, X_t=x\\right] = \\exp\\left(\\beta x + \\tfrac{1}{2}\\sigma^2(T-t)\\beta^2\\right).\n$$\nSubstituting this back into the expression for $u(t,x)$:\n$$\nu(t,x) = \\exp(-c(T-t)) \\exp\\left(\\beta x + \\tfrac{1}{2}\\sigma^2(T-t)\\beta^2\\right).\n$$\nCombining the exponents, we arrive at the explicit analytic solution:\n$$\nu(t,x) = \\exp\\left(\\beta x + \\left(\\tfrac{1}{2}\\sigma^2\\beta^2 - c\\right)(T-t)\\right).\n$$\n\n### Monte Carlo Estimator Design\nThe Monte Carlo method estimates the expectation $u(t,x)$ by averaging sample paths. The value of $X_T$ for a single path can be simulated using the integrated SDE:\n$$\nX_T = x + \\sigma\\sqrt{T-t}\\,Z,\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable. The quantity to estimate is:\n$$\nu(t,x) = \\mathbb{E}\\!\\left[ \\exp(-c(T-t)) \\exp(\\beta(x + \\sigma\\sqrt{T-t}\\,Z)) \\right].\n$$\nThe problem requires using antithetic variates for variance reduction. This technique leverages the symmetry of the standard normal distribution. For each random draw $Z_i$, we also use its antithetic counterpart $-Z_i$. The estimator is constructed from pairs of simulated values. Let $g(Z) = \\exp(-c(T-t)) \\exp(\\beta X_T)$ be the function whose expectation we seek. The antithetic estimator uses pairs of evaluations, averaging $g(Z_i)$ and $g(-Z_i)$:\n$$\n\\hat{u}_i = \\frac{g(Z_i) + g(-Z_i)}{2} = \\frac{1}{2} \\left[ e^{-c(T-t)}e^{\\beta(x + \\sigma\\sqrt{T-t}Z_i)} + e^{-c(T-t)}e^{\\beta(x - \\sigma\\sqrt{T-t}Z_i)} \\right].\n$$\nFactoring out the common terms:\n$$\n\\hat{u}_i = e^{-c(T-t)} e^{\\beta x} \\left[ \\frac{e^{\\beta\\sigma\\sqrt{T-t}Z_i} + e^{-\\beta\\sigma\\sqrt{T-t}Z_i}}{2} \\right].\n$$\nUsing the identity $\\cosh(y) = \\frac{e^y+e^{-y}}{2}$, this simplifies to:\n$$\n\\hat{u}_i = e^{-c(T-t) + \\beta x} \\cosh\\left(\\beta\\sigma\\sqrt{T-t}Z_i\\right).\n$$\nThe final Monte Carlo estimate, $\\hat{u}_N(t,x)$, is the average of $N/2 = 150000$ such paired estimates:\n$$\n\\hat{u}_N(t,x) = \\frac{1}{N/2} \\sum_{i=1}^{N/2} \\hat{u}_i = e^{-c(T-t) + \\beta x} \\frac{1}{N/2} \\sum_{i=1}^{N/2} \\cosh\\left(\\beta\\sigma\\sqrt{T-t}Z_i\\right).\n$$\nFor the special cases where $\\sigma=0$ or $t=T$, the stochastic component vanishes. The process becomes deterministic ($X_s = x$ for all $s \\in [t,T]$) and the Monte Carlo estimator should yield the exact analytic solution without any simulation, thus having zero error. For $\\sigma=0$, $u(t,x) = \\exp(\\beta x - c(T-t))$. For $t=T$, $u(T,x) = \\exp(\\beta x)$. The implementation will handle these cases directly. A fixed seed ensures the sequence of pseudorandom numbers $Z_i$ is identical for each run, making the simulation reproducible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytical and Monte Carlo solutions for a PDE, and compares them.\n    \"\"\"\n\n    def analytic_solution(sigma, beta, c, x, t, T):\n        \"\"\"\n        Calculates the exact analytic solution to the PDE.\n\n        The formula is u(t,x) = exp(beta*x + (0.5*sigma^2*beta^2 - c)*(T-t)).\n        \"\"\"\n        if t  T:\n            # Physically not meaningful for a backward equation, but handled for robustness.\n            return np.nan\n        \n        exponent = beta * x + (0.5 * sigma**2 * beta**2 - c) * (T - t)\n        return np.exp(exponent)\n\n    def monte_carlo_estimate(sigma, beta, c, x, t, T, N, seed):\n        \"\"\"\n        Estimates the solution using a Monte Carlo simulation with antithetic variates.\n        \"\"\"\n        # For the deterministic cases, the MC estimate is exact and equals the analytic solution.\n        # This avoids unnecessary computation and potential floating-point inaccuracies.\n        if t == T:\n            # Terminal condition: u(T,x) = exp(beta*x)\n            return np.exp(beta * x)\n\n        if sigma == 0:\n            # No diffusion: X_s = x. The PDE is u_t - c*u = 0, with solution u(t,x) = exp(beta*x - c*(T-t)).\n            return np.exp(beta * x - c * (T - t))\n\n        # Use the modern recommended way to handle random number generation\n        rng = np.random.default_rng(seed)\n        \n        # We generate N/2 random numbers for N total antithetic samples.\n        num_draws = N // 2\n        dt = T - t\n\n        # Generate N/2 i.i.d. standard normal random variates\n        Z = rng.standard_normal(size=num_draws)\n\n        # Antithetic variate estimator using the cosh identity for efficiency and clarity.\n        # E[f(Z)] is estimated by avg( (f(Z_i) + f(-Z_i))/2 )\n        # Here, f(Z) corresponds to the full expression inside the expectation.\n        # After simplification, the pair average becomes:\n        # exp(-c*dt + beta*x) * cosh(beta*sigma*sqrt(dt)*Z)\n        \n        k = beta * sigma * np.sqrt(dt)\n        \n        # np.mean computes the average of the N/2 paired estimates\n        mean_cosh_term = np.mean(np.cosh(k * Z))\n        \n        # The final estimate combines the deterministic part with the averaged stochastic part\n        mc_val = np.exp(-c * dt + beta * x) * mean_cosh_term\n        \n        return mc_val\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        # (sigma, beta, c, x, t, T, tol)\n        (0.7, 0.3, 0.1, 0.2, 0.0, 0.8, 5e-3),\n        (0.6, 0.5, 0.0, -0.1, 0.0, 1.0, 5e-3),\n        (1.5, -0.4, 0.3, 1.1, 0.5, 0.5, 5e-3),  # t=T case\n        (0.0, 0.9, 0.2, 0.4, 0.1, 0.6, 5e-3),  # sigma=0 case\n        (1.3, 0.8, 0.4, -0.5, 0.0, 0.3, 5e-3)\n    ]\n    \n    # Monte Carlo parameters\n    N = 300000\n    seed = 123  # A fixed seed for reproducibility as required\n\n    results = []\n    for case in test_cases:\n        sigma, beta, c, x, t, T, tol = case\n        \n        analytic_val = analytic_solution(sigma, beta, c, x, t, T)\n        mc_val = monte_carlo_estimate(sigma, beta, c, x, t, T, N, seed)\n        \n        absolute_error = np.abs(analytic_val - mc_val)\n        \n        results.append(absolute_error = tol)\n\n    # Format the final output exactly as specified.\n    # str() on a Python boolean gives \"True\" or \"False\", which is the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3039066"}, {"introduction": "After mastering the standard linear case, it is crucial to understand the boundaries of the theory. This conceptual problem explores what happens when the PDE becomes semilinear, meaning the potential term depends on the unknown solution $u(x,t)$ itself—a common feature in advanced models. This exercise will clarify why the standard Feynman-Kac representation breaks down and introduce the more advanced framework of Backward Stochastic Differential Equations (BSDEs) required to handle such nonlinearities.", "problem": "Consider an Itô diffusion $\\{X_s\\}_{s \\in [t,T]}$ in $\\mathbb{R}^d$ defined on a filtered probability space with a $d$-dimensional standard Brownian motion $\\{W_s\\}_{s \\in [t,T]}$, where $X_t = x$ and\n$$\n\\mathrm{d}X_s = \\mu(X_s,s)\\,\\mathrm{d}s + \\sigma(X_s,s)\\,\\mathrm{d}W_s,\n$$\nwith $\\mu:\\mathbb{R}^d\\times[0,T]\\to\\mathbb{R}^d$ and $\\sigma:\\mathbb{R}^d\\times[0,T]\\to\\mathbb{R}^{d\\times d}$ sufficiently regular and of at most linear growth. Let $\\mathcal{L}$ denote the infinitesimal generator\n$$\n\\mathcal{L}\\varphi(x,s) = \\sum_{i=1}^d \\mu_i(x,s)\\,\\partial_{x_i}\\varphi(x,s) + \\tfrac{1}{2}\\sum_{i,j=1}^d \\left(\\sigma\\sigma^\\top\\right)_{ij}(x,s)\\,\\partial_{x_ix_j}^2\\varphi(x,s).\n$$\nFor the terminal condition $u(T,x)=g(x)$ with a bounded measurable payoff $g:\\mathbb{R}^d\\to\\mathbb{R}$, consider the backward parabolic partial differential equation (PDE)\n$$\n\\partial_t u(x,t) + \\mathcal{L}u(x,t) - V\\!\\left(x,t,u(x,t)\\right)\\,u(x,t) = 0,\\quad (x,t)\\in \\mathbb{R}^d\\times[0,T),\n$$\nwhere the potential $V:\\mathbb{R}^d\\times[0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ is continuous and may depend on the solution $u$ itself. In computational economics and finance, the classical Feynman–Kac representation connects linear versions of such PDEs to conditional expectations suitable for Monte Carlo methods.\n\nWhich option best explains why the standard Feynman–Kac interpretation (a single conditional expectation under a fixed probability measure of a known path functional) breaks down when $V$ depends on $u$, and what conceptual replacement is appropriate?\n\nA. The factor $V\\!\\left(X_s,s,u(X_s,s)\\right)$ makes the stochastic discount factor depend on the unknown solution along the path, so $u(x,t)$ can no longer be written as a single expectation of a known functional under any fixed measure; one needs a nonlinear representation such as a Backward Stochastic Differential Equation (BSDE) or a fixed-point/branching construction.\n\nB. The dependence of $V$ on $u$ can be absorbed entirely by a change of measure via Girsanov’s theorem, restoring the same linear Feynman–Kac formula under an equivalent martingale measure without any fixed-point iteration.\n\nC. The PDE becomes ill-posed because Brownian motion ceases to be a martingale under any equivalent measure when $V$ depends on $u$, so no stochastic representation exists.\n\nD. Replacing the Brownian driver by a compensated Poisson process removes the nonlinearity in $V$ and reinstates the standard Feynman–Kac expectation.\n\nE. The standard Feynman–Kac formula still holds verbatim by inserting $V\\!\\left(X_s,s,u(X_s,s)\\right)$ inside the exponential discount, and simple Monte Carlo can compute $u$ directly without any iterative scheme because the $u$-dependence cancels out.", "solution": "The problem statement describes a semilinear backward parabolic partial differential equation (PDE) and asks for the correct interpretation of its solution when the standard Feynman-Kac formula fails due to state-dependent potential.\n\n**Step 1: Extract Givens**\n- An Itô diffusion $\\{X_s\\}_{s \\in [t,T]}$ in $\\mathbb{R}^d$: $\\mathrm{d}X_s = \\mu(X_s,s)\\,\\mathrm{d}s + \\sigma(X_s,s)\\,\\mathrm{d}W_s$, with $X_t = x$.\n- The infinitesimal generator is $\\mathcal{L}\\varphi(x,s) = \\sum_{i=1}^d \\mu_i(x,s)\\,\\partial_{x_i}\\varphi(x,s) + \\tfrac{1}{2}\\sum_{i,j=1}^d \\left(\\sigma\\sigma^\\top\\right)_{ij}(x,s)\\,\\partial_{x_ix_j}^2\\varphi(x,s)$.\n- A semilinear backward parabolic PDE: $\\partial_t u(x,t) + \\mathcal{L}u(x,t) - V\\!\\left(x,t,u(x,t)\\right)\\,u(x,t) = 0$.\n- The domain is $(x,t)\\in \\mathbb{R}^d\\times[0,T)$.\n- The terminal condition is $u(T,x)=g(x)$.\n- The potential $V:\\mathbb{R}^d\\times[0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ is a continuous function that depends on the solution $u$ itself.\n- The question is why the standard Feynman-Kac formula breaks down and what is the conceptual replacement.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound and well-posed. It presents a standard setup in the theory of stochastic analysis and its application to finance and economics. The equation is a canonical example of a semilinear PDE for which a probabilistic representation is sought. The assumptions on the coefficients ($\\mu, \\sigma$ having at most linear growth, $V$ continuous, $g$ bounded measurable) are standard to ensure the well-posedness of the underlying stochastic processes and the PDE, at least for the purpose of this conceptual question. The problem is objective, precisely stated, and does not contain internal contradictions or factual errors. It addresses a fundamental topic in its field. Therefore, the problem is valid.\n\n**Step 3: Derivation and Option Analysis**\n\nFirst, let us recall the standard Feynman-Kac formula. This formula provides a probabilistic solution to a *linear* backward parabolic PDE. If the potential $V$ were only a function of state and time, $V(x,t)$, the PDE would be:\n$$\n\\partial_t u(x,t) + \\mathcal{L}u(x,t) - V(x,t)u(x,t) = 0, \\quad u(T,x) = g(x).\n$$\nThe solution to this linear PDE is given by the conditional expectation:\n$$\nu(x,t) = \\mathbb{E}\\left[ g(X_T) \\exp\\left(-\\int_t^T V(X_s, s) \\, \\mathrm{d}s\\right) \\bigg| X_t = x \\right].\n$$\nIn this formula, the quantity inside the expectation is a functional of the path of the process $\\{X_s\\}_{s \\in [t,T]}$. Critically, this functional is *known* once a path is realized. The exponential term acts as a stochastic discount factor. This representation allows for direct estimation of $u(x,t)$ via Monte Carlo methods by simulating paths of $X_s$, computing the functional for each path, and averaging the results.\n\nNow, consider the problem's PDE, where the potential $V$ depends on the solution $u$ itself: $V(x,t,u(x,t))$.\n$$\n\\partial_t u(x,t) + \\mathcal{L}u(x,t) - V(x,t,u(x,t))u(x,t) = 0.\n$$\nIf one were to naively generalize the Feynman-Kac formula, one would arrive at the following expression:\n$$\nu(x,t) = \\mathbb{E}\\left[ g(X_T) \\exp\\left(-\\int_t^T V(X_s, s, u(X_s,s)) \\, \\mathrm{d}s\\right) \\bigg| X_t = x \\right].\n$$\nThis is not an explicit solution for $u(x,t)$. The unknown function $u$ appears on both the left-hand side and the right-hand side. Specifically, to compute the integral in the exponential on the right-hand side, one must know the values of the solution $u(X_s, s)$ along the entire future path of the process $X_s$ from time $t$ to $T$. This turns the expression into a recursive, implicit, fixed-point equation for the function $u$. It is no longer a representation of the solution as an expectation of a *known* path functional. Consequently, it cannot be solved by a simple, non-iterative Monte Carlo simulation.\n\nThe modern mathematical framework for representing solutions to such semilinear PDEs is the theory of Backward Stochastic Differential Equations (BSDEs). The generalized Feynman-Kac formula states that the solution to the semilinear PDE is given by the first component of the solution pair $(Y_s, Z_s)$ to a BSDE. For our specific PDE, the corresponding BSDE is:\n$$\n-\\mathrm{d}Y_s = -V(X_s, s, Y_s)Y_s \\, \\mathrm{d}s + Z_s \\, \\mathrm{d}W_s, \\quad s \\in [t, T]\n$$\nwith terminal condition $Y_T = g(X_T)$. The solution to the PDE is then given by $u(x,t) = Y_t$, where the BSDE is solved on the path of $X_s$ starting from $X_t=x$. This BSDE itself is a complex stochastic fixed-point problem, often requiring advanced numerical methods to solve. Other related concepts include branching diffusions, where the term $V(x,t,u)u$ is interpreted as a rate of particle creation/annihilation.\n\nWe now analyze the given options based on this understanding.\n\n**A. The factor $V\\!\\left(X_s,s,u(X_s,s)\\right)$ makes the stochastic discount factor depend on the unknown solution along the path, so $u(x,t)$ can no longer be written as a single expectation of a known functional under any fixed measure; one needs a nonlinear representation such as a Backward Stochastic Differential Equation (BSDE) or a fixed-point/branching construction.**\nThis statement is perfectly accurate. It correctly identifies that the dependence of the potential $V$ on the solution $u$ makes the integrand (and thus the \"discount factor\") dependent on the unknown solution itself. This breaks the standard interpretation. It then correctly identifies the proper conceptual replacements: BSDEs, which are the canonical tool, or the related ideas of fixed-point problems and branching processes.\n**Verdict: Correct.**\n\n**B. The dependence of $V$ on $u$ can be absorbed entirely by a change of measure via Girsanov’s theorem, restoring the same linear Feynman–Kac formula under an equivalent martingale measure without any fixed-point iteration.**\nThis statement is incorrect. Girsanov's theorem is used to change the drift of a stochastic process, which corresponds to changing the first-order derivative terms (the advection part) in the infinitesimal generator $\\mathcal{L}$. The term $-V(x,t,u)u$ is a zero-order term (a potential or reaction term). A change of measure via Girsanov's theorem cannot eliminate such a term. The nonlinearity persists.\n**Verdict: Incorrect.**\n\n**C. The PDE becomes ill-posed because Brownian motion ceases to be a martingale under any equivalent measure when $V$ depends on $u$, so no stochastic representation exists.**\nThis statement is incorrect on multiple grounds. First, under standard assumptions on $V$ (e.g., Lipschitz continuity in $u$), the semilinear PDE is well-posed. Second, the statement that \"no stochastic representation exists\" is false. As explained above, the BSDE provides exactly such a representation. The martingale property of Brownian motion is a tool, not a condition for the existence of a solution.\n**Verdict: Incorrect.**\n\n**D. Replacing the Brownian driver by a compensated Poisson process removes the nonlinearity in $V$ and reinstates the standard Feynman–Kac expectation.**\nThis statement is illogical. The source of the nonlinearity is the functional form of $V$ depending on $u$. Changing the stochastic driver from a continuous process (Brownian motion) to a jump process (Poisson process) would change the type of differential operator from a second-order elliptic operator to an integro-differential operator. It would not, in any way, alter or remove the nonlinearity present in the zero-order term $V(x,t,u)u$.\n**Verdict: Incorrect.**\n\n**E. The standard Feynman–Kac formula still holds verbatim by inserting $V\\!\\left(X_s,s,u(X_s,s)\\right)$ inside the exponential discount, and simple Monte Carlo can compute $u$ directly without any iterative scheme because the $u$-dependence cancels out.**\nThis statement is fundamentally wrong. While the first part correctly describes the structure of the resulting implicit equation, the assertion that the dependence on $u$ \"cancels out\" is absurd. This dependence is precisely the source of the mathematical difficulty, turning a simple expectation into a complex fixed-point problem that requires iterative or more sophisticated numerical schemes, not \"simple Monte Carlo\".\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2440797"}]}