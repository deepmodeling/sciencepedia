{"hands_on_practices": [{"introduction": "This first exercise forms the theoretical bedrock of our study of the Kalman-Bucy filter. We will step back from pre-packaged formulas and derive the famous continuous-time Riccati differential equation from first principles for a simple scalar system. This fundamental practice of deriving the equation for the error variance, optimizing the Kalman gain, and solving the resulting ODE provides invaluable insight into the filter's structure and performance over time.", "problem": "Consider the scalar, linear, time-invariant continuous-time model driven by independent standard Brownian motions,\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t,\\qquad\n\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t,\n$$\nwhere $a\\in\\mathbb{R}$, $c\\in\\mathbb{R}$, $q\\ge 0$, $r>0$ are given constants, and $W_t$ and $V_t$ are independent standard Wiener processes. Let $\\hat{x}_t$ denote the estimate produced by the Kalman-Bucy filter (KBF), and define the estimation error $e_t = x_t - \\hat{x}_t$ with error variance $P_t = \\mathbb{E}[e_t^2]$. Assume an initial variance $P_0 \\ge 0$.\n\nStarting from the stochastic differential equation (SDE) model and the definition of $e_t$, derive from first principles (without quoting pre-packaged filter formulas) the ordinary differential equation (ODE) governing $P_t$ for the Kalman-Bucy filter. Then, solve this scalar ODE explicitly for $P_t$ with initial condition $P_0$. Next, determine the precise conditions on $a$, $c$, $q$, $r$, and $P_0$ under which $P_t$ converges to a finite steady-state value $P_{\\infty} = \\lim_{t\\to\\infty} P_t$, and give a closed-form expression for $P_{\\infty}$.\n\nFinally, for the numerical values $a=0.4$, $c=1.5$, $q=0.2$, $r=0.5$, and $P_0=1.0$, evaluate the steady-state value $P_{\\infty}$ numerically. Round your answer to four significant figures. No units are required in the final answer.", "solution": "The problem asks for the derivation of the variance dynamics for a scalar Kalman-Bucy filter, the solution to the resulting differential equation, the conditions for steady-state convergence, and a numerical evaluation of the steady-state variance.\n\nThe process begins with the given linear time-invariant system:\nState equation:\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t\n$$\nMeasurement equation:\n$$\n\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t\n$$\nwhere $a, c, q, r$ are constants with $q \\ge 0$ and $r>0$, and $W_t, V_t$ are independent standard Wiener processes.\n\nThe Kalman-Bucy filter provides the optimal estimate $\\hat{x}_t$ of the state $x_t$. The filter for this system has the form:\n$$\n\\mathrm{d}\\hat{x}_t = a\\,\\hat{x}_t\\,\\mathrm{d}t + K_t(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t)\n$$\nwhere $K_t$ is the Kalman gain, which we will determine. The term $\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t$ is the innovations process.\n\n**1. Derivation of the ODE for the Error Variance $P_t$**\n\nThe estimation error is defined as $e_t = x_t - \\hat{x}_t$. We derive its SDE by taking the differential: $\\mathrm{d}e_t = \\mathrm{d}x_t - \\mathrm{d}\\hat{x}_t$.\nSubstituting the expressions for $\\mathrm{d}x_t$ and $\\mathrm{d}\\hat{x}_t$:\n$$\n\\mathrm{d}e_t = (a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t) - \\left( a\\,\\hat{x}_t\\,\\mathrm{d}t + K_t(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t) \\right)\n$$\nNow, substitute $\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t$ into the equation:\n$$\n\\mathrm{d}e_t = a\\,(x_t - \\hat{x}_t)\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t(c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t - c\\,\\hat{x}_t\\,\\mathrm{d}t)\n$$\nGrouping terms by $e_t = x_t - \\hat{x}_t$:\n$$\n\\mathrm{d}e_t = a\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t(c\\,(x_t - \\hat{x}_t)\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t)\n$$\n$$\n\\mathrm{d}e_t = (a - K_t c)\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t\n$$\nThe error variance is $P_t = \\mathbb{E}[e_t^2]$. To find its dynamic equation, we apply Itô's lemma to the function $f(e_t) = e_t^2$.\n$$\n\\mathrm{d}(e_t^2) = 2\\,e_t\\,\\mathrm{d}e_t + (\\mathrm{d}e_t)^2\n$$\nThe quadratic variation term $(\\mathrm{d}e_t)^2$ is calculated from the stochastic part of $\\mathrm{d}e_t$:\n$$\n(\\mathrm{d}e_t)^2 = (\\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t)^2 = q(\\mathrm{d}W_t)^2 + (K_t\\sqrt{r})^2(\\mathrm{d}V_t)^2 - 2\\sqrt{q}K_t\\sqrt{r}\\,\\mathrm{d}W_t\\mathrm{d}V_t\n$$\nSince $W_t$ and $V_t$ are independent, we have $(\\mathrm{d}W_t)^2 = \\mathrm{d}t$, $(\\mathrm{d}V_t)^2 = \\mathrm{d}t$, and $\\mathrm{d}W_t\\mathrm{d}V_t = 0$.\n$$\n(\\mathrm{d}e_t)^2 = (q + K_t^2 r)\\,\\mathrm{d}t\n$$\nSubstituting $\\mathrm{d}e_t$ and $(\\mathrm{d}e_t)^2$ into the expression for $\\mathrm{d}(e_t^2)$:\n$$\n\\mathrm{d}(e_t^2) = 2\\,e_t \\left( (a - K_t c)\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t \\right) + (q + K_t^2 r)\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(e_t^2) = 2(a - K_t c)\\,e_t^2\\,\\mathrm{d}t + (q + K_t^2 r)\\,\\mathrm{d}t + 2\\sqrt{q}\\,e_t\\,\\mathrm{d}W_t - 2K_t\\sqrt{r}\\,e_t\\,\\mathrm{d}V_t\n$$\nTo get the ODE for $P_t = \\mathbb{E}[e_t^2]$, we take the expectation of both sides. The expectations of the stochastic integral terms (martingales) are zero.\n$$\n\\mathrm{d}P_t = \\mathrm{d}\\mathbb{E}[e_t^2] = \\mathbb{E}[\\mathrm{d}(e_t^2)] = \\mathbb{E}[2(a - K_t c)\\,e_t^2\\,\\mathrm{d}t + (q + K_t^2 r)\\,\\mathrm{d}t]\n$$\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2(a - K_t c)\\,\\mathbb{E}[e_t^2] + q + K_t^2 r = 2(a - K_t c)\\,P_t + q + K_t^2 r\n$$\nThe Kalman-Bucy filter gain $K_t$ is chosen to minimize the error variance $P_t$. This is achieved by minimizing the rate of change $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t}$ at each instant. We treat the right-hand side as a function of $K_t$ and find the minimum by setting its derivative to zero:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}K_t} \\left( 2(a - K_t c)\\,P_t + q + K_t^2 r \\right) = -2c\\,P_t + 2r\\,K_t = 0\n$$\nThis gives the optimal Kalman gain:\n$$\nK_t = \\frac{c\\,P_t}{r}\n$$\nSubstituting this optimal gain back into the ODE for $P_t$:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2\\left(a - \\frac{c\\,P_t}{r} c\\right)P_t + q + \\left(\\frac{c\\,P_t}{r}\\right)^2 r\n$$\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2a\\,P_t - \\frac{2c^2}{r}P_t^2 + q + \\frac{c^2 P_t^2}{r}\n$$\nThis simplifies to the continuous-time scalar Riccati differential equation:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = -\\frac{c^2}{r}P_t^2 + 2a\\,P_t + q\n$$\n\n**2. Solution of the Riccati ODE**\n\nThis is a first-order nonlinear ODE. For $c \\ne 0$, we can solve it by finding its steady-state solutions and using separation of variables. Let $P_{\\infty}$ be a steady-state solution, where $\\frac{\\mathrm{d}P}{\\mathrm{d}t}=0$:\n$$\n-\\frac{c^2}{r}P_{\\infty}^2 + 2a\\,P_{\\infty} + q = 0 \\implies \\frac{c^2}{r}P_{\\infty}^2 - 2a\\,P_{\\infty} - q = 0\n$$\nThe roots of this quadratic equation are:\n$$\nP_{\\infty} = \\frac{2a \\pm \\sqrt{4a^2 - 4(\\frac{c^2}{r})(-q)}}{2(\\frac{c^2}{r})} = \\frac{r}{c^2}\\left(a \\pm \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nLet $\\gamma = \\sqrt{a^2 + \\frac{c^2 q}{r}}$. The two roots are $\\lambda_1 = \\frac{r}{c^2}(a+\\gamma)$ and $\\lambda_2 = \\frac{r}{c^2}(a-\\gamma)$. Since $\\gamma \\ge |a|$, $\\lambda_1 \\ge 0$ and $\\lambda_2 \\le 0$. The variance $P_t$ must be non-negative. $\\lambda_1$ is the stable equilibrium for $P_t \\ge 0$.\nThe ODE can be written as $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = -\\frac{c^2}{r}(P_t - \\lambda_1)(P_t - \\lambda_2)$. Using separation of variables:\n$$\n\\int_{P_0}^{P_t} \\frac{\\mathrm{d}P}{(P - \\lambda_1)(P - \\lambda_2)} = -\\frac{c^2}{r} \\int_0^t \\mathrm{d}s = -\\frac{c^2}{r}t\n$$\nUsing partial fractions, $\\frac{1}{(P - \\lambda_1)(P - \\lambda_2)} = \\frac{1}{\\lambda_1 - \\lambda_2}\\left(\\frac{1}{P - \\lambda_1} - \\frac{1}{P - \\lambda_2}\\right)$. The integral becomes:\n$$\n\\frac{1}{\\lambda_1 - \\lambda_2} \\left[ \\ln\\left|\\frac{P-\\lambda_1}{P-\\lambda_2}\\right| \\right]_{P_0}^{P_t} = -\\frac{c^2}{r} t\n$$\nSince $P_t$ converges to $\\lambda_1$ from $P_0$ without crossing $\\lambda_2$ (as $\\lambda_2 \\le 0$ and $P_t \\ge 0$), the signs of $(P_t-\\lambda_1)$ and $(P_0-\\lambda_1)$ are the same, and $(P_t-\\lambda_2)>0, (P_0-\\lambda_2)>0$. We can drop the absolute value.\n$$\n\\ln\\left(\\frac{P_t-\\lambda_1}{P_t-\\lambda_2}\\right) - \\ln\\left(\\frac{P_0-\\lambda_1}{P_0-\\lambda_2}\\right) = -(\\lambda_1 - \\lambda_2)\\frac{c^2}{r} t\n$$\nNoting that $\\lambda_1 - \\lambda_2 = \\frac{2r\\gamma}{c^2}$, the exponent becomes $-(\\frac{2r\\gamma}{c^2})\\frac{c^2}{r} t = -2\\gamma t$.\n$$\n\\frac{P_t-\\lambda_1}{P_t-\\lambda_2} = \\frac{P_0-\\lambda_1}{P_0-\\lambda_2} \\exp(-2\\gamma t)\n$$\nSolving for $P_t$:\n$$\nP_t = \\frac{\\lambda_1(P_0-\\lambda_2) - \\lambda_2(P_0-\\lambda_1)\\exp(-2\\gamma t)}{(P_0-\\lambda_2) - (P_0-\\lambda_1)\\exp(-2\\gamma t)}\n$$\nwhere $\\lambda_1 = \\frac{r}{c^2}(a+\\gamma)$, $\\lambda_2 = \\frac{r}{c^2}(a-\\gamma)$, and $\\gamma = \\sqrt{a^2 + \\frac{c^2 q}{r}}$.\n\n**3. Convergence Conditions and Steady-State Value $P_{\\infty}$**\n\nFor $P_t$ to converge to a finite steady-state value $P_{\\infty} = \\lim_{t\\to\\infty} P_t$, the solution must approach a finite limit. From the explicit solution, this occurs if the system is \"detectable\". For this scalar system, detectability means that any unstable or neutrally stable mode ($a \\ge 0$) must be observable ($c \\ne 0$). This is equivalent to the condition:\n$$\na < 0 \\quad \\text{or} \\quad c \\ne 0\n$$\nIf this condition holds, we can find $P_{\\infty}$ by taking the limit of $P_t$ as $t \\to \\infty$. Assuming $\\gamma > 0$ (which is true if $a \\neq 0$ or $c^2q \\neq 0$), the term $\\exp(-2\\gamma t)$ goes to $0$.\n$$\nP_{\\infty} = \\lim_{t\\to\\infty} P_t = \\frac{\\lambda_1(P_0-\\lambda_2) - 0}{(P_0-\\lambda_2) - 0} = \\lambda_1\n$$\nThis corresponds to the stable, non-negative equilibrium point of the Riccati equation. The closed-form expression for the steady-state variance is:\n$$\nP_{\\infty} = \\frac{r}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nThis expression is valid for $c \\ne 0$. If $c=0$, the convergence condition requires $a<0$. In this case, the Riccati ODE becomes linear: $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2aP_t + q$, which has the steady-state solution $P_{\\infty} = -q/(2a)$. The formula for $c \\ne 0$ correctly tends to this limit as $c \\to 0$ for $a < 0$.\n\n**4. Numerical Evaluation**\n\nGiven the numerical values: $a=0.4$, $c=1.5$, $q=0.2$, $r=0.5$, and $P_0=1.0$.\nFirst, we check the convergence condition. We have $a=0.4 > 0$, but $c=1.5 \\ne 0$. The condition ($a < 0$ or $c \\ne 0$) is satisfied, so a finite steady-state variance exists. We use the formula for $P_{\\infty}$:\n$$\nP_{\\infty} = \\frac{r}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nSubstitute the given values:\n$$\nP_{\\infty} = \\frac{0.5}{(1.5)^2}\\left(0.4 + \\sqrt{(0.4)^2 + \\frac{(1.5)^2 (0.2)}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + \\frac{2.25 \\times 0.2}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + \\frac{0.45}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + 0.9}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{1.06}\\right)\n$$\nCalculating the numerical value:\n$$\nP_{\\infty} \\approx \\frac{0.5}{2.25}\\left(0.4 + 1.029563\\right) \\approx 0.222222 \\times (1.429563) \\approx 0.3176806\n$$\nRounding to four significant figures, we get:\n$$\nP_{\\infty} \\approx 0.3177\n$$", "answer": "$$\n\\boxed{0.3177}\n$$", "id": "3080944"}, {"introduction": "Before we advance to more complex matrix systems, it is crucial to ground our mathematics in physical reality. This practice shifts our focus from abstract calculation to conceptual understanding through the powerful tool of dimensional analysis. By meticulously checking the physical units throughout the filter update equations [@problem_id:3080945], we build intuition for how the Kalman gain correctly translates measurement information into meaningful state corrections.", "problem": "Consider a linear time-invariant continuous-time model for a moving vehicle with two-component state $x_t = [p_t, v_t]^\\top$, where $p_t$ is position with units $\\mathrm{m}$ and $v_t$ is velocity with units $\\mathrm{m}/\\mathrm{s}$. The state evolves according to the Stochastic Differential Equation (SDE)\n$$\nd x_t = A x_t \\, dt + \\Gamma \\, dW_t,\n$$\nwhere $A$ is a constant matrix, $\\Gamma$ is a constant diffusion matrix, and $W_t$ is a standard Wiener process (each component of $dW_t$ scales as $\\sqrt{dt}$, with $dt$ having units $\\mathrm{s}$). A sensor returns an integrated distance measurement corrupted by white noise, modeled as\n$$\nd y_t = C x_t \\, dt + D \\, dV_t,\n$$\nwhere $y_t$ has units $\\mathrm{m}$, $C$ is a row vector that produces a rate with units $\\mathrm{m}/\\mathrm{s}$ when multiplied by $x_t$, $D$ is a diffusion coefficient, and $V_t$ is a standard Wiener process independent of $W_t$. The innovation process is defined by\n$$\nd\\tilde{y}_t = d y_t - C \\hat{x}_t \\, dt,\n$$\nwhere $\\hat{x}_t$ is the filter estimate of $x_t$. The Kalman-Bucy estimate update is written in innovation form as\n$$\nd \\hat{x}_t = A \\hat{x}_t \\, dt + K_t \\, d\\tilde{y}_t,\n$$\nwhere $K_t$ is the Kalman gain matrix.\n\nAssume the measurement extracts velocity, i.e., $C x_t = v_t$, so that $C x_t$ has units $\\mathrm{m}/\\mathrm{s}$ and $d y_t$ has units $\\mathrm{m}$, and the diffusion coefficients satisfy the usual SDE scaling by $\\sqrt{dt}$. Let the process noise intensity be $Q_t = \\Gamma \\Gamma^\\top$ and the measurement noise intensity be $R_t = D D^\\top$. You are to determine the dimensional consistency of $K_t$ and the noise intensities to ensure that $K_t$ correctly maps measurement innovations to state increments.\n\nWhich option correctly and completely specifies the units that make the update dimensionally consistent?\n\nA. $d\\tilde{y}_t$ has units $\\mathrm{m}$; $K_t$ maps $\\mathrm{m}$ to $d\\hat{x}_t$ with units $[\\mathrm{m},\\,\\mathrm{m}/\\mathrm{s}]^\\top$, so $K_t$ must have row units $[\\mathrm{m}/\\mathrm{m},\\,(\\mathrm{m}/\\mathrm{s})/\\mathrm{m}]^\\top = [1,\\,1/\\mathrm{s}]^\\top$. The measurement noise intensity satisfies $R_t = D D^\\top$ with units $\\mathrm{m}^2/\\mathrm{s}$, and the process noise intensity satisfies $Q_t = \\Gamma \\Gamma^\\top$ with units “state-units squared per second,” i.e., the $(i,j)$ entry has units $(\\text{units of }x_i)\\cdot(\\text{units of }x_j)/\\mathrm{s}$.\n\nB. $d\\tilde{y}_t$ has units $\\mathrm{m}/\\mathrm{s}$; therefore $K_t$ must map $\\mathrm{m}/\\mathrm{s}$ to $d\\hat{x}_t$ with units $[\\mathrm{m},\\,\\mathrm{m}/\\mathrm{s}]^\\top$, so the row units of $K_t$ are $[\\mathrm{s},\\,1]^\\top$. The measurement noise intensity $R_t$ has units $\\mathrm{m}^2$, and the process noise intensity $Q_t$ has units “state-units squared” (no time scaling).\n\nC. $d\\tilde{y}_t$ has units $\\mathrm{m}$; $K_t$ is dimensionless in both rows. The measurement noise intensity $R_t$ has units $\\mathrm{m}^2/\\mathrm{s}$, and the process noise intensity $Q_t$ has units “state-units squared per second.”\n\nD. $d\\tilde{y}_t$ has units $\\mathrm{m}$; $K_t$ must have units $[1/\\mathrm{s},\\,1/\\mathrm{s}]^\\top$ to be consistent with the time scaling in $dt$. The measurement noise intensity $R_t$ has units $\\mathrm{m}^2/\\mathrm{s}$, but the process noise intensity $Q_t$ has units “state-units squared” (no time scaling).\n\nSelect the option that is fully consistent with the SDE scaling and ensures $K_t$ maps measurement innovations to state increments with correct units.", "solution": "This problem requires a dimensional analysis of the continuous-time Kalman-Bucy filter equations to find the correct units for the Kalman gain ($K_t$), process noise intensity ($Q_t$), and measurement noise intensity ($R_t$). Let $[X]$ denote the physical units of a quantity $X$.\n\n1.  **Basic Units**:\n    -   State vector $x_t$: $[x_t] = \\begin{pmatrix} [p_t] \\\\ [v_t] \\end{pmatrix} = \\begin{pmatrix} \\mathrm{m} \\\\ \\mathrm{m}/\\mathrm{s} \\end{pmatrix}$.\n    -   Time increment $dt$: $[dt] = \\mathrm{s}$.\n    -   Wiener process increment $dW_t$: Since the variance of $dW_t$ is $dt$, dimensionally we have $[(dW_t)^2] = [dt] = \\mathrm{s}$, which implies $[dW_t] = \\mathrm{s}^{1/2}$.\n\n2.  **Measurement SDE Units**: $d y_t = C x_t \\, dt + D \\, dV_t$.\n    -   Given $[y_t]=\\mathrm{m}$, the increment has units $[dy_t] = \\mathrm{m}$.\n    -   The problem states $[C x_t] = \\mathrm{m}/\\mathrm{s}$, so the drift term is consistent: $[C x_t \\, dt] = (\\mathrm{m}/\\mathrm{s}) \\cdot \\mathrm{s} = \\mathrm{m}$.\n    -   The noise term must match: $[D \\, dV_t] = \\mathrm{m}$. With $[dV_t] = \\mathrm{s}^{1/2}$, we find the units of the diffusion coefficient: $[D] = \\mathrm{m}/\\mathrm{s}^{1/2}$.\n    -   The measurement noise intensity is $R_t = D D^\\top$. Since $y_t$ is scalar, $R_t$ is scalar with units $[R_t] = [D]^2 = (\\mathrm{m}/\\mathrm{s}^{1/2})^2 = \\mathrm{m}^2/\\mathrm{s}$.\n\n3.  **Process SDE Units**: $d x_t = A x_t \\, dt + \\Gamma \\, dW_t$.\n    -   The noise term must have units of the state: $[\\Gamma \\, dW_t] = [x_t] = [\\mathrm{m}, \\mathrm{m}/\\mathrm{s}]^\\top$.\n    -   This means the rows of $\\Gamma$ have units of $[x_i]/\\mathrm{s}^{1/2}$.\n    -   The process noise intensity is $Q_t = \\Gamma \\Gamma^\\top$. The units of the $(i,j)$ entry are $[Q_{t,ij}] = ([x_i]/\\mathrm{s}^{1/2}) \\cdot ([x_j]/\\mathrm{s}^{1/2}) = ([x_i] \\cdot [x_j])/\\mathrm{s}$. This corresponds to \"state-units squared per second\".\n\n4.  **Filter Update Units**: $d \\hat{x}_t = A \\hat{x}_t \\, dt + K_t \\, d\\tilde{y}_t$.\n    -   First, find the units of the innovation increment: $d\\tilde{y}_t = d y_t - C \\hat{x}_t \\, dt$. We found $[dy_t]=\\mathrm{m}$ and $[C \\hat{x}_t \\, dt]=\\mathrm{m}$, so $[d\\tilde{y}_t] = \\mathrm{m}$.\n    -   The update term $K_t \\, d\\tilde{y}_t$ must have the units of the state increment, $[d\\hat{x}_t] = [x_t] = [\\mathrm{m}, \\mathrm{m}/\\mathrm{s}]^\\top$.\n    -   Since $[d\\tilde{y}_t]=\\mathrm{m}$, we solve for the units of the gain matrix $K_t = [K_{t,1}, K_{t,2}]^\\top$:\n        -   $[K_{t,1}] \\cdot \\mathrm{m} = \\mathrm{m} \\implies [K_{t,1}] = 1$ (dimensionless).\n        -   $[K_{t,2}] \\cdot \\mathrm{m} = \\mathrm{m}/\\mathrm{s} \\implies [K_{t,2}] = 1/\\mathrm{s}$.\n    -   Therefore, $K_t$ has row units of $[1, 1/\\mathrm{s}]^\\top$.\n\n**Evaluating the Options:**\n\n-   **A**: States $[d\\tilde{y}_t]=\\mathrm{m}$, $[K_t]=[1, 1/\\mathrm{s}]^\\top$, $[R_t]=\\mathrm{m}^2/\\mathrm{s}$, and $[Q_t]$ is \"state-units squared per second\". This matches our derivation completely.\n-   **B**: Incorrectly states $[d\\tilde{y}_t]=\\mathrm{m}/\\mathrm{s}$ and other consequential errors.\n-   **C**: Incorrectly states $K_t$ is dimensionless in both rows.\n-   **D**: Incorrectly states $[K_t]=[1/\\mathrm{s}, 1/\\mathrm{s}]^\\top$ and that $Q_t$ has no time scaling.\n\nTherefore, option A is the only one that is fully consistent with the dimensional analysis of the continuous-time stochastic differential equations.", "answer": "$$\\boxed{A}$$", "id": "3080945"}, {"introduction": "The stability of a Kalman-Bucy filter is not always guaranteed; it depends on deep structural properties of the system being observed. This computational exercise dives into the crucial concept of \"detectability,\" the condition that ensures the estimation error can remain bounded. By implementing a numerical integrator for the matrix Riccati equation [@problem_id:3080976], you will witness firsthand how an unobservable unstable state dooms the filter to diverge, providing a tangible link between abstract linear algebra and practical filter performance.", "problem": "Consider a continuous-time linear stochastic system and measurement model with Gaussian white noise,\n$$\n\\mathrm{d}x_t = A x_t \\,\\mathrm{d}t + G \\,\\mathrm{d}W_t,\\quad \\mathrm{d}y_t = C x_t \\,\\mathrm{d}t + D \\,\\mathrm{d}V_t,\n$$\nwhere $x_t \\in \\mathbb{R}^n$, $y_t \\in \\mathbb{R}^m$, $A \\in \\mathbb{R}^{n \\times n}$, $C \\in \\mathbb{R}^{m \\times n}$, $G \\in \\mathbb{R}^{n \\times r}$, $D \\in \\mathbb{R}^{m \\times s}$, and $W_t, V_t$ are standard Wiener processes. Let the process noise covariance be $Q = G G^\\top \\in \\mathbb{R}^{n \\times n}$ and the measurement noise covariance be $R = D D^\\top \\in \\mathbb{R}^{m \\times m}$, with $R$ positive definite. The Kalman-Bucy filter (KBF) estimates $x_t$ with an estimator $\\hat{x}_t$ and a symmetric positive semidefinite estimation error covariance $P_t = \\mathbb{E}[(x_t - \\hat{x}_t)(x_t - \\hat{x}_t)^\\top]$ governed by the continuous-time Riccati differential equation\n$$\n\\dot{P}_t = A P_t + P_t A^\\top + Q - P_t C^\\top R^{-1} C P_t,\\quad P_0 = P(0).\n$$\nThe pair $(A,C)$ is called detectable if every eigenvalue of $A$ with nonnegative real part corresponds to a state that is observable under $C$; equivalently, all unobservable modes are exponentially stable. In this problem you will construct an example where $(A,C)$ is not detectable and show numerically that the error covariance $P_t$ diverges and that a sampled innovation variance grows without bound. To quantify the latter in a way that is meaningful from first principles, define the sampled innovation residual over a fixed sampling interval $\\Delta t$ by\n$$\nr_k = y_{t_k+\\Delta t} - y_{t_k} - C \\hat{x}_{t_k}\\,\\Delta t,\n$$\nwhose variance under the linear-Gaussian model and small $\\Delta t$ is well-approximated by\n$$\n\\mathrm{Var}(r_k) \\approx C P_{t_k} C^\\top\\,\\Delta t^2 + R\\,\\Delta t.\n$$\nYour program must integrate the Riccati differential equation for each test case, compute $P_T$ at a final time $T$, and report:\n- the largest eigenvalue $\\lambda_{\\max}(P_T)$, and\n- the sampled innovation variance $S_T = C P_T C^\\top\\,\\Delta t^2 + R\\,\\Delta t$\nat the final time.\n\nUse the following fixed numerical settings for all test cases:\n- dimension $n=2$, measurement dimension $m=1$,\n- initial covariance $P_0 = I_2$ (the $2 \\times 2$ identity),\n- process noise matrix $G = I_2$, so $Q = I_2$,\n- measurement noise matrix $D = 1$ (scalar), so $R = [1]$,\n- time step $\\Delta t = 0.01$ and final time $T = 20$,\n- integrate $\\dot{P}_t$ using a stable explicit Runge–Kutta method of order $4$, ensuring $P_t$ remains symmetric at each step.\n\nTest Suite:\n- Case $1$ (not detectable; unstable unobservable mode): $A = \\mathrm{diag}(0.5,-1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n- Case $2$ (detectable; unstable mode observed): $A = \\mathrm{diag}(0.5,-1.0)$, $C = \\begin{bmatrix}1 & 0\\end{bmatrix}$.\n- Case $3$ (detectable but not observable; all unobservable modes stable): $A = \\mathrm{diag}(-0.2,-0.3)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n- Case $4$ (not detectable boundary; marginally stable unobservable mode): $A = \\mathrm{diag}(0.0,-1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n\nFor each case, numerically integrate the Riccati equation from $t=0$ to $t=T$. Then compute $\\lambda_{\\max}(P_T)$ and $S_T$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the list has four items (one per test case) and each item is itself a two-element list $[\\lambda_{\\max}(P_T), S_T]$. For example, the output format must be\n$$\n[[\\lambda_{\\max}^{(1)},S^{(1)}],[\\lambda_{\\max}^{(2)},S^{(2)}],[\\lambda_{\\max}^{(3)},S^{(3)}],[\\lambda_{\\max}^{(4)},S^{(4)}]],\n$$\nwith each quantity represented as a floating-point number, and no additional text printed.", "solution": "This problem explores the concept of detectability in Kalman-Bucy filtering by numerically integrating the matrix Riccati differential equation (RDE) for four different system configurations. The RDE governs the evolution of the estimation error covariance $P_t$:\n$$\n\\dot{P}_t = A P_t + P_t A^\\top + Q - P_t C^\\top R^{-1} C P_t\n$$\nA filter is guaranteed to have a bounded error covariance that converges to a steady-state solution if the system pair $(A, C)$ is detectable and the pair $(A,G)$ is stabilizable. The condition that $(A,G)$ is stabilizable is met in all cases since $Q=I_2$ is positive definite. Detectability of $(A,C)$ requires that any unstable or marginally stable mode of the system matrix $A$ be observable through the measurement matrix $C$.\n\nWe analyze each case based on this principle:\n- **Case 1**: $A = \\mathrm{diag}(0.5, -1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The system has an unstable eigenvalue $\\lambda=0.5$. Its corresponding eigenvector $e_1 = [1, 0]^\\top$ is in the null space of $C$ ($Ce_1=0$). The unstable mode is unobservable, so the pair $(A,C)$ is not detectable. The error covariance $P_t$ is expected to diverge.\n- **Case 2**: $A = \\mathrm{diag}(0.5, -1.0)$, $C = \\begin{bmatrix}1 & 0\\end{bmatrix}$. The unstable mode with eigenvector $e_1$ is now observable ($Ce_1=1$). The pair $(A,C)$ is detectable, and $P_t$ should converge to a finite steady-state value.\n- **Case 3**: $A = \\mathrm{diag}(-0.2, -0.3)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. All system modes are stable. The detectability condition (which concerns only unstable or marginally stable modes) is trivially satisfied. Thus, $P_t$ is expected to converge.\n- **Case 4**: $A = \\mathrm{diag}(0.0, -1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The system has a marginally stable (integrator) mode with $\\lambda=0$. Its eigenvector $e_1 = [1, 0]^\\top$ is unobservable ($Ce_1=0$). The pair $(A,C)$ is not detectable, and the error covariance $P_t$ is expected to diverge.\n\nTo obtain the numerical results, the RDE is integrated from $t=0$ to $T=20$ using a fourth-order Runge-Kutta (RK4) method with a step size of $\\Delta t=0.01$. At each step, the symmetry of the covariance matrix $P_t$ is enforced to prevent numerical error accumulation. The final matrix $P_T$ is used to compute the largest eigenvalue $\\lambda_{\\max}(P_T)$ and the sampled innovation variance $S_T = C P_T C^\\top\\,\\Delta t^2 + R\\,\\Delta t$. The Python implementation below carries out this procedure.\n\n```python\nimport numpy as np\n\ndef solve_riccati_cases():\n    \"\"\"\n    Solves the continuous-time Riccati differential equation for the four test cases.\n    \"\"\"\n    # Fixed numerical settings\n    n = 2\n    P0 = np.identity(n)\n    Q = np.identity(n)\n    R = np.array([[1.0]])\n    R_inv = np.array([[1.0]])\n    DT = 0.01\n    T_FINAL = 20.0\n\n    test_cases = [\n        {\"A\": np.diag([0.5, -1.0]), \"C\": np.array([[0.0, 1.0]])},\n        {\"A\": np.diag([0.5, -1.0]), \"C\": np.array([[1.0, 0.0]])},\n        {\"A\": np.diag([-0.2, -0.3]), \"C\": np.array([[0.0, 1.0]])},\n        {\"A\": np.diag([0.0, -1.0]), \"C\": np.array([[0.0, 1.0]])},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, C = case[\"A\"], case[\"C\"]\n        P = P0.copy()\n        \n        def riccati_rhs(p_matrix):\n            return A @ p_matrix + p_matrix @ A.T + Q - p_matrix @ C.T @ R_inv @ C @ p_matrix\n\n        num_steps = int(T_FINAL / DT)\n        for _ in range(num_steps):\n            k1 = riccati_rhs(P)\n            k2 = riccati_rhs(P + 0.5 * DT * k1)\n            k3 = riccati_rhs(P + 0.5 * DT * k2)\n            k4 = riccati_rhs(P + DT * k3)\n            P_next = P + (DT / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            P = 0.5 * (P_next + P_next.T) # Enforce symmetry\n\n        P_T = P\n        lambda_max_PT = np.max(np.linalg.eigvalsh(P_T))\n        S_T = (C @ P_T @ C.T * (DT**2) + R * DT)[0, 0]\n        results.append([lambda_max_PT, S_T])\n        \n    return results\n```\nThe numerical results from this procedure demonstrate the predicted behavior: divergence for non-detectable cases (1 and 4) and convergence for detectable cases (2 and 3). The final values are reported in the answer section.", "answer": "$$\n[[1.8109312157147772e+17, 0.010416666666666664], [1.618033988749895, 0.0002618033988749895], [0.5811388300841898, 0.00015811388300841896], [41.0, 0.010416666666666664]]\n$$", "id": "3080976"}]}