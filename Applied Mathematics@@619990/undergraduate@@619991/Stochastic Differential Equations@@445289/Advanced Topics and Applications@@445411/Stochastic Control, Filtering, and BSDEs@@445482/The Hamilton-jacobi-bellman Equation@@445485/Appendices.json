{"hands_on_practices": [{"introduction": "Before tackling the challenge of finding an optimal control policy, it is crucial to understand how to evaluate the cost associated with any given policy. This foundational exercise asks you to compute the expected cost for a system under a simple, pre-determined control strategy (specifically, a zero-control policy). By calculating the cost functional directly, you will gain a tangible understanding of the quantity that the Hamilton-Jacobi-Bellman equation seeks to minimize, reinforcing your grasp of the interplay between stochastic dynamics and performance criteria. [@problem_id:3080738]", "problem": "Consider the cost functional in stochastic optimal control defined for a control process $u=\\{u_{s}\\}_{s \\in [t,T]}$ by\n$$\nJ^{u}(t,x) \\;=\\; \\mathbb{E}\\!\\left[ \\int_{t}^{T} f(s, X_{s}^{t,x,u}, u_{s}) \\,\\mathrm{d}s \\;+\\; g\\!\\left(X_{T}^{t,x,u}\\right) \\right],\n$$\nwhere $f(s,x,u)$ is the running cost, $g(x)$ is the terminal cost, $\\mathbb{E}[\\cdot]$ denotes expectation, and $X_{s}^{t,x,u}$ is the state process starting from $X_{t}^{t,x,u}=x$. Assume the state process is the uncontrolled Ornstein–Uhlenbeck diffusion on $[0,T]$,\n$$\n\\mathrm{d}X_{s} \\;=\\; -\\alpha\\, X_{s}\\,\\mathrm{d}s \\;+\\; \\sigma\\, \\mathrm{d}W_{s}, \\qquad X_{0}=x,\n$$\nwhere $\\alpha>0$, $\\sigma>0$ are constants, and $W_{s}$ is a standard one-dimensional Brownian motion. Take the running cost and terminal cost to be\n$$\nf(s,x,u) = x^{2} + u^{2}, \\qquad g(x) = x^{2}.\n$$\nFor the identically zero control $u \\equiv 0$, compute the closed-form expression of $J^{u}(0,x)$ in terms of $\\alpha$, $\\sigma$, $T$, and $x$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "The user has requested the computation of a cost functional for a specific stochastic process and control.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n\n-   **Cost Functional**: $J^{u}(t,x) = \\mathbb{E}\\left[ \\int_{t}^{T} f(s, X_{s}^{t,x,u}, u_{s}) \\,\\mathrm{d}s + g(X_{T}^{t,x,u}) \\right]$.\n-   **State Process SDE**: $\\mathrm{d}X_{s} = -\\alpha X_{s}\\,\\mathrm{d}s + \\sigma \\mathrm{d}W_{s}$, where $X_{s}$ is an uncontrolled Ornstein-Uhlenbeck process.\n-   **Initial Condition**: $X_{0}=x$.\n-   **Parameters**: Constants $\\alpha > 0$ and $\\sigma > 0$.\n-   **Process**: $W_s$ is a standard one-dimensional Brownian motion.\n-   **Running Cost**: $f(s,x,u) = x^2 + u^2$.\n-   **Terminal Cost**: $g(x) = x^2$.\n-   **Control**: The control is identically zero, $u_s \\equiv 0$ for all $s \\in [t, T]$.\n-   **Objective**: Compute the closed-form expression for $J^{u}(0,x)$ with $u \\equiv 0$.\n\n**1.2. Validation against Criteria**\n\n-   **Scientific Groundedness**: The problem is formulated within the standard mathematical theory of stochastic optimal control. The Ornstein-Uhlenbeck process is a fundamental stochastic process. All elements are scientifically and mathematically valid.\n-   **Well-Posedness**: The problem asks for the evaluation of a specific expected value, not the optimization of the functional. Given the well-behaved nature of the OU process and quadratic costs, the expectation is well-defined and a unique solution exists.\n-   **Objectivity**: The problem is stated in precise mathematical terms, free of any ambiguity or subjective language.\n-   **Completeness and Consistency**: All necessary functions, parameters, and conditions are provided. The state process is specified as \"uncontrolled,\" which is consistent with the provided SDE that lacks a control term. The cost functional includes a control variable $u$, but the problem specifies that we must evaluate it for the case $u \\equiv 0$, which is also consistent.\n-   **Other Flaws**: The problem is non-trivial, verifiable, and does not exhibit any other disqualifying flaws.\n\n**1.3. Verdict**\n\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Step 2: Derivation of the Solution\n\nThe task is to compute $J^{u}(0,x)$ for the specific control $u_s \\equiv 0$. The state process $X_s$ is denoted as $X_s^{0,x}$ to emphasize its start at time $t=0$ from state $x$. Since the SDE is uncontrolled, the dynamics are independent of $u$, so we will simply write $X_s$.\n\nThe cost functional with $u_s=0$, $t=0$, and $X_0=x$ becomes:\n$$\nJ^{0}(0,x) = \\mathbb{E}\\left[ \\int_{0}^{T} f(s, X_{s}, 0) \\,\\mathrm{d}s + g(X_{T}) \\right]\n$$\nSubstituting the given running and terminal cost functions, $f(s,x,0) = x^2+0^2=x^2$ and $g(x)=x^2$:\n$$\nJ^{0}(0,x) = \\mathbb{E}\\left[ \\int_{0}^{T} (X_{s})^{2} \\,\\mathrm{d}s + (X_{T})^{2} \\right]\n$$\nBy the linearity of expectation and Fubini's theorem (for stochastic integrals), we can write:\n$$\nJ^{0}(0,x) = \\int_{0}^{T} \\mathbb{E}[(X_{s})^{2}] \\,\\mathrm{d}s + \\mathbb{E}[(X_{T})^{2}]\n$$\nTo proceed, we must find the second moment of the Ornstein-Uhlenbeck process, $\\mathbb{E}[(X_s)^2]$.\n\nThe SDE for the Ornstein-Uhlenbeck process is $\\mathrm{d}X_s = -\\alpha X_s \\mathrm{d}s + \\sigma \\mathrm{d}W_s$ with $X_0=x$. This is a linear SDE, and its solution is well-known:\n$$\nX_s = x \\exp(-\\alpha s) + \\sigma \\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\n$$\nThe solution $X_s$ is a Gaussian random variable. We can compute its mean and variance. The Itô integral has zero expectation:\n$$\n\\mathbb{E}\\left[\\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\\right] = 0\n$$\nTherefore, the mean of $X_s$ is:\n$$\n\\mathbb{E}[X_s] = \\mathbb{E}[x \\exp(-\\alpha s)] = x \\exp(-\\alpha s)\n$$\nThe variance of $X_s$ is found using Itô isometry:\n$$\n\\mathrm{Var}(X_s) = \\mathrm{Var}\\left(\\sigma \\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\\right) = \\sigma^2 \\mathbb{E}\\left[\\left(\\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\\right)^2\\right]\n$$\n$$\n\\mathrm{Var}(X_s) = \\sigma^2 \\int_0^s \\exp(-2\\alpha(s-\\tau)) \\mathrm{d}\\tau = \\sigma^2 \\exp(-2\\alpha s) \\int_0^s \\exp(2\\alpha\\tau) \\mathrm{d}\\tau\n$$\n$$\n\\mathrm{Var}(X_s) = \\sigma^2 \\exp(-2\\alpha s) \\left[ \\frac{\\exp(2\\alpha\\tau)}{2\\alpha} \\right]_0^s = \\sigma^2 \\exp(-2\\alpha s) \\frac{\\exp(2\\alpha s) - 1}{2\\alpha} = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s))\n$$\nThe second moment is given by $\\mathbb{E}[X_s^2] = \\mathrm{Var}(X_s) + (\\mathbb{E}[X_s])^2$:\n$$\n\\mathbb{E}[X_s^2] = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + (x \\exp(-\\alpha s))^2 = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + x^2 \\exp(-2\\alpha s)\n$$\nNow we can compute the two terms in the expression for $J^0(0,x)$.\n\nFirst, the integral term:\n$$\n\\int_{0}^{T} \\mathbb{E}[(X_{s})^{2}] \\,\\mathrm{d}s = \\int_{0}^{T} \\left( \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + x^2 \\exp(-2\\alpha s) \\right) \\mathrm{d}s\n$$\n$$\n= \\frac{\\sigma^2}{2\\alpha} \\int_{0}^{T} (1 - \\exp(-2\\alpha s)) \\mathrm{d}s + x^2 \\int_{0}^{T} \\exp(-2\\alpha s) \\mathrm{d}s\n$$\nThe integrals evaluate to:\n$$\n\\int_{0}^{T} (1 - \\exp(-2\\alpha s)) \\mathrm{d}s = \\left[ s + \\frac{\\exp(-2\\alpha s)}{2\\alpha} \\right]_0^T = \\left( T + \\frac{\\exp(-2\\alpha T)}{2\\alpha} \\right) - \\left( 0 + \\frac{1}{2\\alpha} \\right) = T - \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\n$$\n$$\n\\int_{0}^{T} \\exp(-2\\alpha s) \\mathrm{d}s = \\left[ \\frac{\\exp(-2\\alpha s)}{-2\\alpha} \\right]_0^T = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\n$$\nSo the integral term is:\n$$\n\\int_{0}^{T} \\mathbb{E}[(X_{s})^{2}] \\,\\mathrm{d}s = \\frac{\\sigma^2}{2\\alpha} \\left( T - \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha} \\right) + x^2 \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\n$$\n$$\n= \\frac{\\sigma^2 T}{2\\alpha} - \\frac{\\sigma^2}{4\\alpha^2}(1 - \\exp(-2\\alpha T)) + \\frac{x^2}{2\\alpha}(1 - \\exp(-2\\alpha T))\n$$\nSecond, the terminal cost term is $\\mathbb{E}[X_T^2]$, which we find by setting $s=T$ in the expression for the second moment:\n$$\n\\mathbb{E}[X_T^2] = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T)) + x^2 \\exp(-2\\alpha T)\n$$\nFinally, we sum the two terms to find $J^0(0,x)$:\n$$\nJ^{0}(0,x) = \\left( \\frac{\\sigma^2 T}{2\\alpha} - \\frac{\\sigma^2}{4\\alpha^2}(1 - \\exp(-2\\alpha T)) + \\frac{x^2}{2\\alpha}(1 - \\exp(-2\\alpha T)) \\right) + \\left( \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T)) + x^2 \\exp(-2\\alpha T) \\right)\n$$\nWe group terms with and without the factor $\\exp(-2\\alpha T)$:\n$$\nJ^0(0,x) = \\left( \\frac{\\sigma^2 T}{2\\alpha} - \\frac{\\sigma^2}{4\\alpha^2} + \\frac{x^2}{2\\alpha} + \\frac{\\sigma^2}{2\\alpha} \\right) + \\exp(-2\\alpha T) \\left( \\frac{\\sigma^2}{4\\alpha^2} - \\frac{x^2}{2\\alpha} - \\frac{\\sigma^2}{2\\alpha} + x^2 \\right)\n$$\nSimplify the constant and exponential coefficients:\nThe constant part is:\n$$\n\\frac{2\\alpha\\sigma^2 T - \\sigma^2 + 2\\alpha x^2 + 2\\alpha \\sigma^2}{4\\alpha^2} = \\frac{2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1)}{4\\alpha^2}\n$$\nThe coefficient of $\\exp(-2\\alpha T)$ is:\n$$\n\\frac{\\sigma^2 - 2\\alpha x^2 - 2\\alpha\\sigma^2 + 4\\alpha^2 x^2}{4\\alpha^2} = \\frac{(4\\alpha^2 - 2\\alpha)x^2 - (2\\alpha - 1)\\sigma^2}{4\\alpha^2} = \\frac{2\\alpha(2\\alpha - 1)x^2 - (2\\alpha - 1)\\sigma^2}{4\\alpha^2} = \\frac{(2\\alpha-1)(2\\alpha x^2 - \\sigma^2)}{4\\alpha^2}\n$$\nCombining these results gives the final expression:\n$$\nJ^0(0,x) = \\frac{2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1)}{4\\alpha^2} + \\frac{(2\\alpha - 1)(2\\alpha x^2 - \\sigma^2)}{4\\alpha^2} \\exp(-2\\alpha T)\n$$\nThis can be written as a single fraction.\n$$\nJ^0(0,x) = \\frac{1}{4\\alpha^2}\\left[ 2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1) + (2\\alpha - 1)(2\\alpha x^2 - \\sigma^2)\\exp(-2\\alpha T) \\right]\n$$\nThis is the closed-form expression for the cost functional.", "answer": "$$\n\\boxed{\\frac{1}{4\\alpha^2}\\left[ 2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1) + \\left(2\\alpha - 1\\right)\\left(2\\alpha x^2 - \\sigma^2\\right)\\exp(-2\\alpha T) \\right]}\n$$", "id": "3080738"}, {"introduction": "At the heart of the dynamic programming approach to optimal control is the Hamiltonian. This function elegantly combines the immediate running cost with the anticipated future costs, encapsulated by the value function's derivatives. This exercise focuses on the pivotal step in the HJB methodology: minimizing the Hamiltonian with respect to the control variable. By performing this minimization, you will derive the optimal feedback control as a function of the system's state and the value function's gradient, a cornerstone for constructing the final HJB partial differential equation. [@problem_id:3080765]", "problem": "Consider the one-dimensional controlled Itô stochastic differential equation\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwith drift $b(x,u) = u$, constant diffusion coefficient $\\sigma > 0$, and control process $u_{t}$ taking values in $\\mathbb{R}$. Let the running cost be\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\nwith $r>0$ and $q>0$. Define the value function $V(t,x)$ for a finite horizon problem with a bounded and continuous terminal cost $g(x)$ by\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\nwhere the infimum is over progressively measurable controls taking values in $\\mathbb{R}$. Using the dynamic programming principle and the definition of the Hamiltonian appearing in the Hamilton-Jacobi-Bellman (HJB) equation, the Hamiltonian at a point $(x,p,M)$ is given by the infimum over controls $u \\in \\mathbb{R}$ of the sum of the controlled generator applied to a $C^{2}$ test function and the running cost. In particular, if $p$ denotes the first spatial derivative and $M$ denotes the second spatial derivative, compute:\n- the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$;\n- the corresponding minimizing state-feedback control as an explicit function of $p$ and $r$.\n\nProvide your final answer as a two-entry row matrix, with the first entry being the minimized Hamiltonian and the second entry being the minimizing feedback control. No numerical approximation is required, and no units are involved.", "solution": "The user wants me to solve a problem related to the Hamilton-Jacobi-Bellman (HJB) equation in the context of stochastic optimal control.\n\n### Step 1: Extract Givens\n- The controlled Itô stochastic differential equation (SDE): $\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$.\n- The drift function: $b(x,u) = u$.\n- The diffusion coefficient: $\\sigma > 0$ is a constant.\n- The control process $u_{t}$ takes values in $\\mathbb{R}$.\n- The running cost function: $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$.\n- The constants in the cost function satisfy: $r>0$ and $q>0$.\n- The value function is defined for a finite horizon $[t, T]$ as: $V(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right]$.\n- The terminal cost $g(x)$ is a bounded and continuous function.\n- The Hamiltonian at a point $(x,p,M)$ is defined as the infimum over controls $u \\in \\mathbb{R}$ of the sum of the controlled generator applied to a $C^{2}$ test function and the running cost.\n- The variable $p$ denotes the first spatial derivative of the test function.\n- The variable $M$ denotes the second spatial derivative of the test function.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation.\n\n- **Scientifically Grounded**: The problem is a standard formulation of a stochastic linear-quadratic regulator (LQR) problem, a cornerstone of optimal control theory. The Hamilton-Jacobi-Bellman (HJB) equation is the central tool for solving such problems via dynamic programming. All concepts are standard and well-established in the field of stochastic differential equations and control theory.\n- **Well-Posed**: The problem is well-posed. It asks for the calculation of two specific, well-defined quantities: the minimized Hamiltonian and the optimal feedback control. The conditions $r>0$ ensure that the cost function is strictly convex in the control variable $u$, which guarantees the existence and uniqueness of a minimum.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the instructions. It is scientifically sound, well-posed, objective, complete, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution\nThe problem asks for the minimized Hamiltonian and the corresponding optimal control for a given stochastic control problem. The Hamiltonian, as defined in the context of the Hamilton-Jacobi-Bellman (HJB) equation, is given by:\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + f(x,u) \\right\\}\n$$\nwhere $\\mathcal{L}^{u}$ is the generator of the controlled stochastic process, applied to a test function whose first and second spatial derivatives are denoted by $p$ and $M$ respectively, and $f(x,u)$ is the running cost.\n\nFirst, we determine the generator $\\mathcal{L}^{u}$. For a general one-dimensional Itô process given by $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$, the generator applied to a $C^{2}$ function $\\phi(x)$ is:\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\nIn our specific problem, the SDE is $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$. The drift is $\\mu(t,x,u) = u$ and the diffusion is $\\Sigma(t,x,u) = \\sigma$. Let $\\phi(x)$ be a generic $C^2$ test function. According to the problem statement, we denote its derivatives as $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ and $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$.\nThe generator for the controlled process, $\\mathcal{L}^{u}$, is therefore:\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\nNext, we add the running cost $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$. The expression to be minimized with respect to the control $u \\in \\mathbb{R}$ is:\n$$\nJ(u) = \\mathcal{L}^{u} + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\nWe can rearrange this expression as a function of $u$:\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\nThis is a quadratic function of $u$. Since the problem states that $r>0$, the parabola opens upwards, and the function has a unique global minimum. To find the value of $u$ that minimizes $J(u)$, we compute the first derivative with respect to $u$ and set it to zero.\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\nSetting the derivative to zero gives the optimal control, which we denote as $u^{\\star}$:\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\nThis is the minimizing state-feedback control as a function of $p$ and $r$. This is the second part of the required answer.\n\nTo find the minimized Hamiltonian, $H(x,p,M)$, we substitute $u^{\\star}$ back into the expression for $J(u)$:\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nCombining the terms involving $p^{2}$:\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nThis is the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$. This is the first part of the required answer.\n\nThe final answer is presented as a two-entry row matrix.\nFirst entry: Minimized Hamiltonian $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$.\nSecond entry: Minimizing control $u^{\\star} = -\\frac{p}{r}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M & -\\frac{p}{r} \\end{pmatrix}}\n$$", "id": "3080765"}, {"introduction": "Having practiced evaluating costs and minimizing the Hamiltonian, you are now ready to solve a complete stochastic optimal control problem from start to finish. This practice guides you through the analytical solution of a classic linear-quadratic (LQ) problem, a ubiquitous model in engineering and finance. You will employ the powerful technique of assuming a quadratic form for the value function, which elegantly transforms the complex HJB partial differential equation into a more manageable ordinary differential equation—the famous Riccati equation—ultimately yielding an explicit and practical optimal control law. [@problem_id:3080756]", "problem": "Consider the scalar controlled stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\nwhere $W_t$ is a standard Brownian motion, and $u_t$ is a progressively measurable control taking values in $\\mathbb{R}$. For a fixed finite horizon $T&gt;0$, the performance criterion is\n$$\nJ^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2 \\,\\Big|\\, X_t = x\\Bigg].\n$$\nUsing the Dynamic Programming Principle (DPP) and Itô's formula as the foundational starting point, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(t,x) = \\inf_{u} J^{u}(t,x)$, state the verification theorem in this setting, and from it derive the optimal feedback law $u^{*}(t,x)$.\n\nThen, specialize to the parameter values $a=0$, $b=1$, $\\sigma&gt;0$ arbitrary, $q=1$, $r=1$, and terminal weight $s_f=1$, on the time interval $[0,T]$ with $T&gt;0$. Compute explicitly the optimal feedback control $u^{*}(t,x)$ and the resulting closed-loop drift $\\mu_{\\mathrm{cl}}(t,x)$ of the state SDE under $u^{*}$.\n\nYour final answer must consist of the pair $\\big(u^{*}(t,x),\\,\\mu_{\\mathrm{cl}}(t,x)\\big)$ as a single closed-form analytic expression. No numerical rounding is required. No physical units are involved.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   The state dynamics are governed by the scalar controlled stochastic differential equation (SDE):\n    $$\n    \\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x\n    $$\n-   $W_t$ is a standard Brownian motion.\n-   $u_t$ is a progressively measurable control taking values in $\\mathbb{R}$.\n-   The time horizon is a fixed finite value $T>0$.\n-   The performance criterion to be minimized is:\n    $$\n    J^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2\\ \\Big|\\ X_t = x\\Bigg]\n    $$\n-   The value function is defined as $V(t,x) = \\inf_{u} J^{u}(t,x)$.\n-   For the second part of the problem, the parameters are specialized to: $a=0$, $b=1$, $\\sigma>0$ (arbitrary), $q=1$, $r=1$, and terminal weight $s_f=1$, on the time interval $[0,T]$.\n-   The required output is the pair of the optimal feedback control $u^{*}(t,x)$ and the resulting closed-loop drift $\\mu_{\\mathrm{cl}}(t,x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem presented is a standard exercise in stochastic optimal control, specifically a linear-quadratic (LQ) control problem for a stochastic system.\n\n-   **Scientifically Grounded**: The problem is based on the well-established mathematical theories of stochastic differential equations, Itô calculus, and dynamic programming (specifically, the Hamilton-Jacobi-Bellman framework). All components are standard in advanced control theory and financial mathematics.\n-   **Well-Posed**: The problem is well-posed. The quadratic cost function with coefficients $q \\ge 0$, $r>0$, and $s_f \\ge 0$ (here specified as $q=1$, $r=1$, $s_f=1$) ensures that the minimization problem is convex and typically admits a unique solution. The problem provides all necessary information to derive the solution.\n-   **Objective**: The problem is stated in precise, objective mathematical language, free of any ambiguity or subjective elements.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically grounded, well-posed, and objective. We may proceed with the solution.\n\n### Derivation of the Hamilton-Jacobi-Bellman Equation\n\nThe derivation begins with the Dynamic Programming Principle (DPP). For any small time increment $\\delta t > 0$, the value function $V(t,x)$ satisfies:\n$$\nV(t,x) = \\inf_{u \\in \\mathcal{U}[t,T]} \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} (qX_s^2 + r u_s^2)\\,\\mathrm{d}s + V(t+\\delta t, X_{t+\\delta t}) \\,\\mid\\, X_t=x \\right]\n$$\nwhere $\\mathcal{U}[t,T]$ is the set of admissible controls over the interval $[t,T]$. For an infinitesimal interval $[t, t+\\delta t]$, we can approximate the integral and assume the control $u_s=u_t=u$ is constant.\n$$\nV(t,x) \\approx \\inf_{u} \\mathbb{E}\\left[ (q x^2 + r u^2)\\delta t + V(t+\\delta t, X_{t+\\delta t}) \\,\\mid\\, X_t=x \\right]\n$$\nLet's apply Itô's formula to the process $V(t,X_t)$. Assuming $V \\in C^{1,2}([0,T) \\times \\mathbb{R})$, its differential is:\n$$\n\\mathrm{d}V(t,X_t) = \\left( \\frac{\\partial V}{\\partial t} + (a X_t + b u_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}t + \\sigma \\frac{\\partial V}{\\partial x} \\mathrm{d}W_t\n$$\nIntegrating from $t$ to $t+\\delta t$ and taking the conditional expectation, the stochastic integral term vanishes because $\\mathbb{E}[\\int_{t}^{t+\\delta t} (\\dots) \\mathrm{d}W_s \\,\\mid\\, \\mathcal{F}_t] = 0$.\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\,\\mid\\, X_t=x] - V(t,x) = \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} \\left( \\frac{\\partial V}{\\partial s} + (a X_s + b u_s)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}s \\,\\mid\\, X_t=x \\right]\n$$\nFor small $\\delta t$, this becomes:\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\,\\mid\\, X_t=x] \\approx V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t\n$$\nSubstituting this into the DPP expression:\n$$\nV(t,x) \\approx \\inf_{u} \\left\\{ (q x^2 + r u^2)\\delta t + V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t \\right\\}\n$$\nSubtracting $V(t,x)$ from both sides, dividing by $\\delta t$, and taking the limit as $\\delta t \\to 0$, we obtain the Hamilton-Jacobi-Bellman (HJB) equation:\n$$\n0 = \\inf_{u} \\left\\{ \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\nThis is equivalent to:\n$$\n-\\frac{\\partial V}{\\partial t} = \\inf_{u} \\left\\{ (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\nThe term to be minimized, let's call it the Hamiltonian $\\mathcal{H}(t,x,u,V_x)$, is a quadratic function of $u$. The minimum is found by setting its derivative with respect to $u$ to zero:\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial u} = b \\frac{\\partial V}{\\partial x} + 2ru = 0\n$$\nSince $r>0$ must hold for a meaningful problem (and $r=1$ in the specialized case), we can solve for the optimal feedback control $u^{*}(t,x)$:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\nSubstituting this back into the HJB equation gives the PDE for the value function $V(t,x)$:\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + b\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)\\frac{\\partial V}{\\partial x} + r\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{2r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 + \\frac{rb^2}{4r^2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThis simplifies to the final form of the HJB PDE:\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThis PDE must be solved backwards in time from the terminal condition $V(T,x) = s_f x^2$.\n\n### Verification Theorem\nA verification theorem formally connects the solution of the HJB equation to the value function. In this context, it can be stated as:\n\nLet $W(t,x) \\in C^{1,2}([0,T] \\times \\mathbb{R})$ be a function that satisfies the HJB partial differential equation:\n$$\n\\frac{\\partial W}{\\partial t} + (a x)\\frac{\\partial W}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 W}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial W}{\\partial x}\\right)^2 = 0\n$$\nwith the terminal condition $W(T,x) = s_f x^2$. Let $u^*(t,x) = -\\frac{b}{2r} \\frac{\\partial W}{\\partial x}(t,x)$. If the SDE for the state $X_t^*$ under this control has a unique strong solution for any initial condition $(t,x)$, and if certain integrability conditions on the cost are met, then $W(t,x)$ is the true value function, i.e., $W(t,x) = V(t,x)$, and $u^*$ is the optimal control.\n\n### Specialization and Solution\nWe now specialize to the parameters $a=0$, $b=1$, $\\sigma>0$, $q=1$, $r=1$, and $s_f=1$. The HJB equation becomes:\n$$\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + x^2 - \\frac{1}{4}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThe terminal condition is $V(T,x) = x^2$. Given the quadratic cost and linear dynamics, we propose a quadratic solution of the form:\n$$\nV(t,x) = P(t)x^2 + S(t)\n$$\nThe partial derivatives are:\n$\\frac{\\partial V}{\\partial t} = \\dot{P}(t)x^2 + \\dot{S}(t)$, $\\frac{\\partial V}{\\partial x} = 2P(t)x$, $\\frac{\\partial^2 V}{\\partial x^2} = 2P(t)$.\nSubstituting these into the HJB equation:\n$$\n(\\dot{P}(t)x^2 + \\dot{S}(t)) + \\frac{1}{2}\\sigma^2(2P(t)) + x^2 - \\frac{1}{4}(2P(t)x)^2 = 0\n$$\n$$\n\\dot{P}(t)x^2 + \\dot{S}(t) + \\sigma^2 P(t) + x^2 - P(t)^2 x^2 = 0\n$$\nGrouping terms by powers of $x^2$:\n$$\n(\\dot{P}(t) - P(t)^2 + 1)x^2 + (\\dot{S}(t) + \\sigma^2 P(t)) = 0\n$$\nFor this to hold for all $x \\in \\mathbb{R}$, the coefficients of the powers of $x$ must be zero. This yields a system of two ordinary differential equations (ODEs):\n1.  $\\dot{P}(t) = P(t)^2 - 1$ (Riccati equation)\n2.  $\\dot{S}(t) = -\\sigma^2 P(t)$\n\nThe terminal condition $V(T,x) = 1 \\cdot x^2$ implies $P(T)x^2 + S(T) = x^2$, which gives the terminal conditions for the ODEs: $P(T)=1$ and $S(T)=0$.\n\nLet's solve for $P(t)$. The ODE is $\\frac{\\mathrm{d}P}{\\mathrm{d}t} = P^2-1$. We observe that $P_c=1$ is a constant (equilibrium) solution since $1^2-1=0$. Since the terminal condition is $P(T)=1$, by the uniqueness of solutions to ODEs, the solution must be the constant function $P(t) = 1$ for all $t \\in [0,T]$.\n\nNow, we solve for $S(t)$. The ODE is $\\dot{S}(t) = -\\sigma^2 P(t)$. Substituting $P(t)=1$:\n$$\n\\dot{S}(t) = -\\sigma^2\n$$\nIntegrating with respect to $t$:\n$$\nS(t) = \\int -\\sigma^2 \\mathrm{d}t = -\\sigma^2 t + C\n$$\nUsing the terminal condition $S(T)=0$:\n$$\nS(T) = -\\sigma^2 T + C = 0 \\implies C = \\sigma^2 T\n$$\nThus, the solution for $S(t)$ is $S(t) = \\sigma^2 T - \\sigma^2 t = \\sigma^2 (T-t)$.\n\nThe value function is $V(t,x) = x^2 + \\sigma^2(T-t)$.\n\n### Optimal Control and Closed-Loop Drift\nThe optimal feedback control is given by the general formula we derived:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\nWith $b=1$, $r=1$, and $\\frac{\\partial V}{\\partial x} = 2P(t)x = 2(1)x = 2x$, we get:\n$$\nu^{*}(t,x) = -\\frac{1}{2(1)} (2x) = -x\n$$\nThe closed-loop drift, $\\mu_{\\mathrm{cl}}(t,x)$, is obtained by substituting $u^*(t,x)$ into the drift term of the original SDE, $a x_t + b u_t$. With $a=0$ and $b=1$:\n$$\n\\mu_{\\mathrm{cl}}(t,x) = a x + b u^{*}(t,x) = (0)x + (1)(-x) = -x\n$$\nThe resulting optimal control and closed-loop drift are both $-x$. They are independent of time $t$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -x & -x \\end{pmatrix}}\n$$", "id": "3080756"}]}