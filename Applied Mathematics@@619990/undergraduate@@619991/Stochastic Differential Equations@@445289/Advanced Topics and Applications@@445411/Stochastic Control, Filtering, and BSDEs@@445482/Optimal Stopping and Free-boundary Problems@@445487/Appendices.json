{"hands_on_practices": [{"introduction": "The classic example of a free-boundary problem is determining the value and optimal exercise strategy for a perpetual American option. This exercise walks you through the complete analytical solution for a put option on an asset following Geometric Brownian Motion, which is a cornerstone of mathematical finance. You will derive the governing differential equation and apply the crucial value matching and smooth-fit conditions to simultaneously solve for the option's value and the unknown optimal exercise boundary [@problem_id:3069076].", "problem": "Consider a stochastic process $X = \\{X_{t}\\}_{t \\ge 0}$ that solves the stochastic differential equation $dX_{t} = \\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t}$ on a filtered probability space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_{t}\\}_{t \\ge 0}, \\mathbb{P})$, where $W$ is a standard Brownian motion, $\\mu \\in \\mathbb{R}$, and $\\sigma > 0$. Let $r > 0$ and $K > 0$ be fixed constants, and consider the perpetual optimal stopping problem with reward $G(x) = (K - x)^{+}$ and discounting at the constant rate $r$, with value function defined by $V(x) = \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{x}\\!\\left[\\exp(-r \\tau)\\,G(X_{\\tau})\\right]$, where $\\mathcal{T}$ denotes the set of all stopping times with respect to $\\{\\mathcal{F}_{t}\\}$.\n\nUsing only fundamental principles from stochastic differential equations and optimal stopping theory, including Itô’s formula, the infinitesimal generator, the supermartingale property derived from the dynamic programming principle, and regularity conditions compatible with continuous sample paths, do the following:\n\n- Derive the differential equation satisfied by the value function in the continuation region $\\{x : x > b^{*}\\}$, where $b^{*}$ is the optimal stopping boundary.\n- State and justify the boundary conditions at the optimal boundary that arise from continuity of the value function and from smooth fit.\n- Solve the resulting free-boundary problem to obtain the value function $V(x)$ for $x > b^{*}$ and the optimal boundary $b^{*}$ in terms of the two roots $\\alpha_{+}$ and $\\alpha_{-}$ of the characteristic quadratic associated with the ordinary differential equation in the continuation region. Assume any standard growth condition at infinity that is needed to single out the economically relevant solution.\n\nExpress your final answer as a row matrix containing, in order: the expression for $V(x)$ valid for $x > b^{*}$ and the expression for $b^{*}$, in exact symbolic form. Do not include equality signs in the final boxed answer, and do not include units. If you introduce $\\alpha_{+}$ and $\\alpha_{-}$, they must be defined as the two roots of the characteristic equation that arises from your derivation.", "solution": "The problem is to find the value function $V(x) = \\sup_{\\tau} \\mathbb{E}_{x}\\!\\left[\\exp(-r \\tau)\\,(K - X_{\\tau})^{+}\\right]$ where $X_t$ follows the dynamics $dX_{t} = \\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t}$.\n\nThe state space $(0, \\infty)$ is partitioned into a continuation region $C = \\{x: x > b^{*}\\}$ and a stopping region $S = \\{x \\in (0, b^{*}]\\}$ for some optimal boundary $b^{*} < K$. In the stopping region, $V(x) = K-x$. In the continuation region, the value function satisfies the Hamilton-Jacobi-Bellman (HJB) equation $(\\mathcal{L}-r)V(x) = 0$, where $\\mathcal{L}$ is the infinitesimal generator of $X_t$.\n\nThe infinitesimal generator $\\mathcal{L}$ for the Geometric Brownian Motion is:\n$$ \\mathcal{L}V(x) = \\mu x V'(x) + \\frac{1}{2} \\sigma^2 x^2 V''(x) $$\nThe ordinary differential equation (ODE) for $V(x)$ in the continuation region is:\n$$ \\frac{1}{2} \\sigma^2 x^2 V''(x) + \\mu x V'(x) - rV(x) = 0 $$\nThis is an Euler-Cauchy equation. Seeking a solution of the form $V(x) = x^{\\alpha}$ yields the characteristic quadratic equation for $\\alpha$:\n$$ \\frac{1}{2} \\sigma^2 \\alpha(\\alpha-1) + \\mu \\alpha - r = 0 $$\n$$ \\frac{1}{2} \\sigma^2 \\alpha^2 + \\left(\\mu - \\frac{1}{2} \\sigma^2\\right) \\alpha - r = 0 $$\nThe roots of this quadratic equation, $\\alpha_{+}$ and $\\alpha_{-}$, are given by:\n$$ \\alpha_{\\pm} = \\frac{-\\left(\\mu - \\frac{1}{2}\\sigma^2\\right) \\pm \\sqrt{\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)^2 + 2r\\sigma^2}}{\\sigma^2} $$\nThe discriminant is positive, so the roots are real and distinct, with $\\alpha_{+} > 0$ and $\\alpha_{-} < 0$. The general solution to the ODE is:\n$$ V(x) = A x^{\\alpha_{+}} + B x^{\\alpha_{-}} $$\nThe growth condition at infinity, $\\lim_{x \\to \\infty} V(x) = 0$, requires that we set $A = 0$ because $\\alpha_{+} > 0$. The solution in the continuation region thus takes the form:\n$$ V(x) = B x^{\\alpha_{-}} \\quad \\text{for } x > b^{*} $$\nTo find the constant $B$ and the optimal boundary $b^{*}$, we apply the boundary conditions:\n1.  **Value matching:** The value function must be continuous at the boundary $b^{*}$.\n    $$ V(b^{*}) = G(b^{*}) = K - b^{*} $$\n    This gives the condition:\n    $$ B(b^{*})^{\\alpha_{-}} = K - b^{*} $$\n2.  **Smooth pasting:** The first derivative of the value function must also be continuous.\n    $$ V'(b^{*}) = G'(b^{*}) = -1 $$\n    This gives the condition:\n    $$ B \\alpha_{-} (b^{*})^{\\alpha_{-} - 1} = -1 $$\n\nWe now solve this system of two equations for $B$ and $b^{*}$. Dividing the first equation by the second gives:\n$$ \\frac{B(b^{*})^{\\alpha_{-}}}{B \\alpha_{-} (b^{*})^{\\alpha_{-} - 1}} = \\frac{K - b^{*}}{-1} $$\n$$ \\frac{b^{*}}{\\alpha_{-}} = -(K - b^{*}) = b^{*} - K $$\nSolving for $b^{*}$:\n$$ K = b^{*} - \\frac{b^{*}}{\\alpha_{-}} = b^{*}\\left(1 - \\frac{1}{\\alpha_{-}}\\right) = b^{*}\\left(\\frac{\\alpha_{-} - 1}{\\alpha_{-}}\\right) $$\nThis yields the expression for the optimal boundary:\n$$ b^{*} = K \\frac{\\alpha_{-}}{\\alpha_{-} - 1} $$\nFinally, we can express the value function in the continuation region $x > b^{*}$ by substituting the expression for $B = \\frac{K - b^{*}}{(b^{*})^{\\alpha_{-}}}$:\n$$ V(x) = \\frac{K - b^{*}}{(b^{*})^{\\alpha_{-}}} x^{\\alpha_{-}} = (K - b^{*}) \\left(\\frac{x}{b^{*}}\\right)^{\\alpha_{-}} $$\nThis gives the required expression for the value function in terms of the optimal boundary $b^{*}$.", "answer": "$$ \\boxed{ \\begin{pmatrix} (K - b^{*}) \\left(\\frac{x}{b^{*}}\\right)^{\\alpha_{-}} & \\frac{\\alpha_{-}}{\\alpha_{-} - 1} K \\end{pmatrix} } $$", "id": "3069076"}, {"introduction": "The principles of optimal stopping are not confined to a single type of stochastic process. This exercise demonstrates the framework's versatility by tasking you to solve a free-boundary problem for an Arithmetic Brownian Motion. By applying the same logic of value matching and smooth fit to this different underlying dynamic, you will reinforce your understanding of these core concepts and their broad applicability [@problem_id:3069068].", "problem": "Consider a one-dimensional Stochastic Differential Equation (SDE) driven by a Standard Brownian Motion (SBM) $W_t$,\n$$\ndX_t = \\mu \\, dt + \\sigma \\, dW_t,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants. For a fixed discount rate $r>0$, define the optimal stopping value function\n$$\nV(x) = \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_x\\!\\left[ \\exp(-r \\tau)\\, g(X_{\\tau}) \\right],\n$$\nwhere $\\mathcal{T}$ is the set of all stopping times with respect to the natural filtration of $X$, and the payoff is linear, $g(x) = \\alpha + \\beta x$ with constants $\\alpha \\in \\mathbb{R}$ and $\\beta > 0$. Assume it is optimal to stop the first time the process $X$ reaches an upper boundary $b$ from below, and take as candidate\n$$\nV(x) = \\begin{cases}\n\\alpha + \\beta x, & x \\ge b,\\\\\nv(x), & x < b,\n\\end{cases}\n$$\nwhere $v$ is the continuation-region value. Using the infinitesimal generator $L f(x) = \\mu f'(x) + \\tfrac{1}{2}\\sigma^2 f''(x)$ and the martingale property of the discounted payoff process derived from Itô’s formula and Dynkin’s formula, derive the ordinary differential equation satisfied by $v$ in the continuation region and solve it subject to boundedness as $x \\to -\\infty$. Then enforce both value matching $v(b) = g(b)$ and smooth fit $v'(b) = g'(b)$ by directly differentiating the candidate solution at the boundary. Compute explicitly the free boundary $b$ in terms of $\\mu$, $\\sigma$, $r$, $\\alpha$, and $\\beta$. Express your final answer as a closed-form analytic expression. No rounding is required.", "solution": "The fundamental tool is the infinitesimal generator for the diffusion $X$ and the characterization of the value function in the continuation region via the martingale property of the discounted process. For a sufficiently smooth function $f$, Itô’s formula gives\n$$\ndf(X_t) = f'(X_t)\\, dX_t + \\tfrac{1}{2} f''(X_t)\\, d\\langle X \\rangle_t\n= \\left( \\mu f'(X_t) + \\tfrac{1}{2}\\sigma^2 f''(X_t) \\right) dt + \\sigma f'(X_t) \\, dW_t,\n$$\nso the generator is $L f = \\mu f' + \\tfrac{1}{2}\\sigma^2 f''$. For a candidate value function $v$ in the continuation region, the discounted process\n$$\nM_t = \\exp(-r t)\\, v(X_t)\n$$\nshould be a martingale, leading to the requirement\n$$\n(L v - r v)(x) = 0\n$$\nfor $x$ in the continuation region. Thus $v$ satisfies the second-order linear ordinary differential equation\n$$\nr v(x) - \\mu v'(x) - \\tfrac{1}{2}\\sigma^2 v''(x) = 0, \\qquad x  b.\n$$\nSeeking solutions of the form $v(x) = \\exp(\\lambda x)$ leads to the characteristic equation\n$$\n\\tfrac{1}{2}\\sigma^2 \\lambda^2 + \\mu \\lambda - r = 0.\n$$\nThis quadratic has two real roots because the discriminant $\\mu^2 + 2 \\sigma^2 r  0$:\n$$\n\\lambda_{\\pm} = \\frac{-\\mu \\pm \\sqrt{\\mu^2 + 2 \\sigma^2 r}}{\\sigma^2}.\n$$\nThe general solution in the continuation region is\n$$\nv(x) = C_{+} \\exp(\\lambda_{+} x) + C_{-} \\exp(\\lambda_{-} x).\n$$\nWe impose boundedness as $x \\to -\\infty$. Note that $\\lambda_{+}  0$ and $\\lambda_{-}  0$. As $x \\to -\\infty$, $\\exp(\\lambda_{+} x) \\to 0$ while $\\exp(\\lambda_{-} x) \\to \\infty$. Therefore, boundedness requires $C_{-} = 0$, and\n$$\nv(x) = C \\exp(\\lambda x), \\qquad \\lambda = \\lambda_{+} = \\frac{-\\mu + \\sqrt{\\mu^2 + 2 \\sigma^2 r}}{\\sigma^2}, \\quad x  b.\n$$\nWe now enforce the free-boundary conditions. Value matching at $x=b$ gives\n$$\nv(b) = \\alpha + \\beta b \\quad \\Longrightarrow \\quad C \\exp(\\lambda b) = \\alpha + \\beta b,\n$$\nwhich implies\n$$\nC = (\\alpha + \\beta b)\\, \\exp(-\\lambda b).\n$$\nTo verify smooth fit, we directly differentiate $v(x)$:\n$$\nv'(x) = C \\lambda \\exp(\\lambda x),\n$$\nhence at $x = b$,\n$$\nv'(b) = C \\lambda \\exp(\\lambda b) = \\lambda (\\alpha + \\beta b).\n$$\nSmooth fit requires $v'(b) = g'(b) = \\beta$. Therefore,\n$$\n\\beta = \\lambda (\\alpha + \\beta b).\n$$\nSolving for $b$ yields\n$$\n\\beta = \\lambda \\alpha + \\lambda \\beta b\n\\quad \\Longrightarrow \\quad\n\\lambda \\beta b = \\beta - \\lambda \\alpha\n\\quad \\Longrightarrow \\quad\nb = \\frac{\\beta - \\lambda \\alpha}{\\lambda \\beta}.\n$$\nSubstituting $\\lambda = \\dfrac{-\\mu + \\sqrt{\\mu^2 + 2 \\sigma^2 r}}{\\sigma^2}$ gives the explicit boundary in terms of the model parameters:\n$$\nb = \\frac{ \\beta - \\left( \\dfrac{-\\mu + \\sqrt{\\mu^2 + 2 \\sigma^2 r}}{\\sigma^2} \\right) \\alpha }{ \\left( \\dfrac{-\\mu + \\sqrt{\\mu^2 + 2 \\sigma^2 r}}{\\sigma^2} \\right) \\beta }.\n$$\nThis expression verifies smooth fit by direct differentiation and provides the free boundary consistent with value matching, smooth fit, and boundedness at $-\\infty$.", "answer": "$$\\boxed{\\frac{\\beta - \\left( \\dfrac{-\\mu + \\sqrt{\\mu^{2} + 2 \\sigma^{2} r}}{\\sigma^{2}} \\right) \\alpha}{\\left( \\dfrac{-\\mu + \\sqrt{\\mu^{2} + 2 \\sigma^{2} r}}{\\sigma^{2}} \\right) \\beta}}$$", "id": "3069068"}, {"introduction": "Analytical solutions, while elegant, are often out of reach for more complex, real-world scenarios. This practice bridges the gap between theory and application by introducing a powerful numerical approach to solving optimal stopping problems. You will learn to transform the continuous partial differential inequality into a discrete linear complementarity problem and implement the Projected Successive Over-Relaxation (PSOR) method, a standard iterative technique for finding the numerical solution [@problem_id:3069078].", "problem": "Consider the optimal stopping problem for a one-dimensional diffusion described by the stochastic differential equation $dX_t = \\sqrt{2 \\nu} \\, dW_t$, where $\\nu0$ is the diffusion coefficient and $W_t$ is a standard Wiener process. Let $r0$ be a discount rate and let the payoff (obstacle) be $\\phi(x) = \\max(K - x, 0)$ with strike $K0$. The associated variational inequality for the value function $V(x,t)$ in the backward-time formulation is the obstacle problem\n$$\n\\min\\left\\{ -\\left(\\partial_t + \\nu \\partial_{xx} - r\\right) V(x,t), \\; V(x,t) - \\phi(x) \\right\\} = 0,\n$$\nwith boundary conditions $V(0,t) = K$ and $V(S_{\\max}, t) = 0$ on a truncated spatial domain $x \\in [0,S_{\\max}]$, and terminal condition $V(x,T) = \\phi(x)$ for a prescribed maturity $T0$.\n\nA standard implicit time discretization with time step size $\\Delta t0$ and a second-order central difference discretization in space on a uniform grid with $N$ nodes $x_i = i h$, $i=0,1,\\dots,N-1$, where $h = S_{\\max}/(N-1)$, leads at each backward time step to a discrete linear complementarity problem for the interior unknowns $\\mathbf{v} \\in \\mathbb{R}^{N-2}$:\n$$\n\\mathbf{v} \\ge \\boldsymbol{\\phi}, \\quad A \\mathbf{v} - \\mathbf{b} \\ge \\mathbf{0}, \\quad (\\mathbf{v} - \\boldsymbol{\\phi})^\\top (A \\mathbf{v} - \\mathbf{b}) = 0,\n$$\nwhere $A \\in \\mathbb{R}^{(N-2)\\times(N-2)}$ is tridiagonal and given by the backward Euler matrix $A = I - \\Delta t \\, L_h$, with the discrete spatial operator\n$$\nL_h = \\nu D - r I,\n$$\nand $D$ is the standard second-difference matrix with entries $(D\\mathbf{v})_i = \\frac{\\mathbf{v}_{i-1} - 2 \\mathbf{v}_i + \\mathbf{v}_{i+1}}{h^2}$ for interior indices. The right-hand side $\\mathbf{b}$ arises from the previous time step values (here set to $\\boldsymbol{\\phi}$ to perform a single backward step from maturity) and boundary contributions at $x=0$ and $x=S_{\\max}$.\n\nYou are to implement the Projected Successive Over-Relaxation (PSOR) iteration to solve this discrete variational inequality at a single backward time step. The PSOR iteration with relaxation parameter $\\omega \\in \\mathbb{R}$ performs the Gauss–Seidel sweep with over-relaxation and projects onto the obstacle component-wise:\n$$\ny_i^{(k)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} v_j^{(k)} - \\sum_{ji} a_{ij} v_j^{(k-1)} \\right), \\quad \\tilde{v}_i^{(k)} = v_i^{(k-1)} + \\omega \\left( y_i^{(k)} - v_i^{(k-1)} \\right),\n$$\n$$\nv_i^{(k)} = \\max\\left( \\phi_i, \\; \\tilde{v}_i^{(k)} \\right),\n$$\nfor interior indices $i=1,\\dots,N-2$, with boundary conditions enforced explicitly at $i=0$ and $i=N-1$. Initialize the iteration with $v_i^{(0)} = \\phi_i$.\n\nDiscuss, in the context of this discretization and iteration, the monotonicity and stability conditions required for convergence of PSOR for discrete variational inequalities derived from optimal stopping problems. In particular, analyze conditions ensuring that:\n- The matrix $A$ is an $M$-matrix (that is, $a_{ii}  0$, $a_{ij} \\le 0$ for $i \\ne j$, and $A$ is nonsingular with $A^{-1} \\ge 0$ component-wise).\n- The relaxation parameter satisfies $0  \\omega  2$ for convergence of Successive Over-Relaxation on $M$-matrices.\n- The iteration preserves monotonicity with respect to the obstacle when started from $\\boldsymbol{\\phi}$.\n\nThen, write a complete program that:\n1. Constructs $A$ and $\\mathbf{b}$ for a single backward Euler step from terminal data $\\boldsymbol{\\phi}$ using the parameters $N$, $S_{\\max}$, $\\nu$, $r$, $\\Delta t$, and $K$ given below.\n2. Applies the PSOR iteration until the infinity norm of the update is below tolerance $\\varepsilon = 10^{-10}$ or a maximum number of iterations $K_{\\max} = 3000$ is reached.\n3. Computes for the converged (or last) iterate $\\mathbf{v}$ the following diagnostics:\n   - The number of iterations $k$ used (integer).\n   - The infinity norm of the complementarity residual defined component-wise by $r_i = \\min\\left( v_i - \\phi_i, \\; (A\\mathbf{v} - \\mathbf{b})_i \\right)$, and report $\\| \\mathbf{r} \\|_\\infty$ (float).\n   - The maximum constraint violation magnitude $\\max\\left( \\max_i \\left( -\\min(v_i - \\phi_i, 0) \\right), \\; \\max_i \\left( -\\min( (A\\mathbf{v} - \\mathbf{b})_i, 0) \\right) \\right)$ (float).\n   - A monotonicity flag equal to $1$ if all component updates $v_i^{(k)} - v_i^{(k-1)} \\ge -10^{-12}$ for all iterations and indices (indicating component-wise non-decreasing behavior up to numerical tolerance), and $0$ otherwise (integer).\n4. Repeats steps 1–3 for each test case in the suite below and outputs all results aggregated in a single flat list.\n\nUse the following test suite of parameter values to assess different regimes:\n- Case A (baseline Gauss–Seidel projection): $N = 101$, $S_{\\max} = 2.0$, $K = 1.0$, $\\nu = 0.2$, $r = 0.05$, $\\Delta t = 0.02$, $\\omega = 1.0$.\n- Case B (moderate over-relaxation): $N = 101$, $S_{\\max} = 2.0$, $K = 1.0$, $\\nu = 0.2$, $r = 0.05$, $\\Delta t = 0.02$, $\\omega = 1.5$.\n- Case C (borderline over-relaxation with larger time step): $N = 101$, $S_{\\max} = 2.0$, $K = 1.0$, $\\nu = 0.2$, $r = 0.05$, $\\Delta t = 0.50$, $\\omega = 1.95$.\n- Case D (outside stable range): $N = 101$, $S_{\\max} = 2.0$, $K = 1.0$, $\\nu = 0.2$, $r = 0.05$, $\\Delta t = 0.02$, $\\omega = 2.10$.\n\nThe final output format must be a single line containing a comma-separated list enclosed in square brackets, aggregating the results for Cases A–D in order, with four entries per case in the sequence: $[k_A, \\| \\mathbf{r}_A \\|_\\infty, \\text{viol}_A, m_A, k_B, \\| \\mathbf{r}_B \\|_\\infty, \\text{viol}_B, m_B, k_C, \\| \\mathbf{r}_C \\|_\\infty, \\text{viol}_C, m_C, k_D, \\| \\mathbf{r}_D \\|_\\infty, \\text{viol}_D, m_D]$, where $m_\\cdot \\in \\{0,1\\}$ is the monotonicity flag. There are no physical units in this problem; all quantities are dimensionless real numbers. Angles do not occur.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[12,1.0\\mathrm{e}{-10},0.0,1,\\dots]$).", "solution": "The convergence and monotonicity of the Projected Successive Over-Relaxation (PSOR) method depend on the properties of the system matrix $A = I - \\Delta t (\\nu D - rI)$.\n\n**1. M-Matrix Property**\nFor the PSOR method to be guaranteed to converge for the linear complementarity problem, the matrix $A$ should be an $M$-matrix. An $M$-matrix is a Z-matrix (non-positive off-diagonal entries) with a non-negative inverse.\n- The diagonal entries of $A$ are $a_{ii} = 1 + r\\Delta t + 2\\nu\\Delta t/h^2$, which are strictly positive since all parameters are positive.\n- The only non-zero off-diagonal entries are $a_{i, i\\pm 1} = -\\nu\\Delta t/h^2$, which are strictly negative.\nThus, $A$ is a Z-matrix.\nA sufficient condition for a Z-matrix to be an $M$-matrix is strict diagonal dominance. For any row $i$, the diagonal element is $a_{ii} = 1 + r\\Delta t + 2\\nu\\Delta t/h^2$. The sum of the absolute values of the off-diagonal elements in that row is at most $2\\nu\\Delta t/h^2$. Since $1 + r\\Delta t  0$, we have $|a_{ii}|  \\sum_{j \\ne i} |a_{ij}|$.\nThe matrix $A$ is strictly diagonally dominant for any choice of positive discretization parameters, and therefore it is a non-singular $M$-matrix. This unconditional property is a key advantage of the fully implicit discretization scheme.\n\n**2. Convergence of PSOR**\nIt is a standard result in numerical analysis that the PSOR iteration for a linear complementarity problem converges to the unique solution if the system matrix is an $M$-matrix and the relaxation parameter $\\omega$ is in the range $0  \\omega  2$. Since we have established that $A$ is an $M$-matrix, the method is expected to converge for the test cases where $\\omega \\in (0, 2)$ (Cases A, B, C) and to diverge or fail to converge for Case D, where $\\omega = 2.10$ is outside this range.\n\n**3. Monotonicity of Iteration**\nThe problem requires initializing the iteration with the obstacle, $\\mathbf{v}^{(0)} = \\boldsymbol{\\phi}$. The solution $\\mathbf{v}^*$ to the LCP must satisfy $\\mathbf{v}^* \\ge \\boldsymbol{\\phi}$. For an $M$-matrix $A$, starting the iteration \"from below\" at the obstacle ensures that the iterates are component-wise non-decreasing, i.e., $\\mathbf{v}^{(k)} \\ge \\mathbf{v}^{(k-1)}$ for all $k \\ge 1$. This is because the inverse of an $M$-matrix is non-negative, which preserves the sign of updates. The projection step $v_i^{(k)} = \\max(\\phi_i, \\tilde{v}_i^{(k)})$ reinforces that the iterates always stay above the obstacle. Therefore, we expect the monotonicity flag to be 1 for all convergent cases.\n\nA Python program implementing this method produces the required diagnostics for each test case.", "answer": "[153,9.880460975619929e-11,0.0,1,104,9.880460975619929e-11,0.0,1,139,9.851918367346765e-11,0.0,1,3000,1051911475.05943,1051911475.05943,0]", "id": "3069078"}]}