{"hands_on_practices": [{"introduction": "The Linear-Quadratic (LQ) framework is a cornerstone of optimal control, but its power relies on specific assumptions about the problem's structure. This exercise explores a crucial requirement: the positivity of the control cost weighting. By examining a scenario where this assumption is violated [@problem_id:3077858], you will discover why a seemingly straightforward minimization problem can become unbounded, highlighting the importance of a well-posed cost function in control theory.", "problem": "Consider a scalar Stochastic Differential Equation (SDE) on the finite horizon $[0,1]$ driven by a standard Brownian motion. Let the state process $(x_t)_{t \\in [0,1]}$ satisfy\n$$\n\\mathrm{d}x_t = u_t \\,\\mathrm{d}t + \\mathrm{d}W_t, \\qquad x_0 = 0,\n$$\nwhere $(u_t)_{t \\in [0,1]}$ is a control process. An admissible control is any real-valued, progressively measurable process $(u_t)_{t \\in [0,1]}$ such that\n$$\n\\mathbb{E}\\!\\left[\\int_0^1 |u_t|^2 \\,\\mathrm{d}t\\right]  \\infty.\n$$\nConsider the stochastic linear quadratic regulation problem with running cost matrix coefficients $Q=0$ and $R=-1$, and no terminal cost, so that the performance index is\n$$\nJ(u) = \\mathbb{E}\\!\\left[\\int_0^1 \\left(0 \\cdot x_t^2 + (-1)\\, u_t^2\\right)\\mathrm{d}t\\right] = -\\,\\mathbb{E}\\!\\left[\\int_0^1 u_t^2\\,\\mathrm{d}t\\right].\n$$\nUsing only the foundational definitions of admissible controls and the performance index above, construct an explicit admissible control sequence that demonstrates the cost can be made arbitrarily negative when $R$ is not positive definite. Then determine the infimum\n$$\nJ^{\\star} := \\inf_{u} J(u)\n$$\nover all admissible controls. Express your final answer as a single symbolic expression for $J^{\\star}$ with no units required.", "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- State dynamics: $\\mathrm{d}x_t = u_t \\,\\mathrm{d}t + \\mathrm{d}W_t$\n- Time horizon: $t \\in [0,1]$\n- Initial condition: $x_0 = 0$\n- Driving process: $(W_t)_{t \\in [0,1]}$ is a standard Brownian motion.\n- Control process: $(u_t)_{t \\in [0,1]}$\n- Admissibility condition: $(u_t)$ is a real-valued, progressively measurable process such that $\\mathbb{E}\\!\\left[\\int_0^1 |u_t|^2 \\,\\mathrm{d}t\\right]  \\infty$.\n- Performance index (cost functional): $J(u) = \\mathbb{E}\\!\\left[\\int_0^1 \\left(0 \\cdot x_t^2 + (-1)\\, u_t^2\\right)\\mathrm{d}t\\right] = -\\,\\mathbb{E}\\!\\left[\\int_0^1 u_t^2\\,\\mathrm{d}t\\right]$.\n- Objective: Construct a sequence of admissible controls to show the cost can be arbitrarily negative, and determine the infimum $J^{\\star} := \\inf_{u} J(u)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem describes a stochastic linear quadratic (LQ) regulation problem. The state dynamics are linear, and the cost functional is quadratic in the control. The key feature is that the control-weighting matrix, here a scalar $R$, is given as $R=-1$. Standard LQ theory requires $R$ to be positive definite ($R > 0$) for a well-posed minimization problem leading to a finite minimum cost and a unique optimal control via the Riccati equation. The problem as stated, with $R=-1$, violates this standard assumption. However, this is not a flaw in the problem's formulation. Instead, it defines a specific, atypical scenario and asks for its logical consequences. The question is to determine the infimum of the cost, not to find an optimal control that attains it. The problem is scientifically grounded in control theory, well-posed in its request for an infimum, objective, and internally consistent. It is a valid mathematical problem designed to probe the boundary conditions of LQ theory.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be constructed.\n\nThe objective is to determine the infimum of the performance index $J(u)$ over all admissible controls $u$. The performance index is given by\n$$\nJ(u) = -\\,\\mathbb{E}\\!\\left[\\int_0^1 u_t^2\\,\\mathrm{d}t\\right].\n$$\nThe set of admissible controls consists of all real-valued, progressively measurable processes $(u_t)_{t \\in [0,1]}$ for which the following condition holds:\n$$\n\\mathbb{E}\\!\\left[\\int_0^1 u_t^2 \\,\\mathrm{d}t\\right]  \\infty.\n$$\nThe state equation $\\mathrm{d}x_t = u_t \\,\\mathrm{d}t + \\mathrm{d}W_t$ with $x_0 = 0$ is provided to frame the context as a stochastic control problem. However, since the state-weighting matrix is $Q=0$, the state process $(x_t)$ does not appear in the cost functional. Therefore, the problem reduces to analyzing the cost functional $J(u)$ subject only to the admissibility condition on $u_t$.\n\nThe task of finding $J^{\\star} = \\inf_u J(u)$ is equivalent to finding the infimum of $-\\,\\mathbb{E}\\!\\left[\\int_0^1 u_t^2\\,\\mathrm{d}t\\right]$. This, in turn, is equivalent to solving for $-\\sup_u \\mathbb{E}\\!\\left[\\int_0^1 u_t^2\\,\\mathrm{d}t\\right]$, where the supremum is taken over the set of admissible controls.\n\nTo demonstrate that the cost can be made arbitrarily negative, we will construct a sequence of admissible controls, denoted by $\\{u^{(n)}\\}_{n=1}^{\\infty}$, such that $\\lim_{n \\to \\infty} J(u^{(n)}) = -\\infty$.\n\nConsider the sequence of deterministic control policies defined for each integer $n \\ge 1$ as:\n$$\nu^{(n)}_t = n \\quad \\text{for all } t \\in [0,1].\n$$\nWe must first verify that each $u^{(n)}$ is an admissible control.\n$1$. A deterministic function of time, such as a constant, is progressively measurable with respect to any filtration, including the one generated by the Brownian motion $(W_t)$.\n$2$. We check the integrability condition:\n$$\n\\mathbb{E}\\!\\left[\\int_0^1 \\left(u^{(n)}_t\\right)^2 \\,\\mathrm{d}t\\right] = \\mathbb{E}\\!\\left[\\int_0^1 n^2 \\,\\mathrm{d}t\\right] = \\mathbb{E}[n^2] = n^2.\n$$\nSince $n$ is a finite integer, $n^2$ is a finite real number. Thus, the condition $\\mathbb{E}\\!\\left[\\int_0^1 u_t^2 \\,\\mathrm{d}t\\right]  \\infty$ is satisfied for each $n$.\nTherefore, $u^{(n)}_t = n$ is an admissible control for every $n \\in \\{1, 2, 3, \\dots\\}$.\n\nNext, we evaluate the cost functional $J(u^{(n)})$ for this sequence of controls:\n$$\nJ(u^{(n)}) = -\\,\\mathbb{E}\\!\\left[\\int_0^1 \\left(u^{(n)}_t\\right)^2 \\,\\mathrm{d}t\\right] = -n^2.\n$$\nWe have constructed a sequence of admissible controls $\\{u^{(n)}\\}$ for which the corresponding costs are $J(u^{(n)}) = -n^2$.\nAs $n$ approaches infinity, the cost becomes arbitrarily large in magnitude and negative in sign:\n$$\n\\lim_{n \\to \\infty} J(u^{(n)}) = \\lim_{n \\to \\infty} (-n^2) = -\\infty.\n$$\nThis demonstrates that for any real number $M  0$, we can find an admissible control $u$ such that $J(u)  M$. Specifically, we can choose an integer $n  \\sqrt{-M}$, and the control $u^{(n)}_t = n$ will yield a cost $J(u^{(n)}) = -n^2  M$.\n\nThe infimum of a set of real numbers that is not bounded below is, by definition, $-\\infty$. Since the set of possible cost values $\\{ J(u) \\mid u \\text{ is admissible} \\}$ is not bounded below, its infimum is $-\\infty$.\n\nTherefore, the infimum of the performance index is\n$$\nJ^{\\star} = \\inf_{u} J(u) = -\\infty.\n$$\nThis result highlights why the condition $R0$ is crucial in standard LQ theory for ensuring a well-posed minimization problem with a finite-valued solution. When this condition is violated, as in this case with $R=-1$, the controller is incentivized to use infinitely large control actions to drive the cost to $-\\infty$.", "answer": "$$\n\\boxed{-\\infty}\n$$", "id": "3077858"}, {"introduction": "Even when a control problem is well-posed with a finite minimum cost, an optimal solution may not exist within the class of conventional 'bang-bang' or ordinary controls. This thought-provoking exercise uses a simple deterministic system to demonstrate this phenomenon and introduce the powerful concept of 'relaxed controls'. You will construct a sequence of 'chattering' controls [@problem_id:3077006] to understand how the space of admissible strategies can be expanded to guarantee the existence of an optimum.", "problem": "Consider the controlled stochastic differential equation (in this case, deterministic as no diffusion is present)\n$$\ndX_t \\;=\\; u_t\\,dt,\\qquad X_0 \\;=\\; 0,\\qquad t \\in [0,T],\n$$\nwhere $T0$ is fixed. The set of ordinary admissible controls consists of all measurable processes $u_t$ taking values in the control set $U=\\{-1,1\\}$. The performance criterion is\n$$\nJ(u) \\;=\\; \\mathbb{E}\\!\\left[\\int_0^T |X_t|\\,dt\\right],\n$$\nwhere the expectation is with respect to the underlying probability space (here degenerate since there is no noise). A relaxed control is a progressively measurable process $\\mu_t$ taking values in the set of probability measures on $U$, with the relaxed state dynamics\n$$\ndX_t \\;=\\; \\int_U u\\,\\mu_t(du)\\,dt,\\qquad X_0 \\;=\\; 0.\n$$\nYour tasks are:\n- Using only the definitions above and basic properties of integrals, show that among ordinary admissible controls there is no optimal control minimizing $J(u)$, even though the infimum of $J(u)$ over ordinary admissible controls exists.\n- Exhibit a relaxed admissible control $\\mu_t$ that attains the infimum, thereby providing an optimal relaxed control.\n- Compute the minimal value of the performance criterion under relaxed controls. Your final answer must be a single real number or a closed-form analytic expression with no units required.", "solution": "The problem asks for an analysis of an optimal control problem, demonstrating the non-existence of an optimal control within the class of ordinary admissible controls and the existence of one within the class of relaxed controls.\n\nThe state dynamics are given by the ordinary differential equation:\n$$\ndX_t = u_t dt, \\quad X_0 = 0, \\quad t \\in [0,T]\n$$\nfor a fixed terminal time $T0$. The state at time $t$ is thus $X_t = \\int_0^t u_s ds$.\n\nThe performance criterion to be minimized is:\n$$\nJ(u) = \\mathbb{E}\\left[\\int_0^T |X_t|\\,dt\\right]\n$$\nSince the system dynamics are deterministic (no diffusion term), the trajectory $X_t$ for a given control $u_t$ is a deterministic path. Therefore, the expectation operator is superfluous, and the cost functional simplifies to:\n$$\nJ(u) = \\int_0^T |X_t|\\,dt = \\int_0^T \\left|\\int_0^t u_s ds\\right| dt\n$$\nThe set of ordinary admissible controls, $\\mathcal{U}_{ord}$, consists of all measurable processes $u_t$ taking values in the discrete set $U = \\{-1, 1\\}$.\n\n### Part 1: Non-existence of an Optimal Ordinary Control\n\nFirst, we establish the infimum of the cost $J(u)$ over the set of ordinary admissible controls $\\mathcal{U}_{ord}$. The integrand $|X_t|$ is always non-negative, so $J(u) \\ge 0$ for any control $u$. The infimum is therefore also non-negative.\n\nTo find the value of the infimum, we construct a sequence of \"chattering\" controls, $\\{u_n\\}_{n \\in \\mathbb{N}}$, that cause the state $X_t$ to oscillate rapidly around $0$. For each integer $n \\ge 1$, define the control $u_n(t)$ to alternate between $+1$ and $-1$ on intervals of length $\\epsilon_n = \\frac{T}{2n}$:\n$$\nu_n(t) = \\begin{cases} +1  \\text{if } t \\in [k \\cdot 2\\epsilon_n, (k \\cdot 2 + 1)\\epsilon_n) \\\\ -1  \\text{if } t \\in [(k \\cdot 2 + 1)\\epsilon_n, (k \\cdot 2 + 2)\\epsilon_n) \\end{cases}\n$$\nfor $k = 0, 1, \\dots, n-1$, with the last interval closed at $T$.\n\nLet's analyze the trajectory $X_t^{(n)}$ corresponding to $u_n(t)$.\nFor $t \\in [0, \\epsilon_n]$, $X_t^{(n)} = \\int_0^t 1 ds = t$. The maximum value is $X_{\\epsilon_n}^{(n)} = \\epsilon_n$.\nFor $t \\in [\\epsilon_n, 2\\epsilon_n]$, $X_t^{(n)} = X_{\\epsilon_n}^{(n)} + \\int_{\\epsilon_n}^t (-1) ds = \\epsilon_n - (t - \\epsilon_n) = 2\\epsilon_n - t$. At $t=2\\epsilon_n$, $X_{2\\epsilon_n}^{(n)} = 0$.\nThe trajectory from $t=0$ to $t=2\\epsilon_n$ forms a triangular pulse with height $\\epsilon_n$ and base $2\\epsilon_n$. This pattern repeats $n$ times over the interval $[0, T]$. The maximum value of $|X_t^{(n)}|$ on $[0,T]$ is $\\epsilon_n = \\frac{T}{2n}$.\n\nThe cost for the control $u_n(t)$ is the sum of the areas of these $n$ triangles:\n$$\nJ(u_n) = \\int_0^T |X_t^{(n)}|\\,dt = n \\times \\left(\\text{Area of one triangle}\\right)\n$$\nThe area of one triangle is $\\int_0^{2\\epsilon_n} |X_t^{(n)}|\\,dt = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} (2\\epsilon_n)(\\epsilon_n) = \\epsilon_n^2$.\nSo, the total cost is:\n$$\nJ(u_n) = n \\cdot \\epsilon_n^2 = n \\left(\\frac{T}{2n}\\right)^2 = n \\frac{T^2}{4n^2} = \\frac{T^2}{4n}\n$$\nAs we let $n \\to \\infty$, the cost approaches zero:\n$$\n\\lim_{n \\to \\infty} J(u_n) = \\lim_{n \\to \\infty} \\frac{T^2}{4n} = 0\n$$\nSince we have found a sequence of admissible controls whose cost approaches $0$, and we know $J(u) \\ge 0$, we can conclude that the infimum is $0$:\n$$\n\\inf_{u \\in \\mathcal{U}_{ord}} J(u) = 0\n$$\nNow, we must show that no ordinary admissible control can actually attain this value. For a cost of $0$ to be achieved, we must have $J(u) = \\int_0^T |X_t| dt = 0$.\nThe state $X_t = \\int_0^t u_s ds$ is an absolutely continuous function of $t$, and thus continuous. The function $t \\mapsto |X_t|$ is therefore also continuous and non-negative. For the integral of a non-negative continuous function to be zero, the function itself must be identically zero. That is:\n$$\n|X_t| = 0 \\quad \\implies \\quad X_t = 0 \\quad \\text{for all } t \\in [0, T]\n$$\nIf $X_t = \\int_0^t u_s ds = 0$ for all $t \\in [0,T]$, then its derivative with respect to $t$ must be zero. By the Fundamental Theorem of Calculus for Lebesgue integrals, this implies $\\dot{X}_t = u_t = 0$ for almost every $t \\in [0,T]$.\nHowever, the set of ordinary admissible controls is defined such that $u_t$ must take values in $U = \\{-1, 1\\}$. The value $0$ is not in this set. A measurable function $u_t: [0, T] \\to \\{-1, 1\\}$ cannot be equal to $0$ on a set of positive measure, let alone almost everywhere.\nThis creates a contradiction. Therefore, there exists no ordinary admissible control $u_t$ that can make $X_t$ identically zero, and thus no ordinary control can achieve the infimum cost of $0$. We conclude that an optimal control does not exist in the space of ordinary admissible controls.\n\n### Part 2: Existence of an Optimal Relaxed Control\n\nA relaxed control is a process $\\mu_t$ taking values in the set of probability measures on $U$. The relaxed state dynamics are given by:\n$$\ndX_t = \\left(\\int_U u\\,\\mu_t(du)\\right) dt, \\quad X_0 = 0\n$$\nLet's denote the effective control as $v_t = \\int_U u\\,\\mu_t(du)$. A probability measure $\\mu_t$ on $U=\\{-1,1\\}$ can be written as $\\mu_t = \\alpha_t \\delta_{1} + (1-\\alpha_t)\\delta_{-1}$ for some measurable process $\\alpha_t \\in [0,1]$, where $\\delta_c$ is the Dirac measure at point $c$. The effective control is then:\n$$\nv_t = (+1)\\alpha_t + (-1)(1-\\alpha_t) = 2\\alpha_t - 1\n$$\nAs $\\alpha_t$ ranges over $[0,1]$, $v_t$ ranges over the interval $[-1,1]$. This interval is the convex hull of the original control set $U$, i.e., $v_t \\in \\text{conv}(U) = [-1,1]$. The relaxed control problem is thus equivalent to finding a measurable control function $v_t: [0,T] \\to [-1,1]$ that minimizes $J(v) = \\int_0^T |X_t| dt$ with $X_t = \\int_0^t v_s ds$.\n\nThe infimum of the cost is still $0$. We seek a relaxed control that achieves this value. As established before, a cost of $0$ requires $X_t = 0$ for all $t$, which in turn requires the effective control to be $v_t = 0$ for almost every $t \\in [0,T]$.\nThe control $v_t = 0$ is an admissible relaxed control, since $0 \\in [-1,1]$. With this control, the state trajectory is $X_t = \\int_0^t 0 \\,ds = 0$ for all $t$. The cost is $J(v) = \\int_0^T |0| dt = 0$.\nSince this control achieves the infimum value of $0$, it is an optimal relaxed control.\n\nTo exhibit the optimal relaxed control $\\mu_t$, we must find $\\alpha_t$ such that $v_t = 2\\alpha_t - 1 = 0$. This gives $\\alpha_t = 1/2$. The optimal choice for $\\alpha_t$ is a constant function $\\alpha_t = 1/2$ for all $t \\in [0,T]$.\nThis corresponds to the relaxed control process $\\mu_t^*$ which is constant in time:\n$$\n\\mu_t^* = \\frac{1}{2}\\delta_{1} + \\frac{1}{2}\\delta_{-1}, \\quad \\text{for all } t \\in [0,T]\n$$\nThis control is progressively measurable (as it is constant) and takes values in the set of probability measures on $U$. It is therefore an optimal relaxed control. This control represents an infinitely fast chattering between the values $-1$ and $1$, spending equal time at each.\n\n### Part 3: Minimal Value of the Performance Criterion\n\nThe optimal relaxed control policy $v_t = 0$ for $t \\in [0,T]$ yields the state trajectory $X_t = 0$ for all $t \\in [0,T]$.\nThe minimal value of the performance criterion is the value of $J(u)$ evaluated for this optimal policy:\n$$\n\\min J = J(\\mu_t^*) = \\int_0^T |X_t^*|\\,dt = \\int_0^T |0|\\,dt = 0\n$$\nThe minimal value of the performance criterion is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3077006"}, {"introduction": "With a solid understanding of how control problems are formulated, we can now apply the central tool for solving them: the Hamilton-Jacobi-Bellman (HJB) equation. This comprehensive practice will guide you through deriving the optimal state-feedback law for a continuous-time stochastic system [@problem_id:3077001]. You will solve the associated Riccati equation and compare the results for both standard risk-neutral and more advanced risk-sensitive performance criteria, gaining insight into how risk preference affects control strategy.", "problem": "Consider the scalar controlled Itô stochastic differential equation (SDE)\n$$\n\\mathrm{d}x_{t} = a\\, x_{t}\\, \\mathrm{d}t + b\\, u_{t}\\, \\mathrm{d}t + \\sigma\\, \\mathrm{d}W_{t},\n$$\nwhere $W_{t}$ is a standard Wiener process, and $a$, $b$, and $\\sigma$ are fixed real constants. The control $u_{t}$ is required to be admissible in the following sense: it is progressively measurable with respect to the filtration generated by $W_{t}$, and satisfies\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\gamma t)\\, u_{t}^{2}\\, \\mathrm{d}t\\right]  \\infty\n$$\nfor a fixed discount rate $\\gamma  0$. Take $a = 1$, $b = 1$, $\\sigma = 1$, and $\\gamma = 1$.\n\nCompare the risk-neutral and risk-sensitive optimal state-feedbacks for this system under the following two infinite-horizon discounted performance criteria, with $q = 1$ and $r = 1$:\n\n1. Risk-neutral criterion:\n$$\nJ_{0}(u) = \\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\gamma t)\\, \\big(q\\, x_{t}^{2} + r\\, u_{t}^{2}\\big)\\, \\mathrm{d}t\\right].\n$$\n\n2. Risk-sensitive criterion with parameter $\\theta  0$:\n$$\nJ_{\\theta}(u) = \\frac{1}{\\theta}\\, \\ln\\, \\mathbb{E}\\!\\left[\\exp\\!\\left(\\theta \\int_{0}^{\\infty} \\exp(-\\gamma t)\\, \\big(q\\, x_{t}^{2} + r\\, u_{t}^{2}\\big)\\, \\mathrm{d}t\\right)\\right],\n$$\nwith $\\theta = \\frac{1}{4}$.\n\nStarting from the definitions of admissible controls and the dynamic programming principle for controlled diffusion processes, write down the corresponding Hamilton–Jacobi–Bellman (HJB) equations for the value functions under stationary Markov feedback $u_{t} = -K x_{t}$. Then, using a quadratic ansatz for the value function of the form $V(x) = P x^{2} + C$ with constants $P  0$ and $C \\in \\mathbb{R}$, derive the algebraic equations that determine $P$ in the risk-neutral and risk-sensitive cases and solve them explicitly. Deduce the optimal gains $K_{0}$ and $K_{\\theta}$ in each case.\n\nAssume that $K$ must be such that the closed-loop drift $a - bK$ renders the process mean-square stable so that the control is admissible. Provide your final answer as a row matrix containing the two gains $\\big(K_{0}, K_{\\theta}\\big)$ in exact analytic form. No numerical rounding is required, and no units are involved.", "solution": "We consider the scalar controlled Itô SDE\n$$\n\\mathrm{d}x_{t} = a\\, x_{t}\\, \\mathrm{d}t + b\\, u_{t}\\, \\mathrm{d}t + \\sigma\\, \\mathrm{d}W_{t},\n$$\nwith $a=1$, $b=1$, and $\\sigma=1$. The admissible controls are progressively measurable processes $u_{t}$ with respect to the filtration generated by $W_{t}$ satisfying\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\gamma t)\\, u_{t}^{2}\\, \\mathrm{d}t\\right]  \\infty,\n$$\nwhere $\\gamma=1$. We analyze stationary Markov feedbacks of the form $u_{t} = -K x_{t}$ and seek optimal $K$ under two criteria.\n\nFor controlled diffusions, the dynamic programming principle yields a Hamilton–Jacobi–Bellman (HJB) equation for the value function. For the risk-neutral discounted quadratic cost\n$$\nJ_{0}(u) = \\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\gamma t)\\, \\big(q\\, x_{t}^{2} + r\\, u_{t}^{2}\\big)\\, \\mathrm{d}t\\right],\n$$\nwith $q=r=1$ and $\\gamma=1$, the HJB equation for the value function $V(x)$ reads\n$$\n0 = \\min_{u}\\left\\{q x^{2} + r u^{2} + (a x + b u) V'(x) + \\frac{1}{2}\\sigma^{2} V''(x) - \\gamma V(x)\\right\\}.\n$$\nThis equation is derived from the generator of the controlled diffusion applied to $V$ plus the running cost, balanced by the discount term $-\\gamma V(x)$.\n\nFor the risk-sensitive discounted cost\n$$\nJ_{\\theta}(u) = \\frac{1}{\\theta}\\, \\ln\\, \\mathbb{E}\\!\\left[\\exp\\!\\left(\\theta \\int_{0}^{\\infty} \\exp(-\\gamma t)\\, \\big(q\\, x_{t}^{2} + r\\, u_{t}^{2}\\big)\\, \\mathrm{d}t\\right)\\right],\n$$\nwith $\\theta = \\frac{1}{4}$, the corresponding risk-sensitive HJB equation introduces an additional quadratic gradient term due to the exponential cost transform. A well-tested form for the risk-sensitive HJB for controlled diffusions is\n$$\n0 = \\min_{u}\\left\\{q x^{2} + r u^{2} + (a x + b u) V'(x) + \\frac{1}{2}\\sigma^{2} V''(x) + \\frac{\\theta}{2} \\sigma^{2} \\big(V'(x)\\big)^{2} - \\gamma V(x)\\right\\}.\n$$\n\nWe now adopt the quadratic ansatz\n$$\nV(x) = P x^{2} + C,\n$$\nwith constants $P0$ and $C \\in \\mathbb{R}$. Then\n$$\nV'(x) = 2 P x, \\qquad V''(x) = 2 P.\n$$\nSubstitute the ansatz into the risk-neutral HJB:\n$$\n0 = \\min_{u}\\left\\{x^{2} + u^{2} + (a x + b u) (2 P x) + \\frac{1}{2}\\sigma^{2} (2 P) - \\gamma (P x^{2} + C)\\right\\}.\n$$\nUsing $a=b=\\sigma=\\gamma=1$, this becomes\n$$\n0 = \\min_{u}\\left\\{x^{2} + u^{2} + 2 P a x^{2} + 2 P b u x + \\sigma^{2} P - \\gamma P x^{2} - \\gamma C\\right\\}\n= \\min_{u}\\left\\{\\big(1 + 2 P - \\gamma P\\big) x^{2} + u^{2} + 2 P u x + \\sigma^{2} P - \\gamma C\\right\\}.\n$$\nTo minimize over $u$, treat $x$ as a parameter. The function in $u$ is quadratic, and the minimizer satisfies\n$$\n\\frac{\\partial}{\\partial u}\\left(u^{2} + 2 P u x\\right) = 2 u + 2 P x = 0 \\quad \\Rightarrow \\quad u^{*}(x) = - P x.\n$$\nThus the optimal control is $u_{t}^{*} = -K_{0} x_{t}$ with $K_{0} = P$ for the risk-neutral case. Substituting $u^{*}$ back yields\n$$\nu^{2} + 2 P u x = P^{2} x^{2} - 2 P^{2} x^{2} = - P^{2} x^{2}.\n$$\nTherefore, the HJB reduces to\n$$\n0 = \\left(1 + 2 P - \\gamma P - P^{2}\\right) x^{2} + \\sigma^{2} P - \\gamma C.\n$$\nFor the equality to hold for all $x$, the coefficient of $x^{2}$ must vanish, and the constant term must vanish:\n$$\n1 + 2 P - \\gamma P - P^{2} = 0, \\qquad \\sigma^{2} P - \\gamma C = 0.\n$$\nWith $\\gamma=1, \\sigma=1$, the algebraic Riccati equation for $P$ is\n$$\n1 + 2 P - P - P^{2} = 0 \\quad \\Rightarrow \\quad 1 + P - P^{2} = 0 \\quad \\Rightarrow \\quad P^{2} - P - 1 = 0.\n$$\nThe positive stabilizing solution is\n$$\nP = \\frac{1 + \\sqrt{1 + 4}}{2} = \\frac{1 + \\sqrt{5}}{2}.\n$$\nHence the risk-neutral optimal gain is\n$$\nK_{0} = P = \\frac{1 + \\sqrt{5}}{2}.\n$$\nThe constant $C$ is given by $C = \\sigma^{2} P / \\gamma = P$, but it does not affect the gain.\n\nNext, consider the risk-sensitive HJB with $\\theta = \\frac{1}{4}$:\n$$\n0 = \\min_{u}\\left\\{x^{2} + u^{2} + (a x + b u) (2 P x) + \\frac{1}{2}\\sigma^{2} (2 P) + \\frac{\\theta}{2} \\sigma^{2} (2 P x)^{2} - \\gamma (P x^{2} + C)\\right\\}.\n$$\nUsing $a=b=\\sigma=\\gamma=1$ and simplifying,\n$$\n0 = \\min_{u}\\left\\{\\big(1 + 2 P - \\gamma P\\big) x^{2} + u^{2} + 2 P u x + \\sigma^{2} P + 2 \\theta \\sigma^{2} P^{2} x^{2} - \\gamma C\\right\\}.\n$$\nThe minimization in $u$ is identical and yields $u^{*}(x) = - P x$, so the feedback structure is the same with gain $K_{\\theta} = P$, but the algebraic equation for $P$ is modified by the additional gradient term. Substituting $u^{*}$ back gives\n$$\n0 = \\left(1 + 2 P - \\gamma P - P^{2} + 2 \\theta \\sigma^{2} P^{2}\\right) x^{2} + \\sigma^{2} P - \\gamma C.\n$$\nThus\n$$\n1 + 2 P - \\gamma P - P^{2} + 2 \\theta \\sigma^{2} P^{2} = 0, \\qquad \\sigma^{2} P - \\gamma C = 0.\n$$\nWith $\\gamma=1, \\sigma=1$ and $\\theta = \\frac{1}{4}$, the Riccati equation becomes\n$$\n1 + 2 P - P - P^{2} + 2 \\cdot \\frac{1}{4} \\cdot 1 \\cdot P^{2} = 0 \\quad \\Rightarrow \\quad 1 + P + \\left(-1 + \\frac{1}{2}\\right) P^{2} = 0,\n$$\ni.e.\n$$\n1 + P - \\frac{1}{2} P^{2} = 0 \\quad \\Rightarrow \\quad -\\frac{1}{2} P^{2} + P + 1 = 0.\n$$\nMultiplying by $-2$,\n$$\nP^{2} - 2 P - 2 = 0.\n$$\nSolving,\n$$\nP = \\frac{2 \\pm \\sqrt{4 + 8}}{2} = \\frac{2 \\pm \\sqrt{12}}{2} = \\frac{2 \\pm 2 \\sqrt{3}}{2} = 1 \\pm \\sqrt{3}.\n$$\nWe select the positive stabilizing solution $P = 1 + \\sqrt{3}$ so that the closed-loop drift $a - b K_{\\theta} = 1 - P$ is negative, ensuring stability and admissibility. Therefore,\n$$\nK_{\\theta} = P = 1 + \\sqrt{3}.\n$$\n\nAdmissibility verification: Under $u_{t} = -K x_{t}$, the closed-loop SDE becomes an Ornstein–Uhlenbeck process\n$$\n\\mathrm{d}x_{t} = (a - b K)\\, x_{t}\\, \\mathrm{d}t + \\sigma\\, \\mathrm{d}W_{t}.\n$$\nWith $a=1$, $b=1$, and the computed gains $K_{0} = \\frac{1 + \\sqrt{5}}{2}$ and $K_{\\theta} = 1 + \\sqrt{3}$, both satisfy $a - b K  0$, so the process is mean-square stable. Consequently,\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\gamma t)\\, u_{t}^{2}\\, \\mathrm{d}t\\right]  \\infty\n$$\nholds, and the controls are admissible.\n\nIn summary, the two optimal gains are\n$$\nK_{0} = \\frac{1 + \\sqrt{5}}{2}, \\qquad K_{\\theta} = 1 + \\sqrt{3}.\n$$\nAs expected, the risk-sensitive gain is larger than the risk-neutral gain in this example, reflecting increased caution in the presence of process noise when optimizing the exponential-of-integral criterion.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1 + \\sqrt{5}}{2}  1 + \\sqrt{3}\\end{pmatrix}}$$", "id": "3077001"}]}