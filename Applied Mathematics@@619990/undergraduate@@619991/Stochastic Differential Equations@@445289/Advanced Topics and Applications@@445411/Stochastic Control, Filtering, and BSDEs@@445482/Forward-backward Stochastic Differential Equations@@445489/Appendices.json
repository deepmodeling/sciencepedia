{"hands_on_practices": [{"introduction": "While many forward-backward systems are too complex to solve with pen and paper, linear FBSDEs offer a fantastic opportunity to build intuition and practice fundamental techniques. This exercise guides you through finding an explicit, closed-form solution to a decoupled linear system. Mastering this process is crucial as it involves core concepts like Girsanov's theorem for changing the probability measure and the use of an integrating factor to solve the resulting backward equation, both of which are powerful tools in stochastic analysis [@problem_id:3054739].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$, with a fixed horizon $T0$. The forward component of a decoupled Forward-Backward Stochastic Differential Equation (FBSDE, Forward-Backward Stochastic Differential Equation) is given by the stochastic differential equation\n$$\ndX_t=\\mu\\,dt+\\sigma\\,dW_t,\\qquad X_0=x_0,\n$$\nwhere $\\mu\\in\\mathbb{R}$ and $\\sigma0$ are constants, and $x_0\\in\\mathbb{R}$ is deterministic. The backward component $(Y,Z)$ satisfies\n$$\ndY_t=-f(t,X_t,Y_t,Z_t)\\,dt+Z_t\\,dW_t,\\qquad Y_T=g(X_T),\n$$\nwith the driver and terminal function specified by\n$$\nf(t,x,y,z)=\\alpha\\,y+\\beta\\,z+\\gamma,\\qquad g(x)=\\lambda\\,x+\\delta,\n$$\nwhere $\\alpha\\in\\mathbb{R}\\setminus\\{0\\}$, $\\beta,\\gamma,\\lambda,\\delta\\in\\mathbb{R}$ are constants. Assume all usual conditions (including integrability and the Novikov condition for the constant $\\beta$) ensuring existence and uniqueness of a square-integrable solution $(Y,Z)$.\n\nStarting from the fundamental definitions of Itô processes and the backward stochastic differential equation, derive an explicit closed-form expression for $Y_0$ in terms of $x_0$, $T$, and the constants $\\mu$, $\\sigma$, $\\alpha$, $\\beta$, $\\gamma$, $\\lambda$, and $\\delta$. Your final answer must be a single closed-form analytic expression. No rounding is required and no physical units are involved.", "solution": "The problem provides a decoupled forward-backward stochastic differential equation (FBSDE) system and asks for the value of the backward process at time $t=0$, denoted as $Y_0$. The system is defined on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ with a standard one-dimensional Brownian motion $W_t$.\n\nThe forward process $X_t$ is an arithmetic Brownian motion given by the stochastic differential equation (SDE):\n$$dX_t = \\mu\\,dt + \\sigma\\,dW_t, \\quad X_0 = x_0$$\nwhere $\\mu \\in \\mathbb{R}$, $\\sigma  0$, and $x_0 \\in \\mathbb{R}$ are constants.\n\nThe backward process $(Y_t, Z_t)$ is defined by the backward SDE (BSDE):\n$$dY_t = -f(t,X_t,Y_t,Z_t)\\,dt + Z_t\\,dW_t, \\quad Y_T = g(X_T)$$\nThe driver function $f$ and terminal function $g$ are linear:\n$$f(t,x,y,z) = \\alpha y + \\beta z + \\gamma$$\n$$g(x) = \\lambda x + \\delta$$\nwhere $\\alpha \\in \\mathbb{R}\\setminus\\{0\\}$, and $\\beta, \\gamma, \\lambda, \\delta \\in \\mathbb{R}$ are constants. The problem assumes conditions for the existence of a unique square-integrable solution pair $(Y_t, Z_t)$.\n\nSubstituting the specific form of the driver $f$ into the BSDE yields:\n$$dY_t = -(\\alpha Y_t + \\beta Z_t + \\gamma)\\,dt + Z_t\\,dW_t$$\nThis is a linear BSDE. A standard method to solve this is to use a change of measure to eliminate the term involving $Z_t$ in the drift. We define a new probability measure $\\mathbb{Q}$ equivalent to $\\mathbb{P}$. Let $M_t = \\int_0^t \\beta \\, dW_s = \\beta W_t$. The Doléans-Dade exponential of $M_t$ is $\\mathcal{E}(M)_t = \\exp(\\beta W_t - \\frac{1}{2}\\beta^2 t)$. We define the Radon-Nikodym derivative as $\\frac{d\\mathbb{Q}}{d\\mathbb{P}}|_{\\mathcal{F}_T} = \\mathcal{E}(M)_T$. The assumption that the Novikov condition is satisfied ensures that $\\mathcal{E}(M)_t$ is a martingale and $\\mathbb{E}[\\mathcal{E}(M)_T] = 1$.\n\nBy Girsanov's theorem, the process $\\widetilde{W}_t$ defined by\n$$\\widetilde{W}_t = W_t - \\int_0^t \\beta \\, ds = W_t - \\beta t$$\nis a standard Brownian motion under the measure $\\mathbb{Q}$. We can express $dW_t$ as $dW_t = d\\widetilde{W}_t + \\beta \\, dt$.\n\nNow, we rewrite both SDEs under the measure $\\mathbb{Q}$.\nThe forward SDE for $X_t$ becomes:\n$$dX_t = \\mu\\,dt + \\sigma(d\\widetilde{W}_t + \\beta\\,dt) = (\\mu + \\sigma\\beta)\\,dt + \\sigma\\,d\\widetilde{W}_t$$\nThe backward SDE for $Y_t$ becomes:\n$$dY_t = -(\\alpha Y_t + \\beta Z_t + \\gamma)\\,dt + Z_t(d\\widetilde{W}_t + \\beta\\,dt)$$\n$$dY_t = -(\\alpha Y_t + \\gamma)\\,dt - \\beta Z_t\\,dt + Z_t\\,\\beta\\,dt + Z_t\\,d\\widetilde{W}_t$$\n$$dY_t = -(\\alpha Y_t + \\gamma)\\,dt + Z_t\\,d\\widetilde{W}_t$$\nThis BSDE under $\\mathbb{Q}$ is simpler as its drift is independent of $Z_t$. To solve it, we use an integrating factor. Let's consider the process $e^{\\alpha t}Y_t$. Applying Itô's product rule:\n$$d(e^{\\alpha t}Y_t) = (\\alpha e^{\\alpha t}Y_t)\\,dt + e^{\\alpha t}\\,dY_t$$\nSubstituting the expression for $dY_t$:\n$$d(e^{\\alpha t}Y_t) = \\alpha e^{\\alpha t}Y_t\\,dt + e^{\\alpha t}(-(\\alpha Y_t + \\gamma)\\,dt + Z_t\\,d\\widetilde{W}_t)$$\n$$d(e^{\\alpha t}Y_t) = \\alpha e^{\\alpha t}Y_t\\,dt - \\alpha e^{\\alpha t}Y_t\\,dt - \\gamma e^{\\alpha t}\\,dt + e^{\\alpha t}Z_t\\,d\\widetilde{W}_t$$\n$$d(e^{\\alpha t}Y_t) = -\\gamma e^{\\alpha t}\\,dt + e^{\\alpha t}Z_t\\,d\\widetilde{W}_t$$\nIntegrating this equation from an arbitrary time $t \\in [0,T]$ to $T$:\n$$e^{\\alpha T}Y_T - e^{\\alpha t}Y_t = \\int_t^T -\\gamma e^{\\alpha s}\\,ds + \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$$\nWe can solve for $e^{\\alpha t}Y_t$:\n$$e^{\\alpha t}Y_t = e^{\\alpha T}Y_T + \\gamma \\int_t^T e^{\\alpha s}\\,ds - \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$$\nTaking the conditional expectation under $\\mathbb{Q}$ with respect to the filtration $\\mathcal{F}_t$:\n$$\\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha t}Y_t | \\mathcal{F}_t] = \\mathbb{E}_{\\mathbb{Q}}\\left[e^{\\alpha T}Y_T + \\gamma \\int_t^T e^{\\alpha s}\\,ds - \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s \\bigg| \\mathcal{F}_t\\right]$$\nSince $Y_t$ is $\\mathcal{F}_t$-measurable, $\\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha t}Y_t | \\mathcal{F}_t] = e^{\\alpha t}Y_t$. The stochastic integral $\\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$ is a $\\mathbb{Q}$-martingale starting from $0$ at time $t$, so its conditional expectation is zero. The term involving the integral of $\\gamma e^{\\alpha s}$ is deterministic.\n$$e^{\\alpha t}Y_t = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_t] + \\gamma \\int_t^T e^{\\alpha s}\\,ds$$\nLet's evaluate the deterministic integral:\n$$\\int_t^T e^{\\alpha s}\\,ds = \\left[\\frac{e^{\\alpha s}}{\\alpha}\\right]_t^T = \\frac{1}{\\alpha}(e^{\\alpha T} - e^{\\alpha t})$$\nSubstituting this back and solving for $Y_t$:\n$$Y_t = e^{-\\alpha t} \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_t] + \\frac{\\gamma}{\\alpha}e^{-\\alpha t}(e^{\\alpha T} - e^{\\alpha t})$$\n$$Y_t = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha(T-t)}Y_T | \\mathcal{F}_t] + \\frac{\\gamma}{\\alpha}(e^{\\alpha(T-t)} - 1)$$\nWe are asked to find $Y_0$. Setting $t=0$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_0] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nSince $X_0=x_0$ is deterministic, the filtration $\\mathcal{F}_0$ is trivial. Thus, the conditional expectation is just the unconditional expectation under $\\mathbb{Q}$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nNow substitute the terminal condition $Y_T = g(X_T) = \\lambda X_T + \\delta$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}(\\lambda X_T + \\delta)] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\n$$Y_0 = \\lambda e^{\\alpha T}\\mathbb{E}_{\\mathbb{Q}}[X_T] + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nThe final step is to compute $\\mathbb{E}_{\\mathbb{Q}}[X_T]$. The dynamics of $X_t$ under $\\mathbb{Q}$ are $dX_t = (\\mu + \\sigma\\beta)\\,dt + \\sigma\\,d\\widetilde{W}_t$. Integrating from $0$ to $T$:\n$$X_T = X_0 + \\int_0^T (\\mu + \\sigma\\beta)\\,ds + \\int_0^T \\sigma\\,d\\widetilde{W}_s = x_0 + (\\mu + \\sigma\\beta)T + \\sigma\\widetilde{W}_T$$\nTaking the expectation under $\\mathbb{Q}$ and using $\\mathbb{E}_{\\mathbb{Q}}[\\widetilde{W}_T]=0$:\n$$\\mathbb{E}_{\\mathbb{Q}}[X_T] = x_0 + (\\mu + \\sigma\\beta)T$$\nFinally, we substitute this into the expression for $Y_0$:\n$$Y_0 = \\lambda e^{\\alpha T}(x_0 + (\\mu + \\sigma\\beta)T) + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nThis can be expanded and rearranged to provide the final closed-form expression:\n$$Y_0 = \\lambda x_0 e^{\\alpha T} + \\lambda(\\mu + \\sigma\\beta)T e^{\\alpha T} + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}e^{\\alpha T} - \\frac{\\gamma}{\\alpha}$$\n$$Y_0 = e^{\\alpha T}\\left(\\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta + \\frac{\\gamma}{\\alpha}\\right) - \\frac{\\gamma}{\\alpha}$$\nUsing the `\\exp` function syntax as per the output rules, this is:\n$$Y_0 = \\exp(\\alpha T)\\left(\\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta\\right) + \\frac{\\gamma}{\\alpha}(\\exp(\\alpha T) - 1)$$", "answer": "$$\\boxed{\\exp(\\alpha T)\\left( \\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta \\right) + \\frac{\\gamma}{\\alpha}(\\exp(\\alpha T) - 1)}$$", "id": "3054739"}, {"introduction": "Analytical solutions are the exception, not the rule; to tackle the nonlinear and complex FBSDEs that arise in finance and control theory, we must rely on numerical methods. This exercise introduces one of the most common approaches: a time-stepping algorithm that computes the solution backward in time, from the terminal condition to the initial time [@problem_id:3054605]. This practice is essential for bridging the gap between continuous-time theory and the discrete-time algorithms used in practical applications.", "problem": "Consider a one-dimensional forward-backward stochastic differential equation on the time interval $[0,T]$ driven by a standard Brownian motion, with the forward component given by\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t, \\quad X_0 = x_0,\n$$\nand the backward component given in integral form by\n$$\nY_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s,\n$$\nwhere $b$, $\\sigma$, $f$, and $g$ are suitably regular functions and $(W_t)_{t\\in[0,T]}$ is a standard Brownian motion. Let a uniform time grid be defined by $0=t_0t_1\\cdotst_N=T$ with step size $h=T/N$, and denote Brownian increments by $\\Delta W_i = W_{t_{i+1}}-W_{t_i}$ and the $\\sigma$-algebra at time $t_i$ by $\\mathcal{F}_{t_i}$. You are to identify a scientifically consistent time discretization scheme in which the forward process $X$ is approximated by the Euler–Maruyama method, while the backward pair $(Y,Z)$ is approximated by a backward recursion using conditional expectations. Select the option that correctly specifies such a scheme.\n\nA. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{h}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_i^h\\right)\\,h \\,\\big|\\, \\mathcal{F}_{t_i} \\right],\n$$\nfor $i=N-1,\\dots,0$.\n\nB. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{\\sqrt{h}}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_i^h\\right)\\,h,\n$$\nfor $i=N-1,\\dots,0$.\n\nC. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\mathbb{E}\\!\\left[\\,Z_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right] + f\\!\\left(t_i,X_i^h,Y_i^h,Z_i^h\\right)\\,h,\n$$\nfor $i=N-1,\\dots,0$.\n\nD. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{h}\\,\\mathbb{E}\\!\\left[\\,\\big(Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_{i+1}^h\\right)\\,h\\big)\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right],\n$$\nfor $i=N-1,\\dots,0$.\n\nE. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward via a partial differential equation ansatz:\n$$\nY_i^h = u(t_i,X_i^h), \\quad Z_i^h = \\partial_x u(t_i,X_i^h)\\,\\sigma(t_i,X_i^h),\n$$\nwhere $u$ solves a semilinear parabolic partial differential equation with terminal condition $u(T,x)=g(x)$.", "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\nThe problem statement provides the following information:\n- A one-dimensional forward-backward stochastic differential equation (FBSDE) on the time interval $[0,T]$.\n- The forward component is given by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t, \\quad X_0 = x_0\n$$\n- The backward component is given by the backward stochastic differential equation (BSDE) in integral form:\n$$\nY_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s\n$$\n- The functions $b$, $\\sigma$, $f$, and $g$ are \"suitably regular\".\n- $(W_t)_{t\\in[0,T]}$ is a standard one-dimensional Brownian motion.\n- A uniform time grid is defined: $0=t_0t_1\\cdotst_N=T$ with step size $h=T/N$.\n- Notation for Brownian increments: $\\Delta W_i = W_{t_{i+1}}-W_{t_i}$.\n- Notation for the filtration: $\\mathcal{F}_{t_i}$ is the $\\sigma$-algebra at time $t_i$.\n- The task is to identify a time discretization scheme with two properties:\n    1.  The forward process $X$ is approximated by the Euler–Maruyama method.\n    2.  The backward pair $(Y,Z)$ is approximated by a backward recursion using conditional expectations.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n\n- **Scientifically Grounded**: The problem is set within the well-established mathematical framework of stochastic differential equations, specifically forward-backward systems. The methods mentioned—Euler-Maruyama and backward recursions involving conditional expectations—are standard topics in the numerical analysis of SDEs and BSDEs.\n- **Well-Posed**: The problem asks to identify the correct discretization scheme from a given list. This is a well-defined task in numerical mathematics. The phrase \"suitably regular functions\" is standard shorthand in the field, implying conditions (e.g., Lipschitz continuity, polynomial growth) that ensure the existence and uniqueness of a solution to the continuous FBSDE, making the underlying system non-problematic.\n- **Objective**: The problem is stated using precise mathematical language and notation. It is free of ambiguity, subjectivity, or opinion.\n\nThe problem does not exhibit any of the following flaws:\n1.  **Scientific or Factual Unsoundness**: The formulation is standard and correct.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly about the discretization of FBSDEs, a core topic in its field.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained for the task of identifying a consistent numerical scheme.\n4.  **Unrealistic or Infeasible**: Not applicable, as it is a purely mathematical problem.\n5.  **Ill-Posed or Poorly Structured**: The question is clear and has a definite answer based on standard derivations.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The derivation of the correct scheme, especially for the backward component, requires non-trivial application of stochastic calculus principles.\n7.  **Outside Scientific Verifiability**: The correctness of a numerical scheme is a mathematically verifiable fact.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will now proceed.\n\n## DERIVATION  SOLUTION\n\nThe problem requires a numerical scheme for a coupled FBSDE system. The scheme must use the Euler-Maruyama method for the forward SDE and a backward recursion based on conditional expectations for the BSDE.\n\n### Forward Process Discretization\nThe problem specifies the Euler-Maruyama method for the forward SDE:\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t, \\quad X_0 = x_0\n$$\nIntegrating from $t_i$ to $t_{i+1}$ yields:\n$$\nX_{t_{i+1}} = X_{t_i} + \\int_{t_i}^{t_{i+1}} b(s,X_s)\\,\\mathrm{d}s + \\int_{t_i}^{t_{i+1}} \\sigma(s,X_s)\\,\\mathrm{d}W_s\n$$\nThe Euler-Maruyama approximation uses a left-point rule for the integrands:\n$$\n\\int_{t_i}^{t_{i+1}} b(s,X_s)\\,\\mathrm{d}s \\approx b(t_i,X_{t_i}) (t_{i+1}-t_i) = b(t_i,X_{t_i}) h\n$$\n$$\n\\int_{t_i}^{t_{i+1}} \\sigma(s,X_s)\\,\\mathrm{d}W_s \\approx \\sigma(t_i,X_{t_i}) (W_{t_{i+1}}-W_{t_i}) = \\sigma(t_i,X_{t_i}) \\Delta W_i\n$$\nDenoting the numerical approximation of $X_{t_i}$ by $X_i^h$, we obtain the scheme:\n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i, \\quad X_0^h = x_0\n$$\nThis part of the scheme is correctly stated in options A, B, C, D, and E.\n\n### Backward Process Discretization\nThe BSDE is given in integral form:\n$$\nY_t = Y_T + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s\n$$\nwhere $Y_T = g(X_T)$. Let us consider this equation between times $t_i$ and $t_{i+1}$:\n$$\nY_{t_i} = Y_{t_{i+1}} + \\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s\n$$\nThe numerical scheme must be a backward recursion, calculating $(Y_i^h, Z_i^h)$ based on information available at or after time $t_i$. The terminal condition is straightforwardly discretized as $Y_N^h = g(X_N^h)$.\n\n**Derivation of the scheme for $Y_i^h$:**\nTaking the conditional expectation with respect to $\\mathcal{F}_{t_i}$ on both sides of the equation for $Y_{t_i}$:\n$$\n\\mathbb{E}[Y_{t_i} | \\mathcal{F}_{t_i}] = \\mathbb{E}[Y_{t_{i+1}} | \\mathcal{F}_{t_i}] + \\mathbb{E}\\left[\\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s \\bigg| \\mathcal{F}_{t_i}\\right] - \\mathbb{E}\\left[\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s \\bigg| \\mathcal{F}_{t_i}\\right]\n$$\nSince $Y_{t_i}$ is $\\mathcal{F}_{t_i}$-measurable, $\\mathbb{E}[Y_{t_i} | \\mathcal{F}_{t_i}] = Y_{t_i}$. The expectation of the Itô integral is zero because $Z_s$ is adapted. Thus:\n$$\nY_{t_i} = \\mathbb{E}[Y_{t_{i+1}} | \\mathcal{F}_{t_i}] + \\mathbb{E}\\left[\\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s \\bigg| \\mathcal{F}_{t_i}\\right]\n$$\nTo create a numerical scheme, we approximate the integral. A first-order explicit scheme approximates the integral as $h$ times the integrand evaluated at time $t_i$, using known or previously computed values for the arguments of $f$. A common explicit choice is to approximate $(s, X_s, Y_s, Z_s)$ by $(t_i, X_{t_i}, Y_{t_{i+1}}, Z_{t_i})$. The use of $Y_{t_{i+1}}$ is a standard technique to avoid implicit equations. This gives:\n$$\n\\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s \\approx f(t_i,X_{t_i},Y_{t_{i+1}},Z_{t_i})h\n$$\nSubstituting this into the equation for $Y_{t_i}$ and using discrete notation yields:\n$$\nY_i^h \\approx \\mathbb{E}[Y_{i+1}^h | \\mathcal{F}_{t_i}] + \\mathbb{E}[f(t_i,X_i^h,Y_{i+1}^h,Z_i^h)h | \\mathcal{F}_{t_i}]\n$$\nThis can be written as a single conditional expectation:\n$$\nY_i^h = \\mathbb{E}\\left[Y_{i+1}^h + f(t_i,X_i^h,Y_{i+1}^h,Z_i^h)h \\mid \\mathcal{F}_{t_i}\\right]\n$$\n\n**Derivation of the scheme for $Z_i^h$:**\nTo find an expression for $Z_i^h$, we multiply the discretized BSDE by $\\Delta W_i$:\n$$\n(Y_{t_{i+1}} - Y_{t_i})\\Delta W_i \\approx \\left(-\\int_{t_i}^{t_{i+1}} f\\,\\mathrm{d}s + \\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s\\right)\\Delta W_i\n$$\nApproximating $\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s \\approx Z_{t_i}\\Delta W_i$ and taking conditional expectation w.r.t. $\\mathcal{F}_{t_i}$:\n$$\n\\mathbb{E}[(Y_{t_{i+1}} - Y_{t_i})\\Delta W_i | \\mathcal{F}_{t_i}] \\approx -\\mathbb{E}\\left[\\left(\\int_{t_i}^{t_{i+1}} f\\,\\mathrm{d}s\\right)\\Delta W_i \\bigg| \\mathcal{F}_{t_i}\\right] + \\mathbb{E}[Z_{t_i}(\\Delta W_i)^2 | \\mathcal{F}_{t_i}]\n$$\nThe left side is $\\mathbb{E}[Y_{t_{i+1}}\\Delta W_i | \\mathcal{F}_{t_i}] - Y_{t_i}\\mathbb{E}[\\Delta W_i | \\mathcal{F}_{t_i}] = \\mathbb{E}[Y_{t_{i+1}}\\Delta W_i | \\mathcal{F}_{t_i}]$.\nOn the right side, $\\mathbb{E}[Z_{t_i}(\\Delta W_i)^2 | \\mathcal{F}_{t_i}] = Z_{t_i}\\mathbb{E}[(\\Delta W_i)^2 | \\mathcal{F}_{t_i}] = Z_{t_i}h$. The term involving $f$ is of order $\\mathcal{O}(h^{3/2})$ and can be neglected in a first-order scheme. Thus, we have:\n$$\n\\mathbb{E}[Y_{t_{i+1}}\\Delta W_i | \\mathcal{F}_{t_i}] \\approx Z_{t_i}h\n$$\nleading to the discretization for $Z_i^h$:\n$$\nZ_i^h = \\frac{1}{h}\\mathbb{E}[Y_{i+1}^h \\Delta W_i | \\mathcal{F}_{t_i}]\n$$\nThis relation is a direct consequence of the martingale representation theorem, where $Z$ is related to the volatility of the martingale part of $Y$.\n\n### Option-by-Option Analysis\n\n**A. Forward Euler–Maruyama for $X$: Correct.**\n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\n**Backward recursion: Correct.**\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{h}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_i^h\\right)\\,h \\,\\big|\\, \\mathcal{F}_{t_i} \\right].\n$$\nThe formula for $Y_N^h$ is the correct terminal condition. The formulas for $Z_i^h$ and $Y_i^h$ match our derivations for a standard explicit scheme. This scheme is computationally feasible: at each step $i$ (from $N-1$ down to $0$), $Y_{i+1}^h$ is known from the previous step, so one can compute the conditional expectation for $Z_i^h$, then $Z_i^h$, and finally the conditional expectation for $Y_i^h$ to find $Y_i^h$. This is a consistent and scientifically sound scheme.\n**Verdict: Correct.**\n\n**B. Forward Euler–Maruyama for $X$: Correct.**\n**Backward recursion: Incorrect.**\nThe formula $Z_i^h = \\frac{1}{\\sqrt{h}}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right]$ has an incorrect scaling factor of $1/\\sqrt{h}$. The variance of $\\Delta W_i$ is $h$, not $\\sqrt{h}$, so the correct factor is $1/h$. Additionally, the formula $Y_i^h = Y_{i+1}^h + f(\\dots)h$ incorrectly omits the conditional expectation $\\mathbb{E}[\\cdot|\\mathcal{F}_{t_i}]$. Since $Y_{i+1}^h$ is not $\\mathcal{F}_{t_i}$-measurable, it cannot be pulled out of the expectation.\n**Verdict: Incorrect.**\n\n**C. Forward Euler–Maruyama for $X$: Correct.**\n**Backward recursion: Incorrect.**\nThe update rule $Z_i^h = \\mathbb{E}\\!\\left[\\,Z_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right]$ is incorrect. This would imply that the process $Z^h$ is a martingale, which is not true in general. The process $Z$ is determined by the martingale representation of $Y$, not by a simple martingale property of its own. The formula for $Y_i^h$ represents an implicit scheme, which is valid in principle, but needs to be paired with a correct formula for $Z_i^h$. The one provided is wrong.\n**Verdict: Incorrect.**\n\n**D. Forward Euler–Maruyama for $X$: Correct.**\n**Backward recursion: Incorrect.**\nThe formula for $Y_i^h$, given as $Y_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right]$, is incorrect because it omits the driver term involving $f$. This would only be correct if $f \\equiv 0$, which is not assumed. The BSDE would be a simple martingale if $f=0$. The proposed formula for $Z_i^h$ is a more accurate one than in option A, but the scheme is invalidated by the incorrect formula for $Y_i^h$.\n**Verdict: Incorrect.**\n\n**E. Forward Euler–Maruyama for $X$: Correct.**\n**Backward recursion: Methodology mismatched.**\nThis option describes an alternative, valid method for solving FBSDEs via their connection to semi-linear parabolic partial differential equations (PDEs), known as the four-step scheme or Feynman-Kac representation. The formulas $Y_i^h = u(t_i,X_i^h)$ and $Z_i^h = \\partial_x u(t_i,X_i^h)\\,\\sigma(t_i,X_i^h)$ are correct under this PDE-based approach. However, the problem explicitly asks for a scheme where the backward pair is approximated by a **\"backward recursion using conditional expectations\"**. The PDE method does not fit this description. It requires solving a PDE, not performing a backward recursion based on conditional expectations in the time-discrete domain. Therefore, it is not the answer to the question as posed.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3054605"}, {"introduction": "A careful look at the numerical scheme from the previous practice reveals a subtle challenge: how do we estimate the control process $Z_{t_k}$ at each step? This exercise delves into the theoretical underpinnings of a widely used technique that connects $Z_{t_k}$ to the correlation between the future value $Y_{t_{k+1}}$ and the Brownian increment $\\Delta W_k$ [@problem_id:3054590]. Understanding this justification, which relies on the Itô isometry, is key to appreciating the elegance and logic behind modern BSDE solvers.", "problem": "Consider a Markovian forward-backward stochastic differential equation (FBSDE) on a filtered probability space with a standard Brownian motion $W$, where the forward component $X$ solves the stochastic differential equation\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\nand the backward component $(Y,Z)$ solves\n$$\nY_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s,\\quad t\\in[0,T].\n$$\nFix a uniform grid $0=t_0t_1\\cdotst_N=T$ with time step $\\Delta t = t_{k+1}-t_k$ and Brownian increment $\\Delta W_k := W_{t_{k+1}}-W_{t_k}$. In Monte Carlo least-squares algorithms for backward stochastic differential equations (BSDEs), one often estimates $Z_{t_k}$ by regressing the random variable $Y_{t_{k+1}}\\Delta W_k$ on a finite set of basis functions of $X_{t_k}$.\n\nWhich of the following statements correctly explains and justifies, using the Itô isometry and basic conditional expectation properties, why regressing $Y_{t_{k+1}}\\Delta W_k$ on functions of $X_{t_k}$ produces an $L^2$-best approximation to $Z_{t_k}\\Delta t$ (and hence to $Z_{t_k}$ after division by $\\Delta t$), under standard Markovian and square-integrability assumptions?\n\nA. Because $\\Delta W_k$ is independent of $\\mathcal{F}_{t_k}$ with $\\mathbb{E}[\\Delta W_k]=0$ and $\\mathbb{E}[(\\Delta W_k)^2]=\\Delta t$, and because $Y_{t_{k+1}}-Y_{t_k} = -\\int_{t_k}^{t_{k+1}} f\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s$, taking $\\mathbb{E}[\\cdot\\mid\\mathcal{F}_{t_k}]$ of the product with $\\Delta W_k$ and using the Itô isometry yields $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k\\mid\\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$. In the Markovian case $Z_{t_k}=z(t_k,X_{t_k})$, so least-squares regression of $Y_{t_{k+1}}\\Delta W_k$ onto functions of $X_{t_k}$ computes the $L^2$-projection of $Z_{t_k}\\Delta t$ onto the chosen function space, justifying the estimator.\n\nB. By the Itô isometry, $\\mathbb{E}[Y_{t_{k+1}}^2]=\\mathbb{E}\\!\\left[\\int_0^{t_{k+1}} Z_s^2\\,\\mathrm{d}s\\right]$, so regressing $Y_{t_{k+1}}$ on functions of $X_{t_k}$ directly recovers $Z_{t_k}$ without using $\\Delta W_k$.\n\nC. Since $Y_{t_{k+1}}$ is independent of $\\Delta W_k$, $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k\\mid\\mathcal{F}_{t_k}]=0$, and therefore regression correctly yields $Z_{t_k}=0$.\n\nD. Because $\\mathbb{E}[\\Delta W_k\\mid\\mathcal{F}_{t_k}]\\neq 0$ when the drift $b$ is nonzero, a bias appears. Regressing on functions of $X_{t_k}$ removes this bias and reconstructs $Z_{t_k}$.\n\nE. The validity of the regression relies on identifying $Z_{t_k}$ with the Malliavin derivative of $Y_{t_{k+1}}$; Itô isometry is not relevant for justifying the use of $Y_{t_{k+1}}\\Delta W_k$ in regression.", "solution": "The user requires a validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Forward SDE**: $\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t$, with initial condition $X_0 = x$.\n-   **Backward SDE**: $Y_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s$, for $t\\in[0,T]$.\n-   **Problem Type**: The system is specified as a Markovian forward-backward stochastic differential equation (FBSDE).\n-   **Discretization**: A uniform time grid $0=t_0t_1\\cdotst_N=T$ with step size $\\Delta t = t_{k+1}-t_k$.\n-   **Brownian Increment**: $\\Delta W_k := W_{t_{k+1}}-W_{t_k}$.\n-   **Numerical Method**: The problem describes a key step in Monte Carlo least-squares algorithms for BSDEs, where $Z_{t_k}$ is estimated by regressing the random variable $Y_{t_{k+1}}\\Delta W_k$ on a set of basis functions of $X_{t_k}$.\n-   **Question**: The task is to identify the correct justification for this regression procedure, specifically using Itô isometry and properties of conditional expectation, under standard assumptions.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientific or Factual Unsoundness**: The problem is well-grounded in the theory of stochastic calculus and numerical methods for stochastic differential equations. The FBSDE setup and the regression-based approach (a variant of the Longstaff-Schwartz method adapted for BSDEs) are standard in the field. There are no violations of scientific principles.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formal and directly relevant to the specified topic of FBSDEs.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. The phrase \"standard Markovian and square-integrability assumptions\" is a conventional and acceptable way to denote that the coefficients $b, \\sigma, f, g$ are well-behaved enough to guarantee the existence and uniqueness of a solution with sufficient integrability. This does not render the problem incomplete.\n4.  **Unrealistic or Infeasible**: The problem is purely mathematical; physical realism is not a relevant criterion. The mathematical setup is standard.\n5.  **Ill-Posed or Poorly Structured**: The question is well-posed, asking for the theoretical justification of a specific numerical technique. A unique and correct justification exists based on fundamental principles of stochastic analysis.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a core, non-trivial concept in the numerical solution of BSDEs, requiring a solid understanding of Itô calculus.\n7.  **Outside Scientific Verifiability**: The justification can be rigorously derived and verified using established mathematical theorems.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. We may proceed with the solution.\n\n### Derivation and Option Analysis\n\nThe goal is to justify the approximation $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$ and its connection to least-squares regression.\n\n**Principle-Based Derivation**\n1.  We start from the integral representation of the BSDE for the process $Y_t$. Over the time interval $[t_k, t_{k+1}]$, we have:\n    $$\n    Y_{t_{k+1}} - Y_{t_k} = -\\int_{t_k}^{t_{k+1}} f(s, X_s, Y_s, Z_s)\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\n    $$\n    Rearranging for $Y_{t_{k+1}}$ gives:\n    $$\n    Y_{t_{k+1}} = Y_{t_k} - \\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\n    $$\n    where we use the shorthand $f_s = f(s, X_s, Y_s, Z_s)$.\n\n2.  We are interested in the quantity $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}]$. Let's multiply the expression for $Y_{t_{k+1}}$ by $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$ and take the conditional expectation with respect to the filtration $\\mathcal{F}_{t_k}$.\n    $$\n    \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] = \\mathbb{E}\\left[\\left(Y_{t_k} - \\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\\right) \\Delta W_k \\mid \\mathcal{F}_{t_k}\\right]\n    $$\n\n3.  We analyze each term using linearity of conditional expectation:\n    -   **Term 1**: $\\mathbb{E}[Y_{t_k}\\Delta W_k \\mid \\mathcal{F}_{t_k}]$. Since $Y_{t_k}$ is $\\mathcal{F}_{t_k}$-measurable, we can pull it out: $Y_{t_k}\\mathbb{E}[\\Delta W_k \\mid \\mathcal{F}_{t_k}]$. The Brownian increment $\\Delta W_k$ is independent of $\\mathcal{F}_{t_k}$ and has mean zero. Thus, $\\mathbb{E}[\\Delta W_k \\mid \\mathcal{F}_{t_k}] = \\mathbb{E}[\\Delta W_k] = 0$. This term is exactly zero.\n\n    -   **Term 2**: $-\\mathbb{E}\\left[\\left(\\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s\\right) \\Delta W_k \\mid \\mathcal{F}_{t_k}\\right]$. For a small time step $\\Delta t$, the integral $\\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s$ is of order $O(\\Delta t)$. The Brownian increment $\\Delta W_k$ is of order $O(\\sqrt{\\Delta t})$. Their product is of order $O((\\Delta t)^{3/2})$. This term is a higher-order term compared to the main contribution we expect, which is of order $O(\\Delta t)$. In the context of first-order numerical schemes, this term is negligible.\n\n    -   **Term 3**: $\\mathbb{E}\\left[\\left(\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\\right) \\Delta W_k \\mid \\mathcal{F}_{t_k}\\right]$. Noting that $\\Delta W_k = \\int_{t_k}^{t_{k+1}} 1 \\cdot \\mathrm{d}W_s$, this term is the conditional expectation of a product of two Itô integrals over the same interval. A key property, which is a consequence of the Itô isometry, is that for predictable processes $H_s$ and $G_s$:\n        $$\n        \\mathbb{E}\\left[\\left(\\int_{t_k}^{t_{k+1}} H_s\\,\\mathrm{d}W_s\\right)\\left(\\int_{t_k}^{t_{k+1}} G_s\\,\\mathrm{d}W_s\\right) \\mid \\mathcal{F}_{t_k}\\right] = \\mathbb{E}\\left[\\int_{t_k}^{t_{k+1}} H_s G_s\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_k}\\right]\n        $$\n        Applying this with $H_s = Z_s$ and $G_s=1$, we get:\n        $$\n        \\mathbb{E}\\left[\\left(\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\\right) \\Delta W_k \\mid \\mathcal{F}_{t_k}\\right] = \\mathbb{E}\\left[\\int_{t_k}^{t_{k+1}} Z_s \\cdot 1\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_k}\\right]\n        $$\n\n4.  Combining the results and making a standard approximation, we have:\n    $$\n    \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx \\mathbb{E}\\left[\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_k}\\right]\n    $$\n    Assuming $Z_s$ is continuous, for small $\\Delta t$, the integral $\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}s \\approx Z_{t_k}\\Delta t$. Since $Z_{t_k}$ is $\\mathcal{F}_{t_k}$-measurable:\n    $$\n    \\mathbb{E}\\left[\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_k}\\right] \\approx \\mathbb{E}[Z_{t_k}\\Delta t \\mid \\mathcal{F}_{t_k}] = Z_{t_k}\\Delta t\n    $$\n    This yields the fundamental approximation:\n    $$\n    \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t\n    $$\n\n5.  **Connection to Regression**: In a Markovian setting, the adapted process $Z_t$ is a deterministic function of time and the state process $X_t$, i.e., $Z_t = z(t, X_t)$. Thus, $Z_{t_k} = z(t_k, X_{t_k})$. For Markov processes, conditioning on the filtration $\\mathcal{F}_{t_k}$ is equivalent to conditioning on the current state $X_{t_k}$. Therefore:\n    $$\n    \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid X_{t_k}] \\approx z(t_k, X_{t_k})\\Delta t = Z_{t_k}\\Delta t\n    $$\n    The goal of least-squares regression of a random variable $A$ on a set of basis functions of a random variable $B$ is to find the best approximation of the conditional expectation $\\mathbb{E}[A \\mid B]$ within the function space spanned by the basis functions (i.e., the $L^2$-projection).\n    Therefore, regressing the numerically sampled values of $Y_{t_{k+1}}\\Delta W_k$ on basis functions of $X_{t_k}$ provides a numerical approximation to $Z_{t_k}\\Delta t$.\n\n---\n**Option-by-Option Analysis**\n\n**A.** This option provides a complete and correct summary of the derivation. It correctly cites the properties of $\\Delta W_k$, the decomposition of $Y_{t_{k+1}}$, the use of conditional expectation and Itô isometry, the resulting approximation $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$, and the final link to least-squares regression in the Markovian setting. The reasoning aligns perfectly with the derivation above.\n-   **Verdict: Correct**\n\n**B.** This option incorrectly applies the Itô isometry. The formula $\\mathbb{E}[Y_{t_{k+1}}^2]=\\mathbb{E}[\\int_0^{t_{k+1}} Z_s^2\\,\\mathrm{d}s]$ is only valid for martingales of the form $Y_t = Y_0 + \\int_0^t Z_s dW_s$. A general BSDE solution $Y_t$ is not a martingale due to the drift term $\\int f_s ds$ and the terminal condition $g(X_T)$. Furthermore, regressing $Y_{t_{k+1}}$ on functions of $X_{t_k}$ would approximate $\\mathbb{E}[Y_{t_{k+1}} \\mid X_{t_k}] \\approx Y_{t_k}$, not $Z_{t_k}$.\n-   **Verdict: Incorrect**\n\n**C.** This option claims that $Y_{t_{k+1}}$ is independent of $\\Delta W_k$. This is false. The value of $Y_{t_{k+1}}$ depends on the future path of the Brownian motion from $t_{k+1}$ to $T$, and its value at $t_{k+1}$ is determined by integrating backward from time $T$. In particular, the dynamics over $[t_k, t_{k+1}]$ show a direct dependence: $Y_{t_{k+1}} - Y_{t_k}$ includes the term $\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s$, which is correlated with $\\Delta W_k$. The premise is factually wrong.\n-   **Verdict: Incorrect**\n\n**D.** This option claims that $\\mathbb{E}[\\Delta W_k\\mid\\mathcal{F}_{t_k}]\\neq 0$. This is a fundamental error. By the definition of a standard Brownian motion with respect to a filtration $\\mathcal{F}_t$, its increments are independent of the past filtration and have zero mean. Therefore, $\\mathbb{E}[\\Delta W_k\\mid\\mathcal{F}_{t_k}] = \\mathbb{E}[\\Delta W_k] = 0$, regardless of any other process dynamics. The premise is false.\n-   **Verdict: Incorrect**\n\n**E.** This option makes two claims. First, it suggests an alternative justification using Malliavin calculus. While it is true that $Z_t$ is related to the Malliavin derivative of $Y_T$ (via the Clark-Ocone formula), providing a deeper theoretical link, the second claim is that \"Itô isometry is not relevant for justifying the use of $Y_{t_{k+1}}\\Delta W_k$ in regression.\" This is incorrect. As demonstrated in our derivation, the conditional Itô isometry is the central tool used in the most direct justification of this numerical method, which is exactly what the question asks for. The option wrongly dismisses the relevance of the very tool the question specifies.\n-   **Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3054590"}]}