{"hands_on_practices": [{"introduction": "The best way to master the Dynamic Programming Principle (DPP) is to apply it. We begin with the Linear-Quadratic (LQ) control problem, a cornerstone of modern control theory due to its wide applicability and elegant solution structure. This exercise guides you through the process of using the DPP to derive the Hamilton-Jacobi-Bellman (HJB) equation for an LQ problem, revealing how the quadratic cost function leads to a quadratic value function and a solvable set of differential equations [@problem_id:3051347].", "problem": "Consider the scalar controlled stochastic differential equation driven by a standard one-dimensional Brownian motion,\n$$\ndX_{s}=\\left(A X_{s}+B a_{s}\\right)ds+\\Sigma\\, dW_{s},\\quad s\\in[t,T],\\quad X_{t}=x,\n$$\nwith the quadratic cost functional\n$$\nJ(t,x;a)=\\mathbb{E}\\left[\\int_{t}^{T}\\left(X_{s}^{2}Q+a_{s}^{2}R\\right)ds+X_{T}^{2}S\\right].\n$$\nLet the value function be\n$$\nV(t,x)=\\inf_{a}\\,J(t,x;a),\n$$\nwhere the infimum is taken over progressively measurable controls for which the cost is finite. Starting from the definition of the value function and the dynamic programming principle (DPP), derive the associated Hamilton–Jacobi–Bellman (HJB) equation, then motivate a quadratic-in-state ansatz for $V(t,x)$ and identify the ordinary differential equations governing its coefficients.\n\nSpecialize to the concrete parameter choice $A=0$, $B=1$, $\\Sigma=2$, $Q=0$, $R=1$, $S=1$, with a finite horizon $T>0$ and initial state $X_{0}=x_{0}$. Using only first principles (the DPP and Itô’s formula), determine the closed-form analytic expression for $V(0,x_{0})$ under the optimal control. Express your final answer as a single simplified symbolic expression in terms of $x_{0}$ and $T$. No numerical approximation or rounding is required.", "solution": "The user wants to find the value function for a specific stochastic optimal control problem. The solution process involves deriving the general Hamilton-Jacobi-Bellman (HJB) equation from first principles, specializing it for the given parameters, and solving it to find the value function.\n\n**Problem Validation**\n\nFirst, I will verify the problem's validity.\n\n**Step 1: Extract Givens**\n-   Stochastic Differential Equation (SDE) for the state $X_s$:\n    $$dX_{s}=\\left(A X_{s}+B a_{s}\\right)ds+\\Sigma\\, dW_{s},\\quad s\\in[t,T],\\quad X_{t}=x$$\n-   Cost functional to be minimized:\n    $$J(t,x;a)=\\mathbb{E}\\left[\\int_{t}^{T}\\left(X_{s}^{2}Q+a_{s}^{2}R\\right)ds+X_{T}^{2}S\\right]$$\n-   Value function definition:\n    $$V(t,x)=\\inf_{a}\\,J(t,x;a)$$\n-   Specific parameter values: $A=0$, $B=1$, $\\Sigma=2$, $Q=0$, $R=1$, $S=1$.\n-   Initial conditions: time $t=0$, state $X_0=x_0$.\n-   Horizon: finite time $T>0$.\n-   Methodology: The solution must be derived starting from the dynamic programming principle (DPP) and using Itô's formula.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a classic linear-quadratic (LQ) stochastic control problem. The mathematical framework, including the SDE, cost functional, DPP, and HJB equation, are fundamental concepts in control theory and stochastic calculus. The problem is free of any scientific or factual unsoundness.\n-   **Well-Posed:** The problem is fully specified. All parameters, dynamics, and objectives are clearly defined. It is a standard problem setup known to have a unique, stable, and meaningful solution.\n-   **Objective:** The problem statement is written in precise, formal mathematical language, with no subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is a standard, well-posed problem in stochastic optimal control. I will proceed with the solution.\n\n**Derivation of the Hamilton-Jacobi-Bellman (HJB) Equation**\n\nThe dynamic programming principle (DPP) for the value function $V(t,x)$ over an infinitesimal time interval $[t, t+dt]$ is:\n$$\nV(t,x) = \\inf_{a}\\mathbb{E}\\left[\\int_{t}^{t+dt}\\left(X_{u}^{2}Q+a_{u}^{2}R\\right)du + V(t+dt, X_{t+dt}) \\mid X_{t}=x \\right]\n$$\nFor a small interval $dt$, we can approximate the integral as $(X_t^2 Q + a_t^2 R)dt = (x^2 Q + a_t^2 R)dt$. The DPP becomes:\n$$\nV(t,x) \\approx \\inf_{a_t}\\left\\{ (x^2 Q + a_t^2 R)dt + \\mathbb{E}[V(t+dt, X_{t+dt})] \\right\\}\n$$\nWe apply Itô's formula to expand $V(t,X_t)$:\n$$\ndV(t,X_t) = \\frac{\\partial V}{\\partial t}dt + \\frac{\\partial V}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 V}{\\partial x^2}(dX_t)^2\n$$\nThe Ito rule for $(dX_t)^2$ with $dX_{t}=\\left(A X_{t}+B a_{t}\\right)dt+\\Sigma\\, dW_{t}$ gives $(dX_t)^2 = (\\Sigma dW_t)^2 = \\Sigma^2 dt$. Substituting this and $dX_t$ into the expansion for $dV$:\n$$\ndV(t,X_t) = \\left(\\frac{\\partial V}{\\partial t} + (AX_t+Ba_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\Sigma^2\\frac{\\partial^2 V}{\\partial x^2}\\right)dt + \\Sigma\\frac{\\partial V}{\\partial x}dW_t\n$$\nTaking the expectation of the integral of $dV$ from $t$ to $t+dt$:\n$$\n\\mathbb{E}[V(t+dt,X_{t+dt})] - V(t,x) \\approx \\left(\\frac{\\partial V}{\\partial t} + (Ax+Ba_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\Sigma^2\\frac{\\partial^2 V}{\\partial x^2}\\right)dt\n$$\nsince $\\mathbb{E}[dW_t]=0$. Substituting this back into the DPP approximation:\n$$\nV(t,x) \\approx \\inf_{a_t}\\left\\{ (x^2 Q + a_t^2 R)dt + V(t,x) + \\left(\\frac{\\partial V}{\\partial t} + (Ax+Ba_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\Sigma^2\\frac{\\partial^2 V}{\\partial x^2}\\right)dt \\right\\}\n$$\nRearranging, dividing by $dt$, and taking the limit as $dt \\to 0$ yields the HJB equation:\n$$\n0 = \\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\Sigma^2\\frac{\\partial^2 V}{\\partial x^2} + Qx^2 + \\inf_{a}\\left\\{ (Ax+Ba)\\frac{\\partial V}{\\partial x} + a^2 R \\right\\}\n$$\nTo find the optimal control $a^*$, we minimize the term in the brackets with respect to $a$. Differentiating and setting to zero:\n$$\n\\frac{\\partial}{\\partial a}\\left( Ba\\frac{\\partial V}{\\partial x} + a^2 R \\right) = B\\frac{\\partial V}{\\partial x} + 2aR = 0 \\implies a^* = -\\frac{B}{2R}\\frac{\\partial V}{\\partial x}\n$$\nSubstituting $a^*$ back into the HJB equation:\n$$\n-\\frac{\\partial V}{\\partial t} = \\frac{1}{2}\\Sigma^2\\frac{\\partial^2 V}{\\partial x^2} + Ax\\frac{\\partial V}{\\partial x} + Qx^2 - \\frac{B^2}{4R}\\left(\\frac{\\partial V}{\\partial x}\\right)^2\n$$\nThe terminal condition is given by the final cost term: $V(T,x) = Sx^2$.\n\n**Solving the HJB Equation with a Quadratic Ansatz**\n\nThe linear dynamics and quadratic cost structure motivate a quadratic ansatz for the value function:\n$V(t,x) = P(t)x^2 + q(t)$\nThe relevant derivatives are:\n$\\frac{\\partial V}{\\partial t} = P'(t)x^2 + q'(t)$, $\\frac{\\partial V}{\\partial x} = 2P(t)x$, $\\frac{\\partial^2 V}{\\partial x^2} = 2P(t)$\nSubstituting these into the HJB equation:\n$$\n-(P'(t)x^2 + q'(t)) = \\frac{1}{2}\\Sigma^2(2P(t)) + Ax(2P(t)x) + Qx^2 - \\frac{B^2}{4R}(2P(t)x)^2\n$$\n$$\n-P'(t)x^2 - q'(t) = \\left(2AP(t) + Q - \\frac{B^2}{R}P(t)^2\\right)x^2 + \\Sigma^2 P(t)\n$$\nEquating coefficients of powers of $x$:\n- Coefficient of $x^2$: $-P'(t) = 2AP(t) + Q - \\frac{B^2}{R}P(t)^2 \\implies P'(t) = -2AP(t) - Q + \\frac{B^2}{R}P(t)^2$\n- Constant term: $-q'(t) = \\Sigma^2 P(t) \\implies q'(t) = -\\Sigma^2 P(t)$\n\nThe terminal conditions $P(T)=S$ and $q(T)=0$ are obtained from $V(T,x)=Sx^2$.\n\n**Specialization and Solution**\n\nWe use the specific parameters: $A=0$, $B=1$, $\\Sigma=2$, $Q=0$, $R=1$, $S=1$.\n\nThe ODE for $P(t)$ becomes a Riccati equation:\n$P'(t) = -2(0)P(t) - 0 + \\frac{1^2}{1}P(t)^2 \\implies P'(t) = P(t)^2$\nwith terminal condition $P(T)=1$. We solve by separation of variables, integrating backwards from $T$:\n$$\n\\int_{P(t)}^{P(T)} \\frac{d\\tilde{P}}{\\tilde{P}^2} = \\int_t^T d\\tau \\implies \\left[-\\frac{1}{\\tilde{P}}\\right]_{P(t)}^{1} = T-t\n$$\n$$\n-1 - \\left(-\\frac{1}{P(t)}\\right) = T-t \\implies \\frac{1}{P(t)} = 1+T-t \\implies P(t) = \\frac{1}{1+T-t}\n$$\nThe ODE for $q(t)$ is:\n$q'(t) = -\\Sigma^2 P(t) = -4P(t) = -\\frac{4}{1+T-t}$\nwith terminal condition $q(T)=0$. We integrate from $t$ to $T$:\n$$\nq(T) - q(t) = \\int_t^T q'(\\tau)d\\tau \\implies 0 - q(t) = \\int_t^T \\left(-\\frac{4}{1+T-\\tau}\\right) d\\tau\n$$\nLet $u = 1+T-\\tau$, so $du = -d\\tau$. The limits become $1+T-t$ and $1$.\n$$\n-q(t) = \\int_{1+T-t}^1 -\\frac{4}{u}(-du) = -4\\int_1^{1+T-t} \\frac{du}{u} = -4[\\ln(u)]_1^{1+T-t}\n$$\n$$\n-q(t) = -4(\\ln(1+T-t) - \\ln(1)) = -4\\ln(1+T-t)\n$$\nSo, $q(t) = 4\\ln(1+T-t)$.\n\n**Final Answer Calculation**\n\nWe need to find $V(0, x_0)$. We substitute $t=0$ and $x=x_0$ into the expression $V(t,x) = P(t)x^2 + q(t)$:\n$V(0,x_0) = P(0)x_0^2 + q(0)$\nEvaluating the coefficients at $t=0$:\n$P(0) = \\frac{1}{1+T-0} = \\frac{1}{1+T}$\n$q(0) = 4\\ln(1+T-0) = 4\\ln(1+T)$\nCombining these results gives the final expression for the value function at the initial time:\n$V(0, x_0) = \\frac{x_0^2}{1+T} + 4\\ln(1+T)$", "answer": "$$\n\\boxed{\\frac{x_{0}^{2}}{1+T} + 4\\ln(1+T)}\n$$", "id": "3051347"}, {"introduction": "After solving the HJB equation to find the value function $v(t,x)$, a crucial practical question remains: how do we construct the optimal control policy itself? This practice addresses the synthesis of an optimal feedback control $\\alpha^*(t,x)$ by minimizing the Hamiltonian. It challenges us to consider the essential technical requirements, such as Borel measurability, that ensure the resulting feedback law is mathematically sound and constitutes an admissible control process within the stochastic framework [@problem_id:3051402].", "problem": "Consider a finite-horizon stochastic control problem on the interval $[0,T]$ with state process $X^{\\alpha}$ solving the controlled stochastic differential equation\n$$\ndX^{\\alpha}_t \\;=\\; b\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dt \\;+\\; \\sigma\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dW_t,\\qquad t\\in[s,T],\\qquad X^{\\alpha}_s=x,\n$$\nwhere $W$ is a $d$-dimensional Wiener process (standard Brownian motion) on a filtered probability space satisfying the usual conditions, and controls $\\alpha$ take values in a compact metric action set $A$ and are progressively measurable with respect to the filtration generated by $W$. The performance criterion is\n$$\nJ^{\\alpha}(s,x)\\;=\\;\\mathbb{E}\\!\\left[\\int_s^T f\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dt \\;+\\; g\\!\\left(X^{\\alpha}_T\\right)\\right],\n$$\nfor given terminal cost $g$ and running cost $f$. Assume the data satisfy the following: the functions $b$, $\\sigma$, and $f$ are Borel in $(t,x)$, continuous in $a\\in A$ for each $(t,x)$, and satisfy local Lipschitz and linear growth conditions in $x$ uniformly in $a\\in A$ so that the stochastic differential equation is well-posed for admissible controls and the cost is finite.\n\nAssume the value function\n$$\nv(s,x)\\;=\\;\\inf_{\\alpha}\\,J^{\\alpha}(s,x)\n$$\nis sufficiently smooth, specifically $v\\in C^{1,2}([0,T]\\times\\mathbb{R}^d)$, so that the Dynamic Programming Principle (DPP) implies that $v$ is a classical solution of the Hamilton–Jacobi–Bellman (HJB) equation\n$$\n-\\,\\partial_t v(t,x)\\;=\\;\\inf_{a\\in A}\\Big\\{ f(t,x,a)\\;+\\; b(t,x,a)\\cdot Dv(t,x)\\;+\\;\\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\sigma\\sigma^{\\top}(t,x,a)\\,D^2v(t,x)\\big)\\Big\\},\n$$\nwith terminal condition $v(T,x)=g(x)$. Define the Hamiltonian\n$$\nH(t,x,p,M,a)\\;=\\; f(t,x,a)\\;+\\; b(t,x,a)\\cdot p\\;+\\;\\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\sigma\\sigma^{\\top}(t,x,a)\\,M\\big).\n$$\n\nYour task is to determine which statement correctly describes how to construct a candidate optimal feedback control $\\alpha^*(t,x)$ by minimizing the Hamiltonian and what measurability properties are minimally required of $\\alpha^*$ so that the feedback control process $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ is admissible and compatible with the Dynamic Programming Principle.\n\nChoose the single best option.\n\nA. Under the given regularity and compactness assumptions, for each $(t,x)$ there exists a minimizer of $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$, and a Borel measurable selector $\\alpha^*:[0,T]\\times\\mathbb{R}^d\\to A$ can be chosen so that\n$$\n\\alpha^*(t,x)\\in\\operatorname*{arg\\,min}_{a\\in A}H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)\\quad\\text{for all }(t,x).\n$$\nThen $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ is progressively measurable because $(t,X^{\\alpha^*}_t)$ is progressively measurable and $\\alpha^*$ is Borel. If $\\alpha^*$ also obeys the linear growth bound inherited from the minimizers of $H$, this feedback is admissible.\n\nB. Pointwise minimization of $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$ yields some choice $\\alpha^*(t,x)$ for each $(t,x)$, and no measurability of $\\alpha^*$ is needed because the stochastic differential equation depends only on $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ at each time, which is determined pointwise.\n\nC. It is necessary that the selector $\\alpha^*(t,x)$ be continuous in $(t,x)$; Borel measurability is not enough to guarantee that $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ is progressively measurable, so a merely Borel selector cannot be used to define an admissible feedback.\n\nD. If the map $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$ is strictly convex for each $(t,x)$, then uniqueness of the minimizer at each $(t,x)$ automatically guarantees that $\\alpha^*(t,x)$ is measurable in $(t,x)$, so no separate measurability argument is needed to ensure admissibility.\n\nE. The feedback control can be taken in the universal form $\\alpha^*(t,x)=-\\,Dv(t,x)$ regardless of the coefficients $b$ and $\\sigma$, and since $v\\in C^{1,2}$ this immediately implies that $\\alpha^*$ is measurable and admissible.", "solution": "The user wants me to act as a STEM professor and solve a problem about constructing an optimal feedback control in a stochastic optimal control setting.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- A controlled stochastic differential equation (SDE) on the interval $[0,T]$:\n$$\ndX^{\\alpha}_t \\;=\\; b\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dt \\;+\\; \\sigma\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dW_t,\\qquad t\\in[s,T],\\qquad X^{\\alpha}_s=x\n$$\n- $W$ is a $d$-dimensional Wiener process.\n- Controls $\\alpha$ are processes with values $\\alpha_t$ in a compact metric action set $A$.\n- Admissible controls $\\alpha$ are progressively measurable with respect to the filtration generated by $W$.\n- The performance criterion (cost functional) to be minimized is:\n$$\nJ^{\\alpha}(s,x)\\;=\\;\\mathbb{E}\\!\\left[\\int_s^T f\\!\\left(t,X^{\\alpha}_t,\\alpha_t\\right)\\,dt \\;+\\; g\\!\\left(X^{\\alpha}_T\\right)\\right]\n$$\n- The functions $b$, $\\sigma$, and $f$ are Borel in $(t,x)$, continuous in $a\\in A$ for each $(t,x)$, and satisfy local Lipschitz and linear growth conditions in $x$ uniformly in $a\\in A$.\n- The value function is $v(s,x) = \\inf_{\\alpha} J^{\\alpha}(s,x)$.\n- The value function is assumed to be smooth: $v \\in C^{1,2}([0,T]\\times\\mathbb{R}^d)$.\n- The value function $v$ is a classical solution to the Hamilton-Jacobi-Bellman (HJB) equation:\n$$\n-\\,\\partial_t v(t,x)\\;=\\;\\inf_{a\\in A}\\Big\\{ f(t,x,a)\\;+\\; b(t,x,a)\\cdot Dv(t,x)\\;+\\;\\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\sigma\\sigma^{\\top}(t,x,a)\\,D^2v(t,x)\\big)\\Big\\}\n$$\nwith terminal condition $v(T,x)=g(x)$.\n- The Hamiltonian is defined as:\n$$\nH(t,x,p,M,a)\\;=\\; f(t,x,a)\\;+\\; b(t,x,a)\\cdot p\\;+\\;\\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\sigma\\sigma^{\\top}(t,x,a)\\,M\\big)\n$$\n- **Task**: To determine the correct statement about constructing a candidate optimal feedback control $\\alpha^*(t,x)$ and its required measurability properties for admissibility.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a standard setup in the theory of stochastic optimal control for diffusion processes.\n- **Scientifically Grounded**: The problem is entirely based on established mathematical principles within stochastic control theory, including controlled SDEs, the Dynamic Programming Principle, and the Hamilton-Jacobi-Bellman equation. All concepts are standard and well-defined.\n- **Well-Posed**: The problem asks for the properties of an optimal feedback control, which is a central question in the verification theorem of stochastic control. The assumptions on the coefficients ($b$, $\\sigma$, $f$) and the value function ($v \\in C^{1,2}$) are strong but standard for establishing the \"classical\" theory, ensuring the problem is tractable and has a definite answer within this framework.\n- **Objective**: The problem is stated in precise mathematical language, free from ambiguity or subjective content.\n\nThe problem does not violate any of the invalidity criteria. It is a well-formulated question in mathematical control theory. The assumptions provided are standard and internally consistent.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the derivation of the solution and the evaluation of the options.\n\n### Solution Derivation\n\nThe Hamilton-Jacobi-Bellman (HJB) equation provides a necessary and, under suitable conditions (a \"verification theorem\"), sufficient condition for optimality. The equation is\n$$\n-\\,\\partial_t v(t,x) = \\inf_{a\\in A} H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)\n$$\nA candidate for an optimal control is a feedback (or Markovian) control law $\\alpha^*(t,x)$ that, for each state $(t,x)$, achieves the infimum in the HJB equation. That is,\n$$\n\\alpha^*(t,x) \\in \\operatorname*{arg\\,min}_{a \\in A} H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big) \\quad \\text{for all } (t,x) \\in [0,T] \\times \\mathbb{R}^d.\n$$\nOnce such a function $\\alpha^*(t,x)$ is found, the candidate optimal control process is $\\alpha^*_t = \\alpha^*(t, X^*_t)$, where $X^*$ is the solution to the SDE with this control inserted:\n$$\ndX^*_t = b(t,X^*_t,\\alpha^*(t,X^*_t))\\,dt + \\sigma(t,X^*_t,\\alpha^*(t,X^*_t))\\,dW_t.\n$$\nFor this program to be well-defined and for the resulting control process $\\alpha^*_t$ to be admissible, several technical points must be addressed.\n\n1.  **Existence of a minimizer**: For any fixed $(t,x) \\in [0,T] \\times \\mathbb{R}^d$, we need to ensure that the set of minimizers $\\operatorname*{arg\\,min}_{a \\in A} H(\\dots,a)$ is non-empty. Let $p = Dv(t,x)$ and $M = D^2v(t,x)$, which are well-defined since $v \\in C^{1,2}$. The problem states that the functions $f(t,x,a)$, $b(t,x,a)$, and $\\sigma(t,x,a)$ are continuous in $a$ for each fixed $(t,x)$. Therefore, the Hamiltonian $H(t,x,p,M,a)$ is also a continuous function of $a$. Since the control set $A$ is assumed to be a compact metric space, the Extreme Value Theorem (Weierstrass) guarantees that the continuous function $a \\mapsto H(t,x,p,M,a)$ attains its minimum on $A$. Thus, for each $(t,x)$, a minimizer exists.\n\n2.  **Measurability of the selector**: The existence of a minimizer for each $(t,x)$ is not sufficient. We need to be able to select a minimizer $\\alpha^*(t,x)$ for each $(t,x)$ in such a way that the resulting function $\\alpha^*: [0,T] \\times \\mathbb{R}^d \\to A$ is measurable (specifically, Borel measurable). This is required so that the composed process $\\alpha^*(t, X^*_t)$ is well-defined in the context of stochastic integration. The existence of such a a measurable function is not automatic but is guaranteed by a class of results known as \"measurable selection theorems\". A standard version (like the Kuratowski and Ryll-Nardzewski selection theorem) applies here. The required conditions are that the function to be minimized, $(t,x,a) \\mapsto H(t,x,Dv(t,x),D^2v(t,x),a)$, is a Carathéodory function: it is measurable in $(t,x)$ for fixed $a$, and continuous in $a$ for fixed $(t,x)$.\n    - The problem states $b$, $\\sigma$, and $f$ are Borel in $(t,x)$, and continuous in $a$.\n    - The value function $v$ is assumed to be in $C^{1,2}$, which implies its derivatives $Dv(t,x)$ and $D^2v(t,x)$ are continuous functions of $(t,x)$, and therefore Borel measurable.\n    - Since products and sums of Borel measurable functions are Borel measurable, the map $(t,x) \\mapsto H(t,x,Dv(t,x),D^2v(t,x),a)$ is Borel measurable for each fixed $a$.\n    - The conditions for the measurable selection theorem are met. Therefore, there exists a Borel measurable function $\\alpha^*: [0,T]\\times\\mathbb{R}^d\\to A$ that is a pointwise minimizer of the Hamiltonian.\n\n3.  **Admissibility of the feedback control process**: An admissible control process must be progressively measurable. The candidate process is $\\alpha^*_t = \\alpha^*(t,X^*_t)$. The solution $X^*_t$ to the SDE is a continuous, adapted process. A standard result in stochastic processes states that a continuous, adapted process is progressively measurable. This implies that the process $(t, \\omega) \\mapsto (t, X^*_t(\\omega))$ is a progressively measurable process from $([0,T]\\times\\Omega, \\mathcal{P})$ to $([0,T]\\times\\mathbb{R}^d, \\mathcal{B}([0,T]\\times\\mathbb{R}^d))$, where $\\mathcal{P}$ is the progressive sigma-algebra. Since a Borel measurable function of a progressively measurable process is itself progressively measurable, and we have established that a Borel measurable selector $\\alpha^*$ exists, the composite process $\\alpha^*_t = \\alpha^*(t,X^*_t)$ is progressively measurable. This ensures the stochastic integral involving $\\alpha^*_t$ is well-defined.\n\n4.  **Well-posedness of the SDE**: The feedback SDE requires that the coefficients $t \\mapsto b(t, X^*_t, \\alpha^*(t,X^*_t))$ and $t \\mapsto \\sigma(t, X^*_t, \\alpha^*(t,X^*_t))$ satisfy conditions (e.g., local Lipschitz and linear growth in the state variable) that guarantee existence and uniqueness of a solution. The problem statement includes a strong assumption that $b$, $\\sigma$ (and $f$) \"satisfy local Lipschitz and linear growth conditions in $x$ uniformly in $a\\in A$\". This means the Lipschitz and growth constants are independent of $a$. Consequently, for any measurable selector $\\alpha^*(t,x)$, the resulting composed coefficients $x \\mapsto b(t,x,\\alpha^*(t,x))$ and $x \\mapsto \\sigma(t,x,\\alpha^*(t,x))$ automatically satisfy these conditions. This ensures the SDE for $X^*$ is well-posed.\n\n### Option-by-Option Analysis\n\n**A. Under the given regularity and compactness assumptions, for each $(t,x)$ there exists a minimizer of $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$, and a Borel measurable selector $\\alpha^*:[0,T]\\times\\mathbb{R}^d\\to A$ can be chosen so that\n$$\n\\alpha^*(t,x)\\in\\operatorname*{arg\\,min}_{a\\in A}H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)\\quad\\text{for all }(t,x).\n$$\nThen $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ is progressively measurable because $(t,X^{\\alpha^*}_t)$ is progressively measurable and $\\alpha^*$ is Borel. If $\\alpha^*$ also obeys the linear growth bound inherited from the minimizers of $H$, this feedback is admissible.**\n\nThis statement accurately follows the line of reasoning detailed above.\n- Existence of a minimizer: Correct, due to continuity of $H$ in $a$ and compactness of $A$.\n- Existence of a Borel measurable selector: Correct, via a measurable selection theorem.\n- Progressive measurability of the feedback process: Correct, as it is a Borel function of a progressively measurable process.\n- Admissibility condition: The final phrase concerning the linear growth bound is slightly redundant given the problem's assumption of uniformity in $a$, but it correctly points to the need for the SDE coefficients to satisfy such bounds for the SDE to be well-posed, which is a component of admissibility. This statement is the most complete and correct description of the procedure.\n**Verdict: Correct**\n\n**B. Pointwise minimization of $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$ yields some choice $\\alpha^*(t,x)$ for each $(t,x)$, and no measurability of $\\alpha^*$ is needed because the stochastic differential equation depends only on $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ at each time, which is determined pointwise.**\n\nThis statement is fundamentally flawed. Measurability is a prerequisite for a process to be an integrand in a stochastic integral (like the Itô integral). If $\\alpha^*(t,x)$ is not a measurable function, the process $\\alpha^*_t = \\alpha^*(t,X^*_t)$ may not be progressively measurable, and the term $\\int_s^t \\sigma(u,X^*_u,\\alpha^*_u)dW_u$ may be ill-defined. The entire theory of SDEs rests on measurability assumptions.\n**Verdict: Incorrect**\n\n**C. It is necessary that the selector $\\alpha^*(t,x)$ be continuous in $(t,x)$; Borel measurability is not enough to guarantee that $\\alpha^*_t=\\alpha^*(t,X^{\\alpha^*}_t)$ is progressively measurable, so a merely Borel selector cannot be used to define an admissible feedback.**\n\nThis statement contains two errors. First, continuity of the selector $\\alpha^*(t,x)$ is a sufficient condition for measurability, but it is by no means necessary. Optimal controls are frequently discontinuous. Second, the claim that \"Borel measurability is not enough\" is false. As explained in the main derivation, a Borel measurable function of a progressively measurable process (like $(t,X^*_t)$) is indeed progressively measurable. This is a standard result in stochastic calculus.\n**Verdict: Incorrect**\n\n**D. If the map $a\\mapsto H\\big(t,x,Dv(t,x),D^2v(t,x),a\\big)$ is strictly convex for each $(t,x)$, then uniqueness of the minimizer at each $(t,x)$ automatically guarantees that $\\alpha^*(t,x)$ is measurable in $(t,x)$, so no separate measurability argument is needed to ensure admissibility.**\n\nThis is a subtle but incorrect point. The uniqueness of a minimizer at each point in the domain does not, by itself, guarantee the measurability of the function that maps each point to its unique minimizer. One can construct counterexamples using non-measurable sets where a function has a unique minimizer for each parameter value, but the resulting selection function is not measurable. Measurability must be established through other means, such as a measurable selection theorem, as mentioned in the analysis of option A.\n**Verdict: Incorrect**\n\n**E. The feedback control can be taken in the universal form $\\alpha^*(t,x)=-\\,Dv(t,x)$ regardless of the coefficients $b$ and $\\sigma$, and since $v\\in C^{1,2}$ this immediately implies that $\\alpha^*$ is measurable and admissible.**\n\nThis statement is patently false. The form of the optimal control $\\alpha^*(t,x)$ is determined by the minimization of the Hamiltonian and therefore depends explicitly on the functional forms of the cost $f$ and the dynamics coefficients $b$ and $\\sigma$. The form $\\alpha^*(t,x)=-\\,Dv(t,x)$ arises only in very specific cases (e.g., linear-quadratic problems with control entering the drift linearly and with quadratic cost on the control). Claiming this form is \"universal\" and holds \"regardless of the coefficients\" is a gross overgeneralization.\n**Verdict: Incorrect**\n\nBased on the detailed analysis, option A provides the only correct and complete description of the construction and properties of the optimal feedback control.", "answer": "$$\\boxed{A}$$", "id": "3051402"}, {"introduction": "The power of the Dynamic Programming Principle extends beyond continuous adjustments to situations requiring a single, optimally-timed decision. This optimal stopping problem, framed as the pricing of a perpetual American option, is a classic example. Here, the DPP helps partition the state space into a region where we wait (continuation) and a region where we act (stopping), leading to a powerful method for solving for the optimal decision boundary known as a free-boundary problem [@problem_id:3051361].", "problem": "Consider a one-dimensional optimal stopping problem for a perpetual American put option in continuous time. Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space satisfying the usual conditions. Under the risk-neutral measure, the price process $\\{S_{t}\\}_{t \\geq 0}$ of a dividend-paying asset evolves as the geometric Brownian motion\n$$\n\\mathrm{d} S_{t} = (r - q) S_{t} \\,\\mathrm{d} t + \\sigma S_{t} \\,\\mathrm{d} W_{t},\n$$\nwhere $r > 0$ is the constant risk-free rate, $q \\geq 0$ is the constant dividend yield, and $\\sigma > 0$ is the constant volatility. Let $K > 0$ be the strike. Define the value function\n$$\nV(s) = \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{s}\\!\\left[ \\exp(- r \\tau) \\, (K - S_{\\tau})_{+} \\right],\n$$\nwhere $\\mathcal{T}$ is the set of all stopping times with respect to the filtration of $\\{W_{t}\\}_{t \\geq 0}$ and $\\mathbb{E}_{s}[\\cdot]$ denotes conditional expectation given $S_{0} = s$.\n\nUsing only the dynamic programming principle and Itô calculus as the fundamental base, derive the associated free-boundary problem that characterizes $V(s)$. Show that the optimal stopping region is of threshold type, $\\{ s \\leq s^{*} \\}$ for some $s^{*} \\in (0, K)$, and determine $s^{*}$ completely by solving the free-boundary problem (value matching and smooth fit), together with appropriate boundary behavior for large $s$. Express your final result as a single closed-form analytic expression in terms of $r$, $q$, $\\sigma$, and $K$. No numerical approximation is required and no rounding is needed.", "solution": "The user wants to solve for the optimal exercise boundary of a perpetual American put option.\n\n### Step 1: Problem Validation\n\nThe problem is a classic and well-established problem in quantitative finance and stochastic control theory.\n\n**Extracted Givens:**\n- The asset price process $\\{S_t\\}_{t \\geq 0}$ follows a Geometric Brownian Motion under the risk-neutral measure: $\\mathrm{d} S_{t} = (r - q) S_{t} \\,\\mathrm{d} t + \\sigma S_{t} \\,\\mathrm{d} W_{t}$.\n- $r > 0$: constant risk-free rate.\n- $q \\geq 0$: constant dividend yield.\n- $\\sigma > 0$: constant volatility.\n- $\\{W_t\\}_{t \\geq 0}$: standard Brownian motion.\n- $K > 0$: strike price.\n- The value function is defined as $V(s) = \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{s}\\!\\left[ \\exp(- r \\tau) \\, (K - S_{\\tau})_{+} \\right]$, where $\\mathcal{T}$ is the set of all stopping times and $\\mathbb{E}_{s}[\\cdot]$ is the expectation conditional on $S_0=s$.\n\n**Validation Verdict:**\nThe problem is **valid**. It is scientifically grounded in the theory of stochastic differential equations and option pricing, it is well-posed, objective, and self-contained. It is a standard textbook problem that can be solved using the requested methods (dynamic programming and Itô calculus).\n\n### Step 2: Solution Derivation\n\nThe problem is an optimal stopping problem. The dynamic programming principle states that the state space of the asset price, $s \\in [0, \\infty)$, can be partitioned into two regions: a continuation region $\\mathcal{C}$ where it is optimal to hold the option, and a stopping region $\\mathcal{S}$ where it is optimal to exercise it.\n\nThe value of the option, $V(s)$, must satisfy the following conditions:\n1.  $V(s) \\geq (K-s)_{+}$ for all $s \\geq 0$. This is because one can always choose to exercise immediately. $(K-s)_{+}$ is the intrinsic value or payoff function.\n2.  In the stopping region $\\mathcal{S}$, the option is exercised, so its value is equal to the payoff: $V(s) = (K-s)_{+}$.\n3.  In the continuation region $\\mathcal{C}$, it is not optimal to exercise. The value function $V(s)$ must satisfy the Hamilton-Jacobi-Bellman (HJB) equation. For a perpetual option, this equation is time-independent. The discounted value process, $\\exp(-rt)V(S_t)$, must be a supermartingale, and a martingale in the continuation region. Applying Itô's formula to $\\exp(-rt)V(S_t)$ and setting the drift term to zero gives the HJB equation:\n    $$\n    \\frac{1}{2}\\sigma^2 s^2 \\frac{\\mathrm{d}^2 V}{\\mathrm{d}s^2} + (r-q)s \\frac{\\mathrm{d} V}{\\mathrm{d}s} - rV = 0\n    $$\n\nNow, we characterize the stopping and continuation regions.\n- If $s \\geq K$, the payoff $(K-s)_{+}$ is $0$. Since $r>0$, there's a time value to money, and there's a non-zero probability that $S_t$ will fall below $K$ in the future, yielding a positive payoff. Thus, it is never optimal to exercise when $s \\geq K$. The stopping region $\\mathcal{S}$ must be a subset of $[0, K)$.\n- As $s \\to 0$, the stock becomes worthless, and the payoff approaches $K$. It is clearly optimal to exercise. So, a neighborhood of $s=0$ is in the stopping region.\n- As $s \\to \\infty$, the option is deep out-of-the-money. The probability of the stock price ever falling below $K$ becomes vanishingly small. Therefore, the value of the option should approach zero: $\\lim_{s \\to \\infty} V(s) = 0$.\n\nThese observations strongly suggest that the stopping region is of the threshold type, i.e., an interval $[0, s^*]$ for some optimal exercise boundary $s^*$. The continuation region is then $(s^*, \\infty)$. We must have $s^* < K$.\n\nThe problem is now a free-boundary problem defined by:\n1.  The ODE in the continuation region $(s^*, \\infty)$:\n    $$\n    \\frac{1}{2}\\sigma^2 s^2 V''(s) + (r-q)s V'(s) - rV(s) = 0 \\quad \\text{for } s > s^*\n    $$\n2.  The boundary condition at infinity:\n    $$\n    \\lim_{s \\to \\infty} V(s) = 0\n    $$\n3.  The value matching condition at the free boundary $s^*$:\n    $V(s^*) = K - s^*$\n4.  The smooth-pasting (or smooth-fit) condition at the free boundary $s^*$:\n    $V'(s^*) = -1$\n\nThe ODE is a Cauchy-Euler equation. We seek a solution of the form $V(s) = s^{\\gamma}$. Substituting this into the ODE yields the characteristic quadratic equation for $\\gamma$:\n$$\n\\frac{1}{2}\\sigma^2 \\gamma(\\gamma-1) + (r-q)\\gamma - r = 0\n$$\n$$\n\\frac{1}{2}\\sigma^2 \\gamma^2 + (r - q - \\frac{1}{2}\\sigma^2)\\gamma - r = 0\n$$\nLet the roots be $\\gamma_1$ and $\\gamma_2$. The product of the roots is $\\gamma_1 \\gamma_2 = \\frac{-r}{\\frac{1}{2}\\sigma^2} = -\\frac{2r}{\\sigma^2} < 0$. This implies one root is positive ($\\gamma_1 > 0$) and one is negative ($\\gamma_2 < 0$).\nThe general solution to the ODE is $V(s) = C_1 s^{\\gamma_1} + C_2 s^{\\gamma_2}$.\n\nApplying the boundary condition at infinity, $\\lim_{s \\to \\infty} V(s) = 0$. Since $\\gamma_1 > 0$, the term $C_1 s^{\\gamma_1}$ diverges as $s \\to \\infty$ unless $C_1=0$. The term $C_2 s^{\\gamma_2}$ vanishes as $s \\to \\infty$ because $\\gamma_2 < 0$. Thus, we must have $C_1=0$.\nThe solution in the continuation region takes the form:\n$$\nV(s) = C_2 s^{\\gamma_2} \\quad \\text{for } s > s^*\n$$\nNow, we apply the two conditions at the free boundary $s^*$:\n1.  Value matching: $C_2 (s^*)^{\\gamma_2} = K - s^*$\n2.  Smooth fit: The derivative is $V'(s) = C_2 \\gamma_2 s^{\\gamma_2-1}$. At $s=s^*$, we have $C_2 \\gamma_2 (s^*)^{\\gamma_2-1} = -1$.\n\nWe have a system of two equations for the two unknowns, $C_2$ and $s^*$. We can solve for $s^*$ by dividing the first equation by the second:\n$$\n\\frac{C_2 (s^*)^{\\gamma_2}}{C_2 \\gamma_2 (s^*)^{\\gamma_2-1}} = \\frac{K - s^*}{-1}\n$$\n$$\n\\frac{s^*}{\\gamma_2} = -(K - s^*)\n$$\n$$\ns^* = -\\gamma_2 (K - s^*) = -\\gamma_2 K + \\gamma_2 s^*\n$$\n$$\ns^*(1 - \\gamma_2) = -\\gamma_2 K\n$$\n$$\ns^* = \\frac{-\\gamma_2}{1 - \\gamma_2} K = \\frac{\\gamma_2}{\\gamma_2 - 1} K\n$$\nSince $\\gamma_2 < 0$, the factor $\\frac{\\gamma_2}{\\gamma_2 - 1}$ is in the interval $(0, 1)$, which confirms that $0 < s^* < K$.\n\nFinally, we need the explicit expression for the negative root $\\gamma_2$. Using the quadratic formula on the characteristic equation:\n$$\n\\gamma = \\frac{-(r - q - \\frac{1}{2}\\sigma^2) \\pm \\sqrt{(r - q - \\frac{1}{2}\\sigma^2)^2 - 4(\\frac{1}{2}\\sigma^2)(-r)}}{\\sigma^2}\n$$\nThe negative root $\\gamma_2$ is obtained by taking the minus sign in the numerator:\n$$\n\\gamma_2 = \\frac{-(r-q-\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}}{\\sigma^2}\n$$\nSubstituting this into the expression for $s^*$ gives the complete solution. To present the final answer as a single expression, we substitute $\\gamma_2$ into the formula for $s^*$. Let's simplify the fraction $\\frac{\\gamma_2}{\\gamma_2 - 1}$ by multiplying the numerator and denominator by $\\sigma^2$:\n$$\ns^* = K \\left( \\frac{\\sigma^2 \\gamma_2}{\\sigma^2 (\\gamma_2 - 1)} \\right) = K \\left( \\frac{\\sigma^2 \\gamma_2}{\\sigma^2 \\gamma_2 - \\sigma^2} \\right)\n$$\nLet $\\Omega = -(r-q-\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}$. Then $\\sigma^2 \\gamma_2 = \\Omega$.\n$$\ns^* = K \\left( \\frac{\\Omega}{\\Omega - \\sigma^2} \\right)\n$$\nSubstituting back for $\\Omega$:\n$$\ns^* = K \\left( \\frac{-(r-q-\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}}{-(r-q-\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2} - \\sigma^2} \\right)\n$$\nThe denominator simplifies to:\n$$\n-(r-q+\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}\n$$\nThis gives the final closed-form expression for the optimal exercise boundary $s^*$.", "answer": "$$\n\\boxed{K \\left( \\frac{-(r-q-\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}}{-(r-q+\\frac{1}{2}\\sigma^2) - \\sqrt{(r-q-\\frac{1}{2}\\sigma^2)^2 + 2r\\sigma^2}} \\right)}\n$$", "id": "3051361"}]}