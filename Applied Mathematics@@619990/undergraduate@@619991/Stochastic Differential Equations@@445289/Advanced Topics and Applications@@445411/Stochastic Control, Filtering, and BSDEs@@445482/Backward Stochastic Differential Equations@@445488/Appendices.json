{"hands_on_practices": [{"introduction": "The journey into understanding Backward Stochastic Differential Equations (BSDEs) begins with mastering the fundamental analytical solution techniques. This first exercise provides a cornerstone method for a broad class of linear BSDEs, where the driver function does not depend on the solution processes $Y$ and $Z$. By skillfully applying the properties of conditional expectation and martingales, we can derive a general explicit formula for the solution $Y_t$, transforming the BSDE problem into a more familiar calculation of an expected value [@problem_id:3040153].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{0\\le t\\le T},\\mathbb{P})$ satisfying the usual conditions and carrying a one-dimensional standard Brownian motion $W$. Let $\\xi\\in L^{2}(\\Omega,\\mathcal{F}_{T},\\mathbb{P})$ be a square-integrable terminal random variable and let $c=(c_{s})_{0\\le s\\le T}$ be a predictable process with $\\mathbb{E}\\left[\\int_{0}^{T}|c_{s}|^{2}\\,ds\\right]\\infty$. Consider the backward stochastic differential equation (BSDE) with generator $f(s,y,z)=c_{s}$:\n$$\nY_{t} \\;=\\; \\xi \\;+\\; \\int_{t}^{T} c_{s}\\,ds \\;-\\; \\int_{t}^{T} Z_{s}\\,dW_{s}, \\qquad 0\\le t\\le T.\n$$\nStarting from the definition of a BSDE and the martingale properties of stochastic integrals with respect to Brownian motion, derive an expression for $Y_{t}$ in terms of a conditional expectation that depends only on $\\xi$, $c$, and $\\mathcal{F}_{t}$, and explain how to identify $Z$ via the martingale representation of an appropriate square-integrable martingale.\n\nThen, specialize to the case in which the terminal condition and the generator are given by\n$$\n\\xi \\;=\\; \\alpha W_{T} + \\beta, \\qquad c_{s} \\;=\\; \\rho s + \\eta,\n$$\nwhere $\\alpha,\\beta,\\rho,\\eta\\in\\mathbb{R}$ are constants. Using only fundamental properties of Brownian motion and conditional expectation in the Brownian filtration, compute the explicit process $Y_{t}$ and identify the corresponding process $Z_{t}$.\n\nExpress your final answer as a single analytic expression. No rounding is required.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{0\\le t\\le T},\\mathbb{P})$ satisfying the usual conditions.\n- A one-dimensional standard Brownian motion $W = (W_t)_{0 \\le t \\le T}$.\n- A terminal condition $\\xi\\in L^{2}(\\Omega,\\mathcal{F}_{T},\\mathbb{P})$.\n- A generator function $f(s,y,z)=c_{s}$.\n- The process $c = (c_{s})_{0\\le s\\le T}$ is predictable with $\\mathbb{E}\\left[\\int_{0}^{T}|c_{s}|^{2}\\,ds\\right]\\infty$.\n- The Backward Stochastic Differential Equation (BSDE) is given by:\n$$\nY_{t} \\;=\\; \\xi \\;+\\; \\int_{t}^{T} c_{s}\\,ds \\;-\\; \\int_{t}^{T} Z_{s}\\,dW_{s}, \\qquad 0\\le t\\le T.\n$$\n- A specific case is defined with:\n  - $\\xi = \\alpha W_{T} + \\beta$\n  - $c_{s} = \\rho s + \\eta$\n  - $\\alpha,\\beta,\\rho,\\eta\\in\\mathbb{R}$ are constants.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem a standard one in the theory of Backward Stochastic Differential Equations, a well-established subfield of stochastic analysis. The generator is linear, and the conditions on $\\xi$ and $c$ are standard for ensuring the existence and uniqueness of a solution pair $(Y, Z)$. The problem is mathematically and scientifically sound.\n- **Well-Posed:** The problem is well-posed. The standard existence and uniqueness theorem for linear BSDEs guarantees that for a square-integrable terminal condition $\\xi$ and a generator coefficient process $c$ that is square-integrable in expectation, a unique adapted solution pair $(Y, Z)$ exists such that $Y$ is a continuous process and $\\mathbb{E}\\left[\\int_0^T |Z_s|^2 ds\\right]  \\infty$.\n- **Objective:** The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency:** The problem provides all necessary definitions, variables, and conditions to derive the solution. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### General Derivation\nThe BSDE is given by\n$$\nY_{t} = \\xi + \\int_{t}^{T} c_{s}\\,ds - \\int_{t}^{T} Z_{s}\\,dW_{s}.\n$$\nWe can rearrange this equation as\n$$\nY_{t} + \\int_{t}^{T} Z_{s}\\,dW_{s} = \\xi + \\int_{t}^{T} c_{s}\\,ds.\n$$\nLet's take the conditional expectation with respect to the filtration $\\mathcal{F}_t$ on both sides of the equation:\n$$\n\\mathbb{E}\\left[Y_{t} + \\int_{t}^{T} Z_{s}\\,dW_{s} \\Big| \\mathcal{F}_{t}\\right] = \\mathbb{E}\\left[\\xi + \\int_{t}^{T} c_{s}\\,ds \\Big| \\mathcal{F}_{t}\\right].\n$$\nUsing the linearity of conditional expectation, we get\n$$\n\\mathbb{E}\\left[Y_{t} | \\mathcal{F}_{t}\\right] + \\mathbb{E}\\left[\\int_{t}^{T} Z_{s}\\,dW_{s} \\Big| \\mathcal{F}_{t}\\right] = \\mathbb{E}\\left[\\xi + \\int_{t}^{T} c_{s}\\,ds \\Big| \\mathcal{F}_{t}\\right].\n$$\nBy definition of the solution $(Y, Z)$ to a BSDE, the process $Y$ is adapted to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$, meaning $Y_t$ is $\\mathcal{F}_t$-measurable for each $t$. Therefore, $\\mathbb{E}[Y_t | \\mathcal{F}_t] = Y_t$.\nThe stochastic integral term $\\int_{t}^{T} Z_{s}\\,dW_{s}$ can be written as $\\int_{0}^{T} Z_{s}\\,dW_{s} - \\int_{0}^{t} Z_{s}\\,dW_{s}$. The process $M_u = \\int_0^u Z_s dW_s$ is a martingale under the given integrability condition on $Z$. A fundamental property of martingales is that for $u > t$, $\\mathbb{E}[M_u|\\mathcal{F}_t] = M_t$. Thus, the conditional expectation of a future increment is zero:\n$$\n\\mathbb{E}\\left[\\int_{t}^{T} Z_{s}\\,dW_{s} \\Big| \\mathcal{F}_{t}\\right] = \\mathbb{E}\\left[M_T - M_t \\Big| \\mathcal{F}_{t}\\right] = \\mathbb{E}[M_T|\\mathcal{F}_t] - \\mathbb{E}[M_t|\\mathcal{F}_t] = M_t - M_t = 0.\n$$\nSubstituting these results back into the equation yields the expression for $Y_t$:\n$$\nY_{t} = \\mathbb{E}\\left[\\xi + \\int_{t}^{T} c_{s}\\,ds \\Big| \\mathcal{F}_{t}\\right].\n$$\nTo identify the process $Z$, we consider the martingale $M_{t} = \\mathbb{E}\\left[\\xi + \\int_{0}^{T} c_{u}\\,du \\Big| \\mathcal{F}_{t}\\right]$. We can show that $M_t = Y_t + \\int_0^t c_s ds$.\nThe BSDE can be written in differential form as $dY_t = -c_t dt + Z_t dW_t$.\nLet us define a process $M'_t = Y_t + \\int_0^t c_s ds$. Using Itô's rule for $dM'_t$:\n$$\ndM'_{t} = dY_t + c_t dt = (-c_t dt + Z_t dW_t) + c_t dt = Z_t dW_t.\n$$\nIntegrating from $0$ to $t$ gives $M'_t = M'_0 + \\int_0^t Z_s dW_s$. This shows that $M'_t$ is a martingale. Since $Y_t$ must be continuous and $Y_T = \\xi$, we have $M'_T = Y_T + \\int_0^T c_s ds = \\xi + \\int_0^T c_s ds$. As $M'_t$ is a martingale, $M'_t = \\mathbb{E}[M'_T | \\mathcal{F}_t] = \\mathbb{E}[\\xi + \\int_0^T c_s ds | \\mathcal{F}_t]$.\nSo, $M'_{t} = M_t$. The process $Z$ is therefore the integrand in the martingale representation of $M_{t} = \\mathbb{E}\\left[\\xi + \\int_{0}^{T} c_{u}\\,du \\Big| \\mathcal{F}_{t}\\right]$.\n\n### Specific Case\nWe now specialize to the case where $\\xi = \\alpha W_{T} + \\beta$ and $c_{s} = \\rho s + \\eta$.\n\nFirst, we compute the process $Y_t$ using the general formula derived above:\n$$\nY_{t} = \\mathbb{E}\\left[ (\\alpha W_{T} + \\beta) + \\int_{t}^{T} (\\rho s + \\eta)\\,ds \\Big| \\mathcal{F}_{t} \\right].\n$$\nThe integral term is deterministic and can be computed explicitly:\n$$\n\\int_{t}^{T} (\\rho s + \\eta)\\,ds = \\left[ \\frac{\\rho s^2}{2} + \\eta s \\right]_{s=t}^{s=T} = \\left(\\frac{\\rho T^2}{2} + \\eta T\\right) - \\left(\\frac{\\rho t^2}{2} + \\eta t\\right) = \\frac{\\rho}{2}(T^2-t^2) + \\eta(T-t).\n$$\nSince this term is deterministic and known at time $t$, it is $\\mathcal{F}_t$-measurable and can be taken out of the conditional expectation:\n$$\nY_{t} = \\mathbb{E}\\left[ \\alpha W_T + \\beta \\Big| \\mathcal{F}_t \\right] + \\frac{\\rho}{2}(T^2-t^2) + \\eta(T-t).\n$$\nNow we compute the conditional expectation term. We decompose $W_T$ as $W_T = W_t + (W_T - W_t)$.\n$$\n\\mathbb{E}\\left[ \\alpha W_T + \\beta \\Big| \\mathcal{F}_t \\right] = \\mathbb{E}\\left[ \\alpha (W_t + W_T - W_t) + \\beta \\Big| \\mathcal{F}_t \\right].\n$$\nBy linearity of conditional expectation:\n$$\n\\mathbb{E}\\left[ \\alpha W_t \\Big| \\mathcal{F}_t \\right] + \\mathbb{E}\\left[ \\alpha(W_T - W_t) \\Big| \\mathcal{F}_t \\right] + \\mathbb{E}\\left[ \\beta \\Big| \\mathcal{F}_t \\right].\n$$\nSince $W_t$ is $\\mathcal{F}_t$-measurable, $\\mathbb{E}[\\alpha W_t | \\mathcal{F}_t] = \\alpha W_t$. The constant $\\beta$ is also $\\mathcal{F}_t$-measurable, so $\\mathbb{E}[\\beta | \\mathcal{F}_t] = \\beta$. The Brownian increment $W_T - W_t$ is independent of the filtration $\\mathcal{F}_t$ and has mean zero. Therefore, $\\mathbb{E}[\\alpha(W_T - W_t) | \\mathcal{F}_t] = \\alpha \\mathbb{E}[W_T-W_t] = \\alpha \\cdot 0 = 0$.\nCombining these gives $\\mathbb{E}\\left[ \\alpha W_T + \\beta \\Big| \\mathcal{F}_t \\right] = \\alpha W_t + \\beta$.\nSubstituting this back into the expression for $Y_t$:\n$$\nY_{t} = \\alpha W_t + \\beta + \\frac{\\rho}{2}(T^2-t^2) + \\eta(T-t).\n$$\nTo find the process $Z_t$, we can compute the stochastic differential of $Y_t$ and match it to the SDE form of the BSDE, $dY_t = -c_t dt + Z_t dW_t$.\nThe differential form of $Y_t$ is:\n$$\ndY_t = d\\left( \\alpha W_t + \\beta + \\eta T - \\eta t + \\frac{\\rho}{2} T^2 - \\frac{\\rho}{2} t^2 \\right).\n$$\nApplying Itô's differentiation rules (noting that $W_t$ has quadratic variation $dt$, but here it appears linearly, and $t$ has zero quadratic variation):\n$$\ndY_t = \\alpha dW_t - \\eta dt - \\frac{\\rho}{2}(2t)dt = \\alpha dW_t - (\\rho t + \\eta) dt.\n$$\nThe SDE form of the BSDE is $dY_t = Z_t dW_t - c_t dt$. In this specific case, $c_t = \\rho t + \\eta$, so\n$$\ndY_t = Z_t dW_t - (\\rho t + \\eta) dt.\n$$\nBy comparing the two expressions for $dY_t$, we can identify the coefficients of the $dW_t$ and $dt$ terms. The drift terms $-(\\rho t + \\eta) dt$ are identical. Comparing the diffusion terms gives:\n$$\nZ_t dW_t = \\alpha dW_t.\n$$\nThis implies that the process $Z_t$ is the constant $\\alpha$.\nThus, the solution pair is $(Y_t, Z_t)$ where $Y_t = \\alpha W_t + \\beta + \\eta(T-t) + \\frac{\\rho}{2}(T^2-t^2)$ and $Z_t = \\alpha$.\n\nThe final answer is the pair of processes $(Y_t, Z_t)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\alpha W_{t} + \\beta + \\eta(T-t) + \\frac{\\rho}{2}(T^{2} - t^{2})  \\alpha \\end{pmatrix}}\n$$", "id": "3040153"}, {"introduction": "One of the most powerful aspects of BSDE theory is its deep connection to the world of partial differential equations (PDEs), a link often formalized by the Feynman-Kac formula. This practice guides you through discovering this connection in a hands-on manner. By postulating that the solution has a Markovian structure, $Y_t = u(t, X_t)$, and applying the indispensable Itô's formula, you will see how the components of the BSDE elegantly map to the partial derivatives of the function $u(t,x)$ [@problem_id:3040157].", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P}\\right)$ be a filtered probability space supporting a one-dimensional standard Brownian motion $W$. Fix constants $\\sigma0$, $\\beta\\in\\mathbb{R}$, $T0$, and $x\\in\\mathbb{R}$. Consider the forward stochastic differential equation $dX_{t}=\\sigma\\,dW_{t}$ with $X_{0}=x$, and the backward stochastic differential equation (BSDE) with zero driver and terminal condition $Y_{T}=g(X_{T})$ where $g(x)=\\exp(\\beta x)$.\n\nDefine the Markovian candidate $u(t,x):=\\mathbb{E}\\!\\left[g(X_{T})\\mid X_{t}=x\\right]$ and the process $Y_{t}:=u(t,X_{t})$. Using only Itô’s formula, properties of conditional expectations, and the stochastic differential equations definitions above, carry out the following steps:\n\n- Compute the explicit closed-form expression for $u(t,x)$ and verify that $u$ is differentiable in $t$ and twice differentiable in $x$.\n- Apply Itô’s formula to $u(t,X_{t})$ to identify the $(\\mathcal{F}_{t})$-adapted process $Z_{t}$ appearing in the martingale representation $dY_{t}=Z_{t}\\,dW_{t}$ for the zero-driver BSDE with $Y_{T}=g(X_{T})$.\n- Provide the resulting closed-form analytic expression for $Z_{t}$ in terms of $t$, $X_{t}$, and the parameters.\n\nYour final answer must be a single closed-form analytic expression for $Z_{t}$. Do not approximate or round.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and objective.\n\n### Step 1: Extract Givens\n- A filtered probability space $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P}\\right)$.\n- A one-dimensional standard Brownian motion $W$.\n- Constants $\\sigma0$, $\\beta\\in\\mathbb{R}$, $T0$, and $x\\in\\mathbb{R}$.\n- A forward stochastic differential equation (SDE): $dX_{t}=\\sigma\\,dW_{t}$ with initial condition $X_{0}=x$.\n- A backward stochastic differential equation (BSDE) with zero driver and terminal condition $Y_{T}=g(X_{T})$, where $g(x)=\\exp(\\beta x)$.\n- A Markovian candidate function $u(t,x):=\\mathbb{E}\\!\\left[g(X_{T})\\mid X_{t}=x\\right]$.\n- A process $Y_{t}:=u(t,X_{t})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the established mathematical theory of stochastic differential equations, specifically Itô calculus and backward stochastic differential equations. The forward SDE is a standard model for a variable driven by Brownian motion. The terminal condition is a well-behaved exponential function. The definition of the value function $u(t,x)$ via conditional expectation is a cornerstone of the connection between SDEs and partial differential equations (PDEs), often related to the Feynman-Kac formula. The problem is well-posed; it provides all necessary information (dynamics of $X_t$, terminal function $g$, definitions of $u$ and $Y_t$) to uniquely determine the process $Z_t$. The language is precise and objective. The problem does not contain any scientific falsehoods, contradictions, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution follows the three steps outlined in the problem statement.\n\n**Part 1: Compute the explicit expression for $u(t,x)$**\n\nThe forward process is given by the SDE $dX_{s}=\\sigma\\,dW_{s}$. To find the distribution of $X_{T}$ conditional on $X_{t}=x$, we solve this SDE over the interval $[t, T]$ with the starting condition $X_{t}=x$.\nIntegrating the SDE from $t$ to $T$ yields:\n$$ X_{T} = X_{t} + \\int_{t}^{T} \\sigma \\,dW_{s} $$\nGiven $X_{t}=x$, we have:\n$$ X_{T} = x + \\sigma (W_{T}-W_{t}) $$\nThe increment of a standard Brownian motion, $W_{T}-W_{t}$, is a normally distributed random variable with mean $0$ and variance $T-t$. That is, $W_{T}-W_{t} \\sim \\mathcal{N}(0, T-t)$.\nConsequently, conditional on $X_{t}=x$, the random variable $X_{T}$ is also normally distributed:\n$$ X_{T} \\mid (X_{t}=x) \\sim \\mathcal{N}(x, \\sigma^2(T-t)) $$\nThe function $u(t,x)$ is defined as the conditional expectation of $g(X_T)$:\n$$ u(t,x) = \\mathbb{E}\\!\\left[g(X_{T})\\mid X_{t}=x\\right] = \\mathbb{E}\\!\\left[\\exp(\\beta X_{T})\\mid X_{t}=x\\right] $$\nThis is the moment-generating function of the normal distribution $\\mathcal{N}(x, \\sigma^2(T-t))$ evaluated at $\\beta$. For a random variable $V \\sim \\mathcal{N}(\\mu, \\Sigma^2)$, its moment-generating function is $M_V(k) = \\mathbb{E}[\\exp(kV)] = \\exp(k\\mu + \\frac{1}{2}k^2\\Sigma^2)$.\nIn our case, $\\mu=x$, $\\Sigma^2 = \\sigma^2(T-t)$, and $k=\\beta$. Therefore, the explicit closed-form expression for $u(t,x)$ is:\n$$ u(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nTo verify differentiability, we compute the partial derivatives:\n$$ \\frac{\\partial u}{\\partial t}(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) \\cdot \\left(-\\frac{1}{2}\\beta^2\\sigma^2\\right) = -\\frac{1}{2}\\beta^2\\sigma^2 u(t,x) $$\n$$ \\frac{\\partial u}{\\partial x}(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) \\cdot \\beta = \\beta u(t,x) $$\n$$ \\frac{\\partial^2 u}{\\partial x^2}(t,x) = \\frac{\\partial}{\\partial x}(\\beta u(t,x)) = \\beta^2 u(t,x) = \\beta^2 \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nSince the exponential function is infinitely differentiable and all parameters are constants, these derivatives exist and are continuous for $(t,x) \\in [0,T) \\times \\mathbb{R}$. Thus, $u$ is at least $C^{1,2}$, satisfying the differentiability requirement.\n\n**Part 2: Apply Itô’s formula to $Y_t = u(t,X_t)$**\n\nWe apply Itô’s formula for a function of time and a stochastic process. For $Y_t = u(t, X_t)$, the formula is:\n$$ dY_{t} = \\frac{\\partial u}{\\partial t}(t,X_{t})\\,dt + \\frac{\\partial u}{\\partial x}(t,X_{t})\\,dX_{t} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,X_{t})\\,d\\langle X \\rangle_{t} $$\nWe have the dynamics of $X_t$, which is $dX_t = \\sigma\\,dW_t$. The quadratic variation of $X_t$ is $d\\langle X \\rangle_{t} = (\\sigma)^2\\,dt = \\sigma^2\\,dt$.\nNow, we substitute the expressions for the partial derivatives (evaluated at $(t, X_t)$) and the dynamics of $X_t$ into Itô's formula:\n$$ dY_{t} = \\left(-\\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t})\\right)dt + \\left(\\beta u(t,X_{t})\\right)(\\sigma\\,dW_{t}) + \\frac{1}{2}\\left(\\beta^2 u(t,X_{t})\\right)(\\sigma^2\\,dt) $$\nWe collect the $dt$ and $dW_t$ terms:\n$$ dY_{t} = \\left(-\\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t}) + \\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t})\\right)dt + \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t} $$\nThe terms in the coefficient of $dt$ cancel out:\n$$ dY_{t} = (0)\\,dt + \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t} $$\n$$ dY_{t} = \\beta \\sigma u(t,X_{t})\\,dW_{t} $$\nThe problem states that the pair $(Y_t, Z_t)$ solves the BSDE, which in the general case is written as $dY_t = f(t, X_t, Y_t, Z_t)dt + Z_t dW_t$. For a zero-driver BSDE, $f=0$, so the representation is $dY_t = Z_t dW_t$. By comparing this form with our result from Itô's formula, we can identify the process $Z_t$.\n\n**Part 3: Provide the closed-form analytic expression for $Z_t$**\n\nFrom the previous step, we have $dY_{t} = \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t}$. Comparing this with the martingale representation $dY_t = Z_t dW_t$, we identify $Z_t$ as:\n$$ Z_t = \\beta \\sigma u(t,X_{t}) $$\nSubstituting the explicit expression for $u(t,X_{t})$:\n$$ Z_t = \\beta \\sigma \\exp\\left(\\beta X_t + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nThis is the required closed-form analytic expression for the process $Z_t$ in terms of $t$, $X_t$, and the given parameters.", "answer": "$$\\boxed{\\beta\\sigma\\exp\\left(\\beta X_{t} + \\frac{1}{2}\\beta^{2}\\sigma^{2}(T-t)\\right)}$$", "id": "3040157"}, {"introduction": "While analytical solutions provide crucial insight, many BSDEs encountered in finance and control theory are too complex to be solved with pen and paper. This exercise bridges the gap between theory and practice by introducing a powerful numerical scheme to approximate BSDE solutions. You will implement a regression-based method, which discretizes time and uses least-squares regression at each step to estimate the unknown processes, giving you first-hand experience with the computational tools used in modern quantitative analysis [@problem_id:3040102].", "problem": "Consider a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$ on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$. Let the forward process $X=(X_t)_{t\\in[0,T]}$ satisfy the stochastic differential equation $dX_t=\\sigma\\,dW_t$ with $X_0=x_0$ and constant volatility $\\sigma0$. Consider the Backward Stochastic Differential Equation (BSDE) defined by $Y_t=g(X_T)+\\int_t^T f(s,Y_s,Z_s)\\,ds-\\int_t^T Z_s\\,dW_s$, where $g:\\mathbb{R}\\to\\mathbb{R}$ is a terminal payoff and $f:[0,T]\\times\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ is a driver function. In this problem, take the terminal function $g(x)=x^2$ and the driver $f(t,y,z)=\\lambda\\,y+\\mu\\,z$ for constants $\\lambda\\in\\mathbb{R}$ and $\\mu\\in\\mathbb{R}$.\n\nStarting from the fundamental definition of a Backward Stochastic Differential Equation (BSDE) and the standard Euler discretization for stochastic differential equations, construct an Euler time discretization for the BSDE on a uniform grid $0=t_0t_1\\cdotst_M=T$ with time step $\\Delta t=T/M$. Use the backward recursion $Y_{t_k}=\\mathbb{E}\\!\\left[Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},Z_{t_k})\\,\\Delta t\\mid\\mathcal{F}_{t_k}\\right]$ where $Z_{t_k}$ is approximated via regression on a fixed polynomial basis of the forward state $X_{t_k}$. Specifically, estimate $Z_{t_k}$ using Least Squares (LS) regression of the target $U_k=\\frac{Y_{t_{k+1}}\\,\\Delta W_k}{\\Delta t}$ onto the basis $\\phi(X_{t_k})=(1,X_{t_k},X_{t_k}^2)$, where $\\Delta W_k=W_{t_{k+1}}-W_{t_k}$. Then estimate $Y_{t_k}$ by LS regression of the target $V_k=Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},\\widehat{Z}_{t_k})\\,\\Delta t$ onto the same basis, with $\\widehat{Z}_{t_k}$ the regression estimate.\n\nYour program must:\n- Simulate $N$ independent paths of the forward process $X$ using the Euler scheme with the given parameters.\n- At $t_M=T$, set $Y_{t_M}=g(X_{t_M})$ pathwise.\n- For $k=M-1,M-2,\\ldots,0$, perform the two-stage LS regression described above to obtain pathwise estimates of $\\widehat{Z}_{t_k}$ and $\\widehat{Y}_{t_k}$. Use the polynomial basis $\\phi(x)=(1,x,x^2)$ at each time level.\n- Return the final estimate of $Y_{t_0}$ as a single float for each test case.\n\nAll random number generation must be reproducible by fixing the seed specified in each test case. All quantities in this problem are dimensionless, so no unit specification is required.\n\nImplement the algorithm for the following test suite of parameter values, using the notation $(T,M,N,x_0,\\sigma,\\lambda,\\mu,\\text{seed})$:\n- Test case $1$: $(T=\\;1,\\;M=\\;40,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0,\\;\\mu=\\;0,\\;\\text{seed}=\\;12345)$, a baseline case where the true value satisfies $Y_{0}=\\mathbb{E}[W_T^2]=T$.\n- Test case $2$: $(T=\\;0.5,\\;M=\\;30,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0.3,\\;\\mu=\\;0,\\;\\text{seed}=\\;23456)$, a case with a nonzero linear dependence on $Y$ in the driver.\n- Test case $3$: $(T=\\;1,\\;M=\\;40,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0,\\;\\mu=\\;0.2,\\;\\text{seed}=\\;34567)$, a case with a nonzero linear dependence on $Z$ in the driver.\n- Test case $4$: $(T=\\;0.1,\\;M=\\;10,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0.5,\\;\\mu=\\;-0.1,\\;\\text{seed}=\\;45678)$, a small-horizon edge case with mixed signs in the driver coefficients.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the estimated value of $Y_{0}$ for the corresponding test case.", "solution": "The user has provided a well-defined problem in the domain of numerical methods for stochastic differential equations, specifically focusing on Backward Stochastic Differential Equations (BSDEs).\n\n### Step 1: Extract Givens\n-   **Forward Process**: A one-dimensional stochastic process $X=(X_t)_{t\\in[0,T]}$ governed by the Stochastic Differential Equation (SDE) $dX_t=\\sigma\\,dW_t$ with initial condition $X_0=x_0$. $W_t$ is a standard Brownian motion and $\\sigma0$ is a constant.\n-   **Backward Stochastic Differential Equation (BSDE)**: The pair of processes $(Y_t, Z_t)$ must satisfy $Y_t=g(X_T)+\\int_t^T f(s,Y_s,Z_s)\\,ds-\\int_t^T Z_s\\,dW_s$.\n-   **Terminal Condition**: The function $g:\\mathbb{R}\\to\\mathbb{R}$ is given by $g(x)=x^2$.\n-   **Driver Function**: The function $f:[0,T]\\times\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ is given by $f(t,y,z)=\\lambda\\,y+\\mu\\,z$ for constants $\\lambda,\\mu\\in\\mathbb{R}$.\n-   **Numerical Scheme**: An Euler time discretization on a uniform grid $0=t_0t_1\\cdotst_M=T$ with time step $\\Delta t=T/M$.\n-   **Discretized Backward Recursion**: $Y_{t_k}=\\mathbb{E}\\!\\left[Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},Z_{t_k})\\,\\Delta t\\mid\\mathcal{F}_{t_k}\\right]$.\n-   **Approximation of $Z_{t_k}$**: $Z_{t_k}$ is estimated via Least Squares (LS) regression. The regression target is $U_k=\\frac{Y_{t_{k+1}}\\,\\Delta W_k}{\\Delta t}$, where $\\Delta W_k=W_{t_{k+1}}-W_{t_k}$. The regression is performed on the polynomial basis $\\phi(X_{t_k})=(1,X_{t_k},X_{t_k}^2)$.\n-   **Approximation of $Y_{t_k}$**: $Y_{t_k}$ is estimated via LS regression. The regression target is $V_k=Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},\\widehat{Z}_{t_k})\\,\\Delta t$, using the previously estimated $\\widehat{Z}_{t_k}$. The regression is performed on the same basis $\\phi(X_{t_k})$.\n-   **Simulation Parameters**: $N$ independent paths are simulated. The process is repeated for several tuples of parameters $(T,M,N,x_0,\\sigma,\\lambda,\\mu,\\text{seed})$.\n-   **Output**: The final estimate of $Y_{t_0}$ for each parameter set.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Groundedness**: The problem is firmly rooted in the theory of stochastic calculus and numerical analysis. BSDEs are a well-established field, and regression-based numerical methods for their solution, akin to the Longstaff-Schwartz algorithm for American options, are standard techniques. The specified method is a valid numerical scheme.\n-   **Well-Posedness**: The problem statement provides a complete and unambiguous description of an algorithm. All required parameters, functions, and procedures are defined, ensuring that a unique computational result can be obtained. The underlying continuous BSDE with a linear driver and quadratic terminal condition is known to be well-posed.\n-   **Objectivity**: The problem is stated using precise mathematical terminology and is free from subjective or opinion-based content.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. For instance, the first test case $(\\lambda=0, \\mu=0)$ serves as a sound sanity check. In this case, the BSDE simplifies to $Y_t = \\mathbb{E}[g(X_T) \\mid \\mathcal{F}_t] = \\mathbb{E}[X_T^2 \\mid \\mathcal{F}_t]$. With $X_t = x_0 + \\sigma W_t$, the analytical solution at $t=0$ is $Y_0 = \\mathbb{E}[X_T^2] = \\mathbb{E}[(x_0 + \\sigma W_T)^2] = x_0^2 + \\sigma^2 \\mathbb{E}[W_T^2] = x_0^2 + \\sigma^2 T$. For the test case parameters $(T=1, x_0=0, \\sigma=1)$, the true value is $Y_0=1$. This provides a benchmark against which the numerical implementation can be partially verified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided by implementing the specified numerical algorithm.\n\n### Solution Derivation\n\nThe problem requires the implementation of a regression-based numerical method to solve a specific BSDE. The method proceeds backward in time, from $T$ to $0$.\n\n1.  **Forward Path Simulation**: First, we simulate $N$ paths of the forward process $X_t$. The SDE is $dX_t = \\sigma dW_t$ with $X_0 = x_0$. Using an Euler-Maruyama scheme on the time grid $t_k = k\\Delta t$ for $k=0, \\dots, M$, we have:\n    $$\n    X_{t_{k+1}} = X_{t_k} + \\sigma \\Delta W_k\n    $$\n    where $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$ are independent random variables drawn from a normal distribution $\\mathcal{N}(0, \\Delta t)$. We generate and store the paths $\\{X_{t_k}^{(i)}\\}_{k=0,\\dots,M}$ for each simulation $i=1,\\dots,N$.\n\n2.  **Terminal Condition**: The backward recursion starts at the terminal time $t_M = T$. For each path $i$, the value of $Y$ is given by the terminal function $g$:\n    $$\n    \\widehat{Y}_{t_M}^{(i)} = g(X_{t_M}^{(i)}) = (X_{t_M}^{(i)})^2\n    $$\n    This gives us a vector of $N$ values for $Y$ at time $T$.\n\n3.  **Backward Recursion Loop**: We iterate backward from $k = M-1$ down to $0$. At each step $k$, we have the estimates $\\widehat{Y}_{t_{k+1}}^{(i)}$ from the previous step (time $t_{k+1}$) and aim to compute $\\widehat{Y}_{t_k}^{(i)}$. This is done in two stages.\n\n    **Stage A: Estimating $Z_{t_k}$**\n    The process $Z_t$ is related to $Y_t$ and $W_t$ through the martingale representation part of the BSDE. A common approximation stems from the discretized SDE for $Y_t$: $Y_{t_{k+1}} - Y_{t_k} \\approx -f(\\dots)\\Delta t + Z_{t_k}\\Delta W_k$. Multiplying by $\\Delta W_k$ and taking conditional expectation $\\mathbb{E}[\\cdot \\mid \\mathcal{F}_{t_k}]$, we use $\\mathbb{E}[\\Delta W_k \\mid \\mathcal{F}_{t_k}]=0$ and $\\mathbb{E}[(\\Delta W_k)^2 \\mid \\mathcal{F}_{t_k}]=\\Delta t$ to show that $Z_{t_k} \\approx \\frac{1}{\\Delta t} \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}]$.\n    The problem specifies approximating this conditional expectation by assuming $Z_{t_k}$ is a function of $X_{t_k}$ within the space spanned by $\\phi(x) = (1, x, x^2)$. We perform a least-squares regression of the sample values of the target $U_k^{(i)} = \\frac{\\widehat{Y}_{t_{k+1}}^{(i)}\\Delta W_k^{(i)}}{\\Delta t}$ on the basis functions evaluated at the states $X_{t_k}^{(i)}$. Let $\\mathbf{A}_k$ be the $N \\times 3$ matrix whose rows are $\\phi(X_{t_k}^{(i)})$. We solve for the coefficient vector $\\mathbf{c}_{Z,k}$:\n    $$\n    \\mathbf{c}_{Z,k} = \\arg\\min_{\\mathbf{c} \\in \\mathbb{R}^3} \\sum_{i=1}^N \\left( \\left( \\mathbf{A}_k \\mathbf{c} \\right)_i - U_k^{(i)} \\right)^2\n    $$\n    The pathwise estimates for $Z_{t_k}$ are then given by $\\widehat{\\mathbf{Z}}_{t_k} = \\mathbf{A}_k \\mathbf{c}_{Z,k}$.\n\n    **Stage B: Estimating $Y_{t_k}$**\n    The recursion for $Y_{t_k}$ is given by $Y_{t_k} = \\mathbb{E}[V_k \\mid \\mathcal{F}_{t_k}]$, where $V_k = Y_{t_{k+1}} + f(t_k, Y_{t_{k+1}}, \\widehat{Z}_{t_k}) \\Delta t$.\n    Again, we approximate the conditional expectation via regression on the same basis $\\phi(X_{t_k})$. We compute the sample values of the target for each path:\n    $$\n    V_k^{(i)} = \\widehat{Y}_{t_{k+1}}^{(i)} + \\left( \\lambda \\widehat{Y}_{t_{k+1}}^{(i)} + \\mu \\widehat{Z}_{t_k}^{(i)} \\right) \\Delta t\n    $$\n    We then solve the least-squares problem for the coefficient vector $\\mathbf{c}_{Y,k}$:\n    $$\n    \\mathbf{c}_{Y,k} = \\arg\\min_{\\mathbf{c} \\in \\mathbb{R}^3} \\sum_{i=1}^N \\left( \\left( \\mathbf{A}_k \\mathbf{c} \\right)_i - V_k^{(i)} \\right)^2\n    $$\n    The new estimates for $Y$ at time $t_k$ are $\\widehat{\\mathbf{Y}}_{t_k} = \\mathbf{A}_k \\mathbf{c}_{Y,k}$. This vector of estimates is then used as $\\widehat{\\mathbf{Y}}_{t_{k+1}}$ in the next iteration of the loop (for time $t_{k-1}$).\n\n4.  **Final Result**: The loop continues until $k = 0$. At this final step, we compute the coefficients $\\mathbf{c}_{Y,0}$. The estimated value of $Y_0$ is the value of the resulting regression function evaluated at the initial state $X_0 = x_0$. The estimated function is $\\widehat{y}_0(x) = (\\mathbf{c}_{Y,0})_1 \\cdot 1 + (\\mathbf{c}_{Y,0})_2 \\cdot x + (\\mathbf{c}_{Y,0})_3 \\cdot x^2$. Since all test cases have $x_0=0$, the regression matrix $\\mathbf{A}_0$ consists of $N$ identical rows $(1, 0, 0)$. In this specific case, evaluating $\\widehat{y}_0(0)$ simply yields the first coefficient, $(\\mathbf{c}_{Y,0})_1$. More generally, the least-squares solution for the evaluated function $\\widehat{y}_0(x_0)$ is precisely the sample mean of the regression targets $V_0^{(i)}$. The implemented algorithm will compute `poly_basis @ coeffs_Y`, which correctly evaluates to this mean value for all paths, as $\\mathbf{A}_0 \\mathbf{c}_{Y,0}$ simplifies to this result. Therefore, the estimate for $Y_0$ is the first (and only unique) element of the final vector $\\widehat{\\mathbf{Y}}_{t_0}$.\n\nThis procedure is implemented for each set of parameters provided in the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve_bsde_regression(T, M, N, x0, sigma, lam, mu, seed):\n    \"\"\"\n    Solves a Backward Stochastic Differential Equation (BSDE) using a\n    regression-based Euler scheme (Longstaff-Schwartz type method).\n\n    Parameters:\n    T (float): Time horizon.\n    M (int): Number of time steps.\n    N (int): Number of simulated paths.\n    x0 (float): Initial value of the forward process X.\n    sigma (float): Volatility of the forward process X.\n    lam (float): Coefficient for the Y term in the BSDE driver.\n    mu (float): Coefficient for the Z term in the BSDE driver.\n    seed (int): Seed for the random number generator.\n\n    Returns:\n    float: The estimated value of Y_0.\n    \"\"\"\n    # Time step size\n    dt = T / M\n    \n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Forward Path Simulation\n    # Generate N paths of Brownian increments\n    dW = rng.normal(0, np.sqrt(dt), size=(N, M))\n    \n    # Simulate N paths of the forward process X_t\n    X = np.zeros((N, M + 1))\n    X[:, 0] = x0\n    # Use cumsum for efficient path generation\n    X[:, 1:] = x0 + sigma * np.cumsum(dW, axis=1)\n\n    # 2. Terminal Condition for Y\n    # At time T, Y_T = g(X_T) = X_T^2\n    Y = X[:, -1]**2\n\n    # 3. Backward Recursion Loop\n    for k in range(M - 1, -1, -1):\n        # Get states and increments for the current time step t_k\n        X_k = X[:, k]\n        dW_k = dW[:, k]\n        \n        # Construct the regression basis matrix from state X_k\n        # Basis is phi(x) = (1, x, x^2)\n        poly_basis = np.stack([np.ones(N), X_k, X_k**2], axis=1)\n\n        # --------- Stage A: Estimate Z_k ---------\n        # Define the regression target for Z_k\n        target_Z = Y * dW_k / dt\n        \n        # Perform least-squares regression to find coefficients for Z_k\n        coeffs_Z = np.linalg.lstsq(poly_basis, target_Z, rcond=None)[0]\n        \n        # Compute the pathwise estimate of Z_k using the regression model\n        Z_k = poly_basis @ coeffs_Z\n\n        # --------- Stage B: Estimate Y_k ---------\n        # Define the regression target for Y_k\n        # V_k = Y_{k+1} + f(t_k, Y_{k+1}, Z_k_hat) * dt\n        # f(t,y,z) = lam*y + mu*z\n        target_Y = Y + (lam * Y + mu * Z_k) * dt\n        \n        # Perform least-squares regression to find coefficients for Y_k\n        coeffs_Y = np.linalg.lstsq(poly_basis, target_Y, rcond=None)[0]\n        \n        # Compute the pathwise estimate for Y_k. This becomes Y_{k+1} for the next loop.\n        Y = poly_basis @ coeffs_Y\n\n    # 4. Final Result\n    # At k=0, X_k is a constant x0 vector. The regression result Y will be a\n    # constant vector where each element is the mean of target_Y.\n    # We return this single value.\n    return Y[0]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the BSDE solver on the suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, M, N, x0, sigma, lambda, mu, seed)\n        (1.0, 40, 5000, 0.0, 1.0, 0.0, 0.0, 12345),\n        (0.5, 30, 5000, 0.0, 1.0, 0.3, 0.0, 23456),\n        (1.0, 40, 5000, 0.0, 1.0, 0.0, 0.2, 34567),\n        (0.1, 10, 5000, 0.0, 1.0, 0.5, -0.1, 45678),\n    ]\n\n    results = []\n    for params in test_cases:\n        T, M, N, x0, sigma, lam, mu, seed = params\n        y0_estimate = solve_bsde_regression(T, M, N, x0, sigma, lam, mu, seed)\n        results.append(y0_estimate)\n\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3040102"}]}