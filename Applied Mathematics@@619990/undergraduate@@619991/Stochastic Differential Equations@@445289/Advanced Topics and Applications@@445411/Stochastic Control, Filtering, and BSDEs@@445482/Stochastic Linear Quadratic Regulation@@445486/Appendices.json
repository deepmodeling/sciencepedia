{"hands_on_practices": [{"introduction": "The cornerstone of solving any linear quadratic regulation problem is the Algebraic Riccati Equation (ARE). This first exercise provides a foundational walkthrough of deriving this crucial equation from first principles for a simple scalar system [@problem_id:3077803]. By working through this problem, you will not only derive the ARE from the Hamilton-Jacobi-Bellman equation but also learn the critical skill of selecting the unique, stabilizing solution from the possible mathematical candidates, which is essential for designing a functional controller.", "problem": "Consider the scalar controlled Ornstein–Uhlenbeck process governed by the stochastic differential equation\n$$\n\\mathrm{d}X_t = a X_t\\,\\mathrm{d}t + b U_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\n$$\nwhere $a<0$, $b\\neq 0$, $\\sigma \\in \\mathbb{R}$, and $(W_t)_{t\\ge 0}$ is a standard Wiener process. For a given finite horizon $T>0$, the performance index is\n$$\nJ_T(x_0;U) = \\mathbb{E}\\left[\\int_{0}^{T} \\big(q X_t^2 + r U_t^2\\big)\\,\\mathrm{d}t\\right],\n$$\nwith $q>0$ and $r>0$. Assume the value function has the quadratic form $V(t,x)=P(t)\\,x^2 + c(t)$ with terminal condition $V(T,x)=0$.\n\nStarting from the dynamic programming principle and the Hamilton–Jacobi–Bellman equation for this finite-horizon problem, derive the ordinary differential equation that $P(t)$ must satisfy. Then, by considering the infinite-horizon limit $T\\to\\infty$ under the standing assumptions ($a<0$, $q>0$, $r>0$, $b\\neq 0$), determine the constant steady-state coefficient $P_{\\infty}$ that solves the resulting algebraic equation and select the stabilizing solution that makes the closed-loop drift $a - \\frac{b^2}{r} P_{\\infty}$ strictly negative.\n\nExpress your final answer as a single closed-form analytic expression for $P_{\\infty}$ in terms of $a$, $b$, $q$, and $r$. No numerical evaluation or rounding is required. The final answer must be a single expression.", "solution": "The problem statement is a standard formulation of a scalar finite-horizon stochastic linear quadratic regulator (SLQR) problem, and its extension to the infinite-horizon case. All provided data, conditions, and parameters are consistent, scientifically sound, and well-posed within the established framework of stochastic control theory. The problem is valid.\n\nThe value function for this problem is defined as\n$$\nV(t, x) = \\min_{U} \\mathbb{E}\\left[\\int_{t}^{T} \\big(q X_s^2 + r U_s^2\\big)\\,\\mathrm{d}s \\;\\Bigg|\\; X_t=x\\right].\n$$\nThe value function $V(t,x)$ must satisfy the Hamilton-Jacobi-Bellman (HJB) partial differential equation:\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{U} \\left\\{ (ax + bU)\\frac{\\partial V}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2}(t,x) + qx^2 + rU^2 \\right\\},\n$$\nwith the terminal condition $V(T,x) = 0$.\n\nWe are given the ansatz that the value function has the quadratic form $V(t,x) = P(t)x^2 + c(t)$. We compute its partial derivatives:\n$$\n\\frac{\\partial V}{\\partial t} = \\dot{P}(t)x^2 + \\dot{c}(t), \\quad \\frac{\\partial V}{\\partial x} = 2P(t)x, \\quad \\frac{\\partial^2 V}{\\partial x^2} = 2P(t).\n$$\nSubstituting these into the HJB equation yields:\n$$\n-\\left(\\dot{P}(t)x^2 + \\dot{c}(t)\\right) = \\min_{U} \\left\\{ (ax + bU)(2P(t)x) + \\frac{1}{2}\\sigma^2(2P(t)) + qx^2 + rU^2 \\right\\}.\n$$\n$$\n-\\dot{P}(t)x^2 - \\dot{c}(t) = \\min_{U} \\left\\{ 2aP(t)x^2 + 2bP(t)xU + \\sigma^2P(t) + qx^2 + rU^2 \\right\\}.\n$$\nTo find the optimal control $U_t^*$, we minimize the expression in the curly braces with respect to $U$. The expression is a convex quadratic in $U$, so its minimum is found by setting its first derivative with respect to $U$ to zero:\n$$\n\\frac{\\partial}{\\partial U} \\left(rU^2 + 2bP(t)xU + \\dots \\right) = 2rU + 2bP(t)x = 0.\n$$\nSince $r>0$, this gives the unique optimal control:\n$$\nU_t^* = -\\frac{b P(t)}{r} X_t.\n$$\nNow, we substitute this optimal control back into the HJB equation:\n$$\n-\\dot{P}(t)x^2 - \\dot{c}(t) = 2aP(t)x^2 + 2bP(t)x\\left(-\\frac{bP(t)x}{r}\\right) + \\sigma^2P(t) + qx^2 + r\\left(-\\frac{bP(t)x}{r}\\right)^2.\n$$\nSimplifying the right-hand side:\n$$\n-\\dot{P}(t)x^2 - \\dot{c}(t) = 2aP(t)x^2 - \\frac{2b^2P(t)^2}{r}x^2 + \\sigma^2P(t) + qx^2 + \\frac{b^2P(t)^2}{r}x^2.\n$$\nCombine terms:\n$$\n-\\dot{P}(t)x^2 - \\dot{c}(t) = \\left(2aP(t) - \\frac{b^2P(t)^2}{r} + q\\right)x^2 + \\sigma^2P(t).\n$$\nThis equation must hold for all $x \\in \\mathbb{R}$. By separating the terms that depend on $x^2$ from the constant terms, we obtain a system of two ordinary differential equations (ODEs):\n1.  $-\\dot{P}(t) = q + 2aP(t) - \\frac{b^2}{r}P(t)^2$\n2.  $-\\dot{c}(t) = \\sigma^2P(t)$\n\nThe terminal condition $V(T,x) = P(T)x^2 + c(T) = 0$ for all $x$ implies the boundary conditions $P(T)=0$ and $c(T)=0$. The first equation is the differential Riccati equation for $P(t)$.\n\nFor the infinite-horizon problem ($T \\to \\infty$), we seek a steady-state solution where the value function is time-invariant. This implies that $P(t)$ becomes a constant, which we denote by $P_{\\infty}$. In this steady state, its time derivative is zero, $\\dot{P}(t) = 0$. The differential Riccati equation reduces to the continuous-time Algebraic Riccati Equation (ARE):\n$$\n0 = q + 2aP_{\\infty} - \\frac{b^2}{r}P_{\\infty}^2.\n$$\nRearranging this into a standard quadratic form $Ax^2+Bx+C=0$ for $P_{\\infty}$:\n$$\n\\frac{b^2}{r}P_{\\infty}^2 - 2aP_{\\infty} - q = 0.\n$$\nWe solve for $P_{\\infty}$ using the quadratic formula:\n$$\nP_{\\infty} = \\frac{-(-2a) \\pm \\sqrt{(-2a)^2 - 4\\left(\\frac{b^2}{r}\\right)(-q)}}{2\\left(\\frac{b^2}{r}\\right)} = \\frac{2a \\pm \\sqrt{4a^2 + \\frac{4b^2q}{r}}}{\\frac{2b^2}{r}}.\n$$\nSimplifying this expression:\n$$\nP_{\\infty} = \\frac{r}{b^2}\\left(a \\pm \\sqrt{a^2 + \\frac{b^2q}{r}}\\right).\n$$\nThe problem requires selecting the solution that stabilizes the closed-loop system. The steady-state optimal control is $U_t^* = -\\frac{b P_{\\infty}}{r} X_t$. Substituting this into the SDE for $X_t$ gives the closed-loop dynamics:\n$$\n\\mathrm{d}X_t = a X_t\\,\\mathrm{d}t + b\\left(-\\frac{b P_{\\infty}}{r} X_t\\right)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t = \\left(a - \\frac{b^2 P_{\\infty}}{r}\\right)X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t.\n$$\nFor the process to be stable, the closed-loop drift coefficient, $a_{cl} = a - \\frac{b^2}{r}P_{\\infty}$, must be strictly negative. Let's test the two solutions for $P_{\\infty}$.\n\nCase 1: $P_{\\infty,1} = \\frac{r}{b^2}\\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right)$\n$$\na_{cl,1} = a - \\frac{b^2}{r} \\left[ \\frac{r}{b^2}\\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) \\right] = a - \\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) = -\\sqrt{a^2 + \\frac{b^2q}{r}}.\n$$\nSince $b \\neq 0$, $q > 0$, and $r > 0$, the term $\\frac{b^2q}{r}$ is positive. Thus, $a^2 + \\frac{b^2q}{r} > 0$, and its square root is a positive real number. Therefore, $a_{cl,1} < 0$, which corresponds to a stable closed-loop system.\n\nCase 2: $P_{\\infty,2} = \\frac{r}{b^2}\\left(a - \\sqrt{a^2 + \\frac{b^2q}{r}}\\right)$\n$$\na_{cl,2} = a - \\frac{b^2}{r} \\left[ \\frac{r}{b^2}\\left(a - \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) \\right] = a - \\left(a - \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) = +\\sqrt{a^2 + \\frac{b^2q}{r}}.\n$$\nThis drift coefficient is strictly positive, $a_{cl,2} > 0$, which corresponds to an unstable closed-loop system.\n\nFurthermore, the value function must be non-negative, which requires $P_{\\infty} \\ge 0$. Given $a<0$, we have $|a|=-a$. The term under the square root satisfies $\\sqrt{a^2 + \\frac{b^2q}{r}} > \\sqrt{a^2} = |a| = -a$.\nFor $P_{\\infty,1}$, the term $a + \\sqrt{a^2 + \\frac{b^2q}{r}} > a + (-a) = 0$. Since $\\frac{r}{b^2}>0$, $P_{\\infty,1}$ is positive.\nFor $P_{\\infty,2}$, the term $a - \\sqrt{a^2 + \\frac{b^2q}{r}}$ is negative since $a<0$ and $-\\sqrt{\\cdot}<0$. Thus $P_{\\infty,2}$ is negative.\nBoth the stability requirement and the positivity of the value function select the same solution. The correct steady-state coefficient is $P_{\\infty,1}$.", "answer": "$$\\boxed{\\frac{r}{b^2}\\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right)}$$", "id": "3077803"}, {"introduction": "Designing an optimal controller is only half the battle; we must also understand how well it performs. This practice shifts the focus from deriving the control law to analyzing the behavior of the system under that optimal control [@problem_id:3077740]. You will find that the closed-loop system behaves as a classic Ornstein-Uhlenbeck process and will compute its stationary variance, a key metric that quantifies the system's long-term fluctuations in the presence of noise. This exercise builds a crucial bridge between the theory of optimal control and the practical evaluation of its performance.", "problem": "Consider the scalar controlled Itô Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + b\\,u_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\n$$\nwhere $a \\in \\mathbb{R}$, $b \\neq 0$, $\\sigma > 0$, and $\\{W_t\\}_{t \\ge 0}$ is a standard Wiener process. The performance criterion is the infinite-horizon ergodic average cost\n$$\nJ(u) \\;=\\; \\limsup_{T \\to \\infty} \\frac{1}{T}\\,\\mathbb{E}\\!\\left[\\int_0^T \\big(q\\,x_t^2 + r\\,u_t^2\\big)\\,\\mathrm{d}t\\right],\n$$\nwith $q>0$ and $r>0$. Assume full state observation and restrict attention to time-invariant state-feedback controls of the form $u_t = \\pi(x_t)$ that render the closed-loop system mean-square stable.\n\nStarting from the dynamic programming principle and the Hamilton–Jacobi–Bellman (HJB) equation for the ergodic cost, derive the optimal infinite-horizon Linear Quadratic Regulator (LQR) feedback law, and show that the closed-loop dynamics are Ornstein–Uhlenbeck. Then, compute the stationary (steady-state) variance $\\operatorname{Var}_{\\pi}(x_t)$ of $x_t$ under the optimal feedback as an explicit function of $a$, $b$, $q$, $r$, and $\\sigma$. Finally, explain qualitatively how the stationary variance depends on the diffusion intensity $\\sigma$. \n\nYour final reported answer must be the single closed-form expression for the steady-state variance $\\operatorname{Var}_{\\pi}(x_t)$ in terms of $a$, $b$, $q$, $r$, and $\\sigma$. No rounding is required.", "solution": "The problem specified is a continuous-time, infinite-horizon, scalar stochastic linear quadratic regulator (LQR) problem with an ergodic (average cost) performance criterion. We begin by validating the problem statement.\n\nThe givens are:\n- The state dynamics, a linear Itô Stochastic Differential Equation (SDE): $\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + b\\,u_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t$, with parameters $a \\in \\mathbb{R}$, $b \\neq 0$, and $\\sigma > 0$.\n- The cost functional: $J(u) = \\limsup_{T \\to \\infty} \\frac{1}{T}\\,\\mathbb{E}\\!\\left[\\int_0^T (q\\,x_t^2 + r\\,u_t^2)\\,\\mathrm{d}t\\right]$, with weighting factors $q>0$ and $r>0$.\n- The class of admissible controls: time-invariant state-feedback laws $u_t = \\pi(x_t)$ that ensure mean-square stability of the closed-loop system.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a canonical problem in stochastic control theory. The system is controllable (since $b \\neq 0$), and the cost function is convex with positive definite weighting on state and control, which guarantees the existence of a unique optimal and stabilizing control law. The problem is therefore valid.\n\nWe proceed with the solution, starting from the dynamic programming principle for the ergodic cost problem. This principle leads to the Hamilton–Jacobi–Bellman (HJB) equation. For a given state $x$ and control $u$, the cost rate is $L(x, u) = q\\,x^2 + r\\,u^2$. The infinitesimal generator of the SDE, acting on a twice-differentiable function $V(x)$, is given by\n$$\n\\mathcal{A}^u V(x) = (a\\,x + b\\,u)\\frac{\\mathrm{d}V}{\\mathrm{d}x} + \\frac{1}{2}\\sigma^2\\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2}.\n$$\nThe HJB equation for the optimal average cost $\\rho$ is\n$$\n\\rho = \\min_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + \\mathcal{A}^u V(x) \\right\\},\n$$\nwhere $V(x)$ is the relative value function. Substituting the generator, we have\n$$\n\\rho = \\min_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + (a\\,x + b\\,u)V'(x) + \\frac{1}{2}\\sigma^2V''(x) \\right\\}.\n$$\nThe expression inside the minimum is a convex quadratic function of $u$. The minimum is found by setting the partial derivative with respect to $u$ to zero:\n$$\n\\frac{\\partial}{\\partial u} \\left[ r\\,u^2 + b\\,u\\,V'(x) \\right] = 2\\,r\\,u + b\\,V'(x) = 0.\n$$\nThis yields the optimal control law in feedback form:\n$$\nu^*(x) = -\\frac{b}{2r}V'(x).\n$$\nWe substitute this optimal control back into the HJB equation:\n$$\n\\rho = q\\,x^2 + r\\left(-\\frac{b}{2r}V'(x)\\right)^2 + \\left(a\\,x + b\\left(-\\frac{b}{2r}V'(x)\\right)\\right)V'(x) + \\frac{1}{2}\\sigma^2V''(x).\n$$\nSimplifying the expression gives\n$$\n\\rho = q\\,x^2 + \\frac{b^2}{4r}(V'(x))^2 + a\\,x\\,V'(x) - \\frac{b^2}{2r}(V'(x))^2 + \\frac{1}{2}\\sigma^2V''(x),\n$$\n$$\n\\rho = q\\,x^2 + a\\,x\\,V'(x) - \\frac{b^2}{4r}(V'(x))^2 + \\frac{1}{2}\\sigma^2V''(x).\n$$\nFor a linear system with a quadratic cost, we hypothesize a quadratic relative value function of the form $V(x) = \\frac{1}{2}Px^2$, where $P$ is a constant to be determined. The derivatives are $V'(x) = Px$ and $V''(x) = P$. Substituting these into the HJB equation:\n$$\n\\rho = q\\,x^2 + a\\,x\\,(Px) - \\frac{b^2}{4r}(Px)^2 + \\frac{1}{2}\\sigma^2P.\n$$\nCollecting terms in powers of $x$:\n$$\n\\rho = \\left(q + aP - \\frac{b^2P^2}{4r}\\right)x^2 + \\frac{1}{2}\\sigma^2P.\n$$\nThis equation must hold for all $x \\in \\mathbb{R}$. This is only possible if the coefficient of the $x^2$ term is zero. This requirement yields the scalar algebraic Riccati equation (ARE):\n$$\nq + aP - \\frac{b^2P^2}{4r} = 0 \\quad \\implies \\quad \\frac{b^2}{4r}P^2 - aP - q = 0.\n$$\nThe solutions for $P$ from this quadratic equation are\n$$\nP = \\frac{a \\pm \\sqrt{a^2 - 4\\left(\\frac{b^2}{4r}\\right)(-q)}}{2\\left(\\frac{b^2}{4r}\\right)} = \\frac{a \\pm \\sqrt{a^2 + \\frac{b^2q}{r}}}{\\frac{b^2}{2r}} = \\frac{2r}{b^2}\\left(a \\pm \\sqrt{a^2 + \\frac{b^2q}{r}}\\right).\n$$\nThe problem requires a control that renders the system mean-square stable. The optimal control is $u_t = u^*(x_t) = -\\frac{b}{2r}V'(x_t) = -\\frac{b P}{2r}x_t$. The closed-loop SDE becomes\n$$\n\\mathrm{d}x_t = \\left(a - \\frac{b^2P}{2r}\\right)x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t.\n$$\nFor mean-square stability of this linear SDE, the drift coefficient must be negative: $a_{cl} = a - \\frac{b^2P}{2r} < 0$.\nSince $q>0$, $r>0$, we have $\\sqrt{a^2 + b^2q/r} > \\sqrt{a^2} = |a|$.\nThe solution $P_{-} = \\frac{2r}{b^2}\\left(a - \\sqrt{a^2 + \\frac{b^2q}{r}}\\right)$ is negative, because $a < \\sqrt{a^2 + \\dots}$. A negative $P$ would imply a negative value (cost), which is inconsistent for a cost function with $q>0, r>0$.\nThe other solution, $P_{+} = \\frac{2r}{b^2}\\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right)$, is positive, since $a + \\sqrt{a^2+\\dots} > a+|a| \\ge 0$. Let's check the stability condition with $P=P_{+}$:\n$$\na_{cl} = a - \\frac{b^2}{2r} P_{+} = a - \\frac{b^2}{2r} \\frac{2r}{b^2}\\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) = a - \\left(a + \\sqrt{a^2 + \\frac{b^2q}{r}}\\right) = -\\sqrt{a^2 + \\frac{b^2q}{r}}.\n$$\nSince $b \\neq 0$, $q > 0$, $r > 0$, the term under the square root is strictly positive, so $a_{cl} < 0$. Thus, $P = P_{+}$ is the unique positive definite solution to the ARE that provides a stabilizing feedback control.\n\nThe closed-loop system is described by the SDE\n$$\n\\mathrm{d}x_t = a_{cl}\\,x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t, \\quad \\text{where} \\quad a_{cl} = -\\sqrt{a^2 + \\frac{b^2q}{r}}.\n$$\nThis is the equation for an Ornstein–Uhlenbeck (OU) process, which has the general form $\\mathrm{d}X_t = -\\theta X_t \\mathrm{d}t + \\nu \\mathrm{d}W_t$. In our case, the mean-reversion rate is $\\theta = -a_{cl} > 0$ and the volatility is $\\nu = \\sigma$.\n\nAn OU process has a stationary (or ergodic) distribution, which is a Gaussian distribution with mean $0$ and variance $\\operatorname{Var}(X) = \\frac{\\nu^2}{2\\theta}$.\nApplying this to our closed-loop system, the stationary variance of $x_t$ under the optimal control policy $\\pi$ is\n$$\n\\operatorname{Var}_{\\pi}(x_t) = \\frac{\\sigma^2}{2(-a_{cl})} = \\frac{\\sigma^2}{2\\left(\\sqrt{a^2 + \\frac{b^2q}{r}}\\right)}.\n$$\n\nFinally, we analyze the dependence of the stationary variance on the diffusion intensity $\\sigma$. The derived formula shows that $\\operatorname{Var}_{\\pi}(x_t)$ is directly proportional to $\\sigma^2$. Qualitatively, $\\sigma$ represents the magnitude of the random noise perturbing the system. An increase in $\\sigma$ implies stronger random forces, pushing the state further from its equilibrium at $x=0$. While the optimal controller continuously acts to regulate the state, larger noise naturally results in larger stationary fluctuations. The variance, being the expected squared deviation from the mean, scales quadratically with the noise amplitude. This is a fundamental characteristic of linear systems driven by white noise.", "answer": "$$\\boxed{\\frac{\\sigma^2}{2\\sqrt{a^2 + \\frac{b^2 q}{r}}}}$$", "id": "3077740"}, {"introduction": "In many practical applications, we are interested in regulating certain outputs (like position) while the system dynamics involve other related states (like velocity). This exercise explores such a scenario, where the cost function explicitly penalizes only one of the system's two states [@problem_id:3077758]. You will discover that, due to the coupling in the system dynamics, the optimal controller must still regulate the unpenalized state to achieve its goal. This practice offers valuable insight into how the LQR framework intelligently manages the entire state vector, even when the objective function appears limited in scope.", "problem": "Consider the continuous-time linear stochastic system driven by standard Brownian motion (Wiener process)\n$$\nd x(t) = \\big(A\\,x(t) + B\\,u(t)\\big)\\,dt + D\\,dW_t,\n$$\nwith state $x(t) \\in \\mathbb{R}^{2}$, control $u(t) \\in \\mathbb{R}$, and matrices\n$$\nA=\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},\\quad B=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\\quad D=\\begin{pmatrix} 0 \\\\ \\sigma \\end{pmatrix},\n$$\nwhere $\\sigma>0$ is a fixed constant. Consider the infinite-horizon ergodic average cost\n$$\nJ(u) = \\limsup_{T\\to\\infty}\\frac{1}{T}\\,\\mathbb{E}\\left[ \\int_{0}^{T} \\Big(x(t)^{\\top} Q\\, x(t) + u(t)^{\\top} R\\, u(t)\\Big)\\,dt \\right],\n$$\nwhere the state penalty $Q$ penalizes only the first component,\n$$\nQ=\\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix},\\quad q>0,\n$$\nand the control penalty is $R=1$. Assume full-state feedback is admissible and that a stationary optimal solution exists.\n\nStarting from the principles of stochastic dynamic programming and the Hamilton–Jacobi–Bellman (HJB) equation for ergodic average cost, and postulating a quadratic value function, derive the stationary optimal state-feedback control law of the form $u(t)=-K\\,x(t)$, where $K\\in\\mathbb{R}^{1\\times 2}$. In particular:\n\n1. Derive the algebraic condition that determines the value function’s quadratic form and the optimal feedback gain.\n2. Solve this algebraic condition explicitly for the present system and compute the optimal gain $K$ in exact closed form as a function of $q$.\n3. Within your derivation, explain why the noise intensity $\\sigma$ does not affect the optimal feedback gain and clarify how the second (unpenalized) state is nevertheless indirectly regulated through the system dynamics.\n\nProvide your final answer as the exact expression for the optimal gain $K$ written as a $1\\times 2$ row vector. No numerical rounding is required. Do not include units in your final answer. Express your final answer as a row matrix using the $\\text{pmatrix}$ format.", "solution": "The problem is valid as it constitutes a well-posed and standard problem in stochastic linear quadratic regulation with an infinite-horizon ergodic average cost. All necessary parameters and conditions are provided, and there are no scientific or logical contradictions.\n\nThe system dynamics are given by the stochastic differential equation (SDE):\n$$\nd x(t) = \\big(A\\,x(t) + B\\,u(t)\\big)\\,dt + D\\,dW_t\n$$\nwith state $x(t) \\in \\mathbb{R}^{2}$ and control $u(t) \\in \\mathbb{R}$. The matrices are:\n$$\nA=\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},\\quad B=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\\quad D=\\begin{pmatrix} 0 \\\\ \\sigma \\end{pmatrix}\n$$\nThe objective is to minimize the infinite-horizon ergodic average cost:\n$$\nJ(u) = \\limsup_{T\\to\\infty}\\frac{1}{T}\\,\\mathbb{E}\\left[ \\int_{0}^{T} \\Big(x(t)^{\\top} Q\\, x(t) + u(t)^{\\top} R\\, u(t)\\Big)\\,dt \\right]\n$$\nwhere the weighting matrices are:\n$$\nQ=\\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix},\\quad q>0,\\quad R=1\n$$\n\n### 1. Derivation of the Algebraic Condition\nThe solution is found using the principles of stochastic dynamic programming. For an ergodic average cost problem, the Hamilton-Jacobi-Bellman (HJB) equation is:\n$$\n\\rho = \\min_{u} \\left\\{ \\mathcal{L}^u V(x) + l(x, u) \\right\\}\n$$\nHere, $\\rho$ is the optimal average cost (a constant), $V(x)$ is the differential value function, $l(x, u) = x^{\\top} Q x + u^{\\top} R u$ is the running cost, and $\\mathcal{L}^u$ is the infinitesimal generator of the process $x(t)$ for a given control $u$. For the given SDE, the generator applied to a twice-differentiable function $V(x)$ is:\n$$\n\\mathcal{L}^u V(x) = \\nabla V(x)^{\\top} (Ax + Bu) + \\frac{1}{2} \\text{Tr}\\left( D^{\\top} \\text{Hess}(V(x)) D \\right)\n$$\nwhere $\\nabla V(x)$ is the gradient and $\\text{Hess}(V(x))$ is the Hessian matrix of $V(x)$.\n\nGiven the linear system and quadratic cost, we postulate a quadratic differential value function of the form $V(x) = x^{\\top} P x$, where $P$ is a symmetric positive semi-definite matrix. The gradient and Hessian are $\\nabla V(x) = 2Px$ and $\\text{Hess}(V(x)) = 2P$. Substituting these into the generator gives:\n$$\n\\mathcal{L}^u V(x) = (2Px)^{\\top} (Ax + Bu) + \\frac{1}{2} \\text{Tr}(D^{\\top} (2P) D) = 2x^{\\top}P(Ax+Bu) + \\text{Tr}(D^{\\top}PD)\n$$\nThe HJB equation becomes:\n$$\n\\rho = \\min_{u} \\left\\{ 2x^{\\top}PAx + 2x^{\\top}PBu + \\text{Tr}(D^{\\top}PD) + x^{\\top}Qx + u^{\\top}Ru \\right\\}\n$$\nTo find the optimal control $u^*$, we minimize the expression with respect to $u$. The terms involving $u$ are $2x^{\\top}PBu + u^{\\top}Ru$. This is a quadratic function of $u$. Taking the derivative with respect to $u$ and setting it to zero yields:\n$$\n\\frac{\\partial}{\\partial u} (2x^{\\top}PBu + u^{\\top}Ru) = 2(PBx)^{\\top} + 2Ru = 2B^{\\top}Px + 2Ru = 0\n$$\nSince $R > 0$, we can solve for the optimal control law:\n$$\nu^*(t) = -R^{-1}B^{\\top}Px(t)\n$$\nThis is a linear state-feedback control law $u(t) = -Kx(t)$, with the optimal gain matrix $K = R^{-1}B^{\\top}P$.\n\nSubstituting $u^*$ back into the HJB equation:\n$$\n\\rho = 2x^{\\top}PAx + 2x^{\\top}PB(-R^{-1}B^{\\top}Px) + \\text{Tr}(D^{\\top}PD) + x^{\\top}Qx + (-R^{-1}B^{\\top}Px)^{\\top}R(-R^{-1}B^{\\top}Px)\n$$\n$$\n\\rho = x^{\\top}(A^{\\top}P+PA)x - 2x^{\\top}PBR^{-1}B^{\\top}Px + \\text{Tr}(D^{\\top}PD) + x^{\\top}Qx + x^{\\top}PBR^{-1}B^{\\top}Px\n$$\n$$\n\\rho = x^{\\top}\\left( A^{\\top}P + PA - PBR^{-1}B^{\\top}P + Q \\right)x + \\text{Tr}(D^{\\top}PD)\n$$\nThis equation must hold for all $x(t) \\in \\mathbb{R}^2$. This can only be true if the quadratic term in $x$ is identically zero. This gives the algebraic condition that determines $P$:\n$$\nA^{\\top}P + PA - PBR^{-1}B^{\\top}P + Q = 0\n$$\nThis is the Continuous-time Algebraic Riccati Equation (CARE). This equation determines the matrix $P$ and, consequently, the optimal feedback gain $K = R^{-1}B^{\\top}P$. The optimal average cost is then given by $\\rho = \\text{Tr}(D^{\\top}PD)$.\n\n### 3. Independence of Gain from Noise and Regulation of the Second State\nThe CARE, $A^{\\top}P + PA - PBR^{-1}B^{\\top}P + Q = 0$, which determines the matrix $P$ and thus the optimal gain $K$, does not involve the noise matrix $D$ or its intensity $\\sigma$. This is a manifestation of the separation principle or certainty equivalence in LQG control: the optimal feedback gain is identical to that of the deterministic problem (where $D=0$). The noise only affects the minimal achievable cost $\\rho = \\text{Tr}(D^{\\top}PD)$, but not the optimal control strategy.\n\nThe state vector is $x(t) = [x_1(t), x_2(t)]^{\\top}$, where $x_1$ is position and $x_2$ is velocity ($dx_1/dt = x_2$). The cost function $x^{\\top}Qx = qx_1^2$ only penalizes the position $x_1$. However, to keep $x_1$ small, the controller must regulate $x_2$. If $x_2$ were not driven towards zero, $x_1$ would integrate to large values, resulting in high cost. The optimal controller utilizes feedback from both states, $u = -k_1x_1 - k_2x_2$, to stabilize the entire system. Feedback from $x_2$ (velocity feedback) provides necessary damping to the system, ensuring that both position and velocity converge towards the origin in a stochastic sense.\n\n### 2. Explicit Solution for the Optimal Gain\nWe now solve the CARE for the given system. Let $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$ be the symmetric solution matrix.\nThe matrices are $A=\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$, $B=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $Q=\\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix}$, and $R=1$.\n\nWe compute the terms of the CARE:\n$A^{\\top}P + PA = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & p_{11} \\\\ p_{11} & 2p_{12} \\end{pmatrix}$.\n$PBR^{-1}B^{\\top}P = \\begin{pmatrix} p_{12}^2  p_{12}p_{22} \\\\ p_{12}p_{22}  p_{22}^2 \\end{pmatrix}$.\n\nThe CARE becomes:\n$$\n\\begin{pmatrix} 0  p_{11} \\\\ p_{11}  2p_{12} \\end{pmatrix} - \\begin{pmatrix} p_{12}^2  p_{12}p_{22} \\\\ p_{12}p_{22}  p_{22}^2 \\end{pmatrix} + \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} q - p_{12}^2  p_{11} - p_{12}p_{22} \\\\ p_{11} - p_{12}p_{22}  2p_{12} - p_{22}^2 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThis yields a system of three algebraic equations:\n1. $q - p_{12}^2 = 0 \\implies p_{12}^2 = q$\n2. $2p_{12} - p_{22}^2 = 0 \\implies p_{22}^2 = 2p_{12}$\n3. $p_{11} - p_{12}p_{22} = 0 \\implies p_{11} = p_{12}p_{22}$\n\nFrom (1), since $q0$, we have $p_{12} = \\pm\\sqrt{q}$. For the closed-loop system to be stable, the solution $P$ must be positive definite. From (2), $p_{22}^2 = 2p_{12}$. For $p_{22}$ to be a real number, $p_{12}$ must be non-negative. This requires $p_{12} = \\sqrt{q}$.\nWith $p_{12} = \\sqrt{q}$, equation (2) gives $p_{22}^2 = 2\\sqrt{q}$. Since $p_{22}$ must be a positive diagonal element of a positive definite matrix, we take the positive root: $p_{22} = \\sqrt{2\\sqrt{q}}$.\nFrom (3), we find $p_{11} = p_{12}p_{22} = \\sqrt{q} \\sqrt{2\\sqrt{q}} = \\sqrt{2}q^{\\frac{3}{4}}$.\nThe solution matrix is $P = \\begin{pmatrix} \\sqrt{2}q^{\\frac{3}{4}}  \\sqrt{q} \\\\ \\sqrt{q}  \\sqrt{2\\sqrt{q}} \\end{pmatrix}$. Since $p_{11}0$ and $\\det(P) = p_{11}p_{22} - p_{12}^2 = (\\sqrt{2}q^{\\frac{3}{4}})(\\sqrt{2\\sqrt{q}}) - (\\sqrt{q})^2 = 2q-q=q0$, $P$ is positive definite.\n\nThe optimal feedback gain $K$ is given by:\n$$\nK = R^{-1}B^{\\top}P = (1) \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix} = \\begin{pmatrix} p_{12}  p_{22} \\end{pmatrix}\n$$\nSubstituting the derived values for $p_{12}$ and $p_{22}$:\n$$\nK = \\begin{pmatrix} \\sqrt{q}  \\sqrt{2\\sqrt{q}} \\end{pmatrix}\n$$\nThis is the stationary optimal state-feedback gain.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{q}  \\sqrt{2\\sqrt{q}} \\end{pmatrix}}\n$$", "id": "3077758"}]}