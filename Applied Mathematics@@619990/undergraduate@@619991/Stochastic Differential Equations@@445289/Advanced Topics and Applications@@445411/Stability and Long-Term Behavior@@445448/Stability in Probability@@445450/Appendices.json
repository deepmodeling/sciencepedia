{"hands_on_practices": [{"introduction": "In the world of deterministic systems, the concept of stability is quite straightforward. However, when randomness is introduced, \"stability\" can take on different, and sometimes counterintuitive, meanings. This exercise explores the crucial distinction between stability in probability and mean-square stability using the classic geometric Brownian motion model. You will see firsthand how a system can have almost all its trajectories decay to zero, yet its average behavior, influenced by rare but extreme events, can be explosive. This practice [@problem_id:3060618] is fundamental to understanding the nuanced nature of stochastic stability.", "problem": "Consider the scalar Itô Stochastic Differential Equation (SDE)\n$$\ndX_{t}=-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\qquad X_{0}=1,\n$$\nwhere $a0$ and $b0$ are constants, and $W_{t}$ is a standard Wiener process. The equilibrium $X=0$ is said to be asymptotically stable in probability if $X_{t}\\to 0$ in probability as $t\\to\\infty$ for initial conditions sufficiently close to $0$, and it is said to be mean-square stable if $\\lim_{t\\to\\infty}\\mathbb{E}[\\,|X_{t}|^{2}\\,]=0$.\n\nUsing only fundamental definitions and properties of Itô calculus (in particular, Itô’s formula and basic moment computations for Gaussian random variables), do the following for the specific choice $a=\\frac{1}{4}$ and $b=1$:\n\n1. Derive the exact representation of $X_{t}$ and compute the almost-sure sample-path growth exponent\n$$\nL_{1}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|.\n$$\n2. Compute the mean-square growth exponent\n$$\nL_{2}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\big(\\mathbb{E}[X_{t}^{2}]\\big).\n$$\n3. Using the signs of $L_{1}$ and $L_{2}$, explain why in this example the equilibrium $X=0$ is asymptotically stable in probability but not mean-square stable, and articulate the probabilistic mechanism that differentiates these two notions of stability for this SDE.\n\nReport your final values for $L_{1}$ and $L_{2}$ as a single row matrix. No rounding is needed.", "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for its resolution. It represents a standard, illustrative example in the theory of stochastic stability. We proceed with the solution.\n\nThe given Itô Stochastic Differential Equation (SDE) is:\n$$\ndX_{t}=-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\qquad X_{0}=1\n$$\nwith constants $a=\\frac{1}{4}  0$ and $b=1  0$. This is a linear SDE known as the equation for geometric Brownian motion.\n\n### 1. Derivation of $X_{t}$ and computation of $L_{1}$\n\nTo find the explicit solution for $X_{t}$, we apply Itô's formula to the function $f(X_{t}) = \\ln(X_{t})$. The derivatives of $f(x) = \\ln(x)$ are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$.\nAccording to Itô's formula, the differential of $Y_{t} = f(X_{t})$ is given by:\n$$\ndY_{t} = f'(X_{t})\\,dX_{t} + \\frac{1}{2}f''(X_{t})\\,(dX_{t})^2\n$$\nThe quadratic variation term $(dX_{t})^2$ is calculated using the Itô rules: $(dt)^2=0$, $dt\\,dW_{t}=0$, and $(dW_{t})^2=dt$.\n$$\n(dX_{t})^2 = (-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t})^2 = (b\\,X_{t})^2(dW_{t})^2 = b^2\\,X_{t}^2\\,dt\n$$\nSubstituting $dX_{t}$ and $(dX_{t})^2$ into Itô's formula for $Y_{t} = \\ln(X_{t})$:\n$$\nd(\\ln X_{t}) = \\frac{1}{X_{t}}(-a\\,X_{t}\\,dt + b\\,X_{t}\\,dW_{t}) + \\frac{1}{2}\\left(-\\frac{1}{X_{t}^2}\\right)(b^2\\,X_{t}^2\\,dt)\n$$\n$$\nd(\\ln X_{t}) = -a\\,dt + b\\,dW_{t} - \\frac{1}{2}b^2\\,dt\n$$\n$$\nd(\\ln X_{t}) = -\\left(a + \\frac{1}{2}b^2\\right)dt + b\\,dW_{t}\n$$\nIntegrating this from $s=0$ to $s=t$:\n$$\n\\int_{0}^{t} d(\\ln X_{s}) = \\int_{0}^{t} -\\left(a + \\frac{1}{2}b^2\\right)ds + \\int_{0}^{t} b\\,dW_{s}\n$$\n$$\n\\ln(X_{t}) - \\ln(X_{0}) = -\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\n$$\nGiven the initial condition $X_{0}=1$, we have $\\ln(X_{0})=\\ln(1)=0$. Thus:\n$$\n\\ln(X_{t}) = -\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\n$$\nExponentiating both sides gives the exact representation of $X_{t}$:\n$$\nX_{t} = \\exp\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\n$$\nSince $X_{0}=1  0$ and the exponential function is always positive, $X_{t}0$ for all $t$, which means $|X_{t}| = X_{t}$.\n\nNow, we compute the almost-sure sample-path growth exponent, $L_{1}$:\n$$\nL_{1} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}| = \\lim_{t\\to\\infty}\\frac{1}{t}\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\n$$\n$$\nL_{1} = \\lim_{t\\to\\infty}\\left(-\\left(a + \\frac{1}{2}b^2\\right) + b\\,\\frac{W_{t}}{t}\\right)\n$$\nBy the Strong Law of Large Numbers for the Wiener process, we know that $\\lim_{t\\to\\infty} \\frac{W_{t}}{t} = 0$ almost surely. Therefore, the limit becomes:\n$$\nL_{1} = -\\left(a + \\frac{1}{2}b^2\\right)\n$$\nSubstituting the given values $a=\\frac{1}{4}$ and $b=1$:\n$$\nL_{1} = -\\left(\\frac{1}{4} + \\frac{1}{2}(1)^2\\right) = -\\left(\\frac{1}{4} + \\frac{1}{2}\\right) = -\\frac{3}{4}\n$$\n\n### 2. Computation of the mean-square growth exponent $L_{2}$\n\nFirst, we must compute the second moment $\\mathbb{E}[X_{t}^2]$. Using the solution for $X_{t}$:\n$$\nX_{t}^2 = \\left[\\exp\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\\right]^2 = \\exp\\left(-2\\left(a + \\frac{1}{2}b^2\\right)t + 2b\\,W_{t}\\right)\n$$\n$$\nX_{t}^2 = \\exp\\left((-2a - b^2)t + 2b\\,W_{t}\\right)\n$$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{t}^2] = \\mathbb{E}\\left[\\exp\\left((-2a - b^2)t + 2b\\,W_{t}\\right)\\right] = \\exp\\left((-2a - b^2)t\\right) \\mathbb{E}\\left[\\exp(2b\\,W_{t})\\right]\n$$\nThe term $\\mathbb{E}[\\exp(k\\,Z)]$ is the moment-generating function (MGF) of a random variable $Z$. For a standard Wiener process, $W_{t}$ is a Gaussian random variable with mean $0$ and variance $t$, i.e., $W_{t} \\sim \\mathcal{N}(0, t)$. The MGF of a general normal random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(kZ)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$.\nHere, the random variable is $W_t$, with $\\mu=0, \\sigma^2=t$, and the constant is $k=2b$.\n$$\n\\mathbb{E}[\\exp(2b\\,W_{t})] = \\exp\\left((2b)(0) + \\frac{1}{2}(2b)^2t\\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2t)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[X_{t}^2]$:\n$$\n\\mathbb{E}[X_{t}^2] = \\exp\\left((-2a - b^2)t\\right)\\exp(2b^2t) = \\exp\\left((-2a - b^2 + 2b^2)t\\right) = \\exp\\left((b^2 - 2a)t\\right)\n$$\nNow, we compute the mean-square growth exponent, $L_{2}$:\n$$\nL_{2} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\mathbb{E}[X_{t}^{2}]\\right) = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\exp\\left((b^2 - 2a)t\\right)\\right)\n$$\n$$\nL_{2} = \\lim_{t\\to\\infty}\\frac{1}{t}(b^2 - 2a)t = b^2 - 2a\n$$\nSubstituting the given values $a=\\frac{1}{4}$ and $b=1$:\n$$\nL_{2} = (1)^2 - 2\\left(\\frac{1}{4}\\right) = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\n\n### 3. Stability Analysis and Probabilistic Mechanism\n\nWe have found that $L_{1} = -\\frac{3}{4}$ and $L_{2} = \\frac{1}{2}$.\n\nThe sign of $L_{1}$ determines the almost-sure stability of the equilibrium $X=0$. Since $L_{1}  0$, we have $\\lim_{t\\to\\infty} \\ln|X_t| = -\\infty$ almost surely. This implies that $\\lim_{t\\to\\infty} X_t = 0$ almost surely. Almost-sure convergence implies convergence in probability. Therefore, since $L_{1}  0$, the equilibrium $X=0$ is asymptotically stable in probability. The negative drift term in the exponent of $X_t$ dominates the diffusion term for a typical sample path, causing trajectories to decay to zero.\n\nThe sign of $L_{2}$ determines the mean-square stability of the equilibrium $X=0$. Since $L_{2}  0$, we have $\\mathbb{E}[X_{t}^2] = \\exp(L_{2}t) = \\exp\\left(\\frac{1}{2}t\\right)$, which diverges to infinity as $t\\to\\infty$. Thus, the condition for mean-square stability, $\\lim_{t\\to\\infty}\\mathbb{E}[|X_{t}|^2]=0$, is not met. The equilibrium is mean-square unstable.\n\nThe probabilistic mechanism that differentiates these two stability notions lies in the nature of averaging over stochastic paths.\n- **Asymptotic stability in probability** (and its stronger form, almost-sure stability) concerns the behavior of a *typical* or *almost every* sample path. For this SDE, the deterministic drift component pulls most trajectories towards the equilibrium $X=0$. The exponent $L_{1} = -(a+b^2/2)$ represents the growth rate of the typical path. A negative value ensures this decay.\n- **Mean-square stability** concerns the behavior of the second moment, $\\mathbb{E}[X_{t}^2]$, which is an average over the ensemble of all possible sample paths. The value of this average is heavily influenced by rare, extreme events. The distribution of $X_t$ is log-normal, which has a heavy right tail. Although almost all paths converge to $0$, there is a small set of paths for which the Wiener process $W_t$ experiences large positive excursions. For these rare paths, the term $bW_t$ in the exponent of $X_t$ becomes large and positive, overpowering the negative drift term $- (a + b^2/2)t$. This leads to extraordinarily large values of $X_t$ for a small fraction of trajectories. When computing the second moment $\\mathbb{E}[X_t^2]$, the contribution of these rare but enormous values completely dominates the contributions from the vast majority of decaying paths. The result is an exponential growth of the second moment, as indicated by $L_2  0$.\n\nIn conclusion, the system is stable for almost every realization (almost-sure stability) but unstable in the mean-square sense because the expectation is skewed by infrequent but very large fluctuations.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{3}{4}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3060618"}, {"introduction": "While solving a stochastic differential equation explicitly is instructive, it is often impossible for more complex, nonlinear systems. This is where more general analytical tools become essential. This practice [@problem_id:2996128] introduces the powerful Lyapunov function method, an approach that allows us to determine stability without ever finding a closed-form solution. By constructing an \"energy-like\" function $V(x)$ and analyzing its expected rate of change using the infinitesimal generator $\\mathcal{L}$, we can draw firm conclusions about the mean-square stability of an equilibrium point.", "problem": "Consider the one-dimensional stochastic differential equation (SDE)\n$$\ndX_t=-\\alpha X_t^{3}\\,dt+\\beta X_t^{2}\\,dW_t,\n$$\non $\\mathbb{R}$ with parameters $\\alpha0$ and $\\beta0$, where $W_t$ is a standard Brownian motion (Wiener process). Let the candidate Lyapunov function be $V(x)=|x|^{2}$. Using the infinitesimal generator associated with the SDE and the Lyapunov approach grounded in It\\^o calculus, compute $\\mathcal{L}V(x)$ and use its sign to assess local mean-square stability in a neighborhood of the equilibrium $x=0$. Report your final answer as the explicit closed-form expression for $\\mathcal{L}V(x)$ in terms of $x$, $\\alpha$, and $\\beta$.", "solution": "The problem is validated as scientifically sound, well-posed, objective, and self-contained. It is a standard exercise in applying Lyapunov stability theory to stochastic differential equations (SDEs).\n\nThe given one-dimensional SDE is of the form $dX_t = f(X_t) \\, dt + g(X_t) \\, dW_t$, where $W_t$ is a standard Wiener process. By comparing the given equation\n$$\ndX_t = -\\alpha X_t^3 \\, dt + \\beta X_t^2 \\, dW_t\n$$\nwith the general form, we identify the drift coefficient $f(x)$ and the diffusion coefficient $g(x)$ as:\n$$\nf(x) = -\\alpha x^3\n$$\n$$\ng(x) = \\beta x^2\n$$\nThe problem asks to use the candidate Lyapunov function $V(x) = |x|^2$. For $x \\in \\mathbb{R}$, this simplifies to $V(x) = x^2$. This function is twice continuously differentiable, i.e., $V(x) \\in C^2(\\mathbb{R})$, which is a necessary condition for applying Itô's lemma.\n\nThe infinitesimal generator of the SDE, denoted by $\\mathcal{L}$, when applied to a time-independent, twice-differentiable function $V(x)$, is defined by\n$$\n\\mathcal{L}V(x) = f(x) \\frac{dV(x)}{dx} + \\frac{1}{2} [g(x)]^2 \\frac{d^2V(x)}{dx^2}\n$$\nThis formula is a direct consequence of Itô's lemma applied to $V(X_t)$.\n\nFirst, we compute the necessary derivatives of the Lyapunov function $V(x) = x^2$:\nThe first derivative is:\n$$\n\\frac{dV}{dx} = \\frac{d}{dx}(x^2) = 2x\n$$\nThe second derivative is:\n$$\n\\frac{d^2V}{dx^2} = \\frac{d}{dx}(2x) = 2\n$$\nNext, we substitute the expressions for $f(x)$, $g(x)$, $\\frac{dV}{dx}$, and $\\frac{d^2V}{dx^2}$ into the formula for the infinitesimal generator $\\mathcal{L}V(x)$:\n$$\n\\mathcal{L}V(x) = (-\\alpha x^3)(2x) + \\frac{1}{2} (\\beta x^2)^2 (2)\n$$\nWe simplify the terms. The first term becomes:\n$$\n(-\\alpha x^3)(2x) = -2\\alpha x^4\n$$\nThe second term becomes:\n$$\n\\frac{1}{2} (\\beta x^2)^2 (2) = (\\beta^2 x^4)(1) = \\beta^2 x^4\n$$\nCombining these terms, we obtain the expression for $\\mathcal{L}V(x)$:\n$$\n\\mathcal{L}V(x) = -2\\alpha x^4 + \\beta^2 x^4\n$$\nFactoring out the common term $x^4$, we arrive at the final closed-form expression:\n$$\n\\mathcal{L}V(x) = (\\beta^2 - 2\\alpha)x^4\n$$\nTo assess local mean-square stability at the equilibrium point $x=0$, we examine the sign of $\\mathcal{L}V(x)$ in a neighborhood of $x=0$. The point $x=0$ is indeed an equilibrium because $f(0) = -\\alpha(0)^3 = 0$ and $g(0) = \\beta(0)^2 = 0$. The Lyapunov function $V(x) = x^2$ is positive definite, as $V(0) = 0$ and $V(x)  0$ for all $x \\neq 0$.\n\nAccording to the Lyapunov stability criterion for SDEs, the equilibrium $x=0$ is asymptotically stable in the mean-square sense if $\\mathcal{L}V(x)$ is negative definite, i.e., $\\mathcal{L}V(x)  0$ for all $x \\neq 0$ in a neighborhood of the origin.\nIn our expression $\\mathcal{L}V(x) = (\\beta^2 - 2\\alpha)x^4$, the term $x^4$ is strictly positive for any $x \\neq 0$. Therefore, the sign of $\\mathcal{L}V(x)$ is determined entirely by the sign of the constant factor $(\\beta^2 - 2\\alpha)$.\n\nFor the system to be locally mean-square stable, we require:\n$$\n\\beta^2 - 2\\alpha  0 \\implies \\beta^2  2\\alpha\n$$\nIf this condition holds, $\\mathcal{L}V(x)  0$ for all $x \\neq 0$, and the origin is asymptotically stable in the mean-square sense. Conversely, if $\\beta^2 - 2\\alpha  0$, then $\\mathcal{L}V(x)  0$ for $x \\neq 0$, indicating instability. If $\\beta^2 - 2\\alpha = 0$, then $\\mathcal{L}V(x) = 0$, and this Lyapunov function provides no conclusive information about stability.\n\nThe problem asks for the explicit closed-form expression for $\\mathcal{L}V(x)$, which we have computed.", "answer": "$$\n\\boxed{(\\beta^2 - 2\\alpha)x^4}\n$$", "id": "2996128"}, {"introduction": "A solid theoretical understanding of stability is only half the battle; we must also ensure our practical tools, like numerical simulations, are reliable. This exercise [@problem_id:3075314] bridges the gap between the continuous-time theory of SDEs and their discrete-time approximation on a computer. You will analyze the stability of the ubiquitous Ornstein-Uhlenbeck process and compare it to its popular Euler-Maruyama discretization. The goal is to discover that the numerical scheme's stability is not guaranteed and depends critically on the simulation step size $h$, a vital lesson for anyone performing computational analysis of stochastic systems.", "problem": "Consider the scalar Ornstein–Uhlenbeck process defined by the stochastic differential equation (SDE) $$\\mathrm{d}X_{t}=-\\lambda X_{t}\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},$$ where $\\lambda0$ and $\\sigma0$ are fixed constants and $W_{t}$ is a standard Brownian motion. Let $X_{0}$ be an $\\mathbb{R}$-valued random variable with finite second moment. Define the explicit Euler–Maruyama time discretization with uniform step size $h0$ by $$X_{n+1}=X_{n}-\\lambda h\\,X_{n}+\\sigma\\left(W_{(n+1)h}-W_{nh}\\right), \\quad n\\in\\mathbb{N}_{0}.$$\n\nUsing only foundational definitions and facts (in particular, the independence and Gaussianity of Brownian increments, the Itô formula, and basic properties of first-order linear recurrences), do the following:\n\n- Derive from first principles a condition under which the continuous-time process is bounded in probability uniformly over $t\\geq 0$, in the sense that for every $\\varepsilon0$ there exists $r0$ (which may depend on $\\varepsilon$, $\\lambda$, $\\sigma$, and the law of $X_{0}$) such that $$\\sup_{t\\geq 0}\\mathbb{P}\\!\\left(|X_{t}|\\geq r\\right)\\leq \\varepsilon.$$\n- Derive from first principles a condition under which the discrete-time Euler–Maruyama scheme is bounded in probability uniformly over $n\\in\\mathbb{N}_{0}$, in the analogous sense that for every $\\varepsilon0$ there exists $r0$ such that $$\\sup_{n\\in\\mathbb{N}_{0}}\\mathbb{P}\\!\\left(|X_{n}|\\geq r\\right)\\leq \\varepsilon.$$\n- Compare these two conditions and identify regimes of the step size $h$ where the SDE is bounded in probability but the numerical scheme fails to be bounded in probability (i.e., is unstable).\n\nYour final task is to compute, in closed form as a function of $\\lambda$, the largest step size $h_{\\max}$ such that the Euler–Maruyama scheme is bounded in probability uniformly over $n\\in\\mathbb{N}_{0}$ whenever $0hh_{\\max}$, while the continuous-time SDE remains bounded in probability for all $t\\geq 0$.\n\nExpress your final answer for $h_{\\max}$ as a single analytic expression. No rounding is required.", "solution": "The problem asks for an analysis of the stability, defined as boundedness in probability, for both a continuous-time Ornstein–Uhlenbeck process and its discrete-time Euler–Maruyama approximation. A sufficient condition for a process $\\{Y_k\\}_{k \\in I}$ (where the index set $I$ is $[0, \\infty)$ or $\\mathbb{N}_0$) to be bounded in probability is that its second moments are uniformly bounded, i.e., $\\sup_{k \\in I} \\mathbb{E}[|Y_k|^2]  \\infty$. This is a direct consequence of Chebyshev's inequality, which states $\\mathbb{P}(|Y_k| \\ge r) \\le \\frac{\\mathbb{E}[|Y_k|^2]}{r^2}$. If we can establish that $\\sup_{k \\in I} \\mathbb{E}[|Y_k|^2] \\le M$ for some finite constant $M$, then for any given $\\varepsilon  0$, choosing $r = \\sqrt{M/\\varepsilon}$ yields $\\sup_{k \\in I} \\mathbb{P}(|Y_k| \\ge r) \\le \\varepsilon$. We will use this approach to analyze the second moments of both the continuous and discrete processes.\n\nFirst, we analyze the continuous-time process. The SDE is given by\n$$ \\mathrm{d}X_{t}=-\\lambda X_{t}\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}, $$\nwith $\\lambda  0$, $\\sigma  0$, and $\\mathbb{E}[X_0^2]  \\infty$. To study the evolution of the second moment, $\\mathbb{E}[X_t^2]$, we apply the Itô formula to the function $f(x)=x^2$. The Itô formula states that for a process $X_t$ and a twice-differentiable function $f$,\n$$ \\mathrm{d}f(X_t) = f'(X_t)\\,\\mathrm{d}X_t + \\frac{1}{2}f''(X_t)\\,(\\mathrm{d}X_t)^2. $$\nFor $f(x)=x^2$, we have $f'(x)=2x$ and $f''(x)=2$. The quadratic variation $(\\mathrm{d}X_t)^2$ is computed by keeping only the term with the highest order in $\\mathrm{d}t$:\n$$ (\\mathrm{d}X_t)^2 = (-\\lambda X_{t}\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t})^2 = \\sigma^2(\\mathrm{d}W_t)^2 + O((\\mathrm{d}t)^{3/2}) = \\sigma^2\\,\\mathrm{d}t. $$\nSubstituting these into the Itô formula, we get the SDE for $X_t^2$:\n$$ \\mathrm{d}(X_t^2) = (2X_t)(-\\lambda X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t) + \\frac{1}{2}(2)(\\sigma^2\\,\\mathrm{d}t) $$\n$$ \\mathrm{d}(X_t^2) = (-2\\lambda X_t^2 + \\sigma^2)\\,\\mathrm{d}t + 2\\sigma X_t\\,\\mathrm{d}W_t. $$\nTo find the dynamics of the expected value $m(t) = \\mathbb{E}[X_t^2]$, we take the expectation of the integral form of this SDE. The expectation of the Itô integral term is zero, i.e., $\\mathbb{E}\\left[\\int_0^t 2\\sigma X_s\\,\\mathrm{d}W_s\\right] = 0$. This leaves us with an ordinary differential equation (ODE) for $m(t)$:\n$$ \\frac{\\mathrm{d}m(t)}{\\mathrm{d}t} = \\mathbb{E}[-2\\lambda X_t^2 + \\sigma^2] = -2\\lambda m(t) + \\sigma^2. $$\nThis is a first-order linear ODE with initial condition $m(0) = \\mathbb{E}[X_0^2]$. The solution is:\n$$ m(t) = \\left(m(0) - \\frac{\\sigma^2}{2\\lambda}\\right)\\exp(-2\\lambda t) + \\frac{\\sigma^2}{2\\lambda}. $$\nSince it is given that $\\lambda  0$, the exponential term $\\exp(-2\\lambda t)$ decays to $0$ as $t \\to \\infty$. The initial second moment $m(0)$ is finite by assumption. Therefore, $m(t)$ is bounded for all $t \\geq 0$. Specifically, $\\sup_{t\\geq 0} m(t) = \\max\\left(m(0), \\frac{\\sigma^2}{2\\lambda}\\right)$, which is a finite constant. As established earlier, this uniform boundedness of the second moment implies that the process $X_t$ is bounded in probability. The condition required for this result is $\\lambda  0$, which is given in the problem statement. Thus, the continuous-time process is always bounded in probability under the stated assumptions.\n\nNext, we analyze the discrete-time Euler–Maruyama scheme:\n$$ X_{n+1}=X_{n}-\\lambda h\\,X_{n}+\\sigma\\left(W_{(n+1)h}-W_{nh}\\right). $$\nThis can be rewritten as $X_{n+1} = (1 - \\lambda h)X_n + \\sigma \\Delta W_n$, where $\\Delta W_n = W_{(n+1)h}-W_{nh}$. The increments $\\Delta W_n$ of the standard Brownian motion are independent and identically distributed Gaussian random variables with mean $0$ and variance $h$, i.e., $\\Delta W_n \\sim N(0, h)$. We analyze the second moment $m_n = \\mathbb{E}[X_n^2]$. Squaring the recurrence relation, we have:\n$$ X_{n+1}^2 = \\left((1 - \\lambda h)X_n + \\sigma \\Delta W_n\\right)^2 = (1 - \\lambda h)^2 X_n^2 + 2\\sigma (1 - \\lambda h) X_n \\Delta W_n + \\sigma^2 (\\Delta W_n)^2. $$\nTaking the expectation of both sides, we get:\n$$ \\mathbb{E}[X_{n+1}^2] = \\mathbb{E}[(1 - \\lambda h)^2 X_n^2] + \\mathbb{E}[2\\sigma (1 - \\lambda h) X_n \\Delta W_n] + \\mathbb{E}[\\sigma^2 (\\Delta W_n)^2]. $$\nLet $m_n = \\mathbb{E}[X_n^2]$. $X_n$ is determined by the history of the Brownian motion up to time $nh$, and is therefore independent of the future increment $\\Delta W_n$. This allows us to separate the expectations:\n$$ m_{n+1} = (1 - \\lambda h)^2 m_n + 2\\sigma (1 - \\lambda h) \\mathbb{E}[X_n] \\mathbb{E}[\\Delta W_n] + \\sigma^2 \\mathbb{E}[(\\Delta W_n)^2]. $$\nSince $\\mathbb{E}[\\Delta W_n]=0$ and $\\mathbb{E}[(\\Delta W_n)^2] = \\mathrm{Var}(\\Delta W_n)=h$, the equation simplifies to a first-order linear recurrence relation for the second moments:\n$$ m_{n+1} = (1 - \\lambda h)^2 m_n + \\sigma^2 h. $$\nFor the sequence $(m_n)_{n\\in\\mathbb{N}_0}$ to be uniformly bounded, given a finite initial value $m_0 = \\mathbb{E}[X_0^2]$, the coefficient of $m_n$ in the recurrence must have a magnitude strictly less than $1$. That is, $|(1-\\lambda h)^2|  1$. Since a square is always non-negative, this is equivalent to $(1 - \\lambda h)^2  1$. Taking the square root of both sides gives $|1 - \\lambda h|  1$. This inequality can be expressed as:\n$$ -1  1 - \\lambda h  1. $$\nThe right-hand inequality, $1 - \\lambda h  1$, implies $-\\lambda h  0$. Since $\\lambda  0$ and $h  0$, this is always satisfied.\nThe left-hand inequality, $-1  1 - \\lambda h$, implies $\\lambda h  2$, which gives the condition $h  \\frac{2}{\\lambda}$.\nThus, the Euler–Maruyama scheme is bounded in probability (in the sense of uniformly bounded second moments) if and only if $0  h  \\frac{2}{\\lambda}$.\n\nComparing the two results, the continuous-time SDE is bounded in probability for any $\\lambda  0$. The numerical scheme, however, is only bounded in probability if the step size $h$ is restricted to the interval $(0, \\frac{2}{\\lambda})$. The numerical scheme fails to be bounded in probability, and is therefore unstable, for any step size $h \\ge \\frac{2}{\\lambda}$. In this regime, the second moment $\\mathbb{E}[X_n^2]$ grows without bound as $n \\to \\infty$.\n\nThe final task is to compute the largest step size $h_{\\max}$ such that the scheme is bounded in probability for all $h$ in the interval $(0, h_{\\max})$. Based on our derivation, the set of step sizes for which the scheme is bounded in probability is $(0, \\frac{2}{\\lambda})$. The supremum of this set is $\\frac{2}{\\lambda}$. Any choice of $h_{\\max}  \\frac{2}{\\lambda}$ would mean the interval $(0, h_{\\max})$ contains step sizes for which the scheme is not bounded in probability. Therefore, the largest such value is $h_{\\max} = \\frac{2}{\\lambda}$.", "answer": "$$\\boxed{\\frac{2}{\\lambda}}$$", "id": "3075314"}]}