{"hands_on_practices": [{"introduction": "Our first practice involves the cornerstone model of geometric Brownian motion, described by the stochastic differential equation $dX_t = a X_t dt + \\sigma X_t dW_t$. This exercise [@problem_id:3064458] provides a foundational experience in calculating the top Lyapunov exponent, which quantifies the long-term average exponential growth rate of a system. By applying Itô's lemma to $\\ln|X_t|$, you will directly derive how the deterministic drift $a$ and the stochastic noise intensity $\\sigma$ compete to determine the system's ultimate fate, revealing the crucial stability criterion $\\lambda = a - \\frac{1}{2}\\sigma^2$.", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = a X_t \\, dt + \\sigma X_t \\, dW_t$ on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$, where $W_t$ is a standard Brownian motion (also called a Wiener process), $a \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}$ are constants, and $X_0 \\in \\mathbb{R} \\setminus \\{0\\}$ is deterministic. Using only foundational tools such as Itô’s lemma and basic martingale properties, compute the $\\mathbb{P}$-almost sure (a.s.) limit $\\lim_{t \\to \\infty} \\frac{1}{t} \\ln |X_t|$. Your final answer must be a single closed-form analytic expression in terms of $a$ and $\\sigma$.", "solution": "The problem asks for the computation of the almost sure limit $\\lim_{t \\to \\infty} \\frac{1}{t} \\ln |X_t|$, where $X_t$ is the solution to the scalar stochastic differential equation (SDE):\n$$dX_t = a X_t \\, dt + \\sigma X_t \\, dW_t$$\nThe initial condition is $X_0 \\in \\mathbb{R} \\setminus \\{0\\}$, a deterministic constant. The parameters $a$ and $\\sigma$ are real constants, and $W_t$ is a standard one-dimensional Brownian motion. This SDE is a classical example of Geometric Brownian Motion.\n\nTo find an expression for $\\ln|X_t|$, we apply Itô's lemma to the function $f(x) = \\ln|x|$. For $x \\neq 0$, the first and second derivatives of $f(x)$ are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. The solution to the given SDE starting from a non-zero value $X_0$ will almost surely never reach $0$ for $t > 0$, so $f(X_t)$ is well-defined.\n\nLet $Y_t = f(X_t) = \\ln|X_t|$. According to Itô's lemma, the differential $dY_t$ is given by:\n$$dY_t = f'(X_t) \\, dX_t + \\frac{1}{2} f''(X_t) \\, (dX_t)^2$$\nWe substitute the expressions for $dX_t$, $f'(X_t)$, and $f''(X_t)$:\n$$dY_t = \\frac{1}{X_t} (a X_t \\, dt + \\sigma X_t \\, dW_t) + \\frac{1}{2} \\left(-\\frac{1}{X_t^2}\\right) (dX_t)^2$$\nThe quadratic variation term $(dX_t)^2$ is computed using the rules of Itô calculus: $(dt)^2 = 0$, $dt \\, dW_t = 0$, and $(dW_t)^2 = dt$.\n$$(dX_t)^2 = (a X_t \\, dt + \\sigma X_t \\, dW_t)^2 = (\\sigma X_t)^2 (dW_t)^2 = \\sigma^2 X_t^2 \\, dt$$\nWe substitute this back into the expression for $dY_t$:\n$$dY_t = \\frac{1}{X_t} (a X_t \\, dt + \\sigma X_t \\, dW_t) - \\frac{1}{2} \\frac{1}{X_t^2} (\\sigma^2 X_t^2 \\, dt)$$\nSimplifying the terms, we get:\n$$dY_t = (a \\, dt + \\sigma \\, dW_t) - \\frac{1}{2} \\sigma^2 \\, dt$$\n$$dY_t = \\left(a - \\frac{1}{2} \\sigma^2\\right) dt + \\sigma \\, dW_t$$\nThis is an SDE for $Y_t = \\ln|X_t|$. We can solve for $Y_t$ by integrating from $s=0$ to $s=t$:\n$$\\int_0^t dY_s = \\int_0^t \\left(a - \\frac{1}{2} \\sigma^2\\right) ds + \\int_0^t \\sigma \\, dW_s$$\nThis yields:\n$$Y_t - Y_0 = \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma (W_t - W_0)$$\nGiven that $Y_t = \\ln|X_t|$ and by convention $W_0=0$, we have:\n$$\\ln|X_t| - \\ln|X_0| = \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma W_t$$\nRearranging the terms gives the explicit solution for $\\ln|X_t|$:\n$$\\ln|X_t| = \\ln|X_0| + \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma W_t$$\nNow we can compute the desired limit. We divide the entire expression by $t$:\n$$\\frac{1}{t} \\ln|X_t| = \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2} \\sigma^2 + \\sigma \\frac{W_t}{t}$$\nWe take the limit as $t \\to \\infty$:\n$$\\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t| = \\lim_{t \\to \\infty} \\left( \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2} \\sigma^2 + \\sigma \\frac{W_t}{t} \\right)$$\nWe evaluate the limit of each term separately. The limit operation is performed on a $\\mathbb{P}$-almost sure basis.\n$1$. The term $\\frac{\\ln|X_0|}{t}$: Since $X_0$ is a non-zero deterministic constant, $\\ln|X_0|$ is a finite constant. Thus, $\\lim_{t \\to \\infty} \\frac{\\ln|X_0|}{t} = 0$.\n$2$. The term $a - \\frac{1}{2} \\sigma^2$: This is a constant, so its limit is itself.\n$3$. The term $\\sigma \\frac{W_t}{t}$: The key component is the asymptotic behavior of the standard Brownian motion $W_t$. The Strong Law of Large Numbers for Brownian Motion states that $\\lim_{t \\to \\infty} \\frac{W_t}{t} = 0$ almost surely. Since $\\sigma$ is a constant, we have $\\lim_{t \\to \\infty} \\sigma \\frac{W_t}{t} = \\sigma \\cdot 0 = 0$ almost surely.\n\nCombining these results, we find the almost sure limit:\n$$\\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t| = 0 + \\left(a - \\frac{1}{2} \\sigma^2\\right) + 0 = a - \\frac{1}{2} \\sigma^2$$\nThis limit is known as the top Lyapunov exponent of the stochastic dynamical system defined by the SDE. It characterizes the long-term average exponential growth or decay rate of the solutions.", "answer": "$$\\boxed{a - \\frac{1}{2}\\sigma^2}$$", "id": "3064458"}, {"introduction": "Next, we shift our focus from multiplicative to additive noise with the Ornstein-Uhlenbeck process, a model often used for systems exhibiting mean reversion. This practice [@problem_id:3064437] challenges you to compute the Lyapunov exponent by analyzing the separation between two initially distinct trajectories. You will discover a key principle: for linear systems, additive noise does not influence the Lyapunov exponent, as it affects all trajectories identically, causing the stochastic terms to cancel when their difference is considered.", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck process governed by the stochastic differential equation (SDE), defined as\n$$\ndX_t = -\\gamma X_t\\,dt + \\sigma\\,dW_t,\n$$\nwhere $\\gamma > 0$ and $\\sigma\\ge 0$ are constants and $(W_t)_{t\\ge 0}$ is a standard Wiener process (Brownian motion). Two solutions $X_t^{(1)}$ and $X_t^{(2)}$ are driven by the same realization of the Brownian motion, with distinct initial conditions $X_0^{(1)}\\neq X_0^{(2)}$. The largest Lyapunov exponent $\\lambda$ for this random dynamical system is defined by the long-time separation rate\n$$\n\\lambda \\equiv \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\frac{|X_t^{(1)}-X_t^{(2)}|}{|X_0^{(1)}-X_0^{(2)}|}\\right),\n$$\nwhenever the limit exists almost surely.\n\nStarting from core definitions and well-tested results of stochastic calculus, compute the Lyapunov exponent $\\lambda$ for this system and justify, from first principles, why additive noise does not change the value of $\\lambda$. Express the final answer as a single closed-form analytic expression in terms of $\\gamma$ and $\\sigma$. No rounding is required, and no physical units are involved.", "solution": "The problem asks for the computation of the largest Lyapunov exponent $\\lambda$ for a one-dimensional Ornstein-Uhlenbeck process and a justification for why additive noise does not influence its value.\n\nThe governing stochastic differential equation (SDE) is given by:\n$$\ndX_t = -\\gamma X_t\\,dt + \\sigma\\,dW_t\n$$\nwhere $\\gamma > 0$ and $\\sigma \\ge 0$ are constants, and $(W_t)_{t \\ge 0}$ is a standard Wiener process.\n\nLet us consider two solutions, $X_t^{(1)}$ and $X_t^{(2)}$, originating from distinct initial conditions $X_0^{(1)} \\neq X_0^{(2)}$. Crucially, both solutions are driven by the same realization of the Wiener process $W_t$. The respective SDEs are:\n$$\ndX_t^{(1)} = -\\gamma X_t^{(1)}\\,dt + \\sigma\\,dW_t\n$$\n$$\ndX_t^{(2)} = -\\gamma X_t^{(2)}\\,dt + \\sigma\\,dW_t\n$$\nTo analyze the separation of these two trajectories, we define a new process, $\\Delta X_t$, representing the difference between them:\n$$\n\\Delta X_t \\equiv X_t^{(1)} - X_t^{(2)}\n$$\nThe initial difference is $\\Delta X_0 = X_0^{(1)} - X_0^{(2)}$, which is non-zero by the problem's premise.\n\nWe derive the differential equation for $\\Delta X_t$ by subtracting the second SDE from the first. Using the linearity of the differential operator $d$, we have:\n$$\nd(\\Delta X_t) = d(X_t^{(1)} - X_t^{(2)}) = dX_t^{(1)} - dX_t^{(2)}\n$$\nSubstituting the expressions for $dX_t^{(1)}$ and $dX_t^{(2)}$ yields:\n$$\nd(\\Delta X_t) = (-\\gamma X_t^{(1)}\\,dt + \\sigma\\,dW_t) - (-\\gamma X_t^{(2)}\\,dt + \\sigma\\,dW_t)\n$$\nWe can rearrange the terms as follows:\n$$\nd(\\Delta X_t) = (-\\gamma X_t^{(1)} + \\gamma X_t^{(2)})\\,dt + (\\sigma - \\sigma)\\,dW_t\n$$\n$$\nd(\\Delta X_t) = -\\gamma (X_t^{(1)} - X_t^{(2)})\\,dt + 0 \\cdot dW_t\n$$\nBy substituting the definition $\\Delta X_t = X_t^{(1)} - X_t^{(2)}$, we arrive at the governing equation for the separation:\n$$\nd(\\Delta X_t) = -\\gamma \\Delta X_t\\,dt\n$$\nThis equation is a deterministic linear ordinary differential equation (ODE). The stochastic term has completely vanished. This provides the first-principles justification for why the additive noise, represented by the term $\\sigma\\,dW_t$, does not affect the evolution of the difference between trajectories. The noise term is independent of the state $X_t$, meaning it applies an identical random impulse to both trajectories at every instant in time. Consequently, while the trajectories themselves are stochastic, their separation $\\Delta X_t$ evolves deterministically.\n\nWe can solve this ODE by separation of variables:\n$$\n\\frac{d(\\Delta X_t)}{\\Delta X_t} = -\\gamma\\,dt\n$$\nIntegrating both sides from the initial time $t=0$ to a later time $t$ gives:\n$$\n\\int_{\\Delta X_0}^{\\Delta X_t} \\frac{1}{u}\\,du = \\int_0^t -\\gamma\\,ds\n$$\n$$\n\\ln|\\Delta X_t| - \\ln|\\Delta X_0| = -\\gamma t\n$$\nRearranging the terms, we find the explicit solution for the magnitude of the separation:\n$$\n\\ln\\left(\\frac{|\\Delta X_t|}{|\\Delta X_0|}\\right) = -\\gamma t\n$$\nExponentiating both sides gives the solution for the separation itself:\n$$\n|\\Delta X_t| = |\\Delta X_0| \\exp(-\\gamma t)\n$$\nNow, we can compute the Lyapunov exponent $\\lambda$ using its definition:\n$$\n\\lambda \\equiv \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\frac{|X_t^{(1)}-X_t^{(2)}|}{|X_0^{(1)}-X_0^{(2)}|}\\right)\n$$\nSubstituting $|\\Delta X_t|$ and $|\\Delta X_0|$:\n$$\n\\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\frac{|\\Delta X_t|}{|\\Delta X_0|}\\right)\n$$\nUsing our solution for the evolution of the separation, we have:\n$$\n\\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\frac{|\\Delta X_0| \\exp(-\\gamma t)}{|\\Delta X_0|}\\right)\n$$\nSince $\\Delta X_0 \\neq 0$, we can cancel the term $|\\Delta X_0|$:\n$$\n\\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln(\\exp(-\\gamma t))\n$$\nThe natural logarithm and the exponential function are inverses, so:\n$$\n\\lambda = \\lim_{t\\to\\infty}\\frac{1}{t}(-\\gamma t)\n$$\n$$\n\\lambda = \\lim_{t\\to\\infty}(-\\gamma)\n$$\nThe limit of a constant is the constant itself. Therefore, the Lyapunov exponent is:\n$$\n\\lambda = -\\gamma\n$$\nThis result is independent of $\\sigma$, which formally confirms that the magnitude of the additive noise does not alter the Lyapunov exponent for this linear system. The exponent is determined solely by the deterministic, restoring drift term $-\\gamma X_t$. The condition $\\gamma > 0$ ensures that $\\lambda$ is negative, which corresponds to a stochastically stable system where initially separated trajectories converge exponentially over time. The \"almost surely\" condition in the definition of $\\lambda$ is trivially satisfied here because the evolution of the separation is fully deterministic.", "answer": "$$\n\\boxed{-\\gamma}\n$$", "id": "3064437"}, {"introduction": "Having built a foundation with analytical calculations, we now transition to the practical world of numerical estimation for multi-dimensional systems. This hands-on coding exercise [@problem_id:3064480] guides you through implementing the industry-standard algorithm for computing Lyapunov exponents, which relies on periodic reorthonormalization via QR decomposition. Mastering this technique will equip you to analyze the stability of complex linear SDEs where analytical solutions are unavailable, providing a robust tool for research and application.", "problem": "Consider the linear stochastic differential equation with multiplicative noise in $\\mathbb{R}^d$,\n$$\ndY_t \\;=\\; A\\,Y_t\\,dt \\;+\\; \\sum_{i=1}^{m} B_i\\,Y_t\\,dW_t^{(i)},\n$$\nwhere $A \\in \\mathbb{R}^{d\\times d}$ is a constant drift matrix, each $B_i \\in \\mathbb{R}^{d\\times d}$ is a constant diffusion (noise) matrix, and $W_t^{(i)}$ are independent standard Brownian motions (Wiener processes). The top Lyapunov exponent $\\lambda_1$ of this random linear flow is defined by the limit\n$$\n\\lambda_1 \\;=\\; \\lim_{t\\to\\infty} \\frac{1}{t}\\,\\log \\,\\frac{\\|Y_t\\|}{\\|Y_0\\|},\n$$\nfor typical initial conditions $Y_0 \\neq 0$, where $\\|\\cdot\\|$ denotes the Euclidean norm on $\\mathbb{R}^d$.\n\nWrite a complete, runnable program that estimates $\\lambda_1$ using a numerical algorithm that evolves multiple tangent vectors and performs periodic reorthonormalization via the orthogonal-triangular (QR) decomposition (QR). Your program must:\n- Discretize time with step $\\Delta t$ and use the Euler–Maruyama update for the linear system. For each time step, approximate Brownian increments by independent Gaussian random variables $\\Delta W^{(i)} \\sim \\mathcal{N}(0,\\Delta t)$ and update the flow with the matrix\n$$\nM_k \\;=\\; I_d \\;+\\; A\\,\\Delta t \\;+\\; \\sum_{i=1}^{m} B_i\\,\\Delta W_k^{(i)},\n$$\nwhere $I_d$ is the $d\\times d$ identity matrix and $k$ is the time-step index.\n- Evolve $d$ tangent vectors simultaneously, arranged as columns of a matrix $Q_k \\in \\mathbb{R}^{d\\times d}$, starting from an orthonormal basis (for instance, $Q_0 = I_d$). Over a block of $p$ time steps, accumulate the product $V = M_{k+p-1}\\cdots M_{k} Q_k$.\n- Every $p$ time steps, perform QR decomposition $V = Q\\,R$ with $Q \\in \\mathbb{R}^{d\\times d}$ orthogonal and $R \\in \\mathbb{R}^{d\\times d}$ upper-triangular. Record the logarithm of the absolute value of the first diagonal entry $\\log|R_{11}|$. Replace $Q_k$ by $Q$ and continue. If the simulation ends between reorthonormalization points, perform one final QR step to record the remaining growth.\n- Estimate the top Lyapunov exponent by dividing the accumulated sum of $\\log|R_{11}|$ over all QR steps by the total simulated time $T$.\n\nYour program must implement this algorithm and produce results for the following test suite. Use the given random seed for reproducibility in cases with noise. In all cases, set the initial orthonormal matrix to $Q_0 = I_d$.\n\nTest suite:\n1. Analytic baseline in one dimension:\n   - Dimension: $d=1$.\n   - Number of noise components: $m=1$.\n   - Drift: $A = [0.3]$.\n   - Diffusion matrices: $B_1 = [0.8]$.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $42$.\n\n2. Deterministic baseline in two dimensions:\n   - Dimension: $d=2$.\n   - Number of noise components: $m=0$.\n   - Drift: $A = \\mathrm{diag}(0.1,\\,-0.5)$.\n   - Diffusion matrices: none.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $123$ (unused but include for a consistent interface).\n\n3. Stable drift with isotropic multiplicative noise in two dimensions:\n   - Dimension: $d=2$.\n   - Number of noise components: $m=1$.\n   - Drift: $A = \\mathrm{diag}(-0.2,\\,-0.3)$.\n   - Diffusion matrices: $B_1 = \\mathrm{diag}(0.7,\\,0.7)$.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $2023$.\n\n4. Non-commuting drift and noise in three dimensions:\n   - Dimension: $d=3$.\n   - Number of noise components: $m=2$.\n   - Drift:\n     $$\n     A \\;=\\; \\begin{bmatrix}\n     0.0 & 1.0 & 0.0 \\\\\n     -2.0 & -0.1 & 0.0 \\\\\n     0.0 & 0.0 & -0.2\n     \\end{bmatrix}.\n     $$\n   - Diffusion matrices:\n     $$\n     B_1 \\;=\\; \\begin{bmatrix}\n     0.3 & 0.0 & 0.0 \\\\\n     0.0 & 0.3 & 0.0 \\\\\n     0.0 & 0.0 & 0.0\n     \\end{bmatrix}, \\quad\n     B_2 \\;=\\; \\begin{bmatrix}\n     0.0 & 0.5 & 0.0 \\\\\n     -0.5 & 0.0 & 0.0 \\\\\n     0.0 & 0.0 & 0.0\n     \\end{bmatrix}.\n     $$\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 5$.\n   - Random seed: $7$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_j$ is the estimated top Lyapunov exponent for test case $j$. No other text should be printed. Since no physical units are involved, report each $r_j$ as a floating-point number in plain numerical form.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically sound, well-posed, and contains all necessary information to implement the specified numerical algorithm for estimating the top Lyapunov exponent of a linear stochastic differential equation (SDE).\n\n### Principle of the Method\n\nThe problem asks for the numerical estimation of the top Lyapunov exponent, $\\lambda_1$, for a linear stochastic system. The Lyapunov exponents of a dynamical system quantify the average exponential rates of divergence or convergence of nearby trajectories in the state space. For a linear system $dY_t = F(Y_t) dt + G(Y_t) dW_t$, the evolution of an infinitesimal perturbation vector $\\delta Y_t$ is governed by the linearized equation, which in this case is the original SDE itself because the system is already linear:\n$$\nd(\\delta Y_t) \\;=\\; A\\,(\\delta Y_t)\\,dt \\;+\\; \\sum_{i=1}^{m} B_i\\,(\\delta Y_t)\\,dW_t^{(i)}\n$$\nThe top Lyapunov exponent, $\\lambda_1$, corresponds to the maximum average exponential growth rate of the norm of a solution vector, $\\|\\delta Y_t\\|$.\n\nA direct numerical simulation of this equation for a single vector would cause its direction to rapidly align with the direction of maximal growth, making it impossible to resolve other, smaller exponents. Furthermore, its magnitude would either explode or vanish, leading to numerical overflow or underflow.\n\nThe standard algorithm to overcome this, and the one specified in the problem, is based on tracking the evolution of a full orthonormal basis of $d$ tangent vectors. This basis is periodically reorthonormalized to preserve directional information and numerical stability.\n\n### Algorithmic Steps\n\n1.  **Discretization:** The continuous-time SDE is approximated by a discrete-time map using the Euler-Maruyama scheme. Over a small time interval $\\Delta t$, the change in the state $Y_t$ is approximated as:\n    $$\n    \\Delta Y_t \\;\\approx\\; A\\,Y_t\\,\\Delta t \\;+\\; \\sum_{i=1}^{m} B_i\\,Y_t\\,\\Delta W_t^{(i)}\n    $$\n    where the continuous Brownian motion increment $dW_t^{(i)}$ is replaced by a discrete random variable $\\Delta W_k^{(i)} \\sim \\mathcal{N}(0, \\Delta t)$. This gives the update rule $Y_{k+1} = Y_k + \\Delta Y_k$, which can be written as $Y_{k+1} = M_k Y_k$ where $M_k$ is the random update matrix:\n    $$\n    M_k \\;=\\; I_d \\;+\\; A\\,\\Delta t \\;+\\; \\sum_{i=1}^{m} B_i\\,\\Delta W_k^{(i)}\n    $$\n    Here, $I_d$ is the $d \\times d$ identity matrix.\n\n2.  **Tangent Vector Evolution and Reorthonormalization:** Instead of evolving a single vector, we evolve a set of $d$ orthonormal vectors, which form the columns of an orthogonal matrix $Q_k \\in \\mathbb{R}^{d\\times d}$. We initialize with an orthonormal basis, typically $Q_0 = I_d$.\n    The algorithm proceeds in blocks of $p$ steps.\n    - At the beginning of a block, we have an orthonormal matrix $Q_{\\text{start}}$.\n    - We compute the product of the next $p$ random update matrices with $Q_{\\text{start}}$:\n      $$\n      V \\;=\\; (M_{k+p-1} \\cdots M_{k+1} M_k) \\, Q_{\\text{start}}\n      $$\n    - The columns of $V$ represent the evolved (and now generally non-orthonormal) basis vectors. To extract their growth and new orientations, we perform a QR decomposition: $V = QR$.\n    - The matrix $Q \\in \\mathbb{R}^{d\\times d}$ is orthogonal and its columns form the new orthonormal basis for the next block.\n    - The matrix $R \\in \\mathbb{R}^{d\\times d}$ is upper-triangular. Its diagonal elements $R_{ii}$ represent the expansion or contraction factors of the corresponding vectors in the frame. Specifically, $R_{11}$ measures the growth of the first vector, which aligns with the direction of maximal growth over the block.\n\n3.  **Lyapunov Exponent Estimation:** The top Lyapunov exponent is the average rate of logarithmic growth of the most rapidly growing direction. Over one block of $p$ steps (time duration $p\\,\\Delta t$), the logarithmic growth of the first vector is $\\log|R_{11}|$. To find the average rate over the total time $T$, we sum these logarithmic growths over all reorthonormalization steps and divide by $T$.\n    $$\n    \\lambda_1 \\;\\approx\\; \\frac{1}{T} \\sum_{j=1}^{N_{\\text{blocks}}} \\log|R_{11}^{(j)}|\n    $$\n    where $R^{(j)}$ is the R-matrix from the $j$-th QR decomposition. This procedure is robust and converges to the true exponent as $T \\to \\infty$. The implementation will handle a final, partial block by performing a final QR decomposition on the product accumulated over the remaining steps.\n\n### Implementation Details\n\nA single function will implement the described algorithm for a given set of parameters ($d, m, A, \\{B_i\\}, T, \\Delta t, p$, and a random seed).\n- A modern random number generator (`numpy.random.default_rng`) is used for reproducibility.\n- The total number of steps is calculated as $N = \\text{round}(T/\\Delta t)$.\n- A `while` loop iterates through the time steps, processing them in blocks of size at most $p$. This structure naturally handles any final partial block.\n- Within each block, the product $V = M_{k+\\text{steps\\_in\\_block}-1}\\cdots M_k Q_{start}$ is computed.\n- The QR decomposition is performed using `numpy.linalg.qr`.\n- The sum of $\\log|R_{11}|$ is accumulated.\n- The final estimate for $\\lambda_1$ is computed by dividing this sum by the total time $T$.\n- This process is repeated for each test case provided in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_top_lyapunov_exponent(d, m, A, B_list, T, dt, p, seed):\n    \"\"\"\n    Estimates the top Lyapunov exponent for a linear SDE using QR reorthonormalization.\n\n    Args:\n        d (int): Dimension of the system.\n        m (int): Number of noise components.\n        A (np.ndarray): Drift matrix (d x d).\n        B_list (list of np.ndarray): List of diffusion matrices (each d x d).\n        T (float): Total simulation time.\n        dt (float): Time step for discretization.\n        p (int): Number of steps between reorthonormalizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The estimated top Lyapunov exponent.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    n_steps = int(round(T / dt))\n    \n    # Initialize the orthonormal basis Q and the sum of log-growths.\n    Q = np.identity(d)\n    log_growth_sum = 0.0\n\n    step_idx = 0\n    while step_idx < n_steps:\n        # Determine the number of steps for the current block.\n        # This handles the final partial block automatically.\n        steps_in_block = min(p, n_steps - step_idx)\n        \n        # Start the accumulation with the current orthonormal basis.\n        V = np.copy(Q)\n        \n        # Evolve over the steps in the block.\n        for _ in range(steps_in_block):\n            # Generate Brownian increments for this time step.\n            # dW ~ N(0, dt), so standard normal scaled by sqrt(dt).\n            dW = rng.normal(0.0, np.sqrt(dt), size=m)\n\n            # Construct the Euler-Maruyama update matrix M_k.\n            M = np.identity(d) + A * dt\n            if m > 0:\n                noise_term = sum(B * dW_i for B, dW_i in zip(B_list, dW))\n                M += noise_term\n            \n            # Apply the update matrix.\n            V = M @ V\n        \n        # Perform QR decomposition on the evolved matrix V.\n        # V = Q_new * R\n        Q, R = np.linalg.qr(V)\n        \n        # Accumulate the log of the growth factor for the first vector.\n        # The problem asks for log|R_11|.\n        log_growth_sum += np.log(np.abs(R[0, 0]))\n\n        # Increment step_idx by the number of steps processed.\n        step_idx += steps_in_block\n\n    # The Lyapunov exponent is the time-average of the log-growth.\n    lambda_1_estimate = log_growth_sum / T\n    \n    return lambda_1_estimate\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        # 1. Analytic baseline in one dimension\n        {\n            \"d\": 1, \"m\": 1,\n            \"A\": np.array([[0.3]]),\n            \"B_list\": [np.array([[0.8]])],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 42\n        },\n        # 2. Deterministic baseline in two dimensions\n        {\n            \"d\": 2, \"m\": 0,\n            \"A\": np.diag([0.1, -0.5]),\n            \"B_list\": [],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 123\n        },\n        # 3. Stable drift with isotropic multiplicative noise in two dimensions\n        {\n            \"d\": 2, \"m\": 1,\n            \"A\": np.diag([-0.2, -0.3]),\n            \"B_list\": [np.diag([0.7, 0.7])],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 2023\n        },\n        # 4. Non-commuting drift and noise in three dimensions\n        {\n            \"d\": 3, \"m\": 2,\n            \"A\": np.array([\n                [0.0, 1.0, 0.0],\n                [-2.0, -0.1, 0.0],\n                [0.0, 0.0, -0.2]\n            ]),\n            \"B_list\": [\n                np.array([\n                    [0.3, 0.0, 0.0],\n                    [0.0, 0.3, 0.0],\n                    [0.0, 0.0, 0.0]\n                ]),\n                np.array([\n                    [0.0, 0.5, 0.0],\n                    [-0.5, 0.0, 0.0],\n                    [0.0, 0.0, 0.0]\n                ])\n            ],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 5, \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = calculate_top_lyapunov_exponent(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3064480"}]}