{"hands_on_practices": [{"introduction": "This exercise explores the stability of the foundational linear scalar SDE. By deriving the exact solution, we can directly compute the almost sure and mean-square Lyapunov exponents, which measure the long-term exponential growth rates of the system's paths and moments, respectively. This practice is fundamental for understanding the distinct criteria for different types of stochastic stability and how noise can have a surprisingly subtle impact on system behavior [@problem_id:3075620].", "problem": "Consider the linear scalar stochastic differential equation (SDE) in Itô form\n$$\ndX_{t}=a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\quad X_{0}\\in\\mathbb{R},\n$$\nwhere $a\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$ are constants, and $W_{t}$ is a standard Brownian motion (BM). Work from the basic definitions and well-tested results: Itô's formula for smooth functions of Itô processes, properties of Brownian motion increments, and the definitions of equilibrium and stochastic stability.\n\n1. Verify from first principles that the trivial solution $X_{t}\\equiv 0$ is an equilibrium for this SDE.\n2. Starting from the fundamental definitions, derive the exact pathwise solution for $X_{t}$ with arbitrary deterministic $X_{0}$ by applying Itô's formula to an appropriate transformation of $X_{t}$.\n3. Using your solution, compute the almost sure top Lyapunov exponent\n$$\n\\lambda_{\\mathrm{as}}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|\n$$\nand the mean-square exponent\n$$\n\\lambda_{\\mathrm{ms}}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\mathbb{E}\\big(|X_{t}|^{2}\\big).\n$$\nInterpret these quantities to determine precisely when nontrivial solutions starting sufficiently near $0$ remain close to $0$ in the senses of almost sure exponential stability and mean-square exponential stability. Provide, as your final answer, the explicit closed-form expressions for $\\lambda_{\\mathrm{as}}$ and $\\lambda_{\\mathrm{ms}}$. No numerical approximation is required.", "solution": "The user-provided problem is evaluated as valid. It is a standard, well-posed problem in the theory of stochastic differential equations, grounded in established mathematical principles. All necessary information is provided, and the language is unambiguous.\n\nThe problem is to analyze the linear scalar stochastic differential equation (SDE)\n$$\ndX_{t}=a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\quad X_{0}\\in\\mathbb{R},\n$$\nwhere $a$ and $b$ are real constants and $W_{t}$ is a standard one-dimensional Brownian motion.\n\n1.  **Verification of the Trivial Solution as an Equilibrium**\n\nAn equilibrium solution, or fixed point, of a general SDE $dY_t = f(Y_t)dt + g(Y_t)dW_t$ is a constant value $Y_t \\equiv y^*$ such that the drift and diffusion coefficients vanish, i.e., $f(y^*) = 0$ and $g(y^*) = 0$. For such a value, if the process starts at $y^*$, it remains there for all future times.\n\nIn our case, the SDE is $dX_{t} = f(X_t)dt + g(X_t)dW_t$ with drift function $f(x) = ax$ and diffusion function $g(x) = bx$. We test the trivial solution $X_t \\equiv 0$.\nWe must check if $x^* = 0$ satisfies the equilibrium conditions:\n$$\nf(0) = a \\cdot 0 = 0\n$$\n$$\ng(0) = b \\cdot 0 = 0\n$$\nBoth conditions are satisfied. Thus, if we substitute $X_t = 0$ into the SDE, we get $d(0) = a(0)dt + b(0)dW_t$, which simplifies to $0 = 0$. This confirms that $X_t \\equiv 0$ is a solution.\nSince the coefficients $f(x)=ax$ and $g(x)=bx$ are globally Lipschitz continuous (or at least locally Lipschitz, which is sufficient for local uniqueness), for any given initial condition $X_0$, there exists a unique solution. Specifically, for the initial condition $X_0=0$, the unique solution is $X_t \\equiv 0$. Therefore, $X_t\\equiv 0$ is an equilibrium solution of the SDE.\n\n2.  **Derivation of the Exact Pathwise Solution**\n\nTo find the solution for a non-zero initial condition $X_0$, we use a transformation. The structure of the SDE suggests a logarithmic transformation. Let us define a new process $Y_t = \\ln(X_t)$, which is valid for paths where $X_t > 0$. Let us first assume $X_0 > 0$. The linear nature of the coefficients ensures that if $X_0 > 0$, then $X_t > 0$ for all $t > 0$ almost surely.\n\nWe apply Itô's formula to the function $f(x) = \\ln(x)$. The first and second derivatives are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$.\nItô's formula for $Y_t = f(X_t)$ states:\n$$\ndY_t = f'(X_t)dX_t + \\frac{1}{2}f''(X_t)(dX_t)^2\n$$\nThe quadratic variation term $(dX_t)^2$ is computed from the SDE for $X_t$:\n$$\n(dX_t)^2 = (aX_t dt + bX_t dW_t)^2 = (aX_t)^2 (dt)^2 + 2(aX_t)(bX_t) dt dW_t + (bX_t)^2 (dW_t)^2\n$$\nUsing the Itô rules $(dt)^2=0$, $dt dW_t=0$, and $(dW_t)^2=dt$, this simplifies to:\n$$\n(dX_t)^2 = b^2 X_t^2 dt\n$$\nSubstituting $dX_t$, $(dX_t)^2$, $f'(X_t)$, and $f''(X_t)$ into the Itô formula for $dY_t$:\n$$\ndY_t = \\left(\\frac{1}{X_t}\\right)(aX_t dt + bX_t dW_t) + \\frac{1}{2}\\left(-\\frac{1}{X_t^2}\\right)(b^2 X_t^2 dt)\n$$\n$$\ndY_t = (a\\,dt + b\\,dW_t) - \\frac{1}{2}b^2 dt\n$$\n$$\ndY_t = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_t\n$$\nThis is a simple linear SDE for $Y_t$. Integrating from $0$ to $t$:\n$$\nY_t - Y_0 = \\int_0^t \\left(a - \\frac{1}{2}b^2\\right)ds + \\int_0^t b\\,dW_s\n$$\n$$\nY_t = Y_0 + \\left(a - \\frac{1}{2}b^2\\right)t + bW_t\n$$\nwhere $W_t = \\int_0^t dW_s$ since $W_0=0$.\nSubstituting back $Y_t = \\ln(X_t)$ and $Y_0 = \\ln(X_0)$:\n$$\n\\ln(X_t) = \\ln(X_0) + \\left(a - \\frac{1}{2}b^2\\right)t + bW_t\n$$\nExponentiating both sides gives the solution for $X_t$:\n$$\nX_t = \\exp\\left(\\ln(X_0) + \\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\n$$\n$$\nX_t = X_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\n$$\nThis solution was derived assuming $X_0 > 0$. If $X_0  0$, we can consider the process $Z_t = -X_t$ with $Z_0 = -X_0 > 0$. The SDE for $Z_t$ is $dZ_t = -dX_t = -(aX_t dt + bX_t dW_t) = a(-X_t)dt + b(-X_t)dW_t = aZ_t dt + bZ_t dW_t$. This is the same SDE, so $Z_t = Z_0 \\exp\\left((a - \\frac{1}{2}b^2)t + bW_t\\right)$. Substituting back $Z_t = -X_t$ and $Z_0 = -X_0$, we get $X_t = X_0 \\exp\\left((a - \\frac{1}{2}b^2)t + bW_t\\right)$. Thus, the formula is valid for any $X_0 \\in \\mathbb{R}$.\n\n3.  **Computation of Lyapunov Exponents and Stability Analysis**\n\n**Almost Sure Top Lyapunov Exponent ($\\lambda_{\\mathrm{as}}$)**\n\nThe almost sure Lyapunov exponent is defined as $\\lambda_{\\mathrm{as}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|$, provided the limit exists.\nUsing the solution for $X_t$, we have:\n$$\n|X_t| = |X_0| \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\n$$\nTaking the natural logarithm:\n$$\n\\ln|X_t| = \\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t + bW_t\n$$\nDividing by $t$:\n$$\n\\frac{1}{t}\\ln|X_t| = \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2}b^2 + b\\frac{W_t}{t}\n$$\nWe now take the limit as $t \\to \\infty$. The first term vanishes: $\\lim_{t\\to\\infty} \\frac{\\ln|X_0|}{t} = 0$. For the last term, we use the Strong Law of Large Numbers for Brownian motion, which states that $\\lim_{t\\to\\infty} \\frac{W_t}{t} = 0$ almost surely.\nTherefore, the almost sure limit is:\n$$\n\\lambda_{\\mathrm{as}} = \\lim_{t\\to\\infty}\\left(\\frac{\\ln|X_0|}{t} + a - \\frac{1}{2}b^2 + b\\frac{W_t}{t}\\right) = 0 + a - \\frac{1}{2}b^2 + b \\cdot 0 = a - \\frac{1}{2}b^2\n$$\nThe trivial solution $X_t \\equiv 0$ is almost surely exponentially stable if and only if $\\lambda_{\\mathrm{as}}  0$, which corresponds to the condition $a - \\frac{1}{2}b^2  0$.\n\n**Mean-Square Exponent ($\\lambda_{\\mathrm{ms}}$)**\n\nThe mean-square exponent is defined as $\\lambda_{\\mathrm{ms}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\mathbb{E}\\big[|X_{t}|^{2}\\big]$.\nFirst, we compute the second moment $\\mathbb{E}[|X_t|^2]$. Since $X_t$ is real, $|X_t|^2 = X_t^2$.\n$$\nX_t^2 = \\left(X_0 \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_t\\right)\\right)^2 = X_0^2 \\exp\\left(2\\left(a - \\frac{1}{2}b^2\\right)t + 2bW_t\\right)\n$$\n$$\nX_t^2 = X_0^2 \\exp\\left((2a - b^2)t + 2bW_t\\right)\n$$\nTaking the expectation, and noting that $X_0$ is a deterministic constant:\n$$\n\\mathbb{E}[X_t^2] = X_0^2 \\mathbb{E}\\left[\\exp\\left((2a - b^2)t + 2bW_t\\right)\\right] = X_0^2 \\exp((2a - b^2)t) \\mathbb{E}[\\exp(2bW_t)]\n$$\nThe term $\\mathbb{E}[\\exp(2bW_t)]$ is the moment-generating function of the random variable $2bW_t$. We know that $W_t$ is a normal random variable $W_t \\sim N(0, t)$. The moment-generating function for a general normal variable $Z \\sim N(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(kZ)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$. Here, $Z=W_t$, $\\mu=0$, $\\sigma^2=t$, and our \"k\" is $2b$. Thus,\n$$\n\\mathbb{E}[\\exp(2bW_t)] = \\exp\\left((2b)(0) + \\frac{1}{2}(2b)^2 t\\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2 t)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[X_t^2]$:\n$$\n\\mathbb{E}[X_t^2] = X_0^2 \\exp((2a - b^2)t) \\exp(2b^2 t) = X_0^2 \\exp((2a - b^2 + 2b^2)t) = X_0^2 \\exp((2a + b^2)t)\n$$\nNow we can compute $\\lambda_{\\mathrm{ms}}$:\n$$\n\\ln\\mathbb{E}[X_t^2] = \\ln\\left(X_0^2 \\exp((2a + b^2)t)\\right) = \\ln(X_0^2) + (2a + b^2)t\n$$\n$$\n\\frac{1}{t}\\ln\\mathbb{E}[X_t^2] = \\frac{\\ln(X_0^2)}{t} + 2a + b^2\n$$\nTaking the limit as $t \\to \\infty$:\n$$\n\\lambda_{\\mathrm{ms}} = \\lim_{t\\to\\infty}\\left(\\frac{\\ln(X_0^2)}{t} + 2a + b^2\\right) = 0 + 2a + b^2 = 2a + b^2\n$$\nThe trivial solution is mean-square exponentially stable if and only if $\\lambda_{\\mathrm{ms}}  0$, which corresponds to the condition $2a + b^2  0$.\n\nIn summary, the almost sure Lyapunov exponent is $\\lambda_{\\mathrm{as}} = a - \\frac{1}{2}b^2$ and the mean-square exponent is $\\lambda_{\\mathrm{ms}} = 2a + b^2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} a - \\frac{1}{2}b^{2}  2a + b^{2} \\end{pmatrix}}\n$$", "id": "3075620"}, {"introduction": "Since most SDEs cannot be solved explicitly, we need more general tools to assess stability. This practice introduces the powerful Lyapunov's second method, where stability is determined not by the solution, but by the sign of the infinitesimal generator $LV(x)$ acting on a suitable energy-like function $V(x)$. You will apply this technique to a scalar SDE to determine the stability of the trivial solution based on the system parameters [@problem_id:3075587].", "problem": "Consider the scalar stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; f(X_{t})\\,\\mathrm{d}t \\;+\\; g(X_{t})\\,\\mathrm{d}B_{t},\n$$\nwhere $B_{t}$ is a standard one-dimensional Brownian motion, $f(x) = -\\alpha x + \\beta x^{3}$, $g(x) = \\sigma x$, and the parameters satisfy $\\alpha0$, $\\beta0$, and $\\sigma0$. The point $x=0$ is an equilibrium of the drift and diffusion.\n\nStarting from the fundamental It\\^o formula for twice continuously differentiable functions and the definition of the infinitesimal generator acting on such functions, construct a Lyapunov candidate $V(x)$ that is smooth, positive definite in a neighborhood of $x=0$, and radially unbounded. Then compute the infinitesimal generator applied to $V$, denoted by $L V(x)$, explicitly as a function of $x$, $\\alpha$, $\\beta$, and $\\sigma$. Use your expression to assess the sign of $L V(x)$ near $x=0$ and interpret the local stability of the trivial solution $x=0$.\n\nYour final answer must be the simplified closed-form analytic expression for $L V(x)$ in terms of $x$, $\\alpha$, $\\beta$, and $\\sigma$. No rounding is required. Do not include any inequalities or conditions in your final answer.", "solution": "The user has provided a valid problem statement.\n\nThe problem asks for the construction of a Lyapunov function for a given scalar stochastic differential equation (SDE), the computation of its infinitesimal generator, and an assessment of the local stability of the trivial solution.\n\nThe SDE is given by\n$$\n\\mathrm{d}X_{t} \\;=\\; f(X_{t})\\,\\mathrm{d}t \\;+\\; g(X_{t})\\,\\mathrm{d}B_{t}\n$$\nwhere the drift coefficient is $f(x) = -\\alpha x + \\beta x^{3}$ and the diffusion coefficient is $g(x) = \\sigma x$. The parameters satisfy $\\alpha0$, $\\beta0$, and $\\sigma0$. The point $x=0$ is an equilibrium point since $f(0)=0$ and $g(0)=0$.\n\nThe analysis begins with the fundamental definition of the infinitesimal generator $L$ of the Itô process $X_t$, acting on a twice continuously differentiable function $V(x) \\in C^2(\\mathbb{R}, \\mathbb{R})$. The generator is defined from the drift term of the Itô expansion of $V(X_t)$:\n$$\n\\mathrm{d}V(X_t) = L V(X_t) \\,\\mathrm{d}t + g(X_t) \\frac{\\mathrm{d}V}{\\mathrm{d}x}(X_t) \\,\\mathrm{d}B_t\n$$\nThe explicit form of the generator is\n$$\nL V(x) = f(x) \\frac{\\mathrm{d}V}{\\mathrm{d}x}(x) + \\frac{1}{2} g(x)^{2} \\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2}(x)\n$$\n\nThe first step is to construct a suitable Lyapunov candidate function $V(x)$. According to the problem, $V(x)$ must be smooth, positive definite in a neighborhood of $x=0$, and radially unbounded. A simple and standard choice for such a function is a quadratic form. We propose the function:\n$$\nV(x) = x^{2}\n$$\nLet us verify that this choice meets the required criteria:\n$1$. **Smoothness**: $V(x) = x^{2}$ is a polynomial, and thus is infinitely differentiable ($C^{\\infty}$) on $\\mathbb{R}$. This satisfies the smoothness requirement (at least $C^2$).\n$2$. **Positive Definiteness**: $V(0) = 0^{2} = 0$, and for any $x \\neq 0$, $V(x) = x^{2}  0$. Therefore, $V(x)$ is positive definite on its entire domain.\n$3$. **Radial Unboundedness**: As $|x| \\to \\infty$, $V(x) = x^{2} \\to \\infty$. This condition is satisfied.\n\nHaving chosen a valid Lyapunov candidate $V(x)$, we compute its first and second derivatives with respect to $x$:\n$$\n\\frac{\\mathrm{d}V}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x}(x^{2}) = 2x\n$$\n$$\n\\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2} = \\frac{\\mathrm{d}}{\\mathrm{d}x}(2x) = 2\n$$\n\nNow, we substitute the expressions for $f(x)$, $g(x)$, and the derivatives of $V(x)$ into the formula for the infinitesimal generator $L V(x)$:\n$$\nL V(x) = (-\\alpha x + \\beta x^{3})(2x) + \\frac{1}{2}(\\sigma x)^{2}(2)\n$$\nWe expand and simplify the terms:\n$$\nL V(x) = -2\\alpha x^{2} + 2\\beta x^{4} + \\frac{1}{2}\\sigma^{2}x^{2}(2)\n$$\n$$\nL V(x) = -2\\alpha x^{2} + 2\\beta x^{4} + \\sigma^{2}x^{2}\n$$\nCombining the terms with $x^{2}$:\n$$\nL V(x) = (\\sigma^{2} - 2\\alpha)x^{2} + 2\\beta x^{4}\n$$\nThis is the required simplified closed-form expression for $L V(x)$.\n\nThe final part of the task is to use this expression to assess the sign of $L V(x)$ near the origin $x=0$ and interpret the local stability of the trivial solution. For values of $x$ in a small neighborhood of $0$, the behavior of the polynomial $L V(x)$ is dominated by its lowest-order term. In this case, the lowest-order term is $(\\sigma^{2} - 2\\alpha)x^{2}$.\n\nThe sign of this dominant term near $x=0$ (for $x \\neq 0$) is determined solely by the sign of the coefficient $(\\sigma^{2} - 2\\alpha)$, since $x^{2}$ is always positive.\n\nAccording to Lyapunov's stability theorem for SDEs, the trivial solution $x=0$ is asymptotically stable in probability if there exists a neighborhood of the origin where $L V(x)$ is strictly negative (for $x \\neq 0$). Conversely, if $L V(x)$ is strictly positive in a neighborhood of the origin, the solution is unstable.\n\nLet's analyze the sign of $L V(x)$:\n$1$. If $\\sigma^{2} - 2\\alpha  0$, which is equivalent to $2\\alpha  \\sigma^{2}$:\nIn this case, the term $(\\sigma^{2} - 2\\alpha)x^{2}$ is negative. The full expression is $L V(x) = x^{2}[(\\sigma^{2} - 2\\alpha) + 2\\beta x^{2}]$. For $x$ sufficiently close to $0$, the constant negative term $(\\sigma^{2} - 2\\alpha)$ will dominate the $2\\beta x^{2}$ term. Thus, there exists a neighborhood of $x=0$ where $L V(x)  0$ for $x \\neq 0$. This implies that the trivial solution $x=0$ is locally asymptotically stable in probability. The linear stabilizing drift term proportional to $-\\alpha x$ is strong enough to overcome the destabilizing effect of the noise proportional to $\\sigma x$.\n\n$2$. If $\\sigma^{2} - 2\\alpha  0$, which is equivalent to $2\\alpha  \\sigma^{2}$:\nIn this case, the coefficient $(\\sigma^{2} - 2\\alpha)$ is positive. The second term, $2\\beta x^{4}$, is also positive for all $x \\neq 0$ since $\\beta  0$. Therefore, $L V(x)$ is the sum of two positive terms and is strictly positive for all $x \\neq 0$. This implies that the trivial solution $x=0$ is unstable. The noise is too strong for the linear drift to stabilize the system.\n\n$3$. If $\\sigma^{2} - 2\\alpha = 0$, which is equivalent to $2\\alpha = \\sigma^{2}$:\nIn this case, $L V(x) = 2\\beta x^{4}$. Since $\\beta  0$, $L V(x)  0$ for all $x \\neq 0$. This also implies that the trivial solution $x=0$ is unstable.\n\nIn summary, the local stability of the trivial solution $x=0$ depends critically on the relative magnitudes of the parameters $\\alpha$ and $\\sigma$. The solution is locally asymptotically stable if $2\\alpha  \\sigma^{2}$ and unstable if $2\\alpha \\leq \\sigma^{2}$. The expression for $L V(x)$ provides the formal basis for this conclusion.", "answer": "$$\n\\boxed{(\\sigma^{2} - 2\\alpha)x^{2} + 2\\beta x^{4}}\n$$", "id": "3075587"}, {"introduction": "Real-world systems often involve multiple interacting components, requiring a multi-dimensional analysis. This exercise extends Lyapunov's method to linear systems of SDEs in $\\mathbb{R}^n$. By applying vector Itô calculus to a quadratic Lyapunov function $V(x) = x^{\\top}Px$, you will derive the fundamental expression that forms the basis of the Lyapunov matrix equation for stochastic stability [@problem_id:3075618].", "problem": "Consider a stochastic differential equation (SDE) on $\\mathbb{R}^{n}$ driven by $m$ independent standard Brownian motions $W_t^{1},\\dots,W_t^{m}$, with dynamics\n$$\ndX_t \\;=\\; A X_t \\, dt \\;+\\; \\sum_{i=1}^{m} B_i X_t \\, dW_t^{i},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ and $B_i \\in \\mathbb{R}^{n \\times n}$ for $i=1,\\dots,m$ are constant matrices. Let $P \\in \\mathbb{R}^{n \\times n}$ be constant, symmetric, and positive definite, and define the quadratic Lyapunov candidate\n$$\nV(x) \\;=\\; x^{\\top} P x.\n$$\nUsing the rules of Itô calculus for vector-valued processes and the natural filtration $\\mathcal{F}_t$ generated by $\\{W_s^{i} : 0 \\leq s \\leq t,\\, i=1,\\dots,m\\}$, compute the differential $dV(X_t)$ and then obtain the conditional expectation $\\mathbb{E}\\!\\left[dV(X_t)\\,\\big|\\,\\mathcal{F}_t\\right]$ expressed explicitly in terms of $X_t$, $A$, $B_i$, and $P$. Your final answer must be a single closed-form analytic expression for $\\mathbb{E}\\!\\left[dV(X_t)\\,\\big|\\,\\mathcal{F}_t\\right]$. No rounding is required.", "solution": "The problem asks us to compute the conditional expectation of the differential of the Lyapunov function $V(X_t) = X_t^\\top P X_t$. This quantity, $\\mathbb{E}[dV(X_t) | \\mathcal{F}_t]$, gives the expected instantaneous change of $V(X_t)$ given the history of the process up to time $t$. By definition, this is equal to $L V(X_t) dt$, where $L$ is the infinitesimal generator of the process $X_t$.\n\nTo compute this, we apply the multi-dimensional Itô's formula for a function $V(x) = x^\\top P x$. The formula for $dV(X_t)$ is:\n$$\ndV(X_t) = \\left( \\nabla V(X_t) \\right)^\\top dX_t + \\frac{1}{2} \\mathrm{Tr}\\left( \\nabla^2 V(X_t) \\, d[X, X]_t \\right)\n$$\nwhere $\\nabla V$ is the gradient, $\\nabla^2 V$ is the Hessian matrix, and $d[X, X]_t$ is the quadratic variation tensor of the process $X_t$.\n\n1.  **Compute the Gradient and Hessian of $V(x)$:**\n    For $V(x) = x^\\top P x$, and with $P$ being symmetric, the gradient vector is:\n    $$\n    \\nabla V(x) = (P + P^\\top)x = 2Px\n    $$\n    The Hessian matrix (matrix of second derivatives) is:\n    $$\n    \\nabla^2 V(x) = P + P^\\top = 2P\n    $$\n\n2.  **Compute the Quadratic Variation $d[X, X]_t$:**\n    The quadratic variation tensor is $(dX_t)(dX_t)^\\top$. We only need to consider the stochastic parts, as terms with $(dt)^2$ or $dt \\, dW_t$ are zero in Itô calculus.\n    $$\n    dX_t = A X_t dt + \\sum_{i=1}^{m} B_i X_t dW_t^i\n    $$\n    $$\n    d[X, X]_t = \\left(\\sum_{i=1}^{m} B_i X_t dW_t^i\\right) \\left(\\sum_{j=1}^{m} B_j X_t dW_t^j\\right)^\\top\n    $$\n    Since $dW_t^i dW_t^j = \\delta_{ij} dt$ (where $\\delta_{ij}$ is the Kronecker delta), cross-terms vanish:\n    $$\n    d[X, X]_t = \\sum_{i=1}^{m} (B_i X_t)(B_i X_t)^\\top dt = \\left( \\sum_{i=1}^{m} B_i X_t X_t^\\top B_i^\\top \\right) dt\n    $$\n\n3.  **Apply Itô's Formula:**\n    Now we substitute these components back into Itô's formula.\n    The drift contribution from the first term is:\n    $$\n    (\\text{drift from } dX_t) = (\\nabla V(X_t))^\\top (A X_t dt) = (2PX_t)^\\top (AX_t dt) = 2 X_t^\\top P A X_t dt\n    $$\n    Since $X_t^\\top P A X_t$ is a scalar, it equals its own transpose: $(X_t^\\top P A X_t)^\\top = X_t^\\top A^\\top P^\\top X_t = X_t^\\top A^\\top P X_t$. We can write $2 X_t^\\top P A X_t = X_t^\\top P A X_t + X_t^\\top A^\\top P X_t = X_t^\\top (PA + A^\\top P) X_t$.\n    So the drift from the first term is $X_t^\\top (A^\\top P + PA) X_t dt$.\n\n    The second term (Itô correction) is:\n    $$\n    \\frac{1}{2} \\mathrm{Tr}\\left( (2P) \\left( \\sum_{i=1}^{m} B_i X_t X_t^\\top B_i^\\top \\right) dt \\right) = \\mathrm{Tr}\\left( \\sum_{i=1}^{m} P B_i X_t X_t^\\top B_i^\\top \\right) dt\n    $$\n    Using the cyclic property of the trace, $\\mathrm{Tr}(CDE) = \\mathrm{Tr}(ECD)$:\n    $$\n    \\mathrm{Tr}(P B_i X_t X_t^\\top B_i^\\top) = \\mathrm{Tr}(X_t^\\top B_i^\\top P B_i X_t)\n    $$\n    Since $X_t^\\top B_i^\\top P B_i X_t$ is a scalar (a $1 \\times 1$ matrix), its trace is the scalar itself. So the Itô correction term becomes:\n    $$\n    \\left( \\sum_{i=1}^{m} X_t^\\top B_i^\\top P B_i X_t \\right) dt\n    $$\n\n4.  **Combine Terms and Find the Conditional Expectation:**\n    The full expression for $dV(X_t)$ contains a drift part (terms with $dt$) and a martingale part (terms with $dW_t^i$).\n    The drift part (i.e., $L V(X_t) dt$) is the sum of the two drift terms we calculated:\n    $$\n    L V(X_t) dt = \\left( X_t^\\top (A^\\top P + PA) X_t + \\sum_{i=1}^{m} X_t^\\top B_i^\\top P B_i X_t \\right) dt\n    $$\n    This can be factored as:\n    $$\n    L V(X_t) dt = \\left( X_t^\\top \\left( A^\\top P + PA + \\sum_{i=1}^{m} B_i^\\top P B_i \\right) X_t \\right) dt\n    $$\n    The full differential is $dV(X_t) = L V(X_t) dt + (\\text{martingale terms})$. The conditional expectation $\\mathbb{E}[ \\cdot | \\mathcal{F}_t]$ of the martingale part is zero. Since $X_t$ is $\\mathcal{F}_t$-measurable, $L V(X_t)$ is also $\\mathcal{F}_t$-measurable, so $\\mathbb{E}[L V(X_t) dt | \\mathcal{F}_t] = L V(X_t) dt$.\n\n    Therefore, the final result is:\n    $$\n    \\mathbb{E}\\!\\left[dV(X_t)\\,\\big|\\,\\mathcal{F}_t\\right] = \\left( X_t^{\\top} \\left( A^{\\top}P + PA + \\sum_{i=1}^{m} B_i^{\\top} P B_i \\right) X_t \\right) dt\n    $$\n    This expression is central to the mean-square stability analysis of linear stochastic systems. The system is mean-square stable if the matrix $A^{\\top}P + PA + \\sum_{i=1}^{m} B_i^{\\top} P B_i$ is negative definite.", "answer": "$$ \\boxed{ \\left( X_t^{\\top} \\left( A^{\\top}P + PA + \\sum_{i=1}^{m} B_i^{\\top} P B_i \\right) X_t \\right) dt } $$", "id": "3075618"}]}