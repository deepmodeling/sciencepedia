{"hands_on_practices": [{"introduction": "This first exercise forms the theoretical bedrock for understanding the Ornstein-Uhlenbeck (OU) process in its steady state. By solving the stationary Fokker-Planck equation, you will derive the explicit form of the stationary probability density, revealing its Gaussian nature. This fundamental calculation allows you to determine the process's long-term average value, $\\mathbb{E}_{\\pi}[X]$, and the extent of its fluctuations, $\\operatorname{Var}_{\\pi}(X)$, directly from the model parameters [@problem_id:3076368].", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE)\n$$\ndX_{t}=-\\theta\\big(X_{t}-\\mu\\big)\\,dt+\\sigma\\,dW_{t},\n$$\nwhere $W_{t}$ is a standard Brownian motion, $\\theta0$, $\\sigma0$, and $\\mu\\in\\mathbb{R}$ are constants. Assume the process is ergodic and admits a stationary distribution $\\pi$ with density $p_{\\pi}(x)$ on $\\mathbb{R}$. Starting from the forward Kolmogorov (Fokker–Planck) equation for a one-dimensional diffusion with drift $b(x)$ and constant diffusion coefficient $\\sigma$,\n$$\n\\frac{\\partial}{\\partial t}p(x,t)=-\\frac{\\partial}{\\partial x}\\big(b(x)\\,p(x,t)\\big)+\\frac{1}{2}\\sigma^{2}\\,\\frac{\\partial^{2}}{\\partial x^{2}}p(x,t),\n$$\nderive the stationary density $p_{\\pi}(x)$ by setting $\\frac{\\partial}{\\partial t}p(x,t)=0$ and imposing physically consistent boundary conditions ensuring normalizability and vanishing flux at infinity. Then, using your derived $p_{\\pi}(x)$, compute the stationary mean $\\mathbb{E}_{\\pi}[X]$ and the stationary variance $\\operatorname{Var}_{\\pi}(X)$ in closed form. As a cross-check, you may verify your results by deriving the stationary moment equations via Itô calculus. Express your final answer as a single row matrix containing $\\mathbb{E}_{\\pi}[X]$ and $\\operatorname{Var}_{\\pi}(X)$. No rounding is required, and no units are to be included.", "solution": "The problem statement is a standard exercise in the theory of stochastic differential equations and is determined to be valid. It is scientifically grounded, well-posed, and objective. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe one-dimensional Ornstein-Uhlenbeck (OU) process is described by the stochastic differential equation (SDE):\n$$\ndX_{t}=-\\theta\\big(X_{t}-\\mu\\big)\\,dt+\\sigma\\,dW_{t}\n$$\nwhere $\\theta0$, $\\sigma0$, $\\mu\\in\\mathbb{R}$, and $W_{t}$ is a standard Brownian motion. This SDE is of the general form $dX_t = b(X_t)dt + \\sqrt{2D(X_t)}dW_t$, where $b(x)$ is the drift function and $D(x)$ is the diffusion function. In our case, the drift is $b(x) = -\\theta(x-\\mu)$ and the diffusion term is constant, which we can write as $\\sigma = \\sqrt{2D}$, implying a constant diffusion coefficient $D = \\frac{\\sigma^2}{2}$.\n\nThe time evolution of the probability density function $p(x,t)$ of the process $X_t$ is governed by the forward Kolmogorov equation, also known as the Fokker-Planck equation. For a general one-dimensional diffusion with drift $b(x)$ and constant diffusion coefficient (as given in the problem, $\\frac{1}{2}\\sigma^2$), the equation is:\n$$\n\\frac{\\partial}{\\partial t}p(x,t)=-\\frac{\\partial}{\\partial x}\\big(b(x)\\,p(x,t)\\big)+\\frac{1}{2}\\sigma^{2}\\,\\frac{\\partial^{2}}{\\partial x^{2}}p(x,t)\n$$\nA stationary distribution $\\pi$ with density $p_{\\pi}(x)$ is a probability distribution that does not change over time. Thus, for the stationary density, we must have $\\frac{\\partial}{\\partial t}p_{\\pi}(x,t) = 0$. Substituting $p(x,t)$ with $p_{\\pi}(x)$ and setting the time derivative to zero, we obtain an ordinary differential equation for $p_{\\pi}(x)$:\n$$\n0 = -\\frac{d}{dx}\\big(b(x)\\,p_{\\pi}(x)\\big)+\\frac{1}{2}\\sigma^{2}\\,\\frac{d^{2}}{dx^{2}}p_{\\pi}(x)\n$$\nThis equation can be expressed in terms of the probability flux $J(x)$:\n$$\n\\frac{d}{dx}J(x) = 0, \\quad \\text{where} \\quad J(x) = b(x)p_{\\pi}(x) - \\frac{1}{2}\\sigma^2 \\frac{d p_{\\pi}(x)}{dx}\n$$\nThe condition $\\frac{dJ(x)}{dx} = 0$ implies that the flux $J(x)$ must be a constant. For a process confined to $\\mathbb{R}$, a physically meaningful stationary state requires that the probability flux vanishes at infinity, i.e., $\\lim_{x\\to\\pm\\infty} J(x) = 0$. This is because there should be no net flow of probability out of or into the system at the boundaries for the distribution to remain stationary. This implies that the constant flux must be zero everywhere, $J(x) = 0$.\n\nSetting the flux to zero gives a first-order separable ordinary differential equation for $p_{\\pi}(x)$:\n$$\n\\frac{1}{2}\\sigma^2 \\frac{d p_{\\pi}(x)}{dx} = b(x)p_{\\pi}(x)\n$$\nSubstituting the drift for the OU process, $b(x) = -\\theta(x-\\mu)$:\n$$\n\\frac{1}{2}\\sigma^2 \\frac{d p_{\\pi}(x)}{dx} = -\\theta(x-\\mu)p_{\\pi}(x)\n$$\nWe can separate variables, assuming $p_{\\pi}(x)  0$:\n$$\n\\frac{1}{p_{\\pi}(x)} dp_{\\pi}(x) = -\\frac{2\\theta}{\\sigma^2}(x-\\mu) dx\n$$\nIntegrating both sides, we get:\n$$\n\\int \\frac{1}{p_{\\pi}(x)} dp_{\\pi}(x) = \\int -\\frac{2\\theta}{\\sigma^2}(x-\\mu) dx\n$$\n$$\n\\ln(p_{\\pi}(x)) = -\\frac{2\\theta}{\\sigma^2} \\frac{(x-\\mu)^2}{2} + C' = -\\frac{\\theta(x-\\mu)^2}{\\sigma^2} + C'\n$$\nwhere $C'$ is the constant of integration. Exponentiating both sides yields the form of the stationary density:\n$$\np_{\\pi}(x) = \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2} + C'\\right) = A \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2}\\right)\n$$\nwhere $A = e^{C'}$ is a normalization constant. This expression has the form of a Gaussian (normal) probability density. The standard form of a normal density with mean $\\mu_N$ and variance $\\sigma_N^2$ is:\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_N^2}} \\exp\\left(-\\frac{(x-\\mu_N)^2}{2\\sigma_N^2}\\right)\n$$\nBy comparing the exponent of our derived density with the standard form, we can identify the mean and variance of the stationary distribution.\n$$\n\\frac{(x-\\mu_N)^2}{2\\sigma_N^2} = \\frac{\\theta(x-\\mu)^2}{\\sigma^2}\n$$\nFrom this comparison, we deduce:\n1. The mean of the stationary distribution is $\\mu_N = \\mu$.\n2. The term for the variance gives $2\\sigma_N^2 = \\frac{\\sigma^2}{\\theta}$, which implies the variance is $\\sigma_N^2 = \\frac{\\sigma^2}{2\\theta}$.\n\nThus, the stationary distribution $\\pi$ is a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{2\\theta}$, denoted as $N(\\mu, \\frac{\\sigma^2}{2\\theta})$.\n\nThe stationary mean $\\mathbb{E}_{\\pi}[X]$ and stationary variance $\\operatorname{Var}_{\\pi}(X)$ are, by definition, the mean and variance of this stationary distribution.\nThe stationary mean is:\n$$\n\\mathbb{E}_{\\pi}[X] = \\mu\n$$\nThe stationary variance is:\n$$\n\\operatorname{Var}_{\\pi}(X) = \\frac{\\sigma^2}{2\\theta}\n$$\n\nAs a cross-check, we can use Itô's lemma to derive the equations for the moments. For the first moment $m_1(t) = \\mathbb{E}[X_t]$, taking the expectation of the SDE:\n$$\nd\\mathbb{E}[X_t] = \\mathbb{E}[-\\theta(X_t - \\mu)dt + \\sigma dW_t] = -\\theta(\\mathbb{E}[X_t] - \\mu)dt\n$$\nThe stationary mean $m_{1,\\pi} = \\mathbb{E}_{\\pi}[X]$ must satisfy $\\frac{d m_{1,\\pi}}{dt} = 0$, so $-\\theta(m_{1,\\pi}-\\mu)=0$. Since $\\theta0$, we have $m_{1,\\pi} = \\mu$.\n\nFor the second moment $m_2(t) = \\mathbb{E}[X_t^2]$, we use Itô's formula for $f(x)=x^2$:\n$$\nd(X_t^2) = 2X_t dX_t + \\frac{1}{2}(2)(dX_t)^2 = 2X_t(-\\theta(X_t-\\mu)dt + \\sigma dW_t) + \\sigma^2 dt\n$$\n$$\nd(X_t^2) = (-2\\theta X_t^2 + 2\\theta\\mu X_t + \\sigma^2)dt + 2\\sigma X_t dW_t\n$$\nTaking the expectation:\n$$\nd\\mathbb{E}[X_t^2] = (-2\\theta\\mathbb{E}[X_t^2] + 2\\theta\\mu\\mathbb{E}[X_t] + \\sigma^2)dt\n$$\nThe stationary second moment $m_{2,\\pi} = \\mathbb{E}_{\\pi}[X^2]$ satisfies $\\frac{d m_{2,\\pi}}{dt}=0$ and uses $m_{1,\\pi}=\\mu$:\n$$\n0 = -2\\theta m_{2,\\pi} + 2\\theta\\mu(\\mu) + \\sigma^2 \\implies 2\\theta m_{2,\\pi} = 2\\theta\\mu^2 + \\sigma^2\n$$\n$$\nm_{2,\\pi} = \\mu^2 + \\frac{\\sigma^2}{2\\theta}\n$$\nThe stationary variance is $\\operatorname{Var}_{\\pi}(X) = m_{2,\\pi} - (m_{1,\\pi})^2 = \\left(\\mu^2 + \\frac{\\sigma^2}{2\\theta}\\right) - \\mu^2 = \\frac{\\sigma^2}{2\\theta}$.\nThe results from both methods are consistent.\n\nThe final answer requires the stationary mean and variance as a row matrix.\nStationary Mean: $\\mathbb{E}_{\\pi}[X] = \\mu$\nStationary Variance: $\\operatorname{Var}_{\\pi}(X) = \\frac{\\sigma^2}{2\\theta}$\nThe resulting matrix is $\\begin{pmatrix} \\mu  \\frac{\\sigma^2}{2\\theta} \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\mu  \\frac{\\sigma^2}{2\\theta} \\end{pmatrix}}\n$$", "id": "3076368"}, {"introduction": "Having established how to find the stationary distribution, this next practice challenges you to think critically about *when* such a distribution exists. The exercise focuses on the crucial role of the mean-reversion parameter, $\\theta$, in ensuring the stability of the system. By analyzing the case where $\\theta \\lt 0$, you will gain a deeper intuition for why mean-reverting drift is a necessary condition for the process to settle into a well-defined, normalizable equilibrium [@problem_id:3076448].", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE)\n$$\ndX_t = \\theta\\big(\\mu - X_t\\big)\\,dt + \\sigma\\,dW_t,\n$$\nwhere $W_t$ is a standard Brownian motion, $\\theta \\in \\mathbb{R}$, $\\mu \\in \\mathbb{R}$, and $\\sigma  0$. A stationary distribution is understood here as a time-independent probability density $p^\\ast(x)$ that solves the forward Kolmogorov (Fokker–Planck) equation for the process, is nonnegative, and integrates to $1$ over $\\mathbb{R}$. Starting from the definition of stationarity via the forward Kolmogorov (Fokker–Planck) equation and basic properties of linear drift and constant diffusion, determine which statement correctly explains why the OU process with $\\theta  0$ does not admit a stationary distribution.\n\nSelect the single best option.\n\nA. For $\\theta  0$, the drift pushes the process away from the mean, and the stationary forward Kolmogorov (Fokker–Planck) equation enforces a zero probability current at infinity, which yields a candidate stationary solution whose exponent grows quadratically at infinity, making it nonintegrable; equivalently, the first and second moments grow exponentially in time, ruling out stationarity.\n\nB. For $\\theta \\neq 0$, the presence of constant diffusion $\\sigma  0$ guarantees ergodicity and hence a stationary distribution for all signs of $\\theta$, because the noise term smooths and confines the trajectories.\n\nC. A stationary distribution exists for $\\theta  0$ provided the initial condition satisfies $X_0 = \\mu$; otherwise the mean diverges, but stationarity still holds in distribution due to symmetry around $\\mu$.\n\nD. The existence of a stationary distribution depends on the sign of $\\mu$: choosing $\\mu  0$ yields a stationary distribution for any $\\theta$, while choosing $\\mu  0$ precludes it when $\\theta  0$.\n\nE. When $\\theta  0$, the invariant distribution exists but is non-Gaussian with heavy tails, because negative linear drift produces a balance with diffusion that leads to a stable, nonintegrable but normalizable law.", "solution": "Begin from the forward Kolmogorov (Fokker–Planck) equation for an Itô diffusion $dX_t = b(X_t)\\,dt + a(X_t)\\,dW_t$, which for density $p(x,t)$ reads\n$$\n\\frac{\\partial}{\\partial t} p(x,t) = -\\frac{\\partial}{\\partial x}\\big(b(x)\\,p(x,t)\\big) + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}\\big(a(x)^2\\,p(x,t)\\big).\n$$\nA stationary density $p^\\ast(x)$ satisfies\n$$\n0 = -\\frac{d}{dx}\\big(b(x)\\,p^\\ast(x)\\big) + \\frac{1}{2}\\frac{d^2}{dx^2}\\big(a(x)^2\\,p^\\ast(x)\\big),\n$$\ntogether with normalization $\\int_{\\mathbb{R}} p^\\ast(x)\\,dx = 1$ and appropriate boundary conditions (e.g., zero probability current at $\\pm\\infty$ for natural boundaries).\n\nFor the Ornstein–Uhlenbeck process,\n$$\nb(x) = \\theta(\\mu - x), \\qquad a(x) = \\sigma,\n$$\nso the stationary equation becomes\n$$\n0 = -\\frac{d}{dx}\\Big(\\theta(\\mu - x)\\,p^\\ast(x)\\Big) + \\frac{\\sigma^2}{2}\\,\\frac{d^2}{dx^2} p^\\ast(x).\n$$\nThe stationary probability current $J(x)$ is\n$$\nJ(x) = b(x)\\,p^\\ast(x) - \\frac{\\sigma^2}{2}\\,\\frac{d}{dx}p^\\ast(x).\n$$\nFor a stationary distribution with natural boundaries at $\\pm\\infty$, we require $J(x)\\to 0$ as $|x|\\to\\infty$, and the simplest consistent choice is $J(x)\\equiv 0$, yielding the first-order ordinary differential equation\n$$\n\\frac{\\sigma^2}{2}\\,\\frac{d}{dx}p^\\ast(x) = \\theta(\\mu - x)\\,p^\\ast(x).\n$$\nSeparating variables and integrating,\n$$\n\\frac{d}{dx}\\ln p^\\ast(x) = \\frac{2\\theta}{\\sigma^2}(\\mu - x)\n\\quad\\Longrightarrow\\quad\n\\ln p^\\ast(x) = \\frac{2\\theta}{\\sigma^2}\\left(\\mu x - \\frac{x^2}{2}\\right) + C,\n$$\nso up to a normalization constant,\n$$\np^\\ast(x) \\propto \\exp\\!\\left(\\frac{2\\theta}{\\sigma^2}\\left(\\mu x - \\frac{x^2}{2}\\right)\\right)\n= \\exp\\!\\left(-\\frac{\\theta}{\\sigma^2}(x - \\mu)^2\\right).\n$$\nFor integrability over $\\mathbb{R}$, the quadratic form in the exponent must be negative definite as $|x|\\to\\infty$. This requires $-\\frac{\\theta}{\\sigma^2}  0$, i.e., $\\theta  0$. If $\\theta  0$, then $-\\frac{\\theta}{\\sigma^2}  0$, and the exponent grows like $+\\text{const}\\cdot(x - \\mu)^2$, so $p^\\ast(x)$ diverges as $|x|\\to\\infty$ and cannot be normalized. Hence, there is no stationary density for $\\theta  0$.\n\nA complementary moment-based argument confirms the lack of stationarity. Using $b(x) = \\theta(\\mu - x)$ and $a(x) = \\sigma$, the mean $m(t) = \\mathbb{E}[X_t]$ satisfies\n$$\n\\frac{d}{dt} m(t) = \\mathbb{E}[b(X_t)] = \\theta\\big(\\mu - m(t)\\big),\n$$\nwhose solution is\n$$\nm(t) = \\mu + \\big(m(0) - \\mu\\big)\\,e^{-\\theta t}.\n$$\nIf $\\theta  0$, then $e^{-\\theta t} = e^{|\\theta| t}$ grows exponentially, so unless $m(0) = \\mu$ exactly, the mean diverges as $t\\to\\infty$. Even if $m(0) = \\mu$, fluctuations grow. For the second moment, Itô’s formula applied to $X_t^2$ gives\n$$\n\\frac{d}{dt}\\,\\mathrm{Var}(X_t) = -2\\theta\\,\\mathrm{Var}(X_t) + \\sigma^2,\n$$\nwhich solves to\n$$\n\\mathrm{Var}(X_t) = \\mathrm{Var}(X_0)\\,e^{-2\\theta t} + \\frac{\\sigma^2}{2\\theta}\\Big(1 - e^{-2\\theta t}\\Big).\n$$\nFor $\\theta  0$, the factor $e^{-2\\theta t} = e^{2|\\theta| t}$ grows exponentially and the term $\\frac{\\sigma^2}{2\\theta}$ is negative, confirming that $\\mathrm{Var}(X_t)\\to\\infty$ as $t\\to\\infty$. Unbounded growth of moments is incompatible with a stationary law. Thus, no stationary distribution exists when $\\theta  0$.\n\nOption-by-option analysis:\n\n- Option A: This correctly identifies the mechanism. For $\\theta  0$, the drift is anti-mean-reverting, the zero-current stationary Fokker–Planck reduction yields a quadratic exponent that grows at infinity, so the candidate density is nonintegrable. The moment equations also show exponential growth, further ruling out stationarity. Verdict: Correct.\n\n- Option B: This incorrectly asserts that noise alone guarantees ergodicity and stationarity regardless of drift sign. In fact, constant diffusion cannot counteract an unstable linear drift; the stationary density fails to be normalizable for $\\theta  0$. Verdict: Incorrect.\n\n- Option C: This suggests stationarity can be recovered by a special initial condition $X_0 = \\mu$. Even if the mean starts at $\\mu$, the variance grows exponentially for $\\theta  0$, so the distribution does not settle to a stationary law. Stationarity is a property of the dynamics, not the initial condition. Verdict: Incorrect.\n\n- Option D: This claims dependence on the sign of $\\mu$. The existence of a stationary distribution depends on the sign of $\\theta$, not $\\mu$; $\\mu$ merely shifts the center. For $\\theta  0$, no stationary distribution exists for any $\\mu$. Verdict: Incorrect.\n\n- Option E: This asserts a non-Gaussian heavy-tailed invariant law for $\\theta  0$. For linear drift and constant diffusion, any invariant solution must be of the Gaussian form derived above, and when $\\theta  0$ that form is not normalizable. There is no alternative heavy-tailed stationary density generated by this linear SDE. Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "3076448"}, {"introduction": "This final practice bridges the gap between abstract theory and computational application, a vital skill in modern quantitative science. You will write a program to simulate the OU process and generate data from its stationary distribution. The goal is to use statistical methods to test whether the variance observed in your simulated data is consistent with the theoretical formula, $\\frac{\\sigma^2}{2\\theta}$, providing a powerful, hands-on confirmation of your analytical results [@problem_id:3076386].", "problem": "Consider the Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE) $$dX_t=-\\theta\\,(X_t-\\mu)\\,dt+\\sigma\\,dW_t,$$ where $W_t$ is a standard Brownian motion, and $\\theta0$, $\\mu\\in\\mathbb{R}$, and $\\sigma0$ are parameters. Assume that for sufficiently long time the process admits a stationary distribution. Your task is to design a program that, for each specified parameter set, performs the following steps grounded in first principles of stochastic differential equations and statistical inference for normal distributions.\n\n- Simulate the OU process using the exact discrete-time update implied by the linear SDE solution to obtain a long-run sequence of samples. Use a fixed pseudo-random seed $42$ for reproducibility.\n- Discard an initial burn-in segment to allow the process to reach stationarity. Then collect $N$ consecutive samples at a constant time step $\\Delta$.\n- Fit a Gaussian distribution to the collected samples by estimating its mean and variance from data. Use the maximum-likelihood estimate for the mean, and use the unbiased estimator for the variance (with divisor $N-1$).\n- Compute the theoretical stationary variance of the OU process from first principles and denote it by $v_{\\mathrm{th}}$.\n- Perform a two-sided check of consistency at significance level $\\alpha$ by using the well-tested fact that for samples from a normal distribution, the statistic $\\frac{(N-1)\\,s^2}{v}$ follows a chi-square distribution with $N-1$ degrees of freedom, where $s^2$ is the unbiased sample variance and $v$ is the true variance. Construct the confidence interval for the true variance using the chi-square quantiles and test whether $v_{\\mathrm{th}}$ lies within this interval. Return a boolean indicating whether the theoretical stationary variance is within sampling error of the fitted variance.\n\nUse the following test suite. Each test case is a tuple $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)$ where $\\theta$ is the mean-reversion rate, $\\mu$ is the long-run mean, $\\sigma$ is the diffusion scale, $\\Delta$ is the sampling time step, $B$ is the burn-in time, $N$ is the number of collected samples, and $x_0$ is the initial condition.\n- Test case $1$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(1.0,0.0,2.0,0.05,50.0,60000,5.0)$\n- Test case $2$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(0.1,3.0,1.5,0.1,200.0,100000,-10.0)$\n- Test case $3$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(5.0,-1.0,0.5,0.01,10.0,50000,0.0)$\n- Test case $4$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(2.0,0.0,0.01,0.1,20.0,120000,100.0)$\n\nSet the significance level to $\\alpha=0.05$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is a boolean indicating whether $v_{\\mathrm{th}}$ lies within the chi-square confidence interval for the variance derived from the fitted Gaussian for test case $i$.", "solution": "### Problem Validation\n\nThe problem statement is evaluated against the specified criteria.\n\n#### Step 1: Extract Givens\n\n-   **Stochastic Differential Equation (SDE)**: The Ornstein–Uhlenbeck (OU) process is defined by $dX_t=-\\theta\\,(X_t-\\mu)\\,dt+\\sigma\\,dW_t$.\n-   **Parameters**: $W_t$ is a standard Brownian motion. $\\theta0$, $\\mu\\in\\mathbb{R}$, and $\\sigma0$.\n-   **Assumption**: For a sufficiently long time, the process admits a stationary distribution.\n-   **Simulation Method**: Use the exact discrete-time update implied by the linear SDE solution. A fixed pseudo-random seed of $42$ must be used for reproducibility.\n-   **Sampling Procedure**: Discard an initial burn-in segment of time $B$. Collect $N$ consecutive samples at a constant time step $\\Delta$.\n-   **Statistical Fitting**: Fit a Gaussian distribution to the collected samples. The mean is the maximum-likelihood estimate. The variance is the unbiased estimator (with divisor $N-1$).\n-   **Theoretical Quantity**: The theoretical stationary variance is denoted by $v_{\\mathrm{th}}$.\n-   **Consistency Check**: Perform a two-sided test at significance level $\\alpha=0.05$. The test is based on the statistic $\\frac{(N-1)\\,s^2}{v}$, which follows a chi-square distribution with $N-1$ degrees of freedom, where $s^2$ is the unbiased sample variance and $v$ is the true variance. Construct a confidence interval for the true variance and test if $v_{\\mathrm{th}}$ lies within it.\n-   **Initial Condition**: The process starts at $x_0$.\n-   **Test Cases**: Each case is a tuple $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)$.\n    -   Test case $1$: $(1.0,0.0,2.0,0.05,50.0,60000,5.0)$\n    -   Test case $2$: $(0.1,3.0,1.5,0.1,200.0,100000,-10.0)$\n    -   Test case $3$: $(5.0,-1.0,0.5,0.01,10.0,50000,0.0)$\n    -   Test case $4$: $(2.0,0.0,0.01,0.1,20.0,120000,100.0)$\n-   **Output**: A boolean indicating whether the theoretical stationary variance is within sampling error of the fitted variance for each test case.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is based on the Ornstein-Uhlenbeck process, a cornerstone model in stochastic calculus with wide applications in science and engineering. The methods prescribed—exact simulation, parameter estimation via maximum likelihood and unbiased variance, and hypothesis testing using the chi-square distribution—are all standard, well-established, and rigorously defined within their respective fields of mathematics and statistics. The problem is free of pseudoscience or scientifically unsound premises.\n-   **Well-Posed**: The problem is fully specified. It provides all necessary parameters $(\\theta, \\mu, \\sigma)$, simulation controls $(\\Delta, B, N, x_0)$, statistical parameters ($\\alpha=0.05$, seed=$42$), and a clear, unambiguous objective. The procedure described leads to a unique, deterministic (given the seed) boolean outcome for each test case.\n-   **Objective**: The language is formal, precise, and devoid of any subjectivity, ambiguity, or opinion. All terms are standard in the field.\n\nThe problem does not exhibit any flaws. It is scientifically sound, fully specified, objective, and computationally feasible. The task is a standard exercise in computational statistics and stochastic process simulation.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Solution\n\nThe solution is designed by following the principles and steps outlined in the problem statement.\n\n#### 1. Theoretical Foundation of the Ornstein-Uhlenbeck Process\n\nThe Ornstein-Uhlenbeck (OU) process is described by the linear stochastic differential equation:\n$$dX_t = -\\theta(X_t - \\mu)dt + \\sigma dW_t$$\nwhere $\\theta  0$ is the rate of mean reversion, $\\mu$ is the long-term mean, and $\\sigma  0$ is the scale of the volatility. $W_t$ is a standard Wiener process (Brownian motion).\n\nThis SDE is solvable. Given an initial condition $X_s$ at time $s$, the value of the process at a later time $t  s$ is:\n$$X_t = \\mu + (X_s - \\mu)e^{-\\theta(t-s)} + \\sigma \\int_s^t e^{-\\theta(t-u)}dW_u$$\nThe integral term $\\int_s^t e^{-\\theta(t-u)}dW_u$ is an Itô integral, which represents a normally distributed random variable with mean $0$. By the Itô isometry, its variance is:\n$$\\mathbb{E}\\left[\\left(\\int_s^t e^{-\\theta(t-u)}dW_u\\right)^2\\right] = \\int_s^t e^{-2\\theta(t-u)}du = \\frac{1 - e^{-2\\theta(t-s)}}{2\\theta}$$\nTherefore, for a discrete time step $\\Delta = t-s$, the conditional distribution of $X_t$ given $X_s$ is Gaussian:\n$$X_t | X_s \\sim \\mathcal{N}\\left(\\mu + (X_s - \\mu)e^{-\\theta\\Delta}, \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta\\Delta})\\right)$$\nThis provides an exact update rule for simulating the process:\n$$X_{n+1} = \\mu + (X_n - \\mu)e^{-\\theta\\Delta} + \\sqrt{\\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta\\Delta})} Z_{n+1}$$\nwhere $Z_{n+1}$ are independent and identically distributed standard normal random variables, $Z_{n+1} \\sim \\mathcal{N}(0, 1)$. This formula will be used for the simulation.\n\nAs time $t \\to \\infty$, the initial condition's influence decays ($e^{-\\theta(t-s)} \\to 0$), and the process converges to a stationary distribution. The mean of the stationary distribution is $\\mu$, and the variance converges to:\n$$ v_{\\mathrm{th}} = \\lim_{t-s \\to \\infty} \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta(t-s)}) = \\frac{\\sigma^2}{2\\theta} $$\nThus, the stationary distribution of the OU process is a normal distribution $\\mathcal{N}(\\mu, v_{\\mathrm{th}})$. This theoretical variance $v_{\\mathrm{th}}$ will be the subject of our statistical test.\n\n#### 2. Statistical Validation Methodology\n\nThe program will execute the following sequence for each test case:\n\n1.  **Simulation**: We first simulate a trajectory of the OU process. To ensure the samples are drawn from the stationary distribution, we discard an initial segment of the simulation. The burn-in time is given as $B$. The number of simulation steps to discard is $N_B = \\lceil B/\\Delta \\rceil$. After this burn-in period, we simulate an additional $N$ steps, collecting the values $\\{X_1, X_2, \\dots, X_N\\}$. A fixed pseudo-random number generator seed of $42$ is used to ensure the sequence of generated random numbers is identical for each run, making the simulation reproducible.\n\n2.  **Variance Estimation**: From the collected $N$ samples, we estimate the variance of the stationary distribution. The problem specifies using the unbiased sample variance, $s^2$, defined as:\n    $$s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N}(X_i - \\bar{X})^2$$\n    where $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^{N}X_i$ is the sample mean.\n\n3.  **Consistency Check via Confidence Interval**: The core of the validation is to check if our estimated variance $s^2$ is statistically consistent with the theoretical variance $v_{\\mathrm{th}} = \\frac{\\sigma^2}{2\\theta}$. For $N$ samples drawn from a normal distribution with true variance $v$, the sampling distribution of the statistic $\\frac{(N-1)s^2}{v}$ is a chi-square distribution with $df = N-1$ degrees of freedom, i.e., $\\frac{(N-1)s^2}{v} \\sim \\chi^2_{N-1}$.\n\n    We can use this relationship to construct a $(1-\\alpha)$ confidence interval for the true variance $v$. With a significance level of $\\alpha=0.05$, we find the lower and upper critical values of the $\\chi^2_{N-1}$ distribution that cut off an area of $\\alpha/2 = 0.025$ in each tail. Let these be $c_{\\text{lower}} = \\chi^2_{0.025, N-1}$ and $c_{\\text{upper}} = \\chi^2_{0.975, N-1}$.\n    The confidence interval for $v$ is derived from the probability statement:\n    $$P\\left(c_{\\text{lower}} \\le \\frac{(N-1)s^2}{v} \\le c_{\\text{upper}}\\right) = 1-\\alpha$$\n    Inverting the inequalities for $v$ yields the confidence interval $[CI_{\\text{low}}, CI_{\\text{high}}]$:\n    $$CI_{\\text{low}} = \\frac{(N-1)s^2}{c_{\\text{upper}}}, \\quad CI_{\\text{high}} = \\frac{(N-1)s^2}{c_{\\text{lower}}}$$\n    The test is passed (returning `True`) if the theoretical variance $v_{\\mathrm{th}}$ falls within this interval, i.e., $CI_{\\text{low}} \\le v_{\\mathrm{th}} \\le CI_{\\text{high}}$. Otherwise, the test fails (returning `False`).\n\n#### 3. Algorithmic Implementation\n\nThe Python script implements this logic. A main function `solve()` iterates through the provided test cases. For each case, a helper function will:\n1.  Initialize a random number generator with the specified seed, $42$. This is done for each case to ensure independent and reproducible tests.\n2.  Unpack the parameters $(\\theta, \\mu, \\sigma, \\Delta, B, N, x_0)$.\n3.  Calculate the number of burn-in steps $N_B = \\text{int(np.ceil}(B/\\Delta)\\text{)}$.\n4.  Generate $N_B + N$ standard normal random variates required for the simulation.\n5.  Run the simulation using the exact update rule, first for $N_B$ steps (discarding results) and then for $N$ steps (storing results).\n6.  Compute the unbiased sample variance $s^2$ from the stored samples using `numpy.var(ddof=1)`.\n7.  Compute the theoretical variance $v_{\\mathrm{th}} = \\sigma^2 / (2\\theta)$.\n8.  Find the chi-square critical values $c_{\\text{lower}}$ and $c_{\\text{upper}}$ using `scipy.stats.chi2.ppf`.\n9.  Calculate the confidence interval bounds $CI_{\\text{low}}$ and $CI_{\\text{high}}$.\n10. Return the boolean result of the check $v_{\\mathrm{th}} \\in [CI_{\\text{low}}, CI_{\\text{high}}]$.\n\nThe final results are collected into a list and printed in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the Ornstein-Uhlenbeck process simulation and validation\n    for a suite of test cases.\n    \"\"\"\n    \n    # Each test case is a tuple:\n    # (theta, mu, sigma, delta_t, B, N, x0)\n    # theta: mean-reversion rate\n    # mu: long-run mean\n    # sigma: diffusion scale\n    # delta_t: sampling time step\n    # B: burn-in time\n    # N: number of collected samples\n    # x0: initial condition\n    test_cases = [\n        (1.0, 0.0, 2.0, 0.05, 50.0, 60000, 5.0),\n        (0.1, 3.0, 1.5, 0.1, 200.0, 100000, -10.0),\n        (5.0, -1.0, 0.5, 0.01, 10.0, 50000, 0.0),\n        (2.0, 0.0, 0.01, 0.1, 20.0, 120000, 100.0)\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for case in test_cases:\n        result = run_ou_validation(case, alpha)\n        results.append(result)\n\n    # Format the output as a comma-separated list of booleans in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_ou_validation(case_params, alpha):\n    \"\"\"\n    Performs the OU process simulation and statistical validation for a single case.\n    \n    Args:\n        case_params (tuple): A tuple containing the parameters for the simulation.\n        alpha (float): The significance level for the consistency check.\n        \n    Returns:\n        bool: True if the theoretical variance is within the confidence interval\n              of the sample variance, False otherwise.\n    \"\"\"\n    theta, mu, sigma, delta_t, B, N, x0 = case_params\n\n    # Set a fixed seed for reproducibility for each test case.\n    rng = np.random.default_rng(42)\n\n    # 1. Simulate the OU process using the exact discrete-time update.\n    # The number of burn-in steps islceil(B / delta_t).\n    n_burn = int(math.ceil(B / delta_t))\n    total_steps = n_burn + N\n    \n    # Pre-calculate constants for the update rule for efficiency.\n    # X_{n+1} = mu + (X_n - mu)*exp(-theta*dt) + noise\n    # where noise is a Gaussian with mean 0 and variance V.\n    exp_term = math.exp(-theta * delta_t)\n    var_term = (sigma**2 / (2 * theta)) * (1 - math.exp(-2 * theta * delta_t))\n    std_dev_term = math.sqrt(var_term)\n\n    # Generate all required random numbers at once.\n    z_variates = rng.normal(size=total_steps)\n    \n    samples = np.zeros(N)\n    x_current = float(x0)\n\n    # Burn-in period\n    for i in range(n_burn):\n        x_current = mu + (x_current - mu) * exp_term + std_dev_term * z_variates[i]\n\n    # Sampling period\n    for i in range(N):\n        x_current = mu + (x_current - mu) * exp_term + std_dev_term * z_variates[n_burn + i]\n        samples[i] = x_current\n\n    # 2. Fit a Gaussian: compute the unbiased sample variance (s^2).\n    # ddof=1 provides the unbiased estimator for the variance.\n    sample_variance = np.var(samples, ddof=1)\n\n    # 3. Compute the theoretical stationary variance (v_th).\n    theoretical_variance = sigma**2 / (2 * theta)\n    \n    # 4. Perform the consistency check.\n    # The statistic (N-1)*s^2 / v follows a chi-square distribution with N-1 degrees of freedom.\n    # We construct a (1-alpha) confidence interval for the true variance v.\n    df = N - 1\n    \n    # Find the critical values of the chi-square distribution.\n    # ppf is the percent point function (inverse of cdf).\n    chi2_lower_critical = chi2.ppf(alpha / 2, df)\n    chi2_upper_critical = chi2.ppf(1 - alpha / 2, df)\n    \n    # Construct the confidence interval for the true variance.\n    # [ (df * s^2) / chi2_upper, (df * s^2) / chi2_lower ]\n    ci_lower_bound = (df * sample_variance) / chi2_upper_critical\n    ci_upper_bound = (df * sample_variance) / chi2_lower_critical\n    \n    # Check if the theoretical variance lies within the confidence interval.\n    is_consistent = (theoretical_variance = ci_lower_bound) and \\\n                    (theoretical_variance = ci_upper_bound)\n    \n    return is_consistent\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3076386"}]}