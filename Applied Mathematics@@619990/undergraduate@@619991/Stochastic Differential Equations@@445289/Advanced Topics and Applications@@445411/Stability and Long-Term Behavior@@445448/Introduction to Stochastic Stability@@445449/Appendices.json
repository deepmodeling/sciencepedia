{"hands_on_practices": [{"introduction": "In deterministic systems, stability is often determined by the sign of a coefficient, but introducing randomness can dramatically alter long-term behavior. This first practice explores this fundamental principle by analyzing a simple linear stochastic differential equation [@problem_id:3060649]. By applying Itô's calculus, you will derive the almost-sure Lyapunov exponent, which governs the exponential growth or decay of trajectories, and discover the surprising condition under which noise can stabilize an otherwise unstable system.", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = a X_t\\,dt + b X_t\\,dW_t$ for $t \\geq 0$, where $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are constants, $X_0 \\neq 0$ is a deterministic initial condition, and $W_t$ is a standard one-dimensional Wiener process (Brownian motion). Define the process $Y_t = \\ln|X_t|$. Starting from the Itô formula for twice continuously differentiable functions applied to semimartingales, derive the SDE satisfied by $Y_t$, solve it to obtain $Y_t$ explicitly, and compute $\\mathbb{E}[Y_t]$ as a function of $t$. Then, using the almost sure scaling behavior of Brownian motion, deduce the almost sure limit of $\\frac{1}{t}\\ln|X_t|$ as $t \\to \\infty$, and state the condition under which the origin is exponentially stable almost surely in the sense that $\\limsup_{t \\to \\infty} \\frac{1}{t} \\ln|X_t|  0$. Provide, as your final answer, the explicit analytic expression for the almost sure Lyapunov exponent, defined by $\\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t|$.", "solution": "The problem as stated is a standard exercise in stochastic calculus and stability theory for stochastic differential equations. It is scientifically grounded, well-posed, objective, and internally consistent. All necessary data and definitions are provided, and the task is to derive a known result through a series of logical steps. The problem is therefore valid.\n\nWe are given the scalar stochastic differential equation (SDE) for a process $X_t$:\n$$\ndX_t = a X_t\\,dt + b X_t\\,dW_t\n$$\nwith initial condition $X_0 \\neq 0$ being a deterministic constant. $W_t$ is a standard one-dimensional Wiener process. Our objective is to find the almost sure Lyapunov exponent, defined as $\\lambda = \\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t|$.\n\nThe problem requires us to first analyze the process $Y_t = \\ln|X_t|$. We will use Itô's formula for a function $f(X_t)$ where $X_t$ is an Itô process. Let $f(x) = \\ln|x|$. The Itô formula states:\n$$\ndf(X_t) = f'(X_t) dX_t + \\frac{1}{2} f''(X_t) (dX_t)^2\n$$\nThe function $f(x) = \\ln|x|$ is twice continuously differentiable for all $x \\neq 0$. Its derivatives are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. To apply Itô's formula, we must ensure that the process $X_t$ never takes the value $0$, where $f(x)$ is singular. The given SDE is a geometric Brownian motion, whose explicit solution is known to be:\n$$\nX_t = X_0 \\exp\\left( \\left(a - \\frac{1}{2}b^2\\right)t + b W_t \\right)\n$$\nSince the initial condition $X_0$ is non-zero and the exponential function is always strictly positive, $X_t$ will never be zero for any finite time $t \\geq 0$. Thus, $X_t$ always has the same sign as $X_0$, and the application of Itô's formula to $f(x) = \\ln|x|$ is justified for all $t \\geq 0$.\n\nNow, we compute the terms for Itô's formula. The stochastic differential $dX_t$ is given. The quadratic variation term $(dX_t)^2$ is calculated using the Itô multiplication rules: $(dt)^2 = 0$, $dt\\,dW_t = 0$, and $(dW_t)^2 = dt$.\n$$\n(dX_t)^2 = (a X_t\\,dt + b X_t\\,dW_t)^2 = (a X_t)^2 (dt)^2 + 2 a b X_t^2 dt\\,dW_t + (b X_t)^2 (dW_t)^2 = b^2 X_t^2 dt\n$$\nSubstituting the derivatives of $f(x)$, the differential $dX_t$, and the quadratic variation $(dX_t)^2$ into Itô's formula for $Y_t = f(X_t)$:\n$$\ndY_t = d(\\ln|X_t|) = \\frac{1}{X_t} (a X_t\\,dt + b X_t\\,dW_t) + \\frac{1}{2} \\left(-\\frac{1}{X_t^2}\\right) (b^2 X_t^2 dt)\n$$\nSimplifying the expression:\n$$\ndY_t = (a\\,dt + b\\,dW_t) - \\frac{1}{2} b^2 dt\n$$\nThis gives the SDE satisfied by $Y_t$:\n$$\ndY_t = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_t\n$$\nThis is the first part of the task. Next, we solve this SDE for $Y_t$. We can directly integrate both sides from $0$ to $t$:\n$$\n\\int_0^t dY_s = \\int_0^t \\left(a - \\frac{1}{2}b^2\\right)ds + \\int_0^t b\\,dW_s\n$$\n$$\nY_t - Y_0 = \\left(a - \\frac{1}{2}b^2\\right)t + b (W_t - W_0)\n$$\nGiven that $W_t$ is a standard Wiener process, $W_0=0$. The initial condition is $Y_0 = \\ln|X_0|$. Therefore, the explicit solution for $Y_t$ is:\n$$\nY_t = \\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b W_t\n$$\nThe third task is to compute the expectation $\\mathbb{E}[Y_t]$. Using the linearity of expectation on the expression for $Y_t$:\n$$\n\\mathbb{E}[Y_t] = \\mathbb{E}\\left[\\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b W_t\\right]\n$$\nSince $X_0$, $a$, and $b$ are deterministic constants, this becomes:\n$$\n\\mathbb{E}[Y_t] = \\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b \\mathbb{E}[W_t]\n$$\nA fundamental property of the standard Wiener process is that its expectation is zero for all time, $\\mathbb{E}[W_t] = 0$. Hence,\n$$\n\\mathbb{E}[Y_t] = \\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t\n$$\nNow, we deduce the almost sure limit of $\\frac{1}{t}\\ln|X_t|$ as $t \\to \\infty$. This is equivalent to finding the limit of $\\frac{1}{t}Y_t$.\n$$\n\\frac{1}{t}\\ln|X_t| = \\frac{Y_t}{t} = \\frac{1}{t}\\left(\\ln|X_0| + \\left(a - \\frac{1}{2}b^2\\right)t + b W_t\\right) = \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2}b^2 + \\frac{b W_t}{t}\n$$\nTo find the limit as $t \\to \\infty$, we analyze each term:\nThe first term, $\\frac{\\ln|X_0|}{t}$, tends to $0$ as $t \\to \\infty$ because $\\ln|X_0|$ is a constant.\nThe second term, $a - \\frac{1}{2}b^2$, is a constant.\nFor the third term, we use the almost sure scaling behavior of Brownian motion, specifically the Strong Law of Large Numbers for the Wiener process, which states:\n$$\n\\lim_{t \\to \\infty} \\frac{W_t}{t} = 0 \\quad \\text{almost surely}\n$$\nTherefore, the limit of the entire expression is:\n$$\n\\lim_{t \\to \\infty} \\frac{1}{t}\\ln|X_t| = 0 + \\left(a - \\frac{1}{2}b^2\\right) + b \\cdot 0 = a - \\frac{1}{2}b^2 \\quad \\text{almost surely}\n$$\nThis limit is the almost sure Lyapunov exponent.\n\nThe condition for the origin to be exponentially stable almost surely is that the Lyapunov exponent is negative: $\\limsup_{t \\to \\infty} \\frac{1}{t} \\ln|X_t|  0$. Since the limit exists, this is equivalent to:\n$$\na - \\frac{1}{2}b^2  0\n$$\nThe problem asks for the explicit analytic expression for the almost sure Lyapunov exponent, which we have found.", "answer": "$$\n\\boxed{a - \\frac{1}{2}b^{2}}\n$$", "id": "3060649"}, {"introduction": "Understanding the long-term behavior of a single system path does not always tell the whole story about its stability. This exercise [@problem_id:3060618] presents a classic example highlighting the critical difference between pathwise stability and mean-square stability. You will construct a scenario where a system is stable for almost every trajectory, yet its average energy (second moment) grows exponentially, demonstrating how rare, extreme events can dominate ensemble averages and lead to seemingly paradoxical conclusions.", "problem": "Consider the scalar Itô Stochastic Differential Equation (SDE)\n$$\ndX_{t}=-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\qquad X_{0}=1,\n$$\nwhere $a0$ and $b0$ are constants, and $W_{t}$ is a standard Wiener process. The equilibrium $X=0$ is said to be asymptotically stable in probability if $X_{t}\\to 0$ in probability as $t\\to\\infty$ for initial conditions sufficiently close to $0$, and it is said to be mean-square stable if $\\lim_{t\\to\\infty}\\mathbb{E}[\\,|X_{t}|^{2}\\,]=0$.\n\nUsing only fundamental definitions and properties of Itô calculus (in particular, Itô’s formula and basic moment computations for Gaussian random variables), do the following for the specific choice $a=\\frac{1}{4}$ and $b=1$:\n\n1. Derive the exact representation of $X_{t}$ and compute the almost-sure sample-path growth exponent\n$$\nL_{1}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|.\n$$\n2. Compute the mean-square growth exponent\n$$\nL_{2}=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\big(\\mathbb{E}[X_{t}^{2}]\\big).\n$$\n3. Using the signs of $L_{1}$ and $L_{2}$, explain why in this example the equilibrium $X=0$ is asymptotically stable in probability but not mean-square stable, and articulate the probabilistic mechanism that differentiates these two notions of stability for this SDE.\n\nReport your final values for $L_{1}$ and $L_{2}$ as a single row matrix. No rounding is needed.", "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for its resolution. It represents a standard, illustrative example in the theory of stochastic stability. We proceed with the solution.\n\nThe given Itô Stochastic Differential Equation (SDE) is:\n$$\ndX_{t}=-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\qquad X_{0}=1\n$$\nwith constants $a=\\frac{1}{4} > 0$ and $b=1 > 0$. This is a linear SDE known as the equation for geometric Brownian motion.\n\n### 1. Derivation of $X_{t}$ and computation of $L_{1}$\n\nTo find the explicit solution for $X_{t}$, we apply Itô's formula to the function $f(X_{t}) = \\ln(X_{t})$. The derivatives of $f(x) = \\ln(x)$ are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$.\nAccording to Itô's formula, the differential of $Y_{t} = f(X_{t})$ is given by:\n$$\ndY_{t} = f'(X_{t})\\,dX_{t} + \\frac{1}{2}f''(X_{t})\\,(dX_{t})^2\n$$\nThe quadratic variation term $(dX_{t})^2$ is calculated using the Itô rules: $(dt)^2=0$, $dt\\,dW_{t}=0$, and $(dW_{t})^2=dt$.\n$$\n(dX_{t})^2 = (-a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t})^2 = (b\\,X_{t})^2(dW_{t})^2 = b^2\\,X_{t}^2\\,dt\n$$\nSubstituting $dX_{t}$ and $(dX_{t})^2$ into Itô's formula for $Y_{t} = \\ln(X_{t})$:\n$$\nd(\\ln X_{t}) = \\frac{1}{X_{t}}(-a\\,X_{t}\\,dt + b\\,X_{t}\\,dW_{t}) + \\frac{1}{2}\\left(-\\frac{1}{X_{t}^2}\\right)(b^2\\,X_{t}^2\\,dt)\n$$\n$$\nd(\\ln X_{t}) = -a\\,dt + b\\,dW_{t} - \\frac{1}{2}b^2\\,dt\n$$\n$$\nd(\\ln X_{t}) = -\\left(a + \\frac{1}{2}b^2\\right)dt + b\\,dW_{t}\n$$\nIntegrating this from $s=0$ to $s=t$:\n$$\n\\int_{0}^{t} d(\\ln X_{s}) = \\int_{0}^{t} -\\left(a + \\frac{1}{2}b^2\\right)ds + \\int_{0}^{t} b\\,dW_{s}\n$$\n$$\n\\ln(X_{t}) - \\ln(X_{0}) = -\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\n$$\nGiven the initial condition $X_{0}=1$, we have $\\ln(X_{0})=\\ln(1)=0$. Thus:\n$$\n\\ln(X_{t}) = -\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\n$$\nExponentiating both sides gives the exact representation of $X_{t}$:\n$$\nX_{t} = \\exp\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\n$$\nSince $X_{0}=1 > 0$ and the exponential function is always positive, $X_{t}>0$ for all $t$, which means $|X_{t}| = X_{t}$.\n\nNow, we compute the almost-sure sample-path growth exponent, $L_{1}$:\n$$\nL_{1} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}| = \\lim_{t\\to\\infty}\\frac{1}{t}\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\n$$\n$$\nL_{1} = \\lim_{t\\to\\infty}\\left(-\\left(a + \\frac{1}{2}b^2\\right) + b\\,\\frac{W_{t}}{t}\\right)\n$$\nBy the Strong Law of Large Numbers for the Wiener process, we know that $\\lim_{t\\to\\infty} \\frac{W_{t}}{t} = 0$ almost surely. Therefore, the limit becomes:\n$$\nL_{1} = -\\left(a + \\frac{1}{2}b^2\\right)\n$$\nSubstituting the given values $a=\\frac{1}{4}$ and $b=1$:\n$$\nL_{1} = -\\left(\\frac{1}{4} + \\frac{1}{2}(1)^2\\right) = -\\left(\\frac{1}{4} + \\frac{1}{2}\\right) = -\\frac{3}{4}\n$$\n\n### 2. Computation of the mean-square growth exponent $L_{2}$\n\nFirst, we must compute the second moment $\\mathbb{E}[X_{t}^2]$. Using the solution for $X_{t}$:\n$$\nX_{t}^2 = \\left[\\exp\\left(-\\left(a + \\frac{1}{2}b^2\\right)t + b\\,W_{t}\\right)\\right]^2 = \\exp\\left(-2\\left(a + \\frac{1}{2}b^2\\right)t + 2b\\,W_{t}\\right)\n$$\n$$\nX_{t}^2 = \\exp\\left((-2a - b^2)t + 2b\\,W_{t}\\right)\n$$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{t}^2] = \\mathbb{E}\\left[\\exp\\left((-2a - b^2)t + 2b\\,W_{t}\\right)\\right] = \\exp\\left((-2a - b^2)t\\right) \\mathbb{E}\\left[\\exp(2b\\,W_{t})\\right]\n$$\nThe term $\\mathbb{E}[\\exp(k\\,Z)]$ is the moment-generating function (MGF) of a random variable $Z$. For a standard Wiener process, $W_{t}$ is a Gaussian random variable with mean $0$ and variance $t$, i.e., $W_{t} \\sim \\mathcal{N}(0, t)$. The MGF of a general normal random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(kZ)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$.\nHere, the random variable is $W_t$, with $\\mu=0, \\sigma^2=t$, and the constant is $k=2b$.\n$$\n\\mathbb{E}[\\exp(2b\\,W_{t})] = \\exp\\left((2b)(0) + \\frac{1}{2}(2b)^2t\\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2t)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[X_{t}^2]$:\n$$\n\\mathbb{E}[X_{t}^2] = \\exp\\left((-2a - b^2)t\\right)\\exp(2b^2t) = \\exp\\left((-2a - b^2 + 2b^2)t\\right) = \\exp\\left((b^2 - 2a)t\\right)\n$$\nNow, we compute the mean-square growth exponent, $L_{2}$:\n$$\nL_{2} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\mathbb{E}[X_{t}^{2}]\\right) = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(\\exp\\left((b^2 - 2a)t\\right)\\right)\n$$\n$$\nL_{2} = \\lim_{t\\to\\infty}\\frac{1}{t}(b^2 - 2a)t = b^2 - 2a\n$$\nSubstituting the given values $a=\\frac{1}{4}$ and $b=1$:\n$$\nL_{2} = (1)^2 - 2\\left(\\frac{1}{4}\\right) = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\n\n### 3. Stability Analysis and Probabilistic Mechanism\n\nWe have found that $L_{1} = -\\frac{3}{4}$ and $L_{2} = \\frac{1}{2}$.\n\nThe sign of $L_{1}$ determines the almost-sure stability of the equilibrium $X=0$. Since $L_{1}  0$, we have $\\lim_{t\\to\\infty} \\ln|X_t| = -\\infty$ almost surely. This implies that $\\lim_{t\\to\\infty} X_t = 0$ almost surely. Almost-sure convergence implies convergence in probability. Therefore, since $L_{1}  0$, the equilibrium $X=0$ is asymptotically stable in probability. The negative drift term in the exponent of $X_t$ dominates the diffusion term for a typical sample path, causing trajectories to decay to zero.\n\nThe sign of $L_{2}$ determines the mean-square stability of the equilibrium $X=0$. Since $L_{2} > 0$, we have $\\mathbb{E}[X_{t}^2] = \\exp(L_{2}t) = \\exp\\left(\\frac{1}{2}t\\right)$, which diverges to infinity as $t\\to\\infty$. Thus, the condition for mean-square stability, $\\lim_{t\\to\\infty}\\mathbb{E}[|X_{t}|^2]=0$, is not met. The equilibrium is mean-square unstable.\n\nThe probabilistic mechanism that differentiates these two stability notions lies in the nature of averaging over stochastic paths.\n- **Asymptotic stability in probability** (and its stronger form, almost-sure stability) concerns the behavior of a *typical* or *almost every* sample path. For this SDE, the deterministic drift component pulls most trajectories towards the equilibrium $X=0$. The exponent $L_{1} = -(a+b^2/2)$ represents the growth rate of the typical path. A negative value ensures this decay.\n- **Mean-square stability** concerns the behavior of the second moment, $\\mathbb{E}[X_{t}^2]$, which is an average over the ensemble of all possible sample paths. The value of this average is heavily influenced by rare, extreme events. The distribution of $X_t$ is log-normal, which has a heavy right tail. Although almost all paths converge to $0$, there is a small set of paths for which the Wiener process $W_t$ experiences large positive excursions. For these rare paths, the term $bW_t$ in the exponent of $X_t$ becomes large and positive, overpowering the negative drift term $- (a + b^2/2)t$. This leads to extraordinarily large values of $X_t$ for a small fraction of trajectories. When computing the second moment $\\mathbb{E}[X_t^2]$, the contribution of these rare but enormous values completely dominates the contributions from the vast majority of decaying paths. The result is an exponential growth of the second moment, as indicated by $L_2 > 0$.\n\nIn conclusion, the system is stable for almost every realization (almost-sure stability) but unstable in the mean-square sense because the expectation is skewed by infrequent but very large fluctuations.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{3}{4}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3060618"}, {"introduction": "Real-world systems are rarely one-dimensional, and analyzing their stability requires more powerful tools. This practice [@problem_id:3060607] generalizes the concept of mean-square stability to multidimensional linear systems, a crucial step toward practical applications in engineering and finance. You will learn to use the machinery of Kronecker products and vectorization to transform the complex matrix differential equation for the second moment into a standard linear system, whose stability can be determined by analyzing its eigenvalues.", "problem": "Consider the linear time-invariant stochastic differential equation (SDE) with multiplicative noise in $\\mathbb{R}^{n}$,\n$$\ndX_{t} \\;=\\; A\\,X_{t}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$, $B_{i} \\in \\mathbb{R}^{n \\times n}$ for $i=1,\\dots,m$, and $\\{W_{t}^{(i)}\\}_{i=1}^{m}$ are independent standard Wiener processes. Let $S_{t} := \\mathbb{E}[X_{t} X_{t}^{\\top}]$ denote the second-moment matrix. Using only foundational tools—namely the Itô product rule for matrix-valued processes, linearity of expectation, independence of the Wiener processes with $dW_{t}^{(i)}\\, dW_{t}^{(j)} = \\delta_{ij}\\,dt$, and standard identities for the vectorization operator $\\mathrm{vec}(\\cdot)$ and the Kronecker product $\\otimes$—derive the autonomous linear ordinary differential equation satisfied by $\\mathrm{vec}(S_{t})$ in the form\n$$\n\\frac{d}{dt}\\,\\mathrm{vec}(S_{t}) \\;=\\; M\\,\\mathrm{vec}(S_{t}),\n$$\nand identify $M \\in \\mathbb{R}^{n^{2} \\times n^{2}}$ explicitly in terms of $A$ and $\\{B_{i}\\}_{i=1}^{m}$. Then, state the mean-square stability criterion in terms of the spectrum of $M$ using the Kronecker sum representation of the drift. Your final answer must be the single closed-form analytic expression for $M$ written using Kronecker products (and may include the Kronecker sum). No numerical evaluation is required, and no rounding is needed. Express your answer without physical units.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It represents a standard and fundamental derivation in the theory of stochastic stability for linear systems. All provided data and conditions are consistent and sufficient for deriving a unique solution. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe objective is to derive the ordinary differential equation (ODE) governing the evolution of $\\mathrm{vec}(S_{t})$, where $S_{t} = \\mathbb{E}[X_{t}X_{t}^{\\top}]$, and to identify the system matrix $M$. The starting point is the given stochastic differential equation (SDE):\n$$\ndX_{t} \\;=\\; A\\,X_{t}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)}\n$$\nHere, $X_{t} \\in \\mathbb{R}^{n}$, $A, B_{i} \\in \\mathbb{R}^{n \\times n}$, and $\\{W_{t}^{(i)}\\}_{i=1}^{m}$ are independent standard Wiener processes.\n\nFirst, we find the differential for the matrix process $X_{t}X_{t}^{\\top}$. We apply the Itô product rule for matrix-valued processes, which for a process $Y_{t} = X_{t}X_{t}^{\\top}$ is given by:\n$$\nd(X_{t}X_{t}^{\\top}) \\;=\\; (dX_{t})X_{t}^{\\top} \\;+\\; X_{t}(dX_{t})^{\\top} \\;+\\; (dX_{t})(dX_{t})^{\\top}\n$$\nWe compute each term on the right-hand side.\n\nThe first term is:\n$$\n(dX_{t})X_{t}^{\\top} \\;=\\; \\left( A\\,X_{t}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)} \\right) X_{t}^{\\top} \\;=\\; A\\,X_{t}X_{t}^{\\top}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}\\,dW_{t}^{(i)}\n$$\n\nThe second term is the transpose of the first, with $X_{t}$ and $X_{t}^\\top$ swapped:\n$$\nX_{t}(dX_{t})^{\\top} \\;=\\; X_{t} \\left( A\\,X_{t}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)} \\right)^{\\top} \\;=\\; X_{t} \\left( X_{t}^{\\top}A^{\\top}\\,dt \\;+\\; \\sum_{i=1}^{m} X_{t}^{\\top}B_{i}^{\\top}\\,dW_{t}^{(i)} \\right)\n$$\n$$\nX_{t}(dX_{t})^{\\top} \\;=\\; X_{t}X_{t}^{\\top}A^{\\top}\\,dt \\;+\\; \\sum_{i=1}^{m} X_{t}X_{t}^{\\top}B_{i}^{\\top}\\,dW_{t}^{(i)}\n$$\nNote that the differential $dW_{t}^{(i)}$ is a scalar and thus is its own transpose.\n\nThe third term is the quadratic variation term. Using the Itô multiplication rules $dt \\cdot dt = 0$, $dt \\cdot dW_{t} = 0$, and $dW_{t}^{(i)}dW_{t}^{(j)} = \\delta_{ij}\\,dt$:\n$$\n(dX_{t})(dX_{t})^{\\top} \\;=\\; \\left( A\\,X_{t}\\,dt \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)} \\right) \\left( A\\,X_{t}\\,dt \\;+\\; \\sum_{j=1}^{m} B_{j}\\,X_{t}\\,dW_{t}^{(j)} \\right)^{\\top}\n$$\n$$\n(dX_{t})(dX_{t})^{\\top} \\;=\\; \\left( \\sum_{i=1}^{m} B_{i}\\,X_{t}\\,dW_{t}^{(i)} \\right) \\left( \\sum_{j=1}^{m} X_{t}^{\\top}B_{j}^{\\top}\\,dW_{t}^{(j)} \\right) \\;=\\; \\sum_{i=1}^{m} \\sum_{j=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}B_{j}^{\\top} \\, (dW_{t}^{(i)}dW_{t}^{(j)})\n$$\n$$\n(dX_{t})(dX_{t})^{\\top} \\;=\\; \\sum_{i=1}^{m} \\sum_{j=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}B_{j}^{\\top} \\, \\delta_{ij}\\,dt \\;=\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}B_{i}^{\\top}\\,dt\n$$\n\nCombining all three terms, we obtain the SDE for $X_{t}X_{t}^{\\top}$:\n$$\nd(X_{t}X_{t}^{\\top}) \\;=\\; \\left( A\\,X_{t}X_{t}^{\\top} \\;+\\; X_{t}X_{t}^{\\top}A^{\\top} \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}B_{i}^{\\top} \\right) dt \\;+\\; \\sum_{i=1}^{m} \\left( B_{i}\\,X_{t}X_{t}^{\\top} \\;+\\; X_{t}X_{t}^{\\top}B_{i}^{\\top} \\right) dW_{t}^{(i)}\n$$\nNow, we take the expectation of this equation. The second-moment matrix is $S_{t} = \\mathbb{E}[X_{t}X_{t}^{\\top}]$. Using the linearity of expectation and noting that the expectation and differentiation operators can be interchanged, we have $d S_{t} = \\mathbb{E}[d(X_{t}X_{t}^{\\top})]$. The expectation of the Itô integral term (the term with $dW_{t}^{(i)}$) is zero because the integrand is a non-anticipating process.\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{m} \\left( B_{i}\\,X_{t}X_{t}^{\\top} \\;+\\; X_{t}X_{t}^{\\top}B_{i}^{\\top} \\right) dW_{t}^{(i)} \\right] \\;=\\; 0\n$$\nApplying the expectation to the drift term yields:\n$$\nd S_{t} \\;=\\; \\mathbb{E}\\left[ \\left( A\\,X_{t}X_{t}^{\\top} \\;+\\; X_{t}X_{t}^{\\top}A^{\\top} \\;+\\; \\sum_{i=1}^{m} B_{i}\\,X_{t}X_{t}^{\\top}B_{i}^{\\top} \\right) \\right] dt\n$$\n$$\nd S_{t} \\;=\\; \\left( A\\,\\mathbb{E}[X_{t}X_{t}^{\\top}] \\;+\\; \\mathbb{E}[X_{t}X_{t}^{\\top}]A^{\\top} \\;+\\; \\sum_{i=1}^{m} B_{i}\\,\\mathbb{E}[X_{t}X_{t}^{\\top}]B_{i}^{\\top} \\right) dt\n$$\nSubstituting $S_{t} = \\mathbb{E}[X_{t}X_{t}^{\\top}]$, we arrive at the linear matrix ODE for the second-moment matrix, which is a continuous-time Lyapunov equation:\n$$\n\\frac{d S_{t}}{dt} \\;=\\; A S_{t} \\;+\\; S_{t} A^{\\top} \\;+\\; \\sum_{i=1}^{m} B_{i} S_{t} B_{i}^{\\top}\n$$\nTo obtain the desired vector form, we apply the vectorization operator, $\\mathrm{vec}(\\cdot)$, to both sides of the equation. Using the property that $\\frac{d}{dt}\\mathrm{vec}(S_{t}) = \\mathrm{vec}(\\frac{dS_{t}}{dt})$ and the linearity of the $\\mathrm{vec}$ operator:\n$$\n\\frac{d}{dt}\\,\\mathrm{vec}(S_{t}) \\;=\\; \\mathrm{vec}( A S_{t} ) \\;+\\; \\mathrm{vec}( S_{t} A^{\\top} ) \\;+\\; \\mathrm{vec}\\left(\\sum_{i=1}^{m} B_{i} S_{t} B_{i}^{\\top}\\right)\n$$\nWe now employ the standard Kronecker product identity $\\mathrm{vec}(PQR) = (R^{\\top} \\otimes P)\\mathrm{vec}(Q)$.\nFor the first term, we set $P=A$, $Q=S_{t}$, and $R=I$ (the $n \\times n$ identity matrix):\n$$\n\\mathrm{vec}(A S_{t}) \\;=\\; \\mathrm{vec}(A S_{t} I) \\;=\\; (I^{\\top} \\otimes A)\\mathrm{vec}(S_{t}) \\;=\\; (I \\otimes A)\\mathrm{vec}(S_{t})\n$$\nFor the second term, we set $P=I$, $Q=S_{t}$, and $R=A^{\\top}$:\n$$\n\\mathrm{vec}(S_{t} A^{\\top}) \\;=\\; \\mathrm{vec}(I S_{t} A^{\\top}) \\;=\\; ((A^{\\top})^{\\top} \\otimes I)\\mathrm{vec}(S_{t}) \\;=\\; (A \\otimes I)\\mathrm{vec}(S_{t})\n$$\nFor each term in the summation, we set $P=B_{i}$, $Q=S_{t}$, and $R=B_{i}^{\\top}$:\n$$\n\\mathrm{vec}(B_{i} S_{t} B_{i}^{\\top}) \\;=\\; ((B_{i}^{\\top})^{\\top} \\otimes B_{i})\\mathrm{vec}(S_{t}) \\;=\\; (B_{i} \\otimes B_{i})\\mathrm{vec}(S_{t})\n$$\nSubstituting these back into the vectorized ODE gives:\n$$\n\\frac{d}{dt}\\,\\mathrm{vec}(S_{t}) \\;=\\; (I \\otimes A)\\mathrm{vec}(S_{t}) \\;+\\; (A \\otimes I)\\mathrm{vec}(S_{t}) \\;+\\; \\sum_{i=1}^{m} (B_{i} \\otimes B_{i})\\mathrm{vec}(S_{t})\n$$\nFactoring out $\\mathrm{vec}(S_{t})$, we obtain the final autonomous linear ODE:\n$$\n\\frac{d}{dt}\\,\\mathrm{vec}(S_{t}) \\;=\\; \\left[ (I \\otimes A) \\;+\\; (A \\otimes I) \\;+\\; \\sum_{i=1}^{m} (B_{i} \\otimes B_{i}) \\right] \\mathrm{vec}(S_{t})\n$$\nComparing this to the specified form $\\frac{d}{dt}\\,\\mathrm{vec}(S_{t}) = M\\,\\mathrm{vec}(S_{t})$, we identify the matrix $M \\in \\mathbb{R}^{n^{2} \\times n^{2}}$ as:\n$$\nM \\;=\\; (I \\otimes A) \\;+\\; (A \\otimes I) \\;+\\; \\sum_{i=1}^{m} (B_{i} \\otimes B_{i})\n$$\nThe term $(I \\otimes A) + (A \\otimes I)$ is the Kronecker sum of $A$ with itself, denoted $A \\oplus A$.\n\nFinally, we state the mean-square stability criterion. The system is mean-square stable if $\\lim_{t \\to \\infty} \\mathbb{E}[\\|X_{t}\\|^{2}] = 0$ for any initial condition $X_{0}$. This is equivalent to $\\lim_{t \\to \\infty} \\mathrm{Tr}(S_{t}) = 0$, which requires that $\\lim_{t \\to \\infty} S_{t} = 0$, and thus $\\lim_{t \\to \\infty} \\mathrm{vec}(S_{t}) = 0$. The solution to the linear system $\\frac{d\\mathbf{y}}{dt} = M\\mathbf{y}$ tends to zero for all initial conditions if and only if the matrix $M$ is a Hurwitz matrix (or stability matrix). This means that all eigenvalues of $M$ must have strictly negative real parts. Therefore, the mean-square stability criterion is that the spectral abscissa of $M$, $\\alpha(M) = \\max_{j} \\mathrm{Re}(\\lambda_{j}(M))$, must be negative, where $\\{\\lambda_{j}(M)\\}$ is the set of eigenvalues of $M$.", "answer": "$$\n\\boxed{M = A \\otimes I + I \\otimes A + \\sum_{i=1}^{m} B_{i} \\otimes B_{i}}\n$$", "id": "3060607"}]}