{"hands_on_practices": [{"introduction": "To appreciate why implicit schemes are essential for stiff problems, we must first diagnose the limitations of their explicit counterparts. This exercise [@problem_id:3059094] guides you through a mean-square stability analysis of the explicit Euler-Maruyama method applied to a canonical stiff test equation. By deriving the amplification factor, you will pinpoint the mathematical reason for the restrictive step-size constraint that plagues explicit methods in stiff regimes.", "problem": "Consider the scalar linear stochastic differential equation (SDE)\n$$\ndX_t = a X_t\\,dt + b X_t\\,dW_t,\n$$\nwhere $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are constants, and $W_t$ is a standard Wiener process (standard Brownian motion). Assume $a0$ so that the deterministic drift is stabilizing, and the equation can be stiff when $|a|$ is large. Recall that in mean-square analysis, the exact solution is stable when $2a + b^2  0$.\n\nStarting from the definition of the explicit Euler–Maruyama (EM) method with uniform time step $h0$,\n$$\nX_{n+1} = X_n + a X_n h + b X_n \\Delta W_n,\n$$\nwhere $\\Delta W_n := W_{t_{n+1}} - W_{t_n}$ and $t_n := n h$, use the properties of Wiener increments ($\\Delta W_n \\sim \\mathcal{N}(0,h)$ and independent of $X_n$) to derive the mean-square amplification factor $G(h)$ defined by\n$$\nG(h) := \\frac{\\mathbb{E}\\big[|X_{n+1}|^2\\big]}{\\mathbb{E}\\big[|X_n|^2\\big]}.\n$$\nThen, under the assumption $2a + b^2  0$ and $a \\neq 0$, identify the maximal admissible step size $h_{\\max}0$ such that the explicit EM method is mean-square stable, i.e., $G(h)  1$ for all $h \\in (0,h_{\\max})$.\n\nProvide your final result as two closed-form analytic expressions: the amplification factor $G(h)$ and the maximal admissible step size $h_{\\max}$. No rounding is required, and no units are involved. Express both answers in terms of $a$, $b$, and $h$.", "solution": "The problem asks for the derivation of the mean-square amplification factor $G(h)$ for the explicit Euler-Maruyama (EM) method applied to a scalar linear SDE, and for the subsequent determination of the maximal time step $h_{\\max}$ that ensures mean-square stability.\n\nThe scalar linear SDE is given by:\n$$\ndX_t = a X_t\\,dt + b X_t\\,dW_t\n$$\nwhere $a, b \\in \\mathbb{R}$ are constants, $a0$, $a\\neq 0$, and $W_t$ is a standard Wiener process. The condition for mean-square stability of the exact solution is $2a + b^2  0$.\n\nThe explicit Euler-Maruyama (EM) method provides a discrete-time approximation $X_n \\approx X_{t_n}$ with a uniform time step $h  0$. The scheme is:\n$$\nX_{n+1} = X_n + a X_n h + b X_n \\Delta W_n\n$$\nwhere $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$. The increments $\\Delta W_n$ are independent and identically distributed normal random variables with mean $\\mathbb{E}[\\Delta W_n] = 0$ and variance $\\mathbb{E}[(\\Delta W_n)^2] = h$. Also, $\\Delta W_n$ is independent of the information available at time $t_n$, represented by the $\\sigma$-algebra $\\mathcal{F}_{t_n}$, which includes $X_n$.\n\nFirst, we derive the mean-square amplification factor $G(h) := \\frac{\\mathbb{E}\\big[|X_{n+1}|^2\\big]}{\\mathbb{E}\\big[|X_n|^2\\big]}$. Since $X_t$ is a real-valued scalar process, $|X_t|^2 = X_t^2$.\nWe start by factoring $X_n$ from the EM scheme:\n$$\nX_{n+1} = X_n (1 + ah + b \\Delta W_n)\n$$\nSquaring both sides gives:\n$$\nX_{n+1}^2 = X_n^2 (1 + ah + b \\Delta W_n)^2\n$$\nNow, we take the expectation of both sides. Using the law of total expectation (tower property), we condition on $\\mathcal{F}_{t_n}$:\n$$\n\\mathbb{E}[X_{n+1}^2] = \\mathbb{E}\\left[\\mathbb{E}\\left[X_n^2 (1 + ah + b \\Delta W_n)^2 \\mid \\mathcal{F}_{t_n}\\right]\\right]\n$$\nSince $X_n$ is $\\mathcal{F}_{t_n}$-measurable, we can take it out of the inner conditional expectation:\n$$\n\\mathbb{E}[X_{n+1}^2] = \\mathbb{E}\\left[X_n^2 \\mathbb{E}\\left[(1 + ah + b \\Delta W_n)^2 \\mid \\mathcal{F}_{t_n}\\right]\\right]\n$$\nBecause $\\Delta W_n$ is independent of $\\mathcal{F}_{t_n}$, the conditional expectation becomes an unconditional expectation with respect to the distribution of $\\Delta W_n$:\n$$\n\\mathbb{E}\\left[(1 + ah + b \\Delta W_n)^2 \\mid \\mathcal{F}_{t_n}\\right] = \\mathbb{E}\\left[(1 + ah + b \\Delta W_n)^2\\right]\n$$\nLet's expand the squared term and compute this expectation:\n\\begin{align*} \\mathbb{E}\\left[(1 + ah + b \\Delta W_n)^2\\right] = \\mathbb{E}\\left[(1 + ah)^2 + 2b(1+ah)\\Delta W_n + b^2(\\Delta W_n)^2\\right] \\\\ = (1+ah)^2 + 2b(1+ah)\\mathbb{E}[\\Delta W_n] + b^2\\mathbb{E}[(\\Delta W_n)^2] \\end{align*}\nUsing the properties $\\mathbb{E}[\\Delta W_n] = 0$ and $\\mathbb{E}[(\\Delta W_n)^2] = h$, we get:\n$$\n\\mathbb{E}\\left[(1 + ah + b \\Delta W_n)^2\\right] = (1+ah)^2 + b^2h = 1 + 2ah + a^2h^2 + b^2h\n$$\nSubstituting this back into the expression for $\\mathbb{E}[X_{n+1}^2]$:\n$$\n\\mathbb{E}[X_{n+1}^2] = \\mathbb{E}\\left[X_n^2 (1 + 2ah + a^2h^2 + b^2h)\\right] = \\mathbb{E}[X_n^2] (1 + 2ah + a^2h^2 + b^2h)\n$$\nThe mean-square amplification factor $G(h)$ is the ratio of these expectations:\n$$\nG(h) = \\frac{\\mathbb{E}[X_{n+1}^2]}{\\mathbb{E}[X_n^2]} = 1 + 2ah + b^2h + a^2h^2\n$$\nArranging this as a polynomial in $h$, we get the first part of the answer:\n$$\nG(h) = a^2 h^2 + (2a + b^2)h + 1\n$$\nNext, we find the maximal admissible step size $h_{\\max}$ for mean-square stability. The condition for stability is $G(h)  1$.\n$$\na^2 h^2 + (2a + b^2)h + 1  1\n$$\nSubtracting $1$ from both sides yields:\n$$\na^2 h^2 + (2a + b^2)h  0\n$$\nWe can factor out $h$:\n$$\nh (a^2 h + (2a + b^2))  0\n$$\nSince the time step $h$ must be positive ($h0$), for the product to be negative, the second factor must be negative:\n$$\na^2 h + (2a + b^2)  0\n$$\nNow, we solve for $h$:\n$$\na^2 h  -(2a + b^2)\n$$\nThe problem states $a \\neq 0$, which implies $a^2  0$. We can divide by $a^2$ without changing the direction of the inequality:\n$$\nh  \\frac{-(2a + b^2)}{a^2}\n$$\nCombining this with the requirement that $h  0$, the EM method is mean-square stable for all $h$ in the interval:\n$$\n0  h  \\frac{-(2a + b^2)}{a^2}\n$$\nThe problem states that the exact solution is stable when $2a + b^2  0$. This condition ensures that the upper bound of the interval is positive, so a region of stability for the numerical method exists. The maximal admissible step size $h_{\\max}$ is the supremum of this interval.\n$$\nh_{\\max} = \\frac{-(2a + b^2)}{a^2}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} a^{2}h^{2} + (2a+b^{2})h + 1  \\frac{-(2a+b^{2})}{a^{2}} \\end{pmatrix}}\n$$", "id": "3059094"}, {"introduction": "Having established the stability bottleneck of explicit methods, we now explore the solution. This practice [@problem_id:3059170] applies the drift-implicit Euler-Maruyama scheme to the same test problem, allowing for a direct comparison of performance. Your task is to derive the new mean-square amplification factor and prove that, under suitable conditions, the scheme becomes unconditionally stable, liberating the step size from the constraints of stiffness.", "problem": "Consider the scalar linear Stochastic Differential Equation (SDE), defined as $dX(t) = a\\,X(t)\\,dt + b\\,X(t)\\,dW(t)$, where $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are constants, and $W(t)$ is a standard Wiener process (also called Brownian motion). Let $\\{t_n\\}_{n \\ge 0}$ be a uniform time grid with $t_{n+1} - t_n = h  0$, and let $\\Delta W_n := W(t_{n+1}) - W(t_n)$ denote the independent Wiener increments with $\\Delta W_n \\sim \\mathcal{N}(0,h)$.\n\nFormulate the drift-implicit Euler–Maruyama time-stepping scheme that treats the drift term implicitly and the diffusion term explicitly for this SDE, and from first principles derive the one-step random amplification factor $G_n$ such that $X_{n+1} = G_n\\,X_n$. Then compute the mean-square amplification factor $\\rho(h)$, defined by the relation $\\mathbb{E}\\!\\left[|X_{n+1}|^{2}\\right] = \\rho(h)\\,\\mathbb{E}\\!\\left[|X_n|^{2}\\right]$, in closed form as a function of $a$, $b$, and $h$.\n\nFinally, using only fundamental properties of Wiener increments and conditional expectation, prove that when $a  0$ there exists a threshold on $b$ such that the drift-implicit Euler–Maruyama scheme is mean-square stable for all step sizes $h  0$, in the sense that $\\rho(h)  1$ for every $h  0$. Identify this threshold explicitly in terms of $a$.\n\nReport the mean-square amplification factor $\\rho(h)$ as your final answer in a single closed-form analytic expression. No rounding is required.", "solution": "The scalar linear Stochastic Differential Equation (SDE) is given by:\n$$ dX(t) = a\\,X(t)\\,dt + b\\,X(t)\\,dW(t) $$\nwhere $a, b \\in \\mathbb{R}$ are constants and $W(t)$ is a standard Wiener process. We discretize this SDE on a uniform time grid $\\{t_n\\}_{n \\ge 0}$ with a constant step size $h = t_{n+1} - t_n  0$. Let $X_n$ denote the numerical approximation of $X(t_n)$, and $\\Delta W_n = W(t_{n+1}) - W(t_n)$ be the Wiener increment, which is a random variable with distribution $\\mathcal{N}(0,h)$.\n\nFirst, we formulate the drift-implicit Euler–Maruyama scheme. This scheme treats the drift term, $a\\,X(t)\\,dt$, implicitly (evaluated at time $t_{n+1}$) and the diffusion term, $b\\,X(t)\\,dW(t)$, explicitly (evaluated at time $t_n$). The resulting difference equation is:\n$$ X_{n+1} - X_n = a\\,X_{n+1}\\,h + b\\,X_n\\,\\Delta W_n $$\nTo find the one-step random amplification factor $G_n$ such that $X_{n+1} = G_n X_n$, we rearrange the equation to solve for $X_{n+1}$:\n$$ X_{n+1} - a\\,h\\,X_{n+1} = X_n + b\\,X_n\\,\\Delta W_n $$\n$$ X_{n+1}(1 - a\\,h) = X_n(1 + b\\,\\Delta W_n) $$\nAssuming $1 - a\\,h \\neq 0$, we can write:\n$$ X_{n+1} = \\left( \\frac{1 + b\\,\\Delta W_n}{1 - a\\,h} \\right) X_n $$\nThus, the one-step random amplification factor is $G_n = \\frac{1 + b\\,\\Delta W_n}{1 - a\\,h}$. The case $a0$ and $h0$ ensures $1-ah1$, so the denominator is non-zero.\n\nNext, we compute the mean-square amplification factor $\\rho(h)$, defined by the relation $\\mathbb{E}[|X_{n+1}|^2] = \\rho(h)\\,\\mathbb{E}[|X_n|^2]$. Starting from the expression for $X_{n+1}$:\n$$ |X_{n+1}|^2 = \\left| \\left( \\frac{1 + b\\,\\Delta W_n}{1 - a\\,h} \\right) X_n \\right|^2 = \\frac{|1 + b\\,\\Delta W_n|^2}{(1 - a\\,h)^2} |X_n|^2 $$\nWe take the expectation of both sides. Using the law of total expectation with the filtration $\\mathcal{F}_{t_n}$ (the information available up to time $t_n$), we have:\n$$ \\mathbb{E}[|X_{n+1}|^2] = \\mathbb{E}\\left[ \\mathbb{E}\\left[ \\frac{|1 + b\\,\\Delta W_n|^2}{(1 - a\\,h)^2} |X_n|^2 \\bigg| \\mathcal{F}_{t_n} \\right] \\right] $$\nSince $X_n$ is $\\mathcal{F}_{t_n}$-measurable, $|X_n|^2$ and the deterministic constant $(1-ah)^2$ can be factored out of the inner conditional expectation:\n$$ \\mathbb{E}[|X_{n+1}|^2] = \\mathbb{E}\\left[ \\frac{|X_n|^2}{(1 - a\\,h)^2} \\mathbb{E}\\left[ |1 + b\\,\\Delta W_n|^2 \\bigg| \\mathcal{F}_{t_n} \\right] \\right] $$\nThe Wiener increment $\\Delta W_n$ is independent of the past filtration $\\mathcal{F}_{t_n}$. Therefore, any function of $\\Delta W_n$ is also independent of $\\mathcal{F}_{t_n}$, and its conditional expectation is equal to its unconditional expectation:\n$$ \\mathbb{E}\\left[ |1 + b\\,\\Delta W_n|^2 \\bigg| \\mathcal{F}_{t_n} \\right] = \\mathbb{E}[|1 + b\\,\\Delta W_n|^2] $$\nSubstituting this back and pulling the resulting deterministic term out of the outer expectation gives:\n$$ \\mathbb{E}[|X_{n+1}|^2] = \\frac{\\mathbb{E}[|1 + b\\,\\Delta W_n|^2]}{(1 - a\\,h)^2} \\mathbb{E}[|X_n|^2] $$\nComparing this to the definition of $\\rho(h)$, we find:\n$$ \\rho(h) = \\frac{\\mathbb{E}[|1 + b\\,\\Delta W_n|^2]}{(1 - a\\,h)^2} $$\nWe now compute the expectation in the numerator. We know $\\Delta W_n \\sim \\mathcal{N}(0,h)$, so its moments are $\\mathbb{E}[\\Delta W_n] = 0$ and $\\mathbb{E}[(\\Delta W_n)^2] = \\text{Var}(\\Delta W_n) + (\\mathbb{E}[\\Delta W_n])^2 = h + 0 = h$. Expanding the term and using linearity of expectation:\n$$ \\mathbb{E}[|1 + b\\,\\Delta W_n|^2] = \\mathbb{E}[1 + 2b\\,\\Delta W_n + b^2(\\Delta W_n)^2] = 1 + 2b\\,\\mathbb{E}[\\Delta W_n] + b^2\\,\\mathbb{E}[(\\Delta W_n)^2] $$\n$$ \\mathbb{E}[|1 + b\\,\\Delta W_n|^2] = 1 + 2b(0) + b^2(h) = 1 + b^2h $$\nSubstituting this result into the expression for $\\rho(h)$ yields the final closed form:\n$$ \\rho(h) = \\frac{1 + b^2h}{(1 - a\\,h)^2} $$\n\nFinally, we find the condition on $b$ for mean-square stability when $a  0$. The scheme is mean-square stable if $\\rho(h)  1$ for all step sizes $h  0$.\n$$ \\frac{1 + b^2h}{(1 - a\\,h)^2}  1 $$\nSince $a  0$ and $h  0$, we have $a\\,h  0$, which implies $1 - a\\,h  1$. Thus, the denominator $(1 - a\\,h)^2$ is always positive, and we can multiply both sides by it:\n$$ 1 + b^2h  (1 - a\\,h)^2 $$\n$$ 1 + b^2h  1 - 2ah + a^2h^2 $$\nSubtracting $1$ from both sides gives:\n$$ b^2h  -2ah + a^2h^2 $$\nSince $h  0$, we can divide by $h$:\n$$ b^2  -2a + a^2h $$\nThis inequality must hold for all $h > 0$. Let's rearrange it to isolate the terms without $h$:\n$$ b^2 + 2a  a^2h $$\nThe right-hand side, $a^2h$, is positive for all $h>0$ (since $a0 \\implies a\\neq 0$). The term $a^2h$ can be made arbitrarily close to zero by choosing a sufficiently small $h$. For the inequality $b^2 + 2a  a^2h$ to hold for all positive $h$, the constant term on the left must be less than or equal to zero. That is, $b^2 + 2a \\le 0$.\n- If $b^2 + 2a  0$, the inequality holds because a negative number is always less than a positive number ($a^2h$).\n- If $b^2 + 2a = 0$, the inequality becomes $0  a^2h$, which is true for all $h  0$.\nTherefore, the necessary and sufficient condition for unconditional mean-square stability is $b^2 + 2a \\le 0$, which can be written as:\n$$ b^2 \\le -2a $$\nThis is the explicit threshold on $b$ in terms of $a$.", "answer": "$$\\boxed{\\frac{1 + b^2 h}{(1 - a h)^2}}$$", "id": "3059170"}, {"introduction": "Our analysis so far has relied on a linear SDE where the implicit step could be solved algebraically. In practice, most stiff SDEs are nonlinear, requiring iterative methods to solve the implicit equation at each time step. This exercise [@problem_id:3059087] transitions from theory to application, examining the key computational aspects of implementing a drift-implicit solver, such as the use of Newton's method, the structure of the Jacobian, and the trade-offs between solver accuracy and overall efficiency.", "problem": "Consider a stochastic differential equation (SDE) in $\\mathbb{R}^{d}$ of the form\n$$\n\\mathrm{d}X_{t} = f(X_{t})\\,\\mathrm{d}t + G(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $f:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a drift field, $G:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times m}$ is a diffusion coefficient, and $W_{t}$ is an $m$-dimensional Wiener process (also called standard Brownian motion). Assume $f$ is sufficiently smooth, globally Lipschitz, and exhibits stiffness in the sense that the Jacobian $J_{f}(x)$ has eigenvalues with large negative real parts near a stable equilibrium, and that $G$ is bounded and sufficiently smooth.\n\nA widely used time discretization for stiff problems is the drift-implicit Euler–Maruyama scheme with explicit diffusion:\n$$\nX_{n+1} = X_{n} + h\\, f(X_{n+1}) + G(X_{n})\\,\\Delta W_{n},\n$$\nwhere $h0$ is the time step and $\\Delta W_{n} \\sim \\mathcal{N}(0, h I_{m})$ are independent Gaussian increments. Implementation requires solving, at each time step, the nonlinear system for $X_{n+1}$ obtained by setting the residual\n$$\nR(y) := y - X_{n} - h\\, f(y) - G(X_{n})\\,\\Delta W_{n}\n$$\nto zero. A common choice is Newton’s method applied to $R(y) = 0$, whose iteration uses the Jacobian\n$$\nJ(y) := \\frac{\\partial R}{\\partial y}(y) = I_{d} - h\\, J_{f}(y),\n$$\nwhile fixed-point (Picard) iteration uses the map $T(y) := X_{n} + h\\, f(y) + G(X_{n})\\,\\Delta W_{n}$.\n\nAnswer the following multiple-choice question by selecting all statements that are correct. Your reasoning should be based on the core definitions above together with standard facts about Newton’s method, fixed-point iteration, and computational complexity of dense linear algebra.\n\nWhich of the following statements about implementing the drift-implicit Euler–Maruyama scheme for stiff SDEs are correct?\n\nA. When $G$ is treated explicitly as above, the Newton Jacobian $J(y) = I_{d} - h\\,J_{f}(y)$ does not depend on the Wiener increment $\\Delta W_{n}$. Reusing a Jacobian across inner Newton iterations within a single time step can be effective when $J_{f}(y)$ varies slowly near the solution, because the principal variation comes from $J_{f}$, not from $\\Delta W_{n}$.\n\nB. The fixed-point iteration $y^{(k+1)} = X_{n} + h\\, f\\!\\left(y^{(k)}\\right) + G(X_{n})\\,\\Delta W_{n}$ always converges for stiff drifts, regardless of the step size $h$, because the implicitness stabilizes the iteration.\n\nC. In dimension $d$ with dense Jacobians, forming and factorizing the Newton Jacobian $J(y)$ at each iteration has computational cost that scales on the order of $O(d^{3})$. Quasi-Newton methods that update an approximate inverse or factorization (such as a Broyden-type method) can reduce the per-iteration cost but may sacrifice some global convergence robustness.\n\nD. If the diffusion $G(x)$ is state-dependent but treated explicitly in the scheme $X_{n+1} = X_{n} + h f(X_{n+1}) + G(X_{n})\\,\\Delta W_{n}$, then the Newton linearization must include $\\frac{\\partial}{\\partial y}\\big(G(y)\\,\\Delta W_{n}\\big)$ to retain first-order strong accuracy; otherwise, the method’s strong order drops.\n\nE. For the scalar stiff linear SDE $\\mathrm{d}X_{t} = -\\lambda X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$ with $\\lambda0$ and $\\sigma0$, the drift-implicit update is\n$$\nX_{n+1} = \\frac{X_{n} + \\sigma\\,\\Delta W_{n}}{1 + h\\,\\lambda},\n$$\nwhich requires no nonlinear solve. This update is unconditionally stable with respect to the drift in the sense that the deterministic amplification factor $1/(1 + h\\,\\lambda)$ lies strictly inside the unit disk for all $h0$.\n\nF. Choosing nonlinear solver tolerances (for Newton or fixed-point inner solves) much tighter than the strong discretization error of the overall method (which is $O(h^{1/2})$ for Euler–Maruyama under standard assumptions) typically yields negligible improvement in the pathwise accuracy while increasing computational cost per step.\n\nSelect all correct options.", "solution": "We will evaluate each statement.\n\n**Statement A: Correct**\nThe residual function for the nonlinear system is given by\n$$\nR(y) := y - X_{n} - h\\, f(y) - G(X_{n})\\,\\Delta W_{n}.\n$$\nThe Newton iteration for solving $R(y)=0$ involves the Jacobian of $R$ with respect to $y$. We compute this Jacobian:\n$$\nJ(y) = \\frac{\\partial R}{\\partial y}(y) = \\frac{\\partial}{\\partial y} \\left( y - X_{n} - h\\, f(y) - G(X_{n})\\,\\Delta W_{n} \\right).\n$$\nIn this context, $X_{n}$ and $\\Delta W_{n}$ are fixed values from the previous step and the current Wiener increment, respectively. The diffusion term $G(X_n)$ is evaluated at $X_n$ and is therefore constant with respect to the solver variable $y$. The derivatives of the terms are:\n- $\\frac{\\partial}{\\partial y} y = I_{d}$ (the $d \\times d$ identity matrix)\n- $\\frac{\\partial}{\\partial y} (-X_{n}) = 0$\n- $\\frac{\\partial}{\\partial y} (-h\\,f(y)) = -h\\, J_{f}(y)$, where $J_{f}(y)$ is the Jacobian of $f$.\n- $\\frac{\\partial}{\\partial y} (-G(X_{n})\\,\\Delta W_{n}) = 0$\n\nCombining these gives the Newton Jacobian:\n$$\nJ(y) = I_{d} - h\\, J_{f}(y).\n$$\nThis expression clearly does not depend on the Wiener increment $\\Delta W_{n}$. The variation in $J(y)$ during the Newton solve for a fixed time step $n$ comes only from the change in $y$ as it iterates towards $X_{n+1}$, which in turn affects $J_{f}(y)$. If $J_{f}(y)$ varies slowly with $y$, then $J(y)$ also varies slowly. In such cases, reusing the Jacobian computed at the start of the Newton solve (a \"frozen\" or \"sham\" Newton method) is a standard and effective technique to reduce computational cost, as the dominant cost is often the factorization of this matrix. The reasoning provided in the statement is entirely correct. Thus, this statement is **Correct**.\n\n**Statement B: Incorrect**\nThe fixed-point iteration is given by $y^{(k+1)} = T(y^{(k)})$, where the map is\n$$\nT(y) := X_{n} + h\\, f(y) + G(X_{n})\\,\\Delta W_{n}.\n$$\nAccording to the Banach fixed-point theorem, a sufficient condition for the convergence of this iteration from any starting point in a neighborhood is that the map $T$ is a contraction in that neighborhood. The contraction property is assessed by the Jacobian of $T$ with respect to $y$:\n$$\nJ_{T}(y) = \\frac{\\partial T}{\\partial y} = h\\, J_{f}(y).\n$$\nFor convergence, we require that an induced matrix norm of $J_{T}(y)$ is less than $1$, i.e., $\\|h\\, J_{f}(y)\\|  1$. This is related to the spectral radius condition $\\rho(h\\, J_{f}(y))  1$.\nThe problem states the drift is stiff, meaning the Jacobian $J_{f}(y)$ has eigenvalues $\\mu_{i}$ with large negative real parts. The eigenvalues of $J_{T}(y)$ are $h\\mu_{i}$. The condition for convergence becomes $|h\\mu_{i}|  1$ for all $i$. If there is a stiff eigenvalue with $|\\mu_{i}| = L \\gg 1$, then convergence requires $hL  1$, or $h  1/L$. This imposes a severe restriction on the step size $h$, which is precisely the type of restriction that implicit methods are designed to overcome for stability purposes.\nThe statement claims the iteration *always converges* regardless of $h$. This is false. While the underlying drift-implicit scheme is valuable for its stability properties on stiff problems, solving the resulting nonlinear equation with a simple fixed-point iteration re-introduces a stiffness-related constraint on $h$ for the *solver's* convergence. Thus, this statement is **Incorrect**.\n\n**Statement C: Correct**\nThe Newton iteration at step $k$ requires solving the linear system $J(y^{(k)}) \\delta y^{(k)} = -R(y^{(k)})$.\n1.  **Forming and factorizing the Jacobian:** For a general dense drift Jacobian $J_{f}(y)$, forming the $d \\times d$ matrix $J(y) = I_d - h J_f(y)$ costs $O(d^2)$. The main computational burden is solving the linear system. Using a direct solver like Gaussian elimination to compute an LU factorization of the dense $d \\times d$ matrix $J(y)$ has a computational cost of $O(d^3)$. Therefore, the per-iteration cost is dominated by this factorization and scales as $O(d^3)$.\n2.  **Quasi-Newton methods:** Methods like Broyden's method are designed to reduce this cost. Instead of recomputing and re-factorizing the Jacobian at every step, they maintain an approximation to the Jacobian (or its inverse) and update it at a lower cost. For example, a rank-$1$ update to an approximate inverse matrix can be performed in $O(d^2)$ operations. This reduces the per-iteration cost from $O(d^3)$ to $O(d^2)$.\n3.  **Trade-offs:** The benefit of reduced cost comes at the expense of the convergence rate, which is typically superlinear for quasi-Newton methods, compared to quadratic for Newton's method (near the solution). This can mean more iterations are needed. Furthermore, because an approximate Jacobian is used, the search directions may be less accurate, potentially making the method less robust and more prone to failure, especially when far from the solution (affecting global convergence).\nThe statement accurately describes both the cost scaling and the trade-offs involved. Thus, this statement is **Correct**.\n\n**Statement D: Incorrect**\nThe scheme is explicitly defined as $X_{n+1} = X_{n} + h f(X_{n+1}) + G(X_{n})\\,\\Delta W_{n}$. The order of accuracy is a property of this discretization formula itself, not the method used to solve the algebraic equation for $X_{n+1}$. Under standard assumptions (Lipschitz conditions on $f$ and $G$, etc.), the Euler-Maruyama scheme (both explicit and drift-implicit versions) has a strong order of convergence of $1/2$, meaning the expected pathwise error at a fixed time $T$ behaves as $E[|X_T - X(T)|] = O(h^{1/2})$. The scheme as stated does not have strong order $1$. To achieve strong order $1$, a higher-order scheme like the Milstein scheme is required, which includes correction terms involving derivatives of the diffusion coefficient $G$. The statement incorrectly suggests that this scheme can have strong order $1$ and that the method for solving the nonlinear equation affects the scheme's order. The \"Newton linearization\" is a tool to find $X_{n+1}$ satisfying the given equation; it cannot change the equation or its inherent accuracy properties. The statement is fundamentally confused. Thus, this statement is **Incorrect**.\n\n**Statement E: Correct**\nThe SDE is the scalar Ornstein-Uhlenbeck process: $\\mathrm{d}X_{t} = -\\lambda X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$. Here, $f(x) = -\\lambda x$ and $G(x) = \\sigma$.\nThe drift-implicit Euler-Maruyama scheme is $X_{n+1} = X_n + h f(X_{n+1}) + G(X_n) \\Delta W_n$.\nSubstituting the specific forms of $f$ and $G$:\n$$\nX_{n+1} = X_{n} + h(-\\lambda X_{n+1}) + \\sigma \\Delta W_{n}.\n$$\nThis is a linear equation in $X_{n+1}$. We can solve for it algebraically:\n$$\nX_{n+1} + h\\lambda X_{n+1} = X_{n} + \\sigma\\Delta W_{n}\n$$\n$$\n(1 + h\\lambda)X_{n+1} = X_{n} + \\sigma\\Delta W_{n}\n$$\n$$\nX_{n+1} = \\frac{X_{n} + \\sigma\\Delta W_{n}}{1 + h\\lambda}.\n$$\nThis matches the formula in the statement and, being an explicit formula, requires no iterative nonlinear solve.\nFor stability analysis, we consider the deterministic part of the recursion (i.e., set $\\sigma=0$ or $\\Delta W_n = 0$), which gives $X_{n+1} = \\frac{1}{1+h\\lambda} X_n$. The term $R(h\\lambda) = \\frac{1}{1+h\\lambda}$ is the amplification factor. The method is considered stable with respect to the drift if $|R(h\\lambda)|  1$.\nGiven that $\\lambda  0$ and $h  0$, we have $h\\lambda  0$. Therefore, the denominator $1+h\\lambda  1$. This implies that $0  \\frac{1}{1+h\\lambda}  1$. This condition holds for all $h0$, demonstrating unconditional stability (in the sense of A-stability from ODEs). The amplification factor is always within the open unit disk $(-1, 1)$. Thus, this statement is **Correct**.\n\n**Statement F: Correct**\nThe total error in computing a path is an aggregate of two main error sources at each step:\n1.  **Discretization Error:** The error inherent to the scheme, which approximates a continuous process with a discrete one. For the Euler-Maruyama method, this leads to a global strong error of $O(h^{1/2})$.\n2.  **Solver Error:** The error from solving the implicit equation $R(y)=0$ approximately. The solver tolerance, $\\epsilon_{\\text{tol}}$, controls the accuracy of this inner solve, i.e., how close the computed $\\tilde{X}_{n+1}$ is to the true root $X_{n+1}$ of the algebraic equation.\n\nIt is a general principle in numerical analysis to balance error sources. There is no benefit in reducing one source of error far below the level of another, dominant source of error. In this case, the pathwise accuracy is fundamentally limited by the $O(h^{1/2})$ discretization error. If the nonlinear solver tolerance is set to be much smaller than this (e.g., $\\epsilon_{\\text{tol}} \\ll h^{1/2}$), the additional computations performed to achieve this high precision in the inner solve will be \"wasted,\" as the tiny improvement in the single-step algebraic solution will be completely overwhelmed by the much larger discretization error. This increases the cost per step (more Newton iterations) with no discernible improvement in the final global accuracy of the computed path. Therefore, it is computationally efficient to choose a solver tolerance that is matched to the discretization error, e.g., $\\epsilon_{\\text{tol}} = O(h)$ or $O(h^{3/2})$. The statement reflects this standard and important concept in scientific computing. Thus, this statement is **Correct**.", "answer": "$$\\boxed{ACEF}$$", "id": "3059087"}]}