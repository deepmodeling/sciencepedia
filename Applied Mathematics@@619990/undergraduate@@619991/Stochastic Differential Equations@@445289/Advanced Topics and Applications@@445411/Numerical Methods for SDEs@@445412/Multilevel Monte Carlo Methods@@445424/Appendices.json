{"hands_on_practices": [{"introduction": "The efficiency of the Multilevel Monte Carlo method hinges on the telescoping sum identity $\\mathbb{E}[P_L] = \\sum_{\\ell=0}^L \\mathbb{E}[P_\\ell - P_{\\ell-1}]$, and more specifically, on keeping the variance of each difference term, $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$, as small as possible. This exercise demonstrates that strong coupling is not merely a technical detail but the foundational mechanism enabling this variance reduction. By comparing the variance under correct and incorrect coupling, you will gain a crucial intuition for why the method is so effective. [@problem_id:3068034]", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$ on the time interval $[0,T]$, where $W_t$ is a standard Wiener process and $\\sigma > 0$ is a constant. Let $P_\\ell$ denote the Euler–Maruyama terminal approximation of $X_T$ computed on level $\\ell$ with $N_\\ell$ steps of size $h_\\ell = T/N_\\ell$, and let $P_{\\ell-1}$ denote the corresponding approximation computed on level $\\ell-1$ with $N_{\\ell-1} = N_\\ell/2$ steps of size $h_{\\ell-1} = 2 h_\\ell$. In both cases, the Euler–Maruyama scheme is defined by $X_{n+1} = X_n + \\sigma\\, \\Delta W_n$, where each Brownian increment $\\Delta W_n$ is distributed as $\\mathcal{N}(0,h)$ with $h$ equal to the relevant step size. Define $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$ and $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$. In the Multilevel Monte Carlo (MLMC) method, coupling across levels is used to reduce the variance of the level differences.\n\nTwo coupling strategies are considered:\n- Correct coupling: for each $m \\in \\{1,\\dots,N_{\\ell-1}\\}$, set $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$, where the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed with law $\\mathcal{N}(0,h_\\ell)$.\n- Incorrect coupling: generate $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ as independent and identically distributed $\\mathcal{N}(0,h_{\\ell-1})$ increments that are independent of the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$.\n\nStarting only from the properties of independent Gaussian increments of Brownian motion and the variance additivity for independent random variables, derive $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ under each coupling strategy. Then, express the incorrect-coupling variance $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ as a simplified closed-form analytic expression in terms of $\\sigma$ and $T$. Your final answer must be this expression. No rounding is required.", "solution": "The problem requires the derivation of the variance of the difference between a fine and a coarse Euler-Maruyama approximation, $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$, under two different coupling strategies for the stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$. The final answer is the specific expression for this variance under the incorrect coupling strategy.\n\nLet $P_\\ell$ and $P_{\\ell-1}$ be the random variables representing the numerical approximations at the terminal time $T$ on level $\\ell$ (fine) and level $\\ell-1$ (coarse), respectively. The general formula for the variance of the difference of two random variables is:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1}) - 2 \\mathrm{Cov}(P_\\ell, P_{\\ell-1})\n$$\nWe analyze each coupling strategy by first calculating the individual variances, $\\mathrm{Var}(P_\\ell)$ and $\\mathrm{Var}(P_{\\ell-1})$, and then evaluating the covariance term.\n\nThe fine-level approximation is given by $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$, where $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed (i.i.d.) random variables with distribution $\\mathcal{N}(0,h_\\ell)$. The variance of $P_\\ell$ is calculated using the property of variance additivity for independent random variables and the scaling property $\\mathrm{Var}(aZ) = a^2 \\mathrm{Var}(Z)$:\n$$\n\\mathrm{Var}(P_\\ell) = \\mathrm{Var}\\left(\\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right) = \\sigma^2 \\mathrm{Var}\\left(\\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right)\n$$\nSince the increments $\\Delta W^{(\\ell)}_n$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 \\sum_{n=1}^{N_\\ell} \\mathrm{Var}(\\Delta W^{(\\ell)}_n) = \\sigma^2 \\sum_{n=1}^{N_\\ell} h_\\ell = \\sigma^2 N_\\ell h_\\ell\n$$\nGiven that the step size is $h_\\ell = T/N_\\ell$, we have $N_\\ell h_\\ell = T$. Therefore, the variance of the fine-level approximation is:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 T\n$$\nSimilarly, the coarse-level approximation is given by $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$, where the increments $\\Delta W^{(\\ell-1)}_m$ are i.i.d. $\\mathcal{N}(0,h_{\\ell-1})$. Following the same logic:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} \\mathrm{Var}(\\Delta W^{(\\ell-1)}_m) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} h_{\\ell-1} = \\sigma^2 N_{\\ell-1} h_{\\ell-1}\n$$\nGiven $h_{\\ell-1} = T/N_{\\ell-1}$, we have $N_{\\ell-1} h_{\\ell-1} = T$. Thus, the variance of the coarse-level approximation is:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 T\n$$\nWe now proceed to evaluate $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ for each coupling strategy.\n\n**1. Incorrect Coupling Strategy**\n\nUnder this strategy, the set of coarse increments $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ is generated independently from the set of fine increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$. Since $P_\\ell$ is a function only of the fine increments and $P_{\\ell-1}$ is a function only of the coarse increments, the random variables $P_\\ell$ and $P_{\\ell-1}$ are independent. For independent random variables, the covariance is zero:\n$$\n\\mathrm{Cov}(P_\\ell, P_{\\ell-1}) = 0\n$$\nSubstituting this into the general variance formula gives:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1})\n$$\nUsing the previously derived variances for $P_\\ell$ and $P_{\\ell-1}$:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\sigma^2 T + \\sigma^2 T = 2 \\sigma^2 T\n$$\nThis is the variance under the incorrect coupling.\n\n**2. Correct Coupling Strategy**\n\nUnder this strategy, the coarse increments are constructed from the fine increments: $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$. We compute the difference $P_\\ell - P_{\\ell-1}$ directly:\n$$\nP_\\ell - P_{\\ell-1} = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n - \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m\n$$\nSubstituting the coupling definition into the second term:\n$$\n\\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m = \\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right)\n$$\nThis sum covers all the fine increments from $n=1$ to $n=2N_{\\ell-1}$. Since $N_\\ell = 2N_{\\ell-1}$, this sum is precisely the sum of all fine increments:\n$$\n\\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right) = \\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n = P_\\ell\n$$\nTherefore, for this specific SDE, the coarse approximation $P_{\\ell-1}$ is identical to the fine approximation $P_\\ell$ when correct coupling is used. The difference is:\n$$\nP_\\ell - P_{\\ell-1} = P_\\ell - P_\\ell = 0\n$$\nThe variance of a constant is zero:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(0) = 0\n$$\nThis result highlights the a priori variance reduction achieved by correct coupling, which is the foundational principle of the Multilevel Monte Carlo method. For this simple SDE, the reduction is maximal.\n\nThe problem asks for the simplified closed-form analytic expression for the incorrect-coupling variance in terms of $\\sigma$ and $T$. As derived above, this expression is $2 \\sigma^2 T$.", "answer": "$$\n\\boxed{2 \\sigma^{2} T}\n$$", "id": "3068034"}, {"introduction": "Once we understand that coupling reduces variance, the next logical step is to determine how to distribute our computational budget across the different levels to achieve a target accuracy with minimum effort. This involves choosing the optimal number of samples, $N_\\ell$, for each level $\\ell$. This practice guides you through the foundational constrained optimization problem at the heart of MLMC theory, revealing the elegant principle that governs how the method balances work between cheap, high-variance coarse levels and expensive, low-variance fine levels. [@problem_id:3067970]", "problem": "Consider a scalar stochastic differential equation (SDE) of the form $dX_{t}=a(X_{t})\\,dt+b(X_{t})\\,dW_{t}$, where $a$ and $b$ are sufficiently regular for strong existence and uniqueness, and $W_{t}$ is a standard Brownian motion. The goal is to approximate $\\mathbb{E}[\\varphi(X_{T})]$ for a Lipschitz payoff $\\varphi$ using Multilevel Monte Carlo (MLMC). Let levels be indexed by $\\ell\\in\\{0,1,\\dots,L\\}$ with time steps $h_{\\ell}=h_{0}\\,2^{-\\ell}$, where $h_{0}>0$ and $L\\in\\mathbb{N}$ is chosen to control the bias. Assume Euler–Maruyama coupling of adjacent levels and that the variance of the level-difference estimator and the expected cost per coupled sample scale as\n$$V_{\\ell}=k_{v}\\,h_{\\ell}\\quad\\text{and}\\quad C_{\\ell}=k_{c}\\,h_{\\ell}^{-1},$$\nfor all $\\ell\\in\\{0,1,\\dots,L\\}$, with $k_{v}>0$ and $k_{c}>0$ independent of $\\ell$. Suppose the mean-squared error (MSE) tolerance is $\\varepsilon>0$, and the level $L$ has already been chosen so that the squared bias is at most $\\varepsilon^{2}/2$. You are to choose the number of samples per level $N_{\\ell}\\in\\mathbb{R}_{>0}$ to minimize the total expected cost subject to a variance budget equal to the remaining half of the MSE tolerance:\n$$\\text{minimize }\\sum_{\\ell=0}^{L} C_{\\ell}\\,N_{\\ell}\\quad\\text{subject to}\\quad \\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}}=\\frac{\\varepsilon^{2}}{2}.$$\nStarting only from these assumptions and optimization primitives, derive the exact closed-form expressions for:\n- the optimal $N_{\\ell}$ as a function of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$,\n- the resulting contribution to the expected cost on level $\\ell$, namely $C_{\\ell}\\,N_{\\ell}$,\n- the resulting contribution to the estimator variance from level $\\ell$, namely $V_{\\ell}/N_{\\ell}$.\n\nExpress your final result as three closed-form expressions depending on $\\ell$ in terms of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$. Your final answer must be a single row with three entries, each entry being one of these expressions. Do not introduce any undefined symbols. Do not eliminate $L$ or $h_{\\ell}$ from your final expressions. No numerical rounding is required.", "solution": "The user wants me to solve a constrained optimization problem derived from the Multilevel Monte Carlo (MLMC) method for stochastic differential equations (SDEs).\n\n### Step 1: Extract Givens\n-   SDE: $dX_{t}=a(X_{t})\\,dt+b(X_{t})\\,dW_{t}$\n-   Goal: Approximate $\\mathbb{E}[\\varphi(X_{T})]$\n-   Levels: $\\ell \\in \\{0, 1, \\dots, L\\}$\n-   Time steps: $h_{\\ell} = h_{0}\\,2^{-\\ell}$ for $h_{0} > 0$\n-   Variance scaling: $V_{\\ell} = k_{v}\\,h_{\\ell}$ for $k_{v} > 0$\n-   Cost scaling: $C_{\\ell} = k_{c}\\,h_{\\ell}^{-1}$ for $k_{c} > 0$\n-   Mean-squared error (MSE) tolerance: $\\varepsilon > 0$\n-   Bias constraint: Squared bias is at most $\\varepsilon^{2}/2$, achieved by the choice of $L$.\n-   Optimization problem:\n    -   Minimize: $\\sum_{\\ell=0}^{L} C_{\\ell}\\,N_{\\ell}$\n    -   Subject to: $\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{\\varepsilon^{2}}{2}$\n    -   Variables: $N_{\\ell} \\in \\mathbb{R}_{>0}$ for $\\ell = 0, \\dots, L$.\n-   Required outputs: Closed-form expressions for $N_{\\ell}$, $C_{\\ell}N_{\\ell}$, and $V_{\\ell}/N_{\\ell}$ as functions of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly rooted in the theory of numerical methods for SDEs, specifically the MLMC method. The assumed scaling relations for variance ($V_{\\ell} \\propto h_{\\ell}$) and computational cost ($C_{\\ell} \\propto h_{\\ell}^{-1}$) are standard for the Euler-Maruyama discretization scheme applied to SDEs with globally Lipschitz coefficients. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** The problem is a classic constrained optimization task: minimizing a linear objective function subject to a convex constraint. The domain for the variables $N_{\\ell}$ is the set of positive real numbers, which allows for a straightforward solution using calculus (e.g., Lagrange multipliers). A unique, stable, and meaningful solution exists.\n3.  **Objective:** The problem is stated in precise mathematical language, free of any subjectivity or ambiguity.\n4.  **Complete and Consistent:** The problem provides all necessary information to solve the optimization problem: the objective function, the constraint, the variables, and the definitions of all terms involved. There are no contradictions.\n5.  **Not Unrealistic:** The setup is a standard theoretical model for analyzing the complexity of MLMC methods. It is a valid and widely used simplification.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed to derive the solution.\n\n### Derivation\nThe problem is to minimize the total cost $J = \\sum_{\\ell=0}^{L} C_{\\ell} N_{\\ell}$ subject to the total variance constraint $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\varepsilon^{2}/2$. We can solve this using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{\\ell=0}^{L} C_{\\ell} N_{\\ell} + \\lambda \\left( \\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} - \\frac{\\varepsilon^{2}}{2} \\right) $$\nwhere $\\lambda$ is the Lagrange multiplier. To find the optimal values of $N_{\\ell}$, we take the partial derivative of $\\mathcal{L}$ with respect to each $N_{k}$ for $k \\in \\{0, \\dots, L\\}$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial N_{k}} = C_{k} - \\lambda \\frac{V_{k}}{N_{k}^{2}} = 0 $$\nSolving for $N_{k}$, we get:\n$$ C_{k} N_{k}^{2} = \\lambda V_{k} \\implies N_{k} = \\sqrt{\\lambda} \\sqrt{\\frac{V_{k}}{C_{k}}} $$\nNote that we take the positive root since $N_{k}$ must be positive.\n\nNow, we substitute this expression for $N_{\\ell}$ back into the variance constraint to determine the value of $\\lambda$.\nFirst, let's find the expression for the term $V_{\\ell}/N_{\\ell}$:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{V_{\\ell}}{\\sqrt{\\lambda} \\sqrt{V_{\\ell}/C_{\\ell}}} = \\frac{1}{\\sqrt{\\lambda}} \\sqrt{V_{\\ell} C_{\\ell}} $$\nNext, we substitute the given scaling relations $V_{\\ell} = k_{v} h_{\\ell}$ and $C_{\\ell} = k_{c} h_{\\ell}^{-1}$:\n$$ \\sqrt{V_{\\ell} C_{\\ell}} = \\sqrt{(k_{v} h_{\\ell}) (k_{c} h_{\\ell}^{-1})} = \\sqrt{k_{v} k_{c}} $$\nThis shows that the product $\\sqrt{V_{\\ell} C_{\\ell}}$ is a constant, independent of the level $\\ell$.\nThe variance constraint becomes:\n$$ \\sum_{\\ell=0}^{L} \\frac{\\sqrt{k_{v} k_{c}}}{\\sqrt{\\lambda}} = \\frac{\\varepsilon^{2}}{2} $$\nSince the term in the sum is constant for all $\\ell$, the sum over $L+1$ levels is:\n$$ (L+1) \\frac{\\sqrt{k_{v} k_{c}}}{\\sqrt{\\lambda}} = \\frac{\\varepsilon^{2}}{2} $$\nSolving for $\\sqrt{\\lambda}$:\n$$ \\sqrt{\\lambda} = \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} $$\nNow we have determined the Lagrange multiplier and can find the explicit expressions for the required quantities.\n\n**1. Optimal number of samples $N_{\\ell}$**\nSubstitute the expression for $\\sqrt{\\lambda}$ into the equation for $N_{\\ell}$:\n$$ N_{\\ell} = \\sqrt{\\lambda} \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}} = \\left( \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} \\right) \\sqrt{\\frac{k_{v} h_{\\ell}}{k_{c} h_{\\ell}^{-1}}} $$\n$$ N_{\\ell} = \\left( \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} \\right) \\left( \\sqrt{\\frac{k_{v}}{k_{c}}} h_{\\ell} \\right) $$\n$$ N_{\\ell} = \\frac{2 (L+1)}{\\varepsilon^{2}} \\left( \\sqrt{k_{v}k_{c}} \\sqrt{\\frac{k_{v}}{k_{c}}} \\right) h_{\\ell} = \\frac{2 (L+1)}{\\varepsilon^{2}} \\sqrt{k_{v}^{2}} h_{\\ell} $$\nSince $k_{v} > 0$, we have $\\sqrt{k_{v}^2} = k_v$. Thus, the optimal number of samples on level $\\ell$ is:\n$$ N_{\\ell} = \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} $$\n\n**2. Cost contribution on level $\\ell$, $C_{\\ell} N_{\\ell}$**\nUsing the expression for $N_{\\ell}$ and the given $C_{\\ell} = k_{c} h_{\\ell}^{-1}$:\n$$ C_{\\ell} N_{\\ell} = (k_{c} h_{\\ell}^{-1}) \\left( \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} \\right) $$\nThe terms $h_{\\ell}^{-1}$ and $h_{\\ell}$ cancel out, yielding a constant cost per level:\n$$ C_{\\ell} N_{\\ell} = \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}} $$\n\n**3. Variance contribution from level $\\ell$, $V_{\\ell}/N_{\\ell}$**\nUsing the expression for $N_{\\ell}$ and the given $V_{\\ell} = k_{v} h_{\\ell}$:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{k_{v} h_{\\ell}}{\\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell}} $$\nThe terms $k_{v}$ and $h_{\\ell}$ cancel out, yielding a constant variance contribution per level:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{1}{\\frac{2(L+1)}{\\varepsilon^{2}}} = \\frac{\\varepsilon^{2}}{2 (L+1)} $$\nAs a consistency check, the total variance is $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\sum_{\\ell=0}^{L} \\frac{\\varepsilon^{2}}{2(L+1)} = (L+1) \\frac{\\varepsilon^{2}}{2(L+1)} = \\frac{\\varepsilon^{2}}{2}$, which satisfies the constraint.\n\nThe three required expressions are:\n1.  $N_{\\ell} = \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell}$\n2.  $C_{\\ell} N_{\\ell} = \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}}$\n3.  $\\frac{V_{\\ell}}{N_{\\ell}} = \\frac{\\varepsilon^{2}}{2(L+1)}$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} & \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}} & \\frac{\\varepsilon^{2}}{2(L+1)} \\end{pmatrix} } $$", "id": "3067970"}, {"introduction": "In practice, the theoretical formulas for optimal sample allocation require knowing the level variances, $V_\\ell$, and costs, $C_\\ell$, which are unknown for most real-world problems. The standard workflow is to first run a small pilot simulation to estimate these parameters. This comprehensive coding exercise walks you through this complete, practical pipeline, from simulating coupled paths and estimating parameters to applying the optimization theory to allocate computational resources, thereby bridging the gap from abstract concepts to a functional algorithm. [@problem_id:3067973]", "problem": "Consider the geometric Brownian motion stochastic differential equation (SDE) defined by $dX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t$ with initial condition $X_0 = x_0$ over time horizon $T$. Let $P_\\ell$ denote an Euler–Maruyama approximation of the payoff $\\phi(X_T)$ computed with a uniform time step $h_\\ell = T / M_\\ell$ at level $\\ell$, where $M_\\ell = M_0 \\cdot 2^\\ell$ and $M_0$ is a given base number of steps. In the multilevel Monte Carlo (MLMC) estimator, define $Y_0 = P_0$ and for $\\ell \\geq 1$ define $Y_\\ell = P_\\ell - P_{\\ell-1}$, computed using a strong coupling in which coarse-level Brownian increments are formed by summing blocks of fine-level increments so that the variance is reduced by the telescoping structure. For a given pilot sample of $Y_\\ell$, the goals are to estimate the variance $V_\\ell = \\operatorname{Var}(Y_\\ell)$ and the per-sample computational cost $C_\\ell$, and to use these estimates to determine the optimal number of samples $N_\\ell$ at each level $\\ell$ that minimizes total computational cost under a fixed target sampling variance budget.\n\nYou must start from core definitions and well-tested facts. The Euler–Maruyama scheme for the SDE $dX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t$ is defined by the recursion $X_{k+1} = X_k + \\mu X_k h + \\sigma X_k \\Delta W_k$, where $\\Delta W_k \\sim \\mathcal{N}(0, h)$ are independent Gaussian increments and $h$ is the time step. For the MLMC coupling, when building $P_\\ell$ and $P_{\\ell-1}$ from the finest-level increments, the coarse increments are formed by summing $r = 2^{L-\\ell}$ fine increments so that $\\Delta W^{(\\ell)}_j = \\sum_{k=1}^{r} \\Delta W^{(L)}_{(j-1)r + k}$ has variance $r \\cdot h_L = h_\\ell$, ensuring compatibility. The unbiased sample variance estimator of $V_\\ell$ based on $m$ pilot samples $\\{Y_\\ell^{(i)}\\}_{i=1}^m$ is $\\widehat{V}_\\ell = \\frac{1}{m-1} \\sum_{i=1}^{m} \\left(Y_\\ell^{(i)} - \\overline{Y}_\\ell\\right)^2$, where $\\overline{Y}_\\ell$ is the sample mean. For the computational cost, use the count of Euler steps as a proxy: set $C_0 = M_0$ and for $\\ell \\geq 1$ set $C_\\ell = M_\\ell + M_{\\ell-1}$ since one sample of $Y_\\ell$ requires simulating both the level $\\ell$ and level $\\ell-1$ approximations. The allocation of $N_\\ell$ shall be derived by minimizing total cost subject to a sampling variance budget constraint that allocates a fixed fraction to the MLMC estimator variance.\n\nYour task is to implement a complete program that:\n- Simulates pilot samples $\\{Y_\\ell^{(i)}\\}$ for levels $\\ell = 0, 1, \\dots, L$ using Euler–Maruyama with strong coupling constructed from the finest-level Brownian increments.\n- Estimates $V_\\ell$ using the unbiased sample variance formula.\n- Sets $C_\\ell$ using the step-count proxy $C_0 = M_0$ and $C_\\ell = M_\\ell + M_{\\ell-1}$ for $\\ell \\geq 1$.\n- Derives $N_\\ell$ to minimize total cost under the constraint that the total sampling variance $\\sum_{\\ell=0}^L V_\\ell / N_\\ell$ equals a specified budget, taken as $V_{\\text{target}} = \\varepsilon^2 / 2$, where $\\varepsilon$ is a given accuracy tolerance. If $\\sum_{\\ell=0}^L \\sqrt{V_\\ell C_\\ell} = 0$, set $N_\\ell = 0$ for all $\\ell$.\n- Produces the final output as a single line containing a list of lists, each inner list holding the integers $[N_0, N_1, \\dots, N_L]$ for one test case.\n\nUse the payoff $\\phi(x) = x$ to keep the focus on MLMC allocation. Ensure scientific realism by coupling levels via summed increments from the finest time step.\n\nTest Suite and Required Output Format:\n- Implement the following test cases. For reproducibility, use the specified random seeds. For each case, compute pilot estimates and return the vector $[N_0, N_1, \\dots, N_L]$.\n    - Case $1$: $\\mu = 0.05$, $\\sigma = 0.2$, $T = 1$, $x_0 = 1$, $L = 3$, $M_0 = 8$, pilot count $m = 200$, $\\varepsilon = 0.05$, seed $= 42$.\n    - Case $2$: $\\mu = 0.05$, $\\sigma = 0.2$, $T = 1$, $x_0 = 1$, $L = 4$, $M_0 = 8$, pilot count $m = 300$, $\\varepsilon = 0.01$, seed $= 123$.\n    - Case $3$: $\\mu = 0.05$, $\\sigma = 0.05$, $T = 1$, $x_0 = 1$, $L = 3$, $M_0 = 8$, pilot count $m = 200$, $\\varepsilon = 0.05$, seed $= 7$.\n    - Case $4$ (boundary with a single level): $\\mu = 0.1$, $\\sigma = 0.3$, $T = 1$, $x_0 = 1$, $L = 0$, $M_0 = 16$, pilot count $m = 200$, $\\varepsilon = 0.1$, seed $= 99$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The outer list corresponds to the test cases in the order above, and each inner list contains the integer sample allocations $[N_0, N_1, \\dots, N_L]$ for that case. For example, the output must have the form $[[N_0^{(1)},N_1^{(1)},\\dots],[N_0^{(2)},N_1^{(2)},\\dots],\\dots]$.\n\nNo physical units are involved, and all angles (if any appear) are to be interpreted in radians. The final output must be the single line specified above with integer values only.", "solution": "The user has provided a problem that requires the implementation of a Multilevel Monte Carlo (MLMC) parameterization algorithm for a Geometric Brownian Motion (GBM) Stochastic Differential Equation (SDE). The task is to determine the optimal number of samples, $N_\\ell$, for each level $\\ell$ of the MLMC hierarchy, which minimizes the total computational cost for a given target variance.\n\n### **Problem Validation**\n\nFirst, a meticulous validation of the problem statement is performed.\n\n**Step 1: Extract Givens**\n- **SDE Model**: Geometric Brownian Motion, $dX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t$, with initial condition $X_0 = x_0$ over the time interval $[0, T]$.\n- **Payoff Function**: $\\phi(x) = x$.\n- **Numerical Scheme**: Euler–Maruyama, $X_{k+1} = X_k(1 + \\mu h + \\sigma \\Delta W_k)$, where $\\Delta W_k \\sim \\mathcal{N}(0, h)$.\n- **MLMC Hierarchy**:\n    - Discretization levels $\\ell = 0, 1, \\dots, L$.\n    - Number of time steps $M_\\ell = M_0 \\cdot 2^\\ell$.\n    - Time step size $h_\\ell = T / M_\\ell$.\n- **MLMC Estimator Variables**:\n    - $P_\\ell$: Approximation of $\\phi(X_T)$ at level $\\ell$.\n    - $Y_0 = P_0$.\n    - $Y_\\ell = P_\\ell - P_{\\ell-1}$ for $\\ell \\geq 1$.\n- **Strong Coupling**: Coarse-level Brownian increments are constructed by summing a block of $r = 2^{L-\\ell}$ finest-level increments.\n- **Pilot Simulation**:\n    - $m$ pilot samples are used to estimate variance.\n    - Variance estimator: $\\widehat{V}_\\ell = \\operatorname{Var}(Y_\\ell)$ is estimated using the unbiased sample variance formula $\\frac{1}{m-1} \\sum_{i=1}^{m} (Y_\\ell^{(i)} - \\overline{Y}_\\ell)^2$.\n- **Cost Model**:\n    - $C_0 = M_0$.\n    - $C_\\ell = M_\\ell + M_{\\ell-1}$ for $\\ell \\geq 1$.\n- **Optimization Goal**: Minimize total cost $\\sum_{\\ell=0}^L N_\\ell C_\\ell$ subject to the total sampling variance constraint $\\sum_{\\ell=0}^L V_\\ell / N_\\ell = \\varepsilon^2 / 2$.\n- **Special Condition**: If $\\sum_{\\ell=0}^L \\sqrt{V_\\ell C_\\ell} = 0$, then all $N_\\ell$ are set to $0$.\n- **Test Cases**: Four specific sets of parameters ($\\mu, \\sigma, T, x_0, L, M_0, m, \\varepsilon, \\text{seed}$) are provided.\n- **Output Format**: A single line containing a list of lists of integers, e.g., `[[N_0, ...], [N_0, ...]]`.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on cornerstone principles of stochastic calculus, numerical analysis for SDEs, and Monte Carlo methods. The GBM model, Euler-Maruyama scheme, and MLMC framework are standard and rigorously defined.\n- **Well-Posedness**: The problem is well-posed. The objective is to implement a clearly specified algorithm. The optimization problem for finding $N_\\ell$ is a standard constrained problem with a unique analytical solution (prior to integer rounding). The provision of all necessary parameters, including random seeds, ensures that a unique and reproducible output can be obtained.\n- **Objectivity**: The problem is stated in precise, objective, and mathematical terms. There is no ambiguity or subjective language.\n- **Completeness and Consistency**: The problem statement is self-contained. All variables, constants, and functional relationships are explicitly defined. There are no contradictions in the provided information.\n- **Feasibility**: The parameters for the test cases are realistic, and the required computations are feasible within a standard computing environment.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is a well-defined, scientifically sound, and verifiable computational task. The solution can now be developed.\n\n### **Methodology and Implementation**\n\nThe solution proceeds by first deriving the formula for the optimal number of samples $N_\\ell$ and then implementing an algorithm based on this derivation.\n\n**1. Derivation of Optimal Sample Allocation $N_\\ell$**\n\nThe goal is to minimize the total computational cost, $C_{total} = \\sum_{\\ell=0}^L N_\\ell C_\\ell$, subject to a constraint on the total sampling variance, $\\operatorname{Var}[\\hat{Y}] = \\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell} = K$, where $K = \\varepsilon^2 / 2$. This is a classic optimization problem that can be solved using the method of Lagrange multipliers.\n\nThe Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{\\ell=0}^L N_\\ell C_\\ell + \\lambda \\left( \\sum_{\\ell=0}^L \\frac{V_\\ell}{N_\\ell} - K \\right) $$\n\nTaking the partial derivative with respect to each $N_k$ and setting it to zero gives:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\implies N_k^2 = \\lambda \\frac{V_k}{C_k} \\implies N_k = \\sqrt{\\lambda} \\sqrt{\\frac{V_k}{C_k}} $$\nHere, we use the estimated variances $\\widehat{V}_\\ell$ in place of the true variances $V_\\ell$.\n\nTo find the Lagrange multiplier $\\lambda$, we substitute the expression for $N_k$ back into the variance constraint:\n$$ \\sum_{\\ell=0}^L \\frac{\\widehat{V}_\\ell}{\\sqrt{\\lambda} \\sqrt{\\widehat{V}_\\ell/C_\\ell}} = K \\implies \\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^L \\sqrt{\\widehat{V}_\\ell C_\\ell} = K $$\n$$ \\sqrt{\\lambda} = \\frac{1}{K} \\sum_{\\ell=0}^L \\sqrt{\\widehat{V}_\\ell C_\\ell} $$\n\nSubstituting $\\sqrt{\\lambda}$ back into the equation for $N_k$:\n$$ N_k = \\left( \\frac{1}{K} \\sum_{\\ell=0}^L \\sqrt{\\widehat{V}_\\ell C_\\ell} \\right) \\sqrt{\\frac{\\widehat{V}_\\ell}{C_k}} $$\n\nGiven $K = \\varepsilon^2 / 2$, the final formula for $N_k$ is:\n$$ N_k = \\frac{2}{\\varepsilon^2} \\left( \\sum_{\\ell=0}^L \\sqrt{\\widehat{V}_\\ell C_\\ell} \\right) \\sqrt{\\frac{\\widehat{V}_\\ell}{C_k}} $$\n\nSince the number of samples must be an integer, and to ensure the variance constraint is met, we take the ceiling of the result: $N_k = \\lceil \\dots \\rceil$.\n\n**2. Algorithm Implementation**\n\nA Python program is constructed to execute the following steps for each test case:\n\n- **Initialization**: Set up the random number generator with the specified seed for reproducibility.\n- **Pilot Sample Generation**:\n    - For each of the $m$ pilot samples, generate one realization of the Brownian motion path at the finest resolution ($L$). This path consists of $M_L = M_0 \\cdot 2^L$ increments.\n    - For each level $\\ell \\in \\{0, \\dots, L\\}$, simulate the SDE path $X^{(\\ell)}$ using the Euler-Maruyama scheme. The required Brownian increments for level $\\ell$ are obtained by summing consecutive blocks of $2^{L-\\ell}$ increments from the fine-resolution path. This ensures all levels are strongly coupled.\n    - Compute the final values $P_\\ell = X_T^{(\\ell)}$ for all levels.\n    - From the payoffs, calculate the difference values: $Y_0 = P_0$ and $Y_\\ell = P_\\ell - P_{\\ell-1}$ for $\\ell > 0$. Store these $m$ vectors $(Y_0^{(i)}, \\dots, Y_L^{(i)})$.\n- **Variance and Cost Estimation**:\n    - Calculate the estimated variances $\\widehat{V}_\\ell$ using the unbiased sample variance formula on the $m$ pilot samples of $Y_\\ell$.\n    - Calculate the computational costs $C_\\ell$ using the provided formulas: $C_0=M_0$ and $C_\\ell=M_\\ell+M_{\\ell-1}$.\n- **Optimal Allocation Calculation**:\n    - Apply the derived formula for $N_\\ell$ using the computed $\\widehat{V}_\\ell$ and $C_\\ell$.\n    - The final $N_\\ell$ values are rounded up to the nearest integer.\n    - This process is repeated for each test case, and the results are aggregated.\n- **Output Formatting**: The final list of integer allocations for all test cases is formatted into a single string as specified in the problem statement.\n\nThis step-by-step process ensures that all requirements of the problem are met, providing a correct and reproducible solution.", "answer": "```python\nimport numpy as np\n\ndef compute_mlmc_allocations(mu, sigma, T, x0, L, M0, m, epsilon, seed):\n    \"\"\"\n    Computes optimal multilevel Monte Carlo sample allocations N_l.\n\n    This function performs a pilot simulation to estimate the variance and cost\n    at each level of the MLMC hierarchy for a geometric Brownian motion SDE.\n    It then calculates the optimal number of samples N_l per level to minimize\n    total cost for a given target accuracy.\n\n    Args:\n        mu (float): The drift coefficient of the GBM.\n        sigma (float): The volatility coefficient of the GBM.\n        T (float): The time horizon.\n        x0 (float): The initial value of the SDE.\n        L (int): The maximum level for the MLMC hierarchy.\n        M0 (int): The base number of time steps for level 0.\n        m (int): The number of pilot samples for variance estimation.\n        epsilon (float): The target root-mean-square error.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list[int]: A list of integers [N_0, N_1, ..., N_L] representing the\n                   optimal number of samples for each level.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    M_L = M0 * (2**L)\n    h_L = T / M_L\n\n    Y_samples = np.zeros((m, L + 1))\n\n    # Generate m pilot samples\n    for i in range(m):\n        # Generate the single path of finest-level Brownian increments\n        dW_L = rng.normal(0.0, np.sqrt(h_L), size=M_L)\n\n        P_vals = np.zeros(L + 1)\n\n        # Simulate paths and compute payoffs for all levels from the same Brownian path\n        for l in range(L + 1):\n            M_l = M0 * (2**l)\n            h_l = T / M_l\n            step_ratio = 2**(L - l)\n            X_l = float(x0)\n\n            for j in range(M_l):\n                start_idx = j * step_ratio\n                end_idx = (j + 1) * step_ratio\n                sum_dW = np.sum(dW_L[start_idx:end_idx])\n                X_l *= (1.0 + mu * h_l + sigma * sum_dW)\n            \n            P_vals[l] = X_l\n\n        # Compute the Y_l values (P_0, P_1-P_0, ..., P_L-P_{L-1})\n        Y_samples[i, 0] = P_vals[0]\n        for l in range(1, L + 1):\n            Y_samples[i, l] = P_vals[l] - P_vals[l-1]\n\n    # Estimate variances V_l using unbiased sample variance (ddof=1)\n    V_hat = np.var(Y_samples, axis=0, ddof=1)\n    # Ensure variances are non-negative due to potential floating point errors\n    V_hat[np.isnan(V_hat)] = 0.0\n    V_hat = np.maximum(V_hat, 0.0)\n\n    # Calculate costs C_l\n    C = np.zeros(L + 1)\n    M = M0 * (2**np.arange(L + 1))\n    C[0] = M[0]\n    for l in range(1, L + 1):\n        C[l] = M[l] + M[l-1]\n\n    # Calculate optimal number of samples N_l\n    N = np.zeros(L + 1, dtype=int)\n    \n    sqrt_V = np.sqrt(V_hat)\n    sqrt_C = np.sqrt(C)\n    \n    sum_sqrt_VC = np.sum(sqrt_V * sqrt_C)\n    \n    if sum_sqrt_VC > 0:\n        # Variance budget K = epsilon^2 / 2\n        # N_l = (1/K) * sum(sqrt(V_k*C_k)) * sqrt(V_l/C_l)\n        multiplier = (2.0 / epsilon**2) * sum_sqrt_VC\n        with np.errstate(divide='ignore', invalid='ignore'):\n             N_float = multiplier * sqrt_V / sqrt_C\n        \n        N_float[np.isnan(N_float)] = 0.0\n        \n        # Round up to meet the variance constraint\n        N = np.ceil(N_float).astype(int)\n\n    return N.tolist()\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results in the required format.\n    \"\"\"\n    test_cases = [\n        {'mu': 0.05, 'sigma': 0.2, 'T': 1, 'x0': 1, 'L': 3, 'M0': 8, 'm': 200, 'epsilon': 0.05, 'seed': 42},\n        {'mu': 0.05, 'sigma': 0.2, 'T': 1, 'x0': 1, 'L': 4, 'M0': 8, 'm': 300, 'epsilon': 0.01, 'seed': 123},\n        {'mu': 0.05, 'sigma': 0.05, 'T': 1, 'x0': 1, 'L': 3, 'M0': 8, 'm': 200, 'epsilon': 0.05, 'seed': 7},\n        {'mu': 0.1, 'sigma': 0.3, 'T': 1, 'x0': 1, 'L': 0, 'M0': 16, 'm': 200, 'epsilon': 0.1, 'seed': 99}\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = compute_mlmc_allocations(\n            params['mu'], params['sigma'], params['T'], params['x0'],\n            params['L'], params['M0'], params['m'], params['epsilon'], params['seed']\n        )\n        all_results.append(result)\n\n    # Format the output string as [[N0,N1,...],[N0,N1,...],...]\n    # without any spaces within the inner lists.\n    result_strings = []\n    for res_list in all_results:\n        inner_str = \",\".join(map(str, res_list))\n        result_strings.append(f\"[{inner_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3067973"}]}