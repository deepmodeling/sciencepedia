{"hands_on_practices": [{"introduction": "To appreciate the need for higher-order schemes, we must first rigorously understand the limitations of the most basic method. This practice guides you through an analytical derivation of the strong convergence order for the Euler-Maruyama scheme applied to the canonical geometric Brownian motion (GBM) model [@problem_id:3058155]. By calculating the leading-order term of the global mean-square error, you will quantitatively prove why this fundamental method only achieves a strong order of $0.5$.", "problem": "Consider geometric Brownian motion (GBM), defined as the unique strong solution to the scalar stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t=\\mu X_t\\,\\mathrm{d}t+\\sigma X_t\\,\\mathrm{d}W_t,\\qquad X_0=x_0,\n$$\nwhere $W_t$ is a standard Wiener process, and $\\mu,\\sigma,x_0$ are real constants with $\\sigma\\neq 0$. Let $T>0$ be fixed and consider the Euler–Maruyama (EM) method with uniform step size $h>0$ such that $N=T/h\\in\\mathbb{N}$:\n$$\nX_{n+1}^{\\mathrm{EM}}=X_n^{\\mathrm{EM}}+\\mu X_n^{\\mathrm{EM}}\\,h+\\sigma X_n^{\\mathrm{EM}}\\,\\Delta W_n,\\qquad \\Delta W_n:=W_{t_{n+1}}-W_{t_n},\\quad t_n=nh,\\quad X_0^{\\mathrm{EM}}=x_0.\n$$\nUsing only the fundamental properties of the Itô integral, independence and Gaussianity of Wiener increments, and Taylor expansions of smooth functions around zero, derive the leading-order asymptotics (in $h$) of the global mean-square error at time $T$,\n$$\n\\mathbb{E}\\bigl[|X_T-X_N^{\\mathrm{EM}}|^2\\bigr],\n$$\nand show that it is proportional to $h$ for small $h$. Determine explicitly the constant\n$$\nC=\\lim_{h\\to 0}\\frac{1}{h}\\,\\mathbb{E}\\bigl[|X_T-X_N^{\\mathrm{EM}}|^2\\bigr]\n$$\nin closed form in terms of $x_0,\\mu,\\sigma$, and $T$. Your final answer must be the expression for $C$. No rounding is required, and no units are involved. Conclude from your derivation why the Euler–Maruyama method achieves strong order $0.5$ on GBM.", "solution": "This problem asks for the derivation of the leading-order asymptotics of the global mean-square error for the Euler-Maruyama (EM) approximation of a Geometric Brownian Motion (GBM), and the explicit determination of the convergence constant $C$.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard, non-trivial problem in the field of numerical analysis for stochastic differential equations. All necessary information is provided, and the premises are consistent with established mathematical principles. Therefore, the problem is deemed valid.\n\nLet the exact solution to the SDE be $X_t$ and the Euler-Maruyama approximation at time $t_n = n h$ be $X_n^{\\mathrm{EM}}$. The global error at step $n$ is defined as $e_n = X_{t_n} - X_n^{\\mathrm{EM}}$. The initial conditions match, so $e_0 = X_0 - X_0^{\\mathrm{EM}} = x_0 - x_0 = 0$.\n\nWe aim to find a recurrence relation for the mean-square error, $\\epsilon_n = \\mathbb{E}[|e_n|^2] = \\mathbb{E}[e_n^2]$.\nThe error propagates from one step to the next. The error at step $n+1$ is given by\n$$e_{n+1} = X_{t_{n+1}} - X_{n+1}^{\\mathrm{EM}}$$\nTo analyze this, we introduce an auxiliary process, which is a single EM step starting from the exact solution $X_{t_n}$:\n$$\\hat{X}_{n+1} = X_{t_n} + \\mu X_{t_n} h + \\sigma X_{t_n} \\Delta W_n = X_{t_n}(1 + \\mu h + \\sigma \\Delta W_n)$$\nWe can split the error $e_{n+1}$ into two parts:\n$$e_{n+1} = (X_{t_{n+1}} - \\hat{X}_{n+1}) + (\\hat{X}_{n+1} - X_{n+1}^{\\mathrm{EM}})$$\nThe first part is the local truncation error, $L_{n+1} = X_{t_{n+1}} - \\hat{X}_{n+1}$.\nThe second part describes the propagation of the previous error $e_n$:\n$$\\hat{X}_{n+1} - X_{n+1}^{\\mathrm{EM}} = X_{t_n}(1 + \\mu h + \\sigma \\Delta W_n) - X_n^{\\mathrm{EM}}(1 + \\mu h + \\sigma \\Delta W_n) = (X_{t_n} - X_n^{\\mathrm{EM}})(1+\\mu h + \\sigma \\Delta W_n) = e_n(1+\\mu h + \\sigma \\Delta W_n)$$\nThus, the error recurrence is:\n$$e_{n+1} = e_n(1+\\mu h + \\sigma \\Delta W_n) + L_{n+1}$$\nNow, we compute the mean-square error $\\epsilon_{n+1} = \\mathbb{E}[e_{n+1}^2]$, taking the expectation conditional on the filtration $\\mathcal{F}_{t_n}$ at time $t_n$. Since $e_n$ is $\\mathcal{F}_{t_n}$-measurable, and $L_{n+1}$ and $\\Delta W_n$ are independent of $\\mathcal{F}_{t_n}$ (given the SDE's coefficients depend only on $X_t$):\n$$\\mathbb{E}[e_{n+1}^2 | \\mathcal{F}_{t_n}] = \\mathbb{E}\\left[ \\left(e_n(1+\\mu h + \\sigma \\Delta W_n) + L_{n+1}\\right)^2 \\Big| \\mathcal{F}_{t_n} \\right]$$\n$$= \\mathbb{E}\\left[ e_n^2(1+\\mu h + \\sigma \\Delta W_n)^2 + L_{n+1}^2 + 2e_n(1+\\mu h + \\sigma \\Delta W_n)L_{n+1} \\Big| \\mathcal{F}_{t_n} \\right]$$\n$$= e_n^2 \\mathbb{E}[(1+\\mu h + \\sigma \\Delta W_n)^2] + \\mathbb{E}[L_{n+1}^2|\\mathcal{F}_{t_n}] + 2e_n \\mathbb{E}[(1+\\mu h + \\sigma \\Delta W_n)L_{n+1}|\\mathcal{F}_{t_n}]$$\nWe analyze each term separately. Note that $\\Delta W_n \\sim \\mathcal{N}(0, h)$, so $\\mathbb{E}[\\Delta W_n]=0$, $\\mathbb{E}[(\\Delta W_n)^2]=h$, $\\mathbb{E}[(\\Delta W_n)^3]=0$, and $\\mathbb{E}[(\\Delta W_n)^4]=3h^2$.\n\n1.  **Error Propagation Term:**\n    $$\\mathbb{E}[(1+\\mu h + \\sigma \\Delta W_n)^2] = (1+\\mu h)^2 + \\sigma^2 \\mathbb{E}[(\\Delta W_n)^2] = 1+2\\mu h+\\mu^2 h^2 + \\sigma^2 h = 1 + (2\\mu+\\sigma^2)h + O(h^2)$$\n\n2.  **Local Error Variance:** The local error is $L_{n+1} = \\int_{t_n}^{t_{n+1}} (X_s - X_{t_n}) \\mu ds + \\int_{t_n}^{t_{n+1}} (X_s - X_{t_n}) \\sigma dW_s$.\n    We expand $X_s - X_{t_n}$ for $s \\in [t_n, t_{n+1}]$ using an Itô-Taylor expansion:\n    $$X_s - X_{t_n} = \\mu \\int_{t_n}^s X_u du + \\sigma \\int_{t_n}^s X_u dW_u \\approx \\mu X_{t_n} (s-t_n) + \\sigma X_{t_n} (W_s - W_{t_n})$$\n    Substituting this into the expression for $L_{n+1}$ gives terms involving iterated Itô integrals. The leading stochastic term is from the $\\sigma \\int (X_s-X_{t_n})dW_s$ component, specifically $\\sigma^2 X_{t_n} \\int_{t_n}^{t_{n+1}} (W_s-W_{t_n})dW_s$.\n    Using Itô's formula, $\\int_{t_n}^{t_{n+1}} (W_s-W_{t_n})dW_s = \\frac{1}{2}((W_{t_{n+1}}-W_{t_n})^2 - (t_{n+1}-t_n)) = \\frac{1}{2}((\\Delta W_n)^2 - h)$.\n    The other terms in the expansion of $L_{n+1}$ are of higher order in $h$. The dominant term for the mean-square local error is:\n    $$\\mathbb{E}[L_{n+1}^2 | \\mathcal{F}_{t_n}] \\approx \\mathbb{E}\\left[ \\left( \\sigma^2 X_{t_n} \\frac{(\\Delta W_n)^2-h}{2} \\right)^2 \\Big| \\mathcal{F}_{t_n} \\right] = \\frac{1}{4}\\sigma^4 X_{t_n}^2 \\mathbb{E}[((\\Delta W_n)^2-h)^2]$$\n    $$\\mathbb{E}[((\\Delta W_n)^2-h)^2] = \\mathbb{E}[(\\Delta W_n)^4 - 2h(\\Delta W_n)^2 + h^2] = 3h^2 - 2h(h) + h^2 = 2h^2$$\n    Thus, the leading term of the local error variance is:\n    $$\\mathbb{E}[L_{n+1}^2|\\mathcal{F}_{t_n}] = \\frac{1}{2}\\sigma^4 X_{t_n}^2 h^2 + O(h^3)$$\n\n3.  **Cross Term:** $2e_n \\mathbb{E}[(1+\\mu h + \\sigma \\Delta W_n)L_{n+1}|\\mathcal{F}_{t_n}] = 2e_n \\left( (1+\\mu h)\\mathbb{E}[L_{n+1}|\\mathcal{F}_{t_n}] + \\sigma \\mathbb{E}[\\Delta W_n L_{n+1}|\\mathcal{F}_{t_n}] \\right)$.\n    Detailed calculation shows that $\\mathbb{E}[L_{n+1}|\\mathcal{F}_{t_n}]$ and $\\mathbb{E}[\\Delta W_n L_{n+1}|\\mathcal{F}_{t_n}]$ are both of order $O(h^2)$. For instance, $\\mathbb{E}[L_{n+1}|\\mathcal{F}_{t_n}] = X_{t_n}(\\exp(\\mu h)-1-\\mu h) = \\frac{1}{2}\\mu^2 X_{t_n} h^2 + O(h^3)$. This makes the entire cross term of order $e_n X_{t_n} O(h^2)$. Taking full expectation and using Cauchy-Schwarz inequality, $\\mathbb{E}[e_n X_{t_n}] \\le \\sqrt{\\mathbb{E}[e_n^2]\\mathbb{E}[X_{t_n}^2]} = \\sqrt{\\epsilon_n \\mathbb{E}[X_{t_n}^2]}$. Since we expect $\\epsilon_n = O(h)$, this term's contribution is $O(h^{2.5})$, which is negligible compared to the $O(h^2)$ terms.\n\nCombining the main terms and taking the full expectation, we get the recurrence for $\\epsilon_n=\\mathbb{E}[e_n^2]$:\n$$\\epsilon_{n+1} = \\epsilon_n(1 + (2\\mu+\\sigma^2)h) + \\frac{1}{2}\\sigma^4 \\mathbb{E}[X_{t_n}^2]h^2 + O(h^{2.5})$$\nTo solve this, we find the second moment of the exact solution:\n$$d(X_t^2) = (2\\mu X_t^2 + \\sigma^2 X_t^2)dt + 2\\sigma X_t^2 dW_t$$\nTaking expectations, $\\frac{d}{dt}\\mathbb{E}[X_t^2] = (2\\mu+\\sigma^2)\\mathbb{E}[X_t^2]$. With $\\mathbb{E}[X_0^2]=x_0^2$, the solution is:\n$$\\mathbb{E}[X_t^2] = x_0^2 \\exp((2\\mu+\\sigma^2)t)$$\nSubstituting this into the recurrence for $\\epsilon_n$ at $t=t_n$:\n$$\\epsilon_{n+1} \\approx \\epsilon_n(1 + (2\\mu+\\sigma^2)h) + \\frac{1}{2}\\sigma^4 x_0^2 \\exp((2\\mu+\\sigma^2)t_n)h^2$$\nLet $\\epsilon(t)$ be a continuous function interpolating $\\epsilon_n$ at $t_n=nh$. For small $h$, this recurrence relation can be approximated by a first-order ordinary differential equation (ODE). Let $\\epsilon(t) \\approx \\epsilon_n$.\n$$\\frac{\\epsilon(t+h) - \\epsilon(t)}{h} \\approx (2\\mu+\\sigma^2)\\epsilon(t) + \\frac{1}{2}\\sigma^4 x_0^2 \\exp((2\\mu+\\sigma^2)t)h$$\nThis suggests that $\\epsilon(t)$ is of order $h$. Let's set $\\epsilon(t) = c(t)h$. Substituting this ansatz into the ODE:\n$$\\frac{c(t+h)h - c(t)h}{h} \\approx (2\\mu+\\sigma^2)c(t)h + \\frac{1}{2}\\sigma^4 x_0^2 \\exp((2mu+\\sigma^2)t)h$$\nDividing by $h$:\n$$\\frac{c(t+h)-c(t)}{h} \\approx (2\\mu+\\sigma^2)c(t) + \\frac{1}{2}\\sigma^4 x_0^2 \\exp((2\\mu+\\sigma^2)t)$$\nIn the limit $h\\to 0$, this becomes the ODE for the proportionality factor $c(t)$:\n$$c'(t) = (2\\mu+\\sigma^2)c(t) + \\frac{1}{2}\\sigma^4 x_0^2 \\exp((2\\mu+\\sigma^2)t)$$\nThe initial condition is $\\epsilon_0=0$, which implies $c(0)=0$. Let $A = 2\\mu+\\sigma^2$. The ODE is $c'(t) = Ac(t) + \\frac{1}{2}\\sigma^4 x_0^2 e^{At}$.\nWe solve this using an integrating factor $e^{-At}$:\n$$(c(t)e^{-At})' = \\frac{1}{2}\\sigma^4 x_0^2 e^{At} e^{-At} = \\frac{1}{2}\\sigma^4 x_0^2$$\nIntegrating from $0$ to $T$:\n$$c(T)e^{-AT} - c(0)e^0 = \\int_0^T \\frac{1}{2}\\sigma^4 x_0^2 ds = \\frac{1}{2}\\sigma^4 x_0^2 T$$\nSince $c(0)=0$, we have:\n$$c(T) = \\frac{1}{2}\\sigma^4 x_0^2 T e^{AT} = \\frac{1}{2}\\sigma^4 x_0^2 T \\exp((2\\mu+\\sigma^2)T)$$\nThe global mean-square error at time $T$ has the leading-order asymptotic behavior:\n$$\\mathbb{E}[|X_T - X_N^{\\mathrm{EM}}|^2] = \\epsilon_N \\approx c(T)h = \\left(\\frac{1}{2}\\sigma^4 x_0^2 T \\exp((2\\mu+\\sigma^2)T)\\right)h$$\nThis shows that the error is proportional to $h$ for small $h$. The constant $C$ is the limit of the error divided by $h$:\n$$C = \\lim_{h\\to 0}\\frac{1}{h}\\,\\mathbb{E}\\bigl[|X_T-X_N^{\\mathrm{EM}}|^2\\bigr] = c(T) = \\frac{1}{2}\\sigma^4 x_0^2 T \\exp((2\\mu+\\sigma^2)T)$$\nThe strong order of convergence, denoted by $p$, is defined by the inequality $\\mathbb{E}[|X_T - X_N^{\\mathrm{EM}}|] \\le K h^p$ for some constant $K$. By Jensen's inequality (or Cauchy-Schwarz):\n$$(\\mathbb{E}[|X_T - X_N^{\\mathrm{EM}}|])^2 \\le \\mathbb{E}[|X_T - X_N^{\\mathrm{EM}}|^2] \\approx Ch$$\nTaking the square root of both sides:\n$$\\mathbb{E}[|X_T - X_N^{\\mathrm{EM}}|] \\le \\sqrt{C h + O(h^2)} = \\sqrt{C}\\sqrt{h} + O(h)$$\nThis implies that the strong order of convergence is $p = 1/2 = 0.5$.", "answer": "$$\\boxed{\\frac{1}{2} T x_0^2 \\sigma^4 \\exp\\left(\\left(2\\mu + \\sigma^2\\right)T\\right)}$$", "id": "3058155"}, {"introduction": "Having established that the Euler-Maruyama method is limited to strong order $0.5$, we now turn to improving it. The Milstein method is the first and most fundamental higher-order strong scheme, which arises from including one additional term from the Itô-Taylor expansion [@problem_id:3058125]. This exercise challenges you to derive the Milstein scheme from first principles for GBM and show precisely how it cancels the dominant error term of the Euler-Maruyama method, thereby achieving a full strong order of $1$.", "problem": "Consider the scalar geometric Brownian motion (GBM) defined by the stochastic differential equation\n$$\ndX_{t} = \\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t}, \\quad X_{0} = x_{0} > 0,\n$$\nwith constants $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$, and where $W_{t}$ is a standard Wiener process. Let $h>0$ be a fixed time step and let $\\Delta W := W_{t+h} - W_{t}$ denote the Brownian increment over a single step. The Milstein method applied to a scalar Itô stochastic differential equation with diffusion coefficient $g(x)$ augments the Euler–Maruyama method by a term involving the spatial derivative $g'(x)$, and in the present case $g(x) = \\sigma x$ and $g'(x) = \\sigma$.\n\nYour tasks are as follows.\n\n1) Using only Itô's formula and fundamental properties of Brownian motion, derive an exact expression for $X_{t+h}$ in terms of $X_{t}$, $h$, and $\\Delta W$. Then, expand your expression as a stochastic series up to and including all terms whose $L^{2}$-size is of order $h$ (that is, retain all contributions built from $h$ and $\\Delta W$ whose mean-square size is at least of order $h^{2}$). Express your result in the form\n$$\nX_{t+h} = X_{t} + A_{1}(X_{t})\\,h + A_{2}(X_{t})\\,\\Delta W + A_{3}(X_{t})\\big((\\Delta W)^{2} - h\\big) + R,\n$$\nwhere $R$ is a remainder term you must characterize in $L^{2}$.\n\n2) Write the one-step Milstein update for this GBM over the step $[t, t+h]$ using only $\\mu$, $\\sigma$, $X_{t}$, $h$, and $\\Delta W$, and match the coefficients $A_{1}(X_{t})$, $A_{2}(X_{t})$, and $A_{3}(X_{t})$ from your expansion in part 1). Deduce an explicit expression for the local defect\n$$\nD := \\big(X_{t+h} - X_{t}\\big) \\;-\\; \\Big[ \\mu X_{t}\\,h + \\sigma X_{t}\\,\\Delta W + \\tfrac{1}{2}\\sigma^{2} X_{t}\\,\\big((\\Delta W)^{2} - h\\big) \\Big],\n$$\nand prove that its conditional mean-square scales as\n$$\n\\mathbb{E}\\!\\left[\\,|D|^{2} \\,\\big|\\, \\mathcal{F}_{t}\\right] = \\mathcal{O}\\!\\left(h^{3}\\right),\n$$\nwhere $\\mathcal{F}_{t}$ is the filtration of the Wiener process up to time $t$.\n\n3) Using the standard definition of strong convergence (mean-square root error) and the usual stability and consistency arguments for Lipschitz coefficients, conclude the global strong convergence order $p$ achieved by the Milstein method for GBM on a fixed time interval. Report only the value of $p$ as your final answer, as a single number without units. No rounding is needed.", "solution": "The problem requires an analysis of the Milstein method applied to the geometric Brownian motion (GBM) stochastic differential equation (SDE). The analysis proceeds in three parts: deriving and expanding the exact solution, analyzing the one-step numerical method and its local error, and concluding the global strong convergence order.\n\n**Part 1: Exact Solution and Stochastic Taylor Expansion**\n\nThe SDE for geometric Brownian motion is given by\n$$\ndX_{t} = \\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t}, \\quad X_{0} = x_{0} > 0\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants, and $W_{t}$ is a standard Wiener process.\n\nTo find the exact solution for $X_{t}$, we apply Itô's formula to the function $f(x) = \\ln(x)$. The partial derivatives are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. According to Itô's lemma, the differential of $Y_{t} = \\ln(X_{t})$ is:\n$$\ndY_{t} = d(\\ln X_{t}) = f'(X_{t})\\,dX_{t} + \\frac{1}{2}f''(X_{t})\\,(dX_{t})^{2}\n$$\nThe quadratic variation term $(dX_{t})^{2}$ is calculated using the Itô rules $dt \\cdot dt = 0$, $dt \\cdot dW_{t} = 0$, and $dW_{t} \\cdot dW_{t} = dt$:\n$$\n(dX_{t})^{2} = (\\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t})^{2} = \\sigma^{2} X_{t}^{2}\\,(dW_{t})^{2} = \\sigma^{2} X_{t}^{2}\\,dt\n$$\nSubstituting $dX_{t}$ and $(dX_{t})^{2}$ into the expression for $dY_{t}$:\n$$\nd(\\ln X_{t}) = \\frac{1}{X_{t}}(\\mu X_{t}\\,dt + \\sigma X_{t}\\,dW_{t}) + \\frac{1}{2}\\left(-\\frac{1}{X_{t}^{2}}\\right)(\\sigma^{2} X_{t}^{2}\\,dt)\n$$\n$$\nd(\\ln X_{t}) = (\\mu\\,dt + \\sigma\\,dW_{t}) - \\frac{1}{2}\\sigma^{2}\\,dt = \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)dt + \\sigma\\,dW_{t}\n$$\nIntegrating this SDE from time $t$ to $t+h$:\n$$\n\\int_{t}^{t+h} d(\\ln X_{s}) = \\int_{t}^{t+h} \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)ds + \\int_{t}^{t+h} \\sigma\\,dW_{s}\n$$\n$$\n\\ln(X_{t+h}) - \\ln(X_{t}) = \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\sigma(W_{t+h} - W_{t})\n$$\nDefining the Brownian increment $\\Delta W := W_{t+h} - W_{t}$, we obtain:\n$$\n\\ln\\left(\\frac{X_{t+h}}{X_{t}}\\right) = \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\sigma\\,\\Delta W\n$$\nExponentiating both sides gives the exact expression for $X_{t+h}$:\n$$\nX_{t+h} = X_{t} \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\sigma\\,\\Delta W \\right)\n$$\nNext, we expand this expression as a stochastic series. Let $Z = (\\mu - \\frac{1}{2}\\sigma^{2})h + \\sigma\\,\\Delta W$. Using the Taylor series for the exponential function, $\\exp(Z) = 1 + Z + \\frac{Z^2}{2!} + \\frac{Z^3}{3!} + \\dots$, we have:\n$$\nX_{t+h} = X_{t} \\left( 1 + Z + \\frac{1}{2}Z^{2} + \\frac{1}{6}Z^{3} + \\dots \\right)\n$$\nWe need to retain all terms whose $L^2$-size is of order $h$ or larger. The fundamental scalings are $\\mathbb{E}[h^k] = h^k$ and $\\mathbb{E}[(\\Delta W)^{2k}] = \\frac{(2k)!}{2^k k!} h^k$. This implies that $h$ is of size $\\mathcal{O}(h)$ and $\\Delta W$ is of size $\\mathcal{O}(h^{1/2})$ in $L^2$. Therefore, $(\\Delta W)^2$ is of size $\\mathcal{O}(h)$. Terms like $h\\Delta W$ and $(\\Delta W)^3$ are of size $\\mathcal{O}(h^{3/2})$ and will be part of the remainder term.\n\nLet's expand $Z$ and $Z^2$:\n$$\nZ = \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\sigma\\,\\Delta W\n$$\n$$\nZ^{2} = \\sigma^{2}(\\Delta W)^{2} + 2\\sigma\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h\\,\\Delta W + \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)^{2}h^{2}\n$$\nSubstituting these into the expansion for $X_{t+h}$:\n$$\nX_{t+h} = X_{t} \\left[ 1 + \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\sigma\\,\\Delta W + \\frac{1}{2}\\left(\\sigma^{2}(\\Delta W)^{2} + \\mathcal{O}(h^{3/2})\\right) + \\mathcal{O}(h^{3/2}) \\right]\n$$\n$$\nX_{t+h} = X_{t} + X_{t}\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + X_{t}\\sigma\\,\\Delta W + \\frac{1}{2}X_{t}\\sigma^{2}(\\Delta W)^{2} + R_0\n$$\nwhere $R_0$ contains all terms of $L^2$-size $\\mathcal{O}(h^{3/2})$ or smaller. To match the requested form, we add and subtract the term $\\frac{1}{2}X_{t}\\sigma^{2}h$:\n$$\nX_{t+h} = X_{t} + X_{t}\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h + \\frac{1}{2}X_{t}\\sigma^{2}h + X_{t}\\sigma\\,\\Delta W + \\frac{1}{2}X_{t}\\sigma^{2}\\left((\\Delta W)^{2} - h\\right) + R_0\n$$\n$$\nX_{t+h} = X_{t} + (\\mu X_{t})h + (\\sigma X_{t})\\Delta W + \\left(\\frac{1}{2}\\sigma^{2}X_{t}\\right)\\left((\\Delta W)^{2} - h\\right) + R\n$$\nwhere $R=R_0$ is the remainder. The leading contributions to $R$ come from the term $\\frac{1}{2} X_t \\cdot (2\\sigma(\\mu - \\frac{1}{2}\\sigma^2)h\\Delta W)$ from the expansion of $\\frac{1}{2}Z^2$ and the term $\\frac{1}{6}X_t \\cdot (\\sigma^3 (\\Delta W)^3)$ from $\\frac{1}{6}Z^3$. The mean-square of these terms scales as $\\mathbb{E}[(h\\Delta W)^2] = h^2 \\mathbb{E}[(\\Delta W)^2] = h^3$ and $\\mathbb{E}[((\\Delta W)^3)^2] = \\mathbb{E}[(\\Delta W)^6] = 15h^3$. Thus, the remainder $R$ has a conditional mean-square size of $\\mathcal{O}(h^3)$, i.e., $\\mathbb{E}[|R|^2|\\mathcal{F}_t] = \\mathcal{O}(h^3)$.\n\nFrom this expansion, we identify the coefficients:\n$A_{1}(X_{t}) = \\mu X_{t}$\n$A_{2}(X_{t}) = \\sigma X_{t}$\n$A_{3}(X_{t}) = \\frac{1}{2}\\sigma^{2} X_{t}$\n\n**Part 2: Milstein Method and Local Defect**\n\nThe general one-step Milstein update for an SDE $dX_{t} = a(X_{t})dt + b(X_{t})dW_{t}$ is given by:\n$$\nY_{n+1} = Y_{n} + a(Y_{n})h + b(Y_{n})\\Delta W_{n} + \\frac{1}{2}b(Y_{n})b'(Y_{n})\\left((\\Delta W_{n})^{2} - h\\right)\n$$\nFor the GBM SDE, we have $a(x) = \\mu x$ and $b(x) = \\sigma x$. The derivative of the diffusion coefficient is $b'(x) = \\sigma$.\nSubstituting these into the Milstein formula, the one-step update for GBM is:\n$$\nX_{t+h}^{\\text{Milstein}} = X_{t} + (\\mu X_{t})h + (\\sigma X_{t})\\Delta W + \\frac{1}{2}(\\sigma X_{t})(\\sigma)\\left((\\Delta W)^{2} - h\\right)\n$$\n$$\nX_{t+h}^{\\text{Milstein}} = X_{t} + \\mu X_{t}h + \\sigma X_{t}\\Delta W + \\frac{1}{2}\\sigma^{2}X_{t}\\left((\\Delta W)^{2} - h\\right)\n$$\nComparing this with the form $X_{t} + A_{1}(X_{t})h + A_{2}(X_{t})\\Delta W + A_{3}(X_{t})((\\Delta W)^{2} - h)$, we confirm the coefficients found in Part 1:\n$A_{1}(X_{t}) = \\mu X_{t}$\n$A_{2}(X_{t}) = \\sigma X_{t}$\n$A_{3}(X_{t}) = \\frac{1}{2}\\sigma^{2} X_{t}$\n\nThe local defect $D$ is defined as the difference between the exact increment and the numerical increment provided by the specified formula:\n$$\nD := (X_{t+h} - X_{t}) - \\left[ \\mu X_{t}h + \\sigma X_{t}\\Delta W + \\frac{1}{2}\\sigma^{2} X_{t}\\left((\\Delta W)^{2} - h\\right) \\right]\n$$\nThis is precisely the remainder term $R$ from our expansion in Part 1, representing the local truncation error $X_{t+h} - X_{t+h}^{\\text{Milstein}}$.\n$$\nD = X_{t+h} - X_{t+h}^{\\text{Milstein}}\n$$\nTo prove that $\\mathbb{E}[|D|^2 | \\mathcal{F}_{t}] = \\mathcal{O}(h^3)$, we analyze the higher-order terms in the expansion of $X_{t+h}$.\n$$\nX_{t+h} = X_{t}\\exp(Z) = X_{t}\\left(1 + Z + \\frac{Z^2}{2} + \\frac{Z^3}{6} + \\mathcal{O}(Z^4)\\right)\n$$\n$$\nX_{t+h}^{\\text{Milstein}} = X_t \\left( 1 + (\\mu - \\frac{1}{2}\\sigma^2)h + \\sigma \\Delta W + \\frac{1}{2}\\sigma^2 (\\Delta W)^2 \\right) = X_t \\left(1 + Z + \\frac{1}{2}(\\sigma \\Delta W)^2 - \\frac{1}{2} Z^2 + \\frac{1}{2} Z^2\\right) - \\dots\n$$\nA more direct subtraction gives:\n$$\nD = X_{t} \\exp(Z) - X_t \\left( 1 + (\\mu - \\frac{1}{2}\\sigma^2)h + \\sigma \\Delta W + \\frac{1}{2}\\sigma^2 (\\Delta W)^2 \\right)\n$$\n$D/X_t = (1 + Z + \\frac{1}{2}Z^2 + \\frac{1}{6}Z^3 + \\dots) - (1 + (\\mu - \\frac{1}{2}\\sigma^2)h + \\sigma \\Delta W + \\frac{1}{2}\\sigma^2 (\\Delta W)^2)$.\nThe terms $1$, $(\\mu - \\frac{1}{2}\\sigma^2)h$, and $\\sigma \\Delta W$ from $Z$ cancel.\nThe term $\\frac{1}{2}\\sigma^2 (\\Delta W)^2$ from $\\frac{1}{2}Z^2$ also cancels.\nThe leading terms remaining in $D/X_t$ are the other terms from $\\frac{1}{2}Z^2$ and the leading term from $\\frac{1}{6}Z^3$:\n$$\nD = X_t \\left( \\frac{1}{2} \\left[ 2\\sigma\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h\\,\\Delta W \\right] + \\frac{1}{6} \\left[ \\sigma^3 (\\Delta W)^3 \\right] + \\text{h.o.t.} \\right)\n$$\n$$\nD = X_t \\left( \\sigma\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h\\,\\Delta W + \\frac{1}{6}\\sigma^3(\\Delta W)^3 + \\text{h.o.t.} \\right)\n$$\nwhere h.o.t. stands for higher-order terms in powers of $h$ and $\\Delta W$. The conditional mean-square of $D$ is:\n$$\n\\mathbb{E}[|D|^2 | \\mathcal{F}_{t}] = X_t^2 \\mathbb{E}\\left[ \\left| \\sigma\\left(\\mu - \\frac{1}{2}\\sigma^{2}\\right)h\\,\\Delta W + \\frac{1}{6}\\sigma^3(\\Delta W)^3 + \\dots \\right|^2 \\right]\n$$\nsince $\\Delta W$ is independent of $\\mathcal{F}_t$. The leading term in the expectation will be of order $\\mathbb{E}[(h\\Delta W)^2] \\sim h^3$, $\\mathbb{E}[(\\Delta W)^6] \\sim h^3$, and $\\mathbb{E}[(h\\Delta W)(\\Delta W)^3] = h\\mathbb{E}[(\\Delta W)^4] \\sim h^3$. Specifically:\n$$\n\\mathbb{E}[|D|^2 | \\mathcal{F}_{t}] = X_t^2 \\left( C_1 h^3 + C_2 h^3 + C_3 h^3 + \\dots \\right) = \\mathcal{O}(h^3)\n$$\nwhere $C_1, C_2, C_3$ are constants depending on $\\mu$ and $\\sigma$. This proves that the conditional mean-square of the local defect scales as $\\mathcal{O}(h^3)$.\n\n**Part 3: Global Strong Convergence Order**\n\nThe strong order of convergence, $p$, of a numerical method for an SDE is determined by the rate at which the mean-square error at a fixed final time $T$ converges to zero as the step size $h \\to 0$. A standard result in the numerical analysis of SDEs states that if a one-step method has a local mean-square error that satisfies\n$$\n\\mathbb{E}\\left[|X_{t+h} - Y_{t+h}|^2 | \\mathcal{F}_t\\right] = \\mathcal{O}(h^{2p+1})\n$$\nand the coefficients of the SDE satisfy certain regularity conditions (such as global Lipschitz continuity), then the global strong order of convergence is $p$. That is,\n$$\n\\left(\\mathbb{E}\\left[|X_T - Y_{N_T}|^2\\right]\\right)^{1/2} = \\mathcal{O}(h^p)\n$$\nwhere $Y_{N_T}$ is the numerical approximation at time $T=N_T h$.\n\nThe problem states we should use the usual arguments for Lipschitz coefficients. The drift $a(x)=\\mu x$ and diffusion $b(x)=\\sigma x$ for GBM are indeed globally Lipschitz functions, so the standard stability and convergence theorems apply.\n\nFrom Part 2, we proved that the local defect $D$ (which is the local error $X_{t+h} - X_{t+h}^{\\text{Milstein}}$) has a conditional mean-square error of $\\mathcal{O}(h^3)$.\n$$\n\\mathbb{E}[|D|^2 | \\mathcal{F}_{t}] = \\mathcal{O}(h^3)\n$$\nBy equating this to the general form $\\mathcal{O}(h^{2p+1})$, we can determine the order $p$:\n$$\n2p+1 = 3\n$$\n$$\n2p = 2\n$$\n$$\np = 1\n$$\nTherefore, the Milstein method for geometric Brownian motion achieves a global strong convergence order of $p=1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3058125"}, {"introduction": "The principle of matching an Itô-Taylor expansion is not limited to the Milstein method; it is the foundation for designing entire families of high-order integrators. This final exercise introduces the powerful framework of Stochastic Runge-Kutta (SRK) methods, which offer a systematic way to construct schemes of a desired accuracy [@problem_id:3058176]. By determining the coefficients for a specific two-stage SRK scheme, you will gain insight into the general procedure for engineering numerical methods that cancel out lower-order error terms.", "problem": "Consider the scalar Itô stochastic differential equation (SDE)\n$$\ndX_t \\;=\\; a(X_t)\\,dt \\;+\\; b(X_t)\\,dW_t,\n$$\nwhere $a:\\mathbb{R}\\to\\mathbb{R}$ and $b:\\mathbb{R}\\to\\mathbb{R}$ are three-times continuously differentiable with globally bounded derivatives, and $W_t$ is a standard Wiener process. Let $h>0$ be a fixed time step, let $t_n = nh$, and denote $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$.\n\nDefine a two-stage explicit stochastic Runge–Kutta (SRK) method as follows:\n$$\nY_n \\;=\\; X_n \\;+\\; \\alpha\\, h\\, a(X_n) \\;+\\; \\beta\\, b(X_n)\\,\\Delta W_n,\n$$\n$$\nX_{n+1} \\;=\\; X_n \\;+\\; h\\,a(Y_n) \\;+\\; \\Delta W_n\\, b(Y_n) \\;+\\; \\gamma\\, b(X_n)\\,b'(X_n)\\,\\big(\\Delta W_n^2 - h\\big),\n$$\nwhere $\\alpha,\\beta,\\gamma\\in\\mathbb{R}$ are fixed coefficients, and $b'$ denotes the derivative of $b$.\n\nStarting from the Itô–Taylor expansion up to terms of mean-square order $h^{3/2}$, derive the local expansion of both the exact solution and the SRK scheme, and determine the values of the coefficients $\\alpha$, $\\beta$, and $\\gamma$ such that all local error terms of mean-square order strictly less than $h^{3/2}$ cancel. In other words, choose $\\alpha$, $\\beta$, and $\\gamma$ so that the first nonzero term in the local mean-square error is of order $h^{3/2}$.\n\nExpress your final answer as the ordered triple $(\\alpha,\\beta,\\gamma)$ written as a row vector. No rounding is required.", "solution": "The problem requires determining the coefficients $\\alpha$, $\\beta$, and $\\gamma$ for a given two-stage stochastic Runge-Kutta (SRK) method such that its local expansion matches the Itô-Taylor expansion of the exact solution, canceling all local error terms with a mean-square order strictly less than $h^{3/2}$.\n\nLet the scalar Itô stochastic differential equation (SDE) be\n$$\ndX_t = a(X_t) dt + b(X_t) dW_t, \\quad X_{t_n} = X_n.\n$$\nThe functions $a$ and $b$ are assumed to be sufficiently smooth. For brevity, we use the notation $a_n = a(X_n)$, $b_n = b(X_n)$, and primes to denote derivatives with respect to the spatial variable, e.g., $a'_n = a'(X_n)$, $b'_n = b'(X_n)$.\n\n**1. Itô-Taylor Expansion of the Exact Solution**\n\nWe need to expand the exact solution $X_{t_{n+1}}$ around $X_n$ up to all terms with a mean-square order less than $h^{3/2}$. The solution can be written in integral form as\n$$\nX_{t_{n+1}} = X_n + \\int_{t_n}^{t_{n+1}} a(X_s) ds + \\int_{t_n}^{t_{n+1}} b(X_s) dW_s.\n$$\nWe expand the integrands $a(X_s)$ and $b(X_s)$ around $X_n$. For $s \\in [t_n, t_{n+1}]$, the leading term in the expansion of $X_s$ is $X_s \\approx X_n + a_n(s-t_n) + b_n(W_s - W_{t_n})$.\n\nFor the drift term integral:\n$$\n\\int_{t_n}^{t_{n+1}} a(X_s) ds = \\int_{t_n}^{t_{n+1}} \\left( a_n + a'_n(X_s-X_n) + \\frac{1}{2}a''_n(X_s-X_n)^2 + \\dots \\right) ds.\n$$\nSubstituting the approximation for $X_s-X_n$ and keeping terms with mean-square order less than $h^{3/2}$:\n$$\n\\int_{t_n}^{t_{n+1}} a(X_s) ds = \\int_{t_n}^{t_{n+1}} \\left( a_n + a'_n(a_n(s-t_n) + \\dots) + \\dots \\right) ds = a_n h + \\frac{1}{2} a_n a'_n h^2 + O_{ms}(h^{5/2}).\n$$\nThe term $a_n h$ has order $h$, and the term $\\frac{1}{2} a_n a'_n h^2$ has order $h^2$. Both are of mean-square order less than $h^{3/2}$ for $h<1$.\n\nFor the diffusion term integral:\n$$\n\\int_{t_n}^{t_{n+1}} b(X_s) dW_s = \\int_{t_n}^{t_{n+1}} \\left( b_n + b'_n(X_s-X_n) + \\frac{1}{2}b''_n(X_s-X_n)^2 + \\dots \\right) dW_s.\n$$\nSubstituting for $X_s - X_n$ and evaluating the stochastic integrals:\n$$\n\\int_{t_n}^{t_{n+1}} b(X_s) dW_s = b_n \\int_{t_n}^{t_{n+1}} dW_s + b'_n \\int_{t_n}^{t_{n+1}} (a_n(s-t_n) + b_n(W_s-W_{t_n})) dW_s + \\dots\n$$\n$$\n= b_n \\Delta W_n + a_n b'_n \\int_{t_n}^{t_{n+1}} (s-t_n) dW_s + b_n b'_n \\int_{t_n}^{t_{n+1}} (W_s-W_{t_n}) dW_s + \\dots\n$$\nThe first term is $b_n \\Delta W_n$. The second integral, $\\int (s-t_n) dW_s$, has mean-square order $h^{3/2}$. The third integral is a standard Itô integral, $\\int (W_s-W_{t_n}) dW_s = \\frac{1}{2}((\\Delta W_n)^2 - h)$. This term has mean-square order $h$.\nSo, retaining terms with mean-square order less than $h^{3/2}$:\n$$\n\\int_{t_n}^{t_{n+1}} b(X_s) dW_s = b_n \\Delta W_n + \\frac{1}{2} b_n b'_n ((\\Delta W_n)^2 - h) + O_{ms}(h^{3/2}).\n$$\nCombining the drift and diffusion parts, the expansion of the exact solution is:\n$$\nX_{t_{n+1}} - X_n = a_n h + b_n \\Delta W_n + \\frac{1}{2} a_n a'_n h^2 + \\frac{1}{2} b_n b'_n ((\\Delta W_n)^2 - h) + R_{exact},\n$$\nwhere $R_{exact}$ contains all terms of mean-square order $h^{3/2}$ and higher. We can rewrite this as:\n$$\nX_{t_{n+1}} - X_n = a_n h + b_n \\Delta W_n + \\frac{1}{2} a_n a'_n h^2 + \\frac{1}{2} b_n b'_n (\\Delta W_n)^2 - \\frac{1}{2} b_n b'_n h + R_{exact}.\n$$\n\n**2. Expansion of the SRK Method**\n\nThe given SRK method is:\n$$\nY_n = X_n + \\alpha h a_n + \\beta b_n \\Delta W_n\n$$\n$$\nX_{n+1} = X_n + h a(Y_n) + \\Delta W_n b(Y_n) + \\gamma b_n b'_n (\\Delta W_n^2 - h).\n$$\nLet $\\delta X_n = Y_n - X_n = \\alpha h a_n + \\beta b_n \\Delta W_n$. We expand $a(Y_n)$ and $b(Y_n)$ in a Taylor series around $X_n$:\n$$\na(Y_n) = a_n + a'_n \\delta X_n + \\dots = a_n + a'_n (\\alpha h a_n + \\beta b_n \\Delta W_n) + \\dots\n$$\n$$\nb(Y_n) = b_n + b'_n \\delta X_n + \\dots = b_n + b'_n (\\alpha h a_n + \\beta b_n \\Delta W_n) + \\dots\n$$\nNow we substitute these into the expression for $X_{n+1}$ and collect terms with mean-square order less than $h^{3/2}$:\n$$\nX_{n+1} - X_n = h \\left( a_n + a'_n (\\alpha h a_n + \\beta b_n \\Delta W_n) + \\dots \\right) + \\Delta W_n \\left( b_n + b'_n (\\alpha h a_n + \\beta b_n \\Delta W_n) + \\dots \\right) + \\gamma b_n b'_n ((\\Delta W_n)^2 - h).\n$$\nLet's expand and identify the orders of the resulting terms:\n- $h a_n$: m-s order $h$.\n- $h \\cdot a'_n \\alpha h a_n = \\alpha a_n a'_n h^2$: m-s order $h^2$.\n- $h \\cdot a'_n \\beta b_n \\Delta W_n = \\beta a'_n b_n h \\Delta W_n$: m-s order $h^{3/2}$.\n- $\\Delta W_n \\cdot b_n = b_n \\Delta W_n$: m-s order $h^{1/2}$.\n- $\\Delta W_n \\cdot b'_n \\alpha h a_n = \\alpha a_n b'_n h \\Delta W_n$: m-s order $h^{3/2}$.\n- $\\Delta W_n \\cdot b'_n \\beta b_n \\Delta W_n = \\beta b_n b'_n (\\Delta W_n)^2$: m-s order $h$.\n- $\\gamma b_n b'_n ((\\Delta W_n)^2 - h)$: m-s order $h$.\n\nThe expansion of the numerical scheme, keeping only terms with mean-square order strictly less than $h^{3/2}$, is:\n$$\nX_{n+1} - X_n = h a_n + \\alpha a_n a'_n h^2 + b_n \\Delta W_n + \\beta b_n b'_n (\\Delta W_n)^2 + \\gamma b_n b'_n ((\\Delta W_n)^2 - h) + R_{num}.\n$$\nWe can rewrite this as:\n$$\nX_{n+1} - X_n = h a_n + \\alpha a_n a'_n h^2 + b_n \\Delta W_n + (\\beta + \\gamma) b_n b'_n (\\Delta W_n)^2 - \\gamma b_n b'_n h + R_{num}.\n$$\n\n**3. Matching Coefficients**\n\nTo ensure all local error terms of mean-square order strictly less than $h^{3/2}$ cancel, we must match the coefficients of the terms in the expansions for the exact solution and the numerical scheme. The basis terms are linearly independent functions of the random variable $\\Delta W_n$ and parameters like $h$.\n$$\n\\text{Exact}: a_n h + b_n \\Delta W_n + \\frac{1}{2} a_n a'_n h^2 + \\frac{1}{2} b_n b'_n (\\Delta W_n)^2 - \\frac{1}{2} b_n b'_n h\n$$\n$$\n\\text{Scheme}: a_n h + b_n \\Delta W_n + \\alpha a_n a'_n h^2 + (\\beta + \\gamma) b_n b'_n (\\Delta W_n)^2 - \\gamma b_n b'_n h\n$$\nComparing the coefficients for each distinct term:\n- Coefficient of $a_n a'_n h^2$: For this to hold for arbitrary functions $a$, we must have\n$$\n\\alpha = \\frac{1}{2}.\n$$\n- Coefficient of $b_n b'_n h$: For this to hold for arbitrary functions $b$, we must have\n$$\n-\\gamma = -\\frac{1}{2} \\implies \\gamma = \\frac{1}{2}.\n$$\n- Coefficient of $b_n b'_n (\\Delta W_n)^2$: For this to hold for arbitrary functions $b$, we must have\n$$\n\\beta + \\gamma = \\frac{1}{2}.\n$$\nWe now have a system of equations for $\\alpha$, $\\beta$, and $\\gamma$. Substituting the value of $\\gamma$ into the third equation yields:\n$$\n\\beta + \\frac{1}{2} = \\frac{1}{2} \\implies \\beta = 0.\n$$\nThe values for the coefficients are therefore $\\alpha = 1/2$, $\\beta = 0$, and $\\gamma = 1/2$.\n\nThe final result is the ordered triple $(\\alpha, \\beta, \\gamma)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 0 & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3058176"}]}