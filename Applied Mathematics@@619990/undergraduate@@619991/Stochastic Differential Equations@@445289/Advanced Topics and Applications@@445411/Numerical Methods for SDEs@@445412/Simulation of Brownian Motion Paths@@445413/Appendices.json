{"hands_on_practices": [{"introduction": "A simulated path is our best window into the famously counter-intuitive nature of Brownian motion. This first exercise challenges you to build a Brownian motion from its most basic definition—a sum of independent Gaussian increments—and use this simulation to numerically verify some of its cornerstone properties [@problem_id:3074685]. By computing the quadratic variation and examining the maximum fluctuation over small time intervals, you will gain a practical understanding of why Brownian motion is continuous yet nowhere differentiable, a concept that is central to stochastic calculus.", "problem": "Write a complete, runnable program that simulates sample paths of Brownian motion using independent Gaussian increments and uses the simulations to numerically investigate path continuity and quadratic variation. Base your reasoning on the following fundamental definitions and facts only, without invoking any unproven shortcut formulas: (i) a standard Brownian motion $\\{W_t\\}_{t \\ge 0}$ is a stochastic process with $W_0 = 0$, independent and stationary increments, and $W_t - W_s$ is Gaussian with mean $0$ and variance $t-s$ for all $0 \\le s < t$; (ii) for a uniform partition $\\Pi = \\{0 = t_0 < t_1 < \\dots < t_N = T\\}$ with mesh size $\\Delta t = T/N$, an Euler-type discrete approximation simulates increments $\\Delta W_i = W_{t_i} - W_{t_{i-1}}$ as independent samples from a normal distribution with mean $0$ and variance $\\Delta t$, and constructs a path via partial sums $W_{t_i} = \\sum_{j=1}^i \\Delta W_j$; (iii) the quadratic variation along a partition $\\Pi$ of a process $\\{X_t\\}$ is the sum of squared increments $\\sum_{i=1}^N (X_{t_i} - X_{t_{i-1}})^2$, and for scaled Brownian motion $X_t = \\sigma W_t$ and drifted Brownian motion $X_t = \\mu t + \\sigma W_t$ these definitions apply to their respective increments. Your program must implement the simulation of increments and compute the specified statistics by first principles from these bases.\n\nYour program must do the following for the given test suite. In all cases, use a fixed seed for the pseudorandom number generator to ensure deterministic output. For a given horizon $T$ and number of steps $N$, define the uniform step $\\Delta t = T/N$. For a given volatility $\\sigma$, simulate increments as $\\Delta W_i \\sim \\mathcal{N}(0, \\sigma^2 \\Delta t)$ independently. For a given drift $\\mu$, define drifted increments $\\Delta X_i = \\mu \\Delta t + \\sigma \\Delta W_i$ using the same underlying $\\Delta W_i$.\n\nTest suite and required outputs:\n- Case $1$ (baseline quadratic variation approximation): Simulate a standard Brownian motion with $T = 1$, $N = 100000$, $\\sigma = 1$. Seed the generator with $314159$. Compute the empirical quadratic variation $Q_1 = \\sum_{i=1}^N (\\Delta W_i)^2$ and output $Q_1$ as a float.\n- Case $2$ (scaled variance): Simulate with $T = 2$, $N = 200000$, $\\sigma = 0.8$. Seed the generator with $314160$. Compute the empirical quadratic variation $Q_2 = \\sum_{i=1}^N (\\Delta W_i)^2$ and output $Q_2$ as a float.\n- Case $3$ (drift invariance at vanishing mesh): Using the same underlying Gaussian increments for both processes, simulate a drifted process with $T = 1$, $N = 400000$, $\\sigma = 1$, $\\mu = 0.5$, and its non-drifted counterpart with $T = 1$, $N = 400000$, $\\sigma = 1$, $\\mu = 0$. Seed the generator with $314161$. Compute $Q^{\\text{drift}} = \\sum_{i=1}^N (\\Delta X_i)^2$ and $Q^{\\text{nodrift}} = \\sum_{i=1}^N (\\sigma \\Delta W_i)^2$ using the same $\\Delta W_i$, and output the absolute difference $D_3 = | Q^{\\text{drift}} - Q^{\\text{nodrift}} |$ as a float.\n- Case $4$ (empirical modulus statistic for continuity at the partition scale): For a standard Brownian motion, compute the maximum absolute increment $M = \\max_{1 \\le i \\le N} |\\Delta W_i|$ for $T = 1$, $N = 131072$, $\\sigma = 1$. Seed the generator with $314162$. Form the scale $S = \\sqrt{2 \\Delta t \\log(1/\\Delta t)}$, and output the ratio $R_4 = M / S$ as a float.\n\nFinal output format:\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[Q_1, Q_2, D_3, R_4]$.\n- Each entry must be printed as a decimal number rounded to exactly $6$ digits after the decimal point.\n\nThere are no physical units in this problem. All angles, if any, would be in radians, but no angles are used. Your program must be entirely self-contained, must not take any input, and must not read or write any files or access any network resources.", "solution": "The problem statement has been validated and is deemed valid, albeit with significant notational ambiguity. A consistent interpretation that aligns with the provided fundamental definitions of Brownian motion and standard practices in stochastic calculus is possible. The core ambiguity lies in the inconsistent use of the symbol $\\Delta W_i$. A rigorous interpretation requires distinguishing between the increment of a standard Brownian motion and the increment of a scaled or drifted process.\n\nThe interpretation adopted for the solution is as follows:\n1.  The fundamental building block is the increment of a standard Brownian motion $\\{W_t\\}_{t \\ge 0}$, denoted here as $\\Delta W_i^{\\text{std}} = W_{t_i} - W_{t_{i-1}}$. For a time step $\\Delta t = T/N$, these increments are simulated as independent random variables from a Gaussian distribution $\\mathcal{N}(0, \\Delta t)$. This is achieved by generating standard normal variates $Z_i \\sim \\mathcal{N}(0, 1)$ and setting $\\Delta W_i^{\\text{std}} = Z_i \\sqrt{\\Delta t}$.\n2.  The general process to be simulated is a drifted-diffusive process of the form $X_t = \\mu t + \\sigma W_t$. Its discrete increments are given by $\\Delta X_i = \\mu \\Delta t + \\sigma \\Delta W_i^{\\text{std}}$.\n3.  The term \"quadratic variation\" in each case is interpreted as the empirical quadratic variation of the specific process being simulated, i.e., $\\sum_{i=1}^N (\\Delta X_i)^2$, where $\\Delta X_i$ are the increments of that process. The ambiguous notation in the problem, such as writing $\\sum (\\Delta W_i)^2$, is resolved by assuming it refers to the increments of the process specified in that case's context.\n\nThis interpretation renders the problem well-posed and scientifically sound, allowing for a unique and meaningful solution.\n\nThe simulation and computation for each case are performed as follows:\n\n**General Simulation Step**\nFor each test case with parameters for the time horizon $T$, number of steps $N$, volatility $\\sigma$, drift $\\mu$, and a random seed, the following steps are taken:\n1.  A random number generator is initialized with the specified seed to ensure deterministic output.\n2.  The time step is calculated as $\\Delta t = T/N$.\n3.  A vector of $N$ independent standard normal samples, $\\{Z_i\\}_{i=1}^N$, is generated.\n4.  The increments of the corresponding stochastic process, $\\{ \\Delta X_i \\}_{i=1}^N$, are constructed based on the parameters.\n\n**Case 1: Baseline Quadratic Variation**\nThe parameters are $T = 1$, $N = 100000$, $\\sigma = 1$, and $\\mu = 0$. The process is a standard Brownian motion, $X_t = W_t$.\n-   The time step is $\\Delta t = 1/100000$.\n-   The increments are $\\Delta X_i = 1 \\cdot Z_i \\sqrt{\\Delta t} = Z_i \\sqrt{\\Delta t}$.\n-   The quadratic variation is computed as $Q_1 = \\sum_{i=1}^N (\\Delta X_i)^2 = \\sum_{i=1}^N (Z_i \\sqrt{\\Delta t})^2 = \\Delta t \\sum_{i=1}^N Z_i^2$.\nTheoretically, the quadratic variation converges to $T = 1$. The simulation provides a numerical approximation.\n\n**Case 2: Scaled Quadratic Variation**\nThe parameters are $T = 2$, $N = 200000$, $\\sigma = 0.8$, and $\\mu = 0$. The process is a scaled Brownian motion, $X_t = \\sigma W_t = 0.8 W_t$.\n-   The time step is $\\Delta t = 2/200000 = 1/100000$.\n-   The increments are $\\Delta X_i = \\sigma Z_i \\sqrt{\\Delta t} = 0.8 \\cdot Z_i \\sqrt{\\Delta t}$.\n-   The quadratic variation is computed as $Q_2 = \\sum_{i=1}^N (\\Delta X_i)^2 = \\sum_{i=1}^N (0.8 \\cdot Z_i \\sqrt{\\Delta t})^2 = \\sigma^2 \\Delta t \\sum_{i=1}^N Z_i^2$.\nTheoretically, the quadratic variation of $X_t = \\sigma W_t$ over $[0, T]$ is $\\sigma^2 T = (0.8)^2 \\cdot 2 = 0.64 \\cdot 2 = 1.28$.\n\n**Case 3: Drift Invariance of Quadratic Variation**\nThe parameters are $T = 1$, $N = 400000$, $\\sigma = 1$. Two processes are compared, one with drift $\\mu = 0.5$ and one with $\\mu = 0$. Both are based on the same underlying sequence of random variates $\\{Z_i\\}_{i=1}^N$.\n-   The time step is $\\Delta t = 1/400000$.\n-   The standard Wiener increments are $\\Delta W_i^{\\text{std}} = Z_i \\sqrt{\\Delta t}$.\n-   The increments for the non-drifted process ($\\mu=0, \\sigma=1$) are $\\Delta Y_i = \\sigma \\Delta W_i^{\\text{std}} = 1 \\cdot Z_i \\sqrt{\\Delta t}$. Its quadratic variation is $Q^{\\text{nodrift}} = \\sum_{i=1}^N (\\Delta Y_i)^2$.\n-   The increments for the drifted process ($\\mu=0.5, \\sigma=1$) are $\\Delta X_i = \\mu \\Delta t + \\sigma \\Delta W_i^{\\text{std}} = 0.5 \\Delta t + 1 \\cdot Z_i \\sqrt{\\Delta t}$. Its quadratic variation is $Q^{\\text{drift}} = \\sum_{i=1}^N (\\Delta X_i)^2$.\n-   Expanding $Q^{\\text{drift}}$ gives $\\sum (\\mu \\Delta t)^2 + 2\\mu \\Delta t \\sum \\Delta W_i^{\\text{std}} + \\sum (\\Delta W_i^{\\text{std}})^2$.\nThe difference is $D_3 = |Q^{\\text{drift}} - Q^{\\text{nodrift}}| = |\\sum_{i=1}^N ((\\mu\\Delta t)^2 + 2\\mu\\Delta t \\Delta W_i^{\\text{std}})|$.\nAs $N \\to \\infty$ and $\\Delta t \\to 0$, this difference tends to zero since the terms are of order $\\Delta t$ or smaller. The simulation numerically demonstrates this property.\n\n**Case 4: Empirical Modulus of Continuity**\nThe parameters are $T = 1$, $N = 131072$, $\\sigma = 1$, and $\\mu = 0$. The process is a standard Brownian motion.\n-   The time step is $\\Delta t = 1/131072$.\n-   The increments are $\\Delta X_i = 1 \\cdot Z_i \\sqrt{\\Delta t}$.\n-   The maximum absolute increment is computed: $M = \\max_{1 \\le i \\le N} |\\Delta X_i| = \\sqrt{\\Delta t} \\max_i |Z_i|$.\n-   A theoretical scaling factor from the Law of the Iterated Logarithm for Brownian increments is $S = \\sqrt{2 \\Delta t \\log(1/\\Delta t)}$.\n-   The ratio $R_4 = M/S$ is calculated. The theory suggests that $\\limsup_{\\Delta t \\to 0} \\frac{\\max_{i} |W_{t_i} - W_{t_{i-1}}|}{\\sqrt{2 \\Delta t \\log(1/\\Delta t)}} = 1$ almost surely. The simulation provides a single-path realization of this ratio for a finite $\\Delta t$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates sample paths of Brownian motion to investigate path continuity\n    and quadratic variation based on the problem specification.\n    \"\"\"\n    \n    test_cases = [\n        {'id': 1, 'T': 1.0, 'N': 100000, 'sigma': 1.0, 'seed': 314159},\n        {'id': 2, 'T': 2.0, 'N': 200000, 'sigma': 0.8, 'seed': 314160},\n        {'id': 3, 'T': 1.0, 'N': 400000, 'sigma': 1.0, 'mu': 0.5, 'seed': 314161},\n        {'id': 4, 'T': 1.0, 'N': 131072, 'sigma': 1.0, 'seed': 314162}\n    ]\n\n    results = []\n\n    # Case 1: Baseline quadratic variation\n    case1 = test_cases[0]\n    rng1 = np.random.default_rng(case1['seed'])\n    T1, N1, sigma1 = case1['T'], case1['N'], case1['sigma']\n    dt1 = T1 / N1\n    Z1 = rng1.standard_normal(size=N1)\n    # The increments of the process X_t = sigma * W_t are dX_i = sigma * dW_i\n    # where dW_i ~ N(0, dt). dW_i = Z_i * sqrt(dt). So dX_i = sigma * Z_i * sqrt(dt).\n    # Quadratic variation is Sum(dX_i^2) = Sum( (sigma * Z_i * sqrt(dt))^2 )\n    # = sigma^2 * dt * Sum(Z_i^2)\n    q1 = (sigma1**2) * dt1 * np.sum(Z1**2)\n    results.append(q1)\n    \n    # Case 2: Scaled variance\n    case2 = test_cases[1]\n    rng2 = np.random.default_rng(case2['seed'])\n    T2, N2, sigma2 = case2['T'], case2['N'], case2['sigma']\n    dt2 = T2 / N2\n    Z2 = rng2.standard_normal(size=N2)\n    q2 = (sigma2**2) * dt2 * np.sum(Z2**2)\n    results.append(q2)\n\n    # Case 3: Drift invariance\n    case3 = test_cases[2]\n    rng3 = np.random.default_rng(case3['seed'])\n    T3, N3, sigma3, mu3 = case3['T'], case3['N'], case3['sigma'], case3['mu']\n    dt3 = T3 / N3\n    Z3 = rng3.standard_normal(size=N3)\n    \n    # Standard Wiener increments\n    dW_std = Z3 * np.sqrt(dt3)\n    \n    # Non-drifted process increments: dY_i = sigma * dW_i\n    nodrift_increments = sigma3 * dW_std\n    q_nodrift = np.sum(nodrift_increments**2)\n    \n    # Drifted process increments: dX_i = mu*dt + sigma*dW_i\n    drifted_increments = mu3 * dt3 + sigma3 * dW_std\n    q_drift = np.sum(drifted_increments**2)\n    \n    d3 = np.abs(q_drift - q_nodrift)\n    results.append(d3)\n    \n    # Case 4: Empirical modulus statistic\n    case4 = test_cases[3]\n    rng4 = np.random.default_rng(case4['seed'])\n    T4, N4, sigma4 = case4['T'], case4['N'], case4['sigma']\n    dt4 = T4 / N4\n    Z4 = rng4.standard_normal(size=N4)\n    \n    # Process increments for standard BM (sigma=1)\n    increments = sigma4 * Z4 * np.sqrt(dt4)\n    \n    # M = max |increment_i|\n    M = np.max(np.abs(increments))\n    \n    # S = sqrt(2*dt*log(1/dt))\n    S = np.sqrt(2 * dt4 * np.log(1 / dt4))\n    \n    r4 = M / S\n    results.append(r4)\n\n    # Format output to 6 decimal places and print in the required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3074685"}, {"introduction": "Having mastered the simulation of a free Brownian motion, we now explore how to constrain its path. This practice guides you through the concept of a Brownian bridge, which is a Brownian path conditioned to start at one point and end at a specific value at a future time [@problem_id:3074672]. You will first derive the mean and variance of this conditional process from the fundamental properties of multivariate Gaussian distributions and then implement a simulation to verify your analytical results, providing a powerful lesson in the interplay between stochastic theory and computational practice.", "problem": "Given the definition of a standard Brownian motion (also called a Wiener process) $\\{W_t\\}_{t \\ge 0}$ as a Gaussian process with $W_0 = 0$ almost surely, independent increments, and $W_t - W_s \\sim \\mathcal{N}(0, t - s)$ for $0 \\le s < t$, consider conditioning a Brownian motion on a fixed terminal value at a future time. Let $T > 0$ and a terminal value $a \\in \\mathbb{R}$ be given. Define the conditioned process (the Brownian bridge) at an intermediate time $t \\in [0, T]$ by $B_t := W_t \\mid \\{W_T = a\\}$. Starting from the fundamental properties of Gaussian processes and Brownian motion, derive the conditional distribution of $B_t$ in terms of its mean and variance, and derive a simulation procedure for $B_t$ that uses only the defining properties of Brownian motion, without invoking any shortcut formulas.\n\nYour program must implement the following, entirely from these principles:\n- Use a Monte Carlo simulation based on the fundamental increment structure of Brownian motion to generate independent samples of the pair $(W_t, W_T)$ for a given $t$ and $T$.\n- From those, construct samples of $B_t$ corresponding to the Brownian bridge conditioned on $W_T = a$.\n- Empirically estimate the mean and the variance of $B_t$ from the generated samples.\n- Compare these empirical values to the analytically derived mean and variance from your derivation and report whether each estimate is within a specified tolerance.\n\nImplementation details that must be followed exactly:\n- Use $N = 400000$ Monte Carlo samples for each test case.\n- Use a fixed pseudorandom number generator seed equal to $314159265$ for reproducibility.\n- Use tolerances $\\varepsilon_{\\mu} = 5 \\times 10^{-3}$ for the mean and $\\varepsilon_{\\sigma^2} = 5 \\times 10^{-3}$ for the variance.\n- For simulating $(W_t, W_T)$, you must use the Brownian motion increment structure: simulate $W_t \\sim \\mathcal{N}(0, t)$ and an independent increment $I_{t \\to T} \\sim \\mathcal{N}(0, T - t)$ (with the understanding that a variance equal to $0$ means the corresponding variable is deterministically $0$), and then set $W_T := W_t + I_{t \\to T}$.\n- Construct a sample of $B_t$ from the simulated $(W_t, W_T)$ using only your derived relation, and then compute its empirical mean and variance.\n\nTest suite:\nYour program must run the following test cases, each specified as a tuple $(T, a, t)$:\n- $(1.0, 0.0, 0.5)$\n- $(2.0, 1.0, 1.0)$\n- $(3.0, -1.5, 0.0)$\n- $(1.25, 0.7, 1.25)$\n\nFor each test case, compute two booleans:\n- $b_{\\mu}$: true if the absolute error between the empirical mean and the analytical mean is at most $\\varepsilon_{\\mu}$.\n- $b_{\\sigma^2}$: true if the absolute error between the empirical variance and the analytical variance is at most $\\varepsilon_{\\sigma^2}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results in a comma-separated list enclosed in square brackets. Concatenate the two booleans per test case in order, resulting in a list of length $8$, that is,\n$[b_{\\mu}^{(1)}, b_{\\sigma^2}^{(1)}, b_{\\mu}^{(2)}, b_{\\sigma^2}^{(2)}, b_{\\mu}^{(3)}, b_{\\sigma^2}^{(3)}, b_{\\mu}^{(4)}, b_{\\sigma^2}^{(4)}]$.\nNo additional text or lines should be printed.\n\nAll quantities in this problem are nondimensional; no physical units are involved. Angles do not appear, and no percentages are required. Ensure $T > 0$ and $t \\in [0, T]$ in all computations.", "solution": "The problem requires the derivation of the conditional distribution of a standard Brownian motion $\\{W_t\\}_{t \\ge 0}$ at an intermediate time $t \\in [0, T]$ given its terminal value $W_T = a$. This conditioned process, $B_t := W_t \\mid \\{W_T = a\\}$, is known as a Brownian bridge. We must derive its mean and variance from first principles, and then derive a simulation procedure, also from first principles. Finally, we will implement this procedure to empirically verify our analytical results.\n\n**1. Derivation of the Conditional Distribution of the Brownian Bridge**\n\nA standard Brownian motion $\\{W_t\\}_{t \\ge 0}$ is a Gaussian process characterized by:\n1. $W_0 = 0$ almost surely.\n2. For any set of times $0 \\le t_1 < t_2 < \\dots < t_n$, the increments $W_{t_1}, W_{t_2}-W_{t_1}, \\dots, W_{t_n}-W_{t_{n-1}}$ are independent.\n3. For $0 \\le s < t$, the increment $W_t - W_s$ is a normally distributed random variable, $W_t - W_s \\sim \\mathcal{N}(0, t-s)$.\n\nFrom these properties, it follows that for any $t>0$, $W_t \\sim \\mathcal{N}(0, t)$, and $\\mathbb{E}[W_t] = 0$.\n\nTo find the distribution of $B_t = W_t \\mid \\{W_T = a\\}$, we consider the joint distribution of the random variables $W_t$ and $W_T$ for a fixed $t$ such that $0 \\le t \\le T$. Since $\\{W_t\\}$ is a Gaussian process, the vector $(W_t, W_T)^T$ has a bivariate normal distribution. We must determine its mean vector and covariance matrix.\n\nThe mean vector is $\\boldsymbol{\\mu} = (\\mathbb{E}[W_t], \\mathbb{E}[W_T])^T = (0, 0)^T$.\n\nThe covariance matrix $\\Sigma$ is given by:\n$$\n\\Sigma = \\begin{pmatrix} \\text{Var}(W_t) & \\text{Cov}(W_t, W_T) \\\\ \\text{Cov}(W_t, W_T) & \\text{Var}(W_T) \\end{pmatrix}\n$$\nThe variances are directly given by the definition: $\\text{Var}(W_t) = t$ and $\\text{Var}(W_T) = T$.\nTo find the covariance $\\text{Cov}(W_t, W_T)$, we use the property of independent increments. We write $W_T$ as $W_t + (W_T - W_t)$. For $t < T$, the random variables $W_t$ and $(W_T - W_t)$ are independent.\n$$\n\\text{Cov}(W_t, W_T) = \\mathbb{E}[W_t W_T] - \\mathbb{E}[W_t]\\mathbb{E}[W_T] = \\mathbb{E}[W_t W_T]\n$$\n$$\n\\mathbb{E}[W_t W_T] = \\mathbb{E}[W_t (W_t + W_T - W_t)] = \\mathbb{E}[W_t^2] + \\mathbb{E}[W_t(W_T - W_t)]\n$$\nBecause of independence and the fact that $\\mathbb{E}[W_s] = 0$ for any $s$, we have:\n$$\n\\mathbb{E}[W_t(W_T - W_t)] = \\mathbb{E}[W_t]\\mathbb{E}[W_T - W_t] = 0 \\cdot 0 = 0\n$$\nTherefore, the covariance is:\n$$\n\\text{Cov}(W_t, W_T) = \\mathbb{E}[W_t^2] = \\text{Var}(W_t) + (\\mathbb{E}[W_t])^2 = t + 0^2 = t\n$$\nThis formula also holds for the boundary case $t=T$, where $\\text{Cov}(W_T, W_T) = \\text{Var}(W_T) = T$.\nThe covariance matrix for $(W_t, W_T)^T$ is thus:\n$$\n\\Sigma = \\begin{pmatrix} t & t \\\\ t & T \\end{pmatrix}\n$$\nFor a general bivariate normal vector $(X_1, X_2)^T$ with mean $(\\mu_1, \\mu_2)^T$ and covariance matrix $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $X_1$ given $X_2 = x_2$ is normal, with conditional mean and variance given by:\n$$\n\\mathbb{E}[X_1 \\mid X_2=x_2] = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2)\n$$\n$$\n\\text{Var}(X_1 \\mid X_2=x_2) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\nApplying these formulae to our case (with $X_1 = W_t, X_2 = W_T, x_2 = a, \\mu_1 = \\mu_2 = 0$), we obtain the analytical mean and variance of the Brownian bridge $B_t$:\nThe analytical mean is:\n$$\n\\mathbb{E}[B_t] = \\mathbb{E}[W_t \\mid W_T=a] = 0 + t \\cdot T^{-1} \\cdot (a - 0) = \\frac{at}{T}\n$$\nThe analytical variance is:\n$$\n\\text{Var}(B_t) = \\text{Var}(W_t \\mid W_T=a) = t - t \\cdot T^{-1} \\cdot t = t - \\frac{t^2}{T} = \\frac{tT - t^2}{T} = \\frac{t(T-t)}{T}\n$$\nSo, the conditional distribution is $B_t \\sim \\mathcal{N}\\left(\\frac{at}{T}, \\frac{t(T-t)}{T}\\right)$.\n\n**2. Derivation of the Simulation Procedure**\n\nThe problem asks for a simulation procedure derived from first principles. We can achieve this by constructing a process related to the Brownian bridge that is independent of the conditioning variable $W_T$. Let's define a new process $X_t$ for $t \\in [0, T]$:\n$$\nX_t = W_t - \\frac{t}{T}W_T\n$$\nSince $X_t$ is a linear combination of jointly Gaussian random variables, it is also a Gaussian process. Let's compute its covariance with $W_T$:\n$$\n\\text{Cov}(X_t, W_T) = \\text{Cov}\\left(W_t - \\frac{t}{T}W_T, W_T\\right) = \\text{Cov}(W_t, W_T) - \\frac{t}{T}\\text{Var}(W_T)\n$$\nUsing our previously derived results $\\text{Cov}(W_t, W_T) = t$ and $\\text{Var}(W_T) = T$:\n$$\n\\text{Cov}(X_t, W_T) = t - \\frac{t}{T} \\cdot T = 0\n$$\nSince $X_t$ and $W_T$ are jointly Gaussian and their covariance is zero, they are independent.\nWe can express $W_t$ in terms of $X_t$ and $W_T$:\n$$\nW_t = X_t + \\frac{t}{T}W_T\n$$\nWhen we condition on the event $\\{W_T = a\\}$, the random variable $W_T$ becomes the constant $a$. Because $X_t$ is independent of $W_T$, its distribution is unaffected by this conditioning. Thus, the conditional process $B_t = W_t \\mid \\{W_T = a\\}$ has the same distribution as the random variable $X_t + \\frac{at}{T}$.\nThe process $X_t$ is a standard Brownian bridge (from $0$ to $0$), and its distribution is normal with mean $\\mathbb{E}[X_t] = \\mathbb{E}[W_t] - \\frac{t}{T}\\mathbb{E}[W_T] = 0$ and variance $\\text{Var}(X_t) = \\frac{t(T-t)}{T}$ (as calculated in the previous section).\n\nThis decomposition provides the simulation procedure:\n1.  Generate a sample of the unconditioned pair $(W_t, W_T)$. The problem requires this to be done using the independent increment structure:\n    a. Generate a sample $w_t$ from $\\mathcal{N}(0, t)$.\n    b. Generate an independent sample $i_{t \\to T}$ from $\\mathcal{N}(0, T-t)$.\n    c. Form the sample $w_T = w_t + i_{t \\to T}$.\n2.  From the sample pair $(w_t, w_T)$, construct a sample of the standard bridge process $X_t$:\n    $$\n    x_t = w_t - \\frac{t}{T}w_T\n    $$\n    This $x_t$ is a random draw from $\\mathcal{N}(0, \\frac{t(T-t)}{T})$.\n3.  Construct the required sample of the conditioned bridge $B_t$ by adding the deterministic drift:\n    $$\n    b_t = x_t + \\frac{at}{T} = \\left(w_t - \\frac{t}{T}w_T\\right) + \\frac{at}{T}\n    $$\nThis procedure strictly adheres to generating samples of $(W_t, W_T)$ and then constructing samples of $B_t$ using the derived relation, based entirely on the fundamental properties of Brownian motion. We will now implement this algorithm to generate $N=400000$ samples for each test case and compare the empirical mean and variance with the analytical results derived above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# SciPy is specified as available but is not required for this problem.\n# from scipy import stats\n\ndef solve():\n    \"\"\"\n    Derives and simulates a Brownian bridge to verify its mean and variance.\n    \"\"\"\n    # Define the simulation parameters from the problem statement.\n    N = 400000\n    SEED = 314159265\n    TOL_MEAN = 5e-3\n    TOL_VAR = 5e-3\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, a, t)\n        (1.0, 0.0, 0.5),\n        (2.0, 1.0, 1.0),\n        (3.0, -1.5, 0.0),\n        (1.25, 0.7, 1.25),\n    ]\n\n    # Set the seed for the pseudorandom number generator for reproducibility.\n    rng = np.random.default_rng(SEED)\n\n    results = []\n    for T, a, t in test_cases:\n        # Step 1: Calculate the analytical mean and variance of the Brownian bridge B_t.\n        # Handle the case T=0 to avoid division by zero, although problem constraints ensure T > 0.\n        if T == 0:\n            # If T=0, then t must be 0. W_0 = 0.\n            # If a is not 0, the conditioning is on an impossible event.\n            # However, problem states T > 0.\n            mean_analytical = 0.0 if a == 0 else np.nan\n            var_analytical = 0.0\n        else:\n            mean_analytical = a * t / T\n            var_analytical = t * (T - t) / T\n\n        # Step 2: Perform Monte Carlo simulation based on the derived procedure.\n        \n        # Variances for the normal distributions. Variances can be zero.\n        # std_dev = sqrt(variance)\n        std_dev_W_t = np.sqrt(t)\n        std_dev_inc = np.sqrt(T - t)\n\n        # Generate N samples of W_t from N(0, t).\n        # If t=0, std_dev_W_t=0, and this correctly generates an array of zeros.\n        W_t_samples = rng.normal(loc=0.0, scale=std_dev_W_t, size=N)\n\n        # Generate N samples of the independent increment I_{t->T} from N(0, T-t).\n        # If t=T, std_dev_inc=0, and this correctly generates an array of zeros.\n        inc_samples = rng.normal(loc=0.0, scale=std_dev_inc, size=N)\n\n        # Construct N samples of W_T = W_t + I_{t->T}.\n        W_T_samples = W_t_samples + inc_samples\n\n        # Construct N samples of the Brownian bridge B_t using the derived relation:\n        # b_t = (w_t - (t/T)*w_T) + (a*t)/T\n        # This is numerically stable since T > 0 for all test cases.\n        bridge_samples = (W_t_samples - (t / T) * W_T_samples) + (a * t / T)\n\n        # Step 3: Calculate empirical mean and variance from the samples.\n        # We use population variance (ddof=0) for comparison against theoretical value.\n        mean_empirical = np.mean(bridge_samples)\n        var_empirical = np.var(bridge_samples, ddof=0)\n\n        # Step 4: Compare empirical results to analytical values within tolerance.\n        b_mu = abs(mean_empirical - mean_analytical) <= TOL_MEAN\n        b_sigma2 = abs(var_empirical - var_analytical) <= TOL_VAR\n\n        results.extend([b_mu, b_sigma2])\n\n    # Final print statement in the exact required format.\n    # The default str() for bool is 'True'/'False' with capitalization.\n    # The problem format example like [b_mu, b_sigma2] suggests this is OK.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3074672"}, {"introduction": "Simulating stochastic processes is often a means to an end, such as pricing a financial derivative or modeling a physical system. This final practice focuses on the practical application of simulation: estimating the expected value of a functional that depends on the entire path of a Brownian motion [@problem_id:3074678]. You will move beyond simple Monte Carlo methods and implement two powerful variance reduction techniques—antithetic variates and control variates—to see firsthand how intelligent simulation design can dramatically improve the efficiency and accuracy of your estimations.", "problem": "You are to design and implement a simulation to quantify variance reduction in estimating the expectation of a path-dependent functional of standard Brownian motion using three Monte Carlo estimators. Work from first principles of standard Brownian motion, where a standard Brownian motion $\\{B_t\\}_{t \\ge 0}$ is a continuous-time stochastic process with $B_0 = 0$, independent increments, and $B_t - B_s \\sim \\mathcal{N}(0, t - s)$ for $0 \\le s < t$. Use a discrete-time approximation with a uniform grid to construct sample paths via independent Gaussian increments.\n\nDefine the path functional for a given parameter $\\alpha$ and time horizon $T$ as\n$$\nI = \\int_0^T \\exp\\!\\big(\\alpha\\, B_t - \\tfrac{1}{2}\\alpha^2 t\\big)\\, dt.\n$$\nApproximate the integral using a left Riemann sum on a uniform grid of $M$ steps with $\\Delta t = T/M$, using the values $\\{B_{k\\Delta t}\\}_{k=0}^{M-1}$. For each path, compute the sample-path approximation\n$$\n\\widehat{I} = \\sum_{k=0}^{M-1} \\exp\\!\\big(\\alpha\\, B_{k\\Delta t} - \\tfrac{1}{2}\\alpha^2 (k\\Delta t)\\big)\\, \\Delta t.\n$$\n\nYour goal is to estimate $\\mu = \\mathbb{E}[\\widehat{I}]$ using three estimators and to quantify the variance of the Monte Carlo estimator (the sample mean) under each method, expressed as the sample variance of the pathwise contributions used by that method divided by the effective number of independent terms contributing to the mean.\n\nImplement the following estimators:\n- Plain Monte Carlo (MC): simulate $N$ independent paths and compute the sample mean of $\\widehat{I}$. Report the estimated variance of this sample mean as $\\widehat{\\mathrm{Var}}_{\\text{plain}} = s^2(\\widehat{I}_1,\\dots,\\widehat{I}_N)/N$, where $s^2$ denotes the unbiased sample variance computed from the $N$ pathwise values.\n- Antithetic variates: for each of $N/2$ independently simulated sets of Gaussian increments $\\{\\Delta B_k\\}_{k=1}^M$, form a pair of paths using $\\Delta B_k$ and $-\\Delta B_k$ to compute $\\widehat{I}^{(+)}$ and $\\widehat{I}^{(-)}$. Use the pairwise averages $Y_i = \\tfrac{1}{2}(\\widehat{I}^{(+)}_i + \\widehat{I}^{(-)}_i)$ as the contributions to the estimator, and compute the variance of the sample mean as $\\widehat{\\mathrm{Var}}_{\\text{anti}} = s^2(Y_1,\\dots,Y_{N/2})/(N/2)$.\n- Control variate: for the same $N$ independent paths as in the plain Monte Carlo method, form a linear control variate using the terminal value $C = B_T$, which has known expectation $\\mathbb{E}[C] = 0$. Construct adjusted contributions $Z_i = \\widehat{I}_i - \\beta C_i$, where the coefficient $\\beta$ is chosen based on the same simulated data to minimize the sample variance of $Z_i$ over all real coefficients. Report the estimated variance of the sample mean as $\\widehat{\\mathrm{Var}}_{\\text{ctrl}} = s^2(Z_1,\\dots,Z_N)/N$.\n\nFollow the principles of standard Brownian motion simulation: generate increments as independent Gaussian random variables with variance equal to the time step, and obtain discrete path values by cumulative summation. Construct the Riemann sum using left endpoints as specified.\n\nTest suite:\nUse a single fixed random number generator seed equal to $12345$ for reproducibility and the following three test cases:\n1. Case A (happy path): $N = 20000$, $T = 1.0$, $M = 200$, $\\alpha = 1.0$.\n2. Case B (small-time boundary): $N = 20000$, $T = 0.05$, $M = 5$, $\\alpha = 1.0$.\n3. Case C (coarse grid edge): $N = 20000$, $T = 1.0$, $M = 1$, $\\alpha = 1.0$.\n\nFor each case, compute and return the triple of floats $\\big(\\widehat{\\mathrm{Var}}_{\\text{plain}}, \\widehat{\\mathrm{Var}}_{\\text{anti}}, \\widehat{\\mathrm{Var}}_{\\text{ctrl}}\\big)$ in this order. Aggregate the results from all cases by concatenation into a single flat list of nine floats in the order Case A then Case B then Case C.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain nine decimal numbers corresponding to the three variances for each of the three cases, in order, rounded to six decimal places. For example, a valid output line would look like\n$[$$0.001234$$,$$0.000987$$,$$0.000543$$,$$0.000111$$,$$0.000098$$,$$0.000076$$,$$0.0$$,$$0.0$$,$$0.0$$]$.\nNo other text should be printed.", "solution": "The user's problem is first assessed for validity.\n\n### Step 1: Extract Givens\n- **Process**: Standard Brownian motion $\\{B_t\\}_{t \\ge 0}$, with $B_0 = 0$, independent increments, and $B_t - B_s \\sim \\mathcal{N}(0, t - s)$ for $0 \\le s < t$.\n- **Path Functional**: $I = \\int_0^T \\exp\\big(\\alpha\\, B_t - \\tfrac{1}{2}\\alpha^2 t\\big)\\, dt$.\n- **Discretization**: Uniform time grid with $M$ steps, $\\Delta t = T/M$. Path values $\\{B_{k\\Delta t}\\}_{k=0}^{M-1}$ are used.\n- **Approximated Functional**: Left Riemann sum $\\widehat{I} = \\sum_{k=0}^{M-1} \\exp\\big(\\alpha\\, B_{k\\Delta t} - \\tfrac{1}{2}\\alpha^2 (k\\Delta t)\\big)\\, \\Delta t$.\n- **Objective**: Estimate $\\mu = \\mathbb{E}[\\widehat{I}]$ and compute the variance of the sample mean for three estimators.\n- **Estimators**:\n    1.  **Plain Monte Carlo (MC)**: $N$ independent paths. Estimator variance: $\\widehat{\\mathrm{Var}}_{\\text{plain}} = s^2(\\widehat{I}_1,\\dots,\\widehat{I}_N)/N$.\n    2.  **Antithetic Variates**: $N/2$ independent sets of increments, forming path pairs $(B, -B)$. Contributions $Y_i = \\tfrac{1}{2}(\\widehat{I}^{(+)}_i + \\widehat{I}^{(-)}_i)$. Estimator variance: $\\widehat{\\mathrm{Var}}_{\\text{anti}} = s^2(Y_1,\\dots,Y_{N/2})/(N/2)$.\n    3.  **Control Variate**: $N$ paths from plain MC. Control variate $C = B_T$. Adjusted contributions $Z_i = \\widehat{I}_i - \\beta C_i$, where $\\beta$ is estimated to minimize sample variance. Estimator variance: $\\widehat{\\mathrm{Var}}_{\\text{ctrl}} = s^2(Z_1,\\dots,Z_N)/N$.\n- **Simulation Parameters**:\n    - Random seed: $12345$.\n    - Case A: $N = 20000$, $T = 1.0$, $M = 200$, $\\alpha = 1.0$.\n    - Case B: $N = 20000$, $T = 0.05$, $M = 5$, $\\alpha = 1.0$.\n    - Case C: $N = 20000$, $T = 1.0$, $M = 1$, $\\alpha = 1.0$.\n- **Output**: A single flat list of nine floats $(\\widehat{\\mathrm{Var}}_{\\text{plain}}, \\widehat{\\mathrm{Var}}_{\\text{anti}}, \\widehat{\\mathrm{Var}}_{\\text{ctrl}})$ for each case, in order, rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem is based on the well-established mathematical theory of stochastic processes (Brownian motion) and statistical simulation (Monte Carlo methods). All definitions and concepts are standard in quantitative finance and computational mathematics. The quantity $X_t = \\exp(\\alpha B_t - \\frac{1}{2}\\alpha^2 t)$ is a geometric Brownian motion, which is a fundamental object in financial modeling.\n- **Well-Posed**: The problem provides a clear, unambiguous computational task. All parameters are specified, and the output format is precise. A unique numerical solution exists for the given seed.\n- **Objective**: The problem statement is formal and objective, devoid of subjective or speculative elements.\n- **No Flaws**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-posed. The special case $M=1$ is a valid edge case, not a flaw.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of the problem is to estimate the expectation of a functional of a stochastic process, $\\mu = \\mathbb{E}[\\widehat{I}]$, and to quantify the efficiency of different Monte Carlo estimators.\n\n**Theoretical Foundation**\nThe process $X_t = \\exp(\\alpha B_t - \\frac{1}{2}\\alpha^2 t)$ is a martingale with respect to the filtration generated by the standard Brownian motion $B_t$. This is a direct consequence of Girsanov's theorem and can be verified using Itô's lemma, which shows that $dX_t = \\alpha X_t dB_t$, a stochastic differential equation with no drift term. As a martingale, its expectation is constant over time: $\\mathbb{E}[X_t] = X_0 = \\exp(0-0) = 1$ for all $t \\ge 0$.\nThe expectation of the discretized functional $\\widehat{I}$ can thus be computed analytically:\n$$\n\\mu = \\mathbb{E}[\\widehat{I}] = \\mathbb{E}\\left[\\sum_{k=0}^{M-1} X_{k\\Delta t} \\Delta t\\right] = \\sum_{k=0}^{M-1} \\mathbb{E}[X_{k\\Delta t}] \\Delta t = \\sum_{k=0}^{M-1} (1) \\Delta t = M \\Delta t = T.\n$$\nThe exact value of the expectation is the time horizon $T$. This provides a valuable sanity check for the simulation results, as the sample means of all estimators should converge to $T$.\n\n**Brownian Path Simulation**\nA path of standard Brownian motion is simulated on a discrete time grid $t_k = k \\Delta t$ for $k = 0, 1, \\dots, M$, where $\\Delta t = T/M$. The path is constructed from its fundamental properties:\n1.  $B_{t_0} = B_0 = 0$.\n2.  The increments $\\Delta B_k = B_{t_k} - B_{t_{k-1}}$ for $k=1, \\dots, M$ are independent and identically distributed (i.i.d.) normal random variables, $\\Delta B_k \\sim \\mathcal{N}(0, \\Delta t)$.\nThese increments are generated by scaling standard normal variates $Z_k \\sim \\mathcal{N}(0, 1)$, such that $\\Delta B_k = \\sqrt{\\Delta t} Z_k$. A path is then constructed by the cumulative sum: $B_{t_k} = \\sum_{j=1}^k \\Delta B_j$.\n\n**Estimator Implementation**\n\nFor each test case, the three estimators are implemented according to the problem specification. A single random number generator, re-initialized with the specified seed for each case, provides the source of randomness.\n\n1.  **Plain Monte Carlo (MC)**: This is the baseline method. We generate $N$ independent paths of the Brownian motion. For each path $i \\in \\{1, \\dots, N\\}$, we compute the functional value $\\widehat{I}_i$. The estimate of $\\mu$ is the sample mean $\\bar{I} = \\frac{1}{N} \\sum_{i=1}^N \\widehat{I}_i$. The variance of this sample mean, which measures the estimator's precision, is estimated by $\\widehat{\\mathrm{Var}}_{\\text{plain}} = s^2(\\{\\widehat{I}_i\\}) / N$, where $s^2$ is the unbiased sample variance of the collected $\\widehat{I}_i$ values.\n\n2.  **Antithetic Variates**: This technique exploits the symmetry of the Brownian motion's increments. Since the distribution of the driving noise $\\{Z_k\\}$ is symmetric around $0$, the path $-B_t$ generated from increments $-\\Delta B_k$ is equally likely as the path $B_t$. For a convex functional like $\\widehat{I}$ (the exponential function is convex), the average of $\\widehat{I}(B_t)$ and $\\widehat{I}(-B_t)$ is expected to have lower variance than $\\widehat{I}(B_t)$ alone. We generate $N/2$ independent paths, denoted $B^{(+)}_i$, and their antithetic counterparts, $B^{(-)}_i = -B^{(+)}_i$. We then form $N/2$ new observations $Y_i = \\frac{1}{2}(\\widehat{I}(B^{(+)}_i) + \\widehat{I}(B^{(-)}_i))$. The variance of the resulting sample mean is estimated by $\\widehat{\\mathrm{Var}}_{\\text{anti}} = s^2(\\{Y_i\\}) / (N/2)$.\n\n3.  **Control Variates**: This method reduces variance by subtracting a correlated variable with a known mean. We use the terminal value of the Brownian path, $C = B_T$, as the control variate. We know $\\mathbb{E}[C] = \\mathbb{E}[B_T] = 0$. We form a new set of observations $Z_i = \\widehat{I}_i - \\beta(C_i - \\mathbb{E}[C_i]) = \\widehat{I}_i - \\beta C_i$. The optimal coefficient $\\beta$ that minimizes $\\mathrm{Var}(Z_i)$ is $\\beta^* = \\mathrm{Cov}(\\widehat{I}, C) / \\mathrm{Var}(C)$. We estimate $\\beta^*$ from the $N$ simulated samples of $\\widehat{I}_i$ and $C_i$:\n    $$\n    \\hat{\\beta} = \\frac{\\widehat{\\mathrm{Cov}}(\\{\\widehat{I}_i\\}, \\{C_i\\})}{\\widehat{\\mathrm{Var}}(\\{C_i\\})}.\n    $$\n    Using this estimated $\\hat{\\beta}$, we compute the adjusted values $Z_i = \\widehat{I}_i - \\hat{\\beta} C_i$. The variance of the controlled estimator's mean is then given by $\\widehat{\\mathrm{Var}}_{\\text{ctrl}} = s^2(\\{Z_i\\}) / N$. This method is effective if $\\widehat{I}$ and $B_T$ are significantly correlated. Since the integrand value at later times depends on larger-variance $B_t$ values, which are highly correlated with $B_T$, a strong correlation is expected.\n\nFor edge case $C$ where $M=1$, the Riemann sum simplifies to a single term for $k=0$. Since $B_0=0$ and $t_0=0$, $\\widehat{I} = \\exp(0)\\Delta t = \\Delta t = T/M = 1.0/1.0 = 1.0$. As $\\widehat{I}$ is a constant, the variance of all estimators is $0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates Brownian motion paths to quantify variance reduction in estimating the\n    expectation of a path-dependent functional using three Monte Carlo techniques:\n    Plain Monte Carlo, Antithetic Variates, and Control Variates.\n    \"\"\"\n\n    def calculate_variances_for_case(N, T, M, alpha, rng):\n        \"\"\"\n        Calculates the variances of the sample mean for the three estimators\n        for a single set of parameters.\n\n        Args:\n            N (int): Number of paths for Plain MC and Control Variates.\n            T (float): Time horizon.\n            M (int): Number of time steps in the discretization.\n            alpha (float): Parameter in the path functional.\n            rng (np.random.Generator): The random number generator instance.\n\n        Returns:\n            tuple: A triple of floats (var_plain, var_anti, var_ctrl).\n        \"\"\"\n        delta_t = T / M\n\n        # Handle the trivial edge case where the functional is constant.\n        if M == 1:\n            # For M=1, the sum has one term at k=0.\n            # I_hat = exp(alpha*B_0 - 0.5*alpha^2*t_0) * delta_t\n            # Since B_0=0 and t_0=0, I_hat = exp(0) * T = T.\n            # The value is constant, so variance is 0 for all methods.\n            return 0.0, 0.0, 0.0\n\n        # --- Plain MC and Control Variate Calculations ---\n        # Generate N paths, each with M steps.\n        # Z_mc are the standard normal innovations.\n        Z_mc = rng.standard_normal(size=(N, M))\n        delta_B_mc = Z_mc * np.sqrt(delta_t)\n        \n        # B_mc are the Brownian paths. Shape: (N, M+1) since B_0=0 is prepended.\n        B_mc = np.cumsum(delta_B_mc, axis=1)\n        B_mc = np.hstack((np.zeros((N, 1)), B_mc))\n        \n        # Time grid for the Riemann sum (k=0 to M-1).\n        t_grid = np.arange(M) * delta_t\n        \n        # Calculate the approximate functional I_hat for each of the N paths.\n        # B_for_I uses path values from t_0 to t_{M-1}.\n        B_for_I = B_mc[:, :M]\n        exponent = alpha * B_for_I - 0.5 * alpha**2 * t_grid\n        integrand = np.exp(exponent)\n        I_hats_mc = np.sum(integrand, axis=1) * delta_t\n        \n        # 1. Plain MC variance of the sample mean\n        var_plain_samples = np.var(I_hats_mc, ddof=1)\n        var_plain_mean = var_plain_samples / N\n        \n        # 3. Control Variate variance of the sample mean\n        C = B_mc[:, -1] # Control variate C = B_T\n        cov_matrix = np.cov(I_hats_mc, C, ddof=1)\n        \n        # Estimate the optimal coefficient beta.\n        # Check for Var(C)=0 to avoid division by zero, though unlikely here.\n        if cov_matrix[1, 1] == 0:\n            beta_hat = 0.0\n        else:\n            beta_hat = cov_matrix[0, 1] / cov_matrix[1, 1]\n\n        Z = I_hats_mc - beta_hat * C\n        var_Z_samples = np.var(Z, ddof=1)\n        var_ctrl_mean = var_Z_samples / N\n\n        # --- Antithetic Variate Calculation ---\n        N_half = N // 2\n        \n        # Generate N/2 new sets of increments.\n        Z_anti = rng.standard_normal(size=(N_half, M))\n        \n        # Construct positive and negative paths from the same increments.\n        delta_B_pos = Z_anti * np.sqrt(delta_t)\n        B_pos = np.cumsum(delta_B_pos, axis=1)\n        B_pos = np.hstack((np.zeros((N_half, 1)), B_pos))\n        B_neg = -B_pos\n        \n        # Calculate I_hat for positive paths.\n        B_for_I_pos = B_pos[:, :M]\n        exponent_pos = alpha * B_for_I_pos - 0.5 * alpha**2 * t_grid\n        integrand_pos = np.exp(exponent_pos)\n        I_hats_pos = np.sum(integrand_pos, axis=1) * delta_t\n        \n        # Calculate I_hat for negative paths.\n        B_for_I_neg = B_neg[:, :M]\n        exponent_neg = alpha * B_for_I_neg - 0.5 * alpha**2 * t_grid\n        integrand_neg = np.exp(exponent_neg)\n        I_hats_neg = np.sum(integrand_neg, axis=1) * delta_t\n        \n        # 2. Antithetic Variate variance of the sample mean.\n        # Y are the pairwise averages.\n        Y = 0.5 * (I_hats_pos + I_hats_neg)\n        var_Y_samples = np.var(Y, ddof=1)\n        var_anti_mean = var_Y_samples / N_half\n\n        return var_plain_mean, var_anti_mean, var_ctrl_mean\n\n    seed = 12345\n    \n    test_cases = [\n        # (N, T, M, alpha)\n        (20000, 1.0, 200, 1.0),  # Case A\n        (20000, 0.05, 5, 1.0),   # Case B\n        (20000, 1.0, 1, 1.0)     # Case C\n    ]\n\n    results = []\n    for case_params in test_cases:\n        N, T, M, alpha = case_params\n        # A new RNG is created for each test case, seeded with the same value\n        # to ensure each case is independently reproducible.\n        rng = np.random.default_rng(seed)\n        case_variances = calculate_variances_for_case(N, T, M, alpha, rng)\n        results.extend(case_variances)\n\n    # Format the final output list as a comma-separated string of floats\n    # rounded to six decimal places, enclosed in square brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3074678"}]}