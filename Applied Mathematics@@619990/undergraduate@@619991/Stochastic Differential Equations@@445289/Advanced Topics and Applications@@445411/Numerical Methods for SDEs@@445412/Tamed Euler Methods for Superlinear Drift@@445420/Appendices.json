{"hands_on_practices": [{"introduction": "Before we can fix a numerical problem, we must understand its mathematical origins. This exercise guides you through analyzing the classic superlinear drift function, $b(x) = -x^3$, to see why it poses a challenge for standard numerical methods [@problem_id:3079385]. You will verify that it fails the global Lipschitz condition required by many convergence theorems, but satisfies a weaker one-sided Lipschitz condition, a property that hints at the underlying stability of the true SDE and is key to more advanced stability proofs.", "problem": "Consider the scalar drift function $b:\\mathbb{R}\\to\\mathbb{R}$ defined by $b(x)=-x^3$, which arises as a prototype of a superlinear drift in the stochastic differential equation (SDE) $dX_t=b(X_t)dt+\\sigma dW_t$, where $W_t$ is a standard Brownian motion and $\\sigma\\in\\mathbb{R}$ is a constant. In the analysis of numerical methods such as tamed Euler schemes for SDEs with superlinear drift, one-sided Lipschitz (monotonicity) conditions play a central role. Starting only from the definitions of one-sided Lipschitz and global Lipschitz continuity, and standard calculus facts such as the mean value theorem, do the following:\n\n- Using the definition of one-sided Lipschitz continuity, determine the minimal constant $L^{\\star}\\in\\mathbb{R}$ such that\n$$\n(x-y)\\big(b(x)-b(y)\\big)\\leq L^{\\star}\\,|x-y|^{2}\\quad\\text{for all }x,y\\in\\mathbb{R}.\n$$\n\n- Using the definition of global Lipschitz continuity, argue whether $b$ is globally Lipschitz on $\\mathbb{R}$.\n\nYour final answer must be the single value of the minimal one-sided Lipschitz constant $L^{\\star}$. No rounding is required.", "solution": "The problem requires us to analyze the function $b(x) = -x^3$ with respect to two properties: one-sided Lipschitz continuity and global Lipschitz continuity. We will address each part in sequence, starting from the formal definitions and using standard calculus.\n\nFirst, we determine the minimal one-sided Lipschitz constant $L^{\\star} \\in \\mathbb{R}$. The one-sided Lipschitz condition is given by the inequality\n$$\n(x-y)\\big(b(x)-b(y)\\big) \\leq L^{\\star}\\,|x-y|^{2}\n$$\nfor all $x,y \\in \\mathbb{R}$. The goal is to find the smallest possible value of $L^{\\star}$ for which this inequality holds universally.\n\nLet's substitute the definition of the function $b(x)=-x^3$ into the left-hand side of the inequality:\n$$\n(x-y)\\big(b(x)-b(y)\\big) = (x-y)\\big((-x^3) - (-y^3)\\big) = (x-y)(y^3-x^3).\n$$\nWe use the algebraic identity for the difference of cubes, $a^3-b^3 = (a-b)(a^2+ab+b^2)$, to factor the term $(x^3-y^3)$:\n$$\ny^3-x^3 = -(x^3-y^3) = -(x-y)(x^2+xy+y^2).\n$$\nSubstituting this back, we obtain:\n$$\n(x-y)(y^3-x^3) = (x-y)\\big(-(x-y)(x^2+xy+y^2)\\big) = -(x-y)^2(x^2+xy+y^2).\n$$\nThe one-sided Lipschitz inequality now becomes:\n$$\n-(x-y)^2(x^2+xy+y^2) \\leq L^{\\star}\\,|x-y|^2.\n$$\nSince $|x-y|^2 = (x-y)^2$, the inequality is equivalent to:\n$$\n-(x-y)^2(x^2+xy+y^2) \\leq L^{\\star}\\,(x-y)^2.\n$$\nFor any $x \\neq y$, the term $(x-y)^2$ is strictly positive, so we can divide both sides by it without changing the direction of the inequality:\n$$\n-(x^2+xy+y^2) \\leq L^{\\star}.\n$$\nThis inequality must hold for all $x,y \\in \\mathbb{R}$, including the limit as $y \\to x$. To find the minimal constant $L^{\\star}$, we must find the supremum of the expression on the left-hand side over all $x,y \\in \\mathbb{R}$. Let $Q(x,y) = x^2+xy+y^2$. We seek $\\sup_{x,y \\in \\mathbb{R}} \\{-Q(x,y)\\}$, which is equivalent to finding $-\\inf_{x,y \\in \\mathbb{R}} \\{Q(x,y)\\}$.\n\nWe can find the infimum of the quadratic form $Q(x,y)$ by completing the square with respect to one of the variables, say $x$:\n$$\nQ(x,y) = x^2+xy+y^2 = \\left(x^2 + xy + \\frac{1}{4}y^2\\right) - \\frac{1}{4}y^2 + y^2 = \\left(x+\\frac{1}{2}y\\right)^2 + \\frac{3}{4}y^2.\n$$\nThe expression $(x+\\frac{1}{2}y)^2 + \\frac{3}{4}y^2$ is a sum of two squared real terms. The square of any real number is non-negative, so $(x+\\frac{1}{2}y)^2 \\ge 0$ and $\\frac{3}{4}y^2 \\ge 0$. Therefore, their sum is always non-negative:\n$$\nQ(x,y) = x^2+xy+y^2 \\ge 0.\n$$\nThe minimum value of $Q(x,y)$ is attained when both terms are simultaneously zero. The term $\\frac{3}{4}y^2$ is zero if and only if $y=0$. Substituting $y=0$ into the first term, we get $(x+0)^2 = x^2$, which is zero if and only if $x=0$. Thus, the infimum of $Q(x,y)$ is $0$, and it is attained at the point $(x,y)=(0,0)$.\n$$\n\\inf_{x,y \\in \\mathbb{R}} \\{x^2+xy+y^2\\} = 0.\n$$\nThe supremum of the left-hand side of our inequality is therefore:\n$$\n\\sup_{x,y \\in \\mathbb{R}} \\{-(x^2+xy+y^2)\\} = -\\inf_{x,y \\in \\mathbb{R}} \\{x^2+xy+y^2\\} = -0 = 0.\n$$\nThe minimal constant $L^{\\star}$ must be greater than or equal to this supremum. Thus, the minimal one-sided Lipschitz constant is $L^{\\star}=0$.\n\nNext, we address whether the function $b(x)=-x^3$ is globally Lipschitz on $\\mathbb{R}$. A function $b$ is globally Lipschitz continuous if there exists a non-negative constant $K \\in \\mathbb{R}$ such that for all $x,y \\in \\mathbb{R}$, the following inequality holds:\n$$\n|b(x)-b(y)| \\le K|x-y|.\n$$\nFor a differentiable function like $b(x)$, a necessary condition for it to be globally Lipschitz is that its derivative must be bounded. This follows from the Mean Value Theorem, which states that for any $x \\neq y$, there exists a point $c$ between $x$ and $y$ such that:\n$$\n\\frac{b(x)-b(y)}{x-y} = b'(c).\n$$\nIf $b$ were globally Lipschitz with constant $K$, we would have:\n$$\n\\left|\\frac{b(x)-b(y)}{x-y}\\right| \\leq K \\quad \\text{for all } x \\neq y.\n$$\nThis implies $|b'(c)| \\le K$ for all possible values of $c \\in \\mathbb{R}$, meaning that the derivative $b'(x)$ must be a bounded function on $\\mathbb{R}$.\n\nLet's compute the derivative of $b(x)=-x^3$:\n$$\nb'(x) = \\frac{d}{dx}(-x^3) = -3x^2.\n$$\nWe now examine if this derivative is bounded on $\\mathbb{R}$. The absolute value of the derivative is $|b'(x)| = |-3x^2| = 3x^2$. As $x$ approaches infinity, $3x^2$ also approaches infinity:\n$$\n\\lim_{x \\to \\infty} |b'(x)| = \\lim_{x \\to \\infty} 3x^2 = \\infty.\n$$\nSince the magnitude of the derivative, $|b'(x)|$, is unbounded on $\\mathbb{R}$, there exists no finite constant $K$ that can satisfy $|b'(x)| \\le K$ for all $x \\in \\mathbb{R}$. Therefore, the function $b(x)=-x^3$ is not globally Lipschitz continuous on $\\mathbb{R}$. This finding is consistent with the fact that its drift is \"superlinear\", which is the reason specialized numerical methods like tamed Euler schemes are required for the corresponding SDE.\n\nThe primary task is to report the minimal one-sided Lipschitz constant $L^{\\star}$, which we have determined to be $0$.", "answer": "$$\\boxed{0}$$", "id": "3079385"}, {"introduction": "This practice exposes a fundamental paradox: a stochastic system can be inherently stable, yet a naive numerical simulation of it can be violently unstable. You will first use a Lyapunov function to prove that the exact solution to an SDE with a $-x^3$ drift has bounded moments, confirming its stability [@problem_id:3079366]. Then, by constructing a counterexample, you will demonstrate why the standard Euler-Maruyama method fails to capture this stability, providing a clear motivation for the development of tamed schemes.", "problem": "Consider the stochastic differential equation (SDE) in one dimension\n$$\ndX_t = -X_t^3 dt + dW_t, \\qquad X_0 = 0, \\qquad t \\in [0,T],\n$$\nwhere $W_t$ is a standard one-dimensional Brownian motion. Let $V(x) = x^2$ be a Lyapunov-type function and let $\\mathcal{L}$ denote the infinitesimal generator of the Itô diffusion associated with the SDE.\n\n(a) Starting from the Itô formula and the definition of the generator, derive an inequality of the form\n$$\n\\mathcal{L} V(x) \\le - \\lambda |x|^4 + 1\n$$\nvalid for all $x \\in \\mathbb{R}$, and determine the largest possible constant $\\lambda$ that makes this inequality true.\n\n(b) Using the inequality from part (a), explain why the exact solution $(X_t)_{t \\in [0,T]}$ has bounded second moments on $[0,T]$.\n\n(c) Now consider the explicit Euler–Maruyama method (i.e., untamed Euler) with step size $h = T/N$ applied to the SDE:\n$$\nY_{n+1} = Y_n - h Y_n^3 + \\Delta W_n, \\qquad Y_0 = 0, \\qquad \\Delta W_n := W_{(n+1)h} - W_{nh}.\n$$\nUsing first principles, construct a counterexample argument—based on a suitable sequence of rare events and the superlinear growth of the drift—that shows the mean-square strong error at time $T$,\n$$\n\\mathbb{E}[|X_T - Y_N|^2],\n$$\ndoes not converge to zero as $h \\to 0$ (i.e., as $N \\to \\infty$), even though $(X_t)_{t \\in [0,T]}$ has bounded moments. Your reasoning must be scientifically sound and self-contained, and must clearly connect the superlinear drift to the mechanism by which untamed Euler fails.\n\nYour final answer must be the exact value of the constant $\\lambda$ found in part (a). No rounding is required and no units are involved.", "solution": "(a) We are asked to derive an inequality for the infinitesimal generator $\\mathcal{L}$ applied to the function $V(x) = x^2$. The one-dimensional stochastic differential equation (SDE) is given by\n$$\ndX_t = a(X_t) \\, dt + b(X_t) \\, dW_{t}\n$$\nwhere the drift coefficient is $a(x) = -x^3$ and the diffusion coefficient is $b(x) = 1$. The infinitesimal generator $\\mathcal{L}$ acting on a twice continuously differentiable function $V(x)$ is defined as\n$$\n\\mathcal{L} V(x) = a(x) \\frac{dV}{dx}(x) + \\frac{1}{2} b(x)^2 \\frac{d^2V}{dx^2}(x).\n$$\nFor the given function $V(x) = x^2$, its first and second derivatives are:\n$$\n\\frac{dV}{dx}(x) = 2x\n$$\n$$\n\\frac{d^2V}{dx^2}(x) = 2.\n$$\nSubstituting $a(x)$, $b(x)$, and the derivatives of $V(x)$ into the definition of the generator, we obtain\n$$\n\\mathcal{L} V(x) = (-x^3)(2x) + \\frac{1}{2}(1)^2(2) = -2x^4 + 1.\n$$\nWe need to find the largest constant $\\lambda$ such that the inequality $\\mathcal{L} V(x) \\le - \\lambda |x|^4 + 1$ holds for all $x \\in \\mathbb{R}$. Using our derived expression for $\\mathcal{L} V(x)$, this inequality becomes\n$$\n-2x^4 + 1 \\le - \\lambda |x|^4 + 1.\n$$\nSince $x^4 = |x|^4$, we can write this as\n$$\n-2|x|^4 + 1 \\le - \\lambda |x|^4 + 1.\n$$\nSubtracting $1$ from both sides gives\n$$\n-2|x|^4 \\le - \\lambda |x|^4.\n$$\nMultiplying by $-1$ and reversing the inequality sign, we get\n$$\n2|x|^4 \\ge \\lambda |x|^4.\n$$\nThis can be rearranged to\n$$\n(2 - \\lambda)|x|^4 \\ge 0.\n$$\nSince $|x|^4 \\ge 0$ for all $x \\in \\mathbb{R}$, this inequality holds true if and only if the coefficient $(2 - \\lambda)$ is non-negative. That is,\n$$\n2 - \\lambda \\ge 0 \\implies \\lambda \\le 2.\n$$\nThe set of all valid $\\lambda$ is $(-\\infty, 2]$. The largest possible constant $\\lambda$ that makes the inequality true is therefore $\\lambda = 2$.\n\n(b) We use the result from part (a) to explain why the second moments of the exact solution, $\\mathbb{E}[X_t^2]$, are bounded on the interval $[0, T]$. We apply the Itô formula to $V(X_t) = X_t^2$:\n$$\ndV(X_t) = \\mathcal{L}V(X_t) \\, dt + \\frac{dV}{dx}(X_t) b(X_t) \\, dW_t.\n$$\nIntegrating from $0$ to $t$ gives\n$$\nV(X_t) - V(X_0) = \\int_0^t \\mathcal{L}V(X_s) \\, ds + \\int_0^t 2X_s \\cdot 1 \\, dW_s.\n$$\nTaking the expectation of both sides, we get\n$$\n\\mathbb{E}[V(X_t)] - \\mathbb{E}[V(X_0)] = \\mathbb{E}\\left[\\int_0^t \\mathcal{L}V(X_s) \\, ds\\right] + \\mathbb{E}\\left[\\int_0^t 2X_s \\, dW_s\\right].\n$$\nThe expectation of the stochastic integral term is zero, i.e., $\\mathbb{E}[\\int_0^t 2X_s \\, dW_s] = 0$, provided that $\\mathbb{E}[\\int_0^t (2X_s)^2 \\, ds]  \\infty$, which holds in our case. Using Fubini's theorem, we can swap expectation and integration for the drift term. Given the initial condition $X_0 = 0$, we have $V(X_0) = 0^2 = 0$. Thus, we arrive at Dynkin's formula:\n$$\n\\mathbb{E}[V(X_t)] = \\int_0^t \\mathbb{E}[\\mathcal{L}V(X_s)] \\, ds.\n$$\nFrom part (a), we have the inequality $\\mathcal{L}V(x) \\le -2|x|^4 + 1$. Applying this, we get\n$$\n\\mathbb{E}[V(X_t)] \\le \\int_0^t \\mathbb{E}[-2|X_s|^4 + 1] \\, ds = \\int_0^t (-2\\mathbb{E}[|X_s|^4] + 1) \\, ds.\n$$\nSubstituting $V(X_t) = X_t^2$, we have\n$$\n\\mathbb{E}[X_t^2] \\le t - 2 \\int_0^t \\mathbb{E}[X_s^4] \\, ds.\n$$\nSince $\\mathbb{E}[X_s^4]$ is an expectation of a non-negative random variable, $\\mathbb{E}[X_s^4] \\ge 0$. Therefore, the integral term $-2 \\int_0^t \\mathbb{E}[X_s^4] \\, ds$ is less than or equal to zero. This gives us a simple upper bound:\n$$\n\\mathbb{E}[X_t^2] \\le t.\n$$\nFor any time $t$ in the interval $[0, T]$, it follows that $\\mathbb{E}[X_t^2] \\le T$. This demonstrates that the second moment of the exact solution is uniformly bounded on the interval $[0, T]$ by the constant $T$. The strong restoring nature of the drift term $-x^3$, captured by the negative quartic term in $\\mathcal{L}V(x)$, confines the process and ensures its moments remain finite.\n\n(c) The untamed Euler–Maruyama method for the SDE fails to converge in the mean-square sense due to a numerical instability caused by the superlinear growth of the drift coefficient $a(x) = -x^3$. The following counterexample argument explains this mechanism.\n\nThe numerical scheme is given by $Y_{n+1} = Y_n - h Y_n^3 + \\Delta W_n$. The update can be viewed as the composition of a deterministic step, $y \\mapsto y - hy^3$, and a random perturbation. Let us analyze the stability of this deterministic map. The magnitude of the result is $|y-hy^3| = |y||1-hy^2|$. If $|1-hy^2| > 1$, the map is expansive, meaning it increases the magnitude of its input. The condition $|1-hy^2| > 1$ is met if $1-hy^2 > 1$ (which is impossible for $h>0$) or if $1-hy^2  -1$. The latter inequality simplifies to $hy^2 > 2$, which implies $|y| > \\sqrt{2/h}$.\n\nThis defines a region of numerical instability: $\\{y \\in \\mathbb{R} : |y| > \\sqrt{2/h}\\}$. If the numerical solution $Y_n$ enters this region, the deterministic part of the next step will amplify its magnitude. For instance, if $Y_n = \\sqrt{3/h}$, then $Y_n - hY_n^3 = \\sqrt{3/h} - h(3\\sqrt{3}/h^{3/2}) = -2\\sqrt{3/h}$. The magnitude has doubled. In the subsequent step, starting from $-2\\sqrt{3/h}$, the deterministic part would yield $-2\\sqrt{3/h} - h(-8 \\cdot 3\\sqrt{3}/h^{3/2}) = 22\\sqrt{3/h}$, a further dramatic increase.\n\nThe random increment $\\Delta W_n = W_{(n+1)h} - W_{nh}$ follows a normal distribution $N(0, h)$. Crucially, this distribution has unbounded support. This means that for any step $n$ and any threshold $C$, the probability $P(|\\Delta W_n| > C)$ is greater than zero.\n\nNow, we can construct the failure scenario.\n1.  Let the simulation start at $Y_0=0$. At any given step $n$, assume the process $Y_n$ has a moderate value (e.g., $|Y_n| \\le 1$).\n2.  The next state is $Y_{n+1} = Y_n - h Y_n^3 + \\Delta W_n$. For a sufficiently large random kick $\\Delta W_n$, the value of $Y_{n+1}$ can be propelled into the instability region. For example, the event $A_n = \\{|\\Delta W_n| > \\sqrt{2/h} + 1 + h\\}$ has a small but positive probability. If this event occurs, then $|Y_{n+1}| \\ge |\\Delta W_n| - |Y_n - hY_n^3| > (\\sqrt{2/h} + 1 + h) - (1+h) = \\sqrt{2/h}$.\n3.  Once $Y_{n+1}$ lies in the instability region, the subsequent iterates $Y_{n+2}, Y_{n+3}, \\dots$ are very likely to experience explosive growth in magnitude, as shown by the analysis of the deterministic map. While a subsequent large random increment could coincidentally cancel this explosion, this is highly improbable. The dominant behavior is a rapid divergence to infinity.\n\nThe mean-square error is $\\mathbb{E}[|X_T - Y_N|^2]$. For this to converge to zero, it is necessary that $\\mathbb{E}[Y_N^2]$ remains bounded as $h \\to 0$ (since $\\mathbb{E}[X_T^2]$ is bounded). However, the expectation $\\mathbb{E}[Y_N^2]$ is an average over all possible paths of the random walk. The existence of these rare but catastrophic paths—where an early large increment triggers numerical instability and explosion—contributes enormously to the expectation. The value of $Y_N^2$ on these paths can be so large (scaling inverse powers of $h$) that their contribution, even when weighted by their small probability, causes the overall expectation $\\mathbb{E}[Y_N^2]$ to diverge as $h \\to 0$.\n\nIn stark contrast, the true solution $X_t$ is always stable. The drift $-X_t^3$ is applied continuously, not in discrete steps. If $X_t$ becomes large, the drift provides an instantaneous and powerful restoring force, preventing escape to infinity. The failure of the untamed Euler method is a direct result of the discrete-time approximation of a superlinear drift, which creates an artificial numerical instability that the continuous-time process does not possess. Therefore, the mean-square error does not converge to zero.", "answer": "$$\n\\boxed{2}\n$$", "id": "3079366"}, {"introduction": "Having established why the standard Euler-Maruyama method fails, this final practice empowers you to fix it. We will dissect the one-step evolution of the numerical solution's second moment to pinpoint the source of the explosive error in the Euler-Maruyama scheme [@problem_id:3079328]. You will then apply the 'taming' principle and perform an asymptotic analysis to determine the minimum taming exponent needed to restore stability, turning an unreliable method into a robust one.", "problem": "Consider the one-dimensional stochastic differential equation (SDE) driven by a standard Wiener process (Brownian motion) $W_t$,\n$$\ndX_t = b(X_t)dt + \\sigma dW_t,\\quad b(x) = -x^3,\\quad \\sigma > 0,\n$$\nand fix a uniform time step $h>0$ with discrete times $t_n = n h$. Let $\\Delta W_n := W_{t_{n+1}} - W_{t_n}$ denote the Wiener increment, which satisfies $\\mathbb{E}[\\Delta W_n] = 0$ and $\\mathbb{E}[(\\Delta W_n)^2] = h$. \n\n1. Derive the Euler–Maruyama update $X_{n+1} = X_n + h\\,b(X_n) + \\sigma\\,\\Delta W_n$ for this SDE, and from first principles obtain an expression for the conditional one-step mean-square $\\mathbb{E}[|X_{n+1}|^2 \\mid X_n]$ in terms of $X_n$, $h$, and $\\sigma$. Using only this expression and asymptotic reasoning grounded in the polynomial growth of $b(x)$, explain why the naive Euler–Maruyama method may lead to $\\mathbb{E}[|X_n|^2]$ growing without bound as $n\\to\\infty$ for fixed $h>0$.\n\n2. To mitigate this superlinear drift effect, consider the tamed Euler method defined by\n$$\nX_{n+1}^{(p)} = X_n + h\\,\\frac{b(X_n)}{1 + h\\,|X_n|^{p}} + \\sigma\\,\\Delta W_n,\n$$\nwhere $p\\in\\mathbb{N}$ is a taming exponent. Starting from the fundamental properties of $\\Delta W_n$ and the structure of the update, determine the minimal integer taming exponent $p$ that ensures, for sufficiently large $|X_n|$, the conditional mean-square decreases in the sense that\n$$\n\\mathbb{E}[|X_{n+1}^{(p)}|^2 | X_n]  |X_n|^2,\n$$\nso that the scheme counteracts the positive contribution from the superlinear drift and prevents mean-square blow-up. Give your final answer as the value of $p$ (a single integer).", "solution": "We address the two parts of the problem in sequence.\n\n## Part 1: Euler–Maruyama Method\n\nThe one-dimensional stochastic differential equation (SDE) is given by\n$$\ndX_t = b(X_t)dt + \\sigma dW_t\n$$\nwith drift $b(x) = -x^3$ and diffusion $\\sigma > 0$. The Euler–Maruyama discretization with a time step $h>0$ is a direct transcription of the SDE, where the infinitesimal increments $dX_t$, $dt$, and $dW_t$ are replaced by their finite-step counterparts $\\Delta X_n = X_{n+1} - X_n$, $h$, and $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$, respectively.\n\nThis yields the update rule:\n$$\nX_{n+1} - X_n = b(X_n)h + \\sigma \\Delta W_n\n$$\nSubstituting $b(X_n) = -X_n^3$, the Euler–Maruyama update for this specific SDE is:\n$$\nX_{n+1} = X_n - hX_n^3 + \\sigma\\Delta W_n\n$$\nTo analyze the stability of this method, we compute the conditional one-step mean-square, $\\mathbb{E}[|X_{n+1}|^2 \\mid X_n]$. Since $X_t$ is a real-valued process, this is equivalent to $\\mathbb{E}[X_{n+1}^2 \\mid X_n]$. We consider $X_n$ to be a fixed value, as it belongs to the information set $\\mathcal{F}_{t_n}$ upon which we are conditioning.\n\nWe square the update expression:\n$$\nX_{n+1}^2 = (X_n - hX_n^3 + \\sigma\\Delta W_n)^2 = \\big((X_n - hX_n^3) + \\sigma\\Delta W_n\\big)^2\n$$\nExpanding the square gives:\n$$\nX_{n+1}^2 = (X_n - hX_n^3)^2 + 2\\sigma(X_n - hX_n^3)\\Delta W_n + \\sigma^2(\\Delta W_n)^2\n$$\nNow, we take the conditional expectation with respect to $X_n$. We use the fundamental properties of the Wiener increment $\\Delta W_n$: it is independent of the past information (including $X_n$), its mean is zero, and its variance is the time step $h$.\n$$\n\\mathbb{E}[\\Delta W_n \\mid X_n] = \\mathbb{E}[\\Delta W_n] = 0\n$$\n$$\n\\mathbb{E}[(\\Delta W_n)^2 \\mid X_n] = \\mathbb{E}[(\\Delta W_n)^2] = h\n$$\nApplying this to the expanded expression for $X_{n+1}^2$:\n$$\n\\mathbb{E}[X_{n+1}^2 \\mid X_n] = \\mathbb{E}\\big[(X_n - hX_n^3)^2 \\mid X_n\\big] + 2\\sigma(X_n - hX_n^3)\\mathbb{E}[\\Delta W_n \\mid X_n] + \\sigma^2\\mathbb{E}[(\\Delta W_n)^2 \\mid X_n]\n$$\nSince $X_n$ is fixed under this conditioning, the expression simplifies to:\n$$\n\\mathbb{E}[X_{n+1}^2 \\mid X_n] = (X_n - hX_n^3)^2 + 2\\sigma(X_n - hX_n^3) \\cdot 0 + \\sigma^2h\n$$\nExpanding the remaining squared term, we obtain the expression for the conditional one-step mean-square:\n$$\n\\mathbb{E}[X_{n+1}^2 \\mid X_n] = X_n^2 - 2hX_n^4 + h^2X_n^6 + \\sigma^2h\n$$\nTo understand why this scheme may lead to numerical explosion, we examine the change in the conditional mean-square from one step to the next:\n$$\n\\mathbb{E}[X_{n+1}^2 \\mid X_n] - X_n^2 = h^2X_n^6 - 2hX_n^4 + \\sigma^2h\n$$\nThe continuous SDE has a drift $-x^3$ that is strongly mean-reverting, pulling the process towards the origin. In the discretized version, the term $-2hX_n^4$ represents this stabilizing effect. However, the discretization introduces a new term, $h^2X_n^6$, which arises from squaring the discrete drift update $h b(X_n)$. This term is an artifact of the numerical method.\n\nFor small values of $|X_n|$, the dynamics are stable. However, for large $|X_n|$, the term $h^2X_n^6$ dominates the expression. Specifically, its growth as $X_n^6$ is of a higher order than the stabilizing term $-2hX_n^4$. When $|X_n|$ becomes large enough such that $h^2X_n^6 > 2hX_n^4$, the net effect of the drift terms in the one-step mean-square becomes positive and rapidly increasing. This occurs when $h X_n^2 > 2$, or $|X_n| > \\sqrt{2/h}$. Once the process enters this region, the conditional expectation of its next squared value is even larger, creating a positive feedback loop. This leads to the unbounded growth (explosion) of the second moment, $\\mathbb{E}[|X_n|^2]$, as $n \\to \\infty$ for any fixed step size $h>0$.\n\n## Part 2: Tamed Euler Method\n\nThe tamed Euler method is designed to control this artificial growth by modifying the drift term. The update rule is:\n$$\nX_{n+1}^{(p)} = X_n + h\\,\\frac{b(X_n)}{1 + h|X_n|^p} + \\sigma\\Delta W_n = X_n - h\\,\\frac{X_n^3}{1 + h|X_n|^p} + \\sigma\\Delta W_n\n$$\nwhere $p \\in \\mathbb{N}$ is the taming exponent. The denominator $1 + h|X_n|^p$ \"tames\" the drift for large $|X_n|$.\n\nWe seek the minimal integer $p$ that ensures the conditional mean-square decreases for sufficiently large $|X_n|$, i.e., $\\mathbb{E}[|X_{n+1}^{(p)}|^2 | X_n]  |X_n|^2$.\n\nFollowing the same procedure as in Part 1, we find the conditional mean-square for the tamed scheme:\n$$\n\\mathbb{E}[(X_{n+1}^{(p)})^2 | X_n] = \\left(X_n - h\\,\\frac{X_n^3}{1 + h|X_n|^p}\\right)^2 + \\sigma^2h\n$$\nThe condition for decrease becomes:\n$$\n\\left(X_n - h\\,\\frac{X_n^3}{1 + h|X_n|^p}\\right)^2 + \\sigma^2h  X_n^2\n$$\nExpanding the square and subtracting $X_n^2$ from both sides, we get:\n$$\n-2h\\,\\frac{X_n^4}{1 + h|X_n|^p} + h^2\\,\\frac{X_n^6}{\\left(1 + h|X_n|^p\\right)^2} + \\sigma^2h  0\n$$\nTo determine the minimal integer $p$ for which this inequality holds for \"sufficiently large $|X_n|$\", we perform an asymptotic analysis as $|X_n| \\to \\infty$. Let $x = |X_n|$. Since $X_n^4 = x^4$ and $X_n^6 = x^6$, the inequality is:\n$$\n-2h\\,\\frac{x^4}{1 + hx^p} + h^2\\,\\frac{x^6}{(1 + hx^p)^2} + \\sigma^2h  0\n$$\nFor large $x$, we can approximate the denominators: $1 + hx^p \\sim hx^p$. The inequality becomes asymptotically equivalent to:\n$$\n-2h\\,\\frac{x^4}{hx^p} + h^2\\,\\frac{x^6}{(hx^p)^2} + \\sigma^2h \\lesssim 0\n$$\n$$\n-2x^{4-p} + x^{6-2p} + \\sigma^2h \\lesssim 0\n$$\nWe need to find the minimal integer $p \\ge 1$ such that the left-hand side is negative for large $x$. This requires the dominant term (the one with the highest power of $x$) to be negative. Let's examine the exponents $4-p$ and $6-2p$.\n\nCase $p=1$: The exponents are $4-1=3$ and $6-2(1)=4$. The dominant term is $x^4$, which has a positive coefficient of $1$. The expression grows towards $+\\infty$. Thus, $p=1$ is insufficient.\n\nCase $p=2$: The exponents are $4-2=2$ and $6-2(2)=2$. The powers are equal. We must sum the coefficients of the leading terms. The asymptotic expression is $-2x^2 + x^2 = -x^2$. This is negative and grows towards $-\\infty$ for large $x$. Thus, the inequality holds for sufficiently large $x$. $p=2$ is a valid solution.\n\nLet's verify this more rigorously for $p=2$:\n$$\n\\lim_{x\\to\\infty} \\left( -2h\\,\\frac{x^4}{1 + hx^2} + h^2\\,\\frac{x^6}{(1 + hx^2)^2} + \\sigma^2h \\right)\n$$\nDividing numerators and denominators by the highest powers of $x$:\n$$\n= \\lim_{x\\to\\infty} \\left( -2h\\,\\frac{x^2}{1/x^2 + h} + h^2\\,\\frac{x^2}{(1/x^2 + h)^2} + \\sigma^2h \\right)\n$$\n$$\n= \\lim_{x\\to\\infty} \\left( -2h\\,\\frac{x^2}{h} + h^2\\,\\frac{x^2}{h^2} \\right) = \\lim_{x\\to\\infty} (-2x^2 + x^2) = \\lim_{x\\to\\infty} (-x^2) = -\\infty\n$$\nSince the limit is $-\\infty$, the expression is guaranteed to be negative for sufficiently large $x$. So $p=2$ works.\n\nCase $p=3$: The exponents are $4-3=1$ and $6-2(3)=0$. The dominant term is $-2x^1$, which is negative for $x>0$. The expression tends to $-\\infty$. Thus, $p=3$ also works.\n\nCase $p \\ge 4$: The exponent $4-p$ is less than or equal to $0$, and $6-2p$ is negative.\nFor $p=4$, the asymptotic expression is $-2x^0 + x^{-2} + \\sigma^2h \\to -2+\\sigma^2h$. This is not guaranteed to be negative for all $h, \\sigma > 0$.\nFor $p>4$, both $4-p$ and $6-2p$ are negative, so the first two terms vanish as $x \\to \\infty$. The expression limit is $\\sigma^2h > 0$.\nTherefore, only $p=2$ and $p=3$ satisfy the condition.\n\nThe problem asks for the minimal integer taming exponent $p$. Comparing the valid solutions $p=2$ and $p=3$, the minimal value is $p=2$.", "answer": "$$\\boxed{2}$$", "id": "3079328"}]}