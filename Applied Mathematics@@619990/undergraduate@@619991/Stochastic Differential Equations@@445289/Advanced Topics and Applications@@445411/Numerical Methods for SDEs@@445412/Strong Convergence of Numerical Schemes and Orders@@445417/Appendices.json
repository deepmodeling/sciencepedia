{"hands_on_practices": [{"introduction": "Any numerical simulation of a stochastic differential equation begins with the crucial step of generating the random noise that drives the system. The strong convergence of a numerical scheme depends critically on how faithfully the simulated noise mimics the properties of the underlying Wiener process, particularly its Gaussian distribution and the independence of its increments. This exercise [@problem_id:3079040] challenges you to identify the correct simulation procedure among several plausible but flawed alternatives, ensuring your numerical experiments are built on a solid statistical foundation.", "problem": "Consider a stochastic differential equation driven by a Wiener process (also called Brownian motion) $W_t$, and a uniform time grid $t_n = n h$ with step size $h>0$. For strong convergence analysis of a one-step numerical scheme such as Euler–Maruyama, the discrete noise inputs must faithfully reproduce the law and structural properties of the driving Wiener process. By the defining properties of a Wiener process, increments over disjoint intervals are independent and stationary, and satisfy $W_{t_{n+1}} - W_{t_n} \\sim N(0,h)$ with mean $0$ and variance $h$.\n\nYou are implementing a numerical scheme of the form $X_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n$ and must choose how to simulate the Brownian increments $\\Delta W_n$ and ensure independence across steps in a way that supports strong error analysis (e.g., pathwise $L^p$ convergence). Which option below correctly describes a statistically valid procedure for generating $\\Delta W_n$ with the required distribution and independence across steps?\n\nA. Initialize a pseudo-random normal generator once, then for each step $n$ draw an independent $Z_n \\sim N(0,1)$ and set $\\Delta W_n = \\sqrt{h}\\, Z_n$. Do not reseed inside the loop; rely on the generator’s stream to produce independent draws.\n\nB. At each step $n$, reinitialize the random number generator with the same fixed seed, draw $Z_n \\sim N(0,1)$, and set $\\Delta W_n = \\sqrt{h}\\, Z_n$, ensuring reproducibility within the loop.\n\nC. For each step $n$, draw $U_n \\sim \\mathrm{Unif}(-\\sqrt{3h},\\sqrt{3h})$ independently so that $\\mathbb{E}[U_n]=0$ and $\\mathrm{Var}(U_n)=h$, and set $\\Delta W_n = U_n$, matching the first two moments of $N(0,h)$.\n\nD. Generate independent standard normals $Z_n \\sim N(0,1)$ by a method such as the Box–Muller transform, and set $\\Delta W_n = h\\, Z_n$ to respect the time step length.\n\nE. Use antithetic variates across time to reduce Monte Carlo variance: for each pair of steps $(2k,2k+1)$, draw one $Z \\sim N(0,1)$ and set $\\Delta W_{2k} = \\sqrt{h}\\, Z$ and $\\Delta W_{2k+1} = -\\sqrt{h}\\, Z$.", "solution": "A Wiener process $W_t$ is characterized by the following fundamental properties: $W_0=0$, sample paths are continuous, increments over disjoint intervals are independent, and for $0 \\le s < t$, the increment $W_t - W_s$ is normally distributed with mean $0$ and variance $t-s$. On a uniform grid $t_n = n h$, it follows that $\\Delta W_n := W_{t_{n+1}} - W_{t_n} \\sim N(0,h)$, and the family $\\{\\Delta W_n\\}_{n\\ge 0}$ must be mutually independent for disjoint time steps. Strong error analysis (for example, establishing that $\\mathbb{E}\\big[\\sup_{0\\le n\\le N} |X_{t_n} - X_n|^p\\big]^{1/p}$ decays at a given rate as $h \\to 0$) relies critically on these increment properties to preserve the martingale structure and use tools such as the Burkholder–Davis–Gundy inequality. Therefore, the simulation must produce increments that are exactly Gaussian with variance $h$ and independent across steps.\n\nA principled way to obtain $\\Delta W_n \\sim N(0,h)$ is to draw independent standard normal random variables $Z_n \\sim N(0,1)$ and set\n$$\n\\Delta W_n = \\sqrt{h}\\, Z_n.\n$$\nThe scaling by $\\sqrt{h}$ ensures that $\\mathrm{Var}(\\Delta W_n) = h \\cdot \\mathrm{Var}(Z_n) = h$, and independence of the $Z_n$ implies independence of the $\\Delta W_n$. In practice with a pseudo-random number generator, one initializes the generator once to fix the stream and then calls it repeatedly to obtain independent draws; reseeding inside the loop is avoided because it can induce dependence or repetition.\n\nWe now analyze each option:\n\nA. Initialize a pseudo-random normal generator once, draw independent $Z_n \\sim N(0,1)$, and set $\\Delta W_n = \\sqrt{h}\\, Z_n$, without reseeding inside the loop. This matches the required distribution: $\\Delta W_n$ is Gaussian with mean $0$ and variance $h$ due to the $\\sqrt{h}$ scaling. Independence is achieved by using independent $Z_n$ draws. Not reseeding inside the loop is the correct practical approach to preserve independence along the generator’s stream. Verdict: Correct.\n\nB. Reseed the generator with the same fixed seed at every step, then draw $Z_n \\sim N(0,1)$ and set $\\Delta W_n = \\sqrt{h}\\, Z_n$. Reseeding with the same seed each time typically produces the same sequence element repeatedly, making $\\Delta W_n$ perfectly dependent across steps. Even if implementation details vary, this practice risks strong dependence and is not acceptable for strong error analysis, which requires independent increments. Verdict: Incorrect.\n\nC. Use independent uniform random variables $U_n \\sim \\mathrm{Unif}(-\\sqrt{3h},\\sqrt{3h})$ so that the mean is $0$ and variance is $h$, and set $\\Delta W_n = U_n$. While independence holds if the $U_n$ are independent and the first two moments match those of $N(0,h)$, the Brownian increment law is specifically Gaussian, not uniform. Replacing $N(0,h)$ by a non-Gaussian distribution invalidates pathwise properties needed for strong convergence analysis of schemes driven by $W_t$. Moment matching is sometimes used for weak approximation, but it is not appropriate for strong approximation of Brownian motion. Verdict: Incorrect.\n\nD. Use a correct normal generator but scale as $\\Delta W_n = h\\, Z_n$. This yields $\\mathrm{Var}(\\Delta W_n) = h^2$, not $h$, so the increments do not have the correct distribution. The correct scaling is $\\sqrt{h}$, not $h$. Verdict: Incorrect.\n\nE. Use antithetic variates across successive time steps: $\\Delta W_{2k} = \\sqrt{h}\\, Z$, $\\Delta W_{2k+1} = -\\sqrt{h}\\, Z$. Although each marginal has the correct $N(0,h)$ distribution, the construction imposes perfect negative correlation between adjacent increments, violating the independence of Brownian increments on disjoint intervals. Antithetic variates can be useful across independent replications, not across time steps of a single path. Verdict: Incorrect.\n\nTherefore, the only option that correctly simulates Brownian increments with the required distribution and independence across steps for strong error analysis is option A.", "answer": "$$\\boxed{A}$$", "id": "3079040"}, {"introduction": "Once the simulation is set up correctly, a key theoretical task is to predict how the global error behaves as the step size $h$ shrinks. For many important SDEs, such as the geometric Brownian motion model used in finance, the Euler-Maruyama method exhibits a strong convergence order of $1/2$, meaning the mean-square error scales linearly with $h$. This practice [@problem_id:3079059] guides you through a complete, first-principles derivation of this fundamental result, solidifying your understanding of how local errors accumulate and what determines the global rate of convergence.", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t}$ with initial condition $X_{0}=x_{0}$, where $\\mu\\in\\mathbb{R}$, $\\sigma\\in\\mathbb{R}$, and $\\{W_{t}\\}_{t\\ge 0}$ is a standard Brownian motion. Fix a terminal time $T>0$ and a uniform time-step $h=T/N$ with $N\\in\\mathbb{N}$. Let $\\{t_{n}\\}_{n=0}^{N}$ be the grid $t_{n}=nh$ and consider the Euler–Maruyama (EM) scheme defined by\n$$\nX_{n+1}^{h}=X_{n}^{h}+\\mu X_{n}^{h}\\,h+\\sigma X_{n}^{h}\\,\\Delta W_{n},\\qquad X_{0}^{h}=x_{0},\n$$\nwhere $\\Delta W_{n}=W_{t_{n+1}}-W_{t_{n}}$.\n\nUsing only foundational facts about geometric Brownian motion, independence and Gaussianity of Brownian increments, and basic properties of the moment generating function of a Gaussian random variable, do the following:\n\n- Derive a closed-form expression for the global strong mean-square error at the terminal time $T$, namely\n$$\n\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr],\n$$\nas an explicit function of $\\mu$, $\\sigma$, $T$, $x_{0}$, and $h$.\n\n- Then verify the strong order $h^{1/2}$ by determining the limit\n$$\nL=\\lim_{h\\to 0}\\frac{1}{h}\\,\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr].\n$$\n\nProvide all derivations starting from the exact solution of the SDE and the explicit product form of the EM solution. Justify each step using properties of Gaussian random variables and independence of increments. Do not assume any convergence rate a priori. Your final numerical target is the exact closed-form expression for $L$. Report $L$ as your final answer. No rounding is required and no units are involved. Express your final answer symbolically in terms of $\\mu$, $\\sigma$, $T$, and $x_{0}$ only.", "solution": "The objective is to derive a closed-form expression for the global strong mean-square error $\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr]$ for the Euler–Maruyama (EM) approximation of a geometric Brownian motion, and then to compute the limit $L=\\lim_{h\\to 0}\\frac{1}{h}\\,\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr]$.\n\nThe scalar stochastic differential equation (SDE) is given by\n$$dX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t}$$\nwith initial condition $X_{0}=x_{0}$. This is a linear SDE, commonly known as geometric Brownian motion. Its exact solution at time $t$ is\n$$X_{t} = x_{0}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)t + \\sigma W_{t}\\right)$$\nAt the terminal time $T$, the exact solution is\n$$X_{T} = x_{0}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T + \\sigma W_{T}\\right)$$\n\nThe Euler-Maruyama (EM) scheme is given by the recurrence relation\n$$X_{n+1}^{h}=X_{n}^{h}(1+\\mu h+\\sigma\\Delta W_{n})$$\nwith $X_{0}^{h}=x_{0}$, where $\\Delta W_{n} = W_{t_{n+1}}-W_{t_{n}}$ are independent random variables, each distributed as a normal distribution $\\mathcal{N}(0, h)$. Unfolding the recurrence from $n=0$ to $N-1$ gives the explicit solution for the numerical approximation at time $T=t_N=Nh$:\n$$X_{N}^{h} = x_{0}\\prod_{n=0}^{N-1}(1+\\mu h+\\sigma\\Delta W_{n})$$\n\nWe want to compute the mean-square error, which can be expanded as:\n$$\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr] = \\mathbb{E}[X_{T}^{2}] - 2\\mathbb{E}[X_{T}X_{N}^{h}] + \\mathbb{E}[(X_{N}^{h})^{2}]$$\nWe will compute each of the three terms on the right-hand side separately.\n\n**1. Calculation of $\\mathbb{E}[X_{T}^{2}]$**\nThe square of the exact solution is\n$$X_{T}^{2} = x_{0}^{2}\\exp\\left(2\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T + 2\\sigma W_{T}\\right) = x_{0}^{2}\\exp\\left((2\\mu-\\sigma^{2})T + 2\\sigma W_{T}\\right)$$\nTo find its expectation, we use the moment-generating function (MGF) of a Gaussian random variable. Since $W_{T} \\sim \\mathcal{N}(0, T)$, its MGF is $M_{W_{T}}(k) = \\mathbb{E}[\\exp(k W_{T})] = \\exp(\\frac{1}{2}k^{2}T)$.\n$$\\mathbb{E}[X_{T}^{2}] = x_{0}^{2}\\exp\\left((2\\mu-\\sigma^{2})T\\right)\\mathbb{E}[\\exp(2\\sigma W_{T})]$$\nSetting $k=2\\sigma$ in the MGF, we get $\\mathbb{E}[\\exp(2\\sigma W_{T})] = \\exp(\\frac{1}{2}(2\\sigma)^{2}T) = \\exp(2\\sigma^{2}T)$.\nTherefore,\n$$\\mathbb{E}[X_{T}^{2}] = x_{0}^{2}\\exp\\left((2\\mu-\\sigma^{2})T\\right)\\exp(2\\sigma^{2}T) = x_{0}^{2}\\exp\\left((2\\mu+\\sigma^{2})T\\right)$$\n\n**2. Calculation of $\\mathbb{E}[(X_{N}^{h})^{2}]$**\nThe square of the numerical solution is\n$$(X_{N}^{h})^{2} = x_{0}^{2}\\prod_{n=0}^{N-1}(1+\\mu h+\\sigma\\Delta W_{n})^{2}$$\nDue to the independence of the Brownian increments $\\Delta W_{n}$, the expectation of the product is the product of the expectations:\n$$\\mathbb{E}[(X_{N}^{h})^{2}] = x_{0}^{2}\\prod_{n=0}^{N-1}\\mathbb{E}\\left[(1+\\mu h+\\sigma\\Delta W_{n})^{2}\\right]$$\nFor each increment $\\Delta W_{n} \\sim \\mathcal{N}(0, h)$, we have $\\mathbb{E}[\\Delta W_{n}]=0$ and $\\mathbb{E}[(\\Delta W_{n})^{2}]=\\text{Var}(\\Delta W_{n})=h$.\nExpanding the square inside the expectation:\n$$\\mathbb{E}\\left[1+2\\mu h+\\mu^{2}h^{2} + 2(1+\\mu h)\\sigma\\Delta W_{n} + \\sigma^{2}(\\Delta W_{n})^{2}\\right] = 1+2\\mu h+\\mu^{2}h^{2} + 0 + \\sigma^{2}h$$\nSo, for each $n$, the expectation is $1+(2\\mu+\\sigma^{2})h+\\mu^{2}h^{2}$. Since there are $N$ such identical terms in the product:\n$$\\mathbb{E}[(X_{N}^{h})^{2}] = x_{0}^{2}\\left(1+(2\\mu+\\sigma^{2})h+\\mu^{2}h^{2}\\right)^{N}$$\n\n**3. Calculation of $\\mathbb{E}[X_{T}X_{N}^{h}]$**\nThis is the cross-term. We write $W_{T}=\\sum_{n=0}^{N-1}\\Delta W_{n}$.\n$$X_{T}X_{N}^{h} = x_{0}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T + \\sigma \\sum_{n=0}^{N-1}\\Delta W_{n}\\right) \\cdot x_{0}\\prod_{j=0}^{N-1}(1+\\mu h+\\sigma\\Delta W_{j})$$\n$$X_{T}X_{N}^{h} = x_{0}^{2}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T\\right) \\prod_{n=0}^{N-1}\\left[\\exp(\\sigma\\Delta W_{n})(1+\\mu h+\\sigma\\Delta W_{n})\\right]$$\nAgain, by independence of the increments $\\Delta W_{n}$:\n$$\\mathbb{E}[X_{T}X_{N}^{h}] = x_{0}^{2}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T\\right) \\prod_{n=0}^{N-1}\\mathbb{E}\\left[\\exp(\\sigma\\Delta W_{n})(1+\\mu h+\\sigma\\Delta W_{n})\\right]$$\nLet $\\Delta W \\sim \\mathcal{N}(0, h)$. We need to compute $\\mathbb{E}[\\exp(\\sigma\\Delta W)(1+\\mu h+\\sigma\\Delta W)] = (1+\\mu h)\\mathbb{E}[\\exp(\\sigma\\Delta W)]+\\sigma\\mathbb{E}[\\Delta W\\exp(\\sigma\\Delta W)]$.\nThe MGF of $\\Delta W$ is $M_{\\Delta W}(k) = \\exp(\\frac{1}{2}k^{2}h)$.\n$\\mathbb{E}[\\exp(\\sigma\\Delta W)] = M_{\\Delta W}(\\sigma) = \\exp(\\frac{1}{2}\\sigma^{2}h)$.\n$\\mathbb{E}[\\Delta W\\exp(\\sigma\\Delta W)]$ is the first derivative of the MGF with respect to $k$, evaluated at $k=\\sigma$.\n$\\frac{d}{dk}M_{\\Delta W}(k) = kh \\exp(\\frac{1}{2}k^{2}h)$. At $k=\\sigma$, this is $\\sigma h \\exp(\\frac{1}{2}\\sigma^{2}h)$.\nSo, the single-step expectation is $(1+\\mu h)\\exp(\\frac{1}{2}\\sigma^{2}h) + \\sigma(\\sigma h \\exp(\\frac{1}{2}\\sigma^{2}h)) = (1+\\mu h+\\sigma^{2}h)\\exp(\\frac{1}{2}\\sigma^{2}h)$.\nSubstituting this into the product:\n$$\\mathbb{E}[X_{T}X_{N}^{h}] = x_{0}^{2}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T\\right) \\left[(1+(\\mu+\\sigma^{2})h)\\exp\\left(\\frac{1}{2}\\sigma^{2}h\\right)\\right]^{N}$$\n\n**Closed-form expression for the error**\nCombining the three terms, and using $N=T/h$, we obtain the exact closed-form expression for the mean-square error:\n$$ \\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr] = x_{0}^{2}\\exp\\left((2\\mu+\\sigma^{2})T\\right) - 2x_{0}^{2}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T\\right)\\left[(1+(\\mu+\\sigma^{2})h)\\exp\\left(\\frac{1}{2}\\sigma^{2}h\\right)\\right]^{T/h} + x_{0}^{2}\\left(1+(2\\mu+\\sigma^{2})h+\\mu^{2}h^{2}\\right)^{T/h} $$\n\n**Asymptotic analysis and determination of the limit $L$**\nTo find $L$, we need to analyze the behavior of the error as $h\\to 0$. This requires expanding the terms in powers of $h$.\nWe use the general expansion for small $x$: $\\ln(1+x)=x-\\frac{x^{2}}{2}+O(x^{3})$. Then $(1+ah+bh^{2})^{T/h} = \\exp(\\frac{T}{h}\\ln(1+ah+bh^{2}))$.\n$$\\frac{T}{h}\\ln(1+ah+bh^{2}) = \\frac{T}{h}\\left((ah+bh^{2})-\\frac{1}{2}(ah)^{2}+O(h^{3})\\right) = T\\left(a+\\left(b-\\frac{a^{2}}{2}\\right)h+O(h^{2})\\right)$$\nSo, $(1+ah+bh^{2})^{T/h} = \\exp(aT)\\exp\\left(T\\left(b-\\frac{a^{2}}{2}\\right)h+O(h^{2})\\right) = \\exp(aT)\\left[1+T\\left(b-\\frac{a^{2}}{2}\\right)h+O(h^{2})\\right]$.\n\nFor $\\mathbb{E}[(X_{N}^{h})^{2}]$, we have $a=2\\mu+\\sigma^{2}$ and $b=\\mu^{2}$.\n$$\\mathbb{E}[(X_{N}^{h})^{2}] = x_{0}^{2}\\exp((2\\mu+\\sigma^{2})T)\\left[1+T\\left(\\mu^{2}-\\frac{(2\\mu+\\sigma^{2})^{2}}{2}\\right)h+O(h^{2})\\right]$$\n\nFor the cross-term $\\mathbb{E}[X_{T}X_{N}^{h}]$, we first expand the term inside the product:\n$$(1+(\\mu+\\sigma^{2})h)\\left(1+\\frac{\\sigma^{2}}{2}h+\\frac{\\sigma^{4}}{8}h^{2}+O(h^{3})\\right) = 1+\\left(\\mu+\\frac{3}{2}\\sigma^{2}\\right)h+\\left(\\frac{\\mu\\sigma^{2}}{2}+\\frac{5}{8}\\sigma^{4}\\right)h^{2}+O(h^{3})$$\nLet $a' = \\mu+\\frac{3}{2}\\sigma^{2}$ and $b' = \\frac{\\mu\\sigma^{2}}{2}+\\frac{5}{8}\\sigma^{4}$. Then the product term becomes $\\left(1+a'h+b'h^2+O(h^3)\\right)^N$.\nIts expansion for small $h$ is $\\exp(a'T)\\left[1+T\\left(b'-\\frac{(a')^{2}}{2}\\right)h+O(h^{2})\\right]$.\nSo,\n$$\\mathbb{E}[X_{T}X_{N}^{h}] = x_{0}^{2}\\exp\\left(\\left(\\mu-\\frac{1}{2}\\sigma^{2}\\right)T\\right)\\exp\\left(\\left(\\mu+\\frac{3}{2}\\sigma^{2}\\right)T\\right)\\left[1+T\\left(b'-\\frac{(a')^{2}}{2}\\right)h+O(h^{2})\\right]$$\nThe pre-factor is $x_{0}^{2}\\exp\\left((2\\mu+\\sigma^{2})T\\right)$. Let $K=x_{0}^{2}\\exp\\left((2\\mu+\\sigma^{2})T\\right)$.\n\nThe error is: $\\mathbb{E}[|X_{T}-X_{N}^{h}|^{2}] = \\mathbb{E}[X_T^2] - 2\\mathbb{E}[X_{T}X_{N}^{h}] + \\mathbb{E}[(X_{N}^{h})^{2}]$. The leading terms $K-2K+K=0$, as expected. We collect the $O(h)$ terms.\n$$\\frac{\\mathbb{E}[|...|^{2}]}{h} \\approx K T \\left[\\left(\\mu^{2}-\\frac{(2\\mu+\\sigma^{2})^{2}}{2}\\right)-2\\left(\\left(\\frac{\\mu\\sigma^{2}}{2}+\\frac{5}{8}\\sigma^{4}\\right)-\\frac{(\\mu+\\frac{3}{2}\\sigma^{2})^{2}}{2}\\right)\\right]$$\nLet's compute the two main parts of the coefficient:\nPart 1: $\\mu^{2}-\\frac{(2\\mu+\\sigma^{2})^{2}}{2} = \\mu^{2}-\\frac{1}{2}(4\\mu^{2}+4\\mu\\sigma^{2}+\\sigma^{4}) = -\\mu^{2}-2\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4}$.\nPart 2: $\\left(\\frac{\\mu\\sigma^{2}}{2}+\\frac{5}{8}\\sigma^{4}\\right)-\\frac{1}{2}(\\mu+\\frac{3}{2}\\sigma^{2})^{2} = \\frac{\\mu\\sigma^{2}}{2}+\\frac{5}{8}\\sigma^{4}-\\frac{1}{2}(\\mu^{2}+3\\mu\\sigma^{2}+\\frac{9}{4}\\sigma^{4}) = -\\frac{\\mu^{2}}{2}-\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4}$.\nThe coefficient of $h$ in the error expression is $KT$ times:\n$$ \\left(-\\mu^{2}-2\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4}\\right) - 2\\left(-\\frac{\\mu^{2}}{2}-\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4}\\right) $$\n$$ = -\\mu^{2}-2\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4} + \\mu^{2}+2\\mu\\sigma^{2}+\\sigma^{4} = \\frac{1}{2}\\sigma^{4} $$\nThus, we found that:\n$$\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr] = K T \\frac{\\sigma^{4}}{2} h + O(h^{2}) = x_{0}^{2}\\exp((2\\mu+\\sigma^{2})T)\\frac{T\\sigma^{4}}{2} h + O(h^{2})$$\nThis result shows that the mean-square error is of order $h$, which verifies a strong convergence order of $p=1/2$ since the mean-square error is $O(h^{2p})$.\n\nThe limit $L$ is a direct consequence of this expansion:\n$$L = \\lim_{h\\to 0}\\frac{1}{h}\\,\\mathbb{E}\\bigl[\\,|X_{T}-X_{N}^{h}|^{2}\\,\\bigr] = \\lim_{h\\to 0}\\frac{1}{h}\\left(x_{0}^{2}\\exp((2\\mu+\\sigma^{2})T)\\frac{T\\sigma^{4}}{2} h + O(h^{2})\\right)$$\n$$L = \\frac{1}{2}x_{0}^{2}\\sigma^{4}T\\exp\\left((2\\mu+\\sigma^{2})T\\right)$$", "answer": "$$\n\\boxed{\\frac{1}{2}x_{0}^{2}\\sigma^{4}T\\exp\\left((2\\mu+\\sigma^{2})T\\right)}\n$$", "id": "3079059"}, {"introduction": "Theoretical error rates provide a vital benchmark, but in practice, we must confirm them with computational experiments and careful statistical analysis. This requires generating multiple sample paths to estimate the error and quantifying the uncertainty in that estimate. Using tools like the Central Limit Theorem and the delta method, we can construct confidence intervals for both the error and the observed convergence rate. This problem [@problem_id:3079024] immerses you in the practical workflow of a computational scientist, teaching you how to use simulation data not just to obtain a result, but to statistically validate a theoretical convergence order from a log-log plot.", "problem": "Consider a scalar Itô stochastic differential equation with additive noise, for which the Euler–Maruyama method is known to have strong convergence of order $r \\approx 1$. Let $X_T$ denote the exact solution at terminal time $T$, and let $X_T^h$ denote the numerical approximation with time step $h$. Define the strong error at terminal time $T$ in mean-square as $e(h) \\equiv \\left(\\mathbb{E}\\lvert X_T - X_T^h\\rvert^2\\right)^{1/2}$. You approximate $e(h)$ by Monte Carlo (MC), using a very fine reference solution to represent $X_T$. For a given $h$, let $Z(h) \\equiv \\lvert X_T - X_T^h\\rvert^2$. From $M$ independent and identically distributed MC replicates $\\{Z^{(m)}(h)\\}_{m=1}^M$, you record the sample mean $\\overline{Z}(h)$ and sample standard deviation $s_Z(h)$. Assume $M$ is large so that the Central Limit Theorem (CLT) applies to $\\overline{Z}(h)$.\n\nYou run this procedure for three step sizes $h \\in \\{0.25, 0.125, 0.0625\\}$, with $M = 1000$, and obtain the following summary statistics:\n- For $h = 0.25$: $\\overline{Z} = 0.0100$, $s_Z = 0.012$.\n- For $h = 0.125$: $\\overline{Z} = 0.0025$, $s_Z = 0.0032$.\n- For $h = 0.0625$: $\\overline{Z} = 0.00062$, $s_Z = 0.00085$.\n\nYou seek to:\n(i) construct $95\\%$ confidence intervals for $e(h)$ at these $h$ values based on the CLT and appropriate transformations; and\n(ii) use the resulting error bars to obtain a reliable estimate of the convergence rate $r$ from the log–log relation $e(h) \\approx C h^r$, including a conservative confidence interval for $r$.\n\nSelect all statements below that are correct.\n\nA. A valid $95\\%$ confidence interval for $e(0.25)$ is obtained by first forming a $95\\%$ confidence interval for $\\mu_Z(0.25) \\equiv \\mathbb{E}[Z(0.25)]$ as $\\overline{Z} \\pm 1.96\\, s_Z/\\sqrt{M}$, and then applying the monotone map $x \\mapsto \\sqrt{x}$ to the interval endpoints, yielding approximately $\\left[\\sqrt{0.0100 - 1.96 \\cdot 0.012/\\sqrt{1000}},\\ \\sqrt{0.0100 + 1.96 \\cdot 0.012/\\sqrt{1000}}\\right] \\approx [0.0963,\\ 0.1037]$.\n\nB. Using the delta method with the transformation $g(x) = \\sqrt{x}$ at $\\mu_Z(0.25) \\approx \\overline{Z} = 0.0100$ gives an approximate $95\\%$ confidence interval for $e(0.25)$ of the form $g(\\overline{Z}) \\pm 1.96\\, g'(\\overline{Z}) \\cdot s_Z/\\sqrt{M}$, i.e., approximately $0.10 \\pm 1.96 \\cdot \\left(\\frac{1}{2 \\sqrt{0.0100}}\\right)\\cdot \\frac{0.012}{\\sqrt{1000}} \\approx [0.0963,\\ 0.1037]$.\n\nC. A slope estimate of $r$ from the log–log plot based on $y_i \\equiv \\log e(h_i)$ versus $x_i \\equiv \\log h_i$ is approximately $r \\approx 1.00$ for the three given $h$ values; a conservative $95\\%$ confidence interval for $r$, obtained by combining the $95\\%$ confidence intervals for $e(0.25)$ and $e(0.0625)$ through the monotone map $(\\log e(0.0625) - \\log e(0.25))/(\\log 0.0625 - \\log 0.25)$, is approximately $[0.95,\\ 1.06]$.\n\nD. If one regresses $\\log \\overline{Z}(h)$ on $\\log h$, the slope of this regression directly estimates the strong order $r$ for $e(h)$ without any adjustment.\n\nE. A $95\\%$ confidence interval for $\\mu_Z(0.0625)$ is $[0.00054,\\ 0.00070]$, which, upon taking square roots of the endpoints, gives a valid $95\\%$ interval for $e(0.0625)$ of approximately $[0.0232,\\ 0.0265]$.\n\nChoose all that apply.", "solution": "### Solution Derivation and Option Analysis\n\nThe core of this problem lies in constructing confidence intervals for a parameter and for a transformed version of that parameter. The parameter of primary interest is the mean-square error $e(h)$. The Monte Carlo simulation provides an estimate for $\\mu_Z(h) = \\mathbb{E}[Z(h)] = [e(h)]^2$.\n\nBy the CLT, for a large sample size $M$, the sample mean $\\overline{Z}(h)$ is approximately normally distributed around the true mean $\\mu_Z(h)$ with a standard deviation estimated by the standard error of the mean, $SE(\\overline{Z}(h)) = s_Z(h)/\\sqrt{M}$.\nA $95\\%$ confidence interval for $\\mu_Z(h)$ is given by $\\overline{Z}(h) \\pm z_{0.025} \\frac{s_Z(h)}{\\sqrt{M}}$, where $z_{0.025} \\approx 1.96$ is the critical value for the standard normal distribution. Given $M=1000$, $\\sqrt{M} \\approx 31.623$.\n\nWe want a CI for $e(h) = \\sqrt{\\mu_Z(h)}$. This requires transforming the CI for $\\mu_Z(h)$.\n\n**Analysis of Option A:**\nThis option proposes to find the CI for $e(0.25)$ by first constructing a CI for $\\mu_Z(0.25)$ and then applying the function $g(x) = \\sqrt{x}$ to the endpoints of that interval. Since $g(x)$ is a monotonically increasing function for $x>0$, this is a standard and valid method for transforming a confidence interval.\n\nLet's verify the calculation for $h=0.25$:\n- $\\overline{Z}(0.25) = 0.0100$, $s_Z(0.25) = 0.012$.\n- The standard error is $SE(\\overline{Z}(0.25)) = 0.012 / \\sqrt{1000} \\approx 0.0003795$.\n- The margin of error is $1.96 \\times SE(\\overline{Z}(0.25)) \\approx 1.96 \\times 0.0003795 \\approx 0.0007438$.\n- The $95\\%$ CI for $\\mu_Z(0.25)$ is $0.0100 \\pm 0.0007438$, which is approximately $[0.009256, 0.010744]$.\n- Applying the square root transformation to the endpoints gives the CI for $e(0.25)$:\n  - Lower bound: $\\sqrt{0.009256} \\approx 0.09621$.\n  - Upper bound: $\\sqrt{0.010744} \\approx 0.10365$.\n- So, the CI is approximately $[0.0962, 0.1037]$. The option states the interval is approximately $[0.0963, 0.1037]$. This is a very close approximation, differing only in the fourth decimal place of the lower bound. The methodology is entirely correct.\n- Verdict: **Correct**.\n\n**Analysis of Option B:**\nThis option proposes using the delta method. For a function $g(x)$, the delta method approximates the variance of $g(\\overline{Z})$ as $\\text{Var}(g(\\overline{Z})) \\approx [g'(\\mu_Z)]^2 \\text{Var}(\\overline{Z})$. The resulting CI for $g(\\mu_Z)$ is $g(\\overline{Z}) \\pm z_{\\alpha/2} |g'(\\overline{Z})| SE(\\overline{Z})$. This is a standard asymptotic method.\n\nLet's verify the calculation for $h=0.25$ with $g(x)=\\sqrt{x}$:\n- Point estimate for $e(0.25)$ is $g(\\overline{Z}(0.25)) = \\sqrt{0.0100} = 0.1$.\n- The derivative is $g'(x) = 1/(2\\sqrt{x})$. At $\\overline{Z}=0.01$, $g'(\\overline{Z}) = 1/(2\\sqrt{0.01}) = 1/0.2 = 5$.\n- The standard error for $e(0.25)$ is approximately $|g'(\\overline{Z})| \\times SE(\\overline{Z}) \\approx 5 \\times 0.0003795 = 0.0018975$.\n- The margin of error for $e(0.25)$ is $1.96 \\times 0.0018975 \\approx 0.003719$.\n- The $95\\%$ CI is $0.1 \\pm 0.003719$, which is $[0.096281, 0.103719]$, approximately $[0.0963, 0.1037]$.\n- The formula and the numerical result in the option are consistent with this derivation. The delta method is a valid procedure.\n- Verdict: **Correct**.\n\n**Analysis of Option D:**\nThis option claims that the slope of a regression of $\\log \\overline{Z}(h)$ on $\\log h$ directly estimates $r$.\nFrom the problem definition, we have the relations:\n$e(h) \\approx C h^r$\n$e(h) = \\sqrt{\\mu_Z(h)}$\nAn estimator for $e(h)$ is $\\sqrt{\\overline{Z}(h)}$.\nSo, $\\sqrt{\\overline{Z}(h)} \\approx C h^r$.\nTaking the natural logarithm of both sides:\n$\\log\\left(\\sqrt{\\overline{Z}(h)}\\right) = \\log(C) + r \\log(h)$\n$\\frac{1}{2} \\log\\left(\\overline{Z}(h)\\right) = \\log(C) + r \\log(h)$\nRearranging for $\\log(\\overline{Z}(h))$ yields:\n$\\log(\\overline{Z}(h)) = 2\\log(C) + 2r \\log(h)$\nThis is a linear relationship of the form $y = a + b x$ where $y = \\log(\\overline{Z}(h))$ and $x = \\log(h)$. The slope is $b = 2r$.\nTherefore, the slope of a regression of $\\log \\overline{Z}(h)$ on $\\log h$ provides an estimate for $2r$, not $r$. One must divide the slope by $2$. The statement that no adjustment is needed is false.\n- Verdict: **Incorrect**.\n\n**Analysis of Option E:**\nThis option provides a CI for $\\mu_Z(0.0625)$ and the subsequent CI for $e(0.0625)$.\nLet's calculate the $95\\%$ CI for $\\mu_Z(0.0625)$:\n- $\\overline{Z}(0.0625) = 0.00062$, $s_Z(0.0625) = 0.00085$.\n- $SE(\\overline{Z}(0.0625)) = 0.00085 / \\sqrt{1000} \\approx 0.00002688$.\n- Margin of error: $1.96 \\times 0.00002688 \\approx 0.00005268$.\n- The $95\\%$ CI for $\\mu_Z(0.0625)$ is $0.00062 \\pm 0.00005268$, which is $[0.00056732, 0.00067268]$, or approximately $[0.00057, 0.00067]$.\n- The option states the CI is $[0.00054, 0.00070]$. This is numerically incorrect. The stated interval is substantially wider than a $95\\%$ CI.\n- Although the second part of the statement (taking the square root of its own stated interval's endpoints) is arithmetically correct ($\\sqrt{0.00054} \\approx 0.0232$, $\\sqrt{0.00070} \\approx 0.0265$), the entire claim is invalidated by the incorrectness of its premise.\n- Verdict: **Incorrect**.\n\n**Analysis of Option C:**\nThis option makes two claims: an estimate for the convergence rate $r$ and a CI for $r$.\n\n(i) Estimate of $r$:\nThe rate $r$ is the slope of $\\log e(h)$ versus $\\log h$. We can estimate this slope using the point estimates $e(h_i) \\approx \\sqrt{\\overline{Z}(h_i)}$. Let's use the two extreme points, $h_1=0.25$ and $h_3=0.0625$, for a stable estimate.\n- Point estimate for $e(0.25) \\approx \\sqrt{0.0100} = 0.1$.\n- Point estimate for $e(0.0625) \\approx \\sqrt{0.00062} \\approx 0.0249$.\n- The slope estimate is $\\hat{r} = \\frac{\\log(e(h_3)) - \\log(e(h_1))}{\\log(h_3) - \\log(h_1)} = \\frac{\\log(0.0249) - \\log(0.1)}{\\log(0.0625) - \\log(0.25)} = \\frac{-3.6929 - (-2.3026)}{-2.7726 - (-1.3863)} = \\frac{-1.3903}{-1.3863} \\approx 1.0029$.\nAn estimate of $r \\approx 1.00$ is accurate.\n\n(ii) Confidence interval for $r$:\nThe proposal is to form a conservative CI for $r$ using the CIs for $e(0.25)$ and $e(0.0625)$. The formula for the slope is $r = \\frac{\\log e(h_3) - \\log e(h_1)}{\\log h_3 - \\log h_1}$. The denominator is $\\log(0.0625/0.25) = \\log(0.25) \\approx -1.3863$, which is negative. To find the CI for $r = [r_L, r_U]$, we must find the minimum and maximum values of this expression over the CIs for $e(h_1)$ and $e(h_3)$.\n- Let $[e_1^L, e_1^U]$ be the CI for $e(h_1)$. From A/B, this is approximately $[0.0963, 0.1037]$.\n- Let $[e_3^L, e_3^U]$ be the CI for $e(h_3)$. From our calculation for E, the correct CI for $\\mu_Z(h_3)$ is $[0.0005673, 0.0006727]$. Applying the square root gives $[e_3^L, e_3^U] \\approx [\\sqrt{0.0005673}, \\sqrt{0.0006727}] \\approx [0.02382, 0.02594]$.\n- The function $r(e_1, e_3) = (\\log e_3 - \\log e_1) / (\\text{negative constant})$ is increasing in $e_1$ and decreasing in $e_3$. To get the minimum $r$, we need minimum $e_1$ and maximum $e_3$.\n- Lower bound $r_L$: Use smallest $e_1$ ($e_1^L=0.0963$) and largest $e_3$ ($e_3^U=0.02594$).\n  $r_L = \\frac{\\log(0.02594) - \\log(0.0963)}{-1.3863} = \\frac{-3.652 - (-2.340)}{-1.3863} = \\frac{-1.312}{-1.3863} \\approx 0.946$.\n- Upper bound $r_U$: Use largest $e_1$ ($e_1^U=0.1037$) and smallest $e_3$ ($e_3^L=0.02382$).\n  $r_U = \\frac{\\log(0.02382) - \\log(0.1037)}{-1.3863} = \\frac{-3.737 - (-2.266)}{-1.3863} = \\frac{-1.471}{-1.3863} \\approx 1.061$.\nThis gives the interval $[0.946, 1.061]$. The option states CI is approximately $[0.95, 1.06]$. This is a correct match after rounding. The method is sound for obtaining a conservative interval.\n- Verdict: **Correct**.\n\nFinal evaluation yields A, B, and C as correct.", "answer": "$$\\boxed{ABC}$$", "id": "3079024"}]}