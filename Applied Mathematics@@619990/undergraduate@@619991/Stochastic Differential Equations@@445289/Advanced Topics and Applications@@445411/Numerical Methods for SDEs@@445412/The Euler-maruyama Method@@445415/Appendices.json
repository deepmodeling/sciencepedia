{"hands_on_practices": [{"introduction": "A cornerstone of numerical analysis is verifying that an algorithm performs as theory predicts. This first practice provides a hands-on method to estimate the strong convergence rate of the Euler-Maruyama scheme. You will apply the method to the Ornstein-Uhlenbeck process and, using a powerful technique called pathwise coupling, confirm the method's theoretical strong convergence order of $0.5$ without needing to know the exact solution [@problem_id:3226818]. This exercise builds foundational skills in designing and interpreting numerical experiments for stochastic differential equations.", "problem": "You are asked to design and implement a numerical experiment to estimate the empirical strong convergence rate of the Euler–Maruyama method for the Ornstein–Uhlenbeck stochastic differential equation. Consider the stochastic process defined by the stochastic differential equation\n$$\ndX_t = -\\lambda X_t\\,dt + \\sigma\\,dW_t,\\quad X_0=x_0,\n$$\nwhere $W_t$ is a standard Wiener process, $\\lambda \\ge 0$ and $\\sigma \\ge 0$ are constants, and $x_0 \\in \\mathbb{R}$ is deterministic.\n\nStart from the integral form of the stochastic differential equation over one time step of size $h0$,\n$$\nX_{t+h} - X_t = \\int_t^{t+h} -\\lambda X_s\\,ds + \\int_t^{t+h} \\sigma\\,dW_s,\n$$\nand the defining properties of the Itô integral and Wiener process: $W_{t+h}-W_t \\sim \\mathcal{N}(0,h)$, independent increments, and $\\mathbb{E}[W_{t+h}-W_t]=0$. From these bases, derive a first-order time-stepping approximation that updates an approximation $X_n \\approx X_{t_n}$ at times $t_n = nh$ using only local information over a step of size $h$. Do not use any prepackaged shortcut formulas beyond these fundamental definitions and facts.\n\nTo estimate a strong convergence rate without using a closed-form exact solution, use pathwise coupling across time steps. For a given level with coarse step size $h$, define the fine step size as $h/2$. On a common probability space, generate independent and identically distributed fine Brownian increments $\\Delta W^{(h/2)}_k \\sim \\mathcal{N}(0,h/2)$ and construct coarse increments by pairing adjacent fine increments,\n$$\n\\Delta W^{(h)}_j = \\Delta W^{(h/2)}_{2j} + \\Delta W^{(h/2)}_{2j+1}.\n$$\nSimulate the Euler–Maruyama approximation on the coarse grid using $\\Delta W^{(h)}_j$ and on the fine grid using $\\Delta W^{(h/2)}_k$, both starting from the same initial value $x_0$, to obtain terminal values $X_T^{(h)}$ and $X_T^{(h/2)}$ at $t=T$. Define the level-wise root-mean-square coupling error as\n$$\ne(h) = \\left(\\mathbb{E}\\left[\\left|X_T^{(h)} - X_T^{(h/2)}\\right|^2\\right]\\right)^{1/2}.\n$$\nFor a sequence of levels with step sizes $h_\\ell = T/(N_0\\,2^{\\ell-1})$ for $\\ell=1,2,\\dots,L$, estimate an empirical strong convergence rate $p$ by fitting a least-squares line to the data $\\{(\\log h_\\ell, \\log e(h_\\ell))\\}_{\\ell=1}^L$ and taking the slope as the estimate $p$.\n\nImplement a program that carries out this experiment under the following constraints and outputs numerical results for the specified test suite.\n\nImplementation requirements:\n- Use a pseudo-random number generator with fixed seed $123456$ for reproducibility.\n- For each stochastic test, approximate the expectation $\\mathbb{E}[\\cdot]$ by a Monte Carlo average over $N_{\\text{paths}}$ independent coupled sample paths. For the deterministic case $\\sigma=0$, no randomness is needed; compute the errors exactly from the deterministic iterations.\n- Use coupling of Brownian increments exactly as specified above for the stochastic cases $\\sigma0$.\n- For each stochastic level, compute $e(h)$ from the Monte Carlo estimate\n$$\ne(h) \\approx \\left(\\frac{1}{N_{\\text{paths}}}\\sum_{i=1}^{N_{\\text{paths}}} \\left|X_{T,i}^{(h)} - X_{T,i}^{(h/2)}\\right|^2\\right)^{1/2}.\n$$\n- For each case where $\\sigma0$, estimate the slope $p$ by least-squares regression on $\\{(\\log h_\\ell,\\log e(h_\\ell))\\}$ across all levels where $e(h_\\ell)0$.\n- For the degenerate Brownian case $\\lambda=0$ and $\\sigma0$, report the largest value of $e(h_\\ell)$ over the levels rather than a slope.\n- For the deterministic case $\\sigma=0$, estimate the slope $p$ using the same regression but without randomness.\n\nTest suite:\n- Case $1$ (typical stochastic): $\\lambda=1.2$, $\\sigma=0.8$, $T=1.0$, $x_0=1.0$, $N_0=16$, $L=4$, $N_{\\text{paths}}=12000$.\n- Case $2$ (stiffer stochastic): $\\lambda=4.0$, $\\sigma=0.3$, $T=1.0$, $x_0=0.1$, $N_0=64$, $L=3$, $N_{\\text{paths}}=8000$.\n- Case $3$ (pure Brownian limit): $\\lambda=0.0$, $\\sigma=0.9$, $T=1.0$, $x_0=0.0$, $N_0=16$, $L=4$, $N_{\\text{paths}}=20000$. Return $\\max_{\\ell} e(h_\\ell)$ for this case.\n- Case $4$ (deterministic limit): $\\lambda=1.5$, $\\sigma=0.0$, $T=1.0$, $x_0=2.0$, $N_0=16$, $L=4$. Return the slope $p$.\n\nFinal output format:\n- Your program must produce a single line containing all results in order for Cases $1$ through $4$ as a comma-separated list enclosed in square brackets, for example [$r_1$,$r_2$,$r_3$,$r_4$], where each $r_k$ is a real number rounded to three decimal places.\n- There are no physical units to specify.\n\nYour program must be self-contained, must not read any input, and must not access files or networks. It must use only standard libraries and numerical arrays. The pseudo-random number generator seed must be set to $123456$ at the start of the simulation.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It poses a standard task in the numerical analysis of stochastic differential equations (SDEs) with a clear and complete set of instructions.\n\nThe objective is to derive the Euler-Maruyama method for the Ornstein-Uhlenbeck SDE and then use it to numerically estimate its strong convergence rate. The Ornstein-Uhlenbeck process is defined by the SDE:\n$$\ndX_t = -\\lambda X_t\\,dt + \\sigma\\,dW_t, \\quad X_0=x_0\n$$\nwhere $\\lambda \\ge 0$ and $\\sigma \\ge 0$ are constants, $x_0$ is the deterministic initial condition, and $W_t$ is a standard Wiener process.\n\nFirst, we derive the numerical time-stepping scheme. The problem statement provides the integral form of the SDE over a time interval $[t, t+h]$:\n$$\nX_{t+h} - X_t = \\int_t^{t+h} -\\lambda X_s\\,ds + \\int_t^{t+h} \\sigma\\,dW_s\n$$\nTo derive a first-order approximation, we approximate the integrals using information available at the beginning of the interval, time $t$. Let $t_n = nh$ for an integer $n \\ge 0$ and a step size $h  0$. We denote the numerical approximation of $X_{t_n}$ as $X_n$. The update from $X_n$ to $X_{n+1}$ is derived by approximating the SDE over $[t_n, t_{n+1}]$.\n\nThe drift term integral is approximated using a left-hand rectangular rule. We assume that for a small step size $h$, the process $X_s$ does not change significantly from its value at the start of the interval, $X_{t_n}$. Thus, we make the approximation $X_s \\approx X_{t_n}$ for $s \\in [t_n, t_{n+1}]$:\n$$\n\\int_{t_n}^{t_{n+1}} -\\lambda X_s\\,ds \\approx \\int_{t_n}^{t_{n+1}} -\\lambda X_{t_n}\\,ds = -\\lambda X_{t_n} \\int_{t_n}^{t_{n+1}} ds = -\\lambda X_{t_n} h\n$$\nIn terms of our numerical approximation, this becomes $-\\lambda X_n h$.\n\nThe diffusion term is an Itô stochastic integral. Since $\\sigma$ is a constant, it can be factored out:\n$$\n\\int_{t_n}^{t_{n+1}} \\sigma\\,dW_s = \\sigma \\int_{t_n}^{t_{n+1}} dW_s = \\sigma (W_{t_{n+1}} - W_{t_n})\n$$\nThe term $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is the increment of the Wiener process over the interval $[t_n, t_{n+1}]$. Based on the fundamental properties of the Wiener process, these increments are independent for non-overlapping intervals and are normally distributed with a mean of $0$ and a variance equal to the length of the time interval, $h$. Therefore, $\\Delta W_n \\sim \\mathcal{N}(0,h)$. In a numerical implementation, we can generate this increment as $\\Delta W_n = \\sqrt{h} Z_n$, where $Z_n$ is a random variable drawn from the standard normal distribution $\\mathcal{N}(0,1)$.\n\nCombining these two approximations, we replace the exact values $X_{t_n}$ and $X_{t_{n+1}}$ with their numerical counterparts $X_n$ and $X_{n+1}$:\n$$\nX_{n+1} - X_n \\approx -\\lambda X_n h + \\sigma \\Delta W_n\n$$\nRearranging gives the explicit Euler-Maruyama update rule for the Ornstein-Uhlenbeck process:\n$$\nX_{n+1} = X_n - \\lambda X_n h + \\sigma \\Delta W_n = (1 - \\lambda h)X_n + \\sigma \\Delta W_n\n$$\nThis scheme is first-order in time and uses only local information at time $t_n$ to advance the solution to $t_{n+1}$.\n\nThe second part of the problem is to design a numerical experiment to estimate the strong convergence rate $p$. The strong error is concerned with the pathwise difference between the numerical and exact solutions. The rate $p$ is defined such that the error at a fixed time $T$, $\\mathbb{E}[|X_N^{(h)} - X_T|]$, is proportional to $h^p$ as the step size $h \\to 0$.\n\nSince the exact solution is not to be used, we estimate the rate by comparing solutions from two different step sizes, a coarse step $h$ and a fine step $h/2$. Crucially, for the comparison to be valid, both simulations must be driven by the same underlying random process. This is achieved by pathwise coupling. We generate Brownian increments $\\Delta W^{(h/2)}_k \\sim \\mathcal{N}(0, h/2)$ for the fine grid. The coarse grid increments $\\Delta W^{(h)}_j$ are then constructed by summing pairs of adjacent fine increments: $\\Delta W^{(h)}_j = \\Delta W^{(h/2)}_{2j} + \\Delta W^{(h/2)}_{2j+1}$. By the properties of normal variables, this construction correctly yields coarse increments with the distribution $\\mathcal{N}(0,h)$.\n\nFor a sequence of coarse step sizes $h_\\ell = T/(N_0 2^{\\ell-1})$, we compute numerical solutions on both coarse and fine grids, $X_T^{(h_\\ell)}$ and $X_T^{(h_\\ell/2)}$, starting from the same $x_0$ and using the coupled increments. The root-mean-square error between these two approximations is defined as $e(h_\\ell) = (\\mathbb{E}[|X_T^{(h_\\ell)} - X_T^{(h_\\ell/2)}|^2])^{1/2}$. The expectation $\\mathbb{E}[\\cdot]$ is estimated by a Monte Carlo average over a large number of independent path pairs.\n\nAssuming the error behaves as $e(h) \\approx C h^p$ for some constant $C$, taking the logarithm gives $\\log e(h) \\approx \\log C + p \\log h$. This is a linear relationship between $\\log e(h)$ and $\\log h$. The convergence rate $p$ can therefore be estimated as the slope of a least-squares line fitted to the set of points $\\{(\\log h_\\ell, \\log e(h_\\ell))\\}$.\n\nSpecial cases are handled as specified:\n-   For the deterministic case ($\\sigma=0$), the problem reduces to the Forward Euler method for an ODE. The error is computed directly without Monte Carlo, and the rate $p$ is found by regression. We expect $p \\approx 1$.\n-   For the pure Brownian case ($\\lambda=0$), the Euler-Maruyama scheme is exact, meaning $X_T^{(h)} - X_T^{(h/2)} = 0$ for coupled paths. The error $e(h)$ will be zero (or close to it due to floating-point precision). The requested output is the maximum error observed, which should be near zero.\n-   For standard stochastic cases ($\\sigma0, \\lambda0$), the strong convergence rate is theoretically $p=0.5$. The numerical experiment should yield a slope close to this value.\n\nThe following program implements this entire procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (lambda, sigma, T, x0, N0, L, N_paths)\n        {'lambda_val': 1.2, 'sigma': 0.8, 'T': 1.0, 'x0': 1.0, 'N0': 16, 'L': 4, 'N_paths': 12000, 'case_type': 'slope'},\n        # Case 2: (lambda, sigma, T, x0, N0, L, N_paths)\n        {'lambda_val': 4.0, 'sigma': 0.3, 'T': 1.0, 'x0': 0.1, 'N0': 64, 'L': 3, 'N_paths': 8000, 'case_type': 'slope'},\n        # Case 3: (lambda, sigma, T, x0, N0, L, N_paths)\n        {'lambda_val': 0.0, 'sigma': 0.9, 'T': 1.0, 'x0': 0.0, 'N0': 16, 'L': 4, 'N_paths': 20000, 'case_type': 'max_error'},\n        # Case 4: (lambda, sigma, T, x0, N0, L, N_paths)\n        {'lambda_val': 1.5, 'sigma': 0.0, 'T': 1.0, 'x0': 2.0, 'N0': 16, 'L': 4, 'N_paths': None, 'case_type': 'slope'},\n    ]\n\n    # Initialize a single random number generator for the entire suite for reproducibility.\n    rng = np.random.default_rng(123456)\n    \n    results = []\n    \n    for case_params in test_cases:\n        result = run_single_case(rng, **case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_single_case(rng, lambda_val, sigma, T, x0, N0, L, N_paths, case_type):\n    \"\"\"\n    Executes the simulation for a single test case.\n    \"\"\"\n    log_h_values = []\n    log_e_values = []\n    e_values = []\n\n    for l in range(1, L + 1):\n        h_coarse = T / (N0 * (2**(l - 1)))\n        h_fine = h_coarse / 2.0\n        \n        n_coarse = int(round(T / h_coarse))\n        n_fine = int(round(T / h_fine))\n\n        if sigma == 0.0:  # Deterministic case\n            # Simulate one path without randomness\n            X_coarse = float(x0)\n            for _ in range(n_coarse):\n                X_coarse = (1.0 - lambda_val * h_coarse) * X_coarse\n            \n            X_fine = float(x0)\n            for _ in range(n_fine):\n                X_fine = (1.0 - lambda_val * h_fine) * X_fine\n\n            e_h = np.abs(X_coarse - X_fine)\n        else:  # Stochastic case\n            # Generate fine-grid Brownian increments for all paths\n            delta_W_fine = rng.normal(loc=0.0, scale=np.sqrt(h_fine), size=(N_paths, n_fine))\n            \n            # Construct coarse-grid increments by coupling\n            delta_W_coarse = delta_W_fine[:, 0::2] + delta_W_fine[:, 1::2]\n\n            # Initialize paths\n            X_coarse = np.full(N_paths, x0, dtype=float)\n            X_fine = np.full(N_paths, x0, dtype=float)\n\n            # Evolve coarse paths\n            for j in range(n_coarse):\n                X_coarse = (1.0 - lambda_val * h_coarse) * X_coarse + sigma * delta_W_coarse[:, j]\n\n            # Evolve fine paths\n            for k in range(n_fine):\n                X_fine = (1.0 - lambda_val * h_fine) * X_fine + sigma * delta_W_fine[:, k]\n            \n            # Calculate RMS error for the level\n            squared_errors = (X_coarse - X_fine)**2\n            mean_squared_error = np.mean(squared_errors)\n            e_h = np.sqrt(mean_squared_error)\n\n        e_values.append(e_h)\n        if e_h > 0:\n            log_h_values.append(np.log(h_coarse))\n            log_e_values.append(np.log(e_h))\n\n    # Determine the final result based on the case type\n    if case_type == 'max_error':\n        return max(e_values)\n    elif case_type == 'slope':\n        # Perform least-squares regression if there are enough data points\n        if len(log_h_values)  2:\n            return np.nan  # Not enough points to fit a line\n\n        x = np.array(log_h_values, dtype=float)\n        y = np.array(log_e_values, dtype=float)\n        \n        # Using numpy's polyfit to get the slope of the best-fit line\n        # This is equivalent to manual least-squares calculation.\n        slope, _ = np.polyfit(x, y, 1)\n        return slope\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3226818"}, {"introduction": "While the Euler-Maruyama method is a versatile tool, it is crucial to understand its limitations. This practice explores one such critical flaw by applying the method to Geometric Brownian Motion (GBM), a process fundamental to financial modeling that must remain positive. You will derive an exact analytical expression for the probability that a single step of the numerical simulation produces a non-positive value, a qualitative failure of the method [@problem_id:3080335]. This analysis reveals how the interplay between time step size $h$, drift $\\mu$, and volatility $\\sigma$ can cause the approximation to diverge from physical or financial reality.", "problem": "Consider the geometric Brownian motion (GBM) stochastic differential equation (SDE)\n$$\ndX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t},\\quad X_{0}=x_{0}0,\n$$\nwhere $W_{t}$ is a standard Wiener process and $\\mu,\\sigma$ are real constants with $\\sigma0$. Let a uniform time grid $t_{n}=n h$ with step size $h0$ be given.\n\nUsing only the integral form of the SDE over $[t_{n},t_{n+1}]$ and the definition of the Euler–Maruyama (EM) method as a left-point Riemann approximation to the Itô integrals, derive the EM update for $X_{n+1}$ in terms of $X_{n}$. Then, conditioning on $X_{n}0$, use the basic properties of Gaussian increments of the Wiener process to obtain a closed-form analytic expression for the probability that the EM one-step approximation $X_{n+1}$ is nonpositive.\n\nFinally, explain qualitatively how this probability depends on $h$, $\\mu$, and $\\sigma$, and under what conditions the EM approximation is likely to violate positivity, even though the exact GBM solution is strictly positive for all $t0$ given $x_{0}0$.\n\nProvide your final answer as the closed-form analytic expression for the one-step probability $\\mathbb{P}(X_{n+1}\\le 0\\,|\\,X_{n}0)$ as a function of $\\mu$, $\\sigma$, and $h$. No numerical approximation or rounding is required, and no units are to be reported.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary information is provided, and the tasks are clearly defined within the standard framework of stochastic differential equations and numerical methods. The problem is valid.\n\nThe stochastic differential equation (SDE) for geometric Brownian motion (GBM) is given by\n$$\ndX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t},\n$$\nwith initial condition $X_{0}=x_{0}0$. Here, $W_{t}$ is a standard Wiener process, and $\\mu, \\sigma$ are real constants with $\\sigma0$.\n\nThe first step is to derive the Euler–Maruyama (EM) numerical scheme for this SDE. The integral form of the SDE over a time interval $[t_{n}, t_{n+1}]$ is\n$$\nX_{t_{n+1}} - X_{t_{n}} = \\int_{t_{n}}^{t_{n+1}} \\mu X_{s}\\,ds + \\int_{t_{n}}^{t_{n+1}} \\sigma X_{s}\\,dW_{s}.\n$$\nLet $h = t_{n+1} - t_{n}$ be the uniform time step. Let $X_{n}$ denote the numerical approximation to $X_{t_{n}}$. The Euler-Maruyama method is derived by approximating the integrands in the above equation by their values at the left endpoint of the interval, $t_{n}$. This is a left-point Riemann approximation.\n\nFor the drift term (a standard Riemann integral):\n$$\n\\int_{t_{n}}^{t_{n+1}} \\mu X_{s}\\,ds \\approx \\mu X_{t_{n}} \\int_{t_{n}}^{t_{n+1}} ds = \\mu X_{n} (t_{n+1} - t_{n}) = \\mu X_{n} h.\n$$\nFor the diffusion term (an Itô integral):\n$$\n\\int_{t_{n}}^{t_{n+1}} \\sigma X_{s}\\,dW_{s} \\approx \\sigma X_{t_{n}} \\int_{t_{n}}^{t_{n+1}} dW_{s} = \\sigma X_{n} (W_{t_{n+1}} - W_{t_{n}}).\n$$\nSubstituting these approximations into the integral equation yields the EM update rule:\n$$\nX_{n+1} = X_{n} + \\mu X_{n} h + \\sigma X_{n} (W_{t_{n+1}} - W_{t_{n}}).\n$$\nThe increment of the Wiener process, $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$, is a normally distributed random variable with mean $0$ and variance $t_{n+1} - t_{n} = h$. We can write $\\Delta W_{n} = \\sqrt{h} Z_{n}$, where $Z_{n}$ is a standard normal random variable, $Z_{n} \\sim \\mathcal{N}(0, 1)$.\n\nThe EM update rule can therefore be written as:\n$$\nX_{n+1} = X_{n} (1 + \\mu h + \\sigma \\sqrt{h} Z_{n}).\n$$\nThis is the derived EM update for $X_{n+1}$ in terms of $X_{n}$.\n\nNext, we calculate the probability that the one-step approximation $X_{n+1}$ is nonpositive, conditioned on $X_{n}0$. We are interested in $\\mathbb{P}(X_{n+1} \\le 0 \\,|\\, X_{n}  0)$.\nUsing the EM update formula:\n$$\n\\mathbb{P}(X_{n} (1 + \\mu h + \\sigma \\sqrt{h} Z_{n}) \\le 0 \\,|\\, X_{n}  0).\n$$\nSince the conditioning is on $X_{n}  0$, we can divide the inequality by $X_{n}$ without changing its direction:\n$$\n\\mathbb{P}(1 + \\mu h + \\sigma \\sqrt{h} Z_{n} \\le 0).\n$$\nWe now isolate the standard normal random variable $Z_{n}$:\n$$\n\\sigma \\sqrt{h} Z_{n} \\le -(1 + \\mu h).\n$$\nSince $\\sigma0$ and $h0$, we have $\\sigma\\sqrt{h}0$. We can divide by this term:\n$$\nZ_{n} \\le -\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}.\n$$\nThe probability of this event is given by the cumulative distribution function (CDF) of the standard normal distribution, denoted by $\\Phi(z) = \\mathbb{P}(Z \\le z)$ for $Z \\sim \\mathcal{N}(0,1)$.\nTherefore, the probability is:\n$$\n\\mathbb{P}(X_{n+1} \\le 0 \\,|\\, X_{n}  0) = \\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right).\n$$\nThis is the closed-form analytic expression for the probability.\n\nFinally, we provide a qualitative explanation of the result. The probability of violating positivity, $P = \\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right)$, depends on the argument of the CDF. Since $\\Phi$ is a monotonically increasing function, the probability $P$ increases as its argument, $z = -\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$, increases (becomes less negative).\n\n- **Dependence on step size $h$**: As the step size $h$ increases from $0$, the term $\\sqrt{h}$ in the denominator increases, and the term $(1+\\mu h)$ in the numerator also increases. The dominant term for small $h$ is $-1/(\\sigma\\sqrt{h})$. As $h \\to 0^{+}$, this term goes to $-\\infty$, and thus the probability goes to $0$. As $h$ increases, the argument $z$ generally increases, leading to a higher probability of negativity. A larger time step allows for a larger random jump variance $(\\sigma X_n \\sqrt{h})^2$, making it more likely that a single step can overcome the current positive value.\n\n- **Dependence on volatility $\\sigma$**: As the volatility $\\sigma$ increases, the magnitude of the denominator $\\sigma\\sqrt{h}$ increases. This makes the argument $z$, which is negative (assuming $1+\\mu h  0$), less negative (i.e., it increases). A larger $z$ results in a larger value of $\\Phi(z)$. Therefore, a higher volatility $\\sigma$ increases the probability of the EM approximation becoming nonpositive. This is because higher volatility implies larger potential random fluctuations.\n\n- **Dependence on drift $\\mu$**: As the drift $\\mu$ increases, the numerator $1+\\mu h$ increases. This makes the argument $z$ more negative (i.e., it decreases). A smaller $z$ results in a smaller value of $\\Phi(z)$. Thus, a higher positive drift $\\mu$ decreases the probability of non-positivity, as it systematically pushes the process to higher values, providing a larger buffer against negative random shocks.\n\nThe exact solution of the GBM SDE is $X_t = X_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)t + \\sigma W_t\\right)$, which is strictly positive for $X_0  0$. The EM method, being a discrete approximation, can fail to preserve this positivity. The violation of positivity becomes likely when the argument $-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$ is not a large negative number. This occurs when the \"safety margin\" represented by the number of standard deviations to reach zero, $\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$, is small. This condition is promoted by a large step size $h$, a large volatility $\\sigma$, or a small (or negative) drift $\\mu$. In essence, the EM approximation can become nonpositive when the random component of a single step, $\\sigma X_{n}\\sqrt{h}Z_{n}$, is sufficiently large and negative to overwhelm the deterministic part, $X_{n}(1+\\mu h)$.", "answer": "$$\n\\boxed{\\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right)}\n$$", "id": "3080335"}, {"introduction": "Having seen a limitation of the Euler-Maruyama method, we now turn to a solution. The low strong convergence order of EM is often the source of its inaccuracies, especially for SDEs with multiplicative noise like GBM. This practice introduces the Milstein method, a higher-order scheme designed to improve strong convergence [@problem_id:3226794]. By implementing both the Euler-Maruyama and Milstein methods and comparing their empirical convergence rates, you will quantitatively demonstrate the superiority of the Milstein scheme and understand why it is a preferred tool for this important class of SDEs.", "problem": "You are asked to compare the strong convergence behavior of two numerical schemes for stochastic differential equations (SDEs): the Euler–Maruyama method and the Milstein method. Work entirely in a scalar setting for a Geometric Brownian Motion, whose dynamics are given by the Itô SDE\n$$\ndX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t,\n$$\nwith initial condition $X_0 = x_0$, where $\\mu \\in \\mathbb{R}$ is the drift, $\\sigma \\in \\mathbb{R}$ is the diffusion parameter, and $W_t$ is a standard Wiener process. The exact solution is the well-known closed form\n$$\nX_T = X_0 \\exp\\big((\\mu - \\tfrac{1}{2}\\sigma^2) T + \\sigma W_T\\big).\n$$\nYour task is to implement a Monte Carlo study to estimate the strong convergence order at terminal time $T$ of:\n- the Euler–Maruyama method (Euler–Maruyama is the simplest Itô-integral-based discretization that replaces the Itô integral by the increment $\\,\\Delta W_n\\,$), and\n- the Milstein method (the first-order Itô–Taylor method for scalar SDEs that augments the Euler–Maruyama increment by the leading stochastic Taylor correction involving the diffusion derivative $\\,b'(x)\\,$ for $\\,b(x)=\\sigma x\\,$).\n\nBegin from the core definitions of the Itô integral, Itô’s formula, and the Itô–Taylor expansion, and use these to design the two time-stepping updates. Use the exact solution above to compute the terminal strong error. For each method and each time step, estimate the root mean square (RMS) strong error at time $T$:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) = \\Big(\\mathbb{E}\\big[|X_T^{(h)} - X_T|^2\\big]\\Big)^{1/2},\n$$\nwhere $X_T^{(h)}$ denotes the numerical approximation using uniform step size $h$.\n\nTo estimate the convergence order, consider a set of uniform step sizes $h_k = T/N_k$ and fit the model\n$$\n\\log \\varepsilon_{\\mathrm{RMS}}(h_k) \\approx p \\log h_k + C\n$$\nby least squares in $\\log$–$\\log$ scale to obtain the estimated strong order $p$.\n\nMonte Carlo requirements:\n- Use $M = 10000$ independent sample paths.\n- For variance reduction and fair comparison across step sizes, generate the Brownian increments on the finest grid and obtain coarser-grid increments by summing consecutive fine-grid increments (i.e., use a consistent refinement so that $N_{\\max}/N_k \\in \\mathbb{N}$ and the same underlying noise drives all resolutions).\n- For reproducibility, use the specified pseudorandom seeds per test case.\n\nUse the following test suite. For each test, simulate with the common set of time partitions $N_k \\in \\{16, 32, 64, 128\\}$ (so $h_k = T/N_k$), and the given number of paths and seed:\n- Test A (happy path, moderate noise): $x_0 = 1.0$, $\\mu = 1.0$, $\\sigma = 0.5$, $T = 1.0$, seed $= 314159$.\n- Test B (edge case with small noise): $x_0 = 2.0$, $\\mu = 0.3$, $\\sigma = 0.05$, $T = 2.0$, seed $= 271828$.\n- Test C (negative drift, stronger noise): $x_0 = 1.0$, $\\mu = -1.0$, $\\sigma = 1.0$, $T = 1.0$, seed $= 161803$.\n\nYour program must:\n- Implement both the Euler–Maruyama method and the Milstein method for the given SDE using their respective principled constructions from Itô calculus.\n- For each test case and each $N_k \\in \\{16, 32, 64, 128\\}$, compute the RMS strong error using $M = 10000$ simulated paths with coupled Brownian increments as described, and then fit the slope $p$ in the $\\log$–$\\log$ model separately for Euler–Maruyama and for Milstein.\n- Return, for each test case, the pair of estimated strong orders $[p_{\\mathrm{EM}}, p_{\\mathrm{Mil}}]$, each rounded to two decimal places.\n\nFinal output format:\n- Your program should print a single line containing a list of three entries, one per test case, in the order A, B, C. Each entry is the two-element list $[p_{\\mathrm{EM}}, p_{\\mathrm{Mil}}]$ for that test.\n- Concretely, your program should produce exactly one line like\n$$\n[[p_{\\mathrm{EM,A}}, p_{\\mathrm{Mil,A}}],[p_{\\mathrm{EM,B}}, p_{\\mathrm{Mil,B}}],[p_{\\mathrm{EM,C}}, p_{\\mathrm{Mil,C}}]]\n$$\nwith all six values as decimal floats rounded to two decimal places, and with no extra characters or whitespace beyond commas, brackets, and digits.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the theory of stochastic calculus and numerical methods for SDEs, well-posed with all necessary parameters defined, and objective in its formulation and requirements.\n\nThis problem requires a comparative study of the strong convergence orders for the Euler-Maruyama and Milstein numerical methods applied to the Geometric Brownian Motion (GBM) stochastic differential equation (SDE). The SDE is given by:\n$$\ndX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t, \\quad X(0) = x_0\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the drift parameter, $\\sigma \\in \\mathbb{R}$ is the diffusion parameter, and $W_t$ is a standard Wiener process. This is an Itô process with drift coefficient $a(x) = \\mu x$ and diffusion coefficient $b(x) = \\sigma x$. The problem provides the exact solution at time $T$:\n$$\nX_T = X_0 \\exp\\left((\\mu - \\tfrac{1}{2}\\sigma^2) T + \\sigma W_T\\right)\n$$\nwhere $W_T = \\int_0^T dW_s$ is the total increment of the Wiener process over $[0, T]$.\n\nThe solution involves deriving the numerical schemes, implementing them in a Monte Carlo simulation, computing the strong error, and finally estimating the convergence order via regression.\n\n**1. Derivation of Numerical Schemes**\n\nThe numerical schemes are derived by discretizing the integral form of the SDE over a time interval $[t_n, t_{n+1}]$ of length $h = t_{n+1} - t_n$. The integral form is:\n$$\nX_{t_{n+1}} = X_{t_n} + \\int_{t_n}^{t_{n+1}} a(X_s) \\, ds + \\int_{t_n}^{t_{n+1}} b(X_s) \\, dW_s\n$$\n\n**Euler-Maruyama (EM) Method:**\nThe EM method is the simplest discretization, obtained by approximating the integrands $a(X_s)$ and $b(X_s)$ as constants over the interval, equal to their values at the start of the interval, $X_n \\equiv X_{t_n}$.\n$$\nX_{n+1} \\approx X_n + a(X_n) \\int_{t_n}^{t_{n+1}} \\, ds + b(X_n) \\int_{t_n}^{t_{n+1}} \\, dW_s\n$$\nThe integrals evaluate to $\\int_{t_n}^{t_{n+1}} ds = h$ and $\\int_{t_n}^{t_{n+1}} dW_s = \\Delta W_n = W_{t_{n+1}} - W_{t_n}$. The term $\\Delta W_n$ represents a random draw from a normal distribution with mean $0$ and variance $h$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0, h)$. The EM update rule is:\n$$\nX_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n\n$$\nFor the given GBM SDE with $a(x) = \\mu x$ and $b(x) = \\sigma x$, the EM scheme is:\n$$\nX_{n+1} = X_n + \\mu X_n h + \\sigma X_n \\Delta W_n = X_n (1 + \\mu h + \\sigma \\Delta W_n)\n$$\nThe EM method has a strong order of convergence of $0.5$ for general SDEs.\n\n**Milstein Method:**\nThe Milstein method improves upon the EM method by including an additional term from the Itô-Taylor expansion. For a scalar SDE, the expansion is:\n$$\nX_{t_{n+1}} = X_{t_n} + a(X_n) h + b(X_n) \\Delta W_n + \\frac{1}{2} b(X_n) b'(X_n) \\left((\\Delta W_n)^2 - h\\right) + \\mathcal{O}(h^{3/2})\n$$\nwhere $b'(x)$ is the derivative of the diffusion coefficient with respect to $x$. This scheme is obtained by including the next-order term in the stochastic integral approximation.\nThe update rule for the Milstein method is:\n$$\nX_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n + \\frac{1}{2} b(X_n) b'(X_n) \\left((\\Delta W_n)^2 - h\\right)\n$$\nFor the GBM SDE, we have $b(x) = \\sigma x$ and its derivative $b'(x) = \\sigma$. Substituting these into the Milstein scheme gives:\n$$\nX_{n+1} = X_n + \\mu X_n h + \\sigma X_n \\Delta W_n + \\frac{1}{2} (\\sigma X_n) (\\sigma) \\left((\\Delta W_n)^2 - h\\right)\n$$\n$$\nX_{n+1} = X_n \\left(1 + \\mu h + \\sigma \\Delta W_n + \\frac{1}{2} \\sigma^2 \\left((\\Delta W_n)^2 - h\\right)\\right)\n$$\nThe Milstein method typically has a strong order of convergence of $1.0$.\n\n**2. Monte Carlo Simulation Framework**\n\nThe goal is to estimate the strong convergence order $p$ from the relationship $\\varepsilon_{\\mathrm{RMS}}(h) \\propto h^p$. The root mean square (RMS) strong error is defined as:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) = \\left( \\mathbb{E}\\left[|X_T^{(h)} - X_T|^2\\right] \\right)^{1/2}\n$$\nwhere $X_T^{(h)}$ is the numerical solution at time $T$ with step size $h$, and $X_T$ is the exact solution. The expectation $\\mathbb{E}[\\cdot]$ is approximated by a sample mean over $M = 10000$ independent simulated paths:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) \\approx \\sqrt{ \\frac{1}{M} \\sum_{i=1}^{M} \\left|X_{T,i}^{(h)} - X_{T,i}\\right|^2 }\n$$\nTo ensure a fair comparison and reduce variance, the simulations for different step sizes $h_k = T/N_k$ for $N_k \\in \\{16, 32, 64, 128\\}$ are driven by the same underlying noise. This is achieved by first generating Brownian increments for the finest grid, $N_{\\max} = 128$, with step size $h_{\\text{fine}} = T/N_{\\max}$. For a coarser grid with $N_k  N_{\\max}$, the a coarse increment $\\Delta W_n^{(k)}$ is obtained by summing $R_k = N_{\\max}/N_k$ consecutive fine increments. This ensures that the total Wiener path $W_T$ is identical for all simulations.\n\n**3. Estimation of Convergence Order**\n\nThe convergence order $p$ is estimated by fitting a linear model to the log-transformed errors and step sizes:\n$$\n\\log \\varepsilon_{\\mathrm{RMS}}(h_k) = p \\log h_k + C\n$$\nThis is a simple linear regression problem for the data points $(\\log h_k, \\log \\varepsilon_{\\mathrm{RMS}}(h_k))$. The slope of the best-fit line, $p$, is the estimated order of convergence. This is computed separately for the Euler-Maruyama and Milstein methods for each test case.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Initialize parameters ($\\mu, \\sigma, x_0, T$) and simulation settings ($M, N_k, \\text{seed}$).\n2.  Generate $M$ paths of fine-grid Brownian increments $\\{\\Delta W_n^{\\text{fine}}\\}$ for $N_{\\max}=128$ steps.\n3.  Calculate the exact solution $X_{T,i}$ for each path $i$ using the total Wiener increment $W_{T,i} = \\sum_n \\Delta W_{n,i}^{\\text{fine}}$.\n4.  For each step count $N_k \\in \\{16, 32, 64, 128\\}$:\n    a. Determine the step size $h_k = T/N_k$.\n    b. Construct coarse-grid Brownian increments by summing fine-grid increments.\n    c. Simulate $M$ paths from $t=0$ to $t=T$ using both the EM and Milstein schemes.\n    d. Compute the RMS strong error $\\varepsilon_{\\mathrm{RMS}}(h_k)$ for both methods.\n5.  Using the set of calculated errors, perform a linear regression on $(\\log h_k, \\log \\varepsilon_k)$ to find the slope $p_{\\mathrm{EM}}$ for the Euler-Maruyama method and $p_{\\mathrm{Mil}}$ for the Milstein method.\n6.  Round the estimated orders to two decimal places and store the resulting pair.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo study to estimate the strong convergence orders of the\n    Euler-Maruyama and Milstein methods for the Geometric Brownian Motion SDE.\n    \"\"\"\n    test_cases = [\n        # (x0, mu, sigma, T, seed)\n        (1.0, 1.0, 0.5, 1.0, 314159),  # Test A\n        (2.0, 0.3, 0.05, 2.0, 271828), # Test B\n        (1.0, -1.0, 1.0, 1.0, 161803), # Test C\n    ]\n\n    all_results = []\n\n    M = 10000  # Number of Monte Carlo paths\n    N_steps_list = [16, 32, 64, 128]\n    N_max = max(N_steps_list)\n\n    for x0, mu, sigma, T, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # Step 1: Generate fine-grained Brownian increments for all paths\n        h_fine = T / N_max\n        # (M, N_max) matrix of increments\n        fine_increments = rng.normal(0, np.sqrt(h_fine), (M, N_max))\n        \n        # Step 2: Calculate the exact solution at time T\n        # Total Wiener increment for each path\n        W_T_paths = np.sum(fine_increments, axis=1)\n        X_exact_T = x0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * W_T_paths)\n        \n        log_h_values = []\n        log_em_errors = []\n        log_mil_errors = []\n\n        # Step 3: Loop over different step sizes\n        for N_k in N_steps_list:\n            h_k = T / N_k\n            log_h_values.append(np.log(h_k))\n            \n            # Step 3a: Construct coarse Brownian increments from fine ones\n            refinement_ratio = N_max // N_k\n            coarse_increments = fine_increments.reshape(M, N_k, refinement_ratio).sum(axis=2)\n            \n            # Initialize numerical solutions at t=0\n            X_em = np.full(M, x0)\n            X_mil = np.full(M, x0)\n            \n            # Step 3b: Simulate paths for both schemes\n            for n in range(N_k):\n                dW = coarse_increments[:, n]\n                \n                # Euler-Maruyama update\n                X_em = X_em * (1 + mu * h_k + sigma * dW)\n                \n                # Milstein update\n                mil_correction = 0.5 * sigma**2 * (dW**2 - h_k)\n                X_mil = X_mil * (1 + mu * h_k + sigma * dW + mil_correction)\n            \n            # Step 3c: Calculate RMS strong error for this step size\n            em_error_sq = np.mean((X_em - X_exact_T)**2)\n            mil_error_sq = np.mean((X_mil - X_exact_T)**2)\n            \n            log_em_errors.append(0.5 * np.log(em_error_sq))\n            log_mil_errors.append(0.5 * np.log(mil_error_sq))\n\n        # Step 4: Perform log-log regression to find the convergence order\n        # np.polyfit returns [slope, intercept]\n        p_em = np.polyfit(log_h_values, log_em_errors, 1)[0]\n        p_mil = np.polyfit(log_h_values, log_mil_errors, 1)[0]\n        \n        # Round results to two decimal places\n        p_em_rounded = round(p_em, 2)\n        p_mil_rounded = round(p_mil, 2)\n        \n        all_results.append([p_em_rounded, p_mil_rounded])\n\n    # Step 5: Format the final output string\n    inner_strings = [f\"[{p_em:.2f},{p_mil:.2f}]\" for p_em, p_mil in all_results]\n    print(f\"[{','.join(inner_strings)}]\")\n\nsolve()\n```", "id": "3226794"}]}