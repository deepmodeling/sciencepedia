{"hands_on_practices": [{"introduction": "Understanding weak convergence begins with the ability to precisely quantify the error for a given numerical scheme. In this exercise, you will work with the Ornstein–Uhlenbeck process, a fundamental model in finance and physics, to directly compute the weak error of the Euler–Maruyama method. By deriving exact formulas for both the true and numerical expectations, you will gain a concrete understanding of how these quantities diverge and see firsthand how the error depends on the time step $h$ [@problem_id:3083367].", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE) driven by a standard Brownian motion (BM) on the interval $[0,T]$ with deterministic initial condition $X_{0}=x_{0}\\in\\mathbb{R}$:\n$dX_{t} = -\\lambda X_{t}\\,dt + \\sigma\\,dW_{t}$,\nwhere $\\lambda>0$, $\\sigma\\in\\mathbb{R}$, and $W_{t}$ is a standard Brownian motion. Let the observable be $\\varphi(x)=x$. \n\nYour tasks are:\n- Using only fundamental properties of Itô calculus and linearity of expectation, derive from first principles a closed-form expression for $m(T)=\\mathbb{E}[\\varphi(X_{T})]=\\mathbb{E}[X_{T}]$ by obtaining and solving the deterministic ordinary differential equation satisfied by $m(t)=\\mathbb{E}[X_{t}]$ with $m(0)=x_{0}$.\n- Consider the Euler–Maruyama (EM) method with uniform time step $h>0$ and grid $t_{k}=kh$ for $k=0,1,\\dots,N$, where $N=T/h\\in\\mathbb{N}$. The scheme is\n$X^{h}_{k+1}=X^{h}_{k}-\\lambda X^{h}_{k}\\,h+\\sigma\\,(W_{t_{k+1}}-W_{t_{k}})$,\nwith $X^{h}_{0}=x_{0}$. Compute $\\mathbb{E}[X^{h}_{N}]$ exactly in terms of $x_{0}$, $\\lambda$, $h$, and $N$.\n- Define the weak error on the observable $\\varphi$ at time $T$ by $e_{\\text{weak}}(h)=\\mathbb{E}[\\varphi(X_{T})]-\\mathbb{E}[\\varphi(X^{h}_{N})]$. Using your exact expressions, write $e_{\\text{weak}}(h)$ explicitly as a function of $x_{0}$, $\\lambda$, $T$, and $h$. Then determine the leading-order behavior of $e_{\\text{weak}}(h)$ as $h\\to 0$ and state the weak order on this observable, justifying your conclusion from the expansion you obtain.\n\nProvide, as your final answer, the exact closed-form expression for $e_{\\text{weak}}(h)$ in terms of $x_{0}$, $\\lambda$, $T$, and $h$. No rounding is required.", "solution": "The problem is validated as scientifically sound, well-posed, objective, and self-contained. The Ornstein–Uhlenbeck process and the Euler–Maruyama method are standard topics in stochastic differential equations and their numerical analysis. All provided information is consistent and sufficient to derive the requested quantities.\n\nThe solution is organized into three parts as requested by the problem statement.\n\n### Part 1: Mean of the Exact Solution\n\nWe are given the Ornstein–Uhlenbeck stochastic differential equation (SDE):\n$$dX_{t} = -\\lambda X_{t}\\,dt + \\sigma\\,dW_{t}$$\nwith a deterministic initial condition $X_{0} = x_{0}$, where $\\lambda > 0$, $\\sigma \\in \\mathbb{R}$, and $W_{t}$ is a standard Brownian motion. We are interested in the observable $\\varphi(x)=x$, so we need to compute $m(T) = \\mathbb{E}[\\varphi(X_{T})] = \\mathbb{E}[X_{T}]$.\n\nFirst, we write the SDE in its integral form:\n$$X_{t} = X_{0} + \\int_{0}^{t} (-\\lambda X_{s})\\,ds + \\int_{0}^{t} \\sigma\\,dW_{s}$$\nLet $m(t) = \\mathbb{E}[X_{t}]$. We take the expectation of both sides of the integral equation. By the linearity of the expectation operator, we have:\n$$m(t) = \\mathbb{E}[X_{t}] = \\mathbb{E}\\left[ X_{0} - \\lambda \\int_{0}^{t} X_{s}\\,ds + \\sigma \\int_{0}^{t} dW_{s} \\right]$$\n$$m(t) = \\mathbb{E}[X_{0}] - \\lambda \\mathbb{E}\\left[\\int_{0}^{t} X_{s}\\,ds\\right] + \\sigma \\mathbb{E}\\left[\\int_{0}^{t} dW_{s}\\right]$$\n\nWe evaluate each term:\n1.  The initial condition is deterministic, so $\\mathbb{E}[X_{0}] = \\mathbb{E}[x_{0}] = x_{0}$.\n2.  The expectation of the Itô integral with a deterministic integrand over a standard Brownian motion is zero. Since $X_s$ is adapted, the stochastic Fubini theorem allows us to interchange expectation and integration, and for the Itô integral, we have $\\mathbb{E}\\left[\\int_{0}^{t} dW_{s}\\right] = 0$. This is a fundamental property of Itô calculus, as the integral is a martingale starting at $0$.\n3.  Under mild conditions on the process $X_s$ (which are met here), we can apply the stochastic Fubini theorem to interchange the expectation and the Lebesgue integral: $\\mathbb{E}\\left[\\int_{0}^{t} X_{s}\\,ds\\right] = \\int_{0}^{t} \\mathbb{E}[X_{s}]\\,ds = \\int_{0}^{t} m(s)\\,ds$.\n\nSubstituting these back into the equation for $m(t)$, we obtain an integral equation for the mean:\n$$m(t) = x_{0} - \\lambda \\int_{0}^{t} m(s)\\,ds$$\nThis is a Volterra integral equation of the second kind. Differentiating both sides with respect to $t$ using the Fundamental Theorem of Calculus yields the ordinary differential equation (ODE) for $m(t)$:\n$$\\frac{d m(t)}{dt} = -\\lambda m(t)$$\nThe initial condition is obtained by setting $t=0$ in the integral equation: $m(0) = x_{0}$.\n\nThis is a first-order linear homogeneous ODE. The solution is:\n$$m(t) = m(0)\\exp(-\\lambda t) = x_{0}\\exp(-\\lambda t)$$\nAt the final time $T$, the mean of the exact solution is:\n$$m(T) = \\mathbb{E}[X_{T}] = x_{0}\\exp(-\\lambda T)$$\n\n### Part 2: Mean of the Numerical Solution\n\nThe Euler–Maruyama (EM) scheme with time step $h$ is given by:\n$$X^{h}_{k+1} = X^{h}_{k} - \\lambda X^{h}_{k}\\,h + \\sigma\\,(W_{t_{k+1}} - W_{t_{k}})$$\nwith the initial condition $X^{h}_{0}=x_{0}$. This can be rewritten as:\n$$X^{h}_{k+1} = (1 - \\lambda h)X^{h}_{k} + \\sigma\\,\\Delta W_{k}$$\nwhere $\\Delta W_{k} = W_{t_{k+1}} - W_{t_{k}}$ is the increment of the Brownian motion over the interval $[t_k, t_{k+1}]$.\n\nLet $\\mu_{k} = \\mathbb{E}[X^{h}_{k}]$. We take the expectation of the scheme:\n$$\\mathbb{E}[X^{h}_{k+1}] = \\mathbb{E}\\left[ (1 - \\lambda h)X^{h}_{k} + \\sigma\\,\\Delta W_{k} \\right]$$\nUsing the linearity of expectation:\n$$\\mu_{k+1} = (1 - \\lambda h)\\mathbb{E}[X^{h}_{k}] + \\sigma\\mathbb{E}[\\Delta W_{k}]$$\nThe increments of a standard Brownian motion $\\Delta W_{k}$ are independent and identically distributed normal random variables with mean $0$ and variance $h$. Thus, $\\mathbb{E}[\\Delta W_{k}] = 0$.\nThe recurrence relation for the mean of the numerical solution simplifies to:\n$$\\mu_{k+1} = (1 - \\lambda h)\\mu_{k}$$\nThe initial condition is $\\mu_{0} = \\mathbb{E}[X^{h}_{0}] = \\mathbb{E}[x_{0}] = x_{0}$.\n\nThis is a geometric progression. The solution at step $k$ is given by:\n$$\\mu_{k} = (1 - \\lambda h)^{k} \\mu_{0} = x_{0}(1 - \\lambda h)^{k}$$\nWe need the mean at the final time $T$, which corresponds to the $N$-th step, where $N=T/h$. Thus, for $k=N$:\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{N}$$\n\n### Part 3: Weak Error and Leading-Order Analysis\n\nThe weak error on the observable $\\varphi(x)=x$ at time $T$ is defined as:\n$$e_{\\text{weak}}(h) = \\mathbb{E}[\\varphi(X_{T})] - \\mathbb{E}[\\varphi(X^{h}_{N})] = \\mathbb{E}[X_{T}] - \\mathbb{E}[X^{h}_{N}]$$\nSubstituting the exact expressions derived in the previous parts:\n$$\\mathbb{E}[X_{T}] = x_{0}\\exp(-\\lambda T)$$\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{N}$$\nUsing the relation $N=T/h$, we can write $\\mathbb{E}[X^{h}_{N}]$ as a function of $h$ and $T$:\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{T/h}$$\nTherefore, the exact expression for the weak error is:\n$$e_{\\text{weak}}(h) = x_{0}\\exp(-\\lambda T) - x_{0}(1 - \\lambda h)^{T/h} = x_{0}\\left(\\exp(-\\lambda T) - (1 - \\lambda h)^{T/h}\\right)$$\n\nTo determine the leading-order behavior as $h \\to 0$, we find the Taylor expansion of the term $(1 - \\lambda h)^{T/h}$. We can write this term using the exponential function:\n$$(1 - \\lambda h)^{T/h} = \\exp\\left(\\frac{T}{h} \\ln(1 - \\lambda h)\\right)$$\nFor small $h$, we use the Taylor series for the natural logarithm, $\\ln(1-u) = -u - \\frac{u^2}{2} - O(u^3)$, setting $u = \\lambda h$:\n$$\\ln(1 - \\lambda h) = -\\lambda h - \\frac{(\\lambda h)^2}{2} - O(h^3)$$\nSubstituting this into the exponent:\n$$\\frac{T}{h}\\ln(1 - \\lambda h) = \\frac{T}{h}\\left(-\\lambda h - \\frac{\\lambda^2 h^2}{2} - O(h^3)\\right) = -\\lambda T - \\frac{\\lambda^2 T}{2}h - O(h^2)$$\nSo, the numerical mean expression becomes:\n$$(1 - \\lambda h)^{T/h} = \\exp\\left(-\\lambda T - \\frac{\\lambda^2 T}{2}h - O(h^2)\\right) = \\exp(-\\lambda T)\\exp\\left(-\\frac{\\lambda^2 T}{2}h - O(h^2)\\right)$$\nNow, we use the Taylor series for the exponential function, $\\exp(v) = 1 + v + O(v^2)$, with $v = -\\frac{\\lambda^2 T}{2}h$:\n$$\\exp\\left(-\\frac{\\lambda^2 T}{2}h - O(h^2)\\right) = 1 - \\frac{\\lambda^2 T}{2}h + O(h^2)$$\nPutting it all together:\n$$(1 - \\lambda h)^{T/h} = \\exp(-\\lambda T)\\left(1 - \\frac{\\lambda^2 T}{2}h + O(h^2)\\right) = \\exp(-\\lambda T) - \\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)$$\nNow we substitute this expansion back into the weak error expression:\n$$e_{\\text{weak}}(h) = x_{0}\\left(\\exp(-\\lambda T) - \\left[\\exp(-\\lambda T) - \\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)\\right]\\right)$$\n$$e_{\\text{weak}}(h) = x_{0}\\left(\\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h - O(h^2)\\right)$$\n$$e_{\\text{weak}}(h) = \\frac{x_{0}\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)$$\nThe leading-order behavior of the weak error is proportional to $h$. Therefore, the weak order of convergence for the Euler-Maruyama method on this observable is $1$.", "answer": "$$\\boxed{x_{0}\\left(\\exp(-\\lambda T) - (1 - \\lambda h)^{T/h}\\right)}$$", "id": "3083367"}, {"introduction": "Once we understand the structure of the weak error, we can devise methods to accelerate convergence. This practice introduces Richardson extrapolation, a powerful technique that uses results from different step sizes to cancel out the leading-order error term. Applying this to the Ornstein–Uhlenbeck model demonstrates how a deeper analysis of the error expansion, which often takes the form $C_1 h + C_2 h^2 + \\dots$, leads to significantly more accurate approximations [@problem_id:3083381].", "problem": "Consider the scalar Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} = -a\\,X_{t}\\,\\mathrm{d}t + b\\,\\mathrm{d}W_{t}, \\qquad X_{0} = x_{0},\n$$\nwhere $a>0$, $b \\in \\mathbb{R}$, $x_{0} \\in \\mathbb{R}$, $T>0$, and $W_{t}$ is a standard Wiener process. Let $\\varphi(x) = x$ and denote by $m(T) = \\mathbb{E}[X_{T}]$ the exact mean at time $T$. Define the Euler–Maruyama time-stepping scheme with uniform step size $h = T/N$ by\n$$\nX_{n+1}^{(h)} = X_{n}^{(h)} - a\\,X_{n}^{(h)}\\,h + b\\,\\Delta W_{n}, \\quad n=0,1,\\dots,N-1, \\qquad X_{0}^{(h)} = x_{0},\n$$\nand let $m_{h} := \\mathbb{E}[X_{N}^{(h)}]$. Similarly, let $m_{h/2}$ denote the mean produced by the Euler–Maruyama scheme with step size $h/2$ over the same terminal time $T$.\n\nUsing only fundamental properties of linear SDEs (linearity of expectation and Itô calculus) and the definition of the Euler–Maruyama scheme, derive explicit expressions for $m(T)$, $m_{h}$, and $m_{h/2}$, and then apply Richardson extrapolation to form\n$$\nR(h) := 2\\,m_{h/2} - m_{h}.\n$$\nBy carrying out a small-$h$ analysis based on first principles of series expansions for smooth functions, determine the leading constant $L$ defined by the limit\n$$\nL := \\lim_{h \\to 0} \\frac{R(h) - m(T)}{h^{2}},\n$$\nexpressed in closed form in terms of $a$, $T$, and $x_{0}$ only. Your final answer must be a single analytic expression. No numerical rounding is required, and no units are involved.", "solution": "The problem statement is analyzed for validity.\n\n### Step 1: Extract Givens\n- **SDE**: $\\mathrm{d}X_{t} = -a\\,X_{t}\\,\\mathrm{d}t + b\\,\\mathrm{d}W_{t}$, with initial condition $X_{0} = x_{0}$.\n- **Parameters**: $a>0$, $b \\in \\mathbb{R}$, $x_{0} \\in \\mathbb{R}$, $T>0$.\n- **Process**: $W_{t}$ is a standard Wiener process.\n- **Test function**: $\\varphi(x) = x$.\n- **Exact Mean**: $m(T) = \\mathbb{E}[X_{T}]$.\n- **Numerical Scheme**: The Euler–Maruyama scheme with uniform step size $h = T/N$ is $X_{n+1}^{(h)} = X_{n}^{(h)} - a\\,X_{n}^{(h)}\\,h + b\\,\\Delta W_{n}$ for $n=0,1,\\dots,N-1$, with $X_{0}^{(h)} = x_{0}$. $\\Delta W_n = W_{(n+1)h} - W_{nh}$.\n- **Numerical Mean**: $m_{h} := \\mathbb{E}[X_{N}^{(h)}]$.\n- **Refined Numerical Mean**: $m_{h/2}$ is the mean from the same scheme with step size $h/2$.\n- **Richardson Extrapolation**: $R(h) := 2\\,m_{h/2} - m_{h}$.\n- **Objective**: Determine the leading constant $L := \\lim_{h \\to 0} \\frac{R(h) - m(T)}{h^{2}}$ in terms of $a$, $T$, and $x_{0}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard exercise in the numerical analysis of stochastic differential equations. The Ornstein–Uhlenbeck process, the Euler–Maruyama method, and Richardson extrapolation are all fundamental and well-established concepts in the field. The analysis of weak convergence order is a core topic.\n- **Well-Posed**: The problem is clearly stated, with all necessary parameters and definitions provided. It asks for a specific, derivable constant $L$, which points to a unique and meaningful solution.\n- **Objective**: The problem is phrased in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, objective, and self-contained. The solution process may proceed.\n\n### Derivation of the solution\n\nThe solution requires three main components: the exact mean $m(T)$, the numerical means $m_{h}$ and $m_{h/2}$, and a small-$h$ asymptotic analysis to evaluate the limit $L$.\n\n**1. Exact Mean $m(T)$**\n\nThe Ornstein-Uhlenbeck SDE is a linear SDE. We can solve it explicitly. Its solution at time $t$ is given by\n$$\nX_{t} = X_0 \\exp(-at) + \\int_{0}^{t} b \\exp(-a(t-s)) \\mathrm{d}W_s.\n$$\nTaking the expectation, we get the mean value $m(t) = \\mathbb{E}[X_t]$. Using the linearity of expectation,\n$$\nm(t) = \\mathbb{E}[X_0 \\exp(-at)] + \\mathbb{E}\\left[\\int_{0}^{t} b \\exp(-a(t-s)) \\mathrm{d}W_s\\right].\n$$\nSince $X_0 = x_0$ is a deterministic initial condition, $\\mathbb{E}[X_0 \\exp(-at)] = x_0 \\exp(-at)$. The second term is the expectation of an Itô integral with a deterministic integrand, which is zero.\n$$\n\\mathbb{E}\\left[\\int_{0}^{t} b \\exp(-a(t-s)) \\mathrm{d}W_s\\right] = 0.\n$$\nThus, the mean of the process at time $t$ is $m(t) = x_0 \\exp(-at)$. At the terminal time $T$, the exact mean is\n$$\nm(T) = x_{0} \\exp(-aT).\n$$\n\n**2. Numerical Mean $m_h$**\n\nThe Euler–Maruyama scheme is given by\n$$\nX_{n+1}^{(h)} = (1 - ah) X_{n}^{(h)} + b \\Delta W_{n}.\n$$\nLet $m_n^{(h)} = \\mathbb{E}[X_{n}^{(h)}]$. Taking the expectation of the scheme, we obtain a recurrence relation for the mean:\n$$\n\\mathbb{E}[X_{n+1}^{(h)}] = \\mathbb{E}[(1 - ah) X_{n}^{(h)}] + \\mathbb{E}[b \\Delta W_{n}].\n$$\nThe increments of the Wiener process $\\Delta W_n$ have zero mean, $\\mathbb{E}[\\Delta W_n] = 0$. This simplifies the recurrence to\n$$\nm_{n+1}^{(h)} = (1 - ah) m_n^{(h)}.\n$$\nWith the initial condition $m_0^{(h)} = \\mathbb{E}[X_0^{(h)}] = x_0$, we can solve this geometric progression:\n$$\nm_n^{(h)} = (1-ah)^n x_0.\n$$\nWe are interested in the mean at the terminal time $T=Nh$, which corresponds to $n=N$. Therefore,\n$$\nm_h = m_N^{(h)} = (1-ah)^N x_0.\n$$\nSubstituting $N=T/h$, we get an expression for $m_h$ in terms of $h$:\n$$\nm_h = x_0 (1-ah)^{T/h}.\n$$\nSimilarly, for a step size of $h/2$, the number of steps to reach $T$ is $2N$. The numerical mean is\n$$\nm_{h/2} = x_0 (1-a(h/2))^{2N} = x_0 (1-ah/2)^{2T/h}.\n$$\n\n**3. Small-$h$ Asymptotic Analysis**\n\nTo find the limit for $L$, we need to perform a Taylor expansion of $m_h$ around $h=0$. It is convenient to first expand the logarithm of the term $(1-ah)^{T/h}$.\nUsing the series expansion $\\ln(1-x) = -x - \\frac{x^2}{2} - \\frac{x^3}{3} - O(x^4)$ for small $x$, we set $x=ah$:\n$$\n\\ln\\left((1-ah)^{T/h}\\right) = \\frac{T}{h} \\ln(1-ah) = \\frac{T}{h} \\left( -ah - \\frac{(ah)^2}{2} - \\frac{(ah)^3}{3} - O(h^4) \\right)\n$$\n$$\n= T \\left( -a - \\frac{a^2h}{2} - \\frac{a^3h^2}{3} - O(h^3) \\right) = -aT - \\frac{a^2Th}{2} - \\frac{a^3Th^2}{3} - O(h^3).\n$$\nNow, we exponentiate this expression. Let $y = - \\frac{a^2Th}{2} - \\frac{a^3Th^2}{3}$.\n$$\n(1-ah)^{T/h} = \\exp\\left(-aT - \\frac{a^2Th}{2} - \\frac{a^3Th^2}{3} - O(h^3)\\right) = \\exp(-aT) \\exp\\left(y - O(h^3)\\right).\n$$\nUsing the expansion $\\exp(y) = 1 + y + \\frac{y^2}{2} + O(y^3)$, we get\n$$\n\\exp(y) = 1 + \\left(-\\frac{a^2Th}{2} - \\frac{a^3Th^2}{3}\\right) + \\frac{1}{2}\\left(-\\frac{a^2Th}{2} - \\frac{a^3Th^2}{3}\\right)^2 + O(h^3).\n$$\nExpanding and keeping terms up to order $h^2$:\n$$\n\\exp(y) = 1 - \\frac{a^2T}{2}h - \\frac{a^3T}{3}h^2 + \\frac{1}{2}\\left(-\\frac{a^2Th}{2}\\right)^2 + O(h^3)\n$$\n$$\n= 1 - \\frac{a^2T}{2}h - \\frac{a^3T}{3}h^2 + \\frac{a^4T^2}{8}h^2 + O(h^3) = 1 - \\frac{a^2T}{2}h + \\left(\\frac{a^4T^2}{8} - \\frac{a^3T}{3}\\right)h^2 + O(h^3).\n$$\nThus, the expansion for $m_h$ is\n$$\nm_h = x_0 \\exp(-aT) \\left[ 1 - \\frac{a^2T}{2}h + \\left(\\frac{a^4T^2}{8} - \\frac{a^3T}{3}\\right)h^2 + O(h^3) \\right].\n$$\nThis expansion is of the form $m_h = m(T) + C_1 h + C_2 h^2 + O(h^3)$, where $m(T) = x_0 \\exp(-aT)$, and the coefficients are\n$$\nC_1 = -x_0 \\exp(-aT) \\frac{a^2T}{2}\n$$\n$$\nC_2 = x_0 \\exp(-aT) \\left(\\frac{a^4T^2}{8} - \\frac{a^3T}{3}\\right).\n$$\nThe expansion for $m_{h/2}$ is obtained by replacing $h$ with $h/2$:\n$$\nm_{h/2} = m(T) + C_1 \\frac{h}{2} + C_2 \\left(\\frac{h}{2}\\right)^2 + O(h^3) = m(T) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + O(h^3).\n$$\n\n**4. Richardson Extrapolation and the Limit $L$**\n\nNow we form the Richardson extrapolated value $R(h)$:\n$$\nR(h) = 2m_{h/2} - m_h\n$$\n$$\n= 2\\left(m(T) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2\\right) - \\left(m(T) + C_1 h + C_2 h^2\\right) + O(h^3)\n$$\n$$\n= (2m(T) + C_1 h + \\frac{C_2}{2}h^2) - (m(T) + C_1 h + C_2 h^2) + O(h^3)\n$$\n$$\n= m(T) - \\frac{C_2}{2}h^2 + O(h^3).\n$$\nThe difference between the extrapolated value and the exact mean is\n$$\nR(h) - m(T) = -\\frac{C_2}{2}h^2 + O(h^3).\n$$\nThe limit $L$ is then the coefficient of the $h^2$ term:\n$$\nL = \\lim_{h \\to 0} \\frac{R(h) - m(T)}{h^2} = -\\frac{C_2}{2}.\n$$\nSubstituting the expression for $C_2$:\n$$\nL = -\\frac{1}{2} \\left[ x_0 \\exp(-aT) \\left(\\frac{a^4T^2}{8} - \\frac{a^3T}{3}\\right) \\right]\n$$\n$$\nL = x_0 \\exp(-aT) \\left(-\\frac{a^4T^2}{16} + \\frac{a^3T}{6}\\right).\n$$\nThis is the desired closed-form expression for the leading constant $L$.\n$$\nL = x_0 \\exp(-aT) \\left( \\frac{a^3 T}{6} - \\frac{a^4 T^2}{16} \\right).\n$$", "answer": "$$\n\\boxed{x_{0} \\exp(-aT) \\left( \\frac{a^{3} T}{6} - \\frac{a^{4} T^{2}}{16} \\right)}\n$$", "id": "3083381"}, {"introduction": "Ultimately, the goal of error analysis is to design efficient and reliable simulations. This final exercise connects the theoretical concept of weak order to the practical task of running a Monte Carlo simulation within a specific error tolerance $\\varepsilon$. You will learn how to optimally balance the systematic error from time discretization, controlled by step size $h$, and the statistical error from finite sampling, controlled by the number of paths $M$, to achieve a desired accuracy with minimal computational cost [@problem_id:3083399].", "problem": "Consider a one-dimensional stochastic differential equation (SDE) given by $dX_{t} = a(X_{t})\\,dt + b(X_{t})\\,dW_{t}$ with deterministic initial condition $X_{0} = x_{0}$ on a fixed time horizon $[0,T]$, where $a$ and $b$ are globally Lipschitz continuous functions and $(W_{t})_{t \\geq 0}$ is a standard Brownian motion. Let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a test function with polynomial growth, and denote the quantity of interest by $\\mu := \\mathbb{E}[\\varphi(X_{T})]$.\n\nTo approximate $\\mu$, consider the Euler–Maruyama (EM) scheme with time step $h>0$ and $N := T/h \\in \\mathbb{N}$ steps:\n$$\nY_{0} := x_{0}, \\quad\nY_{n+1} := Y_{n} + a(Y_{n})\\,h + b(Y_{n})\\,\\Delta W_{n}, \\quad \\Delta W_{n} \\sim \\mathcal{N}(0,h), \\quad n=0,1,\\dots,N-1.\n$$\nDefine the Monte Carlo estimator based on $M \\in \\mathbb{N}$ independent and identically distributed paths $(Y_{N}^{(m)})_{m=1}^{M}$ of the EM terminal value:\n$$\n\\widehat{\\mu}_{M,h} := \\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big).\n$$\n\nAssume the following two properties hold uniformly for sufficiently small $h$:\n1. Weak order one for Euler–Maruyama: there exists a constant $C_{w}>0$ such that\n$$\n\\big|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{T})]\\big| \\leq C_{w}\\,h.\n$$\n2. Unit variance bound for the payoff under discretization: there exists a constant $C_{v} \\leq 1$ such that\n$$\n\\mathrm{Var}\\big(\\varphi(Y_{N})\\big) \\leq C_{v} \\leq 1.\n$$\n\nUsing the mean squared error (MSE) of the estimator $\\widehat{\\mu}_{M,h}$, defined by\n$$\n\\mathrm{MSE}(M,h) := \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big],\n$$\nderive the asymptotic scalings of the time step $h$ and the number of samples $M$ in terms of a user-prescribed accuracy $\\varepsilon \\in (0,1)$ such that the root-mean-square error $\\sqrt{\\mathrm{MSE}(M,h)}$ is bounded by $\\varepsilon$ up to multiplicative constants that are independent of $\\varepsilon$. Express your final answer as a single row matrix containing the leading-order scalings of $h$ and $M$ in terms of $\\varepsilon$.", "solution": "The problem statement is parsed and validated as follows.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $dX_{t} = a(X_{t})\\,dt + b(X_{t})\\,dW_{t}$\n- **Initial Condition:** $X_{0} = x_{0}$ (deterministic)\n- **Time Horizon:** $[0,T]$\n- **Coefficients:** $a$ and $b$ are globally Lipschitz continuous.\n- **Stochastic Process:** $(W_{t})_{t \\geq 0}$ is a standard Brownian motion.\n- **Test Function:** $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ with polynomial growth.\n- **Quantity of Interest:** $\\mu := \\mathbb{E}[\\varphi(X_{T})]$\n- **Euler–Maruyama (EM) Scheme:**\n  - Time step $h>0$ with $N := T/h \\in \\mathbb{N}$.\n  - $Y_{0} := x_{0}$\n  - $Y_{n+1} := Y_{n} + a(Y_{n})\\,h + b(Y_{n})\\,\\Delta W_{n}$ for $n=0,1,\\dots,N-1$.\n  - $\\Delta W_{n}$ are independent and identically distributed (i.i.d.) with $\\Delta W_{n} \\sim \\mathcal{N}(0,h)$.\n- **Monte Carlo Estimator:**\n  - $\\widehat{\\mu}_{M,h} := \\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)$, where $M \\in \\mathbb{N}$ is the number of i.i.d. paths.\n- **Assumption 1 (Weak Order):** There exists a constant $C_{w}>0$ such that $\\big|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{T})]\\big| \\leq C_{w}\\,h$.\n- **Assumption 2 (Variance Bound):** There exists a constant $C_{v} \\leq 1$ such that $\\mathrm{Var}\\big(\\varphi(Y_{N})\\big) \\leq C_{v}$.\n- **Error Metric:** Mean Squared Error (MSE), $\\mathrm{MSE}(M,h) := \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big]$.\n- **Objective:** Derive the asymptotic scalings of $h$ and $M$ in terms of a prescribed accuracy $\\varepsilon \\in (0,1)$ such that $\\sqrt{\\mathrm{MSE}(M,h)} \\leq \\varepsilon$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically well-grounded in the field of numerical analysis for stochastic differential equations. It presents a standard task: analyzing the error of a Monte Carlo method combined with a time discretization scheme. The premises, including the form of the SDE, the properties of the coefficients, the Euler-Maruyama scheme, and the assumptions on weak order and variance, are all standard in the literature (e.g., in the works of Kloeden & Platen, or Glasserman). The problem is well-posed, objective, self-contained, and free from any scientific or logical flaws.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\nThe goal is to find the asymptotic scalings of the time step $h$ and the number of Monte Carlo samples $M$ that ensure the root-mean-square error (RMSE) is bounded by a given tolerance $\\varepsilon$. The RMSE is defined as $\\sqrt{\\mathrm{MSE}(M,h)}$. The condition is $\\sqrt{\\mathrm{MSE}(M,h)} \\leq \\varepsilon$, which is equivalent to $\\mathrm{MSE}(M,h) \\leq \\varepsilon^{2}$.\n\nThe Mean Squared Error can be decomposed into the square of the bias and the variance of the estimator:\n$$\n\\mathrm{MSE}(M,h) = \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big] = \\Big(\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu\\Big)^{2} + \\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big).\n$$\n\nFirst, we analyze the bias term, $\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu$. The expectation of the estimator $\\widehat{\\mu}_{M,h}$ is:\n$$\n\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] = \\mathbb{E}\\left[\\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)\\right].\n$$\nBy linearity of expectation, and since the paths $(Y_{N}^{(m)})_{m=1}^{M}$ are independent and identically distributed, we have:\n$$\n\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] = \\frac{1}{M}\\sum_{m=1}^{M} \\mathbb{E}\\left[\\varphi\\big(Y_{N}^{(m)}\\big)\\right] = \\frac{1}{M} \\cdot M \\cdot \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right] = \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right].\n$$\nThe bias is therefore the weak error of the Euler-Maruyama scheme:\n$$\n\\text{Bias} = \\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu = \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right] - \\mathbb{E}\\left[\\varphi\\big(X_{T}\\big)\\right].\n$$\nUsing Assumption 1, the magnitude of the bias is bounded by:\n$$\n\\big|\\text{Bias}\\big| \\leq C_{w}h.\n$$\nThus, the squared bias is bounded by:\n$$\n(\\text{Bias})^{2} \\leq (C_{w}h)^{2} = C_{w}^{2}h^{2}.\n$$\n\nSecond, we analyze the variance term, $\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big)$.\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)\\right).\n$$\nSince the paths are i.i.d., the random variables $\\varphi\\big(Y_{N}^{(m)}\\big)$ are also i.i.d. For a sum of i.i.d. random variables, the variance is:\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) = \\frac{1}{M^{2}}\\sum_{m=1}^{M} \\mathrm{Var}\\left(\\varphi\\big(Y_{N}^{(m)}\\big)\\right) = \\frac{1}{M^{2}} \\cdot M \\cdot \\mathrm{Var}\\left(\\varphi\\big(Y_{N}\\big)\\right) = \\frac{1}{M} \\mathrm{Var}\\left(\\varphi\\big(Y_{N}\\big)\\right).\n$$\nUsing Assumption 2, the variance is bounded by:\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) \\leq \\frac{C_{v}}{M}.\n$$\n\nCombining the bounds for the squared bias and the variance, we get an upper bound for the MSE:\n$$\n\\mathrm{MSE}(M,h) \\leq C_{w}^{2}h^{2} + \\frac{C_{v}}{M}.\n$$\nTo satisfy the accuracy requirement $\\mathrm{MSE}(M,h) \\leq \\varepsilon^{2}$, we must have:\n$$\nC_{w}^{2}h^{2} + \\frac{C_{v}}{M} \\leq \\varepsilon^{2}.\n$$\nTo find the asymptotic scalings of $h$ and $M$ with respect to $\\varepsilon$, we need to choose $h$ and $M$ such that this inequality holds. A standard approach is to balance the two error components, the discretization error (from $h$) and the statistical error (from $M$), by making them both of the same order of magnitude as the target total squared error, $\\varepsilon^{2}$. We can require each term to be bounded by $\\frac{\\varepsilon^{2}}{2}$:\n$$\nC_{w}^{2}h^{2} \\leq \\frac{\\varepsilon^{2}}{2} \\quad \\text{and} \\quad \\frac{C_{v}}{M} \\leq \\frac{\\varepsilon^{2}}{2}.\n$$\nSumming these two inequalities guarantees that the total MSE is bounded by $\\varepsilon^{2}$.\n\nFrom the first inequality, we derive the scaling for $h$:\n$$\nh^{2} \\leq \\frac{\\varepsilon^{2}}{2C_{w}^{2}} \\implies h \\leq \\frac{\\varepsilon}{\\sqrt{2}C_{w}}.\n$$\nThis indicates that to achieve the desired accuracy, $h$ must be chosen to be proportional to $\\varepsilon$. The asymptotic scaling is $h = \\mathcal{O}(\\varepsilon)$.\n\nFrom the second inequality, we derive the scaling for $M$:\n$$\nM \\geq \\frac{2C_{v}}{\\varepsilon^{2}}.\n$$\nThis indicates that $M$ must be chosen to be proportional to $\\varepsilon^{-2}$. The asymptotic scaling is $M = \\mathcal{O}(\\varepsilon^{-2})$.\n\nThe leading-order scalings are the dominant dependencies on $\\varepsilon$, which are $\\varepsilon$ for $h$ and $\\varepsilon^{-2}$ for $M$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\varepsilon & \\varepsilon^{-2}\n\\end{pmatrix}\n}\n$$", "id": "3083399"}]}