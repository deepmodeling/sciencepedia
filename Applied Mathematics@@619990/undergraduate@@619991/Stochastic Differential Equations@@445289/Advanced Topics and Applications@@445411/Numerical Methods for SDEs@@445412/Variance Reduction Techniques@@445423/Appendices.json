{"hands_on_practices": [{"introduction": "The control variates method is a cornerstone technique for improving the precision of Monte Carlo estimates. It operates on a simple but powerful principle: if you want to estimate the mean of a random variable $Y$, you can use another random variable $C$ that is correlated with $Y$ and whose mean is known exactly. By subtracting a scaled version of the \"error\" in $C$ from $Y$, we construct a new estimator with lower variance. This exercise [@problem_id:1348947] provides hands-on practice in calculating the optimal scaling factor $\\beta^*$ that minimizes this variance, giving the most efficient estimate possible.", "problem": "In Monte Carlo methods, a common task is to estimate the value of an integral $I = \\int_a^b g(x) dx$. This is often achieved by reformulating the integral as an expected value, $E[f(X)]$, where $X$ is a random variable and $f$ is a related function. A standard 'crude' Monte Carlo estimator then uses the sample mean of $N$ independent draws of $f(X_i)$.\n\nTo improve the precision of the estimation for a given number of samples, variance reduction techniques are employed. One such technique is the method of control variates. This involves finding a random variable $C$ that is correlated with $f(X)$ and has a known expectation, $\\mu_C = E[C]$. A new, improved estimator is then formed as $Z(b) = f(X) - b(C - \\mu_C)$, where $b$ is a constant coefficient. The value of $b$ is chosen to minimize the variance of $Z(b)$. This optimal coefficient, denoted $b^*$, is given by the formula:\n$$b^* = \\frac{\\text{Cov}(f(X), C)}{\\text{Var}(C)}$$\n\nConsider the problem of estimating the integral $I = \\int_0^1 \\cos\\left(\\frac{\\pi x}{2}\\right) dx$. This is equivalent to finding the expected value of $Y = \\cos\\left(\\frac{\\pi X}{2}\\right)$ where $X$ is a random variable uniformly distributed on the interval $[0, 1]$. We will use the random variable $C = X$ as a control variate, since it is correlated with $Y$ and its properties are easily determined.\n\nYour task is to calculate the exact theoretical value of the optimal coefficient $b^*$ for this control variate. Express your answer as a single closed-form analytic expression in terms of $\\pi$.", "solution": "We have $X \\sim \\text{Unif}[0,1]$, $Y=\\cos\\left(\\frac{\\pi X}{2}\\right)$, and $C=X$. The optimal control variate coefficient is\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}.\n$$\nFirst compute $\\text{Var}(C)$. Using $E[X]=\\int_{0}^{1}x\\,dx=\\frac{1}{2}$ and $E[X^{2}]=\\int_{0}^{1}x^{2}\\,dx=\\frac{1}{3}$, we obtain\n$$\n\\text{Var}(C)=E[X^{2}]-\\left(E[X]\\right)^{2}=\\frac{1}{3}-\\frac{1}{4}=\\frac{1}{12}.\n$$\nNext compute $E[Y]$:\n$$\nE[Y]=\\int_{0}^{1}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,dx=\\left.\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi x}{2}\\right)\\right|_{0}^{1}=\\frac{2}{\\pi}.\n$$\nNow compute $E[CY]=E\\!\\left[X\\cos\\left(\\frac{\\pi X}{2}\\right)\\right]$. Let $a=\\frac{\\pi}{2}$. Then\n$$\nE[CY]=\\int_{0}^{1}x\\cos(ax)\\,dx.\n$$\nUsing integration by parts with $u=x$, $dv=\\cos(ax)\\,dx$, so $du=dx$ and $v=\\frac{1}{a}\\sin(ax)$, we get\n$$\n\\int x\\cos(ax)\\,dx=\\frac{x\\sin(ax)}{a}+\\frac{\\cos(ax)}{a^{2}}.\n$$\nEvaluating from $0$ to $1$,\n$$\n\\int_{0}^{1}x\\cos(ax)\\,dx=\\frac{\\sin(a)}{a}+\\frac{\\cos(a)-1}{a^{2}}.\n$$\nWith $a=\\frac{\\pi}{2}$, we have $\\sin(a)=1$ and $\\cos(a)=0$, hence\n$$\nE[CY]=\\frac{1}{a}-\\frac{1}{a^{2}}=\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}.\n$$\nTherefore,\n$$\n\\text{Cov}(Y,C)=E[CY]-E[Y]E[C]=\\left(\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}\\right)-\\left(\\frac{2}{\\pi}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{\\pi}-\\frac{4}{\\pi^{2}}=\\frac{\\pi-4}{\\pi^{2}}.\n$$\nFinally,\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}=\\frac{\\frac{\\pi-4}{\\pi^{2}}}{\\frac{1}{12}}=\\frac{12(\\pi-4)}{\\pi^{2}}.\n$$", "answer": "$$\\boxed{\\frac{12(\\pi - 4)}{\\pi^{2}}}$$", "id": "1348947"}, {"introduction": "The conditional Monte Carlo method is based on an elegant insight from the law of total variance: conditioning reduces variance. Instead of simulating all sources of randomness in a problem, we can sometimes compute the expectation with respect to one variable analytically, leaving less work for the simulation to do. This practice problem [@problem_id:1348948] asks you to derive a new, more precise estimator by conditioning and to see firsthand how this process can simplify a problem and dramatically reduce the estimator's variance.", "problem": "In a simplified model of a digital communications system, the detection of a bit depends on the relationship between a signal's amplitude, $X$, and a noise-dependent threshold. Both the signal amplitude $X$ and a noise-related variable $Y$ are modeled as independent random variables, uniformly distributed on the interval $[0, 1]$. A successful detection occurs if $X > Y^2$.\n\nWe wish to estimate the probability of successful detection, $\\theta = P(X > Y^2)$, using a Monte Carlo method. A standard approach involves defining an indicator random variable $I = \\mathbf{1}_{X > Y^2}$ and estimating its expectation. To improve upon this, we will use the conditional Monte Carlo technique.\n\nLet $Z$ be the new estimator obtained by taking the conditional expectation of $I$ with respect to the random variable $Y$. That is, $Z = E[I | Y]$. Your tasks are:\n1.  Determine the analytical expression for the random variable $Z$ as a function of $Y$.\n2.  Calculate the exact variance of this new estimator, $\\text{Var}(Z)$.\n\nProvide your answers as a pair of values: the expression for $Z$ and the numerical value for $\\text{Var}(Z)$.", "solution": "Let $X$ and $Y$ be independent and uniformly distributed on $[0,1]$. Define $I=\\mathbf{1}_{X>Y^{2}}$. The conditional Monte Carlo estimator is $Z=E[I\\mid Y]$.\n\n1) For a fixed $Y=y\\in[0,1]$, independence gives\n$$\nZ(y)=E[I\\mid Y=y]=P(X>y^{2}\\mid Y=y)=P(X>y^{2})=\\int_{0}^{1}\\mathbf{1}_{x>y^{2}}\\,f_{X}(x)\\,dx.\n$$\nSince $f_{X}(x)=1$ on $[0,1]$, this becomes\n$$\nZ(y)=\\int_{y^{2}}^{1}1\\,dx=1-y^{2}.\n$$\nThus, as a random variable,\n$$\nZ=1-Y^{2}.\n$$\n\n2) The variance of $Z$ is\n$$\n\\operatorname{Var}(Z)=\\operatorname{Var}(1-Y^{2})=\\operatorname{Var}(Y^{2})=E[Y^{4}]-\\left(E[Y^{2}]\\right)^{2}.\n$$\nFor $Y\\sim\\text{Uniform}(0,1)$, $E[Y^{k}]=\\frac{1}{k+1}$ for integers $k\\geq 0$, hence $E[Y^{2}]=\\frac{1}{3}$ and $E[Y^{4}]=\\frac{1}{5}$. Therefore,\n$$\n\\operatorname{Var}(Z)=\\frac{1}{5}-\\left(\\frac{1}{3}\\right)^{2}=\\frac{1}{5}-\\frac{1}{9}=\\frac{9-5}{45}=\\frac{4}{45}.\n$$\nEquivalently, one can verify via $E[Z]=E[1-Y^{2}]=1-\\frac{1}{3}=\\frac{2}{3}$ and $E[Z^{2}]=E[(1-Y^{2})^{2}]=1-2E[Y^{2}]+E[Y^{4}]=1-\\frac{2}{3}+\\frac{1}{5}=\\frac{8}{15}$, yielding $\\operatorname{Var}(Z)=\\frac{8}{15}-\\left(\\frac{2}{3}\\right)^{2}=\\frac{4}{45}$.\n\nHence, $Z=1-Y^{2}$ and $\\operatorname{Var}(Z)=\\frac{4}{45}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 - Y^{2} & \\frac{4}{45} \\end{pmatrix}}$$", "id": "1348948"}, {"introduction": "Antithetic variates are a popular technique intended to reduce variance by introducing negative correlation between samples. For a simulation driven by a uniform random variable $X$, using the pair $(X, 1-X)$ is a standard approach that works exceptionally well for monotonic functions. However, understanding a tool's limitations is as critical as knowing its strengths. This insightful problem [@problem_id:2446675] serves as a crucial cautionary tale, demonstrating a scenario where this exact technique, when applied to a non-monotonic function, unexpectedly *increases* the variance, forcing you to think critically about the conditions under which these methods succeed.", "problem": "In a Monte Carlo (MC) simulation for pricing in computational finance, consider estimating the expected payoff $\\mu = \\mathbb{E}[g(X)]$ where $X \\sim U(0,1)$ and $g:[0,1]\\to\\mathbb{R}$ is a binary payoff function. Construct the payoff by taking a symmetric union of two boundary intervals: for a parameter $a \\in (0,\\frac{1}{2})$, define\n$$\ng(x) = \\mathbf{1}\\{x \\in [0,a] \\cup [1-a,1]\\}.\n$$\nLet $\\widehat{\\mu}_{c}$ be the crude MC estimator based on $2$ independent samples $X_{1},X_{2} \\sim U(0,1)$:\n$$\n\\widehat{\\mu}_{c} = \\frac{g(X_{1}) + g(X_{2})}{2},\n$$\nand let $\\widehat{\\mu}_{a}$ be the antithetic variates estimator based on one antithetic pair $(X,1-X)$ with $X \\sim U(0,1)$:\n$$\n\\widehat{\\mu}_{a} = \\frac{g(X) + g(1-X)}{2}.\n$$\nCompute the exact variance ratio\n$$\nR \\equiv \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})}.\n$$\nGive your answer as a single real number; no rounding is required.", "solution": "The problem requires the computation of the variance ratio $R \\equiv \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})}$, where $\\widehat{\\mu}_{c}$ is the crude Monte Carlo estimator and $\\widehat{\\mu}_{a}$ is the antithetic variates estimator for the expected value $\\mu = \\mathbb{E}[g(X)]$.\n\nFirst, we define the random variable $Y = g(X)$, where $X \\sim U(0,1)$ and the payoff function is given by $g(x) = \\mathbf{1}\\{x \\in [0,a] \\cup [1-a,1]\\}$ for $a \\in (0, \\frac{1}{2})$. The random variable $Y$ is a Bernoulli variable, as it can only take values $0$ or $1$.\n\nThe probability of success, $p$, is the expected value of $Y$:\n$$\np = \\mathbb{E}[Y] = \\mathbb{E}[g(X)] = \\int_{0}^{1} g(x) dx = \\int_{0}^{a} 1 \\,dx + \\int_{1-a}^{1} 1 \\,dx = a + (1 - (1-a)) = 2a.\n$$\nSince $Y$ is a Bernoulli random variable with parameter $p=2a$, its variance is given by:\n$$\n\\operatorname{Var}(Y) = p(1-p) = 2a(1-2a).\n$$\nThis can also be computed directly:\n$$\n\\mathbb{E}[Y^{2}] = \\mathbb{E}[g(X)^2] = \\mathbb{E}[g(X)] = 2a,\n$$\nbecause $g(x)^2 = g(x)$ for an indicator function.\nTherefore,\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = 2a - (2a)^2 = 2a(1-2a).\n$$\nSince $a \\in (0, \\frac{1}{2})$, we have $a > 0$ and $1-2a > 0$, which ensures that $\\operatorname{Var}(Y) > 0$.\n\nNext, we compute the variance of the crude Monte Carlo estimator, $\\widehat{\\mu}_{c}$.\nThis estimator is defined as $\\widehat{\\mu}_{c} = \\frac{g(X_{1}) + g(X_{2})}{2}$, where $X_{1}, X_{2}$ are independent and identically distributed (i.i.d.) samples from $U(0,1)$. Let $Y_1 = g(X_1)$ and $Y_2 = g(X_2)$. $Y_1$ and $Y_2$ are i.i.d. random variables with $\\operatorname{Var}(Y_1) = \\operatorname{Var}(Y_2) = \\operatorname{Var}(Y) = 2a(1-2a)$.\nDue to the independence of $Y_1$ and $Y_2$, the variance of their sum is the sum of their variances.\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{c}) = \\operatorname{Var}\\left(\\frac{Y_{1} + Y_{2}}{2}\\right) = \\frac{1}{4}\\operatorname{Var}(Y_1 + Y_2) = \\frac{1}{4}(\\operatorname{Var}(Y_{1}) + \\operatorname{Var}(Y_{2})).\n$$\nSubstituting the variance of $Y$:\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{c}) = \\frac{1}{4}(2a(1-2a) + 2a(1-2a)) = \\frac{1}{4}(4a(1-2a)) = a(1-2a).\n$$\nNow, we compute the variance of the antithetic variates estimator, $\\widehat{\\mu}_{a}$.\nThe estimator is defined as $\\widehat{\\mu}_{a} = \\frac{g(X) + g(1-X)}{2}$. We must analyze the relationship between $g(X)$ and $g(1-X)$.\nThe function $g(x)$ is symmetric about $x=\\frac{1}{2}$. Let's verify this property for $g(1-x)$:\n$$\ng(1-x) = \\mathbf{1}\\{1-x \\in [0,a] \\cup [1-a,1]\\}.\n$$\nThe condition $1-x \\in [0,a]$ is equivalent to $1-a \\le x \\le 1$.\nThe condition $1-x \\in [1-a,1]$ is equivalent to $0 \\le x \\le a$.\nThus, $g(1-x) = 1$ if and only if $x \\in [0,a] \\cup [1-a,1]$. This is the same condition for $g(x)=1$. For all other $x \\in (0,1)$, both $g(x)$ and $g(1-x)$ are $0$.\nTherefore, $g(x) = g(1-x)$ for all $x \\in [0,1]$.\n\nThis implies that the random variables $g(X)$ and $g(1-X)$ are identical. The antithetic estimator simplifies to:\n$$\n\\widehat{\\mu}_{a} = \\frac{g(X) + g(X)}{2} = g(X) = Y.\n$$\nThe variance of the antithetic estimator is thus the variance of $Y$:\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{a}) = \\operatorname{Var}(g(X)) = 2a(1-2a).\n$$\nFinally, we compute the desired ratio $R$:\n$$\nR = \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})} = \\frac{2a(1-2a)}{a(1-2a)}.\n$$\nSince $a \\in (0, \\frac{1}{2})$, the term $a(1-2a)$ is strictly positive and can be cancelled from the numerator and the denominator.\n$$\nR = 2.\n$$\nThe ratio is a constant value, independent of the parameter $a$. This demonstrates a case where the antithetic sampling technique, applied to a symmetric function, increases the variance compared to the crude Monte Carlo method for the same number of function evaluations. Specifically, it doubles the sampling variance.", "answer": "$$\n\\boxed{2}\n$$", "id": "2446675"}]}