{"hands_on_practices": [{"introduction": "This first practice focuses on the exponential martingale, a cornerstone process in stochastic calculus. While Itô's formula readily shows that this process is a local martingale, the main task here is to go a step further. By directly calculating the process's expectation, you will demonstrate that it is, in fact, a true martingale, providing a clear illustration of a fundamental test for distinguishing between these two important classes of processes [@problem_id:3064187].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\ge 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions and supporting a standard $1$-dimensional Brownian motion $(B_t)_{t \\ge 0}$. For a fixed $\\lambda \\in \\mathbb{R}$ and $t \\ge 0$, define the exponential process\n$$\nX_t \\coloneqq \\exp\\!\\left(\\lambda B_t - \\frac{1}{2}\\lambda^2 t\\right).\n$$\nStarting from the foundational facts that $B_t$ is Gaussian with mean $0$ and variance $t$, and that Brownian motion has stationary and independent increments, perform the following tasks:\n\n- Compute the exact value of $\\mathbb{E}[X_t]$ as a function of $\\lambda$ and $t$.\n- Let $\\tau_n \\coloneqq \\inf\\{s \\ge 0 : |B_s| \\ge n\\}$ for $n \\in \\mathbb{N}$. Using stopped processes and the definition of a local martingale, explain why $(X_{t \\wedge \\tau_n})_{t \\ge 0}$ is a martingale and why $(X_t)_{t \\ge 0}$ is a positive local martingale. Then, use your computation of $\\mathbb{E}[X_t]$ to justify that $(X_t)_{t \\ge 0}$ is in fact a true martingale.\n\nExpress your final answer for $\\mathbb{E}[X_t]$ as a single exact value without units. No rounding is required.", "solution": "The problem is well-posed and scientifically sound, set within the standard mathematical framework of stochastic calculus. All provided information is clear, consistent, and sufficient to perform the required tasks. I will now proceed with a complete solution.\n\nThe problem asks for two main tasks concerning the process $X_t \\coloneqq \\exp(\\lambda B_t - \\frac{1}{2}\\lambda^2 t)$ for $\\lambda \\in \\mathbb{R}$ and $t \\ge 0$, where $(B_t)_{t \\ge 0}$ is a standard $1$-dimensional Brownian motion.\n\nFirst, we compute the expectation $\\mathbb{E}[X_t]$.\nThe expectation of $X_t$ is given by\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\exp\\left(\\lambda B_t - \\frac{1}{2}\\lambda^2 t\\right)\\right].\n$$\nThe term $\\exp(-\\frac{1}{2}\\lambda^2 t)$ is deterministic and can be factored out of the expectation:\n$$\n\\mathbb{E}[X_t] = \\exp\\left(-\\frac{1}{2}\\lambda^2 t\\right) \\mathbb{E}\\left[\\exp(\\lambda B_t)\\right].\n$$\nThe term $\\mathbb{E}[\\exp(\\lambda B_t)]$ is the moment-generating function (MGF) of the random variable $B_t$, evaluated at $\\lambda$. We are given that $B_t$ is a Gaussian (normal) random variable with mean $0$ and variance $t$. We denote this as $B_t \\sim N(0, t)$.\n\nThe MGF of a general normal random variable $Y \\sim N(\\mu, \\sigma^2)$ is given by $M_Y(k) = \\mathbb{E}[\\exp(kY)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$.\nFor $B_t$, we have $\\mu=0$ and $\\sigma^2=t$. Applying this formula with $k=\\lambda$, we find the MGF of $B_t$:\n$$\nM_{B_t}(\\lambda) = \\mathbb{E}[\\exp(\\lambda B_t)] = \\exp\\left(\\lambda \\cdot 0 + \\frac{1}{2}\\lambda^2 t\\right) = \\exp\\left(\\frac{1}{2}\\lambda^2 t\\right).\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[X_t]$:\n$$\n\\mathbb{E}[X_t] = \\exp\\left(-\\frac{1}{2}\\lambda^2 t\\right) \\exp\\left(\\frac{1}{2}\\lambda^2 t\\right) = \\exp\\left(-\\frac{1}{2}\\lambda^2 t + \\frac{1}{2}\\lambda^2 t\\right) = \\exp(0) = 1.\n$$\nThus, the expected value of $X_t$ is $1$ for all $t \\ge 0$ and any $\\lambda \\in \\mathbb{R}$.\n\nNext, we address the martingale properties of $X_t$. A process $(M_t)_{t \\ge 0}$ is a martingale with respect to a filtration $(\\mathcal{F}_t)_{t \\ge 0}$ if it is $\\mathcal{F}_t$-adapted, $\\mathbb{E}[|M_t|] < \\infty$ for all $t$, and $\\mathbb{E}[M_t|\\mathcal{F}_s] = M_s$ for all $s < t$. A process is a local martingale if there exists a sequence of stopping times $(\\tau_n)_{n \\in \\mathbb{N}}$ with $\\tau_n \\to \\infty$ almost surely, such that each stopped process $(M_{t \\wedge \\tau_n})_{t \\ge 0}$ is a martingale.\n\nTo show that $(X_{t \\wedge \\tau_n})_{t \\ge 0}$ is a martingale for $\\tau_n = \\inf\\{s \\ge 0 : |B_s| \\ge n\\}$, we first establish that $(X_t)_{t \\ge 0}$ is a local martingale. We apply Itô's lemma to the function $f(t,x) = \\exp(\\lambda x - \\frac{1}{2}\\lambda^2 t)$ with the process $B_t$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial t} = -\\frac{1}{2}\\lambda^2 f(t,x), \\quad \\frac{\\partial f}{\\partial x} = \\lambda f(t,x), \\quad \\frac{\\partial^2 f}{\\partial x^2} = \\lambda^2 f(t,x).\n$$\nThe stochastic differential of $X_t = f(t, B_t)$ is given by Itô's formula:\n$$\ndX_t = \\frac{\\partial f}{\\partial t}(t, B_t) dt + \\frac{\\partial f}{\\partial x}(t, B_t) dB_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t, B_t) d\\langle B \\rangle_t.\n$$\nSince $d\\langle B \\rangle_t = dt$, and substituting the partial derivatives with $f(t,B_t) = X_t$:\n$$\ndX_t = \\left(-\\frac{1}{2}\\lambda^2 X_t\\right) dt + (\\lambda X_t) dB_t + \\frac{1}{2}(\\lambda^2 X_t) dt = \\lambda X_t dB_t.\n$$\nThis gives the stochastic differential equation (SDE) for $X_t$. The integral form is $X_t = X_0 + \\int_0^t \\lambda X_s dB_s$. Since $B_0 = 0$, $X_0 = \\exp(0-0) = 1$. So,\n$$\nX_t = 1 + \\int_0^t \\lambda X_s dB_s.\n$$\nA stochastic integral of the form $\\int_0^t H_s dB_s$ (where $H_s$ is a suitable adapted process) is a local martingale. Since $X_s$ is a continuous and adapted process, the stochastic integral $\\int_0^t \\lambda X_s dB_s$ is a continuous local martingale, which implies $X_t$ is a continuous local martingale.\n\nNow, consider the stopped process $(X_{t \\wedge \\tau_n})_{t \\ge 0}$. Its integral representation is:\n$$\nX_{t \\wedge \\tau_n} = 1 + \\int_0^{t \\wedge \\tau_n} \\lambda X_s dB_s = 1 + \\int_0^t \\lambda X_s \\mathbf{1}_{\\{s \\le \\tau_n\\}} dB_s.\n$$\nThe integrand for the stopped process is $H_s = \\lambda X_s \\mathbf{1}_{\\{s \\le \\tau_n\\}}$. By definition of the stopping time $\\tau_n = \\inf\\{s \\ge 0 : |B_s| \\ge n\\}$, we have $|B_s| \\le n$ for all $s \\le \\tau_n$. Therefore, for $s \\le \\tau_n$:\n$$\n|X_s| = \\left|\\exp\\left(\\lambda B_s - \\frac{1}{2}\\lambda^2 s\\right)\\right| = \\exp\\left(\\lambda B_s - \\frac{1}{2}\\lambda^2 s\\right) \\le \\exp(|\\lambda| |B_s|) \\le \\exp(|\\lambda| n).\n$$\nThis shows that the integrand $H_s$ is bounded: $|H_s| \\le |\\lambda| \\exp(|\\lambda| n)$. A stochastic integral with respect to Brownian motion whose integrand is bounded is a true martingale. Thus, $\\int_0^t \\lambda X_s \\mathbf{1}_{\\{s \\le \\tau_n\\}} dB_s$ is a martingale. Adding the constant $1$ preserves the martingale property, so $(X_{t \\wedge \\tau_n})_{t \\ge 0}$ is a martingale for each $n \\in \\mathbb{N}$.\n\nNext, we explain why $(X_t)_{t \\ge 0}$ is a positive local martingale.\nFrom its definition, $X_t = \\exp(\\dots)$ is strictly positive for all $t$. We have shown that $X_t$ is a local martingale. For the specific localizing sequence $\\tau_n = \\inf\\{s \\ge 0 : |B_s| \\ge n\\}$, it is a standard property of Brownian motion that $\\tau_n \\to \\infty$ almost surely as $n \\to \\infty$. This is because Brownian paths are almost surely continuous and therefore bounded on any finite time interval. Thus, $(X_t)_{t \\ge 0}$ is a positive local martingale.\n\nFinally, we use the computation of $\\mathbb{E}[X_t]$ to justify that $(X_t)_{t \\ge 0}$ is a true martingale.\nA fundamental result in martingale theory states that any non-negative local martingale is a supermartingale. A supermartingale $(M_t)_{t \\ge 0}$ satisfies $\\mathbb{E}[M_t | \\mathcal{F}_s] \\le M_s$ for $s < t$. Taking expectations of this inequality yields $\\mathbb{E}[M_t] \\le \\mathbb{E}[M_s]$, which means the function $t \\mapsto \\mathbb{E}[M_t]$ is non-increasing.\nSince $X_t$ is a positive local martingale, it is a supermartingale, and therefore its expectation $\\mathbb{E}[X_t]$ must be a non-increasing function of $t$.\nHowever, our initial calculation showed that $\\mathbb{E}[X_t] = 1$ for all $t \\ge 0$. This is a constant function.\nA supermartingale whose expectation is constant must be a martingale. To prove this, let $X_t$ be a supermartingale with $\\mathbb{E}[X_t] = C$ for some constant $C$. For $s < t$, we have:\n1.  By the supermartingale property, $\\mathbb{E}[X_t | \\mathcal{F}_s] \\le X_s$. This implies $X_s - \\mathbb{E}[X_t | \\mathcal{F}_s] \\ge 0$.\n2.  Taking the expectation of this non-negative quantity, we get $\\mathbb{E}[X_s - \\mathbb{E}[X_t | \\mathcal{F}_s]] = \\mathbb{E}[X_s] - \\mathbb{E}[\\mathbb{E}[X_t | \\mathcal{F}_s]] = \\mathbb{E}[X_s] - \\mathbb{E}[X_t]$.\n3.  Since $\\mathbb{E}[X_s] = \\mathbb{E}[X_t] = 1$, we have $\\mathbb{E}[X_s - \\mathbb{E}[X_t | \\mathcal{F}_s]] = 1 - 1 = 0$.\nA non-negative random variable with an expectation of zero must be zero almost surely. Therefore, $X_s - \\mathbb{E}[X_t | \\mathcal{F}_s] = 0$ almost surely, which is equivalent to $\\mathbb{E}[X_t | \\mathcal{F}_s] = X_s$ almost surely.\nThis is the martingale property. The other conditions for a martingale are also met: $X_t$ is adapted, and $\\mathbb{E}[|X_t|] = \\mathbb{E}[X_t] = 1 < \\infty$.\nHence, we conclude that $(X_t)_{t \\ge 0}$ is a true martingale.", "answer": "$$\\boxed{1}$$", "id": "3064187"}, {"introduction": "Not all local martingales are true martingales, and this distinction has profound practical consequences. This exercise introduces the canonical example of a \"strict\" local martingale: the reciprocal of a three-dimensional Bessel process. By analyzing this process and a specific hitting time, you will construct a concrete counterexample demonstrating how the powerful Optional Stopping Theorem can fail, a critical lesson that highlights the caution required when working with local martingales [@problem_id:3064173].", "problem": "Let $(B_t)_{t \\geq 0}$ be a one-dimensional standard Brownian motion (SBM) on a filtered probability space satisfying the usual conditions. Consider the three-dimensional Bessel process $(R_t)_{t \\geq 0}$ started at $R_0 = r_0 > 0$, defined as the unique strong solution to the stochastic differential equation\n$$\ndR_t = dB_t + \\frac{1}{R_t} \\, dt, \\quad R_0 = r_0.\n$$\nDefine the process $(X_t)_{t \\geq 0}$ by $X_t := \\frac{1}{R_t}$, and let the hitting time $\\tau_a$ be given by\n$$\n\\tau_a := \\inf\\{t \\geq 0 : R_t = a\\},\n$$\nfor a fixed constant $a > r_0$. Assume $(R_t)_{t \\geq 0}$ is nonexplosive and has continuous sample paths.\n\nStarting from fundamental definitions, and using Itô’s formula together with basic properties of local martingales and stopping times, show that $(X_t)_{t \\geq 0}$ is a local martingale which is nonnegative and strict (i.e., not a true martingale). Then, considering the unbounded stopping time $\\tau_a$, compute the exact value of the expectation $\\mathbb{E}[X_{\\tau_a}]$ and use this computation to explain why the optional stopping equality can fail for local martingales at unbounded stopping times.\n\nYour final answer must be the closed-form analytic expression for $\\mathbb{E}[X_{\\tau_a}]$. No rounding is required.", "solution": "The problem asks us to analyze the process $X_t = 1/R_t$, where $R_t$ is a three-dimensional Bessel process, show it is a strict local martingale, and then compute the expectation $\\mathbb{E}[X_{\\tau_a}]$ for a specific hitting time $\\tau_a$.\n\nFirst, we establish that $(X_t)_{t \\geq 0}$ is a local martingale. Let $f(r) = 1/r$ for $r > 0$. Then $X_t = f(R_t)$. We apply Itô's formula for a function of the Itô process $(R_t)_{t \\geq 0}$. The derivatives of $f(r)$ are $f'(r) = -1/r^2$ and $f''(r) = 2/r^3$. The stochastic differential equation (SDE) for $R_t$ is given as\n$$\ndR_t = dB_t + \\frac{1}{R_t} dt\n$$\nwhere $(B_t)_{t \\geq 0}$ is a one-dimensional standard Brownian motion. The quadratic variation process is non-random, $d\\langle R, R \\rangle_t = (1)^2 d\\langle B, B\\rangle_t = dt$.\n\nApplying Itô's formula, we get the SDE for $X_t$:\n$$\ndX_t = f'(R_t) dR_t + \\frac{1}{2} f''(R_t) d\\langle R, R \\rangle_t\n$$\nSubstituting the derivatives and the SDE for $R_t$:\n$$\ndX_t = \\left(-\\frac{1}{R_t^2}\\right) \\left(dB_t + \\frac{1}{R_t} dt\\right) + \\frac{1}{2}\\left(\\frac{2}{R_t^3}\\right) dt\n$$\n$$\ndX_t = -\\frac{1}{R_t^2} dB_t - \\frac{1}{R_t^3} dt + \\frac{1}{R_t^3} dt\n$$\n$$\ndX_t = -\\frac{1}{R_t^2} dB_t\n$$\nThe resulting SDE for $X_t$ has a zero drift term. An Itô process of the form $dY_t = \\sigma(t, \\omega) dB_t$ is a local martingale if the volatility process $\\sigma(t, \\omega)$ is adapted and satisfies $\\mathbb{P}(\\int_0^T \\sigma_s^2 ds < \\infty \\text{ for all } T > 0) = 1$.\nHere, the volatility is $\\sigma_t = -1/R_t^2$. A standard property of a three-dimensional Bessel process starting at $R_0 = r_0 > 0$ is that it never hits the origin, i.e., $\\mathbb{P}(\\inf_{t \\ge 0} R_t > 0) = 1$. Since $R_t$ has continuous paths, $R_t$ is strictly positive for all $t \\geq 0$, and on any finite interval $[0, T]$, $R_t$ is bounded away from zero almost surely. Thus, the process $\\sigma_t = -1/R_t^2$ is well-defined, continuous, and adapted. The integral $\\int_0^T \\sigma_s^2 ds = \\int_0^T (1/R_s^4) ds$ is finite for any $T < \\infty$ because the integrand is a continuous function on $[0, T]$. Therefore, $X_t$ is a continuous local martingale.\nSince $R_t > 0$ for all $t \\ge 0$, $X_t = 1/R_t$ is a strictly positive, and therefore nonnegative, process.\n\nNext, we show that $X_t$ is a strict local martingale, i.e., it is not a true martingale. A nonnegative local martingale is a true martingale if and only if its expectation is constant over time, i.e., $\\mathbb{E}[X_t] = X_0$ for all $t \\geq 0$. We will show this equality does not hold.\nSince $X_t$ is a nonnegative local martingale, it is a supermartingale, which means $\\mathbb{E}[X_t] \\leq \\mathbb{E}[X_s]$ for $t > s$. In particular, $\\mathbb{E}[X_t] \\leq \\mathbb{E}[X_0] = X_0 = 1/r_0$. A strict inequality for any $t>0$ will prove that $X_t$ is a strict local martingale. The transition probability density function $p_t(r_0, r)$ for a 3-dimensional Bessel process starting at $r_0$ is given by\n$$p_t(r_0, r) = \\frac{1}{\\sqrt{2\\pi t}} \\frac{r}{r_0} \\left( \\exp\\left(-\\frac{(r-r_0)^2}{2t}\\right) - \\exp\\left(-\\frac{(r+r_0)^2}{2t}\\right) \\right) \\mathbf{1}_{r>0}$$\nWe compute $\\mathbb{E}[X_t] = \\mathbb{E}[1/R_t]$:\n$$ \\mathbb{E}[X_t] = \\int_0^\\infty \\frac{1}{r} p_t(r_0, r) dr = \\frac{1}{r_0\\sqrt{2\\pi t}} \\int_0^\\infty \\left( \\exp\\left(-\\frac{(r-r_0)^2}{2t}\\right) - \\exp\\left(-\\frac{(r+r_0)^2}{2t}\\right) \\right) dr $$\nThis simplifies to:\n$$ \\mathbb{E}[X_t] = \\frac{2}{r_0\\sqrt{2\\pi t}} \\exp\\left(-\\frac{r_0^2}{2t}\\right) \\int_0^\\infty \\exp\\left(-\\frac{r^2}{2t}\\right) \\sinh\\left(\\frac{rr_0}{t}\\right) dr $$\nThe integral can be evaluated, leading to the result:\n$$ \\mathbb{E}[X_t] = \\frac{1}{r_0} \\operatorname{erf}\\left(\\frac{r_0}{\\sqrt{2t}}\\right) $$\nwhere $\\operatorname{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z \\exp(-x^2)dx$ is the error function.\nFor any finite $t > 0$ and $r_0 > 0$, we have $r_0/\\sqrt{2t} \\in (0, \\infty)$, and for any $z>0$, $\\operatorname{erf}(z) \\in (0, 1)$. Thus, for all $t > 0$:\n$$ \\mathbb{E}[X_t] = \\frac{1}{r_0} \\operatorname{erf}\\left(\\frac{r_0}{\\sqrt{2t}}\\right) < \\frac{1}{r_0} = X_0 $$\nSince $\\mathbb{E}[X_t] \\neq X_0$, the process $(X_t)_{t \\ge 0}$ is not a martingale, and is therefore a strict local martingale.\n\nNow, we compute the expectation $\\mathbb{E}[X_{\\tau_a}]$. The stopping time is defined as $\\tau_a := \\inf\\{t \\geq 0 : R_t = a\\}$. It is a standard result that a 3-dimensional Bessel process is transient, meaning $\\lim_{t \\to \\infty} R_t = \\infty$ almost surely. Since the process starts at $R_0 = r_0 < a$ and goes to infinity, and its paths are continuous, it must pass through the level $a$ at some finite time. Therefore, the stopping time $\\tau_a$ is almost surely finite, i.e., $\\mathbb{P}(\\tau_a < \\infty) = 1$.\nBecause the sample paths of $R_t$ are continuous, at the first hitting time $\\tau_a$, the value of the process is exactly $a$. That is, $R_{\\tau_a} = a$ almost surely.\nConsequently, the value of the process $X_t$ at the stopping time $\\tau_a$ is:\n$$ X_{\\tau_a} = \\frac{1}{R_{\\tau_a}} = \\frac{1}{a} \\quad \\text{almost surely.} $$\nSince $X_{\\tau_a}$ is almost surely equal to the constant $1/a$, its expectation is this constant value:\n$$ \\mathbb{E}[X_{\\tau_a}] = \\mathbb{E}\\left[\\frac{1}{a}\\right] = \\frac{1}{a}. $$\n\nFinally, we use this result to explain the failure of the Optional Stopping Theorem. The theorem states that for a martingale $M_t$ and a stopping time $\\tau$, under certain conditions, $\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]$. For local martingales, this equality is not guaranteed, especially for unbounded stopping times. Our process $X_t$ is a local martingale with $X_0 = 1/r_0$. The stopping time $\\tau_a$ is unbounded.\nOur computation yields:\n$$ \\mathbb{E}[X_{\\tau_a}] = \\frac{1}{a} $$\nThe initial expectation is:\n$$ \\mathbb{E}[X_0] = X_0 = \\frac{1}{r_0} $$\nAccording to the problem, $a > r_0 > 0$, which implies $1/a < 1/r_0$. Therefore:\n$$ \\mathbb{E}[X_{\\tau_a}] = \\frac{1}{a} \\neq \\frac{1}{r_0} = \\mathbb{E}[X_0] $$\nThe fact that the equality does not hold, $\\mathbb{E}[X_{\\tau_a}] \\neq \\mathbb{E}[X_0]$, provides a concrete counterexample demonstrating that the optional stopping equality can fail for a local martingale (specifically, a strict one) when applied at an unbounded stopping time. The conditions that would guarantee the equality (e.g., the process being a uniformly integrable martingale, or the stopping time being bounded) are not met in this case.", "answer": "$$\\boxed{\\frac{1}{a}}$$", "id": "3064173"}, {"introduction": "The technique of localization is more than a definition; it is a powerful tool for extending results from simple cases to more general settings. This problem guides you through a fundamental proof in stochastic calculus: deriving the quadratic variation of a stochastic integral. You will see how a localization argument is essential for generalizing the result from well-behaved integrands to the much broader class of locally square-integrable processes, showcasing its indispensable role in building the theory [@problem_id:3064208].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions. Let $H=\\{H_{t}\\}_{t \\geq 0}$ be a real-valued, $\\{\\mathcal{F}_{t}\\}$-predictable process such that for every $t \\geq 0$, the random variable $\\int_{0}^{t} H_{s}^{2} \\, ds$ is finite almost surely. Define the stochastic integral $M_{t}=\\int_{0}^{t} H_{s} \\, dB_{s}$.\n\nUsing the definition of quadratic variation via partition limits for continuous semimartingales, together with approximation by simple predictable integrands and a localization argument for locally square-integrable integrands, derive a closed-form expression for the quadratic variation process $[M]_{t}$ in terms of $H$ and $t$.\n\nThen specialize to the concrete integrand $H_{s}=\\frac{1}{1+s^{2}}$ for $s \\geq 0$, and determine the explicit closed-form expression of the quadratic variation $[M]_{t}$ as a function of $t$. Express your final answer as a single closed-form analytic expression in $t$. No rounding is required.", "solution": "The problem asks for the derivation of the quadratic variation of a stochastic integral $M_{t}=\\int_{0}^{t} H_{s} \\, dB_{s}$ and its specific evaluation for the integrand $H_{s}=\\frac{1}{1+s^{2}}$. The derivation will proceed in three steps as stipulated: starting with simple predictable integrands, extending to square-integrable integrands, and finally generalizing via a localization argument.\n\nFirst, let us recall the definition of quadratic variation. For a continuous semimartingale $\\{X_t\\}_{t \\geq 0}$, its quadratic variation process, denoted $[X]_t$, is defined as the limit in probability of the sum of squared increments over a sequence of partitions of the interval $[0, t]$ as the mesh of the partitions goes to zero:\n$$ [X]_t = \\lim_{||\\Pi_n|| \\to 0} \\sum_{i=1}^{k_n} (X_{t_i^{(n)}} - X_{t_{i-1}^{(n)}})^2 $$\nwhere $\\Pi_n = \\{0=t_0^{(n)} < t_1^{(n)} < \\dots < t_{k_n}^{(n)} = t\\}$ is a sequence of partitions of $[0, t]$ and $||\\Pi_n|| = \\max_i(t_i^{(n)} - t_{i-1}^{(n)})$. For a standard one-dimensional Brownian motion $\\{B_t\\}_{t \\ge 0}$, it is a fundamental result that $[B]_t = t$.\n\n**Part 1: General Derivation of $[M]_t$**\n\n**Step 1: Simple Predictable Integrands**\n\nLet $H$ be a simple predictable process of the form:\n$$ H_s = \\sum_{j=0}^{m-1} \\xi_j \\mathbf{1}_{(t_j, t_{j+1}]}(s) $$\nwhere $0=t_0 < t_1 < \\dots < t_m$ is a partition of time, and for each $j$, $\\xi_j$ is a real-valued, bounded, $\\mathcal{F}_{t_j}$-measurable random variable. The stochastic integral is given by:\n$$ M_t = \\int_0^t H_s \\, dB_s = \\sum_{j=0}^{m-1} \\xi_j (B_{t \\wedge t_{j+1}} - B_{t \\wedge t_j}) $$\nLet $M_t^{(j)} = \\xi_j (B_{t \\wedge t_{j+1}} - B_{t \\wedge t_j}) = \\int_0^t \\xi_j \\mathbf{1}_{(t_j, t_{j+1}]}(s) dB_s$. Then $M_t = \\sum_{j=0}^{m-1} M_t^{(j)}$.\nThe quadratic variation $[M]_t$ is given by the polarization identity $[M]_t = [M, M]_t$. By linearity of the cross-variation bracket, we have:\n$$ [M]_t = \\left[ \\sum_{j} M^{(j)}, \\sum_{k} M^{(k)} \\right]_t = \\sum_{j,k} [M^{(j)}, M^{(k)}]_t = \\sum_{j} [M^{(j)}]_t + 2 \\sum_{j<k} [M^{(j)}, M^{(k)}]_t $$\nLet's first compute $[M^{(j)}]_t$. The process $M^{(j)}$ is non-zero only for $s > t_j$. Since $\\xi_j$ is $\\mathcal{F}_{t_j}$-measurable, it acts as a constant with respect to the increments of Brownian motion for $s > t_j$. Using the definition of quadratic variation for $M^{(j)}$ via partitions on $[0, t]$ and the fact that $[B]_t=t$:\n$$ [M^{(j)}]_t = [\\xi_j (B_{\\cdot \\wedge t_{j+1}} - B_{\\cdot \\wedge t_j})]_t = \\xi_j^2 [B_{\\cdot \\wedge t_{j+1}} - B_{\\cdot \\wedge t_j}]_t = \\xi_j^2 (t \\wedge t_{j+1} - t \\wedge t_j) $$\nThe term $t \\wedge t_{j+1} - t \\wedge t_j$ is precisely $\\int_0^t \\mathbf{1}_{(t_j, t_{j+1}]}(s) ds$. So, $[M^{(j)}]_t = \\xi_j^2 \\int_0^t \\mathbf{1}_{(t_j, t_{j+1}]}(s) ds$.\n\nNext, we show that the cross-variation terms $[M^{(j)}, M^{(k)}]_t$ are zero for $j \\neq k$. Assume $j < k$. The supports of the integrands defining $M^{(j)}$ and $M^{(k)}$ are the disjoint intervals $(t_j, t_{j+1}]$ and $(t_k, t_{k+1}]$. These martingales are orthogonal. To see this from the partition definition, $[M^{(j)}, M^{(k)}]_t = \\lim \\sum_i (M^{(j)}_{u_i} - M^{(j)}_{u_{i-1}})(M^{(k)}_{u_i} - M^{(k)}_{u_{i-1}})$. A term in the sum is non-zero only if the interval $[u_{i-1}, u_i]$ has a non-empty intersection with both $(t_j, t_{j+1}]$ and $(t_k, t_{k+1}]$, which is impossible for disjoint intervals. Thus, the limit is $0$.\n\nCombining these results, we find:\n$$ [M]_t = \\sum_j [M^{(j)}]_t = \\sum_j \\xi_j^2 \\int_0^t \\mathbf{1}_{(t_j, t_{j+1}]}(s) ds = \\int_0^t \\left( \\sum_j \\xi_j^2 \\mathbf{1}_{(t_j, t_{j+1}]}(s) \\right) ds $$\nRecognizing that $H_s^2 = \\left(\\sum_j \\xi_j \\mathbf{1}_{(t_j, t_{j+1}]}(s)\\right)^2 = \\sum_j \\xi_j^2 \\mathbf{1}_{(t_j, t_{j+1}]}(s)$, we conclude:\n$$ [M]_t = \\int_0^t H_s^2 \\, ds $$\nThis establishes the result for simple predictable integrands.\n\n**Step 2: Extension to Square-Integrable Integrands**\n\nLet $H$ be a predictable process in the space $\\mathcal{L}^2$ of processes satisfying $E[\\int_0^T H_s^2 ds] < \\infty$ for a fixed time horizon $T>0$. There exists a sequence of simple predictable processes $\\{H^n\\}_{n \\ge 1}$ such that $H^n \\to H$ in the $\\mathcal{L}^2$ norm, i.e., $E[\\int_0^T (H_s - H_s^n)^2 ds] \\to 0$ as $n \\to \\infty$.\nLet $M_t = \\int_0^t H_s dB_s$ and $M_t^n = \\int_0^t H_s^n dB_s$. By the Itô isometry,\n$$ E[(M_T^n - M_T)^2] = E\\left[\\int_0^T (H_s^n - H_s)^2 ds\\right] \\to 0. $$\nBy Doob's maximal inequality, this implies that $\\sup_{t \\in [0,T]}|M_t^n - M_t| \\to 0$ in probability.\nThe convergence of a sequence of semimartingales to a semimartingale in this manner implies the convergence of their quadratic variations in probability. Thus, $[M^n]_t \\to [M]_t$ in probability for each $t \\in [0, T]$.\nFrom Step 1, we have $[M^n]_t = \\int_0^t (H_s^n)^2 ds$. We must show that this converges to $\\int_0^t H_s^2 ds$.\n$$ E\\left| \\int_0^t (H_s^n)^2 ds - \\int_0^t H_s^2 ds \\right| \\le E\\left[ \\int_0^T |(H_s^n)^2 - H_s^2| ds \\right] = E\\left[ \\int_0^T |H_s^n - H_s||H_s^n + H_s| ds \\right] $$\nBy the Cauchy-Schwarz inequality, this is bounded by:\n$$ \\left(E\\left[\\int_0^T (H_s^n - H_s)^2 ds\\right]\\right)^{1/2} \\left(E\\left[\\int_0^T (H_s^n + H_s)^2 ds\\right]\\right)^{1/2} $$\nThe first term converges to $0$ by construction. The second term is bounded because $H^n$ converges to $H$ in $\\mathcal{L}^2$. Thus, $\\int_0^t (H_s^n)^2 ds \\to \\int_0^t H_s^2 ds$ in $L^1$, and hence in probability.\nSince we have $[M^n]_t \\to [M]_t$ and $[M^n]_t \\to \\int_0^t H_s^2 ds$, both in probability, and limits in probability are unique, we conclude that for any $t \\in [0, T]$,\n$$ [M]_t = \\int_0^t H_s^2 \\, ds \\quad \\text{a.s.} $$\n\n**Step 3: Localization for Locally Square-Integrable Integrands**\n\nFinally, we consider the general case where $H$ is a predictable process such that for every $t \\ge 0$, $\\int_0^t H_s^2 ds < \\infty$ almost surely. For this class of integrands, $M_t = \\int_0^t H_s dB_s$ is a continuous local martingale.\nBy definition, there exists a localizing sequence of stopping times $\\{\\tau_k\\}_{k\\ge 1}$ such that $\\tau_k \\uparrow \\infty$ a.s. and each stopped process $M^{\\tau_k}_t = M_{t \\wedge \\tau_k}$ is a true martingale (in fact, a uniformly integrable martingale).\nWe can choose the sequence $\\tau_k = \\inf\\{t \\ge 0: \\int_0^t H_s^2 ds \\ge k\\}$. Since for any fixed $t$, $\\int_0^t H_s^2 ds$ is finite a.s., $\\tau_k$ must tend to infinity a.s.\nFor each $k$, define a stopped integrand $H_s^k = H_s \\mathbf{1}_{\\{s \\le \\tau_k\\}}$. The process $H^k$ is predictable. For any $T>0$, $E[\\int_0^T (H_s^k)^2 ds] = E[\\int_0^{T \\wedge \\tau_k} H_s^2 ds] \\le k < \\infty$. Thus, $H^k$ is a square-integrable integrand.\nThe corresponding stochastic integral is $M_t^k = \\int_0^t H_s^k dB_s = \\int_0^{t \\wedge \\tau_k} H_s dB_s = M_{t \\wedge \\tau_k}$.\nFrom Step 2, we can compute the quadratic variation of $M^k$:\n$$ [M^k]_t = \\int_0^t (H_s^k)^2 ds = \\int_0^t H_s^2 \\mathbf{1}_{\\{s \\le \\tau_k\\}} ds = \\int_0^{t \\wedge \\tau_k} H_s^2 ds $$\nOn the other hand, a general property of quadratic variation is that for any stopping time $\\tau$, $[X_{\\cdot \\wedge \\tau}]_t = [X]_{t \\wedge \\tau}$. Applying this with $\\tau_k$:\n$$ [M^k]_t = [M_{\\cdot \\wedge \\tau_k}]_t = [M]_{t \\wedge \\tau_k} $$\nCombining these two expressions for $[M^k]_t$, we have for each $k \\ge 1$:\n$$ [M]_{t \\wedge \\tau_k} = \\int_0^{t \\wedge \\tau_k} H_s^2 ds \\quad \\text{a.s. for all } t \\ge 0. $$\nLet $A_t = [M]_t$ and $C_t = \\int_0^t H_s^2 ds$. Both are continuous non-decreasing processes starting at $A_0=C_0=0$. The equality is $A_{t \\wedge \\tau_k} = C_{t \\wedge \\tau_k}$.\nSince $\\tau_k \\to \\infty$ a.s., for any given $t \\ge 0$ and for almost every $\\omega \\in \\Omega$, we can find a $K(\\omega)$ such that for all $k \\ge K(\\omega)$, we have $\\tau_k(\\omega) > t$. For such $k$, $t \\wedge \\tau_k(\\omega) = t$, and thus $A_t(\\omega) = C_t(\\omega)$.\nThis implies that $A_t = C_t$ almost surely for each $t \\ge 0$. By continuity of both processes, we can state that the paths are indistinguishable. The desired general result is:\n$$ [M]_t = \\int_0^t H_s^2 \\, ds $$\n\n**Part 2: Specialization to $H_s = \\frac{1}{1+s^2}$**\n\nWe are given the specific integrand $H_s = \\frac{1}{1+s^2}$ for $s \\ge 0$. This is a deterministic function, hence it is predictable. We check the condition for the integral to be well-defined as a local martingale:\n$$ \\int_0^t H_s^2 ds = \\int_0^t \\left(\\frac{1}{1+s^2}\\right)^2 ds = \\int_0^t \\frac{1}{(1+s^2)^2} ds $$\nThe integrand $\\frac{1}{(1+s^2)^2}$ is a continuous and positive function on $[0, \\infty)$, so for any finite $t \\ge 0$, the integral is finite. Thus, $H$ is a valid integrand.\nUsing the general formula derived above, the quadratic variation of $M_t = \\int_0^t \\frac{1}{1+s^2} dB_s$ is given by:\n$$ [M]_t = \\int_0^t \\frac{1}{(1+s^2)^2} ds $$\nTo evaluate this integral, we use the substitution $s = \\tan\\theta$, which implies $ds = \\sec^2\\theta \\, d\\theta$. The limits of integration become $0$ and $\\arctan(t)$.\n$$ [M]_t = \\int_0^{\\arctan(t)} \\frac{1}{(1+\\tan^2\\theta)^2} \\sec^2\\theta \\, d\\theta = \\int_0^{\\arctan(t)} \\frac{1}{(\\sec^2\\theta)^2} \\sec^2\\theta \\, d\\theta = \\int_0^{\\arctan(t)} \\cos^2\\theta \\, d\\theta $$\nUsing the half-angle identity $\\cos^2\\theta = \\frac{1+\\cos(2\\theta)}{2}$:\n$$ [M]_t = \\int_0^{\\arctan(t)} \\frac{1+\\cos(2\\theta)}{2} d\\theta = \\frac{1}{2} \\left[ \\theta + \\frac{\\sin(2\\theta)}{2} \\right]_0^{\\arctan(t)} $$\n$$ [M]_t = \\frac{1}{2} \\left( \\arctan(t) + \\frac{\\sin(2\\arctan(t))}{2} \\right) - 0 $$\nTo simplify $\\sin(2\\arctan(t))$, let $\\theta = \\arctan(t)$, so $\\tan\\theta = t$. We use the identity $\\sin(2\\theta) = \\frac{2\\tan\\theta}{1+\\tan^2\\theta}$:\n$$ \\sin(2\\arctan(t)) = \\frac{2t}{1+t^2} $$\nSubstituting this back into the expression for $[M]_t$:\n$$ [M]_t = \\frac{1}{2} \\left( \\arctan(t) + \\frac{1}{2} \\frac{2t}{1+t^2} \\right) = \\frac{1}{2} \\left( \\arctan(t) + \\frac{t}{1+t^2} \\right) $$\nThis is the required explicit closed-form expression for the quadratic variation.", "answer": "$$\\boxed{\\frac{1}{2} \\left( \\arctan(t) + \\frac{t}{1+t^{2}} \\right)}$$", "id": "3064208"}]}