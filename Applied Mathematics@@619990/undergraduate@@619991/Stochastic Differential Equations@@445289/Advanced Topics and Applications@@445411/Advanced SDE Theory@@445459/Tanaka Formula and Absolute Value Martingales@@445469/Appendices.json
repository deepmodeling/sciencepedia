{"hands_on_practices": [{"introduction": "Tanaka's formula is a cornerstone of stochastic calculus, extending Itô's lemma to functions that are not twice differentiable, such as the absolute value function $f(x)=|x|$. A key technique to rigorously derive this formula is to approximate the non-smooth function with a family of smooth functions. This first exercise [@problem_id:3079502] guides you through the essential calculus of constructing and analyzing such a smooth approximation, laying the analytical groundwork for understanding the origin of the local time term.", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space satisfying the usual conditions, and consider the absolute value function $x \\mapsto |x|$. In deriving Tanaka's formula for $|B_{t}|$ via approximation by twice continuously differentiable functions, a canonical choice is to smooth $|x|$ by the family $f_{\\epsilon}(x) = \\sqrt{x^{2} + \\epsilon}$ for $\\epsilon > 0$. Working from first principles of classical calculus and convex analysis, and keeping in mind the use of such approximations in Itô-type arguments for stochastic differential equations (SDE), perform the following:\n\n1. Justify that for each fixed $\\epsilon > 0$, the function $f_{\\epsilon}$ is twice continuously differentiable on $\\mathbb{R}$ and strictly convex.\n2. Compute the first and second derivatives $f_{\\epsilon}'(x)$ and $f_{\\epsilon}''(x)$ explicitly as functions of $x$ and $\\epsilon$.\n3. Prove the uniform approximation bound $\\sup_{x \\in \\mathbb{R}} \\big| f_{\\epsilon}(x) - |x| \\big| \\leq \\sqrt{\\epsilon}$.\n4. Show that $f_{\\epsilon}''(x) \\geq 0$ for all $x \\in \\mathbb{R}$ and that $\\int_{-\\infty}^{\\infty} f_{\\epsilon}''(x)\\,dx = 2$. Briefly explain how this identity relates to the heuristic that $f_{\\epsilon}''$ approximates $2 \\delta_{0}$ in the sense of distributions as $\\epsilon \\downarrow 0$.\n\nYour final submitted answer should consist only of the explicit expressions for $f_{\\epsilon}'(x)$ and $f_{\\epsilon}''(x)$, arranged in a single row matrix. No rounding is required; provide exact expressions.", "solution": "The problem statement is a valid exercise in real and convex analysis, situated within the context of stochastic calculus, specifically the approximation methods used to derive Tanaka's formula for non-smooth functions. The problem is self-contained, scientifically grounded, and well-posed. We shall proceed with the solution by addressing each of the four parts in order.\n\nLet the family of functions be defined as $f_{\\epsilon}(x) = \\sqrt{x^{2} + \\epsilon}$ for a fixed parameter $\\epsilon > 0$ and $x \\in \\mathbb{R}$.\n\n1.  **Justification of Differentiability and Convexity**\n    For a fixed $\\epsilon > 0$, the argument of the square root function, $x^2 + \\epsilon$, is always strictly positive. Specifically, $x^2 + \\epsilon \\geq \\epsilon > 0$ for all $x \\in \\mathbb{R}$. The function $g(y) = \\sqrt{y}$ is infinitely differentiable ($C^{\\infty}$) for $y > 0$. The function $h(x) = x^2 + \\epsilon$ is a polynomial and is therefore also infinitely differentiable on $\\mathbb{R}$. Since $f_{\\epsilon}(x) = g(h(x))$ is a composition of infinitely differentiable functions, $f_{\\epsilon}(x)$ is itself infinitely differentiable ($C^{\\infty}$) on $\\mathbb{R}$. A function that is $C^{\\infty}$ is necessarily twice continuously differentiable ($C^2$).\n\n    To establish strict convexity, we must show that the second derivative, $f_{\\epsilon}''(x)$, is strictly positive for all $x \\in \\mathbb{R}$. We will compute this derivative in the next step and then refer back to its properties. As will be shown, $f_{\\epsilon}''(x) = \\epsilon (x^2 + \\epsilon)^{-3/2}$. Since $\\epsilon > 0$ and $x^2 + \\epsilon$ is always positive, the denominator $(x^2 + \\epsilon)^{3/2}$ is also a positive real number. Consequently, $f_{\\epsilon}''(x)$ is strictly positive for all $x \\in \\mathbb{R}$. A function with a strictly positive second derivative on its entire domain is strictly convex.\n\n2.  **Computation of Derivatives**\n    We compute the first and second derivatives of $f_{\\epsilon}(x) = (x^2 + \\epsilon)^{1/2}$ using the chain rule.\n    The first derivative is:\n    $$f_{\\epsilon}'(x) = \\frac{d}{dx} (x^2 + \\epsilon)^{1/2} = \\frac{1}{2}(x^2 + \\epsilon)^{-1/2} \\cdot \\frac{d}{dx}(x^2 + \\epsilon) = \\frac{1}{2}(x^2 + \\epsilon)^{-1/2} \\cdot (2x)$$\n    $$f_{\\epsilon}'(x) = x(x^2 + \\epsilon)^{-1/2} = \\frac{x}{\\sqrt{x^2 + \\epsilon}}$$\n    To find the second derivative, we apply the quotient rule to $f_{\\epsilon}'(x)$:\n    $$f_{\\epsilon}''(x) = \\frac{d}{dx} \\left( \\frac{x}{\\sqrt{x^2 + \\epsilon}} \\right) = \\frac{(1) \\cdot \\sqrt{x^2 + \\epsilon} - x \\cdot \\frac{d}{dx}(\\sqrt{x^2 + \\epsilon})}{(\\sqrt{x^2 + \\epsilon})^2}$$\n    From the calculation of the first derivative, we know $\\frac{d}{dx}(\\sqrt{x^2 + \\epsilon}) = \\frac{x}{\\sqrt{x^2 + \\epsilon}}$. Substituting this in:\n    $$f_{\\epsilon}''(x) = \\frac{\\sqrt{x^2 + \\epsilon} - x \\left( \\frac{x}{\\sqrt{x^2 + \\epsilon}} \\right)}{x^2 + \\epsilon} = \\frac{\\sqrt{x^2 + \\epsilon} - \\frac{x^2}{\\sqrt{x^2 + \\epsilon}}}{x^2 + \\epsilon}$$\n    To simplify this expression, we multiply the numerator and the denominator by $\\sqrt{x^2 + \\epsilon}$:\n    $$f_{\\epsilon}''(x) = \\frac{(\\sqrt{x^2 + \\epsilon})^2 - x^2}{(x^2 + \\epsilon)\\sqrt{x^2 + \\epsilon}} = \\frac{(x^2 + \\epsilon) - x^2}{(x^2 + \\epsilon)^{3/2}} = \\frac{\\epsilon}{(x^2 + \\epsilon)^{3/2}}$$\n    These are the explicit expressions for the first and second derivatives.\n\n3.  **Proof of Uniform Approximation Bound**\n    We need to prove that $\\sup_{x \\in \\mathbb{R}} \\big| f_{\\epsilon}(x) - |x| \\big| \\leq \\sqrt{\\epsilon}$.\n    Let $g(x) = f_{\\epsilon}(x) - |x| = \\sqrt{x^2 + \\epsilon} - \\sqrt{x^2}$.\n    By multiplying and dividing by the conjugate, we get:\n    $$g(x) = \\frac{(\\sqrt{x^2 + \\epsilon} - \\sqrt{x^2})(\\sqrt{x^2 + \\epsilon} + \\sqrt{x^2})}{\\sqrt{x^2 + \\epsilon} + \\sqrt{x^2}} = \\frac{(x^2 + \\epsilon) - x^2}{\\sqrt{x^2 + \\epsilon} + |x|} = \\frac{\\epsilon}{\\sqrt{x^2 + \\epsilon} + |x|}$$\n    Since $\\epsilon > 0$ and the denominator is always positive, we have $g(x) > 0$ for all $x \\in \\mathbb{R}$. This means $\\big| f_{\\epsilon}(x) - |x| \\big| = g(x)$.\n    We wish to find the supremum of $g(x)$. The denominator $\\sqrt{x^2 + \\epsilon} + |x|$ is a non-negative, increasing function of $|x|$. Therefore, $g(x)$ is maximized when its denominator is minimized. The minimum value of the denominator occurs at $x = 0$, where it is $\\sqrt{0^2 + \\epsilon} + |0| = \\sqrt{\\epsilon}$.\n    At $x=0$, the value of the function is $g(0) = \\frac{\\epsilon}{\\sqrt{\\epsilon}} = \\sqrt{\\epsilon}$.\n    For any $x \\neq 0$, the denominator $\\sqrt{x^2 + \\epsilon} + |x|$ is strictly greater than $\\sqrt{\\epsilon}$, so $g(x)  \\sqrt{\\epsilon}$.\n    Thus, the supremum of $g(x)$ over all $x \\in \\mathbb{R}$ is achieved at $x=0$:\n    $$\\sup_{x \\in \\mathbb{R}} \\big| f_{\\epsilon}(x) - |x| \\big| = \\sup_{x \\in \\mathbb{R}} g(x) = g(0) = \\sqrt{\\epsilon}$$\n    The statement is that the supremum is less than or equal to $\\sqrt{\\epsilon}$, which is proven by our finding that it is exactly equal to $\\sqrt{\\epsilon}$.\n\n4.  **Analysis of the Second Derivative**\n    First, we show $f_{\\epsilon}''(x) \\geq 0$. From Part 2, $f_{\\epsilon}''(x) = \\frac{\\epsilon}{(x^2 + \\epsilon)^{3/2}}$. Given $\\epsilon > 0$, the numerator is positive. The term $x^2 \\geq 0$, so $x^2+\\epsilon > 0$. Any positive real number raised to the power of $3/2$ is positive. Thus, the denominator is always positive, and the entire expression $f_{\\epsilon}''(x)$ is strictly positive for all $x \\in \\mathbb{R}$. This implies $f_{\\epsilon}''(x) \\geq 0$.\n\n    Second, we compute the improper integral $\\int_{-\\infty}^{\\infty} f_{\\epsilon}''(x)\\,dx$. By the Fundamental Theorem of Calculus, this integral is equal to the change in the antiderivative, $f_{\\epsilon}'(x)$, over the interval $(-\\infty, \\infty)$:\n    $$\\int_{-\\infty}^{\\infty} f_{\\epsilon}''(x)\\,dx = \\lim_{b \\to \\infty} f_{\\epsilon}'(b) - \\lim_{a \\to -\\infty} f_{\\epsilon}'(a)$$\n    Using the expression $f_{\\epsilon}'(x) = \\frac{x}{\\sqrt{x^2 + \\epsilon}}$:\n    $$\\lim_{x \\to \\infty} f_{\\epsilon}'(x) = \\lim_{x \\to \\infty} \\frac{x}{\\sqrt{x^2 + \\epsilon}} = \\lim_{x \\to \\infty} \\frac{x}{|x|\\sqrt{1 + \\epsilon/x^2}} = \\lim_{x \\to \\infty} \\frac{x}{x\\sqrt{1 + \\epsilon/x^2}} = \\lim_{x \\to \\infty} \\frac{1}{\\sqrt{1 + \\epsilon/x^2}} = 1$$\n    $$\\lim_{x \\to -\\infty} f_{\\epsilon}'(x) = \\lim_{x \\to -\\infty} \\frac{x}{\\sqrt{x^2 + \\epsilon}} = \\lim_{x \\to -\\infty} \\frac{x}{|x|\\sqrt{1 + \\epsilon/x^2}} = \\lim_{x \\to -\\infty} \\frac{x}{(-x)\\sqrt{1 + \\epsilon/x^2}} = \\lim_{x \\to -\\infty} \\frac{-1}{\\sqrt{1 + \\epsilon/x^2}} = -1$$\n    Therefore, the integral is:\n    $$\\int_{-\\infty}^{\\infty} f_{\\epsilon}''(x)\\,dx = (1) - (-1) = 2$$\n\n    **Relation to the Dirac Delta Distribution:** The family of functions $\\{f_{\\epsilon}''\\}_{\\epsilon>0}$ is an approximation to the distribution $2\\delta_0$. A sequence of functions $g_{\\epsilon}(x)$ approximates $c\\delta_0$ as $\\epsilon \\downarrow 0$ if (i) $\\int_{-\\infty}^{\\infty} g_{\\epsilon}(x)\\,dx = c$ for all $\\epsilon$, and (ii) for any fixed $x \\neq 0$, $\\lim_{\\epsilon \\downarrow 0} g_{\\epsilon}(x) = 0$.\n    Our family $f_{\\epsilon}''(x)$ satisfies these conditions for $c=2$:\n    (i) We have just shown that $\\int_{-\\infty}^{\\infty} f_{\\epsilon}''(x)\\,dx = 2$.\n    (ii) For any fixed $x \\neq 0$, we have $\\lim_{\\epsilon \\downarrow 0} f_{\\epsilon}''(x) = \\lim_{\\epsilon \\downarrow 0} \\frac{\\epsilon}{(x^2 + \\epsilon)^{3/2}} = \\frac{0}{(x^2)^{3/2}} = 0$.\n    The functions $f_{\\epsilon}''(x)$ are non-negative and their total area is concentrated in an ever-smaller neighborhood of $x=0$ as $\\epsilon \\downarrow 0$. This behavior is characteristic of a sequence converging to a scaled Dirac delta distribution. Heuristically, as $\\epsilon \\downarrow 0$, $f_{\\epsilon}(x) \\to |x|$. The first derivative $f_{\\epsilon}'(x) = \\frac{x}{\\sqrt{x^2+\\epsilon}}$ converges to $\\frac{x}{|x|} = \\text{sgn}(x)$ for $x \\neq 0$. The derivative of the signum function, $\\text{sgn}(x)$, in the sense of distributions is $2\\delta_0(x)$, which represents the jump of size $2$ at the origin. Our calculation shows how this singular behavior is regularized and recovered in the limit.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{x}{\\sqrt{x^2 + \\epsilon}}  \\frac{\\epsilon}{(x^2 + \\epsilon)^{3/2}} \\end{pmatrix}}\n$$", "id": "3079502"}, {"introduction": "Having explored the analytical technique for smoothing a function, it is crucial to understand *why* this leads to a new structure in stochastic calculus. This practice [@problem_id:3079561] offers a direct comparison between a process generated by a smooth function, $B_t^2$, and one generated by a non-smooth function, $|B_t|$. By applying the standard Itô formula to one and Tanaka's formula to the other, you will see precisely how the non-differentiability at the origin gives rise to the local time term, a feature absent in the smooth case.", "problem": "Let $\\{B_t : t \\ge 0\\}$ denote a standard Brownian motion with $B_0 = 0$. Consider the functions $f(x) = |x|$ and $g(x) = x^2$. Use the Itô formula for twice continuously differentiable functions and the fact that $f$ is convex but not twice differentiable at $x = 0$ to analyze their stochastic decompositions when composed with $B_t$. In particular, determine where a local time term at $0$ appears in the decomposition and how this contrasts between $f(B_t)$ and $g(B_t)$. Select all statements that are true.\n\nA. Under the Itô formula applied to $g(x) = x^2$, the decomposition of $g(B_t)$ has no local time term and is given by $B_t^2 = 2\\int_0^t B_s \\, dB_s + t$, so the finite variation part equals $t$.\n\nB. Under Tanaka’s formula applied to $f(x) = |x|$, the decomposition of $f(B_t)$ is $|B_t| = \\int_0^t \\operatorname{sgn}(B_s)\\, dB_s + L_t^0(B)$, where $L_t^0(B)$ is the local time of $B$ at $0$, and the finite variation part equals $L_t^0(B)$.\n\nC. The process $\\{|B_t| : t \\ge 0\\}$ is a true martingale with respect to the natural filtration of $\\{B_t : t \\ge 0\\}$.\n\nD. The quadratic variation of the process $\\{|B_t| : t \\ge 0\\}$ over $[0,t]$ equals $t$.\n\nE. A local time term appears in the decomposition of $g(B_t) = B_t^2$ because $g$ is convex.\n\nChoose all that apply.", "solution": "We work from the following foundational facts:\n- Itô’s formula for twice continuously differentiable functions states that if $X_t$ is a continuous semimartingale and $h \\in C^2(\\mathbb{R})$, then\n$$\nh(X_t) = h(X_0) + \\int_0^t h'(X_s)\\, dX_s + \\frac{1}{2}\\int_0^t h''(X_s)\\, d[X]_s,\n$$\nwhere $[X]_t$ denotes the quadratic variation of $X$.\n- For standard Brownian motion, $[B]_t = t$.\n- For convex functions that are not twice differentiable (such as $f(x) = |x|$), the appropriate extension involves Tanaka’s formula, in which the nondifferentiable point produces a local time term at that point.\n\nWe analyze each option in turn.\n\nOption A: For $g(x) = x^2$, we have $g'(x) = 2x$ and $g''(x) = 2$, and $g \\in C^2(\\mathbb{R})$. Applying Itô’s formula with $X_t = B_t$ and $B_0 = 0$ gives\n$$\nB_t^2 = B_0^2 + \\int_0^t 2B_s\\, dB_s + \\frac{1}{2}\\int_0^t 2\\, d[B]_s = 0 + 2\\int_0^t B_s\\, dB_s + \\int_0^t 1\\, ds = 2\\int_0^t B_s\\, dB_s + t.\n$$\nThere is no local time term because $g$ is twice continuously differentiable everywhere, and Itô’s formula does not produce local time when $h''$ is a classical function. The finite variation part is $\\int_0^t 1 \\, ds = t$. Therefore, Option A is Correct.\n\nOption B: For $f(x) = |x|$, $f$ is convex but not twice differentiable at $x = 0$. To derive its decomposition, consider a smooth approximation $f_\\varepsilon(x) = \\sqrt{x^2 + \\varepsilon}$, which satisfies $f_\\varepsilon'(x) = \\dfrac{x}{\\sqrt{x^2 + \\varepsilon}}$ and $f_\\varepsilon''(x) = \\dfrac{\\varepsilon}{(x^2 + \\varepsilon)^{3/2}}$. Applying Itô’s formula to $f_\\varepsilon(B_t)$ yields\n$$\nf_\\varepsilon(B_t) = f_\\varepsilon(B_0) + \\int_0^t f_\\varepsilon'(B_s)\\, dB_s + \\frac{1}{2}\\int_0^t f_\\varepsilon''(B_s)\\, ds.\n$$\nThe second derivative term concentrates mass near $0$ as $\\varepsilon \\downarrow 0$, and via the occupation time formula and the standard identification of local time $L_t^0(B)$ as the density at $0$ of the occupation measure, one shows\n$$\n\\lim_{\\varepsilon \\downarrow 0} \\frac{1}{2}\\int_0^t f_\\varepsilon''(B_s)\\, ds = L_t^0(B),\n$$\nwhile $f_\\varepsilon'(B_s) \\to \\operatorname{sgn}(B_s)$ almost surely for $B_s \\neq 0$, and $f_\\varepsilon(B_0) \\to |B_0| = 0$ because $B_0=0$. Passing to the limit gives Tanaka’s formula:\n$$\n|B_t| = \\int_0^t \\operatorname{sgn}(B_s)\\, dB_s + L_t^0(B).\n$$\nThe stochastic integral is a continuous martingale (indeed, a local martingale that is a true martingale since it is an Itô integral with square-integrable integrand $\\operatorname{sgn}(B_s)$), while the local time $L_t^0(B)$ is an increasing process of finite variation. Hence the finite variation part is $L_t^0(B)$. Therefore, Option B is Correct.\n\nOption C: The process $|B_t|$ is not a martingale. From Tanaka’s formula,\n$$\n|B_t| = \\int_0^t \\operatorname{sgn}(B_s)\\, dB_s + L_t^0(B),\n$$\nwhere $L_t^0(B)$ is increasing and nonnegative with $L_0^0(B) = 0$. Taking expectations and noting that the stochastic integral has expectation $0$, we obtain\n$$\n\\mathbb{E}[|B_t|] = \\mathbb{E}[L_t^0(B)] > 0 \\quad \\text{for } t>0.\n$$\nAlternatively, one may compute $\\mathbb{E}[|B_t|] = \\sqrt{\\frac{2t}{\\pi}}$, which is strictly increasing in $t$. A martingale starting at $0$ would have constant expectation equal to $0$. Therefore, $|B_t|$ is a submartingale (with Doob–Meyer decomposition given by Tanaka’s formula), not a martingale. Option C is Incorrect.\n\nOption D: The quadratic variation of $|B|$ over $[0,t]$ equals $t$. From Tanaka’s formula, we can write\n$$\n|B_t| = M_t + A_t, \\quad \\text{where } M_t := \\int_0^t \\operatorname{sgn}(B_s)\\, dB_s \\text{ and } A_t := L_t^0(B).\n$$\nQuadratic variation is carried entirely by the local martingale part in continuous semimartingales, since finite variation processes have zero quadratic variation. Hence\n$$\n[|B|]_t = [M]_t = \\int_0^t \\operatorname{sgn}^2(B_s)\\, d[B]_s = \\int_0^t 1\\, ds = t,\n$$\nbecause $[B]_s = s$ and $\\operatorname{sgn}^2(B_s) = 1$ for all $s$ except $B_s=0$, which is a null set for the integral in time. Therefore, Option D is Correct.\n\nOption E: A local time term does not appear in the decomposition of $g(B_t) = B_t^2$ under Itô’s formula. The presence of local time is tied to nondifferentiability (specifically, a “kink”) of the function being applied at a point, which $f(x) = |x|$ has at $x = 0$. In contrast, $g(x) = x^2$ is twice continuously differentiable everywhere, so Itô’s formula produces only the usual drift term involving $g''$ and no local time. Therefore, Option E is Incorrect.\n\nSummary:\n- A: Correct.\n- B: Correct.\n- C: Incorrect.\n- D: Correct.\n- E: Incorrect.", "answer": "$$\\boxed{ABD}$$", "id": "3079561"}, {"introduction": "Tanaka’s formula elegantly decomposes the submartingale $|B_t|$ into a martingale component and an increasing process, the local time. This final exercise [@problem_id:3079519] focuses on the martingale part, $M_t = \\int_0^t \\operatorname{sgn}(B_s) dB_s$. You will apply the powerful Itô isometry to compute its second moment, thereby gaining a deeper appreciation for the properties of stochastic integrals and the connection between a martingale's second moment and its quadratic variation.", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion adapted to its natural filtration. Define the sign function $\\operatorname{sgn}:\\mathbb{R}\\to\\mathbb{R}$ by $\\operatorname{sgn}(x)=1$ for $x0$, $\\operatorname{sgn}(x)=-1$ for $x0$, and $\\operatorname{sgn}(0)=0$. Consider the stochastic integral\n$$\nM_{t}=\\int_{0}^{t}\\operatorname{sgn}(B_{s})\\,dB_{s}.\n$$\nStarting from the definition of the Itô integral and standard properties of Brownian motion, compute the quantity\n$$\n\\mathbb{E}\\left[\\left(M_{t}\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\int_{0}^{t}\\operatorname{sgn}(B_{s})\\,dB_{s}\\right)^{2}\\right]\n$$\nas a closed-form symbolic expression in $t$. Then, use Tanaka’s formula to identify $M_{t}$ as the martingale part of the absolute value process $\\{|B_{t}|\\}_{t\\ge 0}$ and explain why your computed expression coincides with the quadratic variation of $M_{t}$. Express your final answer as a single symbolic expression; no rounding is required.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- $\\{B_{t}\\}_{t \\geq 0}$ is a one-dimensional standard Brownian motion.\n- The filtration is the natural filtration of $\\{B_t\\}$.\n- The sign function is defined as $\\operatorname{sgn}(x)=1$ for $x0$, $\\operatorname{sgn}(x)=-1$ for $x0$, and $\\operatorname{sgn}(0)=0$.\n- The stochastic process $M_t$ is defined by the Itô integral $M_{t}=\\int_{0}^{t}\\operatorname{sgn}(B_{s})\\,dB_{s}$.\n- The primary task is to compute $\\mathbb{E}[(M_{t})^{2}]$.\n- A secondary task is to use Tanaka's formula to identify $M_t$ as the martingale part of $\\{|B_{t}|\\}_{t\\ge 0}$ and explain why $\\mathbb{E}[(M_{t})^{2}]$ coincides with the quadratic variation of $M_t$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical framework of stochastic calculus. All terms such as \"standard Brownian motion,\" \"Itô integral,\" and \"Tanaka's formula\" are standard and rigorously defined. The integrand $\\operatorname{sgn}(B_s)$ is an adapted process. The condition for the Itô integral $M_t$ to be a square-integrable martingale is that $\\mathbb{E}[\\int_0^t (\\operatorname{sgn}(B_s))^2 ds]  \\infty$. As shown below, this condition holds since $(\\operatorname{sgn}(B_s))^2 \\le 1$. The problem is scientifically grounded, objective, self-contained, and well-posed. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\nWe are asked to compute the quantity $\\mathbb{E}[(M_t)^2]$, where $M_t$ is the Itô integral $M_{t}=\\int_{0}^{t}\\operatorname{sgn}(B_{s})\\,dB_{s}$.\n\nWe can compute this expectation directly using the Itô isometry. The Itô isometry states that for a predictable process $H_s$ adapted to the filtration of the Brownian motion $B_s$, if $\\mathbb{E}[\\int_0^t H_s^2 ds]  \\infty$, then the Itô integral $I_t = \\int_0^t H_s dB_s$ is a well-defined square-integrable martingale, and its second moment is given by:\n$$\n\\mathbb{E}\\left[ \\left( \\int_{0}^{t} H_{s} dB_{s} \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t} H_{s}^2 ds \\right]\n$$\nIn our case, the integrand is $H_s = \\operatorname{sgn}(B_s)$. We apply the Itô isometry to $M_t$:\n$$\n\\mathbb{E}\\left[(M_t)^2\\right] = \\mathbb{E}\\left[ \\left( \\int_{0}^{t} \\operatorname{sgn}(B_{s}) dB_{s} \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t} (\\operatorname{sgn}(B_{s}))^2 ds \\right]\n$$\nThe integrand $(\\operatorname{sgn}(B_s))^2$ can be analyzed based on the value of $B_s$:\n- If $B_s \\neq 0$, then $\\operatorname{sgn}(B_s)$ is either $1$ or $-1$, so $(\\operatorname{sgn}(B_s))^2 = 1$.\n- If $B_s = 0$, then $\\operatorname{sgn}(B_s) = 0$, so $(\\operatorname{sgn}(B_s))^2 = 0$.\n\nThus, we can write $(\\operatorname{sgn}(B_s))^2 = 1_{\\{B_s \\neq 0\\}}$, where $1_A$ is the indicator function of a set $A$. The expression becomes:\n$$\n\\mathbb{E}\\left[(M_t)^2\\right] = \\mathbb{E}\\left[ \\int_{0}^{t} 1_{\\{B_s \\neq 0\\}} ds \\right]\n$$\nSince the integrand is non-negative, we can use Tonelli's theorem to interchange the expectation and the integral:\n$$\n\\mathbb{E}\\left[(M_t)^2\\right] = \\int_{0}^{t} \\mathbb{E}\\left[ 1_{\\{B_s \\neq 0\\}} \\right] ds\n$$\nThe expectation inside the integral is $\\mathbb{E}[1_{\\{B_s \\neq 0\\}}] = P(B_s \\neq 0)$. For a standard Brownian motion, $B_0 = 0$. For any $s > 0$, $B_s$ follows a normal distribution with mean $0$ and variance $s$, i.e., $B_s \\sim N(0, s)$. This is a continuous probability distribution, so the probability of $B_s$ taking any specific value is zero. In particular, $P(B_s = 0) = 0$ for any $s > 0$.\nTherefore, for $s>0$, $P(B_s \\neq 0) = 1 - P(B_s = 0) = 1$.\nThe case $s=0$ corresponds to a single point in the domain of integration $[0, t]$, and its contribution to the Lebesgue integral is zero. We can thus write:\n$$\n\\mathbb{E}\\left[(M_t)^2\\right] = \\int_{0}^{t} 1 \\, ds = t\n$$\n\nNext, we use Tanaka's formula to provide further insight. Tanaka's formula is an extension of Itô's lemma for convex functions. For the function $f(x)=|x|$, which is convex but not $C^2$ at $x=0$, applied to the process $B_t$, Tanaka's formula states:\n$$\n|B_t| - |B_0| = \\int_0^t \\operatorname{sgn}(B_s) dB_s + L_t^0\n$$\nwhere $L_t^0$ is the local time of the Brownian motion $B_t$ at level $0$.\nGiven that $B_0=0$ and $M_t = \\int_0^t \\operatorname{sgn}(B_s) dB_s$, we have:\n$$\n|B_t| = M_t + L_t^0\n$$\nThis equation represents the Doob-Meyer decomposition of the submartingale $|B_t|$ into its martingale part, $M_t$, and a non-decreasing, non-negative predictable process, $L_t^0$ (the increasing part). This confirms that $M_t$ is indeed the martingale part of the absolute value process $\\{|B_t|\\}_{t \\ge 0}$.\n\nFinally, we explain why the computed expression $\\mathbb{E}[(M_t)^2]$ coincides with the quadratic variation of $M_t$. The quadratic variation of a continuous local martingale $N_t$ is denoted by $\\langle N, N \\rangle_t$. For an Itô integral of the form $N_t = \\int_0^t H_s dB_s$, the quadratic variation is given by:\n$$\n\\langle N, N \\rangle_t = \\int_0^t H_s^2 ds\n$$\nFor our martingale $M_t = \\int_0^t \\operatorname{sgn}(B_s) dB_s$, the quadratic variation is:\n$$\n\\langle M, M \\rangle_t = \\int_0^t (\\operatorname{sgn}(B_s))^2 ds\n$$\nIt is a fundamental property of one-dimensional Brownian motion that the set of times $\\{s \\in [0, t] : B_s = 0\\}$ has Lebesgue measure zero, almost surely. This means that for a typical sample path of the Brownian motion, $(\\operatorname{sgn}(B_s))^2 = 1$ for almost every $s \\in [0, t]$. Consequently, the integral evaluates to $t$ almost surely:\n$$\n\\langle M, M \\rangle_t = \\int_0^t 1 \\, ds = t \\quad \\text{(a.s.)}\n$$\nSo, the quadratic variation of $M_t$ is the deterministic process $\\langle M, M \\rangle_t = t$.\n\nA key property of continuous square-integrable martingales like $M_t$ (with $M_0=0$) is that the process $M_t^2 - \\langle M, M \\rangle_t$ is itself a martingale. This implies:\n$$\n\\mathbb{E}[M_t^2 - \\langle M, M \\rangle_t] = \\mathbb{E}[M_0^2 - \\langle M, M \\rangle_0]\n$$\nSince $M_0 = 0$ and $\\langle M, M \\rangle_0 = 0$, the right-hand side is $0$. Therefore,\n$$\n\\mathbb{E}[M_t^2] = \\mathbb{E}[\\langle M, M \\rangle_t]\n$$\nSubstituting our result for the quadratic variation, $\\langle M, M \\rangle_t = t$, we get:\n$$\n\\mathbb{E}[M_t^2] = \\mathbb{E}[t]\n$$\nSince $t$ is a deterministic quantity, its expectation is simply itself.\n$$\n\\mathbb{E}[M_t^2] = t\n$$\nThis confirms our initial calculation using the Itô isometry and explains the connection: the expected value of the square of the martingale, $\\mathbb{E}[M_t^2]$, is equal to the expected value of its quadratic variation, $\\mathbb{E}[\\langle M, M \\rangle_t]$. Since the quadratic variation $\\langle M, M \\rangle_t$ is the deterministic function $t$, $\\mathbb{E}[M_t^2]$ is equal to $t$.", "answer": "$$\n\\boxed{t}\n$$", "id": "3079519"}]}