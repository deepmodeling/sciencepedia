{"hands_on_practices": [{"introduction": "We begin our practice by calculating the quadratic variation for one of the most fundamental objects in stochastic calculus: the Itô integral. This exercise [@problem_id:3071372] demonstrates how the accumulated variance of a process driven by Brownian motion is directly related to the squared magnitude of its integrand. Mastering this calculation is the first step toward understanding the \"stochastic energy\" of more complex processes.", "problem": "Let $W = \\{W_{t}\\}_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space satisfying the usual conditions, and let $H : [0,\\infty) \\to \\mathbb{R}$ be a deterministic, continuous function. Define the stochastic process $X = \\{X_{t}\\}_{t \\geq 0}$ by the Itô integral\n$$\nX_{t} = \\int_{0}^{t} H_{s} \\, dW_{s}.\n$$\nUsing the definition of quadratic variation for continuous semimartingales as the limit in probability of sums of squared increments along partitions, together with fundamental results such as the Itô formula for twice continuously differentiable functions, derive an explicit expression for the quadratic variation process $[X]_{t}$ in terms of $H$. Then, compute this expression explicitly when $H_{s} = \\sin s$. Your final answer must be a single analytic expression in the variable $t$.", "solution": "The problem asks for the derivation of the quadratic variation process, $[X]_{t}$, for a stochastic process $X_{t}$ defined by the Itô integral $X_{t} = \\int_{0}^{t} H_{s} \\, dW_{s}$, where $H_{s}$ is a deterministic, continuous function and $W_{t}$ is a standard Brownian motion. We are then asked to compute this for the specific case where $H_{s} = \\sin s$.\n\nThe quadratic variation of a continuous semimartingale $X = \\{X_{t}\\}_{t \\geq 0}$ is defined as the unique, predictable, continuous, increasing process, denoted $[X]$, starting at $[X]_{0}=0$, such that $X_{t}^{2} - [X]_{t}$ is a continuous local martingale. This definition arises from the Doob-Meyer decomposition of the submartingale $X_{t}^{2}$. Fundamentally, this process is the limit in probability of the sum of squared increments over a sequence of partitions of the interval $[0, t]$ whose mesh tends to zero. That is, for a partition $\\Pi = \\{0=t_{0}  t_{1}  \\dots  t_{n}=t\\}$,\n$$\n[X]_{t} = \\lim_{|\\Pi| \\to 0} \\sum_{i=0}^{n-1} (X_{t_{i+1}} - X_{t_{i}})^{2}\n$$\nwhere the limit is taken in probability.\n\nThe problem specifies that $H: [0, \\infty) \\to \\mathbb{R}$ is a deterministic and continuous function. This ensures that the Itô integral $X_{t} = \\int_{0}^{t} H_{s} \\, dW_{s}$ is well-defined. Specifically, for any $t \\geq 0$, the condition $\\int_{0}^{t} H_{s}^{2} \\, ds  \\infty$ is satisfied, and the process $X_{t}$ is a continuous martingale with $X_{0} = 0$.\n\nTo find the quadratic variation $[X]_{t}$, we shall use Itô's formula, which is a cornerstone result in stochastic calculus. Let $f(x) = x^{2}$, which is a twice continuously differentiable function with derivatives $f'(x) = 2x$ and $f''(x) = 2$. We apply Itô's formula to the process $Y_{t} = f(X_{t}) = X_{t}^{2}$.\n\nThe process $X_{t}$ has the stochastic differential form $dX_{t} = H_{t} \\, dW_{t}$. This is an Itô process with zero drift and diffusion coefficient $H_{t}$.\n\nItô's formula for a function $f(X_{t})$ is given by:\n$$\ndf(X_{t}) = f'(X_{t}) \\, dX_{t} + \\frac{1}{2} f''(X_{t}) \\, d[X, X]_{t}\n$$\nwhere $d[X, X]_{t}$ is the quadratic variation differential, often written informally as $(dX_{t})^{2}$. For an Itô process of the form $dX_{t} = \\mu_{t} \\, dt + \\sigma_{t} \\, dW_{t}$, the quadratic variation differential is $d[X]_{t} = \\sigma_{t}^{2} \\, dt$.\nIn our case, $\\mu_{t} = 0$ and $\\sigma_{t} = H_{t}$. Therefore, the quadratic variation differential is $d[X]_{t} = H_{t}^{2} \\, dt$.\n\nLet us demonstrate this result using the Itô formula for $X_{t}^{2}$ directly. Applying the formula with $f(x)=x^2$:\n$$\nd(X_{t}^{2}) = f'(X_{t}) \\, dX_{t} + \\frac{1}{2} f''(X_{t}) (dX_{t})^{2}\n$$\nSubstituting $f'(X_t) = 2X_t$ and $f''(X_t) = 2$, we get:\n$$\nd(X_{t}^{2}) = 2X_{t} \\, dX_{t} + \\frac{1}{2} (2) (dX_{t})^{2} = 2X_{t} \\, dX_{t} + (dX_{t})^{2}\n$$\nNow we substitute $dX_{t} = H_{t} \\, dW_{t}$ and use the formal multiplication rule of Itô calculus, $(dW_{t})^{2} = dt$:\n$$\n(dX_{t})^{2} = (H_{t} \\, dW_{t})^{2} = H_{t}^{2} (dW_{t})^{2} = H_{t}^{2} \\, dt\n$$\nSubstituting this back into the expression for $d(X_{t}^{2})$:\n$$\nd(X_{t}^{2}) = 2X_{t} (H_{t} \\, dW_{t}) + H_{t}^{2} \\, dt\n$$\nThis is the stochastic differential for $X_{t}^{2}$. To find the process $X_{t}^{2}$ itself, we integrate from $0$ to $t$:\n$$\n\\int_{0}^{t} d(X_{s}^{2}) = \\int_{0}^{t} 2X_{s} H_{s} \\, dW_{s} + \\int_{0}^{t} H_{s}^{2} \\, ds\n$$\n$$\nX_{t}^{2} - X_{0}^{2} = 2 \\int_{0}^{t} X_{s} H_{s} \\, dW_{s} + \\int_{0}^{t} H_{s}^{2} \\, ds\n$$\nSince $X_{0} = \\int_{0}^{0} H_{s} \\, dW_{s} = 0$, we have $X_{0}^{2} = 0$. The equation becomes:\n$$\nX_{t}^{2} = 2 \\int_{0}^{t} X_{s} H_{s} \\, dW_{s} + \\int_{0}^{t} H_{s}^{2} \\, ds\n$$\nWe can rearrange this equation to isolate the martingale component:\n$$\nX_{t}^{2} - \\int_{0}^{t} H_{s}^{2} \\, ds = 2 \\int_{0}^{t} X_{s} H_{s} \\, dW_{s}\n$$\nThe right-hand side is an Itô integral with respect to Brownian motion. The integrand $2X_{s}H_{s}$ is an adapted process, and under suitable conditions (which hold here since $X_{s}$ is a continuous martingale and $H_{s}$ is continuous and deterministic), the integral defines a continuous local martingale.\nLet $M_{t} = 2 \\int_{0}^{t} X_{s} H_{s} \\, dW_{s}$. Let $A_{t} = \\int_{0}^{t} H_{s}^{2} \\, ds$.\nThe equation is $X_{t}^{2} - A_{t} = M_{t}$.\nSince $H_{s}$ is deterministic and continuous, $A_{t}$ is a continuous, deterministic, and hence predictable process. Also, since $H_{s}^{2} \\geq 0$, $A_{t}$ is non-decreasing (increasing if $H$ is not identically zero). Thus, $A_{t}$ is a predictable, continuous, increasing process of finite variation with $A_{0}=0$.\nWe have expressed the submartingale $X_{t}^{2}$ in its unique Doob-Meyer decomposition $X_{t}^{2} = M_{t} + A_{t}$, where $M_{t}$ is a continuous local martingale and $A_{t}$ is the predictable, continuous, increasing process. By definition, this process $A_{t}$ is the quadratic variation of $X_{t}$.\nTherefore, we conclude that the explicit expression for the quadratic variation process $[X]_{t}$ is:\n$$\n[X]_{t} = \\int_{0}^{t} H_{s}^{2} \\, ds\n$$\nThis is a fundamental result for Itô integrals with respect to Brownian motion.\n\nNow, we must compute this expression for the specific case where $H_{s} = \\sin s$.\nSubstituting $H_{s} = \\sin s$ into our derived formula for $[X]_{t}$:\n$$\n[X]_{t} = \\int_{0}^{t} (\\sin s)^{2} \\, ds = \\int_{0}^{t} \\sin^{2} s \\, ds\n$$\nTo evaluate this integral, we use the power-reduction trigonometric identity $\\sin^{2} \\theta = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$\n[X]_{t} = \\int_{0}^{t} \\frac{1 - \\cos(2s)}{2} \\, ds\n$$\nWe can split the integral:\n$$\n[X]_{t} = \\frac{1}{2} \\int_{0}^{t} (1 - \\cos(2s)) \\, ds = \\frac{1}{2} \\left( \\int_{0}^{t} 1 \\, ds - \\int_{0}^{t} \\cos(2s) \\, ds \\right)\n$$\nEvaluating the integrals:\n$$\n\\int_{0}^{t} 1 \\, ds = [s]_{0}^{t} = t - 0 = t\n$$\n$$\n\\int_{0}^{t} \\cos(2s) \\, ds = \\left[ \\frac{\\sin(2s)}{2} \\right]_{0}^{t} = \\frac{\\sin(2t)}{2} - \\frac{\\sin(0)}{2} = \\frac{\\sin(2t)}{2}\n$$\nSubstituting these results back:\n$$\n[X]_{t} = \\frac{1}{2} \\left( t - \\frac{\\sin(2t)}{2} \\right)\n$$\nThis simplifies to the final analytical expression for $[X]_{t}$ in the variable $t$:\n$$\n[X]_{t} = \\frac{t}{2} - \\frac{\\sin(2t)}{4}\n$$", "answer": "$$\n\\boxed{\\frac{t}{2} - \\frac{\\sin(2t)}{4}}\n$$", "id": "3071372"}, {"introduction": "Having established how to compute quadratic variation for a single process, we now extend the concept to understand the relationship between two processes. This practice [@problem_id:3071356] explores the quadratic covariation of two correlated Brownian motions, revealing how this measure precisely captures their instantaneous correlation structure. This is a vital skill for working with multi-dimensional stochastic models, such as those used in modern finance to model related assets.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions. Let $W^1=(W^1_t)_{t\\ge 0}$ and $\\widetilde{W}=(\\widetilde{W}_t)_{t\\ge 0}$ be independent standard Brownian motions (also called Wiener processes), both adapted to $(\\mathcal{F}_t)_{t\\ge 0}$. Fix a parameter $\\rho\\in(-1,1)$ and define the continuous semimartingale $W^2=(W^2_t)_{t\\ge 0}$ by\n$$\nW^2_t \\;=\\; \\rho\\, W^1_t \\;+\\; \\sqrt{1-\\rho^2}\\,\\widetilde{W}_t,\\quad t\\ge 0.\n$$\nUsing the definition of quadratic covariation for continuous semimartingales and only foundational properties of Brownian motion and independent increments, compute the quadratic covariation process $[W^1,W^2]_t$ for $t\\ge 0$. Your final answer must be a single closed-form expression in $\\rho$ and $t$. No rounding is required.", "solution": "The problem is to compute the quadratic covariation process $[W^1, W^2]_t$, where $W^1$ and $\\widetilde{W}$ are independent standard Brownian motions, and $W^2$ is defined as a linear combination of them: $W^2_t = \\rho W^1_t + \\sqrt{1-\\rho^2} \\widetilde{W}_t$. The parameter $\\rho$ is in the interval $(-1, 1)$. We are asked to use the definition of quadratic covariation for continuous semimartingales and foundational properties of Brownian motion.\n\nLet $X$ and $Y$ be two continuous semimartingales. Their quadratic covariation process, $[X, Y]_t$, is defined as the limit in probability of the sum of products of their increments over a partition of the interval $[0, t]$. Let $\\Pi_n = \\{0 = t_0  t_1  \\dots  t_{k_n} = t\\}$ be a sequence of partitions of $[0, t]$ such that the mesh of the partition, $||\\Pi_n|| = \\max_{i} (t_i - t_{i-1})$, converges to $0$ as $n \\to \\infty$. The quadratic covariation is then given by:\n$$\n[X, Y]_t = \\lim_{||\\Pi_n||\\to 0} \\sum_{i=1}^{k_n} (X_{t_i} - X_{t_{i-1}})(Y_{t_i} - Y_{t_{i-1}})\n$$\nwhere the limit is taken in probability. For simplicity, we denote the increments as $\\Delta X_i = X_{t_i} - X_{t_{i-1}}$ and $\\Delta Y_i = Y_{t_i} - Y_{t_{i-1}}$.\n\nIn our case, $X_t = W^1_t$ and $Y_t = W^2_t$. The increments of $W^2$ are:\n$$\n\\Delta W^2_i = W^2_{t_i} - W^2_{t_{i-1}} = (\\rho W^1_{t_i} + \\sqrt{1-\\rho^2} \\widetilde{W}_{t_i}) - (\\rho W^1_{t_{i-1}} + \\sqrt{1-\\rho^2} \\widetilde{W}_{t_{i-1}})\n$$\n$$\n\\Delta W^2_i = \\rho (W^1_{t_i} - W^1_{t_{i-1}}) + \\sqrt{1-\\rho^2} (\\widetilde{W}_{t_i} - \\widetilde{W}_{t_{i-1}}) = \\rho \\Delta W^1_i + \\sqrt{1-\\rho^2} \\Delta \\widetilde{W}_i\n$$\nThe sum for the quadratic covariation is:\n$$\nS_n = \\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta W^2_i = \\sum_{i=1}^{k_n} \\Delta W^1_i \\left( \\rho \\Delta W^1_i + \\sqrt{1-\\rho^2} \\Delta \\widetilde{W}_i \\right)\n$$\nUsing the linearity of summation, we can split this into two parts:\n$$\nS_n = \\rho \\sum_{i=1}^{k_n} (\\Delta W^1_i)^2 + \\sqrt{1-\\rho^2} \\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i\n$$\nTaking the limit as $||\\Pi_n|| \\to 0$, we get:\n$$\n[W^1, W^2]_t = \\rho \\lim_{||\\Pi_n||\\to 0} \\sum_{i=1}^{k_n} (\\Delta W^1_i)^2 + \\sqrt{1-\\rho^2} \\lim_{||\\Pi_n||\\to 0} \\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i\n$$\nBy definition, the first limit is the quadratic variation of $W^1$, denoted $[W^1, W^1]_t$. The second limit is the quadratic covariation of $W^1$ and $\\widetilde{W}$, denoted $[W^1, \\widetilde{W}]_t$.\n$$\n[W^1, W^2]_t = \\rho [W^1, W^1]_t + \\sqrt{1-\\rho^2} [W^1, \\widetilde{W}]_t\n$$\nWe now need to compute $[W^1, W^1]_t$ and $[W^1, \\widetilde{W}]_t$ using foundational principles. We will show that the defining sums converge in $L^2$, which implies convergence in probability.\n\nFirst, let's compute $[W^1, W^1]_t$. Let $S^{(1)}_n = \\sum_{i=1}^{k_n} (\\Delta W^1_i)^2$.\nThe increments $\\Delta W^1_i = W^1_{t_i} - W^1_{t_{i-1}}$ are independent random variables, as they are over non-overlapping time intervals. Each $\\Delta W^1_i$ follows a normal distribution $N(0, \\Delta t_i)$, where $\\Delta t_i = t_i - t_{i-1}$.\nThe expectation of $S^{(1)}_n$ is:\n$$\n\\mathbb{E}[S^{(1)}_n] = \\mathbb{E}\\left[\\sum_{i=1}^{k_n} (\\Delta W^1_i)^2\\right] = \\sum_{i=1}^{k_n} \\mathbb{E}[(\\Delta W^1_i)^2] = \\sum_{i=1}^{k_n} \\mathrm{Var}(\\Delta W^1_i) = \\sum_{i=1}^{k_n} \\Delta t_i = t\n$$\nThe variance of $S^{(1)}_n$ is (using the independence of increments):\n$$\n\\mathrm{Var}(S^{(1)}_n) = \\mathrm{Var}\\left(\\sum_{i=1}^{k_n} (\\Delta W^1_i)^2\\right) = \\sum_{i=1}^{k_n} \\mathrm{Var}((\\Delta W^1_i)^2)\n$$\nFor a random variable $Z \\sim N(0, \\sigma^2)$, $\\mathrm{Var}(Z^2) = \\mathbb{E}[Z^4] - (\\mathbb{E}[Z^2])^2 = 3(\\sigma^2)^2 - (\\sigma^2)^2 = 2\\sigma^4$.\nHere, $\\sigma^2 = \\Delta t_i$, so $\\mathrm{Var}((\\Delta W^1_i)^2) = 2(\\Delta t_i)^2$.\n$$\n\\mathrm{Var}(S^{(1)}_n) = \\sum_{i=1}^{k_n} 2(\\Delta t_i)^2 = 2 \\sum_{i=1}^{k_n} (\\Delta t_i)^2 \\le 2 \\left(\\max_i \\Delta t_i\\right) \\sum_{i=1}^{k_n} \\Delta t_i = 2 ||\\Pi_n|| t\n$$\nAs $||\\Pi_n|| \\to 0$, we have $\\mathrm{Var}(S^{(1)}_n) \\to 0$. Since $\\mathbb{E}[S^{(1)}_n] = t$ and its variance tends to $0$, $S^{(1)}_n$ converges to $t$ in $L^2$. Therefore, $[W^1, W^1]_t = t$.\n\nSecond, let's compute $[W^1, \\widetilde{W}]_t$. Let $S^{(1, \\widetilde{W})}_n = \\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i$.\nThe processes $W^1$ and $\\widetilde{W}$ are independent. This implies that for any set of times, the collection of random variables $\\{W^1_{t_k}\\}$ is independent of the collection $\\{\\widetilde{W}_{s_j}\\}$. Consequently, the increments $\\Delta W^1_i$ and $\\Delta \\widetilde{W}_j$ are independent for all $i, j$.\nThe expectation of $S^{(1, \\widetilde{W})}_n$ is:\n$$\n\\mathbb{E}[S^{(1, \\widetilde{W})}_n] = \\mathbb{E}\\left[\\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i\\right] = \\sum_{i=1}^{k_n} \\mathbb{E}[\\Delta W^1_i \\Delta \\widetilde{W}_i]\n$$\nDue to independence of $\\Delta W^1_i$ and $\\Delta \\widetilde{W}_i$:\n$$\n\\mathbb{E}[\\Delta W^1_i \\Delta \\widetilde{W}_i] = \\mathbb{E}[\\Delta W^1_i] \\mathbb{E}[\\Delta \\widetilde{W}_i] = 0 \\cdot 0 = 0\n$$\nSo, $\\mathbb{E}[S^{(1, \\widetilde{W})}_n] = 0$.\nThe variance of $S^{(1, \\widetilde{W})}_n$ is $\\mathrm{Var}(S^{(1, \\widetilde{W})}_n) = \\mathbb{E}[(S^{(1, \\widetilde{W})}_n)^2] - (\\mathbb{E}[S^{(1, \\widetilde{W})}_n])^2 = \\mathbb{E}[(S^{(1, \\widetilde{W})}_n)^2]$.\n$$\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{k_n}\\sum_{j=1}^{k_n} \\Delta W^1_i \\Delta \\widetilde{W}_i \\Delta W^1_j \\Delta \\widetilde{W}_j\\right] = \\sum_{i,j=1}^{k_n} \\mathbb{E}[\\Delta W^1_i \\Delta \\widetilde{W}_i \\Delta W^1_j \\Delta \\widetilde{W}_j]\n$$\nIf $i \\neq j$, the time intervals are disjoint. The four random variables $\\Delta W^1_i, \\Delta \\widetilde{W}_i, \\Delta W^1_j, \\Delta \\widetilde{W}_j$ are mutually independent. The expectation of their product is the product of their expectations, which is $0$.\nIf $i = j$, the term is $\\mathbb{E}[(\\Delta W^1_i)^2 (\\Delta \\widetilde{W}_i)^2]$. Due to the independence of $W^1$ and $\\widetilde{W}$, $\\Delta W^1_i$ and $\\Delta \\widetilde{W}_i$ are independent.\n$$\n\\mathbb{E}[(\\Delta W^1_i)^2 (\\Delta \\widetilde{W}_i)^2] = \\mathbb{E}[(\\Delta W^1_i)^2] \\mathbb{E}[(\\Delta \\widetilde{W}_i)^2] = (\\Delta t_i)(\\Delta t_i) = (\\Delta t_i)^2\n$$\nSo, the variance becomes:\n$$\n\\mathrm{Var}(S^{(1, \\widetilde{W})}_n) = \\sum_{i=1}^{k_n} (\\Delta t_i)^2 \\le ||\\Pi_n|| t\n$$\nAs $||\\Pi_n|| \\to 0$, we have $\\mathrm{Var}(S^{(1, \\widetilde{W})}_n) \\to 0$. Since $\\mathbb{E}[S^{(1, \\widetilde{W})}_n] = 0$ and its variance tends to $0$, $S^{(1, \\widetilde{W})}_n$ converges to $0$ in $L^2$. Therefore, $[W^1, \\widetilde{W}]_t = 0$.\n\nSubstituting these results back into the expression for $[W^1, W^2]_t$:\n$$\n[W^1, W^2]_t = \\rho [W^1, W^1]_t + \\sqrt{1-\\rho^2} [W^1, \\widetilde{W}]_t = \\rho \\cdot t + \\sqrt{1-\\rho^2} \\cdot 0 = \\rho t\n$$\nThe quadratic covariation process is $[W^1, W^2]_t = \\rho t$ for $t \\ge 0$.", "answer": "$$\\boxed{\\rho t}$$", "id": "3071356"}, {"introduction": "Our final practice addresses a crucial and often subtle principle: the interaction between processes of infinite variation (like Brownian motion) and those of finite variation (like standard integrals). By computing the quadratic covariation between a Brownian motion and its own time integral [@problem_id:3071373], we discover that they are \"stochastically orthogonal,\" meaning their quadratic covariation is zero despite them being statistically correlated. This result is fundamental to Itô calculus, as it explains why drift terms do not contribute to quadratic variation.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions. Define the continuous process $Y=\\{Y_{t}\\}_{t \\geq 0}$ by\n$$\nY_{t}=\\int_{0}^{t} W_{s}\\, ds, \\quad t \\geq 0.\n$$\nRecall that for continuous semimartingales $X$ and $Z$, the quadratic covariation $[X,Z]_{t}$ can be defined as the limit in probability\n$$\n[X,Z]_{t}=\\lim_{|\\pi|\\to 0}\\sum_{i}\\left(X_{t_{i+1}}-X_{t_{i}}\\right)\\left(Z_{t_{i+1}}-Z_{t_{i}}\\right),\n$$\nwhere the limit is taken over any sequence of partitions $\\pi=\\{0=t_{0}t_{1}\\cdotst_{n}=t\\}$ of $[0,t]$ with mesh $|\\pi|=\\max_{i}(t_{i+1}-t_{i})$ tending to $0$. Starting from this definition and the notion of finite variation, compute $[W,Y]_{t}$ for a fixed $t0$. Your derivation should justify why $[W,Y]_{t}$ vanishes by showing that $Y$ has finite variation on compact intervals and that this forces the partition sums to converge to zero. Express your final answer as a single exact analytical expression. No units are required.", "solution": "The problem asks for the computation of the quadratic covariation $[W,Y]_{t}$ for a fixed time $t  0$, where $\\{W_{t}\\}_{t \\geq 0}$ is a standard one-dimensional Brownian motion and $Y_{t} = \\int_{0}^{t} W_{s}\\, ds$. The computation must proceed from the limit definition of the quadratic covariation and must explicitly demonstrate that the process $Y$ has finite variation on compact intervals.\n\nLet $\\pi = \\{0=t_{0}  t_{1}  \\cdots  t_{n}=t\\}$ be a partition of the interval $[0,t]$. The mesh of the partition is $|\\pi|=\\max_{i}(t_{i+1}-t_{i})$. The quadratic covariation $[W,Y]_{t}$ is defined as the limit in probability:\n$$\n[W,Y]_{t} = \\lim_{|\\pi|\\to 0} \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_{i}})(Y_{t_{i+1}} - Y_{t_{i}})\n$$\nLet $S(\\pi)$ denote the sum for a given partition $\\pi$:\n$$\nS(\\pi) = \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_{i}})(Y_{t_{i+1}} - Y_{t_{i}})\n$$\nWe will proceed in two steps as required. First, we show that the process $Y$ has sample paths of finite variation almost surely. Second, we use this property to show that the limit of $S(\\pi)$ is $0$.\n\nStep 1: Show that $Y$ has finite variation on $[0,t]$.\nA process $X$ is said to have finite variation on $[0,t]$ if its sample paths have finite total variation on $[0,t]$ almost surely. The total variation of a sample path $Y(\\omega)$ on the interval $[0,t]$ is given by:\n$$\nV_{t}(Y)(\\omega) = \\sup_{\\pi} \\sum_{i=0}^{n-1} |Y_{t_{i+1}}(\\omega) - Y_{t_{i}}(\\omega)|\n$$\nwhere the supremum is taken over all partitions $\\pi$ of $[0,t]$.\n\nFor a given partition $\\pi$, let us analyze the increment $Y_{t_{i+1}} - Y_{t_{i}}$. From the definition of $Y_{t}$, we have:\n$$\nY_{t_{i+1}} - Y_{t_{i}} = \\int_{0}^{t_{i+1}} W_{s}\\, ds - \\int_{0}^{t_{i}} W_{s}\\, ds = \\int_{t_{i}}^{t_{i+1}} W_{s}\\, ds\n$$\nThe sum in the definition of total variation is then:\n$$\n\\sum_{i=0}^{n-1} |Y_{t_{i+1}} - Y_{t_{i}}| = \\sum_{i=0}^{n-1} \\left| \\int_{t_{i}}^{t_{i+1}} W_{s}\\, ds \\right|\n$$\nBy the triangle inequality for integrals, we have $\\left| \\int_{a}^{b} f(s)\\, ds \\right| \\leq \\int_{a}^{b} |f(s)|\\, ds$. Applying this property, we get:\n$$\n\\sum_{i=0}^{n-1} \\left| \\int_{t_{i}}^{t_{i+1}} W_{s}\\, ds \\right| \\leq \\sum_{i=0}^{n-1} \\int_{t_{i}}^{t_{i+1}} |W_{s}|\\, ds = \\int_{0}^{t} |W_{s}|\\, ds\n$$\nThis inequality holds for any partition $\\pi$ of $[0,t]$. Therefore, taking the supremum over all partitions, we obtain an upper bound for the total variation:\n$$\nV_{t}(Y) = \\sup_{\\pi} \\sum_{i=0}^{n-1} |Y_{t_{i+1}} - Y_{t_{i}}| \\leq \\int_{0}^{t} |W_{s}|\\, ds\n$$\nA standard property of Brownian motion is that its sample paths $s \\mapsto W_{s}(\\omega)$ are continuous functions for almost every $\\omega \\in \\Omega$. Since $W_{s}$ is continuous on the compact interval $[0,t]$, it is bounded, and the integral of its absolute value is finite:\n$$\n\\int_{0}^{t} |W_{s}(\\omega)|\\, ds  \\infty \\quad \\text{a.s.}\n$$\nThis implies that $V_{t}(Y)  \\infty$ almost surely. Therefore, the process $Y_{t}$ has sample paths of finite variation on any compact interval $[0,t]$. In fact, since $Y_t$ is an absolutely continuous function of $t$ with derivative $W_t$, its total variation is exactly $\\int_0^t |W_s|ds$.\n\nStep 2: Compute the quadratic covariation $[W,Y]_{t}$.\nWe now analyze the sum $S(\\pi)$ as the mesh $|\\pi| \\to 0$.\n$$\nS(\\pi) = \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_{i}})(Y_{t_{i+1}} - Y_{t_{i}})\n$$\nLet's consider the absolute value of the sum:\n$$\n|S(\\pi)| = \\left| \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_{i}})(Y_{t_{i+1}} - Y_{t_{i}}) \\right| \\leq \\sum_{i=0}^{n-1} |W_{t_{i+1}} - W_{t_{i}}| |Y_{t_{i+1}} - Y_{t_{i}}|\n$$\nWe can bound this sum by factoring out the maximum increment of the $W$ process over the partition:\n$$\n|S(\\pi)| \\leq \\left( \\max_{0 \\leq i \\leq n-1} |W_{t_{i+1}} - W_{t_{i}}| \\right) \\left( \\sum_{i=0}^{n-1} |Y_{t_{i+1}} - Y_{t_{i}}| \\right)\n$$\nLet's analyze the two factors on the right-hand side.\nThe second factor is a sum for a partition $\\pi$. By the definition of total variation, this sum is bounded by the total variation of $Y$ on $[0,t]$:\n$$\n\\sum_{i=0}^{n-1} |Y_{t_{i+1}} - Y_{t_{i}}| \\leq V_{t}(Y)\n$$\nAs established in Step 1, $V_{t}(Y)$ is finite almost surely.\n\nThe first factor is $\\max_{i} |W_{t_{i+1}} - W_{t_{i}}|$. Since the sample paths of Brownian motion are almost surely continuous on the compact interval $[0,t]$, they are also almost surely uniformly continuous on $[0,t]$. This means that for a given sample path, as the mesh of the partition $|\\pi| = \\max_{i}(t_{i+1}-t_{i})$ tends to $0$, the maximum oscillation of $W$ over the subintervals must also tend to $0$. Formally,\n$$\n\\lim_{|\\pi| \\to 0} \\left( \\max_{0 \\leq i \\leq n-1} |W_{t_{i+1}} - W_{t_{i}}| \\right) = 0 \\quad \\text{a.s.}\n$$\nCombining these results, we can take the limit of the inequality for $|S(\\pi)|$ as $|\\pi| \\to 0$:\n$$\n\\lim_{|\\pi| \\to 0} |S(\\pi)| \\leq \\lim_{|\\pi| \\to 0} \\left[ \\left( \\max_{i} |W_{t_{i+1}} - W_{t_{i}}| \\right) \\left( \\sum_{i} |Y_{t_{i+1}} - Y_{t_{i}}| \\right) \\right]\n$$\n$$\n\\lim_{|\\pi| \\to 0} |S(\\pi)| \\leq \\left( \\lim_{|\\pi| \\to 0} \\max_{i} |W_{t_{i+1}} - W_{t_{i}}| \\right) \\cdot \\left( \\sup_{\\pi} \\sum_{i} |Y_{t_{i+1}} - Y_{t_{i}}| \\right)\n$$\nThe second term on the right is $V_t(Y)$. So we get:\n$$\n\\lim_{|\\pi| \\to 0} |S(\\pi)| \\leq 0 \\cdot V_{t}(Y) = 0 \\quad \\text{a.s.}\n$$\nSince $|S(\\pi)| \\geq 0$, this implies that $\\lim_{|\\pi| \\to 0} |S(\\pi)| = 0$ almost surely. This, in turn, implies that $\\lim_{|\\pi| \\to 0} S(\\pi) = 0$ almost surely. Almost sure convergence implies convergence in probability.\n\nTherefore, the quadratic covariation is $0$:\n$$\n[W,Y]_{t} = \\lim_{|\\pi|\\to 0} S(\\pi) = 0 \\quad \\text{(in probability)}\n$$\nThis result is a specific instance of a general theorem in stochastic calculus: the quadratic covariation between a continuous local martingale (or more generally, a continuous semimartingale) and any continuous process of finite variation is identically zero.", "answer": "$$\n\\boxed{0}\n$$", "id": "3071373"}]}