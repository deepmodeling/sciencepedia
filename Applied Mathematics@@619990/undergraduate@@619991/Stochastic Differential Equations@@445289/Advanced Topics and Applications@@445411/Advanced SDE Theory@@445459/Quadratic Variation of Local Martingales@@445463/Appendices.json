{"hands_on_practices": [{"introduction": "Understanding quadratic variation begins with the simplest non-trivial continuous martingale: the Itô integral with a deterministic integrand. This practice guides you through a foundational proof, using only the basic properties of the Wiener integral to derive its quadratic variation. Successfully completing this exercise demonstrates how the quadratic variation is intrinsically linked to the Itô isometry, acting as a cumulative measure of the integrand's squared magnitude.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space carrying a standard Brownian motion (Wiener process) $W=(W_{t})_{t\\in[0,T]}$. Fix $t\\in[0,T]$. Let $h\\in L^{2}([0,T])$ be deterministic and define the Itô integral process $X=(X_{u})_{u\\in[0,T]}$ by $X_{u}=\\int_{0}^{u} h_{s}\\,dW_{s}$. The quadratic variation $[X]_{t}$ of the continuous process $X$ at time $t$ is defined as the limit in probability (with respect to $\\mathbb{P}$) of the sums of squared increments along partitions whose mesh goes to $0$, that is,\n$$\n[X]_{t}=\\lim_{|\\pi|\\to 0}\\sum_{i=0}^{n-1}\\bigl(X_{t_{i+1}}-X_{t_{i}}\\bigr)^{2},\n$$\nwhere $\\pi=\\{0=t_{0}<t_{1}<\\cdots<t_{n}=t\\}$ and $|\\pi|=\\max_{0\\le i\\le n-1}(t_{i+1}-t_{i})$. Using only foundational properties of the Itô integral for deterministic integrands (in particular, Gaussianity, independent increments induced by Brownian motion, and the Itô isometry) together with the above definition of quadratic variation, compute the closed-form expression for $[X]_{t}$ in terms of $h$ and $t$. Give your final answer as a single analytic expression. No rounding is required.", "solution": "The problem asks for the computation of the quadratic variation $[X]_{t}$ of the Itô process $X_{u} = \\int_{0}^{u} h_{s}\\,dW_{s}$, where $h$ is a deterministic function in $L^{2}([0,T])$. We must use the provided definition of quadratic variation as a limit in probability:\n$$\n[X]_{t}=\\lim_{|\\pi|\\to 0}\\sum_{i=0}^{n-1}\\bigl(X_{t_{i+1}}-X_{t_{i}}\\bigr)^{2}\n$$\nfor a partition $\\pi=\\{0=t_{0}<t_{1}<\\cdots<t_{n}=t\\}$ of the interval $[0,t]$. The limit is taken as the mesh of the partition, $|\\pi|=\\max_{0\\le i\\le n-1}(t_{i+1}-t_{i})$, approaches $0$. The derivation must rely on the fundamental properties of the Itô integral for deterministic integrands.\n\nLet $\\pi = \\{0=t_{0}<t_{1}<\\cdots<t_{n}=t\\}$ be a partition of $[0,t]$. Let $S_{\\pi}$ denote the sum of squared increments:\n$$\nS_{\\pi} = \\sum_{i=0}^{n-1}\\bigl(X_{t_{i+1}}-X_{t_{i}}\\bigr)^{2}\n$$\nOur goal is to find the limit of $S_{\\pi}$ in probability as $|\\pi| \\to 0$. We will prove that this limit is $\\int_{0}^{t} h_{s}^{2}\\,ds$ by showing that $S_{\\pi}$ converges to this value in $L^{2}(\\Omega, \\mathcal{F}, \\mathbb{P})$, which implies convergence in probability. Convergence in $L^{2}$ means we must show that:\n$$\n\\lim_{|\\pi|\\to 0} E\\left[\\left(S_{\\pi} - \\int_{0}^{t} h_{s}^{2}\\,ds\\right)^{2}\\right] = 0\n$$\nFirst, let's analyze the increments of the process $X$. For each $i \\in \\{0, 1, \\dots, n-1\\}$, the increment is:\n$$\n\\Delta X_{i} = X_{t_{i+1}}-X_{t_{i}} = \\int_{0}^{t_{i+1}}h_{s}\\,dW_{s} - \\int_{0}^{t_{i}}h_{s}\\,dW_{s} = \\int_{t_{i}}^{t_{i+1}}h_{s}\\,dW_{s}\n$$\nThe problem states we must use the properties of Itô integrals with deterministic integrands.\n1.  **Gaussianity**: Since $h$ is deterministic, the Itô integral $\\int_{t_{i}}^{t_{i+1}}h_{s}\\,dW_{s}$ is a Gaussian random variable. Its mean is $E[\\Delta X_{i}]=0$.\n2.  **Itô Isometry**: The variance of the increment is given by the Itô isometry:\n    $$\n    \\text{Var}(\\Delta X_{i}) = E\\left[(\\Delta X_{i})^{2}\\right] - \\left(E[\\Delta X_{i}]\\right)^{2} = E\\left[\\left(\\int_{t_{i}}^{t_{i+1}}h_{s}\\,dW_{s}\\right)^{2}\\right] = \\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\n    $$\n3.  **Independent Increments**: Because the standard Brownian motion $W$ has independent increments over non-overlapping time intervals, and $h$ is deterministic, the random variables $\\Delta X_{i} = \\int_{t_{i}}^{t_{i+1}}h_{s}\\,dW_{s}$ and $\\Delta X_{j} = \\int_{t_{j}}^{t_{j+1}}h_{s}\\,dW_{s}$ are independent for $i \\neq j$.\n\nNow, let's compute the expectation of the sum $S_{\\pi}$:\n$$\nE[S_{\\pi}] = E\\left[\\sum_{i=0}^{n-1}(\\Delta X_{i})^{2}\\right] = \\sum_{i=0}^{n-1}E\\left[(\\Delta X_{i})^{2}\\right]\n$$\nUsing the Itô isometry for each term, we get:\n$$\nE[S_{\\pi}] = \\sum_{i=0}^{n-1}\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds = \\int_{0}^{t} h_{s}^{2}\\,ds\n$$\nThis shows that the expected value of our approximating sum $S_{\\pi}$ is the candidate for the limit. Now, we analyze the variance of $S_{\\pi}$ to prove convergence.\nThe quantity we wish to show goes to $0$ is $E[(S_{\\pi} - \\int_{0}^{t} h_{s}^{2}\\,ds)^{2}]$. Since $E[S_{\\pi}] = \\int_{0}^{t} h_{s}^{2}\\,ds$, this is exactly the variance of $S_{\\pi}$:\n$$\n\\text{Var}(S_{\\pi}) = E\\left[\\left(S_{\\pi} - E[S_{\\pi}]\\right)^{2}\\right] = \\text{Var}\\left(\\sum_{i=0}^{n-1}(\\Delta X_{i})^{2}\\right)\n$$\nSince the increments $\\Delta X_{i}$ are independent random variables, the random variables $(\\Delta X_{i})^{2}$ are also independent. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(S_{\\pi}) = \\sum_{i=0}^{n-1}\\text{Var}\\left((\\Delta X_{i})^{2}\\right)\n$$\nFor each $i$, the variance is given by $\\text{Var}((\\Delta X_{i})^{2}) = E[(\\Delta X_{i})^{4}] - (E[(\\Delta X_{i})^{2}])^{2}$.\nWe already know $E[(\\Delta X_{i})^{2}] = \\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds$. To find $E[(\\Delta X_{i})^{4}]$, we use the fact that $\\Delta X_{i}$ is a centered Gaussian random variable with variance $\\sigma_{i}^{2} = \\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds$. For a Gaussian random variable $Z \\sim N(0, \\sigma^{2})$, the fourth moment is $E[Z^{4}] = 3\\sigma^{4}$.\nThus,\n$$\nE[(\\Delta X_{i})^{4}] = 3\\left(\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\\right)^{2}\n$$\nSubstituting this back into the expression for the variance of $(\\Delta X_{i})^{2}$:\n$$\n\\text{Var}((\\Delta X_{i})^{2}) = 3\\left(\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\\right)^{2} - \\left(\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\\right)^{2} = 2\\left(\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\\right)^{2}\n$$\nSumming over all $i$ gives the variance of $S_{\\pi}$:\n$$\n\\text{Var}(S_{\\pi}) = 2\\sum_{i=0}^{n-1}\\left(\\int_{t_{i}}^{t_{i+1}}h_{s}^{2}\\,ds\\right)^{2}\n$$\nTo complete the proof, we must show that $\\text{Var}(S_{\\pi}) \\to 0$ as $|\\pi| \\to 0$. Let $g(s) = h_{s}^{2}$. Since $h \\in L^{2}([0,T])$, $g \\in L^{1}([0,T])$. Let $G(u) = \\int_{0}^{u}g(s)\\,ds = \\int_{0}^{u}h_{s}^{2}\\,ds$. As the integral of an $L^{1}$ function, $G$ is absolutely continuous on $[0,T]$, and therefore uniformly continuous on this compact interval.\nWe can rewrite the variance as:\n$$\n\\text{Var}(S_{\\pi}) = 2\\sum_{i=0}^{n-1}\\left(G(t_{i+1}) - G(t_{i})\\right)^{2}\n$$\nSince $g(s)=h_s^2 \\ge 0$, the function $G(u)$ is non-decreasing. Thus $G(t_{i+1}) - G(t_{i}) \\ge 0$. We can bound the sum:\n$$\n\\text{Var}(S_{\\pi}) \\le 2 \\left(\\max_{0\\le i \\le n-1} (G(t_{i+1}) - G(t_{i}))\\right) \\left(\\sum_{j=0}^{n-1}(G(t_{j+1}) - G(t_{j}))\\right)\n$$\nThe second part is a telescoping sum:\n$$\n\\sum_{j=0}^{n-1}(G(t_{j+1}) - G(t_{j})) = G(t_{n}) - G(t_{0}) = G(t) - G(0) = \\int_{0}^{t}h_{s}^{2}\\,ds\n$$\nSo, we have the inequality:\n$$\n\\text{Var}(S_{\\pi}) \\le 2\\left(\\int_{0}^{t}h_{s}^{2}\\,ds\\right) \\left(\\max_{0\\le i \\le n-1} (G(t_{i+1}) - G(t_{i}))\\right)\n$$\nAs the mesh of the partition $|\\pi| = \\max_{i}(t_{i+1}-t_{i})$ approaches $0$, the uniform continuity of $G$ on $[0,t]$ implies that $\\max_{i}(G(t_{i+1}) - G(t_{i}))$ must approach $0$.\nTherefore, $\\lim_{|\\pi|\\to 0} \\text{Var}(S_{\\pi}) = 0$.\nWe have demonstrated that $S_{\\pi}$ converges to its mean, $\\int_{0}^{t}h_{s}^{2}\\,ds$, in $L^{2}$. Since convergence in $L^{2}$ implies convergence in probability, we have:\n$$\n[X]_{t} = \\lim_{|\\pi|\\to 0} S_{\\pi} = \\int_{0}^{t}h_{s}^{2}\\,ds\n$$\nThis is the closed-form expression for the quadratic variation of $X$ at time $t$.", "answer": "$$\n\\boxed{\\int_{0}^{t} h_{s}^{2}\\,ds}\n$$", "id": "3071614"}, {"introduction": "Real-world models are rarely as simple as a Wiener integral; they are typically described by stochastic differential equations (SDEs) containing both drift and diffusion terms. This exercise challenges you to compute the quadratic variation for a general Itô process, the solution to such an SDE. You will discover the remarkable fact that the finite-variation drift component vanishes in the limit, leaving the quadratic variation determined solely by the diffusion coefficient.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ satisfying the usual conditions and carrying a one-dimensional standard Brownian motion (BM) $(W_t)_{t\\in[0,T]}$. Let $(X_t)_{t\\in[0,T]}$ be the unique strong solution to the stochastic differential equation\n$$\ndX_t \\,=\\, \\mu(X_t,t)\\,dt \\,+\\, \\sigma(X_t,t)\\,dW_t,\\qquad X_0\\in L^2(\\Omega,\\mathcal{F}_0,\\mathbb{P}),\n$$\nwhere $\\mu:\\mathbb{R}\\times[0,T]\\to\\mathbb{R}$ and $\\sigma:\\mathbb{R}\\times[0,T]\\to\\mathbb{R}$ are Borel measurable in both arguments, globally Lipschitz in the space variable uniformly in time, and of linear growth. Assume further that $t\\mapsto \\mu(x,t)$ and $t\\mapsto \\sigma(x,t)$ are continuous for each fixed $x\\in\\mathbb{R}$. Define the quadratic variation $[X]_t$ of the continuous semimartingale $(X_t)_{t\\in[0,T]}$ by\n$$\n[X]_t \\,=\\, \\mathbb{P}\\text{-}\\lim_{|\\pi|\\to 0}\\sum_{i=1}^{n} \\bigl(X_{t_i}-X_{t_{i-1}}\\bigr)^2,\n$$\nwhere $\\pi=\\{0=t_0<t_1<\\cdots<t_n=t\\}$ is any sequence of partitions of $[0,t]$ with mesh $|\\pi|=\\max_{1\\le i\\le n}(t_i-t_{i-1})$ converging to $0$.\n\nStarting from the fundamental definitions of finite variation and local martingale parts of a semimartingale and using only standard facts such as Itô’s formula, Itô’s isometry, and basic inequalities, derive a closed-form expression for $[X]_t$ in terms of $\\sigma$, $X$, and $t$. Your derivation must justify why any bounded variation contributions and cross terms vanish in the quadratic variation limit and must establish convergence in probability of the Riemann sums to your candidate limit. Express your final answer as a single analytic expression in $t$.\n\nYour final answer must be a single closed-form analytic expression. No numerical rounding is required.", "solution": "The problem asks for the derivation of the quadratic variation, $[X]_t$, of a process $(X_t)_{t\\in[0,T]}$ which is the unique strong solution to a stochastic differential equation (SDE).\n\nThe SDE $dX_t = \\mu(X_t,t)dt + \\sigma(X_t,t)dW_t$ can be written in integral form as:\n$$\nX_t = X_0 + \\int_0^t \\mu(X_s, s) ds + \\int_0^t \\sigma(X_s, s) dW_s\n$$\nThe solution $X_t$ is a continuous semimartingale, as it is the sum of a continuous process of finite variation and a continuous local martingale.\nLet $A_t = \\int_0^t \\mu(X_s, s) ds$ and $M_t = \\int_0^t \\sigma(X_s, s) dW_s$.\nThus, $X_t = X_0 + A_t + M_t$. The initial condition $X_0$ is constant over time, so its quadratic variation is $0$. Therefore, $[X]_t = [X-X_0]_t = [A+M]_t$.\nBy the polarization identity for quadratic variation, $[A+M]_t = [A]_t + 2[A,M]_t + [M]_t$. We will analyze each term.\n\nLet $\\pi = \\{0=t_0 < t_1 < \\dots < t_n = t\\}$ be a partition of $[0,t]$ with mesh $|\\pi| = \\max_i(t_i - t_{i-1})$.\nLet $\\Delta Y_i = Y_{t_i} - Y_{t_{i-1}}$ for any process $Y$. The defining sum for the quadratic variation of $X$ is\n$$\n\\sum_{i=1}^n (\\Delta X_i)^2 = \\sum_{i=1}^n (\\Delta A_i + \\Delta M_i)^2 = \\sum_{i=1}^n (\\Delta A_i)^2 + 2\\sum_{i=1}^n \\Delta A_i \\Delta M_i + \\sum_{i=1}^n (\\Delta M_i)^2\n$$\n\n**1. The Bounded Variation Term, $[A]_t$**\nThe process $A_t$ is absolutely continuous and thus has continuous paths of finite variation. Its total variation on $[0,t]$ is $V_t(A) = \\int_0^t |\\mu(X_s, s)| ds$. The problem's conditions ensure that $V_t(A)$ is finite almost surely.\nThe quadratic variation sum for A is $\\sum_{i=1}^n (\\Delta A_i)^2$. We have:\n$$\n|\\Delta A_i| = \\left|\\int_{t_{i-1}}^{t_i} \\mu(X_s, s) ds\\right| \\le \\int_{t_{i-1}}^{t_i} |\\mu(X_s, s)| ds\n$$\nThe sum can be bounded as follows:\n$$\n0 \\le \\sum_{i=1}^n (\\Delta A_i)^2 \\le \\max_{1 \\le j \\le n} |\\Delta A_j| \\sum_{i=1}^n |\\Delta A_i| \\le \\left( \\max_{1 \\le j \\le n} \\int_{t_{j-1}}^{t_j} |\\mu(X_s, s)| ds \\right) V_t(A)\n$$\nThe paths $s \\mapsto X_s(\\omega)$ are continuous a.s. The map $(x,s) \\mapsto \\mu(x,s)$ is continuous in $s$ and Lipschitz in $x$. Thus, the composite function $s \\mapsto |\\mu(X_s(\\omega), s)|$ is continuous on $[0,t]$ for a.e. $\\omega$. Consequently, it is uniformly continuous. As $|\\pi| \\to 0$, $\\max_{j} (t_j - t_{j-1}) \\to 0$, which implies $\\max_{j} \\int_{t_{j-1}}^{t_j} |\\mu(X_s, s)| ds \\to 0$ a.s. Since $V_t(A)$ is finite a.s., the entire expression tends to $0$ a.s. Convergence a.s. implies convergence in probability. Therefore,\n$$\n[A]_t = \\mathbb{P}\\text{-}\\lim_{|\\pi|\\to 0} \\sum_{i=1}^n (\\Delta A_i)^2 = 0\n$$\n\n**2. The Cross-Variation Term, $[A,M]_t$**\nThe defining sum is $\\sum_{i=1}^n \\Delta A_i \\Delta M_i$. By the Cauchy-Schwarz inequality for sums:\n$$\n\\left| \\sum_{i=1}^n \\Delta A_i \\Delta M_i \\right|^2 \\le \\left( \\sum_{i=1}^n (\\Delta A_i)^2 \\right) \\left( \\sum_{i=1}^n (\\Delta M_i)^2 \\right)\n$$\nAs $|\\pi| \\to 0$, the first factor on the right converges in probability to $[A]_t = 0$. The second factor converges in probability to $[M]_t$, which we will show is a well-defined process. The product of a sequence of random variables converging to $0$ in probability and a sequence converging in probability is a sequence that converges to $0$ in probability. Thus,\n$$\n[A,M]_t = \\mathbb{P}\\text{-}\\lim_{|\\pi|\\to 0} \\sum_{i=1}^n \\Delta A_i \\Delta M_i = 0\n$$\n\n**3. The Martingale Term, $[M]_t$**\nWe have established that $[X]_t = [M]_t$. We now derive $[M]_t$. We need to find the limit in probability of $S_\\pi = \\sum_{i=1}^n (\\Delta M_i)^2 = \\sum_{i=1}^n \\left(\\int_{t_{i-1}}^{t_i} \\sigma(X_s, s) dW_s\\right)^2$. We will show this converges to $I_t = \\int_0^t \\sigma^2(X_s, s) ds$.\nWe will show that $S_\\pi - I_t \\to 0$ in $L^1(\\Omega)$, which implies convergence in probability.\nLet $\\sigma_s = \\sigma(X_s, s)$ and $\\sigma_{t_{i-1}} = \\sigma(X_{t_{i-1}}, t_{i-1})$.\nLet's decompose the difference $S_\\pi - I_t$ as a sum of three terms:\n$S_\\pi - I_t = T_1 + T_2 + T_3$, where\n$$\nT_1 = \\sum_{i=1}^n \\left( (\\Delta M_i)^2 - \\left(\\int_{t_{i-1}}^{t_i} \\sigma_{t_{i-1}} dW_s\\right)^2 \\right)\n$$\n$$\nT_2 = \\sum_{i=1}^n \\left( \\left(\\int_{t_{i-1}}^{t_i} \\sigma_{t_{i-1}} dW_s\\right)^2 - \\int_{t_{i-1}}^{t_i} \\sigma_{t_{i-1}}^2 ds \\right)\n$$\n$$\nT_3 = \\sum_{i=1}^n \\left( \\int_{t_{i-1}}^{t_i} \\sigma_{t_{i-1}}^2 ds - \\int_{t_{i-1}}^{t_i} \\sigma_s^2 ds \\right) = \\int_0^t (\\sigma_{\\tau(s)}^2 - \\sigma_s^2) ds\n$$\nwhere $\\tau(s) = t_{i-1}$ for $s \\in [t_{i-1}, t_i)$.\nThe process $s \\mapsto \\sigma_s^2$ is almost surely continuous, thus uniformly continuous on $[0,t]$ for a given path. As $|\\pi| \\to 0$, $\\tau(s) \\to s$ uniformly, so $\\sigma_{\\tau(s)}^2 \\to \\sigma_s^2$ uniformly on $[0,t]$ a.s. By bounded convergence, $T_3 \\to 0$ a.s., and thus in probability.\n\nFor $T_2$, we have $\\int_{t_{i-1}}^{t_i} \\sigma_{t_{i-1}} dW_s = \\sigma_{t_{i-1}} (W_{t_i} - W_{t_{i-1}}) = \\sigma_{t_{i-1}}\\Delta W_i$.\n$$\nT_2 = \\sum_{i=1}^n \\sigma_{t_{i-1}}^2 ((\\Delta W_i)^2 - \\Delta t_i)\n$$\nThis is a sum of martingale differences with respect to the filtration $(\\mathcal{F}_{t_i})$. The expectation of each term is $0$, so $E[T_2]=0$. The variance is:\n$$\n\\text{Var}(T_2) = E[T_2^2] = \\sum_{i=1}^n E[\\sigma_{t_{i-1}}^4 ((\\Delta W_i)^2 - \\Delta t_i)^2] = \\sum_{i=1}^n E\\left[ \\sigma_{t_{i-1}}^4 E[((\\Delta W_i)^2 - \\Delta t_i)^2 | \\mathcal{F}_{t_{i-1}}] \\right]\n$$\nSince $\\Delta W_i \\sim N(0, \\Delta t_i)$ and is independent of $\\mathcal{F}_{t_{i-1}}$, we have $E[((\\Delta W_i)^2 - \\Delta t_i)^2] = E[(\\Delta W_i)^4] - (\\Delta t_i)^2 = 3(\\Delta t_i)^2 - (\\Delta t_i)^2 = 2(\\Delta t_i)^2$.\nLet $K = \\sup_{s \\in [0,T]} E[\\sigma_s^4]$, which is finite under the given assumptions.\n$\\text{Var}(T_2) \\le \\sum_{i=1}^n K \\cdot 2(\\Delta t_i)^2 \\le 2K \\max_i(\\Delta t_i) \\sum_i \\Delta t_i = 2Kt|\\pi|$.\nAs $|\\pi| \\to 0$, $\\text{Var}(T_2) \\to 0$, so $T_2 \\to 0$ in $L^2(\\Omega)$ and thus in probability.\n\nFor $T_1$, using the identity $a^2-b^2 = (a-b)(a+b)$ and Cauchy-Schwarz inequality for expectations:\n$$\nE[|T_1|] \\le \\sum_{i=1}^n E\\left| \\left(\\int_{t_{i-1}}^{t_i} (\\sigma_s - \\sigma_{t_{i-1}})dW_s\\right) \\left(\\int_{t_{i-1}}^{t_i} (\\sigma_s + \\sigma_{t_{i-1}})dW_s\\right) \\right|\n$$\n$$\n\\le \\sum_{i=1}^n \\left(E\\left[\\left(\\int_{t_{i-1}}^{t_i} (\\sigma_s - \\sigma_{t_{i-1}})dW_s\\right)^2\\right]\\right)^{1/2} \\left(E\\left[\\left(\\int_{t_{i-1}}^{t_i} (\\sigma_s + \\sigma_{t_{i-1}})dW_s\\right)^2\\right]\\right)^{1/2}\n$$\nBy Itô's isometry:\n$$\nE[|T_1|] \\le \\sum_{i=1}^n \\left(E\\left[\\int_{t_{i-1}}^{t_i} (\\sigma_s - \\sigma_{t_{i-1}})^2 ds\\right]\\right)^{1/2} \\left(E\\left[\\int_{t_{i-1}}^{t_i} (\\sigma_s + \\sigma_{t_{i-1}})^2 ds\\right]\\right)^{1/2}\n$$\nThe continuity properties of $\\sigma$ and $X_t$ imply that $E[(\\sigma_s - \\sigma_{t_{i-1}})^2] \\to 0$ as $s \\to t_{i-1}$. Also, $E[(\\sigma_s - \\sigma_{t_{i-1}})^2] \\le C(s-t_{i-1})$ for some constant $C>0$. The integral is bounded by $C'(\\Delta t_i)^2$. The second term under the square root is bounded by $C''\\Delta t_i$.\nSo, $E[|T_1|] \\le \\sum_i \\sqrt{C'(\\Delta t_i)^2} \\sqrt{C''\\Delta t_i} = \\sqrt{C'C''}\\sum_i (\\Delta t_i)^{3/2} \\le \\sqrt{C'C''}\\sqrt{|\\pi|}t$.\nThis tends to $0$ as $|\\pi| \\to 0$. So $T_1 \\to 0$ in $L^1(\\Omega)$ and thus in probability.\n\nSince $T_1$, $T_2$, and $T_3$ all converge to $0$ in probability, their sum $S_\\pi - I_t$ also converges to $0$ in probability.\nWe conclude that:\n$$\n[X]_t = [M]_t = \\mathbb{P}\\text{-}\\lim_{|\\pi|\\to 0} \\sum_{i=1}^n (\\Delta M_i)^2 = \\int_0^t \\sigma^2(X_s, s) ds\n$$\nThis is the desired closed-form expression. It is an adapted stochastic process, representing the cumulative variance of the martingale part of $X_t$.", "answer": "$$\\boxed{\\int_0^t \\sigma^2(X_s, s) ds}$$", "id": "3071633"}, {"introduction": "Quadratic variation is not limited to continuous processes like solutions to SDEs; it is a fundamental property of all semimartingales, including those with jumps. This practice shifts our focus to a pure-jump martingale constructed from a Poisson process, a common model for discrete events. By calculating its quadratic variation directly from the definition, you will find that it equals the sum of the squared jumps, revealing a more general interpretation of this crucial concept.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\geq 0},\\mathbb{P})$ satisfying the usual conditions. Let $(N_t)_{t\\geq 0}$ be a Poisson process with intensity $\\lambda>0$. Denote the jump times by $(T_k)_{k\\in\\mathbb{N}}$, so that $N_t=\\sum_{k\\geq 1}\\mathbf{1}_{\\{T_k\\leq t\\}}$. Let $(J_k)_{k\\in\\mathbb{N}}$ be a sequence of independent and identically distributed random variables, each taking values $+1$ and $-1$ with probability $1/2$, independent of $(N_t)_{t\\geq 0}$. Define the pure-jump process $(M_t)_{t\\geq 0}$ by\n$$\nM_t:=\\sum_{k=1}^{N_t}J_k,\\quad t\\geq 0,\n$$\nand take the filtration $(\\mathcal{F}_t)_{t\\geq 0}$ to be generated by $\\{N_s,J_k:s\\leq t,k\\leq N_t\\}$, completed in the usual way.\n\nUsing only the definition of quadratic variation of a process $M$ over $[0,t]$ as the limit of the sums of squared increments along partitions,\n$$\n[M]_t:=\\lim_{|\\pi|\\to 0}\\sum_{i=1}^n\\big(M_{t_i\\wedge t}-M_{t_{i-1}\\wedge t}\\big)^2,\n$$\nwhere $\\pi=\\{0=t_0<t_1<\\dots<t_n=t\\}$ and $|\\pi|=\\max_i(t_i-t_{i-1})$, do the following:\n\n1. Show that $(M_t)_{t\\geq 0}$ is a martingale with respect to $(\\mathcal{F}_t)_{t\\geq 0}$.\n\n2. Compute the quadratic variation process $[M]_t$ for $t\\geq 0$ using the above definition, and verify that it equals the counting process of jumps. Express your final answer as a single closed-form analytical expression in terms of the given objects. No rounding is required.", "solution": "We begin by establishing that $(M_t)_{t\\geq 0}$ is a martingale, and then we compute its quadratic variation using the definition via partition limits.\n\nFirst, we show the martingale property. Observe that $M_t=\\sum_{k=1}^{N_t}J_k$ is a compound Poisson process with independent and identically distributed jumps of mean zero. We check integrability and the conditional expectation:\n\n- Integrability: Since $|J_k|=1$ almost surely, we have $|M_t|\\leq N_t$. Therefore,\n$$\n\\mathbb{E}[|M_t|]\\leq \\mathbb{E}[N_t]=\\lambda t<\\infty.\n$$\n\n- Martingale property: For $0\\leq s\\leq t$,\n$$\n\\mathbb{E}[M_t\\mid\\mathcal{F}_s]\n=\\mathbb{E}\\left[\\sum_{k=1}^{N_s}J_k+\\sum_{k=N_s+1}^{N_t}J_k\\,\\middle|\\,\\mathcal{F}_s\\right]\n= M_s+\\mathbb{E}\\left[\\sum_{k=1}^{N_t-N_s}J_{N_s+k}\\,\\middle|\\,\\mathcal{F}_s\\right].\n$$\nBy independence of increments of the Poisson process and independence of the jump sizes $(J_k)$ from $(N_t)$, the random variables $\\{J_{N_s+k}\\}_{1\\leq k\\leq N_t-N_s}$ are independent of $\\mathcal{F}_s$, and $N_t-N_s$ is independent of $\\mathcal{F}_s$. Also, $\\mathbb{E}[J_1]=0$. Hence\n$$\n\\mathbb{E}\\left[\\sum_{k=1}^{N_t-N_s}J_{N_s+k}\\,\\middle|\\,\\mathcal{F}_s\\right]\n=\\mathbb{E}[N_t-N_s]\\cdot \\mathbb{E}[J_1]\n=\\lambda(t-s)\\cdot 0=0.\n$$\nTherefore $\\mathbb{E}[M_t\\mid\\mathcal{F}_s]=M_s$, and $(M_t)_{t\\geq 0}$ is a martingale (hence, in particular, a local martingale).\n\nNext, we compute the quadratic variation via the definition\n$$\n[M]_t:=\\lim_{|\\pi|\\to 0}\\sum_{i=1}^n\\big(M_{t_i\\wedge t}-M_{t_{i-1}\\wedge t}\\big)^2.\n$$\nFix $t\\geq 0$ and a partition $\\pi=\\{0=t_0<t_1<\\dots<t_n=t\\}$. For each interval $I_i=(t_{i-1},t_i]$, let $K_i:=N_{t_i}-N_{t_{i-1}}$ denote the number of jumps in $I_i$, which is a Poisson random variable with parameter $\\lambda\\delta_i$, where $\\delta_i:=t_i-t_{i-1}$. On $I_i$, the increment of $M$ is the sum of the $K_i$ jumps,\n$$\nM_{t_i}-M_{t_{i-1}}=\\sum_{j=1}^{K_i}J_{i,j},\n$$\nwhere $\\{J_{i,j}\\}$ are the jump sizes occurring in $I_i$, each equal to $\\pm 1$. Then\n$$\n\\big(M_{t_i}-M_{t_{i-1}}\\big)^2=\\left(\\sum_{j=1}^{K_i}J_{i,j}\\right)^2\n=\\sum_{j=1}^{K_i}J_{i,j}^2+2\\sum_{1\\leq j<\\ell\\leq K_i}J_{i,j}J_{i,\\ell}.\n$$\nBecause $J_{i,j}^2=1$ for all $j$, the first sum is exactly $K_i$. Therefore, the total sum of squared increments over the partition decomposes as\n$$\n\\sum_{i=1}^n\\big(M_{t_i}-M_{t_{i-1}}\\big)^2\n=\\sum_{i=1}^n K_i \\;+\\; 2\\sum_{i=1}^n\\sum_{1\\leq j<\\ell\\leq K_i}J_{i,j}J_{i,\\ell}.\n$$\nThe first term, $\\sum_{i=1}^n K_i$, is exactly the total number of jumps in $(0,t]$, which is $N_t$. The second term consists of cross terms that only occur on intervals containing at least two jumps. We now show that, as $|\\pi|\\to 0$, the contribution of the cross terms vanishes in $L^1$ (and hence in probability), so that the limit of the sum of squared increments equals $N_t$.\n\nDefine the discrepancy\n$$\nD(\\pi):=\\left|\\sum_{i=1}^n\\big(M_{t_i}-M_{t_{i-1}}\\big)^2 - N_t\\right|\n= \\left|2\\sum_{i=1}^n\\sum_{1\\leq j<\\ell\\leq K_i}J_{i,j}J_{i,\\ell}\\right|.\n$$\nOn any interval with $K_i\\leq 1$, the inner sum is empty and contributes $0$. On an interval with $K_i\\geq 2$, we have\n$$\n\\left|2\\sum_{1\\leq j<\\ell\\leq K_i}J_{i,j}J_{i,\\ell}\\right|\\leq 2\\binom{K_i}{2}.\n$$\nTaking expectations and using the fact that $K_i\\sim\\operatorname{Poisson}(\\lambda\\delta_i)$ and the identity for the factorial moments of a Poisson random variable,\n$$\n\\mathbb{E}\\left[\\binom{K_i}{2}\\right]=\\frac{1}{2}\\mathbb{E}[K_i(K_i-1)]=\\frac{1}{2}(\\lambda\\delta_i)^2,\n$$\nwe find\n$$\n\\mathbb{E}[D(\\pi)]\\leq 2\\sum_{i=1}^n \\mathbb{E}\\left[\\binom{K_i}{2}\\right]\n=2\\sum_{i=1}^n \\frac{1}{2}(\\lambda\\delta_i)^2\n=\\sum_{i=1}^n (\\lambda\\delta_i)^2\n=\\lambda^2\\sum_{i=1}^n \\delta_i^2.\n$$\nSince $\\sum_{i=1}^n \\delta_i^2 \\leq t\\,|\\pi|$ by the Cauchy–Schwarz inequality (or the bound $\\sum \\delta_i^2 \\leq (\\max_i \\delta_i)\\sum \\delta_i = |\\pi|\\,t$), we have\n$$\n\\mathbb{E}[D(\\pi)]\\leq \\lambda^2 t\\,|\\pi|\\xrightarrow[|\\pi|\\to 0]{}0.\n$$\nTherefore $D(\\pi)\\to 0$ in $L^1$ and hence in probability. Consequently,\n$$\n\\lim_{|\\pi|\\to 0}\\sum_{i=1}^n\\big(M_{t_i\\wedge t}-M_{t_{i-1}\\wedge t}\\big)^2\n=\\sum_{s\\leq t}(\\Delta M_s)^2\n=N_t,\n$$\nwhere $\\Delta M_s:=M_s-M_{s-}$ denotes the jump of $M$ at time $s$, and $(\\Delta M_s)^2=1$ exactly when $s$ is a jump time of $M$, and $0$ otherwise. This shows that the quadratic variation of $M$ up to time $t$ is the counting process of jumps,\n$$\n[M]_t=N_t.\n$$\nThis verifies that the quadratic variation equals the counting process of jumps for the given pure-jump martingale with jumps of size $\\pm 1$ occurring at Poisson times.", "answer": "$$\\boxed{N_t}$$", "id": "3071629"}]}