{"hands_on_practices": [{"introduction": "To appreciate the unique properties of stochastic processes, it is often helpful to first consider their deterministic counterparts. This exercise invites you to examine a simple, continuously differentiable path through the lens of semimartingale theory [@problem_id:3064274]. By showing that such a path is a semimartingale of finite variation and then calculating its quadratic variation from first principles, you will establish a crucial baseline: smooth, predictable paths have zero \"stochastic roughness,\" a concept fundamentally captured by quadratic variation.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions. Fix a finite time horizon $T0$, and let $x:[0,T]\\to\\mathbb{R}$ be a deterministic function that is continuously differentiable on $[0,T]$, i.e., $x\\in C^{1}([0,T])$. Consider the stochastic process defined by $X_{t}=x(t)$ for $t\\in[0,T]$.\n\nUsing only foundational definitions and well-tested facts, justify why $X$ is a continuous semimartingale by identifying an admissible decomposition into a local martingale part and a finite variation part, and by verifying the finite variation property from first principles. Then, starting from the definition of quadratic variation via partition limits, compute the quadratic variation of $X$ over $[0,T]$, denoted $[X]_{T}$, by evaluating the limit of the sum of squared increments over a sequence of partitions whose mesh tends to $0$.\n\nYour final answer must be a single exact value for $[X]_{T}$. No rounding is required.", "solution": "The process $X_{t}=x(t)$ is deterministic and continuous on $[0,T]$ because $x\\in C^{1}([0,T])$ implies $x$ is continuous. It is adapted to any filtration $(\\mathcal{F}_{t})_{t\\geq 0}$ because a deterministic process is measurable with respect to the trivial $\\sigma$-algebra at all times and thus adapted.\n\nBy definition, a semimartingale is a process that can be decomposed as the sum of a local martingale and a finite variation process. We exhibit the decomposition\n$$\nX_{t} \\;=\\; M_{t} + A_{t}, \\quad \\text{with} \\quad M_{t}=0 \\quad \\text{and} \\quad A_{t}=x(t).\n$$\nThe process $M$ is trivially a (continuous) local martingale since it is constant zero. To complete the justification that $X$ is a continuous semimartingale, it suffices to show that $A$ has finite variation on $[0,T]$.\n\nRecall the total variation of a function $x$ on $[0,T]$ is defined as\n$$\nV_{T}(x) \\;=\\; \\sup_{\\Pi}\\sum_{i=1}^{n} \\big| x(t_{i}) - x(t_{i-1}) \\big|,\n$$\nwhere the supremum is taken over all finite partitions $\\Pi=\\{0=t_{0}t_{1}\\dotst_{n}=T\\}$ of $[0,T]$. Since $x\\in C^{1}([0,T])$, the fundamental theorem of calculus yields\n$$\nx(t_{i}) - x(t_{i-1}) \\;=\\; \\int_{t_{i-1}}^{t_{i}} x'(s)\\, \\mathrm{d}s.\n$$\nApplying the triangle inequality and summing over the partition,\n$$\n\\sum_{i=1}^{n} \\big| x(t_{i}) - x(t_{i-1}) \\big|\n\\;=\\; \\sum_{i=1}^{n} \\left| \\int_{t_{i-1}}^{t_{i}} x'(s)\\, \\mathrm{d}s \\right|\n\\;\\leq\\; \\sum_{i=1}^{n} \\int_{t_{i-1}}^{t_{i}} |x'(s)|\\, \\mathrm{d}s\n\\;=\\; \\int_{0}^{T} |x'(s)|\\, \\mathrm{d}s.\n$$\nBecause $x'$ is continuous on $[0,T]$, the integral $\\int_{0}^{T} |x'(s)|\\, \\mathrm{d}s$ is finite. Hence $V_{T}(x)\\infty$, so $A$ has finite variation on $[0,T]$. Therefore $X$ is a continuous semimartingale via the decomposition $X=M+A$ with $M=0$ and $A=x$.\n\nWe now compute the quadratic variation $[X]_{T}$ from first principles. By definition, for a continuous process $X$, the quadratic variation on $[0,T]$ is given by the limit\n$$\n[X]_{T} \\;=\\; \\lim_{n\\to\\infty} \\sum_{i=1}^{m(n)} \\big( X_{t_{i}^{(n)}} - X_{t_{i-1}^{(n)}} \\big)^{2},\n$$\nwhere $\\{0=t_{0}^{(n)}t_{1}^{(n)}\\dotst_{m(n)}^{(n)}=T\\}$ is a sequence of partitions of $[0,T]$ whose mesh $\\|P_{n}\\|=\\max_{i}(t_{i}^{(n)}-t_{i-1}^{(n)})$ tends to $0$ as $n\\to\\infty$. Since $X_{t}=x(t)$ and $x\\in C^{1}([0,T])$, the mean value theorem ensures that for each interval $[t_{i-1}^{(n)},t_{i}^{(n)}]$ there exists $\\xi_{i}^{(n)}\\in (t_{i-1}^{(n)},t_{i}^{(n)})$ such that\n$$\nx\\big(t_{i}^{(n)}\\big) - x\\big(t_{i-1}^{(n)}\\big) \\;=\\; x'\\big(\\xi_{i}^{(n)}\\big)\\, \\big( t_{i}^{(n)} - t_{i-1}^{(n)} \\big).\n$$\nTherefore,\n$$\n\\sum_{i=1}^{m(n)} \\big( x\\big(t_{i}^{(n)}\\big) - x\\big(t_{i-1}^{(n)}\\big) \\big)^{2}\n\\;=\\; \\sum_{i=1}^{m(n)} \\big( x'\\big(\\xi_{i}^{(n)}\\big) \\big)^{2} \\big( t_{i}^{(n)} - t_{i-1}^{(n)} \\big)^{2}.\n$$\nLet $K=\\sup_{t\\in[0,T]} |x'(t)|$, which is finite because $x'$ is continuous on the compact interval $[0,T]$. Then\n$$\n\\sum_{i=1}^{m(n)} \\big( x\\big(t_{i}^{(n)}\\big) - x\\big(t_{i-1}^{(n)}\\big) \\big)^{2}\n\\;\\leq\\; K^{2}\\, \\sum_{i=1}^{m(n)} \\big( t_{i}^{(n)} - t_{i-1}^{(n)} \\big)^{2}\n\\;\\leq\\; K^{2}\\, \\|P_{n}\\| \\sum_{i=1}^{m(n)} \\big( t_{i}^{(n)} - t_{i-1}^{(n)} \\big)\n\\;=\\; K^{2}\\, \\|P_{n}\\|\\, T.\n$$\nAs $n\\to\\infty$, $\\|P_{n}\\|\\to 0$, hence the upper bound $K^{2}\\, \\|P_{n}\\|\\, T \\to 0$, which forces\n$$\n\\lim_{n\\to\\infty} \\sum_{i=1}^{m(n)} \\big( x\\big(t_{i}^{(n)}\\big) - x\\big(t_{i-1}^{(n)}\\big) \\big)^{2} \\;=\\; 0.\n$$\nTherefore, the quadratic variation of $X$ on $[0,T]$ is\n$$\n[X]_{T} \\;=\\; 0.\n$$\n\nAs a contextual remark related to local time of continuous semimartingales, note that continuous finite variation processes (such as $X$ here) have vanishing quadratic variation and consequently have trivial local time at any level: the local time $L^{a}_{t}$ of $X$ is identically $0$ for all $a\\in\\mathbb{R}$ and $t\\in[0,T]$. This aligns with the occupation density perspective, where local time reflects time spent oscillating at a level in a quadratic variation sense, which does not occur for finite variation paths.\n\nThe required computation is complete.", "answer": "$$\\boxed{0}$$", "id": "3064274"}, {"introduction": "We now turn to the canonical stochastic process, the standard Brownian motion, to explore the concept of local time in a concrete setting. This exercise guides you through a classic derivation that connects the absolute value of a Brownian motion, $|W_t|$, to its local time at zero, $L_t^0(W)$, using the powerful Tanaka's formula [@problem_id:3064283]. Completing this practice will not only yield a famous result for the expected local time but also deepen your understanding of the reflection principle and the properties of It√¥ integrals.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion starting at $W_{0} = 0$. For a fixed $t  0$, let $L_{t}^{0}(W)$ denote the local time of $W$ at level $0$ up to time $t$, defined as the continuous, increasing process satisfying the occupation density property for continuous semimartingales, and equivalently appearing in Tanaka's formula. Denote by $M_{t} = \\sup_{0 \\leq s \\leq t} W_{s}$ the running maximum of $W$ up to time $t$.\n\nUsing only fundamental properties of Brownian motion (independent stationary Gaussian increments, symmetry, continuity of paths) together with the reflection principle and the characterization of local time via Tanaka's formula, carry out the following steps:\n\n1. Derive the tail distribution of $M_{t}$ via the reflection principle, and use it to show that $M_{t}$ and $|W_{t}|$ have the same distribution.\n2. Compute $E[|W_{t}|]$ in closed form by an explicit integration against the Gaussian distribution, without invoking any pre-tabulated integrals beyond the basic properties of the Gaussian density.\n3. Justify carefully, using Tanaka's formula and martingale properties, that $E[L_{t}^{0}(W)] = E[|W_{t}|]$.\n\nYour final task is to provide a single closed-form analytic expression for $E[L_{t}^{0}(W)]$ as a function of $t$. Do not approximate or round; provide the exact expression.", "solution": "The problem is valid as it is scientifically grounded in the theory of stochastic processes, is well-posed with a unique and meaningful solution, and is stated objectively using precise mathematical language. We shall proceed by following the three steps laid out in the problem statement to derive the final expression for $E[L_{t}^{0}(W)]$.\n\nLet $\\{W_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion starting at $W_{0} = 0$. Let $M_{t} = \\sup_{0 \\leq s \\leq t} W_{s}$ be its running maximum.\n\nStep 1: Derivation of the distribution of $M_{t}$ and its relation to $|W_{t}|$.\nWe first determine the tail distribution of $M_{t}$, which is $P(M_{t} \\geq a)$ for any real number $a$. Since $W_{0}=0$ and the paths of $W$ are continuous, $M_{t} \\geq 0$ almost surely. Thus, for any $a \\leq 0$, $P(M_{t} \\geq a) = 1$.\n\nNow, consider $a  0$. The event $\\{M_{t} \\geq a\\}$ means that the Brownian path has reached or exceeded the level $a$ at some time in the interval $[0, t]$. Let $\\tau_{a} = \\inf\\{s \\geq 0: W_{s} = a\\}$ be the first hitting time of level $a$. Then $\\{M_{t} \\geq a\\} = \\{\\tau_{a} \\leq t\\}$.\nWe use the reflection principle for Brownian motion. We can partition the event $\\{M_{t} \\geq a\\}$ as follows:\n$$P(M_{t} \\geq a) = P(M_{t} \\geq a, W_{t} \\geq a) + P(M_{t} \\geq a, W_{t}  a)$$\nIf $W_{t} \\geq a$, then its maximum over $[0, t]$ must also be at least $a$. Thus, the event $\\{M_{t} \\geq a, W_{t} \\geq a\\}$ is the same as the event $\\{W_{t} \\geq a\\}$. So,\n$$P(M_{t} \\geq a, W_{t} \\geq a) = P(W_{t} \\geq a)$$\nFor the second term, on the event $\\{M_{t} \\geq a, W_{t}  a\\}$, the process must have hit level $a$ at some time $\\tau_{a} \\leq t$ and then finished at a value below $a$. By the strong Markov property of Brownian motion, the process $\\{W_{\\tau_{a}+u} - W_{\\tau_{a}}\\}_{u \\geq 0}$ is a standard Brownian motion independent of the pre-$\\tau_{a}$ filtration. At time $\\tau_a$, we have $W_{\\tau_a}=a$. The condition $W_{t}  a$ on the set $\\{\\tau_a \\le t\\}$ is equivalent to $W_t - W_{\\tau_a}  0$. By the symmetry of Brownian motion, the probability of the process being negative at time $t-\\tau_a$ is the same as the probability of it being positive.\nThus, conditioned on the event $\\{\\tau_{a} \\leq t\\}$, we have $P(W_{t}  a) = P(W_{t}  a)$. This implies:\n$$P(M_{t} \\geq a, W_{t}  a) = P(M_{t} \\geq a, W_{t}  a)$$\nThe event $\\{M_{t} \\geq a, W_{t}  a\\}$ is simply $\\{W_{t}  a\\}$, because if $W_{t}  a$, its maximum $M_{t}$ is certainly greater than $a$. Since $W_{t}$ has a continuous distribution, $P(W_{t}  a) = P(W_{t} \\geq a)$.\nTherefore, $P(M_{t} \\geq a, W_{t}  a) = P(W_{t} \\geq a)$.\nCombining these results, we have for $a  0$:\n$$P(M_{t} \\geq a) = P(W_{t} \\geq a) + P(W_{t} \\geq a) = 2 P(W_{t} \\geq a)$$\nNow, we find the tail distribution of $|W_{t}|$. For $a \\leq 0$, $P(|W_{t}| \\geq a) = 1$, which matches $P(M_{t} \\geq a)$. For $a  0$:\n$$P(|W_{t}| \\geq a) = P(W_{t} \\geq a \\text{ or } W_{t} \\leq -a)$$\nSince these two events are disjoint, we have:\n$$P(|W_{t}| \\geq a) = P(W_{t} \\geq a) + P(W_{t} \\leq -a)$$\nBy the symmetry of the standard Brownian motion, $W_{t}$ has the same distribution as $-W_{t}$. Hence, $P(W_{t} \\leq -a) = P(-W_{t} \\geq a) = P(W_{t} \\geq a)$.\nSubstituting this into the expression for the tail probability of $|W_{t}|$:\n$$P(|W_{t}| \\geq a) = P(W_{t} \\geq a) + P(W_{t} \\geq a) = 2 P(W_{t} \\geq a)$$\nWe have shown that for all $a \\in \\mathbb{R}$, $P(M_{t} \\geq a) = P(|W_{t}| \\geq a)$. This implies that the random variables $M_{t}$ and $|W_{t}|$ have the same distribution, denoted $M_{t} \\stackrel{d}{=} |W_{t}|$.\n\nStep 2: Computation of $E[|W_{t}|]$.\nThe random variable $W_{t}$ follows a normal distribution with mean $0$ and variance $t$, denoted $W_{t} \\sim N(0, t)$. Its probability density function (PDF) is given by:\n$$f_{W_{t}}(x) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right)$$\nThe expected value of $|W_{t}|$ is computed by integrating $|x|$ against this density:\n$$E[|W_{t}|] = \\int_{-\\infty}^{\\infty} |x| \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right) dx$$\nThe integrand is an even function of $x$. We can therefore simplify the integral:\n$$E[|W_{t}|] = 2 \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right) dx = \\frac{2}{\\sqrt{2\\pi t}} \\int_{0}^{\\infty} x \\exp\\left(-\\frac{x^{2}}{2t}\\right) dx$$\nTo evaluate the integral, we perform a substitution. Let $u = \\frac{x^{2}}{2t}$. Then $du = \\frac{2x}{2t} dx = \\frac{x}{t} dx$, which implies $x \\, dx = t \\, du$. The limits of integration remain $0$ to $\\infty$.\n$$E[|W_{t}|] = \\frac{2}{\\sqrt{2\\pi t}} \\int_{0}^{\\infty} \\exp(-u) (t \\, du) = \\frac{2t}{\\sqrt{2\\pi t}} \\int_{0}^{\\infty} \\exp(-u) du$$\nThe standard integral $\\int_{0}^{\\infty} \\exp(-u) du = [-\\exp(-u)]_{0}^{\\infty} = -(0 - 1) = 1$.\nSubstituting this result back, we get:\n$$E[|W_{t}|] = \\frac{2t}{\\sqrt{2\\pi t}} = \\frac{2t}{\\sqrt{2\\pi}\\sqrt{t}} = \\frac{2\\sqrt{t}}{\\sqrt{2\\pi}} = \\sqrt{\\frac{4t}{2\\pi}} = \\sqrt{\\frac{2t}{\\pi}}$$\n\nStep 3: Justification that $E[L_{t}^{0}(W)] = E[|W_{t}|]$.\nWe apply Tanaka's formula to the process $W_{t}$ and the function $f(x) = |x|$. For a level $a=0$ and starting point $W_0=0$, Tanaka's formula states:\n$$|W_{t}| - |W_{0}| = \\int_{0}^{t} \\text{sgn}(W_{s}) dW_{s} + L_{t}^{0}(W)$$\nwhere we use the standard definition of local time that appears in this identity. Since $W_{0}=0$, this simplifies to:\n$$|W_{t}| = \\int_{0}^{t} \\text{sgn}(W_{s}) dW_{s} + L_{t}^{0}(W)$$\nWe take the expectation of both sides of this equation:\n$$E[|W_{t}|] = E\\left[\\int_{0}^{t} \\text{sgn}(W_{s}) dW_{s}\\right] + E[L_{t}^{0}(W)]$$\nLet us analyze the stochastic integral term, $I_{t} = \\int_{0}^{t} \\text{sgn}(W_{s}) dW_{s}$. The integrand is $H_{s} = \\text{sgn}(W_{s})$. This process is adapted to the filtration generated by $W$. To determine if $I_{t}$ is a martingale, we check the It√¥ isometry condition:\n$$E\\left[\\int_{0}^{t} H_{s}^{2} ds\\right] = E\\left[\\int_{0}^{t} (\\text{sgn}(W_{s}))^{2} ds\\right]$$\nThe set of times $\\{s \\in [0,t] : W_{s} = 0\\}$ has Lebesgue measure zero almost surely. Thus, $(\\text{sgn}(W_{s}))^{2} = 1$ for almost every $s \\in [0,t]$.\n$$E\\left[\\int_{0}^{t} 1 \\, ds\\right] = E[t] = t$$\nSince $t  \\infty$, the stochastic integral $I_{t}$ is a true martingale. The martingale starts at $I_{0} = 0$. A fundamental property of martingales is that their expectation is constant over time. Therefore, for all $t \\geq 0$:\n$$E[I_{t}] = E[I_{0}] = 0$$\nHence, $E\\left[\\int_{0}^{t} \\text{sgn}(W_{s}) dW_{s}\\right] = 0$.\nSubstituting this back into the expectation of Tanaka's formula yields:\n$$E[|W_{t}|] = 0 + E[L_{t}^{0}(W)]$$\nThis gives the desired identity: $E[L_{t}^{0}(W)] = E[|W_{t}|]$.\n\nFinal Task: Provide the closed-form expression.\nBy combining the results from Step 2 and Step 3, we obtain the final answer.\nFrom Step 3, $E[L_{t}^{0}(W)] = E[|W_{t}|]$.\nFrom Step 2, $E[|W_{t}|] = \\sqrt{\\frac{2t}{\\pi}}$.\nTherefore, the expected local time of a standard Brownian motion at level $0$ up to time $t$ is:\n$$E[L_{t}^{0}(W)] = \\sqrt{\\frac{2t}{\\pi}}$$", "answer": "$$\\boxed{\\sqrt{\\frac{2t}{\\pi}}}$$", "id": "3064283"}, {"introduction": "Building on the foundational case of standard Brownian motion, this practice challenges you to generalize the method to a wider class of diffusions. You will compute the expected local time for a process described by a linear stochastic differential equation with constant drift $\\mu$ and volatility $\\sigma$ [@problem_id:3064268]. This exercise demonstrates the robustness of using Tanaka's formula and reinforces your ability to analyze semimartingales beyond the simplest examples, a key skill in applied stochastic analysis.", "problem": "Let $\\{W_t\\}_{t \\geq 0}$ be a standard Brownian motion, and consider the linear stochastic differential equation (SDE), which is the It\\^{o} form of a diffusion with constant drift and volatility,\n$$\ndX_t \\;=\\; \\mu \\, dt \\;+\\; \\sigma \\, dW_t, \\qquad X_0 \\;=\\; x_0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma  0$ are constants, and $x_0 \\in \\mathbb{R}$ is the deterministic initial condition. For a fixed level $a \\in \\mathbb{R}$ and a fixed time $t0$, let $L_t^a(X)$ denote the local time accumulated by the continuous semimartingale $X$ at level $a$ up to time $t$.\n\nStarting only from fundamental results for continuous semimartingales (including Tanaka‚Äôs formula for the absolute value of a semimartingale), properties of the It\\^{o} integral, and standard facts about the normal distribution, derive a closed-form analytic expression for the expectation $\\mathbb{E}[L_t^a(X)]$ in terms of $\\mu$, $\\sigma$, $t$, $x_0$, and $a$. You may express your final result using the standard normal probability density function $\\phi$ and cumulative distribution function $\\Phi$, and, if necessary, a single definite integral. Your final answer must be a single analytic expression. No numerical approximation or rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem in stochastic calculus that can be solved using fundamental principles.\n\nWe are tasked with finding the expectation of the local time, $\\mathbb{E}[L_t^a(X)]$, for the process $X_t$ defined by the stochastic differential equation (SDE):\n$$\ndX_t = \\mu \\, dt + \\sigma \\, dW_t\n$$\nwith initial condition $X_0 = x_0$, where $\\mu, \\sigma  0,$ and $x_0$ are constants.\n\nThe solution begins with Tanaka's formula for a continuous semimartingale. For the process $X_t$ and level $a \\in \\mathbb{R}$, Tanaka's formula is given by:\n$$\n|X_t - a| = |X_0 - a| + \\int_0^t \\text{sgn}(X_s - a) \\, dX_s + L_t^a(X)\n$$\nwhere $\\text{sgn}(\\cdot)$ is the sign function, and $L_t^a(X)$ is the local time of $X$ at level $a$ up to time $t$.\n\nSubstituting the given SDE for $dX_s$ and the initial condition $X_0=x_0$, we get:\n$$\n|X_t - a| = |x_0 - a| + \\int_0^t \\text{sgn}(X_s - a) (\\mu \\, ds + \\sigma \\, dW_s) + L_t^a(X)\n$$\nWe can split the integral into a Lebesgue integral part and an It√¥ integral part:\n$$\n|X_t - a| = |x_0 - a| + \\mu \\int_0^t \\text{sgn}(X_s - a) \\, ds + \\sigma \\int_0^t \\text{sgn}(X_s - a) \\, dW_s + L_t^a(X)\n$$\nTo find the expectation $\\mathbb{E}[L_t^a(X)]$, we first rearrange the equation to isolate $L_t^a(X)$:\n$$\nL_t^a(X) = |X_t - a| - |x_0 - a| - \\mu \\int_0^t \\text{sgn}(X_s - a) \\, ds - \\sigma \\int_0^t \\text{sgn}(X_s - a) \\, dW_s\n$$\nNow, we take the expectation of both sides. By the linearity of expectation:\n$$\n\\mathbb{E}[L_t^a(X)] = \\mathbb{E}[|X_t - a|] - |x_0 - a| - \\mu \\, \\mathbb{E}\\left[\\int_0^t \\text{sgn}(X_s - a) \\, ds\\right] - \\sigma \\, \\mathbb{E}\\left[\\int_0^t \\text{sgn}(X_s - a) \\, dW_s\\right]\n$$\nLet us evaluate each term in the expectation.\n\nFirst, consider the It√¥ integral term. The integrand $H_s = \\text{sgn}(X_s - a)$ is an adapted process, and it is bounded, i.e., $|H_s| \\le 1$. The It√¥ isometry condition for the integral to be a martingale with zero expectation is satisfied:\n$$\n\\mathbb{E}\\left[\\int_0^t (\\sigma \\, \\text{sgn}(X_s - a))^2 \\, ds\\right] = \\sigma^2 \\int_0^t \\mathbb{E}[(\\text{sgn}(X_s - a))^2] \\, ds \\le \\sigma^2 \\int_0^t 1 \\, ds = \\sigma^2 t  \\infty\n$$\nTherefore, the It√¥ integral is a martingale starting at $0$, and its expectation is $0$:\n$$\n\\mathbb{E}\\left[\\int_0^t \\text{sgn}(X_s - a) \\, dW_s\\right] = 0\n$$\nNext, for the Lebesgue integral term, we can apply Fubini's theorem since the integrand is bounded:\n$$\n\\mathbb{E}\\left[\\int_0^t \\text{sgn}(X_s - a) \\, ds\\right] = \\int_0^t \\mathbb{E}[\\text{sgn}(X_s - a)] \\, ds\n$$\nThe equation for the expected local time simplifies to:\n$$\n\\mathbb{E}[L_t^a(X)] = \\mathbb{E}[|X_t - a|] - |x_0 - a| - \\mu \\int_0^t \\mathbb{E}[\\text{sgn}(X_s - a)] \\, ds\n$$\nTo proceed, we need the distribution of $X_s$. The solution to the SDE is $X_s = x_0 + \\mu s + \\sigma W_s$. Thus, for any $s  0$, $X_s$ follows a normal distribution $X_s \\sim N(m_s, v_s^2)$, where the mean is $m_s = x_0 + \\mu s$ and the variance is $v_s^2 = \\sigma^2 s$.\n\nLet's compute $\\mathbb{E}[\\text{sgn}(X_s - a)]$. Since $X_s$ is a continuous random variable, $P(X_s=a)=0$.\n$$\n\\mathbb{E}[\\text{sgn}(X_s - a)] = (1) \\cdot P(X_s  a) + (-1) \\cdot P(X_s  a) = P(X_s  a) - P(X_s  a)\n$$\nLet $Z_s = \\frac{X_s - m_s}{v_s}$ be a standard normal variable, $Z_s \\sim N(0,1)$.\n$$\nP(X_s  a) = P\\left(Z_s  \\frac{a - m_s}{v_s}\\right) = 1 - \\Phi\\left(\\frac{a-m_s}{v_s}\\right) = \\Phi\\left(\\frac{m_s-a}{v_s}\\right)\n$$\n$$\nP(X_s  a) = P\\left(Z_s  \\frac{a - m_s}{v_s}\\right) = \\Phi\\left(\\frac{a-m_s}{v_s}\\right) = 1 - \\Phi\\left(\\frac{m_s-a}{v_s}\\right)\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Thus,\n$$\n\\mathbb{E}[\\text{sgn}(X_s - a)] = \\Phi\\left(\\frac{m_s-a}{v_s}\\right) - \\left(1 - \\Phi\\left(\\frac{m_s-a}{v_s}\\right)\\right) = 2\\Phi\\left(\\frac{m_s-a}{v_s}\\right) - 1\n$$\nNow, let's compute $\\mathbb{E}[|X_t - a|]$. Let $Y_t = X_t - a$. $Y_t$ is normally distributed with mean $m'_t = m_t - a = x_0 + \\mu t - a$ and variance $v_t^2 = \\sigma^2 t$. We need to find $\\mathbb{E}[|Y_t|]$. For a general normal random variable $Y \\sim N(m', v^2)$, its expectation is:\n$$\n\\mathbb{E}[|Y|] = \\int_{-\\infty}^{\\infty} |y| \\frac{1}{v\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-m')^2}{2v^2}\\right) dy\n$$\nThis evaluates to a standard result:\n$$\n\\mathbb{E}[|Y|] = 2v \\phi\\left(\\frac{m'}{v}\\right) + m'\\left(2\\Phi\\left(\\frac{m'}{v}\\right) - 1\\right)\n$$\nwhere $\\phi$ is the standard normal probability density function.\nApplying this to $Y_t = X_t - a$, we have:\n$$\n\\mathbb{E}[|X_t - a|] = 2v_t \\phi\\left(\\frac{m_t - a}{v_t}\\right) + (m_t - a)\\left(2\\Phi\\left(\\frac{m_t - a}{v_t}\\right) - 1\\right)\n$$\nSubstituting $m_t = x_0 + \\mu t$ and $v_t = \\sigma\\sqrt{t}$:\n$$\n\\mathbb{E}[|X_t - a|] = 2\\sigma\\sqrt{t} \\phi\\left(\\frac{x_0 + \\mu t - a}{\\sigma\\sqrt{t}}\\right) + (x_0 + \\mu t - a)\\left(2\\Phi\\left(\\frac{x_0 + \\mu t - a}{\\sigma\\sqrt{t}}\\right) - 1\\right)\n$$\nFinally, we assemble all the pieces into the expression for $\\mathbb{E}[L_t^a(X)]$:\n$$\n\\mathbb{E}[L_t^a(X)] = \\mathbb{E}[|X_t - a|] - |x_0 - a| - \\mu \\int_0^t \\mathbb{E}[\\text{sgn}(X_s - a)] \\, ds\n$$\nSubstituting the derived expressions:\n$$\n\\mathbb{E}[L_t^a(X)] = \\left[ 2\\sigma\\sqrt{t} \\phi\\left(\\frac{x_0 + \\mu t - a}{\\sigma\\sqrt{t}}\\right) + (x_0 + \\mu t - a)\\left(2\\Phi\\left(\\frac{x_0 + \\mu t - a}{\\sigma\\sqrt{t}}\\right) - 1\\right) \\right] - |x_0-a| - \\mu \\int_0^t \\left(2\\Phi\\left(\\frac{x_0 + \\mu s - a}{\\sigma\\sqrt{s}}\\right) - 1\\right) \\, ds\n$$\nThis expression is the closed-form analytic solution for the expected local time as requested, given in terms of the problem parameters and standard functions, and including a definite integral as permitted.", "answer": "$$\n\\boxed{2\\sigma\\sqrt{t} \\phi\\left(\\frac{x_0+\\mu t-a}{\\sigma\\sqrt{t}}\\right) + (x_0+\\mu t - a)\\left(2\\Phi\\left(\\frac{x_0+\\mu t-a}{\\sigma\\sqrt{t}}\\right)-1\\right) - |x_0-a| - \\mu \\int_0^t \\left(2\\Phi\\left(\\frac{x_0+\\mu s-a}{\\sigma\\sqrt{s}}\\right) - 1\\right) \\, ds}\n$$", "id": "3064268"}]}