{"hands_on_practices": [{"introduction": "Before applying advanced theorems, we must master the fundamentals. This first practice invites you to compute the matrix-valued quadratic variation for a standard multidimensional Brownian motion, a foundational concept in stochastic calculus. This result, which is the multidimensional analogue of the one-dimensional property $[W]_t = t$, is essential for correctly applying Itô's lemma and understanding the structure of multidimensional SDEs. [@problem_id:3067562]", "problem": "Let $\\{W_{t}\\}_{t \\geq 0} = (W^{1}_{t},\\dots,W^{d}_{t})$ be a $d$-dimensional standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\geq 0},\\mathbb{P})$, where the components $\\{W^{i}_{t}\\}_{t \\geq 0}$, for $i \\in \\{1,\\dots,d\\}$, are independent one-dimensional standard Brownian motions. For a fixed $t \\geq 0$, let $\\pi = \\{0 = t_{0}  t_{1}  \\cdots  t_{n} = t\\}$ be a partition of $[0,t]$ with mesh $\\|\\pi\\| = \\max_{k} (t_{k+1} - t_{k})$. Define the matrix-valued quadratic variation $[W]_{t}$ to be the $d \\times d$ matrix with entries\n$$\n[W]_{t}^{ij} \\equiv \\lim_{\\|\\pi\\|\\to 0} \\sum_{k=0}^{n-1} \\big(W^{i}_{t_{k+1}} - W^{i}_{t_{k}}\\big)\\big(W^{j}_{t_{k+1}} - W^{j}_{t_{k}}\\big),\n$$\nwhere the limit is taken in probability. Using only the defining properties of Brownian motion (independent, stationary Gaussian increments with mean $0$ and variance increment $\\Delta t$), basic moment computations for Gaussian random variables, and Itô calculus for one-dimensional continuous martingales, compute the closed-form expression of $[W]_{t}$ as a function of $t$ and the identity matrix $I_{d}$. Your final answer must be a single analytic expression.", "solution": "The problem is valid. It asks for the computation of the matrix-valued quadratic variation of a $d$-dimensional standard Brownian motion, which is a fundamental and well-posed problem in stochastic calculus. The constraints on the methodology are clear and sufficient to derive the result.\n\nLet $\\{W_{t}\\}_{t \\geq 0} = (W^{1}_{t},\\dots,W^{d}_{t})$ be a $d$-dimensional standard Brownian motion. The components $W^{i}$ and $W^{j}$ are independent standard one-dimensional Brownian motions for $i \\neq j$. The entries of the quadratic variation matrix $[W]_{t}$ are defined as the limit in probability:\n$$[W]_{t}^{ij} = \\lim_{\\|\\pi\\|\\to 0} \\sum_{k=0}^{n-1} \\big(W^{i}_{t_{k+1}} - W^{i}_{t_{k}}\\big)\\big(W^{j}_{t_{k+1}} - W^{j}_{t_{k}}\\big)$$\nwhere $\\pi = \\{0 = t_{0}  t_{1}  \\cdots  t_{n} = t\\}$ is a partition of the interval $[0,t]$. Let us denote the discrete sum for a given partition $\\pi$ by $S_{\\pi}^{ij}$.\n$$S_{\\pi}^{ij} = \\sum_{k=0}^{n-1} \\big(W^{i}_{t_{k+1}} - W^{i}_{t_{k}}\\big)\\big(W^{j}_{t_{k+1}} - W^{j}_{t_{k}}\\big)$$\nWe will compute the limit of $S_{\\pi}^{ij}$ by analyzing its mean and variance. Convergence in $L^2$ (mean square) implies convergence in probability, so we will show that $S_{\\pi}^{ij}$ converges in $L^2$ as the mesh of the partition $\\|\\pi\\| = \\max_{k} (t_{k+1} - t_{k})$ approaches $0$.\n\nLet $\\Delta W_{k}^{i} = W^{i}_{t_{k+1}} - W^{i}_{t_{k}}$ and $\\Delta t_{k} = t_{k+1} - t_{k}$. By the definition of standard Brownian motion, the increment $\\Delta W_{k}^{i}$ follows a Gaussian distribution with mean $0$ and variance $\\Delta t_{k}$, i.e., $\\Delta W_{k}^{i} \\sim \\mathcal{N}(0, \\Delta t_{k})$. The increments over disjoint time intervals are independent.\n\nWe consider two cases for the indices $i$ and $j$.\n\nCase 1: $i = j$ (Diagonal entries)\nIn this case, we compute $[W]_{t}^{ii}$. The sum is $S_{\\pi}^{ii} = \\sum_{k=0}^{n-1} (\\Delta W_{k}^{i})^{2}$.\nFirst, we compute the expectation of $S_{\\pi}^{ii}$. By linearity of expectation:\n$$\\mathbb{E}[S_{\\pi}^{ii}] = \\mathbb{E}\\left[\\sum_{k=0}^{n-1} (\\Delta W_{k}^{i})^{2}\\right] = \\sum_{k=0}^{n-1} \\mathbb{E}[(\\Delta W_{k}^{i})^{2}]$$\nThe second moment $\\mathbb{E}[(\\Delta W_{k}^{i})^{2}]$ is the variance of $\\Delta W_{k}^{i}$ since its mean is $0$.\n$$\\mathbb{E}[(\\Delta W_{k}^{i})^{2}] = \\text{Var}(\\Delta W_{k}^{i}) = \\Delta t_{k}$$\nSubstituting this back, we get:\n$$\\mathbb{E}[S_{\\pi}^{ii}] = \\sum_{k=0}^{n-1} \\Delta t_{k} = \\sum_{k=0}^{n-1} (t_{k+1} - t_{k}) = t_{n} - t_{0} = t$$\nNext, we compute the variance of $S_{\\pi}^{ii}$. Since the increments $\\Delta W_{k}^{i}$ over disjoint intervals $[t_{k}, t_{k+1}]$ are independent, the random variables $(\\Delta W_{k}^{i})^{2}$ are also independent. Therefore, the variance of the sum is the sum of the variances:\n$$\\text{Var}(S_{\\pi}^{ii}) = \\text{Var}\\left(\\sum_{k=0}^{n-1} (\\Delta W_{k}^{i})^{2}\\right) = \\sum_{k=0}^{n-1} \\text{Var}((\\Delta W_{k}^{i})^{2})$$\nFor a random variable $X \\sim \\mathcal{N}(0, \\sigma^2)$, its fourth moment is $\\mathbb{E}[X^4] = 3(\\sigma^2)^2$. The variance of $X^2$ is then:\n$$\\text{Var}(X^2) = \\mathbb{E}[(X^2)^2] - (\\mathbb{E}[X^2])^2 = \\mathbb{E}[X^4] - (\\sigma^2)^2 = 3(\\sigma^2)^2 - (\\sigma^2)^2 = 2(\\sigma^2)^2$$\nApplying this to $\\Delta W_{k}^{i} \\sim \\mathcal{N}(0, \\Delta t_{k})$, we find:\n$$\\text{Var}((\\Delta W_{k}^{i})^{2}) = 2(\\Delta t_{k})^2$$\nThe variance of $S_{\\pi}^{ii}$ is thus:\n$$\\text{Var}(S_{\\pi}^{ii}) = \\sum_{k=0}^{n-1} 2(\\Delta t_{k})^2 = 2\\sum_{k=0}^{n-1} (\\Delta t_{k})^2$$\nAs $\\|\\pi\\| \\to 0$, this variance must go to $0$. We can bound the sum:\n$$\\sum_{k=0}^{n-1} (\\Delta t_{k})^2 \\leq \\left(\\max_{k} \\Delta t_{k}\\right) \\sum_{k=0}^{n-1} \\Delta t_{k} = \\|\\pi\\| \\cdot t$$\nSo, $\\text{Var}(S_{\\pi}^{ii}) \\leq 2t\\|\\pi\\|$, which tends to $0$ as $\\|\\pi\\| \\to 0$.\nSince $\\mathbb{E}[S_{\\pi}^{ii}] = t$ and $\\text{Var}(S_{\\pi}^{ii}) \\to 0$, $S_{\\pi}^{ii}$ converges to $t$ in $L^2$, and therefore in probability.\n$$[W]_{t}^{ii} = t$$\n\nCase 2: $i \\neq j$ (Off-diagonal entries)\nIn this case, we compute $[W]_{t}^{ij}$. The sum is $S_{\\pi}^{ij} = \\sum_{k=0}^{n-1} \\Delta W_{k}^{i} \\Delta W_{k}^{j}$.\nWe compute the expectation of $S_{\\pi}^{ij}$:\n$$\\mathbb{E}[S_{\\pi}^{ij}] = \\mathbb{E}\\left[\\sum_{k=0}^{n-1} \\Delta W_{k}^{i} \\Delta W_{k}^{j}\\right] = \\sum_{k=0}^{n-1} \\mathbb{E}[\\Delta W_{k}^{i} \\Delta W_{k}^{j}]$$\nSince $i \\neq j$, the Brownian motions $W^i$ and $W^j$ are independent. This implies that their increments over any time interval, $\\Delta W_{k}^{i}$ and $\\Delta W_{k}^{j}$, are independent random variables. Thus, the expectation of their product is the product of their expectations:\n$$\\mathbb{E}[\\Delta W_{k}^{i} \\Delta W_{k}^{j}] = \\mathbb{E}[\\Delta W_{k}^{i}] \\mathbb{E}[\\Delta W_{k}^{j}] = 0 \\cdot 0 = 0$$\nTherefore, the expectation of the sum is zero:\n$$\\mathbb{E}[S_{\\pi}^{ij}] = \\sum_{k=0}^{n-1} 0 = 0$$\nNow, we compute the variance of $S_{\\pi}^{ij}$. Since the increments are independent across disjoint intervals, the terms $\\Delta W_{k}^{i} \\Delta W_{k}^{j}$ are independent for different values of $k$.\n$$\\text{Var}(S_{\\pi}^{ij}) = \\sum_{k=0}^{n-1} \\text{Var}(\\Delta W_{k}^{i} \\Delta W_{k}^{j})$$\nSince the mean of $\\Delta W_{k}^{i} \\Delta W_{k}^{j}$ is $0$, its variance is its second moment:\n$$\\text{Var}(\\Delta W_{k}^{i} \\Delta W_{k}^{j}) = \\mathbb{E}[(\\Delta W_{k}^{i} \\Delta W_{k}^{j})^2] - (\\mathbb{E}[\\Delta W_{k}^{i} \\Delta W_{k}^{j}])^2 = \\mathbb{E}[(\\Delta W_{k}^{i})^2 (\\Delta W_{k}^{j})^2] - 0$$\nDue to the independence of $\\Delta W_{k}^{i}$ and $\\Delta W_{k}^{j}$:\n$$\\mathbb{E}[(\\Delta W_{k}^{i})^2 (\\Delta W_{k}^{j})^2] = \\mathbb{E}[(\\Delta W_{k}^{i})^2] \\mathbb{E}[(\\Delta W_{k}^{j})^2]$$\nWe already know that $\\mathbb{E}[(\\Delta W_{k}^{i})^2] = \\Delta t_{k}$ and $\\mathbb{E}[(\\Delta W_{k}^{j})^2] = \\Delta t_{k}$.\nSo, $\\text{Var}(\\Delta W_{k}^{i} \\Delta W_{k}^{j}) = (\\Delta t_{k})(\\Delta t_{k}) = (\\Delta t_{k})^2$.\nThe variance of $S_{\\pi}^{ij}$ is:\n$$\\text{Var}(S_{\\pi}^{ij}) = \\sum_{k=0}^{n-1} (\\Delta t_{k})^2$$\nAs shown in Case 1, this sum goes to $0$ as $\\|\\pi\\| \\to 0$.\nSince $\\mathbb{E}[S_{\\pi}^{ij}] = 0$ and $\\text{Var}(S_{\\pi}^{ij}) \\to 0$, $S_{\\pi}^{ij}$ converges to $0$ in $L^2$ and therefore in probability.\n$$[W]_{t}^{ij} = 0 \\quad \\text{for } i \\neq j$$\n\nCombining the results, the entries of the matrix $[W]_{t}$ are given by:\n$$[W]_{t}^{ij} = \\begin{cases} t  \\text{if } i=j \\\\ 0  \\text{if } i\\neq j \\end{cases}$$\nThis can be written using the Kronecker delta as $[W]_{t}^{ij} = t \\delta_{ij}$. This is the definition of a scalar multiple of the identity matrix. Therefore, the matrix $[W]_{t}$ is:\n$$[W]_{t} = t I_{d}$$\nwhere $I_d$ is the $d \\times d$ identity matrix.", "answer": "$$\\boxed{t I_{d}}$$", "id": "3067562"}, {"introduction": "Girsanov's theorem provides a powerful tool for changing probability measures, but this change must be mathematically valid. Novikov's condition is a crucial check that guarantees the associated density process is a true martingale, making the change of measure well-defined. This exercise moves from the abstract to the concrete, asking you to verify the condition for a specific process and solidifying your understanding of this theoretical prerequisite. [@problem_id:3067604]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ satisfying the usual conditions and supporting a three-dimensional standard Brownian motion (BM) $(W_{t})_{t\\geq 0}$ adapted to $(\\mathcal{F}_{t})_{t\\geq 0}$. Let $T=2$. Construct a bounded predictable process $\\theta=(\\theta_{t})_{t\\in[0,T]}$ with values in $\\mathbb{R}^{3}$ by setting\n$$\n\\theta_{t}=\\begin{cases}\n(1,-2,3)  \\text{for } t\\in[0,1],\\\\\n(0,1,-1)  \\text{for } t\\in(1,2].\n\\end{cases}\n$$\nUsing the definition of predictability and boundedness for stochastic processes, verify that $\\theta$ is predictable and bounded. Then, starting only from the definitions of expectation, Euclidean norm, and Lebesgue integration on $[0,T]$, compute the quantity\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right],\n$$\nand conclude whether Novikov's condition for the multidimensional Girsanov theorem holds on $[0,T]$ for this choice of $\\theta$. Express your final answer as an exact analytic expression; no rounding is required.", "solution": "The problem asks for three things: first, to verify that the given process $\\theta = (\\theta_t)_{t \\in [0,T]}$ is predictable and bounded; second, to compute the value of a specific expectation involving $\\theta$; and third, to use this result to determine if Novikov's condition is met. We will address each part in sequence. The time horizon is $T=2$.\n\nFirst, we verify the properties of the process $\\theta_t$. The process is defined as:\n$$\n\\theta_{t}=\\begin{cases}\n(1,-2,3)  \\text{for } t\\in[0,1],\\\\\n(0,1,-1)  \\text{for } t\\in(1,2].\n\\end{cases}\n$$\nThis is a process with values in $\\mathbb{R}^3$.\n\n**Verification of Predictability and Boundedness**\n\nA process is predictable if it is measurable with respect to the predictable $\\sigma$-algebra on $[0,T] \\times \\Omega$. A sufficient condition for a process to be predictable is that it is adapted to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$ and its sample paths are left-continuous.\n\n1.  **Adaptedness**: The process $\\theta_t$ is a deterministic function of time $t$. For any given $t \\in [0,T]$, $\\theta_t$ is a constant vector. A constant random variable is measurable with respect to any $\\sigma$-algebra. Therefore, for each $t \\ge 0$, $\\theta_t$ is $\\mathcal{F}_t$-measurable. Thus, the process $(\\theta_t)_{t \\in [0,T]}$ is adapted to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$.\n\n2.  **Left-Continuity**: We examine the sample paths $t \\mapsto \\theta_t$. Since the process is deterministic, we only need to check the function $t \\mapsto \\theta_t$ for left-continuity on the interval $(0,T]$. The function is constant on the intervals $[0,1]$ and $(1,2]$, so it is continuous at all points $t \\in (0,2]$ except possibly at $t=1$. We check the left limit at $t=1$:\n    $$ \\lim_{s \\to 1^{-}} \\theta_s = \\lim_{s \\to 1^{-}} (1, -2, 3) = (1, -2, 3) $$\n    The value of the process at $t=1$ is $\\theta_1 = (1, -2, 3)$. Since $\\lim_{s \\to 1^{-}} \\theta_s = \\theta_1$, the process is left-continuous at $t=1$. It is continuous (and thus left-continuous) at all other points in $(0,2]$. Therefore, the process $\\theta_t$ is left-continuous.\n\nSince $\\theta_t$ is both adapted and left-continuous, it is a predictable process.\n\n3.  **Boundedness**: A process $(\\theta_t)_{t \\in [0,T]}$ is bounded if there exists a constant $M  0$ such that $\\|\\theta_t\\| \\le M$ for all $t \\in [0,T]$. We compute the Euclidean norm $\\|\\theta_t\\|$ for $t \\in [0,2]$.\n    For $t \\in [0,1]$, the vector is $\\theta_t = (1, -2, 3)$. Its squared norm is $\\|\\theta_t\\|^2 = 1^2 + (-2)^2 + 3^2 = 1 + 4 + 9 = 14$. Thus, $\\|\\theta_t\\| = \\sqrt{14}$.\n    For $t \\in (1,2]$, the vector is $\\theta_t = (0, 1, -1)$. Its squared norm is $\\|\\theta_t\\|^2 = 0^2 + 1^2 + (-1)^2 = 0 + 1 + 1 = 2$. Thus, $\\|\\theta_t\\| = \\sqrt{2}$.\n\n    The set of values for $\\|\\theta_t\\|$ over the interval $[0,2]$ is $\\{\\sqrt{2}, \\sqrt{14}\\}$. The maximum value is $\\sqrt{14}$. We can choose $M = \\sqrt{14}$. For all $t \\in [0,2]$, we have $\\|\\theta_t\\| \\le \\sqrt{14}$. Hence, the process $\\theta_t$ is bounded.\n\n**Computation of the Expectation**\n\nWe are asked to compute $E^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right]$, with $T=2$. Let's first evaluate the integral inside the exponential.\nThe integrand is $\\|\\theta_s\\|^2$. Since $\\theta_s$ is a deterministic process, so is $\\|\\theta_s\\|^2$. The integral is a deterministic quantity, not a random variable.\nUsing the definition of the Lebesgue integral, we can split the domain of integration $[0,2]$ into $[0,1]$ and $(1,2]$:\n$$\n\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds = \\int_{0}^{1}\\|\\theta_{s}\\|^{2}\\,ds + \\int_{1}^{2}\\|\\theta_{s}\\|^{2}\\,ds\n$$\nOn the interval $s \\in [0,1]$, we have $\\|\\theta_s\\|^2 = 14$.\nOn the interval $s \\in (1,2]$, we have $\\|\\theta_s\\|^2 = 2$.\nThe integral is therefore:\n$$\n\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds = \\int_{0}^{1} 14 \\,ds + \\int_{1}^{2} 2 \\,ds = 14 \\cdot (1-0) + 2 \\cdot (2-1) = 14 + 2 = 16\n$$\nNow, we substitute this value back into the expression for the expectation:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = E^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2} \\cdot 16\\right)\\right] = E^{\\mathbb{P}}[\\exp(8)]\n$$\nThe expression inside the expectation, $\\exp(8)$, is a constant. The expectation of a constant random variable is the constant itself. Formally, if $C$ is a constant, then by definition of expectation:\n$$\nE^{\\mathbb{P}}[C] = \\int_{\\Omega} C \\,d\\mathbb{P}(\\omega) = C \\int_{\\Omega} d\\mathbb{P}(\\omega) = C \\cdot \\mathbb{P}(\\Omega) = C \\cdot 1 = C\n$$\nIn our case, the constant is $\\exp(8)$. Therefore,\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = \\exp(8)\n$$\n\n**Conclusion on Novikov's Condition**\n\nNovikov's condition for the predictable process $\\theta$ on the time interval $[0,T]$ is the requirement that the following expectation is finite:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right]  \\infty\n$$\nThis condition is a sufficient condition to ensure that the Doléans-Dade exponential $Z_t = \\exp\\left(\\int_0^t \\theta_s \\cdot dW_s - \\frac{1}{2}\\int_0^t \\|\\theta_s\\|^2 ds\\right)$ is a martingale on $[0,T]$.\n\nFrom our calculation above, with $T=2$, we found:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = \\exp(8)\n$$\nSince $\\exp(8)$ is a positive, finite real number, we have $\\exp(8)  \\infty$.\nTherefore, Novikov's condition holds for the given process $\\theta$ on the interval $[0,2]$.", "answer": "$$\\boxed{\\exp(8)}$$", "id": "3067604"}, {"introduction": "With the foundational concepts in place, we can now put the Girsanov theorem to its primary use: transforming the drift of a stochastic process. In this practice, you will take a given SDE and a specified change of measure and derive the process's dynamics under the new probability measure. This exercise demonstrates the core utility of the theorem in a direct, computational manner, revealing how a change of perspective alters the deterministic evolution of a system while leaving the random component's structure intact. [@problem_id:3067598]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\in [0,T]},\\mathbb{P})$ supporting a two-dimensional standard Brownian motion $(W_t)_{t \\in [0,T]}$ with $W_t=\\big(W_t^{(1)},W_t^{(2)}\\big)$. Let $X_t \\in \\mathbb{R}^2$ solve the linear stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t \\;=\\; A\\,X_t\\,\\mathrm{d}t \\;+\\; \\Sigma\\,\\mathrm{d}W_t,\\qquad X_0=x\\in\\mathbb{R}^2,\n$$\nwhere\n$$\nA \\;=\\; \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}, \\qquad \\Sigma \\;=\\; \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix},\n$$\nwith $a\\in\\mathbb{R}$ and $\\sigma0$. Let $\\theta=(\\theta_t)_{t\\in[0,T]}$ be an $\\mathbb{R}^2$-valued progressively measurable process with components $\\theta_t=\\big(\\theta_t^{(1)},\\theta_t^{(2)}\\big)$ satisfying the Novikov condition\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_0^T \\|\\theta_s\\|^2\\,\\mathrm{d}s\\right)\\right] \\;\\;\\infty.\n$$\nDefine a new probability measure $\\mathbb{Q}$ on $(\\Omega,\\mathcal{F}_T)$ by the Radon–Nikodym derivative\n$$\n\\frac{\\mathrm{d}\\mathbb{Q}}{\\mathrm{d}\\mathbb{P}}\\Big|_{\\mathcal{F}_T}\n\\;=\\;\n\\exp\\!\\left(\\,-\\int_0^T \\theta_s^\\top\\,\\mathrm{d}W_s \\;-\\; \\frac{1}{2}\\int_0^T \\|\\theta_s\\|^2\\,\\mathrm{d}s\\right).\n$$\nUsing only the foundational definition of the change of measure via the stochastic exponential and the multidimensional version of the Girsanov theorem (which asserts that $W_t^{\\mathbb{Q}}:=W_t+\\int_0^t \\theta_s\\,\\mathrm{d}s$ is a two-dimensional Brownian motion under $\\mathbb{Q}$), write the SDE for $X_t$ under $\\mathbb{Q}$ in terms of the $\\mathbb{Q}$-Brownian motion $W^{\\mathbb{Q}}$, and determine the drift vector under $\\mathbb{Q}$ as an explicit function of $a$, $\\sigma$, $X_t$, and $\\theta_t$. Your final answer should be a single closed-form analytic expression for the drift vector, written as a $1\\times 2$ row using the $\\mathrm{pmatrix}$ environment. No numerical rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard application of the multidimensional Girsanov theorem for changing the drift of a linear stochastic differential equation (SDE). All necessary components are provided and are mathematically consistent.\n\nThe process $X_t \\in \\mathbb{R}^2$ is governed by the following SDE under the probability measure $\\mathbb{P}$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t\n$$\nwhere $W_t = \\big(W_t^{(1)}, W_t^{(2)}\\big)^\\top$ is a two-dimensional standard Brownian motion under $\\mathbb{P}$. The matrices $A$ and $\\Sigma$ are given by:\n$$\nA = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}, \\qquad \\Sigma = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe problem defines a new probability measure $\\mathbb{Q}$ and states that, by Girsanov's theorem, the process $W_t^{\\mathbb{Q}}$ defined as\n$$\nW_t^{\\mathbb{Q}} = W_t + \\int_0^t \\theta_s\\,\\mathrm{d}s\n$$\nis a standard two-dimensional Brownian motion under $\\mathbb{Q}$. Here, $\\theta_t = \\big(\\theta_t^{(1)}, \\theta_t^{(2)}\\big)^\\top$ is a progressively measurable process satisfying the Novikov condition.\n\nOur objective is to find the SDE for $X_t$ under the new measure $\\mathbb{Q}$, which means expressing it in terms of the $\\mathbb{Q}$-Brownian motion $W_t^{\\mathbb{Q}}$. To do this, we first express the $\\mathbb{P}$-Brownian motion $W_t$ in terms of $W_t^{\\mathbb{Q}}$. From the definition of $W_t^{\\mathbb{Q}}$, we can write its differential form:\n$$\n\\mathrm{d}W_t^{\\mathbb{Q}} = \\mathrm{d}W_t + \\theta_t\\,\\mathrm{d}t\n$$\nRearranging this equation to solve for $\\mathrm{d}W_t$, we get:\n$$\n\\mathrm{d}W_t = \\mathrm{d}W_t^{\\mathbb{Q}} - \\theta_t\\,\\mathrm{d}t\n$$\nNow, we substitute this expression for $\\mathrm{d}W_t$ into the original SDE for $X_t$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t + \\Sigma\\,(\\mathrm{d}W_t^{\\mathbb{Q}} - \\theta_t\\,\\mathrm{d}t)\n$$\nTo identify the drift and diffusion terms under $\\mathbb{Q}$, we group the terms containing $\\mathrm{d}t$ and $\\mathrm{d}W_t^{\\mathbb{Q}}$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t - \\Sigma\\,\\theta_t\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t^{\\mathbb{Q}}\n$$\n$$\n\\mathrm{d}X_t = (A\\,X_t - \\Sigma\\,\\theta_t)\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t^{\\mathbb{Q}}\n$$\nThis is the SDE for $X_t$ under the measure $\\mathbb{Q}$. The drift vector under $\\mathbb{Q}$ is the coefficient of the $\\mathrm{d}t$ term. Let us denote this drift vector by $\\mu_t^{\\mathbb{Q}}$.\n$$\n\\mu_t^{\\mathbb{Q}} = A\\,X_t - \\Sigma\\,\\theta_t\n$$\nThe problem requires this drift vector to be expressed as an explicit function of $a$, $\\sigma$, $X_t$, and $\\theta_t$. Let $X_t = \\begin{pmatrix} X_t^{(1)} \\\\ X_t^{(2)} \\end{pmatrix}$ and $\\theta_t = \\begin{pmatrix} \\theta_t^{(1)} \\\\ \\theta_t^{(2)} \\end{pmatrix}$. We can now compute the vector $\\mu_t^{\\mathbb{Q}}$ by performing the matrix-vector multiplications:\n$$\nA\\,X_t = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} X_t^{(1)} \\\\ X_t^{(2)} \\end{pmatrix} = \\begin{pmatrix} a\\,X_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Sigma\\,\\theta_t = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} \\theta_t^{(1)} \\\\ \\theta_t^{(2)} \\end{pmatrix} = \\begin{pmatrix} \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\nSubstituting these results back into the expression for $\\mu_t^{\\mathbb{Q}}$:\n$$\n\\mu_t^{\\mathbb{Q}} = \\begin{pmatrix} a\\,X_t^{(1)} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} a\\,X_t^{(1)} - \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\nThis is the drift vector, which is a $2 \\times 1$ column vector. The problem asks for the final answer to be written as a $1 \\times 2$ row vector. Therefore, we take the transpose of $\\mu_t^{\\mathbb{Q}}$:\n$$\n(\\mu_t^{\\mathbb{Q}})^\\top = \\begin{pmatrix} a\\,X_t^{(1)} - \\sigma\\,\\theta_t^{(1)}  0 \\end{pmatrix}\n$$\nThe first component of the drift is $a\\,X_t^{(1)} - \\sigma\\,\\theta_t^{(1)}$, and the second component is $0$. This reflects the fact that the dynamics of the second component of $X_t$, i.e., $X_t^{(2)}$, are trivial ($\\mathrm{d}X_t^{(2)}=0$) and unaffected by the change of measure.", "answer": "$$\n\\boxed{\\begin{pmatrix} a X_t^{(1)} - \\sigma \\theta_t^{(1)}  0 \\end{pmatrix}}\n$$", "id": "3067598"}]}