{"hands_on_practices": [{"introduction": "This first exercise provides a foundational check of Novikov's condition in a clear and manageable setting. By considering a process driven by a deterministic integrand, you will see how the condition simplifies and holds trivially, confirming that the associated stochastic exponential is a true martingale. This practice reinforces the core definitions and provides a direct calculation to confirm the fundamental property that the expectation of this martingale is one, i.e., $E[\\mathcal{E}(M)_T] = 1$ [@problem_id:3068882].", "problem": "Let $\\{W_t\\}_{t \\in [0,\\pi]}$ be a standard Brownian motion on a filtered probability space. Consider the deterministic bounded function $u_t = \\lambda \\cos(t)$ for a fixed real constant $\\lambda \\in \\mathbb{R}$, and define the continuous local martingale $M_t = \\int_{0}^{t} u_s \\,\\mathrm{d}W_s$ for $t \\in [0,\\pi]$. Let $\\mathcal{E}(M)_t$ denote the stochastic exponential (also called the Doléans–Dade exponential) of $M_t$.\n\nUsing only the foundational definitions and properties of the Itô integral and Brownian motion, and standard facts about Gaussian random variables, perform the following:\n\n1. Argue from first principles why Novikov's condition holds trivially for this choice of deterministic bounded integrand $u_t$ on the finite time horizon $[0,\\pi]$.\n2. Compute the exact value of the expectation $E\\!\\left[\\mathcal{E}(M)_{\\pi}\\right]$.\n\nYour final answer must be a single exact real number. No rounding is required.", "solution": "The user-provided problem is assessed as valid as it is scientifically grounded in the theory of stochastic calculus, well-posed, objective, and internally consistent. We may therefore proceed with a full solution.\n\nThe problem asks for two items: first, an argument for why Novikov's condition holds trivially, and second, the computation of the expectation $E[\\mathcal{E}(M)_{\\pi}]$.\n\nLet $\\{W_t\\}_{t \\in [0,\\pi]}$ be a standard Brownian motion. We are given the deterministic function $u_t = \\lambda \\cos(t)$ for $\\lambda \\in \\mathbb{R}$, and the continuous local martingale $M_t = \\int_{0}^{t} u_s \\,\\mathrm{d}W_s$.\n\n**1. Validation of Novikov's Condition**\n\nNovikov's condition provides a sufficient criterion for the stochastic exponential $\\mathcal{E}(M)_t$ of a continuous local martingale $M_t$ to be a true martingale. The condition states that if\n$$\nE\\left[ \\exp\\left( \\frac{1}{2} \\langle M \\rangle_T \\right) \\right]  \\infty\n$$\nfor a given time horizon $T$, then $\\{\\mathcal{E}(M)_t\\}_{t \\in [0,T]}$ is a martingale.\n\nFirst, we must compute the quadratic variation process, $\\langle M \\rangle_t$, for $M_t = \\int_0^t u_s \\,\\mathrm{d}W_s$. By the properties of the Itô integral with a deterministic integrand, the quadratic variation is given by:\n$$\n\\langle M \\rangle_t = \\left\\langle \\int_0^t u_s \\,\\mathrm{d}W_s \\right\\rangle_t = \\int_0^t u_s^2 \\,\\mathrm{d}s\n$$\nSubstituting $u_s = \\lambda \\cos(s)$, we have:\n$$\n\\langle M \\rangle_t = \\int_0^t (\\lambda \\cos(s))^2 \\,\\mathrm{d}s = \\lambda^2 \\int_0^t \\cos^2(s) \\,\\mathrm{d}s\n$$\nWe need to check the condition at the terminal time $T = \\pi$. Thus, we compute $\\langle M \\rangle_{\\pi}$:\n$$\n\\langle M \\rangle_{\\pi} = \\lambda^2 \\int_0^{\\pi} \\cos^2(s) \\,\\mathrm{d}s\n$$\nUsing the trigonometric identity $\\cos^2(s) = \\frac{1 + \\cos(2s)}{2}$, the integral becomes:\n$$\n\\int_0^{\\pi} \\cos^2(s) \\,\\mathrm{d}s = \\int_0^{\\pi} \\frac{1 + \\cos(2s)}{2} \\,\\mathrm{d}s = \\frac{1}{2} \\left[ s + \\frac{\\sin(2s)}{2} \\right]_0^{\\pi} = \\frac{1}{2} \\left[ (\\pi + \\frac{\\sin(2\\pi)}{2}) - (0 + \\frac{\\sin(0)}{2}) \\right] = \\frac{\\pi}{2}\n$$\nTherefore, the quadratic variation at time $\\pi$ is:\n$$\n\\langle M \\rangle_{\\pi} = \\frac{\\lambda^2 \\pi}{2}\n$$\nNow we can evaluate the expectation in Novikov's condition:\n$$\nE\\left[ \\exp\\left( \\frac{1}{2} \\langle M \\rangle_{\\pi} \\right) \\right] = E\\left[ \\exp\\left( \\frac{1}{2} \\cdot \\frac{\\lambda^2 \\pi}{2} \\right) \\right] = E\\left[ \\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right) \\right]\n$$\nThe crucial point here is that the integrand $u_t$ is deterministic. This implies that the quadratic variation $\\langle M \\rangle_t$ is also a deterministic function of time. Consequently, $\\langle M \\rangle_{\\pi}$ is not a random variable but a fixed constant value, $\\frac{\\lambda^2 \\pi}{2}$. The expectation of a constant is the constant itself:\n$$\nE\\left[ \\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right) \\right] = \\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right)\n$$\nFor any real number $\\lambda$, the value $\\frac{\\lambda^2 \\pi}{4}$ is a non-negative real number. The exponential of any real number is a finite positive number. Thus,\n$$\n\\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right)  \\infty\n$$\nNovikov's condition holds trivially because the expression inside the expectation is a constant, not a random variable, making the verification of finiteness immediate.\n\n**2. Computation of $E[\\mathcal{E}(M)_{\\pi}]$**\n\nThe stochastic exponential (or Doléans–Dade exponential) of $M_t$ is defined as:\n$$\n\\mathcal{E}(M)_t = \\exp\\left( M_t - \\frac{1}{2} \\langle M \\rangle_t \\right)\n$$\nWe wish to compute the expectation $E[\\mathcal{E}(M)_{\\pi}]$. We have:\n$$\nE[\\mathcal{E}(M)_{\\pi}] = E\\left[ \\exp\\left( M_{\\pi} - \\frac{1}{2} \\langle M \\rangle_{\\pi} \\right) \\right]\n$$\nSince we have established that $\\langle M \\rangle_{\\pi} = \\frac{\\lambda^2 \\pi}{2}$ is a deterministic constant, we can factor the exponential term involving it outside the expectation:\n$$\nE[\\mathcal{E}(M)_{\\pi}] = \\exp\\left( -\\frac{1}{2} \\langle M \\rangle_{\\pi} \\right) E\\left[ \\exp(M_{\\pi}) \\right] = \\exp\\left( -\\frac{\\lambda^2 \\pi}{4} \\right) E\\left[ \\exp(M_{\\pi}) \\right]\n$$\nTo proceed, we must determine the distribution of the random variable $M_{\\pi}$. The process $M_{\\pi}$ is an Itô integral with a deterministic integrand:\n$$\nM_{\\pi} = \\int_0^{\\pi} u_s \\,\\mathrm{d}W_s = \\int_0^{\\pi} \\lambda \\cos(s) \\,\\mathrm{d}W_s\n$$\nA fundamental property of the Itô integral is that for a deterministic, square-integrable integrand $f(t)$, the integral $\\int_0^T f(t) \\,\\mathrm{d}W_t$ is a Gaussian (normally distributed) random variable. The mean of this random variable is zero:\n$$\nE[M_{\\pi}] = E\\left[ \\int_0^{\\pi} \\lambda \\cos(s) \\,\\mathrm{d}W_s \\right] = 0\n$$\nThe variance is given by the Itô isometry:\n$$\n\\text{Var}(M_{\\pi}) = E[M_{\\pi}^2] = E\\left[ \\left(\\int_0^{\\pi} \\lambda \\cos(s) \\,\\mathrm{d}W_s\\right)^2 \\right] = \\int_0^{\\pi} (\\lambda \\cos(s))^2 \\,\\mathrm{d}s = \\langle M \\rangle_{\\pi} = \\frac{\\lambda^2 \\pi}{2}\n$$\nSo, the random variable $M_{\\pi}$ has a normal distribution $M_{\\pi} \\sim N\\left(0, \\frac{\\lambda^2 \\pi}{2}\\right)$.\n\nNext, we compute $E[\\exp(M_{\\pi})]$. This is the value of the moment-generating function (MGF) of $M_{\\pi}$ evaluated at $t=1$. For a general normal random variable $X \\sim N(\\mu, \\sigma^2)$, the MGF is $M_X(t) = E[\\exp(tX)] = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nFor $M_{\\pi}$, we have $\\mu=0$ and $\\sigma^2 = \\frac{\\lambda^2 \\pi}{2}$. Evaluating its MGF at $t=1$:\n$$\nE[\\exp(M_{\\pi})] = \\exp\\left( 0 \\cdot 1 + \\frac{1}{2} \\left(\\frac{\\lambda^2 \\pi}{2}\\right) 1^2 \\right) = \\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right)\n$$\nFinally, we substitute this result back into our expression for $E[\\mathcal{E}(M)_{\\pi}]$:\n$$\nE[\\mathcal{E}(M)_{\\pi}] = \\exp\\left( -\\frac{\\lambda^2 \\pi}{4} \\right) E\\left[ \\exp(M_{\\pi}) \\right] = \\exp\\left( -\\frac{\\lambda^2 \\pi}{4} \\right) \\exp\\left( \\frac{\\lambda^2 \\pi}{4} \\right) = \\exp\\left( -\\frac{\\lambda^2 \\pi}{4} + \\frac{\\lambda^2 \\pi}{4} \\right) = \\exp(0) = 1\n$$\nAlternatively, since Novikov's condition holds, $\\{\\mathcal{E}(M)_t\\}_{t \\in [0,\\pi]}$ is a martingale. A property of martingales is that their expectation is constant over time. Therefore, $E[\\mathcal{E}(M)_{\\pi}] = E[\\mathcal{E}(M)_0]$. We have $M_0 = \\int_0^0 u_s \\,\\mathrm{d}W_s = 0$ and $\\langle M \\rangle_0 = \\int_0^0 u_s^2 \\,\\mathrm{d}s = 0$. Thus, $\\mathcal{E}(M)_0 = \\exp(0 - \\frac{1}{2} \\cdot 0) = 1$. The expectation is $E[\\mathcal{E}(M)_0] = E[1] = 1$. Both methods yield the same result. The first derivation, using the MGF of a Gaussian, adheres more closely to the prompt's request to use first principles.\nThe expectation is independent of the constant $\\lambda$.", "answer": "$$\\boxed{1}$$", "id": "3068882"}, {"introduction": "Building upon the verification of the martingale property, this problem explores one of its most powerful applications: the change of probability measure via the Girsanov theorem. Here, the stochastic exponential serves as the density for a new measure under which the dynamics of the underlying Brownian motion are altered. This exercise provides hands-on practice with the change of measure technique, a cornerstone for pricing derivatives in mathematical finance, by explicitly computing a probability under the new measure [@problem_id:3068900].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{0\\le t\\le T},\\mathbb{P})$ be a filtered probability space supporting a standard one-dimensional Brownian motion $W=(W_t)_{0\\le t\\le T}$, where $T0$. Fix a constant $\\mu\\in\\mathbb{R}$ and define the continuous local martingale $M_t=\\int_0^t \\mu\\, dW_s=\\mu W_t$ with quadratic variation $\\langle M\\rangle_t=\\int_0^t \\mu^2\\, ds=\\mu^2 t$. Let the Doléans–Dade stochastic exponential be given by\n$$\n\\mathcal{E}(M)_t=\\exp\\!\\Big(M_t-\\tfrac{1}{2}\\langle M\\rangle_t\\Big)=\\exp\\!\\Big(\\mu W_t-\\tfrac{1}{2}\\mu^2 t\\Big).\n$$\n  \nUsing Novikov's condition, verify that $\\mathcal{E}(M)_t$ is a true martingale and that $\\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_T]=1$. Then define a new probability measure $\\mathbb{Q}$ on $\\mathcal{F}_T$ by the Radon–Nikodym (RN) derivative \n$$\n\\frac{d\\mathbb{Q}}{d\\mathbb{P}}=\\mathcal{E}(M)_T.\n$$\n\nFor a fixed $a\\in\\mathbb{R}$, compute the expectation $\\mathbb{E}_{\\mathbb{Q}}\\big[\\mathbf{1}_{\\{W_T\\le a\\}}\\big]$ explicitly by expressing it as $\\mathbb{E}_{\\mathbb{P}}\\!\\big[\\mathbf{1}_{\\{W_T\\le a\\}}\\,\\mathcal{E}(M)_T\\big]$ and evaluating the resulting integral. Express your final answer in closed form in terms of the standard normal cumulative distribution function $\\Phi$. No rounding is required. The final answer must be a single analytic expression in $a$, $\\mu$, and $T$.", "solution": "The problem is valid. It presents a standard, well-posed question in the field of stochastic calculus, specifically concerning the application of Novikov's condition, the properties of stochastic exponentials, and the computation of expectations under a change of measure (Girsanov's theorem). All provided definitions and conditions are mathematically sound, internally consistent, and sufficient for reaching a unique solution.\n\nThe solution proceeds in three parts as requested by the problem statement.\n\nFirst, we verify that the Doléans–Dade stochastic exponential $\\mathcal{E}(M)_t$ is a true martingale. The process is given by $M_t = \\mu W_t$, which is a continuous local martingale starting at $M_0 = \\mu W_0 = 0$. Its quadratic variation is $\\langle M \\rangle_t = \\mu^2 t$. Novikov's condition states that if $M_t$ is a continuous local martingale with $M_0=0$, then its stochastic exponential $\\mathcal{E}(M)_t$ is a uniformly integrable martingale provided that $\\mathbb{E}_{\\mathbb{P}}\\big[\\exp(\\frac{1}{2}\\langle M \\rangle_T)\\big]  \\infty$ for the time horizon $T$. We check this condition:\n$$\n\\mathbb{E}_{\\mathbb{P}}\\Big[\\exp\\Big(\\tfrac{1}{2}\\langle M \\rangle_T\\Big)\\Big] = \\mathbb{E}_{\\mathbb{P}}\\Big[\\exp\\Big(\\tfrac{1}{2}\\mu^2 T\\Big)\\Big].\n$$\nSince $\\mu \\in \\mathbb{R}$ and $T  0$ are fixed constants, the term $\\exp(\\frac{1}{2}\\mu^2 T)$ is a deterministic constant. The expectation of a constant is the constant itself. Therefore,\n$$\n\\mathbb{E}_{\\mathbb{P}}\\Big[\\exp\\Big(\\tfrac{1}{2}\\mu^2 T\\Big)\\Big] = \\exp\\Big(\\tfrac{1}{2}\\mu^2 T\\Big).\n$$\nThis value is finite, so Novikov's condition is satisfied. This confirms that $\\mathcal{E}(M)_t = \\exp(\\mu W_t - \\frac{1}{2}\\mu^2 t)$ is a true martingale for $t \\in [0, T]$.\n\nSecond, we verify that $\\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_T] = 1$. Since $\\mathcal{E}(M)_t$ is a true martingale, its expectation is constant over time. Specifically, for any $t \\in [0, T]$, we have $\\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_t] = \\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_0]$. We calculate $\\mathcal{E}(M)_0$ at time $t=0$:\n$$\n\\mathcal{E}(M)_0 = \\exp\\Big(\\mu W_0 - \\tfrac{1}{2}\\mu^2 \\cdot 0\\Big) = \\exp(0) = 1.\n$$\nTherefore, the expectation at time $T$ is\n$$\n\\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_T] = \\mathbb{E}_{\\mathbb{P}}[\\mathcal{E}(M)_0] = 1.\n$$\nThis confirms that the Radon–Nikodym derivative $\\frac{d\\mathbb{Q}}{d\\mathbb{P}} = \\mathcal{E}(M)_T$ correctly defines a new probability measure $\\mathbb{Q}$ that is equivalent to $\\mathbb{P}$.\n\nThird, we compute the expectation $\\mathbb{E}_{\\mathbb{Q}}\\big[\\mathbf{1}_{\\{W_T\\le a\\}}\\big]$. Using the change of measure formula (also known as the abstract Bayes' rule), the expectation of a random variable under the new measure $\\mathbb{Q}$ can be expressed as an expectation under the original measure $\\mathbb{P}$:\n$$\n\\mathbb{E}_{\\mathbb{Q}}\\big[\\mathbf{1}_{\\{W_T\\le a\\}}\\big] = \\mathbb{E}_{\\mathbb{P}}\\Big[\\mathbf{1}_{\\{W_T\\le a\\}} \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\Big] = \\mathbb{E}_{\\mathbb{P}}\\Big[\\mathbf{1}_{\\{W_T\\le a\\}} \\mathcal{E}(M)_T\\Big].\n$$\nSubstituting the expression for $\\mathcal{E}(M)_T$, we get:\n$$\n\\mathbb{E}_{\\mathbb{P}}\\Big[\\mathbf{1}_{\\{W_T\\le a\\}} \\exp\\Big(\\mu W_T - \\tfrac{1}{2}\\mu^2 T\\Big)\\Big].\n$$\nUnder the measure $\\mathbb{P}$, the random variable $W_T$ follows a normal distribution with mean $0$ and variance $T$, i.e., $W_T \\sim \\mathcal{N}(0, T)$. Its probability density function (PDF) is $f_{W_T}(x) = \\frac{1}{\\sqrt{2\\pi T}} \\exp(-\\frac{x^2}{2T})$. The expectation can be written as an integral with respect to this density:\n$$\n\\int_{-\\infty}^{\\infty} \\mathbf{1}_{\\{x\\le a\\}} \\exp\\Big(\\mu x - \\tfrac{1}{2}\\mu^2 T\\Big) f_{W_T}(x) \\,dx = \\int_{-\\infty}^{a} \\exp\\Big(\\mu x - \\tfrac{1}{2}\\mu^2 T\\Big) \\frac{1}{\\sqrt{2\\pi T}} \\exp\\Big(-\\frac{x^2}{2T}\\Big) \\,dx.\n$$\nWe can combine the constant and exponential terms:\n$$\n\\frac{\\exp(-\\frac{1}{2}\\mu^2 T)}{\\sqrt{2\\pi T}} \\int_{-\\infty}^{a} \\exp\\Big(\\mu x - \\frac{x^2}{2T}\\Big) \\,dx.\n$$\nThe key step is to complete the square for the argument of the exponent inside the integral, $\\mu x - \\frac{x^2}{2T}$:\n$$\n\\mu x - \\frac{x^2}{2T} = -\\frac{1}{2T}(x^2 - 2T\\mu x) = -\\frac{1}{2T}\\big((x-T\\mu)^2 - T^2\\mu^2\\big) = -\\frac{(x-T\\mu)^2}{2T} + \\frac{T\\mu^2}{2}.\n$$\nSubstituting this back into the integral, we have:\n$$\n\\frac{\\exp(-\\frac{1}{2}\\mu^2 T)}{\\sqrt{2\\pi T}} \\int_{-\\infty}^{a} \\exp\\Big(-\\frac{(x-T\\mu)^2}{2T} + \\frac{T\\mu^2}{2}\\Big) \\,dx.\n$$\nThe term $\\exp(\\frac{T\\mu^2}{2})$ is constant with respect to $x$ and can be moved outside the integral. It cancels with the pre-factor $\\exp(-\\frac{1}{2}\\mu^2 T)$:\n$$\n\\frac{\\exp(-\\frac{1}{2}\\mu^2 T)\\exp(\\frac{1}{2}\\mu^2 T)}{\\sqrt{2\\pi T}} \\int_{-\\infty}^{a} \\exp\\Big(-\\frac{(x-T\\mu)^2}{2T}\\Big) \\,dx = \\int_{-\\infty}^{a} \\frac{1}{\\sqrt{2\\pi T}} \\exp\\Big(-\\frac{(x-T\\mu)^2}{2T}\\Big) \\,dx.\n$$\nThis resulting integral is precisely the cumulative distribution function (CDF) of a normally distributed random variable with mean $\\mu T$ and variance $T$, evaluated at $x=a$. Let $Y \\sim \\mathcal{N}(\\mu T, T)$. The expression is $\\mathbb{P}(Y \\le a)$. To express this in terms of the standard normal CDF $\\Phi(z) = \\mathbb{P}(Z \\le z)$ where $Z \\sim \\mathcal{N}(0, 1)$, we standardize the variable $Y$:\n$$\n\\mathbb{P}(Y \\le a) = \\mathbb{P}\\left(\\frac{Y - \\mu T}{\\sqrt{T}} \\le \\frac{a - \\mu T}{\\sqrt{T}}\\right) = \\Phi\\left(\\frac{a - \\mu T}{\\sqrt{T}}\\right).\n$$\nThis is the desired expectation.", "answer": "$$\n\\boxed{\\Phi\\left(\\frac{a - \\mu T}{\\sqrt{T}}\\right)}\n$$", "id": "3068900"}, {"introduction": "This final practice demonstrates another essential tool for analyzing martingales: the Optional Sampling Theorem. By introducing a stopping time based on the value of the stochastic exponential, you can explore the process's behavior and derive powerful analytical results. This exercise will guide you through using the martingale property in conjunction with a stopping time to establish a non-trivial probability bound, a technique with wide applications in risk management and the study of a stochastic processes [@problem_id:3068939].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions, supporting a standard one-dimensional Brownian motion $W=(W_t)_{t \\geq 0}$. Fix a finite time horizon $T0$ and a constant $\\lambda \\in \\mathbb{R}$. Define the continuous local martingale $M=(M_t)_{0 \\leq t \\leq T}$ by $M_t=\\lambda W_t$ and its Doléans–Dade exponential (also called the stochastic exponential)\n$$\n\\mathcal{E}(M)_t \\equiv \\exp\\!\\Big(M_t - \\frac{1}{2}\\langle M\\rangle_t\\Big) = \\exp\\!\\Big(\\lambda W_t - \\frac{1}{2}\\lambda^2 t\\Big), \\quad 0 \\leq t \\leq T,\n$$\nwhere $\\langle M\\rangle_t$ denotes the quadratic variation of $M$ up to time $t$. For a threshold $a \\in (0,1)$, let the stopping time $\\tau_a$ be the first time $\\mathcal{E}(M)$ drops below $a$:\n$$\n\\tau_a \\equiv \\inf\\{ t \\in [0,T] : \\mathcal{E}(M)_t \\leq a \\}.\n$$\n\nStarting from the definitions above and using fundamental facts of Itô calculus and well-tested integrability criteria for exponential local martingales, justify that $(\\mathcal{E}(M)_t)_{0 \\leq t \\leq T}$ is a uniformly integrable martingale. Then, using the Optional Sampling Theorem (OST) for bounded stopping times, evaluate the expectation $\\mathbb{E}\\!\\left[\\mathcal{E}(M)_{T \\wedge \\tau_a}\\right]$ and explain how the stopping at $\\tau_a$ helps analyze integrability by deriving a bound on $\\mathbb{P}(\\tau_a \\leq T)$ via OST.\n\nYour final answer must be the value of the single quantity $\\mathbb{E}\\!\\left[\\mathcal{E}(M)_{T \\wedge \\tau_a}\\right]$; no rounding is required.", "solution": "### Solution\nLet the process $Z_t$ be the Doléans–Dade exponential of $M_t$, given by\n$$\nZ_t = \\mathcal{E}(M)_t = \\exp\\left(\\lambda W_t - \\frac{1}{2}\\lambda^2 t\\right), \\quad 0 \\leq t \\leq T.\n$$\nBy construction, $Z_t$ is a positive continuous local martingale. We first justify that it is a true martingale on the interval $[0, T]$ and, moreover, is uniformly integrable.\n\nA sufficient condition for a local martingale $\\mathcal{E}(M)_t$ to be a true martingale is Novikov's condition. For a continuous local martingale $M_t$ with $M_0=0$, Novikov's condition states that if $\\mathbb{E}[\\exp(\\frac{1}{2}\\langle M \\rangle_t)]  \\infty$ for all $t$ in the time horizon, then $(\\mathcal{E}(M)_s)_{0 \\leq s \\leq t}$ is a martingale. We check this condition for our process on the interval $[0,T]$.\n\nThe quadratic variation of $M_t = \\lambda W_t$ is\n$$\n\\langle M \\rangle_t = \\langle \\lambda W \\rangle_t = \\lambda^2 \\langle W \\rangle_t = \\lambda^2 t.\n$$\nWe evaluate the expectation in Novikov's condition for any $t \\in [0,T]$:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\langle M \\rangle_t\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\lambda^2 t\\right)\\right].\n$$\nSince $\\lambda$, and $t$ are deterministic constants, the term $\\exp(\\frac{1}{2}\\lambda^2 t)$ is a constant. The expectation of a constant is the constant itself. Thus,\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\lambda^2 t\\right)\\right] = \\exp\\left(\\frac{1}{2}\\lambda^2 t\\right).\n$$\nFor any finite $t \\in [0, T]$ and any $\\lambda \\in \\mathbb{R}$, this value is finite. Therefore, Novikov's condition is satisfied for all $t \\in [0,T]$. This proves that $(Z_t)_{0 \\leq t \\leq T}$ is a true martingale on the interval $[0,T]$.\n\nFurthermore, a continuous martingale on a finite time interval $[0,T]$ is always uniformly integrable. This is a standard result in martingale theory, often shown by noting that for any stopping time $\\sigma \\leq T$, $Z_\\sigma = \\mathbb{E}[Z_T | \\mathcal{F}_\\sigma]$, and the family of conditional expectations of a single integrable random variable ($Z_T$) is uniformly integrable.\n\nNext, we evaluate the expectation $\\mathbb{E}[Z_{T \\wedge \\tau_a}]$.\nThe stopping time is defined as $\\tau_a = \\inf\\{ t \\in [0,T] : Z_t \\leq a \\}$. We consider the composed stopping time $\\sigma = T \\wedge \\tau_a$. Since $T$ is a fixed, finite time, $\\sigma$ is a bounded stopping time, i.e., $0 \\leq \\sigma \\leq T$ almost surely.\n\nThe Optional Sampling Theorem (OST) for uniformly integrable martingales states that for any stopping time $\\sigma$, $\\mathbb{E}[Z_\\sigma] = \\mathbb{E}[Z_0]$.\nWe first compute the value of the process at time $t=0$:\n$$\nZ_0 = \\exp\\left(\\lambda W_0 - \\frac{1}{2}\\lambda^2 \\cdot 0\\right).\n$$\nFor a standard Brownian motion, $W_0=0$ almost surely. Therefore,\n$$\nZ_0 = \\exp(0) = 1.\n$$\nThe expectation is $\\mathbb{E}[Z_0] = \\mathbb{E}[1] = 1$.\nApplying the OST with the stopping time $\\sigma = T \\wedge \\tau_a$, we have:\n$$\n\\mathbb{E}[Z_{T \\wedge \\tau_a}] = \\mathbb{E}[Z_0] = 1.\n$$\nThis is the value of the requested expectation.\n\nFinally, we explain how the use of the stopping time $\\tau_a$ helps in analysis, by deriving a bound on the probability $\\mathbb{P}(\\tau_a \\leq T)$. We use the result we just derived:\n$$\n\\mathbb{E}[Z_{T \\wedge \\tau_a}] = 1.\n$$\nWe can split the expectation over the disjoint events $\\{\\tau_a \\leq T\\}$ and $\\{\\tau_a  T\\}$. Note that by definition, $\\tau_a  T$ implies that for all $t \\in [0,T]$, $Z_t  a$.\n$$\n1 = \\mathbb{E}[Z_{T \\wedge \\tau_a} \\mathbb{I}_{\\{\\tau_a \\leq T\\}}] + \\mathbb{E}[Z_{T \\wedge \\tau_a} \\mathbb{I}_{\\{\\tau_a  T\\}}].\n$$\nLet's analyze each term:\n1.  On the event $\\{\\tau_a \\leq T\\}$, the stopping time is $\\tau_a$, so $T \\wedge \\tau_a = \\tau_a$. Since the paths of $Z_t$ are continuous (as $W_t$ has continuous paths), at the first time $\\tau_a$ the process reaches the set $(-\\infty, a]$, it must be that $Z_{\\tau_a} = a$. Thus, the first term becomes:\n    $$\n    \\mathbb{E}[Z_{\\tau_a} \\mathbb{I}_{\\{\\tau_a \\leq T\\}}] = \\mathbb{E}[a \\cdot \\mathbb{I}_{\\{\\tau_a \\leq T\\}}] = a \\cdot \\mathbb{P}(\\tau_a \\leq T).\n    $$\n2.  On the event $\\{\\tau_a  T\\}$, the stopping time is $T$, so $T \\wedge \\tau_a = T$. The second term becomes:\n    $$\n    \\mathbb{E}[Z_T \\mathbb{I}_{\\{\\tau_a  T\\}}].\n    $$\nSubstituting these back into the equation gives:\n$$\n1 = a \\cdot \\mathbb{P}(\\tau_a \\leq T) + \\mathbb{E}[Z_T \\mathbb{I}_{\\{\\tau_a  T\\}}].\n$$\nThe process $Z_t$ is strictly positive for all $t$. Therefore, the term $\\mathbb{E}[Z_T \\mathbb{I}_{\\{\\tau_a  T\\}}]$ is non-negative. Dropping this term from the equation yields an inequality:\n$$\n1 \\geq a \\cdot \\mathbb{P}(\\tau_a \\leq T).\n$$\nSince $a \\in (0,1)$, we can divide by $a$ to get a bound on the probability of the process hitting the barrier $a$:\n$$\n\\mathbb{P}(\\tau_a \\leq T) \\leq \\frac{1}{a}.\n$$\nThis demonstrates how the OST, applied to a strategically chosen stopped process, allows us to derive a non-trivial bound on the probability of an event, a common and powerful technique in the analysis of stochastic processes.\nThe required value for the final answer is $\\mathbb{E}[\\mathcal{E}(M)_{T \\wedge \\tau_a}]$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3068939"}]}