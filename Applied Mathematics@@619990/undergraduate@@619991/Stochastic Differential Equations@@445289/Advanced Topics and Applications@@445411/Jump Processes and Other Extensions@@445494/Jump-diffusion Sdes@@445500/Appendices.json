{"hands_on_practices": [{"introduction": "The cornerstone of stochastic calculus for continuous processes is Itô's formula. When we introduce jumps, this celebrated formula must be extended to account for discrete, sudden changes. This foundational exercise guides you through applying the generalized Itô formula to a jump-diffusion process for the function $f(x)=x^2$, which is essential for understanding how the variance and second moment of a process evolve over time. Mastering this calculation ([@problem_id:3062569]) is a crucial first step toward analyzing the dynamics of more complex systems.", "problem": "Consider a scalar process $X_t$ defined on a filtered probability space supporting a standard Brownian motion $W_t$ and a Poisson random measure $N(\\mathrm{d}t,\\mathrm{d}z)$ on $(0,\\infty)\\times\\mathbb{R}$ with intensity (Lévy) measure $\\nu(\\mathrm{d}z)$, and compensated Poisson random measure $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z) = N(\\mathrm{d}t,\\mathrm{d}z) - \\nu(\\mathrm{d}z)\\,\\mathrm{d}t$. Assume the coefficients $b:\\mathbb{R}\\to\\mathbb{R}$, $\\sigma:\\mathbb{R}\\to\\mathbb{R}$, and $\\gamma:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ are measurable and satisfy conditions ensuring existence of a unique càdlàg (right-continuous with left limits) solution and square-integrability of all stochastic integrals (for example, local Lipschitz and linear growth in the first argument, and $\\int_{\\mathbb{R}}\\min\\{|\\gamma(x,z)|^{2},1\\}\\,\\nu(\\mathrm{d}z)\\infty$ for each $x$). The process $X_t$ solves the scalar jump-diffusion stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t \\;=\\; b(X_{t-})\\,\\mathrm{d}t \\;+\\; \\sigma(X_{t-})\\,\\mathrm{d}W_t \\;+\\; \\int_{\\mathbb{R}} \\gamma(X_{t-},z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z),\n$$\nwith given initial condition $X_0\\in\\mathbb{R}$. Here $X_{t-}$ denotes the left limit of $X$ at time $t$.\n\nUsing the fundamental Itô formula for semimartingales with jumps and the definition of quadratic variation, apply Itô's formula to the twice continuously differentiable function $f(x)=x^2$ to compute the differential $\\mathrm{d}(X_t^{2})$ in terms of $b$, $\\sigma$, $\\gamma$, $W$, $\\tilde{N}$, and $\\nu$. In your derivation, explicitly identify the continuous martingale contribution and the pure-jump contribution to the quadratic variation of $X_t$ before assembling the final expression for $\\mathrm{d}(X_t^{2})$.\n\nProvide your final result as a single closed-form analytic expression for $\\mathrm{d}(X_t^{2})$. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Process:** A scalar process $X_t$.\n- **SDE:** $\\mathrm{d}X_t = b(X_{t-})\\,\\mathrm{d}t + \\sigma(X_{t-})\\,\\mathrm{d}W_t + \\int_{\\mathbb{R}} \\gamma(X_{t-},z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$.\n- **Initial Condition:** $X_0 \\in \\mathbb{R}$.\n- **Driving Processes:** A standard Brownian motion $W_t$ and a compensated Poisson random measure $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z) = N(\\mathrm{d}t,\\mathrm{d}z) - \\nu(\\mathrm{d}z)\\,\\mathrm{d}t$.\n- **Coefficients:** Measurable functions $b$, $\\sigma$, and $\\gamma$.\n- **Assumptions:** The coefficients guarantee a unique càdlàg solution and well-defined stochastic integrals.\n- **Task:** Apply Itô's formula to $f(x)=x^2$ to find the differential $\\mathrm{d}(X_t^2)$. During the derivation, explicitly identify the continuous martingale and pure-jump contributions to the quadratic variation of $X_t$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard application of Itô's formula for jump-diffusion processes (semimartingales), a cornerstone of modern stochastic calculus. All definitions and notations are standard.\n- **Well-Posed:** The problem is well-posed. It asks for the derivation of a specific quantity using a fundamental theorem, given a well-defined mathematical object (the SDE) and sufficient regularity conditions. The result is unique.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n- **Conclusion:** The problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nWe are asked to compute the differential of $X_t^2$ where $X_t$ is a jump-diffusion process. This requires the application of Itô's formula for semimartingales with jumps. For a twice continuously differentiable function $f:\\mathbb{R} \\to \\mathbb{R}$ and a semimartingale $X_t$ given by the specified SDE, the general Itô formula (also known as the Itô-Lévy formula) is:\n$$\n\\mathrm{d}f(X_t) = f'(X_{t-})\\, \\mathrm{d}X_t^{\\text{cont}} + \\frac{1}{2} f''(X_{t-})\\, \\mathrm{d}[X,X]_t^c \\\\\n+ \\int_{\\mathbb{R}} \\left[ f(X_{t-} + \\gamma(X_{t-},z)) - f(X_{t-}) \\right] \\tilde{N}(\\mathrm{d}t,\\mathrm{d}z) \\\\\n+ \\int_{\\mathbb{R}} \\left[ f(X_{t-} + \\gamma(X_{t-},z)) - f(X_{t-}) - f'(X_{t-})\\gamma(X_{t-},z) \\right] \\nu(\\mathrm{d}z)\\,\\mathrm{d}t\n$$\nwhere $\\mathrm{d}X_t^{\\text{cont}} = b(X_{t-})\\,\\mathrm{d}t + \\sigma(X_{t-})\\,\\mathrm{d}W_t$ is the continuous part of the evolution of $X_t$ and $[X,X]_t^c$ is the quadratic variation of the continuous martingale part of $X_t$.\n\nIn our case, the function is $f(x)=x^2$. Its derivatives are $f'(x)=2x$ and $f''(x)=2$. The process $X_t$ has a continuous martingale part given by $\\int_0^t \\sigma(X_{s-})\\,\\mathrm{d}W_s$.\n\nFirst, as requested, we identify the contributions to the quadratic variation.\n1.  **Continuous Martingale Contribution:** The quadratic variation of the continuous martingale part, $M_t^c = \\int_0^t \\sigma(X_{s-})\\,\\mathrm{d}W_s$, is given by $[X,X]_t^c = [M^c,M^c]_t = \\int_0^t \\sigma^2(X_{s-})\\,\\mathrm{d}s$. In differential form, this is:\n    $$\n    \\mathrm{d}[X,X]_t^c = \\sigma^2(X_{t-})\\,\\mathrm{d}t\n    $$\n2.  **Pure-Jump Contribution:** The jumps of the process $X_t$ at time $t$ are given by $\\Delta X_t = \\int_{\\mathbb{R}} \\gamma(X_{t-},z) N(\\{t\\}, \\mathrm{d}z)$. The quadratic variation from the jumps is the sum of the squares of these jumps, $[X,X]_t^d = \\sum_{0s\\le t} (\\Delta X_s)^2$. This can be expressed as an integral with respect to the Poisson random measure:\n    $$\n    \\mathrm{d}[X,X]_t^d = \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) N(\\mathrm{d}t,\\mathrm{d}z)\n    $$\nThe compensator of this jump quadratic variation process is $\\langle X^d, X^d \\rangle_t = \\int_0^t \\int_{\\mathbb{R}} \\gamma^2(X_{s-},z) \\nu(\\mathrm{d}z)\\,\\mathrm{d}s$, which in differential form is $\\mathrm{d}\\langle X^d, X^d \\rangle_t = \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) \\nu(\\mathrm{d}z)\\,\\mathrm{d}t$.\n\nNow we apply Itô's formula to $f(x)=x^2$. We compute each term in the formula.\n\n- **Term 1:** $f'(X_{t-})\\, \\mathrm{d}X_t^{\\text{cont}} = 2X_{t-}(b(X_{t-})\\,\\mathrm{d}t + \\sigma(X_{t-})\\,\\mathrm{d}W_t)$.\n- **Term 2:** $\\frac{1}{2} f''(X_{t-})\\, \\mathrm{d}[X,X]_t^c = \\frac{1}{2}(2)\\sigma^2(X_{t-})\\,\\mathrm{d}t = \\sigma^2(X_{t-})\\,\\mathrm{d}t$. This term arises directly from the continuous martingale contribution to the quadratic variation.\n- **Term 3 (Jump Martingale):** The integrand is $f(X_{t-} + \\gamma(X_{t-},z)) - f(X_{t-}) = (X_{t-}+\\gamma(X_{t-},z))^2 - X_{t-}^2 = 2X_{t-}\\gamma(X_{t-},z) + \\gamma^2(X_{t-},z)$. The full term is:\n$$ \\int_{\\mathbb{R}} \\left( 2X_{t-}\\gamma(X_{t-},z) + \\gamma^2(X_{t-},z) \\right) \\tilde{N}(\\mathrm{d}t,\\mathrm{d}z) $$\n- **Term 4 (Jump compensator contribution to drift):** The integrand is $f(X_{t-} + \\gamma(X_{t-},z)) - f(X_{t-}) - f'(X_{t-})\\gamma(X_{t-},z) = (X_{t-}+\\gamma(X_{t-},z))^2 - X_{t-}^2 - 2X_{t-}\\gamma(X_{t-},z) = \\gamma^2(X_{t-},z)$. The full term is:\n$$ \\left( \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) \\nu(\\mathrm{d}z) \\right) \\mathrm{d}t $$\nThis term can be interpreted as the contribution to the drift from the second-order effect of the jumps, and it is built from the compensator of the pure-jump quadratic variation.\n\nCombining all terms, we get the expression for $\\mathrm{d}(X_t^2)$. We group the terms by their differential type ($\\mathrm{d}t$, $\\mathrm{d}W_t$, $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$).\n\n- **Drift part (coefficient of $\\mathrm{d}t$):** This combines the original drift from $X_t$ (multiplied by $f'$), the Itô correction from the continuous part, and the Itô correction from the jump part.\n$$\n2X_{t-}b(X_{t-}) + \\sigma^2(X_{t-}) + \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) \\nu(\\mathrm{d}z)\n$$\n- **Continuous martingale part (coefficient of $\\mathrm{d}W_t$):**\n$$\n2X_{t-}\\sigma(X_{t-})\n$$\n- **Pure-jump martingale part (integrand for $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$):**\n$$\n2X_{t-}\\gamma(X_{t-},z) + \\gamma^2(X_{t-},z)\n$$\nAssembling these components gives the final SDE for $X_t^2$:\n$$\n\\mathrm{d}(X_t^2) = \\left( 2X_{t-}b(X_{t-}) + \\sigma^2(X_{t-}) + \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) \\nu(\\mathrm{d}z) \\right) \\mathrm{d}t \\\\\n+ 2X_{t-}\\sigma(X_{t-})\\mathrm{d}W_t \\\\\n+ \\int_{\\mathbb{R}} \\left( 2X_{t-}\\gamma(X_{t-},z) + \\gamma^2(X_{t-},z) \\right) \\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)\n$$\nThis is the complete expression for the differential $\\mathrm{d}(X_t^2)$ in the standard canonical form for a jump-diffusion process.", "answer": "$$\\boxed{\\left( 2X_{t-}b(X_{t-}) + \\sigma^2(X_{t-}) + \\int_{\\mathbb{R}} \\gamma^2(X_{t-},z) \\nu(\\mathrm{d}z) \\right) \\mathrm{d}t + 2X_{t-}\\sigma(X_{t-})\\mathrm{d}W_t + \\int_{\\mathbb{R}} \\left( 2X_{t-}\\gamma(X_{t-},z) + \\gamma^2(X_{t-},z) \\right) \\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)}$$", "id": "3062569"}, {"introduction": "While general SDEs are often intractable, a few key models can be solved analytically, providing deep insight into stochastic dynamics. This practice focuses on the linear jump-diffusion SDE, an extension of the famous Ornstein-Uhlenbeck process that includes jumps. By leveraging the rules of Itô calculus, you will derive and solve ordinary differential equations for the mean and variance of the process ([@problem_id:3062553]). This powerful technique is fundamental for characterizing the statistical properties and long-term behavior of stochastic models with mean-reversion and shocks.", "problem": "Consider the linear jump-diffusion Stochastic Differential Equation (SDE)\n$$\ndX_t = \\big(\\beta_0 + \\beta_1 X_t\\big)\\,dt + \\sigma\\,dW_t + \\gamma\\,dN_t,\\qquad X_0 = x_0,\n$$\nwhere $W_t$ is a standard Brownian motion, $N_t$ is a Poisson process of intensity $\\lambda0$, and $W_t$ and $N_t$ are independent. The constants $\\beta_0,\\beta_1,\\sigma,\\gamma\\in\\mathbb{R}$, with $\\beta_1\\neq 0$, and $x_0\\in\\mathbb{R}$.\n\nStarting from the core definitions of stochastic integrals and expectations for Brownian motion and Poisson processes, and using the compensation of the Poisson process together with Itô’s isometry for both the Brownian and compensated Poisson integrals, derive closed-form analytical expressions for the first and second moments sufficient to obtain the expectation $E[X_t]$ and the variance $\\mathrm{Var}(X_t)$ for $t\\ge 0$.\n\nExpress your final result as analytic expressions in terms of $t$, $x_0$, $\\beta_0$, $\\beta_1$, $\\sigma$, $\\gamma$, and $\\lambda$. No rounding is required, and no units are needed. Your final answer must be the pair $(E[X_t], \\mathrm{Var}(X_t))$.", "solution": "The problem statement is a well-posed and scientifically grounded question in the field of stochastic differential equations. It provides a complete set of parameters and conditions for a linear jump-diffusion SDE and requests the derivation of its first two moments, which are well-defined quantities. The model is a standard extension of the Ornstein-Uhlenbeck process, and all terms and concepts are standard in stochastic calculus. There are no contradictions, ambiguities, or factual unsoundness. Therefore, the problem is valid, and we may proceed with the solution.\n\nOur objective is to find the expectation $E[X_t]$ and variance $\\mathrm{Var}(X_t)$ of the process $X_t$ defined by the SDE:\n$$\ndX_t = \\big(\\beta_0 + \\beta_1 X_t\\big)\\,dt + \\sigma\\,dW_t + \\gamma\\,dN_t,\\qquad X_0 = x_0\n$$\nwhere $W_t$ is a standard Brownian motion and $N_t$ is an independent Poisson process with constant intensity $\\lambda  0$.\n\nThe derivation will proceed in two main parts: first, we compute the expectation $E[X_t]$, and second, we compute the second moment $E[X_t^2]$ to find the variance $\\mathrm{Var}(X_t) = E[X_t^2] - (E[X_t])^2$. The method relies on constructing and solving ordinary differential equations (ODEs) for the moments, a technique that stems from Itô's formula for jump-diffusion processes.\n\nAs stipulated, we begin by introducing the compensated Poisson process, $\\tilde{N}_t$, defined as $\\tilde{N}_t = N_t - \\lambda t$. This process is a martingale, and its differential is $d\\tilde{N}_t = dN_t - \\lambda\\,dt$. Rearranging, we have $dN_t = d\\tilde{N}_t + \\lambda\\,dt$. Substituting this into the original SDE gives:\n$$\ndX_t = \\big(\\beta_0 + \\beta_1 X_t\\big)\\,dt + \\sigma\\,dW_t + \\gamma\\,(d\\tilde{N}_t + \\lambda\\,dt)\n$$\nCollecting the $dt$ terms, we obtain the SDE in terms of martingales $W_t$ and $\\tilde{N}_t$:\n$$\ndX_t = \\big(\\beta_0 + \\gamma\\lambda + \\beta_1 X_t\\big)\\,dt + \\sigma\\,dW_t + \\gamma\\,d\\tilde{N}_t\n$$\n\n**1. Derivation of the Expectation $E[X_t]$**\n\nTo find the expectation, we consider the integral form of the SDE:\n$$\nX_t = X_0 + \\int_0^t \\big(\\beta_0 + \\gamma\\lambda + \\beta_1 X_s\\big)\\,ds + \\int_0^t \\sigma\\,dW_s + \\int_0^t \\gamma\\,d\\tilde{N}_s\n$$\nLet $\\mu_t = E[X_t]$. We take the expectation of both sides. By the linearity of expectation and Fubini's theorem (for interchanging expectation and the Lebesgue integral), we have:\n$$\nE[X_t] = E[X_0] + E\\left[\\int_0^t \\big(\\beta_0 + \\gamma\\lambda + \\beta_1 X_s\\big)\\,ds\\right] + E\\left[\\int_0^t \\sigma\\,dW_s\\right] + E\\left[\\int_0^t \\gamma\\,d\\tilde{N}_s\\right]\n$$\n$$\n\\mu_t = x_0 + \\int_0^t \\big(\\beta_0 + \\gamma\\lambda + \\beta_1 E[X_s]\\big)\\,ds + 0 + 0\n$$\nThe expectations of the stochastic integrals involving $dW_s$ and $d\\tilde{N}_s$ are zero. This is a fundamental property of Itô integrals with respect to martingales (under suitable integrability conditions on the integrands, which are met here).\nThe resulting integral equation for $\\mu_t$ is:\n$$\n\\mu_t = x_0 + \\int_0^t \\big(\\beta_0 + \\gamma\\lambda + \\beta_1 \\mu_s\\big)\\,ds\n$$\nDifferentiating with respect to $t$ gives a first-order linear ODE for $\\mu_t$:\n$$\n\\frac{d\\mu_t}{dt} = \\beta_1 \\mu_t + \\beta_0 + \\gamma\\lambda, \\qquad \\mu_0 = x_0\n$$\nTo solve this ODE, we rearrange it as $\\frac{d\\mu_t}{dt} - \\beta_1 \\mu_t = \\beta_0 + \\gamma\\lambda$. The integrating factor is $\\exp(-\\beta_1 t)$. Multiplying the ODE by the integrating factor yields:\n$$\n\\frac{d}{dt}\\big(\\mu_t \\exp(-\\beta_1 t)\\big) = (\\beta_0 + \\gamma\\lambda) \\exp(-\\beta_1 t)\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\n\\mu_t \\exp(-\\beta_1 t) - \\mu_0 \\exp(0) = \\int_0^t (\\beta_0 + \\gamma\\lambda) \\exp(-\\beta_1 s)\\,ds\n$$\n$$\n\\mu_t \\exp(-\\beta_1 t) - x_0 = (\\beta_0 + \\gamma\\lambda) \\left[ \\frac{\\exp(-\\beta_1 s)}{-\\beta_1} \\right]_0^t = \\frac{\\beta_0 + \\gamma\\lambda}{-\\beta_1} (\\exp(-\\beta_1 t) - 1)\n$$\nSolving for $\\mu_t$:\n$$\n\\mu_t = x_0 \\exp(\\beta_1 t) + \\frac{\\beta_0 + \\gamma\\lambda}{\\beta_1} (1 - \\exp(-\\beta_1 t))\\exp(\\beta_1 t)\n$$\n$$\nE[X_t] = \\mu_t = x_0 \\exp(\\beta_1 t) + \\frac{\\beta_0 + \\gamma\\lambda}{\\beta_1} (\\exp(\\beta_1 t) - 1)\n$$\n\n**2. Derivation of the Variance $\\mathrm{Var}(X_t)$**\n\nTo find the variance, we first need the second moment $m_2(t) = E[X_t^2]$. We use Itô's lemma for jump-diffusion processes for the function $f(X_t) = X_t^2$. The general form is:\n$$\ndf(X_t) = \\left( \\frac{\\partial f}{\\partial x} a_t + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} b_t^2 \\right) dt + \\frac{\\partial f}{\\partial x} b_t dW_t + \\left(f(X_{t^-} + c_t) - f(X_{t^-})\\right) dN_t\n$$\nFor our process, $f(x)=x^2$, the drift is $a_t = \\beta_0 + \\beta_1 X_t$, the diffusion coefficient is $b_t = \\sigma$, and the jump size is $c_t = \\gamma$. The derivatives are $\\frac{\\partial f}{\\partial x} = 2x$ and $\\frac{\\partial^2 f}{\\partial x^2} = 2$. Applying the lemma:\n$$\nd(X_t^2) = \\big(2X_t(\\beta_0 + \\beta_1 X_t) + \\frac{1}{2}(2)\\sigma^2\\big)dt + 2X_t \\sigma dW_t + \\big((X_{t^-}+\\gamma)^2 - X_{t^-}^2\\big) dN_t\n$$\n$$\nd(X_t^2) = (2\\beta_0 X_t + 2\\beta_1 X_t^2 + \\sigma^2)dt + 2\\sigma X_t dW_t + (2\\gamma X_{t^-} + \\gamma^2)dN_t\n$$\nUsing the compensated process $dN_t = d\\tilde{N}_t + \\lambda dt$ and grouping terms (using $X_t$ in place of $X_{t^-}$ as is standard in this context):\n$$\nd(X_t^2) = (2\\beta_0 X_t + 2\\beta_1 X_t^2 + \\sigma^2 + \\lambda(2\\gamma X_t + \\gamma^2))dt + 2\\sigma X_t dW_t + (2\\gamma X_t + \\gamma^2)d\\tilde{N}_t\n$$\n$$\nd(X_t^2) = \\big(2\\beta_1 X_t^2 + (2\\beta_0 + 2\\gamma\\lambda)X_t + \\sigma^2 + \\gamma^2\\lambda\\big)dt + 2\\sigma X_t dW_t + (2\\gamma X_t + \\gamma^2)d\\tilde{N}_t\n$$\nLet $m_2(t) = E[X_t^2]$. Taking the expectation of the integral form of this SDE, the martingale integral terms vanish:\n$$\nE[X_t^2] = E[X_0^2] + \\int_0^t E\\big[2\\beta_1 X_s^2 + (2\\beta_0 + 2\\gamma\\lambda)X_s + \\sigma^2 + \\gamma^2\\lambda\\big]ds\n$$\n$$\nm_2(t) = x_0^2 + \\int_0^t \\big(2\\beta_1 m_2(s) + (2\\beta_0 + 2\\gamma\\lambda)\\mu_s + \\sigma^2 + \\gamma^2\\lambda\\big)ds\n$$\nThis gives the ODE for the second moment:\n$$\n\\frac{dm_2(t)}{dt} = 2\\beta_1 m_2(t) + (2\\beta_0 + 2\\gamma\\lambda)\\mu_t + \\sigma^2 + \\gamma^2\\lambda, \\qquad m_2(0) = x_0^2\n$$\nThis is a linear ODE for $m_2(t)$. Let $C_1 = \\beta_0 + \\gamma\\lambda$ and $K = \\sigma^2 + \\gamma^2\\lambda$. The ODE is $\\frac{dm_2}{dt} - 2\\beta_1 m_2 = 2C_1 \\mu_t + K$. We solve it using an integrating factor $\\exp(-2\\beta_1 t)$.\n$$\n\\frac{d}{dt}\\big(m_2(t)\\exp(-2\\beta_1 t)\\big) = (2C_1 \\mu_t + K)\\exp(-2\\beta_1 t)\n$$\nSubstituting $\\mu_t = (x_0 + C_1/\\beta_1)\\exp(\\beta_1 t) - C_1/\\beta_1$:\n$$\n\\frac{d}{dt}\\big(m_2(t)\\exp(-2\\beta_1 t)\\big) = \\left(2C_1\\left[\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\exp(\\beta_1 t) - \\frac{C_1}{\\beta_1}\\right] + K\\right)\\exp(-2\\beta_1 t)\n$$\n$$\n= 2C_1\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\exp(-\\beta_1 t) + \\left(K - \\frac{2C_1^2}{\\beta_1}\\right)\\exp(-2\\beta_1 t)\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\nm_2(t)\\exp(-2\\beta_1 t) - x_0^2 = \\int_0^t \\left[2C_1\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\exp(-\\beta_1 s) + \\left(K - \\frac{2C_1^2}{\\beta_1}\\right)\\exp(-2\\beta_1 s)\\right] ds\n$$\n$$\n= 2C_1\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\left[\\frac{1-\\exp(-\\beta_1 t)}{\\beta_1}\\right] + \\left(K - \\frac{2C_1^2}{\\beta_1}\\right)\\left[\\frac{1-\\exp(-2\\beta_1 t)}{2\\beta_1}\\right]\n$$\nMultiplying by $\\exp(2\\beta_1 t)$ gives $m_2(t)$. Now we compute the variance $\\mathrm{Var}(X_t) = m_2(t) - \\mu_t^2$.\n$$\n\\mu_t^2 = \\left(\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\exp(\\beta_1 t) - \\frac{C_1}{\\beta_1}\\right)^2 = \\left(x_0 + \\frac{C_1}{\\beta_1}\\right)^2\\exp(2\\beta_1 t) - \\frac{2C_1}{\\beta_1}\\left(x_0 + \\frac{C_1}{\\beta_1}\\right)\\exp(\\beta_1 t) + \\left(\\frac{C_1}{\\beta_1}\\right)^2\n$$\nThe calculation of $m_2(t) - \\mu_t^2$ involves significant algebraic simplification. A more direct method is to find the variance of the solution. Let $Y_t = X_t - \\mu_t$. Then $dY_t = dX_t - d\\mu_t$. From the ODE for $\\mu_t$, $d\\mu_t = (\\beta_1 \\mu_t + C_1)dt$.\n$$\ndY_t = (\\beta_0 + \\beta_1 X_t)dt + \\sigma dW_t + \\gamma dN_t - (\\beta_1 \\mu_t + C_1)dt\n$$\n$$\ndY_t = (\\beta_1(X_t - \\mu_t) + \\beta_0 - C_1)dt + \\sigma dW_t + \\gamma(d\\tilde{N}_t + \\lambda dt) = \\beta_1 Y_t dt + \\sigma dW_t + \\gamma d\\tilde{N}_t\n$$\nsince $\\beta_0 - C_1 + \\gamma\\lambda = \\beta_0 - (\\beta_0+\\gamma\\lambda) + \\gamma\\lambda = 0$.\nThe SDE for the centered process $Y_t$ is $dY_t = \\beta_1 Y_t dt + \\sigma dW_t + \\gamma d\\tilde{N}_t$, with $Y_0=0$.\nThe variance is $\\mathrm{Var}(X_t) = E[Y_t^2]$. Using Itô's lemma for $f(y)=y^2$:\n$$\nd(Y_t^2) = (2\\beta_1 Y_t^2 + \\sigma^2)dt + 2\\sigma Y_t dW_t + (\\gamma^2 + 2\\gamma Y_t)dN_t\n$$\nLet $v(t)=\\mathrm{Var}(X_t)=E[Y_t^2]$. Taking expectations and using $E[Y_t]=0$:\n$$\nd(E[Y_t^2]) = E[2\\beta_1 Y_t^2 + \\sigma^2]dt + E[(\\gamma^2 + 2\\gamma Y_t)\\lambda]dt\n$$\n$$\n\\frac{dv(t)}{dt} = 2\\beta_1 v(t) + \\sigma^2 + \\gamma^2\\lambda\n$$\nThis is a simpler ODE for the variance $v(t)$, with initial condition $v(0) = \\mathrm{Var}(X_0) = 0$.\n$$\n\\frac{dv}{dt} - 2\\beta_1 v = \\sigma^2 + \\gamma^2\\lambda\n$$\nIntegrating factor is $\\exp(-2\\beta_1 t)$.\n$$\n\\frac{d}{dt}\\big(v(t)\\exp(-2\\beta_1 t)\\big) = (\\sigma^2 + \\gamma^2\\lambda)\\exp(-2\\beta_1 t)\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\nv(t)\\exp(-2\\beta_1 t) - v(0) = (\\sigma^2 + \\gamma^2\\lambda) \\int_0^t \\exp(-2\\beta_1 s)ds\n$$\n$$\nv(t)\\exp(-2\\beta_1 t) - 0 = (\\sigma^2 + \\gamma^2\\lambda) \\left[\\frac{\\exp(-2\\beta_1 s)}{-2\\beta_1}\\right]_0^t = \\frac{\\sigma^2 + \\gamma^2\\lambda}{2\\beta_1}(1 - \\exp(-2\\beta_1 t))\n$$\nSolving for $v(t)$:\n$$\n\\mathrm{Var}(X_t) = v(t) = \\frac{\\sigma^2 + \\lambda\\gamma^2}{2\\beta_1}(\\exp(2\\beta_1 t) - 1)\n$$\nThis result also correctly reflects the contribution to variance from the diffusion term (via Itô isometry for Brownian motion, contributing $\\sigma^2$) and the jump term (via Itô isometry for compensated Poisson process, contributing $\\lambda\\gamma^2$).\n\nFinal expressions for the expectation and variance are:\n$$\nE[X_t] = x_0 \\exp(\\beta_1 t) + \\frac{\\beta_0 + \\gamma\\lambda}{\\beta_1}(\\exp(\\beta_1 t) - 1)\n$$\n$$\n\\mathrm{Var}(X_t) = \\frac{\\sigma^2 + \\lambda\\gamma^2}{2\\beta_1}(\\exp(2\\beta_1 t) - 1)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_0 \\exp(\\beta_1 t) + \\frac{\\beta_0 + \\gamma\\lambda}{\\beta_1}(\\exp(\\beta_1 t) - 1)  \\frac{\\sigma^2 + \\lambda\\gamma^2}{2\\beta_1}(\\exp(2\\beta_1 t) - 1)\n\\end{pmatrix}\n}\n$$", "id": "3062553"}, {"introduction": "Our final practice shifts focus to a pure-jump process—the compound Poisson process—and a calculation with direct applications in fields like finance and insurance. You are tasked with finding the expected value of the positive part of the process, $E[\\max(X_t, 0)]$, which is analogous to calculating the price of a simple financial derivative. This problem ([@problem_id:3062559]) introduces a different but equally important problem-solving method: the law of total expectation, conditioning on the number of jumps. It provides a concrete example of how to reason about the aggregate behavior of a process driven by random events.", "problem": "Consider a compound Poisson process $X_t$ defined by $X_t = \\sum_{i=1}^{N_t} Y_i$, where $N_t$ is a Poisson counting process with rate $\\lambda  0$ and $Y_i$ are independent and identically distributed jump sizes, independent of $N_t$. Assume the jump size distribution is symmetric about $0$ and specifically $Y_i \\sim \\mathcal{N}(0,\\sigma^2)$ for a fixed $\\sigma  0$. Starting from the definitions of a compound Poisson process, the symmetry of the jump distribution, and the law of total expectation, derive an analytic expression for the expectation of the positive part $E[\\max(X_t,0)]$ in terms of $\\lambda$, $t$, and $\\sigma$. Express your final answer as a single closed-form series in $\\lambda t$ with no numerical approximation.", "solution": "The problem statement is a well-posed and scientifically grounded question in the theory of stochastic processes. All necessary parameters and distributions are provided, and there are no internal contradictions or logical flaws. The problem is valid and admits a unique, meaningful solution. We may proceed with the derivation.\n\nThe objective is to find an analytic expression for the expectation of the positive part of a compound Poisson process $X_t$, denoted as $E[\\max(X_t, 0)]$. The process is defined as $X_t = \\sum_{i=1}^{N_t} Y_i$, where $N_t$ is a Poisson process with rate $\\lambda  0$, and the jump sizes $Y_i$ are independent and identically distributed (i.i.d.) random variables with a normal distribution $Y_i \\sim \\mathcal{N}(0, \\sigma^2)$ for $\\sigma  0$. The jump sizes $Y_i$ are also independent of the counting process $N_t$.\n\nThe derivation will proceed by applying the law of total expectation, conditioning on the number of jumps $N_t$ that have occurred by time $t$. The law of total expectation states that for two random variables $A$ and $B$, $E[A] = E[E[A|B]]$. In our case, we compute the expectation of $\\max(X_t, 0)$ by first finding its expectation conditional on the value of $N_t$, and then taking the expectation of that result over the distribution of $N_t$.\n$E[\\max(X_t, 0)] = E[E[\\max(X_t, 0) | N_t]]$\nThe outer expectation is taken over the possible values of $N_t$, which is a discrete random variable. Thus, we can write the expectation as a sum over all possible numbers of jumps $n$:\n$$E[\\max(X_t, 0)] = \\sum_{n=0}^{\\infty} E[\\max(X_t, 0) | N_t = n] P(N_t = n)$$\n\nFirst, we determine the conditional expectation $E[\\max(X_t, 0) | N_t = n]$.\nIf $N_t = n$, the compound Poisson process $X_t$ becomes a sum of a fixed number $n$ of i.i.d. random variables: $X_t |_{N_t=n} = S_n = \\sum_{i=1}^{n} Y_i$.\nSince each $Y_i \\sim \\mathcal{N}(0, \\sigma^2)$, the sum $S_n$ is also a normal random variable. The mean of the sum is the sum of the means, and the variance of the sum of independent variables is the sum of the variances.\n$$E[S_n] = \\sum_{i=1}^{n} E[Y_i] = \\sum_{i=1}^{n} 0 = 0$$\n$$\\text{Var}(S_n) = \\sum_{i=1}^{n} \\text{Var}(Y_i) = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2$$\nThus, conditional on $N_t=n$, the random variable $X_t$ is distributed as $S_n \\sim \\mathcal{N}(0, n\\sigma^2)$. The standard deviation of $S_n$ is $\\sqrt{n\\sigma^2} = \\sigma\\sqrt{n}$. For the case $n=0$, the sum is empty and defined as $S_0=0$. The distribution is a point mass at $0$, which is consistent with $\\mathcal{N}(0,0)$.\n\nWe now calculate $E[\\max(S_n, 0)]$ for $S_n \\sim \\mathcal{N}(0, n\\sigma^2)$. Let $f_{S_n}(x)$ be the probability density function (PDF) of $S_n$.\n$$f_{S_n}(x) = \\frac{1}{\\sqrt{2\\pi n\\sigma^2}} \\exp\\left(-\\frac{x^2}{2n\\sigma^2}\\right)$$\nThe expectation is given by the integral:\n$$E[\\max(S_n, 0)] = \\int_{-\\infty}^{\\infty} \\max(x, 0) f_{S_n}(x) dx = \\int_{0}^{\\infty} x f_{S_n}(x) dx$$\n$$E[\\max(S_n, 0)] = \\int_{0}^{\\infty} x \\frac{1}{\\sigma\\sqrt{2\\pi n}} \\exp\\left(-\\frac{x^2}{2n\\sigma^2}\\right) dx$$\nTo solve this integral, we use the substitution $u = \\frac{x^2}{2n\\sigma^2}$, which implies $du = \\frac{2x}{2n\\sigma^2} dx = \\frac{x}{n\\sigma^2} dx$, so $x dx = n\\sigma^2 du$. The limits of integration remain $0$ to $\\infty$.\n$$E[\\max(S_n, 0)] = \\frac{1}{\\sigma\\sqrt{2\\pi n}} \\int_{0}^{\\infty} \\exp(-u) (n\\sigma^2 du) = \\frac{n\\sigma^2}{\\sigma\\sqrt{2\\pi n}} \\int_{0}^{\\infty} \\exp(-u) du$$\n$$E[\\max(S_n, 0)] = \\frac{\\sigma\\sqrt{n}}{\\sqrt{2\\pi}} [-\\exp(-u)]_{0}^{\\infty} = \\frac{\\sigma\\sqrt{n}}{\\sqrt{2\\pi}} (0 - (-1)) = \\sigma\\sqrt{\\frac{n}{2\\pi}}$$\nThis result is valid for $n  0$. For $n=0$, we have $S_0=0$, so $E[\\max(S_0, 0)] = 0$, which is consistent with the formula.\nAlternatively, one could note that because the distribution of $S_n$ is symmetric about $0$, $E[\\max(S_n, 0)] = \\frac{1}{2}E[|S_n|]$. The expected value of the absolute value of a normal variable with mean $0$ and variance $v^2$ is $v\\sqrt{2/\\pi}$. Here $v^2=n\\sigma^2$, so $E[|S_n|] = \\sigma\\sqrt{n}\\sqrt{2/\\pi} = \\sigma\\sqrt{2n/\\pi}$. This gives $E[\\max(S_n, 0)] = \\frac{1}{2}\\sigma\\sqrt{2n/\\pi} = \\sigma\\sqrt{n/(2\\pi)}$, confirming the result.\n\nNext, we identify the probability mass function (PMF) $P(N_t=n)$. The number of events $N_t$ in a Poisson process with rate $\\lambda$ by time $t$ follows a Poisson distribution with parameter $\\lambda t$.\n$$P(N_t=n) = \\frac{(\\lambda t)^n \\exp(-\\lambda t)}{n!}, \\quad \\text{for } n = 0, 1, 2, \\ldots$$\n\nFinally, we substitute the conditional expectation and the probability into the summation:\n$$E[\\max(X_t, 0)] = \\sum_{n=0}^{\\infty} \\left(\\sigma\\sqrt{\\frac{n}{2\\pi}}\\right) \\left(\\frac{(\\lambda t)^n \\exp(-\\lambda t)}{n!}\\right)$$\nWe can factor out terms that do not depend on the summation index $n$:\n$$E[\\max(X_t, 0)] = \\frac{\\sigma \\exp(-\\lambda t)}{\\sqrt{2\\pi}} \\sum_{n=0}^{\\infty} \\sqrt{n} \\frac{(\\lambda t)^n}{n!}$$\nThe term for $n=0$ is $0$, so the summation can start from $n=1$:\n$$E[\\max(X_t, 0)] = \\frac{\\sigma \\exp(-\\lambda t)}{\\sqrt{2\\pi}} \\sum_{n=1}^{\\infty} \\sqrt{n} \\frac{(\\lambda t)^n}{n!}$$\nThis expression is the required single closed-form series. The sum does not correspond to an elementary function, but it is a well-defined series where each term is an analytic function of the parameters $\\lambda, t, \\sigma$. This satisfies the conditions of the problem.", "answer": "$$\\boxed{\\frac{\\sigma \\exp(-\\lambda t)}{\\sqrt{2\\pi}} \\sum_{n=1}^{\\infty} \\frac{\\sqrt{n}}{n!} (\\lambda t)^n}$$", "id": "3062559"}]}