{"hands_on_practices": [{"introduction": "The power of the Martingale Representation Theorem is built upon the foundation of the Itô stochastic integral. This first exercise takes us back to basics, guiding you to construct an Itô integral for the simplest non-trivial predictable process—a constant. By explicitly computing the integral and its variance [@problem_id:3065258], you will gain a concrete understanding of the Itô isometry, a fundamental property that connects the geometry of Hilbert spaces to stochastic integration and is central to the proof of representation theorems.", "problem": "Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$ satisfying the usual conditions, where $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$ is the completed natural filtration of $W$. Fix $T>0$ and a constant $a \\in \\mathbb{R}$. Consider the predictable process $H_{t} \\equiv a$ for all $t \\in [0,T]$ and the Itô integral $\\int_{0}^{T} H_{s}\\,dW_{s}$. Starting from the definition of the Itô integral for simple predictable processes and the basic properties of Brownian motion (in particular, independent Gaussian increments with variance equal to the time increment), do the following:\n\n- Compute $\\int_{0}^{T} H_{s}\\,dW_{s}$ explicitly in terms of $a$ and $W$.\n- Compute its variance under $\\mathbb{P}$.\n- Verify directly, without assuming it a priori, that the Itô isometry holds in this case by showing that $\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T} H_{s}\\,dW_{s}\\right)^{2}\\right]$ equals $\\int_{0}^{T}\\mathbb{E}[H_{s}^{2}]\\,ds$.\n\nYour final answer must be given as a single row matrix containing, in this order, the expression for the Itô integral and its variance, both in closed form as functions of $a$ and $T$. No rounding is required.", "solution": "The problem requires us to compute the Itô integral of a constant predictable process, find its variance, and verify the Itô isometry for this specific case. Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$, with $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$ being the completed natural filtration of $W$. By definition, a standard Brownian motion starts at zero, so $W_0=0$ almost surely. Its increments $W_t - W_s$ over non-overlapping time intervals are independent, and for $s < t$, the increment $W_t - W_s$ follows a normal distribution with mean $0$ and variance $t-s$, denoted $N(0, t-s)$. The process $H_t = a$ for $a \\in \\mathbb{R}$ is a deterministic, hence predictable, process.\n\nThe problem asks to proceed from the definition of the Itô integral for simple predictable processes. A general predictable process $H_{s}$ is approximated by a sequence of simple predictable processes. For a simple process of the form $K_t = \\sum_{i=0}^{n-1} Y_i \\mathbf{1}_{(t_i, t_{i+1}]}(t)$, where $0 = t_0 < t_1 < \\dots < t_n = T$ and each $Y_i$ is an $\\mathcal{F}_{t_i}$-measurable random variable, the Itô integral is defined as $\\int_{0}^{T} K_s \\,dW_s = \\sum_{i=0}^{n-1} Y_i (W_{t_{i+1}} - W_{t_i})$. For a general process $H$, we find a sequence of simple processes $H^{(k)}$ such that $\\mathbb{E}[\\int_{0}^{T} (H_t - H_t^{(k)})^2 dt] \\to 0$ as $k \\to \\infty$. The Itô integral $\\int_{0}^{T} H_s \\,dW_s$ is then defined as the limit in $L^2(\\Omega)$ of the integrals $\\int_{0}^{T} H_s^{(k)} \\,dW_s$.\n\nLet's construct an approximating sequence for $H_t = a$. For any partition $\\pi_n = \\{0 = t_0 < t_1 < \\dots < t_n = T\\}$ of the interval $[0, T]$, we can define a simple process\n$$ H_t^{(n)} = \\sum_{i=0}^{n-1} H_{t_i} \\mathbf{1}_{(t_i, t_{i+1}]}(t) $$\nSince $H_t = a$ for all $t$, we have $H_{t_i} = a$ for all $i$. Thus,\n$$ H_t^{(n)} = \\sum_{i=0}^{n-1} a \\cdot \\mathbf{1}_{(t_i, t_{i+1}]}(t) = a \\cdot \\mathbf{1}_{(0, T]}(t) $$\nThis approximating process is actually independent of the specific partition $\\pi_n$, as long as it spans $[0,T]$.\n\n**Part 1: Computation of the Itô Integral**\n\nThe Itô integral for the simple process $H_t^{(n)}$ is given by the definition:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = \\sum_{i=0}^{n-1} a (W_{t_{i+1}} - W_{t_i}) $$\nWe can factor out the constant $a$:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = a \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i}) $$\nThe sum is a telescoping series:\n$$ \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i}) = (W_{t_1} - W_{t_0}) + (W_{t_2} - W_{t_1}) + \\dots + (W_{t_n} - W_{t_{n-1}}) = W_{t_n} - W_{t_0} $$\nGiven that $t_n = T$ and $t_0 = 0$, and standard Brownian motion starts at $0$ (i.e., $W_0 = 0$ a.s.), the sum simplifies to $W_T - W_0 = W_T$.\nTherefore, for any partition $\\pi_n$, the integral of the approximating simple process is:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = a W_T $$\nThe Itô integral of $H_s = a$ is the $L^2$ limit of these integrals. Since the integral for every approximating simple process is the same constant random variable $a W_T$, the limit is trivially $a W_T$.\n$$ \\int_{0}^{T} H_s \\,dW_s = \\int_{0}^{T} a \\,dW_s = a W_T $$\n\n**Part 2: Computation of the Variance**\n\nThe random variable representing the integral is $I_T = a W_T$. We need to compute its variance, $\\text{Var}(I_T)$.\nUsing the properties of variance, for a constant $c$ and a random variable $X$, $\\text{Var}(cX) = c^2 \\text{Var}(X)$.\n$$ \\text{Var}(I_T) = \\text{Var}(a W_T) = a^2 \\text{Var}(W_T) $$\nFor a standard Brownian motion, the random variable $W_T$ is normally distributed with mean $\\mathbb{E}[W_T] = 0$ and variance $\\text{Var}(W_T) = T$.\nSubstituting the variance of $W_T$:\n$$ \\text{Var}(I_T) = a^2 T $$\n\n**Part 3: Verification of the Itô Isometry**\n\nThe Itô isometry states that for a predictable process $K_s$, $\\mathbb{E}\\left[\\left(\\int_{0}^{T} K_s \\,dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{T} K_s^2 \\,ds\\right]$. The problem asks to verify this for $H_s = a$ by showing that $\\mathbb{E}\\left[\\left(\\int_{0}^{T} H_s \\,dW_s\\right)^2\\right] = \\int_{0}^{T} \\mathbb{E}[H_s^2] \\,ds$.\n\nLet's compute the left-hand side (LHS) and the right-hand side (RHS) separately.\n\n**Left-Hand Side (LHS):**\nThe LHS is $\\mathbb{E}\\left[\\left(\\int_{0}^{T} H_s \\,dW_s\\right)^2\\right]$. From Part 1, we have $\\int_{0}^{T} H_s \\,dW_s = a W_T$.\nSo,\n$$ \\text{LHS} = \\mathbb{E}[(a W_T)^2] = \\mathbb{E}[a^2 W_T^2] = a^2 \\mathbb{E}[W_T^2] $$\nThe variance of $W_T$ is defined as $\\text{Var}(W_T) = \\mathbb{E}[W_T^2] - (\\mathbb{E}[W_T])^2$.\nWe know $\\text{Var}(W_T) = T$ and $\\mathbb{E}[W_T] = 0$.\nTherefore, $T = \\mathbb{E}[W_T^2] - 0^2$, which implies $\\mathbb{E}[W_T^2] = T$.\nSubstituting this into the expression for the LHS:\n$$ \\text{LHS} = a^2 T $$\nTo be more rigorous and follow the instruction to verify this \"directly\", we can compute the second moment of the integral of the simple approximating process $H_t^{(n)}$ over a partition $\\pi_n = \\{0=t_0, \\dots, t_n=T\\}$. Let $\\Delta W_i = W_{t_{i+1}} - W_{t_i}$.\n$$ \\mathbb{E}\\left[\\left(\\int_0^T H_s^{(n)} \\,dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{i=0}^{n-1} a \\Delta W_i\\right)^2\\right] = a^2 \\mathbb{E}\\left[\\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} \\Delta W_i \\Delta W_j \\right] $$\nBy linearity of expectation:\n$$ a^2 \\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} \\mathbb{E}[\\Delta W_i \\Delta W_j] $$\nThe increments of Brownian motion are independent over non-overlapping intervals. Thus, for $i \\ne j$, $\\Delta W_i$ and $\\Delta W_j$ are independent.\nSince $\\mathbb{E}[\\Delta W_i] = \\mathbb{E}[W_{t_{i+1}} - W_{t_i}] = 0$ for all $i$, we have for $i \\ne j$:\n$$ \\mathbb{E}[\\Delta W_i \\Delta W_j] = \\mathbb{E}[\\Delta W_i] \\mathbb{E}[\\Delta W_j] = 0 \\cdot 0 = 0 $$\nThe double summation reduces to the terms where $i=j$:\n$$ a^2 \\sum_{i=0}^{n-1} \\mathbb{E}[(\\Delta W_i)^2] $$\nSince $\\mathbb{E}[\\Delta W_i] = 0$, we have $\\mathbb{E}[(\\Delta W_i)^2] = \\text{Var}(\\Delta W_i) = t_{i+1} - t_i$.\nThe expression becomes:\n$$ a^2 \\sum_{i=0}^{n-1} (t_{i+1} - t_i) = a^2 (t_n - t_0) = a^2 (T - 0) = a^2 T $$\nSince this result holds for any partition, the second moment of the limit is also $a^2 T$. This rigorously confirms our LHS calculation.\n\n**Right-Hand Side (RHS):**\nThe RHS is $\\int_{0}^{T}\\mathbb{E}[H_{s}^{2}]\\,ds$.\nThe process is $H_s = a$, which is a constant.\nSo, $H_s^2 = a^2$.\nSince $a^2$ is a deterministic constant, its expectation is itself: $\\mathbb{E}[H_s^2] = \\mathbb{E}[a^2] = a^2$.\nNow we compute the integral:\n$$ \\text{RHS} = \\int_{0}^{T} a^2 \\,ds $$\nSince $a^2$ is a constant with respect to the integration variable $s$, we have:\n$$ \\text{RHS} = a^2 \\int_{0}^{T} 1 \\,ds = a^2 [s]_{0}^{T} = a^2(T-0) = a^2 T $$\n\n**Conclusion of Verification:**\nWe have found that LHS $= a^2 T$ and RHS $= a^2 T$. Thus, LHS $=$ RHS, which verifies that the Itô isometry holds for the constant process $H_t = a$.\n\nThe final answer requires the expression for the Itô integral and its variance.\nExpression for the Itô integral: $aW_T$.\nVariance of the Itô integral: $a^2 T$.", "answer": "$$ \\boxed{ \\begin{pmatrix} aW_T & a^2 T \\end{pmatrix} } $$", "id": "3065258"}, {"introduction": "With the foundational tools in place, we now apply the Predictable Representation Property to a classic and important problem: finding the hedging strategy for a financial derivative with a polynomial payoff. This practice [@problem_id:3065239] demonstrates a powerful technique for identifying the unique predictable integrand $H_t$ for a martingale of the form $M_t = \\mathbb{E}[\\xi \\mid \\mathcal{F}_t]$. You will uncover the deep connection between conditional expectations, the backward heat equation, and special functions like Hermite polynomials, revealing a practical toolkit for solving representation problems.", "problem": "Let $(W_t)_{t\\ge 0}$ be a one-dimensional standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ with the natural filtration. Fix $T>0$ and an integer $n\\ge 1$. Consider the random variable $\\xi = W_T^n$ and the martingale $M_t = \\mathbb{E}\\!\\left[\\xi \\mid \\mathcal{F}_t\\right]$. The Brownian filtration has the Predictable Representation Property (PRP), meaning any square-integrable $(\\mathcal{F}_t)$-martingale can be represented as a stochastic integral with respect to $W$. \n\nStarting from the Markov property of Brownian motion, the independence and Gaussianity of increments, the generator characterization of the heat semigroup, and Itô’s formula, carry out the following:\n\n- Show that there exists a function $u:[0,T]\\times \\mathbb{R}\\to \\mathbb{R}$ with $M_t = u(t,W_t)$, where $u$ solves the backward heat equation $u_t + \\tfrac{1}{2}u_{xx} = 0$ with terminal condition $u(T,x) = x^n$.\n\n- Derive an explicit closed-form expression for $u(t,x)$ by writing $u(t,x) = \\mathbb{E}\\!\\left[(x+Z)^n\\right]$ with $Z \\sim \\mathcal{N}(0,T-t)$ independent of $W_t$, and evaluating the Gaussian moments. Identify the resulting polynomial family in $x$ as scaled probabilists’ Hermite polynomials.\n\n- Using the backward heat equation and Itô’s formula, deduce the martingale representation $M_t = M_0 + \\int_0^t H_s\\,\\mathrm{d}W_s$ and determine the integrand $H_t$ as $H_t = u_x(t,W_t)$.\n\nCompute $H_t$ explicitly for the cases $n=1$, $n=2$, $n=3$, and $n=4$. Provide your final answer as a single row vector containing the four expressions for $H_t$ in the order $n=1,2,3,4$. No approximation or rounding is required.", "solution": "Let $(W_t)_{t \\ge 0}$ be a standard one-dimensional Brownian motion on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$, with $(\\mathcal{F}_t)_{t \\ge 0}$ being the natural filtration generated by $W$. We are given a terminal time $T>0$, an integer $n \\ge 1$, and a random variable $\\xi = W_T^n$. We consider the martingale $M_t = \\mathbb{E}[\\xi \\mid \\mathcal{F}_t]$ for $t \\in [0,T]$.\n\n### Part 1: PDE Formulation for the Martingale\n\nThe martingale $M_t$ is the conditional expectation of $\\xi = W_T^n$ given the information available at time $t$, which is represented by the sigma-algebra $\\mathcal{F}_t$. Due to the Markov property of Brownian motion, this conditional expectation depends only on the current value of the process, $W_t$.\n$$\nM_t = \\mathbb{E}[W_T^n \\mid \\mathcal{F}_t] = \\mathbb{E}[W_T^n \\mid W_t]\n$$\nThis implies that there exists a deterministic function $u(t,x)$ such that $M_t = u(t, W_t)$. To find this function, we express the conditional expectation explicitly.\n$$\nu(t,x) = \\mathbb{E}[W_T^n \\mid W_t = x]\n$$\nWe can write $W_T = W_t + (W_T - W_t)$. The increment $W_T - W_t$ is independent of $\\mathcal{F}_t$ and thus independent of $W_t$. The distribution of this increment is Gaussian with mean $0$ and variance $T-t$. So, conditioning on $W_t = x$, the random variable $W_T$ has the same distribution as $x + Z$, where $Z \\sim \\mathcal{N}(0, T-t)$.\n$$\nu(t,x) = \\mathbb{E}[(x+Z)^n] \\quad \\text{where } Z \\sim \\mathcal{N}(0, T-t)\n$$\nThis is an application of the Feynman-Kac theorem. The generator of a standard one-dimensional Brownian motion is the operator $\\mathcal{A} = \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2}$. The function $u(t,x)$ defined as $\\mathbb{E}[f(X_T) \\mid X_t=x]$ for a diffusion $X$ with generator $\\mathcal{A}$ and terminal value $f(x)$ must satisfy the backward Kolmogorov equation $(\\partial_t + \\mathcal{A})u = 0$.\nIn our case, with $X_t = W_t$ and $f(x) = x^n$, the function $u(t,x)$ must satisfy:\n$$\n\\frac{\\partial u}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} = 0, \\quad \\text{for } (t,x) \\in [0,T) \\times \\mathbb{R}\n$$\nThis is the backward heat equation. The terminal condition at $t=T$ is given by:\n$$\nu(T,x) = \\mathbb{E}[W_T^n \\mid W_T=x] = x^n\n$$\nThus, we have shown that $M_t = u(t,W_t)$, where $u(t,x)$ is the solution to the backward heat equation $u_t + \\frac{1}{2} u_{xx} = 0$ with the terminal condition $u(T,x) = x^n$.\n\n### Part 2: Explicit Expression for $u(t,x)$\n\nWe derive the explicit form of $u(t,x)$ by evaluating the expectation $u(t,x) = \\mathbb{E}[(x+Z)^n]$, where $Z \\sim \\mathcal{N}(0, T-t)$. Let $\\sigma^2 = T-t$.\nUsing the binomial theorem, we have:\n$$\nu(t,x) = \\mathbb{E}\\left[\\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} Z^k\\right] = \\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} \\mathbb{E}[Z^k]\n$$\nThe moments of a centered Gaussian random variable $Z \\sim \\mathcal{N}(0,\\sigma^2)$ are given by:\n$$\n\\mathbb{E}[Z^k] =\n\\begin{cases}\n0 & \\text{if } k \\text{ is odd} \\\\\n\\sigma^k (k-1)!! = \\sigma^k \\frac{k!}{(k/2)! 2^{k/2}} & \\text{if } k \\text{ is even}\n\\end{cases}\n$$\nSubstituting $k=2j$ for the even-indexed terms, the sum becomes:\n$$\nu(t,x) = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\binom{n}{2j} x^{n-2j} \\mathbb{E}[Z^{2j}] = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(2j)!(n-2j)!} x^{n-2j} (T-t)^j \\frac{(2j)!}{j! 2^j}\n$$\nSimplifying the expression, we obtain the closed-form for $u(t,x)$:\n$$\nu(t,x) = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(n-2j)! j!} \\left(\\frac{T-t}{2}\\right)^j x^{n-2j}\n$$\nThis family of polynomials in $x$ with parameter $\\tau = T-t$ are known as the heat polynomials, which we denote by $p_n(x, \\tau)$. So, $u(t,x) = p_n(x, T-t)$. These polynomials are a generalization of the probabilists' Hermite polynomials $He_n(x)$, which can be recovered by setting the time parameter to $-1$, i.e., $He_n(x) = p_n(x,-1)$.\n\n### Part 3: Martingale Representation and the Integrand $H_t$\n\nThe Predictable Representation Property (PRP) states that any $(\\mathcal{F}_t)$-martingale (in this case, since $\\xi=W_T^n \\in L^2$, $M_t$ is a square-integrable martingale) can be written as a stochastic integral with respect to the Brownian motion $W$. We seek the predictable process $H_s$ such that $M_t = M_0 + \\int_0^t H_s \\, \\mathrm{d}W_s$.\n\nWe apply Itô's formula to the process $M_t = u(t,W_t)$:\n$$\n\\mathrm{d}M_t = \\frac{\\partial u}{\\partial t}(t,W_t)\\,\\mathrm{d}t + \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,W_t)\\,\\mathrm{d}\\langle W,W \\rangle_t\n$$\nSince $\\mathrm{d}\\langle W,W \\rangle_t = \\mathrm{d}t$, we can group the terms:\n$$\n\\mathrm{d}M_t = \\left(\\frac{\\partial u}{\\partial t}(t,W_t) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,W_t)\\right)\\,\\mathrm{d}t + \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t\n$$\nFrom Part 1, we know that $u(t,x)$ solves the backward heat equation $u_t + \\frac{1}{2}u_{xx} = 0$. Therefore, the drift term (the term multiplying $\\mathrm{d}t$) is zero.\n$$\n\\mathrm{d}M_t = \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t\n$$\nIntegrating from $s=0$ to $s=t$ yields the integral representation:\n$$\nM_t - M_0 = \\int_0^t \\frac{\\partial u}{\\partial x}(s,W_s)\\,\\mathrm{d}W_s\n$$\nBy comparing this with the general form $M_t = M_0 + \\int_0^t H_s\\,\\mathrm{d}W_s$, we identify the integrand as $H_s = \\frac{\\partial u}{\\partial x}(s,W_s)$. At time $t$, the integrand is $H_t = \\frac{\\partial u}{\\partial x}(t,W_t)$.\n\nTo find $H_t$ explicitly, we compute the derivative of $u(t,x) = p_n(x, T-t)$ with respect to $x$. Let $\\tau=T-t$.\n$$\n\\frac{\\partial u}{\\partial x}(t,x) = \\frac{\\partial}{\\partial x} p_n(x, \\tau) = \\frac{\\partial}{\\partial x} \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(n-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-2j}\n$$\n$$\n= \\sum_{j=0}^{\\lfloor (n-1)/2 \\rfloor} \\frac{n! (n-2j)}{(n-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-2j-1} = n \\sum_{j=0}^{\\lfloor (n-1)/2 \\rfloor} \\frac{(n-1)!}{(n-1-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-1-2j}\n$$\nThe resulting sum is precisely $n \\cdot p_{n-1}(x,\\tau)$. Thus, $\\frac{\\partial u}{\\partial x}(t,x) = n \\cdot p_{n-1}(x, T-t)$.\nThe integrand process is therefore $H_t = n \\cdot p_{n-1}(W_t, T-t)$.\n\n### Part 4: Explicit Computation of $H_t$ for $n=1,2,3,4$\n\nWe use the formula $H_t = n \\cdot p_{n-1}(W_t, T-t)$ and the explicit forms of the heat polynomials $p_k(x,\\tau) = \\sum_{j=0}^{\\lfloor k/2 \\rfloor} \\frac{k!}{(k-2j)! j!} (\\frac{\\tau}{2})^j x^{k-2j}$.\nLet $x=W_t$ and $\\tau=T-t$.\n\n- For $n=1$:\n$H_t = 1 \\cdot p_0(x, \\tau)$.\n$p_0(x,\\tau) = \\frac{0!}{0!0!}(\\frac{\\tau}{2})^0 x^0 = 1$.\nSo, $H_t = 1$.\n\n- For $n=2$:\n$H_t = 2 \\cdot p_1(x, \\tau)$.\n$p_1(x,\\tau) = \\frac{1!}{1!0!}(\\frac{\\tau}{2})^0 x^1 = x$.\nSo, $H_t = 2x = 2W_t$.\n\n- For $n=3$:\n$H_t = 3 \\cdot p_2(x, \\tau)$.\n$p_2(x,\\tau) = \\frac{2!}{2!0!}(\\frac{\\tau}{2})^0 x^2 + \\frac{2!}{0!1!}(\\frac{\\tau}{2})^1 x^0 = x^2 + \\tau$.\nSo, $H_t = 3(x^2 + \\tau) = 3(W_t^2 + T-t)$.\n\n- For $n=4$:\n$H_t = 4 \\cdot p_3(x, \\tau)$.\n$p_3(x,\\tau) = \\frac{3!}{3!0!}(\\frac{\\tau}{2})^0 x^3 + \\frac{3!}{1!1!}(\\frac{\\tau}{2})^1 x^1 = x^3 + 3\\tau x$.\nSo, $H_t = 4(x^3 + 3\\tau x) = 4(W_t^3 + 3(T-t)W_t)$.\n\nThese are the explicit expressions for the integrand $H_t$ for $n=1,2,3,4$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2W_t & 3(W_t^2 + T-t) & 4(W_t^3 + 3(T-t)W_t) \\end{pmatrix}}\n$$", "id": "3065239"}, {"introduction": "Mathematical theorems are powerful but come with crucial assumptions that define their scope. This final practice explores the boundaries of the $L^2$-based Predictable Representation Property by focusing on the condition of square-integrability. By examining a martingale that fails this condition [@problem_id:3065262], you will appreciate why the Hilbert space structure of square-integrable martingales is so essential and learn the importance of verifying a theorem's hypotheses before applying it.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ satisfying the usual conditions and supporting both a standard Brownian motion $W=(W_t)_{t\\ge 0}$ and a unit-rate Poisson process $N=(N_t)_{t\\ge 0}$. Let $(Y_k)_{k\\in\\mathbb{N}}$ be an independent and identically distributed sequence, independent of $N$, with a symmetric heavy-tailed law such that $\\mathbb{E}[|Y_1|]<\\infty$ and $\\mathbb{E}[Y_1^2]=\\infty$. Define a process $M=(M_t)_{t\\ge 0}$ by $M_t=\\sum_{k=1}^{N_t} Y_k$ with respect to the natural filtration generated by $\\{N_s,Y_k: s\\le t, k\\le N_t\\}$. Recall the fundamental definitions: a process $X=(X_t)_{t\\ge 0}$ adapted to $(\\mathcal{F}_t)_{t\\ge 0}$ is a martingale if $\\mathbb{E}[|X_t|]<\\infty$ for all $t\\ge 0$ and $\\mathbb{E}[X_t\\mid \\mathcal{F}_s]=X_s$ for all $0\\le s\\le t$, and a martingale is square-integrable if $\\mathbb{E}[X_t^2]<\\infty$ for all $t\\ge 0$. Representation results such as the predictable representation property (PRP) in classical settings (for example, the Brownian filtration) are stated for square-integrable martingales and rely on integrability bounds in $L^2$.\n\nWhich of the following options correctly provides an example of a non-square-integrable martingale together with a valid explanation for why $L^2$-based representation results (such as PRP) do not apply to it?\n\nA. The process $M_t=\\sum_{k=1}^{N_t} Y_k$ is a martingale with $\\mathbb{E}[|M_t|]<\\infty$ but $\\mathbb{E}[M_t^2]=\\infty$ for any $t>0$, because the heavy-tailed jumps have infinite second moment; therefore, $M$ is not in $L^2$ and $L^2$-based representation theorems do not apply.\n\nB. The process $M_t=e^{W_t}$ is a non-square-integrable martingale since exponentials of Brownian motion always have infinite second moment; therefore, $L^2$-based representation fails.\n\nC. The process $M_t=W_t^3$ is a martingale with infinite variance, so it is not in $L^2$ and cannot be represented via $L^2$-based PRP.\n\nD. The process $M_t=\\int_0^t s^{-1/2}\\,dW_s$ is a martingale that is not square-integrable because the integrand is not in $L^2([0,t])$, so PRP does not apply.", "solution": "The question requires identifying a correct example of a martingale that is not square-integrable, along with a valid explanation for why standard $L^2$-based representation theorems do not apply to it. We will proceed by first recalling the necessary definitions and then analyzing each option systematically.\n\nA process $X=(X_t)_{t\\ge 0}$ is a martingale with respect to a filtration $(\\mathcal{F}_t)_{t\\ge 0}$ if it is adapted to the filtration, $\\mathbb{E}[|X_t|]<\\infty$ for all $t\\ge 0$, and for all $0 \\le s \\le t$, $\\mathbb{E}[X_t\\mid \\mathcal{F}_s]=X_s$.\nA martingale $X$ is termed square-integrable if, in addition, $\\mathbb{E}[X_t^2]<\\infty$ for all $t\\ge 0$.\n\n$L^2$-based representation theorems, such as the predictable representation property (PRP) for Brownian martingales, are formulated for the space of square-integrable martingales. The structure of this space as a Hilbert space is fundamental to the standard proofs, which rely on tools like the Itô isometry. Consequently, if a martingale is not square-integrable (i.e., not in $L^2$), these specific theorems and their associated proofs are not directly applicable.\n\nWe now evaluate each option.\n\n### Option A\n\nThe process is given by $M_t=\\sum_{k=1}^{N_t} Y_k$, a compound Poisson process. The jumps $(Y_k)_{k\\in\\mathbb{N}}$ are i.i.d., independent of the unit-rate Poisson process $N_t$, and have a symmetric law with $\\mathbb{E}[|Y_1|]<\\infty$ and $\\mathbb{E}[Y_1^2]=\\infty$. The filtration is the natural one for this process.\n\n1.  **Is $M_t$ a martingale?**\n    *   Adaptivity: $M_t$ is adapted to the specified natural filtration by definition.\n    *   $L^1$-integrability: We check if $\\mathbb{E}[|M_t|]<\\infty$.\n        Using the triangle inequality and Wald's identity (which applies since $N_t$ is a stopping time for the sequence $(|Y_k|)_{k\\ge 1}$ and has finite expectation, and the $|Y_k|$ are i.i.d.):\n        $$ \\mathbb{E}[|M_t|] = \\mathbb{E}\\left[\\left|\\sum_{k=1}^{N_t} Y_k\\right|\\right] \\le \\mathbb{E}\\left[\\sum_{k=1}^{N_t} |Y_k|\\right] = \\mathbb{E}[N_t]\\mathbb{E}[|Y_1|] $$\n        Since $N_t$ is a unit-rate Poisson process, $\\mathbb{E}[N_t]=t$. The problem states $\\mathbb{E}[|Y_1|]<\\infty$. Thus, $\\mathbb{E}[|M_t|] \\le t \\mathbb{E}[|Y_1|] < \\infty$ for any finite $t$.\n    *   Martingale property: The symmetry of the law of $Y_k$ and $\\mathbb{E}[|Y_1|]<\\infty$ imply that the expectation exists and is zero, $\\mathbb{E}[Y_1]=0$. For $0 \\le s \\le t$:\n        $$ \\mathbb{E}[M_t\\mid \\mathcal{F}_s] = \\mathbb{E}\\left[\\sum_{k=1}^{N_t} Y_k \\bigg| \\mathcal{F}_s\\right] = \\mathbb{E}\\left[\\sum_{k=1}^{N_s} Y_k + \\sum_{k=N_s+1}^{N_t} Y_k \\bigg| \\mathcal{F}_s\\right] $$\n        Since $\\sum_{k=1}^{N_s} Y_k = M_s$ is $\\mathcal{F}_s$-measurable:\n        $$ = M_s + \\mathbb{E}\\left[\\sum_{k=N_s+1}^{N_t} Y_k \\bigg| \\mathcal{F}_s\\right] $$\n        The increment $\\sum_{k=N_s+1}^{N_t} Y_k$ depends on $N_t-N_s$ and $Y_k$ for $k>N_s$. These are independent of $\\mathcal{F}_s$. Therefore, the conditional expectation is the unconditional expectation:\n        $$ \\mathbb{E}\\left[\\sum_{k=N_s+1}^{N_t} Y_k\\right] = \\mathbb{E}\\left[\\sum_{j=1}^{N_t-N_s} Y_{N_s+j}\\right] = \\mathbb{E}[N_t-N_s]\\mathbb{E}[Y_1] $$\n        Since $\\mathbb{E}[N_t-N_s] = t-s$ and $\\mathbb{E}[Y_1]=0$, this term is $0$.\n        Thus, $\\mathbb{E}[M_t\\mid \\mathcal{F}_s] = M_s$.\n        Conclusion: $M_t$ is a martingale.\n\n2.  **Is $M_t$ square-integrable?**\n    We compute $\\mathbb{E}[M_t^2]$ using the law of total expectation (tower property):\n    $$ \\mathbb{E}[M_t^2] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\left(\\sum_{k=1}^{N_t} Y_k\\right)^2 \\bigg| N_t\\right]\\right] $$\n    Given $N_t=n$, the inner expectation is $\\mathbb{E}[(\\sum_{k=1}^n Y_k)^2]$. Since the $Y_k$ are i.i.d. with $\\mathbb{E}[Y_k]=0$, for $i \\neq j$, $\\mathbb{E}[Y_i Y_j] = \\mathbb{E}[Y_i]\\mathbb{E}[Y_j] = 0$.\n    $$ \\mathbb{E}\\left[\\left(\\sum_{k=1}^n Y_k\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{k=1}^n Y_k^2 + \\sum_{i\\neq j} Y_i Y_j\\right] = \\sum_{k=1}^n \\mathbb{E}[Y_k^2] = n\\mathbb{E}[Y_1^2] $$\n    So, $\\mathbb{E}[M_t^2 \\mid N_t] = N_t \\mathbb{E}[Y_1^2]$. Taking the outer expectation:\n    $$ \\mathbb{E}[M_t^2] = \\mathbb{E}[N_t \\mathbb{E}[Y_1^2]] = \\mathbb{E}[N_t] \\mathbb{E}[Y_1^2] = t \\mathbb{E}[Y_1^2] $$\n    The problem states $\\mathbb{E}[Y_1^2]=\\infty$. Therefore, for any $t>0$, $\\mathbb{E}[M_t^2]=\\infty$. The martingale $M_t$ is not square-integrable.\n\n3.  **Validity of the explanation:** The reason given is that the heavy-tailed jumps have an infinite second moment. Our calculation confirms this is precisely why $\\mathbb{E}[M_t^2]=\\infty$. The conclusion is that $M$ is not in $L^2$ and $L^2$-based representation theorems do not apply. This is correct, as these theorems are premised on the martingale being square-integrable.\n\nVerdict for A: **Correct**.\n\n### Option B\n\nThe process is $M_t=e^{W_t}$.\n1.  **Is $M_t$ a martingale?** For $0 \\le s \\le t$,\n    $$ \\mathbb{E}[M_t\\mid \\mathcal{F}_s] = \\mathbb{E}[e^{W_t}\\mid \\mathcal{F}_s] = \\mathbb{E}[e^{W_s + (W_t-W_s)}\\mid \\mathcal{F}_s] = e^{W_s} \\mathbb{E}[e^{W_t-W_s}] $$\n    The increment $W_t-W_s$ is a normal random variable $N(0, t-s)$, independent of $\\mathcal{F}_s$. Its moment-generating function is $\\mathbb{E}[e^{u(W_t-W_s)}] = e^{\\frac{1}{2}u^2(t-s)}$. With $u=1$:\n    $$ \\mathbb{E}[e^{W_t-W_s}] = e^{\\frac{1}{2}(t-s)} $$\n    So, $\\mathbb{E}[M_t\\mid \\mathcal{F}_s] = e^{W_s} e^{\\frac{1}{2}(t-s)}$. For $t>s$, this is strictly greater than $M_s=e^{W_s}$. The process $M_t=e^{W_t}$ is a strict submartingale, not a martingale. The process $e^{W_t-\\frac{1}{2}t}$ is a martingale.\n\n2.  **Is $M_t$ non-square-integrable?** We check $\\mathbb{E}[M_t^2]$:\n    $$ \\mathbb{E}[M_t^2] = \\mathbb{E}[(e^{W_t})^2] = \\mathbb{E}[e^{2W_t}] $$\n    Using the MGF of $W_t \\sim N(0,t)$ with $u=2$:\n    $$ \\mathbb{E}[e^{2W_t}] = e^{\\frac{1}{2}(2^2)t} = e^{2t} $$\n    For any finite $t$, $e^{2t}<\\infty$. Thus, the process is square-integrable. The claim that it is non-square-integrable is false.\n\nThe option is incorrect on two fundamental points: the process is not a martingale, and it is square-integrable.\n\nVerdict for B: **Incorrect**.\n\n### Option C\n\nThe process is $M_t=W_t^3$.\n1.  **Is $M_t$ a martingale?** Using Itô's formula for $f(x)=x^3$, we get $df(W_t) = f'(W_t)dW_t + \\frac{1}{2}f''(W_t)dt$. With $f'(x)=3x^2$ and $f''(x)=6x$:\n    $$ d(W_t^3) = 3W_t^2 dW_t + 3W_t dt $$\n    The process has a non-zero drift term $3W_t dt$, so it is not a martingale. The process $W_t^3 - 3\\int_0^t W_s ds$ is a local martingale, and $W_t^3 - 3tW_t$ is the corresponding Itô-Hermite polynomial martingale.\n\n2.  **Does it have infinite variance?** The variance is $\\text{Var}(M_t) = \\mathbb{E}[M_t^2] - (\\mathbb{E}[M_t])^2$.\n    The odd moments of a centered normal distribution are zero, so $\\mathbb{E}[M_t] = \\mathbb{E}[W_t^3]=0$.\n    The second moment is $\\mathbb{E}[M_t^2] = \\mathbb{E}[W_t^6]$. For a standard normal variable $Z \\sim N(0,1)$, $\\mathbb{E}[Z^6]=15$. As $W_t = \\sqrt{t}Z$ in distribution:\n    $$ \\mathbb{E}[W_t^6] = \\mathbb{E}[(\\sqrt{t}Z)^6] = t^3\\mathbb{E}[Z^6] = 15t^3 $$\n    This is finite for any finite $t$. The process does not have infinite variance; it is square-integrable and indeed in $L^p$ for all $p \\ge 1$.\n\nThe option is incorrect as the process is not a martingale and it is square-integrable.\n\nVerdict for C: **Incorrect**.\n\n### Option D\n\nThe process is $M_t=\\int_0^t s^{-1/2}\\,dW_s$.\n1.  **Is $M_t$ a martingale?** A stochastic integral $I_t = \\int_0^t H_s dW_s$ with a deterministic integrand $H_s$ is a Gaussian process. For it to be a square-integrable martingale, it is required that $\\int_0^t H_s^2 ds < \\infty$. Here, $H_s = s^{-1/2}$, so we check:\n    $$ \\int_0^t H_s^2 ds = \\int_0^t (s^{-1/2})^2 ds = \\int_0^t s^{-1} ds = [\\ln s]_0^t = \\infty $$\n    This condition fails for any $t>0$. Therefore, the process is not a square-integrable martingale. The standard Itô $L^2$-theory does not define this integral as an element of the space of square-integrable martingales. In fact, a condition for the integral $\\int_0^t H_s dW_s$ to be defined even pathwise is $\\int_0^t H_s^2 ds < \\infty$ almost surely. For our deterministic $H_s$, this is not satisfied. More advanced theories can define such objects, typically as local martingales. However, this process is known to be a *strict* local martingale, meaning it is a local martingale that is not a true martingale. A true martingale must be uniformly integrable, a condition which fails here. The problem's definition requires $\\mathbb{E}[|X_t|]<\\infty$, which is not satisfied. The claim that this process \"is a martingale\" is false under the standard definition of a true martingale.\n\n2.  **Validity of the explanation:** The statement says $M_t$ is not square-integrable because the integrand is not in $L^2([0,t])$. This refers to the space of functions $f$ on $[0,t]$ such that $\\int_0^t f(s)^2 ds < \\infty$. As shown, $H_s=s^{-1/2}$ is not in this space. This reasoning correctly identifies why the process is not a *square-integrable martingale*. However, the initial premise, that the process is a martingale at all, is false.\n\nThe option incorrectly identifies a strict local martingale as a martingale.\n\nVerdict for D: **Incorrect**.\n\n### Conclusion\n\nOnly option A provides a process that is a true martingale, is not square-integrable, and offers a correct explanation for these properties and their consequences for representation theorems.", "answer": "$$\\boxed{A}$$", "id": "3065262"}]}