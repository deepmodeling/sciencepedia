{"hands_on_practices": [{"introduction": "Compound Poisson processes are a cornerstone for modeling systems that evolve through sudden, random jumps. This first practice bridges abstract theory and concrete application by tasking you with both deriving fundamental properties and implementing a simulation. By translating the process's definition into a computational algorithm, you will make the random sum $X_t = \\sum_{i=1}^{N_t} Y_i$ come to life and verify its behavior against theoretically derived moments and its characteristic function [@problem_id:3063719]. This exercise solidifies the core concepts of how a Lévy process is constructed and measured.", "problem": "A compound Poisson process is a canonical example of a pure-jump Lévy process with stationary independent increments. Let $t \\geq 0$ and fix a rate parameter $\\lambda  0$. Consider a Poisson process $\\{N_s : s \\geq 0\\}$ with rate $\\lambda$ and an independent sequence of identically distributed jump sizes $\\{Y_i\\}_{i \\geq 1}$ with common distribution $F$ and finite second moment. Define the compound Poisson process $\\{X_s : s \\geq 0\\}$ by\n$$\nX_s = \\sum_{i=1}^{N_s} Y_i, \\quad s \\geq 0,\n$$\nwith the convention that $X_s = 0$ if $N_s = 0$. The process $\\{X_s\\}$ has stationary independent increments and is a Lévy process. The Characteristic Function (CF) of a random variable $Z$ is defined by $\\varphi_Z(u) = \\mathbb{E}[e^{i u Z}]$, where $i = \\sqrt{-1}$.\n\nStarting only from the definitions of the Poisson process and independence, derive how to simulate the process $X_s$ on the interval $[0,t]$ by first sampling the number of jumps and then sampling the jump sizes. Derive the distributional properties needed to justify the simulation and compute the expectation $\\mathbb{E}[X_t]$, the variance $\\mathrm{Var}(X_t)$, and the CF $\\varphi_{X_t}(u)$ in terms of $\\lambda$, $t$, and the distribution of $Y_1$.\n\nThen, implement a program that:\n- For each specified test case, simulates a single realization of $X_t$ by:\n  1. Sampling $N_t \\sim \\mathrm{Poisson}(\\lambda t)$.\n  2. Sampling $N_t$ independent and identically distributed jump sizes from the specified distribution $F$.\n  3. Computing $X_t = \\sum_{i=1}^{N_t} Y_i$.\n- Computes, for each test case, the theoretically derived quantities $\\mathbb{E}[X_t]$, $\\mathrm{Var}(X_t)$, and the CF $\\varphi_{X_t}(u)$ evaluated at the given real argument $u$, using only your derivations.\n- Uses a fixed random seed per test case to ensure deterministic output.\n\nYour program must use the following test suite of parameter values, which collectively test typical and boundary scenarios:\n1. $t = 5.0$, $\\lambda = 1.2$, $F$ is Normal with mean $\\mu = 0.5$ and standard deviation $\\sigma = 0.3$, and CF argument $u = 1.0$. Seed $= 42$.\n2. $t = 5.0$, $\\lambda = 0.0$, $F$ is Normal with mean $\\mu = 0.5$ and standard deviation $\\sigma = 0.3$, and CF argument $u = 1.0$. Seed $= 123$.\n3. $t = 0.0$, $\\lambda = 3.0$, $F$ is Exponential with rate $\\beta = 2.0$ (that is, mean $1/\\beta$), and CF argument $u = 0.5$. Seed $= 7$.\n4. $t = 1.0$, $\\lambda = 5.0$, $F$ is symmetric Bernoulli with values $\\pm 1$ each with probability $1/2$, and CF argument $u = 2.0$. Seed $= 99$.\n\nFor each test case, your program must output a list in the order:\n$$\n\\left[ N_t, \\; X_t, \\; \\mathbb{E}[X_t], \\; \\mathrm{Var}(X_t), \\; \\operatorname{Re}\\left(\\varphi_{X_t}(u)\\right), \\; \\operatorname{Im}\\left(\\varphi_{X_t}(u)\\right) \\right],\n$$\nwhere $N_t$ is an integer and the other quantities are real numbers. Your program should produce a single line of output containing all test case results as a comma-separated list of these lists, enclosed in square brackets, for example:\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$\nNo physical units are involved; angles for characteristic functions are in radians.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the theory of stochastic processes, specifically Lévy processes, and is well-posed with all necessary information provided for both the theoretical derivations and the computational implementation.\n\nThe solution proceeds in two parts. First, we provide the requested theoretical derivations for the simulation method, expectation, variance, and characteristic function of a compound Poisson process. Second, we apply these derivations to the specific jump distributions given in the test cases. Every mathematical entity is rendered in LaTeX as required.\n\n### 1. Justification of the Simulation Method\nThe compound Poisson process $\\{X_s : s \\geq 0\\}$ is defined by the random sum:\n$$\nX_s = \\sum_{i=1}^{N_s} Y_i\n$$\nwhere $\\{N_s : s \\geq 0\\}$ is a Poisson process with rate $\\lambda$, and $\\{Y_i\\}_{i \\geq 1}$ is a sequence of independent and identically distributed (i.i.d.) random variables, also independent of the process $\\{N_s\\}$. By convention, if the number of jumps $N_s$ is zero, the sum is empty and $X_s = 0$.\n\nTo simulate a single realization of the process at a fixed time $t  0$, we are asked to first sample the number of jumps and then the jump sizes. This procedure is a direct implementation of the definition of the process and relies on the properties of conditional probability. The joint distribution of $(N_t, X_t)$ can be factored as $P(N_t=n, X_t \\leq x) = P(X_t \\leq x | N_t=n) P(N_t=n)$. The simulation algorithm follows this factorization:\n\n1.  **Sample the number of jumps:** The random variable $N_t$ counts the number of events of a Poisson process in the interval $[0, t]$. Its distribution is known to be Poisson with parameter $\\lambda t$. We draw a single integer sample, let's call it $n$, from this distribution: $n \\sim \\mathrm{Poisson}(\\lambda t)$.\n\n2.  **Sample the jump sizes:** Conditional on $N_t = n$, the process value is $X_t = \\sum_{i=1}^{n} Y_i$. Since the jump sizes $\\{Y_i\\}$ are i.i.d. and independent of $N_t$, this conditional sum is simply a sum of $n$ i.i.d. random variables drawn from their common distribution $F$. We therefore generate $n$ independent samples, $y_1, y_2, \\dots, y_n$, from the distribution $F$.\n\n3.  **Compute the sum:** The realization of $X_t$ is the sum of these jump sizes, $x_t = \\sum_{i=1}^{n} y_i$. If $n=0$, this sum is empty and $x_t=0$ as per the convention.\n\nThis simulation procedure correctly generates a sample from the distribution of $X_t$.\n\n### 2. Derivation of Distributional Properties\nLet $\\mu_Y = \\mathbb{E}[Y_1]$ and $\\sigma_Y^2 = \\mathrm{Var}(Y_1)$ be the mean and variance of the jump size distribution, respectively. We are given that the second moment, $\\mathbb{E}[Y_1^2] = \\sigma_Y^2 + \\mu_Y^2$, is finite. For the Poisson process counter $N_t$, we know that $\\mathbb{E}[N_t] = \\mathrm{Var}(N_t) = \\lambda t$.\n\n#### Expectation $\\mathbb{E}[X_t]$\nWe use the Law of Total Expectation (also known as the tower property): $\\mathbb{E}[Z] = \\mathbb{E}[\\mathbb{E}[Z|W]]$. Let $Z=X_t$ and $W=N_t$.\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\mathbb{E}\\left[X_t | N_t\\right]\\right]\n$$\nFirst, we compute the inner expectation, conditional on $N_t = n$:\n$$\n\\mathbb{E}[X_t | N_t = n] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} Y_i \\right] = \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = n \\mathbb{E}[Y_1] = n \\mu_Y\n$$\nThis holds for any specific value $n$. Thus, as a random variable, the conditional expectation is $\\mathbb{E}[X_t | N_t] = N_t \\mu_Y$.\nNow, we take the outer expectation:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[N_t \\mu_Y] = \\mu_Y \\mathbb{E}[N_t] = \\mu_Y (\\lambda t)\n$$\nSo, the expectation is $\\mathbb{E}[X_t] = \\lambda t \\mu_Y$.\n\n#### Variance $\\mathrm{Var}(X_t)$\nWe use the Law of Total Variance: $\\mathrm{Var}(Z) = \\mathbb{E}[\\mathrm{Var}(Z|W)] + \\mathrm{Var}(\\mathbb{E}[Z|W])$.\n$$\n\\mathrm{Var}(X_t) = \\mathbb{E}[\\mathrm{Var}(X_t | N_t)] + \\mathrm{Var}(\\mathbb{E}[X_t | N_t])\n$$\nWe compute each term separately.\n1.  **First term: $\\mathbb{E}[\\mathrm{Var}(X_t | N_t)]$**\n    The conditional variance, given $N_t=n$, is:\n    $$\n    \\mathrm{Var}(X_t | N_t = n) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(Y_i) = n \\mathrm{Var}(Y_1) = n \\sigma_Y^2\n    $$\n    This uses the independence of the $Y_i$. Thus, $\\mathrm{Var}(X_t | N_t) = N_t \\sigma_Y^2$. Taking the expectation:\n    $$\n    \\mathbb{E}[\\mathrm{Var}(X_t | N_t)] = \\mathbb{E}[N_t \\sigma_Y^2] = \\sigma_Y^2 \\mathbb{E}[N_t] = \\sigma_Y^2 (\\lambda t)\n    $$\n2.  **Second term: $\\mathrm{Var}(\\mathbb{E}[X_t | N_t])$**\n    From the expectation calculation, we know $\\mathbb{E}[X_t | N_t] = N_t \\mu_Y$. We compute its variance:\n    $$\n    \\mathrm{Var}(\\mathbb{E}[X_t | N_t]) = \\mathrm{Var}(N_t \\mu_Y) = \\mu_Y^2 \\mathrm{Var}(N_t) = \\mu_Y^2 (\\lambda t)\n    $$\nCombining the two terms:\n$$\n\\mathrm{Var}(X_t) = \\lambda t \\sigma_Y^2 + \\lambda t \\mu_Y^2 = \\lambda t (\\sigma_Y^2 + \\mu_Y^2)\n$$\nSince $\\mathbb{E}[Y_1^2] = \\mathrm{Var}(Y_1) + (\\mathbb{E}[Y_1])^2 = \\sigma_Y^2 + \\mu_Y^2$, the variance can be expressed as:\n$$\n\\mathrm{Var}(X_t) = \\lambda t \\mathbb{E}[Y_1^2]\n$$\n\n#### Characteristic Function $\\varphi_{X_t}(u)$\nLet $\\varphi_Y(u) = \\mathbb{E}[e^{iuY_1}]$ be the characteristic function (CF) of the jump size distribution. We again use the Law of Total Expectation.\n$$\n\\varphi_{X_t}(u) = \\mathbb{E}[e^{iuX_t}] = \\mathbb{E}\\left[\\mathbb{E}[e^{iuX_t} | N_t]\\right]\n$$\nThe inner conditional expectation, given $N_t=n$:\n$$\n\\mathbb{E}[e^{iuX_t} | N_t=n] = \\mathbb{E}\\left[e^{iu\\sum_{j=1}^{n} Y_j}\\right] = \\mathbb{E}\\left[\\prod_{j=1}^{n} e^{iuY_j}\\right]\n$$\nDue to the independence of the $Y_j$, this becomes:\n$$\n\\prod_{j=1}^{n} \\mathbb{E}[e^{iuY_j}] = \\prod_{j=1}^{n} \\varphi_Y(u) = (\\varphi_Y(u))^n\n$$\nThus, as a random variable, $\\mathbb{E}[e^{iuX_t} | N_t] = (\\varphi_Y(u))^{N_t}$. Now we take the outer expectation:\n$$\n\\varphi_{X_t}(u) = \\mathbb{E}\\left[(\\varphi_Y(u))^{N_t}\\right]\n$$\nThis expression is the probability-generating function (PGF) of $N_t$, $G_{N_t}(z) = \\mathbb{E}[z^{N_t}]$, evaluated at $z = \\varphi_Y(u)$. The PGF for a Poisson random variable $K \\sim \\mathrm{Poisson}(\\Lambda)$ is $G_K(z) = e^{\\Lambda(z-1)}$. For $N_t \\sim \\mathrm{Poisson}(\\lambda t)$, we have $\\Lambda = \\lambda t$. Substituting $z = \\varphi_Y(u)$, we obtain the characteristic function of the compound Poisson process:\n$$\n\\varphi_{X_t}(u) = \\exp\\left(\\lambda t \\left(\\varphi_Y(u) - 1\\right)\\right)\n$$\nThis is the Lévy–Khintchine formula for a compound Poisson process.\n\n### 3. Properties for Specific Test Cases\nWe now state the required properties for the jump distributions specified in the problem.\n\n1.  **Normal Distribution:** $Y \\sim N(\\mu, \\sigma^2)$\n    -   Mean: $\\mu_Y = \\mu$\n    -   Second Moment: $\\mathbb{E}[Y^2] = \\sigma^2 + \\mu^2$\n    -   Characteristic Function: $\\varphi_Y(u) = \\exp(i u \\mu - \\frac{1}{2}\\sigma^2 u^2)$\n\n2.  **Exponential Distribution:** $Y \\sim \\mathrm{Exponential}(\\beta)$ (with PDF $f(y) = \\beta e^{-\\beta y}$ for $y \\ge 0$)\n    -   Mean: $\\mu_Y = 1/\\beta$\n    -   Variance: $\\sigma_Y^2 = 1/\\beta^2$\n    -   Second Moment: $\\mathbb{E}[Y^2] = \\sigma_Y^2 + \\mu_Y^2 = 2/\\beta^2$\n    -   Characteristic Function: $\\varphi_Y(u) = \\mathbb{E}[e^{iuY}] = \\int_0^\\infty e^{iuy}\\beta e^{-\\beta y} dy = \\beta/(\\beta - iu) = 1/(1 - iu/\\beta)$\n\n3.  **Symmetric Bernoulli Distribution:** $P(Y=1) = P(Y=-1) = 1/2$\n    -   Mean: $\\mu_Y = (1)(1/2) + (-1)(1/2) = 0$\n    -   Second Moment: $\\mathbb{E}[Y^2] = (1)^2(1/2) + (-1)^2(1/2) = 1$\n    -   Characteristic Function: $\\varphi_Y(u) = e^{iu(1)}(1/2) + e^{iu(-1)}(1/2) = \\frac{e^{iu}+e^{-iu}}{2} = \\cos(u)$\n\nFor cases where $t=0$ or $\\lambda=0$, the product $\\lambda t = 0$. In these scenarios, $N_t$ is deterministically $0$, which implies $X_t=0$. The theoretical formulas correctly degenerate:\n- $\\mathbb{E}[X_t] = 0 \\cdot \\mu_Y = 0$\n- $\\mathrm{Var}(X_t) = 0 \\cdot \\mathbb{E}[Y_1^2] = 0$\n- $\\varphi_{X_t}(u) = \\exp(0 \\cdot (\\varphi_Y(u)-1)) = e^0 = 1$. The CF of a constant $0$ is indeed $\\mathbb{E}[e^{iu \\cdot 0}] = 1$.\n\nThe implementation will now follow from these derived formulas.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used but is an allowed library.\n\ndef solve():\n    \"\"\"\n    Simulates a compound Poisson process and calculates its theoretical properties\n    for a given set of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"t\": 5.0, \"lam\": 1.2, \"u\": 1.0, \"seed\": 42,\n            \"dist\": \"normal\", \"params\": {\"mu\": 0.5, \"sigma\": 0.3}\n        },\n        {\n            \"t\": 5.0, \"lam\": 0.0, \"u\": 1.0, \"seed\": 123,\n            \"dist\": \"normal\", \"params\": {\"mu\": 0.5, \"sigma\": 0.3}\n        },\n        {\n            \"t\": 0.0, \"lam\": 3.0, \"u\": 0.5, \"seed\": 7,\n            \"dist\": \"exponential\", \"params\": {\"beta\": 2.0}\n        },\n        {\n            \"t\": 1.0, \"lam\": 5.0, \"u\": 2.0, \"seed\": 99,\n            \"dist\": \"bernoulli\", \"params\": {}\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        t, lam, u, seed = case[\"t\"], case[\"lam\"], case[\"u\"], case[\"seed\"]\n        dist, params = case[\"dist\"], case[\"params\"]\n\n        rng = np.random.default_rng(seed)\n\n        # Theoretical properties of the jump distribution Y\n        mu_y, e_y2, phi_y = 0.0, 0.0, 0.0 + 0.0j\n\n        if dist == \"normal\":\n            mu, sigma = params[\"mu\"], params[\"sigma\"]\n            mu_y = mu\n            e_y2 = sigma**2 + mu**2\n            phi_y = np.exp(1j * u * mu - 0.5 * sigma**2 * u**2)\n        elif dist == \"exponential\":\n            beta = params[\"beta\"]\n            mu_y = 1.0 / beta\n            e_y2 = 2.0 / beta**2\n            phi_y = 1.0 / (1.0 - 1j * u / beta)\n        elif dist == \"bernoulli\":\n            mu_y = 0.0\n            e_y2 = 1.0\n            phi_y = np.cos(u)\n\n        # 1. Simulation\n        # Sample N_t from Poisson(lambda*t)\n        lambda_t = lam * t\n        n_t = rng.poisson(lambda_t)\n\n        # Sample N_t jumps and compute X_t\n        x_t = 0.0\n        if n_t > 0:\n            if dist == \"normal\":\n                jumps = rng.normal(params[\"mu\"], params[\"sigma\"], size=n_t)\n            elif dist == \"exponential\":\n                jumps = rng.exponential(scale=1.0/params[\"beta\"], size=n_t)\n            elif dist == \"bernoulli\":\n                jumps = rng.choice([-1, 1], size=n_t)\n            x_t = np.sum(jumps)\n        # If n_t is 0, x_t remains 0.0, which is correct.\n\n        # 2. Theoretical Calculations\n        # Expectation E[X_t] = lambda*t*E[Y]\n        e_xt = lambda_t * mu_y\n\n        # Variance Var(X_t) = lambda*t*E[Y^2]\n        var_xt = lambda_t * e_y2\n\n        # Characteristic function phi_Xt(u) = exp(lambda*t*(phi_Y(u) - 1))\n        phi_xt = np.exp(lambda_t * (phi_y - 1))\n\n        # Store results for this case\n        all_results.append([\n            n_t,\n            x_t,\n            e_xt,\n            var_xt,\n            phi_xt.real,\n            phi_xt.imag\n        ])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(all_results)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3063719"}, {"introduction": "A key insight in the study of Lévy processes is that not just any probability distribution can describe the state of the process $X_t$ at a given time $t \\gt 0$. This practice introduces a fundamental constraint known as infinite divisibility, which serves as a litmus test for candidate distributions. You will use the characteristic function—a powerful analytical tool—to prove that the common uniform distribution fails this test, and therefore cannot be the law of any Lévy process [@problem_id:3063708]. Working through this classic counterexample provides a deeper appreciation for the unique mathematical structure that stationary, independent increments impose.", "problem": "Let $\\left(X_{t}\\right)_{t \\geq 0}$ be a Lévy process, that is, a stochastic process with stationary and independent increments and $X_{0}=0$ almost surely. It is a well-tested fact that for each fixed $t0$, the law of $X_{t}$ is infinitely divisible and its characteristic function has the Lévy–Khintchine representation $\\varphi_{X_{t}}(\\theta)=\\exp\\!\\left(t\\,\\Psi(\\theta)\\right)$ for a suitable characteristic exponent $\\Psi$. Consider the uniform distribution on $\\left[0,1\\right]$ as a candidate marginal law for $X_{t}$.\n\nStarting from the definition of a characteristic function $\\varphi_{Y}(\\theta)=\\mathbb{E}\\left[\\exp\\!\\left(i\\,\\theta\\,Y\\right)\\right]$, do the following:\n- derive the characteristic function of a random variable $U$ uniformly distributed on $\\left[0,1\\right]$;\n- compute the smallest positive real number $\\theta^{\\ast}0$ such that $\\varphi_{U}\\!\\left(\\theta^{\\ast}\\right)=0$;\n- use this to justify why the uniform distribution on $\\left[0,1\\right]$ cannot be the law of $X_{t}$ for any Lévy process at any $t0$.\n\nProvide as your final answer the value of $\\theta^{\\ast}$. No rounding is required, and no units are involved.", "solution": "We begin from the definition of a characteristic function. If $U \\sim \\mathrm{Unif}\\!\\left(0,1\\right)$, then for any real $\\theta$,\n\n$$\n\\varphi_{U}(\\theta)=\\mathbb{E}\\!\\left[\\exp\\!\\left(i\\,\\theta\\,U\\right)\\right]=\\int_{0}^{1}\\exp\\!\\left(i\\,\\theta\\,x\\right)\\,\\mathrm{d}x.\n$$\n\nThis integral can be computed explicitly:\n\n$$\n\\varphi_{U}(\\theta)=\\left.\\frac{\\exp\\!\\left(i\\,\\theta\\,x\\right)}{i\\,\\theta}\\right|_{x=0}^{x=1}=\\frac{\\exp\\!\\left(i\\,\\theta\\right)-1}{i\\,\\theta}.\n$$\n\nAn equivalent factorized form that makes zeros apparent is obtained by multiplying numerator and denominator by $\\exp\\!\\left(-i\\,\\theta/2\\right)$:\n\n$$\n\\varphi_{U}(\\theta)=\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)\\,\\frac{\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)-\\exp\\!\\left(-i\\,\\frac{\\theta}{2}\\right)}{i\\,\\theta}\n=\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)\\,\\frac{2\\,\\sin\\!\\left(\\frac{\\theta}{2}\\right)}{\\theta}.\n$$\n\nThus, the zeros of $\\varphi_{U}$ occur exactly when $\\sin\\!\\left(\\frac{\\theta}{2}\\right)=0$ with $\\theta \\neq 0$ (to avoid the removable singularity at $\\theta=0$). Hence,\n\n$$\n\\frac{\\theta}{2}=\\pi\\,k \\quad \\text{for} \\quad k \\in \\mathbb{Z}\\setminus\\{0\\} \\;\\;\\Longleftrightarrow\\;\\; \\theta=2\\pi\\,k,\\; k \\in \\mathbb{Z}\\setminus\\{0\\}.\n$$\n\nThe smallest positive zero is therefore\n\n$$\n\\theta^{\\ast}=2\\pi.\n$$\n\n\nWe now justify why the uniform distribution on $\\left[0,1\\right]$ cannot be the law of $X_{t}$ for any Lévy process at any $t0$. A Lévy process has stationary and independent increments and $X_{0}=0$ almost surely. A well-tested and fundamental characterization states that for each fixed $t0$, the characteristic function of $X_{t}$ admits the Lévy–Khintchine representation\n\n$$\n\\varphi_{X_{t}}(\\theta)=\\exp\\!\\left(t\\,\\Psi(\\theta)\\right),\n$$\n\nwhere $\\Psi$ is the characteristic (Lévy–Khintchine) exponent. Since the exponential function never vanishes, $\\varphi_{X_{t}}(\\theta)\\neq 0$ for all real $\\theta$. In particular, the characteristic function of any $X_{t}$ has no real zeros.\n\nBy contrast, we have shown that $\\varphi_{U}(\\theta)$ has real zeros, with the smallest positive one at $\\theta^{\\ast}=2\\pi$. Therefore, the uniform distribution on $\\left[0,1\\right]$ is not infinitely divisible and cannot coincide with the law of $X_{t}$ for any Lévy process at any $t0$.\n\nConsequently, the requested smallest positive zero is $\\theta^{\\ast}=2\\pi$.", "answer": "$$\\boxed{2\\pi}$$", "id": "3063708"}, {"introduction": "The macroscopic behavior of a Lévy process, such as its overall volatility, is dictated by the microscopic properties of its jumps. This exercise explores the crucial link between the variance of a compound Poisson process and the tail behavior of its jump distribution, a property formally captured by the Lévy measure. You will investigate a process with heavy-tailed jumps and determine the precise conditions on the tail's decay rate that ensure the process has finite variance [@problem_id:3063738]. This practice illuminates how the Lévy measure governs the process's moments and explains how the possibility of large, infrequent jumps is a critical feature in modeling extreme events.", "problem": "Consider a compound Poisson process $X_{t}$ defined by $X_{t}=\\sum_{i=1}^{N_{t}} Y_{i}$, where $\\{N_{t}:t\\geq 0\\}$ is a Poisson process of rate $\\lambda0$, and $\\{Y_{i}\\}_{i\\geq 1}$ are independent and identically distributed jumps, independent of $\\{N_{t}\\}$. Assume the jump distribution is symmetric and heavy-tailed with density\n$$\nf(y)=\\frac{\\alpha\\,y_{0}^{\\alpha}}{2}\\,|y|^{-(1+\\alpha)}\\,\\mathbf{1}_{\\{|y|\\geq y_{0}\\}},\n$$\nwhere $\\alpha0$ and $y_{0}0$ are fixed constants, and $\\mathbf{1}_{A}$ denotes the indicator of the event $A$. The process $\\{X_{t}\\}$ is a Lévy process with stationary independent increments.\n\nUsing only the definitions of a compound Poisson process and Lévy measure, along with basic properties of conditional expectation and variance, do the following:\n\n- Derive the necessary and sufficient condition on the tail index $\\alpha$ under which the variance $\\operatorname{Var}(X_{t})$ is finite for some (equivalently, all) $t0$.\n- Express the Lévy measure $\\nu$ of $X$ in terms of $\\lambda$ and $f$, and restate your finite-variance condition as an integrability condition involving $\\int_{\\mathbb{R}} x^{2}\\,\\nu(dx)$.\n- When the variance is finite, compute the closed-form expression of $\\operatorname{Var}(X_{t})$ as a function of $\\lambda$, $t$, $\\alpha$, and $y_{0}$.\n\nYour final answer must be a single analytical expression for $\\operatorname{Var}(X_{t})$ in terms of $\\lambda$, $t$, $\\alpha$, and $y_{0}$ in the regime where it is finite. No numerical evaluation or rounding is required.", "solution": "The problem is a valid exercise in stochastic process theory. All provided definitions and conditions are standard and self-consistent. We can proceed with the solution.\n\nThe compound Poisson process is defined as $X_{t}=\\sum_{i=1}^{N_{t}} Y_{i}$, where $\\{N_{t}:t\\geq 0\\}$ is a Poisson process with rate $\\lambda  0$, and $\\{Y_{i}\\}_{i\\geq 1}$ are independent and identically distributed (i.i.d.) random variables, independent of $N_t$. The variance of $X_t$, denoted $\\operatorname{Var}(X_{t})$, can be computed using the law of total variance (also known as the law of iterated variances):\n$$\n\\operatorname{Var}(X_{t}) = \\mathbb{E}[\\operatorname{Var}(X_{t}|N_{t})] + \\operatorname{Var}(\\mathbb{E}[X_{t}|N_{t}])\n$$\n\nFirst, we find the conditional expectation and variance of $X_t$ given $N_t$.\nGiven $N_{t}=n$, the process $X_t$ is a sum of $n$ i.i.d. random variables $Y_i$: $X_{t}|_{N_t=n} = \\sum_{i=1}^{n} Y_{i}$.\nThe conditional expectation is:\n$$\n\\mathbb{E}[X_{t}|N_{t}=n] = \\mathbb{E}\\left[\\sum_{i=1}^{n} Y_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[Y_{i}] = n\\,\\mathbb{E}[Y_{1}]\n$$\nThis implies that the random variable $\\mathbb{E}[X_{t}|N_{t}]$ is $N_{t}\\mathbb{E}[Y_{1}]$.\n\nThe conditional variance is:\n$$\n\\operatorname{Var}(X_{t}|N_{t}=n) = \\operatorname{Var}\\left(\\sum_{i=1}^{n} Y_{i}\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(Y_{i}) = n\\,\\operatorname{Var}(Y_{1})\n$$\nThis implies the random variable $\\operatorname{Var}(X_{t}|N_{t})$ is $N_{t}\\operatorname{Var}(Y_{1})$.\n\nNow, we substitute these back into the law of total variance. The two terms are:\n1.  $\\operatorname{Var}(\\mathbb{E}[X_{t}|N_{t}]) = \\operatorname{Var}(N_{t}\\mathbb{E}[Y_{1}]) = (\\mathbb{E}[Y_{1}])^{2} \\operatorname{Var}(N_{t})$.\n2.  $\\mathbb{E}[\\operatorname{Var}(X_{t}|N_{t})] = \\mathbb{E}[N_{t}\\operatorname{Var}(Y_{1})] = \\operatorname{Var}(Y_{1}) \\mathbb{E}[N_{t}]$.\n\nFor a Poisson process $N_t$ with rate $\\lambda$, we know that $\\mathbb{E}[N_{t}] = \\lambda t$ and $\\operatorname{Var}(N_{t}) = \\lambda t$. Substituting these gives:\n$$\n\\operatorname{Var}(X_{t}) = \\lambda t \\operatorname{Var}(Y_{1}) + \\lambda t (\\mathbb{E}[Y_{1}])^{2} = \\lambda t (\\operatorname{Var}(Y_{1}) + (\\mathbb{E}[Y_{1}])^{2})\n$$\nUsing the definition of variance, $\\operatorname{Var}(Y_{1}) = \\mathbb{E}[Y_{1}^{2}] - (\\mathbb{E}[Y_{1}])^{2}$, we obtain:\n$$\n\\operatorname{Var}(X_{t}) = \\lambda t \\mathbb{E}[Y_{1}^{2}]\n$$\nThis result is also known as the Blackwell-Girshick equation. For $\\operatorname{Var}(X_t)$ to be finite for any $t0$, $\\mathbb{E}[Y_1^2]$ must be finite. Note that a finite second moment $\\mathbb{E}[Y_1^2]$ implies a finite first moment $\\mathbb{E}[|Y_1|]$ by the Cauchy-Schwarz inequality.\n\nNow, we must determine the condition on $\\alpha$ for which $\\mathbb{E}[Y_{1}^{2}]$ is finite. First, let's examine the mean $\\mathbb{E}[Y_1]$. The jump density is symmetric, $f(y) = f(-y)$, and is given by\n$$\nf(y)=\\frac{\\alpha\\,y_{0}^{\\alpha}}{2}\\,|y|^{-(1+\\alpha)}\\,\\mathbf{1}_{\\{|y|\\geq y_{0}\\}}\n$$\nThe mean is $\\mathbb{E}[Y_{1}] = \\int_{-\\infty}^{\\infty} y f(y) dy$. The integrand $y f(y)$ is an odd function, and the domain of integration $\\{y:|y| \\ge y_0\\}$ is symmetric about $y=0$. Thus, if the integral converges, its value is $0$. The convergence depends on the integral of the absolute value:\n$$\n\\int_{-\\infty}^{\\infty} |y| f(y) dy = 2 \\int_{y_0}^{\\infty} y \\left(\\frac{\\alpha\\,y_{0}^{\\alpha}}{2}\\,y^{-(1+\\alpha)}\\right) dy = \\alpha y_0^\\alpha \\int_{y_0}^{\\infty} y^{-\\alpha} dy\n$$\nThis integral converges if and only if $-\\alpha  -1$, which means $\\alpha  1$. If $\\alpha \\le 1$, the mean $\\mathbb{E}[Y_1]$ is undefined, and consequently the variance $\\operatorname{Var}(X_t)$ is infinite. If $\\alpha  1$, then $\\mathbb{E}[Y_{1}] = 0$.\n\nNext, we compute the second moment $\\mathbb{E}[Y_{1}^{2}]$:\n$$\n\\mathbb{E}[Y_{1}^{2}] = \\int_{-\\infty}^{\\infty} y^{2} f(y) dy\n$$\nThe integrand $y^2 f(y)$ is an even function, so we can write:\n$$\n\\mathbb{E}[Y_{1}^{2}] = 2 \\int_{y_{0}}^{\\infty} y^{2} f(y) dy = 2 \\int_{y_{0}}^{\\infty} y^{2} \\left(\\frac{\\alpha\\,y_{0}^{\\alpha}}{2}\\,y^{-(1+\\alpha)}\\right) dy = \\alpha y_{0}^{\\alpha} \\int_{y_{0}}^{\\infty} y^{1-\\alpha} dy\n$$\nThis integral converges if and only if the exponent $1-\\alpha$ is less than $-1$, i.e., $1-\\alpha  -1$, which simplifies to $\\alpha  2$. When $\\alpha \\le 2$, $\\mathbb{E}[Y_1^2]$ is infinite, and so is $\\operatorname{Var}(X_t)$. The condition $\\alpha  2$ implies $\\alpha  1$, so the condition for the mean to exist is satisfied. Therefore, the necessary and sufficient condition for $\\operatorname{Var}(X_t)$ to be finite is $\\alpha  2$.\n\nThe Lévy measure $\\nu$ of a compound Poisson process is given by $\\nu(A) = \\lambda P(Y_1 \\in A)$ for any Borel set $A \\subset \\mathbb{R}\\setminus\\{0\\}$. In terms of densities, this means $\\nu(dx) = \\lambda f(x) dx$.\nOur condition for finite variance is that $\\mathbb{E}[Y_1^2]  \\infty$. We can express this in terms of the Lévy measure:\n$$\n\\mathbb{E}[Y_1^2] = \\int_{\\mathbb{R}} x^2 f(x) dx = \\int_{\\mathbb{R}} x^2 \\frac{1}{\\lambda} \\nu(dx) = \\frac{1}{\\lambda} \\int_{\\mathbb{R}} x^2 \\nu(dx)\n$$\nSince $\\lambda$ is a positive constant, the condition $\\mathbb{E}[Y_1^2]  \\infty$ is equivalent to the condition $\\int_{\\mathbb{R}} x^{2}\\,\\nu(dx)  \\infty$. This aligns with the general theory of Lévy processes, where for a process with no Brownian component and finite second moment, $\\operatorname{Var}(X_t) = t \\int_{\\mathbb{R}} x^2 \\nu(dx)$. Our derivation confirms this:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\mathbb{E}[Y_1^2] = \\lambda t \\left(\\frac{1}{\\lambda} \\int_{\\mathbb{R}} x^2 \\nu(dx)\\right) = t \\int_{\\mathbb{R}} x^2 \\nu(dx)\n$$\n\nFinally, we compute the closed-form expression for $\\operatorname{Var}(X_t)$ under the condition $\\alpha2$. We evaluate the integral for $\\mathbb{E}[Y_{1}^{2}]$:\n$$\n\\int_{y_{0}}^{\\infty} y^{1-\\alpha} dy = \\left[ \\frac{y^{2-\\alpha}}{2-\\alpha} \\right]_{y_{0}}^{\\infty}\n$$\nSince $\\alpha  2$, the exponent $2-\\alpha$ is negative, so $\\lim_{y\\to\\infty} y^{2-\\alpha} = 0$. The integral evaluates to:\n$$\n0 - \\frac{y_{0}^{2-\\alpha}}{2-\\alpha} = \\frac{y_{0}^{2-\\alpha}}{\\alpha-2}\n$$\nSubstituting this back into the expression for the second moment:\n$$\n\\mathbb{E}[Y_{1}^{2}] = \\alpha y_{0}^{\\alpha} \\left( \\frac{y_{0}^{2-\\alpha}}{\\alpha-2} \\right) = \\frac{\\alpha y_{0}^{\\alpha+2-\\alpha}}{\\alpha-2} = \\frac{\\alpha y_{0}^{2}}{\\alpha-2}\n$$\nThe variance of $X_t$ is therefore:\n$$\n\\operatorname{Var}(X_{t}) = \\lambda t \\mathbb{E}[Y_{1}^{2}] = \\lambda t \\frac{\\alpha y_{0}^{2}}{\\alpha-2}\n$$\nThis expression is valid for $\\alpha  2$.", "answer": "$$\\boxed{\\frac{\\lambda t \\alpha y_{0}^{2}}{\\alpha-2}}$$", "id": "3063738"}]}