{"hands_on_practices": [{"introduction": "The Doob decomposition provides a fundamental way to understand stochastic processes by separating their predictable trend from their underlying random behavior. This first exercise applies the theorem in a discrete-time setting, which is the most direct way to build intuition. By analyzing a simple random walk with a constant drift, you will see how to explicitly construct the martingale component and the predictable, increasing compensator using their core definitions [@problem_id:3050559].", "problem": "Consider a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_n)_{n \\geq 0}, \\mathbb{P})$ and an adapted, integrable sequence $(\\xi_n)_{n \\geq 1}$ satisfying $E[\\xi_n \\mid \\mathcal{F}_{n-1}] = 0$ for all $n \\geq 1$. Define the martingale difference sum $S_n = \\sum_{k=1}^{n} \\xi_k$ with $S_0 = 0$, and fix a constant drift $\\mu \\geq 0$. Consider the process $X_n = S_n + \\mu n$ for $n \\geq 0$.\n\nUsing only core definitions of martingales, submartingales, and the discrete-time Doob decomposition (Doob-Meyer decomposition in discrete time), determine the decomposition of the submartingale $(X_n)$ into a sum $X_n = M_n + A_n$ where $(M_n)$ is a martingale and $(A_n)$ is a predictable, increasing process with $A_0 = 0$. Identify explicit expressions for $M_n$ and $A_n$ in terms of $S_n$ and $\\mu$.\n\nYour final answer must be a single analytical expression that lists $M_n$ and $A_n$ as a row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- A filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_n)_{n \\geq 0}, \\mathbb{P})$.\n- An adapted, integrable sequence of random variables $(\\xi_n)_{n \\geq 1}$.\n- A condition on the sequence: $E[\\xi_n \\mid \\mathcal{F}_{n-1}] = 0$ for all $n \\geq 1$. This identifies $(\\xi_n)$ as a martingale difference sequence.\n- A process $S_n = \\sum_{k=1}^{n} \\xi_k$ for $n \\geq 1$, with the initial condition $S_0 = 0$.\n- A constant drift $\\mu \\geq 0$.\n- A process $X_n = S_n + \\mu n$ for $n \\geq 0$.\n- The task is to find the Doob decomposition of $(X_n)$, which is stated to be a submartingale, into $X_n = M_n + A_n$.\n- $(M_n)$ must be a martingale relative to $(\\mathcal{F}_n)_{n \\geq 0}$.\n- $(A_n)$ must be a predictable, increasing process with $A_0 = 0$.\n- The final answer should be explicit expressions for $M_n$ and $A_n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical framework of discrete-time stochastic processes.\n- **Scientifically Grounded**: All concepts—filtered probability space, adapted process, martingale difference sequence, martingale, submartingale, and the Doob decomposition—are standard and fundamental in modern probability theory. The setup is logically and mathematically sound.\n- **Well-Posed**: The Doob decomposition theorem guarantees the existence and uniqueness of the decomposition for any submartingale. The problem provides all necessary information to apply this theorem and find the unique components $M_n$ and $A_n$.\n- **Objective**: The problem is stated using formal, precise mathematical language, with no ambiguity or subjective elements.\n- **Completeness and Consistency**: The problem provides a complete set of definitions and conditions. We can verify the premise that $(X_n)$ is a submartingale from the givens. The condition $\\mu \\geq 0$ is crucial and consistent with the properties of a submartingale.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe goal is to find the Doob decomposition of the process $X_n = S_n + \\mu n$. This decomposition expresses a submartingale $(X_n)$ as the sum of a martingale $(M_n)$ and a predictable, increasing process $(A_n)$ with $A_0 = 0$.\n\nFirst, we must confirm that $(X_n)_{n \\geq 0}$ is indeed a submartingale with respect to the filtration $(\\mathcal{F}_n)_{n \\geq 0}$. A process is a submartingale if it is adapted, integrable, and satisfies $E[X_n \\mid \\mathcal{F}_{n-1}] \\geq X_{n-1}$ for all $n \\geq 1$.\n\n1.  **Adaptedness**: The sequence $(\\xi_k)_{k \\geq 1}$ is adapted, meaning each $\\xi_k$ is $\\mathcal{F}_k$-measurable. Therefore, $S_n = \\sum_{k=1}^{n} \\xi_k$ is $\\mathcal{F}_n$-measurable as it is a sum of $\\mathcal{F}_n$-measurable random variables. The term $\\mu n$ is a deterministic constant for each $n$, so it is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_n$. Thus, $X_n = S_n + \\mu n$ is $\\mathcal{F}_n$-adapted.\n\n2.  **Integrability**: We are given that $(\\xi_n)$ is an integrable sequence, so $E[|\\xi_n|] < \\infty$ for all $n \\geq 1$. By the triangle inequality for expectations, $E[|S_n|] = E\\left[\\left|\\sum_{k=1}^{n} \\xi_k\\right|\\right] \\leq \\sum_{k=1}^{n} E[|\\xi_k|]$. Since this is a finite sum of finite numbers, $E[|S_n|] < \\infty$. Consequently, $E[|X_n|] = E[|S_n + \\mu n|] \\leq E[|S_n|] + |\\mu n| < \\infty$. Thus, $(X_n)$ is an integrable process.\n\n3.  **Submartingale Property**: We compute the conditional expectation of $X_n$ given $\\mathcal{F}_{n-1}$ for $n \\geq 1$:\n    $$E[X_n \\mid \\mathcal{F}_{n-1}] = E[S_n + \\mu n \\mid \\mathcal{F}_{n-1}]$$\n    Using the definition $S_n = S_{n-1} + \\xi_n$ and the linearity of conditional expectation:\n    $$E[X_n \\mid \\mathcal{F}_{n-1}] = E[S_{n-1} + \\xi_n + \\mu n \\mid \\mathcal{F}_{n-1}] = E[S_{n-1} \\mid \\mathcal{F}_{n-1}] + E[\\xi_n \\mid \\mathcal{F}_{n-1}] + E[\\mu n \\mid \\mathcal{F}_{n-1}]$$\n    - Since $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable, $E[S_{n-1} \\mid \\mathcal{F}_{n-1}] = S_{n-1}$.\n    - By the problem statement, $E[\\xi_n \\mid \\mathcal{F}_{n-1}] = 0$.\n    - Since $\\mu n$ is a constant, it is $\\mathcal{F}_{n-1}$-measurable, so $E[\\mu n \\mid \\mathcal{F}_{n-1}] = \\mu n$.\n    Combining these results:\n    $$E[X_n \\mid \\mathcal{F}_{n-1}] = S_{n-1} + 0 + \\mu n = S_{n-1} + \\mu n$$\n    We compare this to $X_{n-1} = S_{n-1} + \\mu(n-1)$:\n    $$E[X_n \\mid \\mathcal{F}_{n-1}] = (S_{n-1} + \\mu(n-1)) + \\mu = X_{n-1} + \\mu$$\n    Since we are given $\\mu \\geq 0$, we have $E[X_n \\mid \\mathcal{F}_{n-1}] \\geq X_{n-1}$. This confirms that $(X_n)$ is a submartingale.\n\nNow we apply the discrete-time Doob decomposition theorem. The unique decomposition $X_n = M_n + A_n$ is given by:\n- $A_0 = 0$\n- $A_n = \\sum_{k=1}^{n} (E[X_k \\mid \\mathcal{F}_{k-1}] - X_{k-1})$ for $n \\geq 1$\n- $M_n = X_n - A_n$ for $n \\geq 0$\n\nLet's compute the predictable, increasing process $(A_n)$. Using our earlier result that $E[X_k \\mid \\mathcal{F}_{k-1}] = X_{k-1} + \\mu$:\n$$E[X_k \\mid \\mathcal{F}_{k-1}] - X_{k-1} = \\mu$$\nTherefore, for $n \\geq 1$:\n$$A_n = \\sum_{k=1}^{n} \\mu = \\mu n$$\nAnd for $n=0$, $A_0 = \\mu \\cdot 0 = 0$. This matches the requirement.\nLet's check the properties of $A_n = \\mu n$:\n- **Predictable**: For any $n \\geq 1$, $A_n = \\mu n$ is a deterministic constant. Any constant is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_{n-1}$. Thus, $(A_n)$ is a predictable process.\n- **Increasing**: For any $n \\geq 1$, $A_n - A_{n-1} = \\mu n - \\mu(n-1) = \\mu$. Since $\\mu \\geq 0$, the sequence $(A_n)$ is non-decreasing (increasing).\n\nNow, we find the martingale component $(M_n)$:\n$$M_n = X_n - A_n = (S_n + \\mu n) - (\\mu n) = S_n$$\nSo, $M_n = S_n$ for all $n \\geq 0$. We must verify that $(M_n) = (S_n)$ is a martingale.\n- **Adaptedness and Integrability**: We have already shown that $(S_n)$ is adapted and integrable.\n- **Martingale Property**: For $n \\geq 1$, we need to check if $E[M_n \\mid \\mathcal{F}_{n-1}] = M_{n-1}$.\n$$E[S_n \\mid \\mathcal{F}_{n-1}] = E[S_{n-1} + \\xi_n \\mid \\mathcal{F}_{n-1}] = E[S_{n-1} \\mid \\mathcal{F}_{n-1}] + E[\\xi_n \\mid \\mathcal{F}_{n-1}]$$\nAs $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable and $E[\\xi_n \\mid \\mathcal{F}_{n-1}] = 0$:\n$$E[S_n \\mid \\mathcal{F}_{n-1}] = S_{n-1} + 0 = S_{n-1}$$\nSince $M_n = S_n$ and $M_{n-1} = S_{n-1}$, the condition $E[M_n \\mid \\mathcal{F}_{n-1}] = M_{n-1}$ is satisfied. Thus, $(M_n)$ is a martingale.\n\nThe Doob decomposition of $X_n$ is $X_n = S_n + \\mu n$, where the martingale part is $M_n = S_n$ and the predictable, increasing part is $A_n = \\mu n$.\n\nThe explicit expressions are:\n- $M_n = S_n$\n- $A_n = \\mu n$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_n & \\mu n\n\\end{pmatrix}\n}\n$$", "id": "3050559"}, {"introduction": "Moving from pure theory to application, this practice challenges you to implement the Doob decomposition numerically. You will work with the Autoregressive (AR(1)) model, a cornerstone of time-series analysis, and write code to simulate its path and compute the components of its decomposition [@problem_id:2388954]. This exercise is crucial for developing practical skills, as it forces you to translate the abstract formula for the predictable part into a concrete algorithm.", "problem": "You are given a discrete-time, real-valued, adapted process defined by an Autoregressive of order 1 (AR(1)) model. For each test case, the process is specified by parameters $\\mu$, $\\phi$, $\\sigma$, an initial value $X_0$, and a time horizon $T$. The dynamics are\n$$\nX_t \\;=\\; \\mu \\;+\\; \\phi\\,X_{t-1} \\;+\\; \\varepsilon_t,\\quad t=1,2,\\dots,T,\n$$\nwhere $\\{\\varepsilon_t\\}_{t\\ge 1}$ are independent and identically distributed Gaussian innovations with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and the filtration is the natural filtration $\\mathcal{F}_t=\\sigma(X_0,\\varepsilon_1,\\dots,\\varepsilon_t)$.\n\nFor each test case:\n- Simulate one sample path $\\{X_t\\}_{t=0}^T$ with the specified parameters using the given random seed to initialize your pseudo-random number generator. Draw $\\varepsilon_t$ as independent Gaussian random variables with mean $0$ and standard deviation $\\sigma$.\n- Numerically compute the discrete-time Doob–Meyer (Doob) decomposition of $\\{X_t\\}$ into processes $\\{M_t\\}$ and $\\{A_t\\}$ such that\n$$\nX_t \\;=\\; X_0 \\;+\\; M_t \\;+\\; A_t,\\quad M_0=0,\\;A_0=0,\n$$\nwith $\\{M_t\\}$ a martingale with respect to $\\{\\mathcal{F}_t\\}$ and $\\{A_t\\}$ predictable with respect to $\\{\\mathcal{F}_t\\}$.\n- Report the pair of terminal values $[A_T, M_T]$ for that test case.\n\nTest suite:\n- Case 1 (general non-martingale): $\\mu=0.5$, $\\phi=0.8$, $\\sigma=1.0$, $X_0=0.7$, $T=12$, seed $=314159$.\n- Case 2 (martingale boundary): $\\mu=0.0$, $\\phi=1.0$, $\\sigma=0.8$, $X_0=1.2$, $T=10$, seed $=271828$.\n- Case 3 (deterministic edge): $\\mu=0.1$, $\\phi=0.9$, $\\sigma=0.0$, $X_0=-0.3$, $T=15$, seed $=42$.\n- Case 4 (unit root with drift): $\\mu=-0.2$, $\\phi=1.0$, $\\sigma=0.5$, $X_0=0.0$, $T=8$, seed $=7$.\n\nYour program must:\n- For each case, simulate exactly one path using the provided seed.\n- Compute terminal values $A_T$ and $M_T$ consistent with the discrete-time Doob decomposition definition with respect to the natural filtration.\n- Round each reported number to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be the two-element list $[A_T, M_T]$ for a test case, in the same order as above. For example, the overall output should look like\n\"[ [a1,m1],[a2,m2],[a3,m3],[a4,m4] ]\"\nbut with each $a_i$ and $m_i$ replaced by the corresponding rounded numeric values.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Process Dynamics**: A discrete-time Autoregressive model of order 1 (AR(1)) is given by $X_t = \\mu + \\phi X_{t-1} + \\varepsilon_t$ for $t=1, 2, \\dots, T$.\n- **Initial Value**: The process starts at a given value $X_0$.\n- **Innovations**: $\\{\\varepsilon_t\\}_{t \\ge 1}$ is a sequence of independent and identically distributed (i.i.d.) random variables, with $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n- **Filtration**: The natural filtration is $\\mathcal{F}_t = \\sigma(X_0, \\varepsilon_1, \\dots, \\varepsilon_t)$.\n- **Decomposition**: The process $\\{X_t\\}$ is to be decomposed as $X_t = X_0 + M_t + A_t$, where $\\{M_t\\}$ is a martingale and $\\{A_t\\}$ is a predictable process with respect to $\\{\\mathcal{F}_t\\}$, and $M_0 = A_0 = 0$.\n- **Task**: For a given sample path, find the terminal values $[A_T, M_T]$.\n- **Test Cases**:\n    - Case 1: $\\mu=0.5$, $\\phi=0.8$, $\\sigma=1.0$, $X_0=0.7$, $T=12$, seed $=314159$.\n    - Case 2: $\\mu=0.0$, $\\phi=1.0$, $\\sigma=0.8$, $X_0=1.2$, $T=10$, seed $=271828$.\n    - Case 3: $\\mu=0.1$, $\\phi=0.9$, $\\sigma=0.0$, $X_0=-0.3$, $T=15$, seed $=42$.\n    - Case 4: $\\mu=-0.2$, $\\phi=1.0$, $\\sigma=0.5$, $X_0=0.0$, $T=8$, seed $=7$.\n- **Output Requirements**: Report $[A_T, M_T]$ for each case, with values rounded to six decimal places, in a specified list-of-lists format.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is built upon fundamental concepts of discrete-time stochastic processes. The AR(1) model is a canonical example of a linear stochastic process, and the Doob-Meyer decomposition is a cornerstone theorem in martingale theory. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. For any adapted process, the Doob-Meyer decomposition is unique. All parameters, including the random seed for each simulation, are specified, ensuring that the required computations lead to a single, verifiable result.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task based on established mathematical principles. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe Doob-Meyer decomposition for a discrete-time adapted process $\\{X_t\\}_{t \\ge 0}$ states that it can be uniquely written as $X_t = X_0 + M_t + A_t$, where $\\{M_t\\}_{t \\ge 0}$ is a martingale and $\\{A_t\\}_{t \\ge 0}$ is a predictable process, with $M_0 = A_0 = 0$.\n\nThe decomposition is constructed from the process increments, $\\Delta X_t = X_t - X_{t-1}$. Each increment is split into its predictable part and a martingale difference:\n$$\n\\Delta X_t = \\Delta A_t + \\Delta M_t\n$$\nwhere $\\Delta A_t = A_t - A_{t-1}$ and $\\Delta M_t = M_t - M_{t-1}$.\n\nBy definition, the increment of the predictable process, $\\Delta A_t$, is the conditional expectation of the process increment given the information available at the previous time step:\n$$\n\\Delta A_t = \\mathbb{E}[\\Delta X_t | \\mathcal{F}_{t-1}] = \\mathbb{E}[X_t - X_{t-1} | \\mathcal{F}_{t-1}]\n$$\nThe increment of the martingale component, $\\Delta M_t$, is the innovation or \"surprise\" part of the increment:\n$$\n\\Delta M_t = \\Delta X_t - \\Delta A_t = (X_t - X_{t-1}) - \\mathbb{E}[X_t - X_{t-1} | \\mathcal{F}_{t-1}]\n$$\nBy construction, $\\mathbb{E}[\\Delta M_t | \\mathcal{F}_{t-1}] = 0$, which is the defining property of a martingale difference sequence.\n\nFor the given AR(1) process, $X_t = \\mu + \\phi X_{t-1} + \\varepsilon_t$, the increment is:\n$$\n\\Delta X_t = X_t - X_{t-1} = (\\mu + \\phi X_{t-1} + \\varepsilon_t) - X_{t-1} = \\mu + (\\phi - 1)X_{t-1} + \\varepsilon_t\n$$\nWe now compute the conditional expectation to find $\\Delta A_t$:\n$$\n\\Delta A_t = \\mathbb{E}[\\mu + (\\phi - 1)X_{t-1} + \\varepsilon_t | \\mathcal{F}_{t-1}]\n$$\nUsing the linearity of expectation and properties of conditional expectation:\n1.  $\\mu$ is a constant, so $\\mathbb{E}[\\mu | \\mathcal{F}_{t-1}] = \\mu$.\n2.  $X_{t-1}$ is known at time $t-1$, thus it is $\\mathcal{F}_{t-1}$-measurable. Therefore, $\\mathbb{E}[(\\phi - 1)X_{t-1} | \\mathcal{F}_{t-1}] = (\\phi - 1)X_{t-1}$.\n3.  The innovation $\\varepsilon_t$ is independent of the past filtration $\\mathcal{F}_{t-1} = \\sigma(X_0, \\varepsilon_1, \\dots, \\varepsilon_{t-1})$ and has mean zero. Thus, $\\mathbb{E}[\\varepsilon_t | \\mathcal{F}_{t-1}] = \\mathbb{E}[\\varepsilon_t] = 0$.\n\nCombining these results gives the increment of the predictable process:\n$$\n\\Delta A_t = \\mu + (\\phi - 1)X_{t-1}\n$$\nThe increment of the martingale is then:\n$$\n\\Delta M_t = \\Delta X_t - \\Delta A_t = (\\mu + (\\phi - 1)X_{t-1} + \\varepsilon_t) - (\\mu + (\\phi - 1)X_{t-1}) = \\varepsilon_t\n$$\nThe terminal values $A_T$ and $M_T$ are the sums of their respective increments from $t=1$ to $T$, since $A_0 = M_0 = 0$:\n$$\nA_T = \\sum_{t=1}^T \\Delta A_t = \\sum_{t=1}^T (\\mu + (\\phi - 1)X_{t-1})\n$$\n$$\nM_T = \\sum_{t=1}^T \\Delta M_t = \\sum_{t=1}^T \\varepsilon_t\n$$\n\n### Algorithmic Procedure\nThe numerical solution involves the following steps for each test case:\n1.  Initialize the parameters: $\\mu, \\phi, \\sigma, X_0, T,$ and the random seed.\n2.  Use the seed to initialize a pseudo-random number generator. Generate the entire sequence of $T$ innovations, $\\{\\varepsilon_t\\}_{t=1}^T$, from the distribution $\\mathcal{N}(0, \\sigma^2)$.\n3.  Initialize an array for the path $\\{X_t\\}$ of size $T+1$ with $X[0] = X_0$. Initialize the accumulators for the terminal values, $A_T = 0.0$ and $M_T = 0.0$.\n4.  Iterate from $t=1$ to $T$:\n    a. Retrieve the innovation $\\varepsilon_t$ for the current step.\n    b. Calculate $X_t = \\mu + \\phi X_{t-1} + \\varepsilon_t$.\n    c. Calculate the predictable increment $\\Delta A_t = \\mu + (\\phi - 1)X_{t-1}$.\n    d. Add this increment to the accumulator: $A_T \\leftarrow A_T + \\Delta A_t$.\n    e. Add the martingale increment $\\varepsilon_t$ to its accumulator: $M_T \\leftarrow M_T + \\varepsilon_t$. Note that this can also be calculated as a single sum of all innovations after the loop.\n5.  After the loop finishes, round the final values of $A_T$ and $M_T$ to six decimal places.\n6.  Store the pair $[A_T, M_T]$ and format the collected results as specified.\nThis procedure correctly implements the derived decomposition and produces the required outputs.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates AR(1) processes and computes the terminal values of their\n    Doob-Meyer decomposition components.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'mu': 0.5, 'phi': 0.8, 'sigma': 1.0, 'X0': 0.7, 'T': 12, 'seed': 314159},\n        {'mu': 0.0, 'phi': 1.0, 'sigma': 0.8, 'X0': 1.2, 'T': 10, 'seed': 271828},\n        {'mu': 0.1, 'phi': 0.9, 'sigma': 0.0, 'X0': -0.3, 'T': 15, 'seed': 42},\n        {'mu': -0.2, 'phi': 1.0, 'sigma': 0.5, 'X0': 0.0, 'T': 8, 'seed': 7},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu = case['mu']\n        phi = case['phi']\n        sigma = case['sigma']\n        X0 = case['X0']\n        T = case['T']\n        seed = case['seed']\n\n        # 1. Initialize RNG and generate all innovations\n        rng = np.random.default_rng(seed)\n        epsilons = rng.normal(loc=0.0, scale=sigma, size=T)\n\n        # 2. Initialize path array and terminal value accumulators\n        x_path = np.zeros(T + 1)\n        x_path[0] = X0\n        \n        A_T = 0.0\n\n        # 3. Simulate the path and compute the predictable component A_T\n        # The martingale component M_T is simply the sum of all innovations.\n        for t in range(1, T + 1):\n            # The t-th innovation corresponds to index t-1 in the epsilons array\n            epsilon_t = epsilons[t - 1]\n            x_prev = x_path[t - 1]\n            \n            # Update the process\n            x_path[t] = mu + phi * x_prev + epsilon_t\n            \n            # Increment for the predictable part A_t\n            # delta_A_t = E[X_t - X_{t-1} | F_{t-1}] = mu + (phi - 1) * X_{t-1}\n            delta_A_t = mu + (phi - 1) * x_prev\n            A_T += delta_A_t\n\n        # The martingale part M_T is the sum of innovations\n        M_T = np.sum(epsilons)\n        \n        # 4. Round to six decimal places as required\n        A_T_rounded = round(A_T, 6)\n        M_T_rounded = round(M_T, 6)\n        \n        results.append([A_T_rounded, M_T_rounded])\n\n    # 5. Format the final output string as a compact list of lists\n    # Each sublist [a, m] is formatted to ensure fixed precision and no spaces\n    string_parts = []\n    for res_pair in results:\n        a_str = f\"{res_pair[0]:.6f}\"\n        m_str = f\"{res_pair[1]:.6f}\"\n        string_parts.append(f\"[{a_str},{m_str}]\")\n        \n    final_output = f\"[{','.join(string_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "2388954"}, {"introduction": "Many important processes in science and finance evolve in continuous time, requiring a more powerful set of tools. This problem transitions to the continuous-time version of the Doob-Meyer decomposition, where Itô's formula becomes our primary instrument for analysis. By decomposing the square of a drifted Brownian motion, you will learn how to identify the local martingale and finite-variation components from a stochastic differential equation, a key skill in stochastic calculus [@problem_id:3050549].", "problem": "Consider a filtered probability space supporting a standard Brownian motion $B_t$ with $B_0=0$ and its usual augmented filtration. Fix a real constant $\\mu \\in \\mathbb{R}$, and define the continuous adapted process $X_t$ by $X_t = (B_t - \\mu t)^2$. Starting only from the core properties of Brownian motion (independent increments, stationarity, and quadratic variation $\\langle B \\rangle_t = t$) and Itô's formula for twice continuously differentiable functions of an Itô process, compute the canonical decomposition of $X_t$ into a continuous local martingale plus a predictable finite variation process with respect to the filtration of $B_t$. In particular, determine the compensator (the predictable finite variation component) $A_t$ in the decomposition $X_t = X_0 + M_t + A_t$, and express $A_t$ explicitly as a function of $t$, $\\mu$, and the path of $B$.\n\nYour final answer must be the closed-form analytic expression for $A_t$ only. In your derivation, explain how this compensator can be interpreted as $t$ plus a drift-dependent term. No numerical approximation is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and objective.\n\n### Step 1: Extract Givens\n-   A standard Brownian motion $B_t$ on a filtered probability space, with $B_0 = 0$.\n-   The filtration is the usual augmented filtration generated by $B_t$.\n-   A constant $\\mu \\in \\mathbb{R}$.\n-   A continuous adapted process $X_t$ defined by $X_t = (B_t - \\mu t)^2$.\n-   Required tools: Core properties of Brownian motion (independent increments, stationarity, quadratic variation $\\langle B \\rangle_t = t$) and Itô's formula.\n-   Objective: Compute the canonical decomposition of $X_t$ into a continuous local martingale $M_t$ and a predictable finite variation process $A_t$, such that $X_t = X_0 + M_t + A_t$.\n-   Specific task: Find the compensator $A_t$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically rigorous and falls squarely within the domain of stochastic calculus. The definition of the process $X_t$, the underlying Brownian motion, and the requested decomposition are all standard concepts. The problem is self-contained, providing all necessary information. It is well-posed, as the canonical (Doob-Meyer) decomposition of a continuous semimartingale is known to exist and be unique. The problem is objective and free of any scientific or logical flaws.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We proceed to the solution.\n\nThe process $X_t$ is given by $X_t = (B_t - \\mu t)^2$. We can express this as $X_t = f(t, B_t)$ for the function $f(t, x) = (x - \\mu t)^2$. The function $f(t,x)$ is twice continuously differentiable with respect to $x$ and once with respect to $t$. We can apply Itô's formula for a function of time and a stochastic process.\n\nThe general form of Itô's formula for a process $Y_t = f(t, W_t)$, where $W_t$ is an Itô process, is:\n$$dY_t = \\frac{\\partial f}{\\partial t}(t, W_t) dt + \\frac{\\partial f}{\\partial x}(t, W_t) dW_t + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2}(t, W_t) d\\langle W \\rangle_t$$\nIn our case, the process is the standard Brownian motion $W_t = B_t$. Its quadratic variation is given by $\\langle B \\rangle_t = t$, which in differential form is $d\\langle B \\rangle_t = dt$.\n\nFirst, we compute the necessary partial derivatives of $f(t, x) = (x - \\mu t)^2$:\n1.  The partial derivative with respect to time $t$:\n    $$\\frac{\\partial f}{\\partial t} = \\frac{\\partial}{\\partial t} (x^2 - 2\\mu tx + \\mu^2 t^2) = -2\\mu x + 2\\mu^2 t$$\n2.  The partial derivative with respect to the spatial variable $x$:\n    $$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} (x - \\mu t)^2 = 2(x - \\mu t)$$\n3.  The second partial derivative with respect to $x$:\n    $$\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} (2(x - \\mu t)) = 2$$\n\nNow, we substitute these derivatives evaluated at $(t, B_t)$ into Itô's formula for $X_t = f(t, B_t)$:\n$$dX_t = \\left(-2\\mu B_t + 2\\mu^2 t\\right) dt + 2(B_t - \\mu t) dB_t + \\frac{1}{2}(2) d\\langle B \\rangle_t$$\nUsing $d\\langle B \\rangle_t = dt$, the expression becomes:\n$$dX_t = \\left(-2\\mu B_t + 2\\mu^2 t\\right) dt + 2(B_t - \\mu t) dB_t + dt$$\nWe can collect the terms multiplied by $dt$:\n$$dX_t = (1 - 2\\mu B_t + 2\\mu^2 t) dt + 2(B_t - \\mu t) dB_t$$\nThis is the stochastic differential equation (SDE) for the process $X_t$. To find the decomposition of $X_t$, we integrate this SDE from $0$ to $t$:\n$$X_t - X_0 = \\int_0^t (1 - 2\\mu B_s + 2\\mu^2 s) ds + \\int_0^t 2(B_s - \\mu s) dB_s$$\nThe initial value of the process is $X_0 = (B_0 - \\mu \\cdot 0)^2 = (0 - 0)^2 = 0$.\nSo, we have the decomposition:\n$$X_t = \\int_0^t (1 - 2\\mu B_s + 2\\mu^2 s) ds + \\int_0^t 2(B_s - \\mu s) dB_s$$\nThis decomposition is of the form $X_t = A_t + M_t$, which is the canonical decomposition for a continuous semimartingale if $A_t$ is a predictable finite variation process and $M_t$ is a continuous local martingale, with $A_0 = M_0 = 0$.\n\nLet's identify the two components:\n1.  The martingale component $M_t$:\n    $$M_t = \\int_0^t 2(B_s - \\mu s) dB_s$$\n    This is an Itô integral. The integrand, $2(B_s - \\mu s)$, is a continuous and adapted process. Therefore, $M_t$ is a continuous local martingale, and $M_0 = 0$.\n\n2.  The finite variation component $A_t$ (the compensator):\n    $$A_t = \\int_0^t (1 - 2\\mu B_s + 2\\mu^2 s) ds$$\n    This is a Riemann integral with respect to time. The integrand, $(1 - 2\\mu B_s + 2\\mu^2 s)$, is a continuous and adapted process, which implies it is a predictable process. The integral of a predictable process with respect to time yields a predictable process. Furthermore, since the integrand is a.s. continuous on any finite time interval $[0, T]$, its integral is a.s. of finite variation on $[0, T]$. We also have $A_0 = 0$. Thus, $A_t$ is the unique predictable finite variation process in the canonical decomposition of $X_t$.\n\nThe problem asks for an explicit expression for the compensator $A_t$. We can evaluate the part of the integral that depends on $s$ explicitly:\n$$A_t = \\int_0^t (1 - 2\\mu B_s) ds + \\int_0^t 2\\mu^2 s ds$$\n$$A_t = \\int_0^t ds - 2\\mu \\int_0^t B_s ds + 2\\mu^2 \\left[\\frac{s^2}{2}\\right]_0^t$$\n$$A_t = t - 2\\mu \\int_0^t B_s ds + \\mu^2 t^2$$\nThis expression gives $A_t$ as a function of $t$, $\\mu$, and the path of the Brownian motion $B$ (via the integral $\\int_0^t B_s ds$).\n\nAs requested, we can interpret this compensator as $t$ plus a drift-dependent term. The term $t$ arises from the Itô correction, which accounts for the non-zero quadratic variation of the Brownian motion. It is the term $\\frac{1}{2} \\int_0^t \\frac{\\partial^2 f}{\\partial x^2} d\\langle B \\rangle_s = \\frac{1}{2} \\int_0^t 2 ds = t$. The remaining part, $\\mu^2 t^2 - 2\\mu \\int_0^t B_s ds$, originates from the classical chain rule term $\\int_0^t \\frac{\\partial f}{\\partial s}(s, B_s) ds$ and clearly depends on the drift parameter $\\mu$.", "answer": "$$\\boxed{t + \\mu^2 t^2 - 2\\mu \\int_0^t B_s ds}$$", "id": "3050549"}]}