{"hands_on_practices": [{"introduction": "We begin by exploring the crucial difference between knowing something \"now\" (adaptedness) and knowing it \"just a moment ago\" (predictability). This exercise [@problem_id:3070261] guides you to construct a simple process based on the increments of a random walk and prove from first principles that while its value is known at time $n$, it cannot be perfectly predicted at time $n-1$. This hands-on verification is fundamental to grasping the essence of predictability and computing the \"best guess\" given past information, known as the predictable projection.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{n})_{n\\geq 0},\\mathbb{P})$ where $(\\mathcal{F}_{n})_{n\\geq 0}$ is the natural filtration generated by an independent and identically distributed sequence $(\\varepsilon_{n})_{n\\geq 1}$ with $\\varepsilon_{n}\\in\\{-1,+1\\}$, $\\mathbb{P}(\\varepsilon_{n}=+1)=\\mathbb{P}(\\varepsilon_{n}=-1)=\\frac{1}{2}$, and $\\mathcal{F}_{n}=\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$. Define the process $(M_{n})_{n\\geq 0}$ by $M_{0}=0$ and $M_{n}=\\sum_{k=1}^{n}\\varepsilon_{k}$ for $n\\geq 1$. Let $(X_{n})_{n\\geq 1}$ be defined by $X_{n}=\\mathbf{1}_{\\{\\Delta M_{n}>0\\}}$, where $\\Delta M_{n}=M_{n}-M_{n-1}$.\n\nUsing only fundamental definitions of martingales, filtrations, adapted processes, and predictable processes in discrete time, proceed from first principles to:\n1. Establish that $(M_{n})_{n\\geq 0}$ is a symmetric martingale with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$.\n2. Show that $(X_{n})_{n\\geq 1}$ is an adapted process with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$.\n3. Prove that $(X_{n})_{n\\geq 1}$ is not predictable, that is, $X_{n}\\notin\\mathcal{F}_{n-1}$ for any $n\\geq 1$.\n\nFinally, compute the predictable projection of $X_{n}$, namely the conditional expectation $\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}]$, and present it as a single exact number. No rounding is required.", "solution": "This problem requires a rigorous, step-by-step verification of several properties of a discrete-time stochastic process, based on fundamental definitions. We will address each part of the problem in the order presented.\n\nThe problem defines a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{n})_{n\\geq 0},\\mathbb{P})$, with the filtration $(\\mathcal{F}_{n})_{n\\geq 0}$ being the natural filtration of an independent and identically distributed (i.i.d.) sequence of random variables $(\\varepsilon_{n})_{n\\geq 1}$. These variables follow a Rademacher distribution, $\\mathbb{P}(\\varepsilon_{n}=+1)=\\mathbb{P}(\\varepsilon_{n}=-1)=\\frac{1}{2}$. The filtration is given by $\\mathcal{F}_{n}=\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$ for $n\\geq 1$, and $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ is the trivial sigma-algebra. The process $(M_{n})_{n\\geq 0}$ is a simple symmetric random walk defined by $M_{0}=0$ and $M_{n}=\\sum_{k=1}^{n}\\varepsilon_{k}$ for $n\\geq 1$. The process $(X_{n})_{n\\geq 1}$ is defined as $X_{n}=\\mathbf{1}_{\\{\\Delta M_{n}>0\\}}$, where $\\Delta M_{n}=M_{n}-M_{n-1}$.\n\n**1. Proof that $(M_{n})_{n\\geq 0}$ is a symmetric martingale**\n\nTo establish that $(M_{n})_{n\\geq 0}$ is a martingale with respect to the filtration $(\\mathcal{F}_{n})_{n\\geq 0}$, we must verify three fundamental conditions.\n\n- **Adaptedness**: A process $(Y_{n})_{n\\geq 0}$ is adapted to $(\\mathcal{F}_{n})_{n\\geq 0}$ if $Y_{n}$ is $\\mathcal{F}_{n}$-measurable for every $n\\geq 0$.\nFor $n=0$, $M_{0}=0$. A constant random variable is measurable with respect to any sigma-algebra, including the trivial sigma-algebra $\\mathcal{F}_{0}$.\nFor $n\\geq 1$, $M_{n}=\\sum_{k=1}^{n}\\varepsilon_{k}$. By definition, $\\mathcal{F}_{n}=\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$. Each random variable $\\varepsilon_{k}$ for $k\\in\\{1, \\ldots, n\\}$ is $\\mathcal{F}_{n}$-measurable. The sum of $\\mathcal{F}_{n}$-measurable random variables is also $\\mathcal{F}_{n}$-measurable. Therefore, $M_{n}$ is $\\mathcal{F}_{n}$-measurable for all $n\\geq 1$.\nThus, the process $(M_{n})_{n\\geq 0}$ is adapted to $(\\mathcal{F}_{n})_{n\\geq 0}$.\n\n- **Integrability**: A random variable $Y$ is integrable if $\\mathbb{E}[|Y|]<\\infty$.\nFor any $n\\geq 0$, we have $|M_{n}|=|\\sum_{k=1}^{n}\\varepsilon_{k}|$. Using the triangle inequality, $|M_{n}|\\leq\\sum_{k=1}^{n}|\\varepsilon_{k}|$. Since $\\varepsilon_{k}\\in\\{-1,+1\\}$, we have $|\\varepsilon_{k}|=1$ for all $k$. Thus, $|M_{n}|\\leq\\sum_{k=1}^{n}1=n$.\nThe expectation of the absolute value is $\\mathbb{E}[|M_{n}|]\\leq\\mathbb{E}[n]=n$. Since $n$ is a finite integer, we have $\\mathbb{E}[|M_{n}|]<\\infty$ for all $n\\geq 0$.\nThus, $M_{n}$ is integrable for all $n\\geq 0$.\n\n- **Martingale Property**: The core property is $\\mathbb{E}[M_{n+1}\\mid\\mathcal{F}_{n}]=M_{n}$ for all $n\\geq 0$.\nFor any $n\\geq 0$, we can write $M_{n+1}=M_{n}+\\varepsilon_{n+1}$.\nTaking the conditional expectation with respect to $\\mathcal{F}_{n}$:\n$$ \\mathbb{E}[M_{n+1}\\mid\\mathcal{F}_{n}] = \\mathbb{E}[M_{n}+\\varepsilon_{n+1}\\mid\\mathcal{F}_{n}] $$\nBy the linearity of conditional expectation:\n$$ \\mathbb{E}[M_{n+1}\\mid\\mathcal{F}_{n}] = \\mathbb{E}[M_{n}\\mid\\mathcal{F}_{n}] + \\mathbb{E}[\\varepsilon_{n+1}\\mid\\mathcal{F}_{n}] $$\nSince $M_{n}$ is $\\mathcal{F}_{n}$-measurable (as shown in the adaptedness proof), $\\mathbb{E}[M_{n}\\mid\\mathcal{F}_{n}]=M_{n}$.\nThe random variable $\\varepsilon_{n+1}$ is, by definition, independent of the collection $(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$, and therefore independent of the sigma-algebra $\\mathcal{F}_{n}=\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$. For a random variable independent of a sigma-algebra, its conditional expectation is its unconditional expectation: $\\mathbb{E}[\\varepsilon_{n+1}\\mid\\mathcal{F}_{n}]=\\mathbb{E}[\\varepsilon_{n+1}]$.\nLet's compute $\\mathbb{E}[\\varepsilon_{n+1}]$:\n$$ \\mathbb{E}[\\varepsilon_{n+1}] = (+1)\\cdot\\mathbb{P}(\\varepsilon_{n+1}=+1) + (-1)\\cdot\\mathbb{P}(\\varepsilon_{n+1}=-1) = 1\\cdot\\frac{1}{2} - 1\\cdot\\frac{1}{2} = 0 $$\nSubstituting these results back, we get:\n$$ \\mathbb{E}[M_{n+1}\\mid\\mathcal{F}_{n}] = M_{n} + 0 = M_{n} $$\nThis holds for all $n\\geq 0$. The process is a symmetric martingale because the increments have zero conditional mean.\n\n**2. Proof that $(X_{n})_{n\\geq 1}$ is an adapted process**\n\nFor $(X_{n})_{n\\geq 1}$ to be adapted to $(\\mathcal{F}_{n})_{n\\geq 0}$, we must show that $X_{n}$ is $\\mathcal{F}_{n}$-measurable for each $n\\geq 1$.\nThe process is defined as $X_{n}=\\mathbf{1}_{\\{\\Delta M_{n}>0\\}}$. Let's first analyze the conditioning event.\n$$ \\Delta M_{n} = M_{n}-M_{n-1} = \\left(\\sum_{k=1}^{n}\\varepsilon_{k}\\right) - \\left(\\sum_{k=1}^{n-1}\\varepsilon_{k}\\right) = \\varepsilon_{n} $$\nSo, the definition of $X_{n}$ simplifies to $X_{n}=\\mathbf{1}_{\\{\\varepsilon_{n}>0\\}}$.\nSince the values of $\\varepsilon_{n}$ are restricted to $\\{-1,+1\\}$, the condition $\\varepsilon_{n}>0$ is equivalent to the condition $\\varepsilon_{n}=+1$.\nTherefore, $X_{n}=\\mathbf{1}_{\\{\\varepsilon_{n}=+1\\}}$.\nFor $X_{n}$ to be $\\mathcal{F}_{n}$-measurable, the set $\\{\\omega\\in\\Omega \\mid X_{n}(\\omega)=1\\}$ must be in $\\mathcal{F}_{n}$. This set is precisely $\\{\\omega\\in\\Omega \\mid \\varepsilon_{n}(\\omega)=+1\\}$.\nBy definition, the filtration is $\\mathcal{F}_{n}=\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n})$. This means $\\varepsilon_{n}$ is an $\\mathcal{F}_{n}$-measurable random variable. Consequently, for any Borel set $B\\subseteq\\mathbb{R}$, the pre-image $\\{\\omega\\in\\Omega \\mid \\varepsilon_{n}(\\omega)\\in B\\}$ is in $\\mathcal{F}_{n}$. Choosing $B=\\{+1\\}$, we find that the set $\\{\\varepsilon_{n}=+1\\}$ is in $\\mathcal{F}_{n}$.\nSince $X_{n}$ is the indicator function of an $\\mathcal{F}_{n}$-measurable set, $X_{n}$ is an $\\mathcal{F}_{n}$-measurable random variable. This holds for all $n\\geq 1$.\nThus, $(X_{n})_{n\\geq 1}$ is an adapted process.\n\n**3. Proof that $(X_{n})_{n\\geq 1}$ is not predictable**\n\nA discrete-time process $(Y_{n})_{n\\geq 1}$ is predictable with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$ if $Y_{n}$ is $\\mathcal{F}_{n-1}$-measurable for all $n\\geq 1$. We must show this condition fails for $(X_{n})_{n\\geq 1}$.\nWe need to show that for any $n\\geq 1$, $X_{n}$ is not $\\mathcal{F}_{n-1}$-measurable.\nAs shown above, $X_{n}=\\mathbf{1}_{\\{\\varepsilon_{n}=+1\\}}$. Let $A_{n}$ be the event $\\{\\varepsilon_{n}=+1\\}$.\nFor $X_{n}$ to be $\\mathcal{F}_{n-1}$-measurable, the set $A_{n}$ must be an element of the sigma-algebra $\\mathcal{F}_{n-1}$.\nThe filtration $\\mathcal{F}_{n-1}$ is defined as $\\sigma(\\varepsilon_{1},\\ldots,\\varepsilon_{n-1})$ (with $\\mathcal{F}_{0}$ being trivial). By the problem statement, the sequence $(\\varepsilon_{k})_{k\\geq 1}$ is independent. This implies that the random variable $\\varepsilon_{n}$ is independent of the random vector $(\\varepsilon_{1},\\ldots,\\varepsilon_{n-1})$, and therefore independent of the sigma-algebra $\\mathcal{F}_{n-1}$ generated by it.\nA standard result in probability theory states that if an event $A$ is independent of a sigma-algebra $\\mathcal{G}$, then if $A\\in\\mathcal{G}$, it must be that $\\mathbb{P}(A)=0$ or $\\mathbb{P}(A)=1$.\nLet's apply this. We are checking if $A_{n}\\in\\mathcal{F}_{n-1}$. Since $A_{n}$ is defined by $\\varepsilon_{n}$, it is independent of $\\mathcal{F}_{n-1}$. If we assume $A_{n}\\in\\mathcal{F}_{n-1}$, then we must have $\\mathbb{P}(A_{n})=0$ or $\\mathbb{P}(A_{n})=1$.\nHowever, the probability of the event $A_{n}$ is given as:\n$$ \\mathbb{P}(A_{n}) = \\mathbb{P}(\\varepsilon_{n}=+1) = \\frac{1}{2} $$\nSince $\\mathbb{P}(A_{n})=\\frac{1}{2}$, which is neither $0$ nor $1$, we have a contradiction.\nTherefore, the assumption that $A_{n}\\in\\mathcal{F}_{n-1}$ must be false. This means $X_{n}=\\mathbf{1}_{A_{n}}$ is not $\\mathcal{F}_{n-1}$-measurable for any $n\\geq 1$.\nConsequently, the process $(X_{n})_{n\\geq 1}$ is not predictable.\n\n**4. Computation of the predictable projection of $X_{n}$**\n\nThe predictable projection of the adapted process $(X_{n})_{n\\geq 1}$ is the process $(X_{n}^{*})_{n\\geq 1}$ defined by $X_{n}^{*}=\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}]$.\nWe need to compute this conditional expectation for any $n\\geq 1$.\n$$ X_{n}^{*} = \\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[\\mathbf{1}_{\\{\\varepsilon_{n}=+1\\}}\\mid\\mathcal{F}_{n-1}] $$\nBy the definition of conditional expectation of an indicator function, this is the conditional probability of the event:\n$$ X_{n}^{*} = \\mathbb{P}(\\varepsilon_{n}=+1\\mid\\mathcal{F}_{n-1}) $$\nAs established in the previous part, the random variable $\\varepsilon_{n}$ is independent of the sigma-algebra $\\mathcal{F}_{n-1}$. When an event is independent of the conditioning sigma-algebra, its conditional probability equals its unconditional probability.\nTherefore,\n$$ \\mathbb{P}(\\varepsilon_{n}=+1\\mid\\mathcal{F}_{n-1}) = \\mathbb{P}(\\varepsilon_{n}=+1) $$\nFrom the problem statement, we are given $\\mathbb{P}(\\varepsilon_{n}=+1)=\\frac{1}{2}$.\nThus, the predictable projection of $X_{n}$ is:\n$$ \\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] = \\frac{1}{2} $$\nThis result is a constant, independent of $n$ (for $n\\geq 1$) and of the specific realization of the process up to time $n-1$. This is the best prediction of $X_n$ given the information available at time $n-1$.\n\nThe final requested value is this predictable projection, presented as a single exact number.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3070261"}, {"introduction": "Now that we understand what a predictable process is, we explore why this property is so important in the context of stochastic integration. This practice [@problem_id:3070242] investigates the discrete stochastic integral, a central tool in financial mathematics and stochastic calculus, and reveals what happens when the trading strategy or integrand process is not predictable. By working through the calculation, you will see precisely how the desirable martingale property of the resulting wealth process breaks down, demonstrating the necessity of predictability.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be the countable product of the two-point space with equal weights, that is, $\\Omega=\\{-1,1\\}^{\\mathbb{N}}$ with the product $\\sigma$-algebra and product probability measure. For $\\omega=(\\omega_{1},\\omega_{2},\\dots)\\in\\Omega$, define $\\xi_{n}(\\omega)=\\omega_{n}$, so that $\\{\\xi_{n}:n\\in\\mathbb{N}\\}$ are independent and identically distributed (i.i.d.) random variables with $\\mathbb{P}(\\xi_{n}=1)=\\mathbb{P}(\\xi_{n}=-1)=\\frac{1}{2}$. Let the filtration be $\\mathcal{F}_{n}=\\sigma(\\xi_{1},\\dots,\\xi_{n})$ for $n\\geq 0$, with $\\mathcal{F}_{0}$ trivial.\n\nDefine the discrete-time martingale $M_{0}=0$ and\n$$\nM_{n}=\\sum_{k=1}^{n}\\xi_{k},\\qquad n\\geq 1,\n$$\nwith increments $\\Delta M_{n}=M_{n}-M_{n-1}=\\xi_{n}$. Consider the adapted process $H_{n}=\\xi_{n}$, $n\\geq 1$. The discrete-time stochastic integral of $H$ against $M$ is the process\n$$\n(H\\cdot M)_{n}=\\sum_{k=1}^{n}H_{k}\\,\\Delta M_{k},\\qquad n\\geq 1.\n$$\n\nTasks:\n1. Using the definitions of adapted and predictable processes relative to a filtration, justify that $(H_{n})$ is adapted to $(\\mathcal{F}_{n})$ but is not predictable.\n2. Compute the conditional expectation $\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]$ and use it to demonstrate that $(H\\cdot M)_{n}$ fails to be a martingale.\n3. Provide the exact common value of $\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]$ for all $n\\geq 1$ as the final answer.\n\nNo rounding is necessary. Express the final answer as an exact real number.", "solution": "The problem as stated is mathematically well-defined, self-contained, and consistent. It represents a standard exercise in the theory of discrete-time stochastic processes. We may therefore proceed with a complete solution.\n\nThe solution is divided into three parts corresponding to the tasks posed.\n\n**1. Justification of Adaptedness and Non-Predictability of $(H_{n})$**\n\nFirst, we recall the definitions of adapted and predictable processes. A process $(X_{n})_{n\\geq 0}$ is said to be **adapted** to a filtration $(\\mathcal{F}_{n})_{n\\geq 0}$ if for every $n\\geq 0$, the random variable $X_{n}$ is $\\mathcal{F}_{n}$-measurable. A process $(X_{n})_{n\\geq 1}$ is said to be **predictable** with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$ if for every $n\\geq 1$, the random variable $X_{n}$ is $\\mathcal{F}_{n-1}$-measurable.\n\nThe process in question is $H_{n}=\\xi_{n}$ for $n\\geq 1$. The filtration is defined as $\\mathcal{F}_{n}=\\sigma(\\xi_{1},\\dots,\\xi_{n})$ for $n\\geq 1$ and $\\mathcal{F}_{0}$ is the trivial $\\sigma$-algebra.\n\nTo show that $(H_{n})$ is adapted, we must verify that for each $n \\geq 1$, $H_{n}$ is $\\mathcal{F}_{n}$-measurable. By definition, $H_{n}=\\xi_{n}$. The filtration $\\mathcal{F}_{n}$ is the $\\sigma$-algebra generated by the random variables $\\xi_{1}, \\dots, \\xi_{n}$. A generating random variable of a $\\sigma$-algebra is, by definition, measurable with respect to that $\\sigma$-algebra. Therefore, $\\xi_{n}$ is $\\mathcal{F}_{n}$-measurable for each $n\\geq 1$. This confirms that the process $(H_{n})_{n\\geq 1}$ is adapted to the filtration $(\\mathcal{F}_{n})_{n\\geq 1}$.\n\nTo show that $(H_{n})$ is not predictable, we must verify that for some $n\\geq 1$, $H_{n}$ is not $\\mathcal{F}_{n-1}$-measurable. Let's consider any $n\\geq 1$. Here $H_{n}=\\xi_{n}$ and the relevant $\\sigma$-algebra is $\\mathcal{F}_{n-1}=\\sigma(\\xi_{1},\\dots,\\xi_{n-1})$ (where for $n=1$, $\\mathcal{F}_{0}$ is trivial). The problem states that the random variables $\\{\\xi_{k}\\}$ are independent. This implies that $\\xi_{n}$ is independent of the collection $\\{\\xi_{1},\\dots,\\xi_{n-1}\\}$, and therefore $\\xi_{n}$ is independent of the $\\sigma$-algebra $\\mathcal{F}_{n-1}$ that they generate.\n\nA random variable that is measurable with respect to a $\\sigma$-algebra cannot be independent of it, unless the random variable is constant almost surely. Here, $\\xi_{n}$ is not constant, as $\\mathbb{P}(\\xi_{n}=1)=\\mathbb{P}(\\xi_{n}=-1)=\\frac{1}{2}$. To be more formal, suppose for contradiction that $\\xi_{n}$ is $\\mathcal{F}_{n-1}$-measurable. Then the conditional expectation $\\mathbb{E}[\\xi_{n}|\\mathcal{F}_{n-1}]$ would be equal to $\\xi_{n}$ itself. However, due to independence, we have $\\mathbb{E}[\\xi_{n}|\\mathcal{F}_{n-1}]=\\mathbb{E}[\\xi_{n}]$. The expectation is $\\mathbb{E}[\\xi_{n}]=(1)\\cdot\\frac{1}{2}+(-1)\\cdot\\frac{1}{2}=0$. This would imply $\\xi_{n}=0$ almost surely, which contradicts the fact that $\\xi_{n}$ only takes values $1$ and $-1$. Therefore, $\\xi_{n}$ is not $\\mathcal{F}_{n-1}$-measurable for any $n\\geq 1$. Consequently, the process $(H_{n})$ is not predictable.\n\n**2. Failure of $(H\\cdot M)_{n}$ to be a Martingale**\n\nLet $X_{n}=(H\\cdot M)_{n}=\\sum_{k=1}^{n}H_{k}\\,\\Delta M_{k}$. For $(X_{n})$ to be a martingale with respect to $(\\mathcal{F}_{n})$, it must satisfy, among other properties, that $\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}]=X_{n-1}$ for all $n\\geq 1$. Let us examine this condition.\n\nWe can write $X_{n}=X_{n-1}+H_{n}\\Delta M_{n}$ for $n\\geq 1$ (with $X_0=0$). Using the linearity of conditional expectation, we have:\n$$\n\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n-1}+H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n-1}\\mid\\mathcal{F}_{n-1}] + \\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]\n$$\nThe process term $X_{n-1}$ is a sum of terms $H_{k}\\Delta M_{k} = \\xi_k^2$ for $k=1, \\dots, n-1$. Each $\\xi_{k}$ is $\\mathcal{F}_{k}$-measurable and thus $\\mathcal{F}_{n-1}$-measurable for $k \\le n-1$. Therefore, $X_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable. For any $\\mathcal{F}_{n-1}$-measurable random variable $Y$, we have $\\mathbb{E}[Y\\mid\\mathcal{F}_{n-1}]=Y$. Thus, $\\mathbb{E}[X_{n-1}\\mid\\mathcal{F}_{n-1}]=X_{n-1}$. The equation becomes:\n$$\n\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] = X_{n-1} + \\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]\n$$\nNow, we compute the conditional expectation term. We are given $H_{n}=\\xi_{n}$ and $\\Delta M_{n}=\\xi_{n}$. So, the term is $\\mathbb{E}[\\xi_{n}\\cdot\\xi_{n}\\mid\\mathcal{F}_{n-1}]=\\mathbb{E}[\\xi_{n}^{2}\\mid\\mathcal{F}_{n-1}]$.\nThe random variable $\\xi_{n}$ takes values in the set $\\{-1, 1\\}$. For any outcome $\\omega\\in\\Omega$, $\\xi_{n}(\\omega)$ is either $-1$ or $1$. In either case, $\\xi_{n}^{2}(\\omega)=1$. This means $\\xi_{n}^{2}$ is a constant random variable, equal to $1$ everywhere. A constant random variable is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_{n-1}$. Its conditional expectation is just the constant itself:\n$$\n\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[\\xi_{n}^{2}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[1\\mid\\mathcal{F}_{n-1}] = 1\n$$\nSubstituting this result back, we find:\n$$\n\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] = X_{n-1} + 1\n$$\nSince $\\mathbb{E}[X_{n}\\mid\\mathcal{F}_{n-1}] \\neq X_{n-1}$, the process $(H\\cdot M)_{n}$ is not a martingale with respect to $(\\mathcal{F}_{n})$. This failure is a direct consequence of the integrand $(H_n)$ not being predictable. The discrete stochastic integral of a non-predictable process with respect to a martingale is not guaranteed to be a martingale.\n\n**3. The Common Value of $\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]$**\n\nAs calculated in the second part, for any integer $n\\geq 1$:\n$$\n\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}] = \\mathbb{E}[\\xi_{n}^{2}\\mid\\mathcal{F}_{n-1}]\n$$\nSince $\\xi_{n}\\in\\{-1,1\\}$, it follows that $\\xi_{n}^{2}=1$ for all $n$. The expression simplifies to:\n$$\n\\mathbb{E}[1\\mid\\mathcal{F}_{n-1}] = 1\n$$\nThis value is constant and does not depend on $n$. Therefore, the exact common value of $\\mathbb{E}[H_{n}\\Delta M_{n}\\mid\\mathcal{F}_{n-1}]$ for all $n\\geq 1$ is $1$.", "answer": "$$\\boxed{1}$$", "id": "3070242"}, {"introduction": "The concept of predictability is not limited to value processes; it also provides a powerful way to classify random times. This exercise [@problem_id:3070247] introduces the concept of a predictable stopping time, which can be thought of as a random time that is \"announced\" one step before it occurs. You will analyze the first time a random walk's increment is positive, prove it is a stopping time, and then demonstrate that it is not predictable, making it a classic example of a \"surprising\" event.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $(\\mathcal{F}_{n})_{n\\ge 0}$ be the natural filtration generated by a sequence of independent and identically distributed random variables $(X_{n})_{n\\ge 1}$ with $X_{n}\\sim \\mathcal{N}(0,1)$ for all $n\\ge 1$, where $\\mathcal{N}(0,1)$ denotes the standard normal distribution. Define the discrete-time process $(M_{n})_{n\\ge 0}$ by $M_{0}=0$ and $M_{n}=\\sum_{k=1}^{n}X_{k}$ for $n\\ge 1$, and its increment process $\\Delta M_{n}=M_{n}-M_{n-1}=X_{n}$. Recall the fundamental definitions: a process $(M_{n})_{n\\ge 0}$ adapted to $(\\mathcal{F}_{n})_{n\\ge 0}$ is a martingale if $\\mathbb{E}[|M_{n}|]<\\infty$, $\\mathbb{E}[M_{n}\\mid \\mathcal{F}_{n-1}]=M_{n-1}$ for all $n\\ge 1$; a random time $\\tau:\\Omega\\to\\{0,1,2,\\dots\\}\\cup\\{\\infty\\}$ is a stopping time if $\\{\\tau\\le n\\}\\in\\mathcal{F}_{n}$ for all $n\\ge 0$; and in discrete time, a stopping time $\\tau$ is predictable if $\\{\\tau=n\\}\\in\\mathcal{F}_{n-1}$ for all $n\\ge 1$. Consider the random time\n$$\n\\tau=\\inf\\{n\\ge 1:\\Delta M_{n}>0\\}=\\inf\\{n\\ge 1:X_{n}>0\\},\n$$\nthe first index at which the increment becomes strictly positive.\n\nTasks:\n- Starting from the above definitions and the properties of $(X_{n})_{n\\ge 1}$, justify that $\\tau$ is a stopping time with respect to $(\\mathcal{F}_{n})_{n\\ge 0}$.\n- Using properties of conditional expectation and independence, compute the conditional expectation $\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right]$ in closed form for any fixed $n\\ge 1$, where $\\mathbf{1}_{A}$ denotes the indicator of an event $A$.\n- Explain how this computation implies that, for some $n\\ge 1$, the event $\\{\\tau=n\\}\\notin\\mathcal{F}_{n-1}$, so that $\\tau$ is not predictable.\n\nYour final answer must be the single closed-form analytic expression for $\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right]$. No rounding is required, and no units apply.", "solution": "The problem is analyzed and solved in three parts as requested.\n\nFirst, we justify that the random time $\\tau$ is a stopping time with respect to the filtration $(\\mathcal{F}_{n})_{n\\ge 0}$.\nA random time $\\tau$ is a stopping time if the event $\\{\\tau \\le n\\}$ is in the sigma-algebra $\\mathcal{F}_{n}$ for all $n \\ge 0$. The filtration is given by $\\mathcal{F}_n = \\sigma(X_1, X_2, \\dots, X_n)$ for $n \\ge 1$ and $\\mathcal{F}_0$ is the trivial sigma-algebra $\\{\\emptyset, \\Omega\\}$.\n\nFor $n=0$: The definition of $\\tau$ is $\\tau=\\inf\\{k\\ge 1:X_{k}>0\\}$, so $\\tau$ must be greater than or equal to $1$. This means the event $\\{\\tau \\le 0\\}$ is impossible, so $\\{\\tau \\le 0\\} = \\emptyset$. The empty set is an element of any sigma-algebra, so $\\{\\tau \\le 0\\} \\in \\mathcal{F}_0$.\n\nFor $n\\ge 1$: It is often easier to work with the complement event $\\{\\tau > n\\}$. The event $\\{\\tau > n\\}$ occurs if and only if the condition $X_k > 0$ is not met for any $k \\in \\{1, 2, \\dots, n\\}$. This is equivalent to the condition that $X_k \\le 0$ for all $k \\in \\{1, 2, \\dots, n\\}$. We can write this event as an intersection:\n$$\n\\{\\tau > n\\} = \\{X_1 \\le 0, X_2 \\le 0, \\dots, X_n \\le 0\\} = \\bigcap_{k=1}^{n} \\{X_k \\le 0\\}\n$$\nFor each $k \\in \\{1, 2, \\dots, n\\}$, the random variable $X_k$ is, by definition of the natural filtration, measurable with respect to $\\mathcal{F}_n = \\sigma(X_1, \\dots, X_n)$. The set $(-\\infty, 0]$ is a Borel set in $\\mathbb{R}$. Therefore, the event $\\{X_k \\le 0\\} = X_k^{-1}((-\\infty, 0])$ is in $\\mathcal{F}_n$. Since a sigma-algebra is closed under finite intersections, the event $\\{\\tau > n\\}$ is also in $\\mathcal{F}_n$.\nA sigma-algebra is also closed under complementation. Thus, the complement of $\\{\\tau > n\\}$, which is $\\{\\tau \\le n\\}$, must also be in $\\mathcal{F}_n$.\nThis holds for all $n \\ge 1$. Combining with the result for $n=0$, we have shown that $\\{\\tau \\le n\\} \\in \\mathcal{F}_n$ for all $n \\ge 0$. Therefore, $\\tau$ is a stopping time.\n\nSecond, we compute the conditional expectation $\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right]$ for a fixed integer $n\\ge 1$.\nThe event $\\{\\tau=n\\}$ occurs if and only if $X_n > 0$ and $X_k \\le 0$ for all $k \\in \\{1, 2, \\dots, n-1\\}$. The indicator function of this event can be written as a product:\n$$\n\\mathbf{1}_{\\{\\tau=n\\}} = \\mathbf{1}_{\\{X_1 \\le 0, \\dots, X_{n-1} \\le 0\\}} \\mathbf{1}_{\\{X_n > 0\\}} = \\left(\\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}\\right) \\mathbf{1}_{\\{X_n > 0\\}}\n$$\nWe wish to compute its expectation conditional on $\\mathcal{F}_{n-1} = \\sigma(X_1, \\dots, X_{n-1})$. We use the property of \"taking out what is known\": if a random variable $Y$ is $\\mathcal{G}$-measurable, then $\\mathbb{E}[YZ \\mid \\mathcal{G}] = Y\\mathbb{E}[Z \\mid \\mathcal{G}]$.\nLet's define $Y = \\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}$. This random variable is a function of $X_1, \\dots, X_{n-1}$, so it is measurable with respect to $\\mathcal{F}_{n-1}$. We can therefore take it outside the conditional expectation:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] = \\mathbb{E}\\!\\left[\\left(\\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}\\right) \\mathbf{1}_{\\{X_n > 0\\}} \\mid \\mathcal{F}_{n-1}\\right] = \\left(\\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}\\right) \\mathbb{E}\\!\\left[\\mathbf{1}_{\\{X_n > 0\\}} \\mid \\mathcal{F}_{n-1}\\right]\n$$\nNow we evaluate the remaining term, $\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{X_n > 0\\}} \\mid \\mathcal{F}_{n-1}\\right]$. The sequence $(X_k)_{k \\ge 1}$ is independent. This implies that the random variable $X_n$ is independent of the sigma-algebra $\\mathcal{F}_{n-1} = \\sigma(X_1, \\dots, X_{n-1})$. Consequently, any function of $X_n$, such as $\\mathbf{1}_{\\{X_n > 0\\}}$, is also independent of $\\mathcal{F}_{n-1}$. For a random variable $Z$ independent of a sigma-algebra $\\mathcal{G}$, we have $\\mathbb{E}[Z \\mid \\mathcal{G}] = \\mathbb{E}[Z]$. Thus:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{X_n > 0\\}} \\mid \\mathcal{F}_{n-1}\\right] = \\mathbb{E}\\!\\left[\\mathbf{1}_{\\{X_n > 0\\}}\\right] = \\mathbb{P}(X_n > 0)\n$$\nThe random variable $X_n$ follows a standard normal distribution $\\mathcal{N}(0,1)$, which is continuous and symmetric about $0$. Therefore, $\\mathbb{P}(X_n > 0) = \\frac{1}{2}$.\nSubstituting this result back, we get:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] = \\left(\\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}\\right) \\frac{1}{2}\n$$\nThe product of indicators $\\prod_{k=1}^{n-1}\\mathbf{1}_{\\{X_k \\le 0\\}}$ is the indicator function for the event $\\{X_1 \\le 0, \\dots, X_{n-1} \\le 0\\}$. This event is precisely $\\{\\tau > n-1\\}$, or equivalently, $\\{\\tau \\ge n\\}$. So we can write:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] = \\frac{1}{2}\\mathbf{1}_{\\{\\tau \\ge n\\}}\n$$\nThis is the required closed-form expression.\n\nThird, we explain how this computation implies that $\\tau$ is not predictable.\nA stopping time $\\tau$ is predictable if, by definition, the event $\\{\\tau=n\\}$ is in the sigma-algebra $\\mathcal{F}_{n-1}$ for all $n \\ge 1$.\nA fundamental theorem of conditional expectation states that a random variable $Y$ is measurable with respect to a sigma-algebra $\\mathcal{G}$ if and only if $\\mathbb{E}[Y \\mid \\mathcal{G}] = Y$.\nIf $\\tau$ were a predictable stopping time, then for any $n \\ge 1$, the event $\\{\\tau=n\\}$ would belong to $\\mathcal{F}_{n-1}$. This would mean its indicator function, $Y = \\mathbf{1}_{\\{\\tau=n\\}}$, is an $\\mathcal{F}_{n-1}$-measurable random variable.\nApplying the theorem with $\\mathcal{G} = \\mathcal{F}_{n-1}$, we would necessarily have:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] = \\mathbf{1}_{\\{\\tau=n\\}}\n$$\nHowever, our calculation in the second part yielded:\n$$\n\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] = \\frac{1}{2}\\mathbf{1}_{\\{\\tau \\ge n\\}}\n$$\nThus, for $\\tau$ to be predictable, the following equality of random variables must hold for all $n \\ge 1$:\n$$\n\\mathbf{1}_{\\{\\tau=n\\}} = \\frac{1}{2}\\mathbf{1}_{\\{\\tau \\ge n\\}}\n$$\nThis equality cannot be true in general. The random variable on the left, $\\mathbf{1}_{\\{\\tau=n\\}}$, takes values in $\\{0, 1\\}$. The random variable on the right, $\\frac{1}{2}\\mathbf{1}_{\\{\\tau \\ge n\\}}$, takes values in $\\{0, \\frac{1}{2}\\}$.\nThe equality can only hold if both sides are identically the zero random variable. However, the event $\\{\\tau=n\\}$ is not a null event. Its probability is $\\mathbb{P}(\\tau=n) = \\mathbb{P}(X_1 \\le 0, \\dots, X_{n-1} \\le 0, X_n > 0) = (\\frac{1}{2})^{n-1}(\\frac{1}{2}) = (\\frac{1}{2})^n > 0$.\nSince $\\mathbb{P}(\\tau=n) > 0$, there exists some outcome $\\omega \\in \\Omega$ for which $\\tau(\\omega)=n$. For such an $\\omega$, $\\mathbf{1}_{\\{\\tau=n\\}}(\\omega)=1$ and $\\mathbf{1}_{\\{\\tau \\ge n\\}}(\\omega)=1$. The purported equality would require $1 = \\frac{1}{2}$, which is a contradiction.\nSince $\\mathbb{E}\\!\\left[\\mathbf{1}_{\\{\\tau=n\\}}\\mid \\mathcal{F}_{n-1}\\right] \\neq \\mathbf{1}_{\\{\\tau=n\\}}$, the random variable $\\mathbf{1}_{\\{\\tau=n\\}}$ is not $\\mathcal{F}_{n-1}$-measurable. This means the event $\\{\\tau=n\\}$ is not in $\\mathcal{F}_{n-1}$ (for any $n$ such that the event has non-zero probability). This violates the definition of a predictable stopping time. Therefore, $\\tau$ is not predictable.\nThe intuition is that the event $\\{\\tau=n\\}$ depends on the outcome of $X_n$, which is new information that becomes available at time $n$ and is not known at time $n-1$. A predictable time is one that can be \"announced\" at the step just before it happens. This is not the case for $\\tau$.\nThe final answer is the expression derived in the second part.", "answer": "$$\n\\boxed{\\frac{1}{2}\\mathbf{1}_{\\{\\tau \\ge n\\}}}\n$$", "id": "3070247"}]}