{"hands_on_practices": [{"introduction": "Mastering stochastic calculus begins with a deep understanding of its cornerstone: Brownian motion. This practice guides you through the essential exercise of verifying, from first principles, that both standard Brownian motion $B_t$ and the related process $B_t^2 - t$ are martingales. By then applying Doob's maximal inequality, you will see how identifying a martingale structure immediately provides powerful tools for controlling the pathwise behavior of a process [@problem_id:3050343].", "problem": "Let $\\{B_t\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t \\ge 0},\\mathbb{P})$, where $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ is the natural filtration generated by $\\{B_t\\}_{t \\ge 0}$ and augmented to satisfy the usual conditions. Starting only from the defining properties of standard Brownian motion (stationary independent increments, Gaussian increments with mean $0$ and variance equal to the time increment, and continuity), do the following:\n- Verify that $\\{B_t\\}_{t \\ge 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\ge 0}$.\n- Verify that $\\{B_t^2 - t\\}_{t \\ge 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\ge 0}$.\n\nThen, using Doob’s square-integrable maximal inequality for martingales together with your verification above, derive an explicit upper bound for $\\mathbb{E}\\!\\left[\\sup_{0 \\le s \\le t} B_s^2\\right]$ in terms of $t$. Express your final result as a single simplified symbolic expression in $t$ (without inequality symbols). No rounding is required.", "solution": "First, we address the two verification tasks. A process $\\{X_t\\}_{t \\ge 0}$ is a martingale with respect to the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ if it satisfies three conditions for all $s, t$ with $0 \\le s \\le t$:\n1. $X_t$ is $\\mathcal{F}_t$-measurable.\n2. $\\mathbb{E}[|X_t|]  \\infty$.\n3. $\\mathbb{E}[X_t | \\mathcal{F}_s] = X_s$.\n\n**Verification that $\\{B_t\\}_{t \\ge 0}$ is a martingale:**\n\n1.  **Measurability**: The filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ is the natural filtration generated by $\\{B_t\\}_{t \\ge 0}$, defined as $\\mathcal{F}_t = \\sigma(B_u : u \\le t)$ (augmented with null sets). By this definition, $B_t$ is $\\mathcal{F}_t$-measurable for every $t \\ge 0$.\n2.  **Integrability**: $B_t$ follows a normal distribution $\\mathcal{N}(0, t)$. The expected value of its absolute value is given by $\\mathbb{E}[|B_t|] = \\sqrt{\\frac{2}{\\pi}} \\sqrt{\\text{Var}(B_t)} = \\sqrt{\\frac{2t}{\\pi}}$. For any finite $t$, this value is finite. Thus, $\\mathbb{E}[|B_t|]  \\infty$.\n3.  **Martingale Property**: For $s \\le t$, we compute the conditional expectation $\\mathbb{E}[B_t | \\mathcal{F}_s]$. We decompose $B_t$ as $B_t = B_s + (B_t - B_s)$.\n    $$ \\mathbb{E}[B_t | \\mathcal{F}_s] = \\mathbb{E}[B_s + (B_t - B_s) | \\mathcal{F}_s] $$\n    By the linearity of conditional expectation:\n    $$ \\mathbb{E}[B_t | \\mathcal{F}_s] = \\mathbb{E}[B_s | \\mathcal{F}_s] + \\mathbb{E}[B_t - B_s | \\mathcal{F}_s] $$\n    Since $B_s$ is $\\mathcal{F}_s$-measurable, $\\mathbb{E}[B_s | \\mathcal{F}_s] = B_s$. The increment $B_t - B_s$ is independent of the sigma-algebra $\\mathcal{F}_s = \\sigma(B_u : u \\le s)$ due to the independent increments property of Brownian motion. Therefore, its conditional expectation equals its unconditional expectation:\n    $$ \\mathbb{E}[B_t - B_s | \\mathcal{F}_s] = \\mathbb{E}[B_t - B_s] $$\n    The increment $B_t - B_s$ has mean $0$, so $\\mathbb{E}[B_t - B_s] = 0$. Combining these results, we get:\n    $$ \\mathbb{E}[B_t | \\mathcal{F}_s] = B_s + 0 = B_s $$\n    All three conditions are satisfied, confirming that $\\{B_t\\}_{t \\ge 0}$ is a martingale.\n\n**Verification that $\\{B_t^2 - t\\}_{t \\ge 0}$ is a martingale:**\n\nLet $X_t = B_t^2 - t$.\n1.  **Measurability**: Since $B_t$ is $\\mathcal{F}_t$-measurable, its square $B_t^2$ is also $\\mathcal{F}_t$-measurable. The term $t$ is a deterministic function of time, and is thus adapted. Therefore, their difference, $X_t = B_t^2 - t$, is $\\mathcal{F}_t$-measurable.\n2.  **Integrability**: We need to show $\\mathbb{E}[|B_t^2 - t|]  \\infty$. Using the triangle inequality, $\\mathbb{E}[|B_t^2 - t|] \\le \\mathbb{E}[|B_t^2|] + |t| = \\mathbb{E}[B_t^2] + t$. The variance of $B_t$ is $\\text{Var}(B_t) = \\mathbb{E}[B_t^2] - (\\mathbb{E}[B_t])^2$. Since $\\mathbb{E}[B_t]=0$ and $\\text{Var}(B_t)=t$, we have $\\mathbb{E}[B_t^2] = t$. Thus, $\\mathbb{E}[|B_t^2 - t|] \\le t + t = 2t$, which is finite for any finite $t$.\n3.  **Martingale Property**: For $s \\le t$, we compute $\\mathbb{E}[B_t^2 - t | \\mathcal{F}_s]$.\n    $$ \\mathbb{E}[B_t^2 - t | \\mathcal{F}_s] = \\mathbb{E}[B_t^2 | \\mathcal{F}_s] - \\mathbb{E}[t | \\mathcal{F}_s] = \\mathbb{E}[B_t^2 | \\mathcal{F}_s] - t $$\n    We decompose $B_t^2$ as we did for $B_t$:\n    $$ B_t^2 = (B_s + (B_t - B_s))^2 = B_s^2 + 2B_s(B_t - B_s) + (B_t - B_s)^2 $$\n    Taking the conditional expectation with respect to $\\mathcal{F}_s$:\n    $$ \\mathbb{E}[B_t^2 | \\mathcal{F}_s] = \\mathbb{E}[B_s^2 | \\mathcal{F}_s] + \\mathbb{E}[2B_s(B_t - B_s) | \\mathcal{F}_s] + \\mathbb{E}[(B_t - B_s)^2 | \\mathcal{F}_s] $$\n    - Since $B_s^2$ is $\\mathcal{F}_s$-measurable, $\\mathbb{E}[B_s^2 | \\mathcal{F}_s] = B_s^2$.\n    - Since $B_s$ is $\\mathcal{F}_s$-measurable, we can take it out of the conditional expectation: $\\mathbb{E}[2B_s(B_t - B_s) | \\mathcal{F}_s] = 2B_s \\mathbb{E}[B_t - B_s | \\mathcal{F}_s]$. As established before, $\\mathbb{E}[B_t - B_s | \\mathcal{F}_s] = 0$. So this term is $0$.\n    - Since $B_t - B_s$ is independent of $\\mathcal{F}_s$, $\\mathbb{E}[(B_t - B_s)^2 | \\mathcal{F}_s] = \\mathbb{E}[(B_t - B_s)^2]$. This is the variance of the increment, which is $t-s$.\n    Combining these terms, we have:\n    $$ \\mathbb{E}[B_t^2 | \\mathcal{F}_s] = B_s^2 + 0 + (t-s) = B_s^2 + t - s $$\n    Substituting this back into our original expression:\n    $$ \\mathbb{E}[B_t^2 - t | \\mathcal{F}_s] = (B_s^2 + t - s) - t = B_s^2 - s $$\n    This is precisely $X_s$. All three conditions are satisfied, confirming that $\\{B_t^2-t\\}_{t \\ge 0}$ is a martingale.\n\n**Derivation of the upper bound:**\n\nThe problem requires using Doob’s square-integrable maximal inequality for martingales. For a right-continuous martingale $\\{M_t\\}_{t \\ge 0}$, this inequality states:\n$$ \\mathbb{E}\\left[\\sup_{0 \\le s \\le t} M_s^2\\right] \\le 4 \\mathbb{E}[M_t^2] $$\nWe have verified that standard Brownian motion $\\{B_t\\}_{t \\ge 0}$ is a martingale. Since its paths are continuous, it is also right-continuous. We can therefore apply the inequality directly to the martingale $M_t = B_t$.\n$$ \\mathbb{E}\\left[\\sup_{0 \\le s \\le t} B_s^2\\right] \\le 4 \\mathbb{E}[B_t^2] $$\nWe have already calculated $\\mathbb{E}[B_t^2]$ as the variance of $B_t$, which is given as $t$.\n$$ \\mathbb{E}[B_t^2] = t $$\nSubstituting this value into the inequality gives:\n$$ \\mathbb{E}\\left[\\sup_{0 \\le s \\le t} B_s^2\\right] \\le 4t $$\nThis provides an explicit upper bound for the desired quantity. The problem asks for the single simplified symbolic expression for this upper bound.\n\nThe upper bound for $\\mathbb{E}\\!\\left[\\sup_{0 \\le s \\le t} B_s^2\\right]$ is $4t$.", "answer": "$$\\boxed{4t}$$", "id": "3050343"}, {"introduction": "The abstract definitions of martingale theory are best understood through concrete examples, especially those that test the boundaries of the definitions. This exercise strips away complexity by using a simple, finite probability space to construct an adapted and integrable process. Your task is to perform explicit calculations of conditional expectation and demonstrate precisely why this process fails to be a submartingale, thereby reinforcing the importance of each condition in the definition [@problem_id:3050362].", "problem": "Consider a finite probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ with $\\Omega=\\{\\omega_1,\\omega_2,\\omega_3,\\omega_4\\}$ and $\\mathbb{P}(\\{\\omega_i\\})=\\frac{1}{4}$ for each $i\\in\\{1,2,3,4\\}$. Let the filtration $(\\mathcal{F}_t)_{t=0,1,2}$ be given by $\\mathcal{F}_0=\\{\\varnothing,\\Omega\\}$, $\\mathcal{F}_1=\\sigma(\\{\\omega_1,\\omega_2\\},\\{\\omega_3,\\omega_4\\})$, and $\\mathcal{F}_2=\\mathcal{F}=\\mathcal{P}(\\Omega)$. Define an adapted process $(X_t)_{t=0,1,2}$ by\n- $X_0=0$,\n- $X_1(\\omega)=1$ for $\\omega\\in\\{\\omega_1,\\omega_2\\}$ and $X_1(\\omega)=-1$ for $\\omega\\in\\{\\omega_3,\\omega_4\\}$,\n- $X_2(\\omega_1)=-4$, $X_2(\\omega_2)=0$, $X_2(\\omega_3)=0$, and $X_2(\\omega_4)=0$.\n\nUsing only the core definitions of adaptedness, integrability, conditional expectation, and submartingale, do the following:\n1. Verify that $(X_t)_{t=0,1,2}$ is adapted and integrable.\n2. Compute $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]$ explicitly on the atoms $\\{\\omega_1,\\omega_2\\}$ and $\\{\\omega_3,\\omega_4\\}$, and conclude that the submartingale condition $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]\\ge X_1$ fails on a non-null event, so $(X_t)_{t=0,1,2}$ is not a submartingale.\n3. Let $x_{-}=\\max\\{-x,0\\}$ denote the negative part of a real number $x$. Compute the exact value of the scalar\n$$\n\\mathbb{E}\\big[\\big(\\mathbb{E}[X_2\\mid \\mathcal{F}_1]-X_1\\big)_{-}\\big].\n$$\n\nGive your final answer as an exact value (no rounding).", "solution": "First, we verify that the process $(X_t)_{t=0,1,2}$ is adapted to the filtration $(\\mathcal{F}_t)_{t=0,1,2}$ and is integrable.\n\nA stochastic process $(X_t)$ is adapted to a filtration $(\\mathcal{F}_t)$ if for each time $t$, the random variable $X_t$ is $\\mathcal{F}_t$-measurable.\n-   For $t=0$: The random variable $X_0$ is defined as $X_0(\\omega)=0$ for all $\\omega \\in \\Omega$. Since $X_0$ is a constant function on $\\Omega$, it is measurable with respect to any sigma-algebra on $\\Omega$, including the trivial sigma-algebra $\\mathcal{F}_0=\\{\\varnothing,\\Omega\\}$. Thus, $X_0$ is $\\mathcal{F}_0$-measurable.\n-   For $t=1$: The sigma-algebra is $\\mathcal{F}_1=\\sigma(\\{\\omega_1,\\omega_2\\},\\{\\omega_3,\\omega_4\\})$. Let $A_1 = \\{\\omega_1,\\omega_2\\}$ and $A_2 = \\{\\omega_3,\\omega_4\\}$. Then $\\mathcal{F}_1 = \\{\\varnothing, A_1, A_2, \\Omega\\}$. A random variable is $\\mathcal{F}_1$-measurable if and only if it is constant on the atoms $A_1$ and $A_2$. The random variable $X_1$ is defined as $X_1(\\omega)=1$ for $\\omega \\in A_1$ and $X_1(\\omega)=-1$ for $\\omega \\in A_2$. Since $X_1$ is constant on $A_1$ and on $A_2$, it is $\\mathcal{F}_1$-measurable.\n-   For $t=2$: The sigma-algebra is $\\mathcal{F}_2=\\mathcal{P}(\\Omega)$, the power set of $\\Omega$. Any function from $\\Omega$ to $\\mathbb{R}$ is measurable with respect to $\\mathcal{P}(\\Omega)$. The random variable $X_2$ is defined by its values on each singleton $\\{\\omega_i\\}$, so it is a function from $\\Omega$ to $\\mathbb{R}$ and is therefore $\\mathcal{F}_2$-measurable.\nSince $X_t$ is $\\mathcal{F}_t$-measurable for $t=0,1,2$, the process $(X_t)$ is adapted.\n\nA random variable $X$ is integrable if $\\mathbb{E}[|X|]  \\infty$. Since the probability space is finite, any random variable is integrable. We verify this explicitly.\n-   For $t=0$: $\\mathbb{E}[|X_0|] = \\mathbb{E}[|0|] = 0  \\infty$.\n-   For $t=1$: $\\mathbb{E}[|X_1|] = \\sum_{i=1}^4 |X_1(\\omega_i)|\\mathbb{P}(\\{\\omega_i\\}) = |1|\\frac{1}{4} + |1|\\frac{1}{4} + |-1|\\frac{1}{4} + |-1|\\frac{1}{4} = \\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4} = 1  \\infty$.\n-   For $t=2$: $\\mathbb{E}[|X_2|] = \\sum_{i=1}^4 |X_2(\\omega_i)|\\mathbb{P}(\\{\\omega_i\\}) = |-4|\\frac{1}{4} + |0|\\frac{1}{4} + |0|\\frac{1}{4} + |0|\\frac{1}{4} = 4 \\cdot \\frac{1}{4} = 1  \\infty$.\nThe process is thus adapted and integrable.\n\nSecond, we compute $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]$ and check the submartingale property.\nThe conditional expectation $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]$ is an $\\mathcal{F}_1$-measurable random variable. Since $\\mathcal{F}_1$ is generated by the partition $\\{A_1, A_2\\}$, $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]$ must be constant on $A_1$ and $A_2$. The values are given by the formula $\\mathbb{E}[X | A] = \\frac{1}{\\mathbb{P}(A)} \\int_A X d\\mathbb{P}$. We have $\\mathbb{P}(A_1) = \\mathbb{P}(\\{\\omega_1\\}) + \\mathbb{P}(\\{\\omega_2\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$ and similarly $\\mathbb{P}(A_2)=\\frac{1}{2}$.\n\nFor any $\\omega \\in A_1 = \\{\\omega_1,\\omega_2\\}$, the value of the conditional expectation is:\n$$ \\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega) = \\frac{\\mathbb{E}[X_2 \\cdot 1_{A_1}]}{\\mathbb{P}(A_1)} = \\frac{X_2(\\omega_1)\\mathbb{P}(\\{\\omega_1\\}) + X_2(\\omega_2)\\mathbb{P}(\\{\\omega_2\\})}{\\mathbb{P}(A_1)} = \\frac{(-4)(\\frac{1}{4}) + (0)(\\frac{1}{4})}{1/2} = \\frac{-1}{1/2} = -2. $$\nFor any $\\omega \\in A_2 = \\{\\omega_3,\\omega_4\\}$, the value is:\n$$ \\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega) = \\frac{\\mathbb{E}[X_2 \\cdot 1_{A_2}]}{\\mathbb{P}(A_2)} = \\frac{X_2(\\omega_3)\\mathbb{P}(\\{\\omega_3\\}) + X_2(\\omega_4)\\mathbb{P}(\\{\\omega_4\\})}{\\mathbb{P}(A_2)} = \\frac{(0)(\\frac{1}{4}) + (0)(\\frac{1}{4})}{1/2} = 0. $$\nSo, $\\mathbb{E}[X_2\\mid \\mathcal{F}_1]$ is the random variable that takes value $-2$ on $A_1$ and $0$ on $A_2$.\n\nThe submartingale condition is $\\mathbb{E}[X_t \\mid \\mathcal{F}_s] \\ge X_s$ for all $st$. We check this for $s=1, t=2$.\n-   On the event $A_1 = \\{\\omega_1,\\omega_2\\}$, we have $X_1(\\omega) = 1$ and $\\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega)=-2$. The condition requires $-2 \\ge 1$, which is false.\n-   On the event $A_2 = \\{\\omega_3,\\omega_4\\}$, we have $X_1(\\omega) = -1$ and $\\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega)=0$. The condition requires $0 \\ge -1$, which is true.\nSince the submartingale condition fails on the event $A_1$ and $\\mathbb{P}(A_1) = 1/2  0$, the process $(X_t)_{t=0,1,2}$ is not a submartingale.\n\nThird, we compute $\\mathbb{E}\\big[\\big(\\mathbb{E}[X_2\\mid \\mathcal{F}_1]-X_1\\big)_{-}\\big]$.\nLet $Z = \\mathbb{E}[X_2\\mid \\mathcal{F}_1] - X_1$. We compute the values of $Z$ on the atoms of $\\mathcal{F}_1$.\n-   For $\\omega \\in A_1$: $Z(\\omega) = \\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega) - X_1(\\omega) = -2 - 1 = -3$.\n-   For $\\omega \\in A_2$: $Z(\\omega) = \\mathbb{E}[X_2\\mid \\mathcal{F}_1](\\omega) - X_1(\\omega) = 0 - (-1) = 1$.\nNow we compute the negative part $Z_{-} = \\max\\{-Z, 0\\}$.\n-   For $\\omega \\in A_1$: $Z_{-}(\\omega) = \\max\\{-(-3), 0\\} = \\max\\{3, 0\\} = 3$.\n-   For $\\omega \\in A_2$: $Z_{-}(\\omega) = \\max\\{-1, 0\\} = 0$.\nThe random variable $Z_{-}$ takes the value $3$ on $A_1$ and $0$ on $A_2$. We can now compute its expectation:\n$$ \\mathbb{E}[Z_{-}] = \\sum_{\\omega \\in \\Omega} Z_{-}(\\omega)\\mathbb{P}(\\{\\omega\\}) = Z_{-}|_{A_1} \\cdot \\mathbb{P}(A_1) + Z_{-}|_{A_2} \\cdot \\mathbb{P}(A_2). $$\nSubstituting the values we found:\n$$ \\mathbb{E}[Z_{-}] = (3) \\cdot \\mathbb{P}(A_1) + (0) \\cdot \\mathbb{P}(A_2) = 3 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{2} = \\frac{3}{2}. $$\nThe required value is $\\frac{3}{2}$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "3050362"}, {"introduction": "The Martingale Convergence Theorems are among the most celebrated results in probability theory, but their conclusions require careful interpretation. This practice explores the subtle yet crucial distinction between almost sure convergence and convergence in $L^1$ by constructing a non-negative martingale that converges to zero with probability one, yet whose expectation remains constant. By analyzing this process, you will uncover the vital role of uniform integrability and understand why it is the necessary condition to bridge these two fundamental modes of convergence [@problem_id:3050367].", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $(\\xi_{n})_{n\\geq 1}$ be a sequence of independent random variables such that for each $n\\geq 1$,\n$$\n\\mathbb{P}(\\xi_{n}=0)=1-\\frac{1}{n+1}\n\\quad\\text{and}\\quad\n\\mathbb{P}(\\xi_{n}=n+1)=\\frac{1}{n+1}.\n$$\nDefine the filtration $(\\mathcal{F}_{n})_{n\\geq 0}$ by $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ and $\\mathcal{F}_{n}=\\sigma(\\xi_{1},\\dots,\\xi_{n})$ for $n\\geq 1$. Consider the process $(M_{n})_{n\\geq 0}$ defined by $M_{0}=1$ and\n$$\nM_{n}=\\prod_{k=1}^{n}\\xi_{k},\\quad n\\geq 1.\n$$\nStarting from the fundamental definitions of a martingale, filtration, uniform integrability, and almost sure convergence, and using only widely accepted results such as the Borel–Cantelli Lemma and Doob’s Martingale Convergence Theorem, address the following:\n\n1. Establish that $(M_{n})_{n\\geq 0}$ is a nonnegative $(\\mathcal{F}_{n})$-martingale.\n2. Show that $M_{n}$ converges almost surely (a.s.) to a limit $M_{\\infty}$ and identify $M_{\\infty}$ explicitly.\n3. Explain, in light of standard convergence theorems for martingales, why $(M_{n})_{n\\geq 0}$ is not uniformly integrable (uniform integrability (UI)), thereby demonstrating that almost sure convergence of a martingale does not imply convergence in $L^{1}$ without UI.\n4. Compute the value of the limit\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\bigl|M_{n}-M_{\\infty}\\bigr|,\n$$\nand present your final answer as a single exact number. No rounding is required and no physical units are involved.", "solution": "The problem asks us to analyze the process $(M_{n})_{n\\geq 0}$ defined by $M_{0}=1$ and $M_{n}=\\prod_{k=1}^{n}\\xi_{k}$ for $n\\geq 1$, where $(\\xi_{k})_{k\\geq 1}$ are independent random variables with distribution $\\mathbb{P}(\\xi_{k}=0)=1-\\frac{1}{k+1}$ and $\\mathbb{P}(\\xi_{k}=k+1)=\\frac{1}{k+1}$. The filtration is given by $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ and $\\mathcal{F}_{n}=\\sigma(\\xi_{1},\\dots,\\xi_{n})$ for $n\\geq 1$.\n\n1.  **Establish that $(M_{n})_{n\\geq 0}$ is a nonnegative $(\\mathcal{F}_{n})$-martingale.**\n\nA process $(M_{n})_{n\\geq 0}$ is an $(\\mathcal{F}_{n})$-martingale if it satisfies three conditions:\n(i) $M_{n}$ is $\\mathcal{F}_{n}$-measurable for all $n\\geq 0$.\n(ii) $M_{n}$ is integrable, i.e., $\\mathbb{E}[|M_{n}|]\\infty$ for all $n\\geq 0$.\n(iii) $\\mathbb{E}[M_{n+1}|\\mathcal{F}_{n}]=M_{n}$ for all $n\\geq 0$.\n\nFirst, we establish that $(M_{n})_{n\\geq 0}$ is nonnegative. The random variables $\\xi_{k}$ take values in $\\{0, k+1\\}$, which are nonnegative. Since $M_{0}=1$ and $M_{n}$ for $n\\geq 1$ is a product of these nonnegative random variables, $M_{n}\\geq 0$ for all $n\\geq 0$.\n\nNow we verify the three martingale conditions:\n(i) For $n=0$, $M_{0}=1$ is a constant, hence it is $\\mathcal{F}_{0}$-measurable. For $n\\geq 1$, $M_{n}=\\prod_{k=1}^{n}\\xi_{k}$. By definition of the filtration, $\\xi_{k}$ is $\\mathcal{F}_{k}$-measurable for each $k$. Since $\\mathcal{F}_{k}\\subseteq\\mathcal{F}_{n}$ for $k\\leq n$, each $\\xi_{k}$ in the product is $\\mathcal{F}_{n}$-measurable. A product of $\\mathcal{F}_{n}$-measurable functions is $\\mathcal{F}_{n}$-measurable. Thus, $M_{n}$ is $\\mathcal{F}_{n}$-measurable for all $n\\geq 1$.\n\n(ii) Since $M_{n}\\geq 0$, we have $|M_{n}|=M_{n}$. We check for integrability by computing the expectation.\nFor $k\\geq 1$, the expectation of $\\xi_{k}$ is:\n$$\n\\mathbb{E}[\\xi_{k}] = 0 \\cdot \\mathbb{P}(\\xi_{k}=0) + (k+1) \\cdot \\mathbb{P}(\\xi_{k}=k+1) = 0 \\cdot \\left(1-\\frac{1}{k+1}\\right) + (k+1) \\cdot \\frac{1}{k+1} = 1.\n$$\nThe random variables $(\\xi_{k})_{k\\geq 1}$ are independent. For $n\\geq 1$, the expectation of $M_{n}$ is:\n$$\n\\mathbb{E}[M_{n}] = \\mathbb{E}\\left[\\prod_{k=1}^{n}\\xi_{k}\\right] = \\prod_{k=1}^{n}\\mathbb{E}[\\xi_{k}] = \\prod_{k=1}^{n}1 = 1.\n$$\nFor $n=0$, $\\mathbb{E}[M_{0}]=\\mathbb{E}[1]=1$.\nThus, $\\mathbb{E}[|M_{n}|]=\\mathbb{E}[M_{n}]=1\\infty$ for all $n\\geq 0$. The process is integrable.\n\n(iii) We verify the martingale property. For any $n\\geq 0$:\n$$\n\\mathbb{E}[M_{n+1}|\\mathcal{F}_{n}] = \\mathbb{E}[M_{n}\\cdot\\xi_{n+1}|\\mathcal{F}_{n}].\n$$\nSince $M_{n}$ is $\\mathcal{F}_{n}$-measurable, we can take it out of the conditional expectation:\n$$\n\\mathbb{E}[M_{n}\\cdot\\xi_{n+1}|\\mathcal{F}_{n}] = M_{n}\\mathbb{E}[\\xi_{n+1}|\\mathcal{F}_{n}].\n$$\nThe random variable $\\xi_{n+1}$ is independent of the sigma-algebra $\\mathcal{F}_{n}=\\sigma(\\xi_{1},\\dots,\\xi_{n})$. Therefore, the conditional expectation of $\\xi_{n+1}$ is just its unconditional expectation:\n$$\n\\mathbb{E}[\\xi_{n+1}|\\mathcal{F}_{n}] = \\mathbb{E}[\\xi_{n+1}] = 1.\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[M_{n+1}|\\mathcal{F}_{n}] = M_{n} \\cdot 1 = M_{n}.\n$$\nThis holds for all $n\\geq 0$. All three conditions are satisfied, so $(M_{n})_{n\\geq 0}$ is a nonnegative $(\\mathcal{F}_{n})$-martingale.\n\n2.  **Show that $M_{n}$ converges almost surely (a.s.) to a limit $M_{\\infty}$ and identify $M_{\\infty}$ explicitly.**\n\nSince $(M_{n})_{n\\geq 0}$ is a nonnegative martingale, by Doob's Martingale Convergence Theorem, there exists a random variable $M_{\\infty}$ such that $M_{n}\\to M_{\\infty}$ almost surely as $n\\to\\infty$, and $\\mathbb{E}[|M_{\\infty}|] \\leq \\liminf_{n\\to\\infty} \\mathbb{E}[|M_{n}|] = 1$.\n\nTo identify $M_{\\infty}$, we analyze the structure of $M_{n}=\\prod_{k=1}^{n}\\xi_{k}$. If for some $k_{0}(\\omega)$, we have $\\xi_{k_{0}(\\omega)}(\\omega)=0$, then for all $n\\geq k_{0}(\\omega)$, $M_{n}(\\omega)=0$. The limit of the sequence $(M_{n}(\\omega))_{n\\geq 0}$ will therefore be $0$.\nThe limit $M_{\\infty}$ can be non-zero only if $\\xi_{k}\\neq 0$ for all $k\\geq 1$. Let's calculate the probability of this event, which we denote by $A = \\bigcap_{k=1}^{\\infty}\\{\\xi_{k}\\neq 0\\}$. Since the events $\\{\\xi_{k}\\neq 0\\}$ are independent, we have:\n$$\n\\mathbb{P}(A) = \\mathbb{P}\\left(\\bigcap_{k=1}^{\\infty}\\{\\xi_{k}\\neq 0\\}\\right) = \\prod_{k=1}^{\\infty}\\mathbb{P}(\\xi_{k}\\neq 0).\n$$\nFor each $k$, $\\mathbb{P}(\\xi_{k}\\neq 0) = \\mathbb{P}(\\xi_{k}=k+1) = \\frac{1}{k+1}$. The infinite product is:\n$$\n\\mathbb{P}(A) = \\prod_{k=1}^{\\infty}\\frac{1}{k+1} = \\lim_{N\\to\\infty}\\prod_{k=1}^{N}\\frac{1}{k+1} = \\lim_{N\\to\\infty}\\frac{1}{(N+1)!}.\n$$\nThis limit is clearly $0$. Therefore, $\\mathbb{P}(A)=0$. This means that with probability $1$, the event $A^{c}=\\bigcup_{k=1}^{\\infty}\\{\\xi_{k}=0\\}$ occurs. This is the event that at least one $\\xi_{k}$ is zero. As explained above, if this event occurs, the sequence $M_{n}$ eventually becomes zero and stays zero. Thus, $\\lim_{n\\to\\infty}M_{n}=0$ almost surely.\nThe almost sure limit is $M_{\\infty}=0$.\n\n3.  **Explain why $(M_{n})_{n\\geq 0}$ is not uniformly integrable (UI).**\n\nA sequence of random variables $(X_{n})$ is defined as uniformly integrable (UI) if $\\lim_{K\\to\\infty}\\sup_{n}\\mathbb{E}[|X_{n}|\\mathbf{1}_{\\{|X_{n}|K\\}}]=0$.\nA key result in martingale theory states that for a martingale $(M_{n})$ that converges almost surely to a limit $M_{\\infty}$, the following conditions are equivalent:\n(a) The sequence $(M_{n})_{n\\geq 0}$ is uniformly integrable.\n(b) $M_{n}$ converges to $M_{\\infty}$ in $L^{1}$, i.e., $\\lim_{n\\to\\infty}\\mathbb{E}[|M_{n}-M_{\\infty}|]=0$.\n(c) $\\mathbb{E}[M_{\\infty}] = \\lim_{n\\to\\infty}\\mathbb{E}[M_{n}]$.\n\nWe have established that $M_{n}\\to M_{\\infty}=0$ almost surely.\nLet's check condition (c). The expectation of the limit is $\\mathbb{E}[M_{\\infty}] = \\mathbb{E}[0] = 0$.\nThe expectation of $M_n$ for any $n\\geq 0$ is $\\mathbb{E}[M_{n}]=1$, as shown in part 1.\nTherefore, the limit of the expectations is $\\lim_{n\\to\\infty}\\mathbb{E}[M_{n}] = \\lim_{n\\to\\infty}1 = 1$.\nSince $\\lim_{n\\to\\infty}\\mathbb{E}[M_{n}]=1 \\neq 0 = \\mathbb{E}[M_{\\infty}]$, condition (c) is not met.\nThis implies that the martingale $(M_{n})_{n\\geq 0}$ is not uniformly integrable.\nThis example demonstrates that almost sure convergence of a martingale is not sufficient to guarantee $L^{1}$ convergence. The failure of uniform integrability is precisely the reason for the discrepancy between a.s. and $L^{1}$ convergence modes. Intuitively, while $M_{n}$ is zero 'most of the time' for large $n$, there is a very small probability that it takes a very large value, which keeps its expectation at $1$. This \"escape of mass to infinity\" is characteristic of non-UI sequences.\n\n4.  **Compute the value of the limit $\\lim_{n\\to\\infty}\\mathbb{E}\\bigl|M_{n}-M_{\\infty}\\bigr|$.**\n\nFrom the previous parts, we have identified the almost sure limit as $M_{\\infty}=0$. We need to compute:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\bigl|M_{n}-M_{\\infty}\\bigr| = \\lim_{n\\to\\infty}\\mathbb{E}\\bigl|M_{n}-0\\bigr| = \\lim_{n\\to\\infty}\\mathbb{E}\\bigl|M_{n}\\bigr|.\n$$\nAs established in part 1, the process $(M_{n})$ is nonnegative, so $|M_{n}|=M_{n}$. The expression simplifies to:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[M_{n}].\n$$\nAgain, from part 1, we know that for any $n\\geq 0$, $\\mathbb{E}[M_{n}]=1$.\nTherefore, the limit is:\n$$\n\\lim_{n\\to\\infty}1 = 1.\n$$\nThe fact that this limit is not $0$ confirms that $M_n$ does not converge to $M_\\infty$ in $L^1$, as discussed in part 3.", "answer": "$$\\boxed{1}$$", "id": "3050367"}]}