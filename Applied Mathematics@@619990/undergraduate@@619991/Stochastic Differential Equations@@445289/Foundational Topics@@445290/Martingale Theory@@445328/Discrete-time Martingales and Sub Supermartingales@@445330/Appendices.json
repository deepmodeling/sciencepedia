{"hands_on_practices": [{"introduction": "The journey into martingales often begins by understanding their fundamental building blocks: the increments. This practice focuses on these increments, formally known as a martingale difference sequence (MDS), which represent the \"new information\" or \"surprise\" at each step of a process. By verifying the defining properties of an MDS from first principles, you will solidify your grasp of core concepts like conditional expectation and measurability [@problem_id:3049353]. Calculating the variance of the resulting martingale sum beautifully illustrates a key feature: the increments are uncorrelated, a property that greatly simplifies many stochastic calculations.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space supporting a sequence $\\left(\\xi_{k}\\right)_{k \\geq 1}$ of independent and identically distributed (i.i.d.) Rademacher random variables, that is, $\\mathbb{P}\\!\\left(\\xi_{k}=1\\right)=\\mathbb{P}\\!\\left(\\xi_{k}=-1\\right)=\\frac{1}{2}$ for every $k \\in \\mathbb{N}$. Fix a constant $r$ with $0r1$, and define the filtration $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$ by $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ and $\\mathcal{F}_{k}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k}\\right)$ for $k \\geq 1$. Consider the sequence $\\left(D_{k}\\right)_{k \\geq 1}$ defined by $D_{k}=r^{k-1}\\xi_{k}$, and let the partial sums be $S_{n}=\\sum_{k=1}^{n}D_{k}$.\n\nUsing only fundamental definitions and properties of conditional expectation, martingale difference sequences, independence, and variance, do the following:\n\n- Show that $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence (MDS) with respect to $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n- Verify that the differences are bounded almost surely by a finite constant that does not depend on $k$.\n- Compute the variance $\\mathrm{Var}\\!\\left(S_{n}\\right)$ as a closed-form analytical expression in terms of $n$ and $r$.\n\nYour final answer must be a single closed-form expression for $\\mathrm{Var}\\!\\left(S_{n}\\right)$, with no units. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and complete. All provided definitions and constants are standard in the theory of stochastic processes. The tasks are specific, mathematically formalizable, and do not contain any contradictions or ambiguities. The conditions on the constant $r$ are consistent with the required calculations. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe problem asks for three things:\n1.  To show that the sequence $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence (MDS) with respect to the filtration $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n2.  To verify that a.s. $|D_k| \\le M$ for a constant $M$ independent of $k$.\n3.  To compute the variance $\\mathrm{Var}\\!\\left(S_{n}\\right)$.\n\nWe will address each part in sequence.\n\nA sequence of random variables $\\left(D_k\\right)_{k \\geq 1}$ is a martingale difference sequence with respect to a filtration $\\left(\\mathcal{F}_k\\right)_{k \\geq 0}$ if it satisfies three conditions for all $k \\ge 1$:\n(i) $D_k$ is $\\mathcal{F}_k$-measurable.\n(ii) $D_k$ is integrable, i.e., $\\mathbb{E}\\!\\left[|D_k|\\right]  \\infty$.\n(iii) The conditional expectation of $D_k$ given the past is zero, i.e., $\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = 0$.\n\nLet's verify these for $D_{k}=r^{k-1}\\xi_{k}$.\n\n(i) **Measurability**: The filtration is defined as $\\mathcal{F}_{k}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k}\\right)$. The random variable $\\xi_k$ is, by definition, measurable with respect to the sigma-algebra it helps generate, $\\mathcal{F}_k$. Since $D_k$ is a constant multiple of $\\xi_k$ (namely, $D_k = f(\\xi_k)$ where $f(x)=r^{k-1}x$), and $f$ is a Borel-measurable function, $D_k$ is also $\\mathcal{F}_k$-measurable.\n\n(ii) **Integrability**: We compute the expectation of the absolute value of $D_k$:\n$$\n\\mathbb{E}\\!\\left[|D_k|\\right] = \\mathbb{E}\\!\\left[|r^{k-1}\\xi_{k}|\\right]\n$$\nSince $0  r  1$ and $k \\geq 1$, the term $r^{k-1}$ is a positive constant. The random variable $\\xi_k$ takes values in $\\{-1, 1\\}$, so $|\\xi_k|=1$ almost surely.\n$$\n\\mathbb{E}\\!\\left[|D_k|\\right] = \\mathbb{E}\\!\\left[r^{k-1}|\\xi_k|\\right] = r^{k-1}\\mathbb{E}\\!\\left[|\\xi_k|\\right] = r^{k-1}\\mathbb{E}\\!\\left[1\\right] = r^{k-1}\n$$\nSince $r^{k-1}$ is a finite real number for any $k \\geq 1$, the condition $\\mathbb{E}\\!\\left[|D_k|\\right]  \\infty$ is satisfied.\n\n(iii) **MDS Condition**: We must show that $\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = 0$.\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = \\mathbb{E}\\!\\left[r^{k-1}\\xi_k|\\mathcal{F}_{k-1}\\right]\n$$\nThe term $r^{k-1}$ is a deterministic constant, so it can be factored out of the conditional expectation:\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = r^{k-1}\\mathbb{E}\\!\\left[\\xi_k|\\mathcal{F}_{k-1}\\right]\n$$\nThe sequence $\\left(\\xi_i\\right)_{i \\geq 1}$ is independent. This means that $\\xi_k$ is independent of the set of random variables $\\{\\xi_1, \\dots, \\xi_{k-1}\\}$. Consequently, $\\xi_k$ is independent of the sigma-algebra generated by them, $\\mathcal{F}_{k-1}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k-1}\\right)$. A fundamental property of conditional expectation is that if a random variable $X$ is independent of a sigma-algebra $\\mathcal{G}$, then $\\mathbb{E}\\!\\left[X|\\mathcal{G}\\right]=\\mathbb{E}\\!\\left[X\\right]$. Applying this property, we have:\n$$\n\\mathbb{E}\\!\\left[\\xi_k|\\mathcal{F}_{k-1}\\right] = \\mathbb{E}\\!\\left[\\xi_k\\right]\n$$\nThe expectation of a Rademacher random variable $\\xi_k$ is:\n$$\n\\mathbb{E}\\!\\left[\\xi_k\\right] = (-1) \\cdot \\mathbb{P}\\!\\left(\\xi_k=-1\\right) + (1) \\cdot \\mathbb{P}\\!\\left(\\xi_k=1\\right) = (-1)\\cdot\\frac{1}{2} + (1)\\cdot\\frac{1}{2} = 0\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = r^{k-1} \\cdot 0 = 0\n$$\nAll three conditions are satisfied. Thus, $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence with respect to $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n\nNext, we verify that the differences are bounded almost surely. We consider the absolute value of $D_k$:\n$$\n|D_k| = |r^{k-1}\\xi_k| = |r^{k-1}||\\xi_k|\n$$\nSince $0r1$, we have $|r^{k-1}| = r^{k-1}$. For $k \\geq 1$, we have $k-1 \\geq 0$, which implies $0  r^{k-1} \\leq r^0 = 1$. The variable $\\xi_k$ takes values in $\\{-1, 1\\}$, so $|\\xi_k| = 1$ almost surely.\nTherefore, for all $k \\geq 1$:\n$$\n|D_k| = r^{k-1} \\cdot 1 \\leq 1\n$$\nThis shows that the sequence is uniformly bounded almost surely by the constant $M=1$, which does not depend on $k$.\n\nFinally, we compute the variance of the partial sum $S_{n}=\\sum_{k=1}^{n}D_{k}$. The variance of a sum of random variables is the sum of their covariances:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\mathrm{Var}\\!\\left(\\sum_{k=1}^{n}D_{k}\\right) = \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\mathrm{Cov}\\!\\left(D_i, D_j\\right)\n$$\nAn important property of a martingale difference sequence is that its elements are uncorrelated. To show this, let's compute the covariance for $i \\neq j$. Assume without loss of generality that $i  j$.\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i D_j\\right] - \\mathbb{E}\\!\\left[D_i\\right]\\mathbb{E}\\!\\left[D_j\\right]\n$$\nFirst, let's find the unconditional expectation of $D_k$:\n$$\n\\mathbb{E}\\!\\left[D_k\\right] = \\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right]\\right] = \\mathbb{E}\\!\\left[0\\right] = 0\n$$\nwhere we used the law of total expectation and the MDS property. Thus, $\\mathbb{E}\\!\\left[D_i\\right]=\\mathbb{E}\\!\\left[D_j\\right]=0$, and the covariance simplifies to:\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i D_j\\right]\n$$\nUsing the tower property of conditional expectation and the fact that $D_i$ is $\\mathcal{F}_{j-1}$-measurable for $ij$:\n$$\n\\mathbb{E}\\!\\left[D_i D_j\\right] = \\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[D_i D_j|\\mathcal{F}_{j-1}\\right]\\right] = \\mathbb{E}\\!\\left[D_i \\mathbb{E}\\!\\left[D_j|\\mathcal{F}_{j-1}\\right]\\right]\n$$\nSince $\\left(D_k\\right)$ is an MDS, we have $\\mathbb{E}\\!\\left[D_j|\\mathcal{F}_{j-1}\\right]=0$. Therefore:\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i \\cdot 0\\right] = 0 \\quad \\text{for } i \\neq j\n$$\nThe cross-terms in the variance sum are all zero. The variance of the sum simplifies to the sum of the variances:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\sum_{k=1}^{n} \\mathrm{Var}\\!\\left(D_k\\right)\n$$\nNow, we compute $\\mathrm{Var}\\!\\left(D_k\\right)$:\n$$\n\\mathrm{Var}\\!\\left(D_k\\right) = \\mathbb{E}\\!\\left[D_k^2\\right] - \\left(\\mathbb{E}\\!\\left[D_k\\right]\\right)^2\n$$\nSince $\\mathbb{E}\\!\\left[D_k\\right]=0$, this is simply $\\mathrm{Var}\\!\\left(D_k\\right) = \\mathbb{E}\\!\\left[D_k^2\\right]$.\n$$\n\\mathbb{E}\\!\\left[D_k^2\\right] = \\mathbb{E}\\!\\left[\\left(r^{k-1}\\xi_k\\right)^2\\right] = \\mathbb{E}\\!\\left[\\left(r^{k-1}\\right)^2\\xi_k^2\\right] = r^{2(k-1)}\\mathbb{E}\\!\\left[\\xi_k^2\\right]\n$$\nFor a Rademacher random variable, $\\xi_k^2$ is always $1$ (since $(-1)^2=1$ and $1^2=1$). Thus, $\\mathbb{E}\\!\\left[\\xi_k^2\\right]=1$.\nSo, the variance of an individual term is:\n$$\n\\mathrm{Var}\\!\\left(D_k\\right) = r^{2(k-1)}\n$$\nFinally, we sum these variances to get $\\mathrm{Var}\\!\\left(S_{n}\\right)$:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\sum_{k=1}^{n} r^{2(k-1)}\n$$\nThis is a finite geometric series with first term $a = r^{2(1-1)} = r^0 = 1$, common ratio $q = r^2$, and $n$ terms. The sum is given by the formula $a \\frac{1-q^n}{1-q}$.\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = 1 \\cdot \\frac{1 - (r^2)^n}{1-r^2} = \\frac{1 - r^{2n}}{1-r^2}\n$$\nThis is the closed-form analytical expression for the variance of $S_n$.", "answer": "$$\n\\boxed{\\frac{1-r^{2n}}{1-r^2}}\n$$", "id": "3049353"}, {"introduction": "Once we can identify a martingale, what can we do with it? This practice introduces one of the most powerful tools in the martingale toolkit: the Optional Stopping Theorem (OST). The OST provides a profound link between the initial state of a process and its value at a future, random \"stopping time.\" We explore this through the classic \"Gambler's Ruin\" problem, a cornerstone example of stochastic analysis [@problem_id:3049332]. By constructing two distinct martingales from a simple random walk and applying the OST, you will see how this elegant theory can be used to effortlessly solve for complex quantities like hitting probabilities and expected game durations.", "problem": "Consider a simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on the integers defined by $S_{0} = 0$ and $S_{n} = \\sum_{k=1}^{n} X_{k}$ for $n \\geq 1$, where the increments $\\{X_{k}\\}_{k \\geq 1}$ are independent and identically distributed with $\\mathbb{P}(X_{k} = 1) = \\mathbb{P}(X_{k} = -1) = \\frac{1}{2}$. Let $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ denote the natural filtration of $\\{S_{n}\\}_{n \\geq 0}$. We desire a stopping time $T$ such that the stopped value $S_{T}$ has the two-point distribution $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$ and $\\mathbb{P}(S_{T} = -3) = \\frac{4}{7}$.\n\nConstruct the stopping time $T$ as the first hitting time of the set $\\{-3, 4\\}$, that is,\n$$\nT := \\inf\\{n \\geq 0 : S_{n} \\in \\{-3, 4\\}\\}.\n$$\nStarting from the core definitions of martingales and stopping times, and using only well-tested facts about conditional expectations and the optional stopping theorem under its standard sufficient conditions, do the following:\n\n1. Prove that $T$ is a stopping time with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n2. Verify that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n3. Justify the use of the optional stopping theorem for the bounded stopping times $T \\wedge n$ (where $a \\wedge b$ denotes the minimum of $a$ and $b$), and then, by an appropriate limiting argument, conclude that $\\mathbb{E}[S_{T}] = \\mathbb{E}[S_{0}]$.\n4. Using the conclusion in the previous step, derive the distribution of $S_{T}$ and confirm that $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$ and $\\mathbb{P}(S_{T} = -3) = \\frac{4}{7}$.\n5. Consider the process $\\{M_{n}\\}_{n \\geq 0}$ defined by $M_{n} := S_{n}^{2} - n$. Prove that $\\{M_{n}\\}_{n \\geq 0}$ is a martingale, verify optional stopping for $T \\wedge n$, and use dominated and monotone convergence theorems to obtain a closed-form expression for $\\mathbb{E}[T]$.\n\nCompute the exact value of $\\mathbb{E}[T]$ for the constructed stopping time $T$. No rounding is required. Express your final answer as a single real number.", "solution": "The problem is analyzed and solved in five parts as requested.\n\n### Part 1: Prove that $T$ is a stopping time\nA random variable $T$ taking values in $\\{0, 1, 2, \\dots\\} \\cup \\{\\infty\\}$ is a stopping time with respect to the filtration $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ if the event $\\{T \\leq n\\}$ is in $\\mathcal{F}_{n}$ for every $n \\geq 0$. The filtration is given as the natural filtration of the process $\\{S_{n}\\}$, i.e., $\\mathcal{F}_{n} = \\sigma(S_{0}, S_{1}, \\dots, S_{n})$.\n\nThe stopping time $T$ is defined as $T := \\inf\\{k \\geq 0 : S_{k} \\in \\{-3, 4\\}\\}$.\nThe event $\\{T \\leq n\\}$ can be written as the union of events that the random walk has hit the set $\\{-3, 4\\}$ at or before time $n$:\n$$\n\\{T \\leq n\\} = \\bigcup_{k=0}^{n} \\{S_{k} \\in \\{-3, 4\\}\\}.\n$$\nFor any $k \\in \\{0, 1, \\dots, n\\}$, the event $\\{S_{k} \\in \\{-3, 4\\}\\}$ is the union of two events, $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$.\nSince $S_{k}$ is by definition an $\\mathcal{F}_{k}$-measurable random variable, the events $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$ are both in $\\mathcal{F}_{k}$.\nThe filtration is non-decreasing, so $\\mathcal{F}_{k} \\subseteq \\mathcal{F}_{n}$ for any $k \\leq n$.\nTherefore, $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$ are both in $\\mathcal{F}_{n}$ for any $k \\leq n$.\nThe finite union of sets in a $\\sigma$-algebra is also in that $\\sigma$-algebra. Thus, for each $k \\in \\{0, \\dots, n\\}$, the event $\\{S_{k} \\in \\{-3, 4\\}\\}$ is in $\\mathcal{F}_{n}$.\nConsequently, the event $\\{T \\leq n\\} = \\bigcup_{k=0}^{n} \\{S_{k} \\in \\{-3, 4\\}\\}$, being a finite union of $\\mathcal{F}_{n}$-measurable sets, is itself $\\mathcal{F}_{n}$-measurable. This holds for all $n \\geq 0$.\nHence, $T$ is a stopping time with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n\n### Part 2: Verify that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale\nTo show that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$, we must verify three conditions:\n1.  $S_{n}$ is $\\mathcal{F}_{n}$-measurable for each $n \\geq 0$. This is true by definition of the natural filtration $\\mathcal{F}_{n}$.\n2.  $\\mathbb{E}[|S_{n}|]  \\infty$ for each $n \\geq 0$. Since $S_{n} = \\sum_{k=1}^{n} X_{k}$, we have $|S_{n}| = |\\sum_{k=1}^{n} X_{k}| \\leq \\sum_{k=1}^{n} |X_{k}|$. As $|X_{k}|=1$ for all $k$, we get $|S_{n}| \\leq n$. Thus, $\\mathbb{E}[|S_{n}|] \\leq \\mathbb{E}[n] = n  \\infty$.\n3.  $\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1}$ for all $n \\geq 1$.\nWe compute the conditional expectation:\n$$\n\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n-1} + X_{n} | \\mathcal{F}_{n-1}].\n$$\nBy the linearity of conditional expectation, this becomes:\n$$\n\\mathbb{E}[S_{n-1} | \\mathcal{F}_{n-1}] + \\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}].\n$$\nSince $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable, $\\mathbb{E}[S_{n-1} | \\mathcal{F}_{n-1}] = S_{n-1}$.\nThe increment $X_{n}$ is independent of the past increments $\\{X_{1}, \\dots, X_{n-1}\\}$, and thus independent of the $\\sigma$-algebra they generate, $\\mathcal{F}_{n-1} = \\sigma(S_0, ..., S_{n-1})$. Therefore, $\\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n}]$.\nThe expectation of $X_{n}$ is:\n$$\n\\mathbb{E}[X_{n}] = (1) \\cdot \\mathbb{P}(X_{n}=1) + (-1) \\cdot \\mathbb{P}(X_{n}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0.\n$$\nSubstituting these results back, we obtain:\n$$\n\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1} + 0 = S_{n-1}.\n$$\nAll three conditions are satisfied, so $\\{S_{n}\\}_{n \\geq 0}$ is a martingale.\n\n### Part 3: Justify the use of the optional stopping theorem\nFor each $n \\geq 0$, define the stopped time $T_{n} = T \\wedge n = \\min(T, n)$. Since $T$ is a stopping time and $n$ is a constant, $T_{n}$ is also a stopping time. Furthermore, $T_{n}$ is bounded by $n$.\nThe optional stopping theorem for bounded stopping times states that if $\\{S_{k}\\}$ is a martingale and $\\tau$ is a bounded stopping time, then $\\mathbb{E}[S_{\\tau}] = \\mathbb{E}[S_{0}]$.\nApplying this to the bounded stopping time $T_{n}$, we have:\n$$\n\\mathbb{E}[S_{T \\wedge n}] = \\mathbb{E}[S_{0}].\n$$\nSince $S_{0}=0$, we have $\\mathbb{E}[S_{T \\wedge n}] = 0$ for all $n \\geq 0$.\n\nTo extend this to the unbounded stopping time $T$, we must justify taking the limit as $n \\to \\infty$. One of the sufficient conditions for the optional stopping theorem is that the stopped process is uniformly bounded. Let's check this condition for $S_{T \\wedge n}$.\nBy construction of $T$, for any $k  T$, the position of the walk $S_{k}$ is strictly between $-3$ and $4$, i.e., $-3  S_{k}  4$.\nIf the stopping occurs at or before time $n$ (i.e., $T \\leq n$), then $S_{T \\wedge n} = S_{T}$, which is either $-3$ or $4$.\nIf the stopping occurs after time $n$ (i.e., $T  n$), then $S_{T \\wedge n} = S_{n}$, and since $n  T$, we have $-3  S_{n}  4$.\nIn all cases, $|S_{T \\wedge n}| \\leq 4$ for all $n \\geq 0$ and all sample paths. The stopped process $\\{S_{T \\wedge n}\\}_{n \\geq 0}$ is uniformly bounded by the constant $4$.\nA one-dimensional simple symmetric random walk is recurrent, which implies that it will eventually visit any integer state. Thus, it will eventually hit the set $\\{-3, 4\\}$, which means $\\mathbb{P}(T  \\infty)=1$.\nAs $n \\to \\infty$, $T \\wedge n \\to T$ almost surely. Since $S$ is a function defined on the integers, this implies $S_{T \\wedge n} \\to S_{T}$ almost surely.\nThe sequence of random variables $\\{S_{T \\wedge n}\\}_{n \\geq 0}$ converges almost surely to $S_{T}$ and is uniformly bounded in absolute value by $4$. The constant random variable $Y=4$ is integrable. Therefore, by the Dominated Convergence Theorem, we can interchange the limit and the expectation:\n$$\n\\mathbb{E}[S_{T}] = \\mathbb{E}[\\lim_{n \\to \\infty} S_{T \\wedge n}] = \\lim_{n \\to \\infty} \\mathbb{E}[S_{T \\wedge n}].\n$$\nSince $\\mathbb{E}[S_{T \\wedge n}] = 0$ for all $n$, we conclude:\n$$\n\\mathbb{E}[S_{T}] = \\lim_{n \\to \\infty} 0 = 0.\n$$\nThus, $\\mathbb{E}[S_{T}] = \\mathbb{E}[S_{0}]$.\n\n### Part 4: Derive the distribution of $S_{T}$\nThe random variable $S_{T}$ takes values in the set $\\{-3, 4\\}$. Let $p = \\mathbb{P}(S_{T} = 4)$. Then, because $\\mathbb{P}(T  \\infty) = 1$, we have $\\mathbb{P}(S_{T} = -3) = 1-p$.\nThe expectation of $S_{T}$ can be calculated using its distribution:\n$$\n\\mathbb{E}[S_{T}] = (4) \\cdot \\mathbb{P}(S_{T} = 4) + (-3) \\cdot \\mathbb{P}(S_{T} = -3) = 4p - 3(1-p).\n$$\nFrom the previous step, we know $\\mathbb{E}[S_{T}] = 0$. So we set up the equation:\n$$\n4p - 3(1-p) = 0\n$$\n$$\n4p - 3 + 3p = 0\n$$\n$$\n7p = 3\n$$\n$$\np = \\frac{3}{7}.\n$$\nTherefore, $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$, and $\\mathbb{P}(S_{T} = -3) = 1 - \\frac{3}{7} = \\frac{4}{7}$. This confirms the desired distribution.\n\n### Part 5: Compute $\\mathbb{E}[T]$\nFirst, we prove that $M_{n} = S_{n}^{2} - n$ is a martingale.\n1.  $M_{n}$ is $\\mathcal{F}_{n}$-measurable because $S_{n}$ is $\\mathcal{F}_{n}$-measurable and $n$ is a deterministic constant.\n2.  $\\mathbb{E}[|M_{n}|] = \\mathbb{E}[|S_{n}^{2} - n|] \\leq \\mathbb{E}[S_{n}^{2}] + n$. The variance of $X_{k}$ is $\\mathbb{E}[X_{k}^{2}] - (\\mathbb{E}[X_{k}])^{2} = 1 - 0^{2} = 1$. Since the $X_{k}$ are independent, $\\text{Var}(S_{n}) = \\sum_{k=1}^{n}\\text{Var}(X_{k}) = n$. Also $\\mathbb{E}[S_{n}]=0$. So, $\\mathbb{E}[S_{n}^{2}] = \\text{Var}(S_{n}) + (\\mathbb{E}[S_{n}])^{2} = n + 0 = n$. Thus, $\\mathbb{E}[|M_{n}|] \\leq n + n = 2n  \\infty$.\n3.  We check the martingale property: $\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = M_{n-1}$.\n$$\n\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n}^{2} - n | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] - n.\n$$\nSubstitute $S_{n} = S_{n-1} + X_{n}$:\n$$\n\\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[(S_{n-1} + X_{n})^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n-1}^{2} + 2S_{n-1}X_{n} + X_{n}^{2} | \\mathcal{F}_{n-1}].\n$$\nUsing linearity and properties of conditional expectation:\n$$\n\\mathbb{E}[S_{n-1}^{2} | \\mathcal{F}_{n-1}] + 2\\mathbb{E}[S_{n-1}X_{n} | \\mathcal{F}_{n-1}] + \\mathbb{E}[X_{n}^{2} | \\mathcal{F}_{n-1}].\n$$\n- Since $S_{n-1}^{2}$ is $\\mathcal{F}_{n-1}$-measurable, $\\mathbb{E}[S_{n-1}^{2} | \\mathcal{F}_{n-1}] = S_{n-1}^{2}$.\n- Taking out what is known, $\\mathbb{E}[S_{n-1}X_{n} | \\mathcal{F}_{n-1}] = S_{n-1}\\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}] = S_{n-1}\\mathbb{E}[X_{n}] = S_{n-1} \\cdot 0 = 0$.\n- Due to independence, $\\mathbb{E}[X_{n}^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n}^{2}] = (1)^{2}\\frac{1}{2} + (-1)^{2}\\frac{1}{2} = 1$.\nCombining these results, we get $\\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] = S_{n-1}^{2} + 1$.\nSo, $\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = (S_{n-1}^{2} + 1) - n = S_{n-1}^{2} - (n-1) = M_{n-1}$.\nThus, $\\{M_{n}\\}$ is a martingale.\n\nNow, we apply the optional stopping theorem to $M_{n}$ and the bounded stopping time $T \\wedge n$.\n$$\n\\mathbb{E}[M_{T \\wedge n}] = \\mathbb{E}[M_{0}] = S_{0}^{2} - 0 = 0.\n$$\nThis means $\\mathbb{E}[S_{T \\wedge n}^{2} - (T \\wedge n)] = 0$, or $\\mathbb{E}[S_{T \\wedge n}^{2}] = \\mathbb{E}[T \\wedge n]$.\nWe need to take the limit as $n \\to \\infty$.\nFor the right-hand side, the sequence of random variables $\\{T \\wedge n\\}_{n \\geq 0}$ is non-negative and non-decreasing, converging point-wise to $T$. By the Monotone Convergence Theorem:\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[T \\wedge n] = \\mathbb{E}[\\lim_{n \\to \\infty} T \\wedge n] = \\mathbb{E}[T].\n$$\nFor the left-hand side, we have $S_{T \\wedge n}^{2} \\to S_{T}^{2}$ almost surely. As shown before, $|S_{T \\wedge n}| \\leq 4$, so $S_{T \\wedge n}^{2} \\leq 16$. Since the constant random variable $Y=16$ is integrable, we can use the Dominated Convergence Theorem:\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[S_{T \\wedge n}^{2}] = \\mathbb{E}[\\lim_{n \\to \\infty} S_{T \\wedge n}^{2}] = \\mathbb{E}[S_{T}^{2}].\n$$\nEquating the limits, we find $\\mathbb{E}[T] = \\mathbb{E}[S_{T}^{2}]$.\nWe can now compute $\\mathbb{E}[S_{T}^{2}]$ using the distribution of $S_{T}$ found in Part 4. The random variable $S_{T}^{2}$ takes the value $4^{2}=16$ with probability $\\frac{3}{7}$ and $(-3)^{2}=9$ with probability $\\frac{4}{7}$.\n$$\n\\mathbb{E}[S_{T}^{2}] = (16) \\cdot \\mathbb{P}(S_{T} = 4) + (9) \\cdot \\mathbb{P}(S_{T} = -3) = 16 \\cdot \\frac{3}{7} + 9 \\cdot \\frac{4}{7}.\n$$\n$$\n\\mathbb{E}[S_{T}^{2}] = \\frac{48}{7} + \\frac{36}{7} = \\frac{48+36}{7} = \\frac{84}{7} = 12.\n$$\nTherefore, $\\mathbb{E}[T] = 12$.", "answer": "$$\n\\boxed{12}\n$$", "id": "3049332"}, {"introduction": "A true mastery of any powerful theorem requires knowing not only when it applies, but also when it breaks down. The Optional Stopping Theorem rests on crucial assumptions, and overlooking them can lead to paradoxical results. This exercise presents a classic and essential counterexample that explores the theorem's boundaries [@problem_id:3049397]. By analyzing a scenario where the expected stopping time is infinite, you will explicitly demonstrate that $\\mathbb{E}[M_{\\tau}] \\neq \\mathbb{E}[M_{0}]$. This hands-on experience with the failure of the OST is critical for developing a nuanced and robust understanding of martingale theory.", "problem": "Consider a simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on the integers defined by $S_{0} = -1$ and $S_{n} = S_{n-1} + X_{n}$ for $n \\geq 1$, where $\\{X_{n}\\}_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = \\frac{1}{2}$. Let $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ be the natural filtration generated by $\\{X_{k}\\}_{k \\leq n}$, and define the process $M_{n} := S_{n}$. Define the stopping time $\\tau := \\inf\\{n \\geq 0 : S_{n} = 1\\}$ with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n\nStarting from first principles and well-tested facts in discrete-time martingale theory:\n- Use the definition of a martingale to justify that $\\{M_{n}\\}_{n \\geq 0}$ is an integrable martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n- For integers $a \\geq 1$ and $b \\geq 1$, introduce the bounded two-sided hitting time $\\tau_{a,b} := \\inf\\{n \\geq 0 : S_{n} \\in \\{a,-b\\}\\}$ and, using only valid martingale arguments for bounded stopping times, determine the probability $\\mathbb{P}(S_{\\tau_{a,b}} = a)$ as a function of $a$, $b$, and the initial state $S_{0} = -1$. Then compute $\\mathbb{E}[S_{\\tau_{a,b}}^{2}]$ and deduce $\\mathbb{E}[\\tau_{a,b}]$ via the martingale $S_{n}^{2} - n$.\n- Letting $a = 1$ and sending $b \\to \\infty$, justify carefully why $\\tau_{1,b} \\uparrow \\tau$ almost surely and conclude $\\mathbb{E}[\\tau] = \\infty$ by an appropriate convergence theorem.\n- Compute $\\mathbb{E}[M_{\\tau}]$ and $\\mathbb{E}[M_{0}]$ explicitly.\n\nFinally, provide the single numerical value of the quantity $\\mathbb{E}[M_{\\tau}] - \\mathbb{E}[M_{0}]$. No rounding is required.", "solution": "The problem statement is a well-posed and standard problem in the theory of discrete-time martingales. It is scientifically grounded, self-contained, and objective. We proceed with the solution.\n\nThe process is a simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on $\\mathbb{Z}$ with $S_{0} = -1$ and $S_{n} = S_{n-1} + X_{n}$ for $n \\geq 1$, where $\\{X_{n}\\}_{n \\geq 1}$ are i.i.d. random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = \\frac{1}{2}$. The process $M_{n}$ is defined as $M_{n} := S_{n}$. The filtration $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ is the natural filtration of $\\{X_{k}\\}$.\n\nFirst, we justify that $\\{M_{n}\\}_{n \\geq 0}$ is a martingale. We must check three conditions:\n1.  **Integrability**: For any $n \\geq 0$, $S_{n} = S_{0} + \\sum_{k=1}^{n} X_{k}$. Taking the absolute value, $|S_{n}| \\leq |S_{0}| + \\sum_{k=1}^{n} |X_{k}|$. Since $|S_{0}|=|-1|=1$ and $|X_{k}|=1$ for all $k$, we have $|S_{n}| \\leq 1 + n$. The expectation is therefore bounded: $\\mathbb{E}[|S_{n}|] \\leq 1+n  \\infty$. Thus, $M_{n}$ is integrable for all $n \\geq 0$.\n2.  **Adaptedness**: $S_{n} = -1 + \\sum_{k=1}^{n} X_{k}$. Since each $X_{k}$ is $\\mathcal{F}_{k}$-measurable and $\\mathcal{F}_{k} \\subseteq \\mathcal{F}_{n}$ for $k \\leq n$, each $X_{k}$ for $k \\leq n$ is $\\mathcal{F}_{n}$-measurable. The sum of $\\mathcal{F}_{n}$-measurable random variables is also $\\mathcal{F}_{n}$-measurable. Thus, $S_{n}$ is adapted to the filtration $\\{\\mathcal{F}_{n}\\}$.\n3.  **Martingale Property**: We must show $\\mathbb{E}[M_{n+1} | \\mathcal{F}_{n}] = M_{n}$.\n    $$\n    \\mathbb{E}[M_{n+1} | \\mathcal{F}_{n}] = \\mathbb{E}[S_{n+1} | \\mathcal{F}_{n}] = \\mathbb{E}[S_{n} + X_{n+1} | \\mathcal{F}_{n}]\n    $$\n    By linearity of conditional expectation,\n    $$\n    \\mathbb{E}[S_{n} + X_{n+1} | \\mathcal{F}_{n}] = \\mathbb{E}[S_{n} | \\mathcal{F}_{n}] + \\mathbb{E}[X_{n+1} | \\mathcal{F}_{n}]\n    $$\n    Since $S_{n}$ is $\\mathcal{F}_{n}$-measurable, $\\mathbb{E}[S_{n} | \\mathcal{F}_{n}] = S_{n}$. The random variable $X_{n+1}$ is independent of the filtration $\\mathcal{F}_{n}$ (which is generated by $X_{1}, \\dots, X_{n}$). Therefore, $\\mathbb{E}[X_{n+1} | \\mathcal{F}_{n}] = \\mathbb{E}[X_{n+1}]$.\n    The expectation of $X_{n+1}$ is $\\mathbb{E}[X_{n+1}] = (1)\\mathbb{P}(X_{n+1}=1) + (-1)\\mathbb{P}(X_{n+1}=-1) = (1)(\\frac{1}{2}) + (-1)(\\frac{1}{2}) = 0$.\n    Substituting back, we get:\n    $$\n    \\mathbb{E}[M_{n+1} | \\mathcal{F}_{n}] = S_{n} + 0 = S_{n} = M_{n}\n    $$\n    All three conditions are satisfied, so $\\{M_{n}\\}_{n \\geq 0} = \\{S_{n}\\}_{n \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n\nNext, we consider the bounded two-sided hitting time $\\tau_{a,b} := \\inf\\{n \\geq 0 : S_{n} \\in \\{a, -b\\}\\}$ for integers $a \\geq 1$ and $b \\geq 1$. Since a simple symmetric random walk on $\\mathbb{Z}$ is recurrent, it will eventually reach any integer state, which implies that $\\tau_{a,b}$ is finite almost surely, i.e., $\\mathbb{P}(\\tau_{a,b}  \\infty) = 1$. The stopped process $S_{n \\wedge \\tau_{a,b}}$ is a bounded martingale, as $|S_{n \\wedge \\tau_{a,b}}| \\leq \\max(a, b, |S_{0}|) = \\max(a,b,1)$. By the Optional Stopping Theorem for bounded martingales, $\\mathbb{E}[S_{\\tau_{a,b}}] = \\mathbb{E}[S_{0}]$.\nWith $S_{0} = -1$, we have $\\mathbb{E}[S_{\\tau_{a,b}}] = -1$.\nThe value of $S_{\\tau_{a,b}}$ can only be $a$ or $-b$. Let $p = \\mathbb{P}(S_{\\tau_{a,b}} = a)$. Then $\\mathbb{P}(S_{\\tau_{a,b}} = -b) = 1-p$.\nThe expectation is $\\mathbb{E}[S_{\\tau_{a,b}}] = a \\cdot p + (-b)(1-p) = ap - b + bp$.\nSetting this equal to $-1$:\n$$\np(a+b) - b = -1 \\implies p(a+b) = b-1 \\implies p = \\frac{b-1}{a+b}\n$$\nSo, $\\mathbb{P}(S_{\\tau_{a,b}} = a) = \\frac{b-1}{a+b}$.\n\nNow we compute $\\mathbb{E}[S_{\\tau_{a,b}}^{2}]$.\n$$\n\\mathbb{E}[S_{\\tau_{a,b}}^{2}] = a^{2}\\mathbb{P}(S_{\\tau_{a,b}} = a) + (-b)^{2}\\mathbb{P}(S_{\\tau_{a,b}} = -b) = a^{2}p + b^{2}(1-p)\n$$\n$$\n\\mathbb{E}[S_{\\tau_{a,b}}^{2}] = a^{2}\\left(\\frac{b-1}{a+b}\\right) + b^{2}\\left(1 - \\frac{b-1}{a+b}\\right) = a^{2}\\left(\\frac{b-1}{a+b}\\right) + b^{2}\\left(\\frac{a+1}{a+b}\\right)\n$$\n$$\n\\mathbb{E}[S_{\\tau_{a,b}}^{2}] = \\frac{a^{2}b - a^{2} + ab^{2} + b^{2}}{a+b} = \\frac{ab(a+b) - a^{2} + b^{2}}{a+b} = ab + \\frac{(b-a)(b+a)}{a+b} = ab + b - a\n$$\nTo deduce $\\mathbb{E}[\\tau_{a,b}]$, we define the process $Y_{n} := S_{n}^{2} - n$. Let's show it's a martingale.\n$$\n\\mathbb{E}[Y_{n+1}|\\mathcal{F}_{n}] = \\mathbb{E}[S_{n+1}^{2} - (n+1) | \\mathcal{F}_{n}] = \\mathbb{E}[(S_{n}+X_{n+1})^{2} - n - 1 | \\mathcal{F}_{n}]\n$$\n$$\n= \\mathbb{E}[S_{n}^{2} + 2S_{n}X_{n+1} + X_{n+1}^{2} - n - 1 | \\mathcal{F}_{n}]\n$$\n$$\n= S_{n}^{2} + 2S_{n}\\mathbb{E}[X_{n+1}|\\mathcal{F}_{n}] + \\mathbb{E}[X_{n+1}^{2}|\\mathcal{F}_{n}] - n - 1\n$$\nSince $\\mathbb{E}[X_{n+1}]=0$ and $\\mathbb{E}[X_{n+1}^{2}] = (1)^{2}(\\frac{1}{2}) + (-1)^{2}(\\frac{1}{2}) = 1$:\n$$\n\\mathbb{E}[Y_{n+1}|\\mathcal{F}_{n}] = S_{n}^{2} + 0 + 1 - n - 1 = S_{n}^{2} - n = Y_{n}\n$$\nSo, $\\{Y_{n}\\}$ is a martingale. To apply OST for $\\tau_{a,b}$, which is not a bounded time, we use a limit argument with bounded stopping times. Let $\\tau_{N} = \\tau_{a,b} \\wedge N = \\min(\\tau_{a,b}, N)$. Since $\\tau_{N}$ is bounded, OST applies to $Y_{n}$ and $\\tau_{N}$: $\\mathbb{E}[Y_{\\tau_{N}}] = \\mathbb{E}[Y_{0}]$.\n$\\mathbb{E}[Y_{0}] = S_{0}^{2} - 0 = (-1)^{2} = 1$.\nThus, $\\mathbb{E}[Y_{\\tau_{N}}] = \\mathbb{E}[S_{\\tau_{N}}^{2} - \\tau_{N}] = \\mathbb{E}[S_{\\tau_{N}}^{2}] - \\mathbb{E}[\\tau_{N}] = 1$.\nAs $N \\to \\infty$, $\\tau_{N} \\to \\tau_{a,b}$ a.s. and $S_{\\tau_{N}} \\to S_{\\tau_{a,b}}$ a.s. Since $|S_{\\tau_{N}}| = |S_{\\tau_{a,b} \\wedge N}| \\leq \\max(a,b,1)$, the random variables $S_{\\tau_{N}}^{2}$ are uniformly bounded. By the Bounded Convergence Theorem, $\\lim_{N \\to \\infty} \\mathbb{E}[S_{\\tau_{N}}^{2}] = \\mathbb{E}[S_{\\tau_{a,b}}^{2}]$. The sequence of random variables $\\tau_{N}$ is non-negative and non-decreasing, so by the Monotone Convergence Theorem, $\\lim_{N \\to \\infty} \\mathbb{E}[\\tau_{N}] = \\mathbb{E}[\\tau_{a,b}]$.\nTaking the limit as $N \\to \\infty$ in the equation for $\\tau_N$:\n$$\n\\mathbb{E}[S_{\\tau_{a,b}}^{2}] - \\mathbb{E}[\\tau_{a,b}] = 1\n$$\nSolving for $\\mathbb{E}[\\tau_{a,b}]$:\n$$\n\\mathbb{E}[\\tau_{a,b}] = \\mathbb{E}[S_{\\tau_{a,b}}^{2}] - 1 = (ab+b-a) - 1 = ab - a + b - 1 = (a+1)(b-1)\n$$\nNow, let $a=1$ and send $b \\to \\infty$. The stopping time is $\\tau_{1,b} = \\inf\\{n \\geq 0: S_{n} \\in \\{1, -b\\}\\}$. The stopping time $\\tau$ is $\\tau = \\inf\\{n \\geq 0: S_{n}=1\\}$.\nNote that $\\tau_{1,b} = \\min(\\tau, \\tau_{-b})$, where $\\tau_{-b} = \\inf\\{n \\geq 0: S_{n}=-b\\}$. For any fixed realization of the random walk, the sequence $\\{\\tau_{1,b}\\}_{b \\geq 1}$ is non-decreasing since the set of states to be hit $\\{1, -b-1\\}$ is \"further away\" than $\\{1,-b\\}$. As $b \\to \\infty$, for a path to reach $-b$, it must not have hit $1$. Because a 1D SRW is recurrent, it visits every state with probability $1$. Thus, $\\tau  \\infty$ almost surely. From any finite starting state, the time to reach an infinitely distant state must be infinite. So, $\\tau_{-b} \\to \\infty$ a.s. as $b \\to \\infty$. Therefore, $\\lim_{b\\to\\infty}\\tau_{1,b} = \\lim_{b\\to\\infty} \\min(\\tau, \\tau_{-b}) = \\tau$ a.s.\nBy the Monotone Convergence Theorem, since $\\tau_{1,b}$ is a non-decreasing sequence of non-negative random variables:\n$$\n\\mathbb{E}[\\tau] = \\mathbb{E}[\\lim_{b\\to\\infty}\\tau_{1,b}] = \\lim_{b\\to\\infty}\\mathbb{E}[\\tau_{1,b}]\n$$\nUsing our derived formula for $\\mathbb{E}[\\tau_{a,b}]$ with $a=1$:\n$$\n\\mathbb{E}[\\tau_{1,b}] = (1+1)(b-1) = 2(b-1)\n$$\nTaking the limit:\n$$\n\\mathbb{E}[\\tau] = \\lim_{b\\to\\infty} 2(b-1) = \\infty\n$$\nNext, we compute $\\mathbb{E}[M_{\\tau}]$ and $\\mathbb{E}[M_{0}]$.\n$$\n\\mathbb{E}[M_{0}] = \\mathbb{E}[S_{0}] = \\mathbb{E}[-1] = -1\n$$\nThe stopping time $\\tau$ is the first time the process $S_{n}$ hits the value $1$. By definition, $S_{\\tau} = 1$. Thus, $M_{\\tau}=1$.\n$$\n\\mathbb{E}[M_{\\tau}] = \\mathbb{E}[S_{\\tau}] = \\mathbb{E}[1] = 1\n$$\nNote that $\\mathbb{E}[M_{\\tau}] \\neq \\mathbb{E}[M_{0}]$. This is because the conditions for the Optional Stopping Theorem are not met for the martingale $M_n$ and stopping time $\\tau$, as we have shown that $\\mathbb{E}[\\tau]=\\infty$.\n\nFinally, we compute the requested quantity $\\mathbb{E}[M_{\\tau}] - \\mathbb{E}[M_{0}]$.\n$$\n\\mathbb{E}[M_{\\tau}] - \\mathbb{E}[M_{0}] = 1 - (-1) = 2\n$$", "answer": "$$\\boxed{2}$$", "id": "3049397"}]}