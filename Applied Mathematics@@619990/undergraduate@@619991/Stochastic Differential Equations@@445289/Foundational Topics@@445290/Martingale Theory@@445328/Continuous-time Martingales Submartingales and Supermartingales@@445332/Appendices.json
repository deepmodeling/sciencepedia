{"hands_on_practices": [{"introduction": "Before building complex martingales, we must grasp the foundational properties of the processes that drive them. This practice explores the quadratic variation of Brownian motion, a non-intuitive yet crucial concept that distinguishes stochastic calculus from classical calculus. By deriving the famous result $[B]_t = t$ from first principles, you will gain a concrete understanding of the rule that informally states $(dB_t)^2 = dt$, which is the cornerstone of Itô's formula. [@problem_id:3045851]", "problem": "Let $\\left( B_{t} \\right)_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space satisfying the usual conditions, with $B_{0} = 0$, independent increments, and Gaussian increments with $\\left( B_{t} - B_{s} \\right) \\sim \\mathcal{N}\\left( 0, t - s \\right)$ for $t \\geq s$. The quadratic variation $[B]_{t}$ of a continuous semimartingale is defined pathwise as the limit in probability of the sum of squared increments along partitions, that is,\n$$\n[B]_{t} = \\lim_{\\lvert \\Pi \\rvert \\to 0} \\sum_{k=0}^{n-1} \\left( B_{t_{k+1}} - B_{t_{k}} \\right)^{2},\n$$\nwhere $\\Pi = \\{ 0 = t_{0} < t_{1} < \\cdots < t_{n} = t \\}$ is a partition of the interval $[0,t]$ and $\\lvert \\Pi \\rvert$ is the mesh of the partition. The Itô integral $\\int_{0}^{t} H_{s} \\, \\mathrm{d} B_{s}$ is defined for square-integrable adapted processes $H$, and Itô's isometry states that\n$$\n\\mathbb{E}\\!\\left[ \\left( \\int_{0}^{t} H_{s} \\, \\mathrm{d} B_{s} \\right)^{2} \\right] = \\mathbb{E}\\!\\left[ \\int_{0}^{t} H_{s}^{2} \\, \\mathrm{d} s \\right].\n$$\nCompute $[B]_{t}$ in closed form by:\n- Direct approximation using deterministic partitions to show convergence in mean square of the sum of squared increments.\n- Using Itô's isometry for the stochastic integral representation of $B_{t}$ and the characterization of the predictable quadratic variation.\n\nExplain why the two approaches are consistent with one another for continuous martingales. Provide your final answer as a single closed-form expression for $[B]_{t}$. No rounding is required.", "solution": "The problem asks for the computation of the quadratic variation of a standard Brownian motion, denoted as $[B]_t$, using two distinct methods, and to explain their consistency. We assume $\\left( B_t \\right)_{t \\ge 0}$ is a standard one-dimensional Brownian motion with $B_0 = 0$.\n\nFirst, we compute $[B]_t$ by direct approximation of the defining sum, showing convergence in mean square ($L^2$).\nLet $\\Pi_n = \\{0 = t_0 < t_1 < \\dots < t_n = t\\}$ be a sequence of partitions of the interval $[0,t]$ such that the mesh $|\\Pi_n| = \\max_{k} (t_{k+1}-t_k)$ goes to $0$ as $n \\to \\infty$. The quadratic variation is defined as the limit in probability of the sum of squared increments:\n$$\n[B]_t = \\lim_{|\\Pi_n| \\to 0} \\sum_{k=0}^{n-1} (B_{t_{k+1}} - B_{t_k})^2\n$$\nLet $S_n = \\sum_{k=0}^{n-1} (B_{t_{k+1}} - B_{t_k})^2$. Let $\\Delta B_k = B_{t_{k+1}} - B_{t_k}$ and $\\Delta t_k = t_{k+1} - t_k$. From the properties of Brownian motion, the increments $\\Delta B_k$ are independent and normally distributed, $\\Delta B_k \\sim \\mathcal{N}(0, \\Delta t_k)$.\nThe first two moments of $\\Delta B_k$ are $\\mathbb{E}[\\Delta B_k] = 0$ and $\\mathbb{E}[(\\Delta B_k)^2] = \\text{Var}(\\Delta B_k) = \\Delta t_k$.\nFor a normally distributed random variable $X \\sim \\mathcal{N}(0, \\sigma^2)$, the fourth central moment is $\\mathbb{E}[X^4] = 3\\sigma^4$. Therefore, $\\mathbb{E}[(\\Delta B_k)^4] = 3(\\Delta t_k)^2$.\n\nWe first compute the expectation of $S_n$:\n$$\n\\mathbb{E}[S_n] = \\mathbb{E}\\left[\\sum_{k=0}^{n-1} (\\Delta B_k)^2\\right] = \\sum_{k=0}^{n-1} \\mathbb{E}[(\\Delta B_k)^2] = \\sum_{k=0}^{n-1} \\Delta t_k = t\n$$\nThis shows that $S_n$ is an unbiased estimator for $t$. To show convergence in mean square, we must show that the variance of $S_n$ converges to $0$.\n$$\n\\text{Var}(S_n) = \\mathbb{E}[(S_n - \\mathbb{E}[S_n])^2] = \\mathbb{E}[(S_n - t)^2]\n$$\nSince the increments $\\Delta B_k$ are independent, the random variables $(\\Delta B_k)^2$ are also independent. Thus, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(S_n) = \\text{Var}\\left(\\sum_{k=0}^{n-1} (\\Delta B_k)^2\\right) = \\sum_{k=0}^{n-1} \\text{Var}((\\Delta B_k)^2)\n$$\nThe variance of each term is:\n$$\n\\text{Var}((\\Delta B_k)^2) = \\mathbb{E}[((\\Delta B_k)^2)^2] - (\\mathbb{E}[(\\Delta B_k)^2])^2 = \\mathbb{E}[(\\Delta B_k)^4] - (\\Delta t_k)^2\n$$\nSubstituting the fourth moment, we get:\n$$\n\\text{Var}((\\Delta B_k)^2) = 3(\\Delta t_k)^2 - (\\Delta t_k)^2 = 2(\\Delta t_k)^2\n$$\nSumming these variances gives the variance of $S_n$:\n$$\n\\text{Var}(S_n) = \\sum_{k=0}^{n-1} 2(\\Delta t_k)^2 = 2 \\sum_{k=0}^{n-1} (\\Delta t_k)^2\n$$\nWe can bound this sum by using the mesh of the partition, $|\\Pi_n| = \\max_k(\\Delta t_k)$:\n$$\n\\sum_{k=0}^{n-1} (\\Delta t_k)^2 \\le \\sum_{k=0}^{n-1} |\\Pi_n| \\Delta t_k = |\\Pi_n| \\sum_{k=0}^{n-1} \\Delta t_k = |\\Pi_n| t\n$$\nThus, we have the bound:\n$$\n0 \\le \\text{Var}(S_n) \\le 2t |\\Pi_n|\n$$\nAs $|\\Pi_n| \\to 0$, we have $\\text{Var}(S_n) \\to 0$. This proves that $S_n$ converges to its mean $t$ in mean square ($L^2$ convergence). Convergence in $L^2$ implies convergence in probability, so we conclude that:\n$$\n[B]_t = t\n$$\n\nSecond, we use the martingale characterization of the quadratic variation. For a continuous local martingale $M_t$, its predictable quadratic variation, denoted $\\langle M \\rangle_t$, is the unique continuous, predictable process of finite variation with $\\langle M \\rangle_0 = 0$ such that $M_t^2 - \\langle M \\rangle_t$ is a local martingale. For continuous martingales, the quadratic variation $[M]_t$ and the predictable quadratic variation $\\langle M \\rangle_t$ coincide, i.e., $[M]_t = \\langle M \\rangle_t$.\nItô's isometry, in its general form for a continuous local martingale $M_t$ and a suitable integrand process $H_s$, states that $\\mathbb{E}\\left[ \\left(\\int_0^t H_s dM_s\\right)^2 \\right] = \\mathbb{E}\\left[ \\int_0^t H_s^2 d\\langle M \\rangle_s \\right]$.\nThe problem provides the specific form for Brownian motion: $\\mathbb{E}\\left[ \\left( \\int_0^t H_s dB_s \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_0^t H_s^2 ds \\right]$.\nComparing the general form with the given form for $B_t$, we must have:\n$$\n\\mathbb{E}\\left[ \\int_0^t H_s^2 d\\langle B \\rangle_s \\right] = \\mathbb{E}\\left[ \\int_0^t H_s^2 ds \\right]\n$$\nThis can be rewritten as $\\mathbb{E}\\left[ \\int_0^t H_s^2 d(\\langle B \\rangle_s - s) \\right] = 0$. This identity must hold for a large class of predictable processes $H_s$. Let $C_t = \\langle B \\rangle_t - t$. The process $C_t$ is the difference of two continuous, predictable, finite-variation processes, and is therefore itself a continuous, predictable, finite-variation process with $C_0 = \\langle B \\rangle_0 - 0 = 0$.\nThe identity $\\mathbb{E}\\left[ \\int_0^t H_s^2 dC_s \\right]=0$ implies (by a version of the fundamental lemma of calculus of variations for stochastic integrals) that $C_t$ must be a martingale. To see this more directly, let $H_s = \\mathbf{1}_{(a,b]}(s)$ for $0 \\le a < b \\le t$. Then $\\mathbb{E}[C_b - C_a]=0$. More generally, for any $s < t$, $\\mathbb{E}[C_t - C_s | \\mathcal{F}_s] = 0$, which is the martingale property for $C_t$.\nSo, $C_t = \\langle B \\rangle_t - t$ is a continuous local martingale.\nA fundamental result of stochastic calculus states that any process that is both a continuous local martingale and has finite variation must be constant. Since $C_0=0$, we must have $C_t = 0$ for all $t \\ge 0$.\nThis implies $\\langle B \\rangle_t - t = 0$, or $\\langle B \\rangle_t = t$.\nSince for the continuous martingale $B_t$ we have $[B]_t = \\langle B \\rangle_t$, we conclude:\n$$\n[B]_t = t\n$$\n\nThe two approaches are consistent. The first method computes $[B]_t$ directly from its pathwise definition as a limit of sums of squared increments. This is a constructive approach based on the sample paths of the process. The result is the deterministic process $f(t) = t$. The second method uses the defining property of the (predictable) quadratic variation as the unique process $\\langle B \\rangle_t$ that makes $B_t^2 - \\langle B \\rangle_t$ a martingale. The provided Itô isometry is a consequence of this definition and directly encodes the fact that for Brownian motion, the compensator $\\langle B \\rangle_t$ is simply $t$. The argument that a finite-variation martingale must be constant is then used to show uniqueness and conclude $\\langle B \\rangle_t = t$. The consistency lies in the fact that Itô's formula, which underpins the martingale approach (e.g., in showing $B_t^2 - t$ is a martingale), is itself derived using Taylor series expansions where the second-order term depends on the quadratic variation. Specifically, the informal relation $(dB_t)^2 = dt$ used to derive Itô's formula is precisely the result obtained from the first, pathwise-limit approach. Thus, the second, more abstract method is built upon the foundational result established by the first. The uniqueness of the quadratic variation guarantees that both paths lead to the same destination.", "answer": "$$\n\\boxed{t}\n$$", "id": "3045851"}, {"introduction": "With an understanding of quadratic variation, we can now construct and analyze one of the most important processes in stochastic analysis: the exponential martingale. This exercise guides you through proving that the process $M_t = \\exp(B_t - \\frac{1}{2}t)$ is a martingale, a result with profound implications in financial mathematics for changing probability measures (Girsanov's theorem). Furthermore, by analyzing the expectation of $M_t^2$, you will see a direct application of Jensen's inequality and understand how a convex function of a martingale gives rise to a submartingale. [@problem_id:3045867]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion adapted to its natural filtration on a probability space that satisfies the usual conditions. Define the process $\\{M_{t}\\}_{t \\geq 0}$ by $M_{t} := \\exp\\!\\left(B_{t} - \\frac{1}{2} t\\right)$. Using the fundamental definition of a continuous-time martingale and Itô's formula from stochastic differential equations (SDE), justify that $\\{M_{t}\\}_{t \\geq 0}$ is a martingale. Then, for the non-affine function $f(x) := x^{2}$, compute the exact value of $\\mathbb{E}[f(M_{t})]$ as a function of $t$, using only well-established properties of Brownian motion (such as the normal distribution and the moment generating function (MGF) of a Gaussian random variable). Your final answer must be a single closed-form analytic expression for $\\mathbb{E}[M_{t}^{2}]$ in terms of $t$. No rounding is required, and no physical units apply.", "solution": "The problem requires a two-part analysis of the process $\\{M_{t}\\}_{t \\geq 0}$ defined by $M_{t} := \\exp(B_{t} - \\frac{1}{2} t)$, where $\\{B_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. First, we must justify that $\\{M_{t}\\}$ is a martingale. Second, we must compute the expectation $\\mathbb{E}[M_{t}^{2}]$.\n\nPart 1: Justification of the Martingale Property\n\nAs requested, we will use both Itô's formula and the fundamental definition of a martingale. Itô's formula will show that $\\{M_{t}\\}$ has a stochastic differential equation (SDE) with a zero drift term, which establishes it as a local martingale. Then, we will verify the conditions of the fundamental definition to confirm it is a true martingale.\n\nLet $M_t = f(t, B_t)$ where $f(t, x) = \\exp(x - \\frac{1}{2}t)$. The partial derivatives of $f(t, x)$ are:\n$$\n\\frac{\\partial f}{\\partial t} = -\\frac{1}{2} \\exp\\left(x - \\frac{1}{2}t\\right) = -\\frac{1}{2} f(t, x)\n$$\n$$\n\\frac{\\partial f}{\\partial x} = \\exp\\left(x - \\frac{1}{2}t\\right) = f(t, x)\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\exp\\left(x - \\frac{1}{2}t\\right) = f(t, x)\n$$\nApplying Itô's formula to $M_t = f(t, B_t)$:\n$$\ndM_{t} = \\frac{\\partial f}{\\partial t}(t, B_t) dt + \\frac{\\partial f}{\\partial x}(t, B_t) dB_{t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2}(t, B_t) (dB_{t})^{2}\n$$\nSubstituting the partial derivatives and using the quadratic variation of Brownian motion, $(dB_{t})^{2} = dt$:\n$$\ndM_{t} = \\left(-\\frac{1}{2} M_{t}\\right) dt + (M_{t}) dB_{t} + \\frac{1}{2} (M_{t}) dt\n$$\n$$\ndM_{t} = \\left(-\\frac{1}{2} M_{t} + \\frac{1}{2} M_{t}\\right) dt + M_{t} dB_{t}\n$$\n$$\ndM_{t} = M_{t} dB_{t}\n$$\nThis is the SDE for the process $\\{M_{t}\\}$. In its integral form, since $M_0 = \\exp(B_0 - 0) = \\exp(0) = 1$, we have:\n$$\nM_{t} = 1 + \\int_{0}^{t} M_{s} dB_{s}\n$$\nA stochastic integral with respect to a Brownian motion is always a local martingale. Thus, $\\{M_{t}\\}$ is a local martingale.\n\nTo show it is a true martingale, we must verify the three conditions from its fundamental definition with respect to the natural filtration of the Brownian motion, $\\{\\mathcal{F}_{t}\\}_{t \\geq 0}$.\n\n1.  **Adaptedness:** The process $M_{t} = \\exp(B_{t} - \\frac{1}{2}t)$ is a measurable function of $B_{t}$ and $t$. Since $\\{B_{t}\\}$ is adapted to the filtration $\\{\\mathcal{F}_{t}\\}$, $M_{t}$ is also $\\mathcal{F}_{t}$-adapted for all $t \\geq 0$.\n\n2.  **Integrability:** We must show that $\\mathbb{E}[|M_{t}|] < \\infty$ for all $t \\geq 0$. Since $M_{t} = \\exp(B_{t} - \\frac{1}{2}t)$ is always positive, $|M_{t}| = M_{t}$. We compute its expectation:\n    $$\n    \\mathbb{E}[M_{t}] = \\mathbb{E}\\left[\\exp\\left(B_{t} - \\frac{1}{2}t\\right)\\right] = \\exp\\left(-\\frac{1}{2}t\\right) \\mathbb{E}[\\exp(B_{t})]\n    $$\n    The random variable $B_{t}$ follows a normal distribution, $B_{t} \\sim \\mathcal{N}(0, t)$. The moment generating function (MGF) of a general normal random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_{X}(k) = \\mathbb{E}[\\exp(kX)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$. Here, $X=B_t$, $\\mu=0$, $\\sigma^2=t$, and we evaluate the MGF at $k=1$.\n    $$\n    \\mathbb{E}[\\exp(B_{t})] = \\exp\\left(1 \\cdot 0 + \\frac{1}{2} \\cdot 1^2 \\cdot t\\right) = \\exp\\left(\\frac{1}{2}t\\right)\n    $$\n    Substituting this back into the expectation for $M_t$:\n    $$\n    \\mathbb{E}[M_{t}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp\\left(\\frac{1}{2}t\\right) = \\exp(0) = 1\n    $$\n    Since $\\mathbb{E}[|M_{t}|] = 1$, the integrability condition is satisfied.\n\n3.  **Conditional Expectation Property:** For any $s < t$, we must show that $\\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = M_{s}$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[\\exp\\left(B_{t} - \\frac{1}{2}t\\right) \\Big| \\mathcal{F}_{s}\\right]\n    $$\n    We decompose the Brownian motion increment: $B_{t} = B_{s} + (B_{t} - B_{s})$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[\\exp\\left(B_{s} + (B_{t} - B_{s}) - \\frac{1}{2}t\\right) \\Big| \\mathcal{F}_{s}\\right] = \\exp\\left(-\\frac{1}{2}t\\right) \\mathbb{E}\\left[\\exp(B_{s})\\exp(B_{t} - B_{s}) \\Big| \\mathcal{F}_{s}\\right]\n    $$\n    Since $B_{s}$ is $\\mathcal{F}_{s}$-measurable, $\\exp(B_{s})$ can be treated as a constant with respect to the conditional expectation. By the independence of increments of Brownian motion, the increment $(B_{t} - B_{s})$ is independent of the past filtration $\\mathcal{F}_{s}$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp(B_{s}) \\mathbb{E}[\\exp(B_{t} - B_{s})]\n    $$\n    The increment $B_{t} - B_{s}$ is normally distributed with mean $0$ and variance $t-s$, i.e., $B_{t} - B_{s} \\sim \\mathcal{N}(0, t-s)$. Using the MGF formula with $k=1$, $\\mu=0$, and $\\sigma^2=t-s$:\n    $$\n    \\mathbb{E}[\\exp(B_{t} - B_{s})] = \\exp\\left(1 \\cdot 0 + \\frac{1}{2} \\cdot 1^2 \\cdot (t-s)\\right) = \\exp\\left(\\frac{t-s}{2}\\right)\n    $$\n    Substituting this result back:\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp(B_{s}) \\exp\\left(\\frac{t-s}{2}\\right) = \\exp\\left(B_{s} - \\frac{t}{2} + \\frac{t-s}{2}\\right) = \\exp\\left(B_{s} - \\frac{s}{2}\\right) = M_{s}\n    $$\n    Since all three conditions are satisfied, $\\{M_{t}\\}_{t \\geq 0}$ is a true martingale. This process is a classic example of an exponential martingale, also known as a Doléans-Dade exponential.\n\nPart 2: Computation of $\\mathbb{E}[M_{t}^{2}]$\n\nWe are asked to compute $\\mathbb{E}[f(M_{t})]$ for $f(x)=x^2$, which is $\\mathbb{E}[M_t^2]$. We first express $M_t^2$ in terms of $B_t$ and $t$.\n$$\nM_{t}^{2} = \\left(\\exp\\left(B_{t} - \\frac{1}{2}t\\right)\\right)^{2} = \\exp\\left(2\\left(B_{t} - \\frac{1}{2}t\\right)\\right) = \\exp(2B_{t} - t)\n$$\nNow, we compute the expectation:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\mathbb{E}[\\exp(2B_{t} - t)]\n$$\nSince $\\exp(-t)$ is a deterministic quantity, it can be factored out of the expectation:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\exp(-t) \\mathbb{E}[\\exp(2B_{t})]\n$$\nTo evaluate $\\mathbb{E}[\\exp(2B_{t})]$, we again use the MGF of the normal distribution $B_{t} \\sim \\mathcal{N}(0, t)$. This time, we evaluate the MGF at $k=2$.\n$$\n\\mathbb{E}[\\exp(2B_{t})] = \\exp\\left(2 \\cdot 0 + \\frac{1}{2} \\cdot 2^2 \\cdot t\\right) = \\exp\\left(\\frac{1}{2} \\cdot 4 \\cdot t\\right) = \\exp(2t)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[M_{t}^{2}]$:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\exp(-t) \\cdot \\exp(2t) = \\exp(-t+2t) = \\exp(t)\n$$\nThus, the exact value of $\\mathbb{E}[M_t^2]$ is $\\exp(t)$.\nIt is noteworthy that since $\\mathbb{E}[M_t^2] = \\exp(t)$, the variance of $M_t$ is $\\text{Var}(M_t) = \\mathbb{E}[M_t^2] - (\\mathbb{E}[M_t])^2 = \\exp(t) - 1^2 = \\exp(t) - 1$.\nThe process $\\{M_t^2\\}_{t\\ge0}$ is an example of a submartingale, as $\\mathbb{E}[M_t^2]$ is an increasing function of $t$.", "answer": "$$\\boxed{\\exp(t)}$$", "id": "3045867"}, {"introduction": "Martingale theory includes powerful theorems about long-term behavior, but their conditions are not mere technicalities. This practice delves into the nuances of the submartingale convergence theorem by constructing a simple counterexample, $X_t = \\max\\{B_t, 0\\}$. By proving this process is a submartingale and then demonstrating why it fails to converge, you will identify the exact integrability condition that is violated, providing a deeper appreciation for the precise requirements of stochastic convergence theorems. [@problem_id:3045869]", "problem": "Consider a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$ supporting a standard Brownian motion $B = (B_t)_{t \\ge 0}$ with $B_0 = 0$ and its natural filtration. Define the process $X = (X_t)_{t \\ge 0}$ by $X_t = B_t^{+} = \\max\\{B_t, 0\\}$. \n\nUsing only core definitions and first principles, carry out the following steps:\n\n1. Prove that $(X_t)_{t \\ge 0}$ is a submartingale with respect to $(\\mathcal{F}_t)_{t \\ge 0}$.\n2. Explain why $(X_t)_{t \\ge 0}$ does not converge almost surely (almost surely (a.s.) means with probability one) as $t \\to \\infty$, and identify the specific integrability condition from the submartingale convergence theorem that fails for $(X_t)_{t \\ge 0}$. Justify your conclusions from foundational properties of Brownian motion and integrability.\n3. For any fixed $T > 0$ and $b > 0$, define the upcrossing count $U_T(0,b)$ of the interval $[0,b]$ by the process $(X_t)_{t \\in [0,T]}$ as the number of completed passages from below $0$ to above $b$ by the continuous path $t \\mapsto X_t$ up to time $T$. Derive, from first principles and standard inequalities for submartingales, a closed-form analytic expression for the upper bound on $\\mathbb{E}[U_T(0,b)]$ stated purely in terms of $T$ and $b$.\n\nExpress your final answer as a single simplified analytic expression. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded within the theory of stochastic processes. We will address the three parts in sequence.\n\nThe process $X = (X_t)_{t \\ge 0}$ is defined on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$ by $X_t = B_t^{+} = \\max\\{B_t, 0\\}$, where $B = (B_t)_{t \\ge 0}$ is a standard one-dimensional Brownian motion with $B_0 = 0$ and $(\\mathcal{F}_t)_{t \\ge 0}$ is its natural filtration.\n\n### 1. Proof that $(X_t)_{t \\ge 0}$ is a Submartingale\n\nA process $(X_t)_{t \\ge 0}$ is a submartingale with respect to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$ if it satisfies three conditions:\n(i) $X_t$ is $\\mathcal{F}_t$-measurable for all $t \\ge 0$.\n(ii) $\\mathbb{E}[|X_t|] < \\infty$ for all $t \\ge 0$.\n(iii) For any $s < t$, $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ a.s. (almost surely).\n\nWe verify each condition for $X_t = \\max\\{B_t, 0\\}$.\n\n(i) **Measurability:** By definition, the process $B$ is adapted to its natural filtration $(\\mathcal{F}_t)_{t \\ge 0}$, meaning $B_t$ is an $\\mathcal{F}_t$-measurable random variable for every $t \\ge 0$. The function $f(x) = \\max\\{x, 0\\}$ is a continuous function from $\\mathbb{R}$ to $\\mathbb{R}$, and is therefore a Borel-measurable function. Since $X_t = f(B_t)$, and $B_t$ is $\\mathcal{F}_t$-measurable, $X_t$ is also $\\mathcal{F}_t$-measurable.\n\n(ii) **Integrability:** We must show that $\\mathbb{E}[|X_t|] < \\infty$ for all $t \\ge 0$. Since $X_t = \\max\\{B_t, 0\\}$ is always non-negative, $|X_t| = X_t$. We need to compute $\\mathbb{E}[X_t]$.\nThe random variable $B_t$ follows a normal distribution with mean $0$ and variance $t$, i.e., $B_t \\sim N(0, t)$. Its probability density function is $p(x,t) = \\frac{1}{\\sqrt{2\\pi t}} \\exp(-\\frac{x^2}{2t})$.\nThe expectation is given by:\n$$ \\mathbb{E}[X_t] = \\mathbb{E}[\\max\\{B_t, 0\\}] = \\int_{-\\infty}^{\\infty} \\max\\{x, 0\\} p(x,t) dx = \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right) dx $$\nWe perform a substitution. Let $u = \\frac{x^2}{2t}$, so $du = \\frac{2x}{2t}dx = \\frac{x}{t}dx$, which implies $x dx = t du$. The limits of integration remain from $0$ to $\\infty$.\n$$ \\mathbb{E}[X_t] = \\frac{1}{\\sqrt{2\\pi t}} \\int_0^{\\infty} \\exp(-u) (t du) = \\frac{t}{\\sqrt{2\\pi t}} \\int_0^{\\infty} \\exp(-u) du $$\nThe integral $\\int_0^{\\infty} \\exp(-u) du = [-\\exp(-u)]_0^{\\infty} = 0 - (-1) = 1$.\nThus,\n$$ \\mathbb{E}[X_t] = \\frac{t}{\\sqrt{2\\pi t}} = \\frac{\\sqrt{t}}{\\sqrt{2\\pi}} = \\sqrt{\\frac{t}{2\\pi}} $$\nFor any finite $t \\ge 0$, this value is finite. Thus, the integrability condition is satisfied.\n\n(iii) **Submartingale Property:** We need to show $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ for $s < t$.\nWe use Jensen's inequality for conditional expectations, which states that for any convex function $f$ and integrable random variable $Y$, $\\mathbb{E}[f(Y) | \\mathcal{G}] \\ge f(\\mathbb{E}[Y | \\mathcal{G}])$ for a sub-$\\sigma$-algebra $\\mathcal{G}$.\nThe function $f(x) = \\max\\{x, 0\\}$ is convex. Applying Jensen's inequality with $Y = B_t$ and $\\mathcal{G} = \\mathcal{F}_s$:\n$$ \\mathbb{E}[X_t | \\mathcal{F}_s] = \\mathbb{E}[\\max\\{B_t, 0\\} | \\mathcal{F}_s] \\ge \\max\\{\\mathbb{E}[B_t | \\mathcal{F}_s], 0\\} $$\nSince standard Brownian motion $(B_t)_{t \\ge 0}$ is a martingale with respect to its natural filtration, we have $\\mathbb{E}[B_t | \\mathcal{F}_s] = B_s$ for $s < t$.\nSubstituting this into the inequality gives:\n$$ \\mathbb{E}[X_t | \\mathcal{F}_s] \\ge \\max\\{B_s, 0\\} $$\nBy definition, $\\max\\{B_s, 0\\} = X_s$. Therefore, we have shown that $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ a.s.\n\nSince all three conditions are met, $(X_t)_{t \\ge 0}$ is a submartingale with respect to $(\\mathcal{F}_t)_{t \\ge 0}$.\n\n### 2. Non-Convergence and the Submartingale Convergence Theorem\n\nThe submartingale convergence theorem states that if $(Y_t)_{t \\ge 0}$ is a submartingale such that $\\sup_{t \\ge 0} \\mathbb{E}[Y_t] < \\infty$, then $Y_t$ converges a.s. to an integrable random variable $Y_\\infty$ as $t \\to \\infty$.\n\nFirst, we explain why $(X_t)_{t \\ge 0}$ does not converge almost surely as $t \\to \\infty$. The behavior of $X_t = \\max\\{B_t, 0\\}$ is dictated by the long-term behavior of the Brownian motion $B_t$. The Law of the Iterated Logarithm for Brownian motion states that:\n$$ \\limsup_{t \\to \\infty} \\frac{B_t}{\\sqrt{2t \\ln(\\ln t)}} = 1 \\quad \\text{a.s.} \\quad \\text{and} \\quad \\liminf_{t \\to \\infty} \\frac{B_t}{\\sqrt{2t \\ln(\\ln t)}} = -1 \\quad \\text{a.s.} $$\nFrom the $\\limsup$ result, it follows that $B_t$ will take arbitrarily large positive values as $t \\to \\infty$. This implies that $\\limsup_{t \\to \\infty} B_t = \\infty$ a.s. Consequently, for our process $X_t$:\n$$ \\limsup_{t \\to \\infty} X_t = \\limsup_{t \\to \\infty} \\max\\{B_t, 0\\} = \\max\\{\\limsup_{t \\to \\infty} B_t, 0\\} = \\infty \\quad \\text{a.s.} $$\nFrom the $\\liminf$ result, it follows that $B_t$ will take arbitrarily large negative values. More fundamentally, a one-dimensional Brownian motion is recurrent, meaning it returns to any neighborhood of $0$ infinitely often with probability $1$. In particular, $B_t$ will be less than or equal to $0$ for arbitrarily large values of $t$. For any such $t$, $X_t = \\max\\{B_t, 0\\} = 0$. This implies that:\n$$ \\liminf_{t \\to \\infty} X_t = 0 \\quad \\text{a.s.} $$\nSince $\\liminf_{t \\to \\infty} X_t \\neq \\limsup_{t \\to \\infty} X_t$, the limit $\\lim_{t \\to \\infty} X_t$ does not exist almost surely. The process does not converge.\n\nNext, we identify the failing condition of the submartingale convergence theorem. The theorem requires the submartingale to be bounded in $L^1$, i.e., $\\sup_{t \\ge 0} \\mathbb{E}[|X_t|] < \\infty$. As $|X_t| = X_t$, this condition is $\\sup_{t \\ge 0} \\mathbb{E}[X_t] < \\infty$.\nFrom our calculation in Part 1, we found that $\\mathbb{E}[X_t] = \\sqrt{\\frac{t}{2\\pi}}$.\nWe evaluate the supremum over $t \\ge 0$:\n$$ \\sup_{t \\ge 0} \\mathbb{E}[X_t] = \\sup_{t \\ge 0} \\sqrt{\\frac{t}{2\\pi}} = \\lim_{t \\to \\infty} \\sqrt{\\frac{t}{2\\pi}} = \\infty $$\nThe condition $\\sup_{t \\ge 0} \\mathbb{E}[X_t] < \\infty$ is not satisfied. This is the specific integrability condition from the submartingale convergence theorem that fails for the process $(X_t)_{t \\ge 0}$.\n\n### 3. Upper Bound on Expected Upcrossings\n\nWe need to derive an upper bound for $\\mathbb{E}[U_T(0,b)]$, the expected number of upcrossings of the interval $[0,b]$ by the process $(X_t)_{t \\in [0,T]}$, where $T>0$ and $b>0$. The problem specifies that an upcrossing is a passage from \"below $0$\" to \"above $b$\". For the process $X_t = \\max\\{B_t, 0\\}$, which is always non-negative, \"below $0$\" must be interpreted as being at the level $0$.\n\nThe derivation relies on Doob's Upcrossing Inequality, a foundational result for submartingales. For a continuous-time submartingale $(Y_t)_{t \\in [0,T]}$ and an interval $[a,b]$ with $a < b$, the expected number of upcrossings $U_T(a,b)$ is bounded by:\n$$ \\mathbb{E}[U_T(a,b)] \\le \\frac{\\mathbb{E}[(Y_T - a)^+] - \\mathbb{E}[(Y_0 - a)^+]}{b - a} $$\nwhere $y^+ = \\max\\{y,0\\}$.\n\nWe apply this standard inequality to our process $Y_t = X_t = \\max\\{B_t,0\\}$, which we have proven to be a submartingale. The interval is $[a,b] = [0,b]$.\nThe inequality becomes:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\mathbb{E}[(X_T - 0)^+] - \\mathbb{E}[(X_0 - 0)^+]}{b - 0} $$\nLet's evaluate the terms in the numerator:\n1.  The term $\\mathbb{E}[(X_T - 0)^+]$ simplifies to $\\mathbb{E}[X_T^+]$. Since $X_T \\ge 0$, we have $X_T^+ = X_T$. So this term is just $\\mathbb{E}[X_T]$. From our calculation in Part 1, with $t=T$, we have:\n    $$ \\mathbb{E}[X_T] = \\sqrt{\\frac{T}{2\\pi}} $$\n2.  The term $\\mathbb{E}[(X_0 - 0)^+]$ simplifies to $\\mathbb{E}[X_0]$. We are given $B_0 = 0$, so $X_0 = \\max\\{B_0, 0\\} = \\max\\{0, 0\\} = 0$. Therefore:\n    $$ \\mathbb{E}[X_0] = 0 $$\n\nSubstituting these results into the inequality:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\sqrt{\\frac{T}{2\\pi}} - 0}{b} = \\frac{\\sqrt{\\frac{T}{2\\pi}}}{b} $$\nSimplifying the expression gives the closed-form analytic upper bound:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\sqrt{T}}{b\\sqrt{2\\pi}} $$\nThis expression is the required upper bound on the expected number of upcrossings, stated purely in terms of $T$ and $b$. The final answer is this upper bound.", "answer": "$$ \\boxed{\\frac{\\sqrt{T}}{b\\sqrt{2\\pi}}} $$", "id": "3045869"}]}