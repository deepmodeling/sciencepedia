{"hands_on_practices": [{"introduction": "This first exercise serves as a bridge from the intuitive idea of conditioning on an event to the more abstract measure-theoretic definition. By working through the case of a finite partition, you will build a concrete understanding of how conditional expectation acts as a projection, averaging the random variable over known pieces of information. This fundamental example [@problem_id:3045153] provides the essential building blocks for understanding conditional expectation in more complex settings.", "problem": "Consider a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ and a finite measurable partition $\\{A_{i}\\}_{i=1}^{n}$ of $\\Omega$ with $\\mathbb{P}(A_{i})0$ for all $i \\in \\{1,\\dots,n\\}$. Let $\\mathcal{G}=\\sigma(\\{A_{i}\\}_{i=1}^{n})$ be the $\\sigma$-algebra generated by this partition, and let $X$ be an integrable random variable, $X\\in L^{1}(\\Omega,\\mathcal{F},\\mathbb{P})$. In the context of stochastic differential equations, conditional expectation with respect to a sub-$\\sigma$-algebra serves both as an information update and, when $X\\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$, as an $L^{2}$ (square-integrable functions) projection onto the closed subspace of $\\mathcal{G}$-measurable random variables.\n\nStarting only from the fundamental definition of conditional expectation with respect to a sub-$\\sigma$-algebra and basic properties of Lebesgue integration, derive the explicit $\\mathcal{G}$-measurable form of $\\mathbb{E}[X\\mid \\mathcal{G}]$ as a random variable that is constant on each atom $A_{i}$. Then, assuming additionally that $X\\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$, justify why this conditional expectation is the unique minimizer of the mean square error (MSE), in the sense that it minimizes $\\mathbb{E}\\!\\left[(X-Y)^{2}\\right]$ over all $\\mathcal{G}$-measurable $Y$.\n\nProvide your final answer as a single closed-form analytic expression for $\\mathbb{E}[X\\mid \\mathcal{G}]$ written entirely in terms of $\\{A_{i}\\}_{i=1}^{n}$, their probabilities, and the integrals of $X$ over those atoms. No numerical approximation is required.", "solution": "The problem statement is a standard and well-posed exercise in measure-theoretic probability theory. It is scientifically grounded, self-contained, and objective. All provided information is necessary and sufficient for deriving the requested results. The problem is therefore valid.\n\nWe will proceed in two parts. First, we will derive the explicit form for the conditional expectation $\\mathbb{E}[X\\mid \\mathcal{G}]$. Second, we will prove that this conditional expectation is the unique minimizer of the mean square error, which corresponds to the $L^2$ projection.\n\n**Part 1: Derivation of the Explicit Form of $\\mathbb{E}[X\\mid \\mathcal{G}]$**\n\nLet $Z = \\mathbb{E}[X\\mid \\mathcal{G}]$. By the fundamental definition of conditional expectation, $Z$ is the unique (up to almost sure equality) random variable that satisfies two conditions:\n1.  $Z$ is $\\mathcal{G}$-measurable.\n2.  For every set $A \\in \\mathcal{G}$, the following integral identity holds: $\\int_A Z \\,d\\mathbb{P} = \\int_A X \\,d\\mathbb{P}$.\n\nThe sub-$\\sigma$-algebra $\\mathcal{G}$ is generated by the finite partition $\\{A_i\\}_{i=1}^n$ of $\\Omega$. A random variable $Y$ is $\\mathcal{G}$-measurable if and only if it is constant on each atom $A_i$ of the partition. This means that any $\\mathcal{G}$-measurable random variable can be expressed as a simple function of the form:\n$$Y(\\omega) = \\sum_{i=1}^n c_i \\mathbb{1}_{A_i}(\\omega)$$\nfor some real constants $\\{c_i\\}_{i=1}^n$, where $\\mathbb{1}_{A_i}$ is the indicator function of the set $A_i$.\n\nSince $Z = \\mathbb{E}[X\\mid \\mathcal{G}]$ must be $\\mathcal{G}$-measurable (Condition 1), it must have this form. Let us write:\n$$Z = \\sum_{i=1}^n c_i \\mathbb{1}_{A_i}$$\nOur task is to determine the values of the constants $c_i$ by using Condition 2.\n\nAny set $A \\in \\mathcal{G}$ is a finite disjoint union of some of the atoms $\\{A_i\\}$. Therefore, it is sufficient to enforce Condition 2 for each atom $A_j \\in \\{A_i\\}_{i=1}^n$, as linearity of the integral will then ensure it holds for any $A \\in \\mathcal{G}$.\n\nFor any specific atom $A_j$ where $j \\in \\{1, \\dots, n\\}$, we must have:\n$$\\int_{A_j} Z \\,d\\mathbb{P} = \\int_{A_j} X \\,d\\mathbb{P}$$\nLet's evaluate the left-hand side. We substitute the expression for $Z$:\n$$\\int_{A_j} Z \\,d\\mathbb{P} = \\int_{A_j} \\left( \\sum_{i=1}^n c_i \\mathbb{1}_{A_i} \\right) d\\mathbb{P}$$\nSince the integral is over $A_j$ and the sets $\\{A_i\\}$ are disjoint, the indicator function $\\mathbb{1}_{A_i}$ is equal to $1$ if $i=j$ and $0$ if $i \\neq j$ for any point within the domain of integration. Thus, the sum collapses to a single term:\n$$\\int_{A_j} c_j \\mathbb{1}_{A_j} \\,d\\mathbb{P} = c_j \\int_{A_j} 1 \\,d\\mathbb{P} = c_j \\mathbb{P}(A_j)$$\nNow, we equate this with the right-hand side of the definitional identity:\n$$c_j \\mathbb{P}(A_j) = \\int_{A_j} X \\,d\\mathbb{P}$$\nThe problem statement specifies that $\\mathbb{P}(A_j)  0$ for all $j$. Therefore, we can solve for $c_j$:\n$$c_j = \\frac{1}{\\mathbb{P}(A_j)} \\int_{A_j} X \\,d\\mathbb{P}$$\nThis expression for $c_j$ is precisely the definition of the expectation of $X$ conditional on the event $A_j$, denoted $\\mathbb{E}[X \\mid A_j]$.\n\nSubstituting this back into the form for $Z$, we obtain the explicit expression for the conditional expectation as a random variable:\n$$\\mathbb{E}[X \\mid \\mathcal{G}] = \\sum_{i=1}^n \\left( \\frac{1}{\\mathbb{P}(A_i)} \\int_{A_i} X \\, d\\mathbb{P} \\right) \\mathbb{1}_{A_i}$$\n\n**Part 2: Justification as the Unique Mean Square Error Minimizer**\n\nWe now assume that $X \\in L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$. We want to show that $Z = \\mathbb{E}[X \\mid \\mathcal{G}]$ uniquely minimizes the mean square error (MSE), $\\mathbb{E}[(X-Y)^2]$, over all $\\mathcal{G}$-measurable random variables $Y \\in L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$.\n\nThe space $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a Hilbert space with the inner product $\\langle U, V \\rangle = \\mathbb{E}[UV]$. The set of all $\\mathcal{G}$-measurable random variables in $L^2$, denoted $L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$, is a closed subspace of $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$. The problem is equivalent to showing that $Z$ is the orthogonal projection of $X$ onto this subspace.\n\nThe key property of an orthogonal projection is that the error vector $(X-Z)$ must be orthogonal to every vector in the subspace. That is, we must show that for any $Y \\in L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$, we have $\\langle X-Z, Y \\rangle = 0$, which is equivalent to $\\mathbb{E}[(X-Z)Y] = 0$.\n\nLet's verify this orthogonality condition. For any $Y \\in L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$:\n$$\\mathbb{E}[(X-Z)Y] = \\mathbb{E}[XY] - \\mathbb{E}[ZY]$$\nWe use the tower property of expectation, $\\mathbb{E}[W] = \\mathbb{E}[\\mathbb{E}[W \\mid \\mathcal{G}]]$.\n$$\\mathbb{E}[XY] = \\mathbb{E}[\\mathbb{E}[XY \\mid \\mathcal{G}]]$$\nSince $Y$ is $\\mathcal{G}$-measurable, we can use the \"taking out what is known\" property of conditional expectation: $\\mathbb{E}[XY \\mid \\mathcal{G}] = Y \\mathbb{E}[X \\mid \\mathcal{G}]$. By our notation, this is $YZ$.\nSo, we have:\n$$\\mathbb{E}[XY] = \\mathbb{E}[YZ]$$\nSubstituting this back into the expression for orthogonality:\n$$\\mathbb{E}[(X-Z)Y] = \\mathbb{E}[YZ] - \\mathbb{E}[ZY] = 0$$\nThis confirms that $(X-Z)$ is orthogonal to the subspace $L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$.\n\nNow, we show that this orthogonality implies that $Z$ is the unique minimizer of the MSE. Let $Y$ be any arbitrary random variable in $L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$. We can write the error term $X-Y$ as $(X-Z) + (Z-Y)$. Note that since both $Z$ and $Y$ are $\\mathcal{G}$-measurable, their difference $(Z-Y)$ is also $\\mathcal{G}$-measurable, and thus lies in the subspace $L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$.\n\nLet's expand the MSE for $Y$:\n$$\\mathbb{E}[(X-Y)^2] = \\mathbb{E}[((X-Z) + (Z-Y))^2]$$\n$$= \\mathbb{E}[(X-Z)^2 + 2(X-Z)(Z-Y) + (Z-Y)^2]$$\nBy linearity of expectation:\n$$= \\mathbb{E}[(X-Z)^2] + 2\\mathbb{E}[(X-Z)(Z-Y)] + \\mathbb{E}[(Z-Y)^2]$$\nThe cross-term is $2\\mathbb{E}[(X-Z)(Z-Y)] = 2\\langle X-Z, Z-Y \\rangle$. Since $(Z-Y) \\in L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$, the orthogonality property we just proved implies this term is zero.\nTherefore, the expression simplifies to the Pythagorean theorem for $L^2$ spaces:\n$$\\mathbb{E}[(X-Y)^2] = \\mathbb{E}[(X-Z)^2] + \\mathbb{E}[(Z-Y)^2]$$\nSince $\\mathbb{E}[(Z-Y)^2] = \\|Z-Y\\|_{L^2}^2 \\ge 0$, we have:\n$$\\mathbb{E}[(X-Y)^2] \\ge \\mathbb{E}[(X-Z)^2]$$\nThis shows that $Z = \\mathbb{E}[X\\mid \\mathcal{G}]$ indeed minimizes the mean square error.\n\nFor uniqueness, the equality $\\mathbb{E}[(X-Y)^2] = \\mathbb{E}[(X-Z)^2]$ holds if and only if $\\mathbb{E}[(Z-Y)^2] = 0$. In the $L^2$ space, this implies that $Z-Y=0$ almost surely, meaning $Y=Z$ almost surely. Thus, the minimizer is unique in $L^2(\\Omega, \\mathcal{G}, \\mathbb{P})$. This completes the justification.", "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\left( \\frac{1}{\\mathbb{P}(A_i)} \\int_{A_i} X \\, d\\mathbb{P} \\right) \\mathbb{1}_{A_i}}\n$$", "id": "3045153"}, {"introduction": "Now that we have a foundational understanding, let's apply it to the cornerstone of stochastic calculus: Brownian motion. This practice involves calculating the expected future value of powers of a Brownian motion, given its history up to the present. These computations [@problem_id:3045096] are not just academic; they are essential in financial modeling for pricing derivatives and in physics for describing diffusion processes, demonstrating how conditional expectation serves as a powerful tool for prediction.", "problem": "Let $\\{B_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$ satisfying the usual conditions, where $\\mathcal{F}_{t}$ is the natural filtration augmented to satisfy completeness and right-continuity. Recall the foundational facts: for $0 \\le s  t$, the increment $B_{t} - B_{s}$ is independent of $\\mathcal{F}_{s}$ and is Gaussian with mean $0$ and variance $t-s$, and the conditional expectation $\\mathbb{E}[X \\mid \\mathcal{F}_{s}]$ is the $L^{2}$ (square-integrable) projection of $X$ onto the closed subspace of $\\mathcal{F}_{s}$-measurable square-integrable random variables.\n\nFix $0 \\le s  t$. Using only the linearity and tower properties of conditional expectation, the $L^{2}$ projection viewpoint (orthogonality to $\\mathcal{F}_{s}$-measurable variables), and the independent Gaussian increments of Brownian motion with their centered moments, compute the conditional expectations $\\mathbb{E}[B_{t}^{k} \\mid \\mathcal{F}_{s}]$ for $k \\in \\{1,2,3,4\\}$, and express each answer explicitly as a function of $B_{s}$ and $t-s$.\n\nYour final answer must list the four expressions in order $k=1,2,3,4$ as a single row matrix. No numerical approximation is required.", "solution": "The problem is to compute the conditional expectation $\\mathbb{E}[B_{t}^{k} \\mid \\mathcal{F}_{s}]$ for $k \\in \\{1,2,3,4\\}$, where $\\{B_{t}\\}_{t \\ge 0}$ is a standard Brownian motion and $0 \\le s  t$. The computation must rely on fundamental properties of conditional expectation and Brownian motion.\n\nThe core strategy is to decompose the random variable $B_t$ into two components: one that is measurable with respect to the filtration $\\mathcal{F}_s$ and another that is independent of it. We write:\n$$B_{t} = B_{s} + (B_{t} - B_{s})$$\nLet the increment be denoted by $\\Delta B = B_{t} - B_{s}$. From the problem definition, we have the following key properties:\n1.  $B_s$ is an $\\mathcal{F}_s$-measurable random variable.\n2.  $\\Delta B$ is independent of the sigma-algebra $\\mathcal{F}_s$.\n3.  $\\Delta B$ is a Gaussian random variable with mean $\\mathbb{E}[\\Delta B] = 0$ and variance $\\mathbb{E}[(\\Delta B)^2] = t-s$.\n\nThe calculation of the conditional expectations will leverage the following standard properties of conditional expectation, which are direct consequences of its definition as an $L^2$ projection:\n-   **Linearity**: $\\mathbb{E}[aX + bY \\mid \\mathcal{G}] = a\\mathbb{E}[X \\mid \\mathcal{G}] + b\\mathbb{E}[Y \\mid \\mathcal{G}]$.\n-   **Taking out what is known**: If a random variable $Z$ is $\\mathcal{G}$-measurable, then $\\mathbb{E}[ZX \\mid \\mathcal{G}] = Z\\mathbb{E}[X \\mid \\mathcal{G}]$. A special case is $\\mathbb{E}[Z \\mid \\mathcal{G}] = Z$.\n-   **Independence**: If a random variable $X$ is independent of $\\mathcal{G}$, then $\\mathbb{E}[X \\mid \\mathcal{G}] = \\mathbb{E}[X]$.\n\nWe will also require the moments of the centered Gaussian increment $\\Delta B \\sim \\mathcal{N}(0, t-s)$. Let $\\sigma^2 = t-s$.\n-   $\\mathbb{E}[\\Delta B] = 0$ (first moment).\n-   $\\mathbb{E}[(\\Delta B)^2] = \\sigma^2 = t-s$ (second moment).\n-   $\\mathbb{E}[(\\Delta B)^3] = 0$ (third moment, as for any centered symmetric distribution).\n-   $\\mathbb{E}[(\\Delta B)^4] = 3\\sigma^4 = 3(t-s)^2$ (fourth moment of a centered Gaussian).\n\nWe now compute the conditional expectation for each value of $k$.\n\n**Case k=1:**\nWe compute $\\mathbb{E}[B_t \\mid \\mathcal{F}_s]$. Using the decomposition and linearity:\n$$ \\mathbb{E}[B_{t} \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{s} + (B_{t} - B_{s}) \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{s} \\mid \\mathcal{F}_{s}] + \\mathbb{E}[B_{t} - B_{s} \\mid \\mathcal{F}_{s}] $$\nSince $B_s$ is $\\mathcal{F}_s$-measurable, $\\mathbb{E}[B_{s} \\mid \\mathcal{F}_{s}] = B_s$. Since $B_t - B_s$ is independent of $\\mathcal{F}_s$, $\\mathbb{E}[B_{t} - B_{s} \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{t} - B_{s}] = 0$.\nTherefore,\n$$ \\mathbb{E}[B_{t} \\mid \\mathcal{F}_{s}] = B_{s} + 0 = B_{s} $$\n\n**Case k=2:**\nWe compute $\\mathbb{E}[B_t^2 \\mid \\mathcal{F}_s]$. We expand the square:\n$$ \\mathbb{E}[B_{t}^2 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[(B_{s} + \\Delta B)^2 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{s}^2 + 2B_{s}\\Delta B + (\\Delta B)^2 \\mid \\mathcal{F}_{s}] $$\nBy linearity:\n$$ = \\mathbb{E}[B_{s}^2 \\mid \\mathcal{F}_{s}] + \\mathbb{E}[2B_{s}\\Delta B \\mid \\mathcal{F}_{s}] + \\mathbb{E}[(\\Delta B)^2 \\mid \\mathcal{F}_{s}] $$\nWe evaluate each term:\n-   $B_s^2$ is $\\mathcal{F}_s$-measurable: $\\mathbb{E}[B_{s}^2 \\mid \\mathcal{F}_{s}] = B_{s}^2$.\n-   Taking out the known term $2B_s$: $\\mathbb{E}[2B_{s}\\Delta B \\mid \\mathcal{F}_{s}] = 2B_{s}\\mathbb{E}[\\Delta B \\mid \\mathcal{F}_{s}] = 2B_{s}\\mathbb{E}[\\Delta B] = 2B_{s} \\cdot 0 = 0$.\n-   $(\\Delta B)^2$ is independent of $\\mathcal{F}_s$: $\\mathbb{E}[(\\Delta B)^2 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[(\\Delta B)^2] = t-s$.\nCombining these results:\n$$ \\mathbb{E}[B_{t}^2 \\mid \\mathcal{F}_{s}] = B_{s}^2 + 0 + (t-s) = B_{s}^2 + t - s $$\n\n**Case k=3:**\nWe compute $\\mathbb{E}[B_t^3 \\mid \\mathcal{F}_s]$ using the binomial expansion of $(B_s + \\Delta B)^3$:\n$$ \\mathbb{E}[B_{t}^3 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{s}^3 + 3B_{s}^2\\Delta B + 3B_{s}(\\Delta B)^2 + (\\Delta B)^3 \\mid \\mathcal{F}_{s}] $$\nBy linearity:\n$$ = \\mathbb{E}[B_{s}^3 \\mid \\mathcal{F}_{s}] + 3\\mathbb{E}[B_{s}^2\\Delta B \\mid \\mathcal{F}_{s}] + 3\\mathbb{E}[B_{s}(\\Delta B)^2 \\mid \\mathcal{F}_{s}] + \\mathbb{E}[(\\Delta B)^3 \\mid \\mathcal{F}_{s}] $$\nWe evaluate each term by pulling out the $\\mathcal{F}_s$-measurable parts:\n-   $\\mathbb{E}[B_{s}^3 \\mid \\mathcal{F}_{s}] = B_{s}^3$.\n-   $3B_{s}^2\\mathbb{E}[\\Delta B \\mid \\mathcal{F}_{s}] = 3B_{s}^2 \\cdot 0 = 0$.\n-   $3B_{s}\\mathbb{E}[(\\Delta B)^2 \\mid \\mathcal{F}_{s}] = 3B_{s}\\mathbb{E}[(\\Delta B)^2] = 3B_{s}(t-s)$.\n-   $\\mathbb{E}[(\\Delta B)^3 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[(\\Delta B)^3] = 0$.\nCombining these results:\n$$ \\mathbb{E}[B_{t}^3 \\mid \\mathcal{F}_{s}] = B_{s}^3 + 0 + 3B_{s}(t-s) + 0 = B_{s}^3 + 3B_{s}(t-s) $$\n\n**Case k=4:**\nWe compute $\\mathbb{E}[B_t^4 \\mid \\mathcal{F}_s]$ using the binomial expansion of $(B_s + \\Delta B)^4$:\n$$ \\mathbb{E}[B_{t}^4 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[B_{s}^4 + 4B_{s}^3\\Delta B + 6B_{s}^2(\\Delta B)^2 + 4B_{s}(\\Delta B)^3 + (\\Delta B)^4 \\mid \\mathcal{F}_{s}] $$\nBy linearity and taking out known terms:\n$$ = \\mathbb{E}[B_{s}^4 \\mid \\mathcal{F}_{s}] + 4B_{s}^3\\mathbb{E}[\\Delta B \\mid \\mathcal{F}_{s}] + 6B_{s}^2\\mathbb{E}[(\\Delta B)^2 \\mid \\mathcal{F}_{s}] + 4B_{s}\\mathbb{E}[(\\Delta B)^3 \\mid \\mathcal{F}_{s}] + \\mathbb{E}[(\\Delta B)^4 \\mid \\mathcal{F}_{s}] $$\nWe evaluate each term:\n-   $\\mathbb{E}[B_{s}^4 \\mid \\mathcal{F}_{s}] = B_{s}^4$.\n-   $4B_{s}^3\\mathbb{E}[\\Delta B \\mid \\mathcal{F}_{s}] = 4B_{s}^3 \\cdot 0 = 0$.\n-   $6B_{s}^2\\mathbb{E}[(\\Delta B)^2 \\mid \\mathcal{F}_{s}] = 6B_{s}^2\\mathbb{E}[(\\Delta B)^2] = 6B_{s}^2(t-s)$.\n-   $4B_{s}\\mathbb{E}[(\\Delta B)^3 \\mid \\mathcal{F}_{s}] = 4B_{s}\\mathbb{E}[(\\Delta B)^3] = 4B_{s} \\cdot 0 = 0$.\n-   $\\mathbb{E}[(\\Delta B)^4 \\mid \\mathcal{F}_{s}] = \\mathbb{E}[(\\Delta B)^4] = 3(t-s)^2$.\nCombining these results:\n$$ \\mathbb{E}[B_{t}^4 \\mid \\mathcal{F}_{s}] = B_{s}^4 + 0 + 6B_{s}^2(t-s) + 0 + 3(t-s)^2 = B_{s}^4 + 6B_{s}^2(t-s) + 3(t-s)^2 $$\n\nTo summarize, the four conditional expectations are:\nFor $k=1$: $B_s$\nFor $k=2$: $B_s^2 + t-s$\nFor $k=3$: $B_s^3 + 3B_s(t-s)$\nFor $k=4$: $B_s^4 + 6B_s^2(t-s) + 3(t-s)^2$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nB_s  B_s^2 + t - s  B_s^3 + 3B_s(t-s)  B_s^4 + 6B_s^2(t-s) + 3(t-s)^2\n\\end{pmatrix}\n}\n$$", "id": "3045096"}, {"introduction": "Our final practice addresses a subtle but critical distinction in probability theory: the difference between orthogonality and independence. While independent random variables are always orthogonal (if they have zero mean), the converse is not true, a common source of confusion. This exercise [@problem_id:3045189] guides you through constructing a clear counterexample using Brownian motion, solidifying your understanding of the geometric nature of the $L^2$ space and the true meaning of statistical independence.", "problem": "Consider a complete probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ supporting a standard Brownian motion (also called a Wiener process) $(W_{t})_{t \\geq 0}$ adapted to its natural filtration $(\\mathcal{F}_{t})_{t \\geq 0}$. Recall the foundational properties: for $0 \\leq s  t$, the increment $W_{t} - W_{s}$ is independent of $\\mathcal{F}_{s}$ and is Gaussian with mean $0$ and variance $t-s$. Let $Y := W_{1}$ and define\n$$\nX := \\big(W_{1}^{2} - 1\\big) + \\big(W_{2} - W_{1}\\big).\n$$\nThis construction is motivated by the $L^{2}$ projection viewpoint of conditional expectation in stochastic analysis, where $\\mathbb{E}[X \\mid \\sigma(Y)]$ is characterized as the unique $Y$-measurable random variable minimizing mean squared error among all measurable functions of $Y$.\n\nTasks:\n1. Using only the above foundational properties, verify that $X \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ and $Y \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$.\n2. Show that $X$ and $Y$ are orthogonal in $L^{2}$, in the sense that $\\mathbb{E}[XY] = 0$.\n3. Explain why $X$ and $Y$ are not independent, appealing to the characterization of independence via conditional expectation as a constant and the $L^{2}$ projection property of conditional expectation.\n4. Compute the conditional expectation $\\mathbb{E}[X \\mid Y]$ explicitly in closed form, and use this computation to demonstrate that $\\mathbb{E}[X \\mid Y]$ is not the zero random variable almost surely.\n\nExpress your final answer for $\\mathbb{E}[X \\mid Y]$ as a single analytic expression in terms of $Y$. No rounding is required, and no physical units are involved.", "solution": "The problem statement is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in stochastic calculus that correctly uses established definitions and properties of Brownian motion and conditional expectation. All necessary information is provided, and there are no internal contradictions. We may proceed with the solution.\n\nThe solution is organized according to the four tasks specified in the problem statement.\n\nTask 1: Verify that $X \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ and $Y \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$.\nA random variable $Z$ belongs to the space $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ if its second moment is finite, i.e., $\\mathbb{E}[Z^2]  \\infty$.\n\nFor $Y := W_{1}$:\n$Y$ is a standard Brownian motion evaluated at time $t=1$. By definition, $W_1$ follows a Gaussian distribution with mean $0$ and variance $1$, denoted $W_1 \\sim N(0,1)$. The second moment of a random variable $Z$ is related to its variance and mean by $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2$.\nFor $Y=W_1$, we have $\\mathbb{E}[Y] = 0$ and $\\operatorname{Var}(Y) = 1$.\nTherefore, the second moment of $Y$ is:\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}[W_1^2] = \\operatorname{Var}(W_1) + (\\mathbb{E}[W_1])^2 = 1 + 0^2 = 1.\n$$\nSince $\\mathbb{E}[Y^2] = 1  \\infty$, we confirm that $Y \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$.\n\nFor $X := \\big(W_{1}^{2} - 1\\big) + \\big(W_{2} - W_{1}\\big)$:\nTo show $X \\in L^2$, we must compute $\\mathbb{E}[X^2]$. Let $A := W_1^2 - 1$ and $B := W_2 - W_1$. Then $X = A+B$.\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[(A+B)^2] = \\mathbb{E}[A^2 + 2AB + B^2] = \\mathbb{E}[A^2] + 2\\mathbb{E}[AB] + \\mathbb{E}[B^2].\n$$\nWe compute each term separately:\n- $\\mathbb{E}[B^2]$: The increment $B = W_2 - W_1$ is Gaussian with mean $0$ and variance $2-1=1$. So, $B \\sim N(0,1)$. Its second moment is $\\mathbb{E}[B^2] = \\operatorname{Var}(B) + (\\mathbb{E}[B])^2 = 1 + 0^2 = 1$.\n- $\\mathbb{E}[A^2]$: $A = W_1^2 - 1$. We have $\\mathbb{E}[A^2] = \\mathbb{E}[(W_1^2 - 1)^2] = \\mathbb{E}[W_1^4 - 2W_1^2 + 1] = \\mathbb{E}[W_1^4] - 2\\mathbb{E}[W_1^2] + 1$. For a standard normal variable $Z \\sim N(0,1)$, the second moment is $\\mathbb{E}[Z^2]=1$ and the fourth moment is $\\mathbb{E}[Z^4]=3$. Since $W_1 \\sim N(0,1)$, we have $\\mathbb{E}[W_1^2]=1$ and $\\mathbb{E}[W_1^4]=3$. Substituting these values gives $\\mathbb{E}[A^2] = 3 - 2(1) + 1 = 2$.\n- $\\mathbb{E}[AB]$: The random variable $A = W_1^2 - 1$ is $\\mathcal{F}_1$-measurable. The increment $B = W_2 - W_1$ is, by definition of Brownian motion, independent of the sigma-algebra $\\mathcal{F}_1$. Consequently, $A$ and $B$ are independent random variables. For independent variables, the expectation of their product is the product of their expectations: $\\mathbb{E}[AB] = \\mathbb{E}[A]\\mathbb{E}[B]$.\nThe expectation of $A$ is $\\mathbb{E}[A] = \\mathbb{E}[W_1^2 - 1] = \\mathbb{E}[W_1^2] - 1 = 1 - 1 = 0$.\nThe expectation of $B$ is $\\mathbb{E}[B] = \\mathbb{E}[W_2 - W_1] = 0$.\nThus, $\\mathbb{E}[AB] = 0 \\cdot 0 = 0$.\n\nCombining these results, the second moment of $X$ is:\n$$\n\\mathbb{E}[X^2] = 2 + 2(0) + 1 = 3.\n$$\nSince $\\mathbb{E}[X^2] = 3  \\infty$, we confirm that $X \\in L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$.\n\nTask 2: Show that $X$ and $Y$ are orthogonal in $L^{2}$, i.e., $\\mathbb{E}[XY] = 0$.\nTwo random variables in $L^2$ are orthogonal if the expectation of their product is zero. We compute $\\mathbb{E}[XY]$:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}\\left[\\left( (W_1^2 - 1) + (W_2 - W_1) \\right) W_1\\right].\n$$\nBy linearity of expectation, this is:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[(W_1^2 - 1)W_1] + \\mathbb{E}[(W_2 - W_1)W_1].\n$$\n- First term: $\\mathbb{E}[W_1^3 - W_1] = \\mathbb{E}[W_1^3] - \\mathbb{E}[W_1]$. Since $W_1 \\sim N(0,1)$, all its odd moments are zero. Thus, $\\mathbb{E}[W_1^3] = 0$ and $\\mathbb{E}[W_1] = 0$, which means $\\mathbb{E}[W_1^3 - W_1] = 0-0=0$.\n- Second term: $\\mathbb{E}[(W_2 - W_1)W_1]$. We use the tower property of conditional expectation, conditioning on $\\mathcal{F}_1$:\n$$\n\\mathbb{E}[(W_2 - W_1)W_1] = \\mathbb{E}\\left[ \\mathbb{E}[(W_2 - W_1)W_1 \\mid \\mathcal{F}_1] \\right].\n$$\nSince $W_1$ is $\\mathcal{F}_1$-measurable, it can be factored out of the inner conditional expectation:\n$$\n\\mathbb{E}\\left[ W_1 \\mathbb{E}[W_2 - W_1 \\mid \\mathcal{F}_1] \\right].\n$$\nThe increment $W_2 - W_1$ is independent of $\\mathcal{F}_1$, so its conditional expectation given $\\mathcal{F}_1$ is just its unconditional expectation: $\\mathbb{E}[W_2 - W_1 \\mid \\mathcal{F}_1] = \\mathbb{E}[W_2 - W_1] = 0$.\nThe expression becomes $\\mathbb{E}[W_1 \\cdot 0] = \\mathbb{E}[0] = 0$.\n\nCombining the two terms, we get $\\mathbb{E}[XY] = 0 + 0 = 0$. This demonstrates that $X$ and $Y$ are orthogonal in $L^2$.\n\nTask 4: Compute the conditional expectation $\\mathbb{E}[X \\mid Y]$.\nWe first compute the conditional expectation from Task 4 as it is required for Task 3. We must find $\\mathbb{E}[X \\mid Y]$, which is the projection of $X$ onto the space of $Y$-measurable random variables. Since $Y=W_1$, we are conditioning on $\\sigma(W_1)$.\n$$\n\\mathbb{E}[X \\mid Y] = \\mathbb{E}[(W_1^2 - 1) + (W_2 - W_1) \\mid Y]\n$$\nUsing the linearity of conditional expectation:\n$$\n\\mathbb{E}[X \\mid Y] = \\mathbb{E}[W_1^2 - 1 \\mid Y] + \\mathbb{E}[W_2 - W_1 \\mid Y].\n$$\n- First term: The random variable $W_1^2 - 1$ is a direct function of $Y=W_1$, namely $Y^2-1$. A random variable that is measurable with respect to the conditioning sigma-algebra is left unchanged by the conditional expectation. Thus:\n$$\n\\mathbb{E}[W_1^2 - 1 \\mid Y] = W_1^2 - 1 = Y^2 - 1.\n$$\n- Second term: $\\mathbb{E}[W_2 - W_1 \\mid Y]$. Since $Y=W_1$, this is $\\mathbb{E}[W_2 - W_1 \\mid W_1]$. The sigma-algebra generated by $W_1$, $\\sigma(W_1)$, is a sub-sigma-algebra of $\\mathcal{F}_1$. The increment $W_2 - W_1$ is independent of $\\mathcal{F}_1$ and therefore is also independent of $\\sigma(W_1)$. The conditional expectation of a random variable, given a sigma-algebra it is independent of, is simply its unconditional expectation.\n$$\n\\mathbb{E}[W_2 - W_1 \\mid Y] = \\mathbb{E}[W_2 - W_1] = 0.\n$$\nCombining the two terms, we obtain the conditional expectation of $X$ given $Y$:\n$$\n\\mathbb{E}[X \\mid Y] = (Y^2 - 1) + 0 = Y^2 - 1.\n$$\nTo demonstrate this is not the zero random variable almost surely, we consider the event $\\mathbb{E}[X \\mid Y] = 0$. This is equivalent to the event $Y^2 - 1 = 0$, or $Y \\in \\{-1, 1\\}$. Since $Y = W_1$ follows a continuous distribution ($N(0,1)$), the probability of it taking any specific value is zero.\n$$\n\\mathbb{P}(\\mathbb{E}[X \\mid Y] = 0) = \\mathbb{P}(Y^2 = 1) = \\mathbb{P}(Y=1) + \\mathbb{P}(Y=-1) = 0 + 0 = 0.\n$$\nThus, $\\mathbb{E}[X \\mid Y]$ is not the zero random variable almost surely.\n\nTask 3: Explain why $X$ and $Y$ are not independent.\nTwo random variables $X$ and $Y$ are independent if and only if for any Borel-measurable function $g$, $\\mathbb{E}[g(X) \\mid Y] = \\mathbb{E}[g(X)]$. A necessary condition for independence is $\\mathbb{E}[X \\mid Y] = \\mathbb{E}[X]$ almost surely.\nFirst, we compute the unconditional expectation of $X$:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[(W_1^2 - 1) + (W_2 - W_1)] = \\mathbb{E}[W_1^2 - 1] + \\mathbb{E}[W_2 - W_1].\n$$\nAs shown in Task 1, $\\mathbb{E}[W_1^2 - 1] = 0$ and $\\mathbb{E}[W_2 - W_1] = 0$. So, $\\mathbb{E}[X] = 0$.\nIf $X$ and $Y$ were independent, it would follow that $\\mathbb{E}[X \\mid Y] = \\mathbb{E}[X] = 0$ almost surely.\nHowever, in Task 4 we computed $\\mathbb{E}[X \\mid Y] = Y^2 - 1$.\nWe have also shown that $Y^2 - 1$ is not zero almost surely. This creates a contradiction. Therefore, the initial assumption of independence must be false. $X$ and $Y$ are not independent.\n\nThe problem's reference to the $L^2$ projection property provides the geometric intuition behind this result. The conditional expectation $\\mathbb{E}[X \\mid Y]$ is the orthogonal projection of $X$ onto the closed subspace of $L^2$ functions of $Y$. If $X$ were independent of $Y$, $X - \\mathbb{E}[X]$ would be orthogonal to this subspace, meaning the projection of $X - \\mathbb{E}[X]$ is zero. As $\\mathbb{E}[X]=0$, this implies the projection of $X$ itself, $\\mathbb{E}[X \\mid Y]$, must be zero. Since our calculation yields a non-zero projection, $X$ and $Y$ cannot be independent. The orthogonality found in Task 2 ($\\mathbb{E}[XY]=0$) shows that $X$ is orthogonal to the simplest non-constant function of $Y$ (namely $Y$ itself), but non-independence is revealed because $X$ is not orthogonal to all functions of $Y$ (e.g., it is not orthogonal to $Y^2-1$, as $\\mathbb{E}[X(Y^2-1)] = \\mathbb{E}[XY^2] = 2 \\neq 0$).", "answer": "$$\n\\boxed{Y^2 - 1}\n$$", "id": "3045189"}]}