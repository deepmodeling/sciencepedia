{"hands_on_practices": [{"introduction": "Before tackling complex applications, it is crucial to have a solid grasp of the fundamental definition of convergence in probability. This exercise [@problem_id:1910711] serves as a powerful litmus test for your understanding. It presents a simple, deterministic sequence that oscillates, forcing you to rigorously apply the definition rather than relying on intuition, and revealing that not all bounded sequences converge.", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ that models the state of a simple digital switch at discrete time steps $n=1, 2, 3, \\ldots$. The switch is designed to be in one of two states, represented by the values $-1$ and $1$. The state of the switch at time $n$ is deterministic and follows the rule $X_n = (-1)^n$. This means that for each $n$, the random variable $X_n$ takes the value $(-1)^n$ with a probability of 1.\n\nThe sequence of states is therefore $X_1 = -1$, $X_2 = 1$, $X_3 = -1$, $X_4 = 1$, and so on.\n\nWhich of the following statements correctly describes the convergence behavior of this sequence?\n\nA. The sequence $\\{X_n\\}$ converges in probability to 0.\n\nB. The sequence $\\{X_n\\}$ converges in probability to 1.\n\nC. The sequence $\\{X_n\\}$ converges in probability to -1.\n\nD. The sequence $\\{X_n\\}$ does not converge in probability.\n\nE. The sequence $\\{X_n\\}$ converges in probability, but the limit is not a constant.", "solution": "We recall the definition: a sequence of random variables $\\{X_{n}\\}$ converges in probability to a random variable $X$ if for every $\\varepsilon>0$,\n$$\n\\lim_{n\\to\\infty} \\mathbb{P}\\left(|X_{n}-X|>\\varepsilon\\right)=0.\n$$\nHere each $X_{n}$ is deterministic with $X_{n}=(-1)^{n}$ almost surely.\n\nFirst, check convergence in probability to a constant $c\\in\\mathbb{R}$. If $c\\notin\\{-1,1\\}$, set $\\delta=\\min\\{|c-1|,|c+1|\\}>0$ and choose $\\varepsilon=\\delta/2$. Then for every $n$,\n$$\n|X_{n}-c|=\\left|(-1)^{n}-c\\right|\\geq \\delta>\\varepsilon,\n$$\nso $\\mathbb{P}(|X_{n}-c|>\\varepsilon)=1$ for all $n$, which cannot tend to $0$. Hence no limit $c\\notin\\{-1,1\\}$ is possible.\n\nIf $c=1$, choose $\\varepsilon=\\frac{1}{2}$. Then for odd $n$,\n$$\n|X_{n}-1|=|(-1)-1|=2>\\frac{1}{2},\n$$\nso $\\mathbb{P}(|X_{n}-1|>\\frac{1}{2})=1$ for all odd $n$, which does not tend to $0$. Similarly, if $c=-1$, for even $n$,\n$$\n|X_{n}-(-1)|=|1-(-1)|=2>\\frac{1}{2},\n$$\nso $\\mathbb{P}(|X_{n}+1|>\\frac{1}{2})=1$ for all even $n$, which also does not tend to $0$. Therefore $\\{X_{n}\\}$ does not converge in probability to any constant, ruling out A, B, and C.\n\nNext, consider whether $\\{X_{n}\\}$ could converge in probability to a non-constant random variable $X$. Take $\\varepsilon=\\frac{1}{2}$. Then\n$$\n\\mathbb{P}\\left(|X_{n}-X|>\\frac{1}{2}\\right)=\\mathbb{P}\\left(|(-1)^{n}-X|>\\frac{1}{2}\\right).\n$$\nDecompose $X$ by its mass on $\\{-1,1\\}$: let $p=\\mathbb{P}(X=1)$, $q=\\mathbb{P}(X=-1)$, and $r=1-p-q=\\mathbb{P}(X\\notin\\{-1,1\\})$. For even $n$ (so $X_{n}=1$),\n$$\n\\mathbb{P}\\left(|X_{n}-X|>\\frac{1}{2}\\right)=\\mathbb{P}\\left(|1-X|>\\frac{1}{2}\\right)=q+r.\n$$\nFor odd $n$ (so $X_{n}=-1$),\n$$\n\\mathbb{P}\\left(|X_{n}-X|>\\frac{1}{2}\\right)=\\mathbb{P}\\left(|-1-X|>\\frac{1}{2}\\right)=p+r.\n$$\nThus the sequence of probabilities alternates between the constants $q+r$ and $p+r$. For convergence in probability, we would need both $q+r\\to 0$ and $p+r\\to 0$, which forces $p=q=r=0$, contradicting $p+q+r=1$. Hence no random limit (constant or not) exists. Therefore the sequence does not converge in probability, and option D is correct.", "answer": "$$\\boxed{D}$$", "id": "1910711"}, {"introduction": "A key feature of convergence in probability is how it behaves under algebraic operations, which is formalized by Slutsky's theorem and its related properties. This practice problem [@problem_id:1910723] allows you to explore this by combining two sequences. You will demonstrate that the sum of a sequence converging to a constant and another sequence converging to zero results in a new sequence that converges to that same constant, a foundational concept for analyzing more complex stochastic models.", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables. The sequence $\\{X_n\\}$ is known to converge in probability to the constant 5. The sequence $\\{Y_n\\}$ is characterized by having a mean of $\\mathbb{E}[Y_n] = 0$ and a variance of $\\text{Var}(Y_n) = \\frac{1}{\\sqrt{n}}$ for all positive integers $n$.\n\nA new sequence of random variables, $\\{Z_n\\}_{n=1}^{\\infty}$, is defined by the sum $Z_n = X_n + Y_n$.\n\nDetermine the numerical value to which the sequence $\\{Z_n\\}$ converges in probability.", "solution": "We are given that $X_{n} \\xrightarrow{p} 5$, that is, for every $\\varepsilon>0$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}\\big(|X_{n}-5|>\\varepsilon\\big)=0.\n$$\nFor $Y_{n}$, we have $\\mathbb{E}[Y_{n}]=0$ and $\\text{Var}(Y_{n})=n^{-1/2}$ for all $n$. By Chebyshev’s inequality, for any $\\varepsilon>0$,\n$$\n\\mathbb{P}\\big(|Y_{n}|>\\varepsilon\\big)=\\mathbb{P}\\big(|Y_{n}-\\mathbb{E}[Y_{n}]|>\\varepsilon\\big)\\leq \\frac{\\text{Var}(Y_{n})}{\\varepsilon^{2}}=\\frac{n^{-1/2}}{\\varepsilon^{2}}\\xrightarrow[n\\to\\infty]{}0.\n$$\nHence $Y_{n}\\xrightarrow{p}0$.\n\nDefine $Z_{n}=X_{n}+Y_{n}$. For any $\\varepsilon>0$, by the triangle inequality,\n$$\n|Z_{n}-5|=\\big|(X_{n}-5)+Y_{n}\\big|\\leq |X_{n}-5|+|Y_{n}|.\n$$\nTherefore, using the union bound,\n$$\n\\mathbb{P}\\big(|Z_{n}-5|>\\varepsilon\\big)\\leq \\mathbb{P}\\big(|X_{n}-5|>\\varepsilon/2\\big)+\\mathbb{P}\\big(|Y_{n}|>\\varepsilon/2\\big)\\xrightarrow[n\\to\\infty]{}0,\n$$\nbecause the first term tends to zero by $X_{n}\\xrightarrow{p}5$ and the second term tends to zero by Chebyshev’s inequality shown above. Thus $Z_{n}\\xrightarrow{p}5$.\n\nTherefore, the numerical value to which $Z_{n}$ converges in probability is $5$.", "answer": "$$\\boxed{5}$$", "id": "1910723"}, {"introduction": "Convergence in probability is the theoretical backbone for one of the most desirable properties of a statistical estimator: consistency. This practice problem [@problem_id:1293154] frames this concept within a practical online learning algorithm, where an estimate is updated sequentially as new data arrives. By analyzing the recursive formula, you will not only show that the estimator converges to the true value but also quantify the rate at which its variance decreases, providing deeper insight into the efficiency of the estimation process.", "problem": "Consider a simple online learning algorithm designed to estimate an unknown, constant parameter. The algorithm receives a sequence of measurements $Z_1, Z_2, Z_3, \\ldots$. These measurements are random variables that are independent and identically distributed (i.i.d.) with a true mean $\\mathbb{E}[Z_k] = \\mu$ and a finite, positive variance $\\text{Var}(Z_k) = \\sigma^2$ for all $k \\ge 1$.\n\nThe algorithm's estimate of $\\mu$ after $n$ measurements is denoted by $X_n$. The process begins with the initial estimate being the first measurement, so $X_1 = Z_1$. For all subsequent steps ($n \\ge 1$), the estimate is updated using the following recursive formula:\n$$X_{n+1} = \\left(1 - \\frac{1}{n+1}\\right)X_n + \\frac{1}{n+1}Z_{n+1}$$\nThis type of update, where the new estimate is a weighted average of the old estimate and the new data, is common in adaptive systems.\n\nThe quality of the estimation process can be analyzed by its variance. We are interested in the asymptotic behavior of the variance of the estimate $X_n$. Determine the value of the limit $\\lim_{n \\to \\infty} n \\cdot \\text{Var}(X_n)$. Express your answer as a symbolic expression in terms of the given parameters $\\mu$ and/or $\\sigma^2$.", "solution": "We start by identifying the form of the estimator $X_{n}$. We prove by induction that $X_{n}$ is the sample mean of the first $n$ measurements:\nBase case: $X_{1} = Z_{1}$ agrees with the sample mean for $n=1$.\nInductive step: Assume $X_{n} = \\frac{1}{n}\\sum_{k=1}^{n} Z_{k}$. Then, using the given recursion,\n$$\nX_{n+1} = \\left(1 - \\frac{1}{n+1}\\right)X_{n} + \\frac{1}{n+1}Z_{n+1}\n= \\frac{n}{n+1}\\cdot \\frac{1}{n}\\sum_{k=1}^{n} Z_{k} + \\frac{1}{n+1}Z_{n+1}\n= \\frac{1}{n+1}\\sum_{k=1}^{n+1} Z_{k}.\n$$\nThus, by induction, for all $n \\ge 1$,\n$$\nX_{n} = \\frac{1}{n}\\sum_{k=1}^{n} Z_{k}.\n$$\n\nNext, we compute the variance of $X_{n}$. Using linearity of variance for independent random variables and the scaling property of variance, we have\n$$\n\\text{Var}(X_{n}) = \\text{Var}\\!\\left(\\frac{1}{n}\\sum_{k=1}^{n} Z_{k}\\right)\n= \\frac{1}{n^{2}}\\,\\text{Var}\\!\\left(\\sum_{k=1}^{n} Z_{k}\\right).\n$$\nSince the $Z_{k}$ are independent and identically distributed with $\\text{Var}(Z_{k}) = \\sigma^{2}$, the variance of the sum is the sum of variances:\n$$\n\\text{Var}\\!\\left(\\sum_{k=1}^{n} Z_{k}\\right) = \\sum_{k=1}^{n} \\text{Var}(Z_{k}) = n\\sigma^{2}.\n$$\nTherefore,\n$$\n\\text{Var}(X_{n}) = \\frac{1}{n^{2}} \\cdot n \\sigma^{2} = \\frac{\\sigma^{2}}{n}.\n$$\n\nMultiplying by $n$ and taking the limit yields\n$$\n\\lim_{n \\to \\infty} n \\cdot \\text{Var}(X_{n}) = \\lim_{n \\to \\infty} n \\cdot \\frac{\\sigma^{2}}{n} = \\sigma^{2}.\n$$\nThis limit does not depend on $\\mu$ and equals the common variance $\\sigma^{2}$ of the measurements.", "answer": "$$\\boxed{\\sigma^{2}}$$", "id": "1293154"}]}