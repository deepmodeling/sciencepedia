{"hands_on_practices": [{"introduction": "A central reason for the prominence of Gaussian distributions in finance and physics is that the solution to many linear stochastic differential equations can be expressed as an Itô integral. This exercise takes you back to first principles, asking you to show how the Itô integral of a simple deterministic function against a Brownian motion results in a normally distributed random variable. By building the result from its definition as a limit of Riemann sums, you will gain a foundational understanding of how Gaussian noise accumulates over time.", "problem": "Let $T>0$ and $\\alpha>0$ be fixed constants. Let $\\{W_{t}\\}_{t\\geq 0}$ be a standard Brownian motion (also called a Wiener process), characterized by $W_{0}=0$, independent increments, and $W_{t}-W_{s}\\sim \\mathcal{N}(0,t-s)$ for $0\\leq s<t$, with continuous sample paths. Consider the stochastic Itô integral\n$$\nX \\;=\\; \\int_{0}^{T} \\exp(-\\alpha t)\\, \\mathrm{d}W_{t}.\n$$\nStarting from the construction of the Itô integral as an $L^{2}$-limit of stochastic Riemann sums for deterministic square-integrable integrands, and using only the defining properties of Brownian motion and the characterization of Gaussian random vectors by linear combinations of independent Gaussian increments, determine the probability distribution of the random variable $X$. Your answer must identify the mean and variance of $X$ and give its law in closed form. Express the final distribution concisely as a single analytic expression. No numerical rounding is required.", "solution": "The problem asks for the probability distribution of the random variable $X$ defined by the Itô integral $X = \\int_{0}^{T} \\exp(-\\alpha t)\\, \\mathrm{d}W_{t}$, where $\\{W_{t}\\}_{t\\geq 0}$ is a standard Brownian motion and $T>0$, $\\alpha>0$ are constants. We are instructed to derive this starting from the definition of the Itô integral as a limit of stochastic Riemann sums.\n\nThe integrand is the deterministic function $f(t) = \\exp(-\\alpha t)$. This function is continuous on the interval $[0, T]$ and therefore square-integrable, i.e., $\\int_{0}^{T} |f(t)|^{2} \\, \\mathrm{d}t < \\infty$. For such an integrand, the Itô integral is well-defined as the limit in mean square ($L^{2}$) of a sequence of approximating sums.\n\nLet us construct such a sequence. Consider a sequence of partitions of the interval $[0, T]$, denoted by $\\Pi_{n} = \\{0 = t_{0}^{(n)} < t_{1}^{(n)} < \\dots < t_{m_{n}}^{(n)} = T\\}$, such that the mesh of the partition, $\\|\\Pi_{n}\\| = \\max_{k} (t_{k+1}^{(n)} - t_{k}^{(n)})$, converges to $0$ as $n \\to \\infty$.\n\nThe Itô integral $X$ is defined as the $L^{2}$-limit of the sequence of random variables $\\{X_{n}\\}_{n \\in \\mathbb{N}}$, where each $X_{n}$ is a stochastic Riemann sum corresponding to the partition $\\Pi_n$:\n$$\nX_{n} = \\sum_{k=0}^{m_{n}-1} f(t_{k}^{(n)}) (W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}})\n$$\nSubstituting the specific integrand $f(t) = \\exp(-\\alpha t)$, we have:\n$$\nX_{n} = \\sum_{k=0}^{m_{n}-1} \\exp(-\\alpha t_{k}^{(n)}) (W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}})\n$$\nBy definition, $X = \\lim_{n\\to\\infty} X_{n}$ in the $L^{2}$ sense.\n\nWe now determine the probability distribution of each $X_{n}$. Each $X_{n}$ is a finite linear combination of the increments of the Brownian motion, $\\Delta W_{k}^{(n)} = W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}}$. According to the properties of Brownian motion, these increments are independent random variables. Furthermore, for any $s < t$, the increment $W_{t} - W_{s}$ is a Gaussian (normal) random variable with mean $0$ and variance $t-s$, which we denote as $W_{t} - W_{s} \\sim \\mathcal{N}(0, t-s)$.\nTherefore, each increment $\\Delta W_{k}^{(n)}$ in the sum follows the distribution $\\mathcal{N}(0, t_{k+1}^{(n)} - t_{k}^{(n)})$.\n\nThe term $\\exp(-\\alpha t_{k}^{(n)})$ is a deterministic constant for each $k$. A constant multiple of a Gaussian random variable is also a Gaussian random variable. Let $Y_{k}^{(n)} = \\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}$. The mean of $Y_{k}^{(n)}$ is:\n$$\nE[Y_{k}^{(n)}] = E[\\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}] = \\exp(-\\alpha t_{k}^{(n)}) E[\\Delta W_{k}^{(n)}] = \\exp(-\\alpha t_{k}^{(n)}) \\cdot 0 = 0\n$$\nThe variance of $Y_{k}^{(n)}$ is:\n$$\n\\text{Var}(Y_{k}^{(n)}) = \\text{Var}(\\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}) = (\\exp(-\\alpha t_{k}^{(n)}))^{2} \\text{Var}(\\Delta W_{k}^{(n)}) = \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nSo, $Y_{k}^{(n)} \\sim \\mathcal{N}(0, \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)}))$.\n\nThe random variable $X_{n}$ is the sum of these independent Gaussian random variables $Y_{k}^{(n)}$: $X_{n} = \\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}$. A fundamental property of Gaussian distributions is that a sum of independent Gaussian random variables is also Gaussian. Thus, $X_{n}$ is a Gaussian random variable.\n\nThe mean of $X_{n}$ is the sum of the means of $Y_{k}^{(n)}$:\n$$\nE[X_{n}] = E\\left[\\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}\\right] = \\sum_{k=0}^{m_{n}-1} E[Y_{k}^{(n)}] = \\sum_{k=0}^{m_{n}-1} 0 = 0\n$$\nThe variance of $X_{n}$ is the sum of the variances of the independent $Y_{k}^{(n)}$:\n$$\n\\text{Var}(X_{n}) = \\text{Var}\\left(\\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}\\right) = \\sum_{k=0}^{m_{n}-1} \\text{Var}(Y_{k}^{(n)}) = \\sum_{k=0}^{m_{n}-1} \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nSo, for each $n$, the distribution of $X_{n}$ is $\\mathcal{N}(0, \\sigma_{n}^{2})$ where $\\sigma_{n}^{2} = \\text{Var}(X_{n})$.\n\nNow we take the limit as $n \\to \\infty$. The sequence of random variables $X_{n}$ converges in $L^{2}$ to $X$. This implies that $X_{n}$ also converges in distribution to $X$. A standard result in probability theory states that if a sequence of Gaussian random variables converges in distribution, the limiting distribution is also Gaussian. The parameters of the limiting Gaussian distribution are the limits of the parameters of the sequence.\n\nThe mean of $X$ is the limit of the means of $X_{n}$:\n$$\nE[X] = \\lim_{n\\to\\infty} E[X_{n}] = \\lim_{n\\to\\infty} 0 = 0\n$$\nThe variance of $X$ is the limit of the variances of $X_{n}$:\n$$\n\\text{Var}(X) = \\lim_{n\\to\\infty} \\text{Var}(X_{n}) = \\lim_{n\\to\\infty} \\sum_{k=0}^{m_{n}-1} \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nThis sum is a Riemann sum for the function $g(t) = \\exp(-2\\alpha t)$ over the interval $[0, T]$. As the mesh of the partition $\\|\\Pi_{n}\\|$ goes to $0$, this sum converges to the definite integral of $g(t)$:\n$$\n\\text{Var}(X) = \\int_{0}^{T} \\exp(-2\\alpha t) \\, \\mathrm{d}t\n$$\nThis integral is straightforward to evaluate. Since $\\alpha > 0$:\n$$\n\\int_{0}^{T} \\exp(-2\\alpha t) \\, \\mathrm{d}t = \\left[ \\frac{\\exp(-2\\alpha t)}{-2\\alpha} \\right]_{0}^{T} = \\frac{\\exp(-2\\alpha T)}{-2\\alpha} - \\frac{\\exp(-2\\alpha \\cdot 0)}{-2\\alpha} = -\\frac{\\exp(-2\\alpha T)}{2\\alpha} + \\frac{1}{2\\alpha} = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\n$$\nThis result is a specific case of the Itô isometry for deterministic integrands, which states that $E\\left[\\left(\\int_{0}^{T} f(t) \\mathrm{d}W_{t}\\right)^{2}\\right] = \\int_{0}^{T} |f(t)|^{2} \\mathrm{d}t$. Since the mean is $0$, the second moment is equal to the variance.\n\nIn summary, the random variable $X = \\int_{0}^{T} \\exp(-\\alpha t) \\, \\mathrm{d}W_{t}$ is the $L^{2}$-limit of a sequence of Gaussian random variables $\\{X_{n}\\}$, each with mean $0$. Therefore, $X$ itself must be a Gaussian random variable with a mean equal to the limit of the means ($0$) and a variance equal to the limit of the variances.\n\nThe mean of $X$ is $E[X]=0$.\nThe variance of $X$ is $\\text{Var}(X) = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$.\n\nThus, the probability distribution of $X$ is a normal (Gaussian) distribution with mean $0$ and variance $\\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$. This can be concisely written as:\n$$\nX \\sim \\mathcal{N}\\left(0, \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\\right)\n$$\nThis is the closed-form law for the random variable $X$.", "answer": "$$\n\\boxed{\\mathcal{N}\\left(0, \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\\right)}\n$$", "id": "3068161"}, {"introduction": "While single stochastic processes often result in a Gaussian variable, real-world systems typically involve multiple, interacting processes. This leads to Gaussian random vectors where the components are correlated. This practice provides a crucial tool for simplifying such systems: the whitening transformation, which converts a vector of correlated Gaussian variables into a set of independent standard normal variables that are much easier to work with.", "problem": "Let $W(t)$ be a standard Brownian motion (also called a Wiener process) on a filtered probability space over the time interval $[0,T]$ with $T>0$. Let $f:[0,T]\\to\\mathbb{R}$ and $g:[0,T]\\to\\mathbb{R}$ be deterministic, square-integrable functions such that\n$$\n\\int_{0}^{T} f(t)^{2}\\,dt>0,\\quad \\int_{0}^{T} g(t)^{2}\\,dt>0,\\quad \\int_{0}^{T} f(t)g(t)\\,dt\\neq 0,\n$$\nand assume $f$ and $g$ are not proportional almost everywhere on $[0,T]$ so that the covariance matrix below is positive definite. Consider the Gaussian random vector\n$$\n\\begin{pmatrix}\nX_{1}\\\\\nX_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\int_{0}^{T} f(t)\\,dW(t)\n\\\\\n\\int_{0}^{T} g(t)\\,dW(t)\n\\end{pmatrix}.\n$$\nStarting from the foundational properties of Brownian motion and Itō integration in stochastic differential equations, derive the covariance matrix of $\\left(X_{1},X_{2}\\right)$, and construct an explicit $2\\times 2$ matrix $A$ (with entries expressed in terms of $\\int_{0}^{T} f(t)^{2}\\,dt$, $\\int_{0}^{T} g(t)^{2}\\,dt$, and $\\int_{0}^{T} f(t)g(t)\\,dt$) such that the transformed vector\n$$\n\\begin{pmatrix}\nZ_{1}\\\\\nZ_{2}\n\\end{pmatrix}\n=\nA\n\\begin{pmatrix}\nX_{1}\\\\\nX_{2}\n\\end{pmatrix}\n$$\nconsists of two independent standard normal random variables. Verify independence by computing the covariance matrix of $\\left(Z_{1},Z_{2}\\right)$ and invoking the defining property of joint Gaussian random vectors. Your final answer must be the explicit matrix $A$ as a single analytic expression.", "solution": "The vector $X = (X_1, X_2)^T$ is a Gaussian random vector because its components are linear functionals (Itō integrals) of a Gaussian process, the standard Brownian motion $W(t)$. A Gaussian random vector is fully characterized by its mean vector and covariance matrix.\n\nFirst, we compute the mean vector $\\mu_X = E[X]$. The expectation of an Itō integral with a deterministic integrand is zero.\n$$\nE[X_1] = E\\left[ \\int_{0}^{T} f(t)\\,dW(t) \\right] = 0\n$$\n$$\nE[X_2] = E\\left[ \\int_{0}^{T} g(t)\\,dW(t) \\right] = 0\n$$\nThus, the mean vector is $\\mu_X = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nNext, we derive the covariance matrix $\\Sigma_X = E[X X^T]$. The entries of this matrix are given by the Itō isometry property and its extension to covariances.\nThe diagonal entries are the variances:\n$$\n\\text{Var}(X_1) = E[X_1^2] = \\int_{0}^{T} f(t)^2\\,dt\n$$\n$$\n\\text{Var}(X_2) = E[X_2^2] = \\int_{0}^{T} g(t)^2\\,dt\n$$\nThe off-diagonal entries are the covariances:\n$$\n\\text{Cov}(X_1, X_2) = E[X_1 X_2] = \\int_{0}^{T} f(t)g(t)\\,dt\n$$\nTherefore, the covariance matrix of $X$ is:\n$$\n\\Sigma_X =\n\\begin{pmatrix}\n\\int_{0}^{T} f(t)^{2}\\,dt & \\int_{0}^{T} f(t)g(t)\\,dt \\\\\n\\int_{0}^{T} f(t)g(t)\\,dt & \\int_{0}^{T} g(t)^{2}\\,dt\n\\end{pmatrix}\n$$\nWe are looking for a $2 \\times 2$ matrix $A$ such that the vector $Z = AX$ has components $Z_1, Z_2$ that are independent standard normal random variables. Since $X$ is a Gaussian vector, the transformed vector $Z$ is also a Gaussian vector. Its mean is $E[Z] = A E[X] = A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. For $Z_1$ and $Z_2$ to be independent standard normal variables, their covariance matrix $\\Sigma_Z$ must be the $2 \\times 2$ identity matrix $I$.\nThe covariance matrix of $Z$ is given by $\\Sigma_Z = E[Z Z^T] = E[(AX)(AX)^T] = A E[XX^T] A^T = A \\Sigma_X A^T$.\nSo we must find a matrix $A$ satisfying the equation:\n$$\nA \\Sigma_X A^T = I\n$$\nThis is a whitening transformation. One systematic way to find such a matrix $A$ is through the Cholesky decomposition of $\\Sigma_X$. Let $\\Sigma_X = LL^T$, where $L$ is a lower-triangular matrix with positive diagonal entries. Then we can choose $A = L^{-1}$. With this choice, we have:\n$$\nA \\Sigma_X A^T = (L^{-1}) (LL^T) (L^{-1})^T = (L^{-1}L)(L^T(L^{-1})^T) = I (L^{-1}L)^T = I(I^T) = I\n$$\nLet's simplify notation for the derivation. Let $a = \\int_0^T f(t)^2 dt$, $b = \\int_0^T f(t)g(t) dt$, and $c = \\int_0^T g(t)^2 dt$. So, $\\Sigma_X = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$. We seek $L = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}$ such that $LL^T=\\Sigma_X$.\n$$\n\\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}\n\\begin{pmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{pmatrix}\n=\n\\begin{pmatrix} l_{11}^2 & l_{11}l_{21} \\\\ l_{11}l_{21} & l_{21}^2 + l_{22}^2 \\end{pmatrix}\n=\n\\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}\n$$\nSolving for the entries of $L$:\n$l_{11}^2 = a \\implies l_{11} = \\sqrt{a}$\n$l_{11}l_{21} = b \\implies l_{21} = b/\\sqrt{a}$\n$l_{21}^2 + l_{22}^2 = c \\implies l_{22}^2 = c - l_{21}^2 = c - b^2/a = (ac - b^2)/a \\implies l_{22} = \\sqrt{(ac-b^2)/a}$\nSo the Cholesky factor is:\n$$\nL =\n\\begin{pmatrix}\n\\sqrt{a} & 0 \\\\\n\\frac{b}{\\sqrt{a}} & \\frac{\\sqrt{ac - b^2}}{\\sqrt{a}}\n\\end{pmatrix}\n$$\nNow we compute the inverse matrix $A = L^{-1}$. For a generic $2 \\times 2$ lower triangular matrix $\\begin{pmatrix} p & 0 \\\\ q & r \\end{pmatrix}$, the inverse is $\\frac{1}{pr}\\begin{pmatrix} r & 0 \\\\ -q & p \\end{pmatrix} = \\begin{pmatrix} 1/p & 0 \\\\ -q/(pr) & 1/r \\end{pmatrix}$.\nApplying this to $L$:\n$$\nA = L^{-1} =\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{a}} & 0 \\\\\n-\\frac{b/\\sqrt{a}}{\\sqrt{a} \\frac{\\sqrt{ac-b^2}}{\\sqrt{a}}} & \\frac{\\sqrt{a}}{\\sqrt{ac-b^2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{a}} & 0 \\\\\n-\\frac{b}{\\sqrt{ac-b^2}} & \\frac{\\sqrt{a}}{\\sqrt{ac-b^2}}\n\\end{pmatrix}\n$$\nThe problem states that $f$ and $g$ are not proportional, which guarantees that $ac-b^2 > 0$.\n\nSubstituting back the integral expressions for $a$, $b$, and $c$, we obtain the explicit matrix $A$. For clarity, let $D = ac-b^2 = \\left(\\int_{0}^{T} f(t)^2 dt\\right)\\left(\\int_{0}^{T} g(t)^2 dt\\right) - \\left(\\int_{0}^{T} f(t)g(t) dt\\right)^2$.\nThen the matrix $A$ is:\n$$\nA =\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{\\int_{0}^{T} f(t)^2 dt}} & 0 \\\\\n-\\frac{\\int_{0}^{T} f(t)g(t) dt}{\\sqrt{D}} & \\frac{\\sqrt{\\int_{0}^{T} f(t)^2 dt}}{\\sqrt{D}}\n\\end{pmatrix}\n$$\nWe can factor out common terms to match the format of the boxed answer.\n$$\nA = \\frac{1}{\\sqrt{D \\int_0^T f(t)^2 dt}}\n\\begin{pmatrix}\n\\sqrt{D} & 0 \\\\\n-\\int_0^T f(t)g(t) dt & \\int_0^T f(t)^2 dt\n\\end{pmatrix}\n$$\nBy construction, the covariance matrix of $Z=AX$ is $\\Sigma_Z = A \\Sigma_X A^T = I$. Since $Z$ is a jointly Gaussian vector with zero mean and identity covariance matrix, its components $Z_1$ and $Z_2$ are independent standard normal random variables, as required.", "answer": "$$\n\\boxed{\n\\frac{1}{\\sqrt{\\left(\\int_{0}^{T} f(t)^2 dt\\right) \\left[ \\left(\\int_{0}^{T} f(t)^2 dt\\right)\\left(\\int_{0}^{T} g(t)^2 dt\\right) - \\left(\\int_{0}^{T} f(t)g(t) dt\\right)^2 \\right]}}\n\\begin{pmatrix}\n\\sqrt{\\left(\\int_{0}^{T} f(t)^2 dt\\right)\\left(\\int_{0}^{T} g(t)^2 dt\\right) - \\left(\\int_{0}^{T} f(t)g(t) dt\\right)^2} & 0 \\\\\n-\\int_{0}^{T} f(t)g(t) dt & \\int_{0}^{T} f(t)^2 dt\n\\end{pmatrix}\n}\n$$", "id": "3068176"}, {"introduction": "How do we define a 'high-probability' region for a multidimensional Gaussian distribution? Simple circles or boxes don't account for the correlations and differing variances between components. This exercise introduces the Mahalanobis distance, the natural metric for Gaussian vectors, and asks you to prove that the quadratic form $(X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)$ follows a chi-squared distribution. This powerful result allows us to define and calculate the probability of confidence ellipsoids, a fundamental technique in statistics and machine learning.", "problem": "Consider a $d$-dimensional Gaussian random vector $X$ that arises as the stationary state of a stable linear stochastic differential equation (SDE), so that $X \\sim \\mathcal{N}_{d}(\\mu,\\Sigma)$ with mean vector $\\mu \\in \\mathbb{R}^{d}$ and a symmetric positive definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. For a fixed threshold $c>0$, define the ellipsoid \n$$\\mathcal{E}_{c}=\\{x \\in \\mathbb{R}^{d}:(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)\\le c\\}.$$\nStarting only from core definitions of the multivariate normal distribution and basic properties of linear transformations, derive from first principles a closed-form analytic expression, without unevaluated integrals, for the probability content \n$$\\mathbb{P}\\big(X \\in \\mathcal{E}_{c}\\big)=\\mathbb{P}\\big((X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)\\le c\\big),$$\nexpressed purely in terms of $d$ and $c$ using the Gamma function. Your final answer must be a single closed-form expression. Do not perform numerical approximations.", "solution": "The problem is to compute the probability $\\mathbb{P}\\big((X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)\\le c\\big)$. The strategy is to transform the random vector $X$ into a standard normal vector, which simplifies the quadratic form into a sum of squares.\n\nFirst, we perform a standardization of the random vector $X$. Let $Z$ be a random vector defined as $Z = X - \\mu$. Since $X$ is a Gaussian random vector, any affine transformation of $X$ is also Gaussian. The mean of $Z$ is\n$$ \\mathbb{E}[Z] = \\mathbb{E}[X - \\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0. $$\nThe covariance matrix of $Z$ is $\\text{Cov}(Z) = \\text{Cov}(X - \\mu) = \\text{Cov}(X) = \\Sigma$. Therefore, $Z$ follows a centered multivariate normal distribution, $Z \\sim \\mathcal{N}_{d}(0, \\Sigma)$. The probability we wish to compute can be expressed in terms of $Z$ as\n$$ \\mathbb{P}(Z^{\\top}\\Sigma^{-1}Z \\le c). $$\nThe quadratic form $Z^{\\top}\\Sigma^{-1}Z$ suggests a whitening transformation. Since $\\Sigma$ is a symmetric positive definite matrix, its inverse $\\Sigma^{-1}$ is also symmetric positive definite. There exists a unique symmetric positive definite matrix, denoted $\\Sigma^{-1/2}$, such that $(\\Sigma^{-1/2})^2 = \\Sigma^{-1}$. We define a new random vector $U$ by the linear transformation\n$$ U = \\Sigma^{-1/2} Z. $$\nSince $Z$ is a Gaussian vector, $U$ is also a Gaussian vector. Its mean is $\\mathbb{E}[U] = \\Sigma^{-1/2} \\mathbb{E}[Z] = 0$. Its covariance matrix is\n$$ \\text{Cov}(U) = \\Sigma^{-1/2} \\text{Cov}(Z) (\\Sigma^{-1/2})^{\\top} = \\Sigma^{-1/2} \\Sigma (\\Sigma^{-1/2}) = I_d. $$\nSo, the random vector $U$ follows a standard multivariate normal distribution, $U \\sim \\mathcal{N}_{d}(0, I_d)$. This implies that the components of $U$, denoted $U_1, \\dots, U_d$, are independent and identically distributed standard normal random variables, $U_i \\sim \\mathcal{N}(0, 1)$.\n\nNow, let's express the quadratic form in terms of $U$.\n$$ Z^{\\top}\\Sigma^{-1}Z = Z^{\\top}(\\Sigma^{-1/2}\\Sigma^{-1/2})Z = (\\Sigma^{-1/2}Z)^{\\top}(\\Sigma^{-1/2}Z) = U^{\\top}U. $$\nThe term $U^{\\top}U$ is the sum of the squares of the components of $U$:\n$$ U^{\\top}U = \\sum_{i=1}^{d} U_i^2. $$\nLet the random variable $Y = (X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)$. We have shown that $Y = \\sum_{i=1}^{d} U_i^2$, where $U_i$ are independent $\\mathcal{N}(0,1)$ random variables. By definition, a random variable that is the sum of the squares of $d$ independent standard normal variables follows a chi-squared distribution with $d$ degrees of freedom, denoted $Y \\sim \\chi^2_d$.\n\nThe problem is now reduced to finding $\\mathbb{P}(Y \\le c)$ where $Y \\sim \\chi^2_d$. This requires the cumulative distribution function (CDF) of the $\\chi^2_d$ distribution. The probability density function (PDF) of $Y \\sim \\chi^2_d$ is given by\n$$ f_Y(y) = \\frac{y^{(d/2) - 1} e^{-y/2}}{2^{d/2} \\Gamma(d/2)}, \\quad \\text{for } y > 0, $$\nwhere $\\Gamma(s) = \\int_0^\\infty t^{s-1} e^{-t} dt$ is the Gamma function.\nThe probability $\\mathbb{P}(Y \\le c)$ is the CDF of $Y$ evaluated at $c$, found by integrating the PDF from $0$ to $c$:\n$$ \\mathbb{P}(Y \\le c) = \\int_0^c \\frac{y^{(d/2) - 1} e^{-y/2}}{2^{d/2} \\Gamma(d/2)} dy. $$\nTo evaluate this integral, we perform a change of variables. Let $t = y/2$. Then $y = 2t$ and $dy = 2dt$. The limits of integration change from $y=0$ to $t=0$ and from $y=c$ to $t=c/2$.\n$$ \\mathbb{P}(Y \\le c) = \\frac{1}{2^{d/2} \\Gamma(d/2)} \\int_0^{c/2} (2t)^{(d/2) - 1} e^{-t} (2dt) $$\n$$ = \\frac{2^{(d/2) - 1} \\cdot 2}{2^{d/2} \\Gamma(d/2)} \\int_0^{c/2} t^{(d/2) - 1} e^{-t} dt $$\n$$ = \\frac{1}{\\Gamma(d/2)} \\int_0^{c/2} t^{d/2 - 1} e^{-t} dt. $$\nThe integral in this final expression is the definition of the lower incomplete gamma function, $\\gamma(s, x)$, defined as\n$$ \\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt. $$\nIn our case, the shape parameter is $s = d/2$ and the upper limit of integration is $x = c/2$. Therefore, the probability is\n$$ \\mathbb{P}(Y \\le c) = \\frac{\\gamma(d/2, c/2)}{\\Gamma(d/2)}. $$\nThis expression is the regularized lower incomplete gamma function, which provides the required closed-form analytic expression for the probability.", "answer": "$$\\boxed{\\frac{\\gamma\\left(\\frac{d}{2}, \\frac{c}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}\\right)}}$$", "id": "3068167"}]}