{"hands_on_practices": [{"introduction": "The Law of Large Numbers often materializes through a direct, hands-on calculation. This exercise provides a foundational example by examining the quadratic variation of Brownian motion, a central concept in stochastic calculus. By calculating the mean and variance of the sum of squared increments and applying Chebyshev’s inequality, you will rigorously prove convergence in probability—the essence of the Weak Law of Large Numbers [@problem_id:3083233]. This practice solidifies the connection between the statistical properties of a process and its long-term limiting behavior.", "problem": "Let $W = (W_{t})_{t \\geq 0}$ be a standard Brownian motion, meaning a continuous-time stochastic process with $W_{0} = 0$, independent increments, and for any $0 \\leq s  t$, the increment $W_{t} - W_{s}$ is Gaussian with mean $0$ and variance $t - s$. Fix a time $t  0$ and, for each integer $n \\geq 1$, consider the equal partition $t_{k} = \\frac{k t}{n}$ for $k = 0, 1, \\dots, n$. Define the quadratic variation approximation\n$$\nS_{n} = \\sum_{k=1}^{n} \\left(W_{t_{k}} - W_{t_{k-1}}\\right)^{2}.\n$$\nUsing only the defining properties of Brownian motion and Chebyshev’s inequality, compute the mean and variance of $S_{n}$, and deduce the limiting value of $S_{n}$ in probability as $n \\to \\infty$. Express your final answer as a single closed-form symbolic expression.", "solution": "The problem is to compute the mean and variance of the quadratic variation approximation $S_{n}$ and then find its limit in probability as $n \\to \\infty$.\n\nFirst, we validate the problem statement.\nThe givens are:\n1. $W = (W_{t})_{t \\geq 0}$ is a standard Brownian motion.\n2. $W_{0} = 0$.\n3. The increments are independent. For $0 \\leq s_1  t_1 \\leq s_2  t_2$, the random variables $W_{t_1} - W_{s_1}$ and $W_{t_2} - W_{s_2}$ are independent.\n4. For any $0 \\leq s  t$, the increment $W_{t} - W_{s}$ is a Gaussian random variable with mean $0$ and variance $t-s$, denoted as $W_{t} - W_{s} \\sim \\mathcal{N}(0, t-s)$.\n5. A fixed time $t  0$.\n6. A partition $t_{k} = \\frac{k t}{n}$ for $k = 0, 1, \\dots, n$ and integer $n \\geq 1$.\n7. The quadratic variation approximation $S_{n} = \\sum_{k=1}^{n} \\left(W_{t_{k}} - W_{t_{k-1}}\\right)^{2}$.\n8. The task is to compute the mean and variance of $S_{n}$ and find its limit in probability using only the defining properties of Brownian motion and Chebyshev’s inequality.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard and fundamental exercise in the theory of stochastic processes. All terms are defined, and the premises are consistent. The problem is valid.\n\nLet's begin the solution.\nLet $\\Delta W_{k}$ denote the increment of the Brownian motion over the $k$-th interval of the partition:\n$$\n\\Delta W_{k} = W_{t_{k}} - W_{t_{k-1}}\n$$\nThe duration of each interval is $\\Delta t_{k} = t_{k} - t_{k-1} = \\frac{k t}{n} - \\frac{(k-1)t}{n} = \\frac{t}{n}$.\nAccording to the properties of Brownian motion, each increment $\\Delta W_{k}$ is a normally distributed random variable with mean $0$ and variance $\\Delta t_{k} = \\frac{t}{n}$. That is,\n$$\n\\Delta W_{k} \\sim \\mathcal{N}\\left(0, \\frac{t}{n}\\right)\n$$\nThe quadratic variation approximation is given by $S_{n} = \\sum_{k=1}^{n} (\\Delta W_{k})^{2}$.\n\n**1. Mean of $S_{n}$**\n\nWe compute the expectation of $S_{n}$. By the linearity of expectation:\n$$\nE[S_{n}] = E\\left[\\sum_{k=1}^{n} (\\Delta W_{k})^{2}\\right] = \\sum_{k=1}^{n} E\\left[(\\Delta W_{k})^{2}\\right]\n$$\nFor any random variable $X$ with mean $\\mu$ and variance $\\sigma^{2}$, we have $\\mathrm{Var}(X) = E[X^{2}] - (E[X])^{2}$.\nThis gives $E[X^{2}] = \\mathrm{Var}(X) + (E[X])^{2}$.\nFor the increment $\\Delta W_{k}$, the mean is $E[\\Delta W_{k}] = 0$ and the variance is $\\mathrm{Var}(\\Delta W_{k}) = \\frac{t}{n}$.\nSo, the second moment is:\n$$\nE\\left[(\\Delta W_{k})^{2}\\right] = \\mathrm{Var}(\\Delta W_{k}) + \\left(E[\\Delta W_{k}]\\right)^{2} = \\frac{t}{n} + 0^{2} = \\frac{t}{n}\n$$\nSince this holds for each $k = 1, \\dots, n$, we can substitute this back into the expression for $E[S_{n}]$:\n$$\nE[S_{n}] = \\sum_{k=1}^{n} \\frac{t}{n} = n \\cdot \\frac{t}{n} = t\n$$\nThus, the mean of $S_{n}$ is $t$.\n\n**2. Variance of $S_{n}$**\n\nNext, we compute the variance of $S_{n}$.\n$$\n\\mathrm{Var}(S_{n}) = \\mathrm{Var}\\left(\\sum_{k=1}^{n} (\\Delta W_{k})^{2}\\right)\n$$\nThe time intervals $[t_{k-1}, t_{k}]$ are non-overlapping for different $k$. By the property of independent increments of Brownian motion, the random variables $\\Delta W_{1}, \\Delta W_{2}, \\dots, \\Delta W_{n}$ are mutually independent. Consequently, their squares, $(\\Delta W_{1})^{2}, (\\Delta W_{2})^{2}, \\dots, (\\Delta W_{n})^{2}$, are also mutually independent random variables.\nFor a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(S_{n}) = \\sum_{k=1}^{n} \\mathrm{Var}\\left((\\Delta W_{k})^{2}\\right)\n$$\nWe need to compute $\\mathrm{Var}\\left((\\Delta W_{k})^{2}\\right)$. For any random variable $Y$, $\\mathrm{Var}(Y) = E[Y^{2}] - (E[Y])^{2}$. Let $Y = (\\Delta W_{k})^{2}$. Then:\n$$\n\\mathrm{Var}\\left((\\Delta W_{k})^{2}\\right) = E\\left[\\left((\\Delta W_{k})^{2}\\right)^{2}\\right] - \\left(E\\left[(\\Delta W_{k})^{2}\\right]\\right)^{2} = E\\left[(\\Delta W_{k})^{4}\\right] - \\left(\\frac{t}{n}\\right)^{2}\n$$\nWe need the fourth moment of $\\Delta W_{k} \\sim \\mathcal{N}\\left(0, \\frac{t}{n}\\right)$. For a centered normal random variable $X \\sim \\mathcal{N}(0, \\sigma^{2})$, the fourth moment is $E[X^{4}] = 3\\sigma^{4}$.\nIn our case, $\\sigma^{2} = \\frac{t}{n}$, so:\n$$\nE\\left[(\\Delta W_{k})^{4}\\right] = 3 \\left(\\frac{t}{n}\\right)^{2} = \\frac{3t^{2}}{n^{2}}\n$$\nSubstituting this into the expression for the variance of $(\\Delta W_{k})^{2}$:\n$$\n\\mathrm{Var}\\left((\\Delta W_{k})^{2}\\right) = \\frac{3t^{2}}{n^{2}} - \\left(\\frac{t}{n}\\right)^{2} = \\frac{2t^{2}}{n^{2}}\n$$\nThis variance is the same for all $k = 1, \\dots, n$. Now we can compute the variance of $S_{n}$:\n$$\n\\mathrm{Var}(S_{n}) = \\sum_{k=1}^{n} \\frac{2t^{2}}{n^{2}} = n \\cdot \\frac{2t^{2}}{n^{2}} = \\frac{2t^{2}}{n}\n$$\nThus, the variance of $S_{n}$ is $\\frac{2t^{2}}{n}$.\n\n**3. Limiting value of $S_{n}$ in probability**\n\nWe are asked to deduce the limiting value of $S_{n}$ in probability as $n \\to \\infty$ using Chebyshev’s inequality. A sequence of random variables $X_{n}$ converges in probability to a constant $c$ if for every $\\epsilon  0$,\n$$\n\\lim_{n \\to \\infty} P(|X_{n} - c| \\geq \\epsilon) = 0\n$$\nChebyshev's inequality states that for a random variable $X$ with finite mean $\\mu$ and finite variance $\\sigma^{2}$, and for any constant $k  0$:\n$$\nP(|X - \\mu| \\geq k) \\leq \\frac{\\sigma^{2}}{k^{2}}\n$$\nWe apply this inequality to $S_{n}$. We found that its mean is $E[S_{n}] = t$ and its variance is $\\mathrm{Var}(S_{n}) = \\frac{2t^{2}}{n}$. For any arbitrary $\\epsilon  0$, we have:\n$$\nP(|S_{n} - t| \\geq \\epsilon) \\leq \\frac{\\mathrm{Var}(S_{n})}{\\epsilon^{2}}\n$$\nSubstituting the expression for the variance:\n$$\nP(|S_{n} - t| \\geq \\epsilon) \\leq \\frac{2t^{2}/n}{\\epsilon^{2}} = \\frac{2t^{2}}{n\\epsilon^{2}}\n$$\nNow we take the limit as $n \\to \\infty$. Since $t$ and $\\epsilon$ are fixed positive constants:\n$$\n\\lim_{n \\to \\infty} \\frac{2t^{2}}{n\\epsilon^{2}} = 0\n$$\nSince probability is non-negative, we have the squeeze theorem:\n$$\n0 \\leq \\lim_{n \\to \\infty} P(|S_{n} - t| \\geq \\epsilon) \\leq 0\n$$\nThis implies that $\\lim_{n \\to \\infty} P(|S_{n} - t| \\geq \\epsilon) = 0$. By the definition of convergence in probability, $S_{n}$ converges in probability to $t$. This is often written as $S_{n} \\xrightarrow{P} t$.\n\nThe limiting value of $S_{n}$ in probability as $n \\to \\infty$ is $t$. This result establishes that the quadratic variation of a standard Brownian motion over the interval $[0, t]$ is equal to $t$.", "answer": "$$\\boxed{t}$$", "id": "3083233"}, {"introduction": "Beyond sums of independent variables, the Laws of Large Numbers are crucial for understanding ergodic systems, where time averages converge to ensemble averages. This problem explores this principle through the Ornstein-Uhlenbeck process, a fundamental model for mean-reverting systems in physics, finance, and biology [@problem_id:3083237]. Deriving the variance of the time-averaged process and its asymptotic decay rate provides a concrete understanding of how quickly the process \"forgets\" its past, connecting the abstract law to the physical concept of a spectral gap.", "problem": "Consider the Ornstein–Uhlenbeck (OU) process $X_t$ defined as the unique strong solution to the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t \\,=\\, -\\gamma\\, X_t\\, \\mathrm{d}t \\,+\\, \\sigma\\, \\mathrm{d}W_t,\n$$\nwhere $\\gamma0$ is the mean-reversion rate, $\\sigma0$ is the noise amplitude, and $W_t$ is a standard one-dimensional Wiener process. Assume $X_0$ is distributed according to the stationary distribution of the OU process so that $\\{X_t\\}_{t\\ge 0}$ is strictly stationary with mean zero.\n\nStarting only from the SDE, core definitions of covariance for a stationary process, and well-tested facts about Gaussian processes, derive an exact expression for\n$$\n\\mathrm{Var}\\!\\left(\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right)\n$$\nfor finite $T0$. Then, determine the asymptotic behavior as $T\\to\\infty$ and identify a constant $C$ such that\n$$\n\\mathrm{Var}\\!\\left(\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right) \\sim \\frac{C}{T}.\n$$\nExpress $C$ explicitly in terms of $\\gamma$ and $\\sigma$, and explain how the decay rate connects to the spectral gap of the OU generator.\n\nYour final answer must be the single analytic expression for $C$. No numerical evaluation is required.", "solution": "The objective is to derive an expression for the variance of the time-averaged Ornstein–Uhlenbeck (OU) process, analyze its asymptotic behavior for large time intervals, and relate the result to the spectral properties of the process generator.\n\nThe OU process $X_t$ is described by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = -\\gamma X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nwith $\\gamma  0$ and $\\sigma  0$. We are given that the process $\\{X_t\\}_{t \\ge 0}$ is strictly stationary, with $X_0$ drawn from the stationary distribution. For an OU process, the stationary distribution is a Gaussian distribution with mean $0$ and variance $\\frac{\\sigma^2}{2\\gamma}$. This implies that for any time $t \\ge 0$, $\\mathbb{E}[X_t] = 0$ and $\\mathrm{Var}(X_t) = \\mathbb{E}[X_t^2] = \\frac{\\sigma^2}{2\\gamma}$.\n\nLet $Y_T = \\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t$. We wish to compute $\\mathrm{Var}(Y_T)$.\nFirst, we find the mean of $Y_T$:\n$$\n\\mathbb{E}[Y_T] = \\mathbb{E}\\left[\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right] = \\frac{1}{T}\\int_0^T \\mathbb{E}[X_t]\\, \\mathrm{d}t = \\frac{1}{T}\\int_0^T 0\\, \\mathrm{d}t = 0\n$$\nSince the mean is zero, the variance is equal to the second moment:\n$$\n\\mathrm{Var}(Y_T) = \\mathbb{E}[Y_T^2] = \\mathbb{E}\\left[ \\left(\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right)^2 \\right] = \\frac{1}{T^2} \\mathbb{E}\\left[ \\int_0^T X_t\\, \\mathrm{d}t \\int_0^T X_s\\, \\mathrm{d}s \\right]\n$$\nUsing Fubini's theorem, we can interchange the expectation and the integrals:\n$$\n\\mathrm{Var}(Y_T) = \\frac{1}{T^2} \\int_0^T \\int_0^T \\mathbb{E}[X_t X_s]\\, \\mathrm{d}s\\, \\mathrm{d}t\n$$\nThe term $\\mathbb{E}[X_t X_s]$ is the autocovariance function of the process, which we denote by $R(t,s) = \\mathrm{Cov}(X_t, X_s)$. Since the process is stationary, this function depends only on the time difference $\\tau = t-s$, so we write $R(\\tau) = \\mathbb{E}[X_u X_{u+\\tau}]$ for any $u$.\n\nTo find $R(\\tau)$, we use the formal solution of the OU SDE. For $t  s$, the solution is:\n$$\nX_t = X_s e^{-\\gamma(t-s)} + \\sigma \\int_s^t e^{-\\gamma(t-u)}\\, \\mathrm{d}W_u\n$$\nNow we compute $\\mathbb{E}[X_s X_t]$ for $t  s$:\n$$\n\\mathbb{E}[X_s X_t] = \\mathbb{E}\\left[X_s \\left(X_s e^{-\\gamma(t-s)} + \\sigma \\int_s^t e^{-\\gamma(t-u)}\\, \\mathrm{d}W_u\\right)\\right]\n$$\n$$\n\\mathbb{E}[X_s X_t] = \\mathbb{E}[X_s^2] e^{-\\gamma(t-s)} + \\sigma \\mathbb{E}\\left[X_s \\int_s^t e^{-\\gamma(t-u)}\\, \\mathrm{d}W_u\\right]\n$$\nThe random variable $X_s$ is $\\mathcal{F}_s$-measurable, meaning it depends on the history of the Wiener process up to time $s$. The Itō integral $\\int_s^t \\dots \\mathrm{d}W_u$ depends on the increments of the Wiener process for times $u \\in [s, t]$, which are independent of the filtration $\\mathcal{F}_s$. Therefore, $X_s$ and the integral term are independent. Since the Itō integral has zero mean, the expectation of their product is zero:\n$$\n\\mathbb{E}\\left[X_s \\int_s^t e^{-\\gamma(t-u)}\\, \\mathrm{d}W_u\\right] = \\mathbb{E}[X_s] \\mathbb{E}\\left[\\int_s^t e^{-\\gamma(t-u)}\\, \\mathrm{d}W_u\\right] = 0 \\cdot 0 = 0\n$$\nThus, for $ts$, we have:\n$$\n\\mathbb{E}[X_s X_t] = \\mathbb{E}[X_s^2] e^{-\\gamma(t-s)}\n$$\nUsing the stationary variance $\\mathbb{E}[X_s^2] = \\frac{\\sigma^2}{2\\gamma}$ and setting $\\tau = t-s  0$:\n$$\nR(\\tau) = \\frac{\\sigma^2}{2\\gamma} e^{-\\gamma \\tau}\n$$\nSince the autocovariance function must be an even function, $R(\\tau) = R(-\\tau)$, we can write the general form for any $\\tau \\in \\mathbb{R}$:\n$$\nR(\\tau) = \\frac{\\sigma^2}{2\\gamma} e^{-\\gamma |\\tau|}\n$$\nNow we substitute this back into the expression for the variance:\n$$\n\\mathrm{Var}(Y_T) = \\frac{1}{T^2} \\int_0^T \\int_0^T \\frac{\\sigma^2}{2\\gamma} e^{-\\gamma|t-s|}\\, \\mathrm{d}s\\, \\mathrm{d}t = \\frac{\\sigma^2}{2\\gamma T^2} \\int_0^T \\int_0^T e^{-\\gamma|t-s|}\\, \\mathrm{d}s\\, \\mathrm{d}t\n$$\nWe evaluate the double integral. Due to the symmetry of the integrand and the domain of integration (a square $[0,T]\\times[0,T]$), the integral over the region $ts$ is equal to the integral over the region $st$. We can therefore write:\n$$\n\\int_0^T \\int_0^T e^{-\\gamma|t-s|}\\, \\mathrm{d}s\\, \\mathrm{d}t = 2 \\int_0^T \\left( \\int_0^t e^{-\\gamma(t-s)}\\, \\mathrm{d}s \\right) \\mathrm{d}t\n$$\nFirst, we evaluate the inner integral with respect to $s$:\n$$\n\\int_0^t e^{-\\gamma(t-s)}\\, \\mathrm{d}s = e^{-\\gamma t} \\int_0^t e^{\\gamma s}\\, \\mathrm{d}s = e^{-\\gamma t} \\left[ \\frac{1}{\\gamma}e^{\\gamma s} \\right]_0^t = \\frac{e^{-\\gamma t}}{\\gamma}(e^{\\gamma t} - 1) = \\frac{1}{\\gamma}(1 - e^{-\\gamma t})\n$$\nNext, we evaluate the outer integral with respect to $t$:\n$$\n2 \\int_0^T \\frac{1}{\\gamma}(1 - e^{-\\gamma t})\\, \\mathrm{d}t = \\frac{2}{\\gamma} \\left[ t + \\frac{1}{\\gamma}e^{-\\gamma t} \\right]_0^T = \\frac{2}{\\gamma} \\left( \\left(T + \\frac{1}{\\gamma}e^{-\\gamma T}\\right) - \\left(0 + \\frac{1}{\\gamma}e^0\\right) \\right)\n$$\n$$\n= \\frac{2}{\\gamma} \\left( T - \\frac{1-e^{-\\gamma T}}{\\gamma} \\right) = \\frac{2}{\\gamma^2} (\\gamma T - 1 + e^{-\\gamma T})\n$$\nSubstituting this result back into the expression for the variance:\n$$\n\\mathrm{Var}(Y_T) = \\frac{\\sigma^2}{2\\gamma T^2} \\cdot \\frac{2}{\\gamma^2} (\\gamma T - 1 + e^{-\\gamma T}) = \\frac{\\sigma^2}{\\gamma^3 T^2} (\\gamma T - 1 + e^{-\\gamma T})\n$$\nThis can be rewritten as:\n$$\n\\mathrm{Var}\\!\\left(\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right) = \\frac{\\sigma^2}{\\gamma^2 T^2} \\left( T - \\frac{1 - e^{-\\gamma T}}{\\gamma} \\right)\n$$\nThis is the exact expression for the variance for finite $T0$.\n\nNext, we determine the asymptotic behavior as $T\\to\\infty$. Since $\\gamma0$, the term $e^{-\\gamma T}$ approaches $0$.\n$$\n\\mathrm{Var}(Y_T) = \\frac{\\sigma^2}{\\gamma^2 T^2} \\left( T - \\frac{1}{\\gamma} + O(e^{-\\gamma T}) \\right) = \\frac{\\sigma^2}{\\gamma^2 T} - \\frac{\\sigma^2}{\\gamma^3 T^2} + O\\left(\\frac{e^{-\\gamma T}}{T^2}\\right)\n$$\nFor large $T$, the dominant term is proportional to $T^{-1}$. Thus, the asymptotic behavior is:\n$$\n\\mathrm{Var}\\!\\left(\\frac{1}{T}\\int_0^T X_t\\, \\mathrm{d}t\\right) \\sim \\frac{\\sigma^2}{\\gamma^2 T}\n$$\nComparing this to the specified form $\\frac{C}{T}$, we identify the constant $C$ as:\n$$\nC = \\frac{\\sigma^2}{\\gamma^2}\n$$\nThis constant can also be found using the Green-Kubo formula, which relates the asymptotic variance to the integral of the autocovariance function:\n$$\nC = \\lim_{T\\to\\infty} T \\cdot \\mathrm{Var}(Y_T) = \\int_{-\\infty}^{\\infty} R(\\tau)\\, \\mathrm{d}\\tau\n$$\n$$\nC = \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2}{2\\gamma} e^{-\\gamma|\\tau|}\\, \\mathrm{d}\\tau = 2 \\int_0^{\\infty} \\frac{\\sigma^2}{2\\gamma} e^{-\\gamma\\tau}\\, \\mathrm{d}\\tau = \\frac{\\sigma^2}{\\gamma} \\left[ -\\frac{1}{\\gamma}e^{-\\gamma\\tau} \\right]_0^\\infty = \\frac{\\sigma^2}{\\gamma} \\left( 0 - \\left(-\\frac{1}{\\gamma}\\right) \\right) = \\frac{\\sigma^2}{\\gamma^2}\n$$\nThis confirms our result.\n\nFinally, we explain the connection to the spectral gap of the OU generator. The infinitesimal generator of the OU process is the operator $\\mathcal{L} = -\\gamma x \\frac{d}{dx} + \\frac{\\sigma^2}{2} \\frac{d^2}{dx^2}$. This operator is self-adjoint in the Hilbert space $L^2(\\mu)$, where $\\mu$ is the invariant (stationary) measure. The spectrum of the negative operator $-\\mathcal{L}$ is discrete and given by the set $\\{n\\gamma\\}_{n=0,1,2,\\dots}$. The smallest non-zero eigenvalue, $\\lambda_1 = \\gamma$, is known as the spectral gap.\nThe spectral gap governs the rate of convergence of the process to its stationary distribution. Specifically, the autocovariance function of any observable $f(X_t)$ with $\\mathbb{E}[f(X_t)]=0$ decays at a rate determined by the spectral gap. For the observable $f(x)=x$, the autocovariance is $R(\\tau)$, which we found to be proportional to $e^{-\\gamma|\\tau|}$. The decay rate is precisely the spectral gap $\\gamma$.\nThe constant $C = \\int_{-\\infty}^{\\infty} R(\\tau)\\mathrm{d}\\tau = \\sigma^2/\\gamma^2$ is directly related to the integrated correlation time. A larger spectral gap $\\gamma$ implies a faster exponential decay of correlations. This means the process \"forgets\" its past more quickly, and samples $X_t$ and $X_{t+\\tau}$ become effectively independent over shorter time scales $\\tau$. Consequently, the time integral $\\int_0^T X_t \\mathrm{d}t$ benefits from more effective averaging, leading to a smaller variance for a given $T$. The inverse-square dependence $C \\propto 1/\\gamma^2$ quantifies this relationship: a faster decay of correlations (large $\\gamma$) leads to a much smaller asymptotic variance of the time average.", "answer": "$$\\boxed{\\frac{\\sigma^2}{\\gamma^2}}$$", "id": "3083237"}, {"introduction": "The distinction between the Weak and Strong Laws of Large Numbers often hinges on subtle differences in their underlying assumptions. This problem delves into these theoretical nuances by exploring the conditions required for convergence [@problem_id:3083239]. By considering a sequence of statistics generated from a numerical simulation, you will analyze the impact of using pairwise independence instead of the more restrictive mutual independence, leading to an appreciation of powerful results like Etemadi's Strong Law of Large Numbers.", "problem": "Consider the stochastic differential equation $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ on a finite horizon $[0,T]$, where $W_t$ is a standard Brownian motion and the coefficients $a(\\cdot)$ and $b(\\cdot)$ are globally Lipschitz with linear growth. Let $\\Delta t  0$ be a fixed time step and consider Euler–Maruyama discretizations run in $n$ separate simulation blocks indexed by $j \\in \\{1,\\dots,n\\}$, each of length $m \\in \\mathbb{N}$ steps:\n$$\nX_{k+1}^{(j)} = X_{k}^{(j)} + a\\!\\left(X_{k}^{(j)}\\right)\\,\\Delta t + b\\!\\left(X_{k}^{(j)}\\right)\\,\\Delta W_{j,k},\\quad k=0,1,\\dots,m-1,\n$$\nwith $X_{0}^{(j)}$ sampled from a common initial distribution $\\mu$ and $\\Delta W_{j,k} \\sim \\mathcal{N}(0,\\Delta t)$ the Brownian increments used in block $j$. For each block, define the block-averaged increment statistic\n$$\nS_j \\;=\\; \\frac{1}{m}\\sum_{k=1}^{m} \\psi\\!\\left(\\Delta X_{j,k}\\right),\\quad \\text{where }\\Delta X_{j,k} := X_{k}^{(j)} - X_{k-1}^{(j)},\n$$\nfor a measurable function $\\psi:\\mathbb{R}\\to\\mathbb{R}$ such that $\\mathbb{E}\\!\\left[\\,\\big|\\psi(\\Delta X_{j,k})\\big|\\,\\right]  \\infty$. Assume the following construction is enforced at the block level: for $j\\neq \\ell$, the pairs $(X_0^{(j)},\\{\\Delta W_{j,k}\\}_{k=1}^{m})$ and $(X_0^{(\\ell)},\\{\\Delta W_{\\ell,k}\\}_{k=1}^{m})$ are pairwise independent; blocks share the same law (so $\\{S_j\\}$ are identically distributed), but no assumption of mutual (collective) independence beyond pairs is made.\n\nDefine the global average\n$$\n\\overline{S}_n := \\frac{1}{n}\\sum_{j=1}^{n} S_j.\n$$\nWhich of the following statements are correct under the setting described?\n\nA. If $\\{S_j\\}_{j\\ge 1}$ are pairwise independent, identically distributed, and integrable (that is, $\\mathbb{E}[|S_1|]\\infty$), then $\\overline{S}_n \\to \\mathbb{E}[S_1]$ almost surely by Etemadi’s Strong Law of Large Numbers (SLLN).\n\nB. Pairwise independence is not sufficient for a strong law; mutual independence of all blocks is necessary even when $\\{S_j\\}$ are identically distributed and integrable.\n\nC. A finite second moment, $\\mathbb{E}[S_1^2]\\infty$, is necessary for a strong law in this setting.\n\nD. If the blocks are pairwise independent but not identically distributed, the strong law still holds provided $\\sup_{j\\ge 1}\\mathbb{E}[|S_j|]\\infty$.\n\nE. If the blocks are pairwise independent with common mean $\\mathbb{E}[S_j]=\\mu$ for all $j$, and uniformly bounded variances $\\sup_{j\\ge 1}\\mathrm{Var}(S_j)\\infty$, then $\\overline{S}_n \\to \\mu$ in probability, that is, the Weak Law of Large Numbers (WLLN) holds.", "solution": "The user wants me to validate the problem and then solve it.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $dX_t = a(X_t)\\,dt + b(X_t)\\,dW_t$ on $[0,T]$.\n- **Brownian Motion:** $W_t$ is a standard Brownian motion.\n- **Coefficients:** $a(\\cdot)$ and $b(\\cdot)$ are globally Lipschitz with linear growth.\n- **Discretization:** Euler–Maruyama scheme with time step $\\Delta t  0$.\n- **Simulation Blocks:** $n$ blocks, indexed by $j \\in \\{1,\\dots,n\\}$. Each block has $m$ steps.\n- **Numerical Scheme:** $X_{k+1}^{(j)} = X_{k}^{(j)} + a(X_{k}^{(j)})\\,\\Delta t + b(X_{k}^{(j)})\\,\\Delta W_{j,k}$ for $k=0,1,\\dots,m-1$.\n- **Initial Conditions:** $X_{0}^{(j)}$ are sampled from a common initial distribution $\\mu$.\n- **Increments:** $\\Delta W_{j,k} \\sim \\mathcal{N}(0,\\Delta t)$ are the Brownian increments.\n- **Block Statistic:** $S_j = \\frac{1}{m}\\sum_{k=1}^{m} \\psi(\\Delta X_{j,k})$, with $\\Delta X_{j,k} := X_{k}^{(j)} - X_{k-1}^{(j)}$.\n- **Function $\\psi$:** A measurable function $\\psi:\\mathbb{R}\\to\\mathbb{R}$ such that $\\mathbb{E}[|\\psi(\\Delta X_{j,k})|]  \\infty$.\n- **Independence Structure:** For $j\\neq \\ell$, the pairs of random vectors $(X_0^{(j)},\\{\\Delta W_{j,k}\\}_{k=1}^{m})$ and $(X_0^{(\\ell)},\\{\\Delta W_{\\ell,k}\\}_{k=1}^{m})$ are pairwise independent. This implies that the resulting statistics $\\{S_j\\}_{j=1}^n$ are pairwise independent.\n- **Distribution Structure:** Blocks share the same law, meaning the random variables $\\{S_j\\}$ are identically distributed.\n- **Global Average:** $\\overline{S}_n = \\frac{1}{n}\\sum_{j=1}^{n} S_j$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is grounded in the well-established fields of stochastic calculus, numerical analysis for SDEs, and probability theory. The SDE with Lipschitz coefficients is standard. The Euler-Maruyama method is a fundamental numerical scheme. The Law of Large Numbers (LLN) is a cornerstone of probability theory. The distinction between mutual and pairwise independence is a classical and important topic. All premises are scientifically sound.\n- **Well-Posedness:** The question asks to identify which of the given statements about the convergence of $\\overline{S}_n$ are correct. This is a well-defined mathematical problem. The provided information is sufficient to analyze the probabilistic properties of the sequence $\\{S_j\\}$ and apply the relevant convergence theorems.\n- **Objectivity:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjectivity.\n- **Flaw Analysis:**\n    1.  **Scientific/Factual Unsoundness:** None.\n    2.  **Non-Formalizable/Irrelevant:** The problem is formalizable and directly pertains to the LLNs.\n    3.  **Incomplete/Contradictory Setup:** The setup is complete and self-consistent. The explicit statement that \"no assumption of mutual (collective) independence beyond pairs is made\" is a crucial, non-contradictory piece of information that defines the scope of the problem.\n    4.  **Unrealistic/Infeasible:** The setup is a standard theoretical model in computational finance and physics. It is mathematically and computationally feasible.\n    5.  **Ill-Posed/Poorly Structured:** The problem is well-structured.\n    6.  **Pseudo-Profound/Trivial:** The problem is not trivial; it tests a nuanced understanding of the LLNs beyond the typical introductory case of mutually independent, identically distributed variables.\n    7.  **Outside Scientific Verifiability:** The statements can be rigorously proven or disproven using mathematical theorems.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the detailed solution and evaluation of the options.\n\n### Solution Derivation\nThe problem concerns the convergence of the sample mean $\\overline{S}_n$ of a sequence of random variables $\\{S_j\\}$. The core of the problem lies in the assumptions about this sequence: the variables are pairwise independent and identically distributed. The context of the SDE and its numerical simulation serves to generate these variables and ensure they are well-behaved (e.g., have finite moments). Let's analyze each statement.\n\n**A. If $\\{S_j\\}_{j\\ge 1}$ are pairwise independent, identically distributed, and integrable (that is, $\\mathbb{E}[|S_1|]\\infty$), then $\\overline{S}_n \\to \\mathbb{E}[S_1]$ almost surely by Etemadi’s Strong Law of Large Numbers (SLLN).**\n\nThe problem statement establishes that the blocks $\\{S_j\\}$ are pairwise independent and identically distributed. The integrability condition $\\mathbb{E}[|S_1|]  \\infty$ follows from the problem's assumption that $\\mathbb{E}[|\\psi(\\Delta X_{j,k})|]  \\infty$. Specifically,\n$$\n\\mathbb{E}[|S_1|] = \\mathbb{E}\\left[\\left| \\frac{1}{m}\\sum_{k=1}^{m} \\psi(\\Delta X_{1,k}) \\right|\\right] \\le \\frac{1}{m}\\sum_{k=1}^{m} \\mathbb{E}[|\\psi(\\Delta X_{1,k})|].\n$$\nSince $m$ is finite and each term in the sum is finite, $\\mathbb{E}[|S_1|]$ is finite.\nThe hypotheses of the statement are therefore satisfied by the problem's setup.\n\nThe statement invokes Etemadi's Strong Law of Large Numbers. This theorem, published by Nasrollah Etemadi in $1981$, states that for a sequence of pairwise independent, identically distributed (i.i.d.) random variables $\\{X_j\\}$ with $\\mathbb{E}[|X_1|]  \\infty$, the sample mean converges almost surely to the expected value:\n$$\n\\frac{1}{n}\\sum_{j=1}^{n} X_j \\xrightarrow{\\text{a.s.}} \\mathbb{E}[X_1] \\quad \\text{as } n \\to \\infty.\n$$\nThis theorem notably relaxes the condition of mutual independence from a full i.i.d. sequence (as required in Kolmogorov's SLLN) to pairwise independence.\n\nSince the sequence $\\{S_j\\}$ meets all the conditions of Etemadi's SLLN, the conclusion that $\\overline{S}_n \\to \\mathbb{E}[S_1]$ almost surely is correct.\n\nVerdict: **Correct**.\n\n**B. Pairwise independence is not sufficient for a strong law; mutual independence of all blocks is necessary even when $\\{S_j\\}$ are identically distributed and integrable.**\n\nThis statement is a direct contradiction of Etemadi's SLLN, which was discussed in the analysis of option A. Etemadi's theorem proves precisely that pairwise independence, along with the other two conditions (identically distributed and integrable), *is* sufficient for the Strong Law of Large Numbers. Therefore, mutual independence is not a necessary condition.\n\nVerdict: **Incorrect**.\n\n**C. A finite second moment, $\\mathbb{E}[S_1^2]\\infty$, is necessary for a strong law in this setting.**\n\nThis statement claims that a finite variance is a necessary condition for the SLLN. This is false. The necessary and sufficient condition for the SLLN to hold for a sequence of (mutually or pairwise) identically distributed random variables is a finite first absolute moment, i.e., $\\mathbb{E}[|S_1|]  \\infty$. While some proofs of the SLLN (particularly those for non-identically distributed variables or simpler proofs for the i.i.d. case) assume a finite second moment, it is a sufficient condition, not a necessary one. The definitive theorems from Kolmogorov and Etemadi establish the weaker requirement of a finite first moment.\n\nVerdict: **Incorrect**.\n\n**D. If the blocks are pairwise independent but not identically distributed, the strong law still holds provided $\\sup_{j\\ge 1}\\mathbb{E}[|S_j|]\\infty$.**\n\nThis statement proposes a version of the SLLN for non-identically distributed, pairwise independent variables. The condition given, a uniform bound on the first absolute moments, is not sufficient for the SLLN to hold. Even for mutually independent random variables, this condition is too weak. For instance, a SLLN for independent, non-identically distributed variables (e.g., Kolmogorov's general SLLN) typically requires a condition on the variances, such as $\\sum_{j=1}^{\\infty} \\frac{\\mathrm{Var}(S_j)}{j^2}  \\infty$. The simple requirement $\\sup_{j\\ge 1}\\mathbb{E}[|S_j|]\\infty$ does not guarantee convergence of the sample mean to the mean of the expectations. Etemadi's SLLN relies crucially on the 'identically distributed' assumption to function with only a first-moment condition.\n\nVerdict: **Incorrect**.\n\n**E. If the blocks are pairwise independent with common mean $\\mathbb{E}[S_j]=\\mu$ for all $j$, and uniformly bounded variances $\\sup_{j\\ge 1}\\mathrm{Var}(S_j)\\infty$, then $\\overline{S}_n \\to \\mu$ in probability, that is, the Weak Law of Large Numbers (WLLN) holds.**\n\nThis statement describes a version of the Weak Law of Large Numbers. Let us prove it from first principles. We need to show that for any $\\epsilon  0$, $P(|\\overline{S}_n - \\mu| \\ge \\epsilon) \\to 0$ as $n \\to \\infty$. We use Chebyshev's inequality, which states that for a random variable $Y$ with finite variance, $P(|Y - \\mathbb{E}[Y]| \\ge \\epsilon) \\le \\frac{\\mathrm{Var}(Y)}{\\epsilon^2}$.\n\nLet $Y = \\overline{S}_n$. First, we compute its expectation:\n$$\n\\mathbb{E}[\\overline{S}_n] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{j=1}^{n} S_j\\right] = \\frac{1}{n}\\sum_{j=1}^{n} \\mathbb{E}[S_j] = \\frac{1}{n}\\sum_{j=1}^{n} \\mu = \\mu.\n$$\nNext, we compute its variance. The variance of a sum of random variables is $\\mathrm{Var}(\\sum S_j) = \\sum \\mathrm{Var}(S_j) + \\sum_{i \\neq j} \\mathrm{Cov}(S_i, S_j)$. A key consequence of pairwise independence between $S_i$ and $S_j$ for $i \\neq j$ is that their covariance is zero: $\\mathrm{Cov}(S_i, S_j) = \\mathbb{E}[S_i S_j] - \\mathbb{E}[S_i]\\mathbb{E}[S_j] = 0$. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}\\left(\\sum_{j=1}^{n} S_j\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(S_j).\n$$\nThis allows us to compute the variance of the sample mean:\n$$\n\\mathrm{Var}(\\overline{S}_n) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{j=1}^{n} S_j\\right) = \\frac{1}{n^2}\\mathrm{Var}\\left(\\sum_{j=1}^{n} S_j\\right) = \\frac{1}{n^2}\\sum_{j=1}^{n} \\mathrm{Var}(S_j).\n$$\nThe statement assumes uniformly bounded variances, i.e., there exists a constant $C  \\infty$ such that $\\mathrm{Var}(S_j) \\le C$ for all $j$. Using this, we can bound the variance of $\\overline{S}_n$:\n$$\n\\mathrm{Var}(\\overline{S}_n) \\le \\frac{1}{n^2}\\sum_{j=1}^{n} C = \\frac{nC}{n^2} = \\frac{C}{n}.\n$$\nApplying Chebyshev's inequality:\n$$\nP(|\\overline{S}_n - \\mu| \\ge \\epsilon) \\le \\frac{\\mathrm{Var}(\\overline{S}_n)}{\\epsilon^2} \\le \\frac{C}{n\\epsilon^2}.\n$$\nAs $n \\to \\infty$, the upper bound $\\frac{C}{n\\epsilon^2}$ goes to $0$. By the squeeze theorem, $P(|\\overline{S}_n - \\mu| \\ge \\epsilon) \\to 0$. This is the definition of convergence in probability. The statement is a correct theorem.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{AE}$$", "id": "3083239"}]}