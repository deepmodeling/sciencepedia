{"hands_on_practices": [{"introduction": "This first exercise takes us back to the fundamentals. We will compute the Moment Generating Function ($M_X(t)$) and Characteristic Function ($\\phi_X(u)$) for a Gamma-distributed random variable directly from their integral definitions. This practice is essential for mastering the core mechanics of these transforms and for understanding the crucial difference in their domains of validity, a concept explored through the abscissa of convergence [@problem_id:3066890].", "problem": "Let $X$ be a positive random variable with the $\\mathrm{Gamma}(k,\\theta)$ distribution, with shape parameter $k>0$ and scale parameter $\\theta>0$. Its probability density function is\n$$\nf_X(x)=\\frac{1}{\\Gamma(k)\\,\\theta^{k}}\\,x^{k-1}\\,\\exp\\!\\left(-\\frac{x}{\\theta}\\right),\\quad x>0,\n$$\nwhere $\\Gamma(k)$ denotes the gamma function. Using only the definitions of the moment generating function $M_X(t)=\\mathbb{E}[\\exp(tX)]$ and the characteristic function $\\phi_X(u)=\\mathbb{E}[\\exp(iuX)]$, derive closed-form expressions for $M_X(t)$ and $\\phi_X(u)$ in terms of $k$ and $\\theta$. Then, determine the abscissa of convergence $t_{\\star}=\\sup\\{t\\in\\mathbb{R}:M_X(t)<\\infty\\}$ of the moment generating function. Express your final results in closed form; no numerical approximation is required. The final answer must be a single composite expression containing all three items.", "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed, objective, and self-contained. All definitions and parameters are standard and correctly specified.\n\nWe are tasked with deriving the moment generating function (MGF), $M_X(t)$, the characteristic function (CF), $\\phi_X(u)$, and the abscissa of convergence, $t_{\\star}$, for a random variable $X$ following a Gamma distribution, $X \\sim \\mathrm{Gamma}(k, \\theta)$, with shape parameter $k>0$ and scale parameter $\\theta>0$.\n\nThe probability density function (PDF) is given by:\n$$f_X(x) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x>0$$\n\nFirst, we derive the moment generating function, $M_X(t) = \\mathbb{E}[\\exp(tX)]$. By definition, for a continuous random variable, this is:\n$$M_X(t) = \\int_{-\\infty}^{\\infty} \\exp(tx) f_X(x) \\, dx$$\nSince $X$ is a positive random variable, the lower limit of integration is $0$. Substituting the PDF, we have:\n$$M_X(t) = \\int_{0}^{\\infty} \\exp(tx) \\left(\\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} \\exp\\left(-\\frac{x}{\\theta}\\right)\\right) \\, dx$$\nWe can factor out the constant term and combine the exponentials:\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(tx - \\frac{x}{\\theta}\\right) \\, dx$$\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - t\\right)\\right) \\, dx$$\nFor this integral to converge, the term in the exponent must be negative for all $x>0$. This requires that the coefficient of $-x$ be positive:\n$$\\frac{1}{\\theta} - t > 0 \\implies t  \\frac{1}{\\theta}$$\nAssuming this condition holds, we can evaluate the integral. The integral is in the form of the kernel of a Gamma distribution. We can make a substitution. Let $y = x\\left(\\frac{1}{\\theta} - t\\right) = x\\frac{1-t\\theta}{\\theta}$. Then $x = y \\frac{\\theta}{1-t\\theta}$, and $dx = \\frac{\\theta}{1-t\\theta} dy$. The limits of integration remain $0$ and $\\infty$.\nAlternatively, we directly recognize the form of the Gamma function integral: $\\int_0^\\infty z^{a-1} \\exp(-bz) \\, dz = \\frac{\\Gamma(a)}{b^a}$. In our case, $a=k$ and $b = \\frac{1}{\\theta} - t$.\nUsing this identity, the integral becomes:\n$$\\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - t\\right)\\right) \\, dx = \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - t\\right)^k}$$\nNow, we substitute this back into the expression for $M_X(t)$:\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\cdot \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - t\\right)^k} = \\frac{1}{\\theta^k \\left(\\frac{1-t\\theta}{\\theta}\\right)^k}$$\n$$M_X(t) = \\frac{1}{\\theta^k \\frac{(1-t\\theta)^k}{\\theta^k}} = \\frac{1}{(1-t\\theta)^k}$$\nThis expression for the MGF is valid for $t  1/\\theta$. The MGF is $M_X(t) = (1 - t\\theta)^{-k}$.\n\nNext, we determine the abscissa of convergence, $t_{\\star}$, which is defined as $t_{\\star} = \\sup\\{t \\in \\mathbb{R} : M_X(t)  \\infty\\}$. The derived MGF is finite if and only if $1-t\\theta > 0$, which is the condition $t  1/\\theta$. The set of values for which the MGF is finite is the interval $(-\\infty, 1/\\theta)$. The supremum of this set is $1/\\theta$.\n$$t_{\\star} = \\sup\\left\\{ t \\in \\mathbb{R} : t  \\frac{1}{\\theta} \\right\\} = \\frac{1}{\\theta}$$\n\nFinally, we derive the characteristic function, $\\phi_X(u) = \\mathbb{E}[\\exp(iuX)]$, where $i$ is the imaginary unit. Using its definition:\n$$\\phi_X(u) = \\int_{0}^{\\infty} \\exp(iux) f_X(x) \\, dx$$\nThe calculation is analogous to the MGF derivation. We substitute the PDF:\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(iux - \\frac{x}{\\theta}\\right) \\, dx$$\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - iu\\right)\\right) \\, dx$$\nThis integral is of the form $\\int_0^\\infty z^{a-1} \\exp(-bz) \\, dz$, with $a=k$ and $b = \\frac{1}{\\theta} - iu$. The integral converges if the real part of the coefficient $b$ is positive.\n$$\\mathrm{Re}(b) = \\mathrm{Re}\\left(\\frac{1}{\\theta} - iu\\right) = \\frac{1}{\\theta}$$\nSince $\\theta > 0$ is given, $\\mathrm{Re}(b) > 0$, so the integral converges for all real values of $u$. The value of the integral is $\\frac{\\Gamma(k)}{b^k}$.\nSubstituting this result back into the expression for $\\phi_X(u)$:\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\cdot \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - iu\\right)^k} = \\frac{1}{\\theta^k \\left(\\frac{1 - i\\theta u}{\\theta}\\right)^k}$$\n$$\\phi_X(u) = \\frac{1}{(1 - i\\theta u)^k}$$\nThis is the characteristic function, $\\phi_X(u) = (1 - i\\theta u)^{-k}$, valid for all $u \\in \\mathbb{R}$. This result can also be obtained by substituting $t = iu$ into the expression for the MGF, as $\\phi_X(u) = M_X(iu)$.\n\nIn summary, the three quantities are:\n1. Moment Generating Function: $M_X(t) = (1 - t\\theta)^{-k}$\n2. Characteristic Function: $\\phi_X(u) = (1 - i\\theta u)^{-k}$\n3. Abscissa of Convergence: $t_{\\star} = \\frac{1}{\\theta}$", "answer": "$$\\boxed{\\begin{pmatrix} (1 - t\\theta)^{-k}  (1 - i\\theta u)^{-k}  \\frac{1}{\\theta} \\end{pmatrix}}$$", "id": "3066890"}, {"introduction": "Moving from static random variables to dynamic processes, this problem tackles an object central to stochastic calculus: the time integral of a standard Brownian motion, $X_t = \\int_0^t B_s\\,ds$. You will derive its characteristic function by leveraging the fundamental properties of Brownian motion, such as its Gaussian nature. This exercise demonstrates the power of characteristic functions in analyzing the distributions of more complex random variables that arise from stochastic processes [@problem_id:3066861].", "problem": "Let $\\{B_{s}\\}_{s \\geq 0}$ be a standard Brownian motion (Wiener process), meaning $B_{0}=0$, it has independent increments, and for $0 \\leq r \\leq s$ the increment $B_{s}-B_{r}$ is Gaussian with mean $0$ and variance $s-r$, and the covariance is $\\mathbb{E}[B_{r}B_{s}]=\\min\\{r,s\\}$. For a fixed $t0$, define the random variable\n$$\nX=\\int_{0}^{t} B_{s}\\,ds.\n$$\nUsing only foundational properties of Gaussian processes and Brownian motion, compute the characteristic function (CF), defined by $\\varphi_{X}(u)=\\mathbb{E}[\\exp(i\\,u\\,X)]$, of $X$. Your derivation must begin from the defining properties stated above and proceed by:\n- establishing that $X$ is Gaussian and determining its mean and variance from first principles, and\n- independently verifying the same characteristic function by evaluating $\\mathbb{E}[\\exp(i\\,u\\,X)]$ through an approximation of the integral by simple functions and using the independence and Gaussianity of Brownian increments.\n\nExpress your final answer as a single closed-form analytic expression in terms of $u$ and $t$.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained, providing all necessary definitions and properties of standard Brownian motion. It presents a standard, solvable problem in stochastic calculus. Therefore, the problem is valid. The solution will proceed in two parts as requested.\n\nLet $\\{B_{s}\\}_{s \\geq 0}$ be a standard Brownian motion. We are given the random variable $X = \\int_{0}^{t} B_{s}\\,ds$ for a fixed $t > 0$. We want to find its characteristic function (CF), $\\varphi_{X}(u) = \\mathbb{E}[\\exp(i\\,u\\,X)]$.\n\n**Part 1: Derivation by determining the distribution of $X$**\n\nFirst, we establish that $X$ is a Gaussian random variable. A Gaussian process is a stochastic process $\\{Y_t\\}_{t \\in T}$ such that for any finite set of indices $t_1, \\dots, t_k$ in the index set $T$, the random vector $(Y_{t_1}, \\dots, Y_{t_k})$ has a multivariate normal distribution. Brownian motion $\\{B_s\\}_{s \\ge 0}$ is a Gaussian process. The integral $X = \\int_{0}^{t} B_{s}\\,ds$ is a linear functional of the Gaussian process $\\{B_s\\}_{s \\ge 0}$. Specifically, it can be viewed as the limit of linear combinations of the form $\\sum_k B_{s_k} \\Delta s_k$, which are themselves Gaussian. The limit of a sequence of Gaussian random variables is also Gaussian. Thus, $X$ is a Gaussian random variable.\n\nA Gaussian random variable is completely characterized by its mean and variance.\n\nThe mean of $X$ is:\n$$\n\\mathbb{E}[X] = \\mathbb{E}\\left[\\int_{0}^{t} B_{s}\\,ds\\right]\n$$\nBy Fubini's theorem, we can interchange the expectation and the integral:\n$$\n\\mathbb{E}[X] = \\int_{0}^{t} \\mathbb{E}[B_{s}]\\,ds\n$$\nFor a standard Brownian motion, $B_s = B_s - B_0$ is a Gaussian random variable with mean $0$ and variance $s$. Thus, $\\mathbb{E}[B_{s}] = 0$ for all $s \\ge 0$.\n$$\n\\mathbb{E}[X] = \\int_{0}^{t} 0\\,ds = 0\n$$\n\nThe variance of $X$ is $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since $\\mathbb{E}[X]=0$, the variance is simply $\\mathbb{E}[X^2]$.\n$$\n\\text{Var}(X) = \\mathbb{E}[X^2] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} B_{s}\\,ds\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} B_{r}\\,dr\\right) \\left(\\int_{0}^{t} B_{s}\\,ds\\right)\\right]\n$$\nThis can be written as a double integral:\n$$\n\\text{Var}(X) = \\mathbb{E}\\left[\\int_{0}^{t}\\int_{0}^{t} B_{r}B_{s}\\,dr\\,ds\\right]\n$$\nUsing Fubini's theorem again to interchange expectation and integration:\n$$\n\\text{Var}(X) = \\int_{0}^{t}\\int_{0}^{t} \\mathbb{E}[B_{r}B_{s}]\\,dr\\,ds\n$$\nWe are given that the covariance of Brownian motion is $\\mathbb{E}[B_{r}B_{s}] = \\min\\{r,s\\}$.\n$$\n\\text{Var}(X) = \\int_{0}^{t}\\int_{0}^{t} \\min\\{r,s\\}\\,dr\\,ds\n$$\nWe can evaluate this integral by splitting the domain of integration, the square $[0,t] \\times [0,t]$, into two triangles where $r \\le s$ and $s  r$.\n$$\n\\text{Var}(X) = \\int_{0}^{t} \\left( \\int_{0}^{s} \\min\\{r,s\\}\\,dr + \\int_{s}^{t} \\min\\{r,s\\}\\,dr \\right) ds\n$$\nIn the first inner integral, $r \\le s$, so $\\min\\{r,s\\} = r$. In the second, $r > s$, so $\\min\\{r,s\\} = s$.\n$$\n\\text{Var}(X) = \\int_{0}^{t} \\left( \\int_{0}^{s} r\\,dr + \\int_{s}^{t} s\\,dr \\right) ds\n$$\nEvaluating the inner integrals:\n$$\n\\int_{0}^{s} r\\,dr = \\left[\\frac{r^2}{2}\\right]_0^s = \\frac{s^2}{2}\n$$\n$$\n\\int_{s}^{t} s\\,dr = s [r]_s^t = s(t-s) = st - s^2\n$$\nSubstituting these back into the outer integral:\n$$\n\\text{Var}(X) = \\int_{0}^{t} \\left( \\frac{s^2}{2} + st - s^2 \\right) ds = \\int_{0}^{t} \\left( st - \\frac{s^2}{2} \\right) ds\n$$\nNow, we evaluate the outer integral:\n$$\n\\text{Var}(X) = \\left[ t\\frac{s^2}{2} - \\frac{s^3}{6} \\right]_0^t = \\frac{t \\cdot t^2}{2} - \\frac{t^3}{6} = \\frac{t^3}{2} - \\frac{t^3}{6} = \\frac{3t^3 - t^3}{6} = \\frac{2t^3}{6} = \\frac{t^3}{3}\n$$\nSo, $X$ is a Gaussian random variable with mean $\\mu = 0$ and variance $\\sigma^2 = \\frac{t^3}{3}$. The characteristic function of a Gaussian random variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is given by $\\varphi_Y(u) = \\exp(i u \\mu - \\frac{1}{2} u^2 \\sigma^2)$.\nFor $X$, this becomes:\n$$\n\\varphi_X(u) = \\exp\\left(i u (0) - \\frac{1}{2} u^2 \\left(\\frac{t^3}{3}\\right)\\right) = \\exp\\left(-\\frac{t^3 u^2}{6}\\right)\n$$\n\n**Part 2: Verification by direct computation via approximation**\n\nWe now verify this result by approximating the integral with a Riemann sum. Let's partition the interval $[0,t]$ into $n$ subintervals of equal width $\\Delta t = t/n$, with points $t_k = k\\Delta t$ for $k=0, 1, \\dots, n$. We approximate $X$ using a left-endpoint sum:\n$$\nX_n = \\sum_{k=0}^{n-1} B_{t_k} \\Delta t\n$$\nIn the limit as $n \\to \\infty$, $X_n \\to X$. Thus, the characteristic function $\\varphi_{X_n}(u)$ will converge to $\\varphi_X(u)$.\nWe have $\\varphi_{X_n}(u) = \\mathbb{E}[\\exp(i u X_n)] = \\mathbb{E}\\left[\\exp\\left(i u \\Delta t \\sum_{k=0}^{n-1} B_{t_k}\\right)\\right]$.\nTo analyze the sum, we express the correlated variables $B_{t_k}$ in terms of independent increments. Let $\\Delta B_j = B_{t_j} - B_{t_{j-1}}$ for $j=1, \\dots, n$. The increments $\\Delta B_j$ are independent and distributed as $\\mathcal{N}(0, \\Delta t)$. Since $B_{t_0}=B_0=0$, we have $B_{t_k} = \\sum_{j=1}^{k} \\Delta B_j$.\nThe sum in the exponent becomes:\n$$\n\\sum_{k=0}^{n-1} B_{t_k} = B_{t_0} + \\sum_{k=1}^{n-1} B_{t_k} = 0 + \\sum_{k=1}^{n-1} \\sum_{j=1}^{k} \\Delta B_j\n$$\nBy changing the order of summation:\n$$\n\\sum_{k=1}^{n-1} \\sum_{j=1}^{k} \\Delta B_j = \\sum_{j=1}^{n-1} \\sum_{k=j}^{n-1} \\Delta B_j = \\sum_{j=1}^{n-1} (n-j) \\Delta B_j\n$$\nSo, $X_n = \\Delta t \\sum_{j=1}^{n-1} (n-j) \\Delta B_j$. This is a linear combination of independent Gaussian random variables, so $X_n$ is also Gaussian. Its mean is $\\mathbb{E}[X_n] = \\Delta t \\sum_{j=1}^{n-1} (n-j) \\mathbb{E}[\\Delta B_j] = 0$.\nThe variance is:\n$$\n\\text{Var}(X_n) = \\text{Var}\\left(\\Delta t \\sum_{j=1}^{n-1} (n-j) \\Delta B_j\\right) = (\\Delta t)^2 \\sum_{j=1}^{n-1} (n-j)^2 \\text{Var}(\\Delta B_j)\n$$\nSince $\\text{Var}(\\Delta B_j) = \\Delta t$:\n$$\n\\text{Var}(X_n) = (\\Delta t)^3 \\sum_{j=1}^{n-1} (n-j)^2 = (\\Delta t)^3 \\sum_{k=1}^{n-1} k^2\n$$\nUsing the formula for the sum of squares, $\\sum_{k=1}^{m} k^2 = \\frac{m(m+1)(2m+1)}{6}$, with $m=n-1$:\n$$\n\\text{Var}(X_n) = (\\Delta t)^3 \\frac{(n-1)n(2n-1)}{6}\n$$\nSubstituting $\\Delta t = t/n$:\n$$\n\\text{Var}(X_n) = \\frac{t^3}{n^3} \\frac{(n-1)n(2n-1)}{6} = \\frac{t^3}{6} \\frac{(n-1)n(2n-1)}{n^3}\n$$\nThe characteristic function of the Gaussian variable $X_n$ is $\\varphi_{X_n}(u) = \\exp\\left(-\\frac{1}{2} u^2 \\text{Var}(X_n)\\right)$:\n$$\n\\varphi_{X_n}(u) = \\exp\\left(-\\frac{u^2}{2} \\cdot \\frac{t^3}{6} \\frac{(n-1)n(2n-1)}{n^3}\\right) = \\exp\\left(-\\frac{t^3 u^2}{12} \\frac{(n-1)n(2n-1)}{n^3}\\right)\n$$\nNow, we take the limit as $n \\to \\infty$:\n$$\n\\lim_{n\\to\\infty} \\frac{(n-1)n(2n-1)}{n^3} = \\lim_{n\\to\\infty} \\left(1-\\frac{1}{n}\\right)(1)\\left(2-\\frac{1}{n}\\right) = (1)(1)(2) = 2\n$$\nTherefore, the characteristic function of $X$ is:\n$$\n\\varphi_X(u) = \\lim_{n\\to\\infty} \\varphi_{X_n}(u) = \\exp\\left(-\\frac{t^3 u^2}{12} \\cdot 2\\right) = \\exp\\left(-\\frac{t^3 u^2}{6}\\right)\n$$\nThis result matches the one obtained in the first part, verifying the calculation.", "answer": "$$ \\boxed{\\exp\\left(-\\frac{t^{3} u^{2}}{6}\\right)} $$", "id": "3066861"}, {"introduction": "While analytical solutions are elegant, many problems involving stochastic differential equations (SDEs) rely on numerical methods. This final practice bridges the gap between theory and computation by using Monte Carlo simulation to estimate the MGF and CGF for an Ornstein-Uhlenbeck process [@problem_id:3066856]. You will implement estimators for these functions and, critically, analyze the different sources of error, distinguishing between the discretization bias from the numerical scheme and the statistical variance from sampling.", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE): $$dX_t=-\\theta X_t\\,dt+\\sigma\\,dW_t,$$ with initial condition $$X_0=x_0,$$ where $$\\theta0$$ and $$\\sigma0$$ are constants and $$\\{W_t\\}_{t\\ge 0}$$ is a standard Wiener process. Let $$T0$$ be a fixed time horizon. You will approximate the law of $$X_T$$ via the Euler–Maruyama time discretization with time step $$\\Delta t0$$ and compute moment generating function (MGF) and cumulant generating function (CGF) estimators by Monte Carlo (MC). Your goals are: (i) implement a Monte Carlo estimator of the MGF and CGF from simulated SDE paths, (ii) derive and compute the estimator’s bias and variance contributions, and (iii) compare the discretization bias with the sampling variability.\n\nFundamental definitions and facts to use as starting points:\n- The Euler–Maruyama scheme for the SDE over $$n$$ steps with $$\\Delta t=T/n$$ is $$X_{k+1}=X_k+(-\\theta X_k)\\Delta t+\\sigma\\sqrt{\\Delta t}\\,\\xi_k,$$ for $$k=0,1,\\dots,n-1,$$ with $$X_0=x_0$$ and independent standard normal random variables $$\\xi_k\\sim\\mathcal{N}(0,1)$$.\n- The moment generating function (MGF) of a real-valued random variable $$X$$ is $$M_X(t)=\\mathbb{E}[e^{tX}]$$ for real $$t$$ in a neighborhood of $$0$$.\n- The cumulant generating function (CGF) is $$K_X(t)=\\log M_X(t)$$ where the logarithm is the natural logarithm.\n- For a Monte Carlo sample $$X^{(1)},\\dots,X^{(N)}$$ from a distribution, the MGF estimator at $$t$$ is $$\\widehat{M}(t)=\\frac{1}{N}\\sum_{i=1}^N e^{t X^{(i)}}.$$\n- For large $$N,$$ a second-order Taylor expansion implies an approximate bias for the CGF estimator $$\\widehat{K}(t)=\\log \\widehat{M}(t)$$ of order $$\\mathcal{O}(1/N),$$ specifically $$\\mathbb{E}[\\widehat{K}(t)]-K_X(t)\\approx -\\frac{\\mathrm{Var}(e^{tX})}{2N\\,M_X(t)^2}.$$\n\nTasks to perform in your program:\n1. Simulate $$N$$ independent paths of the Euler–Maruyama scheme with step size $$\\Delta t$$ up to time $$T$$ and collect the terminal values $$X_T^{(1)},\\dots,X_T^{(N)}.$$\n2. Compute the Monte Carlo MGF estimator $$\\widehat{M}(t)=\\frac{1}{N}\\sum_{i=1}^N e^{t X_T^{(i)}}.$$\n3. Compute the Monte Carlo CGF estimator $$\\widehat{K}(t)=\\log \\widehat{M}(t).$$\n4. For the Euler–Maruyama terminal distribution, use the fact that $$X_T$$ is Gaussian (by linear-Gaussian recursion) with mean $$\\mu_{\\mathrm{E}}=x_0 a^n$$ and variance $$v_{\\mathrm{E}}=\\sigma^2 \\Delta t\\sum_{j=0}^{n-1} a^{2j},$$ where $$a=1-\\theta\\Delta t$$ and $$n=T/\\Delta t$$ is an integer. Using these, compute the “Euler law” MGF $$M_{\\mathrm{E}}(t)$$ and the theoretical variance of the MGF estimator $$\\mathrm{Var}(\\widehat{M}(t))=\\frac{1}{N}\\left(\\mathbb{E}[e^{2tX_T}]-M_{\\mathrm{E}}(t)^2\\right),$$ both under the Euler–Maruyama distribution.\n5. Compute the “true OU law” mean and variance at time $$T,$$ namely $$\\mu_{\\mathrm{OU}}=x_0 e^{-\\theta T}$$ and $$v_{\\mathrm{OU}}=\\frac{\\sigma^2}{2\\theta}\\left(1-e^{-2\\theta T}\\right),$$ and from them compute the “true OU” MGF $$M_{\\mathrm{OU}}(t).$$\n6. Report for each test case:\n   - the Monte Carlo MGF estimate $$\\widehat{M}(t),$$\n   - the true OU MGF $$M_{\\mathrm{OU}}(t),$$\n   - the Euler–Maruyama MGF $$M_{\\mathrm{E}}(t),$$\n   - the Monte Carlo bias relative to the true OU MGF, $$\\widehat{M}(t)-M_{\\mathrm{OU}}(t),$$\n   - the discretization bias in MGF, $$M_{\\mathrm{E}}(t)-M_{\\mathrm{OU}}(t),$$\n   - the theoretical variance of $$\\widehat{M}(t)$$ under the Euler–Maruyama law, $$\\mathrm{Var}(\\widehat{M}(t)),$$\n   - the Monte Carlo CGF estimate $$\\widehat{K}(t),$$\n   - the second-order delta-method approximation to the CGF estimator bias under the Euler–Maruyama law, $$-\\frac{\\mathrm{Var}(\\widehat{M}(t))}{2 M_{\\mathrm{E}}(t)^2}.$$\n\nImplementation requirements:\n- Use a fixed pseudorandom seed to ensure deterministic output. Take the seed to be $$s=123456.$$\n- If $$T/\\Delta t$$ is not an integer, replace $$\\Delta t$$ by $$T/n$$ with $$n=\\mathrm{round}(T/\\Delta t)$$ so that $$n$$ is an integer and $$n\\Delta t=T.$$\n- For each test case, output the eight quantities listed in item $$6$$ above, in that order.\n\nTest suite:\n- Case $$1$$ (happy path): $$\\theta=1.0,$$ $$\\sigma=0.8,$$ $$x_0=0.5,$$ $$T=1.0,$$ $$\\Delta t=0.01,$$ $$N=30000,$$ $$t=0.5.$$\n- Case $$2$$ (boundary at $$t=0$$): $$\\theta=1.0,$$ $$\\sigma=0.8,$$ $$x_0=0.5,$$ $$T=1.0,$$ $$\\Delta t=0.01,$$ $$N=30000,$$ $$t=0.0.$$\n- Case $$3$$ (higher drift and different sign initial condition): $$\\theta=1.5,$$ $$\\sigma=0.7,$$ $$x_0=-0.4,$$ $$T=1.2,$$ $$\\Delta t=0.01,$$ $$N=40000,$$ $$t=1.0.$$\n\nFinal output format:\nYour program should produce a single line containing a comma-separated list of all results concatenated in the order of the test cases, with each test case contributing its eight numbers in the order specified in item $$6$$. The line must be enclosed in square brackets, with no spaces. For example, the format is\n$$[\\widehat{M}_1,M_{\\mathrm{OU},1},M_{\\mathrm{E},1},\\widehat{M}_1-M_{\\mathrm{OU},1},M_{\\mathrm{E},1}-M_{\\mathrm{OU},1},\\mathrm{Var}(\\widehat{M}_1),\\widehat{K}_1,\\mathrm{Bias}_{\\widehat{K},1},\\widehat{M}_2,\\dots].$$\nAll outputs must be real numbers (floating-point). No physical units are involved. Angles are not used. Express any ratios as decimal numbers.", "solution": "The problem requires a numerical and theoretical analysis of the moment generating function (MGF) and cumulant generating function (CGF) for the terminal value of an Ornstein-Uhlenbeck (OU) process. The analysis is based on the Euler-Maruyama discretization scheme and Monte Carlo (MC) simulation. We will compute estimators for the MGF and CGF and analyze their errors, separating the discretization bias from the statistical sampling variability.\n\nThe Ornstein-Uhlenbeck stochastic differential equation (SDE) is given by\n$$dX_t = -\\theta X_t dt + \\sigma dW_t$$\nwith a deterministic initial condition $X_0 = x_0$, and positive constants $\\theta > 0$ and $\\sigma > 0$.\n\nFirst, we establish the theoretical ground truths for both the continuous-time process and its discrete-time approximation.\n\n**1. True Ornstein-Uhlenbeck Law**\nThe exact solution to the OU SDE is a Gaussian process. At any fixed time $T > 0$, the random variable $X_T$ follows a Normal distribution, $X_T \\sim \\mathcal{N}(\\mu_{\\mathrm{OU}}, v_{\\mathrm{OU}})$. The mean and variance are given by:\n$$ \\mu_{\\mathrm{OU}} = x_0 e^{-\\theta T} $$\n$$ v_{\\mathrm{OU}} = \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta T}) $$\nThe MGF of a general Normal random variable $Z \\sim \\mathcal{N}(\\mu, v)$ is $M_Z(t) = \\mathbb{E}[e^{tZ}] = \\exp(\\mu t + \\frac{1}{2}v t^2)$. Applying this formula, the true MGF of $X_T$, denoted $M_{\\mathrm{OU}}(t)$, is:\n$$ M_{\\mathrm{OU}}(t) = \\exp\\left(\\mu_{\\mathrm{OU}} t + \\frac{1}{2} v_{\\mathrm{OU}} t^2\\right) $$\nThis value serves as the benchmark against which our numerical results will be compared.\n\n**2. Euler-Maruyama Discretization Law**\nThe Euler-Maruyama scheme approximates the SDE over a time grid $0, \\Delta t, 2\\Delta t, \\dots, n\\Delta t=T$. The recursive formula for the approximate process value, denoted $X_k$, is:\n$$ X_{k+1} = X_k + (-\\theta X_k)\\Delta t + \\sigma\\sqrt{\\Delta t}\\,\\xi_k = (1-\\theta\\Delta t)X_k + \\sigma\\sqrt{\\Delta t}\\,\\xi_k $$\nwhere $\\xi_k \\sim \\mathcal{N}(0, 1)$ are independent standard normal random variables. Let $a = 1 - \\theta\\Delta t$. The solution to this linear recurrence is:\n$$ X_n = a^n X_0 + \\sigma\\sqrt{\\Delta t} \\sum_{j=0}^{n-1} a^{n-1-j} \\xi_j $$\nSince $X_n$ is a sum of Gaussian random variables (plus a constant), it is also normally distributed, $X_n \\sim \\mathcal{N}(\\mu_{\\mathrm{E}}, v_{\\mathrm{E}})$. The mean is $\\mathbb{E}[X_n] = a^n x_0$, so:\n$$ \\mu_{\\mathrm{E}} = x_0 (1-\\theta\\Delta t)^n $$\nThe variance is the variance of the stochastic part:\n$$ v_{\\mathrm{E}} = \\mathrm{Var}\\left(\\sigma\\sqrt{\\Delta t} \\sum_{j=0}^{n-1} a^{n-1-j} \\xi_j\\right) = \\sigma^2 \\Delta t \\sum_{j=0}^{n-1} (a^{n-1-j})^2 = \\sigma^2 \\Delta t \\sum_{k=0}^{n-1} (a^2)^k $$\nThe sum is a geometric series. If $a^2 = 1$, the sum is $n$. Otherwise, it is $\\frac{(a^2)^n - 1}{a^2 - 1}$. The \"Euler law\" MGF, $M_{\\mathrm{E}}(t)$, for the terminal value of the discretized process is:\n$$ M_{\\mathrm{E}}(t) = \\exp\\left(\\mu_{\\mathrm{E}} t + \\frac{1}{2} v_{\\mathrm{E}} t^2\\right) $$\nThe difference $M_{\\mathrm{E}}(t) - M_{\\mathrm{OU}}(t)$ is the discretization bias, an error introduced by approximating the continuous SDE with a discrete-time scheme.\n\n**3. Monte Carlo Estimation**\nWe simulate $N$ independent paths of the Euler-Maruyama scheme to obtain $N$ samples of the terminal value, $\\{X_T^{(i)}\\}_{i=1}^N$, which are i.i.d. draws from $\\mathcal{N}(\\mu_{\\mathrm{E}}, v_{\\mathrm{E}})$.\nThe MGF estimator, $\\widehat{M}(t)$, is the sample mean of $e^{tX_T^{(i)}}$:\n$$ \\widehat{M}(t) = \\frac{1}{N} \\sum_{i=1}^N e^{tX_T^{(i)}} $$\nThe CGF estimator, $\\widehat{K}(t)$, is the natural logarithm of the MGF estimator:\n$$ \\widehat{K}(t) = \\log\\left(\\widehat{M}(t)\\right) $$\n\n**4. Error Analysis**\nThe total error of the MC estimator relative to the true MGF is $\\widehat{M}(t) - M_{\\mathrm{OU}}(t)$. This error can be additively decomposed into a statistical error and a discretization error:\n$$ \\widehat{M}(t) - M_{\\mathrm{OU}}(t) = \\underbrace{\\left(\\widehat{M}(t) - M_{\\mathrm{E}}(t)\\right)}_{\\text{Statistical Error}} + \\underbrace{\\left(M_{\\mathrm{E}}(t) - M_{\\mathrm{OU}}(t)\\right)}_{\\text{Discretization Error}} $$\nThe problem requires reporting the total error and the discretization error component.\n\nThe variability of the statistical error is quantified by the variance of the estimator $\\widehat{M}(t)$. Treating the Euler-Maruyama law as the ground truth for the simulation, the variance of $\\widehat{M}(t)$ is:\n$$ \\mathrm{Var}(\\widehat{M}(t)) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{i=1}^N e^{tX_T^{(i)}}\\right) = \\frac{1}{N} \\mathrm{Var}(e^{tX_T^{(i)}}) $$\nUsing $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, we get:\n$$ \\mathrm{Var}(e^{tX_T^{(i)}}) = \\mathbb{E}[e^{2tX_T^{(i)}}] - (\\mathbb{E}[e^{tX_T^{(i)}}])^2 = M_{\\mathrm{E}}(2t) - (M_{\\mathrm{E}}(t))^2 $$\nTherefore, the theoretical variance of the MGF estimator is:\n$$ \\mathrm{Var}(\\widehat{M}(t)) = \\frac{1}{N} \\left(M_{\\mathrm{E}}(2t) - (M_{\\mathrm{E}}(t))^2\\right) $$\n\nFinally, the CGF estimator $\\widehat{K}(t) = \\log\\widehat{M}(t)$ is biased with respect to $K_{\\mathrm{E}}(t) = \\log(M_{\\mathrm{E}}(t))$ due to the nonlinear logarithm function. A second-order Taylor expansion (delta method) provides an approximation for this bias:\n$$ \\mathbb{E}[\\widehat{K}(t)] - K_{\\mathrm{E}}(t) \\approx -\\frac{\\mathrm{Var}(\\widehat{M}(t))}{2 (M_{\\mathrm{E}}(t))^2} $$\nThis term is of order $\\mathcal{O}(1/N)$ and quantifies the systematic error introduced by applying the logarithm to the MGF sample mean.\n\nThe implementation will first compute the theoretical quantities $M_{\\mathrm{OU}}(t)$ and $M_{\\mathrm{E}}(t)$. Then, it will perform the Monte Carlo simulation to find $\\widehat{M}(t)$ and $\\widehat{K}(t)$. Finally, it will use these intermediate results to calculate the required eight output values for each test case, employing a fixed random seed for reproducibility. The number of steps $n$ is adjusted via $n=\\mathrm{round}(T/\\Delta t)$ to ensure $n\\Delta t=T$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating an Ornstein-Uhlenbeck process,\n    computing MGF and CGF estimators, and analyzing their biases and variances.\n    \"\"\"\n    # Use a fixed pseudorandom seed for deterministic output.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (theta, sigma, x0, T, dt_in, N, t_mgf)\n        (1.0, 0.8, 0.5, 1.0, 0.01, 30000, 0.5),\n        (1.0, 0.8, 0.5, 1.0, 0.01, 30000, 0.0),\n        (1.5, 0.7, -0.4, 1.2, 0.01, 40000, 1.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        theta, sigma, x0, T, dt_in, N, t_mgf = case\n\n        # 1. Adjust n and dt as per implementation requiremements.\n        if T == 0:\n            n = 0\n            dt = 0.0\n        else:\n            n = int(round(T / dt_in))\n            dt = T / n\n\n        # 2. Simulate N independent paths of the Euler-Maruyama scheme.\n        X_T_samples = np.full(N, x0, dtype=np.float64)\n        if n > 0:\n            a_sim = 1.0 - theta * dt\n            for _ in range(n):\n                xi = rng.standard_normal(N)\n                X_T_samples = a_sim * X_T_samples + sigma * np.sqrt(dt) * xi\n\n        # 3. Compute Monte Carlo MGF and CGF estimators.\n        exp_tX = np.exp(t_mgf * X_T_samples)\n        M_hat = np.mean(exp_tX)\n        K_hat = np.log(M_hat)\n\n        # 4. Compute \"Euler law\" quantities.\n        # This is for the terminal distribution of the Euler-Maruyama scheme.\n        a = 1.0 - theta * dt\n        if n == 0:\n            mu_E = x0\n            v_E = 0.0\n        else:\n            mu_E = x0 * (a**n)\n            a_sq = a**2\n            # Use geometric series sum formula for variance.\n            # Handle the case a^2=1 separately for numerical stability.\n            if np.isclose(a_sq, 1.0):\n                sum_a_2j = n\n            else:\n                sum_a_2j = (a_sq**n - 1.0) / (a_sq - 1.0)\n            v_E = sigma**2 * dt * sum_a_2j\n        \n        M_E = np.exp(mu_E * t_mgf + 0.5 * v_E * t_mgf**2)\n\n        # 5. Compute the theoretical variance of the MGF estimator.\n        # This is under the Euler-Maruyama law.\n        M_E_2t = np.exp(mu_E * (2 * t_mgf) + 0.5 * v_E * (2 * t_mgf)**2)\n        Var_M_hat = (M_E_2t - M_E**2) / N\n\n        # 6. Compute \"true OU law\" quantities.\n        mu_OU = x0 * np.exp(-theta * T)\n        if T == 0:\n            v_OU = 0.0\n        else:\n            if theta == 0: # Problem states theta > 0, but for robustness\n                v_OU = sigma**2 * T\n            else:\n                v_OU = (sigma**2 / (2 * theta)) * (1.0 - np.exp(-2 * theta * T))\n        \n        M_OU = np.exp(mu_OU * t_mgf + 0.5 * v_OU * t_mgf**2)\n\n        # 7. Report the eight required quantities.\n        # MC bias relative to true OU MGF\n        mc_bias_vs_true = M_hat - M_OU\n        # Discretization bias in MGF\n        discretization_bias = M_E - M_OU\n        # Second-order delta-method approximation to the CGF estimator bias\n        # Check for M_E being zero is not necessary as it's an exponential.\n        cgf_bias_approx = -Var_M_hat / (2.0 * M_E**2) if M_E > 0 else 0.0\n\n        results_case = [\n            M_hat,                 # 1. Monte Carlo MGF estimate\n            M_OU,                  # 2. True OU MGF\n            M_E,                   # 3. Euler-Maruyama MGF\n            mc_bias_vs_true,       # 4. MC bias relative to true OU MGF\n            discretization_bias,   # 5. Discretization bias in MGF\n            Var_M_hat,             # 6. Theoretical variance of M_hat\n            K_hat,                 # 7. Monte Carlo CGF estimate\n            cgf_bias_approx        # 8. CGF estimator bias approximation\n        ]\n        all_results.extend(results_case)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3066856"}]}