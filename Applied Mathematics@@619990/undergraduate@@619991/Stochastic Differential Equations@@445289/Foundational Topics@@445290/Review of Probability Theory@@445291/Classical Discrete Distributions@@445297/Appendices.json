{"hands_on_practices": [{"introduction": "Before we can analyze or model systems with random jumps, we first need a way to generate the number of jumps in a given time interval. This practice introduces a classic and ingenious algorithm, often attributed to Donald Knuth, for sampling from a Poisson distribution using only a sequence of uniform random numbers. By working through this problem [@problem_id:3044321], you will uncover the beautiful theoretical link between the Poisson process, the exponential distribution, and simple uniform draws, providing a cornerstone for simulating jump-diffusion processes.", "problem": "In simulating jump terms of stochastic models driven by a Poisson process within stochastic differential equations, a common task is to sample a Poisson-distributed random variable with parameter $\\lambda>0$. One classical approach uses only independent Uniform$(0,1)$ random variables. Consider the following algorithm that outputs a nonnegative integer:\n- Initialize $k \\leftarrow 0$ and $p \\leftarrow 1$.\n- Repeat:\n  - Generate $U \\sim \\text{Uniform}(0,1)$ independently of all previous random variables.\n  - Update $p \\leftarrow p \\cdot U$ and $k \\leftarrow k+1$.\n- Until $p \\leq \\exp(-\\lambda)$.\n- Return $k-1$.\n\nUsing only foundational facts that can be justified from first principles, namely: (i) if $U \\sim \\text{Uniform}(0,1)$ then $-\\ln U$ is Exponential$(1)$, (ii) sums of independent Exponential$(1)$ random variables have the Gamma$(n,1)$ distribution with density $f_{S_{n}}(t)=\\frac{t^{n-1}\\exp(-t)}{(n-1)!}$ for $t>0$, and (iii) standard properties of integrals and expectations, do the following:\n1) State clearly why the algorithm above is a valid method to sample a Poisson-distributed random variable with parameter $\\lambda$.\n2) Let $L$ denote the total number of iterations of the loop (i.e., the number of independent Uniform$(0,1)$ random variables actually generated). Derive, from the facts listed, a closed-form expression for $\\mathbb{E}[L]$ as a function of $\\lambda$.\nProvide your final answer as a single analytic expression. No numerical approximation is required.", "solution": "The problem will be addressed in two parts as specified: first, a justification of the algorithm's validity, and second, the derivation of the expected number of iterations.\n\nPart 1: Justification of the Algorithm\n\nThe algorithm is designed to generate a random variable, let's call it $K$, which we must show follows a Poisson distribution with parameter $\\lambda$. The algorithm proceeds as follows:\n1.  A sequence of independent random variables $U_1, U_2, \\dots, U_L$ is generated, where each $U_i \\sim \\text{Uniform}(0,1)$. The total number of such variables generated is $L$.\n2.  The process terminates at the first integer $L \\ge 1$ for which the product $p_L = \\prod_{i=1}^{L} U_i$ satisfies the condition $p_L \\leq \\exp(-\\lambda)$.\n3.  The value returned by the algorithm is $K = L-1$.\n\nTo analyze this procedure, we apply a logarithmic transformation to the stopping condition. The loop continues as long as $\\prod_{i=1}^{k} U_i > \\exp(-\\lambda)$ and stops at the first iteration $L$ where $\\prod_{i=1}^{L} U_i \\leq \\exp(-\\lambda)$. This means that for $L$ to be the stopping iteration, two conditions must be met:\n-   At iteration $L-1$ (for $L>1$): $\\prod_{i=1}^{L-1} U_i > \\exp(-\\lambda)$.\n-   At iteration $L$: $\\prod_{i=1}^{L} U_i \\leq \\exp(-\\lambda)$.\n\nTaking the natural logarithm of these inequalities gives:\n-   $\\sum_{i=1}^{L-1} \\ln(U_i) > -\\lambda$\n-   $\\sum_{i=1}^{L} \\ln(U_i) \\leq -\\lambda$\n\nMultiplying by $-1$ reverses the inequalities:\n-   $\\sum_{i=1}^{L-1} (-\\ln U_i) < \\lambda$\n-   $\\sum_{i=1}^{L} (-\\ln U_i) \\geq \\lambda$\n\nLet us define a new sequence of random variables $X_i = -\\ln U_i$. According to the provided fact (i), since $U_i \\sim \\text{Uniform}(0,1)$, each $X_i$ is an independent random variable following an Exponential distribution with rate parameter $1$, i.e., $X_i \\sim \\text{Exponential}(1)$.\n\nLet $S_k = \\sum_{i=1}^{k} X_i$ be the sum of the first $k$ of these exponential variables. By convention, we set $S_0 = 0$. The stopping conditions for the algorithm can now be expressed in terms of the sequence of partial sums $\\{S_k\\}$: the algorithm terminates at the first integer $L$ such that $S_L \\geq \\lambda$. This implies that $S_{L-1} < \\lambda$. Thus, the stopping iteration $L$ is characterized by the event $\\{S_{L-1} < \\lambda \\leq S_L\\}$.\n\nThe algorithm returns the value $K = L-1$. Let's determine the probability distribution of $K$. For any non-negative integer $n \\in \\{0, 1, 2, \\dots\\}$, the event $\\{K=n\\}$ is equivalent to the event $\\{L-1=n\\}$, or $\\{L=n+1\\}$. This means the algorithm terminates at the $(n+1)$-th iteration. This occurs if and only if $S_n < \\lambda \\leq S_{n+1}$.\n\nWe now relate this to the definition of a Poisson process. A homogeneous Poisson process, let's denote it by $N(t)$, with a rate of $1$ counts the number of events occurring in the time interval $[0, t]$. From first principles, such a process is characterized by the property that the inter-arrival times between consecutive events are independent and identically distributed Exponential(1) random variables. In our formulation, the variables $X_i$ precisely represent these inter-arrival times, and $S_k$ represents the time of the $k$-th event.\n\nThe event that the number of arrivals in the interval $[0, \\lambda]$ is exactly $n$, denoted $\\{N(\\lambda)=n\\}$, is equivalent to the condition that the $n$-th event occurred by time $\\lambda$, but the $(n+1)$-th event occurred after time $\\lambda$. Formally, this is $\\{S_n \\leq \\lambda < S_{n+1}\\}$.\n\nTherefore, we have the equality of events:\n$$\n\\{K=n\\} \\iff \\{S_n < \\lambda \\leq S_{n+1}\\}\n$$\nSince $S_n$ and $S_{n+1}$ are continuous random variables (sums of exponential variables), the probability of $S_n=\\lambda$ or $S_{n+1}=\\lambda$ is zero. Thus, $P(S_n < \\lambda \\leq S_{n+1}) = P(S_n \\leq \\lambda < S_{n+1})$.\n\nThis establishes that $P(K=n) = P(N(\\lambda)=n)$. By the definition of a Poisson process, the number of events $N(\\lambda)$ in an interval of length $\\lambda$ for a process with rate $1$ follows a Poisson distribution with parameter (mean) $\\lambda$.\nHence, the random variable $K$ returned by the algorithm follows a Poisson($\\lambda$) distribution. This validates the algorithm.\n\nPart 2: Derivation of $\\mathbb{E}[L]$\n\nLet $L$ be the total number of iterations of the loop, which is the number of Uniform$(0,1)$ random variables generated. $L$ is a random variable taking values in $\\{1, 2, 3, \\dots\\}$. To find its expectation $\\mathbb{E}[L]$, we can use the standard property for non-negative integer-valued random variables:\n$$\n\\mathbb{E}[L] = \\sum_{k=1}^{\\infty} P(L \\geq k)\n$$\nThe event $\\{L \\geq k\\}$ signifies that the loop did not terminate on or before the $(k-1)$-th iteration. This is equivalent to the condition that the stopping criterion was not met after $k-1$ steps, i.e., $\\prod_{i=1}^{k-1} U_i > \\exp(-\\lambda)$.\nIn terms of the sums of exponential variables $S_j = \\sum_{i=1}^{j} X_i$, this is equivalent to the event $\\{S_{k-1} < \\lambda\\}$.\nThus, we can write the expectation as:\n$$\n\\mathbb{E}[L] = \\sum_{k=1}^{\\infty} P(S_{k-1} < \\lambda)\n$$\nLet's change the index of summation by setting $j=k-1$. The sum becomes:\n$$\n\\mathbb{E}[L] = \\sum_{j=0}^{\\infty} P(S_j < \\lambda) = P(S_0 < \\lambda) + \\sum_{j=1}^{\\infty} P(S_j < \\lambda)\n$$\nBy convention, $S_0=0$. Since the problem statement specifies $\\lambda > 0$, the event $\\{S_0 < \\lambda\\}$ is $\\{0 < \\lambda\\}$, which is a certain event. So, $P(S_0 < \\lambda) = 1$.\n\nFor $j \\ge 1$, $S_j$ is the sum of $j$ independent Exponential(1) random variables. According to the provided fact (ii), $S_j$ follows a Gamma($j,1$) distribution with probability density function $f_{S_j}(t) = \\frac{t^{j-1}\\exp(-t)}{(j-1)!}$ for $t>0$.\nThe probability $P(S_j < \\lambda)$ is the cumulative distribution function evaluated at $\\lambda$:\n$$\nP(S_j < \\lambda) = \\int_0^{\\lambda} f_{S_j}(t) dt = \\int_0^{\\lambda} \\frac{t^{j-1}\\exp(-t)}{(j-1)!} dt\n$$\nSubstituting this into the expression for $\\mathbb{E}[L]$:\n$$\n\\mathbb{E}[L] = 1 + \\sum_{j=1}^{\\infty} \\int_0^{\\lambda} \\frac{t^{j-1}\\exp(-t)}{(j-1)!} dt\n$$\nSince the integrand $\\frac{t^{j-1}\\exp(-t)}{(j-1)!}$ is non-negative for $t \\in [0, \\lambda]$, we can interchange the summation and integration (by the Fubini-Tonelli theorem):\n$$\n\\mathbb{E}[L] = 1 + \\int_0^{\\lambda} \\sum_{j=1}^{\\infty} \\frac{t^{j-1}\\exp(-t)}{(j-1)!} dt\n$$\nWe can factor out the term $\\exp(-t)$ from the sum:\n$$\n\\mathbb{E}[L] = 1 + \\int_0^{\\lambda} \\exp(-t) \\left( \\sum_{j=1}^{\\infty} \\frac{t^{j-1}}{(j-1)!} \\right) dt\n$$\nLet's analyze the sum inside the parentheses. By setting the index $m=j-1$, the sum becomes:\n$$\n\\sum_{m=0}^{\\infty} \\frac{t^m}{m!}\n$$\nThis is the Maclaurin series expansion for the exponential function, $\\exp(t)$. Substituting this back into the integral:\n$$\n\\mathbb{E}[L] = 1 + \\int_0^{\\lambda} \\exp(-t) \\exp(t) dt = 1 + \\int_0^{\\lambda} 1 \\cdot dt\n$$\nEvaluating the simple integral gives:\n$$\n\\mathbb{E}[L] = 1 + [t]_0^{\\lambda} = 1 + (\\lambda - 0) = 1 + \\lambda\n$$\nThus, the expected number of iterations, which is the expected number of uniform random variables generated, is $1 + \\lambda$.", "answer": "$$\\boxed{1+\\lambda}$$", "id": "3044321"}, {"introduction": "While simulating a process with a known rate $\\lambda$ is essential, in many real-world applications we face the inverse problem: we have observed data, and we need to estimate the underlying rate. This exercise [@problem_id:3044313] guides you through the fundamental statistical task of estimating the Poisson rate parameter from a series of counts. You will derive the Cramér–Rao Lower Bound, a theoretical floor on the variance of any unbiased estimator, and then determine if the intuitive sample mean estimator is statistically 'efficient'—that is, if it is the best possible estimator you can construct.", "problem": "A homogeneous Poisson process with rate parameter $\\lambda \\in (0,\\infty)$ models the count of jump events driving a jump-diffusion component within a stochastic differential equation over disjoint unit-length time windows. You observe $n$ independent counts $X_{1}, X_{2}, \\dots, X_{n}$, each corresponding to the number of jumps in a single unit-length window. Assume $X_{i} \\sim \\text{Poisson}(\\lambda)$ independently for $i \\in \\{1,2,\\dots,n\\}$.\n\nStarting from core definitions in statistical estimation, including the likelihood function for the Poisson model, the score function as the derivative of the log-likelihood, and the Fisher information as the expected negative second derivative of the log-likelihood, derive the Cramér–Rao Lower Bound (CRLB) for the variance of any unbiased estimator of $\\lambda$ based on $X_{1},\\dots,X_{n}$. Then derive the maximum likelihood estimator (MLE) for $\\lambda$, determine its expectation and variance, and assess whether it attains the CRLB.\n\nYour final answer must be a single closed-form analytic expression for the CRLB as a function of $\\lambda$ and $n$. No rounding is required.", "solution": "The problem requires the derivation of the Cramér–Rao Lower Bound (CRLB) for the variance of any unbiased estimator of the rate parameter $\\lambda$ of a Poisson distribution, given a sample of $n$ independent and identically distributed (i.i.d.) observations. It also asks for the derivation of the Maximum Likelihood Estimator (MLE) of $\\lambda$ and an assessment of its efficiency.\n\nLet $X_{1}, X_{2}, \\dots, X_{n}$ be a set of $n$ independent random variables, where each $X_{i}$ follows a Poisson distribution with parameter $\\lambda > 0$, denoted as $X_{i} \\sim \\text{Poisson}(\\lambda)$. The probability mass function (PMF) for a single observation $X_{i}$ is given by:\n$$P(X_{i} = k) = \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}$$\nfor $k \\in \\{0, 1, 2, \\dots\\}$.\n\nFirst, we construct the likelihood function, $L(\\lambda)$, which is the joint probability of observing the sample $x_{1}, x_{2}, \\dots, x_{n}$. Due to the independence of the observations, the likelihood function is the product of the individual PMFs:\n$$L(\\lambda; x_{1}, \\dots, x_{n}) = \\prod_{i=1}^{n} P(X_{i} = x_{i}) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}$$\n\nIt is computationally more convenient to work with the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$:\n$$ \\ell(\\lambda) = \\ln\\left(\\prod_{i=1}^{n} \\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}\\right) = \\sum_{i=1}^{n} \\ln\\left(\\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}\\right) $$\n$$ \\ell(\\lambda) = \\sum_{i=1}^{n} (x_{i}\\ln(\\lambda) - \\lambda - \\ln(x_{i}!)) $$\n$$ \\ell(\\lambda) = \\ln(\\lambda)\\left(\\sum_{i=1}^{n} x_{i}\\right) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_{i}!) $$\n\nThe score function, $S(\\lambda)$, is the first derivative of the log-likelihood function with respect to the parameter $\\lambda$:\n$$ S(\\lambda) = \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ \\ln(\\lambda)\\left(\\sum_{i=1}^{n} x_{i}\\right) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_{i}!) \\right] $$\n$$ S(\\lambda) = \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n $$\n\nNext, we calculate the Fisher Information, $I(\\lambda)$. The Fisher Information is defined as the negative of the expected value of the second derivative of the log-likelihood function. First, we compute the second derivative:\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = \\frac{\\partial S(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n \\right] = -\\frac{1}{\\lambda^2}\\left(\\sum_{i=1}^{n} x_{i}\\right) $$\n\nNow, we take the expectation of this quantity. For this step, we treat the observations $x_{i}$ as random variables $X_{i}$. We use the fact that for a Poisson-distributed random variable $X_{i}$, its expectation is $\\mathbb{E}[X_{i}] = \\lambda$.\n$$ I(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] = -\\mathbb{E}\\left[-\\frac{1}{\\lambda^2}\\left(\\sum_{i=1}^{n} X_{i}\\right)\\right] $$\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} \\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] $$\nBy linearity of expectation, $\\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\sum_{i=1}^{n} \\lambda = n\\lambda$.\nSubstituting this result back into the expression for the Fisher Information:\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} (n\\lambda) = \\frac{n}{\\lambda} $$\n\nThe Cramér–Rao Lower Bound (CRLB) gives a lower bound on the variance of any unbiased estimator $\\hat{\\lambda}$ of $\\lambda$. The bound is the reciprocal of the Fisher Information:\n$$ \\text{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)} $$\nTherefore, the CRLB for $\\lambda$ is:\n$$ \\text{CRLB} = \\frac{1}{I(\\lambda)} = \\frac{1}{n/\\lambda} = \\frac{\\lambda}{n} $$\n\nNow, let's derive the Maximum Likelihood Estimator (MLE) for $\\lambda$, denoted $\\hat{\\lambda}_{\\text{MLE}}$. We find the MLE by setting the score function to zero and solving for $\\lambda$:\n$$ S(\\lambda) = \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n = 0 $$\n$$ \\frac{1}{\\lambda}\\sum_{i=1}^{n} x_{i} = n $$\n$$ \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} = \\bar{x} $$\nThe MLE for $\\lambda$ is the sample mean of the observations. The second derivative of the log-likelihood, $-\\frac{1}{\\lambda^2}\\sum_{i=1}^{n} x_{i}$, is negative for $\\lambda>0$ and non-zero counts, confirming that this is a maximum.\n\nTo assess whether the MLE attains the CRLB, we must first check if it is an unbiased estimator and then compute its variance.\nThe expectation of the MLE is:\n$$ \\mathbb{E}[\\hat{\\lambda}_{\\text{MLE}}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n} = \\lambda $$\nSince $\\mathbb{E}[\\hat{\\lambda}_{\\text{MLE}}] = \\lambda$, the MLE is an unbiased estimator of $\\lambda$.\n\nThe variance of the MLE is:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) $$\nSince the $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_{i}) $$\nFor a Poisson distribution, we have $\\text{Var}(X_i) = \\lambda$.\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n^2} = \\frac{\\lambda}{n} $$\n\nComparing the variance of the MLE with the CRLB:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{\\lambda}{n} = \\text{CRLB} $$\nSince the variance of the unbiased MLE is equal to the Cramér–Rao Lower Bound, the MLE $\\hat{\\lambda}_{\\text{MLE}} = \\bar{X}$ is an efficient estimator for $\\lambda$.\n\nThe problem asks for the closed-form analytic expression for the CRLB. As derived, this is $\\frac{\\lambda}{n}$.", "answer": "$$\\boxed{\\frac{\\lambda}{n}}$$", "id": "3044313"}, {"introduction": "In realistic stochastic models, the intensity of jumps, $\\lambda$, is often not constant but changes with the state of the system. This poses a major challenge for numerical simulation: how large a time step $h$ can we take while ensuring we don't miss important jump events? This problem [@problem_id:3044306] demonstrates how to use properties of the Poisson distribution to design a robust, adaptive time-stepping algorithm. You will derive a rule that controls the probability of multiple jumps occurring within a single step, allowing your simulation to automatically adjust its step size for both accuracy and efficiency.", "problem": "Consider a one-dimensional Stochastic Differential Equation (SDE) with a pure-jump component whose jump intensity is state dependent, denoted by $\\lambda(x) \\ge 0$. You are implementing a jump-adapted Euler scheme over a single step $[t, t + h]$. Let $N_h$ denote the number of jumps that occur in this interval. Suppose that, based on local information at time $t$, you have computed an upper bound $\\overline{\\lambda} > 0$ that satisfies $\\lambda(X_s) \\le \\overline{\\lambda}$ for all $s \\in [t, t+h]$ on the step you contemplate taking. Your goal is to choose an adaptive step size $h$ so that the probability of observing more than one jump in the step is controlled by a user-specified tolerance $\\varepsilon \\in (0,1)$.\n\nStarting from first principles for counting processes and classical discrete distributions, use the following foundational facts:\n\n- If the jump intensity is bounded above by a constant $\\overline{\\lambda}$ on an interval of length $h$, then the jump count on that interval is stochastically dominated by a Poisson random variable with mean $\\mu = \\overline{\\lambda} h$.\n\n- Using only the definition of the Poisson distribution and standard inequalities for the exponential function, derive a bound for $\\mathbb{P}(N_h \\ge 2)$ in terms of $\\mu$ that holds for all $\\mu \\ge 0$.\n\nUse this bound to produce a conservative adaptive step-size prescription $h_{\\max}(\\varepsilon, \\overline{\\lambda})$ that guarantees $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$.\n\nProvide your final answer as a single closed-form symbolic expression for $h_{\\max}(\\varepsilon, \\overline{\\lambda})$ in terms of $\\varepsilon$ and $\\overline{\\lambda}$. Do not perform any numerical rounding. Express your final answer symbolically; no physical units are required in the final expression.", "solution": "The objective is to derive a conservative adaptive step-size prescription, denoted $h_{\\max}(\\varepsilon, \\overline{\\lambda})$, which guarantees that the probability of observing two or more jumps in a time interval of length $h$ is no greater than a specified tolerance $\\varepsilon$. The condition to be satisfied is $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$, where $N_h$ is the number of jumps in the interval $[t, t+h]$.\n\nThe problem provides a foundational fact: if the state-dependent jump intensity $\\lambda(X_s)$ is bounded above by a constant $\\overline{\\lambda}$ for $s \\in [t, t+h]$, then the jump count $N_h$ in this interval is stochastically dominated by a Poisson random variable, which we shall call $M$, with mean parameter $\\mu = \\overline{\\lambda} h$. Stochastic dominance implies that for any integer $k \\ge 0$, the cumulative distribution functions satisfy $\\mathbb{P}(N_h \\le k) \\ge \\mathbb{P}(M \\le k)$, which is equivalent to $\\mathbb{P}(N_h > k) \\le \\mathbb{P}(M > k)$ or $\\mathbb{P}(N_h \\ge k) \\le \\mathbb{P}(M \\ge k)$.\n\nTo satisfy the target condition $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$, it is therefore sufficient to enforce a more stringent condition on the dominating variable $M$:\n$$\n\\mathbb{P}(M \\ge 2) \\le \\varepsilon\n$$\n\nNext, we must derive a usable bound for $\\mathbb{P}(M \\ge 2)$ in terms of $\\mu$. The probability mass function of a Poisson random variable $M$ with mean $\\mu$ is given by $P(M=k) = \\frac{e^{-\\mu} \\mu^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$. The probability of observing two or more events is the complement of observing zero or one event:\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\mathbb{P}(M < 2) = 1 - (\\mathbb{P}(M=0) + \\mathbb{P}(M=1))\n$$\nSubstituting the Poisson probabilities:\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\left( \\frac{e^{-\\mu} \\mu^0}{0!} + \\frac{e^{-\\mu} \\mu^1}{1!} \\right) = 1 - (e^{-\\mu} + \\mu e^{-\\mu}) = 1 - e^{-\\mu}(1+\\mu)\n$$\nThis expression is exact. However, the problem directs us to derive a simpler bound. We can express $\\mathbb{P}(M \\ge 2)$ as an infinite series:\n$$\n\\mathbb{P}(M \\ge 2) = \\sum_{k=2}^{\\infty} \\frac{e^{-\\mu} \\mu^k}{k!} = e^{-\\mu} \\left( \\frac{\\mu^2}{2!} + \\frac{\\mu^3}{3!} + \\frac{\\mu^4}{4!} + \\dots \\right)\n$$\nWe can establish an upper bound for this series. Factoring out the term $\\frac{\\mu^2}{2}$:\n$$\n\\mathbb{P}(M \\ge 2) = e^{-\\mu} \\frac{\\mu^2}{2} \\left( 1 + \\frac{\\mu}{3} + \\frac{\\mu^2}{3 \\cdot 4} + \\frac{\\mu^3}{3 \\cdot 4 \\cdot 5} + \\dots \\right) = e^{-\\mu} \\frac{\\mu^2}{2} \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!}\n$$\nFor any integer $j \\ge 0$, we have the inequality $(j+2)! = (j+2)(j+1)j! \\ge 2 \\cdot j!$. Therefore, $\\frac{1}{(j+2)!} \\le \\frac{1}{2 \\cdot j!}$. Using this, we can bound the sum:\n$$\n\\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!} \\le \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{2 j!} = \\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!}\n$$\nThe resulting sum on the right-hand side is the Taylor series expansion for the exponential function, $\\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!} = e^{\\mu}$.\nSubstituting this inequality back into our expression for $\\mathbb{P}(M \\ge 2)$, we obtain the bound:\n$$\n\\mathbb{P}(M \\ge 2) \\le e^{-\\mu} \\frac{\\mu^2}{2} (e^{\\mu}) = \\frac{\\mu^2}{2}\n$$\nThis inequality, $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$, holds for all $\\mu \\ge 0$. To formally verify this, let $f(\\mu) = \\frac{\\mu^2}{2} - \\mathbb{P}(M \\ge 2) = \\frac{\\mu^2}{2} - (1 - (1+\\mu)e^{-\\mu})$. At $\\mu=0$, $f(0) = 0 - (1 - (1)e^0) = 0$. The derivative with respect to $\\mu$ is:\n$$\nf'(\\mu) = \\frac{d}{d\\mu} \\left( \\frac{\\mu^2}{2} - 1 + e^{-\\mu} + \\mu e^{-\\mu} \\right) = \\mu - e^{-\\mu} + (e^{-\\mu} - \\mu e^{-\\mu}) = \\mu - \\mu e^{-\\mu} = \\mu(1 - e^{-\\mu})\n$$\nFor all $\\mu > 0$, we have $\\mu > 0$ and $e^{-\\mu} < 1$, which implies $(1 - e^{-\\mu}) > 0$. Thus, $f'(\\mu) > 0$ for $\\mu > 0$. Since $f(0)=0$ and the function is monotonically increasing for $\\mu \\ge 0$, we conclude that $f(\\mu) \\ge 0$ for all $\\mu \\ge 0$, confirming that $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$ is a valid upper bound for all non-negative $\\mu$.\n\nWith this conservative bound, we can satisfy the control objective $\\mathbb{P}(M \\ge 2) \\le \\varepsilon$ by enforcing the stricter condition:\n$$\n\\frac{\\mu^2}{2} \\le \\varepsilon\n$$\nNow, substitute the definition of the mean, $\\mu = \\overline{\\lambda} h$:\n$$\n\\frac{(\\overline{\\lambda} h)^2}{2} \\le \\varepsilon\n$$\nWe solve this inequality for the step size $h$. Given that $\\overline{\\lambda} > 0$ and $h \\ge 0$:\n$$\n(\\overline{\\lambda} h)^2 \\le 2\\varepsilon\n$$\n$$\n\\overline{\\lambda} h \\le \\sqrt{2\\varepsilon}\n$$\n$$\nh \\le \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\nThis inequality provides an upper limit on the step size $h$ that guarantees our probabilistic constraint is met. The problem asks for the maximum such step size, $h_{\\max}(\\varepsilon, \\overline{\\lambda})$, which is the upper boundary of this interval.\n$$\nh_{\\max}(\\varepsilon, \\overline{\\lambda}) = \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\nThis is the desired conservative adaptive step-size prescription.", "answer": "$$\n\\boxed{\\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}}\n$$", "id": "3044306"}]}