{"hands_on_practices": [{"introduction": "The concept of quadratic variation is a cornerstone of stochastic calculus, distinguishing it from classical calculus. This exercise [@problem_id:3066772] explores the crucial conditions under which the sum of squared increments of a Brownian motion converges to its theoretical value. By analyzing a carefully constructed sequence of partitions whose mesh size does not shrink to zero, you will discover why this condition is essential and witness how its violation prevents convergence to the expected deterministic constant.", "problem": "Let $\\{B_{t}\\}_{t \\in [0,1]}$ be a standard Brownian motion (also called Wiener process), that is, a continuous stochastic process with $B_{0} = 0$, independent increments, and for any $0 \\leq s < t \\leq 1$, the increment $B_{t} - B_{s}$ is normally distributed with mean $0$ and variance $t-s$. For a finite partition $\\Pi = \\{0 = t_{0} < t_{1} < \\cdots < t_{m} = 1\\}$ of $[0,1]$, define the mesh size by $\\|\\Pi\\| = \\max_{0 \\leq k < m}(t_{k+1} - t_{k})$ and the discrete quadratic variation of $B$ along $\\Pi$ by\n$$\nV(B,\\Pi) = \\sum_{k=0}^{m-1} \\left(B_{t_{k+1}} - B_{t_{k}}\\right)^{2}.\n$$\nA core result in stochastic differential equations is that, for Brownian motion, if $(\\Pi_{n})_{n \\geq 1}$ is a sequence of partitions with $\\|\\Pi_{n}\\| \\to 0$, then $V(B,\\Pi_{n})$ converges in probability to the quadratic variation $[B]_{1} = 1$. This problem asks you to demonstrate the necessity of the mesh condition by constructing a sequence of partitions whose mesh size does not go to zero and computing the limiting expression of the discrete quadratic variation.\n\nDefine, for each $n \\in \\mathbb{N}$, the irregular partition\n$$\n\\Pi_{n} = \\left\\{0,\\ \\frac{1}{2},\\ \\frac{1}{2} + \\frac{1}{2n},\\ \\frac{1}{2} + \\frac{2}{2n},\\ \\ldots,\\ \\frac{1}{2} + \\frac{n}{2n} = 1\\right\\}.\n$$\nNote that $\\|\\Pi_{n}\\| = \\frac{1}{2}$ for all $n$.\n\nStarting from the definitions above and the basic properties of Brownian increments, do the following:\n- Derive the limiting expression (in probability) of $V(B,\\Pi_{n})$ as $n \\to \\infty$.\n- Explain, in terms of modes of convergence (e.g., convergence in probability versus almost sure convergence), why this limiting expression shows that the mesh condition $\\|\\Pi_{n}\\| \\to 0$ is necessary to recover $[B]_{1} = 1$ from discrete sums.\n\nYour final answer must be the single closed-form analytical expression to which $V(B,\\Pi_{n})$ converges in probability as $n \\to \\infty$. Do not provide an inequality or an equation as your final answer. No rounding is required.", "solution": "The problem is valid. It is a well-posed, scientifically grounded question in the field of stochastic calculus, specifically concerning the quadratic variation of Brownian motion. All terms are defined, and the premises are standard results from the theory of stochastic processes.\n\nThe problem asks for the limit in probability of the discrete quadratic variation $V(B, \\Pi_n)$ for a specific sequence of partitions $\\Pi_n$ and to explain the role of the mesh condition.\n\nThe sequence of partitions is given by\n$$\n\\Pi_{n} = \\left\\{0,\\ \\frac{1}{2},\\ \\frac{1}{2} + \\frac{1}{2n},\\ \\frac{1}{2} + \\frac{2}{2n},\\ \\ldots,\\ \\frac{1}{2} + \\frac{n}{2n} = 1\\right\\}.\n$$\nThe partition $\\Pi_n$ divides the interval $[0,1]$ into $n+1$ subintervals. The first subinterval is $[0, 1/2]$, and the subsequent $n$ subintervals are of the form $[\\frac{1}{2} + \\frac{k-1}{2n}, \\frac{1}{2} + \\frac{k}{2n}]$ for $k = 1, \\ldots, n$.\n\nThe discrete quadratic variation $V(B, \\Pi_n)$ is the sum of the squared increments of the Brownian motion $B_t$ over these subintervals. We can decompose this sum into two parts, corresponding to the interval $[0, 1/2]$ and the interval $[1/2, 1]$.\n$$\nV(B,\\Pi_{n}) = \\left(B_{1/2} - B_{0}\\right)^{2} + \\sum_{k=1}^{n} \\left(B_{\\frac{1}{2} + \\frac{k}{2n}} - B_{\\frac{1}{2} + \\frac{k-1}{2n}}\\right)^{2}.\n$$\nLet's analyze each part separately.\n\nThe first term involves the increment over $[0, 1/2]$. Since $B_t$ is a standard Brownian motion, we have $B_0=0$. Therefore, this term is simply $B_{1/2}^2$.\n$$\n\\left(B_{1/2} - B_{0}\\right)^{2} = (B_{1/2} - 0)^2 = B_{1/2}^2.\n$$\nThis term is a random variable. Specifically, since $B_t - B_s \\sim \\mathcal{N}(0, t-s)$, we have $B_{1/2} \\sim \\mathcal{N}(0, 1/2)$. The first term, $B_{1/2}^2$, does not depend on $n$.\n\nThe second part is a sum over the partition of the interval $[1/2, 1]$. Let us define this sum as $S_n$:\n$$\nS_{n} = \\sum_{k=1}^{n} \\left(B_{\\frac{1}{2} + \\frac{k}{2n}} - B_{\\frac{1}{2} + \\frac{k-1}{2n}}\\right)^{2}.\n$$\nThis expression is the discrete quadratic variation of $B_t$ calculated over the interval $[1/2, 1]$ using the partition $P'_n = \\{\\frac{1}{2}, \\frac{1}{2}+\\frac{1}{2n}, \\ldots, 1\\}$. The mesh of this partition is\n$$\n\\|P'_n\\| = \\max_{1 \\leq k \\leq n} \\left( \\left(\\frac{1}{2} + \\frac{k}{2n}\\right) - \\left(\\frac{1}{2} + \\frac{k-1}{2n}\\right) \\right) = \\frac{1}{2n}.\n$$\nAs $n \\to \\infty$, the mesh size $\\|P'_n\\|$ converges to $0$.\n\nAccording to the fundamental result on the quadratic variation of Brownian motion, if the mesh of a sequence of partitions of an interval $[a, b]$ tends to zero, the corresponding discrete quadratic variation converges in probability to the length of the interval, $b-a$. In our case, the interval is $[1/2, 1]$, so its length is $1 - 1/2 = 1/2$.\nTherefore, as $n \\to \\infty$, the sum $S_n$ converges in probability to $1/2$:\n$$\nS_n \\xrightarrow{p} \\frac{1}{2}.\n$$\nHere, $\\xrightarrow{p}$ denotes convergence in probability.\n\nNow we can determine the limit of $V(B, \\Pi_n)$. We have\n$$\nV(B, \\Pi_n) = B_{1/2}^2 + S_n.\n$$\nSince $B_{1/2}^2$ is a random variable that does not depend on $n$, and $S_n$ converges in probability to the constant $1/2$, we can use the properties of convergence in probability (or Slutsky's theorem). If a sequence of random variables $X_n \\xrightarrow{p} c$, where $c$ is a constant, and $Y$ is another random variable, then $X_n+Y \\xrightarrow{p} c+Y$.\nIn our case, $Y = B_{1/2}^2$ and $c = 1/2$. Thus,\n$$\nV(B, \\Pi_n) \\xrightarrow{p} B_{1/2}^2 + \\frac{1}{2}.\n$$\nThis is the limiting expression for the discrete quadratic variation for the given sequence of partitions.\n\nThe second part of the problem asks to explain why this result demonstrates the necessity of the mesh condition $\\|\\Pi_n\\| \\to 0$ to recover the true quadratic variation $[B]_1 = 1$.\n\nThe standard result states that if $(\\Pi'_n)$ is a sequence of partitions of $[0,1]$ with mesh $\\|\\Pi'_n\\| \\to 0$, then $V(B, \\Pi'_n) \\xrightarrow{p} [B]_1 = 1$. Convergence in probability to a constant, such as $1$, means that for any arbitrarily small $\\epsilon > 0$, the probability that $V(B, \\Pi'_n)$ is outside the interval $(1-\\epsilon, 1+\\epsilon)$ tends to zero as $n \\to \\infty$. The probability distribution of $V(B, \\Pi'_n)$ becomes increasingly concentrated at the value $1$.\n\nIn the case of the given sequence of partitions $\\Pi_n$, the mesh size is $\\|\\Pi_n\\| = \\max\\{1/2, 1/(2n)\\} = 1/2$ for all $n \\geq 1$. This mesh size does not converge to $0$. We found that the limit of $V(B, \\Pi_n)$ is not the constant $1$, but the random variable $L = B_{1/2}^2 + 1/2$.\n\nThe distribution of this limit $L$ is not a point mass at $1$. Since $B_{1/2} \\sim \\mathcal{N}(0, 1/2)$, $L$ is a random variable with a continuous distribution for $L > 1/2$. For example, the probability that $L$ equals $1$ is $\\mathbb{P}(L=1) = \\mathbb{P}(B_{1/2}^2 + 1/2 = 1) = \\mathbb{P}(B_{1/2}^2 = 1/2) = 0$, since $B_{1/2}$ has a continuous distribution. The value of the limit depends on the particular realization (path) of the Brownian motion, specifically on its value at time $t=1/2$.\n\nThe failure to recover the deterministic value $[B]_1 = 1$ is a direct consequence of the mesh condition not being satisfied. The interval $[0, 1/2]$ is never refined in the sequence of partitions $\\Pi_n$. Its contribution to the sum, $(B_{1/2}-B_0)^2$, remains a random term in the limit. While the sum over the interval $[1/2, 1]$ does converge to the length of that interval ($1/2$) because the partition becomes finer there, the \"frozen\" first interval prevents the total sum from converging to the total length of $[0,1]$. This demonstrates that the requirement for the mesh to shrink to zero is essential to ensure that the contributions from all parts of the interval are properly averaged out, leading to a deterministic limit. The failure of this condition for even one subinterval is enough to introduce randomness into the final limit, thus failing to recover the true quadratic variation.", "answer": "$$\n\\boxed{B_{1/2}^2 + \\frac{1}{2}}\n$$", "id": "3066772"}, {"introduction": "Analyzing the convergence of numerical methods is a central task in the study of SDEs. This practice [@problem_id:3066786] focuses on proving the strong convergence of the fundamental Euler-Maruyama scheme when applied to the Ornstein-Uhlenbeck process, a ubiquitous model in science and finance. Working through this problem will provide you with a blueprint for error analysis, demonstrating how to decompose the approximation error and determine the convergence rate in the strong (pathwise) sense.", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE) on a finite time horizon $[0,T]$:\n$$\ndX_t=-\\lambda X_t\\,dt+\\sigma\\,dW_t,\\quad X_0=x_0,\n$$\nwhere $W_t$ is a standard Brownian motion, and $\\lambda>0$, $\\sigma>0$, $x_0\\in\\mathbb{R}$, and $T>0$ are fixed constants. Let the time step be $h=T/N$ with $N\\in\\mathbb{N}$, and define the Euler–Maruyama time-discrete approximation $\\{X_k^h\\}_{k=0}^{N}$ by\n$$\nX_{k+1}^h=X_k^h-\\lambda X_k^h\\,h+\\sigma\\,\\Delta W_k,\\quad \\Delta W_k:=W_{t_{k+1}}-W_{t_k},\\quad t_k:=kh,\n$$\nwith $X_0^h=x_0$. Define the continuous-time Euler–Maruyama interpolation $\\bar{X}_t^h$ on each subinterval $[t_k,t_{k+1})$ as the unique strong solution of\n$$\nd\\bar{X}_t^h=-\\lambda X_k^h\\,dt+\\sigma\\,dW_t,\\quad \\bar{X}_{t_k}^h=X_k^h,\\quad t\\in[t_k,t_{k+1}).\n$$\nAdopt the following strong mean-supremum error norm on $[0,T]$:\n$$\n\\|X-\\bar{X}^h\\|_{2,\\infty}:=\\Big(\\mathbb{E}\\big[\\sup_{0\\le t\\le T}|X_t-\\bar{X}_t^h|^2\\big]\\Big)^{1/2}.\n$$\nStarting from first principles, including the definition of strong convergence, stability and moment bounds for the Euler–Maruyama method on linear SDEs, and standard inequalities for stochastic processes, derive an upper bound of the form\n$$\n\\|X-\\bar{X}^h\\|_{2,\\infty}\\le C\\,h^{p},\n$$\nfor a constant $C$ independent of $h$ (but possibly depending on $\\lambda$, $\\sigma$, $x_0$, and $T$). Determine the order exponent $p$ that this scheme attains for the Ornstein–Uhlenbeck SDE in the mean supremum norm defined above. Report only the value of $p$ as your final answer.", "solution": "The user wants to determine the order of strong convergence $p$ for the continuous-time Euler–Maruyama interpolation of the Ornstein–Uhlenbeck SDE, measured in the mean-supremum norm. The problem is well-posed and scientifically grounded in the theory of numerical analysis for stochastic differential equations.\n\nThe Ornstein–Uhlenbeck SDE is given by\n$$dX_t = -\\lambda X_t dt + \\sigma dW_t, \\quad X_0 = x_0$$\non the time interval $[0, T]$, where $\\lambda > 0$, $\\sigma > 0$. This is a linear SDE with constant coefficients.\n\nThe discrete Euler–Maruyama approximation $\\{X_k^h\\}_{k=0}^N$ is defined by the recurrence\n$$X_{k+1}^h = X_k^h - \\lambda X_k^h h + \\sigma \\Delta W_k, \\quad X_0^h = x_0$$\nwhere $h=T/N$, $t_k=kh$, and $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$.\n\nThe continuous-time Euler–Maruyama interpolation $\\bar{X}_t^h$ is defined for $t \\in [t_k, t_{k+1})$ by the SDE\n$$d\\bar{X}_t^h = -\\lambda X_k^h dt + \\sigma dW_t, \\quad \\bar{X}_{t_k}^h = X_k^h$$\nIntegrating this equation gives the explicit form for $t \\in [t_k, t_{k+1})$:\n$$\\bar{X}_t^h = X_k^h - \\lambda X_k^h (t-t_k) + \\sigma (W_t - W_{t_k})$$\n\nWe want to find the exponent $p$ in the bound for the strong mean-supremum error:\n$$\\|X - \\bar{X}^h\\|_{2,\\infty} = \\Big(\\mathbb{E}\\big[\\sup_{0 \\le t \\le T}|X_t - \\bar{X}_t^h|^2\\big]\\Big)^{1/2} \\le C h^p$$\n\nTo analyze this error, we decompose it by introducing an auxiliary process, $\\bar{X}_t^*$. This process uses the same continuous interpolation scheme but starts from the exact solution $X_{t_k}$ at the beginning of each interval $[t_k, t_{k+1})$. For $t \\in [t_k, t_{k+1})$, $\\bar{X}_t^*$ is the solution to\n$$d\\bar{X}_t^* = -\\lambda X_{t_k} dt + \\sigma dW_t, \\quad \\bar{X}_{t_k}^* = X_{t_k}$$\nBy the triangle inequality for the $\\|\\cdot\\|_{2,\\infty}$ norm, the total error can be bounded as:\n$$\\|X - \\bar{X}^h\\|_{2,\\infty} \\le \\|X - \\bar{X}^*\\|_{2,\\infty} + \\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty}$$\nWe will analyze the order of convergence for each term on the right-hand side.\n\n**1. Analysis of the Interpolation Error Term $\\|X - \\bar{X}^*\\|_{2,\\infty}$**\n\nThis term represents the error purely due to the continuous interpolation, assuming no error at the grid points. The squared norm is\n$$\\|X - \\bar{X}^*\\|_{2,\\infty}^2 = \\mathbb{E}\\left[\\sup_{0 \\le t \\le T} |X_t - \\bar{X}_t^*|^2\\right] = \\mathbb{E}\\left[\\max_{0 \\le k < N} \\sup_{t \\in [t_k, t_{k+1})} |X_t - \\bar{X}_t^*|^2\\right]$$\nFor $t \\in [t_k, t_{k+1})$, the local interpolation error $e_t^* = X_t - \\bar{X}_t^*$ satisfies\n$$de_t^* = dX_t - d\\bar{X}_t^* = (-\\lambda X_t dt + \\sigma dW_t) - (-\\lambda X_{t_k} dt + \\sigma dW_t) = -\\lambda (X_t - X_{t_k}) dt$$\nSince $e_{t_k}^*=X_{t_k}-\\bar{X}_{t_k}^*=0$, we integrate to get\n$$e_t^* = -\\lambda \\int_{t_k}^t (X_s - X_{t_k}) ds$$\nWe need to bound the mean-supremum of this local error.\n$$\\mathbb{E}\\left[\\sup_{t \\in [t_k, t_{k+1})} |e_t^*|^2\\right] = \\lambda^2 \\mathbb{E}\\left[\\sup_{t \\in [t_k, t_{k+1})} \\left|\\int_{t_k}^t (X_s - X_{t_k}) ds\\right|^2\\right]$$\nBy the Cauchy-Schwarz inequality,\n$$\\left|\\int_{t_k}^t (X_s - X_{t_k}) ds\\right|^2 \\le (t-t_k) \\int_{t_k}^t |X_s - X_{t_k}|^2 ds \\le h \\int_{t_k}^{t_{k+1}} |X_s - X_{t_k}|^2 ds$$\nThus,\n$$\\mathbb{E}\\left[\\sup_{t \\in [t_k, t_{k+1})} |e_t^*|^2\\right] \\le \\lambda^2 h \\int_{t_k}^{t_{k+1}} \\mathbb{E}\\left[|X_s - X_{t_k}|^2\\right] ds$$\nFor an Ito process with bounded moments of its coefficients (which is true for the Ornstein-Uhlenbeck process), we have the standard estimate $\\mathbb{E}[|X_s - X_{t_k}|^2] \\le K(s-t_k)$ for some constant $K$.\n$$\\mathbb{E}\\left[\\sup_{t \\in [t_k, t_{k+1})} |e_t^*|^2\\right] \\le \\lambda^2 h \\int_{t_k}^{t_{k+1}} K(s-t_k) ds = \\lambda^2 h K \\left[\\frac{(s-t_k)^2}{2}\\right]_{t_k}^{t_{k+1}} = \\frac{K\\lambda^2}{2} h^3 = O(h^3)$$\nNow we bound the global interpolation error. Using the property $\\mathbb{E}[\\max(Z_k)] \\le \\sum \\mathbb{E}[Z_k]$ for non-negative random variables $Z_k$,\n$$\\|X - \\bar{X}^*\\|_{2,\\infty}^2 = \\mathbb{E}\\left[\\max_{k} \\sup_{t \\in [t_k, t_{k+1})} |e_t^*|^2\\right] \\le \\sum_{k=0}^{N-1} \\mathbb{E}\\left[\\sup_{t \\in [t_k, t_{k+1})} |e_t^*|^2\\right] = \\sum_{k=0}^{N-1} O(h^3) = N \\cdot O(h^3)$$\nSince $N=T/h$, we have $\\|X - \\bar{X}^*\\|_{2,\\infty}^2 = (T/h) \\cdot O(h^3) = O(h^2)$. Taking the square root, we get\n$$\\|X - \\bar{X}^*\\|_{2,\\infty} = O(h)$$\nThis implies that the interpolation scheme itself has a local error of order $h^{3/2}$ and a global error of order $h^1$.\n\n**2. Analysis of the Propagated Error Term $\\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty}$**\n\nThis term measures how the error from the discrete Euler-Maruyama scheme propagates through the continuous interpolation. Let $\\delta_t = \\bar{X}_t^* - \\bar{X}_t^h$. For $t \\in [t_k, t_{k+1})$,\n$$d\\delta_t = d\\bar{X}_t^* - d\\bar{X}_t^h = -\\lambda X_{t_k} dt + \\sigma dW_t - (-\\lambda X_k^h dt + \\sigma dW_t) = -\\lambda (X_{t_k} - X_k^h) dt$$\nLet $\\epsilon_k = X_{t_k} - X_k^h$ be the error of the discrete EM scheme at time $t_k$. At the start of the interval, $\\delta_{t_k} = \\bar{X}_{t_k}^* - \\bar{X}_{t_k}^h = X_{t_k} - X_k^h = \\epsilon_k$.\nIntegrating the SDE for $\\delta_t$ gives\n$$\\delta_t = \\delta_{t_k} - \\lambda \\int_{t_k}^t \\epsilon_k ds = \\epsilon_k - \\lambda \\epsilon_k (t-t_k) = \\epsilon_k(1 - \\lambda(t-t_k))$$\nNow we bound the squared norm:\n$$\\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty}^2 = \\mathbb{E}\\left[\\sup_{0 \\le t \\le T} |\\delta_t|^2\\right] = \\mathbb{E}\\left[\\max_{0 \\le k < N} \\sup_{t \\in [t_k, t_{k+1})} |\\epsilon_k(1 - \\lambda(t-t_k))|^2\\right]$$\nThe term $(1-\\lambda(t-t_k))$ is bounded by a constant independent of $h$ for $t \\in [0, T]$ (e.g., $\\max(1, |1-\\lambda T|)$). Therefore,\n$$\\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty}^2 \\le C \\mathbb{E}\\left[\\max_{0 \\le k < N} |\\epsilon_k|^2\\right]$$\nWe must now invoke a standard result for the strong convergence of the discrete Euler-Maruyama scheme. For SDEs with globally Lipschitz coefficients (which the OU process satisfies), the strong sup-norm convergence rate is $1/2$. That is,\n$$\\left(\\mathbb{E}\\left[\\max_{0 \\le k < N} |X_{t_k} - X_k^h|^2\\right]\\right)^{1/2} = O(h^{1/2})$$\nThis implies $\\mathbb{E}\\left[\\max_{0 \\le k < N} |\\epsilon_k|^2\\right] = O(h)$. It is important to note that even for linear SDEs, where the strong pointwise error is of order $1$ (i.e., $\\mathbb{E}[|X_{t_k} - X_k^h|^2] = O(h^2)$), the strong supremum (or pathwise) error is limited to order $1/2$ by the nature of approximating the Brownian path.\nSubstituting this result, we get\n$$\\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty}^2 \\le C \\cdot O(h) = O(h)$$\nTaking the square root gives\n$$\\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty} = O(h^{1/2})$$\n\n**3. Conclusion**\n\nCombining the bounds for the two components of the error:\n$$\\|X - \\bar{X}^h\\|_{2,\\infty} \\le \\|X - \\bar{X}^*\\|_{2,\\infty} + \\|\\bar{X}^* - \\bar{X}^h\\|_{2,\\infty} = O(h) + O(h^{1/2})$$\nThe overall rate of convergence is determined by the slower of the two terms, which is $O(h^{1/2})$.\nTherefore, we have $\\|X - \\bar{X}^h\\|_{2,\\infty} \\le C h^{1/2}$ for some constant $C$. The order exponent is $p = 1/2$.\nThe limiting factor is not the continuous interpolation itself, but the underlying discrete Euler-Maruyama scheme upon which the interpolation is built. The pathwise error of the discrete scheme contaminates the overall continuous-time approximation and limits its convergence rate to $p = 1/2$.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3066786"}, {"introduction": "After establishing when a numerical method works, it is equally important to understand when it fails. This problem [@problem_id:3066779] serves as a critical counterexample, investigating the behavior of the Euler-Maruyama scheme for an SDE whose coefficients violate the standard conditions required for strong convergence. By dissecting a single step of the scheme, you will gain a tangible understanding of how superlinear growth in the drift can lead to instability, highlighting the importance of verifying theoretical assumptions before trusting numerical results.", "problem": "Consider a one-dimensional stochastic differential equation (SDE) driven by Standard Brownian Motion (SBM), $W_t$, on a fixed time interval $[0,T]$, with drift coefficient $b$ and diffusion coefficient $\\sigma$. The Euler–Maruyama (EM) method is the explicit time discretization defined by the recursion $Y_{n+1} = Y_n + b(Y_n)\\,h + \\sigma(Y_n)\\,\\Delta W_n$, where $h = T/N$ is the uniform step size, $N \\in \\mathbb{N}$, and $\\Delta W_n = W_{(n+1)h} - W_{nh}$. Strong convergence of a numerical scheme is defined by the requirement that $\\mathbb{E}\\!\\left[|X_T - Y_N|^p\\right] \\to 0$ as $h \\to 0$ for some $p \\geq 1$, where $X_t$ is the exact solution of the SDE.\n\nStarting from the fundamental definitions of strong convergence, state broad structural conditions on $b$ and $\\sigma$ under which the EM method is known to converge strongly, and explain why violations such as superlinear growth in $b$ may lead to a failure of strong convergence. Then construct a concrete SDE example that exhibits superlinear growth in the drift and explain, in terms of the distribution of Brownian increments and the explicit EM update, why rare but sufficiently large increments can trigger large deviations of the EM iterates, undermining strong convergence.\n\nFor a specific calculation tied to your example, take the SDE\n$$\n\\mathrm{d}X_t = -X_t^{3}\\,\\mathrm{d}t + X_t\\,\\mathrm{d}W_t, \\quad X_0 = 1,\n$$\nand the Euler–Maruyama scheme\n$$\nY_{n+1} = Y_n - h\\,Y_n^{3} + Y_n\\,\\Delta W_n, \\quad Y_0 = 1,\n$$\non a single step from $t=0$ to $t=h$. Use the fact that $\\Delta W_0 \\sim \\mathcal{N}(0,h)$ and compute, in closed form, the exact probability that the one-step EM iterate satisfies the inequality $Y_1 \\geq 1 - h + 3\\,h^{1/2}$, expressed in terms of the complementary error function. Your final answer must be a single analytical expression. No numerical rounding is required.", "solution": "The problem is evaluated as valid, as it is scientifically grounded in the theory of numerical stochastic differential equations, is well-posed with a clear objective, and contains no ambiguities, contradictions, or factual errors.\n\nThe problem investigates the strong convergence properties of the Euler–Maruyama (EM) method for a one-dimensional stochastic differential equation (SDE) of the form\n$$\n\\mathrm{d}X_t = b(X_t)\\,\\mathrm{d}t + \\sigma(X_t)\\,\\mathrm{d}W_t, \\quad t \\in [0,T].\n$$\nThe Euler–Maruyama scheme provides a numerical approximation, $Y_n \\approx X_{t_n}$, where $t_n = n h$, through the recurrence\n$$\nY_{n+1} = Y_n + b(Y_n)\\,h + \\sigma(Y_n)\\,\\Delta W_n,\n$$\nwith $h=T/N$ being the step size and $\\Delta W_n = W_{(n+1)h} - W_{nh}$ being the increments of a standard Brownian motion.\n\nStrong convergence requires that $\\mathbb{E}\\!\\left[|X_T - Y_N|^p\\right] \\to 0$ as $h \\to 0$ for some $p \\geq 1$. The standard theorem establishing strong convergence of the Euler–Maruyama method relies on specific structural conditions on the drift coefficient $b$ and the diffusion coefficient $\\sigma$. Specifically, if both $b$ and $\\sigma$ satisfy a global Lipschitz condition and a linear growth condition, then the EM method converges strongly with order $1/2$. The conditions are as follows:\n\n1.  **Global Lipschitz Condition:** There exists a constant $K_1 > 0$ such that for all $x, y \\in \\mathbb{R}$,\n    $$\n    |b(x) - b(y)| + |\\sigma(x) - \\sigma(y)| \\leq K_1 |x - y|.\n    $$\n2.  **Linear Growth Condition:** There exists a constant $K_2 > 0$ such that for all $x \\in \\mathbb{R}$,\n    $$\n    |b(x)|^2 + |\\sigma(x)|^2 \\leq K_2^2 (1 + |x|^2).\n    $$\n\nUnder these conditions, it can be proven that for any $p \\geq 2$, there exists a constant $C$ (dependent on $T$ and $p$) such that the strong error is bounded:\n$$\n\\left(\\mathbb{E}\\!\\left[\\max_{0 \\leq n \\leq N} |X_{t_n} - Y_n|^p\\right]\\right)^{1/p} \\leq C h^{1/2}.\n$$\nThis implies $\\mathbb{E}\\!\\left[|X_T - Y_N|^p\\right] \\to 0$ as $h \\to 0$, establishing strong convergence.\n\nThe global Lipschitz condition is crucial for controlling the growth of the error between the true solution and the numerical approximation. When coefficients, particularly the drift $b$, exhibit superlinear growth, this condition is violated. This can lead to a failure of strong convergence. A canonical example is the SDE presented in the problem:\n$$\n\\mathrm{d}X_t = -X_t^3\\,\\mathrm{d}t + X_t\\,\\mathrm{d}W_t.\n$$\nHere, the drift is $b(x) = -x^3$ and the diffusion is $\\sigma(x) = x$. The diffusion coefficient $\\sigma(x)$ is globally Lipschitz. However, the drift $b(x)$ is not, as $|b(x) - b(y)| = |y^3 - x^3| = |y-x||y^2+xy+x^2|$, which cannot be bounded by $K_1|y-x|$ for all $x,y$. The drift also exhibits superlinear growth, $|b(x)| = |x|^3$.\n\nThe failure of strong convergence for the corresponding EM scheme, $Y_{n+1} = Y_n - h\\,Y_n^3 + Y_n\\,\\Delta W_n$, can be understood by analyzing the behavior of the iterates. The increments $\\Delta W_n$ are independent and identically distributed normal random variables with mean $0$ and variance $h$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0,h)$. While most increments are small (of order $h^{1/2}$), the Gaussian distribution has tails extending to infinity, meaning that very large increments are possible, albeit with low probability.\n\nConsider an iterate $Y_n$. The next iterate $Y_{n+1}$ is determined by a competition between the deterministic drift term, $-h\\,Y_n^3$, and the stochastic diffusion term, $Y_n\\,\\Delta W_n$. For small $h$, the standard deviation of the diffusion term is proportional to $h^{1/2}$, while the magnitude of the drift term is proportional to $h$. Since $h \\ll h^{1/2}$ for $h < 1$, the random fluctuations dominate the systematic drift over a single step.\n\nIf a rare event occurs where a large Brownian increment $\\Delta W_n$ is realized, the term $Y_n\\,\\Delta W_n$ can cause a massive change in the value of the iterate, potentially sending $Y_{n+1}$ to a very large value. Although the drift $b(x)=-x^3$ is stabilizing (it pushes large positive values down and large negative values up), its effect is of order $h$. A large random kick of order $h^{1/2}$ can overwhelm this stabilizing influence, leading to moments of the numerical solution, such as $\\mathbb{E}[|Y_n|^p]$, growing without bound as the simulation progresses. For the EM method to converge strongly, the moments of the numerical solution must remain bounded. The superlinear drift, combined with the unbounded nature of Gaussian increments, allows the EM iterates to explore regions of the state space far from the true solution, causing the expected error $\\mathbb{E}[|X_T - Y_N|^p]$ to fail to converge to zero.\n\nWe now turn to the specific calculation. We are given the EM scheme:\n$$\nY_{n+1} = Y_n - h\\,Y_n^3 + Y_n\\,\\Delta W_n,\n$$\nwith initial condition $Y_0=1$. We need to compute the probability $\\mathbb{P}(Y_1 \\geq 1 - h + 3\\,h^{1/2})$ after a single step.\n\nFor the first step ($n=0$), we have:\n$$\nY_1 = Y_0 - h\\,Y_0^3 + Y_0\\,\\Delta W_0.\n$$\nSubstituting the initial condition $Y_0=1$:\n$$\nY_1 = 1 - h(1)^3 + (1)\\Delta W_0 = 1 - h + \\Delta W_0.\n$$\nThe inequality we must analyze is:\n$$\nY_1 \\geq 1 - h + 3\\,h^{1/2}.\n$$\nSubstituting the expression for $Y_1$:\n$$\n1 - h + \\Delta W_0 \\geq 1 - h + 3\\,h^{1/2}.\n$$\nSimplifying the inequality by subtracting $1-h$ from both sides gives:\n$$\n\\Delta W_0 \\geq 3\\,h^{1/2}.\n$$\nWe are given that the Brownian increment $\\Delta W_0$ follows a normal distribution with mean $0$ and variance $h$, denoted as $\\Delta W_0 \\sim \\mathcal{N}(0,h)$. To compute the probability, we standardize this random variable. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$. The relationship between $\\Delta W_0$ and $Z$ is:\n$$\nZ = \\frac{\\Delta W_0 - 0}{\\sqrt{h}} = \\frac{\\Delta W_0}{h^{1/2}}.\n$$\nWe can rewrite the inequality in terms of $Z$ by dividing both sides by $h^{1/2}$:\n$$\n\\frac{\\Delta W_0}{h^{1/2}} \\geq \\frac{3\\,h^{1/2}}{h^{1/2}},\n$$\nwhich simplifies to:\n$$\nZ \\geq 3.\n$$\nThe desired probability is therefore $\\mathbb{P}(Z \\geq 3)$. This probability can be expressed in terms of the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\mathbb{P}(Z \\leq z)$, as:\n$$\n\\mathbb{P}(Z \\geq 3) = 1 - \\mathbb{P}(Z < 3) = 1 - \\Phi(3).\n$$\nThe problem requires the answer in terms of the complementary error function, $\\text{erfc}(x)$. The relationship between the standard normal CDF $\\Phi(z)$ and the error function $\\text{erf}(x)$ is:\n$$\n\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right).\n$$\nThe complementary error function is defined as $\\text{erfc}(x) = 1 - \\text{erf}(x)$. We can therefore express the probability $\\mathbb{P}(Z \\geq z)$ as:\n$$\n\\mathbb{P}(Z \\geq z) = 1 - \\Phi(z) = 1 - \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right) = \\frac{1}{2} - \\frac{1}{2}\\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right) = \\frac{1}{2}\\left(1 - \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right).\n$$\nSubstituting the definition of $\\text{erfc}(x)$:\n$$\n\\mathbb{P}(Z \\geq z) = \\frac{1}{2}\\text{erfc}\\left(\\frac{z}{\\sqrt{2}}\\right).\n$$\nFor our specific case, $z=3$. Thus, the final probability is:\n$$\n\\mathbb{P}(Y_1 \\geq 1 - h + 3\\,h^{1/2}) = \\frac{1}{2}\\text{erfc}\\left(\\frac{3}{\\sqrt{2}}\\right).\n$$\nThis result shows that there is a non-zero probability, independent of the step size $h$, for the numerical solution to experience a deviation of at least $3$ standard deviations of the noise process in a single step. This is a manifestation of the mechanism that undermines strong convergence.", "answer": "$$\n\\boxed{\\frac{1}{2}\\text{erfc}\\left(\\frac{3}{\\sqrt{2}}\\right)}\n$$", "id": "3066779"}]}