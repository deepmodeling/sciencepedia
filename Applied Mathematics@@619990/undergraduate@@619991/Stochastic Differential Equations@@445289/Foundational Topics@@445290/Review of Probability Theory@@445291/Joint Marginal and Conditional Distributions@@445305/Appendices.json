{"hands_on_practices": [{"introduction": "This first exercise provides essential practice in the fundamental mechanics of manipulating multivariate probability distributions. Starting with a simple arithmetic Brownian motion, you will apply the change of variables formula to find the joint distribution of a new set of variables and then use integration to derive a marginal distribution [@problem_id:3062416]. Mastering these core techniques is the first step toward analyzing more complex stochastic systems.", "problem": "Let $\\{X_{r}\\}_{r \\geq 0}$ be the unique strong solution to the stochastic differential equation $dX_{r}=\\mu\\,dr+\\sigma\\,dW_{r}$ with deterministic initial condition $X_{0}=x_{0}$, where $\\mu \\in \\mathbb{R}$, $\\sigma0$, and $\\{W_{r}\\}_{r \\geq 0}$ is a standard Brownian motion (SBM). Fix times $0st$, and define the random vector $(U,V)$ by $U=X_{t}$ and $V=X_{t}+X_{s}$. Starting from the fundamental facts that $W_{t}$ has independent increments, $W_{r}\\sim \\mathcal{N}(0,r)$, and linear transformations of jointly Gaussian vectors are Gaussian, derive the joint probability density function (PDF) $f_{U,V}(u,v)$ from the joint PDF of $(X_{s},X_{t})$ via a change of variables, including the appropriate Jacobian. Then obtain the marginal PDF $f_{V}(v)$ by integrating $f_{U,V}(u,v)$ over all $u \\in \\mathbb{R}$.\n\nProvide your final answer as a single closed-form analytic expression for $f_{V}(v)$ in terms of $x_{0}$, $\\mu$, $\\sigma$, $s$, $t$, and $v$. No numerical approximation is required.", "solution": "The problem as stated is valid. It is scientifically grounded in the theory of stochastic processes, well-posed with sufficient information for a unique solution, and expressed in objective, formal language. We may therefore proceed with the derivation.\n\nThe stochastic differential equation (SDE) is given by $dX_{r}=\\mu\\,dr+\\sigma\\,dW_{r}$ with a deterministic initial condition $X_{0}=x_{0}$, where $\\mu \\in \\mathbb{R}$ and $\\sigma0$. The process $\\{W_{r}\\}_{r \\geq 0}$ is a standard Brownian motion. This is a linear SDE, and its unique strong solution is found by direct integration from $r=0$ to a generic time $r0$:\n$$ X_{r} - X_{0} = \\int_{0}^{r} \\mu \\,d\\tau + \\int_{0}^{r} \\sigma \\,dW_{\\tau} $$\n$$ X_{r} = x_{0} + \\mu r + \\sigma W_{r} $$\nwhere we have used the property $W_{0}=0$ almost surely.\n\nSince $W_{r}$ is a Gaussian random variable with mean $0$ and variance $r$, i.e., $W_{r} \\sim \\mathcal{N}(0,r)$, the process $X_{r}$ is also Gaussian. Its mean and variance are:\n$$ \\mathbb{E}[X_{r}] = \\mathbb{E}[x_{0} + \\mu r + \\sigma W_{r}] = x_{0} + \\mu r $$\n$$ \\text{Var}(X_{r}) = \\text{Var}(x_{0} + \\mu r + \\sigma W_{r}) = \\sigma^2 \\text{Var}(W_{r}) = \\sigma^2 r $$\nThus, $X_{r} \\sim \\mathcal{N}(x_{0}+\\mu r, \\sigma^2 r)$.\n\nFor times $0st$, the random vector $(X_{s}, X_{t})$ is a linear transformation of the jointly Gaussian vector $(W_{s}, W_{t})$, and is therefore itself jointly Gaussian. To find its joint probability density function (PDF) $f_{X_{s},X_{t}}(x_s, x_t)$, we utilize the independence of increments of Brownian motion. We have $X_{s} \\sim \\mathcal{N}(x_{0}+\\mu s, \\sigma^2 s)$ and the conditional distribution of $X_t$ given $X_s=x_s$ is found by considering the evolution from $s$ to $t$:\n$$ X_t = X_s + \\int_s^t \\mu\\,dr + \\int_s^t \\sigma\\,dW_r = X_s + \\mu(t-s) + \\sigma(W_t - W_s) $$\nGiven $X_s=x_s$, the term $W_t-W_s$ is a random increment, independent of $X_s$, distributed as $\\mathcal{N}(0, t-s)$. Therefore, the conditional distribution is:\n$$ X_{t} | X_{s}=x_s \\sim \\mathcal{N}(x_s + \\mu(t-s), \\sigma^2(t-s)) $$\nThe joint PDF is the product of the marginal PDF of $X_s$ and the conditional PDF of $X_t$ given $X_s$:\n$$ f_{X_{s},X_{t}}(x_s, x_t) = f_{X_{s}}(x_s) f_{X_{t}|X_{s}}(x_t|x_s) $$\n$$ f_{X_{s},X_{t}}(x_s, x_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2 s}} \\exp\\left(-\\frac{(x_s - (x_0+\\mu s))^2}{2\\sigma^2 s}\\right) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2(t-s)}} \\exp\\left(-\\frac{(x_t - (x_s+\\mu(t-s)))^2}{2\\sigma^2(t-s)}\\right) $$\n$$ f_{X_{s},X_{t}}(x_s, x_t) = \\frac{1}{2\\pi\\sigma^2\\sqrt{s(t-s)}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\left[ \\frac{(x_s - x_0 - \\mu s)^2}{s} + \\frac{(x_t - x_s - \\mu(t-s))^2}{t-s} \\right]\\right) $$\n\nNext, we perform a change of variables to the random vector $(U,V)$ defined by $U=X_t$ and $V=X_t+X_s$. The inverse transformation from $(u,v)$ to $(x_s,x_t)$ is:\n$$ x_t = u $$\n$$ x_s = v - x_t = v-u $$\nThe Jacobian determinant of this inverse transformation $g:(u,v) \\mapsto (x_s, x_t)$ is:\n$$ J = \\det \\begin{pmatrix} \\frac{\\partial x_s}{\\partial u}  \\frac{\\partial x_s}{\\partial v} \\\\ \\frac{\\partial x_t}{\\partial u}  \\frac{\\partial x_t}{\\partial v} \\end{pmatrix} = \\det \\begin{pmatrix} -1  1 \\\\ 1  0 \\end{pmatrix} = ( -1 \\cdot 0 ) - ( 1 \\cdot 1 ) = -1 $$\nThe absolute value of the Jacobian is $|J|=1$.\nThe joint PDF of $(U,V)$ is given by $f_{U,V}(u,v) = f_{X_{s},X_{t}}(v-u, u)|J|$. Substituting $x_s=v-u$ and $x_t=u$:\n$$ f_{U,V}(u,v) = \\frac{1}{2\\pi\\sigma^2\\sqrt{s(t-s)}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\left[ \\frac{(v-u - x_0 - \\mu s)^2}{s} + \\frac{(u - (v-u) - \\mu(t-s))^2}{t-s} \\right]\\right) $$\nSimplifying the second term in the numerator of the exponent: $u - (v-u) - \\mu(t-s) = 2u-v-\\mu(t-s)$.\n$$ f_{U,V}(u,v) = \\frac{1}{2\\pi\\sigma^2\\sqrt{s(t-s)}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\left[ \\frac{(v-u-x_0-\\mu s)^2}{s} + \\frac{(2u-v-\\mu(t-s))^2}{t-s} \\right]\\right) $$\n\nTo find the marginal PDF $f_V(v)$, we integrate $f_{U,V}(u,v)$ over all $u \\in \\mathbb{R}$:\n$$ f_V(v) = \\int_{-\\infty}^{\\infty} f_{U,V}(u,v) \\,du $$\nThe exponent's argument is a quadratic function of $u$. Let's denote the term in the brackets by $Q(u,v)$:\n$$ Q(u,v) = \\frac{1}{s}( (v-x_0-\\mu s) - u )^2 + \\frac{1}{t-s}( 2u - (v+\\mu(t-s)) )^2 $$\nExpanding this with respect to $u$:\n$$ Q(u,v) = u^2\\left(\\frac{1}{s} + \\frac{4}{t-s}\\right) - 2u\\left(\\frac{v-x_0-\\mu s}{s} + \\frac{2(v+\\mu(t-s))}{t-s}\\right) + \\text{terms not in } u $$\nLet's complete the square for $u$. The coefficient of $u^2$ is $\\frac{t-s+4s}{s(t-s)} = \\frac{t+3s}{s(t-s)}$.\nThe expression $Q(u,v)$ is of the form $C_1 u^2 - 2C_2 u + C_3$. This can be rewritten as $C_1(u-C_2/C_1)^2 + C_3 - C_2^2/C_1$. The integral over $u$ of $\\exp(-\\frac{1}{2\\sigma^2}C_1(u-\\mu_u)^2)$ is $\\sqrt{2\\pi\\sigma^2/C_1}$.\nHere, $C_1 = \\frac{t+3s}{s(t-s)}$. The integral gives a factor:\n$$ \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\left(\\frac{t+3s}{s(t-s)}\\right) + \\dots \\right) du = \\sqrt{2\\pi\\sigma^2 \\frac{s(t-s)}{t+3s}} $$\nSo, the marginal PDF becomes:\n$$ f_V(v) = \\frac{1}{2\\pi\\sigma^2\\sqrt{s(t-s)}} \\cdot \\sqrt{2\\pi\\sigma^2 \\frac{s(t-s)}{t+3s}} \\cdot \\exp\\left(-\\frac{1}{2\\sigma^2}\\left( C_3 - \\frac{C_2^2}{C_1} \\right)\\right) $$\n$$ f_V(v) = \\frac{1}{\\sqrt{2\\pi\\sigma^2(t+3s)}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left[ C_3 - \\frac{C_2^2}{C_1} \\right]\\right) $$\nThis demonstrates that $V$ is a Gaussian random variable with variance $\\sigma^2(t+3s)$. We now simplify the exponent term. Let $A = v-x_0-\\mu s$ and $B = v+\\mu(t-s)$.\n$$ Q(u,v) = \\frac{(A-u)^2}{s} + \\frac{(2u-B)^2}{t-s} $$\nThe non-$u$ part of the completed square is $\\frac{(2A-B)^2}{t+3s}$. Let's verify this algebraic reduction. The quadratic form in $u$ is $u^2 \\frac{t+3s}{s(t-s)} - 2u \\frac{A(t-s)+2Bs}{s(t-s)} + \\frac{A^2}{s} + \\frac{B^2}{t-s}$. The term $C_3 - C_2^2/C_1$ is:\n$$ \\left(\\frac{A^2}{s} + \\frac{B^2}{t-s}\\right) - \\frac{\\left(\\frac{A(t-s)+2Bs}{s(t-s)}\\right)^2}{\\frac{t+3s}{s(t-s)}} = \\frac{(A^2(t-s)+B^2s)(t+3s) - (A(t-s)+2Bs)^2}{s(t-s)(t+3s)} $$\nThe numerator simplifies to $s(t-s)(2A-B)^2$. Thus, the term becomes $\\frac{s(t-s)(2A-B)^2}{s(t-s)(t+3s)} = \\frac{(2A-B)^2}{t+3s}$.\nFinally, we substitute $A$ and $B$ back to find the mean of $V$:\n$$ 2A-B = 2(v-x_0-\\mu s) - (v+\\mu(t-s)) = 2v-2x_0-2\\mu s - v - \\mu t + \\mu s $$\n$$ 2A-B = v - 2x_0 - (\\mu s + \\mu t) = v - (2x_0 + \\mu(s+t)) $$\nThe exponent is $-\\frac{1}{2\\sigma^2}\\frac{(v - (2x_0+\\mu(s+t)))^2}{t+3s} = -\\frac{(v - (2x_0+\\mu(s+t)))^2}{2\\sigma^2(t+3s)}$.\nThe mean of $V$ is $\\mathbb{E}[V] = 2x_0+\\mu(s+t)$. The variance is $\\text{Var}(V) = \\sigma^2(t+3s)$, which we can write as $\\sigma^2(3s+t)$.\nThe marginal PDF for $V$ is therefore:\n$$ f_V(v) = \\frac{1}{\\sqrt{2\\pi\\sigma^2(3s+t)}} \\exp\\left(-\\frac{(v - (2x_0+\\mu(s+t)))^2}{2\\sigma^2(3s+t)}\\right) $$\nThis is the PDF for a normal distribution $\\mathcal{N}(2x_0+\\mu(s+t), \\sigma^2(3s+t))$.", "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\pi\\sigma^2(3s+t)}} \\exp\\left(-\\frac{(v - (2x_0+\\mu(s+t)))^2}{2\\sigma^2(3s+t)}\\right)}\n$$", "id": "3062416"}, {"introduction": "Real-world modeling often involves uncertainty not just in the dynamics, but also in the initial state of the system. This practice explores how to formally propagate this initial uncertainty through time by combining a prior distribution for the initial state with the transition law of the process. You will derive the marginal distribution of the process at a later time, demonstrating how initial uncertainty evolves and how the solution can be interpreted as a convolution [@problem_id:3062429].", "problem": "Consider the scalar It≈ç stochastic differential equation (SDE) $dX_{t} = a X_{t}\\,dt + \\sigma\\,dW_{t}$ with constant drift coefficient $a \\in \\mathbb{R}$ satisfying $a \\neq 0$, diffusion coefficient $\\sigma  0$, and standard Brownian motion $W_{t}$ starting at $W_{0} = 0$. The initial condition $X_{0}$ is unknown and modeled by a prior density $\\pi(x_{0})$ that is Gaussian with mean $\\mu_{0}$ and variance $s_{0}^{2}$, i.e., $\\pi(x_{0}) = \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}}\\exp\\!\\left(-\\frac{(x_{0}-\\mu_{0})^{2}}{2 s_{0}^{2}}\\right)$. Suppose you observe the state $X_{t}$ exactly at a fixed time $t0$. Starting only from the defining properties of Brownian motion and It≈ç calculus, derive the joint, marginal, and conditional distributions relevant to $X_{t}$ and $X_{0}$, and then compute the marginal likelihood density $p_{X_{t}}(x)$ of observing $X_{t}=x$ by integrating the likelihood $p_{X_{t}\\mid X_{0}}(x\\mid x_{0})$ against the prior $\\pi(x_{0})$. Interpret the resulting marginalization as a convolution between the distribution of a transformed initial condition and a diffusion kernel induced by the SDE. Express your final answer as a single closed-form analytic expression for the marginal density $p_{X_{t}}(x)$ in terms of $a$, $\\sigma$, $t$, $\\mu_{0}$, $s_{0}^{2}$, and $x$. No numerical rounding is required.", "solution": "We begin with the linear It≈ç stochastic differential equation $dX_{t} = a X_{t}\\,dt + \\sigma\\,dW_{t}$ with $a \\neq 0$ and $\\sigma0$. Introduce the integrating factor $\\exp(-at)$ and define $Y_{t} = \\exp(-at) X_{t}$. By It≈ç‚Äôs product rule and the fact that $d(\\exp(-at)) = -a \\exp(-at)\\,dt$, we get\n$$\ndY_{t} \\;=\\; \\exp(-at)\\,dX_{t} + X_{t}\\,d(\\exp(-at)) \\;=\\; \\exp(-at)\\big(a X_{t}\\,dt + \\sigma\\,dW_{t}\\big) - a \\exp(-at) X_{t}\\,dt \\;=\\; \\sigma \\exp(-at)\\,dW_{t}.\n$$\nTherefore,\n$$\nY_{t} \\;=\\; Y_{0} + \\sigma \\int_{0}^{t} \\exp(-as)\\,dW_{s} \\;=\\; X_{0} + \\sigma \\int_{0}^{t} \\exp(-as)\\,dW_{s}.\n$$\nMultiplying by $\\exp(at)$ gives the explicit strong solution\n$$\nX_{t} \\;=\\; \\exp(at)\\,X_{0} + \\sigma \\int_{0}^{t} \\exp\\!\\big(a (t-s)\\big)\\,dW_{s}.\n$$\nConditional on $X_{0}=x_{0}$, the random variable $X_{t}$ is Gaussian because it is an affine transformation of a Gaussian It≈ç integral. Its conditional mean is\n$$\n\\mathbb{E}[\\,X_{t}\\mid X_{0}=x_{0}\\,] \\;=\\; \\exp(at)\\,x_{0},\n$$\nand its conditional variance is\n$$\n\\operatorname{Var}[\\,X_{t}\\mid X_{0}=x_{0}\\,] \\;=\\; \\sigma^{2}\\,\\mathbb{E}\\!\\left[\\left(\\int_{0}^{t} \\exp\\!\\big(a (t-s)\\big)\\,dW_{s}\\right)^{2}\\right] \\;=\\; \\sigma^{2}\\int_{0}^{t} \\exp\\!\\big(2a(t-s)\\big)\\,ds \\;=\\; \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a},\n$$\nwhere we used the It≈ç isometry for stochastic integrals with respect to Brownian motion. Hence the conditional density (likelihood) $p_{X_{t}\\mid X_{0}}(x\\mid x_{0})$ is\n$$\np_{X_{t}\\mid X_{0}}(x\\mid x_{0}) \\;=\\; \\frac{1}{\\sqrt{2\\pi v_{t}}}\\,\\exp\\!\\left(-\\frac{(x - \\exp(at)\\,x_{0})^{2}}{2 v_{t}}\\right), \\quad v_{t} \\;=\\; \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}.\n$$\n\nThe prior density for $X_{0}$ is Gaussian with mean $\\mu_{0}$ and variance $s_{0}^{2}$:\n$$\n\\pi(x_{0}) \\;=\\; \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}}\\,\\exp\\!\\left(-\\frac{(x_{0}-\\mu_{0})^{2}}{2 s_{0}^{2}}\\right).\n$$\nTo obtain the marginal density of $X_{t}$, integrate the conditional density against the prior:\n$$\np_{X_{t}}(x) \\;=\\; \\int_{-\\infty}^{\\infty} p_{X_{t}\\mid X_{0}}(x\\mid x_{0})\\,\\pi(x_{0})\\,dx_{0} \\;=\\; \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi v_{t}}}\\,\\exp\\!\\left(-\\frac{(x - \\exp(at)\\,x_{0})^{2}}{2 v_{t}}\\right)\\cdot \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}}\\,\\exp\\!\\left(-\\frac{(x_{0}-\\mu_{0})^{2}}{2 s_{0}^{2}}\\right)\\,dx_{0}.\n$$\nThis integral has a natural convolution interpretation. Consider the random decomposition\n$$\nX_{t} \\;=\\; \\underbrace{\\exp(at)\\,X_{0}}_{\\text{scaled initial state}} \\;+\\; \\underbrace{\\sigma \\int_{0}^{t} \\exp\\!\\big(a (t-s)\\big)\\,dW_{s}}_{\\text{independent Gaussian noise}}.\n$$\nLet $Y = \\exp(at)\\,X_{0}$ and $Z = \\sigma \\int_{0}^{t} \\exp\\!\\big(a (t-s)\\big)\\,dW_{s}$. Then $Z$ is Gaussian with mean $0$ and variance $v_{t}$, independent of $Y$. The distribution of $Y$ is Gaussian with mean $\\exp(at)\\,\\mu_{0}$ and variance $\\exp(2at)\\,s_{0}^{2}$. The marginal density $p_{X_{t}}(x)$ is the convolution of the density of $Y$ with the Gaussian kernel of $Z$:\n$$\np_{X_{t}}(x) \\;=\\; \\int_{-\\infty}^{\\infty} \\underbrace{\\frac{1}{\\sqrt{2\\pi v_{t}}}\\,\\exp\\!\\left(-\\frac{(x-y)^{2}}{2 v_{t}}\\right)}_{\\text{diffusion kernel for }Z}\\cdot \\underbrace{\\frac{1}{\\sqrt{2\\pi \\exp(2at)\\,s_{0}^{2}}}\\,\\exp\\!\\left(-\\frac{(y-\\exp(at)\\,\\mu_{0})^{2}}{2\\,\\exp(2at)\\,s_{0}^{2}}\\right)}_{\\text{density of }Y}\\,dy.\n$$\nSince the convolution of two Gaussian densities is Gaussian, $X_{t}$ is Gaussian with mean\n$$\n\\mathbb{E}[X_{t}] \\;=\\; \\exp(at)\\,\\mu_{0},\n$$\nand variance\n$$\n\\operatorname{Var}(X_{t}) \\;=\\; \\exp(2at)\\,s_{0}^{2} + v_{t} \\;=\\; \\exp(2at)\\,s_{0}^{2} + \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}.\n$$\nTherefore, the closed-form marginal likelihood density is\n$$\np_{X_{t}}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi\\left(\\exp(2at)\\,s_{0}^{2} + \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}\\right)}}\\,\\exp\\!\\left(-\\frac{\\left(x - \\exp(at)\\,\\mu_{0}\\right)^{2}}{2\\left(\\exp(2at)\\,s_{0}^{2} + \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}\\right)}\\right).\n$$\nThis expression both computes the marginalization by integrating the likelihood against the prior and exhibits the convolution structure: $X_{t}$ is the sum of an independently perturbed scaled initial state and a zero-mean Gaussian diffusion term, so its density is the convolution of the scaled prior distribution with the diffusion kernel.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi\\left(\\exp(2at)\\,s_{0}^{2} + \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}\\right)}}\\,\\exp\\!\\left(-\\frac{\\left(x - \\exp(at)\\,\\mu_{0}\\right)^{2}}{2\\left(\\exp(2at)\\,s_{0}^{2} + \\sigma^{2}\\,\\frac{\\exp(2at)-1}{2a}\\right)}\\right)}$$", "id": "3062429"}, {"introduction": "Not all stochastic processes are memoryless like the Wiener process. This exercise focuses on the Ornstein-Uhlenbeck (OU) process, a cornerstone for modeling mean-reverting systems, to explore the concept of process memory. By calculating the covariance between successive increments, you will demonstrate that they are not independent, a crucial feature that reflects the system's tendency to return to its long-term mean [@problem_id:3062459].", "problem": "Consider the Ornstein‚ÄìUhlenbeck (OU) process defined as the unique strong solution to the stochastic differential equation $dX_{t}=-\\theta X_{t}\\,dt+\\sigma\\,dW_{t}$, where $\\theta0$, $\\sigma0$, and $(W_{t})_{t\\geq 0}$ is a standard Wiener process. Assume the process is strictly stationary by taking $X_{0}\\sim \\mathcal{N}\\!\\left(0,\\frac{\\sigma^{2}}{2\\theta}\\right)$ independent of $(W_{t})_{t\\geq 0}$. For fixed $t\\geq h0$, define the forward and backward increments $\\Delta_{+}=X_{t+h}-X_{t}$ and $\\Delta_{-}=X_{t}-X_{t-h}$.\n\nStarting from the integral representation of the solution and fundamental properties of Gaussian processes driven by Wiener noise, derive the joint, marginal, and conditional distributions needed to assess dependence between $\\Delta_{+}$ and $\\Delta_{-}$. In particular, compute $\\operatorname{Cov}(\\Delta_{+},\\Delta_{-})$ in closed form as a function of $\\theta$, $\\sigma$, and $h$, and use the joint distribution to conclude whether these increments are independent.\n\nYour final answer should be the closed-form expression for $\\operatorname{Cov}(X_{t+h}-X_{t},\\,X_{t}-X_{t-h})$ in terms of $\\theta$, $\\sigma$, and $h$. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on the standard Ornstein-Uhlenbeck (OU) process, a fundamental model in the theory of stochastic differential equations. All parameters and conditions are clearly defined, leading to a unique, solvable mathematical problem. Therefore, the problem is valid.\n\nThe Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$dX_{t} = -\\theta X_{t}\\,dt + \\sigma\\,dW_{t}$$\nwith parameters $\\theta  0$ and $\\sigma  0$. The solution to this SDE starting from a value $X_s$ at time $s$ is given by:\n$$X_{t} = X_{s}e^{-\\theta(t-s)} + \\sigma \\int_{s}^{t} e^{-\\theta(t-u)}\\,dW_{u} \\quad \\text{for } t \\geq s$$\nThe process is specified to be strictly stationary, achieved by setting the initial condition $X_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma^{2}}{2\\theta}\\right)$, where $\\mathcal{N}(\\mu, \\nu)$ denotes a normal distribution with mean $\\mu$ and variance $\\nu$. Since $X_0$ is a Gaussian random variable and the stochastic integral with respect to a Wiener process is also Gaussian, $X_t$ is a Gaussian process. This implies that any linear combination of the process values at different times results in a Gaussian random variable. Consequently, the vector of increments $(\\Delta_{+}, \\Delta_{-}) = (X_{t+h}-X_{t}, X_{t}-X_{t-h})$ has a bivariate normal distribution.\n\nA multivariate normal distribution is completely characterized by its mean vector and its covariance matrix.\nFirst, we determine the mean of the process. Due to stationarity, the distribution of $X_t$ is the same for all $t \\geq 0$. Thus, $\\mathbb{E}[X_t] = \\mathbb{E}[X_0] = 0$ for all $t$. The means of the increments are therefore:\n$$\\mathbb{E}[\\Delta_{+}] = \\mathbb{E}[X_{t+h} - X_{t}] = \\mathbb{E}[X_{t+h}] - \\mathbb{E}[X_{t}] = 0 - 0 = 0$$\n$$\\mathbb{E}[\\Delta_{-}] = \\mathbb{E}[X_{t} - X_{t-h}] = \\mathbb{E}[X_{t}] - \\mathbb{E}[X_{t-h}] = 0 - 0 = 0$$\nThe mean vector of $(\\Delta_{+}, \\Delta_{-})$ is thus $(0, 0)$.\n\nNext, we compute the covariance structure. This requires the autocovariance function of the stationary OU process, $R(\\tau) = \\operatorname{Cov}(X_{t+\\tau}, X_t) = \\mathbb{E}[X_{t+\\tau}X_{t}]$. Assuming $\\tau \\geq 0$:\n$$X_{t+\\tau} = X_{t}e^{-\\theta\\tau} + \\sigma\\int_{t}^{t+\\tau}e^{-\\theta(t+\\tau-u)}\\,dW_{u}$$\nMultiplying by $X_t$ and taking the expectation, we get:\n$$\\mathbb{E}[X_{t+\\tau}X_{t}] = \\mathbb{E}[X_{t}^2]e^{-\\theta\\tau} + \\sigma\\mathbb{E}\\left[X_t \\int_{t}^{t+\\tau}e^{-\\theta(t+\\tau-u)}\\,dW_{u}\\right]$$\nThe second term is zero because $X_t$ is determined by the Wiener process up to time $t$ and is therefore independent of the future increments of the Wiener process $(W_u - W_t)$ for $u  t$. By the It≈ç isometry property, the expectation of the stochastic integral term is zero. Therefore:\n$$\\mathbb{E}[X_{t+\\tau}X_{t}] = \\mathbb{E}[X_{t}^2]e^{-\\theta\\tau}$$\nDue to stationarity, the variance $\\mathbb{E}[X_{t}^2] = \\operatorname{Var}(X_t) = \\operatorname{Var}(X_0) = \\frac{\\sigma^2}{2\\theta}$.\nThe autocovariance function for a time lag of $\\tau$ is thus:\n$$R(\\tau) = \\operatorname{Cov}(X_s, X_{s+\\tau}) = \\frac{\\sigma^2}{2\\theta}e^{-\\theta|\\tau|}$$\nThe core task is to compute $\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-})$. Using the bilinearity of the covariance operator:\n$$\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = \\operatorname{Cov}(X_{t+h}-X_{t}, X_{t}-X_{t-h})$$\n$$= \\operatorname{Cov}(X_{t+h}, X_{t}) - \\operatorname{Cov}(X_{t+h}, X_{t-h}) - \\operatorname{Cov}(X_{t}, X_{t}) + \\operatorname{Cov}(X_{t}, X_{t-h})$$\nWe can express each term using the autocovariance function $R(\\tau)$:\n\\begin{itemize}\n    \\item $\\operatorname{Cov}(X_{t+h}, X_{t}) = R((t+h)-t) = R(h) = \\frac{\\sigma^2}{2\\theta}e^{-\\theta h}$\n    \\item $\\operatorname{Cov}(X_{t+h}, X_{t-h}) = R((t+h)-(t-h)) = R(2h) = \\frac{\\sigma^2}{2\\theta}e^{-2\\theta h}$\n    \\item $\\operatorname{Cov}(X_{t}, X_{t}) = \\operatorname{Var}(X_t) = R(0) = \\frac{\\sigma^2}{2\\theta}$\n    \\item $\\operatorname{Cov}(X_{t}, X_{t-h}) = R(t-(t-h)) = R(h) = \\frac{\\sigma^2}{2\\theta}e^{-\\theta h}$\n\\end{itemize}\nSubstituting these into the expression for $\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-})$:\n$$\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = R(h) - R(2h) - R(0) + R(h) = 2R(h) - R(0) - R(2h)$$\n$$\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = 2\\left(\\frac{\\sigma^2}{2\\theta}e^{-\\theta h}\\right) - \\frac{\\sigma^2}{2\\theta} - \\frac{\\sigma^2}{2\\theta}e^{-2\\theta h}$$\nFactoring out the constant term $\\frac{\\sigma^2}{2\\theta}$:\n$$\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = \\frac{\\sigma^2}{2\\theta} \\left(2e^{-\\theta h} - 1 - e^{-2\\theta h}\\right)$$\nThe term in parentheses can be rearranged and factored:\n$$2e^{-\\theta h} - 1 - e^{-2\\theta h} = -(e^{-2\\theta h} - 2e^{-\\theta h} + 1) = -(e^{-\\theta h} - 1)^2$$\nThus, the final closed-form expression for the covariance is:\n$$\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = -\\frac{\\sigma^2}{2\\theta}(e^{-\\theta h} - 1)^2 = -\\frac{\\sigma^2}{2\\theta}(1 - e^{-\\theta h})^2$$\n\nTo complete the characterization and assess independence, we find the variances of the increments.\n$$\\operatorname{Var}(\\Delta_{+}) = \\operatorname{Var}(X_{t+h} - X_t) = \\operatorname{Var}(X_{t+h}) + \\operatorname{Var}(X_t) - 2\\operatorname{Cov}(X_{t+h}, X_t)$$\n$$= R(0) + R(0) - 2R(h) = 2(R(0) - R(h))$$\n$$= 2\\left(\\frac{\\sigma^2}{2\\theta} - \\frac{\\sigma^2}{2\\theta}e^{-\\theta h}\\right) = \\frac{\\sigma^2}{\\theta}(1 - e^{-\\theta h})$$\nBy stationarity, $\\operatorname{Var}(\\Delta_{-}) = \\operatorname{Var}(X_t - X_{t-h}) = \\operatorname{Var}(\\Delta_{+})$.\n\nThe joint distribution of $(\\Delta_{+}, \\Delta_{-})$ is bivariate normal $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ with mean vector $\\boldsymbol{\\mu} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and covariance matrix:\n$$\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\frac{\\sigma^2}{\\theta}(1 - e^{-\\theta h})  -\\frac{\\sigma^2}{2\\theta}(1 - e^{-\\theta h})^2 \\\\ -\\frac{\\sigma^2}{2\\theta}(1 - e^{-\\theta h})^2  \\frac{\\sigma^2}{\\theta}(1 - e^{-\\theta h}) \\end{pmatrix}$$\nThe marginal distributions are $\\Delta_{+} \\sim \\mathcal{N}(0, \\frac{\\sigma^2}{\\theta}(1 - e^{-\\theta h}))$ and $\\Delta_{-} \\sim \\mathcal{N}(0, \\frac{\\sigma^2}{\\theta}(1 - e^{-\\theta h}))$. The conditional distribution of $\\Delta_{+}$ given $\\Delta_{-}=\\delta$ is also normal.\n\nFor jointly Gaussian variables, independence is equivalent to having zero covariance. Here, we have $\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-}) = -\\frac{\\sigma^2}{2\\theta}(1 - e^{-\\theta h})^2$. Since $\\sigma  0$, $\\theta  0$, and $h  0$, it follows that $e^{-\\theta h}  1$, so $(1 - e^{-\\theta h})^2  0$. Therefore, $\\operatorname{Cov}(\\Delta_{+}, \\Delta_{-})  0$. Since the covariance is non-zero, the increments $\\Delta_{+}$ and $\\Delta_{-}$ are not independent. They are negatively correlated, which is consistent with the mean-reverting property of the OU process. A positive past increment tends to be followed by a negative future increment as the process is pulled back toward its mean of zero.", "answer": "$$\\boxed{-\\frac{\\sigma^{2}}{2\\theta}(1 - e^{-\\theta h})^{2}}$$", "id": "3062459"}]}