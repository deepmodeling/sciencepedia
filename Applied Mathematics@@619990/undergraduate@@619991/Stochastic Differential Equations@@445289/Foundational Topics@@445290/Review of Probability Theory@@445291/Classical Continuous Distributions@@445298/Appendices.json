{"hands_on_practices": [{"introduction": "To build robust stochastic models, we must first master the properties of their core components. The exponential distribution is the cornerstone for modeling random waiting times in continuous-time processes. This first practice challenges you to derive the expectation and variance of an exponential random variable directly from its probability density function, a fundamental skill that connects calculus to the statistical characterization of jump events [@problem_id:3043905].", "problem": "Consider a simple jump stochastic differential equation driven by a homogeneous Poisson process: let $X_{t}$ satisfy $\\mathrm{d}X_{t}=\\mathrm{d}N_{t}$, where $N_{t}$ is a homogeneous Poisson process with rate $\\lambda>0$. The waiting time $T$ until the first jump of $N_{t}$ is known to be exponentially distributed with parameter $\\lambda$. Let $f_{T}(t)$ denote the probability density function of $T$. Starting strictly from the core definitions of expectation and variance for a continuous random variable and using direct integration with $f_{T}(t)$, compute $E[T]$ and $\\mathrm{Var}(T)$ in terms of $\\lambda$. You must not use any pre-memorized moment formulas or properties specific to the exponential distribution beyond its probability density function and the basic definitions of expectation and variance. Report your final results as the two-entry row vector $\\big(E[T],\\,\\mathrm{Var}(T)\\big)$. No numerical approximation is required; provide exact symbolic expressions.", "solution": "The interarrival time $T$ of a homogeneous Poisson process with rate $\\lambda>0$ has the exponential distribution with probability density function\n$$\nf_{T}(t)=\\lambda\\,\\exp(-\\lambda t),\\quad t\\geq 0,\n$$\nand $f_{T}(t)=0$ for $t<0$. By the definition of expectation for a continuous random variable,\n$$\nE[T]=\\int_{-\\infty}^{\\infty} t\\,f_{T}(t)\\,\\mathrm{d}t=\\int_{0}^{\\infty} t\\,\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nWe evaluate this integral by integration by parts. Recall the integration by parts identity\n$$\n\\int u\\,\\mathrm{d}v = u\\,v - \\int v\\,\\mathrm{d}u.\n$$\nSet $u=t$ and $\\mathrm{d}v=\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t$, so that $\\mathrm{d}u=\\mathrm{d}t$ and $v=-\\exp(-\\lambda t)$. Then\n$$\n\\int_{0}^{\\infty} t\\,\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t\n= \\left[-t\\,\\exp(-\\lambda t)\\right]_{0}^{\\infty} + \\int_{0}^{\\infty} \\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nWe evaluate the boundary term: as $t\\to\\infty$, $t\\,\\exp(-\\lambda t)\\to 0$ since the exponential decay dominates linear growth; at $t=0$, the product $t\\,\\exp(-\\lambda t)$ equals $0$. Therefore the boundary term is $0$. The remaining integral is\n$$\n\\int_{0}^{\\infty} \\exp(-\\lambda t)\\,\\mathrm{d}t=\\left[-\\frac{1}{\\lambda}\\exp(-\\lambda t)\\right]_{0}^{\\infty}=\\frac{1}{\\lambda}.\n$$\nHence,\n$$\nE[T]=\\frac{1}{\\lambda}.\n$$\n\nNext, by the definition of variance,\n$$\n\\mathrm{Var}(T)=E\\!\\left[(T-E[T])^{2}\\right]=E[T^{2}] - \\big(E[T]\\big)^{2}.\n$$\nWe have already found $E[T]=\\frac{1}{\\lambda}$, so we compute $E[T^{2}]$ directly:\n$$\nE[T^{2}]=\\int_{0}^{\\infty} t^{2}\\,\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nWe again use integration by parts. Set $u=t^{2}$ and $\\mathrm{d}v=\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t$, so that $\\mathrm{d}u=2t\\,\\mathrm{d}t$ and $v=-\\exp(-\\lambda t)$. Then\n$$\n\\int_{0}^{\\infty} t^{2}\\,\\lambda\\,\\exp(-\\lambda t)\\,\\mathrm{d}t\n= \\left[-t^{2}\\,\\exp(-\\lambda t)\\right]_{0}^{\\infty} + \\int_{0}^{\\infty} 2t\\,\\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nThe boundary term vanishes: as $t\\to\\infty$, $t^{2}\\,\\exp(-\\lambda t)\\to 0$, and at $t=0$, it equals $0$. We focus on\n$$\n\\int_{0}^{\\infty} 2t\\,\\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nApply integration by parts once more with $u=2t$ and $\\mathrm{d}v=\\exp(-\\lambda t)\\,\\mathrm{d}t$, so that $\\mathrm{d}u=2\\,\\mathrm{d}t$ and $v=-\\frac{1}{\\lambda}\\exp(-\\lambda t)$. Then\n$$\n\\int_{0}^{\\infty} 2t\\,\\exp(-\\lambda t)\\,\\mathrm{d}t\n= \\left[-\\frac{2t}{\\lambda}\\,\\exp(-\\lambda t)\\right]_{0}^{\\infty} + \\int_{0}^{\\infty} \\frac{2}{\\lambda}\\,\\exp(-\\lambda t)\\,\\mathrm{d}t.\n$$\nAgain, the boundary term vanishes, leaving\n$$\n\\frac{2}{\\lambda}\\int_{0}^{\\infty} \\exp(-\\lambda t)\\,\\mathrm{d}t = \\frac{2}{\\lambda}\\cdot \\frac{1}{\\lambda} = \\frac{2}{\\lambda^{2}}.\n$$\nTherefore,\n$$\nE[T^{2}] = \\frac{2}{\\lambda^{2}}.\n$$\nFinally,\n$$\n\\mathrm{Var}(T)=E[T^{2}] - \\big(E[T]\\big)^{2} = \\frac{2}{\\lambda^{2}} - \\left(\\frac{1}{\\lambda}\\right)^{2} = \\frac{1}{\\lambda^{2}}.\n$$\n\nCollecting the results,\n$$\nE[T]=\\frac{1}{\\lambda},\\qquad \\mathrm{Var}(T)=\\frac{1}{\\lambda^{2}}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\lambda} & \\frac{1}{\\lambda^{2}}\\end{pmatrix}}$$", "id": "3043905"}, {"introduction": "Why is the exponential distribution so ubiquitous in modeling random arrivals, from radioactive decay to customer service queues? The answer lies in its unique 'memoryless' property. This exercise asks you to explore this concept by directly calculating the conditional survival probability for a uniform distribution and observing its dependence on elapsed time, which provides a sharp contrast to the memoryless nature of the exponential distribution [@problem_id:3043856].", "problem": "In modeling arrival times within Stochastic Differential Equations (SDE), the memoryless property is often associated with exponential waiting times. A nonnegative random variable $X$ is memoryless if and only if for all $u \\ge 0$ and $t \\ge 0$,\n$$\n\\mathbb{P}(X>u+t \\mid X>u) = \\mathbb{P}(X>t).\n$$\nConsider instead a uniform waiting time $U \\sim \\mathrm{Uniform}(0,L)$ on a bounded interval $[0,L]$ with $L>0$. Without assuming any special properties beyond the definitions of conditional probability and the uniform distribution, analyze the conditional survival function $\\mathbb{P}(U>u+t \\mid U>u)$ for $u \\in [0,L)$ and $t>0$, with particular attention to the behavior as $u$ approaches the right endpoint $L$. Which of the following statements most accurately characterizes $\\mathbb{P}(U>u+t \\mid U>u)$ and the implication for the memoryless property of $U$?\n\nA. For all $u \\in [0,L)$ and $t>0$ with $u+t<L$, one has $\\mathbb{P}(U>u+t \\mid U>u) = \\mathbb{P}(U>t)$, so $U$ is memoryless.\n\nB. For $0 \\le u < L-t$, $\\mathbb{P}(U>u+t \\mid U>u) = 1 - \\dfrac{t}{L-u}$, which depends on $u$ and tends to $0$ as $u \\to L^{-}$ for any fixed $t \\in (0,L)$; for $u \\ge L-t$, the conditional probability equals $0$. Hence $U$ is not memoryless.\n\nC. One has $\\mathbb{P}(U>u+t \\mid U>u) = 1$ for all $u \\in [0,L)$ and $t>0$ with $u+t<L$, because conditioning on survival removes the effect of elapsed time.\n\nD. For any $t \\in (0,L)$ and any $u \\in [0,L)$, $\\mathbb{P}(U>u+t \\mid U>u) = 0$, since the support is bounded; therefore $U$ fails to be memoryless.\n\nE. For $0 \\le u < L-t$, $\\mathbb{P}(U>u+t \\mid U>u) = \\dfrac{L-t}{L}$, independent of $u$, while for $u \\ge L-t$ it equals $0$; thus $U$ is memoryless except at the endpoint.", "solution": "Let $U$ be a random variable with a uniform distribution on the interval $[0,L]$, where $L>0$. The probability density function (PDF) is given by:\n$$\nf_U(x) = \\begin{cases} \\frac{1}{L} & \\text{for } 0 \\le x \\le L \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe cumulative distribution function (CDF), $F_U(x) = \\mathbb{P}(U \\le x)$, is:\n$$\nF_U(x) = \\begin{cases} 0 & \\text{for } x < 0 \\\\ \\frac{x}{L} & \\text{for } 0 \\le x \\le L \\\\ 1 & \\text{for } x > L \\end{cases}\n$$\nThe survival function, $S_U(x) = \\mathbb{P}(U > x) = 1 - F_U(x)$, is:\n$$\n\\mathbb{P}(U > x) = \\begin{cases} 1 & \\text{for } x < 0 \\\\ 1 - \\frac{x}{L} = \\frac{L-x}{L} & \\text{for } 0 \\le x < L \\\\ 0 & \\text{for } x \\ge L \\end{cases}\n$$\nWe are asked to analyze the conditional probability $\\mathbb{P}(U>u+t \\mid U>u)$ for $u \\in [0,L)$ and $t>0$. Using the definition of conditional probability, $\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$, we have:\n$$\n\\mathbb{P}(U>u+t \\mid U>u) = \\frac{\\mathbb{P}(\\{U>u+t\\} \\cap \\{U>u\\})}{\\mathbb{P}(U>u)}\n$$\nSince $t>0$, the event $\\{U>u+t\\}$ is a subset of the event $\\{U>u\\}$. Therefore, their intersection is simply $\\{U>u+t\\}$. The expression simplifies to:\n$$\n\\mathbb{P}(U>u+t \\mid U>u) = \\frac{\\mathbb{P}(U>u+t)}{\\mathbb{P}(U>u)}\n$$\nWe evaluate the numerator and the denominator using the survival function derived above.\nThe denominator is $\\mathbb{P}(U>u)$. Since $u \\in [0,L)$, we have:\n$$\n\\mathbb{P}(U>u) = \\frac{L-u}{L}\n$$\nThis probability is strictly positive because $u < L$.\n\nFor the numerator, $\\mathbb{P}(U>u+t)$, we must consider two cases based on the value of $u+t$:\n**Case 1: $u+t < L$**\nThis condition holds for $0 \\le u < L-t$. In this case, $u+t$ is within the interval $[0,L)$, so:\n$$\n\\mathbb{P}(U>u+t) = \\frac{L-(u+t)}{L}\n$$\nThe conditional probability is:\n$$\n\\mathbb{P}(U>u+t \\mid U>u) = \\frac{\\frac{L-u-t}{L}}{\\frac{L-u}{L}} = \\frac{L-u-t}{L-u} = 1 - \\frac{t}{L-u}\n$$\n**Case 2: $u+t \\ge L$**\nThis condition holds for $u \\in [L-t, L)$. In this case, $u+t$ is outside the interval $[0,L)$. The probability of $U$ exceeding $L$ (or being equal to it) is $0$.\n$$\n\\mathbb{P}(U>u+t) = 0\n$$\nThe conditional probability is:\n$$\n\\mathbb{P}(U>u+t \\mid U>u) = \\frac{0}{\\frac{L-u}{L}} = 0\n$$\nCombining these results, for $u \\in [0,L)$ and $t>0$:\n$$\n\\mathbb{P}(U>u+t \\mid U>u) = \\begin{cases} 1 - \\frac{t}{L-u} & \\text{if } 0 \\le u < L-t \\\\ 0 & \\text{if } L-t \\le u < L \\end{cases}\n$$\nFor the random variable $U$ to be memoryless, this conditional probability must equal $\\mathbb{P}(U>t)$ for all valid $u$ and $t$. For $t \\in (0,L)$, $\\mathbb{P}(U>t) = \\frac{L-t}{L} = 1 - \\frac{t}{L}$.\nThe condition for memorylessness would be $1 - \\frac{t}{L-u} = 1 - \\frac{t}{L}$ for $0 \\le u < L-t$. This implies $\\frac{t}{L-u} = \\frac{t}{L}$, which requires $L-u = L$, or $u=0$. Since this equality does not hold for any $u \\in (0, L-t)$, the uniform distribution is not memoryless.\n\nFurthermore, the expression $1 - \\frac{t}{L-u}$ clearly depends on $u$. As $u$ increases, the denominator $L-u$ decreases, so the fraction $\\frac{t}{L-u}$ increases, and the conditional probability decreases.\n\nLet's examine the limit as $u$ approaches $L$ from the left, i.e., $u \\to L^{-}$. For any fixed $t>0$, eventually $u$ will be greater than $L-t$. For all $u \\in [L-t, L)$, the conditional probability is $0$. Therefore,\n$$\n\\lim_{u \\to L^{-}} \\mathbb{P}(U>u+t \\mid U>u) = 0\n$$\n\n### Option-by-Option Analysis\n\n**A. For all $u \\in [0,L)$ and $t>0$ with $u+t<L$, one has $\\mathbb{P}(U>u+t \\mid U>u) = \\mathbb{P}(U>t)$, so $U$ is memoryless.**\nFrom our derivation, for $u+t<L$, $\\mathbb{P}(U>u+t \\mid U>u) = 1 - \\frac{t}{L-u}$. The term $\\mathbb{P}(U>t) = 1 - \\frac{t}{L}$ (for $t<L$). The equality $1 - \\frac{t}{L-u} = 1 - \\frac{t}{L}$ holds only if $u=0$. It does not hold \"for all\" $u$ in the specified range. Thus, the premise is false, and the conclusion that $U$ is memoryless is also false.\n**Verdict: Incorrect.**\n\n**B. For $0 \\le u < L-t$, $\\mathbb{P}(U>u+t \\mid U>u) = 1 - \\dfrac{t}{L-u}$, which depends on $u$ and tends to $0$ as $u \\to L^{-}$ for any fixed $t \\in (0,L)$; for $u \\ge L-t$, the conditional probability equals $0$. Hence $U$ is not memoryless.**\nThis statement is a complete and accurate summary of our findings.\n- For $0 \\le u < L-t$, the expression $\\mathbb{P}(U>u+t \\mid U>u) = 1 - \\frac{t}{L-u}$ is correct.\n- This expression clearly depends on $u$.\n- For $u \\ge L-t$ (and $u < L$), the probability is $0$.\n- The limit as $u \\to L^-$ is indeed $0$, because for any fixed $t>0$, any $u$ sufficiently close to $L$ will satisfy $u > L-t$.\n- The conclusion that $U$ is not memoryless is correct because the conditional probability depends on $u$.\n**Verdict: Correct.**\n\n**C. One has $\\mathbb{P}(U>u+t \\mid U>u) = 1$ for all $u \\in [0,L)$ and $t>0$ with $u+t<L$, because conditioning on survival removes the effect of elapsed time.**\nOur derived probability for this case is $1 - \\frac{t}{L-u}$. Since $L-u > t > 0$, this value is strictly between $0$ and $1$. It is not equal to $1$. The reasoning provided is a fallacious intuition; conditioning on $U>u$ changes the sample space, but does not make further survival a certainty.\n**Verdict: Incorrect.**\n\n**D. For any $t \\in (0,L)$ and any $u \\in [0,L)$, $\\mathbb{P}(U>u+t \\mid U>u) = 0$, since the support is bounded; therefore $U$ fails to be memoryless.**\nThis is incorrect. The probability is only $0$ if $u+t \\ge L$. For $u+t<L$, the probability is $1 - \\frac{t}{L-u}$, which is positive. For example, if $L=4$, $u=1$, $t=1$, then $u+t=2<4$ and the probability is $1 - \\frac{1}{4-1} = \\frac{2}{3} \\ne 0$. The statement's premise is false.\n**Verdict: Incorrect.**\n\n**E. For $0 \\le u < L-t$, $\\mathbb{P}(U>u+t \\mid U>u) = \\dfrac{L-t}{L}$, independent of $u$, while for $u \\ge L-t$ it equals $0$; thus $U$ is memoryless except at the endpoint.**\nThe first part of the statement claims the probability is $\\frac{L-t}{L} = 1-\\frac{t}{L}$. Our derivation gives $1-\\frac{t}{L-u}$. These are not equal for $u>0$. The claim that the probability is independent of $u$ in this range is false. The concluding phrase \"memoryless except at the endpoint\" is mathematically imprecise and the conclusion itself is wrong because the property fails for a whole interval of $u$ values.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3043856"}, {"introduction": "The ultimate test of understanding is the ability to build and simulate. This final practice moves from theory to application by tasking you with the construction of a simulator for a homogeneous Poisson process [@problem_id:3043907]. You will implement the inverse transform method to turn simple uniform random numbers into exponential inter-jump times, culminating in a tangible model of a pure-jump stochastic process.", "problem": "Consider a homogeneous Poisson process, a canonical pure-jump stochastic process frequently used in the analysis of stochastic differential equations, whose jump intensity is constant. Let the constant jump intensity be denoted by $\\lambda>0$. A foundational characterization states that the successive interarrival times between jumps are independent and identically distributed exponential random variables. Let $U$ denote a random variable with the uniform distribution on the unit interval, written as $U\\sim\\mathrm{Unif}(0,1)$, with cumulative distribution function (CDF) $F_U(u)=u$ for $u\\in(0,1)$. Let $T$ denote an exponential random variable with rate parameter $\\lambda$, written as $T\\sim\\mathrm{Exp}(\\lambda)$, with probability density function (PDF) $f_T(t)=\\lambda e^{-\\lambda t}$ for $t\\ge 0$ and CDF $F_T(t)=1-e^{-\\lambda t}$ for $t\\ge 0$.\n\nInverse Transform Sampling (ITS) is the procedure that constructs a random variable with target CDF $F$ by applying the inverse of its CDF to a uniform random variable. For the exponential distribution with rate $\\lambda$, the inverse CDF is given by $F_T^{-1}(u)=-\\ln(1-u)/\\lambda$ for $u\\in(0,1)$. Since $1-U$ has the same distribution as $U$, one can equivalently simulate $T$ via $T=-\\ln(U)/\\lambda$. This produces independent and identically distributed exponential samples, which, when interpreted as interarrival times, generate Poisson jump times by cumulative summation. Let $S_n=\\sum_{i=1}^n T_i$ denote the $n$-th jump time.\n\nYour task is to implement a complete, reproducible simulator based precisely on the ITS principle described above. The simulator must:\n\n- Generate independent and identically distributed uniform random variables $U_i\\sim\\mathrm{Unif}(0,1)$.\n- Map these variables to independent and identically distributed exponential interarrival times $T_i=-\\ln(U_i)/\\lambda$.\n- Accumulate interarrival times into jump times $S_n=\\sum_{i=1}^n T_i$ to count the number of jumps that occur on a finite horizon $H>0$.\n- Provide diagnostic computations to verify core distributional properties related to $\\mathrm{Exp}(\\lambda)$ samples.\n\nStart only from the following fundamental bases:\n- The definition of the uniform distribution and its CDF $F_U(u)=u$ for $u\\in(0,1)$.\n- The definition of the exponential distribution with rate $\\lambda$ and its CDF $F_T(t)=1-e^{-\\lambda t}$ for $t\\ge 0$.\n- The inverse transform principle: If $U\\sim\\mathrm{Unif}(0,1)$ and $X=F^{-1}(U)$, then $X$ has CDF $F$.\n- The characterization of interarrival times for a homogeneous Poisson process: successive interarrival times are independent and identically distributed with distribution $\\mathrm{Exp}(\\lambda)$.\n\nProgram requirements:\n- Use a fixed pseudo-random seed per test case to ensure reproducibility.\n- Implement a function that simulates and returns the number of jumps within horizon $H$ by sequentially generating interarrival times via $T_i=-\\ln(U_i)/\\lambda$ and stopping once $S_n>H$.\n- Implement a function that returns a sample mean of $N$ exponential interarrival times, comparing this mean indirectly to the theoretical value $1/\\lambda$.\n- Implement a function that computes the supremum norm of the difference between the empirical CDF of generated exponential samples and the theoretical CDF $F_T(t)=1-e^{-\\lambda t}$ evaluated on a uniform grid of $M$ points over an interval $[0,b]$, where $b>0$.\n\nTest suite:\n- Case $1$ (general “happy path”): $\\lambda=1.7$, $H=5.0$, seed $=123456$. Output the integer number of jumps within the horizon $H$.\n- Case $2$ (sample mean check): $\\lambda=2.0$, $N=10000$, seed $=42$. Output the float sample mean of the $N$ generated interarrival times.\n- Case $3$ (empirical CDF sup norm): $\\lambda=0.5$, $N=5000$, $M=1000$, $b=10.0$, seed $=202310$. Output the float supremum norm $\\sup_{t\\in\\mathcal{G}} \\left| \\widehat{F}_N(t)-\\left(1-e^{-\\lambda t}\\right) \\right|$, where $\\mathcal{G}$ is a grid of $M$ points uniformly spaced in $[0,b]$ and $\\widehat{F}_N(t)$ is the empirical CDF at $t$.\n- Case $4$ (small-rate boundary): $\\lambda=10^{-6}$, $H=1.0$, seed $=7$. Output the integer number of jumps within the horizon $H$.\n- Case $5$ (large-rate, short horizon): $\\lambda=100.0$, $H=0.05$, seed $=99$. Output the integer number of jumps within the horizon $H$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above, for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$. All integers and floats must be printed directly without additional text. No physical units are involved in this task.", "solution": "### Principle 1: Inverse Transform Sampling for the Exponential Distribution\n\nThe core of the simulation is generating random variates from an exponential distribution with rate parameter $\\lambda > 0$. The inverse transform sampling (ITS) method is a general technique for this purpose. It relies on the fact that if a random variable $U$ is uniformly distributed on the interval $(0, 1)$, then the random variable $X = F^{-1}(U)$ has the cumulative distribution function (CDF) $F$.\n\nFor an exponential random variable $T \\sim \\mathrm{Exp}(\\lambda)$, its CDF is given by:\n$$\nF_T(t) = 1 - e^{-\\lambda t} \\quad \\text{for } t \\ge 0\n$$\nTo find the inverse CDF, $F_T^{-1}(u)$, we set $F_T(t) = u$ for some $u \\in (0, 1)$ and solve for $t$:\n$$\nu = 1 - e^{-\\lambda t}\n$$\n$$\ne^{-\\lambda t} = 1 - u\n$$\n$$\n-\\lambda t = \\ln(1 - u)\n$$\n$$\nt = -\\frac{\\ln(1 - u)}{\\lambda}\n$$\nThus, the inverse CDF is $F_T^{-1}(u) = -\\frac{\\ln(1 - u)}{\\lambda}$. To generate a sample $T_i$ from $\\mathrm{Exp}(\\lambda)$, we first draw a sample $U_i$ from $\\mathrm{Unif}(0, 1)$ and then apply the transformation $T_i = F_T^{-1}(U_i)$.\n\nA key property of the uniform distribution is that if $U \\sim \\mathrm{Unif}(0, 1)$, then the random variable $V = 1 - U$ is also distributed as $\\mathrm{Unif}(0, 1)$. This allows for a simplification of the transformation formula, which is the one specified in the problem statement:\n$$\nT_i = -\\frac{\\ln(U_i)}{\\lambda}\n$$\nThis formula will be the foundation for generating all exponential random variates.\n\n### Task 1: Simulating Poisson Jumps within a Horizon\n\nA homogeneous Poisson process is characterized by the property that the interarrival times between successive events, $T_1, T_2, \\dots$, are independent and identically distributed (i.i.d.) random variables following an $\\mathrm{Exp}(\\lambda)$ distribution. The time of the $n$-th event (or jump) is the sum of the first $n$ interarrival times:\n$$\nS_n = \\sum_{i=1}^{n} T_i\n$$\nThe problem asks for the total number of jumps that occur within a time horizon $[0, H]$. This corresponds to finding the largest integer $k$ such that the $k$-th jump time $S_k$ is less than or equal to $H$.\nThe simulation algorithm proceeds as follows:\n1. Initialize a jump counter, $k$, to $0$ and the cumulative time, $S$, to $0$.\n2. Initialize a pseudo-random number generator with the specified seed.\n3. Enter a loop:\n    a. Generate a uniform random number $U \\sim \\mathrm{Unif}(0, 1)$.\n    b. Compute the next interarrival time $T = -\\frac{\\ln(U)}{\\lambda}$.\n    c. Add this time to the cumulative sum: $S \\leftarrow S + T$.\n    d. If the new cumulative time $S$ is greater than the horizon $H$, the loop terminates.\n    e. Otherwise, a jump has occurred within the horizon. Increment the jump counter: $k \\leftarrow k + 1$.\n4. The final value of $k$ is the result.\n\n### Task 2: Verifying the Sample Mean\n\nA fundamental property of the exponential distribution $T \\sim \\mathrm{Exp}(\\lambda)$ is that its theoretical mean or expected value is $E[T] = 1/\\lambda$. The Law of Large Numbers states that for a large sample of $N$ i.i.d. random variables $T_1, \\dots, T_N$, their sample mean $\\bar{T}_N$ converges to the theoretical mean:\n$$\n\\bar{T}_N = \\frac{1}{N} \\sum_{i=1}^{N} T_i \\xrightarrow{N\\to\\infty} E[T] = \\frac{1}{\\lambda}\n$$\nThis task requires computing the sample mean for a large sample size $N$. The algorithm is:\n1. Initialize a pseudo-random number generator with the specified seed.\n2. Generate $N$ uniform random numbers, $U_1, \\dots, U_N$.\n3. Transform each $U_i$ into an exponential variate $T_i = -\\frac{\\ln(U_i)}{\\lambda}$. This can be done efficiently using vector operations.\n4. Compute and return the arithmetic mean of the resulting sample $T_1, \\dots, T_N$.\n\n### Task 3: Verifying the Empirical CDF\n\nThe Glivenko-Cantelli theorem provides another powerful method for verifying a simulation. It states that the empirical cumulative distribution function (ECDF), $\\widehat{F}_N(t)$, converges uniformly to the true CDF, $F_T(t)$, as the sample size $N \\to \\infty$. The ECDF is defined for a sample $T_1, \\dots, T_N$ as the proportion of observations less than or equal to $t$:\n$$\n\\widehat{F}_N(t) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\{T_i \\le t\\}}\n$$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function. The task is to compute the supremum norm (or Kolmogorov-Smirnov distance) between the ECDF and the true CDF over a discrete grid of points:\n$$\nD_N = \\sup_{t \\in \\mathcal{G}} \\left| \\widehat{F}_N(t) - F_T(t) \\right|\n$$\nwhere $\\mathcal{G}$ is a uniform grid of $M$ points in the interval $[0, b]$. The algorithm is:\n1. Initialize a pseudo-random number generator with the specified seed.\n2. Generate a sample of $N$ exponential variates $T_1, \\dots, T_N$ using the ITS method.\n3. Create the evaluation grid $\\mathcal{G}$ of $M$ points $\\{t_j\\}$ uniformly spaced from $0$ to $b$.\n4. For each point $t_j$ in the grid, calculate the theoretical CDF value: $F_T(t_j) = 1 - e^{-\\lambda t_j}$.\n5. For each point $t_j$, calculate the ECDF value $\\widehat{F}_N(t_j)$. This is done most efficiently by first sorting the sample $T_1, \\dots, T_N$. Then, for each $t_j$, $\\widehat{F}_N(t_j)$ is found by counting how many samples are less than or equal to $t_j$ and dividing by $N$.\n6. Compute the absolute differences $|\\widehat{F}_N(t_j) - F_T(t_j)|$ for all $j$.\n7. The result is the maximum of these differences.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_jumps(lambda_val, H, seed):\n    \"\"\"\n    Simulates the number of jumps of a homogeneous Poisson process within a time horizon.\n    \n    Args:\n        lambda_val (float): The jump intensity (rate) of the Poisson process.\n        H (float): The time horizon.\n        seed (int): The seed for the pseudo-random number generator.\n        \n    Returns:\n        int: The number of jumps that occurred within the interval [0, H].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    current_time = 0.0\n    jump_count = 0\n    \n    while True:\n        # Generate a uniform random variable U ~ Unif(0, 1)\n        u = rng.uniform()\n        # Transform it into an exponential interarrival time T ~ Exp(lambda)\n        interarrival_time = -np.log(u) / lambda_val\n        \n        current_time += interarrival_time\n        \n        if current_time > H:\n            # The latest jump occurred after the horizon, so we break.\n            break\n        \n        # A jump occurred within the horizon.\n        jump_count += 1\n        \n    return jump_count\n\ndef calculate_mean(lambda_val, N, seed):\n    \"\"\"\n    Calculates the sample mean of N exponential random variates.\n    \n    Args:\n        lambda_val (float): The rate parameter of the exponential distribution.\n        N (int): The number of samples to generate.\n        seed (int): The seed for the pseudo-random number generator.\n        \n    Returns:\n        float: The sample mean of the generated variates.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate N uniform random samples\n    U = rng.uniform(size=N)\n    # Transform them into N exponential random samples\n    T = -np.log(U) / lambda_val\n    # Return the sample mean\n    return np.mean(T)\n\ndef calculate_ecdf_sup_norm(lambda_val, N, M, b, seed):\n    \"\"\"\n    Computes the supremum norm of the difference between the empirical CDF and the\n    theoretical CDF of an exponential distribution.\n    \n    Args:\n        lambda_val (float): The rate parameter of the exponential distribution.\n        N (int): The number of samples for the empirical CDF.\n        M (int): The number of points in the evaluation grid.\n        b (float): The upper bound of the interval for the grid.\n        seed (int): The seed for the pseudo-random number generator.\n        \n    Returns:\n        float: The supremum norm difference.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate N exponential samples\n    U = rng.uniform(size=N)\n    T_samples = -np.log(U) / lambda_val\n    \n    # Create the evaluation grid\n    grid_points = np.linspace(0, b, M)\n    \n    # Calculate the theoretical CDF on the grid\n    theoretical_cdf = 1.0 - np.exp(-lambda_val * grid_points)\n    \n    # Calculate the empirical CDF on the grid\n    # For efficiency, we first sort the samples.\n    T_samples.sort()\n    # np.searchsorted gives the count of samples <= each grid point.\n    empirical_cdf = np.searchsorted(T_samples, grid_points, side='right') / N\n    \n    # Compute the supremum norm of the difference\n    sup_norm = np.max(np.abs(empirical_cdf - theoretical_cdf))\n    \n    return sup_norm\n\ndef solve():\n    \"\"\"\n    Executes the test suite and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        # (lambda, H, seed) for case 1\n        (1.7, 5.0, 123456),\n        # (lambda, N, seed) for case 2\n        (2.0, 10000, 42),\n        # (lambda, N, M, b, seed) for case 3\n        (0.5, 5000, 1000, 10.0, 202310),\n        # (lambda, H, seed) for case 4\n        (1e-6, 1.0, 7),\n        # (lambda, H, seed) for case 5\n        (100.0, 0.05, 99),\n    ]\n\n    results = []\n\n    # Case 1\n    params1 = test_cases[0]\n    result1 = simulate_jumps(lambda_val=params1[0], H=params1[1], seed=params1[2])\n    results.append(result1)\n\n    # Case 2\n    params2 = test_cases[1]\n    result2 = calculate_mean(lambda_val=params2[0], N=params2[1], seed=params2[2])\n    results.append(result2)\n\n    # Case 3\n    params3 = test_cases[2]\n    result3 = calculate_ecdf_sup_norm(lambda_val=params3[0], N=params3[1], M=params3[2], b=params3[3], seed=params3[4])\n    results.append(result3)\n\n    # Case 4\n    params4 = test_cases[3]\n    result4 = simulate_jumps(lambda_val=params4[0], H=params4[1], seed=params4[2])\n    results.append(result4)\n\n    # Case 5\n    params5 = test_cases[4]\n    result5 = simulate_jumps(lambda_val=params5[0], H=params5[1], seed=params5[2])\n    results.append(result5)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3043907"}]}