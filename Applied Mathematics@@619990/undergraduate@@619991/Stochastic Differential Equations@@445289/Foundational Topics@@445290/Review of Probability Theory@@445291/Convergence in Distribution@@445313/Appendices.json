{"hands_on_practices": [{"introduction": "The Central Limit Theorem (CLT) is a cornerstone of probability, stating that under certain conditions, the standardized sample mean of independent and identically distributed random variables converges to a standard normal distribution. This exercise [@problem_id:1910214] provides a direct, hands-on application of the CLT. You will demonstrate how the sample mean of a discrete distribution—in this case, the Geometric distribution—can be approximated by the continuous normal curve as the sample size grows.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed random variables, each following a Geometric distribution with a probability of success denoted by $p$. The probability mass function for each $X_i$ is given by $P(X_i=k) = (1-p)^{k-1}p$ for $k = 1, 2, 3, \\dots$. This represents the number of trials needed to get the first success.\n\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample mean of these variables. Consider the standardized random variable $Z_n$ defined as:\n$$Z_n = \\frac{\\sqrt{n}(\\bar{X}_n - 1/p)}{\\sqrt{\\frac{1-p}{p^2}}}$$\nGiven a success probability of $p=0.4$, calculate the value of the limit $\\lim_{n \\to \\infty} P(Z_n \\le 1.5)$.\n\nRound your final answer to four significant figures.", "solution": "The given $X_{i}$ are independent and identically distributed with the geometric distribution on $\\{1,2,\\dots\\}$, with success probability $p$. For this parameterization, the mean and variance are\n$$\n\\mu=\\mathbb{E}[X_{i}]=\\frac{1}{p}, \\qquad \\sigma^{2}=\\operatorname{Var}(X_{i})=\\frac{1-p}{p^{2}}.\n$$\nThe sample mean is $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. By the Lindeberg–Levy Central Limit Theorem (CLT), since the $X_{i}$ are iid with finite mean $\\mu$ and variance $\\sigma^{2}$, we have\n$$\n\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\;\\xrightarrow{d}\\;N(0,1).\n$$\nThe standardized variable in the problem is\n$$\nZ_{n}=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\frac{1}{p}\\right)}{\\sqrt{\\frac{1-p}{p^{2}}}},\n$$\nwhich matches the CLT normalization using $\\mu=\\frac{1}{p}$ and $\\sigma=\\sqrt{\\frac{1-p}{p^{2}}}$. Therefore,\n$$\nZ_{n}\\;\\xrightarrow{d}\\;N(0,1).\n$$\nBy the Continuous Mapping Theorem (equivalently, by convergence in distribution at continuity points of the limit CDF), for any real $a$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Z_{n}\\le a)=\\Phi(a),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Taking $a=1.5$, and using $p=0.4$ as specified (which does not affect the standardized limit),\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Z_{n}\\le 1.5)=\\Phi(1.5).\n$$\nEvaluating the standard normal CDF at $1.5$ and rounding to four significant figures gives\n$$\n\\Phi(1.5)\\approx 0.9332.\n$$", "answer": "$$\\boxed{0.9332}$$", "id": "1910214"}, {"introduction": "While the Central Limit Theorem is powerful, it is not universal; its conclusions rely on the assumption of finite variance. This practice [@problem_id:1292889] explores a famous counterexample, the Cauchy distribution, which lacks a well-defined mean and variance. By analyzing the sample mean of Cauchy variables, you will discover why it fails to converge as the CLT would predict, highlighting the importance of verifying theoretical assumptions and introducing the power of characteristic functions.", "problem": "In a simplified physical model for a signal scattering experiment, a particle emitter is placed at a specific location. It fires particles at a horizontal detector screen. Due to the scattering process, the angle at which a particle is emitted, relative to the perpendicular axis to the screen, is random. Let this angle be $\\Theta$. It has been established that for this specific process, the random variable $X = \\tan(\\Theta)$ represents the position where the particle hits the screen, and $X$ follows a standard Cauchy distribution. The probability density function (PDF) for this distribution is given by $f(x) = \\frac{1}{\\pi(1+x^2)}$ for $x \\in (-\\infty, \\infty)$.\n\nAn experiment is conducted where $n$ such particles are fired independently, and their hit positions on the screen, $X_1, X_2, \\ldots, X_n$, are recorded. Each $X_i$ is an independent random variable following the standard Cauchy distribution. An analyst wants to characterize the average position by computing the sample mean, $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\nIdentify the limiting probability distribution of the sample mean $\\bar{X}_n$ as the number of trials $n \\to \\infty$.\n\nA. A Normal distribution with mean 0 and variance 1.\n\nB. A standard Cauchy distribution.\n\nC. A degenerate distribution (a point mass) at the value 0.\n\nD. A Uniform distribution on the interval $[-1, 1]$.\n\nE. The sequence of distributions for $\\bar{X}_n$ does not converge to any well-defined probability distribution.", "solution": "To determine the limiting distribution of the sample mean $\\bar{X}_n$, we will use the method of characteristic functions. The characteristic function of a random variable $X$ is defined as $\\phi_X(t) = E[\\exp(itX)]$. One of the key properties of characteristic functions is that they uniquely determine the distribution of a random variable. The Lévy Continuity Theorem states that a sequence of random variables $Y_n$ converges in distribution to a random variable $Y$ if and only if their characteristic functions $\\phi_{Y_n}(t)$ converge pointwise to the characteristic function $\\phi_Y(t)$.\n\nFirst, we need the characteristic function of a single standard Cauchy random variable $X_i$. For the standard Cauchy distribution with PDF $f(x) = \\frac{1}{\\pi(1+x^2)}$, the characteristic function is a standard result given by:\n$$ \\phi_{X_i}(t) = \\exp(-|t|) $$\nIt's worth noting that the conditions for the Central Limit Theorem (CLT) are not met. The CLT requires the random variables to have a finite variance (and finite mean). The mean and variance of the Cauchy distribution are undefined. Similarly, the Law of Large Numbers, which would suggest convergence to a point mass at the mean, does not apply because the mean is undefined. Therefore, we must use a more fundamental tool like characteristic functions.\n\nNext, let's find the characteristic function of the sum $S_n = \\sum_{i=1}^{n} X_i$. Since the random variables $X_1, X_2, \\ldots, X_n$ are independent and identically distributed (i.i.d.), the characteristic function of their sum is the product of their individual characteristic functions:\n$$ \\phi_{S_n}(t) = E[\\exp(itS_n)] = E\\left[\\exp\\left(it\\sum_{i=1}^{n} X_i\\right)\\right] = \\prod_{i=1}^{n} E[\\exp(itX_i)] = (\\phi_{X_i}(t))^n $$\nSubstituting the characteristic function of a standard Cauchy variable, we get:\n$$ \\phi_{S_n}(t) = (\\exp(-|t|))^n = \\exp(-n|t|) $$\nThis result shows that the sum of $n$ i.i.d. standard Cauchy variables is a Cauchy variable, but scaled. Specifically, $S_n$ has the distribution of $n$ times a standard Cauchy variable.\n\nNow, we can find the characteristic function of the sample mean, $\\bar{X}_n = \\frac{S_n}{n}$. We use the scaling property of characteristic functions: if $Y = aX$, then $\\phi_Y(t) = \\phi_X(at)$. In our case, $a = 1/n$ and the variable is $S_n$.\n$$ \\phi_{\\bar{X}_n}(t) = \\phi_{S_n}\\left(\\frac{t}{n}\\right) $$\nSubstituting the expression we found for $\\phi_{S_n}(t)$:\n$$ \\phi_{\\bar{X}_n}(t) = \\exp\\left(-n\\left|\\frac{t}{n}\\right|\\right) = \\exp\\left(-n\\frac{|t|}{n}\\right) = \\exp(-|t|) $$\nThis is a remarkable result. The characteristic function of the sample mean $\\bar{X}_n$ is $\\exp(-|t|)$, which is exactly the same as the characteristic function of a single standard Cauchy random variable, $\\phi_{X_i}(t)$.\n\nThis holds true for any sample size $n \\ge 1$. Since the characteristic function of $\\bar{X}_n$ is the same for all $n$, the sequence of characteristic functions trivially converges to $\\exp(-|t|)$ as $n \\to \\infty$.\n$$ \\lim_{n \\to \\infty} \\phi_{\\bar{X}_n}(t) = \\exp(-|t|) $$\nBy the Lévy Continuity Theorem, since the limiting characteristic function is that of a standard Cauchy distribution, the limiting distribution of the sample mean $\\bar{X}_n$ is a standard Cauchy distribution.\n\nTherefore, the correct option is B. Option A is incorrect because the CLT does not apply. Option C is incorrect because the Law of Large Numbers does not apply.", "answer": "$$\\boxed{B}$$", "id": "1292889"}, {"introduction": "Once we have established that a sequence of random variables converges in distribution, we can often find the limits of related sequences using the Continuous Mapping Theorem. This powerful theorem states that convergence is preserved under continuous functions, and this exercise [@problem_id:1292917] provides a perfect illustration. You will start with a sequence known to converge to a standard normal variable and determine the limiting distribution of its square, linking the normal distribution to the chi-squared distribution.", "problem": "Consider a sequence of random variables $\\{Z_n\\}_{n=1}^{\\infty}$. It is established that this sequence converges in distribution to a standard normal random variable, which we denote as $Z$. The probability density function of $Z$ is given by $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ for $z \\in (-\\infty, \\infty)$.\n\nA new sequence of random variables, $\\{Y_n\\}_{n=1}^{\\infty}$, is constructed by squaring each term of the original sequence, such that $Y_n = Z_n^2$.\n\nWhat is the limiting distribution of the sequence $\\{Y_n\\}$ as $n$ approaches infinity? Select the correct option from the choices below.\n\nA. A Chi-squared distribution with 1 degree of freedom, $\\chi^2(1)$.\n\nB. A standard Normal distribution, $N(0, 1)$.\n\nC. An Exponential distribution with a rate parameter of $\\lambda = 1/2$.\n\nD. A Gamma distribution with shape parameter $\\alpha=1/2$ and rate parameter $\\beta=1$.\n\nE. F-distribution with $(1, 1)$ degrees of freedom, $F(1, 1)$.", "solution": "The problem asks for the limiting distribution of the sequence $Y_n = Z_n^2$, given that the sequence $Z_n$ converges in distribution to a standard normal random variable $Z \\sim N(0, 1)$.\n\nThis problem can be solved using the Continuous Mapping Theorem. The theorem states that if a sequence of random variables $X_n$ converges in distribution to a random variable $X$ (denoted $X_n \\xrightarrow{d} X$), and $g$ is a real-valued function that is continuous at all points in the support of $X$, then the sequence of transformed random variables $g(X_n)$ also converges in distribution to the random variable $g(X)$. Symbolically, $g(X_n) \\xrightarrow{d} g(X)$.\n\nIn our case, the sequence of random variables is $\\{Z_n\\}$, and we are given that it converges in distribution to $Z \\sim N(0, 1)$.\n$$Z_n \\xrightarrow{d} Z \\quad \\text{where} \\quad Z \\sim N(0, 1)$$\n\nThe new sequence is defined by the transformation $Y_n = Z_n^2$. This corresponds to applying a function $g(x) = x^2$ to each term of the sequence $\\{Z_n\\}$.\nThe function $g(x) = x^2$ is a polynomial and is continuous for all real numbers $x \\in (-\\infty, \\infty)$. The support of the standard normal distribution $Z$ is the entire real line, and since $g(x)$ is continuous everywhere, the conditions for the Continuous Mapping Theorem are satisfied.\n\nAccording to the Continuous Mapping Theorem, the sequence $Y_n = g(Z_n) = Z_n^2$ must converge in distribution to the random variable $g(Z) = Z^2$.\n$$Y_n = Z_n^2 \\xrightarrow{d} Z^2$$\n\nNow, we need to identify the distribution of the random variable $Z^2$, where $Z$ is a standard normal random variable. By definition, the chi-squared distribution with $k$ degrees of freedom, denoted $\\chi^2(k)$, is the distribution of a sum of the squares of $k$ independent standard normal random variables.\n\nFor our case, we have the square of a single standard normal random variable. This corresponds to the case where $k=1$. Therefore, the random variable $Z^2$ follows a chi-squared distribution with 1 degree of freedom.\n$$Z^2 \\sim \\chi^2(1)$$\n\nCombining our results, the sequence $\\{Y_n\\}$ converges in distribution to a chi-squared distribution with 1 degree of freedom.\n$$Y_n \\xrightarrow{d} \\chi^2(1)$$\n\nLet's examine the options:\nA. A Chi-squared distribution with 1 degree of freedom, $\\chi^2(1)$. This matches our result.\nB. A standard Normal distribution, $N(0, 1)$. This is the limiting distribution of $Z_n$, not $Y_n$.\nC. An Exponential distribution with a rate parameter of $\\lambda = 1/2$. This is equivalent to a $\\chi^2(2)$ distribution, not $\\chi^2(1)$.\nD. A Gamma distribution with shape parameter $\\alpha=1/2$ and rate parameter $\\beta=1$. The $\\chi^2(k)$ distribution is a special case of the Gamma distribution with shape $\\alpha = k/2$ and rate $\\beta = 1/2$. For $k=1$, this would be $\\Gamma(\\alpha=1/2, \\beta=1/2)$. Option D has the wrong rate parameter.\nE. F-distribution with $(1, 1)$ degrees of freedom, $F(1, 1)$. An $F(d_1, d_2)$ distribution is the ratio of two independent chi-squared variables divided by their degrees of freedom. This is not the case here.\n\nThus, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1292917"}]}