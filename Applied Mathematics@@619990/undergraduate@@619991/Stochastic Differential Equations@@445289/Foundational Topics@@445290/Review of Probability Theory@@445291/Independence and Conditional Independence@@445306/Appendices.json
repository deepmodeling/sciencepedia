{"hands_on_practices": [{"introduction": "A powerful feature of statistical independence is that it remains intact even after we apply separate transformations to our random variables. This first exercise provides a hands-on opportunity to prove this fundamental result, starting from the formal definition of independence based on $\\sigma$-algebras. You will then apply this theorem to the increments of a Brownian motion, showing concretely how an abstract property translates into an intuitive result for a cornerstone process in SDE theory [@problem_id:3059599].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (BM). Recall that for $0 \\leq s < t$, the increment $B_{t} - B_{s}$ is Gaussian with mean $0$ and variance $t - s$, and increments over disjoint time intervals are independent. Define the sign function $\\operatorname{sgn}:\\mathbb{R}\\to\\{-1,0,1\\}$ by $\\operatorname{sgn}(x)=1$ if $x>0$, $\\operatorname{sgn}(x)=-1$ if $x<0$, and $\\operatorname{sgn}(0)=0$.\n\nStarting only from the definition of independence of random variables and the basic properties of $\\sigma$-algebras and measurable functions, establish that if $X$ and $Y$ are independent real-valued random variables and $f$ and $g$ are monotone Borel measurable functions, then $f(X)$ and $g(Y)$ are independent. Then apply this to Brownian increments as follows.\n\nFix times $0 = u_{0} < u_{1} < u_{2} < u_{3}$. Let $X := B_{u_{1}} - B_{u_{0}}$ and $Y := B_{u_{3}} - B_{u_{2}}$. Define $S_{1} := \\operatorname{sgn}(X)$ and $S_{2} := \\operatorname{sgn}(Y)$. Compute the joint probability $\\mathbb{P}(S_{1} = 1, S_{2} = 1)$. Express your final answer as a fraction.", "solution": "The problem consists of two parts. First, to establish a general theorem about the independence of functions of independent random variables. Second, to apply this theorem to a specific problem involving Brownian motion increments.\n\nPart 1: Proof of the General Theorem\n\nWe will prove a general result: if $X$ and $Y$ are independent real-valued random variables and $f$ and $g$ are Borel measurable functions, then the random variables $f(X)$ and $g(Y)$ are independent. The problem's condition of monotonicity is a sufficient, but not necessary, condition for being Borel measurable.\n\nLet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be the underlying probability space. The independence of two random variables $X$ and $Y$ means the $\\sigma$-algebras they generate, $\\sigma(X)$ and $\\sigma(Y)$, are independent. This means that for any set $A' \\in \\sigma(X)$ and any set $B' \\in \\sigma(Y)$, we have $\\mathbb{P}(A' \\cap B') = \\mathbb{P}(A') \\mathbb{P}(B')$.\n\nThe $\\sigma$-algebra generated by a random variable, e.g., $\\sigma(X)$, is the collection of all preimages of Borel sets: $\\sigma(X) = \\{X^{-1}(B) : B \\in \\mathcal{B}(\\mathbb{R})\\}$.\n\nNow, consider the random variable $U = f(X)$. The $\\sigma$-algebra generated by $U$ is $\\sigma(U) = \\sigma(f(X))$. Any set in this $\\sigma$-algebra is of the form $(f(X))^{-1}(A)$ for some Borel set $A \\in \\mathcal{B}(\\mathbb{R})$. This preimage can be written as:\n$$ (f(X))^{-1}(A) = \\{\\omega \\in \\Omega : f(X(\\omega)) \\in A\\} = \\{\\omega \\in \\Omega : X(\\omega) \\in f^{-1}(A)\\} = X^{-1}(f^{-1}(A)) $$\nSince $f$ is a Borel measurable function, the preimage $f^{-1}(A)$ is also a Borel set. Let $C = f^{-1}(A)$. Then any set in $\\sigma(f(X))$ is of the form $X^{-1}(C)$ for some $C \\in \\mathcal{B}(\\mathbb{R})$. By definition, any such set is an element of $\\sigma(X)$. Therefore, we have shown that $\\sigma(f(X)) \\subseteq \\sigma(X)$.\n\nSimilarly, for the random variable $V = g(Y)$ and the Borel measurable function $g$, we can show that $\\sigma(g(Y)) \\subseteq \\sigma(Y)$.\n\nWe are given that $X$ and $Y$ are independent, which means $\\sigma(X)$ and $\\sigma(Y)$ are independent. To show that $f(X)$ and $g(Y)$ are independent, we need to show that $\\sigma(f(X))$ and $\\sigma(g(Y))$ are independent.\n\nLet $A''$ be an arbitrary set in $\\sigma(f(X))$ and $B''$ be an arbitrary set in $\\sigma(g(Y))$. Since $\\sigma(f(X)) \\subseteq \\sigma(X)$, we have $A'' \\in \\sigma(X)$. Similarly, since $\\sigma(g(Y)) \\subseteq \\sigma(Y)$, we have $B'' \\in \\sigma(Y)$. Because $\\sigma(X)$ and $\\sigma(Y)$ are independent, it follows that:\n$$ \\mathbb{P}(A'' \\cap B'') = \\mathbb{P}(A'') \\mathbb{P}(B'') $$\nThis is the definition of independence for the $\\sigma$-algebras $\\sigma(f(X))$ and $\\sigma(g(Y))$. Consequently, the random variables $f(X)$ and $g(Y)$ are independent.\n\nPart 2: Application to Brownian Motion\n\nWe are asked to compute $\\mathbb{P}(S_{1} = 1, S_{2} = 1)$.\nThe random variables are $X := B_{u_{1}} - B_{u_{0}}$ and $Y := B_{u_{3}} - B_{u_{2}}$. Since $0 = u_{0} < u_{1} < u_{2} < u_{3}$, the time intervals $[u_{0}, u_{1}]$ and $[u_{2}, u_{3}]$ are disjoint. A fundamental property of Brownian motion is that its increments over disjoint time intervals are independent. Therefore, $X$ and $Y$ are independent.\n\nThe variables $S_{1}$ and $S_{2}$ are defined as $S_{1} := \\operatorname{sgn}(X)$ and $S_{2} := \\operatorname{sgn}(Y)$. The sign function, $h(x) = \\operatorname{sgn}(x)$, is a non-decreasing (monotone) function and is therefore Borel measurable.\n\nSince $X$ and $Y$ are independent and $\\operatorname{sgn}$ is a Borel measurable function, we can apply the theorem from Part 1 to conclude that $S_{1} = \\operatorname{sgn}(X)$ and $S_{2} = \\operatorname{sgn}(Y)$ are independent random variables.\n\nBecause $S_1$ and $S_2$ are independent, the joint probability is the product of the marginal probabilities:\n$$ \\mathbb{P}(S_{1} = 1, S_{2} = 1) = \\mathbb{P}(S_{1} = 1) \\mathbb{P}(S_{2} = 1) $$\nNow we compute the marginal probabilities.\nThe event $\\{S_{1} = 1\\}$ is equivalent to the event $\\{X > 0\\}$. The random variable $X = B_{u_{1}} - B_{u_{0}}$ is normally distributed with mean $0$ and variance $u_{1} - u_{0} > 0$. For any continuous normal distribution with mean $0$, the probability of being positive is exactly $\\frac{1}{2}$, because the probability density function is symmetric about $0$ and the probability of being exactly $0$ is zero.\nTherefore, $\\mathbb{P}(S_{1} = 1) = \\mathbb{P}(X > 0) = \\frac{1}{2}$.\n\nSimilarly, the event $\\{S_{2} = 1\\}$ is equivalent to $\\{Y > 0\\}$. The random variable $Y = B_{u_{3}} - B_{u_{2}}$ is normally distributed with mean $0$ and variance $u_{3} - u_{2} > 0$. By the same reasoning, its distribution is symmetric and continuous.\nThus, $\\mathbb{P}(S_{2} = 1) = \\mathbb{P}(Y > 0) = \\frac{1}{2}$.\n\nFinally, we can compute the joint probability:\n$$ \\mathbb{P}(S_{1} = 1, S_{2} = 1) = \\mathbb{P}(S_{1} = 1) \\mathbb{P}(S_{2} = 1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\nThe result is independent of the specific times $u_1, u_2, u_3$ as long as their ordering is preserved.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "3059599"}, {"introduction": "While applying separate functions to independent variables preserves their independence, what happens when a function combines them? This practice problem delves into this crucial distinction, using the simple case of summing two independent variables, $Z = X+Y$, to demonstrate how joint transformations typically create dependence. Working through these conceptual checks will help you avoid common pitfalls and build a more robust intuition for the behavior of stochastic systems [@problem_id:3059605].", "problem": "Consider independent random variables $X$ and $Y$ on a common probability space, neither almost surely constant. Define $Z = X + Y$. In addition, let $(B_t)_{t \\ge 0}$ and $(W_t)_{t \\ge 0}$ be independent standard Brownian motions (also called Wiener processes), and for fixed $t \\ge 0$ define $S_t = B_t + W_t$. Using only the foundational definitions of independence and conditional independence of random variables and sigma-algebras, the linearity of expectation, the definition of covariance, and the basic properties of Brownian motion (in particular, independent increments and joint Gaussianity of finite-dimensional distributions), determine which of the following statements are true. Select all that apply.\n\nA. If $X$ and $Y$ are independent, then $X$ and $Z$ are independent.\n\nB. If $X$ and $Y$ are independent with finite variance and $\\operatorname{Var}(X) > 0$, then $X$ and $Z$ are not independent.\n\nC. If $X$ and $Y$ are independent, then $X$ and $Z$ are conditionally independent given $Y$.\n\nD. For any measurable functions $g$ and $h$, the random variables $g(X)$ and $h(Y)$ are independent, but $X$ and $f(X,Y)$ for a measurable function $f$ of both arguments need not be independent.\n\nE. For any fixed $t > 0$, $B_t$ and $S_t$ are independent if and only if $t = 0$.", "solution": "We will analyze each statement individually based on the provided framework.\n\n**A. If $X$ and $Y$ are independent, then $X$ and $Z$ are independent.**\nThis statement is incorrect. A necessary condition for independence (for variables with finite variance) is that their covariance is zero. We compute the covariance of $X$ and $Z=X+Y$:\n$$ \\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X + Y) = \\operatorname{Cov}(X, X) + \\operatorname{Cov}(X, Y) $$\nSince $X$ and $Y$ are independent, $\\operatorname{Cov}(X, Y) = 0$. This leaves $\\operatorname{Cov}(X, Z) = \\operatorname{Var}(X)$. The problem states that $X$ is not an almost surely constant random variable, so its variance (if finite) must be strictly positive. Because the covariance is non-zero, $X$ and $Z$ are correlated and therefore not independent. Intuitively, knowing the value of $X$ provides information about the value of $Z$.\n\n**B. If $X$ and $Y$ are independent with finite variance and $\\operatorname{Var}(X) > 0$, then $X$ and $Z$ are not independent.**\nThis statement is correct. It formalizes the argument used to disprove option A. The premises are that $X$ and $Y$ are independent, they have finite variance, and $\\operatorname{Var}(X) > 0$. As derived above:\n$$ \\operatorname{Cov}(X, Z) = \\operatorname{Var}(X) $$\nGiven that $\\operatorname{Var}(X) > 0$, the covariance is non-zero. Random variables with non-zero covariance are not independent.\n\n**C. If $X$ and $Y$ are independent, then $X$ and $Z$ are conditionally independent given $Y$.**\nThis statement is incorrect. Conditional independence of $X$ and $Z$ given $Y$ means that knowledge of $Y$ makes $X$ and $Z$ independent. However, if we know $Y=y$, then $Z = X+y$. In this case, $Z$ is a simple deterministic, invertible function of $X$. Knowing the value of $X$ perfectly determines the value of $Z$ (and vice-versa). They are maximally dependent, not independent, unless $X$ is almost surely constant, which is ruled out.\n\n**D. For any measurable functions $g$ and $h$, the random variables $g(X)$ and $h(Y)$ are independent, but $X$ and $f(X,Y)$ for a measurable function $f$ of both arguments need not be independent.**\nThis statement is correct. It consists of two parts.\n1. The first part, that $g(X)$ and $h(Y)$ are independent if $X$ and $Y$ are, is a fundamental theorem of probability theory. The intuition is that since $X$ and $Y$ carry no information about each other, functions applied separately to them also cannot.\n2. The second part, that $X$ and a joint function $f(X,Y)$ need not be independent, is also true. The counterexample from option A, where $f(X,Y)=X+Y$, demonstrates this.\n\n**E. For any fixed $t > 0$, $B_t$ and $S_t$ are independent if and only if $t = 0$.**\nThis statement is correct. Since $B_t$ and $W_t$ are independent Brownian motions, for any fixed $t$, $B_t \\sim N(0,t)$ and $W_t \\sim N(0,t)$ are independent Gaussian random variables. The vector $(B_t, S_t) = (B_t, B_t+W_t)$ is a linear transformation of the Gaussian vector $(B_t, W_t)$, and is therefore itself a bivariate Gaussian vector. For jointly Gaussian variables, independence is equivalent to zero covariance. Let's compute their covariance:\n$$ \\operatorname{Cov}(B_t, S_t) = \\operatorname{Cov}(B_t, B_t + W_t) = \\operatorname{Cov}(B_t, B_t) + \\operatorname{Cov}(B_t, W_t) $$\nWe have $\\operatorname{Cov}(B_t, B_t) = \\operatorname{Var}(B_t) = t$. Since $B_t$ and $W_t$ are independent, $\\operatorname{Cov}(B_t, W_t) = 0$. Thus:\n$$ \\operatorname{Cov}(B_t, S_t) = t $$\nFor $B_t$ and $S_t$ to be independent, their covariance must be zero, which happens if and only if $t=0$.\nThe statement is \"For any fixed $t > 0$, ($B_t$ and $S_t$ are independent) iff ($t = 0$)\". Let's fix a $t_0 > 0$. The proposition becomes \"(independence at $t_0$) $\\iff (t_0=0)$\". Since $t_0 > 0$, the right side is false. We also showed that for $t_0 > 0$, the variables are not independent, so the left side is also false. The biconditional \"False $\\iff$ False\" is a true statement. This holds for any choice of $t>0$.\n\nTherefore, the correct statements are B, D, and E.", "answer": "$$\\boxed{BDE}$$", "id": "3059605"}, {"introduction": "The theoretical underpinnings of independence have profound practical consequences, especially in the numerical simulation of SDEs. This problem illustrates what happens when a core assumption—the independence of noise increments—is violated in a common algorithm like the Euler-Maruyama method. By calculating the resulting error, you will see firsthand why a rigorous grasp of independence is not just an academic exercise but is essential for ensuring the accuracy of computational models in science and finance [@problem_id:3059597].", "problem": "Consider the stochastic differential equation (SDE) $dX_{t}=\\sigma\\,dW_{t}$ with deterministic initial condition $X_{0}=x_{0}$, where $W_{t}$ is a standard Brownian motion. The Euler–Maruyama time discretization with uniform step size $\\Delta t=T/N$ over $[0,T]$ constructs the approximate path via\n$$\nX_{n+1}^{\\Delta t}=X_{n}^{\\Delta t}+\\sigma\\,\\Delta W_{n},\\quad n=0,1,\\dots,N-1,\\quad X_{0}^{\\Delta t}=x_{0},\n$$\nwhere $\\Delta W_{n}=W_{(n+1)\\Delta t}-W_{n\\Delta t}$. In the ideal setting, the increments $(\\Delta W_{n})$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\Delta t$. Suppose, however, that the pseudo-random number generator used in the simulation produces a sequence of Gaussian increments with correct marginal distribution $N(0,\\Delta t)$ but with nearest-neighbor correlation: for all $i,j\\in\\{0,1,\\dots,N-1\\}$,\n$$\n\\mathbb{E}[\\Delta W_{i}]=0,\\quad \\mathbb{E}[(\\Delta W_{i})^{2}]=\\Delta t,\\quad \\mathbb{E}[\\Delta W_{i}\\Delta W_{j}]=\\begin{cases}\n\\rho\\,\\Delta t,&|i-j|=1,\\\\\n0,&|i-j|\\geq 2,\\\\\n\\Delta t,&i=j,\n\\end{cases}\n$$\nwhere $\\rho\\in[-\\tfrac{1}{2},\\tfrac{1}{2}]$ is a fixed constant ensuring that the covariance matrix is positive semidefinite.\n\nStarting from the definitions of Brownian motion, independence, and the Euler–Maruyama method, derive the weak error for the test function $\\varphi(x)=x^{2}$ at the terminal time $T$, defined as\n$$\n\\mathbb{E}\\big[\\varphi(X_{T}^{\\Delta t})\\big]-\\mathbb{E}\\big[\\varphi(X_{T})\\big]=\\mathbb{E}\\big[(X_{T}^{\\Delta t})^{2}\\big]-\\mathbb{E}\\big[X_{T}^{2}\\big],\n$$\nand show how the dependence in $(\\Delta W_{n})$ alters the asymptotic behavior as $\\Delta t\\to 0$. Express the final answer as the closed-form expression for the limiting weak error $\\displaystyle\\lim_{\\Delta t\\to 0}\\big(\\mathbb{E}[(X_{T}^{\\Delta t})^{2}]-\\mathbb{E}[X_{T}^{2}]\\big)$ in terms of $\\sigma$, $\\rho$, and $T$. Your final answer must be a single analytic expression. No rounding is required, and no units are involved.", "solution": "To find the limiting weak error, we must compute the second moment for both the exact analytical solution and the numerical approximation, and then find the limit of their difference as the time step $\\Delta t \\to 0$.\n\n**1. Analysis of the Exact Solution**\n\nThe SDE is $dX_t = \\sigma dW_t$ with initial condition $X_0 = x_0$. Integrating from $0$ to $T$ gives the exact solution:\n$$\nX_T = X_0 + \\sigma (W_T - W_0) = x_0 + \\sigma W_T\n$$\nWe compute the expectation of its square, $\\mathbb{E}[X_T^2]$:\n$$\n\\mathbb{E}[X_T^2] = \\mathbb{E}[(x_0 + \\sigma W_T)^2] = \\mathbb{E}[x_0^2 + 2x_0\\sigma W_T + \\sigma^2 W_T^2]\n$$\nUsing the linearity of expectation and the properties of standard Brownian motion ($\\mathbb{E}[W_T] = 0$ and $\\mathbb{E}[W_T^2] = \\text{Var}(W_T) = T$):\n$$\n\\mathbb{E}[X_T^2] = x_0^2 + 2x_0\\sigma \\mathbb{E}[W_T] + \\sigma^2 \\mathbb{E}[W_T^2] = x_0^2 + \\sigma^2 T\n$$\n\n**2. Analysis of the Numerical Solution**\n\nThe Euler–Maruyama scheme is $X_{n+1}^{\\Delta t} = X_n^{\\Delta t} + \\sigma \\Delta W_n$. Unrolling the recursion from $n=0$ to $N-1$ gives the solution at time $T=N\\Delta t$, denoted $X_T^{\\Delta t}$:\n$$\nX_T^{\\Delta t} = X_N^{\\Delta t} = x_0 + \\sigma \\sum_{n=0}^{N-1} \\Delta W_n\n$$\nNext, we compute the expectation of its square:\n$$\n\\mathbb{E}[(X_T^{\\Delta t})^2] = \\mathbb{E}\\left[\\left(x_0 + \\sigma \\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right] = x_0^2 + 2x_0\\sigma \\mathbb{E}\\left[\\sum_{n=0}^{N-1} \\Delta W_n\\right] + \\sigma^2 \\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right]\n$$\nSince $\\mathbb{E}[\\Delta W_n]=0$ for all $n$, the middle term vanishes. The expression simplifies to:\n$$\n\\mathbb{E}[(X_T^{\\Delta t})^2] = x_0^2 + \\sigma^2 \\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right]\n$$\nWe expand the squared sum:\n$$\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{N-1} \\Delta W_i\\right)^2\\right] = \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\mathbb{E}[\\Delta W_i \\Delta W_j]\n$$\nWe use the given covariance structure to evaluate this sum by considering three cases:\n-   Case 1 ($i=j$): The diagonal terms.\n$$\n\\sum_{i=0}^{N-1} \\mathbb{E}[(\\Delta W_i)^2] = \\sum_{i=0}^{N-1} \\Delta t = N \\Delta t = T\n$$\n-   Case 2 ($|i-j|=1$): The nearest-neighbor off-diagonal terms. There are $2(N-1)$ such pairs in the double summation (e.g., $(0,1), (1,0), (1,2), (2,1), \\dots$).\n$$\n\\sum_{|i-j|=1} \\mathbb{E}[\\Delta W_i \\Delta W_j] = 2(N-1) \\times (\\rho \\Delta t)\n$$\n-   Case 3 ($|i-j|\\geq 2$): For these terms, the expectation is zero.\n\nCombining the contributions, we get:\n$$\n\\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right] = T + 2(N-1)\\rho \\Delta t\n$$\nSubstituting this back, we have:\n$$\n\\mathbb{E}[(X_T^{\\Delta t})^2] = x_0^2 + \\sigma^2 \\big(T + 2(N-1)\\rho \\Delta t\\big)\n$$\n\n**3. Calculation of the Weak Error and its Limit**\n\nThe weak error is the difference between the numerical and exact second moments:\n$$\n\\mathbb{E}[(X_T^{\\Delta t})^2] - \\mathbb{E}[X_T^2] = \\left[x_0^2 + \\sigma^2 \\big(T + 2(N-1)\\rho \\Delta t\\big)\\right] - \\left[x_0^2 + \\sigma^2 T\\right]\n$$\n$$\n\\text{Weak Error} = 2\\sigma^2(N-1)\\rho \\Delta t\n$$\nTo find the limit as $\\Delta t \\to 0$, we substitute $N = T/\\Delta t$:\n$$\n\\text{Weak Error} = 2\\sigma^2\\left(\\frac{T}{\\Delta t}-1\\right)\\rho \\Delta t = 2\\sigma^2\\rho\\left(T - \\Delta t\\right)\n$$\nTaking the limit as $\\Delta t \\to 0$:\n$$\n\\lim_{\\Delta t \\to 0} \\left(\\mathbb{E}[(X_T^{\\Delta t})^2] - \\mathbb{E}[X_T^2]\\right) = \\lim_{\\Delta t \\to 0} 2\\sigma^2\\rho(T - \\Delta t) = 2\\sigma^2\\rho T\n$$\nThis result demonstrates that while an ideal Euler-Maruyama scheme would have zero weak error for this problem, the presence of correlation ($\\rho \\neq 0$) introduces a systematic bias that does not vanish as the step size decreases. Instead, it converges to a constant value.", "answer": "$$\n\\boxed{2 \\rho \\sigma^{2} T}\n$$", "id": "3059597"}]}