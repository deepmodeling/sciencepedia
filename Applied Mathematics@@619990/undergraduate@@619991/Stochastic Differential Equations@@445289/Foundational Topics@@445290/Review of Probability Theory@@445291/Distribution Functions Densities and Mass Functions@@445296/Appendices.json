{"hands_on_practices": [{"introduction": "A cornerstone of stochastic calculus is the deep connection between stochastic differential equations and partial differential equations. The probability density of a diffusion process evolves according to a PDE known as the Fokker-Planck equation, which for standard Brownian motion simplifies to the heat equation. This exercise will guide you through deriving the famous Gaussian transition density, not from probabilistic arguments, but by solving the heat equation using the powerful method of Fourier transforms. This practice illuminates how analytical tools can be used to uncover the fundamental probability distributions in stochastic processes [@problem_id:3049562].", "problem": "Consider a one-dimensional standard Brownian motion $X_{t}$ with initial condition $X_{0} = x_{0} \\in \\mathbb{R}$. The transition probability density function (PDF) $p(t,x \\mid x_{0})$ satisfies the heat partial differential equation (PDE) $$\\frac{\\partial}{\\partial t}p(t,x \\mid x_{0}) = \\frac{1}{2}\\frac{\\partial^{2}}{\\partial x^{2}}p(t,x \\mid x_{0}),$$ with the initial condition $p(0,x \\mid x_{0}) = \\delta(x - x_{0})$, where $\\delta$ denotes the Dirac delta function. Using characteristic functions and Fourier methods for densities, apply the Fourier transform (FT) in the spatial variable $x$ with the convention $$\\hat{p}(t,k) = \\int_{-\\infty}^{\\infty} \\exp(i k x)\\, p(t,x \\mid x_{0})\\, dx,$$ and inverse Fourier transform $$p(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(-i k x)\\, \\hat{p}(t,k)\\, dk.$$ Starting only from the PDE, the initial condition, and these transform definitions, derive a closed-form analytic expression for $p(t,x \\mid x_{0})$. Show that the inverse transform yields the Gaussian kernel. Your final answer must be a single analytical expression in $x$, $t$, and $x_{0}$ with no unspecified constants. If you choose to approximate any constants, round your answer to four significant figures; otherwise provide the exact expression.", "solution": "The fundamental base is the characterization of the transition probability density function (PDF) $p(t,x \\mid x_{0})$ for standard Brownian motion via the heat partial differential equation (PDE) $$\\frac{\\partial}{\\partial t}p(t,x \\mid x_{0}) = \\frac{1}{2}\\frac{\\partial^{2}}{\\partial x^{2}}p(t,x \\mid x_{0}),$$ with the Dirac delta function initial condition $p(0,x \\mid x_{0}) = \\delta(x - x_{0})$. We use the Fourier transform (FT) in $x$ defined by $$\\hat{p}(t,k) = \\int_{-\\infty}^{\\infty} \\exp(i k x)\\, p(t,x \\mid x_{0})\\, dx,$$ with inverse $$p(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(-i k x)\\, \\hat{p}(t,k)\\, dk.$$\n\nWe first transform the PDE. Using the property of the Fourier transform that $$\\mathcal{F}\\left\\{\\frac{\\partial^{2}}{\\partial x^{2}}p(t,x \\mid x_{0})\\right\\}(k) = -k^{2}\\hat{p}(t,k),$$ and linearity of the transform, we obtain from the PDE $$\\frac{\\partial}{\\partial t}\\hat{p}(t,k) = \\frac{1}{2}\\,\\mathcal{F}\\left\\{\\frac{\\partial^{2}}{\\partial x^{2}}p(t,x \\mid x_{0})\\right\\}(k) = \\frac{1}{2}(-k^{2})\\hat{p}(t,k) = -\\frac{1}{2}k^{2}\\hat{p}(t,k).$$ Thus, for each fixed $k \\in \\mathbb{R}$, $\\hat{p}(t,k)$ satisfies the ordinary differential equation in $t$ $$\\frac{\\partial}{\\partial t}\\hat{p}(t,k) = -\\frac{1}{2}k^{2}\\hat{p}(t,k).$$\n\nThe initial condition in $x$ transforms to an initial condition in $k$ via $$\\hat{p}(0,k) = \\int_{-\\infty}^{\\infty} \\exp(i k x)\\,\\delta(x - x_{0})\\, dx = \\exp(i k x_{0}).$$ Solving the linear ordinary differential equation with this initial condition yields $$\\hat{p}(t,k) = \\exp(i k x_{0})\\,\\exp\\!\\left(-\\frac{1}{2}k^{2}t\\right).$$\n\nWe now invert the Fourier transform to recover $p(t,x \\mid x_{0})$. Using the inverse transform definition, we have\n\n$$\np(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(-i k x)\\,\\hat{p}(t,k)\\, dk = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(-i k x)\\,\\exp(i k x_{0})\\,\\exp\\!\\left(-\\frac{1}{2}k^{2}t\\right) dk.\n$$\n\nCombine the exponentials to obtain\n\n$$\np(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{1}{2}t\\,k^{2} - i k (x - x_{0})\\right)\\, dk.\n$$\n\nThis is a standard Gaussian integral in $k$ with a linear term in $k$. Complete the square in the exponent. Write\n\n$$\n-\\frac{1}{2}t\\,k^{2} - i k (x - x_{0}) = -\\frac{t}{2}\\left[k^{2} + \\frac{2 i (x - x_{0})}{t}k\\right] = -\\frac{t}{2}\\left[k + \\frac{i (x - x_{0})}{t}\\right]^{2} - \\frac{(x - x_{0})^{2}}{2 t}.\n$$\n\nTherefore,\n\n$$\np(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{(x - x_{0})^{2}}{2 t}\\right)\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{t}{2}\\left[k + \\frac{i (x - x_{0})}{t}\\right]^{2}\\right)\\, dk.\n$$\n\nShift the integration variable by setting $u = k + \\frac{i (x - x_{0})}{t}$. Because the integrand is entire and the Gaussian decays sufficiently fast, the contour shift does not change the value of the integral, yielding\n\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{t}{2}\\,u^{2}\\right)\\, du = \\sqrt{\\frac{2\\pi}{t}},\n$$\n\nusing the classical Gaussian integral identity $\\int_{-\\infty}^{\\infty}\\exp(-a u^{2})\\, du = \\sqrt{\\frac{\\pi}{a}}$ for $a  0$, here with $a = \\frac{t}{2}$. Substituting this into the expression for $p(t,x \\mid x_{0})$ gives\n\n$$\np(t,x \\mid x_{0}) = \\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{(x - x_{0})^{2}}{2 t}\\right)\\,\\sqrt{\\frac{2\\pi}{t}} = \\frac{1}{\\sqrt{2\\pi t}}\\,\\exp\\!\\left(-\\frac{(x - x_{0})^{2}}{2 t}\\right).\n$$\n\nThis is the Gaussian kernel in $(x,t)$ centered at $x_{0}$ with variance parameter $t$, which is the known transition density for standard Brownian motion. It satisfies the PDE and the initial condition in the sense of distributions and is normalized, as $\\int_{-\\infty}^{\\infty} p(t,x \\mid x_{0})\\, dx = 1$ for each $t  0$, confirming it is a valid probability density function.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi t}}\\exp\\!\\left(-\\frac{(x - x_{0})^{2}}{2 t}\\right)}$$", "id": "3049562"}, {"introduction": "Having derived the transition density for a single time step, a natural question is how these densities compose over time. For Markov processes like Brownian motion, the evolution of the system is governed by the Chapman-Kolmogorov equation, which states that a transition from point $x$ to $z$ can be seen as a transition from $x$ to an intermediate point $y$, followed by a transition from $y$ to $z$, summed over all possible intermediate points. This exercise provides hands-on verification of this principle by showing that the convolution of two Gaussian transition densities yields another Gaussian density corresponding to the summed time interval, reinforcing the semigroup property of the Brownian motion transition kernel [@problem_id:3049567].", "problem": "Let $\\{X_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion, which is the canonical solution to the stochastic differential equation (SDE) $dX_{t} = dW_{t}$ with $X_{0} = x \\in \\mathbb{R}$, where $\\{W_{t}\\}_{t \\geq 0}$ denotes a standard Wiener process. Brownian motion has stationary and independent increments: for any $t,s  0$, the increment $X_{t+s} - X_{t}$ is independent of $\\{X_{u}\\}_{0 \\leq u \\leq t}$ and is normally distributed with mean $0$ and variance $s$. The transition density of a Gaussian with mean $0$ and variance $t$ evaluated at $u \\in \\mathbb{R}$ is given by\n$$\n\\phi_{t}(u) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\!\\left(-\\frac{u^{2}}{2t}\\right), \\quad t0.\n$$\nConsider the integral representation of the two-step transition via an intermediate spatial point $y \\in \\mathbb{R}$,\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} \\phi_{t}(y-x)\\,\\phi_{s}(z-y)\\,dy,\n$$\nfor fixed $t0$, $s0$, and $x,z \\in \\mathbb{R}$. Using only the fundamental properties of Gaussian densities and independence of increments, evaluate $I(t,s,x,z)$ explicitly as a simplified closed-form analytic expression in terms of $t$, $s$, $x$, and $z$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "This integral represents the Chapman-Kolmogorov equation for the transition density of a one-dimensional standard Brownian motion. Let $p(t, x, y)$ be the probability density of the process being at position $y$ at time $t$, given it started at position $x$ at time $0$. For a standard Brownian motion starting at $X_0=x$, the position $X_t$ is a random variable with a normal distribution of mean $x$ and variance $t$. Its density is given by $p(t,x,y) = \\frac{1}{\\sqrt{2\\pi t}} \\exp(-\\frac{(y-x)^2}{2t}) = \\phi_t(y-x)$. The integral $I(t,s,x,z)$ is thus a convolution of two such transition densities:\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} p(t,x,y) p(s,y,z) dy.\n$$\nThis calculates the probability density of transitioning from $x$ to $z$ in a total time of $t+s$ by passing through any intermediate point $y$ at time $t$. Due to the stationary and independent increments of Brownian motion, the total displacement $X_{t+s} - X_0$ is the sum of two independent increments, $(X_t - X_0)$ and $(X_{t+s} - X_t)$, which are normally distributed with variances $t$ and $s$, respectively. Their sum is therefore also a normal random variable with variance $t+s$. Thus, we expect the result to be the transition density for a time interval of $t+s$, which is $\\phi_{t+s}(z-x)$. We shall now verify this through direct computation of the integral.\n\nWe begin by substituting the explicit form of the Gaussian densities into the integral:\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{(y-x)^{2}}{2t}\\right) \\frac{1}{\\sqrt{2\\pi s}} \\exp\\left(-\\frac{(z-y)^{2}}{2s}\\right) dy.\n$$\nWe can combine the constant pre-factors and the arguments of the exponential functions:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(y-x)^{2}}{2t} - \\frac{(z-y)^{2}}{2s} \\right) dy.\n$$\nLet us analyze the exponent, which we denote by $E(y)$:\n$$\nE(y) = -\\left( \\frac{(y-x)^{2}}{2t} + \\frac{(z-y)^{2}}{2s} \\right) = -\\frac{s(y-x)^{2} + t(z-y)^{2}}{2ts}.\n$$\nExpanding the squared terms in the numerator:\n$$\ns(y^{2} - 2xy + x^{2}) + t(z^{2} - 2zy + y^{2}) = sy^{2} - 2sxy + sx^{2} + tz^{2} - 2tzy + ty^{2}.\n$$\nWe group the terms based on powers of the integration variable $y$:\n$$\n(s+t)y^{2} - 2(sx+tz)y + (sx^{2}+tz^{2}).\n$$\nOur goal is to complete the square for the terms involving $y$. The general form of a quadratic $Ay^{2} - 2By + C$ can be written as $A(y-B/A)^2 + C - B^2/A$. Here, $A = s+t$ and $B = sx+tz$.\nSo, the quadratic in $y$ can be expressed as:\n$$\n(s+t)\\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(sx+tz)^{2}}{s+t} + (sx^{2}+tz^{2}).\n$$\nLet's simplify the terms not involving $y$:\n\\begin{align*}\n(sx^{2}+tz^{2}) - \\frac{(sx+tz)^{2}}{s+t} = \\frac{(s+t)(sx^{2}+tz^{2}) - (s^{2}x^{2}+2stxz+t^{2}z^{2})}{s+t} \\\\\n= \\frac{(s^{2}x^{2}+stz^{2}+stx^{2}+t^{2}z^{2}) - (s^{2}x^{2}+2stxz+t^{2}z^{2})}{s+t} \\\\\n= \\frac{stx^{2} - 2stxz + stz^{2}}{s+t} \\\\\n= \\frac{st(x^{2}-2xz+z^{2})}{s+t} = \\frac{st(z-x)^{2}}{s+t}.\n\\end{align*}\nSubstituting this back into the expression for the exponent $E(y)$:\n$$\nE(y) = -\\frac{1}{2ts} \\left[ (s+t)\\left(y - \\frac{sx+tz}{s+t}\\right)^{2} + \\frac{st(z-x)^{2}}{s+t} \\right].\n$$\n$$\nE(y) = -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{st(z-x)^{2}}{2ts(s+t)} = -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(z-x)^{2}}{2(s+t)}.\n$$\nNow, we substitute this form of the exponent back into the integral for $I(t,s,x,z)$:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(z-x)^{2}}{2(s+t)} \\right) dy.\n$$\nThe term involving $(z-x)^{2}$ is constant with respect to $y$ and can be factored out of the integral:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} \\right) dy.\n$$\nThe remaining integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-a(y-\\mu)^{2})dy = \\sqrt{\\frac{\\pi}{a}}$.\nIn our case, $a = \\frac{s+t}{2ts}$ and $\\mu = \\frac{sx+tz}{s+t}$. The value of the integral is:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\mu\\right)^{2} \\right) dy = \\sqrt{\\frac{\\pi}{(s+t)/(2ts)}} = \\sqrt{\\frac{2\\pi ts}{s+t}}.\n$$\nSubstituting this result back into the expression for $I(t,s,x,z)$:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) \\sqrt{\\frac{2\\pi ts}{s+t}}.\n$$\nSimplifying the expression by canceling terms:\n$$\nI(t,s,x,z) = \\frac{\\sqrt{2\\pi ts}}{2\\pi\\sqrt{ts}\\sqrt{s+t}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) = \\frac{1}{\\sqrt{2\\pi(s+t)}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right).\n$$\nThis is the probability density function for a normal distribution with mean $0$ and variance $t+s$, evaluated at $z-x$. According to the problem's notation, this is $\\phi_{t+s}(z-x)$. The result matches our initial expectation based on the properties of Brownian motion, thus confirming the calculation.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi(t+s)}} \\exp\\left(-\\frac{(z-x)^{2}}{2(t+s)}\\right)}$$", "id": "3049567"}, {"introduction": "Beyond understanding the distribution of the process at a single point in time, we often need to characterize the distribution of functionals of the entire process path. This exercise focuses on one of the simplest yet most important examples: a linear combination of the values of a Brownian motion at several distinct time points. By directly computing the mean and variance from the fundamental covariance structure of Brownian motion, you will derive the exact probability density for this new random variable. This practice demonstrates a key property of Gaussian processes: that linear operations on the process yield new random variables that are also Gaussian, whose distributions are fully determined by their first two moments [@problem_id:3049614].", "problem": "Let $\\{W_t\\}_{t \\ge 0}$ be a standard Brownian motion, meaning $W_0 = 0$, $\\mathbb{E}[W_t] = 0$ for all $t \\ge 0$, and $\\operatorname{Cov}(W_s, W_t) = \\min\\{s,t\\}$ for all $s,t \\ge 0$. Fix integers $n \\ge 1$, real times $0  t_1  t_2  \\dots  t_n$, and real coefficients $c_1, \\dots, c_n$ that are not all zero. Consider the linear functional\n$$\nL(W) \\;=\\; \\sum_{i=1}^{n} c_i\\, W_{t_i}.\n$$\nUsing only the defining properties of Brownian motion stated above, the linearity of expectation and covariance, and the well-tested fact that finite collections of Brownian motion values are jointly Gaussian so that linear functionals of them are Gaussian, compute the mean and variance of $L(W)$ from first principles via the covariance structure. Then, by the change-of-variables formula for probability densities applied to a Gaussian transformation, derive the exact analytic expression for the probability density function $f_L(x)$ of $L(W)$ as a function of $x \\in \\mathbb{R}$, in closed form in terms of $c_i$ and $t_i$.\n\nYour final answer must be a single closed-form expression for $f_L(x)$, written in terms of $x$, $\\{c_i\\}_{i=1}^{n}$, and $\\{t_i\\}_{i=1}^{n}$. Do not include any integrals in the final expression. No rounding is required.", "solution": "The problem asks for the probability density function (PDF) of the random variable $L(W) = \\sum_{i=1}^{n} c_i W_{t_i}$. It is given that $L(W)$ is a a Gaussian (normal) random variable because it is a linear combination of jointly Gaussian random variables $(W_{t_1}, \\dots, W_{t_n})$. A Gaussian distribution is completely characterized by its mean and variance. Therefore, our first step is to compute these two parameters for $L(W)$.\n\nFirst, we compute the mean of $L(W)$, denoted by $\\mu_L$. Using the linearity of the expectation operator, we have:\n$$\n\\mu_L = \\mathbb{E}[L(W)] = \\mathbb{E}\\left[\\sum_{i=1}^{n} c_i W_{t_i}\\right] = \\sum_{i=1}^{n} c_i \\mathbb{E}[W_{t_i}]\n$$\nAccording to the problem definition, a standard Brownian motion has zero mean, i.e., $\\mathbb{E}[W_t] = 0$ for all $t \\ge 0$. Substituting this into our expression gives:\n$$\n\\mu_L = \\sum_{i=1}^{n} c_i (0) = 0\n$$\nThus, the mean of the random variable $L(W)$ is $0$.\n\nNext, we compute the variance of $L(W)$, denoted by $\\sigma_L^2$. The variance is defined as $\\operatorname{Var}(L(W)) = \\mathbb{E}[(L(W) - \\mu_L)^2]$. Since we found that $\\mu_L = 0$, the variance simplifies to the second moment:\n$$\n\\sigma_L^2 = \\mathbb{E}[L(W)^2]\n$$\nWe substitute the definition of $L(W)$:\n$$\n\\sigma_L^2 = \\mathbb{E}\\left[ \\left(\\sum_{i=1}^{n} c_i W_{t_i}\\right)^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{i=1}^{n} c_i W_{t_i}\\right) \\left(\\sum_{j=1}^{n} c_j W_{t_j}\\right) \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j W_{t_i} W_{t_j} \\right]\n$$\nBy the linearity of expectation, we can move the expectation operator inside the summations:\n$$\n\\sigma_L^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\mathbb{E}[W_{t_i} W_{t_j}]\n$$\nThe term $\\mathbb{E}[W_{t_i} W_{t_j}]$ is related to the covariance. The covariance between two random variables $X$ and $Y$ is $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. For Brownian motion, $\\mathbb{E}[W_{t_i}]=0$ and $\\mathbb{E}[W_{t_j}]=0$, so $\\mathbb{E}[W_{t_i} W_{t_j}] = \\operatorname{Cov}(W_{t_i}, W_{t_j})$. The problem states that $\\operatorname{Cov}(W_s, W_t) = \\min\\{s,t\\}$. Therefore, $\\mathbb{E}[W_{t_i} W_{t_j}] = \\min\\{t_i, t_j\\}$.\nSubstituting this result into the expression for the variance yields:\n$$\n\\sigma_L^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\n$$\nThis expression is a quadratic form $\\mathbf{c}^T \\Sigma \\mathbf{c}$, where $\\mathbf{c} = (c_1, \\dots, c_n)^T$ and $\\Sigma$ is the covariance matrix with entries $\\Sigma_{ij} = \\min\\{t_i, t_j\\}$. This matrix is known to be positive definite. Since the coefficients $c_i$ are not all zero, the vector $\\mathbf{c}$ is non-zero, which guarantees that $\\sigma_L^2 = \\mathbf{c}^T \\Sigma \\mathbf{c}  0$.\n\nNow that we have the mean $\\mu_L = 0$ and the variance $\\sigma_L^2$, we can determine the PDF of $L(W)$. We are given that $L(W)$ is a Gaussian random variable. Any Gaussian variable can be expressed as an affine transformation of a standard normal variable $Z \\sim N(0,1)$. The PDF of a standard normal variable $Z$ is $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\nLet $x$ be a realization of the random variable $L(W)$. We can write $L(W) = \\mu_L + \\sigma_L Z$. Since $\\mu_L = 0$, we have $L(W) = \\sigma_L Z$. This is the \"Gaussian transformation\" mentioned in the problem, mapping the standard normal variable $Z$ to our variable $L(W)$.\nTo find the PDF $f_L(x)$ of $L(W)$, we use the change of variables formula for probability densities. The transformation is $x(z) = \\sigma_L z$. The inverse transformation is $z(x) = x/\\sigma_L$. The formula for the PDF is:\n$$\nf_L(x) = f_Z(z(x)) \\left| \\frac{dz}{dx} \\right|\n$$\nThe derivative is $\\frac{dz}{dx} = \\frac{1}{\\sigma_L}$. Since $\\sigma_L^2  0$, $\\sigma_L$ is a positive real number, so the absolute value is $|\\frac{1}{\\sigma_L}| = \\frac{1}{\\sigma_L}$.\nSubstituting the expressions for $z(x)$ and the Jacobian into the formula:\n$$\nf_L(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{x}{\\sigma_L}\\right)^2\\right) \\cdot \\frac{1}{\\sigma_L}\n$$\nSimplifying this gives the general form of a centered Gaussian PDF:\n$$\nf_L(x) = \\frac{1}{\\sigma_L \\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma_L^2}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma_L^2}} \\exp\\left(-\\frac{x^2}{2\\sigma_L^2}\\right)\n$$\nFinally, we substitute the full expression for the variance $\\sigma_L^2$ to obtain the final analytic expression for the PDF in terms of the given parameters:\n$$\nf_L(x) = \\frac{1}{\\sqrt{2\\pi \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\\right)}} \\exp\\left( -\\frac{x^2}{2 \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\\right)} \\right)\n$$\nThis is the closed-form expression for the probability density function of $L(W)$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}}} \\exp\\left( -\\frac{x^2}{2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}} \\right)}$$", "id": "3049614"}]}