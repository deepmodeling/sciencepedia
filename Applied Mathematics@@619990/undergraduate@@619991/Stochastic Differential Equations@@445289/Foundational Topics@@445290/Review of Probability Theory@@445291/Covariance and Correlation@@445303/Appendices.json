{"hands_on_practices": [{"introduction": "This first exercise provides a foundational workout in calculating covariance directly from a joint probability mass function. By working through a concrete scenario involving a binary classifier, you will practice computing marginal probabilities and expected values to apply the core covariance formula, $\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$ [@problem_id:1614699]. Mastering this fundamental calculation is the first step toward understanding the relationships between random variables.", "problem": "An engineer is analyzing the performance of a simple binary classification model for detecting faulty components on an assembly line. Let $X$ be a discrete random variable representing the true state of a component, where $X=1$ if the component is faulty and $X=0$ if it is not. Let $Y$ be a discrete random variable representing the model's prediction, where $Y=1$ if the model predicts the component is faulty and $Y=0$ if it predicts it is not.\n\nAfter observing a large number of components, the engineer has determined the joint probability mass function, $P(X=x, Y=y)$, for these two variables. The probabilities are as follows:\n\n-   $P(X=0, Y=0) = 0.65$\n-   $P(X=0, Y=1) = 0.05$\n-   $P(X=1, Y=0) = 0.10$\n-   $P(X=1, Y=1) = 0.20$\n\nCalculate the covariance, $\\text{Cov}(X, Y)$, between the true state of a component and the model's prediction.", "solution": "We use the definition of covariance for discrete random variables:\n$$\n\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y].\n$$\nFrom the given joint pmf, convert the decimals to exact fractions:\n$$\nP(X=0,Y=0)=0.65=\\frac{13}{20},\\quad P(X=0,Y=1)=0.05=\\frac{1}{20},\\quad P(X=1,Y=0)=0.10=\\frac{1}{10},\\quad P(X=1,Y=1)=0.20=\\frac{1}{5}.\n$$\n\nFirst compute the marginal distributions. For $X$:\n$$\nP(X=1)=P(1,0)+P(1,1)=\\frac{1}{10}+\\frac{1}{5}=\\frac{3}{10},\\quad P(X=0)=1-P(X=1)=1-\\frac{3}{10}=\\frac{7}{10}.\n$$\nFor $Y$:\n$$\nP(Y=1)=P(0,1)+P(1,1)=\\frac{1}{20}+\\frac{1}{5}=\\frac{1}{4},\\quad P(Y=0)=1-P(Y=1)=1-\\frac{1}{4}=\\frac{3}{4}.\n$$\n\nNow compute the expectations. Since $X,Y\\in\\{0,1\\}$, $\\mathbb{E}[X]=P(X=1)$ and $\\mathbb{E}[Y]=P(Y=1)$:\n$$\n\\mathbb{E}[X]=\\frac{3}{10},\\qquad \\mathbb{E}[Y]=\\frac{1}{4}.\n$$\nCompute $\\mathbb{E}[XY]$ via the joint pmf:\n$$\n\\mathbb{E}[XY]=\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}}xy\\,P(X=x,Y=y)=1\\cdot 1\\cdot P(1,1)=\\frac{1}{5}.\n$$\n\nTherefore,\n$$\n\\text{Cov}(X,Y)=\\frac{1}{5}-\\left(\\frac{3}{10}\\right)\\left(\\frac{1}{4}\\right)=\\frac{1}{5}-\\frac{3}{40}=\\frac{8}{40}-\\frac{3}{40}=\\frac{5}{40}=\\frac{1}{8}.\n$$", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "1614699"}, {"introduction": "We now transition from discrete to continuous random variables, where summations are replaced by integrals. This practice explores the covariance between a voltage fluctuation $X$ and its resulting power $Y = X^2$, a common non-linear relationship in physical systems [@problem_id:1614684]. The exercise demonstrates how to compute the necessary moments for a continuous distribution and reveals how functionally dependent variables can still have a non-trivial, calculable covariance.", "problem": "In a simplified model of a noisy electronic component, the voltage fluctuation is represented by a continuous random variable $X$. This voltage is uniformly distributed over the interval $[0, V_0]$, where $V_0$ is a positive constant representing the maximum possible voltage. The power dissipated by the component is related to the square of the voltage, and we model it with another random variable $Y = X^2$.\n\nDetermine the covariance, $\\operatorname{Cov}(X, Y)$, between the voltage $X$ and the power $Y$. Express your answer in terms of $V_0$.", "solution": "Let $X$ be uniformly distributed on $[0, V_{0}]$, so its probability density function is $f_{X}(x)=\\frac{1}{V_{0}}$ for $0 \\leq x \\leq V_{0}$. Define $Y=X^{2}$. The covariance is\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y].\n$$\nSince $Y=X^{2}$, we have $XY=X^{3}$ and $Y=X^{2}$, hence\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}[X^{3}]-\\mathbb{E}[X]\\mathbb{E}[X^{2}].\n$$\nCompute the moments using the definition of expectation for a continuous random variable:\n$$\n\\mathbb{E}[X^{n}]=\\int_{0}^{V_{0}} x^{n}\\,\\frac{1}{V_{0}}\\,dx=\\frac{1}{V_{0}}\\cdot\\frac{V_{0}^{n+1}}{n+1}=\\frac{V_{0}^{n}}{n+1}.\n$$\nThus,\n$$\n\\mathbb{E}[X]=\\frac{V_{0}}{2},\\quad \\mathbb{E}[X^{2}]=\\frac{V_{0}^{2}}{3},\\quad \\mathbb{E}[X^{3}]=\\frac{V_{0}^{3}}{4}.\n$$\nSubstituting into the covariance expression gives\n$$\n\\operatorname{Cov}(X,Y)=\\frac{V_{0}^{3}}{4}-\\left(\\frac{V_{0}}{2}\\right)\\left(\\frac{V_{0}^{2}}{3}\\right)=\\frac{V_{0}^{3}}{4}-\\frac{V_{0}^{3}}{6}=\\frac{V_{0}^{3}}{12}.\n$$", "answer": "$$\\boxed{\\frac{V_{0}^{3}}{12}}$$", "id": "1614684"}, {"introduction": "This final practice takes a significant leap from the static covariance of random variables to the dynamic *quadratic covariation* of stochastic processes, a central concept in It√¥ calculus. You will derive the quadratic covariation for the components of a correlated Brownian motion, starting from its fundamental definition [@problem_id:3046943]. This derivation is not just a calculation; it provides a crucial insight into how the constant covariance matrix $\\Sigma$ governs the moment-to-moment co-movements of the stochastic process itself.", "problem": "Let $\\{B^{1}_{t},\\dots,B^{d}_{t}\\}_{t\\ge 0}$ be a $d$-dimensional standard Brownian motion with independent components, meaning each $B^{k}_{t}$ is a one-dimensional Brownian motion and for $k\\neq \\ell$ the processes $B^{k}$ and $B^{\\ell}$ are independent. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be a fixed symmetric positive semidefinite matrix. Consider the correlated $d$-dimensional Brownian motion $\\{W_{t}\\}_{t\\ge 0}$ defined by $W_{t}=C\\,B_{t}$, where $B_{t}=(B^{1}_{t},\\dots,B^{d}_{t})^{\\top}$ and $C\\in\\mathbb{R}^{d\\times d}$ is a deterministic matrix satisfying $C\\,C^{\\top}=\\Sigma$. In particular, the $i$-th component is $W^{i}_{t}=\\sum_{k=1}^{d}C_{ik}\\,B^{k}_{t}$.\n\nStarting from the definition of quadratic covariation for continuous semimartingales,\n$$[X,Y]_{t}=\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m}(X_{t_{m+1}}-X_{t_{m}})(Y_{t_{m+1}}-Y_{t_{m}}),$$\nwhere the limit is in probability over partitions $\\mathcal{P}:\\,0=t_{0}<t_{1}<\\dots<t_{n}=t$ with mesh $\\|\\mathcal{P}\\|=\\max_{m}(t_{m+1}-t_{m})$, and using only well-tested properties of standard Brownian motion and linear operations, derive an explicit expression for the quadratic covariation $[W^{i},W^{j}]_{t}$ in terms of $\\Sigma$ and $t$. Express your final answer as a closed-form analytic expression. No numerical approximation or rounding is required.", "solution": "We begin with the construction $W_{t}=C\\,B_{t}$, where $B_{t}=(B^{1}_{t},\\dots,B^{d}_{t})^{\\top}$ is a standard $d$-dimensional Brownian motion with independent components and $C\\in\\mathbb{R}^{d\\times d}$ is deterministic with $C\\,C^{\\top}=\\Sigma$. The $i$-th component is\n$$\nW^{i}_{t}=\\sum_{k=1}^{d}C_{ik}\\,B^{k}_{t}.\n$$\nThe quadratic covariation of two continuous semimartingales $X$ and $Y$ up to time $t$ is defined by\n$$\n[X,Y]_{t}=\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m=0}^{n-1}\\big(X_{t_{m+1}}-X_{t_{m}}\\big)\\big(Y_{t_{m+1}}-Y_{t_{m}}\\big),\n$$\nwhere the limit is taken in probability as the mesh of the partition $\\mathcal{P}$ goes to zero. For Brownian motions, a well-tested property is that for standard independent components $B^{k}$ and $B^{\\ell}$,\n$$\n[B^{k},B^{\\ell}]_{t}=\\begin{cases}\nt, & k=\\ell,\\\\\n0, & k\\neq \\ell.\n\\end{cases}\n$$\nEquivalently, $[B^{k},B^{\\ell}]_{t}=\\delta_{k\\ell}\\,t$, where $\\delta_{k\\ell}$ is the Kronecker delta.\n\nWe compute $[W^{i},W^{j}]_{t}$ using the definition and linearity. For a partition $\\mathcal{P}:\\,0=t_{0}<\\dots<t_{n}=t$, set $\\Delta B^{k}_{m}=B^{k}_{t_{m+1}}-B^{k}_{t_{m}}$ and $\\Delta W^{i}_{m}=W^{i}_{t_{m+1}}-W^{i}_{t_{m}}$. Then\n$$\n\\Delta W^{i}_{m}=\\sum_{k=1}^{d}C_{ik}\\,\\Delta B^{k}_{m},\\qquad \\Delta W^{j}_{m}=\\sum_{\\ell=1}^{d}C_{j\\ell}\\,\\Delta B^{\\ell}_{m}.\n$$\nHence\n\\begin{align*}\n\\sum_{m=0}^{n-1}\\Delta W^{i}_{m}\\,\\Delta W^{j}_{m}\n&=\\sum_{m=0}^{n-1}\\left(\\sum_{k=1}^{d}C_{ik}\\,\\Delta B^{k}_{m}\\right)\\left(\\sum_{\\ell=1}^{d}C_{j\\ell}\\,\\Delta B^{\\ell}_{m}\\right)\\\\\n&=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\left(\\sum_{m=0}^{n-1}\\Delta B^{k}_{m}\\,\\Delta B^{\\ell}_{m}\\right).\n\\end{align*}\nTaking the limit in probability as $\\|\\mathcal{P}\\|\\to 0$, we use the well-tested property of standard Brownian motion increments to identify\n$$\n\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m=0}^{n-1}\\Delta B^{k}_{m}\\,\\Delta B^{\\ell}_{m}=[B^{k},B^{\\ell}]_{t}=\\delta_{k\\ell}\\,t.\n$$\nTherefore,\n\\begin{align*}\n[W^{i},W^{j}]_{t}\n&=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\,[B^{k},B^{\\ell}]_{t}\\\\\n&=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\,\\delta_{k\\ell}\\,t\\\\\n&=\\sum_{k=1}^{d}C_{ik}C_{jk}\\,t.\n\\end{align*}\nRecognizing the matrix product, the $(i,j)$ entry of $C\\,C^{\\top}$ is $(C\\,C^{\\top})_{ij}=\\sum_{k=1}^{d}C_{ik}C_{jk}$. By the assumption $C\\,C^{\\top}=\\Sigma$, we conclude\n$$\n[W^{i},W^{j}]_{t}=(C\\,C^{\\top})_{ij}\\,t=\\Sigma_{ij}\\,t.\n$$\nThis provides the desired closed-form expression for the quadratic covariation in terms of the covariance matrix $\\Sigma$ and time $t$.", "answer": "$$\\boxed{\\Sigma_{ij}\\,t}$$", "id": "3046943"}]}