{"hands_on_practices": [{"introduction": "Before applying the normal distribution to complex models, we must deeply understand its fundamental properties. This foundational exercise guides you through calculating the first four moments of a general normal random variable, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, using the power of standardization. By deriving these moments from the simpler standard normal case, you will see precisely why the normal distribution has zero skewness and a constant kurtosis of $3$, key features that define its symmetric, bell-shaped curve [@problem_id:3068835].", "problem": "Consider the stochastic differential equation (SDE) $dX_{t} = a\\,dt + b\\,dW_{t}$, where $W_{t}$ is a standard Wiener process and $a$, $b$ are constants. Over a fixed time step $\\Delta t$, the Eulerâ€“Maruyama increment $\\Delta X := X_{t+\\Delta t} - X_{t}$ is normally distributed with mean $\\mu := a\\,\\Delta t$ and variance $\\sigma^{2} := b^{2}\\,\\Delta t$. Thus, we may write $\\Delta X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Let $X$ denote a generic normal random variable with $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Use standardization to compute the first four raw moments $\\mathbb{E}[X^{k}]$ for $k \\in \\{1,2,3,4\\}$ starting from the definitions of moments and well-tested facts about the standard normal distribution. Then, using the definitions of standardized skewness and kurtosis,\n- skewness $\\gamma_{1} := \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{3}\\right]$,\n- kurtosis $\\kappa := \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{4}\\right]$,\nuse your computed moments to illustrate the zero skewness and the specific kurtosis of the normal distribution.\n\nReport your final results as the single row matrix $(\\mathbb{E}[X],\\ \\mathbb{E}[X^{2}],\\ \\mathbb{E}[X^{3}],\\ \\mathbb{E}[X^{4}],\\ \\gamma_{1},\\ \\kappa)$ in that order. No numerical rounding is required, and no physical units are involved.", "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. We may therefore proceed with the derivation.\n\nThe problem requires the calculation of the first four raw moments, the skewness, and the kurtosis for a random variable $X$ with a normal distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The core technique is standardization, which relates $X$ to a standard normal random variable $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe relationship is given by the transformation:\n$$Z = \\frac{X - \\mu}{\\sigma}$$\nFrom this, we can express $X$ in terms of $Z$:\n$$X = \\mu + \\sigma Z$$\nTo compute the raw moments of $X$, $\\mathbb{E}[X^k]$, we will use the above expression for $X$ and the linearity of the expectation operator. This requires the moments of the standard normal variable $Z$. As per the problem's allowance to use \"well-tested facts about the standard normal distribution,\" we will use the following known moments of $Z$:\nThe odd moments of $Z$ are all zero due to the symmetry of the distribution around $0$.\n$$\\mathbb{E}[Z^n] = 0 \\quad \\text{for odd } n$$\nSpecifically, $\\mathbb{E}[Z^1] = \\mathbb{E}[Z] = 0$ and $\\mathbb{E}[Z^3] = 0$.\n\nThe even moments are given by the double factorial $(n-1)!! = (n-1)(n-3)\\cdots1$.\n$$\\mathbb{E}[Z^2] = (2-1)!! = 1$$\nThis is consistent with the fact that $Z$ has a variance of $1$, since $\\text{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2 = 1 - 0^2 = 1$.\n$$\\mathbb{E}[Z^4] = (4-1)!! = 3 \\cdot 1 = 3$$\n\nNow, we compute the first four raw moments of $X$.\n\nFirst raw moment ($k=1$):\n$$\\mathbb{E}[X] = \\mathbb{E}[\\mu + \\sigma Z] = \\mathbb{E}[\\mu] + \\sigma\\mathbb{E}[Z] = \\mu + \\sigma(0) = \\mu$$\n\nSecond raw moment ($k=2$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^2$.\n$$\\mathbb{E}[X^2] = \\mathbb{E}[(\\mu + \\sigma Z)^2] = \\mathbb{E}[\\mu^2 + 2\\mu\\sigma Z + \\sigma^2 Z^2]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^2] = \\mathbb{E}[\\mu^2] + 2\\mu\\sigma\\mathbb{E}[Z] + \\sigma^2\\mathbb{E}[Z^2] = \\mu^2 + 2\\mu\\sigma(0) + \\sigma^2(1) = \\mu^2 + \\sigma^2$$\n\nThird raw moment ($k=3$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^3$.\n$$\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^3] = \\mathbb{E}[\\mu^3] + 3\\mu^2\\sigma\\mathbb{E}[Z] + 3\\mu\\sigma^2\\mathbb{E}[Z^2] + \\sigma^3\\mathbb{E}[Z^3]$$\n$$\\mathbb{E}[X^3] = \\mu^3 + 3\\mu^2\\sigma(0) + 3\\mu\\sigma^2(1) + \\sigma^3(0) = \\mu^3 + 3\\mu\\sigma^2$$\n\nFourth raw moment ($k=4$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^4$.\n$$\\mathbb{E}[X^4] = \\mathbb{E}[(\\mu + \\sigma Z)^4] = \\mathbb{E}[\\mu^4 + 4\\mu^3\\sigma Z + 6\\mu^2\\sigma^2 Z^2 + 4\\mu\\sigma^3 Z^3 + \\sigma^4 Z^4]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^4] = \\mathbb{E}[\\mu^4] + 4\\mu^3\\sigma\\mathbb{E}[Z] + 6\\mu^2\\sigma^2\\mathbb{E}[Z^2] + 4\\mu\\sigma^3\\mathbb{E}[Z^3] + \\sigma^4\\mathbb{E}[Z^4]$$\n$$\\mathbb{E}[X^4] = \\mu^4 + 4\\mu^3\\sigma(0) + 6\\mu^2\\sigma^2(1) + 4\\mu\\sigma^3(0) + \\sigma^4(3) = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$$\n\nNext, we compute the skewness and kurtosis as defined in the problem.\n\nSkewness ($\\gamma_1$):\nThe skewness is defined as the third standardized moment.\n$$\\gamma_1 := \\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right]$$\nRecognizing that $\\frac{X-\\mu}{\\sigma} = Z$, this is simply the third moment of the standard normal distribution.\n$$\\gamma_1 = \\mathbb{E}[Z^3] = 0$$\nThe zero skewness reflects the symmetry of the normal distribution about its mean.\n\nKurtosis ($\\kappa$):\nThe kurtosis is defined as the fourth standardized moment.\n$$\\kappa := \\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right]$$\nSimilarly, this is the fourth moment of the standard normal distribution.\n$$\\kappa = \\mathbb{E}[Z^4] = 3$$\nThis value is a defining characteristic of the mesokurtic nature of the normal distribution.\n\nThe problem asks to illustrate the zero skewness and specific kurtosis using the computed moments. The definitions provided for $\\gamma_1$ and $\\kappa$ are directly in terms of the standardized variable $Z$, whose moments we have used throughout. The calculation of these values from the moments of $Z$ serves as the required illustration.\n\nFinally, we assemble the results into the specified single row matrix $(\\mathbb{E}[X], \\mathbb{E}[X^2], \\mathbb{E}[X^3], \\mathbb{E}[X^4], \\gamma_1, \\kappa)$.\nThe elements are:\n$\\mathbb{E}[X] = \\mu$\n$\\mathbb{E}[X^2] = \\mu^2 + \\sigma^2$\n$\\mathbb{E}[X^3] = \\mu^3 + 3\\mu\\sigma^2$\n$\\mathbb{E}[X^4] = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$\n$\\gamma_1 = 0$\n$\\kappa = 3$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\mu & \\mu^2 + \\sigma^2 & \\mu^3 + 3\\mu\\sigma^2 & \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4 & 0 & 3 \\end{pmatrix} } $$", "id": "3068835"}, {"introduction": "The ability to simulate stochastic processes is a cornerstone of modern quantitative finance and science. This practice directly bridges theory and application, challenging you to implement the fundamental step in simulating any SDE: generating the random increments of a Wiener process. You will use the scaling property of the normal distribution, a direct consequence of standardization, to generate these increments and then empirically verify that your simulation's statistics converge to their theoretical values as the sample size grows [@problem_id:3068854].", "problem": "Consider a Wiener process (also called Brownian motion) $\\{B_t\\}_{t \\geq 0}$, which by definition has independent increments and satisfies that for any fixed time step $\\Delta t > 0$, the increment $\\Delta B := B_{t+\\Delta t} - B_t$ is normally distributed with mean $0$ and variance $\\Delta t$. In the numerical simulation of a stochastic differential equation (SDE), such as $dX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dB_t$, one needs to approximate increments $\\Delta B$ over small time steps $\\Delta t$. A standard approach uses the scaling property of the normal distribution: if $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable, then for any $\\sigma > 0$, the scaled variable $\\sigma Z$ satisfies $\\sigma Z \\sim \\mathcal{N}(0,\\sigma^2)$. Therefore, the increment can be simulated by the transformation $\\Delta B = \\sqrt{\\Delta t}\\,Z$.\n\nTask: Starting from these foundational definitions and facts, you must do two things:\n- Derive, from first principles of the normal distribution and the definition of Brownian motion increments, why $\\Delta B = \\sqrt{\\Delta t}\\,Z$ has distribution $\\mathcal{N}(0,\\Delta t)$ when $Z \\sim \\mathcal{N}(0,1)$.\n- Empirically verify the convergence of moment estimates by simulating independent samples $\\{\\Delta B_i\\}_{i=1}^N$ using $\\Delta B_i = \\sqrt{\\Delta t}\\,Z_i$ with $Z_i \\sim \\mathcal{N}(0,1)$, and computing the sample mean $\\hat{m} = \\frac{1}{N}\\sum_{i=1}^N \\Delta B_i$ and the unbiased sample variance $\\hat{s}^2 = \\frac{1}{N-1}\\sum_{i=1}^N (\\Delta B_i - \\hat{m})^2$. Use probabilistic reasoning to establish the expected accuracy of these estimators, and then implement a test that decides whether the empirical estimates are consistent with the theoretical targets $0$ for the mean and $\\Delta t$ for the variance at a quantified level.\n\nAcceptance criterion for each test case: Let $\\hat{m}$ and $\\hat{s}^2$ be the empirical estimators. The theoretical variance of the sample mean for independent draws with variance $\\Delta t$ is $\\mathrm{Var}(\\hat{m}) = \\Delta t / N$, so the natural scale for fluctuations of $\\hat{m}$ is $\\sqrt{\\Delta t / N}$. Moreover, for normal data, the unbiased sample variance satisfies $(N-1)\\hat{s}^2 / \\Delta t \\sim \\chi^2_{N-1}$, which implies $\\mathbb{E}[\\hat{s}^2] = \\Delta t$ and $\\mathrm{Var}(\\hat{s}^2) = \\frac{2\\,\\Delta t^2}{N-1}$, so the natural scale for fluctuations of $\\hat{s}^2 - \\Delta t$ is $\\sqrt{\\frac{2\\,\\Delta t^2}{N-1}}$. For each test case, declare it a pass if both inequalities hold:\n$$|\\hat{m} - 0| \\leq c \\sqrt{\\frac{\\Delta t}{N}}, \\quad |\\hat{s}^2 - \\Delta t| \\leq c \\sqrt{\\frac{2\\,\\Delta t^2}{N-1}},$$\nwhere $c$ is a fixed constant multiplier (use $c = 3$). This quantifies the notion that empirical estimates lie within a few standard errors of their targets.\n\nYour program must:\n- Simulate the specified test cases using a reproducible random number generator.\n- For each case, compute $\\hat{m}$ and $\\hat{s}^2$ and check the acceptance criterion above with $c = 3$.\n- Produce a single line of output containing a list of boolean values indicating pass or fail for each test case, formatted as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$: $(\\Delta t, N) = (0, 1000)$, which probes the boundary case where the increment has zero variance and is deterministically $0$.\n- Case $2$: $(\\Delta t, N) = (0.1, 5000)$, a small time step with moderate sample size.\n- Case $3$: $(\\Delta t, N) = (1.0, 100000)$, a unit time step with large sample size.\n- Case $4$: $(\\Delta t, N) = (10^{-4}, 200000)$, a very small time step testing numerical precision.\n- Case $5$: $(\\Delta t, N) = (2.5, 50000)$, a larger time step with moderate sample size.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$ corresponding to the pass/fail outcome of the $i$-th test case.", "solution": "The problem is valid as it is scientifically grounded in the principles of stochastic calculus and statistics, is well-posed with a clear objective, and provides all necessary information for a unique solution. It contains no contradictions, ambiguities, or factual errors.\n\nThe solution consists of two parts as requested: a theoretical derivation and an empirical verification.\n\n### Part 1: Theoretical Derivation\n\nThe derivation proceeds from the fundamental properties of expectation and variance for linear transformations of random variables. Let $Z$ be a random variable with mean $\\mathbb{E}[Z]$ and variance $\\mathrm{Var}(Z)$. For constants $a$ and $b$, the transformed variable $Y = aZ + b$ has mean $\\mathbb{E}[Y] = a\\mathbb{E}[Z] + b$ and variance $\\mathrm{Var}(Y) = a^2\\mathrm{Var}(Z)$. A critical property of the normal distribution is its closure under affine transformations: if $Z$ is normally distributed, so is $Y$.\n\nWe are given a standard normal random variable $Z \\sim \\mathcal{N}(0, 1)$, for which its expectation is $\\mathbb{E}[Z] = 0$ and its variance is $\\mathrm{Var}(Z) = 1$. The Brownian increment $\\Delta B$ is constructed as the random variable $\\Delta B = \\sqrt{\\Delta t}\\,Z$. This is a linear transformation with a scaling factor $a = \\sqrt{\\Delta t}$ and a shift $b = 0$.\n\nFirst, we compute the expectation of $\\Delta B$:\n$$\n\\mathbb{E}[\\Delta B] = \\mathbb{E}[\\sqrt{\\Delta t}\\,Z] = \\sqrt{\\Delta t}\\,\\mathbb{E}[Z]\n$$\nSubstituting the known value $\\mathbb{E}[Z] = 0$:\n$$\n\\mathbb{E}[\\Delta B] = \\sqrt{\\Delta t} \\cdot 0 = 0\n$$\nThis establishes that the mean of the increment is $0$.\n\nSecond, we compute the variance of $\\Delta B$:\n$$\n\\mathrm{Var}(\\Delta B) = \\mathrm{Var}(\\sqrt{\\Delta t}\\,Z) = (\\sqrt{\\Delta t})^2\\,\\mathrm{Var}(Z)\n$$\nSubstituting the known value $\\mathrm{Var}(Z) = 1$:\n$$\n\\mathrm{Var}(\\Delta B) = \\Delta t \\cdot 1 = \\Delta t\n$$\nThis establishes that the variance of the increment is $\\Delta t$.\n\nGiven that $Z$ is a normally distributed random variable, the linearly transformed variable $\\Delta B$ must also be normally distributed. Combining these findingsâ€”a normal distribution with a mean of $0$ and a variance of $\\Delta t$â€”we conclude that $\\Delta B$ follows the distribution $\\mathcal{N}(0, \\Delta t)$. This completes the derivation from first principles, confirming the scaling property used to simulate Wiener process increments.\n\n### Part 2: Empirical Verification\n\nFor the empirical verification, a numerical simulation is implemented according to the problem specification. The core of the implementation processes a suite of test cases, each defined by a pair of parameters $(\\Delta t, N)$, representing the time step size and the number of samples, respectively.\n\nThe procedure for each test case is as follows:\n$1$. A reproducible random number generator is used to generate $N$ independent samples, $\\{Z_i\\}_{i=1}^N$, from the standard normal distribution $\\mathcal{N}(0, 1)$.\n$2$. These standard samples are scaled to produce increments of a Wiener process, $\\{\\Delta B_i\\}_{i=1}^N$, using the theoretically-derived relationship $\\Delta B_i = \\sqrt{\\Delta t}\\,Z_i$.\n$3$. The sample mean, $\\hat{m}$, and the unbiased sample variance, $\\hat{s}^2$, of the generated increments are computed using the standard estimators:\n$$ \\hat{m} = \\frac{1}{N}\\sum_{i=1}^N \\Delta B_i $$\n$$ \\hat{s}^2 = \\frac{1}{N-1}\\sum_{i=1}^N (\\Delta B_i - \\hat{m})^2 $$\n$4$. These empirical statistics are then compared against their theoretical expected values, which are $\\mathbb{E}[\\hat{m}] = 0$ for the mean and $\\mathbb{E}[\\hat{s}^2] = \\Delta t$ for the variance.\n$5$. The acceptance criterion quantifies whether the empirical estimates are statistically consistent with the theoretical targets. A test case is considered a pass if the estimates lie within a specified number of standard errors of their expected values. The problem specifies a multiplier of $c=3$.\n$6$. The first condition checks the sample mean. The theoretical variance of the sample mean is $\\mathrm{Var}(\\hat{m}) = \\frac{\\Delta t}{N}$. The condition is:\n$$ |\\hat{m} - 0| \\leq c \\sqrt{\\frac{\\Delta t}{N}} $$\n$7$. The second condition checks the sample variance. For samples from a normal distribution, the variance of the unbiased sample variance estimator is $\\mathrm{Var}(\\hat{s}^2) = \\frac{2\\Delta t^2}{N-1}$. The condition is:\n$$ |\\hat{s}^2 - \\Delta t| \\leq c \\sqrt{\\frac{2\\Delta t^2}{N-1}} $$\n$8$. A test case is marked as `True` if both conditions are met, and `False` otherwise. For the boundary case where $\\Delta t = 0$, the increments $\\Delta B_i$ are all identically $0$, leading to $\\hat{m}=0$ and $\\hat{s}^2=0$. The acceptance thresholds also evaluate to $0$, so both conditions $|0| \\leq 0$ are satisfied, and the case passes. The implementation correctly handles this limiting case without special conditional logic, as the numerical computations correctly yield the zero values throughout.\n\nThe final output is a list of boolean values, one for each test case, indicating the outcome of the verification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates Brownian motion increments and verifies their statistical properties.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (delta_t, N_samples)\n        (0.0, 1000),\n        (0.1, 5000),\n        (1.0, 100000),\n        (1e-4, 200000),\n        (2.5, 50000),\n    ]\n\n    # Acceptance criterion multiplier\n    c = 3.0\n\n    # Initialize a reproducible random number generator for consistent results.\n    # The seed value is arbitrary but fixed.\n    rng = np.random.default_rng(42)\n\n    results = []\n    for dt, N in test_cases:\n        # The case N <= 1 is not in the test suite. If it were, the denominator\n        # N-1 in the variance of the sample variance would be problematic.\n        # We assume N > 1 as per the test suite.\n        \n        # 1. Generate N standard normal random variables Z ~ N(0,1).\n        Z = rng.standard_normal(size=N)\n        \n        # 2. Scale Z to obtain Brownian increments dB ~ N(0, dt).\n        dB = np.sqrt(dt) * Z\n        \n        # 3. Compute the sample mean and unbiased sample variance.\n        m_hat = np.mean(dB)\n        # Using ddof=1 ensures the denominator is N-1 for an unbiased estimate.\n        s2_hat = np.var(dB, ddof=1)\n        \n        # 4. Calculate the acceptance thresholds.\n        # The formulas are robust and handle the dt=0 case correctly, where\n        # thresholds become 0.\n        mean_threshold = c * np.sqrt(dt / N)\n        var_threshold = c * np.sqrt(2 * (dt**2) / (N - 1))\n        \n        # 5. Check if the empirical estimates are within the thresholds.\n        mean_check = np.abs(m_hat - 0) <= mean_threshold\n        var_check = np.abs(s2_hat - dt) <= var_threshold\n        \n        # A test case passes if and only if both conditions are met.\n        results.append(mean_check and var_check)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3068854"}, {"introduction": "A simulation is only as good as its underlying random number generator. How can we be confident that our simulated process truly behaves like a theoretical Brownian motion? This advanced practice challenges you to act as a quantitative analyst by building a statistical validation suite to answer that very question. You will use standardization to transform simulated data and then apply formal statistical tests to check for the defining properties of normality and independence [@problem_id:3068822].", "problem": "You are asked to implement and apply a statistical validation of a one-dimensional Brownian motion simulator by using standardized increments. Let $\\{W_t\\}_{t \\ge 0}$ denote a Brownian motion, defined by the core properties from stochastic differential equations: for any partition $0 = t_0 < t_1 < \\dots < t_n$, the increments $\\Delta W_k := W_{t_k} - W_{t_{k-1}}$ are independent and normally distributed with mean $0$ and variance $t_k - t_{k-1}$. In particular, for an equidistant grid with step $\\Delta t > 0$, each increment satisfies $\\Delta W_k \\sim \\mathcal{N}(0,\\Delta t)$ and the standardized increment $Z_k := \\Delta W_k / \\sqrt{\\Delta t}$ satisfies $Z_k \\sim \\mathcal{N}(0,1)$ and is independent of $Z_j$ for $j \\ne k$.\n\nYour task is to design a programmatic test that, given simulated increments for a time-discretized process, uses the standardized increments to validate two fundamental properties required of Brownian motion:\n- Normality of the standardized increments, assessed by a Kolmogorovâ€“Smirnov test against the standard normal distribution.\n- Independence of the standardized increments, assessed by approximate significance tests of sample autocorrelations at multiple lags.\n\nStart from the following foundational base:\n- The increment property for Brownian motion: for equidistant times $t_k = k \\Delta t$, the increments satisfy $\\Delta W_k \\sim \\mathcal{N}(0,\\Delta t)$ and are independent.\n- Standardization principle: if $X \\sim \\mathcal{N}(0,\\sigma^2)$ then $X/\\sigma \\sim \\mathcal{N}(0,1)$.\n- The Kolmogorovâ€“Smirnov test statistic for a sample $z_1,\\dots,z_n$ compared to the standard normal distribution yields a $p$-value under the null hypothesis $\\mathcal{N}(0,1)$.\n- For a large sample size $n$, if $\\{z_k\\}$ is an independent sequence with zero mean and finite variance, then the sample autocorrelation at lag $\\ell$, denoted $r_\\ell$, is approximately distributed as $\\mathcal{N}(0,1/n)$ under the null hypothesis of independence. Equivalently, $\\sqrt{n}\\, r_\\ell$ is approximately $\\mathcal{N}(0,1)$, enabling two-sided $p$-values.\n\nYou must implement the following validation pipeline:\n- Given simulated increments $\\Delta W_k$ on an equidistant grid with step $\\Delta t$, compute standardized increments $Z_k := \\Delta W_k/\\sqrt{\\Delta t}$.\n- Perform a Kolmogorovâ€“Smirnov test of $\\{Z_k\\}$ against $\\mathcal{N}(0,1)$ with significance level $\\alpha_{\\text{norm}}$. Declare the normality test as passed if the Kolmogorovâ€“Smirnov $p$-value satisfies $p_{\\text{KS}} \\ge \\alpha_{\\text{norm}}$.\n- For a given finite set of lags $\\mathcal{L} = \\{\\ell_1,\\dots,\\ell_m\\}$, compute the sample autocorrelation $r_{\\ell}$ for each $\\ell \\in \\mathcal{L}$ using the standard centered estimator\n$$\nr_{\\ell} = \\frac{\\sum_{k=1}^{n-\\ell} \\left(Z_k - \\bar{Z}\\right)\\left(Z_{k+\\ell} - \\bar{Z}\\right)}{\\sum_{k=1}^{n} \\left(Z_k - \\bar{Z}\\right)^2},\n$$\nwhere $\\bar{Z}$ is the sample mean of $\\{Z_k\\}_{k=1}^n$. Under the independence null, approximate the two-sided $p$-value by treating $\\sqrt{n}\\, r_{\\ell}$ as standard normal. Use a Bonferroni correction across lags to control the family-wise error rate at $\\alpha_{\\text{indep}}$, that is, require $p_{\\ell} \\ge \\alpha_{\\text{indep}}/m$ for all $\\ell \\in \\mathcal{L}$ to declare the independence test as passed. Here $m = |\\mathcal{L}|$.\n- Declare an overall pass if and only if both the normality and independence tests pass.\n\nImplement the above pipeline and apply it to the following test suite, where each case specifies how the increments are generated. In all cases, use the provided pseudorandom seed to ensure deterministic behavior. The angle unit is not applicable, and no physical units are involved. All significance levels must be treated as decimals, not percentages.\n\nGlobal test parameters:\n- Use $\\mathcal{L} = \\{1,2,5\\}$, so $m = 3$ lags.\n- Use $\\alpha_{\\text{norm}} = 0.01$ and $\\alpha_{\\text{indep}} = 0.01$.\n\nTest suite of four cases:\n- Case $1$ (ideal Brownian simulator, \"happy path\"): simulate $n = 10000$ increments with $\\Delta t = 0.001$ and seed $123456$. Generate $\\Delta W_k = \\sqrt{\\Delta t}\\, \\xi_k$ with $\\xi_k \\sim \\mathcal{N}(0,1)$ independent and identically distributed.\n- Case $2$ (non-normal but independent increments): simulate $n = 10000$ increments with $\\Delta t = 0.001$ and seed $24681012$. Generate independent standardized increments $T_k$ from a Student $t$ distribution with $\\nu = 3$ degrees of freedom, scaled to unit variance, and set $\\Delta W_k = \\sqrt{\\Delta t}\\, T_k$. This preserves independence but violates normality. Specifically, if $U_k \\sim \\chi^2_{\\nu}$ and $G_k \\sim \\mathcal{N}(0,1)$ independent, then define $S_k = G_k/\\sqrt{U_k/\\nu}$ and $T_k = S_k/\\sqrt{\\nu/(\\nu-2)}$ for $\\nu > 2$ to achieve unit variance.\n- Case $3$ (normal but dependent increments): simulate $n = 10000$ increments with $\\Delta t = 0.001$, seed $314159$, and autoregressive correlation $\\rho = 0.3$. Generate a stationary autoregressive process of order $1$ (AR($1$)) by $Y_1 \\sim \\mathcal{N}(0,1)$ and $Y_k = \\rho Y_{k-1} + \\sqrt{1-\\rho^2}\\, \\xi_k$ with $\\xi_k \\sim \\mathcal{N}(0,1)$ independent and identically distributed. Set $\\Delta W_k = \\sqrt{\\Delta t}\\, Y_k$. This yields standard normal marginals but dependence across time.\n- Case $4$ (boundary sample size): simulate $n = 400$ increments with $\\Delta t = 0.01$ and seed $271828$ using the ideal Brownian simulator as in Case $1$.\n\nProgram requirements:\n- Implement the simulator logic for each case precisely as defined, driven by a seeded pseudorandom number generator.\n- Implement the standardization, Kolmogorovâ€“Smirnov test, and autocorrelation independence test with Bonferroni correction as specified.\n- For each case, output a single boolean indicating whether the process passes both the normality and independence tests.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each entry is the boolean result for the corresponding case in the order $1$ through $4$.\n\nYour final output must be exactly one line in the format described, with no extra text or lines. The answer does not involve any physical units, and angles are not applicable. All numerical values should be treated as pure numbers.", "solution": "The problem requires the implementation of a statistical validation pipeline for simulated increments of a one-dimensional Brownian motion. The validation is based on two fundamental properties of the standardized increments: normality and independence. The solution involves developing a programmatic test and applying it to four distinct test cases designed to probe the validator's effectiveness.\n\nThe validation pipeline proceeds in a sequence of steps for a given series of increments $\\{\\Delta W_k\\}_{k=1}^n$ corresponding to a time step $\\Delta t$.\n\n**Step 1: Standardization of Increments**\n\nThe theoretical foundation of Brownian motion states that for an equidistant time grid with step size $\\Delta t > 0$, the increments $\\Delta W_k$ are independently and identically distributed (i.i.d.) according to a normal distribution with mean $0$ and variance $\\Delta t$, denoted $\\Delta W_k \\sim \\mathcal{N}(0, \\Delta t)$.\n\nTo facilitate a standard statistical test, we first transform the increments into a standard normal scale. The standardized increment, $Z_k$, is defined as:\n$$\nZ_k = \\frac{\\Delta W_k}{\\sqrt{\\Delta t}}\n$$\nUnder the null hypothesis that the original increments are from a true Brownian motion, the standardized increments $\\{Z_k\\}$ must be i.i.d. samples from a standard normal distribution, $Z_k \\sim \\mathcal{N}(0,1)$. This standardized series $\\{Z_k\\}_{k=1}^n$ is the subject of the subsequent statistical tests.\n\n**Step 2: Normality Test**\n\nThe first property to validate is that the standardized increments follow a standard normal distribution. We employ the Kolmogorovâ€“Smirnov (KS) test for this purpose. The KS test compares the empirical cumulative distribution function (ECDF) of the sample data $\\{Z_k\\}$ to the cumulative distribution function (CDF) of the hypothesized distribution, which in this case is $\\mathcal{N}(0,1)$.\n\nThe null hypothesis ($H_0$) is that the data are drawn from the standard normal distribution. The test yields a $p$-value, $p_{\\text{KS}}$. A small $p$-value suggests that the sample distribution is significantly different from the theoretical distribution. We compare this $p$-value against a pre-defined significance level, $\\alpha_{\\text{norm}}$. The test is declared \"passed\" if the evidence against the null hypothesis is not statistically significant, i.e., if:\n$$\np_{\\text{KS}} \\ge \\alpha_{\\text{norm}}\n$$\nFor this problem, $\\alpha_{\\text{norm}} = 0.01$.\n\n**Step 3: Independence Test**\n\nThe second property to validate is the independence of the standardized increments. This is assessed by examining the sample autocorrelation function (ACF) at various time lags. If the series $\\{Z_k\\}$ is truly independent, its autocorrelation should be statistically indistinguishable from zero for all non-zero lags.\n\nFor a given lag $\\ell > 0$, the sample autocorrelation $r_{\\ell}$ is calculated using the formula:\n$$\nr_{\\ell} = \\frac{\\sum_{k=1}^{n-\\ell} (Z_k - \\bar{Z})(Z_{k+\\ell} - \\bar{Z})}{\\sum_{k=1}^{n} (Z_k - \\bar{Z})^2}\n$$\nwhere $n$ is the sample size and $\\bar{Z}$ is the sample mean of $\\{Z_k\\}$.\n\nUnder the null hypothesis of independence and for a large sample size $n$, the sample autocorrelation $r_{\\ell}$ is approximately normally distributed with mean $0$ and variance $1/n$. Consequently, the test statistic $\\sqrt{n}\\, r_{\\ell}$ is approximately standard normal:\n$$\n\\sqrt{n}\\, r_{\\ell} \\sim \\mathcal{N}(0,1)\n$$\nFrom this, we can calculate a two-sided $p$-value, $p_{\\ell}$, which represents the probability of observing an autocorrelation as extreme as $r_{\\ell}$ if the series were truly independent. $p_{\\ell} = 2 \\cdot (1 - \\Phi(|\\sqrt{n}\\, r_{\\ell}|))$, where $\\Phi$ is the standard normal CDF.\n\nSince we are testing for independence at multiple lags, specifically $\\mathcal{L} = \\{1, 2, 5\\}$, we perform three separate hypothesis tests. To control the family-wise error rate (the probability of making at least one Type I error) at a level $\\alpha_{\\text{indep}}$, we use the Bonferroni correction. The significance level for each individual test is adjusted to $\\alpha_{\\text{indep}}/m$, where $m = |\\mathcal{L}| = 3$. The independence test is declared \"passed\" only if the condition $p_{\\ell} \\ge \\alpha_{\\text{indep}}/m$ holds for all lags $\\ell \\in \\mathcal{L}$. For this problem, $\\alpha_{\\text{indep}} = 0.01$, so the corrected threshold is $0.01/3$.\n\n**Step 4: Overall Validation and Test Case Implementation**\n\nThe overall validation for a given series of increments passes if and only if both the normality test and the independence test pass.\n\nThe implementation will proceed by generating four sets of increments according to the problem specification, using a seeded pseudorandom number generator for reproducibility.\n\n- **Case 1 (Ideal):** Increments are generated as $\\Delta W_k = \\sqrt{\\Delta t}\\, \\xi_k$ with $\\xi_k \\sim \\mathcal{N}(0,1)$ i.i.d. Here, $Z_k = \\xi_k$, so both normality and independence are satisfied by construction. This case is expected to pass.\n\n- **Case 2 (Non-normal):** Increments are generated using scaled Student's $t$-distributed variates. A standard Student's $t$ variable $S_k$ with $\\nu=3$ degrees of freedom is scaled to have unit variance, $T_k = S_k / \\sqrt{\\nu/(\\nu-2)}$, before being used to define $\\Delta W_k = \\sqrt{\\Delta t}\\, T_k$. The resulting $Z_k = T_k$ are independent but not normally distributed. This case is expected to fail the normality test.\n\n- **Case 3 (Dependent):** Increments are generated from a standard normal autoregressive process of order $1$ (AR($1$)), $Y_k = \\rho Y_{k-1} + \\sqrt{1-\\rho^2}\\, \\xi_k$, with $\\rho=0.3$. The marginal distribution of each $Y_k$ is $\\mathcal{N}(0,1)$, but the terms are serially correlated. By setting $\\Delta W_k = \\sqrt{\\Delta t}\\, Y_k$, we get $Z_k = Y_k$. This series should pass the normality test but fail the independence test, particularly at lag $\\ell=1$.\n\n- **Case 4 (Boundary Sample Size):** This case uses the ideal generator but with a smaller sample size ($n=400$). The statistical tests have less power, but given the parameters, it is still expected to pass, testing the procedure's robustness with smaller datasets.\n\nThe final output is a list of four booleans, each indicating the overall pass/fail status for the corresponding test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements a statistical validation pipeline for Brownian motion simulators\n    and applies it to four test cases.\n    \"\"\"\n\n    # Global test parameters\n    LAGS = [1, 2, 5]\n    ALPHA_NORM = 0.01\n    ALPHA_INDEP = 0.01\n\n    def calculate_autocorrelation(z_series, lag):\n        \"\"\"\n        Calculates the sample autocorrelation for a given lag.\n        \"\"\"\n        n = len(z_series)\n        z_demeaned = z_series - np.mean(z_series)\n        \n        # Numerator: sum_{k=1}^{n-l} (Z_k - Z_bar)(Z_{k+l} - Z_bar)\n        # Python indices: z_demeaned[:n-lag] and z_demeaned[lag:]\n        numerator = np.sum(z_demeaned[:n-lag] * z_demeaned[lag:])\n        \n        # Denominator: sum_{k=1}^{n} (Z_k - Z_bar)^2\n        denominator = np.sum(z_demeaned**2)\n        \n        if denominator == 0:\n            return 0.0\n            \n        return numerator / denominator\n\n    def validate_increments(delta_W, delta_t):\n        \"\"\"\n        Runs the full validation pipeline for a given set of increments.\n        \n        Args:\n            delta_W (np.ndarray): The simulated increments.\n            delta_t (float): The time step.\n\n        Returns:\n            bool: True if both normality and independence tests pass, False otherwise.\n        \"\"\"\n        n = len(delta_W)\n        \n        # Step 1: Standardization\n        if delta_t == 0: # Handle edge case\n            if np.all(delta_W == 0):\n                return True # All increments are zero, trivially normal and independent\n            else:\n                return False\n        \n        Z = delta_W / np.sqrt(delta_t)\n        \n        # Step 2: Normality Test (Kolmogorov-Smirnov)\n        _ks_stat, ks_p_value = stats.kstest(Z, 'norm')\n        normality_pass = (ks_p_value >= ALPHA_NORM)\n        \n        # Step 3: Independence Test (Autocorrelation)\n        independence_pass = True\n        m = len(LAGS)\n        alpha_bonferroni = ALPHA_INDEP / m\n        \n        for lag in LAGS:\n            if lag >= n:\n                continue  # Lag is too large for the series\n            \n            r_lag = calculate_autocorrelation(Z, lag)\n            \n            # Test statistic: sqrt(n) * r_lag ~ N(0,1) under H0\n            test_stat = np.sqrt(n) * r_lag\n            \n            # Two-sided p-value from standard normal distribution\n            p_value_lag = 2 * (1 - stats.norm.cdf(np.abs(test_stat)))\n            \n            if p_value_lag < alpha_bonferroni:\n                independence_pass = False\n                break  # Fail fast\n                \n        # Step 4: Overall Result\n        return normality_pass and independence_pass\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 10000, 'dt': 0.001, 'seed': 123456, 'type': 'ideal', 'params': {}},\n        {'n': 10000, 'dt': 0.001, 'seed': 24681012, 'type': 'student_t', 'params': {'nu': 3}},\n        {'n': 10000, 'dt': 0.001, 'seed': 314159, 'type': 'ar1', 'params': {'rho': 0.3}},\n        {'n': 400, 'dt': 0.01, 'seed': 271828, 'type': 'ideal', 'params': {}},\n    ]\n\n    results = []\n    for case in test_cases:\n        n, dt, seed = case['n'], case['dt'], case['seed']\n        gen_type, params = case['type'], case['params']\n        \n        rng = np.random.default_rng(seed)\n        delta_W = np.array([])\n\n        if gen_type == 'ideal':\n            # Case 1 & 4: Ideal Brownian motion increments\n            xi = rng.normal(loc=0.0, scale=1.0, size=n)\n            delta_W = np.sqrt(dt) * xi\n        elif gen_type == 'student_t':\n            # Case 2: Non-normal (Student's t) but independent increments\n            nu = params['nu']\n            # Generate standard t-distributed variates\n            s_k = rng.standard_t(df=nu, size=n)\n            # Scale to have unit variance. Var(t_nu) = nu / (nu - 2) for nu > 2.\n            scale_factor = np.sqrt(nu / (nu - 2))\n            t_k = s_k / scale_factor\n            delta_W = np.sqrt(dt) * t_k\n        elif gen_type == 'ar1':\n            # Case 3: Normal but dependent (AR(1)) increments\n            rho = params['rho']\n            Y = np.zeros(n)\n            # Generate N(0,1) innovations\n            innovations = rng.normal(loc=0.0, scale=1.0, size=n)\n            # Initialize with stationary distribution\n            Y[0] = innovations[0]\n            # Generate the AR(1) series\n            for k in range(1, n):\n                Y[k] = rho * Y[k-1] + np.sqrt(1 - rho**2) * innovations[k]\n            delta_W = np.sqrt(dt) * Y\n\n        # Run the validation pipeline for the generated increments\n        pass_status = validate_increments(delta_W, dt)\n        results.append(pass_status)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3068822"}]}