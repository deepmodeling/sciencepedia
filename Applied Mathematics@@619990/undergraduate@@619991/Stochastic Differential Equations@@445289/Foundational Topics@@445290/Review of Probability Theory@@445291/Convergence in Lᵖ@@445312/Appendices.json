{"hands_on_practices": [{"introduction": "To truly master convergence concepts, it's essential to understand not just their definitions, but their relationships and limitations. This first exercise builds a foundational counterexample to illustrate a crucial distinction: convergence in probability does not imply the much stronger convergence in $L^p$, even when moments are bounded. By constructing a simple \"traveling spike\" function, you will see precisely why this is the case and uncover the critical role of uniform integrability in bridging this gap [@problem_id:3046407].", "problem": "Let $p \\geq 1$ be fixed. In the study of stochastic differential equations (SDE), one often considers convergence of terminal random variables in various senses. Recall the following foundational definitions:\n- Convergence in probability: a sequence of random variables $X_{n}$ converges to $X$ in probability if for every $\\varepsilon > 0$, $\\mathbb{P}\\left(|X_{n} - X| > \\varepsilon\\right) \\to 0$ as $n \\to \\infty$.\n- Convergence in $L^{p}$: a sequence $X_{n}$ converges to $X$ in $L^{p}$ if $\\mathbb{E}\\left(|X_{n} - X|^{p}\\right) \\to 0$ as $n \\to \\infty$.\n- Uniform integrability: a family $\\{Y_{n}\\}$ of integrable random variables is uniformly integrable if $\\lim_{M \\to \\infty} \\sup_{n} \\mathbb{E}\\left(|Y_{n}| \\,\\mathbf{1}_{\\{|Y_{n}| > M\\}}\\right) = 0$, where $\\mathbf{1}_{A}$ denotes the indicator of the event $A$.\n\nConstruct, on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ with $\\Omega = [0,1]$ and $\\mathbb{P}$ the Lebesgue measure, an explicit sequence $\\{X_{n}\\}_{n \\geq 1}$ and a limit $X$ such that $X_{n} \\to X$ in probability and $\\sup_{n} \\mathbb{E}\\left(|X_{n}|^{p}\\right)  \\infty$, but $X_{n} \\not\\to X$ in $L^{p}$. Explain, in terms of the definition of uniform integrability, why the failure of $\\{\\,|X_{n}|^{p}\\,\\}$ to be uniformly integrable obstructs $L^{p}$ convergence despite bounded $p$-th moments.\n\nFinally, compute the value of the limit superior\n$$\n\\limsup_{n \\to \\infty} \\mathbb{E}\\left(|X_{n} - X|^{p}\\right),\n$$\nand present your answer as a single real number. No rounding is required.", "solution": "The problem requires the construction of a sequence of random variables $\\{X_n\\}_{n \\ge 1}$ on the probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ where $\\Omega = [0,1]$, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0,1]$, and $\\mathbb{P}$ is the Lebesgue measure. This sequence must satisfy several specified properties concerning convergence. We will construct such a sequence, verify that it meets all the stated criteria, provide an explanation for its convergence behavior based on the concept of uniform integrability, and finally compute the requested limit superior for our constructed sequence.\n\nFirst, we establish the framework for our construction. Let the limit random variable be the zero function, $X(\\omega) = 0$ for all $\\omega \\in [0,1]$. We define the sequence of random variables $\\{X_n\\}_{n \\ge 1}$ on this space as a \"traveling spike\" that becomes progressively taller and narrower. For any fixed $p \\ge 1$, we define $X_n$ for $n \\ge 1$ as:\n$$\nX_n(\\omega) = n^{1/p} \\mathbf{1}_{[0, 1/n]}(\\omega)\n$$\nThis means $X_n(\\omega) = n^{1/p}$ if $\\omega \\in [0, 1/n]$ and $X_n(\\omega) = 0$ if $\\omega \\in (1/n, 1]$.\n\nWe now verify that this sequence and its limit $X=0$ satisfy all the required properties.\n\n1.  **Convergence in probability**: We must show that $X_n \\to 0$ in probability, which means $\\lim_{n \\to \\infty} \\mathbb{P}(|X_n - 0| > \\varepsilon) = 0$ for every $\\varepsilon > 0$.\n    For any given $\\varepsilon > 0$, consider the inequality $|X_n(\\omega)| > \\varepsilon$, which is $n^{1/p} > \\varepsilon$. Since $p \\ge 1$ is fixed, the term $n^{1/p}$ grows to infinity as $n \\to \\infty$. Therefore, we can find an integer $N$ such that for all $n > N$, we have $n^{1/p} > \\varepsilon$. For any such $n$, the set of $\\omega$ where $|X_n(\\omega)| > \\varepsilon$ is precisely the set where $X_n(\\omega) = n^{1/p}$, which is the interval $[0, 1/n]$.\n    The probability of this event is its Lebesgue measure:\n    $$\n    \\mathbb{P}(|X_n| > \\varepsilon) = \\mathbb{P}([0, 1/n]) = \\frac{1}{n}\n    $$\n    As $n \\to \\infty$, we have $\\mathbb{P}(|X_n| > \\varepsilon) = \\frac{1}{n} \\to 0$. Thus, $X_n \\to 0$ in probability.\n\n2.  **Boundedness of $p$-th moments**: We need to show that $\\sup_n \\mathbb{E}[|X_n|^p]  \\infty$.\n    We compute the expectation of $|X_n|^p$ by integrating over $\\Omega = [0,1]$:\n    $$\n    \\mathbb{E}[|X_n|^p] = \\int_0^1 |X_n(\\omega)|^p \\,d\\omega = \\int_0^{1/n} (n^{1/p})^p \\,d\\omega + \\int_{1/n}^1 0^p \\,d\\omega = \\int_0^{1/n} n \\,d\\omega = n \\cdot \\left(\\frac{1}{n} - 0\\right) = 1\n    $$\n    Since $\\mathbb{E}[|X_n|^p] = 1$ for all integers $n \\ge 1$, the supremum of these values is $\\sup_n \\mathbb{E}[|X_n|^p] = 1$, which is finite.\n\n3.  **Failure of convergence in $L^p$**: We must demonstrate that $X_n$ does not converge to $X=0$ in $L^p$. This requires showing that $\\lim_{n \\to \\infty} \\mathbb{E}[|X_n - 0|^p] \\neq 0$.\n    From the previous calculation, we have that $\\mathbb{E}[|X_n - 0|^p] = \\mathbb{E}[|X_n|^p] = 1$ for all $n \\ge 1$.\n    Therefore, the limit is:\n    $$\n    \\lim_{n \\to \\infty} \\mathbb{E}[|X_n - 0|^p] = \\lim_{n \\to \\infty} 1 = 1\n    $$\n    Since this limit is $1$ and not $0$, the sequence $\\{X_n\\}$ does not converge to $0$ in $L^p$.\n\nNext, we explain why the failure of uniform integrability obstructs $L^p$ convergence. The Vitali Convergence Theorem states that for a sequence of random variables $\\{Z_n\\}$ in $L^1$, convergence $Z_n \\to Z$ in $L^1$ holds if and only if $Z_n \\to Z$ in probability and the family $\\{Z_n\\}$ is uniformly integrable. Let $Y_n = |X_n - X|^p = |X_n|^p$. Then $L^p$ convergence of $X_n$ to $X$ is equivalent to $L^1$ convergence of $Y_n$ to $Y=0$. We have established that $Y_n \\to 0$ in probability. According to the theorem, the failure of $L^1$ convergence (and thus $L^p$ convergence) must be due to the fact that the family $\\{Y_n\\} = \\{|X_n|^p\\}$ is not uniformly integrable.\n\nLet's show this explicitly. By definition, $\\{Y_n\\}$ is uniformly integrable if $\\lim_{M \\to \\infty} \\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n| > M\\}}] = 0$. In our construction, $Y_n(\\omega) = n \\cdot \\mathbf{1}_{[0, 1/n]}(\\omega)$. The random variable $Y_n$ only takes values $n$ or $0$.\nThe condition $|Y_n| > M$ is met only if $n > M$.\n- If $n \\le M$, the set $\\{\\omega: |Y_n(\\omega)| > M\\}$ is empty, so $\\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n| > M\\}}] = 0$.\n- If $n > M$, the set $\\{\\omega: |Y_n(\\omega)| > M\\}$ is the interval $[0, 1/n]$. For such $n$:\n  $$\n  \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n| > M\\}}] = \\mathbb{E}[n \\cdot \\mathbf{1}_{[0, 1/n]}] = n \\cdot \\mathbb{P}([0, 1/n]) = n \\cdot \\frac{1}{n} = 1\n  $$\nFor any $M > 0$, we can always find an integer $n > M$. Thus, the supremum over all $n$ will always include values of $1$:\n$$\n\\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n| > M\\}}] = \\sup(\\{0 \\text{ for } n \\le M\\} \\cup \\{1 \\text{ for } n > M\\}) = 1\n$$\nTaking the limit as $M \\to \\infty$:\n$$\n\\lim_{M \\to \\infty} \\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n| > M\\}}] = \\lim_{M \\to \\infty} 1 = 1\n$$\nSince this limit is not $0$, the family $\\{|X_n|^p\\}$ is not uniformly integrable. This failure reflects that a persistent amount of the integral's mass ($1$ in this case) comes from arbitrarily large values of $|X_n|^p$, even as the probability of observing these values shrinks. This \"escaping mass\" prevents the total integral from converging to $0$.\n\nFinally, we compute the value of the requested limit superior for our constructed sequence. We need to find $\\limsup_{n \\to \\infty} \\mathbb{E}[|X_n - X|^p]$.\nAs we have shown, for our choice of $X_n$ and $X=0$, the expectation is constant for all $n \\ge 1$:\n$$\n\\mathbb{E}[|X_n - X|^p] = 1\n$$\nThe limit superior of a constant sequence $\\{1, 1, 1, \\dots\\}$ is simply the value of the constant.\n$$\n\\limsup_{n \\to \\infty} \\mathbb{E}[|X_n - X|^p] = \\limsup_{n \\to \\infty} 1 = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "3046407"}, {"introduction": "Now, let's apply these abstract convergence principles to the core of our study: the It么 stochastic integral. It might seem intuitive that if the integrand process $H_n(t)$ converges to zero at every point in time, the resulting integral $\\int H_n(t) \\mathrm{d}W_t$ should also converge to zero. This practice challenges that intuition by constructing a specific counterexample where the integrand vanishes pointwise but the $L^p$ norm of the integral remains constant [@problem_id:3046422]. This reveals that the behavior of the It么 integral depends on the \"total energy\" of the integrand over time, not just its pointwise values.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a standard Brownian motion $\\left(W_{t}\\right)_{t\\geq 0}$ with its usual augmentation. Fix $p\\in(0,\\infty)$. For each $n\\in\\mathbb{N}$, define the deterministic adapted process $H_{n}:\\mathbb{R}_{+}\\times\\Omega\\to\\mathbb{R}$ by\n$$\nH_{n}(t,\\omega)\\equiv H_{n}(t)=n\\,\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t).\n$$\nConsider the It么 integral\n$$\nI_{n}\\equiv \\int_{0}^{\\infty} H_{n}(t)\\,\\mathrm{d}W_{t}.\n$$\nUsing only foundational facts about Brownian motion (independent stationary Gaussian increments) and the construction of the It么 integral for simple adapted processes, answer the following:\n\n- Verify that for every fixed $t\\geq 0$, one has $H_{n}(t)\\to 0$ as $n\\to\\infty$.\n- Compute the value of $\\lim_{n\\to\\infty}\\mathbb{E}\\!\\left[\\,|I_{n}|^{p}\\,\\right]$ in closed form as a function of $p$.\n\nYour final answer must be a single exact analytic expression in terms of $p$. No rounding is required. Explain why this construction shows that pointwise convergence of the integrands $H_{n}$ does not imply convergence of the It么 integrals $I_{n}$ in $L^{p}$, highlighting the role of the absence of a uniform $L^{2}$-dominating envelope over time.", "solution": "The solution proceeds in three parts as requested by the problem statement.\n\n**Part 1: Pointwise convergence of the integrands $H_{n}(t)$**\n\nWe are given the sequence of deterministic processes $H_{n}(t) = n\\,\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t)$. The indicator function $\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t)$ is equal to $1$ if $n  t \\leq n+1/n^{2}$ and $0$ otherwise.\nTo verify the pointwise convergence, we fix an arbitrary $t \\geq 0$. We need to analyze the behavior of $H_{n}(t)$ as $n \\to \\infty$.\nThe condition for $H_n(t)$ to be non-zero is $n  t$. For any fixed $t$, there exists a natural number $N_t = \\lfloor t \\rfloor + 1$ such that for all $n \\geq N_t$, we have $n  t$.\nWhen $n  t$, the condition $n  t$ is false, which implies that the indicator function $\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t)$ is equal to $0$.\nTherefore, for any fixed $t \\geq 0$, for all $n > t$, we have $H_{n}(t) = 0$.\nBy the definition of a limit, this means that\n$$\n\\lim_{n\\to\\infty} H_{n}(t) = 0 \\quad \\text{for every fixed } t \\geq 0.\n$$\nThis verifies the pointwise convergence of the sequence of integrands to the zero function.\n\n**Part 2: Computation of $\\lim_{n\\to\\infty}\\mathbb{E}\\!\\left[\\,|I_{n}|^{p}\\,\\right]$**\n\nThe It么 integral $I_n$ is defined as $I_{n} = \\int_{0}^{\\infty} H_{n}(t)\\,\\mathrm{d}W_{t}$.\nSince $H_{n}(t)$ is a simple function (it is constant on the interval of integration), the integral simplifies.\n$$\nI_{n} = \\int_{0}^{\\infty} n\\,\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t)\\,\\mathrm{d}W_{t} = n \\int_{n}^{n+1/n^{2}} \\mathrm{d}W_{t}.\n$$\nBy the definition of the It么 integral, $\\int_{a}^{b} \\mathrm{d}W_{t} = W_{b} - W_{a}$. Thus,\n$$\nI_{n} = n \\left( W_{n+1/n^{2}} - W_{n} \\right).\n$$\nAccording to the foundational properties of standard Brownian motion, the increment $W_{s} - W_{r}$ for $s > r$ is a normally distributed random variable with mean $0$ and variance $s-r$. That is, $W_{s} - W_{r} \\sim \\mathcal{N}(0, s-r)$.\nIn our case, the increment is $W_{n+1/n^{2}} - W_{n}$. The time difference is $(n+1/n^{2}) - n = 1/n^{2}$.\nSo, the increment has the distribution $W_{n+1/n^{2}} - W_{n} \\sim \\mathcal{N}(0, 1/n^2)$.\nThe random variable $I_n$ is this increment scaled by the constant $n$. If a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $c X \\sim \\mathcal{N}(c\\mu, c^2\\sigma^2)$.\nApplying this rule with $c=n$, $\\mu=0$, and $\\sigma^2=1/n^2$, we find the distribution of $I_n$:\n$$\nI_{n} \\sim \\mathcal{N}\\left(n \\cdot 0, n^2 \\cdot \\frac{1}{n^2}\\right) = \\mathcal{N}(0, 1).\n$$\nRemarkably, for every $n \\in \\mathbb{N}$, the random variable $I_n$ has a standard normal distribution.\nWe are asked to compute the limit of $\\mathbb{E}[|I_{n}|^{p}]$. Since the distribution of $I_n$ is the same for all $n$, the expectation $\\mathbb{E}[|I_{n}|^{p}]$ is a constant value independent of $n$.\nLet $Z \\sim \\mathcal{N}(0, 1)$. We need to calculate $\\mathbb{E}[|Z|^p]$. The probability density function of $Z$ is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\n$$\n\\mathbb{E}[|Z|^p] = \\int_{-\\infty}^{\\infty} |z|^p \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\mathrm{d}z.\n$$\nThe integrand is an even function of $z$. We can rewrite the integral as:\n$$\n\\mathbb{E}[|Z|^p] = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} z^p \\exp\\left(-\\frac{z^2}{2}\\right) \\mathrm{d}z.\n$$\nTo evaluate this integral, we perform a substitution. Let $u = z^2/2$. Then $z^2 = 2u$, which means $z = \\sqrt{2u}$ for $z>0$. The differential is $\\mathrm{d}z = \\frac{1}{2\\sqrt{2u}} \\cdot 2\\,\\mathrm{d}u = \\frac{1}{\\sqrt{2u}}\\mathrm{d}u$.\nSubstituting these into the integral gives:\n$$\n\\int_{0}^{\\infty} (\\sqrt{2u})^p \\exp(-u) \\frac{1}{\\sqrt{2u}}\\mathrm{d}u = \\int_{0}^{\\infty} (2u)^{p/2} (2u)^{-1/2} \\exp(-u) \\mathrm{d}u = \\int_{0}^{\\infty} 2^{(p-1)/2} u^{(p-1)/2} \\exp(-u) \\mathrm{d}u.\n$$\nThis can be expressed using the Gamma function, $\\Gamma(x) = \\int_0^\\infty t^{x-1} e^{-t} \\mathrm{d}t$.\n$$\n2^{(p-1)/2} \\int_{0}^{\\infty} u^{\\frac{p+1}{2}-1} \\exp(-u) \\mathrm{d}u = 2^{(p-1)/2} \\Gamma\\left(\\frac{p+1}{2}\\right).\n$$\nNow, we combine this with the pre-factor:\n$$\n\\mathbb{E}[|Z|^p] = \\frac{2}{\\sqrt{2\\pi}} \\cdot 2^{(p-1)/2} \\Gamma\\left(\\frac{p+1}{2}\\right) = \\frac{2^1}{2^{1/2}\\pi^{1/2}} \\cdot 2^{(p-1)/2} \\Gamma\\left(\\frac{p+1}{2}\\right) = 2^{1 - 1/2 + (p-1)/2} \\pi^{-1/2} \\Gamma\\left(\\frac{p+1}{2}\\right).\n$$\nSimplifying the exponent of $2$:\n$$\n1 - \\frac{1}{2} + \\frac{p-1}{2} = \\frac{2-1+p-1}{2} = \\frac{p}{2}.\n$$\nThus, the expectation is:\n$$\n\\mathbb{E}[|I_{n}|^p] = \\mathbb{E}[|Z|^p] = 2^{p/2} \\pi^{-1/2} \\Gamma\\left(\\frac{p+1}{2}\\right).\n$$\nSince this value is independent of $n$, the limit is the value itself.\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\!\\left[\\,|I_{n}|^{p}\\,\\right] = 2^{p/2} \\pi^{-1/2} \\Gamma\\left(\\frac{p+1}{2}\\right).\n$$\n\n**Part 3: Explanation of non-convergence in $L^p$**\n\nWe have shown that $H_n(t) \\to 0$ for every fixed $t \\geq 0$. However, the sequence of It么 integrals $I_n$ does not converge to $0$ in $L^p(\\Omega)$. Convergence of $I_n$ to $0$ in $L^p$ would require $\\lim_{n\\to\\infty}\\mathbb{E}[|I_n - 0|^p] = \\lim_{n\\to\\infty}\\mathbb{E}[|I_n|^p] = 0$.\nOur calculation in Part 2 shows that this limit is a positive constant for any $p>0$, since $2^{p/2}>0$, $\\pi^{-1/2}>0$, and $\\Gamma((p+1)/2)>0$ for $p>0$. Therefore, $I_n$ does not converge to $0$ in $L^p$.\n\nThis example demonstrates that pointwise convergence of integrands is not a sufficient condition for the convergence of the corresponding It么 integrals in $L^p$. The reason lies in the failure of a crucial condition, often found in dominated convergence-type theorems for stochastic integrals.\n\nFor the special case of $p=2$, the It么 isometry states that $\\mathbb{E}[I_n^2] = \\mathbb{E}[\\int_0^\\infty H_n(t)^2 \\mathrm{d}t]$. Since $H_n$ is deterministic, this simplifies to $\\mathbb{E}[I_n^2] = \\int_0^\\infty H_n(t)^2 \\mathrm{d}t$.\nLet's compute this integral:\n$$\n\\int_0^\\infty H_n(t)^2 \\mathrm{d}t = \\int_0^\\infty \\left(n\\,\\mathbf{1}_{(n,\\,n+1/n^{2}]}(t)\\right)^2 \\mathrm{d}t = \\int_n^{n+1/n^2} n^2 \\mathrm{d}t = n^2 \\left( (n+1/n^2) - n \\right) = n^2 \\cdot \\frac{1}{n^2} = 1.\n$$\nThis result is consistent with our general finding for $p=2$: $\\mathbb{E}[I_n^2] = 2^{2/2}\\pi^{-1/2}\\Gamma(3/2) = 2\\pi^{-1/2}(\\frac{1}{2}\\sqrt{\\pi}) = 1$.\nThe $L^2$ convergence theorem for It么 integrals states that if $\\lim_{n \\to \\infty} \\mathbb{E}[\\int_0^\\infty (H_n(t) - H(t))^2 \\mathrm{d}t] = 0$, then $I_n \\to I$ in $L^2(\\Omega)$. Here, $H(t)=0$. The condition becomes $\\lim_{n \\to \\infty} \\int_0^\\infty H_n(t)^2 \\mathrm{d}t = 0$. We have just shown this limit is $1$, not $0$. The condition is not met, and indeed, the conclusion does not hold.\n\nThe problem asks to highlight the role of the \"absence of a uniform $L^2$-dominating envelope over time\". This refers to the fact that there is no single function $K(t) \\in L^2(\\mathbb{R}_+)$, i.e., $\\int_0^\\infty K(t)^2 \\mathrm{d}t  \\infty$, such that $|H_n(t)| \\leq K(t)$ for all $n$ and almost all $t$.\nIf such a dominating function $K(t)$ existed, then we would have $|H_n(t)|^2 \\leq K(t)^2$. Since we know $H_n(t)^2 \\to 0$ for each $t$, the Lebesgue Dominated Convergence Theorem would apply to the sequence of functions $f_n(t) = H_n(t)^2$:\n$$\n\\lim_{n\\to\\infty} \\int_0^\\infty H_n(t)^2 \\mathrm{d}t = \\int_0^\\infty \\lim_{n\\to\\infty} H_n(t)^2 \\mathrm{d}t = \\int_0^\\infty 0 \\mathrm{d}t = 0.\n$$\nThis would imply $L^2$ convergence of the integrals. However, our calculation shows $\\int_0^\\infty H_n(t)^2 \\mathrm{d}t = 1$, so such a dominating function cannot exist. The peak of the integrand $H_n(t)$ grows as $n$, preventing the existence of an $L^2$-integrable envelope. The \"mass\" of the integrand, measured by its $L^2$ norm, does not vanish but instead \"escapes to infinity\" along the time axis, maintaining a constant value of $1$. This escaping mass is precisely why the variance of $I_n$ remains constant and prevents convergence in $L^p$.", "answer": "$$\\boxed{2^{p/2} \\pi^{-1/2} \\Gamma\\left(\\frac{p+1}{2}\\right)}$$", "id": "3046422"}, {"introduction": "We now advance to a more subtle and powerful result relevant to the analysis of SDE solutions. The previous exercise showed that pointwise convergence of integrands is insufficient for $L^p$ convergence of the integral. Here, we investigate a much stronger condition: convergence of the integrands in the $L^2(\\Omega\\times[0,T])$ norm. This practice demonstrates that even this is not enough to guarantee convergence for the supremum of the integral process in $L^p$ when $p \\gt 2$ [@problem_id:3046412]. This non-trivial result underscores the importance of tools like the Burkholder-Davis-Gundy inequalities for controlling the pathwise behavior of stochastic integrals.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\in[0,T]},\\mathbb{P}\\right)$ be a filtered probability space that supports a standard Brownian motion $\\{W_t\\}_{t\\in[0,T]}$, with the usual augmentation, and let $p>2$ be fixed. Consider the It么 integral of a predictable, square-integrable process $H$ against $\\{W_t\\}$, defined for $t\\in[0,T]$ by $M_t=\\int_{0}^{t} H_s\\,\\mathrm{d}W_s$. Convergence in $L^2$ for integrands means $\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right]\\to 0$ as $n\\to\\infty$. Convergence in $L^p$ for the supremum of stochastic integrals means $\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s\\right|^p\\right]\\to 0$ as $n\\to\\infty$.\n\nConstruct an explicit sequence of predictable processes $\\{H^n\\}_{n\\in\\mathbb{N}}$ such that $H^n\\to 0$ in $L^2(\\Omega\\times[0,T])$ but the sequence of stochastic integrals $M_t^n=\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$ fails to converge to $0$ in $L^p$ in the sense of the supremum over $[0,T]$. Your construction must use only simple predictable processes and standard properties of Brownian motion.\n\nConcretely, define events $A_n\\in\\mathcal{F}_0$ with $\\mathbb{P}(A_n)=n^{-p}$, independent of $\\{W_t\\}_{t\\in[0,T]}$, and set\n$$\nH^n_t = n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t),\\qquad t\\in[0,T].\n$$\nVerify that $H^n\\to 0$ in $L^2(\\Omega\\times[0,T])$. Then, for $M_t^n=\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$, compute the exact value of the limit\n$$\n\\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right].\n$$\nYour final answer must be a single closed-form analytic expression. Do not provide inequalities. If you express your answer in terms of an expectation involving Brownian motion, standardize it on the unit interval $[0,1]$ using Brownian scaling.", "solution": "We begin by verifying the first required property: the convergence of $\\{H^n_t\\}$ to $0$ in $L^2(\\Omega\\times[0,T])$. The squared norm in this space is given by $\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right]$. We compute this quantity for the given process $H^n_t$.\nThe process is defined as $H^n_t = n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)$. Its squared magnitude is $|H^n_t|^2 = \\left(n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)\\right)^2 = n^2\\,\\mathbf{1}_{A_n}^2\\,\\mathbf{1}_{[0,T]}(t)^2 = n^2\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)$, since the indicator function $\\mathbf{1}_E$ satisfies $\\mathbf{1}_E^2 = \\mathbf{1}_E$.\n\nWe first integrate with respect to time over the interval $[0,T]$. The random variable $\\mathbf{1}_{A_n}$ is constant with respect to the time variable $s$.\n$$\n\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s = \\int_{0}^{T} n^2\\,\\mathbf{1}_{A_n}\\,\\mathrm{d}s = n^2\\,\\mathbf{1}_{A_n} \\int_{0}^{T} \\mathrm{d}s = n^2\\,T\\,\\mathbf{1}_{A_n}.\n$$\nNext, we take the expectation with respect to the probability measure $\\mathbb{P}$.\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = \\mathbb{E}\\!\\left[n^2\\,T\\,\\mathbf{1}_{A_n}\\right] = n^2\\,T\\,\\mathbb{E}\\!\\left[\\mathbf{1}_{A_n}\\right].\n$$\nThe expectation of an indicator function of an event is the probability of that event, $\\mathbb{E}[\\mathbf{1}_{A_n}] = \\mathbb{P}(A_n)$. We are given that $\\mathbb{P}(A_n) = n^{-p}$.\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = n^2\\,T\\,n^{-p} = T\\,n^{2-p}.\n$$\nWe are given that $p2$, which implies $2-p  0$. Therefore, as $n \\to \\infty$, the term $n^{2-p}$ approaches $0$.\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = \\lim_{n\\to\\infty} T\\,n^{2-p} = 0.\n$$\nThis verifies that $H^n \\to 0$ in $L^2(\\Omega\\times[0,T])$.\n\nNow, we proceed to compute the limit of the expectation of the $p$-th power of the supremum of the stochastic integral $M_t^n = \\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$.\nFirst, we express $M_t^n$ more explicitly. Since $A_n \\in \\mathcal{F}_0$, the process $\\mathbf{1}_{A_n}$ is $\\mathcal{F}_0$-measurable, and therefore predictable. We can pull the random variable $\\mathbf{1}_{A_n}$ and the constant $n$ out of the stochastic integral.\n$$\nM_t^n = \\int_{0}^{t} n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(s)\\,\\mathrm{d}W_s = n\\,\\mathbf{1}_{A_n} \\int_{0}^{t} \\mathrm{d}W_s = n\\,\\mathbf{1}_{A_n}\\,W_t, \\quad \\text{for } t \\in [0,T].\n$$\nNext, we find the supremum of the absolute value of this process over the interval $[0,T]$.\n$$\n\\sup_{0\\le t\\le T} \\left|M_t^n\\right| = \\sup_{0\\le t\\le T} \\left|n\\,\\mathbf{1}_{A_n}\\,W_t\\right| = n\\,|\\mathbf{1}_{A_n}| \\sup_{0\\le t\\le T} |W_t| = n\\,\\mathbf{1}_{A_n} \\sup_{0\\le t\\le T} |W_t|.\n$$\nWe are interested in the expectation of the $p$-th power of this expression.\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = \\mathbb{E}\\!\\left[\\left(n\\,\\mathbf{1}_{A_n} \\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[n^p\\,\\mathbf{1}_{A_n}^p \\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nUsing $\\mathbf{1}_{A_n}^p = \\mathbf{1}_{A_n}$, this simplifies to:\n$$\n\\mathbb{E}\\!\\left[n^p\\,\\mathbf{1}_{A_n} \\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nThe problem states that the event $A_n$ is independent of the Brownian motion $\\{W_t\\}_{t\\in[0,T]}$. This means the random variable $\\mathbf{1}_{A_n}$ is independent of the random variable $\\sup_{0\\le t\\le T} |W_t|$. For independent random variables, the expectation of their product is the product of their expectations.\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = n^p\\,\\mathbb{E}\\!\\left[\\mathbf{1}_{A_n}\\right]\\,\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nSubstituting $\\mathbb{E}[\\mathbf{1}_{A_n}]=\\mathbb{P}(A_n)=n^{-p}$:\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = n^p\\,n^{-p}\\,\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nThis expression is a constant with respect to $n$. Therefore, its limit as $n\\to\\infty$ is the expression itself.\n$$\n\\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nSince $p>0$, this expectation is strictly positive, proving that the sequence of stochastic integrals does not converge to $0$ in the specified $L^p$ sense.\n\nFinally, we must express this result in the required form, standardized on the unit interval $[0,1]$ using Brownian scaling. The scaling property of Brownian motion states that for any constant $c>0$, the process $\\{W_{ct}\\}_{t\\ge 0}$ has the same distribution as the process $\\{\\sqrt{c}\\,W_t\\}_{t\\ge 0}$.\nLet $t=Tu$, where $u \\in [0,1]$. Then $\\{W_{Tu}\\}_{u\\in[0,1]}$ has the same distribution as $\\{\\sqrt{T}\\,W_u\\}_{u\\in[0,1]}$.\nTherefore, the random variable $\\sup_{0\\le t\\le T}|W_t| = \\sup_{0\\le u\\le 1}|W_{Tu}|$ has the same distribution as $\\sup_{0\\le u\\le 1}|\\sqrt{T}\\,W_u| = \\sqrt{T}\\sup_{0\\le u\\le 1}|W_u|$.\nThis implies that their moments are equal.\n$$\n\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[\\left(\\sqrt{T}\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right] = \\mathbb{E}\\!\\left[T^{\\frac{p}{2}}\\left(\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right].\n$$\nPulling the constant $T^{\\frac{p}{2}}$ out of the expectation gives the final expression:\n$$\nT^{\\frac{p}{2}} \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right].\n$$\nAs per the problem's instructions, this is a closed-form expression in terms of $T$, $p$, and a universal constant defined by an expectation of a functional of standard Brownian motion on the unit interval.", "answer": "$$\\boxed{T^{\\frac{p}{2}} \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le 1} |W_t|\\right)^{p}\\right]}$$", "id": "3046412"}]}