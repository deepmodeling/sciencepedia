{"hands_on_practices": [{"introduction": "Fubini's and Tonelli's theorems are cornerstones of measure theory that provide the essential justification for changing the order of integration. This practice [@problem_id:3070770] solidifies your understanding of these powerful results, not just by proving them, but by immediately applying them to a tangible problem in probability. You will see firsthand how the abstract concept of a product measure allows you to calculate the expected value of a product of independent random variables using simple, one-dimensional iterated integrals.", "problem": "Let $(S,\\Sigma,\\mu)$ and $(T,\\mathcal{T},\\nu)$ be $\\sigma$-finite measure spaces, and let $f:S\\times T\\to[0,\\infty]$ be $\\Sigma\\otimes\\mathcal{T}$-measurable. State Tonelli’s theorem for nonnegative measurable functions and prove it from first principles by reducing to simple functions and invoking the monotone convergence theorem. Next, let $f:S\\times T\\to\\mathbb{R}$ be $\\Sigma\\otimes\\mathcal{T}$-measurable with $\\int_{S\\times T}|f|\\,\\mathrm{d}(\\mu\\otimes\\nu)\\infty$. State Fubini’s theorem for integrable functions and prove it by decomposing $f$ into positive and negative parts and applying Tonelli’s theorem to those parts together with linearity of the integral.\n\nThen, consider a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ and two independent nonnegative random variables $X:\\Omega\\to[0,\\infty)$ and $Y:\\Omega\\to[0,\\infty)$ with densities (with respect to Lebesgue measure on $[0,\\infty)$)\n$$\nf_{X}(x)=a^{2}x\\,\\exp(-a x)\\,\\mathbf{1}_{[0,\\infty)}(x),\\qquad f_{Y}(y)=b\\,\\exp(-b y)\\,\\mathbf{1}_{[0,\\infty)}(y),\n$$\nfor fixed parameters $a0$ and $b0$. Using the product measure formalism, justify each step by the appropriate theorem (Tonelli or Fubini) to write the expectation $\\mathbb{E}[XY]$ as an iterated integral over $[0,\\infty)\\times[0,\\infty)$ and evaluate it explicitly. Express your final answer as a closed-form expression in terms of $a$ and $b$ only.", "solution": "The solution is presented in three parts: first, the statement and proof of Tonelli's theorem; second, the statement and proof of Fubini's theorem; and third, an application of these theorems to calculate an expectation for a product of two independent random variables.\n\n### Part 1: Tonelli's Theorem\n\n**Statement of Tonelli's Theorem:**\n\nLet $(S,\\Sigma,\\mu)$ and $(T,\\mathcal{T},\\nu)$ be $\\sigma$-finite measure spaces. Let $f:S \\times T \\to [0,\\infty]$ be a non-negative, $\\Sigma\\otimes\\mathcal{T}$-measurable function. Then the function $g:S \\to [0,\\infty]$ defined by $g(s) = \\int_T f(s,t)\\,\\mathrm{d}\\nu(t)$ is $\\Sigma$-measurable, the function $h:T \\to [0,\\infty]$ defined by $h(t) = \\int_S f(s,t)\\,\\mathrm{d}\\mu(s)$ is $\\mathcal{T}$-measurable, and\n$$\n\\int_S \\left(\\int_T f(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_T \\left(\\int_S f(s,t)\\,\\mathrm{d}\\mu(s)\\right)\\,\\mathrm{d}\\nu(t) = \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu)\n$$\nThe value of these integrals can be infinite.\n\n**Proof of Tonelli's Theorem:**\n\nThe proof proceeds in four steps, starting from the simplest functions and building up to general non-negative measurable functions.\n\n**Step 1: Indicator functions of measurable rectangles.**\nLet $f = \\mathbf{1}_{A \\times B}$ where $A \\in \\Sigma$ and $B \\in \\mathcal{T}$.\nThe function $g(s)$ is:\n$$ g(s) = \\int_T \\mathbf{1}_{A \\times B}(s,t)\\,\\mathrm{d}\\nu(t) = \\int_T \\mathbf{1}_A(s)\\mathbf{1}_B(t)\\,\\mathrm{d}\\nu(t) = \\mathbf{1}_A(s) \\int_T \\mathbf{1}_B(t)\\,\\mathrm{d}\\nu(t) = \\mathbf{1}_A(s)\\nu(B) $$\nSince $\\mathbf{1}_A(s)$ is a $\\Sigma$-measurable function, $g(s)$ is $\\Sigma$-measurable.\nThe iterated integral is:\n$$ \\int_S g(s)\\,\\mathrm{d}\\mu(s) = \\int_S \\mathbf{1}_A(s)\\nu(B)\\,\\mathrm{d}\\mu(s) = \\nu(B)\\int_S \\mathbf{1}_A(s)\\,\\mathrm{d}\\mu(s) = \\nu(B)\\mu(A) $$\nBy definition of the product measure $\\mu\\otimes\\nu$ on the semi-algebra of measurable rectangles, $(\\mu\\otimes\\nu)(A\\times B) = \\mu(A)\\nu(B)$. The integral of $f$ over the product space is:\n$$ \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\int_{S \\times T} \\mathbf{1}_{A\\times B}\\,\\mathrm{d}(\\mu\\otimes\\nu) = (\\mu\\otimes\\nu)(A\\times B) = \\mu(A)\\nu(B) $$\nThus, the theorem holds for indicator functions of measurable rectangles. The argument for the other iterated integral is symmetrical. The $\\sigma$-finiteness condition ensures that if $\\mu(A)=\\infty$ and $\\nu(B)=0$ (or vice-versa), the product $\\mu(A)\\nu(B)$ is consistently defined as $0$, which matches the integral calculation.\n\n**Step 2: Indicator functions of general measurable sets.**\nLet $\\mathcal{A}$ be the collection of sets $E \\in \\Sigma\\otimes\\mathcal{T}$ for which the theorem holds, i.e., for which the functions $s \\mapsto \\nu(E_s)$ and $t \\mapsto \\mu(E_t)$ are measurable and $(\\mu\\otimes\\nu)(E) = \\int_S \\nu(E_s)\\,\\mathrm{d}\\mu(s) = \\int_T \\mu(E_t)\\,\\mathrm{d}\\nu(t)$, where $E_s = \\{t \\in T : (s,t) \\in E\\}$ and $E_t = \\{s \\in S : (s,t) \\in E\\}$.\nFrom Step $1$, $\\mathcal{A}$ contains all measurable rectangles. The collection of measurable rectangles is a $\\pi$-system (it is closed under finite intersections).\nWe use the $\\sigma$-finiteness of the measures. Let $S = \\bigcup_{i=1}^\\infty S_i$ and $T = \\bigcup_{j=1}^\\infty T_j$ with $\\mu(S_i)  \\infty$ and $\\nu(T_j)  \\infty$ for all $i,j$.\nConsider the theorem restricted to a space $S_i \\times T_j$ with finite measure. The collection $\\mathcal{A}_{ij} = \\{E \\in \\Sigma\\otimes\\mathcal{T} \\mid E \\subseteq S_i \\times T_j, \\text{theorem holds for } \\mathbf{1}_E\\}$ is a $\\lambda$-system (or Dynkin system). It contains $S_i \\times T_j$, is closed under proper differences, and under countable increasing unions. Since $\\mathcal{A}_{ij}$ contains the $\\pi$-system of measurable rectangles within $S_i \\times T_j$, the $\\pi$-$\\lambda$ theorem implies that $\\mathcal{A}_{ij}$ contains $\\sigma(\\text{rectangles}) \\cap (S_i \\times T_j) = (\\Sigma\\otimes\\mathcal{T}) \\cap (S_i \\times T_j)$.\nA general set $E \\in \\Sigma\\otimes\\mathcal{T}$ can be written as $E = \\bigcup_{i,j} (E \\cap (S_i \\times T_j))$. Let $E_{ij} = E \\cap (S_i \\times T_j)$. The theorem holds for each $\\mathbf{1}_{E_{ij}}$. By applying the monotone convergence theorem to the increasing sequence of sets $F_N = \\bigcup_{i,j=1}^N E_{ij}$, we extend the result to all $E \\in \\Sigma\\otimes\\mathcal{T}$.\n\n**Step 3: Non-negative simple functions.**\nLet $f$ be a non-negative, $\\Sigma\\otimes\\mathcal{T}$-measurable simple function. It can be written as a finite linear combination of indicator functions of disjoint measurable sets: $f = \\sum_{k=1}^n c_k \\mathbf{1}_{E_k}$, where $c_k \\ge 0$ and $E_k \\in \\Sigma\\otimes\\mathcal{T}$.\nBy linearity of the integral:\n$$ \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\int_{S \\times T} \\sum_{k=1}^n c_k \\mathbf{1}_{E_k}\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\sum_{k=1}^n c_k \\int_{S \\times T} \\mathbf{1}_{E_k}\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\sum_{k=1}^n c_k (\\mu\\otimes\\nu)(E_k) $$\nFor the iterated integral, we use linearity for each integral:\n\\begin{align*}\n\\int_S \\left(\\int_T f(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_S \\left(\\int_T \\sum_{k=1}^n c_k \\mathbf{1}_{E_k}(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) \\\\\n= \\int_S \\left(\\sum_{k=1}^n c_k \\int_T \\mathbf{1}_{E_k}(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) \\\\\n= \\sum_{k=1}^n c_k \\int_S \\left(\\int_T \\mathbf{1}_{E_k}(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s)\n\\end{align*}\nFrom Step 2, we know that for each $k$, $\\int_S (\\int_T \\mathbf{1}_{E_k}\\,\\mathrm{d}\\nu)\\,\\mathrm{d}\\mu = (\\mu\\otimes\\nu)(E_k)$.\nTherefore, $\\int_S (\\int_T f\\,\\mathrm{d}\\nu)\\,\\mathrm{d}\\mu = \\sum_{k=1}^n c_k (\\mu\\otimes\\nu)(E_k)$, establishing the equality. The measurability of $s \\mapsto \\int_T f(s,t)\\mathrm{d}\\nu(t)$ follows from it being a finite linear combination of measurable functions from Step 2.\n\n**Step 4: General non-negative measurable functions.**\nLet $f$ be a non-negative, $\\Sigma\\otimes\\mathcal{T}$-measurable function. There exists a sequence of non-negative, measurable simple functions $(f_n)_{n \\in \\mathbb{N}}$ such that $0 \\le f_n \\uparrow f$ pointwise on $S \\times T$.\nFor each $s \\in S$, the sequence of functions $t \\mapsto f_n(s,t)$ is non-decreasing and converges to $t \\mapsto f(s,t)$. By the Monotone Convergence Theorem (MCT) applied to the integral over $(T, \\mathcal{T}, \\nu)$:\n$$ \\int_T f_n(s,t)\\,\\mathrm{d}\\nu(t) \\xrightarrow{n\\to\\infty} \\int_T f(s,t)\\,\\mathrm{d}\\nu(t) $$\nLet $g_n(s) = \\int_T f_n(s,t)\\,\\mathrm{d}\\nu(t)$ and $g(s) = \\int_T f(s,t)\\,\\mathrm{d}\\nu(t)$. We have $g_n(s) \\uparrow g(s)$. From Step $3$, each $g_n$ is measurable. Thus $g$, as a pointwise limit of measurable functions, is measurable.\nNow, we apply MCT to the integral over $(S, \\Sigma, \\mu)$:\n$$ \\lim_{n\\to\\infty} \\int_S g_n(s)\\,\\mathrm{d}\\mu(s) = \\int_S g(s)\\,\\mathrm{d}\\mu(s) = \\int_S \\left(\\int_T f(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) $$\nFrom Step $3$, for each $n$, we have $\\int_S g_n(s)\\,\\mathrm{d}\\mu(s) = \\int_{S \\times T} f_n\\,\\mathrm{d}(\\mu\\otimes\\nu)$.\nApplying MCT on the product space $(S \\times T, \\Sigma\\otimes\\mathcal{T}, \\mu\\otimes\\nu)$:\n$$ \\lim_{n\\to\\infty} \\int_{S \\times T} f_n\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu) $$\nCombining these equalities, we obtain the main result:\n$$ \\int_S \\left(\\int_T f\\,\\mathrm{d}\\nu\\right)\\,\\mathrm{d}\\mu = \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu) $$\nThe other equality follows by symmetry. This completes the proof of Tonelli's theorem.\n\n### Part 2: Fubini's Theorem\n\n**Statement of Fubini's Theorem:**\n\nLet $(S,\\Sigma,\\mu)$ and $(T,\\mathcal{T},\\nu)$ be $\\sigma$-finite measure spaces. Let $f:S\\times T\\to\\mathbb{R}$ be a $\\Sigma\\otimes\\mathcal{T}$-measurable function. If $f$ is integrable with respect to $\\mu\\otimes\\nu$, i.e., $\\int_{S\\times T} |f|\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty$, then:\n\\begin{enumerate}\n    \\item For $\\mu$-almost every $s \\in S$, the function $t \\mapsto f(s,t)$ is $\\nu$-integrable (i.e., $\\int_T |f(s,t)|\\,\\mathrm{d}\\nu(t)  \\infty$).\n    \\item The function $s \\mapsto \\int_T f(s,t)\\,\\mathrm{d}\\nu(t)$ (defined for a.e. $s$) is $\\mu$-integrable.\n    \\item Symmetrically, for $\\nu$-almost every $t \\in T$, the function $s \\mapsto f(s,t)$ is $\\mu$-integrable.\n    \\item The function $t \\mapsto \\int_S f(s,t)\\,\\mathrm{d}\\mu(s)$ (defined for a.e. $t$) is $\\nu$-integrable.\n\\end{enumerate}\nFurthermore, the iterated integrals exist and are equal to the integral over the product space:\n$$ \\int_S \\left(\\int_T f(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_T \\left(\\int_S f(s,t)\\,\\mathrm{d}\\mu(s)\\right)\\,\\mathrm{d}\\nu(t) = \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu) $$\n\n**Proof of Fubini's Theorem:**\n\nThe proof relies on decomposing $f$ into its positive and negative parts and applying Tonelli's theorem.\nLet $f$ be an integrable function on $S \\times T$. We decompose $f$ as $f = f^+ - f^-$, where $f^+(s,t) = \\max(f(s,t), 0)$ and $f^-(s,t) = \\max(-f(s,t), 0)$. Both $f^+$ and $f^-$ are non-negative, $\\Sigma\\otimes\\mathcal{T}$-measurable functions.\nThe integrability condition on $f$ is $\\int_{S \\times T} |f|\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty$. Since $|f| = f^+ + f^-$, this implies:\n$$ \\int_{S \\times T} f^+\\,\\mathrm{d}(\\mu\\otimes\\nu) + \\int_{S \\times T} f^-\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty $$\nAs $f^+$ and $f^-$ are non-negative, their integrals must be finite:\n$$ \\int_{S \\times T} f^+\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty \\quad \\text{and} \\quad \\int_{S \\times T} f^-\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty $$\nWe can now apply Tonelli's theorem to $f^+$ and $f^-$. For $f^+$:\n$$ \\int_S \\left(\\int_T f^+(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_{S \\times T} f^+\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty $$\nLet $g^+(s) = \\int_T f^+(s,t)\\,\\mathrm{d}\\nu(t)$. The finiteness of $\\int_S g^+(s)\\,\\mathrm{d}\\mu(s)$ implies that $g^+(s)$ must be finite for $\\mu$-almost every $s \\in S$.\nSimilarly, for $f^-$:\n$$ \\int_S \\left(\\int_T f^-(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_{S \\times T} f^-\\,\\mathrm{d}(\\mu\\otimes\\nu)  \\infty $$\nLetting $g^-(s) = \\int_T f^-(s,t)\\,\\mathrm{d}\\nu(t)$, we find that $g^-(s)$ is finite for $\\mu$-almost every $s \\in S$.\nThe integrability of the function $t \\mapsto f(s,t)$ for a fixed $s$ requires $\\int_T |f(s,t)|\\,\\mathrm{d}\\nu(t)  \\infty$. We have:\n$$ \\int_T |f(s,t)|\\,\\mathrm{d}\\nu(t) = \\int_T (f^+(s,t) + f^-(s,t))\\,\\mathrm{d}\\nu(t) = g^+(s) + g^-(s) $$\nSince both $g^+(s)$ and $g^-(s)$ are finite for $\\mu$-a.e. $s$, their sum is also finite for $\\mu$-a.e. $s$. This proves conclusion $1$.\nFor any $s$ where $g^+(s)$ and $g^-(s)$ are finite, the integral of $f(s,t)$ is well-defined by linearity:\n$$ \\int_T f(s,t)\\,\\mathrm{d}\\nu(t) = \\int_T f^+(s,t)\\,\\mathrm{d}\\nu(t) - \\int_T f^-(s,t)\\,\\mathrm{d}\\nu(t) = g^+(s) - g^-(s) $$\nThis defines the function $s \\mapsto \\int_T f(s,t)\\,\\mathrm{d}\\nu(t)$ on a set of full measure. Let's call this function $g(s)$. The functions $g^+(s)$ and $g^-(s)$ are $\\mu$-integrable (as shown by Tonelli's theorem). Since $g(s) = g^+(s) - g^-(s)$, $g(s)$ is the difference of two integrable functions, and is therefore itself integrable. This proves conclusion $2$.\nFinally, we compute the iterated integral:\n\\begin{align*}\n\\int_S \\left(\\int_T f(s,t)\\,\\mathrm{d}\\nu(t)\\right)\\,\\mathrm{d}\\mu(s) = \\int_S g(s)\\,\\mathrm{d}\\mu(s) = \\int_S (g^+(s) - g^-(s))\\,\\mathrm{d}\\mu(s) \\\\\n= \\int_S g^+(s)\\,\\mathrm{d}\\mu(s) - \\int_S g^-(s)\\,\\mathrm{d}\\mu(s) \\\\\n= \\int_{S \\times T} f^+\\,\\mathrm{d}(\\mu\\otimes\\nu) - \\int_{S \\times T} f^-\\,\\mathrm{d}(\\mu\\otimes\\nu) \\\\\n= \\int_{S \\times T} (f^+ - f^-)\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\int_{S \\times T} f\\,\\mathrm{d}(\\mu\\otimes\\nu)\n\\end{align*}\nThe argument for the other iterated integral is identical. This completes the proof of Fubini's theorem.\n\n### Part 3: Application\n\nWe are asked to compute $\\mathbb{E}[XY]$ for two independent, non-negative random variables $X$ and $Y$ with given probability density functions:\n$$\nf_{X}(x)=a^{2}x\\,\\exp(-a x)\\,\\mathbf{1}_{[0,\\infty)}(x) \\quad \\text{and} \\quad f_{Y}(y)=b\\,\\exp(-b y)\\,\\mathbf{1}_{[0,\\infty)}(y)\n$$\nfor parameters $a0$, $b0$.\n\nBy the law of the unconscious statistician, the expectation $\\mathbb{E}[XY]$ is given by the integral of the function $g(x,y)=xy$ with respect to the joint probability measure of $(X,Y)$. Since $X$ and $Y$ are independent, their joint probability density function is the product of their individual densities:\n$$ f_{X,Y}(x,y) = f_X(x)f_Y(y) = \\left(a^{2}x\\,\\exp(-a x)\\right) \\left(b\\,\\exp(-b y)\\right) $$\nfor $(x,y) \\in [0,\\infty) \\times [0,\\infty)$, and $0$ otherwise.\nThe expectation is the integral of $xy f_{X,Y}(x,y)$ over $\\mathbb{R}^2$:\n$$ \\mathbb{E}[XY] = \\int_0^\\infty \\int_0^\\infty xy \\, f_{X,Y}(x,y) \\, \\mathrm{d}y \\mathrm{d}x = \\int_0^\\infty \\int_0^\\infty xy (a^2 x \\exp(-ax))(b \\exp(-by)) \\, \\mathrm{d}y \\mathrm{d}x $$\nThe integrand is $h(x,y) = a^2 b x^2 y \\exp(-ax)\\exp(-by)$.\nThe measure spaces are $([0,\\infty), \\mathcal{B}([0,\\infty)), \\lambda)$ for both $x$ and $y$, where $\\lambda$ is the Lebesgue measure. These are $\\sigma$-finite spaces.\nThe integrand $h(x,y)$ is a product of non-negative terms for $x, y \\in [0,\\infty)$, so $h(x,y) \\ge 0$. It is also a continuous function, hence Borel measurable.\nWe can therefore apply **Tonelli's theorem**, which allows us to compute the double integral as an iterated integral:\n$$ \\mathbb{E}[XY] = \\int_0^\\infty \\left( \\int_0^\\infty a^2 b x^2 y \\exp(-ax) \\exp(-by) \\, \\mathrm{d}y \\right) \\mathrm{d}x $$\nWe can separate the variables:\n$$ \\mathbb{E}[XY] = \\int_0^\\infty a^2 x^2 \\exp(-ax) \\left( \\int_0^\\infty b y \\exp(-by) \\, \\mathrm{d}y \\right) \\mathrm{d}x $$\nLet's evaluate the inner integral with respect to $y$:\n$$ I_y = \\int_0^\\infty b y \\exp(-by) \\, \\mathrm{d}y $$\nThis integral is the expectation of an exponential random variable with rate $b$, which is $1/b$. We can verify this using integration by parts: let $u=y$ and $\\mathrm{d}v = b\\exp(-by)\\mathrm{d}y$. Then $\\mathrm{d}u=\\mathrm{d}y$ and $v=-\\exp(-by)$.\n$$ I_y = \\left[-y\\exp(-by)\\right]_0^\\infty - \\int_0^\\infty (-\\exp(-by))\\,\\mathrm{d}y = (0-0) + \\int_0^\\infty \\exp(-by)\\,\\mathrm{d}y = \\left[-\\frac{1}{b}\\exp(-by)\\right]_0^\\infty = 0 - \\left(-\\frac{1}{b}\\right) = \\frac{1}{b} $$\nSubstituting this result back:\n$$ \\mathbb{E}[XY] = \\int_0^\\infty a^2 x^2 \\exp(-ax) \\left(\\frac{1}{b}\\right) \\mathrm{d}x = \\frac{a^2}{b} \\int_0^\\infty x^2 \\exp(-ax) \\, \\mathrm{d}x $$\nNow we evaluate the integral with respect to $x$:\n$$ I_x = \\int_0^\\infty x^2 \\exp(-ax) \\, \\mathrm{d}x $$\nThis is a standard Gamma function integral. Let $u=ax$, so $x=u/a$ and $\\mathrm{d}x = \\mathrm{d}u/a$.\n$$ I_x = \\int_0^\\infty \\left(\\frac{u}{a}\\right)^2 \\exp(-u) \\frac{\\mathrm{d}u}{a} = \\frac{1}{a^3} \\int_0^\\infty u^2 \\exp(-u) \\, \\mathrm{d}u $$\nThe integral is the Gamma function $\\Gamma(3) = 2! = 2$.\nSo, $I_x = \\frac{2}{a^3}$.\nFinally, we compute the expectation:\n$$ \\mathbb{E}[XY] = \\frac{a^2}{b} \\cdot I_x = \\frac{a^2}{b} \\cdot \\frac{2}{a^3} = \\frac{2}{ab} $$\nThe justification for writing the expectation as an iterated integral is Tonelli's theorem, as the integrand $xyf_X(x)f_Y(y)$ is non-negative. Because the resulting integral is finite ($\\frac{2}{ab}  \\infty$), the function $xyf_X(x)f_Y(y)$ is integrable on $[0,\\infty)^2$. This means **Fubini's theorem** also applies and confirms the validity of the calculation.", "answer": "$$\\boxed{\\frac{2}{ab}}$$", "id": "3070770"}, {"introduction": "In both theory and application, we often need to quantify how \"different\" two probability measures are. This practice [@problem_id:3070782] introduces two of the most important tools for this task: the total variation distance and the Kullback-Leibler divergence. By calculating these quantities for the canonical example of two normal distributions with a small shift in mean, you will gain concrete intuition for their meaning and differing sensitivities, a concept crucial for statistical modeling and information theory.", "problem": "Consider the probability measures $P_{0}$ and $P_{\\theta}$ on $\\mathbb{R}$, where $P_{0}$ is the standard normal distribution $N(0,1)$ and $P_{\\theta}$ is the normal distribution $N(\\theta,1)$ with a location shift $\\theta \\in \\mathbb{R}$. In the context of comparing laws of stochastic processes (such as solutions to stochastic differential equations) under parameter perturbations, two foundational notions are the total variation distance and the Kullback–Leibler divergence. Using only the core definitions of probability measures, their densities, and these divergences, compute the exact total variation distance $\\mathrm{TV}(P_{0},P_{\\theta})$ and the exact Kullback–Leibler divergence $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ (from $P_{0}$ to $P_{\\theta}$). Then, derive the leading-order small-$\\theta$ asymptotic term for each of these quantities as $\\theta \\to 0$. Your derivation must start from the definitions\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\big|P_{0}(A)-P_{\\theta}(A)\\big|$$\nand\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta})=\\int_{\\mathbb{R}} p_{0}(x)\\,\\ln\\!\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right)\\,\\mathrm{d}x,$$\nwhere $p_{0}$ and $p_{\\theta}$ are the Lebesgue densities of $P_{0}$ and $P_{\\theta}$, respectively, and $\\mathcal{B}(\\mathbb{R})$ denotes the Borel $\\sigma$-algebra on $\\mathbb{R}$. Express your final answer as a single row matrix containing, in order: the exact $\\mathrm{TV}(P_{0},P_{\\theta})$, the exact $\\mathrm{KL}(P_{0}\\|P_{\\theta})$, the leading-order term of $\\mathrm{TV}(P_{0},P_{\\theta})$ as $\\theta \\to 0$, and the leading-order term of $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ as $\\theta \\to 0$. No rounding is required, and no physical units are involved. Define any special functions you use in your solution, and do not invoke any prepackaged formulas beyond the stated definitions.", "solution": "The problem asks for the computation of the total variation distance $\\mathrm{TV}(P_{0},P_{\\theta})$ and the Kullback–Leibler divergence $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ between two normal distributions, $P_{0} \\sim N(0,1)$ and $P_{\\theta} \\sim N(\\theta,1)$, along with their leading-order asymptotics as $\\theta \\to 0$.\n\nThe Lebesgue densities for $P_0$ and $P_\\theta$ are, respectively:\n$$p_{0}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\n$$p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)$$\n\n**1. Total Variation Distance**\n\nThe total variation distance is defined as:\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\big|P_{0}(A)-P_{\\theta}(A)\\big|$$\nwhere $P(A) = \\int_A p(x)\\,\\mathrm{d}x$. This can be written as:\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\left|\\int_{A} \\left(p_{0}(x) - p_{\\theta}(x)\\right)\\,\\mathrm{d}x\\right|$$\nThe supremum is attained on the set $A^* = \\{x \\in \\mathbb{R} : p_{0}(x) \\ge p_{\\theta}(x)\\}$. On this set, the absolute value is unnecessary. Thus,\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{A^*} \\left(p_{0}(x) - p_{\\theta}(x)\\right)\\,\\mathrm{d}x$$\nThis is also equal to the well-known formula $\\frac{1}{2}\\int_{\\mathbb{R}} |p_0(x) - p_\\theta(x)| \\, \\mathrm{d}x$. Let's proceed from the maximizing set $A^*$.\n\nWe determine the set $A^*$ by solving the inequality $p_{0}(x) \\ge p_{\\theta}(x)$:\n$$\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) \\ge \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)$$\n$$-\\frac{x^2}{2} \\ge -\\frac{(x-\\theta)^2}{2}$$\n$$x^2 \\le (x-\\theta)^2 = x^2 - 2x\\theta + \\theta^2$$\n$$0 \\le -2x\\theta + \\theta^2 \\implies 2x\\theta \\le \\theta^2$$\nIf $\\theta  0$, this simplifies to $x \\le \\frac{\\theta}{2}$. So, $A^* = (-\\infty, \\theta/2]$.\nIf $\\theta  0$, this simplifies to $x \\ge \\frac{\\theta}{2}$. So, $A^* = [\\theta/2, \\infty)$.\nIf $\\theta = 0$, the inequality is $0 \\le 0$, which is true for all $x$, so $p_0(x) = p_\\theta(x)$ and $\\mathrm{TV}=0$.\n\nCase 1: $\\theta  0$.\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{-\\infty}^{\\theta/2} p_{0}(x)\\,\\mathrm{d}x - \\int_{-\\infty}^{\\theta/2} p_{\\theta}(x)\\,\\mathrm{d}x$$\nLet $\\Phi(z) = \\int_{-\\infty}^{z} p_{0}(t)\\,\\mathrm{d}t$ be the cumulative distribution function (CDF) of the standard normal distribution. The first term is simply $\\Phi(\\theta/2)$. For the second term, we change variables with $u = x-\\theta$, $\\mathrm{d}u=\\mathrm{d}x$:\n$$\\int_{-\\infty}^{\\theta/2} p_{\\theta}(x)\\,\\mathrm{d}x = \\int_{-\\infty}^{\\theta/2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)\\,\\mathrm{d}x = \\int_{-\\infty}^{-\\theta/2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)\\,\\mathrm{d}u = \\Phi(-\\theta/2)$$\nSo, for $\\theta  0$, $\\mathrm{TV}(P_{0},P_{\\theta}) = \\Phi(\\theta/2) - \\Phi(-\\theta/2)$.\nUsing the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, this becomes:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\Phi(\\theta/2) - (1 - \\Phi(\\theta/2)) = 2\\Phi(\\theta/2) - 1$$\n\nCase 2: $\\theta  0$. $A^* = [\\theta/2, \\infty)$.\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{\\theta/2}^{\\infty} p_{0}(x)\\,\\mathrm{d}x - \\int_{\\theta/2}^{\\infty} p_{\\theta}(x)\\,\\mathrm{d}x$$\nThis equals $(1 - \\Phi(\\theta/2)) - (1 - \\Phi(\\theta/2 - \\theta)) = \\Phi(-\\theta/2) - \\Phi(\\theta/2)$.\nSince $\\theta  0$, $|\\theta| = -\\theta$. The expression becomes $\\Phi(|\\theta|/2) - \\Phi(-|\\theta|/2) = 2\\Phi(|\\theta|/2) - 1$.\n\nThus, for any $\\theta \\in \\mathbb{R}$, the exact total variation distance is:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = 2\\Phi(|\\theta|/2) - 1$$\n\nTo find the asymptotic behavior as $\\theta \\to 0$, we perform a Taylor expansion of $\\Phi(z)$ around $z=0$:\n$$\\Phi(z) = \\Phi(0) + \\Phi'(0)z + O(z^3)$$\nWe have $\\Phi(0) = 1/2$. The derivative is $\\Phi'(z) = p_{0}(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$.\nSo, $\\Phi'(0) = p_{0}(0) = \\frac{1}{\\sqrt{2\\pi}}$. The second derivative $\\Phi''(z) = p_0'(z) = -z p_0(z)$ is zero at $z=0$, so the next term is of order $z^3$.\n$$\\Phi(z) = \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}z + O(z^3)$$\nSubstituting $z=|\\theta|/2$:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = 2\\left(\\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}\\frac{|\\theta|}{2} + O(|\\theta|^3)\\right) - 1 = 1 + \\frac{|\\theta|}{\\sqrt{2\\pi}} + O(|\\theta|^3) - 1 = \\frac{|\\theta|}{\\sqrt{2\\pi}} + O(|\\theta|^3)$$\nThe leading-order term for $\\mathrm{TV}(P_{0},P_{\\theta})$ as $\\theta \\to 0$ is $\\frac{|\\theta|}{\\sqrt{2\\pi}}$.\n\n**2. Kullback-Leibler Divergence**\n\nThe Kullback-Leibler divergence is defined as:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta})=\\int_{\\mathbb{R}} p_{0}(x)\\,\\ln\\!\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right)\\,\\mathrm{d}x$$\nFirst, we compute the logarithm of the ratio of the densities:\n$$\\frac{p_{0}(x)}{p_{\\theta}(x)} = \\frac{\\exp(-x^2/2)}{\\exp(-(x-\\theta)^2/2)} = \\exp\\left(-\\frac{x^2}{2} + \\frac{(x-\\theta)^2}{2}\\right) = \\exp\\left(-\\frac{x^2}{2} + \\frac{x^2 - 2x\\theta + \\theta^2}{2}\\right)$$\n$$\\frac{p_{0}(x)}{p_{\\theta}(x)} = \\exp\\left(\\frac{-2x\\theta + \\theta^2}{2}\\right) = \\exp\\left(-x\\theta + \\frac{\\theta^2}{2}\\right)$$\nTaking the natural logarithm:\n$$\\ln\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right) = -x\\theta + \\frac{\\theta^2}{2}$$\nNow, we substitute this into the integral definition of KL divergence:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = \\int_{\\mathbb{R}} p_{0}(x) \\left(-x\\theta + \\frac{\\theta^2}{2}\\right)\\,\\mathrm{d}x$$\nWe can split the integral due to linearity:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = -\\theta \\int_{\\mathbb{R}} x p_{0}(x)\\,\\mathrm{d}x + \\frac{\\theta^2}{2} \\int_{\\mathbb{R}} p_{0}(x)\\,\\mathrm{d}x$$\nThe first integral, $\\int_{\\mathbb{R}} x p_{0}(x)\\,\\mathrm{d}x$, is the expected value of a random variable following the standard normal distribution $N(0,1)$, which is $0$.\nThe second integral, $\\int_{\\mathbb{R}} p_{0}(x)\\,\\mathrm{d}x$, is the total probability, which is $1$.\nSubstituting these values:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = -\\theta \\cdot 0 + \\frac{\\theta^2}{2} \\cdot 1 = \\frac{\\theta^2}{2}$$\nThis is the exact expression for the KL divergence.\n\nFor the asymptotic behavior as $\\theta \\to 0$, the exact expression is a polynomial in $\\theta$. The leading-order term is the lowest power term, which is the expression itself.\nThe leading-order term for $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ as $\\theta \\to 0$ is $\\frac{\\theta^2}{2}$.\n\n**Summary of Results**\n- Exact total variation distance: $2\\Phi(|\\theta|/2) - 1$, where $\\Phi$ is the CDF of the standard normal distribution.\n- Exact Kullback-Leibler divergence: $\\frac{\\theta^2}{2}$.\n- Leading-order term of TV distance as $\\theta \\to 0$: $\\frac{|\\theta|}{\\sqrt{2\\pi}}$.\n- Leading-order term of KL divergence as $\\theta \\to 0$: $\\frac{\\theta^2}{2}$.\nThese four results will be presented in the final answer as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2\\Phi(|\\theta|/2) - 1  \\frac{\\theta^2}{2}  \\frac{|\\theta|}{\\sqrt{2\\pi}}  \\frac{\\theta^2}{2}\n\\end{pmatrix}\n}\n$$", "id": "3070782"}, {"introduction": "A stochastic differential equation defines a probability measure not just on $\\mathbb{R}^n$, but on the infinite-dimensional space of continuous paths. A key property of this measure is its support—the set of paths that can actually occur. This practice [@problem_id:3070785] explores how the structure of an SDE can force its solution paths to remain on a geometric manifold, such as a circle, and highlights the crucial distinction between the Itô and Stratonovich integrals in preserving such geometric constraints.", "problem": "Let $T0$. Consider the metric space $C([0,T];\\mathbb{R}^2)$ equipped with the uniform metric $d(f,g)=\\sup_{t\\in[0,T]}\\|f(t)-g(t)\\|$. The support of a probability measure $\\mu$ on a metric space $(E,d)$ is the set of points $x\\in E$ such that every open ball $B(x,\\varepsilon)$ with radius $\\varepsilon0$ has strictly positive $\\mu$-measure. Let $x_0\\in\\mathbb{R}^2$ satisfy $\\|x_0\\|=1$. Let $W$ denote a $1$-dimensional standard Brownian motion and let $J=\\begin{pmatrix}0  -1\\\\ 1  0\\end{pmatrix}$ be the rotation by $90^\\circ$. A diffusion is called degenerate if its diffusion matrix $a(x)=\\sigma(x)\\sigma(x)^\\top$ is singular for all relevant $x$.\n\nWhich of the following stochastic differential equations (SDEs) on $\\mathbb{R}^2$ define a degenerate diffusion starting at $X_0=x_0$ whose induced law on $C([0,T];\\mathbb{R}^2)$ has support contained in the set of continuous paths that remain on the unit circle $S^1=\\{x\\in\\mathbb{R}^2:\\|x\\|=1\\}$ for all $t\\in[0,T]$?\n\nA. Itô SDE: $dX_t=JX_t\\,dW_t$.\n\nB. Stratonovich SDE: $dX_t=JX_t\\circ dW_t$.\n\nC. Itô SDE: $dX_t=-\\tfrac{1}{2}X_t\\,dt+JX_t\\,dW_t$.\n\nD. Reflecting Brownian motion in the closed unit disk: $dX_t=dW^{(2)}_t+n(X_t)\\,dL_t$, where $W^{(2)}$ is a $2$-dimensional standard Brownian motion, $n(x)$ is the inward unit normal at $x$ on the boundary $\\{x:\\|x\\|=1\\}$, and $L_t$ is the boundary local time increasing only when $\\|X_t\\|=1$.", "solution": "The problem asks to identify which of the given stochastic differential equations (SDEs) define a process $X_t$ starting from $X_0=x_0$ on the unit circle $S^1$ that satisfies two conditions:\n1. The diffusion is degenerate.\n2. The support of the induced probability measure on the space of continuous paths $C([0,T];\\mathbb{R}^2)$ is contained in the set of paths that remain on the unit circle, $S^1 = \\{x \\in \\mathbb{R}^2 : \\|x\\|=1\\}$.\n\nWe will analyze each option against the two conditions.\n\n**Condition 1: Degenerate Diffusion**\nA diffusion is defined as degenerate if its diffusion matrix $a(x) = \\sigma(x)\\sigma(x)^\\top$ is singular. The process $X_t$ takes values in $\\mathbb{R}^2$.\n\nFor options A, B, and C, the SDE is driven by a $1$-dimensional standard Brownian motion $W_t$. The general form is $dX_t = b(X_t) dt + \\sigma(X_t) dW_t$, where $\\sigma(x)$ is a $2 \\times 1$ matrix (a column vector). The diffusion matrix is $a(x) = \\sigma(x)\\sigma(x)^\\top$, which is a $2 \\times 2$ matrix. The rank of $a(x)$ is at most the rank of $\\sigma(x)$, which is at most $1$. Any $2 \\times 2$ matrix with rank less than $2$ is singular. Thus, the diffusions defined by SDEs A, B, and C are all degenerate.\n\nFor option D, the SDE is $dX_t=dW^{(2)}_t+n(X_t)\\,dL_t$. The diffusion part is driven by a $2$-dimensional standard Brownian motion $W^{(2)}_t$. This corresponds to a diffusion coefficient matrix $\\sigma(x) = I$, where $I$ is the $2 \\times 2$ identity matrix. The diffusion matrix is $a(x) = \\sigma(x)\\sigma(x)^\\top = I I^\\top = I$. The determinant of the identity matrix is $1$, which is non-zero. Therefore, the diffusion is non-degenerate.\n\nBased on the first condition, option D is incorrect. We proceed to check options A, B, and C against the second condition.\n\n**Condition 2: Support of the Law on the Unit Circle**\nThe set of continuous paths that remain on the unit circle, let's call it $C_{S^1} = \\{\\gamma \\in C([0,T];\\mathbb{R}^2) : \\|\\gamma(t)\\|=1 \\text{ for all } t \\in [0,T]\\}$, is a closed set under the uniform metric. If we can show that for a process $X_t$, the probability of its paths remaining on the circle is $1$, i.e., $\\mathbb{P}(X_\\cdot \\in C_{S^1}) = 1$, then the support of its law must be contained in $C_{S^1}$.\nThis is equivalent to showing that if $\\|X_0\\|=1$, then $\\|X_t\\|=1$ for all $t  0$ almost surely. Let $f(x) = \\|x\\|^2 = x_1^2+x_2^2$. We need to check if $d(f(X_t)) = d(\\|X_t\\|^2) = 0$.\n\nLet's analyze the remaining options. For all these options, we have the matrix $J=\\begin{pmatrix}0  -1\\\\ 1  0\\end{pmatrix}$. A key property is that for any vector $x \\in \\mathbb{R}^2$, the vectors $x$ and $Jx$ are orthogonal, meaning $x^\\top Jx=0$. Also, $\\|Jx\\|^2 = \\|x\\|^2$ since $J$ is an orthogonal matrix.\n\n**Option A: Itô SDE: $dX_t=JX_t\\,dW_t$.**\nThis is an Itô SDE with drift $b(x)=0$ and diffusion coefficient $\\sigma(x)=Jx$. We use Itô's formula for $f(x)=\\|x\\|^2$. The gradient is $\\nabla f(x) = 2x$ and the Hessian is $H_f(x) = 2I$.\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top dX_t + \\frac{1}{2}\\text{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^\\top H_f(X_t)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = (2X_t^\\top)(JX_t\\,dW_t) + \\frac{1}{2}\\text{Tr}\\left((JX_t)(JX_t)^\\top(2I)\\right)\\,dt$$\nThe first term is $2(X_t^\\top JX_t)dW_t = 0 \\cdot dW_t = 0$.\nThe second term is $\\text{Tr}((JX_t)(JX_t)^\\top)\\,dt = \\|JX_t\\|^2\\,dt = \\|X_t\\|^2\\,dt$.\nSo, we have $d\\|X_t\\|^2 = \\|X_t\\|^2\\,dt$.\nGiven the initial condition $\\|X_0\\|^2=1$, this ordinary differential equation for $\\|X_t\\|^2$ has the solution $\\|X_t\\|^2=e^t$. The norm of the process grows exponentially, so the paths do not stay on the unit circle.\n**Verdict for A: Incorrect.**\n\n**Option B: Stratonovich SDE: $dX_t=JX_t\\circ dW_t$.**\nFor a Stratonovich SDE, we can use the Stratonovich chain rule, which follows the rules of ordinary calculus. For $f(x)=\\|x\\|^2$:\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top \\circ dX_t$$\n$$d\\|X_t\\|^2 = 2X_t^\\top \\circ (JX_t \\circ dW_t) = (2X_t^\\top JX_t) \\circ dW_t$$\nAs shown before, $X_t^\\top JX_t=0$ for any $X_t \\in \\mathbb{R}^2$. Thus, the process $2X_t^\\top JX_t$ is identically zero.\n$$d\\|X_t\\|^2 = 0 \\circ dW_t = 0$$\nThis means $\\|X_t\\|^2$ is a conserved quantity. Since $\\|X_0\\|^2=1$, it follows that $\\|X_t\\|^2=1$ for all $t$. The process remains on the unit circle. Since it also defines a degenerate diffusion, option B satisfies both conditions.\n**Verdict for B: Correct.**\n\n**Option C: Itô SDE: $dX_t=-\\tfrac{1}{2}X_t\\,dt+JX_t\\,dW_t$.**\nThis is an Itô SDE with drift $b(x)=-\\frac{1}{2}x$ and diffusion $\\sigma(x)=Jx$. We apply Itô's formula for $f(x)=\\|x\\|^2$:\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top dX_t + \\frac{1}{2}\\text{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^\\top H_f(X_t)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = (2X_t^\\top)\\left(-\\frac{1}{2}X_t\\,dt+JX_t\\,dW_t\\right) + \\frac{1}{2}\\text{Tr}\\left((JX_t)(JX_t)^\\top(2I)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = -X_t^\\top X_t\\,dt + 2X_t^\\top JX_t\\,dW_t + \\|JX_t\\|^2\\,dt$$\nUsing $X_t^\\top X_t = \\|X_t\\|^2$, $X_t^\\top JX_t = 0$, and $\\|JX_t\\|^2 = \\|X_t\\|^2$, this simplifies to:\n$$d\\|X_t\\|^2 = -\\|X_t\\|^2\\,dt + 0\\cdot dW_t + \\|X_t\\|^2\\,dt = 0$$\nThus, $\\|X_t\\|^2$ is conserved. With $\\|X_0\\|^2=1$, all paths remain on the unit circle. The diffusion is degenerate. Option C satisfies both conditions. (Note: The SDE in C is the Itô representation of the Stratonovich SDE in B, so they describe the same process.)\n**Verdict for C: Correct.**\n\n**Option D: Reflecting Brownian motion in the closed unit disk.**\nAs determined earlier, this SDE defines a non-degenerate diffusion, so it fails the first condition. Furthermore, the process explores the interior of the unit disk, not just its boundary. A path of a reflecting Brownian motion in a disk is not confined to the boundary circle for all $t \\in [0,T]$. So it fails the second condition as well.\n**Verdict for D: Incorrect.**\n\nIn summary, options B and C both describe a degenerate diffusion whose paths, starting on the unit circle, remain on the unit circle for all time.", "answer": "$$\\boxed{BC}$$", "id": "3070785"}]}