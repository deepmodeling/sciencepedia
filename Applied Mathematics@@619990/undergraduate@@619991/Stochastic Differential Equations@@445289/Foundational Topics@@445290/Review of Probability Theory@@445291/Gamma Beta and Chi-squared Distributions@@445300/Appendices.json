{"hands_on_practices": [{"introduction": "The gamma distribution is a cornerstone for modeling continuous positive random variables, such as waiting times in stochastic processes. A key step in defining any probability distribution is ensuring its density function integrates to one. This foundational exercise guides you through the essential process of normalizing the gamma density, demonstrating how the Euler gamma function, $\\Gamma(k)$, arises naturally from this requirement through a simple change of variables. Mastering this calculation is crucial for understanding how probability distributions are constructed and validated from first principles. [@problem_id:3056353]", "problem": "In the study of one-dimensional Itô diffusions, such as the Cox–Ingersoll–Ross square-root process, candidate stationary densities often take the form of a kernel proportional to $x^{k-1}\\exp(-x/\\theta)$ on $(0,\\infty)$, where $k0$ and $\\theta0$. To check normalizability from first principles, consider the integral\n$$\nI(k,\\theta)=\\int_{0}^{\\infty} x^{k-1}\\exp\\!\\left(-\\frac{x}{\\theta}\\right)\\,dx,\n$$\nwith parameters $k0$ and $\\theta0$.\n\nUsing only the definition of the Gamma function $\\Gamma(k)=\\int_{0}^{\\infty} u^{\\,k-1}\\exp(-u)\\,du$ for $k0$ and the change-of-variables rule for integrals, evaluate $I(k,\\theta)$ in closed form as a function of $k$ and $\\theta$. Your final answer must be a single analytic expression. No numerical rounding is required.", "solution": "The problem requires the evaluation of the integral\n$$\nI(k,\\theta)=\\int_{0}^{\\infty} x^{k-1}\\exp\\left(-\\frac{x}{\\theta}\\right)\\,dx\n$$\nfor parameters $k0$ and $\\theta0$. The evaluation must be performed using the definition of the Gamma function, $\\Gamma(k)$, and the change-of-variables rule for integrals.\n\nFirst, we state the provided definition of the Gamma function for $k0$:\n$$\n\\Gamma(k)=\\int_{0}^{\\infty} u^{k-1}\\exp(-u)\\,du\n$$\n\nThe task is to transform the integral $I(k,\\theta)$ into a form that allows the direct application of this definition. We compare the integrand of $I(k,\\theta)$, which is $x^{k-1}\\exp(-x/\\theta)$, with the integrand of $\\Gamma(k)$, which is $u^{k-1}\\exp(-u)$. The key difference is in the argument of the exponential function. In $I(k,\\theta)$ it is $-x/\\theta$, while in $\\Gamma(k)$ it is $-u$. This observation motivates a change of variable.\n\nLet us define a new variable of integration, $u$, such that the exponential terms match. We set:\n$$\nu = \\frac{x}{\\theta}\n$$\nFrom this substitution, we solve for $x$ in terms of $u$:\n$$\nx = \\theta u\n$$\nTo complete the change of variables, we need to find the differential $dx$ in terms of $du$. Differentiating the expression for $x$ with respect to $u$, we get:\n$$\n\\frac{dx}{du} = \\theta\n$$\nwhich implies that the differential element transforms as:\n$$\ndx = \\theta \\, du\n$$\nNext, we must transform the limits of integration. The original integral is over the interval $x \\in (0, \\infty)$.\nFor the lower limit, when $x=0$, the new variable is $u = \\frac{0}{\\theta} = 0$.\nFor the upper limit, as $x \\to \\infty$, the new variable $u = \\frac{x}{\\theta}$ also approaches $\\infty$, since we are given that $\\theta  0$.\nThus, the limits of integration in the variable $u$ remain from $0$ to $\\infty$.\n\nNow, we substitute $x = \\theta u$ and $dx = \\theta \\, du$ into the integral for $I(k,\\theta)$:\n$$\nI(k,\\theta) = \\int_{0}^{\\infty} (\\theta u)^{k-1} \\exp\\left(-\\frac{\\theta u}{\\theta}\\right) (\\theta \\, du)\n$$\nWe simplify the expression within the integral. The term $(\\theta u)^{k-1}$ becomes $\\theta^{k-1} u^{k-1}$, and the argument of the exponential simplifies to $-u$.\n$$\nI(k,\\theta) = \\int_{0}^{\\infty} \\theta^{k-1} u^{k-1} \\exp(-u) \\, \\theta \\, du\n$$\nWe can combine the factors of $\\theta$ using the properties of exponents:\n$$\nI(k,\\theta) = \\int_{0}^{\\infty} \\theta^{k-1+1} u^{k-1} \\exp(-u) \\, du = \\int_{0}^{\\infty} \\theta^k u^{k-1} \\exp(-u) \\, du\n$$\nThe term $\\theta^k$ does not depend on the variable of integration $u$, so it can be factored out of the integral:\n$$\nI(k,\\theta) = \\theta^k \\int_{0}^{\\infty} u^{k-1} \\exp(-u) \\, du\n$$\nThe remaining integral is now identical to the definition of the Gamma function provided in the problem statement:\n$$\n\\int_{0}^{\\infty} u^{k-1} \\exp(-u) \\, du = \\Gamma(k)\n$$\nBy substituting this definition into our expression for $I(k, \\theta)$, we obtain the final closed-form result:\n$$\nI(k,\\theta) = \\theta^k \\Gamma(k)\n$$\nThis result is valid for all $k0$ and $\\theta0$, the domain for which the original integral and the Gamma function are defined as specified. The problem is thus solved as required.", "answer": "$$\\boxed{\\theta^k \\Gamma(k)}$$", "id": "3056353"}, {"introduction": "For a large shape parameter, the gamma distribution can be effectively approximated by the more tractable normal distribution. This connection is not a coincidence but a direct consequence of the powerful Central Limit Theorem, given that a gamma-distributed variable can be represented as a sum of independent exponential variables. This exercise challenges you to formalize this relationship by deriving the normal approximation and to quantify its accuracy using a Berry-Esseen-type inequality. This practice is crucial for understanding the asymptotic behavior of distributions and their practical application in statistical modeling. [@problem_id:3056367]", "problem": "Consider a sequence of independent and identically distributed random variables $\\{Y_{i}\\}_{i=1}^{k}$ where each $Y_{i}$ has the exponential distribution with rate $1/\\theta$ (so $\\mathbb{E}[Y_{i}]=\\theta$ and $\\operatorname{Var}(Y_{i})=\\theta^{2}$), and let $X_{k}=\\sum_{i=1}^{k}Y_{i}$. It is a well-tested fact that $X_{k}$ then has the gamma distribution with shape $k$ and scale $\\theta$, denoted $X_{k}\\sim\\Gamma(k,\\theta)$. Define the standardized sum\n$$\nZ_{k}=\\frac{X_{k}-k\\theta}{\\theta\\sqrt{k}}.\n$$\nStarting from the Central Limit Theorem (CLT) and fully justifying each step from fundamental definitions, do the following:\n- Derive the normal approximation for $Z_{k}$ as $k$ becomes large, explaining the roles of $\\mathbb{E}[Y_{i}]$ and $\\operatorname{Var}(Y_{i}]$.\n- Compute the third absolute centered moment $\\rho=\\mathbb{E}\\big[|Y_{1}-\\theta|^{3}\\big]$ in closed form, showing all integral steps.\n- Use a Berry–Esseen-type inequality (you may take the universal Berry–Esseen constant to be $C_{\\mathrm{BE}}=0.56$) to obtain an explicit upper bound on the Kolmogorov distance $\\sup_{x\\in\\mathbb{R}}\\big|\\mathbb{P}(Z_{k}\\le x)-\\Phi(x)\\big|$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nFinally, evaluate this explicit bound at $k=100$ and report its numerical value. Round your answer to four significant figures. The final answer must be a single real number.", "solution": "The solution is presented in three parts, as requested by the problem statement.\n\n**Part 1: Normal Approximation via the Central Limit Theorem**\n\nThe Central Limit Theorem (CLT) states that for a sequence of independent and identically distributed (i.i.d.) random variables $\\{Y_i\\}_{i=1}^k$, each with a finite mean $\\mu = \\mathbb{E}[Y_i]$ and a finite non-zero variance $\\sigma^2 = \\operatorname{Var}(Y_i)$, the distribution of the standardized sample mean converges to a standard normal distribution as the sample size $k$ approaches infinity. More formally, if $X_k = \\sum_{i=1}^k Y_i$, then the standardized sum\n$$ \\frac{X_k - \\mathbb{E}[X_k]}{\\sqrt{\\operatorname{Var}(X_k)}} \\xrightarrow{d} N(0,1) \\quad \\text{as } k \\to \\infty $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution.\n\nIn this problem, the random variables are $\\{Y_i\\}_{i=1}^k$, which are i.i.d. exponential with:\n- Mean: $\\mu = \\mathbb{E}[Y_i] = \\theta$\n- Variance: $\\sigma^2 = \\operatorname{Var}(Y_i) = \\theta^2$\n\nThe sum of these variables is $X_k = \\sum_{i=1}^k Y_i$. Due to the linearity of expectation and the independence of the variables, the mean and variance of $X_k$ are:\n- Mean of the sum: $\\mathbb{E}[X_k] = \\mathbb{E}\\left[\\sum_{i=1}^k Y_i\\right] = \\sum_{i=1}^k \\mathbb{E}[Y_i] = k\\mu = k\\theta$.\n- Variance of the sum: $\\operatorname{Var}(X_k) = \\operatorname{Var}\\left(\\sum_{i=1}^k Y_i\\right) = \\sum_{i=1}^k \\operatorname{Var}(Y_i) = k\\sigma^2 = k\\theta^2$.\n\nThe standard deviation of the sum is $\\sqrt{\\operatorname{Var}(X_k)} = \\sqrt{k\\theta^2} = \\theta\\sqrt{k}$.\n\nThe standardized sum is formed by subtracting the mean and dividing by the standard deviation:\n$$ \\frac{X_k - \\mathbb{E}[X_k]}{\\sqrt{\\operatorname{Var}(X_k)}} = \\frac{X_k - k\\theta}{\\theta\\sqrt{k}} $$\nThis is precisely the variable $Z_k$ defined in the problem.\n\nTherefore, by the Central Limit Theorem, as $k$ becomes large, the distribution of $Z_k = \\frac{X_k - k\\theta}{\\theta\\sqrt{k}}$ is approximated by the standard normal distribution, $N(0,1)$.\nThe role of $\\mathbb{E}[Y_i] = \\theta$ is to determine the mean of the sum $X_k$, which is used to center the variable $X_k$. The role of $\\operatorname{Var}(Y_i) = \\theta^2$ is to determine the standard deviation of the sum, which is used to scale the centered variable $X_k-k\\theta$. This centering and scaling ensures that the resulting variable $Z_k$ has a mean of $0$ and a variance of $1$ for any $k$, and its distribution converges to the standard normal distribution.\n\n**Part 2: Computation of the Third Absolute Centered Moment**\n\nWe are asked to compute $\\rho = \\mathbb{E}[|Y_1 - \\theta|^3]$. The random variable $Y_1$ follows an exponential distribution with rate $1/\\theta$, so its probability density function (PDF) is $f(y) = \\frac{1}{\\theta}\\exp(-y/\\theta)$ for $y \\ge 0$.\n\nThe expectation is given by the integral:\n$$ \\rho = \\int_0^\\infty |y - \\theta|^3 f(y) dy = \\int_0^\\infty |y - \\theta|^3 \\frac{1}{\\theta}\\exp(-y/\\theta) dy $$\nWe split the integral at the point $y=\\theta$ where the absolute value function changes its form:\n$$ \\rho = \\frac{1}{\\theta} \\left[ \\int_0^\\theta (\\theta - y)^3 \\exp(-y/\\theta) dy + \\int_\\theta^\\infty (y - \\theta)^3 \\exp(-y/\\theta) dy \\right] $$\nLet's evaluate each integral separately.\n\nFor the first integral, let $u = \\theta - y$. This implies $y = \\theta - u$ and $dy = -du$. The limits of integration change from $y=0$ to $u=\\theta$, and $y=\\theta$ to $u=0$.\n$$ \\int_0^\\theta (\\theta - y)^3 \\exp(-y/\\theta) dy = \\int_\\theta^0 u^3 \\exp\\left(-\\frac{\\theta-u}{\\theta}\\right) (-du) = \\int_0^\\theta u^3 \\exp(-1 + u/\\theta) du = \\exp(-1) \\int_0^\\theta u^3 \\exp(u/\\theta) du $$\nWe use integration by parts. The indefinite integral is:\n$$ \\int u^3 \\exp(u/\\theta) du = \\exp(u/\\theta) (\\theta u^3 - 3\\theta^2 u^2 + 6\\theta^3 u - 6\\theta^4) $$\nEvaluating this from $u=0$ to $u=\\theta$:\n$$ \\left[ \\exp(u/\\theta) (\\theta u^3 - 3\\theta^2 u^2 + 6\\theta^3 u - 6\\theta^4) \\right]_0^\\theta $$\n$$ = \\exp(1) (\\theta \\cdot \\theta^3 - 3\\theta^2 \\cdot \\theta^2 + 6\\theta^3 \\cdot \\theta - 6\\theta^4) - \\exp(0) (0 - 0 + 0 - 6\\theta^4) $$\n$$ = \\exp(1) (\\theta^4 - 3\\theta^4 + 6\\theta^4 - 6\\theta^4) - (-6\\theta^4) = -2\\theta^4\\exp(1) + 6\\theta^4 = (6 - 2\\exp(1))\\theta^4 $$\nSo the first part of $\\rho$ is $\\frac{1}{\\theta} \\exp(-1) \\left( (6 - 2\\exp(1))\\theta^4 \\right) = (6\\exp(-1) - 2)\\theta^3$.\n\nFor the second integral, let $v = y - \\theta$. This implies $y = \\theta + v$ and $dy = dv$. The limits change from $y=\\theta$ to $v=0$, and $y \\to \\infty$ to $v \\to \\infty$.\n$$ \\int_\\theta^\\infty (y - \\theta)^3 \\exp(-y/\\theta) dy = \\int_0^\\infty v^3 \\exp\\left(-\\frac{\\theta+v}{\\theta}\\right) dv = \\int_0^\\infty v^3 \\exp(-1 - v/\\theta) dv = \\exp(-1) \\int_0^\\infty v^3 \\exp(-v/\\theta) dv $$\nLet $w = v/\\theta$, so $v=w\\theta$ and $dv = \\theta dw$.\n$$ \\exp(-1) \\int_0^\\infty (w\\theta)^3 \\exp(-w) (\\theta dw) = \\exp(-1)\\theta^4 \\int_0^\\infty w^3 \\exp(-w) dw $$\nThe integral is the Gamma function $\\Gamma(4) = (4-1)! = 3! = 6$.\nThus, the second integral evaluates to $\\exp(-1)\\theta^4 \\cdot 6 = 6\\exp(-1)\\theta^4$.\nThe second part of $\\rho$ is $\\frac{1}{\\theta} \\left( 6\\exp(-1)\\theta^4 \\right) = 6\\exp(-1)\\theta^3$.\n\nCombining both parts, we find $\\rho$:\n$$ \\rho = (6\\exp(-1) - 2)\\theta^3 + 6\\exp(-1)\\theta^3 = (12\\exp(-1) - 2)\\theta^3 $$\n\n**Part 3: Berry–Esseen Bound and Numerical Evaluation**\n\nThe Berry–Esseen inequality provides an upper bound on the maximum difference between the CDF of a standardized sum and the CDF of the standard normal distribution. For $Z_k = \\frac{\\sum_{i=1}^k Y_i - k\\mu}{\\sigma\\sqrt{k}}$, the inequality is:\n$$ \\sup_{x\\in\\mathbb{R}}\\big|\\mathbb{P}(Z_{k}\\le x)-\\Phi(x)\\big| \\le C \\frac{\\mathbb{E}[|Y_1 - \\mu|^3]}{\\sigma^3 \\sqrt{k}} $$\nIn our problem, we have:\n- The Berry–Esseen constant $C = C_{\\mathrm{BE}} = 0.56$.\n- The mean of $Y_i$ is $\\mu = \\theta$.\n- The standard deviation of $Y_i$ is $\\sigma = \\sqrt{\\theta^2} = \\theta$.\n- The third absolute centered moment is $\\rho = \\mathbb{E}[|Y_1 - \\theta|^3] = (12\\exp(-1) - 2)\\theta^3$.\n\nSubstituting these into the inequality:\n$$ \\sup_{x\\in\\mathbb{R}}\\big|\\mathbb{P}(Z_{k}\\le x)-\\Phi(x)\\big| \\le 0.56 \\cdot \\frac{(12\\exp(-1) - 2)\\theta^3}{(\\theta)^3 \\sqrt{k}} $$\nThe $\\theta^3$ terms cancel, yielding a bound that is independent of the scale parameter $\\theta$:\n$$ \\text{Bound} = \\frac{0.56(12\\exp(-1) - 2)}{\\sqrt{k}} $$\nWe are asked to evaluate this bound for $k=100$.\n$$ \\text{Bound}_{k=100} = \\frac{0.56(12\\exp(-1) - 2)}{\\sqrt{100}} = \\frac{0.56(12\\exp(-1) - 2)}{10} = 0.056(12\\exp(-1) - 2) $$\nNow, we compute the numerical value:\n$$ 12\\exp(-1) - 2 \\approx 12(0.36787944) - 2 = 4.41455328 - 2 = 2.41455328 $$\n$$ \\text{Bound}_{k=100} \\approx 0.056 \\times 2.41455328 \\approx 0.13521498 $$\nRounding the result to four significant figures gives $0.1352$.", "answer": "$$\n\\boxed{0.1352}\n$$", "id": "3056367"}, {"introduction": "Bridging theory and practice, this exercise focuses on simulating the chi-squared distribution, an important special case of the gamma distribution where $\\chi^2_{\\nu} \\sim \\Gamma(\\nu/2, 2)$. The simulation is built from its constructive definition: the sum of squares of $\\nu$ independent standard normal variables, a structure that appears frequently in statistics and the study of stochastic processes. This hands-on problem solidifies the theoretical link between the normal and chi-squared distributions while introducing practical challenges in computational science, such as the numerical stability of summation algorithms. [@problem_id:3056438]", "problem": "You are asked to connect a core construction from stochastic differential equations to classical continuous distributions, and then to implement a numerically robust simulator. In the Euler–Maruyama discretization of an Itô stochastic differential equation driven by a standard Brownian motion, independent standardized Gaussian increments arise naturally. Consider independent standardized increments $\\{Z_k\\}_{k=1}^{\\nu}$ with $Z_k \\sim \\mathcal{N}(0,1)$, and note that sums of squares $\\sum_{k=1}^{\\nu} Z_k^2$ appear in estimators of quadratic variation and related likelihoods. Work from first principles of independence and the standard normal distribution to determine the distribution of $\\sum_{k=1}^{\\nu} Z_k^2$, and deduce its mean and variance from well-tested facts about the gamma distribution and its stability under addition. Then implement a simulation that uses only the definition in terms of independent standard normals (no direct calls to chi-squared or gamma samplers for the main simulation), and quantify numerical stability of different summation strategies for large degree-of-freedom parameters.\n\nYour program must do the following for each test case with parameters $(\\nu, M, s)$:\n- Use a pseudorandom number generator initialized with seed $s$ to simulate $M$ independent draws of the sum $\\sum_{k=1}^{\\nu} Z_k^2$ by explicitly generating $\\nu$ independent $\\mathcal{N}(0,1)$ variables for each draw and summing their squares.\n- Compute the sample mean $\\widehat{\\mu}$ and the unbiased sample variance $\\widehat{\\sigma}^2$ across the $M$ draws, and then compute the absolute mean error $|\\widehat{\\mu} - \\nu|$ and the absolute variance error $|\\widehat{\\sigma}^2 - 2\\nu|$. These should be reported as floating-point numbers.\n- To assess numerical stability of summation for large $\\nu$, using a fresh pseudorandom number generator initialized with seed $s + 1337$, draw a single vector $(Z_1,\\dots,Z_{\\nu})$ with $Z_k \\sim \\mathcal{N}(0,1)$, and compute the same sum of squares using three methods:\n  1. Naïve Python accumulation $S_{\\text{naive}} = \\sum_{k=1}^{\\nu} Z_k^2$ via a plain loop,\n  2. Compensated Kahan summation $S_{\\text{kahan}}$ applied to the sequence $\\{Z_k^2\\}_{k=1}^{\\nu}$,\n  3. Vectorized pairwise summation $S_{\\text{np}}$ using a high-level vectorized dot-product.\n  From these, compute two relative discrepancies (dimensionless floats): $r_1 = |S_{\\text{naive}} - S_{\\text{kahan}}| / S_{\\text{kahan}}$ and $r_2 = |S_{\\text{np}} - S_{\\text{kahan}}| / S_{\\text{kahan}}$. If $S_{\\text{kahan}} = 0$, define both relative discrepancies to be $0$.\n- For each test case, output four floats, in the fixed order: absolute mean error, absolute variance error, $r_1$, $r_2$.\n\nThe fundamental bases you may use in your derivation are:\n- The definition of the standard normal distribution $\\mathcal{N}(0,1)$, independence, and additivity of independent increments in Brownian motion.\n- The well-tested facts that if $X \\sim \\operatorname{Gamma}(k,\\theta)$ and $Y \\sim \\operatorname{Gamma}(k',\\theta)$ are independent with common scale $\\theta$, then $X+Y \\sim \\operatorname{Gamma}(k+k',\\theta)$, and that $\\operatorname{Gamma}(k,\\theta)$ has mean $k\\theta$ and variance $k\\theta^2$.\n- Basic properties of numerical floating-point summation and the Kahan compensated summation algorithm.\n\nEdge cases and numerical stability considerations must be addressed. In particular, for large $\\nu$ the summation of many small numbers can accumulate rounding error, and for $\\nu = 0$ the distribution degenerates at $0$.\n\nPhysical units are not involved. All angles, if any, are in radians by default (not used in this problem). When expressing proportions, always use decimal numbers.\n\nTest Suite:\n- Case A (boundary): $(\\nu, M, s) = (0, 1000, 1234)$.\n- Case B (happy path): $(\\nu, M, s) = (10, 100000, 202311)$.\n- Case C (large-$\\nu$ stability): $(\\nu, M, s) = (200000, 200, 7)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate the four floats from Case A, then the four floats from Case B, then the four floats from Case C, all in that order. For example, the output must look like\n$[\\text{A\\_mean\\_err},\\text{A\\_var\\_err},\\text{A\\_r1},\\text{A\\_r2},\\text{B\\_mean\\_err},\\dots,\\text{C\\_r2}]$.", "solution": "**Theoretical Foundation**\n\nThe problem requires us to determine the distribution of the sum of squares of $\\nu$ independent standard normal random variables, $S_{\\nu} = \\sum_{k=1}^{\\nu} Z_k^2$, where each $Z_k \\sim \\mathcal{N}(0,1)$ are independent. We will derive this distribution and its mean and variance from first principles and the provided properties of the gamma distribution.\n\n**Step 1: Distribution of a single squared standard normal variable**\n\nLet $Z \\sim \\mathcal{N}(0,1)$. The probability density function (PDF) of $Z$ is $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$ for $z \\in (-\\infty, \\infty)$. We seek the distribution of $Y = Z^2$. Since $Y$ must be non-negative, we consider $y \\geq 0$.\n\nThe cumulative distribution function (CDF) of $Y$ is $F_Y(y) = P(Y \\leq y) = P(Z^2 \\leq y) = P(-\\sqrt{y} \\leq Z \\leq \\sqrt{y})$.\n$$F_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz$$\nWe can find the PDF of $Y$ by differentiating its CDF with respect to $y$, using the Leibniz integral rule:\n$$f_Y(y) = \\frac{d}{dy} F_Y(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-(\\sqrt{y})^2/2} \\cdot \\frac{d}{dy}(\\sqrt{y}) - \\left(\\frac{1}{\\sqrt{2\\pi}} e^{-(-\\sqrt{y})^2/2} \\cdot \\frac{d}{dy}(-\\sqrt{y})\\right)$$\n$$f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-y/2} \\left(\\frac{1}{2\\sqrt{y}}\\right) - \\frac{1}{\\sqrt{2\\pi}} e^{-y/2} \\left(-\\frac{1}{2\\sqrt{y}}\\right) = 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-y/2} \\left(\\frac{1}{2\\sqrt{y}}\\right) = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}$$\nThis is the PDF for $y  0$.\n\nNow, we compare this to the PDF of a $\\operatorname{Gamma}(k, \\theta)$ distribution, which is given by $f(x; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} e^{-x/\\theta}$.\nThe problem states that the chi-squared distribution with $\\nu$ degrees of freedom, $\\chi^2(\\nu)$, is a special case of the gamma distribution, specifically $\\operatorname{Gamma}(k=\\nu/2, \\theta=2)$. For one degree of freedom ($\\nu=1$), this corresponds to $\\operatorname{Gamma}(k=1/2, \\theta=2)$.\nLet's substitute $k=1/2$ and $\\theta=2$ into the gamma PDF formula. Knowing that $\\Gamma(1/2) = \\sqrt{\\pi}$:\n$$f(y; 1/2, 2) = \\frac{1}{\\Gamma(1/2) 2^{1/2}} y^{1/2-1} e^{-y/2} = \\frac{1}{\\sqrt{\\pi} \\sqrt{2}} y^{-1/2} e^{-y/2} = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}$$\nThis confirms that the PDF of $Y=Z^2$ is indeed that of a $\\operatorname{Gamma}(1/2, 2)$ distribution. This is also known as the chi-squared distribution with one degree of freedom, $\\chi^2(1)$.\n\n**Step 2: Distribution of the sum of squares**\n\nWe are interested in the sum $S_{\\nu} = \\sum_{k=1}^{\\nu} Z_k^2$. The random variables $Z_k$ are independent, which implies that the variables $Y_k = Z_k^2$ are also independent. Each $Y_k \\sim \\operatorname{Gamma}(1/2, 2)$.\n\nThe problem provides a key property of the gamma distribution: if $X_1 \\sim \\operatorname{Gamma}(k_1, \\theta)$ and $X_2 \\sim \\operatorname{Gamma}(k_2, \\theta)$ are independent, then their sum $X_1+X_2 \\sim \\operatorname{Gamma}(k_1+k_2, \\theta)$. This additivity property extends to the sum of any number of independent gamma variables with a common scale parameter $\\theta$.\n\nApplying this property to our sum $S_{\\nu}$:\n$$S_{\\nu} = \\sum_{k=1}^{\\nu} Y_k \\sim \\operatorname{Gamma}\\left(\\sum_{k=1}^{\\nu} \\frac{1}{2}, 2\\right) = \\operatorname{Gamma}\\left(\\frac{\\nu}{2}, 2\\right)$$\nThus, the sum of squares of $\\nu$ independent standard normal variables follows a gamma distribution with shape parameter $k=\\nu/2$ and scale parameter $\\theta=2$. This is, by definition, the chi-squared distribution with $\\nu$ degrees of freedom, $\\chi^2(\\nu)$.\n\n**Step 3: Mean and Variance**\n\nThe problem states that for a random variable $X \\sim \\operatorname{Gamma}(k, \\theta)$, its mean is $\\mathbb{E}[X] = k\\theta$ and its variance is $\\operatorname{Var}(X) = k\\theta^2$.\nFor our variable $S_{\\nu} \\sim \\operatorname{Gamma}(\\nu/2, 2)$, we have $k = \\nu/2$ and $\\theta=2$. Therefore:\n- Mean: $\\mathbb{E}[S_{\\nu}] = \\left(\\frac{\\nu}{2}\\right) \\cdot 2 = \\nu$.\n- Variance: $\\operatorname{Var}(S_{\\nu}) = \\left(\\frac{\\nu}{2}\\right) \\cdot 2^2 = \\frac{\\nu}{2} \\cdot 4 = 2\\nu$.\n\nThese theoretical results, $\\mathbb{E}[S_{\\nu}]=\\nu$ and $\\operatorname{Var}(S_{\\nu})=2\\nu$, provide the ground truth against which the sample statistics from the simulation will be compared to calculate the absolute errors. For the boundary case $\\nu=0$, the sum is empty and defined as $0$. The distribution is degenerate at $0$, and our formulas correctly yield a mean of $0$ and a variance of $0$.\n\n**Implementation Strategy**\n\nThe implementation will be divided into two main parts for each test case $(\\nu, M, s)$.\n\n**Part A: Statistical Estimation**\nA pseudorandom number generator (PRNG) is initialized with the given seed $s$. We will generate $M$ independent draws of the sum $S_{\\nu}$. For computational efficiency when $\\nu  0$, this is best achieved by generating an $M \\times \\nu$ matrix of $\\mathcal{N}(0,1)$ samples, squaring each element, and then summing along the rows to obtain $M$ values of $S_{\\nu}$. From this sample of $M$ values, we compute the sample mean $\\widehat{\\mu}$ and the unbiased sample variance $\\widehat{\\sigma}^2$ (using a denominator of $M-1$). The absolute errors $|\\widehat{\\mu} - \\nu|$ and $|\\widehat{\\sigma}^2 - 2\\nu|$ are then calculated. For $\\nu=0$, all sums are $0$, so the sample mean and variance are $0$, and the errors are also $0$.\n\n**Part B: Numerical Stability Assessment**\nA second, independent PRNG is initialized with seed $s+1337$. A single vector of $\\nu$ standard normal samples is generated. The sum of the squares of these samples is computed using three different algorithms to highlight the effects of floating-point error accumulation, which is particularly relevant for large $\\nu$.\n1.  **Naïve Summation ($S_{\\text{naive}}$):** A standard `for` loop in Python accumulates the sum. This method is susceptible to precision loss when adding a small number to a much larger running total.\n2.  **Kahan Summation ($S_{\\text{kahan}}$):** The Kahan compensated summation algorithm maintains a running compensation term to track and correct for the low-order bits lost in each addition. This provides a highly accurate \"ground truth\" for the sum.\n3.  **Vectorized Summation ($S_{\\text{np}}$):** Using `numpy.dot(v, v)` on the vector of samples $v$ computes the sum of squares. NumPy's underlying BLAS/LAPACK libraries often employ numerically stable techniques like pairwise summation or use extended precision accumulators, making this method far more accurate than naïve summation.\n\nThe relative discrepancies $r_1 = |S_{\\text{naive}} - S_{\\text{kahan}}| / S_{\\text{kahan}}$ and $r_2 = |S_{\\text{np}} - S_{\\text{kahan}}| / S_{\\text{kahan}}$ are then calculated. A special case handles $S_{\\text{kahan}}=0$ to prevent division by zero, setting both discrepancies to $0$. This occurs if and only if $\\nu=0$.\n\nThe final code will encapsulate this logic, iterating through the provided test cases and formatting the results as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (nu, M, s)\n        (0, 1000, 1234),          # Case A (boundary)\n        (10, 100000, 202311),      # Case B (happy path)\n        (200000, 200, 7),         # Case C (large-nu stability)\n    ]\n\n    all_results = []\n    for nu, M, s in test_cases:\n        results_for_case = process_case(nu, M, s)\n        all_results.extend(results_for_case)\n\n    # Format output as a single line: [res1,res2,res3,...]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef process_case(nu, M, s):\n    \"\"\"\n    Process a single test case (nu, M, s) and return a list of the four\n    required float values: [mean_err, var_err, r1, r2].\n    \"\"\"\n\n    # --- Part A: Statistical Estimation ---\n    \n    true_mean = float(nu)\n    true_variance = 2.0 * nu\n    \n    if nu == 0:\n        # Trivial case: sum is always 0.\n        mean_err = 0.0\n        var_err = 0.0\n    else:\n        rng_A = np.random.default_rng(seed=s)\n        # Generate M x nu standard normal variates\n        z_matrix = rng_A.standard_normal(size=(M, nu), dtype=np.float64)\n        # Square them and sum along rows to get M draws of the sum-of-squares\n        sums_of_squares = np.sum(z_matrix**2, axis=1)\n        \n        # Calculate sample mean and unbiased sample variance\n        mu_hat = np.mean(sums_of_squares)\n        sigma2_hat = np.var(sums_of_squares, ddof=1)\n        \n        # Calculate absolute errors\n        mean_err = abs(mu_hat - true_mean)\n        var_err = abs(sigma2_hat - true_variance)\n\n    # --- Part B: Numerical Stability Assessment ---\n\n    if nu == 0:\n        # Trivial case: all sums are 0, so discrepancies are 0.\n        r1 = 0.0\n        r2 = 0.0\n    else:\n        # Initialize a new RNG with a different seed for independence\n        rng_B = np.random.default_rng(seed=s + 1337)\n        z_vector = rng_B.standard_normal(size=nu, dtype=np.float64)\n        z_vector_sq = z_vector**2\n        \n        # 1. Naïve summation in a plain Python loop\n        s_naive = 0.0\n        for x in z_vector_sq:\n            s_naive += x\n            \n        # 2. Kahan compensated summation\n        s_kahan = 0.0\n        c = 0.0  # A running compensation for lost low-order bits.\n        for x in z_vector_sq:\n            y = x - c\n            t = s_kahan + y\n            c = (t - s_kahan) - y\n            s_kahan = t\n            \n        # 3. Vectorized numpy.dot summation\n        s_np = np.dot(z_vector, z_vector)\n        \n        # Calculate relative discrepancies using Kahan sum as the baseline\n        if s_kahan == 0.0:\n            # This case is extremely unlikely for nu  0 but is handled for correctness.\n            r1 = 0.0\n            r2 = 0.0\n        else:\n            r1 = abs(s_naive - s_kahan) / s_kahan\n            r2 = abs(s_np - s_kahan) / s_kahan\n\n    return [mean_err, var_err, r1, r2]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3056438"}]}