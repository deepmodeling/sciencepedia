{"hands_on_practices": [{"introduction": "A key feature of the Poisson process is the memoryless property of its exponential interarrival times. This can lead to some counter-intuitive results, such as the famous \"waiting time paradox.\" This exercise [@problem_id:1366282] provides a concrete way to explore this property by calculating the probability that your wait for the next event is longer than the average time between events, demonstrating that for a Poisson process, the past has no bearing on the future.", "problem": "Alerts from a distributed sensor network arrive at a central monitoring station according to a Poisson process with an average rate of $\\lambda$ alerts per hour. An analyst, whose arrival at the station is independent of the alert timings, begins monitoring at an arbitrary point in time. What is the probability that the time the analyst must wait for the next alert is longer than the average time interval between consecutive alerts? Provide your answer as a decimal rounded to four significant figures.", "solution": "Alerts follow a Poisson process with rate $\\lambda$, so interarrival times $S$ are independent and identically distributed exponential random variables with parameter $\\lambda$. Thus the mean interarrival time is\n$$\n\\mathbb{E}[S]=\\frac{1}{\\lambda}.\n$$\nIf the analyst starts monitoring at an independent, arbitrary time, the waiting time $W$ until the next alert has the same exponential distribution due to stationarity and the lack-of-memory property of the exponential distribution. Specifically,\n$$\n\\mathbb{P}(W>t)=\\exp(-\\lambda t)\\quad \\text{for } t\\geq 0.\n$$\nWe are asked for the probability that the waiting time exceeds the average interarrival time:\n$$\n\\mathbb{P}\\!\\left(W>\\mathbb{E}[S]\\right)=\\mathbb{P}\\!\\left(W>\\frac{1}{\\lambda}\\right)=\\exp\\!\\left(-\\lambda\\cdot \\frac{1}{\\lambda}\\right)=\\exp(-1).\n$$\nAs a decimal rounded to four significant figures, $\\exp(-1)\\approx 0.3679$.", "answer": "$$\\boxed{0.3679}$$", "id": "1366282"}, {"introduction": "Building on the concept of a single waiting period, we often need to understand the time it takes for multiple events to occur. Since the interarrival times in a Poisson process are independent and identically distributed, we can use the linearity of expectation to find the total expected waiting time for $k$ events. This practice [@problem_id:1366250] walks you through this fundamental calculation, a crucial skill for applications ranging from particle physics to queuing theory.", "problem": "A physicist is operating a small, heavily shielded particle detector designed to capture rare cosmic ray events. The detection of these specific events can be modeled as occurring randomly and independently over time. Based on long-term observation, the detector registers an average of 3.5 of these events per day.\n\nWhat is the expected waiting time, starting from an arbitrary point in time, until the detector has registered its 6th event? Express your answer in units of days, rounded to three significant figures.", "solution": "Model the detections as a homogeneous Poisson process with rate $\\lambda$ events per day. Given the average is $3.5$ events per day, set $\\lambda=3.5$.\n\nIn a Poisson process, interarrival times $X_{i}$ are independent and identically distributed exponential random variables with parameter $\\lambda$, so $\\mathbb{E}[X_{i}]=\\frac{1}{\\lambda}$. The waiting time to the $k$-th event is the sum $T_{k}=\\sum_{i=1}^{k}X_{i}$, which has a Gamma (Erlang) distribution with shape $k$ and rate $\\lambda$. By linearity of expectation,\n$$\n\\mathbb{E}[T_{k}]=\\sum_{i=1}^{k}\\mathbb{E}[X_{i}]=\\frac{k}{\\lambda}.\n$$\nFor $k=6$ and $\\lambda=3.5=\\frac{7}{2}$,\n$$\n\\mathbb{E}[T_{6}]=\\frac{6}{3.5}=\\frac{6}{\\frac{7}{2}}=\\frac{12}{7}\\ \\text{days}\\approx 1.714285\\ \\text{days}.\n$$\nRounded to three significant figures, this is $1.71$ days.", "answer": "$$\\boxed{1.71}$$", "id": "1366250"}, {"introduction": "Our perspective on a random process changes when we are given additional information. This final practice [@problem_id:1366258] challenges you to think conditionally: if we know that exactly $n$ events occurred within a time interval of length $T$, what is the expected time of the very first event? Solving this problem reveals a beautiful and simple result, illustrating the powerful concept that, conditional on the total count, the event times in a Poisson process behave like uniformly distributed random points.", "problem": "The detection of high-energy muons at a mountain-top observatory is modeled as a Poisson process with a constant average rate $\\lambda$. During a specific continuous observation period of total duration $T$, a data-logging error occurred. While the individual arrival times were lost, the system did correctly record that exactly $n$ muons were detected within this interval, where $n$ is a positive integer. Your task is to determine the expected arrival time of the first muon, measured from the beginning of the observation period.\n\nExpress your answer as a closed-form analytic expression in terms of $n$ and $T$.", "solution": "Let $\\{N(t): t \\geq 0\\}$ be a homogeneous Poisson process with rate $\\lambda$. Condition on the event $\\{N(T)=n\\}$, where $n$ is a positive integer. Denote the ordered arrival times within $(0,T)$ by $0<S_{1}<S_{2}<\\cdots<S_{n}<T$.\n\nA fundamental property of the homogeneous Poisson process is that, conditional on $N(T)=n$, the unordered arrival times are i.i.d. uniform on $(0,T)$, and equivalently, the ordered times have joint density\n$$\nf_{S_{1},\\ldots,S_{n}\\mid N(T)=n}(t_{1},\\ldots,t_{n})=\\frac{n!}{T^{n}}, \\quad 0<t_{1}<\\cdots<t_{n}<T.\n$$\nTo find the marginal density of $S_{1}$ given $N(T)=n$, integrate out $t_{2},\\ldots,t_{n}$ over the simplex $t<t_{2}<\\cdots<t_{n}<T$:\n$$\nf_{S_{1}\\mid N(T)=n}(t)=\\int_{t<t_{2}<\\cdots<t_{n}<T}\\frac{n!}{T^{n}}\\,dt_{2}\\cdots dt_{n}, \\quad 0<t<T.\n$$\nThe $(n-1)$-dimensional simplex has volume $(T-t)^{n-1}/(n-1)!$, hence\n$$\nf_{S_{1}\\mid N(T)=n}(t)=\\frac{n!}{T^{n}}\\cdot\\frac{(T-t)^{n-1}}{(n-1)!}\n=\\frac{n}{T}\\left(1-\\frac{t}{T}\\right)^{n-1}, \\quad 0<t<T.\n$$\nTherefore, the conditional expectation of the first arrival time is\n$$\n\\mathbb{E}[S_{1}\\mid N(T)=n]=\\int_{0}^{T}t\\,f_{S_{1}\\mid N(T)=n}(t)\\,dt\n=\\frac{n}{T}\\int_{0}^{T}t\\left(1-\\frac{t}{T}\\right)^{n-1}dt.\n$$\nWith the change of variables $u=t/T$ (so $t=Tu$ and $dt=T\\,du$), this becomes\n$$\n\\mathbb{E}[S_{1}\\mid N(T)=n]=nT\\int_{0}^{1}u(1-u)^{n-1}\\,du.\n$$\nUsing the Beta integral,\n$$\n\\int_{0}^{1}u^{a-1}(1-u)^{b-1}\\,du=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n$$\nwith $a=2$ and $b=n$, we get\n$$\n\\int_{0}^{1}u(1-u)^{n-1}\\,du=\\frac{\\Gamma(2)\\Gamma(n)}{\\Gamma(n+2)}=\\frac{1!\\,(n-1)!}{(n+1)!}=\\frac{1}{n(n+1)}.\n$$\nThus,\n$$\n\\mathbb{E}[S_{1}\\mid N(T)=n]=nT\\cdot\\frac{1}{n(n+1)}=\\frac{T}{n+1}.\n$$\nThis expectation depends only on $n$ and $T$, and not on $\\lambda$, as expected from the conditional uniformity property.", "answer": "$$\\boxed{\\frac{T}{n+1}}$$", "id": "1366258"}]}