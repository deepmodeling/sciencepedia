{"hands_on_practices": [{"introduction": "This first practice takes us back to fundamentals, demonstrating how the mathematical formula for a covariance kernel can be derived directly from the axiomatic properties of a stochastic process. By working through the example of fractional Brownian motion (fBm), you will see how its defining characteristics of self-similarity and stationary increments are all that is needed to construct its kernel. This exercise [@problem_id:3047382] builds a crucial bridge between the abstract definition of a process and the concrete analytical tool used to describe its second-order structure.", "problem": "Consider a zero-mean Gaussian process called fractional Brownian motion (fBm), denoted by $B_{H}(t)$, with Hurst parameter $H \\in (0,1)$, indexed by time $t \\in \\mathbb{R}$. The process satisfies the following fundamental properties:\n\n- Stationary increments: for any $s,t \\in \\mathbb{R}$ and any shift $u \\in \\mathbb{R}$, the increment $B_{H}(t+u)-B_{H}(s+u)$ has the same distribution as $B_{H}(t)-B_{H}(s)$.\n- Self-similarity of index $H$: for any $c>0$, the process $\\{B_{H}(ct): t \\in \\mathbb{R}\\}$ has the same finite-dimensional distributions as $\\{c^{H} B_{H}(t): t \\in \\mathbb{R}\\}$.\n- Normalization: $\\operatorname{Var}(B_{H}(1)) = 1$.\n\nStarting only from these properties and the core definitions of covariance and variance for Gaussian processes, derive the explicit covariance kernel $k(s,t) = \\mathbb{E}[B_{H}(s) B_{H}(t)]$ as a closed-form analytic expression in terms of $s$, $t$, and $H$. Provide your final expression for $k(s,t)$.", "solution": "The problem asks for the derivation of the covariance kernel $k(s,t) = \\mathbb{E}[B_{H}(s) B_{H}(t)]$ for a zero-mean fractional Brownian motion (fBm) $B_H(t)$, starting from its fundamental properties. Since the process is specified as having zero mean, $\\mathbb{E}[B_{H}(t)] = 0$ for all $t \\in \\mathbb{R}$.\n\nFor any two zero-mean random variables $X$ and $Y$, their covariance is $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY]$. We can relate this to their variances using the identity for the variance of their difference:\n$$ \\operatorname{Var}(X-Y) = \\mathbb{E}[(X-Y)^2] = \\mathbb{E}[X^2 - 2XY + Y^2] = \\mathbb{E}[X^2] + \\mathbb{E}[Y^2] - 2\\mathbb{E}[XY] $$\nSince $X$ and $Y$ are zero-mean, $\\operatorname{Var}(X) = \\mathbb{E}[X^2]$ and $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2]$. Substituting these gives:\n$$ \\operatorname{Var}(X-Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2\\mathbb{E}[XY] $$\nRearranging this equation to solve for $\\mathbb{E}[XY]$ yields a crucial identity:\n$$ \\mathbb{E}[XY] = \\frac{1}{2} \\left( \\operatorname{Var}(X) + \\operatorname{Var}(Y) - \\operatorname{Var}(X-Y) \\right) $$\nApplying this identity to the stochastic process $B_H(t)$ by setting $X=B_H(s)$ and $Y=B_H(t)$, we obtain the expression for the covariance kernel:\n$$ k(s,t) = \\mathbb{E}[B_H(s)B_H(t)] = \\frac{1}{2} \\left( \\operatorname{Var}(B_H(s)) + \\operatorname{Var}(B_H(t)) - \\operatorname{Var}(B_H(s) - B_H(t)) \\right) $$\nTo find the explicit form of $k(s,t)$, we must determine the terms $\\operatorname{Var}(B_H(t))$ and $\\operatorname{Var}(B_H(s) - B_H(t))$ using the properties provided in the problem statement.\n\nFirst, let us determine $\\operatorname{Var}(B_H(t))$. The property of self-similarity states that for any $c>0$, the process $\\{B_H(ct): t \\in \\mathbb{R}\\}$ has the same finite-dimensional distributions as $\\{c^H B_H(t): t \\in \\mathbb{R}\\}$. For a single time point, this implies that the random variable $B_H(ct)$ has the same distribution as $c^H B_H(t)$.\nLet us first establish that $B_H(0)=0$ almost surely. From self-similarity, $B_H(c \\cdot 0)$ has the same distribution as $c^H B_H(0)$ for any $c>0$. This simplifies to $B_H(0)$ having the same distribution as $c^H B_H(0)$. The variance must therefore satisfy $\\operatorname{Var}(B_H(0)) = \\operatorname{Var}(c^H B_H(0)) = (c^H)^2 \\operatorname{Var}(B_H(0)) = c^{2H} \\operatorname{Var}(B_H(0))$. Since $H \\in (0,1)$, the term $c^{2H}$ is not equal to $1$ for all $c>0$. The only way for the equality $\\operatorname{Var}(B_H(0)) = c^{2H} \\operatorname{Var}(B_H(0))$ to hold is if $\\operatorname{Var}(B_H(0)) = 0$. Since $B_H(0)$ is a Gaussian random variable with zero mean and zero variance, it must be equal to $0$ almost surely.\n\nNow, we can find $\\operatorname{Var}(B_H(t))$ for any $t \\in \\mathbb{R}$.\nCase 1: $t>0$. We can use the self-similarity property with $c=t$ and a base time of $1$. The random variable $B_H(t) = B_H(t \\cdot 1)$ has the same distribution as $t^H B_H(1)$. Therefore, their variances are equal:\n$$ \\operatorname{Var}(B_H(t)) = \\operatorname{Var}(t^H B_H(1)) $$\nUsing the variance property $\\operatorname{Var}(aZ) = a^2 \\operatorname{Var}(Z)$, we get:\n$$ \\operatorname{Var}(B_H(t)) = (t^H)^2 \\operatorname{Var}(B_H(1)) = t^{2H} \\operatorname{Var}(B_H(1)) $$\nUsing the normalization condition $\\operatorname{Var}(B_H(1)) = 1$, we find:\n$$ \\operatorname{Var}(B_H(t)) = t^{2H} \\quad \\text{for } t>0 $$\nCase 2: $t<0$. Let $t = -u$ where $u>0$. We need to find $\\operatorname{Var}(B_H(-u))$. We use the property of stationary increments. The increment $B_H(0) - B_H(-u)$ has the same distribution as the increment $B_H(u) - B_H(0)$ (by shifting time by $u$). Since $B_H(0)=0$ a.s., this implies that $-B_H(-u)$ has the same distribution as $B_H(u)$. This means $B_H(-u)$ has the same distribution as $-B_H(u)$.\nTherefore, their variances must be equal:\n$$ \\operatorname{Var}(B_H(-u)) = \\operatorname{Var}(-B_H(u)) = (-1)^2 \\operatorname{Var}(B_H(u)) = \\operatorname{Var}(B_H(u)) $$\nSince $u>0$, we use the result from Case 1: $\\operatorname{Var}(B_H(u)) = u^{2H}$.\nThus, for $t<0$, $\\operatorname{Var}(B_H(t)) = \\operatorname{Var}(B_H(-u)) = u^{2H} = (-t)^{2H} = |t|^{2H}$.\nCombining cases, and noting that for $t=0$, $\\operatorname{Var}(B_H(0))=0$, we have for all $t \\in \\mathbb{R}$:\n$$ \\operatorname{Var}(B_H(t)) = |t|^{2H} $$\n\nSecond, let us determine $\\operatorname{Var}(B_H(s) - B_H(t))$. We use the property of stationary increments, which states that for any shift $u \\in \\mathbb{R}$, $B_H(s+u)-B_H(t+u)$ has the same distribution as $B_H(s)-B_H(t)$. Let's choose the shift $u=-t$. Then the increment $B_H(s)-B_H(t)$ has the same distribution as the increment $B_H(s-t) - B_H(t-t) = B_H(s-t) - B_H(0)$.\nSince $B_H(0)=0$ a.s., the distribution of $B_H(s)-B_H(t)$ is the same as the distribution of $B_H(s-t)$. Thus, their variances are equal:\n$$ \\operatorname{Var}(B_H(s) - B_H(t)) = \\operatorname{Var}(B_H(s-t)) $$\nUsing our previously derived formula for the variance, we get:\n$$ \\operatorname{Var}(B_H(s) - B_H(t)) = |s-t|^{2H} $$\n\nFinally, we substitute these variance expressions back into our formula for the covariance kernel $k(s,t)$:\n$$ k(s,t) = \\frac{1}{2} \\left( \\operatorname{Var}(B_H(s)) + \\operatorname{Var}(B_H(t)) - \\operatorname{Var}(B_H(s) - B_H(t)) \\right) $$\n$$ k(s,t) = \\frac{1}{2} \\left( |s|^{2H} + |t|^{2H} - |s-t|^{2H} \\right) $$\nThis expression is the closed-form analytic representation of the covariance kernel for fractional Brownian motion, derived solely from the provided axiomatic properties.", "answer": "$$\\boxed{\\frac{1}{2} \\left( |s|^{2H} + |t|^{2H} - |s-t|^{2H} \\right)}$$", "id": "3047382"}, {"introduction": "Building on our understanding of kernels, this practice explores the deep connection between the local behavior of a process and the parameters of its governing stochastic differential equation (SDE). By analyzing the covariance kernel of an Ornstein-Uhlenbeck process at infinitesimally small time lags, you will learn how to quantify the process's regularity and relate it directly to the intensity of the driving noise, $\\sigma$. This exercise [@problem_id:3047384] showcases the power of the covariance kernel as a diagnostic tool for understanding the fine structure and path properties of a process defined by an SDE.", "problem": "Consider the strictly stationary Ornstein–Uhlenbeck process defined as the unique stationary solution to the linear stochastic differential equation\n$$\n\\mathrm{d}X_{t} \\,=\\, -\\alpha X_{t}\\,\\mathrm{d}t \\,+\\, \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $\\alpha>0$, $\\sigma>0$, and $\\{W_{t}\\}_{t\\in\\mathbb{R}}$ is a standard Wiener process (also called standard Brownian motion). Let the covariance kernel of the stationary process be\n$$\nC(h) \\,:=\\, \\operatorname{Cov}(X_{t}, X_{t+h}) \\,=\\, \\mathbb{E}[X_{t}\\,X_{t+h}],\n$$\nfor $h\\in\\mathbb{R}$, where stationarity implies that $C(h)$ depends only on the lag $h$ and the mean is zero.\n\nStarting from the fundamental definitions of covariance for stationary processes and Itô calculus (in particular, Itô’s formula and the Itô isometry), analyze the small-lag behavior of $C(h)$ near $h=0$ and determine the right-hand limit\n$$\nL \\,:=\\, \\lim_{h\\to 0^{+}} \\frac{\\operatorname{Var}(X_{t+h}-X_{t})}{h}.\n$$\nYour reasoning should clearly establish the necessary regularity at $h=0$ that justifies the limit and should derive any intermediate quantities from first principles, without assuming closed-form kernels in advance.\n\nProvide your final answer for $L$ as a closed-form analytic expression in terms of $\\alpha$ and $\\sigma$. No numerical approximation is required.", "solution": "The problem requires the evaluation of the limit $L \\,:=\\, \\lim_{h\\to 0^{+}} \\frac{\\operatorname{Var}(X_{t+h}-X_{t})}{h}$ for a stationary Ornstein-Uhlenbeck process $X_t$. We will derive this from the fundamental properties of the process and Itô calculus, without assuming the closed-form expression for the covariance kernel $C(h)$.\n\nFirst, we express the variance term $\\operatorname{Var}(X_{t+h}-X_{t})$ in terms of the covariance kernel $C(h) = \\operatorname{Cov}(X_{t}, X_{t+h})$.\nThe Ornstein-Uhlenbeck process is defined by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} \\,=\\, -\\alpha X_{t}\\,\\mathrm{d}t \\,+\\, \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\alpha>0$ and $\\sigma>0$. For the process to be strictly stationary, its statistical properties must be independent of time. This implies that the mean $\\mathbb{E}[X_t]$ must be a constant, say $\\mu$. Taking the expectation of the SDE, we get $\\mathrm{d}\\mathbb{E}[X_t] = -\\alpha \\mathbb{E}[X_t] \\mathrm{d}t$. For a constant mean, $\\mathrm{d}\\mu/\\mathrm{d}t = 0$, so we must have $0 = -\\alpha \\mu$. Since $\\alpha > 0$, this forces the mean to be $\\mu = \\mathbb{E}[X_t] = 0$.\n\nWith a zero mean, the variance of the increment is:\n$$\n\\operatorname{Var}(X_{t+h}-X_{t}) \\,=\\, \\mathbb{E}[(X_{t+h}-X_{t})^2] - (\\mathbb{E}[X_{t+h}-X_{t}])^2\n$$\nThe second term is $(\\mathbb{E}[X_{t+h}]-\\mathbb{E}[X_t])^2 = (0-0)^2 = 0$.\nThe first term can be expanded:\n$$\n\\mathbb{E}[(X_{t+h}-X_{t})^2] \\,=\\, \\mathbb{E}[X_{t+h}^2 - 2X_t X_{t+h} + X_t^2]\n$$\nBy the linearity of expectation, this becomes:\n$$\n\\mathbb{E}[X_{t+h}^2] - 2\\mathbb{E}[X_t X_{t+h}] + \\mathbb{E}[X_t^2]\n$$\nFor a stationary process with zero mean, the variance is constant, $\\operatorname{Var}(X_s) = \\mathbb{E}[X_s^2] - (\\mathbb{E}[X_s])^2 = \\mathbb{E}[X_s^2]$. The covariance is defined as $C(s) = \\operatorname{Cov}(X_t, X_{t+s}) = \\mathbb{E}[X_t X_{t+s}]$.\nBy stationarity, $\\mathbb{E}[X_{t+h}^2] = \\mathbb{E}[X_t^2] = \\operatorname{Var}(X_t) = C(0)$.\nTherefore, the variance of the increment is:\n$$\n\\operatorname{Var}(X_{t+h}-X_{t}) \\,=\\, C(0) - 2C(h) + C(0) \\,=\\, 2(C(0) - C(h))\n$$\nSubstituting this into the definition of $L$:\n$$\nL \\,=\\, \\lim_{h\\to 0^{+}} \\frac{2(C(0) - C(h))}{h} \\,=\\, -2 \\lim_{h\\to 0^{+}} \\frac{C(h) - C(0)}{h}\n$$\nThis limit is, by definition, $-2$ times the right-hand derivative of the covariance function at $h=0$, which we denote as $C'(0^{+})$. So, our task is to find $L = -2C'(0^{+})$. This requires deriving the behavior of $C(h)$ for small positive $h$.\n\nWe can derive an ordinary differential equation for $C(h)$ by integrating the SDE from $t$ to $t+h$:\n$$\nX_{t+h} \\,=\\, X_t + \\int_{t}^{t+h} (-\\alpha X_s \\mathrm{d}s + \\sigma \\mathrm{d}W_s)\n$$\nNow we compute $\\mathbb{E}[X_t X_{t+h}]$ for $h>0$:\n$$\n\\mathbb{E}[X_t X_{t+h}] \\,=\\, \\mathbb{E}\\left[X_t \\left(X_t - \\alpha \\int_{t}^{t+h} X_s \\mathrm{d}s + \\sigma \\int_{t}^{t+h} \\mathrm{d}W_s\\right)\\right]\n$$\nUsing linearity of expectation:\n$$\nC(h) \\,=\\, \\mathbb{E}[X_t^2] - \\alpha \\mathbb{E}\\left[X_t \\int_{t}^{t+h} X_s \\mathrm{d}s\\right] + \\sigma \\mathbb{E}\\left[X_t \\int_{t}^{t+h} \\mathrm{d}W_s\\right]\n$$\nThe first term is $\\mathbb{E}[X_t^2] = C(0)$. The last term is zero because $X_t$ is adapted to the filtration $\\mathcal{F}_t$, while the increment of the Wiener process $\\int_{t}^{t+h} \\mathrm{d}W_s = W_{t+h} - W_t$ is independent of $\\mathcal{F}_t$ and has zero mean.\nThe middle term can be rewritten using Fubini's theorem for stochastic integrals:\n$$\n\\mathbb{E}\\left[X_t \\int_{t}^{t+h} X_s \\mathrm{d}s\\right] \\,=\\, \\int_{t}^{t+h} \\mathbb{E}[X_t X_s] \\mathrm{d}s \\,=\\, \\int_{t}^{t+h} C(s-t) \\mathrm{d}s\n$$\nChanging the integration variable to $u=s-t$, with $\\mathrm{d}u=\\mathrm{d}s$, the integral becomes $\\int_{0}^{h} C(u) \\mathrm{d}u$.\nThus, we have derived a Volterra integral equation for $C(h)$:\n$$\nC(h) \\,=\\, C(0) - \\alpha \\int_{0}^{h} C(u) \\mathrm{d}u, \\quad \\text{for } h>0.\n$$\nThe Ornstein-Uhlenbeck process is known to be mean-square continuous, which implies that its covariance function $C(h)$ is continuous. Therefore, by the Fundamental Theorem of Calculus, we can differentiate the integral equation with respect to $h$:\n$$\nC'(h) \\,=\\, -\\alpha C(h), \\quad \\text{for } h>0.\n$$\nNow we can evaluate the right-hand derivative at $h=0$:\n$$\nC'(0^{+}) \\,=\\, \\lim_{h\\to 0^{+}} C'(h) \\,=\\, \\lim_{h\\to 0^{+}} (-\\alpha C(h))\n$$\nSince $C(h)$ is continuous at $h=0$, $\\lim_{h\\to 0^{+}} C(h) = C(0)$.\n$$\nC'(0^{+}) \\,=\\, -\\alpha C(0)\n$$\nSubstituting this back into the expression for $L$:\n$$\nL \\,=\\, -2C'(0^{+}) \\,=\\, -2(-\\alpha C(0)) \\,=\\, 2\\alpha C(0)\n$$\nThe final step is to determine the stationary variance $C(0) = \\mathbb{E}[X_t^2]$ from first principles. We use Itô's formula for the function $f(x) = x^2$:\n$$\n\\mathrm{d}(X_t^2) \\,=\\, 2X_t \\mathrm{d}X_t + \\frac{1}{2}(2)(\\mathrm{d}X_t)^2\n$$\nSubstituting $\\mathrm{d}X_t = -\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t$ and using the Itô multiplication rule $(\\mathrm{d}t)^2=0$, $(\\mathrm{d}t)(\\mathrm{d}W_t)=0$, $(\\mathrm{d}W_t)^2=\\mathrm{d}t$, we find the quadratic variation term:\n$$\n(\\mathrm{d}X_t)^2 \\,=\\, (-\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t)^2 \\,=\\, \\sigma^2 (\\mathrm{d}W_t)^2 \\,=\\, \\sigma^2 \\mathrm{d}t\n$$\nSo, the Itô-Doeblin formula gives:\n$$\n\\mathrm{d}(X_t^2) \\,=\\, 2X_t(-\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t) + \\sigma^2 \\mathrm{d}t\n$$\n$$\n\\mathrm{d}(X_t^2) \\,=\\, (-2\\alpha X_t^2 + \\sigma^2)\\mathrm{d}t + 2\\sigma X_t \\mathrm{d}W_t\n$$\nTaking the expectation of this differential form:\n$$\n\\mathrm{d}\\mathbb{E}[X_t^2] \\,=\\, \\mathbb{E}[(-2\\alpha X_t^2 + \\sigma^2)\\mathrm{d}t + 2\\sigma X_t \\mathrm{d}W_t]\n$$\n$$\n\\frac{\\mathrm{d}\\mathbb{E}[X_t^2]}{\\mathrm{d}t} \\,=\\, -2\\alpha \\mathbb{E}[X_t^2] + \\sigma^2\n$$\nWe used the property that the expectation of the Itô integral term $\\mathbb{E}[\\int 2\\sigma X_s \\mathrm{d}W_s]$ is zero.\nFor a stationary process, the variance $\\mathbb{E}[X_t^2] = C(0)$ is constant with respect to time $t$. Therefore, its time derivative is zero:\n$$\n0 \\,=\\, -2\\alpha C(0) + \\sigma^2\n$$\nSolving for $C(0)$, we find the stationary variance:\n$$\nC(0) \\,=\\, \\frac{\\sigma^2}{2\\alpha}\n$$\nFinally, we substitute this expression for $C(0)$ into our formula for $L$:\n$$\nL \\,=\\, 2\\alpha C(0) \\,=\\, 2\\alpha \\left(\\frac{\\sigma^2}{2\\alpha}\\right) \\,=\\, \\sigma^2\n$$\nThe limit $L$ measures the local rate of increase of variance, which is determined by the magnitude of the diffusion term in the SDE, and is independent of the mean-reversion parameter $\\alpha$.", "answer": "$$\\boxed{\\sigma^{2}}$$", "id": "3047384"}, {"introduction": "Our final practice demonstrates one of the most powerful applications of a covariance kernel: its role in the spectral decomposition of a stochastic process. You will work with the Brownian bridge process and use its kernel to define a covariance operator, for which you will find the eigenvalues and eigenfunctions. This exercise [@problem_id:3047388] reveals that the kernel provides a complete blueprint for the process via the Karhunen-Loève expansion, representing it as a sum of deterministic orthogonal functions with random coefficients, which is a foundational concept in signal processing and functional data analysis.", "problem": "Consider the centered Gaussian process $X(t)$ on $[0,1]$ known as the Brownian bridge, characterized by the covariance kernel $K(s,t) = \\min\\{s,t\\} - s t$. Define the covariance operator $C:L^{2}(0,1)\\to L^{2}(0,1)$ by\n$$\n(Cf)(t) = \\int_{0}^{1} K(s,t)\\,f(s)\\,ds,\n$$\nfor $f\\in L^{2}(0,1)$. Let $\\{(\\lambda_{n},\\varphi_{n})\\}_{n=1}^{\\infty}$ be the eigenpairs of $C$ with $\\{\\varphi_{n}\\}$ forming an orthonormal basis of $L^{2}(0,1)$. The Karhunen–Loève (KL) expansion expresses $X$ as\n$$\nX(t) = \\sum_{n=1}^{\\infty} \\xi_{n}\\,\\sqrt{\\lambda_{n}}\\,\\varphi_{n}(t),\n$$\nwhere $\\{\\xi_{n}\\}$ are independent standard normal random variables.\n\nStarting from fundamental definitions and properties of covariance kernels and linear differential operators on $L^{2}(0,1)$, derive the eigenpairs $\\{(\\lambda_{n},\\varphi_{n})\\}$ of $C$ and use them to compute the expected squared $L^{2}(0,1)$ truncation error of the KL expansion after $N$ modes,\n$$\n\\mathbb{E}\\!\\left[\\left\\|X - \\sum_{n=1}^{N} \\xi_{n}\\,\\sqrt{\\lambda_{n}}\\,\\varphi_{n}\\right\\|_{L^{2}(0,1)}^{2}\\right].\n$$\nProvide your final answer as a single closed-form analytic expression in terms of $N$ (no numerical rounding is required).", "solution": "### Derivation of the Eigenpairs\nThe core of the problem lies in solving the eigenvalue equation for the operator $C$:\n$$ (C\\varphi)(t) = \\lambda \\varphi(t) $$\nSubstituting the definition of $C$, we obtain the Fredholm integral equation of the second kind:\n$$ \\int_{0}^{1} K(s,t)\\,\\varphi(s)\\,ds = \\lambda \\varphi(t) $$\nA standard method for solving such integral equations is to convert them into an equivalent differential equation problem. The kernel $K(s,t) = \\min\\{s,t\\} - st$ is the Green's function for the linear differential operator $L = -\\frac{d^2}{dt^2}$ subject to the homogeneous Dirichlet boundary conditions $y(0)=0$ and $y(1)=0$.\n\nTo demonstrate this, we can differentiate the integral equation with respect to $t$:\n$$ \\lambda \\varphi(t) = \\int_{0}^{t} (s - st)\\varphi(s)ds + \\int_{t}^{1} (t-st)\\varphi(s)ds $$\n$$ \\lambda \\varphi(t) = (1-t)\\int_{0}^{t} s\\varphi(s)ds + t\\int_{t}^{1} (1-s)\\varphi(s)ds $$\nDifferentiating once with respect to $t$ using Leibniz integral rule:\n$$ \\lambda \\varphi'(t) = (-1)\\int_{0}^{t} s\\varphi(s)ds + (1-t)(t\\varphi(t)) + (1)\\int_{t}^{1} (1-s)\\varphi(s)ds + t(-(1-t)\\varphi(t)) $$\n$$ \\lambda \\varphi'(t) = -\\int_{0}^{t} s\\varphi(s)ds + \\int_{t}^{1} (1-s)\\varphi(s)ds $$\nDifferentiating a second time:\n$$ \\lambda \\varphi''(t) = -t\\varphi(t) - (-(1-t)\\varphi(t)) = -t\\varphi(t) - \\varphi(t) + t\\varphi(t) = -\\varphi(t) $$\nThis gives the differential equation:\n$$ -\\varphi''(t) = \\frac{1}{\\lambda} \\varphi(t) $$\nThe boundary conditions on $\\varphi(t)$ must also be derived from the integral equation. At $t=0$, we have $\\lambda \\varphi(0) = \\int_0^1 (s\\cdot 0)\\varphi(s)ds = 0$, so $\\varphi(0)=0$ (assuming $\\lambda \\neq 0$). At $t=1$, we have $\\lambda \\varphi(1) = \\int_0^1 (\\min\\{s,1\\}-s)\\varphi(s)ds = \\int_0^1 (s-s)\\varphi(s)ds = 0$, so $\\varphi(1)=0$.\n\nThus, the integral eigenvalue problem is equivalent to the Sturm-Liouville boundary value problem:\n$$ -\\varphi''(t) = \\mu \\varphi(t) \\quad \\text{with} \\quad \\varphi(0)=0, \\varphi(1)=0 $$\nwhere $\\mu = 1/\\lambda$. The general solution to $\\varphi''(t) + \\mu \\varphi(t) = 0$ depends on the sign of $\\mu$. For eigenfunctions to exist in $L^2(0,1)$, $\\mu$ must be positive. Let $\\mu = k^2$ for some $k>0$.\nThe general solution is $\\varphi(t) = A\\cos(kt) + B\\sin(kt)$.\nApplying the boundary conditions:\n1. $\\varphi(0)=0 \\implies A\\cos(0) + B\\sin(0) = A = 0$.\n2. The solution is of the form $\\varphi(t) = B\\sin(kt)$.\n3. $\\varphi(1)=0 \\implies B\\sin(k)=0$. For a non-trivial solution ($B \\neq 0$), we must have $\\sin(k)=0$.\nThis implies $k = n\\pi$ for any non-zero integer $n$. Since $\\sin(-x) = -\\sin(x)$, negative integers do not produce new eigenfunctions, so we take $n = 1, 2, 3, \\ldots$.\n\nThe eigenvalues of the differential operator are $\\mu_n = (n\\pi)^2$ for $n=1, 2, \\dots$. The corresponding eigenvalues of the integral operator $C$ are:\n$$ \\lambda_n = \\frac{1}{\\mu_n} = \\frac{1}{(n\\pi)^2} $$\nThe associated eigenfunctions are $\\varphi_n(t) = B_n \\sin(n\\pi t)$. To form an orthonormal basis, we must normalize them in $L^2(0,1)$:\n$$ \\int_{0}^{1} (\\varphi_n(t))^2 dt = 1 $$\n$$ B_n^2 \\int_{0}^{1} \\sin^2(n\\pi t) dt = B_n^2 \\int_{0}^{1} \\frac{1-\\cos(2n\\pi t)}{2} dt = B_n^2 \\left[ \\frac{t}{2} - \\frac{\\sin(2n\\pi t)}{4n\\pi} \\right]_0^1 = B_n^2 \\left(\\frac{1}{2}\\right) = 1 $$\nThis gives $B_n = \\sqrt{2}$. The orthonormal eigenfunctions are:\n$$ \\varphi_n(t) = \\sqrt{2}\\sin(n\\pi t) $$\n\n### Calculation of Truncation Error\nThe quantity to be computed is the expected squared $L^2$ norm of the truncation error after $N$ terms:\n$$ E_N = \\mathbb{E}\\!\\left[\\left\\|X - \\sum_{n=1}^{N} \\xi_{n}\\,\\sqrt{\\lambda_{n}}\\,\\varphi_{n}\\right\\|_{L^{2}(0,1)}^{2}\\right] $$\nSubstituting the full KL expansion for $X(t)$, the term inside the norm is the tail of the series:\n$$ X(t) - \\sum_{n=1}^{N} \\xi_{n}\\,\\sqrt{\\lambda_{n}}\\,\\varphi_{n}(t) = \\sum_{n=N+1}^{\\infty} \\xi_{n}\\,\\sqrt{\\lambda_{n}}\\,\\varphi_{n}(t) $$\nWe compute the squared $L^2$ norm of this error term. The $L^2(0,1)$ norm is induced by the inner product $\\langle f,g \\rangle = \\int_0^1 f(t)g(t)dt$.\n$$ \\left\\| \\sum_{n=N+1}^{\\infty} \\xi_{n}\\sqrt{\\lambda_{n}}\\varphi_{n} \\right\\|^2 = \\left\\langle \\sum_{n=N+1}^{\\infty} \\xi_{n}\\sqrt{\\lambda_{n}}\\varphi_{n}, \\sum_{m=N+1}^{\\infty} \\xi_{m}\\sqrt{\\lambda_{m}}\\varphi_{m} \\right\\rangle $$\nBy linearity of the inner product and the orthonormality of the eigenfunctions ($\\langle\\varphi_n, \\varphi_m\\rangle = \\delta_{nm}$), this simplifies to:\n$$ = \\sum_{n=N+1}^{\\infty} \\sum_{m=N+1}^{\\infty} \\xi_n\\xi_m\\sqrt{\\lambda_n\\lambda_m} \\langle\\varphi_n, \\varphi_m\\rangle = \\sum_{n=N+1}^{\\infty} (\\xi_n\\sqrt{\\lambda_n})^2 = \\sum_{n=N+1}^{\\infty} \\xi_n^2\\lambda_n $$\nNow, we take the expectation of this quantity. Using the linearity of expectation:\n$$ E_N = \\mathbb{E}\\left[ \\sum_{n=N+1}^{\\infty} \\xi_n^2\\lambda_n \\right] = \\sum_{n=N+1}^{\\infty} \\mathbb{E}[\\xi_n^2]\\lambda_n $$\nThe variables $\\{\\xi_n\\}$ are given as independent standard normal random variables, $\\xi_n \\sim \\mathcal{N}(0,1)$. For a standard normal variable, the mean is $\\mathbb{E}[\\xi_n] = 0$ and the variance is $\\text{Var}(\\xi_n) = \\mathbb{E}[\\xi_n^2] - (\\mathbb{E}[\\xi_n])^2 = 1$. Therefore, $\\mathbb{E}[\\xi_n^2] = 1$.\nSubstituting this into the expression for the error gives:\n$$ E_N = \\sum_{n=N+1}^{\\infty} (1) \\cdot \\lambda_n = \\sum_{n=N+1}^{\\infty} \\lambda_n $$\nThe expected truncation error is the sum of the eigenvalues corresponding to the truncated modes. We can express this sum in a more convenient form. The sum of all eigenvalues is related to the trace of the operator $C$. By Mercer's theorem, for a continuous kernel, the trace of a Hilbert-Schmidt operator is the integral of the kernel along its diagonal:\n$$ \\sum_{n=1}^{\\infty} \\lambda_n = \\int_{0}^{1} K(t,t) dt $$\n$$ \\int_{0}^{1} (\\min\\{t,t\\} - t^2) dt = \\int_{0}^{1} (t-t^2) dt = \\left[ \\frac{t^2}{2} - \\frac{t^3}{3} \\right]_0^1 = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6} $$\nThis confirms the well-known result from the theory of the Basel problem, $\\sum_{n=1}^{\\infty} \\frac{1}{(n\\pi)^2} = \\frac{1}{\\pi^2}\\sum_{n=1}^{\\infty} \\frac{1}{n^2} = \\frac{1}{\\pi^2}\\frac{\\pi^2}{6} = \\frac{1}{6}$.\n\nThe desired truncation error is the infinite sum $\\sum_{n=N+1}^{\\infty} \\lambda_n$, which can be computed as the total sum minus the partial sum of the first $N$ terms:\n$$ E_N = \\sum_{n=N+1}^{\\infty} \\lambda_n = \\left(\\sum_{n=1}^{\\infty} \\lambda_n\\right) - \\left(\\sum_{n=1}^{N} \\lambda_n\\right) $$\nSubstituting the values we have found:\n$$ E_N = \\frac{1}{6} - \\sum_{n=1}^{N} \\frac{1}{(n\\pi)^2} $$\nFactoring out the constant $\\frac{1}{\\pi^2}$ from the sum gives the final closed-form expression in terms of $N$:\n$$ E_N = \\frac{1}{6} - \\frac{1}{\\pi^2} \\sum_{n=1}^{N} \\frac{1}{n^2} $$\nThis expression requires only elementary operations and a finite sum, satisfying the condition for a closed-form analytic expression.", "answer": "$$ \\boxed{\\frac{1}{6} - \\frac{1}{\\pi^{2}} \\sum_{n=1}^{N} \\frac{1}{n^{2}}} $$", "id": "3047388"}]}