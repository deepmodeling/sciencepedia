{"hands_on_practices": [{"introduction": "The Kolmogorov Extension Theorem is built upon the idea of consistent projections. Before we can check for consistency, we must be proficient in the fundamental operation of projection, which in the context of probability density functions, means calculating marginal distributions. This exercise provides a concrete joint density function for three variables, giving you hands-on practice in finding the marginal density for a subset of those variables by integrating out the others [@problem_id:1454517].", "problem": "In the framework of measure-theoretic probability, a key concept is the consistency of a family of probability measures on product spaces, as formalized by the Kolmogorov extension theorem. This theorem guarantees the existence of a single probability measure on an infinite product space, provided its finite-dimensional projections (marginals) are consistent.\n\nLet's explore a simple case of this projection. Consider a random vector $(X_1, X_2, X_3)$ defined on a probability space. The joint probability measure $\\mu$ on $\\mathbb{R}^3$ for this vector is absolutely continuous with respect to the three-dimensional Lebesgue measure. Its corresponding probability density function (PDF), $f(x_1, x_2, x_3)$, is given by:\n$$\nf(x_1, x_2, x_3) = \n\\begin{cases} \nK \\exp(-\\alpha x_1 - \\beta x_2 - \\gamma x_3) & \\text{if } x_1 > 0, x_2 > 0, x_3 > 0 \\\\\n0 & \\text{otherwise} \n\\end{cases}\n$$\nwhere $K, \\alpha, \\beta,$ and $\\gamma$ are positive real constants.\n\nAccording to the theory, we can find the marginal PDF for any sub-vector. For the sub-vector $(X_1, X_3)$, its marginal PDF, denoted $f_{1,3}(x_1, x_3)$, is the density of the measure obtained by projecting $\\mu$ onto the $(x_1, x_3)$-plane.\n\nDetermine the marginal probability density function $f_{1,3}(x_1, x_3)$ for the region where it is non-zero. Express your answer in terms of $x_1, x_3,$ and the constants $K, \\alpha, \\beta, \\gamma$.", "solution": "We are given a joint probability density function\n$$\nf(x_{1},x_{2},x_{3})=\n\\begin{cases}\nK \\exp(-\\alpha x_{1}-\\beta x_{2}-\\gamma x_{3}) & \\text{if } x_{1}>0, x_{2}>0, x_{3}>0,\\\\\n0 & \\text{otherwise},\n\\end{cases}\n$$\nwith $K,\\alpha,\\beta,\\gamma>0$. The marginal density of $(X_{1},X_{3})$ is obtained by integrating out $x_{2}$:\n$$\nf_{1,3}(x_{1},x_{3})=\\int_{-\\infty}^{\\infty} f(x_{1},x_{2},x_{3})\\,dx_{2}.\n$$\nBecause $f$ vanishes unless $x_{2}>0$, and is nonzero only when $x_{1}>0$ and $x_{3}>0$, we have, for $x_{1}>0$ and $x_{3}>0$,\n$$\nf_{1,3}(x_{1},x_{3})=\\int_{0}^{\\infty} K \\exp(-\\alpha x_{1}-\\beta x_{2}-\\gamma x_{3})\\,dx_{2}.\n$$\nFactor out the terms independent of $x_{2}$:\n$$\nf_{1,3}(x_{1},x_{3})=K \\exp(-\\alpha x_{1}-\\gamma x_{3}) \\int_{0}^{\\infty} \\exp(-\\beta x_{2})\\,dx_{2}.\n$$\nSince $\\beta>0$, the integral evaluates to\n$$\n\\int_{0}^{\\infty} \\exp(-\\beta x_{2})\\,dx_{2}=\\frac{1}{\\beta}.\n$$\nTherefore, for $x_{1}>0$ and $x_{3}>0$,\n$$\nf_{1,3}(x_{1},x_{3})=\\frac{K}{\\beta}\\exp(-\\alpha x_{1}-\\gamma x_{3}).\n$$\nOutside this region, the marginal density is zero, but the problem asks specifically for the expression on the region where it is non-zero, which is given above.", "answer": "$$\\boxed{\\frac{K}{\\beta}\\exp(-\\alpha x_{1}-\\gamma x_{3})}$$", "id": "1454517"}, {"introduction": "A key requirement for constructing a unified stochastic process is that all of its finite-dimensional snapshots must agree on their common parts. This exercise presents a scenario with two different joint distributions for overlapping pairs of variables, $(X_1, X_2)$ and $(X_1, X_3)$. By calculating and comparing the marginal distribution for the shared variable $X_1$ from both sources, you will directly test for a failure of consistency and quantify the discrepancy, illustrating a fundamental condition of the Kolmogorov theorem [@problem_id:1454497].", "problem": "Consider a hypothetical system of three binary random variables $X_1, X_2, X_3$, each taking values in the set $\\{0, 1\\}$. Suppose we are given two candidate probability measures for pairs of these variables.\n\nThe first measure, $\\mu_{12}$, describes the joint distribution of $(X_1, X_2)$ and is defined by the following probability mass function on $\\{0, 1\\}^2$:\n$P(X_1=0, X_2=0) = \\frac{1}{6}$\n$P(X_1=0, X_2=1) = \\frac{1}{3}$\n$P(X_1=1, X_2=0) = \\frac{1}{2}$\n$P(X_1=1, X_2=1) = 0$\n\nThe second measure, $\\mu_{13}$, describes the joint distribution of $(X_1, X_3)$ and is defined by the following probability mass function on $\\{0, 1\\}^2$:\n$P(X_1=0, X_3=0) = \\frac{1}{4}$\n$P(X_1=0, X_3=1) = \\frac{1}{8}$\n$P(X_1=1, X_3=0) = \\frac{1}{8}$\n$P(X_1=1, X_3=1) = \\frac{1}{2}$\n\nA necessary condition for the existence of a single, well-defined joint probability measure $\\mu_{123}$ for $(X_1, X_2, X_3)$ that is consistent with both $\\mu_{12}$ and $\\mu_{13}$ is that the marginal distribution of the common variable, $X_1$, must be the same when derived from either measure. The degree of this inconsistency can be quantified.\n\nCalculate the total variation distance between the marginal probability distribution of $X_1$ derived from $\\mu_{12}$ and the one derived from $\\mu_{13}$. The total variation distance between two probability mass functions $p$ and $q$ on a discrete sample space $\\Omega$ is defined as $\\frac{1}{2} \\sum_{\\omega \\in \\Omega} |p(\\omega) - q(\\omega)|$.\n\nExpress your answer as a single fraction in simplest form.", "solution": "We first compute the marginal distributions of $X_{1}$ from each given joint measure by summing over the other variable.\n\nFrom $\\mu_{12}$:\n$$\nP_{12}(X_{1}=0)=P(0,0)+P(0,1)=\\frac{1}{6}+\\frac{1}{3}=\\frac{1}{2}, \\quad\nP_{12}(X_{1}=1)=P(1,0)+P(1,1)=\\frac{1}{2}+0=\\frac{1}{2}.\n$$\n\nFrom $\\mu_{13}$:\n$$\nP_{13}(X_{1}=0)=P(0,0)+P(0,1)=\\frac{1}{4}+\\frac{1}{8}=\\frac{3}{8}, \\quad\nP_{13}(X_{1}=1)=P(1,0)+P(1,1)=\\frac{1}{8}+\\frac{1}{2}=\\frac{5}{8}.\n$$\n\nThe total variation distance between these two marginal distributions on $\\{0,1\\}$ is\n$$\n\\frac{1}{2}\\left(\\left|P_{12}(0)-P_{13}(0)\\right|+\\left|P_{12}(1)-P_{13}(1)\\right|\\right)\n=\\frac{1}{2}\\left(\\left|\\frac{1}{2}-\\frac{3}{8}\\right|+\\left|\\frac{1}{2}-\\frac{5}{8}\\right|\\right)\n=\\frac{1}{2}\\left(\\frac{1}{8}+\\frac{1}{8}\\right)=\\frac{1}{8}.\n$$", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "1454497"}, {"introduction": "While consistency on overlapping marginals is necessary, a deeper question arises: is it sufficient to only check consistency between pairs of variables? This advanced problem explores a classic counterexample that demonstrates why the full Kolmogorov condition is stricter and more powerful. You will construct a system of distributions that is perfectly consistent for every pair of variables, yet paradoxically, cannot be extended to a single consistent distribution for a triple of variables, revealing a crucial subtlety in the construction of stochastic processes [@problem_id:3062997].", "problem": "Let $\\{X_{t}\\}_{t \\in T}$ be a family of random variables with state space $E$ and finite-dimensional distributions $\\{\\mu_{I}\\}_{I \\subset T,\\, |I|<\\infty}$, where $\\mu_{I}$ is a probability measure on $E^{I}$. The Kolmogorov Extension Theorem (KET) requires a projective family that is consistent on overlaps: for any finite $I \\subset J \\subset T$, the marginal of $\\mu_{J}$ on $E^{I}$ equals $\\mu_{I}$. In contrast, call a family pairwise consistent if this marginal agreement is only required for sets $I$ with $\\lvert I \\rvert \\leq 2$ (that is, all singleton marginals agree across all pairs, and each bivariate marginal agrees with its singletons).\n\n1. Explain why Kolmogorov consistency is strictly stronger than pairwise consistency by giving a construction on $T=\\{1,2,3\\}$ where each pairwise marginal $\\mu_{\\{i,j\\}}$ has the same singleton marginals, but there does not exist any triple distribution $\\mu_{\\{1,2,3\\}}$ on $E^{\\{1,2,3\\}}$ whose pairwise marginals coincide with the given ones.\n\n2. Work on $E=\\{-1,1\\}$. Define the three bivariate laws $\\mu_{12}$, $\\mu_{23}$, and $\\mu_{31}$ so that for each pair $(i,j)$, the variables $X_{i}$ and $X_{j}$ are almost surely unequal and each singleton marginal is uniform. Concretely, for each $(i,j)$ set\n   $$\\mu_{ij}(X_{i}=1,X_{j}=-1)=\\mu_{ij}(X_{i}=-1,X_{j}=1)=\\tfrac{1}{2}, \\qquad \\mu_{ij}(X_{i}=X_{j})=0.$$\n   Verify that these three bivariate laws are pairwise consistent in the above sense, and prove that there is no probability measure $\\mu_{123}$ on $E^{\\{1,2,3\\}}$ whose pairwise marginals are the three $\\mu_{ij}$.\n\n3. To quantify the obstruction in part 2, consider arbitrary random variables $(X_{1},X_{2},X_{3})$ taking values in $E=\\{-1,1\\}$. Define\n   $$S \\;=\\; \\mathbb{P}(X_{1}=X_{2}) \\;+\\; \\mathbb{P}(X_{2}=X_{3}) \\;+\\; \\mathbb{P}(X_{3}=X_{1}).$$\n   Compute the sharp lower bound $L$ of $S$ over all joint laws on $E^{3}$, and use it to certify the impossibility in part 2 by comparing with the value of $S$ implied by the three prescribed bivariate laws. Your final answer should be the value of $L$ as a single real number. Do not include units.", "solution": "The problem asks for an explanation of why Kolmogorov consistency is a stricter condition than pairwise consistency, to demonstrate this with a specific construction, and to quantify the obstruction in that construction.\n\n### Part 1: Kolmogorov Consistency vs. Pairwise Consistency\n\nKolmogorov consistency for a family of finite-dimensional distributions $\\{\\mu_{I}\\}_{I \\subset T, |I| < \\infty}$ requires that for any two finite index sets $I \\subset J \\subset T$, the marginal distribution of $\\mu_J$ on the coordinates in $I$ is precisely $\\mu_I$. Symbolically, if $\\pi_{JI}: E^J \\to E^I$ is the projection map, then $\\mu_I = \\mu_J \\circ \\pi_{JI}^{-1}$. This condition ensures that all finite-dimensional distributions are compatible and can be \"glued together\" to define a probability measure on the (potentially infinite-dimensional) product space $E^T$, which is the essence of the Kolmogorov Extension Theorem.\n\nPairwise consistency, as defined in the problem, is a weaker condition. It only requires this marginalization property to hold for sets of size at most $2$. Specifically, for any three distinct indices $i, j, k \\in T$, the singleton marginals derived from the bivariate distributions must agree: the marginal of $\\mu_{\\{i,j\\}}$ for $X_i$ must be the same as the marginal of $\\mu_{\\{i,k\\}}$ for $X_i$. This ensures that there is a well-defined family of singleton distributions $\\{\\mu_{\\{i\\}}\\}_{i \\in T}$.\n\nKolmogorov consistency is strictly stronger because consistency of all pairs of variables does not guarantee consistency for triples, quadruples, and larger sets of variables. A set of pairwise-consistent bivariate distributions might contain structural incompatibilities that prevent them from being the marginals of a single, unified joint distribution over three or more variables. The construction in Part 2 provides a canonical example of this phenomenon.\n\n### Part 2: A Specific Counterexample\n\nWe are given $T=\\{1,2,3\\}$, the state space $E=\\{-1,1\\}$, and three bivariate laws $\\mu_{12}$, $\\mu_{23}$, and $\\mu_{31}$ defined for any pair $(i,j) \\in \\{(1,2), (2,3), (3,1)\\}$ as:\n$$\n\\mu_{ij}(X_i=x_i, X_j=x_j) = \\begin{cases} \\frac{1}{2} & \\text{if } x_i \\neq x_j \\\\ 0 & \\text{if } x_i = x_j \\end{cases}\n$$\n\n**Verification of Pairwise Consistency**\n\nWe must verify that the singleton marginals are consistent across these bivariate laws. Let's compute the marginal distribution for $X_1$.\nFrom $\\mu_{12}$:\n$$ \\mu_{12}(X_1=1) = \\mu_{12}(X_1=1, X_2=1) + \\mu_{12}(X_1=1, X_2=-1) = 0 + \\frac{1}{2} = \\frac{1}{2} $$\n$$ \\mu_{12}(X_1=-1) = \\mu_{12}(X_1=-1, X_2=1) + \\mu_{12}(X_1=-1, X_2=-1) = \\frac{1}{2} + 0 = \\frac{1}{2} $$\nSo the marginal for $X_1$ from $\\mu_{12}$ is the uniform distribution on $\\{-1,1\\}$.\n\nFrom $\\mu_{31}$ (or $\\mu_{13}$), the definition is symmetric in $i, j$, so the calculation is identical:\n$$ \\mu_{31}(X_1=1) = \\mu_{31}(X_3=-1, X_1=1) + \\mu_{31}(X_3=1, X_1=1) = \\frac{1}{2} + 0 = \\frac{1}{2} $$\n$$ \\mu_{31}(X_1=-1) = \\mu_{31}(X_3=1, X_1=-1) + \\mu_{31}(X_3=-1, X_1=-1) = \\frac{1}{2} + 0 = \\frac{1}{2} $$\nThe marginal for $X_1$ from $\\mu_{31}$ is also the uniform distribution. Thus, the marginals for $X_1$ agree.\n\nBy the symmetric nature of the definitions of $\\mu_{12}$, $\\mu_{23}$, and $\\mu_{31}$, the same result holds for $X_2$ (from $\\mu_{12}$ and $\\mu_{23}$) and for $X_3$ (from $\\mu_{23}$ and $\\mu_{31}$). In all cases, the singleton marginal for each $X_i$ is the uniform distribution on $\\{-1,1\\}$. Therefore, the family of bivariate laws is pairwise consistent.\n\n**Proof of Non-existence of a Joint Distribution**\n\nNow we prove that there is no probability measure $\\mu_{123}$ on $E^{\\{1,2,3\\}} = \\{-1,1\\}^3$ whose pairwise marginals are $\\mu_{12}$, $\\mu_{23}$, and $\\mu_{31}$.\nAssume, for the sake of contradiction, that such a measure $\\mu_{123}$ exists.\nThis would imply that the random variables $(X_1, X_2, X_3)$ governed by $\\mu_{123}$ must satisfy the conditions imposed by the marginals.\n1. The marginal for $(X_1, X_2)$ must be $\\mu_{12}$. This implies $\\mathbb{P}(X_1 = X_2) = 0$, or equivalently, $X_1 \\neq X_2$ almost surely. Since the variables are discrete, this means $X_1 X_2 = -1$ almost surely.\n2. The marginal for $(X_2, X_3)$ must be $\\mu_{23}$. This implies $X_2 \\neq X_3$ almost surely, which means $X_2 X_3 = -1$ almost surely.\n3. The marginal for $(X_3, X_1)$ must be $\\mu_{31}$. This implies $X_3 \\neq X_1$ almost surely, which means $X_3 X_1 = -1$ almost surely.\n\nIf such a joint distribution existed, all three conditions must hold simultaneously. Let's consider the product of these three relations:\n$$ (X_1 X_2) (X_2 X_3) (X_3 X_1) = (-1)(-1)(-1) = -1 $$\nThe left side of the equation can be rearranged:\n$$ (X_1 X_2) (X_2 X_3) (X_3 X_1) = X_1^2 X_2^2 X_3^2 = (X_1 X_2 X_3)^2 $$\nSince each $X_i$ takes values in $\\{-1,1\\}$, the product $X_1 X_2 X_3$ must also take values in $\\{-1,1\\}$. Therefore, its square must be $1$:\n$$ (X_1 X_2 X_3)^2 = 1 $$\nCombining these results, we arrive at the contradiction $1 = -1$.\nThis contradiction shows that our initial assumption was false. No such joint probability measure $\\mu_{123}$ can exist.\n\n### Part 3: Quantifying the Obstruction\n\nWe are asked to find the sharp lower bound $L$ for the quantity $S = \\mathbb{P}(X_{1}=X_{2}) + \\mathbb{P}(X_{2}=X_{3}) + \\mathbb{P}(X_{3}=X_{1})$ over all possible joint distributions for three random variables $(X_1, X_2, X_3)$ taking values in $E=\\{-1,1\\}$.\n\nLet $A_1$ be the event $X_1=X_2$, $A_2$ be the event $X_2=X_3$, and $A_3$ be the event $X_3=X_1$. The quantity is $S = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3)$.\n\nConsider any outcome $(x_1, x_2, x_3)$ in the sample space $\\{-1,1\\}^3$. By the pigeonhole principle, since there are $3$ variables but only $2$ possible values, at least two of the variables must take the same value. In other words, for any outcome, at least one of the conditions $x_1=x_2$, $x_2=x_3$, or $x_3=x_1$ must be true. This means that the union of the events $A_1, A_2, A_3$ covers the entire sample space:\n$$ A_1 \\cup A_2 \\cup A_3 = \\Omega $$\nTherefore, $\\mathbb{P}(A_1 \\cup A_2 \\cup A_3) = 1$.\n\nUsing the principle of inclusion-exclusion for three events:\n$$ \\mathbb{P}(A_1 \\cup A_2 \\cup A_3) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) - \\left( \\mathbb{P}(A_1 \\cap A_2) + \\mathbb{P}(A_2 \\cap A_3) + \\mathbb{P}(A_3 \\cap A_1) \\right) + \\mathbb{P}(A_1 \\cap A_2 \\cap A_3) $$\nRecognizing $S = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3)$, we have:\n$$ 1 = S - \\left( \\mathbb{P}(A_1 \\cap A_2) + \\mathbb{P}(A_2 \\cap A_3) + \\mathbb{P}(A_3 \\cap A_1) \\right) + \\mathbb{P}(A_1 \\cap A_2 \\cap A_3) $$\nLet's analyze the intersection events:\n- $A_1 \\cap A_2$ is the event $\\{X_1=X_2 \\text{ and } X_2=X_3\\}$, which is equivalent to $\\{X_1=X_2=X_3\\}$.\n- $A_2 \\cap A_3$ is the event $\\{X_2=X_3 \\text{ and } X_3=X_1\\}$, which is also $\\{X_1=X_2=X_3\\}$.\n- $A_3 \\cap A_1$ is the event $\\{X_3=X_1 \\text{ and } X_1=X_2\\}$, which is also $\\{X_1=X_2=X_3\\}$.\n- $A_1 \\cap A_2 \\cap A_3$ is clearly also the event $\\{X_1=X_2=X_3\\}$.\n\nLet $p_{\\text{all}} = \\mathbb{P}(X_1=X_2=X_3)$. The inclusion-exclusion formula simplifies to:\n$$ 1 = S - (p_{\\text{all}} + p_{\\text{all}} + p_{\\text{all}}) + p_{\\text{all}} $$\n$$ 1 = S - 3p_{\\text{all}} + p_{\\text{all}} $$\n$$ 1 = S - 2p_{\\text{all}} $$\nSolving for $S$, we get the identity:\n$$ S = 1 + 2p_{\\text{all}} = 1 + 2\\mathbb{P}(X_1=X_2=X_3) $$\nTo find the sharp lower bound $L$ of $S$, we need to find the minimum possible value of the right side. Since $\\mathbb{P}(X_1=X_2=X_3)$ is a probability, its value is bounded below by $0$. The minimum value of $S$ is achieved when $\\mathbb{P}(X_1=X_2=X_3)$ is minimized.\n$$ L = \\min(S) = 1 + 2 \\cdot \\min(\\mathbb{P}(X_1=X_2=X_3)) = 1 + 2 \\cdot 0 = 1 $$\nThis lower bound $L=1$ is achievable. For example, consider the joint distribution that places all probability mass on the outcome $(1, 1, -1)$. For this distribution, $\\mathbb{P}(X_1=X_2=X_3) = \\mathbb{P}(1,1,1) + \\mathbb{P}(-1,-1,-1) = 0 + 0 = 0$. The value of $S$ is $\\mathbb{P}(X_1=X_2) + \\mathbb{P}(X_2=X_3) + \\mathbb{P}(X_3=X_1) = 1 + 0 + 0 = 1$. Thus, the sharp lower bound is $L=1$.\n\n**Certification of Impossibility**\n\nThis result certifies the impossibility demonstrated in Part 2. Any valid joint probability distribution on $\\{-1,1\\}^3$ must satisfy $S \\ge 1$.\nHowever, the set of bivariate laws $\\{\\mu_{12}, \\mu_{23}, \\mu_{31}\\}$ from Part 2 would require a hypothetical joint distribution to have marginals satisfying:\n- $\\mathbb{P}(X_1=X_2) = \\mu_{12}(X_1=X_2) = 0$\n- $\\mathbb{P}(X_2=X_3) = \\mu_{23}(X_2=X_3) = 0$\n- $\\mathbb{P}(X_3=X_1) = \\mu_{31}(X_3=X_1) = 0$\nFor such a distribution, the value of $S$ would be $S = 0 + 0 + 0 = 0$.\nSince $0 < 1$, this violates the necessary condition $S \\ge 1$. This contradiction provides a quantitative proof that no such joint distribution can exist.\nThe sharp lower bound is $L=1$.", "answer": "$$\\boxed{1}$$", "id": "3062997"}]}