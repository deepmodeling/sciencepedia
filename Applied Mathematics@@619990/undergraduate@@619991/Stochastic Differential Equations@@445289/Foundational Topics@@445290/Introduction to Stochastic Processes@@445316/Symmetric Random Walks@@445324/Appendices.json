{"hands_on_practices": [{"introduction": "Understanding the path of a random walk requires calculating the likelihood of it visiting certain positions at specific times. This foundational exercise asks you to compute a joint probability for a simple symmetric random walk, reinforcing the crucial concepts of independent increments and the Markov property. Mastering this type of direct calculation is the first step toward analyzing more complex stochastic behaviors. [@problem_id:1313996]", "problem": "Let $X_i$ for $i \\ge 1$ be a sequence of independent and identically distributed random variables, where the probability of each outcome is $P(X_i = 1) = p$ and $P(X_i = -1) = 1-p$. A general one-dimensional random walk on the integers is defined by the process $S_n = \\sum_{i=1}^{n} X_i$ for $n \\ge 1$, with the starting position fixed at $S_0 = 0$.\n\nFor the specific case of a simple symmetric random walk, where $p = 1/2$, consider the positions of the walk at time $n=2$ and time $n=4$. Calculate the joint probability that the walk is at position 0 at time $n=2$ and at position 2 at time $n=4$, which is denoted by $P(S_2 = 0, S_4 = 2)$.\n\nExpress your answer as an exact fraction.", "solution": "We use the independence of increments of the simple symmetric random walk. By the Markov property and independent increments,\n$$\nP(S_{2}=0,\\,S_{4}=2)=P(S_{2}=0)\\,P(S_{4}=2\\,|\\,S_{2}=0)=P(S_{2}=0)\\,P(S_{4}-S_{2}=2).\n$$\nFirst, compute $P(S_{2}=0)$:\n$$\nS_{2}=X_{1}+X_{2},\\quad P(S_{2}=0)=P((X_{1},X_{2})=(1,-1)) + P((X_{1},X_{2})=(-1,1)).\n$$\nSince $P(X_{i}=1)=P(X_{i}=-1)=\\frac{1}{2}$ and the $X_{i}$ are independent,\n$$\nP(S_{2}=0)=2\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{2}.\n$$\nNext, compute $P(S_{4}-S_{2}=2)$:\n$$\nS_{4}-S_{2}=X_{3}+X_{4},\\quad P(S_{4}-S_{2}=2)=P(X_{3}=1,\\,X_{4}=1)=\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4}.\n$$\nTherefore,\n$$\nP(S_{2}=0,\\,S_{4}=2)=\\frac{1}{2}\\cdot\\frac{1}{4}=\\frac{1}{8}.\n$$", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "1313996"}, {"introduction": "We now move from tracking specific paths to analyzing behavior within boundaries, a classic scenario known as the \"Gambler's Ruin\" problem. This practice challenges you to derive the probability of a random walk hitting one boundary before another, a question with applications in finance and physics. By setting up and solving a difference equation, you will employ a powerful technique for handling such boundary value problems and also explore the elegant connection between biased and symmetric walks. [@problem_id:3079254]", "problem": "Let $\\{S_{n}\\}_{n\\geq 0}$ be a discrete-time nearest-neighbor random walk on $\\mathbb{Z}$ defined by $S_{0}=i$ and $S_{n}=\\sum_{k=1}^{n}X_{k}+i$, where $\\{X_{k}\\}_{k\\geq 1}$ are independent and identically distributed with $\\mathbb{P}(X_{k}=1)=p$ and $\\mathbb{P}(X_{k}=-1)=q$, where $p\\in(0,1)$, $q=1-p$, and $p\\neq \\frac{1}{2}$. Fix an integer $N\\geq 2$ and take an initial state $i\\in\\{1,2,\\dots,N-1\\}$. Define the hitting times $\\tau_{A}=\\inf\\{n\\geq 0:S_{n}\\in A\\}$ for $A\\subset\\mathbb{Z}$, and set $\\tau_{0}=\\tau_{\\{0\\}}$ and $\\tau_{N}=\\tau_{\\{N\\}}$.\n\nUsing only the definition of conditional expectation and the Markov property, derive an explicit closed-form expression for the function\n$$\nu(i)=\\mathbb{P}_{i}\\big(\\tau_{N}<\\tau_{0}\\big)\n$$\nwhen $p\\neq q$. Then, take the symmetric limit $p\\to \\frac{1}{2}$ and determine the corresponding limiting expression. Provide your final answer as two expressions in the order: the general biased case for $p\\neq q$ and the symmetric limit as $p\\to \\frac{1}{2}$. No numerical approximation is required, and no physical units are involved.", "solution": "The user wants to derive the probability of a biased random walk hitting state $N$ before state $0$, and then find the limit of this probability for a symmetric random walk.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A discrete-time nearest-neighbor random walk $\\{S_{n}\\}_{n\\geq 0}$ on $\\mathbb{Z}$.\n- Initial state: $S_{0}=i$.\n- Walk definition: $S_{n}=\\sum_{k=1}^{n}X_{k}+i$.\n- The steps $\\{X_{k}\\}_{k\\geq 1}$ are independent and identically distributed random variables.\n- Step probabilities: $\\mathbb{P}(X_{k}=1)=p$ and $\\mathbb{P}(X_{k}=-1)=q$, with $q=1-p$.\n- Constraints on $p$: $p\\in(0,1)$ and $p\\neq \\frac{1}{2}$ (which implies $p\\neq q$).\n- Boundary states are $0$ and $N$, with $N\\geq 2$.\n- The initial state $i$ is in the set $\\{1,2,\\dots,N-1\\}$.\n- Hitting time definition: $\\tau_{A}=\\inf\\{n\\geq 0:S_{n}\\in A\\}$.\n- Specific hitting times: $\\tau_{0}=\\tau_{\\{0\\}}$ and $\\tau_{N}=\\tau_{\\{N\\}}$.\n- Function to derive: $u(i)=\\mathbb{P}_{i}\\big(\\tau_{N}<\\tau_{0}\\big)$, where $\\mathbb{P}_{i}(\\cdot) = \\mathbb{P}(\\cdot | S_0=i)$.\n- Required derivation method: \"Using only the definition of conditional expectation and the Markov property\".\n- Final output requirements: Provide the expression for $u(i)$ for the case $p\\neq q$, and the limiting expression as $p\\to \\frac{1}{2}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a classic exercise in the theory of stochastic processes, commonly known as the Gambler's Ruin problem.\n- **Scientifically Grounded:** The problem is based on the fundamental principles of probability theory and the theory of random walks. It is scientifically and mathematically sound.\n- **Well-Posed:** The quantity to be calculated, $u(i)$, is a well-defined probability. The boundary conditions are implicitly defined by the problem setup, leading to a unique solution. For $i=0$, the walk has already hit $0$, so $\\tau_0=0$ and $\\tau_N>0$, meaning $\\mathbb{P}_0(\\tau_N<\\tau_0)=0$. For $i=N$, the walk has already hit $N$, so $\\tau_N=0$ and $\\tau_0>0$, meaning $\\mathbb{P}_N(\\tau_N<\\tau_0)=1$. The problem is well-posed.\n- **Objective:** The problem is stated in precise, formal mathematical language, with no ambiguity or subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the solution.\n\n### Derivation of the Expression for $u(i)$\n\nLet $u(i) = \\mathbb{P}_{i}(\\tau_{N} < \\tau_{0})$ be the probability that the random walk, starting from state $i$, hits state $N$ before hitting state $0$. We are interested in finding $u(i)$ for $i \\in \\{1, 2, \\dots, N-1\\}$.\n\nThe boundary conditions are determined by the definition of the event.\nIf the walk starts at $i=0$, it has hit state $0$ at time $n=0$. Thus, $\\tau_{0}=0$. Since $N \\ge 2$, it cannot be at state $N$ at time $n=0$, so $\\tau_{N}>0$. The event $\\tau_{N} < \\tau_{0}$ is impossible. Therefore,\n$$u(0) = 0$$\nIf the walk starts at $i=N$, it has hit state $N$ at time $n=0$. Thus, $\\tau_{N}=0$. Since $i \\neq 0$, $\\tau_{0}>0$. The event $\\tau_{N} < \\tau_{0}$ is certain. Therefore,\n$$u(N) = 1$$\n\nFor any interior state $i \\in \\{1, 2, \\dots, N-1\\}$, we can condition on the outcome of the first step, $S_{1}$. The first step is either to $i+1$ with probability $p$ or to $i-1$ with probability $q$. Using the law of total probability (or the definition of conditional expectation):\n$$u(i) = \\mathbb{P}_{i}(S_1=i+1) \\mathbb{P}_{i}(\\tau_N < \\tau_0 | S_1=i+1) + \\mathbb{P}_{i}(S_1=i-1) \\mathbb{P}_{i}(\\tau_N < \\tau_0 | S_1=i-1)$$\nBy definition, $\\mathbb{P}_{i}(S_1=i+1) = \\mathbb{P}(X_1=1) = p$ and $\\mathbb{P}_{i}(S_1=i-1) = \\mathbb{P}(X_1=-1) = q$.\n\nThe random walk has the Markov property, which means the future evolution of the process from state $S_1$ is independent of the path taken to reach $S_1$. Therefore, the probability of reaching $N$ before $0$ starting from $S_1=j$ is the same as the probability of reaching $N$ before $0$ if the process started at $j$ initially.\n$$\\mathbb{P}_{i}(\\tau_N < \\tau_0 | S_1=i+1) = \\mathbb{P}_{i+1}(\\tau_N<\\tau_0) = u(i+1)$$\n$$\\mathbb{P}_{i}(\\tau_N < \\tau_0 | S_1=i-1) = \\mathbb{P}_{i-1}(\\tau_N<\\tau_0) = u(i-1)$$\nSubstituting these into the equation for $u(i)$ yields a linear second-order homogeneous difference equation:\n$$u(i) = p \\cdot u(i+1) + q \\cdot u(i-1) \\quad \\text{for } i \\in \\{1, 2, \\dots, N-1\\}$$\n\nWe solve this equation subject to the boundary conditions $u(0)=0$ and $u(N)=1$.\nSince $p+q=1$, we can write $u(i) = (p+q)u(i)$. The equation becomes:\n$$p u(i) + q u(i) = p u(i+1) + q u(i-1)$$\nRearranging terms, we get:\n$$p(u(i+1) - u(i)) = q(u(i) - u(i-1))$$\nLet $\\Delta(i) = u(i) - u(i-1)$ be the difference. The relation is $\\Delta(i+1) = \\frac{q}{p} \\Delta(i)$. This shows that the differences form a geometric progression.\nThe general solution to the difference equation $u(i) = p u(i+1) + q u(i-1)$ can be found using its characteristic equation. Assuming a solution of the form $u(i) = r^i$, we substitute it into the rearranged equation $p u(i+1) - u(i) + q u(i-1) = 0$:\n$$p r^{i+1} - r^i + q r^{i-1} = 0$$\nDividing by $r^{i-1}$ (since $r=0$ is not a solution), we get the characteristic equation:\n$$p r^2 - r + q = 0$$\nThis quadratic equation can be factored as $(pr - q)(r - 1) = 0$.\nThe roots are $r_1=1$ and $r_2=\\frac{q}{p}$.\n\n**Case 1: Biased walk ($p \\neq q$)**\nSince $p \\neq q$, the roots $r_1=1$ and $r_2=\\frac{q}{p}$ are distinct. The general solution is a linear combination of the powers of the roots:\n$$u(i) = A \\cdot (r_1)^i + B \\cdot (r_2)^i = A + B \\left(\\frac{q}{p}\\right)^i$$\nWe determine the constants $A$ and $B$ using the boundary conditions $u(0)=0$ and $u(N)=1$.\nFrom $u(0)=0$:\n$$A + B \\left(\\frac{q}{p}\\right)^0 = A+B = 0 \\implies A = -B$$\nFrom $u(N)=1$:\n$$A + B \\left(\\frac{q}{p}\\right)^N = 1$$\nSubstituting $A=-B$ into the second equation:\n$$-B + B \\left(\\frac{q}{p}\\right)^N = 1 \\implies B\\left(\\left(\\frac{q}{p}\\right)^N - 1\\right) = 1$$\n$$B = \\frac{1}{\\left(\\frac{q}{p}\\right)^N - 1}$$\nAnd thus:\n$$A = -B = \\frac{-1}{\\left(\\frac{q}{p}\\right)^N - 1} = \\frac{1}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nSubstituting $A$ and $B$ back into the general solution:\n$$u(i) = \\frac{1}{1 - \\left(\\frac{q}{p}\\right)^N} - \\frac{1}{\\left(\\frac{q}{p}\\right)^N - 1} \\left(\\frac{q}{p}\\right)^i = \\frac{1 - \\left(\\frac{q}{p}\\right)^i}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nThis is the required expression for $u(i)$ when $p \\neq q$.\n\n**Case 2: Symmetric limit ($p \\to \\frac{1}{2}$)**\nWe are asked to find the limit of the expression for $u(i)$ as $p \\to \\frac{1}{2}$. As $p \\to \\frac{1}{2}$, we have $q = 1-p \\to \\frac{1}{2}$, and thus the ratio $\\frac{q}{p} \\to 1$.\nThe expression for $u(i)$ becomes an indeterminate form $\\frac{0}{0}$:\n$$\\lim_{p \\to 1/2} u(i) = \\lim_{\\frac{q}{p} \\to 1} \\frac{1 - \\left(\\frac{q}{p}\\right)^i}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nLet $x = \\frac{q}{p}$. We evaluate the limit $\\lim_{x \\to 1} \\frac{1 - x^i}{1 - x^N}$ using L'HÃ´pital's Rule. We differentiate the numerator and the denominator with respect to $x$:\n$$\\frac{d}{dx}(1-x^i) = -i x^{i-1}$$\n$$\\frac{d}{dx}(1-x^N) = -N x^{N-1}$$\nThe limit is:\n$$\\lim_{x \\to 1} \\frac{-i x^{i-1}}{-N x^{N-1}} = \\frac{-i \\cdot 1^{i-1}}{-N \\cdot 1^{N-1}} = \\frac{-i}{-N} = \\frac{i}{N}$$\nSo, in the symmetric case, the probability is $u(i) = \\frac{i}{N}$.\n\nWe can verify this result by solving the difference equation directly for $p=q=\\frac{1}{2}$.\nThe characteristic equation $pr^2 - r + q = 0$ becomes $\\frac{1}{2}r^2 - r + \\frac{1}{2} = 0$, or $r^2 - 2r + 1 = 0$, which is $(r-1)^2=0$.\nThis equation has a repeated root $r_1=r_2=1$. In this case, the general solution to the difference equation is:\n$$u(i) = A \\cdot (1)^i + B \\cdot i \\cdot (1)^i = A + Bi$$\nUsing the boundary conditions $u(0)=0$ and $u(N)=1$:\nFrom $u(0)=0$:\n$$A + B \\cdot 0 = 0 \\implies A=0$$\nFrom $u(N)=1$:\n$$A + B \\cdot N = 1$$\nSubstituting $A=0$ gives $BN=1$, so $B=\\frac{1}{N}$.\nThe solution is therefore $u(i) = \\frac{1}{N} \\cdot i = \\frac{i}{N}$, which confirms the result obtained from the limit.\n\nThe two expressions requested are:\n1. For $p \\neq q$: $u(i) = \\frac{1 - (q/p)^i}{1 - (q/p)^N}$\n2. The symmetric limit ($p \\to 1/2$): $u(i) = \\frac{i}{N}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1 - \\left(\\frac{q}{p}\\right)^{i}}{1 - \\left(\\frac{q}{p}\\right)^{N}} & \\frac{i}{N}\n\\end{pmatrix}\n}\n$$", "id": "3079254"}, {"introduction": "Having determined the probability of hitting a boundary, a natural next question is: how long does it take? This advanced practice introduces martingales and the Optional Stopping Theorem, a cornerstone of modern stochastic theory, to calculate the expected absorption time. By constructing a clever martingale, $S_n^2 - n$, you will see how this abstract framework provides an elegant and powerful shortcut to solving for quantities that are often difficult to compute directly. [@problem_id:3079267]", "problem": "Consider the discrete-time simple symmetric random walk on the integers. Let $N \\in \\mathbb{N}$ be fixed and let $S_{0} = i$ with $i \\in \\{0, 1, \\dots, N\\}$. For $n \\geq 1$, define $S_{n} = S_{n-1} + X_{n}$, where $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = \\tfrac{1}{2}$. Define the hitting times\n$$\n\\tau_{0} = \\inf\\{n \\geq 0 : S_{n} = 0\\}, \\qquad \\tau_{N} = \\inf\\{n \\geq 0 : S_{n} = N\\},\n$$\nand the absorption time $\\tau = \\tau_{0} \\wedge \\tau_{N}$. Using only fundamental definitions of martingales and the optional stopping theorem (OST) for bounded stopping times, derive a closed-form expression for the expected absorption time $E_{i}[\\tau]$ as a function of $i$ and $N$. Your final answer must be a single analytic expression. Do not provide intermediate formulas that directly reveal the final expression; instead, justify each step from first principles. No rounding is required.", "solution": "The problem asks for the expected absorption time, denoted $E_{i}[\\tau]$, for a simple symmetric random walk on the integers, starting at $S_{0} = i$, with absorbing barriers at $0$ and $N$. The absorption time is $\\tau = \\inf\\{n \\geq 0 : S_{n} \\in \\{0, N\\}\\}$. The solution will be derived using martingale theory and the optional stopping theorem (OST).\n\nFirst, let us formalize the process. The random walk is defined by $S_{n} = S_{n-1} + X_{n}$ for $n \\geq 1$, with $S_{0}=i$. The steps $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = 1/2$. Let $\\mathcal{F}_{n} = \\sigma(X_{1}, \\dots, X_{n})$ be the natural filtration of the process, with $\\mathcal{F}_{0} = \\{\\emptyset, \\Omega\\}$. The expectation conditional on starting at $S_0=i$ is denoted by $E_i[\\cdot]$.\n\nThe Optional Stopping Theorem (OST) states that for a martingale $(M_{n})_{n \\geq 0}$ and a stopping time $T$, under certain conditions, $E[M_{T}] = E[M_{0}]$. The problem specifies using the OST for bounded stopping times. The stopping time $\\tau$ is not deterministically bounded. However, for a one-dimensional random walk on a finite interval, it is known that $\\tau$ is almost surely finite and $E_i[\\tau] < \\infty$. A rigorous application of the OST involves considering the bounded stopping times $\\tau_{k} = \\tau \\wedge k = \\min(\\tau, k)$ for $k \\in \\mathbb{N}$. Applying the OST to $\\tau_k$ gives $E[M_{\\tau_k}] = E[M_0]$. We can then take the limit as $k \\to \\infty$ and, provided appropriate convergence conditions (such as the Dominated Convergence Theorem or Monotone Convergence Theorem) are met, we can deduce that $E[M_{\\tau}] = E[M_0]$. We will proceed with this understanding.\n\nOur derivation requires two distinct martingales.\n\nFirst, we determine the probability of absorption at each boundary. Let $p_{i} = \\mathbb{P}_{i}(S_{\\tau} = N)$ be the probability that the walk is absorbed at $N$, given it starts at $i$. Consequently, $\\mathbb{P}_{i}(S_{\\tau} = 0) = 1 - p_{i}$. To find $p_{i}$, we consider the process $(S_{n})_{n \\geq 0}$ itself. We verify it is a martingale with respect to the filtration $(\\mathcal{F}_{n})_{n \\geq 0}$. For any $n \\geq 1$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = E[S_{n-1} + X_{n} | \\mathcal{F}_{n-1}]\n$$\nSince $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable and $X_{n}$ is independent of $\\mathcal{F}_{n-1}$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1} + E[X_{n}]\n$$\nThe expectation of a single step is $E[X_{n}] = (1) \\cdot \\mathbb{P}(X_{n}=1) + (-1) \\cdot \\mathbb{P}(X_{n}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0$.\nThus, $E[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1}$, confirming that $(S_{n})_{n \\geq 0}$ is a martingale.\n\nThe increments of the martingale are bounded, $|S_{n} - S_{n-1}| = |X_{n}| = 1$. The stopping time $\\tau$ has a finite expectation. These are sufficient conditions for the OST to apply to the martingale $S_n$ and stopping time $\\tau$. Applying the OST:\n$$\nE_{i}[S_{\\tau}] = E_{i}[S_{0}] = i\n$$\nThe expected value of the position at absorption time $\\tau$ is given by:\n$$\nE_{i}[S_{\\tau}] = N \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + 0 \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N \\cdot p_{i}\n$$\nEquating the two expressions for $E_{i}[S_{\\tau}]$, we get $N \\cdot p_{i} = i$, which yields the absorption probability:\n$$\np_{i} = \\frac{i}{N}\n$$\n\nSecond, to find the expected time $E_{i}[\\tau]$, we need a martingale that incorporates the time index $n$. Let us consider the process $M_{n} = S_{n}^2 - n$. We verify if it is a martingale. For any $n \\geq 1$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = E[S_{n}^2 - n | \\mathcal{F}_{n-1}] = E[S_{n}^2 | \\mathcal{F}_{n-1}] - n\n$$\nWe expand $S_{n}^2 = (S_{n-1} + X_{n})^2 = S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2$.\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = E[S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nUsing the linearity of conditional expectation and the fact that $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}E[X_{n} | \\mathcal{F}_{n-1}] + E[X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nAs before, $E[X_{n} | \\mathcal{F}_{n-1}] = E[X_{n}] = 0$. For the term $E[X_{n}^2 | \\mathcal{F}_{n-1}]$, we have $E[X_{n}^2] = (1)^2 \\cdot \\frac{1}{2} + (-1)^2 \\cdot \\frac{1}{2} = 1$.\nSubstituting these results back:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}(0) + 1 = S_{n-1}^2 + 1\n$$\nNow we can complete the calculation for $E[M_{n} | \\mathcal{F}_{n-1}]$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = (S_{n-1}^2 + 1) - n = S_{n-1}^2 - (n-1) = M_{n-1}\n$$\nThis demonstrates that $(M_{n})_{n \\geq 0} = (S_{n}^2 - n)_{n \\geq 0}$ is indeed a martingale.\n\nNow we apply the OST to the martingale $M_{n}$ and the stopping time $\\tau$. The conditions for OST are met because $E_i[\\tau] < \\infty$ and the martingale increments are bounded over a finite time interval.\n$$\nE_{i}[M_{\\tau}] = E_{i}[M_{0}]\n$$\nThe initial value is $M_{0} = S_{0}^2 - 0 = i^2$.\nThe value at the stopping time is $M_{\\tau} = S_{\\tau}^2 - \\tau$. Its expectation is:\n$$\nE_{i}[M_{\\tau}] = E_{i}[S_{\\tau}^2 - \\tau] = E_{i}[S_{\\tau}^2] - E_{i}[\\tau]\n$$\nEquating the two expressions for the expectation gives:\n$$\nE_{i}[S_{\\tau}^2] - E_{i}[\\tau] = i^2\n$$\nWe can rearrange this to express the expected absorption time:\n$$\nE_{i}[\\tau] = E_{i}[S_{\\tau}^2] - i^2\n$$\nTo finalize the solution, we must compute $E_{i}[S_{\\tau}^2]$. The position at absorption, $S_{\\tau}$, can only be $0$ or $N$.\n$$\nE_{i}[S_{\\tau}^2] = (N^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + (0^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N^2 \\cdot p_{i}\n$$\nUsing our previously derived result for the absorption probability, $p_{i} = i/N$:\n$$\nE_{i}[S_{\\tau}^2] = N^2 \\cdot \\left(\\frac{i}{N}\\right) = Ni\n$$\nFinally, substituting this back into our expression for $E_{i}[\\tau]$:\n$$\nE_{i}[\\tau] = Ni - i^2\n$$\nThis can be factored to give the final closed-form expression for the expected absorption time as a function of the starting position $i$ and the boundary $N$.\n$$\nE_{i}[\\tau] = i(N-i)\n$$\nThis result is valid for any starting position $i \\in \\{0, 1, \\dots, N\\}$. If $i=0$ or $i=N$, the starting position is already at an absorbing boundary, so $\\tau=0$, and the formula correctly yields $0(N-0)=0$ and $N(N-N)=0$, respectively.", "answer": "$$\n\\boxed{i(N-i)}\n$$", "id": "3079267"}]}