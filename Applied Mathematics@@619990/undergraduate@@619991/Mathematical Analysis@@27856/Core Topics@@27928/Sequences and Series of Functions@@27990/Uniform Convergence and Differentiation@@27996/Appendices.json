{"hands_on_practices": [{"introduction": "This first exercise serves as a crucial cautionary tale in the study of function sequences. We will analyze the seemingly simple sequence $f_n(x) = \\frac{x^n}{n}$ on the interval $[0, 1]$, which converges to a well-behaved function. By calculating the derivative of the limit and the limit of the derivatives separately, you will uncover a discrepancy that highlights why pointwise convergence alone is insufficient for interchanging these two fundamental operations of calculus. [@problem_id:2332580]", "problem": "Consider the sequence of real-valued functions $\\{f_n\\}_{n=1}^{\\infty}$ defined on the closed interval $I = [0, 1]$ by the formula:\n$$f_n(x) = \\frac{x^n}{n}$$\nLet $f(x)$ be the pointwise limit of this sequence, i.e., $f(x) = \\lim_{n \\to \\infty} f_n(x)$ for all $x \\in I$.\nLet $g(x)$ be the pointwise limit of the sequence of derivatives $\\{f_n'\\}_{n=1}^{\\infty}$, i.e., $g(x) = \\lim_{n \\to \\infty} f_n'(x)$, where $f_n'(x)$ denotes the derivative of $f_n(x)$ with respect to $x$.\n\nCompute the value of $g(1) - f'(1)$, where $f'(1)$ is the derivative of $f(x)$ evaluated at $x=1$.", "solution": "For each $n \\in \\mathbb{N}$, the function is $f_{n}(x) = \\frac{x^{n}}{n}$ on $[0,1]$. Its derivative is\n$$\nf_{n}'(x) = \\frac{n x^{n-1}}{n} = x^{n-1}.\n$$\nFirst, compute the pointwise limit $f(x) = \\lim_{n \\to \\infty} f_{n}(x)$. For $x \\in [0,1)$, we have $0 \\leq \\frac{x^{n}}{n} \\leq x^{n}$ for all $n \\geq 1$, and since $\\lim_{n \\to \\infty} x^{n} = 0$, the squeeze theorem implies\n$$\n\\lim_{n \\to \\infty} \\frac{x^{n}}{n} = 0 \\quad \\text{for } x \\in [0,1).\n$$\nAt $x=1$, $f_{n}(1) = \\frac{1}{n} \\to 0$. At $x=0$, $f_{n}(0) = 0$ for all $n$. Therefore,\n$$\nf(x) \\equiv 0 \\quad \\text{on } [0,1].\n$$\nHence the derivative of the limit function is\n$$\nf'(x) = 0 \\quad \\text{for } x \\in (0,1),\n$$\nand the one-sided derivative at the endpoint $x=1$ (from the left) also equals $0$, so\n$$\nf'(1) = 0.\n$$\nNext, compute $g(x) = \\lim_{n \\to \\infty} f_{n}'(x) = \\lim_{n \\to \\infty} x^{n-1}$. For $x \\in [0,1)$, $\\lim_{n \\to \\infty} x^{n-1} = 0$. At $x=1$,\n$$\ng(1) = \\lim_{n \\to \\infty} 1^{n-1} = 1.\n$$\nTherefore,\n$$\ng(1) - f'(1) = 1 - 0 = 1.\n$$", "answer": "$$\\boxed{1}$$", "id": "2332580"}, {"introduction": "After observing how the interchange of limit and derivative can fail, we now turn to a powerful class of functions where it is guaranteed to succeed: power series. This practice involves the Maclaurin series for $f(x) = \\ln(1-x)$, a cornerstone example in calculus. You will see how the property of uniform convergence within the interval of convergence provides the rigorous justification for term-by-term differentiation, a technique you have likely used before. [@problem_id:1343029]", "problem": "Consider the function $f(x) = \\ln(1-x)$. The Maclaurin series for this function is given by\n$$ S(x) = -\\sum_{n=1}^{\\infty} \\frac{x^n}{n} $$\nThe interval of convergence for this power series is $x \\in [-1, 1)$.\n\nBy formally differentiating the series term-by-term, one obtains a new series:\n$$ G(x) = -\\sum_{n=1}^{\\infty} \\frac{d}{dx}\\left(\\frac{x^n}{n}\\right) = -\\sum_{n=1}^{\\infty} x^{n-1} $$\nFor this procedure to be mathematically rigorous, the equality $f'(x) = G(x)$ must be justified by the standard theorems on the differentiation of series of functions.\n\nDetermine the values of $a$ and $b$ that define the largest possible open interval $I = (a, b)$ for which the term-by-term differentiation of the series $S(x)$ is valid.", "solution": "The problem asks for the largest open interval $(a, b)$ where the term-by-term differentiation of the Maclaurin series for $f(x) = \\ln(1-x)$ is valid. The given series is a power series centered at $x_0 = 0$.\n\nThe central theorem governing the differentiation of power series states that if a power series $S(x) = \\sum_{n=0}^{\\infty} c_n (x-x_0)^n$ has a radius of convergence $R > 0$, then the function $f(x)$ defined by the series is differentiable on the open interval of convergence $(x_0 - R, x_0 + R)$. Furthermore, its derivative $f'(x)$ can be found by differentiating the series term-by-term. The resulting series of derivatives has the same radius of convergence $R$.\n\nLet's apply this theorem to the given series $S(x) = -\\sum_{n=1}^{\\infty} \\frac{x^n}{n}$. This is a power series centered at $x_0=0$ with coefficients $c_n = -\\frac{1}{n}$ for $n \\ge 1$ and $c_0 = 0$.\n\nFirst, we must determine the radius of convergence, $R$, for the series $S(x)$. We can use the ratio test for the absolute values of the terms. Let $a_n = \\frac{x^n}{n}$.\n$$ \\lim_{n \\to \\infty} \\left| \\frac{a_{n+1}}{a_n} \\right| = \\lim_{n \\to \\infty} \\left| \\frac{x^{n+1}}{n+1} \\cdot \\frac{n}{x^n} \\right| = \\lim_{n \\to \\infty} \\left| x \\cdot \\frac{n}{n+1} \\right| = |x| \\lim_{n \\to \\infty} \\frac{n}{n+1} = |x| \\cdot 1 = |x| $$\nThe series converges when this limit is less than 1, so we require $|x|  1$. This implies that the radius of convergence is $R=1$.\n\nAccording to the theorem on term-by-term differentiation of power series, the differentiation is valid within the open interval of convergence. For a series centered at $x_0=0$ with $R=1$, this interval is $(0 - R, 0 + R) = (-1, 1)$.\n\nSo, for any $x \\in (-1, 1)$, we can write:\n$$ f'(x) = \\frac{d}{dx} \\left( \\ln(1-x) \\right) = \\frac{d}{dx} \\left( -\\sum_{n=1}^{\\infty} \\frac{x^n}{n} \\right) = -\\sum_{n=1}^{\\infty} \\frac{d}{dx}\\left(\\frac{x^n}{n}\\right) = -\\sum_{n=1}^{\\infty} \\frac{nx^{n-1}}{n} = -\\sum_{n=1}^{\\infty} x^{n-1} $$\nThis resulting series is $G(x)$.\n\nWe need to ensure that this is the *largest* such open interval. The theorem guarantees validity on $(-1, 1)$. We must check if the interval can be extended. The series of derivatives is $G(x) = -\\sum_{n=1}^{\\infty} x^{n-1} = -(1 + x + x^2 + \\dots)$. This is a geometric series which diverges if $|x| \\ge 1$. For the equality $f'(x) = G(x)$ to hold, the series $G(x)$ must converge. Since $G(x)$ diverges for $x \\ge 1$ and for $x \\le -1$, the term-by-term differentiation cannot be valid outside the interval $(-1, 1)$. Therefore, $(-1, 1)$ is the largest open interval on which the operation is valid.\n\nThe problem asks for the values of $a$ and $b$ for the interval $(a, b)$. Comparing with our result $(-1, 1)$, we have $a = -1$ and $b = 1$.", "answer": "$$\\boxed{\\begin{pmatrix}-1  1\\end{pmatrix}}$$", "id": "1343029"}, {"introduction": "Our final practice pushes the boundaries of our theoretical understanding, presenting a more subtle scenario. We will examine a sequence of functions that, along with its derivatives, converges to the zero function, allowing for the interchange of limit and differentiation. However, you will demonstrate that the sequence of derivatives does not converge uniformly, revealing that the standard theorem for this interchange provides a *sufficient* condition, but not a *necessary* one. [@problem_id:2332578]", "problem": "Consider the sequence of real-valued functions $(f_n)_{n=1}^{\\infty}$ defined on the entire real line $\\mathbb{R}$ by\n$$ f_n(x) = \\frac{\\alpha}{n} \\exp(-n^2 x^2) $$\nwhere $\\alpha$ is a positive real constant.\n\nIt can be shown that this sequence of functions, $(f_n)$, converges uniformly to the zero function, $f(x)=0$. Furthermore, the corresponding sequence of derivatives, $(f_n')$, converges pointwise to the zero function, $g(x)=0$.\n\nLet $S_n = \\sup_{x \\in \\mathbb{R}} |f_n(x)|$ and $D_n = \\sup_{x \\in \\mathbb{R}} |f_n'(x)|$. The condition for uniform convergence of $(f_n)$ to zero is $\\lim_{n \\to \\infty} S_n = 0$. Similarly, the condition for uniform convergence of $(f_n')$ to zero is $\\lim_{n \\to \\infty} D_n = 0$.\n\nFor the given sequence, the sequence of derivatives $(f_n')$ fails to converge uniformly to the zero function. Your task is to calculate the value of the limit $L = \\lim_{n \\to \\infty} D_n$.\n\nExpress your answer as a closed-form analytic expression in terms of $\\alpha$ and fundamental mathematical constants.", "solution": "We are given $f_{n}(x) = \\frac{\\alpha}{n}\\exp(-n^{2}x^{2})$ with $\\alpha0$. First compute the derivative:\n$$\nf_{n}'(x) = \\frac{\\alpha}{n}\\cdot \\exp(-n^{2}x^{2})\\cdot (-2n^{2}x) = -2\\alpha n x\\,\\exp(-n^{2}x^{2}).\n$$\nHence the absolute value is\n$$\n|f_{n}'(x)| = 2\\alpha n|x|\\,\\exp(-n^{2}x^{2}).\n$$\nDefine $D_{n} = \\sup_{x\\in\\mathbb{R}}|f_{n}'(x)|$. To evaluate $D_{n}$, perform the change of variables $y = nx$, which gives $|x| = |y|/n$ and, as $x$ ranges over $\\mathbb{R}$, so does $y$. Then\n$$\n|f_{n}'(x)| = 2\\alpha n\\left(\\frac{|y|}{n}\\right)\\exp(-y^{2}) = 2\\alpha |y|\\,\\exp(-y^{2}).\n$$\nTherefore\n$$\nD_{n} = \\sup_{y\\in\\mathbb{R}} 2\\alpha |y|\\,\\exp(-y^{2}) = 2\\alpha \\sup_{y\\ge 0} y\\,\\exp(-y^{2}).\n$$\nLet $h(y) = y\\,\\exp(-y^{2})$ for $y\\ge 0$. Differentiate:\n$$\nh'(y) = \\exp(-y^{2})(1 - 2y^{2}).\n$$\nCritical points satisfy $h'(y)=0$, i.e., $1 - 2y^{2} = 0$, which gives $y = \\frac{1}{\\sqrt{2}}$ (the nonnegative solution). Since $h(0) = 0$ and $\\lim_{y\\to\\infty}h(y) = 0$, and there is a single critical point on $[0,\\infty)$, this point yields the global maximum. Its value is\n$$\nh\\!\\left(\\frac{1}{\\sqrt{2}}\\right) = \\frac{1}{\\sqrt{2}}\\,\\exp\\!\\left(-\\frac{1}{2}\\right).\n$$\nThus\n$$\nD_{n} = 2\\alpha \\cdot \\frac{1}{\\sqrt{2}}\\,\\exp\\!\\left(-\\frac{1}{2}\\right) = \\alpha \\sqrt{2}\\,\\exp\\!\\left(-\\frac{1}{2}\\right),\n$$\nwhich is independent of $n$. Therefore the limit exists and equals this constant:\n$$\nL = \\lim_{n\\to\\infty} D_{n} = \\alpha \\sqrt{2}\\,\\exp\\!\\left(-\\frac{1}{2}\\right).\n$$\nThis shows $(f_{n}')$ does not converge uniformly to zero.", "answer": "$$\\boxed{\\alpha \\sqrt{2}\\,\\exp\\!\\left(-\\frac{1}{2}\\right)}$$", "id": "2332578"}]}