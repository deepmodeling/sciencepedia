{"hands_on_practices": [{"introduction": "The familiar dot product in $\\mathbb{R}^n$ is just one example of a broader concept known as an inner product. This powerful idea can be extended to more abstract vector spaces, such as spaces of matrices, allowing us to apply geometric intuition like length and orthogonality in surprising new contexts. This exercise [@problem_id:1367530] provides a concrete opportunity to work with the Frobenius inner product, a natural way to define an inner product for matrices, and apply the fundamental concept of orthogonality.", "problem": "In the vector space of $2 \\times 2$ real matrices, $M_2(\\mathbb{R})$, the Frobenius inner product is defined for any two matrices $A$ and $B$ as $\\langle A, B \\rangle = \\text{tr}(A^T B)$, where $A^T$ is the transpose of $A$ and $\\text{tr}(C)$ denotes the trace of a matrix $C$.\n\nConsider the following two matrices in $M_2(\\mathbb{R})$:\n$$A = \\begin{pmatrix} 1 & -2 \\\\ 3 & 4 \\end{pmatrix}$$\n$$B = \\begin{pmatrix} \\alpha & -1 \\\\ 1 & 2\\alpha \\end{pmatrix}$$\nwhere $\\alpha$ is a real number.\n\nTwo matrices are said to be orthogonal with respect to a given inner product if their inner product is zero. Determine the value of $\\alpha$ for which the matrix $A$ is orthogonal to the matrix $B$ with respect to the Frobenius inner product.", "solution": "The problem asks for the value of the parameter $\\alpha$ that makes the matrices $A$ and $B$ orthogonal with respect to the Frobenius inner product. The condition for orthogonality is $\\langle A, B \\rangle = 0$. The Frobenius inner product is defined as $\\langle A, B \\rangle = \\text{tr}(A^T B)$.\n\nFirst, we need to find the transpose of matrix $A$. The given matrix is:\n$$A = \\begin{pmatrix} 1 & -2 \\\\ 3 & 4 \\end{pmatrix}$$\nThe transpose, $A^T$, is obtained by interchanging the rows and columns:\n$$A^T = \\begin{pmatrix} 1 & 3 \\\\ -2 & 4 \\end{pmatrix}$$\n\nNext, we compute the matrix product $A^T B$. The matrix $B$ is given by:\n$$B = \\begin{pmatrix} \\alpha & -1 \\\\ 1 & 2\\alpha \\end{pmatrix}$$\nThe product $A^T B$ is:\n$$A^T B = \\begin{pmatrix} 1 & 3 \\\\ -2 & 4 \\end{pmatrix} \\begin{pmatrix} \\alpha & -1 \\\\ 1 & 2\\alpha \\end{pmatrix}$$\nPerforming the matrix multiplication:\nThe element in the first row, first column is $(1)(\\alpha) + (3)(1) = \\alpha + 3$.\nThe element in the first row, second column is $(1)(-1) + (3)(2\\alpha) = -1 + 6\\alpha$.\nThe element in the second row, first column is $(-2)(\\alpha) + (4)(1) = -2\\alpha + 4$.\nThe element in the second row, second column is $(-2)(-1) + (4)(2\\alpha) = 2 + 8\\alpha$.\nSo, the resulting matrix is:\n$$A^T B = \\begin{pmatrix} \\alpha + 3 & 6\\alpha - 1 \\\\ 4 - 2\\alpha & 8\\alpha + 2 \\end{pmatrix}$$\n\nNow, we calculate the trace of this product matrix, $\\text{tr}(A^T B)$. The trace is the sum of the elements on the main diagonal.\n$$\\text{tr}(A^T B) = (\\alpha + 3) + (8\\alpha + 2)$$\nCombining like terms:\n$$\\text{tr}(A^T B) = (\\alpha + 8\\alpha) + (3 + 2) = 9\\alpha + 5$$\n\nFor the matrices $A$ and $B$ to be orthogonal, their inner product must be zero:\n$$\\langle A, B \\rangle = \\text{tr}(A^T B) = 0$$\nSubstituting the expression we found for the trace:\n$$9\\alpha + 5 = 0$$\nNow, we solve for $\\alpha$:\n$$9\\alpha = -5$$\n$$\\alpha = -\\frac{5}{9}$$\n\nThus, the value of $\\alpha$ for which $A$ and $B$ are orthogonal is $-\\frac{5}{9}$.", "answer": "$$\\boxed{-\\frac{5}{9}}$$", "id": "1367530"}, {"introduction": "While standard bases are convenient, orthogonal bases offer significant computational advantages, especially when dealing with projections and decompositions. The Gram-Schmidt process is the essential algorithmic tool for transforming any basis into an orthogonal one. This practice problem [@problem_id:2302687] will guide you through applying this procedure in the space of polynomials, reinforcing how a systematic, step-by-step process can build an orthogonal set from any linearly independent set.", "problem": "Let $P_2(\\mathbb{R})$ be the vector space of all polynomials of degree at most 2 with real coefficients. An inner product on this space is defined by\n$$ \\langle p, q \\rangle = \\int_{0}^{1} p(x)q(x) dx $$\nfor any two polynomials $p(x), q(x) \\in P_2(\\mathbb{R})$.\nConsider the subspace $W$ of $P_2(\\mathbb{R})$ spanned by the set of linearly independent vectors $\\{v_1(x), v_2(x)\\}$, where $v_1(x) = 1$ and $v_2(x) = 2x+1$.\nUse the Gram-Schmidt process, starting with $v_1(x)$, to find an orthogonal basis $\\{w_1(x), w_2(x)\\}$ for the subspace $W$.\nPresent your answer as a row matrix of the two basis polynomials, in the format $\\begin{pmatrix} w_1(x) & w_2(x) \\end{pmatrix}$.", "solution": "We work in the inner product space $P_{2}(\\mathbb{R})$ with inner product defined by $\\langle p, q \\rangle = \\int_{0}^{1} p(x) q(x) \\, dx$. The Gram-Schmidt process on $\\{v_{1}, v_{2}\\}$ with $v_{1}(x)=1$ and $v_{2}(x)=2x+1$ proceeds as follows.\n\nFirst, set $w_{1}(x)=v_{1}(x)=1$. Compute its norm squared using the definition of the inner product:\n$$\n\\langle w_{1}, w_{1} \\rangle = \\int_{0}^{1} 1 \\cdot 1 \\, dx = \\int_{0}^{1} 1 \\, dx = 1.\n$$\n\nNext, compute the projection coefficient of $v_{2}$ onto $w_{1}$:\n$$\n\\langle v_{2}, w_{1} \\rangle = \\int_{0}^{1} (2x+1)\\cdot 1 \\, dx = \\int_{0}^{1} (2x+1)\\, dx = \\left[ x^{2} + x \\right]_{0}^{1} = 1 + 1 = 2.\n$$\nTherefore,\n$$\n\\frac{\\langle v_{2}, w_{1} \\rangle}{\\langle w_{1}, w_{1} \\rangle} = \\frac{2}{1} = 2.\n$$\n\nDefine\n$$\nw_{2}(x) = v_{2}(x) - \\frac{\\langle v_{2}, w_{1} \\rangle}{\\langle w_{1}, w_{1} \\rangle} w_{1}(x) = (2x+1) - 2\\cdot 1 = 2x - 1.\n$$\n\nVerify orthogonality:\n$$\n\\langle w_{1}, w_{2} \\rangle = \\int_{0}^{1} 1 \\cdot (2x-1) \\, dx = \\int_{0}^{1} (2x-1)\\, dx = \\left[ x^{2} - x \\right]_{0}^{1} = (1 - 1) - 0 = 0.\n$$\nThus $\\{w_{1}(x), w_{2}(x)\\} = \\{1, 2x - 1\\}$ is an orthogonal basis for $W$.\n\nHence, the requested row matrix is $\\begin{pmatrix} 1 & 2x - 1 \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 2x - 1 \\end{pmatrix}}$$", "id": "2302687"}, {"introduction": "One of the most compelling applications of inner product spaces is finding the \"best\" approximation of a complex function using simpler ones. This idea is central to fields from signal processing to numerical analysis. By projecting a function onto a subspace, we find the element in that subspace that is \"closest\" to our original function. This exercise [@problem_id:2154976] beautifully illustrates this principle by asking you to find the best quadratic polynomial approximation to a sine wave, effectively minimizing the error in a least-squares sense.", "problem": "In many areas of signal processing and numerical analysis, it is useful to approximate a complex transcendental function with a simpler polynomial function over a specific interval. The \"best\" approximation is often defined in the least-squares sense, where one seeks to minimize the total squared error between the two functions.\n\nConsider the function $f(x) = \\sin(\\pi x)$ defined on the interval $[-1, 1]$. Your task is to find the polynomial $p(x)$ of degree at most two that provides the best approximation to $f(x)$ on this interval. The best-approximating polynomial $p(x)$ is the one that minimizes the mean-square error integral, given by:\n$$E = \\int_{-1}^{1} [f(x) - p(x)]^2 \\, dx$$\nExpress the resulting polynomial $p(x)$ as a function of $x$ and the constant $\\pi$.", "solution": "We approximate $f(x)=\\sin(\\pi x)$ on $[-1,1]$ by a polynomial $p(x)$ of degree at most two in the least-squares sense with respect to the inner product $\\langle g,h\\rangle=\\int_{-1}^{1} g(x)h(x)\\,dx$. The best approximation $p$ is the orthogonal projection of $f$ onto the subspace $\\mathcal{P}_{2}=\\operatorname{span}\\{1,x,x^{2}\\}$. Equivalently, the residual $r(x)=f(x)-p(x)$ must be orthogonal to each basis element:\n$$\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,dx=0,\\quad\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,x\\,dx=0,\\quad\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,x^{2}\\,dx=0.\n$$\nLet $p(x)=a+bx+cx^{2}$. Then the normal equations are\n$$\n\\int_{-1}^{1} f(x)\\,dx-a\\int_{-1}^{1}1\\,dx-b\\int_{-1}^{1}x\\,dx-c\\int_{-1}^{1}x^{2}\\,dx=0,\n$$\n$$\n\\int_{-1}^{1} f(x)\\,x\\,dx-a\\int_{-1}^{1}x\\,dx-b\\int_{-1}^{1}x^{2}\\,dx-c\\int_{-1}^{1}x^{3}\\,dx=0,\n$$\n$$\n\\int_{-1}^{1} f(x)\\,x^{2}\\,dx-a\\int_{-1}^{1}x^{2}\\,dx-b\\int_{-1}^{1}x^{3}\\,dx-c\\int_{-1}^{1}x^{4}\\,dx=0.\n$$\nWe use parity and basic integrals on $[-1,1]$:\n- $f(x)=\\sin(\\pi x)$ is odd, so $\\int_{-1}^{1} f(x)\\,dx=0$ and $\\int_{-1}^{1} f(x)\\,x^{2}\\,dx=0$.\n- $\\int_{-1}^{1} x\\,dx=0$, $\\int_{-1}^{1} x^{3}\\,dx=0$.\n- $\\int_{-1}^{1} x^{2}\\,dx=\\frac{2}{3}$, $\\int_{-1}^{1} x^{4}\\,dx=\\frac{2}{5}$.\n- To compute $\\int_{-1}^{1} f(x)\\,x\\,dx=\\int_{-1}^{1} x\\sin(\\pi x)\\,dx$, integrate by parts with $u=x$, $dv=\\sin(\\pi x)\\,dx$, so $du=dx$ and $v=-\\cos(\\pi x)/\\pi$. Then\n$$\n\\int_{-1}^{1} x\\sin(\\pi x)\\,dx=\\left[-\\frac{x\\cos(\\pi x)}{\\pi}\\right]_{-1}^{1}+\\int_{-1}^{1}\\frac{\\cos(\\pi x)}{\\pi}\\,dx\n=\\left[-\\frac{x\\cos(\\pi x)}{\\pi}\\right]_{-1}^{1}+\\left[\\frac{\\sin(\\pi x)}{\\pi^{2}}\\right]_{-1}^{1}.\n$$\nUsing $\\cos(\\pi)=-1$, $\\cos(-\\pi)=-1$, and $\\sin(\\pm\\pi)=0$, we obtain\n$$\n\\int_{-1}^{1} x\\sin(\\pi x)\\,dx=\\left(\\frac{1}{\\pi}\\right)-\\left(-\\frac{1}{\\pi}\\right)=\\frac{2}{\\pi}.\n$$\nSubstituting these into the normal equations, we get\n$$\n0-2a-0-c\\cdot\\frac{2}{3}=0,\\qquad \\frac{2}{\\pi}-0-b\\cdot\\frac{2}{3}-0=0,\\qquad 0-a\\cdot\\frac{2}{3}-0-c\\cdot\\frac{2}{5}=0.\n$$\nThe first and third equations are\n$$\n2a+\\frac{2}{3}c=0,\\qquad \\frac{2}{3}a+\\frac{2}{5}c=0.\n$$\nSolving, subtracting appropriate multiples yields $c=0$, and then $a=0$. The second equation reduces to\n$$\n\\frac{2}{\\pi}-b\\cdot\\frac{2}{3}=0 \\quad\\Rightarrow\\quad b=\\frac{3}{\\pi}.\n$$\nTherefore, the best $L^{2}$ approximation of degree at most two is\n$$\np(x)=\\frac{3}{\\pi}\\,x.\n$$", "answer": "$$\\boxed{\\frac{3}{\\pi}x}$$", "id": "2154976"}]}