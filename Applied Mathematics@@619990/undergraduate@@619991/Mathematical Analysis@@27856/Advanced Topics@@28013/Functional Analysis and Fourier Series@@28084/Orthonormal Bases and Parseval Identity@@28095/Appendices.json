{"hands_on_practices": [{"introduction": "Let's begin our journey into orthonormal bases with a familiar setting: the three-dimensional space we experience every day, $\\mathbb{R}^3$. While we often use the standard $(x, y, z)$ axes, any set of three mutually perpendicular unit vectors can form a valid basis. This exercise demonstrates a profound principle: the length of a vector is an intrinsic property and does not depend on the orthonormal basis you choose to measure it in. By working through this problem, you will gain a concrete understanding of Parseval's identity in its simplest form, seeing how it elegantly connects geometry and algebra. [@problem_id:2310339]", "problem": "Consider the vector space $\\mathbb{R}^3$. Let $\\mathcal{B} = \\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}$ be an orthonormal basis for $\\mathbb{R}^3$, where the basis vectors are defined as:\n$$\n\\mathbf{u}_1 = \\frac{1}{\\sqrt{2}}(1, 1, 0)\n$$\n$$\n\\mathbf{u}_2 = \\frac{1}{\\sqrt{3}}(1, -1, 1)\n$$\n$$\n\\mathbf{u}_3 = \\frac{1}{\\sqrt{6}}(1, -1, -2)\n$$\nA vector $\\mathbf{v} = (1, 2, 3)$ in $\\mathbb{R}^3$ can be uniquely expressed as a linear combination of these basis vectors: $\\mathbf{v} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2 + c_3 \\mathbf{u}_3$. The coefficients $(c_1, c_2, c_3)$ are the coordinates of $\\mathbf{v}$ with respect to the basis $\\mathcal{B}$.\n\nCalculate the numerical value of the expression $c_1^2 + c_2^2 + c_3^2$.", "solution": "Because $\\mathcal{B}=\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\mathbf{u}_{3}\\}$ is an orthonormal basis, the coordinates are $c_{i}=\\mathbf{v}\\cdot\\mathbf{u}_{i}$ and the norm-squared identity (Parseval) holds:\n$$\n\\|\\mathbf{v}\\|^{2}=\\mathbf{v}\\cdot\\mathbf{v}=\\left(\\sum_{i=1}^{3}c_{i}\\mathbf{u}_{i}\\right)\\cdot\\left(\\sum_{j=1}^{3}c_{j}\\mathbf{u}_{j}\\right)=\\sum_{i,j=1}^{3}c_{i}c_{j}\\left(\\mathbf{u}_{i}\\cdot\\mathbf{u}_{j}\\right)=\\sum_{i=1}^{3}c_{i}^{2},\n$$\nsince $\\mathbf{u}_{i}\\cdot\\mathbf{u}_{j}=\\delta_{ij}$ for an orthonormal basis.\n\nTherefore,\n$$\nc_{1}^{2}+c_{2}^{2}+c_{3}^{2}=\\|\\mathbf{v}\\|^{2}=\\mathbf{v}\\cdot\\mathbf{v}=1^{2}+2^{2}+3^{2}=14.\n$$", "answer": "$$\\boxed{14}$$", "id": "2310339"}, {"introduction": "Now, we expand our perspective from finite-dimensional vectors to the infinite-dimensional world of functions. In the space of square-integrable functions $L^2[-1, 1]$, we can use an orthogonal basis, like the Legendre polynomials, to approximate more complex functions. This practice asks you to find the \"best\" quadratic approximation of the function $f(x) = |x|$, which amounts to finding its orthogonal projection onto the subspace spanned by the first three Legendre polynomials. This technique of projecting a function onto a simpler subspace is a cornerstone of approximation theory and is essential for modeling and data compression in science and engineering. [@problem_id:2310334]", "problem": "Consider the vector space $L^2[-1, 1]$ of square-integrable real functions on the interval $[-1, 1]$, equipped with the inner product defined as $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)g(x) \\, dx$.\n\nThe first three Legendre polynomials form an orthogonal basis for a subspace of $L^2[-1, 1]$. They are given by:\n- $P_0(x) = 1$\n- $P_1(x) = x$\n- $P_2(x) = \\frac{1}{2}(3x^2 - 1)$\n\nA key property of Legendre polynomials is their orthogonality, which for any non-negative integers $n$ and $m$ is expressed by the relation:\n$$ \\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1}\\delta_{nm} $$\nwhere $\\delta_{nm}$ is the Kronecker delta.\n\nLet $W$ be the subspace of $L^2[-1, 1]$ spanned by the set $\\{P_0(x), P_1(x), P_2(x)\\}$. Determine the orthogonal projection of the function $f(x) = |x|$ onto the subspace $W$. Express your answer as a polynomial in $x$.", "solution": "We seek the orthogonal projection of $f(x)=|x|$ onto $W=\\operatorname{span}\\{P_{0},P_{1},P_{2}\\}$ with respect to the inner product $\\langle f,g\\rangle=\\int_{-1}^{1} f(x)g(x)\\,dx$. For an orthogonal basis $\\{P_{n}\\}$, the projection is\n$$\n\\operatorname{proj}_{W} f=\\sum_{n=0}^{2} a_{n} P_{n}(x), \\quad a_{n}=\\frac{\\langle f,P_{n}\\rangle}{\\langle P_{n},P_{n}\\rangle}.\n$$\nUsing the orthogonality relation $\\int_{-1}^{1} P_{n}(x)P_{m}(x)\\,dx=\\frac{2}{2n+1}\\delta_{nm}$, we have $\\langle P_{n},P_{n}\\rangle=\\frac{2}{2n+1}$, hence\n$$\na_{n}=\\frac{2n+1}{2}\\int_{-1}^{1} f(x)P_{n}(x)\\,dx.\n$$\nSince $f(x)=|x|$ is even, and $P_{0}$ and $P_{2}$ are even while $P_{1}$ is odd, it follows that\n$$\n\\int_{-1}^{1} |x|\\,P_{1}(x)\\,dx=0,\n$$\nso only $n=0$ and $n=2$ contribute.\n\nFor $n=0$,\n$$\n\\langle |x|,P_{0}\\rangle=\\int_{-1}^{1} |x|\\,dx=2\\int_{0}^{1} x\\,dx=2\\left[\\frac{x^{2}}{2}\\right]_{0}^{1}=1,\n$$\nthus\n$$\na_{0}=\\frac{1}{2}\\cdot 1=\\frac{1}{2}.\n$$\n\nFor $n=2$, using $P_{2}(x)=\\frac{1}{2}(3x^{2}-1)$ and evenness,\n$$\n\\langle |x|,P_{2}\\rangle=\\int_{-1}^{1} |x|\\,\\frac{1}{2}(3x^{2}-1)\\,dx=2\\int_{0}^{1} x\\cdot \\frac{1}{2}(3x^{2}-1)\\,dx=\\int_{0}^{1} x(3x^{2}-1)\\,dx.\n$$\nCompute the integral:\n$$\n\\int_{0}^{1} x(3x^{2}-1)\\,dx=\\int_{0}^{1} (3x^{3}-x)\\,dx=\\left[\\frac{3}{4}x^{4}-\\frac{1}{2}x^{2}\\right]_{0}^{1}=\\frac{3}{4}-\\frac{1}{2}=\\frac{1}{4}.\n$$\nTherefore,\n$$\na_{2}=\\frac{5}{2}\\cdot \\frac{1}{4}=\\frac{5}{8}.\n$$\n\nAssembling the projection,\n$$\n\\operatorname{proj}_{W} f(x)=a_{0}P_{0}(x)+a_{2}P_{2}(x)=\\frac{1}{2}\\cdot 1+\\frac{5}{8}\\cdot \\frac{1}{2}(3x^{2}-1)=\\frac{1}{2}+\\frac{5}{16}(3x^{2}-1).\n$$\nSimplifying,\n$$\n\\operatorname{proj}_{W} f(x)=\\frac{15}{16}x^{2}+\\frac{3}{16}.\n$$", "answer": "$$\\boxed{\\frac{15}{16} x^{2} + \\frac{3}{16}}$$", "id": "2310334"}, {"introduction": "This final exercise challenges you to generalize the concept of \"best approximation\" even further. Not all errors are created equal; in some physical or statistical problems, we might care more about accuracy in one region of an interval than another. We can encode this preference by introducing a weight function $w(x)$ into our inner product. This problem asks for the best linear approximation of $f(x) = x^2$ in a weighted space, forcing you to adapt the projection method to a non-standard way of measuring distance. Mastering this demonstrates a deeper understanding of how the abstract framework of inner product spaces can be flexibly applied to solve diverse, real-world problems. [@problem_id:2310315]", "problem": "Consider the vector space of real-valued functions on the interval $[0, 1]$ that are square-integrable with respect to the weight function $w(x)=x$. This space, which we can denote as $L^2_w([0,1])$, is equipped with the inner product defined by:\n$$\n\\langle f, g \\rangle = \\int_0^1 f(x)g(x)w(x) dx\n$$\nThe \"best\" approximation of a function $f(x)$ by a polynomial from a given subspace is the one that minimizes the squared norm of the error, defined as $\\|f - p\\|^2 = \\langle f - p, f - p \\rangle$.\n\nFind the best linear polynomial approximation, $p(x) = c_1 x + c_0$, for the function $f(x) = x^2$ in the space $L^2_w([0,1])$ with the specified weight function $w(x)=x$.", "solution": "We work in the weighted inner-product space with weight $w(x)=x$, so for $f,g$ the inner product is $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)x\\,dx$. The best linear approximation $p(x)=c_{1}x+c_{0}$ is the orthogonal projection of $f(x)=x^{2}$ onto the subspace spanned by $\\{1,x\\}$, characterized by the normal equations\n$$\n\\langle f-p,1\\rangle=0,\\qquad \\langle f-p,x\\rangle=0.\n$$\nExplicitly,\n$$\n\\int_{0}^{1}\\big(x^{2}-c_{1}x-c_{0}\\big)\\,x\\,dx=0,\\qquad\n\\int_{0}^{1}\\big(x^{2}-c_{1}x-c_{0}\\big)\\,x^{2}\\,dx=0.\n$$\nCompute each integral:\n$$\n\\int_{0}^{1}(x^{2}-c_{1}x-c_{0})x\\,dx=\\int_{0}^{1}(x^{3}-c_{1}x^{2}-c_{0}x)\\,dx=\\frac{1}{4}-\\frac{c_{1}}{3}-\\frac{c_{0}}{2}=0,\n$$\n$$\n\\int_{0}^{1}(x^{2}-c_{1}x-c_{0})x^{2}\\,dx=\\int_{0}^{1}(x^{4}-c_{1}x^{3}-c_{0}x^{2})\\,dx=\\frac{1}{5}-\\frac{c_{1}}{4}-\\frac{c_{0}}{3}=0.\n$$\nThis gives the linear system\n$$\n\\frac{1}{3}c_{1}+\\frac{1}{2}c_{0}=\\frac{1}{4},\\qquad \\frac{1}{4}c_{1}+\\frac{1}{3}c_{0}=\\frac{1}{5}.\n$$\nSolving by Cramer's rule, with $D=\\frac{1}{3}\\cdot\\frac{1}{3}-\\frac{1}{2}\\cdot\\frac{1}{4}=-\\frac{1}{72}$,\n$$\nc_{1}=\\frac{\\frac{1}{4}\\cdot\\frac{1}{3}-\\frac{1}{5}\\cdot\\frac{1}{2}}{D}=\\frac{-\\frac{1}{60}}{-\\frac{1}{72}}=\\frac{6}{5},\\qquad\nc_{0}=\\frac{\\frac{1}{3}\\cdot\\frac{1}{5}-\\frac{1}{4}\\cdot\\frac{1}{4}}{D}=\\frac{\\frac{1}{240}}{-\\frac{1}{72}}=-\\frac{3}{10}.\n$$\nTherefore, the best weighted $L^{2}$ approximation by a linear polynomial is\n$$\np(x)=\\frac{6}{5}x-\\frac{3}{10}.\n$$", "answer": "$$\\boxed{\\frac{6}{5}x-\\frac{3}{10}}$$", "id": "2310315"}]}