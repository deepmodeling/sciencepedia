{"hands_on_practices": [{"introduction": "The theorem of norm equivalence can feel abstract. The best way to make it concrete is to calculate the equivalence constants for a familiar space like $\\mathbb{R}^2$. This first exercise [@problem_id:1859194] asks you to compare two of the most common norms: the Euclidean norm ($\\|x\\|_2$), which measures straight-line distance, and the taxicab norm ($\\|x\\|_1$), which measures distance as if navigating a city grid. By finding the tightest possible bounds relating these two norms, you will build a foundational intuition for what norm equivalence really means geometrically.", "problem": "In the vector space $\\mathbb{R}^2$, consider two different ways to measure the \"length\" of a vector $x = (x_1, x_2)$. The first is the taxicab norm, or $\\ell_1$-norm, defined as $\\|x\\|_1 = |x_1| + |x_2|$. The second is the standard Euclidean norm, or $\\ell_2$-norm, defined as $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$.\n\nA fundamental theorem in functional analysis states that on any finite-dimensional vector space, all norms are equivalent. For the norms $\\|x\\|_1$ and $\\|x\\|_2$ on $\\mathbb{R}^2$, this means there exist positive constants $C_1$ and $C_2$ such that the following double inequality holds for all vectors $x \\in \\mathbb{R}^2$:\n$$C_1 \\|x\\|_2 \\le \\|x\\|_1 \\le C_2 \\|x\\|_2$$\nYour task is to find the *optimal* pair of constants $(C_1, C_2)$. This means you must find the largest possible value for $C_1$ and the smallest possible value for $C_2$ for which the inequalities are true for every vector in $\\mathbb{R}^2$.\n\nWhich of the following pairs represents the optimal constants $(C_1, C_2)$?\n\nA. $(1, \\sqrt{2})$\n\nB. $(\\sqrt{2}, 2)$\n\nC. $(1/2, 1)$\n\nD. $(1, 2)$\n\nE. $(1/\\sqrt{2}, \\sqrt{2})$", "solution": "We seek the largest $C_{1}$ and smallest $C_{2}$ such that $C_{1}\\|x\\|_{2} \\le \\|x\\|_{1} \\le C_{2}\\|x\\|_{2}$ holds for all $x \\in \\mathbb{R}^{2}$. For any nonzero $x$, define the homogeneous ratio\n$$\nr(x) = \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = \\frac{|x_{1}| + |x_{2}|}{\\sqrt{x_{1}^{2} + x_{2}^{2}}}.\n$$\nSince $r(\\alpha x) = r(x)$ for all $\\alpha \\neq 0$, it suffices to analyze $r$ on the unit circle $S = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} = 1\\}$. On $S$, we have\n$$\nr(x) = |x_{1}| + |x_{2}|.\n$$\nUpper bound: By the Cauchyâ€“Schwarz inequality,\n$$\n|x_{1}| + |x_{2}| \\le \\sqrt{1^{2} + 1^{2}}\\,\\sqrt{x_{1}^{2} + x_{2}^{2}} = \\sqrt{2}.\n$$\nEquality holds when $|x_{1}| = |x_{2}|$ and $x$ is aligned with $(1,1)$ with nonnegative coordinates, specifically $x_{1} = x_{2} = \\frac{1}{\\sqrt{2}}$ on $S$. Hence\n$$\n\\sup_{x \\neq 0} \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = \\sqrt{2},\n$$\nso the optimal $C_{2} = \\sqrt{2}$.\n\nLower bound: For any $x$,\n$$\n(|x_{1}| + |x_{2}|)^{2} = x_{1}^{2} + x_{2}^{2} + 2|x_{1}x_{2}| \\ge x_{1}^{2} + x_{2}^{2}.\n$$\nOn $S$ this gives $|x_{1}| + |x_{2}| \\ge 1$. Equality holds when $|x_{1}x_{2}| = 0$, i.e., one coordinate is zero. On $S$ this is achieved at $(\\pm 1, 0)$ or $(0, \\pm 1)$. Therefore\n$$\n\\inf_{x \\neq 0} \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = 1,\n$$\nso the optimal $C_{1} = 1$.\n\nThus the optimal pair is $(C_{1}, C_{2}) = (1, \\sqrt{2})$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1859194"}, {"introduction": "The power of linear algebra lies in its abstract nature, where 'vectors' can be arrows, polynomials, or even matrices. This practice [@problem_id:2308369] extends our investigation to the vector space of $2 \\times 2$ matrices, $M_2(\\mathbb{R})$. You will explore the equivalence between the Frobenius norm, analogous to the vector Euclidean norm, and the maximum entry norm, a simple measure of the largest component. This problem reinforces that the principles of norm equivalence are not confined to $\\mathbb{R}^n$ but apply to any finite-dimensional space.", "problem": "Consider the vector space $V = M_2(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. For any matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ in $V$, we define two different ways to measure its size, known as norms:\n\n1.  The **maximum absolute entry norm**, denoted by $\\|A\\|_{\\max}$, is given by $\\|A\\|_{\\max} = \\max\\{|a|, |b|, |c|, |d|\\}$.\n2.  The **Frobenius norm**, denoted by $\\|A\\|_{F}$, is given by $\\|A\\|_{F} = \\sqrt{a^2 + b^2 + c^2 + d^2}$.\n\nIt is a known result that for this space, there exist positive constants $C_1$ and $C_2$ such that for every non-zero matrix $A \\in V$, the following inequality holds:\n$$\nC_1 \\|A\\|_{\\max} \\leq \\|A\\|_{F} \\leq C_2 \\|A\\|_{\\max}\n$$\nDetermine the values of the constants $C_1$ and $C_2$ that make this inequality as tight as possible. Specifically, find the largest possible value for $C_1$ and the smallest possible value for $C_2$.\n\nPresent your answer for the constants $C_1$ and $C_2$, in that order.", "solution": "Identify $V$ with $\\mathbb{R}^{4}$ via the entries of $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$. Let $m = \\|A\\|_{\\max} = \\max\\{|a|,|b|,|c|,|d|\\}$. Then, by definition of the Frobenius norm,\n$$\n\\|A\\|_{F} = \\sqrt{a^{2} + b^{2} + c^{2} + d^{2}}.\n$$\n\nFor the lower bound, note that at least one of $|a|,|b|,|c|,|d|$ equals $m$, hence $a^{2} + b^{2} + c^{2} + d^{2} \\geq m^{2}$. Therefore,\n$$\n\\|A\\|_{F} \\geq m = \\|A\\|_{\\max}.\n$$\nThis shows the inequality holds with $C_{1} = 1$. To see this is optimal, take $A$ with exactly one nonzero entry, say $A = \\begin{pmatrix} t  0 \\\\ 0  0 \\end{pmatrix}$ with $t \\neq 0$. Then $\\|A\\|_{F} = |t| = \\|A\\|_{\\max}$, so any $C_{1}  1$ would violate $C_{1}\\|A\\|_{\\max} \\leq \\|A\\|_{F}$.\n\nFor the upper bound, since each of $|a|,|b|,|c|,|d|$ is at most $m$, we have\n$$\na^{2} + b^{2} + c^{2} + d^{2} \\leq 4 m^{2},\n$$\nhence\n$$\n\\|A\\|_{F} \\leq 2 m = 2 \\|A\\|_{\\max}.\n$$\nThus the inequality holds with $C_{2} = 2$. To see minimality, take all entries equal to a common nonzero value $t$, i.e., $A = \\begin{pmatrix} t  t \\\\ t  t \\end{pmatrix}$ with $t \\neq 0$. Then $\\|A\\|_{\\max} = |t|$ and $\\|A\\|_{F} = \\sqrt{4 t^{2}} = 2|t|$, so any $C_{2}  2$ would fail.\n\nTherefore, the largest possible $C_{1}$ is $1$, and the smallest possible $C_{2}$ is $2$.", "answer": "$$\\boxed{\\begin{pmatrix} 1  2 \\end{pmatrix}}$$", "id": "2308369"}, {"introduction": "Beyond standard $p$-norms, we can construct custom norms tailored for specific applications, often using matrices to define a unique geometry. This problem [@problem_id:2308400] introduces a norm defined by a symmetric positive-definite matrix $P$, given by $\\|x\\|_P = \\sqrt{x^T P x}$. This type of norm is crucial in fields like optimization and physics. This exercise will guide you to discover the profound link between the norm equivalence constants and the eigenvalues of the matrix $P$, showcasing a powerful analytical tool known as the Rayleigh quotient.", "problem": "In the vector space $\\mathbb{R}^2$, the familiar Euclidean norm of a vector $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ is given by $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$. However, other norms can be defined. Consider the matrix $P$ given by\n$$\nP = \\begin{pmatrix} 5  -2 \\\\ -2  2 \\end{pmatrix}.\n$$\nThis matrix can be used to define a new norm, the $P$-norm, for any vector $x \\in \\mathbb{R}^2$ as $\\|x\\|_P = \\sqrt{x^T P x}$, where $x^T$ is the transpose of $x$.\n\nIn a finite-dimensional vector space, any two norms are equivalent. Two norms, $\\|\\cdot\\|_a$ and $\\|\\cdot\\|_b$, are said to be equivalent if there exist positive real constants $c_1$ and $c_2$ such that the inequality $c_1 \\|x\\|_a \\le \\|x\\|_b \\le c_2 \\|x\\|_a$ holds for all vectors $x$ in the space.\n\nFind the best possible positive constants $c_1$ and $c_2$ that satisfy the equivalence relation $c_1 \\|x\\|_2 \\le \\|x\\|_P \\le c_2 \\|x\\|_2$ for all $x \\in \\mathbb{R}^2$. The \"best possible\" constants are those that make the bounds as tight as possible. Present your final answer as the pair of values $(c_1, c_2)$, with each value rounded to three significant figures.", "solution": "We seek the sharp constants $c_{1},c_{2}  0$ such that for all $x \\in \\mathbb{R}^{2}$,\n$$\nc_{1}\\|x\\|_{2} \\le \\|x\\|_{P} \\le c_{2}\\|x\\|_{2},\n$$\nwhere $\\|x\\|_{P} = \\sqrt{x^{T}Px}$ and $\\|x\\|_{2} = \\sqrt{x^{T}x}$. Squaring the inequality and using homogeneity, this is equivalent to finding the smallest and largest possible values of the Rayleigh quotient\n$$\nR(x) = \\frac{x^{T}Px}{x^{T}x}\n$$\nover all nonzero $x$. For a real symmetric matrix $P$, the Rayleigh quotient satisfies\n$$\n\\lambda_{\\min}(P) \\le R(x) \\le \\lambda_{\\max}(P)\n$$\nfor all nonzero $x$, with equality attained at eigenvectors corresponding to the extreme eigenvalues. Therefore, the best constants are\n$$\nc_{1} = \\sqrt{\\lambda_{\\min}(P)}, \\quad c_{2} = \\sqrt{\\lambda_{\\max}(P)}.\n$$\n\nWe compute the eigenvalues of $P = \\begin{pmatrix} 5  -2 \\\\ -2  2 \\end{pmatrix}$ from its characteristic polynomial:\n$$\n\\det(P - \\lambda I) = (5 - \\lambda)(2 - \\lambda) - (-2)(-2) = \\lambda^{2} - 7\\lambda + 6.\n$$\nSolving $\\lambda^{2} - 7\\lambda + 6 = 0$ yields\n$$\n\\lambda = \\frac{7 \\pm \\sqrt{49 - 24}}{2} = \\frac{7 \\pm 5}{2},\n$$\nso $\\lambda_{\\min}(P) = 1$ and $\\lambda_{\\max}(P) = 6$. Hence\n$$\nc_{1} = \\sqrt{1} = 1, \\quad c_{2} = \\sqrt{6}.\n$$\nRounding each to three significant figures gives $c_{1} = 1.00$ and $c_{2} = 2.45$.", "answer": "$$\\boxed{\\begin{pmatrix} 1.00  2.45 \\end{pmatrix}}$$", "id": "2308400"}]}