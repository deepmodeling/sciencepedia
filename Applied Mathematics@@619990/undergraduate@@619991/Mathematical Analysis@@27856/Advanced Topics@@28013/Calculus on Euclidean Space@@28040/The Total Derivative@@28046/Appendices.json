{"hands_on_practices": [{"introduction": "The total derivative is more than just a matrix of partial derivatives; it is the linear transformation that best approximates a function's behavior near a point. This first exercise makes that abstract concept tangible. You will compute the Jacobian matrix of a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ and then apply it to a vector $\\vec{v}$, directly calculating how the function transforms an infinitesimal displacement at the input. [@problem_id:37812]", "problem": "Consider a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by:\n$$\nf(x,y) = (f_1(x,y), f_2(x,y)) = (xy, x-y)\n$$\nThe total derivative of $f$ at a point $\\vec{p} = (x_0, y_0)$, denoted as $Df_{\\vec{p}}$, is a linear transformation that can be represented by the Jacobian matrix of $f$ evaluated at $\\vec{p}$, which we call $J_f(\\vec{p})$. The action of this total derivative on a vector $\\vec{v}$ is given by the matrix-vector product $Df_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}$.\n\nGiven the point $\\vec{p} = (2, 1)$ and the vector $\\vec{v} = (3, -1)$, compute the resulting vector from the action of the total derivative of $f$ at $\\vec{p}$ on the vector $\\vec{v}$. Express your final answer as a column vector.", "solution": "The problem asks for the computation of $Df_{\\vec{p}}(\\vec{v})$, where $f(x,y) = (xy, x-y)$, $\\vec{p} = (2, 1)$, and $\\vec{v} = (3, -1)$.\n\nThe total derivative's action on a vector is given by the product of the Jacobian matrix and the vector:\n$$\nDf_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}\n$$\n\nFirst, we must find the Jacobian matrix of the function $f(x,y)$. The components of $f$ are $f_1(x,y) = xy$ and $f_2(x,y) = x-y$. The Jacobian matrix, $J_f(x,y)$, is a $2 \\times 2$ matrix of the partial derivatives of $f$:\n$$\nJ_f(x,y) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{pmatrix}\n$$\n\nWe compute each partial derivative:\n- $\\frac{\\partial f_1}{\\partial x} = \\frac{\\partial}{\\partial x}(xy) = y$\n- $\\frac{\\partial f_1}{\\partial y} = \\frac{\\partial}{\\partial y}(xy) = x$\n- $\\frac{\\partial f_2}{\\partial x} = \\frac{\\partial}{\\partial x}(x-y) = 1$\n- $\\frac{\\partial f_2}{\\partial y} = \\frac{\\partial}{\\partial y}(x-y) = -1$\n\nSubstituting these into the Jacobian matrix gives:\n$$\nJ_f(x,y) = \\begin{pmatrix} y & x \\\\ 1 & -1 \\end{pmatrix}\n$$\n\nNext, we evaluate this matrix at the specified point $\\vec{p} = (x_0, y_0) = (2, 1)$:\n$$\nJ_f(2,1) = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\n\nNow, we can compute the action of the total derivative on the vector $\\vec{v} = (3, -1)$. We can write $\\vec{v}$ as a column vector $\\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$.\n$$\nDf_{(2,1)}(\\vec{v}) = J_f(2,1) \\vec{v} = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n$$\n\nFinally, we perform the matrix-vector multiplication:\n$$\n\\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (2)(-1) \\\\ (1)(3) + (-1)(-1) \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 3 + 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n$$\n\nThe resulting vector is $(1, 4)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}}\n$$", "id": "37812"}, {"introduction": "The chain rule is a cornerstone of calculus, and its power truly shines in multivariable settings like machine learning and signal processing. This problem explores a typical scenario where a data vector $\\mathbf{x} \\in \\mathbb{R}^n$ is first transformed by a projection matrix $P$ before being evaluated by a cost function $f$. By applying the chain rule, you will derive an elegant and widely used formula for the gradient of the overall process, illustrating how total derivatives are essential tools for analyzing and optimizing complex systems. [@problem_id:2330045]", "problem": "In many machine learning and signal processing applications, a high-dimensional data vector is first projected onto a lower-dimensional subspace before further analysis. This projection serves as a form of dimensionality reduction or feature extraction.\n\nConsider such a system where an input data vector $\\mathbf{x} \\in \\mathbb{R}^n$ is operated on by a linear transformation represented by a matrix $P \\in M_n(\\mathbb{R})$. The matrix $P$ is a projection matrix, which means it is idempotent, satisfying the property $P^2 = P$.\n\nAfter this transformation, a differentiable, real-valued cost function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is applied to the resulting vector. The overall operation is thus described by the composite function $g(\\mathbf{x}) = f(P\\mathbf{x})$.\n\nFor a scalar function $h: \\mathbb{R}^n \\to \\mathbb{R}$, its gradient at a point $\\mathbf{a}$, denoted $\\nabla h(\\mathbf{a})$, is defined as the column vector of its partial derivatives, i.e., $\\nabla h(\\mathbf{a}) = \\left(\\frac{\\partial h}{\\partial x_1}(\\mathbf{a}), \\frac{\\partial h}{\\partial x_2}(\\mathbf{a}), \\dots, \\frac{\\partial h}{\\partial x_n}(\\mathbf{a})\\right)^T$. The derivative (or Jacobian matrix) of $h$ at $\\mathbf{a}$, denoted $Dh(\\mathbf{a})$, is the $1 \\times n$ row vector $(\\nabla h(\\mathbf{a}))^T$.\n\nDetermine the gradient of the function $g$ at an arbitrary point $\\mathbf{x} \\in \\mathbb{R}^n$. Your final answer should be an expression involving the matrix $P$, its transpose $P^T$, and the gradient of $f$ evaluated at the appropriate point.", "solution": "We are given $g(\\mathbf{x}) = f(P\\mathbf{x})$, with $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ differentiable and $P\\in M_{n}(\\mathbb{R})$ linear and idempotent. To determine $\\nabla g(\\mathbf{x})$, we use the chain rule for differentiable maps between Euclidean spaces.\n\nFirst, since the transformation given by $P$ is linear, its derivative at any point $\\mathbf{x}$ is the constant matrix itself:\n$$\nD(P\\mathbf{x}) = P.\n$$\nNext, the derivative (Jacobian) of $f$ at a point $\\mathbf{y}\\in\\mathbb{R}^{n}$ is the $1\\times n$ row vector $Df(\\mathbf{y}) = \\left(\\nabla f(\\mathbf{y})\\right)^{T}$. Applying the chain rule to $g(\\mathbf{x})=f(P\\mathbf{x})$ yields\n$$\nDg(\\mathbf{x}) = Df(P\\mathbf{x})\\,D(P\\mathbf{x}) = Df(P\\mathbf{x})\\,P.\n$$\nBy the given convention, the gradient is the transpose of the Jacobian:\n$$\n\\nabla g(\\mathbf{x}) = \\left(Dg(\\mathbf{x})\\right)^{T} = \\left(Df(P\\mathbf{x})\\,P\\right)^{T} = P^{T}\\left(Df(P\\mathbf{x})\\right)^{T}.\n$$\nSince $\\left(Df(P\\mathbf{x})\\right)^{T} = \\nabla f(P\\mathbf{x})$, we conclude\n$$\n\\nabla g(\\mathbf{x}) = P^{T}\\,\\nabla f(P\\mathbf{x}).\n$$\nThe idempotence $P^{2}=P$ is not required for this derivation; only the linearity of the transformation by $P$ and differentiability of $f$ are used.", "answer": "$$\\boxed{P^{T}\\,\\nabla f(P\\mathbf{x})}$$", "id": "2330045"}, {"introduction": "The power of the total derivative extends far beyond functions on Euclidean space. This final practice moves into the more abstract setting of matrix calculus, asking you to find the derivative of the determinant function, which maps $2 \\times 2$ matrices to real numbers. By calculating how the determinant responds to a small perturbation around the identity matrix $I_2$, you will uncover a fundamental and elegant relationship between the total derivative, the determinant, and the matrix trace. [@problem_id:2330088]", "problem": "Consider the vector space $M_2(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. The determinant of a matrix in this space can be seen as a function that maps matrices to real numbers. Let's define this function as $f: M_2(\\mathbb{R}) \\to \\mathbb{R}$ where $f(X) = \\det(X)$. We are interested in how the value of this function changes for matrices that are very close to the identity matrix.\n\nThe total derivative of $f$ at the identity matrix $I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, denoted as $Df(I_2)$, is a linear map from $M_2(\\mathbb{R})$ to $\\mathbb{R}$ that provides the best linear approximation for the change in $f$ near $I_2$.\n\nYour task is to find a symbolic expression for the action of this total derivative on an arbitrary matrix $H \\in M_2(\\mathbb{R})$. Let the matrix $H$ be represented by its components as:\n$$\nH = \\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\end{pmatrix}\n$$\nDetermine the value of $Df(I_2)(H)$, which represents the first-order change in the determinant as the identity matrix is perturbed by the matrix $H$. Express your answer solely in terms of the components of $H$ (i.e., $h_{11}, h_{12}, h_{21}, h_{22}$).", "solution": "Let $f(X) = \\det(X)$ for $X \\in M_{2}(\\mathbb{R})$. The total derivative $Df(I_{2})$ applied to a direction $H$ can be computed via the directional derivative along the curve $X(t) = I_{2} + tH$, using the definition of the Fr√©chet derivative:\n$$\nDf(I_{2})(H) = \\left.\\frac{d}{dt}\\right|_{t=0} \\det(I_{2} + tH).\n$$\nWrite $H = \\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\end{pmatrix}$. Then\n$$\nI_{2} + tH = \\begin{pmatrix} 1 + t h_{11} & t h_{12} \\\\ t h_{21} & 1 + t h_{22} \\end{pmatrix}.\n$$\nUsing the $2 \\times 2$ determinant formula,\n$$\n\\det(I_{2} + tH) = (1 + t h_{11})(1 + t h_{22}) - (t h_{12})(t h_{21}) = 1 + t(h_{11} + h_{22}) + t^{2}(h_{11} h_{22} - h_{12} h_{21}).\n$$\nDifferentiate with respect to $t$ and evaluate at $t=0$:\n$$\n\\left.\\frac{d}{dt}\\right|_{t=0} \\det(I_{2} + tH) = h_{11} + h_{22}.\n$$\nTherefore,\n$$\nDf(I_{2})(H) = h_{11} + h_{22},\n$$\nwhich is the trace of $H$ expressed in terms of its components.", "answer": "$$\\boxed{h_{11}+h_{22}}$$", "id": "2330088"}]}