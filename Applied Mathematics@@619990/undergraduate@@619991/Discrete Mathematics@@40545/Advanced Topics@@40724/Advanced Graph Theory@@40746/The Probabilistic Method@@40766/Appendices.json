{"hands_on_practices": [{"introduction": "The probabilistic method's elegance often lies in its ability to simplify complex counting problems through the linearity of expectation. This first practice applies this core principle to a relatable scenario involving permutations. You will calculate the expected number of cycles in a random assignment, which reveals a surprisingly beautiful and fundamental result in combinatorics. This exercise is designed to build your skills in defining indicator random variables and leveraging them to find an expected value that would be difficult to compute directly.", "problem": "In a modern twist on the classic \"Secret Santa\" gift exchange, a group of five software engineers uses a script to automate the assignment process. The script assigns each engineer a recipient for their gift. The process ensures that every engineer is assigned to give exactly one gift and to receive exactly one gift. The assignment is chosen uniformly at random from all possible valid assignment configurations.\n\nOccasionally, this random process results in self-contained \"gift-exchange cycles.\" For example, in a small cycle of three people (Alice, Bob, and Chloe), Alice gives to Bob, Bob gives to Chloe, and Chloe gives back to Alice. A cycle can be of any length from 1 (a person is assigned to give a gift to themselves) up to 5.\n\nGiven this setup for the five engineers, what is the expected total number of distinct gift-exchange cycles?\n\nExpress your answer as a fraction in simplest form.", "solution": "Let $n=5$ and let $\\sigma$ be a uniformly random permutation of $\\{1,2,3,4,5\\}$. Write the total number of cycles in the disjoint cycle decomposition of $\\sigma$ as\n$$\nC=\\sum_{k=1}^{n} Y_{k},\n$$\nwhere $Y_{k}$ is the number of $k$-cycles in $\\sigma$. By linearity of expectation,\n$$\n\\mathbb{E}[C]=\\sum_{k=1}^{n} \\mathbb{E}[Y_{k}].\n$$\nTo compute $\\mathbb{E}[Y_{k}]$, fix a subset $S \\subset \\{1,\\dots,n\\}$ with $|S|=k$, and define the indicator $I_{S}$ that $S$ forms a $k$-cycle in $\\sigma$. Then\n$$\nY_{k}=\\sum_{S:\\,|S|=k} I_{S},\n$$\nso\n$$\n\\mathbb{E}[Y_{k}]=\\sum_{S:\\,|S|=k} \\mathbb{P}(S \\text{ is a } k\\text{-cycle}).\n$$\nFor a fixed $S$ with $|S|=k$, the number of permutations in which $S$ forms a single $k$-cycle is $(k-1)!(n-k)!$: there are $(k-1)!$ cyclic orderings of $S$, and the remaining $n-k$ elements can be permuted arbitrarily in $(n-k)!$ ways. Since there are $n!$ total permutations, the probability is\n$$\n\\mathbb{P}(S \\text{ is a } k\\text{-cycle})=\\frac{(k-1)!(n-k)!}{n!}.\n$$\nThere are $\\binom{n}{k}$ choices of $k$-element subset $S$, hence\n$$\n\\mathbb{E}[Y_{k}]=\\binom{n}{k}\\frac{(k-1)!(n-k)!}{n!}=\\frac{1}{k}.\n$$\nTherefore,\n$$\n\\mathbb{E}[C]=\\sum_{k=1}^{n} \\frac{1}{k}.\n$$\nFor $n=5$,\n$$\n\\mathbb{E}[C]=1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\frac{1}{5}=\\frac{60+30+20+15+12}{60}=\\frac{137}{60}.\n$$\nThus, the expected total number of distinct gift-exchange cycles is $\\frac{137}{60}$.", "answer": "$$\\boxed{\\frac{137}{60}}$$", "id": "1410244"}, {"introduction": "After calculating an expected value, it is crucial to correctly interpret what that value implies. This exercise challenges you to analyze a classic argument related to Ramsey numbers, a famous application of the probabilistic method. The goal is to identify a common logical flaw when reasoning about an expected value that is less than one. Mastering this distinction is key to understanding why the probabilistic method is such a powerful tool for proving the *existence* of combinatorial structures without needing to construct them explicitly.", "problem": "In a graph theory course, a student is learning about Ramsey numbers and the probabilistic method for finding their lower bounds. The Ramsey number $R(k, t)$ is the smallest integer $n$ such that any red-blue coloring of the edges of a complete graph on $n$ vertices, $K_n$, must contain either a red $K_k$ (a complete subgraph of size $k$ with all edges colored red) or a blue $K_t$.\n\nThe student attempts to formulate an argument to find a lower bound for the diagonal Ramsey number $R(k, k)$. The student's argument proceeds as follows:\n\n1.  Consider a complete graph $K_n$ on $n$ vertices. Color each of its $\\binom{n}{2}$ edges independently and uniformly at random, either red or blue, with probability $\\frac{1}{2}$ for each color.\n\n2.  Let $S$ be any specific subset of $k$ vertices. The probability that the subgraph induced by $S$ is a monochromatic $K_k$ is $2 \\cdot \\left(\\frac{1}{2}\\right)^{\\binom{k}{2}} = 2^{1-\\binom{k}{2}}$.\n\n3.  Let $X$ be the random variable for the total number of monochromatic $K_k$ subgraphs in the coloring. By linearity of expectation, the expected number of monochromatic $K_k$ subgraphs is $E[X] = \\binom{n}{k} 2^{1-\\binom{k}{2}}$.\n\n4.  Now, suppose we choose $n$ and $k$ such that $E[X] < 1$.\n\n5.  The student concludes: \"The expected number of monochromatic $K_k$s is the average number over all possible colorings. If this average is less than one, then every single coloring must result in a graph with fewer than one monochromatic $K_k$. Since the number of such subgraphs must be an integer, this means every coloring must have zero monochromatic $K_k$s. Therefore, if $\\binom{n}{k} 2^{1-\\binom{k}{2}} < 1$, then $R(k, k) > n$.\"\n\nWhat is the primary logical flaw in the student's reasoning, specifically in their conclusion in Step 5?\n\nA. The formula for the expected value $E[X]$ is incorrect; it should not be a product of the number of subgraphs and the probability.\n\nB. The expectation $E[X]$ is an average over all outcomes, and having $E[X] < 1$ does not imply that every outcome must be zero; it only guarantees that at least one outcome must be zero.\n\nC. The argument is invalid because the expectation of a discrete random variable, like the number of subgraphs, must be an integer. The condition $E[X] < 1$ is therefore a contradiction.\n\nD. The conclusion that $R(k, k) > n$ is incorrect. Even if a coloring with no monochromatic $K_k$ exists, it does not provide information about the Ramsey number.\n\nE. The student's argument breaks down because the random variable $X$ does not follow a uniform distribution, which is a required condition for this type of probabilistic argument.", "solution": "We model the random coloring by taking the sample space of all $2^{\\binom{n}{2}}$ red-blue edge colorings of $K_{n}$ with the uniform distribution. For each $k$-subset $S$ of vertices, the event that $S$ induces a monochromatic $K_{k}$ has probability $2 \\cdot \\left(\\frac{1}{2}\\right)^{\\binom{k}{2}} = 2^{1-\\binom{k}{2}}$. Let $X$ be the number of monochromatic $K_{k}$ subgraphs; by linearity of expectation,\n$$\nE[X] \\;=\\; \\sum_{S \\in \\binom{[n]}{k}} \\Pr(\\text{$S$ is monochromatic}) \\;=\\; \\binom{n}{k}\\, 2^{\\,1-\\binom{k}{2}}.\n$$\nAssume $E[X] < 1$. The student’s Step 5 claims that if the average number of monochromatic $K_{k}$ is less than $1$, then every coloring must have fewer than $1$ such subgraph, hence $0$. This is a logical error: an average bound does not impose a pointwise bound on every outcome.\n\nFormally, since $X \\ge 0$, Markov’s inequality gives\n$$\n\\Pr(X \\ge 1) \\le \\frac{E[X]}{1} = E[X] < 1,\n$$\nso\n$$\n\\Pr(X = 0) = 1 - \\Pr(X \\ge 1) \\ge 1 - E[X] > 0.\n$$\nThus there exists at least one coloring with $X=0$, but it is false that every coloring must have $X=0$. A simple counterexample to the student’s inference pattern: a random variable that is $0$ with probability $\\frac{3}{4}$ and $2$ with probability $\\frac{1}{4}$ has $E[X] = \\frac{1}{2} < 1$, yet not every outcome is $0$.\n\nTherefore, the primary flaw in Step 5 is conflating the expectation (an average) with a universal guarantee about all outcomes. The correct statement is that $E[X] < 1$ implies the existence of at least one coloring with no monochromatic $K_{k}$, not that every coloring has none. This corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1485029"}, {"introduction": "While the probabilistic method often provides non-constructive existence proofs, these proofs can frequently be converted into deterministic algorithms through a process called derandomization. This practice introduces you to one such powerful technique: the method of conditional expectations. By applying this method to the MAX-CUT problem, you will see how to make a sequence of locally optimal choices that guarantees a globally good result, effectively building the object whose existence was merely promised by probability. This exercise bridges the gap between abstract existence proofs and concrete, efficient algorithms.", "problem": "In computational complexity theory, the probabilistic method is a powerful non-constructive tool used to prove the existence of a combinatorial object with certain properties. A follow-up technique, known as derandomization, aims to convert such an existence proof into a deterministic algorithm that constructs the object.\n\nConsider the MAX-CUT problem on an undirected graph $G=(V, E)$, which asks for a partition of the vertex set $V$ into two disjoint subsets, $S_0$ and $S_1$, such that the number of edges with one endpoint in $S_0$ and the other in $S_1$ is maximized. The set of such edges is called a cut.\n\nLet's analyze a specific graph with vertex set $V = \\{v_1, v_2, v_3, v_4\\}$ and edge set $E = \\{\\{v_1, v_2\\}, \\{v_1, v_3\\}, \\{v_2, v_3\\}, \\{v_2, v_4\\}, \\{v_3, v_4\\}\\}$.\n\nA simple randomized algorithm for MAX-CUT assigns each vertex to either $S_0$ or $S_1$ independently and uniformly at random. The expected size of the cut produced by this method proves the existence of a cut of at least that size.\n\nWe can derandomize this process using the method of conditional expectations. We will decide the partition for each vertex one by one, in the order $v_1, v_2, v_3, v_4$. For each vertex $v_k$, we deterministically place it into the partition ($S_0$ or $S_1$) that maximizes the conditional expectation of the final cut size, given the placements of all preceding vertices $v_1, \\dots, v_{k-1}$. If placing $v_k$ into $S_0$ or $S_1$ yields the same conditional expectation, you must place it into $S_0$ as a tie-breaking rule.\n\nFollowing this deterministic procedure, which of the following represents the final set of vertices in the partition $S_0$?\n\nA. $\\{v_2, v_3\\}$\n\nB. $\\{v_1, v_4\\}$\n\nC. $\\{v_2\\}$\n\nD. $\\{v_1, v_3, v_4\\}$\n\nE. $\\{v_1, v_2\\}$", "solution": "Let $X$ denote the final cut size. By linearity of expectation,\n$$\n\\mathbb{E}[X\\,|\\,\\text{partial assignment}]=\\sum_{\\{u,w\\}\\in E}\\mathbb{E}[I_{\\{u,w\\}}\\,|\\,\\text{partial assignment}],\n$$\nwhere $I_{\\{u,w\\}}$ is the indicator that edge $\\{u,w\\}$ is cut. For any edge, if both endpoints are not yet assigned or exactly one endpoint is assigned, then $\\mathbb{E}[I_{\\{u,w\\}}\\,|\\,\\cdot]=\\frac{1}{2}$. If both endpoints are assigned, then the contribution is $1$ if they are placed in different parts and $0$ otherwise. Thus, when placing $v_{k}$, only edges from $v_{k}$ to already assigned neighbors affect the conditional expectation; all other edges contribute $\\frac{1}{2}$ independent of the choice.\n\nProcess vertices in order $v_{1},v_{2},v_{3},v_{4}$ with the tie-breaking rule to $S_{0}$.\n\nStep for $v_{1}$: No previously assigned vertices. Both choices yield the same conditional expectation, so place $v_{1}\\in S_{0}$.\n\nStep for $v_{2}$: The only previously assigned neighbor is $v_{1}$ with edge $\\{v_{1},v_{2}\\}\\in E$.\n- If $v_{2}\\in S_{0}$, then $\\{v_{1},v_{2}\\}$ contributes $0$, and the other $4$ edges contribute $\\frac{1}{2}$ each, so\n$$\n\\mathbb{E}[X\\,|\\,v_{1}\\in S_{0},v_{2}\\in S_{0}]=0+4\\cdot\\frac{1}{2}=2.\n$$\n- If $v_{2}\\in S_{1}$, then $\\{v_{1},v_{2}\\}$ contributes $1$, others $\\frac{1}{2}$ each, so\n$$\n\\mathbb{E}[X\\,|\\,v_{1}\\in S_{0},v_{2}\\in S_{1}]=1+4\\cdot\\frac{1}{2}=3.\n$$\nThus place $v_{2}\\in S_{1}$.\n\nStep for $v_{3}$: Previously assigned are $v_{1}\\in S_{0}$ and $v_{2}\\in S_{1}$, with edges $\\{v_{1},v_{3}\\},\\{v_{2},v_{3}\\}\\in E$.\n- If $v_{3}\\in S_{0}$, then $\\{v_{1},v_{3}\\}$ contributes $0$, $\\{v_{2},v_{3}\\}$ contributes $1$. Edges $\\{v_{2},v_{4}\\},\\{v_{3},v_{4}\\}$ each contribute $\\frac{1}{2}$, and $\\{v_{1},v_{2}\\}$ is already fixed at $1$. Hence\n$$\n\\mathbb{E}[X\\,|\\,v_{3}\\in S_{0}]=1+0+1+\\frac{1}{2}+\\frac{1}{2}=3.\n$$\n- If $v_{3}\\in S_{1}$, then $\\{v_{1},v_{3}\\}$ contributes $1$, $\\{v_{2},v_{3}\\}$ contributes $0$. The same two edges to $v_{4}$ contribute $\\frac{1}{2}$ each, and $\\{v_{1},v_{2}\\}=1$, so\n$$\n\\mathbb{E}[X\\,|\\,v_{3}\\in S_{1}]=1+1+0+\\frac{1}{2}+\\frac{1}{2}=3.\n$$\nTie, so place $v_{3}\\in S_{0}$.\n\nStep for $v_{4}$: Previously assigned are $v_{1}\\in S_{0}$, $v_{2}\\in S_{1}$, $v_{3}\\in S_{0}$. The relevant edges are $\\{v_{2},v_{4}\\},\\{v_{3},v_{4}\\}\\in E$.\n- If $v_{4}\\in S_{0}$, then $\\{v_{2},v_{4}\\}$ contributes $1$, $\\{v_{3},v_{4}\\}$ contributes $0$. The already fixed edges are $\\{v_{1},v_{2}\\}=1$, $\\{v_{1},v_{3}\\}=0$, $\\{v_{2},v_{3}\\}=1$, so\n$$\n\\mathbb{E}[X\\,|\\,v_{4}\\in S_{0}]=1+0+1+0+1=3.\n$$\n- If $v_{4}\\in S_{1}$, then $\\{v_{2},v_{4}\\}$ contributes $0$, $\\{v_{3},v_{4}\\}$ contributes $1$, and the three fixed edges remain the same, giving\n$$\n\\mathbb{E}[X\\,|\\,v_{4}\\in S_{1}]=1+0+1+0+1=3.\n$$\nTie, so place $v_{4}\\in S_{0}$.\n\nThe final partition has $S_{0}=\\{v_{1},v_{3},v_{4}\\}$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1420467"}]}