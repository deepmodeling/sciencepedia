{"hands_on_practices": [{"introduction": "Mastering a theoretical tool like the Master Theorem begins with applying it to practical scenarios. This first exercise challenges you to analyze and compare two distinct divide-and-conquer algorithms. By applying the Master Theorem to both, you will practice identifying which component of each algorithm—the recursive calls or the combination step—dominates its long-term performance, a crucial skill in algorithm design. [@problem_id:1408675]", "problem": "A distributed computing firm is developing a new service and is evaluating two competing algorithms, Algorithm A and Algorithm B, for a core processing task on a dataset of size $n$.\n\nAlgorithm A is a divide-and-conquer algorithm that splits the problem into 2 subproblems of size $n/2$, solves them recursively, and combines the results. The combination step takes a time proportional to $n^2$. Its running time, $T_A(n)$, can be described by the recurrence relation $T_A(n) = 2T_A(n/2) + n^2$.\n\nAlgorithm B is another divide-and-conquer algorithm that generates 7 subproblems, each on a dataset of size $n/3$, and solves them recursively. Its combining step also takes time proportional to $n^2$. Its running time, $T_B(n)$, is described by the recurrence $T_B(n) = 7T_B(n/3) + n^2$.\n\nFor the purpose of asymptotic analysis, you can ignore base cases and proportionality constants. Based on these recurrences, which of the following statements correctly describes the long-term efficiency relationship between the two algorithms?\n\nA. Algorithm A is asymptotically more efficient than Algorithm B. In mathematical terms, $T_A(n) \\in o(T_B(n))$.\n\nB. Algorithm B is asymptotically more efficient than Algorithm A. In mathematical terms, $T_B(n) \\in o(T_A(n))$.\n\nC. Both algorithms have the same asymptotic time complexity. In mathematical terms, $T_A(n) \\in \\Theta(T_B(n))$.\n\nD. The Master Theorem is not applicable to at least one of these recurrences, so their complexities cannot be determined using it.\n\nE. The asymptotic behavior of one or both algorithms cannot be determined from the information given.", "solution": "We analyze each recurrence using the Master Theorem. For a recurrence of the form $T(n)=a\\,T(n/b)+f(n)$, define $d=\\log_{b}(a)$ and compare $f(n)$ to $n^{d}$. If $f(n)=\\Omega\\!\\left(n^{d+\\epsilon}\\right)$ for some $\\epsilon0$ and the regularity condition $a\\,f(n/b)\\leq c\\,f(n)$ for some constant $c1$ holds for all sufficiently large $n$, then $T(n)=\\Theta\\!\\left(f(n)\\right)$.\n\nFor Algorithm A, we have $a=2$, $b=2$, and $f(n)=n^{2}$. Then\n$$\nd=\\log_{2}(2)=1,\\quad f(n)=n^{2}=\\Omega\\!\\left(n^{1+\\epsilon}\\right)\\ \\text{with}\\ \\epsilon=1.\n$$\nCheck the regularity condition:\n$$\na\\,f(n/b)=2\\left(\\frac{n}{2}\\right)^{2}=\\frac{n^{2}}{2}\\leq c\\,n^{2}\\ \\text{with}\\ c=\\frac{1}{2}1.\n$$\nTherefore, by the Master Theorem, \n$$\nT_{A}(n)=\\Theta\\!\\left(n^{2}\\right).\n$$\n\nFor Algorithm B, we have $a=7$, $b=3$, and $f(n)=n^{2}$. Then\n$$\nd=\\log_{3}(7),\\quad \\text{and since }2\\log_{3}(7),\\ f(n)=n^{2}=\\Omega\\!\\left(n^{\\log_{3}(7)+\\epsilon}\\right)\\ \\text{with}\\ \\epsilon=2-\\log_{3}(7)0.\n$$\nCheck the regularity condition:\n$$\na\\,f(n/b)=7\\left(\\frac{n}{3}\\right)^{2}=\\frac{7}{9}\\,n^{2}\\leq c\\,n^{2}\\ \\text{with}\\ c=\\frac{7}{9}1.\n$$\nTherefore, by the Master Theorem,\n$$\nT_{B}(n)=\\Theta\\!\\left(n^{2}\\right).\n$$\n\nSince both $T_{A}(n)$ and $T_{B}(n)$ are $\\Theta\\!\\left(n^{2}\\right)$, it follows that $T_{A}(n)\\in\\Theta\\!\\left(T_{B}(n)\\right)$. Hence the correct choice is C, and the Master Theorem is applicable to both recurrences.", "answer": "$$\\boxed{C}$$", "id": "1408675"}, {"introduction": "Real-world algorithms are often composed of several complex parts, where one subroutine's cost impacts the main procedure. This problem models such a scenario, where the 'combine' step of an algorithm is itself a separate divide-and-conquer process. To find the overall complexity, you must first analyze the inner recurrence and use that result to solve the outer one, demonstrating how to analyze nested computational structures. [@problem_id:1408677]", "problem": "An algorithm designer is developing a novel computational method called the Hierarchical Data Fusion (HDF) algorithm, intended for processing large-scale structured data. For an input of size $n$, the HDF algorithm operates as follows: it divides the data into two subproblems of size $n/2$, recursively calls itself on each subproblem, and then combines the results.\n\nThe combination step is computationally intensive and is handled by a separate subroutine, the Cross-Correlation Aligner (CCA). The running time of the CCA for an input of size $n$, denoted as $C(n)$, itself follows a divide-and-conquer strategy. The CCA algorithm divides its task into three subproblems of size $n/3$, recursively solves them, and then merges the results in a time that is linear with respect to the input size $n$.\n\nLet $T(n)$ be the total running time of the HDF algorithm on an input of size $n$, and let $C(n)$ be the running time of the CCA subroutine which constitutes the combination cost for HDF. The running times are described by the following recurrence relations for $n  1$, assuming that $T(1)$ and $C(1)$ are constant.\n\nThe HDF recurrence is:\n$T(n) = 2T(n/2) + C(n)$\n\nThe CCA recurrence is:\n$C(n) = 3C(n/3) + f(n)$, where $f(n) = \\Theta(n)$.\n\nDetermine the tightest asymptotic bound for the running time $T(n)$ of the HDF algorithm. Choose from the options below.\n\nA. $\\Theta(n^2)$\n\nB. $\\Theta(n \\log \\log n)$\n\nC. $\\Theta(n (\\log n)^2)$\n\nD. $\\Theta(n \\log n)$\n\nE. $\\Theta(n^{\\log_2 3})$", "solution": "We are given two recurrences. For $n1$,\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+C(n),\\qquad C(n)=3\\,C\\!\\left(\\frac{n}{3}\\right)+f(n),\\quad f(n)=\\Theta(n),\n$$\nwith $T(1)$ and $C(1)$ constant. We first solve $C(n)$, then substitute into $T(n)$.\n\nTo solve $C(n)$, apply the Master Theorem with $a=3$, $b=3$, and $f(n)=\\Theta(n)$. We compute\n$$\nn^{\\log_{3}3}=n.\n$$\nThus $f(n)=\\Theta\\!\\big(n^{\\log_{3}3}\\big)$, which is Master Theorem case 2. Therefore,\n$$\nC(n)=\\Theta\\!\\big(n\\log_{3}n\\big).\n$$\nSince logarithms of different bases differ by a constant factor, $\\log_{3}n=\\frac{\\log_{2}n}{\\log_{2}3}$, we can equivalently write\n$$\nC(n)=\\Theta\\!\\big(n\\log_{2}n\\big).\n$$\n\nNow substitute this into the recurrence for $T(n)$:\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+\\Theta\\!\\big(n\\log_{2}n\\big).\n$$\nHere $a=2$, $b=2$, so $n^{\\log_{2}2}=n$. The toll term satisfies\n$$\nf(n)=\\Theta\\!\\big(n\\log_{2}n\\big)=\\Theta\\!\\big(n^{\\log_{2}2}\\,(\\log_{2}n)^{1}\\big).\n$$\nThis matches the extended Master Theorem case where $f(n)=\\Theta\\!\\big(n^{\\log_{b}a}(\\log n)^{k}\\big)$ with $k=1$. Hence,\n$$\nT(n)=\\Theta\\!\\big(n^{\\log_{2}2}(\\log_{2}n)^{k+1}\\big)=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big).\n$$\n\nFor completeness, a recursion-tree summation yields the same result. At level $i$ of the $T$-recursion ($0\\le i\\le \\log_{2}n-1$), there are $2^{i}$ nodes, each of size $n/2^{i}$, and each contributes a combine cost $C(n/2^{i})=\\Theta\\!\\big(\\frac{n}{2^{i}}\\log_{2}\\frac{n}{2^{i}}\\big)$. The total combine cost at level $i$ is\n$$\n2^{i}\\cdot \\Theta\\!\\left(\\frac{n}{2^{i}}\\log_{2}\\frac{n}{2^{i}}\\right)=\\Theta\\!\\big(n(\\log_{2}n-i)\\big).\n$$\nSumming over all levels with $L=\\log_{2}n$,\n$$\n\\sum_{i=0}^{L-1}\\Theta\\!\\big(n(\\log_{2}n-i)\\big)\n=\\Theta\\!\\left(n\\sum_{i=0}^{L-1}(L-i)\\right)\n=\\Theta\\!\\left(n\\cdot \\frac{L(L+1)}{2}\\right)\n=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big).\n$$\nThe leaves contribute $\\Theta(n)$, which is asymptotically dominated by $\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big)$. Therefore,\n$$\nT(n)=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big)=\\Theta\\!\\big(n(\\log n)^{2}\\big).\n$$\n\nAmong the given options, this matches option C.", "answer": "$$\\boxed{C}$$", "id": "1408677"}, {"introduction": "A true test of understanding is knowing a tool's limitations. This final practice problem presents a recurrence with subproblems of unequal sizes, a structure that falls outside the direct scope of the standard Master Theorem. This will encourage you to think from first principles, using methods like the recursion tree to sum the work at each level and understand the fundamental mechanics that the Master Theorem neatly automates. [@problem_id:1408680]", "problem": "A new Hierarchical Network Routing Algorithm (HNRA) is proposed for optimizing data transmission paths in large-scale decentralized networks. The algorithm operates recursively on a network characterized by a size parameter $n$.\n\nThe process for a network of size $n$ is as follows:\n1.  A partitioning phase divides the network into two smaller, independent sub-networks. This phase requires a computational effort of $c_1 n$, where $c_1$ is a positive constant. The resulting sub-networks have sizes $n/2$ and $n/3$.\n2.  The HNRA is then called recursively on each of these two sub-networks.\n3.  Once the recursive calls return solutions for the sub-networks, a merging phase combines these solutions to construct the final routing table for the original network. This merging phase requires a computational effort of $c_2 n$, where $c_2$ is a positive constant.\n\nFor simplicity, assume that for any network size $n \\leq n_0$, where $n_0$ is a small constant, the algorithm can find the solution in constant time. The total running time of the algorithm for a network of size $n  n_0$ can be represented by a recurrence relation $T(n)$.\n\nGiven this description, which of the following options represents the tightest asymptotic bound for the running time $T(n)$ of the HNRA for large $n$?\n\nA. $\\Theta(n)$\n\nB. $\\Theta(n \\log n)$\n\nC. $\\Theta(n^2)$\n\nD. $\\Theta(n^{1.5})$\n\nE. $\\Theta(\\log n)$", "solution": "Let $T(n)$ denote the running time for $nn_{0}$ and assume $T(n)=\\Theta(1)$ for $n \\leq n_{0}$. From the description, each call on size $n$ performs a partition of cost $c_{1} n$, makes two recursive calls on sizes $n/2$ and $n/3$, and performs a merge of cost $c_{2} n$. Thus there exists $c=c_{1}+c_{2}0$ such that\n$$\nT(n)=T\\!\\left(\\frac{n}{2}\\right)+T\\!\\left(\\frac{n}{3}\\right)+c\\,n \\quad \\text{for } nn_{0}, \\quad T(n)=\\Theta(1) \\text{ for } n \\leq n_{0}.\n$$\nConsider the recursion tree. The nonrecursive work performed at a node of size $m$ is $c m$. At level $k$ of the tree, the sum of subproblem sizes equals the sum over all sequences of $k$ choices of factors $\\frac{1}{2}$ and $\\frac{1}{3}$ multiplied by $n$. There are $\\binom{k}{i}$ nodes of size $n \\left(\\frac{1}{2}\\right)^{i} \\left(\\frac{1}{3}\\right)^{k-i}$, for $i=0,1,\\dots,k$. Hence the total size at level $k$ is\n$$\n\\sum_{i=0}^{k} \\binom{k}{i} \\, n \\left(\\frac{1}{2}\\right)^{i} \\left(\\frac{1}{3}\\right)^{k-i}\n= n \\left(\\frac{1}{2}+\\frac{1}{3}\\right)^{k}\n$$\nby the binomial theorem. Therefore, the total work at level $k$ is\n$$\nc \\, n \\left(\\frac{1}{2}+\\frac{1}{3}\\right)^{k}.\n$$\nSince $\\frac{1}{2}+\\frac{1}{3}1$, the sum of per-level costs over all levels is a convergent geometric series. Thus, summing over all levels until the base case is reached (and extending to infinity for an upper bound that differs only by a constant factor),\n$$\nT(n)=\\Theta\\!\\left(n \\sum_{k=0}^{\\infty} \\left(\\frac{1}{2}+\\frac{1}{3}\\right)^{k}\\right)=\\Theta(n).\n$$\nEquivalently, applying the Akra–Bazzi method to $T(n)=T(n/2)+T(n/3)+c n$ with $a_{1}=a_{2}=1$, $b_{1}=\\frac{1}{2}$, $b_{2}=\\frac{1}{3}$, and $g(n)=c n$, one finds $p \\in (0,1)$ solving $\\left(\\frac{1}{2}\\right)^{p}+\\left(\\frac{1}{3}\\right)^{p}=1$, and\n$$\nT(n)=\\Theta\\!\\left(n^{p}\\left(1+\\int_{1}^{n} \\frac{g(u)}{u^{p+1}} \\, du\\right)\\right)\n=\\Theta\\!\\left(n^{p}\\left(1+\\int_{1}^{n} c\\, u^{-p} \\, du\\right)\\right)\n=\\Theta\\!\\left(n^{p}\\left(1+n^{1-p}\\right)\\right)=\\Theta(n).\n$$\nIn either derivation, the tightest asymptotic bound is linear.", "answer": "$$\\boxed{A}$$", "id": "1408680"}]}