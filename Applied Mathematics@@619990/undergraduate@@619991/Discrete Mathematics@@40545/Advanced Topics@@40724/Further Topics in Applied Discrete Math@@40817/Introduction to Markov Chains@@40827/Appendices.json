{"hands_on_practices": [{"introduction": "The core of a Markov chain is its step-by-step evolution, governed by transition probabilities. This first practice provides a concrete exercise in calculating these probabilities over a finite number of steps by tracking the possible paths of a particle in a simple system. Mastering this skill by tracing the system's evolution is essential for understanding how its state changes over time and serves as a foundation for analyzing more complex, long-term behaviors. [@problem_id:1378025]", "problem": "A particle performs a random walk on a set of five vertices labeled A, B, C, D, and E. The connections between the vertices are as follows: vertices A, B, C, and D form a cycle where A is connected to B, B to C, C to D, and D back to A. Additionally, vertex E is connected only to vertex A.\n\nAt each step, the particle, currently at a given vertex, moves to one of its adjacent (connected) vertices. The choice of which adjacent vertex to move to is random, with each neighbor having an equal probability of being chosen.\n\nIf the particle starts at vertex C, what is the probability that it will be at vertex E after exactly three steps?\n\nExpress your answer as an exact fraction in simplest form.", "solution": "Model the motion as a discrete-time Markov chain on vertices $\\{A,B,C,D,E\\}$ where the transition probability from a vertex is uniform over its neighbors. Thus, for any connected pair $(v,u)$, the one-step transition probability is $P(v \\to u)=\\frac{1}{\\deg(v)}$. The degrees are $\\deg(A)=3$ (neighbors $B,D,E$), $\\deg(B)=\\deg(C)=\\deg(D)=2$, and $\\deg(E)=1$.\n\nTo be at $E$ after exactly three steps starting from $C$, the third step must be $A \\to E$, since $E$ is only adjacent to $A$. Therefore,\n$$\n\\mathbb{P}(X_{3}=E \\mid X_{0}=C)=\\mathbb{P}(X_{2}=A \\mid X_{0}=C)\\cdot \\mathbb{P}(A \\to E)=\\mathbb{P}(X_{2}=A \\mid X_{0}=C)\\cdot \\frac{1}{3}.\n$$\nCompute $\\mathbb{P}(X_{2}=A \\mid X_{0}=C)$ by enumerating two-step paths from $C$ to $A$. From $C$, the first step goes to $B$ or $D$ with probability $\\frac{1}{2}$ each. From $B$, the second step goes to $A$ with probability $\\frac{1}{2}$; from $D$, the second step goes to $A$ with probability $\\frac{1}{2}$. Hence,\n$$\n\\mathbb{P}(X_{2}=A \\mid X_{0}=C)=\\left(\\frac{1}{2}\\cdot \\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\cdot \\frac{1}{2}\\right)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\mathbb{P}(X_{3}=E \\mid X_{0}=C)=\\frac{1}{2}\\cdot \\frac{1}{3}=\\frac{1}{6}.\n$$", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "1378025"}, {"introduction": "After many steps, many Markov chains settle into a predictable long-term pattern, described by a stationary distribution. This exercise flips the typical problem on its head: instead of deriving the long-term behavior from the transition rules, you will use the observed long-term state to deduce the rules themselves. This practice is key to understanding the deep relationship between a system's dynamics, represented by the transition matrix $P$, and its equilibrium state $\\pi$, a common task in data-driven modeling. [@problem_id:1378050]", "problem": "A data scientist is modeling the daily work habits of a remote employee using a simple two-state Markov chain. The two states are \"Focused\" (State 1) and \"Distracted\" (State 2). The model assumes that the employee's state in any given hour depends only on their state in the previous hour.\n\nAfter collecting a large amount of data, the scientist observes that over the long run, the employee is in the \"Focused\" state one-third of the time and in the \"Distracted\" state two-thirds of the time. This corresponds to a stationary distribution vector $\\pi = \\begin{pmatrix} 1/3 & 2/3 \\end{pmatrix}$.\n\nFurthermore, the data reveals a specific transition probability: if the employee is currently in the \"Distracted\" state, the probability that they will be in the \"Focused\" state in the next hour is $1/4$.\n\nLet $P$ be the $2 \\times 2$ right stochastic transition matrix for this Markov chain, where the entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. Determine the complete transition matrix $P$. Present your answer as a $2 \\times 2$ matrix with fractional entries.", "solution": "Let state 1 be Focused and state 2 be Distracted. For a right stochastic transition matrix $P$, each row sums to $1$, and $P_{ij}$ is the probability to go from state $i$ to state $j$. The given stationary distribution $\\pi = \\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}$ satisfies the stationarity condition for a row vector:\n$$\n\\pi P = \\pi.\n$$\n\nWe are given $P_{21} = \\frac{1}{4}$. By row-stochasticity, $P_{22} = 1 - P_{21} = \\frac{3}{4}$. Let $P_{11} = x$, so $P_{12} = 1 - x$. Thus\n$$\nP = \\begin{pmatrix}\nx & 1 - x \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$\n\nImpose the stationarity condition:\n$$\n\\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}\n\\begin{pmatrix}\nx & 1 - x \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}.\n$$\nEquating the first component gives\n$$\n\\frac{1}{3} x + \\frac{2}{3} \\cdot \\frac{1}{4} = \\frac{1}{3}\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{1}{3} x + \\frac{1}{6} = \\frac{1}{3}\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{1}{3} x = \\frac{1}{6}\n\\;\\;\\Longrightarrow\\;\\;\nx = \\frac{1}{2}.\n$$\nTherefore $P_{11} = \\frac{1}{2}$ and $P_{12} = \\frac{1}{2}$, yielding\n$$\nP = \\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{2} \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$\nA direct check shows $\\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix} P = \\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}$, as required.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{3}{4} \\end{pmatrix}}$$", "id": "1378050"}, {"introduction": "Markov chains are powerful tools for modeling real-world processes, even those with a large number of states, such as shuffling a deck of cards. This practice challenges you to analyze such a system, where direct computation of the full transition matrix would be tedious. The key to solving it efficiently lies in recognizing the underlying symmetry of the process, a valuable problem-solving insight that often reveals elegant solutions to seemingly complex problems. [@problem_id:1378035]", "problem": "A small deck of three cards, uniquely labeled 1, 2, and 3, is shuffled according to a specific, repeated procedure. A state of the deck is defined by the permutation of the cards when ordered from top to bottom. For example, the state (1, 2, 3) means card 1 is on top, card 2 is in the middle, and card 3 is at the bottom. The state space, therefore, consists of all $3! = 6$ possible permutations of the three cards.\n\nThe shuffling procedure at each step is as follows: take the card currently at the top of the deck and re-insert it into one of the three possible positions (top, middle, or bottom), with each position being chosen with equal probability of $1/3$.\n\nThis shuffling process can be modeled as a discrete-time Markov chain on the space of permutations. After the shuffling process has been repeated for a very long time, the deck will approach a stationary distribution of probabilities for being in any given state.\n\nDetermine the long-term probability of finding the deck in the specific state (3, 2, 1). Express your answer as a fraction.", "solution": "Let the state space be the six permutations of $\\{1,2,3\\}$. From a generic state $(a,b,c)$, the shuffle “remove the top card and reinsert uniformly at one of the three positions” yields exactly three possible next states, each with probability $\\frac{1}{3}$:\n$$\n(a,b,c)\\xrightarrow[\\text{prob}=\\frac{1}{3}]{}(a,b,c),\\quad\n(a,b,c)\\xrightarrow[\\text{prob}=\\frac{1}{3}]{}(b,a,c),\\quad\n(a,b,c)\\xrightarrow[\\text{prob}=\\frac{1}{3}]{}(b,c,a).\n$$\nThus the transition matrix $P$ is defined by\n$$\nP\\big((a,b,c)\\to(a,b,c)\\big)=\\frac{1}{3},\\quad\nP\\big((a,b,c)\\to(b,a,c)\\big)=\\frac{1}{3},\\quad\nP\\big((a,b,c)\\to(b,c,a)\\big)=\\frac{1}{3},\n$$\nand zero otherwise.\n\nFix any state $s=(x,y,z)$. The only predecessors $t$ with $P(t\\to s)>0$ are\n$$\nt_{1}=(x,y,z),\\quad t_{2}=(y,x,z),\\quad t_{3}=(z,x,y),\n$$\nbecause:\n- From $t_{1}=(x,y,z)$, reinserting $x$ on top (position $1$) keeps $(x,y,z)$.\n- From $t_{2}=(y,x,z)$, reinserting $y$ in the middle (position $2$) yields $(x,y,z)$.\n- From $t_{3}=(z,x,y)$, reinserting $z$ at the bottom (position $3$) yields $(x,y,z)$.\n\nEach of these transitions has probability $\\frac{1}{3}$, hence the column sum for $s$ is\n$$\n\\sum_{t}P(t\\to s)=\\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}=1.\n$$\nSince this holds for every state $s$, the transition matrix $P$ is doubly stochastic.\n\nConsider the uniform distribution $\\pi$ on the six states, $\\pi(s)=\\frac{1}{6}$ for all $s$. Then for any state $s$,\n$$\n\\sum_{t}\\pi(t)P(t\\to s)=\\frac{1}{6}\\sum_{t}P(t\\to s)=\\frac{1}{6}\\cdot 1=\\frac{1}{6}=\\pi(s),\n$$\nso $\\pi$ is stationary. The chain is irreducible (from any permutation, a sequence of allowed insertions can reach any other permutation) and aperiodic (each state has a self-loop with probability $\\frac{1}{3}$). Therefore, by standard Markov chain theory, the stationary distribution is unique and the chain converges to it from any start.\n\nHence, in the long run, each state has probability $\\frac{1}{6}$. In particular, the probability of state $(3,2,1)$ is $\\frac{1}{6}$.", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "1378035"}]}