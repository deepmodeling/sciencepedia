{"hands_on_practices": [{"introduction": "Let's begin with a relatable scenario to see Bayes' Theorem in action. This exercise models a common situation—answering a multiple-choice question—to illustrate the core principle of updating our belief about an event (knowing the answer) after we receive new evidence (the answer was correct). By working through this hypothetical problem [@problem_id:339], you will gain a foundational understanding of how to apply the theorem's formula in a concrete, step-by-step manner.", "problem": "A student is taking a multiple-choice examination. For any given question, the probability that the student genuinely knows the correct answer is $p$. Each question on the exam has $m$ possible choices, of which only one is correct.\n\nThe student's behavior is modeled as follows:\n- If the student knows the answer to a question, they will always select the correct choice.\n- If the student does not know the answer, they will choose one of the $m$ options uniformly at random (i.e., they will guess).\n\nGiven that the student has answered a particular question correctly, derive an expression for the conditional probability that the student actually knew the answer to that question. Express your result in a simplified form in terms of $p$ and $m$.", "solution": "We seek $P(\\text{know}\\mid\\text{correct})$. By Bayes’ theorem,\n$$\nP(\\text{know}\\mid\\text{correct})\n=\\frac{P(\\text{know}\\cap\\text{correct})}{P(\\text{correct})}.\n$$\n\nHere $P(\\text{know})=p$ and $P(\\text{correct}\\mid\\text{know})=1$, so\n$$\nP(\\text{know}\\cap\\text{correct})=p\\cdot1=p.\n$$\n\nAlso \n$$\nP(\\text{correct})\n=P(\\text{know})\\cdot1+P(\\text{not know})\\cdot\\frac1m\n=p+(1-p)\\frac1m.\n$$\n\nHence\n$$\nP(\\text{know}\\mid\\text{correct})\n=\\frac{p}{p+\\frac{1-p}m}\n=\\frac{mp}{(m-1)p+1}\n=\\frac{mp}{1+(m-1)p}.\n$$", "answer": "$$\\boxed{\\frac{mp}{1+(m-1)p}}$$", "id": "339"}, {"introduction": "Building on the fundamentals, we now explore a classic application of Bayes' Theorem common in fields from medical diagnostics to machine learning. This problem uses the context of weather forecasting to introduce the critical concepts of true positive and false positive rates. Calculating the actual probability of rain given a positive forecast [@problem_id:357] demonstrates how the theorem helps us assess the reliability of a test or prediction based on its historical performance.", "problem": "A local weather station is evaluating the accuracy of its new forecasting model. Let $R$ be the event that it rains on a given day, and let $F$ be the event that the model forecasts rain for that day.\n\nThe station has collected historical data and determined the following probabilities:\n1.  The unconditional probability of rain on any given day is $P(R) = p_r$.\n2.  The probability that the model correctly forecasts rain, given that it actually rains, is $P(F|R) = p_t$. This represents the true positive rate.\n3.  The probability that the model incorrectly forecasts rain, given that it does not rain, is $P(F|R^c) = p_f$, where $R^c$ is the event that it does not rain. This represents the false positive rate.\n\nUsing these probabilities, derive an expression for the probability that it will actually rain, given that the model has forecasted rain, i.e., $P(R|F)$. Express your answer in terms of $p_r$, $p_t$, and $p_f$.", "solution": "We seek $P(R\\mid F)$.  By Bayes’ theorem,\n$$\nP(R\\mid F)=\\frac{P(F\\mid R)\\,P(R)}{P(F)}.\n$$\nThe total probability of a forecast of rain is\n$$\nP(F)=P(F\\mid R)\\,P(R)+P(F\\mid R^c)\\,P(R^c).\n$$\nSubstitute $P(R)=p_r$, $P(F\\mid R)=p_t$, $P(F\\mid R^c)=p_f$, and $P(R^c)=1-p_r$ to obtain\n$$\nP(F)=p_t\\,p_r+p_f\\,(1-p_r).\n$$\nTherefore\n$$\nP(R\\mid F)\n=\\frac{p_t\\,p_r}{p_t\\,p_r+p_f\\,(1-p_r)}.\n$$", "answer": "$$\\boxed{\\frac{p_t\\,p_r}{p_t\\,p_r+p_f\\,(1-p_r)}}$$", "id": "357"}, {"introduction": "To deepen our intuition, we now tackle a variation of a famous thought experiment known as Bertrand's Box Paradox. This problem challenges us to move beyond simple, uniform assumptions and consider how varying prior probabilities can significantly alter our conclusions. By determining the probability that the second coin in a chosen box is also gold [@problem_id:691205], you will see how Bayes' Theorem elegantly resolves apparent paradoxes and highlights the crucial role of initial beliefs in probabilistic reasoning.", "problem": "Consider a variation of the classic Bertrand's Box Paradox. There are three boxes, indistinguishable from the outside.\n- Box 1 contains two gold coins (GG).\n- Box 2 contains two silver coins (SS).\n- Box 3 contains one gold and one silver coin (GS).\n\nUnlike the classic paradox, the boxes are not chosen with equal probability. The prior probability of selecting the GG box is $p_G$, of selecting the SS box is $p_S$, and of selecting the GS box is $p_M$. These probabilities are known constants, greater than zero, and sum to one: $p_G + p_S + p_M = 1$.\n\nAn experiment is conducted where a box is chosen according to these prior probabilities, and then a single coin is drawn uniformly at random from the selected box. Upon inspection, the drawn coin is found to be gold.\n\nDerive a closed-form analytic expression for the posterior probability that the *other* coin in the chosen box is also gold, given the observation that the first drawn coin is gold. Express your answer in terms of the prior probabilities $p_G$ and $p_M$.", "solution": "We denote the events $B_{GG},B_{GS},B_{SS}$ for choosing the respective boxes, and $D$ for drawing a gold coin. The priors and conditional probabilities are\n$$P(B_{GG})=p_G,\\quad P(B_{GS})=p_M,\\quad P(B_{SS})=p_S,$$\n$$P(D\\mid B_{GG})=1,\\quad P(D\\mid B_{GS})=\\frac{1}{2},\\quad P(D\\mid B_{SS})=0.$$\n\nThe total probability of drawing gold is\n$$P(D)=p_G\\cdot1 + p_M\\cdot\\frac{1}{2} + p_S\\cdot0 = p_G + \\frac{1}{2}p_M.$$\n\nBy Bayes' theorem,\n$$P(B_{GG}\\mid D)=\\frac{P(D\\mid B_{GG})\\,P(B_{GG})}{P(D)} = \\frac{p_G}{p_G + \\frac{1}{2}p_M}.$$\n\nSince only the $GG$ box yields the other coin gold, the desired posterior probability is $P(\\text{other coin is gold}\\mid D)=P(B_{GG}\\mid D)=\\frac{p_G}{p_G + \\frac{1}{2}p_M}$.", "answer": "$$\\boxed{\\frac{p_G}{p_G + \\frac{1}{2}p_M}}$$", "id": "691205"}]}