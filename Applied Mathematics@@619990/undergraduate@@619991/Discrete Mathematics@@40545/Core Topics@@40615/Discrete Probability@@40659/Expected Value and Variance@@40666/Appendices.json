{"hands_on_practices": [{"introduction": "The principle of linearity of expectation is a remarkably powerful tool, allowing us to calculate the expected value of a complex system by summing the expected values of its simpler parts, even when those parts are not independent. This first practice demonstrates this core concept in a tangible setting. By analyzing a hypothetical computer network, you will learn how to use indicator random variables to deconstruct a larger problem into manageable pieces, a fundamental skill in probabilistic analysis [@problem_id:1369264].", "problem": "Consider a model for a simple, fully connected computer network consisting of $n$ nodes, where $n \\geq 2$. A complete graph $K_n$ is used to represent this network, where the vertices are the nodes and the edges represent direct communication links between every pair of nodes. Each node in the network is independently assigned a state, either \"active\" or \"idle\", with an equal probability of $\\frac{1}{2}$ for each state. An edge in this network is defined as \"monochromatic\" if the two nodes it connects are in the same state (i.e., both are active or both are idle).\n\nDetermine a closed-form expression in terms of $n$ for the expected number of monochromatic edges in the network.", "solution": "Let $K_{n}$ be the complete graph on $n$ vertices. The total number of edges is $\\binom{n}{2}$. Assign to each vertex independently one of two states, \"active\" or \"idle\", each with probability $\\frac{1}{2}$.\n\nDefine for each edge $e$ the indicator random variable $I_{e}$ where $I_{e}=1$ if $e$ is monochromatic (its endpoints share the same state) and $I_{e}=0$ otherwise. The total number of monochromatic edges is\n$$\nX=\\sum_{e} I_{e},\n$$\nwhere the sum is over all $\\binom{n}{2}$ edges.\n\nFor any fixed edge $e=\\{u,v\\}$, the probability that it is monochromatic is\n$$\n\\mathbb{P}(I_{e}=1)=\\mathbb{P}(\\text{$u$ and $v$ both active})+\\mathbb{P}(\\text{$u$ and $v$ both idle})=\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{2}.\n$$\n\nUsing linearity of expectation,\n$$\n\\mathbb{E}[X]=\\sum_{e} \\mathbb{E}[I_{e}]=\\sum_{e} \\mathbb{P}(I_{e}=1)=\\binom{n}{2}\\cdot \\frac{1}{2}.\n$$\nSince $\\binom{n}{2}=\\frac{n(n-1)}{2}$, we obtain\n$$\n\\mathbb{E}[X]=\\frac{n(n-1)}{2}\\cdot \\frac{1}{2}=\\frac{n(n-1)}{4}.\n$$", "answer": "$$\\boxed{\\frac{n(n-1)}{4}}$$", "id": "1369264"}, {"introduction": "While expected value tells us the average outcome, variance measures the 'spread' or 'variability' around that average. This practice delves into variance calculations through the lens of the classic \"Coupon Collector's Problem,\" a scenario that models many real-world processes from software testing to data collection. Here, you will learn to model a process as a sum of independent geometric random variables, a sophisticated technique that simplifies the calculation of both expectation and variance [@problem_id:1369276].", "problem": "A software company is deploying a new machine learning algorithm. The quality assurance team has identified $n$ critical and distinct edge cases that must be validated by an automated testing system before deployment. In each test run, the system simulates a scenario that triggers exactly one of these $n$ edge cases. The selection of the edge case for any given run is independent and uniformly random from the set of all $n$ possibilities.\n\nLet $T$ be the random variable representing the total number of test runs required until each of the $n$ distinct edge cases has been triggered at least once.\n\nFind a closed-form analytic expression for the variance of $T$, denoted as $\\text{Var}(T)$, in terms of $n$. Your expression may be written using summation notation.", "solution": "Let $T$ be the total number of test runs required to observe all $n$ distinct edge cases. We can decompose this total time into a sum of random variables representing the waiting times for each new edge case.\n\nLet $T_i$ be the number of additional test runs required to find the $i$-th new edge case, given that $i-1$ distinct edge cases have already been found. The total number of runs is then the sum of these waiting times:\n$$T = T_1 + T_2 + \\dots + T_n$$\nThe first run always yields a new edge case, so $T_1 = 1$, which is a deterministic constant.\n\nNow, let's determine the distribution of $T_i$ for $i > 1$. Suppose we have already observed $i-1$ distinct edge cases. There are $n - (i-1) = n-i+1$ new, unobserved edge cases. Since each of the $n$ edge cases is equally likely to be triggered in any given run, the probability of observing a new edge case (a \"success\") in the next run is:\n$$p_i = \\frac{n - (i-1)}{n} = \\frac{n-i+1}{n}$$\nThe random variable $T_i$ represents the number of trials needed to get the first success in a sequence of independent Bernoulli trials with success probability $p_i$. This means $T_i$ follows a geometric distribution, $T_i \\sim \\text{Geometric}(p_i)$.\n\nThe process of finding the next new edge case is independent of the history of finding the previous ones. Therefore, the random variables $T_1, T_2, \\dots, T_n$ are mutually independent. The variance of a sum of independent random variables is the sum of their variances:\n$$\\text{Var}(T) = \\text{Var}\\left(\\sum_{i=1}^{n} T_i\\right) = \\sum_{i=1}^{n} \\text{Var}(T_i)$$\nThe variance of a random variable $X \\sim \\text{Geometric}(p)$ is given by $\\text{Var}(X) = \\frac{1-p}{p^2}$. We apply this formula to each $T_i$ with its corresponding success probability $p_i$:\n$$\\text{Var}(T_i) = \\frac{1-p_i}{p_i^2} = \\frac{1 - \\frac{n-i+1}{n}}{\\left(\\frac{n-i+1}{n}\\right)^2}$$\nSimplifying the numerator:\n$$1 - \\frac{n-i+1}{n} = \\frac{n - (n-i+1)}{n} = \\frac{i-1}{n}$$\nSubstituting this back into the variance expression:\n$$\\text{Var}(T_i) = \\frac{\\frac{i-1}{n}}{\\frac{(n-i+1)^2}{n^2}} = \\frac{i-1}{n} \\cdot \\frac{n^2}{(n-i+1)^2} = \\frac{n(i-1)}{(n-i+1)^2}$$\nNote that for $i=1$, $\\text{Var}(T_1) = \\frac{n(1-1)}{(n-1+1)^2} = 0$, which is correct since $T_1$ is a constant.\n\nNow we can find the total variance by summing $\\text{Var}(T_i)$ from $i=1$ to $n$:\n$$\\text{Var}(T) = \\sum_{i=1}^{n} \\text{Var}(T_i) = \\sum_{i=1}^{n} \\frac{n(i-1)}{(n-i+1)^2}$$\nTo simplify this summation, we perform a change of index. Let $k = n-i+1$.\n- When $i=1$, $k = n-1+1=n$.\n- When $i=n$, $k = n-n+1=1$.\n- The term $i-1$ can be expressed in terms of $k$: $i = n-k+1 \\Rightarrow i-1 = n-k$.\n\nSubstituting these into the summation, and noting that summing from $k=n$ down to $1$ is the same as summing from $k=1$ up to $n$:\n$$\\text{Var}(T) = \\sum_{k=1}^{n} \\frac{n(n-k)}{k^2}$$\nWe can split this summation into two parts:\n$$\\text{Var}(T) = \\sum_{k=1}^{n} \\left(\\frac{n^2}{k^2} - \\frac{nk}{k^2}\\right) = \\sum_{k=1}^{n} \\frac{n^2}{k^2} - \\sum_{k=1}^{n} \\frac{n}{k}$$\nFactoring out the terms that do not depend on the summation index $k$:\n$$\\text{Var}(T) = n^2 \\sum_{k=1}^{n} \\frac{1}{k^2} - n \\sum_{k=1}^{n} \\frac{1}{k}$$\nThis is the final closed-form expression for the variance of $T$.", "answer": "$$\\boxed{n^{2} \\sum_{k=1}^{n} \\frac{1}{k^{2}} - n \\sum_{k=1}^{n} \\frac{1}{k}}$$", "id": "1369276"}, {"introduction": "Our final practice tackles a more advanced and realistic scenario: calculating the variance of a sum of dependent random variables. In many systems, components interact, and their outcomes are correlated. This exercise uses a specific Boolean satisfiability problem to explore how these dependencies affect overall variance, introducing the crucial concept of covariance. Mastering this allows you to analyze the variability of complex interacting systems, a key skill in fields ranging from computer science to statistical physics [@problem_id:1369279].", "problem": "Consider a set of $n$ Boolean variables, $x_1, x_2, \\dots, x_n$, where $n \\geq 3$. A truth assignment is chosen by setting each variable $x_i$ to be True or False independently and with equal probability of $1/2$. A literal is defined as either a variable $x_i$ or its negation $\\neg x_i$.\n\nWe construct a Boolean formula consisting of the logical AND of $m=n$ clauses. Each clause is the logical OR of two literals. Specifically, the clauses are defined as follows:\n- For each $i$ from $1$ to $n-1$, the clause is $C_i = (x_i \\lor \\neg x_{i+1})$.\n- The final clause is $C_n = (x_n \\lor \\neg x_1)$.\n\nLet $X$ be the random variable representing the total number of satisfied clauses for a randomly chosen truth assignment. Determine the variance of $X$, denoted as $\\text{Var}(X)$, as an expression in terms of $n$.", "solution": "Let $I_{i}$ be the indicator that clause $C_{i}$ is satisfied. Then $X=\\sum_{i=1}^{n} I_{i}$. We use\n$$\n\\text{Var}(X)=\\sum_{i=1}^{n} \\text{Var}(I_{i})+2\\sum_{1 \\leq i<j \\leq n} \\text{Cov}(I_{i},I_{j}).\n$$\n\nEach clause $C_{i}=(x_{i}\\lor \\neg x_{i+1})$ is false only when $(x_{i},x_{i+1})=(0,1)$. Since the variables are independent and fair,\n$$\n\\mathbb{P}(C_{i}\\ \\text{is satisfied})=1-\\mathbb{P}(x_{i}=0,x_{i+1}=1)=1-\\frac{1}{4}=\\frac{3}{4}.\n$$\nHence\n$$\n\\mathbb{E}[I_{i}]=\\frac{3}{4},\\qquad \\text{Var}(I_{i})=\\frac{3}{4}\\left(1-\\frac{3}{4}\\right)=\\frac{3}{16}.\n$$\n\nFor $i$ and $j$ with clauses sharing no variables (i.e., $j\\notin\\{i-1,i+1\\}$ modulo $n$), the indicators $I_{i}$ and $I_{j}$ depend on disjoint sets of independent variables, so they are independent and\n$$\n\\text{Cov}(I_{i},I_{j})=0.\n$$\n\nIt remains to compute $\\text{Cov}(I_{i},I_{i+1})$. Let $a=x_{i}$, $b=x_{i+1}$, and $c=x_{i+2}$. Then $I_{i}=1$ unless $(a,b)=(0,1)$, and $I_{i+1}=1$ unless $(b,c)=(0,1)$. The two bad events $(a=0,b=1)$ and $(b=0,c=1)$ are disjoint because they require $b=1$ and $b=0$, respectively. Therefore,\n$$\n\\mathbb{P}(I_{i}=1,\\ I_{i+1}=1)=1-\\mathbb{P}(a=0,b=1)-\\mathbb{P}(b=0,c=1)=1-\\frac{1}{4}-\\frac{1}{4}=\\frac{1}{2}.\n$$\nThus,\n$$\n\\mathbb{E}[I_{i}I_{i+1}]=\\frac{1}{2},\\qquad \\text{Cov}(I_{i},I_{i+1})=\\mathbb{E}[I_{i}I_{i+1}]-\\mathbb{E}[I_{i}]\\mathbb{E}[I_{i+1}]=\\frac{1}{2}-\\left(\\frac{3}{4}\\right)^{2}=-\\frac{1}{16}.\n$$\n\nOn the cycle, there are exactly $n$ adjacent pairs contributing this covariance: $(1,2),(2,3),\\dots,(n-1,n),(n,1)$. Therefore,\n$$\n\\text{Var}(X)=n\\cdot \\frac{3}{16}+2\\cdot n\\left(-\\frac{1}{16}\\right)=\\frac{3n}{16}-\\frac{2n}{16}=\\frac{n}{16}.\n$$", "answer": "$$\\boxed{\\frac{n}{16}}$$", "id": "1369279"}]}