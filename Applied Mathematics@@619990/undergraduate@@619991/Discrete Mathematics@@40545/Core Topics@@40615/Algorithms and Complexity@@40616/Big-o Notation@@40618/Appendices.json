{"hands_on_practices": [{"introduction": "Understanding Big-O notation begins with its formal definition, which can often feel abstract. This first practice problem makes the concept concrete by tasking you with finding the specific constants that prove an asymptotic relationship holds. By directly manipulating the inequality involved in the Big-O definition, you will build a foundational skill for more complex algorithmic analysis.", "problem": "Let a function $f(n)$ be defined for positive integers $n$ as $f(n) = 3n^2 \\log_{10}(n) + 12n^2$. Find the smallest positive integer $C$ such that the inequality $f(n) \\le C \\cdot n^2 \\log_{10}(n)$ is satisfied for all integers $n \\ge 100$.", "solution": "We are given $f(n) = 3n^{2}\\log_{10}(n) + 12n^{2}$ and seek the smallest positive integer $C$ such that for all integers $n \\ge 100$,\n$$\nf(n) \\le C \\cdot n^{2}\\log_{10}(n).\n$$\nFor $n \\ge 100$, we have $n^{2} > 0$ and $\\log_{10}(n) > 0$, so we may divide both sides of the inequality by $n^{2}\\log_{10}(n)$ without changing the inequality direction:\n$$\n\\frac{3n^{2}\\log_{10}(n) + 12n^{2}}{n^{2}\\log_{10}(n)} \\le C\n\\;\\;\\Longleftrightarrow\\;\\;\n3 + \\frac{12}{\\log_{10}(n)} \\le C.\n$$\nDefine $h(n) = 3 + \\frac{12}{\\log_{10}(n)}$. Since $\\log_{10}(n)$ is strictly increasing for $n \\ge 1$, the function $\\frac{12}{\\log_{10}(n)}$ is strictly decreasing, hence $h(n)$ is strictly decreasing for $n \\ge 1$. Therefore, on the domain of integers $n \\ge 100$, the maximum of $h(n)$ occurs at the smallest $n$, namely $n=100$. Evaluating,\n$$\nh(100) = 3 + \\frac{12}{\\log_{10}(100)} = 3 + \\frac{12}{2} = 9.\n$$\nThus, for all integers $n \\ge 100$,\n$$\n3 + \\frac{12}{\\log_{10}(n)} \\le 9,\n$$\nso $C=9$ satisfies the required inequality. Moreover, any $C < 9$ would violate the inequality at $n=100$, hence the smallest positive integer $C$ is $9$.", "answer": "$$\\boxed{9}$$", "id": "1351719"}, {"introduction": "Once you are comfortable with the formal definition, the next step is to apply it efficiently to functions commonly encountered in computer science. Many algorithms have running times described by rational functions, which are ratios of polynomials. This exercise guides you in determining the tight asymptotic bound for such a function, illustrating a practical shortcut based on the dominant terms that is much faster than working from first principles every time.", "problem": "The running time of a particular algorithm on an input of size $n$ (for $n \\ge 1$) is described by the function $f(n)$ given by:\n$$f(n) = \\frac{(n^2+1)(n+3)}{2n+1}$$\nDetermine the tightest asymptotic bound for this function, expressed using Big-Theta ($\\Theta$) notation. Select the correct complexity class from the options below.\n\nA. $\\Theta(n)$\n\nB. $\\Theta(n \\log n)$\n\nC. $\\Theta(n^2)$\n\nD. $\\Theta(n^3)$", "solution": "We are given the function\n$$\nf(n)=\\frac{(n^{2}+1)(n+3)}{2n+1}, \\quad n\\geq 1.\n$$\nFirst expand the numerator:\n$$\n(n^{2}+1)(n+3)=n^{3}+3n^{2}+n+3.\n$$\nThus,\n$$\nf(n)=\\frac{n^{3}+3n^{2}+n+3}{2n+1}.\n$$\nPerform polynomial division by seeking constants $A$, $B$, and $C$ and a constant remainder $R$ such that\n$$\n\\frac{n^{3}+3n^{2}+n+3}{2n+1}=A n^{2}+B n+C+\\frac{R}{2n+1}.\n$$\nEquivalently,\n$$\nn^{3}+3n^{2}+n+3=(2n+1)(A n^{2}+B n+C)+R.\n$$\nExpand the product on the right:\n$$\n(2n+1)(A n^{2}+B n+C)=2A n^{3}+(2B+A)n^{2}+(2C+B)n+C.\n$$\nMatch coefficients with the left-hand side:\n- From $n^{3}$: $2A=1 \\implies A=\\frac{1}{2}$.\n- From $n^{2}$: $2B+A=3 \\implies 2B+\\frac{1}{2}=3 \\implies B=\\frac{5}{4}$.\n- From $n^{1}$: $2C+B=1 \\implies 2C+\\frac{5}{4}=1 \\implies C=-\\frac{1}{8}$.\n- For the constant term, the product contributes $C=-\\frac{1}{8}$, so to reach $3$ we require\n$$\nR=3-\\left(-\\frac{1}{8}\\right)=\\frac{25}{8}.\n$$\nTherefore,\n$$\nf(n)=\\frac{1}{2}n^{2}+\\frac{5}{4}n-\\frac{1}{8}+\\frac{\\frac{25}{8}}{2n+1}.\n$$\nThe last term is $\\Theta\\!\\left(\\frac{1}{n}\\right)$, and the remaining polynomial part is dominated by its leading term $\\frac{1}{2}n^{2}$. Hence\n$$\nf(n)=\\frac{1}{2}n^{2}+O(n)=\\Theta(n^{2}).\n$$\nTherefore, the tightest asymptotic bound is $\\Theta(n^{2})$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1351732"}, {"introduction": "Real-world algorithms often involve sequences of operations where the cost is not uniform; some steps are cheap, while others are occasionally expensive. This problem models a classic scenario of a self-expanding data structure, introducing the powerful concept of amortized analysis. By calculating the total cost over a sequence of $n$ operations, you will discover how the overall complexity can be determined by the average cost, which is often more manageable than the worst-case cost of a single operation might suggest.", "problem": "A new type of Self-Expanding Digital Archive (SEDA) is being developed to store a growing collection of data files. The process of adding files to the archive is sequential. For the $i$-th file added to the SEDA (where $i$ is the total number of files after the addition), the computational cost is determined as follows:\n\n- If the total number of files $i$ is a power of 2 (e.g., 1, 2, 4, 8, ...), the SEDA must perform a full re-indexing of all $i$ files. The cost of this operation is $i$ computational units.\n- Otherwise, the SEDA performs a simple insertion with a cost of 1 computational unit.\n\nStarting with an empty archive (0 files), $n$ files are added one by one. Determine the asymptotic complexity of the total computational cost required to add all $n$ files. Select the tightest simple Big-O bound from the options below.\n\nA. $O(\\log n)$\n\nB. $O(n)$\n\nC. $O(n \\log n)$\n\nD. $O(n^2)$\n\nE. $O(2^n)$", "solution": "Let $c_{i}$ denote the cost of adding the $i$-th file. Then\n$$\nc_{i}=\\begin{cases}\ni, & \\text{if } i \\text{ is a power of } 2,\\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\nThe total cost after adding $n$ files is\n$$\nT(n)=\\sum_{i=1}^{n} c_{i}.\n$$\nLet $k=\\lfloor \\log_{2}(n)\\rfloor$. The powers of $2$ not exceeding $n$ are $2^{0},2^{1},\\ldots,2^{k}$, so there are $k+1$ such indices. The total re-indexing cost is\n$$\n\\sum_{j=0}^{k} 2^{j}=2^{k+1}-1.\n$$\nAll other $n-(k+1)$ insertions cost $1$ each. Therefore,\n$$\nT(n)=\\bigl(n-(k+1)\\bigr)+\\bigl(2^{k+1}-1\\bigr)=n+2^{k+1}-k-2.\n$$\nUsing $2^{k}\\le n<2^{k+1}$, we obtain bounds for $2^{k+1}$:\n$$\nn<2^{k+1}\\le 2n.\n$$\nSubstituting into $T(n)$ gives\n$$\n2n-k-2<T(n)\\le 3n-k-2.\n$$\nHence $T(n)$ grows linearly with $n$, i.e., $T(n)\\in \\Theta(n)$, so the tightest simple Big-O bound is $O(n)$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1351702"}]}