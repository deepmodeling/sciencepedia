{"hands_on_practices": [{"introduction": "To truly grasp Tucker decomposition, we begin with the fundamental process of tensor reconstruction. This practice demystifies the abstract formula by having you compute a single element of a tensor from its core and factor matrices. By working through this calculation, you will gain a concrete intuition for how the smaller core tensor $\\mathcal{G}$ and the factor matrices interact to build the full, higher-dimensional data structure [@problem_id:1561851].", "problem": "In tensor analysis, the Tucker decomposition provides a way to express a higher-order tensor as a core tensor multiplied by a matrix along each mode. For a third-order tensor $\\mathcal{T}$ of size $I_1 \\times I_2 \\times I_3$, the decomposition is given by\n$$\n\\mathcal{T} \\approx \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}\n$$\nwhere $\\mathcal{G}$ is the core tensor of size $J_1 \\times J_2 \\times J_3$, and $U^{(1)}$, $U^{(2)}$, $U^{(3)}$ are the factor matrices of sizes $I_1 \\times J_1$, $I_2 \\times J_2$, and $I_3 \\times J_3$ respectively. The notation $\\times_n$ denotes the $n$-mode product.\n\nAn element $t_{i_1 i_2 i_3}$ of the tensor $\\mathcal{T}$ can be calculated using the elements of the core tensor ($g_{j_1 j_2 j_3}$) and the factor matrices ($u^{(n)}_{i_n j_n}$) with the following formula:\n$$\nt_{i_1 i_2 i_3} = \\sum_{j_1=1}^{J_1} \\sum_{j_2=1}^{J_2} \\sum_{j_3=1}^{J_3} g_{j_1 j_2 j_3} u^{(1)}_{i_1 j_1} u^{(2)}_{i_2 j_2} u^{(3)}_{i_3 j_3}\n$$\n\nConsider a third-order tensor $\\mathcal{T}$ of size $2 \\times 3 \\times 2$ that is reconstructed from a core tensor $\\mathcal{G}$ of size $2 \\times 2 \\times 2$ and three factor matrices $U^{(1)}$, $U^{(2)}$, and $U^{(3)}$.\n\nThe core tensor $\\mathcal{G}$ is given by its frontal slices $\\mathcal{G}_{:,:,1}$ (where $j_3=1$) and $\\mathcal{G}_{:,:,2}$ (where $j_3=2$):\n$$\n\\mathcal{G}_{:,:,1} = \\begin{pmatrix} 1 & 2 \\\\ 0 & -1 \\end{pmatrix}, \\quad \\mathcal{G}_{:,:,2} = \\begin{pmatrix} 3 & 0 \\\\ -2 & 1 \\end{pmatrix}\n$$\n\nThe factor matrices are given as:\n$$\nU^{(1)} = \\begin{pmatrix} 1 & 5 \\\\ 2 & 1 \\end{pmatrix} \\quad (\\text{size } 2 \\times 2)\n$$\n$$\nU^{(2)} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 2 \\\\ 3 & 1 \\end{pmatrix} \\quad (\\text{size } 3 \\times 2)\n$$\n$$\nU^{(3)} = \\begin{pmatrix} 4 & 1 \\\\ 0 & 2 \\end{pmatrix} \\quad (\\text{size } 2 \\times 2)\n$$\n\nUsing the information provided, compute the value of the element $t_{212}$ of the reconstructed tensor $\\mathcal{T}$.", "solution": "We use the Tucker reconstruction formula for an element:\n$$\nt_{i_{1}i_{2}i_{3}}=\\sum_{j_{1}=1}^{J_{1}}\\sum_{j_{2}=1}^{J_{2}}\\sum_{j_{3}=1}^{J_{3}} g_{j_{1}j_{2}j_{3}}\\,u^{(1)}_{i_{1}j_{1}}\\,u^{(2)}_{i_{2}j_{2}}\\,u^{(3)}_{i_{3}j_{3}}.\n$$\nFor $t_{212}$, we set $i_{1}=2$, $i_{2}=1$, $i_{3}=2$, and with $J_{1}=J_{2}=J_{3}=2$ obtain\n$$\nt_{212}=\\sum_{j_{1}=1}^{2}\\sum_{j_{2}=1}^{2}\\sum_{j_{3}=1}^{2} g_{j_{1}j_{2}j_{3}}\\,u^{(1)}_{2j_{1}}\\,u^{(2)}_{1j_{2}}\\,u^{(3)}_{2j_{3}}.\n$$\nFrom the given factor matrices,\n$$\nu^{(1)}_{21}=2,\\quad u^{(1)}_{22}=1,\\quad u^{(2)}_{11}=1,\\quad u^{(2)}_{12}=0,\\quad u^{(3)}_{21}=0,\\quad u^{(3)}_{22}=2.\n$$\nTherefore only the terms with $j_{2}=1$ and $j_{3}=2$ contribute, reducing the sum to\n$$\nt_{212}=\\sum_{j_{1}=1}^{2} g_{j_{1},1,2}\\,u^{(1)}_{2j_{1}}\\,u^{(2)}_{1,1}\\,u^{(3)}_{2,2}\n=2\\sum_{j_{1}=1}^{2} g_{j_{1},1,2}\\,u^{(1)}_{2j_{1}}.\n$$\nFrom the core slices,\n$$\n\\mathcal{G}_{:,:,2}=\\begin{pmatrix}3 & 0 \\\\ -2 & 1\\end{pmatrix}\\;\\Rightarrow\\; g_{1,1,2}=3,\\; g_{2,1,2}=-2.\n$$\nHence,\n$$\n\\sum_{j_{1}=1}^{2} g_{j_{1},1,2}\\,u^{(1)}_{2j_{1}}=3\\cdot 2+(-2)\\cdot 1=6-2=4,\n$$\nand therefore\n$$\nt_{212}=2\\cdot 4=8.\n$$", "answer": "$$\\boxed{8}$$", "id": "1561851"}, {"introduction": "One of the most powerful applications of Tucker decomposition is its ability to achieve significant data compression. This exercise shifts our focus from the mechanics of reconstruction to the practical benefits of the representation [@problem_id:1561886]. By calculating the total number of parameters required to store the decomposition's components, you will directly quantify its efficiency compared to storing the original, large-scale tensor.", "problem": "In machine learning, tensor decompositions are used to compress large multi-dimensional arrays of data, such as a video stream or data from sensor arrays. One common method is the Tucker decomposition.\n\nConsider a 3rd-order tensor $\\mathcal{X}$ of size $60 \\times 50 \\times 40$. We wish to approximate this tensor using a Tucker decomposition, which represents $\\mathcal{X}$ in terms of a smaller core tensor $\\mathcal{G}$ and a set of factor matrices. The relationship is given by $\\mathcal{X} \\approx \\mathcal{G} \\times_1 A^{(1)} \\times_2 A^{(2)} \\times_3 A^{(3)}$, where $\\times_n$ denotes the n-mode product.\n\nThe dimensions of the components are determined by the original tensor's dimensions and the chosen ranks of the decomposition. For our tensor $\\mathcal{X} \\in \\mathbb{R}^{60 \\times 50 \\times 40}$, we choose the ranks to be $(R_1, R_2, R_3) = (5, 4, 3)$. This means the core tensor is $\\mathcal{G} \\in \\mathbb{R}^{5 \\times 4 \\times 3}$, and the factor matrices are $A^{(1)} \\in \\mathbb{R}^{60 \\times 5}$, $A^{(2)} \\in \\mathbb{R}^{50 \\times 4}$, and $A^{(3)} \\in \\mathbb{R}^{40 \\times 3}$.\n\nCalculate the total number of parameters required to store this Tucker decomposition, which is the sum of all elements in the core tensor $\\mathcal{G}$ and the three factor matrices $A^{(1)}$, $A^{(2)}$, and $A^{(3)}$.", "solution": "The Tucker decomposition stores all elements of the core tensor and the factor matrices. The number of parameters is the sum of their entries, computed as the product of their respective dimensions.\n\nThe core tensor has\n$$\nN_{\\mathcal{G}} = R_{1} R_{2} R_{3} = 5 \\times 4 \\times 3 = 60\n$$\nparameters.\n\nThe factor matrices have\n$$\nN_{A^{(1)}} = 60 \\times 5 = 300,\\quad N_{A^{(2)}} = 50 \\times 4 = 200,\\quad N_{A^{(3)}} = 40 \\times 3 = 120\n$$\nparameters.\n\nTherefore, the total number of parameters is\n$$\nN_{\\text{total}} = N_{\\mathcal{G}} + N_{A^{(1)}} + N_{A^{(2)}} + N_{A^{(3)}} = 60 + 300 + 200 + 120 = 680.\n$$", "answer": "$$\\boxed{680}$$", "id": "1561886"}, {"introduction": "Beyond direct calculation, a deeper understanding of Tucker decomposition involves knowing how the algorithm behaves under different conditions, a crucial skill for practical application. This practice presents a thought experiment concerning the Higher-Order Singular Value Decomposition (HOSVD), a standard algorithm for computing the decomposition [@problem_id:1561841]. By predicting the structure of the core tensor when the rank is overestimated, you will develop a more nuanced insight into the concepts of multilinear rank and algorithmic stability.", "problem": "In tensor analysis, the Tucker decomposition is a fundamental tool for representing a higher-order tensor in terms of a smaller core tensor and a set of factor matrices. The decomposition of a third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}$ is given by\n$$ \\mathcal{X} \\approx \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)} $$\nwhere $\\mathcal{G} \\in \\mathbb{R}^{J_1 \\times J_2 \\times J_3}$ is the core tensor, and $U^{(n)} \\in \\mathbb{R}^{I_n \\times J_n}$ for $n=1,2,3$ are the factor matrices with orthonormal columns. The tuple $(J_1, J_2, J_3)$ represents the size of the core tensor, often referred to as the target rank.\n\nOne of the most common algorithms for computing this decomposition is the Higher-Order Singular Value Decomposition (HOSVD). This algorithm determines the factor matrices $U^{(n)}$ from the singular vectors of the mode-$n$ unfoldings (or matricizations) of $\\mathcal{X}$, denoted by $\\mathbf{X}_{(n)}$.\n\nSuppose we have a tensor $\\mathcal{X}$ whose true multilinear rank is exactly $(R_1, R_2, R_3)$, where $R_n$ is the rank of the mode-$n$ unfolding $\\mathbf{X}_{(n)}$. We then perform HOSVD on $\\mathcal{X}$ but specify a target rank of $(J_1, J_2, J_3) = (R_1+1, R_2, R_3)$. This means we are overestimating the rank in the first mode while correctly specifying it for the other two. In the absence of any numerical precision errors, what is the resulting structure of the computed core tensor $\\mathcal{G} \\in \\mathbb{R}^{(R_1+1) \\times R_2 \\times R_3}$?\n\nA. The frontal slice $\\mathcal{G}(R_1+1, :, :)$ is a zero matrix, while the subtensor $\\mathcal{G}(1:R_1, :, :)$ is generally not all-zero.\n\nB. The frontal slice $\\mathcal{G}(1, :, :)$ is a zero matrix, while the other slices are generally not all-zero.\n\nC. The core tensor $\\mathcal{G}$ is diagonal, i.e., $\\mathcal{G}(j_1, j_2, j_3) = 0$ unless $j_1=j_2=j_3$.\n\nD. All entries of the core tensor $\\mathcal{G}$ are generally non-zero, and there is no guaranteed structural pattern of zeros.\n\nE. The first $R_1$ frontal slices, contained in the subtensor $\\mathcal{G}(1:R_1, :, :)$, are all-zero matrices, while the last slice $\\mathcal{G}(R_1+1, :, :)$ is generally non-zero.", "solution": "We write the Tucker/HOSVD model as\n$$\n\\mathcal{X}\\approx \\mathcal{G}\\times_{1}U^{(1)}\\times_{2}U^{(2)}\\times_{3}U^{(3)},\n$$\nwith orthonormal factor matrices $U^{(n)}\\in\\mathbb{R}^{I_{n}\\times J_{n}}$. In the exact (noise-free) HOSVD, the core is computed by orthogonal projections:\n$$\n\\mathcal{G}=\\mathcal{X}\\times_{1}\\left(U^{(1)}\\right)^{T}\\times_{2}\\left(U^{(2)}\\right)^{T}\\times_{3}\\left(U^{(3)}\\right)^{T}.\n$$\nEquivalently, for the mode-$1$ unfolding,\n$$\n\\mathbf{G}_{(1)}=\\left(U^{(1)}\\right)^{T}\\mathbf{X}_{(1)}\\left(U^{(3)}\\otimes U^{(2)}\\right),\n$$\nwhere $\\otimes$ denotes the Kronecker product.\n\nAssume the true multilinear rank is $(R_{1},R_{2},R_{3})$, so $\\operatorname{rank}\\left(\\mathbf{X}_{(1)}\\right)=R_{1}$. In HOSVD, $U^{(1)}$ is formed from the leading $J_{1}=R_{1}+1$ left singular vectors of $\\mathbf{X}_{(1)}$. Let the thin SVD of $\\mathbf{X}_{(1)}$ be\n$$\n\\mathbf{X}_{(1)}=\\widetilde{U}^{(1)}\\Sigma^{(1)}\\left(\\widetilde{V}^{(1)}\\right)^{T},\n$$\nwhere $\\Sigma^{(1)}\\in\\mathbb{R}^{R_{1}\\times R_{1}}$ has strictly positive diagonal entries. Extending $\\widetilde{U}^{(1)}$ to include one additional orthonormal vector $u_{R_{1}+1}$ in the orthogonal complement of the column space of $\\mathbf{X}_{(1)}$ yields\n$$\nU^{(1)}=\\begin{bmatrix}\\widetilde{U}^{(1)} & u_{R_{1}+1}\\end{bmatrix}\\in\\mathbb{R}^{I_{1}\\times (R_{1}+1)}.\n$$\nBecause $u_{R_{1}+1}$ lies in the orthogonal complement of the column space of $\\mathbf{X}_{(1)}$, it belongs to the left nullspace of $\\mathbf{X}_{(1)}$, hence\n$$\nu_{R_{1}+1}^{T}\\mathbf{X}_{(1)}=\\mathbf{0}^{T}.\n$$\nUsing the unfolding relation for the core,\n$$\n\\mathbf{G}_{(1)}=\\left(U^{(1)}\\right)^{T}\\mathbf{X}_{(1)}\\left(U^{(3)}\\otimes U^{(2)}\\right),\n$$\nthe $(R_{1}+1)$-st row of $\\mathbf{G}_{(1)}$ is\n$$\ne_{R_{1}+1}^{T}\\mathbf{G}_{(1)}=e_{R_{1}+1}^{T}\\left(U^{(1)}\\right)^{T}\\mathbf{X}_{(1)}\\left(U^{(3)}\\otimes U^{(2)}\\right)=u_{R_{1}+1}^{T}\\mathbf{X}_{(1)}\\left(U^{(3)}\\otimes U^{(2)}\\right)=\\mathbf{0}^{T}.\n$$\nTherefore, every entry of $\\mathcal{G}$ with first index $j_{1}=R_{1}+1$ is zero, i.e.,\n$$\n\\mathcal{G}(R_{1}+1,:,:)=0.\n$$\nFor $j_{1}\\leq R_{1}$, we have\n$$\ne_{j_{1}}^{T}\\mathbf{G}_{(1)}=e_{j_{1}}^{T}\\left(U^{(1)}\\right)^{T}\\mathbf{X}_{(1)}\\left(U^{(3)}\\otimes U^{(2)}\\right),\n$$\nand since $e_{j_{1}}^{T}\\left(U^{(1)}\\right)^{T}\\mathbf{X}_{(1)}$ corresponds to a nonzero singular value row (because $j_{1}\\leq R_{1}$), these rows are generally nonzero; right multiplication by $\\left(U^{(3)}\\otimes U^{(2)}\\right)$ does not annihilate them generically. Hence the subtensor $\\mathcal{G}(1:R_{1},:,:)$ is generally not all zero.\n\nThis exactly matches option A: the extra slice induced by overestimating the first mode rank is identically zero, while the others are generally nonzero.", "answer": "$$\\boxed{A}$$", "id": "1561841"}]}