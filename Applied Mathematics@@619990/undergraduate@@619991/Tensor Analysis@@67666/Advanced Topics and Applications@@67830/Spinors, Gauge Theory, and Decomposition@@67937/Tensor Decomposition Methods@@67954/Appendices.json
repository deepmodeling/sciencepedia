{"hands_on_practices": [{"introduction": "The CANDECOMP/PARAFAC (CP) decomposition expresses a tensor as a sum of rank-one components. A key to understanding and implementing this method is the Khatri-Rao product, which is used to formulate the decomposition as a linear system. This exercise [@problem_id:1542398] provides direct practice in computing the column-wise Khatri-Rao product, a fundamental operation you will encounter frequently when working with matricized tensor representations.", "problem": "In tensor analysis, the column-wise Khatri-Rao product is a fundamental operation used in various tensor decomposition methods, such as CANDECOMP/PARAFAC (CP) decomposition.\n\nConsider two matrices, $B$ and $C$, with real-valued entries, given by:\n$$\nB = \\begin{pmatrix} 1 & 5 \\\\ 2 & 0 \\\\ 3 & 1 \\end{pmatrix}\n$$\nand\n$$\nC = \\begin{pmatrix} 4 & 2 \\\\ 6 & 3 \\\\ 7 & 8 \\end{pmatrix}\n$$\n\nCompute their column-wise Khatri-Rao product, denoted as $A = C \\odot B$.", "solution": "The column-wise Khatri-Rao product of two matrices with the same number of columns is defined as follows: if $B \\in \\mathbb{R}^{I \\times K}$ and $C \\in \\mathbb{R}^{J \\times K}$, then $C \\odot B \\in \\mathbb{R}^{IJ \\times K}$ has columns given by $(C \\odot B)_{: , k} = c_{k} \\otimes b_{k}$ for $k = 1, \\ldots, K$, where $\\otimes$ denotes the Kronecker product.\n\nGiven\n$$\nB = \\begin{pmatrix} 1 & 5 \\\\ 2 & 0 \\\\ 3 & 1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 4 & 2 \\\\ 6 & 3 \\\\ 7 & 8 \\end{pmatrix},\n$$\nboth have $K = 2$ columns, so $A = C \\odot B \\in \\mathbb{R}^{9 \\times 2}$.\n\nCompute the first column using $c_{1} = \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\end{pmatrix}$ and $b_{1} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$:\n$$\nc_{1} \\otimes b_{1} = \\begin{pmatrix} 4 b_{1} \\\\ 6 b_{1} \\\\ 7 b_{1} \\end{pmatrix}\n= \\begin{pmatrix} 4 \\\\ 8 \\\\ 12 \\\\ 6 \\\\ 12 \\\\ 18 \\\\ 7 \\\\ 14 \\\\ 21 \\end{pmatrix}.\n$$\n\nCompute the second column using $c_{2} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 8 \\end{pmatrix}$ and $b_{2} = \\begin{pmatrix} 5 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\n$$\nc_{2} \\otimes b_{2} = \\begin{pmatrix} 2 b_{2} \\\\ 3 b_{2} \\\\ 8 b_{2} \\end{pmatrix}\n= \\begin{pmatrix} 10 \\\\ 0 \\\\ 2 \\\\ 15 \\\\ 0 \\\\ 3 \\\\ 40 \\\\ 0 \\\\ 8 \\end{pmatrix}.\n$$\n\nStacking these as columns gives\n$$\nA = C \\odot B = \\begin{pmatrix}\n4 & 10 \\\\\n8 & 0 \\\\\n12 & 2 \\\\\n6 & 15 \\\\\n12 & 0 \\\\\n18 & 3 \\\\\n7 & 40 \\\\\n14 & 0 \\\\\n21 & 8\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\n4 & 10 \\\\\n8 & 0 \\\\\n12 & 2 \\\\\n6 & 15 \\\\\n12 & 0 \\\\\n18 & 3 \\\\\n7 & 40 \\\\\n14 & 0 \\\\\n21 & 8\n\\end{pmatrix}}$$", "id": "1542398"}, {"introduction": "While CP decomposition characterizes a tensor with a single rank, the Tucker decomposition offers a more flexible model by defining a rank for each mode. To determine these ranks, we must first \"unfold\" or \"matricize\" the tensor into a series of matrices and then find their respective matrix ranks. This practice [@problem_id:1542439] walks you through this essential process, helping you calculate the multilinear rank of a tensor and gain a deeper intuition for its underlying structure.", "problem": "In tensor analysis, a higher-order tensor can be represented in a matrix format through a process called matricization or unfolding. This allows for the application of standard matrix analysis tools to understand the tensor's properties. One such property is the multilinear rank.\n\nConsider a third-order tensor $T \\in \\mathbb{R}^{2 \\times 3 \\times 4}$. The elements of $T$, denoted $t_{ijk}$ where $i \\in \\{1,2\\}$, $j \\in \\{1,2,3\\}$, and $k \\in \\{1,2,3,4\\}$, are given by its four frontal slices, $F_k = T(:,:,k) \\in \\mathbb{R}^{2 \\times 3}$:\n\n$F_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n\n$F_2 = \\begin{pmatrix} 0 & 1 & 1 \\\\ 1 & -1 & 0 \\end{pmatrix}$\n\n$F_3 = \\begin{pmatrix} 2 & 1 & 3 \\\\ 1 & 0 & 1 \\end{pmatrix}$\n\n$F_4 = \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 0 & 1 \\end{pmatrix}$\n\nThe tensor $T$ can be matricized in three primary ways, corresponding to each of its modes. For a tensor of size $I_1 \\times I_2 \\times I_3$:\n- The mode-1 matricization, $T_{(1)}$, is a matrix of size $I_1 \\times (I_2 I_3)$ whose rows are the vectorized horizontal slices $T(i,:,:)$.\n- The mode-2 matricization, $T_{(2)}$, is a matrix of size $I_2 \\times (I_1 I_3)$ whose rows are the vectorized lateral slices $T(:,j,:)$.\n- The mode-3 matricization, $T_{(3)}$, is a matrix of size $I_3 \\times (I_1 I_2)$ whose rows are the vectorized frontal slices $T(:,:,k)$.\n\nFor the purpose of vectorization, a matrix is flattened by stacking its columns sequentially. For example, for a matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, its vectorization is $\\text{vec}(M) = \\begin{pmatrix} a \\\\ c \\\\ b \\\\ d \\end{pmatrix}$.\n\nThe multilinear rank of the tensor $T$ is defined as the tuple of the ranks of its matricizations, $(R_1, R_2, R_3)$, where $R_n = \\text{rank}(T_{(n)})$.\n\nCalculate the multilinear rank $(R_1, R_2, R_3)$ for the given tensor $T$. Present your answer as a row matrix with three integer elements.", "solution": "We are given a third-order tensor $T \\in \\mathbb{R}^{2 \\times 3 \\times 4}$ via its frontal slices $F_{k} = T(:,:,k) \\in \\mathbb{R}^{2 \\times 3}$, with vectorization defined by stacking matrix columns. The multilinear rank is $(R_{1},R_{2},R_{3})$ where $R_{n} = \\operatorname{rank}(T_{(n)})$.\n\nMode-3 matricization $T_{(3)}$ has size $I_{3} \\times (I_{1} I_{2}) = 4 \\times 6$ and its $k$-th row is $\\operatorname{vec}(F_{k})^{\\top}$. Compute the four vectors:\n$$\n\\operatorname{vec}(F_{1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix},\\quad\n\\operatorname{vec}(F_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix},\\quad\n\\operatorname{vec}(F_{3}) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 3 \\\\ 1 \\end{pmatrix},\\quad\n\\operatorname{vec}(F_{4}) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nObserve that $F_{4} = F_{1} + F_{2}$, hence $\\operatorname{vec}(F_{4}) = \\operatorname{vec}(F_{1}) + \\operatorname{vec}(F_{2})$, so $\\operatorname{rank}(T_{(3)}) \\leq 3$. To check that $\\operatorname{vec}(F_{1}), \\operatorname{vec}(F_{2}), \\operatorname{vec}(F_{3})$ are independent, suppose $a\\,F_{1} + b\\,F_{2} + c\\,F_{3} = 0$. Looking entrywise at positions $(2,2)$, $(1,1)$, and $(2,1)$ yields\n$$\na - b = 0,\\quad a + 2c = 0,\\quad b + c = 0.\n$$\nFrom $a=b$ and $b = -c$, we get $a = -c$, and then $a + 2c = 0$ gives $c = 0$, hence $a=b=0$. Thus the three are independent, so $R_{3} = \\operatorname{rank}(T_{(3)}) = 3$.\n\nMode-1 matricization $T_{(1)}$ has size $I_{1} \\times (I_{2} I_{3}) = 2 \\times 12$, with row $i$ equal to $\\operatorname{vec}(T(i,:,:))^{\\top}$. For $i=1$, form $M_{1} = T(1,:,:) \\in \\mathbb{R}^{3 \\times 4}$ whose $k$-th column is the first row of $F_{k}$ transposed:\n$$\nM_{1} = \\begin{pmatrix}\n1 & 0 & 2 & 1 \\\\\n0 & 1 & 1 & 1 \\\\\n1 & 1 & 3 & 2\n\\end{pmatrix},\\quad\n\\operatorname{vec}(M_{1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 1 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\nFor $i=2$, form $M_{2} = T(2,:,:) \\in \\mathbb{R}^{3 \\times 4}$ whose $k$-th column is the second row of $F_{k}$ transposed:\n$$\nM_{2} = \\begin{pmatrix}\n0 & 1 & 1 & 1 \\\\\n1 & -1 & 0 & 0 \\\\\n1 & 0 & 1 & 1\n\\end{pmatrix},\\quad\n\\operatorname{vec}(M_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe two row vectors of $T_{(1)}$ are $\\operatorname{vec}(M_{1})^{\\top}$ and $\\operatorname{vec}(M_{2})^{\\top}$. They are not proportional because their first components differ ($1$ versus $0$). Therefore $R_{1} = \\operatorname{rank}(T_{(1)}) = 2$.\n\nMode-2 matricization $T_{(2)}$ has size $I_{2} \\times (I_{1} I_{3}) = 3 \\times 8$, with row $j$ equal to $\\operatorname{vec}(T(:,j,:))^{\\top}$. For each $j$, $T(:,j,:)$ is the $2 \\times 4$ matrix whose $k$-th column is the $j$-th column of $F_{k}$. Thus:\n$$\nA_{1} = T(:,1,:) = \\begin{pmatrix} 1 & 0 & 2 & 1 \\\\ 0 & 1 & 1 & 1 \\end{pmatrix},\\quad\n\\operatorname{vec}(A_{1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix},\n$$\n$$\nA_{2} = T(:,2,:) = \\begin{pmatrix} 0 & 1 & 1 & 1 \\\\ 1 & -1 & 0 & 0 \\end{pmatrix},\\quad\n\\operatorname{vec}(A_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\n$$\n$$\nA_{3} = T(:,3,:) = \\begin{pmatrix} 1 & 1 & 3 & 2 \\\\ 1 & 0 & 1 & 1 \\end{pmatrix},\\quad\n\\operatorname{vec}(A_{3}) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 3 \\\\ 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nWe have $A_{3} = A_{1} + A_{2}$, hence $\\operatorname{vec}(A_{3}) = \\operatorname{vec}(A_{1}) + \\operatorname{vec}(A_{2})$, so $\\operatorname{rank}(T_{(2)}) \\leq 2$. Since $\\operatorname{vec}(A_{1})$ and $\\operatorname{vec}(A_{2})$ are not proportional (their first entries are $1$ and $0$), they are independent. Therefore $R_{2} = \\operatorname{rank}(T_{(2)}) = 2$.\n\nCollecting the three ranks gives the multilinear rank tuple $(R_{1}, R_{2}, R_{3}) = (2, 2, 3)$.", "answer": "$$\\boxed{\\begin{pmatrix} 2 & 2 & 3 \\end{pmatrix}}$$", "id": "1542439"}, {"introduction": "Finding the components of a tensor decomposition often requires iterative algorithms, much like finding eigenvectors in linear algebra. This problem [@problem_id:1542377] introduces the tensor power method, a powerful generalization of the classic matrix power method, for finding the dominant rank-one component of a symmetric tensor. By formulating this algorithm, you will connect the abstract concept of tensor decomposition to the practical steps of computational implementation.", "problem": "In higher-order data analysis, a 3rd-order tensor $T \\in \\mathbb{R}^{d \\times d \\times d}$ can model three-way relationships within a dataset. We consider a tensor $T$ that is fully symmetric, meaning its components are invariant under any permutation of their indices, i.e., $T_{ijk} = T_{ikj} = T_{jik}$, etc.\n\nA fundamental task is to find the dominant rank-1 component of $T$. This involves finding the best approximation of $T$ by a rank-1 tensor of the form $\\lambda(u \\otimes u \\otimes u)$, where $u \\in \\mathbb{R}^d$ is a unit vector ($||u||_2=1$), $\\lambda$ is a scalar, and $\\otimes$ denotes the outer product. The components of this rank-1 tensor are given by $(\\lambda(u \\otimes u \\otimes u))_{ijk} = \\lambda u_i u_j u_k$.\n\nThe vector $u$ that provides the best approximation is one that maximizes the objective function $f(u) = \\sum_{i,j,k=1}^{d} T_{ijk} u_i u_j u_k$, subject to the constraint $||u||_2 = 1$. The stationary points of this constrained optimization problem satisfy the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, where $T(u,u)$ is a vector whose $i$-th component is defined by the double contraction $(T(u,u))_i = \\sum_{j,k=1}^{d} T_{ijk} u_j u_k$.\n\nThis equation inspires a simple iterative algorithm, analogous to the matrix power method, to find the \"dominant\" eigenvector $u$. Starting with a random initial unit vector $u_0$, the algorithm generates a sequence of vectors via the two-step process:\n1.  Compute an unnormalized vector for the next step: $v_{k+1} = \\text{operation}(T, u_k)$\n2.  Normalize the vector: $u_{k+1} = \\frac{v_{k+1}}{||v_{k+1}||_2}$\n\nBased on the structure of the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, determine the expression for the unnormalized vector $v_{k+1}$ in step 1. Express the $i$-th component of this vector, $(v_{k+1})_i$, in terms of the components of the tensor $T$ (e.g., $T_{ijl}$) and the components of the vector from the previous step, $u_k$ (e.g., $(u_k)_j$).", "solution": "We seek the dominant rank-1 component of a fully symmetric tensor $T \\in \\mathbb{R}^{d \\times d \\times d}$ by maximizing $f(u) = \\sum_{i,j,k=1}^{d} T_{ijk} u_{i} u_{j} u_{k}$ subject to the constraint $\\|u\\|_{2} = 1$. The stationary points of this constrained optimization satisfy the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, where $T(u,u)$ denotes the double contraction of $T$ with $u$ in two modes and has components\n$$\n\\big(T(u,u)\\big)_{i} = \\sum_{j=1}^{d} \\sum_{k=1}^{d} T_{ijk} u_{j} u_{k}.\n$$\nThis follows from the Lagrangian $L(u,\\mu) = f(u) - \\mu(u^{\\top}u - 1)$, whose gradient condition gives $\\nabla f(u) = 2 \\mu u$. Using symmetry of $T$, the gradient of $f$ is\n$$\n\\frac{\\partial f}{\\partial u_{i}} = 3 \\sum_{j=1}^{d} \\sum_{k=1}^{d} T_{ijk} u_{j} u_{k} = 3 \\big(T(u,u)\\big)_{i},\n$$\nleading to $T(u,u) = \\lambda u$ with $\\lambda = \\frac{2}{3}\\mu$. \n\nA power-method-like iteration mirrors this eigen-equation by evaluating its left-hand side at the current iterate $u_{k}$ and then normalizing. Therefore, the unnormalized update is the double contraction of $T$ with $u_{k}$:\n$$\nv_{k+1} = T(u_{k}, u_{k}),\n$$\nwhose $i$-th component is obtained by contracting over the second and third indices:\n$$\n(v_{k+1})_{i} = \\sum_{j=1}^{d} \\sum_{l=1}^{d} T_{ijl} (u_{k})_{j} (u_{k})_{l}.\n$$\nBecause $T$ is fully symmetric, the choice of which two indices to contract is immaterial, and the expression above is the standard form for the unnormalized update.", "answer": "$$\\boxed{\\sum_{j=1}^{d}\\sum_{l=1}^{d} T_{ijl}\\,(u_{k})_{j}\\,(u_{k})_{l}}$$", "id": "1542377"}]}