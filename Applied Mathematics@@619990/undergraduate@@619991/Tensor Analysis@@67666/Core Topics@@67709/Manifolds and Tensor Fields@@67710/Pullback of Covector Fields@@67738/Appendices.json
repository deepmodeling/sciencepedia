{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of pullbacks, we will start with a foundational calculation. This exercise asks you to compute the pullback of a simple covector field on a higher-dimensional space ($\\mathbb{R}^4$) to a lower-dimensional one ($\\mathbb{R}^2$). This practice [@problem_id:1533235] is designed to solidify the core mechanics of the pullback operation, showing how to transform both the component functions and the basis covectors according to the defining map.", "problem": "Let the Cartesian coordinates on $\\mathbb{R}^4$ be $(x_1, x_2, x_3, x_4)$ and on $\\mathbb{R}^2$ be $(u,v)$. Consider a smooth map $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^4$ defined by $\\phi(u,v) = (u, v, uv, u+v)$. Let $\\omega$ be a covector field (also known as a 1-form) on $\\mathbb{R}^4$ given by the expression:\n$$\n\\omega = dx_1 + x_3 dx_4\n$$\nCalculate the pullback of $\\omega$ by the map $\\phi$, which is denoted by $\\phi^*\\omega$. Express the resulting covector field on $\\mathbb{R}^2$ in the standard basis $\\{du, dv\\}$.", "solution": "We are given the smooth map $\\phi: \\mathbb{R}^{2} \\to \\mathbb{R}^{4}$ defined by $\\phi(u,v) = (x_{1},x_{2},x_{3},x_{4}) = (u,v,uv,u+v)$ and the 1-form $\\omega = dx_{1} + x_{3}\\,dx_{4}$ on $\\mathbb{R}^{4}$. The pullback of a 1-form under $\\phi$ is determined by linearity and the rules:\n- $\\phi^{*}(dx_{i}) = d(x_{i} \\circ \\phi)$ for coordinate 1-forms,\n- $\\phi^{*}(f\\,\\alpha) = (f \\circ \\phi)\\,\\phi^{*}\\alpha$ for a function $f$ and 1-form $\\alpha$.\n\nApplying these, we compute each component:\n1) $\\phi^{*}(dx_{1}) = d(x_{1} \\circ \\phi) = d(u) = du$.\n2) For the term $x_{3}\\,dx_{4}$, we have $x_{3} \\circ \\phi = uv$ and $x_{4} \\circ \\phi = u+v$, hence\n$$\n\\phi^{*}(x_{3}\\,dx_{4}) = (x_{3} \\circ \\phi)\\,d(x_{4} \\circ \\phi) = (uv)\\,d(u+v) = (uv)\\,(du + dv).\n$$\n\nTherefore,\n$$\n\\phi^{*}\\omega = \\phi^{*}(dx_{1}) + \\phi^{*}(x_{3}\\,dx_{4}) = du + uv\\,(du + dv) = (1+uv)\\,du + uv\\,dv,\n$$\nwhich is expressed in the standard basis $\\{du,dv\\}$ on $\\mathbb{R}^{2}$.", "answer": "$$\\boxed{(1+uv)\\,du+uv\\,dv}$$", "id": "1533235"}, {"introduction": "Having mastered the basic computation, we now shift our focus from \"how\" to \"why.\" This problem [@problem_id:1533219] presents a more conceptual challenge: instead of computing a pullback, you will use a property of the pullback—specifically, that it is identically zero—to deduce the geometric nature of a set of curves. This exercise beautifully illustrates how the pullback serves as a powerful bridge between algebraic formalism and intuitive geometric understanding, allowing us to classify objects based on their differential properties.", "problem": "Consider the Euclidean plane $\\mathbb{R}^2$ with Cartesian coordinates $(x, y)$. Let $\\omega$ be a differential 1-form (also known as a covector field) defined on $\\mathbb{R}^2$ by the expression:\n$$ \\omega = -y \\, dx + x \\, dy $$\nLet $\\gamma: I \\to \\mathbb{R}^2$ be a smooth, non-constant curve defined on an open interval $I \\subseteq \\mathbb{R}$, parameterized by $t \\in I$ as $\\gamma(t) = (x(t), y(t))$.\n\nThe condition for the curve $\\gamma$ is that the pullback of $\\omega$ along $\\gamma$, denoted as $\\gamma^*\\omega$, is identically zero for all $t \\in I$.\n\nWhich of the following options provides a complete geometric description of all such curves $\\gamma$?\n\nA. All circles centered at the origin.\nB. All ellipses centered at the origin.\nC. All straight lines passing through the origin.\nD. All hyperbolas with asymptotes passing through the origin.\nE. All straight lines regardless of their position.", "solution": "We are given the 1-form $\\omega=-y\\,dx+x\\,dy$ on $\\mathbb{R}^{2}$ and a smooth curve $\\gamma(t)=(x(t),y(t))$. The pullback of $\\omega$ along $\\gamma$ is computed by substituting $dx=x'(t)\\,dt$ and $dy=y'(t)\\,dt$, yielding\n$$\n\\gamma^{*}\\omega=\\big(-y(t)\\,x'(t)+x(t)\\,y'(t)\\big)\\,dt=\\big(x(t)\\,y'(t)-y(t)\\,x'(t)\\big)\\,dt.\n$$\nThe condition $\\gamma^{*}\\omega\\equiv 0$ for all $t$ is therefore equivalent to the scalar differential equation\n$x(t)\\,y'(t)-y(t)\\,x'(t)=0 \\quad \\text{for all } t\\in I.$\n\nTo interpret this geometrically, write the curve in polar coordinates as $x=r\\cos\\theta$ and $y=r\\sin\\theta$, where $r=r(t)$ and $\\theta=\\theta(t)$. A direct computation gives\n$x\\,y'-y\\,x' = r^{2}\\,\\theta'$.\nThus the condition becomes\n$r^{2}(t)\\,\\theta'(t)=0 \\quad \\text{for all } t.$\nSince the curve is non-constant, it cannot have $r(t)\\equiv 0$. Therefore, on any interval where $r(t)\\neq 0$, we must have $\\theta'(t)=0$, so $\\theta$ is constant. Hence the image of the curve lies on a straight line through the origin (constant angle with respect to the $x$-axis). The curve may pass through the origin, where $\\theta$ is not defined, but by continuity it remains on the same line through the origin.\n\nEquivalently, the condition $x\\,y'-y\\,x'=0$ states that the tangent vector $(x',y')$ is always colinear with the position vector $(x,y)$, so the curve moves along a fixed line through the origin, with arbitrary speed and orientation, which is a complete geometric characterization.\n\nTherefore, the set of all such curves is exactly all straight lines passing through the origin.\n\nNo inconsistencies are present in the problem statement.", "answer": "$$\\boxed{C}$$", "id": "1533219"}, {"introduction": "In this final practice, we elevate our analysis to a higher level of abstraction, exploring the structural properties of transformations. You are tasked with identifying an entire family of linear maps that preserve a specific covector field up to a constant scaling factor [@problem_id:1533211]. This type of problem is fundamental in modern physics and geometry, as it relates to finding the symmetries of a system—the transformations that leave essential quantities, such as energy or the metric itself, invariant.", "problem": "Let $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^2$ be a linear map. We use coordinates $(x_1, x_2)$ on the domain space and $(y_1, y_2)$ on the codomain space. Any such linear map $\\phi$ can be represented by a unique $2 \\times 2$ real matrix $A$ such that if $(y_1, y_2) = \\phi(x_1, x_2)$, then $\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = A \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nConsider the 1-form (also known as a covector field) $\\omega$ defined on the codomain space, given by the expression $\\omega = y_1 \\, dy_1 + y_2 \\, dy_2$. The pullback of $\\omega$ by the map $\\phi$, denoted $\\phi^*\\omega$, is a 1-form on the domain space.\n\nThe problem is to identify all linear maps $\\phi$ for which the pullback $\\phi^*\\omega$ is a scalar multiple of the corresponding 1-form $x_1 \\, dx_1 + x_2 \\, dx_2$ on the domain space. That is, we seek all maps $\\phi$ for which there exists a real constant $\\lambda$ satisfying the equation $\\phi^*\\omega = \\lambda(x_1 \\, dx_1 + x_2 \\, dx_2)$.\n\nWhich of the following descriptions accurately characterizes the set of all matrices $A$ that represent such maps?\n\nA. All symmetric matrices (i.e., matrices $A$ such that $A^T = A$).\nB. All diagonal matrices.\nC. All matrices that are scalar multiples of an orthogonal matrix.\nD. All orthogonal matrices (i.e., matrices $A$ such that $A^T A = I$, where $I$ is the identity matrix).\nE. All invertible matrices.", "solution": "Let $A$ be the matrix of $\\phi$ so that $y = A x$, where $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ and $y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix}$. The $1$-form on the codomain is $\\omega = y_{1}\\,dy_{1} + y_{2}\\,dy_{2}$, which can be written as $\\omega = y^{T} dy$.\n\nThe pullback by $\\phi$ satisfies\n$$\n\\phi^{*}\\omega \\;=\\; (A x)^{T} \\, d(A x).\n$$\nSince $A$ is constant, $d(A x) = A\\,dx$. Therefore,\n$$\n\\phi^{*}\\omega \\;=\\; (A x)^{T} A\\,dx \\;=\\; x^{T} A^{T} A\\, dx.\n$$\nWriting $S := A^{T}A$, this becomes\n$$\n\\phi^{*}\\omega \\;=\\; x^{T} S\\, dx \\;=\\; \\sum_{j=1}^{2} \\left(\\sum_{i=1}^{2} s_{ij} x_{i}\\right) dx_{j}.\n$$\nWe require\n$$\n\\phi^{*}\\omega \\;=\\; \\lambda \\,(x_{1}\\,dx_{1} + x_{2}\\,dx_{2}) \\;=\\; \\lambda\\, x^{T} I\\, dx.\n$$\nEquating coefficients of $dx_{j}$ for all $x$ gives\n$$\n\\sum_{i=1}^{2} s_{ij} x_{i} \\;=\\; \\lambda x_{j} \\quad \\text{for each } j,\n$$\nwhich is equivalent to\n$$\nS^{T} x \\;=\\; \\lambda x \\quad \\text{for all } x.\n$$\nSince $S = A^{T}A$ is symmetric, $S^{T} = S$, so this implies\n$$\nS x \\;=\\; \\lambda x \\quad \\text{for all } x \\;\\;\\Longrightarrow\\;\\; S \\;=\\; \\lambda I.\n$$\nThus the necessary and sufficient condition is\n$$\nA^{T} A \\;=\\; \\lambda I.\n$$\nBecause $A^{T}A$ is positive semidefinite, $\\lambda \\ge 0$. If $\\lambda > 0$, then $A$ is invertible and $A = \\sqrt{\\lambda}\\,Q$ with $Q$ orthogonal. If $\\lambda = 0$, then $A = 0$, which is also a scalar multiple of any orthogonal matrix. Therefore, the set of all such $A$ is precisely the set of scalar multiples of orthogonal matrices.\n\nAmong the options, this is exactly described by option C.", "answer": "$$\\boxed{C}$$", "id": "1533211"}]}