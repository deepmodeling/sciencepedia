{"hands_on_practices": [{"introduction": "The Gram-Schmidt process is a fundamental algorithm for constructing an orthogonal or orthonormal basis from an arbitrary set of linearly independent vectors. This exercise provides a valuable opportunity to apply the process and observe its behavior when the initial set of vectors is linearly dependent. Understanding this outcome is key to appreciating the deep connection between orthogonality and linear independence [@problem_id:1381095].", "problem": "Consider the vector space $\\mathbb{R}^3$ equipped with the standard Euclidean inner product (dot product). Let $S$ be the following ordered set of vectors:\n$$S = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$$\nwhere $\\mathbf{x}_1 = (1, 1, 0)$, $\\mathbf{x}_2 = (1, 0, 1)$, and $\\mathbf{x}_3 = (2, 1, 1)$.\n\nApply the Gram-Schmidt orthogonalization process to the set $S$ to produce an orthogonal set of vectors $U = \\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}$. Which of the following options represents the set $U$?\n\nA. $\\{(1, 1, 0), (\\frac{1}{2}, -\\frac{1}{2}, 1), (0, 0, 0)\\}$\n\nB. $\\{(1, 1, 0), (1, 0, 1), (2, 1, 1)\\}$\n\nC. $\\{(1, 1, 0), (\\frac{1}{2}, -\\frac{1}{2}, 1), (\\frac{1}{2}, -\\frac{1}{2}, 1)\\}$\n\nD. $\\{(1, 1, 0), (\\frac{1}{2}, -\\frac{1}{2}, 1), (-\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})\\}$\n\nE. $\\{(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0), (\\frac{1}{\\sqrt{6}}, -\\frac{1}{\\sqrt{6}}, \\sqrt{\\frac{2}{3}}), (0, 0, 0)\\}$", "solution": "We work in $\\mathbb{R}^{3}$ with the standard dot product. Apply Gram–Schmidt to $S=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{3}\\}$ with $\\mathbf{x}_{1}=(1,1,0)$, $\\mathbf{x}_{2}=(1,0,1)$, and $\\mathbf{x}_{3}=(2,1,1)$.\n\nSet $\\mathbf{u}_{1}=\\mathbf{x}_{1}=(1,1,0)$.\n\nCompute\n$$\n\\operatorname{proj}_{\\mathbf{u}_{1}}(\\mathbf{x}_{2})=\\frac{\\mathbf{x}_{2}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1}\n=\\frac{(1,0,1)\\cdot(1,1,0)}{(1,1,0)\\cdot(1,1,0)}\\mathbf{u}_{1}\n=\\frac{1}{2}\\mathbf{u}_{1}=\\left(\\frac{1}{2},\\frac{1}{2},0\\right),\n$$\nso\n$$\n\\mathbf{u}_{2}=\\mathbf{x}_{2}-\\operatorname{proj}_{\\mathbf{u}_{1}}(\\mathbf{x}_{2})\n=\\left(1,0,1\\right)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)\n=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nNext,\n$$\n\\operatorname{proj}_{\\mathbf{u}_{1}}(\\mathbf{x}_{3})=\\frac{\\mathbf{x}_{3}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1}\n=\\frac{(2,1,1)\\cdot(1,1,0)}{2}\\mathbf{u}_{1}\n=\\frac{3}{2}\\mathbf{u}_{1}=\\left(\\frac{3}{2},\\frac{3}{2},0\\right),\n$$\nand\n$$\n\\operatorname{proj}_{\\mathbf{u}_{2}}(\\mathbf{x}_{3})=\\frac{\\mathbf{x}_{3}\\cdot\\mathbf{u}_{2}}{\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}}\\mathbf{u}_{2}.\n$$\nCompute $\\mathbf{x}_{3}\\cdot\\mathbf{u}_{2}=(2,1,1)\\cdot\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=\\frac{3}{2}$ and $\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}=\\left(\\frac{1}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+1^{2}=\\frac{3}{2}$, hence\n$$\n\\operatorname{proj}_{\\mathbf{u}_{2}}(\\mathbf{x}_{3})=\\mathbf{u}_{2}=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nTherefore,\n$$\n\\mathbf{u}_{3}=\\mathbf{x}_{3}-\\operatorname{proj}_{\\mathbf{u}_{1}}(\\mathbf{x}_{3})-\\operatorname{proj}_{\\mathbf{u}_{2}}(\\mathbf{x}_{3})\n=(2,1,1)-\\left(\\frac{3}{2},\\frac{3}{2},0\\right)-\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=(0,0,0).\n$$\nThus the orthogonal set produced by Gram–Schmidt is $U=\\{(1,1,0),(\\frac{1}{2},-\\frac{1}{2},1),(0,0,0)\\}$, which matches option A.", "answer": "$$\\boxed{A}$$", "id": "1381095"}, {"introduction": "Exploring the structure of vector spaces often involves decomposing them into a subspace and its orthogonal complement. This practice focuses on finding a basis for the orthogonal complement, $W^\\perp$, of a subspace $W$ spanned by the columns of a matrix. Mastering this skill is essential, as it leverages the powerful relationship between the column space of a matrix $A$ and the null space of its transpose, $A^T$ [@problem_id:1381128].", "problem": "Consider the matrix $A$ given by\n$$ A = \\begin{pmatrix} 1 & 3 \\\\ 0 & 1 \\\\ -2 & -1 \\end{pmatrix} $$\nLet $W = \\text{Col}(A)$ be the column space of $A$, which is a subspace of $\\mathbb{R}^3$. The orthogonal complement of $W$, denoted by $W^\\perp$, is the subspace of all vectors in $\\mathbb{R}^3$ that are orthogonal to every vector in $W$.\n\nFind a basis for the subspace $W^\\perp$. Express the basis vector as a column vector.", "solution": "We are given the matrix\n$$\nA=\\begin{pmatrix}1 & 3 \\\\ 0 & 1 \\\\ -2 & -1\\end{pmatrix}\n$$\nwith column space $W=\\text{Col}(A)\\subset \\mathbb{R}^{3}$. By definition, $W^{\\perp}=\\{v\\in \\mathbb{R}^{3}:\\langle v,w\\rangle=0 \\text{ for all } w\\in W\\}$. A standard linear algebra fact is that $W^{\\perp}=\\text{Null}(A^{T})$, the null space of the transpose of $A$.\n\nLet $v=\\begin{pmatrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{pmatrix}\\in \\mathbb{R}^{3}$. Then $v\\in W^{\\perp}$ if and only if\n$$\nA^{T}v=0.\n$$\nCompute\n$$\nA^{T}=\\begin{pmatrix}1 & 0 & -2 \\\\ 3 & 1 & -1\\end{pmatrix},\n$$\nso the system $A^{T}v=0$ is\n$$\n\\begin{cases}\nx_{1}-2x_{3}=0,\\\\\n3x_{1}+x_{2}-x_{3}=0.\n\\end{cases}\n$$\nFrom the first equation, $x_{1}=2x_{3}$. Substitute into the second equation:\n$$\n3(2x_{3})+x_{2}-x_{3}=0 \\;\\Rightarrow\\; 6x_{3}+x_{2}-x_{3}=0 \\;\\Rightarrow\\; x_{2}=-5x_{3}.\n$$\nLet $x_{3}=t$ be a free parameter. Then\n$$\nv=\\begin{pmatrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{pmatrix}=\\begin{pmatrix}2t\\\\ -5t\\\\ t\\end{pmatrix}=t\\begin{pmatrix}2\\\\ -5\\\\ 1\\end{pmatrix}.\n$$\nThus $W^{\\perp}$ is one-dimensional and a basis is given by the single nonzero vector $\\begin{pmatrix}2\\\\ -5\\\\ 1\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}2\\\\ -5\\\\ 1\\end{pmatrix}}$$", "id": "1381128"}, {"introduction": "Orthogonal projection is one of the most important applications of orthogonal sets, allowing us to find the \"best approximation\" of a vector within a subspace. This problem offers a concrete, geometric challenge: projecting a vector onto a plane in $\\mathbb{R}^3$. The solution demonstrates an elegant and efficient strategy that involves projecting onto the much simpler one-dimensional orthogonal complement and using vector decomposition [@problem_id:1381123].", "problem": "In the Euclidean vector space $\\mathbb{R}^3$, equipped with the standard inner product (dot product), consider a subspace $W$ consisting of all vectors $[x, y, z]$ such that their components satisfy the linear equation $x - y + 2z = 0$. Determine the orthogonal projection of the vector $\\mathbf{v} = [1, 1, 5]$ onto this subspace $W$.", "solution": "We are given the subspace $W \\subset \\mathbb{R}^{3}$ defined by the linear equation $x - y + 2z = 0$. This is the plane orthogonal to the normal vector $\\mathbf{n} = (1,-1,2)$. In a Euclidean space, any vector $\\mathbf{v}$ decomposes uniquely as $\\mathbf{v} = \\mathbf{v}_{W} + \\mathbf{v}_{\\perp}$ with $\\mathbf{v}_{W} \\in W$ and $\\mathbf{v}_{\\perp} \\in \\operatorname{span}\\{\\mathbf{n}\\}$. Hence the orthogonal projection onto $W$ is\n$$\n\\operatorname{proj}_{W}(\\mathbf{v}) = \\mathbf{v} - \\operatorname{proj}_{\\mathbf{n}}(\\mathbf{v}) = \\mathbf{v} - \\frac{\\mathbf{v} \\cdot \\mathbf{n}}{\\mathbf{n} \\cdot \\mathbf{n}}\\,\\mathbf{n}.\n$$\nFor $\\mathbf{v} = (1,1,5)$ and $\\mathbf{n} = (1,-1,2)$, compute\n$$\n\\mathbf{v} \\cdot \\mathbf{n} = 1\\cdot 1 + 1\\cdot(-1) + 5\\cdot 2 = 10,\n\\qquad\n\\mathbf{n} \\cdot \\mathbf{n} = 1^{2} + (-1)^{2} + 2^{2} = 6.\n$$\nThus\n$$\n\\operatorname{proj}_{\\mathbf{n}}(\\mathbf{v}) = \\frac{10}{6}\\,(1,-1,2) = \\left(\\frac{5}{3}, -\\frac{5}{3}, \\frac{10}{3}\\right),\n$$\nand therefore\n$$\n\\operatorname{proj}_{W}(\\mathbf{v}) = (1,1,5) - \\left(\\frac{5}{3}, -\\frac{5}{3}, \\frac{10}{3}\\right) = \\left(-\\frac{2}{3}, \\frac{8}{3}, \\frac{5}{3}\\right).\n$$\nVerification that this lies in $W$: $x - y + 2z = -\\frac{2}{3} - \\frac{8}{3} + 2\\cdot \\frac{5}{3} = 0$, as required.", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{2}{3} \\\\ \\frac{8}{3} \\\\ \\frac{5}{3}\\end{pmatrix}}$$", "id": "1381123"}]}