{"hands_on_practices": [{"introduction": "Real-world data is rarely perfect. When we try to fit experimental measurements to a theoretical model, such as determining physical constants from a series of observations, we often end up with an overdetermined system of equations that has no exact solution. This is where the method of least squares comes in, providing a robust way to find the best possible compromise. This first practice [@problem_id:1378919] guides you through a classic data-fitting scenario, using the normal equations to find the model parameters that provide the 'best fit' by minimizing the sum of squared errors.", "problem": "An experimental physicist is investigating the behavior of a subatomic particle under the influence of a symmetric potential field. A theoretical model suggests that the particle's energy, $E$, should be related to a tunable experimental parameter, $q$, by the equation $E(q) = c_1 + c_2 q^2$, where $c_1$ and $c_2$ are constants related to the particle's intrinsic properties and its interaction with the field. To determine these constants, the physicist performs an experiment and records four data pairs of $(q, E)$:\n$(-2, 5)$, $(-1, 2)$, $(1, 1)$, and $(2, 4)$.\n\nDue to unavoidable experimental noise, the data points do not perfectly lie on a single curve of the form $E(q) = c_1 + c_2 q^2$. Your task is to find the parameter values for the model that best represent the collected data. The \"best\" model is defined as the one that minimizes the sum of the squares of the vertical-axis differences between the measured energy values and the energy values predicted by the model.\n\nDetermine the ordered pair of coefficients $(c_1, c_2)$ that defines this best-fit model.", "solution": "The problem asks us to find the coefficients $c_1$ and $c_2$ for the model $E(q) = c_1 + c_2 q^2$ that best fit the given data points in a least-squares sense. This is equivalent to finding the least-squares solution to an overdetermined system of linear equations.\n\nFirst, we set up the system of linear equations by substituting each data point $(q_i, E_i)$ into the model equation. For a vector of unknown coefficients $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$, we want to solve the system $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{b}$ is the vector of observed energy values.\n\nFor the point $(-2, 5)$: $c_1 + c_2(-2)^2 = c_1 + 4c_2 = 5$\nFor the point $(-1, 2)$: $c_1 + c_2(-1)^2 = c_1 + c_2 = 2$\nFor the point $(1, 1)$: $c_1 + c_2(1)^2 = c_1 + c_2 = 1$\nFor the point $(2, 4)$: $c_1 + c_2(2)^2 = c_1 + 4c_2 = 4$\n\nThis system of equations can be written in matrix form $A\\mathbf{x} = \\mathbf{b}$ as:\n$$\n\\begin{pmatrix}\n1 & 4 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nc_1 \\\\\nc_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n5 \\\\\n2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n$$\nHere, the matrix $A$ is the design matrix, $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$ is the vector of parameters, and $\\mathbf{b}$ is the vector of observations.\n$$\nA = \\begin{pmatrix}\n1 & 4 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 4\n\\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\\\ 4 \\end{pmatrix}\n$$\nThis system is inconsistent, as stated in the problem. The least-squares solution $\\hat{\\mathbf{x}}$ which minimizes the sum of the squares of the errors, $\\|A\\hat{\\mathbf{x}} - \\mathbf{b}\\|^2$, is found by solving the normal equations:\n$$ A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b} $$\nFirst, we compute the matrix $A^T A$:\n$$\nA^T A = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n4 & 1 & 1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 4 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 4 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 4 \\\\\n4 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 4 \\cdot 1 & 4 \\cdot 4 + 1 \\cdot 1 + 1 \\cdot 1 + 4 \\cdot 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 & 10 \\\\\n10 & 34\n\\end{pmatrix}\n$$\nNext, we compute the vector $A^T \\mathbf{b}$:\n$$\nA^T \\mathbf{b} = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n4 & 1 & 1 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\n5 \\\\\n2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 5 + 1 \\cdot 2 + 1 \\cdot 1 + 1 \\cdot 4 \\\\\n4 \\cdot 5 + 1 \\cdot 2 + 1 \\cdot 1 + 4 \\cdot 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\\n39\n\\end{pmatrix}\n$$\nNow we solve the system $A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}$:\n$$\n\\begin{pmatrix}\n4 & 10 \\\\\n10 & 34\n\\end{pmatrix}\n\\begin{pmatrix}\nc_1 \\\\\nc_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n12 \\\\\n39\n\\end{pmatrix}\n$$\nThis corresponds to the system of two linear equations:\n1) $4c_1 + 10c_2 = 12$\n2) $10c_1 + 34c_2 = 39$\n\nWe can simplify the first equation by dividing by 2:\n$2c_1 + 5c_2 = 6$\nFrom this, we can express $c_1$ in terms of $c_2$:\n$2c_1 = 6 - 5c_2 \\implies c_1 = 3 - \\frac{5}{2}c_2$\n\nSubstitute this expression for $c_1$ into the second equation:\n$10(3 - \\frac{5}{2}c_2) + 34c_2 = 39$\n$30 - 25c_2 + 34c_2 = 39$\n$9c_2 = 9$\n$c_2 = 1$\n\nNow, substitute the value of $c_2$ back into the expression for $c_1$:\n$c_1 = 3 - \\frac{5}{2}(1) = 3 - \\frac{5}{2} = \\frac{6}{2} - \\frac{5}{2} = \\frac{1}{2}$\n\nAlternatively, we can use Gaussian elimination on the augmented matrix:\n$$\n\\left[ \\begin{array}{cc|c} 4 & 10 & 12 \\\\ 10 & 34 & 39 \\end{array} \\right]\n$$\nDivide the first row by 2:\n$$\n\\left[ \\begin{array}{cc|c} 2 & 5 & 6 \\\\ 10 & 34 & 39 \\end{array} \\right]\n$$\nReplace the second row with $R_2 - 5R_1$:\n$$\n\\left[ \\begin{array}{cc|c} 2 & 5 & 6 \\\\ 10 - 5(2) & 34 - 5(5) & 39 - 5(6) \\end{array} \\right] = \\left[ \\begin{array}{cc|c} 2 & 5 & 6 \\\\ 0 & 9 & 9 \\end{array} \\right]\n$$\nFrom the second row, we have $9c_2 = 9$, which gives $c_2 = 1$.\nSubstituting $c_2 = 1$ into the first row equation, $2c_1 + 5c_2 = 6$:\n$2c_1 + 5(1) = 6 \\implies 2c_1 = 1 \\implies c_1 = \\frac{1}{2}$.\n\nThe coefficients for the best-fit model are $c_1 = 1/2$ and $c_2 = 1$. The ordered pair is $(1/2, 1)$.", "answer": "$$\\boxed{(\\frac{1}{2}, 1)}$$", "id": "1378919"}, {"introduction": "After learning how to compute a least-squares solution, we now explore a deeper property of these solutions and what they represent. The normal equations, $A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}$, can sometimes yield a unique solution vector $\\hat{\\mathbf{x}}$ (when $A^T A$ is invertible), but what happens if they don't? This exercise [@problem_id:14459] investigates the scenario of multiple distinct least-squares solutions, revealing a fundamental insight about the uniqueness of the resulting approximation vector $A\\hat{\\mathbf{x}}$ and strengthening our understanding of the solution as a geometric projection.", "problem": "Consider a linear system of equations represented by $A\\mathbf{x} = \\mathbf{b}$, where $A$ is an $m \\times n$ real matrix, $\\mathbf{x}$ is a vector in $\\mathbb{R}^n$, and $\\mathbf{b}$ is a vector in $\\mathbb{R}^m$. In general, this system may not have an exact solution. The method of least squares seeks a vector $\\hat{\\mathbf{x}}$ that minimizes the squared Euclidean norm of the residual, $\\|A\\mathbf{x} - \\mathbf{b}\\|^2$.\n\nA vector $\\hat{\\mathbf{x}}$ is a least-squares solution if and only if it satisfies the **normal equations**:\n$$\nA^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}\n$$\nThe set of least-squares solutions can contain a single vector or infinitely many vectors.\n\nSuppose you are given that a particular system $A\\mathbf{x} = \\mathbf{b}$ has at least two distinct least-squares solutions. Let $\\hat{\\mathbf{x}}_1$ and $\\hat{\\mathbf{x}}_2$ be two such distinct solutions, i.e., $\\hat{\\mathbf{x}}_1 \\neq \\hat{\\mathbf{x}}_2$.\n\nYour task is to derive the value of the norm of the vector difference $A\\hat{\\mathbf{x}}_1 - A\\hat{\\mathbf{x}}_2$. That is, calculate $\\|A(\\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2)\\|$.", "solution": "Let $\\hat{\\mathbf{x}}_1$ and $\\hat{\\mathbf{x}}_2$ be two distinct least-squares solutions to the system $A\\mathbf{x} = \\mathbf{b}$.\n\nBy definition, any least-squares solution must satisfy the normal equations:\n$$\nA^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}\n$$\nSince both $\\hat{\\mathbf{x}}_1$ and $\\hat{\\mathbf{x}}_2$ are least-squares solutions, they both must satisfy this equation.\n\nFor $\\hat{\\mathbf{x}}_1$, we have:\n$$\nA^T A \\hat{\\mathbf{x}}_1 = A^T \\mathbf{b} \\quad (*)\n$$\nFor $\\hat{\\mathbf{x}}_2$, we have:\n$$\nA^T A \\hat{\\mathbf{x}}_2 = A^T \\mathbf{b} \\quad (**)\n$$\nNow, we can subtract equation $(**)$ from equation $(*)$:\n$$\nA^T A \\hat{\\mathbf{x}}_1 - A^T A \\hat{\\mathbf{x}}_2 = A^T \\mathbf{b} - A^T \\mathbf{b}\n$$\nUsing the distributive property of matrix multiplication, we can factor out $A^T A$ on the left-hand side:\n$$\nA^T A (\\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2) = \\mathbf{0}\n$$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^n$.\n\nWe are asked to find the value of $\\|A(\\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2)\\|$. Let's define a new vector $\\mathbf{v} = \\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2$. Since $\\hat{\\mathbf{x}}_1$ and $\\hat{\\mathbf{x}}_2$ are distinct, $\\mathbf{v} \\neq \\mathbf{0}$. The equation above becomes:\n$$\nA^T A \\mathbf{v} = \\mathbf{0}\n$$\nThe norm we wish to calculate is $\\|A\\mathbf{v}\\|$. Let's consider the square of this norm, which is given by the dot product of the vector with itself:\n$$\n\\|A\\mathbf{v}\\|^2 = (A\\mathbf{v})^T (A\\mathbf{v})\n$$\nUsing the property of transposes $(XY)^T = Y^T X^T$, we get:\n$$\n\\|A\\mathbf{v}\\|^2 = (\\mathbf{v}^T A^T) (A\\mathbf{v})\n$$\nBy the associativity of matrix multiplication, we can regroup the terms:\n$$\n\\|A\\mathbf{v}\\|^2 = \\mathbf{v}^T (A^T A \\mathbf{v})\n$$\nFrom our result derived from the normal equations, we know that $A^T A \\mathbf{v} = \\mathbf{0}$. Substituting this into the expression for the squared norm:\n$$\n\\|A\\mathbf{v}\\|^2 = \\mathbf{v}^T (\\mathbf{0})\n$$\nThe dot product of any vector with the zero vector is zero:\n$$\n\\|A\\mathbf{v}\\|^2 = 0\n$$\nThe norm of a vector is zero if and only if the vector itself is the zero vector. Therefore, taking the square root of both sides gives:\n$$\n\\|A\\mathbf{v}\\| = 0\n$$\nSubstituting $\\mathbf{v} = \\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2$ back into the expression, we find the final result:\n$$\n\\|A(\\hat{\\mathbf{x}}_1 - \\hat{\\mathbf{x}}_2)\\| = 0\n$$", "answer": "$$\\boxed{0}$$", "id": "14459"}, {"introduction": "The structure of the matrix $A$ in a least-squares problem can greatly influence the complexity of finding the solution. This final practice [@problem_id:1378910] examines a special, highly elegant case where the columns of $A$ form an orthonormal set, a scenario commonly encountered in fields like signal processing after applying transformations like the Fourier or wavelet transform. By working through this derivation, you will see how the normal equations simplify dramatically, providing a direct and powerful connection between the algebraic solution and the geometric concept of an orthogonal projection.", "problem": "Consider a standard least-squares problem, where we aim to find an approximate solution to an inconsistent linear system $A\\mathbf{x} = \\mathbf{b}$. Let $A$ be a real $m \\times n$ matrix with $m > n$, and let $\\mathbf{b}$ be a vector in $\\mathbb{R}^m$. The columns of the matrix $A$, which we denote as $\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_n$, are given to form an orthonormal set. The least-squares solution, denoted $\\hat{\\mathbf{x}}$, is the vector in $\\mathbb{R}^n$ that minimizes the Euclidean norm $\\|A\\mathbf{x} - \\mathbf{b}\\|$. The vector $\\hat{\\mathbf{b}} = A\\hat{\\mathbf{x}}$ represents the orthogonal projection of $\\mathbf{b}$ onto the column space of $A$.\n\nYour task is to derive a simplified closed-form expression for the projection vector $\\hat{\\mathbf{b}}$ in terms of the matrix $A$ and the vector $\\mathbf{b}$, under the specific condition that the columns of $A$ are orthonormal.", "solution": "We consider the least-squares problem for an inconsistent system $A\\mathbf{x}=\\mathbf{b}$ with $A \\in \\mathbb{R}^{m \\times n}$, $m>n$, and orthonormal columns $\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{n}$. Orthonormality implies\n$$\nA^{T}A=I_{n}.\n$$\nThe least-squares solution $\\hat{\\mathbf{x}}$ satisfies the normal equations\n$$\nA^{T}A\\,\\hat{\\mathbf{x}}=A^{T}\\mathbf{b}.\n$$\nUsing $A^{T}A=I_{n}$, we obtain\n$$\n\\hat{\\mathbf{x}}=(A^{T}A)^{-1}A^{T}\\mathbf{b}=I_{n}^{-1}A^{T}\\mathbf{b}=A^{T}\\mathbf{b}.\n$$\nThe projection of $\\mathbf{b}$ onto the column space of $A$ is\n$$\n\\hat{\\mathbf{b}}=A\\hat{\\mathbf{x}}=A\\left(A^{T}\\mathbf{b}\\right)=AA^{T}\\mathbf{b}.\n$$\nEquivalently, since the columns of $A$ form an orthonormal set, the projector onto $\\operatorname{col}(A)$ is $P=AA^{T}$, yielding the same expression.", "answer": "$$\\boxed{AA^{T}\\mathbf{b}}$$", "id": "1378910"}]}