{"hands_on_practices": [{"introduction": "The least-squares method provides the best possible approximation when an exact solution to a system of equations does not exist. But what does it mean if the approximation error turns out to be zero? This exercise explores the fundamental geometric implication of a perfect fit, connecting the algebraic condition of zero error to the relationship between the data vector and the column space of the system's matrix.", "problem": "An engineer is analyzing temperature data from a new experimental device. At four distinct time points $t_1, t_2, t_3, t_4$, they record four corresponding temperature measurements $y_1, y_2, y_3, y_4$. To model the device's thermal behavior, they propose a quadratic relationship of the form $y(t) = c_0 + c_1 t + c_2 t^2$.\n\nThis modeling attempt leads to an overdetermined system of linear equations, which can be written in the matrix form $A\\mathbf{c} = \\mathbf{y}$, where:\n- $\\mathbf{c} = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\end{pmatrix}$ is the vector of unknown model coefficients.\n- $\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{pmatrix}$ is the vector of observed temperature measurements.\n- $A$ is a $4 \\times 3$ matrix derived from the time points and the structure of the quadratic model.\n\nThe engineer uses a standard computational library to find the least-squares solution, $\\hat{\\mathbf{c}}$, which minimizes the squared Euclidean norm of the error vector, $\\|A\\mathbf{c} - \\mathbf{y}\\|^2$. After the computation, the engineer is surprised to find that the least-squares error, defined as $\\|A\\hat{\\mathbf{c}} - \\mathbf{y}\\|$, is exactly zero.\n\nBased on this result, what definitive conclusion can be drawn about the data vector $\\mathbf{y}$ in relation to the matrix $A$?\n\nA. The vector $\\mathbf{y}$ must be the zero vector.\n\nB. The vector $\\mathbf{y}$ is orthogonal to the column space of $A$.\n\nC. The vector $\\mathbf{y}$ lies in the column space of $A$.\n\nD. The vector $\\mathbf{y}$ is a vector in $\\mathbb{R}^3$.\n\nE. The vector $\\mathbf{y}$ lies in the null space of the transpose matrix, $A^T$.", "solution": "We model the data by minimizing the least-squares objective\n$$\n\\min_{\\mathbf{c}\\in\\mathbb{R}^{3}} \\|A\\mathbf{c}-\\mathbf{y}\\|^{2}.\n$$\nLet the residual be $\\mathbf{r}(\\mathbf{c})=\\mathbf{y}-A\\mathbf{c}$. The necessary condition for a minimizer $\\hat{\\mathbf{c}}$ is obtained by setting the gradient to zero:\n$$\n\\nabla_{\\mathbf{c}} \\|A\\mathbf{c}-\\mathbf{y}\\|^{2} = 2 A^{T}(A\\mathbf{c}-\\mathbf{y})=0,\n$$\nwhich yields the normal equations\n$$\nA^{T}A\\,\\hat{\\mathbf{c}}=A^{T}\\mathbf{y}, \\quad \\text{equivalently} \\quad A^{T}\\bigl(\\mathbf{y}-A\\hat{\\mathbf{c}}\\bigr)=\\mathbf{0}.\n$$\nThus, at the least-squares solution, the residual $\\hat{\\mathbf{r}}=\\mathbf{y}-A\\hat{\\mathbf{c}}$ is orthogonal to the column space of $A$.\n\nThe engineer observes that the least-squares error norm is exactly zero:\n$$\n\\|A\\hat{\\mathbf{c}}-\\mathbf{y}\\|=0.\n$$\nA norm equals zero if and only if the vector is the zero vector, hence\n$$\nA\\hat{\\mathbf{c}}-\\mathbf{y}=\\mathbf{0} \\quad \\Longrightarrow \\quad \\mathbf{y}=A\\hat{\\mathbf{c}}.\n$$\nBy definition, any vector of the form $A\\mathbf{c}$ lies in the column space of $A$. Therefore,\n$$\n\\mathbf{y}\\in \\operatorname{Col}(A).\n$$\nThis is the definitive conclusion implied by a zero least-squares residual.", "answer": "$$\\boxed{C}$$", "id": "1371626"}, {"introduction": "A primary application of least-squares is fitting a model, like a polynomial, to a set of experimental data points. This practice problem walks you through the essential first step of this procedure: translating a set of observations into the matrix form $A\\mathbf{x} = \\mathbf{b}$ and constructing the corresponding normal equations, $A^T A \\hat{\\mathbf{x}} = A^T \\mathbf{b}$. Mastering this setup is the key to solving any linear data-fitting problem.", "problem": "A signal processing engineer is analyzing a noisy signal. They hypothesize that a segment of the signal, $y(t)$, can be approximated by a quadratic polynomial of the form $p(t) = c_0 + c_1 t + c_2 t^2$, where $t$ is time in seconds. To determine the unknown coefficients $c_0, c_1, c_2$, the engineer has collected four data samples $(t_i, y_i)$:\n$$(0, 1.0), (1, 0.5), (2, 2.0), (3, 4.5)$$\nPlugging these data points into the model creates an over-determined system of linear equations $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\end{pmatrix}$. The matrix $A$ is constructed using the basis functions of the polynomial evaluated at the sample times, and the vector $\\mathbf{b}$ contains the observed signal values.\n\nThe principle of least squares states that the best approximation is found when the error vector, $\\mathbf{e} = \\mathbf{b} - A\\mathbf{x}$, is orthogonal to the column space of $A$. This orthogonality condition translates into a solvable system of linear equations of the form $M\\mathbf{x} = \\mathbf{v}$.\n\nYour task is to determine the matrix $M$ and the vector $\\mathbf{v}$ for this system. Which of the following options represents the correct pair $(M, \\mathbf{v})$?\n\nA. $M = \\begin{pmatrix} 4 & 6 & 14 \\\\ 6 & 14 & 36 \\\\ 14 & 36 & 98 \\end{pmatrix}, \\mathbf{v} = \\begin{pmatrix} 8 \\\\ 18 \\\\ 49 \\end{pmatrix}$\n\nB. $M = \\begin{pmatrix} 14 & 36 & 98 \\\\ 36 & 98 & 258 \\\\ 98 & 258 & 794 \\end{pmatrix}, \\mathbf{v} = \\begin{pmatrix} 18 \\\\ 49 \\\\ 138.5 \\end{pmatrix}$\n\nC. $M = \\begin{pmatrix} 4 & 6 & 14 \\\\ 6 & 14 & 36 \\\\ 14 & 36 & 98 \\end{pmatrix}, \\mathbf{v} = \\begin{pmatrix} 8.0 \\\\ 18.0 \\\\ 45.0 \\end{pmatrix}$\n\nD. $M = \\begin{pmatrix} 4 & 6 & 14 \\\\ 6 & 14 & 36 \\\\ 14 & 36 & 98 \\end{pmatrix}, \\mathbf{v} = \\begin{pmatrix} 6 \\\\ 14 \\\\ 36 \\end{pmatrix}$\n\nE. $M = \\begin{pmatrix} 4 & 6 & 14 \\\\ 6 & 14 & 36 \\\\ 14 & 36 & 108 \\end{pmatrix}, \\mathbf{v} = \\begin{pmatrix} 8 \\\\ 18 \\\\ 49 \\end{pmatrix}$", "solution": "We model the data with $p(t)=c_{0}+c_{1}t+c_{2}t^{2}$. Using the sample times $t\\in\\{0,1,2,3\\}$, the design matrix $A$ has rows $[1,\\ t,\\ t^{2}]$ evaluated at each $t$, and $\\mathbf{b}$ collects the corresponding $y$-values:\n$$\nA=\\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 1\\\\\n1 & 2 & 4\\\\\n1 & 3 & 9\n\\end{pmatrix},\\qquad\n\\mathbf{b}=\\begin{pmatrix}\n1.0\\\\\n0.5\\\\\n2.0\\\\\n4.5\n\\end{pmatrix}.\n$$\nThe least-squares solution satisfies the normal equations\n$$\nA^{\\top}A\\,\\mathbf{x}=A^{\\top}\\mathbf{b},\n$$\nso $M=A^{\\top}A$ and $\\mathbf{v}=A^{\\top}\\mathbf{b}$.\n\nCompute the sums over the sample times $t_{i}\\in\\{0,1,2,3\\}$:\n$$\nS_{0}=\\sum 1=4,\\quad S_{1}=\\sum t_{i}=0+1+2+3=6,\\quad S_{2}=\\sum t_{i}^{2}=0+1+4+9=14,\n$$\n$$\nS_{3}=\\sum t_{i}^{3}=0+1+8+27=36,\\quad S_{4}=\\sum t_{i}^{4}=0+1+16+81=98.\n$$\nThus\n$$\nM=A^{\\top}A=\\begin{pmatrix}\nS_{0} & S_{1} & S_{2}\\\\\nS_{1} & S_{2} & S_{3}\\\\\nS_{2} & S_{3} & S_{4}\n\\end{pmatrix}\n=\\begin{pmatrix}\n4 & 6 & 14\\\\\n6 & 14 & 36\\\\\n14 & 36 & 98\n\\end{pmatrix}.\n$$\n\nNow compute $\\mathbf{v}=A^{\\top}\\mathbf{b}$ by components:\n$$\nv_{0}=\\sum y_{i}=1.0+0.5+2.0+4.5=8.0,\n$$\n$$\nv_{1}=\\sum t_{i}y_{i}=0\\cdot 1.0+1\\cdot 0.5+2\\cdot 2.0+3\\cdot 4.5=0.5+4.0+13.5=18.0,\n$$\n$$\nv_{2}=\\sum t_{i}^{2}y_{i}=0^{2}\\cdot 1.0+1^{2}\\cdot 0.5+2^{2}\\cdot 2.0+3^{2}\\cdot 4.5=0+0.5+8.0+40.5=49.0.\n$$\nTherefore,\n$$\n\\mathbf{v}=\\begin{pmatrix}8\\\\18\\\\49\\end{pmatrix}.\n$$\nComparing with the options, this corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1371660"}, {"introduction": "While the normal equations provide a general path to the least-squares solution, the calculation can be greatly simplified if the model's basis vectors have special properties. This exercise demonstrates the remarkable computational advantage gained when the columns of the matrix $A$ are orthogonal. You will see how this property beautifully decouples the system, transforming the problem of finding a projection into a far simpler calculation.", "problem": "A laboratory instrument records a signal at four consecutive, equally spaced time intervals. The resulting measurements are compiled into a vector $b \\in \\mathbb{R}^4$. According to theory, a 'pure' signal, devoid of noise, must be a linear combination of three specific basis signals. These basis signals are given as the columns of the matrix $A$. The specific matrices are:\n$$\nA = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & -1 & -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}\n$$\nThe observed vector $b$ is presumed to be a pure signal corrupted by noise, and therefore does not lie in the subspace spanned by the columns of $A$. To recover the most likely pure signal, we must find the vector in the column space of $A$ that is closest to $b$. This is known as the orthogonal projection of $b$ onto the column space of $A$.\n\nFind the coordinate vector $\\hat{x} \\in \\mathbb{R}^3$ of this projection, relative to the basis given by the columns of $A$. Express your answer as an exact column vector.", "solution": "We seek the orthogonal projection of $b$ onto the column space of $A$, which is the least-squares solution $\\hat{x}$ of $A \\hat{x} \\approx b$. This satisfies the normal equations\n$$\nA^{T}A\\,\\hat{x} \\;=\\; A^{T}b.\n$$\nLet the columns of $A$ be $c_{1},c_{2},c_{3}$, where\n$$\nc_{1}=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix},\\quad\nc_{2}=\\begin{pmatrix}1\\\\-1\\\\1\\\\-1\\end{pmatrix},\\quad\nc_{3}=\\begin{pmatrix}1\\\\1\\\\-1\\\\-1\\end{pmatrix}.\n$$\nCompute the Gram matrix entries $c_{i}\\cdot c_{j}$:\n$$\nc_{1}\\cdot c_{1}=4,\\quad c_{2}\\cdot c_{2}=4,\\quad c_{3}\\cdot c_{3}=4,\n$$\n$$\nc_{1}\\cdot c_{2}=0,\\quad c_{1}\\cdot c_{3}=0,\\quad c_{2}\\cdot c_{3}=0.\n$$\nHence,\n$$\nA^{T}A \\;=\\; \\begin{pmatrix}4&0&0\\\\0&4&0\\\\0&0&4\\end{pmatrix} \\;=\\; 4I_{3}.\n$$\nNext compute $A^{T}b$, whose entries are $c_{i}\\cdot b$ with $b=\\begin{pmatrix}1\\\\2\\\\3\\\\4\\end{pmatrix}$:\n$$\nc_{1}\\cdot b = 1+2+3+4=10,\\quad\nc_{2}\\cdot b = 1-2+3-4=-2,\\quad\nc_{3}\\cdot b = 1+2-3-4=-4,\n$$\nso\n$$\nA^{T}b \\;=\\; \\begin{pmatrix}10\\\\-2\\\\-4\\end{pmatrix}.\n$$\nSolving the normal equations gives\n$$\n4I_{3}\\,\\hat{x} \\;=\\; \\begin{pmatrix}10\\\\-2\\\\-4\\end{pmatrix}\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{x} \\;=\\; \\frac{1}{4}\\begin{pmatrix}10\\\\-2\\\\-4\\end{pmatrix}\n\\;=\\; \\begin{pmatrix}\\frac{5}{2}\\\\-\\frac{1}{2}\\\\-1\\end{pmatrix}.\n$$\nTherefore, the coordinate vector of the orthogonal projection of $b$ onto the column space of $A$, relative to the given basis, is $\\hat{x}=\\begin{pmatrix}\\frac{5}{2}\\\\ -\\frac{1}{2}\\\\ -1\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{5}{2}\\\\ -\\frac{1}{2}\\\\ -1\\end{pmatrix}}$$", "id": "1371641"}]}