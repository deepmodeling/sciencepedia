{"hands_on_practices": [{"introduction": "An essential application of orthonormalization arises in the study of eigenvalues and eigenvectors. While eigenvectors of a symmetric matrix corresponding to distinct eigenvalues are guaranteed to be orthogonal, the same is not true for eigenvectors within a single eigenspace for a repeated eigenvalue. This practice [@problem_id:1381405] guides you through the crucial skill of applying the Gram-Schmidt process to construct an orthonormal basis for such an eigenspace, a fundamental task in fields ranging from quantum mechanics to data analysis.", "problem": "Consider the real symmetric matrix $A$ given by:\n$$\nA = \\begin{pmatrix} 3 & -2 & -2 \\\\ -2 & 3 & -2 \\\\ -2 & -2 & 3 \\end{pmatrix}\n$$\nIt is known that this matrix has an eigenvalue with an algebraic multiplicity of two. Your task is to find an orthonormal basis for the eigenspace corresponding to this repeated eigenvalue.\n\nLet the basis vectors be $\\vec{v}_1$ and $\\vec{v}_2$. To ensure a unique answer, you must construct your basis according to the following rules:\n1. The last component of the vector $\\vec{v}_1$ must be zero, and its first non-zero component must be positive.\n2. The vector $\\vec{v}_2$ must be orthogonal to $\\vec{v}_1$, and its first non-zero component must be positive.\n\nExpress your answer as a single row matrix containing the components of $\\vec{v}_1$ followed by the components of $\\vec{v}_2$. The components within each vector should be listed in order (i.e., first component, second component, third component).", "solution": "We first identify the repeated eigenvalue and its eigenspace. Observe that\n$$\nA = \\begin{pmatrix} 3 & -2 & -2 \\\\ -2 & 3 & -2 \\\\ -2 & -2 & 3 \\end{pmatrix} = 5I - 2J,\n$$\nwhere $I$ is the $3\\times 3$ identity matrix and $J$ is the $3\\times 3$ all-ones matrix. The matrix $J$ has eigenvalue $3$ with eigenvector $(1,1,1)^{\\top}$ and eigenvalue $0$ with multiplicity $2$ corresponding to the subspace $\\{ \\mathbf{x} \\in \\mathbb{R}^{3} : x_{1} + x_{2} + x_{3} = 0 \\}$. Therefore, for any vector $\\mathbf{v}$ with $v_{1} + v_{2} + v_{3} = 0$, we have $J\\mathbf{v} = 0$ and hence\n$$\nA\\mathbf{v} = (5I - 2J)\\mathbf{v} = 5\\mathbf{v},\n$$\nso $5$ is an eigenvalue of $A$ with algebraic and geometric multiplicity $2$. The remaining eigenvalue is obtained by applying $A$ to $(1,1,1)^{\\top}$:\n$$\nA(1,1,1)^{\\top} = (5I - 2J)(1,1,1)^{\\top} = (5 - 2\\cdot 3)(1,1,1)^{\\top} = -1\\cdot (1,1,1)^{\\top}.\n$$\nThus the repeated eigenvalue is $5$, and its eigenspace is\n$$\nE_{5} = \\{(x_{1},x_{2},x_{3})^{\\top} \\in \\mathbb{R}^{3} : x_{1} + x_{2} + x_{3} = 0\\}.\n$$\n\nWe construct an orthonormal basis for $E_{5}$ satisfying the given constraints.\n\n1) To satisfy that the last component of $\\vec{v}_{1}$ is zero and $x_{1}$ is the first non-zero positive component, we impose $x_{3}=0$ and $x_{1} + x_{2} = 0$. A suitable choice is $\\vec{v}_{1}^{\\ast} = (1,-1,0)^{\\top}$. We normalize it:\n$$\n\\|\\vec{v}_{1}^{\\ast}\\| = \\sqrt{1^{2} + (-1)^{2} + 0^{2}} = \\sqrt{2}, \\quad \\vec{v}_{1} = \\frac{1}{\\sqrt{2}}(1,-1,0)^{\\top} = \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right)^{\\top}.\n$$\nThis vector lies in $E_{5}$, has last component zero, and its first non-zero component is positive.\n\n2) For $\\vec{v}_{2}$, we require $\\vec{v}_{2} \\in E_{5}$ and $\\vec{v}_{2} \\perp \\vec{v}_{1}$. Let $\\vec{v}_{2} = (a,b,c)^{\\top}$ with $a + b + c = 0$ and\n$$\n\\vec{v}_{1} \\cdot \\vec{v}_{2} = \\frac{1}{\\sqrt{2}}a + \\left(-\\frac{1}{\\sqrt{2}}\\right)b + 0 \\cdot c = \\frac{1}{\\sqrt{2}}(a - b) = 0 \\quad \\Rightarrow \\quad a = b.\n$$\nThen $c = -a - b = -2a$, so an unnormalized choice is $\\vec{v}_{2}^{\\ast} = (1,1,-2)^{\\top}$. We normalize it:\n$$\n\\|\\vec{v}_{2}^{\\ast}\\| = \\sqrt{1^{2} + 1^{2} + (-2)^{2}} = \\sqrt{6}, \\quad \\vec{v}_{2} = \\frac{1}{\\sqrt{6}}(1,1,-2)^{\\top} = \\left(\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, -\\frac{2}{\\sqrt{6}}\\right)^{\\top}.\n$$\nThis $\\vec{v}_{2}$ lies in $E_{5}$, is orthogonal to $\\vec{v}_{1}$, has unit norm, and its first non-zero component is positive.\n\nTherefore, an orthonormal basis for the eigenspace corresponding to the repeated eigenvalue, obeying the stated rules and ordering $\\vec{v}_{1}$ then $\\vec{v}_{2}$, is given by the row matrix containing their components in order.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}} & -\\frac{2}{\\sqrt{6}}\\end{pmatrix}}$$", "id": "1381405"}, {"introduction": "The power of orthogonality extends far beyond the familiar Euclidean dot product. Many applications in physics and engineering require generalized notions of distance and angle, defined by custom inner products. This exercise [@problem_id:1381374] challenges you to adapt your understanding by performing the Gram-Schmidt process within a vector space equipped with a non-standard inner product, thereby reinforcing the core algebraic structure of the method and its versatility.", "problem": "In certain physical systems, such as the analysis of vibrations in a discrete lattice, the standard Euclidean inner product is insufficient. Instead, a weighted inner product is used to account for the interactions between components. Consider the vector space $\\mathbb{R}^3$ equipped with a non-standard inner product defined by $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^T A \\mathbf{v}$, where $\\mathbf{u}$ and $\\mathbf{v}$ are column vectors in $\\mathbb{R}^3$ and $A$ is the positive definite matrix given by:\n$$A = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}$$\nYou are given a basis $B = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$ for $\\mathbb{R}^3$, where\n$$ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nApply the Gram-Schmidt orthogonalization process to the basis $B$ to produce an orthonormal basis $W = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}$ with respect to the given inner product.\n\nDetermine the third vector, $\\mathbf{w}_3$, of this new orthonormal basis.", "solution": "We use the inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^{T}A\\mathbf{v}$ with \n$$\nA=\\begin{pmatrix}2&-1&0\\\\-1&2&-1\\\\0&-1&2\\end{pmatrix},\n$$\nand apply Gramâ€“Schmidt in the form\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1},\\quad \\mathbf{u}_{k}=\\mathbf{v}_{k}-\\sum_{j=1}^{k-1}\\frac{\\langle \\mathbf{v}_{k},\\mathbf{u}_{j}\\rangle}{\\langle \\mathbf{u}_{j},\\mathbf{u}_{j}\\rangle}\\mathbf{u}_{j},\\quad \\mathbf{w}_{k}=\\frac{\\mathbf{u}_{k}}{\\|\\mathbf{u}_{k}\\|},\n$$\nwhere $\\|\\mathbf{x}\\|=\\sqrt{\\langle \\mathbf{x},\\mathbf{x}\\rangle}$.\n\nFirst, compute for $\\mathbf{v}_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$:\n$$\nA\\mathbf{v}_{1}=\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix},\\quad \\langle \\mathbf{v}_{1},\\mathbf{v}_{1}\\rangle=\\mathbf{v}_{1}^{T}A\\mathbf{v}_{1}=2,\n$$\nso $\\mathbf{u}_{1}=\\mathbf{v}_{1}$ and $\\mathbf{w}_{1}=\\mathbf{v}_{1}/\\sqrt{2}$.\n\nNext, for $\\mathbf{v}_{2}=\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$:\n$$\n\\langle \\mathbf{v}_{2},\\mathbf{u}_{1}\\rangle=\\mathbf{v}_{2}^{T}A\\mathbf{v}_{1}=\\begin{pmatrix}1&1&0\\end{pmatrix}\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix}=1,\\quad \\langle \\mathbf{u}_{1},\\mathbf{u}_{1}\\rangle=2,\n$$\nhence\n$$\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{1}{2}\\mathbf{u}_{1}=\\begin{pmatrix}\\frac{1}{2}\\\\1\\\\0\\end{pmatrix}.\n$$\nCompute\n$$\nA\\mathbf{u}_{2}=\\begin{pmatrix}0\\\\\\frac{3}{2}\\\\-1\\end{pmatrix},\\quad \\langle \\mathbf{u}_{2},\\mathbf{u}_{2}\\rangle=\\mathbf{u}_{2}^{T}A\\mathbf{u}_{2}=\\frac{3}{2},\n$$\nso $\\|\\mathbf{u}_{2}\\|=\\sqrt{\\frac{3}{2}}$ and $\\mathbf{w}_{2}=\\mathbf{u}_{2}/\\sqrt{\\frac{3}{2}}$.\n\nFor $\\mathbf{v}_{3}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, compute the needed inner products:\n$$\n\\langle \\mathbf{v}_{3},\\mathbf{u}_{1}\\rangle=\\mathbf{v}_{3}^{T}A\\mathbf{v}_{1}=\\begin{pmatrix}1&1&1\\end{pmatrix}\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix}=1,\\quad \\langle \\mathbf{u}_{1},\\mathbf{u}_{1}\\rangle=2,\n$$\nso the first projection coefficient is $\\frac{1}{2}$. Also,\n$$\n\\langle \\mathbf{v}_{3},\\mathbf{u}_{2}\\rangle=\\mathbf{v}_{3}^{T}A\\mathbf{u}_{2}=\\begin{pmatrix}1&1&1\\end{pmatrix}\\begin{pmatrix}0\\\\\\frac{3}{2}\\\\-1\\end{pmatrix}=\\frac{1}{2},\\quad \\langle \\mathbf{u}_{2},\\mathbf{u}_{2}\\rangle=\\frac{3}{2},\n$$\nso the second projection coefficient is $\\frac{1/2}{3/2}=\\frac{1}{3}$. Therefore\n$$\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}-\\frac{1}{3}\\mathbf{u}_{2}\n=\\mathbf{v}_{3}-\\frac{1}{3}\\mathbf{v}_{2}-\\frac{1}{3}\\mathbf{v}_{1}\n=\\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\1\\end{pmatrix}.\n$$\nCompute its norm:\n$$\nA\\mathbf{u}_{3}=\\begin{pmatrix}0\\\\0\\\\\\frac{4}{3}\\end{pmatrix},\\quad \\langle \\mathbf{u}_{3},\\mathbf{u}_{3}\\rangle=\\mathbf{u}_{3}^{T}A\\mathbf{u}_{3}=\\frac{4}{3},\n$$\nso $\\|\\mathbf{u}_{3}\\|=\\frac{2}{\\sqrt{3}}$. Hence the normalized third vector is\n$$\n\\mathbf{w}_{3}=\\frac{\\mathbf{u}_{3}}{\\|\\mathbf{u}_{3}\\|}=\\frac{\\sqrt{3}}{2}\\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\1\\end{pmatrix}\n=\\begin{pmatrix}\\frac{\\sqrt{3}}{6}\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{\\sqrt{3}}{2}\\end{pmatrix}.\n$$\nThis $\\mathbf{w}_{3}$ is orthonormal (with respect to $\\langle \\cdot,\\cdot\\rangle$) to $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ and has unit norm.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sqrt{3}}{6}\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{\\sqrt{3}}{2}\\end{pmatrix}}$$", "id": "1381374"}, {"introduction": "Orthonormal bases are not just theoretically elegant; they provide significant computational advantages. This problem [@problem_id:1381365] illustrates a powerful application by showing how to find the distance from a vector to a subspace. Instead of a direct projection, you will use the properties of an orthonormal basis to work with the much simpler orthogonal complement, a technique that demonstrates the profound geometric and computational simplifications offered by orthonormality.", "problem": "Let the columns of the $4 \\times 4$ orthogonal matrix $Q$ be denoted by $\\mathbf{q}_1, \\mathbf{q}_2, \\mathbf{q}_3, \\mathbf{q}_4$. The matrix is given by:\n$$ Q = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1 \\end{pmatrix} $$\nConsider the subspace $W$ of $\\mathbb{R}^4$ defined as the linear span of the first two columns of $Q$, that is, $W = \\text{span}\\{\\mathbf{q}_1, \\mathbf{q}_2\\}$.\nLet the vector $\\mathbf{v}$ be defined as:\n$$ \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix} $$\nCalculate the Euclidean distance from the vector $\\mathbf{v}$ to the subspace $W$. Express your answer as an exact value in simplest radical form.", "solution": "The Euclidean distance from a vector $\\mathbf{v}$ to a subspace $W$ is defined as the norm of the difference between $\\mathbf{v}$ and its orthogonal projection onto $W$, denoted $\\text{proj}_W(\\mathbf{v})$. The distance is given by $d(\\mathbf{v}, W) = \\|\\mathbf{v} - \\text{proj}_W(\\mathbf{v})\\|$.\n\nAccording to the Orthogonal Decomposition Theorem, any vector $\\mathbf{v}$ in a vector space can be uniquely written as the sum of a vector in a subspace $W$ and a vector in its orthogonal complement $W^{\\perp}$. Specifically, $\\mathbf{v} = \\text{proj}_W(\\mathbf{v}) + \\text{proj}_{W^{\\perp}}(\\mathbf{v})$. From this, we can see that the vector component orthogonal to $W$ is $\\mathbf{v} - \\text{proj}_W(\\mathbf{v}) = \\text{proj}_{W^{\\perp}}(\\mathbf{v})$. Therefore, the distance from $\\mathbf{v}$ to $W$ is simply the norm of the projection of $\\mathbf{v}$ onto $W^{\\perp}$:\n$$ d(\\mathbf{v}, W) = \\|\\text{proj}_{W^{\\perp}}(\\mathbf{v})\\| $$\nThis approach is often computationally simpler than finding $\\text{proj}_W(\\mathbf{v})$ first.\n\nThe problem states that the columns of the matrix $Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\mathbf{q}_3, \\mathbf{q}_4]$ form an orthonormal basis for $\\mathbb{R}^4$. The subspace $W$ is given as $W = \\text{span}\\{\\mathbf{q}_1, \\mathbf{q}_2\\}$. The orthogonal complement $W^{\\perp}$ is the subspace of all vectors in $\\mathbb{R}^4$ that are orthogonal to every vector in $W$. Since $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\mathbf{q}_3, \\mathbf{q}_4\\}$ is an orthonormal basis for the entire space $\\mathbb{R}^4$, the subspace $W^{\\perp}$ is spanned by the basis vectors that are not in the span of $W$. Thus, an orthonormal basis for $W^{\\perp}$ is $\\{\\mathbf{q}_3, \\mathbf{q}_4\\}$.\n\nThe projection of $\\mathbf{v}$ onto $W^{\\perp}$ can be calculated using the formula for projection onto a space with an orthonormal basis:\n$$ \\text{proj}_{W^{\\perp}}(\\mathbf{v}) = (\\mathbf{v} \\cdot \\mathbf{q}_3)\\mathbf{q}_3 + (\\mathbf{v} \\cdot \\mathbf{q}_4)\\mathbf{q}_4 $$\nThe norm of this projection, and hence the distance, is found using the Pythagorean theorem for orthogonal vectors. Since $\\mathbf{q}_3$ and $\\mathbf{q}_4$ are orthonormal, we have:\n$$ d(\\mathbf{v}, W)^2 = \\|\\text{proj}_{W^{\\perp}}(\\mathbf{v})\\|^2 = ((\\mathbf{v} \\cdot \\mathbf{q}_3)\\mathbf{q}_3 + (\\mathbf{v} \\cdot \\mathbf{q}_4)\\mathbf{q}_4) \\cdot ((\\mathbf{v} \\cdot \\mathbf{q}_3)\\mathbf{q}_3 + (\\mathbf{v} \\cdot \\mathbf{q}_4)\\mathbf{q}_4) $$\n$$ d(\\mathbf{v}, W)^2 = (\\mathbf{v} \\cdot \\mathbf{q}_3)^2 \\|\\mathbf{q}_3\\|^2 + (\\mathbf{v} \\cdot \\mathbf{q}_4)^2 \\|\\mathbf{q}_4\\|^2 + 2(\\mathbf{v} \\cdot \\mathbf{q}_3)(\\mathbf{v} \\cdot \\mathbf{q}_4)(\\mathbf{q}_3 \\cdot \\mathbf{q}_4) $$\nSince $\\|\\mathbf{q}_3\\| = 1$, $\\|\\mathbf{q}_4\\| = 1$, and $\\mathbf{q}_3 \\cdot \\mathbf{q}_4 = 0$, this simplifies to:\n$$ d(\\mathbf{v}, W)^2 = (\\mathbf{v} \\cdot \\mathbf{q}_3)^2 + (\\mathbf{v} \\cdot \\mathbf{q}_4)^2 $$\n$$ d(\\mathbf{v}, W) = \\sqrt{(\\mathbf{v} \\cdot \\mathbf{q}_3)^2 + (\\mathbf{v} \\cdot \\mathbf{q}_4)^2} $$\nNow we compute the required dot products. The vectors are:\n$$ \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{q}_3 = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix}, \\quad \\mathbf{q}_4 = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} $$\nThe first dot product is:\n$$ \\mathbf{v} \\cdot \\mathbf{q}_3 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix} \\cdot \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} = \\frac{1}{2}\\left( (1)(1) + (0)(1) + (2)(-1) + (0)(-1) \\right) = \\frac{1}{2}(1 + 0 - 2 + 0) = -\\frac{1}{2} $$\nThe second dot product is:\n$$ \\mathbf{v} \\cdot \\mathbf{q}_4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix} \\cdot \\frac{1}{2} \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{2}\\left( (1)(1) + (0)(-1) + (2)(-1) + (0)(1) \\right) = \\frac{1}{2}(1 + 0 - 2 + 0) = -\\frac{1}{2} $$\nFinally, we substitute these values into our distance formula:\n$$ d(\\mathbf{v}, W) = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{2}{4}} = \\sqrt{\\frac{1}{2}} $$\nIn simplest radical form, this is:\n$$ d(\\mathbf{v}, W) = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} $$", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{2}}$$", "id": "1381365"}]}