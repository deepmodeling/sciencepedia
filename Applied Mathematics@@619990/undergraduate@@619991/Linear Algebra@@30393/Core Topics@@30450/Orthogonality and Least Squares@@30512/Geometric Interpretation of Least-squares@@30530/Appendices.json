{"hands_on_practices": [{"introduction": "The core purpose of the least-squares method is to find the best possible approximation to a vector within a given subspace. But what happens in the ideal scenario where the vector is already in that subspace? This practice [@problem_id:1363836] explores this fundamental \"perfect fit\" case, establishing a crucial baseline for understanding the geometric nature of approximation error and confirming that when an exact solution exists, the least-squares method will find it.", "problem": "Let $A$ be an $m \\times n$ real matrix and let $b$ be a vector in $\\mathbb{R}^m$. The set of all linear combinations of the columns of $A$ forms a subspace of $\\mathbb{R}^m$, known as the column space of $A$, denoted $\\text{Col}(A)$. A least-squares solution to the linear system $Ax = b$ is a vector $\\hat{x}$ in $\\mathbb{R}^n$ that minimizes the Euclidean norm $\\|Ax - b\\|$. Geometrically, the vector $A\\hat{x}$ is the orthogonal projection of $b$ onto $\\text{Col}(A)$. The value $\\|A\\hat{x} - b\\|$ is the least-squares error.\n\nConsider a specific case where the vector $b$ is known to lie within the column space of $A$, i.e., $b \\in \\text{Col}(A)$.\n\nWhich of the following statements provides the most accurate geometric and algebraic description of the least-squares solution and its associated error for this particular scenario?\n\nA. The least-squares error is zero, and the set of all least-squares solutions is identical to the set of all exact solutions to $Ax = b$.\n\nB. There is a unique least-squares solution $\\hat{x}$, which is given by the projection of $b$ onto the null space of $A^T$. The least-squares error is zero.\n\nC. The least-squares error is non-zero, as the least-squares procedure always finds an approximation. The error vector $b - A\\hat{x}$ is the projection of $b$ onto the null space of $A$.\n\nD. Any least-squares solution vector $\\hat{x}$ must be orthogonal to the null space of $A$, but the error $\\|A\\hat{x} - b\\|$ might be non-zero.\n\nE. The vector $A\\hat{x}$ is orthogonal to the vector $b$, resulting in a minimal-norm, but non-zero, error.", "solution": "By definition, a least-squares solution $\\hat{x}$ minimizes $\\|Ax - b\\|$. The projection theorem states that for any $b \\in \\mathbb{R}^{m}$,\n$$\nb = p + r, \\quad p \\in \\text{Col}(A), \\quad r \\in \\text{Col}(A)^{\\perp},\n$$\nand for any $x$,\n$$\n\\|Ax - b\\|^{2} = \\|Ax - p - r\\|^{2} = \\|(Ax - p)\\|^{2} + \\|r\\|^{2},\n$$\nwhere the minimum is achieved when $Ax = p$, i.e., when $Ax$ is the orthogonal projection of $b$ onto $\\text{Col}(A)$.\n\nIn least squares, $A\\hat{x}$ is the orthogonal projection of $b$ onto $\\text{Col}(A)$, and $\\hat{x}$ satisfies the normal equations\n$$\nA^{T}(A\\hat{x} - b) = 0.\n$$\nEquivalently, the residual $r = b - A\\hat{x}$ lies in $\\text{Col}(A)^{\\perp}$.\n\nNow assume $b \\in \\text{Col}(A)$. Then the orthogonal projection of $b$ onto $\\text{Col}(A)$ is $b$ itself. Therefore, there exists $x_{0}$ with\n$$\nAx_{0} = b,\n$$\nand the least-squares residual achieves\n$$\n\\|A x_{0} - b\\| = 0,\n$$\nwhich is minimal. Hence the least-squares error is zero.\n\nMoreover, any $x$ satisfying $Ax = b$ yields the same minimal residual zero, so every exact solution is a least-squares solution. Conversely, any least-squares solution $\\hat{x}$ must satisfy $A\\hat{x}$ equal to the projection of $b$ onto $\\text{Col}(A)$, which here equals $b$, so $A\\hat{x} = b$. Thus the set of least-squares solutions coincides exactly with the solution set of $Ax = b$:\n$$\n\\{\\hat{x} : \\hat{x} \\text{ is LS for } Ax=b\\} = \\{x : Ax=b\\}.\n$$\nAlgebraically, this also follows from the normal equations: if $Ax=b$ is consistent, any solution to $Ax=b$ satisfies $A^{T}(A x - b)=0$, and the minimal norm of the residual is $0$.\n\nTherefore:\n- The least-squares error is zero.\n- The set of least-squares solutions equals the set of exact solutions to $Ax = b$.\n- Uniqueness is not guaranteed unless $\\text{Null}(A) = \\{0\\}$.\n\nOption A states precisely these facts. Options B, C, D, and E contradict these conclusions in various ways (B incorrectly asserts uniqueness and mischaracterizes the solution; C and E assert non-zero error; D allows non-zero error in this scenario).", "answer": "$$\\boxed{A}$$", "id": "1363836"}, {"introduction": "After exploring the case of a perfect fit, we turn to the opposite extreme. What is the geometric meaning if the best approximation of a non-zero vector $\\mathbf{b}$ in a subspace is simply the zero vector? This exercise [@problem_id:1363842] delves into this scenario, forcing us to consider the concept of orthogonality. By analyzing the case where the projection $\\mathbf{p}$ is $\\mathbf{0}$, we can solidify our understanding of the orthogonal complement and its role in the least-squares decomposition.", "problem": "Let $A$ be an $m \\times n$ real matrix and $\\mathbf{b}$ be a vector in $\\mathbb{R}^m$. Consider the linear system $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x}$ is a vector in $\\mathbb{R}^n$. The least-squares problem seeks to find a vector $\\hat{\\mathbf{x}}$ that minimizes the Euclidean norm of the residual, $\\|A\\mathbf{x} - \\mathbf{b}\\|$. The vector $\\mathbf{p} = A\\hat{\\mathbf{x}}$ is defined as the orthogonal projection of the vector $\\mathbf{b}$ onto the column space of $A$, denoted $C(A)$.\n\nSuppose that for a specific matrix $A$ and a non-zero vector $\\mathbf{b}$, the orthogonal projection $\\mathbf{p}$ is found to be the zero vector, $\\mathbf{p} = \\mathbf{0}$. Which of the following statements provides the most accurate and general geometric description of the relationship between the vector $\\mathbf{b}$ and the column space $C(A)$?\n\nA. The vector $\\mathbf{b}$ must be a scalar multiple of a single column of $A$.\n\nB. The vector $\\mathbf{b}$ lies in the null space of $A$.\n\nC. The vector $\\mathbf{b}$ is a linear combination of the columns of $A$.\n\nD. The vector $\\mathbf{b}$ is orthogonal to every vector in the column space of $A$.\n\nE. The matrix $A$ must be the zero matrix.", "solution": "We consider the least-squares projection of $\\mathbf{b} \\in \\mathbb{R}^{m}$ onto the column space $C(A) \\subseteq \\mathbb{R}^{m}$. By the projection theorem, the orthogonal projection $\\mathbf{p}$ of $\\mathbf{b}$ onto $C(A)$ is characterized by $\\mathbf{p} \\in C(A)$ and the orthogonality condition\n$$\n\\mathbf{b} - \\mathbf{p} \\perp C(A).\n$$\nEquivalently, since $C(A)$ is spanned by the columns of $A$, this orthogonality condition can be written as\n$$\nA^{T}(\\mathbf{b} - \\mathbf{p}) = \\mathbf{0}.\n$$\nIn least squares, the projection satisfies $\\mathbf{p} = A\\hat{\\mathbf{x}}$ for some $\\hat{\\mathbf{x}} \\in \\mathbb{R}^{n}$, and the normal equations are\n$$\nA^{T}A\\hat{\\mathbf{x}} = A^{T}\\mathbf{b}, \\quad \\text{with} \\quad \\mathbf{p} = A\\hat{\\mathbf{x}}.\n$$\nNow suppose $\\mathbf{p} = \\mathbf{0}$. Then the orthogonality condition reduces to\n$$\nA^{T}\\mathbf{b} = \\mathbf{0}.\n$$\nThis means that for each column $\\mathbf{a}_{j}$ of $A$, one has $\\mathbf{a}_{j}^{T}\\mathbf{b} = 0$. Therefore $\\mathbf{b}$ is orthogonal to every column of $A$, and hence to every linear combination of the columns of $A$, i.e., to all of $C(A)$. Thus, the correct geometric description is that $\\mathbf{b}$ is orthogonal to the entire column space $C(A)$.\n\nWe briefly rule out the other options:\n- A is false because $\\mathbf{p} = \\mathbf{0}$ implies orthogonality to each column, not alignment with a column (unless that column is the zero vector).\n- B is incorrect dimensionally: the null space of $A$ is a subspace of $\\mathbb{R}^{n}$, while $\\mathbf{b} \\in \\mathbb{R}^{m}$.\n- C is false because if $\\mathbf{b} \\in C(A)$ and $\\mathbf{b} \\neq \\mathbf{0}$, then the projection would be $\\mathbf{p} = \\mathbf{b} \\neq \\mathbf{0}$.\n- E is false because one can have nonzero $A$ with a nontrivial $C(A)$ and choose $\\mathbf{b}$ orthogonal to $C(A)$; then $\\mathbf{p} = \\mathbf{0}$ without $A$ being the zero matrix.\n\nTherefore, the most accurate and general statement is that $\\mathbf{b}$ is orthogonal to every vector in $C(A)$.", "answer": "$$\\boxed{D}$$", "id": "1363842"}, {"introduction": "Having examined the static boundary cases, we can now investigate the dynamics of the projection. How does the best approximation and its associated error change when we alter the target vector? This problem [@problem_id:1363790] provides a key insight by showing what happens when we shift the target vector $\\mathbf{b}$ by a vector $\\mathbf{w}$ that already lies within the approximation subspace. This reveals the powerful geometric principle that the error vector, representing the orthogonal distance to the subspace, remains unchanged by such a shift.", "problem": "Let $A$ be an $m \\times n$ real matrix with linearly independent columns, and let $b$ be a vector in $\\mathbb{R}^m$. In the context of a least-squares problem, the vector $\\hat{x}$ is the unique vector in $\\mathbb{R}^n$ that minimizes the squared Euclidean norm $\\|A\\mathbf{x} - b\\|^2$. The vector $p = A\\hat{x}$ is the component of $b$ that lies in the column space of $A$, and the vector $e = b - p$ is the error component.\n\nNow, suppose we define a new target vector $b' = b + w$, where $w$ is a non-zero vector that also lies in the column space of $A$. Let $\\hat{x}'$ be the unique vector that minimizes the new squared norm $\\|A\\mathbf{x} - b'\\|^2$. The new corresponding vectors are $p' = A\\hat{x}'$ and $e' = b' - p'$.\n\nWhich of the following statements correctly describes the new vectors $p'$ and $e'$ in terms of the original vectors?\n\nA. $p' = p$ and $e' = e + w$\n\nB. $p' = p + w$ and $e' = e$\n\nC. $p' = p + w$ and $e' = e + w$\n\nD. $p' = p$ and $e' = e$\n\nE. The relationship cannot be determined without knowing the specific matrix $A$.", "solution": "The problem asks for the new projected vector $p'$ and the new error vector $e'$ resulting from a least-squares problem with a modified target vector $b'$.\n\nLet $C(A)$ denote the column space of the matrix $A$. By definition, the least-squares solution $A\\hat{x}$ is the orthogonal projection of the vector $b$ onto the subspace $C(A)$. Let's denote the projection operator onto $C(A)$ as $\\text{proj}_{C(A)}$.\n\nThe original projected vector is $p = A\\hat{x} = \\text{proj}_{C(A)}(b)$.\nThe original error vector is $e = b - p = b - \\text{proj}_{C(A)}(b)$. A key property of this decomposition is that the error vector $e$ is orthogonal to the subspace $C(A)$.\n\nWe are given a new target vector $b' = b + w$. We are also told that the vector $w$ lies in the column space of $A$, i.e., $w \\in C(A)$.\n\nThe new projected vector $p'$ is the orthogonal projection of $b'$ onto the column space $C(A)$.\n$$p' = \\text{proj}_{C(A)}(b')$$\nSubstituting the expression for $b'$, we get:\n$$p' = \\text{proj}_{C(A)}(b + w)$$\nThe projection operator is a linear transformation. Therefore, we can distribute it over the sum:\n$$p' = \\text{proj}_{C(A)}(b) + \\text{proj}_{C(A)}(w)$$\nWe know that $\\text{proj}_{C(A)}(b)$ is just the original projected vector $p$.\nNow consider $\\text{proj}_{C(A)}(w)$. Since the vector $w$ is already in the subspace $C(A)$, its projection onto $C(A)$ is the vector $w$ itself.\n$$\\text{proj}_{C(A)}(w) = w$$\nSubstituting these back into the expression for $p'$, we find:\n$$p' = p + w$$\nSo, the new projected vector is the sum of the original projected vector and the perturbation vector $w$.\n\nNext, let's find the new error vector $e'$. By definition, $e'$ is the difference between the new target vector $b'$ and the new projected vector $p'$.\n$$e' = b' - p'$$\nWe can substitute the expressions we have for $b'$ and $p'$:\n$$e' = (b + w) - (p + w)$$\nSimplifying the expression, we get:\n$$e' = b + w - p - w = b - p$$\nThe expression $b - p$ is the definition of the original error vector $e$.\nTherefore, we have:\n$$e' = e$$\nThis means that the error vector does not change when the target vector $b$ is shifted by a vector $w$ that is already in the column space of $A$. Geometrically, adding a vector from the subspace only shifts the target vector parallel to the subspace, so its orthogonal distance to the subspace remains the same.\n\nIn summary, the new vectors are $p' = p + w$ and $e' = e$. This corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1363790"}]}