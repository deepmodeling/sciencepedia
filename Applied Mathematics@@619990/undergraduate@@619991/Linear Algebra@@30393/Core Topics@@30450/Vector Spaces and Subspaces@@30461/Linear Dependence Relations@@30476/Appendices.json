{"hands_on_practices": [{"introduction": "The principles of linear algebra extend far beyond geometric vectors; they apply to any objects that can be added and scaled, including polynomials. This exercise [@problem_id:1372989] demonstrates how to determine if a set of polynomials is linearly dependent by translating them into coordinate vectors relative to a standard basis. By doing so, you can leverage powerful, concrete tools like the determinant to answer abstract questions about function spaces.", "problem": "In a simplified model of a physical system, the state of a particle is described by a function of time, $p(t)$, which is a polynomial of degree at most 2. These states are elements of the vector space $P_2$, which consists of all polynomials of the form $a_0 + a_1 t + a_2 t^2$, where $a_0, a_1, a_2$ are real numbers.\n\nAn engineer defines three fundamental state configurations for an experiment, represented by the following polynomials:\n$p_1(t) = 1 + 2t - 3t^2$\n$p_2(t) = 3 + 5t - 2t^2$\n$p_3(t) = -4 + ht + t^2$\n\nThe parameter $h$ represents a tunable setting in the experimental apparatus. For the three states to serve as a distinct set of references, they must be linearly independent. If the set of states is linearly dependent, it implies a redundancy in the setup, as one state can be expressed as a linear combination of the others.\n\nDetermine the specific value of the parameter $h$ for which the set of polynomials $\\{p_1(t), p_2(t), p_3(t)\\}$ is linearly dependent.", "solution": "We work in the basis $\\{1, t, t^{2}\\}$ of $P_{2}$, so each polynomial is represented by its coefficient vector:\n$$\np_{1}(t) \\leftrightarrow (1,\\,2,\\,-3),\\quad\np_{2}(t) \\leftrightarrow (3,\\,5,\\,-2),\\quad\np_{3}(t) \\leftrightarrow (-4,\\,h,\\,1).\n$$\nThe three polynomials are linearly dependent if and only if the determinant of the coefficient matrix is zero. Using the vectors as rows, define\n$$\nM=\\begin{pmatrix}\n1 & 2 & -3\\\\\n3 & 5 & -2\\\\\n-4 & h & 1\n\\end{pmatrix}.\n$$\nThen linear dependence is equivalent to $\\det(M)=0$. Compute $\\det(M)$ by expanding along the first row:\n$$\n\\det(M)=1\\cdot\\det\\begin{pmatrix}5 & -2\\\\ h & 1\\end{pmatrix}\n-2\\cdot\\det\\begin{pmatrix}3 & -2\\\\ -4 & 1\\end{pmatrix}\n+(-3)\\cdot\\det\\begin{pmatrix}3 & 5\\\\ -4 & h\\end{pmatrix}.\n$$\nEvaluate the $2\\times 2$ determinants:\n$$\n\\det\\begin{pmatrix}5 & -2\\\\ h & 1\\end{pmatrix}=5+2h,\\quad\n\\det\\begin{pmatrix}3 & -2\\\\ -4 & 1\\end{pmatrix}=3-8=-5,\\quad\n\\det\\begin{pmatrix}3 & 5\\\\ -4 & h\\end{pmatrix}=3h+20.\n$$\nSubstitute to obtain\n$$\n\\det(M)=(5+2h)-2(-5)+(-3)(3h+20)=(5+2h)+10-9h-60=-7h-45.\n$$\nSet this equal to zero for linear dependence:\n$$\n-7h-45=0\\quad\\Rightarrow\\quad h=-\\frac{45}{7}.\n$$\nTherefore, the set $\\{p_{1},p_{2},p_{3}\\}$ is linearly dependent precisely when $h=-\\frac{45}{7}$.", "answer": "$$\\boxed{-\\frac{45}{7}}$$", "id": "1372989"}, {"introduction": "Linear independence in infinite-dimensional vector spaces, such as the space of continuous functions, requires moving beyond matrix-based shortcuts and returning to first principles. This practice [@problem_id:1372948] challenges you to apply the fundamental definition of linear dependence to various sets of functions, revealing how the choice of domain can surprisingly determine whether a set is dependent or independent. This is a crucial exercise for understanding the nuances that distinguish infinite-dimensional spaces.", "problem": "In linear algebra, a set of vectors $\\{v_1, v_2, \\dots, v_n\\}$ in a vector space $V$ is defined as **linearly independent** if the only solution to the equation $c_1 v_1 + c_2 v_2 + \\dots + c_n v_n = \\mathbf{0}$ (where $\\mathbf{0}$ is the zero vector in $V$) is the trivial solution $c_1=c_2=\\dots=c_n=0$. If a non-trivial solution exists, the set is **linearly dependent**.\n\nConsider the vector space $V = C(I)$, which is the space of all continuous real-valued functions defined on an interval $I \\subseteq \\mathbb{R}$. In this space, the \"vectors\" are the functions, and the zero vector is the function $f(x)=0$ for all $x \\in I$.\n\nAnalyze the linear independence of the following sets of functions within different vector spaces.\n\n*   **Set 1:** $\\{f_1(x) = \\exp(x), f_2(x) = \\exp(2x)\\}$\n*   **Set 2:** $\\{g_1(x) = 1, g_2(x) = \\cos(2x), g_3(x) = \\sin^2(x)\\}$\n*   **Set 3:** $\\{h_1(x) = x, h_2(x) = |x|\\}$\n*   **Set 4:** $\\{k_1(x) = x^3, k_2(x) = |x^3|\\}$\n\nWhich of the following statements is true?\n\nA. Set 1 is linearly dependent in the space $C(\\mathbb{R})$.\n\nB. Set 2 is linearly independent in the space $C(\\mathbb{R})$.\n\nC. Set 3 is linearly dependent in the space $C([-2, 2])$, but linearly independent in the space $C([0, 2])$.\n\nD. Set 4 is linearly independent in the space $C([-2, 2])$, but linearly dependent in the space $C([-2, 0])$.", "solution": "We use the definition of linear independence in the function space context: a set $\\{f_{1},\\dots,f_{n}\\} \\subset C(I)$ is linearly independent if the only solution to $c_{1}f_{1}(x)+\\cdots+c_{n}f_{n}(x)=0$ for all $x \\in I$ is $c_{1}=\\cdots=c_{n}=0$.\n\nSet 1: $\\{f_{1}(x)=\\exp(x), f_{2}(x)=\\exp(2x)\\}$ in $C(\\mathbb{R})$. Suppose $a\\exp(x)+b\\exp(2x)=0$ for all $x \\in \\mathbb{R}$. Factor $\\exp(x)$ to obtain\n$$\n\\exp(x)\\big(a+b\\exp(x)\\big)=0 \\quad \\text{for all } x.\n$$\nSince $\\exp(x)\\neq 0$ for all $x$, this implies $a+b\\exp(x)=0$ for all $x$. The function $b\\exp(x)$ is nonconstant unless $b=0$. Hence $b=0$, which then forces $a=0$. Therefore $f_{1},f_{2}$ are linearly independent, so the statement “Set 1 is linearly dependent in $C(\\mathbb{R})$” is false.\n\nSet 2: $\\{g_{1}(x)=1,g_{2}(x)=\\cos(2x),g_{3}(x)=\\sin^{2}(x)\\}$ in $C(\\mathbb{R})$. Use the identity\n$$\n\\sin^{2}(x)=\\frac{1-\\cos(2x)}{2}.\n$$\nRewriting gives\n$$\n2\\sin^{2}(x)-1+\\cos(2x)=0 \\quad \\text{for all } x.\n$$\nThus the nontrivial linear combination $(-1)\\cdot g_{1}(x)+(1)\\cdot g_{2}(x)+2\\cdot g_{3}(x)=0$ holds identically, so the set is linearly dependent. Therefore the statement “Set 2 is linearly independent in $C(\\mathbb{R})$” is false.\n\nSet 3: $\\{h_{1}(x)=x,h_{2}(x)=|x|\\}$. First consider $C([-2,2])$. Suppose $a x+b|x|=0$ for all $x\\in[-2,2]$. For $x\\in[0,2]$ we have $|x|=x$, hence $(a+b)x=0$ for all $x\\in[0,2]$, which implies $a+b=0$. For $x\\in[-2,0]$ we have $|x|=-x$, hence $(a-b)x=0$ for all $x\\in[-2,0]$, which implies $a-b=0$. Solving $a+b=0$ and $a-b=0$ gives $a=0$ and $b=0$, so the set is linearly independent on $[-2,2]$. Now consider $C([0,2])$. On $[0,2]$, $|x|=x$, so $h_{2}=h_{1}$, and thus there exists the nontrivial relation $h_{2}-h_{1}=0$. Hence the set is linearly dependent on $[0,2]$. Therefore the statement “Set 3 is linearly dependent in $C([-2,2])$, but linearly independent in $C([0,2])$” is false.\n\nSet 4: $\\{k_{1}(x)=x^{3},k_{2}(x)=|x^{3}|\\}$. Note that $|x^{3}|=x^{3}$ for $x\\geq 0$ and $|x^{3}|=-x^{3}$ for $x\\leq 0$. In $C([-2,2])$, suppose $a x^{3}+b|x^{3}|=0$ for all $x\\in[-2,2]$. For $x\\in[0,2]$, $(a+b)x^{3}=0$ for all $x$ implies $a+b=0$. For $x\\in[-2,0]$, $(a-b)x^{3}=0$ for all $x$ implies $a-b=0$. Thus $a=b=0$, and the set is linearly independent on $[-2,2]$. In $C([-2,0])$, for all $x\\leq 0$ we have $|x^{3}|=-x^{3}$, so $k_{2}=-k_{1}$ on this interval. Therefore $k_{1}+k_{2}=0$ is a nontrivial linear relation, and the set is linearly dependent on $[-2,0]$. Hence the statement “Set 4 is linearly independent in $C([-2,2])$, but linearly dependent in $C([-2,0])$” is true.\n\nOnly statement D is true.", "answer": "$$\\boxed{D}$$", "id": "1372948"}, {"introduction": "What happens to linear independence when we create new vectors by combining elements of an already independent set? This problem [@problem_id:1372976] explores that very question, revealing a subtle but profound truth: the underlying number system, or field, can dictate the outcome. This exercise pushes beyond straightforward computation to cultivate a more abstract and robust understanding of the structural rules that govern vector spaces.", "problem": "In a theoretical model of a multi-particle system, its possible configurations are represented by vectors in a vector space $V$ over a field $F$. A set of three fundamental states, represented by the vectors $\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3 \\}$, has been identified and is known to form a linearly independent set.\n\nTo analyze the system's dynamics, an analyst constructs a new set of three 'composite' states, $S'$, which are formed by pairwise combinations of the fundamental states:\n- $\\mathbf{w}_1 = \\mathbf{v}_1 + \\mathbf{v}_2$\n- $\\mathbf{w}_2 = \\mathbf{v}_2 + \\mathbf{v}_3$\n- $\\mathbf{w}_3 = \\mathbf{v}_3 + \\mathbf{v}_1$\n\nThe set of these composite states is thus $S' = \\{ \\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3 \\}$. Which of the following statements provides a complete and accurate description of the linear independence of the set $S'$?\n\nA. The set $S'$ is always linearly independent, regardless of the field $F$.\n\nB. The set $S'$ is always linearly dependent, regardless of the field $F$.\n\nC. The linear independence of $S'$ depends on the specific choice of the vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3$.\n\nD. The linear independence of $S'$ depends on the characteristic of the field $F$.\n\nE. The linear independence of $S'$ depends on the dimension of the vector space $V$.", "solution": "Let $\\alpha,\\beta,\\gamma \\in F$ satisfy the linear relation\n$$\n\\alpha \\mathbf{w}_{1}+\\beta \\mathbf{w}_{2}+\\gamma \\mathbf{w}_{3}=\\mathbf{0}.\n$$\nUsing the definitions $\\mathbf{w}_{1}=\\mathbf{v}_{1}+\\mathbf{v}_{2}$, $\\mathbf{w}_{2}=\\mathbf{v}_{2}+\\mathbf{v}_{3}$, and $\\mathbf{w}_{3}=\\mathbf{v}_{3}+\\mathbf{v}_{1}$, expand and collect terms:\n$$\n\\alpha(\\mathbf{v}_{1}+\\mathbf{v}_{2})+\\beta(\\mathbf{v}_{2}+\\mathbf{v}_{3})+\\gamma(\\mathbf{v}_{3}+\\mathbf{v}_{1})\n=(\\alpha+\\gamma)\\mathbf{v}_{1}+(\\alpha+\\beta)\\mathbf{v}_{2}+(\\beta+\\gamma)\\mathbf{v}_{3}.\n$$\nSince $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\mathbf{v}_{3}\\}$ is linearly independent, each coefficient must vanish:\n$$\n\\alpha+\\gamma=0,\\quad \\alpha+\\beta=0,\\quad \\beta+\\gamma=0.\n$$\nFrom $\\alpha+\\beta=0$ and $\\alpha+\\gamma=0$ we obtain $\\beta=-\\alpha$ and $\\gamma=-\\alpha$. Substituting into $\\beta+\\gamma=0$ gives\n$$\n(-\\alpha)+(-\\alpha)= -2\\alpha=0 \\quad \\Longleftrightarrow \\quad 2\\alpha=0.\n$$\nTherefore:\n- If $\\operatorname{char}(F)\\neq 2$, then $2\\neq 0$ in $F$, so $\\alpha=0$, hence $\\beta=0$ and $\\gamma=0$. Thus $\\{\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}\\}$ is linearly independent.\n- If $\\operatorname{char}(F)=2$, then $2=0$ in $F$, so any $\\alpha\\in F$ with $\\beta=\\gamma=-\\alpha=\\alpha$ yields a nontrivial relation. Indeed,\n$$\n\\mathbf{w}_{1}+\\mathbf{w}_{2}+\\mathbf{w}_{3}\n=2(\\mathbf{v}_{1}+\\mathbf{v}_{2}+\\mathbf{v}_{3})=\\mathbf{0}\n$$\nin characteristic $2$, so $\\{\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}\\}$ is linearly dependent.\n\nHence the linear independence of $S'$ depends precisely on the characteristic of the field $F$.", "answer": "$$\\boxed{D}$$", "id": "1372976"}]}