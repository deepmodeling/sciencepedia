{"hands_on_practices": [{"introduction": "Understanding linear independence begins with its fundamental definition. This exercise moves beyond simple numerical examples to test your grasp of the core concept in a more abstract setting. By determining if linear combinations of independent vectors remain independent, you will practice applying the definition directly, a key skill for constructing proofs and reasoning about vector spaces [@problem_id:1373433].", "problem": "In the design of a digital communication system, signals are modeled as vectors in a vector space $V$ over the real numbers. Two fundamental source signals, represented by the vectors $u$ and $v$, are known to be \"informationally distinct,\" which in the language of linear algebra means that the set $\\{u, v\\}$ is linearly independent.\n\nA signal processing component is designed to create two new transmission channels from these source signals. The first new signal is the \"sum signal,\" $s_1 = u + v$, and the second is the \"difference signal,\" $s_2 = u - v$. For the system to function without loss of information, the set of new signals $\\{s_1, s_2\\}$ must also be informationally distinct, i.e., linearly independent.\n\nBased on the initial condition that $\\{u, v\\}$ is a linearly independent set, what can be concluded about the linear independence of the set $\\{s_1, s_2\\} = \\{u+v, u-v\\}$?\n\nA. The set $\\{u+v, u-v\\}$ is always linearly independent.\n\nB. The set $\\{u+v, u-v\\}$ is always linearly dependent.\n\nC. The linear independence of $\\{u+v, u-v\\}$ depends on the specific choice of $u$ and $v$.\n\nD. The linear independence of $\\{u+v, u-v\\}$ is only guaranteed if $u$ and $v$ are orthogonal.\n\nE. More information about the vector space $V$ is required to make a determination.", "solution": "We are given a real vector space $V$ and two vectors $u,v \\in V$ such that the set $\\{u,v\\}$ is linearly independent. We want to determine whether the set $\\{u+v, u-v\\}$ is linearly independent.\n\nBy the definition of linear independence, to test $\\{u+v, u-v\\}$ we consider a linear combination that equals the zero vector:\n$$\n\\alpha(u+v) + \\beta(u-v) = 0.\n$$\nExpanding and grouping like terms gives\n$$\n(\\alpha + \\beta)u + (\\alpha - \\beta)v = 0.\n$$\nSince $\\{u,v\\}$ is linearly independent, the only way this linear combination equals the zero vector is if each coefficient is zero:\n$$\n\\alpha + \\beta = 0, \\quad \\alpha - \\beta = 0.\n$$\nSolving this system, add the equations to obtain\n$$\n2\\alpha = 0 \\quad \\Rightarrow \\quad \\alpha = 0,\n$$\nand subtract the second from the first to obtain\n$$\n2\\beta = 0 \\quad \\Rightarrow \\quad \\beta = 0.\n$$\nBecause we are over the real numbers, $2 \\neq 0$, so the implications above are valid. Hence the only solution is the trivial one, and therefore the set $\\{u+v, u-v\\}$ is linearly independent.\n\nThus, based on the given condition that $\\{u,v\\}$ is linearly independent, the set $\\{u+v, u-v\\}$ is always linearly independent. The correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1373433"}, {"introduction": "Is linear dependence simply about one vector being a multiple of another? This practice challenges that common oversimplification, pushing you to explore a more nuanced geometric case. You will identify a set of vectors in $\\mathbb{R}^3$ that are coplanar (linearly dependent) even though no single vector is a scaled version of another, deepening your intuition for what linear dependence truly represents in three-dimensional space [@problem_id:1374338].", "problem": "In the vector space $\\mathbb{R}^3$, a set of vectors $\\{v_1, v_2, \\dots, v_k\\}$ is defined as **linearly dependent** if there exist scalars $c_1, c_2, \\dots, c_k$, not all zero, such that $c_1 v_1 + c_2 v_2 + \\dots + c_k v_k = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n\nConsider the following properties for a set of three distinct vectors in $\\mathbb{R}^3$:\n1. The set is linearly dependent.\n2. No vector in the set is a scalar multiple of another vector in the set.\n\nWhich of the following sets of vectors satisfies both of these properties?\n\nA. $S = \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}$\n\nB. $S = \\{(1, 1, 2), (2, 2, 4), (1, 0, 0)\\}$\n\nC. $S = \\{(1, 2, 1), (2, 1, 0), (4, 5, 2)\\}$\n\nD. $S = \\{(1, 2, 3), (4, 5, 6), (0, 0, 0)\\}$\n\nE. $S = \\{(1, 1, 0), (0, 1, 1), (1, 0, 1)\\}$", "solution": "We are to find, among the given sets of three distinct vectors in $\\mathbb{R}^{3}$, one that is linearly dependent and also has no pair where one vector is a scalar multiple of another. For three vectors $v_{1},v_{2},v_{3}$ in $\\mathbb{R}^{3}$, linear independence is equivalent to the determinant of the $3\\times 3$ matrix formed by placing the vectors as rows (or columns) being nonzero; linear dependence occurs if and only if this determinant is zero. A vector $u$ is a scalar multiple of $v$ if there exists $\\lambda \\in \\mathbb{R}$ with $u=\\lambda v$.\n\nAnalyze each option:\n\nA. $S=\\{(1,0,0),(0,1,0),(0,0,1)\\}$. The matrix with these as rows is the identity, whose determinant equals $1\\neq 0$. Hence the set is linearly independent, violating property 1. Therefore A does not satisfy both properties.\n\nB. $S=\\{(1,1,2),(2,2,4),(1,0,0)\\}$. Here $(2,2,4)=2(1,1,2)$, so one vector is a scalar multiple of another, violating property 2. The set is linearly dependent since $2(1,1,2)-(2,2,4)=(0,0,0)$ gives a nontrivial relation, but property 2 fails. Therefore B does not satisfy both properties.\n\nC. $S=\\{(1,2,1),(2,1,0),(4,5,2)\\}$. Form the matrix with rows equal to the vectors:\n$$\nM=\\begin{pmatrix}\n1 & 2 & 1\\\\\n2 & 1 & 0\\\\\n4 & 5 & 2\n\\end{pmatrix}.\n$$\nCompute $\\det(M)$ by cofactor expansion along the first row:\n$$\n\\det(M)=1\\cdot\\det\\begin{pmatrix}1 & 0\\\\ 5 & 2\\end{pmatrix}-2\\cdot\\det\\begin{pmatrix}2 & 0\\\\ 4 & 2\\end{pmatrix}+1\\cdot\\det\\begin{pmatrix}2 & 1\\\\ 4 & 5\\end{pmatrix}.\n$$\nEvaluate the $2\\times 2$ determinants:\n$$\n\\det\\begin{pmatrix}1 & 0\\\\ 5 & 2\\end{pmatrix}=1\\cdot 2-0\\cdot 5=2,\n$$\n$$\n\\det\\begin{pmatrix}2 & 0\\\\ 4 & 2\\end{pmatrix}=2\\cdot 2-0\\cdot 4=4,\n$$\n$$\n\\det\\begin{pmatrix}2 & 1\\\\ 4 & 5\\end{pmatrix}=2\\cdot 5-1\\cdot 4=10-4=6.\n$$\nThus\n$$\n\\det(M)=1\\cdot 2-2\\cdot 4+1\\cdot 6=2-8+6=0.\n$$\nTherefore the set is linearly dependent (property 1 holds). Now check that no vector is a scalar multiple of another:\n\n- Suppose $(2,1,0)=\\lambda(1,2,1)$. Comparing components gives $2=\\lambda\\cdot 1$, $1=\\lambda\\cdot 2$, and $0=\\lambda\\cdot 1$. From $0=\\lambda\\cdot 1$ we get $\\lambda=0$, which contradicts $2=\\lambda\\cdot 1$. Hence not a scalar multiple.\n\n- Suppose $(4,5,2)=\\mu(1,2,1)$. Then $4=\\mu\\cdot 1$, $5=\\mu\\cdot 2$, and $2=\\mu\\cdot 1$. From the first and third, $\\mu=4$ and $\\mu=2$, a contradiction. Hence not a scalar multiple.\n\n- Suppose $(4,5,2)=\\nu(2,1,0)$. Then $4=\\nu\\cdot 2$ implies $\\nu=2$, but $5=\\nu\\cdot 1$ would require $\\nu=5$, and also $2=\\nu\\cdot 0$ is impossible. Hence not a scalar multiple.\n\nTherefore property 2 also holds. So C satisfies both properties.\n\nD. $S=\\{(1,2,3),(4,5,6),(0,0,0)\\}$. Any set containing the zero vector is linearly dependent, since $1\\cdot(0,0,0)+0\\cdot(1,2,3)+0\\cdot(4,5,6)=(0,0,0)$ is a nontrivial relation. However, property 2 fails because $(0,0,0)=0\\cdot(1,2,3)$ is a scalar multiple relation with another vector in the set. Therefore D does not satisfy both properties.\n\nE. $S=\\{(1,1,0),(0,1,1),(1,0,1)\\}$. Form the matrix with these as rows:\n$$\nN=\\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\\\\\n1 & 0 & 1\n\\end{pmatrix}.\n$$\nCompute the determinant:\n$$\n\\det(N)=1\\cdot\\det\\begin{pmatrix}1 & 1\\\\ 0 & 1\\end{pmatrix}-1\\cdot\\det\\begin{pmatrix}0 & 1\\\\ 1 & 1\\end{pmatrix}+0\\cdot\\det\\begin{pmatrix}0 & 1\\\\ 1 & 0\\end{pmatrix}.\n$$\nEvaluate:\n$$\n\\det\\begin{pmatrix}1 & 1\\\\ 0 & 1\\end{pmatrix}=1\\cdot 1-1\\cdot 0=1,\\quad\n\\det\\begin{pmatrix}0 & 1\\\\ 1 & 1\\end{pmatrix}=0\\cdot 1-1\\cdot 1=-1.\n$$\nHence\n$$\n\\det(N)=1-(-1)+0=2\\neq 0,\n$$\nso the set is linearly independent, violating property 1. Therefore E does not satisfy both properties.\n\nThe only set satisfying both properties is option C.", "answer": "$$\\boxed{C}$$", "id": "1374338"}, {"introduction": "In many practical applications, we need an efficient way to check for linear dependence. This problem introduces a powerful computational tool: the determinant. You will see how setting up a matrix with vectors as columns allows us to translate the abstract question of dependence into a concrete calculation, finding the specific parameter that makes a set of signal vectors lose their independence [@problem_id:1374383].", "problem": "In a signal processing experiment, three signals are sampled at three distinct moments in time, yielding three vectors in $\\mathbb{R}^3$. Let these signal vectors be $v_1 = (1, 0, 1)$, $v_2 = (2, 1, 3)$, and $v_3 = (1, k, 0)$, where $k$ is a tunable real-valued parameter related to the amplification of the third signal. For the system to be efficient, the three signals must be linearly independent. Your task is to identify the specific value(s) of $k$ that would compromise this efficiency. Find all real values of $k$ for which the set of vectors $\\{v_1, v_2, v_3\\}$ is linearly dependent.", "solution": "To determine when the set $\\{v_{1}, v_{2}, v_{3}\\}$ is linearly dependent, use the fact that three vectors in $\\mathbb{R}^{3}$ are linearly dependent if and only if the determinant of the matrix formed by taking them as columns is zero.\n\nForm the matrix with columns $v_{1}=(1,0,1)$, $v_{2}=(2,1,3)$, and $v_{3}=(1,k,0)$:\n$$\nM=\\begin{pmatrix}\n1 & 2 & 1 \\\\\n0 & 1 & k \\\\\n1 & 3 & 0\n\\end{pmatrix}.\n$$\nCompute $\\det(M)$ by cofactor expansion along the first row:\n$$\n\\det(M)=1\\cdot\\det\\begin{pmatrix}1 & k \\\\ 3 & 0\\end{pmatrix}\n-2\\cdot\\det\\begin{pmatrix}0 & k \\\\ 1 & 0\\end{pmatrix}\n+1\\cdot\\det\\begin{pmatrix}0 & 1 \\\\ 1 & 3\\end{pmatrix}.\n$$\nEvaluate the $2\\times 2$ determinants:\n$$\n\\det\\begin{pmatrix}1 & k \\\\ 3 & 0\\end{pmatrix}=1\\cdot 0 - k\\cdot 3=-3k,\n$$\n$$\n\\det\\begin{pmatrix}0 & k \\\\ 1 & 0\\end{pmatrix}=0\\cdot 0 - k\\cdot 1=-k,\n$$\n$$\n\\det\\begin{pmatrix}0 & 1 \\\\ 1 & 3\\end{pmatrix}=0\\cdot 3 - 1\\cdot 1=-1.\n$$\nSubstitute back:\n$$\n\\det(M) = 1\\cdot(-3k) - 2\\cdot(-k) + 1\\cdot(-1) = -3k + 2k - 1 = -k - 1.\n$$\nFor linear dependence, set $\\det(M)=0$:\n$$\n-k - 1 = 0 \\quad \\Rightarrow \\quad k = -1.\n$$\nThus, the vectors are linearly dependent precisely when $k=-1$.", "answer": "$$\\boxed{-1}$$", "id": "1374383"}]}