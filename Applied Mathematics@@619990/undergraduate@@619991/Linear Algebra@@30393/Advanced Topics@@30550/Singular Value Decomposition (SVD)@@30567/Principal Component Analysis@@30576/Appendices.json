{"hands_on_practices": [{"introduction": "The theoretical foundation of Principal Component Analysis (PCA) lies in the eigendecomposition of a covariance matrix. To truly grasp this connection, it is invaluable to move beyond abstract definitions and perform the core calculations by hand. This exercise [@problem_id:2416060] guides you through this fundamental process on a small, illustrative dataset, solidifying your understanding of how eigenvectors correspond to the principal components that capture the directions of maximum variance.", "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.", "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive yields\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "PCA is sensitive to the variance of variables, which can lead to misleading results when features are measured on different scales. This computational practice [@problem_id:2421735] demonstrates the critical importance of standardization by generating synthetic data where one variable's scale artificially dominates the analysis. You will quantitatively compare the principal components derived from raw versus standardized data, revealing how standardization is essential for uncovering the true underlying correlation structure.", "problem": "You are asked to demonstrate, using first principles of Principal Component Analysis (PCA), how failing to standardize variables measured in different units can distort the estimated principal directions and the explained variance. Work in a purely mathematical framework with a synthetic data-generating process that models typical financial variables such as prices and volumes. You will implement the full pipeline and report quantitative diagnostics that compare PCA on raw data versus PCA on standardized data.\n\nFundamental base:\n- PCA seeks orthonormal directions that maximize sample variance. Given a centered data matrix $X \\in \\mathbb{R}^{T \\times n}$, the sample covariance matrix is $\\Sigma = \\frac{1}{T-1} X^\\top X$. The principal components are the eigenvectors of $\\Sigma$ corresponding to its eigenvalues, ordered from largest to smallest.\n- Standardization transforms each variable $x_j$ to $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$, where $\\bar{x}_j$ is the sample mean and $\\hat{\\sigma}_j$ is the sample standard deviation, so that each standardized variable has unit sample variance. PCA on standardized data is PCA on the sample correlation matrix.\n- Diagonal rescaling $D = \\operatorname{diag}(s_1,\\dots,s_n)$ applied to variables, $X \\mapsto X D$, multiplies the covariance entries by $s_i s_j$, thereby altering eigenvectors unless all $s_j$ are equal.\n\nData-generating process:\n- For each test case $k$, fix $T_k \\in \\mathbb{N}$, number of variables $n_k \\in \\mathbb{N}$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n- Generate a single common factor $f_t \\sim \\mathcal{N}(0,1)$ for $t = 1,\\dots,T_k$, and idiosyncratic noises $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$, all mutually independent across $t$ and $j$.\n- Construct raw observations $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$ for $t=1,\\dots,T_k$ and $j=1,\\dots,n_k$.\n- Center each column of $X$ by subtracting its sample mean before computing any covariance.\n\nComputation tasks per test case:\n- Compute the sample covariance matrix $\\Sigma_{\\text{raw}}$ from the centered raw data $X$ and obtain the first principal component eigenvector $v_{\\text{raw}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{raw}}$.\n- Standardize each column of $X$ to unit sample variance to obtain $Z$, compute $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$ (the sample correlation matrix), and obtain the first principal component eigenvector $v_{\\text{std}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{std}}$.\n- Compute the angle $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$; report $\\theta$ in radians.\n- Compute the difference in explained variance shares as $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$, which must be reported as a decimal fraction (not a percentage).\n\nRandomness and reproducibility:\n- Use a fixed pseudorandom number generator seed equal to $314159$ for the entire experiment to ensure reproducible results.\n\nTest suite:\n- There are $3$ test cases. For each test case $k$, use the following parameters $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$:\n  - Case $1$ (similar units, two variables):\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - Case $2$ (mismatched units, two variables: one dominates by scale):\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - Case $3$ (mismatched units, three variables: one huge-scale, one moderate, one tiny-scale):\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\nRequired outputs per test case:\n- A list of two floats $[\\theta, \\Delta]$ where $\\theta$ is the angle in radians and $\\Delta$ is the absolute difference in explained variance shares. Both values must be rounded to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-case lists, enclosed in square brackets, for example, $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$, with each float rounded to exactly $6$ decimal places and angles in radians.", "solution": "The problem presented is a valid and well-posed exercise in computational statistics, specifically demonstrating the sensitivity of Principal Component Analysis (PCA) to the scaling of variables. It is scientifically sound, resting on foundational principles of linear algebra and statistics, and all parameters and procedures are specified with sufficient clarity to permit a unique, verifiable solution. We will proceed with the analysis.\n\nThe central thesis is that PCA, as a variance-maximization technique, is not scale-invariant. When variables are measured in disparate units (e.g., a stock price in dollars versus its trading volume in millions of shares), the variable with the largest variance will mechanically dominate the first principal component. This is often an artifact of the units chosen rather than an indicator of true underlying importance. Standardization is the standard remedy, transforming all variables to a common scale (unit variance) so that the analysis focuses on the correlation structure of the data, not the arbitrary measurement scales.\n\nWe begin by formalizing the data generation and analysis pipeline.\n\n**1. Data-Generating Process**\n\nFor each test case $k$, we are given a sample size $T_k$, number of variables $n_k$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n\nThe data are generated from a single-factor model. A common latent factor $f_t$ is drawn from a standard normal distribution, $f_t \\sim \\mathcal{N}(0, 1)$, for each time point $t=1, \\dots, T_k$. For each variable $j=1, \\dots, n_k$, an idiosyncratic noise term $e_{t,j}$ is drawn from $\\mathcal{N}(0, (u^{(k)}_j)^2)$. All $f_t$ and $e_{t,j}$ are mutually independent.\n\nThe observed value for variable $j$ at time $t$ is constructed as:\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\nThis forms a data matrix $X \\in \\mathbb{R}^{T_k \\times n_k}$ whose columns represent the different variables. The scale factor $s^{(k)}_j$ represents the arbitrary unit of measurement for variable $j$.\n\n**2. PCA on Raw Data (Covariance-Based PCA)**\n\nThe first step in PCA is to center the data by subtracting the column-wise sample mean. Let $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ be the sample mean of the $j$-th variable. The centered data matrix, denoted $X_c$, has entries $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$.\n\nThe sample covariance matrix $\\Sigma_{\\text{raw}}$ is then computed:\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\nThe principal components are the eigenvectors of $\\Sigma_{\\text{raw}}$. We perform an eigendecomposition of this matrix:\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\nwhere $V$ is the matrix of orthonormal eigenvectors and $\\Lambda$ is the diagonal matrix of corresponding eigenvalues. The eigenvalues are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$. The first principal component is the eigenvector $v_1$ associated with the largest eigenvalue $\\lambda_1$. For this problem, we denote this eigenvector as $v_{\\text{raw}}$ and the eigenvalue as $\\lambda_{\\text{raw}}$.\n\n**3. PCA on Standardized Data (Correlation-Based PCA)**\n\nTo remove the effect of arbitrary scaling, we standardize the data. For each column $j$ of the original data matrix $X$, we compute its sample standard deviation, $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$.\n\nThe standardized data matrix $Z$ is constructed with entries:\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\nBy construction, each column of $Z$ has a sample mean of $0$ and a sample variance of $1$.\n\nPCA is then performed on this standardized data $Z$. The relevant matrix is the sample covariance matrix of $Z$, which we denote $\\Sigma_{\\text{std}}$:\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\nSince each column of $Z$ has unit variance, the diagonal elements of $\\Sigma_{\\text{std}}$ are all $1$, and the off-diagonal elements $(i, j)$ are the sample correlation coefficients between the original variables $x_i$ and $x_j$. Thus, $\\Sigma_{\\text{std}}$ is the sample correlation matrix of $X$.\n\nWe perform an eigendecomposition of $\\Sigma_{\\text{std}}$ to find its largest eigenvalue, $\\lambda_{\\text{std}}$, and the corresponding eigenvector, $v_{\\text{std}}$.\n\n**4. Diagnostic Metrics**\n\nTo quantify the distortion caused by failing to standardize, we compute two metrics:\n\n- **Angle between Principal Components**: The principal component directions $v_{\\text{raw}}$ and $v_{\\text{std}}$ are unit vectors in $\\mathbb{R}^{n_k}$. The angle between them measures how much the direction of maximum variance shifts. Since eigenvectors are defined only up to a sign (i.e., if $v$ is an eigenvector, so is $-v$), we compute the acute angle between the lines they span:\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  A value of $\\theta=0$ indicates perfect alignment, while a large angle (approaching $\\pi/2$) indicates severe misalignment.\n\n- **Difference in Explained Variance Share**: The fraction of total variance explained by the first principal component is given by its eigenvalue divided by the sum of all eigenvalues. The sum of eigenvalues is equal to the trace of the matrix, $\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$, which represents the total variance in the data. We compute the absolute difference in the explained variance share:\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  Note that for standardized data, $\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$, the number of variables. A large $\\Delta$ indicates that the two methods give a very different assessment of the importance of the first component.\n\nThe procedure will be executed for each test case using the specified parameters and a fixed random seed for reproducibility. The results are expected to show minimal distortion for Case $1$ (similar scales) and significant distortion for Cases $2$ and $3$ (disparate scales), validating the necessity of standardization in practice.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}, {"introduction": "In the era of big data, datasets are often too large to fit into a computer's memory, making the direct computation of the covariance matrix $S = \\frac{1}{n-1} X^\\top X$ infeasible. This advanced practice [@problem_id:2421742] introduces a powerful, memory-efficient solution: the power iteration method. You will implement this iterative algorithm to find the first principal component of a large-scale dataset in a \"streaming\" fashion, demonstrating how to apply PCA even when faced with significant computational constraints.", "problem": "You are given a data-generation model for asset returns motivated by a single latent market factor in computational economics and finance. Let $R \\in \\mathbb{R}^{n \\times d}$ denote the matrix of asset returns, where $n$ is the number of time periods and $d$ is the number of assets. Rows correspond to time and columns to assets. The data are generated as\n$$\nR_{t,\\cdot} \\;=\\; z_t \\, a^\\top \\;+\\; \\varepsilon_{t,\\cdot}, \\quad \\text{for } t \\in \\{1,\\dots,n\\},\n$$\nwhere $z_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed scalar factor realizations, $a \\in \\mathbb{R}^d$ is a fixed loading vector, and $\\varepsilon_{t,\\cdot} \\in \\mathbb{R}^d$ has independent and identically distributed entries with distribution $\\mathcal{N}(0,\\sigma^2)$, independent of $\\{z_t\\}_{t=1}^n$. The sample covariance matrix of the column-centered data is\n$$\nS \\;=\\; \\frac{1}{n-1} \\, X^\\top X,\n$$\nwhere $X$ is formed by column-centering $R$ using the empirical mean of each column.\n\nThe objective is to compute only the first principal component loading vector, that is, a unit vector $v^\\ast \\in \\mathbb{R}^d$ that solves\n$$\nS v^\\ast \\;=\\; \\lambda_1 \\, v^\\ast, \\quad \\|v^\\ast\\|_2 \\;=\\; 1,\n$$\nwith $\\lambda_1$ being the largest eigenvalue of $S$. You must assume that explicitly forming $S$ is infeasible. You may access $R$ only by streaming its rows in chunks (sequential blocks of rows), and you may make multiple passes over the data. You must center the columns using empirical means computed from the data. Your program must not construct $X^\\top X$ and must use memory that scales linearly in $d$.\n\nFor testing, your program must internally generate $R$ for three independent cases using the model above and compute, for each case, two quantities:\n1. The absolute cosine similarity between the computed unit vector $\\hat v$ and the unit-norm version of the true loading vector $\\bar a = a / \\|a\\|_2$, given by\n$$\nc \\;=\\; \\left| \\hat v^\\top \\bar a \\right|.\n$$\n2. The explained variance ratio of the first component, defined as\n$$\n\\rho \\;=\\; \\frac{\\hat \\lambda_1}{\\operatorname{tr}(S)},\n$$\nwhere $\\hat \\lambda_1$ is estimated by the Rayleigh quotient\n$$\n\\hat \\lambda_1 \\;=\\; \\hat v^\\top S \\hat v \\;=\\; \\frac{1}{n-1} \\, \\| X \\hat v \\|_2^2,\n$$\nand $\\operatorname{tr}(S)$ is the trace of $S$, equivalently\n$$\n\\operatorname{tr}(S) \\;=\\; \\frac{1}{n-1} \\, \\| X \\|_F^2.\n$$\nAll norms are the standard Euclidean or Frobenius norms as indicated.\n\nYour program must simulate the data in a streaming fashion (without storing the full matrix) using the exact parameter sets below. In each case, the vector $a$ is defined deterministically as specified and must be normalized to unit norm before computing $c$.\n\nCase A (general well-conditioned):\n- $n = 5000$, $d = 50$, $\\sigma = 0.5$, random seed $s = 42$, chunk size $m = 250$.\n- Loadings $a \\in \\mathbb{R}^{50}$ defined by $a_i = i$ for $i \\in \\{1,\\dots,50\\}$.\n\nCase B (high-dimensional with $d \\gt n$):\n- $n = 120$, $d = 600$, $\\sigma = 0.6$, random seed $s = 7$, chunk size $m = 64$.\n- Loadings $a \\in \\mathbb{R}^{600}$ defined by $a_i = (-1)^i \\left(0.5 + \\frac{i}{d}\\right)$ for $i \\in \\{1,\\dots,600\\}$.\n\nCase C (small eigengap, slower convergence):\n- $n = 4000$, $d = 100$, $\\sigma = 0.97$, random seed $s = 123$, chunk size $m = 200$.\n- Loadings $a \\in \\mathbb{R}^{100}$ defined by $a_i = \\sin\\!\\left(\\frac{2\\pi i}{d}\\right) + 0.1 \\frac{i}{d}$ for $i \\in \\{1,\\dots,100\\}$, where $\\pi$ is the circle constant.\n\nIn all cases, the random seed $s$ controls the generation of $\\{z_t\\}$ and the noise $\\{\\varepsilon_{t,\\cdot}\\}$ and must be used so that every pass over the data reproduces the same sequence. You may use at most a fixed number of iterations (choose suitable values) and a numerical tolerance to terminate iteration early if the direction stabilizes. Angles, if any are used internally, need not be reported. No physical units apply.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, with each inner list containing the two floats $[c,\\rho]$ for Cases A, B, and C in that order, rounded to six decimal places. For example, the output format must be exactly\n$$\n\\big[ [c_A,\\rho_A], [c_B,\\rho_B], [c_C,\\rho_C] \\big],\n$$\nprinted as a single line, such as\n$$\n\\texttt{[[0.999000,0.850000],[0.990000,0.300000],[0.950000,0.600000]]}.\n$$", "solution": "The problem presented is a well-posed and scientifically grounded task in computational finance. It asks for the computation of the first principal component of a sample covariance matrix under memory and data access constraints, a standard problem in large-scale statistical analysis. All parameters and conditions are specified with mathematical precision. Therefore, the problem is valid, and I will proceed with its solution.\n\nThe core of the problem is to find the unit-norm eigenvector $v^\\ast$ corresponding to the largest eigenvalue $\\lambda_1$ of the sample covariance matrix $S = \\frac{1}{n-1} X^\\top X$. Here, $X \\in \\mathbb{R}^{n \\times d}$ is the data matrix of asset returns, centered by subtracting the column-wise empirical means. The crucial constraints are that neither the full data matrix $R \\in \\mathbb{R}^{n \\times d}$ nor the matrix $X^\\top X \\in \\mathbb{R}^{d \\times d}$ can be explicitly formed or stored in memory. Data is only accessible by streaming rows in sequential chunks.\n\nThis setting necessitates an iterative algorithm. The power iteration method is the canonical algorithm for finding the eigenvector associated with the largest-magnitude eigenvalue of a matrix. The method begins with an initial random vector $v_0$ and iteratively applies the matrix $S$ to produce a sequence of vectors that converges to the dominant eigenvector:\n$$\nv_{k+1} = \\frac{S v_k}{\\|S v_k\\|_2}\n$$\nThe constant factor $\\frac{1}{n-1}$ in the definition of $S$ can be omitted during the iteration, as the vector is renormalized at each step. Thus, the core computation is the matrix-vector product $w_k = X^\\top X v_k$.\n\nTo implement this under streaming constraints, we must perform the computation without explicitly forming $X$ or $X^\\top X$.\nFirst, the data must be centered. The centered data matrix is $X = R - \\mathbf{1}\\mu^\\top$, where $\\mathbf{1}$ is an $n \\times 1$ vector of ones and $\\mu \\in \\mathbb{R}^d$ is the vector of column means. The means are computed as $\\mu = \\frac{1}{n} \\sum_{t=1}^n R_{t,\\cdot}$, which requires a single, full pass over the data matrix $R$. We can compute and store the vector $\\mu$, as its memory footprint, $O(d)$, is permissible.\n\nWith $\\mu$ known, the product $w_k = X^\\top (X v_k)$ can be computed in a single pass over the data for each iteration $k$. The product can be expressed as a sum over the rows $x_{t,\\cdot}$ of $X$:\n$$\nw_k = X^\\top (X v_k) = \\sum_{t=1}^{n} x_{t,\\cdot}^\\top (x_{t,\\cdot} v_k)\n$$\nDuring the pass, for each row $R_{t,\\cdot}$ from the data stream, we first compute the centered row $x_{t,\\cdot} = R_{t,\\cdot} - \\mu$. Then, we calculate the scalar projection $p_t = x_{t,\\cdot} v_k$. This scalar is used to scale the vector $x_{t,\\cdot}^\\top$, and the result is added to an accumulator vector, which, at the end of the pass, holds the desired vector $w_k$. The new estimate for the eigenvector is then $v_{k+1} = w_k / \\|w_k\\|_2$. This iterative process is repeated until the change in the vector $v_k$ becomes negligible, for instance, when $1 - |v_k^\\top v_{k-1}| < \\epsilon$ for some small tolerance $\\epsilon > 0$.\n\nThe data generation process for $R$ must be reproducible for each pass. This is achieved by implementing a generator function that re-initializes its random number generator with a specific seed $s$ at the beginning of each pass.\n\nAfter the power iteration converges to a stable eigenvector $\\hat{v}$, the problem requires computing two metrics: the absolute cosine similarity $c = |\\hat{v}^\\top \\bar{a}|$ and the explained variance ratio $\\rho = \\hat{\\lambda}_1 / \\operatorname{tr}(S)$.\nThe true loading vector $a$ is given, and its unit-norm version $\\bar{a}$ is computed as $\\bar{a} = a / \\|a\\|_2$. The value $c$ follows directly.\n\nThe explained variance ratio $\\rho$ requires computing $\\hat{\\lambda}_1$ and $\\operatorname{tr}(S)$. These are given by:\n$$\n\\hat{\\lambda}_1 = \\hat{v}^\\top S \\hat{v} = \\frac{1}{n-1} \\|X\\hat{v}\\|_2^2\n$$\n$$\n\\operatorname{tr}(S) = \\frac{1}{n-1} \\operatorname{tr}(X^\\top X) = \\frac{1}{n-1} \\|X\\|_F^2\n$$\nThe ratio simplifies to $\\rho = \\frac{\\|X\\hat{v}\\|_2^2}{\\|X\\|_F^2}$. Both $\\|X\\hat{v}\\|_2^2 = \\sum_{t=1}^n (x_{t,\\cdot}\\hat{v})^2$ and the squared Frobenius norm $\\|X\\|_F^2 = \\sum_{t=1}^n \\sum_{j=1}^d X_{t,j}^2 = \\sum_{t=1}^n \\|x_{t,\\cdot}\\|_2^2$ can be computed efficiently in one final pass over the data. In this pass, we initialize two scalar accumulators to zero and, for each centered row $x_{t,\\cdot}$, we add $(x_{t,\\cdot}\\hat{v})^2$ to the first and $\\|x_{t,\\cdot}\\|_2^2$ to the second.\n\nThe complete algorithm is as follows:\n1.  **Mean Computation (Pass 1)**: Make one pass over the data stream to compute the mean vector $\\mu$.\n2.  **Power Iteration (Multiple Passes)**: Initialize a random unit vector $v$. For a fixed number of iterations or until convergence, perform a pass over the data to compute $w = X^\\top(Xv)$ and update $v \\leftarrow w/\\|w\\|_2$.\n3.  **Metrics Computation (Final Pass)**: After convergence to $\\hat{v}$, perform a final pass to compute $\\|X\\hat{v}\\|_2^2$ and $\\|X\\|_F^2$.\n4.  **Final Calculation**: Compute $c$ and $\\rho$ from the collected statistics and the true loading vector $a$.\n\nThis multi-pass, streaming approach respects all problem constraints, scaling with memory of $O(d)$ and processing the data in manageable chunks.", "answer": "```python\nimport numpy as np\n\ndef generate_R_chunks(n, d, a, sigma, seed, chunk_size):\n    \"\"\"\n    Generator for streaming the data matrix R in chunks.\n    A new np.random.RandomState is created to ensure reproducibility for each pass.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    current_pos = 0\n    while current_pos < n:\n        size = min(chunk_size, n - current_pos)\n        \n        # Generate z_t and epsilon_t for the whole chunk for efficiency\n        z = rng.normal(0, 1, size=(size, 1))\n        epsilon = rng.normal(0, sigma, size=(size, d))\n        \n        # R_chunk = z * a + epsilon\n        chunk = z * a.reshape(1, d) + epsilon\n        \n        yield chunk\n        current_pos += size\n\ndef solve_pca_streaming(n, d, a, sigma, seed, chunk_size, max_iter=100, tol=1e-9):\n    \"\"\"\n    Computes the first PC and related metrics using a streaming algorithm.\n    \"\"\"\n    # Pass 1: Compute column means\n    mean_vec = np.zeros(d)\n    data_stream_mean = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_mean:\n        mean_vec += np.sum(chunk, axis=0)\n    mu = mean_vec / n\n\n    # Power Iteration to find the first principal component vector\n    v = np.random.rand(d)\n    v /= np.linalg.norm(v)\n\n    for _ in range(max_iter):\n        v_old = v\n        w = np.zeros(d)\n\n        # New pass for this iteration to compute w = X^T @ (X @ v)\n        data_stream_iter = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n        for chunk in data_stream_iter:\n            X_chunk = chunk - mu\n            p_chunk = X_chunk @ v\n            w += X_chunk.T @ p_chunk\n\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0:\n            # Should not happen in this problem\n            break\n        \n        v = w / norm_w\n\n        # Check for convergence (sign-invariant)\n        if 1 - abs(v @ v_old) < tol:\n            break\n    \n    v_hat = v\n\n    # Final pass: Compute metrics (explained variance ratio)\n    sq_norm_Xv = 0.0\n    sq_frob_norm_X = 0.0\n    data_stream_metrics = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_metrics:\n        X_chunk = chunk - mu\n        sq_norm_Xv += np.sum((X_chunk @ v_hat)**2)\n        sq_frob_norm_X += np.sum(X_chunk**2) \n\n    # Explained variance ratio\n    rho = sq_norm_Xv / sq_frob_norm_X if sq_frob_norm_X > 0 else 0.0\n\n    # Absolute cosine similarity\n    a_norm = np.linalg.norm(a)\n    if a_norm == 0:\n        a_bar = a # Or handle as an error\n    else:\n        a_bar = a / a_norm\n    c = abs(v_hat @ a_bar)\n\n    return c, rho\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define test cases as per the problem statement\n    test_cases = [\n        # Case A: general well-conditioned\n        {'n': 5000, 'd': 50, 'sigma': 0.5, 's': 42, 'm': 250, \n         'a_def': lambda i, d_val: i},\n        # Case B: high-dimensional with d > n\n        {'n': 120, 'd': 600, 'sigma': 0.6, 's': 7, 'm': 64, \n         'a_def': lambda i, d_val: ((-1)**i) * (0.5 + i / d_val)},\n        # Case C: small eigengap, slower convergence\n        {'n': 4000, 'd': 100, 'sigma': 0.97, 's': 123, 'm': 200, \n         'a_def': lambda i, d_val: np.sin(2 * np.pi * i / d_val) + 0.1 * (i / d_val)}\n    ]\n    \n    results = []\n    # Use a fixed seed for initializing the random vector v in power iteration\n    # to ensure the entire program's output is deterministic.\n    np.random.seed(0)\n\n    for case in test_cases:\n        d = case['d']\n        indices = np.arange(1, d + 1)\n        a = case['a_def'](indices, d)\n        \n        c, rho = solve_pca_streaming(\n            n=case['n'], d=d, a=a, \n            sigma=case['sigma'], seed=case['s'], chunk_size=case['m']\n        )\n        results.append((c, rho))\n\n    # Format the output exactly as required\n    formatted_results = [f\"[{c:.6f},{rho:.6f}]\" for c, rho in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421742"}]}