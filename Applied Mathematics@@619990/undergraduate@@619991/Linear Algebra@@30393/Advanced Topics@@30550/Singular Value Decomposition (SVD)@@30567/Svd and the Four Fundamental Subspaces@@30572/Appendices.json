{"hands_on_practices": [{"introduction": "The Singular Value Decomposition (SVD) provides a powerful way to understand a matrix's structure, starting with its rank. This first practice focuses on the most fundamental application of rank: determining the dimensions of the four fundamental subspaces [@problem_id:1391198]. By working through this exercise, you will solidify your understanding of the core dimensional relationships that connect a matrix's size and rank to its associated subspaces.", "problem": "Consider a matrix $A$ with 2 rows and 3 columns, whose elements are real numbers. The rank of this matrix, which is the dimension of the vector space spanned by its columns, is known to be 1.\n\nDetermine the dimensions of the four fundamental subspaces associated with this matrix $A$:\n1. The column space, $C(A)$\n2. The null space, $N(A)$\n3. The row space, $C(A^T)$\n4. The left null space, $N(A^T)$\n\nPresent your answer as a row matrix containing the four dimensions in the specific order listed above.", "solution": "Let $A$ be an $m \\times n$ matrix with rank $r$. The four fundamental subspace dimensions follow from standard linear algebra facts:\n\n- The dimension of the column space is the rank: $\\dim C(A) = r$.\n- By the rank-nullity theorem applied to $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$, the nullity is $\\dim N(A) = n - r$.\n- The row space dimension equals the column rank, so $\\dim C(A^{T}) = r$.\n- Applying the rank-nullity theorem to $A^{T} : \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$ gives the left null space dimension $\\dim N(A^{T}) = m - r$.\n\nHere, $m = 2$, $n = 3$, and $r = 1$. Substituting these values yields:\n$$\n\\dim C(A) = 1, \\quad \\dim N(A) = 3 - 1 = 2, \\quad \\dim C(A^{T}) = 1, \\quad \\dim N(A^{T}) = 2 - 1 = 1.\n$$\nThus the ordered tuple of dimensions is $\\begin{pmatrix}1 & 2 & 1 & 1\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}1 & 2 & 1 & 1\\end{pmatrix}}$$", "id": "1391198"}, {"introduction": "Moving beyond simply calculating dimensions, the SVD provides a concrete orthonormal basis for each fundamental subspace. This practice explores a common scenario where a matrix has more rows than columns and full column rank, leading to a predictable geometric consequence [@problem_id:1391150]. You will see how the structure of the SVD provides a clear, intuitive explanation for why one of the fundamental subspaces becomes trivial.", "problem": "In the field of system identification, an engineer models a dynamic system using a set of linear equations. The system's response to various inputs is captured in a data matrix $A$ of size $m \\times n$. Each of the $m$ rows corresponds to a measurement at a different point in time, and each of the $n$ columns corresponds to a basis function used in the model. Due to a desire for a high-fidelity model and the availability of extensive sensor data, the engineer collects far more measurements than the number of basis functions, such that $m > n$. The chosen basis functions are known to be linearly independent, which means the columns of matrix $A$ are linearly independent.\n\nThe Singular Value Decomposition (SVD) of this matrix $A$ is expressed as $A = U \\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ matrix containing the singular values. The rank of $A$ is denoted by $r$. The four fundamental subspaces associated with matrix $A$ are the Column Space $C(A)$, the Null Space $N(A)$, the Row Space $C(A^T)$, and the Left Null Space $N(A^T)$.\n\nGiven this specific configuration ($m \\times n$ matrix with $m > n$ and full column rank), one of the four fundamental subspaces is guaranteed to be the trivial subspace, i.e., the subspace containing only the zero vector. Which of the following statements correctly identifies this trivial subspace and accurately explains why, based on the structure provided by the SVD?\n\nA. The Column Space, $C(A)$. The SVD shows this because its basis dimension is the rank $r$. Since $r=n$ and $n<m$, the column space is a proper subspace of $\\mathbb{R}^m$, which by definition makes it trivial.\n\nB. The Left Null Space, $N(A^T)$. The SVD shows this because its basis is given by the columns of $U$ indexed from $r+1$ to $m$. Since the basis functions in $A$ are independent, any error must be zero, forcing this subspace to be trivial.\n\nC. The Null Space, $N(A)$. The SVD shows this because its basis is given by the columns of $V$ indexed from $r+1$ to $n$. Since the rank is $r=n$, there are no indices in the range from $n+1$ to $n$, leading to an empty basis which spans the trivial subspace $\\{\\mathbf{0}\\}$.\n\nD. The Row Space, $C(A^T)$. The SVD shows this because its basis is given by the first $r$ columns of $V$. Despite the rank being full ($r=n$), these vectors might not be linearly independent, resulting in a collapse to the trivial subspace.\n\nE. None of the subspaces are guaranteed to be trivial. Since the matrix $A$ is composed of data from a real system, all four subspaces must have non-zero dimensions to account for signal, noise, and model parameters.", "solution": "We are given an $m \\times n$ matrix $A$ with $m>n$ and linearly independent columns. Therefore $A$ has full column rank, so the rank is $r=n$.\n\nConsider the Singular Value Decomposition $A=U\\Sigma V^{T}$, where $U$ is $m \\times m$ orthogonal, $V$ is $n \\times n$ orthogonal, and $\\Sigma$ is $m \\times n$ with diagonal entries $\\sigma_{1},\\dots,\\sigma_{r}$ positive and the remaining singular values zero. Since $r=n$, all $n$ singular values are strictly positive.\n\nThe four fundamental subspaces have standard characterizations via the SVD:\n\n1) Column space $C(A)$: This is $C(A)=\\operatorname{span}\\{u_{1},\\dots,u_{r}\\}=\\operatorname{span}\\{u_{1},\\dots,u_{n}\\}$. Its dimension is $\\dim C(A)=r=n>0$. Since $n<m$, $C(A)$ is a proper subspace of $\\mathbb{R}^{m}$ but not the trivial subspace.\n\n2) Row space $C(A^{T})$: This is $C(A^{T})=\\operatorname{span}\\{v_{1},\\dots,v_{r}\\}=\\operatorname{span}\\{v_{1},\\dots,v_{n}\\}$. The vectors $v_{1},\\dots,v_{n}$ are orthonormal, hence linearly independent, so $\\dim C(A^{T})=r=n>0$. Thus it is not trivial.\n\n3) Null space $N(A)$: By definition, $x\\in N(A)$ if and only if $Ax=0$. Using the SVD, $Ax=0$ is equivalent to $U\\Sigma V^{T}x=0$. Left-multiplying by $U^{T}$ and setting $y=V^{T}x$ gives $\\Sigma y=0$. Since $r=n$ and $\\Sigma$ has strictly positive diagonal entries $\\sigma_{1},\\dots,\\sigma_{n}$, the equation $\\Sigma y=0$ implies $y=0$. Therefore $x=Vy=0$, so $N(A)=\\{\\mathbf{0}\\}$. Equivalently, the SVD subspace description gives $N(A)=\\operatorname{span}\\{v_{r+1},\\dots,v_{n}\\}$, which is the span of an empty set when $r=n$, hence the trivial subspace.\n\n4) Left null space $N(A^{T})$: By definition, $y\\in N(A^{T})$ if and only if $A^{T}y=0$. Using the SVD, $A^{T}y=0$ implies $V\\Sigma^{T}U^{T}y=0$. Left-multiplying by $V^{T}$ and setting $z=U^{T}y$ gives $\\Sigma^{T}z=0$. Since $\\Sigma^{T}$ is $n \\times m$ with full row rank $n$ and $m>n$, the solution space has dimension $m-n>0$, which is spanned by $\\{u_{n+1},\\dots,u_{m}\\}$. Hence $N(A^{T})$ is nontrivial.\n\nFrom these SVD-based characterizations, the only fundamental subspace that is guaranteed to be trivial for a tall full column-rank matrix ($m>n$, $r=n$) is the null space $N(A)$. This matches option C: $N(A)$ is trivial because its SVD basis would be the columns of $V$ with indices from $r+1$ to $n$, and when $r=n$ there are no such columns, so the span is $\\{\\mathbf{0}\\}$.\n\nTherefore, the correct choice is C, and all other options are incorrect for the reasons outlined above.", "answer": "$$\\boxed{C}$$", "id": "1391150"}, {"introduction": "This final practice serves as a capstone exercise, challenging you to use the SVD as a constructive tool for solving a more complex problem. You will use the SVD to extract the bases for the null spaces of two different matrices and then apply your linear algebra skills to find the one-dimensional subspace where they intersect [@problem_id:1391167]. This task models practical problems in fields like signal processing and control theory, bridging the gap between the theory of fundamental subspaces and their computational application.", "problem": "Consider two real matrices $A \\in \\mathbb{R}^{3 \\times 4}$ and $B \\in \\mathbb{R}^{3 \\times 4}$. Their Singular Value Decompositions (SVDs) are given by $A = U_A \\Sigma_A V_A^T$ and $B = U_B \\Sigma_B V_B^T$. The matrices $U_A$ and $U_B$ are $3 \\times 3$ orthogonal matrices. The remaining matrices in the SVDs are given as follows:\n\nFor matrix $A$:\n$$ \\Sigma_A = \\begin{pmatrix} 3 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}, \\quad V_A = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1 \\end{pmatrix} $$\n\nFor matrix $B$:\n$$ \\Sigma_B = \\begin{pmatrix} 4 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}, \\quad V_B = \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\\\ 1 & 0 & 0 & 0 \\end{pmatrix} $$\n\nIt is known that the intersection of the null spaces, $S = N(A) \\cap N(B)$, is a one-dimensional subspace of $\\mathbb{R}^4$. Determine the unique unit vector $\\mathbf{z}$ that forms an orthonormal basis for $S$, such that the first non-zero component of $\\mathbf{z}$ is positive. Express the vector $\\mathbf{z}$ as a column vector.", "solution": "The problem asks for a unit vector that forms a basis for the intersection of the null spaces of two matrices, $N(A) \\cap N(B)$, given their Singular Value Decompositions (SVDs).\n\n**Step 1: Determine the basis for the null space of A, N(A).**\nThe null space of a matrix $A$ with SVD $A = U_A \\Sigma_A V_A^T$ is spanned by the columns of $V_A$ that correspond to the zero singular values. The singular values of $A$ are the diagonal entries of $\\Sigma_A$.\nFor matrix $A \\in \\mathbb{R}^{3 \\times 4}$, the singular values are $\\sigma_1=3, \\sigma_2=1, \\sigma_3=0, \\sigma_4=0$. The zero singular values correspond to the 3rd and 4th columns of $V_A$. Let these columns be $\\mathbf{v}_{A,3}$ and $\\mathbf{v}_{A,4}$.\n$$ \\mathbf{v}_{A,3} = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix}, \\quad \\mathbf{v}_{A,4} = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} $$\nThus, an orthonormal basis for $N(A)$ is $\\{\\mathbf{v}_{A,3}, \\mathbf{v}_{A,4}\\}$. Any vector $\\mathbf{x} \\in N(A)$ can be written as a linear combination $\\mathbf{x} = c_1 \\mathbf{v}_{A,3} + c_2 \\mathbf{v}_{A,4}$ for some scalars $c_1, c_2$.\n\n**Step 2: Determine the basis for the null space of B, N(B).**\nSimilarly, for matrix $B \\in \\mathbb{R}^{3 \\times 4}$, the singular values from $\\Sigma_B$ are $\\sigma_1=4, \\sigma_2=2, \\sigma_3=0, \\sigma_4=0$. The null space of $B$ is spanned by the 3rd and 4th columns of $V_B$, which we denote as $\\mathbf{v}_{B,3}$ and $\\mathbf{v}_{B,4}$.\n$$ \\mathbf{v}_{B,3} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_{B,4} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThus, an orthonormal basis for $N(B)$ is $\\{\\mathbf{v}_{B,3}, \\mathbf{v}_{B,4}\\}$. Any vector $\\mathbf{x} \\in N(B)$ can be written as a linear combination $\\mathbf{x} = d_1 \\mathbf{v}_{B,3} + d_2 \\mathbf{v}_{B,4}$ for some scalars $d_1, d_2$.\n\n**Step 3: Find the intersection N(A) $\\cap$ N(B).**\nA vector $\\mathbf{x}$ lies in the intersection if it can be expressed as a linear combination of the basis vectors for both null spaces.\n$$ \\mathbf{x} = c_1 \\mathbf{v}_{A,3} + c_2 \\mathbf{v}_{A,4} = d_1 \\mathbf{v}_{B,3} + d_2 \\mathbf{v}_{B,4} $$\nThis gives the equation:\n$$ c_1 \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} + c_2 \\frac{1}{2}\\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = d_1 \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} + d_2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nWe can rewrite this as a homogeneous system of linear equations. Let $M$ be the matrix whose columns are the basis vectors of $N(A)$ and the negative basis vectors of $N(B)$. We are looking for the null space of $M$.\n$$ M = \\begin{pmatrix} \\mathbf{v}_{A,3} & \\mathbf{v}_{A,4} & -\\mathbf{v}_{B,3} & -\\mathbf{v}_{B,4} \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/2 & -1/\\sqrt{2} & 0 \\\\ 1/2 & -1/2 & 0 & -1 \\\\ -1/2 & -1/2 & 1/\\sqrt{2} & 0 \\\\ -1/2 & 1/2 & 0 & 0 \\end{pmatrix} $$\nWe need to find the null space of $M$ by solving $M \\begin{pmatrix} c_1, c_2, d_1, d_2 \\end{pmatrix}^T = \\mathbf{0}$. We use Gaussian elimination on $M$.\n$R_3 \\to R_3 + R_1$:\n$$ \\begin{pmatrix} 1/2 & 1/2 & -1/\\sqrt{2} & 0 \\\\ 1/2 & -1/2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ -1/2 & 1/2 & 0 & 0 \\end{pmatrix} $$\n$R_4 \\to R_4 + R_1$:\n$$ \\begin{pmatrix} 1/2 & 1/2 & -1/\\sqrt{2} & 0 \\\\ 1/2 & -1/2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 1 & -1/\\sqrt{2} & 0 \\end{pmatrix} $$\n$R_2 \\to R_2 - R_1$:\n$$ \\begin{pmatrix} 1/2 & 1/2 & -1/\\sqrt{2} & 0 \\\\ 0 & -1 & 1/\\sqrt{2} & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 1 & -1/\\sqrt{2} & 0 \\end{pmatrix} $$\n$R_4 \\to R_4 + R_2$:\n$$ \\begin{pmatrix} 1/2 & 1/2 & -1/\\sqrt{2} & 0 \\\\ 0 & -1 & 1/\\sqrt{2} & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\end{pmatrix} $$\nThis gives the system of equations:\n1. $\\frac{1}{2}c_1 + \\frac{1}{2}c_2 - \\frac{1}{\\sqrt{2}}d_1 = 0$\n2. $-c_2 + \\frac{1}{\\sqrt{2}}d_1 - d_2 = 0$\n3. $-d_2 = 0 \\implies d_2 = 0$\n\nSubstituting $d_2=0$ into the second equation gives $-c_2 + \\frac{1}{\\sqrt{2}}d_1 = 0$, so $c_2 = \\frac{d_1}{\\sqrt{2}}$.\nSubstituting this into the first equation: $\\frac{1}{2}c_1 + \\frac{1}{2}\\left(\\frac{d_1}{\\sqrt{2}}\\right) - \\frac{1}{\\sqrt{2}}d_1 = 0$.\n$\\frac{1}{2}c_1 - \\frac{d_1}{2\\sqrt{2}} = 0 \\implies c_1 = \\frac{d_1}{\\sqrt{2}}$.\nSo we have $c_1=c_2$ and $c_1=d_1/\\sqrt{2}$. Let's choose $d_1=\\sqrt{2}$ as our free parameter. This gives $c_1=1$, $c_2=1$, and $d_2=0$.\nThe vector in the intersection $\\mathbf{x}$ can be found using either set of coefficients. Using the basis for $N(B)$:\n$$ \\mathbf{x} = d_1 \\mathbf{v}_{B,3} + d_2 \\mathbf{v}_{B,4} = \\sqrt{2} \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} + 0 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix} $$\nThis vector forms a basis for the one-dimensional intersection space $N(A) \\cap N(B)$.\n\n**Step 4: Normalize the basis vector and ensure uniqueness.**\nThe problem asks for a unique unit vector $\\mathbf{z}$ where the first non-zero component is positive.\nThe basis vector we found is $\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\nThe norm of $\\mathbf{x}$ is $\\|\\mathbf{x}\\| = \\sqrt{1^2 + 0^2 + (-1)^2 + 0^2} = \\sqrt{2}$.\nThe unit vector is $\\mathbf{z} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix}$.\nThe first non-zero component is $1/\\sqrt{2}$, which is positive. This satisfies the condition.\n\nThe final answer is the vector $\\mathbf{z}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\\n0 \\\\\n-\\frac{1}{\\sqrt{2}} \\\\\n0\n\\end{pmatrix}\n}\n$$", "id": "1391167"}]}