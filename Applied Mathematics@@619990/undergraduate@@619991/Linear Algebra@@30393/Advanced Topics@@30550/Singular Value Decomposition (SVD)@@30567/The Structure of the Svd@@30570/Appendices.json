{"hands_on_practices": [{"introduction": "To build a solid understanding of the Singular Value Decomposition (SVD), it is often best to start with a simple and familiar case. This first practice problem challenges you to apply the definition of the SVD to the identity matrix, a cornerstone of linear algebra. By verifying the required properties of the matrices $U$, $\\Sigma$, and $V$, you will reinforce the fundamental rules of the decomposition and discover an important subtlety: the SVD of a matrix is not always unique [@problem_id:1399080].", "problem": "The Singular Value Decomposition (SVD) of a real $m \\times n$ matrix $A$ is a factorization of the form $A = U \\Sigma V^T$, where:\n1.  $U$ is an $m \\times m$ orthogonal matrix.\n2.  $\\Sigma$ is an $m \\times n$ diagonal matrix with non-negative real numbers on the diagonal, sorted in descending order ($\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$). These are the singular values of $A$.\n3.  $V$ is an $n \\times n$ orthogonal matrix, and $V^T$ is its transpose.\n\nConsider the $3 \\times 3$ identity matrix, $I_3$. Which of the following triples $(U, \\Sigma, V)$ represent a valid SVD for $I_3$? Select all valid options.\n\nA. $U = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, V = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$\n\nB. $U = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, V = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}$\n\nC. $U = \\begin{pmatrix} 1  1  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, V = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  0  1 \\end{pmatrix}$\n\nD. $U = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} -1  0  0 \\\\ 0  -1  0 \\\\ 0  0  1 \\end{pmatrix}, V = \\begin{pmatrix} -1  0  0 \\\\ 0  -1  0 \\\\ 0  0  1 \\end{pmatrix}$", "solution": "By definition, an SVD of a real matrix $A$ is a factorization $A = U \\Sigma V^{T}$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries (the singular values) sorted in descending order.\n\nFor $A = I_3$, we have $A^T A = I_3$, whose eigenvalues are all $1$. Therefore, the singular values of $I_3$ are all $1$, so any valid SVD must have\n$$\n\\Sigma = I_3.\n$$\nHence\n$$\nI_3 = U \\Sigma V^T = U I_3 V^T = U V^T.\n$$\nThis implies\n$$\nU V^T = I_3 \\quad \\Longleftrightarrow \\quad U = V,\n$$\nwith the additional requirement that $U$ and $V$ are orthogonal, and that $\\Sigma$ has nonnegative diagonal entries (which here must be $1,1,1$).\n\nNow check each option:\n\nA. $U = I_3$, $\\Sigma = I_3$, $V = I_3$. Both $U$ and $V$ are orthogonal, $\\Sigma$ has nonnegative diagonal entries in descending order, and $U \\Sigma V^T = I_3$. Valid.\n\nB. $U = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}$, $\\Sigma = I_3$, $V$ equal to the same orthogonal matrix. The columns of $U$ form an orthonormal set, so $U^T U = I_3$; similarly for $V$. With $\\Sigma = I_3$,\n$$\nU \\Sigma V^T = U I_3 V^T = U V^T = U U^T = I_3.\n$$\n$\\Sigma$ has nonnegative and sorted diagonal entries. Valid.\n\nC. $U = \\begin{pmatrix} 1  1  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$. This $U$ is not orthogonal since its columns are not orthonormal (for instance, the second column has norm $\\sqrt{2}$), so $U^T U \\ne I_3$. Therefore this is not a valid SVD, regardless of $V$.\n\nD. $U = I_3$ and $V = \\operatorname{diag}(-1,-1,1)$ are orthogonal, and $U \\Sigma V^T = I_3$ holds numerically because $I_3 \\cdot \\operatorname{diag}(-1,-1,1) \\cdot \\operatorname{diag}(-1,-1,1)^T = I_3$. However, $\\Sigma = \\operatorname{diag}(-1,-1,1)$ has negative diagonal entries, violating the requirement that singular values be nonnegative. Hence this is not a valid SVD.\n\nTherefore, the valid options are A and B.", "answer": "$$\\boxed{AB}$$", "id": "1399080"}, {"introduction": "Having explored the SVD of the identity matrix, we now turn to another fundamental structure: the rank-one matrix. Such matrices represent the essential building blocks in the SVD's expansion of any matrix. This exercise provides a hands-on opportunity to construct the SVD for a matrix defined as an outer product of two vectors, revealing the direct link between a matrix's rank and its singular value spectrum [@problem_id:1399063].", "problem": "Consider a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ defined by the outer product $A = \\vec{u}\\vec{v}^T$, where the column vectors $\\vec{u}$ and $\\vec{v}$ are given by\n$$\n\\vec{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} \\quad \\text{and} \\quad \\vec{v} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}.\n$$\nA Singular Value Decomposition (SVD) of $A$ is a factorization of the form $A = U\\Sigma V^T$, where $U$ is a $3 \\times 3$ orthogonal matrix, $V$ is a $2 \\times 2$ orthogonal matrix, and $\\Sigma$ is a $3 \\times 2$ rectangular diagonal matrix with non-negative real numbers (the singular values) on the diagonal, arranged in descending order.\n\nLet $\\vec{u}_1$ be the first column of $U$ and $\\vec{v}_1$ be the first column of $V$. Which of the following options correctly identifies the matrix $\\Sigma$ and the vectors $\\vec{u}_1$ and $\\vec{v}_1$?\n\nA. $\\Sigma = \\begin{pmatrix} 15  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$\n\nB. $\\Sigma = \\begin{pmatrix} 9  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$\n\nC. $\\Sigma = \\begin{pmatrix} 15  0  0 \\\\ 0  0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\\\ 0 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\end{pmatrix}$\n\nD. $\\Sigma = \\begin{pmatrix} 225  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$", "solution": "We are given $A \\in \\mathbb{R}^{3 \\times 2}$ defined by $A=\\vec{u}\\vec{v}^T$ with $\\vec{u}=\\begin{pmatrix}1\\\\2\\\\2\\end{pmatrix}$ and $\\vec{v}=\\begin{pmatrix}4\\\\3\\end{pmatrix}$. Since $A$ is an outer product of two nonzero vectors, it has rank $1$. For a rank-$1$ matrix of the form $\\vec{u}\\vec{v}^T$, the singular value decomposition has exactly one nonzero singular value.\n\nA standard derivation uses\n$$\nA A^T = (\\vec{u}\\vec{v}^T)(\\vec{u}\\vec{v}^T)^T = \\vec{u}\\vec{v}^T\\vec{v}\\vec{u}^T = (\\vec{v}^T\\vec{v})\\,\\vec{u}\\vec{u}^T.\n$$\nThe matrix $\\vec{u}\\vec{u}^T$ has one nonzero eigenvalue $\\vec{u}^T\\vec{u}$ with eigenvector $\\vec{u}$. Therefore $A A^T$ has one nonzero eigenvalue\n$$\n\\lambda_1 = (\\vec{v}^T\\vec{v})(\\vec{u}^T\\vec{u}) = \\|\\vec{v}\\|^2\\,\\|\\vec{u}\\|^2,\n$$\nand the corresponding unit eigenvector is $\\vec{u}/\\|\\vec{u}\\|$. The unique nonzero singular value is $\\sigma_1 = \\sqrt{\\lambda_1} = \\|\\vec{u}\\|\\,\\|\\vec{v}\\|$, and the first left singular vector is\n$$\n\\vec{u}_1 = \\frac{\\vec{u}}{\\|\\vec{u}\\|}.\n$$\nSimilarly, from $A^T A = (\\vec{u}^T\\vec{u})\\,\\vec{v}\\vec{v}^T$, the first right singular vector is\n$$\n\\vec{v}_1 = \\frac{\\vec{v}}{\\|\\vec{v}\\|}.\n$$\n\nCompute the norms:\n$$\n\\|\\vec{u}\\| = \\sqrt{1^2+2^2+2^2} = \\sqrt{9} = 3, \\quad \\|\\vec{v}\\| = \\sqrt{4^2+3^2} = \\sqrt{25} = 5.\n$$\nHence the unique nonzero singular value is\n$$\n\\sigma_1 = \\|\\vec{u}\\|\\,\\|\\vec{v}\\| = 3 \\cdot 5 = 15.\n$$\nThe corresponding singular vectors are\n$$\n\\vec{u}_1 = \\frac{1}{3}\\begin{pmatrix}1\\\\2\\\\2\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\\\frac{2}{3}\\end{pmatrix}, \n\\quad\n\\vec{v}_1 = \\frac{1}{5}\\begin{pmatrix}4\\\\3\\end{pmatrix} = \\begin{pmatrix}\\frac{4}{5}\\\\\\frac{3}{5}\\end{pmatrix}.\n$$\nTherefore, the SVD has $\\Sigma$ as a $3 \\times 2$ diagonal rectangular matrix with the first diagonal entry equal to $15$ and the rest zero:\n$$\n\\Sigma = \\begin{pmatrix} 15  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nComparing with the given options, this matches option A.", "answer": "$$\\boxed{A}$$", "id": "1399063"}, {"introduction": "Now we leverage our understanding of SVD's structure to solve a classic problem in matrix theory with profound practical implications. This exercise asks you to find the smallest possible modification that will render an invertible matrix singular, a question central to numerical stability and data analysis. By using the components of the SVD, you will see how the smallest singular value and its corresponding singular vectors hold the key to this powerful result [@problem_id:1399073].", "problem": "Let $A$ be a real, invertible $n \\times n$ matrix. The Singular Value Decomposition (SVD) of $A$ is given by $A = U\\Sigma V^T$, where $U$ and $V$ are $n \\times n$ orthogonal matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are the left singular vectors, denoted $\\{u_1, u_2, \\dots, u_n\\}$, and the columns of $V$ are the right singular vectors, denoted $\\{v_1, v_2, \\dots, v_n\\}$. The diagonal entries of $\\Sigma$ are the singular values of $A$, ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n  0$.\n\nConsider a rank-one matrix $E$ that, when added to $A$, makes the resulting matrix $A+E$ singular (i.e., not invertible). Your task is to find the specific rank-one matrix $E$ for which the Frobenius norm, $\\|E\\|_F$, is minimized. The Frobenius norm of a matrix $M$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n M_{ij}^2}$.\n\nExpress the matrix $E$ that satisfies these conditions in terms of the singular values and singular vectors of $A$.", "solution": "Let $A = U \\Sigma V^T$ be the SVD of $A$, where $\\Sigma = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_n)$ with $\\sigma_1 \\ge \\cdots \\ge \\sigma_n > 0$, and $U = [u_1\\ \\cdots\\ u_n]$, $V = [v_1\\ \\cdots\\ v_n]$ are orthogonal.\n\nWe seek a rank-one matrix $E$ such that $A+E$ is singular and $\\|E\\|_F$ is minimized. Use the orthogonal invariance of the Frobenius norm and rank: for $\\tilde{E} \\coloneqq U^T E V$, one has $\\|E\\|_F = \\|\\tilde{E}\\|_F$ and\n$$\nA + E \\text{ is singular } \\Longleftrightarrow \\Sigma + \\tilde{E} \\text{ is singular},\n$$\nbecause $A+E = U(\\Sigma + \\tilde{E})V^T$ and $U,V$ are orthogonal.\n\nTo produce singularity with a rank-one perturbation of smallest Frobenius norm, set\n$$\n\\tilde{E} = - \\sigma_n e_n e_n^T,\n$$\nwhere $e_n$ is the $n$-th standard basis vector. Then $\\Sigma + \\tilde{E} = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_{n-1},0)$ is singular, and $\\tilde{E}$ is rank one with\n$$\n\\|\\tilde{E}\\|_F = \\sigma_n\\|e_n e_n^T\\|_F = \\sigma_n.\n$$\nMapping back gives\n$$\nE = U \\tilde{E} V^T = -\\sigma_n U e_n e_n^T V^T = -\\sigma_n u_n v_n^T.\n$$\nThis $E$ indeed makes $A+E$ singular because\n$$\n(A+E) v_n = A v_n + E v_n = \\sigma_n u_n - \\sigma_n u_n = 0.\n$$\n\nTo prove minimality, let $s_1 \\ge \\cdots \\ge s_n \\ge 0$ be the singular values of $A+E$. Since $A+E$ is singular, $s_n = 0$. By the Hoffman–Wielandt inequality,\n$$\n\\sum_{i=1}^{n} (\\sigma_i - s_i)^2 \\le \\|E\\|_F^2.\n$$\nThus\n$$\n\\|E\\|_F^2 \\ge (\\sigma_n - 0)^2 = \\sigma_n^2,\n$$\nso $\\|E\\|_F \\ge \\sigma_n$. Our construction achieves $\\|E\\|_F = \\sigma_n$, hence it is optimal. Therefore, a minimizing rank-one perturbation is\n$$\nE = -\\sigma_n u_n v_n^T.\n$$\nIf $\\sigma_n$ has multiplicity greater than one, any choice of unit left and right singular vectors from the corresponding singular subspaces yields an optimal $E$ of the same form.", "answer": "$$\\boxed{-\\sigma_{n} u_{n} v_{n}^{T}}$$", "id": "1399073"}]}