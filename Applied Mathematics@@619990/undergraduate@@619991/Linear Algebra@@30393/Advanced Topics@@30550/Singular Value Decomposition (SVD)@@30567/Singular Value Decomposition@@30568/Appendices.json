{"hands_on_practices": [{"introduction": "Mastering any new concept in linear algebra starts with the fundamentals of calculation. This practice problem is designed to build your foundational skills in Singular Value Decomposition (SVD) by guiding you through the direct computation of singular values for a simple $2 \\times 2$ matrix. By working through the steps, you'll solidify your understanding of the relationship between a matrix $A$, its transpose product $A^T A$, and the resulting singular values [@problem_id:1071366].", "problem": "Compute the singular values of the $2 \\times 2$ matrix \n\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n$$\n\nExpress the singular values in simplified radical form and list them in decreasing order.", "solution": "1. The singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues of $A^T A$.\n\n2. Compute \n$$A^T A = \\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\n=\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}.$$\n\n3. The characteristic polynomial of $A^T A$ is\n$$\\det\\bigl(\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}-\\lambda I\\bigr)\n=(2-\\lambda)(1-\\lambda)-1\n=\\lambda^2-3\\lambda+1.$$\n\n4. Solve $\\lambda^2-3\\lambda+1=0$:\n$$\\lambda=\\frac{3\\pm\\sqrt{9-4}}{2}=\\frac{3\\pm\\sqrt5}{2}.$$\n\n5. Thus the singular values are\n$$\\sigma_1=\\sqrt{\\frac{3+\\sqrt5}{2}},\\quad\n\\sigma_2=\\sqrt{\\frac{3-\\sqrt5}{2}},$$\nlisted in decreasing order.", "answer": "$$\\boxed{\\sqrt{\\frac{3+\\sqrt{5}}{2}},\\ \\sqrt{\\frac{3-\\sqrt{5}}{2}}}$$", "id": "1071366"}, {"introduction": "Singular values are more than just computational results; they are powerful descriptors of a matrix's properties. This exercise invites you to think conceptually about the implications of these values, specifically asking what a singular value of zero reveals about a matrix's determinant. Answering this question connects the SVD to the fundamental concepts of matrix singularity and invertibility [@problem_id:2203362].", "problem": "Let $A$ be an $n \\times n$ square matrix with real entries. The Singular Value Decomposition (SVD) of $A$ is given by $A = U \\Sigma V^T$, where $U$ and $V$ are $n \\times n$ orthogonal matrices and $\\Sigma$ is an $n \\times n$ diagonal matrix whose diagonal entries, $\\sigma_1, \\sigma_2, \\ldots, \\sigma_n$, are the singular values of $A$.\n\nSuppose it is known that at least one of the singular values of matrix $A$ is exactly zero. Based solely on this information, what can be definitively concluded about the determinant of $A$, denoted as $\\det(A)$?\n\nA. $\\det(A) = 0$\n\nB. $|\\det(A)| = 1$\n\nC. The determinant of $A$ must be a negative value.\n\nD. The value of $\\det(A)$ cannot be determined from the given information.", "solution": "We use the Singular Value Decomposition $A=U\\Sigma V^{T}$, where $U$ and $V$ are orthogonal, so $\\det(U)\\in\\{-1,1\\}$ and $\\det(V)\\in\\{-1,1\\}$, and $\\Sigma$ is diagonal with diagonal entries $\\sigma_{1},\\ldots,\\sigma_{n}$, the singular values of $A$. The determinant is multiplicative, and the determinant of a transpose equals the determinant of the original matrix, so\n$$\n\\det(A)=\\det(U)\\det(\\Sigma)\\det(V^{T})=\\det(U)\\det(\\Sigma)\\det(V).\n$$\nSince $\\Sigma$ is diagonal, its determinant is the product of its diagonal entries:\n$$\n\\det(\\Sigma)=\\prod_{i=1}^{n}\\sigma_{i}.\n$$\nIt is given that at least one singular value is zero, so there exists $j$ with $\\sigma_{j}=0$. Therefore,\n$$\n\\prod_{i=1}^{n}\\sigma_{i}=0,\n$$\nwhich implies\n$$\n\\det(\\Sigma)=0.\n$$\nHence,\n$$\n\\det(A)=\\det(U)\\cdot 0 \\cdot \\det(V)=0.\n$$\nThus, the determinant of $A$ is definitively zero, corresponding to option A.", "answer": "$$\\boxed{A}$$", "id": "2203362"}, {"introduction": "One of the most powerful applications of SVD is its ability to deconstruct a matrix into a sum of simpler, rank-one matrices, which is key to data compression and dimensionality reduction. This hands-on problem challenges you to perform this decomposition for a given matrix, transforming the abstract formula $A = \\sum_{i} \\sigma_i u_i v_i^T$ into a concrete result. This practice illuminates how SVD provides a structured way to understand a matrix's constituent parts [@problem_id:1388906].", "problem": "In many applications, from image compression to recommender systems, a key operation is to approximate a large matrix by a sum of simpler, rank-one matrices. The Singular Value Decomposition (SVD) provides a systematic way to construct such representations.\n\nThe SVD allows for an exact representation of any $m \\times n$ matrix $A$ of rank $r$ as a sum of $r$ rank-one matrices:\n$$A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nwhere $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ are the singular values, the vectors $u_i \\in \\mathbb{R}^m$ are the left singular vectors (which form an orthonormal set), and the vectors $v_i \\in \\mathbb{R}^n$ are the right singular vectors (which also form an orthonormal set).\n\nConsider the matrix:\n$$A = \\begin{pmatrix} 4 & 1 \\\\ 0 & 0 \\\\ 1 & 4 \\end{pmatrix}$$\nThis matrix has rank $r=2$. Determine its representation as a sum of two rank-one matrices based on its Singular Value Decomposition.\n\nYour final answer should be the explicit sum of two $3 \\times 2$ matrices with rational entries.", "solution": "We are asked to represent the matrix $A \\in \\mathbb{R}^{3 \\times 2}$ given by\n$$\nA=\\begin{pmatrix}4 & 1 \\\\ 0 & 0 \\\\ 1 & 4\\end{pmatrix}\n$$\nas a sum of two rank-one matrices using its Singular Value Decomposition and to provide an explicit sum with rational entries.\n\nBy the SVD, $A=\\sum_{i=1}^{r}\\sigma_{i}u_{i}v_{i}^{T}$, where $\\sigma_{i}>0$ are the singular values, and $u_{i}$ and $v_{i}$ are orthonormal left and right singular vectors, respectively. The singular values are the square roots of the eigenvalues of $A^{T}A$.\n\nCompute\n$$\nA^{T}A=\\begin{pmatrix}4 & 0 & 1 \\\\ 1 & 0 & 4\\end{pmatrix}\\begin{pmatrix}4 & 1 \\\\ 0 & 0 \\\\ 1 & 4\\end{pmatrix}=\\begin{pmatrix}17 & 8 \\\\ 8 & 17\\end{pmatrix}.\n$$\nFor a matrix of the form $\\begin{pmatrix}a & b \\\\ b & a\\end{pmatrix}$, the eigenvalues are $a+b$ and $a-b$. Hence the eigenvalues of $A^{T}A$ are\n$$\n\\lambda_{1}=25,\\quad \\lambda_{2}=9,\n$$\nso the singular values are\n$$\n\\sigma_{1}=5,\\quad \\sigma_{2}=3.\n$$\nCorresponding orthonormal right singular vectors are eigenvectors of $A^{T}A$. For $\\lambda_{1}=25$, an eigenvector is proportional to $\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and for $\\lambda_{2}=9$, an eigenvector is proportional to $\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$. Normalizing gives\n$$\nv_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad v_{2}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}.\n$$\nThe left singular vectors are $u_{i}=\\frac{1}{\\sigma_{i}}Av_{i}$. Compute\n$$\nAv_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}4 & 1 \\\\ 0 & 0 \\\\ 1 & 4\\end{pmatrix}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}5 \\\\ 0 \\\\ 5\\end{pmatrix}=\\frac{5}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix},\n$$\nso\n$$\nu_{1}=\\frac{1}{5}Av_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}.\n$$\nSimilarly,\n$$\nAv_{2}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}4 & 1 \\\\ 0 & 0 \\\\ 1 & 4\\end{pmatrix}\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}3 \\\\ 0 \\\\ -3\\end{pmatrix}=\\frac{3}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix},\n$$\nso\n$$\nu_{2}=\\frac{1}{3}Av_{2}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix}.\n$$\nTherefore,\n$$\nA=\\sigma_{1}u_{1}v_{1}^{T}+\\sigma_{2}u_{2}v_{2}^{T}\n=5\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}\\right)\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1\\end{pmatrix}\\right)+3\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix}\\right)\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & -1\\end{pmatrix}\\right).\n$$\nCarrying out the outer products and simplifying the scalar factors gives\n$$\nA=\\frac{5}{2}\\begin{pmatrix}1 & 1 \\\\ 0 & 0 \\\\ 1 & 1\\end{pmatrix}+\\frac{3}{2}\\begin{pmatrix}1 & -1 \\\\ 0 & 0 \\\\ -1 & 1\\end{pmatrix}.\n$$\nBoth summands are rank-one matrices, and all entries are rational, as required. A direct sum check yields\n$$\n\\begin{pmatrix}\\frac{5}{2} & \\frac{5}{2} \\\\ 0 & 0 \\\\ \\frac{5}{2} & \\frac{5}{2}\\end{pmatrix}+\\begin{pmatrix}\\frac{3}{2} & -\\frac{3}{2} \\\\ 0 & 0 \\\\ -\\frac{3}{2} & \\frac{3}{2}\\end{pmatrix}=\\begin{pmatrix}4 & 1 \\\\ 0 & 0 \\\\ 1 & 4\\end{pmatrix}=A.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{5}{2} & \\frac{5}{2} \\\\ 0 & 0 \\\\ \\frac{5}{2} & \\frac{5}{2}\\end{pmatrix}+\\begin{pmatrix}\\frac{3}{2} & -\\frac{3}{2} \\\\ 0 & 0 \\\\ -\\frac{3}{2} & \\frac{3}{2}\\end{pmatrix}}$$", "id": "1388906"}]}