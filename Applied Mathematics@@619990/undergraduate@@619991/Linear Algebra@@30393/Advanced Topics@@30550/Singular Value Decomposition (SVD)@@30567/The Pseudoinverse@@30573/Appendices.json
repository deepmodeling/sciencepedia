{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a foundational example that connects a geometric transformation to the algebraic concept of the pseudoinverse. This exercise asks you to consider a transformation that first projects a vector onto an axis and then scales it, a process represented by a non-invertible matrix. By directly applying the four Moore-Penrose conditions, you will build intuition for how the pseudoinverse provides a meaningful 'best-fit' inverse for singular matrices. [@problem_id:1397327]", "problem": "Consider a linear transformation $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$. This transformation takes any vector $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ and performs the following two operations in sequence:\n1. It projects the vector $\\mathbf{v}$ orthogonally onto the $x$-axis.\n2. It scales the resulting projected vector by a non-zero real constant $\\alpha$.\n\nLet $A$ be the matrix that represents this linear transformation $T$ with respect to the standard basis in $\\mathbb{R}^2$.\n\nIn linear algebra, the Moore-Penrose pseudoinverse of a matrix $M$, denoted $M^+$, is a generalization of the matrix inverse. It is particularly useful for finding a 'best fit' (least squares) solution to a system of linear equations that lacks a unique solution.\n\nYour task is to find the Moore-Penrose pseudoinverse, $A^+$, of the matrix $A$. Your answer should be expressed in terms of the constant $\\alpha$.", "solution": "The orthogonal projection onto the $x$-axis maps $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$ to $\\begin{pmatrix} x \\\\ 0 \\end{pmatrix}$. Its matrix in the standard basis is\n$$\nP=\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nScaling by a non-zero real constant $\\alpha$ yields the linear transformation with matrix\n$$\nA=\\alpha P=\\begin{pmatrix} \\alpha & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nTo find the Moore-Penrose pseudoinverse $A^{+}$, let\n$$\nX=\\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}\n$$\nand impose the Moore-Penrose conditions:\n$$\n\\text{(i) } A X A = A, \\quad \\text{(ii) } X A X = X, \\quad \\text{(iii) } (A X)^{T} = A X, \\quad \\text{(iv) } (X A)^{T} = X A.\n$$\nCompute\n$$\nA X = \\begin{pmatrix} \\alpha x_{11} & \\alpha x_{12} \\\\ 0 & 0 \\end{pmatrix}, \\quad\nA X A = \\begin{pmatrix} \\alpha^{2} x_{11} & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nCondition (i) gives $\\alpha^{2} x_{11} = \\alpha$, hence $x_{11} = \\frac{1}{\\alpha}$.\n\nNext,\n$$\nX A = \\begin{pmatrix} x_{11} \\alpha & 0 \\\\ x_{21} \\alpha & 0 \\end{pmatrix}, \\quad\nX A X = \\begin{pmatrix} \\alpha x_{11}^{2} & \\alpha x_{11} x_{12} \\\\ \\alpha x_{21} x_{11} & \\alpha x_{21} x_{12} \\end{pmatrix}.\n$$\nCondition (ii) gives $\\alpha x_{11}^{2} = x_{11}$ (satisfied by $x_{11}=\\frac{1}{\\alpha}$), $\\alpha x_{11} x_{12} = x_{12}$, $\\alpha x_{21} x_{11} = x_{21}$, and $x_{22} = \\alpha x_{21} x_{12}$.\n\nCondition (iii) requires $A X$ to be symmetric:\n$$\nA X = \\begin{pmatrix} \\alpha x_{11} & \\alpha x_{12} \\\\ 0 & 0 \\end{pmatrix} \\quad \\Rightarrow \\quad \\alpha x_{12} = 0 \\ \\Rightarrow \\ x_{12} = 0\n$$\nsince $\\alpha \\neq 0$.\n\nCondition (iv) requires $X A$ to be symmetric:\n$$\nX A = \\begin{pmatrix} x_{11} \\alpha & 0 \\\\ x_{21} \\alpha & 0 \\end{pmatrix} \\quad \\Rightarrow \\quad x_{21} \\alpha = 0 \\ \\Rightarrow \\ x_{21} = 0.\n$$\nWith $x_{12}=x_{21}=0$, the relation $x_{22}=\\alpha x_{21} x_{12}$ gives $x_{22}=0$. Therefore,\n$$\nA^{+}=X=\\begin{pmatrix} \\frac{1}{\\alpha} & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nEquivalently, since $A=\\alpha P$ with $P$ an orthogonal projection that satisfies $P^{+}=P$, we have $(\\alpha P)^{+}=\\frac{1}{\\alpha} P^{+}$, yielding the same result.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\alpha} & 0 \\\\ 0 & 0\\end{pmatrix}}$$", "id": "1397327"}, {"introduction": "Moving from a specific case to a general class, we now explore rank-one matrices, which are formed by the outer product of two vectors, $uv^T$. These matrices are fundamental in many areas of science and engineering, from signal processing to machine learning. This practice will guide you in deriving a closed-form expression for the pseudoinverse of any rank-one matrix, introducing the powerful connection to Singular Value Decomposition (SVD) and providing a highly useful formula for your mathematical toolkit. [@problem_id:1397317]", "problem": "Let $u$ be a non-zero column vector in $\\mathbb{R}^m$ and $v$ be a non-zero column vector in $\\mathbb{R}^n$. Consider the $m \\times n$ matrix $A$ formed by the outer product of these vectors, given by $A = uv^T$.\n\nThe Moore-Penrose pseudoinverse of a matrix $A$, denoted $A^+$, is the unique matrix that satisfies the following four criteria:\n1. $A A^+ A = A$\n2. $A^+ A A^+ = A^+$\n3. $(A A^+)^T = A A^+$\n4. $(A^+ A)^T = A^+ A$\n\nYour task is to find a closed-form expression for the pseudoinverse $A^+$. Express your answer in terms of the vectors $u$ and $v$ and their transposes. The squared Euclidean norm of a vector $x$ is defined as $\\|x\\|^2 = x^T x$ and may be used in your final expression.", "solution": "The problem asks for the Moore-Penrose pseudoinverse, $A^+$, of a rank-one matrix $A = uv^T$. A systematic method to compute the pseudoinverse is to first find the Singular Value Decomposition (SVD) of the matrix $A$.\n\nIf the SVD of an $m \\times n$ matrix $A$ is given by $A = U\\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ diagonal matrix of singular values, then the pseudoinverse $A^+$ is given by $A^+ = V\\Sigma^+ U^T$. Here, $\\Sigma^+$ is the $n \\times m$ matrix obtained by taking the transpose of $\\Sigma$ and then taking the reciprocal of each non-zero entry.\n\nLet's find the SVD of $A = uv^T$. First, we normalize the vectors $u$ and $v$. Let $\\hat{u} = \\frac{u}{\\|u\\|}$ and $\\hat{v} = \\frac{v}{\\|v\\|}$. Note that $\\hat{u}$ and $\\hat{v}$ are unit vectors, and the norms are $\\|u\\| = \\sqrt{u^T u}$ and $\\|v\\| = \\sqrt{v^T v}$.\n\nWe can rewrite the matrix $A$ in terms of these unit vectors:\n$A = uv^T = (\\|u\\|\\hat{u})(\\|v\\|\\hat{v})^T = (\\|u\\| \\|v\\|) \\hat{u} \\hat{v}^T$\n\nThis expression is already in the form of a singular value decomposition. For a rank-one matrix, there is only one non-zero singular value, $\\sigma_1$. Comparing with the general SVD form for a rank-one matrix, $A = \\sigma_1 u_1 v_1^T$, we can identify:\n- The single non-zero singular value: $\\sigma_1 = \\|u\\| \\|v\\|$.\n- The first column of $U$: $u_1 = \\hat{u}$.\n- The first column of $V$: $v_1 = \\hat{v}$.\n\nThe matrix $\\Sigma$ is an $m \\times n$ matrix with $\\sigma_1$ as its top-left element and zeros everywhere else:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nTo construct $A^+$, we first need $\\Sigma^+$. This is an $n \\times m$ matrix where the only non-zero element is at the top-left, and its value is the reciprocal of $\\sigma_1$:\n$$\n\\Sigma^+ = \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nThe full matrices $U$ and $V$ can be formed by completing the orthonormal bases. Let $U = [u_1 \\ u_2 \\dots u_m]$ and $V = [v_1 \\ v_2 \\dots v_n]$. The pseudoinverse is then calculated as $A^+ = V\\Sigma^+ U^T$.\n\n$A^+ = V \\Sigma^+ U^T = \\begin{pmatrix} v_1 & v_2 & \\dots & v_n \\end{pmatrix} \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots \\\\\n0 & 0 & \\dots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix} \\begin{pmatrix} u_1^T \\\\ u_2^T \\\\ \\vdots \\\\ u_m^T \\end{pmatrix}$\n\nPerforming the matrix multiplication, the product $\\Sigma^+ U^T$ yields an $n \\times m$ matrix where the first row is $(1/\\sigma_1)u_1^T$ and all other rows are zero. Then, multiplying by $V$ from the left picks out the first column of $V$, $v_1$, scaled by the first row of $\\Sigma^+ U^T$. This results in:\n$A^+ = v_1 \\left( \\frac{1}{\\sigma_1} u_1^T \\right) = \\frac{1}{\\sigma_1} v_1 u_1^T$.\n\nNow, we substitute back the expressions for $\\sigma_1$, $u_1$, and $v_1$:\n$\\sigma_1 = \\|u\\| \\|v\\|$\n$u_1 = \\hat{u} = \\frac{u}{\\|u\\|}$\n$v_1 = \\hat{v} = \\frac{v}{\\|v\\|}$\n\nSo,\n$A^+ = \\frac{1}{\\|u\\| \\|v\\|} \\left( \\frac{v}{\\|v\\|} \\right) \\left( \\frac{u}{\\|u\\|} \\right)^T = \\frac{1}{\\|u\\|^2 \\|v\\|^2} v u^T$.\n\nFinally, using the provided definition for the squared norm, $\\|x\\|^2 = x^T x$, we can write the final expression as:\n$A^+ = \\frac{1}{(u^T u)(v^T v)} v u^T$.", "answer": "$$\\boxed{\\frac{1}{(u^T u)(v^T v)} v u^T}$$", "id": "1397317"}, {"introduction": "To solidify your understanding, this final practice is a conceptual check that addresses a common pitfall. While general methods for finding $A^+$ are always valid, simpler formulas exist for special cases, such as the right pseudoinverse $A^+ = A^T(AA^T)^{-1}$ for matrices with full row rank. This problem investigates why this shortcut fails when its conditions are not met, sharpening your ability to diagnose problems and select the correct mathematical tool by analyzing the matrix's rank. [@problem_id:1400715]", "problem": "A student in a linear algebra course is tasked with finding the minimum norm solution to an underdetermined linear system $A\\mathbf{x} = \\mathbf{b}$, where the coefficient matrix $A$ is a real $m \\times n$ matrix with $m < n$. For a specific problem, the matrix is given as:\n$$A = \\begin{pmatrix} 1 & -2 & 3 \\\\ -2 & 4 & -6 \\end{pmatrix}$$\nThe student recalls that for a matrix with full row rank, the minimum norm solution is given by $\\mathbf{x} = A^+\\mathbf{b}$, where $A^+ = A^T(AA^T)^{-1}$ is the right pseudoinverse of $A$. However, upon attempting to compute $A^+$, their computational software returns an error, indicating the procedure cannot be completed.\n\nWhich of the following statements provides the fundamental mathematical reason for this failure?\n\nA. The matrix $A^TA$ is singular.\n\nB. The number of columns of $A$ is greater than the number of rows.\n\nC. The matrix $AA^T$ is singular.\n\nD. The system $A\\mathbf{x} = \\mathbf{b}$ has no solution for any non-zero vector $\\mathbf{b}$.\n\nE. The rows of matrix $A$ are not orthogonal to each other.", "solution": "The right pseudoinverse formula $A^{+} = A^{T}(AA^{T})^{-1}$ requires that $A$ have full row rank so that $AA^{T}$ is invertible. For the given matrix\n$$\nA=\\begin{pmatrix} 1 & -2 & 3 \\\\ -2 & 4 & -6 \\end{pmatrix},\n$$\nthe second row is $-2$ times the first row, so the rows are linearly dependent and $\\operatorname{rank}(A)=1<m=2$. Consequently, $AA^{T}$ is singular.\n\nCompute $AA^{T}$ explicitly. Let the rows of $A$ be $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}=-2\\mathbf{r}_{1}$. Then\n$$\nAA^{T}=\\begin{pmatrix}\n\\mathbf{r}_{1}\\cdot\\mathbf{r}_{1} & \\mathbf{r}_{1}\\cdot\\mathbf{r}_{2} \\\\\n\\mathbf{r}_{2}\\cdot\\mathbf{r}_{1} & \\mathbf{r}_{2}\\cdot\\mathbf{r}_{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n14 & -28 \\\\\n-28 & 56\n\\end{pmatrix},\n$$\nsince $\\mathbf{r}_{1}\\cdot\\mathbf{r}_{1}=1^{2}+(-2)^{2}+3^{2}=14$, $\\mathbf{r}_{1}\\cdot\\mathbf{r}_{2}=-2(\\mathbf{r}_{1}\\cdot\\mathbf{r}_{1})=-28$, and $\\mathbf{r}_{2}\\cdot\\mathbf{r}_{2}=4(\\mathbf{r}_{1}\\cdot\\mathbf{r}_{1})=56$. Its determinant is\n$$\n\\det(AA^{T})=14\\cdot 56-(-28)^{2}=784-784=0,\n$$\nso $(AA^{T})^{-1}$ does not exist. Therefore, the computation of $A^{+}=A^{T}(AA^{T})^{-1}$ fails because $AA^{T}$ is singular.\n\nThus, the fundamental mathematical reason for the failure is that $AA^{T}$ is singular.", "answer": "$$\\boxed{C}$$", "id": "1400715"}]}