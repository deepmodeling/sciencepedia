{"hands_on_practices": [{"introduction": "Understanding theory is the first step, but mastery comes from practice. This first exercise provides a direct, hands-on opportunity to apply the Eckart-Young-Mirsky theorem. You will calculate the best rank-1 approximation for a given matrix, reinforcing the core computational procedure of using the Singular Value Decomposition (SVD) to distill a matrix into its most significant component [@problem_id:1374759]. This skill is foundational for applications ranging from image compression to statistical analysis.", "problem": "In many applications, such as image compression and data analysis, it is useful to approximate a given matrix with another matrix of a lower rank. For a given matrix $A$, the \"best\" rank-$k$ approximation is a matrix $A_k$ of rank $k$ that minimizes the Frobenius norm of the error, $\\|A - A_k\\|_F$, where the Frobenius norm of a matrix $M$ with entries $m_{ij}$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i,j} |m_{ij}|^2}$.\n\nConsider the matrix $A$ given by:\n$$A = \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 1 & -1 & 0 \\end{pmatrix}$$\nThe rank of this matrix is 2. Your task is to find the matrix $A_1$ which is the best rank-1 approximation of $A$.", "solution": "To find the best rank-1 approximation under the Frobenius norm, use the truncated singular value decomposition: if $A = \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{T}$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots$, then the best rank-1 approximation is $A_{1} = \\sigma_{1} u_{1} v_{1}^{T}$.\n\nCompute $A A^{T}$ to obtain the squared singular values:\n$$\nA = \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 1 & -1 & 0 \\end{pmatrix}, \\quad\nA A^{T} = \\begin{pmatrix} 6 & 6 & 0 \\\\ 6 & 6 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}.\n$$\nThe eigenvalues of $A A^{T}$ are $12$, $2$, and $0$. Hence the singular values are $\\sigma_{1} = 2\\sqrt{3}$, $\\sigma_{2} = \\sqrt{2}$, and $\\sigma_{3} = 0$.\n\nAn eigenvector of $A A^{T}$ for eigenvalue $12$ is proportional to $\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$. Normalizing gives\n$$\nu_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the corresponding right singular vector via\n$$\nv_{1} = \\frac{1}{\\sigma_{1}} A^{T} u_{1}.\n$$\nFirst obtain\n$$\nA^{T} u_{1} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & -1 \\\\ 2 & 2 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix} = \\sqrt{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix},\n$$\nso\n$$\nv_{1} = \\frac{1}{2\\sqrt{3}} \\sqrt{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\nTherefore,\n$$\nA_{1} = \\sigma_{1} u_{1} v_{1}^{T}\n= \\left(2\\sqrt{3}\\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 & 1 & 2 \\end{pmatrix} \\right)\n= \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nThis matrix has rank $1$ and is the best rank-1 approximation of $A$ in the Frobenius norm.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix}}$$", "id": "1374759"}, {"introduction": "Moving beyond pure computation, this problem tests your conceptual understanding of the SVD and its structure. Instead of performing a full SVD calculation from scratch, you are given the decomposition and asked to reason about successive approximations [@problem_id:1374757]. This exercise highlights a crucial property: the best rank-$k$ approximation of a matrix has an SVD that is simply a truncated version of the original, a concept that is essential for computational efficiency in real-world applications.", "problem": "Consider a real matrix $A$ of size $3 \\times 4$. The Singular Value Decomposition (SVD) of $A$ is given by the product $A = U\\Sigma V^T$, where the component matrices are given by:\n$$\nU = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\n\\Sigma = \\begin{pmatrix} 10 & 0 & 0 & 0 \\\\ 0 & 5 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\end{pmatrix}\n$$\n$$\nV^T = \\begin{pmatrix} 1/2 & 1/2 & 1/2 & 1/2 \\\\ 1/2 & -1/2 & 1/2 & -1/2 \\\\ 1/2 & 1/2 & -1/2 & -1/2 \\\\ 1/2 & -1/2 & -1/2 & 1/2 \\end{pmatrix}\n$$\nLet $A_2$ be the best rank-2 approximation of the matrix $A$. Subsequently, let $B$ be the best rank-1 approximation of the matrix $A_2$. The 'best' approximation is defined as the matrix of the target rank that minimizes the Frobenius norm of the difference with the original matrix.\n\nDetermine the value of the entry in the first row and third column of matrix $B$.", "solution": "We use the Singular Value Decomposition $A=U\\Sigma V^{T}$, where $U\\in\\mathbb{R}^{3\\times 3}$ is orthogonal, $V\\in\\mathbb{R}^{4\\times 4}$ is orthogonal, and $\\Sigma\\in\\mathbb{R}^{3\\times 4}$ has singular values $\\sigma_{1}\\geq\\sigma_{2}\\geq\\sigma_{3}\\geq 0$ on its main diagonal. The best rank-$k$ approximation in the Frobenius norm is given by the truncated SVD (Eckartâ€“Young theorem):\n$$\nA_{k}=\\sum_{i=1}^{k}\\sigma_{i}u_{i}v_{i}^{T},\n$$\nwhere $u_{i}$ and $v_{i}$ are the $i$-th columns of $U$ and $V$, respectively.\n\nFrom the data, $U=I_{3}$, so $u_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$, $u_{2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$, $u_{3}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}$. The singular values are read from $\\Sigma$ as $\\sigma_{1}=10$, $\\sigma_{2}=5$, $\\sigma_{3}=2$. The matrix $V^{T}$ is given, so $v_{1}$ is the first column of $V$, which equals the transpose of the first row of $V^{T}$:\n$$\nv_{1}=\\begin{pmatrix}\\frac{1}{2}\\\\ \\frac{1}{2}\\\\ \\frac{1}{2}\\\\ \\frac{1}{2}\\end{pmatrix}.\n$$\n\nFirst, $A_{2}$, the best rank-$2$ approximation of $A$, is\n$$\nA_{2}=\\sigma_{1}u_{1}v_{1}^{T}+\\sigma_{2}u_{2}v_{2}^{T}.\n$$\nIts singular values are $\\sigma_{1}$ and $\\sigma_{2}$ with the same corresponding singular vectors $u_{1},v_{1}$ and $u_{2},v_{2}$.\n\nNext, $B$ is the best rank-$1$ approximation of $A_{2}$. By the same truncated SVD principle, this keeps only the leading singular triplet:\n$$\nB=\\sigma_{1}u_{1}v_{1}^{T}.\n$$\n\nTo extract the entry in the first row and third column of $B$, use the outer-product form. For $B=\\sigma_{1}u_{1}v_{1}^{T}$, the $(i,j)$ entry is\n$$\nB_{ij}=\\sigma_{1}\\,u_{1,i}\\,v_{1,j}.\n$$\nSpecifically, $u_{1,1}=1$ and $v_{1,3}=\\frac{1}{2}$. Therefore,\n$$\nB_{1,3}=\\sigma_{1}\\cdot 1\\cdot \\frac{1}{2}=10\\cdot\\frac{1}{2}=5.\n$$", "answer": "$$\\boxed{5}$$", "id": "1374757"}, {"introduction": "This final practice problem challenges you to apply the SVD methodology to a more specialized and interesting case: a nilpotent matrix. While the underlying principle of finding the dominant singular value and vectors remains the same, the unique structure of this matrix requires careful algebraic manipulation [@problem_id:1374797]. Successfully solving this demonstrates the true generality of SVD as a tool, showcasing its power to analyze matrices that may not be diagonalizable and have complex structural properties.", "problem": "In various fields such as data compression and machine learning, it is a common task to approximate a complex matrix with a simpler one of lower rank. According to the Eckart-Young-Mirsky theorem, the best rank-$k$ approximation of a matrix can be found using its Singular Value Decomposition (SVD).\n\nConsider the nilpotent $3 \\times 3$ matrix $A$ defined as:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nYour task is to find the matrix $B$ of rank 1 that is closest to $A$. The \"closeness\" between matrices is measured here using the Frobenius norm. For two matrices $M$ and $N$ of the same dimensions, the squared Frobenius distance is $\\|M-N\\|_F^2 = \\sum_{i,j} |M_{ij} - N_{ij}|^2$. Finding the matrix $B$ that minimizes this distance is equivalent to finding the best rank-1 approximation.\n\nProvide the resulting matrix $B$. The entries of your matrix should be expressed as exact values, not decimal approximations.", "solution": "By the Eckart-Young-Mirsky theorem, the best rank-1 approximation to a matrix $A$ in Frobenius norm is given by $B=\\sigma_{1}u_{1}v_{1}^{T}$, where $\\sigma_{1}$ is the largest singular value of $A$ and $u_{1},v_{1}$ are the corresponding left and right singular vectors. The singular values are the square roots of the eigenvalues of $A^{T}A$.\n\nCompute $A^{T}A$:\n$$\nA^{T}A=\\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}.\n$$\nThe nonzero part is the $2\\times 2$ block $\\begin{pmatrix}1 & 1 \\\\ 1 & 2\\end{pmatrix}$, whose characteristic polynomial is\n$$\n\\det\\begin{pmatrix}1-\\lambda & 1 \\\\ 1 & 2-\\lambda\\end{pmatrix}=(1-\\lambda)(2-\\lambda)-1=\\lambda^{2}-3\\lambda+1.\n$$\nThus the eigenvalues of $A^{T}A$ are $\\lambda_{1}=\\frac{3+\\sqrt{5}}{2}$, $\\lambda_{2}=\\frac{3-\\sqrt{5}}{2}$, and $0$. The largest singular value is $\\sigma_{1}=\\sqrt{\\lambda_{1}}$.\n\nFor $\\lambda_{1}$, an eigenvector $v$ of $A^{T}A$ has first component zero (since $\\lambda_{1}\\neq 0$ and the first row of $A^{T}A$ is zero), so write $v=[0,x,y]^{T}$, and solve\n$$\n\\begin{pmatrix}1 & 1 \\\\ 1 & 2\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\lambda_{1}\\begin{pmatrix}x \\\\ y\\end{pmatrix}.\n$$\nFrom $(1-\\lambda_{1})x+y=0$ we get $y=(\\lambda_{1}-1)x$. Choosing $x=1$ gives an eigenvector $v_{1,\\text{unnorm}}=[0,1,\\lambda_{1}-1]^{T}$. Its squared norm is\n$$\n\\|v_{1,\\text{unnorm}}\\|^{2}=1+(\\lambda_{1}-1)^{2}.\n$$\nUsing $\\lambda_{1}^{2}-3\\lambda_{1}+1=0$, we have $(\\lambda_{1}-1)^{2}=\\lambda_{1}$, hence $\\|v_{1,\\text{unnorm}}\\|^{2}=\\lambda_{1}+1$. Therefore the unit right singular vector is\n$$\nv_{1}=\\frac{1}{\\sqrt{\\lambda_{1}+1}}\\begin{pmatrix}0 \\\\ 1 \\\\ \\lambda_{1}-1\\end{pmatrix}.\n$$\nNext compute $A v_{1}$:\n$$\nA\\begin{pmatrix}0 \\\\ 1 \\\\ \\lambda_{1}-1\\end{pmatrix}=\\begin{pmatrix}1+(\\lambda_{1}-1) \\\\ \\lambda_{1}-1 \\\\ 0\\end{pmatrix}=\\begin{pmatrix}\\lambda_{1} \\\\ \\lambda_{1}-1 \\\\ 0\\end{pmatrix},\n$$\nso\n$$\nA v_{1}=\\frac{1}{\\sqrt{\\lambda_{1}+1}}\\begin{pmatrix}\\lambda_{1} \\\\ \\lambda_{1}-1 \\\\ 0\\end{pmatrix}.\n$$\nSince $\\|A v_{1}\\|=\\sigma_{1}$, the left singular vector is $u_{1}=(A v_{1})/\\sigma_{1}$. The best rank-1 approximation is $B=\\sigma_{1}u_{1}v_{1}^{T}=(A v_{1})v_{1}^{T}$. Therefore\n$$\nB=\\frac{1}{\\lambda_{1}+1}\\begin{pmatrix}\\lambda_{1} \\\\ \\lambda_{1}-1 \\\\ 0\\end{pmatrix}\\begin{pmatrix}0 & 1 & \\lambda_{1}-1\\end{pmatrix}.\n$$\nCarrying out the outer product gives\n$$\nB=\\frac{1}{\\lambda_{1}+1}\\begin{pmatrix}\n0 & \\lambda_{1} & \\lambda_{1}(\\lambda_{1}-1) \\\\\n0 & \\lambda_{1}-1 & (\\lambda_{1}-1)^{2} \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nUsing $\\lambda_{1}^{2}=3\\lambda_{1}-1$ and $(\\lambda_{1}-1)^{2}=\\lambda_{1}$, we simplify entries:\n$$\nB=\\frac{1}{\\lambda_{1}+1}\\begin{pmatrix}\n0 & \\lambda_{1} & 2\\lambda_{1}-1 \\\\\n0 & \\lambda_{1}-1 & \\lambda_{1} \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nSubstitute $\\lambda_{1}=\\frac{3+\\sqrt{5}}{2}$ and rationalize each fraction:\n- $\\frac{\\lambda_{1}}{\\lambda_{1}+1}=\\frac{\\frac{3+\\sqrt{5}}{2}}{\\frac{5+\\sqrt{5}}{2}}=\\frac{3+\\sqrt{5}}{5+\\sqrt{5}}=\\frac{5+\\sqrt{5}}{10}$,\n- $\\frac{\\lambda_{1}-1}{\\lambda_{1}+1}=\\frac{\\frac{1+\\sqrt{5}}{2}}{\\frac{5+\\sqrt{5}}{2}}=\\frac{1+\\sqrt{5}}{5+\\sqrt{5}}=\\frac{\\sqrt{5}}{5}$,\n- $\\frac{2\\lambda_{1}-1}{\\lambda_{1}+1}=\\frac{2+\\sqrt{5}}{\\frac{5+\\sqrt{5}}{2}}=\\frac{5+3\\sqrt{5}}{10}$.\n\nThus the best rank-1 approximation is\n$$\nB=\\begin{pmatrix}\n0 & \\frac{5+\\sqrt{5}}{10} & \\frac{5+3\\sqrt{5}}{10} \\\\\n0 & \\frac{\\sqrt{5}}{5} & \\frac{5+\\sqrt{5}}{10} \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\n0 & \\frac{5+\\sqrt{5}}{10} & \\frac{5+3\\sqrt{5}}{10} \\\\\n0 & \\frac{\\sqrt{5}}{5} & \\frac{5+\\sqrt{5}}{10} \\\\\n0 & 0 & 0\n\\end{pmatrix}}$$", "id": "1374797"}]}