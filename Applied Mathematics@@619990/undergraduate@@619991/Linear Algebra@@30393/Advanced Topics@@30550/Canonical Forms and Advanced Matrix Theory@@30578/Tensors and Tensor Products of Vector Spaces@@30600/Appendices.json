{"hands_on_practices": [{"introduction": "To build our understanding of tensors, we must start with the basics: how to combine two vectors into a tensor. This first exercise [@problem_id:1087841] grounds the abstract definition of the tensor product in a concrete calculation. By applying the property of bilinearity, you will see how the components of two vectors in $\\mathbb{R}^2$ combine to form the components of their tensor product, an element in a higher-dimensional space $\\mathbb{R}^4$.", "problem": "Consider two vectors $v = (1, 2) \\in \\mathbb{R}^2$ and $w = (3, 4) \\in \\mathbb{R}^2$ in the standard basis $\\{e_1, e_2\\}$, where $e_1 = (1, 0)$ and $e_2 = (0, 1)$. The tensor product $v \\otimes w$ is an element of the vector space $\\mathbb{R}^2 \\otimes \\mathbb{R}^2$, which has the standard basis $\\{e_i \\otimes e_j \\mid i,j \\in \\{1,2\\}\\}$ ordered as $\\{e_1 \\otimes e_1, e_1 \\otimes e_2, e_2 \\otimes e_1, e_2 \\otimes e_2\\}$.\n\nCompute the coordinate vector of $v \\otimes w$ in this basis. Express the result as a column vector in $\\mathbb{R}^4$.", "solution": "1. In the standard bases we have \n$$v=1\\,e_1+2\\,e_2,\\quad w=3\\,e_1+4\\,e_2.$$\n2. By bilinearity of the tensor product,\n$$v\\otimes w=(1\\,e_1+2\\,e_2)\\otimes(3\\,e_1+4\\,e_2)\n=1\\cdot3\\,(e_1\\otimes e_1)+1\\cdot4\\,(e_1\\otimes e_2)\n+2\\cdot3\\,(e_2\\otimes e_1)+2\\cdot4\\,(e_2\\otimes e_2).$$\n3. Collecting coefficients gives\n$$v\\otimes w=3\\,(e_1\\otimes e_1)+4\\,(e_1\\otimes e_2)+6\\,(e_2\\otimes e_1)+8\\,(e_2\\otimes e_2).$$\n4. In the ordered basis $\\{e_1\\otimes e_1,e_1\\otimes e_2,e_2\\otimes e_1,e_2\\otimes e_2\\}$ the coordinate vector is\n$$\\begin{pmatrix}3\\\\4\\\\6\\\\8\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}3\\\\4\\\\6\\\\8\\end{pmatrix}}$$", "id": "1087841"}, {"introduction": "Having learned to combine vectors, we can now explore how to combine linear transformations, which are the actions upon those vectors. This practice [@problem_id:1392569] demonstrates a cornerstone result: the tensor product of linear operators has a matrix representation given by the Kronecker product of their individual matrices. You will apply this principle to familiar operators—a projection and a rotation—to see how their combined action is represented in the tensor product space.", "problem": "Let $V = \\mathbb{R}^2$ be a real vector space equipped with its standard ordered basis $\\mathcal{B} = \\{e_1, e_2\\}$, where $e_1 = (1, 0)$ and $e_2 = (0, 1)$.\n\nConsider two linear operators on $V$:\n1.  The operator $P: V \\to V$ is the orthogonal projection onto the one-dimensional subspace spanned by the vector $e_1$.\n2.  The operator $R: V \\to V$ is the linear transformation that rotates vectors counter-clockwise by an angle of $\\frac{\\pi}{2}$ radians.\n\nNow, consider the tensor product space $W = V \\otimes V$. An ordered basis for $W$ can be constructed from the basis of $V$, given by $\\mathcal{B}_{\\otimes} = \\{e_1 \\otimes e_1, e_1 \\otimes e_2, e_2 \\otimes e_1, e_2 \\otimes e_2\\}$.\n\nDetermine the matrix representation of the tensor product operator $T = P \\otimes R: W \\to W$ with respect to the ordered basis $\\mathcal{B}_{\\otimes}$. The final answer should be a $4 \\times 4$ matrix.", "solution": "The problem asks for the matrix representation of the tensor product operator $T = P \\otimes R$ acting on the space $W = \\mathbb{R}^2 \\otimes \\mathbb{R}^2$. The matrix representation of a tensor product of linear operators, with respect to the corresponding tensor product basis, is the Kronecker product of their individual matrix representations.\n\nFirst, we determine the matrix representation of the operator $P: \\mathbb{R}^2 \\to \\mathbb{R}^2$ with respect to the standard basis $\\mathcal{B} = \\{e_1, e_2\\}$. The operator $P$ performs an orthogonal projection onto the subspace spanned by $e_1$. This means it projects vectors onto the x-axis.\n\nWe find the images of the basis vectors under $P$:\n$P(e_1) = P((1, 0)) = (1, 0) = 1 \\cdot e_1 + 0 \\cdot e_2$\n$P(e_2) = P((0, 1)) = (0, 0) = 0 \\cdot e_1 + 0 \\cdot e_2$\n\nThe columns of the matrix representation $[P]_{\\mathcal{B}}$ are the coordinate vectors of $P(e_1)$ and $P(e_2)$ with respect to $\\mathcal{B}$.\n$$\n[P]_{\\mathcal{B}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\nNext, we determine the matrix representation of the operator $R: \\mathbb{R}^2 \\to \\mathbb{R}^2$ with respect to the basis $\\mathcal{B}$. The operator $R$ rotates vectors counter-clockwise by $\\frac{\\pi}{2}$ radians.\n\nWe find the images of the basis vectors under $R$:\n$R(e_1) = R((1, 0)) = (0, 1) = 0 \\cdot e_1 + 1 \\cdot e_2$\n$R(e_2) = R((0, 1)) = (-1, 0) = -1 \\cdot e_1 + 0 \\cdot e_2$\n\nThe columns of the matrix representation $[R]_{\\mathcal{B}}$ are the coordinate vectors of $R(e_1)$ and $R(e_2)$ with respect to $\\mathcal{B}$.\n$$\n[R]_{\\mathcal{B}} = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n\nThe matrix representation of the tensor product operator $T = P \\otimes R$ with respect to the basis $\\mathcal{B}_{\\otimes}$ is the Kronecker product of the matrices $[P]_{\\mathcal{B}}$ and $[R]_{\\mathcal{B}}$.\n$$\n[T]_{\\mathcal{B}_{\\otimes}} = [P \\otimes R]_{\\mathcal{B}_{\\otimes}} = [P]_{\\mathcal{B}} \\otimes [R]_{\\mathcal{B}}\n$$\n\nLet $[P]_{\\mathcal{B}} = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{21} & p_{22} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$. The Kronecker product is defined as:\n$$\n[P]_{\\mathcal{B}} \\otimes [R]_{\\mathcal{B}} = \\begin{pmatrix} p_{11} [R]_{\\mathcal{B}} & p_{12} [R]_{\\mathcal{B}} \\\\ p_{21} [R]_{\\mathcal{B}} & p_{22} [R]_{\\mathcal{B}} \\end{pmatrix}\n$$\n\nSubstituting the matrices we found:\n$$\n[T]_{\\mathcal{B}_{\\otimes}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} & 0 \\cdot \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\\\ 0 \\cdot \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} & 0 \\cdot \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\end{pmatrix}\n$$\n\nExpanding this block matrix gives the final $4 \\times 4$ matrix:\n$$\n[T]_{\\mathcal{B}_{\\otimes}} = \\begin{pmatrix} 0 & -1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\nThis is the matrix representation of $T = P \\otimes R$ with respect to the ordered basis $\\mathcal{B}_{\\otimes} = \\{e_1 \\otimes e_1, e_1 \\otimes e_2, e_2 \\otimes e_1, e_2 \\otimes e_2\\}$.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & -1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}}$$", "id": "1392569"}, {"introduction": "Not all tensors are \"simple\" (of the form $v \\otimes w$). Most are sums of simple tensors, and the minimum number of terms in this sum defines the tensor's rank. This practice [@problem_id:1392570] explores this fundamental concept by connecting it to a more familiar idea: the rank of a matrix. You will see how the rank of a tensor corresponding to a linear operator is equivalent to the rank of that operator, providing a powerful tool for analyzing tensor structure.", "problem": "Let $V = \\mathbb{R}^5$ be a 5-dimensional real vector space, equipped with the standard inner product $\\langle \\cdot, \\cdot \\rangle$. Let $\\{e_i\\}_{i=1}^5$ denote the standard orthonormal basis for $V$.\n\nConsider the tensor product space $W = V \\otimes V$. A tensor in $W$ is called a simple tensor (or a rank-one tensor) if it can be written in the form $v_1 \\otimes v_2$ for some vectors $v_1, v_2 \\in V$. The rank of a general tensor $T \\in W$, denoted $\\text{rank}(T)$, is defined as the minimum integer $r$ such that $T$ can be expressed as a sum of $r$ simple tensors.\n\nThere exists a canonical isomorphism between the space of linear operators on $V$, denoted $\\text{End}(V)$, and the tensor space $W$. Under this isomorphism, a linear operator $L \\in \\text{End}(V)$ with matrix representation $A = [a_{ij}]$ with respect to the basis $\\{e_i\\}$ corresponds to the tensor $T_L = \\sum_{i=1}^5 \\sum_{j=1}^5 a_{ij} e_i \\otimes e_j$.\n\nConsider the specific linear operator $H: V \\to V$ defined as the reflection across the hyperplane orthogonal to the vector $u = \\sum_{i=1}^5 e_i$. Explicitly, the action of this operator on any vector $v \\in V$ is given by\n$$H(v) = v - 2 \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u.$$\nLet $T_H \\in W$ be the tensor that corresponds to the operator $H$ under the isomorphism described above.\n\nDetermine the rank of the tensor $T_H$.", "solution": "We identify $W=V\\otimes V$ with $\\text{End}(V)$ via the canonical isomorphism determined by the orthonormal basis $\\{e_{i}\\}_{i=1}^{5}$: if $L\\in\\text{End}(V)$ has matrix $A=[a_{ij}]$ in this basis, then $T_{L}=\\sum_{i=1}^{5}\\sum_{j=1}^{5}a_{ij}\\,e_{i}\\otimes e_{j}$. Under the inner product identification $V\\cong V^{*}$, a simple tensor $v\\otimes w$ corresponds to the rank-one operator $x\\mapsto v\\langle w,x\\rangle$; conversely, a rank-one operator has the form $v\\otimes w$. Therefore, under this isomorphism, the rank (in the tensor sense) of $T_{L}$ equals the minimal number of rank-one operators whose sum is $L$, which is exactly the matrix rank of $L$.\n\nTo justify this equality precisely, suppose $T=\\sum_{k=1}^{r}v_{k}\\otimes w_{k}$. Then the associated operator $L_{T}$ satisfies\n$$\nL_{T}(x)=\\sum_{k=1}^{r}v_{k}\\langle w_{k},x\\rangle,\n$$\nso $\\text{im}(L_{T})\\subseteq\\text{span}\\{v_{1},\\dots,v_{r}\\}$ and hence $\\text{rank}(L_{T})\\leq r$. Taking the minimum over all such decompositions shows $\\text{tensor-rank}(T)\\geq\\text{rank}(L_{T})$. Conversely, if $\\text{rank}(L)=s$, there exist vectors $u_{1},\\dots,u_{s}$ and $w_{1},\\dots,w_{s}$ such that\n$$\nL=\\sum_{k=1}^{s}u_{k}\\otimes w_{k},\n$$\nwhich implies $\\text{tensor-rank}(T_{L})\\leq s$. Hence $\\text{tensor-rank}(T_{L})=\\text{rank}(L)$.\n\nNow compute the rank of $H$. Let $u=\\sum_{i=1}^{5}e_{i}$. Then $\\langle u,u\\rangle=5$ and\n$$\nH=I-\\frac{2}{\\langle u,u\\rangle}\\,u\\otimes u=I-\\frac{2}{5}\\,u\\otimes u,\n$$\nwhere we view $u\\otimes u$ as the rank-one operator $x\\mapsto u\\langle u,x\\rangle$. The action of $H$ on $u$ and on vectors orthogonal to $u$ is\n$$\nH(u)=u-\\frac{2}{5}\\,u\\langle u,u\\rangle=u-2u=-u,\\qquad H(w)=w\\quad\\text{for all }w\\perp u.\n$$\nThus the eigenvalues of $H$ are $-1$ along the $1$-dimensional span of $u$ and $1$ on the $4$-dimensional orthogonal complement, so $H$ is invertible and\n$$\n\\text{rank}(H)=5.\n$$\nBy the equality between tensor rank and operator rank under the given isomorphism, we conclude\n$$\n\\text{rank}(T_{H})=\\text{rank}(H)=5.\n$$", "answer": "$$\\boxed{5}$$", "id": "1392570"}]}