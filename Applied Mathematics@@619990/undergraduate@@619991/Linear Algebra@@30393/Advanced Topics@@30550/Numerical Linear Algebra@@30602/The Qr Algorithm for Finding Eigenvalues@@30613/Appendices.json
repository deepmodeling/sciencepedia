{"hands_on_practices": [{"introduction": "To truly understand the QR algorithm, it's best to start with a direct calculation. This first exercise guides you through one complete iteration of the unshifted QR algorithm, the simplest form of this powerful method. By computing the QR decomposition and then reversing the multiplication, you will see the core mechanics in action and verify that key matrix properties like trace and determinant are preserved, reinforcing the concept of similarity transformations [@problem_id:1397703].", "problem": "The QR algorithm is an iterative method used in numerical linear algebra to find the eigenvalues of a matrix. The unshifted version of the algorithm generates a sequence of matrices $A_0, A_1, A_2, \\dots$ starting from an initial matrix $A$. The process for generating the next matrix in the sequence, $A_{k+1}$, from the current matrix, $A_k$, is as follows:\n1. Find the QR decomposition of the current matrix: $A_k = Q_k R_k$, where $Q_k$ is an orthogonal matrix and $R_k$ is an upper triangular matrix.\n2. Compute the next matrix by reversing the order of multiplication: $A_{k+1} = R_k Q_k$.\n\nConsider the initial matrix $A_0 = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$.\n\nPerform one full iteration of the QR algorithm to compute the matrix $A_1$. Then, calculate the trace and the determinant for both matrices, $A_0$ and $A_1$.\n\nProvide your answer as a $1 \\times 4$ row matrix containing the following four values in order: the trace of $A_0$, the determinant of $A_0$, the trace of $A_1$, and the determinant of $A_1$.", "solution": "We apply one unshifted QR iteration to $A_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$.\n\nFirst compute the QR decomposition $A_{0} = Q R$ via Gramâ€“Schmidt on the columns $a_{1}$ and $a_{2}$ of $A_{0}$:\n$$\na_{1} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad \\|a_{1}\\| = \\sqrt{2^{2} + (-1)^{2}} = \\sqrt{5}, \\quad q_{1} = \\frac{a_{1}}{\\|a_{1}\\|} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad r_{11} = \\sqrt{5}.\n$$\nLet $a_{2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$. Then\n$$\nr_{12} = q_{1}^{T} a_{2} = \\frac{1}{\\sqrt{5}}(2\\cdot(-1) + (-1)\\cdot 2) = -\\frac{4}{\\sqrt{5}}.\n$$\nCompute\n$$\nu_{2} = a_{2} - r_{12} q_{1} = a_{2} + \\frac{4}{\\sqrt{5}} q_{1} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} + \\frac{4}{\\sqrt{5}} \\cdot \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} + \\frac{4}{5} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{6}{5} \\end{pmatrix}.\n$$\nHence\n$$\nr_{22} = \\|u_{2}\\| = \\sqrt{\\left(\\frac{3}{5}\\right)^{2} + \\left(\\frac{6}{5}\\right)^{2}} = \\sqrt{\\frac{45}{25}} = \\frac{3}{\\sqrt{5}}, \\quad q_{2} = \\frac{u_{2}}{r_{22}} = \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix}.\n$$\nThus\n$$\nQ = \\begin{pmatrix} \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} \\sqrt{5} & -\\frac{4}{\\sqrt{5}} \\\\ 0 & \\frac{3}{\\sqrt{5}} \\end{pmatrix}.\n$$\n\nCompute $A_{1} = R Q$:\n$$\nR Q = \\begin{pmatrix} \\sqrt{5} & -\\frac{4}{\\sqrt{5}} \\\\ 0 & \\frac{3}{\\sqrt{5}} \\end{pmatrix}\n\\begin{pmatrix} \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} \\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{14}{5} & -\\frac{3}{5} \\\\\n-\\frac{3}{5} & \\frac{6}{5}\n\\end{pmatrix}.\n$$\n\nNow compute traces and determinants.\nFor $A_{0}$:\n$$\n\\operatorname{tr}(A_{0}) = 2 + 2 = 4, \\quad \\det(A_{0}) = 2\\cdot 2 - (-1)\\cdot(-1) = 4 - 1 = 3.\n$$\nFor $A_{1}$:\n$$\n\\operatorname{tr}(A_{1}) = \\frac{14}{5} + \\frac{6}{5} = \\frac{20}{5} = 4, \\quad\n\\det(A_{1}) = \\frac{14}{5}\\cdot \\frac{6}{5} - \\left(-\\frac{3}{5}\\right)^{2} = \\frac{84}{25} - \\frac{9}{25} = \\frac{75}{25} = 3.\n$$\nAs a consistency check, since $A_{1} = Q^{T} A_{0} Q$ for orthogonal $Q$, trace and determinant are invariants and must match those of $A_{0}$, which they do. Therefore, the requested $1 \\times 4$ row matrix is $\\begin{pmatrix} 4 & 3 & 4 & 3 \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} 4 & 3 & 4 & 3 \\end{pmatrix}}$$", "id": "1397703"}, {"introduction": "While the basic QR algorithm is elegant, its convergence can be slow. To make it a practical tool, we introduce shifts to accelerate the process, a crucial enhancement used in all modern implementations. This practice problem has you perform one step of the shifted QR algorithm, using a simple but effective shift strategy to see how this modification changes the calculation [@problem_id:1397701].", "problem": "The shifted QR algorithm is an iterative method used to find the eigenvalues of a matrix. A single iteration, starting from a matrix $A_k$, is defined by the following sequence of operations:\n1. A shift, $\\sigma_k$, is chosen based on the entries of $A_k$.\n2. The QR decomposition of the shifted matrix $A_k - \\sigma_k I$ is computed, yielding $A_k - \\sigma_k I = Q_k R_k$. Here, $I$ is the identity matrix, $Q_k$ is an orthogonal matrix, and $R_k$ is an upper triangular matrix whose diagonal entries are non-negative.\n3. The next matrix in the sequence, $A_{k+1}$, is then calculated as $A_{k+1} = R_k Q_k + \\sigma_k I$.\n\nThis process generates a sequence of matrices $A_0, A_1, A_2, \\dots$ that are similar to $A_0$ and, under certain conditions, converge to an upper triangular form, revealing the eigenvalues on the diagonal.\n\nConsider the initial matrix:\n$$\nA_0 = \\begin{pmatrix} 5 & -2 \\\\ 4 & -1 \\end{pmatrix}\n$$\nFor the first step of the algorithm, we will use the simple Rayleigh quotient shift, which is defined as the bottom-right entry of the current matrix. Thus, the shift is $\\sigma_0 = (A_0)_{22}$.\n\nFollowing the procedure described, determine the first shifted iterate, the matrix $A_1$.", "solution": "The problem asks for the first iterate, $A_1$, of the shifted QR algorithm applied to the matrix $A_0 = \\begin{pmatrix} 5 & -2 \\\\ 4 & -1 \\end{pmatrix}$.\n\nStep 1: Determine the shift $\\sigma_0$.\nThe shift is given as the bottom-right entry of $A_0$.\n$$\n\\sigma_0 = (A_0)_{22} = -1\n$$\n\nStep 2: Form the shifted matrix $A_0 - \\sigma_0 I$.\n$$\nA_0 - \\sigma_0 I = \\begin{pmatrix} 5 & -2 \\\\ 4 & -1 \\end{pmatrix} - (-1)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 5 & -2 \\\\ 4 & -1 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 6 & -2 \\\\ 4 & 0 \\end{pmatrix}\n$$\n\nStep 3: Find the QR decomposition of the shifted matrix, $A_0 - \\sigma_0 I = Q_0 R_0$.\nLet $B = A_0 - \\sigma_0 I = \\begin{pmatrix} 6 & -2 \\\\ 4 & 0 \\end{pmatrix}$. We will find the QR decomposition of $B$ using the Gram-Schmidt process on its column vectors, $b_1 = \\begin{pmatrix} 6 \\\\ 4 \\end{pmatrix}$ and $b_2 = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$. The columns of $Q_0$ will be an orthonormal basis $\\{q_1, q_2\\}$.\n\nFirst, we find $q_1$. The first vector of the orthogonal basis is $u_1 = b_1 = \\begin{pmatrix} 6 \\\\ 4 \\end{pmatrix}$.\nIts norm is $\\|u_1\\| = \\sqrt{6^2 + 4^2} = \\sqrt{36 + 16} = \\sqrt{52} = 2\\sqrt{13}$.\nThe first orthonormal vector is $q_1$:\n$$\nq_1 = \\frac{u_1}{\\|u_1\\|} = \\frac{1}{2\\sqrt{13}}\\begin{pmatrix} 6 \\\\ 4 \\end{pmatrix} = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n$$\n\nNext, we find $q_2$. We first find a vector $u_2$ orthogonal to $q_1$ by subtracting the projection of $b_2$ onto $q_1$:\n$$\nu_2 = b_2 - (q_1^T b_2)q_1\n$$\nThe projection term is:\n$$\nq_1^T b_2 = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 & 2 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} = \\frac{-6}{\\sqrt{13}}\n$$\nSo, $u_2$ is:\n$$\nu_2 = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} - \\left(\\frac{-6}{\\sqrt{13}}\\right) \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + \\frac{6}{13}\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 + \\frac{18}{13} \\\\ \\frac{12}{13} \\end{pmatrix} = \\begin{pmatrix} -\\frac{8}{13} \\\\ \\frac{12}{13} \\end{pmatrix}\n$$\nThe norm of $u_2$ is $\\|u_2\\| = \\sqrt{\\left(-\\frac{8}{13}\\right)^2 + \\left(\\frac{12}{13}\\right)^2} = \\frac{1}{13}\\sqrt{64 + 144} = \\frac{\\sqrt{208}}{13} = \\frac{\\sqrt{16 \\times 13}}{13} = \\frac{4\\sqrt{13}}{13}$.\nThe second orthonormal vector is $q_2$:\n$$\nq_2 = \\frac{u_2}{\\|u_2\\|} = \\frac{13}{4\\sqrt{13}}\\begin{pmatrix} -\\frac{8}{13} \\\\ \\frac{12}{13} \\end{pmatrix} = \\frac{1}{4\\sqrt{13}}\\begin{pmatrix} -8 \\\\ 12 \\end{pmatrix} = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} -2 \\\\ 3 \\end{pmatrix}\n$$\nThe orthogonal matrix $Q_0$ is formed by the columns $q_1$ and $q_2$:\n$$\nQ_0 = \\begin{pmatrix} q_1 & q_2 \\end{pmatrix} = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 & -2 \\\\ 2 & 3 \\end{pmatrix}\n$$\nThe upper triangular matrix $R_0$ can be found using $R_0 = Q_0^T (A_0 - \\sigma_0 I)$:\n$$\nQ_0^T = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 & 2 \\\\ -2 & 3 \\end{pmatrix}\n$$\n$$\nR_0 = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 & 2 \\\\ -2 & 3 \\end{pmatrix} \\begin{pmatrix} 6 & -2 \\\\ 4 & 0 \\end{pmatrix} = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3(6)+2(4) & 3(-2)+2(0) \\\\ -2(6)+3(4) & -2(-2)+3(0) \\end{pmatrix} = \\frac{1}{\\sqrt{13}}\\begin{pmatrix} 26 & -6 \\\\ 0 & 4 \\end{pmatrix}\n$$\nThe diagonal entries, $26/\\sqrt{13}$ and $4/\\sqrt{13}$, are non-negative, as required.\n\nStep 4: Compute the next matrix iterate, $A_1 = R_0 Q_0 + \\sigma_0 I$.\nFirst, we compute the product $R_0 Q_0$:\n$$\nR_0 Q_0 = \\left(\\frac{1}{\\sqrt{13}}\\begin{pmatrix} 26 & -6 \\\\ 0 & 4 \\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{13}}\\begin{pmatrix} 3 & -2 \\\\ 2 & 3 \\end{pmatrix}\\right) = \\frac{1}{13}\\begin{pmatrix} 26(3)+(-6)(2) & 26(-2)+(-6)(3) \\\\ 0(3)+4(2) & 0(-2)+4(3) \\end{pmatrix}\n$$\n$$\nR_0 Q_0 = \\frac{1}{13}\\begin{pmatrix} 78-12 & -52-18 \\\\ 8 & 12 \\end{pmatrix} = \\frac{1}{13}\\begin{pmatrix} 66 & -70 \\\\ 8 & 12 \\end{pmatrix}\n$$\nNow, we add back the shifted term $\\sigma_0 I$:\n$$\nA_1 = R_0 Q_0 + \\sigma_0 I = \\frac{1}{13}\\begin{pmatrix} 66 & -70 \\\\ 8 & 12 \\end{pmatrix} + (-1)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nA_1 = \\begin{pmatrix} \\frac{66}{13} & -\\frac{70}{13} \\\\ \\frac{8}{13} & \\frac{12}{13} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{66}{13} - 1 & -\\frac{70}{13} \\\\ \\frac{8}{13} & \\frac{12}{13} - 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{13} & -\\frac{70}{13} \\\\ \\frac{8}{13} & -\\frac{1}{13} \\end{pmatrix}\n$$\nThus, the first shifted iterate is the matrix $A_1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{53}{13} & -\\frac{70}{13} \\\\ \\frac{8}{13} & -\\frac{1}{13} \\end{pmatrix}}\n$$", "id": "1397701"}, {"introduction": "The QR algorithm is an infinite process, so how do we use it in the finite world of computers? The answer lies in 'deflation,' where we treat sufficiently small subdiagonal elements as zero, breaking the problem into smaller pieces. This exercise moves from pure theory to the practical art of numerical computation, asking you to identify the most robust criterion for deciding when an element is negligible, a critical choice for the stability and efficiency of any real-world implementation [@problem_id:1397748].", "problem": "In the field of numerical linear algebra, the QR algorithm is a fundamental iterative method for computing the eigenvalues of a general square matrix $A$. The process begins by setting $A^{(0)} = A$. Then, for each step $k=0, 1, 2, \\dots$, one computes the QR decomposition of the current matrix, $A^{(k)} = Q^{(k)}R^{(k)}$, where $Q^{(k)}$ is an orthogonal matrix and $R^{(k)}$ is an upper triangular matrix. The next matrix in the sequence is then formed by reversing the order of multiplication: $A^{(k+1)} = R^{(k)}Q^{(k)}$. This new matrix $A^{(k+1)}$ is orthogonally similar to $A^{(k)}$ and thus has the same eigenvalues.\n\nFor a general nonsymmetric matrix, the sequence $\\{A^{(k)}\\}$ converges to an upper triangular (or quasi-triangular) form, with the eigenvalues appearing on the diagonal. However, the convergence is infinite. In a practical implementation, the algorithm is accelerated by a process called \"deflation.\" When a subdiagonal element $a_{i, i-1}^{(k)}$ becomes sufficiently small, it is treated as zero. This effectively decouples the matrix into two smaller, independent subproblems, significantly reducing the computational cost.\n\nThe choice of the stopping criterion for deciding when $a_{i, i-1}^{(k)}$ is \"small enough\" is critical for the stability and efficiency of the algorithm. Let $\\epsilon_{\\text{mach}}$ be the machine epsilon, which is the smallest positive floating-point number such that $1.0 + \\epsilon_{\\text{mach}}$ is computationally distinct from `1.0`. For a general $n \\times n$ matrix $A$, which of the following criteria for zeroing out the subdiagonal element $a_{i, i-1}^{(k)}$ is the most robust and widely accepted in high-quality numerical software?\n\nA. $|a_{i, i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} (|a_{i,i}^{(k)}| + |a_{i-1, i-1}^{(k)}|)$\n\nB. $|a_{i, i-1}^{(k)}| \\le \\epsilon_{\\text{mach}}$\n\nC. $|a_{i, i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot \\|A^{(0)}\\|_F$, where $\\| \\cdot \\|_F$ is the Frobenius norm of the original matrix.\n\nD. $|a_{i, i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot |a_{i-1, i-1}^{(k)}|$\n\nE. $|a_{i, i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot \\max_{j,l} |a_{jl}^{(k)}|$", "solution": "We consider the QR algorithm applied to a general nonsymmetric matrix, where the iterates $A^{(k)}$ are (implicitly) kept in upper Hessenberg form, and deflation occurs when a subdiagonal element $a_{i,i-1}^{(k)}$ is deemed negligible relative to the local scale of the $2 \\times 2$ diagonal block it couples. A robust deflation test should satisfy the following principles:\n\n1) Scale invariance: If $A$ is scaled by a nonzero scalar $\\alpha$, the decision to deflate should be unchanged. Formally, replacing $A^{(k)}$ by $\\alpha A^{(k)}$ scales both $a_{i,i-1}^{(k)}$ and the diagonal entries $a_{i-1,i-1}^{(k)}$, $a_{i,i}^{(k)}$ by $|\\alpha|$. Thus a sound test should compare $|a_{i,i-1}^{(k)}|$ to a quantity that scales by the same factor $|\\alpha|$, ensuring homogeneity.\n\n2) Locality: The coupling $a_{i,i-1}^{(k)}$ connects the local $2 \\times 2$ diagonal block. Therefore, the threshold should be determined by the local scale set by $|a_{i-1,i-1}^{(k)}|$ and $|a_{i,i}^{(k)}|$, not by unrelated entries elsewhere or by the initial matrix.\n\n3) Backward stability and common practice: High-quality implementations adopt a relative test that treats $a_{i,i-1}^{(k)}$ as negligible if it is on the order of machine roundoff times the local scale of the adjacent diagonal entries. Symbolically, this is of the form\n$$\n|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\big(|a_{i-1,i-1}^{(k)}| + |a_{i,i}^{(k)}|\\big).\n$$\n\nWe now examine each option against these principles:\n\n- Option A: $|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\big(|a_{i,i}^{(k)}| + |a_{i-1,i-1}^{(k)}|\\big)$. This is scale invariant: both sides scale by $|\\alpha|$ under $A^{(k)} \\mapsto \\alpha A^{(k)}$. It is local: it uses only the neighboring diagonal magnitudes that define the relevant $2 \\times 2$ scale. It is the standard criterion used in high-quality software, often with minor safeguards for underflow.\n\n- Option B: $|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}}$. This is not scale invariant. Under large scaling, the left-hand side scales but the right-hand side does not, making the test fail spuriously; under small scaling, it may deflate too aggressively. Hence it is not robust.\n\n- Option C: $|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot \\|A^{(0)}\\|_{F}$. This uses a fixed global scale from the initial matrix, not reflecting the local and current scale in $A^{(k)}$. It is neither strictly local nor adaptive to changes during the iteration, leading to potential instability or inefficiency.\n\n- Option D: $|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot |a_{i-1,i-1}^{(k)}|$. This is scale invariant but not symmetric with respect to the two coupled diagonal entries. When $|a_{i,i}^{(k)}|$ is much larger or smaller than $|a_{i-1,i-1}^{(k)}|$, the test can be too strict or too lenient. The symmetric sum in Option A is preferred.\n\n- Option E: $|a_{i,i-1}^{(k)}| \\le \\epsilon_{\\text{mach}} \\cdot \\max_{j,l} |a_{jl}^{(k)}|$. This is scale invariant but not local; it depends on the global maximum entry of $A^{(k)}$, which can be dominated by unrelated parts of the matrix, leading to delayed or premature deflation.\n\nBy the criteria of scale invariance, locality, and alignment with widely accepted implementations, Option A is the most robust and standard deflation test.", "answer": "$$\\boxed{A}$$", "id": "1397748"}]}