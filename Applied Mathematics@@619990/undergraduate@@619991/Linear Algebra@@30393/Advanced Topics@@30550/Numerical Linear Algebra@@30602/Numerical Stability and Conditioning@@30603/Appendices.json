{"hands_on_practices": [{"introduction": "Understanding a matrix's condition number begins with mastering its calculation. This exercise provides direct, hands-on practice in applying the definition $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$ using two of the most common matrix norms: the $L_1$-norm and the $L_\\infty$-norm [@problem_id:1379501]. By working through these concrete calculations, you will solidify your understanding of how the condition number is determined from its fundamental definition.", "problem": "In numerical linear algebra, the condition number of a matrix provides a measure of how sensitive the solution of a linear system $Ax=b$ is to perturbations in the input data $A$ and $b$. A high condition number indicates that the problem is ill-conditioned, meaning small relative errors in the input can lead to large relative errors in the output solution.\n\nThe condition number of a non-singular square matrix $A$ with respect to a given matrix norm $\\|\\cdot\\|$ is defined as $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$.\n\nFor a matrix $M$ with elements $m_{ij}$, the $L_1$-norm and $L_\\infty$-norm are defined as:\n- a) The $L_1$-norm, or maximum absolute column sum norm: $\\|M\\|_1 = \\max_{j} \\sum_{i} |m_{ij}|$\n- b) The $L_\\infty$-norm, or maximum absolute row sum norm: $\\|M\\|_\\infty = \\max_{i} \\sum_{j} |m_{ij}|$\n\nConsider the non-symmetric $2 \\times 2$ matrix $A$ given by:\n$$\nA = \\begin{pmatrix} -2 & 1 \\\\ 5 & -10 \\end{pmatrix}\n$$\n\nCalculate the condition number of $A$ with respect to the $L_1$-norm, denoted $\\kappa_1(A)$, and the condition number with respect to the $L_\\infty$-norm, denoted $\\kappa_\\infty(A)$.\n\nPresent your final answer as an ordered pair $(\\kappa_1(A), \\kappa_\\infty(A))$, with each value rounded to three significant figures.", "solution": "We use the definition of the condition number with respect to a norm $\\|\\cdot\\|$: $\\kappa(A)=\\|A\\|\\,\\|A^{-1}\\|$. The matrix is $A=\\begin{pmatrix}-2 & 1 \\\\ 5 & -10\\end{pmatrix}$.\n\nFirst, compute the $L_{1}$-norm of $A$, which is the maximum absolute column sum:\n$$\n\\|A\\|_{1}=\\max\\left\\{|-2|+|5|,\\;|1|+|-10|\\right\\}=\\max\\{7,11\\}=11.\n$$\n\nNext, compute the $L_{\\infty}$-norm of $A$, which is the maximum absolute row sum:\n$$\n\\|A\\|_{\\infty}=\\max\\left\\{|-2|+|1|,\\;|5|+|-10|\\right\\}=\\max\\{3,15\\}=15.\n$$\n\nCompute $A^{-1}$. For a $2\\times 2$ matrix $A=\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$, $A^{-1}=\\frac{1}{ad-bc}\\begin{pmatrix}d & -b \\\\ -c & a\\end{pmatrix}$. Here $a=-2$, $b=1$, $c=5$, $d=-10$, so\n$$\n\\det(A)=ad-bc=(-2)(-10)-(1)(5)=20-5=15,\n$$\nand\n$$\nA^{-1}=\\frac{1}{15}\\begin{pmatrix}-10 & -1 \\\\ -5 & -2\\end{pmatrix}.\n$$\n\nCompute the $L_{1}$-norm of $A^{-1}$:\n$$\n\\|A^{-1}\\|_{1}=\\max\\left\\{\\left|\\frac{-10}{15}\\right|+\\left|\\frac{-5}{15}\\right|,\\;\\left|\\frac{-1}{15}\\right|+\\left|\\frac{-2}{15}\\right|\\right\\}\n=\\max\\left\\{\\frac{10+5}{15},\\;\\frac{1+2}{15}\\right\\}=\\max\\left\\{1,\\;\\frac{1}{5}\\right\\}=1.\n$$\nThus,\n$$\n\\kappa_{1}(A)=\\|A\\|_{1}\\,\\|A^{-1}\\|_{1}=11\\cdot 1=11.\n$$\n\nCompute the $L_{\\infty}$-norm of $A^{-1}$:\n$$\n\\|A^{-1}\\|_{\\infty}=\\max\\left\\{\\left|\\frac{-10}{15}\\right|+\\left|\\frac{-1}{15}\\right|,\\;\\left|\\frac{-5}{15}\\right|+\\left|\\frac{-2}{15}\\right|\\right\\}\n=\\max\\left\\{\\frac{10+1}{15},\\;\\frac{5+2}{15}\\right\\}=\\max\\left\\{\\frac{11}{15},\\;\\frac{7}{15}\\right\\}=\\frac{11}{15}.\n$$\nTherefore,\n$$\n\\kappa_{\\infty}(A)=\\|A\\|_{\\infty}\\,\\|A^{-1}\\|_{\\infty}=15\\cdot \\frac{11}{15}=11.\n$$\n\nRounded to three significant figures, both values are $11.0$. The ordered pair $(\\kappa_{1}(A),\\kappa_{\\infty}(A))$ is thus $(11.0,11.0)$.", "answer": "$$\\boxed{(11.0, 11.0)}$$", "id": "1379501"}, {"introduction": "The condition number isn't just an abstract value; it's a vital indicator of a problem's sensitivity. This practice explores a crucial question: how can a well-behaved system become ill-conditioned? By introducing a small scaling error into an otherwise simple matrix, you will witness firsthand how the condition number can skyrocket, revealing the system's newfound sensitivity to perturbations [@problem_id:1379479]. This thought experiment models real-world issues like measurement errors and highlights why identifying ill-conditioned problems is critical for reliable computation.", "problem": "In computational linear algebra, the sensitivity of a system's solution to small changes in its input data is a critical concern. This sensitivity is quantified by the condition number. Consider a scenario where a system of linear equations is modeled by the matrix $A$ given by\n$$ A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} $$\nDue to a measurement scaling error, the coefficients in the second equation (corresponding to the second row of the matrix) are inadvertently multiplied by a very small factor. This results in a new, perturbed matrix $A_{\\epsilon}$, where the second row of $A$ is multiplied by the constant $\\epsilon = 1.0 \\times 10^{-6}$.\n\nThe condition number of a non-singular square matrix $M$ with respect to the infinity-norm, denoted $\\text{cond}_{\\infty}(M)$, is defined as $\\text{cond}_{\\infty}(M) = \\|M\\|_{\\infty} \\|M^{-1}\\|_{\\infty}$, where $\\|M\\|_{\\infty}$ is the maximum absolute row sum of $M$.\n\nCalculate the condition number $\\text{cond}_{\\infty}(A_{\\epsilon})$. Round your final answer to four significant figures.", "solution": "We are given $A=\\begin{pmatrix}1 & 2 \\\\ 3 & 4\\end{pmatrix}$ and the perturbed matrix $A_{\\epsilon}$ obtained by multiplying the second row by $\\epsilon=1.0 \\times 10^{-6}$:\n$$\nA_{\\epsilon}=\\begin{pmatrix}1 & 2 \\\\ 3\\epsilon & 4\\epsilon\\end{pmatrix}.\n$$\nThe infinity-norm condition number is defined by $\\text{cond}_{\\infty}(M)=\\|M\\|_{\\infty}\\|M^{-1}\\|_{\\infty}$, where $\\|M\\|_{\\infty}$ is the maximum absolute row sum.\n\nFirst, compute $\\|A_{\\epsilon}\\|_{\\infty}$. The absolute row sums are\n$$\n|1|+|2|=3,\\qquad |3\\epsilon|+|4\\epsilon|=7\\epsilon.\n$$\nTherefore,\n$$\n\\|A_{\\epsilon}\\|_{\\infty}=\\max\\{3,7\\epsilon\\}.\n$$\nSince $\\epsilon=1.0\\times 10^{-6}$ and $7\\epsilon \\ll 3$, we have $\\|A_{\\epsilon}\\|_{\\infty}=3$.\n\nNext, compute $A_{\\epsilon}^{-1}$. For a $2\\times 2$ matrix $\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix}d & -b \\\\ -c & a\\end{pmatrix}$. Here $a=1$, $b=2$, $c=3\\epsilon$, $d=4\\epsilon$, so\n$$\n\\det(A_{\\epsilon})=ad-bc=4\\epsilon-6\\epsilon=-2\\epsilon,\n$$\nand\n$$\nA_{\\epsilon}^{-1}=\\frac{1}{-2\\epsilon}\\begin{pmatrix}4\\epsilon & -2 \\\\ -3\\epsilon & 1\\end{pmatrix}\n=\\begin{pmatrix}-2 & \\frac{1}{\\epsilon} \\\\ \\frac{3}{2} & -\\frac{1}{2\\epsilon}\\end{pmatrix}.\n$$\nCompute $\\|A_{\\epsilon}^{-1}\\|_{\\infty}$ via row sums:\n$$\n|-2|+\\left|\\frac{1}{\\epsilon}\\right|=2+\\frac{1}{\\epsilon},\\qquad \\left|\\frac{3}{2}\\right|+\\left|-\\frac{1}{2\\epsilon}\\right|=\\frac{3}{2}+\\frac{1}{2\\epsilon}.\n$$\nFor $\\epsilon>0$, $2+\\frac{1}{\\epsilon}\\geq \\frac{3}{2}+\\frac{1}{2\\epsilon}$, hence\n$$\n\\|A_{\\epsilon}^{-1}\\|_{\\infty}=2+\\frac{1}{\\epsilon}.\n$$\nThus,\n$$\n\\text{cond}_{\\infty}(A_{\\epsilon})=\\|A_{\\epsilon}\\|_{\\infty}\\,\\|A_{\\epsilon}^{-1}\\|_{\\infty}=3\\left(2+\\frac{1}{\\epsilon}\\right).\n$$\nSubstituting $\\epsilon=1.0\\times 10^{-6}$ gives\n$$\n\\text{cond}_{\\infty}(A_{\\epsilon})=3\\left(2+1.0\\times 10^{6}\\right)=3{,}000{,}006.\n$$\nRounding to four significant figures yields $3.000\\times 10^{6}$.", "answer": "$$\\boxed{3.000 \\times 10^{6}}$$", "id": "1379479"}, {"introduction": "While an ill-conditioned matrix signals potential trouble, the stability of our chosen algorithm determines whether we get a meaningful answer. This exercise moves from theory to practice by simulating how a computer with finite precision solves a linear system [@problem_id:1379519]. You will compare the results of Gaussian elimination with and without partial pivoting to see how a simple algorithmic strategy can prevent catastrophic errors caused by round-off, even when dealing with an ill-conditioned matrix. This practice provides a powerful demonstration of why numerically stable algorithms are essential tools in computational science.", "problem": "A numerical analyst is studying the effects of finite-precision arithmetic on solving systems of linear equations. The analyst considers the system $A\\mathbf{x} = \\mathbf{b}$, where:\n$$\nA = \\begin{pmatrix} \\epsilon & 1.00 \\\\ 1.00 & 1.00 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1.01 \\\\ 3.00 \\end{pmatrix}\n$$\nand the parameter $\\epsilon$ is a small number, given as $\\epsilon = 1.00 \\times 10^{-4}$.\n\nThe analyst uses a simplified computer model that performs all arithmetic operations (addition, subtraction, multiplication, and division) using **three-significant-figure floating-point arithmetic with rounding**. This means that after each operation, the result is rounded to three significant figures. For example, a calculation resulting in $2.458$ would be stored as $2.46$, and a result of $0.07891$ would be stored as $0.0789$.\n\nYour task is to compare two methods for solving this system under these arithmetic constraints.\n\nFirst, solve the system using Gaussian elimination **without partial pivoting**. Let the computed solution vector be $\\mathbf{x}_{\\text{no-pivot}} = (x_{1, \\text{np}}, x_{2, \\text{np}})^T$.\n\nSecond, solve the same system using Gaussian elimination **with partial pivoting**. In this method, before each elimination step, rows are swapped to ensure the pivot element (the diagonal element used for elimination) has the largest possible absolute value in its column among the rows not yet processed. Let this computed solution vector be $\\mathbf{x}_{\\text{pivot}} = (x_{1, \\text{p}}, x_{2, \\text{p}})^T$.\n\nFinally, calculate the Euclidean norm ($L_2$ norm) of the difference between the two computed solutions: $\\|\\mathbf{x}_{\\text{pivot}} - \\mathbf{x}_{\\text{no-pivot}}\\|_2$.\n\nRound your final answer to three significant figures.", "solution": "We are asked to solve the linear system $A\\mathbf{x} = \\mathbf{b}$ in two ways using three-significant-figure floating-point arithmetic and then find the Euclidean norm of the difference between the two solutions. The system is defined by $\\epsilon = 1.00 \\times 10^{-4}$:\n$$\n\\begin{pmatrix} 1.00 \\times 10^{-4} & 1.00 \\\\ 1.00 & 1.00 \\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n=\n\\begin{pmatrix} 1.01 \\\\ 3.00 \\end{pmatrix}\n$$\n\n**Part 1: Solution with Gaussian Elimination without Partial Pivoting**\n\nWe start with the augmented matrix $[A|\\mathbf{b}]$:\n$$\n\\left[\n\\begin{array}{cc|c}\n1.00 \\times 10^{-4} & 1.00 & 1.01 \\\\\n1.00 & 1.00 & 3.00\n\\end{array}\n\\right]\n$$\nThe goal is to eliminate the entry in the second row, first column. The pivot is $a_{11} = 1.00 \\times 10^{-4}$. We compute the multiplier for the second row:\n$$\nm_{21} = \\frac{a_{21}}{a_{11}} = \\frac{1.00}{1.00 \\times 10^{-4}} = 1.00 \\times 10^{4}\n$$\nThis calculation is exact in our 3-SF arithmetic. Now we perform the row operation $R_2 \\leftarrow R_2 - m_{21} R_1$. We must round each new entry in the second row to 3 significant figures.\n\nThe new entry $a'_{22}$ is:\n$$\na'_{22} = a_{22} - m_{21} \\times a_{12} = 1.00 - (1.00 \\times 10^{4}) \\times 1.00 = 1.00 - 10000 = -9999\n$$\nRounding to 3 significant figures, we get $a'_{22} = -1.00 \\times 10^{4}$.\n\nThe new entry $b'_{2}$ is:\n$$\nb'_{2} = b_2 - m_{21} \\times b_1 = 3.00 - (1.00 \\times 10^{4}) \\times 1.01 = 3.00 - 10100 = -10097\n$$\nRounding to 3 significant figures, we get $b'_{2} = -1.01 \\times 10^{4}$.\n\nThe augmented matrix becomes:\n$$\n\\left[\n\\begin{array}{cc|c}\n1.00 \\times 10^{-4} & 1.00 & 1.01 \\\\\n0 & -1.00 \\times 10^{4} & -1.01 \\times 10^{4}\n\\end{array}\n\\right]\n$$\nNow we perform back substitution. From the second row:\n$$\n(-1.00 \\times 10^{4}) x_{2, \\text{np}} = -1.01 \\times 10^{4}\n$$\n$$\nx_{2, \\text{np}} = \\frac{-1.01 \\times 10^{4}}{-1.00 \\times 10^{4}} = 1.01\n$$\nThis is exact in 3-SF arithmetic. Now, using the first row:\n$$\n(1.00 \\times 10^{-4}) x_{1, \\text{np}} + 1.00 \\times x_{2, \\text{np}} = 1.01\n$$\n$$\n(1.00 \\times 10^{-4}) x_{1, \\text{np}} + 1.00 \\times 1.01 = 1.01\n$$\nThe term $1.00 \\times 1.01$ is $1.01$ (exact in 3-SF).\n$$\n(1.00 \\times 10^{-4}) x_{1, \\text{np}} + 1.01 = 1.01\n$$\n$$\n(1.00 \\times 10^{-4}) x_{1, \\text{np}} = 1.01 - 1.01 = 0\n$$\nTherefore, $x_{1, \\text{np}} = 0$. The solution without pivoting is $\\mathbf{x}_{\\text{no-pivot}} = \\begin{pmatrix} 0 \\\\ 1.01 \\end{pmatrix}$.\n\n**Part 2: Solution with Gaussian Elimination with Partial Pivoting**\n\nWe start again with the augmented matrix:\n$$\n\\left[\n\\begin{array}{cc|c}\n1.00 \\times 10^{-4} & 1.00 & 1.01 \\\\\n1.00 & 1.00 & 3.00\n\\end{array}\n\\right]\n$$\nFor partial pivoting, we compare the absolute values of the elements in the first column: $|a_{11}| = 1.00 \\times 10^{-4}$ and $|a_{21}| = 1.00$. Since $|a_{21}| > |a_{11}|$, we swap Row 1 and Row 2.\n$$\n\\left[\n\\begin{array}{cc|c}\n1.00 & 1.00 & 3.00 \\\\\n1.00 \\times 10^{-4} & 1.00 & 1.01\n\\end{array}\n\\right]\n$$\nNow the pivot is $a_{11} = 1.00$. We compute the multiplier:\n$$\nm_{21} = \\frac{a_{21}}{a_{11}} = \\frac{1.00 \\times 10^{-4}}{1.00} = 1.00 \\times 10^{-4}\n$$\nWe perform the row operation $R_2 \\leftarrow R_2 - m_{21} R_1$.\nThe new entry $a'_{22}$ is:\n$$\na'_{22} = a_{22} - m_{21} \\times a_{12} = 1.00 - (1.00 \\times 10^{-4}) \\times 1.00 = 1.00 - 0.0001 = 0.9999\n$$\nRounding to 3 significant figures, we get $a'_{22} = 1.00$.\n\nThe new entry $b'_{2}$ is:\n$$\nb'_{2} = b_2 - m_{21} \\times b_1 = 1.01 - (1.00 \\times 10^{-4}) \\times 3.00 = 1.01 - 0.0003 = 1.0097\n$$\nRounding to 3 significant figures, we get $b'_{2} = 1.01$.\n\nThe augmented matrix becomes:\n$$\n\\left[\n\\begin{array}{cc|c}\n1.00 & 1.00 & 3.00 \\\\\n0 & 1.00 & 1.01\n\\end{array}\n\\right]\n$$\nNow we perform back substitution. From the second row:\n$$\n1.00 \\times x_{2, \\text{p}} = 1.01 \\implies x_{2, \\text{p}} = 1.01\n$$\nFrom the first row:\n$$\n1.00 \\times x_{1, \\text{p}} + 1.00 \\times x_{2, \\text{p}} = 3.00\n$$\n$$\n1.00 \\times x_{1, \\text{p}} + 1.00 \\times 1.01 = 3.00\n$$\n$$\n1.00 \\times x_{1, \\text{p}} + 1.01 = 3.00\n$$\n$$\n1.00 \\times x_{1, \\text{p}} = 3.00 - 1.01 = 1.99\n$$\nTherefore, $x_{1, \\text{p}} = 1.99$. The solution with pivoting is $\\mathbf{x}_{\\text{pivot}} = \\begin{pmatrix} 1.99 \\\\ 1.01 \\end{pmatrix}$.\n\n**Part 3: Euclidean Norm of the Difference**\n\nWe need to calculate $\\|\\mathbf{x}_{\\text{pivot}} - \\mathbf{x}_{\\text{no-pivot}}\\|_2$.\nFirst, find the difference vector:\n$$\n\\mathbf{d} = \\mathbf{x}_{\\text{pivot}} - \\mathbf{x}_{\\text{no-pivot}} = \\begin{pmatrix} 1.99 \\\\ 1.01 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.01 \\end{pmatrix} = \\begin{pmatrix} 1.99 - 0 \\\\ 1.01 - 1.01 \\end{pmatrix} = \\begin{pmatrix} 1.99 \\\\ 0 \\end{pmatrix}\n$$\nNow, calculate the Euclidean norm of $\\mathbf{d}$:\n$$\n\\|\\mathbf{d}\\|_2 = \\sqrt{(1.99)^2 + (0)^2} = \\sqrt{(1.99)^2} = 1.99\n$$\nThe result $1.99$ is already in three significant figures.", "answer": "$$\\boxed{1.99}$$", "id": "1379519"}]}