{"hands_on_practices": [{"introduction": "Let's begin by mastering the fundamental computations of the most common induced matrix norms. This exercise uses a diagonal matrix, which represents a simple scaling transformation in geometry, providing a clear and intuitive context. By calculating the induced 1-norm, 2-norm, and $\\infty$-norm, you will gain a practical understanding of how these norms measure the maximum \"stretching\" effect a matrix can have on a vector [@problem_id:1376555].", "problem": "In two-dimensional computer graphics, a scaling transformation is often represented by a matrix. Consider a non-uniform scaling transformation in a 2D Cartesian coordinate system represented by the matrix $S$ given by:\n$$\nS = \\begin{pmatrix} 4.5 & 0 \\\\ 0 & 2.1 \\end{pmatrix}\n$$\nThis matrix scales the x-component of a vector by a factor of $4.5$ and the y-component by a factor of $2.1$.\n\nFor this scaling matrix $S$, compute its induced 1-norm $\\|S\\|_1$, its induced 2-norm (also known as the spectral norm) $\\|S\\|_2$, and its induced $\\infty$-norm $\\|S\\|_\\infty$. Select the option that correctly lists all three values.\n\nA. $\\|S\\|_1 = 6.6$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 6.6$\n\nB. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 4.5$\n\nC. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.5$, $\\|S\\|_\\infty = 4.5$\n\nD. $\\|S\\|_1 = 2.1$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 2.1$\n\nE. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 4.5$", "solution": "We are given the diagonal matrix\n$$\nS=\\begin{pmatrix}4.5 & 0 \\\\ 0 & 2.1\\end{pmatrix}.\n$$\nFor the induced 1-norm, by definition,\n$$\n\\|S\\|_{1}=\\max_{j}\\sum_{i}|a_{ij}|,\n$$\nwhich is the maximum absolute column sum. The column sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{1}=4.5.\n$$\n\nFor the induced infinity-norm, by definition,\n$$\n\\|S\\|_{\\infty}=\\max_{i}\\sum_{j}|a_{ij}|,\n$$\nwhich is the maximum absolute row sum. The row sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{\\infty}=4.5.\n$$\n\nFor the induced 2-norm (spectral norm), by definition,\n$$\n\\|S\\|_{2}=\\sqrt{\\lambda_{\\max}(S^{\\mathsf{T}}S)}.\n$$\nSince $S$ is diagonal, we have\n$$\nS^{\\mathsf{T}}S=\\begin{pmatrix}4.5^{2} & 0 \\\\ 0 & 2.1^{2}\\end{pmatrix},\n$$\nwhose eigenvalues are $4.5^{2}$ and $2.1^{2}$. Therefore,\n$$\n\\|S\\|_{2}=\\sqrt{\\max\\{4.5^{2},\\,2.1^{2}\\}}=4.5.\n$$\n\nThus,\n$$\n\\|S\\|_{1}=4.5,\\quad \\|S\\|_{2}=4.5,\\quad \\|S\\|_{\\infty}=4.5,\n$$\nwhich corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1376555"}, {"introduction": "Beyond simple calculation, it is crucial to understand the properties that define a useful matrix norm. One of the most important is submultiplicativity, the property that $\\|AB\\| \\le \\|A\\|\\|B\\|$, which is essential for analyzing error propagation and the convergence of iterative methods. This practice challenges you to verify this property for several candidate norms and identify a function that, despite appearing norm-like, fails this critical test [@problem_id:2186695].", "problem": "In numerical analysis, a function $\\|\\cdot\\|: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}$ is called a matrix norm on the space of $n \\times n$ real matrices if it satisfies the standard norm axioms: non-negativity ($\\|A\\| \\ge 0$), definiteness ($\\|A\\| = 0$ if and only if $A$ is the zero matrix), absolute homogeneity ($\\|\\alpha A\\| = |\\alpha| \\|A\\|$ for any scalar $\\alpha$), and the triangle inequality ($\\|A + B\\| \\le \\|A\\| + \\|B\\|$). A matrix norm is further defined as being **submultiplicative** if for any two matrices $A, B \\in \\mathbb{R}^{n \\times n}$, the inequality $\\|AB\\| \\le \\|A\\| \\|B\\|$ holds. This property is crucial for analyzing iterative methods and matrix condition numbers.\n\nConsider the following functions defined on the space of $n \\times n$ real matrices, where $A = [a_{ij}]$. Which of the following statements about these functions and the submultiplicative property is **FALSE**?\n\nA. The function $\\|A\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |a_{ij}|$ (the maximum absolute column sum) is a submultiplicative matrix norm for all $n \\ge 1$.\n\nB. The function $\\|A\\|_\\infty = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |a_{ij}|$ (the maximum absolute row sum) is a submultiplicative matrix norm for all $n \\ge 1$.\n\nC. The function $\\|A\\|_F = \\left(\\sum_{i=1}^n \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}$ (the Frobenius norm) is a submultiplicative matrix norm for all $n \\ge 1$.\n\nD. The function $\\|A\\|_{\\max} = \\max_{1 \\le i,j \\le n} |a_{ij}|$ (the maximum absolute element norm) is a submultiplicative matrix norm for all $n \\ge 2$.", "solution": "We analyze each statementâ€™s submultiplicativity.\n\nA. For the maximum absolute column sum, write $B=[b_{1}\\ \\cdots\\ b_{n}]$ with columns $b_{j}$. Then $AB=[A b_{1}\\ \\cdots\\ A b_{n}]$ and\n$$\n\\|AB\\|_{1}=\\max_{1\\leq j\\leq n}\\|A b_{j}\\|_{1}.\n$$\nFor any vector $x$,\n$$\n\\|A x\\|_{1}=\\sum_{i=1}^{n}\\left|\\sum_{k=1}^{n}a_{ik}x_{k}\\right|\n\\leq \\sum_{i=1}^{n}\\sum_{k=1}^{n}|a_{ik}|\\,|x_{k}|\n= \\sum_{k=1}^{n}\\left(\\sum_{i=1}^{n}|a_{ik}|\\right)|x_{k}|\n\\leq \\left(\\max_{1\\leq k\\leq n}\\sum_{i=1}^{n}|a_{ik}|\\right)\\sum_{k=1}^{n}|x_{k}|\n= \\|A\\|_{1}\\|x\\|_{1}.\n$$\nHence,\n$$\n\\|AB\\|_{1}=\\max_{j}\\|A b_{j}\\|_{1}\\leq \\|A\\|_{1}\\max_{j}\\|b_{j}\\|_{1}=\\|A\\|_{1}\\|B\\|_{1},\n$$\nso $\\|\\cdot\\|_{1}$ is submultiplicative. Statement A is true.\n\nB. For the maximum absolute row sum, compute for each $i$:\n$$\n\\sum_{j=1}^{n}|(AB)_{ij}|=\\sum_{j=1}^{n}\\left|\\sum_{k=1}^{n}a_{ik}b_{kj}\\right|\n\\leq \\sum_{j=1}^{n}\\sum_{k=1}^{n}|a_{ik}|\\,|b_{kj}|\n= \\sum_{k=1}^{n}|a_{ik}|\\left(\\sum_{j=1}^{n}|b_{kj}|\\right)\n\\leq \\left(\\sum_{k=1}^{n}|a_{ik}|\\right)\\left(\\max_{1\\leq k\\leq n}\\sum_{j=1}^{n}|b_{kj}|\\right).\n$$\nTaking the maximum over $i$ yields\n$$\n\\|AB\\|_{\\infty}\\leq \\|A\\|_{\\infty}\\|B\\|_{\\infty},\n$$\nso $\\|\\cdot\\|_{\\infty}$ is submultiplicative. Statement B is true.\n\nC. For the Frobenius norm, use vectorization: $\\operatorname{vec}(AB)=(I\\otimes A)\\operatorname{vec}(B)$ and $\\|\\operatorname{vec}(M)\\|_{2}=\\|M\\|_{F}$. Thus\n$$\n\\|AB\\|_{F}=\\|(I\\otimes A)\\operatorname{vec}(B)\\|_{2}\\leq \\|I\\otimes A\\|_{2}\\,\\|\\operatorname{vec}(B)\\|_{2}\n= \\|A\\|_{2}\\,\\|B\\|_{F}\\leq \\|A\\|_{F}\\,\\|B\\|_{F},\n$$\nsince $\\|I\\otimes A\\|_{2}=\\|A\\|_{2}$ and $\\|A\\|_{2}\\leq \\|A\\|_{F}$. Hence $\\|\\cdot\\|_{F}$ is submultiplicative. Statement C is true.\n\nD. For the maximum absolute element norm, observe that for any $i,j$,\n$$\n|(AB)_{ij}|=\\left|\\sum_{k=1}^{n}a_{ik}b_{kj}\\right|\\leq \\sum_{k=1}^{n}|a_{ik}|\\,|b_{kj}|\\leq n\\,\\|A\\|_{\\max}\\,\\|B\\|_{\\max},\n$$\nso only the weaker bound $\\|AB\\|_{\\max}\\leq n\\,\\|A\\|_{\\max}\\|B\\|_{\\max}$ holds in general. A concrete counterexample for $n\\geq 2$ is\n$$\nA=B=\\begin{pmatrix}1 & 1\\\\ 1 & 1\\end{pmatrix},\\quad \\|A\\|_{\\max}=\\|B\\|_{\\max}=1,\\quad AB=\\begin{pmatrix}2 & 2\\\\ 2 & 2\\end{pmatrix},\\ \\ \\|AB\\|_{\\max}=2,\n$$\nwhich violates $\\|AB\\|_{\\max}\\leq \\|A\\|_{\\max}\\|B\\|_{\\max}$. Therefore $\\|\\cdot\\|_{\\max}$ is not submultiplicative for $n\\geq 2$. Statement D is false.\n\nConsequently, the false statement is D.", "answer": "$$\\boxed{D}$$", "id": "2186695"}, {"introduction": "Matrix norms are powerful tools not just for analysis, but also for optimization. In this problem, we move from theory to application by using the Frobenius norm to define the \"size\" or \"cost\" of a matrix. Your task is to find the \"simplest\" linear transformation that satisfies a specific constraint, a common scenario in fields like control theory and machine learning where we seek the most efficient solution [@problem_id:2186715].", "problem": "In many applications, from control theory to machine learning, it is often desirable to find the \"simplest\" linear transformation that satisfies certain criteria. One way to quantify the complexity or \"size\" of a transformation represented by a matrix is using a matrix norm.\n\nThe Frobenius norm of an $m \\times n$ matrix $M$ with real entries $m_{ij}$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n m_{ij}^2}$.\n\nLet two vectors in $\\mathbb{R}^2$ be given as $x_0 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$ and $b_0 = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$.\n\nYour task is to find the unique $2 \\times 2$ matrix $A$ with real entries that has the minimum possible Frobenius norm among all matrices satisfying the condition $A x_0 = b_0$. Express your answer as a $2 \\times 2$ matrix whose entries are exact fractions in their simplest form.", "solution": "We seek a real $2 \\times 2$ matrix $A$ that minimizes the Frobenius norm subject to $A x_{0} = b_{0}$, where $x_{0} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$ and $b_{0} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$. Minimizing $\\|A\\|_{F}$ is equivalent to minimizing $\\|A\\|_{F}^{2}$, which is strictly convex in $A$, ensuring uniqueness when combined with linear constraints.\n\nUse Lagrange multipliers. Define the Lagrangian with multiplier $\\lambda \\in \\mathbb{R}^{2}$:\n$$\nL(A,\\lambda) = \\|A\\|_{F}^{2} + 2 \\lambda^{T}(A x_{0} - b_{0}).\n$$\nTaking the gradient with respect to $A$ and setting it to zero (using the Frobenius inner product) gives\n$$\n\\frac{\\partial L}{\\partial A} = 2A + 2 \\lambda x_{0}^{T} = 0 \\quad \\Longrightarrow \\quad A = - \\lambda x_{0}^{T}.\n$$\nImpose the constraint $A x_{0} = b_{0}$:\n$$\nA x_{0} = (-\\lambda x_{0}^{T}) x_{0} = -\\lambda (x_{0}^{T} x_{0}) = b_{0}.\n$$\nSince $x_{0} \\neq 0$, we solve for $\\lambda$:\n$$\n\\lambda = - \\frac{b_{0}}{x_{0}^{T} x_{0}}.\n$$\nHence the minimizer is\n$$\nA = \\frac{b_{0} x_{0}^{T}}{x_{0}^{T} x_{0}}.\n$$\nCompute $x_{0}^{T} x_{0} = 3^{2} + (-1)^{2} = 10$ and\n$$\nb_{0} x_{0}^{T} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\begin{pmatrix} 3 & -1 \\end{pmatrix} = \\begin{pmatrix} 6 & -2 \\\\ 15 & -5 \\end{pmatrix}.\n$$\nTherefore,\n$$\nA = \\frac{1}{10} \\begin{pmatrix} 6 & -2 \\\\ 15 & -5 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} & -\\frac{1}{5} \\\\ \\frac{3}{2} & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThis $A$ satisfies $A x_{0} = b_{0}$ and is unique due to the strict convexity of $\\|A\\|_{F}^{2}$ and linearity of the constraint.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{5} & -\\frac{1}{5} \\\\ \\frac{3}{2} & -\\frac{1}{2}\\end{pmatrix}}$$", "id": "2186715"}]}