{"hands_on_practices": [{"introduction": "The inverse power method is a powerful iterative algorithm, and like any complex process, it is best understood by starting with its most fundamental building block. This first practice isolates the core computational step that is repeated in every iteration. By solving the linear system $(A - \\sigma I)y_1 = x_0$, you will gain hands-on experience with the engine that drives the entire method, grounding your understanding in a concrete calculation [@problem_id:1395843].", "problem": "In a numerical algorithm, a sequence of vectors is generated starting from an initial vector $x_0$. The first unnormalized vector in this sequence, denoted as $y_1$, is found by solving the linear system $(A-\\sigma I)y_1 = x_0$, where $A$ is a square matrix, $\\sigma$ is a scalar shift, and $I$ is the identity matrix of the same dimension as $A$.\n\nGiven the matrix $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, the shift $\\sigma = 1.5$, and the initial vector $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, determine the components of the vector $y_1$. Present your answer as a row matrix where each component is an exact fraction or decimal.", "solution": "We are asked to solve the linear system $(A-\\sigma I) y_{1} = x_{0}$ for $y_{1}$, where $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, $\\sigma = \\frac{3}{2}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst compute the shifted matrix:\n$$\nA - \\sigma I = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -1 \\\\ -1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nDenote $M = A - \\sigma I$. Then $y_{1} = M^{-1} x_{0}$. For a $2 \\times 2$ matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, we use $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here $a = d = \\frac{3}{2}$ and $b = c = -1$, so\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\nand\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nTherefore,\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nMultiplying by $x_{0}$,\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\nThus the components of $y_{1}$ are $\\frac{6}{5}$ and $\\frac{4}{5}$, which we present as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{4}{5} \\end{pmatrix}}$$", "id": "1395843"}, {"introduction": "After mastering the mechanics of a single iteration, we must address the ultimate goal: what does this process converge to, and are there situations where it might fail? This exercise shifts our focus from computation to convergence theory. It presents a hypothetical scenario designed to illustrate a crucial limitation of the inverse power method, forcing us to analyze the eigenvalue spectrum to predict the algorithm's behavior and understand why a unique solution isn't always guaranteed [@problem_id:2216127].", "problem": "In the field of computational science, iterative methods are essential for finding eigenvalues and eigenvectors of large matrices. One such method is the inverse power method.\n\nConsider a real, non-defective 3x3 matrix $A$ whose eigenvalues are known to be $\\lambda_1 = 5$, $\\lambda_2 = 2$, and $\\lambda_3 = -2$. An engineer applies the unshifted inverse power method to this matrix, starting with a randomly chosen initial vector $x_0 \\in \\mathbb{R}^3$ that is not an eigenvector of $A$. The iterative scheme is given by:\n$$ x_{k+1} = \\frac{A^{-1}x_k}{\\|A^{-1}x_k\\|} $$\nfor $k = 0, 1, 2, \\dots$, where $\\|\\cdot\\|$ denotes the Euclidean norm.\n\nWhich one of the following statements best explains the expected behavior of the sequence of vectors $\\{x_k\\}$?\n\nA. The sequence $\\{x_k\\}$ will fail to converge to a single eigenvector because the eigenvalue of $A$ with the largest magnitude is not unique.\n\nB. The sequence $\\{x_k\\}$ will fail to converge to a single eigenvector because there are two distinct eigenvalues of $A$ that share the same minimal magnitude.\n\nC. The sequence $\\{x_k\\}$ will fail to converge to a single eigenvector because the matrix $A$ is singular.\n\nD. The sequence $\\{x_k\\}$ will converge to the eigenvector corresponding to the eigenvalue $\\lambda=2$, as it is the smallest positive eigenvalue.\n\nE. The sequence $\\{x_k\\}$ will converge to the eigenvector corresponding to the eigenvalue $\\lambda=5$, because the power method always finds the eigenvector for the dominant eigenvalue.", "solution": "We analyze the unshifted inverse power method applied to a real, non-defective matrix $A$ with eigenvalues $\\lambda_{1}=5$, $\\lambda_{2}=2$, and $\\lambda_{3}=-2$. Since $A$ is non-defective, there exists a basis of eigenvectors $\\{v_{1},v_{2},v_{3}\\}$ with $Av_{i}=\\lambda_{i}v_{i}$ for $i\\in\\{1,2,3\\}$.\n\nThe inverse power method applies the power method to $A^{-1}$. The eigenvalues of $A^{-1}$ are the reciprocals of those of $A$:\n$$\n\\mu_{1}=\\frac{1}{\\lambda_{1}}=\\frac{1}{5},\\quad \\mu_{2}=\\frac{1}{\\lambda_{2}}=\\frac{1}{2},\\quad \\mu_{3}=\\frac{1}{\\lambda_{3}}=-\\frac{1}{2}.\n$$\nLet the initial vector be $x_{0}=c_{1}v_{1}+c_{2}v_{2}+c_{3}v_{3}$ with at least two coefficients nonzero (since $x_{0}$ is not an eigenvector). After $k$ inverse iterations without normalization, one has\n$$\ny_{k}=A^{-1}{}^{k}x_{0}=c_{1}\\mu_{1}^{k}v_{1}+c_{2}\\mu_{2}^{k}v_{2}+c_{3}\\mu_{3}^{k}v_{3}.\n$$\nThe normalized iterate is\n$$\nx_{k}=\\frac{y_{k}}{\\|y_{k}\\|}.\n$$\nBy the standard power method theory, $x_{k}$ converges to a single eigenvector of $A^{-1}$ (hence of $A$) if and only if there is a unique eigenvalue of $A^{-1}$ of maximal magnitude and the initial vector has a nonzero component in its eigenvector direction.\n\nCompute magnitudes:\n$$\n|\\mu_{1}|=\\frac{1}{5},\\quad |\\mu_{2}|=\\frac{1}{2},\\quad |\\mu_{3}|=\\frac{1}{2}.\n$$\nThere is no unique dominant magnitude for $A^{-1}$ because $|\\mu_{2}|=|\\mu_{3}|$ are both maximal. Consequently, the iterates $x_{k}$ are dominated by the components in the invariant subspace spanned by $v_{2}$ and $v_{3}$:\n$$\ny_{k}\\approx c_{2}\\left(\\frac{1}{2}\\right)^{k}v_{2}+c_{3}\\left(-\\frac{1}{2}\\right)^{k}v_{3},\n$$\nso after normalization the sequence typically does not converge to a single eigenvector; it may oscillate or converge to a direction depending on the relative phases/signs but not to a unique eigenvector in general.\n\nWe also note that $A$ is nonsingular because\n$$\n\\det(A)=\\lambda_{1}\\lambda_{2}\\lambda_{3}=5\\cdot 2\\cdot(-2)=-20\\neq 0,\n$$\nso nonconvergence is not due to singularity. Moreover, the inverse power method targets the eigenvalue of $A$ with minimal magnitude; here $|\\lambda_{2}|=|\\lambda_{3}|=2$ tie for minimal magnitude, preventing convergence to a single eigenvector. It does not converge to the eigenvector for $\\lambda=5$ (that would be the standard power method on $A$), nor specifically to $\\lambda=2$ because of the tie with $\\lambda=-2$.\n\nTherefore, the correct explanation is that there are two distinct eigenvalues of $A$ sharing the same minimal magnitude, causing the inverse power method to fail to converge to a single eigenvector.", "answer": "$$\\boxed{B}$$", "id": "2216127"}, {"introduction": "In real-world scientific computing, we often use algorithms to discover properties we don't know in advance. This advanced practice beautifully illustrates that principle by turning the problem on its head. Instead of using known eigenvalues to predict an algorithm's behavior, you will use the observed convergence rate from a hypothetical computational run to deduce the underlying spectral properties of the matrix. This powerful technique connects abstract convergence theory to the practical analysis of numerical methods [@problem_id:1395834].", "problem": "A computational scientist is using the shifted inverse power method to find an eigenpair of a very large, real, symmetric matrix $A$. The method iteratively computes a sequence of vectors $\\{x_k\\}$ starting from an initial random vector $x_0$. The iteration is defined by solving $(A - \\sigma I) y_{k+1} = x_k$ for $y_{k+1}$, and then normalizing to get $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|_2$, where $I$ is the identity matrix. A shift of $\\sigma = 4.5$ is used.\n\nAfter many iterations, the convergence has become linear, and the scientist records the Euclidean norms of the differences between consecutive iterates:\n$$\n\\|x_{150} - x_{149}\\|_2 = 8.12 \\times 10^{-7}\n$$\n$$\n\\|x_{151} - x_{150}\\|_2 = 1.48 \\times 10^{-7}\n$$\n\nLet $\\lambda_{target}$ be the eigenvalue of $A$ closest to the shift $\\sigma$, and let $\\lambda_{subdominant}$ be the second-closest eigenvalue to $\\sigma$. Based on the observed data, estimate the value of the spectral gap ratio $R = \\frac{|\\lambda_{target} - \\sigma|}{|\\lambda_{subdominant} - \\sigma|}$. Round your final answer to three significant figures.", "solution": "The problem asks us to estimate a ratio involving eigenvalues of a matrix $A$ based on the convergence behavior of the shifted inverse power method.\n\nThe shifted inverse power method is defined by the iteration:\n1. Solve $(A - \\sigma I) y_{k+1} = x_k$ for $y_{k+1}$. This is equivalent to $y_{k+1} = (A - \\sigma I)^{-1} x_k$.\n2. Normalize the vector: $x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|_2}$.\n\nThis iterative process is mathematically equivalent to applying the standard power method to the matrix $B = (A - \\sigma I)^{-1}$. Let the eigenvalues of $A$ be $\\{\\lambda_i\\}$ with corresponding eigenvectors $\\{v_i\\}$. The eigenvalues of $B$ are then $\\mu_i = \\frac{1}{\\lambda_i - \\sigma}$, and they share the same eigenvectors $\\{v_i\\}$.\n\nThe power method converges to the eigenvector associated with the eigenvalue of largest magnitude (the dominant eigenvalue). For the matrix $B = (A - \\sigma I)^{-1}$, the dominant eigenvalue $\\mu_{dom}$ is the one for which $|\\mu_i| = \\frac{1}{|\\lambda_i - \\sigma|}$ is maximized. This occurs for the eigenvalue $\\lambda_i$ of $A$ that minimizes $|\\lambda_i - \\sigma|$. This eigenvalue is denoted as $\\lambda_{target}$ in the problem statement. Thus, the dominant eigenvalue of $B$ is $\\mu_{dom}$ with magnitude $|\\mu_{dom}| = \\frac{1}{|\\lambda_{target} - \\sigma|}$. The inverse power method causes the vector sequence $\\{x_k\\}$ to converge to the eigenvector corresponding to $\\lambda_{target}$.\n\nThe rate of convergence of the power method is determined by the ratio of the magnitudes of the second-largest eigenvalue (the subdominant eigenvalue) to the dominant eigenvalue. Let the subdominant eigenvalue of $B$ be $\\mu_{sub}$. Its magnitude is determined by the eigenvalue of $A$, denoted $\\lambda_{subdominant}$, that is second-closest to the shift $\\sigma$. Thus, $|\\mu_{sub}| = \\frac{1}{|\\lambda_{subdominant} - \\sigma|}$.\n\nThe asymptotic rate of convergence for the vector sequence is given by the ratio $\\rho = \\frac{|\\mu_{sub}|}{|\\mu_{dom}|}$. For large $k$, the error in the vector $x_k$ (i.e., its component orthogonal to the true eigenvector) is reduced by a factor of approximately $\\rho$ at each step.\n\nA common way to monitor convergence in practice is to observe the norm of the difference between successive iterates, $\\|x_{k+1} - x_k\\|_2$. When the method is converging linearly and $k$ is large, the ratio of the norms of consecutive difference vectors provides a good estimate of the convergence rate $\\rho$.\n$$\n\\rho \\approx \\frac{\\|x_{k+1} - x_k\\|_2}{\\|x_k - x_{k-1}\\|_2}\n$$\n\nUsing the data provided in the problem statement for $k=150$:\n$$\n\\rho \\approx \\frac{\\|x_{151} - x_{150}\\|_2}{\\|x_{150} - x_{149}\\|_2}\n$$\nSubstituting the given numerical values:\n$$\n\\rho \\approx \\frac{1.48 \\times 10^{-7}}{8.12 \\times 10^{-7}} = \\frac{1.48}{8.12} \\approx 0.18226600985\n$$\n\nThe problem asks for the spectral gap ratio $R = \\frac{|\\lambda_{target} - \\sigma|}{|\\lambda_{subdominant} - \\sigma|}$. We can relate this to the convergence rate $\\rho$:\n$$\n\\rho = \\frac{|\\mu_{sub}|}{|\\mu_{dom}|} = \\frac{\\frac{1}{|\\lambda_{subdominant} - \\sigma|}}{\\frac{1}{|\\lambda_{target} - \\sigma|}} = \\frac{|\\lambda_{target} - \\sigma|}{|\\lambda_{subdominant} - \\sigma|}\n$$\nThus, the quantity we estimated, $\\rho$, is exactly the spectral gap ratio $R$ that we need to find.\n$$\nR = \\rho \\approx 0.18226600985\n$$\n\nThe problem requires the final answer to be rounded to three significant figures.\n$$\nR \\approx 0.182\n$$", "answer": "$$\\boxed{0.182}$$", "id": "1395834"}]}