{"hands_on_practices": [{"introduction": "The best way to understand an algorithm is to execute it yourself. This first exercise provides a direct, hands-on opportunity to apply the core formulas of the Conjugate Gradient method. By calculating the first iteration for a simple $2 \\times 2$ system [@problem_id:1393666], you will solidify your understanding of how the residual, search direction, and step size come together to produce the next approximation of the solution.", "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.", "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$", "id": "1393666"}, {"introduction": "While the Conjugate Gradient method is an iterative process, under certain special conditions, it can find the exact solution in a single step. This intriguing scenario is not just a curiosity; it reveals a profound connection between the method's convergence and the eigenspace of the matrix $A$. This practice challenges you to work backward and find the specific initial guess that makes the initial error an eigenvector of the system matrix, thus triggering this remarkable one-step convergence [@problem_id:1393668].", "problem": "The conjugate gradient (CG) method is an iterative algorithm for solving systems of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a symmetric and positive-definite matrix. The performance of the method can depend on the choice of the initial guess, $\\mathbf{x}_0$.\n\nConsider the linear system defined by the matrix $A = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$ and the vector $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$. The CG method is initialized with a starting vector of the form $\\mathbf{x}_0 = \\begin{pmatrix} 1 \\\\ y_0 \\end{pmatrix}$, where $y_0$ is a real number.\n\nFind all possible values of $y_0$ for which the CG method converges to the exact solution in a single iteration. This means the first iterate, $\\mathbf{x}_1$, is the exact solution. You may assume that the initial guess $\\mathbf{x}_0$ is not the exact solution itself. Calculate the sum of all such valid values of $y_0$.", "solution": "The conjugate gradient method for symmetric positive-definite $A$ starts with $x_{0}$, residual $r_{0}=b-Ax_{0}$, search direction $p_{0}=r_{0}$, step size\n$$\n\\alpha_{0}=\\frac{r_{0}^{T}r_{0}}{p_{0}^{T}Ap_{0}},\n$$\nand update $x_{1}=x_{0}+\\alpha_{0}p_{0}$. The exact solution $x^{*}$ satisfies $Ax^{*}=b$. Define the initial error $e_{0}=x^{*}-x_{0}$. Then\n$$\nr_{0}=b-Ax_{0}=Ax^{*}-Ax_{0}=A(x^{*}-x_{0})=Ae_{0}.\n$$\nConvergence in one iteration means $x_{1}=x^{*}$, i.e.,\n$$\nx^{*}=x_{0}+\\alpha_{0}r_{0}\\quad\\Longleftrightarrow\\quad e_{0}=\\alpha_{0}r_{0}=\\alpha_{0}Ae_{0}.\n$$\nThus $Ae_{0}=(1/\\alpha_{0})e_{0}$, so $e_{0}$ must be an eigenvector of $A$. Conversely, if $Ae_{0}=\\lambda e_{0}$, then $r_{0}=\\lambda e_{0}$ and\n$$\n\\alpha_{0}=\\frac{r_{0}^{T}r_{0}}{r_{0}^{T}Ar_{0}}=\\frac{\\lambda^{2}e_{0}^{T}e_{0}}{\\lambda^{3}e_{0}^{T}e_{0}}=\\frac{1}{\\lambda},\n$$\nwhich gives $x_{1}=x_{0}+\\alpha_{0}r_{0}=x_{0}+(1/\\lambda)\\lambda e_{0}=x^{*}$. Therefore, the necessary and sufficient condition for one-step convergence is that $e_{0}$ be an eigenvector of $A$.\n\nCompute $x^{*}$ for $A=\\begin{pmatrix}2 & -1\\\\ -1 & 2\\end{pmatrix}$ and $b=\\begin{pmatrix}1\\\\ 3\\end{pmatrix}$. Solve\n$$\n\\begin{cases}\n2x_{1}-x_{2}=1,\\\\\n-x_{1}+2x_{2}=3,\n\\end{cases}\n$$\nwhich yields $x_{1}=5/3$, $x_{2}=7/3$, hence $x^{*}=\\begin{pmatrix}5/3\\\\ 7/3\\end{pmatrix}$.\n\nWith $x_{0}=\\begin{pmatrix}1\\\\ y_{0}\\end{pmatrix}$, the initial error is\n$$\ne_{0}=x^{*}-x_{0}=\\begin{pmatrix}5/3-1\\\\ 7/3-y_{0}\\end{pmatrix}=\\begin{pmatrix}2/3\\\\ 7/3-y_{0}\\end{pmatrix}.\n$$\nThe eigenvalues of $A$ are $\\lambda_{1}=1$ with eigenvector $v_{1}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$ and $\\lambda_{2}=3$ with eigenvector $v_{2}=\\begin{pmatrix}1\\\\ -1\\end{pmatrix}$. Thus $e_{0}$ must be parallel to $v_{1}$ or $v_{2}$.\n\nCase 1: $e_{0}=c\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$. Then $2/3=c$ and $7/3-y_{0}=c$, so $y_{0}=7/3-2/3=5/3$.\n\nCase 2: $e_{0}=c\\begin{pmatrix}1\\\\ -1\\end{pmatrix}$. Then $2/3=c$ and $7/3-y_{0}=-c=-2/3$, so $y_{0}=7/3+2/3=3$.\n\nBoth values yield $e_{0}\\neq 0$, so $x_{0}\\neq x^{*}$ as required. The sum of all such $y_{0}$ is\n$$\n\\frac{5}{3}+3=\\frac{5}{3}+\\frac{9}{3}=\\frac{14}{3}.\n$$", "answer": "$$\\boxed{\\frac{14}{3}}$$", "id": "1393668"}, {"introduction": "The theoretical elegance of the Conjugate Gradient method hinges on one critical assumption: the matrix $A$ must be symmetric and positive-definite (SPD). But what happens if we apply the algorithm to a system where this condition is not met? This computational exercise [@problem_id:2382427] allows you to explore this question by implementing the CG algorithm and observing its behavior on both SPD and non-symmetric matrices, offering a practical demonstration of why theoretical requirements are crucial for algorithmic stability and convergence.", "problem": "You are given square matrices, right-hand side vectors, an initial vector, a tolerance, and a maximum number of allowed iterations. For each case, generate the sequence produced by the standard unpreconditioned conjugate gradient iteration for solving the linear system $A x = b$ starting from $x_0$, and assess its behavior by three quantitative criteria. Use the Euclidean vector norm throughout.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, and $x_0 \\in \\mathbb{R}^{n}$. Define the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. For iteration index $k = 0, 1, 2, \\dots$, define\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nIf $p_k^\\top A p_k = 0$ at any iteration, declare a breakdown and stop. At each step, track the relative residual norm\n$$\n\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}.\n$$\nStop the iteration when either $\\rho_k \\le \\tau$ or the iteration count reaches $k_{\\max}$ or a breakdown is detected. Define that the sequence $\\{\\rho_k\\}$ exhibits monotone nonincreasing behavior if $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all consecutive pairs, where $\\varepsilon = 10^{-12}$.\n\nFor each test case below, produce a result list containing four entries in the following order:\n1. A boolean indicating whether the stopping condition $\\rho_k \\le \\tau$ was met before reaching $k_{\\max}$ iterations and without breakdown.\n2. An integer equal to the number of iterations actually performed (the count of updates from $x_k$ to $x_{k+1}$).\n3. A float equal to the final relative residual norm $\\rho_{\\text{final}}$ rounded to six decimal places.\n4. A boolean indicating whether the sequence $\\{\\rho_k\\}$ was monotone nonincreasing according to the definition above.\n\nTest suite (use exactly these data, in the order listed):\n\n- Case $1$ (symmetric positive definite reference):\n  - Dimension $n = 10$.\n  - Matrix $A_1$ with entries $(A_1)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_1)_{i,i+1} = (A_1)_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Right-hand side $b_1$ with components $(b_1)_i = 1$ for $i = 1, \\dots, n$.\n  - Initial vector $x_{0,1}$ with $(x_{0,1})_i = 0$ for $i = 1, \\dots, n$.\n  - Tolerance $\\tau_1 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,1} = 10$.\n\n- Case $2$ (mildly non-symmetric):\n  - Dimension $n = 10$.\n  - Let $S$ be the strictly skew-symmetric matrix with $S_{i,i+1} = 1$ and $S_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Matrix $A_2 = A_1 + \\gamma S$ with $\\gamma = 0.1$.\n  - Right-hand side $b_2 = b_1$.\n  - Initial vector $x_{0,2} = x_{0,1}$.\n  - Tolerance $\\tau_2 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,2} = 10$.\n\n- Case $3$ (strongly non-symmetric, upper triangular non-normal):\n  - Dimension $n = 10$.\n  - Matrix $A_3$ with $(A_3)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_3)_{ij} = 1$ for $i  j$, and $(A_3)_{ij} = 0$ for $i > j$.\n  - Right-hand side $b_3 = b_1$.\n  - Initial vector $x_{0,3} = x_{0,1}$.\n  - Tolerance $\\tau_3 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,3} = 10$.\n\n- Case $4$ (boundary, minimal dimension):\n  - Dimension $n = 1$.\n  - Matrix $A_4 = [2]$.\n  - Right-hand side $b_4 = [1]$.\n  - Initial vector $x_{0,4} = [0]$.\n  - Tolerance $\\tau_4 = 10^{-14}$.\n  - Maximum iterations $k_{\\max,4} = 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sublist in the same order as above. The format must be:\n\"[[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool]]\"\nFor the float, round to six decimal places. No physical units are involved, angles are not used, and no percentages appear in the output.", "solution": "The problem as stated is a valid and well-posed exercise in the field of computational physics and numerical linear algebra. It is scientifically grounded, free of contradictions, and provides all necessary information to proceed with a solution. The task is to implement the standard Conjugate Gradient (CG) algorithm and evaluate its performance on a series of well-defined test cases.\n\nThe Conjugate Gradient method is an iterative algorithm designed for solving large, sparse linear systems of equations of the form $A x = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite (SPD). The method's effectiveness is based on the iterative construction of a set of $A$-orthogonal (or conjugate) search directions $\\{p_k\\}_{k=0}^{n-1}$, which satisfy $p_i^\\top A p_j = 0$ for $i \\neq j$. This $A$-orthogonality guarantees that the error is minimized in the $A$-norm at each step over the expanding Krylov subspace, $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$. A key consequence is that the sequence of residuals $\\{r_k\\}$ are mutually orthogonal, i.e., $r_i^\\top r_j = 0$ for $i \\neq j$. In exact arithmetic, this process guarantees convergence to the exact solution in at most $n$ iterations.\n\nWhen the matrix $A$ is not symmetric, as in Cases $2$ and $3$ of the problem, the theoretical foundations of the CG method are no longer valid. The properties of $A$-orthogonality of search directions and orthogonality of residuals are lost. As a result, convergence is not guaranteed. The residual norm, $\\lVert r_k \\rVert_2$, may exhibit erratic, non-monotonic behavior, and the algorithm may fail to converge or even diverge. The provided test cases are structured to demonstrate this principle:\n- Case $1$: The matrix $A_1$ is a symmetric positive-definite matrix (the discrete one-dimensional Laplacian), representing the ideal scenario for the CG method.\n- Case $2$: The matrix $A_2$ is a mildly non-symmetric perturbation of $A_1$. The CG method may still converge, but its ideal performance characteristics, such as monotonic residual reduction, may be lost.\n- Case $3$: The matrix $A_3$ is a strongly non-symmetric, non-normal, upper triangular matrix. Applying the formal CG algorithm here is expected to yield poor results, demonstrating the method's limitations.\n- Case $4$: A trivial $n=1$ case which must converge in a single step to the exact solution.\n\nThe implementation will strictly follow the algorithm provided in the problem statement. Starting from an initial guess $x_0$, we compute the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. The iterative process for $k = 0, 1, 2, \\dots$ involves the following calculations:\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nThe iteration terminates based on three conditions:\n1.  **Convergence**: The relative residual norm $\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}$ drops below a specified tolerance $\\tau$.\n2.  **Maximum Iterations**: The number of iterations reaches the maximum allowed, $k_{\\max}$.\n3.  **Breakdown**: The denominator in the expression for $\\alpha_k$, $p_k^\\top A p_k$, becomes zero. For an SPD matrix this can only occur if $p_k=0$, which implies the solution has been found. For a general matrix, this can happen for $p_k \\neq 0$, constituting a fatal breakdown of the algorithm.\n\nFrom the execution trace of the algorithm on each test case, the four specified quantitative metrics will be determined:\n1.  A boolean indicating if convergence (via the $\\tau$ condition) was achieved without breakdown and within the $k_{\\max}$ limit.\n2.  The total number of iterations performed.\n3.  The final relative residual norm $\\rho_{\\text{final}}$, rounded to six decimal places.\n4.  A boolean indicating whether the sequence of relative residual norms $\\{\\rho_k\\}$ was monotonically non-increasing, defined as $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all $k$, where the tolerance $\\varepsilon = 10^{-12}$ accounts for minor floating-point fluctuations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the CG solver, and print results.\n    \"\"\"\n\n    def cg_solver(A, b, x0, tau, k_max):\n        \"\"\"\n        Implements the Conjugate Gradient algorithm as specified in the problem.\n\n        Args:\n            A (np.ndarray): The square matrix.\n            b (np.ndarray): The right-hand side vector.\n            x0 (np.ndarray): The initial guess vector.\n            tau (float): The tolerance for the relative residual norm.\n            k_max (int): The maximum number of iterations.\n\n        Returns:\n            list: A list containing the four required result metrics.\n        \"\"\"\n        epsilon_monotonicity = 1e-12\n        # A small number to check against for floating point zero\n        machine_zero = 1e-40\n\n        x = np.copy(x0).astype(float)\n        r = b - A @ x\n        p = np.copy(r)\n\n        norm_r0 = np.linalg.norm(r)\n\n        # If the initial guess is already the solution\n        if norm_r0  machine_zero:\n            return [True, 0, 0.0, True]\n\n        rho_history = [1.0]\n        \n        num_iterations = 0\n        converged_by_tau = False\n        breakdown = False\n\n        r_T_r = r.T @ r\n\n        for k in range(k_max):\n            # Calculate alpha_k\n            Ap = A @ p\n            p_T_Ap = p.T @ Ap\n            \n            # Breakdown condition\n            if abs(p_T_Ap)  machine_zero:\n                breakdown = True\n                break\n\n            alpha_k = r_T_r / p_T_Ap\n\n            # Update solution and residual\n            x = x + alpha_k * p\n            r_next = r - alpha_k * Ap\n            \n            num_iterations += 1\n            \n            # Check stopping condition based on relative residual norm\n            rho_k_plus_1 = np.linalg.norm(r_next) / norm_r0\n            rho_history.append(rho_k_plus_1)\n            \n            if rho_k_plus_1 = tau:\n                converged_by_tau = True\n                r = r_next # Finalize r for correct rho reporting\n                break\n\n            # Update search direction\n            r_next_T_r_next = r_next.T @ r_next\n            \n            # Robustness check to avoid division by zero if r becomes zero\n            if r_T_r  machine_zero:\n                converged_by_tau = True # Implicitly converged\n                r = r_next\n                break\n\n            beta_k_plus_1 = r_next_T_r_next / r_T_r\n            p = r_next + beta_k_plus_1 * p\n            \n            # Update residual for the next iteration\n            r = r_next\n            r_T_r = r_next_T_r_next\n\n        # Post-processing after the loop\n        final_rho = rho_history[-1]\n        \n        is_converged_output = converged_by_tau and not breakdown\n        \n        is_monotone = True\n        for i in range(len(rho_history) - 1):\n            if rho_history[i+1]  rho_history[i] + epsilon_monotonicity:\n                is_monotone = False\n                break\n                \n        return [is_converged_output, num_iterations, round(final_rho, 6), is_monotone]\n\n    # --- Define Test Cases ---\n    \n    # Common parameters for cases 1-3\n    n = 10\n    b_common = np.ones(n, dtype=float)\n    x0_common = np.zeros(n, dtype=float)\n    \n    # Case 1: Symmetric positive definite\n    A1 = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    case1 = (A1, b_common, x0_common, 1e-10, 10)\n\n    # Case 2: Mildly non-symmetric\n    gamma = 0.1\n    S = np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n    A2 = A1 + gamma * S\n    case2 = (A2, b_common, x0_common, 1e-10, 10)\n\n    # Case 3: Strongly non-symmetric, upper triangular\n    A3 = np.diag(np.full(n, 2.0)) + np.triu(np.ones((n, n)), k=1)\n    case3 = (A3, b_common, x0_common, 1e-10, 10)\n    \n    # Case 4: Minimal dimension (scalar)\n    A4 = np.array([[2.0]])\n    b4 = np.array([1.0])\n    x04 = np.array([0.0])\n    case4 = (A4, b4, x04, 1e-14, 1)\n    \n    test_cases = [case1, case2, case3, case4]\n\n    results = []\n    for A, b, x0, tau, kmax in test_cases:\n        result = cg_solver(A, b, x0, tau, kmax)\n        results.append(result)\n\n    # Format output as specified: \"[[...],[...],...]\"\n    # The str() on a list gives a string representation \"[...]\"\n    # Joining these with a comma and enclosing in brackets produces the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382427"}]}