{"hands_on_practices": [{"introduction": "In the language of quantum mechanics, operators like $\\hat{A}$ that represent physical observables become matrices, $A$, in a chosen basis. This exercise [@problem_id:1379886] explores how an operator applied twice, $\\hat{A}^2$, is represented by the matrix product $A^2$. Mastering this fundamental calculation, especially with the complex-valued matrices common in quantum theory, is a crucial first step toward using matrix mechanics to predict experimental outcomes.", "problem": "In the quantum mechanical description of a two-state system, an operator $\\hat{A}$ represents a particular physical observable. The system is defined with respect to an orthonormal basis $\\{|\\psi_1\\rangle, |\\psi_2\\rangle\\}$. In this basis, the matrix representation of the operator $\\hat{A}$ is given by the matrix $A$, where the elements are defined as $A_{ij} = \\langle \\psi_i | \\hat{A} | \\psi_j \\rangle$.\n\nThe specific matrix for the operator $\\hat{A}$ is:\n$$\nA = \\begin{pmatrix} 1 & 2-3i \\\\ 2+3i & 1 \\end{pmatrix}\n$$\nwhere $i$ is the imaginary unit, satisfying $i^2 = -1$.\n\nDetermine the matrix representation for the operator $\\hat{A}^2$ in the same basis. Present your final answer as a 2x2 matrix.", "solution": "We are given the matrix representation of the operator $\\hat{A}$ in the orthonormal basis $\\{|\\psi_{1}\\rangle,|\\psi_{2}\\rangle\\}$ as\n$$\nA=\\begin{pmatrix}1 & 2-3i \\\\ 2+3i & 1\\end{pmatrix}.\n$$\nThe matrix representation of $\\hat{A}^{2}$ in the same basis is the matrix square $A^{2}=A\\cdot A$. Writing $A=\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$ with $a=d=1$, $b=2-3i$, and $c=2+3i$, the product is\n$$\nA^{2}=\\begin{pmatrix}a^{2}+bc & ab+bd \\\\ ca+dc & cb+d^{2}\\end{pmatrix}.\n$$\nCompute each entry explicitly:\n- For the diagonal terms, use $bc=(2-3i)(2+3i)=4-9i^{2}=4+9=13$ since $i^{2}=-1$. Thus\n$$\na^{2}+bc=1+13=14,\\qquad cb+d^{2}=13+1=14.\n$$\n- For the off-diagonal terms,\n$$\nab+bd=b(a+d)=(2-3i)(1+1)=2(2-3i)=4-6i,\n$$\n$$\nca+dc=c(a+d)=(2+3i)(1+1)=2(2+3i)=4+6i.\n$$\nTherefore,\n$$\nA^{2}=\\begin{pmatrix}14 & 4-6i \\\\ 4+6i & 14\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}14 & 4-6i \\\\ 4+6i & 14\\end{pmatrix}}$$", "id": "1379886"}, {"introduction": "Matrices are not just for abstract operations; they are powerful tools for solving tangible problems in chemistry. This practice [@problem_id:1379910] demonstrates this by applying matrix methods to Hückel Molecular Orbital theory, a model for understanding conjugated $\\pi$ systems. You will set up and solve the secular equation, $\\det(\\mathbf{H} - E\\mathbf{I}) = 0$, a foundational procedure that directly connects the determinant of a matrix to the allowed energy levels, $E$, of a molecule.", "problem": "Within the framework of Hückel Molecular Orbital (HMO) theory, we can approximate the energies of the $\\pi$ molecular orbitals for conjugated systems. Consider the cyclopropenyl radical, C$_3$H$_3$, which consists of three carbon atoms arranged in a cycle, with each carbon atom contributing one p-orbital to the $\\pi$ system.\n\nThe HMO theory uses the following approximations for the matrix elements of the effective one-electron Hamiltonian, $\\hat{H}$:\n1.  The Coulomb integral for each carbon atom $i$ is $H_{ii} = \\int \\phi_i^* \\hat{H} \\phi_i d\\tau = \\alpha$.\n2.  The resonance integral between two carbon atoms $i$ and $j$ is $H_{ij} = \\int \\phi_i^* \\hat{H} \\phi_j d\\tau = \\beta$ if the atoms are directly bonded, and $H_{ij} = 0$ otherwise.\n3.  The overlap integrals are simplified as $S_{ij} = \\int \\phi_i^* \\phi_j d\\tau = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe energies $E$ of the molecular orbitals are found by solving the secular equation $\\det(\\mathbf{H} - E\\mathbf{I}) = 0$, where $\\mathbf{H}$ is the Hamiltonian matrix and $\\mathbf{I}$ is the identity matrix.\n\nBy making the substitution $x = \\frac{\\alpha - E}{\\beta}$, this secular equation can be expressed as a polynomial in $x$. Determine this polynomial, $P(x)$, such that the secular equation is equivalent to $P(x)=0$. Provide the polynomial $P(x)$ as your answer.", "solution": "In Hückel theory for the cyclopropenyl radical, label the three carbon p-orbitals as $\\phi_{1}, \\phi_{2}, \\phi_{3}$. The Hamiltonian matrix in this basis, using $H_{ii}=\\alpha$ and $H_{ij}=\\beta$ for bonded $i,j$, is\n$$\n\\mathbf{H}=\n\\begin{pmatrix}\n\\alpha & \\beta & \\beta \\\\\n\\beta & \\alpha & \\beta \\\\\n\\beta & \\beta & \\alpha\n\\end{pmatrix}.\n$$\nThe secular equation is $\\det(\\mathbf{H}-E\\mathbf{I})=0$, i.e.\n$$\n\\det\n\\begin{pmatrix}\n\\alpha - E & \\beta & \\beta \\\\\n\\beta & \\alpha - E & \\beta \\\\\n\\beta & \\beta & \\alpha - E\n\\end{pmatrix}\n=0.\n$$\nIntroduce $x=\\frac{\\alpha - E}{\\beta}$ and factor out $\\beta$ from each row (or column) to obtain\n$$\n\\beta^{3}\\det\n\\begin{pmatrix}\nx & 1 & 1 \\\\\n1 & x & 1 \\\\\n1 & 1 & x\n\\end{pmatrix}\n=0,\n$$\nwhich is equivalent to\n$$\n\\det\n\\begin{pmatrix}\nx & 1 & 1 \\\\\n1 & x & 1 \\\\\n1 & 1 & x\n\\end{pmatrix}\n=0.\n$$\nCompute the determinant by expansion:\n$$\n\\det\n\\begin{pmatrix}\nx & 1 & 1 \\\\\n1 & x & 1 \\\\\n1 & 1 & x\n\\end{pmatrix}\n= x\\bigl(x^{2}-1\\bigr)\\;-\\;1\\bigl(x-1\\bigr)\\;+\\;1\\bigl(1-x\\bigr)\n= x^{3}-x - x + 1 + 1 - x\n= x^{3}-3x+2.\n$$\nThus the polynomial is $P(x)=x^{3}-3x+2$, and the secular equation is equivalent to $P(x)=0$.", "answer": "$$\\boxed{x^{3}-3x+2}$$", "id": "1379910"}, {"introduction": "A defining feature of the quantum world is that the order of operations can change the outcome, a property known as non-commutativity. This exercise [@problem_id:1379891] offers a hands-on exploration of this principle by examining matrix exponentials for two non-commuting Pauli matrices, $\\sigma_x$ and $\\sigma_y$. By comparing the series expansions of $e^{\\alpha(\\sigma_x + \\sigma_y)}$ and $e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y}$, you will gain a concrete understanding of a concept central to quantum dynamics and the mathematical structure of quantum theory.", "problem": "In the quantum theory of spin, the Pauli matrices are fundamental operators. For a spin-1/2 particle, two of these matrices are given by:\n$$ \\sigma_x = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} $$\nwhere $i^2 = -1$.\n\nA key property of matrix exponentiation is that for non-commuting matrices $A$ and $B$, $e^{A+B} \\neq e^A e^B$. Consider the difference matrix $D(\\alpha)$ defined for a real parameter $\\alpha$ as:\n$$ D(\\alpha) = e^{\\alpha(\\sigma_x + \\sigma_y)} - e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y} $$\nBy performing a series expansion of the matrix exponentials for small $\\alpha$, this difference can be approximated as a power series in $\\alpha$. Determine the matrix $M$ which represents the coefficient of the lowest-order non-vanishing term in this expansion. Specifically, find the matrix $M$ such that $D(\\alpha) = M \\alpha^2 + O(\\alpha^3)$, where $O(\\alpha^3)$ denotes terms of order $\\alpha^3$ and higher.", "solution": "Our goal is to find the matrix coefficient $M$ of the $\\alpha^2$ term in the series expansion of $D(\\alpha) = e^{\\alpha(\\sigma_x + \\sigma_y)} - e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y}$. We will use the Taylor series for a matrix exponential, $e^X = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\dots$, where $I$ is the identity matrix. We need to expand both terms in the expression for $D(\\alpha)$ up to the order of $\\alpha^2$.\n\nFirst, let's analyze the term $e^{\\alpha(\\sigma_x + \\sigma_y)}$. Let $A = \\alpha(\\sigma_x + \\sigma_y)$. The expansion is:\n$$ e^A = I + A + \\frac{A^2}{2} + O(\\alpha^3) $$\nWe need to compute $A^2$:\n$$ A^2 = (\\alpha(\\sigma_x + \\sigma_y))^2 = \\alpha^2 (\\sigma_x + \\sigma_y)^2 = \\alpha^2 (\\sigma_x^2 + \\sigma_x\\sigma_y + \\sigma_y\\sigma_x + \\sigma_y^2) $$\nLet's compute the necessary matrix products:\n$$ \\sigma_x^2 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I $$\n$$ \\sigma_y^2 = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} = \\begin{pmatrix} (-i)(i) & 0 \\\\ 0 & (i)(-i) \\end{pmatrix} = \\begin{pmatrix} -i^2 & 0 \\\\ 0 & -i^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I $$\n$$ \\sigma_x\\sigma_y = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} = \\begin{pmatrix} i & 0 \\\\ 0 & -i \\end{pmatrix} $$\n$$ \\sigma_y\\sigma_x = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} -i & 0 \\\\ 0 & i \\end{pmatrix} $$\nSubstituting these back into the expression for $A^2$:\n$$ A^2 = \\alpha^2 (I + \\sigma_x\\sigma_y + \\sigma_y\\sigma_x + I) = \\alpha^2 (2I + \\sigma_x\\sigma_y + \\sigma_y\\sigma_x) $$\nSo, the first exponential term is:\n$$ e^{\\alpha(\\sigma_x + \\sigma_y)} = I + \\alpha(\\sigma_x + \\sigma_y) + \\frac{\\alpha^2}{2}(2I + \\sigma_x\\sigma_y + \\sigma_y\\sigma_x) + O(\\alpha^3) $$\n$$ e^{\\alpha(\\sigma_x + \\sigma_y)} = I + \\alpha(\\sigma_x + \\sigma_y) + \\alpha^2 I + \\frac{\\alpha^2}{2}(\\sigma_x\\sigma_y + \\sigma_y\\sigma_x) + O(\\alpha^3) $$\n\nNext, we analyze the term $e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y}$. We expand each exponential separately up to order $\\alpha^2$ and then multiply them.\n$$ e^{\\alpha\\sigma_x} = I + \\alpha\\sigma_x + \\frac{(\\alpha\\sigma_x)^2}{2} + O(\\alpha^3) = I + \\alpha\\sigma_x + \\frac{\\alpha^2}{2}\\sigma_x^2 + O(\\alpha^3) = I + \\alpha\\sigma_x + \\frac{\\alpha^2}{2}I + O(\\alpha^3) $$\n$$ e^{\\alpha\\sigma_y} = I + \\alpha\\sigma_y + \\frac{(\\alpha\\sigma_y)^2}{2} + O(\\alpha^3) = I + \\alpha\\sigma_y + \\frac{\\alpha^2}{2}\\sigma_y^2 + O(\\alpha^3) = I + \\alpha\\sigma_y + \\frac{\\alpha^2}{2}I + O(\\alpha^3) $$\nNow, we multiply these two expansions and keep terms up to order $\\alpha^2$:\n$$ e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y} = \\left( I + \\alpha\\sigma_x + \\frac{\\alpha^2}{2}I \\right) \\left( I + \\alpha\\sigma_y + \\frac{\\alpha^2}{2}I \\right) + O(\\alpha^3) $$\n$$ = I(I + \\alpha\\sigma_y + \\frac{\\alpha^2}{2}I) + \\alpha\\sigma_x(I + \\alpha\\sigma_y + \\dots) + \\frac{\\alpha^2}{2}I(I + \\dots) + O(\\alpha^3) $$\n$$ = (I + \\alpha\\sigma_y + \\frac{\\alpha^2}{2}I) + (\\alpha\\sigma_x + \\alpha^2\\sigma_x\\sigma_y) + \\frac{\\alpha^2}{2}I + O(\\alpha^3) $$\nCollecting terms by powers of $\\alpha$:\n$$ e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y} = I + \\alpha(\\sigma_x + \\sigma_y) + \\alpha^2\\left(\\frac{1}{2}I + \\sigma_x\\sigma_y + \\frac{1}{2}I\\right) + O(\\alpha^3) $$\n$$ e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y} = I + \\alpha(\\sigma_x + \\sigma_y) + \\alpha^2(I + \\sigma_x\\sigma_y) + O(\\alpha^3) $$\n\nFinally, we compute the difference $D(\\alpha)$:\n$$ D(\\alpha) = e^{\\alpha(\\sigma_x + \\sigma_y)} - e^{\\alpha\\sigma_x} e^{\\alpha\\sigma_y} $$\n$$ D(\\alpha) = \\left[ I + \\alpha(\\sigma_x + \\sigma_y) + \\alpha^2 I + \\frac{\\alpha^2}{2}(\\sigma_x\\sigma_y + \\sigma_y\\sigma_x) \\right] - \\left[ I + \\alpha(\\sigma_x + \\sigma_y) + \\alpha^2(I + \\sigma_x\\sigma_y) \\right] + O(\\alpha^3) $$\nThe terms of order $\\alpha^0$ and $\\alpha^1$ cancel out. The terms proportional to $\\alpha^2 I$ also cancel.\n$$ D(\\alpha) = \\left[ \\frac{\\alpha^2}{2}(\\sigma_x\\sigma_y + \\sigma_y\\sigma_x) \\right] - \\left[ \\alpha^2\\sigma_x\\sigma_y \\right] + O(\\alpha^3) $$\n$$ D(\\alpha) = \\alpha^2 \\left( \\frac{1}{2}\\sigma_x\\sigma_y + \\frac{1}{2}\\sigma_y\\sigma_x - \\sigma_x\\sigma_y \\right) + O(\\alpha^3) $$\n$$ D(\\alpha) = \\alpha^2 \\left( \\frac{1}{2}\\sigma_y\\sigma_x - \\frac{1}{2}\\sigma_x\\sigma_y \\right) + O(\\alpha^3) $$\n$$ D(\\alpha) = \\frac{\\alpha^2}{2} (\\sigma_y\\sigma_x - \\sigma_x\\sigma_y) + O(\\alpha^3) $$\nBy definition, $D(\\alpha) = M\\alpha^2 + O(\\alpha^3)$, so we can identify the matrix $M$:\n$$ M = \\frac{1}{2} (\\sigma_y\\sigma_x - \\sigma_x\\sigma_y) $$\nUsing our previously computed matrices:\n$$ M = \\frac{1}{2} \\left( \\begin{pmatrix} -i & 0 \\\\ 0 & i \\end{pmatrix} - \\begin{pmatrix} i & 0 \\\\ 0 & -i \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} -2i & 0 \\\\ 0 & 2i \\end{pmatrix} = \\begin{pmatrix} -i & 0 \\\\ 0 & i \\end{pmatrix} $$\nThis is the coefficient matrix of the leading non-zero term in the expansion.", "answer": "$$\\boxed{\\begin{pmatrix} -i & 0 \\\\ 0 & i \\end{pmatrix}}$$", "id": "1379891"}]}