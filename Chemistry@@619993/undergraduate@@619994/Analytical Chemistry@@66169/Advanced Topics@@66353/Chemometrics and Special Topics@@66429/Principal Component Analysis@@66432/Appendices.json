{"hands_on_practices": [{"introduction": "Principal Component Analysis works by transforming a set of correlated variables into a new set of uncorrelated variables called principal components. This first exercise provides a foundational look into how this transformation captures information. By mathematically deriving the principal components for a simple, idealized bivariate system, you will uncover the direct relationship between the original data's correlation, $\\rho$, and the proportion of total variance explained by the first principal component [@problem_id:1946278]. This practice builds intuition for why PCA is so effective at data compression.", "problem": "An autonomous environmental monitoring drone uses a pair of identical sensors to measure atmospheric pressure. Let the readings of the two sensors, after being centered by subtracting their long-term average, be represented by the random variables $X_1$ and $X_2$.\n\nThe joint behavior of these readings is described by a bivariate random vector $(X_1, X_2)$ with a covariance matrix $\\Sigma$. Because the sensors are of the same type and subject to similar environmental fluctuations, they have the same variance, $\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$, for some constant $\\sigma > 0$. Their readings are also correlated, with a correlation coefficient $\\rho$ such that $0 < \\rho < 1$. The covariance matrix is therefore given by:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2 & \\rho\\sigma^2 \\\\ \\rho\\sigma^2 & \\sigma^2 \\end{pmatrix}\n$$\nTo reduce data redundancy and identify the primary axis of variation, the engineering team applies Principal Component Analysis (PCA). PCA transforms the original correlated variables $(X_1, X_2)$ into a new set of uncorrelated variables, known as principal components. The first principal component is defined as the linear combination of $X_1$ and $X_2$ that captures the maximum possible variance.\n\nDetermine the proportion of the total variance in the data that is explained by the first principal component. Express your answer as a symbolic expression in terms of $\\rho$.", "solution": "We are given a centered bivariate random vector with covariance matrix\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2} & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}\\end{pmatrix},\n$$\nwhere $0<\\rho<1$ and $\\sigma>0$. In PCA, the variances of the principal components are the eigenvalues of the covariance matrix. The proportion of total variance explained by the first principal component equals its eigenvalue divided by the total variance, which is the trace of $\\Sigma$.\n\nFirst, compute the eigenvalues of $\\Sigma$ by solving the characteristic equation\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\nWe have\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\nThus,\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\nSince $0<\\rho<1$, the largest eigenvalue is\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\nThe total variance equals the trace of $\\Sigma$,\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\nwhich also equals the sum of the eigenvalues $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$. Therefore, the proportion of total variance explained by the first principal component is\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "In many analytical chemistry applications, such as spectroscopy, we are faced with high-dimensional datasets where many variables are redundant. A key practical question after performing PCA is deciding how many principal components are needed to represent the data without significant loss of information. This exercise simulates a common scenario in chemometrics, using hypothetical data from an FT-IR analysis to practice applying a standard cutoff threshold for retained variance [@problem_id:1461616]. This skill is essential for building simple, yet powerful, models from complex data.", "problem": "In an analytical chemistry laboratory, a researcher is developing a new blend of biodegradable polymers. To characterize the composition and intermolecular interactions within various formulations, she collects spectra using Fourier-Transform Infrared (FT-IR) spectroscopy. The resulting dataset is large and complex, containing hundreds of highly correlated variables (absorbance at different wavenumbers). To simplify the data and identify the most significant sources of variation, she applies a chemometric technique called Principal Component Analysis (PCA).\n\nThe PCA algorithm transforms the original data into a new set of uncorrelated variables called Principal Components (PCs). Each successive PC accounts for a decreasing amount of the total variance in the original dataset. The percentage of total variance explained by the first six principal components is found to be:\n\n- PC1: 61.45%\n- PC2: 22.81%\n- PC3: 8.03%\n- PC4: 4.12%\n- PC5: 2.57%\n- PC6: 0.88%\n\nIn chemometrics, it is common practice to retain only the number of PCs necessary to explain a significant portion of the data's variance, thereby reducing dimensionality while preserving essential information. Determine the minimum number of principal components that must be retained to account for at least 98% of the total variance in the FT-IR dataset.", "solution": "The goal is to find the minimum number of Principal Components (PCs) required to explain at least 98% of the total variance. To do this, we must calculate the cumulative percentage of variance explained as we add each successive PC and stop when the cumulative total first meets or exceeds 98%.\n\nThe percentage of variance explained by each PC is given:\n- PC1: 61.45%\n- PC2: 22.81%\n- PC3: 8.03%\n- PC4: 4.12%\n- PC5: 2.57%\n- PC6: 0.88%\n\nLet's calculate the cumulative variance step-by-step.\n\nStep 1: Consider only the first Principal Component (PC1).\nThe cumulative variance is 61.45%.\nThis is less than 98%, so we need more components.\n\nStep 2: Add the second Principal Component (PC2).\nThe cumulative variance is the sum of the variances of PC1 and PC2.\nCumulative Variance = 61.45% + 22.81% = 84.26%.\nThis is less than 98%, so we need more components.\n\nStep 3: Add the third Principal Component (PC3).\nThe cumulative variance is the sum of the variances of PC1, PC2, and PC3.\nCumulative Variance = 84.26% + 8.03% = 92.29%.\nThis is less than 98%, so we need more components.\n\nStep 4: Add the fourth Principal Component (PC4).\nThe cumulative variance is the sum of the variances of PC1, PC2, PC3, and PC4.\nCumulative Variance = 92.29% + 4.12% = 96.41%.\nThis is still less than 98%, so we need more components.\n\nStep 5: Add the fifth Principal Component (PC5).\nThe cumulative variance is the sum of the variances of PC1, PC2, PC3, PC4, and PC5.\nCumulative Variance = 96.41% + 2.57% = 98.98%.\nThis value, 98.98%, is greater than the required threshold of 98%.\n\nTherefore, the minimum number of principal components that must be retained is 5. We do not need to consider PC6, as the condition has already been met.", "answer": "$$\\boxed{5}$$", "id": "1461616"}, {"introduction": "The PCA algorithm is built on a specific set of mathematical procedures, and deviating from them can lead to fundamentally incorrect results. This exercise explores one of the most common pitfalls: failing to mean-center the data before analysis. Through a carefully constructed thought experiment, you will see how a non-centered analysis focuses on the data's location in space rather than its internal variance structure, yielding a completely misleading 'principal component' [@problem_id:1383917]. Understanding this concept is critical for correctly applying PCA and interpreting its results.", "problem": "An analyst is studying a dataset comprising three 2-dimensional data points: $P_1 = (99, 98)$, $P_2 = (100, 100)$, and $P_3 = (101, 102)$. They intend to use Principal Component Analysis (PCA), a technique for dimensionality reduction, to find the direction of maximum variance in the data.\n\nThe standard textbook procedure for finding the principal components is as follows:\n1.  Compute the mean vector (centroid) $\\boldsymbol{\\mu}$ of the data points.\n2.  Mean-center the data by subtracting $\\boldsymbol{\\mu}$ from each data point $\\mathbf{p}_i$ to get centered points $\\mathbf{p}'_i = \\mathbf{p}_i - \\boldsymbol{\\mu}$.\n3.  Compute the covariance matrix $C$ of the centered data. For $N$ data points, the covariance matrix is given by $C = \\frac{1}{N-1} \\sum_{i=1}^{N} \\mathbf{p}'_i (\\mathbf{p}'_i)^T$.\n4.  The principal components are the normalized eigenvectors of the covariance matrix $C$. The first principal component is the eigenvector corresponding to the largest eigenvalue.\n\nLet's denote the first principal component vector found using this correct procedure as $\\mathbf{v}_{\\text{correct}}$.\n\nNow, consider a flawed procedure where the analyst forgets to mean-center the data (i.e., they skip Step 2). In this incorrect method, they compute a matrix $S$ directly from the original (uncentered) data points $\\mathbf{p}_i$:\n$$S = \\sum_{i=1}^{3} \\mathbf{p}_i (\\mathbf{p}_i)^T$$\nThey then find the eigenvector of $S$ corresponding to the largest eigenvalue and call this the \"first principal component.\" Let's denote this incorrectly obtained vector as $\\mathbf{v}_{\\text{incorrect}}$.\n\nWhich of the following statements best describes the orientation of the vector $\\mathbf{v}_{\\text{incorrect}}$?\n\nA. $\\mathbf{v}_{\\text{incorrect}}$ is approximately parallel to the vector $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nB. $\\mathbf{v}_{\\text{incorrect}}$ is approximately parallel to the vector $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nC. $\\mathbf{v}_{\\text{incorrect}}$ is approximately parallel to the vector $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$.\n\nD. $\\mathbf{v}_{\\text{incorrect}}$ is approximately the zero vector.\n\nE. $\\mathbf{v}_{\\text{incorrect}}$ is identical to $\\mathbf{v}_{\\text{correct}}$.", "solution": "The problem asks us to determine the direction of the first principal component of a dataset when the crucial mean-centering step is omitted, and compare it to the options provided. We will analyze both the correct and incorrect procedures. The data points are given as column vectors: $\\mathbf{p}_1 = \\begin{pmatrix} 99 \\\\ 98 \\end{pmatrix}$, $\\mathbf{p}_2 = \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix}$, and $\\mathbf{p}_3 = \\begin{pmatrix} 101 \\\\ 102 \\end{pmatrix}$.\n\nFirst, let's analyze the correct procedure to understand the true direction of variance.\nStep 1: Compute the mean vector $\\boldsymbol{\\mu}$.\n$$ \\boldsymbol{\\mu} = \\frac{1}{3}(\\mathbf{p}_1 + \\mathbf{p}_2 + \\mathbf{p}_3) = \\frac{1}{3} \\begin{pmatrix} 99+100+101 \\\\ 98+100+102 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 300 \\\\ 300 \\end{pmatrix} = \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} $$\nStep 2: Mean-center the data points $\\mathbf{p}'_i = \\mathbf{p}_i - \\boldsymbol{\\mu}$.\n$$ \\mathbf{p}'_1 = \\begin{pmatrix} 99 \\\\ 98 \\end{pmatrix} - \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} $$\n$$ \\mathbf{p}'_2 = \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} - \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n$$ \\mathbf{p}'_3 = \\begin{pmatrix} 101 \\\\ 102 \\end{pmatrix} - \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} $$\nStep 3: Compute the covariance matrix $C = \\frac{1}{N-1} \\sum_{i=1}^{N} \\mathbf{p}'_i (\\mathbf{p}'_i)^T$. Here $N=3$.\n$$ C = \\frac{1}{2} \\left[ \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} \\begin{pmatrix} -1 & -2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\right] $$\n$$ C = \\frac{1}{2} \\left[ \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} $$\nStep 4: Find the eigenvectors of $C$. The characteristic equation is $\\det(C - \\lambda I) = 0$.\n$$ (1-\\lambda)(4-\\lambda) - (2)(2) = 0 \\implies 4 - 5\\lambda + \\lambda^2 - 4 = 0 \\implies \\lambda(\\lambda-5) = 0 $$\nThe eigenvalues are $\\lambda_1 = 5$ and $\\lambda_2 = 0$. The first principal component $\\mathbf{v}_{\\text{correct}}$ is the eigenvector for the larger eigenvalue, $\\lambda_1=5$.\n$$ (C-5I)\\mathbf{v} = \\mathbf{0} \\implies \\begin{pmatrix} 1-5 & 2 \\\\ 2 & 4-5 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies \\begin{pmatrix} -4 & 2 \\\\ 2 & -1 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the equation $-4v_x + 2v_y = 0$, which simplifies to $v_y = 2v_x$. So, the eigenvector is any scalar multiple of $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. Thus, $\\mathbf{v}_{\\text{correct}}$ is parallel to $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. This matches option A. However, the question asks for $\\mathbf{v}_{\\text{incorrect}}$.\n\nNow, we analyze the incorrect procedure. The matrix $S$ is computed from the original data points: $S = \\sum_{i=1}^{3} \\mathbf{p}_i (\\mathbf{p}_i)^T$.\nLet's substitute $\\mathbf{p}_i = \\boldsymbol{\\mu} + \\mathbf{p}'_i$ into the expression for $S$:\n$$ S = \\sum_{i=1}^{3} (\\boldsymbol{\\mu} + \\mathbf{p}'_i) (\\boldsymbol{\\mu} + \\mathbf{p}'_i)^T = \\sum_{i=1}^{3} (\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\boldsymbol{\\mu}(\\mathbf{p}'_i)^T + \\mathbf{p}'_i\\boldsymbol{\\mu}^T + \\mathbf{p}'_i(\\mathbf{p}'_i)^T) $$\nDistributing the summation:\n$$ S = \\sum_{i=1}^{3} \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\boldsymbol{\\mu}\\left(\\sum_{i=1}^{3} \\mathbf{p}'_i\\right)^T + \\left(\\sum_{i=1}^{3} \\mathbf{p}'_i\\right)\\boldsymbol{\\mu}^T + \\sum_{i=1}^{3} \\mathbf{p}'_i(\\mathbf{p}'_i)^T $$\nBy definition of centered data, $\\sum_{i=1}^{3} \\mathbf{p}'_i = \\mathbf{0}$. So the middle two terms vanish.\n$$ S = N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\sum_{i=1}^{3} \\mathbf{p}'_i(\\mathbf{p}'_i)^T $$\nFrom the definition of $C$, we have $\\sum \\mathbf{p}'_i(\\mathbf{p}'_i)^T = (N-1)C$.\n$$ S = N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + (N-1)C $$\nNow we can compute the two parts of this sum.\n$$ N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T = 3 \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix} \\begin{pmatrix} 100 & 100 \\end{pmatrix} = 3 \\begin{pmatrix} 10000 & 10000 \\\\ 10000 & 10000 \\end{pmatrix} = \\begin{pmatrix} 30000 & 30000 \\\\ 30000 & 30000 \\end{pmatrix} $$\n$$ (N-1)C = 2C = 2 \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix} $$\nAdding these two matrices gives $S$:\n$$ S = \\begin{pmatrix} 30000 & 30000 \\\\ 30000 & 30000 \\end{pmatrix} + \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 30002 & 30004 \\\\ 30004 & 30008 \\end{pmatrix} $$\nThe first term, $N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$, has matrix elements that are vastly larger than the elements of the second term, $(N-1)C$. The matrix $S$ is dominated by $N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$. The dominant eigenvector of a rank-1 matrix like $\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$ is the vector $\\boldsymbol{\\mu}$ itself. Because the perturbation $(N-1)C$ is small in comparison, the dominant eigenvector of $S$ will be very close to the dominant eigenvector of $N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$.\nThe dominant eigenvector of $N\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$ is $\\boldsymbol{\\mu} = \\begin{pmatrix} 100 \\\\ 100 \\end{pmatrix}$, which is parallel to $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nTherefore, $\\mathbf{v}_{\\text{incorrect}}$ must be approximately parallel to $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThis explains conceptually why forgetting to mean-center the data is problematic: the resulting \"principal component\" primarily points from the origin to the center of mass of the data cloud, rather than along the direction of the data's internal variance.\n\nComparing our result with the given choices:\nA. $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$: This is the direction of $\\mathbf{v}_{\\text{correct}}$.\nB. $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$: This is the direction of the mean vector $\\boldsymbol{\\mu}$, which we determined dominates $\\mathbf{v}_{\\text{incorrect}}$.\nC. $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$: This vector is orthogonal to $\\mathbf{v}_{\\text{correct}}$ and represents the second principal component of the centered data.\nD. the zero vector: A principal component is a direction and must be a non-zero vector.\nE. identical to $\\mathbf{v}_{\\text{correct}}$: This is clearly false, as $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ is not parallel to $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nThus, the vector $\\mathbf{v}_{\\text{incorrect}}$ is approximately parallel to $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.", "answer": "$$\\boxed{B}$$", "id": "1383917"}]}