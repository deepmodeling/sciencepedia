{"hands_on_practices": [{"introduction": "The cornerstone of building any reliable predictive model is the ability to quantify its performance. One of the most fundamental metrics for this is the Root Mean Square Error of Calibration (RMSEC), which measures the average error between the model's predictions and the known values for the calibration set. This exercise [@problem_id:1459311] will guide you through the essential calculation of RMSEC, a critical first step in assessing the fit of your Partial Least Squares (PLS) model.", "problem": "An analytical chemist is developing a rapid quality control method using spectroscopy to measure the concentration of a key active ingredient in a pharmaceutical formulation. A calibration model is built using Partial Least Squares (PLS) regression, a multivariate statistical method that relates spectral data to the concentration of the analyte. To assess the performance of this calibration model, five standard samples with accurately known concentrations are measured. The PLS model then predicts the concentration for each of these same five samples. The actual (reference) and PLS-predicted concentrations are provided below.\n\nActual Concentrations (mg/mL):\n1.25, 1.40, 1.52, 1.68, 1.81\n\nPredicted Concentrations (mg/mL):\n1.22, 1.43, 1.55, 1.65, 1.83\n\nCalculate the Root Mean Square Error of Calibration (RMSEC) for this model. Express your answer in mg/mL and round your final answer to three significant figures.", "solution": "The Root Mean Square Error of Calibration (RMSEC) is a metric used to evaluate the performance of a calibration model by quantifying the average difference between the known (actual) values and the values predicted by the model for the calibration set.\n\nThe formula for RMSEC is:\n$$\n\\text{RMSEC} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_{i, \\text{actual}} - y_{i, \\text{predicted}})^{2}}{n}}\n$$\nwhere $y_{i, \\text{actual}}$ is the actual concentration of the $i$-th sample, $y_{i, \\text{predicted}}$ is the concentration predicted by the model for the $i$-th sample, and $n$ is the total number of samples in the calibration set.\n\nIn this problem, we are given $n=5$ samples. Let's calculate the squared difference for each sample.\n\nStep 1: Calculate the difference ($y_{i, \\text{actual}} - y_{i, \\text{predicted}}$) for each sample.\n- Sample 1: $1.25 - 1.22 = 0.03$ mg/mL\n- Sample 2: $1.40 - 1.43 = -0.03$ mg/mL\n- Sample 3: $1.52 - 1.55 = -0.03$ mg/mL\n- Sample 4: $1.68 - 1.65 = 0.03$ mg/mL\n- Sample 5: $1.81 - 1.83 = -0.02$ mg/mL\n\nStep 2: Square each of these differences.\n- Sample 1: $(0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 2: $(-0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 3: $(-0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 4: $(0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 5: $(-0.02)^{2} = 0.0004$ $(\\text{mg/mL})^2$\n\nStep 3: Sum the squared differences (Sum of Squared Errors, SSE).\n$$\n\\sum_{i=1}^{5} (y_{i, \\text{actual}} - y_{i, \\text{predicted}})^{2} = 0.0009 + 0.0009 + 0.0009 + 0.0009 + 0.0004 = 0.0040 \\ (\\text{mg/mL})^2\n$$\n\nStep 4: Divide the sum by the number of samples, $n=5$, to find the Mean Squared Error (MSE).\n$$\n\\text{MSE} = \\frac{0.0040}{5} = 0.0008 \\ (\\text{mg/mL})^2\n$$\n\nStep 5: Take the square root of the MSE to find the RMSEC.\n$$\n\\text{RMSEC} = \\sqrt{0.0008} \\approx 0.02828427... \\ \\text{mg/mL}\n$$\n\nStep 6: Round the final answer to three significant figures as requested.\nThe first significant figure is 2, the second is 8, and the third is 2. The next digit is 8, which is 5 or greater, so we round up the last significant figure.\n$$\n\\text{RMSEC} \\approx 0.0283 \\ \\text{mg/mL}\n$$", "answer": "$$\\boxed{0.0283}$$", "id": "1459311"}, {"introduction": "While calculating error is essential, a true understanding of your model comes from diagnosing the *source* of that error. Poor model performance often stems from a mismatch in complexity: the model is either too simple (underfitting) or too complex (overfitting). This thought experiment [@problem_id:1459317] presents a classic diagnostic scenario, teaching you to interpret error metrics from both calibration and validation sets to determine if your model is failing to capture the underlying chemical relationships.", "problem": "An analytical chemist is developing a quantitative model using Partial Least Squares (PLS) regression to determine the concentration of an active pharmaceutical ingredient (API) in a liquid formulation. The model uses near-infrared (NIR) spectra as the predictor variables ($X$) and the known API concentration as the response variable ($Y$).\n\nThe chemist builds an initial PLS model using only one latent variable. To evaluate its performance, they calculate the Root Mean Square Error of Calibration (RMSEC) on the training dataset and the Root Mean Square Error of Prediction (RMSEP) on an independent validation dataset. The chemist observes that both the RMSEC and the RMSEP are unacceptably high, and their values are very close to each other.\n\nBased on this observation, which of the following statements provides the most likely diagnosis for the model's poor performance?\n\nA. The model is overfitting the training data, as it is capturing random noise instead of the systematic relationship between the spectra and concentration.\n\nB. The model is underfitting the data, as a single latent variable is insufficient to capture the complex, concentration-relevant variance within the spectral data.\n\nC. The concentrations in the validation set fall outside the range of concentrations used in the calibration set, making accurate prediction impossible.\n\nD. The NIR spectra are dominated by random instrumental noise that is much larger than the signal related to the API, so no meaningful model can be built.", "solution": "The goal of this problem is to diagnose the performance of a Partial Least Squares (PLS) regression model based on the error metrics from the calibration and validation datasets.\n\nFirst, let's understand the key concepts. PLS is a multivariate regression technique that models the relationship between a set of predictor variables ($X$, the spectra) and one or more response variables ($Y$, the concentration). It does so by constructing a set of orthogonal latent variables (LVs) that are linear combinations of the original predictor variables. These LVs are chosen to maximize the covariance between $X$ and $Y$.\n\nThe number of latent variables used in a PLS model determines its complexity.\n- A model with too few latent variables is a simple model. It may not be complex enough to capture all the systematic variation in the data that is relevant for prediction. This condition is known as **underfitting**. An underfit model performs poorly on the data it was trained on and also performs poorly on new, unseen data. A key characteristic of underfitting is that the error on the training set (e.g., Root Mean Square Error of Calibration, RMSEC) is high, and the error on an independent test set (e.g., Root Mean Square Error of Prediction, RMSEP) is also high and has a similar magnitude to the training error (RMSEC ≈ RMSEP).\n\n- A model with too many latent variables is an overly complex model. It begins to fit not only the systematic, meaningful variation in the training data but also the random noise. This condition is known as **overfitting**. An overfit model performs exceptionally well on the training data (very low RMSEC) but performs poorly on new, unseen data because the noise it learned in the training set does not exist in the new data. A key characteristic of overfitting is a very low RMSEC and a significantly higher RMSEP.\n\nIn the scenario described, the chemist builds a PLS model with only one latent variable. They find that both the RMSEC and the RMSEP are high and numerically close to each other. This pattern—high error on both training and validation sets—is the classic signature of an underfitting model. The model is too simple to adequately describe the relationship between the NIR spectra and the API concentration. A single latent variable is not sufficient to capture the necessary information.\n\nLet's evaluate the given options:\n\nA. The model is overfitting. This is incorrect. Overfitting would be characterized by a very low RMSEC and a high RMSEP, which contradicts the observation that RMSEC and RMSEP are both high and similar. Overfitting typically occurs when *too many* latent variables are used, not too few.\n\nB. The model is underfitting. This is correct. The observation of high and similar RMSEC and RMSEP is the definitive sign of an underfit model. A single latent variable is likely not enough to capture the spectral features correlated with the API concentration in a complex formulation.\n\nC. The validation set concentrations are outside the calibration range. While this would certainly lead to a high RMSEP (an extrapolation problem), it does not explain why the RMSEC on the calibration set is also unacceptably high. The high RMSEC indicates the model is failing to describe the training data itself, which points to a fundamental model deficiency (underfitting) rather than just an issue with the validation set.\n\nD. The data is dominated by noise, so no model is possible. While high noise can degrade model performance, the diagnostic information given (RMSEC ≈ RMSEP and both are high) points specifically to underfitting as the primary issue with *this particular model construction* (using only one LV). It's likely that a more complex model (with more LVs) could extract the signal from the noise and perform better. This option suggests the data is useless, which is a more extreme conclusion than what is warranted by the evidence. The most direct diagnosis is that the model's complexity is too low.\n\nTherefore, the most accurate diagnosis is that the model is underfitting the data.", "answer": "$$\\boxed{B}$$", "id": "1459317"}, {"introduction": "Building a powerful PLS model often involves more than just the regression step; preparing the data is equally crucial. Spectral preprocessing techniques can correct for physical effects like light scattering and baseline drift, dramatically improving predictive accuracy. This practice [@problem_id:1459349] places you in the role of a method developer, tasking you with using the robust Root Mean Square Error of Cross-Validation (RMSECV) to systematically compare two common preprocessing strategies and select the one that builds a superior model.", "problem": "An analytical chemist is tasked with developing a rapid, non-destructive method for quantifying the concentration of a novel, non-proteinogenic amino acid, \"isovaline,\" in a turbid fermentation broth using Near-Infrared (NIR) spectroscopy. The high turbidity of the samples introduces significant baseline shifts and light scattering effects, which can corrupt the spectral data and degrade the performance of a quantitative model.\n\nTo mitigate these issues, a Partial Least Squares (PLS) regression model is being developed. The chemist decides to compare two common spectral preprocessing techniques to determine which is more effective at handling the sample matrix effects:\n1.  Standard Normal Variate (SNV): A method that centers and scales each individual spectrum.\n2.  Savitzky-Golay Second Derivative (SG-2): A digital filter applied with a 15-point window and a 2nd-order polynomial.\n\nTo evaluate the predictive performance of the PLS model resulting from each preprocessing step, a set of 5 calibration samples with known isovaline concentrations (determined by a slower, high-performance liquid chromatography reference method) is used. A leave-one-out cross-validation (LOOCV) procedure is performed. In LOOCV, a model is built using 4 of the 5 samples, and the concentration of the left-out sample is predicted. This process is repeated 5 times, with each sample being left out once.\n\nThe true reference concentrations and the corresponding LOOCV-predicted concentrations for both the SNV-preprocessed and SG-2-preprocessed models are provided below. All concentrations are in milligrams per liter (mg/L).\n\n| Sample ID | True Concentration (mg/L) | Predicted Conc. (SNV Model) | Predicted Conc. (SG-2 Model) |\n|-----------|-----------------------------|-------------------------------|--------------------------------|\n| 1         | 12.5                        | 13.1                          | 14.5                           |\n| 2         | 25.0                        | 24.2                          | 22.1                           |\n| 3         | 37.5                        | 38.5                          | 40.2                           |\n| 4         | 50.0                        | 48.9                          | 53.6                           |\n| 5         | 62.5                        | 63.8                          | 59.9                           |\n\nYour task is to determine which preprocessing method results in a more accurate predictive model. To do this, you must calculate the Root Mean Square Error of Cross-Validation (RMSECV) for both models. The RMSECV is defined as $\\sqrt{\\frac{\\sum_{i=1}^{n} (y_{\\text{pred},i} - y_{\\text{true},i})^2}{n}}$, where $y_{\\text{pred},i}$ is the predicted concentration for the $i$-th sample, $y_{\\text{true},i}$ is the true concentration for the $i$-th sample, and $n$ is the total number of samples. The preprocessing method that yields the lower RMSECV is considered superior.\n\nReport the RMSECV for the superior preprocessing method. Express your final answer in mg/L, rounded to three significant figures.", "solution": "We are asked to compute the Root Mean Square Error of Cross-Validation (RMSECV) for two PLS models built with different spectral preprocessing methods, then report the RMSECV of the superior method (the one with lower RMSECV). The RMSECV for a model with predictions $y_{\\text{pred},i}$ and true values $y_{\\text{true},i}$ over $n$ samples is defined by:\n$$\n\\mathrm{RMSECV}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(y_{\\text{pred},i}-y_{\\text{true},i}\\right)^{2}}{n}}.\n$$\nHere, $n=5$.\n\nCompute RMSECV for the SNV-preprocessed model. The prediction errors for samples $i=1$ to $5$ are:\n$$\n\\begin{aligned}\n\\epsilon_{1} &= 13.1-12.5=0.6,\\\\\n\\epsilon_{2} &= 24.2-25.0=-0.8,\\\\\n\\epsilon_{3} &= 38.5-37.5=1.0,\\\\\n\\epsilon_{4} &= 48.9-50.0=-1.1,\\\\\n\\epsilon_{5} &= 63.8-62.5=1.3.\n\\end{aligned}\n$$\nSquare and sum the errors:\n$$\n\\sum_{i=1}^{5}\\epsilon_{i}^{2}=0.6^{2}+(-0.8)^{2}+1.0^{2}+(-1.1)^{2}+1.3^{2}=0.36+0.64+1.00+1.21+1.69=4.90.\n$$\nCompute the mean squared error and its square root:\n$$\n\\mathrm{MSE}=\\frac{4.90}{5}=0.98,\\quad \\mathrm{RMSECV}_{\\mathrm{SNV}}=\\sqrt{0.98}\\approx 0.989949\\ldots\n$$\nThus, to three significant figures, $\\mathrm{RMSECV}_{\\mathrm{SNV}}=0.990$.\n\nCompute RMSECV for the SG-2-preprocessed model. The prediction errors are:\n$$\n\\begin{aligned}\n\\epsilon_{1} &= 14.5-12.5=2.0,\\\\\n\\epsilon_{2} &= 22.1-25.0=-2.9,\\\\\n\\epsilon_{3} &= 40.2-37.5=2.7,\\\\\n\\epsilon_{4} &= 53.6-50.0=3.6,\\\\\n\\epsilon_{5} &= 59.9-62.5=-2.6.\n\\end{aligned}\n$$\nSquare and sum:\n$$\n\\sum_{i=1}^{5}\\epsilon_{i}^{2}=2.0^{2}+(-2.9)^{2}+2.7^{2}+3.6^{2}+(-2.6)^{2}=4.00+8.41+7.29+12.96+6.76=39.42.\n$$\nCompute the mean squared error and square root:\n$$\n\\mathrm{MSE}=\\frac{39.42}{5}=7.884,\\quad \\mathrm{RMSECV}_{\\mathrm{SG\\text{-}2}}=\\sqrt{7.884}\\approx 2.81.\n$$\nComparison shows $\\mathrm{RMSECV}_{\\mathrm{SNV}}\\approx 0.990$ versus $\\mathrm{RMSECV}_{\\mathrm{SG\\text{-}2}}\\approx 2.81$, so SNV yields the lower RMSECV and is the superior preprocessing method. The requested output is the RMSECV for the superior method, rounded to three significant figures in mg/L, so we report $0.990$.", "answer": "$$\\boxed{0.990}$$", "id": "1459349"}]}