{"hands_on_practices": [{"introduction": "A well-designed experiment is the cornerstone of determining any rate law. This practice explores the critical concept of parameter identifiability, demonstrating how a seemingly reasonable experimental plan can lead to collinearity in your data, making it impossible to disentangle the individual reaction orders [@problem_id:2642183]. By working through a hypothetical scenario where experimental constraints cause this issue, you will develop a deeper appreciation for the mathematical underpinnings of experimental design and learn how to construct robust, orthogonal designs that yield unambiguous results.", "problem": "A homogeneous, irreversible bimolecular reaction between species $A$ and $B$ is studied under isothermal, well-mixed conditions in a batch reactor. Over the initial transient where conversion is negligible, the initial rate $v_0$ is observed to follow a power-law dependence on the initial concentrations,\n$$\nv_0 = k [A]_0^{\\alpha} [B]_0^{\\beta},\n$$\nwhere $k$ is a temperature-dependent rate coefficient and $\\alpha,\\beta$ are real-valued reaction orders to be determined experimentally. To estimate $\\alpha$ and $\\beta$ from initial-rate data, the standard approach is to perform multiple runs with different initial concentrations $\\{([A]_0^{(i)},[B]_0^{(i)})\\}_{i=1}^n$ and fit the log-linear model obtained by taking natural logarithms:\n$$\n\\ln v_0^{(i)} = \\ln k + \\alpha \\ln [A]_0^{(i)} + \\beta \\ln [B]_0^{(i)} + \\varepsilon^{(i)},\n$$\nwhere $\\varepsilon^{(i)}$ encompasses experimental error. Consider that, due to a solubility constraint and the use of a coupled dosing manifold, the initial concentrations are restricted to lie on a curve\n$$\n[B]_0^{(i)} = C \\left([A]_0^{(i)}\\right)^2 \\quad \\text{for all runs } i=1,\\dots,n,\n$$\nwith a fixed, known proportionality constant $C>0$. You aim to determine which aspects of the rate law are experimentally identifiable from the initial-rate data collected under this constraint, and to propose a remedy grounded in experimental design that restores identifiability using the initial-rate method, the isolation method, or pseudo-first-order conditions.\n\nStarting from the definition of the initial-rate method and the log-linearization above, analyze the structure of the resulting linear regression design in the predictors $\\ln [A]_0$ and $\\ln [B]_0$ under the stated constraint. Explain, using linear algebra arguments about the design matrix, how collinearity arises and which parameter combinations become non-identifiable. Then, propose a concrete remedy that uses an orthogonal design in the space of $\\ln [A]_0$ and $\\ln [B]_0$ (for example, a two-level full factorial in coded logarithmic concentrations, or an isolation/pseudo-first-order scheme that achieves orthogonality upon centering) to restore identifiability of both $\\alpha$ and $\\beta$. Describe how such a design eliminates the collinearity in the design matrix.\n\nYour final answer must be the unique linear combination of $\\alpha$ and $\\beta$ that remains identifiable under the constrained design $[B]_0 = C [A]_0^2$, expressed as a closed-form analytic expression in $\\alpha$ and $\\beta$ only. Do not include any units in your final answer. If any numerical approximation were required, you would be told how many significant figures to use; here, no numerical approximation is required.", "solution": "The initial-rate method assumes that over a sufficiently short time window at the start of a batch experiment, the concentrations have not changed appreciably from their initial values. Under this approximation, the instantaneous rate is well represented by the initial rate,\n$$\nv_0 = k [A]_0^{\\alpha} [B]_0^{\\beta},\n$$\nwith $k$ constant for all runs at fixed temperature. Taking the natural logarithm yields a linear regression model in the predictors $\\ln [A]_0$ and $\\ln [B]_0$,\n$$\n\\ln v_0 = \\ln k + \\alpha \\ln [A]_0 + \\beta \\ln [B]_0 + \\varepsilon,\n$$\nwhere $\\varepsilon$ captures measurement noise and any residual model discrepancy.\n\nLet us assemble the model for $n$ experiments in matrix form. Define the response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ with entries $y_i = \\ln v_0^{(i)}$, the parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{3}$ as\n$$\n\\boldsymbol{\\theta} = \\begin{pmatrix} \\ln k \\\\ \\alpha \\\\ \\beta \\end{pmatrix},\n$$\nand the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times 3}$ whose columns are the intercept (a column of ones), $\\mathbf{x}_A$ with entries $x_{A,i} = \\ln [A]_0^{(i)}$, and $\\mathbf{x}_B$ with entries $x_{B,i} = \\ln [B]_0^{(i)}$. Then\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}, \\quad \\mathbf{X} = \\begin{pmatrix} 1 & x_{A,1} & x_{B,1} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{A,n} & x_{B,n} \\end{pmatrix}.\n$$\n\nUnder the constraint $[B]_0^{(i)} = C \\left([A]_0^{(i)}\\right)^2$ for all $i$, we have\n$$\nx_{B,i} = \\ln [B]_0^{(i)} = \\ln C + 2 \\ln [A]_0^{(i)} = \\ln C + 2 x_{A,i}.\n$$\nTherefore, the third column of $\\mathbf{X}$ is an affine combination of the first and second columns. Specifically,\n$$\n\\mathbf{x}_B = (\\ln C)\\,\\mathbf{1} + 2\\,\\mathbf{x}_A,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{n}$ denotes the intercept column. Consequently, the three columns of $\\mathbf{X}$ are linearly dependent: the column space of $\\mathbf{X}$ is at most two-dimensional. This implies that $\\mathbf{X}^{\\top}\\mathbf{X}$ is singular and ordinary least squares parameter estimates for $(\\ln k,\\alpha,\\beta)$ are not unique. This is the manifestation of collinearity leading to non-identifiability of the individual parameters $\\alpha$ and $\\beta$.\n\nTo determine what is identifiable, substitute the constraint into the model at the scalar level:\n$$\n\\ln v_0^{(i)} = \\ln k + \\alpha \\ln [A]_0^{(i)} + \\beta\\big(\\ln C + 2 \\ln [A]_0^{(i)}\\big) + \\varepsilon^{(i)}.\n$$\nCollecting terms yields\n$$\n\\ln v_0^{(i)} = \\big(\\ln k + \\beta \\ln C\\big) + \\big(\\alpha + 2\\beta\\big)\\,\\ln [A]_0^{(i)} + \\varepsilon^{(i)}.\n$$\nThis shows that, under the constraint, all the data can at most resolve two quantities: an effective intercept $\\ln k_{\\text{eff}} = \\ln k + \\beta \\ln C$ and a single effective slope\n$$\ns = \\alpha + 2\\beta.\n$$\nThe intercept $\\ln k_{\\text{eff}}$ conflates $\\ln k$ with $\\beta$, and thus $k$ cannot be separated from $\\beta$ without additional information about $C$ and one of the orders. More relevantly for the orders themselves, the only identifiable linear combination of $\\alpha$ and $\\beta$ from variation along this constrained curve is the slope $s = \\alpha + 2\\beta$. There is no information in the data to separate $\\alpha$ from $\\beta$ individually because $\\ln [B]_0$ does not vary independently of $\\ln [A]_0$; their regressors are affinely dependent.\n\nA remedy is to redesign the set of initial concentrations so that the regressors $\\ln [A]_0$ and $\\ln [B]_0$ vary independently and, ideally, orthogonally. Orthogonality in the regression sense can be enforced by centering the predictors and choosing the runs so that the sample cross-product vanishes:\n$$\n\\sum_{i=1}^{n} \\big(\\ln [A]_0^{(i)} - \\overline{\\ln [A]_0}\\big)\\big(\\ln [B]_0^{(i)} - \\overline{\\ln [B]_0}\\big) = 0,\n$$\nwhich makes the off-diagonal entry of the centered Gram matrix zero and yields a block-diagonal $\\mathbf{X}^{\\top}\\mathbf{X}$ in the slope subspace. A concrete construction is a two-level full factorial in log-space: pick two levels for $A$, $\\ln [A]_0 \\in \\{-a,+a\\}$, and two levels for $B$, $\\ln [B]_0 \\in \\{-b,+b\\}$, and perform all four combinations with equal replication. Then the centered predictors take values in $\\{-a,+a\\}$ and $\\{-b,+b\\}$ with zero means, and the sample cross-product is\n$$\n\\sum \\big(\\ln [A]_0\\big)\\big(\\ln [B]_0\\big) = (+a)(+b) + (+a)(-b) + (-a)(+b) + (-a)(-b) = 0,\n$$\nso the design is orthogonal in the slope space and both $\\alpha$ and $\\beta$ become identifiable. An alternative remedy grounded in the isolation or pseudo-first-order method is to execute two experiment blocks: in block 1, hold $[B]_0$ constant and vary $[A]_0$ (isolation of $A$), which identifies $\\alpha$ from the slope in $\\ln v_0$ versus $\\ln [A]_0$; in block 2, hold $[A]_0$ constant and vary $[B]_0$ (isolation of $B$), which identifies $\\beta$. Either approach breaks the affine dependence between $\\ln [A]_0$ and $\\ln [B]_0$ and removes the collinearity.\n\nIn summary, under the constrained design $[B]_0 = C [A]_0^2$, the unique identifiable linear combination of the reaction orders is the effective slope $s$ given by\n$$\ns = \\alpha + 2\\beta.\n$$\nThis is the only combination that can be estimated from initial-rate variation along the constraint curve without additional design changes.", "answer": "$$\\boxed{\\alpha + 2\\beta}$$", "id": "2642183"}, {"introduction": "Kinetic models are built upon the concept of the instantaneous initial rate, $r_0$, but experiments provide a series of discrete, noisy concentration measurements over time. This exercise bridges that gap by introducing a powerful and widely used numerical technique to estimate the initial rate from time-series data [@problem_id:2642177]. You will implement a local polynomial regression method to find the initial slope and, crucially, propagate the measurement uncertainty into your final estimate of $r_0$, a fundamental step in rigorous data analysis.", "problem": "You are given time series measurements of a reactant concentration $[A](t)$ collected at uniformly spaced times $t_i = i \\,\\Delta t$ for $i \\in \\{0,1,\\dots,m-1\\}$, with independent additive measurement noise that is Gaussian with zero mean and known standard deviation $\\sigma$. The system evolves under elementary kinetics that are either first order in $[A]$ or second order in $[A]$ and $[B]$ under pseudo-first-order conditions via the isolation method (that is, $[B] \\gg [A]$, so that $[B]$ is effectively constant over the sampling window). The initial rate method defines the initial rate as $r_0 = -\\left.\\dfrac{d[A]}{dt}\\right|_{t=0}$.\n\nDesign and implement a method based on a one-sided local polynomial regression (a boundary form of the Savitzky–Golay idea) on the first $m$ samples to estimate the initial slope $s_0 = \\left.\\dfrac{d[A]}{dt}\\right|_{t=0}$ and report the initial rate $r_0 = -s_0$. Use a polynomial of degree $p$ fitted by ordinary least squares to the pairs $\\{(t_i,[A]_i)\\}_{i=0}^{m-1}$, and propagate the known noise level $\\sigma$ through the linear least squares estimator to quantify the standard uncertainty of $r_0$ arising from measurement noise. Express $r_0$ and its standard uncertainty in molar per second (M/s). Round each reported numeric value to six significant figures.\n\nFundamental base you may assume: definitions of reaction rate, the notion of the isolation method and pseudo-first-order conditions, the linear least squares estimator under independent, identically distributed Gaussian noise, and the standard propagation of uncertainty for linear estimators.\n\nImplement your program to handle the following test suite. For each test case, generate synthetic data according to the specified kinetic law and then add Gaussian noise with a fixed seed for reproducibility. Use the provided $\\Delta t$, $m$, $p$, and $\\sigma$. For the pseudo-first-order case, the second-order rate constant $k_2$ and the isolated excess concentration $[B]_0$ define the effective first-order constant $k' = k_2 [B]_0$ over the sampling window.\n\nAll physical quantities must be treated in coherent units as given. Your program must output a single line containing the results as a comma-separated list enclosed in square brackets, with no spaces. For each case, output two floats: the estimated initial rate $r_0$ in $\\mathrm{M/s}$, followed by its propagated standard uncertainty in $\\mathrm{M/s}$, both rounded to six significant figures. The cases must be processed and output in the order listed.\n\nTest Suite:\n- Case 1 (first-order decay): $[A]_0 = 0.010\\,\\mathrm{M}$, $k = 0.30\\,\\mathrm{s}^{-1}$, $\\Delta t = 0.05\\,\\mathrm{s}$, $m = 15$, $p = 3$, $\\sigma = 5.0 \\times 10^{-5}\\,\\mathrm{M}$.\n  Data model: $[A](t) = [A]_0 \\exp(-k t)$.\n- Case 2 (pseudo-first-order via isolation for a second-order reaction $A + B \\to P$): $[A]_0 = 0.005\\,\\mathrm{M}$, $[B]_0 = 0.20\\,\\mathrm{M}$, $k_2 = 2.0\\,\\mathrm{M}^{-1}\\mathrm{s}^{-1}$, $\\Delta t = 0.02\\,\\mathrm{s}$, $m = 20$, $p = 3$, $\\sigma = 2.0 \\times 10^{-5}\\,\\mathrm{M}$. Over the window, $k' = k_2 [B]_0$, and $[A](t) = [A]_0 \\exp(-k' t)$.\n- Case 3 (edge case with small $\\Delta t$ and higher noise): $[A]_0 = 0.003\\,\\mathrm{M}$, $k = 1.5\\,\\mathrm{s}^{-1}$, $\\Delta t = 0.005\\,\\mathrm{s}$, $m = 8$, $p = 2$, $\\sigma = 1.0 \\times 10^{-4}\\,\\mathrm{M}$.\n  Data model: $[A](t) = [A]_0 \\exp(-k t)$.\n\nNumerical data generation:\n- For each case, construct $t_i = i \\,\\Delta t$ for $i = 0,1,\\dots,m-1$, compute the noise-free $[A](t_i)$ from the given model, and then add independent Gaussian noise with zero mean and standard deviation $\\sigma$ to each sample to obtain $[A]_i^{\\mathrm{obs}}$.\n- Use a fixed random seed of $123456$ to ensure reproducibility.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets: $[r_{0,1},u_{1},r_{0,2},u_{2},r_{0,3},u_{3}]$, where $r_{0,j}$ is the estimated initial rate for case $j$ in $\\mathrm{M/s}$ and $u_j$ is its propagated standard uncertainty in $\\mathrm{M/s}$, each rounded to six significant figures.", "solution": "The problem presented is a well-posed and scientifically sound exercise in numerical data analysis as applied to chemical kinetics. It requires the estimation of an initial reaction rate from noisy time-series data. This is a common and fundamental task in experimental chemistry. The proposed method, local polynomial regression, is a standard and robust technique for this purpose. We shall proceed with a rigorous derivation of the solution.\n\nThe objective is to determine the initial rate $r_0$ of a reaction, defined as the negative of the time derivative of a reactant concentration $[A]$ at time $t=0$:\n$$\nr_0 = -\\left.\\frac{d[A]}{dt}\\right|_{t=0}\n$$\nWe are given a series of $m$ concentration measurements, $[A]_i^{\\mathrm{obs}}$, at discrete times $t_i = i \\cdot \\Delta t$ for $i \\in \\{0, 1, \\dots, m-1\\}$. These measurements are corrupted by additive, independent, and identically distributed (i.i.d.) Gaussian noise with mean $0$ and known standard deviation $\\sigma$.\n\nThe core of the method is to approximate the function $[A](t)$ in the vicinity of $t=0$ with a polynomial of degree $p$:\n$$\nP(t) = \\sum_{j=0}^{p} c_j t^j\n$$\nThe derivative of this polynomial is:\n$$\n\\frac{dP(t)}{dt} = \\sum_{j=1}^{p} j c_j t^{j-1}\n$$\nEvaluating this derivative at $t=0$ gives the initial slope, $s_0$:\n$$\ns_0 = \\left.\\frac{dP(t)}{dt}\\right|_{t=0} = c_1\n$$\nTherefore, the initial rate is directly related to the first-order coefficient of the polynomial fit: $r_0 = -s_0 = -c_1$. Our task thus transforms into estimating the coefficient $c_1$ and its associated uncertainty.\n\nThe coefficients $\\boldsymbol{c} = [c_0, c_1, \\dots, c_p]^T$ are determined by fitting the polynomial to the $m$ observed data points, $(t_i, [A]_i^{\\mathrm{obs}})$, using the method of Ordinary Least Squares (OLS). This establishes a linear model of the form $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{c} + \\boldsymbol{\\epsilon}$, where:\n- $\\boldsymbol{y}$ is the $m \\times 1$ vector of observed concentrations $[A]_i^{\\mathrm{obs}}$.\n- $\\boldsymbol{X}$ is the $m \\times (p+1)$ design matrix, whose elements are $X_{ij} = t_i^j$ for $i \\in \\{0, \\dots, m-1\\}$ and $j \\in \\{0, \\dots, p\\}$.\n- $\\boldsymbol{c}$ is the $(p+1) \\times 1$ vector of aformentioned polynomial coefficients.\n- $\\boldsymbol{\\epsilon}$ is the $m \\times 1$ vector of unknown measurement errors.\n\nThe OLS estimator $\\hat{\\boldsymbol{c}}$ which minimizes the sum of squared residuals is given by the normal equations:\n$$\n\\hat{\\boldsymbol{c}} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n$$\nThe estimated initial rate is then $\\hat{r}_0 = -\\hat{c}_1$, where $\\hat{c}_1$ is the second component (index $1$) of the vector $\\hat{\\boldsymbol{c}}$.\n\nTo quantify the uncertainty of this estimate, we must propagate the measurement noise through the linear estimator. The covariance matrix of the observations is $\\mathrm{Cov}(\\boldsymbol{y}) = \\sigma^2 \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the $m \\times m$ identity matrix, reflecting the assumption of i.i.d. noise with variance $\\sigma^2$. The covariance matrix of the estimated coefficient vector $\\hat{\\boldsymbol{c}}$ is then derived as follows:\n$$\n\\mathrm{Cov}(\\hat{\\boldsymbol{c}}) = \\mathrm{Cov}((\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}) = ((\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T) \\mathrm{Cov}(\\boldsymbol{y}) ((\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T)^T\n$$\n$$\n\\mathrm{Cov}(\\hat{\\boldsymbol{c}}) = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T (\\sigma^2 \\boldsymbol{I}) \\boldsymbol{X} ((\\boldsymbol{X}^T \\boldsymbol{X})^{-1})^T = \\sigma^2 (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X} (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\n$$\nThis simplifies to:\n$$\n\\mathrm{Cov}(\\hat{\\boldsymbol{c}}) = \\sigma^2 (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\n$$\nThe variance of the estimator $\\hat{c}_1$ is the diagonal element of this covariance matrix corresponding to $c_1$. Using $0$-based indexing, this is the element at position $(1, 1)$:\n$$\n\\mathrm{Var}(\\hat{c}_1) = \\sigma^2 [(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}]_{11}\n$$\nThe standard uncertainty of the estimated initial rate, $u(\\hat{r}_0)$, is the square root of the variance of $\\hat{r}_0 = -\\hat{c}_1$. Since variance is insensitive to a sign change, $\\mathrm{Var}(\\hat{r}_0) = \\mathrm{Var}(\\hat{c}_1)$. Thus:\n$$\nu(\\hat{r}_0) = \\sqrt{\\mathrm{Var}(\\hat{c}_1)} = \\sigma \\sqrt{[(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}]_{11}}\n$$\nThis formula provides a direct method to calculate the uncertainty of the estimated rate based on the known measurement noise $\\sigma$ and the time points of the samples.\n\nThe implementation will proceed by executing the following steps for each test case:\n1. Generate the synthetic data. For a given case, construct the time vector $\\boldsymbol{t}$. Compute the true concentrations $[A](t_i)$ according to the specified kinetic model ($ [A](t) = [A]_0 \\exp(-kt)$ or $[A](t) = [A]_0 \\exp(-k't)$). Add Gaussian noise generated from a fixed random seed to ensure reproducibility.\n2. Construct the design matrix $\\boldsymbol{X}$ based on the time vector $\\boldsymbol{t}$ and polynomial degree $p$.\n3. Solve the OLS problem $\\boldsymbol{X}\\boldsymbol{c} = \\boldsymbol{y}$ to obtain the coefficient estimate vector $\\hat{\\boldsymbol{c}}$.\n4. Calculate the estimated rate $\\hat{r}_0 = -\\hat{c}_1$.\n5. Compute the matrix $(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}$ and use its $(1,1)$ element to find the uncertainty $u(\\hat{r}_0)$.\n6. Report the final values for $\\hat{r}_0$ and $u(\\hat{r}_0)$, rounded to six significant figures.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for initial reaction rates and their uncertainties from synthetic data\n    using one-sided local polynomial regression.\n    \"\"\"\n\n    def round_to_sig_figs(x, p):\n        \"\"\"\n        Rounds a number x to p significant figures.\n        \"\"\"\n        if x == 0:\n            return 0.0\n        # Calculate the position of the most significant digit.\n        magnitude = np.floor(np.log10(np.abs(x)))\n        # Determine the number of decimal places needed for rounding.\n        rounding_decimal = int(p - 1 - magnitude)\n        return np.round(x, rounding_decimal)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A0\": 0.010, \"k\": 0.30, \"dt\": 0.05, \"m\": 15, \"p\": 3, \"sigma\": 5.0e-5,\n            \"k2\": None, \"B0\": None\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A0\": 0.005, \"k\": None, \"dt\": 0.02, \"m\": 20, \"p\": 3, \"sigma\": 2.0e-5,\n            \"k2\": 2.0, \"B0\": 0.20\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A0\": 0.003, \"k\": 1.5, \"dt\": 0.005, \"m\": 8, \"p\": 2, \"sigma\": 1.0e-4,\n            \"k2\": None, \"B0\": None\n        },\n    ]\n\n    # Global settings for reproducibility and output format\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    num_sig_figs = 6\n    \n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        A0 = case['A0']\n        k = case['k']\n        dt = case['dt']\n        m = case['m']\n        p = case['p']\n        sigma = case['sigma']\n        k2 = case['k2']\n        B0 = case['B0']\n\n        # Determine the effective rate constant k_eff\n        if k is None:\n            # Pseudo-first-order case\n            k_eff = k2 * B0\n        else:\n            # First-order case\n            k_eff = k\n        \n        # Step 1: Generate synthetic noisy data\n        t = np.arange(m) * dt\n        A_true = A0 * np.exp(-k_eff * t)\n        noise = rng.normal(loc=0.0, scale=sigma, size=m)\n        A_obs = A_true + noise\n\n        # Step 2: Construct the design matrix for polynomial regression\n        # np.vander with increasing=True produces [1, t, t^2, ...], which is correct\n        X = np.vander(t, N=p + 1, increasing=True)\n        \n        # Step 3: Solve for polynomial coefficients using OLS\n        # np.linalg.lstsq solves the equation X*c = A_obs for c\n        c_hat, _, _, _ = np.linalg.lstsq(X, A_obs, rcond=None)\n        \n        # The estimated initial rate is -c_1\n        r0_est = -c_hat[1]\n        \n        # Step 4: Calculate the uncertainty of the initial rate\n        # Compute the inverse of the matrix (X^T * X)\n        XTX_inv = np.linalg.inv(X.T @ X)\n        \n        # The variance of c1 is sigma^2 times the (1,1) element of (X^T*X)^-1\n        # Using 0-based indexing for c1\n        var_c1 = sigma**2 * XTX_inv[1, 1]\n        \n        # The standard uncertainty is the square root of the variance\n        u_r0 = np.sqrt(var_c1)\n        \n        # Step 5: Round results to six significant figures\n        r0_rounded = round_to_sig_figs(r0_est, num_sig_figs)\n        u_r0_rounded = round_to_sig_figs(u_r0, num_sig_figs)\n\n        results.extend([r0_rounded, u_r0_rounded])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2642177"}, {"introduction": "Often, the central challenge in kinetics is not just fitting parameters to a given rate law, but deciding which of several plausible rate laws best describes the reaction mechanism. This practice introduces a cornerstone of modern statistical modeling: the Akaike Information Criterion (AIC) [@problem_id:2642276]. You will use AIC to compare different candidate models, learning how to penalize complexity to avoid overfitting and to quantify the strength of evidence supporting the best model, a vital skill for robust scientific inference.", "problem": "You are given three candidate rate laws to model initial-rate data for a bimolecular reaction under conditions that include isolation and pseudo-first-order regimes. The goal is to compare these candidates using the Akaike Information Criterion (AIC) and report the preferred model and the strength of evidence in its favor.\n\nFundamental base:\n- Assume initial-rate measurements are independent with additive, identically distributed, zero-mean Gaussian errors of unknown variance. Under this assumption, the log-likelihood for residuals is proportional to the negative residual sum of squares (RSS), and maximum likelihood estimation (MLE) of parameters corresponds to minimizing RSS.\n- The Akaike Information Criterion (AIC) for a model with $p$ fitted parameters and maximized likelihood $\\hat{L}$ is $ \\mathrm{AIC} = 2p - 2 \\ln(\\hat{L}) $. For Gaussian errors with unknown variance estimated by $ \\widehat{\\sigma}^2 = \\mathrm{RSS}/N $ where $N$ is the number of data points, model rankings by AIC are equivalently obtained from $ \\mathrm{AIC} = N \\ln(\\mathrm{RSS}/N) + 2p + C $, where $C$ is a constant independent of the model and cancels in differences.\n\nCandidate rate laws (with $[\\cdot]$ denoting concentration):\n- Model $\\mathcal{M}_1$ (index $1$): $ r = k_1 [A] $. Parameters: $p=1$ ($k_1$).\n- Model $\\mathcal{M}_2$ (index $2$): $ r = k_2 [A][B] $. Parameters: $p=1$ ($k_2$).\n- Model $\\mathcal{M}_3$ (index $3$): $ r = \\dfrac{k_3 [A][B]}{1 + K [B]} $. Parameters: $p=2$ ($k_3, K$).\n\nAll rate constants and parameters are constrained to be nonnegative. Concentrations $[A]$ and $[B]$ are in $\\mathrm{mol}\\,\\mathrm{L}^{-1}$ and initial rates $r$ are in $\\mathrm{mol}\\,\\mathrm{L}^{-1}\\,\\mathrm{s}^{-1}$. The outputs requested are dimensionless.\n\nParameter estimation requirement:\n- For each model, fit parameters by minimizing $ \\mathrm{RSS} = \\sum_{i=1}^{N} \\left( r_i^{\\mathrm{obs}} - r_i^{\\mathrm{model}} \\right)^2 $ using nonlinear least squares under nonnegativity constraints on parameters.\n\nModel comparison requirement:\n- Compute $ \\mathrm{AIC} $ for each candidate using the Gaussian-error formulation above. Choose the best model as the one with the smallest AIC. In case of ties within absolute tolerance $10^{-12}$, choose the smallest model index.\n- Compute the evidence ratio favoring the best model over the next best (second-smallest AIC) as $ \\exp\\!\\left( \\dfrac{\\mathrm{AIC}_{\\mathrm{second}} - \\mathrm{AIC}_{\\mathrm{best}}}{2} \\right) $. This ratio is unitless and greater than or equal to $1$. If there is an AIC tie, this ratio equals $1$.\n\nTest suite:\n- Case $\\#1$ (general two-reactant variation, second-order baseline with small noise). Data points $( [A], [B], r )$:\n  - (0.02, 0.03, 0.0003)\n  - (0.02, 0.06, 0.000599)\n  - (0.02, 0.12, 0.001198)\n  - (0.05, 0.03, 0.000751)\n  - (0.05, 0.06, 0.0015)\n  - (0.05, 0.12, 0.002999)\n  - (0.10, 0.03, 0.001502)\n  - (0.10, 0.06, 0.003001)\n  - (0.10, 0.12, 0.006)\n- Case $\\#2$ (pseudo-first-order in $[A]$ with $[B]$ held constant and small noise). Data points $( [A], [B], r )$:\n  - (0.02, 0.20, 0.000602)\n  - (0.05, 0.20, 0.001498)\n  - (0.10, 0.20, 0.003002)\n  - (0.20, 0.20, 0.005998)\n- Case $\\#3$ (saturable dependence on $[B]$ under isolation of $[A]$, with small noise). Data points $( [A], [B], r )$:\n  - (0.10, 0.01, 0.000924925926)\n  - (0.10, 0.02, 0.001723637931)\n  - (0.10, 0.05, 0.003571428571)\n  - (0.10, 0.10, 0.005556055556)\n  - (0.10, 0.20, 0.007693307692)\n  - (0.10, 0.50, 0.0100015)\n\nYour task:\n- Implement a program that, for each of the three cases, fits all three models, computes AIC values, selects the best model as specified, and computes the evidence ratio favoring the best model over the second-best model.\n- Report, for each case, the pair $[m, e]$ where $m$ is the integer model index in $\\{1,2,3\\}$ and $e$ is the evidence ratio rounded to exactly six digits after the decimal point.\n\nFinal output format:\n- Your program should produce a single line of output containing the three results as a comma-separated list of lists with no spaces, for example, $[[m_1,e_1],[m_2,e_2],[m_3,e_3]]$. The evidence ratios must be printed with exactly six digits after the decimal point.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of chemical kinetics and statistical model selection, is well-posed with clear and objective criteria, and provides a complete set of data and instructions for a solvable computational task. There are no contradictions, ambiguities, or factual unsoundness.\n\nThe task is to perform model selection among three candidate rate laws for three distinct experimental datasets. The selection is based on the Akaike Information Criterion (AIC), a standard method for comparing statistical models. The procedure for each dataset is as follows:\n\n1.  **Parameter Estimation via Non-Linear Least Squares**: For each of the three models, $\\mathcal{M}_1$, $\\mathcal{M}_2$, and $\\mathcal{M}_3$, the parameters must be estimated by minimizing the Residual Sum of Squares (RSS). The objective function is:\n    $$ \\mathrm{RSS}(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left( r_i^{\\mathrm{obs}} - r^{\\mathrm{model}}([A]_i, [B]_i; \\boldsymbol{\\theta}) \\right)^2 $$\n    where $r_i^{\\mathrm{obs}}$ is the observed initial rate for the $i$-th data point, $r^{\\mathrm{model}}$ is the rate predicted by the model for given concentrations $[A]_i$ and $[B]_i$ and parameter set $\\boldsymbol{\\theta}$, and $N$ is the number of data points. The parameters $\\boldsymbol{\\theta}$ are constrained to be non-negative, consistent with their physical meaning as rate or equilibrium constants. This constrained optimization problem is solved using a numerical non-linear least squares algorithm. The candidate models are:\n    -   Model $\\mathcal{M}_1$: $r = k_1 [A]$, with parameter vector $\\boldsymbol{\\theta}_1 = (k_1)$ and $p_1=1$ parameter.\n    -   Model $\\mathcal{M}_2$: $r = k_2 [A] [B]$, with parameter vector $\\boldsymbol{\\theta}_2 = (k_2)$ and $p_2=1$ parameter.\n    -   Model $\\mathcal{M}_3$: $r = \\dfrac{k_3 [A][B]}{1 + K [B]}$, with parameter vector $\\boldsymbol{\\theta}_3 = (k_3, K)$ and $p_3=2$ parameters.\n\n2.  **AIC Calculation**: After determining the optimal parameters $\\hat{\\boldsymbol{\\theta}}$ and the corresponding minimum $\\mathrm{RSS}_{\\min}$ for each model, the AIC is calculated. For a model with $p$ parameters, the formula is:\n    $$ \\mathrm{AIC} = N \\ln\\left(\\frac{\\mathrm{RSS}_{\\min}}{N}\\right) + 2p $$\n    The AIC balances the goodness of fit (represented by $\\mathrm{RSS}_{\\min}$) with model complexity (represented by $p$). A lower AIC indicates a more preferable model. A perfect fit ($\\mathrm{RSS}_{\\min} = 0$) results in $\\mathrm{AIC} = -\\infty$, making it unequivocally the best model. However, due to experimental noise, $\\mathrm{RSS}_{\\min}$ is expected to be a small positive value.\n\n3.  **Model Selection**: For each case, the three models are ranked based on their AIC values. The model with the minimum AIC is selected as the best. In the event of a tie, defined as an absolute difference in AIC values smaller than $10^{-12}$, the model with the smaller index ($1 < 2 < 3$) is chosen. Let the chosen best model have an AIC of $\\mathrm{AIC}_{\\text{best}}$.\n\n4.  **Evidence Ratio Calculation**: The strength of evidence for the best model is quantified by the evidence ratio, $E$. This ratio compares the likelihood of the best model to that of the second-best model (the one with the next-lowest AIC, $\\mathrm{AIC}_{\\text{second}}$).\n    $$ E = \\exp\\left( \\frac{\\mathrm{AIC}_{\\text{second}} - \\mathrm{AIC}_{\\text{best}}}{2} \\right) $$\n    If the AIC of the best and second-best models are considered tied (within the $10^{-12}$ tolerance), the evidence ratio is $E=1$. Otherwise, $E > 1$, with larger values indicating stronger support for the best model.\n\nThis entire procedure is applied independently to each of the three test cases. The final output for each case is a pair $[m, e]$, where $m$ is the index of the best model and $e$ is the computed evidence ratio.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import least_squares\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for three test cases in chemical kinetics.\n    For each case, it fits three candidate rate laws, calculates their AIC,\n    selects the best model, and computes the evidence ratio.\n    \"\"\"\n\n    # Define the three candidate model functions.\n    # Each function takes parameters and concentrations as input and returns the predicted rate.\n    def model1(params, A, B):\n        k1, = params\n        return k1 * A\n\n    def model2(params, A, B):\n        k2, = params\n        return k2 * A * B\n\n    def model3(params, A, B):\n        k3, K = params\n        # Denominator is 1 + K*B. With K>=0 and B>=0, it is always >= 1, so no risk of division by zero.\n        return (k3 * A * B) / (1.0 + K * B)\n\n    def process_case(data_points):\n        \"\"\"\n        Processes a single test case: fits models, calculates AICs, and determines the best model and evidence ratio.\n        \"\"\"\n        data = np.array(data_points)\n        A = data[:, 0]\n        B = data[:, 1]\n        r_obs = data[:, 2]\n        N = len(r_obs)\n\n        models_spec = [\n            {'name': 'M1', 'func': model1, 'p': 1, 'p0': [1.0], 'bounds': ([0], [np.inf]), 'id': 1},\n            {'name': 'M2', 'func': model2, 'p': 1, 'p0': [1.0], 'bounds': ([0], [np.inf]), 'id': 2},\n            {'name': 'M3', 'func': model3, 'p': 2, 'p0': [1.0, 1.0], 'bounds': ([0, 0], [np.inf, np.inf]), 'id': 3}\n        ]\n\n        results = []\n        for spec in models_spec:\n            # Define the residual function for the least squares optimizer.\n            def residuals(params, A, B, r_obs):\n                r_pred = spec['func'](params, A, B)\n                return r_obs - r_pred\n\n            # Perform non-linear least squares fitting with non-negativity constraints.\n            fit_result = least_squares(\n                residuals,\n                spec['p0'],\n                bounds=spec['bounds'],\n                args=(A, B, r_obs)\n            )\n\n            # RSS is 2 * cost, as least_squares minimizes 0.5 * sum(residuals^2).\n            rss = 2 * fit_result.cost\n            p = spec['p']\n            \n            # Calculate AIC. Handle the case of a perfect fit (RSS=0).\n            if rss <= 1e-30:  # Use a small threshold to handle floating point near-zero\n                aic = -np.inf\n            else:\n                aic = N * math.log(rss / N) + 2 * p\n            \n            results.append({'model_index': spec['id'], 'aic': aic})\n\n        # Sort results primarily by AIC, secondarily by model index for tie-breaking.\n        # Python's default sort is stable. Sorting by index then by AIC achieves the desired outcome.\n        results.sort(key=lambda x: x['model_index'])\n        results.sort(key=lambda x: x['aic'])\n\n        best_model = results[0]\n        second_best_model = results[1]\n\n        m = best_model['model_index']\n        aic_best = best_model['aic']\n        aic_second = second_best_model['aic']\n\n        # Calculate evidence ratio, checking for ties.\n        if abs(aic_second - aic_best) < 1e-12:\n            e = 1.0\n        else:\n            e = math.exp((aic_second - aic_best) / 2.0)\n            \n        return [m, e]\n\n    # Test suite data.\n    test_cases = [\n        # Case #1\n        [\n            (0.02, 0.03, 0.0003),\n            (0.02, 0.06, 0.000599),\n            (0.02, 0.12, 0.001198),\n            (0.05, 0.03, 0.000751),\n            (0.05, 0.06, 0.0015),\n            (0.05, 0.12, 0.002999),\n            (0.10, 0.03, 0.001502),\n            (0.10, 0.06, 0.003001),\n            (0.10, 0.12, 0.006)\n        ],\n        # Case #2\n        [\n            (0.02, 0.20, 0.000602),\n            (0.05, 0.20, 0.001498),\n            (0.10, 0.20, 0.003002),\n            (0.20, 0.20, 0.005998)\n        ],\n        # Case #3\n        [\n            (0.10, 0.01, 0.000924925926),\n            (0.10, 0.02, 0.001723637931),\n            (0.10, 0.05, 0.003571428571),\n            (0.10, 0.10, 0.005556055556),\n            (0.10, 0.20, 0.007693307692),\n            (0.10, 0.50, 0.0100015)\n        ]\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        result_pair = process_case(case_data)\n        all_results.append(result_pair)\n\n    # Format the final output string exactly as specified.\n    result_strings = [f\"[{m},{e:.6f}]\" for m, e in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2642276"}]}