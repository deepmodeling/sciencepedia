{"hands_on_practices": [{"introduction": "Before we can quantify the uncertainty in our estimated parameters, we must first understand how a small change in a parameter's value affects the model's predictions. This is a core task of sensitivity analysis. This exercise [@problem_id:2692536] invites you to derive the sensitivity coefficient for a simple first-order reaction, providing a foundational understanding of how a model's state becomes more or less sensitive to a kinetic parameter over the course of a reaction.", "problem": "In a well-mixed, isothermal, constant-volume batch reactor, consider the irreversible unimolecular reaction from species $A$ to species $B$ ($A \\to B$) governed by the law of mass action. Let $x_A(t)$ denote the concentration of species $A$ at time $t$, and let $k>0$ be the first-order rate constant. The dynamics of $x_A(t)$ obey the ordinary differential equation (ODE) $\\,\\frac{d x_A}{d t}=-k\\,x_A\\,$ with initial condition $\\,x_A(0)=x_0>0\\,$.\n\nTasks:\n- Starting from the ODE and initial condition only, derive the closed-form solution for $x_A(t)$.\n- Using the definition of a local sensitivity coefficient with respect to a parameter, $S_k(t)=\\frac{\\partial x_A(t)}{\\partial k}$, compute a closed-form expression for $S_k(t)$.\n- Briefly explain from first principles how $|S_k(t)|$ scales with time $t$ for small $t$ and large $t$, and identify the time at which $|S_k(t)|$ is maximized and the corresponding maximal magnitude.\n\nAnswer specification:\n- Provide your final answer as the single symbolic expression for $S_k(t)$.\n- No numerical rounding is required.\n- Do not include units in your final boxed answer.", "solution": "The supplied problem statement has been analyzed and is determined to be valid. It is a scientifically grounded, well-posed, and objective problem from the field of chemical kinetics. It is a standard exercise in solving a first-order ordinary differential equation and performing a sensitivity analysis, both of which are fundamental procedures. We shall proceed with the derivation.\n\nThe dynamics of the concentration of species $A$, denoted $x_A(t)$, are described by the initial value problem:\n$$ \\frac{dx_A}{dt} = -k x_A, \\quad x_A(0) = x_0 $$\nwhere $k > 0$ and $x_0 > 0$. This is a first-order linear homogeneous ordinary differential equation. The equation is separable and can be solved by direct integration.\n$$ \\frac{dx_A}{x_A} = -k dt $$\nWe integrate both sides from the initial time $t=0$ to an arbitrary time $t$.\n$$ \\int_{x_A(0)}^{x_A(t)} \\frac{1}{\\xi} d\\xi = \\int_{0}^{t} -k d\\tau $$\nThe integration yields:\n$$ [\\ln|\\xi|]_{x_0}^{x_A(t)} = [-k\\tau]_{0}^{t} $$\n$$ \\ln|x_A(t)| - \\ln|x_0| = -kt $$\nGiven that concentration $x_A$ must be non-negative, and the initial concentration $x_0$ is positive, $x_A(t)$ will remain positive for all finite time $t$. Therefore, the absolute value signs can be removed.\n$$ \\ln\\left(\\frac{x_A(t)}{x_0}\\right) = -kt $$\nBy exponentiating both sides, we obtain the closed-form solution for the concentration profile:\n$$ x_A(t) = x_0 \\exp(-kt) $$\nThis completes the first task.\n\nThe second task is to compute the local sensitivity coefficient of $x_A(t)$ with respect to the parameter $k$, which is defined as $S_k(t) = \\frac{\\partial x_A(t)}{\\partial k}$. We can compute this directly by differentiating the derived expression for $x_A(t)$ with respect to $k$, treating $x_0$ and $t$ as constants.\n$$ S_k(t) = \\frac{\\partial}{\\partial k} \\left( x_0 \\exp(-kt) \\right) $$\nUsing the chain rule for differentiation, we find:\n$$ S_k(t) = x_0 \\cdot \\exp(-kt) \\cdot \\frac{\\partial}{\\partial k}(-kt) $$\n$$ S_k(t) = x_0 \\exp(-kt) (-t) $$\nThus, the closed-form expression for the sensitivity coefficient is:\n$$ S_k(t) = -t x_0 \\exp(-kt) $$\n\nThe final task is to analyze the behavior of the magnitude of the sensitivity, $|S_k(t)|$, and find its maximum. Since $t \\ge 0$, $x_0 > 0$, and $\\exp(-kt) > 0$, the magnitude is:\n$$ |S_k(t)| = t x_0 \\exp(-kt) $$\n\nBehavior for small $t$: For $t \\to 0^{+}$, we can analyze the behavior by considering the Taylor series expansion of the exponential term, $\\exp(-kt) \\approx 1 - kt + \\mathcal{O}((kt)^{2})$.\n$$ |S_k(t)| \\approx t x_0 (1 - kt) = x_0t - kx_0t^{2} $$\nFor small $t$, the linear term dominates, so $|S_k(t)|$ scales linearly with time, i.e., $|S_k(t)| \\propto t$. This is physically intuitive: at $t=0$, no reaction has occurred, so the concentration $x_A(0)=x_0$ is independent of $k$, making the sensitivity zero. As the reaction begins, the concentration becomes increasingly dependent on the rate constant $k$.\n\nBehavior for large $t$: For $t \\to \\infty$, the exponential term $\\exp(-kt)$ decays to zero much faster than the linear term $t$ grows. We can evaluate the limit formally:\n$$ \\lim_{t \\to \\infty} |S_k(t)| = \\lim_{t \\to \\infty} t x_0 \\exp(-kt) = x_0 \\lim_{t \\to \\infty} \\frac{t}{\\exp(kt)} $$\nThis is an indeterminate form $\\frac{\\infty}{\\infty}$. Applying L'HÃ´pital's rule:\n$$ x_0 \\lim_{t \\to \\infty} \\frac{\\frac{d}{dt}(t)}{\\frac{d}{dt}(\\exp(kt))} = x_0 \\lim_{t \\to \\infty} \\frac{1}{k \\exp(kt)} = 0 $$\nThus, $|S_k(t)| \\to 0$ as $t \\to \\infty$. This is also physically sound: after a long duration, the reaction approaches completion ($x_A \\to 0$), and this final state is independent of the specific value of $k$ (as long as $k>0$). The state of the system becomes insensitive to the rate at which it reached that state.\n\nTo find the time at which $|S_k(t)|$ is maximized, we must find the critical points by setting its first derivative with respect to time to zero.\n$$ \\frac{d}{dt}|S_k(t)| = \\frac{d}{dt} \\left( t x_0 \\exp(-kt) \\right) $$\nUsing the product rule:\n$$ \\frac{d}{dt}|S_k(t)| = x_0 \\left[ (1)\\exp(-kt) + t(-k \\exp(-kt)) \\right] = x_0 \\exp(-kt) (1-kt) $$\nSetting the derivative to zero:\n$$ x_0 \\exp(-kt) (1-kt) = 0 $$\nSince $x_0 > 0$ and $\\exp(-kt) > 0$, the equation is satisfied only if $1-kt = 0$. This gives the time of maximum sensitivity:\n$$ t_{\\text{max}} = \\frac{1}{k} $$\nThis time corresponds to the characteristic time constant of the first-order decay process. To confirm it is a maximum, we can examine the sign of the second derivative, or simply note that $|S_k(0)|=0$ and $|S_k(t)| \\to 0$ as $t \\to \\infty$, and $t=1/k$ is the sole positive critical point. The function must therefore attain its maximum here.\n\nThe maximal magnitude of the sensitivity is found by substituting $t_{\\text{max}}$ into the expression for $|S_k(t)|$:\n$$ |S_k(t_{\\text{max}})| = \\left(\\frac{1}{k}\\right) x_0 \\exp\\left(-k\\left(\\frac{1}{k}\\right)\\right) = \\frac{x_0}{k} \\exp(-1) = \\frac{x_0}{ke} $$\nIn summary, the sensitivity of the concentration $x_A$ to the rate constant $k$ is zero initially, increases to a maximum value of $\\frac{x_0}{ke}$ at the characteristic time $t=1/k$, and then decays to zero as the reaction reaches completion. This time represents the point where a small relative change in the rate constant $k$ has the largest absolute impact on the concentration of species $A$.", "answer": "$$\n\\boxed{-t x_{0} \\exp(-k t)}\n$$", "id": "2692536"}, {"introduction": "A model's sensitivity to a parameter is a key ingredient in determining our ability to estimate that parameter from experimental data. The Fisher Information Matrix formalizes this connection, quantifying the \"information\" an experiment yields about a parameter. In this hands-on practice [@problem_id:2692578], you will use the sensitivities derived previously to construct the Fisher Information for a rate constant and explore a classic problem in optimal experimental design: finding the best time to take a measurement.", "problem": "Consider an irreversible, isothermal, well-mixed, batch reaction $A \\to B$ governed by mass-action kinetics with a first-order rate constant $k$ (units $\\,\\mathrm{s}^{-1}$). The concentration of species $A$, denoted $x_A(t)$, evolves according to the ordinary differential equation $\\mathrm{d}x_A/\\mathrm{d}t=-k\\,x_A$ with the initial condition $x_A(0)=x_0$, where $x_0>0$ is known. You collect $n$ independent measurements of $x_A(t)$ at sampling times $\\{t_i\\}_{i=1}^n$ with the observation model $y_i=x_A(t_i)+\\varepsilon_i$, where the noise $\\varepsilon_i$ are independent and identically distributed as zero-mean Gaussian with variance $\\sigma^2$, and $\\sigma^2$ is known.\n\nUsing only the stated kinetics, the observation model, and the definition of Fisher information from statistical estimation theory, derive a closed-form expression for the scalar Fisher information for $k$, denoted $F_{kk}(k)$, in terms of $x_0$, $\\sigma^2$, $k$, and $\\{t_i\\}_{i=1}^n$. Then analyze how $F_{kk}(k)$ depends on the sampling schedule $\\{t_i\\}_{i=1}^n$ by identifying the sampling time $t^\\star$ (treated as a continuous variable) that maximizes the per-sample contribution to $F_{kk}(k)$ when $k$ is treated as known. Provide a brief qualitative explanation of the trade-off that governs this maximizer.\n\nExpress the Fisher information in seconds squared, but do not include units in your final boxed answer. Your final answer must be a single analytic expression for $F_{kk}(k)$, simplified as far as possible.", "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- Reaction: $A \\to B$, irreversible, isothermal, well-mixed, batch.\n- Kinetics: Mass-action, first-order.\n- Rate constant: $k$ with units $\\mathrm{s}^{-1}$.\n- State model (ODE): $\\frac{\\mathrm{d}x_A}{\\mathrm{d}t}=-k\\,x_A$.\n- Initial condition: $x_A(0)=x_0$, with $x_0 > 0$ known.\n- Number of measurements: $n$.\n- Sampling times: $\\{t_i\\}_{i=1}^n$.\n- Observation model: $y_i=x_A(t_i)+\\varepsilon_i$.\n- Noise model: $\\varepsilon_i$ are independent and identically distributed (i.i.d.) random variables from a zero-mean Gaussian distribution with known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded**: The problem is built upon fundamental principles of chemical kinetics (first-order decay) and statistical estimation theory (Fisher information for parameter estimation under additive Gaussian noise). These are standard, well-established models. The setup is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information to derive the requested quantity. The model is fully specified, the parameters are defined, and the task is stated with mathematical precision. A unique, meaningful solution exists.\n- **Objective**: The problem is formulated using objective, technical language, free of any subjective or ambiguous terms.\n\nThe problem does not exhibit any of the listed flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or unverifiable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe derivation proceeds from first principles.\nFirst, we must solve the ordinary differential equation that governs the concentration of species $A$, $x_A(t)$. The equation is $\\frac{\\mathrm{d}x_A}{\\mathrm{d}t} = -k x_A$. This is a separable first-order linear differential equation.\n$$\n\\frac{\\mathrm{d}x_A}{x_A} = -k \\, \\mathrm{d}t\n$$\nIntegrating both sides from time $0$ to $t$ and from concentration $x_A(0)$ to $x_A(t)$:\n$$\n\\int_{x_A(0)}^{x_A(t)} \\frac{1}{\\xi} \\, \\mathrm{d}\\xi = \\int_0^t -k \\, \\mathrm{d}\\tau\n$$\n$$\n\\ln(x_A(t)) - \\ln(x_A(0)) = -kt\n$$\nGiven the initial condition $x_A(0) = x_0$, we have:\n$$\n\\ln\\left(\\frac{x_A(t)}{x_0}\\right) = -kt\n$$\nExponentiating both sides gives the solution for the concentration profile:\n$$\nx_A(t) = x_0 \\exp(-kt)\n$$\nThe observation model is $y_i = x_A(t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that each measurement $y_i$ is a random variable drawn from a Gaussian distribution with mean $\\mu_i = x_A(t_i)$ and variance $\\sigma^2$. The probability density function for a single measurement $y_i$ given the parameter $k$ is:\n$$\np(y_i | k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_A(t_i))^2}{2\\sigma^2}\\right)\n$$\nThe Fisher information provides a measure of the amount of information that an observable random variable carries about an unknown parameter upon which the probability of the random variable depends. For a single parameter $\\theta$ and observations corrupted by independent, additive, zero-mean Gaussian noise with constant variance $\\sigma^2$, the Fisher information simplifies to a sum over all measurements:\n$$\nF_{\\theta\\theta}(\\theta) = \\sum_{i=1}^n \\frac{1}{\\sigma^2} \\left( \\frac{\\partial \\mu_i}{\\partial \\theta} \\right)^2\n$$\nIn our problem, the parameter is $\\theta = k$ and the mean of the measurement at time $t_i$ is $\\mu_i = x_A(t_i; k)$. Thus, the scalar Fisher information for $k$, denoted $F_{kk}(k)$, is:\n$$\nF_{kk}(k) = \\sum_{i=1}^n \\frac{1}{\\sigma^2} \\left( \\frac{\\partial x_A(t_i)}{\\partial k} \\right)^2\n$$\nWe must compute the partial derivative of the state $x_A(t)$ with respect to the parameter $k$. This derivative is known as the sensitivity of the state with respect to the parameter.\n$$\n\\frac{\\partial x_A(t)}{\\partial k} = \\frac{\\partial}{\\partial k} \\left( x_0 \\exp(-kt) \\right)\n$$\nSince $x_0$ and $t$ are not functions of $k$, we apply the chain rule:\n$$\n\\frac{\\partial x_A(t)}{\\partial k} = x_0 \\exp(-kt) \\cdot \\frac{\\partial}{\\partial k}(-kt) = x_0 \\exp(-kt) \\cdot (-t) = -t x_0 \\exp(-kt)\n$$\nNow, we substitute this sensitivity expression back into the formula for the Fisher information:\n$$\nF_{kk}(k) = \\sum_{i=1}^n \\frac{1}{\\sigma^2} \\left( -t_i x_0 \\exp(-kt_i) \\right)^2\n$$\nSimplifying the expression within the sum:\n$$\nF_{kk}(k) = \\sum_{i=1}^n \\frac{1}{\\sigma^2} \\left( t_i^2 x_0^2 \\exp(-2kt_i) \\right)\n$$\nSince $x_0$ and $\\sigma^2$ are constant for all measurements, they can be factored out of the summation:\n$$\nF_{kk}(k) = \\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 \\exp(-2kt_i)\n$$\nThis is the closed-form expression for the Fisher information for the rate constant $k$. The units of $x_0$ and $\\sigma$ are concentration, making the ratio $x_0^2/\\sigma^2$ dimensionless. The rate constant $k$ has units of $\\mathrm{s}^{-1}$ and time $t_i$ has units of $\\mathrm{s}$, so the argument of the exponential, $-2kt_i$, is dimensionless. The term $t_i^2$ has units of $\\mathrm{s}^2$, and therefore the entire expression for $F_{kk}(k)$ has units of $\\mathrm{s}^2$, as required.\n\nNext, we analyze the sampling time $t^\\star$ that maximizes the per-sample contribution to the Fisher information. The contribution from a single sample taken at time $t$ is the summand:\n$$\nf(t; k) = \\frac{x_0^2}{\\sigma^2} t^2 \\exp(-2kt)\n$$\nTo find the time $t$ that maximizes this function, we compute its derivative with respect to $t$ and set it to zero. The term $\\frac{x_0^2}{\\sigma^2}$ is a positive constant and does not affect the location of the maximum. Let $g(t) = t^2 \\exp(-2kt)$.\n$$\n\\frac{\\mathrm{d}g}{\\mathrm{d}t} = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( t^2 \\exp(-2kt) \\right)\n$$\nUsing the product rule for differentiation:\n$$\n\\frac{\\mathrm{d}g}{\\mathrm{d}t} = (2t) \\exp(-2kt) + t^2 \\exp(-2kt) \\cdot (-2k) = (2t - 2kt^2) \\exp(-2kt)\n$$\nWe set the derivative to zero to find the critical points:\n$$\n(2t - 2kt^2) \\exp(-2kt) = 2t(1-kt) \\exp(-2kt) = 0\n$$\nFor physically meaningful time $t > 0$, the term $2t \\exp(-2kt)$ is always positive. Therefore, the extremum occurs when:\n$$\n1-kt = 0 \\implies kt = 1 \\implies t^\\star = \\frac{1}{k}\n$$\nTo confirm this is a maximum, we examine the second derivative.\n$$\n\\frac{\\mathrm{d}^2g}{\\mathrm{d}t^2} = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( (2t - 2kt^2) \\exp(-2kt) \\right) = (2 - 4kt)\\exp(-2kt) + (2t - 2kt^2)(-2k)\\exp(-2kt)\n$$\n$$\n\\frac{\\mathrm{d}^2g}{\\mathrm{d}t^2} = (2 - 4kt - 4kt + 4k^2t^2)\\exp(-2kt) = (2 - 8kt + 4k^2t^2)\\exp(-2kt)\n$$\nEvaluating at $t = t^\\star = 1/k$:\n$$\n\\left. \\frac{\\mathrm{d}^2g}{\\mathrm{d}t^2} \\right|_{t=1/k} = (2 - 8k(1/k) + 4k^2(1/k)^2)\\exp(-2k(1/k)) = (2 - 8 + 4)\\exp(-2) = -2\\exp(-2)\n$$\nSince the second derivative is negative, $t^\\star = 1/k$ is indeed a local maximum. As $g(t) \\to 0$ for $t \\to 0^+$ and $g(t) \\to 0$ for $t \\to \\infty$, this is the global maximum for $t>0$.\n\nThe existence of this optimal sampling time reflects a fundamental trade-off. The information content is proportional to the square of the sensitivity, $(\\frac{\\partial x_A}{\\partial k})^2 = t^2 x_A(t)^2$. This expression contains two competing factors. The $t^2$ factor indicates that sensitivity to $k$ increases with time; waiting longer allows the effect of the rate to accumulate. However, the $x_A(t)^2 = (x_0 \\exp(-kt))^2$ factor indicates that as time progresses, the reactant concentration decays exponentially. As the concentration approaches zero, the signal-to-noise ratio diminishes, and measurements become less informative. The optimal sampling time $t^\\star = 1/k$, which is the characteristic time constant of the reaction, represents the point where this trade-off is perfectly balanced. At this time, the concentration has decayed to $x_A(1/k) = x_0 \\exp(-1)$, a value that is substantially different from the initial state but still large enough to be measured with reasonable precision.", "answer": "$$\n\\boxed{\\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 \\exp(-2kt_i)}\n$$", "id": "2692578"}, {"introduction": "Beyond quantifying uncertainty in existing parameters, a crucial task in modeling is to decide between competing model structures. The Likelihood Ratio Test provides a powerful framework for this, allowing us to ask questions like, \"Is this more complex model statistically justified by the data?\". This culminating exercise [@problem_id:2692475] challenges you to implement this test computationally to determine if a specific reaction pathway is supported by the data, bridging the gap from parameter estimation to rigorous model selection.", "problem": "Consider the irreversible, isothermal, well-mixed branched first-order reaction network with a single reactant species and two product branches: $A \\rightarrow B$ with rate constant $k_1$ and $A \\rightarrow C$ with rate constant $k_2$. The deterministic mass-action dynamics for the species concentrations $A(t)$, $B(t)$, and $C(t)$ satisfy the system of ordinary differential equations (ODEs)\n$$\n\\frac{dA}{dt} = -(k_1 + k_2) A, \\quad \\frac{dB}{dt} = k_1 A, \\quad \\frac{dC}{dt} = k_2 A,\n$$\nwith initial condition $A(0) = A_0$, $B(0) = 0$, $C(0) = 0$. The initial concentration is known and equal to $A_0 = 1.0 \\text{ mol}\\cdot \\text{L}^{-1}$.\n\nSuppose that at measurement times $t_i$ (in seconds), the measured product concentrations $y_{B,i}$ and $y_{C,i}$ are independent Gaussian measurements around the true state values $B(t_i)$ and $C(t_i)$:\n$$\ny_{B,i} \\sim \\mathcal{N}(B(t_i), \\sigma_B^2), \\quad y_{C,i} \\sim \\mathcal{N}(C(t_i), \\sigma_C^2),\n$$\nwith known and constant standard deviations $\\sigma_B = \\sigma_C = 2.0 \\times 10^{-2} \\text{ mol}\\cdot \\text{L}^{-1}$.\n\nYour task is to implement a Likelihood Ratio Test (LRT) for the composite hypothesis $H_0: k_1 = 0$ (no direct $A \\rightarrow B$ pathway) versus the alternative $H_1: k_1 > 0$, treating $k_2$ as a nuisance parameter in both $H_0$ and $H_1$. You will:\n- Compute the Maximum Likelihood Estimation (MLE) under the full model $H_1$ (both $k_1$ and $k_2$ free and nonnegative) and under the constrained model $H_0$ (fix $k_1 = 0$, optimize $k_2 \\ge 0$).\n- Use the Gaussian measurement model with known variances to form the log-likelihood for a dataset $\\{(t_i, y_{B,i}, y_{C,i})\\}_{i=1}^n$, and compute the likelihood ratio statistic\n$$\n\\Lambda = 2\\left(\\ell(\\widehat{k}_1,\\widehat{k}_2) - \\ell(0,\\widehat{k}_2^{(0)})\\right),\n$$\nwhere $\\ell(\\cdot)$ is the log-likelihood and the hats denote MLEs under the corresponding model. Under standard regularity assumptions, treat $\\Lambda$ as approximately $\\chi^2_1$ distributed under $H_0$ and compute the $p$-value as $p = 1 - F_{\\chi^2_1}(\\Lambda)$, expressed as a decimal (unitless). Note: In the special case that $\\Lambda$ is numerically negative due to optimization tolerances, set $\\Lambda = 0$.\n\nYou must implement a complete program that, for each test case below, constructs a noise-free dataset by solving the ODEs exactly and evaluating $B(t)$ and $C(t)$ at the given times for the specified âtrueâ parameter values, then performs the LRT as described. All physical units must be handled consistently: times in seconds, rate constants in $\\text{s}^{-1}$, and concentrations in $\\text{mol}\\cdot \\text{L}^{-1}$. The output $p$-values are pure numbers (decimals without any percent sign).\n\nFundamental base and modeling assumptions you must use and not deviate from:\n- First-order mass-action kinetics as stated in the ODE system.\n- Independence and Gaussianity of measurement errors with known variances.\n- Maximum Likelihood Estimation (MLE) for parameter estimation under each hypothesis.\n\nTest Suite (construct $y_{B,i}$ and $y_{C,i}$ exactly from the model with $A_0 = 1.0 \\text{ mol}\\cdot \\text{L}^{-1}$ and the given true parameters, with no added noise):\n- Case $1$ (general, $H_0$ false): times $t = [0.0, 0.5, 1.0, 2.0, 3.0, 4.0]$ in seconds; true parameters $k_1^\\star = 0.25 \\text{ s}^{-1}$, $k_2^\\star = 0.50 \\text{ s}^{-1}$.\n- Case $2$ (boundary, $H_0$ true): times $t = [0.0, 0.5, 1.0, 2.0, 3.0, 4.0]$ in seconds; true parameters $k_1^\\star = 0.0 \\text{ s}^{-1}$, $k_2^\\star = 0.50 \\text{ s}^{-1}$.\n- Case $3$ (weak signal): times $t = [0.0, 0.5, 1.0, 2.0, 3.0, 4.0]$ in seconds; true parameters $k_1^\\star = 0.05 \\text{ s}^{-1}$, $k_2^\\star = 0.50 \\text{ s}^{-1}$.\n\nNumerical and statistical requirements:\n- Solve the ODE system exactly for first-order kinetics to generate $B(t_i)$ and $C(t_i)$ for each case.\n- Under each hypothesis, obtain MLEs by maximizing the likelihood (equivalently minimizing the weighted sum of squared residuals implied by the Gaussian model).\n- Compute $\\Lambda$ and then compute the $p$-value assuming a $\\chi^2_1$ reference distribution.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three $p$-values, in the order of the cases above, formatted as a comma-separated list enclosed in square brackets, each reported with at least six digits after the decimal point. For example, if the three $p$-values were $p_1$, $p_2$, $p_3$, the output must look like\n$[p_1,p_2,p_3]$.", "solution": "The problem requires the implementation of a Likelihood Ratio Test (LRT) to distinguish between two nested kinetic models. The core of the task is to perform Maximum Likelihood Estimation (MLE) of the rate constants under two different hypotheses and then use the results to compute a test statistic. The validity of the problem is confirmed, as it is scientifically grounded in established principles of chemical kinetics and statistical inference, is well-posed with all necessary information provided, and is objective in its formulation. We shall proceed with a rigorous, step-by-step solution.\n\nFirst, we define the kinetic model and its analytical solution. The reaction network is given by a branched first-order decay of species $A$:\n$$\nA \\xrightarrow{k_1} B \\\\\nA \\xrightarrow{k_2} C\n$$\nThe dynamics are described by a system of linear ordinary differential equations (ODEs) based on the law of mass action:\n$$\n\\frac{d A}{dt} = -(k_1 + k_2) A, \\quad \\frac{d B}{dt} = k_1 A, \\quad \\frac{d C}{dt} = k_2 A\n$$\nwith initial conditions $A(0) = A_0$, $B(0) = 0$, and $C(0) = 0$. For brevity, we denote concentrations as $A(t)$, $B(t)$, and $C(t)$. The initial concentration is given as $A_0 = 1.0 \\text{ mol}\\cdot \\text{L}^{-1}$.\n\nThe ODE for $A(t)$ is separable and integrates to:\n$$\nA(t) = A_0 e^{-(k_1 + k_2)t}\n$$\nSubstituting this into the equations for $B(t)$ and $C(t)$ and integrating from $t=0$ to $t$ yields the exact analytical solutions for the product concentrations. For any non-zero sum of rate constants $k_1+k_2 > 0$:\n$$\nB(t) = \\int_0^t k_1 A_0 e^{-(k_1 + k_2)\\tau} d\\tau = \\frac{k_1 A_0}{k_1 + k_2} \\left(1 - e^{-(k_1 + k_2)t}\\right)\n$$\n$$\nC(t) = \\int_0^t k_2 A_0 e^{-(k_1 + k_2)\\tau} d\\tau = \\frac{k_2 A_0}{k_1 + k_2} \\left(1 - e^{-(k_1 + k_2)t}\\right)\n$$\nIn the specific case where $k_1 + k_2 = 0$, which implies $k_1 = 0$ and $k_2 = 0$ due to the non-negativity constraint on rate constants, the solutions are trivial: $A(t) = A_0$ and $B(t) = C(t) = 0$.\n\nNext, we formulate the statistical model for the measurements. The measured concentrations at times $\\{t_i\\}_{i=1}^n$ are $y_{B,i}$ and $y_{C,i}$. They are assumed to be independent random variables drawn from Gaussian distributions centered at the true concentrations:\n$$\ny_{B,i} \\sim \\mathcal{N}(B(t_i | k_1, k_2), \\sigma_B^2), \\quad y_{C,i} \\sim \\mathcal{N}(C(t_i | k_1, k_2), \\sigma_C^2)\n$$\nThe standard deviations are known and equal: $\\sigma_B = \\sigma_C = \\sigma = 2.0 \\times 10^{-2} \\text{ mol}\\cdot \\text{L}^{-1}$. The log-likelihood function $\\ell(k_1, k_2)$ for the entire dataset $\\{(t_i, y_{B,i}, y_{C,i})\\}_{i=1}^n$ is the sum of the log-probabilities for each independent measurement:\n$$\n\\ell(k_1, k_2) = \\sum_{i=1}^n \\left( -\\frac{(y_{B,i} - B(t_i))^2}{2\\sigma^2} - \\frac{(y_{C,i} - C(t_i))^2}{2\\sigma^2} - \\log(2\\pi\\sigma^2) \\right)\n$$\nMaximizing this log-likelihood is equivalent to minimizing its negative, which, after removing constant terms, is equivalent to minimizing the weighted sum of squared residuals (SSR). Since the variances are constant, this simplifies to minimizing the unweighted SSR, defined as:\n$$\nS(k_1, k_2) = \\sum_{i=1}^n \\left[ (y_{B,i} - B(t_i | k_1, k_2))^2 + (y_{C,i} - C(t_i | k_1, k_2))^2 \\right]\n$$\nThe Maximum Likelihood Estimates (MLEs) of the parameters are the values that minimize this SSR function.\n\nThe problem poses a hypothesis test. The null hypothesis, $H_0$, posits that the pathway $A \\rightarrow B$ does not exist, while the alternative hypothesis, $H_1$, represents the full model. The rate constants must be non-negative.\n- Null Hypothesis $H_0: k_1 = 0$, with $k_2 \\ge 0$ as a nuisance parameter.\n- Alternative Hypothesis $H_1: k_1 > 0$, with $k_2 \\ge 0$ as a nuisance parameter. The MLE is found over the space $k_1 \\ge 0, k_2 \\ge 0$.\n\nTo perform the LRT, we find the MLEs under both hypotheses by minimizing the SSR:\n1.  Under $H_1$, we find $(\\widehat{k}_1, \\widehat{k}_2) = \\text{argmin}_{k_1 \\ge 0, k_2 \\ge 0} S(k_1, k_2)$. Let the minimum SSR be $\\widehat{S}_1 = S(\\widehat{k}_1, \\widehat{k}_2)$. This requires a $2$-dimensional constrained numerical optimization.\n2.  Under $H_0$, we set $k_1 = 0$. The model simplifies to $B(t) = 0$ and $C(t) = A_0(1 - e^{-k_2 t})$. We find $\\widehat{k}_2^{(0)} = \\text{argmin}_{k_2 \\ge 0} S(0, k_2)$. Let the minimum SSR be $\\widehat{S}_0 = S(0, \\widehat{k}_2^{(0)})$. This is a $1$-dimensional constrained numerical optimization.\n\nThe likelihood ratio test statistic $\\Lambda$ is defined as:\n$$\n\\Lambda = 2 \\left(\\ell(\\widehat{k}_1, \\widehat{k}_2) - \\ell(0, \\widehat{k}_2^{(0)})\\right)\n$$\nSubstituting the expression for the log-likelihood in terms of the SSR values:\n$$\n\\Lambda = 2 \\left( \\left(-\\frac{\\widehat{S}_1}{2\\sigma^2} - C\\right) - \\left(-\\frac{\\widehat{S}_0}{2\\sigma^2} - C\\right) \\right) = \\frac{\\widehat{S}_0 - \\widehat{S}_1}{\\sigma^2}\n$$\nwhere $C$ represents the constant terms. Since $H_0$ is a nested sub-model of $H_1$, it must be that $\\widehat{S}_0 \\ge \\widehat{S}_1$ and thus $\\Lambda \\ge 0$. Any numerically negative value for $\\Lambda$ due to optimizer tolerance is set to $0$.\n\nUnder the null hypothesis $H_0$, and subject to standard regularity conditions, $\\Lambda$ follows a chi-squared distribution. The number of degrees of freedom is the difference in the number of free parameters between $H_1$ and $H_0$, which is $2 - 1 = 1$. The problem specifies using the $\\chi^2_1$ distribution. The $p$-value is the probability of observing a test statistic at least as extreme as the one computed, assuming $H_0$ is true:\n$$\np = P(\\chi^2_1 \\ge \\Lambda) = 1 - F_{\\chi^2_1}(\\Lambda)\n$$\nwhere $F_{\\chi^2_1}$ is the cumulative distribution function (CDF) of the $\\chi^2$ distribution with $1$ degree of freedom.\n\nThe overall algorithm for each test case is as follows:\n1.  Generate a noise-free dataset $\\{ (t_i, y_{B,i}, y_{C,i}) \\}$ using the analytical solutions for $B(t)$ and $C(t)$ with the given \"true\" parameters $k_1^\\star$ and $k_2^\\star$.\n2.  Define the objective functions for the SSR, $S(k_1, k_2)$ for $H_1$ and $S(0, k_2)$ for $H_0$.\n3.  Use a numerical optimization algorithm (e.g., L-BFGS-B from SciPy) to find the minimum SSR under each hypothesis, $\\widehat{S}_1$ and $\\widehat{S}_0$, subject to non-negativity constraints on the rate constants.\n4.  Calculate the LRT statistic $\\Lambda = \\max(0, (\\widehat{S}_0 - \\widehat{S}_1)/\\sigma^2)$.\n5.  Compute the $p$-value using the $\\chi^2_1$ CDF.\nThis procedure will be implemented for the $3$ provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Implements a Likelihood Ratio Test for a branched first-order reaction network.\n    \"\"\"\n    # Define problem constants\n    A0 = 1.0\n    SIGMA = 2.0e-2\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"times\": np.array([0.0, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"k1_true\": 0.25,\n            \"k2_true\": 0.50,\n        },\n        {\n            \"times\": np.array([0.0, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"k1_true\": 0.0,\n            \"k2_true\": 0.50,\n        },\n        {\n            \"times\": np.array([0.0, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"k1_true\": 0.05,\n            \"k2_true\": 0.50,\n        },\n    ]\n\n    p_values = []\n\n    def model_full(t, k1, k2):\n        \"\"\"Analytical solution for B(t) and C(t) for the full model.\"\"\"\n        k_sum = k1 + k2\n        t = np.asarray(t)\n        \n        # Handle the case k1+k2 -> 0 to avoid numerical instability\n        if np.abs(k_sum) < 1e-9:\n            B = k1 * A0 * t\n            C = k2 * A0 * t\n        else:\n            # Use of np.expm1 to maintain precision for small arguments\n            term = -k_sum * t\n            factor = -np.expm1(term) / k_sum\n            B = k1 * A0 * factor\n            C = k2 * A0 * factor\n        return B, C\n\n    def model_H0(t, k2):\n        \"\"\"Analytical solution for C(t) under H0 (k1=0). B(t) is always 0.\"\"\"\n        t = np.asarray(t)\n        # Use of np.expm1 for precision\n        C = A0 * (-np.expm1(-k2 * t))\n        return C\n\n    for case in test_cases:\n        t_data = case[\"times\"]\n        k1_true = case[\"k1_true\"]\n        k2_true = case[\"k2_true\"]\n\n        # Generate noise-free measurement data\n        y_B, y_C = model_full(t_data, k1_true, k2_true)\n\n        # Objective function for H1 (minimizing SSR)\n        def objective_H1(k):\n            k1, k2 = k\n            B_model, C_model = model_full(t_data, k1, k2)\n            ssr = np.sum((y_B - B_model) ** 2) + np.sum((y_C - C_model) ** 2)\n            return ssr\n        \n        # Objective function for H0 (minimizing SSR)\n        def objective_H0(k):\n            k2 = k[0]\n            # Under H0, B_model is always 0\n            C_model = model_H0(t_data, k2)\n            ssr = np.sum(y_B ** 2) + np.sum((y_C - C_model) ** 2)\n            return ssr\n\n        # Perform MLE for H1\n        # Initial guess can be small positive numbers\n        initial_guess_H1 = [0.1, 0.1]\n        bounds_H1 = [(0, None), (0, None)]\n        res_H1 = minimize(\n            objective_H1, initial_guess_H1, method='L-BFGS-B', bounds=bounds_H1\n        )\n        S_hat_1 = res_H1.fun\n        \n        # Perform MLE for H0\n        initial_guess_H0 = [0.1]\n        bounds_H0 = [(0, None)]\n        res_H0 = minimize(\n            objective_H0, initial_guess_H0, method='L-BFGS-B', bounds=bounds_H0\n        )\n        S_hat_0 = res_H0.fun\n\n        # Calculate the Likelihood Ratio Test statistic\n        Lambda = (S_hat_0 - S_hat_1) / (SIGMA ** 2)\n        \n        # Enforce non-negativity due to numerical tolerances\n        Lambda = max(0.0, Lambda)\n        \n        # Calculate the p-value using the chi-squared distribution with 1 df\n        p_val = 1.0 - chi2.cdf(Lambda, df=1)\n        p_values.append(p_val)\n\n    # Format the final output as specified\n    print(f\"[{','.join([f'{p:.6f}' for p in p_values])}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2692475"}]}