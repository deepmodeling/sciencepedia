{"hands_on_practices": [{"introduction": "Parameter estimation in chemical kinetics often begins with fitting experimental data to a model. Before we dive into computational tools, it is crucial to understand the statistical principles that underpin these methods. This first exercise [@problem_id:2660590] guides you through a foundational derivation, showing how the principle of maximum likelihood leads directly to the familiar equations of linear regression for a simple zero-order kinetic model. Mastering this derivation provides a solid theoretical basis for the more complex nonlinear and weighted fitting techniques to come.", "problem": "Consider a batch reaction where the concentration of species $[A]$ follows a zero-order consumption model $[A](t) = [A]_{0} - k t$ under isothermal, constant-volume conditions. You collect $n$ independent measurements $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$, where $t_{i}$ denotes the known observation time and $y_{i}$ denotes the observed concentration. Assume the measurement errors are independent and identically distributed as Gaussian with mean $0$ and known variance $\\sigma^{2}$, and that there is nonzero variability in the design times so that $\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$, where $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^{n} t_{i}$.\n\nStarting from the definition of the likelihood under independent Gaussian errors and the model $y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$, where $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, derive the closed-form maximum likelihood estimates for $[A]_{0}$ and $k$. Express your final result explicitly in terms of the sums $\\sum_{i=1}^{n} y_{i}$, $\\sum_{i=1}^{n} t_{i}$, $\\sum_{i=1}^{n} t_{i}^{2}$, and $\\sum_{i=1}^{n} t_{i} y_{i}$, and the sample size $n$.\n\nYour final answer must be a single analytic expression giving both estimates as a row matrix in the order $\\big(\\widehat{[A]}_{0}, \\widehat{k}\\big)$. No numerical evaluation or rounding is required, and no units should be included in the final expression.", "solution": "The problem statement is critically examined for validity before any attempt at a solution.\n\n**Step 1: Extract Givens**\n- **Reaction Model:** Zero-order consumption, `$[A](t) = [A]_{0} - k t$`.\n- **System Conditions:** Isothermal, constant-volume batch reactor.\n- **Data:** `$n$` independent measurements `$\\{(t_{i}, y_{i})\\}_{i=1}^{n}$`.\n- **Measurement Error:** Errors are independent and identically distributed (i.i.d.) from a Gaussian distribution with mean `$0$` and known variance `$\\sigma^{2}$`.\n- **Statistical Model:** `$y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$`, where `$\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$`.\n- **Constraint:** The observation times exhibit nonzero variability, `$\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$`, where `$\\bar{t} = \\frac{1}{n}\\sum_{i=1}^{n} t_{i}$`.\n- **Objective:** Derive the closed-form maximum likelihood estimates (MLE) for the parameters `$[A]_{0}$` and `$k$`. The result is to be expressed in terms of the sums `$\\sum_{i=1}^{n} y_{i}$`, `$\\sum_{i=1}^{n} t_{i}$`, `$\\sum_{i=1}^{n} t_{i}^{2}$`, `$\\sum_{i=1}^{n} t_{i} y_{i}$`, and the sample size `$n$`.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded:** The problem uses a standard zero-order kinetic model and a classical statistical framework (linear regression with Gaussian errors). These are fundamental concepts in physical chemistry and statistics. It is scientifically sound.\n- **Well-Posed:** The problem is well-posed. The model is linear in the parameters `$[A]_{0}$` and `$k$`. The assumption of i.i.d. Gaussian errors is standard. The explicit constraint `$\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$` ensures that the design matrix has full rank, which guarantees the existence of a unique solution for the parameters.\n- **Objective:** The language is precise, quantitative, and free of any subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is found to be **valid**. It is a standard, well-defined problem in statistical parameter estimation. A solution will be derived.\n\n**Derivation of Maximum Likelihood Estimates**\n\nThe statistical model for an observation `$y_i$` is given by `$y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$`. Since `$\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$`, it follows that the observation `$y_i$` is a random variable from a Gaussian distribution with mean `$\\mu_i = E[y_i] = [A]_{0} - k t_{i}$` and variance `$\\sigma^{2}$`. The probability density function (PDF) for a single observation `$y_i$` is:\n$$p(y_i \\mid [A]_0, k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - ([A]_0 - k t_i))^2}{2\\sigma^2}\\right)$$\nThe measurements are independent, so the likelihood function `$L([A]_0, k)$` for the entire dataset `$\\{(t_i, y_i)\\}_{i=1}^n$` is the product of the individual PDFs:\n$$L([A]_0, k) = \\prod_{i=1}^{n} p(y_i \\mid [A]_0, k) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2\\right)$$\nTo find the maximum likelihood estimates (MLEs), we maximize `$L$`, which is equivalent to maximizing its natural logarithm, the log-likelihood function `$\\ln L$`:\n$$\\ln L([A]_0, k) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2$$\nMaximizing `$\\ln L([A]_0, k)$` with respect to `$ [A]_0 $` and `$k$` is equivalent to minimizing the sum of squared errors term, `$S([A]_0, k)$`, because all other terms are constant with respect to the parameters:\n$$S([A]_0, k) = \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2$$\nNote the model `$E[y_i] = [A]_0 - k t_i$` means the residual is `$y_i - E[y_i] = y_i - ([A]_0 - k t_i) = y_i - [A]_0 + k t_i$`.\n\nTo minimize `$S$`, we take the partial derivatives with respect to `$[A]_0$` and `$k$`, and set them to zero. The resulting estimates will be denoted `$\\widehat{[A]}_0$` and `$\\widehat{k}$`.\n\nFirst, the partial derivative with respect to `$[A]_0$`:\n$$\\frac{\\partial S}{\\partial [A]_0} = \\sum_{i=1}^{n} 2(y_i - [A]_0 + k t_i)(-1) = -2 \\left( \\sum_{i=1}^{n} y_i - n[A]_0 + k \\sum_{i=1}^{n} t_i \\right)$$\nSetting this derivative to zero gives the first normal equation:\n$$n \\widehat{[A]}_0 - \\widehat{k} \\sum_{i=1}^{n} t_i = \\sum_{i=1}^{n} y_i \\quad (1)$$\n\nSecond, the partial derivative with respect to `$k$`:\n$$\\frac{\\partial S}{\\partial k} = \\sum_{i=1}^{n} 2(y_i - [A]_0 + k t_i)(t_i) = 2 \\left( \\sum_{i=1}^{n} y_i t_i - [A]_0 \\sum_{i=1}^{n} t_i + k \\sum_{i=1}^{n} t_i^2 \\right)$$\nSetting this derivative to zero gives the second normal equation:\n$$\\widehat{[A]}_0 \\sum_{i=1}^{n} t_i - \\widehat{k} \\sum_{i=1}^{n} t_i^2 = \\sum_{i=1}^{n} y_i t_i \\quad (2)$$\n\nWe now have a system of two linear equations in the two unknowns, `$\\widehat{[A]}_0$` and `$\\widehat{k}$`:\n$$\n\\begin{cases}\n    n \\widehat{[A]}_0 - \\left(\\sum_{i=1}^{n} t_i\\right) \\widehat{k} = \\sum_{i=1}^{n} y_i \\\\\n    \\left(\\sum_{i=1}^{n} t_i\\right) \\widehat{[A]}_0 - \\left(\\sum_{i=1}^{n} t_i^2\\right) \\widehat{k} = \\sum_{i=1}^{n} y_i t_i\n\\end{cases}\n$$\nThis system can be solved using various methods, such as substitution or Cramer's rule. Using Cramer's rule, the determinant of the coefficient matrix is:\n$$\\Delta = (n)\\left(-\\sum_{i=1}^{n} t_i^2\\right) - \\left(-\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} t_i\\right) = -\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)$$\nThe estimate `$\\widehat{[A]}_0$` is given by:\n$$\\widehat{[A]}_0 = \\frac{\\begin{vmatrix} \\sum_{i=1}^{n} y_i & -\\sum_{i=1}^{n} t_i \\\\ \\sum_{i=1}^{n} y_i t_i & -\\sum_{i=1}^{n} t_i^2 \\end{vmatrix}}{\\Delta} = \\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(-\\sum_{i=1}^{n} t_i^2\\right) - \\left(-\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)}$$\n$$\\widehat{[A]}_0 = \\frac{-\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) + \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)} = \\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}$$\nThe estimate `$\\widehat{k}$` is given by:\n$$\\widehat{k} = \\frac{\\begin{vmatrix} n & \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} t_i & \\sum_{i=1}^{n} y_i t_i \\end{vmatrix}}{\\Delta} = \\frac{n\\left(\\sum_{i=1}^{n} y_i t_i\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)}$$\n$$\\widehat{k} = \\frac{\\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right) - n \\sum_{i=1}^{n} y_i t_i}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}$$\nThese are the closed-form solutions for the maximum likelihood estimates of `$[A]_0$` and `$k$`.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} t_i y_i\\right)}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2} & \\frac{\\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right) - n \\sum_{i=1}^{n} t_i y_i}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}\n\\end{pmatrix}\n}\n$$", "id": "2660590"}, {"introduction": "While linear models provide a crucial theoretical starting point, most kinetic systems are described by nonlinear integrated rate laws, such as the exponential decay of a first-order reaction. This hands-on coding exercise [@problem_id:2660532] transitions from theory to practice, tasking you with fitting a nonlinear first-order model to synthetic data using weighted least squares. You will also compute the approximate covariance matrix for your parameter estimates, an essential step for understanding the reliability and interplay of your fitted kinetic constants.", "problem": "You are given a dynamic model for a single-step, irreversible, first-order decay reaction, where the concentration of species $A$ obeys the ordinary differential equation $d[A]/dt = -k[A]$ with initial condition $[A](0) = [A]_0$. Measurements of the concentration are made at discrete times and are corrupted by independent additive noise. The goal is to estimate the parameter vector $\\theta = ([A]_0, k)$ by fitting the integrated rate law to data using Weighted Least Squares (WLS) under a Gaussian noise model, and to compute an approximate covariance matrix of the estimator.\n\nFundamental base and assumptions:\n- The deterministic dynamics are governed by the law of mass action for a first-order process. The measurement model is $y_i = [A](t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ independently across $i$.\n- The independent variable is time $t$ measured in seconds, and the observable concentration $[A]$ is measured in moles per liter.\n- Under the stated assumptions, the maximum likelihood estimator coincides with the WLS estimator, where the weights are $w_i = 1/\\sigma_i^2$.\n\nTasks your program must perform for each test case:\n1. Starting from the fundamental rate law and initial condition, derive the expression for $[A](t)$ as a function of $[A]_0$ and $k$.\n2. Formulate the WLS objective implied by the Gaussian noise model with known variances $\\sigma_i^2$. Estimate $\\theta = ([A]_0, k)$ by minimizing this objective with respect to $[A]_0 > 0$ and $k > 0$.\n3. Using a first-order (Gaussâ€“Newton) approximation consistent with the WLS formulation, compute the approximate covariance matrix $\\mathrm{Cov}(\\hat{\\theta})$ at the optimum. Report the entries $\\mathrm{Var}([A]_0)$, $\\mathrm{Var}(k)$, and $\\mathrm{Cov}([A]_0,k)$.\n4. Implement a numerically stable solver that enforces positivity of $[A]_0$ and $k$, and uses an analytically correct sensitivity (Jacobian) of the model with respect to the parameters when forming the approximation.\n5. Use the exact test suite below, including the specified pseudorandom seeds to generate the synthetic observations $y_i$, and the given known standard deviations $\\sigma_i$ to define the WLS weights.\n\nPhysical and numerical units to use:\n- Report $[A]_0$ in $\\mathrm{mol\\,L^{-1}}$ and $k$ in $\\mathrm{s^{-1}}$. The variances and covariance must be in the corresponding squared and mixed units.\n- The final program output must be numerical values without unit symbols; however, all computations must be carried out in the stated units.\n\nTest suite (three datasets). For each dataset $j \\in \\{1,2,3\\}$:\n- Generate synthetic observations using $y_i^{(j)} = [A]_0^{\\mathrm{true}(j)} \\exp(-k^{\\mathrm{true}(j)} t_i^{(j)}) + \\varepsilon_i^{(j)}$, where $\\varepsilon_i^{(j)} \\sim \\mathcal{N}(0, (\\sigma_i^{(j)})^2)$, independently. Use the specified seed to initialize a pseudorandom number generator for each dataset, so that the resulting $y_i^{(j)}$ are deterministic and reproducible.\n\nDataset $1$ (happy path, homoscedastic):\n- True parameters: $[A]_0^{\\mathrm{true}(1)} = 1.25$ $\\mathrm{mol\\,L^{-1}}$, $k^{\\mathrm{true}(1)} = 0.0075$ $\\mathrm{s^{-1}}$.\n- Time points (in seconds): $t^{(1)} = (0, 50, 100, 150, 200, 250, 300)$.\n- Known standard deviations (in $\\mathrm{mol\\,L^{-1}}$): $\\sigma_i^{(1)} \\equiv 0.02$ for all $i$.\n- Seed: $314159$.\n\nDataset $2$ (heteroscedastic noise increasing with time):\n- True parameters: $[A]_0^{\\mathrm{true}(2)} = 0.8$ $\\mathrm{mol\\,L^{-1}}$, $k^{\\mathrm{true}(2)} = 0.01$ $\\mathrm{s^{-1}}$.\n- Time points (in seconds): $t^{(2)} = (0, 40, 80, 120, 160, 200, 240, 280, 320)$.\n- Known standard deviations (in $\\mathrm{mol\\,L^{-1}}$): $\\sigma_i^{(2)} = 0.01 + 0.00005\\, t_i^{(2)}$.\n- Seed: $271828$.\n\nDataset $3$ (boundary case with weak information on $k$ over a short window):\n- True parameters: $[A]_0^{\\mathrm{true}(3)} = 1.0$ $\\mathrm{mol\\,L^{-1}}$, $k^{\\mathrm{true}(3)} = 0.001$ $\\mathrm{s^{-1}}$.\n- Time points (in seconds): $t^{(3)} = (0, 5, 10, 15, 20, 25, 30)$.\n- Known standard deviations (in $\\mathrm{mol\\,L^{-1}}$): $\\sigma_i^{(3)} \\equiv 0.005$ for all $i$.\n- Seed: $161803$.\n\nAlgorithmic requirements:\n- Use a nonlinear least-squares solver with WLS residuals. Provide an analytic Jacobian of the model with respect to $[A]_0$ and $k$ to the solver. Enforce bounds $[A]_0 > 0$ and $k > 0$.\n- Compute the approximate covariance matrix at the optimum using a first-order information approximation consistent with the WLS weights and the Gaussian assumption with known $\\sigma_i$.\n\nFinal output format:\n- Your program should produce a single line of output containing the concatenated results across the three datasets, in the exact order\n$[$$\\hat{[A]}_0^{(1)}$, $\\hat{k}^{(1)}$, $\\mathrm{Var}^{(1)}([A]_0)$, $\\mathrm{Var}^{(1)}(k)$, $\\mathrm{Cov}^{(1)}([A]_0,k)$, $\\hat{[A]}_0^{(2)}$, $\\hat{k}^{(2)}$, $\\mathrm{Var}^{(2)}([A]_0)$, $\\mathrm{Var}^{(2)}(k)$, $\\mathrm{Cov}^{(2)}([A]_0,k)$, $\\hat{[A]}_0^{(3)}$, $\\hat{k}^{(3)}$, $\\mathrm{Var}^{(3)}([A]_0)$, $\\mathrm{Var}^{(3)}(k)$, $\\mathrm{Cov}^{(3)}([A]_0,k)$$]$ with no spaces.\n- Each numeric value must be rounded to exactly $6$ digits after the decimal point.\n- Angles are not involved. All quantities are real-valued floats.\n\nYour program must be complete and runnable as is, without any external input, and must strictly follow the specified output format. The only permitted libraries are the Python standard library, NumPy, and SciPy.", "solution": "The problem posed is a standard exercise in parameter estimation for a dynamic system in chemical kinetics. It is scientifically grounded, well-posed, and contains all necessary information for a deterministic and verifiable solution. The problem is therefore deemed valid.\n\nThe solution proceeds in four principal steps:\n1.  Derivation of the analytical model from the governing differential equation.\n2.  Formulation of the Weighted Least Squares (WLS) objective function for parameter estimation.\n3.  Derivation of the analytical Jacobian for the optimization algorithm.\n4.  Formulation of the parameter covariance matrix approximation.\n\n**1. Integrated Rate Law**\n\nThe problem starts with the fundamental rate law for a first-order irreversible decay process:\n$$\n\\frac{d[A]}{dt} = -k[A]\n$$\nThis is a first-order linear ordinary differential equation. We solve it by separation of variables, subject to the initial condition $[A](t=0) = [A]_0$.\n$$\n\\frac{d[A]}{[A]} = -k \\, dt\n$$\nIntegrating both sides from the initial state $([A]_0, 0)$ to a state $([A], t)$ gives:\n$$\n\\int_{[A]_0}^{[A]} \\frac{1}{[A]'} \\, d[A]' = \\int_0^t -k \\, dt'\n$$\n$$\n\\ln([A]) - \\ln([A]_0) = -k(t - 0)\n$$\n$$\n\\ln\\left(\\frac{[A]}{[A]_0}\\right) = -kt\n$$\nExponentiating both sides yields the integrated rate law, which is our model function for the concentration of species $A$ at time $t$:\n$$\n[A](t) = [A]_0 e^{-kt}\n$$\nThis function, which we denote $f(t; \\theta)$, depends on the parameter vector $\\theta = ([A]_0, k)^T$.\n\n**2. Weighted Least Squares (WLS) Formulation**\n\nThe measurements $y_i$ at times $t_i$ are modeled as $y_i = f(t_i; \\theta) + \\varepsilon_i$, where the errors $\\varepsilon_i$ are independent and normally distributed, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$, with known variances $\\sigma_i^2$. Under these assumptions, the maximum likelihood estimator for $\\theta$ is the one that minimizes the sum of squared, weighted residuals. This is the Weighted Least Squares (WLS) estimator.\n\nThe WLS objective function $S(\\theta)$ is given by:\n$$\nS(\\theta) = \\sum_{i=1}^{N} \\left( \\frac{y_i - f(t_i; \\theta)}{\\sigma_i} \\right)^2 = \\sum_{i=1}^{N} \\left( \\frac{y_i - [A]_0 e^{-kt_i}}{\\sigma_i} \\right)^2\n$$\nwhere $N$ is the number of observations. We seek the parameter values $\\hat{\\theta} = (\\hat{[A]}_0, \\hat{k})^T$ that minimize this function, subject to the physical constraints $[A]_0 > 0$ and $k > 0$.\n\n**3. Nonlinear Optimization and Analytical Jacobian**\n\nMinimizing $S(\\theta)$ is a nonlinear least-squares problem. We employ a numerical solver, specifically `scipy.optimize.least_squares`, which is well-suited for this task. The solver's efficiency and reliability are greatly enhanced by providing the analytical Jacobian of the residuals vector with respect to the parameters.\n\nThe solver minimizes the sum of squares of a vector function. Let this vector function be $\\mathbf{r}_w(\\theta)$, the vector of weighted residuals, whose $i$-th component is:\n$$\nr_{w,i}(\\theta) = \\frac{f(t_i; \\theta) - y_i}{\\sigma_i} = \\frac{[A]_0 e^{-kt_i} - y_i}{\\sigma_i}\n$$\nThe Jacobian matrix, $J_w$, is an $N \\times 2$ matrix where $(J_w)_{ij} = \\frac{\\partial r_{w,i}}{\\partial \\theta_j}$. The two columns correspond to the partial derivatives with respect to $[A]_0$ and $k$.\n\nFor the first parameter, $[A]_0$:\n$$\n\\frac{\\partial r_{w,i}}{\\partial [A]_0} = \\frac{1}{\\sigma_i} \\frac{\\partial}{\\partial [A]_0} \\left( [A]_0 e^{-kt_i} - y_i \\right) = \\frac{e^{-kt_i}}{\\sigma_i}\n$$\nFor the second parameter, $k$:\n$$\n\\frac{\\partial r_{w,i}}{\\partial k} = \\frac{1}{\\sigma_i} \\frac{\\partial}{\\partial k} \\left( [A]_0 e^{-kt_i} - y_i \\right) = \\frac{[A]_0}{\\sigma_i} \\frac{\\partial}{\\partial k} \\left( e^{-kt_i} \\right) = \\frac{[A]_0}{\\sigma_i} \\left( -t_i e^{-kt_i} \\right) = -\\frac{[A]_0 t_i e^{-kt_i}}{\\sigma_i}\n$$\nThese expressions for the partial derivatives form the columns of the Jacobian matrix that is supplied to the optimization routine.\n\n**4. Covariance Matrix of the Estimator**\n\nThe uncertainty in the estimated parameters $\\hat{\\theta}$ is characterized by their covariance matrix, $\\mathrm{Cov}(\\hat{\\theta})$. For a nonlinear least squares problem, a common and effective approximation is derived from the Gauss-Newton method. This approximation relates the covariance matrix to the inverse of the Fisher Information Matrix, which itself is approximated by $J_w^T J_w$ evaluated at the solution $\\hat{\\theta}$.\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx \\left( J_w(\\hat{\\theta})^T J_w(\\hat{\\theta}) \\right)^{-1}\n$$\nThis $2 \\times 2$ matrix has the variances of the estimators on its diagonal and the covariance between them on the off-diagonal:\n$$\n\\mathrm{Cov}(\\hat{\\theta}) = \\begin{pmatrix} \\mathrm{Var}(\\hat{[A]}_0) & \\mathrm{Cov}(\\hat{[A]}_0, \\hat{k}) \\\\ \\mathrm{Cov}(\\hat{[A]}_0, \\hat{k}) & \\mathrm{Var}(\\hat{k}) \\end{pmatrix}\n$$\nThe `scipy.optimize.least_squares` function conveniently returns the Jacobian matrix $J_w$ evaluated at the final parameter estimates, allowing for a direct computation of this covariance matrix.\n\nThe implementation will follow these derivations precisely. For each dataset, synthetic data is generated using the provided true parameters and a seeded random number generator for reproducibility. The WLS optimization is then performed to find $\\hat{[A]}_0$ and $\\hat{k}$, followed by the calculation of the required variance and covariance terms.", "answer": "```python\n# Final runnable Python code for the specified environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Solves for the parameters of a first-order decay model for three\n    different datasets using Weighted Least Squares and computes the\n    covariance matrix of the estimators.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Dataset 1\",\n            \"true_params\": [1.25, 0.0075], # [A0_true, k_true]\n            \"times\": np.array([0., 50., 100., 150., 200., 250., 300.]),\n            \"sigma\": lambda t: np.full_like(t, 0.02),\n            \"seed\": 314159\n        },\n        {\n            \"name\": \"Dataset 2\",\n            \"true_params\": [0.8, 0.01],\n            \"times\": np.array([0., 40., 80., 120., 160., 200., 240., 280., 320.]),\n            \"sigma\": lambda t: 0.01 + 0.00005 * t,\n            \"seed\": 271828\n        },\n        {\n            \"name\": \"Dataset 3\",\n            \"true_params\": [1.0, 0.001],\n            \"times\": np.array([0., 5., 10., 15., 20., 25., 30.]),\n            \"sigma\": lambda t: np.full_like(t, 0.005),\n            \"seed\": 161803\n        }\n    ]\n\n    # --- Model and Objective Function Definitions ---\n\n    def model(params, t_vals):\n        \"\"\"\n        Calculates concentration based on first-order decay model.\n        params: [A0, k]\n        \"\"\"\n        A0, k = params\n        return A0 * np.exp(-k * t_vals)\n\n    def residuals_wls(params, t_vals, y_obs, sigma_vals):\n        \"\"\"\n        Calculates weighted residuals for WLS fitting.\n        (model - y) / sigma\n        \"\"\"\n        return (model(params, t_vals) - y_obs) / sigma_vals\n\n    def jacobian_wls(params, t_vals, y_obs, sigma_vals):\n        \"\"\"\n        Calculates the analytical Jacobian of the weighted residuals.\n        \"\"\"\n        A0, k = params\n        jac = np.zeros((len(t_vals), 2))\n        exp_term = np.exp(-k * t_vals)\n        \n        # d(residual)/d(A0) = (1/sigma) * d(model)/d(A0)\n        jac[:, 0] = exp_term / sigma_vals\n        \n        # d(residual)/d(k) = (1/sigma) * d(model)/d(k)\n        jac[:, 1] = -A0 * t_vals * exp_term / sigma_vals\n        \n        return jac\n\n    all_results = []\n    \n    for case in test_cases:\n        A0_true, k_true = case[\"true_params\"]\n        t = case[\"times\"]\n        seed = case[\"seed\"]\n        sigma = case[\"sigma\"](t)\n        \n        # 1. Generate synthetic data\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma)\n        y_obs = model([A0_true, k_true], t) + noise\n        \n        # 2. Perform nonlinear least-squares optimization\n        # Use true parameters as initial guess for robust convergence in this test setup\n        initial_guess = case[\"true_params\"]\n        # Enforce positivity constraints [A0 > 0, k > 0]\n        bounds = ([1e-9, 1e-9], [np.inf, np.inf])\n        \n        lsq_result = least_squares(\n            residuals_wls,\n            x0=initial_guess,\n            jac=jacobian_wls,\n            bounds=bounds,\n            method='trf',\n            args=(t, y_obs, sigma)\n        )\n        \n        A0_est, k_est = lsq_result.x\n        \n        # 3. Compute the approximate covariance matrix\n        # Cov(theta) approx (J^T J)^-1, where J is Jacobian of weighted residuals\n        J = lsq_result.jac\n        try:\n            cov_matrix = np.linalg.inv(J.T @ J)\n            var_A0 = cov_matrix[0, 0]\n            var_k = cov_matrix[1, 1]\n            cov_A0_k = cov_matrix[0, 1]\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix is singular (e.g., poor data)\n            var_A0, var_k, cov_A0_k = (np.inf, np.inf, np.inf)\n\n        # 4. Store the results in the required order\n        all_results.extend([A0_est, k_est, var_A0, var_k, cov_A0_k])\n\n    # 5. Format and print the final output string\n    # E.g. [1.249123,0.007505,...]\n    output_str = \",\".join([f\"{val:.6f}\" for val in all_results])\n    print(f\"[{output_str}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2660532"}, {"introduction": "Often, a single chemical reaction can be monitored by observing the concentration of multiple species over time. A common but suboptimal approach is to analyze each concentration profile separately. This final practice [@problem_id:2660601] illuminates a more powerful strategy: global analysis. By simultaneously fitting the concentration data for both a reactant and a product, while enforcing a common rate constant and the principle of mass balance, you will directly compare the results to separate fits and discover the significant improvement in precision that this unified approach provides.", "problem": "Consider the irreversible unimolecular reaction governed by mass-action kinetics, where species $A$ converts to species $B$ with a first-order rate constant $k$:\n$$ A \\xrightarrow{k} B. $$\nLet $[A](t)$ and $[B](t)$ denote the concentrations of $A$ and $B$ at time $t$, respectively. Under well-mixed conditions and constant volume, the governing ordinary differential equations (ODEs) are\n$$ \\frac{d[A]}{dt} = -k [A], \\quad \\frac{d[B]}{dt} = k [A], $$\ntogether with the conservation law (mass balance)\n$$ [A](t) + [B](t) = M, $$\nwhere $M$ is a constant equal to $[A](0) + [B](0)$.\n\nYou will generate synthetic measurements of $[A](t)$ and $[B](t)$ by evaluating the exact trajectories implied by the ODEs at specified times and then adding independent Gaussian measurement noise with specified standard deviations. You will then estimate the kinetic parameter $k$ in three ways:\n- Separate fit to $[A](t)$ only: estimate $k_A$ by minimizing the weighted sum of squared residuals between the measured $[A](t_i)$ and the corresponding model predictions. Treat $[A](0)$ as an unknown parameter to be estimated. Enforce non-negativity constraints on all concentration parameters and on $k_A$.\n- Separate fit to $[B](t)$ only: estimate $k_B$ by minimizing the weighted sum of squared residuals between the measured $[B](t_i)$ and the corresponding model predictions. Treat $[B](0)$, $[A](0)$, and $k_B$ as unknown parameters to be estimated. Enforce non-negativity constraints on all concentration parameters and on $k_B$.\n- Joint fit to $[A](t)$ and $[B](t)$ enforcing a common rate constant and mass balance: estimate a single $k_{\\mathrm{joint}}$ by minimizing the combined weighted sum of squared residuals for both series simultaneously, using a model that is consistent with the ODEs and satisfies the conservation law for all $t$, with unknown parameters $[A](0)$, $[B](0)$, and $k_{\\mathrm{joint}}$. Enforce non-negativity constraints on all concentration parameters and on $k_{\\mathrm{joint}}$.\n\nIn all three estimation tasks, assume that the measurement errors are independent and Gaussian with known standard deviations $\\sigma_A$ for $[A](t)$ and $\\sigma_B$ for $[B](t)$, and use weights equal to the inverse variances, that is, divide residuals by the corresponding standard deviations before squaring and summing. Use deterministic pseudo-random number generation with the specified seeds to ensure reproducibility.\n\nUnits and reporting requirements:\n- Time $t$ must be in seconds.\n- Concentrations $[A]$ and $[B]$ must be in moles per liter.\n- The rate constants $k_A$, $k_B$, and $k_{\\mathrm{joint}}$ must be reported in inverse seconds, that is, $\\mathrm{s}^{-1}$.\n- Express all reported rate constants in $\\mathrm{s}^{-1}$ as decimal floats rounded to exactly six digits after the decimal point.\n\nTest suite specification. For each test case below, generate the time grid, compute the noise-free trajectories consistent with the ODEs and conservation law, add Gaussian noise with the given standard deviations and seed, and then perform the three estimation procedures described above. Use the following test cases:\n\n- Case $1$ (general, well-conditioned):\n  - True parameters: $k_\\text{true} = 0.35$ $\\mathrm{s}^{-1}$, $[A](0) = 1.1$ $\\mathrm{mol \\cdot L^{-1}}$, $[B](0) = 0.3$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Time grid: $N = 41$ equally spaced points from $t=0$ to $t_\\max = 8.0 \\ \\mathrm{s}$ inclusive.\n  - Noise levels: $\\sigma_A = 0.01$ $\\mathrm{mol \\cdot L^{-1}}$, $\\sigma_B = 0.01$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Random seed: $13579$.\n\n- Case $2$ (slow kinetics, long observation window):\n  - True parameters: $k_\\text{true} = 0.02$ $\\mathrm{s}^{-1}$, $[A](0) = 2.0$ $\\mathrm{mol \\cdot L^{-1}}$, $[B](0) = 0.4$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Time grid: $N = 51$ equally spaced points from $t=0$ to $t_\\max = 200.0 \\ \\mathrm{s}$ inclusive.\n  - Noise levels: $\\sigma_A = 0.01$ $\\mathrm{mol \\cdot L^{-1}}$, $\\sigma_B = 0.015$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Random seed: $24680$.\n\n- Case $3$ (sparse sampling, fast kinetics):\n  - True parameters: $k_\\text{true} = 1.5$ $\\mathrm{s}^{-1}$, $[A](0) = 0.5$ $\\mathrm{mol \\cdot L^{-1}}$, $[B](0) = 0.0$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Time grid: explicitly specified as $t \\in \\{0.0, 0.2, 0.5, 1.0, 1.5\\} \\ \\mathrm{s}$.\n  - Noise levels: $\\sigma_A = 0.02$ $\\mathrm{mol \\cdot L^{-1}}$, $\\sigma_B = 0.02$ $\\mathrm{mol \\cdot L^{-1}}$.\n  - Random seed: $97531$.\n\nYour program must perform the following steps for each case:\n- Generate $[A](t)$ and $[B](t)$ trajectories from the ODEs and conservation law using the true parameters.\n- Add independent Gaussian noise with the specified standard deviations and seed to obtain the noisy measurements.\n- Fit the separate $[A](t)$ model to estimate $k_A$; fit the separate $[B](t)$ model to estimate $k_B$; fit the joint model enforcing a common rate constant and mass balance to estimate $k_{\\mathrm{joint}}$.\n- Round the three estimated rate constants to exactly six digits after the decimal point, expressed in $\\mathrm{s}^{-1}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, one inner list per test case in the order of the cases above. Each inner list must contain the three rounded floats $[k_A, k_B, k_{\\mathrm{joint}}]$ in $\\mathrm{s}^{-1}$ for that case. For example, an output with one case would look like $[[0.123456,0.123457,0.123450]]$. For this problem with three cases, the required output must have the form $[[k_{A,1},k_{B,1},k_{\\mathrm{joint},1}],[k_{A,2},k_{B,2},k_{\\mathrm{joint},2}],[k_{A,3},k_{B,3},k_{\\mathrm{joint},3}]]$ with no spaces anywhere in the line.", "solution": "The problem as stated is logically consistent, scientifically sound, and provides all necessary information for a unique and verifiable solution. It represents a standard and well-posed problem in chemical kinetics, specifically parameter estimation from time-series data. We will therefore proceed with a full solution.\n\nThe core of the problem is to estimate a kinetic parameter from synthetic data using three different statistical models and to compare the results. The reaction is the unimolecular, irreversible conversion of a species $A$ to a species $B$, represented by $A \\xrightarrow{k} B$.\n\nFirst, we establish the analytical model for the species concentrations. The rate of disappearance of $A$ is given by the first-order rate law:\n$$ \\frac{d[A]}{dt} = -k [A] $$\nThis is a linear first-order ordinary differential equation. With the initial condition $[A](t=0) = [A]_0$, this equation is integrated to yield the exponential decay function for the concentration of $A$:\n$$ [A](t) = [A]_0 e^{-kt} $$\nThe total concentration of species $A$ and $B$ is conserved. The mass balance equation is:\n$$ [A](t) + [B](t) = M = \\text{constant} $$\nThe constant $M$ is the total initial concentration, $M = [A]_0 + [B]_0$. Using this conservation law, we derive the concentration of species $B$ as a function of time:\n$$ [B](t) = M - [A](t) = ([A]_0 + [B]_0) - [A]_0 e^{-kt} $$\nThis can be rearranged to a more common form:\n$$ [B](t) = [B]_0 + [A]_0 (1 - e^{-kt}) $$\nThese two equations, for $[A](t)$ and $[B](t)$, form the deterministic part of our model. They will be used both to generate the synthetic data and to fit the parameters.\n\nSynthetic data generation proceeds in two steps. First, for each test case, the \"true\" noise-free trajectories $[A]_{\\text{true}}(t_i)$ and $[B]_{\\text{true}}(t_i)$ are computed at a set of discrete time points $\\{t_i\\}$ using the provided true parameters ($k_{\\text{true}}$, $[A]_{0, \\text{true}}$, $[B]_{0, \\text{true}}$). Second, independent, zero-mean Gaussian measurement noise is added to these true concentrations:\n$$ [A]_{\\text{meas}}(t_i) = [A]_{\\text{true}}(t_i) + \\epsilon_{A,i}, \\quad \\text{where} \\quad \\epsilon_{A,i} \\sim \\mathcal{N}(0, \\sigma_A^2) $$\n$$ [B]_{\\text{meas}}(t_i) = [B]_{\\text{true}}(t_i) + \\epsilon_{B,i}, \\quad \\text{where} \\quad \\epsilon_{B,i} \\sim \\mathcal{N}(0, \\sigma_B^2) $$\nThe standard deviations of the noise, $\\sigma_A$ and $\\sigma_B$, are specified for each case. To ensure reproducibility, a pseudo-random number generator is initialized with a specific seed for each case.\n\nParameter estimation is performed by minimizing a weighted sum of squared residuals (WSSR), also known as the chi-squared ($\\chi^2$) objective function. Given a set of measured data points $(t_i, y_i)$, their uncertainties $\\sigma_i$, and a model function $y_{\\text{model}}(t; \\boldsymbol{\\theta})$ dependent on parameters $\\boldsymbol{\\theta}$, the objective is to find the parameters $\\boldsymbol{\\theta}^*$ that minimize $\\chi^2$:\n$$ \\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\chi^2(\\boldsymbol{\\theta}) = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i} \\left( \\frac{y_i - y_{\\text{model}}(t_i; \\boldsymbol{\\theta})}{\\sigma_i} \\right)^2 $$\nThis minimization will be performed numerically using the `scipy.optimize.minimize` function, with bounds to enforce the physical non-negativity constraints on all parameters (concentrations and rate constants).\n\nWe will apply this framework to three distinct estimation scenarios:\n\n1.  **Separate Fit to $[A](t)$ only**:\n    The model is $[A]_{\\text{model}}(t) = [A]_0 e^{-k_A t}$. The vector of unknown parameters is $\\boldsymbol{\\theta}_A = ([A]_0, k_A)$. The objective function to minimize is:\n    $$ \\chi^2_A([A]_0, k_A) = \\sum_{i} \\left( \\frac{[A]_{\\text{meas}}(t_i) - [A]_0 e^{-k_A t_i}}{\\sigma_A} \\right)^2 $$\n    The parameters $[A]_0$ and $k_A$ are estimated subject to the constraints $[A]_0 \\ge 0$ and $k_A \\ge 0$. The resulting estimate for the rate constant is denoted $k_A$.\n\n2.  **Separate Fit to $[B](t)$ only**:\n    The model is $[B]_{\\text{model}}(t) = [B]_0 + [A]_0(1 - e^{-k_B t})$. Here, the vector of unknown parameters is $\\boldsymbol{\\theta}_B = ([B]_0, [A]_0, k_B)$. The objective function is:\n    $$ \\chi^2_B([B]_0, [A]_0, k_B) = \\sum_{i} \\left( \\frac{[B]_{\\text{meas}}(t_i) - \\left( [B]_0 + [A]_0(1 - e^{-k_B t_i}) \\right)}{\\sigma_B} \\right)^2 $$\n    All three parameters are estimated from the single time series for $[B]$, subject to $[B]_0 \\ge 0$, $[A]_0 \\ge 0$, and $k_B \\ge 0$. The rate constant estimate is $k_B$.\n\n3.  **Joint Fit to $[A](t)$ and $[B](t)$**:\n    This approach uses all available information simultaneously and enforces the physical model consistently across both datasets. The vector of unknown parameters is $\\boldsymbol{\\theta}_{\\text{joint}} = ([A]_0, [B]_0, k_{\\text{joint}})$. The model for $[A](t)$ and $[B](t)$ are coupled through these shared parameters:\n    $$ [A]_{\\text{model}}(t) = [A]_0 e^{-k_{\\text{joint}} t} $$\n    $$ [B]_{\\text{model}}(t) = [B]_0 + [A]_0(1 - e^{-k_{\\text{joint}} t}) $$\n    The objective function is the sum of the $\\chi^2$ values for each series:\n    $$ \\chi^2_{\\text{joint}}(\\boldsymbol{\\theta}_{\\text{joint}}) = \\sum_{i} \\left( \\frac{[A]_{\\text{meas}}(t_i) - [A]_0 e^{-k_{\\text{joint}} t_i}}{\\sigma_A} \\right)^2 + \\sum_{i} \\left( \\frac{[B]_{\\text{meas}}(t_i) - \\left( [B]_0 + [A]_0(1 - e^{-k_{\\text{joint}} t_i}) \\right)}{\\sigma_B} \\right)^2 $$\n    The parameters are estimated subject to $[A]_0 \\ge 0$, $[B]_0 \\ge 0$, and $k_{\\text{joint}} \\ge 0$. The resulting estimate is $k_{\\text{joint}}$. This joint fit is expected to yield the most statistically robust estimate as it incorporates all data and enforces all known physical constraints.\n\nFor each test case, we will execute these three fitting procedures, extract the estimated rate constants $k_A$, $k_B$, and $k_{\\text{joint}}$, and report them after rounding to the specified precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases for parameter estimation.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general, well-conditioned)\n        {\n            \"k_true\": 0.35, \"A0_true\": 1.1, \"B0_true\": 0.3,\n            \"t_grid\": np.linspace(0, 8.0, 41),\n            \"sigma_A\": 0.01, \"sigma_B\": 0.01,\n            \"seed\": 13579\n        },\n        # Case 2 (slow kinetics, long observation window)\n        {\n            \"k_true\": 0.02, \"A0_true\": 2.0, \"B0_true\": 0.4,\n            \"t_grid\": np.linspace(0, 200.0, 51),\n            \"sigma_A\": 0.01, \"sigma_B\": 0.015,\n            \"seed\": 24680\n        },\n        # Case 3 (sparse sampling, fast kinetics)\n        {\n            \"k_true\": 1.5, \"A0_true\": 0.5, \"B0_true\": 0.0,\n            \"t_grid\": np.array([0.0, 0.2, 0.5, 1.0, 1.5]),\n            \"sigma_A\": 0.02, \"sigma_B\": 0.02,\n            \"seed\": 97531\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Step 1: Generate synthetic data\n        k_true, A0_true, B0_true = case[\"k_true\"], case[\"A0_true\"], case[\"B0_true\"]\n        t = case[\"t_grid\"]\n        sigma_A, sigma_B = case[\"sigma_A\"], case[\"sigma_B\"]\n        seed = case[\"seed\"]\n\n        A_true = A0_true * np.exp(-k_true * t)\n        B_true = B0_true + A0_true * (1 - np.exp(-k_true * t))\n        \n        rng = np.random.default_rng(seed)\n        noise_A = rng.normal(0, sigma_A, size=t.shape)\n        noise_B = rng.normal(0, sigma_B, size=t.shape)\n        \n        A_meas = A_true + noise_A\n        B_meas = B_true + noise_B\n\n        # --- Fitting Procedure A ---\n        def model_A(params, t):\n            A0, k = params\n            return A0 * np.exp(-k * t)\n\n        def objective_A(params, t, A_meas, sigma_A):\n            residuals = (A_meas - model_A(params, t)) / sigma_A\n            return np.sum(residuals**2)\n\n        # Initial guesses and bounds\n        A0_guess_A = A_meas[0] if A_meas[0] > 1e-6 else 1e-6\n        k_guess_A = 0.1 \n        initial_guess_A = [A0_guess_A, k_guess_A]\n        bounds_A = [(0, None), (0, None)]\n\n        res_A = minimize(objective_A, initial_guess_A, args=(t, A_meas, sigma_A),\n                         method='L-BFGS-B', bounds=bounds_A)\n        k_A = res_A.x[1]\n\n        # --- Fitting Procedure B ---\n        def model_B(params, t):\n            B0, A0, k = params\n            return B0 + A0 * (1 - np.exp(-k * t))\n\n        def objective_B(params, t, B_meas, sigma_B):\n            residuals = (B_meas - model_B(params, t)) / sigma_B\n            return np.sum(residuals**2)\n\n        # Initial guesses and bounds\n        B0_guess_B = B_meas[0] if B_meas[0] > 1e-6 else 1e-6\n        A0_guess_B = (B_meas[-1] - B_meas[0]) if (B_meas[-1] - B_meas[0]) > 1e-6 else 1e-6\n        k_guess_B = 0.1\n        initial_guess_B = [B0_guess_B, A0_guess_B, k_guess_B]\n        bounds_B = [(0, None), (0, None), (0, None)]\n        \n        res_B = minimize(objective_B, initial_guess_B, args=(t, B_meas, sigma_B),\n                         method='L-BFGS-B', bounds=bounds_B)\n        k_B = res_B.x[2]\n\n        # --- Fitting Procedure Joint ---\n        def model_joint_A(params, t):\n            A0, _, k = params # A0, B0, k\n            return A0 * np.exp(-k * t)\n\n        def model_joint_B(params, t):\n            A0, B0, k = params\n            return B0 + A0 * (1 - np.exp(-k * t))\n\n        def objective_joint(params, t, A_meas, B_meas, sigma_A, sigma_B):\n            res_A = (A_meas - model_joint_A(params, t)) / sigma_A\n            res_B = (B_meas - model_joint_B(params, t)) / sigma_B\n            return np.sum(res_A**2) + np.sum(res_B**2)\n\n        # Initial guesses and bounds\n        A0_guess_joint = A_meas[0] if A_meas[0] > 1e-6 else 1e-6\n        B0_guess_joint = B_meas[0] if B_meas[0] > 1e-6 else 1e-6\n        k_guess_joint = 0.1\n        initial_guess_joint = [A0_guess_joint, B0_guess_joint, k_guess_joint]\n        bounds_joint = [(0, None), (0, None), (0, None)]\n\n        res_joint = minimize(objective_joint, initial_guess_joint, \n                             args=(t, A_meas, B_meas, sigma_A, sigma_B),\n                             method='L-BFGS-B', bounds=bounds_joint)\n        k_joint = res_joint.x[2]\n\n        # Format and store results\n        k_A_rounded = round(k_A, 6)\n        k_B_rounded = round(k_B, 6)\n        k_joint_rounded = round(k_joint, 6)\n        \n        all_results.append([k_A_rounded, k_B_rounded, k_joint_rounded])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string without spaces, then join them\n    inner_lists_str = [f\"[{','.join(f'{x:.6f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "2660601"}]}