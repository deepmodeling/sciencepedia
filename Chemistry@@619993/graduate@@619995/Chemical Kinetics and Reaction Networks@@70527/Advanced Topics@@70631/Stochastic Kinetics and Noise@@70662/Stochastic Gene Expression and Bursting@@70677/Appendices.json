{"hands_on_practices": [{"introduction": "The two-state \"telegraph\" model is a foundational framework for understanding transcriptional bursting. While its full dynamics are captured by the Chemical Master Equation, approximations like the Linear Noise Approximation (LNA) are often used for analysis. This exercise guides you through a side-by-side comparison of the stationary mRNA variance predicted by the exact moment equations and the LNA, revealing a profound and exact relationship between the two for this class of linear models [@problem_id:2677754].", "problem": "Consider a stochastic gene expression model with promoter switching (the two-state telegraph model). The promoter state $S \\in \\{0,1\\}$ switches between an inactive state $S=0$ and an active state $S=1$ with activation rate $k_{\\mathrm{on}}$ and deactivation rate $k_{\\mathrm{off}}$. When $S=1$, messenger ribonucleic acid (mRNA) molecules $M \\in \\{0,1,2,\\dots\\}$ are synthesized at rate $r$, and each mRNA molecule degrades independently at rate $\\gamma$. Assume the system is at stationarity. Let $p = \\mathbb{E}[S]$ denote the stationary probability that the promoter is active and let $\\mu = \\mathbb{E}[M]$ denote the stationary mean mRNA copy number. Starting from the Chemical Master Equation (CME) and the definition of the Linear Noise Approximation (LNA), perform the following:\n\n- Derive the exact stationary variance $\\mathrm{Var}_{\\mathrm{exact}}(M)$ from closed moment equations of the CME for the joint process $(S,M)$.\n- Derive the LNA-predicted stationary variance $\\mathrm{Var}_{\\mathrm{LNA}}(M)$ by linearizing the deterministic rate equations around the fixed point and solving the associated steady-state covariance (Lyapunov) equation for the linearized fluctuations.\n\nDefine the fractional variance error of the LNA relative to the exact telegraph model as\n$$\n\\varepsilon_{\\mathrm{var}} \\equiv \\frac{\\mathrm{Var}_{\\mathrm{LNA}}(M) - \\mathrm{Var}_{\\mathrm{exact}}(M)}{\\mathrm{Var}_{\\mathrm{exact}}(M)} \\, .\n$$\nExpress $\\varepsilon_{\\mathrm{var}}$ solely as a function of the stationary mean mRNA level $\\mu$ and the switching rates $k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, and the mRNA degradation rate $\\gamma$ (you may use $p = k_{\\mathrm{on}}/(k_{\\mathrm{on}}+k_{\\mathrm{off}})$ to eliminate $r$ in favor of $\\mu$). Your final answer must be a single closed-form expression. No numerical approximation or rounding is required, and no units are to be reported.", "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and well-posedness.\n\n**Step 1: Extract Givens**\n- **Model:** The two-state telegraph model of stochastic gene expression.\n- **State variables:** Promoter state $S \\in \\{0,1\\}$ (inactive/active) and mRNA copy number $M \\in \\{0,1,2,\\dots\\}$.\n- **Reactions and Rates:**\n  - Activation: $S=0 \\xrightarrow{k_{\\mathrm{on}}} S=1$.\n  - Deactivation: $S=1 \\xrightarrow{k_{\\mathrm{off}}} S=0$.\n  - Synthesis: $\\emptyset \\xrightarrow{r} M$, conditional on $S=1$.\n  - Degradation: $M \\xrightarrow{\\gamma} \\emptyset$.\n- **Conditions:** The system is at stationarity.\n- **Definitions:**\n  - Stationary probability of active promoter: $p = \\mathbb{E}[S]$. It is given that $p = k_{\\mathrm{on}}/(k_{\\mathrm{on}}+k_{\\mathrm{off}})$.\n  - Stationary mean mRNA copy number: $\\mu = \\mathbb{E}[M]$.\n- **Task:**\n  1. Derive the exact stationary variance $\\mathrm{Var}_{\\mathrm{exact}}(M)$ from the Chemical Master Equation (CME).\n  2. Derive the Linear Noise Approximation (LNA) variance $\\mathrm{Var}_{\\mathrm{LNA}}(M)$.\n  3. Express the fractional variance error $\\varepsilon_{\\mathrm{var}} \\equiv (\\mathrm{Var}_{\\mathrm{LNA}}(M) - \\mathrm{Var}_{\\mathrm{exact}}(M)) / \\mathrm{Var}_{\\mathrm{exact}}(M)$ as a function of $\\mu$, $k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, and $\\gamma$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is based on the telegraph model, a canonical and scientifically rigorous model in stochastic chemical kinetics. The analysis requested involves standard, well-established theoretical methods (CME moment dynamics and the LNA). The model is scientifically sound.\n- **Well-Posedness:** The reaction network specifies a continuous-time Markov process that possesses a unique stationary distribution. The tasks of deriving exact and approximate moments are well-defined mathematical problems.\n- **Objectivity:** The problem is stated in precise, objective mathematical and scientific language.\n- **Completeness and Consistency:** All necessary parameters ($k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, $r$, $\\gamma$) and relationships are provided. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe solution requires two separate derivations for the variance of the mRNA copy number, $M$.\n\n**1. Derivation of the Exact Stationary Variance $\\mathrm{Var}_{\\mathrm{exact}}(M)$**\n\nWe derive the equations for the moments of the joint process $(S, M)$ from the CME. Let $\\langle \\cdot \\rangle$ denote the expectation at stationarity. Since $S \\in \\{0,1\\}$, we have $S^n = S$ for any integer $n \\geq 1$.\nThe time evolution of the expectation of an arbitrary function $f(S,M)$ is given by $\\frac{d\\langle f \\rangle}{dt} = \\langle \\mathcal{L}f \\rangle$, where $\\mathcal{L}$ is the generator of the process.\n$$\n\\mathcal{L}f(S,M) = k_{\\mathrm{on}}(1-S)[f(1,M)-f(S,M)] + k_{\\mathrm{off}}S[f(0,M)-f(S,M)] + rS[f(S,M+1)-f(S,M)] + \\gamma M[f(S,M-1)-f(S,M)]\n$$\nAt stationarity, $\\frac{d\\langle f \\rangle}{dt} = 0$.\n\n- **Mean Promoter State $\\langle S \\rangle$**: Let $f=S$.\n$0 = \\langle k_{\\mathrm{on}}(1-S)(1-S) + k_{\\mathrm{off}}S(0-S) \\rangle = \\langle k_{\\mathrm{on}}(1-S) - k_{\\mathrm{off}}S \\rangle = k_{\\mathrm{on}}(1-\\langle S \\rangle) - k_{\\mathrm{off}}\\langle S \\rangle$.\nSolving for $\\langle S \\rangle$ gives $p = \\langle S \\rangle = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}}$. This is consistent with the problem statement.\n\n- **Mean mRNA Level $\\langle M \\rangle$**: Let $f=M$.\n$0 = \\langle rS((M+1)-M) + \\gamma M((M-1)-M) \\rangle = \\langle rS - \\gamma M \\rangle = r\\langle S \\rangle - \\gamma\\langle M \\rangle$.\nSolving for $\\langle M \\rangle$ gives $\\mu = \\langle M \\rangle = \\frac{r\\langle S \\rangle}{\\gamma} = \\frac{rp}{\\gamma}$. This implies $r = \\frac{\\mu\\gamma}{p}$.\n\n- **Covariance $\\langle SM \\rangle$**: Let $f=SM$.\n$0 = \\langle k_{\\mathrm{on}}(1-S)(1 \\cdot M - SM) + k_{\\mathrm{off}}S(0 \\cdot M - SM) + rS(S(M+1)-SM) + \\gamma M(S(M-1)-SM) \\rangle$.\nSince $S^2=S$ and $S(1-S)=0$:\n$0 = \\langle k_{\\mathrm{on}}(M-SM) - k_{\\mathrm{off}}SM + rS(M+1-M) - \\gamma SM \\rangle$\n$0 = k_{\\mathrm{on}}\\langle M(1-S) \\rangle - k_{\\mathrm{off}}\\langle SM \\rangle + r\\langle S \\rangle - \\gamma\\langle SM \\rangle$\n$0 = k_{\\mathrm{on}}(\\langle M \\rangle - \\langle SM \\rangle) - (k_{\\mathrm{off}}+\\gamma)\\langle SM \\rangle + r\\langle S \\rangle$\n$0 = k_{\\mathrm{on}}\\mu - (k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)\\langle SM \\rangle + rp$.\nUsing $\\mu = rp/\\gamma$, this becomes $k_{\\mathrm{on}}\\frac{rp}{\\gamma} + rp = (k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)\\langle SM \\rangle$.\n$\\langle SM \\rangle = \\frac{rp(k_{\\mathrm{on}}/\\gamma + 1)}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} = \\frac{rp(k_{\\mathrm{on}}+\\gamma)}{\\gamma(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)} = \\mu \\frac{k_{\\mathrm{on}}+\\gamma}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma}$.\n\n- **Second Moment $\\langle M^2 \\rangle$**: Let $f=M^2$.\n$0 = \\langle rS((M+1)^2-M^2) + \\gamma M((M-1)^2-M^2) \\rangle = \\langle rS(2M+1) + \\gamma M(-2M+1) \\rangle$.\n$0 = 2r\\langle SM \\rangle + r\\langle S \\rangle - 2\\gamma\\langle M^2 \\rangle + \\gamma\\langle M \\rangle$.\n$2\\gamma\\langle M^2 \\rangle = 2r\\langle SM \\rangle + r\\langle S \\rangle + \\gamma\\langle M \\rangle$.\n$\\langle M^2 \\rangle = \\frac{r}{\\gamma}\\langle SM \\rangle + \\frac{r}{2\\gamma}\\langle S \\rangle + \\frac{1}{2}\\langle M \\rangle = \\frac{r}{\\gamma}\\langle SM \\rangle + \\frac{\\mu}{2} + \\frac{\\mu}{2} = \\frac{r}{\\gamma}\\langle SM \\rangle + \\mu$.\nSubstitute the expression for $\\langle SM \\rangle$:\n$\\langle M^2 \\rangle = \\frac{r}{\\gamma} \\left( \\mu \\frac{k_{\\mathrm{on}}+\\gamma}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} \\right) + \\mu$.\nSubstitute $r = \\mu\\gamma/p$:\n$\\langle M^2 \\rangle = \\frac{\\mu\\gamma/p}{\\gamma} \\mu \\frac{k_{\\mathrm{on}}+\\gamma}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} + \\mu = \\frac{\\mu^2}{p} \\frac{k_{\\mathrm{on}}+\\gamma}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} + \\mu$.\nThe variance is $\\mathrm{Var}_{\\mathrm{exact}}(M) = \\langle M^2 \\rangle - \\mu^2$:\n$\\mathrm{Var}_{\\mathrm{exact}}(M) = \\mu + \\mu^2 \\left( \\frac{1}{p} \\frac{k_{\\mathrm{on}}+\\gamma}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} - 1 \\right)$.\nLet $k_s = k_{\\mathrm{on}}+k_{\\mathrm{off}}$. The term in parentheses is:\n$\\frac{k_s}{k_{\\mathrm{on}}} \\frac{k_{\\mathrm{on}}+\\gamma}{k_s+\\gamma} - 1 = \\frac{k_s(k_{\\mathrm{on}}+\\gamma) - k_{\\mathrm{on}}(k_s+\\gamma)}{k_{\\mathrm{on}}(k_s+\\gamma)} = \\frac{\\gamma(k_s-k_{\\mathrm{on}})}{k_{\\mathrm{on}}(k_s+\\gamma)} = \\frac{\\gamma k_{\\mathrm{off}}}{k_{\\mathrm{on}}(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)}$.\nSo, the exact variance is:\n$$\n\\mathrm{Var}_{\\mathrm{exact}}(M) = \\mu + \\mu^2 \\frac{\\gamma k_{\\mathrm{off}}}{k_{\\mathrm{on}}(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)}\n$$\n\n**2. Derivation of the LNA-Predicted Variance $\\mathrm{Var}_{\\mathrm{LNA}}(M)$**\n\nThe LNA linearizes the system around the deterministic fixed point. The deterministic rate equations for the concentrations (means) $s(t)$ and $m(t)$ are:\n$$\n\\frac{ds}{dt} = k_{\\mathrm{on}}(1-s) - k_{\\mathrm{off}} s\n$$\n$$\n\\frac{dm}{dt} = rs - \\gamma m\n$$\nThe fixed point $(\\bar{s}, \\bar{m})$ is obtained by setting the derivatives to zero:\n$\\bar{s} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}} = p$, and $\\bar{m} = \\frac{r\\bar{s}}{\\gamma} = \\frac{rp}{\\gamma} = \\mu$.\nThe Jacobian matrix $J$ of the system at this fixed point is:\n$$\nJ = \\begin{pmatrix} -k_{\\mathrm{on}}-k_{\\mathrm{off}} & 0 \\\\ r & -\\gamma \\end{pmatrix}\n$$\nThe diffusion matrix $D$ is given by $D = \\sum_i \\mathbf{v}_i \\mathbf{v}_i^T w_i(\\bar{s}, \\bar{m})$, where $\\mathbf{v}_i$ are the stoichiometry vectors and $w_i$ are the propensities evaluated at the fixed point.\nPropensities at the fixed point:\n$w_1(\\text{activation}) = k_{\\mathrm{on}}(1-p) = \\frac{k_{\\mathrm{on}}k_{\\mathrm{off}}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}}$\n$w_2(\\text{deactivation}) = k_{\\mathrm{off}}p = \\frac{k_{\\mathrm{on}}k_{\\mathrm{off}}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}}$\n$w_3(\\text{synthesis}) = rp = \\gamma\\mu$\n$w_4(\\text{degradation}) = \\gamma\\mu$\nStoichiometry vectors: $\\mathbf{v}_1=(1,0)^T$, $\\mathbf{v}_2=(-1,0)^T$, $\\mathbf{v}_3=(0,1)^T$, $\\mathbf{v}_4=(0,-1)^T$.\n$$\nD = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\end{pmatrix} w_1 + \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} -1 & 0 \\end{pmatrix} w_2 + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\end{pmatrix} w_3 + \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 0 & -1 \\end{pmatrix} w_4\n$$\n$$\nD = \\begin{pmatrix} w_1+w_2 & 0 \\\\ 0 & w_3+w_4 \\end{pmatrix} = \\begin{pmatrix} 2 \\frac{k_{\\mathrm{on}}k_{\\mathrm{off}}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}} & 0 \\\\ 0 & 2\\gamma\\mu \\end{pmatrix}\n$$\nThe stationary covariance matrix of fluctuations $C$ satisfies the Lyapunov equation $JC + CJ^T + D = 0$. Let $C = \\begin{pmatrix} C_{ss} & C_{sm} \\\\ C_{sm} & C_{mm} \\end{pmatrix}$, where $C_{mm} = \\mathrm{Var}_{\\mathrm{LNA}}(M)$.\nThe equation for the $(2,2)$ element is:\n$2(rC_{sm} - \\gamma C_{mm}) + D_{22} = 0 \\implies C_{mm} = \\frac{rC_{sm}}{\\gamma} + \\frac{D_{22}}{2\\gamma} = \\frac{rC_{sm}}{\\gamma} + \\mu$.\nThe equation for the $(1,2)$ element is:\n$-(k_{\\mathrm{on}}+k_{\\mathrm{off}})C_{sm} + rC_{ss} - \\gamma C_{sm} = 0 \\implies C_{sm} = \\frac{rC_{ss}}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma}$.\nThe equation for the $(1,1)$ element is:\n$-2(k_{\\mathrm{on}}+k_{\\mathrm{off}})C_{ss} + D_{11} = 0 \\implies C_{ss} = \\frac{D_{11}}{2(k_{\\mathrm{on}}+k_{\\mathrm{off}})} = \\frac{k_{\\mathrm{on}}k_{\\mathrm{off}}}{(k_{\\mathrm{on}}+k_{\\mathrm{off}})^2} = p(1-p)$.\nSubstituting back:\n$C_{sm} = \\frac{r p(1-p)}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma}$.\n$C_{mm} = \\frac{r}{\\gamma}\\left(\\frac{r p(1-p)}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma}\\right) + \\mu$.\nLet us re-express this using the variables requested in the problem.\n$r = \\mu\\gamma/p$. $p = k_{\\mathrm{on}}/(k_{\\mathrm{on}}+k_{\\mathrm{off}})$. $1-p = k_{\\mathrm{off}}/(k_{\\mathrm{on}}+k_{\\mathrm{off}})$.\nThe term $\\frac{r^2p(1-p)}{\\gamma(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)} = \\frac{(\\mu\\gamma/p)^2 p(1-p)}{\\gamma(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)} = \\frac{\\mu^2\\gamma(1-p)/p}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} = \\frac{\\mu^2\\gamma(k_{\\mathrm{off}}/k_{\\mathrm{on}})}{k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma} = \\mu^2 \\frac{\\gamma k_{\\mathrm{off}}}{k_{\\mathrm{on}}(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)}$.\nThus, the LNA-predicted variance is:\n$$\n\\mathrm{Var}_{\\mathrm{LNA}}(M) = \\mu + \\mu^2 \\frac{\\gamma k_{\\mathrm{off}}}{k_{\\mathrm{on}}(k_{\\mathrm{on}}+k_{\\mathrm{off}}+\\gamma)}\n$$\n\n**3. Calculation of the Fractional Variance Error $\\varepsilon_{\\mathrm{var}}$**\n\nUpon comparing the expressions for the exact variance and the LNA-predicted variance, we find that they are identical:\n$$\n\\mathrm{Var}_{\\mathrm{exact}}(M) = \\mathrm{Var}_{\\mathrm{LNA}}(M)\n$$\nThis result is not accidental. For any chemical reaction network where all reaction propensities are at most linear functions of the species populations (i.e., affine), the moment equations derived from the CME are closed. The LNA, which is constructed from the linearization of already linear deterministic rate equations, provides the exact first two moments (mean and covariance) for such systems. The telegraph model is an example of such a linear network.\nTherefore, the difference $\\mathrm{Var}_{\\mathrm{LNA}}(M) - \\mathrm{Var}_{\\mathrm{exact}}(M)$ is exactly zero.\nThe fractional variance error is then:\n$$\n\\varepsilon_{\\mathrm{var}} = \\frac{\\mathrm{Var}_{\\mathrm{LNA}}(M) - \\mathrm{Var}_{\\mathrm{exact}}(M)}{\\mathrm{Var}_{\\mathrm{exact}}(M)} = \\frac{0}{\\mathrm{Var}_{\\mathrm{exact}}(M)} = 0\n$$\nThis holds provided that $\\mathrm{Var}_{\\mathrm{exact}}(M) \\neq 0$, which is true for any non-trivial case where $\\mu > 0$. The resulting expression for $\\varepsilon_{\\mathrm{var}}$ is a constant function of $\\mu$, $k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, and $\\gamma$.", "answer": "$$\\boxed{0}$$", "id": "2677754"}, {"introduction": "Noise in gene expression is not random but structured, containing rich information about underlying molecular mechanisms. This practice explores how temporal correlations, specifically the autocovariance function of mRNA levels, can serve as a powerful experimental probe. By deriving this function from first principles, you will discover how its amplitude is sensitive to the second moment of the transcriptional burst size, allowing one to distinguish between different models of promoter kinetics from time-series data [@problem_id:2677667].", "problem": "A single gene produces messenger ribonucleic acid (mRNA) in stochastic transcriptional bursts. Bursts arrive as a homogeneous Poisson process with rate $\\lambda$. At each burst time, an integer number $K$ of mRNA molecules is created instantaneously; different bursts are independent and identically distributed and independent of all subsequent degradation events. Each mRNA degrades independently with first-order rate $\\delta$. Let $M(t)$ denote the stationary mRNA copy number at time $t$, and define the stationary autocovariance function $C(\\tau) \\equiv \\mathbb{E}\\!\\left[(M(t)-\\mathbb{E}[M])(M(t+\\tau)-\\mathbb{E}[M])\\right]$ for $\\tau \\ge 0$.\n\nStarting only from the linear birth–death kinetics for mRNA and the defining properties of the Poisson process and independent exponential lifetimes, do the following:\n\n1. Derive $C(\\tau)$ at short lags (that is, for any fixed $\\tau \\ge 0$) in terms of $\\lambda$, $\\delta$, $\\mathbb{E}[K]$, and $\\mathbb{E}[K^{2}]$. Your derivation must explicitly use first principles (e.g., conditioning on the time since a burst and on the burst size) and must not invoke any pre-packaged correlation formulas.\n\n2. Suppose the mean burst size is fixed to $\\mathbb{E}[K]=b$, with $b>0$. Consider two mechanistic hypotheses for the burst-size distribution:\n   - Geometric bursts on $\\{0,1,2,\\dots\\}$, corresponding to a single-step promoter deactivation (exponentially distributed ON time), which yields a geometric distribution with mean $b$.\n   - Non-geometric bursts arising from an $n$-step promoter deactivation with $n \\in \\mathbb{N}$, modeled as an Erlang-$n$ distributed ON time of fixed mean, during which transcription occurs as a Poisson process. In this case the burst size is negative binomial with mean $b$ and shape parameter $n$.\n   \n   Using only the definition of the Poisson process and the law of total variance, express $\\mathbb{E}[K^{2}]$ under each hypothesis as a function of $b$ and $n$.\n\n3. Define the short-lag diagnostic power as the relative separation of the predicted autocovariance amplitudes at lag $\\tau$ between the geometric model ($n=1$) and the $n$-step non-geometric model ($n>1$), holding $\\lambda$, $\\delta$, $b$, and $\\tau$ fixed:\n   $$P(b,n) \\equiv \\frac{C_{\\text{geom}}(\\tau)-C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)}.$$\n   Derive a closed-form expression for $P(b,n)$ that depends only on $b$ and $n$ (and not on $\\lambda$, $\\delta$, or $\\tau$). Your final answer must be a single analytical expression.", "solution": "The problem presented is a standard, well-posed problem in the theory of stochastic gene expression. It is scientifically grounded, internally consistent, and requires the derivation of fundamental quantities from first principles. The problem is valid and we shall proceed with its solution.\n\nThe solution is structured in three parts as requested by the problem statement.\n\nPart 1: Derivation of the autocovariance function $C(\\tau)$.\n\nLet $M(t)$ be the number of mRNA molecules at time $t$. The process is governed by two reaction types:\n1.  Bursty production: $\\emptyset \\xrightarrow{\\lambda} K$ mRNA, where bursts occur as a Poisson process with rate $\\lambda$ and $K$ is a random variable for the number of molecules produced per burst.\n2.  First-order degradation: mRNA $\\xrightarrow{\\delta}$ $\\emptyset$, where each molecule degrades independently with rate $\\delta$.\n\nThe autocovariance function at stationarity is defined as $C(\\tau) = \\mathbb{E}[M(t)M(t+\\tau)] - (\\mathbb{E}[M])^{2}$ for $\\tau \\ge 0$. Due to stationarity, we can set $t=0$ for simplicity, so $C(\\tau) = \\text{Cov}(M(0), M(\\tau))$.\n\nThe number of molecules at time $\\tau$, $M(\\tau)$, can be decomposed into two populations:\n1.  $M_{s}(\\tau)$: molecules that were present at time $t=0$ and have survived until time $\\tau$.\n2.  $M_{n}(\\tau)$: new molecules produced by bursts in the time interval $(0, \\tau]$.\n\nThus, $M(\\tau) = M_{s}(\\tau) + M_{n}(\\tau)$.\nSince each of the $M(0)$ molecules at time $t=0$ survives until time $\\tau$ independently with probability $\\exp(-\\delta \\tau)$, the number of surviving molecules $M_{s}(\\tau)$ conditioned on $M(0)$ follows a binomial distribution, $M_{s}(\\tau) | M(0) \\sim \\text{Binomial}(M(0), \\exp(-\\delta \\tau))$. The conditional expectation is $\\mathbb{E}[M_s(\\tau) | M(0)] = M(0) \\exp(-\\delta \\tau)$.\n\nThe new molecules $M_{n}(\\tau)$ are produced by bursts occurring in $(0, \\tau]$, which is a Poisson process independent of the state of the system at or before $t=0$. Therefore, $M_{n}(\\tau)$ is independent of $M(0)$.\n\nNow, we compute the covariance:\n$$C(\\tau) = \\text{Cov}(M(0), M(\\tau)) = \\text{Cov}(M(0), M_{s}(\\tau) + M_{n}(\\tau))$$\nBy linearity of covariance:\n$$C(\\tau) = \\text{Cov}(M(0), M_{s}(\\tau)) + \\text{Cov}(M(0), M_{n}(\\tau))$$\nSince $M(0)$ and $M_{n}(\\tau)$ are independent, $\\text{Cov}(M(0), M_{n}(\\tau)) = 0$. We are left with:\n$$C(\\tau) = \\text{Cov}(M(0), M_{s}(\\tau))$$\nUsing the law of total covariance, $\\text{Cov}(X,Y) = \\mathbb{E}[\\text{Cov}(X,Y|Z)] + \\text{Cov}(\\mathbb{E}[X|Z], \\mathbb{E}[Y|Z])$, with $X=M(0)$, $Y=M_s(\\tau)$, and $Z=M(0)$.\n$\\mathbb{E}[M_s(\\tau)|M(0)] = M(0) \\exp(-\\delta \\tau)$. This is not stochastic, so $\\text{Cov}(M(0),M_s(\\tau)|M(0)) = 0$.\nThe second term gives:\n$$C(\\tau) = \\text{Cov}(M(0), \\mathbb{E}[M_s(\\tau)|M(0)]) = \\text{Cov}(M(0), M(0)\\exp(-\\delta \\tau))$$\n$$C(\\tau) = \\exp(-\\delta \\tau) \\text{Cov}(M(0), M(0)) = \\exp(-\\delta \\tau) \\text{Var}(M)$$\nThe problem now reduces to finding the stationary variance of the mRNA copy number, $\\text{Var}(M)$. We use the moment equations derived from the underlying chemical master equation, as requested by the prompt to start from kinetics.\n\nFor the first moment, $\\mathbb{E}[M]$:\n$$\\frac{d\\mathbb{E}[M]}{dt} = (\\text{rate of production}) - (\\text{rate of degradation})$$\n$$\\frac{d\\mathbb{E}[M]}{dt} = \\lambda \\mathbb{E}[K] - \\delta \\mathbb{E}[M]$$\nAt steady state, $\\frac{d\\mathbb{E}[M]}{dt}=0$, which gives $\\mathbb{E}[M] = \\frac{\\lambda \\mathbb{E}[K]}{\\delta}$.\n\nFor the second moment, $\\mathbb{E}[M^2]$:\n$$\\frac{d\\mathbb{E}[M^2]}{dt} = \\sum_{\\text{reactions}} (\\text{propensity}) \\times \\mathbb{E}[\\Delta(M^2)]$$\nChange from a burst: $\\Delta(M^2) = (M+K)^2 - M^2 = 2MK + K^2$. The expected change is $\\lambda \\mathbb{E}[2MK + K^2] = 2\\lambda\\mathbb{E}[M]\\mathbb{E}[K] + \\lambda\\mathbb{E}[K^2]$, since the burst size $K$ is independent of the current molecule count $M$.\nChange from a degradation: $\\Delta(M^2) = (M-1)^2 - M^2 = -2M+1$. The total rate of degradation is $\\delta M$, so the expected change is $\\mathbb{E}[\\delta M(-2M+1)] = -2\\delta\\mathbb{E}[M^2] + \\delta\\mathbb{E}[M]$.\nCombining these:\n$$\\frac{d\\mathbb{E}[M^2]}{dt} = 2\\lambda\\mathbb{E}[K]\\mathbb{E}[M] + \\lambda\\mathbb{E}[K^2] - 2\\delta\\mathbb{E}[M^2] + \\delta\\mathbb{E}[M]$$\nAt steady state, $\\frac{d\\mathbb{E}[M^2]}{dt}=0$:\n$$2\\delta\\mathbb{E}[M^2] = 2\\lambda\\mathbb{E}[K]\\mathbb{E}[M] + \\delta\\mathbb{E}[M] + \\lambda\\mathbb{E}[K^2]$$\nSubstitute $\\mathbb{E}[M] = \\frac{\\lambda \\mathbb{E}[K]}{\\delta}$:\n$$2\\delta\\mathbb{E}[M^2] = 2\\lambda\\mathbb{E}[K] \\left(\\frac{\\lambda \\mathbb{E}[K]}{\\delta}\\right) + \\delta \\left(\\frac{\\lambda \\mathbb{E}[K]}{\\delta}\\right) + \\lambda\\mathbb{E}[K^2]$$\n$$2\\delta\\mathbb{E}[M^2] = \\frac{2(\\lambda\\mathbb{E}[K])^2}{\\delta} + \\lambda\\mathbb{E}[K] + \\lambda\\mathbb{E}[K^2]$$\n$$\\mathbb{E}[M^2] = \\frac{(\\lambda\\mathbb{E}[K])^2}{\\delta^2} + \\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2])$$\nNow we compute the variance:\n$$\\text{Var}(M) = \\mathbb{E}[M^2] - (\\mathbb{E}[M])^2$$\n$$\\text{Var}(M) = \\left(\\frac{(\\lambda\\mathbb{E}[K])^2}{\\delta^2} + \\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2])\\right) - \\left(\\frac{\\lambda \\mathbb{E}[K]}{\\delta}\\right)^2$$\n$$\\text{Var}(M) = \\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2])$$\nFinally, substituting this variance into the expression for $C(\\tau)$:\n$$C(\\tau) = \\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2]) \\exp(-\\delta \\tau)$$\n\nPart 2: Calculation of $\\mathbb{E}[K^2]$ for the two hypotheses.\n\nWe are given $\\mathbb{E}[K]=b$.\n\nHypothesis 1: Geometric bursts ($n=1$).\nThe burst size $K$ follows a geometric distribution on $\\{0,1,2,\\dots\\}$. Let the probability of success be $1-p$ and failure be $p$. $P(K=k) = (1-p)p^k$.\nThe mean is $\\mathbb{E}[K] = \\frac{p}{1-p}$.\nGiven $\\mathbb{E}[K]=b$, we have $b = \\frac{p}{1-p}$, which solves to $p = \\frac{b}{1+b}$.\nThe variance is $\\text{Var}(K) = \\frac{p}{(1-p)^2} = \\left(\\frac{p}{1-p}\\right)\\frac{1}{1-p} = b(1+b)$.\nWe find $\\mathbb{E}[K^2]$ using the relation $\\mathbb{E}[K^2] = \\text{Var}(K) + (\\mathbb{E}[K])^2$:\n$$\\mathbb{E}[K^2]_{\\text{geom}} = b(1+b) + b^2 = b+b^2+b^2 = b+2b^2$$\n\nHypothesis 2: Non-geometric bursts ($n>1$).\nThe burst size $K$ follows a negative binomial distribution with mean $b$ and shape parameter $n$. This distribution can be seen as the sum of $n$ independent and identically distributed geometric random variables. Let the mean of each such geometric component be $b/n$.\nThe variance of a negative binomial is given by $\\text{Var}(K) = \\mathbb{E}[K] \\left(1 + \\frac{\\text{Var}(\\text{geom comp.})}{\\mathbb{E}[\\text{geom comp.}]^2} \\frac{\\mathbb{E}[\\text{geom comp.}]}{n}\\right)$ which simplifies to $\\text{Var}(K) = b\\left(1+\\frac{b}{n}\\right)$.\nUsing the same relation for the second moment:\n$$\\mathbb{E}[K^2]_{n} = \\text{Var}(K) + (\\mathbb{E}[K])^2 = b\\left(1+\\frac{b}{n}\\right) + b^2 = b + \\frac{b^2}{n} + b^2 = b + b^2\\left(1+\\frac{1}{n}\\right)$$\nNote that for $n=1$, this expression gives $b+b^2(2) = b+2b^2$, which correctly reduces to the geometric case.\n\nPart 3: Derivation of the diagnostic power $P(b,n)$.\n\nThe diagnostic power is defined as $P(b,n) \\equiv \\frac{C_{\\text{geom}}(\\tau)-C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)} = 1 - \\frac{C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)}$.\nUsing the result from Part 1, the ratio $\\frac{C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)}$ becomes:\n$$\\frac{C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)} = \\frac{\\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2]_{n}) \\exp(-\\delta \\tau)}{\\frac{\\lambda}{2\\delta}(\\mathbb{E}[K] + \\mathbb{E}[K^2]_{\\text{geom}}) \\exp(-\\delta \\tau)} = \\frac{\\mathbb{E}[K] + \\mathbb{E}[K^2]_{n}}{\\mathbb{E}[K] + \\mathbb{E}[K^2]_{\\text{geom}}}$$\nAll parameters other than moments of $K$ cancel. Substituting the expressions from Part 2 and $\\mathbb{E}[K]=b$:\nNumerator: $b + \\mathbb{E}[K^2]_{n} = b + \\left(b + b^2\\left(1+\\frac{1}{n}\\right)\\right) = 2b + b^2 + \\frac{b^2}{n}$.\nDenominator: $b + \\mathbb{E}[K^2]_{\\text{geom}} = b + (b+2b^2) = 2b + 2b^2$.\nThe ratio is:\n$$\\frac{C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)} = \\frac{2b + b^2 + \\frac{b^2}{n}}{2b + 2b^2}$$\nSince $b>0$, we can factor out $b$ from the numerator and $2b$ from the denominator:\n$$\\frac{C_{n}(\\tau)}{C_{\\text{geom}}(\\tau)} = \\frac{b(2 + b + \\frac{b}{n})}{2b(1+b)} = \\frac{2 + b + \\frac{b}{n}}{2(1+b)}$$\nNow, we compute $P(b,n)$:\n$$P(b,n) = 1 - \\frac{2 + b + \\frac{b}{n}}{2(1+b)} = \\frac{2(1+b) - (2 + b + \\frac{b}{n})}{2(1+b)}$$\n$$P(b,n) = \\frac{2 + 2b - 2 - b - \\frac{b}{n}}{2(1+b)} = \\frac{b - \\frac{b}{n}}{2(1+b)}$$\n$$P(b,n) = \\frac{b(1 - \\frac{1}{n})}{2(1+b)} = \\frac{b(\\frac{n-1}{n})}{2(1+b)}$$\nThis simplifies to the final closed-form expression:\n$$P(b,n) = \\frac{b(n-1)}{2n(1+b)}$$\nThis result depends only on the mean burst size $b$ and the burst complexity parameter $n$, as required.", "answer": "$$\\boxed{\\frac{b(n-1)}{2n(1+b)}}$$", "id": "2677667"}, {"introduction": "Bridging the gap between theoretical models and experimental observation is a central challenge in quantitative biology. This practice guides you through the process of inferring hidden promoter dynamics from a noisy, continuous fluorescence signal, a common type of single-cell data. You will frame the telegraph model as a Hidden Markov Model (HMM) and develop the computational machinery, including the scaled forward algorithm, needed to calculate the data likelihood and ultimately learn model parameters from time-series measurements [@problem_id:2677710].", "problem": "You are asked to formalize and compute the likelihood of continuous fluorescence time-series observations from a gene whose promoter follows a two-state telegraph model, under additive Gaussian measurement noise. The gene promoter switches between an inactive state and an active state that produces fluorescence with a different mean level. The promoter dynamics are modeled as a Continuous-Time Markov Chain (CTMC), while the fluorescence is measured at discrete times with Gaussian noise. You must derive the likelihood for a single fluorescence trace given the model parameters, using a Hidden Markov Model (HMM) formulation and a numerically stable forward algorithm.\n\nUse the following foundational starting points:\n- The telegraph promoter is a CTMC on two states, \"OFF\" and \"ON,\" with switching rates $k_{\\mathrm{on}}$ from \"OFF\" to \"ON\" and $k_{\\mathrm{off}}$ from \"ON\" to \"OFF\".\n- The promoter state is sampled at discrete times $t_1, t_2, \\dots, t_T$ with constant spacing $\\Delta t$.\n- The fluorescence observation at time $t$ is given by $F_t = \\mu_{S_t} + \\eta_t$, where $S_t \\in \\{\\mathrm{OFF}, \\mathrm{ON}\\}$ is the hidden promoter state at time $t$, $\\mu_{\\mathrm{OFF}}$ and $\\mu_{\\mathrm{ON}}$ are state-dependent means, and $\\eta_t$ are independent Gaussian noise variables with $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nMathematical tasks to derive:\n1. From the CTMC generator for the telegraph process, derive the exact discrete-time transition matrix $P(\\Delta t)$ with entries $p_{ij}(\\Delta t) = \\mathbb{P}(S_{t+\\Delta t}=j \\mid S_t=i)$ for $i,j \\in \\{\\mathrm{OFF},\\mathrm{ON}\\}$. Express $P(\\Delta t)$ in terms of $k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, and $\\Delta t$.\n2. Write the Gaussian emission density $f(F_t \\mid S_t=s)$ for each hidden state $s \\in \\{\\mathrm{OFF},\\mathrm{ON}\\}$, and define the emission likelihood for a sequence $\\{F_t\\}_{t=1}^T$ conditioned on a hidden state path $\\{S_t\\}_{t=1}^T$.\n3. Derive the full data likelihood $L = \\mathbb{P}(F_{1:T})$ by marginalizing over all hidden-state trajectories. Provide a computational scheme based on the forward algorithm that yields $\\log L$ stably via normalization at each time step. Assume an initial distribution at $t_1$ equal to the stationary distribution of the CTMC, and justify it from the generator.\n4. Propose a Hidden Markov Model (HMM) inference scheme that includes: computation of the likelihood via the scaled forward recursion; computation of posterior state marginals via the backward recursion; and an outline of maximum-likelihood parameter updates via the Expectation-Maximization (EM) approach (Baum–Welch), identifying the sufficient statistics needed for parameter estimation. You do not need to implement EM in code, but you must precisely state the mathematical objects that would be computed.\n\nComputational task:\n- Implement a program that takes no input and computes the total log-likelihoods, using the scaled forward algorithm, for a fixed test suite defined below. For each test case, use the stationary initial distribution and the exact discrete-time transition matrix derived from the rates and the sampling interval. Use the Gaussian emission model with the given means and variance.\n- Your program must return a single line that is a list of floating-point numbers, each the total log-likelihood for one test case, rounded to six digits after the decimal point.\n\nModel definitions, to be used in your derivations and implementation:\n- The CTMC generator for the telegraph process is\n$$\nQ \\;=\\;\n\\begin{pmatrix}\n- k_{\\mathrm{on}} & k_{\\mathrm{on}} \\\\\nk_{\\mathrm{off}} & -k_{\\mathrm{off}}\n\\end{pmatrix}.\n$$\n- The stationary distribution is\n$$\n\\pi_{\\mathrm{ON}} \\;=\\; \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}, \\qquad\n\\pi_{\\mathrm{OFF}} \\;=\\; \\frac{k_{\\mathrm{off}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}\n$$\nfor $k_{\\mathrm{on}} + k_{\\mathrm{off}} > 0$.\n- The Gaussian emission density in state $s$ is\n$$\nf(F_t \\mid S_t = s) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(F_t - \\mu_s)^2}{2\\sigma^2}\\right).\n$$\n\nTest suite:\nFor each case, compute the log-likelihood using the scaled forward algorithm under the specified parameters.\n\n- Case A:\n  - $\\Delta t = 1.0$\n  - $k_{\\mathrm{on}} = 0.3$, $k_{\\mathrm{off}} = 0.5$\n  - $\\mu_{\\mathrm{OFF}} = 0.2$, $\\mu_{\\mathrm{ON}} = 2.0$\n  - $\\sigma = 0.6$\n  - Fluorescence trace $F_{1:10} = [0.15, 0.30, 1.80, 2.10, 1.90, 0.40, 0.20, 2.30, 2.00, 0.50]$\n\n- Case B (single-observation boundary):\n  - $\\Delta t = 0.2$\n  - $k_{\\mathrm{on}} = 1.0$, $k_{\\mathrm{off}} = 1.0$\n  - $\\mu_{\\mathrm{OFF}} = 0.0$, $\\mu_{\\mathrm{ON}} = 1.0$\n  - $\\sigma = 0.5$\n  - Fluorescence trace $F_{1:1} = [0.10]$\n\n- Case C (fast switching relative to sampling):\n  - $\\Delta t = 0.5$\n  - $k_{\\mathrm{on}} = 3.0$, $k_{\\mathrm{off}} = 4.0$\n  - $\\mu_{\\mathrm{OFF}} = 0.0$, $\\mu_{\\mathrm{ON}} = 3.0$\n  - $\\sigma = 0.8$\n  - Fluorescence trace $F_{1:15} = [0.2, 1.5, 2.8, 0.3, 2.7, 3.2, 0.1, 1.0, 2.5, 3.1, 0.4, 0.7, 2.9, 3.3, 0.2]$\n\n- Case D (low-noise, near-deterministic emissions):\n  - $\\Delta t = 1.0$\n  - $k_{\\mathrm{on}} = 0.8$, $k_{\\mathrm{off}} = 0.2$\n  - $\\mu_{\\mathrm{OFF}} = 0.0$, $\\mu_{\\mathrm{ON}} = 5.0$\n  - $\\sigma = 0.1$\n  - Fluorescence trace $F_{1:8} = [5.10, 4.90, 5.00, 0.10, -0.05, 5.20, 5.00, 4.80]$\n\n- Case E (one absorbing tendency: no activation from OFF):\n  - $\\Delta t = 1.0$\n  - $k_{\\mathrm{on}} = 0.0$, $k_{\\mathrm{off}} = 1.0$\n  - $\\mu_{\\mathrm{OFF}} = 0.0$, $\\mu_{\\mathrm{ON}} = 2.0$\n  - $\\sigma = 0.3$\n  - Fluorescence trace $F_{1:4} = [0.05, 0.10, -0.20, 0.30]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r_A,r_B,r_C,r_D,r_E]\"), where each $r_\\cdot$ is the total log-likelihood for that case, rounded to six digits after the decimal point.\n- No other text should be printed.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extracted Givens**\n-   Promoter model: A Continuous-Time Markov Chain (CTMC) on two states, $S \\in \\{\\mathrm{OFF}, \\mathrm{ON}\\}$, with states indexed as $0$ for \"OFF\" and $1$ for \"ON\".\n-   Switching rates: $k_{\\mathrm{on}}$ (from OFF to ON) and $k_{\\mathrm{off}}$ (from ON to OFF).\n-   CTMC generator matrix:\n    $$\n    Q \\;=\\;\n    \\begin{pmatrix}\n    - k_{\\mathrm{on}} & k_{\\mathrm{on}} \\\\\n    k_{\\mathrm{off}} & -k_{\\mathrm{off}}\n    \\end{pmatrix}\n    $$\n-   Stationary distribution: $\\pi = [\\pi_{\\mathrm{OFF}}, \\pi_{\\mathrm{ON}}]$ where $\\pi_{\\mathrm{OFF}} = \\frac{k_{\\mathrm{off}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}$ and $\\pi_{\\mathrm{ON}} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}$, for $k_{\\mathrm{on}} + k_{\\mathrm{off}} > 0$.\n-   Observation model: Fluorescence $F_t$ is measured at discrete times $t_1, t_2, \\dots, t_T$ with constant spacing $\\Delta t$.\n-   Emission model: $F_t = \\mu_{S_t} + \\eta_t$, where $S_t$ is the hidden state, $\\mu_s$ is the mean for state $s$, and $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise.\n-   Emission probability density: $f(F_t \\mid S_t = s) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\left(-\\frac{(F_t - \\mu_s)^2}{2\\sigma^2}\\right)$.\n-   Initial distribution: Assumed to be the stationary distribution $\\pi$.\n-   Computational Task: Compute the total log-likelihood $\\log \\mathbb{P}(F_{1:T})$ for five specific test cases using a scaled forward algorithm.\n\n**Step 2: Validation Using Extracted Givens**\n-   **Scientifically Grounded**: The problem describes the two-state telegraph model of gene expression, a cornerstone model in quantitative biology. Its formulation as a Hidden Markov Model (HMM) is a standard and rigorous approach. The model is based on established principles of chemical kinetics and probability theory.\n-   **Well-Posed**: The problem is fully specified. All necessary parameters and data are provided for each test case. The objective—to compute the likelihood of the data under the given model—is a well-defined mathematical task with a unique solution.\n-   **Objective**: The problem is stated in precise mathematical and scientific language, free of ambiguity or subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, objective, and contains no contradictions or missing information. We proceed to the solution.\n\n---\n\nThe solution is derived by addressing the four mathematical tasks specified. The states are indexed as $0$ for OFF and $1$ for ON.\n\n**1. Derivation of the Discrete-Time Transition Matrix**\n\nThe transition probability matrix $P(\\Delta t)$ for a CTMC with generator $Q$ over a time interval $\\Delta t$ is given by the matrix exponential $P(\\Delta t) = \\exp(Q \\Delta t)$. To compute this, we diagonalize the generator matrix $Q$.\n\nThe eigenvalues $\\lambda$ of $Q$ are found by solving the characteristic equation $\\det(Q - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -k_{\\mathrm{on}} - \\lambda & k_{\\mathrm{on}} \\\\ k_{\\mathrm{off}} & -k_{\\mathrm{off}} - \\lambda \\end{pmatrix} = (-k_{\\mathrm{on}} - \\lambda)(-k_{\\mathrm{off}} - \\lambda) - k_{\\mathrm{on}}k_{\\mathrm{off}} = 0\n$$\n$$\n\\lambda^2 + (k_{\\mathrm{on}} + k_{\\mathrm{off}})\\lambda + k_{\\mathrm{on}}k_{\\mathrm{off}} - k_{\\mathrm{on}}k_{\\mathrm{off}} = 0 \\implies \\lambda(\\lambda + (k_{\\mathrm{on}} + k_{\\mathrm{off}})) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = -(k_{\\mathrm{on}} + k_{\\mathrm{off}})$. Let $k_s = k_{\\mathrm{on}} + k_{\\mathrm{off}}$. The eigenvalues are $0$ and $-k_s$.\n\nThe corresponding right eigenvectors are:\nFor $\\lambda_1 = 0$: $Q v_1 = 0 \\implies \\begin{pmatrix} -k_{\\mathrm{on}} & k_{\\mathrm{on}} \\\\ k_{\\mathrm{off}} & -k_{\\mathrm{off}} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = 0 \\implies x=y$. We choose $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nFor $\\lambda_2 = -k_s$: $(Q + k_s I) v_2 = 0 \\implies \\begin{pmatrix} k_{\\mathrm{off}} & k_{\\mathrm{on}} \\\\ k_{\\mathrm{off}} & k_{\\mathrm{on}} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = 0 \\implies k_{\\mathrm{off}}x + k_{\\mathrm{on}}y = 0$. We choose $v_2 = \\begin{pmatrix} k_{\\mathrm{on}} \\\\ -k_{\\mathrm{off}} \\end{pmatrix}$.\n\nWe form the matrix of eigenvectors $S = \\begin{pmatrix} 1 & k_{\\mathrm{on}} \\\\ 1 & -k_{\\mathrm{off}} \\end{pmatrix}$ and diagonal matrix $D = \\begin{pmatrix} 0 & 0 \\\\ 0 & -k_s \\end{pmatrix}$. Then $Q = S D S^{-1}$.\nThe inverse of $S$ is $S^{-1} = \\frac{1}{-k_{\\mathrm{off}}-k_{\\mathrm{on}}} \\begin{pmatrix} -k_{\\mathrm{off}} & -k_{\\mathrm{on}} \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{k_s} \\begin{pmatrix} k_{\\mathrm{off}} & k_{\\mathrm{on}} \\\\ 1 & -1 \\end{pmatrix}$.\n\nThe transition matrix is then $P(\\Delta t) = \\exp(Q \\Delta t) = S \\exp(D \\Delta t) S^{-1}$:\n$$\nP(\\Delta t) = \\begin{pmatrix} 1 & k_{\\mathrm{on}} \\\\ 1 & -k_{\\mathrm{off}} \\end{pmatrix} \\begin{pmatrix} e^{0} & 0 \\\\ 0 & e^{-k_s \\Delta t} \\end{pmatrix} \\frac{1}{k_s} \\begin{pmatrix} k_{\\mathrm{off}} & k_{\\mathrm{on}} \\\\ 1 & -1 \\end{pmatrix}\n$$\n$$\nP(\\Delta t) = \\frac{1}{k_s} \\begin{pmatrix} 1 & k_{\\mathrm{on}} e^{-k_s \\Delta t} \\\\ 1 & -k_{\\mathrm{off}} e^{-k_s \\Delta t} \\end{pmatrix} \\begin{pmatrix} k_{\\mathrm{off}} & k_{\\mathrm{on}} \\\\ 1 & -1 \\end{pmatrix}\n$$\n$$\nP(\\Delta t) = \\frac{1}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}\n\\begin{pmatrix}\nk_{\\mathrm{off}} + k_{\\mathrm{on}} e^{-(k_{\\mathrm{on}} + k_{\\mathrm{off}})\\Delta t} & k_{\\mathrm{on}} (1 - e^{-(k_{\\mathrm{on}} + k_{\\mathrm{off}})\\Delta t}) \\\\\nk_{\\mathrm{off}} (1 - e^{-(k_{\\mathrm{on}} + k_{\\mathrm{off}})\\Delta t}) & k_{\\mathrm{on}} + k_{\\mathrm{off}} e^{-(k_{\\mathrm{on}} + k_{\\mathrm{off}})\\Delta t}\n\\end{pmatrix}\n$$\nThe entries are $p_{ij}(\\Delta t) = \\mathbb{P}(S_{t+\\Delta t}=j \\mid S_t=i)$, with $i,j \\in \\{0,1\\}$. For example, $p_{01}(\\Delta t) = \\frac{k_{\\mathrm{on}}}{k_s}(1 - e^{-k_s \\Delta t})$. This formula is valid for $k_s > 0$. If $k_s=0$, then $Q=0$ and $P(\\Delta t)=I$.\n\n**2. Emission Model and Likelihood Formulation**\n\nThe emission probability density for an observation $F_t$ given the hidden state $S_t=s$ is a Gaussian:\n$$\nf(F_t \\mid S_t = s) = \\mathcal{N}(F_t; \\mu_s, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(F_t - \\mu_s)^2}{2\\sigma^2}\\right)\n$$\nwhere $s \\in \\{\\mathrm{OFF}, \\mathrm{ON}\\}$. For a specific path of hidden states $S_{1:T} = \\{S_1, S_2, \\dots, S_T\\}$, the likelihood of the observation sequence $F_{1:T} = \\{F_1, F_2, \\dots, F_T\\}$ is:\n$$\n\\mathbb{P}(F_{1:T} \\mid S_{1:T}) = \\prod_{t=1}^T f(F_t \\mid S_t)\n$$\nThis is due to the assumption that the measurement noises $\\eta_t$ are independent at each time point.\n\n**3. Full Data Likelihood via the Forward Algorithm**\n\nThe total likelihood of the observation sequence, $L = \\mathbb{P}(F_{1:T})$, is obtained by marginalizing over all possible hidden state paths:\n$$\nL = \\sum_{S_{1:T}} \\mathbb{P}(F_{1:T}, S_{1:T}) = \\sum_{S_{1:T}} \\mathbb{P}(S_1) \\left( \\prod_{t=2}^T \\mathbb{P}(S_t \\mid S_{t-1}) \\right) \\left( \\prod_{t=1}^T f(F_t \\mid S_t) \\right)\n$$\nDirect summation is computationally infeasible. The forward algorithm provides an efficient method. We define the forward variable $\\alpha_t(j) = \\mathbb{P}(F_{1:t}, S_t=j)$.\n\nTo prevent numerical underflow, a scaled version is used. We define scaled forward variables $\\hat{\\alpha}_t(j) = \\mathbb{P}(S_t=j \\mid F_{1:t})$ and scaling factors $c_t = \\mathbb{P}(F_t \\mid F_{1:t-1})$. The log-likelihood is then $\\log L = \\sum_{t=1}^T \\log c_t$.\n\nThe algorithm proceeds as follows:\n-   **Initialization ($t=1$)**: The system is assumed to be at steady state.\n    -   The initial state probabilities are given by the stationary distribution $\\pi = [\\pi_0, \\pi_1]$, where $\\pi_0 = \\pi_{\\mathrm{OFF}}$ and $\\pi_1 = \\pi_{\\mathrm{ON}}$.\n    -   Compute the unscaled forward variables: $\\alpha'_1(j) = \\pi_j \\cdot f(F_1 \\mid S_1=j)$ for $j \\in \\{0, 1\\}$.\n    -   Calculate the first scaling factor: $c_1 = \\sum_{j=0}^{1} \\alpha'_1(j)$.\n    -   Compute the scaled variables: $\\hat{\\alpha}_1(j) = \\alpha'_1(j) / c_1$.\n    -   Initialize total log-likelihood: $\\log L = \\log c_1$.\n\n-   **Recursion ($t=2, \\dots, T$)**:\n    -   For each state $j \\in \\{0, 1\\}$, compute the unscaled forward variable for time $t$:\n        $$\n        \\alpha'_t(j) = \\left[ \\sum_{i=0}^{1} \\hat{\\alpha}_{t-1}(i) \\cdot p_{ij}(\\Delta t) \\right] \\cdot f(F_t \\mid S_t=j)\n        $$\n    -   Calculate the scaling factor for time $t$: $c_t = \\sum_{j=0}^{1} \\alpha'_t(j)$.\n    -   Compute the scaled variables: $\\hat{\\alpha}_t(j) = \\alpha'_t(j) / c_t$.\n    -   Update the total log-likelihood: $\\log L \\leftarrow \\log L + \\log c_t$.\n\nThis procedure is numerically stable and yields the exact log-likelihood. For implementation, all calculations are best performed in log-space to handle extremely small probabilities.\n\n**4. HMM Inference Scheme: Forward-Backward and EM**\n\n-   **Backward Recursion**: To compute posterior state probabilities, we introduce the backward variable $\\beta_t(i) = \\mathbb{P}(F_{t+1:T} \\mid S_t=i)$.\n    -   **Initialization ($t=T$)**: $\\beta_T(i) = 1$ for $i \\in \\{0, 1\\}$.\n    -   **Recursion ($t=T-1, \\dots, 1$)**:\n        $$\n        \\beta_t(i) = \\sum_{j=0}^{1} p_{ij}(\\Delta t) \\cdot f(F_{t+1} \\mid S_{t+1}=j) \\cdot \\beta_{t+1}(j)\n        $$\n    These can also be scaled using the factors $c_t$ from the forward pass.\n\n-   **Posterior State Marginals (Smoothing)**:\n    -   The probability of being in state $i$ at time $t$ given all observations is:\n        $$\n        \\gamma_t(i) = \\mathbb{P}(S_t=i \\mid F_{1:T}) = \\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=0}^{1} \\alpha_t(j) \\beta_t(j)}\n        $$\n    -   The pairwise marginal probability of a transition from $i$ to $j$ at time $t$ is:\n        $$\n        \\xi_t(i,j) = \\mathbb{P}(S_{t-1}=i, S_t=j \\mid F_{1:T}) = \\frac{\\alpha_{t-1}(i) p_{ij}(\\Delta t) f(F_t \\mid S_t=j) \\beta_t(j)}{\\sum_{i',j'} \\alpha_{t-1}(i') p_{i'j'}(\\Delta t) f(F_t \\mid S_t=j') \\beta_t(j')}\n        $$\n\n-   **Expectation-Maximization (EM) / Baum-Welch Algorithm Outline**:\n    This iterative algorithm finds maximum-likelihood estimates for the model parameters $\\theta = \\{k_{\\mathrm{on}}, k_{\\mathrm{off}}, \\mu_0, \\mu_1, \\sigma\\}$.\n    -   **E-Step**: With current parameters $\\theta^{(k)}$, compute the posterior marginals $\\gamma_t(i)$ and $\\xi_t(i,j)$ for all $t, i, j$. These represent the expected \"counts\" of states and transitions.\n    -   **M-Step**: Update the parameters to $\\theta^{(k+1)}$ by maximizing the expected complete-data log-likelihood, which leads to the following re-estimation formulas:\n        -   **Emission Means**: The new mean for state $s$ is the weighted average of the observations, where weights are the posterior probabilities $\\gamma_t(s)$.\n            $$ \\mu_s^{(k+1)} = \\frac{\\sum_{t=1}^T \\gamma_t(s) F_t}{\\sum_{t=1}^T \\gamma_t(s)} $$\n        -   **Emission Variance**:\n            $$ (\\sigma^2)^{(k+1)} = \\frac{\\sum_{s=0}^{1} \\sum_{t=1}^T \\gamma_t(s) (F_t - \\mu_s^{(k+1)})^2}{\\sum_{t=1}^T \\sum_{s=0}^{1} \\gamma_t(s) } = \\frac{1}{T} \\sum_{s=0}^{1} \\sum_{t=1}^T \\gamma_t(s) (F_t - \\mu_s^{(k+1)})^2$$\n        -   **Transition Parameters**: First, re-estimate the discrete transition probabilities:\n            $$ p_{ij}^{(k+1)} = \\frac{\\sum_{t=2}^T \\xi_t(i,j)}{\\sum_{t=2}^T \\gamma_{t-1}(i)} $$\n            The continuous-time rates $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$ are then recovered from the re-estimated matrix $P^{(k+1)}$. The second eigenvalue of $P^{(k+1)}$ is $\\lambda_2^{(k+1)} = e^{-(k_{\\mathrm{on}}^{(k+1)} + k_{\\mathrm{off}}^{(k+1)})\\Delta t}$, which gives $k_s^{(k+1)} = k_{\\mathrm{on}}^{(k+1)} + k_{\\mathrm{off}}^{(k+1)}$. The stationary distribution $\\pi^{(k+1)}$, which is the left eigenvector of $P^{(k+1)}$ for eigenvalue $1$, gives the ratio of the rates. From $\\pi_1^{(k+1)} = k_{\\mathrm{on}}^{(k+1)}/k_s^{(k+1)}$, we can solve for $k_{\\mathrm{on}}^{(k+1)}$ and subsequently $k_{\\mathrm{off}}^{(k+1)}$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the log-likelihood of fluorescence time-series data for a two-state\n    telegraph model of gene expression using a scaled forward algorithm.\n    \"\"\"\n    \n    # Define the 5 test cases from the problem statement.\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"params\": {\n                \"delta_t\": 1.0,\n                \"k_on\": 0.3, \"k_off\": 0.5,\n                \"mu_off\": 0.2, \"mu_on\": 2.0, \"sigma\": 0.6\n            },\n            \"trace\": [0.15, 0.30, 1.80, 2.10, 1.90, 0.40, 0.20, 2.30, 2.00, 0.50]\n        },\n        {\n            \"id\": \"B\",\n            \"params\": {\n                \"delta_t\": 0.2,\n                \"k_on\": 1.0, \"k_off\": 1.0,\n                \"mu_off\": 0.0, \"mu_on\": 1.0, \"sigma\": 0.5\n            },\n            \"trace\": [0.10]\n        },\n        {\n            \"id\": \"C\",\n            \"params\": {\n                \"delta_t\": 0.5,\n                \"k_on\": 3.0, \"k_off\": 4.0,\n                \"mu_off\": 0.0, \"mu_on\": 3.0, \"sigma\": 0.8\n            },\n            \"trace\": [0.2, 1.5, 2.8, 0.3, 2.7, 3.2, 0.1, 1.0, 2.5, 3.1, 0.4, 0.7, 2.9, 3.3, 0.2]\n        },\n        {\n            \"id\": \"D\",\n            \"params\": {\n                \"delta_t\": 1.0,\n                \"k_on\": 0.8, \"k_off\": 0.2,\n                \"mu_off\": 0.0, \"mu_on\": 5.0, \"sigma\": 0.1\n            },\n            \"trace\": [5.10, 4.90, 5.00, 0.10, -0.05, 5.20, 5.00, 4.80]\n        },\n        {\n            \"id\": \"E\",\n            \"params\": {\n                \"delta_t\": 1.0,\n                \"k_on\": 0.0, \"k_off\": 1.0,\n                \"mu_off\": 0.0, \"mu_on\": 2.0, \"sigma\": 0.3\n            },\n            \"trace\": [0.05, 0.10, -0.20, 0.30]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p = case[\"params\"]\n        k_on, k_off = p[\"k_on\"], p[\"k_off\"]\n        delta_t = p[\"delta_t\"]\n        mu_off, mu_on, sigma = p[\"mu_off\"], p[\"mu_on\"], p[\"sigma\"]\n        fluorescence_trace = np.array(case[\"trace\"])\n        T = len(fluorescence_trace)\n        \n        # 1. Calculate stationary distribution (initial probabilities)\n        k_sum = k_on + k_off\n        if k_sum > 0:\n            pi_off = k_off / k_sum\n            pi_on = k_on / k_sum\n        else: # Case where both rates are 0\n            # Arbitrarily set to one state if no rates given.\n            # For this problem set, k_on=0 implies pi_on=0.\n            pi_off = 1.0 if k_on == 0 else 0.0\n            pi_on = 1.0 - pi_off\n        \n        log_pi = np.array([np.log(pi_off) if pi_off > 0 else -np.inf, \n                           np.log(pi_on) if pi_on > 0 else -np.inf])\n        \n        # 2. Calculate discrete-time transition matrix P(delta_t)\n        P = np.zeros((2, 2))\n        if k_sum > 0:\n            exp_term = np.exp(-k_sum * delta_t)\n            P[0, 0] = (k_off + k_on * exp_term) / k_sum\n            P[0, 1] = (k_on - k_on * exp_term) / k_sum\n            P[1, 0] = (k_off - k_off * exp_term) / k_sum\n            P[1, 1] = (k_on + k_off * exp_term) / k_sum\n        else: # If rates are 0, no transitions occur\n            P = np.identity(2)\n        \n        log_P = np.log(P, where=P>0)\n        log_P[P==0] = -np.inf\n\n        # 3. Implement scaled forward algorithm in log-space\n        \n        # Emission probabilities for all observations\n        log_em_probs = np.array([\n            norm.logpdf(fluorescence_trace, loc=mu_off, scale=sigma),\n            norm.logpdf(fluorescence_trace, loc=mu_on, scale=sigma)\n        ])\n\n        # Initialization (t=1)\n        log_alpha_unscaled_1 = log_pi + log_em_probs[:, 0]\n        log_c1 = logsumexp(log_alpha_unscaled_1)\n        log_alpha_hat = log_alpha_unscaled_1 - log_c1\n        total_log_likelihood = log_c1\n        \n        # Recursion (t=2 to T)\n        for t in range(1, T):\n            # Propagate forward: log(alpha_hat_{t-1} @ P)\n            # We use logsumexp for the matrix multiplication in log space\n            log_propagated_0 = logsumexp(log_alpha_hat + log_P[:, 0])\n            log_propagated_1 = logsumexp(log_alpha_hat + log_P[:, 1])\n            \n            # log(unscaled alpha_t)\n            log_alpha_unscaled_t = np.array([log_propagated_0, log_propagated_1]) + log_em_probs[:, t]\n            \n            # log(scaling factor c_t)\n            log_ct = logsumexp(log_alpha_unscaled_t)\n            \n            # update log(alpha_hat_t)\n            log_alpha_hat = log_alpha_unscaled_t - log_ct\n            \n            total_log_likelihood += log_ct\n            \n        results.append(round(total_log_likelihood, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2677710"}]}