{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with the canonical model of intrinsic noise: the linear birth-death process. This exercise [@problem_id:2649020] challenges you to derive the exact time-evolution of the mean and variance directly from the Chemical Master Equation. Mastering this derivation is crucial for understanding how macroscopic statistical properties emerge from microscopic stochastic events and for appreciating the connection between transient dynamics and steady-state behavior.", "problem": "Consider a single-species stochastic reaction network representing intrinsic noise in a well-mixed chemical system:\n$$\\varnothing \\xrightarrow{\\lambda} X, \\quad X \\xrightarrow{\\mu} \\varnothing,$$\nwhere $X$ is the molecular count of the species, $\\lambda>0$ is a constant production rate (zeroth-order), and $\\mu>0$ is a first-order degradation rate. Let $X(t)$ denote the random molecular count at time $t\\ge 0$, and assume a deterministic initial condition $X(0)=n_{0}$, where $n_{0}$ is a nonnegative integer. Using only first principles from the stochastic Chemical Master Equation (CME), derive closed-form expressions for the mean $m(t)=\\mathbb{E}[X(t)]$ and the variance $v(t)=\\operatorname{Var}(X(t))$ for all $t\\ge 0$. Then analyze the long-time limit to identify the stationary mean and variance, and verify explicitly that when $\\lambda=\\mu$ the stationary variance approaches the stationary mean. Express your final answer as the pair $(m(t),v(t))$ in terms of $\\lambda$, $\\mu$, $t$, and $n_{0}$. No numerical approximation or rounding is required, and no units are needed.", "solution": "The problem requires the derivation of the mean $m(t)$ and variance $v(t)$ for the number of molecules $X(t)$ governed by the reactions $\\varnothing \\xrightarrow{\\lambda} X$ and $X \\xrightarrow{\\mu} \\varnothing$. This is to be done starting from the Chemical Master Equation (CME).\n\nLet $P(n, t) = \\mathbb{P}(X(t)=n)$ be the probability of having $n$ molecules at time $t$. The propensities for the reactions are $a_1 = \\lambda$ for production and $a_2(n) = \\mu n$ for degradation. The CME is given by:\n$$ \\frac{dP(n,t)}{dt} = \\lambda P(n-1,t) + \\mu(n+1)P(n+1,t) - (\\lambda + \\mu n)P(n,t) $$\nfor $n \\ge 1$, and $\\frac{dP(0,t)}{dt} = \\mu P(1,t) - \\lambda P(0,t)$ for $n=0$.\n\nFrom the CME, one can derive ordinary differential equations (ODEs) for the moments of $X(t)$. For an arbitrary function $f(X)$, the time evolution of its expectation $\\mathbb{E}[f(X)] = \\langle f(X) \\rangle$ is given by:\n$$ \\frac{d\\langle f(X) \\rangle}{dt} = \\sum_{j} \\langle a_j(X) [f(X+\\nu_j) - f(X)] \\rangle $$\nwhere the sum is over all reaction channels $j$, $a_j(X)$ is the propensity of reaction $j$, and $\\nu_j$ is the stoichiometric change in $X$ due to reaction $j$.\nFor this system, we have:\n- Reaction 1 (production): $a_1(n) = \\lambda$, $\\nu_1 = +1$.\n- Reaction 2 (degradation): $a_2(n) = \\mu n$, $\\nu_2 = -1$.\n\n**Derivation of the Mean, $m(t) = \\langle X(t) \\rangle$**\nLet $f(X)=X$. The moment equation becomes:\n$$ \\frac{d\\langle X \\rangle}{dt} = \\langle \\lambda ((X+1) - X) \\rangle + \\langle \\mu X ((X-1) - X) \\rangle $$\n$$ \\frac{d\\langle X \\rangle}{dt} = \\langle \\lambda \\cdot 1 \\rangle + \\langle \\mu X \\cdot (-1) \\rangle $$\n$$ \\frac{dm(t)}{dt} = \\lambda - \\mu \\langle X \\rangle = \\lambda - \\mu m(t) $$\nThis is a first-order linear ODE: $\\frac{dm}{dt} + \\mu m = \\lambda$. The integrating factor is $\\exp(\\int \\mu dt) = \\exp(\\mu t)$.\nMultiplying by the integrating factor gives:\n$$ \\frac{d}{dt}(m(t)\\exp(\\mu t)) = \\lambda \\exp(\\mu t) $$\nIntegrating with respect to $t$:\n$$ m(t)\\exp(\\mu t) = \\int \\lambda \\exp(\\mu t) dt = \\frac{\\lambda}{\\mu} \\exp(\\mu t) + C_1 $$\nwhere $C_1$ is the constant of integration. Dividing by $\\exp(\\mu t)$:\n$$ m(t) = \\frac{\\lambda}{\\mu} + C_1 \\exp(-\\mu t) $$\nThe initial condition is $X(0) = n_0$, which is deterministic. Thus, $m(0) = \\mathbb{E}[X(0)] = n_0$.\n$$ n_0 = \\frac{\\lambda}{\\mu} + C_1 \\implies C_1 = n_0 - \\frac{\\lambda}{\\mu} $$\nSubstituting $C_1$ back, we obtain the expression for the mean:\n$$ m(t) = \\frac{\\lambda}{\\mu} + \\left(n_0 - \\frac{\\lambda}{\\mu}\\right)\\exp(-\\mu t) $$\n\n**Derivation of the Variance, $v(t) = \\operatorname{Var}(X(t))$**\nThe variance is $v(t) = \\langle X(t)^2 \\rangle - m(t)^2$. We first derive an ODE for the second moment, $\\langle X^2 \\rangle$. Let $f(X)=X^2$.\n$$ \\frac{d\\langle X^2 \\rangle}{dt} = \\langle \\lambda ((X+1)^2 - X^2) \\rangle + \\langle \\mu X ((X-1)^2 - X^2) \\rangle $$\n$$ \\frac{d\\langle X^2 \\rangle}{dt} = \\langle \\lambda (2X+1) \\rangle + \\langle \\mu X (-2X+1) \\rangle $$\n$$ \\frac{d\\langle X^2 \\rangle}{dt} = 2\\lambda\\langle X \\rangle + \\lambda - 2\\mu\\langle X^2 \\rangle + \\mu\\langle X \\rangle = (2\\lambda+\\mu)m(t) + \\lambda - 2\\mu\\langle X^2 \\rangle $$\nNow, consider the time derivative of the variance $v(t) = \\langle X^2 \\rangle - m(t)^2$:\n$$ \\frac{dv}{dt} = \\frac{d\\langle X^2 \\rangle}{dt} - 2m(t)\\frac{dm(t)}{dt} $$\nSubstituting the expressions for the derivatives:\n$$ \\frac{dv}{dt} = \\left[ (2\\lambda+\\mu)m(t) + \\lambda - 2\\mu\\langle X^2 \\rangle \\right] - 2m(t)[\\lambda - \\mu m(t)] $$\n$$ \\frac{dv}{dt} = (2\\lambda+\\mu)m(t) + \\lambda - 2\\mu\\langle X^2 \\rangle - 2\\lambda m(t) + 2\\mu m(t)^2 $$\n$$ \\frac{dv}{dt} = \\mu m(t) + \\lambda - 2\\mu(\\langle X^2 \\rangle - m(t)^2) $$\nRecognizing that $v(t) = \\langle X^2 \\rangle - m(t)^2$, we have:\n$$ \\frac{dv(t)}{dt} = \\mu m(t) + \\lambda - 2\\mu v(t) $$\nThis is a first-order linear ODE for $v(t)$: $\\frac{dv}{dt} + 2\\mu v = \\lambda + \\mu m(t)$.\nAn elegant approach to solve for $v(t)$ is to consider the evolution of the difference $f(t) = m(t) - v(t)$.\n$$ \\frac{df(t)}{dt} = \\frac{dm(t)}{dt} - \\frac{dv(t)}{dt} = (\\lambda - \\mu m(t)) - (\\lambda + \\mu m(t) - 2\\mu v(t)) $$\n$$ \\frac{df(t)}{dt} = -2\\mu m(t) + 2\\mu v(t) = -2\\mu (m(t) - v(t)) = -2\\mu f(t) $$\nThe solution to this simple ODE is $f(t) = f(0)\\exp(-2\\mu t)$.\nThe initial condition $X(0) = n_0$ is deterministic, so the variance at $t=0$ is zero.\n$m(0) = n_0$ and $v(0) = 0$.\nTherefore, $f(0) = m(0) - v(0) = n_0 - 0 = n_0$.\nThis gives $m(t) - v(t) = n_0 \\exp(-2\\mu t)$.\nFrom this, we can express the variance in terms of the mean:\n$$ v(t) = m(t) - n_0 \\exp(-2\\mu t) $$\nSubstituting the expression for $m(t)$:\n$$ v(t) = \\left[ \\frac{\\lambda}{\\mu} + \\left(n_0 - \\frac{\\lambda}{\\mu}\\right)\\exp(-\\mu t) \\right] - n_0 \\exp(-2\\mu t) $$\n\n**Long-time Limit Analysis**\nAs $t \\to \\infty$, since $\\mu > 0$, the exponential terms $\\exp(-\\mu t)$ and $\\exp(-2\\mu t)$ both decay to zero.\nThe stationary mean is:\n$$ m_{ss} = \\lim_{t \\to \\infty} m(t) = \\frac{\\lambda}{\\mu} $$\nThe stationary variance is:\n$$ v_{ss} = \\lim_{t \\to \\infty} v(t) = \\frac{\\lambda}{\\mu} $$\nThus, at steady state, the mean equals the variance. This implies the stationary distribution of $X$ is a Poisson distribution with parameter $\\lambda/\\mu$.\n\nThe problem asks to verify that when $\\lambda = \\mu$, the stationary variance approaches the stationary mean. Our analysis shows that $m_{ss} = v_{ss} = \\lambda/\\mu$ for any positive $\\lambda$ and $\\mu$. The special case $\\lambda=\\mu$ is merely an instance of this general result, where $m_{ss} = v_{ss} = 1$. The verification is therefore immediate from the general expressions.\n\nThe final derived expressions for the mean and variance for $t \\ge 0$ are:\n$m(t) = \\frac{\\lambda}{\\mu} + (n_0 - \\frac{\\lambda}{\\mu})\\exp(-\\mu t)$\n$v(t) = \\frac{\\lambda}{\\mu} + (n_0 - \\frac{\\lambda}{\\mu})\\exp(-\\mu t) - n_0 \\exp(-2\\mu t)$\nThese are the complete, reasoned solutions.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\lambda}{\\mu} + \\left(n_{0} - \\frac{\\lambda}{\\mu}\\right)\\exp(-\\mu t) & \\frac{\\lambda}{\\mu} + \\left(n_{0} - \\frac{\\lambda}{\\mu}\\right)\\exp(-\\mu t) - n_{0}\\exp(-2\\mu t)\n\\end{pmatrix}\n}\n$$", "id": "2649020"}, {"introduction": "Real biological systems are subject to both intrinsic noise from reaction stochasticity and extrinsic noise from a fluctuating environment. This practice [@problem_id:2648986] moves beyond the simple birth-death model to explore how to disentangle these two contributions. You will use the law of total variance to derive an expression for the total noise, quantitatively partitioning it into intrinsic and extrinsic components, a critical skill for analyzing noise in complex gene expression models.", "problem": "Consider a well-mixed isothermal reactor containing a single molecular species undergoing the linear birth-death reactions\n$$\\varnothing \\xrightarrow{\\ \\lambda\\ } X, \\quad X \\xrightarrow{\\ \\gamma\\ } \\varnothing,$$\nwhere $\\lambda$ is the zero-order synthesis rate and $\\gamma$ is the first-order degradation rate constant. Assume an ensemble of identically prepared reactors in which $\\lambda$ is subject to slow extrinsic fluctuations that are quasi-static relative to the intrinsic reaction dynamics. Conditional on a fixed $\\lambda$, the system is allowed to reach its stationary state. Across the ensemble, the extrinsic fluctuations are such that $\\ln \\lambda$ is normally distributed with mean $\\mu$ and variance $\\sigma^{2}$, i.e., $\\ln \\lambda \\sim \\mathcal{N}(\\mu,\\sigma^{2})$.\n\nLet $N$ denote the stationary molecule count of $X$ in a randomly chosen reactor from the ensemble. The Fano factor is defined as\n$$F \\equiv \\frac{\\operatorname{Var}(N)}{\\mathbb{E}[N]}.$$\n\nStarting from first principles of chemical kinetics and stochastic processes for linear reaction networks, derive an explicit analytic expression for the stationary ensemble-averaged Fano factor $F$ in terms of $\\mu$, $\\sigma^{2}$, and $\\gamma$. Furthermore, decompose $F$ into an intrinsic contribution and an extrinsic contribution arising from the fluctuations of $\\lambda$. Express your final answer as a row matrix containing three entries in the order\n$$\\bigl[F,\\ F_{\\text{intrinsic}},\\ F_{\\text{extrinsic}}\\bigr],$$\nwhere $F_{\\text{intrinsic}}$ is the contribution that would remain if $\\lambda$ were fixed, and $F_{\\text{extrinsic}}$ is the additive contribution due solely to the variability of $\\lambda$ across the ensemble. No rounding is required. The Fano factor is dimensionless; do not include units in your final answer.", "solution": "The problem investigates a linear birth-death process described by the reactions\n$$\n\\varnothing \\xrightarrow{\\ \\lambda\\ } X, \\quad X \\xrightarrow{\\ \\gamma\\ } \\varnothing\n$$\nThe synthesis rate $\\lambda$ is not constant but is subject to slow extrinsic fluctuations. Conditional on a particular value of $\\lambda$, the system reaches a stationary state. Across an ensemble of such systems, $\\lambda$ is a random variable such that its natural logarithm is normally distributed: $\\ln \\lambda \\sim \\mathcal{N}(\\mu, \\sigma^2)$. This implies $\\lambda$ follows a log-normal distribution.\n\nOur first step is to analyze the system's behavior for a fixed, given value of $\\lambda$. The stationary distribution of the number of molecules $N$ for this linear birth-death process is a Poisson distribution. This can be derived from the chemical master equation. At steady state, the probability distribution $P(n|\\lambda)$ for having $n$ molecules, conditional on $\\lambda$, is given by:\n$$\nP(n|\\lambda) = \\frac{(\\lambda/\\gamma)^n}{n!} \\exp\\left(-\\frac{\\lambda}{\\gamma}\\right)\n$$\nFor a Poisson distribution, the mean is equal to its variance. Therefore, the conditional mean and variance of the molecule count $N$ are:\n$$\n\\mathbb{E}[N|\\lambda] = \\frac{\\lambda}{\\gamma}\n$$\n$$\n\\operatorname{Var}(N|\\lambda) = \\frac{\\lambda}{\\gamma}\n$$\n\nThe overall statistics of $N$ across the ensemble are obtained by averaging over the distribution of $\\lambda$. We employ the law of total expectation and the law of total variance. The ensemble-averaged mean of $N$, denoted $\\mathbb{E}[N]$, is:\n$$\n\\mathbb{E}[N] = \\mathbb{E}_{\\lambda}\\bigl[\\mathbb{E}[N|\\lambda]\\bigr]\n$$\nThe ensemble-averaged variance of $N$, denoted $\\operatorname{Var}(N)$, is given by the law of total variance:\n$$\n\\operatorname{Var}(N) = \\mathbb{E}_{\\lambda}\\bigl[\\operatorname{Var}(N|\\lambda)\\bigr] + \\operatorname{Var}_{\\lambda}\\bigl[\\mathbb{E}[N|\\lambda]\\bigr]\n$$\nThe first term, $\\mathbb{E}_{\\lambda}[\\operatorname{Var}(N|\\lambda)]$, represents the averaged intrinsic variance, while the second term, $\\operatorname{Var}_{\\lambda}[\\mathbb{E}[N|\\lambda]]$, represents the extrinsic variance arising from the fluctuations of the conditional mean.\n\nTo evaluate these expressions, we need the moments of the log-normally distributed rate $\\lambda$. Given $Y = \\ln \\lambda \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the moments of $\\lambda = \\exp(Y)$ are found using the moment-generating function of a normal distribution, $M_Y(t) = \\mathbb{E}[\\exp(tY)] = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nThe mean of $\\lambda$ is:\n$$\n\\mathbb{E}[\\lambda] = \\mathbb{E}[\\exp(Y)] = M_Y(1) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\n$$\nThe second moment of $\\lambda$ is:\n$$\n\\mathbb{E}[\\lambda^2] = \\mathbb{E}[\\exp(2Y)] = M_Y(2) = \\exp\\left(2\\mu + \\frac{1}{2}\\sigma^2(2^2)\\right) = \\exp(2\\mu + 2\\sigma^2)\n$$\nThe variance of $\\lambda$ is then:\n$$\n\\operatorname{Var}(\\lambda) = \\mathbb{E}[\\lambda^2] - (\\mathbb{E}[\\lambda])^2 = \\exp(2\\mu + 2\\sigma^2) - \\left[\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\right]^2 = \\exp(2\\mu + 2\\sigma^2) - \\exp(2\\mu + \\sigma^2) = \\exp(2\\mu + \\sigma^2)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nNow we can compute the ensemble moments of $N$. The mean number of molecules is:\n$$\n\\mathbb{E}[N] = \\mathbb{E}_{\\lambda}\\left[\\frac{\\lambda}{\\gamma}\\right] = \\frac{1}{\\gamma}\\mathbb{E}[\\lambda] = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\n$$\nFor the variance of $N$, we evaluate its two components:\n1. Averaged intrinsic variance:\n$$\n\\mathbb{E}_{\\lambda}\\bigl[\\operatorname{Var}(N|\\lambda)\\bigr] = \\mathbb{E}_{\\lambda}\\left[\\frac{\\lambda}{\\gamma}\\right] = \\frac{1}{\\gamma}\\mathbb{E}[\\lambda] = \\mathbb{E}[N] = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\n$$\n2. Extrinsic variance:\n$$\n\\operatorname{Var}_{\\lambda}\\bigl[\\mathbb{E}[N|\\lambda]\\bigr] = \\operatorname{Var}_{\\lambda}\\left[\\frac{\\lambda}{\\gamma}\\right] = \\frac{1}{\\gamma^2}\\operatorname{Var}(\\lambda) = \\frac{1}{\\gamma^2}\\exp(2\\mu + \\sigma^2)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nThe total variance is the sum of these two terms:\n$$\n\\operatorname{Var}(N) = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + \\frac{1}{\\gamma^2}\\exp(2\\mu + \\sigma^2)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nThe Fano factor is defined as $F \\equiv \\operatorname{Var}(N)/\\mathbb{E}[N]$. Substituting our expressions for $\\operatorname{Var}(N)$ and $\\mathbb{E}[N]$:\n$$\nF = \\frac{\\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + \\frac{1}{\\gamma^2}\\exp(2\\mu + \\sigma^2)\\bigl(\\exp(\\sigma^2) - 1\\bigr)}{\\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}\n$$\n$$\nF = 1 + \\frac{\\frac{1}{\\gamma^2}\\exp(2\\mu + \\sigma^2)\\bigl(\\exp(\\sigma^2) - 1\\bigr)}{\\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}\n$$\nSimplifying the second term:\n$$\n\\frac{1}{\\gamma}\\frac{\\exp(2\\mu + \\sigma^2)}{\\exp(\\mu + \\frac{\\sigma^2}{2})}\\bigl(\\exp(\\sigma^2) - 1\\bigr) = \\frac{1}{\\gamma}\\exp\\left(2\\mu - \\mu + \\sigma^2 - \\frac{\\sigma^2}{2}\\right)\\bigl(\\exp(\\sigma^2) - 1\\bigr) = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nSo, the total Fano factor is:\n$$\nF = 1 + \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nThe problem requires decomposition of $F$ into intrinsic and extrinsic contributions. The decomposition of variance naturally leads to a decomposition of the Fano factor:\n$$\nF = \\frac{\\mathbb{E}_{\\lambda}[\\operatorname{Var}(N|\\lambda)]}{\\mathbb{E}[N]} + \\frac{\\operatorname{Var}_{\\lambda}[\\mathbb{E}[N|\\lambda]]}{\\mathbb{E}[N]} = F_{\\text{intrinsic}} + F_{\\text{extrinsic}}\n$$\nThe intrinsic contribution, $F_{\\text{intrinsic}}$, is what remains if $\\lambda$ is fixed ($\\sigma^2=0$). From the structure of the variance decomposition:\n$$\nF_{\\text{intrinsic}} = \\frac{\\mathbb{E}_{\\lambda}[\\operatorname{Var}(N|\\lambda)]}{\\mathbb{E}[N]} = \\frac{\\mathbb{E}[N]}{\\mathbb{E}[N]} = 1\n$$\nThis corresponds to the Fano factor of a Poisson process, which is the underlying intrinsic process.\nThe extrinsic contribution, $F_{\\text{extrinsic}}$, is the additive term arising from the variability of $\\lambda$:\n$$\nF_{\\text{extrinsic}} = \\frac{\\operatorname{Var}_{\\lambda}[\\mathbb{E}[N|\\lambda]]}{\\mathbb{E}[N]} = F - F_{\\text{intrinsic}} = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\bigl(\\exp(\\sigma^2) - 1\\bigr)\n$$\nThus, the three required quantities are:\n$F = 1 + \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(\\exp(\\sigma^2) - 1\\right)$\n$F_{\\text{intrinsic}} = 1$\n$F_{\\text{extrinsic}} = \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(\\exp(\\sigma^2) - 1\\right)$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 + \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(\\exp(\\sigma^2) - 1\\right) & 1 & \\frac{1}{\\gamma}\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(\\exp(\\sigma^2) - 1\\right)\n\\end{pmatrix}\n}\n$$", "id": "2648986"}, {"introduction": "While analytical methods are powerful, many critical phenomena, such as noise-induced switching between cellular states, are too rare to be studied with direct simulation. This final practice [@problem_id:2648940] introduces a hands-on computational challenge: implementing an importance sampling scheme to efficiently estimate the probability of such rare events. By deriving the likelihood ratio from first principles and building a biased Stochastic Simulation Algorithm (SSA), you will gain proficiency in an advanced simulation technique essential for modern computational systems biology.", "problem": "Consider a well-mixed, isothermal birth–death reaction network modeling a single chemical species subject to intrinsic fluctuations. The network consists of two reactions under the Chemical Master Equation (CME): birth $\\varnothing \\to X$ with propensity $a_1(n) = \\lambda$ and death $X \\to \\varnothing$ with propensity $a_2(n) = \\mu n$, where $n \\in \\mathbb{Z}_{\\ge 0}$ is the molecule copy number, $\\lambda > 0$ and $\\mu > 0$ are constants. The exact Stochastic Simulation Algorithm (SSA) simulates the continuous-time Markov jump process by drawing exponentially distributed waiting times with rate $a_0(n) = a_1(n) + a_2(n)$ and selecting the next reaction with probability proportional to its propensity.\n\nDefine a noise-induced transition as the first-hitting event of the set $\\{n \\ge M\\}$ starting from $n(0) = n_0$ within a finite horizon $[0,T]$. Denote by $\\tau_M = \\inf\\{t \\ge 0 : n(t) \\ge M\\}$ the hitting time, and by $p = \\mathbb{P}[\\tau_M \\le T \\mid n(0) = n_0]$ the transition probability.\n\nYour task is to design and implement an importance sampling scheme embedded in SSA that accelerates sampling of such rare transitions without distorting their probabilities. Proceed from first principles as follows:\n\n1. Base the derivation on the definition of the CME for continuous-time Markov jump processes and on the exact SSA construction of paths via exponentially distributed waiting times. Consider a biased SSA that simulates an alternative process with a modified birth propensity $a_1'(n) = b \\lambda$ and the same death propensity $a_2'(n) = \\mu n$, where $b > 0$ is a user-chosen tilt factor. Let $a_0'(n) = a_1'(n) + a_2'(n)$ denote the total biased propensity.\n\n2. Derive, from the Radon–Nikodym derivative for Markov jump processes, the pathwise likelihood ratio $L(\\omega)$ between the original and the biased dynamics up to a stopping time $\\tau = \\min\\{T,\\tau_M\\}$ when the event is defined by the condition $\\{\\tau_M \\le T\\}$. Express $L(\\omega)$ only in terms of the simulated waiting times and the reaction indices along the path up to $\\tau$.\n\n3. Show that an unbiased estimator for $p$ under the biased dynamics has the form\n$$\n\\widehat{p} = \\frac{1}{N}\\sum_{k=1}^N L(\\omega^{(k)}) \\,\\mathbf{1}\\{\\tau_M^{(k)} \\le T\\},\n$$\nwhere $\\omega^{(k)}$ are independent trajectories generated by the biased SSA, $N \\in \\mathbb{N}$ is the number of simulated trajectories, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Be precise about whether a final no-reaction survival interval contributes to $L(\\omega)$ depending on whether $\\tau_M \\le T$ occurs.\n\n4. Implement a program that:\n   - Uses an exact SSA under the biased propensities $a_1'(n) = b\\lambda$, $a_2'(n) = \\mu n$ to generate independent trajectories starting from $n(0)=n_0$ and stopping at time $\\tau = \\min\\{T,\\tau_M\\}$, where $\\tau_M$ is the first time the process reaches or exceeds $M$.\n   - Accumulates the log-likelihood ratio $\\log L$ along each trajectory via waiting time and reaction selection contributions implied by your derivation, taking care to include the final survival contribution over the interval $[t, T]$ only if no hit occurs (that is, when $\\tau = T$).\n   - Returns the Monte Carlo estimate $\\widehat{p}$ of the noise-induced transition probability $p$.\n\nUse a fixed random seed $12345$ for reproducibility. There are no physical units to report in this problem.\n\nTest Suite:\nFor each of the following parameter sets, compute and return the unbiased importance sampling estimate $\\widehat{p}$ as a float.\n\n- Case A (happy path, accelerated): $(\\lambda, \\mu, n_0, M, T, b, N) = (2.0, 0.2, 10, 40, 10.0, 2.5, 5000)$.\n- Case B (boundary case, no bias): $(\\lambda, \\mu, n_0, M, T, b, N) = (2.0, 0.2, 10, 40, 10.0, 1.0, 5000)$.\n- Case C (short horizon, accelerated): $(\\lambda, \\mu, n_0, M, T, b, N) = (2.0, 0.2, 10, 40, 5.0, 2.5, 5000)$.\n- Case D (harder target, stronger tilt): $(\\lambda, \\mu, n_0, M, T, b, N) = (2.0, 0.2, 10, 50, 10.0, 3.0, 5000)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above, for example, $[\\widehat{p}_A,\\widehat{p}_B,\\widehat{p}_C,\\widehat{p}_D]$. Each $\\widehat{p}$ must be printed as a decimal number.", "solution": "The core of the problem is to compute the probability $p = \\mathbb{P}[\\tau_M \\le T \\mid n(0) = n_0]$, where $\\tau_M = \\inf\\{t \\ge 0 : n(t) \\ge M\\}$ is the first hitting time of the set $\\{n \\ge M\\}$. Since this is a rare event for the chosen parameters, a direct Monte Carlo simulation is inefficient. We employ importance sampling by simulating the system under a biased probability measure $\\mathbb{P}'$ and re-weighting the outcomes to recover an unbiased estimate under the original measure $\\mathbb{P}$.\n\nA path of the system, $\\omega$, is a sequence of states and reaction times. Up to a stopping time $\\tau$, let the path consist of $K$ reactions $j_0, j_1, \\dots, j_{K-1}$ occurring at times $t_1, t_2, \\dots, t_K$, with $t_0=0$. The state is $n_i$ in the interval $[t_i, t_{i+1})$. The probability density of observing such a path under the original dynamics $\\mathbb{P}$ is given by the product of the probabilities of reaction choices and waiting times:\n$$\nf_{\\mathbb{P}}(\\omega) = \\left( \\prod_{i=0}^{K-1} a_{j_i}(n_i) e^{-a_0(n_i)(t_{i+1}-t_i)} \\right) e^{-a_0(n_K)(\\tau-t_K)}\n$$\nwhere $a_{j_i}(n_i)$ is the propensity of the chosen reaction $j_i$ and $a_0(n_i)$ is the total propensity in state $n_i$. The final term represents the probability of no reaction occurring in the interval $[t_K, \\tau]$.\n\nSimilarly, under the biased dynamics $\\mathbb{P}'$ with propensities $a'_j(n)$, the density is:\n$$\nf_{\\mathbb{P}'}(\\omega) = \\left( \\prod_{i=0}^{K-1} a'_{j_i}(n_i) e^{-a'_0(n_i)(t_{i+1}-t_i)} \\right) e^{-a'_0(n_K)(\\tau-t_K)}\n$$\n\nThe likelihood ratio, or Radon-Nikodym derivative, $L(\\omega) = \\frac{d\\mathbb{P}}{d\\mathbb{P}'}(\\omega)$, is the ratio of these path densities:\n$$\nL(\\omega) = \\frac{f_{\\mathbb{P}}(\\omega)}{f_{\\mathbb{P}'}(\\omega)} = \\left( \\prod_{i=0}^{K-1} \\frac{a_{j_i}(n_i)}{a'_{j_i}(n_i)} \\right) \\exp\\left( \\sum_{i=0}^{K-1} (a'_0(n_i) - a_0(n_i))(t_{i+1}-t_i) + (a'_0(n_K) - a_0(n_K))(\\tau-t_K) \\right)\n$$\nThe sum in the exponent is a piecewise constant integral over time. We can write this more compactly as:\n$$\nL(\\omega) = \\left( \\prod_{i=0}^{K-1} \\frac{a_{j_i}(n(t_i))}{a'_{j_i}(n(t_i))} \\right) \\exp\\left( \\int_0^\\tau [a'_0(n(s)) - a_0(n(s))] ds \\right)\n$$\nFor numerical stability, we work with the log-likelihood:\n$$\n\\log L(\\omega) = \\sum_{i=0}^{K-1} \\log\\left(\\frac{a_{j_i}(n(t_i))}{a'_{j_i}(n(t_i))}\\right) + \\int_0^\\tau [a'_0(n(s)) - a_0(n(s))] ds\n$$\nNow, we substitute the specific propensities for the given birth-death process:\nOriginal propensities: $a_1(n) = \\lambda$, $a_2(n) = \\mu n$. Total: $a_0(n) = \\lambda + \\mu n$.\nBiased propensities: $a'_1(n) = b\\lambda$, $a'_2(n) = \\mu n$. Total: $a'_0(n) = b\\lambda + \\mu n$.\n\nThe difference in total propensities is constant: $a'_0(n) - a_0(n) = (b\\lambda + \\mu n) - (\\lambda + \\mu n) = (b-1)\\lambda$.\nThe integral term simplifies to $\\int_0^\\tau (b-1)\\lambda ds = (b-1)\\lambda\\tau$.\n\nThe ratio term depends on which reaction occurs:\n- For a birth reaction ($j=1$): $\\frac{a_1(n)}{a'_1(n)} = \\frac{\\lambda}{b\\lambda} = \\frac{1}{b}$. The log-contribution is $-\\log(b)$.\n- For a death reaction ($j=2$): $\\frac{a_2(n)}{a'_2(n)} = \\frac{\\mu n}{\\mu n} = 1$. The log-contribution is $\\log(1) = 0$.\n\nIf a path undergoes $K_{birth}$ birth reactions up to the stopping time $\\tau$, the log-likelihood ratio becomes:\n$$\n\\log L(\\omega) = -K_{birth} \\log(b) + (b-1)\\lambda\\tau\n$$\nThe stopping time for the problem is $\\tau = \\min(T, \\tau_M)$.\n\nThe probability $p$ is the expectation of an indicator function, $p = \\mathbb{E}_{\\mathbb{P}}[\\mathbf{1}_{\\{\\tau_M \\le T\\}}]$. Using importance sampling, this expectation is transformed into an expectation under the biased measure $\\mathbb{P}'$:\n$$\np = \\mathbb{E}_{\\mathbb{P}'}[L(\\omega) \\cdot \\mathbf{1}_{\\{\\tau_M \\le T\\}}]\n$$\nThe Monte Carlo estimator for $p$ is the sample mean of $N$ independent trajectories $\\{\\omega^{(k)}\\}$ simulated under $\\mathbb{P}'$:\n$$\n\\widehat{p} = \\frac{1}{N}\\sum_{k=1}^N L(\\omega^{(k)}) \\cdot \\mathbf{1}_{\\{\\tau_M^{(k)} \\le T\\}}\n$$\nThis confirms the form of the estimator given in the problem. Notice that if a trajectory $\\omega^{(k)}$ does not hit the target state $M$ before time $T$, i.e., $\\tau_M^{(k)} > T$, then the indicator function is zero. Consequently, the contribution of this path to the sum is zero, regardless of the value of its likelihood ratio $L(\\omega^{(k)})$. Thus, we only need to compute and accumulate $L(\\omega)$ for trajectories that successfully hit the target state within the time horizon.\n\nThe algorithm for the simulation is as follows:\nFor each of $N$ trajectories:\n1. Initialize time $t=0$, molecule count $n=n_0$, and birth event count $K_{birth}=0$.\n2. Simulate the trajectory using SSA with the biased propensities $a'_1(n) = b\\lambda$ and $a'_2(n) = \\mu n$. The simulation proceeds as a sequence of jumps.\n3. The simulation of a single trajectory stops when either $n \\ge M$ (a hit) or $t \\ge T$ (a timeout).\n4. If the trajectory stops because $n \\ge M$ at some time $t_{hit} \\le T$, the event $\\{\\tau_M \\le T\\}$ has occurred. The stopping time is $\\tau = t_{hit}$. Calculate the likelihood weight for this path, $L = \\exp(-K_{birth} \\log(b) + (b-1)\\lambda\\tau)$, and add it to a running sum.\n5. If the trajectory stops because time $t$ reaches or exceeds $T$ while $n < M$, the event has not occurred. The contribution to the sum is $0$.\n6. The final estimate $\\widehat{p}$ is the total accumulated weight divided by the number of trajectories $N$.\n\nThis procedure is implemented in the following Python code. A fixed random seed ensures reproducibility. For the case $b=1$, the biased dynamics are identical to the original dynamics, $L(\\omega)=1$ for all paths, and the method reduces to a standard (direct) Monte Carlo simulation, providing a valuable baseline for comparison.", "answer": "```python\nimport numpy as np\n\ndef run_is_simulation(lambda_, mu, n0, M, T, b, N, rng):\n    \"\"\"\n    Runs an importance sampling simulation for the birth-death process.\n\n    Args:\n        lambda_ (float): Birth rate parameter.\n        mu (float): Death rate parameter.\n        n0 (int): Initial number of molecules.\n        M (int): Target threshold for the number of molecules.\n        T (float): Time horizon.\n        b (float): Importance sampling tilt factor for the birth propensity.\n        N (int): Number of simulation trajectories.\n        rng (numpy.random.Generator): A NumPy random number generator.\n\n    Returns:\n        float: The estimated probability of hitting the threshold M before time T.\n    \"\"\"\n    total_p_weighted_sum = 0.0\n\n    # Pre-compute constants for the likelihood calculation\n    log_b = np.log(b)\n    lambda_term = (b - 1) * lambda_\n\n    for _ in range(N):\n        t = 0.0\n        n = n0\n        n_births = 0\n\n        # Simulate a single trajectory until it hits the target or times out\n        while t < T and n < M:\n            # Biased propensities\n            a1_prime = b * lambda_\n            # Death propensity is 0 if n=0\n            a2_prime = mu * n if n > 0 else 0.0\n            a0_prime = a1_prime + a2_prime\n\n            # All parameters are positive, so a0_prime > 0 is guaranteed.\n            # Draw waiting time from exponential distribution under biased dynamics\n            dt = rng.exponential(scale=1.0 / a0_prime)\n\n            # Determine next state and time\n            t_next = t + dt\n\n            # If the next event would occur after the time horizon T,\n            # this trajectory does not hit the target. The loop will terminate.\n            if t_next >= T:\n                break\n\n            # Advance time and update state\n            t = t_next\n            \n            # Select reaction based on biased propensities\n            if rng.random() < a1_prime / a0_prime:\n                # Birth reaction\n                n += 1\n                n_births += 1\n            else:\n                # Death reaction\n                n -= 1\n        \n        # After the loop, check if the trajectory was a \"hit\"\n        # A hit occurs if the loop terminated because n >= M. \n        # The condition t < T is implicitly satisfied because of the loop break logic.\n        if n >= M:\n            # The path has hit the target M at time t.\n            # The stopping time is tau = t.\n            # Calculate the log-likelihood ratio for this path.\n            log_L = -n_births * log_b + lambda_term * t\n            \n            # Add the likelihood weight to the total sum.\n            L = np.exp(log_L)\n            total_p_weighted_sum += L\n\n    # The Monte Carlo estimate is the average of the weighted samples.\n    # Paths that did not hit contribute 0 to the sum.\n    return total_p_weighted_sum / N\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    # Set fixed random seed for reproducibility as required by the problem\n    rng = np.random.default_rng(12345)\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (lambda, mu, n0, M, T, b, N)\n        (2.0, 0.2, 10, 40, 10.0, 2.5, 5000),  # Case A\n        (2.0, 0.2, 10, 40, 10.0, 1.0, 5000),  # Case B\n        (2.0, 0.2, 10, 40, 5.0, 2.5, 5000),   # Case C\n        (2.0, 0.2, 10, 50, 10.0, 3.0, 5000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_, mu, n0, M, T, b, N = case\n        p_hat = run_is_simulation(lambda_, mu, n0, M, T, b, N, rng)\n        results.append(p_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2648940"}]}