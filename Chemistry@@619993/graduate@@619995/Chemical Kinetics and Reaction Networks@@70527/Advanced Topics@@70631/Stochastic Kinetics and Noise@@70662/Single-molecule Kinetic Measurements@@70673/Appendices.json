{"hands_on_practices": [{"introduction": "Our instruments observe the world in discrete snapshots, but molecular processes occur continuously in time. This practice explores the fundamental consequences of this mismatch, focusing on the concept of \"missed events\"â€”rapid transitions that occur between our observations and are thus invisible. By deriving the relationship between the true microscopic rates, such as $k_{AB}$ and $k_{BA}$, and the probabilities observed frame-to-frame over a time interval $\\Delta t$, this exercise ([@problem_id:2674084]) provides a crucial foundation for correctly interpreting kinetic data from any time-sampled experiment.", "id": "2674084", "problem": "A single biomolecule interconverts between two conformational states $A$ and $B$ according to continuous-time, memoryless (Markovian) kinetics with constant transition rates $k_{AB}$ for $A \\to B$ and $k_{BA}$ for $B \\to A$. The system is monitored by an ideal state discriminator that samples the state only at discrete times $t_{n} = n\\,\\Delta t$, with $\\Delta t &gt; 0$ fixed by the instrumentation. The sampling process has no misclassification at the sampling instants, but it has finite time resolution: any transitions that occur between $t_{n}$ and $t_{n+1}$ are not directly observed unless they lead to a different state at $t_{n+1}$ compared to $t_{n}$. In this context, define precisely what is meant by \"missed events\" due to finite time resolution $\\Delta t$ for this two-state process. Then, starting from the continuous-time master equation for a two-state Markov process, derive a closed-form expression for the effective observed per-frame transition probability from $A$ at time $t$ to $B$ at time $t+\\Delta t$ (that is, the conditional probability that the sampled state changes from $A$ to $B$ between two consecutive frames), expressed as a function of $k_{AB}$, $k_{BA}$, and $\\Delta t$. Your final answer must be a single closed-form analytic expression in terms of $k_{AB}$, $k_{BA}$, and $\\Delta t$.", "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n-   A single biomolecule interconverts between two conformational states $A$ and $B$.\n-   Kinetics are continuous-time, memoryless (Markovian).\n-   Transition rates are constant: $k_{AB}$ for $A \\to B$ and $k_{BA}$ for $B \\to A$.\n-   The system state is sampled at discrete times $t_n = n\\,\\Delta t$, with $\\Delta t > 0$.\n-   The sampling process has no misclassification at the sampling instants.\n-   Transitions between $t_n$ and $t_{n+1}$ are not directly observed.\n-   The task requires two parts: (1) Define \"missed events\" for this process. (2) Derive a closed-form expression for the effective observed per-frame transition probability from $A$ to $B$, $P(B \\text{ at } t+\\Delta t | A \\text{ at } t)$, as a function of $k_{AB}$, $k_{BA}$, and $\\Delta t$, starting from the continuous-time master equation.\n\nValidation against criteria:\n-   **Scientific Grounding**: The problem is fundamentally sound. It describes a two-state continuous-time Markov chain, a canonical model in chemical kinetics, statistical physics, and single-molecule biophysics. The issue of finite time resolution and its effect on measured rates is a central and practical problem in experimental data analysis. The premises are scientifically correct.\n-   **Well-Posedness**: The problem is well-posed. It requests the derivation of a specific physical quantity, a conditional probability, based on a fully specified model. The parameters are defined, and a unique, stable solution is expected to exist.\n-   **Objectivity**: The language is precise, technical, and free of subjective or ambiguous terminology.\n-   **Completeness and Consistency**: The problem is self-contained. It provides all necessary parameters ($k_{AB}$, $k_{BA}$, $\\Delta t$) to derive the requested expression. There are no internal contradictions.\n-   **Feasibility**: The model is a standard idealization used widely in the sciences. The derivation is a standard exercise in the theory of stochastic processes.\n-   **Triviality**: The problem is not trivial. It requires solving a system of differential equations and correctly interpreting the result in the context of discrete sampling, which represents a core conceptual challenge in relating microscopic dynamics to experimental observables.\n\nVerdict: The problem is valid. It is a standard, well-posed problem in physical chemistry. Proceeding to solution.\n\nFirst, we address the definition of \"missed events.\" In the context of discrete sampling of a continuous-time process, an observation consists of a sequence of states recorded at times $t_n = n \\Delta t$. A transition is \"observed\" if the state at $t_{n+1}$ is different from the state at $t_n$. \"Missed events\" refer to any sequence of one or more transitions occurring within the time interval $(t_n, t_{n+1})$ that results in the system state at time $t_{n+1}$ being identical to the state at time $t_n$. For a two-state system starting in state $A$ at time $t_n$, missed events correspond to any trajectory where an even number of state changes occur during the interval $\\Delta t$, for example, the sequence $A \\to B \\to A$. The simplest such case involves a single round-trip, but any path with an even number of transitions (e.g., $A \\to B \\to A \\to B \\to A$) is also unobserved and thus constitutes missed events. The system leaves the initial state and returns to it before the next observation, rendering the excursion undetectable by the sampling protocol.\n\nSecond, we derive the expression for the effective transition probability. We seek the conditional probability $P(B, t_0+\\Delta t | A, t_0)$. Due to the time-homogeneous nature of the Markov process, this probability depends only on the time lag $\\Delta t$, not the absolute time $t_0$. We denote this probability as $P_{AB}(\\Delta t)$.\n\nLet $P_A(\\tau)$ and $P_B(\\tau)$ be the probabilities that the system is in state $A$ and $B$ at time $\\tau$, respectively, given a certain initial condition at $\\tau=0$. The continuous-time master equation for this system is:\n$$ \\frac{d P_A(\\tau)}{d\\tau} = -k_{AB} P_A(\\tau) + k_{BA} P_B(\\tau) $$\n$$ \\frac{d P_B(\\tau)}{d\\tau} = +k_{AB} P_A(\\tau) - k_{BA} P_B(\\tau) $$\nThe total probability is conserved: $P_A(\\tau) + P_B(\\tau) = 1$ for all $\\tau$. We can use this to eliminate $P_B(\\tau) = 1 - P_A(\\tau)$ from the first equation:\n$$ \\frac{d P_A(\\tau)}{d\\tau} = -k_{AB} P_A(\\tau) + k_{BA}(1 - P_A(\\tau)) $$\n$$ \\frac{d P_A(\\tau)}{d\\tau} = -(k_{AB} + k_{BA}) P_A(\\tau) + k_{BA} $$\nThis is a first-order linear ordinary differential equation. Let $k_{sum} = k_{AB} + k_{BA}$. The equation becomes:\n$$ \\frac{d P_A(\\tau)}{d\\tau} + k_{sum} P_A(\\tau) = k_{BA} $$\nThe general solution is the sum of the homogeneous solution and a particular solution. The homogeneous equation $\\frac{d P_A^h}{d\\tau} + k_{sum} P_A^h = 0$ has the solution $P_A^h(\\tau) = C \\exp(-k_{sum} \\tau)$, where $C$ is a constant of integration. For the particular solution, we assume a constant $P_A^p = K_{eq}$, which gives $0 + k_{sum} K_{eq} = k_{BA}$, so $P_A^p = \\frac{k_{BA}}{k_{sum}} = \\frac{k_{BA}}{k_{AB} + k_{BA}}$.\nThe general solution for $P_A(\\tau)$ is:\n$$ P_A(\\tau) = C \\exp(-(k_{AB} + k_{BA})\\tau) + \\frac{k_{BA}}{k_{AB} + k_{BA}} $$\nWe are interested in the evolution of the system *given* it was in state $A$ at time $\\tau=0$. This provides the initial condition $P_A(0) = 1$. Applying this to the general solution:\n$$ 1 = C \\exp(0) + \\frac{k_{BA}}{k_{AB} + k_{BA}} $$\n$$ C = 1 - \\frac{k_{BA}}{k_{AB} + k_{BA}} = \\frac{k_{AB} + k_{BA} - k_{BA}}{k_{AB} + k_{BA}} = \\frac{k_{AB}}{k_{AB} + k_{BA}} $$\nSubstituting $C$ back, we obtain the probability of remaining in state $A$ after a time $\\tau$, given the system started in $A$. This is the survival probability of state $A$, $P_{AA}(\\tau)$:\n$$ P_{AA}(\\tau) = \\frac{k_{AB}}{k_{AB} + k_{BA}} \\exp(-(k_{AB} + k_{BA})\\tau) + \\frac{k_{BA}}{k_{AB} + k_{BA}} $$\nThe quantity required by the problem is the probability of finding the system in state $B$ at time $\\Delta t$, given it started in state $A$ at time $0$. This is $P_{AB}(\\Delta t)$. By the law of total probability, $P_{AA}(\\Delta t) + P_{AB}(\\Delta t) = 1$. Therefore:\n$$ P_{AB}(\\Delta t) = 1 - P_{AA}(\\Delta t) $$\n$$ P_{AB}(\\Delta t) = 1 - \\left( \\frac{k_{AB}}{k_{AB} + k_{BA}} \\exp(-(k_{AB} + k_{BA})\\Delta t) + \\frac{k_{BA}}{k_{AB} + k_{BA}} \\right) $$\n$$ P_{AB}(\\Delta t) = \\frac{k_{AB} + k_{BA}}{k_{AB} + k_{BA}} - \\frac{k_{BA}}{k_{AB} + k_{BA}} - \\frac{k_{AB}}{k_{AB} + k_{BA}} \\exp(-(k_{AB} + k_{BA})\\Delta t) $$\n$$ P_{AB}(\\Delta t) = \\frac{k_{AB}}{k_{AB} + k_{BA}} - \\frac{k_{AB}}{k_{AB} + k_{BA}} \\exp(-(k_{AB} + k_{BA})\\Delta t) $$\nFactoring the common term yields the final expression for the effective observed transition probability from $A$ to $B$ in a time interval $\\Delta t$:\n$$ P_{AB}(\\Delta t) = \\frac{k_{AB}}{k_{AB} + k_{BA}} \\left( 1 - \\exp(-(k_{AB} + k_{BA})\\Delta t) \\right) $$\nThis expression represents the probability that a system starting in state $A$ will be observed in state $B$ after one sampling interval $\\Delta t$. It correctly accounts for all possible event histories, including those with multiple transitions, that result in a final state $B$.", "answer": "$$\\boxed{\\frac{k_{AB}}{k_{AB} + k_{BA}} \\left( 1 - \\exp\\left(-\\left(k_{AB} + k_{BA}\\right)\\Delta t\\right) \\right)}$$"}, {"introduction": "Beyond the blur of time-averaging, some instruments have a \"dead time,\" a period of blindness after an event during which subsequent events cannot be detected. This leads to a systematic exclusion of short-lived states from our datasets, a form of data truncation that can severely bias kinetic estimates. This practice ([@problem_id:2674069]) challenges you to quantify this bias and, more importantly, to use the principles of maximum likelihood estimation to derive a corrected estimator that accounts for the missing data, a vital skill for rigorous quantitative analysis.", "id": "2674069", "problem": "In a single-molecule dwell-time experiment, the true residence time in a state is modeled as an independent and identically distributed exponential random variable with rate parameter $k$, so that the probability density function is $f(t \\mid k) = k \\exp(-k t)$ for $t \\ge 0$. The detector has a fixed dead time $\\tau_{d} &gt; 0$, meaning that any event with true duration $T &lt; \\tau_{d}$ is completely unobserved and hence absent from the recorded data set. For events with $T \\ge \\tau_{d}$, the full true duration $T$ is recorded without error. Let the observed sample be $\\{t_{i}\\}_{i=1}^{n}$ with each $t_{i} \\ge \\tau_{d}$, arising from the conditional distribution of $T$ given $T \\ge \\tau_{d}$.\n\nAn experimentalist naively estimates $k$ by treating $\\{t_{i}\\}$ as if they were untruncated and uses the estimator $\\hat{k}_{\\text{naive}} = 1/\\bar{t}$, where $\\bar{t} = \\frac{1}{n} \\sum_{i=1}^{n} t_{i}$. Starting from the definitions of conditional densities and the law of large numbers, and using the likelihood principle for independent observations:\n\n1. Define the asymptotic bias of the naive estimator as $\\operatorname{plim}_{n \\to \\infty} \\hat{k}_{\\text{naive}} - k$ and derive it as an explicit function of $k$ and $\\tau_{d}$.\n2. Derive, from first principles, the maximum likelihood estimator under the correctly truncated exponential likelihood, expressed in closed form in terms of $\\bar{t}$ and $\\tau_{d}$.\n\nProvide your final answer as a single row matrix containing two entries: first, the asymptotic bias from part 1, and second, the corrected maximum likelihood estimator from part 2. No numerical evaluation is required, and no rounding is needed. Report only the analytic expressions.", "solution": "The problem statement is scientifically sound, well-posed, and objective. It represents a standard and non-trivial problem in the statistical analysis of experimental data subject to truncation, which is a common occurrence in single-molecule biophysics and chemical kinetics. All necessary information is provided for a complete derivation. Therefore, the problem is valid, and a solution will be provided.\n\nThe problem asks for two derivations related to the estimation of a rate constant $k$ from a truncated exponential distribution.\n\nPart 1: Asymptotic Bias of the Naive Estimator\n\nThe true residence time $T$ follows an exponential distribution with probability density function (PDF) $f(t \\mid k) = k \\exp(-k t)$ for $t \\ge 0$. Due to detector dead time $\\tau_{d}$, only events with $T \\ge \\tau_{d}$ are observed. The observed data $\\{t_{i}\\}$ are therefore drawn from the distribution of $T$ conditional on $T \\ge \\tau_{d}$.\n\nThe naive estimator is given by $\\hat{k}_{\\text{naive}} = 1/\\bar{t}$, where $\\bar{t} = \\frac{1}{n} \\sum_{i=1}^{n} t_{i}$. The asymptotic bias is defined as $\\operatorname{plim}_{n \\to \\infty} \\hat{k}_{\\text{naive}} - k$.\n\nBy the law of large numbers, the sample mean $\\bar{t}$ converges in probability to the expected value of the observed random variable. Let us denote the observed time as a random variable $T_{obs}$. The distribution of $T_{obs}$ is that of $T$ conditioned on the event $T \\ge \\tau_{d}$. Thus, $\\operatorname{plim}_{n \\to \\infty} \\bar{t} = E[T \\mid T \\ge \\tau_{d}]$.\n\nUsing the continuous form of the law of large numbers and properties of probability limits, we have:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{k}_{\\text{naive}} = \\operatorname{plim}_{n \\to \\infty} \\frac{1}{\\bar{t}} = \\frac{1}{\\operatorname{plim}_{n \\to \\infty} \\bar{t}} = \\frac{1}{E[T \\mid T \\ge \\tau_{d}]}\n$$\nThe asymptotic bias is therefore $\\frac{1}{E[T \\mid T \\ge \\tau_{d}]} - k$.\n\nTo calculate the conditional expectation $E[T \\mid T \\ge \\tau_{d}]$, we first need the probability of the conditioning event:\n$$\nP(T \\ge \\tau_{d}) = \\int_{\\tau_{d}}^{\\infty} f(t \\mid k) \\, dt = \\int_{\\tau_{d}}^{\\infty} k \\exp(-kt) \\, dt = \\left[ -\\exp(-kt) \\right]_{\\tau_{d}}^{\\infty} = 0 - (-\\exp(-k\\tau_{d})) = \\exp(-k\\tau_{d})\n$$\nThe PDF of the truncated distribution, let's call it $g(t)$, for an observed time $t \\ge \\tau_{d}$ is:\n$$\ng(t) = \\frac{f(t \\mid k)}{P(T \\ge \\tau_{d})} = \\frac{k \\exp(-kt)}{\\exp(-k\\tau_{d})} = k \\exp(-k(t - \\tau_{d})), \\quad \\text{for } t \\ge \\tau_{d}\n$$\nNow, we compute the conditional expectation:\n$$\nE[T \\mid T \\ge \\tau_{d}] = \\int_{\\tau_{d}}^{\\infty} t \\cdot g(t) \\, dt = \\int_{\\tau_{d}}^{\\infty} t \\cdot k \\exp(-k(t - \\tau_{d})) \\, dt\n$$\nLet's perform a change of variables with $u = t - \\tau_{d}$, which implies $t = u + \\tau_{d}$ and $dt = du$. The lower limit of integration becomes $u = \\tau_{d} - \\tau_{d} = 0$, and the upper limit remains $\\infty$.\n$$\nE[T \\mid T \\ge \\tau_{d}] = \\int_{0}^{\\infty} (u + \\tau_{d}) \\cdot k \\exp(-ku) \\, du = \\int_{0}^{\\infty} uk \\exp(-ku) \\, du + \\tau_{d} \\int_{0}^{\\infty} k \\exp(-ku) \\, du\n$$\nThe first integral is the expected value of a standard exponential random variable with rate $k$, which is $1/k$. The second integral is the integral of the exponential PDF over its support, which is $1$.\n$$\nE[T \\mid T \\ge \\tau_{d}] = \\frac{1}{k} + \\tau_{d}\n$$\nThis result is a manifestation of the memoryless property of the exponential distribution. The expected additional time to wait, given that $\\tau_{d}$ has already passed, is the same as the original unconditional expected time, $1/k$. So the total expected time is $\\tau_{d} + 1/k$.\n\nSubstituting this back into the expression for the asymptotic bias:\n$$\n\\text{Asymptotic Bias} = \\frac{1}{\\frac{1}{k} + \\tau_{d}} - k = \\frac{k}{1 + k\\tau_{d}} - k = k \\left( \\frac{1}{1 + k\\tau_{d}} - 1 \\right) = k \\left( \\frac{1 - (1 + k\\tau_{d})}{1 + k\\tau_{d}} \\right) = -\\frac{k^{2} \\tau_{d}}{1 + k \\tau_{d}}\n$$\n\nPart 2: Maximum Likelihood Estimator (MLE)\n\nThe likelihood function $L(k)$ for the observed independent and identically distributed sample $\\{t_{i}\\}_{i=1}^{n}$ is the product of the PDFs of the truncated distribution evaluated at each data point:\n$$\nL(k; \\{t_{i}\\}) = \\prod_{i=1}^{n} g(t_{i}) = \\prod_{i=1}^{n} k \\exp(-k(t_{i} - \\tau_{d}))\n$$\nIt is more convenient to work with the log-likelihood function, $\\mathcal{L}(k) = \\ln(L(k))$:\n$$\n\\mathcal{L}(k) = \\ln \\left( \\prod_{i=1}^{n} k \\exp(-k(t_{i} - \\tau_{d})) \\right) = \\sum_{i=1}^{n} \\ln(k \\exp(-k(t_{i} - \\tau_{d})))\n$$\n$$\n\\mathcal{L}(k) = \\sum_{i=1}^{n} \\left[ \\ln(k) - k(t_{i} - \\tau_{d}) \\right] = n \\ln(k) - k \\sum_{i=1}^{n} (t_{i} - \\tau_{d})\n$$\nWe can express the sum in terms of the sample mean $\\bar{t} = \\frac{1}{n} \\sum_{i=1}^{n} t_{i}$.\n$$\n\\sum_{i=1}^{n} (t_{i} - \\tau_{d}) = \\left(\\sum_{i=1}^{n} t_{i}\\right) - n\\tau_{d} = n\\bar{t} - n\\tau_{d} = n(\\bar{t} - \\tau_{d})\n$$\nSo, the log-likelihood is:\n$$\n\\mathcal{L}(k) = n \\ln(k) - n k (\\bar{t} - \\tau_{d})\n$$\nTo find the maximum likelihood estimator $\\hat{k}_{\\text{MLE}}$, we differentiate $\\mathcal{L}(k)$ with respect to $k$ and set the result to zero:\n$$\n\\frac{d\\mathcal{L}(k)}{dk} = \\frac{n}{k} - n(\\bar{t} - \\tau_{d}) = 0\n$$\nSolving for $k$ gives the estimator $\\hat{k}_{\\text{MLE}}$:\n$$\n\\frac{n}{\\hat{k}_{\\text{MLE}}} = n(\\bar{t} - \\tau_{d}) \\implies \\hat{k}_{\\text{MLE}} = \\frac{1}{\\bar{t} - \\tau_{d}}\n$$\nTo verify that this is a maximum, we examine the second derivative:\n$$\n\\frac{d^{2}\\mathcal{L}(k)}{dk^{2}} = \\frac{d}{dk} \\left( \\frac{n}{k} - n(\\bar{t} - \\tau_{d}) \\right) = -\\frac{n}{k^{2}}\n$$\nSince $n > 0$ and $k^{2} > 0$, the second derivative is always negative, which confirms that the log-likelihood function is concave and our solution corresponds to a maximum.\n\nThe two requested quantities are the asymptotic bias, $-\\frac{k^{2} \\tau_{d}}{1 + k \\tau_{d}}$, and the maximum likelihood estimator, $\\frac{1}{\\bar{t} - \\tau_{d}}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{k^{2} \\tau_{d}}{1 + k \\tau_{d}} & \\frac{1}{\\bar{t} - \\tau_{d}} \\end{pmatrix}}\n$$"}, {"introduction": "Analyzing complex single-molecule trajectories often requires moving beyond simple rate calculations to a more powerful probabilistic framework. The Hidden Markov Model (HMM) provides such a framework, allowing us to infer the sequence of hidden molecular states from a noisy experimental signal. This hands-on coding practice ([@problem_id:2674022]) guides you through the implementation of the cornerstone of HMM analysisâ€”the forward-backward algorithmâ€”and tackles the critical numerical stability issues that arise when working with long data traces.", "id": "2674022", "problem": "Consider a discrete-time Hidden Markov Model (HMM) for a single-molecule observation trace, where a biomolecule switches among hidden conformational states that form a time-homogeneous Markov chain. At each time index $t \\in \\{1,\\dots,T\\}$ there is a hidden state $s_t \\in \\{1,\\dots,K\\}$ with initial distribution $p(s_1=i)=\\pi_i$ and transition probabilities $p(s_{t}=j \\mid s_{t-1}=i)=A_{ij}$ for all $i,j \\in \\{1,\\dots,K\\}$. The observation at time $t$ is a real-valued scalar $y_t \\in \\mathbb{R}$ whose distribution depends only on the current hidden state $s_t$ (conditional independence). Assume that given $s_t=i$, the observation distribution is Gaussian with mean $\\mu_i$ and standard deviation $\\sigma_i$, so that the emission probability density is $b_i(y_t) = \\mathcal{N}(y_t \\mid \\mu_i,\\sigma_i^2)$. All model parameters $(\\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma})$ are known.\n\nYour task is to construct the forwardâ€“backward algorithm from first principles to compute the posterior marginal state probabilities $p(s_t=i \\mid y_{1:T})$ for all $t \\in \\{1,\\dots,T\\}$ and all $i \\in \\{1,\\dots,K\\}$, and to implement two numerically stable strategies to prevent underflow: (1) a scaled forwardâ€“backward recursion using per-time scaling factors, and (2) a log-domain recursion using the log-sum-exp transformation. The derivation must start from basic probability rules such as the definition of conditional probability, the Markov property, and the law of total probability, and it must not assume any pre-written formulas for the forward or backward recursions.\n\nProgram requirements:\n- Implement two independent computations of the posterior matrix $\\boldsymbol{\\gamma}$ where $\\gamma_{i,t} = p(s_t=i \\mid y_{1:T})$:\n  1. A scaled forwardâ€“backward method that maintains forward variables and backward variables scaled by time-dependent positive factors to avoid numerical underflow, and that recovers the sequence log-likelihood $\\log p(y_{1:T})$ from the scaling factors.\n  2. A log-domain forwardâ€“backward method that uses the log-sum-exp identity to compute the same posterior matrix and the same sequence log-likelihood.\n- For each test case below, compute the maximum absolute difference between the two posterior matrices elementwise and verify that it is less than a specified tolerance $\\varepsilon = 10^{-8}$. Also verify that for every time index $t$, the posterior entries sum to one within a tolerance $\\delta = 10^{-12}$, i.e., $\\left|\\sum_{i=1}^K \\gamma_{i,t} - 1\\right| \\le \\delta$. Finally, verify that the two log-likelihood values agree within tolerance $\\eta = 10^{-8}$ in absolute difference.\n- For each test case, return a boolean result that is true if and only if all three verifications pass.\n\nTest suite:\n- Case 1 (two-state conformational switching with moderately distinct Gaussian emissions):\n  - Number of states $K = 2$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.95 & 0.05 \\\\ 0.04 & 0.96 \\end{bmatrix}$.\n  - Emission parameters $\\boldsymbol{\\mu} = [\\,0.2,\\,0.8\\,]$, $\\boldsymbol{\\sigma} = [\\,0.05,\\,0.05\\,]$.\n  - Observation sequence of length $T=20$: $[\\,0.18,\\,0.22,\\,0.19,\\,0.81,\\,0.79,\\,0.82,\\,0.21,\\,0.20,\\,0.78,\\,0.83,\\,0.18,\\,0.17,\\,0.82,\\,0.80,\\,0.22,\\,0.19,\\,0.77,\\,0.84,\\,0.23,\\,0.20\\,]$.\n- Case 2 (three-state model with a single observation to test boundary conditions):\n  - Number of states $K = 3$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.2,\\,0.5,\\,0.3\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.90 & 0.05 & 0.05 \\\\ 0.10 & 0.80 & 0.10 \\\\ 0.05 & 0.15 & 0.80 \\end{bmatrix}$.\n  - Emission parameters $\\boldsymbol{\\mu} = [\\,0.1,\\,0.5,\\,0.9\\,]$, $\\boldsymbol{\\sigma} = [\\,0.1,\\,0.05,\\,0.1\\,]$.\n  - Observation sequence of length $T=1$: $[\\,0.52\\,]$.\n- Case 3 (two-state model with long trace and emissions far from state means to stress numerical stability):\n  - Number of states $K = 2$.\n  - Initial distribution $\\boldsymbol{\\pi} = [\\,0.5,\\,0.5\\,]$.\n  - Transition matrix $\\mathbf{A} = \\begin{bmatrix} 0.995 & 0.005 \\\\ 0.003 & 0.997 \\end{bmatrix}$.\n  - Emission parameters $\\boldsymbol{\\mu} = [\\,0.2,\\,0.8\\,]$, $\\boldsymbol{\\sigma} = [\\,0.1,\\,0.1\\,]$.\n  - Observation sequence of length $T=120$: the first $60$ entries equal to $0.45$ followed by the next $60$ entries equal to $0.55$, i.e., $[\\,\\underbrace{0.45,\\,\\dots,\\,0.45}_{60\\ \\text{times}},\\,\\underbrace{0.55,\\,\\dots,\\,0.55}_{60\\ \\text{times}}\\,]$.\n\nNumerical tolerances (dimensionless):\n- Posterior agreement tolerance $\\varepsilon = 10^{-8}$.\n- Per-time posterior normalization tolerance $\\delta = 10^{-12}$.\n- Log-likelihood agreement tolerance $\\eta = 10^{-8}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of booleans for the three test cases in the order given above, for example $[\\,\\mathrm{True},\\mathrm{False},\\mathrm{True}\\,]$. The list must be printed exactly as a comma-separated list enclosed in square brackets, with each element rendered as the Python boolean literals.", "solution": "The problem posed is a standard and well-defined task in computational statistics, specifically the inference of hidden states in a Hidden Markov Model (HMM). The parameters of the model are fully specified, and the objective is to compute the posterior marginal probabilities of the hidden states given a sequence of observations. This problem is valid, scientifically grounded, and admits a unique solution via the forward-backward algorithm. We will now derive this algorithm from first principles, and then develop two numerically stable implementations as required.\n\nLet the set of hidden states be $\\{1, \\dots, K\\}$ and the sequence of observations be $y_{1:T} = (y_1, \\dots, y_T)$. The model is defined by the initial state probabilities $\\boldsymbol{\\pi}$, where $\\pi_i = p(s_1=i)$; the transition probability matrix $\\mathbf{A}$, where $A_{ij} = p(s_{t}=j \\mid s_{t-1}=i)$; and the emission probability densities $b_i(y_t) = p(y_t \\mid s_t=i)$, which are given as Gaussian distributions $\\mathcal{N}(y_t \\mid \\mu_i, \\sigma_i^2)$.\n\nOur objective is to compute the posterior marginal probability $\\gamma_t(i) = p(s_t=i \\mid y_{1:T})$ for each time step $t \\in \\{1, \\dots, T\\}$ and each state $i \\in \\{1, \\dots, K\\}$.\n\nBy the definition of conditional probability, we have:\n$$\n\\gamma_t(i) = p(s_t=i \\mid y_{1:T}) = \\frac{p(s_t=i, y_{1:T})}{p(y_{1:T})}\n$$\nThe numerator is the joint probability of being in state $i$ at time $t$ and observing the entire sequence $y_{1:T}$. We can split the observation sequence into past ($y_{1:t}$) and future ($y_{t+1:T}$) parts relative to time $t$. Using the chain rule of probability:\n$$\np(s_t=i, y_{1:T}) = p(s_t=i, y_{1:t}, y_{t+1:T}) = p(y_{t+1:T} \\mid s_t=i, y_{1:t}) p(s_t=i, y_{1:t})\n$$\nDue to the conditional independence properties of an HMM, the future observations $y_{t+1:T}$ are independent of past observations $y_{1:t}$ given the current state $s_t=i$. Therefore, $p(y_{t+1:T} \\mid s_t=i, y_{1:t}) = p(y_{t+1:T} \\mid s_t=i)$.\nThis allows us to write:\n$$\np(s_t=i, y_{1:T}) = p(y_{t+1:T} \\mid s_t=i) p(s_t=i, y_{1:t})\n$$\nThis decomposition leads to the definition of the forward and backward variables.\nThe forward variable, $\\alpha_t(i)$, is the joint probability of the partial observation sequence $y_{1:t}$ and the state $s_t=i$:\n$$\n\\alpha_t(i) \\triangleq p(s_t=i, y_{1:t})\n$$\nThe backward variable, $\\beta_t(i)$, is the conditional probability of the future observation sequence $y_{t+1:T}$ given the state $s_t=i$:\n$$\n\\beta_t(i) \\triangleq p(y_{t+1:T} \\mid s_t=i)\n$$\nSubstituting these into the expression for the posterior, we get:\n$$\n\\gamma_t(i) = \\frac{\\alpha_t(i) \\beta_t(i)}{p(y_{1:T})}\n$$\nThe denominator, the marginal probability of the entire observation sequence (also known as the evidence or likelihood), can be obtained by marginalizing the numerator over all states $i$:\n$$\np(y_{1:T}) = \\sum_{j=1}^K p(s_t=j, y_{1:T}) = \\sum_{j=1}^K \\alpha_t(j) \\beta_t(j)\n$$\nThis relationship holds for any time step $t$. A particularly convenient choice is $t=T$, where $\\beta_T(j)$ is defined as $1$, leading to $p(y_{1:T}) = \\sum_{j=1}^K \\alpha_T(j)$.\n\nWe now derive the recursive formulas for $\\alpha_t(i)$ and $\\beta_t(i)$.\n\n**Forward Recursion**\nFor $t=1$, the forward variable is:\n$$\n\\alpha_1(i) = p(s_1=i, y_1) = p(y_1 \\mid s_1=i) p(s_1=i) = b_i(y_1) \\pi_i\n$$\nFor $t > 1$, we expand $\\alpha_t(i)$ by marginalizing over the state at time $t-1$:\n$$\n\\alpha_t(i) = p(s_t=i, y_{1:t}) = p(y_t \\mid s_t=i) \\sum_{j=1}^K p(s_t=i, s_{t-1}=j, y_{1:t-1})\n$$\nUsing the chain rule and HMM properties:\n$$\np(s_t=i, s_{t-1}=j, y_{1:t-1}) = p(s_t=i \\mid s_{t-1}=j, y_{1:t-1}) p(s_{t-1}=j, y_{1:t-1})\n$$\nBy the Markov property, $p(s_t=i \\mid s_{t-1}=j, y_{1:t-1}) = p(s_t=i \\mid s_{t-1}=j) = A_{ji}$. The second term is simply $\\alpha_{t-1}(j)$.\nThis yields the forward recursion:\n$$\n\\alpha_t(i) = b_i(y_t) \\sum_{j=1}^K \\alpha_{t-1}(j) A_{ji} \\quad \\text{for } t=2, \\dots, T\n$$\n\n**Backward Recursion**\nFor the base case $t=T$, $\\beta_T(i) = p(y_{T+1:T} \\mid s_T=i)$. Since there are no future observations, this is the probability of a certain event, so we define $\\beta_T(i) = 1$ for all $i$.\nFor $t < T$, we derive the recursion by marginalizing over the state at time $t+1$:\n$$\n\\beta_t(i) = p(y_{t+1:T} \\mid s_t=i) = \\sum_{j=1}^K p(y_{t+1:T}, s_{t+1}=j \\mid s_t=i)\n$$\nUsing the chain rule:\n$$\np(y_{t+1:T}, s_{t+1}=j \\mid s_t=i) = p(y_{t+1:T} \\mid s_{t+1}=j, s_t=i) p(s_{t+1}=j \\mid s_t=i)\n$$\nFrom HMM properties: $p(s_{t+1}=j \\mid s_t=i) = A_{ij}$, and $p(y_{t+1:T} \\mid s_{t+1}=j, s_t=i) = p(y_{t+1:T} \\mid s_{t+1}=j)$.\nWe further expand $p(y_{t+1:T} \\mid s_{t+1}=j) = p(y_{t+1}, y_{t+2:T} \\mid s_{t+1}=j) = p(y_{t+1} \\mid s_{t+1}=j) p(y_{t+2:T} \\mid s_{t+1}=j) = b_j(y_{t+1}) \\beta_{t+1}(j)$.\nThis yields the backward recursion:\n$$\n\\beta_t(i) = \\sum_{j=1}^K A_{ij} b_j(y_{t+1}) \\beta_{t+1}(j) \\quad \\text{for } t=T-1, \\dots, 1\n$$\n\n**Numerical Stabilization Strategy 1: Scaling**\nThe values of $\\alpha_t(i)$ decrease exponentially with $t$ and can cause numerical underflow. To prevent this, we introduce scaled forward variables $\\hat{\\alpha}_t(i) = p(s_t=i \\mid y_{1:t})$. These are normalized at each time step.\nThe scaling factors are $c_t = p(y_t \\mid y_{1:t-1})$. The forward pass becomes:\n1. Initialize ($t=1$): $\\alpha'_1(i) = \\pi_i b_i(y_1)$. The scaling factor is $c_1 = \\sum_j \\alpha'_1(j)$. The scaled variable is $\\hat{\\alpha}_1(i) = \\alpha'_1(i) / c_1$.\n2. Recurse ($t=2, \\dots, T$): Compute an intermediate value $\\alpha'_t(i) = b_i(y_t) \\sum_j \\hat{\\alpha}_{t-1}(j) A_{ji}$. The scaling factor is $c_t = \\sum_j \\alpha'_t(j)$. The scaled variable is $\\hat{\\alpha}_t(i) = \\alpha'_t(i) / c_t$.\n\nThe log-likelihood of the observations is $\\log p(y_{1:T}) = \\sum_{t=1}^T \\log c_t$.\n\nFor the backward pass, we define scaled backward variables $\\hat{\\beta}_t(i)$ using the same scaling factors: $\\hat{\\beta}_t(i) = \\beta_t(i) / \\prod_{k=t+1}^T c_k$.\n1. Initialize ($t=T$): $\\hat{\\beta}_T(i) = \\beta_T(i) = 1$.\n2. Recurse ($t=T-1, \\dots, 1$): The recursion for $\\hat{\\beta}_t(i)$ becomes $\\hat{\\beta}_t(i) = \\frac{1}{c_{t+1}} \\sum_{j=1}^K A_{ij} b_j(y_{t+1}) \\hat{\\beta}_{t+1}(j)$.\n\nThe posterior probability $\\gamma_t(i)$ is then computed using these scaled variables.\n$$\n\\gamma_t(i) = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_j \\alpha_t(j)\\beta_t(j)} = \\frac{\\hat{\\alpha}_t(i) \\hat{\\beta}_t(i)}{\\sum_j \\hat{\\alpha}_t(j) \\hat{\\beta}_t(j)}\n$$\nThe product term involving the scaling factors cancels out, and the posterior is obtained by normalizing the elementwise product of the scaled forward and backward variables at each time step.\n\n**Numerical Stabilization Strategy 2: Log-Domain Computation**\nAn alternative to scaling is to perform all calculations in the logarithmic domain. We define $\\ln\\alpha_t(i) = \\log \\alpha_t(i)$ and $\\ln\\beta_t(i) = \\log \\beta_t(i)$. The product operations in the original recursions become sums in the log domain. Summations are handled using the log-sum-exp transformation: $\\log(\\sum_i e^{x_i}) = X + \\log(\\sum_i e^{x_i - X})$ where $X=\\max_i x_i$. This operation is numerically stable.\n\nThe log-forward recursion is:\n1. Initialize ($t=1$): $\\ln\\alpha_1(i) = \\log\\pi_i + \\log b_i(y_1)$.\n2. Recurse ($t=2, \\dots, T$): $\\ln\\alpha_t(i) = \\log b_i(y_t) + \\underset{j}{\\text{log-sum-exp}}(\\ln\\alpha_{t-1}(j) + \\log A_{ji})$.\n\nThe log-backward recursion is:\n1. Initialize ($t=T$): $\\ln\\beta_T(i) = \\log 1 = 0$.\n2. Recurse ($t=T-1, \\dots, 1$): $\\ln\\beta_t(i) = \\underset{j}{\\text{log-sum-exp}}(\\log A_{ij} + \\log b_j(y_{t+1}) + \\ln\\beta_{t+1}(j))$.\n\nThe sequence log-likelihood is $\\log p(y_{1:T}) = \\underset{i}{\\text{log-sum-exp}}(\\ln\\alpha_T(i))$.\nThe log-posterior is computed as:\n$$\n\\log \\gamma_t(i) = \\ln\\alpha_t(i) + \\ln\\beta_t(i) - \\underset{j}{\\text{log-sum-exp}}(\\ln\\alpha_t(j) + \\ln\\beta_t(j))\n$$\nThe final posteriors $\\gamma_t(i)$ are obtained by exponentiating: $\\gamma_t(i) = e^{\\log\\gamma_t(i)}$. The emission log-probability $\\log b_i(y_t)$ for a Gaussian distribution $\\mathcal{N}(y_t \\mid \\mu_i, \\sigma_i^2)$ is computed as $-\\log\\sigma_i - \\frac{1}{2}\\log(2\\pi) - \\frac{(y_t - \\mu_i)^2}{2\\sigma_i^2}$.\n\nBoth strategies will now be implemented and verified for correctness and numerical agreement.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the HMM inference problem for the given test cases using two\n    numerically stable forward-backward algorithm implementations.\n    \"\"\"\n\n    def gaussian_log_pdf(y, mu, sigma):\n        \"\"\"\n        Computes the log of the Gaussian probability density function.\n        More stable than taking log of the pdf.\n        \"\"\"\n        return -np.log(sigma) - 0.5 * np.log(2 * np.pi) - 0.5 * ((y - mu) / sigma) ** 2\n\n    def scaled_forward_backward(pi, A, mu, sigma, y, K, T):\n        \"\"\"\n        Computes posterior state probabilities and log-likelihood using the\n        scaled forward-backward algorithm.\n        \"\"\"\n        # Pre-compute emission probabilities\n        emissions = np.zeros((T, K))\n        for t in range(T):\n            for i in range(K):\n                # We can use log-pdf and exponentiate, it is safer than pdf for tiny values\n                emissions[t, i] = np.exp(gaussian_log_pdf(y[t], mu[i], sigma[i]))\n\n        # Forward pass (scaled)\n        alpha_hat = np.zeros((T, K))\n        c = np.zeros(T)\n\n        alpha_hat_t_unscaled = pi * emissions[0, :]\n        c[0] = np.sum(alpha_hat_t_unscaled)\n        alpha_hat[0, :] = alpha_hat_t_unscaled / c[0]\n\n        for t in range(1, T):\n            alpha_hat_t_unscaled = (alpha_hat[t-1, :] @ A) * emissions[t, :]\n            c[t] = np.sum(alpha_hat_t_unscaled)\n            alpha_hat[t, :] = alpha_hat_t_unscaled / c[t]\n\n        log_likelihood = np.sum(np.log(c))\n\n        # Backward pass (scaled)\n        beta_hat = np.zeros((T, K))\n        beta_hat[T-1, :] = 1.0\n\n        for t in range(T - 2, -1, -1):\n            beta_hat[t, :] = (A @ (emissions[t+1, :] * beta_hat[t+1, :])) / c[t+1]\n\n        # Posteriors\n        gamma_unnorm = alpha_hat * beta_hat\n        gamma = gamma_unnorm / np.sum(gamma_unnorm, axis=1, keepdims=True)\n\n        return gamma.T, log_likelihood  # Return as (K, T)\n\n    def log_domain_forward_backward(pi, A, mu, sigma, y, K, T):\n        \"\"\"\n        Computes posterior state probabilities and log-likelihood using the\n        log-domain forward-backward algorithm.\n        \"\"\"\n        log_pi = np.log(pi)\n        log_A = np.log(A)\n\n        log_emissions = np.zeros((T, K))\n        for t in range(T):\n            for i in range(K):\n                log_emissions[t, i] = gaussian_log_pdf(y[t], mu[i], sigma[i])\n\n        # Log-Forward pass\n        log_alpha = np.zeros((T, K))\n        log_alpha[0, :] = log_pi + log_emissions[0, :]\n\n        for t in range(1, T):\n            for j in range(K):\n                log_alpha[t, j] = log_emissions[t, j] + logsumexp(log_alpha[t - 1, :] + log_A[:, j])\n\n        log_likelihood = logsumexp(log_alpha[T - 1, :])\n\n        # Log-Backward pass\n        log_beta = np.zeros((T, K))\n        # log_beta[T-1, :] is already 0.0\n\n        for t in range(T - 2, -1, -1):\n            for i in range(K):\n                log_beta[t, i] = logsumexp(log_A[i, :] + log_emissions[t + 1, :] + log_beta[t + 1, :])\n\n        # Log-Posteriors\n        log_gamma_unnorm = log_alpha + log_beta\n        log_normalizer = logsumexp(log_gamma_unnorm, axis=1, keepdims=True)\n        log_gamma = log_gamma_unnorm - log_normalizer\n        \n        gamma = np.exp(log_gamma)\n\n        return gamma.T, log_likelihood # Return as (K, T)\n\n    test_cases = [\n        {\n            \"K\": 2, \"T\": 20,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([[0.95, 0.05], [0.04, 0.96]]),\n            \"mu\": np.array([0.2, 0.8]), \"sigma\": np.array([0.05, 0.05]),\n            \"y\": np.array([0.18, 0.22, 0.19, 0.81, 0.79, 0.82, 0.21, 0.20, 0.78, 0.83, 0.18, 0.17, 0.82, 0.80, 0.22, 0.19, 0.77, 0.84, 0.23, 0.20]),\n        },\n        {\n            \"K\": 3, \"T\": 1,\n            \"pi\": np.array([0.2, 0.5, 0.3]),\n            \"A\": np.array([[0.90, 0.05, 0.05], [0.10, 0.80, 0.10], [0.05, 0.15, 0.80]]),\n            \"mu\": np.array([0.1, 0.5, 0.9]), \"sigma\": np.array([0.1, 0.05, 0.1]),\n            \"y\": np.array([0.52]),\n        },\n        {\n            \"K\": 2, \"T\": 120,\n            \"pi\": np.array([0.5, 0.5]),\n            \"A\": np.array([[0.995, 0.005], [0.003, 0.997]]),\n            \"mu\": np.array([0.2, 0.8]), \"sigma\": np.array([0.1, 0.1]),\n            \"y\": np.concatenate([np.full(60, 0.45), np.full(60, 0.55)]),\n        }\n    ]\n\n    epsilon = 1e-8\n    delta = 1e-12\n    eta = 1e-8\n\n    results = []\n    \n    for case in test_cases:\n        K, T = case[\"K\"], case[\"T\"]\n        pi, A, mu, sigma, y = case[\"pi\"], case[\"A\"], case[\"mu\"], case[\"sigma\"], case[\"y\"]\n\n        # Suppress warnings for log(0) which is handled correctly as -inf\n        with np.errstate(divide='ignore'):\n            gamma_scaled, ll_scaled = scaled_forward_backward(pi, A, mu, sigma, y, K, T)\n            gamma_log, ll_log = log_domain_forward_backward(pi, A, mu, sigma, y, K, T)\n\n        # 1. Posterior agreement check\n        posterior_diff = np.max(np.abs(gamma_scaled - gamma_log))\n        check1 = posterior_diff < epsilon\n       \n        # 2. Per-time posterior normalization check\n        # We only need to check one result as check1 ensures they are close\n        normalization_diffs = np.abs(np.sum(gamma_log, axis=0) - 1.0)\n        check2 = np.all(normalization_diffs <= delta)\n\n        # 3. Log-likelihood agreement check\n        ll_diff = np.abs(ll_scaled - ll_log)\n        check3 = ll_diff < eta\n\n        results.append(check1 and check2 and check3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}