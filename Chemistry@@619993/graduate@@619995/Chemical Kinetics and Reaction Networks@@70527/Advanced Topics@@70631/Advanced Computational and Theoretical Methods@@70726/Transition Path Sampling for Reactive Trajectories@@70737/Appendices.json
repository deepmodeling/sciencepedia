{"hands_on_practices": [{"introduction": "This first practice problem focuses on the central calculation within Transition Interface Sampling (TIS). You will translate raw simulation outputs—the flux across an initial interface and the conditional crossing probabilities between subsequent interfaces—into the macroscopic rate constant $k_{AB}$. This exercise reinforces the foundational TIS rate expression and provides essential practice in error propagation, a critical skill for assessing the reliability of any simulation result. [@problem_id:2690077]", "problem": "A bistable chemical system has reactant basin $A$ and product basin $B$. You perform Transition Interface Sampling (TIS) across a series of non-intersecting interfaces $\\{\\lambda_0,\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_B\\}$ that monotonically separate $A$ from $B$ along an order parameter $\\lambda$. The Transition Interface Sampling (TIS) framework defines the forward rate constant $k_{AB}$ as the flux of first crossings of the innermost interface $\\lambda_0$ by trajectories leaving $A$ times the probability that such a first-crossing trajectory ultimately reaches $B$ before returning to $A$.\n\nFrom independent simulations you obtain the following estimates with their one-standard-deviation uncertainties (assume all estimates are unbiased, Gaussian, and mutually independent):\n- First-crossing flux at $\\lambda_0$: $\\Phi_{A,\\lambda_0} = 2.50 \\times 10^{-3} \\ \\mathrm{s}^{-1}$ with standard uncertainty $0.20 \\times 10^{-3} \\ \\mathrm{s}^{-1}$.\n- Conditional interface-to-interface crossing probabilities: $P(\\lambda_{1}\\mid \\lambda_0) = 0.45 \\pm 0.03$, $P(\\lambda_{2}\\mid \\lambda_1) = 0.38 \\pm 0.02$, and $P(\\lambda_{B}\\mid \\lambda_2) = 0.22 \\pm 0.015$.\n\nStarting from the stated definition of the rate in Transition Interface Sampling (TIS) and fundamental probability rules, do the following:\n1. Derive an explicit expression for $k_{AB}$ in terms of $\\Phi_{A,\\lambda_0}$ and the conditional probabilities $P(\\lambda_{i+1}\\mid \\lambda_i)$.\n2. Compute the central estimate of $k_{AB}$ using the provided numerical values.\n3. Using the first-order delta method for uncertainty propagation from independent inputs, derive and compute the standard uncertainty of $k_{AB}$.\n\nExpress intermediate symbolic steps clearly. For grading, report only the central estimate of $k_{AB}$, rounded to four significant figures, in units of $\\mathrm{s}^{-1}$.", "solution": "The problem presented is a valid application of the principles of Transition Interface Sampling (TIS) for the calculation of a rate constant, and it is well-posed. We shall proceed with its solution.\n\nThe problem asks for three items: the derivation of the rate constant expression, its central value, and the associated uncertainty. We will address these in order.\n\nFirst, we derive the expression for the rate constant $k_{AB}$. The problem defines the rate constant as the product of the flux out of the reactant basin $A$ across the first interface $\\lambda_0$, and the probability that a trajectory making this first crossing will eventually reach the product basin $B$ before returning to $A$. This is expressed as:\n$$k_{AB} = \\Phi_{A,\\lambda_0} \\times P(B \\mid A \\to \\lambda_0)$$\nwhere $\\Phi_{A,\\lambda_0}$ is the first-crossing flux and $P(B \\mid A \\to \\lambda_0)$ is the conditional probability of reaching basin $B$ given a trajectory has just crossed $\\lambda_0$ from $A$.\n\nThe TIS methodology employs a series of non-intersecting, ordered interfaces that define a path from $A$ to $B$. In this problem, the path is defined by the sequence of interfaces $\\{\\lambda_0, \\lambda_1, \\lambda_2, \\lambda_B\\}$. For a trajectory to proceed from $\\lambda_0$ to $B$, it must successively cross $\\lambda_1$ and $\\lambda_2$ without returning to $A$. The total probability $P(B \\mid A \\to \\lambda_0)$ can be decomposed using the chain rule of probability. Let the event of a trajectory reaching interface $\\lambda_i$ before returning to $A$, having come from $\\lambda_{i-1}$, be denoted $E_i$. We are interested in the probability of reaching $B$ (event $E_B$) given we have crossed $\\lambda_0$ (event $E_0$).\n$$P(B \\mid A \\to \\lambda_0) = P(E_B \\mid E_0)$$\nSince the interfaces are ordered, the event of reaching $E_B$ presupposes the events of reaching $E_2$ and $E_1$. The chain rule gives:\n$$P(E_B \\mid E_0) = P(E_1 \\mid E_0) \\times P(E_2 \\mid E_1 \\cap E_0) \\times P(E_B \\mid E_2 \\cap E_1 \\cap E_0)$$\nA fundamental assumption in TIS is that the crossing process is Markovian with respect to the interfaces. This means the probability of crossing the next interface $\\lambda_{i+1}$ depends only on the fact that the trajectory has reached $\\lambda_i$, not on its prior history. Therefore, $P(E_2 \\mid E_1 \\cap E_0) = P(E_2 \\mid E_1)$ and $P(E_B \\mid E_2 \\cap E_1 \\cap E_0) = P(E_B \\mid E_2)$.\nThe probabilities provided in the problem statement are precisely these conditional probabilities: $P(\\lambda_{i+1} \\mid \\lambda_i)$. Thus, the total probability is the product:\n$$P(B \\mid A \\to \\lambda_0) = P(\\lambda_1 \\mid \\lambda_0) \\times P(\\lambda_2 \\mid \\lambda_1) \\times P(\\lambda_B \\mid \\lambda_2)$$\nSubstituting this into the rate expression, we arrive at the explicit formula for $k_{AB}$:\n$$k_{AB} = \\Phi_{A,\\lambda_0} \\times P(\\lambda_1 \\mid \\lambda_0) \\times P(\\lambda_2 \\mid \\lambda_1) \\times P(\\lambda_B \\mid \\lambda_2)$$\nThis completes the first part of the task.\n\nSecond, we compute the central estimate of $k_{AB}$ using the provided values:\n$\\Phi_{A,\\lambda_0} = 2.50 \\times 10^{-3} \\ \\mathrm{s}^{-1}$\n$P(\\lambda_1 \\mid \\lambda_0) = 0.45$\n$P(\\lambda_2 \\mid \\lambda_1) = 0.38$\n$P(\\lambda_B \\mid \\lambda_2) = 0.22$\n\nSubstituting these values into our derived expression:\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times (0.45) \\times (0.38) \\times (0.22)$$\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times (0.171) \\times (0.22)$$\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times 0.03762$$\n$$k_{AB} = 0.09405 \\times 10^{-3} \\ \\mathrm{s}^{-1} = 9.405 \\times 10^{-5} \\ \\mathrm{s}^{-1}$$\nThe central estimate for the rate constant is $9.405 \\times 10^{-5} \\ \\mathrm{s}^{-1}$.\n\nThird, we derive and compute the standard uncertainty of $k_{AB}$, denoted $\\sigma_{k_{AB}}$. The problem specifies using the first-order delta method for independent inputs. Let the function for the rate constant be $k_{AB} = f(x_1, x_2, x_3, x_4)$, where $x_1 = \\Phi_{A,\\lambda_0}$, $x_2 = P(\\lambda_1 \\mid \\lambda_0)$, $x_3 = P(\\lambda_2 \\mid \\lambda_1)$, and $x_4 = P(\\lambda_B \\mid \\lambda_2)$. The variance, $\\sigma_{k_{AB}}^2$, is given by:\n$$\\sigma_{k_{AB}}^2 \\approx \\sum_{i=1}^{4} \\left( \\frac{\\partial f}{\\partial x_i} \\right)^2 \\sigma_{x_i}^2$$\nThe partial derivatives are:\n$\\frac{\\partial k_{AB}}{\\partial x_1} = x_2 x_3 x_4 = \\frac{k_{AB}}{x_1}$\n$\\frac{\\partial k_{AB}}{\\partial x_2} = x_1 x_3 x_4 = \\frac{k_{AB}}{x_2}$\n$\\frac{\\partial k_{AB}}{\\partial x_3} = x_1 x_2 x_4 = \\frac{k_{AB}}{x_3}$\n$\\frac{\\partial k_{AB}}{\\partial x_4} = x_1 x_2 x_3 = \\frac{k_{AB}}{x_4}$\n\nSubstituting these into the variance formula yields:\n$$\\sigma_{k_{AB}}^2 \\approx \\left(\\frac{k_{AB}}{x_1}\\right)^2 \\sigma_{x_1}^2 + \\left(\\frac{k_{AB}}{x_2}\\right)^2 \\sigma_{x_2}^2 + \\left(\\frac{k_{AB}}{x_3}\\right)^2 \\sigma_{x_3}^2 + \\left(\\frac{k_{AB}}{x_4}\\right)^2 \\sigma_{x_4}^2$$\nDividing by $k_{AB}^2$ gives the expression for the squared relative uncertainty:\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx \\left(\\frac{\\sigma_{x_1}}{x_1}\\right)^2 + \\left(\\frac{\\sigma_{x_2}}{x_2}\\right)^2 + \\left(\\frac{\\sigma_{x_3}}{x_3}\\right)^2 + \\left(\\frac{\\sigma_{x_4}}{x_4}\\right)^2$$\nNow we compute the numerical values for the relative uncertainties of the inputs:\n$\\frac{\\sigma_{x_1}}{x_1} = \\frac{0.20 \\times 10^{-3}}{2.50 \\times 10^{-3}} = 0.08$\n$\\frac{\\sigma_{x_2}}{x_2} = \\frac{0.03}{0.45} = \\frac{1}{15} \\approx 0.06667$\n$\\frac{\\sigma_{x_3}}{x_3} = \\frac{0.02}{0.38} = \\frac{1}{19} \\approx 0.05263$\n$\\frac{\\sigma_{x_4}}{x_4} = \\frac{0.015}{0.22} = \\frac{3}{44} \\approx 0.06818$\n\nSquaring and summing these values:\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx (0.08)^2 + \\left(\\frac{1}{15}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + \\left(\\frac{3}{44}\\right)^2$$\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx 0.0064 + 0.004444... + 0.002770... + 0.004648... \\approx 0.018263$$\nThe relative uncertainty is the square root of this sum:\n$$\\frac{\\sigma_{k_{AB}}}{k_{AB}} \\approx \\sqrt{0.018263} \\approx 0.1351$$\nFinally, the absolute uncertainty $\\sigma_{k_{AB}}$ is:\n$$\\sigma_{k_{AB}} \\approx 0.1351 \\times k_{AB} = 0.1351 \\times (9.405 \\times 10^{-5}) \\approx 1.27 \\times 10^{-5} \\ \\mathrm{s}^{-1}$$\nThe full result is $k_{AB} = (9.405 \\pm 1.27) \\times 10^{-5} \\ \\mathrm{s}^{-1}$. However, the problem instructions for the final answer specify only the central estimate, rounded to four significant figures. The calculated value $9.405 \\times 10^{-5}$ already has four significant figures.", "answer": "$$\\boxed{9.405 \\times 10^{-5}}$$", "id": "2690077"}, {"introduction": "A successful transition path simulation is not just correct, but also efficient. This exercise explores the practical challenge of optimizing interface placement to minimize statistical error for a given computational budget. By analyzing how the variance of the rate estimate depends on the stage-wise probabilities, you will derive the principle behind adaptive algorithms that seek to equalize these probabilities, a key strategy for making rare event simulations tractable. [@problem_id:2690120]", "problem": "A rare reactive event $A \\to B$ is studied by an interface-based transition path method such as Forward Flux Sampling (FFS) or Transition Interface Sampling (TIS). Let $\\lambda(\\mathbf{x})$ be a monotonic order parameter that increases from basin $A$ to $B$, and consider a sequence of interfaces $A \\equiv \\{\\lambda \\le \\lambda_{0}\\} \\prec \\{\\lambda=\\lambda_{1}\\} \\prec \\cdots \\prec \\{\\lambda=\\lambda_{M}\\} \\equiv \\{\\lambda \\ge \\lambda_{B}\\}$ with $\\lambda_{0}\\lambda_{1}\\cdots\\lambda_{M}=\\lambda_{B}$. At each interface $\\lambda_{i}$, $N_{i}$ trial trajectories are fired and terminated when they either reach $\\lambda_{i+1}$ (success) or return to $A$ (failure), producing an estimator $\\hat{p}_{i}$ of the conditional probability $p_{i}\\equiv P(\\lambda_{i+1}\\mid \\lambda_{i})$. The overall crossing probability is $P_{A\\to B}=\\prod_{i=0}^{M-1}p_{i}$, and the rate is $k_{AB}=\\Phi_{A0}\\,P_{A\\to B}$, where $\\Phi_{A0}$ is the steady-state flux of trajectories leaving $A$ and crossing $\\lambda_{0}$. Assume a simple cost model where each trial trajectory cost is approximately constant across interfaces, and that, to leading order, interface estimators are statistically independent with binomial fluctuations at each interface.\n\nWhich option best describes a scientifically sound adaptive algorithm to place the interfaces so that $P(\\lambda_{i+1}\\mid \\lambda_{i})$ is approximately constant across $i$, and provides a correct first-principles justification for the expected reduction in estimator variance under a fixed total computational budget?\n\nA. Start from $\\lambda_{0}$ and choose a target success probability $p^{\\star}\\in(0,1)$ (e.g., $p^{\\star}\\approx 0.2$). For each $i$, use short pilot batches of $K$ trials from $\\lambda_{i}$ to bracket two candidate positions $\\lambda_{i+1}^{-}$ and $\\lambda_{i+1}^{+}$ such that the observed success fractions straddle $p^{\\star}$, then apply a bisection search in $\\lambda$ with fresh pilot trials until the estimated $\\hat{p}_{i}\\approx p^{\\star}$ within a tolerance. Repeat to construct $\\lambda_{i+1}$, $i\\leftarrow i+1$, until $\\lambda_{M}=\\lambda_{B}$. In production, allocate $N_{i}$ roughly equally across interfaces (or according to small adjustments if per-interface costs differ). Justification: with binomial variance $\\mathrm{Var}(\\hat{p}_{i})=p_{i}(1-p_{i})/N_{i}$ and independence, a delta-method expansion gives the relative variance $\\mathrm{Var}(\\hat{P}_{A\\to B})/P_{A\\to B}^{2}\\approx \\sum_{i}(1-p_{i})/(p_{i}N_{i})$. For fixed $\\prod_{i}p_{i}$ and fixed $\\{N_{i}\\}$, this sum is minimized when all $p_{i}$ are equal, so adaptively enforcing $p_{i}\\approx p^{\\star}$ reduces variance for a fixed total cost.\n\nB. Place interfaces uniformly in $\\lambda$: $\\lambda_{i}=\\lambda_{0}+i(\\lambda_{B}-\\lambda_{0})/M$. During sampling, adapt $N_{i}$ so that each interface achieves the same number of successful trials $N_{i} \\hat{p}_{i}$, which equalizes information content and therefore minimizes variance. Justification: equal successes imply balanced contributions to the product, which is optimal.\n\nC. First compute a potential of mean force $F(\\lambda)$ and then place interfaces so that the free energy increments $\\Delta F_{i}=F(\\lambda_{i+1})-F(\\lambda_{i})$ are equal. This partitions the barrier evenly, making $p_{i}$ implicitly uniform and minimizing variance because the barrier is sampled uniformly.\n\nD. Make interfaces very sparse so that each $p_{i}\\ll 1$ and then compensate by choosing very large $N_{i}$ at those interfaces. Because each $\\mathrm{Var}(\\hat{p}_{i})$ then scales as $p_{i}/N_{i}$, driving $N_{i}$ large reduces the variance more efficiently than adding more interfaces.\n\nE. Estimate the committor $q(\\mathbf{x})\\equiv P(B\\ \\text{before}\\ A\\mid \\mathbf{x})$ on the fly and place interfaces at committor quantiles $q_{i}$ so that $q_{i+1}-q_{i}$ is constant. Because the committor changes linearly between interfaces, the conditional probabilities $p_{i}$ are equal and the variance is minimized without further justification.", "solution": "The problem asks for an evaluation of different adaptive algorithms for placing interfaces in transition path sampling methods like Forward Flux Sampling (FFS), with the goal of minimizing the statistical error in the estimated rate constant for a fixed computational cost. The core idea is to understand how the placement of interfaces, which determines the conditional probabilities $p_i$, and the allocation of computational effort, $N_i$, affect the overall variance of the estimator for the total crossing probability, $P_{A\\to B}$.\n\nFirst, let us establish the fundamental principles. The problem states that the total crossing probability is given by the product of conditional probabilities:\n$$P_{A\\to B} = \\prod_{i=0}^{M-1} p_i$$\nwhere $p_i = P(\\lambda_{i+1} \\mid \\lambda_i)$ is the probability that a trajectory initiated at interface $i$ will reach interface $i+1$ before returning to state $A$. The estimator for this probability is $\\hat{P}_{A\\to B} = \\prod_{i=0}^{M-1} \\hat{p}_i$.\n\nThe key to analyzing the statistical error is to calculate the relative variance of this estimator, $\\mathrm{Var}(\\hat{P}_{A\\to B}) / P_{A\\to B}^2$. For small errors, this can be approximated by the variance of the logarithm of the estimator. This is a standard application of the delta method.\n$$ \\frac{\\mathrm{Var}(\\hat{P}_{A\\to B})}{P_{A\\to B}^2} \\approx \\mathrm{Var}(\\ln \\hat{P}_{A\\to B}) = \\mathrm{Var}\\left(\\sum_{i=0}^{M-1} \\ln \\hat{p}_i\\right) $$\nThe problem states that the estimators $\\hat{p}_i$ for different interfaces are statistically independent. Therefore, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}\\left(\\sum_{i=0}^{M-1} \\ln \\hat{p}_i\\right) = \\sum_{i=0}^{M-1} \\mathrm{Var}(\\ln \\hat{p}_i) $$\nUsing the delta method again for each term, $\\mathrm{Var}(\\ln \\hat{p}_i) \\approx \\mathrm{Var}(\\hat{p}_i) / p_i^2$.\nThe problem assumes binomial fluctuations for the success/failure of the $N_i$ trials at each interface. The estimator $\\hat{p}_i$ is the fraction of successful trials, so its variance is given by the binomial variance:\n$$ \\mathrm{Var}(\\hat{p}_i) = \\frac{p_i(1 - p_i)}{N_i} $$\nCombining these results gives the expression for the relative variance of the total probability estimator:\n$$ \\mathcal{V} \\equiv \\frac{\\mathrm{Var}(\\hat{P}_{A\\to B})}{P_{A\\to B}^2} \\approx \\sum_{i=0}^{M-1} \\frac{\\mathrm{Var}(\\hat{p}_i)}{p_i^2} = \\sum_{i=0}^{M-1} \\frac{p_i(1 - p_i)/N_i}{p_i^2} = \\sum_{i=0}^{M-1} \\frac{1 - p_i}{p_i N_i} $$\nThe optimization problem is to minimize this variance $\\mathcal{V}$ subject to a fixed total computational cost. Assuming the cost per trial is constant, this is equivalent to fixing the total number of trials, $N_{tot} = \\sum_{i=0}^{M-1} N_i$. The choice of interface positions determines the set of probabilities $\\{p_i\\}$, constrained by $\\prod_{i=0}^{M-1} p_i = P_{A\\to B}$.\n\nThe most straightforward strategy, and the one implicitly suggested by some options, is to allocate the computational budget evenly, such that $N_i = N_{tot}/M$ for all $i$. In this case, the variance becomes:\n$$ \\mathcal{V} \\approx \\frac{M}{N_{tot}} \\sum_{i=0}^{M-1} \\frac{1 - p_i}{p_i} = \\frac{M}{N_{tot}} \\left( \\sum_{i=0}^{M-1} \\frac{1}{p_i} - M \\right) $$\nTo minimize this expression for a fixed $M$, we must minimize the sum $\\sum_{i=0}^{M-1} 1/p_i$, subject to the constraint that the product $\\prod_{i=0}^{M-1} p_i = P_{A\\to B}$ is constant. This is a classic optimization problem that can be solved using Lagrange multipliers. Let $f(\\{p_i\\}) = \\sum 1/p_i$ and the constraint be $g(\\{p_i\\}) = \\prod p_i - P_{A\\to B} = 0$. The condition $\\nabla f = \\mu \\nabla g$ leads to $-\\frac{1}{p_k^2} = \\mu \\frac{\\prod p_i}{p_k}$, which implies that $1/p_k$ is constant for all $k$. Thus, the minimum is achieved when all probabilities are equal: $p_i = p = (P_{A\\to B})^{1/M}$.\n\nTherefore, the optimal strategy for placing interfaces under the assumption of equal $N_i$ is to choose them such that all conditional probabilities $p_i$ are made equal.\n\nNow, we evaluate each option based on this derivation.\n\n**A. Start from $\\lambda_{0}$ and choose a target success probability $p^{\\star}\\in(0,1)$ (e.g., $p^{\\star}\\approx 0.2$). For each $i$, use short pilot batches of $K$ trials from $\\lambda_{i}$ to bracket two candidate positions $\\lambda_{i+1}^{-}$ and $\\lambda_{i+1}^{+}$ such that the observed success fractions straddle $p^{\\star}$, then apply a bisection search in $\\lambda$ with fresh pilot trials until the estimated $\\hat{p}_{i}\\approx p^{\\star}$ within a tolerance. Repeat to construct $\\lambda_{i+1}$, $i\\leftarrow i+1$, until $\\lambda_{M}=\\lambda_{B}$. In production, allocate $N_{i}$ roughly equally across interfaces (or according to small adjustments if per-interface costs differ). Justification: with binomial variance $\\mathrm{Var}(\\hat{p}_{i})=p_{i}(1-p_{i})/N_{i}$ and independence, a delta-method expansion gives the relative variance $\\mathrm{Var}(\\hat{P}_{A\\to B})/P_{A\\to B}^{2}\\approx \\sum_{i}(1-p_{i})/(p_{i}N_{i})$. For fixed $\\prod_{i}p_{i}$ and fixed $\\{N_{i}\\}$, this sum is minimized when all $p_{i}$ are equal, so adaptively enforcing $p_{i}\\approx p^{\\star}$ reduces variance for a fixed total cost.**\n\nThis option proposes an adaptive algorithm that directly aims to make all $p_i$ equal to a target value $p^\\star$. The described procedure using pilot runs and a bisection search is a practical and robust way to achieve this. The justification provided is precisely the one derived above: it correctly states the formula for the relative variance and asserts that for fixed $\\{N_i\\}$, the minimum is achieved when all $p_i$ are equal. This is a correct statement. Therefore, both the proposed algorithm and its justification are sound. **Correct**.\n\n**B. Place interfaces uniformly in $\\lambda$: $\\lambda_{i}=\\lambda_{0}+i(\\lambda_{B}-\\lambda_{0})/M$. During sampling, adapt $N_{i}$ so that each interface achieves the same number of successful trials $N_{i} \\hat{p}_{i}$, which equalizes information content and therefore minimizes variance. Justification: equal successes imply balanced contributions to the product, which is optimal.**\n\nThis option is flawed. Firstly, placing interfaces uniformly in $\\lambda$ is a naive, non-adaptive strategy that will typically result in highly non-uniform probabilities $p_i$. Secondly, the proposal to adapt $N_i$ such that $N_i p_i$ is constant is not optimal. The optimal allocation of $N_i$ for a given set of $\\{p_i\\}$ is $N_i \\propto \\sqrt{(1-p_i)/p_i}$, not $N_i \\propto 1/p_i$. The justification provided is vague (\"equalizes information content\") and incorrect. As shown in the thought process, this strategy actually maximizes a term related to the variance, making it suboptimal. **Incorrect**.\n\n**C. First compute a potential of mean force $F(\\lambda)$ and then place interfaces so that the free energy increments $\\Delta F_{i}=F(\\lambda_{i+1})-F(\\lambda_{i})$ are equal. This partitions the barrier evenly, making $p_{i}$ implicitly uniform and minimizing variance because the barrier is sampled uniformly.**\n\nThis strategy is inefficient and not generally correct. Computing the potential of mean force (PMF) across a high barrier is often more computationally expensive than the rare event calculation itself, defeating the purpose of FFS. More importantly, the assumption that equal free energy increments $\\Delta F_i$ lead to equal conditional probabilities $p_i$ is a strong oversimplification. The probability $p_i$ depends on the dynamics and the full shape of the free energy surface, specifically the competition between moving forward to $\\lambda_{i+1}$ and returning to state $A$. The justification is hand-waving and lacks rigor. **Incorrect**.\n\n**D. Make interfaces very sparse so that each $p_{i}\\ll 1$ and then compensate by choosing very large $N_{i}$ at those interfaces. Because each $\\mathrm{Var}(\\hat{p}_{i})$ then scales as $p_{i}/N_{i}$, driving $N_{i}$ large reduces the variance more efficiently than adding more interfaces.**\n\nThis approach is fundamentally misguided. The entire purpose of interface-based methods is to break down a single, very rare event (with probability $P_{A\\to B} \\ll 1$) into a series of more frequent events (with probabilities $p_i$ that are not excessively small). For a very small $p_i$, one must run an extremely large number of trials $N_i \\gg 1/p_i$ just to observe a few successes. This leads to a catastrophic loss of efficiency. The relative variance $\\mathcal{V} \\approx \\sum_i 1/(p_i N_i)$ demonstrates that small $p_i$ values are extremely costly in terms of variance. Adding more interfaces to raise the individual $p_i$ values is vastly more efficient than trying to overcome a small $p_i$ with a brute-force increase in $N_i$. **Incorrect**.\n\n**E. Estimate the committor $q(\\mathbf{x})\\equiv P(B\\ \\text{before}\\ A\\mid \\mathbf{x})$ on the fly and place interfaces at committor quantiles $q_{i}$ so that $q_{i+1}-q_{i}$ is constant. Because the committor changes linearly between interfaces, the conditional probabilities $p_{i}$ are equal and the variance is minimized without further justification.**\n\nThis option contains several theoretical and practical issues. While the committor $q(\\mathbf{x})$ is indeed the ideal reaction coordinate, it is generally unknown. Estimating it accurately is typically as difficult as the original rate calculation problem, making the suggestion circular. Furthermore, the claim that \"the committor changes linearly between interfaces\" is false; the committor is a complex, non-linear function of the system's coordinates. Finally, even if one could define interfaces as level sets of the true committor, the relationship between these interfaces and the FFS conditional probabilities $p_i$ is not as simple as implied, and it does not automatically guarantee that the $p_i$ are equal. The option provides no valid justification for variance minimization. **Incorrect**.\n\nIn summary, option A correctly describes a standard, practical adaptive algorithm for interface placement in FFS and provides a rigorous and correct justification for its effectiveness based on first-principles statistical analysis.", "answer": "$$\\boxed{A}$$", "id": "2690120"}, {"introduction": "The validity of any path sampling result hinges on a correct definition of the reactant and product states. This problem confronts a subtle but crucial aspect of modeling: the role of momentum in systems with inertia, such as those described by underdamped Langevin dynamics. By considering the implications of defining basins in configuration space versus phase space, you will appreciate why the committor function $q(x,p)$ is the rigorous tool for identifying true reactive trajectories and avoiding sampling artifacts. [@problem_id:2690116]", "problem": "Consider underdamped Langevin dynamics for a single particle of mass $m$ in $d$ spatial dimensions moving in a smooth bistable potential $U(x)$ at temperature $T$. The equations of motion are\n$$\n\\dot{x} = \\frac{p}{m},\\quad \\dot{p} = -\\nabla U(x) - \\gamma\\,\\frac{p}{m} + \\sqrt{2\\,\\gamma\\,k_{\\mathrm{B}}T}\\,\\eta(t),\n$$\nwhere $x \\in \\mathbb{R}^d$, $p \\in \\mathbb{R}^d$, $\\gamma0$ is a friction coefficient, $k_{\\mathrm{B}}$ is the Boltzmann constant, and $\\eta(t)$ is standard white noise with independent components. The stationary distribution is the Boltzmann distribution on phase space with density proportional to $\\exp\\{-\\beta[U(x)+\\|p\\|^2/(2m)]\\}$, where $\\beta=(k_{\\mathrm{B}}T)^{-1}$. You wish to perform Transition Path Sampling (TPS) to sample reactive trajectories between two metastable basins associated with the two wells.\n\nTwo possible ways to define the basins are:\n- Configuration-space basins: $A_x=\\{x:\\|x-x_A\\|\\varepsilon\\}$ and $B_x=\\{x:\\|x-x_B\\|\\varepsilon\\}$ for small $\\varepsilon0$, ignoring momentum altogether.\n- Phase-space basins: subsets $A,B \\subset \\mathbb{R}^{2d}$ that are disjoint neighborhoods of the two metastable states in phase space, for example defined in terms of the committor function $q$ as $A=\\{(x,p):q(x,p)\\le \\delta\\}$ and $B=\\{(x,p):q(x,p)\\ge 1-\\delta\\}$ with small $\\delta \\in (0,1/2)$, where $q(x,p)$ is the probability to reach $B$ before $A$ starting from $(x,p)$.\n\nAssume the dynamics are reversible with respect to the stationary measure, and let the time-reversal involution be $\\theta(x,p)=(x,-p)$. In particular, you may assume the detailed-balance property holds and that $B=\\theta A$ when basins are defined in phase space.\n\nWhich of the following statements about how including momentum in the definition of basins affects the identification and sampling of reactive trajectories is/are correct?\n\nA. For underdamped Langevin dynamics, the process is Markovian in phase space $(x,p)$ but not Markovian in configuration space $x$ alone; therefore the committor must be defined as $q(x,p)$, and basins defined only in $x$ generally lead to spurious recrossings when identifying reactive trajectories.\n\nB. Defining basins in phase space via $A=\\{(x,p):q(x,p)\\le \\delta\\}$ and $B=\\{(x,p):q(x,p)\\ge 1-\\delta\\}$ for small $\\delta$ makes the identification of reactive trajectories unambiguous and ensures time-reversal symmetry in the transition path ensemble when $B=\\theta A$ with $\\theta(x,p)=(x,-p)$.\n\nC. Including momentum in the definition of basins necessarily lowers the acceptance probability of standard two-way shooting moves in Transition Path Sampling because the phase-space basins are smaller than their configuration-space counterparts.\n\nD. For reversible underdamped Langevin dynamics at equilibrium, if $B=\\theta A$, then the committor satisfies $q(x,-p)=1-q(x,p)$, and a crossing of a configuration-space dividing surface at which $q(x,p)1/2$ should not be counted as a reactive event regardless of the sign of the velocity.\n\nE. The exact equilibrium rate constant computed from Transition Path Sampling is invariant to whether basins are defined in configuration space or in phase space, provided both choices lead to metastable, disjoint sets representing the same chemical species; the primary effect of including momentum is to reduce misclassification and recrossings, improving estimator efficiency rather than changing the true rate.\n\nSelect all that apply.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The dynamics of a single particle of mass $m$ in $d$ dimensions are described by underdamped Langevin equations:\n$$\n\\dot{x} = \\frac{p}{m}\n$$\n$$\n\\dot{p} = -\\nabla U(x) - \\gamma\\,\\frac{p}{m} + \\sqrt{2\\,\\gamma\\,k_{\\mathrm{B}}T}\\,\\eta(t)\n$$\n- $x \\in \\mathbb{R}^d$ is the position, $p \\in \\mathbb{R}^d$ is the momentum.\n- $U(x)$ is a smooth bistable potential.\n- $\\gamma0$ is a friction coefficient.\n- $T$ is the temperature, $k_{\\mathrm{B}}$ is the Boltzmann constant.\n- $\\eta(t)$ is standard white noise with independent components.\n- The stationary distribution has a density proportional to $\\exp\\{-\\beta[U(x)+\\|p\\|^2/(2m)]\\}$ with $\\beta=(k_{\\mathrm{B}}T)^{-1}$.\n- The goal is to use Transition Path Sampling (TPS) to sample reactive trajectories between two metastable basins.\n- Two types of basin definitions are proposed:\n    1.  Configuration-space basins: $A_x=\\{x:\\|x-x_A\\|\\varepsilon\\}$ and $B_x=\\{x:\\|x-x_B\\|\\varepsilon\\}$.\n    2.  Phase-space basins: $A=\\{(x,p):q(x,p)\\le \\delta\\}$ and $B=\\{(x,p):q(x,p)\\ge 1-\\delta\\}$, using the committor function $q(x,p)$.\n- The committor $q(x,p)$ is the probability to reach $B$ before $A$ starting from $(x,p)$.\n- The dynamics are assumed to be reversible with respect to the stationary measure.\n- The time-reversal involution is $\\theta(x,p)=(x,-p)$.\n- The detailed-balance property holds.\n- For phase-space basins, it is assumed that $B=\\theta A$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. The underdamped Langevin equation is a standard model in statistical physics for describing particle motion in a thermal environment where inertial effects are significant. Transition Path Sampling (TPS) is a well-established computational method for studying rare events like chemical reactions. The definitions of basins in both configuration space and phase space are standard in the literature. The properties of the committor function and the implications of time-reversal symmetry are central concepts in the theory of rare events. The problem does not violate any fundamental principles, is self-contained, and uses precise, unambiguous terminology. All provided information is consistent and relevant to the question asked.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed to analyze each option.\n\n**Option-by-Option Analysis**\n\n**A. For underdamped Langevin dynamics, the process is Markovian in phase space $(x,p)$ but not Markovian in configuration space $x$ alone; therefore the committor must be defined as $q(x,p)$, and basins defined only in $x$ generally lead to spurious recrossings when identifying reactive trajectories.**\n\nThe state of the system at time $t$ is fully described by the pair $(x(t), p(t))$. The Langevin equations show that the time derivatives $\\dot{x}$ and $\\dot{p}$ depend only on the current state $(x(t), p(t))$ and the stochastic noise term $\\eta(t)$. This is the definition of a Markov process in the phase space $\\mathbb{R}^{2d}$.\nHowever, if we consider only the position $x(t)$, its future evolution depends not only on $x(t)$ but also on its time derivative, $\\dot{x}(t) = p(t)/m$. Since the momentum $p(t)$ is not included in the description of the state, the process $x(t)$ is not Markovian. The momentum acts as a memory variable.\nBecause the process is Markovian only in the full phase space, the committor function, which is defined for a Markov process, must be a function of the state variables, i.e., $q(x,p)$. A committor defined as $q(x)$ would be an approximation, valid only in the high-friction (overdamped) limit where momentum equilibrates instantaneously and can be integrated out.\nWhen basins are defined solely by position (e.g., $A_x, B_x$), a trajectory can enter a basin, for example $B_x$, but possess a large momentum pointing back towards $A_x$. Consequently, it might immediately leave $B_x$ without equilibrating, leading to a short, unsuccessful crossing event. These events are known as recrossings. Properly defining basins in phase space using the committor $q(x,p)$ largely eliminates this artifact, as a state $(x,p)$ is only considered to be in basin $B$ if its probability of returning to $A$ is negligible. Thus, the statement is a correct summary of fundamental issues in modeling rare events in underdamped systems.\n\nVerdict: **Correct**.\n\n**B. Defining basins in phase space via $A=\\{(x,p):q(x,p)\\le \\delta\\}$ and $B=\\{(x,p):q(x,p)\\ge 1-\\delta\\}$ for small $\\delta$ makes the identification of reactive trajectories unambiguous and ensures time-reversal symmetry in the transition path ensemble when $B=\\theta A$ with $\\theta(x,p)=(x,-p)$.**\n\nA reactive trajectory is defined as a path that starts in basin $A$ and ends in basin $B$ without returning to $A$ in between. With basins defined by level sets of the committor function, a trajectory is considered reactive if it begins from a point where $q \\le \\delta$ and ends upon first reaching a point where $q \\ge 1-\\delta$. This definition is mathematically precise and operationally unambiguous.\nThe time-reversal symmetry of the path ensemble requires both dynamical reversibility and symmetry of the boundary conditions. The problem states the dynamics are reversible. Let $\\mathbf{z}(t)=(x(t), p(t))$ be a reactive trajectory of duration $\\tau$, starting in $A$ and ending in $B$. Its time-reversed counterpart is $\\hat{\\mathbf{z}}(t) = \\theta(\\mathbf{z}(\\tau-t))$. The starting point of the reversed path is $\\hat{\\mathbf{z}}(0) = \\theta(\\mathbf{z}(\\tau))$, which lies in $\\theta B$. The ending point is $\\hat{\\mathbf{z}}(\\tau) = \\theta(\\mathbf{z}(0))$, which lies in $\\theta A$. For the ensemble of forward reactive paths ($A \\to B$) to be identical to the ensemble of backward reactive paths ($B \\to A$) under time reversal, we require the time-reversed forward paths to be valid backward paths. This requires $\\theta A = B$ and $\\theta B = A$. The condition $B=\\theta A$ implies $\\theta B = \\theta(\\theta A) = A$, so both are satisfied. Detailed balance for the path probability measure then ensures that the probability of a path from $A$ to $B$ is the same as its time-reversal, which is a path from $B$ to $A$. The statement says \"time-reversal symmetry in the transition path ensemble,\" which is achieved under these conditions.\n\nVerdict: **Correct**.\n\n**C. Including momentum in the definition of basins necessarily lowers the acceptance probability of standard two-way shooting moves in Transition Path Sampling because the phase-space basins are smaller than their configuration-space counterparts.**\n\nThis statement makes a strong claim about a necessary decrease in acceptance probability. The acceptance probability in a two-way shooting move depends on whether the newly generated trajectory, composed of a forward and a backward segment, still connects basins $A$ and $B$. Using phase-space basins based on the committor provides a more physically meaningful definition of the reactant and product states by filtering out points that are likely to recross. This can prevent the generation of trial paths that are short, spurious excursions. While it might reject some paths that would have been accepted with configuration-space basins (e.g., a path that just grazes the $B_x$ region), it may accept other paths that are more relevant to the true transition mechanism. By focusing the sampling on \"true\" transition paths, the efficiency of the TPS algorithm is generally improved. An improvement in efficiency is not exclusively tied to the acceptance rate, but it is incorrect to claim the rate must necessarily decrease. In some cases, by avoiding futile proposals, the acceptance rate of \"useful\" moves could even increase. The premise that phase-space basins are \"smaller\" is also ill-defined and their comparative effect on acceptance rates is not straightforward. Therefore, this generalization is unfounded.\n\nVerdict: **Incorrect**.\n\n**D. For reversible underdamped Langevin dynamics at equilibrium, if $B=\\theta A$, then the committor satisfies $q(x,-p)=1-q(x,p)$, and a crossing of a configuration-space dividing surface at which $q(x,p)1/2$ should not be counted as a reactive event regardless of the sign of the velocity.**\n\nThe first part addresses a symmetry property of the committor. Let $q(z)$ be the committor for a state $z=(x,p)$. Consider the time-reversed state $\\theta z = (x,-p)$. Due to microscopic reversibility, the probability that a trajectory starting from $z$ reaches $B$ before $A$ is equal to the probability that a time-reversed trajectory starting from $\\theta z$ reaches $\\theta B$ before $\\theta A$. Given the basin symmetry $B=\\theta A$ (which implies $A=\\theta B$), this is the probability of a trajectory starting from $\\theta z$ reaching $A$ before $B$. This probability is, by definition, $1-q(\\theta z)$. So we have $q(z) = 1-q(\\theta z)$, which rearranges to $q(\\theta z) = 1-q(z)$, or $q(x,-p) = 1-q(x,p)$. The surface where $q(x,p)=1/2$ is the transition state surface or separatrix. This first part of the statement is correct.\nThe second part states that a crossing of a configuration-space dividing surface at a point $(x,p)$ where $q(x,p)1/2$ is not a reactive event. The value of the committor is the ultimate measure of commitment to the product state $B$. If $q(x,p)1/2$, the state $(x,p)$ is more likely to return to the reactant basin $A$ than to proceed to the product basin $B$. Therefore, such a state is on the \"reactant side\" of the true separatrix ($q=1/2$). Any trajectory passing through such a point, when it crosses a heuristically defined surface in configuration space, has not yet truly committed to the reaction. This is precisely the principle behind using the committor to distinguish reactive from non-reactive trajectories and to filter out recrossings. The velocity information is already encoded in the committor value $q(x,p)$, so the statement's conclusion is sound.\n\nVerdict: **Correct**.\n\n**E. The exact equilibrium rate constant computed from Transition Path Sampling is invariant to whether basins are defined in configuration space or in phase space, provided both choices lead to metastable, disjoint sets representing the same chemical species; the primary effect of including momentum is to reduce misclassification and recrossings, improving estimator efficiency rather than changing the true rate.**\n\nThe reaction rate constant is a macroscopic observable that characterizes the timescale of transitions between stable thermodynamic states (chemical species). As such, its true value should not depend on the specific microscopic details of how the boundaries of these stable states are defined, as long as the definitions properly enclose the metastable regions and are separated by a significant free energy barrier. This is the \"plateau principle\" in rate theory: the computed rate should be stable over a range of reasonable basin definitions. Therefore, the true rate constant is invariant.\nThe choice of basin definition profoundly impacts the numerical calculation of this rate. As established in the analysis of option A, using configuration-space basins for underdamped dynamics leads to the misclassification of states and counts many short recrossing trajectories as attempted transitions. This degrades the statistical properties of any rate estimator. Including momentum and using phase-space basins based on the committor provides a much cleaner separation between reactant, product, and transition regions. This reduces the variance of the rate estimator and improves the overall efficiency of the TPS calculation. The choice affects the method's performance, not the physical quantity being measured.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "2690116"}]}