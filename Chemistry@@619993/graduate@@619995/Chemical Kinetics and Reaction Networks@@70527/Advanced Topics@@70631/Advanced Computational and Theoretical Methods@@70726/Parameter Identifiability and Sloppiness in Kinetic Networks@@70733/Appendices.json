{"hands_on_practices": [{"introduction": "This first exercise strips a model down to its bare essentials to reveal the mathematical heart of sloppiness. By analyzing a simple \"ridge\" likelihood function, you will directly compute the Fisher Information Matrix (FIM) and its spectral properties, connecting its eigenvalues and eigenvectors to the geometry of the parameter space. This foundational practice is crucial for building an intuition for how the FIM reveals \"stiff\" (well-determined) and \"sloppy\" (ill-determined) combinations of parameters [@problem_id:2661021].", "problem": "Consider a reduced model of a chemical reaction network in which only the ratio of two effective rate constants influences the measured observable. Let the two estimated parameters be the logarithms of those rate constants, denoted by $\\theta_1$ and $\\theta_2$, which are dimensionless. Suppose that, after integrating out data-dependent terms, the likelihood for the parameters takes the ridge form\n$$\nL(\\theta_1,\\theta_2) \\propto \\exp\\!\\left(-\\frac{(\\theta_1-\\theta_2)^2}{\\sigma^2}\\right),\n$$\nwith noise scale $\\sigma>0$. Starting from the definition of the Fisher Information Matrix (FIM) for parameters $\\theta = (\\theta_1,\\theta_2)^{\\top}$,\n$$\nF_{ij}(\\theta) \\equiv -\\mathbb{E}\\!\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ln L(\\theta)\\right],\n$$\nderive the curvature properties of the log-likelihood and show that the model is sloppy (i.e., unidentifiable to leading order) along the direction $\\theta_1+\\theta_2$. In particular, compute the eigenvalues and eigenvectors of the Fisher Information Matrix, identify the stiff and sloppy directions, and explain why sloppiness arises in terms of the parameter combinations that affect the likelihood.\n\nYour final numerical objective is: report the nonzero eigenvalue of the Fisher Information Matrix as a closed-form analytic expression in terms of $\\sigma$. No rounding is required, and no units are involved. Express your final answer as a single analytic expression in terms of $\\sigma$.", "solution": "The problem requires a rigorous analysis of parameter identifiability for a given likelihood function. We must validate the problem statement before proceeding.\n\nFirst, we extract the given information.\nThe parameters are $\\theta_1$ and $\\theta_2$, forming the vector $\\theta = (\\theta_1, \\theta_2)^{\\top}$.\nThe likelihood function is given by the proportionality:\n$$\nL(\\theta_1,\\theta_2) \\propto \\exp\\!\\left(-\\frac{(\\theta_1-\\theta_2)^2}{\\sigma^2}\\right)\n$$\nwhere $\\sigma > 0$ is a constant.\nThe Fisher Information Matrix (FIM) is defined by its components:\n$$\nF_{ij}(\\theta) \\equiv -\\mathbb{E}\\!\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ln L(\\theta)\\right]\n$$\nThe task is to compute the eigenvalues and eigenvectors of this matrix, identify the stiff and sloppy directions, and explain the origin of the model's sloppiness. The final objective is to report the nonzero eigenvalue of the FIM.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in the study of parameter sensitivity and structural identifiability in modeling. The likelihood form, where the output depends only on a specific combination of parameters, is a classic scenario leading to unidentifiability. All necessary information is provided, and the problem is mathematically sound. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe log-likelihood function, denoted by $\\ell(\\theta)$, is obtained by taking the natural logarithm of the likelihood $L(\\theta)$. Ignoring the constant of proportionality, which has no effect on the second derivatives, we have:\n$$\n\\ell(\\theta_1, \\theta_2) = \\ln(L(\\theta_1, \\theta_2)) = -\\frac{(\\theta_1-\\theta_2)^2}{\\sigma^2} = -\\frac{1}{\\sigma^2}(\\theta_1^2 - 2\\theta_1\\theta_2 + \\theta_2^2)\n$$\nTo construct the FIM, we must first compute the Hessian matrix of the log-likelihood, $H(\\theta)$, whose entries are $H_{ij} = \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j}$.\n\nFirst, we compute the first partial derivatives of $\\ell(\\theta_1, \\theta_2)$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_1} = -\\frac{1}{\\sigma^2}(2\\theta_1 - 2\\theta_2) = -\\frac{2(\\theta_1 - \\theta_2)}{\\sigma^2}\n$$\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_2} = -\\frac{1}{\\sigma^2}(-2\\theta_1 + 2\\theta_2) = \\frac{2(\\theta_1 - \\theta_2)}{\\sigma^2}\n$$\nNext, we compute the second partial derivatives:\n$$\nH_{11} = \\frac{\\partial^2 \\ell}{\\partial \\theta_1^2} = \\frac{\\partial}{\\partial \\theta_1}\\left(-\\frac{2\\theta_1 - 2\\theta_2}{\\sigma^2}\\right) = -\\frac{2}{\\sigma^2}\n$$\n$$\nH_{22} = \\frac{\\partial^2 \\ell}{\\partial \\theta_2^2} = \\frac{\\partial}{\\partial \\theta_2}\\left(\\frac{2\\theta_1 - 2\\theta_2}{\\sigma^2}\\right) = -\\frac{2}{\\sigma^2}\n$$\n$$\nH_{12} = \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2}\\left(-\\frac{2\\theta_1 - 2\\theta_2}{\\sigma^2}\\right) = \\frac{2}{\\sigma^2}\n$$\nBy Clairaut's theorem, $H_{21} = H_{12}$, so we have $H_{21} = \\frac{2}{\\sigma^2}$.\n\nThe Hessian matrix $H$ is therefore:\n$$\nH = \\begin{pmatrix} -\\frac{2}{\\sigma^2} & \\frac{2}{\\sigma^2} \\\\ \\frac{2}{\\sigma^2} & -\\frac{2}{\\sigma^2} \\end{pmatrix} = \\frac{2}{\\sigma^2} \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}\n$$\nThe FIM is defined as $F = -\\mathbb{E}[H]$. Since the entries of the Hessian matrix $H$ are constants and do not depend on any random variables (data) or the parameters $\\theta$, the expectation operation is trivial: $\\mathbb{E}[H] = H$.\nThus, the FIM is:\n$$\nF = -H = -\\left(\\frac{2}{\\sigma^2} \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\right) = \\frac{2}{\\sigma^2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nTo analyze the curvature properties and identifiability, we compute the eigenvalues and eigenvectors of the FIM. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(F - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det\\left( \\begin{pmatrix} \\frac{2}{\\sigma^2} - \\lambda & -\\frac{2}{\\sigma^2} \\\\ -\\frac{2}{\\sigma^2} & \\frac{2}{\\sigma^2} - \\lambda \\end{pmatrix} \\right) = 0\n$$\n$$\n\\left(\\frac{2}{\\sigma^2} - \\lambda\\right)^2 - \\left(-\\frac{2}{\\sigma^2}\\right)^2 = 0\n$$\n$$\n\\left(\\frac{2}{\\sigma^2} - \\lambda\\right)^2 = \\left(\\frac{2}{\\sigma^2}\\right)^2\n$$\nThis equation yields two solutions for $\\lambda$:\n$1$. $\\frac{2}{\\sigma^2} - \\lambda_1 = \\frac{2}{\\sigma^2} \\implies \\lambda_1 = 0$\n$2$. $\\frac{2}{\\sigma^2} - \\lambda_2 = -\\frac{2}{\\sigma^2} \\implies \\lambda_2 = \\frac{4}{\\sigma^2}$\n\nThe eigenvalues of the FIM are $\\lambda_1 = 0$ and $\\lambda_2 = \\frac{4}{\\sigma^2}$.\n\nNext, we find the corresponding eigenvectors.\nFor the eigenvalue $\\lambda_1 = 0$, the eigenvector $v_1 = (x, y)^\\top$ satisfies $(F - 0 \\cdot I)v_1 = 0$:\n$$\n\\frac{2}{\\sigma^2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis simplifies to the equation $x - y = 0$, or $x = y$. The eigenvector is any vector proportional to $(1, 1)^\\top$. We choose the normalized eigenvector $v_1 = \\frac{1}{\\sqrt{2}}(1, 1)^\\top$.\n\nFor the eigenvalue $\\lambda_2 = \\frac{4}{\\sigma^2}$, the eigenvector $v_2 = (x, y)^\\top$ satisfies $(F - \\lambda_2 I)v_2 = 0$:\n$$\n\\left( \\frac{2}{\\sigma^2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} - \\frac{4}{\\sigma^2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\left( \\begin{pmatrix} \\frac{2}{\\sigma^2} & -\\frac{2}{\\sigma^2} \\\\ -\\frac{2}{\\sigma^2} & \\frac{2}{\\sigma^2} \\end{pmatrix} - \\begin{pmatrix} \\frac{4}{\\sigma^2} & 0 \\\\ 0 & \\frac{4}{\\sigma^2} \\end{pmatrix} \\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\frac{2}{\\sigma^2} \\begin{pmatrix} -1 & -1 \\\\ -1 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis simplifies to the equation $-x - y = 0$, or $x = -y$. The eigenvector is any vector proportional to $(1, -1)^\\top$. We choose the normalized eigenvector $v_2 = \\frac{1}{\\sqrt{2}}(1, -1)^\\top$.\n\nThe eigenvalues of the FIM correspond to the curvature of the log-likelihood surface in the directions of the eigenvectors.\nThe eigenvector $v_2$, associated with the nonzero eigenvalue $\\lambda_2 = \\frac{4}{\\sigma^2}$, represents the \"stiff\" direction in the parameter space. This direction corresponds to changes in the parameter combination $\\theta_1 - \\theta_2$. The likelihood is highly sensitive to perturbations along this direction, meaning this combination of parameters is well-constrained by the data.\nThe eigenvector $v_1$, associated with the eigenvalue $\\lambda_1 = 0$, represents the \"sloppy\" direction. This direction corresponds to changes in the parameter combination $\\theta_1 + \\theta_2$. A zero eigenvalue signifies that the log-likelihood surface is completely flat along this direction. The model is therefore structurally unidentifiable along this parametric direction.\n\nSloppiness, or in this case, complete unidentifiability, arises because the likelihood function $L(\\theta_1, \\theta_2)$ depends only on the difference $\\theta_1 - \\theta_2$. Any change to the parameters such that $\\theta_1$ and $\\theta_2$ are changed by the same amount, i.e., $\\Delta\\theta_1 = \\Delta\\theta_2 = \\delta$, will leave the quantity $\\theta_1 - \\theta_2$ invariant: $(\\theta_1+\\delta) - (\\theta_2+\\delta) = \\theta_1 - \\theta_2$. Such a change corresponds to a move in the parameter space along the vector $(1, 1)^\\top$, which is the sloppy eigenvector $v_1$. Since the likelihood does not change for motions along this direction, the data provide no information whatsoever about the value of the parameter combination $\\theta_1 + \\theta_2$. This demonstrates that the model is sloppy (unidentifiable) along the direction corresponding to $\\theta_1 + \\theta_2$, as required.\n\nThe final task is to report the nonzero eigenvalue of the Fisher Information Matrix. Based on our derivation, this value is $\\frac{4}{\\sigma^2}$.", "answer": "$$\n\\boxed{\\frac{4}{\\sigma^2}}\n$$", "id": "2661021"}, {"introduction": "Building on the foundational concepts, this problem explores how unidentifiability arises in a realistic catalytic cycle due to specific experimental limitations. You will derive an observable quantity based on initial-rate data and find that it only constrains a product of the underlying rate constants, $k_1$ and $k_2$. This exercise highlights the importance of reparameterization, a powerful technique for separating the identifiable information from the unidentifiable parts of a model, thereby clarifying what an experiment can truly measure [@problem_id:2660969].", "problem": "A two-step catalytic cycle under pseudo-first-order conditions is modeled by mass-action kinetics for an enzyme that cycles between a free state and a catalytic complex before releasing product. Let the free enzyme be denoted by $E$, the enzyme–substrate complex by $C$, and the accumulated product by $P$. Assume a saturating and constant substrate so that the forward binding step is effectively first order with respect to $E$ with rate constant $k_1$, and the catalytic release step converting $C$ back to $E$ while producing $P$ occurs with rate constant $k_2$. The total enzyme $E_{\\mathrm{T}}$ is conserved, so that $E(t) + C(t) = E_{\\mathrm{T}}$ for all $t \\ge 0$. The dynamics are\n$$\n\\frac{dE}{dt} \\;=\\; -\\,k_1\\,E \\;+\\; k_2\\,C,\\qquad\n\\frac{dC}{dt} \\;=\\; k_1\\,E \\;-\\; k_2\\,C,\\qquad\n\\frac{dP}{dt} \\;=\\; k_2\\,C,\n$$\nwith initial conditions $E(0)=E_{\\mathrm{T}}$, $C(0)=0$, and $P(0)=0$. Suppose the experimental apparatus cannot resolve the full time course but returns only the initial curvature of the normalized product, i.e., the scalar statistic\n$$\ng \\;=\\; \\left.\\frac{d^2}{dt^2}\\,\\frac{P(t)}{E_{\\mathrm{T}}}\\right|_{t=0}.\n$$\nUsing only the stated model equations and the conservation law, and without invoking any approximations beyond pseudo-first-order kinetics and conservation, derive $g$ in terms of $k_1$, $k_2$, and $E_{\\mathrm{T}}$ and determine which combinations of $k_1$ and $k_2$ are structurally identifiable from $g$ alone. Then, construct an explicit reparameterization $\\phi:(k_1,k_2)\\mapsto(\\theta_1,\\theta_2)$ such that $\\theta_1$ is structurally identifiable from $g$ and $\\theta_2$ is unidentifiable from $g$, and such that $\\phi$ is one-to-one for any fixed identifiable combination. Finally, provide the explicit expression for $\\theta_1$ as a function of $k_1$ and $k_2$. Express your final answer as a closed-form analytic expression in terms of $k_1$ and $k_2$ only. No numerical approximation is required.", "solution": "The problem requires the derivation of an observable quantity related to the initial curvature of product formation in a simple enzymatic cycle, followed by an analysis of parameter identifiability. We shall proceed with a direct calculation based on the provided system of ordinary differential equations and initial conditions.\n\nThe system is described by the following dynamics and conservation law:\n$$\n\\frac{dE}{dt} = -k_1 E + k_2 C\n$$\n$$\n\\frac{dC}{dt} = k_1 E - k_2 C\n$$\n$$\n\\frac{dP}{dt} = k_2 C\n$$\n$$\nE(t) + C(t) = E_{\\mathrm{T}}\n$$\nWith initial conditions $E(0) = E_{\\mathrm{T}}$, $C(0) = 0$, and $P(0) = 0$. The parameters $k_1$, $k_2$, and $E_{\\mathrm{T}}$ are positive constants.\n\nThe experimental observable is the scalar statistic $g$, defined as the initial curvature of the normalized product concentration:\n$$\ng = \\left.\\frac{d^2}{dt^2}\\,\\frac{P(t)}{E_{\\mathrm{T}}}\\right|_{t=0}\n$$\nTo compute $g$, we must find the second derivative of the function $\\frac{P(t)}{E_{\\mathrm{T}}}$ and evaluate it at $t = 0$.\n\nLet us begin by finding the first derivative of the normalized product with respect to time $t$:\n$$\n\\frac{d}{dt}\\left(\\frac{P(t)}{E_{\\mathrm{T}}}\\right) = \\frac{1}{E_{\\mathrm{T}}} \\frac{dP}{dt}\n$$\nSubstituting the given differential equation for $P$, $\\frac{dP}{dt} = k_2 C(t)$, we obtain:\n$$\n\\frac{d}{dt}\\left(\\frac{P(t)}{E_{\\mathrm{T}}}\\right) = \\frac{k_2 C(t)}{E_{\\mathrm{T}}}\n$$\nNow, we compute the second derivative by differentiating the above expression with respect to $t$:\n$$\n\\frac{d^2}{dt^2}\\left(\\frac{P(t)}{E_{\\mathrm{T}}}\\right) = \\frac{d}{dt}\\left(\\frac{k_2 C(t)}{E_{\\mathrm{T}}}\\right) = \\frac{k_2}{E_{\\mathrm{T}}} \\frac{dC}{dt}\n$$\nThe value of $g$ is this second derivative evaluated at $t=0$:\n$$\ng = \\left.\\frac{k_2}{E_{\\mathrm{T}}} \\frac{dC}{dt}\\right|_{t=0}\n$$\nTo proceed, we must determine the value of $\\frac{dC}{dt}$ at $t=0$. We use the differential equation for $C(t)$:\n$$\n\\frac{dC}{dt} = k_1 E(t) - k_2 C(t)\n$$\nEvaluating this expression at $t=0$ yields:\n$$\n\\left.\\frac{dC}{dt}\\right|_{t=0} = k_1 E(0) - k_2 C(0)\n$$\nWe are given the initial conditions $E(0) = E_{\\mathrm{T}}$ and $C(0) = 0$. Substituting these values gives:\n$$\n\\left.\\frac{dC}{dt}\\right|_{t=0} = k_1 (E_{\\mathrm{T}}) - k_2 (0) = k_1 E_{\\mathrm{T}}\n$$\nNow, we substitute this result back into our expression for $g$:\n$$\ng = \\frac{k_2}{E_{\\mathrm{T}}} (k_1 E_{\\mathrm{T}})\n$$\nThe total enzyme concentration $E_{\\mathrm{T}}$ cancels, leading to the final expression for $g$:\n$$\ng = k_1 k_2\n$$\nThis expression gives the observable $g$ in terms of the model parameters $k_1$ and $k_2$.\n\nNext, we address the question of structural identifiability. The experimental measurement provides a value for $g$. From the relation $g = k_1 k_2$, we see that the experiment only determines the value of the product of the rate constants, $k_1 k_2$. It is impossible to uniquely determine the individual values of $k_1$ and $k_2$ from this single piece of information. For any given measurement $g_0$, any pair of parameters $(k_1, k_2)$ lying on the hyperbola $k_1 k_2 = g_0$ in the parameter space is consistent with the data. Therefore, the parameters $k_1$ and $k_2$ are individually structurally unidentifiable. The only combination of parameters that is structurally identifiable from the observable $g$ is the product $k_1 k_2$.\n\nThe problem then requires the construction of a one-to-one reparameterization $\\phi:(k_1,k_2)\\mapsto(\\theta_1,\\theta_2)$ where $\\theta_1$ is structurally identifiable and $\\theta_2$ is structurally unidentifiable from the measurement of $g$. Based on our analysis, the natural choice for the identifiable parameter $\\theta_1$ is the combination that is directly measured:\n$$\n\\theta_1 = k_1 k_2\n$$\nFor the unidentifiable parameter $\\theta_2$, we must choose a function of $k_1$ and $k_2$ that is functionally independent of $\\theta_1$ and ensures the mapping is invertible. A suitable choice is the ratio of the rate constants:\n$$\n\\theta_2 = \\frac{k_1}{k_2}\n$$\nThe observable $g$ provides no information about this ratio. To confirm this reparameterization is one-to-one for positive $k_1$ and $k_2$, we can solve for $k_1$ and $k_2$ in terms of $\\theta_1$ and $\\theta_2$. Multiplying the two equations gives $k_1^2 = \\theta_1 \\theta_2$, so $k_1 = \\sqrt{\\theta_1 \\theta_2}$. Dividing the equation for $\\theta_1$ by the equation for $\\theta_2$ gives $k_2^2 = \\theta_1 / \\theta_2$, so $k_2 = \\sqrt{\\theta_1 / \\theta_2}$. Since each pair $(\\theta_1, \\theta_2)$ with $\\theta_1 > 0, \\theta_2 > 0$ maps to a unique pair $(k_1, k_2)$ with $k_1 > 0, k_2 > 0$, the mapping is one-to-one.\n\nThis construction successfully separates the model parameters into an identifiable part $\\theta_1$ and an unidentifiable part $\\theta_2$ with respect to the given observable $g$.\n\nThe final question asks for the explicit expression for the identifiable parameter $\\theta_1$ as a function of $k_1$ and $k_2$. As we have defined it, this expression is:\n$$\n\\theta_1 = k_1 k_2\n$$", "answer": "$$\\boxed{k_1 k_2}$$", "id": "2660969"}, {"introduction": "Our final practice moves from analysis to active design, demonstrating the ultimate utility of sloppiness theory in an experimental context. Here, you will not just diagnose identifiability issues but will actively mitigate them by designing a better experiment using the principles of optimal experimental design. By selecting sampling times to maximize the smallest eigenvalue of the FIM—an approach known as E-optimality—you will see how these theoretical tools enable the efficient planning of experiments to achieve maximal parameter certainty [@problem_id:2661040].", "problem": "You are given a scalar output kinetic model for a single-step irreversible approach-to-equilibrium process described by the function $y(t) = A\\bigl(1 - e^{-k t}\\bigr)$, where $A$ and $k$ are unknown positive parameters representing an amplitude and a first-order rate constant, respectively. Observations follow the model $y_i = y(t_i; A, k) + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed as Gaussian random variables with mean $0$ and variance $\\sigma^2$ (that is, $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$). Sampling times $t_i$ may be chosen, under a constraint on the number of samples.\n\nYour task is to design a program that, for each test case specified below, selects a set of sampling times that maximizes the smallest eigenvalue of the Fisher Information Matrix (FIM) for the parameter vector $\\theta = (A,k)$, evaluated at a given nominal parameter pair $(A_0, k_0)$. The design criterion to be maximized is the minimum eigenvalue (often termed an $E$-optimal design objective), which is a standard proxy for avoiding parameter sloppiness by improving the worst-case curvature direction of the expected log-likelihood at the nominal parameter.\n\nFundamental base to use:\n- The measurement model $y(t; A, k) = A\\bigl(1 - e^{-k t}\\bigr)$.\n- Independent Gaussian noise with variance $\\sigma^2$.\n- The Fisher Information Matrix for independent observations under Gaussian noise, defined in terms of the model sensitivities with respect to $\\theta = (A, k)$.\n\nDesign constraints and discretization:\n- You must choose exactly $N$ distinct sampling times from a uniform grid of $M$ points on the open-closed interval $(0,T]$. The grid points are $t_j = \\frac{j}{M} T$ for $j = 1, 2, \\dots, M$ (note that $t = 0$ is excluded).\n- The samples must be reported in nondecreasing order (that is, strictly increasing because times are distinct).\n- Units: Report all times in seconds, rounded to three decimals.\n- Ties: If multiple sets of sampling times achieve the same objective value (within a tolerance of $10^{-12}$ on the objective), select the lexicographically smallest sequence of times (compare the first time, then the second, and so on).\n- The Fisher Information Matrix must be computed at the provided nominal parameter values $(A_0, k_0)$ and noise variance $\\sigma^2$.\n\nWhat to compute:\n- For each test case, compute the $E$-optimal sampling schedule, that is, the set of $N$ grid times that maximizes the smallest eigenvalue of the Fisher Information Matrix.\n\nTest suite:\n- Case $1$: $A_0 = 1.0$, $k_0 = 0.5\\,\\mathrm{s}^{-1}$, $\\sigma^2 = 1.0$, $T = 10.0\\,\\mathrm{s}$, $N = 2$, $M = 40$.\n- Case $2$: $A_0 = 2.0$, $k_0 = 1.0\\,\\mathrm{s}^{-1}$, $\\sigma^2 = 0.5$, $T = 5.0\\,\\mathrm{s}$, $N = 3$, $M = 50$.\n- Case $3$: $A_0 = 1.0$, $k_0 = 0.3\\,\\mathrm{s}^{-1}$, $\\sigma^2 = 2.0$, $T = 8.0\\,\\mathrm{s}$, $N = 1$, $M = 80$.\n- Case $4$: $A_0 = 0.5$, $k_0 = 0.2\\,\\mathrm{s}^{-1}$, $\\sigma^2 = 1.0$, $T = 20.0\\,\\mathrm{s}$, $N = 3$, $M = 40$.\n- Case $5$: $A_0 = 1.0$, $k_0 = 2.0\\,\\mathrm{s}^{-1}$, $\\sigma^2 = 0.1$, $T = 2.0\\,\\mathrm{s}$, $N = 2$, $M = 40$.\n\nAnswer specification:\n- Angle units do not apply in this problem.\n- Physical units: Output sampling times in seconds, each rounded to three decimals.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a bracket-enclosed, comma-separated list of the selected sampling times in seconds. For example, if there were two cases and the optimal times were $[1.250, 3.000]$ and $[0.750]$, the output would be exactly the single line:\n[ [1.250,3.000],[0.750] ]\nFor this problem with five test cases, your program must print exactly one line with a list of five lists, in the same order as the test suite above, with no extra text.\n\nNotes:\n- You must derive the Fisher Information Matrix for the given model from the stated fundamental definitions and use it in your computations.\n- Ensure scientific realism: All computations must be performed at the provided nominal $(A_0, k_0)$ and $\\sigma^2$. The optimal design must be searched over all $\\binom{M}{N}$ subsets of the grid for each test case.", "solution": "The problem presented is a classic exercise in the field of optimal experimental design for parameter estimation in a nonlinear dynamic model. We are tasked with selecting a set of optimal sampling times to best identify the parameters of a kinetic model, a common challenge in chemical kinetics and systems biology. The criterion for optimality is the maximization of the smallest eigenvalue of the Fisher Information Matrix (FIM), known as E-optimality. This criterion aims to minimize the length of the major axis of the parameter confidence ellipsoid, thereby improving the worst-case estimability of the parameters. The problem is well-posed, scientifically grounded, and provides all necessary information for a rigorous solution.\n\nThe solution proceeds in two main stages: first, the derivation of the mathematical objects required, namely the Fisher Information Matrix; and second, the formulation and execution of a computational algorithm to search the design space for the optimal set of sampling times.\n\n**1. The Fisher Information Matrix (FIM)**\n\nThe model for a single observation at time $t$ is given by $y(t) = A\\bigl(1 - e^{-k t}\\bigr)$. The measurements $y_i$ at times $t_i$ are subject to independent and identically distributed Gaussian noise, $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. The parameter vector is $\\theta = (A, k)$.\n\nFor such a model with i.i.d. Gaussian noise, the FIM for a set of $N$ observations at times $\\{t_1, t_2, \\dots, t_N\\}$ is a $2 \\times 2$ matrix given by:\n$$\nF(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\mathbf{s}(t_i; \\theta) \\mathbf{s}(t_i; \\theta)^T\n$$\nwhere $\\mathbf{s}(t_i; \\theta)$ is the sensitivity vector of the model output with respect to the parameters, evaluated at time $t_i$. The components of this vector are the partial derivatives of the model function $y(t; \\theta)$ with respect to each parameter.\n\nLet us compute the sensitivities:\nThe first parameter is $\\theta_1 = A$. The sensitivity is:\n$$\ns_A(t) = \\frac{\\partial y(t; A, k)}{\\partial A} = \\frac{\\partial}{\\partial A} \\left[ A(1 - e^{-kt}) \\right] = 1 - e^{-kt}\n$$\nThe second parameter is $\\theta_2 = k$. The sensitivity is:\n$$\ns_k(t) = \\frac{\\partial y(t; A, k)}{\\partial k} = \\frac{\\partial}{\\partial k} \\left[ A(1 - e^{-kt}) \\right] = A \\frac{\\partial}{\\partial k} ( -e^{-kt} ) = At e^{-kt}\n$$\nThe sensitivity vector is therefore $\\mathbf{s}(t; A, k) = [1 - e^{-kt}, At e^{-kt}]^T$.\n\nThe FIM is the sum of the outer products of these sensitivity vectors. For a set of $N$ measurement times $\\{t_i\\}$, the elements of the FIM, evaluated at the nominal parameters $\\theta_0 = (A_0, k_0)$ and noise variance $\\sigma^2$, are:\n$$\nF_{11} = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\left(1 - e^{-k_0 t_i}\\right)^2\n$$\n$$\nF_{12} = F_{21} = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\left(1 - e^{-k_0 t_i}\\right) (A_0 t_i e^{-k_0 t_i})\n$$\n$$\nF_{22} = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\left(A_0 t_i e^{-k_0 t_i}\\right)^2\n$$\nThe resulting FIM is the symmetric matrix $F = \\begin{pmatrix} F_{11} & F_{12} \\\\ F_{21} & F_{22} \\end{pmatrix}$. Note that the term $1/\\sigma^2$ is a common scalar factor for the entire matrix. Since we are maximizing an eigenvalue, and eigenvalues of a matrix $M$ are scaled by a factor $\\alpha$ when the matrix is scaled to $\\alpha M$, this constant will scale the objective function but will not change the location of the optimum. It must, however, be included for correctness.\n\n**2. Objective Function and Optimization Strategy**\n\nThe objective is to maximize the smallest eigenvalue of the FIM, $\\lambda_{\\min}(F)$. For a general $2 \\times 2$ symmetric matrix $F = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$, the eigenvalues are the roots of the characteristic polynomial $\\det(F - \\lambda I) = 0$, which are given by:\n$$\n\\lambda = \\frac{(a+c) \\pm \\sqrt{(a-c)^2 + 4b^2}}{2}\n$$\nThe smallest eigenvalue is therefore:\n$$\n\\lambda_{\\min}(F) = \\frac{(F_{11} + F_{22}) - \\sqrt{(F_{11} - F_{22})^2 + 4F_{12}^2}}{2}\n$$\nOur task is to choose $N$ distinct time points from the discrete grid $t_j = \\frac{j}{M} T$ for $j \\in \\{1, \\dots, M\\}$ that maximize this value.\n\nThe number of possible ways to choose $N$ times from a grid of $M$ points is given by the binomial coefficient $\\binom{M}{N}$. For the values of $M$ and $N$ specified in the test cases, this number is computationally tractable. For instance, the largest case requires checking $\\binom{50}{3} = 19600$ combinations. Therefore, a direct, exhaustive search is a feasible and guaranteed-optimal strategy.\n\nThe algorithm is as follows for each test case:\n1.  Generate the grid of $M$ possible sampling times.\n2.  Generate all unique combinations of $N$ sampling times from this grid.\n3.  Initialize a variable for the maximum observed minimum eigenvalue, $\\lambda_{\\text{best}} \\leftarrow -1.0$, and a variable for the corresponding time set, $t_{\\text{best}}$.\n4.  Iterate through each combination of times:\n    a. For the current combination, compute the FIM elements $F_{11}, F_{12}, F_{22}$ by summing the contributions from each time point.\n    b. Construct the FIM and calculate its smallest eigenvalue, $\\lambda_{\\text{current}}$.\n    c. If $\\lambda_{\\text{current}}$ is greater than $\\lambda_{\\text{best}}$ by more than the specified tolerance ($10^{-12}$), update $\\lambda_{\\text{best}} \\leftarrow \\lambda_{\\text{current}}$ and $t_{\\text{best}}$ to the current time set.\n5.  The tie-breaking rule states that if multiple sets yield the same maximal eigenvalue (within tolerance), the lexicographically smallest sequence should be chosen. By iterating through the combinations as generated by standard library functions (which typically produce them in lexicographical order) and only updating on a strict improvement (i.e., `new > old + tolerance`), this condition is automatically satisfied. The first combination found to achieve the maximal value will be the lexicographically smallest.\n\n**3. Analysis of the Special Case $N=1$**\n\nA critical detail arises in Case 3, where $N=1$. The number of parameters to be estimated is $p=2$ (namely $A$ and $k$). For the FIM to be of full rank (i.e., invertible, having all non-zero eigenvalues), one requires at least $p$ measurements at time points where the sensitivity vectors are linearly independent.\n\nFor $N=1$, the FIM is the outer product of a single sensitivity vector, $F = \\frac{1}{\\sigma^2} \\mathbf{s}(t_1) \\mathbf{s}(t_1)^T$. This is a $2 \\times 2$ matrix of rank 1 (assuming the sensitivity vector is non-zero, which is true for $t>0$). A rank-1 matrix has one non-zero eigenvalue and $p-1=1$ zero eigenvalue. Thus, for any choice of a single sampling time $t_1$, the smallest eigenvalue of the FIM is $\\lambda_{\\min}(F) = 0$.\n\nThe objective function is constant (zero) across the entire design space. The optimization problem becomes trivial. According to the problem's tie-breaking rule, we must select the lexicographically smallest sequence of times. For $N=1$, this corresponds to the single smallest time point available on the grid, which is $t_1 = T/M$. This case tests the fundamental understanding of FIM properties.\n\nBy implementing this logic, we provide a robust solution for all given test cases.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves for the E-optimal experimental design for a set of kinetic model test cases.\n\n    For each case, it finds the set of N sampling times from a discrete grid that\n    maximizes the smallest eigenvalue of the Fisher Information Matrix (FIM).\n    \"\"\"\n\n    test_cases = [\n        # (A0, k0, sigma^2, T, N, M)\n        {'A0': 1.0, 'k0': 0.5, 'sigma2': 1.0, 'T': 10.0, 'N': 2, 'M': 40},\n        {'A0': 2.0, 'k0': 1.0, 'sigma2': 0.5, 'T': 5.0, 'N': 3, 'M': 50},\n        {'A0': 1.0, 'k0': 0.3, 'sigma2': 2.0, 'T': 8.0, 'N': 1, 'M': 80},\n        {'A0': 0.5, 'k0': 0.2, 'sigma2': 1.0, 'T': 20.0, 'N': 3, 'M': 40},\n        {'A0': 1.0, 'k0': 2.0, 'sigma2': 0.1, 'T': 2.0, 'N': 2, 'M': 40},\n    ]\n\n    all_results = []\n    \n    TOLERANCE = 1e-12\n\n    for case in test_cases:\n        A0 = case['A0']\n        k0 = case['k0']\n        sigma2 = case['sigma2']\n        T = case['T']\n        N = case['N']\n        M = case['M']\n\n        # Generate the grid of possible sampling times\n        time_grid = np.linspace(T/M, T, M)\n\n        # Special case: N < number of parameters (p=2)\n        # The FIM will be rank-deficient, so the smallest eigenvalue is always 0.\n        # By the tie-breaking rule, we must choose the lexicographically smallest sequence.\n        if N < 2:\n            # For N=1, this is the smallest time point.\n            best_times = (time_grid[0],)\n        else:\n            best_min_eig = -1.0\n            best_times = None\n\n            # Pre-calculate sensitivities for all grid points to speed up the loop\n            s_A = 1.0 - np.exp(-k0 * time_grid)\n            s_k = A0 * time_grid * np.exp(-k0 * time_grid)\n\n            # Map sensitivities to their time grid index\n            sens_map = {i: (s_A[i], s_k[i]) for i in range(M)}\n\n            # Generate all combinations of N indices from the grid\n            indices = range(M)\n            for time_indices in itertools.combinations(indices, N):\n                f11, f12, f22 = 0.0, 0.0, 0.0\n                \n                for index in time_indices:\n                    sa, sk = sens_map[index]\n                    f11 += sa**2\n                    f12 += sa * sk\n                    f22 += sk**2\n                \n                # Construct the FIM (including the 1/sigma^2 factor)\n                fim = (1.0 / sigma2) * np.array([[f11, f12], [f12, f22]])\n\n                # Calculate eigenvalues for the symmetric matrix\n                eigenvalues = np.linalg.eigvalsh(fim)\n                min_eig = eigenvalues[0]\n\n                # Update best result if a significantly better one is found.\n                # `itertools.combinations` generates tuples in lexicographical order,\n                # so the first combination that achieves the max value will be the\n                # lexicographically smallest one, satisfying the tie-breaker.\n                if min_eig > best_min_eig + TOLERANCE:\n                    best_min_eig = min_eig\n                    best_times = tuple(time_grid[i] for i in time_indices)\n\n        # Format the result for the current test case\n        formatted_times = [f\"{t:.3f}\" for t in best_times]\n        result_str = f\"[{','.join(formatted_times)}]\"\n        all_results.append(result_str)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "2661040"}]}