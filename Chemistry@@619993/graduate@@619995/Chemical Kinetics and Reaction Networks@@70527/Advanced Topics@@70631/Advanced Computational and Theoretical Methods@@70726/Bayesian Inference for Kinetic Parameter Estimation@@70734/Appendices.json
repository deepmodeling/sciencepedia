{"hands_on_practices": [{"introduction": "The log-posterior probability density is the central object in any Bayesian analysis. For models based on differential equations, this density is composed of a prior on the parameters and a likelihood term that quantifies the misfit between model predictions and experimental data. This first exercise [@problem_id:2628012] focuses on deriving the log-likelihood and its gradient for a general kinetic model with Gaussian measurement noise. Mastering this derivation is a crucial first step, as these quantities are the essential inputs for a vast range of inference algorithms, from gradient-based optimization to find the maximum a posteriori (MAP) estimate to sophisticated samplers like Hamiltonian Monte Carlo.", "problem": "A deterministic, well-mixed chemical reaction network with $d$ chemical species is modeled by the reaction-rate ordinary differential equation (ODE) $\\mathrm{d}x/\\mathrm{d}t = f(x,\\theta)$ with initial condition $x(0;\\theta) = x_{0}$, where $x(t;\\theta) \\in \\mathbb{R}^{d}$ is the concentration vector at time $t$ and $\\theta \\in \\mathbb{R}^{p}$ is a vector of unknown kinetic parameters (for example, rate constants under mass-action kinetics). Let $t_{1},\\dots,t_{n}$ be distinct observation times. At each time $t_{i}$, only a subset of the species is measured: the observation operator $h \\in \\mathbb{R}^{m \\times d}$ is a fixed selection matrix whose rows are standard basis vectors selecting $m \\leq d$ components of $x(t;\\theta)$. The measurement model is\n$$\ny_{i} \\;=\\; h\\,x(t_{i};\\theta) \\;+\\; \\varepsilon_{i}, \\quad i=1,\\dots,n,\n$$\nwhere the errors $\\varepsilon_{i} \\in \\mathbb{R}^{m}$ are independent and identically distributed, with $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$, and the variance $\\sigma^{2} > 0$ is known.\n\nGiven the data $\\{y_{i}\\}_{i=1}^{n}$ and the model above, derive from first principles a closed-form expression for the log-likelihood $\\ell(\\theta) = \\ln p(\\{y_{i}\\}_{i=1}^{n} \\mid \\theta, \\sigma^{2})$ and its gradient with respect to $\\theta$. Express your gradient in terms of the residuals $r_{i}(\\theta) = y_{i} - h\\,x(t_{i};\\theta)$ and the state sensitivities $G_{i}(\\theta) = \\partial x(t_{i};\\theta)/\\partial \\theta \\in \\mathbb{R}^{d \\times p}$. Do not drop additive constants that do not depend on $\\theta$. Your final answer must be the pair consisting of the log-likelihood and its gradient, written as explicit analytic expressions. No numerical evaluation is required, and no units are needed.", "solution": "The problem statement has been validated and is deemed sound. It presents a standard, well-posed problem in parameter estimation for ordinary differential equation models, a common task in chemical kinetics and systems biology. We proceed with the derivation.\n\nThe problem requires the derivation of the log-likelihood function $\\ell(\\theta)$ and its gradient $\\nabla_{\\theta}\\ell(\\theta)$ from first principles.\n\nFirst, we derive the log-likelihood function $\\ell(\\theta) = \\ln p(\\{y_{i}\\}_{i=1}^{n} \\mid \\theta, \\sigma^{2})$.\nThe measurement model for a single observation $y_i$ at time $t_i$ is given by $y_{i} = h\\,x(t_{i};\\theta) + \\varepsilon_{i}$. The measurement error $\\varepsilon_{i}$ is a random variable following a multivariate normal distribution, $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$, where $I_m$ is the $m \\times m$ identity matrix.\nThis implies that the conditional distribution of the observation $y_i$, given the parameter vector $\\theta$, is also a multivariate normal distribution. The mean of this distribution is the deterministic model prediction $\\mu_i(\\theta) = h\\,x(t_{i};\\theta)$, and the covariance matrix is $\\Sigma = \\sigma^{2} I_{m}$.\nSo, $y_{i} \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(h\\,x(t_{i};\\theta), \\sigma^{2} I_{m})$.\n\nThe probability density function (PDF) for a single observation vector $y_i \\in \\mathbb{R}^m$ is given by the general form of the multivariate normal PDF:\n$$p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{(2\\pi)^{m/2} |\\sigma^2 I_m|^{1/2}} \\exp\\left(-\\frac{1}{2} (y_i - h x(t_i;\\theta))^T (\\sigma^2 I_m)^{-1} (y_i - h x(t_i;\\theta))\\right)$$\nThe determinant of the covariance matrix is $|\\sigma^2 I_m| = (\\sigma^2)^m = \\sigma^{2m}$. The inverse of the covariance matrix is $(\\sigma^2 I_m)^{-1} = \\frac{1}{\\sigma^2}I_m$.\nSubstituting these into the PDF expression yields:\n$$p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (y_i - h x(t_i;\\theta))^T (y_i - h x(t_i;\\theta))\\right)$$\nThe quadratic form in the exponent is the squared Euclidean norm of the residual vector, $\\|y_i - h x(t_i;\\theta)\\|_2^2$.\nThus, the PDF for a single data point is:\n$$p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y_i - h x(t_i;\\theta)\\|_2^2\\right)$$\nThe problem states that the errors $\\varepsilon_i$ are independent and identically distributed. This implies that the observations $\\{y_i\\}_{i=1}^n$ are conditionally independent given $\\theta$. The joint likelihood of the entire dataset is therefore the product of the individual likelihoods:\n$$L(\\theta) = p(\\{y_{i}\\}_{i=1}^{n} \\mid \\theta, \\sigma^{2}) = \\prod_{i=1}^{n} p(y_{i} \\mid \\theta, \\sigma^{2})$$\n$$L(\\theta) = \\prod_{i=1}^{n} \\left[ \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y_i - h x(t_i;\\theta)\\|_2^2\\right) \\right]$$\n$$L(\\theta) = \\left( \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\right)^n \\exp\\left( \\sum_{i=1}^{n} -\\frac{1}{2\\sigma^2} \\|y_i - h x(t_i;\\theta)\\|_2^2 \\right)$$\n$$L(\\theta) = \\frac{1}{(2\\pi\\sigma^2)^{nm/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2\\right)$$\nThe log-likelihood, $\\ell(\\theta)$, is the natural logarithm of the likelihood function $L(\\theta)$. Applying the logarithm:\n$$\\ell(\\theta) = \\ln(L(\\theta)) = \\ln\\left( \\frac{1}{(2\\pi\\sigma^2)^{nm/2}} \\right) + \\ln\\left(\\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2\\right)\\right)$$\n$$\\ell(\\theta) = -\\frac{nm}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2$$\nThis is the closed-form expression for the log-likelihood. The first term is a constant with respect to $\\theta$ since $\\sigma^2$ is known, but it is retained as per the problem instructions.\n\nNext, we derive the gradient of the log-likelihood with respect to $\\theta$, denoted $\\nabla_{\\theta}\\ell(\\theta)$.\nWe differentiate the expression for $\\ell(\\theta)$ with respect to the parameter vector $\\theta \\in \\mathbb{R}^p$.\n$$\\nabla_{\\theta}\\ell(\\theta) = \\frac{\\partial}{\\partial\\theta} \\left( -\\frac{nm}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2 \\right)$$\nThe first term is constant with respect to $\\theta$, so its gradient is the zero vector.\n$$\\nabla_{\\theta}\\ell(\\theta) = - \\frac{1}{2\\sigma^2} \\frac{\\partial}{\\partial\\theta} \\left( \\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2 \\right)$$\nThe gradient operator can be moved inside the summation:\n$$\\nabla_{\\theta}\\ell(\\theta) = - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial\\theta} \\|y_i - h x(t_i;\\theta)\\|_2^2$$\nWe use the notation for the residual vector $r_i(\\theta) = y_i - h x(t_i;\\theta)$. The term to differentiate is $\\|r_i(\\theta)\\|_2^2 = r_i(\\theta)^T r_i(\\theta)$.\nUsing the chain rule for vector calculus, the gradient of a squared norm is $\\frac{\\partial}{\\partial\\theta} (u^T u) = 2 \\left(\\frac{\\partial u}{\\partial\\theta}\\right)^T u$, where $\\frac{\\partial u}{\\partial\\theta}$ is the Jacobian matrix of the vector $u$ with respect to the vector $\\theta$.\nLet us apply this rule:\n$$\\frac{\\partial}{\\partial\\theta} \\|r_i(\\theta)\\|_2^2 = 2 \\left(\\frac{\\partial r_i(\\theta)}{\\partial\\theta}\\right)^T r_i(\\theta)$$\nNow we must find the Jacobian of the residual, $\\frac{\\partial r_i(\\theta)}{\\partial\\theta}$.\n$$\\frac{\\partial r_i(\\theta)}{\\partial\\theta} = \\frac{\\partial}{\\partial\\theta} (y_i - h x(t_i;\\theta)) = - \\frac{\\partial}{\\partial\\theta} (h x(t_i;\\theta)) = -h \\frac{\\partial x(t_i;\\theta)}{\\partial\\theta}$$\nThe problem provides the definition for the state sensitivity matrix: $G_i(\\theta) = \\frac{\\partial x(t_i;\\theta)}{\\partial\\theta}$. This is a $d \\times p$ matrix.\nSubstituting this into the Jacobian of the residual gives:\n$$\\frac{\\partial r_i(\\theta)}{\\partial\\theta} = -h G_i(\\theta)$$\nThis is an $m \\times p$ matrix. Now substitute this back into the expression for the gradient of the squared norm:\n$$\\frac{\\partial}{\\partial\\theta} \\|r_i(\\theta)\\|_2^2 = 2 (-h G_i(\\theta))^T r_i(\\theta) = -2 (G_i(\\theta)^T h^T) r_i(\\theta)$$\nThis is a $p \\times 1$ column vector, which is the correct dimension for the gradient.\nFinally, we substitute this result back into the expression for $\\nabla_{\\theta}\\ell(\\theta)$:\n$$\\nabla_{\\theta}\\ell(\\theta) = - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( -2 G_i(\\theta)^T h^T r_i(\\theta) \\right)$$\n$$\\nabla_{\\theta}\\ell(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} G_i(\\theta)^T h^T r_i(\\theta)$$\nThis is the final expression for the gradient of the log-likelihood, expressed in terms of the residuals $r_i(\\theta)$ and sensitivities $G_i(\\theta)$ as requested.\n\nThe pair of results, the log-likelihood and its gradient, are:\nLog-likelihood: $\\ell(\\theta) = -\\frac{nm}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\|r_i(\\theta)\\|_2^2$\nGradient: $\\nabla_{\\theta}\\ell(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} G_i(\\theta)^T h^T r_i(\\theta)$\nwhere $r_{i}(\\theta) = y_{i} - h\\,x(t_{i};\\theta)$.", "answer": "$$\n\\boxed{\n\\pmatrix{\n-\\frac{nm}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} \\|y_i - h x(t_i;\\theta)\\|_2^2 & \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} G_i(\\theta)^T h^T (y_i - h x(t_i;\\theta))\n}\n}\n$$", "id": "2628012"}, {"introduction": "Before applying complex inference algorithms, it is vital to ensure that a model's parameters can, in principle, be uniquely determined from the proposed experiments. This exercise [@problem_id:2627935] delves into the critical concept of structural non-identifiability, a common pitfall where distinct parameter combinations produce identical observable outputs, making them impossible to distinguish. By analyzing a simple parallel reaction network, you will learn to diagnose this issue and use reparameterization as a powerful strategy to isolate the identifiable parameter groups, ensuring a well-posed inference problem.", "problem": "Consider the parallel, irreversible, elementary reactions $A \\to B$ and $A \\to C$ with first-order rate constants $k_{1}>0$ and $k_{2}>0$. The initial amount $A(0)$ is $A_{0}>0$. You observe only the time course of the reactant $A$ at times $t_{i}\\ge 0$, $i=1,\\dots,n$, with an additive Gaussian measurement model: $y_{i}=A(t_{i})+\\varepsilon_{i}$ where the errors are independent and identically distributed (i.i.d.) $\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})$ with known variance $\\sigma^{2}>0$. \n\nStarting from the law of mass action and Bayesâ€™ theorem, derive the solution for $A(t)$, write the likelihood for $y_{1:n}=(y_{1},\\dots,y_{n})$ given $(k_{1},k_{2})$, and determine which combinations of $(k_{1},k_{2})$ are structurally identifiable from measurements of $A$ only. Then, construct a smooth, one-to-one reparameterization $(k_{1},k_{2})\\mapsto (\\kappa,\\phi)$ with $\\kappa>0$ and $\\phi\\in(0,1)$ such that the likelihood depends only on $\\kappa$, while $\\phi$ is not informed by the data when only $A$ is measured. Provide also the inverse mapping $(\\kappa,\\phi)\\mapsto (k_{1},k_{2})$ and the absolute value of the Jacobian determinant of this change of variables suitable for Bayesian change-of-variables calculations. \n\nYour final reported answer must be only the reparameterization $(\\kappa,\\phi)$ as analytic expressions in terms of $(k_{1},k_{2})$, written as a single row vector. Do not include any units. No numerical rounding is required for this problem.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard exercise in chemical kinetics and parameter estimation. I will proceed with its solution.\n\nFirst, we must derive the analytical solution for the concentration of species $A$ as a function of time, $A(t)$. The system comprises two parallel, irreversible, elementary first-order reactions:\n$$A \\xrightarrow{k_1} B$$\n$$A \\xrightarrow{k_2} C$$\nAccording to the law of mass action, the rate of consumption of $A$ is the sum of the rates of the two reactions. The rate of the first reaction is $r_1 = k_1 A(t)$ and the rate of the second is $r_2 = k_2 A(t)$. The differential equation governing the concentration of $A$ is therefore:\n$$\\frac{dA}{dt} = -r_1 - r_2 = -k_1 A(t) - k_2 A(t) = -(k_1 + k_2) A(t)$$\nThis is a first-order linear ordinary differential equation with the initial condition $A(0) = A_0$. We can solve it by separation of variables:\n$$\\frac{dA}{A} = -(k_1 + k_2) dt$$\nIntegrating both sides from time $t=0$ to a general time $t$ yields:\n$$\\int_{A_0}^{A(t)} \\frac{1}{A'} dA' = -\\int_{0}^{t} (k_1 + k_2) d\\tau$$\n$$\\ln(A(t)) - \\ln(A_0) = -(k_1 + k_2) t$$\n$$\\ln\\left(\\frac{A(t)}{A_0}\\right) = -(k_1 + k_2) t$$\nExponentiating both sides gives the solution for $A(t)$:\n$$A(t) = A_0 \\exp(-(k_1 + k_2)t)$$\n\nNext, we construct the likelihood function for the observed data $y_{1:n} = (y_1, \\dots, y_n)$ given the parameters $k_1$ and $k_2$. The measurement model is $y_i = A(t_i) + \\varepsilon_i$, where the measurement errors $\\varepsilon_i$ are independent and identically distributed according to a normal distribution with mean $0$ and known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that each observation $y_i$ is also normally distributed with mean $A(t_i)$ and variance $\\sigma^2$, that is, $y_i \\sim \\mathcal{N}(A(t_i), \\sigma^2)$. The probability density function for a single observation $y_i$ is:\n$$p(y_i | k_1, k_2, A_0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - A(t_i))^2}{2\\sigma^2}\\right)$$\nSubstituting the expression for $A(t_i)$:\n$$p(y_i | k_1, k_2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - A_0 \\exp(-(k_1 + k_2)t_i))^2}{2\\sigma^2}\\right)$$\nSince the measurements are independent, the likelihood of the entire dataset $y_{1:n}$ is the product of the individual probability densities:\n$$L(k_1, k_2 | y_{1:n}) = p(y_{1:n} | k_1, k_2) = \\prod_{i=1}^{n} p(y_i | k_1, k_2)$$\n$$L(k_1, k_2 | y_{1:n}) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left(y_i - A_0 \\exp(-(k_1 + k_2)t_i)\\right)^2\\right)$$\n\nNow, we must determine which combinations of parameters are structurally identifiable. Structural identifiability relates to whether the parameters can be uniquely determined from noise-free observations of the system's outputs. Examining the solution $A(t) = A_0 \\exp(-(k_1 + k_2)t)$ and the corresponding likelihood function, it is evident that both depend on the rate constants $k_1$ and $k_2$ only through their sum, $k_1 + k_2$. Any two distinct pairs of rate constants, say $(k_1, k_2)$ and $(k'_1, k'_2)$, that satisfy $k_1 + k_2 = k'_1 + k'_2$ will produce the identical time course for $A(t)$ and thus yield the same likelihood value. Consequently, it is impossible to distinguish between these parameter sets using only measurements of species $A$. The individual parameters $k_1$ and $k_2$ are structurally non-identifiable. The only identifiable parameter combination is the effective rate constant, which is the sum $\\kappa = k_1 + k_2$.\n\nThe task is to find a reparameterization $(k_1, k_2) \\mapsto (\\kappa, \\phi)$ that separates the identifiable part ($\\kappa$) from the non-identifiable part ($\\phi$).\nLet us define the identifiable parameter $\\kappa$ as the sum of the rate constants:\n$$\\kappa = k_1 + k_2$$\nSince $k_1 > 0$ and $k_2 > 0$, it follows that $\\kappa > 0$.\nThe non-identifiable parameter $\\phi$ should capture the distribution of the total reaction flux between the two pathways. A natural choice is the fraction of the total rate that corresponds to the first reaction:\n$$\\phi = \\frac{k_1}{k_1 + k_2} = \\frac{k_1}{\\kappa}$$\nSince $k_1 > 0$ and $k_2 > 0$, we have $0 < k_1 < k_1 + k_2$, which implies that $0 < \\phi < 1$. The reparameterization is thus given by the mapping:\n$$(k_1, k_2) \\mapsto \\left(\\kappa(k_1, k_2), \\phi(k_1, k_2)\\right) = \\left(k_1 + k_2, \\frac{k_1}{k_1 + k_2}\\right)$$\nThis mapping is smooth and one-to-one for $k_1, k_2 > 0$. In terms of these new parameters, the likelihood function becomes:\n$$L(\\kappa | y_{1:n}) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left(y_i - A_0 \\exp(-\\kappa t_i)\\right)^2\\right)$$\nAs required, the likelihood depends only on $\\kappa$, and $\\phi$ is not informed by the data of $A(t)$.\n\nThe inverse mapping $(\\kappa, \\phi) \\mapsto (k_1, k_2)$ is found by solving for $k_1$ and $k_2$:\nFrom $\\phi = k_1/\\kappa$, we get $k_1 = \\kappa\\phi$.\nSubstituting this into $\\kappa = k_1 + k_2$ gives $\\kappa = \\kappa\\phi + k_2$, which leads to $k_2 = \\kappa - \\kappa\\phi = \\kappa(1-\\phi)$.\nSo, the inverse mapping is:\n$$(k_1, k_2) = (\\kappa\\phi, \\kappa(1-\\phi))$$\n\nFinally, for Bayesian change-of-variables, we need the Jacobian determinant of the transformation from the new parameters $(\\kappa, \\phi)$ to the old parameters $(k_1, k_2)$. The Jacobian matrix $J$ of the mapping $(\\kappa, \\phi) \\mapsto (k_1, k_2)$ is:\n$$J = \\begin{pmatrix} \\frac{\\partial k_1}{\\partial \\kappa} & \\frac{\\partial k_1}{\\partial \\phi} \\\\ \\frac{\\partial k_2}{\\partial \\kappa} & \\frac{\\partial k_2}{\\partial \\phi} \\end{pmatrix} = \\begin{pmatrix} \\phi & \\kappa \\\\ 1 - \\phi & -\\kappa \\end{pmatrix}$$\nThe determinant of this matrix is:\n$$\\det(J) = (\\phi)(-\\kappa) - (\\kappa)(1-\\phi) = -\\kappa\\phi - \\kappa + \\kappa\\phi = -\\kappa$$\nThe absolute value of the Jacobian determinant is $|\\det(J)| = |-\\kappa| = \\kappa$, since $\\kappa > 0$.\n\nThe problem asks for the reparameterization $(\\kappa, \\phi)$ as analytic expressions in terms of $(k_1, k_2)$. These are the expressions derived above.", "answer": "$$\\boxed{\\begin{pmatrix} k_1 + k_2 & \\frac{k_1}{k_1 + k_2} \\end{pmatrix}}$$", "id": "2627935"}, {"introduction": "While the gradient of the log-posterior is essential, its direct computation via forward sensitivity analysis (as implied in our first practice) can become prohibitively expensive for models with many parameters. This advanced exercise [@problem_id:2627942] introduces the continuous adjoint method, a powerful technique that computes the full gradient at a computational cost that is remarkably independent of the number of parameters. You will derive the adjoint system of ODEs from first principles, learning how to structure an efficient and scalable computation for large-scale kinetic models.", "problem": "Consider a well-mixed isothermal reaction network with species concentration vector $x(t) \\in \\mathbb{R}^{n}$ evolving under deterministic mass-action kinetics, written abstractly as the ordinary differential equation (ODE) $\\dot{x}(t) = f\\!\\left(x(t), \\theta, t\\right)$ with initial condition $x(t_{0}) = x_{0}(\\theta)$, where $\\theta \\in \\mathbb{R}^{p}$ denotes kinetic parameters. Measurements $y_{i} \\in \\mathbb{R}^{m}$ are acquired at fixed observation times $t_{i}$, $i = 1,\\dots,N$, via a smooth observation map $h:\\mathbb{R}^{n} \\times \\mathbb{R}^{p} \\to \\mathbb{R}^{m}$ with additive independent Gaussian noise of known positive definite covariances $R_{i} \\in \\mathbb{R}^{m \\times m}$. Assume a Gaussian prior on $\\theta$ with mean $\\mu \\in \\mathbb{R}^{p}$ and positive definite covariance $\\Sigma \\in \\mathbb{R}^{p \\times p}$. Define the negative log-posterior (up to a constant independent of $\\theta$) as\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{2}\\,( \\theta - \\mu )^{\\top}\\Sigma^{-1}( \\theta - \\mu ) \\;+\\; \\sum_{i=1}^{N} \\frac{1}{2}\\,\\big(y_{i} - h(x(t_{i}), \\theta)\\big)^{\\top} R_{i}^{-1} \\big(y_{i} - h(x(t_{i}), \\theta)\\big).\n$$\nYour goal is to compute $\\nabla_{\\theta} \\mathcal{L}(\\theta)$ without forming forward sensitivities $\\partial x / \\partial \\theta$. Starting from first principles of constrained calculus of variations with a Lagrange multiplier function $\\lambda(t) \\in \\mathbb{R}^{n}$ enforcing the ODE constraint, derive the continuous-time adjoint equations for $\\lambda(t)$, including the terminal condition and the jump conditions induced by the discrete observation times $t_{i}$, and use them to express $\\nabla_{\\theta} \\mathcal{L}(\\theta)$ solely in terms of $\\lambda(t)$, $f$, $h$, $x(t)$, and known quantities. You may assume that $f$ and $h$ are continuously differentiable in their arguments and that $t_{i}$ do not depend on $\\theta$. Clearly state how to handle the event times $t_{i}$ in the adjoint dynamics. Provide, as your final answer, a single closed-form analytic expression for $\\nabla_{\\theta} \\mathcal{L}(\\theta)$ in terms of an integral over $[t_{0}, t_{N}]$ and sums over $i = 1,\\dots,N$. No numerical evaluation is required and no units are needed for the final expression.", "solution": "The problem requires the derivation of the gradient of the negative log-posterior, $\\nabla_{\\theta} \\mathcal{L}(\\theta)$, with respect to the parameters $\\theta$, using the continuous adjoint method. This approach avoids the computationally expensive step of solving the forward sensitivity equations for $\\frac{\\partial x}{\\partial \\theta}$. The problem is well-posed and self-contained, so we proceed directly to the derivation.\n\nThe objective function to be differentiated is:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2}( \\theta - \\mu )^{\\top}\\Sigma^{-1}( \\theta - \\mu ) + \\sum_{i=1}^{N} \\frac{1}{2}\\big(y_{i} - h(x(t_{i}), \\theta)\\big)^{\\top} R_{i}^{-1} \\big(y_{i} - h(x(t_{i}), \\theta)\\big)\n$$\nThis function depends on $\\theta$ both explicitly and implicitly through the state vector $x(t)$, which is a solution to the ordinary differential equation (ODE) $\\dot{x}(t) = f(x(t), \\theta, t)$ with initial condition $x(t_0) = x_0(\\theta)$.\n\nTo handle the ODE constraint, we introduce a time-dependent Lagrange multiplier vector $\\lambda(t) \\in \\mathbb{R}^{n}$, also known as the adjoint state. We define an augmented functional, $\\mathcal{J}$, which incorporates the system dynamics over the time interval $[t_0, t_N]$:\n$$\n\\mathcal{J} = \\mathcal{L}(\\theta) + \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\left( f(x(t), \\theta, t) - \\dot{x}(t) \\right) dt\n$$\nSince the ODE constraint is satisfied for the state trajectory $x(t)$, the integral term evaluates to zero for any choice of $\\lambda(t)$. Therefore, $\\mathcal{J} = \\mathcal{L}(\\theta)$, and their gradients with respect to $\\theta$ are equal, i.e., $\\nabla_{\\theta} \\mathcal{J} = \\nabla_{\\theta} \\mathcal{L}(\\theta)$. We will compute the total variation of $\\mathcal{J}$ with respect to a small perturbation $\\delta \\theta$ in the parameters. This perturbation induces a variation $\\delta x(t)$ in the state trajectory.\n\nThe total variation $\\delta \\mathcal{J}$ is given by:\n$$\n\\delta \\mathcal{J} = \\delta \\mathcal{L} + \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\left( \\frac{\\partial f}{\\partial x} \\delta x(t) + \\frac{\\partial f}{\\partial \\theta} \\delta \\theta - \\delta \\dot{x}(t) \\right) dt\n$$\nThe variation of the objective function $\\delta \\mathcal{L}$ is:\n$$\n\\delta \\mathcal{L} = (\\theta - \\mu)^{\\top}\\Sigma^{-1}\\delta\\theta - \\sum_{i=1}^{N} \\big(y_{i} - h(x(t_i), \\theta)\\big)^{\\top} R_{i}^{-1} \\left( \\frac{\\partial h}{\\partial x}\\bigg|_{t_i} \\delta x(t_i) + \\frac{\\partial h}{\\partial \\theta}\\bigg|_{t_i} \\delta \\theta \\right)\n$$\nThe term in $\\delta \\mathcal{J}$ involving $\\delta \\dot{x}(t)$ is handled using integration by parts. Since the discrete observations at times $t_i$ introduce discontinuities in the subsequent definition of the adjoint dynamics, we split the integral over the subintervals $[t_i, t_{i+1}]$ (with $t_0 < t_1 < \\dots < t_N$):\n$$\n- \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\delta \\dot{x}(t) dt = - \\sum_{i=0}^{N-1} \\int_{t_i}^{t_{i+1}} \\lambda(t)^{\\top} \\delta \\dot{x}(t) dt\n$$\nFor each subinterval, integration by parts gives:\n$$\n- \\int_{t_i}^{t_{i+1}} \\lambda(t)^{\\top} \\delta \\dot{x}(t) dt = - \\left[ \\lambda(t)^{\\top} \\delta x(t) \\right]_{t_i^+}^{t_{i+1}^-} + \\int_{t_i}^{t_{i+1}} \\dot{\\lambda}(t)^{\\top} \\delta x(t) dt\n$$\nwhere $t_i^+$ and $t_{i+1}^-$ denote the limits from the right and left, respectively. Since the state trajectory $x(t)$ is continuous, $\\delta x(t_i) = \\delta x(t_i^-) = \\delta x(t_i^+)$. Summing over $i=0, \\dots, N-1$ yields:\n$$\n- \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\delta \\dot{x}(t) dt = - \\lambda(t_N^-)^{\\top} \\delta x(t_N) + \\lambda(t_0^+)^{\\top} \\delta x(t_0) + \\sum_{i=1}^{N-1} \\left( \\lambda(t_i^+)^{\\top} - \\lambda(t_i^-)^{\\top} \\right) \\delta x(t_i) + \\int_{t_0}^{t_N} \\dot{\\lambda}(t)^{\\top} \\delta x(t) dt\n$$\nNow we collect all terms in $\\delta \\mathcal{J}$ and group them by the variations $\\delta\\theta$ and $\\delta x(t)$:\n\\begin{align*}\n\\delta \\mathcal{J} = &\\left[ (\\theta - \\mu)^{\\top}\\Sigma^{-1} - \\sum_{i=1}^{N} \\big(y_i - h_i\\big)^{\\top} R_i^{-1} \\frac{\\partial h_i}{\\partial \\theta} + \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\frac{\\partial f}{\\partial \\theta} dt \\right] \\delta\\theta \\\\\n&+ \\int_{t_0}^{t_N} \\left( \\dot{\\lambda}(t)^{\\top} + \\lambda(t)^{\\top} \\frac{\\partial f}{\\partial x} \\right) \\delta x(t) dt \\\\\n&+ \\sum_{i=1}^{N-1} \\left[ \\lambda(t_i^+)^{\\top} - \\lambda(t_i^-)^{\\top} - \\big(y_i - h_i\\big)^{\\top} R_i^{-1} \\frac{\\partial h_i}{\\partial x} \\right] \\delta x(t_i) \\\\\n&+ \\left[ - \\lambda(t_N^-)^{\\top} - \\big(y_N - h_N\\big)^{\\top} R_N^{-1} \\frac{\\partial h_N}{\\partial x} \\right] \\delta x(t_N) \\\\\n&+ \\lambda(t_0^+)^{\\top} \\delta x(t_0)\n\\end{align*}\nwhere $h_i = h(x(t_i), \\theta)$ and the partial derivatives are evaluated at the corresponding states and parameters.\n\nThe essence of the adjoint method is to choose $\\lambda(t)$ such that all terms involving the unknown state variation $\\delta x(t)$ vanish. This gives us the defining equations for the adjoint state.\n\n1.  **Adjoint ODE**: To eliminate the integral term for any $\\delta x(t)$ in the open intervals $(t_i, t_{i+1})$, the integrand must be zero.\n    $$\n    \\dot{\\lambda}(t)^{\\top} + \\lambda(t)^{\\top} \\frac{\\partial f(x(t), \\theta, t)}{\\partial x} = 0 \\implies \\dot{\\lambda}(t) = - \\left(\\frac{\\partial f(x(t), \\theta, t)}{\\partial x}\\right)^{\\top} \\lambda(t)\n    $$\n    This linear ODE is solved backwards in time.\n\n2.  **Terminal Condition**: To eliminate the term involving $\\delta x(t_N)$, we set its coefficient to zero.\n    $$\n    - \\lambda(t_N^-)^{\\top} - \\big(y_N - h(x(t_N), \\theta)\\big)^{\\top} R_N^{-1} \\frac{\\partial h(x(t_N), \\theta)}{\\partial x} = 0\n    $$\n    Transposing gives the terminal condition for the backward integration of $\\lambda(t)$:\n    $$\n    \\lambda(t_N) = - \\left(\\frac{\\partial h(x(t_N), \\theta)}{\\partial x}\\right)^{\\top} R_N^{-1} \\big(y_N - h(x(t_N), \\theta)\\big)\n    $$\n    We denote $\\lambda(t_N^-)$ simply as $\\lambda(t_N)$.\n\n3.  **Jump Conditions**: For each intermediate observation time $t_i$ ($i=1, \\dots, N-1$), we eliminate the coefficient of $\\delta x(t_i)$.\n    $$\n    \\lambda(t_i^+)^{\\top} - \\lambda(t_i^-)^{\\top} - \\big(y_i - h(x(t_i), \\theta)\\big)^{\\top} R_i^{-1} \\frac{\\partial h(x(t_i), \\theta)}{\\partial x} = 0\n    $$\n    This defines the jump in the adjoint state. When integrating backwards in time, upon reaching $t_i$, the state is $\\lambda(t_i^+)$. The value for the next integration segment, $\\lambda(t_i^-)$, is given by:\n    $$\n    \\lambda(t_i^-) = \\lambda(t_i^+) + \\left(\\frac{\\partial h(x(t_i), \\theta)}{\\partial x}\\right)^{\\top} R_i^{-1} \\big(y_i - h(x(t_i), \\theta)\\big)\n    $$\n\nWith $\\lambda(t)$ chosen to satisfy these conditions, all terms containing $\\delta x(t)$ (for $t \\in (t_0, t_N]$) vanish. The only remaining term involving a state variation is at the initial time $t_0$: $\\lambda(t_0^+)^{\\top} \\delta x(t_0)$. The variation of the initial condition is $\\delta x(t_0) = \\frac{\\partial x_0(\\theta)}{\\partial \\theta} \\delta\\theta$. Substituting this, the total variation $\\delta \\mathcal{J}$ simplifies to:\n$$\n\\delta \\mathcal{J} = \\left[ (\\theta - \\mu)^{\\top}\\Sigma^{-1} - \\sum_{i=1}^{N} \\big(y_i - h_i\\big)^{\\top} R_i^{-1} \\frac{\\partial h_i}{\\partial \\theta} + \\int_{t_0}^{t_N} \\lambda(t)^{\\top} \\frac{\\partial f}{\\partial \\theta} dt + \\lambda(t_0)^{\\top} \\frac{\\partial x_0}{\\partial \\theta} \\right] \\delta\\theta\n$$\nHere, $\\lambda(t_0)$ is the value $\\lambda(t_0^+)$ obtained at the end of the backward integration.\nThe expression inside the brackets is $(\\nabla_{\\theta} \\mathcal{J})^{\\top} = (\\nabla_{\\theta} \\mathcal{L})^{\\top}$. Transposing this expression gives the gradient vector $\\nabla_{\\theta} \\mathcal{L}(\\theta)$:\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\Sigma^{-1}(\\theta - \\mu) - \\sum_{i=1}^{N} \\left(\\frac{\\partial h(x(t_i), \\theta)}{\\partial \\theta}\\right)^{\\top} R_i^{-1} \\big(y_i - h(x(t_i), \\theta)\\big) + \\int_{t_0}^{t_N} \\left(\\frac{\\partial f(x(t), \\theta, t)}{\\partial \\theta}\\right)^{\\top} \\lambda(t) dt + \\left(\\frac{\\partial x_0(\\theta)}{\\partial \\theta}\\right)^{\\top} \\lambda(t_0)\n$$\nThis is the final expression for the gradient, expressed in terms of the adjoint state $\\lambda(t)$ and known quantities, as required.", "answer": "$$\n\\boxed{\\Sigma^{-1}(\\theta - \\mu) + \\left( \\frac{\\partial x_0}{\\partial \\theta} \\right)^{\\top} \\lambda(t_0) + \\int_{t_0}^{t_N} \\left(\\frac{\\partial f}{\\partial \\theta}\\right)^{\\top} \\lambda(t) dt - \\sum_{i=1}^{N} \\left(\\frac{\\partial h}{\\partial \\theta}\\bigg|_{t_i}\\right)^{\\top} R_i^{-1} (y_i - h(x(t_i), \\theta))}\n$$", "id": "2627942"}]}