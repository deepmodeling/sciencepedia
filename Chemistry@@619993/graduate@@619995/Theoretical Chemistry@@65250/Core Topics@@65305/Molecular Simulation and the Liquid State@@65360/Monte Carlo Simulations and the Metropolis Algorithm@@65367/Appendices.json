{"hands_on_practices": [{"introduction": "A successful Monte Carlo simulation relies on the correct implementation of its geometric foundations. This first practice focuses on a crucial aspect of simulations using Periodic Boundary Conditions (PBC) and the Minimum Image Convention (MIC). By determining the maximum allowable displacement for a trial move, you will gain hands-on experience with the practical constraints that ensure algorithmic correctness and prevent common simulation artifacts [@problem_id:2788157].", "problem": "A homogeneous classical fluid of identical particles is simulated with the Metropolis Monte Carlo (MMC) algorithm in a three-dimensional cubic box of side length $L$ with Periodic Boundary Conditions (PBC). Pair interactions are finite-ranged, and the system employs a spherical cutoff of radius $r_{c}$ together with the Minimum Image Convention (MIC), which defines pair separations by wrapping each Cartesian component of the separation into the interval $\\left(-\\frac{L}{2}, \\frac{L}{2}\\right]$. The number density is $\\rho$, the total number of particles is $N$, and the box length satisfies the definition of number density $\\,\\rho = \\frac{N}{L^{3}}\\,$. The cutoff satisfies $\\,r_{c} < \\frac{L}{2}\\,$ so that the MIC is unambiguous.\n\nThe MMC trial move for a single, randomly selected particle consists of adding a random displacement vector $\\boldsymbol{\\Delta}$ with magnitude bounded by a tunable parameter $\\delta_{\\max}$, i.e., $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$. To ensure that no pairwise interaction beyond the nearest periodic image can ever be required when evaluating the energy change of a single-particle trial move, you adopt the following translationally invariant protocol: before proposing the displacement for the selected particle, you shift all particle positions by a common vector (which is permissible under PBC) so that the selected particle is exactly at the geometric center of the primary simulation cell. You then apply the displacement $\\boldsymbol{\\Delta}$ and wrap coordinates back into the primary cell as needed.\n\nUnder this protocol and using only the foundational definitions above, determine the largest displacement magnitude $\\delta_{\\max}$ such that, after any single-particle trial move, the entire interaction sphere of radius $r_{c}$ centered on the trial position is guaranteed to lie within the region where the MIC selects the central image uniquely (so that, by construction, no interaction beyond the cutoff with a non-nearest image is ever needed). Express your answer as a closed-form analytic expression in terms of $N$, $\\rho$, and $r_{c}$. The final answer must be a single analytic expression. No rounding is required, and no numerical values are provided or needed.", "solution": "The problem statement has been subjected to rigorous validation and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a solvable problem in the domain of theoretical chemistry and computational physics. We shall proceed with the derivation.\n\nThe system is a cubic simulation box of side length $L$ with Periodic Boundary Conditions (PBC). The coordinates are defined within the primary simulation cell, which we take to be the cube $C$ centered at the origin, with vertices at $(\\pm \\frac{L}{2}, \\pm \\frac{L}{2}, \\pm \\frac{L}{2})$. Thus, any position vector $\\mathbf{r} = (x, y, z)$ within this cell has components satisfying $-\\frac{L}{2} < x \\le \\frac{L}{2}$, and similarly for $y$ and $z$.\n\nThe problem describes a specific protocol for a Metropolis Monte Carlo (MMC) trial move. For a randomly selected particle, say particle $i$, all particle positions in the system are shifted by a common vector such that particle $i$ is relocated to the geometric center of the primary cell, which is the origin $\\mathbf{0} = (0, 0, 0)$. Then, a random displacement vector $\\boldsymbol{\\Delta}$ is added to the position of particle $i$, resulting in its new trial position $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta}$. The magnitude of the displacement is bounded by $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$.\n\nThe core requirement is to find the largest value of $\\delta_{\\max}$ which guarantees that \"the entire interaction sphere of radius $r_{c}$ centered on the trial position is guaranteed to lie within the region where the MIC selects the central image uniquely\". The \"region where the MIC selects the central image uniquely\" refers to the primary simulation cell $C$ itself. For any position vector $\\mathbf{p}$ inside this cell, the Minimum Image Convention (MIC) returns $\\mathbf{p}$ as its own nearest image vector. If any part of the interaction sphere were to lie outside this primary cell, one would need to consider distances to particles whose central images are near the opposite face of the box, which is what the protocol is designed to avoid.\n\nTherefore, the condition is that the sphere of radius $r_c$ centered at the new trial position $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta}$ must be fully contained within the primary cell $C = [-\\frac{L}{2}, \\frac{L}{2}]^3$.\n\nLet the trial position be $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta} = (\\Delta_x, \\Delta_y, \\Delta_z)$. A sphere $S$ of radius $r_c$ is centered at this point. For this sphere to be contained within the cube defined by $-\\frac{L}{2} \\le x,y,z \\le \\frac{L}{2}$, the maximum and minimum extent of the sphere in each Cartesian direction must fall within this range.\n\nConsider the $x$-component. The extent of the sphere along the $x$-axis is from $\\Delta_x - r_c$ to $\\Delta_x + r_c$. We must satisfy the following two inequalities:\n$$\n\\Delta_x + r_c \\le \\frac{L}{2}\n$$\n$$\n\\Delta_x - r_c \\ge -\\frac{L}{2}\n$$\nThese two inequalities can be combined into a single condition on the absolute value of the component $\\Delta_x$:\n$$\n|\\Delta_x| \\le \\frac{L}{2} - r_c\n$$\nThis condition must hold for all three Cartesian components, so we require $|\\Delta_\\alpha| \\le \\frac{L}{2} - r_c$ for $\\alpha \\in \\{x, y, z\\}$.\n\nThis constraint must be satisfied for *any* possible trial displacement vector $\\boldsymbol{\\Delta}$ such that $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$. To guarantee this, we must consider the worst-case scenario, which is the displacement that produces the largest possible magnitude for one of its components, $|\\Delta_\\alpha|$. Given the constraint $|\\boldsymbol{\\Delta}| = \\sqrt{\\Delta_x^2 + \\Delta_y^2 + \\Delta_z^2} \\le \\delta_{\\max}$, the maximum value that any single component, for instance $|\\Delta_x|$, can attain is $\\delta_{\\max}$. This occurs when the displacement vector is aligned with that coordinate axis, e.g., $\\boldsymbol{\\Delta} = (\\delta_{\\max}, 0, 0)$.\n\nTo ensure our condition holds for all valid displacements, we must impose it on this worst-case magnitude. Substituting $|\\Delta_\\alpha| = \\delta_{\\max}$ into the derived inequality gives:\n$$\n\\delta_{\\max} \\le \\frac{L}{2} - r_c\n$$\nThe problem asks for the largest displacement magnitude $\\delta_{\\max}$ that satisfies this guarantee. This is the equality limit:\n$$\n\\delta_{\\max} = \\frac{L}{2} - r_c\n$$\nThe problem statement requires the final answer to be expressed in terms of the number of particles $N$, the number density $\\rho$, and the cutoff radius $r_c$. The number density is given by the definition $\\rho = \\frac{N}{L^3}$. We can solve for the box length $L$:\n$$\nL^3 = \\frac{N}{\\rho} \\implies L = \\left(\\frac{N}{\\rho}\\right)^{1/3}\n$$\nSubstituting this expression for $L$ into our equation for $\\delta_{\\max}$ yields the final result:\n$$\n\\delta_{\\max} = \\frac{1}{2}\\left(\\frac{N}{\\rho}\\right)^{1/3} - r_c\n$$\nThis expression is the largest possible value for the maximum displacement parameter $\\delta_{\\max}$ consistent with the stated protocol and geometric constraints. The initial given condition $r_c < L/2$ ensures that $\\delta_{\\max}$ is a positive, physically meaningful quantity.", "answer": "$$\n\\boxed{\\frac{1}{2}\\left(\\frac{N}{\\rho}\\right)^{1/3} - r_{c}}\n$$", "id": "2788157"}, {"introduction": "Beyond correctness, the efficiency of a Monte Carlo simulation is paramount. This exercise challenges you to compare two common proposal strategies—uniform and Gaussian displacements—in the context of a dense fluid where interactions are dominated by steep repulsive forces. This is not just a matter of preference; the choice profoundly impacts how quickly the simulation explores the configuration space, and you will find that the optimal choice depends critically on the physical nature of the system under study [@problem_id:2788218].", "problem": "Consider Metropolis Monte Carlo sampling of a dense Lennard–Jones fluid in the canonical ensemble (constant number of particles $N$, volume $V$, and temperature $T$). The potential energy is the standard pairwise Lennard–Jones form,\n$$\nU(\\mathbf{r}^{N})=4\\varepsilon\\sum_{i<j}\\left[\\left(\\frac{\\sigma}{r_{ij}}\\right)^{12}-\\left(\\frac{\\sigma}{r_{ij}}\\right)^{6}\\right],\n$$\nwith $r_{ij}=|\\mathbf{r}_{i}-\\mathbf{r}_{j}|$, at a number density high enough that the short-range repulsive part dominates local energy changes. Consider single-particle random-walk Metropolis proposals for atomic translations,\n$$\n\\mathbf{r}_{k}\\mapsto \\mathbf{r}_{k}+\\Delta \\mathbf{r},\\quad \\Delta \\mathbf{r}=(\\Delta x,\\Delta y,\\Delta z),\n$$\nwith two symmetric proposal classes:\n\n- Uniform-cube: each component is independent and uniformly distributed on $[-\\delta,\\delta]$.\n\n- Gaussian: each component is independent and normally distributed with mean $0$ and variance $\\sigma^{2}$.\n\nThe Metropolis acceptance probability for a proposed move from state $\\mathbf{x}$ to $\\mathbf{y}$ is\n$$\n\\alpha(\\mathbf{x}\\to\\mathbf{y})=\\min\\left\\{1,\\exp\\left[-\\beta\\left(U(\\mathbf{y})-U(\\mathbf{x})\\right)\\right]\\right\\},\n$$\nwith $\\beta=1/(k_{\\mathrm{B}}T)$ and symmetric proposals $q(\\mathbf{y}\\mid \\mathbf{x})=q(\\mathbf{x}\\mid \\mathbf{y})$. Suppose the proposals are tuned so that the mean squared step size is held fixed across the two classes,\n$$\n\\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\right]\\equiv \\mathbb{E}\\left[\\Delta x^{2}+\\Delta y^{2}+\\Delta z^{2}\\right]=s^{2},\n$$\nwith the expectation taken over the proposal distribution. Define the expected squared jumping distance per attempt,\n$$\nJ\\equiv \\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\,\\alpha\\right],\n$$\nwhere the average is over the joint distribution of current configurations and proposals. Assume observables of interest are smooth functions of the configuration and that, in this dense regime, the local energy increment $\\Delta U$ for a single-particle displacement is typically a rapidly increasing function of the displacement magnitude $|\\Delta \\mathbf{r}|$ due to the steep repulsive core, so that the acceptance factor is a rapidly decreasing function of $|\\Delta \\mathbf{r}|$.\n\nWhich of the following statements are correct under these conditions?\n\nA. In a dense Lennard–Jones fluid at fixed inverse temperature $\\beta$, when two symmetric random-walk Metropolis proposals are tuned so that $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ is identical, the uniform-cube proposal yields a higher expected squared jumping distance per proposal, $J=\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}\\,\\alpha]$, and hence a lower asymptotic variance for smooth observables, than the Gaussian proposal.\n\nB. When calibrated to the same $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$, the Gaussian proposal has a higher acceptance probability than the uniform-cube proposal because it concentrates probability near $\\Delta \\mathbf{r}=\\mathbf{0}$, and this necessarily implies strictly smaller integrated autocorrelation times for all observables.\n\nC. If detailed balance is satisfied and $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ is matched, the uniform-cube and Gaussian chains are Peskun-equivalent, hence their asymptotic variances are identical for every observable.\n\nD. In the dilute-gas limit (low number density at fixed $\\beta$), the difference in $J$ between these proposals tends to zero because $\\alpha\\approx 1$ for typical steps; in contrast, in the dense regime with strong short-range repulsion, the compact support of the uniform-cube proposal avoids extremely unlikely large moves and therefore increases $J$ relative to the Gaussian at the same $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$.\n\nE. Matching $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ across proposals implies $\\sigma^{2}=\\delta^{2}$, where $\\sigma^{2}$ is the per-component variance of the Gaussian proposal and $\\delta$ is the half-width of the uniform component distribution.\n\nSelect all that apply.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   **System**: A dense Lennard-Jones (LJ) fluid in the canonical ensemble ($N, V, T$ constant).\n-   **Potential Energy**: $U(\\mathbf{r}^{N})=4\\varepsilon\\sum_{i<j}\\left[\\left(\\frac{\\sigma}{r_{ij}}\\right)^{12}-\\left(\\frac{\\sigma}{r_{ij}}\\right)^{6}\\right]$, where $r_{ij}=|\\mathbf{r}_{i}-\\mathbf{r}_{j}|$. The system is at high density, so the repulsive part dominates local energy changes.\n-   **Simulation Method**: Metropolis Monte Carlo with single-particle trial moves, $\\mathbf{r}_{k}\\mapsto \\mathbf{r}_{k}+\\Delta \\mathbf{r}$.\n-   **Proposal Distributions for $\\Delta \\mathbf{r}=(\\Delta x,\\Delta y,\\Delta z)$**:\n    1.  **Uniform-cube**: Components $\\Delta x, \\Delta y, \\Delta z$ are independent and uniformly distributed on $[-\\delta, \\delta]$.\n    2.  **Gaussian**: Components $\\Delta x, \\Delta y, \\Delta z$ are independent and normally distributed with mean $0$ and variance $\\sigma^{2}$.\n-   **Proposal Symmetry**: Proposals are symmetric, $q(\\mathbf{y} \\mid \\mathbf{x})=q(\\mathbf{x} \\mid \\mathbf{y})$.\n-   **Acceptance Probability**: $\\alpha(\\mathbf{x}\\to\\mathbf{y})=\\min\\left\\{1,\\exp\\left[-\\beta\\left(U(\\mathbf{y})-U(\\mathbf{x})\\right)\\right]\\right\\}$, with $\\beta=1/(k_{\\mathrm{B}}T)$.\n-   **Constraint**: The proposals are tuned to have a fixed mean squared step size: $\\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\right]=s^{2}$.\n-   **Performance Metric**: The expected squared jumping distance per attempt is $J\\equiv \\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\,\\alpha\\right]$.\n-   **Assumption**: In the dense regime, the acceptance factor $\\alpha$ is a rapidly decreasing function of the displacement magnitude $|\\Delta \\mathbf{r}|$. Note: A reuse of the symbol $\\sigma$ for both the LJ potential parameter and the Gaussian proposal variance is present. This is a minor lack of clarity, but the context makes the distinction unambiguous, so it does not invalidate the problem.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is a standard and fundamental topic in computational statistical mechanics, concerning the efficiency of Monte Carlo simulations. All principles (Lennard-Jones potential, canonical ensemble, Metropolis algorithm) are well-established. This is valid.\n-   **Well-Posed**: The problem is clearly defined, with a specific question about comparing two standard proposal schemes under a precise constraint. A unique answer based on established theory can be derived. This is valid.\n-   **Objective**: The problem is stated in precise, objective, technical language. This is valid.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, objective, and contains sufficient information for a rigorous analysis. Therefore, the problem is **valid**. We proceed to the solution.\n\nFirst, we must establish the relationship between the parameters of the two proposal distributions, $\\delta$ and $\\sigma$, under the constraint of a fixed mean squared step size, $\\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\right] = s^{2}$. The components of the displacement $\\Delta \\mathbf{r}$ are independent and identically distributed.\n\nFor the **uniform-cube proposal**, each component, e.g., $\\Delta x$, is drawn from a uniform distribution $U[-\\delta, \\delta]$. The mean is $\\mathbb{E}[\\Delta x] = 0$. The variance, which is equal to the mean square, is $\\mathbb{E}[\\Delta x^{2}] = \\text{Var}(\\Delta x) = \\frac{(\\delta - (-\\delta))^{2}}{12} = \\frac{4\\delta^{2}}{12} = \\frac{\\delta^{2}}{3}$.\nThe total mean squared displacement is:\n$$\n\\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\right] = \\mathbb{E}[\\Delta x^{2} + \\Delta y^{2} + \\Delta z^{2}] = \\mathbb{E}[\\Delta x^{2}] + \\mathbb{E}[\\Delta y^{2}] + \\mathbb{E}[\\Delta z^{2}] = 3 \\left(\\frac{\\delta^{2}}{3}\\right) = \\delta^{2}\n$$\nSo, for the uniform-cube proposal, $s^{2} = \\delta^{2}$.\n\nFor the **Gaussian proposal**, each component, e.g., $\\Delta x$, is drawn from a normal distribution $N(0, \\sigma^{2})$. By definition, the mean is $\\mathbb{E}[\\Delta x] = 0$ and the variance (and mean square) is $\\mathbb{E}[\\Delta x^{2}] = \\sigma^{2}$.\nThe total mean squared displacement is:\n$$\n\\mathbb{E}\\left[|\\Delta \\mathbf{r}|^{2}\\right] = \\mathbb{E}[\\Delta x^{2} + \\Delta y^{2} + \\Delta z^{2}] = \\sigma^{2} + \\sigma^{2} + \\sigma^{2} = 3\\sigma^{2}\n$$\nSo, for the Gaussian proposal, $s^{2} = 3\\sigma^{2}$.\n\nThe constraint that the mean squared step size is matched, $\\mathbb{E}[|\\Delta \\mathbf{r}|^2] = s^2$, implies the following relationship between the parameters:\n$$\n\\delta^{2} = 3\\sigma^{2}\n$$\nThis is the fundamental relation for comparing the two proposals under the given conditions.\n\nThe quantity of interest is $J = \\mathbb{E}[|\\Delta \\mathbf{r}|^2 \\alpha]$. This quantity measures the effective squared displacement per Monte Carlo attempt and serves as a proxy for the algorithm's efficiency in exploring the configuration space. A larger $J$ generally corresponds to a faster decay of autocorrelation functions and thus a smaller asymptotic variance of estimators for smooth observables.\n\nIn a dense fluid, the short-range repulsive part of the Lennard-Jones potential, $\\sim (r_{ij})^{-12}$, dominates. This means that a displacement $\\Delta \\mathbf{r}$ of even moderate magnitude is likely to cause a particle to overlap significantly with a neighbor, leading to a very large positive change in energy, $\\Delta U \\gg k_{\\mathrm{B}}T$. Consequently, the acceptance probability $\\alpha = \\exp(-\\beta \\Delta U)$ becomes vanishingly small for large $|\\Delta \\mathbf{r}|$. The function $\\alpha(|\\Delta \\mathbf{r}|)$ acts as a soft-cutoff filter for large displacements.\n\nThe crucial difference between the two proposals is their support and tail behavior.\n-   The **uniform-cube** proposal has compact support: it never proposes a move outside the cube defined by $|\\Delta x|, |\\Delta y|, |\\Delta z| \\le \\delta$. The maximum possible squared displacement is $3\\delta^2$.\n-   The **Gaussian** proposal has infinite support: it can, in principle, propose a move of any size. However, the probability of very large moves is small.\n\nThe Gaussian proposal 'wastes' a fraction of its attempts by proposing very large displacements. In a dense fluid, these large moves are almost certain to be rejected ($\\alpha \\approx 0$) and thus contribute nothing to $J$. The uniform proposal, by its construction, avoids these futile, large-displacement proposals. It concentrates all its proposals within a bounded region. By tuning $\\delta$ (and thus $s$), one can ensure this region corresponds to step sizes that have a reasonable chance of being accepted while still being large enough to explore the configuration space effectively. The Gaussian proposal, for the same mean squared step size $s^2$, is forced to have a longer tail, which is inefficient. Consequently, for a dense system, the uniform-cube proposal is expected to yield a higher value of $J$.\n\nNow, we evaluate each option.\n\n**A. In a dense Lennard–Jones fluid at fixed inverse temperature $\\beta$, when two symmetric random-walk Metropolis proposals are tuned so that $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ is identical, the uniform-cube proposal yields a higher expected squared jumping distance per proposal, $J=\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}\\,\\alpha]$, and hence a lower asymptotic variance for smooth observables, than the Gaussian proposal.**\nThis statement aligns perfectly with our analysis. In the dense regime, the compact support of the uniform proposal is an advantage, as it avoids proposing large, high-energy moves that are systematically rejected. This leads to a more efficient use of proposals, resulting in a larger effective squared jump distance $J$. A larger $J$ indicates faster exploration of the configuration space, which translates to a smaller integrated autocorrelation time and, consequently, lower asymptotic variance for estimators of smooth observables.\nVerdict: **Correct**.\n\n**B. When calibrated to the same $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$, the Gaussian proposal has a higher acceptance probability than the uniform-cube proposal because it concentrates probability near $\\Delta \\mathbf{r}=\\mathbf{0}$, and this necessarily implies strictly smaller integrated autocorrelation times for all observables.**\nThe first part of the statement is plausible. The Gaussian distribution is more peaked at the origin than the uniform distribution (for $\\delta^2=3\\sigma^2$). Since moves with small $|\\Delta \\mathbf{r}|$ have $\\Delta U \\approx 0$ and thus $\\alpha \\approx 1$, the Gaussian proposal might indeed have a higher overall acceptance rate. However, the conclusion drawn from this is fallacious. A high acceptance rate, in and of itself, is not an indicator of efficiency. An extremely high acceptance rate (e.g., $> 90\\%$) usually implies that the proposed steps are too small, causing the system to diffuse very slowly. This leads to high correlation between successive states and thus *large* integrated autocorrelation times. Efficiency is determined by the product of acceptance rate and step size, which is what $J$ approximates. A higher $J$ for the uniform proposal (as argued in A) implies *better* efficiency, not worse.\nVerdict: **Incorrect**.\n\n**C. If detailed balance is satisfied and $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ is matched, the uniform-cube and Gaussian chains are Peskun-equivalent, hence their asymptotic variances are identical for every observable.**\nDetailed balance, $p(x)P(x \\to y) = p(y)P(y \\to x)$, is satisfied by the Metropolis-Hastings algorithm for symmetric proposals. Peskun's theorem provides a basis for comparing the efficiency of different Markov chains. A chain with transition kernel $P_1$ is said to be superior in the Peskun sense to a chain with kernel $P_2$ if $P_1(x, y) \\ge P_2(x, y)$ for all states $x \\ne y$. This would imply that the asymptotic variance for any observable using $P_1$ is less than or equal to that using $P_2$. The two chains would have identical asymptotic variances for all observables only if $P_1(x, y) = P_2(x, y)$ for all $x \\ne y$. Here, the transition probability is $P(x \\to y) = q(\\Delta \\mathbf{r}) \\alpha(x \\to y)$. The proposal densities $q_{uni}(\\Delta \\mathbf{r})$ and $q_{gauss}(\\Delta \\mathbf{r})$ are fundamentally different functions. Neither is pointwise greater than or equal to the other over the entire space of displacements $\\Delta \\mathbf{r}$. Therefore, there is no Peskun ordering between the two kernels, and they are certainly not equivalent in a way that would imply identical asymptotic variances for all observables.\nVerdict: **Incorrect**.\n\n**D. In the dilute-gas limit (low number density at fixed $\\beta$), the difference in $J$ between these proposals tends to zero because $\\alpha\\approx 1$ for typical steps; in contrast, in the dense regime with strong short-range repulsion, the compact support of the uniform-cube proposal avoids extremely unlikely large moves and therefore increases $J$ relative to the Gaussian at the same $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$.**\nThis statement presents a comparison of the two proposals in two opposite physical regimes.\n-   **Dilute-gas limit**: At low density, particles are far apart. A trial move of a moderate size is very unlikely to cause an energetic clash. Thus, the change in energy $\\Delta U$ is typically near zero, and the acceptance probability $\\alpha \\approx 1$. In this limit, the expression for $J$ simplifies to $J = \\mathbb{E}[|\\Delta \\mathbf{r}|^2 \\alpha] \\approx \\mathbb{E}[|\\Delta \\mathbf{r}|^2]$. Since the proposals are constructed to have the same $\\mathbb{E}[|\\Delta \\mathbf{r}|^2] = s^2$, their respective $J$ values will be approximately equal, $J_{uni} \\approx J_{gauss} \\approx s^2$. The first part of the statement is correct.\n-   **Dense regime**: The second part of the statement repeats the argument from option A: in a dense fluid, the compact support of the uniform proposal makes it more efficient by preventing proposals of large steps that would be rejected anyway. This increases $J_{uni}$ relative to $J_{gauss}$. This contrast is physically and methodologically correct.\nThe statement as a whole is a sophisticated and accurate summary of the behavior.\nVerdict: **Correct**.\n\n**E. Matching $\\mathbb{E}[|\\Delta \\mathbf{r}|^{2}]$ across proposals implies $\\sigma^{2}=\\delta^{2}$, where $\\sigma^{2}$ is the per-component variance of the Gaussian proposal and $\\delta$ is the half-width of the uniform component distribution.**\nThis is a statement of quantitative fact that we can check directly from our initial derivation.\nWe found:\n-   For the uniform-cube proposal, $\\mathbb{E}[|\\Delta \\mathbf{r}|^2] = \\delta^2$.\n-   For the Gaussian proposal, $\\mathbb{E}[|\\Delta \\mathbf{r}|^2] = 3\\sigma^2$, where $\\sigma^2$ is indeed the per-component variance.\nEquating these gives $\\delta^2 = 3\\sigma^2$. The statement claims $\\delta^2 = \\sigma^2$, which is mathematically false. It incorrectly equates the total mean squared displacement for the uniform case with the per-component variance for the Gaussian case.\nVerdict: **Incorrect**.\n\nIn conclusion, options A and D are correct.", "answer": "$$\\boxed{AD}$$", "id": "2788218"}, {"introduction": "The tuning of a Metropolis algorithm often relies on heuristics, but some of these rules of thumb are backed by deep theoretical results. This final practice guides you through the landmark derivation of the optimal acceptance rate for random-walk Metropolis in high-dimensional systems. By tackling this problem, you will see how asymptotic analysis provides powerful, general guidelines for optimizing MCMC simulations and connect abstract theory to practical application in chemistry and physics [@problem_id:2788234].", "problem": "A common scenario in theoretical chemistry is the sampling of the configurational Boltzmann distribution for a many-degree-of-freedom system near a local minimum of the potential energy surface, where the potential can be approximated as harmonic in appropriately mass-weighted normal modes. Consider a system with $d$ independent normal modes with potential energy $U(\\mathbf{x}) = \\frac{1}{2} \\sum_{i=1}^{d} x_{i}^{2}$ in nondimensional units such that the target distribution for the coordinates $\\mathbf{x} \\in \\mathbb{R}^{d}$ at inverse temperature $\\beta = 1$ is $ \\pi(\\mathbf{x}) \\propto \\exp\\!\\left(-U(\\mathbf{x})\\right)$, that is, a $d$-dimensional standard normal distribution.\n\nWe perform local Metropolis random-walk proposals of the form $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$ independent of $\\mathbf{x}$. The Metropolis acceptance probability is $a(\\mathbf{x},\\mathbf{y}) = \\min\\!\\left\\{1, \\exp\\!\\left[\\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x})\\right]\\right\\}$, and the chain is assumed to be at stationarity so that $\\mathbf{x} \\sim \\pi$. To study the high-dimensional behavior, consider the scaling $s = \\ell / \\sqrt{d}$ with fixed $\\ell > 0$ and let $d \\to \\infty$.\n\nStarting only from the definitions above, the properties of the standard normal distribution, and classical limit theorems of probability, carry out the following steps:\n1. Derive the $d \\to \\infty$ limit of the average acceptance probability $\\alpha(\\ell) = \\mathbb{E}[a(\\mathbf{x},\\mathbf{y})]$ as a function of $\\ell$, expressing it in terms of the standard normal cumulative distribution function. You may use the fact that for independent standard normal variables one has weak convergence of suitably scaled sums to a standard normal, and that empirical averages converge almost surely to their expectation.\n2. Define the asymptotic expected squared jump distance per proposal as $J(\\ell) = \\lim_{d \\to \\infty} \\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]$ under the scaling above, and show that $J(\\ell)$ is proportional to $\\ell^{2} \\alpha(\\ell)$ in the limit.\n3. Maximize $J(\\ell)$ over $\\ell > 0$, and determine the corresponding optimal acceptance probability $\\alpha^{\\star}$ in the $d \\to \\infty$ limit.\n\nReport as your final answer only the numerical value of the optimal acceptance probability $\\alpha^{\\star}$, rounded to three significant figures. The answer is dimensionless; do not include any units.", "solution": "We are given a target distribution that is a $d$-dimensional standard normal, $\\pi(\\mathbf{x}) \\propto \\exp\\!\\left(-\\|\\mathbf{x}\\|^{2}/2\\right)$, and a symmetric random-walk proposal $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$, $s = \\ell/\\sqrt{d}$. The Metropolis acceptance probability is\n$$\na(\\mathbf{x},\\mathbf{y}) = \\min\\!\\left\\{1, \\exp\\!\\left[\\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x})\\right]\\right\\}.\n$$\nSince the proposal is symmetric, the Hastings ratio reduces to the ratio of target densities. We analyze the high-dimensional limit with $d \\to \\infty$ and fixed $\\ell > 0$.\n\nStep 1: Derive the limiting average acceptance probability $\\alpha(\\ell)$.\n\nThe log-density difference is\n$$\n\\Delta \\equiv \\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x}) \\;=\\; -\\frac{1}{2}\\big(\\|\\mathbf{y}\\|^{2} - \\|\\mathbf{x}\\|^{2}\\big).\n$$\nWith $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ and $s = \\ell/\\sqrt{d}$,\n$$\n\\|\\mathbf{y}\\|^{2} - \\|\\mathbf{x}\\|^{2} \\;=\\; \\|\\mathbf{x} + s\\,\\boldsymbol{\\eta}\\|^{2} - \\|\\mathbf{x}\\|^{2}\n\\;=\\; 2 s\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} + s^{2} \\|\\boldsymbol{\\eta}\\|^{2}.\n$$\nThus\n$$\n\\Delta \\;=\\; - s\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;-\\; \\frac{1}{2} s^{2} \\|\\boldsymbol{\\eta}\\|^{2}.\n$$\nUnder stationarity, $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$ and is independent of $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$. Consider the two terms separately under the scaling $s = \\ell/\\sqrt{d}$.\n\n- By independence and the Central Limit Theorem for triangular arrays or by direct variance computation, the projection\n$$\n\\frac{1}{\\sqrt{d}} \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;=\\; \\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} x_{i} \\eta_{i}\n$$\nconverges in distribution to a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$, because the summands $x_{i}\\eta_{i}$ are independent, mean zero, and have unit variance. Therefore,\n$$\ns\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;=\\; \\frac{\\ell}{\\sqrt{d}} \\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;\\xrightarrow[d\\to\\infty]{\\mathcal{D}}\\; \\ell\\, Z.\n$$\n\n- By the Strong Law of Large Numbers,\n$$\n\\frac{1}{d}\\|\\boldsymbol{\\eta}\\|^{2} \\;=\\; \\frac{1}{d} \\sum_{i=1}^{d} \\eta_{i}^{2} \\;\\xrightarrow[d\\to\\infty]{\\text{a.s.}}\\; \\mathbb{E}[\\eta_{1}^{2}] \\;=\\; 1.\n$$\nHence\n$$\n\\frac{1}{2} s^{2} \\|\\boldsymbol{\\eta}\\|^{2} \\;=\\; \\frac{1}{2} \\frac{\\ell^{2}}{d} \\|\\boldsymbol{\\eta}\\|^{2} \\;\\xrightarrow[d\\to\\infty]{\\text{a.s.}}\\; \\frac{1}{2} \\ell^{2}.\n$$\n\nCombining these two convergences and invoking standard arguments for convergence of bounded continuous functionals yields the limiting distribution for $\\Delta$:\n$$\n\\Delta \\;\\xrightarrow[d\\to\\infty]{\\mathcal{D}}\\; - \\ell Z \\;-\\; \\frac{1}{2}\\ell^{2},\n$$\nwith $Z \\sim \\mathcal{N}(0,1)$ independent of everything else. Therefore, the average acceptance probability in the limit is\n$$\n\\alpha(\\ell) \\;=\\; \\mathbb{E}\\!\\left[\\min\\!\\left\\{1, \\exp(\\Delta)\\right\\}\\right] \\;=\\; \\mathbb{E}\\!\\left[\\min\\!\\left\\{1, \\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\right\\}\\right].\n$$\nLet $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^{2}/2)$ denote the standard normal probability density function and $\\Phi$ its cumulative distribution function. The event $\\exp(-\\ell Z - \\ell^{2}/2) \\geq 1$ is equivalent to $-\\ell Z - \\ell^{2}/2 \\geq 0$, namely $Z \\leq -\\ell/2$. Hence\n$$\n\\alpha(\\ell) \\;=\\; \\mathbb{P}(Z \\leq -\\ell/2) \\;+\\; \\mathbb{E}\\!\\left[\\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\, \\mathbf{1}_{\\{Z > -\\ell/2\\}}\\right].\n$$\nCompute the second term by completing the square:\n\\begin{align*}\n\\mathbb{E}\\!\\left[\\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\, \\mathbf{1}_{\\{Z > -\\ell/2\\}}\\right]\n&= \\int_{-\\ell/2}^{\\infty} \\exp\\!\\left(-\\ell z - \\frac{1}{2}\\ell^{2}\\right) \\phi(z)\\, \\mathrm{d}z \\\\\n&= \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2} - \\ell z - \\frac{1}{2}\\ell^{2}\\right) \\, \\mathrm{d}z \\\\\n&= \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(z+\\ell)^{2}}{2}\\right) \\, \\mathrm{d}z \\\\\n&= \\int_{-\\ell/2+\\ell}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{u^{2}}{2}\\right) \\, \\mathrm{d}u \\\\\n&= 1 - \\Phi(\\ell/2).\n\\end{align*}\nTherefore,\n$$\n\\alpha(\\ell) \\;=\\; \\Phi(-\\ell/2) \\;+\\; \\big(1 - \\Phi(\\ell/2)\\big) \\;=\\; 2 \\big(1 - \\Phi(\\ell/2)\\big) \\;=\\; 2 \\, \\Phi(-\\ell/2).\n$$\nThis is the desired limiting acceptance probability as a function of $\\ell$.\n\nStep 2: Define and simplify the asymptotic expected squared jump distance $J(\\ell)$.\n\nThe squared jump length is $\\|\\mathbf{y} - \\mathbf{x}\\|^{2} = s^{2} \\|\\boldsymbol{\\eta}\\|^{2}$. Thus\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]\n\\;=\\; s^{2} \\, \\mathbb{E}\\!\\left[\\|\\boldsymbol{\\eta}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right].\n$$\nUnder the scaling $s = \\ell/\\sqrt{d}$ and using that $\\|\\boldsymbol{\\eta}\\|^{2}/d \\to 1$ almost surely, while $a(\\mathbf{x},\\mathbf{y}) \\to \\min\\{1, \\exp(-\\ell Z - \\ell^{2}/2)\\}$ in distribution with expectation $\\alpha(\\ell)$, standard dominated convergence arguments (the acceptance is bounded by $1$ and the squared norm grows linearly in $d$ while $s^{2}$ scales as $1/d$) give\n$$\nJ(\\ell) \\;\\equiv\\; \\lim_{d \\to \\infty} \\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]\n\\;=\\; \\ell^{2} \\, \\alpha(\\ell).\n$$\nThus, up to a constant of proportionality, the asymptotic expected squared jump distance is maximized by maximizing $\\ell^{2} \\alpha(\\ell)$.\n\nStep 3: Maximize $J(\\ell)$ and compute the optimal acceptance probability $\\alpha^{\\star}$.\n\nWe need to maximize\n$$\nH(\\ell) \\;=\\; \\ell^{2} \\, \\alpha(\\ell) \\;=\\; 2 \\, \\ell^{2} \\, \\Phi(-\\ell/2), \\quad \\ell > 0.\n$$\nDifferentiate $H(\\ell)$ with respect to $\\ell$, using that $\\frac{\\mathrm{d}}{\\mathrm{d}\\ell} \\Phi(-\\ell/2) = -\\frac{1}{2} \\phi(\\ell/2)$ and that the standard normal density is even:\n\\begin{align*}\nH'(\\ell)\n&= 4 \\ell \\, \\Phi(-\\ell/2) \\;+\\; 2 \\ell^{2} \\left(-\\frac{1}{2}\\right) \\phi(\\ell/2) \\\\\n&= 4 \\ell \\, \\Phi(-\\ell/2) \\;-\\; \\ell^{2} \\, \\phi(\\ell/2).\n\\end{align*}\nSetting $H'(\\ell) = 0$ and restricting to $\\ell > 0$ yields the optimality condition\n$$\n4 \\, \\Phi(-\\ell/2) \\;=\\; \\ell \\, \\phi(\\ell/2).\n$$\nLet $t = \\ell/2 > 0$. Then the condition becomes\n$$\n2 \\, \\Phi(-t) \\;=\\; t \\, \\phi(t).\n$$\nThis scalar equation has a unique solution $t^{\\star} \\in (0,\\infty)$. Numerically solving yields $t^{\\star} \\approx 1.19$, equivalently $\\ell^{\\star} \\approx 2.38$. The corresponding optimal acceptance probability is\n$$\n\\alpha^{\\star} \\;=\\; \\alpha(\\ell^{\\star}) \\;=\\; 2 \\, \\Phi\\!\\left(-\\frac{\\ell^{\\star}}{2}\\right) \\;=\\; 2 \\, \\Phi(-t^{\\star}).\n$$\nSubstituting $t^{\\star} \\approx 1.19$ gives\n$$\n\\alpha^{\\star} \\;\\approx\\; 2 \\, \\Phi(-1.19).\n$$\nUsing the standard normal cumulative distribution function value $\\Phi(-1.19) \\approx 0.117$, we obtain\n$$\n\\alpha^{\\star} \\;\\approx\\; 0.234.\n$$\nThis value is the high-dimensional optimal acceptance probability for local random-walk Metropolis moves under the harmonic (Gaussian) approximation and the $\\ell/\\sqrt{d}$ scaling, often used as a heuristic target acceptance in high-dimensional molecular Monte Carlo simulations.\n\nRounding to three significant figures, the optimal acceptance probability is $0.234$.", "answer": "$$\\boxed{0.234}$$", "id": "2788234"}]}