{"hands_on_practices": [{"introduction": "Before we can tackle data from complex molecular simulations, it is crucial to master the foundational principles of statistical mechanics. This first practice provides a direct link between a system's microscopic Hamiltonian, $H(\\Gamma)$, and the macroscopic ensemble average of an observable, $\\langle A \\rangle$. By working through the analytical derivation for a system of harmonic oscillators, you will see how the separability of the Hamiltonian allows the otherwise intractable phase-space integral to be factorized into manageable parts, a concept that underpins many theoretical models [@problem_id:2772366].", "id": "2772366", "problem": "Consider a classical canonical ensemble at inverse temperature $\\beta$, with phase-space point $\\Gamma=(q,p)$, where $q=(q_{1},\\dots,q_{N})$ and $p=(p_{1},\\dots,p_{N})$ are the coordinates and momenta of $N$ independent $1$-dimensional harmonic oscillators with masses $\\{m_{i}\\}_{i=1}^{N}$ and angular frequencies $\\{\\omega_{i}\\}_{i=1}^{N}$. The Hamiltonian is $H(\\Gamma)=\\sum_{i=1}^{N}\\left[\\frac{p_{i}^{2}}{2m_{i}}+\\frac{1}{2}m_{i}\\omega_{i}^{2}q_{i}^{2}\\right]$. The canonical phase-space density is $\\rho(\\Gamma)=Z^{-1}\\exp(-\\beta H(\\Gamma))$, where the partition function $Z$ is defined by $Z=\\int \\mathrm{d}\\Gamma\\,\\exp(-\\beta H(\\Gamma))$ and $\\mathrm{d}\\Gamma=\\prod_{i=1}^{N}\\mathrm{d}q_{i}\\,\\mathrm{d}p_{i}$. An observable of interest is separable as $A(q,p)=A_{q}(q)+A_{p}(p)$, where $A_{q}$ depends only on $q$ and $A_{p}$ depends only on $p$. Starting only from the stated definitions, derive the canonical ensemble average of $A$ in a form that makes explicit the factorization into coordinate and momentum contributions and yields normalized configuration-space and momentum-space probability densities. Provide your final result as a single closed-form analytic expression involving $\\beta$, $\\{m_{i}\\}$, $\\{\\omega_{i}\\}$, $A_{q}$, and $A_{p}$, with all Gaussian normalization constants shown explicitly and all integrals written over $\\mathbb{R}^{N}$. Do not assume any particular form for $A_{q}$ or $A_{p}$. Your final answer must be a single expression (no equations) and must not include units. If you introduce any numerical constants, write them in standard mathematical form.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in classical statistical mechanics. Therefore, the problem is valid and I will proceed with its solution.\n\nThe canonical ensemble average of an observable $A$ is defined as\n$$\n\\langle A \\rangle = \\frac{\\int A(\\Gamma) \\exp(-\\beta H(\\Gamma)) \\mathrm{d}\\Gamma}{\\int \\exp(-\\beta H(\\Gamma)) \\mathrm{d}\\Gamma}\n$$\nThe denominator is the partition function, $Z = \\int \\exp(-\\beta H(\\Gamma)) \\mathrm{d}\\Gamma$. The phase-space point is $\\Gamma = (q,p)$, with $q=(q_{1},\\dots,q_{N})$ and $p=(p_{1},\\dots,p_{N})$. The integration measure is $\\mathrm{d}\\Gamma = \\prod_{i=1}^{N}\\mathrm{d}q_{i}\\,\\mathrm{d}p_{i}$, which we can write as $\\mathrm{d}q\\,\\mathrm{d}p$.\n\nThe Hamiltonian is given as $H(\\Gamma) = \\sum_{i=1}^{N}\\left[\\frac{p_{i}^{2}}{2m_{i}}+\\frac{1}{2}m_{i}\\omega_{i}^{2}q_{i}^{2}\\right]$. This is separable into a sum of a momentum-dependent part $H_p(p) = \\sum_{i=1}^{N}\\frac{p_{i}^{2}}{2m_{i}}$ and a coordinate-dependent part $H_q(q) = \\sum_{i=1}^{N}\\frac{1}{2}m_{i}\\omega_{i}^{2}q_{i}^{2}$. Thus, $H(\\Gamma) = H_p(p) + H_q(q)$.\n\nThe observable of interest is also separable: $A(q,p) = A_{q}(q) + A_{p}(p)$.\n\nFirst, we analyze the partition function $Z$. Due to the separability of the Hamiltonian and the integration measure, $Z$ factorizes:\n$$\nZ = \\int \\exp(-\\beta(H_q(q) + H_p(p)))\\,\\mathrm{d}q\\,\\mathrm{d}p = \\left(\\int \\exp(-\\beta H_q(q))\\,\\mathrm{d}q\\right) \\left(\\int \\exp(-\\beta H_p(p))\\,\\mathrm{d}p\\right) = Z_q Z_p\n$$\nwhere $Z_q = \\int_{\\mathbb{R}^N} \\exp(-\\beta H_q(q))\\,\\mathrm{d}q$ and $Z_p = \\int_{\\mathbb{R}^N} \\exp(-\\beta H_p(p))\\,\\mathrm{d}p$.\n\nNext, we evaluate the numerator of the expression for $\\langle A \\rangle$:\n$$\n\\text{Numerator} = \\int (A_{q}(q) + A_{p}(p)) \\exp(-\\beta(H_q(q) + H_p(p)))\\,\\mathrm{d}q\\,\\mathrm{d}p\n$$\nThis integral splits into two terms:\n$$\n\\text{Numerator} = \\int A_{q}(q) \\exp(-\\beta H_q(q)) \\exp(-\\beta H_p(p))\\,\\mathrm{d}q\\,\\mathrm{d}p + \\int A_{p}(p) \\exp(-\\beta H_q(q)) \\exp(-\\beta H_p(p))\\,\\mathrm{d}q\\,\\mathrm{d}p\n$$\nEach term also factorizes with respect to the $q$ and $p$ integrations:\n$$\n\\text{Numerator} = \\left(\\int_{\\mathbb{R}^N} A_{q}(q) \\exp(-\\beta H_q(q))\\,\\mathrm{d}q\\right)\\left(\\int_{\\mathbb{R}^N} \\exp(-\\beta H_p(p))\\,\\mathrm{d}p\\right) + \\left(\\int_{\\mathbb{R}^N} \\exp(-\\beta H_q(q))\\,\\mathrm{d}q\\right)\\left(\\int_{\\mathbb{R}^N} A_{p}(p) \\exp(-\\beta H_p(p))\\,\\mathrm{d}p\\right)\n$$\nUsing the definitions of $Z_q$ and $Z_p$, we can write the numerator as:\n$$\n\\text{Numerator} = \\left(\\int_{\\mathbb{R}^N} A_{q}(q) \\exp(-\\beta H_q(q))\\,\\mathrm{d}q\\right)Z_p + Z_q\\left(\\int_{\\mathbb{R}^N} A_{p}(p) \\exp(-\\beta H_p(p))\\,\\mathrm{d}p\\right)\n$$\nNow we assemble the ensemble average $\\langle A \\rangle$:\n$$\n\\langle A \\rangle = \\frac{\\left(\\int_{\\mathbb{R}^N} A_{q}(q) \\exp(-\\beta H_q(q))\\,\\mathrm{d}q\\right)Z_p + Z_q\\left(\\int_{\\mathbb{R}^N} A_{p}(p) \\exp(-\\beta H_p(p))\\,\\mathrm{d}p\\right)}{Z_q Z_p}\n$$\n$$\n\\langle A \\rangle = \\frac{\\int_{\\mathbb{R}^N} A_{q}(q) \\exp(-\\beta H_q(q))\\,\\mathrm{d}q}{Z_q} + \\frac{\\int_{\\mathbb{R}^N} A_{p}(p) \\exp(-\\beta H_p(p))\\,\\mathrm{d}p}{Z_p}\n$$\nThis expression shows that the average of the sum is the sum of the averages, where the averages are taken over the respective configuration and momentum spaces. We can define normalized probability densities $\\rho_q(q) = Z_q^{-1} \\exp(-\\beta H_q(q))$ and $\\rho_p(p) = Z_p^{-1} \\exp(-\\beta H_p(p))$. Then $\\langle A \\rangle = \\int_{\\mathbb{R}^N} A_q(q) \\rho_q(q)\\,\\mathrm{d}q + \\int_{\\mathbb{R}^N} A_p(p) \\rho_p(p)\\,\\mathrm{d}p$.\n\nTo provide the final expression with explicit normalization constants, we must calculate $Z_q$ and $Z_p$.\nThe configuration-space partition function is:\n$$\nZ_q = \\int_{\\mathbb{R}^N} \\exp\\left(-\\beta \\sum_{i=1}^N \\frac{1}{2}m_{i}\\omega_{i}^{2}q_{i}^{2}\\right) \\prod_{i=1}^N \\mathrm{d}q_i = \\prod_{i=1}^N \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta m_i \\omega_i^2}{2}q_i^2\\right) \\mathrm{d}q_i\n$$\nUsing the standard Gaussian integral $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)\\,\\mathrm{d}x = \\sqrt{\\pi/a}$, with $a_i = \\frac{\\beta m_i \\omega_i^2}{2}$, we get:\n$$\nZ_q = \\prod_{i=1}^N \\sqrt{\\frac{2\\pi}{\\beta m_i \\omega_i^2}}\n$$\nThe momentum-space partition function is:\n$$\nZ_p = \\int_{\\mathbb{R}^N} \\exp\\left(-\\beta \\sum_{i=1}^N \\frac{p_{i}^{2}}{2m_{i}}\\right) \\prod_{i=1}^N \\mathrm{d}p_i = \\prod_{i=1}^N \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta}{2m_i}p_i^2\\right) \\mathrm{d}p_i\n$$\nUsing the same Gaussian integral with $a_i = \\frac{\\beta}{2m_i}$, we get:\n$$\nZ_p = \\prod_{i=1}^N \\sqrt{\\frac{2\\pi m_i}{\\beta}}\n$$\nThe normalization constants for the probability densities are the reciprocals of these partition functions:\n$$\n\\frac{1}{Z_q} = \\prod_{i=1}^N \\sqrt{\\frac{\\beta m_i \\omega_i^2}{2\\pi}} \\quad \\text{and} \\quad \\frac{1}{Z_p} = \\prod_{i=1}^N \\sqrt{\\frac{\\beta}{2\\pi m_i}}\n$$\nSubstituting these into the expression for $\\langle A \\rangle$, we obtain the final expression where the normalized probability densities are explicitly written out. The integrals are specified to be over $\\mathbb{R}^N$, and the integration measures $\\mathrm{d}q$ and $\\mathrm{d}p$ represent the $N$-dimensional volume elements $\\prod_{i=1}^N \\mathrm{d}q_i$ and $\\prod_{i=1}^N \\mathrm{d}p_i$, respectively.\n\nThe final form is the sum of two integrals. The first integral is the average of $A_q(q)$ over the normalized configurational probability distribution. The second is the average of $A_p(p)$ over the normalized momentum probability distribution.", "answer": "$$\n\\boxed{\\int_{\\mathbb{R}^N} A_{q}(q) \\left(\\prod_{i=1}^N \\sqrt{\\frac{\\beta m_i \\omega_i^2}{2\\pi}}\\right) \\exp\\left(-\\frac{\\beta}{2}\\sum_{i=1}^N m_i \\omega_i^2 q_i^2\\right) \\mathrm{d}q + \\int_{\\mathbb{R}^N} A_{p}(p) \\left(\\prod_{i=1}^N \\sqrt{\\frac{\\beta}{2\\pi m_i}}\\right) \\exp\\left(-\\beta\\sum_{i=1}^N \\frac{p_i^2}{2m_i}\\right) \\mathrm{d}p}\n$$"}, {"introduction": "While analytical derivations are exact, real-world simulations produce finite-length time series, meaning our estimates of ensemble averages inherently suffer from statistical noise. This exercise introduces a powerful variance reduction technique known as the method of control variates, which improves the precision of an estimate by leveraging its correlation with another observable whose ensemble average is already known. Mastering this technique [@problem_id:2772376] is essential for extracting reliable quantitative insights from computationally expensive simulations without simply resorting to longer run times.", "id": "2772376", "problem": "In a canonical ensemble at fixed temperature, you run a long Molecular Dynamics (MD) simulation producing a time series $\\{(A_i,B_i)\\}_{i=1}^N$ of two observables. Observable $A$ is the soluteâ€“solvent interaction energy, and observable $B$ is a harmonic restraint energy applied to a collective variable with an analytically known ensemble mean $\\mu_B$ due to equipartition. Both $A$ and $B$ are measured in kJ/mol, so that any linear combination of $A$ and $B$ preserves energy units.\n\nYou wish to estimate the ensemble average $\\langle A \\rangle$ with reduced statistical uncertainty by using $B$ as a control variate. Consider the family of estimators\n$$\n\\widehat{A}_{\\mathrm{cv}}(\\alpha) \\equiv \\frac{1}{N}\\sum_{i=1}^{N} \\left[A_i - \\alpha\\left(B_i - \\mu_B\\right)\\right],\n$$\nwhere $\\alpha$ is a scalar coefficient. Assume the standard statistical-mechanical framework: ergodicity ensures time averages converge to ensemble averages, and the ensemble variance and covariance are defined by the usual second central moments. The simulation is sufficiently long that sample moments are consistent estimators of the corresponding ensemble moments.\n\nTasks:\n1. Starting only from the definitions of ensemble average, variance, and covariance, and from linearity of expectation, show that $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$ is an unbiased estimator of $\\langle A \\rangle$ when $\\mu_B$ is the exact ensemble mean of $B$.\n2. Derive an expression for the variance of $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$ in terms of $\\mathrm{Var}(A)$, $\\mathrm{Var}(B)$, and $\\mathrm{Cov}(A,B)$, and determine the value $\\alpha^\\star$ that minimizes this variance.\n3. A particular MD run yields $N = 2.0\\times 10^{6}$ frames. The known exact mean of the restraint energy is $\\mu_B = 3.000$ kJ/mol. From the trajectory you compute the consistent sample covariance and variance,\n$$\n\\hat{C}_{AB} = -8.6135 \\;\\text{(kJ/mol)}^{2}, \\quad \\hat{C}_{BB} = 24.61 \\;\\text{(kJ/mol)}^{2}.\n$$\nUsing these statistics as estimators of $\\mathrm{Cov}(A,B)$ and $\\mathrm{Var}(B)$, evaluate the optimal coefficient $\\alpha^\\star$. Round your answer to four significant figures. Express the coefficient as a dimensionless number.", "solution": "The problem presented is a standard exercise in variance reduction techniques for statistical estimation from molecular simulations, specifically the method of control variates. It is scientifically grounded, well-posed, and objective. We shall proceed with the solution by addressing the three specified tasks in order.\n\nTask 1: Demonstrate that $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$ is an unbiased estimator of $\\langle A \\rangle$.\n\nAn estimator is unbiased if its expectation value is equal to the true value of the quantity being estimated. We must therefore calculate the expectation value of the estimator $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$. The expectation value of an observable in this context is its ensemble average, denoted by $\\langle \\cdot \\rangle$.\n\nThe estimator is defined as:\n$$\n\\widehat{A}_{\\mathrm{cv}}(\\alpha) = \\frac{1}{N}\\sum_{i=1}^{N} \\left[A_i - \\alpha\\left(B_i - \\mu_B\\right)\\right]\n$$\nWe take the expectation value of both sides. Due to the linearity of the expectation operator, we can write:\n$$\n\\langle \\widehat{A}_{\\mathrm{cv}}(\\alpha) \\rangle = \\left\\langle \\frac{1}{N}\\sum_{i=1}^{N} \\left[A_i - \\alpha\\left(B_i - \\mu_B\\right)\\right] \\right\\rangle\n$$\n$$\n\\langle \\widehat{A}_{\\mathrm{cv}}(\\alpha) \\rangle = \\frac{1}{N}\\sum_{i=1}^{N} \\langle A_i - \\alpha(B_i - \\mu_B) \\rangle\n$$\nApplying linearity again to the term inside the summation:\n$$\n\\langle A_i - \\alpha(B_i - \\mu_B) \\rangle = \\langle A_i \\rangle - \\alpha \\langle B_i - \\mu_B \\rangle\n$$\nThe problem assumes ergodicity and a long simulation, so the ensemble average of a single sample $A_i$ or $B_i$ is equivalent to the ensemble average of the observable $A$ or $B$. Thus, for any sample $i$, $\\langle A_i \\rangle = \\langle A \\rangle$ and $\\langle B_i \\rangle = \\langle B \\rangle$.\nThe problem states that $\\mu_B$ is the exact ensemble mean of $B$, so $\\langle B \\rangle = \\mu_B$.\nLet us evaluate the second term:\n$$\n\\langle B_i - \\mu_B \\rangle = \\langle B_i \\rangle - \\langle \\mu_B \\rangle\n$$\nSince $\\mu_B$ is a constant, its expectation value is itself, $\\langle \\mu_B \\rangle = \\mu_B$.\n$$\n\\langle B_i - \\mu_B \\rangle = \\langle B \\rangle - \\mu_B = \\mu_B - \\mu_B = 0\n$$\nSubstituting this result back, we find:\n$$\n\\langle A_i - \\alpha(B_i - \\mu_B) \\rangle = \\langle A \\rangle - \\alpha(0) = \\langle A \\rangle\n$$\nNow, substituting this into the expression for the expectation of the estimator:\n$$\n\\langle \\widehat{A}_{\\mathrm{cv}}(\\alpha) \\rangle = \\frac{1}{N}\\sum_{i=1}^{N} \\langle A \\rangle = \\frac{1}{N} (N \\langle A \\rangle) = \\langle A \\rangle\n$$\nSince $\\langle \\widehat{A}_{\\mathrm{cv}}(\\alpha) \\rangle = \\langle A \\rangle$ for any value of $\\alpha$, the estimator is indeed unbiased. This completes the first task.\n\nTask 2: Derive the variance of $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$ and find the optimal coefficient $\\alpha^\\star$.\n\nThe variance of the estimator, which represents the statistical uncertainty, is what we wish to minimize. The estimator $\\widehat{A}_{\\mathrm{cv}}(\\alpha)$ is the sample mean of the random variable $Y_i(\\alpha) = A_i - \\alpha(B_i - \\mu_B)$. If we assume for simplicity that the samples $\\{Y_i\\}$ are independent and identically distributed (a common simplification, although MD data are correlated), the variance of the mean is the variance of a single sample divided by the number of samples, $N$:\n$$\n\\mathrm{Var}(\\widehat{A}_{\\mathrm{cv}}(\\alpha)) = \\frac{1}{N} \\mathrm{Var}(Y_i(\\alpha))\n$$\nMinimizing $\\mathrm{Var}(\\widehat{A}_{\\mathrm{cv}}(\\alpha))$ with respect to $\\alpha$ is equivalent to minimizing $\\mathrm{Var}(Y_i(\\alpha))$. Let us compute this variance.\n$$\n\\mathrm{Var}(Y_i(\\alpha)) = \\mathrm{Var}(A_i - \\alpha(B_i - \\mu_B))\n$$\nUsing the general property for variance, $\\mathrm{Var}(X - cZ) = \\mathrm{Var}(X) + c^2\\mathrm{Var}(Z) - 2c\\mathrm{Cov}(X,Z)$, and noting that constants do not affect variance or covariance (i.e., $\\mathrm{Var}(B_i - \\mu_B) = \\mathrm{Var}(B_i)$ and $\\mathrm{Cov}(A_i, B_i - \\mu_B) = \\mathrm{Cov}(A_i, B_i)$), we obtain:\n$$\n\\mathrm{Var}(Y_i(\\alpha)) = \\mathrm{Var}(A_i) + \\alpha^2 \\mathrm{Var}(B_i) - 2\\alpha \\mathrm{Cov}(A_i, B_i)\n$$\nDropping the sample index $i$ for notational clarity, the expression for the full estimator's variance becomes:\n$$\n\\mathrm{Var}(\\widehat{A}_{\\mathrm{cv}}(\\alpha)) = \\frac{1}{N} \\left( \\mathrm{Var}(A) + \\alpha^2 \\mathrm{Var}(B) - 2\\alpha \\mathrm{Cov}(A,B) \\right)\n$$\nThis is the required expression. To find the optimal value $\\alpha^\\star$ that minimizes this variance, we differentiate with respect to $\\alpha$ and set the derivative to zero.\n$$\n\\frac{d}{d\\alpha} \\mathrm{Var}(\\widehat{A}_{\\mathrm{cv}}(\\alpha)) = \\frac{1}{N} \\left( 2\\alpha \\mathrm{Var}(B) - 2 \\mathrm{Cov}(A,B) \\right) = 0\n$$\nSolving for $\\alpha$:\n$$\n2\\alpha \\mathrm{Var}(B) - 2 \\mathrm{Cov}(A,B) = 0\n$$\n$$\n\\alpha \\mathrm{Var}(B) = \\mathrm{Cov}(A,B)\n$$\n$$\n\\alpha^\\star = \\frac{\\mathrm{Cov}(A,B)}{\\mathrm{Var}(B)}\n$$\nThe second derivative, $\\frac{d^2}{d\\alpha^2} \\mathrm{Var}(\\widehat{A}_{\\mathrm{cv}}(\\alpha)) = \\frac{2}{N}\\mathrm{Var}(B)$, is positive since variance is non-negative (and strictly positive for a fluctuating observable $B$), confirming that $\\alpha^\\star$ corresponds to a minimum. It is important to note that this optimal value of $\\alpha$ is independent of the assumption of uncorrelated samples; it minimizes the variance of the underlying variable $Y$, and thus will minimize the variance of the sample mean regardless of time correlations.\n\nTask 3: Evaluate the optimal coefficient $\\alpha^\\star$.\n\nFrom Task 2, the optimal coefficient is given by $\\alpha^\\star = \\frac{\\mathrm{Cov}(A,B)}{\\mathrm{Var}(B)}$.\nThe problem provides consistent sample estimates for these ensemble moments:\nThe sample covariance is $\\hat{C}_{AB} = -8.6135 \\ (\\text{kJ/mol})^2$. This is our estimator for $\\mathrm{Cov}(A,B)$.\nThe sample variance of $B$ is $\\hat{C}_{BB} = 24.61 \\ (\\text{kJ/mol})^2$. This is our estimator for $\\mathrm{Var}(B)$.\n\nWe substitute these numerical values into the expression for $\\alpha^\\star$:\n$$\n\\alpha^\\star \\approx \\frac{\\hat{C}_{AB}}{\\hat{C}_{BB}} = \\frac{-8.6135 \\ (\\text{kJ/mol})^2}{24.61 \\ (\\text{kJ/mol})^2}\n$$\nThe units cancel, yielding a dimensionless coefficient as required.\n$$\n\\alpha^\\star \\approx -0.34999999...\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\alpha^\\star \\approx -0.3500\n$$\nThe other provided values, $N = 2.0 \\times 10^6$ and $\\mu_B = 3.000 \\ \\text{kJ/mol}$, are necessary for the definition and context of the problem but not for the calculation of $\\alpha^\\star$ itself.", "answer": "$$\n\\boxed{-0.3500}\n$$"}, {"introduction": "Often, the most interesting chemical and physical processes, such as reactions or phase transitions, occur in high-energy regions that are poorly sampled by standard simulations. This problem takes us into the realm of enhanced sampling, where we compute an unbiased ensemble average, $\\langle A \\rangle$, by combining data from multiple biased simulations that have been analyzed by methods like the Weighted Histogram Analysis Method (WHAM). This hands-on coding challenge [@problem_id:2772333] requires you to implement the law of total expectation to reweight the biased data, teaching you not only the theory but also the practical necessity of numerical stability when handling real simulation outputs.", "id": "2772333", "problem": "You are given a collection of biased molecular dynamics simulations performed with harmonic umbrella restraints centered at overlapping values of a one-dimensional reaction coordinate $\\xi$. The simulations are analyzed using the Weighted Histogram Analysis Method (WHAM), which returns a discrete, grid-based reduced free energy profile $f(\\xi_i)$ over $\\xi$-bins indexed by $i$, where the reduced free energy is defined so that the unbiased stationary probability density satisfies $\\rho(\\xi_i) \\propto \\exp\\!\\left(-f(\\xi_i)\\right)$. In addition, from the same simulations you have computed unbiased conditional bin-averages $\\overline{A}(\\xi_i)$ that estimate the conditional expectation $\\langle A \\mid \\xi_i \\rangle$ of a scalar observable $A$ within each bin $i$. Some bins may be insufficiently sampled for $A$ and thus $\\overline{A}(\\xi_i)$ can be unavailable; such entries are encoded as missing values. Each bin has a finite width $\\Delta \\xi_i$ that may be nonuniform.\n\nFrom first principles of the canonical ensemble and the law of total expectation, construct a numerically stable estimator for the unbiased ensemble average $\\langle A \\rangle$ marginalized over $\\xi$ using only $f(\\xi_i)$, $\\overline{A}(\\xi_i)$, and $\\Delta \\xi_i$. Your estimator must:\n- Use only the provided WHAM outputs $f(\\xi_i)$, the conditional bin-averages $\\overline{A}(\\xi_i)$, and the bin widths $\\Delta \\xi_i$.\n- Correctly account for nonuniform $\\Delta \\xi_i$ as a Riemann-sum approximation to the integral over $\\xi$.\n- Exclude bins where $\\overline{A}(\\xi_i)$ is missing and renormalize the probability over the remaining bins.\n- Be robust to large values of $f(\\xi_i)$ by employing a numerically stable normalization strategy that avoids underflow or overflow.\n- Treat all quantities as dimensionless.\n\nBase your derivation on the following foundational elements only:\n- The canonical (Boltzmann) distribution for the microscopic state $\\mathbf{x}$: $p(\\mathbf{x}) \\propto \\exp\\!\\left(-\\beta U(\\mathbf{x})\\right)$, where $\\beta$ is the inverse temperature and $U(\\mathbf{x})$ is the potential energy.\n- The definition of a reaction coordinate $\\xi(\\mathbf{x})$ and the marginal probability density $\\rho(\\xi)$ obtained from $p(\\mathbf{x})$.\n- The law of total expectation for conditional expectations.\n\nImplement your estimator as a program that computes $\\langle A \\rangle$ for each of the following test cases. All quantities are dimensionless. Missing $\\overline{A}(\\xi_i)$ values are indicated by the string \"NaN\" and should be treated as missing data.\n\nTest Suite:\n- Test case $1$ (happy path, symmetric, uniform bins):\n  - $\\xi_i = [-2, -1, 0, 1, 2]$\n  - $f(\\xi_i) = [2.0, 1.0, 0.0, 1.0, 2.0]$\n  - $\\overline{A}(\\xi_i) = [4.0, 1.0, 0.0, 1.0, 4.0]$\n  - $\\Delta \\xi_i = [1.0, 1.0, 1.0, 1.0, 1.0]$\n- Test case $2$ (bins with missing data, uniform bins):\n  - $\\xi_i = [0, 1, 2, 3, 4, 5, 6]$\n  - $f(\\xi_i) = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]$\n  - $\\overline{A}(\\xi_i) = [1.0, \\mathrm{NaN}, 2.0, \\mathrm{NaN}, 3.0, 4.0, 5.0]$\n  - $\\Delta \\xi_i = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$\n- Test case $3$ (nonuniform bin widths, extreme reduced free energies):\n  - $\\xi_i = [-1.5, -0.5, 0.5, 1.5]$\n  - $f(\\xi_i) = [10.0, 0.0, 5.0, 20.0]$\n  - $\\overline{A}(\\xi_i) = [10.0, 20.0, 30.0, 40.0]$\n  - $\\Delta \\xi_i = [0.1, 0.4, 0.2, 0.3]$\n\nYour program should:\n- Implement the estimator described above using stable arithmetic. You must explicitly handle missing $\\overline{A}(\\xi_i)$ values by excluding those bins and renormalizing across the remaining bins.\n- Produce outputs as real numbers rounded to six decimal places.\n- Produce a single line of output containing the results for the test cases as a comma-separated list enclosed in square brackets (for example, \"[0.123456,0.234567,0.345678]\").\n\nNo external input should be read. All computations must be performed using only the data provided above. The final output must be the three estimates $\\langle A \\rangle$ for test cases $1$, $2$, and $3$ in that order, formatted as specified.", "solution": "The problem statement presented is chemically and mathematically sound, self-contained, and well-posed. It requests the derivation and implementation of an estimator for an unbiased ensemble average from discretized simulation data. No flaws were detected during validation. We will proceed with the derivation from first principles.\n\nThe canonical ensemble average of a macroscopic observable $A$, which is a function of the microscopic state $\\mathbf{x}$ of the system, is defined as:\n$$\n\\langle A \\rangle = \\frac{\\int A(\\mathbf{x}) \\exp(-\\beta U(\\mathbf{x})) \\, d\\mathbf{x}}{\\int \\exp(-\\beta U(\\mathbf{x})) \\, d\\mathbf{x}}\n$$\nwhere $U(\\mathbf{x})$ is the potential energy, $\\beta = (k_B T)^{-1}$ is the inverse temperature, and the integration is over all accessible phase space.\n\nThe problem is formulated in terms of a one-dimensional reaction coordinate $\\xi = \\xi(\\mathbf{x})$. We can express the ensemble average by applying the law of total expectation. This law allows us to first compute the conditional expectation of $A$ for a fixed value of $\\xi$, denoted $\\langle A \\mid \\xi \\rangle$, and then average this quantity over the marginal probability distribution of $\\xi$, denoted $\\rho(\\xi)$.\n$$\n\\langle A \\rangle = \\int_{-\\infty}^{\\infty} \\langle A \\mid \\xi \\rangle \\rho(\\xi) \\, d\\xi\n$$\n\nThe marginal probability density $\\rho(\\xi)$ is related to the potential of mean force, or free energy $F(\\xi)$, by $\\rho(\\xi) \\propto \\exp(-\\beta F(\\xi))$. The problem provides a discretized, reduced free energy profile $f(\\xi_i)$, defined such that the unbiased stationary probability density satisfies $\\rho(\\xi_i) \\propto \\exp(-f(\\xi_i))$. The term \"reduced\" implies that the factor of $\\beta$ is absorbed into the definition of $f$, so $f(\\xi) = \\beta F(\\xi)$.\n\nThe provided data are discrete, corresponding to bins indexed by $i$, each with a center $\\xi_i$ and a width $\\Delta \\xi_i$. The continuous integral for $\\langle A \\rangle$ can be approximated as a Riemann sum over these bins:\n$$\n\\langle A \\rangle \\approx \\sum_i \\langle A \\mid \\xi_i \\rangle P(\\xi_i)\n$$\nwhere $P(\\xi_i)$ is the total probability of finding the system in bin $i$, and $\\langle A \\mid \\xi_i \\rangle$ is the conditional expectation of $A$ given that the system is in bin $i$. The problem provides estimates for this conditional expectation as $\\overline{A}(\\xi_i)$.\n\nThe probability of being in bin $i$, $P(\\xi_i)$, is the integral of the probability density $\\rho(\\xi)$ over the bin's width. For a sufficiently small bin width $\\Delta \\xi_i$, this can be approximated as:\n$$\nP(\\xi_i) \\approx \\rho(\\xi_i) \\Delta \\xi_i\n$$\nUsing the given relation $\\rho(\\xi_i) \\propto \\exp(-f(\\xi_i))$, we have:\n$$\nP(\\xi_i) = \\frac{\\exp(-f(\\xi_i)) \\Delta \\xi_i}{Z}\n$$\nwhere $Z$ is the normalization constant, or partition function, given by the sum over all bins $j$:\n$$\nZ = \\sum_j \\exp(-f(\\xi_j)) \\Delta \\xi_j\n$$\nSubstituting these expressions into the formula for the ensemble average gives:\n$$\n\\langle A \\rangle = \\sum_i \\overline{A}(\\xi_i) P(\\xi_i) = \\sum_i \\overline{A}(\\xi_i) \\frac{\\exp(-f(\\xi_i)) \\Delta \\xi_i}{\\sum_j \\exp(-f(\\xi_j)) \\Delta \\xi_j}\n$$\nThis can be written as a single fraction:\n$$\n\\langle A \\rangle = \\frac{\\sum_i \\overline{A}(\\xi_i) \\exp(-f(\\xi_i)) \\Delta \\xi_i}{\\sum_j \\exp(-f(\\xi_j)) \\Delta \\xi_j}\n$$\nThe problem specifies that for some bins, the value $\\overline{A}(\\xi_i)$ may be missing. Let $S$ be the set of indices $i$ for which $\\overline{A}(\\xi_i)$ is available. The summations must be restricted to this set of \"valid\" bins. This is equivalent to renormalizing the probability distribution over the subset of states where the observable's average is known. The estimator becomes:\n$$\n\\langle A \\rangle = \\frac{\\sum_{i \\in S} \\overline{A}(\\xi_i) \\exp(-f(\\xi_i)) \\Delta \\xi_i}{\\sum_{j \\in S} \\exp(-f(\\xi_j)) \\Delta \\xi_j}\n$$\nThe evaluation of $\\exp(-f(\\xi_i))$ can lead to numerical underflow if $f(\\xi_i)$ is large and positive, or overflow if it were large and negative. To ensure numerical stability, we subtract a constant from all free energy values before exponentiation. A judicious choice for this constant is the minimum free energy value among the valid bins, $f_{\\min, S} = \\min_{i \\in S} \\{f(\\xi_i)\\}$. The expression is modified as follows:\n$$\n\\langle A \\rangle = \\frac{\\sum_{i \\in S} \\overline{A}(\\xi_i) \\exp(-f(\\xi_i) + f_{\\min, S}) \\Delta \\xi_i}{\\sum_{j \\in S} \\exp(-f(\\xi_j) + f_{\\min, S}) \\Delta \\xi_j}\n$$\nThis expression is mathematically identical to the previous one, as the factor $\\exp(f_{\\min, S})$ appears in both the numerator and the denominator and thus cancels. However, this form is numerically robust. The largest argument to the exponential function is now $0$, corresponding to $\\exp(0)=1$, which prevents overflow. Other arguments are negative, preventing other terms from becoming excessively large while preserving the terms with the highest statistical weight.\n\nThe algorithm is therefore as follows:\n$1$. Identify the set of indices $S$ where $\\overline{A}(\\xi_i)$ is not a missing value.\n$2$. Create filtered arrays $f_S, \\overline{A}_S, \\Delta \\xi_S$ corresponding to the indices in $S$.\n$3$. Find the minimum value in the filtered free energy array: $f_{\\min, S} = \\min(f_S)$.\n$4$. Compute the shifted free energies: $f'_S = f_S - f_{\\min, S}$.\n$5$. Calculate the unnormalized Boltzmann weights for each valid bin: $w_i = \\exp(-f'_i)$ for $i \\in S$.\n$6$. The numerator is the sum of the products of the conditional average, the weight, and the bin width: $N = \\sum_{i \\in S} \\overline{A}_i \\cdot w_i \\cdot \\Delta \\xi_i$.\n$7$. The denominator is the sum of the products of the weight and the bin width, which normalizes the total probability to unity over the set $S$: $D = \\sum_{i \\in S} w_i \\cdot \\Delta \\xi_i$.\n$8$. The final estimate is the ratio $\\langle A \\rangle = N/D$.\nThis procedure correctly implements the principles of statistical mechanics for discretized data, while handling missing information and ensuring numerical stability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the ensemble average <A> for multiple test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f_xi\": np.array([2.0, 1.0, 0.0, 1.0, 2.0]),\n            \"A_bar_xi\": np.array([4.0, 1.0, 0.0, 1.0, 4.0]),\n            \"delta_xi\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n        },\n        {\n            \"f_xi\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]),\n            \"A_bar_xi\": np.array([1.0, np.nan, 2.0, np.nan, 3.0, 4.0, 5.0]),\n            \"delta_xi\": np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n        },\n        {\n            \"f_xi\": np.array([10.0, 0.0, 5.0, 20.0]),\n            \"A_bar_xi\": np.array([10.0, 20.0, 30.0, 40.0]),\n            \"delta_xi\": np.array([0.1, 0.4, 0.2, 0.3]),\n        },\n    ]\n\n    def estimate_ensemble_average(f_xi, A_bar_xi, delta_xi):\n        \"\"\"\n        Computes the unbiased ensemble average <A> using a numerically stable method.\n\n        Args:\n            f_xi (np.ndarray): Array of reduced free energies.\n            A_bar_xi (np.ndarray): Array of conditional averages of A. May contain NaNs.\n            delta_xi (np.ndarray): Array of bin widths.\n\n        Returns:\n            float: The estimated ensemble average <A>.\n        \"\"\"\n        # 1. Identify valid bins where A_bar is not NaN.\n        valid_mask = ~np.isnan(A_bar_xi)\n\n        if not np.any(valid_mask):\n            # If no valid bins exist, the average is undefined.\n            return np.nan\n\n        # 2. Filter arrays to include only data from valid bins.\n        f_valid = f_xi[valid_mask]\n        A_bar_valid = A_bar_xi[valid_mask]\n        delta_xi_valid = delta_xi[valid_mask]\n\n        # 3. Find the minimum free energy among valid bins for numerical stability.\n        f_min = np.min(f_valid)\n\n        # 4. Calculate shifted free energies.\n        f_shifted = f_valid - f_min\n\n        # 5. Compute unnormalized Boltzmann weights.\n        weights = np.exp(-f_shifted)\n\n        # 6. Calculate the numerator of the estimator equation.\n        # This is the sum over valid bins of A_bar * weight * bin_width.\n        numerator = np.sum(A_bar_valid * weights * delta_xi_valid)\n\n        # 7. Calculate the denominator (normalization constant over the valid subset).\n        # This is the sum over valid bins of weight * bin_width.\n        denominator = np.sum(weights * delta_xi_valid)\n\n        # 8. Compute the final estimate.\n        if denominator == 0:\n            # This case is unlikely with the f_min shift but is a safeguard.\n            return np.nan\n\n        ensemble_average = numerator / denominator\n        return ensemble_average\n\n    results = []\n    for case in test_cases:\n        result = estimate_ensemble_average(case[\"f_xi\"], case[\"A_bar_xi\"], case[\"delta_xi\"])\n        # Format the result to six decimal places.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}