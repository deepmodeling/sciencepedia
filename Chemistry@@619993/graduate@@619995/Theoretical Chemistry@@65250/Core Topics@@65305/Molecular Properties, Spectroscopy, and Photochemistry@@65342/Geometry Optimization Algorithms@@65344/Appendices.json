{"hands_on_practices": [{"introduction": "Geometry optimization algorithms form the cornerstone of computational chemistry, allowing us to locate stable molecular structures. The Newton-Raphson method provides the theoretical foundation for many of these algorithms by creating a local quadratic model of the potential energy surface and stepping directly to its minimum. This foundational exercise connects the abstract concepts of the energy gradient $\\mathbf{g}$ and Hessian $\\mathbf{H}$ to a concrete displacement vector, providing a direct calculation of the ideal Newton step. [@problem_id:2894209]", "problem": "In a quantum chemistry geometry optimization performed in scaled, dimensionless internal coordinates, the electronic energy $E(\\mathbf{q})$ in the neighborhood of a current structure $\\mathbf{q}_{0}$ is approximated by the second-order Taylor model\n$$\nm(\\mathbf{s}) \\equiv E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s},\n$$\nwhere $\\mathbf{s} = \\mathbf{q} - \\mathbf{q}_{0}$ is the displacement, $\\mathbf{g} = \\nabla E(\\mathbf{q}_{0})$ is the gradient, and $\\mathbf{H} = \\nabla^{2} E(\\mathbf{q}_{0})$ is the Hessian. Both $\\mathbf{g}$ and $\\mathbf{H}$ have units of energy because the coordinates are dimensionless by construction. The Newton step is defined as the displacement that minimizes the quadratic model.\n\nAt $\\mathbf{q}_{0}$, suppose the gradient and Hessian are\n$$\n\\mathbf{g} =\n\\begin{pmatrix}\n0.06 \\\\\n-0.08\n\\end{pmatrix}\n\\ \\text{Hartree}, \n\\qquad\n\\mathbf{H} =\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n\\ \\text{Hartree}.\n$$\nStarting from the second-order Taylor model and the definition of a minimizer, derive the expression for the Newton step $\\mathbf{s}_{N}$ in terms of $\\mathbf{g}$ and $\\mathbf{H}$, compute $\\mathbf{s}_{N}$ explicitly for the data above, and then evaluate the predicted quadratic energy change\n$$\n\\Delta E_{\\text{pred}} \\equiv m(\\mathbf{s}_{N}) - m(\\mathbf{0}).\n$$\nExpress the final energy change in Hartree and round your answer to five significant figures. The final response must be a single real number.", "solution": "The objective is to find the displacement $\\mathbf{s}$ that minimizes the quadratic model of the electronic energy:\n$$\nm(\\mathbf{s}) = E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s}\n$$\nA necessary condition for a minimum of a differentiable function is that its gradient vanishes. We must compute the gradient of $m(\\mathbf{s})$ with respect to the displacement vector $\\mathbf{s}$ and set it to the zero vector.\nThe gradient is given by:\n$$\n\\nabla_{\\mathbf{s}} m(\\mathbf{s}) = \\nabla_{\\mathbf{s}} \\left( E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s} \\right)\n$$\nThe term $E(\\mathbf{q}_{0})$ is a constant with respect to $\\mathbf{s}$, so its derivative is zero. The derivatives of the linear and quadratic terms are standard results from vector calculus:\n$$\n\\nabla_{\\mathbf{s}} (\\mathbf{g}^{\\top}\\mathbf{s}) = \\mathbf{g}\n$$\n$$\n\\nabla_{\\mathbf{s}} \\left(\\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s}\\right) = \\mathbf{H}\\mathbf{s}\n$$\nwhere the second result assumes a symmetric Hessian matrix $\\mathbf{H}$, which is true for a second derivative matrix. Combining these, the gradient of the model is:\n$$\n\\nabla_{\\mathbf{s}} m(\\mathbf{s}) = \\mathbf{g} + \\mathbf{H}\\mathbf{s}\n$$\nThe Newton step, denoted $\\mathbf{s}_{N}$, is the specific displacement that minimizes $m(\\mathbf{s})$. To find it, we set the gradient to the zero vector:\n$$\n\\mathbf{g} + \\mathbf{H}\\mathbf{s}_{N} = \\mathbf{0}\n$$\nSolving for $\\mathbf{s}_{N}$ yields the expression for the Newton step:\n$$\n\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}\n$$\n$$\n\\mathbf{s}_{N} = -\\mathbf{H}^{-1}\\mathbf{g}\n$$\nThis requires the Hessian matrix $\\mathbf{H}$ to be invertible. The determinant of the given Hessian is $\\det(\\mathbf{H}) = (1.2)(0.9) - (0.3)(0.3) = 1.08 - 0.09 = 0.99$. Since $\\det(\\mathbf{H}) \\neq 0$, the inverse exists and the Newton step is well-defined.\n\nNow, we compute $\\mathbf{s}_{N}$ using the provided data:\n$$\n\\mathbf{g} =\n\\begin{pmatrix}\n0.06 \\\\\n-0.08\n\\end{pmatrix}\n, \\qquad\n\\mathbf{H} =\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n$$\nWe solve the linear system $\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}$:\n$$\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n\\begin{pmatrix}\ns_1 \\\\\ns_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.06 \\\\\n0.08\n\\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n1. $1.2s_1 + 0.3s_2 = -0.06$\n2. $0.3s_1 + 0.9s_2 = 0.08$\n\nSolving this system gives the Newton step $\\mathbf{s}_{N} = \\begin{pmatrix} -13/165 \\\\ 19/165 \\end{pmatrix}$.\n\nNext, we evaluate the predicted quadratic energy change, $\\Delta E_{\\text{pred}}$:\n$$\n\\Delta E_{\\text{pred}} = m(\\mathbf{s}_{N}) - m(\\mathbf{0})\n$$\nSubstituting the definition of $m(\\mathbf{s})$:\n$$\nm(\\mathbf{s}_{N}) = E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N}\n$$\n$$\nm(\\mathbf{0}) = E(\\mathbf{q}_{0})\n$$\nSo,\n$$\n\\Delta E_{\\text{pred}} = \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N}\n$$\nWe can simplify this expression. From the derivation of the Newton step, we have $\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}$. Substituting this into the quadratic term:\n$$\n\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N} = \\mathbf{s}_{N}^{\\top}(-\\mathbf{g}) = - \\mathbf{s}_{N}^{\\top}\\mathbf{g} = -\\mathbf{g}^{\\top}\\mathbf{s}_{N}\n$$\nThe energy change becomes:\n$$\n\\Delta E_{\\text{pred}} = \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}(-\\mathbf{g}^{\\top}\\mathbf{s}_{N}) = \\frac{1}{2}\\mathbf{g}^{\\top}\\mathbf{s}_{N}\n$$\nNow, we compute this value:\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{2}\n\\begin{pmatrix}\n0.06 & -0.08\n\\end{pmatrix}\n\\begin{pmatrix}\n-13/165 \\\\\n19/165\n\\end{pmatrix}\n$$\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{2} \\left[ (0.06)\\left(-\\frac{13}{165}\\right) + (-0.08)\\left(\\frac{19}{165}\\right) \\right]\n$$\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{330} \\left[ -0.78 - 1.52 \\right] = \\frac{-2.3}{330} = -\\frac{23}{3300}\n$$\nPerforming the division:\n$$\n\\Delta E_{\\text{pred}} \\approx -0.00696969... \\ \\text{Hartree}\n$$\nRounding to five significant figures gives:\n$$\n-0.0069697\n$$", "answer": "$$\\boxed{-0.0069697}$$", "id": "2894209"}, {"introduction": "While the pure Newton step is theoretically elegant, its practical application depends on the accuracy of the computed Hessian matrix, which is subject to numerical errors. The sensitivity of the resulting step to these perturbations is a critical aspect of an algorithm's numerical stability. This practice introduces the matrix condition number, $\\kappa_2(\\mathbf{H})$, a key concept from numerical linear algebra that quantifies this sensitivity, helping you understand how errors in the Hessian can be amplified in the calculated step. [@problem_id:2894238]", "problem": "Consider a local quadratic model for the electronic energy surface in mass-weighted Cartesian coordinates near a current geometry in a geometry optimization. Let the Hessian matrix be the symmetric matrix\n$$\n\\mathbf{H} \\;=\\;\n\\begin{pmatrix}\n4 & 1 & 0 \\\\\n1 & 3 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix},\n$$\nand let the gradient be\n$$\n\\mathbf{g} \\;=\\;\n\\begin{pmatrix}\n0.10 \\\\\n-0.20 \\\\\n0.05\n\\end{pmatrix}.\n$$\nThe Newton step $\\mathbf{p}$ is defined implicitly by the linear system $\\mathbf{H}\\,\\mathbf{p} \\;=\\; -\\,\\mathbf{g}$. In practice, the Hessian used in a quantum chemistry code is often perturbed by small errors from numerical differentiation or incomplete convergence, so that the step actually computed, $\\tilde{\\mathbf{p}}$, satisfies $(\\mathbf{H} + \\Delta \\mathbf{H})\\,\\tilde{\\mathbf{p}} \\;=\\; -\\,\\mathbf{g}$ with a small perturbation $\\Delta \\mathbf{H}$. Using the spectral norm, analyze the first-order sensitivity of $\\mathbf{p}$ with respect to $\\Delta \\mathbf{H}$ by relating the relative change in $\\mathbf{p}$ to the relative size of $\\Delta \\mathbf{H}$ through the spectral condition number. Your analysis should begin from the linear algebraic definitions of the Newton step and of the matrix inverse, and proceed by first-order perturbation reasoning, without assuming any pre-stated perturbation bounds.\n\nCompute the spectral condition number $\\kappa_{2}(\\mathbf{H})$ associated with $\\mathbf{H}$, and use it to discuss the numerical stability of the Newton step for this problem in the sense of how the relative error in $\\tilde{\\mathbf{p}}$ scales with the relative perturbation in $\\mathbf{H}$. Provide your final answer as the exact value of $\\kappa_{2}(\\mathbf{H})$ in simplest closed form, with no rounding and no units.", "solution": "The problem requires an analysis of the numerical stability of the Newton step in a geometry optimization procedure by relating the sensitivity of the solution to perturbations in the Hessian matrix. This relationship is quantified by the spectral condition number of the Hessian. We will first derive the general first-order perturbation result and then apply it to the specific matrix provided.\n\nLet the exact Newton step $\\mathbf{p}$ be the solution to the linear system:\n$$\n\\mathbf{H}\\mathbf{p} = -\\mathbf{g}\n$$\nwhere $\\mathbf{H}$ is the exact Hessian matrix and $\\mathbf{g}$ is the gradient. In a practical computation, due to numerical errors, we work with a perturbed Hessian, $\\mathbf{H} + \\Delta\\mathbf{H}$, and solve for a perturbed step, $\\tilde{\\mathbf{p}}$:\n$$\n(\\mathbf{H} + \\Delta\\mathbf{H})\\tilde{\\mathbf{p}} = -\\mathbf{g}\n$$\nLet the error in the Newton step be $\\Delta\\mathbf{p} = \\tilde{\\mathbf{p}} - \\mathbf{p}$, which implies $\\tilde{\\mathbf{p}} = \\mathbf{p} + \\Delta\\mathbf{p}$. Substituting this into the perturbed equation gives:\n$$\n(\\mathbf{H} + \\Delta\\mathbf{H})(\\mathbf{p} + \\Delta\\mathbf{p}) = -\\mathbf{g}\n$$\nExpanding the left side yields:\n$$\n\\mathbf{H}\\mathbf{p} + \\mathbf{H}\\Delta\\mathbf{p} + \\Delta\\mathbf{H}\\mathbf{p} + \\Delta\\mathbf{H}\\Delta\\mathbf{p} = -\\mathbf{g}\n$$\nSince $\\mathbf{H}\\mathbf{p} = -\\mathbf{g}$, this simplifies to:\n$$\n\\mathbf{H}\\Delta\\mathbf{p} + \\Delta\\mathbf{H}\\mathbf{p} + \\Delta\\mathbf{H}\\Delta\\mathbf{p} = \\mathbf{0}\n$$\nFor a first-order analysis, we assume that the perturbations are small, meaning $\\|\\Delta\\mathbf{H}\\|$ and consequently $\\|\\Delta\\mathbf{p}\\|$ are small. Therefore, the term $\\Delta\\mathbf{H}\\Delta\\mathbf{p}$, which is second-order in small quantities, can be neglected. This leads to the first-order approximation:\n$$\n\\mathbf{H}\\Delta\\mathbf{p} \\approx -\\Delta\\mathbf{H}\\mathbf{p}\n$$\nAssuming the Hessian $\\mathbf{H}$ is invertible, we can solve for the error $\\Delta\\mathbf{p}$:\n$$\n\\Delta\\mathbf{p} \\approx -\\mathbf{H}^{-1}\\Delta\\mathbf{H}\\mathbf{p}\n$$\nTo analyze the magnitude of this error, we take the spectral norm (denoted by the subscript $2$) of both sides. Using the submultiplicative property of matrix norms ($\\|\\mathbf{A}\\mathbf{B}\\| \\le \\|\\mathbf{A}\\|\\|\\mathbf{B}\\|$ and $\\|\\mathbf{A}\\mathbf{x}\\| \\le \\|\\mathbf{A}\\|\\|\\mathbf{x}\\|$), we obtain an upper bound for the norm of the error:\n$$\n\\|\\Delta\\mathbf{p}\\|_2 \\le \\|\\mathbf{H}^{-1}\\|_2 \\|\\Delta\\mathbf{H}\\|_2 \\|\\mathbf{p}\\|_2\n$$\nTo express this in terms of relative errors, we divide by $\\|\\mathbf{p}\\|_2$ (assuming $\\mathbf{p} \\neq \\mathbf{0}$, which is true if $\\mathbf{g} \\neq \\mathbf{0}$):\n$$\n\\frac{\\|\\Delta\\mathbf{p}\\|_2}{\\|\\mathbf{p}\\|_2} \\le \\|\\mathbf{H}^{-1}\\|_2 \\|\\Delta\\mathbf{H}\\|_2\n$$\nTo relate this to the relative perturbation in the Hessian, $\\frac{\\|\\Delta\\mathbf{H}\\|_2}{\\|\\mathbf{H}\\|_2}$, we multiply and divide the right side by $\\|\\mathbf{H}\\|_2$:\n$$\n\\frac{\\|\\Delta\\mathbf{p}\\|_2}{\\|\\mathbf{p}\\|_2} \\le \\left(\\|\\mathbf{H}\\|_2 \\|\\mathbf{H}^{-1}\\|_2\\right) \\frac{\\|\\Delta\\mathbf{H}\\|_2}{\\|\\mathbf{H}\\|_2}\n$$\nThe quantity in parentheses is the definition of the spectral condition number of the matrix $\\mathbf{H}$, denoted $\\kappa_2(\\mathbf{H})$. Thus, we have the fundamental result:\n$$\n\\frac{\\|\\Delta\\mathbf{p}\\|_2}{\\|\\mathbf{p}\\|_2} \\le \\kappa_2(\\mathbf{H}) \\frac{\\|\\Delta\\mathbf{H}\\|_2}{\\|\\mathbf{H}\\|_2}\n$$\nThis inequality demonstrates that the condition number $\\kappa_2(\\mathbf{H})$ serves as an amplification factor relating the relative error in the Hessian to the maximum possible relative error in the computed Newton step.\n\nOur task now reduces to calculating $\\kappa_2(\\mathbf{H})$ for the given matrix:\n$$\n\\mathbf{H} = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & \\tfrac{1}{2} \\end{pmatrix}\n$$\nThe spectral norm of a symmetric matrix is equal to its spectral radius, which is the maximum of the absolute values of its eigenvalues, $\\|\\mathbf{H}\\|_2 = \\max_i |\\lambda_i|$. Consequently, for an invertible symmetric matrix, the condition number is the ratio of the largest to the smallest absolute eigenvalue:\n$$\n\\kappa_2(\\mathbf{H}) = \\frac{\\max_i |\\lambda_i|}{\\min_i |\\lambda_i|}\n$$\nThe matrix $\\mathbf{H}$ is block-diagonal. The eigenvalues of $\\mathbf{H}$ are the union of the eigenvalues of its diagonal blocks. One block is the scalar matrix $[\\frac{1}{2}]$, so one eigenvalue is immediately $\\lambda_1 = \\frac{1}{2}$. The other block is the $2 \\times 2$ submatrix:\n$$\n\\mathbf{A} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nThe eigenvalues of $\\mathbf{A}$ are found from its characteristic equation, $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$:\n$$\n\\det \\begin{pmatrix} 4-\\lambda & 1 \\\\ 1 & 3-\\lambda \\end{pmatrix} = (4-\\lambda)(3-\\lambda) - 1 = 0\n$$\n$$\n\\lambda^2 - 7\\lambda + 12 - 1 = 0\n$$\n$$\n\\lambda^2 - 7\\lambda + 11 = 0\n$$\nUsing the quadratic formula, the eigenvalues of this submatrix are:\n$$\n\\lambda = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(1)(11)}}{2} = \\frac{7 \\pm \\sqrt{49-44}}{2} = \\frac{7 \\pm \\sqrt{5}}{2}\n$$\nSo, the three eigenvalues of $\\mathbf{H}$ are $\\lambda_1 = \\frac{1}{2}$, $\\lambda_2 = \\frac{7 - \\sqrt{5}}{2}$, and $\\lambda_3 = \\frac{7 + \\sqrt{5}}{2}$. All eigenvalues are positive, indicating that $\\mathbf{H}$ is positive definite, as is expected for a Hessian matrix near a local minimum.\n\nWe must identify the maximum and minimum eigenvalues.\n$$\n\\lambda_1 = \\frac{1}{2} = 0.5\n$$\nSince $2  \\sqrt{5}  3$, we can estimate $\\lambda_2$:\n$$\n\\frac{7 - 3}{2}  \\frac{7 - \\sqrt{5}}{2}  \\frac{7 - 2}{2} \\implies 2  \\lambda_2  2.5\n$$\nThus, $\\lambda_2 > \\lambda_1$. The maximum eigenvalue is clearly $\\lambda_{\\max} = \\lambda_3 = \\frac{7 + \\sqrt{5}}{2}$. The minimum eigenvalue is $\\lambda_{\\min} = \\lambda_1 = \\frac{1}{2}$.\n\nThe spectral condition number is therefore:\n$$\n\\kappa_2(\\mathbf{H}) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\frac{7 + \\sqrt{5}}{2}}{\\frac{1}{2}} = 7 + \\sqrt{5}\n$$\nThis value, approximately $7 + 2.236 = 9.236$, is a measure of the numerical sensitivity. Since this number is not excessively large, the problem of computing the Newton step from this Hessian is considered well-conditioned. Small relative errors in $\\mathbf{H}$ will not be catastrophically amplified in the resulting step $\\mathbf{p}$.", "answer": "$$\n\\boxed{7 + \\sqrt{5}}\n$$", "id": "2894238"}, {"introduction": "For the large molecular systems common in modern research, computing and storing the full Hessian matrix is often computationally prohibitive. The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm masterfully circumvents this bottleneck by implicitly approximating the action of the inverse Hessian using only a stored history of recent gradient and displacement vectors. This hands-on exercise demystifies this powerful and ubiquitous optimization technique by guiding you through its core \"two-loop recursion\" mechanism. [@problem_id:164314]", "problem": "In computational quantum chemistry, geometry optimization algorithms are essential for finding stable molecular structures, which correspond to minima on the potential energy surface (PES). Quasi-Newton methods are a popular class of algorithms for this task. They iteratively approach a minimum by using the gradient of the energy, $g$, to approximate the inverse Hessian matrix, $H$, which contains information about the curvature of the PES.\n\nThe search direction, $p_k$, at step $k$ is determined by $p_k = -H_k g_k$, where $g_k$ is the gradient at the current geometry $x_k$. The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is a widely used quasi-Newton method that avoids the explicit formation and storage of the dense inverse Hessian matrix. Instead, it stores a small number, $m$, of the most recent displacement vectors, $s_i = x_{i+1} - x_i$, and gradient difference vectors, $y_i = g_{i+1} - g_i$.\n\nThe product $H_k g_k$ is computed efficiently using the L-BFGS two-loop recursion. Given the current gradient $g_k$ and the $m$ previous pairs of $(s_i, y_i)$, the procedure is as follows:\n\n1.  Initialize $q \\leftarrow g_k$.\n2.  **First loop:** For $i$ from $k-1$ down to $k-m$:\n    -   Compute $\\rho_i = \\frac{1}{y_i^T s_i}$.\n    -   Compute $\\alpha_i = \\rho_i s_i^T q$.\n    -   Update $q \\leftarrow q - \\alpha_i y_i$.\n3.  Choose an initial inverse Hessian approximation $H_k^{(0)}$. A standard choice is $H_k^{(0)} = \\gamma_k I$, where $I$ is the identity matrix and $\\gamma_k = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}}$.\n4.  Compute the initial version of the result vector: $z \\leftarrow H_k^{(0)} q$.\n5.  **Second loop:** For $i$ from $k-m$ up to $k-1$:\n    -   Compute $\\beta_i = \\rho_i y_i^T z$.\n    -   Update $z \\leftarrow z + s_i (\\alpha_i - \\beta_i)$.\n6.  The final result is $H_k g_k = z$. The search direction is $p_k = -z$.\n\n**Problem:**\nConsider a geometry optimization of a 2D system at step $k$. The L-BFGS algorithm is used with a memory of $m=2$. The current gradient and the two most recent displacement and gradient-difference vectors are given by:\n-   Current gradient: $g_k = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n-   Most recent update vectors (index $k-1$): $s_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{k-1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n-   Second most recent update vectors (index $k-2$): $s_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\nCalculate the search direction vector $p_k$ for the current step $k$.", "solution": "We apply the L-BFGS two-loop recursion with $m=2$. Let the indices be $i=k-1$ and $i=k-2$.\n\n1.  **First loop.** Initialize $q^{(0)}=g_k=\\begin{pmatrix}-1\\\\2\\end{pmatrix}$.\n\n    For $i=k-1, k-2$ compute $\\rho_i=\\frac{1}{y_i^T s_i}$, $\\alpha_i=\\rho_i s_i^T q$, and update $q$.\n\n    i)  $i=k-1$:\n    $y_{k-1}^T s_{k-1}=(2,1)\\cdot(1,0)=2 \\;\\Rightarrow\\; \\rho_{k-1}=\\frac{1}{2}$.\n    $\\alpha_{k-1}=\\frac{1}{2}(1,0)\\cdot(-1,2)=-\\frac{1}{2}$.\n    $q^{(1)}=(-1,2)-\\left(-\\frac{1}{2}\\right)(2,1) = ( -1,2 )+(1,\\frac{1}{2}) = (0,\\frac{5}{2})$.\n\n    ii) $i=k-2$:\n    $y_{k-2}^T s_{k-2}=(-1,2)\\cdot(0,1)=2 \\;\\Rightarrow\\; \\rho_{k-2}=\\frac{1}{2}$.\n    $\\alpha_{k-2}=\\frac{1}{2}(0,1)\\cdot(0,\\frac{5}{2}) = \\frac{5}{4}$.\n    $q^{(2)}=(0,\\frac{5}{2})-\\frac{5}{4}(-1,2) = (0,\\frac{5}{2})-(-\\frac{5}{4},\\frac{5}{2}) = (\\frac{5}{4},0)$.\n\n    After the first loop, the final $q$ is $q=\\begin{pmatrix}5/4\\\\0\\end{pmatrix}$.\n\n2.  **Form initial Hessian-times-vector.** Choose $\\gamma_k=\\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}} = \\frac{2}{2^2+1^2}=\\frac{2}{5}$.\n    The initial Hessian is $H^{(0)}_k=\\gamma_k I$.\n    Then, $z^{(0)}=H^{(0)}_k q=\\frac{2}{5}\\begin{pmatrix}5/4\\\\0\\end{pmatrix}=\\begin{pmatrix}1/2\\\\0\\end{pmatrix}$.\n\n3.  **Second loop.** For $i=k-2,k-1$ compute $\\beta_i=\\rho_i y_i^T z$ and update $z$.\n\n    i)  $i=k-2$:\n    $\\beta_{k-2}=\\frac{1}{2}(-1,2)\\cdot(\\frac{1}{2},0)=-\\frac{1}{4}$.\n    $z^{(1)}=(\\frac{1}{2},0)+(0,1)\\left(\\frac{5}{4}-\\left(-\\frac{1}{4}\\right)\\right) =(\\frac{1}{2},0)+(0,\\frac{3}{2}) =(\\frac{1}{2},\\frac{3}{2})$.\n\n    ii) $i=k-1$:\n    $\\beta_{k-1}=\\frac{1}{2}(2,1)\\cdot(\\frac{1}{2},\\frac{3}{2}) =\\frac{1}{2}(1+\\frac{3}{2})=\\frac{5}{4}$.\n    $z^{(2)}=(\\frac{1}{2},\\frac{3}{2})+(1,0)\\left(-\\frac{1}{2}-\\frac{5}{4}\\right) =(\\frac{1}{2},\\frac{3}{2})+(-\\frac{7}{4},0) =(-\\frac{5}{4},\\frac{3}{2})$.\n\nThus $H_k g_k=z=\\begin{pmatrix}-5/4\\\\3/2\\end{pmatrix}$ and the search direction is $p_k=-z=\\begin{pmatrix}5/4\\\\-3/2\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\tfrac{5}{4}\\\\-\\tfrac{3}{2}\\end{pmatrix}}$$", "id": "164314"}]}