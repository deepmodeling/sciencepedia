{"hands_on_practices": [{"introduction": "Before we can deploy sophisticated algorithms to find transition states, we must first master their fundamental mathematical identity. This exercise strips away the complexities of a real molecular system to focus on the core properties of a first-order saddle point on a simple, two-dimensional potential energy surface ($PES$). By analytically calculating the gradient and Hessian matrix, you will practice identifying all stationary points and classifying them based on their Morse index, providing a solid conceptual foundation for what transition state search algorithms are designed to locate [@problem_id:2827022].", "id": "2827022", "problem": "A potential energy surface for a two-dimensional model reaction coordinate is defined by the polynomial\n$$V(x,y)=x^{4}+y^{4}-2x^{2}+xy.$$\nIn the context of locating transition states as first-order saddle points on a potential energy surface, proceed as follows using only core definitions:\n- Compute the gradient vector $\\nabla V(x,y)$ and the Hessian matrix $H(x,y)$ of second derivatives.\n- Solve for all stationary points by imposing $\\nabla V(x,y)=\\mathbf{0}$ over $\\mathbb{R}^{2}$.\n- At each stationary point, determine the Morse index (defined as the number of negative eigenvalues of the Hessian matrix at that point), thereby classifying minima and first-order saddle points (transition states).\n\nReport as your final answer the total number of first-order saddle points (Morse index one) of $V(x,y)$. Provide no units. If any numerical evaluation is needed, give exact values; no rounding is required.", "solution": "The task is to find all first-order saddle points, which correspond to stationary points with a Morse index of one, for the given potential energy surface $V(x,y)$. A stationary point is a point $(x,y)$ where the gradient of the potential, $\\nabla V(x,y)$, is zero. The classification of a stationary point depends on the eigenvalues of the Hessian matrix, $H(x,y)$, evaluated at that point. The Morse index is the number of negative eigenvalues of the Hessian.\n\nThe potential energy surface is given by:\n$$V(x,y) = x^{4} + y^{4} - 2x^{2} + xy$$\n\nFirst, we compute the gradient vector $\\nabla V(x,y)$, which is composed of the partial derivatives with respect to $x$ and $y$.\n$$ \\frac{\\partial V}{\\partial x} = 4x^{3} - 4x + y $$\n$$ \\frac{\\partial V}{\\partial y} = 4y^{3} + x $$\nThe gradient vector is:\n$$ \\nabla V(x,y) = \\begin{pmatrix} 4x^{3} - 4x + y \\\\ 4y^{3} + x \\end{pmatrix} $$\n\nNext, we compute the Hessian matrix $H(x,y)$ of second partial derivatives.\n$$ \\frac{\\partial^{2} V}{\\partial x^{2}} = 12x^{2} - 4 $$\n$$ \\frac{\\partial^{2} V}{\\partial y^{2}} = 12y^{2} $$\n$$ \\frac{\\partial^{2} V}{\\partial x \\partial y} = \\frac{\\partial^{2} V}{\\partial y \\partial x} = 1 $$\nThe Hessian matrix is:\n$$ H(x,y) = \\begin{pmatrix} 12x^{2} - 4 & 1 \\\\ 1 & 12y^{2} \\end{pmatrix} $$\n\nTo find the stationary points, we set the gradient vector to the zero vector, $\\nabla V(x,y) = \\mathbf{0}$. This yields a system of two nonlinear equations:\n$$ (1) \\quad 4x^{3} - 4x + y = 0 $$\n$$ (2) \\quad 4y^{3} + x = 0 $$\n\nFrom equation (2), we can express $x$ in terms of $y$: $x = -4y^{3}$.\nAlternatively, we can express $y$ from equation (1): $y = 4x - 4x^{3}$.\nLet us substitute the expression for $y$ from (1) into (2):\n$$ 4(4x - 4x^{3})^{3} + x = 0 $$\n$$ 4[4x(1 - x^{2})]^{3} + x = 0 $$\n$$ 4(64x^{3})(1 - x^{2})^{3} + x = 0 $$\n$$ 256x^{3}(1 - x^{2})^{3} + x = 0 $$\nFactor out $x$:\n$$ x[256x^{2}(1 - x^{2})^{3} + 1] = 0 $$\nThis equation gives two possibilities for the solutions.\nCase A: $x = 0$. Substituting this into equation (2) gives $4y^{3} = 0$, which implies $y=0$. Thus, one stationary point is $(0,0)$.\n\nCase B: $256x^{2}(1 - x^{2})^{3} + 1 = 0$. Let $u = x^{2}$. Since $x$ is real, $u \\ge 0$. The equation becomes:\n$$ 256u(1 - u)^{3} + 1 = 0 $$\nLet $f(u) = 256u(1-u)^{3} + 1$. We need to find the number of positive roots of this function.\nLet's analyze $f(u)$ for $u \\ge 0$.\nThe derivative is $f'(u) = 256(1-u)^{3} + 256u \\cdot 3(1-u)^{2}(-1) = 256(1-u)^{2}[(1-u) - 3u] = 256(1-u)^{2}(1-4u)$.\nFor $u \\ge 0$, $f'(u) = 0$ when $u=1$ or $u=1/4$.\nThe behavior of $f(u)$ is as follows:\n- At $u=0$, $f(0) = 1$.\n- For $0 < u < 1/4$, $f'(u) > 0$, so $f(u)$ is increasing.\n- At $u=1/4$, $f(u)$ has a local maximum: $f(1/4) = 256(1/4)(3/4)^{3} + 1 = 64(27/64)+1 = 28$.\n- For $1/4 < u < 1$, $f'(u) < 0$, so $f(u)$ is decreasing.\n- At $u=1$, $f(1) = 256(1)(0)^{3} + 1 = 1$.\n- For $u > 1$, $f'(u) < 0$, so $f(u)$ continues to decrease. As $u \\to \\infty$, $f(u) \\approx -256u^{4} \\to -\\infty$.\nSince $f(1)=1$ and $f(u)$ is continuous and decreases towards $-\\infty$ for $u>1$, by the Intermediate Value Theorem, there must be exactly one root $u_{0}$ such that $u_{0} > 1$.\nIn the interval $[0,1]$, $f(u)$ is strictly positive, with a minimum value of $1$ at $u=0$ and $u=1$.\nThus, there is only one positive root $u_{0}$ for $f(u)=0$.\nThis single value $u_{0}$ for $x^{2}$ gives two real values for $x$: $x_{0} = \\sqrt{u_{0}}$ and $-x_{0} = -\\sqrt{u_{0}}$.\nFor each $x$, the corresponding $y$ is found from $y=4x-4x^{3}$.\nFor $x_{0}$, we have $y_{0} = 4x_{0} - 4x_{0}^{3}$.\nFor $-x_{0}$, we have $y = 4(-x_{0})-4(-x_{0})^{3} = -(4x_{0}-4x_{0}^{3}) = -y_{0}$.\nSo, in addition to $(0,0)$, we have two more stationary points: $(x_{0}, y_{0})$ and $(-x_{0}, -y_{0})$.\nIn total, there are three stationary points.\n\nNow we classify these points by evaluating the Hessian at each one.\n1. Stationary point $(0,0)$:\n$$ H(0,0) = \\begin{pmatrix} 12(0)^{2} - 4 & 1 \\\\ 1 & 12(0)^{2} \\end{pmatrix} = \\begin{pmatrix} -4 & 1 \\\\ 1 & 0 \\end{pmatrix} $$\nThe determinant of the Hessian is $\\det(H(0,0)) = (-4)(0) - (1)(1) = -1$.\nSince the determinant is the product of the eigenvalues ($\\lambda_{1}\\lambda_{2} = -1$), the eigenvalues must have opposite signs. One is positive, and one is negative. The number of negative eigenvalues, the Morse index, is $1$. Therefore, $(0,0)$ is a first-order saddle point.\n\n2. Stationary points $(\\pm x_{0}, \\pm y_{0})$:\nThe Hessian matrix depends on $x^{2}$ and $y^{2}$, so $H(x_{0}, y_{0}) = H(-x_{0}, -y_{0})$. We can analyze them together.\nThe trace of the Hessian is $\\mathrm{Tr}(H) = 12x_{0}^{2} - 4 + 12y_{0}^{2}$. Since $x_{0}^{2} = u_{0} > 1$, the term $12x_{0}^{2} - 4 > 12(1) - 4 = 8 > 0$. And $12y_{0}^{2} > 0$ because $y_{0} \\neq 0$ (if $y_{0}=0$, then $x_{0}=0$, contradicting $x_{0}^{2} > 1$). Thus, $\\mathrm{Tr}(H) > 0$.\nThe determinant of the Hessian is $\\det(H) = (12x_{0}^{2} - 4)(12y_{0}^{2}) - 1 = 144x_{0}^{2}y_{0}^{2} - 48y_{0}^{2} - 1$.\nTo simplify this, we use the relations from the stationary point conditions.\nFrom $x = -4y^{3}$, we have $x/y = -4y^{2}$, so $12y^{2} = -3x/y$.\nFrom $y = 4x - 4x^{3}$, we have $y/x = 4-4x^{2}$, so $12x^{2} = 3(4-y/x) = 12-3y/x$. Then $12x^{2}-4 = 8-3y/x$.\nSubstitute these into the determinant formula:\n$$ \\det(H) = (8 - 3y/x)(-3x/y) - 1 = -24x/y + 9 - 1 = 8 - 24x/y $$\nNow substitute $x/y = -4y^{2}$:\n$$ \\det(H) = 8 - 24(-4y^{2}) = 8 + 96y^{2} $$\nAt the points $(\\pm x_{0}, \\pm y_{0})$, $y_{0} \\neq 0$, so $y_{0}^{2} > 0$.\nTherefore, $\\det(H) = 8 + 96y_{0}^{2} > 8 > 0$.\nSince $\\mathrm{Tr}(H) > 0$ and $\\det(H) > 0$, both eigenvalues of the Hessian must be positive.\nThe Morse index is $0$. Therefore, the points $(x_{0}, y_{0})$ and $(-x_{0}, -y_{0})$ are both local minima.\n\nIn summary, the potential energy surface $V(x,y)$ has three stationary points:\n- One first-order saddle point at $(0,0)$ (Morse index $1$).\n- Two local minima at $(\\pm x_{0}, \\pm y_{0})$ (Morse index $0$).\n\nThe problem asks for the total number of first-order saddle points. Based on this analysis, there is only one.", "answer": "$$ \\boxed{1} $$"}, {"introduction": "Connecting the abstract mathematics of a saddle point to a physical chemical process requires a crucial change of coordinates. The motion of atoms along a reaction path depends on their masses, a factor not present in a simple Cartesian coordinate system. This practice guides you through the essential derivation of the mass-weighted Hessian, demonstrating how the fundamental equations of motion are transformed to correctly describe the collective movements of atoms, ensuring that calculated vibrational frequencies and reaction pathways are physically meaningful [@problem_id:2826956].", "id": "2826956", "problem": "Consider a molecular system of $N$ nuclei with nuclear Cartesian coordinates arranged into a $3N$-vector $x$, and a differentiable potential energy function $V(x)$ defined on a neighborhood of a stationary point. Let $M$ be the $3N \\times 3N$ diagonal mass matrix with diagonal entries repeating each nuclear mass three times (once per Cartesian component), and let $M^{1/2}$ denote the unique symmetric positive-definite square root of $M$. Define mass-weighted coordinates by $Q = M^{1/2} x$. Denote by $g(x)$ the Cartesian gradient $g(x) = \\nabla_{x} V(x)$ and by $H(x)$ the Cartesian Hessian with elements $H_{ij}(x) = \\partial^{2} V/\\partial x_{i} \\partial x_{j}$. Similarly, define the gradient and Hessian with respect to the mass-weighted coordinates as $\\tilde{g}(Q) = \\nabla_{Q} V(M^{-1/2} Q)$ and $\\tilde{H}(Q)$ with elements $\\tilde{H}_{ab}(Q) = \\partial^{2} V(M^{-1/2} Q)/\\partial Q_{a} \\partial Q_{b}$. Starting from the chain rule for multivariable functions and the linear relation $x = M^{-1/2} Q$, derive how the gradient and Hessian transform under this change of variables, and specifically determine the mass-weighted Hessian in terms of $M$ and the Cartesian Hessian $H$. Assume only that $V$ is twice continuously differentiable and that $M$ is diagonal with strictly positive entries so that $M^{1/2}$ exists and is symmetric.\n\nProvide your final result as a single closed-form analytic matrix expression for the mass-weighted Hessian in terms of $M$ and $H$, without an equality sign. No numerical evaluation is required. Do not include units. If you introduce any acronyms, write their full form at first use.", "solution": "The problem as stated is subjected to validation.\n\nGivens extracted verbatim from the problem statement:\n- A molecular system of $N$ nuclei.\n- Nuclear Cartesian coordinates: a $3N$-vector $x$.\n- Potential energy function: $V(x)$, differentiable.\n- Mass matrix: $M$, a $3N \\times 3N$ diagonal matrix with diagonal entries repeating each nuclear mass three times.\n- Square root of mass matrix: $M^{1/2}$, the unique symmetric positive-definite square root of $M$.\n- Mass-weighted coordinates: $Q = M^{1/2} x$.\n- Cartesian gradient: $g(x) = \\nabla_{x} V(x)$.\n- Cartesian Hessian: $H_{ij}(x) = \\partial^{2} V/\\partial x_{i} \\partial x_{j}$.\n- Mass-weighted gradient: $\\tilde{g}(Q) = \\nabla_{Q} V(M^{-1/2} Q)$.\n- Mass-weighted Hessian: $\\tilde{H}_{ab}(Q) = \\partial^{2} V(M^{-1/2} Q)/\\partial Q_{a} \\partial Q_{b}$.\n- Coordinate relation: $x = M^{-1/2} Q$.\n- Assumptions: $V$ is twice continuously differentiable ($C^2$); $M$ is diagonal with strictly positive entries.\n\nValidation verdict:\nThe problem is valid. It is scientifically grounded, as the transformation to mass-weighted coordinates is a standard and fundamental procedure in computational chemistry and molecular dynamics for analyzing vibrational modes and reaction paths. The problem is well-posed, objective, and self-contained. All necessary definitions and mathematical relationships are provided, and there are no contradictions. The task is a direct application of the chain rule from multivariable calculus, a standard exercise in theoretical physics and chemistry.\n\nThe derivation proceeds as follows.\n\nThe transformation from mass-weighted coordinates $Q$ to Cartesian coordinates $x$ is given by the linear relation $x = M^{-1/2} Q$. In component form, this is $x_i = \\sum_{a=1}^{3N} (M^{-1/2})_{ia} Q_a$, where $i$ and $a$ are indices running from $1$ to $3N$.\n\nFirst, we derive the transformation for the gradient vector. The $a$-th component of the gradient with respect to mass-weighted coordinates, $\\tilde{g}_a(Q)$, is given by the partial derivative $\\frac{\\partial V}{\\partial Q_a}$. Applying the chain rule for multivariable functions, we express this derivative in terms of derivatives with respect to the Cartesian coordinates $x_i$:\n$$\n\\tilde{g}_a(Q) = \\frac{\\partial V}{\\partial Q_a} = \\sum_{i=1}^{3N} \\frac{\\partial V}{\\partial x_i} \\frac{\\partial x_i}{\\partial Q_a}\n$$\nThe term $\\frac{\\partial V}{\\partial x_i}$ is the $i$-th component of the Cartesian gradient, $g_i(x)$. The term $\\frac{\\partial x_i}{\\partial Q_a}$ is an element of the Jacobian matrix of the transformation $x(Q)$. From the linear relation $x_i = \\sum_{k=1}^{3N} (M^{-1/2})_{ik} Q_k$, we find that $\\frac{\\partial x_i}{\\partial Q_a} = (M^{-1/2})_{ia}$. These coefficients are constant with respect to the coordinates.\nSubstituting these into the expression for $\\tilde{g}_a(Q)$:\n$$\n\\tilde{g}_a(Q) = \\sum_{i=1}^{3N} g_i(x) (M^{-1/2})_{ia}\n$$\nThis expression represents the $a$-th component of a matrix-vector product. In matrix notation, this is $\\tilde{g} = (M^{-1/2})^T g$. The mass matrix $M$ is diagonal by definition. Its square root $M^{1/2}$ and inverse square root $M^{-1/2}$ are also diagonal and therefore symmetric. Thus, $(M^{-1/2})^T = M^{-1/2}$. The gradient transformation is:\n$$\n\\tilde{g}(Q) = M^{-1/2} g(x(Q))\n$$\nNext, we derive the transformation for the Hessian matrix. The elements of the mass-weighted Hessian, $\\tilde{H}_{ab}(Q)$, are the second partial derivatives $\\frac{\\partial^2 V}{\\partial Q_a \\partial Q_b}$. We differentiate the expression for the first partial derivative, $\\tilde{g}_a(Q)$, with respect to $Q_b$:\n$$\n\\tilde{H}_{ab}(Q) = \\frac{\\partial}{\\partial Q_b} \\left( \\frac{\\partial V}{\\partial Q_a} \\right) = \\frac{\\partial}{\\partial Q_b} \\left( \\sum_{i=1}^{3N} \\frac{\\partial V}{\\partial x_i} \\frac{\\partial x_i}{\\partial Q_a} \\right)\n$$\nSince the terms $\\frac{\\partial x_i}{\\partial Q_a} = (M^{-1/2})_{ia}$ are constants, we can move the differentiation operator inside the summation:\n$$\n\\tilde{H}_{ab}(Q) = \\sum_{i=1}^{3N} \\left( \\frac{\\partial}{\\partial Q_b} \\frac{\\partial V}{\\partial x_i} \\right) \\frac{\\partial x_i}{\\partial Q_a}\n$$\nWe apply the chain rule again to the term in the parentheses:\n$$\n\\frac{\\partial}{\\partial Q_b} \\left( \\frac{\\partial V}{\\partial x_i} \\right) = \\sum_{j=1}^{3N} \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial V}{\\partial x_i} \\right) \\frac{\\partial x_j}{\\partial Q_b}\n$$\nThe term $\\frac{\\partial}{\\partial x_j} (\\frac{\\partial V}{\\partial x_i})$ is the element $(j,i)$ of the Cartesian Hessian matrix, $H_{ji}(x)$. The term $\\frac{\\partial x_j}{\\partial Q_b}$ is the constant coefficient $(M^{-1/2})_{jb}$.\nSubstituting this back into the expression for $\\tilde{H}_{ab}(Q)$:\n$$\n\\tilde{H}_{ab}(Q) = \\sum_{i=1}^{3N} \\left( \\sum_{j=1}^{3N} H_{ji}(x) (M^{-1/2})_{jb} \\right) (M^{-1/2})_{ia}\n$$\nBy reordering the terms within the finite summations, we get:\n$$\n\\tilde{H}_{ab}(Q) = \\sum_{i=1}^{3N} \\sum_{j=1}^{3N} (M^{-1/2})_{ia} H_{ji}(x) (M^{-1/2})_{jb}\n$$\nThis expression corresponds to the $(a,b)$ element of a product of three matrices. Let matrix $A = M^{-1/2}$ and matrix $B = H$. The expression is $\\sum_{i,j} A_{ia} B_{ji} A_{jb}$. This is recognized as the $(a,b)$ element of the matrix product $(A^T B^T A)$.\nThus, in matrix form, the transformation is:\n$$\n\\tilde{H} = (M^{-1/2})^T H^T M^{-1/2}\n$$\nWe now apply the given properties of the matrices. First, as $M$ is a diagonal matrix, $M^{-1/2}$ is also diagonal and thus is a symmetric matrix, i.e., $(M^{-1/2})^T = M^{-1/2}$. Second, the potential energy function $V(x)$ is assumed to be twice continuously differentiable ($C^2$). By Schwarz's theorem on the equality of mixed partials, its Hessian matrix $H(x)$ is symmetric, i.e., $H^T = H$.\nSubstituting these symmetry properties into the matrix equation gives the final relationship between the mass-weighted Hessian and the Cartesian Hessian:\n$$\n\\tilde{H} = M^{-1/2} H M^{-1/2}\n$$\nThis is the desired result. The mass-weighted Hessian is obtained by a similarity-like transformation of the Cartesian Hessian, where the transformation matrix is the inverse square root of the mass matrix.", "answer": "$$\n\\boxed{M^{-1/2} H M^{-1/2}}\n$$"}, {"introduction": "Locating a saddle point is a delicate balancing act: we must ascend along one direction (the unstable mode) while descending in all others. Standard minimization algorithms are designed only for descent and will fail. This advanced practice takes you under the hood of modern optimization techniques, challenging you to derive from first principles how a well-posed search step is constructed by either projecting the problem into separate subspaces or by using a constrained optimization approach, known as the trust-region method [@problem_id:2827002].", "id": "2827002", "problem": "Consider a local quadratic model of a potential energy surface (PES) near an iterate for locating a first-order saddle point (transition state): the model is given by the quadratic form $q(s) = f + g^{\\top} s + \\tfrac{1}{2} s^{\\top} H s$, where $g$ is the gradient and $H$ is the Hessian. Suppose $H$ is indefinite with exactly one negative eigenvalue $\\lambda_{-} < 0$ and associated unit eigenvector $v_{-}$, and all remaining directions are strictly positive in curvature. You are asked to construct well-posed Newton-type steps from first principles in two conceptually different but mathematically consistent ways.\n\nFirst, justify how a subspace-projected transition-state step can be constructed by decomposing the space into the direct sum of the span of $v_{-}$ and its orthogonal complement. Start from the definitions of the quadratic model and the Newton stationarity condition, and derive the form of a step that ascends along $v_{-}$ but descends along the orthogonal complement. Your derivation must start from the basic definitions of the gradient and Hessian of the quadratic model, and from the decomposition defined by the orthogonal projectors $P_{-} = v_{-} v_{-}^{\\top}$ and $P_{+} = I - P_{-}$.\n\nSecond, starting from constrained optimization fundamentals, derive the shifted-Hessian trust-region formulation that produces a well-posed step by solving the constrained problem $\\min_{s} q(s)$ subject to the norm constraint $\\|s\\| = \\Delta$, where $\\Delta > 0$ is a prescribed trust-region radius. Form the Lagrangian, use the Karush–Kuhn–Tucker (KKT) conditions, and show that the step takes the form of solving a shifted linear system $(H + \\sigma I) s = -g$ with a scalar shift $\\sigma$ chosen so that the trust-region constraint is satisfied. Derive the scalar secular equation that determines $\\sigma$ from first principles.\n\nNow specialize to a two-dimensional case in the orthonormal eigenbasis of $H$, so that $H$ is diagonal with eigenvalues $\\lambda_{-} = -1$ and $\\lambda_{+} = 3$, and the gradient decomposes as $g = a v_{-} + b v_{+}$ with $a = 2$ and $b = 3$. Let the trust-region radius be $\\Delta = \\sqrt{109}/5$. Using only your derived secular equation, determine the unique Lagrange multiplier $\\sigma$ with $\\sigma > -\\lambda_{-}$ that enforces $\\|s\\| = \\Delta$ for the trust-region step.\n\nReport only the value of $\\sigma$ as your final answer. No rounding is required, and no units are involved.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It poses a standard, non-trivial problem in the field of computational chemistry concerning the location of transition states on a potential energy surface. There are no inconsistencies, missing information, or logical flaws. We proceed to the solution.\n\nThe problem requires a two-part theoretical derivation followed by a specific calculation.\n\nFirst, we derive the subspace-projected transition-state step. We begin with the local quadratic model of the potential energy surface (PES) near an iterate $x_k$:\n$$q(s) = f + g^{\\top} s + \\frac{1}{2} s^{\\top} H s$$\nHere, $s$ is the step vector, $f = q(0)$ is the energy, $g = \\nabla q(0)$ is the gradient, and $H = \\nabla^2 q(0)$ is the Hessian matrix. A stationary point is located where the gradient of the model vanishes:\n$$\\nabla q(s) = g + H s = 0$$\nThis defines the Newton step $s$ via the linear system $H s = -g$.\nThe Hessian $H$ is specified to have exactly one negative eigenvalue, $\\lambda_{-} < 0$, with corresponding unit eigenvector $v_{-}$. All other eigenvalues are strictly positive. This structure corresponds to a first-order saddle point. We decompose the vector space into the direct sum of the one-dimensional subspace spanned by $v_{-}$ and its orthogonal complement. This is accomplished using the orthogonal projectors $P_{-} = v_{-} v_{-}^{\\top}$ and $P_{+} = I - P_{-}$, where $I$ is the identity matrix.\n\nAny vector, including the step $s$ and the gradient $g$, can be decomposed as follows:\n$$s = P_{-} s + P_{+} s \\equiv s_{-} + s_{+}$$\n$$g = P_{-} g + P_{+} g \\equiv g_{-} + g_{+}$$\nSince $v_{-}$ is an eigenvector of the symmetric matrix $H$, the subspace it spans is invariant under $H$. Likewise, its orthogonal complement is also an invariant subspace. Therefore, $H$ commutes with both projectors, $HP_{-} = P_{-}H$ and $HP_{+} = P_{+}H$. Substituting the decompositions into the Newton equation gives:\n$$H(s_{-} + s_{+}) = -(g_{-} + g_{+})$$\n$$H s_{-} + H s_{+} = -g_{-} - g_{+}$$\nProjecting this equation onto the two orthogonal subspaces isolates the components:\n$1.$ Projecting with $P_{-}$: $P_{-}(H s_{-} + H s_{+}) = -P_{-}(g_{-} + g_{+}) \\implies P_{-} H s_{-} = -g_{-}$. Since $s_{-}$ is in the range of $P_{-}$, $P_{-}s_{-} = s_{-}$, and commutativity gives $H P_{-} s_{-} = H s_{-} = -g_{-}$. As $s_{-}$ is parallel to $v_{-}$, we have $H s_{-} = \\lambda_{-} s_{-}$. This yields $\\lambda_{-} s_{-} = -g_{-}$, and thus the step component is:\n$$s_{-} = -\\frac{1}{\\lambda_{-}} g_{-}$$\nThe change in energy associated with this component is approximately $\\Delta q_{-} \\approx g_{-}^{\\top}s_{-} = g_{-}^{\\top}(-\\frac{1}{\\lambda_{-}}g_{-}) = -\\frac{1}{\\lambda_{-}} \\|g_{-}\\|^2$. Since $\\lambda_{-} < 0$, the coefficient $-\\frac{1}{\\lambda_{-}}$ is positive. Thus, $\\Delta q_{-} \\ge 0$. This component of the step ascends the potential energy surface along the direction of the gradient component $g_{-}$, which lies along the unstable mode $v_{-}$.\n\n$2.$ Projecting with $P_{+}: P_{+}(H s_{-} + H s_{+}) = -P_{+}(g_{-} + g_{+}) \\implies H s_{+} = -g_{+}$. This is a Newton-Raphson step for a minimization problem restricted to the subspace defined by $P_{+}$. In this subspace, all eigenvalues of $H$ are positive, meaning the restricted Hessian is positive definite. Such a step seeks the minimum of the quadratic model in this subspace. The change in energy is $\\Delta q_{+} = g_{+}^{\\top}s_{+} + \\frac{1}{2}s_{+}^{\\top}Hs_{+} = g_{+}^{\\top}(-H^{-1}g_{+}) + \\frac{1}{2}g_{+}^{\\top}H^{-1}HH^{-1}g_{+} = -\\frac{1}{2} g_{+}^{\\top}H^{-1}g_{+}$, where the inverse is taken on the subspace. Since $H$ is positive definite on this subspace, $H^{-1}$ is also positive definite, and $\\Delta q_{+} \\le 0$. This is a descent step.\n\nThe total step $s = s_{-} + s_{+}$ thus correctly combines an ascent along the unique unstable mode with a descent in all orthogonal directions, which is the required behavior for locating a first-order saddle point.\n\nSecond, we derive the shifted-Hessian trust-region formulation. The problem is to minimize the quadratic model $q(s)$ subject to a strict norm constraint, which defines the trust-region boundary:\n$$\\min_{s} q(s) \\quad \\text{subject to} \\quad \\|s\\|^2 = \\Delta^2$$\nWe use the method of Lagrange multipliers. The Lagrangian function is:\n$$L(s, \\sigma) = q(s) + \\frac{\\sigma}{2} (s^{\\top}s - \\Delta^2) = f + g^{\\top}s + \\frac{1}{2}s^{\\top}Hs + \\frac{\\sigma}{2}(s^{\\top}s - \\Delta^2)$$\nThe Karush-Kuhn-Tucker (KKT) conditions for an optimal solution require that the gradient of the Lagrangian with respect to $s$ is zero:\n$$\\nabla_s L(s, \\sigma) = g + H s + \\sigma s = 0$$\nThis provides the shifted linear system:\n$$(H + \\sigma I)s = -g$$\nAssuming the matrix $(H + \\sigma I)$ is invertible, the step $s$ is a function of the Lagrange multiplier $\\sigma$:\n$$s(\\sigma) = -(H + \\sigma I)^{-1} g$$\nThis step must satisfy the trust-region constraint, $\\|s(\\sigma)\\|^2 = \\Delta^2$. To write this equation more explicitly, we work in the orthonormal eigenbasis of $H$, denoted by $\\{v_i\\}$ with corresponding eigenvalues $\\{\\lambda_i\\}$. The inverse matrix $(H + \\sigma I)^{-1}$ has the same eigenvectors $v_i$ and eigenvalues $(\\lambda_i + \\sigma)^{-1}$. The step can be written as:\n$$s(\\sigma) = -\\sum_i \\frac{v_i^{\\top}g}{\\lambda_i + \\sigma} v_i$$\nThe squared norm of the step is found using the orthonormality of the eigenvectors ($v_i^{\\top}v_j = \\delta_{ij}$):\n$$\\|s(\\sigma)\\|^2 = s(\\sigma)^{\\top}s(\\sigma) = \\left( -\\sum_i \\frac{v_i^{\\top}g}{\\lambda_i + \\sigma} v_i^{\\top} \\right) \\left( -\\sum_j \\frac{v_j^{\\top}g}{\\lambda_j + \\sigma} v_j \\right) = \\sum_i \\frac{(v_i^{\\top}g)^2}{(\\lambda_i + \\sigma)^2}$$\nEquating this to $\\Delta^2$ yields the scalar secular equation for $\\sigma$:\n$$\\sum_i \\frac{(v_i^{\\top}g)^2}{(\\lambda_i + \\sigma)^2} = \\Delta^2$$\nThe KKT conditions for a constrained minimum require the shifted Hessian $(H + \\sigma I)$ to be positive semi-definite, which implies $\\lambda_i + \\sigma \\ge 0$ for all $i$. This sets a lower bound on the multiplier: $\\sigma \\ge -\\lambda_{\\min}$, where $\\lambda_{\\min}$ is the smallest eigenvalue of $H$. In our context, $\\lambda_{\\min} = \\lambda_{-}$.\n\nFinally, we apply this formulation to the specialized two-dimensional case. The given data are:\nEigenvalues: $\\lambda_{-} = -1$, $\\lambda_{+} = 3$.\nGradient components in the eigenbasis: $v_{-}^{\\top}g = a = 2$ and $v_{+}^{\\top}g = b = 3$.\nTrust-region radius: $\\Delta = \\frac{\\sqrt{109}}{5}$, which implies $\\Delta^2 = \\frac{109}{25}$.\n\nThe secular equation becomes:\n$$\\frac{(v_{-}^{\\top}g)^2}{(\\lambda_{-} + \\sigma)^2} + \\frac{(v_{+}^{\\top}g)^2}{(\\lambda_{+} + \\sigma)^2} = \\Delta^2$$\nSubstituting the numerical values:\n$$\\frac{2^2}{(-1 + \\sigma)^2} + \\frac{3^2}{(3 + \\sigma)^2} = \\frac{109}{25}$$\n$$\\frac{4}{(\\sigma - 1)^2} + \\frac{9}{(\\sigma + 3)^2} = \\frac{109}{25}$$\nWe must find the unique solution $\\sigma$ that satisfies $\\sigma > -\\lambda_{-} = -(-1) = 1$. Let us test integer values for $\\sigma > 1$.\nFor $\\sigma = 2$:\n$$\\frac{4}{(2 - 1)^2} + \\frac{9}{(2 + 3)^2} = \\frac{4}{1^2} + \\frac{9}{5^2} = 4 + \\frac{9}{25} = \\frac{100}{25} + \\frac{9}{25} = \\frac{109}{25}$$\nThis matches the right-hand side of the equation. Therefore, $\\sigma=2$ is a solution.\nTo verify uniqueness for $\\sigma > 1$, we examine the function $f(\\sigma) = \\frac{4}{(\\sigma-1)^2} + \\frac{9}{(\\sigma+3)^2}$. Its derivative is $f'(\\sigma) = -\\frac{8}{(\\sigma-1)^3} - \\frac{18}{(\\sigma+3)^3}$. For all $\\sigma > 1$, both $(\\sigma-1)^3$ and $(\\sigma+3)^3$ are positive, making $f'(\\sigma)$ strictly negative. A strictly monotonic function can intersect a horizontal line at most once. Since we have found the solution $\\sigma = 2$ in the interval $(1, \\infty)$, it is the unique solution in this domain.", "answer": "$$\\boxed{2}$$"}]}