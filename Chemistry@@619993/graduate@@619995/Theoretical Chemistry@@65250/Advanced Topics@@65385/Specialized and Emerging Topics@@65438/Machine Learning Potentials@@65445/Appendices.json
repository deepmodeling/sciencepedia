{"hands_on_practices": [{"introduction": "The foundation of any machine learning potential lies in its ability to represent atomic environments in a way that respects fundamental physical symmetries. This exercise [@problem_id:2457438] guides you through the construction of a Behler-Parrinello-style symmetry function, a cornerstone descriptor in modern potentials. By implementing this function from first principles, you will gain a concrete understanding of how the abstract requirements of translational, rotational, and permutational invariance are encoded into a feature vector suitable for a machine learning model.", "problem": "Implement a program that derives and computes a basic two-body Behler–Parrinello-style symmetry function for Argon (Ar) atoms and evaluates it on a small test suite of geometries. The purpose is to connect invariance requirements of machine-learned interatomic potentials to a concrete descriptor and to demonstrate numerical behavior under different parameter choices. The overall context is that the total potential energy surface of a system can be approximated as a sum of atomic contributions, each depending on a localized, symmetry-invariant representation of its neighborhood, as in High-Dimensional Neural Network Potentials (HDNNP). Your task is to derive, from invariance principles, a two-body radial symmetry function and implement it.\n\nStart from the following fundamental base:\n- Translational and rotational invariance of a scalar potential energy imply that a local descriptor for an atom must be constructed from internal coordinates such as interatomic distances.\n- For finite-range interactions and locality in the learned mapping, impose a smooth cutoff at a finite radius so that distant atoms beyond a cutoff do not contribute and forces remain well-behaved.\n- To resolve the radial distribution around an atom, use a radial basis with tunable width and center parameters so that the representation can distinguish environments at different length scales.\n\nFrom these principles, derive and then implement a two-body radial symmetry function $G^2$ for a chosen central atom $i$ of the form\n- a sum over neighbor atoms $j \\neq i$,\n- a smooth, finite-range cutoff function that is $C^1$-continuous and equals zero at the cutoff radius,\n- and a localized radial weight that can shift and sharpen around a chosen distance.\n\nConcretely specify and use the following forms in your derivation and implementation:\n- Use the cosine cutoff\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\nwith the cosine argument in radians.\n- Use a Gaussian-like radial basis\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\nwith width parameter $\\eta$ and shift $R_s$.\n- Combine them in the two-body symmetry function\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\nwhere $r_{ij}$ is the Euclidean distance between atoms $i$ and $j$.\n\nAll atoms are Argon (Ar), treated as a single chemical species, so no species-dependent weighting is required. Distances $r_{ij}$, cutoff $R_c$, and shift $R_s$ must be expressed in Ångström, and $\\eta$ in $\\text{Å}^{-2}$. The cosine function must take its argument in radians.\n\nProgram requirements:\n- Implement a function that, given a set of Cartesian coordinates (in Ångström), an index $i$ for the central atom, and parameters $(\\eta, R_s, R_c)$, computes $G_i^{2}(\\eta, R_s, R_c)$ using the formulas above.\n- Use standard three-dimensional Euclidean distance. Do not apply periodic boundary conditions.\n- Numerical stability: Exclude self-interaction ($j = i$). Distances $r_{ij}$ are strictly nonnegative; do not special-case $r_{ij} = 0$ beyond excluding self-interaction.\n\nTest suite:\nEvaluate $G_i^{2}$ for each of the following five cases. Each case specifies $(\\text{positions}, i, R_c, \\eta, R_s)$, with all distances in Ångström and $\\eta$ in $\\text{Å}^{-2}$:\n- Case A:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- Case B:\n  - positions: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case C:\n  - positions: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case D:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- Case E:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\nOutput specification:\n- For each case, compute a single floating-point value $G_i^{2}$.\n- Round each result to exactly $6$ decimal places using standard rounding.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, E. For example, an output with generic placeholders should look like \"[0.123456,0.000000,0.000000,1.234567,0.654321]\".", "solution": "The problem presented is valid, scientifically sound, and well-posed. It requires the derivation and implementation of a two-body radial symmetry function, a fundamental component of modern machine-learned interatomic potentials such as High-Dimensional Neural Network Potentials (HDNNPs). We shall first deduce the form of this function from first principles, and then detail the algorithm for its computation.\n\nThe potential energy $E$ of a system of atoms is a scalar quantity. For it to be physically meaningful, it must be invariant under translation and rotation of the entire system, as well as under the permutation of identical atoms. In the HDNNP scheme, the total energy is decomposed into atomic contributions $E_i$, where $E = \\sum_i E_i$. Each atomic energy $E_i$ is a function of the local environment of atom $i$, characterized by a set of descriptors or \"symmetry functions,\" $\\{G_i\\}$. Therefore, these symmetry functions must themselves be invariant to the aforementioned transformations.\n\n1.  **Translational and Rotational Invariance**: These symmetries dictate that the descriptors for atom $i$ must depend only on the internal coordinates of its local environment, not on the absolute Cartesian coordinates of the atoms in a global frame. The simplest set of internal coordinates consists of the scalar distances $r_{ij}$ between the central atom $i$ and its neighbors $j$. Any function of these distances, $G_i = F(\\{r_{ij}\\}_{j \\neq i})$, is automatically invariant to rigid translation and rotation of the atomic assembly.\n\n2.  **Permutational Invariance**: The energy contribution of atom $i$ must not depend on the arbitrary labeling of its identical neighbors. If atoms $j$ and $k$ are of the same species, swapping them must not change the value of the descriptor. The simplest mathematical construct that satisfies this is a sum over all neighbors. Thus, we propose a descriptor of the form $G_i = \\sum_{j \\neq i} g(r_{ij})$, where $g$ is some function of the interatomic distance. This form is the basis of the two-body symmetry function.\n\n3.  **Locality and Smoothness**: Physical interactions are local in nature; the influence of very distant atoms is negligible. To model this, we introduce a smooth cutoff function, $f_c(r_{ij}; R_c)$, which multiplies the contribution of each neighbor. This function must be equal to $1$ for small distances, and smoothly go to $0$ as the distance $r_{ij}$ approaches a cutoff radius $R_c$. For distances $r_{ij} > R_c$, the contribution is exactly zero. The requirement for smoothness, specifically $C^1$-continuity (continuous first derivative), is critical. The forces on atoms are calculated as the negative gradient of the potential energy, $\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$. Discontinuities in the first derivative of the energy would lead to unphysical, infinite forces. The provided cosine cutoff function is:\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    At the cutoff radius $r = R_c$, the function value is $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$, ensuring continuity. Its derivative is $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$. At $r = R_c$, the derivative is $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$, which matches the derivative of the zero function for $r > R_c$. Thus, the function is $C^1$-continuous as required.\n\n4.  **Radial Resolution**: A simple sum of cutoff functions would only provide a weighted count of neighbors within the cutoff sphere. To create a descriptor that can distinguish different radial structures, we introduce a radial basis function. The specified Gaussian form, $\\exp[-\\eta(r_{ij} - R_s)^2]$, serves this purpose. This function is centered at a distance $R_s$ and has a characteristic width controlled by the parameter $\\eta$. A larger $\\eta$ corresponds to a narrower, more sharply peaked Gaussian. By using a set of these functions with different parameters $(\\eta, R_s)$, one can resolve the radial distribution of neighbors around the central atom $i$.\n\nCombining these four principles—invariance from using distances, permutation symmetry from summation, locality from a smooth cutoff, and resolution from a radial basis—we arrive at the specified two-body radial symmetry function, designated as $G_i^2$:\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\nThe sum is over all atoms $j$ in the system, excluding the central atom $i$. For each neighbor $j$, we calculate its contribution only if its distance $r_{ij}$ from atom $i$ is less than or equal to the cutoff radius $R_c$.\n\nThe computational procedure is as follows:\nGiven a set of Cartesian coordinates for $N$ atoms, $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$, a central atom index $i$, and parameters $\\eta$, $R_s$, and $R_c$:\n1.  Initialize the symmetry function value, $G_i^2$, to $0$.\n2.  Identify the coordinate vector of the central atom, $\\mathbf{r}_i$.\n3.  Iterate through all other atoms $j$ where $j \\in \\{0, 1, ..., N-1\\}$ and $j \\neq i$.\n4.  For each neighbor $j$, compute the Euclidean distance $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$.\n5.  Check if $r_{ij} \\le R_c$. If not, the contribution from atom $j$ is $0$, and we proceed to the next neighbor.\n6.  If $r_{ij} \\le R_c$, calculate the two components of the term:\n    -   The radial basis term: $T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$.\n    -   The cutoff function term: $T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$.\n7.  Add the product of these terms, $T_{\\text{rad}} \\times T_{\\text{cut}}$, to the running sum for $G_i^2$.\n8.  After iterating through all neighbors $j$, the final sum is the value of the symmetry function for atom $i$.\n\nThis procedure will now be implemented and applied to the five specified test cases. All units must be consistent; distances ($r_{ij}$, $R_s$, $R_c$) are in Ångström ($\\text{Å}$), and the parameter $\\eta$ is in $\\text{Å}^{-2}$, ensuring the argument of the exponential is dimensionless.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "Once we have a suitable representation for atomic structures, the next step is to train a model that maps these descriptors to potential energies. This practice [@problem_id:2457470] uses kernel ridge regression (KRR) on a simple one-dimensional system to explore the relationship between data, model flexibility, and prediction accuracy. By deliberately introducing an erroneous \"poisoned\" data point, you will perform a controlled numerical experiment to investigate how hyperparameters like the kernel length-scale $\\ell$ and regularization parameter $\\lambda$ govern a model's robustness and the locality of its response to data.", "problem": "You will implement and analyze a one-dimensional Machine Learning Potential trained by kernel ridge regression on a diatomic molecule’s bond-stretch coordinate, with and without a deliberately poisoned training point. The task focuses on quantifying how a single egregiously wrong energy label perturbs the learned potential locally (near the poisoned configuration) versus globally (across the entire domain). Your program must be a complete, runnable program that computes specified metrics for a fixed test suite and outputs the results in the required format.\n\nContext and fundamental base:\n- A diatomic potential energy curve is a function of the internuclear distance $r$ in angstroms. A scientifically plausible model is the Morse potential,\n$$\nE_{\\mathrm{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2,\n$$\nwith depth $D_e$ in electronvolts, stiffness parameter $a$ in inverse angstroms, and equilibrium distance $r_e$ in angstroms. This potential is a smooth, well-tested model of bond stretching and is used here as the ground truth.\n- In supervised regression, kernel ridge regression (KRR) arises from minimizing the squared empirical prediction error plus a Tikhonov regularization term. Given inputs $x_i$ and outputs $y_i$, the hypothesis space induced by a positive-definite kernel $k(\\cdot,\\cdot)$ yields a solution that is a linear combination of kernel sections, and the prediction at a point $x$ has the form $f(x) = \\sum_{i=1}^N \\alpha_i k(x, x_i)$, where the coefficients $\\alpha_i$ are determined by minimizing the regularized empirical risk.\n- The Gaussian kernel with length scale $\\ell$,\n$$\nk(x,z) = \\exp\\left(-\\frac{(x - z)^2}{2\\ell^2}\\right),\n$$\nis a standard, smooth, radial basis function. Its locality is governed by $\\ell$: small $\\ell$ concentrates influence locally; large $\\ell$ spreads influence more globally.\n- The regularization parameter $\\lambda > 0$ controls the trade-off between data fit and smoothness, dampening the influence of any single data point.\n\nTraining and evaluation setup:\n- Use the Morse potential with parameters $D_e = 0.2\\ \\mathrm{eV}$, $a = 2.5\\ \\mathrm{\\AA^{-1}}$, and $r_e = 0.74\\ \\mathrm{\\AA}$.\n- Construct a training set of $N=13$ bond lengths $r_i$ uniformly spaced over $[0.6, 1.2]\\ \\mathrm{\\AA}$, inclusive.\n- Define the clean training labels $y_i^{\\mathrm{clean}} = E_{\\mathrm{true}}(r_i)$ in electronvolts.\n- Define a poisoned training set by selecting one training coordinate $r_p$ from the training grid and replacing its label by $y_p^{\\mathrm{poison}} = y_p^{\\mathrm{clean}} + \\Delta$, with $\\Delta = 1.0$ in electronvolts, while all other labels remain as in the clean set.\n- Train two KRR models (one on the clean labels and one on the poisoned labels) using the same kernel and the same regularization.\n- Evaluate on a uniform test grid of $M=1001$ points in $[0.5, 1.5]\\ \\mathrm{\\AA}$, inclusive.\n\nFor each test case below, compute the following three metrics:\n1. Local impact at the poisoned configuration:\n$$\nI_{\\mathrm{local}} = \\left| f_{\\mathrm{poison}}(r_p) - f_{\\mathrm{clean}}(r_p) \\right| \\quad \\text{in electronvolts}.\n$$\n2. Global mean absolute deviation over the test domain:\n$$\nI_{\\mathrm{global}} = \\frac{1}{M}\\sum_{j=1}^{M} \\left| f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) \\right| \\quad \\text{in electronvolts}.\n$$\n3. Relative degradation of accuracy (unitless) measured by the ratio of root-mean-square errors (RMSEs) against the ground truth over the test grid:\n$$\nR_{\\mathrm{RMSE}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}.\n$$\n\nKernel and regularization specification:\n- Use the Gaussian kernel with length scale $\\ell$ specified per test case.\n- Use Tikhonov regularization parameter $\\lambda$ specified per test case.\n\nTest suite:\nCompute the triplet $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$ for each of the following four cases. In all cases, the poison magnitude is $\\Delta = 1.0\\ \\mathrm{eV}$, and the training point $r_p$ is chosen from the training grid.\n- Case $1$ (happy path, moderate locality): $\\ell = 0.05\\ \\mathrm{\\AA}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.90\\ \\mathrm{\\AA}$.\n- Case $2$ (more global influence): $\\ell = 0.20\\ \\mathrm{\\AA}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.90\\ \\mathrm{\\AA}$.\n- Case $3$ (stronger regularization): $\\ell = 0.05\\ \\mathrm{\\AA}$, $\\lambda = 1\\times 10^{-2}$, $r_p = 0.90\\ \\mathrm{\\AA}$.\n- Case $4$ (boundary poison): $\\ell = 0.05\\ \\mathrm{\\AA}$, $\\lambda = 1\\times 10^{-6}$, $r_p = 0.60\\ \\mathrm{\\AA}$.\n\nImplementation requirements:\n- Implement kernel ridge regression in one dimension by solving the normal equations implied by minimizing the regularized empirical risk over the reproducing kernel Hilbert space induced by the Gaussian kernel.\n- Use only the specified parameters and grids given above. All energies must be in electronvolts, all lengths in angstroms, and angles are not used. Report $I_{\\mathrm{local}}$ and $I_{\\mathrm{global}}$ in electronvolts, and $R_{\\mathrm{RMSE}}$ as a unitless ratio.\n- Round each reported float to $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, output a list of four lists, one per test case, each inner list containing the three floats $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$ rounded to $6$ decimals. For example, the printed output must have the form\n$$\n\\left[\\left[i_{1,1},i_{1,2},i_{1,3}\\right],\\left[i_{2,1},i_{2,2},i_{2,3}\\right],\\left[i_{3,1},i_{3,2},i_{3,3}\\right],\\left[i_{4,1},i_{4,2},i_{4,3}\\right]\\right],\n$$\nwhere all $i_{k,m}$ are decimal numbers rounded to $6$ places. The output must be exactly one line and contain no additional text.", "solution": "The problem as stated is valid. It is scientifically grounded in the fields of computational chemistry and machine learning, well-posed with a clear and deterministic procedure, and is formulated with objective, unambiguous language. All necessary parameters and definitions are provided. I will therefore proceed with a full solution.\n\nThe core of the problem is to quantify the impact of a single erroneous data point—a form of data poisoning—on a kernel ridge regression (KRR) model. The model is trained to learn the potential energy surface of a diatomic molecule, for which the ground truth is given by the Morse potential:\n$$E_{\\text{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2$$\nThe parameters are specified as $D_e = 0.2\\ \\mathrm{eV}$, $a = 2.5\\ \\mathrm{\\AA^{-1}}$, and $r_e = 0.74\\ \\mathrm{\\AA}$.\n\nThe KRR model predicts the energy at a given internuclear distance $r$ as a linear combination of kernel functions centered at the training data points $\\{r_i\\}_{i=1}^N$:\n$$f(r) = \\sum_{i=1}^{N} \\alpha_i k(r, r_i)$$\nThe kernel is the Gaussian kernel with a characteristic length scale $\\ell$:\n$$k(r, r') = \\exp\\left(-\\frac{(r - r')^2}{2\\ell^2}\\right)$$\nThe coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_N)^T$ are determined by minimizing a regularized least-squares cost function. This leads to the following system of linear equations:\n$$(\\mathbf{K} + \\lambda \\mathbf{I}) \\alpha = \\mathbf{y}$$\nHere, $\\mathbf{K}$ is the $N \\times N$ kernel matrix with entries $K_{ij} = k(r_i, r_j)$, $\\mathbf{y}$ is the vector of training energy labels, $\\lambda > 0$ is the Tikhonov regularization parameter, and $\\mathbf{I}$ is the $N \\times N$ identity matrix. This linear system has a unique solution for $\\alpha$, which is then used for prediction.\n\nThe procedure is as follows:\n1.  Construct the training grid of $N=13$ points, $\\mathbf{r}_{\\text{train}}$, uniformly spaced in $[0.6, 1.2]\\ \\mathrm{\\AA}$.\n2.  Construct the test grid of $M=1001$ points, $\\mathbf{r}_{\\text{test}}$, uniformly spaced in $[0.5, 1.5]\\ \\mathrm{\\AA}$.\n3.  Generate the \"clean\" training labels, $\\mathbf{y}_{\\text{clean}}$, by evaluating $E_{\\text{true}}(r)$ at each point in $\\mathbf{r}_{\\text{train}}$.\n4.  For each test case, specified by parameters $(\\ell, \\lambda, r_p)$:\n    a.  Create the \"poisoned\" training labels, $\\mathbf{y}_{\\text{poison}}$, by taking $\\mathbf{y}_{\\text{clean}}$ and adding a perturbation $\\Delta = 1.0\\ \\mathrm{eV}$ to the label corresponding to the coordinate $r_p$.\n    b.  Solve for the coefficient vector $\\alpha_{\\text{clean}}$ using $(\\mathbf{r}_{\\text{train}}, \\mathbf{y}_{\\text{clean}})$.\n    c.  Solve for the coefficient vector $\\alpha_{\\text{poison}}$ using $(\\mathbf{r}_{\\text{train}}, \\mathbf{y}_{\\text{poison}})$.\n    d.  Use these coefficients to generate two potential energy functions: $f_{\\text{clean}}(r)$ and $f_{\\text{poison}}(r)$.\n    e.  Evaluate both functions on the test grid $\\mathbf{r}_{\\text{test}}$ and at the specific point $r_p$.\n    f.  Compute the three required metrics:\n        i.  The local impact, $I_{\\text{local}} = | f_{\\text{poison}}(r_p) - f_{\\text{clean}}(r_p) |$.\n        ii. The global mean absolute deviation, $I_{\\text{global}} = \\frac{1}{M}\\sum_{j=1}^{M} | f_{\\text{poison}}(r_j^{\\text{test}}) - f_{\\text{clean}}(r_j^{\\text{test}}) |$.\n        iii. The relative RMSE degradation, $R_{\\text{RMSE}}$, defined as the ratio of the root-mean-square error of $f_{\\text{poison}}$ to the RMSE of $f_{\\text{clean}}$, both with respect to the ground truth $E_{\\text{true}}$ over the test grid.\n\nThis entire computational experiment will be conducted for each of the four specified test cases. The results will be compiled and presented in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the impact of a poisoned data point on a Kernel Ridge Regression\n    model of a diatomic molecule's potential energy curve.\n    \"\"\"\n    # Define constants and problem setup from the statement.\n    DE = 0.2     # eV\n    A = 2.5      # 1/Angstrom\n    RE = 0.74    # Angstrom\n    DELTA = 1.0  # eV\n\n    N_TRAIN = 13\n    N_TEST = 1001\n    \n    # Define training and testing grids.\n    r_train = np.linspace(0.6, 1.2, N_TRAIN)\n    r_test = np.linspace(0.5, 1.5, N_TEST)\n\n    def morse_potential(r, De, a, re):\n        \"\"\"Calculates the Morse potential energy.\"\"\"\n        return De * (1 - np.exp(-a * (r - re)))**2\n\n    def gaussian_kernel(x, z, ell):\n        \"\"\"\n        Computes the Gaussian kernel matrix between two sets of points.\n        x and z are 1D arrays.\n        \"\"\"\n        dist_sq = np.subtract.outer(x, z)**2\n        return np.exp(-dist_sq / (2 * ell**2))\n\n    def train_krr(r_train, y_train, ell, lam):\n        \"\"\"Trains a KRR model and returns the coefficients alpha.\"\"\"\n        K = gaussian_kernel(r_train, r_train, ell)\n        # Solve the linear system (K + lambda*I) * alpha = y\n        A = K + lam * np.identity(K.shape[0])\n        alpha = np.linalg.solve(A, y_train)\n        return alpha\n\n    def predict_krr(r_predict, r_train, alpha, ell):\n        \"\"\"Makes predictions using a trained KRR model.\"\"\"\n        K_pred = gaussian_kernel(r_predict, r_train, ell)\n        return K_pred @ alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (ell, lambda, r_p)\n        (0.05, 1e-6, 0.90), # Case 1: Moderate locality\n        (0.20, 1e-6, 0.90), # Case 2: More global influence\n        (0.05, 1e-2, 0.90), # Case 3: Stronger regularization\n        (0.05, 1e-6, 0.60), # Case 4: Boundary poison\n    ]\n\n    # Generate ground truth data.\n    y_true_train = morse_potential(r_train, DE, A, RE)\n    y_true_test = morse_potential(r_test, DE, A, RE)\n    \n    all_results = []\n    for ell, lam, r_p in test_cases:\n        # Find the index of the poisoned point in the training grid.\n        p_idx = np.argmin(np.abs(r_train - r_p))\n        \n        # Create clean and poisoned training labels.\n        y_clean = np.copy(y_true_train)\n        y_poison = np.copy(y_true_train)\n        y_poison[p_idx] += DELTA\n\n        # Train both clean and poisoned models.\n        alpha_clean = train_krr(r_train, y_clean, ell, lam)\n        alpha_poison = train_krr(r_train, y_poison, ell, lam)\n\n        # Make predictions on the test grid.\n        f_clean_test = predict_krr(r_test, r_train, alpha_clean, ell)\n        f_poison_test = predict_krr(r_test, r_train, alpha_poison, ell)\n\n        # Make predictions at the poisoned point r_p for I_local.\n        f_clean_rp = predict_krr(np.array([r_p]), r_train, alpha_clean, ell)[0]\n        f_poison_rp = predict_krr(np.array([r_p]), r_train, alpha_poison, ell)[0]\n\n        # Metric 1: Local impact I_local\n        i_local = np.abs(f_poison_rp - f_clean_rp)\n\n        # Metric 2: Global mean absolute deviation I_global\n        i_global = np.mean(np.abs(f_poison_test - f_clean_test))\n\n        # Metric 3: Relative degradation of accuracy R_RMSE\n        rmse_clean = np.sqrt(np.mean((f_clean_test - y_true_test)**2))\n        rmse_poison = np.sqrt(np.mean((f_poison_test - y_true_test)**2))\n        \n        if np.isclose(rmse_clean, 0.0):\n            r_rmse = np.inf\n        else:\n            r_rmse = rmse_poison / rmse_clean\n            \n        all_results.append([i_local, i_global, r_rmse])\n\n    # Final print statement in the exact required format.\n    inner_lists_str = []\n    for res in all_results:\n        formatted_res = [f\"{x:.6f}\" for x in res]\n        inner_lists_str.append(f\"[{','.join(formatted_res)}]\")\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "2457470"}, {"introduction": "A key advantage of machine learning potentials is their ability to provide analytical forces, which are essential for running large-scale molecular dynamics simulations. This final exercise [@problem_id:2784660] moves from model training to application, tasking you with a pencil-and-paper derivation of the atomic forces from a neural network potential. You will apply the multivariable chain rule—a process known as backpropagation—to derive a closed-form expression for the energy gradient, solidifying your understanding of how forces emerge from the architecture of the potential.", "problem": "Consider a molecular system of $N$ atoms with Cartesian coordinates collected in $\\mathbf{R}=\\{R_{k\\alpha}\\}$, where $k\\in\\{1,\\dots,N\\}$ indexes atoms and $\\alpha\\in\\{x,y,z\\}$ indexes Cartesian components. On the Born–Oppenheimer potential energy surface, the potential energy $E(\\mathbf{R};\\Theta)$ is modeled by an atom-wise Artificial Neural Network (ANN) potential as\n$$\nE(\\mathbf{R};\\Theta)=\\sum_{i=1}^{N}\\varepsilon\\!\\left(\\mathbf{G}_{i}(\\mathbf{R});\\Theta\\right),\n$$\nwhere $\\mathbf{G}_{i}(\\mathbf{R})\\in\\mathbb{R}^{d}$ is a differentiable, symmetry-preserving descriptor for the local environment of atom $i$, and $\\Theta$ denotes all network parameters. The scalar atomic energy $\\varepsilon(\\mathbf{G}_{i};\\Theta)$ is produced by a feedforward network with $L$ hidden layers:\n- Input: $\\mathbf{h}_{i}^{(0)}=\\mathbf{G}_{i}\\in\\mathbb{R}^{d}$.\n- For each layer $\\ell\\in\\{1,\\dots,L\\}$: preactivation $\\mathbf{a}_{i}^{(\\ell)}=\\mathbf{W}^{(\\ell)}\\mathbf{h}_{i}^{(\\ell-1)}+\\mathbf{b}^{(\\ell)}$, activation $\\mathbf{h}_{i}^{(\\ell)}=\\boldsymbol{\\phi}^{(\\ell)}\\!\\left(\\mathbf{a}_{i}^{(\\ell)}\\right)$, where $\\mathbf{W}^{(\\ell)}\\in\\mathbb{R}^{n_{\\ell}\\times n_{\\ell-1}}$, $\\mathbf{b}^{(\\ell)}\\in\\mathbb{R}^{n_{\\ell}}$, $n_{0}=d$, and $n_{L}$ is the width of the last hidden layer. The activation $\\boldsymbol{\\phi}^{(\\ell)}$ acts elementwise and is differentiable.\n- Output layer (linear): $\\varepsilon(\\mathbf{G}_{i};\\Theta)=\\mathbf{w}^{(L+1)\\top}\\mathbf{h}_{i}^{(L)}+b^{(L+1)}$, with $\\mathbf{w}^{(L+1)}\\in\\mathbb{R}^{n_{L}}$ and $b^{(L+1)}\\in\\mathbb{R}$.\n\nDefine for each $\\ell\\in\\{1,\\dots,L\\}$ the diagonal matrix $\\mathbf{D}_{i}^{(\\ell)}=\\mathrm{diag}\\!\\left(\\boldsymbol{\\phi}^{(\\ell)\\prime}\\!\\left(\\mathbf{a}_{i}^{(\\ell)}\\right)\\right)\\in\\mathbb{R}^{n_{\\ell}\\times n_{\\ell}}$, where $\\boldsymbol{\\phi}^{(\\ell)\\prime}$ is the elementwise derivative of $\\boldsymbol{\\phi}^{(\\ell)}$ evaluated at $\\mathbf{a}_{i}^{(\\ell)}$.\n\nUsing only the chain rule of multivariable calculus and the above definitions, derive a compact, closed-form expression for the Cartesian derivative $\\partial E/\\partial R_{k\\alpha}$ in terms of:\n- Descriptor Jacobians $\\partial \\mathbf{G}_{i}/\\partial R_{k\\alpha}\\in\\mathbb{R}^{d}$, and\n- Neural network parameter-dependent factors built from $\\mathbf{W}^{(\\ell)}$, $\\mathbf{w}^{(L+1)}$, and the activation-derivative matrices $\\mathbf{D}_{i}^{(\\ell)}$.\n\nYour final result must be a single scalar analytic expression that contracts a row vector (depending on $\\Theta$ and activations) with $\\partial \\mathbf{G}_{i}/\\partial R_{k\\alpha}$ and then sums over $i\\in\\{1,\\dots,N\\}$. Provide the final expression only (no equality sign) in your final answer. No numerical evaluation is required. If you choose to mention forces, recall that the physical force component is $-\\partial E/\\partial R_{k\\alpha}$, but your task is to provide the expression for $\\partial E/\\partial R_{k\\alpha}$ itself.", "solution": "The problem statement is a valid, well-posed theoretical derivation within the domain of machine learning potentials in computational chemistry. The task is to derive the analytical gradient of the total potential energy, modeled by an atom-wise neural network, with respect to a Cartesian atomic coordinate. The derivation relies solely on the chain rule of multivariable calculus.\n\nThe total potential energy $E$ is given as a sum of atomic energy contributions $\\varepsilon_i$:\n$$\nE(\\mathbf{R};\\Theta)=\\sum_{i=1}^{N}\\varepsilon_{i}(\\mathbf{G}_{i}(\\mathbf{R});\\Theta)\n$$\nTo find the derivative with respect to a specific Cartesian coordinate $R_{k\\alpha}$, we apply the derivative operator to this sum. By the linearity of differentiation, the derivative of the sum is the sum of the derivatives:\n$$\n\\frac{\\partial E}{\\partial R_{k\\alpha}} = \\frac{\\partial}{\\partial R_{k\\alpha}} \\sum_{i=1}^{N} \\varepsilon_{i} = \\sum_{i=1}^{N} \\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}}\n$$\nEach atomic energy $\\varepsilon_{i}$ depends on the Cartesian coordinates $\\mathbf{R}$ implicitly through its corresponding descriptor vector $\\mathbf{G}_{i}(\\mathbf{R}) \\in \\mathbb{R}^{d}$. Applying the multivariable chain rule to find $\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}}$ gives:\n$$\n\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}} = \\sum_{j=1}^{d} \\frac{\\partial \\varepsilon_{i}}{\\partial G_{ij}} \\frac{\\partial G_{ij}}{\\partial R_{k\\alpha}}\n$$\nwhere $G_{ij}$ is the $j$-th component of the vector $\\mathbf{G}_{i}$. This sum can be written compactly as a matrix product (or dot product) of a row vector and a column vector:\n$$\n\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}} = \\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} \\frac{\\partial \\mathbf{G}_{i}}{\\partial R_{k\\alpha}}\n$$\nHere, $\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}$ is the gradient of the scalar $\\varepsilon_{i}$ with respect to the vector $\\mathbf{G}_{i}$, which is a column vector of size $d$. The term $\\frac{\\partial \\mathbf{G}_{i}}{\\partial R_{k\\alpha}}$ is the Jacobian of the descriptor vector with respect to the scalar coordinate $R_{k\\alpha}$, which is also a column vector of size $d$.\n\nThe central task is to determine the derivative of the neural network output $\\varepsilon_i$ with respect to its input $\\mathbf{G}_i$. This is achieved by applying the chain rule sequentially through the layers of the network, a process known as backpropagation. Let us define the gradient of $\\varepsilon_i$ with respect to the activation vector of layer $\\ell$ as $\\boldsymbol{\\delta}_{i}^{(\\ell)} \\equiv \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell)}}$, which is a column vector of dimension $n_{\\ell}$.\n\nThe derivation proceeds backwards from the output layer $(L+1)$ to the input layer $(0)$.\n\n1.  **Output Layer**: The atomic energy is given by $\\varepsilon_{i} = \\mathbf{w}^{(L+1)\\top}\\mathbf{h}_{i}^{(L)} + b^{(L+1)}$. The gradient with respect to the last hidden layer's activation $\\mathbf{h}_{i}^{(L)}$ is:\n    $$\n    \\boldsymbol{\\delta}_{i}^{(L)} = \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(L)}} = \\mathbf{w}^{(L+1)}\n    $$\n\n2.  **Hidden Layers**: For any layer $\\ell \\in \\{1, \\dots, L\\}$, we must relate $\\boldsymbol{\\delta}_{i}^{(\\ell-1)}$ to $\\boldsymbol{\\delta}_{i}^{(\\ell)}$. The chain of dependencies is $\\mathbf{h}_{i}^{(\\ell-1)} \\to \\mathbf{a}_{i}^{(\\ell)} \\to \\mathbf{h}_{i}^{(\\ell)}$. Using the chain rule:\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\left(\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\\right)^{\\top} \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell)}} = \\left(\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\\right)^{\\top} \\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n    The Jacobian matrix $\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}$ is found by composing the linear and activation steps:\n    $$\n    \\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{a}_{i}^{(\\ell)}} \\frac{\\partial \\mathbf{a}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\n    $$\n    From the problem definitions, $\\frac{\\partial \\mathbf{a}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\mathbf{W}^{(\\ell)}$ and $\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{a}_{i}^{(\\ell)}} = \\mathbf{D}_{i}^{(\\ell)}$. Therefore, the Jacobian is $\\mathbf{D}_{i}^{(\\ell)}\\mathbf{W}^{(\\ell)}$. Substituting this into the recurrence for $\\boldsymbol{\\delta}_{i}^{(\\ell-1)}$:\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = \\left(\\mathbf{D}_{i}^{(\\ell)}\\mathbf{W}^{(\\ell)}\\right)^{\\top} \\boldsymbol{\\delta}_{i}^{(\\ell)} = (\\mathbf{W}^{(\\ell)})^{\\top}(\\mathbf{D}_{i}^{(\\ell)})^{\\top}\\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n    As $\\mathbf{D}_{i}^{(\\ell)}$ is a diagonal matrix, it is symmetric: $(\\mathbf{D}_{i}^{(\\ell)})^{\\top} = \\mathbf{D}_{i}^{(\\ell)}$. The recurrence relation simplifies to:\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = (\\mathbf{W}^{(\\ell)})^{\\top}\\mathbf{D}_{i}^{(\\ell)}\\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n\n3.  **Input Layer**: We unroll this recurrence from $\\ell=L$ down to $\\ell=1$ to find the gradient with respect to the input layer's activation, $\\mathbf{h}_{i}^{(0)} = \\mathbf{G}_{i}$:\n    $$\n    \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}} = \\boldsymbol{\\delta}_{i}^{(0)} = (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\boldsymbol{\\delta}_{i}^{(1)} = (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\left( (\\mathbf{W}^{(2)})^{\\top}\\mathbf{D}_{i}^{(2)}\\boldsymbol{\\delta}_{i}^{(2)} \\right) = \\dots\n    $$\n    This recursive substitution yields the full expression:\n    $$\n    \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}} = \\left( (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\right) \\left( (\\mathbf{W}^{(2)})^{\\top}\\mathbf{D}_{i}^{(2)} \\right) \\cdots \\left( (\\mathbf{W}^{(L)})^{\\top}\\mathbf{D}_{i}^{(L)} \\right) \\mathbf{w}^{(L+1)}\n    $$\n\nTo form the required row vector for the contraction, we transpose this gradient vector:\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = \\left( \\left( (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\right) \\cdots \\left( (\\mathbf{W}^{(L)})^{\\top}\\mathbf{D}_{i}^{(L)} \\right) \\mathbf{w}^{(L+1)} \\right)^{\\top}\n$$\nUsing the property $(ABC)^\\top = C^\\top B^\\top A^\\top$ and recalling that $(\\mathbf{M}^\\top)^\\top = \\mathbf{M}$ and $\\mathbf{D}_{i}^{(\\ell)\\top} = \\mathbf{D}_{i}^{(\\ell)}$, we get:\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = (\\mathbf{w}^{(L+1)})^{\\top} \\left( \\mathbf{D}_{i}^{(L)}\\mathbf{W}^{(L)} \\right) \\left( \\mathbf{D}_{i}^{(L-1)}\\mathbf{W}^{(L-1)} \\right) \\cdots \\left( \\mathbf{D}_{i}^{(1)}\\mathbf{W}^{(1)} \\right)\n$$\nThis expression can be written compactly using product notation, where the product index runs in decreasing order:\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right)\n$$\n\nFinally, substituting this row vector back into the expression for the total energy derivative, we arrive at the final closed-form expression:\n$$\n\\frac{\\partial E}{\\partial R_{k\\alpha}} = \\sum_{i=1}^{N} \\left( (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) \\right) \\frac{\\partial \\mathbf{G}_i}{\\partial R_{k\\alpha}}\n$$\nThis expression represents the total Cartesian derivative as a sum over atomic contributions. Each contribution is a contraction of a row vector, determined by the network parameters and activations for atom $i$, with the Jacobian of the descriptor for atom $i$ with respect to the given Cartesian coordinate.", "answer": "$$\n\\boxed{\\sum_{i=1}^{N} \\left( (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) \\right) \\frac{\\partial \\mathbf{G}_i}{\\partial R_{k\\alpha}}}\n$$", "id": "2784660"}]}