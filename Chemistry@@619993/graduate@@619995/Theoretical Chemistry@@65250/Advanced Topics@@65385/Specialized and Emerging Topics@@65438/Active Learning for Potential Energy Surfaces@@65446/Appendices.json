{"hands_on_practices": [{"introduction": "Active learning aims to intelligently select the most informative data points for model training, minimizing the cost of expensive quantum chemistry calculations. A cornerstone of this approach is uncertainty sampling, where we query the potential energy surface at configurations where the model is most uncertain. This exercise [@problem_id:2760116] takes you to the heart of this principle, asking you to perform a first-principles calculation of how a Gaussian Process (GP) model quantifies predictive uncertainty. By computing the posterior variance and relating it to information gain, you will gain a foundational understanding of the theoretical engine that drives many modern active learning workflows.", "problem": "In data-efficient construction of a Potential Energy Surface (PES), an active learner selects new quantum chemistry calculations to reduce epistemic uncertainty most effectively. Consider a one-dimensional reaction coordinate $s$ along which a zero-mean Gaussian Process (GP) prior is placed on the latent energy $f(s)$ with covariance function $k(s,s') = \\sigma_{f}^{2}\\exp\\!\\big(-\\frac{(s-s')^{2}}{2\\ell^{2}}\\big)$. Independent observation noise is modeled as Gaussian with variance $\\sigma_{n}^{2}$. You have already performed electronic structure calculations at two configurations $s_{1} = 0$ and $s_{2} = 1$ (numerical energy values are not needed to compute the requested quantities). You consider a candidate configuration $s_{c} = 0.5$ for the next computation.\n\nAssume hyperparameters $\\sigma_{f}^{2} = 1$ (in $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$), $\\ell = 1$ (in the same length units as $s$), and $\\sigma_{n}^{2} = 0.01$ (in $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$). Using only fundamental properties of multivariate Gaussian conditioning and differential entropy, compute the posterior variance of the latent energy $f(s_{c})$ given the two existing inputs and then interpret this quantity as a single-step information-gain proxy for active learning. Specifically, treat the expected information gain from querying at $s_{c}$ as the mutual information between the new noisy observation and the latent energy at $s_{c}$ conditioned on the current dataset.\n\nReport as your final answer the numerical value of the expected information gain at $s_{c}$, expressed in nats and rounded to four significant figures. Do not include units in your final boxed answer.", "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The concepts are standard in the field of machine learning for theoretical chemistry. Therefore, I will proceed with the solution.\n\nThe objective is to compute the expected information gain from performing a new quantum chemistry calculation at the candidate configuration $s_c$. This gain is defined as the mutual information between the prospective noisy observation $y_c$ at $s_c$ and the true latent energy $f_c \\equiv f(s_c)$, conditioned on the existing dataset $D$. The dataset $D$ consists of observations at configurations $s_1 = 0$ and $s_2 = 1$. The information gain, denoted $IG$, is thus given by:\n$$\nIG = I(y_c; f_c | D)\n$$\nBy the properties of mutual information and differential entropy $h(\\cdot)$, this can be expressed in two equivalent forms:\n$$\nI(y_c; f_c | D) = h(f_c | D) - h(f_c | y_c, D) = h(y_c | D) - h(y_c | f_c, D)\n$$\nFor a Gaussian random variable $X$ with variance $\\sigma^2$, the differential entropy is $h(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$. The information gain is thus a function of variances.\n\nLet us use the second form, $h(y_c | D) - h(y_c | f_c, D)$.\nThe observation model is $y_c = f_c + \\epsilon_c$, where $\\epsilon_c \\sim \\mathcal{N}(0, \\sigma_n^2)$ is independent noise.\nConditioned on the true latent value $f_c$, the distribution of the observation $y_c$ is $p(y_c | f_c, D) = p(y_c | f_c) = \\mathcal{N}(y_c | f_c, \\sigma_n^2)$. The variance is $\\mathrm{Var}(y_c | f_c, D) = \\sigma_n^2$. Thus, its entropy is:\n$$\nh(y_c | f_c, D) = \\frac{1}{2}\\ln(2\\pi e \\sigma_n^2)\n$$\nNext, we consider the distribution of $y_c$ conditioned only on the existing data $D$. This is the posterior predictive distribution. The latent function value $f_c$ conditioned on $D$ is Gaussian, $p(f_c | D) = \\mathcal{N}(f_c | \\mu_*(s_c), \\sigma^2_*(s_c))$, where $\\mu_*(s_c)$ and $\\sigma^2_*(s_c)$ are the posterior mean and variance from the Gaussian Process regression. The observation $y_c$ is the sum of two independent Gaussian variables, $f_c|D$ and $\\epsilon_c$. The variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(y_c|D) = \\mathrm{Var}(f_c|D) + \\mathrm{Var}(\\epsilon_c) = \\sigma^2_*(s_c) + \\sigma_n^2\n$$\nThe entropy of the posterior predictive distribution for $y_c$ is:\n$$\nh(y_c|D) = \\frac{1}{2}\\ln\\left(2\\pi e (\\sigma^2_*(s_c) + \\sigma_n^2)\\right)\n$$\nCombining these results, the information gain is:\n$$\nIG = \\frac{1}{2}\\ln\\left(2\\pi e (\\sigma^2_*(s_c) + \\sigma_n^2)\\right) - \\frac{1}{2}\\ln(2\\pi e \\sigma_n^2) = \\frac{1}{2}\\ln\\left(\\frac{\\sigma^2_*(s_c) + \\sigma_n^2}{\\sigma_n^2}\\right) = \\frac{1}{2}\\ln\\left(1 + \\frac{\\sigma^2_*(s_c)}{\\sigma_n^2}\\right)\n$$\nThis result shows that the information gain depends only on the ratio of the posterior variance of the latent function to the observation noise variance. To compute $IG$, we must first compute the posterior variance $\\sigma^2_*(s_c)$.\n\nThe posterior variance of a GP at a test point $s_c$ given training points $\\mathbf{s} = [s_1, s_2]^T$ is given by the formula:\n$$\n\\sigma^2_*(s_c) = k(s_c, s_c) - \\mathbf{k}_c^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_c\n$$\nwhere:\n- The covariance function is $k(s, s') = \\sigma_{f}^{2}\\exp\\left(-\\frac{(s-s')^{2}}{2\\ell^{2}}\\right)$.\n- The hyperparameters are $\\sigma_f^2 = 1$, $\\ell = 1$, and $\\sigma_n^2 = 0.01$.\n- The training points are $s_1=0$, $s_2=1$. The candidate point is $s_c=0.5$.\n- $\\mathbf{k}_c$ is the vector of covariances between the test point and the training points: $\\mathbf{k}_c = [k(s_c, s_1), k(s_c, s_2)]^T$.\n- $\\mathbf{K}$ is the covariance matrix of the training points: $\\mathbf{K}_{ij} = k(s_i, s_j)$.\n- $\\mathbf{I}$ is the identity matrix.\n\nWith the given values, the kernel simplifies to $k(s, s') = \\exp\\left(-\\frac{(s-s')^2}{2}\\right)$. We compute the necessary kernel values:\n$k(s_c, s_c) = k(0.5, 0.5) = \\exp(0) = 1$.\n$k(s_c, s_1) = k(0.5, 0) = \\exp\\left(-\\frac{0.5^2}{2}\\right) = \\exp(-0.125)$.\n$k(s_c, s_2) = k(0.5, 1) = \\exp\\left(-\\frac{(-0.5)^2}{2}\\right) = \\exp(-0.125)$.\n$k(s_1, s_1) = k(0, 0) = 1$.\n$k(s_2, s_2) = k(1, 1) = 1$.\n$k(s_1, s_2) = k(0, 1) = \\exp\\left(-\\frac{(-1)^2}{2}\\right) = \\exp(-0.5)$.\n\nThe vector $\\mathbf{k}_c$ and matrix $\\mathbf{K}$ are:\n$$\n\\mathbf{k}_c = \\begin{pmatrix} \\exp(-0.125) \\\\ \\exp(-0.125) \\end{pmatrix} \\quad , \\quad\n\\mathbf{K} = \\begin{pmatrix} 1 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1 \\end{pmatrix}\n$$\nNow we construct the matrix to be inverted, $\\mathbf{M} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$:\n$$\n\\mathbf{M} = \\begin{pmatrix} 1 + 0.01 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1 + 0.01 \\end{pmatrix} = \\begin{pmatrix} 1.01 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1.01 \\end{pmatrix}\n$$\nDue to the symmetry of the problem ($s_c$ is the midpoint of $s_1$ and $s_2$), we can simplify the quadratic form $\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c$. Let $a = 1.01$, $b = \\exp(-0.5)$, and $k_{val} = \\exp(-0.125)$.\n$$\n\\mathbf{M}^{-1} = \\frac{1}{a^2 - b^2}\\begin{pmatrix} a & -b \\\\ -b & a \\end{pmatrix}\n$$\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c = \\begin{pmatrix} k_{val} & k_{val} \\end{pmatrix} \\frac{1}{a^2-b^2} \\begin{pmatrix} a & -b \\\\ -b & a \\end{pmatrix} \\begin{pmatrix} k_{val} \\\\ k_{val} \\end{pmatrix} = \\frac{k_{val}^2}{a^2-b^2} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} a-b \\\\ a-b \\end{pmatrix} = \\frac{2k_{val}^2(a-b)}{a^2-b^2}\n$$\nUsing $a^2-b^2 = (a-b)(a+b)$, this simplifies to:\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c = \\frac{2k_{val}^2}{a+b} = \\frac{2(\\exp(-0.125))^2}{1.01 + \\exp(-0.5)} = \\frac{2\\exp(-0.25)}{1.01 + \\exp(-0.5)}\n$$\nNow, we substitute numerical values:\n$\\exp(-0.25) \\approx 0.77880078$\n$\\exp(-0.5) \\approx 0.60653066$\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c \\approx \\frac{2 \\times 0.77880078}{1.01 + 0.60653066} = \\frac{1.55760156}{1.61653066} \\approx 0.96354636\n$$\nThe posterior variance is then:\n$$\n\\sigma^2_*(s_c) = k(s_c, s_c) - \\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c \\approx 1 - 0.96354636 = 0.03645364\n$$\nFinally, we compute the information gain, which is in units of nats as the natural logarithm was used:\n$$\nIG = \\frac{1}{2}\\ln\\left(1 + \\frac{\\sigma^2_*(s_c)}{\\sigma_n^2}\\right) \\approx \\frac{1}{2}\\ln\\left(1 + \\frac{0.03645364}{0.01}\\right) = \\frac{1}{2}\\ln(1 + 3.645364) = \\frac{1}{2}\\ln(4.645364)\n$$\n$$\nIG \\approx \\frac{1}{2} \\times 1.535876 \\approx 0.767938\n$$\nRounding to four significant figures, the expected information gain is $0.7679$.", "answer": "$$\\boxed{0.7679}$$", "id": "2760116"}, {"introduction": "A machine-learned potential energy surface (PES) is more than just a statistical regression model; it is a surrogate for a physical reality. A fundamental property of any valid PES is that the force field it implies, $\\mathbf{F}(\\mathbf{R}) = -\\nabla E(\\mathbf{R})$, must be conservative. This physical constraint means that the work done by the force field around any closed loop in configuration space must be zero, a property directly related to the curl of the force field being zero. This practical exercise [@problem_id:2760109] challenges you to implement a numerical diagnostic to test for this crucial physical property, allowing you to detect non-conservative artifacts that simple accuracy metrics might miss.", "problem": "You are given the task of designing a numerical diagnostic to detect non-conservative artifacts in a learned potential energy surface (PES) through the force field it induces. In the ideal case, if the force field $\\mathbf{F}(\\mathbf{R})$ is conservative so that $\\mathbf{F}(\\mathbf{R})=-\\nabla E(\\mathbf{R})$ for some scalar potential $E(\\mathbf{R})$, then for any sufficiently smooth closed loop $C$ in configuration space, the circulation satisfies $\\oint_C \\mathbf{F}(\\mathbf{R})\\cdot d\\mathbf{R} = 0$. By Stokes' theorem, this is equivalent to the vanishing of the surface integral $\\iint_{S} (\\nabla \\times \\mathbf{F})\\cdot \\mathbf{\\hat{n}}\\, dA$ over any surface $S$ bounded by $C$, where $\\mathbf{\\hat{n}}$ is the unit normal. In practice, learning errors or numerical inconsistencies can introduce non-conservative components into $\\mathbf{F}(\\mathbf{R})$.\n\nStarting from these fundamental properties, design and implement a numerical test that, given a planar closed loop $C$ and a force predictor $\\mathbf{F}(\\mathbf{R})$, estimates the circulation $\\oint_C \\mathbf{F}\\cdot d\\mathbf{R}$ via a composite midpoint quadrature along a polyline approximation of $C$, and then forms a scalar diagnostic by dividing the estimated circulation by the enclosed area $A(C)$ to approximate the mean normal component of the curl over the loop. Specifically, for a polyline with vertices $\\{\\mathbf{R}_k\\}_{k=0}^{N-1}$ (with $\\mathbf{R}_N\\equiv \\mathbf{R}_0$), use\n$$\n\\widehat{\\Gamma}_N(C) \\equiv \\sum_{k=0}^{N-1} \\mathbf{F}\\!\\left(\\frac{\\mathbf{R}_{k}+\\mathbf{R}_{k+1}}{2}\\right)\\cdot\\left(\\mathbf{R}_{k+1}-\\mathbf{R}_{k}\\right),\n$$\nand define the diagnostic\n$$\n\\omega(C) \\equiv \\frac{\\widehat{\\Gamma}_N(C)}{A(C)}.\n$$\nA nonzero value of $|\\omega(C)|$ beyond a user-specified tolerance indicates a non-conservative artifact. Angles, when used to parametrize curves, must be in radians. All quantities in this problem are dimensionless.\n\nImplement a program that constructs specified loops $C$, evaluates $\\mathbf{F}(\\mathbf{R})$ on required points, computes $\\widehat{\\Gamma}_N(C)$ using the composite midpoint rule, divides by the exact area $A(C)$ for the given loop shape, and returns a boolean classification per test case: return $\\mathrm{True}$ if $|\\omega(C)|$ exceeds the provided tolerance $\\tau$, and $\\mathrm{False}$ otherwise. You must use a fixed pseudorandom seed of $0$ for any stochastic components to ensure deterministic outputs.\n\nYour program must handle the following two loop families in two dimensions:\n- Circles: center at the origin, radius $r$, parametrized by $\\theta\\in[0,2\\pi)$ with $\\mathbf{R}(\\theta)=\\big(r\\cos\\theta,\\, r\\sin\\theta\\big)$, discretized into $N$ equal angular steps. Use exact area $A(C)=\\pi r^2$.\n- Axis-aligned rectangles: centered at the origin, width $w$, height $h$, traversed counterclockwise with $N_{\\text{side}}$ equal segments per side. Use exact area $A(C)=w\\,h$.\n\nYour program must implement the following force fields $\\mathbf{F}(\\mathbf{R})$ in two dimensions:\n- Harmonic conservative: derived from $E(x,y)=\\tfrac{1}{2}\\big(k_x x^2 + k_y y^2\\big)$ so that $\\mathbf{F}(x,y)=\\big(-k_x x,\\,-k_y y\\big)$.\n- Harmonic plus swirl: $\\mathbf{F}(x,y)=\\big(-k_x x,\\,-k_y y\\big) + \\gamma\\,\\big(-y,\\,x\\big)$.\n- Harmonic plus isotropic Gaussian noise: $\\mathbf{F}(x,y)=\\big(-k_x x,\\,-k_y y\\big) + \\boldsymbol{\\xi}$, where each component of $\\boldsymbol{\\xi}$ is an independent sample from a normal distribution with mean $0$ and standard deviation $\\sigma$, resampled independently at every evaluation point; use the fixed pseudorandom seed $0$ for reproducibility.\n- Pure swirl: $\\mathbf{F}(x,y)=\\gamma\\,\\big(-y,\\,x\\big)$.\n- Zero field: $\\mathbf{F}(x,y)=\\big(0,\\,0\\big)$.\n\nTest suite. Your program must compute and report the boolean classification for each of the following cases:\n- Case $1$ (conservative, circle): harmonic with $k_x=3.0$, $k_y=5.0$; circle with $r=1.0$, $N=1024$; tolerance $\\tau=1.0\\times 10^{-8}$.\n- Case $2$ (non-conservative, circle): harmonic plus swirl with $k_x=3.0$, $k_y=5.0$, $\\gamma=0.05$; circle with $r=1.0$, $N=1024$; tolerance $\\tau=1.0\\times 10^{-3}$.\n- Case $3$ (conservative with noise, circle): harmonic plus noise with $k_x=3.0$, $k_y=5.0$, $\\sigma=1.0\\times 10^{-3}$; circle with $r=1.0$, $N=2048$; tolerance $\\tau=1.0\\times 10^{-2}$.\n- Case $4$ (zero field, rectangle): zero field; rectangle with $w=2.0$, $h=1.0$, $N_{\\text{side}}=200$; tolerance $\\tau=1.0\\times 10^{-12}$.\n- Case $5$ (non-conservative, rectangle): pure swirl with $\\gamma=0.02$; rectangle with $w=1.0$, $h=1.0$, $N_{\\text{side}}=16$; tolerance $\\tau=5.0\\times 10^{-3}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False,True]\"), where the $i$-th entry is the boolean result for Case $i$ in the order listed above.", "solution": "The problem requires the design and implementation of a numerical diagnostic to identify non-conservative artifacts in a given two-dimensional force field, $\\mathbf{F}(\\mathbf{R})$. The diagnostic is based on the fundamental principles of vector calculus applied to conservative vector fields.\n\nA force field $\\mathbf{F}$ is defined as conservative if it can be expressed as the negative gradient of a scalar potential energy function $E$, i.e., $\\mathbf{F}(\\mathbf{R}) = -\\nabla E(\\mathbf{R})$. A key property of any conservative field is that the work done along any closed path, known as the circulation $\\Gamma(C)$, is identically zero. This is expressed by the line integral $\\Gamma(C) = \\oint_C \\mathbf{F}(\\mathbf{R})\\cdot d\\mathbf{R} = 0$. By Stokes' theorem, this is equivalent to the condition that the curl of the force field vanishes everywhere, $\\nabla \\times \\mathbf{F} = \\mathbf{0}$. A learned potential energy surface may produce a force field that does not strictly satisfy this condition, and our task is to detect such deviations.\n\nThe numerical procedure to estimate the circulation, $\\widehat{\\Gamma}_N(C)$, approximates the closed loop $C$ with a polyline defined by a sequence of $N$ vertices $\\{\\mathbf{R}_k\\}_{k=0}^{N}$ where $\\mathbf{R}_N \\equiv \\mathbf{R}_0$. The line integral is then approximated using a composite midpoint quadrature rule. For each line segment from $\\mathbf{R}_k$ to $\\mathbf{R}_{k+1}$, the force is evaluated at the midpoint of the segment, $\\frac{\\mathbf{R}_{k}+\\mathbf{R}_{k+1}}{2}$. The contribution to the circulation from this segment is the dot product of this force with the segment's displacement vector, $\\mathbf{R}_{k+1}-\\mathbf{R}_{k}$. Summing these contributions over all segments gives the total estimated circulation:\n$$\n\\widehat{\\Gamma}_N(C) \\equiv \\sum_{k=0}^{N-1} \\mathbf{F}\\!\\left(\\frac{\\mathbf{R}_{k}+\\mathbf{R}_{k+1}}{2}\\right)\\cdot\\left(\\mathbf{R}_{k+1}-\\mathbf{R}_{k}\\right)\n$$\nTo create a scale-invariant diagnostic, we relate the circulation to the curl via Stokes' theorem in two dimensions: $\\oint_C \\mathbf{F}\\cdot d\\mathbf{R} = \\iint_S (\\nabla \\times \\mathbf{F})_z \\,dA$. By the mean value theorem for integrals, there exists a point in the surface $S$ enclosed by $C$ where the value of $(\\nabla \\times \\mathbf{F})_z$ is equal to the average value over the surface, $\\frac{1}{A(C)}\\iint_S (\\nabla \\times \\mathbf{F})_z \\,dA$, where $A(C)$ is the area of $S$. We thus define the diagnostic scalar $\\omega(C)$ as the estimated circulation normalized by the area of the loop:\n$$\n\\omega(C) \\equiv \\frac{\\widehat{\\Gamma}_N(C)}{A(C)}\n$$\nThis quantity, $\\omega(C)$, serves as a numerical approximation to the average normal component of the curl over the area enclosed by the loop. A non-zero value of $|\\omega(C)|$ greater than a specified tolerance $\\tau$ signals a potential non-conservative artifact. The final output is a boolean value indicating whether this condition, $|\\omega(C)| > \\tau$, is met.\n\nThe implementation is structured in a modular fashion to ensure clarity and correctness.\nFirst, functions are designed to generate the vertices of the specified closed loops: circles and axis-aligned rectangles. A circle of radius $r$ is discretized into $N$ equal angular steps, resulting in $N+1$ vertices. A rectangle of width $w$ and height $h$ is discretized with $N_{\\text{side}}$ segments per side. The total number of segments is $4 \\times N_{\\text{side}}$.\nSecond, a set of functions implements the specified force fields. Each function is designed to be vectorized, accepting an array of $M$ position vectors of shape $(M, 2)$ and returning a corresponding array of $M$ force vectors. This approach is critical for computational efficiency.\nFor the case involving stochasticity (`harmonic_noise`), a random number generator is initialized with a fixed seed of $0$ to ensure deterministic and reproducible results.\nThe core of the implementation is a function that calculates the diagnostic. This function takes the polyline vertices, the loop area, the force function, and the tolerance as inputs. It computes the midpoints and displacement vectors for all segments of the polyline. The force function is then called once with the array of all midpoints. The circulation, $\\widehat{\\Gamma}_N(C)$, is calculated by summing the dot products of the force and displacement vectors for each segment. This sum is efficiently computed in NumPy as `np.sum(forces * displacements)`. Finally, the diagnostic $\\omega(C)$ is calculated and its absolute value is compared against the tolerance $\\tau$ to return either `True` or `False`.\nThe main program iterates through a predefined test suite. For each case, it configures the appropriate loop, force field, and parameters, invokes the diagnostic calculation, and appends the boolean result to a list. The final output is a single string representing this list of boolean values, formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# The problem specifies a fixed pseudorandom seed of 0 for reproducibility.\nRNG = np.random.default_rng(seed=0)\n\ndef generate_circle_polyline(r, N):\n    \"\"\"\n    Generates vertices for a circle centered at the origin, discretized into N segments.\n    The polyline contains N+1 points, with the last point being the same as the first.\n    \n    Args:\n        r (float): The radius of the circle.\n        N (int): The number of segments.\n        \n    Returns:\n        np.ndarray: An (N+1, 2) array of vertex coordinates.\n    \"\"\"\n    thetas = np.linspace(0.0, 2.0 * np.pi, N + 1)\n    x = r * np.cos(thetas)\n    y = r * np.sin(thetas)\n    return np.stack([x, y], axis=1)\n\ndef generate_rectangle_polyline(w, h, N_side):\n    \"\"\"\n    Generates vertices for an axis-aligned rectangle centered at the origin.\n    The path is counterclockwise and has N_side segments per side.\n    \n    Args:\n        w (float): The width of the rectangle.\n        h (float): The height of the rectangle.\n        N_side (int): The number of segments per side.\n        \n    Returns:\n        np.ndarray: An (4*N_side + 1, 2) array of vertex coordinates.\n    \"\"\"\n    half_w, half_h = w / 2.0, h / 2.0\n    p1 = np.array([-half_w, -half_h])  # Bottom-left\n    p2 = np.array([half_w, -half_h])   # Bottom-right\n    p3 = np.array([half_w, half_h])    # Top-right\n    p4 = np.array([-half_w, half_h])   # Top-left\n\n    bottom_edge = np.array([np.linspace(p1[0], p2[0], N_side + 1), np.full(N_side + 1, p1[1])]).T\n    right_edge = np.array([np.full(N_side + 1, p2[0]), np.linspace(p2[1], p3[1], N_side + 1)]).T\n    top_edge = np.array([np.linspace(p3[0], p4[0], N_side + 1), np.full(N_side + 1, p3[1])]).T\n    left_edge = np.array([np.full(N_side + 1, p4[0]), np.linspace(p4[1], p1[1], N_side + 1)]).T\n    \n    # Concatenate edges, removing duplicate corners to form a closed loop.\n    return np.concatenate((bottom_edge[:-1], right_edge[:-1], top_edge[:-1], left_edge))\n\n# --- Vectorized Force Field Functions ---\n# Each function accepts an (M, 2) array of positions and returns an (M, 2) array of forces.\n\ndef force_harmonic_conservative(R, kx, ky):\n    return np.array([-kx * R[:, 0], -ky * R[:, 1]]).T\n\ndef force_harmonic_swirl(R, kx, ky, gamma):\n    x, y = R[:, 0], R[:, 1]\n    fx = -kx * x - gamma * y\n    fy = -ky * y + gamma * x\n    return np.stack([fx, fy], axis=1)\n\ndef force_harmonic_noise(R, kx, ky, sigma):\n    M = R.shape[0]\n    base_force = np.array([-kx * R[:, 0], -ky * R[:, 1]]).T\n    noise = RNG.normal(0.0, sigma, size=(M, 2))\n    return base_force + noise\n\ndef force_pure_swirl(R, gamma):\n    x, y = R[:, 0], R[:, 1]\n    return np.stack([-gamma * y, gamma * x], axis=1)\n\ndef force_zero(R):\n    return np.zeros_like(R)\n\n# --- Main Diagnostic Calculation ---\ndef calculate_diagnostic(polyline, area, force_func, tolerance):\n    \"\"\"\n    Calculates the diagnostic omega and compares its magnitude to a tolerance.\n    \"\"\"\n    # Midpoints of the polyline segments\n    midpoints = (polyline[:-1] + polyline[1:]) / 2.0\n    # Displacement vectors for each segment\n    displacements = polyline[1:] - polyline[:-1]\n    \n    # Evaluate forces at all midpoints in a vectorized manner\n    forces = force_func(midpoints)\n    \n    # Calculate circulation using the composite midpoint rule.\n    # The sum of element-wise products is equivalent to the sum of dot products.\n    circulation = np.sum(forces * displacements)\n    \n    # Calculate the diagnostic omega, avoiding division by zero.\n    if area == 0.0:\n        omega = 0.0 if circulation == 0.0 else np.inf\n    else:\n        omega = circulation / area\n        \n    return abs(omega) > tolerance\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Conservative harmonic field on a circle. Expect near-zero circulation.\n        {'loop_type': 'circle', 'loop_params': {'r': 1.0, 'N': 1024},\n         'force_type': 'harmonic', 'force_params': {'kx': 3.0, 'ky': 5.0},\n         'tolerance': 1.0e-8},\n        # Case 2: Non-conservative swirl field on a circle. Expect non-zero circulation.\n        {'loop_type': 'circle', 'loop_params': {'r': 1.0, 'N': 1024},\n         'force_type': 'harmonic_swirl', 'force_params': {'kx': 3.0, 'ky': 5.0, 'gamma': 0.05},\n         'tolerance': 1.0e-3},\n        # Case 3: Conservative field with noise. Circulation expected to be small.\n        {'loop_type': 'circle', 'loop_params': {'r': 1.0, 'N': 2048},\n         'force_type': 'harmonic_noise', 'force_params': {'kx': 3.0, 'ky': 5.0, 'sigma': 1.0e-3},\n         'tolerance': 1.0e-2},\n        # Case 4: Zero field on a rectangle. Circulation must be zero.\n        {'loop_type': 'rectangle', 'loop_params': {'w': 2.0, 'h': 1.0, 'N_side': 200},\n         'force_type': 'zero', 'force_params': {},\n         'tolerance': 1.0e-12},\n        # Case 5: Pure swirl field on a rectangle. Expect non-zero circulation.\n        {'loop_type': 'rectangle', 'loop_params': {'w': 1.0, 'h': 1.0, 'N_side': 16},\n         'force_type': 'pure_swirl', 'force_params': {'gamma': 0.02},\n         'tolerance': 5.0e-3},\n    ]\n\n    results = []\n    \n    force_map = {\n        'harmonic': force_harmonic_conservative,\n        'harmonic_swirl': force_harmonic_swirl,\n        'harmonic_noise': force_harmonic_noise,\n        'pure_swirl': force_pure_swirl,\n        'zero': force_zero\n    }\n\n    for case in test_cases:\n        # Generate polyline and determine loop area\n        if case['loop_type'] == 'circle':\n            r = case['loop_params']['r']\n            N = case['loop_params']['N']\n            polyline = generate_circle_polyline(r, N)\n            area = np.pi * r**2\n        elif case['loop_type'] == 'rectangle':\n            w = case['loop_params']['w']\n            h = case['loop_params']['h']\n            N_side = case['loop_params']['N_side']\n            polyline = generate_rectangle_polyline(w, h, N_side)\n            area = w * h\n        \n        # Create the specific force function with its parameters\n        force_func_base = force_map[case['force_type']]\n        force_params = case['force_params']\n        force_func = lambda R: force_func_base(R, **force_params)\n\n        # Run the diagnostic test and store the boolean result\n        result = calculate_diagnostic(polyline, area, force_func, case['tolerance'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2760109"}, {"introduction": "Uncertainty-driven active learning relies on a critical assumption: that the model's own estimates of its uncertainty are reliable. A model whose confidence does not match its competence can be misleading and inefficient. This is where the concept of calibration comes into play. A model is well-calibrated if its reported predictive variance accurately reflects its empirical mean squared error. In this practical coding exercise [@problem_id:2760108], you will implement reliability diagrams, a standard and powerful tool for diagnosing miscalibration. Mastering this technique will provide you with the means to critically evaluate and trust the uncertainty estimates that guide your active learning decisions.", "problem": "You will implement and run a complete program that constructs reliability diagrams for uncertainty calibration on forces by binning predictive variances and comparing empirical squared errors to expected values.\n\nWe consider a collection of scalar force components, each with a ground-truth value, a model prediction, and a model-reported predictive variance. Each scalar component is measured in electronvolt per angstrom, denoted $\\,\\mathrm{eV}\\,\\mathrm{\\AA}^{-1}\\,$. Predictive variances are measured in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.\n\nStarting from the following fundamental base:\n- For a heteroscedastic Gaussian predictive model of a scalar target $y$, the model returns a predictive mean $\\hat{y}$ and a predictive variance $v$. If the predictive uncertainties are calibrated and the residuals are conditionally Gaussian with zero mean, then $\\mathbb{E}\\left[(y-\\hat{y})^2 \\mid v\\right] = v$.\n- The reliability diagram for variance calibration compares, in bins of predicted variance, the empirical mean squared error to the mean predicted variance.\n\nYou must implement the following definitions precisely. Suppose we have $M$ scalar observations indexed by $j=1,\\dots,M$, with true values $y_j$ in $\\mathrm{eV}\\,\\mathrm{\\AA}^{-1}$, predictions $\\hat{y}_j$ in $\\mathrm{eV}\\,\\mathrm{\\AA}^{-1}$, and predicted variances $v_j$ in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$. Let the bin edge vector be $\\mathbf{e} = (e_0, e_1, \\dots, e_B)$ with $e_0 < e_1 < \\cdots < e_B$. Define $B$ bins by the half-open intervals\n- $[e_b, e_{b+1})$ for $b=0,1,\\dots,B-2$, and\n- $[e_{B-1}, e_B]$ for the last bin,\nso the last bin is right-closed to include the right endpoint. Assign each $v_j$ to exactly one bin using this rule; if a $v_j$ is outside $[e_0, e_B]$, it must be discarded (but no such values will occur in this problem).\n\nFor bin $b$, let $I_b$ be the index set of components assigned to that bin and $n_b = |I_b|$ the count. Define the bin-averaged predicted variance and empirical mean squared error as\n$$\n\\bar{v}_b = \\frac{1}{n_b} \\sum_{j \\in I_b} v_j, \\qquad\n\\mathrm{MSE}_b = \\frac{1}{n_b} \\sum_{j \\in I_b} (y_j - \\hat{y}_j)^2,\n$$\nwith the convention that empty bins ($n_b = 0$) are ignored in any summations and are reported as empty. Define the per-bin calibration gap\n$$\ng_b = \\mathrm{MSE}_b - \\bar{v}_b,\n$$\nwhich is measured in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$. Define two scalar calibration metrics:\n- The weighted Expected Calibration Error (wECE):\n$$\n\\mathrm{wECE} = \\sum_{b=0}^{B-1} \\frac{n_b}{M} \\left| g_b \\right|.\n$$\n- The Root Mean Squared Calibration Error (RMSCE):\n$$\n\\mathrm{RMSCE} = \\sqrt{ \\sum_{b=0}^{B-1} \\frac{n_b}{M} \\, g_b^2 }.\n$$\nBoth $\\mathrm{wECE}$ and $\\mathrm{RMSCE}$ have units of $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$. Empty bins contribute zero to the sums because $n_b = 0$.\n\nYour program must compute these metrics for each of the test cases described below, all of which are deterministic. The generation of residuals is defined using a standard normal quantile sequence to avoid pseudorandomness:\n\n- For a given $N$, define quantiles $u_j = \\frac{j - 0.5}{N}$ for $j = 1, \\dots, N$.\n- Define standard normal scores deterministically by\n$$\ns_j = \\sqrt{2}\\,\\operatorname{erf}^{-1}\\!\\left( 2 u_j - 1 \\right),\n$$\nwhere $\\operatorname{erf}^{-1}$ is the inverse error function. Then apply a deterministic permutation to decorrelate $s_j$ from any simple patterns in variances:\n$$\n\\pi(j) = \\left( a \\cdot (j-1) + c \\right) \\bmod N + 1,\n$$\nwith $a$ and $N$ coprime and $c$ a fixed integer. Use $(a, c) = (7919, 0)$ in all cases below. The permuted normal scores are $s_{\\pi(j)}$. For a specified component-wise true variance $w_j \\ge 0$, set\n$$\n\\hat{y}_j = y_j + \\sqrt{w_j}\\; s_{\\pi(j)}.\n$$\n\nUse the following three test cases. In all cases, the ground-truth force components are $y_j = 0$ in $\\mathrm{eV}\\,\\mathrm{\\AA}^{-1}$.\n\n- Test Case 1 (near-perfect calibration, continuous coverage):\n    - Number of components: $N_1 = 6000$.\n    - Predicted and true variances identical: for $j=1,\\dots,N_1$,\n      $v_j = w_j = v_{\\min} + (v_{\\max} - v_{\\min}) \\cdot \\frac{j - 0.5}{N_1}$\n      with $v_{\\min} = 10^{-4}$ and $v_{\\max} = 0.4$, all in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.\n    - Predicted means: $\\hat{y}_j = \\sqrt{w_j}\\; s_{\\pi(j)}$.\n    - Bin edges: $e_b$ linearly spaced from $0.0$ to $0.5$ with $B_1 = 10$ bins, i.e., $e_b = 0.05\\, b$ for $b = 0,\\dots,10$.\n\n- Test Case 2 (systematically underconfident predictive variances):\n    - Number of components: $N_2 = 6000$.\n    - True variances linearly spaced: $w_j = w_{\\min} + (w_{\\max} - w_{\\min}) \\cdot \\frac{j - 0.5}{N_2}$ with $w_{\\min} = 0.01$ and $w_{\\max} = 0.3$ in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.\n    - Predicted variances: $v_j = \\gamma \\, w_j$ with $\\gamma = 2.0$.\n    - Predicted means: $\\hat{y}_j = \\sqrt{w_j}\\; s_{\\pi(j)}$.\n    - Bin edges: $e_b$ linearly spaced from $0.0$ to $0.6$ with $B_2 = 6$ bins, i.e., $e_b = 0.1\\, b$ for $b = 0,\\dots,6$.\n\n- Test Case 3 (zeros, extreme ranges, and intentionally empty bins):\n    - Number of components: $N_3 = 300$.\n    - Predicted variances $v_j$ take on only the following values (each repeated exactly $50$ times, in the listed order): $0.0$, $10^{-6}$, $5 \\times 10^{-3}$, $5 \\times 10^{-2}$, $2 \\times 10^{-1}$, $1.0$, all in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.\n    - True variances: $w_j = 0.5\\, v_j$.\n    - Predicted means: $\\hat{y}_j = \\sqrt{w_j}\\; s_{\\pi(j)}$.\n    - Bin edges: $e = \\left[ 0.0, 10^{-9}, 10^{-7}, 10^{-6}, 5\\times 10^{-6}, 5\\times 10^{-5}, 5\\times 10^{-4}, 5\\times 10^{-3}, 5\\times 10^{-2}, 5\\times 10^{-1}, 1.0 \\right]$, so $B_3 = 10$ bins.\n\nFor each test case, your program must:\n1. Construct the bins using the left-inclusive, right-exclusive convention $[e_b, e_{b+1})$ for $b=0,\\dots,B-2$ and the last bin $[e_{B-1}, e_{B}]$ right-closed.\n2. Compute $\\bar{v}_b$, $\\mathrm{MSE}_b$, and $g_b$ for each non-empty bin.\n3. Compute $\\mathrm{wECE}$ and $\\mathrm{RMSCE}$ as defined above.\n4. Count the number of empty bins (those with $n_b = 0$).\n\nAnswer specification:\n- Units: All force components $y_j$ and $\\hat{y}_j$ are in $\\mathrm{eV}\\,\\mathrm{\\AA}^{-1}$. All variances, including $v_j$, $\\bar{v}_b$, $\\mathrm{MSE}_b$, $\\mathrm{wECE}$, and $\\mathrm{RMSCE}$, are in $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.\n- For each test case, report three values: $\\mathrm{wECE}$, $\\mathrm{RMSCE}$, and the number of empty bins.\n- Rounding: Round each $\\mathrm{wECE}$ and $\\mathrm{RMSCE}$ to six decimal places. The number of empty bins is an integer and must be reported exactly.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$[\\mathrm{wECE}_1, \\mathrm{RMSCE}_1, \\text{empty}_1, \\mathrm{wECE}_2, \\mathrm{RMSCE}_2, \\text{empty}_2, \\mathrm{wECE}_3, \\mathrm{RMSCE}_3, \\text{empty}_3]$,\nwhere the subscript denotes the test case index. The two calibration error values must be rounded to six decimal places as specified. No other output is permitted.", "solution": "The problem requires the implementation of a procedure for evaluating the calibration of uncertainty predictions for atomic forces, a common task in the development of machine learning potentials in theoretical chemistry and materials science. The evaluation is based on reliability diagrams, which compare the model's reported predictive variance to the observed empirical error. We will compute two standard metrics, the weighted Expected Calibration Error ($\\mathrm{wECE}$) and the Root Mean Squared Calibration Error ($\\mathrm{RMSCE}$), for three distinct, deterministically generated test cases. The entire procedure will be implemented in a self-contained Python program.\n\nThe fundamental principle being tested is the calibration of a heteroscedastic model. For a perfectly calibrated model which outputs a predictive mean $\\hat{y}$ and variance $v$ for a target quantity $y$, the following condition must hold:\n$$\n\\mathbb{E}\\left[(y-\\hat{y})^2 \\mid v\\right] = v\n$$\nThis states that the expected squared error, conditioned on a predicted variance $v$, should be equal to $v$. In practice, we verify this by binning the data according to the predicted variance $v$ and comparing the average squared error within each bin to the average predicted variance.\n\nThe problem specifies a precise, deterministic method for generating the synthetic data to avoid any stochasticity. The ground-truth force components $y_j$ are all set to $0 \\, \\mathrm{eV}\\,\\mathrm{\\AA}^{-1}$. The predicted force components $\\hat{y}_j$ are generated by adding noise to the true values. The noise is sampled from a normal distribution with a true variance $w_j$. The samples themselves are not random but are deterministically generated from standard normal scores $s_j$ derived from the inverse error function:\n$$\ns_j = \\sqrt{2}\\,\\operatorname{erf}^{-1}\\!\\left( 2 u_j - 1 \\right) \\quad \\text{where} \\quad u_j = \\frac{j - 0.5}{N} \\quad \\text{for} \\quad j = 1, \\dots, N\n$$\nThis generates a set of $N$ values that behave like samples from a standard normal distribution, $\\mathcal{N}(0, 1)$. To prevent spurious correlations arising from the ordered generation of $s_j$ and $w_j$, the scores are permuted using a linear congruential generator: $\\pi(j) = (a \\cdot (j-1) + c) \\bmod N + 1$. The predicted mean is then set as:\n$$\n\\hat{y}_j = y_j + \\sqrt{w_j}\\; s_{\\pi(j)} = \\sqrt{w_j}\\; s_{\\pi(j)}\n$$\nThe squared error for component $j$ is thus $(y_j - \\hat{y}_j)^2 = w_j s_{\\pi(j)}^2$.\n\nThe core of the analysis involves partitioning the $M$ data points into $B$ bins based on their predicted variance $v_j$. The bin edges are given by the vector $\\mathbf{e} = (e_0, \\dots, e_B)$. A specific binning rule is mandated: all bins are left-inclusive and right-exclusive, $[e_b, e_{b+1})$, except for the last bin, $[e_{B-1}, e_B]$, which is closed on both ends. This ensures all data points with predicted variances $v_j \\in [e_0, e_B]$ are assigned to a unique bin.\n\nFor each bin $b$, we compute the count of points $n_b$, the average predicted variance $\\bar{v}_b$, and the mean squared error $\\mathrm{MSE}_b$:\n$$\n\\bar{v}_b = \\frac{1}{n_b} \\sum_{j \\in I_b} v_j, \\qquad\n\\mathrm{MSE}_b = \\frac{1}{n_b} \\sum_{j \\in I_b} (y_j - \\hat{y}_j)^2\n$$\nwhere $I_b$ is the set of indices of data points falling into bin $b$. The per-bin calibration gap is $g_b = \\mathrm{MSE}_b - \\bar{v}_b$. For a perfectly calibrated model, we expect $g_b \\approx 0$ for all bins.\n\nFinally, we aggregate these per-bin gaps into two scalar metrics that quantify the overall miscalibration. The weighted Expected Calibration Error ($\\mathrm{wECE}$) measures the average absolute gap, weighted by the number of points in each bin:\n$$\n\\mathrm{wECE} = \\sum_{b=0}^{B-1} \\frac{n_b}{M} \\left| g_b \\right|\n$$\nThe Root Mean Squared Calibration Error ($\\mathrm{RMSCE}$) is sensitive to large deviations in any bin:\n$$\n\\mathrm{RMSCE} = \\sqrt{ \\sum_{b=0}^{B-1} \\frac{n_b}{M} \\, g_b^2 }\n$$\nEmpty bins, where $n_b=0$, contribute nothing to these sums.\n\nThe implementation will proceed by sequentially processing each of the three test cases.\n\nFor each case:\n$1$. The total number of components, $N$, is defined. The ground truth values $y_j$ are all zero.\n$2$. The deterministic standard normal scores $s_j$ are generated and the specified permutation is applied.\n$3$. The arrays for true variances $w_j$ and predicted variances $v_j$ are constructed according to the rules of the test case.\n$4$. The predicted means $\\hat{y}_j$ and the squared errors $(\\hat{y}_j)^2$ are calculated.\n$5$. Each data point $(v_j, (\\hat{y}_j)^2)$ is assigned to one of the predefined bins. This is done by applying Boolean masks for each bin interval to identify the corresponding indices.\n$6$. For each bin, $n_b$, $\\bar{v}_b$, and $\\mathrm{MSE}_b$ are calculated, taking care to handle empty bins where $n_b=0$.\n$7$. The calibration gaps $g_b$ are computed.\n$8$. The final metrics $\\mathrm{wECE}$ and $\\mathrm{RMSCE}$ are calculated using the full set of per-bin statistics.\n$9$. The number of empty bins is counted.\n\nThese steps are executed for each of the three test cases, which are designed to represent near-perfect calibration, systematic underconfidence, and a sparse distribution of variances with empty bins, respectively. The resulting three values for each case—$\\mathrm{wECE}$, $\\mathrm{RMSCE}$, and the empty bin count—are collected and formatted as specified. All quantities related to variance or squared error, including the final metrics, have units of $(\\mathrm{eV}\\,\\mathrm{\\AA}^{-1})^2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfinv\n\ndef solve():\n    \"\"\"\n    Computes reliability diagram metrics for three test cases of uncertainty calibration.\n    \"\"\"\n    results = []\n\n    # --- Test Case 1: Near-perfect calibration ---\n    N1 = 6000\n    B1 = 10\n    bin_edges1 = np.linspace(0.0, 0.5, B1 + 1)\n    \n    # Generate data for Case 1\n    y1 = np.zeros(N1)\n    j1 = np.arange(1, N1 + 1)\n    u1 = (j1 - 0.5) / N1\n    s1 = np.sqrt(2) * erfinv(2 * u1 - 1)\n    \n    a, c = 7919, 0\n    pi1 = (a * (j1 - 1) + c) % N1 + 1\n    s_permuted1 = s1[pi1 - 1]\n    \n    v_min1, v_max1 = 1e-4, 0.4\n    w1 = v_min1 + (v_max1 - v_min1) * (j1 - 0.5) / N1\n    v1 = w1\n    y_hat1 = np.sqrt(w1) * s_permuted1\n    squared_errors1 = (y_hat1 - y1)**2\n    \n    # Binning and statistics for Case 1\n    bin_indices1 = -np.ones(N1, dtype=int)\n    for b in range(B1 - 1):\n        mask = (v1 >= bin_edges1[b]) & (v1 < bin_edges1[b+1])\n        bin_indices1[mask] = b\n    mask_last_bin = (v1 >= bin_edges1[B1-1]) & (v1 <= bin_edges1[B1])\n    bin_indices1[mask_last_bin] = B1 - 1\n    \n    n_b1 = np.bincount(bin_indices1, minlength=B1)\n    v_sums1 = np.bincount(bin_indices1, weights=v1, minlength=B1)\n    mse_sums1 = np.bincount(bin_indices1, weights=squared_errors1, minlength=B1)\n    \n    v_bar_b1 = np.zeros(B1)\n    mse_b1 = np.zeros(B1)\n    non_empty_mask1 = n_b1 > 0\n    v_bar_b1[non_empty_mask1] = v_sums1[non_empty_mask1] / n_b1[non_empty_mask1]\n    mse_b1[non_empty_mask1] = mse_sums1[non_empty_mask1] / n_b1[non_empty_mask1]\n    \n    g_b1 = mse_b1 - v_bar_b1\n    \n    wECE1 = np.sum((n_b1 / N1) * np.abs(g_b1))\n    RMSCE1 = np.sqrt(np.sum((n_b1 / N1) * (g_b1**2)))\n    empty_bins1 = B1 - np.count_nonzero(n_b1)\n    \n    results.extend([round(wECE1, 6), round(RMSCE1, 6), empty_bins1])\n\n    # --- Test Case 2: Systematically underconfident ---\n    N2 = 6000\n    B2 = 6\n    bin_edges2 = np.linspace(0.0, 0.6, B2 + 1)\n    \n    # Generate data for Case 2\n    y2 = np.zeros(N2)\n    j2 = np.arange(1, N2 + 1)\n    u2 = (j2 - 0.5) / N2\n    s2 = np.sqrt(2) * erfinv(2 * u2 - 1)\n    \n    pi2 = (a * (j2 - 1) + c) % N2 + 1\n    s_permuted2 = s2[pi2 - 1]\n    \n    w_min2, w_max2 = 0.01, 0.3\n    w2 = w_min2 + (w_max2 - w_min2) * (j2 - 0.5) / N2\n    gamma2 = 2.0\n    v2 = gamma2 * w2\n    y_hat2 = np.sqrt(w2) * s_permuted2\n    squared_errors2 = (y_hat2 - y2)**2\n    \n    # Binning and statistics for Case 2\n    bin_indices2 = -np.ones(N2, dtype=int)\n    for b in range(B2 - 1):\n        mask = (v2 >= bin_edges2[b]) & (v2 < bin_edges2[b+1])\n        bin_indices2[mask] = b\n    mask_last_bin2 = (v2 >= bin_edges2[B2-1]) & (v2 <= bin_edges2[B2])\n    bin_indices2[mask_last_bin2] = B2 - 1\n\n    n_b2 = np.bincount(bin_indices2, minlength=B2)\n    v_sums2 = np.bincount(bin_indices2, weights=v2, minlength=B2)\n    mse_sums2 = np.bincount(bin_indices2, weights=squared_errors2, minlength=B2)\n    \n    v_bar_b2 = np.zeros(B2)\n    mse_b2 = np.zeros(B2)\n    non_empty_mask2 = n_b2 > 0\n    v_bar_b2[non_empty_mask2] = v_sums2[non_empty_mask2] / n_b2[non_empty_mask2]\n    mse_b2[non_empty_mask2] = mse_sums2[non_empty_mask2] / n_b2[non_empty_mask2]\n    \n    g_b2 = mse_b2 - v_bar_b2\n    \n    wECE2 = np.sum((n_b2 / N2) * np.abs(g_b2))\n    RMSCE2 = np.sqrt(np.sum((n_b2 / N2) * (g_b2**2)))\n    empty_bins2 = B2 - np.count_nonzero(n_b2)\n    \n    results.extend([round(wECE2, 6), round(RMSCE2, 6), empty_bins2])\n\n    # --- Test Case 3: Zeros, extremes, and empty bins ---\n    N3 = 300\n    B3 = 10\n    bin_edges3 = np.array([0.0, 1e-9, 1e-7, 1e-6, 5e-6, 5e-5, 5e-4, 5e-3, 5e-2, 5e-1, 1.0])\n\n    # Generate data for Case 3\n    y3 = np.zeros(N3)\n    j3 = np.arange(1, N3 + 1)\n    u3 = (j3 - 0.5) / N3\n    s3 = np.sqrt(2) * erfinv(2 * u3 - 1)\n    \n    pi3 = (a * (j3 - 1) + c) % N3 + 1\n    s_permuted3 = s3[pi3 - 1]\n    \n    v_values = [0.0, 1e-6, 5e-3, 5e-2, 2e-1, 1.0]\n    v3 = np.repeat(v_values, 50)\n    w3 = 0.5 * v3\n    y_hat3 = np.sqrt(w3) * s_permuted3\n    squared_errors3 = (y_hat3 - y3)**2\n\n    # Binning and statistics for Case 3\n    bin_indices3 = -np.ones(N3, dtype=int)\n    for b in range(B3 - 1):\n        mask = (v3 >= bin_edges3[b]) & (v3 < bin_edges3[b+1])\n        bin_indices3[mask] = b\n    mask_last_bin3 = (v3 >= bin_edges3[B3-1]) & (v3 <= bin_edges3[B3])\n    bin_indices3[mask_last_bin3] = B3 - 1\n\n    n_b3 = np.bincount(bin_indices3, minlength=B3)\n    v_sums3 = np.bincount(bin_indices3, weights=v3, minlength=B3)\n    mse_sums3 = np.bincount(bin_indices3, weights=squared_errors3, minlength=B3)\n    \n    v_bar_b3 = np.zeros(B3)\n    mse_b3 = np.zeros(B3)\n    non_empty_mask3 = n_b3 > 0\n    v_bar_b3[non_empty_mask3] = v_sums3[non_empty_mask3] / n_b3[non_empty_mask3]\n    mse_b3[non_empty_mask3] = mse_sums3[non_empty_mask3] / n_b3[non_empty_mask3]\n    \n    g_b3 = mse_b3 - v_bar_b3\n    \n    wECE3 = np.sum((n_b3 / N3) * np.abs(g_b3))\n    RMSCE3 = np.sqrt(np.sum((n_b3 / N3) * (g_b3**2)))\n    empty_bins3 = B3 - np.count_nonzero(n_b3)\n    \n    results.extend([round(wECE3, 6), round(RMSCE3, 6), empty_bins3])\n\n    # Final print statement\n    formatted_results = []\n    for i in range(len(results)):\n        if (i % 3) < 2:\n            formatted_results.append(f\"{results[i]:.6f}\")\n        else:\n            formatted_results.append(str(results[i]))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2760108"}]}