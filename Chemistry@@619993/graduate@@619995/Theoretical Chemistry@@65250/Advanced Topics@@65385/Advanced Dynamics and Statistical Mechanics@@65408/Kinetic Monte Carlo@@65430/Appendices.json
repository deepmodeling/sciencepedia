{"hands_on_practices": [{"introduction": "A Kinetic Monte Carlo (KMC) trajectory is a numerical realization of an underlying continuous-time Markov process. This exercise takes you back to the theoretical source, asking you to derive a key macroscopic observable—the time autocorrelation function—directly from the master equation for a simple yet illustrative two-state system. This foundational practice [@problem_id:2782393] builds a robust understanding of the connection between the microscopic stochastic rules that define a KMC model and the emergent statistical properties of the system they simulate.", "problem": "A single adsorption site on a catalytic surface is modeled by a two-state continuous-time Markov chain that underlies a Kinetic Monte Carlo (KMC) trajectory. State $\\mathsf{1}$ denotes that the site is occupied and state $\\mathsf{2}$ denotes that the site is vacant. Transitions $\\mathsf{1} \\to \\mathsf{2}$ occur with rate $k_{12}$ and transitions $\\mathsf{2} \\to \\mathsf{1}$ occur with rate $k_{21}$, with $k_{12} > 0$ and $k_{21} > 0$. The system is prepared and observed in its stationary regime. Define the state indicator $I(t)$ by $I(t) = 1$ if the system is in state $\\mathsf{1}$ at time $t$ and $I(t) = 0$ if it is in state $\\mathsf{2}$ at time $t$. Let $\\langle \\cdot \\rangle$ denote a stationary ensemble average and define the zero-mean fluctuation $\\delta I(t) \\equiv I(t) - \\langle I \\rangle$.\n\nStarting only from the continuous-time master equation for the two-state process and standard properties of stationary continuous-time Markov chains, derive the stationary time autocorrelation function\n$$\nC_I(t) \\equiv \\langle \\delta I(0)\\,\\delta I(t)\\rangle\n$$\nfor $t \\ge 0$ as a closed-form expression in terms of $k_{12}$, $k_{21}$, and $t$. Your derivation should proceed from first principles by solving for conditional probabilities or conditional expectations as needed, without invoking pre-memorized correlation formulas. Then, by analyzing the generator of the process or your derived expression, identify the exponential decay constant of $C_I(t)$ in terms of $k_{12}$ and $k_{21}$.\n\nProvide your final result for $C_I(t)$ as a single analytical expression in terms of $k_{12}$, $k_{21}$, and $t$. No numerical evaluation is required. Express the final answer without units.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in the theory of stochastic processes applied to theoretical chemistry. All necessary information is provided, and no contradictions are present. The problem is valid, and a solution will be provided.\n\nThe system is a two-state continuous-time Markov chain with states $\\mathsf{1}$ (occupied) and $\\mathsf{2}$ (vacant). The transition rates are $k_{12}$ for $\\mathsf{1} \\to \\mathsf{2}$ and $k_{21}$ for $\\mathsf{2} \\to \\mathsf{1}$. Let $P_1(t)$ and $P_2(t)$ be the probabilities of the system being in state $\\mathsf{1}$ and $\\mathsf{2}$, respectively, at time $t$. The evolution of these probabilities is governed by the continuous-time master equation:\n$$\n\\frac{d}{dt} \\begin{pmatrix} P_1(t) \\\\ P_2(t) \\end{pmatrix} = \\begin{pmatrix} -k_{12} & k_{21} \\\\ k_{12} & -k_{21} \\end{pmatrix} \\begin{pmatrix} P_1(t) \\\\ P_2(t) \\end{pmatrix}\n$$\nThe problem requires the calculation of the stationary time autocorrelation function $C_I(t) = \\langle \\delta I(0)\\,\\delta I(t)\\rangle$ for $t \\ge 0$. The state indicator is $I(t)=1$ for state $\\mathsf{1}$ and $I(t)=0$ for state $\\mathsf{2}$. The fluctuation is $\\delta I(t) = I(t) - \\langle I \\rangle$.\n\nFirst, we expand the definition of $C_I(t)$:\n$$\nC_I(t) = \\langle (I(0) - \\langle I \\rangle)(I(t) - \\langle I \\rangle) \\rangle = \\langle I(0)I(t) \\rangle - \\langle I(0)\\rangle\\langle I(t)\\rangle - \\langle I(t)\\rangle\\langle I(0)\\rangle + \\langle I\\rangle^2\n$$\nSince the system is in its stationary regime, the ensemble averages are time-independent: $\\langle I(t) \\rangle = \\langle I(0) \\rangle = \\langle I \\rangle$. The expression simplifies to:\n$$\nC_I(t) = \\langle I(0)I(t) \\rangle - \\langle I \\rangle^2\n$$\nOur derivation proceeds in three steps:\n$1$. Calculate the stationary average $\\langle I \\rangle$.\n$2$. Calculate the two-time correlation $\\langle I(0)I(t) \\rangle$.\n$3$. Combine the results to find $C_I(t)$.\n\n_Step 1: Stationary Average $\\langle I \\rangle$_\n\nIn the stationary regime, the probabilities are constant, so $\\frac{dP_1}{dt} = \\frac{dP_2}{dt} = 0$. Let the stationary probabilities be $P_1^{ss}$ and $P_2^{ss}$. The master equation yields the detailed balance condition:\n$$\nk_{12} P_1^{ss} = k_{21} P_2^{ss}\n$$\nAlong with the normalization condition $P_1^{ss} + P_2^{ss} = 1$, we can solve for the stationary probabilities. Substituting $P_2^{ss} = 1 - P_1^{ss}$:\n$$\nk_{12} P_1^{ss} = k_{21} (1 - P_1^{ss}) \\implies (k_{12} + k_{21}) P_1^{ss} = k_{21} \\implies P_1^{ss} = \\frac{k_{21}}{k_{12} + k_{21}}\n$$\nAnd consequently, $P_2^{ss} = 1 - P_1^{ss} = \\frac{k_{12}}{k_{12} + k_{21}}$.\n\nThe average value of the state indicator $I(t)$ is given by the expectation:\n$$\n\\langle I \\rangle = (1) \\cdot P_1^{ss} + (0) \\cdot P_2^{ss} = P_1^{ss} = \\frac{k_{21}}{k_{12} + k_{21}}\n$$\n\n_Step 2: Two-Time Correlation $\\langle I(0)I(t) \\rangle$_\n\nThe two-time correlation is calculated by averaging over all possible initial and final states:\n$$\n\\langle I(0)I(t) \\rangle = \\sum_{i,j \\in \\{1,2\\}} I_i I_j P(X(0)=j, X(t)=i)\n$$\nwhere $X(t)$ is the state of the system at time $t$, and $I_j$ is the value of the indicator in state $j$. Since $I_2=0$, the only non-zero term in the sum is for $i=1, j=1$:\n$$\n\\langle I(0)I(t) \\rangle = I_1 \\cdot I_1 \\cdot P(X(0)=1, X(t)=1) = P(X(0)=1, X(t)=1)\n$$\nUsing the definition of conditional probability and the fact that the system is stationary:\n$$\nP(X(0)=1, X(t)=1) = P(X(t)=1 | X(0)=1) \\cdot P(X(0)=1) = P_{11}(t) \\cdot P_1^{ss}\n$$\nHere, $P_{11}(t) \\equiv P(X(t)=1 | X(0)=1)$ is the conditional probability of being in state $\\mathsf{1}$ at time $t$ given that the system was in state $\\mathsf{1}$ at time $t=0$. To find $P_{11}(t)$, we solve the master equation with the initial condition $P_1(0)=1$ and $P_2(0)=0$.\nThe equation for $P_1(t)$ is $\\frac{dP_1}{dt} = k_{21} P_2 - k_{12} P_1$. Using $P_1(t)+P_2(t)=1$, we get $P_2(t)=1-P_1(t)$. Substituting this gives a first-order linear ordinary differential equation for $P_1(t) \\equiv P_{11}(t)$:\n$$\n\\frac{dP_1}{dt} = k_{21} (1 - P_1) - k_{12} P_1 = k_{21} - (k_{12} + k_{21}) P_1\n$$\nLet $\\lambda = k_{12} + k_{21}$. The equation is $\\frac{dP_1}{dt} + \\lambda P_1 = k_{21}$. The general solution is the sum of a homogeneous solution $A\\exp(-\\lambda t)$ and a particular solution. A particular solution is the stationary value $P_1^{ss} = k_{21}/\\lambda$. Thus:\n$$\nP_1(t) = A\\exp(-\\lambda t) + \\frac{k_{21}}{\\lambda}\n$$\nApplying the initial condition $P_1(0)=1$:\n$$\n1 = A\\exp(0) + \\frac{k_{21}}{\\lambda} \\implies A = 1 - \\frac{k_{21}}{\\lambda} = \\frac{\\lambda - k_{21}}{\\lambda} = \\frac{k_{12}}{\\lambda}\n$$\nSo, the conditional probability is:\n$$\nP_{11}(t) = \\frac{k_{12}}{k_{12} + k_{21}} \\exp(-(k_{12} + k_{21})t) + \\frac{k_{21}}{k_{12} + k_{21}}\n$$\nNow we can compute $\\langle I(0)I(t) \\rangle = P_{11}(t) \\cdot P_1^{ss}$:\n$$\n\\langle I(0)I(t) \\rangle = \\left( \\frac{k_{12}}{k_{12} + k_{21}} \\exp(-(k_{12} + k_{21})t) + \\frac{k_{21}}{k_{12} + k_{21}} \\right) \\left( \\frac{k_{21}}{k_{12} + k_{21}} \\right)\n$$\n\n_Step 3: Final Assembly of $C_I(t)$_\n\nWe now subtract $\\langle I \\rangle^2 = (P_1^{ss})^2$ from the expression for $\\langle I(0)I(t) \\rangle$:\n$$\nC_I(t) = \\left( \\frac{k_{12}}{k_{12} + k_{21}} \\exp(-(k_{12} + k_{21})t) + P_1^{ss} \\right) P_1^{ss} - (P_1^{ss})^2\n$$\n$$\nC_I(t) = \\frac{k_{12}}{k_{12} + k_{21}} \\exp(-(k_{12} + k_{21})t) \\cdot P_1^{ss} + (P_1^{ss})^2 - (P_1^{ss})^2\n$$\nSubstituting $P_1^{ss} = \\frac{k_{21}}{k_{12} + k_{21}}$:\n$$\nC_I(t) = \\frac{k_{12}}{k_{12} + k_{21}} \\left( \\frac{k_{21}}{k_{12} + k_{21}} \\right) \\exp(-(k_{12} + k_{21})t)\n$$\nThis simplifies to the final expression for the autocorrelation function:\n$$\nC_I(t) = \\frac{k_{12}k_{21}}{(k_{12} + k_{21})^2} \\exp(-(k_{12} + k_{21})t)\n$$\nThis expression is valid for $t \\ge 0$. The pre-exponential factor is $C_I(0) = \\frac{k_{12}k_{21}}{(k_{12}+k_{21})^2} = P_1^{ss} P_2^{ss}$, which is the variance of the Bernoulli random variable $I$, as expected.\n\nFinally, we identify the exponential decay constant. The function is of the form $A\\exp(-\\gamma t)$, where $\\gamma$ is the decay constant. By direct inspection of the derived expression, the exponential decay constant is the sum of the forward and backward rates:\n$$\n\\gamma = k_{12} + k_{21}\n$$\nThis rate is the absolute value of the non-zero eigenvalue of the transition rate matrix (the generator) of the Markov process, which governs the relaxation of the system to its stationary state.", "answer": "$$\n\\boxed{\\frac{k_{12}k_{21}}{(k_{12} + k_{21})^2} \\exp(-(k_{12} + k_{21})t)}\n$$", "id": "2782393"}, {"introduction": "After building a model based on theory, a crucial step for any computational scientist is to verify that the computer implementation correctly reflects that theory. This practice [@problem_id:2782332] presents a common and essential task: using simulation output to validate that a fundamental physical principle, in this case detailed balance, is satisfied. By working through this hypothetical scenario, you will develop indispensable skills in statistical hypothesis testing and learn how to rigorously assess the physical fidelity of a KMC code from finite trajectory data.", "problem": "A two-state kinetic Monte Carlo (KMC) simulation is used to model a system with states $A$ and $B$ having energies $E_A$ and $E_B = E_A + \\Delta E$, respectively. The temperature is fixed so that $\\beta \\Delta E = 2.0$, where $\\beta = 1/(k_B T)$ and $k_B$ is the Boltzmann constant. The KMC implementation is supposed to satisfy detailed balance with respect to the canonical (Boltzmann) equilibrium distribution. After discarding an equilibration period, a single long production trajectory is recorded. Over the production run, the total residence times in each state are measured as $\\tau_A = 8810$ and $\\tau_B = 1190$, and the observed numbers of transitions are $n_{A\\to B} = 1220$ and $n_{B\\to A} = 1165$. Assume that the trajectory is stationary and that, conditional on the realized residence times, transition counts from each state arise as independent Poisson processes with intensities equal to the corresponding transition rates.\n\nYou are asked to validate that the implementation satisfies detailed balance by testing whether the empirical rate ratio is statistically consistent with $\\exp(-\\beta \\Delta E)$ at the $\\alpha = 0.05$ significance level. Which of the following testing protocols are statistically sound for this purpose and lead to a correct conclusion given the data?\n\nA. Estimate $k_{A\\to B}$ and $k_{B\\to A}$ by $k_{A\\to B} = n_{A\\to B}/\\tau_A$ and $k_{B\\to A} = n_{B\\to A}/\\tau_B$. Test the null hypothesis $H_0: \\rho = k_{A\\to B}/k_{B\\to A} = \\rho_0 \\equiv \\exp(-\\beta \\Delta E)$ using the conditional binomial formulation: given $n = n_{A\\to B} + n_{B\\to A}$, $X = n_{A\\to B} \\sim \\mathrm{Binomial}(n, p_0)$ with $p_0 = (\\rho_0 \\tau_A)/(\\rho_0 \\tau_A + \\tau_B)$. Use a two-sided test at level $\\alpha = 0.05$ (or equivalently invert to a $95\\%$ confidence interval for $\\rho$ via the mapping $\\rho = (p \\tau_B)/((1-p) \\tau_A)$). Conclude that $H_0$ is not rejected for the given data.\n\nB. Compare the raw count ratio $n_{A\\to B}/n_{B\\to A}$ directly to $\\exp(-\\beta \\Delta E)$. Since $1220/1165 \\gg \\exp(-2)$, reject detailed balance at $\\alpha = 0.05$.\n\nC. Validate detailed balance by checking only that the occupancy ratio $\\tau_B/\\tau_A$ matches $\\exp(-\\beta \\Delta E)$. Since $1190/8810 \\approx \\exp(-2)$, accept that detailed balance holds without analyzing transition counts.\n\nD. Form the log-rate ratio estimator $\\widehat{\\ell} = \\ln(n_{A\\to B}) - \\ln(\\tau_A) - \\ln(n_{B\\to A}) + \\ln(\\tau_B)$ and approximate its sampling distribution by a normal distribution with variance $\\operatorname{Var}(\\widehat{\\ell}) \\approx 1/n_{A\\to B} + 1/n_{B\\to A}$ (treating residence times as fixed). Construct a two-sided $95\\%$ confidence interval for $\\ell = \\ln \\rho$ and check whether $-\\beta \\Delta E$ lies inside. Conclude that $H_0$ is not rejected for the given data.\n\nSelect all that apply.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- System states: $A$ and $B$.\n- State energies: $E_A$, $E_B = E_A + \\Delta E$.\n- Thermal energy parameter: $\\beta \\Delta E = 2.0$, where $\\beta = 1/(k_B T)$.\n- Required property of the KMC implementation: Satisfy detailed balance with respect to the canonical distribution.\n- Simulation data from a single long, stationary production trajectory:\n  - Total residence time in state $A$: $\\tau_A = 8810$.\n  - Total residence time in state $B$: $\\tau_B = 1190$.\n  - Number of transitions from $A$ to $B$: $n_{A\\to B} = 1220$.\n  - Number of transitions from $B$ to $A$: $n_{B\\to A} = 1165$.\n- Statistical assumptions:\n  - The trajectory is stationary.\n  - Conditional on $\\tau_A$ and $\\tau_B$, $n_{A\\to B}$ and $n_{B\\to A}$ arise from independent Poisson processes.\n- Task: Validate detailed balance by statistically testing if the empirical rate ratio is consistent with $\\exp(-\\beta \\Delta E)$ at a significance level of $\\alpha = 0.05$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. It presents a standard scenario in computational statistical mechanics involving a Kinetic Monte Carlo (KMC) simulation of a two-state system. The concepts of detailed balance, the Boltzmann distribution, and the modeling of transition events as Poisson processes are fundamental and correctly applied. The detailed balance condition for microscopic reversibility dictates that in equilibrium, the flux between any two states $i$ and $j$ must be equal in both directions: $p_i^{eq} k_{i\\to j} = p_j^{eq} k_{j\\to i}$, where $p_i^{eq}$ are the equilibrium probabilities and $k_{i\\to j}$ are the transition rates. For a canonical ensemble, $p_i^{eq} \\propto \\exp(-\\beta E_i)$. This leads to the rate ratio condition $k_{A\\to B}/k_{B\\to A} = p_B^{eq}/p_A^{eq} = \\exp(-\\beta(E_B-E_A)) = \\exp(-\\beta \\Delta E)$. The problem is to test this theoretical relationship using simulation data.\n\nThe problem is well-posed. It provides sufficient data and clear statistical assumptions to perform a hypothesis test. The language is precise and objective. There are no contradictions, no missing information, and no reliance on pseudoscience. The data values are physically plausible. The problem is a non-trivial application of statistical inference to simulation data, a common task in the field.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A solution will be derived.\n\n### Derivation and Option Analysis\nThe null hypothesis to be tested is $H_0: \\rho = \\rho_0$, where $\\rho = k_{A\\to B}/k_{B\\to A}$ is the true ratio of the underlying transition rates, and $\\rho_0 = \\exp(-\\beta \\Delta E) = \\exp(-2.0)$ is the theoretically required ratio for detailed balance. The given data are $\\tau_A = 8810$, $\\tau_B = 1190$, $n_{A\\to B} = 1220$, and $n_{B\\to A} = 1165$. The significance level is $\\alpha = 0.05$.\n\nThe assumption that transition counts are from independent Poisson processes implies $n_{A\\to B} \\sim \\mathrm{Poisson}(k_{A\\to B}\\tau_A)$ and $n_{B\\to A} \\sim \\mathrm{Poisson}(k_{B\\to A}\\tau_B)$.\n\n**Option A Evaluation**\n\nThis option proposes a conditional binomial test. Given two independent Poisson variables, $X_1 \\sim \\mathrm{Poisson}(\\lambda_1)$ and $X_2 \\sim \\mathrm{Poisson}(\\lambda_2)$, the distribution of $X_1$ conditional on their sum $N = X_1+X_2$ is binomial: $X_1 | (X_1+X_2=N) \\sim \\mathrm{Binomial}(N, p)$, where $p = \\lambda_1/(\\lambda_1+\\lambda_2)$. In our case, $X_1 = n_{A\\to B}$ and $X_2 = n_{B\\to A}$. The intensities are $\\lambda_1 = k_{A\\to B}\\tau_A$ and $\\lambda_2 = k_{B\\to A}\\tau_B$. The probability parameter $p$ can be expressed in terms of the rate ratio $\\rho$:\n$$p = \\frac{k_{A\\to B}\\tau_A}{k_{A\\to B}\\tau_A + k_{B\\to A}\\tau_B} = \\frac{\\rho \\tau_A}{\\rho \\tau_A + \\tau_B}$$\nUnder the null hypothesis $H_0: \\rho = \\rho_0$, this probability becomes $p_0 = (\\rho_0 \\tau_A)/(\\rho_0 \\tau_A + \\tau_B)$. We test if the observed count $n_{A\\to B}$ is a plausible outcome from the distribution $\\mathrm{Binomial}(n, p_0)$, where $n = n_{A\\to B} + n_{B\\to A}$.\n\nThis is a statistically exact and sound procedure that correctly uses the conditional distribution to eliminate nuisance parameters (the individual rates).\nLet us perform the test.\n- $\\rho_0 = \\exp(-2.0) \\approx 0.135335$.\n- Total transitions $n = 1220 + 1165 = 2385$.\n- Null probability $p_0 = \\frac{0.135335 \\cdot 8810}{0.135335 \\cdot 8810 + 1190} = \\frac{1192.3}{1192.3 + 1190} \\approx 0.50047$.\n- We test if $n_{A\\to B} = 1220$ is consistent with a draw from $\\mathrm{Binomial}(2385, 0.50047)$.\n- Using the normal approximation to the binomial distribution (valid for large $n$):\n  - Mean: $\\mu = n p_0 = 2385 \\cdot 0.50047 \\approx 1193.62$.\n  - Standard deviation: $\\sigma = \\sqrt{n p_0 (1-p_0)} = \\sqrt{2385 \\cdot 0.50047 \\cdot 0.49953} \\approx \\sqrt{596.25} \\approx 24.42$.\n- The test statistic is $Z = (n_{A\\to B} - \\mu) / \\sigma = (1220 - 1193.62) / 24.42 \\approx 1.08$.\n- The critical value for a two-sided test at $\\alpha = 0.05$ is $Z_{crit} \\approx 1.96$. Since $|Z| = 1.08 < 1.96$, we do not reject the null hypothesis $H_0$. The conclusion stated in the option is correct. The protocol is statistically sound.\n\nVerdict for A: **Correct**.\n\n**Option B Evaluation**\n\nThis option compares the raw ratio of transition counts $n_{A\\to B}/n_{B\\to A}$ to $\\exp(-\\beta \\Delta E)$. This is fundamentally erroneous. The ratio of counts is an estimator for the ratio of the Poisson intensities, not the rates:\n$$ E\\left[\\frac{n_{A\\to B}}{n_{B\\to A}}\\right] \\approx \\frac{E[n_{A\\to B}]}{E[n_{B\\to A}]} = \\frac{k_{A\\to B}\\tau_A}{k_{B\\to A}\\tau_B} = \\rho \\frac{\\tau_A}{\\tau_B} $$\nOne must account for the residence times to estimate the rate ratio $\\rho$. The correct estimator for $\\rho$ is $\\widehat{\\rho} = (n_{A\\to B}/\\tau_A) / (n_{B\\to A}/\\tau_B)$. Comparing $n_{A\\to B}/n_{B\\to A}$ directly to $\\rho_0$ is a severe methodological error.\nNumerically, $n_{A\\to B}/n_{B\\to A} = 1220/1165 \\approx 1.047$, whereas $\\rho_0 = \\exp(-2.0) \\approx 0.135$. The proposed procedure compares two quantities that are not expected to be equal, even if detailed balance holds.\n\nVerdict for B: **Incorrect**.\n\n**Option C Evaluation**\n\nThis option suggests validating detailed balance by checking only the ratio of residence times $\\tau_B/\\tau_A$ against the equilibrium probability ratio $p_B^{eq}/p_A^{eq} = \\exp(-\\beta \\Delta E)$.\nIn the long-time limit of a stationary Markov process, the ratio of residence times estimates the ratio of stationary probabilities: $\\tau_B/\\tau_A \\approx \\pi_B/\\pi_A$. For a two-state system, the stationary distribution satisfies $\\pi_A k_{A\\to B} = \\pi_B k_{B\\to A}$, which implies $\\pi_B/\\pi_A = k_{A\\to B}/k_{B\\to A}$. If detailed balance holds, then $k_{A\\to B}/k_{B\\to A} = p_B^{eq}/p_A^{eq}$, and therefore $\\pi_B/\\pi_A = p_B^{eq}/p_A^{eq}$. So checking populations is equivalent to checking rates in this simple two-state case.\nNumerically, $\\tau_B/\\tau_A = 1190/8810 \\approx 0.13507$, which is indeed very close to $\\exp(-2.0) \\approx 0.13534$.\nHowever, the protocol is described as \"checking *only* that the occupancy ratio... matches... *without analyzing transition counts*\". This is methodologically deficient. The problem explicitly provides transition count data and asks to test the ratio of the underlying *rates*. Ignoring the direct data on transitions (the counts) in favor of an integrated quantity (the residence times) constitutes an incomplete and less direct test. A proper validation of a rate-based model must scrutinize the statistics of the transitions themselves. This protocol fails to use all relevant information and does not directly compute the empirical rate ratio as requested.\n\nVerdict for C: **Incorrect**.\n\n**Option D Evaluation**\n\nThis option proposes a test based on the logarithm of the rate ratio estimator, $\\widehat{\\ell} = \\ln(\\widehat{\\rho})$, where $\\widehat{\\rho} = \\frac{n_{A\\to B}/\\tau_A}{n_{B\\to A}/\\tau_B}$. Thus, $\\widehat{\\ell} = \\ln(n_{A\\to B}) - \\ln(\\tau_A) - \\ln(n_{B\\to A}) + \\ln(\\tau_B)$. This is correct.\nThe method uses the Delta method to approximate the variance, treating residence times as fixed (conditioning). The variance of a function $f(X_1, X_2)$ is approximately $\\sum (\\partial f / \\partial X_i)^2 \\operatorname{Var}(X_i)$. Here, the random variables are $n_{A\\to B}$ and $n_{B\\to A}$, which are Poisson-distributed. We can approximate $\\operatorname{Var}(n) \\approx n$ for large $n$.\nFor $\\widehat{\\ell}$, the partial derivatives are $\\partial\\widehat{\\ell}/\\partial n_{A\\to B} = 1/n_{A\\to B}$ and $\\partial\\widehat{\\ell}/\\partial n_{B\\to A} = -1/n_{B\\to A}$.\nThe variance is then $\\operatorname{Var}(\\widehat{\\ell}) \\approx (1/n_{A\\to B})^2 \\operatorname{Var}(n_{A\\to B}) + (-1/n_{B\\to A})^2 \\operatorname{Var}(n_{B\\to A}) \\approx (1/n_{A\\to B})^2 n_{A\\to B} + (1/n_{B\\to A})^2 n_{B\\to A} = 1/n_{A\\to B} + 1/n_{B\\to A}$. This variance formula is correct under the stated approximations. For large counts, the distribution of $\\widehat{\\ell}$ is well approximated by a normal distribution. This entire protocol is a standard, statistically sound large-sample approach.\nLet us perform the test.\n- The log-ratio estimate is $\\widehat{\\ell} = \\ln\\left( \\frac{1220/8810}{1165/1190} \\right) \\approx \\ln\\left( \\frac{0.13848}{0.97899} \\right) \\approx \\ln(0.14145) \\approx -1.9558$.\n- The null hypothesis value is $\\ell_0 = \\ln(\\rho_0) = -2.0$.\n- The estimated standard error is $SE(\\widehat{\\ell}) = \\sqrt{1/1220 + 1/1165} \\approx \\sqrt{0.0008197 + 0.0008584} = \\sqrt{0.0016781} \\approx 0.04096$.\n- A $95\\%$ confidence interval for $\\ell$ is $\\widehat{\\ell} \\pm 1.96 \\cdot SE(\\widehat{\\ell}) = -1.9558 \\pm 1.96 \\cdot 0.04096 = -1.9558 \\pm 0.0803$.\n- The interval is $[-2.0361, -1.8755]$.\n- The null value $\\ell_0 = -2.0$ is inside this confidence interval. Therefore, we do not reject $H_0$. The conclusion is correct. The protocol is statistically sound.\n\nVerdict for D: **Correct**.", "answer": "$$\\boxed{AD}$$", "id": "2782332"}, {"introduction": "The power of KMC is most apparent when applied to complex systems with a vast number of possible kinetic events, but this presents a significant computational bottleneck for naive implementations. This exercise [@problem_id:2782362] tackles the critical issue of algorithmic efficiency by guiding you to derive and analyze the Fenwick tree, a sophisticated data structure that dramatically reduces the computational cost of the event-selection step. Mastering such optimization techniques is essential for transforming KMC from a simple conceptual tool into a high-performance research method capable of tackling large-scale problems.", "problem": "Consider a rejection-free kinetic Monte Carlo (KMC) simulation with $M$ elementary events indexed by $i \\in \\{1,\\dots,M\\}$, each carrying a strictly positive propensity $a_i$. Let $S = \\sum_{i=1}^{M} a_i$ denote the total propensity. Event selection proceeds by drawing $u \\sim \\mathcal{U}(0,1)$ and choosing the minimal index $k$ satisfying $\\sum_{i=1}^{k} a_i \\geq r$, where $r = uS$. After each firing, a localized subset of propensities is modified: $a_j \\leftarrow a_j + \\Delta$ for some $j$ and increment $\\Delta$ that may depend on system state.\n\nYou will design a Binary Indexed Tree (Fenwick tree) data structure storing an auxiliary array `T[1:M]` of partial sums to support both prefix-sum queries and single-index updates in sublinear time. Your derivation must start from the defining requirements of KMC event selection and the need to maintain the cumulative sums under single-index increments, without assuming any pre-existing formula for Binary Indexed Trees.\n\nTasks:\n\n1) Define a function $\\operatorname{lsb}(i)$ as the largest power of two dividing $i$ (i.e., $\\operatorname{lsb}(i) = 2^{\\nu_{2}(i)}$ where $\\nu_{2}$ is the $2$-adic valuation). Propose a representation of the auxiliary array `T` that ensures each `T[i]` stores a contiguous range-sum of the $a_k$ and that the ranges for indices generated by repeatedly subtracting $\\operatorname{lsb}(\\cdot)$ from a query index $j$ partition the prefix $\\{1,\\dots,j\\}$. From this representation, derive:\n- A rule to compute the prefix sum $A(j) = \\sum_{i=1}^{j} a_i$ using only `T` and repeated transitions of the form $i \\leftarrow i - \\operatorname{lsb}(i)$.\n- A rule to update `T` when a single propensity is incremented, $a_j \\leftarrow a_j + \\Delta$, using only transitions of the form $i \\leftarrow i + \\operatorname{lsb}(i)$ starting from $i=j$.\n\n2) Using only the fact that KMC selection requires the smallest $k$ with $\\sum_{i=1}^{k} a_i \\geq r$, construct a search procedure that uses the `T` array to find this $k$ by probing at most one index per bit position of $M$. Justify that the number of probes is $O(\\log M)$ by showing it equals the number of bits needed to represent the largest power of two not exceeding $M$.\n\n3) Give a tight worst-case bound, in terms of $M$, on the number of array entries of `T` visited by:\n- A single prefix-sum query $A(j)$.\n- A single-point update at index $j$.\n- The selection routine from Task $2$.\n\n4) Finally, take $M = 10^{6}$. For the selection routine in Task $2$, determine the exact worst-case number of comparisons (i.e., the number of conditional checks of the form “is a probed partial sum $\\leq r$?”) performed, assuming the standard “binary lifting” implementation that tests exactly one node per bit position.\n\nProvide the final answer to Task $4$ as a single integer. No rounding is necessary and no units should be included in the final answer. All intermediate derivations must be shown symbolically before concluding with the requested integer.", "solution": "The problem requires the derivation and analysis of a Binary Indexed Tree (or Fenwick tree) data structure optimized for event selection in kinetic Monte Carlo simulations. The derivation must proceed from first principles.\n\nLet the array of propensities be $a[1{:}M]$, with $a_i > 0$ for all $i \\in \\{1, \\dots, M\\}$. We are tasked with designing an auxiliary data structure, an array $T[1{:}M]$, to accelerate two operations: the computation of prefix sums $A(j) = \\sum_{i=1}^{j} a_i$, and the update of a single propensity $a_j \\leftarrow a_j + \\Delta$. Subsequently, we must devise a search procedure to find the minimal index $k$ such that $A(k) \\geq r$ for a given value $r$, and analyze the complexity of these operations.\n\nLet $\\operatorname{lsb}(i)$ denote the largest power of two that divides the integer $i$. In binary arithmetic, this corresponds to isolating the least significant bit, which can be computed as `i  (-i)`.\n\n**Task 1: Data Structure Definition and Operational Rules**\n\nThe core requirement is that the set of indices $\\{1, \\dots, j\\}$ can be partitioned by repeatedly applying the transformation $i \\leftarrow i - \\operatorname{lsb}(i)$. An integer $j$ can be uniquely represented as a sum of powers of two, its binary representation. The operation $j - \\operatorname{lsb}(j)$ removes the lowest-order power of two from this sum. Consequently, the sequence $j_0=j, j_1=j_0-\\operatorname{lsb}(j_0), j_2=j_1-\\operatorname{lsb}(j_1), \\dots, 0$ defines a partition of the interval $[1, j]$ into disjoint, contiguous subintervals: $(j_1, j_0], (j_2, j_1], \\dots$.\n\nWe define the value stored in the auxiliary array at index $i$, $T[i]$, to be the sum of propensities over the last subinterval in this partition of $[1, i]$.\n$$ T[i] = \\sum_{k = i - \\operatorname{lsb}(i) + 1}^{i} a_k $$\nThis structure ensures that each $T[i]$ stores a sum over a contiguous range of the base propensities $a_k$.\n\nWith this definition, we derive the rule for computing a prefix sum $A(j) = \\sum_{i=1}^{j} a_i$. We decompose the sum according to the partition of $[1,j]$:\n$$ A(j) = \\left( \\sum_{k=1}^{j-\\operatorname{lsb}(j)} a_k \\right) + \\left( \\sum_{k=j-\\operatorname{lsb}(j)+1}^{j} a_k \\right) $$\nThe second term is, by our definition, $T[j]$. The first term is the prefix sum $A(j - \\operatorname{lsb}(j))$. This gives the recurrence relation:\n$$ A(j) = A(j - \\operatorname{lsb}(j)) + T[j] $$\nUnrolling this recurrence yields the algorithm for a prefix-sum query: to compute $A(j)$, one initializes a sum to $0$ and repeatedly adds $T[i]$ while updating the index $i \\leftarrow i - \\operatorname{lsb}(i)$, starting from $i=j$ until $i=0$.\n\nNext, we derive the update rule. When a single propensity $a_j$ is incremented by $\\Delta$, we must add $\\Delta$ to every $T[i]$ whose summation range includes the index $j$. That is, we must identify all $i \\in [1, M]$ such that $i - \\operatorname{lsb}(i) + 1 \\leq j \\leq i$.\nLet us analyze the update path proposed: $i_0 = j$, $i_{k+1} = i_k + \\operatorname{lsb}(i_k)$, continuing as long as $i_k \\le M$. We claim this sequence comprises exactly the set of indices whose corresponding $T$ values must be updated.\nAn index $i$ covers index $j$ if $i - \\operatorname{lsb}(i)  j \\le i$.\nThe operation $i \\to i+\\operatorname{lsb}(i)$ can be interpreted as finding the next \"ancestor\" in the implicit tree structure whose range contains the range of $i$.\nFor any $i$ in the update sequence starting from $j$, $j$ is contained in the summation interval of $T[i]$. Conversely, if an index $i$ covers $j$, then $i$ must lie on the update path starting from $j$.\nThe proof relies on the bitwise properties of the indices. The range for $T[i]$ covers all indices that share the same binary prefix as $i$ up to the position of $\\operatorname{lsb}(i)$. The update path $i \\leftarrow i + \\operatorname{lsb}(i)$ traverses indices $i$ that are \"responsible\" for $j$ by adding the smallest power of two, which corresponds to finding the next node up in the hierarchy that contains the current interval.\nThe update procedure is therefore: starting with $i=j$, add $\\Delta$ to $T[i]$, then update $i \\leftarrow i + \\operatorname{lsb}(i)$, repeating until $i > M$.\n\n**Task 2: Ranged Search Procedure**\n\nThe KMC selection requires finding the smallest integer $k$ such that $A(k) \\geq r$. Since $A(k)$ is a monotonically non-decreasing function of $k$, this is a search problem. A naive binary search on $k \\in [1, M]$ would require $O(\\log M)$ probes, each costing $O(\\log M)$ for a prefix sum query, leading to an overall complexity of $O((\\log M)^2)$. We can achieve $O(\\log M)$ by exploiting the structure of the tree.\n\nThe procedure, known as binary lifting or walking on the Fenwick tree, constructs the index $k$ bit by bit, from most significant to least significant. Let $p_{max} = \\lfloor \\log_2 M \\rfloor$. We iterate with a step size $p=2^i$ for $i$ from $p_{max}$ down to $0$. We maintain a current index, `k_curr` (initially $0$), and the corresponding prefix sum, `sum_curr` (initially $0$).\n\nIn each step, we consider extending the current solution by $p$: let `k_test = k_curr + p`. If `k_test` is a valid index (i.e., `k_test` $\\le M$), we check the sum up to this point. The sum of propensities in the interval $(k_{curr}, k_{test}]$ is exactly $T[k_{test}]$, because `k_curr` is constructed as a sum of powers of two all larger than $p$, implying $\\operatorname{lsb}(k_{test}) = p$. Therefore, the prefix sum $A(k_{test})$ is `sum_curr` $+ T[k_{test}]$.\n- If this sum `sum_curr` $+ T[k_{test}]$ is less than $r$, it means the target index $k$ must be greater than `k_test`. We thus accept this step: we update $k_{curr} \\leftarrow k_{test}$ and $sum_{curr} \\leftarrow sum_{curr} + T[k_{test}]$.\n- If the sum is greater than or equal to $r$, the target index is in the interval $(k_{curr}, k_{test}]$. We do not take the step, and continue the search within this smaller range by trying the next smaller power of two.\n\nAfter iterating through all powers of two down to $1$, $k_{curr}$ will hold the largest index such that $A(k_{curr})  r$. The desired result, the smallest index $k$ with $A(k) \\ge r$, is therefore $k_{curr} + 1$.\n\nThis procedure probes at most one $T[i]$ entry for each bit position from $\\lfloor \\log_2 M \\rfloor$ down to $0$. The number of bit positions is $\\lfloor \\log_2 M \\rfloor + 1$. Thus, the total number of probes is $O(\\log M)$.\n\n**Task 3: Worst-Case Complexity Bounds**\n\n- **Prefix-sum query $A(j)$**: The number of entries of $T$ visited is equal to the number of set bits in the binary representation of $j$, commonly denoted as the Hamming weight $w(j)$. The worst case corresponds to the integer $j \\in [1, M]$ with the maximum number of set bits. The number of bits in any $j \\le M$ is at most $\\lfloor \\log_2 M \\rfloor + 1$. This theoretical maximum is achieved for some $j$ if $M$ allows for a number of the form $2^k-1$ to exist in the range. The tight worst-case bound on visited entries is $\\max_{1 \\leq j \\leq M} w(j)$, which is upper-bounded by and can be equal to $\\lfloor \\log_2 M \\rfloor + 1$.\n\n- **Single-point update at index $j$**: The number of visited entries is the length of the path $j, j+\\operatorname{lsb}(j), \\dots$ until the index exceeds $M$. The position of the least significant bit strictly increases with each step. Since there are at most $\\lfloor \\log_2 M \\rfloor + 1$ bit positions for an index up to $M$, the path length is bounded by this value. This bound is tight, as for an update at $j=1$, the path is $1, 2, 4, 8, \\dots, 2^{\\lfloor\\log_2 M\\rfloor}$, which has length $\\lfloor \\log_2 M \\rfloor + 1$.\n\n- **Selection routine from Task 2**: The algorithm iterates from $p=2^{\\lfloor \\log_2 M \\rfloor}$ down to $p=1$. This is a total of $\\lfloor \\log_2 M \\rfloor + 1$ iterations. In the worst case, a probe of one $T$ entry is made in each iteration. Thus, the tight worst-case bound on the number of visited entries is $\\lfloor \\log_2 M \\rfloor + 1$.\n\n**Task 4: Numerical Calculation for $M = 10^6$**\n\nWe must find the exact worst-case number of comparisons of the form \"is a probed partial sum $ r$?\" for the selection routine with $M = 10^6$.\n\nThe algorithm iterates through bit positions $i$ from $p_{max} = \\lfloor \\log_2(10^6) \\rfloor$ down to $0$.\nFirst, we compute $p_{max}$:\n$$ \\log_2(10^6) = 6 \\log_2(10) \\approx 6 \\times 3.321928 = 19.931568 $$\nThus, $p_{max} = 19$. The loop runs for $i = 19, 18, \\dots, 0$, which is $19 - 0 + 1 = 20$ iterations.\n\nIn each iteration, the comparison `sum_curr` $+ T[k_{test}]  r$ is performed if and only if the condition $k_{test} \\le M$ holds, where $k_{test} = k_{curr} + 2^i$. We need to find the maximum possible number of times this comparison is executed. The number of executions is maximized when the condition $k_{curr} + 2^i \\le M$ is satisfied as many times as possible. This occurs when $k_{curr}$ remains as small as possible throughout the loop. The minimum value for $k_{curr}$ is its initial value, $0$. This scenario corresponds to a very small target value $r$ (e.g., $r=0$), such that `sum_curr` $+ T[k_{test}]  r$ is always false (given $a_i0$, $T[i]0$).\n\nIf $k_{curr}$ remains $0$, the condition becomes $2^i \\le M = 10^6$. We must check for which $i \\in [0, 19]$ this holds.\nFor $i=19$, $2^{19} = 524288 \\le 10^6$. The condition holds.\nFor all $i  19$, $2^i  2^{19}$, so the condition $2^i \\le 10^6$ also holds.\nTherefore, the condition $k_{curr} + 2^i \\le M$ is satisfied for all $20$ iterations of the loop.\nThis means that in the worst case, the comparison involving the partial sum is performed $20$ times.", "answer": "$$ \\boxed{20} $$", "id": "2782362"}]}