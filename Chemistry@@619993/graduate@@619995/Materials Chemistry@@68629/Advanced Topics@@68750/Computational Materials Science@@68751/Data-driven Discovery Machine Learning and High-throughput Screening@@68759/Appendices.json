{"hands_on_practices": [{"introduction": "The search for novel materials often begins with exploring vast, continuous composition spaces, like the ternary simplex for three-component alloys. This exercise tackles the foundational challenge of translating this continuous space into a discrete set of experimental or computational targets. By deriving the number of compositions for a given resolution and analyzing the resulting interpolation error, you will learn how to design a high-throughput screening plan that is both comprehensive and computationally efficient [@problem_id:2479781].", "problem": "In a high-throughput screening campaign for a ternary alloy system, you plan to discretize the composition simplex to construct a data-driven surrogate for a smooth property field. Let the ternary composition simplex be defined as $S = \\{(x_1,x_2,x_3) \\in \\mathbb{R}^3 : x_i \\ge 0,\\ \\sum_{i=1}^{3} x_i = 1\\}$, and parametrize $S$ by $(x_1,x_2) \\in \\mathbb{R}^2$ with $x_3 = 1 - x_1 - x_2$. Consider a barycentric grid of resolution $m \\in \\mathbb{N}$ consisting of all compositions with barycentric coordinates $\\left(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m}\\right)$ where $i,j,k \\in \\mathbb{Z}_{\\ge 0}$ and $i+j+k = m$. The grid induces a standard piecewise-linear triangulation in the $(x_1,x_2)$-plane comprised of congruent right isosceles triangles whose legs have length $\\frac{1}{m}$.\n\nAssume the target property is a real-valued function $f:S \\to \\mathbb{R}$ that is twice continuously differentiable on $S$, and its Hessian matrix (with respect to $(x_1,x_2)$) has uniform operator norm bound $\\|\\nabla^2 f(x)\\|_2 \\le M$ for all $x \\in S$, where $M$ is a known constant. You will approximate $f$ by the piecewise-linear interpolant built from the values at the grid nodes.\n\nPerform the following, starting from first principles:\n\n1) Using a combinatorial argument that counts the number of nonnegative integer solutions to $i+j+k=m$, derive a closed-form expression for the number of unique grid compositions $N(m)$.\n\n2) Using Taylor’s theorem with the integral or Lagrange form of the remainder in multiple dimensions and the geometry of the right isosceles triangles in the mesh, obtain a sufficient bound for the worst-case pointwise interpolation error in terms of $M$, $m$, and the triangle edge lengths. From this, derive a sufficient condition on $m$ to guarantee that the uniform interpolation error is at most a prescribed tolerance $\\varepsilon  0$.\n\n3) For $M = 500$ and $\\varepsilon = 3.0 \\times 10^{-3}$, compute the smallest integer resolution $m$ that satisfies your sufficient condition, and then compute $N(m)$.\n\nExpress your final answer as the exact integer value of $N(m)$ corresponding to the minimal $m$ you found. Do not include units and do not round; report the exact integer.", "solution": "The problem presents three tasks concerning the numerical approximation of a property function on a ternary composition simplex. We will address each task in sequence, beginning from fundamental principles.\n\nFirst, we must determine the number of grid points, $N(m)$, for a given resolution $m$. The grid points are defined by barycentric coordinates $(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m})$ where $i, j, k$ are non-negative integers such that $i+j+k=m$. The number of such unique compositions, $N(m)$, is therefore equivalent to the number of non-negative integer solutions to the equation $i+j+k=m$. This is a classic combinatorial problem that can be solved using the \"stars and bars\" method. We are distributing $m$ identical items (stars) into $3$ distinct bins (indexed by $i, j, k$). This is equivalent to arranging $m$ stars and $3-1=2$ bars in a sequence. The total number of positions in the sequence is $m+2$. The number of distinct arrangements is the number of ways to choose the $2$ positions for the bars from the $m+2$ available positions. This is given by the binomial coefficient $\\binom{m+2}{2}$.\nTherefore, the number of grid points is:\n$$N(m) = \\binom{m+2}{2} = \\frac{(m+2)!}{2!(m+2-2)!} = \\frac{(m+2)(m+1)}{2}$$\n\nSecond, we must derive a sufficient condition on the resolution $m$ to ensure the uniform interpolation error does not exceed a tolerance $\\varepsilon$. The approximation is a piecewise-linear interpolant $L(x)$ of the function $f(x)$ over a triangulation of the domain. The error of this interpolation is determined by the smoothness of the function $f$ and the size of the triangles in the mesh.\nLet $x=(x_1, x_2)$ be the coordinates in the plane. The function is $f(x)$. The error is $E(x) = f(x) - L(x)$. According to the problem statement, $f$ is twice continuously differentiable ($C^2$), and its Hessian matrix $\\nabla^2 f(x)$ has a uniform operator norm bound $\\|\\nabla^2 f(x)\\|_2 \\le M$.\nFor a single triangle $T$ in the mesh, a standard result from the theory of finite element approximation, derivable from Taylor's theorem, provides an upper bound on the pointwise interpolation error. For a linear interpolant on a triangle $T$, this bound is given by:\n$$\\max_{x \\in T} |f(x) - L(x)| \\le C h_T^2 \\max_{y \\in T} \\|\\nabla^2 f(y)\\|_2$$\nwhere $h_T$ is the diameter of the triangle (the length of its longest side) and $C$ is a constant that depends on the geometry of the triangle. For a general convex domain, a widely used constant is $C = \\frac{1}{8}$, which originates from the one-dimensional case.\nThe problem states that the grid induces a triangulation by congruent right isosceles triangles whose legs have length $h = \\frac{1}{m}$. The diameter $h_T$ of such a triangle is the length of its hypotenuse:\n$$h_T = \\sqrt{h^2 + h^2} = \\sqrt{2h^2} = h\\sqrt{2} = \\frac{\\sqrt{2}}{m}$$\nSubstituting this into the error formula along with the given bound on the Hessian norm, we obtain a bound for the error within a single triangle:\n$$\\max_{x \\in T} |f(x) - L(x)| \\le \\frac{1}{8} h_T^2 M = \\frac{1}{8} \\left(\\frac{\\sqrt{2}}{m}\\right)^2 M = \\frac{1}{8} \\left(\\frac{2}{m^2}\\right) M = \\frac{M}{4m^2}$$\nSince all triangles in the mesh are congruent, this bound is uniform over the entire domain $S$. The uniform interpolation error is $\\sup_{x \\in S} |f(x) - L(x)|$. We can therefore state the sufficient condition that this error is at most $\\varepsilon$:\n$$\\frac{M}{4m^2} \\le \\varepsilon$$\nSolving for $m$, we find the required condition on the resolution:\n$$m^2 \\ge \\frac{M}{4\\varepsilon} \\implies m \\ge \\sqrt{\\frac{M}{4\\varepsilon}} = \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}}$$\nSince $m$ must be an integer, the smallest integer resolution that guarantees the desired accuracy is $m_{min} = \\left\\lceil \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}} \\right\\rceil$.\n\nThird, we compute the specific minimal integer resolution $m$ and the corresponding number of grid points $N(m)$ for the given values $M=500$ and $\\varepsilon = 3.0 \\times 10^{-3}$.\nUsing the condition derived above:\n$$m \\ge \\frac{1}{2}\\sqrt{\\frac{500}{3.0 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5 \\times 10^2}{3 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5}{3} \\times 10^5}$$\nTo evaluate this numerically:\n$$m \\ge \\frac{1}{2}\\sqrt{166666.66...} \\approx \\frac{1}{2}(408.248...) \\approx 204.124$$\nSince $m$ must be an integer, the smallest integer value for $m$ that satisfies this condition is $m = 205$.\n\nNow, we compute the number of unique grid compositions $N(m)$ for this resolution using the formula derived in the first part:\n$$N(m) = \\frac{(m+1)(m+2)}{2}$$\nSubstituting $m=205$:\n$$N(205) = \\frac{(205+1)(205+2)}{2} = \\frac{206 \\times 207}{2} = 103 \\times 207 = 21321$$\nThus, a minimum of $21321$ compositions must be screened to ensure the interpolation error is below the specified tolerance.", "answer": "$$\\boxed{21321}$$", "id": "2479781"}, {"introduction": "A machine learning model with high accuracy on a random test set may still fail at the true goal of materials discovery: predicting properties for genuinely new compounds. This practice demonstrates how to design more rigorous evaluation protocols, such as leave-one-element-out and leave-one-prototype-out splits, that specifically test a model's ability to extrapolate beyond its training data. Mastering these out-of-distribution evaluation techniques is essential for building models that can make truly novel and reliable predictions [@problem_id:2479777].", "problem": "A materials informatics team is building a supervised regression model to predict formation energy per atom for inorganic crystalline compounds. Each sample is represented by an input vector $x$ that includes a composition descriptor, an element-wise embedding, and a crystal structure prototype label. The target is $y \\in \\mathbb{R}$. Let the data-generating distribution over inputs and targets be $P(X,Y)$, and let $f_{\\theta}$ be a model trained by Empirical Risk Minimization (ERM) under the Independent and Identically Distributed (i.i.d.) assumption using an absolute error loss $\\ell(\\hat{y},y) = |\\hat{y}-y|$.\n\nDefine the empirical risk on a sample set $S$ of size $|S|$ as\n$$\n\\hat{R}_{S}(f) = \\frac{1}{|S|} \\sum_{(x_i,y_i)\\in S} \\ell\\big(f(x_i),y_i\\big),\n$$\nand the population risk under a distribution $Q$ as\n$$\nR_{Q}(f) = \\mathbb{E}_{(X,Y)\\sim Q}\\big[\\ell\\big(f(X),Y\\big)\\big].\n$$\nIn random train/test splits drawn from the same pool of compounds, the i.i.d. assumption implies $P_{\\text{train}}(X,Y) \\approx P_{\\text{test}}(X,Y)$, so that $R_{P_{\\text{test}}}(f)$ is approximated by $\\hat{R}_{\\text{test}}(f)$ for sufficiently large test sets.\n\nHowever, the team ultimately cares about out-of-distribution extrapolation to (i) compositions containing an element not seen in training and (ii) structures belonging to a prototype not seen in training. Let $\\mathcal{E}(x)$ denote the set of chemical elements present in $x$, and let $\\mathrm{proto}(x) \\in \\mathcal{P}$ denote the crystal prototype label of $x$. The team suspects that low error under random splits partly arises from target leakage through shared elements and prototypes across train and test, allowing the model to memorize element- or prototype-specific correlations.\n\nWhich option specifies evaluation splits and accompanying rationale that correctly test extrapolation to new elements and new prototypes and explains how these splits can reveal failure modes that random splits may hide?\n\nA. Define leave-one-element-out (LOEO) evaluation by, for each element $e$ in the periodic table subset observed in the dataset, setting the test fold to $\\mathcal{D}^{\\text{test}}_{e} = \\{(x,y): e \\in \\mathcal{E}(x)\\}$ and the training fold to its complement, and compute metrics such as mean absolute error $\\mathrm{MAE} = \\hat{R}_{\\text{test}}(f)$. Define leave-one-prototype-out (LOPO) by, for each prototype $p \\in \\mathcal{P}$, setting the test fold to $\\mathcal{D}^{\\text{test}}_{p} = \\{(x,y): \\mathrm{proto}(x)=p\\}$ and training on its complement. As a random-split baseline, first group by exact reduced formula or structure identifier and keep groups intact to avoid near-duplicate leakage across splits. If the model relies on element- or prototype-specific correlations learned under $P_{\\text{train}}$, then under LOEO and LOPO the test distribution $P_{\\text{test}}$ has support disjoint from $P_{\\text{train}}$ along the element or prototype dimension, so $R_{P_{\\text{test}}}(f)$ can increase markedly compared to grouped random splits, revealing failure modes (for example, out-of-vocabulary element embeddings or prototype memorization) that random i.i.d. splits may hide.\n\nB. Use $k$-fold random splits with importance weighting to upweight rare elements in the loss, that is minimize $\\sum_{(x,y)\\in S} w(x)\\,\\ell(f(x),y)$ with $w(x) \\propto 1/\\pi(\\mathcal{E}(x))$ where $\\pi$ is the empirical element frequency. Because rare elements are emphasized, performance under this scheme tests extrapolation to unseen elements; similarly, prototype rarity weighting tests extrapolation to unseen prototypes.\n\nC. Define leave-one-element-out by keeping all compounds containing a chosen element $e$ in both train and test, but at test time masking the entry of $e$ in the element-wise embedding so the model does not see it. Because the model must infer without the masked element channel, this tests extrapolation to a novel element. For prototypes, mask the prototype feature at test time while keeping all prototypes in training and testing.\n\nD. For prototypes, perform stratified $k$-fold cross-validation such that each fold’s training and test sets maintain the same prototype distribution $\\mathrm{proto}(x)$. This ensures fairness and tests prototype extrapolation because the proportions of prototypes are held constant across splits. For elements, perform a random split, since prototype stratification already controls structural correlations.\n\nSelect the single best option.", "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\n- **Task:** Build a supervised regression model $f_{\\theta}$ to predict formation energy per atom, $y \\in \\mathbb{R}$, for inorganic crystalline compounds.\n- **Input Features:** An input vector $x$ includes a composition descriptor, an element-wise embedding, and a crystal structure prototype label $\\mathrm{proto}(x) \\in \\mathcal{P}$.\n- **Data Distribution:** A data-generating distribution $P(X,Y)$.\n- **Training:** Empirical Risk Minimization (ERM) on a training set drawn i.i.d. from $P(X,Y)$.\n- **Loss Function:** Absolute error, $\\ell(\\hat{y},y) = |\\hat{y}-y|$.\n- **Empirical Risk:** On a set $S$, $\\hat{R}_{S}(f) = \\frac{1}{|S|} \\sum_{(x_i,y_i)\\in S} \\ell\\big(f(x_i),y_i\\big)$.\n- **Population Risk:** Under a distribution $Q$, $R_{Q}(f) = \\mathbb{E}_{(X,Y)\\sim Q}\\big[\\ell\\big(f(X),Y\\big)\\big]$.\n- **Standard Evaluation:** For random splits, $P_{\\text{train}}(X,Y) \\approx P_{\\text{test}}(X,Y)$, and $\\hat{R}_{\\text{test}}(f)$ approximates $R_{P_{\\text{test}}}(f)$.\n- **Primary Goal:** Evaluate out-of-distribution (OOD) extrapolation performance to:\n    1.  Compositions containing an element not seen during training. Let $\\mathcal{E}(x)$ be the set of elements in $x$.\n    2.  Structures belonging to a prototype not seen during training.\n- **Hypothesis:** Low error on random splits may be due to the model memorizing element-specific or prototype-specific correlations that are present in both the training and testing sets (target leakage).\n- **Question:** Identify the evaluation strategy that correctly tests for these extrapolation capabilities and reveals failure modes that random splits might hide.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the established criteria:\n- **Scientifically Grounded:** The problem is a canonical and critical task in materials informatics. Predicting formation energy, using the specified features, and training regression models are all standard practices. The distinction between in-distribution (i.i.d.) and out-of-distribution (OOD) generalization is a fundamental and advanced topic in machine learning, applied here to a realistic scientific context.\n- **Well-Posed:** The problem is clearly articulated. It defines the goal (testing extrapolation), the context (materials property prediction), and the specific OOD challenges (new elements, new prototypes). It requests an evaluation methodology, for which a correct, standard solution exists in the machine learning literature.\n- **Objective:** The language is formal and unambiguous. All terms like ERM, risk, and i.i.d. are standard in statistical learning theory. The problem is free of subjective claims.\n\nThe problem exhibits none of the invalidity flags. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. It poses a meaningful and non-trivial question about robust model evaluation in a scientific domain.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation\nThe core of the problem lies in the distinction between interpolation and extrapolation in a machine learning context.\nStandard cross-validation using random splits on an Independent and Identically Distributed (i.i.d.) dataset tests a model's ability to interpolate. In this setting, the training distribution $P_{\\text{train}}$ and test distribution $P_{\\text{test}}$ are assumed to be the same ($P_{\\text{train}} = P_{\\text{test}} = P$). A test sample $(x, y)$ is new, but its features (e.g., the elements and structure prototype in $x$) are highly likely to have been observed in other samples within the training set. The model's task is essentially to predict a value for a new combination of familiar components.\n\nExtrapolation, by contrast, requires the model to generalize to inputs that are systematically different from the training data. This constitutes an out-of-distribution (OOD) challenge, where $P_{\\text{test}} \\neq P_{\\text{train}}$. The problem specifies two such OOD challenges:\n1.  **Generalization to a new element:** The model must predict the formation energy of a compound containing an element that was entirely absent from the training set.\n2.  **Generalization to a new prototype:** The model must predict the formation energy of a compound whose crystal structure prototype was entirely absent from the training set.\n\nTo design an evaluation that tests these capabilities, one must construct train-test splits where the test set contains instances of a specific element (or prototype) while the training set is guaranteed to contain zero instances of that same element (or prototype). This forces a distributional shift, $P_{\\text{test}} \\neq P_{\\text{train}}$, along the feature dimension of interest.\n\nThe standard methodology for this is **leave-one-group-out cross-validation**.\n- **For elements:** This is called leave-one-element-out (LOEO) cross-validation. For each element $e$ present in the full dataset, a fold is created where the test set comprises all compounds containing element $e$, and the training set comprises all compounds that do *not* contain element $e$. The model is trained on the latter and evaluated on the former. A high error in this setting, compared to a random-split evaluation, indicates that the model heavily relies on having seen the element $e$ during training and cannot generalize its knowledge of the periodic table to infer the behavior of $e$.\n- **For prototypes:** This is called leave-one-prototype-out (LOPO) cross-validation. The logic is identical. For each prototype $p$, the test set consists of all structures with that prototype, and the training set contains none. A high error indicates the model has merely memorized a correlation between the label $p$ and the target value, rather than learning an underlying structure-property relationship from the geometric or topological features that define the prototype.\n\nA proper baseline for comparison is a robust i.i.d. evaluation. A simple random split can be susceptible to \"near-duplicate\" leakage (e.g., very similar compositions ending up in train and test). A superior baseline is a **grouped random split**, where data is first grouped by compound or structure, and all samples belonging to a group are kept together in either the train or test split. This prevents the model from being tested on trivial variations of its training data.\n\nBy comparing the performance on LOEO/LOPO splits to the grouped random split, one can quantify the model's extrapolation capability. A small performance drop indicates good generalization, while a large drop signals a failure to extrapolate.\n\n### Option-by-Option Analysis\n- **A. Define leave-one-element-out (LOEO) evaluation...**\n  This option precisely describes the correct leave-one-group-out methodology. For elements, it defines the LOEO test fold $\\mathcal{D}^{\\text{test}}_{e} = \\{(x,y): e \\in \\mathcal{E}(x)\\}$ and trains on its complement. For prototypes, it defines the LOPO test fold $\\mathcal{D}^{\\text{test}}_{p} = \\{(x,y): \\mathrm{proto}(x)=p\\}$ and trains on its complement. It correctly proposes using a grouped random split as a strong baseline to avoid near-duplicate leakage. The rationale is also perfectly sound: it states that these splits create a disjoint support between $P_{\\text{train}}$ and $P_{\\text{test}}$ along the dimension of interest, and that a large increase in error ($R_{P_{\\text{test}}}(f)$) reveals extrapolation failures like out-of-vocabulary element embeddings or prototype memorization. This aligns exactly with the derivation.\n  **Verdict: Correct.**\n\n- **B. Use $k$-fold random splits with importance weighting...**\n  This option proposes modifying the training objective to upweight rare elements by minimizing $\\sum w(x)\\,\\ell(f(x),y)$. This technique addresses the problem of imbalanced data within an i.i.d. framework. It helps the model learn better from the few examples of rare elements it *does* see, but it does not test extrapolation to elements it has *never* seen. The training set still contains all elements, so the task is not an OOD task in the sense required by the problem statement. The option confuses learning from sparse data with extrapolating to novel data categories.\n  **Verdict: Incorrect.**\n\n- **C. Define leave-one-element-out by keeping all compounds containing a chosen element $e$ in both train and test...**\n  This option is fundamentally flawed. Including test data in the training set constitutes severe data leakage and invalidates any resulting performance metric. A model could achieve zero error simply by memorizing the test set examples. Furthermore, masking features at test time evaluates the model's robustness to missing data, which is a different task from extrapolating to entirely new, unseen feature values (e.g., a new element). This approach does not correctly test for extrapolation as defined in the problem.\n  **Verdict: Incorrect.**\n\n- **D. For prototypes, perform stratified $k$-fold cross-validation...**\n  This option proposes the exact opposite of what is required. Stratified cross-validation ensures that the distribution of a feature (in this case, prototypes) is the *same* in the train and test splits. This is a technique to *enforce* the i.i.d. assumption and obtain a stable estimate of the model's interpolation performance. It is explicitly designed to *prevent* the kind of distributional shift needed to test extrapolation. The rationale provided is therefore completely backward. Using random splits for elements likewise only tests i.i.d. performance.\n  **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2479777"}, {"introduction": "Beyond developing predictive models, data-driven discovery is an engineering challenge of maximizing scientific output with finite resources. This exercise shifts focus to the operational efficiency of a high-throughput screening workflow on a computing cluster. By modeling factors like job bundling, queueing delays, and memory limits, you will apply principles from operations research to determine a strategy that maximizes the number of materials evaluated per day, a crucial step in turning a computational campaign into a successful discovery engine [@problem_id:2479750].", "problem": "A research group is running a high-throughput screening workflow for candidate inorganic crystal structures using surrogate machine learning interatomic potentials on a High-Performance Computing (HPC) cluster. Each submitted job evaluates a bundle of structures sequentially using a shared in-memory model cache to reduce repeated input/output. The cluster has $N$ identical nodes, each node processes one job at a time, and a node remains idle during the scheduler startup latency before the job begins. The following conditions hold.\n\n- Each structure evaluation takes an average wall time of $t$ hours per attempt on a node. Each attempt results in a success with probability $(1 - r)$ and an independent retry is required with probability $r$, with retries occurring in later jobs. Assume attempt outcomes are independent across structures and attempts.\n- Each job experiences a queueing/startup delay $D$ before it begins compute on a node. The queueing delay $D$ is independent and identically distributed across jobs with a $\\text{Gamma}(k,\\theta)$ distribution, where $k$ is the shape and $\\theta$ is the scale, so that $\\mathbb{E}[D] = k\\theta$.\n- Bundling strategy: a single job bundles $b$ structures that are evaluated sequentially within the job. This amortizes the queueing delay across the $b$ attempts contained in that job, but does not change compute time per attempt $t$.\n- Memory constraint: To enable an in-job model cache, the job’s peak memory footprint is $m_{0} + b\\,m$, where $m_{0}$ is the baseline memory for the cache and $m$ is the incremental memory per bundled structure’s metadata. Each node has a hard memory limit $M_{\\text{node}}$. Therefore, the bundle size must satisfy $m_{0} + b\\,m \\leq M_{\\text{node}}$, with $b$ a positive integer.\n\nAssume an effectively infinite backlog of structures is available, so that the system operates in steady state. Using first-principles reasoning based on renewal-reward arguments and basic properties of the geometric and Gamma distributions, derive the expected throughput in structures per day as a function of $b$, and then determine the optimal integer $b^{\\star}$ that maximizes the expected throughput subject to the memory constraint. Finally, evaluate the maximum expected throughput numerically for the following parameters and report the result.\n\n- $N = 128$ nodes,\n- $t = 1.5$ hours,\n- $r = 0.1$,\n- $D \\sim \\text{Gamma}(k,\\theta)$ with $k = 2$ and $\\theta = 0.5$ hours,\n- $M_{\\text{node}} = 128$ GiB,\n- $m_{0} = 8$ GiB,\n- $m = 2.5$ GiB.\n\nExpress the final throughput in structures per day. Round your answer to four significant figures. Use units of structures per day in your reasoning, but the final reported number should be unitless in the answer box.", "solution": "The problem statement will first be validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe following data and definitions are provided:\n- Number of identical HPC nodes: $N$\n- Average wall time per structure evaluation attempt: $t$\n- Probability of a retry for a single attempt: $r$\n- Probability of success for a single attempt: $1 - r$\n- Job queueing/startup delay: a random variable $D \\sim \\text{Gamma}(k, \\theta)$\n- Expected queueing delay: $\\mathbb{E}[D] = k\\theta$\n- Number of structures bundled in a single job: $b$ (a positive integer)\n- Job peak memory footprint: $m_{0} + b\\,m$\n- Baseline memory for the cache: $m_{0}$\n- Incremental memory per bundled structure: $m$\n- Hard memory limit per node: $M_{\\text{node}}$\n- Memory constraint: $m_{0} + b\\,m \\leq M_{\\text{node}}$\n- Numerical values:\n    - $N = 128$\n    - $t = 1.5$ hours\n    - $r = 0.1$\n    - $k = 2$\n    - $\\theta = 0.5$ hours\n    - $M_{\\text{node}} = 128$ GiB\n    - $m_{0} = 8$ GiB\n    - $m = 2.5$ GiB\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is a well-defined optimization task within the domain of operations research and computational science.\n- **Scientifically Grounded**: The problem is based on established principles of probability theory (Gamma and geometric/Bernoulli distributions), queueing theory, and the renewal-reward theorem. The application context—high-throughput screening on an HPC cluster—is a realistic and common scenario in modern materials chemistry.\n- **Well-Posed**: The problem asks for the maximization of a well-defined objective function (throughput) subject to clear constraints (memory limit). The setup allows for the derivation of a unique optimal integer solution for the bundle size, $b^{\\star}$, and a corresponding maximum throughput.\n- **Objective**: The language is precise and quantitative. All parameters are explicitly defined. There are no subjective or ambiguous statements.\n- **Self-Contained and Consistent**: All necessary information is provided. The concepts of job bundling, queueing delays, probabilistic success, and memory constraints are logically consistent and sufficient for formulating a solution. The assumption of an infinite backlog justifies the steady-state analysis.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is scientifically sound, well-posed, and objective. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe objective is to determine the optimal integer bundle size $b^{\\star}$ that maximizes the total expected throughput of successfully evaluated structures, and then to calculate this maximum throughput. The total throughput is the sum of the throughputs of the $N$ identical, independent nodes. We begin by analyzing the process for a single node.\n\nThe operation of a single node can be modeled as a renewal-reward process. A \"renewal cycle\" corresponds to the processing of a single job, from the moment it is submitted to the queue until its computation is complete.\n\nThe duration of one cycle, $T_{\\text{cycle}}$, is the sum of the queueing delay $D$ and the job's computation time.\nThe queueing delay $D$ is a random variable following a Gamma distribution with shape $k$ and scale $\\theta$. The expected queueing delay is given by:\n$$\n\\mathbb{E}[D] = k\\theta\n$$\nA job bundles $b$ structure evaluation attempts. Each attempt takes time $t$. The computation time for the entire job is therefore deterministic (or its expected value is) and equal to $b \\cdot t$.\nThe expected duration of a single cycle is:\n$$\n\\mathbb{E}[T_{\\text{cycle}}] = \\mathbb{E}[D] + b \\cdot t = k\\theta + bt\n$$\nThe \"reward\" for each cycle is the number of successfully completed structure evaluations. Each of the $b$ attempts in a job is an independent Bernoulli trial with a success probability of $(1-r)$. The number of successes in a job, $S$, thus follows a binomial distribution $\\text{Bin}(b, 1-r)$. The expected number of successes (the expected reward) per cycle is:\n$$\n\\mathbb{E}[S] = b(1-r)\n$$\nAccording to the elementary renewal-reward theorem, the long-run average throughput for a single node, expressed in structures per hour, is the ratio of the expected reward per cycle to the expected duration of a cycle. Let this be $\\mathcal{T}_{\\text{node}}(b)$.\n$$\n\\mathcal{T}_{\\text{node}}(b) = \\frac{\\mathbb{E}[S]}{\\mathbb{E}[T_{\\text{cycle}}]} = \\frac{b(1-r)}{k\\theta + bt}\n$$\nThe total throughput for the entire cluster of $N$ nodes, $\\mathcal{T}_{\\text{total}}(b)$, is $N$ times the single-node throughput. We are asked for the throughput in units of structures per day, so we must multiply by $24$ hours/day.\n$$\n\\mathcal{T}_{\\text{total}}(b) = 24 \\cdot N \\cdot \\mathcal{T}_{\\text{node}}(b) = 24N \\frac{b(1-r)}{k\\theta + bt}\n$$\nTo find the optimal bundle size $b^{\\star}$, we must maximize $\\mathcal{T}_{\\text{total}}(b)$ with respect to $b$. The terms $24$, $N$, and $(1-r)$ are positive constants, so maximizing $\\mathcal{T}_{\\text{total}}(b)$ is equivalent to maximizing the function $f(b)$:\n$$\nf(b) = \\frac{b}{k\\theta + bt}\n$$\nWe can analyze the behavior of $f(b)$ by examining its derivative with respect to $b$, treating $b$ as a continuous variable for the moment. Let $A = k\\theta$ and $B = t$.\n$$\nf(b) = \\frac{b}{A + Bb}\n$$\n$$\n\\frac{df}{db} = \\frac{(1)(A + Bb) - b(B)}{(A + Bb)^2} = \\frac{A + Bb - Bb}{(A + Bb)^2} = \\frac{A}{(A + Bb)^2} = \\frac{k\\theta}{(k\\theta + bt)^2}\n$$\nSince $k0$, $\\theta0$, $b0$, and $t0$, the derivative $\\frac{df}{db}$ is strictly positive for all valid $b$. This demonstrates that $\\mathcal{T}_{\\text{total}}(b)$ is a monotonically increasing function of $b$. Therefore, to maximize the throughput, we must choose the largest possible integer value for $b$ that satisfies the given constraints.\n\nThe only constraint on $b$ is the node memory limit:\n$$\nm_{0} + bm \\leq M_{\\text{node}}\n$$\nSolving for $b$:\n$$\nbm \\leq M_{\\text{node}} - m_{0}\n$$\n$$\nb \\leq \\frac{M_{\\text{node}} - m_{0}}{m}\n$$\nSince $b$ must be an integer, the optimal bundle size $b^{\\star}$ is the largest integer that satisfies this inequality. This is given by the floor function:\n$$\nb^{\\star} = \\left\\lfloor \\frac{M_{\\text{node}} - m_{0}}{m} \\right\\rfloor\n$$\nNow we substitute the given numerical values to find $b^{\\star}$ and the corresponding maximum throughput.\n\n**Numerical Calculation**\n\nFirst, calculate the optimal bundle size $b^{\\star}$:\n- $M_{\\text{node}} = 128$ GiB\n- $m_{0} = 8$ GiB\n- $m = 2.5$ GiB\n\n$$\nb^{\\star} = \\left\\lfloor \\frac{128 - 8}{2.5} \\right\\rfloor = \\left\\lfloor \\frac{120}{2.5} \\right\\rfloor = \\left\\lfloor 48 \\right\\rfloor = 48\n$$\nThe optimal bundle size is $b^{\\star} = 48$.\n\nNext, we calculate the maximum expected throughput, $\\mathcal{T}_{\\text{total}}(b^{\\star})$, using $b=48$ and the other parameters:\n- $N = 128$\n- $r = 0.1 \\implies 1-r = 0.9$\n- $k = 2$\n- $\\theta = 0.5$ hours $\\implies k\\theta = 2 \\cdot 0.5 = 1.0$ hour\n- $t = 1.5$ hours\n\n$$\n\\mathcal{T}_{\\text{total}}(48) = 24 \\cdot 128 \\cdot \\frac{48 \\cdot (1 - 0.1)}{2 \\cdot 0.5 + 48 \\cdot 1.5}\n$$\n$$\n\\mathcal{T}_{\\text{total}}(48) = 24 \\cdot 128 \\cdot \\frac{48 \\cdot 0.9}{1.0 + 72.0}\n$$\n$$\n\\mathcal{T}_{\\text{total}}(48) = 3072 \\cdot \\frac{43.2}{73}\n$$\n$$\n\\mathcal{T}_{\\text{total}}(48) = \\frac{132710.4}{73} \\approx 1817.95068\\dots\n$$\nThe problem requires the result to be rounded to four significant figures.\n$$\n\\mathcal{T}_{\\text{total}}(48) \\approx 1818\n$$\nThe maximum expected throughput is approximately $1818$ structures per day.", "answer": "$$\\boxed{1818}$$", "id": "2479750"}]}