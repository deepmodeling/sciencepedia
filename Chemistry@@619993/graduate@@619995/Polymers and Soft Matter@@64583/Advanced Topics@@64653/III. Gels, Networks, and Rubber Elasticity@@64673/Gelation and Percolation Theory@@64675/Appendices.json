{"hands_on_practices": [{"introduction": "This first exercise grounds us in the classical theory of gelation developed by Flory and Stockmayer. By treating polymer network formation as a random branching process, we can derive a powerful criterion for the gel point—the critical extent of reaction at which an infinite network first appears. This practice challenges you to apply this mean-field approach to a mixture of monomers with different functionalities, a foundational skill for predicting the behavior of real polymerizing systems [@problem_id:2916994].", "problem": "Consider an ideal step-growth network-forming polycondensation in which a large ensemble of monomers each carries an integer number of identical, homofunctional reactive groups that react pairwise with one another. The system is a binary mixture of monomers of functionality $f=3$ (trifunctional) and $f=2$ (difunctional). Let $x$ denote the initial mole fraction (number fraction) of trifunctional monomers, so that the fraction of difunctional monomers is $1 - x$. Assume the following ideal conditions: (i) all functional groups are equally reactive at all times, (ii) intramolecular cyclization is negligible in the pre-gel regime so that clusters are locally tree-like, and (iii) the reaction proceeds randomly among all unreacted functional groups. Let $p$ be the extent of reaction, defined as the fraction of all functional groups that have reacted.\n\nUsing first principles of branching processes and percolation theory applied to this ideal random network, derive the gelation criterion in terms of the mean number of new reacted bonds generated when a reacted bond is followed into a new monomer. Then, for the binary mixture specified above, compute the critical extent of reaction $p_c$ at which the gel first appears as an explicit function of $x$. Express your final result as a single simplified analytic expression in terms of $x$. No rounding is required, and no units are to be reported for the final answer.", "solution": "The problem asks for the derivation of the gelation criterion for an ideal step-growth polymerization of a binary mixture of monomers with functionality $f=3$ and $f=2$. The final goal is to express the critical extent of reaction for gelation, $p_c$, as a function of the initial mole fraction, $x$, of the trifunctional monomers.\n\nThe problem statement has been validated and is found to be scientifically grounded, well-posed, objective, and internally consistent. It represents a standard problem in the statistical mechanics of polymers based on the Flory-Stockmayer theory of gelation, which is a mean-field application of percolation theory to a chemical reaction network. The given assumptions—equal reactivity of functional groups, absence of intramolecular cyclization, and random reaction—are the standard ideal conditions for this model. We may proceed with the solution.\n\nThe onset of gelation corresponds to the formation of an infinite, space-spanning polymer network. In the language of branching processes, this is the point at which the probability of a branching cascade continuing indefinitely becomes non-zero. The criterion for this transition is that the branching parameter, $\\alpha$, must be equal to or greater than $1$. The branching parameter $\\alpha$ is defined as the average number of new reacted bonds one encounters by following a randomly chosen reacted bond to the next monomer and then branching out from that monomer. Gelation occurs at the critical point where $\\alpha = 1$.\n\nLet us formulate $\\alpha$ for the given system. We have a binary mixture of monomers:\n1.  Trifunctional monomers with functionality $f_1=3$ and mole fraction $x$.\n2.  Difunctional monomers with functionality $f_2=2$ and mole fraction $1-x$.\n\nThe extent of reaction, $p$, is the probability that any randomly chosen functional group has reacted.\n\nTo calculate $\\alpha$, we perform a two-step statistical average. First, we must determine the probability of arriving at a monomer of a certain functionality by following a randomly chosen chemical bond. This probability is not the mole fraction of the monomer, but rather the fraction of all functional groups that belong to that type of monomer.\n\nThe total number of functional groups in the system is proportional to the number-average functionality, $\\langle f \\rangle$.\n$$ \\langle f \\rangle = \\sum_{i} x_i f_i = x \\cdot 3 + (1-x) \\cdot 2 = 3x + 2 - 2x = x + 2 $$\nThe fraction of functional groups belonging to trifunctional monomers is:\n$$ P(f=3)_{\\text{group}} = \\frac{x \\cdot f_1}{\\langle f \\rangle} = \\frac{3x}{x+2} $$\nThe fraction of functional groups belonging to difunctional monomers is:\n$$ P(f=2)_{\\text{group}} = \\frac{(1-x) \\cdot f_2}{\\langle f \\rangle} = \\frac{2(1-x)}{x+2} $$\nNote that $P(f=3)_{\\text{group}} + P(f=2)_{\\text{group}} = \\frac{3x + 2 - 2x}{x+2} = \\frac{x+2}{x+2} = 1$, as required.\n\nNow, consider a randomly chosen reacted bond and follow it to a monomer unit. The probability that this monomer is trifunctional is $P(f=3)_{\\text{group}}$, and the probability that it is difunctional is $P(f=2)_{\\text{group}}$. Once we arrive at this monomer, one of its functional groups is occupied by the bond we followed. A monomer of functionality $f$ has $f-1$ other \"outgoing\" functional groups available for forming further branches.\n- For a trifunctional monomer ($f=3$), there are $3-1 = 2$ outgoing groups.\n- For a difunctional monomer ($f=2$), there is $2-1 = 1$ outgoing group.\n\nThe average number of outgoing functional groups from a monomer reached by traversing a bond is the group-weighted average of $(f-1)$:\n$$ \\langle f-1 \\rangle_{\\text{group-weighted}} = \\sum_{i} P(f=f_i)_{\\text{group}} \\cdot (f_i - 1) $$\n$$ \\langle f-1 \\rangle_{\\text{group-weighted}} = \\left(\\frac{3x}{x+2}\\right)(3-1) + \\left(\\frac{2(1-x)}{x+2}\\right)(2-1) $$\n$$ \\langle f-1 \\rangle_{\\text{group-weighted}} = \\frac{3x \\cdot 2}{x+2} + \\frac{2(1-x) \\cdot 1}{x+2} $$\n$$ \\langle f-1 \\rangle_{\\text{group-weighted}} = \\frac{6x + 2 - 2x}{x+2} = \\frac{4x+2}{x+2} $$\nThis quantity represents the average number of potential pathways leading away from the monomer.\n\nThe branching parameter $\\alpha$ is the average number of these outgoing pathways that are actually connected to other monomers. Since the probability that any given functional group has reacted is $p$, the average number of new reacted bonds is the product of $p$ and the average number of outgoing groups.\n$$ \\alpha = p \\cdot \\langle f-1 \\rangle_{\\text{group-weighted}} = p \\left(\\frac{4x+2}{x+2}\\right) $$\nThe critical condition for gelation is $\\alpha = 1$. Let $p_c$ be the critical extent of reaction at the gel point.\n$$ 1 = p_c \\left(\\frac{4x+2}{x+2}\\right) $$\nSolving for $p_c$ yields the final expression for the gel point:\n$$ p_c = \\frac{x+2}{4x+2} $$\nThis is the desired result. It expresses the critical extent of reaction as a function of the initial composition of the monomer mixture. The expression correctly recovers the known limits: for a purely difunctional system ($x=0$), $p_c = \\frac{2}{2} = 1$, indicating that gelation (network formation) does not occur, although the weight-average molar mass diverges. For a purely trifunctional system ($x=1$), $p_c = \\frac{1+2}{4+2} = \\frac{3}{6} = \\frac{1}{2}$, which is the classic Flory-Stockmayer result for $f=3$ monomers, $p_c = 1/(f-1)$.", "answer": "$$ \\boxed{\\frac{x+2}{4x+2}} $$", "id": "2916994"}, {"introduction": "Moving from mean-field theory to a more explicit spatial model, this practice immerses you in the world of computational percolation theory. You will implement a simulation to construct a random network on a square lattice and identify its critical structural features. This exercise focuses on computing the \"backbone\" of the spanning cluster, which represents the load-bearing or conductive pathways, by implementing a leaf-stripping algorithm to find the graph's 2-core [@problem_id:2916990].", "problem": "Consider independent bond percolation on a finite square lattice. Let $L \\in \\mathbb{N}$ be the linear system size. Define the underlying deterministic grid graph $G_L = (V,E)$ with open boundaries, where $V = \\{(i,j) \\mid i \\in \\{0,\\dots,L-1\\}, j \\in \\{0,\\dots,L-1\\}\\}$ and $E$ contains an undirected edge between $(i,j)$ and $(i,j+1)$ for all $j \\in \\{0,\\dots,L-2\\}$, and between $(i,j)$ and $(i+1,j)$ for all $i \\in \\{0,\\dots,L-2\\}$. For a bond occupation probability $p \\in [0,1]$ and a pseudo-random seed $s \\in \\mathbb{N}$, form a random subgraph $G = (V, E_{\\mathrm{open}})$ by retaining each edge in $E$ independently with probability $p$, using a pseudo-random number generator initialized by $s$.\n\nDefine the top boundary set $T = \\{(0,j) \\mid j \\in \\{0,\\dots,L-1\\}\\}$ and the bottom boundary set $B = \\{(L-1,j) \\mid j \\in \\{0,\\dots,L-1\\}\\}$. A connected component $C$ of $G$ is called spanning if it intersects both $T$ and $B$. If no such $C$ exists, define the spanning backbone length $\\ell$ to be $0$.\n\nOtherwise, among all spanning components, select $C^\\star$ by the following deterministic rule to avoid ambiguity: maximize $|V(C)|$; in case of ties, maximize $|E(C)|$; if ties remain, minimize the smallest vertex index under the indexing map $\\varphi: V \\to \\{0,\\dots,L^2-1\\}$ given by $\\varphi(i,j) = iL + j$.\n\nFor the selected $C^\\star$, define the induced subgraph $H^\\star$ with vertex set $V^\\star = V(C^\\star)$ and all open edges in $E_{\\mathrm{open}}$ whose endpoints both lie in $V^\\star$. Define the degree $d_{H^\\star}(v)$ of a vertex $v \\in V^\\star$ as the number of neighbors of $v$ in $H^\\star$. The leaf-stripping (also known as iterative pruning) procedure constructs the $2$-core of $H^\\star$ by repeatedly removing all vertices $v$ with $d_{H^\\star}(v) < 2$ and their incident edges, updating degrees after each removal, until no such vertices remain. Let the resulting pruned subgraph be $K^\\star$. Define the spanning backbone length $\\ell$ as the number of undirected edges in $K^\\star$.\n\nYour task is to implement a program that, for each given parameter triple $(L,p,s)$, generates $G$, finds $C^\\star$ if it exists, computes the $2$-core $K^\\star$ by leaf-stripping, and outputs the integer $\\ell$ as defined above. If no spanning component exists, output the integer $0$ for that case.\n\nAlgorithmic constraints and foundations to be used and justified in your design:\n- Use only the definitions of graph connectivity, degree, induced subgraph, and the iterative pruning rule based on degree threshold $2$ to obtain the $2$-core, which removes all dangling trees that cannot lie on any simple path between $T$ and $B$.\n- To find connected components, you may use Breadth-First Search (BFS), which systematically explores vertices by non-decreasing distance in terms of number of edges.\n- The percolation is independent and identically distributed across edges as per the specified $p$, and the pseudo-random generator seeded by $s$ ensures reproducibility.\n\nTest suite:\nProvide outputs for the following parameter tuples $(L,p,s)$:\n- Case $1$: $(L,p,s) = (2, 1.0, 0)$.\n- Case $2$: $(L,p,s) = (3, 1.0, 0)$.\n- Case $3$: $(L,p,s) = (30, 0.4, 7)$.\n- Case $4$: $(L,p,s) = (50, 0.6, 12345)$.\n- Case $5$: $(L,p,s) = (50, 0.55, 999)$.\n\nAll outputs are integers. No physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$) in the same order as the cases above.", "solution": "The problem as stated is valid. It is scientifically and mathematically well-posed, grounded in the established principles of percolation theory, a branch of statistical mechanics and probability theory. The problem is self-contained, its definitions are precise, and the deterministic procedure for generating the random graph and for tie-breaking ensures that a unique, verifiable solution exists for each set of parameters. We will proceed to construct the solution.\n\nThe problem requires the computation of the size of the \"backbone\" of a spanning cluster in a bond percolation model on a finite $L \\times L$ square lattice. The backbone is formally defined as the $2$-core of the spanning cluster. The procedure is as follows:\n\nFirst, we construct the specific realization of the random graph $G = (V, E_{\\mathrm{open}})$ for a given system size $L$, bond occupation probability $p$, and pseudo-random seed $s$. The set of vertices $V$ corresponds to the sites of the square lattice, which we index for computational convenience by a single integer $v \\in \\{0, \\dots, L^2-1\\}$ via the mapping $\\varphi(i,j) = iL + j$ for a site at row $i$ and column $j$, where $i, j \\in \\{0, \\dots, L-1\\}$. The underlying grid graph $G_L$ has edges between adjacent vertices. Each edge is included in the set of open edges $E_{\\mathrm{open}}$ independently with probability $p$. To ensure reproducibility, a pseudo-random number generator is initialized with the seed $s$. We generate the adjacency list representation of $G$ by iterating through each potential edge of the grid, drawing one pseudo-random number for each, and adding the edge to the list if the number is less than $p$.\n\nSecond, we must identify all connected components of the graph $G$. This is a standard graph traversal problem, for which the Breadth-First Search (BFS) algorithm is a suitable and efficient method. We maintain a boolean array to track visited vertices. Iterating through all vertices, if an unvisited vertex is found, a new BFS is initiated from it to find all vertices belonging to its component.\n\nThird, for each component found, we determine if it is a \"spanning\" component. A component $C$ is defined as spanning if its vertex set $V(C)$ has a non-empty intersection with both the top boundary set $T = \\{v \\mid 0 \\le v < L\\}$ and the bottom boundary set $B = \\{v \\mid L(L-1) \\le v < L^2\\}$. If no such spanning component exists, the problem defines the backbone length $\\ell$ to be $0$, and the procedure for that case terminates.\n\nFourth, if one or more spanning components exist, we must select a unique component $C^\\star$ according to the specified deterministic rule. This rule is a lexicographical comparison based on a hierarchy of criteria:\n$1$. The component with the maximum number of vertices, $|V(C)|$.\n$2$. Among those tied for maximum $|V(C)|$, the one with the maximum number of edges, $|E(C)|$.\n$3$. If a tie persists, the component with the minimum value of its smallest vertex index is chosen.\nThis procedure guarantees a unique $C^\\star$.\n\nFifth, we compute the $2$-core of $C^\\star$, denoted $K^\\star$. The $2$-core of a graph is the largest induced subgraph with a minimum degree of at least $2$. It is found using an iterative pruning algorithm, also known as leaf-stripping. The algorithm proceeds as follows:\n$1$. We first compute the degrees $d_{C^\\star}(v)$ for all vertices $v \\in V(C^\\star)$. The degree is the number of neighbors of $v$ that are also in $V(C^\\star)$.\n$2$. All vertices with degree $d_{C^\\star}(v) < 2$ are identified and placed in a queue. These vertices, by definition, cannot be part of any cycle and are thus considered \"leaves\" of the structure.\n$3$. We iteratively process the queue. When a vertex $u$ is removed from the queue, it is pruned from the graph. For each neighbor $v$ of the pruned vertex $u$, its degree is decremented. If the degree of a neighbor $v$ drops to $1$, it has become a new leaf and is added to the queue.\n$4$. This process continues until the queue is empty. The set of vertices that have not been pruned constitutes the vertex set of the $2$-core, $V(K^\\star)$. Physically, this core represents the resilient part of the cluster, containing all cyclic paths, which are essential for robust transport.\n\nFinally, the desired spanning backbone length, $\\ell$, is the number of edges in the resulting $2$-core, $K^\\star$. This is computed by summing the degrees of the vertices within the core and dividing by two, $\\ell = \\frac{1}{2} \\sum_{v \\in V(K^\\star)} d_{K^\\star}(v)$, or by directly counting the edges whose endpoints both lie in $V(K^\\star)$. This completes the required calculation.", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Solves the percolation backbone problem for a given set of test cases.\n    \"\"\"\n    test_cases = [\n        (2, 1.0, 0),\n        (3, 1.0, 0),\n        (30, 0.4, 7),\n        (50, 0.6, 12345),\n        (50, 0.55, 999),\n    ]\n\n    results = []\n    for L, p, s in test_cases:\n        result = calculate_backbone_length(L, p, s)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_backbone_length(L, p, s):\n    \"\"\"\n    Calculates the spanning backbone length for a given (L, p, s) triple.\n    \"\"\"\n    if L < 2: # Spanning is not possible for L=1\n        return 0\n\n    N = L * L\n    rng = np.random.default_rng(s)\n    \n    # 1. Generate the random graph G = (V, E_open)\n    adj = {i: [] for i in range(N)}\n    for i in range(L):\n        for j in range(L):\n            u = i * L + j\n            # Edge to the right neighbor\n            if j < L - 1:\n                v = i * L + (j + 1)\n                if rng.random() < p:\n                    adj[u].append(v)\n                    adj[v].append(u)\n            # Edge to the bottom neighbor\n            if i < L - 1:\n                v = (i + 1) * L + j\n                if rng.random() < p:\n                    adj[u].append(v)\n                    adj[v].append(u)\n\n    # 2. Find all connected components and identify spanning ones\n    visited = np.zeros(N, dtype=bool)\n    spanning_components = []\n    for i in range(N):\n        if not visited[i]:\n            component_nodes = set()\n            q = deque([i])\n            visited[i] = True\n            is_top_connected = False\n            is_bottom_connected = False\n            min_v_idx = i\n\n            # BFS to find one component\n            while q:\n                u = q.popleft()\n                component_nodes.add(u)\n                min_v_idx = min(min_v_idx, u)\n                \n                if u < L:\n                    is_top_connected = True\n                if u >= L * (L - 1):\n                    is_bottom_connected = True\n\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n            \n            if is_top_connected and is_bottom_connected:\n                num_vertices = len(component_nodes)\n                num_edges = sum(len(adj[u]) for u in component_nodes) // 2\n                spanning_components.append((-num_vertices, -num_edges, min_v_idx, component_nodes))\n\n    # 3. Select the primary spanning component C_star\n    if not spanning_components:\n        return 0\n\n    spanning_components.sort()\n    C_star_nodes = spanning_components[0][3]\n\n    # 4. Compute the 2-core of C_star using leaf-stripping\n    \n    # Create subgraph representation for C_star\n    subgraph_adj = {u: [v for v in adj[u] if v in C_star_nodes] for u in C_star_nodes}\n    degrees = {u: len(v) for u, v in subgraph_adj.items()}\n\n    # Queue for leaf-stripping\n    q = deque([u for u, d in degrees.items() if d < 2])\n    \n    while q:\n        u = q.popleft()\n        \n        # u is pruned. Update its neighbors' degrees.\n        for v in subgraph_adj[u]:\n            if v in degrees:  # Check if neighbor has not been pruned yet\n                degrees[v] -= 1\n                if degrees[v] == 1:\n                    q.append(v)\n        \n        # Remove u from the graph\n        del degrees[u]\n\n    core_nodes = set(degrees.keys())\n\n    # 5. Calculate the backbone length (number of edges in the 2-core)\n    if not core_nodes:\n        return 0\n\n    backbone_len = sum(1 for u in core_nodes for v in subgraph_adj[u] if v in core_nodes and v > u)\n    \n    return backbone_len\n\nsolve()\n```", "id": "2916990"}, {"introduction": "Large-scale simulations, like the one in the previous exercise, depend on highly efficient algorithms. This final practice delves into the computational heart of percolation studies by examining the Hoshen–Kopelman algorithm for cluster identification. Understanding its single-pass methodology and impressive memory efficiency is crucial for performing state-of-the-art computational research on disordered systems and critical phenomena [@problem_id:2917012].", "problem": "In site percolation on a hypercubic lattice in $d$ spatial dimensions with linear extent $L$, one must identify all connected clusters of occupied sites, where connectivity is defined by nearest-neighbor adjacency. A widely used approach is to scan the lattice in a fixed lexicographic order and assign integer labels to occupied sites so that sites in the same connected component receive the same final label. Equivalences between provisional labels can be resolved using a disjoint-set (also known as union–find) data structure with near-constant-time merges and finds. Assume the cost model in which inspecting a bounded number of already-visited neighbors per site is $O(1)$, and union–find operations are amortized $O(\\alpha(N))$ with path compression and union by rank, where $\\alpha(\\cdot)$ is the inverse Ackermann function and $N=L^d$ is the number of lattice sites. In implementing the scan, one can choose to store labels only on the immediately preceding hyperplane orthogonal to the scan direction, together with an auxiliary structure that maintains equivalence classes among currently active labels. \n\nWhich option best describes the Hoshen–Kopelman algorithm under these assumptions and correctly specifies its asymptotic memory and time complexity as a function of $L$ and $d$ for a system of size $L^d$?\n\nA. The algorithm performs a single lexicographic pass over the lattice. For each occupied site, it inspects only already-visited neighbors (those in the negative coordinate directions), assigns a new label if isolated or adopts/merges existing neighbor labels via a union–find structure, recording label equivalences. It maintains only the labels on the current and immediately preceding hyperplanes and recycles labels for clusters that no longer intersect the scan front. The auxiliary equivalence structure stores only active labels. The total memory scales as $O(L^{d-1})$, and the total time is $O(L^d\\,\\alpha(L^d))$, which is effectively linear in $L^d$.\n\nB. The algorithm stores a label for every site and repeatedly propagates and updates labels throughout the entire lattice until no changes occur, requiring multiple global sweeps. The memory scales as $O(L^d)$ and the time scales as $O(L^{2d})$ due to iterative relaxation.\n\nC. The algorithm uses union–find to resolve equivalences but, because the number of provisional labels scales with the total number of occupied sites, its memory is necessarily $O(L^d)$ even if labels are recycled along the scan. The time is $O(L^d)$.\n\nD. The algorithm grows only the incipient spanning cluster from a seed by exploring its boundary, visiting no other occupied sites. Its time scales as $O(L^{d_f})$, where $d_f$ is the fractal dimension of the spanning cluster, and its memory is $O(1)$ aside from the growth queue.", "solution": "The problem statement is valid. It presents a clear, self-contained, and scientifically sound question regarding the standard Hoshen-Kopelman algorithm for cluster labeling in site percolation. The assumptions regarding the hypercubic lattice, union–find data structure complexity, and the cost model are standard in computational physics and algorithm analysis. I will now proceed with a first-principles derivation of the algorithm's properties and its complexity.\n\nThe Hoshen–Kopelman (HK) algorithm is designed to identify and label all connected components (clusters) in a grid-like structure, such as a lattice, in a single pass.\n\n**Algorithmic Principles**\n1.  **Lattice Scan:** The algorithm processes each of the $N=L^d$ sites of the lattice exactly once in a predetermined lexicographic order. For example, in $d=2$ dimensions, this corresponds to scanning row by row, and within each row, column by column.\n\n2.  **Neighbor Inspection:** For each occupied site being visited, the algorithm inspects its already-visited neighbors. In a lexicographic scan, these are the neighbors whose coordinates are lexicographically smaller (e.g., in \"past\" rows or columns). The number of such neighbors is bounded by a constant, specifically at most $d$.\n\n3.  **Labeling and Equivalence:**\n    *   If an occupied site has no occupied, already-visited neighbors, it is considered the beginning of a new cluster and is assigned a new, unique provisional label.\n    *   If it has one or more occupied, already-visited neighbors, the current site belongs to the same cluster(s) as these neighbors. The current site is assigned one of their labels (e.g., the smallest). If the neighbors themselves carry different provisional labels, this signifies that clusters previously thought to be separate are, in fact, connected. An equivalence between these labels is recorded.\n\n4.  **Disjoint-Set (Union–Find) Data Structure:** The equivalences between provisional labels are managed by a union–find data structure. When an equivalence between labels $l_1$ and $l_2$ is discovered, a `union($l_1,l_2$)` operation is performed. The `find(l)` operation is used to determine the single representative (root) label for any provisional label $l$. This structure, when implemented with path compression and union by rank/size, resolves equivalence chains with nearly constant amortized time complexity.\n\n**Complexity Analysis**\n\n**Time Complexity:**\nThe total time is the sum of the time for scanning the lattice and the time for all union–find operations.\n*   The algorithm visits each of the $N=L^d$ sites once.\n*   At each site, it performs a constant number of checks (occupation status, neighbor inspection). The problem statement specifies this cost is $O(1)$.\n*   For each occupied site, a small, constant number of `find` and `union` operations are performed (at most $d$ `find`s and $d-1$ `union`s in the worst case for a single site).\n*   The total number of union–find operations across the entire scan is therefore proportional to the total number of sites, $N$.\n*   The amortized cost of each union–find operation is given as $O(\\alpha(N))$, where $\\alpha(\\cdot)$ is the inverse Ackermann function and the size of the set in the union-find structure can be at most $N$.\n*   Thus, the total time complexity $T$ is:\n$$T(L, d) = \\sum_{i=1}^{L^d} (\\text{scan cost}_i + \\text{union-find cost}_i) = O(L^d \\cdot 1) + O(L^d \\cdot \\alpha(L^d)) = O(L^d\\,\\alpha(L^d))$$\nSince $\\alpha(N)$ is a very slowly growing function, the runtime is effectively, but not strictly, linear in the number of sites $N = L^d$.\n\n**Memory Complexity:**\nA naive implementation would store a label for every site, requiring $O(L^d)$ memory. However, the HK algorithm allows for a crucial optimization, which is described in the problem statement.\n*   During the scan, to determine the label for the current site, one only needs the labels of its already-visited neighbors. For a lexicographic scan, these neighbors lie on the boundary separating the visited and unvisited regions of the lattice.\n*   Specifically, if the scan proceeds along the $x_1$ direction, then $x_2$, and so on, the required labels are on the $(d-1)$-dimensional hyperplane immediately preceding the current site in the primary scan direction, and within the current hyperplane being written. The total number of sites whose labels must be kept in memory at any one time is therefore proportional to the size of such a hyperplane.\n*   The size of a $(d-1)$-dimensional hyperplane in a $d$-dimensional lattice of linear extent $L$ is $L^{d-1}$. Therefore, the memory to store the necessary labels scales as $O(L^{d-1})$.\n*   The union–find data structure must maintain the equivalence classes. With the memory optimization, old labels for clusters that no longer intersect the scanning front can be recycled. The number of active provisional labels at any time is bounded by the number of distinct clusters crossing the scanning front, which is at most on the order of the front's size, $O(L^{d-1})$.\n*   Therefore, the auxiliary memory for the union–find structure also scales as $O(L^{d-1})$.\n*   The total memory complexity is dominated by these two components, yielding a memory usage of $O(L^{d-1})$.\n\n**Evaluation of Options**\n\n*   **A. The algorithm performs a single lexicographic pass over the lattice. For each occupied site, it inspects only already-visited neighbors (those in the negative coordinate directions), assigns a new label if isolated or adopts/merges existing neighbor labels via a union–find structure, recording label equivalences. It maintains only the labels on the current and immediately preceding hyperplanes and recycles labels for clusters that no longer intersect the scan front. The auxiliary equivalence structure stores only active labels. The total memory scales as $O(L^{d-1})$, and the total time is $O(L^d\\,\\alpha(L^d))$, which is effectively linear in $L^d$.**\n    This option provides a complete and accurate description of the Hoshen–Kopelman algorithm with its standard memory optimization. The stated time complexity of $O(L^d\\,\\alpha(L^d))$ and memory complexity of $O(L^{d-1})$ are both correct as per the derivation above.\n    **Verdict: Correct.**\n\n*   **B. The algorithm stores a label for every site and repeatedly propagates and updates labels throughout the entire lattice until no changes occur, requiring multiple global sweeps. The memory scales as $O(L^d)$ and the time scales as $O(L^{2d})$ due to iterative relaxation.**\n    This describes an iterative relaxation method, not the single-pass Hoshen–Kopelman algorithm. The fundamental algorithmic description (\"repeatedly propagates ... until no changes occur\") is incorrect. Consequently, the stated complexities do not apply to HK.\n    **Verdict: Incorrect.**\n\n*   **C. The algorithm uses union–find to resolve equivalences but, because the number of provisional labels scales with the total number of occupied sites, its memory is necessarily $O(L^d)$ even if labels are recycled along the scan. The time is $O(L^d)$.**\n    This option correctly identifies the use of union–find, but its reasoning on memory complexity is flawed. The statement that memory is \"necessarily $O(L^d)$\" because the total number of provisional labels could be large fundamentally misunderstands the memory optimization. As shown, by recycling labels, the number of *active* labels at any given time is $O(L^{d-1})$, which dictates the memory requirement. The time complexity is also given as $O(L^d)$, which is a common simplification but omits the more precise $\\alpha(L^d)$ factor.\n    **Verdict: Incorrect.**\n\n*   **D. The algorithm grows only the incipient spanning cluster from a seed by exploring its boundary, visiting no other occupied sites. Its time scales as $O(L^{d_f})$, where $d_f$ is the fractal dimension of the spanning cluster, and its memory is $O(1)$ aside from the growth queue.**\n    This describes a different class of algorithms, such as the Leath–Alexandrowicz algorithm, which are used to study the properties of a *single* cluster, typically at the percolation threshold. The Hoshen–Kopelman algorithm's purpose is to find *all* clusters across the entire lattice, not just one. The method and complexity scaling are characteristic of single-cluster growth, not a full lattice scan.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2917012"}]}