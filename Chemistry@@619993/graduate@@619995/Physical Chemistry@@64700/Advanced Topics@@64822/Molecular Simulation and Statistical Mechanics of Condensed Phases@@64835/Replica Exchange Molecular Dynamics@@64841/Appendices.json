{"hands_on_practices": [{"introduction": "A key metric for the efficiency of a Replica Exchange Molecular Dynamics (REMD) simulation is the exchange acceptance probability between adjacent temperatures. A well-chosen temperature ladder ensures these probabilities are reasonably high and uniform, allowing for efficient diffusion of replicas through temperature space. This first exercise provides a quantitative foundation by asking you to estimate the expected acceptance probability using a Gaussian approximation for the energy distributions, a common and powerful approach for systems with large heat capacity. Mastering this calculation is a crucial first step in diagnosing and designing effective REMD simulations. [@problem_id:2666608]", "problem": "In Replica Exchange Molecular Dynamics (REMD), two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$ attempt a swap of temperatures with Metropolis acceptance probability given by $\\min\\!\\left(1,\\exp\\!\\left((\\beta_1-\\beta_2)\\,(E_1-E_2)\\right)\\right)$, where $E_1$ and $E_2$ are the instantaneous potential energies sampled independently from the canonical ensembles at temperatures $T_1$ and $T_2$. Consider two adjacent temperatures $T_1=300\\ \\mathrm{K}$ and $T_2=330\\ \\mathrm{K}$. Suppose the measured energy statistics at these temperatures (energies expressed per mole) are approximately Gaussian with means $\\mu_1=-500.0\\ \\mathrm{kJ\\,mol^{-1}}$ and $\\mu_2=-440.0\\ \\mathrm{kJ\\,mol^{-1}}$, and variances $\\sigma_1^2=1496.6033\\ \\mathrm{kJ^2\\,mol^{-2}}$ and $\\sigma_2^2=1810.8900\\ \\mathrm{kJ^2\\,mol^{-2}}$. Assume the two energy samples are independent and that the Gaussian approximation for the canonical energy distribution is valid.\n\nStarting only from the canonical ensemble definition, the Metropolis acceptance criterion for exchanges, and the independence and Gaussian assumptions stated above, derive an analytic expression for the expected swap acceptance probability under these conditions and evaluate it numerically. Use the gas constant $R=0.008314462618\\ \\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$ so that $\\beta_i = 1/(R T_i)$ is consistent with the per-mole energy units. Round your final expected acceptance probability to four significant figures. Express the final answer as a pure number (no units).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data and well-defined assumptions are provided. It represents a standard calculation in the analysis of Replica Exchange Molecular Dynamics simulations. Therefore, the problem is valid, and a solution will be provided.\n\nThe objective is to compute the expected value of the Metropolis acceptance probability, $\\langle P_{\\text{acc}} \\rangle$, for a swap between two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$. The acceptance probability is given by\n$$\nP_{\\text{acc}}(E_1, E_2) = \\min\\!\\left(1, \\exp\\!\\left((\\beta_1-\\beta_2)(E_1-E_2)\\right)\\right)\n$$\nwhere $E_1$ and $E_2$ are the potential energies of the replicas. These are random variables drawn independently from their respective canonical ensembles. The problem states that these energy distributions are approximated as Gaussian: $E_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $E_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$.\n\nLet us define the difference in inverse temperature $\\Delta\\beta = \\beta_1 - \\beta_2$ and the difference in energy $\\Delta E = E_1 - E_2$. The acceptance probability simplifies to $P_{\\text{acc}}(\\Delta E) = \\min(1, \\exp(\\Delta\\beta \\Delta E))$. Since $E_1$ and $E_2$ are independent Gaussian random variables, their difference, $\\Delta E$, is also a Gaussian random variable. Its distribution is $\\Delta E \\sim \\mathcal{N}(\\mu_{\\Delta}, \\sigma_{\\Delta}^2)$, with parameters:\nMean: $\\mu_{\\Delta} = \\mathbb{E}[E_1 - E_2] = \\mathbb{E}[E_1] - \\mathbb{E}[E_2] = \\mu_1 - \\mu_2$.\nVariance: $\\sigma_{\\Delta}^2 = \\text{Var}(E_1 - E_2) = \\text{Var}(E_1) + \\text{Var}(-E_2) = \\text{Var}(E_1) + (-1)^2\\text{Var}(E_2) = \\sigma_1^2 + \\sigma_2^2$.\n\nThe expected acceptance probability is found by integrating over the probability distribution of $\\Delta E$, which we denote by $p_{\\Delta}(x)$:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\mathbb{E}[P_{\\text{acc}}(\\Delta E)] = \\int_{-\\infty}^{\\infty} \\min(1, \\exp(\\Delta\\beta x)) p_{\\Delta}(x) dx\n$$\nwhere $p_{\\Delta}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}\\right)$.\n\nGiven $T_1 = 300\\ \\mathrm{K}$ and $T_2 = 330\\ \\mathrm{K}$, we have $T_1 < T_2$, which implies $\\beta_1 = 1/(RT_1) > 1/(RT_2) = \\beta_2$. Thus, $\\Delta\\beta > 0$. This allows us to split the integral based on the sign of the argument of the exponential, $x = \\Delta E$:\n- If $x > 0$, then $\\Delta\\beta x > 0$, so $\\exp(\\Delta\\beta x) > 1$, and $\\min(1, \\exp(\\Delta\\beta x)) = 1$.\n- If $x \\le 0$, then $\\Delta\\beta x \\le 0$, so $\\exp(\\Delta\\beta x) \\le 1$, and $\\min(1, \\exp(\\Delta\\beta x)) = \\exp(\\Delta\\beta x)$.\n\nThe expectation integral is therefore split into two parts:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\int_{0}^{\\infty} 1 \\cdot p_{\\Delta}(x) dx + \\int_{-\\infty}^{0} \\exp(\\Delta\\beta x) p_{\\Delta}(x) dx\n$$\nThe first integral is the probability that $\\Delta E > 0$, which can be expressed using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z \\exp(-t^2/2) dt$.\n$$\n\\int_{0}^{\\infty} p_{\\Delta}(x) dx = P(\\Delta E > 0) = 1 - P(\\Delta E \\le 0) = 1 - \\Phi\\left(\\frac{0 - \\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) = \\Phi\\left(\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right)\n$$\nThe second integral requires us to combine the exponential terms in the integrand:\n$$\n\\int_{-\\infty}^{0} \\exp(\\Delta\\beta x) \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}\\right) dx\n$$\nThe argument of the total exponential is $\\Delta\\beta x - \\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}$. We complete the square with respect to $x$:\n$$\n-\\frac{1}{2\\sigma_{\\Delta}^2} [x^2 - 2x\\mu_{\\Delta} + \\mu_{\\Delta}^2 - 2\\sigma_{\\Delta}^2\\Delta\\beta x] = -\\frac{1}{2\\sigma_{\\Delta}^2} [x^2 - 2x(\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta) + \\mu_{\\Delta}^2]\n$$\nLet $\\mu' = \\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta$. The term in the brackets is rewritten as:\n$$\n[x - \\mu']^2 - (\\mu')^2 + \\mu_{\\Delta}^2 = [x - \\mu']^2 - (\\mu_{\\Delta}^2 + 2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta + (\\sigma_{\\Delta}^2\\Delta\\beta)^2) + \\mu_{\\Delta}^2 = [x - \\mu']^2 - 2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta - \\sigma_{\\Delta}^4(\\Delta\\beta)^2\n$$\nThe exponent becomes:\n$$\n-\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2} + \\frac{2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta + \\sigma_{\\Delta}^4(\\Delta\\beta)^2}{2\\sigma_{\\Delta}^2} = -\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2} + \\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\n$$\nThe second integral is now:\n$$\n\\int_{-\\infty}^{0} \\exp\\left(\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2}\\right) dx\n$$\nThe constant exponential factor can be moved outside the integral. The remaining integral is the probability $P(X' \\le 0)$ for a Gaussian variable $X' \\sim \\mathcal{N}(\\mu', \\sigma_{\\Delta}^2)$.\n$$\nP(X' \\le 0) = \\Phi\\left(\\frac{0 - \\mu'}{\\sigma_{\\Delta}}\\right) = \\Phi\\left(-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}\\right)\n$$\nCombining all parts, the final analytic expression is:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\Phi\\left(\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) + \\exp\\left(\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\\right) \\Phi\\left(-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}\\right)\n$$\nNow, we substitute the given numerical values.\n$R = 0.008314462618\\ \\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$.\nTemperatures: $T_1 = 300\\ \\mathrm{K}$, $T_2 = 330\\ \\mathrm{K}$.\n$\\Delta\\beta = \\frac{1}{R}\\left(\\frac{1}{T_1} - \\frac{1}{T_2}\\right) = \\frac{1}{0.008314462618}\\left(\\frac{1}{300} - \\frac{1}{330}\\right) = \\frac{1}{0.008314462618}\\left(\\frac{30}{99000}\\right) \\approx 0.036446164\\ (\\mathrm{kJ\\,mol^{-1}})^{-1}$.\n\nEnergy statistics:\n$\\mu_1 = -500.0\\ \\mathrm{kJ\\,mol^{-1}}$, $\\mu_2 = -440.0\\ \\mathrm{kJ\\,mol^{-1}}$.\n$\\sigma_1^2 = 1496.6033\\ \\mathrm{kJ^2\\,mol^{-2}}$, $\\sigma_2^2 = 1810.8900\\ \\mathrm{kJ^2\\,mol^{-2}}$.\n\nParameters for the $\\Delta E$ distribution:\n$\\mu_{\\Delta} = \\mu_1 - \\mu_2 = -500.0 - (-440.0) = -60.0\\ \\mathrm{kJ\\,mol^{-1}}$.\n$\\sigma_{\\Delta}^2 = \\sigma_1^2 + \\sigma_2^2 = 1496.6033 + 1810.8900 = 3307.4933\\ \\mathrm{kJ^2\\,mol^{-2}}$.\n$\\sigma_{\\Delta} = \\sqrt{3307.4933} \\approx 57.5108085\\ \\mathrm{kJ\\,mol^{-1}}$.\n\nWe calculate the arguments for the functions in the analytic expression:\nArgument of the first $\\Phi$: $\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}} = \\frac{-60.0}{57.5108085} \\approx -1.043282$.\n$\\Phi(-1.043282) \\approx 0.14841$.\n\nExponent of the exponential term: $\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}$\nTerm 1: $\\mu_{\\Delta}\\Delta\\beta = -60.0 \\times 0.036446164 \\approx -2.186770$.\nTerm 2: $\\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2} = \\frac{3307.4933 \\times (0.036446164)^2}{2} \\approx \\frac{4.393433}{2} = 2.196717$.\nExponent = $-2.186770 + 2.196717 = 0.009947$.\n$\\exp(0.009947) \\approx 1.009996$.\n\nArgument of the second $\\Phi$: $-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}$\n$\\sigma_{\\Delta}^2\\Delta\\beta = 3307.4933 \\times 0.036446164 \\approx 120.5483$.\nArgument $= -\\frac{-60.0 + 120.5483}{57.5108085} = -\\frac{60.5483}{57.5108085} \\approx -1.052816$.\n$\\Phi(-1.052816) \\approx 0.14620$.\n\nFinally, we compute $\\langle P_{\\text{acc}} \\rangle$:\n$\\langle P_{\\text{acc}} \\rangle \\approx 0.14841 + (1.009996) \\times (0.14620) \\approx 0.14841 + 0.147661 \\approx 0.296071$.\n\nRounding to four significant figures, the result is $0.2961$.", "answer": "$$\n\\boxed{0.2961}\n$$", "id": "2666608"}, {"introduction": "While approximations are useful for designing simulations, ensuring the correctness of your simulation code is paramount. This practice introduces the fundamental concept of code verification by testing an implementation against an analytically solvable model. By using a system with a simple quadratic Hamiltonian, where potential energies follow an exact Gamma distribution, you can derive a closed-form expression for the average acceptance probability. [@problem_id:2666530] Comparing the output of a numerical simulation to this exact result provides a rigorous benchmark, building confidence in the correctness of the underlying algorithm before it is applied to more complex, real-world problems.", "problem": "Design a complete, runnable program that verifies a Replica Exchange Molecular Dynamics (REMD) acceptance kernel against an analytic benchmark for a system with a known density of states. Use a mathematically tractable model system: a classical quadratic Hamiltonian with $f$ quadratic configurational degrees of freedom, whose canonical potential-energy distribution at inverse temperature $\\beta$ is gamma with shape $k = f/2$ and scale $1/\\beta$. Specifically, the canonical density for the potential energy $U \\ge 0$ is\n$$\np_{\\beta}(U) = \\frac{\\beta^{k}}{\\Gamma(k)} U^{k - 1} e^{-\\beta U},\n$$\nwhere $k \\in \\{1,2,3,\\dots\\}$ is fixed for the system.\n\nConsider two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$ with $\\beta_1 > \\beta_2 > 0$. The Metropolis exchange acceptance factor for swapping their temperatures (with fixed configurations) is\n$$\nA(U_1, U_2) = \\min\\left(1, \\exp\\left[(\\beta_1 - \\beta_2)(U_1 - U_2)\\right]\\right),\n$$\nwhere $U_1 \\sim p_{\\beta_1}(\\cdot)$ and $U_2 \\sim p_{\\beta_2}(\\cdot)$ are independent random energies drawn from the above canonical densities. The ensemble-average acceptance probability is the double integral\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\int_0^\\infty \\int_0^\\infty p_{\\beta_1}(U_1)\\, p_{\\beta_2}(U_2)\\, \\min\\left(1, \\exp\\left[(\\beta_1 - \\beta_2)(U_1 - U_2)\\right]\\right)\\, dU_1\\, dU_2.\n$$\n\nTask: From first principles, derive a closed-form analytic expression for $\\mathcal{A}(k, \\beta_1, \\beta_2)$ as a function of $k$, $\\beta_1$, and $\\beta_2$. Then implement a program that:\n- Computes the analytic value $\\mathcal{A}(k, \\beta_1, \\beta_2)$.\n- Independently estimates the same quantity by Monte Carlo, by sampling $U_1$ and $U_2$ as independent gamma random variables with shape $k$ and scales $1/\\beta_1$ and $1/\\beta_2$ respectively, and averaging $A(U_1, U_2)$ over $n$ independent draws.\n- Uses the sample standard error to decide whether the Monte Carlo estimate agrees with the analytic value within statistical error, reporting a boolean for each test case defined below.\n\nYou must treat energies and temperatures as dimensionless (no physical units). Angles are not involved. Percentages must not be used; all proportions must be expressed as decimals.\n\nTest Suite:\nFor each test case, the parameters are $(k, \\beta_1, \\beta_2, n, \\text{seed})$:\n- Case $1$: $(k=\\;1,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;1.0,\\; n=\\;100000,\\; \\text{seed}=\\;42)$\n- Case $2$: $(k=\\;1,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.5,\\; n=\\;100000,\\; \\text{seed}=\\;43)$\n- Case $3$: $(k=\\;3,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.8,\\; n=\\;100000,\\; \\text{seed}=\\;44)$\n- Case $4$: $(k=\\;5,\\; \\beta_1=\\;3.0,\\; \\beta_2=\\;2.4,\\; n=\\;100000,\\; \\text{seed}=\\;45)$\n- Case $5$: $(k=\\;10,\\; \\beta_1=\\;2.0,\\; \\beta_2=\\;1.6,\\; n=\\;100000,\\; \\text{seed}=\\;46)$\n- Case $6$: $(k=\\;10,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.7,\\; n=\\;100000,\\; \\text{seed}=\\;47)$\n\nDecision Rule:\nFor each case, compute the Monte Carlo estimate $\\widehat{\\mathcal{A}}$ and its sample standard error $\\mathrm{SE} = s/\\sqrt{n}$, where $s$ is the unbiased sample standard deviation of the $n$ independent values of $A(U_1, U_2)$. Let $\\mathcal{A}_{\\mathrm{an}}$ denote the analytic value. Report $\\text{True}$ if\n$$\n\\left|\\widehat{\\mathcal{A}} - \\mathcal{A}_{\\mathrm{an}}\\right| \\le 4\\, \\mathrm{SE},\n$$\nand $\\text{False}$ otherwise.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., \"[$\\text{result}_1,\\text{result}_2,\\dots$]\"). Each $\\text{result}_i$ must be a boolean value $\\text{True}$ or $\\text{False}$ corresponding to the $i$-th test case, in the same order as listed above. The program must be fully self-contained and not require any user input or external files. Use the specified runtime environment.", "solution": "The problem is valid. It is scientifically grounded in the principles of statistical mechanics, specifically the canonical ensemble and the Metropolis-Hastings algorithm as applied to Replica Exchange Molecular Dynamics (REMD). The model system, a classical quadratic Hamiltonian, is a standard textbook example whose potential energy follows a Gamma distribution, a well-established result. The problem is well-posed, providing all necessary parameters and a clear objective: to derive a closed-form expression for the average exchange acceptance probability and verify it against a Monte Carlo simulation. The language is precise and objective. All conditions for a valid problem are met.\n\nThe objective is to derive an analytic expression for the ensemble-average acceptance probability $\\mathcal{A}(k, \\beta_1, \\beta_2)$ for an REMD temperature swap and to implement a program that verifies this expression via Monte Carlo simulation.\n\nThe average acceptance probability is given by the integral:\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\int_0^\\infty \\int_0^\\infty p_{\\beta_1}(U_1)\\, p_{\\beta_2}(U_2)\\, \\min\\left(1, e^{(\\beta_1 - \\beta_2)(U_1 - U_2)}\\right)\\, dU_1\\, dU_2\n$$\nwhere $U_1$ and $U_2$ are independent random potential energies drawn from the canonical probability densities $p_{\\beta_1}(\\cdot)$ and $p_{\\beta_2}(\\cdot)$ respectively. The density $p_{\\beta}(U)$ is given as a Gamma distribution with shape $k$ and scale $1/\\beta$:\n$$\np_{\\beta}(U) = \\frac{\\beta^{k}}{\\Gamma(k)} U^{k - 1} e^{-\\beta U}\n$$\nGiven the condition $\\beta_1 > \\beta_2$, the exponent's sign is determined by the sign of $U_1 - U_2$. We can split the integral into two domains:\n1.  If $U_1 > U_2$, then $(\\beta_1 - \\beta_2)(U_1 - U_2) > 0$, so $\\min(\\cdot) = 1$.\n2.  If $U_1 \\le U_2$, then $(\\beta_1 - \\beta_2)(U_1 - U_2) \\le 0$, so $\\min(\\cdot) = e^{(\\beta_1 - \\beta_2)(U_1 - U_2)}$.\n\nThis allows us to write $\\mathcal{A}$ as the sum of two integrals, $I_1$ and $I_2$:\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\underbrace{\\iint_{U_1 \\le U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) e^{(\\beta_1 - \\beta_2)(U_1 - U_2)} dU_1 dU_2}_{I_1} + \\underbrace{\\iint_{U_1 > U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) dU_1 dU_2}_{I_2}\n$$\nLet's analyze the integrand of $I_1$. We can substitute the explicit forms of the densities and the exponential term:\n$$\np_{\\beta_1}(U_1) p_{\\beta_2}(U_2) e^{(\\beta_1 - \\beta_2)(U_1 - U_2)} = \\left(\\frac{\\beta_1^k}{\\Gamma(k)} U_1^{k-1} e^{-\\beta_1 U_1}\\right) \\left(\\frac{\\beta_2^k}{\\Gamma(k)} U_2^{k-1} e^{-\\beta_2 U_2}\\right) e^{\\beta_1 U_1 - \\beta_2 U_1 - \\beta_1 U_2 + \\beta_2 U_2}\n$$\nCombining the exponential terms yields:\n$$\ne^{-\\beta_1 U_1 - \\beta_2 U_2 + \\beta_1 U_1 - \\beta_2 U_1 - \\beta_1 U_2 + \\beta_2 U_2} = e^{-\\beta_2 U_1 - \\beta_1 U_2}\n$$\nThe integrand of $I_1$ thus simplifies to:\n$$\n\\frac{\\beta_1^k \\beta_2^k}{\\Gamma(k)^2} U_1^{k-1} U_2^{k-1} e^{-\\beta_2 U_1 - \\beta_1 U_2} = \\left(\\frac{\\beta_2^k}{\\Gamma(k)} U_1^{k-1} e^{-\\beta_2 U_1}\\right) \\left(\\frac{\\beta_1^k}{\\Gamma(k)} U_2^{k-1} e^{-\\beta_1 U_2}\\right) = p_{\\beta_2}(U_1) p_{\\beta_1}(U_2)\n$$\nThis remarkable identity shows a swap in the temperature indices of the probability densities. The integral $I_1$ can now be interpreted probabilistically. Let $X \\sim p_{\\beta_1}(\\cdot)$ and $Y \\sim p_{\\beta_2}(\\cdot)$ be two independent random variables.\n$$\nI_1 = \\iint_{U_1 \\le U_2} p_{\\beta_2}(U_1) p_{\\beta_1}(U_2) dU_1 dU_2 = \\mathbb{P}(Y \\le X)\n$$\nThe integral $I_2$ is, by definition:\n$$\nI_2 = \\iint_{U_1 > U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) dU_1 dU_2 = \\mathbb{P}(X > Y)\n$$\nTherefore, the average acceptance probability is:\n$$\n\\mathcal{A} = \\mathbb{P}(Y \\le X) + \\mathbb{P}(X > Y)\n$$\nSince $X$ and $Y$ are continuous random variables, the probability of them being equal is zero, i.e., $\\mathbb{P}(X=Y)=0$. Thus, $\\mathbb{P}(Y \\le X) = \\mathbb{P}(Y < X) + \\mathbb{P}(Y=X) = \\mathbb{P}(Y < X)$, which is equivalent to $\\mathbb{P}(X > Y)$.\nThis simplifies the expression for $\\mathcal{A}$ to:\n$$\n\\mathcal{A} = \\mathbb{P}(X > Y) + \\mathbb{P}(X > Y) = 2\\, \\mathbb{P}(X > Y)\n$$\nTo obtain a closed-form expression, we must calculate $\\mathbb{P}(X > Y)$. Let $X \\sim \\text{Gamma}(k, \\theta_1)$ and $Y \\sim \\text{Gamma}(k, \\theta_2)$, where the scale parameters are $\\theta_1 = 1/\\beta_1$ and $\\theta_2 = 1/\\beta_2$.\nLet us define two new variables, $X' = X/\\theta_1$ and $Y' = Y/\\theta_2$. Both $X'$ and $Y'$ are independent random variables following the standard Gamma distribution, $\\text{Gamma}(k, 1)$.\nThe condition $X > Y$ is equivalent to $X' \\theta_1 > Y' \\theta_2$, or $\\frac{X'}{Y'} > \\frac{\\theta_2}{\\theta_1}$.\nNow, consider the variable $W = \\frac{X'}{X' + Y'}$. It is a standard result that if $X' \\sim \\text{Gamma}(k, 1)$ and $Y' \\sim \\text{Gamma}(k, 1)$, then $W$ follows a Beta distribution, $W \\sim \\text{Beta}(k, k)$.\nWe can express the ratio $X'/Y'$ in terms of $W$:\n$$\n\\frac{1}{W} = 1 + \\frac{Y'}{X'} \\implies \\frac{Y'}{X'} = \\frac{1-W}{W} \\implies \\frac{X'}{Y'} = \\frac{W}{1-W}\n$$\nThe inequality becomes $\\frac{W}{1-W} > \\frac{\\theta_2}{\\theta_1}$. Since $W \\in (0, 1)$, we can multiply by $1-W > 0$:\n$$\nW\\theta_1 > (1-W)\\theta_2 \\implies W(\\theta_1 + \\theta_2) > \\theta_2 \\implies W > \\frac{\\theta_2}{\\theta_1 + \\theta_2}\n$$\nSo, $\\mathbb{P}(X > Y) = \\mathbb{P}(W > \\frac{\\theta_2}{\\theta_1 + \\theta_2})$. The probability is the integral of the Beta PDF from this value to $1$. This is given by the complement of the cumulative distribution function (CDF), which is the regularized incomplete beta function, $I_x(a, b)$.\n$$\n\\mathbb{P}(X > Y) = 1 - I_{\\frac{\\theta_2}{\\theta_1 + \\theta_2}}(k, k)\n$$\nThe Beta distribution $\\text{Beta}(k, k)$ is symmetric around $1/2$. A property of the regularized incomplete beta function for symmetric parameters is $1 - I_x(k,k) = I_{1-x}(k,k)$.\nLet $x = \\frac{\\theta_2}{\\theta_1 + \\theta_2}$. Then $1-x = \\frac{\\theta_1}{\\theta_1 + \\theta_2}$.\nSubstituting the scale parameters $\\theta_1 = 1/\\beta_1$ and $\\theta_2 = 1/\\beta_2$:\n$$\n1-x = \\frac{1/\\beta_1}{1/\\beta_1 + 1/\\beta_2} = \\frac{1/\\beta_1}{(\\beta_1+\\beta_2)/(\\beta_1\\beta_2)} = \\frac{\\beta_2}{\\beta_1+\\beta_2}\n$$\nSo, $\\mathbb{P}(X>Y) = I_{\\frac{\\beta_2}{\\beta_1+\\beta_2}}(k, k)$.\nFinally, the analytic expression for the average acceptance probability is:\n$$\n\\mathcal{A}_{\\mathrm{an}}(k, \\beta_1, \\beta_2) = 2 \\cdot I_{\\frac{\\beta_2}{\\beta_1+\\beta_2}}(k, k)\n$$\nThis expression will be computed using `scipy.special.betainc`.\n\nThe numerical verification will be performed via a Monte Carlo simulation. For each test case $(k, \\beta_1, \\beta_2, n, \\text{seed})$:\n1.  Generate $n$ independent samples for $U_1$ from $\\text{Gamma}(k, 1/\\beta_1)$.\n2.  Generate $n$ independent samples for $U_2$ from $\\text{Gamma}(k, 1/\\beta_2)$.\n3.  For each pair $(U_{1,i}, U_{2,i})$, calculate the Metropolis acceptance factor $A_i = \\min(1, \\exp[(\\beta_1-\\beta_2)(U_{1,i}-U_{2,i})])$.\n4.  Compute the Monte Carlo estimate $\\widehat{\\mathcal{A}} = \\frac{1}{n} \\sum_{i=1}^n A_i$.\n5.  Compute the unbiased sample standard deviation $s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (A_i - \\widehat{\\mathcal{A}})^2}$.\n6.  Compute the standard error of the mean $\\mathrm{SE} = s/\\sqrt{n}$.\n7.  The verification succeeds if the absolute difference between the analytic value and the Monte Carlo estimate is within $4$ standard errors: $|\\widehat{\\mathcal{A}} - \\mathcal{A}_{\\mathrm{an}}| \\le 4\\cdot\\mathrm{SE}$.\nA sanity check for $\\beta_1=\\beta_2$ gives $(\\beta_1-\\beta_2)=0$, so $A(U_1, U_2)=1$ and $\\mathcal{A}_{\\mathrm{an}}=1$. The analytic formula yields $x = \\beta_1/(2\\beta_1) = 1/2$. For a symmetric Beta distribution, $I_{1/2}(k,k)=1/2$, so $\\mathcal{A}_{\\mathrm{an}} = 2 \\cdot (1/2) = 1$, which is consistent.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import betainc\n\ndef solve():\n    \"\"\"\n    Solves the REMD acceptance kernel verification problem.\n    Derives the analytic solution for the average acceptance probability and\n    verifies it against a Monte Carlo simulation for several test cases.\n    \"\"\"\n    \n    # Test Suite: (k, beta1, beta2, n_samples, seed)\n    test_cases = [\n        (1, 1.0, 1.0, 100000, 42),\n        (1, 1.0, 0.5, 100000, 43),\n        (3, 1.0, 0.8, 100000, 44),\n        (5, 3.0, 2.4, 100000, 45),\n        (10, 2.0, 1.6, 100000, 46),\n        (10, 1.0, 0.7, 100000, 47),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        k, beta1, beta2, n, seed = case\n        \n        # --- Analytic Calculation ---\n        # The average acceptance probability is 2 * I_x(k, k), where\n        # I_x is the regularized incomplete beta function and x = beta2 / (beta1 + beta2).\n        if beta1 + beta2 == 0:\n            # Avoid division by zero, although problem constraints ensure beta > 0.\n            # If beta1=beta2=0, x is undefined. If they are non-zero, this won't happen.\n            x_beta = 0.5 \n        else:\n            x_beta = beta2 / (beta1 + beta2)\n\n        # For the case beta1 == beta2, x_beta is 0.5 and I_0.5(k,k) is 0.5, so A_an = 1.\n        analytic_A = 2.0 * betainc(k, k, x_beta)\n        \n        # --- Monte Carlo Estimation ---\n        rng = np.random.default_rng(seed)\n        \n        # The potential energy U for a canonical ensemble at inverse temperature beta\n        # follows a Gamma distribution with shape k and scale 1/beta.\n        scale1 = 1.0 / beta1\n        scale2 = 1.0 / beta2\n        \n        # Generate n independent energy samples for each replica.\n        U1_samples = rng.gamma(shape=k, scale=scale1, size=n)\n        U2_samples = rng.gamma(shape=k, scale=scale2, size=n)\n        \n        # Calculate the Metropolis acceptance factor for each pair of samples.\n        delta_beta = beta1 - beta2\n        delta_U = U1_samples - U2_samples\n        \n        # A = min(1, exp(delta_beta * delta_U))\n        acceptance_factors = np.minimum(1.0, np.exp(delta_beta * delta_U))\n        \n        # Compute the MC estimate (sample mean) and standard error.\n        mc_A = np.mean(acceptance_factors)\n        # Use ddof=1 for the unbiased sample standard deviation.\n        s = np.std(acceptance_factors, ddof=1)\n        se = s / np.sqrt(n)\n\n        # --- Decision Rule ---\n        # The Monte Carlo estimate agrees if it is within 4 standard errors\n        # of the analytic value.\n        is_consistent = np.abs(mc_A - analytic_A) <= 4.0 * se\n        results.append(is_consistent)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2666530"}, {"introduction": "Having analyzed acceptance rates and verified our tools, we now turn to the advanced task of dynamic optimization. Manually determining an optimal temperature ladder can be a tedious trial-and-error process, so a more sophisticated approach is to use an adaptive algorithm that adjusts the temperature set on-the-fly to achieve a desired, uniform exchange probability. [@problem_id:2461588] This exercise challenges you to design and implement such an algorithm, translating the physical goal into a robust numerical procedure involving feedback and constrained optimization—a skill at the heart of modern computational methods.", "problem": "You are modeling Replica Exchange Molecular Dynamics (REMD), also known as Parallel Tempering Molecular Dynamics. Consider a ladder of inverse temperatures $\\beta_k$ (for $k = 0,1,\\dots,R-1$) associated with replicas, ordered so that $\\beta_{k+1} &gt; \\beta_k$ and, in reduced units, $\\beta_k = 1/T_k$ where $T_k$ are dimensionless temperatures. Exchanges are attempted only between adjacent replicas, and their acceptance probability under the Metropolis criterion is given by $P_{\\text{acc}} = \\min\\left(1, \\exp\\left[(\\beta_i - \\beta_j)(U_j - U_i)\\right]\\right)$ for an attempted swap between replicas $i$ and $j$, where $U_i$ and $U_j$ are the instantaneous potential energies. In practice, you maintain a running average of accepted swaps between adjacent replicas to estimate an exchange probability $p_k$ for each adjacent pair $(k,k+1)$.\n\nYour task is to design, derive, and implement a dynamically adjusting temperature-ladder update that uses only the running averages $p_k$ and a target exchange probability $p^\\star$ to drive the ladder toward uniform target exchange behavior, while preserving REMD consistency constraints. Begin from the definition of the Metropolis acceptance probability, the monotonic relationship between acceptance and inverse-temperature spacing, and the conservation of the total inverse-temperature span between fixed endpoints.\n\nMathematical and algorithmic requirements:\n- Work in reduced units so that $\\beta_k = 1/T_k$.\n- Let the current ladder be temperatures $T_0,T_1,\\dots,T_{R-1}$ that are strictly monotonically decreasing in $k$ (so $\\beta_{k+1} - \\beta_k \\gt 0$).\n- Define the inverse-temperature spacings $s_k = \\beta_{k+1} - \\beta_k$ for $k = 0,\\dots,R-2$. Let $S = \\sum_{k=0}^{R-2} s_k = \\beta_{R-1} - \\beta_0$ be the fixed total span, and let $\\beta_0$ and $\\beta_{R-1}$ be held fixed by the update.\n- Use the running averages $p_k$ and a user-specified target $p^\\star$ and learning rate $\\alpha$ to propose a new set of spacings that adjusts $s_k$ multiplicatively in a way that increases $s_k$ when $p_k \\gt p^\\star$ and decreases $s_k$ when $p_k \\lt p^\\star$. This choice must be justified from the monotonic relation between acceptance and $|\\beta_{k+1}-\\beta_k|$.\n- Enforce the constraints $s_k \\gt 0$ and $\\sum_k s_k = S$ after the adjustment by projecting the proposed $s_k$ onto the positive simplex with a uniform lower bound $s_{\\min} = \\varepsilon S$ for a small $\\varepsilon \\in (0,1)$ specified by the implementation.\n- Reconstruct the updated inverse temperatures as $\\beta_0$ plus the cumulative sum of the adjusted spacings and then recover temperatures $T_k = 1/\\beta_k$.\n- The ordering must remain strictly monotonic in $T_k$ (strictly decreasing).\n\nImplement a program that performs exactly one update step given $(T_0,\\dots,T_{R-1})$, $(p_0,\\dots,p_{R-2})$, $p^\\star$, and $\\alpha$, returning the updated temperatures $(T_0',\\dots,T_{R-1}')$ with endpoints unchanged. All temperatures are dimensionless and must be reported as floating-point numbers rounded to three decimal places.\n\nTest suite:\nImplement your program to process the following four independent test cases. For each case, compute the updated ladder after exactly one adjustment step and output the updated temperatures in the same ordering as the input.\n\n- Case A (happy path):\n  - $T = [\\,560.0,\\,480.0,\\,410.0,\\,350.0,\\,300.0\\,]$\n  - $p = [\\,0.10,\\,0.25,\\,0.35,\\,0.05\\,]$\n  - $p^\\star = 0.23$\n  - $\\alpha = 0.50$\n- Case B (very low acceptances near zero):\n  - $T = [\\,600.0,\\,510.0,\\,430.0,\\,360.0,\\,300.0\\,]$\n  - $p = [\\,0.00,\\,0.00,\\,0.01,\\,0.00\\,]$\n  - $p^\\star = 0.20$\n  - $\\alpha = 0.80$\n- Case C (acceptances very high):\n  - $T = [\\,360.0,\\,340.0,\\,320.0,\\,300.0\\,]$\n  - $p = [\\,0.85,\\,0.90,\\,0.88\\,]$\n  - $p^\\star = 0.30$\n  - $\\alpha = 0.30$\n- Case D (minimal ladder, mixed acceptances):\n  - $T = [\\,500.0,\\,400.0,\\,320.0\\,]$\n  - $p = [\\,0.50,\\,0.05\\,]$\n  - $p^\\star = 0.25$\n  - $\\alpha = 1.00$\n\nFinal output format:\nYour program should produce a single line of output that aggregates the four updated ladders as a single list of lists, where each inner list is the updated temperatures for one test case in the same order as the input, each temperature rounded to three decimal places. The outer list must contain the four inner lists in the order of cases A, B, C, D. The line must be exactly one valid bracketed list of lists with comma separators (no extra text).", "solution": "The problem statement presented is scientifically sound, well-posed, and objective. It outlines a standard procedure for optimizing the temperature ladder in Replica Exchange Molecular Dynamics (REMD) simulations, a cornerstone technique in computational chemistry and physics. The givens are self-contained and consistent, and the task is to implement a single update step of a dynamic temperature adjustment algorithm. The problem is valid and can be solved as stated.\n\nThe solution proceeds in several logical steps:\n1.  Conversion of the input temperature ladder to inverse-temperature spacings.\n2.  Formulation of a multiplicative update rule for these spacings based on the deviation of observed exchange probabilities from a target value.\n3.  Enforcement of physical and algorithmic constraints, namely, the conservation of the total inverse-temperature span and the strict positivity of all spacings. This is accomplished by a projection onto a constrained simplex.\n4.  Reconstruction of the full temperature ladder from the updated spacings.\n\nLet the number of replicas be $R$. The input consists of a temperature ladder $T = (T_0, T_1, \\dots, T_{R-1})$, a set of measured adjacent exchange probabilities $p = (p_0, p_1, \\dots, p_{R-2})$, a target exchange probability $p^\\star$, and a learning rate $\\alpha$. The temperatures are strictly decreasing, $T_k > T_{k+1}$.\n\nFirst, we convert the temperatures $T_k$ to inverse temperatures, $\\beta_k = 1/T_k$, which are given in reduced units. The strict decrease in $T_k$ implies a strict increase in $\\beta_k$, so $\\beta_k < \\beta_{k+1}$. The fundamental quantities for the update are the inverse-temperature spacings between adjacent replicas, defined as:\n$$s_k = \\beta_{k+1} - \\beta_k \\quad \\text{for } k = 0, \\dots, R-2$$\nBy definition, all $s_k > 0$. The endpoints of the ladder, $\\beta_0$ and $\\beta_{R-1}$, are held constant. This fixes the total inverse-temperature span:\n$$S = \\sum_{k=0}^{R-2} s_k = (\\beta_1 - \\beta_0) + (\\beta_2 - \\beta_1) + \\dots + (\\beta_{R-1} - \\beta_{R-2}) = \\beta_{R-1} - \\beta_0$$\nThis conservation of total span $S$ is a critical constraint.\n\nThe acceptance probability for an exchange between adjacent replicas $k$ and $k+1$ is $P_{\\text{acc}} = \\min(1, \\exp[-s_k \\Delta U_k])$, where $\\Delta U_k = U_{k+1} - U_k$ is the difference in potential energy. The observed probability $p_k$ is the simulation-time average of $P_{\\text{acc}}$. There is a monotonic relationship between the spacing $s_k$ and the average acceptance probability $p_k$: a larger spacing $s_k$ corresponds to a larger gap between replica states, leading to a lower acceptance probability. Therefore, to drive the system toward a uniform target probability $p^\\star$, we must adjust the spacings according to the following logic:\n- If $p_k > p^\\star$, the acceptance rate is too high, implying the replicas are too \"close\". We must increase the spacing $s_k$.\n- If $p_k < p^\\star$, the acceptance rate is too low, implying the replicas are too \"far\". We must decrease the spacing $s_k$.\n\nA multiplicative update rule that captures this behavior is an exponential feedback mechanism. We propose a new set of spacings, $s^{\\text{prop}}_k$, as follows:\n$$s^{\\text{prop}}_k = s_k \\exp\\left(\\alpha (p_k - p^\\star)\\right)$$\nHere, the learning rate $\\alpha > 0$ controls the magnitude of the adjustment. If $p_k > p^\\star$, the exponent is positive, yielding a multiplicative factor greater than $1$. If $p_k < p^\\star$, the exponent is negative, yielding a factor less than $1$.\n\nThese proposed spacings $s^{\\text{prop}}_k$ do not, in general, satisfy the constraint that their sum equals the original span $S$. To enforce this, we normalize them:\n$$S^{\\text{prop}} = \\sum_{k=0}^{R-2} s^{\\text{prop}}_k$$\n$$s^{\\text{rescaled}}_k = s^{\\text{prop}}_k \\cdot \\frac{S}{S^{\\text{prop}}}$$\nThis guarantees that $\\sum_{k=0}^{R-2} s^{\\text{rescaled}}_k = S$.\n\nNext, we must enforce the positivity constraint, $s_k > 0$. To prevent spacings from becoming zero or negative, and for numerical stability, we enforce a stronger condition: $s_k \\ge s_{\\min}$, where $s_{\\min}$ is a small positive lower bound. We define this bound relative to the total span: $s_{\\min} = \\varepsilon S$ for a small dimensionless constant $\\varepsilon \\in (0, 1/(R-1))$. We will use $\\varepsilon = 10^{-6}$.\n\nThe vector $s^{\\text{rescaled}}$ must be projected onto the constrained set $\\{ s' \\in \\mathbb{R}^{R-1} \\mid \\sum s'_k = S \\text{ and } s'_k \\ge s_{\\min} \\text{ for all } k \\}$. This is a projection onto a simplex with a uniform lower bound. This can be solved efficiently by transforming the problem. Let $v_k = s'_k - s_{\\min}$. The constraints on $v_k$ become $v_k \\ge 0$ and $\\sum_k (v_k + s_{\\min}) = S$, which simplifies to $\\sum_k v_k = S - (R-1)s_{\\min}$. We define $u_k = s^{\\text{rescaled}}_k - s_{\\min}$ and project the vector $u$ onto the standard simplex defined by $\\{ v \\in \\mathbb{R}^{R-1} \\mid \\sum v_k = C, v_k \\ge 0 \\}$, where $C=S - (R-1)s_{\\min}$.\n\nA standard algorithm for this projection is as follows:\n1.  Sort the elements of $u$ in descending order to get $u_{\\text{sorted}}$.\n2.  Find the largest integer $\\hat{k} \\in \\{1, \\dots, R-1\\}$ such that $u_{\\text{sorted}}[\\hat{k}-1] > \\frac{1}{\\hat{k}} \\left( \\sum_{j=0}^{\\hat{k}-1} u_{\\text{sorted}}[j] - C \\right)$.\n3.  Compute the value $\\theta = \\frac{1}{\\hat{k}} \\left( \\sum_{j=0}^{\\hat{k}-1} u_{\\text{sorted}}[j] - C \\right)$.\n4.  The projected vector $v'$ is given by $v'_k = \\max(0, u_k - \\theta)$.\n5.  The final, constrained spacings $s'$ are recovered by $s'_k = v'_k + s_{\\min}$.\n\nBy construction, these final spacings $s'_k$ satisfy $\\sum_k s'_k = S$ and $s'_k \\ge s_{\\min} > 0$.\n\nFinally, we reconstruct the new temperature ladder from the adjusted spacings $s'_k$. The inverse temperatures are given by the cumulative sum of the spacings, starting from the fixed value $\\beta_0$:\n$$\\beta'_0 = \\beta_0$$\n$$\\beta'_{k} = \\beta_0 + \\sum_{i=0}^{k-1} s'_i \\quad \\text{for } k = 1, \\dots, R-1$$\nThis construction ensures that $\\beta'_0 = \\beta_0$ and $\\beta'_{R-1} = \\beta_0 + \\sum_{i=0}^{R-2} s'_i = \\beta_0 + S = \\beta_{R-1}$. The endpoints are preserved. The new temperatures are then recovered by inversion:\n$$T'_k = 1/\\beta'_k$$\nSince $s'_k > 0$, we have $\\beta'_{k+1} > \\beta'_k$, which guarantees that the new temperature ladder $T'$ is also strictly monotonically decreasing, $T'_k > T'_{k+1}$. The final temperatures are then rounded to three decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_temperature_ladder(temps, probs, p_star, alpha, epsilon=1e-6):\n    \"\"\"\n    Performs one update step for the REMD temperature ladder.\n\n    Args:\n        temps (list[float]): Current temperature ladder, T_0, ..., T_{R-1}.\n        probs (list[float]): Measured exchange probabilities, p_0, ..., p_{R-2}.\n        p_star (float): Target exchange probability.\n        alpha (float): Learning rate.\n        epsilon (float): Small constant for minimum spacing.\n    \n    Returns:\n        list[float]: The updated temperature ladder, rounded to 3 decimal places.\n    \"\"\"\n    R = len(temps)\n    if R < 2:\n        return [round(t, 3) for t in temps]\n\n    # Use numpy arrays for vectorized operations\n    T = np.array(temps, dtype=np.float64)\n    p = np.array(probs, dtype=np.float64)\n\n    # 1. Convert temperatures to inverse temperatures and find spacings\n    beta = 1.0 / T\n    s = beta[1:] - beta[:-1]\n    \n    # Check for valid initial spacings\n    if np.any(s <= 0):\n        # This case should not happen with a valid input T ladder\n        raise ValueError(\"Initial temperature ladder is not strictly monotonic.\")\n        \n    S = beta[-1] - beta[0]\n\n    # 2. Propose new spacings based on a multiplicative update rule\n    s_prop = s * np.exp(alpha * (p - p_star))\n\n    # 3. Normalize to preserve the total inverse temperature span\n    S_prop = np.sum(s_prop)\n    if S_prop <= 0:\n        # Failsafe for extreme updates. Return original temperatures.\n        return [round(t, 3) for t in temps]\n    s_rescaled = s_prop * (S / S_prop)\n    \n    # 4. Project onto constrained simplex to enforce s_k >= s_min\n    num_spacings = R - 1\n    s_min = epsilon * S\n    \n    # Check if projection is possible\n    if num_spacings * s_min > S:\n        raise ValueError(\"Constraints are infeasible: total minimum spacing exceeds total span.\")\n\n    # Transform for projection onto the standard probability simplex\n    u = s_rescaled - s_min\n    C = S - num_spacings * s_min\n\n    # Sort u in descending order to find the projection parameter theta\n    u_sorted = np.sort(u)[::-1]\n    u_cumsum = np.cumsum(u_sorted)\n    \n    # Find k_hat for calculating theta\n    # The condition is u_sorted[k-1] > (u_cumsum[k-1] - C) / k for k=1...num_spacings\n    k_hat_candidates = np.where(u_sorted > (u_cumsum - C) / np.arange(1, num_spacings + 1))[0]\n    \n    if len(k_hat_candidates) == 0:\n        # This can happen if C is very small or negative due to numerical precision\n        # which implies most u_k are negative. Then theta should be large enough\n        # to make almost all v_k = 0.\n        k_hat = num_spacings # A safe fallback, might need more robust handling\n    else:\n        k_hat = k_hat_candidates[-1] + 1\n    \n    theta = (u_cumsum[k_hat - 1] - C) / k_hat\n    \n    # Project and transform back to final spacings\n    v_prime = np.maximum(0, u - theta)\n    s_prime = v_prime + s_min\n\n    # 5. Reconstruct the new temperature ladder\n    beta_prime = np.zeros_like(beta)\n    beta_prime[0] = beta[0]\n    beta_prime[1:] = beta[0] + np.cumsum(s_prime)\n    \n    # Ensure the endpoint is preserved despite potential precision errors\n    beta_prime[-1] = beta[-1]\n\n    T_prime = 1.0 / beta_prime\n\n    # Round to three decimal places for the final output\n    return [round(t, 3) for t in T_prime]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": [560.0, 480.0, 410.0, 350.0, 300.0],\n            \"p\": [0.10, 0.25, 0.35, 0.05],\n            \"p_star\": 0.23,\n            \"alpha\": 0.50\n        },\n        {\n            \"T\": [600.0, 510.0, 430.0, 360.0, 300.0],\n            \"p\": [0.00, 0.00, 0.01, 0.00],\n            \"p_star\": 0.20,\n            \"alpha\": 0.80\n        },\n        {\n            \"T\": [360.0, 340.0, 320.0, 300.0],\n            \"p\": [0.85, 0.90, 0.88],\n            \"p_star\": 0.30,\n            \"alpha\": 0.30\n        },\n        {\n            \"T\": [500.0, 400.0, 320.0],\n            \"p\": [0.50, 0.05],\n            \"p_star\": 0.25,\n            \"alpha\": 1.00\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        updated_temps = update_temperature_ladder(\n            case[\"T\"], case[\"p\"], case[\"p_star\"], case[\"alpha\"]\n        )\n        all_results.append(updated_temps)\n\n    # Format the final output string exactly as specified\n    inner_lists_str = []\n    for res_list in all_results:\n        # Format each temperature to 3 decimal places\n        temps_str = ','.join([f\"{t:.3f}\" for t in res_list])\n        inner_lists_str.append(f\"[{temps_str}]\")\n    \n    final_output = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2461588"}]}