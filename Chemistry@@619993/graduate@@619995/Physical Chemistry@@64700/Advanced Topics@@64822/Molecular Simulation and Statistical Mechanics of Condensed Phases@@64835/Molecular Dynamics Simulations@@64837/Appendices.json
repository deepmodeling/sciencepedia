{"hands_on_practices": [{"introduction": "To simulate a bulk system without dominant surface artifacts, molecular dynamics simulations employ periodic boundary conditions (PBC). A core component of PBC is the Minimum Image Convention (MIC), which defines the physical distance between particles in a tessellated, infinite lattice. This practice guides you through the implementation of this fundamental algorithm [@problem_id:2458300], a cornerstone for correctly calculating forces and energies in any periodic simulation.", "problem": "You are to write a complete, runnable program that demonstrates the effect of the minimum image convention for periodic boundary conditions in molecular dynamics (MD) simulations. The program must compute the Euclidean distance between two particles in an orthorhombic periodic simulation box in two ways: (1) the naive Euclidean distance that ignores periodicity, and (2) the Euclidean distance after applying the minimum image convention. Distances must be expressed in nanometers (nm) and reported rounded to exactly six decimal places.\n\nBackground and fundamental base: In molecular dynamics (MD) simulations with periodic boundary conditions (PBC), an infinite tiling of the finite simulation box is used to mimic bulk behavior. The physical distance between two particles is interpreted as the minimum distance between one particle and any periodic image of the other. The underlying geometric rule is derived from the definition of Euclidean distance and the translation symmetry of the lattice. For an orthorhombic box with side lengths $L_x$, $L_y$, and $L_z$, the minimum distance is obtained by selecting an integer number of box-length translations along each axis that minimizes the Euclidean norm of the displacement vector. Your program must implement this rule.\n\nYour task:\n- Define a function that, given two $3$-dimensional positions $\\mathbf{r}_i$ and $\\mathbf{r}_j$ in nanometers and box lengths $(L_x,L_y,L_z)$ in nanometers, returns two values:\n  1) the naive Euclidean distance $d_{\\text{naive}} = \\lVert \\mathbf{r}_j - \\mathbf{r}_i \\rVert$ in nm,\n  2) the minimum image convention distance $d_{\\text{mic}}$ in nm, obtained by translating the displacement components by integer multiples of $(L_x,L_y,L_z)$ to minimize the norm.\n- Treat the box as orthorhombic and axis-aligned.\n- Positions may lie outside the primary box interval. Your implementation must still correctly compute $d_{\\text{mic}}$ using periodicity.\n- To make your implementation clear, also include within the program a multi-line pseudocode string (not printed) that outlines the minimum image convention logic you implemented.\n\nNumerical and unit requirements:\n- All distances must be in nanometers (nm).\n- Report distances rounded to exactly $6$ decimal places.\n\nTest suite:\nYour program must compute $[d_{\\text{naive}}, d_{\\text{mic}}]$ for each of the following $5$ test cases, in the stated order.\n\n1) Happy path, small separation well within the box:\n- Box: $(L_x,L_y,L_z) = (\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_i = (\\,0.5\\,\\text{nm},\\,1.0\\,\\text{nm},\\,1.5\\,\\text{nm}\\,)$\n- $\\mathbf{r}_j = (\\,0.7\\,\\text{nm},\\,1.2\\,\\text{nm},\\,1.6\\,\\text{nm}\\,)$\n\n2) Crossing a periodic boundary along one axis:\n- Box: $(L_x,L_y,L_z) = (\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_i = (\\,0.1\\,\\text{nm},\\,1.0\\,\\text{nm},\\,1.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_j = (\\,2.9\\,\\text{nm},\\,1.0\\,\\text{nm},\\,1.0\\,\\text{nm}\\,)$\n\n3) Anisotropic box and wrapping along multiple axes:\n- Box: $(L_x,L_y,L_z) = (\\,2.0\\,\\text{nm},\\,4.0\\,\\text{nm},\\,5.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_i = (\\,1.9\\,\\text{nm},\\,0.2\\,\\text{nm},\\,4.8\\,\\text{nm}\\,)$\n- $\\mathbf{r}_j = (\\,0.1\\,\\text{nm},\\,3.9\\,\\text{nm},\\,0.3\\,\\text{nm}\\,)$\n\n4) Exactly half-box separation along one axis (tie case):\n- Box: $(L_x,L_y,L_z) = (\\,4.0\\,\\text{nm},\\,4.0\\,\\text{nm},\\,4.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_i = (\\,0.0\\,\\text{nm},\\,0.0\\,\\text{nm},\\,0.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_j = (\\,2.0\\,\\text{nm},\\,0.0\\,\\text{nm},\\,0.0\\,\\text{nm}\\,)$\n\n5) Positions outside the primary box interval:\n- Box: $(L_x,L_y,L_z) = (\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm},\\,3.0\\,\\text{nm}\\,)$\n- $\\mathbf{r}_i = (\\,{-}0.1\\,\\text{nm},\\,{-}0.1\\,\\text{nm},\\,{-}0.1\\,\\text{nm}\\,)$\n- $\\mathbf{r}_j = (\\,3.1\\,\\text{nm},\\,3.1\\,\\text{nm},\\,3.1\\,\\text{nm}\\,)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of pairs enclosed in square brackets. Each pair corresponds to one test case in the specified order and must have the form $[d_{\\text{naive}},d_{\\text{mic}}]$, with both values rounded to exactly $6$ decimal places in nanometers. There must be no spaces in the output.\n- For example, the required overall format is like $[[a_1,b_1],[a_2,b_2],\\dots]$ where each $a_k$ and $b_k$ are floats in nanometers with exactly $6$ decimal places.", "solution": "The problem posed is a fundamental exercise in computational statistical mechanics, specifically concerning the implementation of periodic boundary conditions (PBC) in molecular dynamics (MD) simulations. The use of PBC is a standard and necessary technique to approximate the properties of a macroscopic system by simulating a small, finite number of particles. It mitigates the severe surface effects that would otherwise dominate the behavior of a small system. The core of this problem is the correct calculation of inter-particle distances, which is governed by the minimum image convention (MIC).\n\nThe problem is well-posed, scientifically grounded, and provides all necessary data for a unique and verifiable solution. We will proceed with a rigorous derivation and implementation.\n\nLet the orthorhombic simulation box be defined by a set of three orthogonal vectors corresponding to the side lengths, $\\mathbf{L} = (L_x, L_y, L_z)$. The position of two particles, $i$ and $j$, are given by vectors $\\mathbf{r}_i$ and $\\mathbf{r}_j$.\n\nFirst, we define the naive Euclidean distance, $d_{\\text{naive}}$. This is the standard distance in a non-periodic, three-dimensional Euclidean space. It is calculated from the norm of the displacement vector $\\Delta\\mathbf{r} = \\mathbf{r}_j - \\mathbf{r}_i$.\n$$\nd_{\\text{naive}} = \\lVert \\Delta\\mathbf{r} \\rVert = \\sqrt{(\\Delta x)^2 + (\\Delta y)^2 + (\\Delta z)^2}\n$$\nwhere $\\Delta\\mathbf{r} = (\\Delta x, \\Delta y, \\Delta z)$. This calculation ignores the periodic nature of the simulation box and treats the system as if it were isolated in a vacuum.\n\nSecond, we address the minimum image convention distance, $d_{\\text{mic}}$. In a periodic system, the simulation box is replicated infinitely in all directions. A particle at position $\\mathbf{r}$ has an infinite lattice of periodic images at positions $\\mathbf{r} + n_x L_x \\hat{\\mathbf{x}} + n_y L_y \\hat{\\mathbf{y}} + n_z L_z \\hat{\\mathbf{z}}$, where $n_x, n_y, n_z$ are any integers. The physical distance between particle $i$ and particle $j$ is the shortest distance between particle $i$ and *any* of the periodic images of particle $j$.\n\nMathematically, this is expressed as:\n$$\nd_{\\text{mic}} = \\min_{n_x, n_y, n_z \\in \\mathbb{Z}} \\left\\lVert (\\mathbf{r}_j - \\mathbf{r}_i) - (n_x L_x \\hat{\\mathbf{x}} + n_y L_y \\hat{\\mathbf{y}} + n_z L_z \\hat{\\mathbf{z}}) \\right\\rVert\n$$\nFor an orthorhombic box, the minimization of the norm can be performed independently for each Cartesian component. Let us consider the $x$-component of the displacement vector, $\\Delta x = x_j - x_i$. We must find an integer $n_x$ that minimizes $|\\Delta x - n_x L_x|$. This is achieved when $n_x$ is the integer nearest to the ratio $\\Delta x / L_x$. This is the \"nearest integer function,\" often denoted as $\\text{nint}(s)$ or implemented via `round(s)`.\n\nThe MIC-adjusted displacement component, $\\Delta x'$, is therefore:\n$$\n\\Delta x' = \\Delta x - L_x \\cdot \\text{round}\\left(\\frac{\\Delta x}{L_x}\\right)\n$$\nThis formula correctly \"wraps\" the displacement vector component into the interval $[-L_x/2, L_x/2]$. The same logic applies to the $y$ and $z$ components. The complete MIC-adjusted displacement vector, $\\Delta\\mathbf{r}'$, is then:\n$$\n\\Delta\\mathbf{r}' = \\left( \\Delta x - L_x \\cdot \\text{round}\\left(\\frac{\\Delta x}{L_x}\\right), \\Delta y - L_y \\cdot \\text{round}\\left(\\frac{\\Delta y}{L_y}\\right), \\Delta z - L_z \\cdot \\text{round}\\left(\\frac{\\Delta z}{L_z}\\right) \\right)\n$$\nThis vector operation is robust and correctly handles cases where particle coordinates may lie outside the primary simulation box, as only their relative displacement matters.\n\nThe minimum image distance, $d_{\\text{mic}}$, is the Euclidean norm of this adjusted displacement vector:\n$$\nd_{\\text{mic}} = \\lVert \\Delta\\mathbf{r}' \\rVert = \\sqrt{(\\Delta x')^2 + (\\Delta y')^2 + (\\Delta z')^2}\n$$\n\nThe provided test cases will be solved using this established methodology. The implementation will utilize the `numpy` library for efficient vector arithmetic. For each test case, we will compute $d_{\\text{naive}}$ and $d_{\\text{mic}}$ and report the results rounded to precisely $6$ decimal places as specified. The special case of a displacement of exactly half a box length, e.g., $\\Delta x = L_x/2$, is handled by the `round` function, which typically rounds to the nearest even integer (e.g., in `numpy`), but the resulting distance is uniquely $L_x/2$ regardless of the sign choice for the adjusted displacement. For $\\Delta x = L_x/2$, $\\Delta x' = L_x/2 - L_x \\cdot \\text{round}(0.5) = L_x/2 - L_x \\cdot 0 = L_x/2$. The magnitude is unambiguous.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating naive and minimum image convention (MIC)\n    distances for a set of test cases in a molecular dynamics context.\n    \"\"\"\n\n    # Per the problem specification, this multi-line string contains the\n    # pseudocode explaining the implemented MIC logic. It is not printed.\n    # noinspection PyUnusedLocal\n    MIC_PSEUDOCODE = \"\"\"\n    function calculate_minimum_image_distance(r_i, r_j, box_dims):\n        // r_i, r_j: 3D position vectors [x, y, z] of two particles in nm.\n        // box_dims: 3D vector of orthorhombic box lengths [L_x, L_y, L_z] in nm.\n\n        // 1. Calculate the raw displacement vector.\n        //    This is a simple vector subtraction.\n        delta_r = r_j - r_i\n\n        // 2. Apply the minimum image convention to each component of the displacement vector.\n        //    For an orthorhombic box, this can be done independently for each axis.\n        //    The principle is to find the closest periodic image by shifting the displacement\n        //    by an integer number of box lengths. This is mathematically equivalent to\n        //    finding the nearest integer multiple of the box length to subtract.\n        \n        //    Let dr_c be a component of delta_r (e.g., delta_x) and L_c be the\n        //    corresponding box length (e.g., L_x).\n        //    The scaled displacement is s = dr_c / L_c.\n        //    The nearest integer number of box lengths to shift by is n = round(s).\n        //    The MIC-adjusted displacement component is dr'_c = dr_c - n * L_c.\n\n        //    In vector notation, this is:\n        mic_delta_r = delta_r - box_dims * np.round(delta_r / box_dims)\n        \n        // 3. Calculate the Euclidean norm (length) of the MIC-adjusted displacement vector.\n        //    This is the final minimum image distance.\n        distance_mic = sqrt(mic_delta_r[0]^2 + mic_delta_r[1]^2 + mic_delta_r[2]^2)\n        \n        return distance_mic\n    \"\"\"\n\n    def compute_distances(r_i_tuple, r_j_tuple, box_dims_tuple):\n        \"\"\"\n        Calculates naive and MIC distances for a single pair of particles.\n        \n        Args:\n            r_i_tuple (tuple): Position of particle i.\n            r_j_tuple (tuple): Position of particle j.\n            box_dims_tuple (tuple): Orthorhombic box dimensions (Lx, Ly, Lz).\n        \n        Returns:\n            A list containing two floats: [d_naive, d_mic].\n        \"\"\"\n        r_i = np.array(r_i_tuple, dtype=float)\n        r_j = np.array(r_j_tuple, dtype=float)\n        box_dims = np.array(box_dims_tuple, dtype=float)\n\n        # 1. Naive Euclidean distance\n        delta_r_naive = r_j - r_i\n        d_naive = np.linalg.norm(delta_r_naive)\n\n        # 2. Minimum Image Convention (MIC) distance\n        # The logic delta_r - L * round(delta_r / L) correctly finds the\n        # shortest vector in a periodic lattice.\n        delta_r_mic = delta_r_naive - box_dims * np.round(delta_r_naive / box_dims)\n        d_mic = np.linalg.norm(delta_r_mic)\n        \n        return [d_naive, d_mic]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1) Happy path, small separation\n        {'r_i': (0.5, 1.0, 1.5), 'r_j': (0.7, 1.2, 1.6), 'box': (3.0, 3.0, 3.0)},\n        # 2) Crossing a periodic boundary\n        {'r_i': (0.1, 1.0, 1.0), 'r_j': (2.9, 1.0, 1.0), 'box': (3.0, 3.0, 3.0)},\n        # 3) Anisotropic box and multi-axis wrapping\n        {'r_i': (1.9, 0.2, 4.8), 'r_j': (0.1, 3.9, 0.3), 'box': (2.0, 4.0, 5.0)},\n        # 4) Exactly half-box separation\n        {'r_i': (0.0, 0.0, 0.0), 'r_j': (2.0, 0.0, 0.0), 'box': (4.0, 4.0, 4.0)},\n        # 5) Positions outside the primary box\n        {'r_i': (-0.1, -0.1, -0.1), 'r_j': (3.1, 3.1, 3.1), 'box': (3.0, 3.0, 3.0)},\n    ]\n\n    result_strings = []\n    for case in test_cases:\n        d_naive, d_mic = compute_distances(case['r_i'], case['r_j'], case['box'])\n        \n        # Format the numbers to exactly 6 decimal places and create the pair string.\n        # This ensures trailing zeros are included and meets the formatting requirement.\n        result_strings.append(f\"[{d_naive:.6f},{d_mic:.6f}]\")\n\n    # Final print statement in the exact required format: [[a1,b1],[a2,b2],...]\n    # without spaces between elements.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2458300"}, {"introduction": "After defining the simulation space, we need a robust algorithm to propagate the system through time. The velocity Verlet integrator is a popular choice due to its excellent long-term stability, which arises from its symplectic nature and the conservation of a \"shadow\" Hamiltonian. This exercise combines theoretical derivation with computational experiment to analyze the energy conservation properties of the Verlet integrator [@problem_id:2651929], providing deep insight into how numerical methods affect the physical validity of a simulation.", "problem": "Consider a one-dimensional harmonic oscillator with mass $m$ and spring constant $k$ evolving under Newton's second law $m\\,d^2x/dt^2 = -\\,dV/dx$ with potential $V(x) = \\tfrac{1}{2}k x^2$. A common second-order symplectic, time-reversible integrator used in Molecular Dynamics (MD) is the velocity Verlet scheme. From the perspective of Hamiltonian mechanics, symplectic integrators preserve phase-space volume and exactly conserve a modified Hamiltonian that differs from the true Hamiltonian by higher-order terms in the time step $\\Delta t$. Starting from this fundamental base (Newton's laws, Hamiltonian structure, and the definitions of symplectic and time-reversible integration), derive the leading-order scaling of the global energy error for the velocity Verlet integrator as a function of $\\Delta t$. Specifically, define the absolute energy deviation $A(\\Delta t)$ over a simulation of duration $T$ as\n$$\nA(\\Delta t) = \\max_{0 \\le t \\le T}\\,\\big|E(t) - E(0)\\big| \\quad \\text{with} \\quad E(t) = \\tfrac{1}{2}m v(t)^2 + \\tfrac{1}{2}k x(t)^2,\n$$\nand quantify how $A(\\Delta t)$ scales with $\\Delta t$ as $\\Delta t \\to 0$ for the second-order symplectic velocity Verlet method.\n\nThen, design an empirical protocol to measure energy drift in a production Molecular Dynamics (MD) simulation in the microcanonical ensemble (constant number of particles $N$, volume $V$, and energy $E$; often called $NVE$), and translate this protocol into a computational experiment on the harmonic oscillator. In particular:\n- Implement the velocity Verlet integrator for the harmonic oscillator with parameters $m$, $k$, initial position $x(0)$, and initial velocity $v(0)$.\n- For a set of time steps $\\{\\Delta t_i\\}$, run the integrator up to total time $T$ and record the total energy $E(t_n)$ at each discrete time $t_n = n \\Delta t_i$. For each $\\Delta t_i$:\n  1. Compute the absolute energy deviation $A(\\Delta t_i) = \\max_n |E(t_n) - E(0)|$ in joules.\n  2. Estimate the empirical linear energy drift per unit time $D(\\Delta t_i)$ (in joules per second) as the least-squares slope of $E(t)$ versus $t$ over the run, that is, the slope of the best-fitting line to the points $(t_n, E(t_n))$ in the ordinary least-squares sense.\n  3. Determine a boolean stability flag by monitoring whether the integration remains numerically stable over $[0,T]$. Declare the trajectory unstable if at any sample the total energy becomes non-finite or grows beyond a prescribed multiple of $E(0)$; otherwise declare it stable.\n- Using only the stable trajectories, fit a power law $A(\\Delta t) \\propto \\Delta t^{p}$ by performing a linear regression of $\\log A$ versus $\\log \\Delta t$ to estimate the scaling exponent $p$.\n\nYour program must use the following physical parameters and test suite, and report the requested quantities in the specified units:\n- Physical parameters:\n  - Mass $m = 1.66053906660 \\times 10^{-27}\\ \\text{kg}$.\n  - Spring constant $k = 1.0\\ \\text{N}\\,\\text{m}^{-1}$.\n  - Initial conditions $x(0) = 1.0 \\times 10^{-9}\\ \\text{m}$ and $v(0) = 0.0\\ \\text{m}\\,\\text{s}^{-1}$.\n- Total simulation time: $T = 2.0 \\times 10^{-11}\\ \\text{s}$.\n- Time step test suite:\n  - $\\Delta t_1 = 1.0 \\times 10^{-15}\\ \\text{s}$ (happy path).\n  - $\\Delta t_2 = 2.0 \\times 10^{-15}\\ \\text{s}$ (happy path).\n  - $\\Delta t_3 = 4.0 \\times 10^{-15}\\ \\text{s}$ (larger but expected stable).\n  - $\\Delta t_4 = 3.0 \\times 10^{-14}\\ \\text{s}$ (beyond the linear stability limit for the harmonic oscillator, i.e., an edge case expected to be unstable).\n- Stability detection rule: if at any time the total energy is non-finite or satisfies $|E(t) - E(0)|/E(0) > 10^{6}$, declare the trajectory unstable and stop the run for that $\\Delta t$.\n\nUnits and numerical reporting requirements:\n- Report $A(\\Delta t_i)$ in joules.\n- Report $D(\\Delta t_i)$ in joules per second.\n- The exponent $p$ is dimensionless.\n- Angles are not used in this problem.\n- All floating-point numbers should be output in standard decimal or scientific notation as produced by the programming language's default conversion, without units embedded in the output.\n\nFinal output format:\n- Your program should produce a single line of output containing a single list. This list must have one entry per time step in the test suite, followed by the exponent $p$ as the final entry. Each per-time-step entry must be a list of the form $[\\text{stable}, A(\\Delta t_i), D(\\Delta t_i)]$, where $\\text{stable}$ is a boolean. Therefore, the output should look like\n$[[\\text{stable}_1, A_1, D_1],[\\text{stable}_2, A_2, D_2],[\\text{stable}_3, A_3, D_3],[\\text{stable}_4, A_4, D_4],p]$\nwith no spaces after commas and enclosed in a single pair of square brackets.\n\nDesign for coverage:\n- The chosen $\\Delta t_1$ and $\\Delta t_2$ represent typical stable cases for assessing scaling.\n- The chosen $\\Delta t_3$ probes larger but still stable steps.\n- The chosen $\\Delta t_4$ exceeds the linear stability limit $\\Delta t < 2/\\omega$ with $\\omega = \\sqrt{k/m}$ and is intended to trigger the instability detector.\n\nExpress the final numeric outputs with the units specified above understood but not included in the printed line. Your derivation must start from the stated fundamental principles and must not assume the target scaling in advance.", "solution": "The problem as stated is well-defined and grounded in the fundamental principles of classical mechanics and numerical analysis. It is scientifically sound and provides all necessary information for a unique solution. We proceed with the resolution, which consists of a theoretical derivation followed by a computational procedure.\n\n**Theoretical Derivation of Energy Error Scaling**\n\nThe system under consideration is a one-dimensional harmonic oscillator governed by the Hamiltonian $H(x, p) = \\frac{p^2}{2m} + \\frac{1}{2}kx^2$, where $p=mv$ is the momentum. The equation of motion is $m\\ddot{x} = -kx$. The dynamics are to be integrated using the velocity Verlet algorithm, a second-order symmetric, symplectic integrator.\n\nThe update scheme for one time step $\\Delta t$ from time $t$ to $t+\\Delta t$ is given by:\n$$x(t + \\Delta t) = x(t) + v(t)\\Delta t + \\frac{1}{2}a(t)(\\Delta t)^2$$\n$$v(t + \\Delta t) = v(t) + \\frac{1}{2}\\left[ a(t) + a(t+\\Delta t) \\right]\\Delta t$$\nwhere $a(t) = F(x(t))/m = -kx(t)/m$.\n\nA fundamental property of a symplectic integrator is that it does not preserve the true Hamiltonian $H$ of the system. Instead, the sequence of points $(x_n, v_n)$ generated by the algorithm lies on an invariant manifold of a modified, or \"shadow,\" Hamiltonian, $\\tilde{H}$. For a symmetric integrator of order $2r$ (for velocity Verlet, the order is $2$, so $r=1$), this shadow Hamiltonian can be expressed as an asymptotic series in even powers of the time step $\\Delta t$:\n$$\\tilde{H}(x, v; \\Delta t) = H(x, v) + (\\Delta t)^2 H_2(x, v) + (\\Delta t)^4 H_4(x, v) + \\mathcal{O}((\\Delta t)^6)$$\nThe functions $H_j(x, v)$ are determined by the original Hamiltonian and the specific form of the integrator.\n\nThe numerical trajectory $(x_n, v_n)$ at times $t_n = n\\Delta t$ exactly conserves this shadow Hamiltonian (up to machine precision):\n$$\\tilde{H}(x_n, v_n; \\Delta t) = \\text{constant} = \\tilde{E}_0$$\nThe value of this conserved quantity $\\tilde{E}_0$ is fixed by the initial conditions $(x_0, v_0)$:\n$$\\tilde{E}_0 = \\tilde{H}(x_0, v_0; \\Delta t) = H(x_0, v_0) + (\\Delta t)^2 H_2(x_0, v_0) + \\mathcal{O}((\\Delta t)^4)$$\nLet $E_0 = H(x_0, v_0)$ be the true initial energy. Then $\\tilde{E}_0 \\approx E_0 + (\\Delta t)^2 H_2(x_0, v_0)$.\n\nThe energy measured during the simulation is the value of the *true* Hamiltonian, $E_n = H(x_n, v_n)$, evaluated at each point of the numerical trajectory. By rearranging the series for $\\tilde{H}$, we can express $E_n$ as:\n$$E_n = H(x_n, v_n) = \\tilde{H}(x_n, v_n; \\Delta t) - (\\Delta t)^2 H_2(x_n, v_n) - \\mathcal{O}((\\Delta t)^4)$$\nSince $\\tilde{H}(x_n, v_n; \\Delta t) = \\tilde{E}_0$ for all $n$, we have:\n$$E_n \\approx \\tilde{E}_0 - (\\Delta t)^2 H_2(x_n, v_n)$$\nWe are interested in the energy deviation from the initial energy, $E_n - E_0$. Substituting the expression for $\\tilde{E}_0$:\n$$E_n - E_0 \\approx \\left( E_0 + (\\Delta t)^2 H_2(x_0, v_0) \\right) - (\\Delta t)^2 H_2(x_n, v_n) - E_0$$\n$$E_n - E_0 \\approx (\\Delta t)^2 \\left( H_2(x_0, v_0) - H_2(x_n, v_n) \\right)$$\nFor a bound system such as the harmonic oscillator, the trajectory $(x_n, v_n)$ is periodic or quasi-periodic in phase space. As the system evolves, the value of the function $H_2(x_n, v_n)$ varies. This causes the computed energy $E_n$ to oscillate around a mean value, rather than remaining constant or drifting systematically. The amplitude of these energy oscillations is proportional to the leading-order term, $(\\Delta t)^2$.\n\nThe quantity of interest is the maximum absolute energy deviation, $A(\\Delta t) = \\max_n |E_n - E_0|$. Based on the above analysis, the magnitude of this deviation is determined by the maximum variation of the $(\\Delta t)^2$ term over the trajectory. Thus, we derive the scaling law:\n$$A(\\Delta t) \\propto (\\Delta t)^2$$\nThis predicts that the scaling exponent $p$ in the relation $A(\\Delta t) \\propto (\\Delta t)^p$ is equal to $2$. The linear energy drift $D(\\Delta t)$ is expected to be negligible, arising from finite-precision arithmetic rather than the algorithm itself, as symplectic integrators exhibit no systematic energy drift for bounded motion.\n\n**Computational Protocol**\n\nThe theoretical result will be tested empirically through a direct numerical simulation as specified. The procedure is as follows:\n1.  **Integrator Implementation**: The velocity Verlet algorithm is implemented for the one-dimensional harmonic oscillator with force $F(x) = -kx$.\n2.  **Simulation Loop**: For each time step $\\Delta t_i$ in the provided test suite, a simulation is executed from $t=0$ to $t=T$. The state $(x(t), v(t))$ is initialized to $(x(0), v(0))$.\n3.  **Data Collection and Stability Check**: At each step $n$, the total energy $E(t_n) = \\frac{1}{2}m v(t_n)^2 + \\frac{1}{2}k x(t_n)^2$ is computed and stored. A stability check is performed: if $|E(t_n) - E(0)|/E(0) > 10^6$ or if $E(t_n)$ becomes non-finite, the trajectory is flagged as unstable, and the simulation for that $\\Delta t_i$ is terminated.\n4.  **Metric Calculation**: Upon completion (or termination) of a run for a given $\\Delta t_i$:\n    *   The absolute energy deviation $A(\\Delta t_i) = \\max_n |E(t_n) - E(0)|$ is calculated over the generated trajectory.\n    *   The linear energy drift per unit time, $D(\\Delta t_i)$, is estimated by performing an ordinary least-squares linear regression of the energy data $\\{E(t_n)\\}$ against the time data $\\{t_n\\}$. $D(\\Delta t_i)$ is the slope of the best-fit line.\n5.  **Scaling Exponent Estimation**: Using the data from all *stable* trajectories, the scaling exponent $p$ is determined. This is achieved by fitting a power law $A(\\Delta t) = C(\\Delta t)^p$. Taking the logarithm of both sides yields $\\log A = p \\log \\Delta t + \\log C$. A linear regression of $\\log A$ versus $\\log \\Delta t$ is performed, and the resulting slope provides the empirical estimate for $p$.\n\nThis computational experiment is designed to validate the theoretical prediction that $p=2$, quantify the energy fluctuations, and test the stability limits of the integrator for this system. The final output will be a program that executes this protocol and reports the results in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing the velocity Verlet integrator for a 1D harmonic oscillator.\n    \n    This function performs the following steps:\n    1. Sets up the physical parameters and simulation settings for a harmonic oscillator.\n    2. Iterates through a test suite of time steps (dt).\n    3. For each dt, runs a molecular dynamics simulation using the velocity Verlet algorithm.\n    4. Monitors the simulation for numerical stability based on energy conservation.\n    5. Calculates the maximum absolute energy deviation (A) and the linear energy drift (D).\n    6. For stable runs, performs a power-law fit to find the scaling exponent (p) of A versus dt.\n    7. Formats and prints the results as specified in the problem statement.\n    \"\"\"\n    \n    # Physical parameters and initial conditions\n    m = 1.66053906660e-27  # Mass in kg\n    k = 1.0                # Spring constant in N/m\n    x0 = 1.0e-9            # Initial position in m\n    v0 = 0.0               # Initial velocity in m/s\n    \n    # Simulation parameters\n    T_total = 2.0e-11      # Total simulation time in s\n    dts = [1.0e-15, 2.0e-15, 4.0e-15, 3.0e-14] # Time step test suite in s\n    \n    # Stability detection rule\n    STABILITY_REL_ENERGY_THRESHOLD = 1e6\n\n    # Initial energy, E(0)\n    E0 = 0.5 * k * x0**2 + 0.5 * m * v0**2\n\n    final_results = []\n    stable_run_data = []\n\n    for dt in dts:\n        # --- Simulation for a single time step dt ---\n        # Initialization\n        x, v = x0, v0\n        t = 0.0\n        \n        times = [t]\n        energies = [E0]\n        \n        stable = True\n        \n        # Initial acceleration a(t=0)\n        a = -k * x / m\n        \n        # Run the integrator up to total time T\n        num_steps = int(np.ceil(T_total / dt))\n        for _ in range(num_steps):\n            # Velocity Verlet Algorithm\n            # 1. Update position x(t + dt)\n            x_new = x + v * dt + 0.5 * a * dt**2\n            \n            # 2. Calculate new acceleration a(t + dt)\n            a_new = -k * x_new / m\n            \n            # 3. Update velocity v(t + dt)\n            v_new = v + 0.5 * (a + a_new) * dt\n            \n            # Update state for the next iteration\n            x, v, a = x_new, v_new, a_new\n            t += dt\n            \n            # Calculate and store current energy E(t)\n            current_E = 0.5 * k * x**2 + 0.5 * m * v**2\n            \n            # Append results for this step\n            times.append(t)\n            energies.append(current_E)\n\n            # Check for numerical instability and terminate run if needed\n            is_finite = np.isfinite(current_E)\n            rel_error_exceeded = (E0 > 0 and abs(current_E - E0) / E0 > STABILITY_REL_ENERGY_THRESHOLD)\n            \n            if not is_finite or rel_error_exceeded:\n                stable = False\n                break\n    \n        # --- Post-processing for this dt ---\n        times_arr = np.array(times)\n        energies_arr = np.array(energies)\n        \n        # Filter out non-finite values that could result from instability\n        finite_indices = np.isfinite(energies_arr)\n        valid_times = times_arr[finite_indices]\n        valid_energies = energies_arr[finite_indices]\n\n        # 1. Compute absolute energy deviation A(dt)\n        if valid_energies.size > 0:\n            A_dt = np.max(np.abs(valid_energies - E0))\n        else:\n            A_dt = np.inf\n\n        # 2. Estimate linear energy drift D(dt) via least-squares\n        if valid_times.size > 1:\n            # linregress returns: slope, intercept, r-value, p-value, stderr\n            D_dt, _, _, _, _ = linregress(valid_times, valid_energies)\n        else:\n            D_dt = 0.0\n        \n        # Store results for this dt\n        final_results.append([stable, A_dt, D_dt])\n        \n        if stable:\n            stable_run_data.append({'dt': dt, 'A': A_dt})\n\n    # --- Final analysis across stable runs ---\n    # Fit power law A(dt) ~ dt^p by linear regression of log(A) on log(dt)\n    if len(stable_run_data) > 1:\n        log_dts = np.log([d['dt'] for d in stable_run_data])\n        log_As = np.log([d['A'] for d in stable_run_data])\n        # The slope of the log-log plot is the scaling exponent p\n        p, _, _, _, _ = linregress(log_dts, log_As)\n    else:\n        # This case should not be reached with the given problem parameters\n        p = np.nan\n\n    # --- Format final output string ---\n    output_parts = []\n    for result_item in final_results:\n        stable_str = str(result_item[0]).lower()\n        A_val = result_item[1]\n        D_val = result_item[2]\n        output_parts.append(f'[{stable_str},{A_val},{D_val}]')\n    \n    output_parts.append(str(p))\n    \n    # Print the single-line output in the exact required format\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "2651929"}, {"introduction": "Many simulations are designed to sample the canonical ($NVT$) ensemble, requiring a thermostat to maintain a constant average temperature. It is essential to verify that the thermostat correctly generates the ensemble's statistical properties, a direct consequence of the Boltzmann distribution. This advanced practice challenges you to design and implement a rigorous statistical goodness-of-fit test [@problem_id:2652001] to validate that the sampled kinetic energies follow their theoretical Gamma distribution, a critical skill for ensuring the scientific reliability of simulation results.", "problem": "You are tasked with designing and implementing a rigorous, programmatic statistical test to verify whether a Molecular Dynamics (MD) thermostat produces the canonical kinetic energy distribution for a system with a known number of quadratic degrees of freedom. In the canonical ensemble at temperature corresponding to inverse thermal energy $\\beta$, the instantaneous scalar kinetic energy $K \\ge 0$ of a system with $f$ independent quadratic degrees of freedom is predicted to follow the distribution\n$$\nP(K) \\propto K^{\\frac{f}{2} - 1} e^{-\\beta K}.\n$$\nThis is the Gamma family with shape parameter $k = \\frac{f}{2}$ and scale $\\theta = \\frac{1}{\\beta}$.\n\nStart from the base of the following facts:\n- The Boltzmann distribution implies that in the canonical ensemble, the probability density is proportional to $\\exp(-\\beta H)$ for Hamiltonian $H$.\n- For a system with $f$ independent quadratic velocity modes, the kinetic energy is a sum of $f$ quadratic terms, yielding a Gamma distribution for $K$ with shape $k = \\frac{f}{2}$.\n- The probability integral transform states that if $X$ has continuous cumulative distribution function $F$, then $U = F(X)$ is uniformly distributed on $[0,1]$.\n\nYour task is to propose, justify, and implement a hypothesis test with the following null hypothesis and operational constraints:\n- Null hypothesis $H_0$: The sample of kinetic energies $\\{K_i\\}_{i=1}^n$ is independent and identically distributed according to the canonical prediction with known $f$ (hence known shape $k = \\frac{f}{2}$) and an unknown $\\beta > 0$ to be estimated from the sample.\n- Test design constraints:\n  1. You must estimate $\\beta$ from the data under $H_0$ using only quantities justified by maximum likelihood for fixed $k$.\n  2. You must transform the sample using the fitted-model cumulative distribution function to obtain values that should be independent and identically distributed Uniform on $[0,1]$ under $H_0$.\n  3. You must compute a distribution-free goodness-of-fit statistic that is sensitive across the support, and calibrate its sampling distribution under $H_0$ even though $\\beta$ is estimated. Analytic calibration must not assume a known parameter; you must instead use a principled resampling method justified from first principles and consistent with $H_0$.\n\nImplementation requirements:\n- Write a complete and runnable program that constructs the following synthetic test datasets (dimensionless energies; no physical units are required), applies your test to each dataset, and outputs a single line containing a list of boolean decisions indicating whether $H_0$ is accepted at significance level $\\alpha = 0.05$ for each test case. Use the following deterministic dataset construction rules:\n  - For any “pure Gamma” dataset, use the inverse cumulative distribution function to construct a deterministically distributed sample of size $n$ at evenly spaced quantiles $u_i = \\frac{i - \\frac{1}{2}}{n}$ for $i = 1,\\dots,n$, by taking $K_i = F^{-1}_{\\Gamma}(u_i; k, \\theta)$ with $k = \\frac{f}{2}$ and $\\theta = \\frac{1}{\\beta_{\\text{true}}}$.\n  - For any “mixture” dataset, form half the sample from inverse cumulative distribution function quantiles of a Gamma distribution with $(k, \\theta_1)$ and the other half from $(k, \\theta_2)$, using the evenly spaced quantiles for each half.\n- Your test must implement:\n  - The maximum likelihood estimator of $\\beta$ for fixed $k = \\frac{f}{2}$ obtained from the sample mean $\\overline{K}$.\n  - A probability integral transform $u_i = F_{\\Gamma}(K_i; k, \\hat{\\theta})$ where $\\hat{\\theta} = \\frac{1}{\\hat{\\beta}}$.\n  - A one-sample Kolmogorov–Smirnov statistic computed from $\\{u_i\\}$, calibrated by a parametric bootstrap with $B$ synthetic datasets generated under $H_0$ at the fitted parameter $\\hat{\\beta}$, where each bootstrap dataset re-estimates $\\hat{\\beta}^{(b)}$ and recomputes the statistic. Use the unbiased finite-sample p-value convention $p = \\frac{1 + \\#\\{D^{(b)} \\ge D_{\\text{obs}}\\}}{B + 1}$. The bootstrap is the required calibration to account for the parameter estimated from the data.\n- Significance level: $\\alpha = 0.05$ (dimensionless).\n- Angle units are not involved.\n- Final output format: Your program should produce a single line of output containing the decisions for the test suite as a comma-separated Python boolean list, for example, \"[True,False,True]\".\n\nTest suite to implement inside your program:\n- Case $1$ (happy path, moderate $f$): $f = 12$, $\\beta_{\\text{true}} = 0.8$, $n = 2500$, pure Gamma, tested with assumed $f = 12$.\n- Case $2$ (happy path, exponential special case): $f = 2$, $\\beta_{\\text{true}} = 1.5$, $n = 1000$, pure Gamma, tested with assumed $f = 2$.\n- Case $3$ (structured model violation; temperature inhomogeneity): $f = 12$, $n = 2500$, mixture with equal weights of $\\beta_1 = 0.8$ and $\\beta_2 = 1.2$, tested with assumed $f = 12$.\n- Case $4$ (structured model violation; incorrect effective degrees of freedom): true generator $f_{\\text{true}} = 16$ with $\\beta_{\\text{true}} = 1.0$, $n = 3000$, but tested under assumed $f = 12$.\n- Case $5$ (boundary condition; small sample, pure Gamma): $f = 12$, $\\beta_{\\text{true}} = 0.8$, $n = 80$, pure Gamma, tested with assumed $f = 12$.\n\nAll energies are dimensionless. Your program must internally construct these datasets exactly as specified, perform the test with a parametric bootstrap using $B = 500$ replicates with a fixed random seed to ensure deterministic output, and print a single line containing the list of boolean decisions in the order of Cases $1$ through $5$.", "solution": "The task is to design and implement a statistical test to validate if a sample of kinetic energies from a Molecular Dynamics simulation follows the theoretically predicted canonical distribution. The problem is well-posed and scientifically sound, resting on foundational principles of statistical mechanics and statistical inference. We will proceed with a complete, reasoned solution.\n\nThe core of the problem is a goodness-of-fit test for a Gamma distribution where the shape parameter is known, but the scale parameter is unknown and must be estimated from the data.\n\n**1. The Null Hypothesis and the Kinetic Energy Distribution**\n\nThe null hypothesis, $H_0$, states that a sample of kinetic energies $\\{K_i\\}_{i=1}^n$ is independent and identically distributed (i.i.d.) according to the distribution predicted for a system in the canonical ensemble. For a system with $f$ independent quadratic degrees of freedom, the kinetic energy $K$ follows a Gamma distribution. The probability density function (PDF) of a Gamma distribution is given by:\n$$\nP(K; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} K^{k-1} e^{-K/\\theta}\n$$\nwhere $k$ is the shape parameter and $\\theta$ is the scale parameter. In our physical context, the shape parameter is determined by the degrees of freedom, $k = f/2$, and the scale parameter is related to the inverse temperature $\\beta = (k_B T)^{-1}$ by $\\theta = 1/\\beta$. Since the problem statement uses $\\beta$ as the parameter of interest, we can write the PDF as:\n$$\nP(K; k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} K^{k-1} e^{-\\beta K}\n$$\nUnder $H_0$, the shape $k$ is known (as $f$ is given), but $\\beta$ is unknown.\n\n**2. Maximum Likelihood Estimation of the Scale Parameter $\\beta$**\n\nTo perform the test, we must first estimate the unknown parameter $\\beta$ from the observed data $\\{K_i\\}_{i=1}^n$. The method of Maximum Likelihood Estimation (MLE) is specified. The likelihood function is the joint probability of observing the data, which for an i.i.d. sample is the product of the individual probabilities:\n$$\n\\mathcal{L}(\\beta | \\{K_i\\}, k) = \\prod_{i=1}^n P(K_i; k, \\beta) = \\prod_{i=1}^n \\frac{\\beta^k}{\\Gamma(k)} K_i^{k-1} e^{-\\beta K_i} \n$$\nIt is more convenient to work with the log-likelihood function, $\\ln \\mathcal{L}$:\n$$\n\\ln \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left( k \\ln \\beta - \\ln \\Gamma(k) + (k-1) \\ln K_i - \\beta K_i \\right)\n$$\n$$\n\\ln \\mathcal{L}(\\beta) = n k \\ln \\beta - n \\ln \\Gamma(k) + (k-1) \\sum_{i=1}^n \\ln K_i - \\beta \\sum_{i=1}^n K_i\n$$\nTo find the value of $\\beta$ that maximizes this function, we take the derivative with respect to $\\beta$ and set it to zero:\n$$\n\\frac{d \\ln \\mathcal{L}}{d \\beta} = \\frac{n k}{\\beta} - \\sum_{i=1}^n K_i = 0\n$$\nSolving for $\\beta$ gives the MLE, denoted $\\hat{\\beta}$:\n$$\n\\frac{n k}{\\hat{\\beta}} = \\sum_{i=1}^n K_i \\implies \\hat{\\beta} = \\frac{n k}{\\sum_{i=1}^n K_i} = \\frac{k}{\\overline{K}}\n$$\nwhere $\\overline{K} = \\frac{1}{n} \\sum_{i=1}^n K_i$ is the sample mean kinetic energy. The corresponding MLE for the scale parameter $\\theta$ is $\\hat{\\theta} = 1/\\hat{\\beta} = \\overline{K}/k$. This confirms the required estimation procedure.\n\n**3. The Goodness-of-Fit Test**\n\nWith the estimated parameter $\\hat{\\beta}$, we can now test the goodness-of-fit.\n\n**Step 3a: Probability Integral Transform (PIT)**\nThe PIT states that for a continuous random variable $X$ with cumulative distribution function (CDF) $F_X(x)$, the random variable $U = F_X(X)$ is uniformly distributed on $[0, 1]$. We apply this principle to our data. Under the assumption that $H_0$ is true and our estimated parameter $\\hat{\\beta}$ is close to the true value, the transformed values\n$$\nu_i = F_{\\Gamma}(K_i; k, \\hat{\\beta}) = \\int_0^{K_i} P(x; k, \\hat{\\beta}) dx\n$$\nshould form a sample that is approximately i.i.d. from a Uniform($0,1$) distribution. Significant deviation from uniformity constitutes evidence against $H_0$.\n\n**Step 3b: The Kolmogorov-Smirnov (KS) Statistic**\nTo quantify the deviation from uniformity, we use the one-sample Kolmogorov-Smirnov (KS) statistic. We first compute the empirical distribution function (EDF) of the transformed sample $\\{u_i\\}_{i=1}^n$. Let $u_{(i)}$ be the sorted values. The EDF is:\n$$\nE_n(u) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(u_i \\le u)\n$$\nThe KS statistic $D_n$ is the maximum absolute difference between the EDF and the CDF of the target distribution, which in this case is the standard uniform distribution with CDF $F_U(u) = u$ for $u \\in [0,1]$:\n$$\nD_{obs} = \\sup_{u \\in [0,1]} |E_n(u) - u|\n$$\nComputationally, this is given by:\n$$\nD_{obs} = \\max_{i=1, \\dots, n} \\left\\{ \\max\\left( \\frac{i}{n} - u_{(i)}, u_{(i)} - \\frac{i-1}{n} \\right) \\right\\}\n$$\n\n**4. Calibration with Parametric Bootstrap**\n\nA critical point is that the standard critical values for the KS statistic are not applicable here. The standard KS test assumes that the parameters of the hypothesized distribution are pre-specified and *not* estimated from the data. By using $\\hat{\\beta}$ estimated from the data, we have tied the null-hypothesis distribution to the specific sample, which generally leads to a smaller $D_{obs}$ than would be expected if the true $\\beta$ were used. This makes the standard test conservative (less likely to reject).\n\nTo obtain a correct p-value, we must determine the sampling distribution of $D_n$ under $H_0$ *given that the parameter is estimated from the data*. This is achieved using a parametric bootstrap, which simulates the entire process of data generation and testing under the null hypothesis.\n\nThe procedure is as follows:\n1.  From the original data $\\{K_i\\}$, calculate $\\overline{K}$ and the MLE $\\hat{\\beta} = k/\\overline{K}$.\n2.  Compute the observed test statistic $D_{obs}$ as described above.\n3.  Generate $B$ bootstrap samples. For each replicate $b=1, \\dots, B$:\n    a.  Generate a synthetic dataset $\\{K_i^{*(b)}\\}_{i=1}^n$ by drawing $n$ random variates from the fitted Gamma distribution, $\\text{Gamma}(k, 1/\\hat{\\beta})$.\n    b.  For this synthetic dataset, compute its own MLE of the parameter: $\\hat{\\beta}^{*(b)} = k/\\overline{K}^{*(b)}$.\n    c.  Apply the PIT to the synthetic data using its own estimated parameter: $u_i^{*(b)} = F_{\\Gamma}(K_i^{*(b)}; k, 1/\\hat{\\beta}^{*(b)})$.\n    d.  Compute the KS statistic for this replicate, $D^{(b)}$.\n4.  The collection of $\\{D^{(b)}\\}_{b=1}^B$ serves as an empirical approximation of the null distribution of our test statistic.\n\n**5. Decision Rule**\n\nThe p-value is the probability of observing a test statistic as extreme as or more extreme than $D_{obs}$, assuming $H_0$ is true. Using the bootstrap distribution, we calculate the p-value as:\n$$\np = \\frac{1 + N_{\\text{extreme}}}{B+1}\n$$\nwhere $N_{\\text{extreme}} = \\#\\{D^{(b)} \\ge D_{obs}\\}$ is the number of bootstrap statistics that meet or exceed the observed one. The addition of $1$ to the numerator and denominator is a standard convention to avoid p-values of $0$ and ensures the p-value is properly calibrated under the null.\n\nFinally, we compare the p-value to the pre-specified significance level $\\alpha = 0.05$.\n- If $p < \\alpha$, we reject the null hypothesis $H_0$. This suggests the data did not come from the specified Gamma distribution family. The corresponding output is `False`.\n- If $p \\ge \\alpha$, we fail to reject $H_0$. This means the data are consistent with the null hypothesis. The corresponding output is `True`.\n\nThis complete procedure provides a rigorous, principled test for the validity of the kinetic energy distribution from a simulation, correctly accounting for parameter estimation.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import gamma, kstest\n\ndef solve():\n    \"\"\"\n    Implements and applies a statistical test to verify the canonical kinetic energy distribution\n    from a Molecular Dynamics simulation, as specified in the problem statement.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the bootstrap procedure\n    np.random.seed(42)\n\n    # Parameters for the test\n    B = 500  # Number of bootstrap replicates\n    alpha = 0.05  # Significance level\n\n    # Define the test suite\n    test_cases = [\n        # Case 1: happy path, moderate f\n        {'type': 'pure', 'f_true': 12, 'beta_true': 0.8, 'n': 2500, 'f_test': 12},\n        # Case 2: happy path, exponential special case\n        {'type': 'pure', 'f_true': 2, 'beta_true': 1.5, 'n': 1000, 'f_test': 2},\n        # Case 3: structured model violation; temperature inhomogeneity\n        {'type': 'mixture', 'f_true': 12, 'beta1': 0.8, 'beta2': 1.2, 'n': 2500, 'f_test': 12},\n        # Case 4: structured model violation; incorrect effective degrees of freedom\n        {'type': 'pure', 'f_true': 16, 'beta_true': 1.0, 'n': 3000, 'f_test': 12},\n        # Case 5: boundary condition; small sample, pure Gamma\n        {'type': 'pure', 'f_true': 12, 'beta_true': 0.8, 'n': 80, 'f_test': 12},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # --- 1. Generate Synthetic Dataset as per problem specification ---\n        n = case['n']\n        f_test = case['f_test']\n        k_test = f_test / 2.0\n\n        if case['type'] == 'pure':\n            f_true = case['f_true']\n            k_true = f_true / 2.0\n            beta_true = case['beta_true']\n            theta_true = 1.0 / beta_true\n            quantiles = (np.arange(1, n + 1) - 0.5) / n\n            K_data = gamma.ppf(quantiles, a=k_true, scale=theta_true)\n        elif case['type'] == 'mixture':\n            f_true = case['f_true']\n            k_true = f_true / 2.0\n            beta1, beta2 = case['beta1'], case['beta2']\n            theta1, theta2 = 1.0 / beta1, 1.0 / beta2\n            \n            n1 = n // 2\n            n2 = n - n1\n            \n            quantiles1 = (np.arange(1, n1 + 1) - 0.5) / n1\n            K1 = gamma.ppf(quantiles1, a=k_true, scale=theta1)\n            \n            quantiles2 = (np.arange(1, n2 + 1) - 0.5) / n2\n            K2 = gamma.ppf(quantiles2, a=k_true, scale=theta2)\n            \n            K_data = np.concatenate([K1, K2])\n\n        # --- 2. Perform Hypothesis Test ---\n        # Estimate parameters from data (MLE for Gamma with known shape)\n        K_bar_obs = np.mean(K_data)\n        # Handle case of zero mean, though unlikely with this data generation\n        if K_bar_obs == 0:\n            # Cannot proceed, but this case won't occur in practice here.\n            # We can treat this as a failure to reject.\n            results.append(True) \n            continue\n            \n        beta_hat_obs = k_test / K_bar_obs\n        theta_hat_obs = 1.0 / beta_hat_obs\n\n        # Apply Probability Integral Transform (PIT)\n        u_obs = gamma.cdf(K_data, a=k_test, scale=theta_hat_obs)\n        \n        # Compute the observed Kolmogorov-Smirnov statistic\n        D_obs = kstest(u_obs, 'uniform').statistic\n\n        # --- 3. Parametric Bootstrap Calibration ---\n        D_bootstrap = np.zeros(B)\n        for i in range(B):\n            # a. Generate bootstrap sample from the fitted null model\n            K_star = gamma.rvs(a=k_test, scale=theta_hat_obs, size=n)\n            \n            # b. Re-estimate beta for the bootstrap sample\n            K_bar_star = np.mean(K_star)\n            if K_bar_star == 0: # Extremely unlikely\n                beta_hat_star = np.inf\n                theta_hat_star = 0\n            else:\n                beta_hat_star = k_test / K_bar_star\n                theta_hat_star = 1.0 / beta_hat_star\n            \n            # c. Apply PIT to the bootstrap sample with its own re-estimated parameter\n            u_star = gamma.cdf(K_star, a=k_test, scale=theta_hat_star)\n            \n            # d. Compute and store the KS statistic for the bootstrap replicate\n            D_bootstrap[i] = kstest(u_star, 'uniform').statistic\n            \n        # --- 4. Calculate p-value and make a decision ---\n        num_extreme = np.sum(D_bootstrap >= D_obs)\n        p_value = (1.0 + num_extreme) / (B + 1.0)\n        \n        decision = (p_value >= alpha)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2652001"}]}