{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it's essential to master the fundamental principle of Monte Carlo methods: estimating quantities by random sampling. This first exercise provides a solid foundation by asking you to estimate a simple integral using a plain Monte Carlo scheme. The core of this practice is to move beyond just getting an answer and to rigorously quantify the uncertainty of your estimate, connecting the number of samples to the statistical error through the Central Limit Theorem. This skill is the bedrock of understanding the convergence and reliability of any Monte Carlo simulation [@problem_id:2653261].", "problem": "In equilibrium statistical mechanics, configurational averages of observables can be expressed as expectations with respect to a target probability density function (PDF). Consider the observable $f(x)=x^{2}$ defined on the configuration space $\\Omega=[0,1]$, and suppose the target PDF $\\pi(x)$ is uniform on $\\Omega$, that is, $\\pi(x)=1$ for $x \\in [0,1]$. You wish to estimate the integral $I=\\int_{0}^{1} f(x)\\,dx$ using a plain Monte Carlo (MC) scheme based on independent sampling from $\\pi(x)$.\n\nTask: \n1. Construct the plain MC estimator for $I$ using independent and identically distributed samples $X_{1},\\dots,X_{N}$ drawn from the uniform distribution on $[0,1]$, and give its standard error in terms of $N$.\n2. Using only first principles of expectation and variance under independence, determine the minimal integer sample size $N$ required so that the standard error of the estimator is strictly less than $10^{-3}$.\n\nProvide as your final answer only the minimal integer $N$ (dimensionless). No derivations should appear in the final answer. The final answer must be a single number. If rounding is needed, report the minimal integer satisfying the requirement (do not apply a significant-figures rule to this integer).", "solution": "The problem requires the determination of a minimal sample size for a Monte Carlo estimation based on a specified precision. We begin by formally defining the components of the problem.\n\nThe integral to be estimated is $I = \\int_{0}^{1} f(x) \\, dx$, with the observable function given as $f(x) = x^{2}$ over the configuration space $\\Omega = [0,1]$. The target probability density function is uniform on this interval, $\\pi(x) = 1$ for $x \\in [0,1]$. The integral $I$ is therefore equivalent to the expectation of $f(X)$, where $X$ is a random variable drawn from the uniform distribution $U(0,1)$.\n$$\nI = \\int_{0}^{1} x^{2} \\cdot \\pi(x) \\, dx = \\mathbb{E}_{\\pi}[f(X)] = \\mathbb{E}[X^{2}]\n$$\nThis establishes the first principle: the integral is an expectation.\n\nThe plain Monte Carlo estimator for $I$, which we denote as $\\hat{I}_{N}$, is constructed as the sample mean of the observable $f(x)$ evaluated over $N$ independent and identically distributed (i.i.d.) samples $X_{1}, \\dots, X_{N}$, drawn from the distribution $\\pi(x)$.\n$$\n\\hat{I}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_{i}) = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\n$$\nThis is the estimator for the first part of the task.\n\nThe second part of the first task is to determine the standard error of this estimator, $\\text{SE}(\\hat{I}_{N})$. The standard error is the square root of the variance of the estimator, $\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\text{Var}(\\hat{I}_{N})}$. Due to the i.i.d. nature of the samples, the variance of the sample mean is the population variance of the observable divided by the sample size $N$.\n$$\n\\text{Var}(\\hat{I}_{N}) = \\text{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\\right) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\text{Var}(X_{i}^{2}) = \\frac{N \\cdot \\text{Var}(X^{2})}{N^{2}} = \\frac{\\text{Var}(X^{2})}{N}\n$$\nHere, $\\text{Var}(X^{2})$ is the variance of the observable $f(X) = X^{2}$. The variance of a random variable $Y$ is given by $\\text{Var}(Y) = \\mathbb{E}[Y^{2}] - (\\mathbb{E}[Y])^{2}$. Applying this to our observable $X^{2}$:\n$$\n\\text{Var}(X^{2}) = \\mathbb{E}\\left[(X^{2})^{2}\\right] - \\left(\\mathbb{E}[X^{2}]\\right)^{2} = \\mathbb{E}[X^{4}] - \\left(\\mathbb{E}[X^{2}]\\right)^{2}\n$$\nTo proceed, we must calculate the necessary moments of the uniform distribution $U(0,1)$. The $k$-th moment of a random variable $X \\sim U(0,1)$ is:\n$$\n\\mathbb{E}[X^{k}] = \\int_{0}^{1} x^{k} \\pi(x) \\, dx = \\int_{0}^{1} x^{k} \\, dx = \\left[ \\frac{x^{k+1}}{k+1} \\right]_{0}^{1} = \\frac{1}{k+1}\n$$\nWe require the second and fourth moments:\n$$\n\\mathbb{E}[X^{2}] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\n$$\n\\mathbb{E}[X^{4}] = \\frac{1}{4+1} = \\frac{1}{5}\n$$\nSubstituting these results back into the expression for the variance of the observable:\n$$\n\\text{Var}(X^{2}) = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^{2} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9 - 5}{45} = \\frac{4}{45}\n$$\nWith this result, the standard error of the estimator $\\hat{I}_{N}$ is:\n$$\n\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\frac{\\text{Var}(X^{2})}{N}} = \\sqrt{\\frac{4/45}{N}} = \\frac{2}{\\sqrt{45N}}\n$$\nThis completes the derivation for the first task.\n\nThe second task is to find the minimal integer sample size $N$ such that the standard error is strictly less than $10^{-3}$. We establish the inequality:\n$$\n\\text{SE}(\\hat{I}_{N}) < 10^{-3}\n$$\n$$\n\\frac{2}{\\sqrt{45N}} < 10^{-3}\n$$\nWe now solve this inequality for $N$.\n$$\n\\sqrt{45N} > \\frac{2}{10^{-3}} = 2 \\times 10^{3} = 2000\n$$\nSquaring both sides yields:\n$$\n45N > (2000)^{2} = 4 \\times 10^{6}\n$$\n$$\nN > \\frac{4 \\times 10^{6}}{45}\n$$\nTo determine the limit on $N$, we evaluate the fraction:\n$$\n\\frac{4000000}{45} = \\frac{800000}{9} = 88888.888\\dots = 88888.\\overline{8}\n$$\nThe condition is thus $N > 88888.\\overline{8}$. Since the sample size $N$ must be an integer, the minimal integer value that satisfies this strict inequality is the smallest integer greater than $88888.\\overline{8}$.\n$$\nN_{\\min} = 88889\n$$\nThis is the minimal required sample size.", "answer": "$$\\boxed{88889}$$", "id": "2653261"}, {"introduction": "While simple Monte Carlo is powerful, most systems in physical chemistry have equilibrium distributions that are too complex to sample directly. This is where Markov Chain Monte Carlo methods, like the Metropolis-Hastings algorithm, become indispensable. This hands-on practice challenges you to implement a simulation for a particle on a sphere, a common scenario for molecules with fixed bond lengths. The key difficulty, and the primary learning objective, is to correctly handle the simulation in non-Cartesian coordinates, which requires deriving and applying a Jacobian correction to the acceptance probability to ensure the simulation samples the correct physical distribution [@problem_id:2458841].", "problem": "A single particle is constrained to move on the surface of a sphere of radius $R$ in three-dimensional Euclidean space. The surface is parametrized by spherical coordinates $(\\theta,\\phi)$, where $\\theta \\in [0,\\pi]$ is the polar angle and $\\phi \\in [0,2\\pi)$ is the azimuthal angle. The equilibrium (target) distribution of the particle is uniform with respect to the surface-area measure on the sphere. A proposal in angle space is constructed by adding independent zero-mean increments $\\Delta \\theta$ and $\\Delta \\phi$ to $(\\theta,\\phi)$; the perturbed angles are then mapped back to the canonical ranges by reflecting $\\theta$ at the boundaries $0$ and $\\pi$ and by wrapping $\\phi$ modulo $2\\pi$. The proposal increments $\\Delta \\theta$ and $\\Delta \\phi$ have Gaussian distributions with prescribed standard deviations. The resulting Markov chain must leave the uniform surface-area distribution invariant. The change-of-variables between $(\\theta,\\phi)$ and surface area induces a Jacobian that affects the proposal density on the surface when the kernel is specified in the angular coordinates.\n\nYour tasks are:\n\n- Using only first principles, determine the Jacobian determinant $J(R,\\theta)$ of the transformation from angular coordinates $(\\theta,\\phi)$ to the surface-area element on the sphere of radius $R$. State your result in terms of $R$ and $\\theta$.\n\n- For symmetric angular perturbations (identical forward and reverse distributions in angle space), determine from first principles the multiplicative factor in the acceptance ratio that arises purely from the proposal-density transformation between $(\\theta,\\phi)$ and the surface-area measure, expressed in terms of $\\theta$ and $\\theta'$ for a move $(\\theta,\\phi) \\to (\\theta',\\phi')$.\n\n- Implement a Monte Carlo (MC) simulation of the particle on the sphere using the above angular proposals. Construct two variants:\n  1. A variant that uses the correct proposal-density transformation implied by your Jacobian in its acceptance decision so that the chain is invariant for the uniform surface-area distribution.\n  2. A variant that incorrectly ignores this transformation in its acceptance decision.\n\n- For each variant, estimate the expectation of the observable $f(\\theta,\\phi) = \\cos^2(\\theta)$ under the chainâ€™s stationary distribution, using a fixed seed for reproducibility.\n\nAngles must be in radians. All numerical answers must be expressed as real numbers without units.\n\nTest suite and required outputs:\n\n1. Evaluate the Jacobian determinant $J(R,\\theta)$ at the following parameter pairs:\n   - $(R,\\theta) = (1,\\pi/6)$,\n   - $(R,\\theta) = (2,\\pi/2)$,\n   - $(R,\\theta) = (3,\\pi)$.\n\n2. For symmetric angular perturbations, evaluate the proposal-density Jacobian ratio factor for the following $(\\theta,\\theta')$ pairs:\n   - $(\\theta,\\theta') = (\\pi/12,\\pi/3)$,\n   - $(\\theta,\\theta') = (\\pi/3,5\\pi/12)$.\n\n3. Run a simulation for $N = 200000$ total steps with burn-in $B = 5000$, radius $R = 1$, starting angles $(\\theta_0,\\phi_0) = (1.234,2.345)$, and Gaussian proposal standard deviations $\\sigma_\\theta = 0.3$ and $\\sigma_\\phi = 0.6$. Use a fixed seed equal to $123$. Produce two estimates for $\\mathbb{E}[\\cos^2(\\theta)]$:\n   - One using the acceptance decision that correctly accounts for the proposal-density transformation due to your Jacobian.\n   - One using an acceptance decision that ignores the proposal-density transformation.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows:\n  - The three Jacobian values from item $1$ in the order given.\n  - The two proposal-density Jacobian ratio factors from item $2$ in the order given.\n  - The two Monte Carlo estimates from item $3$ in the order given (first the correct-variant estimate, then the incorrect-variant estimate).\n  For example, the output must have the form $[\\text{J1},\\text{J2},\\text{J3},\\text{F1},\\text{F2},\\text{E\\_correct},\\text{E\\_wrong}]$.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational statistical mechanics, specifically concerning the application of the Metropolis-Hastings algorithm to a non-Cartesian coordinate system. A rigorous validation of the problem statement finds no inconsistencies, ambiguities, or violations of scientific principles. I will therefore proceed with a complete solution derived from first principles.\n\nThe solution is presented in three parts, corresponding to the tasks outlined in the problem statement.\n\nFirst, we determine the Jacobian determinant of the transformation from spherical angular coordinates to the surface-area element on a sphere. A point on the surface of a sphere of radius $R$ is parametrized in Cartesian coordinates $(x, y, z)$ using spherical coordinates $(\\theta, \\phi)$ as:\n$$\n\\vec{r}(\\theta, \\phi) = (R \\sin\\theta \\cos\\phi, R \\sin\\theta \\sin\\phi, R \\cos\\theta)\n$$\nwhere $\\theta \\in [0, \\pi]$ is the polar angle and $\\phi \\in [0, 2\\pi)$ is the azimuthal angle. An infinitesimal surface-area element, $dA$, is given by the magnitude of the cross product of the tangent vectors $\\frac{\\partial\\vec{r}}{\\partial\\theta}$ and $\\frac{\\partial\\vec{r}}{\\partial\\phi}$.\n\nThe partial derivatives are:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} = (R \\cos\\theta \\cos\\phi, R \\cos\\theta \\sin\\phi, -R \\sin\\theta)\n$$\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\phi} = (-R \\sin\\theta \\sin\\phi, R \\sin\\theta \\cos\\phi, 0)\n$$\nThe cross product is:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} = (R^2 \\sin^2\\theta \\cos\\phi, R^2 \\sin^2\\theta \\sin\\phi, R^2 \\sin\\theta \\cos\\theta)\n$$\nThe magnitude of this vector, which represents the Jacobian of the transformation, is:\n$$\nJ(R, \\theta) = \\left\\| \\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} \\right\\| = \\sqrt{(R^2 \\sin^2\\theta \\cos\\phi)^2 + (R^2 \\sin^2\\theta \\sin\\phi)^2 + (R^2 \\sin\\theta \\cos\\theta)^2}\n$$\n$$\nJ(R, \\theta) = \\sqrt{R^4 \\sin^4\\theta (\\cos^2\\phi + \\sin^2\\phi) + R^4 \\sin^2\\theta \\cos^2\\theta} = \\sqrt{R^4 \\sin^2\\theta (\\sin^2\\theta + \\cos^2\\theta)} = \\sqrt{R^4 \\sin^2\\theta}\n$$\nSince $\\theta \\in [0, \\pi]$, $\\sin\\theta \\ge 0$. Therefore, the Jacobian determinant is:\n$$\nJ(R, \\theta) = R^2 \\sin\\theta\n$$\nThis quantity relates the differential area element $dA$ on the sphere to the differential increments in the angular coordinates: $dA = J(R, \\theta) d\\theta d\\phi = R^2 \\sin\\theta d\\theta d\\phi$.\n\nSecond, we determine the factor in the Metropolis-Hastings acceptance ratio that arises from the coordinate transformation. The acceptance probability $\\alpha$ for a move from a state $s$ to a proposed state $s'$ is given by:\n$$\n\\alpha(s \\to s') = \\min\\left(1, \\frac{\\pi(s')}{\\pi(s)} \\frac{g(s' \\to s)}{g(s \\to s')}\\right)\n$$\nHere, $\\pi(s)$ is the target probability density and $g(s \\to s')$ is the proposal probability density from $s$ to $s'$. The states $s$ and $s'$ are points on the sphere. The target distribution is uniform with respect to the surface-area measure, which implies $\\pi(s) = \\text{constant}$ for all points $s$ on the sphere. Consequently, the ratio of target densities $\\frac{\\pi(s')}{\\pi(s)} = 1$.\n\nThe proposal is made in angular coordinates $(\\theta, \\phi)$, not directly on the surface. Let $g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$ be the proposal density in the angular coordinate space. The corresponding proposal density on the sphere's surface, $g(s \\to s')$, must be defined with respect to the surface-area measure $dA$. The probability conservation requires $g(s \\to s') dA' = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi')) d\\theta' d\\phi'$. Using $dA' = J(\\theta') d\\theta' d\\phi'$, we find:\n$$\ng(s \\to s') = \\frac{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))}{J(\\theta')}\n$$\nSimilarly, for the reverse move:\n$$\ng(s' \\to s) = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{J(\\theta)}\n$$\nThe ratio of proposal densities in the acceptance probability is therefore:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))} \\times \\frac{J(\\theta')}{J(\\theta)}\n$$\nThe problem states that the angular perturbations are symmetric, meaning the forward and reverse proposals in angle space have identical distributions. This implies $g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi)) = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$. The ratio thus simplifies to the ratio of the Jacobians:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{J(\\theta')}{J(\\theta)} = \\frac{R^2 \\sin\\theta'}{R^2 \\sin\\theta} = \\frac{\\sin\\theta'}{\\sin\\theta}\n$$\nThis is the required multiplicative factor. The correct acceptance probability is $\\alpha = \\min\\left(1, \\frac{\\sin\\theta'}{\\sin\\theta}\\right)$.\n\nThird, we design the Monte Carlo simulation. The goal is to estimate the expectation value of the observable $f(\\theta, \\phi) = \\cos^2(\\theta)$ for the uniform distribution on the sphere. The theoretical expectation is:\n$$\n\\mathbb{E}[\\cos^2\\theta] = \\frac{\\int_0^{2\\pi} \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta d\\phi}{\\int_0^{2\\pi} \\int_0^\\pi \\sin\\theta \\,d\\theta d\\phi} = \\frac{2\\pi \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta}{4\\pi} = \\frac{1}{2}\\left[-\\frac{\\cos^3\\theta}{3}\\right]_0^\\pi = \\frac{1}{2}\\left(-\\frac{(-1)^3}{3} - \\left(-\\frac{1^3}{3}\\right)\\right) = \\frac{1}{3}\n$$\nThe simulation is implemented as follows:\n1.  Initialize the state $(\\theta_k, \\phi_k)$ at $k=0$ to $(\\theta_0, \\phi_0)$.\n2.  Iterate for $k = 0, \\dots, N-1$:\n    a. Propose a new state $(\\theta_p, \\phi_p)$ by drawing independent increments $\\Delta\\theta$ and $\\Delta\\phi$ from Gaussian distributions $\\mathcal{N}(0, \\sigma_\\theta^2)$ and $\\mathcal{N}(0, \\sigma_\\phi^2)$, respectively.\n    b. Apply boundary conditions. The new polar angle $\\theta_p$ is obtained by reflecting $\\theta_k + \\Delta\\theta$ at the boundaries $0$ and $\\pi$. This is achieved by the transformation $\\theta_p = \\text{mod}(\\theta_k+\\Delta\\theta, 2\\pi)$ followed by $\\theta_p = 2\\pi - \\theta_p$ if $\\theta_p > \\pi$. The new azimuthal angle $\\phi_p$ is obtained by wrapping $\\phi_k + \\Delta\\phi$ modulo $2\\pi$.\n    c. Calculate the acceptance probability $\\alpha$.\n       - **Correct variant**: $\\alpha = \\min\\left(1, \\frac{\\sin\\theta_p}{\\sin\\theta_k}\\right)$.\n       - **Incorrect variant**: The Jacobian factor is ignored. The acceptance probability becomes $\\alpha = \\min(1, 1) = 1$, meaning all moves are accepted.\n    d. Draw a random number $u \\sim U(0,1)$. If $u < \\alpha$, set $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_p, \\phi_p)$. Otherwise, $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_k, \\phi_k)$.\n3.  After a burn-in period of $B$ steps, the expectation $\\mathbb{E}[\\cos^2\\theta]$ is estimated by averaging $\\cos^2(\\theta_k)$ over the remaining $N-B$ steps.\n\nThe incorrect variant samples a probability density that is uniform in $(\\theta, \\phi)$ space, i.e., $p(\\theta, \\phi) \\propto 1$. The expected value under this incorrect distribution is $\\mathbb{E}_{incorrect}[\\cos^2\\theta] = \\frac{1}{2\\pi^2} \\int_0^{2\\pi} d\\phi \\int_0^\\pi \\cos^2\\theta \\,d\\theta = \\frac{1}{\\pi} \\int_0^\\pi \\frac{1+\\cos(2\\theta)}{2}d\\theta = \\frac{1}{2\\pi}[\\theta + \\frac{\\sin(2\\theta)}{2}]_0^\\pi = \\frac{1}{2}$. The simulation results should conform to these theoretical predictions of $1/3$ and $1/2$. The implementation will follow this design precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_mc_simulation(params, correct_jacobian, seed):\n    \"\"\"\n    Runs a Monte Carlo simulation of a particle on a sphere.\n\n    Args:\n        params (tuple): A tuple containing simulation parameters:\n                        (N, B, R, theta0, phi0, sigma_theta, sigma_phi).\n        correct_jacobian (bool): If True, use the correct acceptance criterion.\n                                 If False, use the incorrect one.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated expectation value of cos^2(theta).\n    \"\"\"\n    N, B, R, theta0, phi0, sigma_theta, sigma_phi = params\n    \n    # Initialize a new random number generator for each independent run\n    rng = np.random.default_rng(seed)\n    \n    theta = theta0\n    phi = phi0\n    \n    observable_sum = 0.0\n    samples_collected = 0\n    \n    for step in range(N):\n        # Propose a move in angular coordinates\n        d_theta = rng.normal(0.0, sigma_theta)\n        d_phi = rng.normal(0.0, sigma_phi)\n        \n        theta_prop = theta + d_theta\n        phi_prop = phi + d_phi\n        \n        # Apply boundary conditions\n        # For theta: reflection at 0 and pi\n        # This maps the real line to [0, pi] via folding\n        theta_p = np.mod(theta_prop, 2.0 * np.pi)\n        if theta_p > np.pi:\n            theta_p = 2.0 * np.pi - theta_p\n            \n        # For phi: wrapping modulo 2*pi\n        phi_p = np.mod(phi_prop, 2.0 * np.pi)\n        \n        # Calculate acceptance probability\n        if correct_jacobian:\n            # The target distribution is uniform on the sphere, so pi(s')/pi(s) = 1.\n            # The acceptance probability is determined by the Jacobian factor.\n            sin_theta_k = np.sin(theta)\n            sin_theta_p = np.sin(theta_p)\n            \n            # To avoid division by zero if theta is at a pole (0 or pi).\n            if sin_theta_k  1e-12:\n                # If moving from a pole, the volume element is increasing from zero,\n                # so the move should always be accepted unless the proposed\n                # point is also a pole, in which case the ratio is 1.\n                acceptance_ratio = 1.0 if sin_theta_p  1e-12 else np.inf\n            else:\n                acceptance_ratio = sin_theta_p / sin_theta_k\n            \n            alpha = min(1.0, acceptance_ratio)\n        else:\n            # Incorrect variant: ignore the Jacobian factor.\n            # Since the target density is uniform, the acceptance probability is 1.\n            alpha = 1.0\n            \n        # Accept or reject the move\n        if rng.uniform(0.0, 1.0)  alpha:\n            theta = theta_p\n            phi = phi_p\n            \n        # Collect samples after the burn-in period\n        if step >= B:\n            observable_sum += np.cos(theta)**2\n            samples_collected += 1\n            \n    if samples_collected == 0:\n        return np.nan\n        \n    return observable_sum / samples_collected\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, calculate all required values,\n    and print the final output in the specified format.\n    \"\"\"\n    # ====== Task 1: Evaluate Jacobian determinant J(R, theta) ======\n    # J(R, theta) = R^2 * sin(theta)\n    \n    # Test case 1: (R, theta) = (1, pi/6)\n    R1, theta1 = 1.0, np.pi/6.0\n    J1 = R1**2 * np.sin(theta1)\n    \n    # Test case 2: (R, theta) = (2, pi/2)\n    R2, theta2 = 2.0, np.pi/2.0\n    J2 = R2**2 * np.sin(theta2)\n    \n    # Test case 3: (R, theta) = (3, pi)\n    R3, theta3 = 3.0, np.pi\n    J3 = R3**2 * np.sin(theta3)\n    \n    # ====== Task 2: Evaluate proposal-density Jacobian ratio factor ======\n    # Factor = sin(theta') / sin(theta)\n    \n    # Test case 1: (theta, theta') = (pi/12, pi/3)\n    theta_a1, theta_a2 = np.pi/12.0, np.pi/3.0\n    F1 = np.sin(theta_a2) / np.sin(theta_a1)\n\n    # Test case 2: (theta, theta') = (pi/3, 5*pi/12)\n    theta_b1, theta_b2 = np.pi/3.0, 5.0*np.pi/12.0\n    F2 = np.sin(theta_b2) / np.sin(theta_b1)\n    \n    # ====== Task 3: Run Monte Carlo simulations ======\n    sim_params = (\n        200000,  # N: total steps\n        5000,    # B: burn-in steps\n        1.0,     # R: radius\n        1.234,   # theta0\n        2.345,   # phi0\n        0.3,     # sigma_theta\n        0.6      # sigma_phi\n    )\n    seed = 123\n    \n    # Run simulation with correct Jacobian factor\n    E_correct = run_mc_simulation(sim_params, correct_jacobian=True, seed=seed)\n    \n    # Run simulation with incorrect (ignored) Jacobian factor\n    E_wrong = run_mc_simulation(sim_params, correct_jacobian=False, seed=seed)\n\n    # Collate results\n    results = [J1, J2, J3, F1, F2, E_correct, E_wrong]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2458841"}, {"introduction": "Monte Carlo methods are not limited to sampling static equilibrium properties; they are also a powerful tool for simulating the time-evolution of stochastic systems. This final practice introduces you to the world of kinetic Monte Carlo (KMC), a method for simulating chemical kinetics one reaction event at a time. By modeling a simple isomerization reaction, you will implement the core logic of the Gillespie algorithm, which connects microscopic reaction rates to the macroscopic behavior of the system. This exercise will deepen your understanding of how to simulate dynamic processes and analyze their time-dependent properties and stationary states, bridging the gap between stochastic events and deterministic rate equations [@problem_id:2653266].", "problem": "You are to implement a kinetic Monte Carlo simulation for a continuous-time, two-state isomerization model in physical chemistry, representing the unimolecular reaction $A \\rightleftharpoons B$. The system is modeled as a continuous-time Markov chain with constant transition rates $k_{AB}$ (from $A$ to $B$) and $k_{BA}$ (from $B$ to $A$), where $k_{AB}$ and $k_{BA}$ have units of $\\mathrm{s}^{-1}$. The fundamental base for this problem is the chemical master equation and the definition of a homogeneous Poisson process governing memoryless transitions with a constant hazard rate. The objective is to compute the waiting time distribution for leaving each state and the stationary populations from first principles via simulation and to compare them to the stationary solution implied by the master equation. All time quantities must be expressed in seconds.\n\nTask specification:\n- Implement a kinetic Monte Carlo algorithm (also known as the direct method) that, starting from an initial state, sequentially generates reaction events:\n  - At each event, when the system is in a state, it waits a random time before transitioning to the other state. The hazard rate is constant and is determined by the outgoing reaction rate from the current state.\n  - The algorithm must record the waiting time samples for departures from $A$ and for departures from $B$ separately, and must accumulate the total time spent in each state.\n- For each set of parameters, compute the following quantities:\n  1. The empirical mean waiting time to leave $A$ in seconds, computed from the waiting time samples obtained when the system was in $A$ just before a transition.\n  2. The empirical mean waiting time to leave $B$ in seconds, computed analogously for state $B$.\n  3. The Kolmogorov distance between the empirical cumulative distribution function of waiting times to leave $A$ and the theoretical cumulative distribution function implied by a constant hazard in state $A$. The Kolmogorov distance is the supremum norm between two cumulative distribution functions over all nonnegative times.\n  4. The Kolmogorov distance between the empirical cumulative distribution function of waiting times to leave $B$ and the theoretical cumulative distribution function implied by a constant hazard in state $B$.\n  5. The empirical stationary population of $A$, computed as the long-time fraction of the total simulated time spent in $A$.\n  6. The absolute error between the empirical stationary population of $A$ and the stationary population given by the stationary solution of the chemical master equation for this two-state system. Do not express this as a percentage; it must be a number in the interval $[0,1]$.\n- Your program must use a fixed number of reaction events (jumps) for each test case, must use a fixed pseudorandom seed per test case for reproducibility, and must report the results in the order specified below. All time outputs must be in seconds.\n\nTest suite:\n- Use the following three test cases. Each test case is specified by $(k_{AB}, k_{BA}, N, \\text{initial state}, \\text{seed})$ where $N$ is the number of reaction events (jumps):\n  - Case 1 (balanced rates): $(k_{AB} = 10.0\\ \\mathrm{s}^{-1},\\ k_{BA} = 10.0\\ \\mathrm{s}^{-1},\\ N = 100000,\\ \\text{initial state} = A,\\ \\text{seed} = 12345)$.\n  - Case 2 (moderately asymmetric): $(k_{AB} = 1.0\\ \\mathrm{s}^{-1},\\ k_{BA} = 9.0\\ \\mathrm{s}^{-1},\\ N = 120000,\\ \\text{initial state} = B,\\ \\text{seed} = 24680)$.\n  - Case 3 (strongly asymmetric): $(k_{AB} = 0.1\\ \\mathrm{s}^{-1},\\ k_{BA} = 50.0\\ \\mathrm{s}^{-1},\\ N = 150000,\\ \\text{initial state} = A,\\ \\text{seed} = 13579)$.\n\nImplementation and numerical details:\n- Use the definition of a homogeneous Poisson process with constant hazard in each state to sample waiting times between jumps.\n- The stationary population to be used for the absolute error is the unique stationary solution of the two-state chemical master equation corresponding to these rates.\n- For the Kolmogorov distance calculation, use the exact cumulative distribution function implied by a constant hazard in each state and the empirical cumulative distribution function obtained from the samples. The supremum may be computed over the set of sample points by considering the stepwise nature of the empirical cumulative distribution function.\n- All time quantities must be reported in seconds.\n- All outputs must be floating-point numbers. Round every reported float to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case, output the six quantities in the following order:\n  1. Empirical mean waiting time to leave $A$ (seconds).\n  2. Empirical mean waiting time to leave $B$ (seconds).\n  3. Kolmogorov distance for leaving $A$.\n  4. Kolmogorov distance for leaving $B$.\n  5. Empirical stationary fraction of time in $A$.\n  6. Absolute error between empirical and stationary-solution fraction of time in $A$.\n- Concatenate the results for the three cases in the order Case 1, Case 2, Case 3 into a single flat list. For example, the output should have the form $[x_1, x_2, \\dots, x_{18}]$ where each $x_i$ is a float rounded to six decimals.", "solution": "We model the two-state isomerization $A \\rightleftharpoons B$ as a continuous-time Markov chain with constant transition rates $k_{AB}$ from $A$ to $B$ and $k_{BA}$ from $B$ to $A$, both in units of $\\mathrm{s}^{-1}$. The fundamental base is the chemical master equation for the state probabilities and the characterization of waiting times via a homogeneous Poisson process.\n\n1. Chemical master equation and stationary solution. Let $p_A(t)$ and $p_B(t)$ denote the probabilities of being in $A$ and $B$ at time $t$. The master equation is\n$$\n\\frac{d}{dt} \\begin{bmatrix} p_A(t) \\\\ p_B(t) \\end{bmatrix} =\n\\begin{bmatrix}\n- k_{AB}  k_{BA} \\\\\nk_{AB}  -k_{BA}\n\\end{bmatrix}\n\\begin{bmatrix} p_A(t) \\\\ p_B(t) \\end{bmatrix}.\n$$\nAt stationarity, $dp_A/dt = dp_B/dt = 0$ with $p_A^\\ast + p_B^\\ast = 1$. Solving gives a unique stationary distribution\n$$\np_A^\\ast = \\frac{k_{BA}}{k_{AB} + k_{BA}}, \\qquad p_B^\\ast = \\frac{k_{AB}}{k_{AB} + k_{BA}}.\n$$\nThis stationary solution is the long-time limit of the time average under ergodicity for an irreducible two-state chain with positive rates.\n\n2. Waiting time distribution from a constant hazard. When the system is in state $A$, the time to leave $A$ is governed by a homogeneous Poisson process with hazard rate equal to the outgoing reaction rate $k_{AB}$. The survival function for the waiting time $\\tau_A$ to leave $A$ is\n$$\n\\Pr(\\tau_A  t) = \\exp(-k_{AB} t), \\quad t \\ge 0,\n$$\nand therefore the cumulative distribution function is\n$$\nF_A(t) = 1 - \\exp(-k_{AB} t), \\quad t \\ge 0.\n$$\nSimilarly, for state $B$ with outgoing rate $k_{BA}$, the waiting time $\\tau_B$ has\n$$\nF_B(t) = 1 - \\exp(-k_{BA} t), \\quad t \\ge 0.\n$$\nThus the waiting times out of each state are independent and identically distributed exponential random variables conditioned on being in that state.\n\n3. Kinetic Monte Carlo (Gillespie direct method). Starting from an initial state, the algorithm iterates event by event. If the current state is $A$, draw a uniform random variable $U \\sim \\mathcal{U}(0,1)$ and transform it to an exponential waiting time $\\tau = -\\ln U / k_{AB}$, append this $\\tau$ to the $A$-departure waiting time sample, and accumulate this time into the total time spent in $A$. Then switch the state to $B$. If the current state is $B$, do the same with rate $k_{BA}$ and accumulate into the total time spent in $B$ before switching to $A$. Repeat this for a fixed number $N$ of reaction events. This produces:\n- A list of waiting times observed before leaving $A$, denoted $\\{\\tau_{A,i}\\}_{i=1}^{n_A}$.\n- A list of waiting times observed before leaving $B$, denoted $\\{\\tau_{B,j}\\}_{j=1}^{n_B}$.\n- Total time spent in $A$, $T_A = \\sum_{i=1}^{n_A} \\tau_{A,i}$, and in $B$, $T_B = \\sum_{j=1}^{n_B} \\tau_{B,j}$.\n\nBecause the process is time-homogeneous and irreducible with $k_{AB}  0$ and $k_{BA}  0$, it is ergodic; hence the time-averaged fraction converges to the stationary distribution almost surely:\n$$\n\\hat{p}_A = \\frac{T_A}{T_A + T_B} \\xrightarrow[N \\to \\infty]{} p_A^\\ast.\n$$\n\n4. Estimators and comparisons.\n- Waiting time distribution: The empirical mean waiting times are $\\bar{\\tau}_A = \\frac{1}{n_A} \\sum_{i=1}^{n_A} \\tau_{A,i}$ and $\\bar{\\tau}_B = \\frac{1}{n_B} \\sum_{j=1}^{n_B} \\tau_{B,j}$, both reported in seconds. For exponential distributions, the maximum likelihood estimator for the rate is $\\hat{\\lambda} = 1/\\bar{\\tau}$, but here we directly report the empirical means as requested.\n- Kolmogorov distance: Let $F_{n,A}(t)$ be the empirical cumulative distribution function of $\\{\\tau_{A,i}\\}$ and $F_A(t)$ the theoretical cumulative distribution function associated with a constant hazard in $A$. The Kolmogorov distance is\n$$\nD_A = \\sup_{t \\ge 0} \\left| F_{n,A}(t) - F_A(t) \\right|.\n$$\nFor implementation, if we sort the samples $\\tau_{A,(1)} \\le \\cdots \\le \\tau_{A,(n_A)}$, then the empirical cumulative distribution function is a right-continuous step function with jumps of size $1/n_A$ at these sample points. The supremum can be computed by taking the maximum over the grid of sample points using the well-known one-sample Kolmogorovâ€“Smirnov statistic:\n$$\nD_A = \\max_{1 \\le i \\le n_A} \\max\\left\\{ \\left|F_A(\\tau_{A,(i)}) - \\frac{i}{n_A} \\right|,\\ \\left|F_A(\\tau_{A,(i)}) - \\frac{i-1}{n_A} \\right| \\right\\}.\n$$\nAn analogous computation gives $D_B$ for state $B$.\n- Stationary population: The empirical stationary fraction of time in $A$ is $\\hat{p}_A = T_A/(T_A+T_B)$, and we compare it to the stationary solution $p_A^\\ast$ to compute the absolute error $|\\hat{p}_A - p_A^\\ast|$.\n\n5. Numerical design and test coverage. We simulate a fixed number $N$ of jumps per test case with a fixed pseudorandom seed to ensure reproducibility. The test suite includes:\n- Balanced rates $k_{AB} = k_{BA}$, for which $p_A^\\ast = 1/2$ and both waiting time distributions are identical.\n- Moderately asymmetric rates with a higher occupancy of $A$ but with both states visited frequently.\n- Strongly asymmetric rates with rare departures from $A$ and very fast departures from $B$, stressing both long- and short-time sampling.\n\n6. Output. For each case, we report six floats in order: $\\bar{\\tau}_A$ (seconds), $\\bar{\\tau}_B$ (seconds), $D_A$, $D_B$, $\\hat{p}_A$, $|\\hat{p}_A - p_A^\\ast|$, each rounded to six decimals. Concatenate the three cases into a single flat list $[x_1, x_2, \\dots, x_{18}]$ and print that as the only line of output.\n\nThe Monte Carlo statistical errors scale approximately like $N^{-1/2}$ by the central limit theorem, so increasing $N$ reduces the variability of $\\bar{\\tau}_A$, $\\bar{\\tau}_B$, and $\\hat{p}_A$, and also tightens the Kolmogorov distances on average.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ks_distance_exponential(samples: np.ndarray, rate: float) - float:\n    \"\"\"\n    Compute the Kolmogorov distance between the empirical CDF of samples and the\n    exponential CDF with given rate. Returns np.nan if no samples.\n    \"\"\"\n    n = samples.size\n    if n == 0:\n        return float('nan')\n    # Sort samples\n    x = np.sort(samples)\n    # Theoretical CDF at sample points: F(t) = 1 - exp(-rate * t)\n    Fx = 1.0 - np.exp(-rate * x)\n    # Empirical CDF brackets at each sample: (i-1)/n and i/n\n    i = np.arange(1, n + 1, dtype=np.float64)\n    ecdf_upper = i / n\n    ecdf_lower = (i - 1.0) / n\n    # Compute the KS one-sample statistic (sup over t = 0)\n    d_upper = np.abs(Fx - ecdf_upper)\n    d_lower = np.abs(Fx - ecdf_lower)\n    D = max(np.max(d_upper), np.max(d_lower))\n    return float(D)\n\ndef simulate_two_state_kmc(k_ab: float, k_ba: float, n_jumps: int, init_state: str, seed: int):\n    \"\"\"\n    Simulate a two-state kinetic Monte Carlo for A - B with rates k_ab and k_ba.\n    Returns:\n        mean_tau_A, mean_tau_B, ks_A, ks_B, pi_A_sim, abs_err_pi_A\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Map state: 0 - A, 1 - B\n    state = 0 if init_state.upper() == 'A' else 1\n\n    # Collect waiting times and accumulated times\n    tau_A = []\n    tau_B = []\n    time_A = 0.0\n    time_B = 0.0\n\n    # Predefine rates per state\n    rates = (float(k_ab), float(k_ba))\n\n    # Main KMC loop\n    for _ in range(n_jumps):\n        r = rates[state]\n        # Draw exponential waiting time via inverse transform: tau = -ln(U)/r\n        u = rng.random()\n        # Ensure u in (0,1], avoid log(0)\n        while u == 0.0:\n            u = rng.random()\n        tau = -np.log(u) / r\n        if state == 0:\n            tau_A.append(tau)\n            time_A += tau\n            state = 1  # switch to B\n        else:\n            tau_B.append(tau)\n            time_B += tau\n            state = 0  # switch to A\n\n    tau_A = np.array(tau_A, dtype=np.float64)\n    tau_B = np.array(tau_B, dtype=np.float64)\n\n    mean_tau_A = float(np.mean(tau_A)) if tau_A.size  0 else float('nan')\n    mean_tau_B = float(np.mean(tau_B)) if tau_B.size  0 else float('nan')\n\n    # Kolmogorov distances with theoretical exponential CDFs parameterized by given rates\n    ks_A = ks_distance_exponential(tau_A, k_ab) if tau_A.size  0 else float('nan')\n    ks_B = ks_distance_exponential(tau_B, k_ba) if tau_B.size  0 else float('nan')\n\n    total_time = time_A + time_B\n    pi_A_sim = float(time_A / total_time) if total_time  0 else float('nan')\n\n    # Theoretical stationary fraction in A from stationary solution of the two-state master equation\n    pi_A_th = float(k_ba / (k_ab + k_ba))\n    abs_err_pi_A = abs(pi_A_sim - pi_A_th) if np.isfinite(pi_A_sim) else float('nan')\n\n    return mean_tau_A, mean_tau_B, ks_A, ks_B, pi_A_sim, abs_err_pi_A\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (k_ab [s^-1], k_ba [s^-1], N_jumps, init_state, seed)\n    test_cases = [\n        (10.0, 10.0, 100_000, 'A', 12345),   # Case 1: balanced\n        (1.0, 9.0, 120_000, 'B', 24680),     # Case 2: moderately asymmetric\n        (0.1, 50.0, 150_000, 'A', 13579),    # Case 3: strongly asymmetric\n    ]\n\n    results = []\n    for k_ab, k_ba, n_jumps, init_state, seed in test_cases:\n        mean_tau_A, mean_tau_B, ks_A, ks_B, pi_A_sim, abs_err_pi_A = simulate_two_state_kmc(\n            k_ab, k_ba, n_jumps, init_state, seed\n        )\n        results.extend([\n            mean_tau_A, mean_tau_B, ks_A, ks_B, pi_A_sim, abs_err_pi_A\n        ])\n\n    # Round each float to six decimals and format as required.\n    formatted = [f\"{x:.6f}\" if np.isfinite(x) else \"nan\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2653266"}]}