{"hands_on_practices": [{"introduction": "The Gaussian wavepacket is a cornerstone of quantum mechanics, representing a state with the maximum possible simultaneous localization in position and momentum space. This exercise [@problem_id:2625877] provides a hands-on opportunity to compute the full covariance matrix for position and momentum operators in such a state. By explicitly calculating the variances and covariance, you will directly verify that Gaussian states are minimum uncertainty states, saturating the fundamental limit imposed by the Robertson-Schrödinger uncertainty relation.", "problem": "Consider a single spinless particle moving in one spatial dimension, with position operator $\\hat{x}$ and momentum operator $\\hat{p}$ acting on square-integrable kets $|\\psi\\rangle$ in the standard representation, where $\\hat{x}$ acts by multiplication and $\\hat{p}$ acts as $\\hat{p}=-i\\hbar\\,\\frac{d}{dx}$ on the position-space wavefunction $\\psi(x)=\\langle x|\\psi\\rangle$. The commutator satisfies $[\\hat{x},\\hat{p}]=i\\hbar$. The expectation value of an observable $\\hat{A}$ in the state $|\\psi\\rangle$ is $\\langle \\hat{A}\\rangle=\\langle\\psi|\\hat{A}|\\psi\\rangle$, and the variance is $\\Delta A^{2}=\\langle(\\hat{A}-\\langle\\hat{A}\\rangle)^{2}\\rangle$. The symmetric covariance of $\\hat{x}$ and $\\hat{p}$ is $\\mathrm{Cov}(x,p)=\\frac{1}{2}\\langle(\\hat{x}-\\langle\\hat{x}\\rangle)(\\hat{p}-\\langle\\hat{p}\\rangle)+(\\hat{p}-\\langle\\hat{p}\\rangle)(\\hat{x}-\\langle\\hat{x}\\rangle)\\rangle$.\n\nLet the state $|\\psi\\rangle$ have the normalized position-space wavefunction\n$$\n\\psi(x)=\\left(\\frac{1}{2\\pi s^{2}}\\right)^{1/4}\\exp\\!\\left[-\\frac{(x-x_{0})^{2}}{4s^{2}}+ik_{0}x\\right],\n$$\nwhere $s>0$ is a real width parameter, $x_{0}\\in\\mathbb{R}$ is the mean position, and $k_{0}\\in\\mathbb{R}$ sets the mean momentum. Work in the International System of Units (SI), but express your final answer symbolically in terms of $s$ and $\\hbar$ (do not attach units to the final, boxed expression).\n\nTasks:\n- Starting only from the definitions above, compute $\\langle\\hat{x}\\rangle$, $\\langle\\hat{p}\\rangle$, $\\Delta x^{2}$, $\\Delta p^{2}$, and $\\mathrm{Cov}(x,p)$ for this state.\n- Using the Cauchy–Schwarz inequality on suitable kets built from $(\\hat{x}-\\langle\\hat{x}\\rangle)$ and $(\\hat{p}-\\langle\\hat{p}\\rangle)$, derive the uncertainty relation implied by $[\\hat{x},\\hat{p}]=i\\hbar$ and determine whether the given Gaussian saturates the bound.\n- Report the full $2\\times 2$ covariance matrix\n$$\n\\Sigma=\\begin{pmatrix}\n\\Delta x^{2} & \\mathrm{Cov}(x,p) \\\\\n\\mathrm{Cov}(x,p) & \\Delta p^{2}\n\\end{pmatrix}\n$$\nin closed form in terms of $s$ and $\\hbar$.\n\nAnswer specification:\n- Provide your final answer as the single closed-form analytic expression for $\\Sigma$.\n- Use SI consistently in your reasoning; however, do not include units in the final boxed expression.", "solution": "The provided normalized position-space wavefunction is\n$$\n\\psi(x)=\\left(\\frac{1}{2\\pi s^{2}}\\right)^{1/4}\\exp\\!\\left[-\\frac{(x-x_{0})^{2}}{4s^{2}}+ik_{0}x\\right]\n$$\nwhere $s>0$, $x_{0}\\in\\mathbb{R}$, and $k_{0}\\in\\mathbb{R}$. The corresponding probability density function is given by\n$$\n|\\psi(x)|^{2} = \\psi^{*}(x)\\psi(x) = \\left(\\frac{1}{2\\pi s^{2}}\\right)^{1/2}\\exp\\!\\left[-\\frac{(x-x_{0})^{2}}{2s^{2}}\\right]\n$$\nThis is the probability density function for a normal distribution with mean $\\mu = x_{0}$ and variance $\\sigma^{2} = s^{2}$. We will use this fact to compute the position-related quantities.\n\nFirst, we compute the expectation value of the position operator, $\\langle\\hat{x}\\rangle$.\n$$\n\\langle\\hat{x}\\rangle = \\int_{-\\infty}^{\\infty} x |\\psi(x)|^{2} dx\n$$\nThis integral represents the mean of the probability distribution $|\\psi(x)|^{2}$. By inspection of the Gaussian form, the mean is $x_{0}$.\n$$\n\\langle\\hat{x}\\rangle = x_{0}\n$$\n\nNext, we compute the variance of the position, $\\Delta x^{2}$.\n$$\n\\Delta x^{2} = \\langle(\\hat{x}-\\langle\\hat{x}\\rangle)^{2}\\rangle = \\int_{-\\infty}^{\\infty} (x-x_{0})^{2} |\\psi(x)|^{2} dx\n$$\nThis integral is the definition of the variance for the distribution $|\\psi(x)|^{2}$, which is $s^{2}$.\n$$\n\\Delta x^{2} = s^{2}\n$$\n\nNow, we turn to the momentum-related quantities. The expectation value of the momentum operator, $\\langle\\hat{p}\\rangle$, is calculated as\n$$\n\\langle\\hat{p}\\rangle = \\langle\\psi|\\hat{p}|\\psi\\rangle = \\int_{-\\infty}^{\\infty} \\psi^{*}(x) \\left(-i\\hbar\\frac{d}{dx}\\right) \\psi(x) dx\n$$\nWe first evaluate the action of the momentum operator on the wavefunction $\\psi(x)$.\n$$\n\\hat{p}\\psi(x) = -i\\hbar\\frac{d}{dx}\\left(\\left(\\frac{1}{2\\pi s^{2}}\\right)^{1/4}\\exp\\!\\left[-\\frac{(x-x_{0})^{2}}{4s^{2}}+ik_{0}x\\right]\\right)\n$$\n$$\n= -i\\hbar\\left(-\\frac{2(x-x_{0})}{4s^{2}} + ik_{0}\\right)\\psi(x) = \\left(\\frac{i\\hbar(x-x_{0})}{2s^{2}} + \\hbar k_{0}\\right)\\psi(x)\n$$\nSubstituting this into the integral for $\\langle\\hat{p}\\rangle$:\n$$\n\\langle\\hat{p}\\rangle = \\int_{-\\infty}^{\\infty} \\psi^{*}(x) \\left(\\frac{i\\hbar(x-x_{0})}{2s^{2}} + \\hbar k_{0}\\right) \\psi(x) dx\n$$\n$$\n= \\frac{i\\hbar}{2s^{2}}\\int_{-\\infty}^{\\infty} (x-x_{0}) |\\psi(x)|^{2} dx + \\hbar k_{0}\\int_{-\\infty}^{\\infty} |\\psi(x)|^{2} dx\n$$\nThe first integral is the expectation value of $x-x_0$, which is $\\langle x \\rangle - x_0 = x_0 - x_0 = 0$. The second integral is $1$ because the wavefunction is normalized. Thus,\n$$\n\\langle\\hat{p}\\rangle = \\hbar k_{0}\n$$\n\nTo find the variance of the momentum, $\\Delta p^{2}$, we first compute $\\langle\\hat{p}^{2}\\rangle$.\n$$\n\\langle\\hat{p}^{2}\\rangle = \\langle\\psi|\\hat{p}^{2}|\\psi\\rangle = \\langle\\hat{p}\\psi|\\hat{p}\\psi\\rangle = \\int_{-\\infty}^{\\infty} (\\hat{p}\\psi(x))^{*} (\\hat{p}\\psi(x)) dx\n$$\nUsing the expression for $\\hat{p}\\psi(x)$, we have\n$$\n(\\hat{p}\\psi(x))^{*} = \\left(-\\frac{i\\hbar(x-x_{0})}{2s^{2}} + \\hbar k_{0}\\right)\\psi^{*}(x)\n$$\nTherefore,\n$$\n(\\hat{p}\\psi(x))^{*} (\\hat{p}\\psi(x)) = \\left|\\frac{i\\hbar(x-x_{0})}{2s^{2}} + \\hbar k_{0}\\right|^{2}|\\psi(x)|^{2} = \\left(\\frac{\\hbar^{2}(x-x_{0})^{2}}{4s^{4}} + (\\hbar k_{0})^{2}\\right)|\\psi(x)|^{2}\n$$\nIntegrating this gives $\\langle\\hat{p}^{2}\\rangle$:\n$$\n\\langle\\hat{p}^{2}\\rangle = \\frac{\\hbar^{2}}{4s^{4}}\\int_{-\\infty}^{\\infty} (x-x_{0})^{2} |\\psi(x)|^{2} dx + (\\hbar k_{0})^{2}\\int_{-\\infty}^{\\infty}|\\psi(x)|^{2} dx\n$$\nThe first integral is $\\Delta x^{2} = s^{2}$, and the second is $1$.\n$$\n\\langle\\hat{p}^{2}\\rangle = \\frac{\\hbar^{2}}{4s^{4}}s^{2} + (\\hbar k_{0})^{2} = \\frac{\\hbar^{2}}{4s^{2}} + (\\hbar k_{0})^{2}\n$$\nThe momentum variance is then\n$$\n\\Delta p^{2} = \\langle\\hat{p}^{2}\\rangle - \\langle\\hat{p}\\rangle^{2} = \\left(\\frac{\\hbar^{2}}{4s^{2}} + (\\hbar k_{0})^{2}\\right) - (\\hbar k_{0})^{2} = \\frac{\\hbar^{2}}{4s^{2}}\n$$\n\nFinally, we compute the symmetric covariance, $\\mathrm{Cov}(x,p)$. Let $\\hat{X} = \\hat{x}-\\langle\\hat{x}\\rangle$ and $\\hat{P}=\\hat{p}-\\langle\\hat{p}\\rangle$.\n$$\n\\mathrm{Cov}(x,p) = \\frac{1}{2}\\langle\\hat{X}\\hat{P} + \\hat{P}\\hat{X}\\rangle = \\frac{1}{2}\\langle\\{\\hat{X}, \\hat{P}\\}\\rangle\n$$\nWe have $\\hat{P}\\psi(x) = (\\hat{p}-\\hbar k_{0})\\psi(x) = \\frac{i\\hbar(x-x_{0})}{2s^{2}}\\psi(x) = \\frac{i\\hbar}{2s^{2}}\\hat{X}\\psi(x)$.\nWe can calculate $\\langle\\hat{X}\\hat{P}\\rangle$:\n$$\n\\langle\\hat{X}\\hat{P}\\rangle = \\langle\\psi|\\hat{X}\\hat{P}|\\psi\\rangle = \\langle\\psi|\\hat{X}\\left(\\frac{i\\hbar}{2s^{2}}\\hat{X}\\right)|\\psi\\rangle = \\frac{i\\hbar}{2s^{2}}\\langle\\psi|\\hat{X}^{2}|\\psi\\rangle = \\frac{i\\hbar}{2s^{2}}\\Delta x^{2} = \\frac{i\\hbar}{2s^{2}}s^{2} = \\frac{i\\hbar}{2}\n$$\nThe expectation value $\\langle\\hat{P}\\hat{X}\\rangle$ is the complex conjugate of $\\langle\\hat{X}\\hat{P}\\rangle$:\n$$\n\\langle\\hat{P}\\hat{X}\\rangle = \\langle(\\hat{X}\\hat{P})^{\\dagger}\\rangle = \\langle\\hat{X}\\hat{P}\\rangle^{*} = \\left(\\frac{i\\hbar}{2}\\right)^{*} = -\\frac{i\\hbar}{2}\n$$\nThe covariance is the real part of $\\langle\\hat{X}\\hat{P}\\rangle$:\n$$\n\\mathrm{Cov}(x,p) = \\frac{1}{2}(\\langle\\hat{X}\\hat{P}\\rangle + \\langle\\hat{P}\\hat{X}\\rangle) = \\frac{1}{2}\\left(\\frac{i\\hbar}{2} - \\frac{i\\hbar}{2}\\right) = 0\n$$\n\nThe general uncertainty relation is derived from the Cauchy-Schwarz inequality for the states $|\\alpha\\rangle = (\\hat{x}-\\langle\\hat{x}\\rangle)|\\psi\\rangle$ and $|\\beta\\rangle = (\\hat{p}-\\langle\\hat{p}\\rangle)|\\psi\\rangle$. The inequality is $\\langle\\alpha|\\alpha\\rangle\\langle\\beta|\\beta\\rangle \\ge |\\langle\\alpha|\\beta\\rangle|^{2}$.\nWe have $\\langle\\alpha|\\alpha\\rangle = \\Delta x^{2}$ and $\\langle\\beta|\\beta\\rangle = \\Delta p^{2}$.\nThe term $\\langle\\alpha|\\beta\\rangle = \\langle(\\hat{x}-\\langle\\hat{x}\\rangle)(\\hat{p}-\\langle\\hat{p}\\rangle)\\rangle$. This can be expressed using the commutator and anticommutator:\n$$\n\\langle\\alpha|\\beta\\rangle = \\frac{1}{2}\\langle\\{(\\hat{x}-\\langle\\hat{x}\\rangle),(\\hat{p}-\\langle\\hat{p}\\rangle)\\}\\rangle + \\frac{1}{2}\\langle[\\hat{x}-\\langle\\hat{x}\\rangle,\\hat{p}-\\langle\\hat{p}\\rangle]\\rangle\n$$\n$$\n= \\mathrm{Cov}(x,p) + \\frac{1}{2}\\langle[\\hat{x},\\hat{p}]\\rangle = \\mathrm{Cov}(x,p) + \\frac{i\\hbar}{2}\n$$\nThe uncertainty principle is therefore\n$$\n\\Delta x^{2} \\Delta p^{2} \\ge |\\mathrm{Cov}(x,p) + i\\hbar/2|^{2} = (\\mathrm{Cov}(x,p))^{2} + \\frac{\\hbar^{2}}{4}\n$$\nThis is the Robertson-Schrödinger uncertainty relation. For the given Gaussian state, we check for saturation:\n$$\n\\Delta x^{2}\\Delta p^{2} = (s^{2})\\left(\\frac{\\hbar^{2}}{4s^{2}}\\right) = \\frac{\\hbar^{2}}{4}\n$$\n$$\n(\\mathrm{Cov}(x,p))^{2} + \\frac{\\hbar^{2}}{4} = (0)^{2} + \\frac{\\hbar^{2}}{4} = \\frac{\\hbar^{2}}{4}\n$$\nSince both sides are equal, the Gaussian wave packet saturates the uncertainty bound. Such states are called minimum uncertainty states.\n\nFinally, we construct the $2\\times 2$ covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix} \\Delta x^{2} & \\mathrm{Cov}(x,p) \\\\ \\mathrm{Cov}(x,p) & \\Delta p^{2} \\end{pmatrix}\n$$\nSubstituting the calculated values: $\\Delta x^{2}=s^{2}$, $\\Delta p^{2}=\\frac{\\hbar^{2}}{4s^{2}}$, and $\\mathrm{Cov}(x,p)=0$.\n$$\n\\Sigma = \\begin{pmatrix} s^{2} & 0 \\\\ 0 & \\frac{\\hbar^{2}}{4s^{2}} \\end{pmatrix}\n$$\nThe matrix is diagonal, which indicates that the position and momentum fluctuations are uncorrelated for this state.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ns^{2} & 0 \\\\\n0 & \\frac{\\hbar^{2}}{4s^{2}}\n\\end{pmatrix}\n}\n$$", "id": "2625877"}, {"introduction": "The spectral theorem provides a powerful method not only for understanding the structure of an observable but also for defining functions of operators, such as the time-evolution operator $U(t)=\\exp(-i\\hat{H}t/\\hbar)$. This problem [@problem_id:2625816] guides you through the formal construction of an operator function, $f(\\hat{A})$, for a simple polynomial $f(x)$ in a discrete three-level system. Calculating its expectation value illuminates the general principle that $\\langle f(\\hat{A}) \\rangle$ is an average of the function's values at each eigenvalue, weighted by the measurement probabilities.", "problem": "Consider a three-level quantum system represented in Dirac notation by an orthonormal eigenbasis $\\{|a_1\\rangle, |a_2\\rangle, |a_3\\rangle\\}$ of a self-adjoint (Hermitian) observable operator $\\hat{A}$ with eigenvalues $a_1=-1$, $a_2=0$, and $a_3=2$. A normalized state is prepared as\n$$\n|\\psi\\rangle = \\sqrt{\\frac{1}{2}}\\,\\exp(i\\alpha)\\,|a_1\\rangle \\;+\\; \\sqrt{\\frac{1}{3}}\\,|a_2\\rangle \\;+\\; \\sqrt{\\frac{1}{6}}\\,\\exp(i\\beta)\\,|a_3\\rangle,\n$$\nwhere $\\alpha$ and $\\beta$ are real phases. Let $f(x)$ be the polynomial $f(x)=x^3-2x+1$.\n\nStarting from the postulates that (i) observables are represented by self-adjoint operators on a Hilbert space and (ii) such operators admit a spectral decomposition into orthogonal projectors onto their eigenkets, explain how a function of an operator, $f(\\hat{A})$, is defined via the spectral theorem for a general polynomial $f$. Then, using that construction, compute the expectation value\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} \\equiv \\langle \\psi|\\,f(\\hat{A})\\,|\\psi\\rangle\n$$\nfor the given $f$ and $|\\psi\\rangle$. Give your final answer as an exact number with no units. Do not approximate or round.", "solution": "First, we address the definition of a function of an operator, $f(\\hat{A})$, for a self-adjoint operator $\\hat{A}$ and a polynomial function $f(x)$. A fundamental postulate of quantum mechanics states that observables are represented by self-adjoint operators. A key consequence, part of the spectral theorem for such operators on a finite-dimensional Hilbert space, is that the operator admits a spectral decomposition. Given that $\\hat{A}$ has a complete orthonormal set of eigenvectors $\\{|a_k\\rangle\\}_{k=1}^3$ corresponding to the distinct real eigenvalues $\\{a_k\\}_{k=1}^3$, we can write $\\hat{A}$ as:\n$$\n\\hat{A} = \\sum_{k=1}^3 a_k \\hat{P}_k\n$$\nwhere $\\hat{P}_k = |a_k\\rangle\\langle a_k|$ is the projection operator onto the eigenspace spanned by the eigenvector $|a_k\\rangle$. These projectors are orthogonal, satisfying the property $\\hat{P}_j \\hat{P}_k = \\delta_{jk} \\hat{P}_k$, where $\\delta_{jk}$ is the Kronecker delta. They also form a complete set, meaning $\\sum_{k=1}^3 \\hat{P}_k = \\hat{I}$, where $\\hat{I}$ is the identity operator.\n\nA polynomial function is of the form $f(x) = \\sum_{n=0}^N c_n x^n$. The function of the operator, $f(\\hat{A})$, is naturally defined by substituting the operator $\\hat{A}$ for the variable $x$:\n$$\nf(\\hat{A}) = \\sum_{n=0}^N c_n \\hat{A}^n\n$$\nTo evaluate this expression using the spectral decomposition, we first consider powers of $\\hat{A}$. For $\\hat{A}^2$, we have:\n$$\n\\hat{A}^2 = \\left(\\sum_{j=1}^3 a_j \\hat{P}_j\\right) \\left(\\sum_{k=1}^3 a_k \\hat{P}_k\\right) = \\sum_{j=1}^3 \\sum_{k=1}^3 a_j a_k \\hat{P}_j \\hat{P}_k = \\sum_{j=1}^3 \\sum_{k=1}^3 a_j a_k \\delta_{jk} \\hat{P}_k = \\sum_{k=1}^3 a_k^2 \\hat{P}_k\n$$\nBy induction, it follows that for any non-negative integer $n$, the $n$-th power of $\\hat{A}$ is given by:\n$$\n\\hat{A}^n = \\sum_{k=1}^3 a_k^n \\hat{P}_k\n$$\nSubstituting this into the polynomial expression for $f(\\hat{A})$ yields:\n$$\nf(\\hat{A}) = \\sum_{n=0}^N c_n \\left(\\sum_{k=1}^3 a_k^n \\hat{P}_k\\right) = \\sum_{k=1}^3 \\left(\\sum_{n=0}^N c_n a_k^n\\right) \\hat{P}_k = \\sum_{k=1}^3 f(a_k) \\hat{P}_k\n$$\nThus, the operator $f(\\hat{A})$ acts on any state by applying the scalar function $f$ to the eigenvalues of $\\hat{A}$ and weighting the corresponding projection operators. This is the required construction based on the spectral theorem.\n\nWith this formal definition, we compute the expectation value $\\langle f(\\hat{A})\\rangle_{\\psi} = \\langle \\psi| f(\\hat{A}) |\\psi\\rangle$. We substitute the spectral form of $f(\\hat{A})$:\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = \\left\\langle \\psi \\left| \\left( \\sum_{k=1}^3 f(a_k) |a_k\\rangle\\langle a_k| \\right) \\right| \\psi \\right\\rangle\n$$\nBy linearity of the inner product, this becomes:\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = \\sum_{k=1}^3 f(a_k) \\langle \\psi | a_k \\rangle \\langle a_k | \\psi \\rangle = \\sum_{k=1}^3 f(a_k) |\\langle a_k | \\psi \\rangle|^2\n$$\nThe term $|\\langle a_k | \\psi \\rangle|^2$ is the probability, $P(a_k)$, of obtaining the eigenvalue $a_k$ upon a measurement of the observable $A$ on the system in the state $|\\psi\\rangle$. The formula states that the expectation value of $f(\\hat{A})$ is the weighted average of the values $f(a_k)$, where the weights are the probabilities of measuring the corresponding eigenvalues $a_k$.\n\nThe given state is:\n$$\n|\\psi\\rangle = \\sqrt{\\frac{1}{2}}\\,\\exp(i\\alpha)\\,|a_1\\rangle \\;+\\; \\sqrt{\\frac{1}{3}}\\,|a_2\\rangle \\;+\\; \\sqrt{\\frac{1}{6}}\\,\\exp(i\\beta)\\,|a_3\\rangle\n$$\nThe coefficients are $c_1 = \\sqrt{\\frac{1}{2}}\\exp(i\\alpha)$, $c_2 = \\sqrt{\\frac{1}{3}}$, and $c_3 = \\sqrt{\\frac{1}{6}}\\exp(i\\beta)$. The probabilities are:\n$$\nP(a_1) = |\\langle a_1 | \\psi \\rangle|^2 = |c_1|^2 = \\left|\\sqrt{\\frac{1}{2}}\\exp(i\\alpha)\\right|^2 = \\frac{1}{2}\n$$\n$$\nP(a_2) = |\\langle a_2 | \\psi \\rangle|^2 = |c_2|^2 = \\left|\\sqrt{\\frac{1}{3}}\\right|^2 = \\frac{1}{3}\n$$\n$$\nP(a_3) = |\\langle a_3 | \\psi \\rangle|^2 = |c_3|^2 = \\left|\\sqrt{\\frac{1}{6}}\\exp(i\\beta)\\right|^2 = \\frac{1}{6}\n$$\nNote that the phases $\\alpha$ and $\\beta$ do not influence the probabilities, as is always the case. The sum of probabilities is $\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = 1$, confirming the state is correctly normalized.\n\nThe eigenvalues are $a_1=-1$, $a_2=0$, and $a_3=2$. The polynomial function is $f(x)=x^3-2x+1$. We must evaluate this function at each eigenvalue:\n$$\nf(a_1) = f(-1) = (-1)^3 - 2(-1) + 1 = -1 + 2 + 1 = 2\n$$\n$$\nf(a_2) = f(0) = (0)^3 - 2(0) + 1 = 0 - 0 + 1 = 1\n$$\n$$\nf(a_3) = f(2) = (2)^3 - 2(2) + 1 = 8 - 4 + 1 = 5\n$$\nNow, we can compute the expectation value by substituting these values into the derived formula:\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = \\sum_{k=1}^3 f(a_k) P(a_k) = f(a_1)P(a_1) + f(a_2)P(a_2) + f(a_3)P(a_3)\n$$\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = (2)\\left(\\frac{1}{2}\\right) + (1)\\left(\\frac{1}{3}\\right) + (5)\\left(\\frac{1}{6}\\right)\n$$\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = 1 + \\frac{1}{3} + \\frac{5}{6}\n$$\nTo sum these terms, we use a common denominator of $6$:\n$$\n\\langle f(\\hat{A})\\rangle_{\\psi} = \\frac{6}{6} + \\frac{2}{6} + \\frac{5}{6} = \\frac{6+2+5}{6} = \\frac{13}{6}\n$$\nThis is the final, exact result.", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "2625816"}, {"introduction": "While orthonormal bases simplify many quantum calculations, real-world applications in fields like solid-state physics and quantum chemistry often involve non-orthogonal basis sets. This advanced exercise [@problem_id:2625827] challenges you to work within such a framework by introducing the concept of a biorthogonal dual basis. By constructing the dual kets and using the corresponding resolution of the identity, you will learn how to correctly calculate expectation values in a non-orthogonal representation, showcasing the full generality and power of the Dirac formalism.", "problem": "In a finite-dimensional complex Hilbert space with the standard Hermitian inner product, let $\\{ \\lvert e_1 \\rangle, \\lvert e_2 \\rangle \\}$ be an orthonormal basis, so that $\\langle e_i \\vert e_j \\rangle = \\delta_{ij}$. Consider the non-orthonormal spanning set $\\{ \\lvert 1 \\rangle, \\lvert 2 \\rangle \\}$ defined by\n$\\lvert 1 \\rangle = \\lvert e_1 \\rangle + \\lvert e_2 \\rangle$ and $\\lvert 2 \\rangle = 2 \\lvert e_1 \\rangle + \\lvert e_2 \\rangle$.\nLet $\\{ \\lvert \\tilde{1} \\rangle, \\lvert \\tilde{2} \\rangle \\}$ denote the biorthogonal dual set, characterized by $\\langle \\tilde{m} \\vert n \\rangle = \\delta_{mn}$ and the associated completeness (resolution of the identity) relation $\\hat{I} = \\sum_{n=1}^{2} \\lvert n \\rangle \\langle \\tilde{n} \\vert$.\n\nLet the observable (Hermitian operator) $\\hat{A}$ be given in the orthonormal basis $\\{ \\lvert e_1 \\rangle, \\lvert e_2 \\rangle \\}$ by the matrix\n$$\nA \\equiv \\begin{pmatrix}\n4 & 1 \\\\\n1 & 2\n\\end{pmatrix},\n$$\nso that $\\hat{A} \\lvert e_j \\rangle = \\sum_{i=1}^{2} A_{ij} \\lvert e_i \\rangle$. Consider the state\n$$\n\\lvert \\psi \\rangle = \\lvert 1 \\rangle + i \\, \\lvert 2 \\rangle,\n$$\nwhere $i$ is the imaginary unit.\n\nStarting only from the definition of the expectation value $\\langle \\hat{A} \\rangle_{\\psi} \\equiv \\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle$, the biorthogonality conditions $\\langle \\tilde{m} \\vert n \\rangle = \\delta_{mn}$, and the completeness relation $\\hat{I} = \\sum_{n=1}^{2} \\lvert n \\rangle \\langle \\tilde{n} \\vert$, derive an explicit expression for $\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle$ in terms of matrix elements of the form $\\langle \\tilde{m} \\vert \\hat{A} \\vert n \\rangle$ and overlaps involving $\\lvert \\psi \\rangle$. Then, by constructing the dual kets $\\lvert \\tilde{1} \\rangle$ and $\\lvert \\tilde{2} \\rangle$ explicitly from the given non-orthonormal set, evaluate $\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle$ exactly. Report the final value as a single exact real number without units.", "solution": "First, we derive the general expression for the expectation value $\\langle \\hat{A} \\rangle_{\\psi} = \\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle$ in the specified form. A general strategy is to insert resolutions of the identity to decompose the expression into matrix elements in a chosen basis. Let us use the identity operator in the form $\\hat{I} = \\sum_{m} \\lvert m \\rangle \\langle \\tilde{m} \\vert$ and also its adjoint $\\hat{I}^{\\dagger} = \\hat{I} = \\sum_{m} \\lvert \\tilde{m} \\rangle \\langle m \\vert$.\nTo obtain the desired matrix elements $\\langle \\tilde{m} \\vert \\hat{A} \\vert n \\rangle$, we can expand $\\hat{A}$ as $\\hat{A} = \\hat{I}\\hat{A}\\hat{I}$:\n$$\n\\hat{A} = \\left(\\sum_m \\lvert m \\rangle \\langle \\tilde{m} \\vert\\right) \\hat{A} \\left(\\sum_n \\lvert \\tilde{n} \\rangle \\langle n \\vert\\right) = \\sum_{m,n} \\langle\\tilde{m} \\vert \\hat{A} \\lvert \\tilde{n} \\rangle \\lvert m \\rangle \\langle n \\vert\n$$\nThis is one representation, but not the most direct for our purpose. Let us instead expand $\\lvert \\psi \\rangle$ and $\\langle \\psi \\vert$ and then compute the sandwich.\nUsing $\\lvert \\psi \\rangle = \\hat{I} \\lvert \\psi \\rangle = \\sum_n \\lvert n \\rangle \\langle \\tilde{n} \\vert \\psi \\rangle$ and $\\langle \\psi \\vert = \\langle \\psi \\vert \\hat{I} = \\sum_m \\langle \\psi \\vert \\tilde{m} \\rangle \\langle m \\vert$.\n$$\n\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle = \\left(\\sum_m \\langle \\psi \\vert \\tilde{m} \\rangle \\langle m \\vert \\right) \\hat{A} \\left(\\sum_n \\lvert n \\rangle \\langle \\tilde{n} \\vert \\psi \\rangle \\right) = \\sum_{m,n} \\langle \\psi \\vert \\tilde{m} \\rangle \\langle m \\vert \\hat{A} \\lvert n \\rangle \\langle \\tilde{n} \\vert \\psi \\rangle.\n$$\nThis expression contains the matrix elements $\\langle m \\vert \\hat{A} \\lvert n \\rangle$. The problem asks for a form involving $\\langle \\tilde{m} \\vert \\hat{A} \\vert n \\rangle$. To achieve this, we insert a different combination of identity operators.\n$$\n\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle = \\langle \\psi \\vert \\hat{I} \\hat{A} \\hat{I} \\vert \\psi \\rangle = \\left( \\sum_m \\langle \\psi \\vert m \\rangle \\langle \\tilde{m} \\vert \\right) \\hat{A} \\left( \\sum_n \\lvert n \\rangle \\langle \\tilde{n} \\vert \\psi \\rangle \\right)\n$$\n$$\n= \\sum_{m,n} \\langle \\psi \\vert m \\rangle \\langle \\tilde{m} \\vert \\hat{A} \\lvert n \\rangle \\langle \\tilde{n} \\vert \\psi \\rangle\n$$\nThis expression meets the requirements. It involves overlaps $\\langle \\psi \\vert m \\rangle$, coefficients $\\langle \\tilde{n} \\vert \\psi \\rangle$, and matrix elements $\\langle \\tilde{m} \\vert \\hat{A} \\lvert n \\rangle$.\n\nNow, we evaluate this expression.\n1.  **Construct the dual basis $\\{ \\lvert \\tilde{m} \\rangle \\}$**.\nLet $\\lvert \\tilde{1} \\rangle = a \\lvert e_1 \\rangle + b \\lvert e_2 \\rangle$ and $\\lvert \\tilde{2} \\rangle = c \\lvert e_1 \\rangle + d \\lvert e_2 \\rangle$. The biorthogonality condition $\\langle \\tilde{m} \\vert n \\rangle = \\delta_{mn}$ gives two systems of linear equations. For $\\lvert \\tilde{1} \\rangle$:\n$$\n\\langle \\tilde{1} \\vert 1 \\rangle = (a^* \\langle e_1 \\vert + b^* \\langle e_2 \\vert) (\\lvert e_1 \\rangle + \\lvert e_2 \\rangle) = a^* + b^* = 1\n$$\n$$\n\\langle \\tilde{1} \\vert 2 \\rangle = (a^* \\langle e_1 \\vert + b^* \\langle e_2 \\vert) (2 \\lvert e_1 \\rangle + \\lvert e_2 \\rangle) = 2 a^* + b^* = 0\n$$\nSolving this system gives $a^* = -1$ and $b^*=2$. Since coefficients can be chosen as real, $a=-1, b=2$. So, $\\lvert \\tilde{1} \\rangle = - \\lvert e_1 \\rangle + 2 \\lvert e_2 \\rangle$.\nFor $\\lvert \\tilde{2} \\rangle$:\n$$\n\\langle \\tilde{2} \\vert 1 \\rangle = c^* + d^* = 0\n$$\n$$\n\\langle \\tilde{2} \\vert 2 \\rangle = 2 c^* + d^* = 1\n$$\nSolving this system gives $c^*=1$ and $d^*=-1$. Thus $c=1, d=-1$, and $\\lvert \\tilde{2} \\rangle = \\lvert e_1 \\rangle - \\lvert e_2 \\rangle$.\nIn the $\\lvert e_i \\rangle$ basis representation: $\\lvert 1 \\rangle = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\lvert 2 \\rangle = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$, $\\lvert \\tilde{1} \\rangle = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$, $\\lvert \\tilde{2} \\rangle = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\n2.  **Calculate the overlaps and coefficients for $\\lvert \\psi \\rangle$**.\nThe coefficients are $c_n = \\langle \\tilde{n} \\vert \\psi \\rangle$.\n$c_1 = \\langle \\tilde{1} \\vert \\psi \\rangle = \\langle \\tilde{1} \\vert (\\lvert 1 \\rangle + i \\lvert 2 \\rangle) = \\langle \\tilde{1} \\vert 1 \\rangle + i \\langle \\tilde{1} \\vert 2 \\rangle = 1 + i(0) = 1$.\n$c_2 = \\langle \\tilde{2} \\vert \\psi \\rangle = \\langle \\tilde{2} \\vert (\\lvert 1 \\rangle + i \\lvert 2 \\rangle) = \\langle \\tilde{2} \\vert 1 \\rangle + i \\langle \\tilde{2} \\vert 2 \\rangle = 0 + i(1) = i$.\nThe overlaps are $\\langle \\psi \\vert m \\rangle = \\langle ( \\lvert 1 \\rangle + i \\lvert 2 \\rangle ) \\vert m \\rangle = \\langle 1 \\vert m \\rangle - i \\langle 2 \\vert m \\rangle$. We need the overlap matrix $S_{mn} = \\langle m \\vert n \\rangle$.\n$S_{11} = \\langle 1 \\vert 1 \\rangle = ( \\langle e_1 \\vert + \\langle e_2 \\vert ) ( \\lvert e_1 \\rangle + \\lvert e_2 \\rangle ) = 1+1 = 2$.\n$S_{12} = \\langle 1 \\vert 2 \\rangle = ( \\langle e_1 \\vert + \\langle e_2 \\vert ) ( 2 \\lvert e_1 \\rangle + \\lvert e_2 \\rangle ) = 2+1 = 3$.\n$S_{21} = \\langle 2 \\vert 1 \\rangle = S_{12}^* = 3$.\n$S_{22} = \\langle 2 \\vert 2 \\rangle = ( 2 \\langle e_1 \\vert + \\langle e_2 \\vert ) ( 2 \\lvert e_1 \\rangle + \\lvert e_2 \\rangle ) = 4+1 = 5$.\nSo, $\\langle \\psi \\vert 1 \\rangle = S_{11} - i S_{21} = 2-3i$.\nAnd $\\langle \\psi \\vert 2 \\rangle = S_{12} - i S_{22} = 3-5i$.\n\n3.  **Calculate the matrix elements $\\langle \\tilde{m} \\vert \\hat{A} \\lvert n \\rangle$**. Let this matrix be $A''$.\n$A''_{11} = \\langle \\tilde{1} \\vert \\hat{A} \\lvert 1 \\rangle = (\\langle e_1 \\vert (-1) + \\langle e_2 \\vert 2) \\hat{A} (\\lvert e_1 \\rangle + \\lvert e_2 \\rangle) = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = -5+6 = 1$.\n$A''_{12} = \\langle \\tilde{1} \\vert \\hat{A} \\lvert 2 \\rangle = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 4 \\end{pmatrix} = -9+8 = -1$.\n$A''_{21} = \\langle \\tilde{2} \\vert \\hat{A} \\lvert 1 \\rangle = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = 5-3 = 2$.\n$A''_{22} = \\langle \\tilde{2} \\vert \\hat{A} \\lvert 2 \\rangle = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 4 \\end{pmatrix} = 9-4 = 5$.\nThus, the matrix is $A'' = \\begin{pmatrix} 1 & -1 \\\\ 2 & 5 \\end{pmatrix}$.\n\n4.  **Assemble the final value**.\nThe expression is a matrix product: $(\\langle \\psi \\vert 1 \\rangle, \\langle \\psi \\vert 2 \\rangle) A'' \\begin{pmatrix} \\langle \\tilde{1} \\vert \\psi \\rangle \\\\ \\langle \\tilde{2} \\vert \\psi \\rangle \\end{pmatrix}$.\n$$\n\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle = \\begin{pmatrix} 2-3i & 3-5i \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 2 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}\n$$\nFirst, multiply the row vector by the matrix:\n$$\n\\begin{pmatrix} (2-3i) \\cdot 1 + (3-5i) \\cdot 2 & (2-3i) \\cdot (-1) + (3-5i) \\cdot 5 \\end{pmatrix}\n= \\begin{pmatrix} 2-3i+6-10i & -2+3i+15-25i \\end{pmatrix}\n= \\begin{pmatrix} 8-13i & 13-22i \\end{pmatrix}\n$$\nNow, multiply by the column vector:\n$$\n\\begin{pmatrix} 8-13i & 13-22i \\end{pmatrix} \\begin{pmatrix} 1 \\\\ i \\end{pmatrix} = (8-13i) \\cdot 1 + (13-22i) \\cdot i = 8 - 13i + 13i - 22i^2 = 8 + 22 = 30\n$$\nThe expectation value is a real number, as required for a Hermitian operator.\n\nFor verification, we can calculate directly in the orthonormal basis.\n$\\lvert \\psi \\rangle = (\\lvert e_1 \\rangle + \\lvert e_2 \\rangle) + i(2 \\lvert e_1 \\rangle + \\lvert e_2 \\rangle) = (1+2i)\\lvert e_1 \\rangle + (1+i)\\lvert e_2 \\rangle$.\n$\\langle \\psi \\vert = (1-2i)\\langle e_1 \\vert + (1-i)\\langle e_2 \\vert$.\n$\\langle \\psi \\vert \\hat{A} \\vert \\psi \\rangle = \\begin{pmatrix} 1-2i & 1-i \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1+2i \\\\ 1+i \\end{pmatrix} = \\begin{pmatrix} 1-2i & 1-i \\end{pmatrix} \\begin{pmatrix} 4(1+2i) + (1+i) \\\\ (1+2i) + 2(1+i) \\end{pmatrix}$\n$= \\begin{pmatrix} 1-2i & 1-i \\end{pmatrix} \\begin{pmatrix} 5+9i \\\\ 3+4i \\end{pmatrix} = (1-2i)(5+9i) + (1-i)(3+4i) = (5+9i-10i+18) + (3+4i-3i+4) = (23-i) + (7+i) = 30$.\nThe result is confirmed.", "answer": "$$\n\\boxed{30}\n$$", "id": "2625827"}]}