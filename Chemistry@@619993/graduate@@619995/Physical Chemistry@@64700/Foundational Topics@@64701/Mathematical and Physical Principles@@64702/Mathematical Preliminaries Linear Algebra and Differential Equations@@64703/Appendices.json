{"hands_on_practices": [{"introduction": "In quantum mechanics and the variational method, trial wavefunctions are often constructed from a basis of simpler functions, a process that is most efficient when the basis is orthonormal. This exercise [@problem_id:2648934] provides essential hands-on practice with the Gram-Schmidt procedure, the fundamental algorithm for converting any set of linearly independent functions into an orthonormal basis. Mastering this technique is a cornerstone for solving a wide variety of problems in physical chemistry.", "problem": "In many variational approximations in physical chemistry, one expands a trial function in a low-degree polynomial basis that is orthonormal on a finite interval. Consider the space $L^{2}([-1,1])$ of square-integrable functions on $[-1,1]$ equipped with the weighted inner product\n$$\\langle f,g\\rangle=\\int_{-1}^{1} f(x)\\,g(x)\\,w(x)\\,dx,$$\nwith weight $w(x)=1$. Starting from the ordered set $\\{1, x, x^2\\}$, apply the Gram–Schmidt orthonormalization procedure (using this inner product) to construct an orthonormal basis $\\{\\varphi_{0}(x),\\varphi_{1}(x),\\varphi_{2}(x)\\}$. Then, identify each $\\varphi_{n}(x)$ with the corresponding normalized Legendre polynomial on $[-1,1]$ for $n=0,1,2$.\n\nProvide the final orthonormal basis explicitly as three closed-form expressions in $x$, in increasing degree order. Your final answer must be a single row matrix containing these three expressions. No rounding is required.", "solution": "The problem as stated is valid. It is a standard, well-posed exercise in applying the Gram-Schmidt orthonormalization procedure to a set of basis functions in a function space, a fundamental technique in physical chemistry and quantum mechanics. All necessary components—the initial set of functions, the interval, and the definition of the inner product—are provided and are mathematically and scientifically sound. We shall proceed with the solution.\n\nLet the initial, linearly independent set of functions be denoted as $\\{v_{0}(x), v_{1}(x), v_{2}(x)\\}$, where $v_{0}(x) = 1$, $v_{1}(x) = x$, and $v_{2}(x) = x^{2}$. The inner product on the space $L^{2}([-1,1])$ is given by $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)g(x)dx$, as the weight function is $w(x) = 1$. The Gram-Schmidt procedure generates an orthogonal basis $\\{u_{0}(x), u_{1}(x), u_{2}(x)\\}$ which is then normalized to produce the orthonormal basis $\\{\\varphi_{0}(x), \\varphi_{1}(x), \\varphi_{2}(x)\\}$, where $\\varphi_{n}(x) = \\frac{u_{n}(x)}{\\|u_{n}(x)\\|}$ and $\\|f(x)\\| = \\sqrt{\\langle f, f \\rangle}$.\n\nStep 1: Construct $\\varphi_{0}(x)$.\nThe first orthogonal function is $u_{0}(x) = v_{0}(x) = 1$.\nWe calculate its norm squared:\n$$ \\|u_{0}(x)\\|^{2} = \\langle 1, 1 \\rangle = \\int_{-1}^{1} (1)(1) \\, dx = [x]_{-1}^{1} = 1 - (-1) = 2. $$\nThe norm is $\\|u_{0}(x)\\| = \\sqrt{2}$.\nThe first orthonormal function is:\n$$ \\varphi_{0}(x) = \\frac{u_{0}(x)}{\\|u_{0}(x)\\|} = \\frac{1}{\\sqrt{2}}. $$\nThis corresponds to the normalized Legendre polynomial of degree $0$, as $P_{0}(x) = 1$ and the normalization constant on $[-1,1]$ is $\\sqrt{\\frac{2(0)+1}{2}} = \\frac{1}{\\sqrt{2}}$.\n\nStep 2: Construct $\\varphi_{1}(x)$.\nThe second orthogonal function $u_{1}(x)$ is found by subtracting the projection of $v_{1}(x)$ onto $u_{0}(x)$:\n$$ u_{1}(x) = v_{1}(x) - \\frac{\\langle v_{1}, u_{0} \\rangle}{\\|u_{0}\\|^{2}} u_{0}(x). $$\nFirst, compute the inner product $\\langle v_{1}, u_{0} \\rangle$:\n$$ \\langle v_{1}, u_{0} \\rangle = \\langle x, 1 \\rangle = \\int_{-1}^{1} x \\cdot 1 \\, dx = \\left[\\frac{x^{2}}{2}\\right]_{-1}^{1} = \\frac{1^{2}}{2} - \\frac{(-1)^{2}}{2} = 0. $$\nSince the inner product is $0$, the functions $v_{1}(x)=x$ and $u_{0}(x)=1$ are already orthogonal. Thus,\n$$ u_{1}(x) = v_{1}(x) = x. $$\nNow, we calculate the norm squared of $u_{1}(x)$:\n$$ \\|u_{1}(x)\\|^{2} = \\langle x, x \\rangle = \\int_{-1}^{1} x^{2} \\, dx = \\left[\\frac{x^{3}}{3}\\right]_{-1}^{1} = \\frac{1^{3}}{3} - \\frac{(-1)^{3}}{3} = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}. $$\nThe norm is $\\|u_{1}(x)\\| = \\sqrt{\\frac{2}{3}}$.\nThe second orthonormal function is:\n$$ \\varphi_{1}(x) = \\frac{u_{1}(x)}{\\|u_{1}(x)\\|} = \\frac{x}{\\sqrt{2/3}} = \\sqrt{\\frac{3}{2}}x. $$\nThis corresponds to the normalized Legendre polynomial of degree $1$, as $P_{1}(x) = x$ and the normalization constant is $\\sqrt{\\frac{2(1)+1}{2}} = \\sqrt{\\frac{3}{2}}$.\n\nStep 3: Construct $\\varphi_{2}(x)$.\nThe third orthogonal function, $u_{2}(x)$, is found by subtracting the projections of $v_{2}(x)$ onto $u_{0}(x)$ and $u_{1}(x)$:\n$$ u_{2}(x) = v_{2}(x) - \\frac{\\langle v_{2}, u_{0} \\rangle}{\\|u_{0}\\|^{2}} u_{0}(x) - \\frac{\\langle v_{2}, u_{1} \\rangle}{\\|u_{1}\\|^{2}} u_{1}(x). $$\nWe compute the required inner products:\n$$ \\langle v_{2}, u_{0} \\rangle = \\langle x^{2}, 1 \\rangle = \\int_{-1}^{1} x^{2} \\, dx = \\left[\\frac{x^{3}}{3}\\right]_{-1}^{1} = \\frac{2}{3}. $$\n$$ \\langle v_{2}, u_{1} \\rangle = \\langle x^{2}, x \\rangle = \\int_{-1}^{1} x^{3} \\, dx = \\left[\\frac{x^{4}}{4}\\right]_{-1}^{1} = 0. $$\nSubstituting the known values $\\|u_{0}\\|^{2}=2$ and $\\|u_{1}\\|^{2}=2/3$:\n$$ u_{2}(x) = x^{2} - \\frac{2/3}{2}(1) - \\frac{0}{2/3}(x) = x^{2} - \\frac{1}{3}. $$\nNext, we calculate the norm squared of $u_{2}(x)$:\n$$ \\|u_{2}(x)\\|^{2} = \\left\\langle x^{2} - \\frac{1}{3}, x^{2} - \\frac{1}{3} \\right\\rangle = \\int_{-1}^{1} \\left(x^{2} - \\frac{1}{3}\\right)^{2} dx = \\int_{-1}^{1} \\left(x^{4} - \\frac{2}{3}x^{2} + \\frac{1}{9}\\right) dx. $$\n$$ = \\left[\\frac{x^{5}}{5} - \\frac{2}{9}x^{3} + \\frac{1}{9}x\\right]_{-1}^{1} = \\left(\\frac{1}{5} - \\frac{2}{9} + \\frac{1}{9}\\right) - \\left(-\\frac{1}{5} + \\frac{2}{9} - \\frac{1}{9}\\right) = 2\\left(\\frac{1}{5} - \\frac{1}{9}\\right) = 2\\left(\\frac{9-5}{45}\\right) = \\frac{8}{45}. $$\nThe norm is $\\|u_{2}(x)\\| = \\sqrt{\\frac{8}{45}}$.\nThe third orthonormal function is:\n$$ \\varphi_{2}(x) = \\frac{u_{2}(x)}{\\|u_{2}(x)\\|} = \\frac{x^{2} - \\frac{1}{3}}{\\sqrt{8/45}} = \\sqrt{\\frac{45}{8}}\\left(x^{2} - \\frac{1}{3}\\right) = \\sqrt{\\frac{5}{8}}(3x^{2}-1). $$\nTo align this with the standard Legendre polynomial $P_{2}(x) = \\frac{1}{2}(3x^{2}-1)$, we can rewrite $u_{2}(x) = x^{2} - \\frac{1}{3} = \\frac{3x^{2}-1}{3} = \\frac{2}{3}P_{2}(x)$.\nThe normalized Legendre polynomial of degree 2 is $\\sqrt{\\frac{2(2)+1}{2}}P_2(x) = \\sqrt{\\frac{5}{2}}P_2(x)$. Our result is $\\sqrt{\\frac{5}{8}}(3x^2-1) = \\sqrt{\\frac{5}{2}}\\frac{1}{2}(3x^2-1) = \\sqrt{\\frac{5}{2}}P_2(x)$, which matches.\n\nThe resulting orthonormal basis is $\\{\\varphi_{0}(x), \\varphi_{1}(x), \\varphi_{2}(x)\\}$. The explicit forms are:\n$\\varphi_{0}(x) = \\frac{1}{\\sqrt{2}}$\n$\\varphi_{1}(x) = \\sqrt{\\frac{3}{2}}x$\n$\\varphi_{2}(x) = \\sqrt{\\frac{5}{8}}(3x^{2}-1)$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}}x & \\sqrt{\\frac{5}{8}}(3x^2-1)\n\\end{pmatrix}\n}\n$$", "id": "2648934"}, {"introduction": "Once an orthonormal basis is available, a key task is to approximate an arbitrary quantum state as a finite linear combination of basis vectors. This practice [@problem_id:2648901] explores the fundamental limit on the accuracy of such an approximation, a result known as Bessel’s inequality. By deriving the relationship between a state's total norm and its projections onto the basis, you will gain a deeper conceptual understanding of truncation error and develop proficiency in the powerful Dirac bra-ket notation.", "problem": "Consider the complex Hilbert space of quantum states relevant to a molecular system in physical chemistry. Let $\\{\\lvert n\\rangle\\}$ be an orthonormal set of states and let $\\lvert\\psi\\rangle$ be a normalizable state with finite norm $\\lVert\\lvert\\psi\\rangle\\rVert=\\sqrt{\\langle\\psi\\vert\\psi\\rangle}$. Starting only from the axioms of an inner product space (conjugate symmetry, linearity in one slot, and positive definiteness), and the orthonormality of $\\{\\lvert n\\rangle\\}$, perform the following tasks in Dirac bra-ket notation:\n- Translate the statement commonly known as Bessel’s inequality for the orthonormal set $\\{\\lvert n\\rangle\\}$ acting on $\\lvert\\psi\\rangle$.\n- Define the finite-rank approximation $\\lvert\\psi_{N}\\rangle=\\sum_{n=1}^{N}\\lvert n\\rangle\\langle n\\vert\\psi\\rangle$, and derive the tightest upper bound that can be obtained from Bessel’s inequality for the squared truncation error $\\varepsilon_{N}^{2}=\\lVert\\lvert\\psi\\rangle-\\lvert\\psi_{N}\\rangle\\rVert^{2}$ expressed purely in terms of $\\langle\\psi\\vert\\psi\\rangle$ and the overlaps $\\{\\langle n\\vert\\psi\\rangle\\}_{n=1}^{N}$.\n\nProvide your final bound as a single closed-form analytic expression. Do not use inequalities in your final answer. No numerical rounding is required and no physical units are needed for the final expression.", "solution": "The problem is subjected to validation before any attempt at a solution.\n\n**Step 1: Extract Givens**\n- The space is a complex Hilbert space of quantum states.\n- $\\{\\lvert n\\rangle\\}$ is an orthonormal set of states.\n- $\\lvert\\psi\\rangle$ is a normalizable state with a finite norm $\\lVert\\lvert\\psi\\rangle\\rVert=\\sqrt{\\langle\\psi\\vert\\psi\\rangle}$.\n- The axioms of an inner product are provided: conjugate symmetry, linearity in one slot, and positive definiteness.\n- The orthonormality of $\\{\\lvert n\\rangle\\}$ is given, implying $\\langle n | m \\rangle = \\delta_{nm}$.\n- The finite-rank approximation is defined as $\\lvert\\psi_{N}\\rangle=\\sum_{n=1}^{N}\\lvert n\\rangle\\langle n\\vert\\psi\\rangle$.\n- The squared truncation error is defined as $\\varepsilon_{N}^{2}=\\lVert\\lvert\\psi\\rangle-\\lvert\\psi_{N}\\rangle\\rVert^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in the application of linear algebra to quantum mechanics, a core component of physical chemistry. It is well-posed, with all terms clearly defined and a specific, derivable result requested. The language is objective and formal. The problem is self-contained and consistent. It is not trivial and is directly relevant to the specified topic.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A rigorous solution will be constructed.\n\nThe fundamental axioms of the inner product in a complex Hilbert space, using Dirac notation, are as follows:\n$1$. Conjugate symmetry: For any two states $\\lvert\\phi\\rangle$ and $\\lvert\\psi\\rangle$, $\\langle\\phi\\vert\\psi\\rangle = (\\langle\\psi\\vert\\phi\\rangle)^*$.\n$2$. Linearity in the ket: For any states $\\lvert\\phi\\rangle, \\lvert\\psi_1\\rangle, \\lvert\\psi_2\\rangle$ and complex scalars $c_1, c_2$, $\\langle\\phi\\vert(c_1\\lvert\\psi_1\\rangle + c_2\\lvert\\psi_2\\rangle)\\rangle = c_1\\langle\\phi\\vert\\psi_1\\rangle + c_2\\langle\\phi\\vert\\psi_2\\rangle$. This implies anti-linearity in the bra: $\\langle(c_1\\lvert\\psi_1\\rangle + c_2\\lvert\\psi_2\\rangle)\\vert\\phi\\rangle = c_1^*\\langle\\psi_1\\vert\\phi\\rangle + c_2^*\\langle\\psi_2\\vert\\phi\\rangle$.\n$3$. Positive definiteness: For any state $\\lvert\\psi\\rangle$, $\\langle\\psi\\vert\\psi\\rangle \\ge 0$, and $\\langle\\psi\\vert\\psi\\rangle = 0$ if and only if $\\lvert\\psi\\rangle$ is the zero vector. The norm is defined as $\\lVert\\lvert\\psi\\rangle\\rVert = \\sqrt{\\langle\\psi\\vert\\psi\\rangle}$.\n\nThe first task is to translate Bessel's inequality. For an arbitrary normalizable state $\\lvert\\psi\\rangle$ and an orthonormal set of states $\\{\\lvert n\\rangle\\}$ (which can be finite or countably infinite), Bessel's inequality states that the sum of the squared magnitudes of the components of $\\lvert\\psi\\rangle$ along the basis vectors is less than or equal to the squared norm of $\\lvert\\psi\\rangle$. In Dirac notation, the component (or projection coefficient) of $\\lvert\\psi\\rangle$ onto $\\lvert n\\rangle$ is the complex number $c_n = \\langle n\\vert\\psi\\rangle$. Thus, Bessel's inequality is:\n$$ \\sum_{n} |\\langle n\\vert\\psi\\rangle|^2 \\le \\langle\\psi\\vert\\psi\\rangle $$\nThe sum runs over all indices $n$ for which the state $\\lvert n\\rangle$ is defined in the orthonormal set.\n\nThe second task is to derive the tightest upper bound for the squared truncation error $\\varepsilon_{N}^{2}$. The finite-rank approximation is given as $\\lvert\\psi_{N}\\rangle = \\sum_{n=1}^{N}\\lvert n\\rangle\\langle n\\vert\\psi\\rangle$. The squared truncation error is $\\varepsilon_{N}^{2} = \\lVert\\lvert\\psi\\rangle-\\lvert\\psi_{N}\\rangle\\rVert^{2}$.\n\nBy the definition of the norm, we have:\n$$ \\varepsilon_{N}^{2} = \\langle(\\lvert\\psi\\rangle - \\lvert\\psi_{N}\\rangle) \\vert (\\lvert\\psi\\rangle-\\lvert\\psi_{N}\\rangle)\\rangle $$\nWe expand this inner product using the properties of linearity and anti-linearity:\n$$ \\varepsilon_{N}^{2} = \\langle\\psi\\vert\\psi\\rangle - \\langle\\psi\\vert\\psi_N\\rangle - \\langle\\psi_N\\vert\\psi\\rangle + \\langle\\psi_N\\vert\\psi_N\\rangle $$\nWe now evaluate each term involving $\\lvert\\psi_N\\rangle$.\nFirst, $\\langle\\psi_N\\vert\\psi\\rangle$:\n$$ \\langle\\psi_N\\vert\\psi\\rangle = \\left\\langle \\left(\\sum_{n=1}^{N}\\lvert n\\rangle\\langle n\\vert\\psi\\rangle\\right) \\bigg\\vert \\psi \\right\\rangle $$\nUsing anti-linearity in the bra:\n$$ \\langle\\psi_N\\vert\\psi\\rangle = \\sum_{n=1}^{N} (\\langle n\\vert\\psi\\rangle)^* \\langle n\\vert\\psi\\rangle = \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2 $$\nNext, $\\langle\\psi\\vert\\psi_N\\rangle$:\n$$ \\langle\\psi\\vert\\psi_N\\rangle = \\langle\\psi_N\\vert\\psi\\rangle^* = \\left(\\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2\\right)^* $$\nSince the sum of squared magnitudes is a real number, it is equal to its own conjugate.\n$$ \\langle\\psi\\vert\\psi_N\\rangle = \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2 $$\nFinally, $\\langle\\psi_N\\vert\\psi_N\\rangle$:\n$$ \\langle\\psi_N\\vert\\psi_N\\rangle = \\left\\langle \\left(\\sum_{n=1}^{N}\\lvert n\\rangle\\langle n\\vert\\psi\\rangle\\right) \\bigg\\vert \\left(\\sum_{m=1}^{N}\\lvert m\\rangle\\langle m\\vert\\psi\\rangle\\right) \\right\\rangle $$\nUsing bi-linearity:\n$$ \\langle\\psi_N\\vert\\psi_N\\rangle = \\sum_{n=1}^{N}\\sum_{m=1}^{N} (\\langle n\\vert\\psi\\rangle)^* \\langle m\\vert\\psi\\rangle \\langle n\\vert m\\rangle $$\nThe set $\\{\\lvert n\\rangle\\}$ is orthonormal, so $\\langle n\\vert m\\rangle = \\delta_{nm}$.\n$$ \\langle\\psi_N\\vert\\psi_N\\rangle = \\sum_{n=1}^{N}\\sum_{m=1}^{N} (\\langle n\\vert\\psi\\rangle)^* \\langle m\\vert\\psi\\rangle \\delta_{nm} = \\sum_{n=1}^{N} (\\langle n\\vert\\psi\\rangle)^* \\langle n\\vert\\psi\\rangle = \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2 $$\nSubstituting these results back into the expression for $\\varepsilon_{N}^{2}$:\n$$ \\varepsilon_{N}^{2} = \\langle\\psi\\vert\\psi\\rangle - \\left(\\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2\\right) - \\left(\\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2\\right) + \\left(\\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2\\right) $$\nThis simplifies to:\n$$ \\varepsilon_{N}^{2} = \\langle\\psi\\vert\\psi\\rangle - \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2 $$\nThis is an exact expression for the squared truncation error. The problem asks for the \"tightest upper bound\" but also requires the answer to be a closed-form expression without inequalities. This implies that the exact expression for the quantity itself is what is being requested. Indeed, no tighter bound can be formulated; this expression is an identity. The mention of Bessel's inequality serves as context, as the non-negativity of the squared error, $\\varepsilon_{N}^{2} \\ge 0$, directly implies the finite version of Bessel's inequality: $\\langle\\psi\\vert\\psi\\rangle \\ge \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2$. Therefore, the requested expression is precisely this identity.", "answer": "$$\\boxed{\\langle\\psi\\vert\\psi\\rangle - \\sum_{n=1}^{N} |\\langle n\\vert\\psi\\rangle|^2}$$", "id": "2648901"}, {"introduction": "Many problems in physical chemistry, from spin relaxation to 2D spectroscopy, involve the dynamics of matrix-valued quantities, leading to complex systems of coupled linear differential equations. This exercise [@problem_id:2648938] introduces a powerful algebraic technique using the Kronecker product, denoted $A \\otimes B$, to decouple such systems. By finding the eigenvectors and eigenvalues of the composite system operator, you will learn how to transform the coupled dynamics into a set of independent \"normal modes,\" a fundamental concept for analyzing complex relaxation phenomena.", "problem": "Consider a linearized relaxation of a matrix-valued state observable in physical chemistry, modeled as follows. Let $X(t) \\in \\mathbb{C}^{2 \\times 3}$ evolve by the linear ordinary differential equation (ODE)\n$$\n\\frac{d}{dt} X(t) \\;=\\; A X(t) \\;+\\; X(t) B,\n$$\nwhere $A \\in \\mathbb{C}^{2 \\times 2}$ and $B \\in \\mathbb{C}^{3 \\times 3}$ are time-independent matrices representing independent generators acting on the left and right, respectively. Let $x(t) = \\mathrm{vec}(X(t)) \\in \\mathbb{C}^{6}$ be the vectorization of $X(t)$ obtained by stacking its columns. Using the standard vectorization identity $\\mathrm{vec}(A X B) = (B^{\\top} \\otimes A)\\,\\mathrm{vec}(X)$, the dynamics of $x(t)$ is\n$$\n\\frac{d}{dt} x(t) \\;=\\; \\Big(I_{3} \\otimes A \\;+\\; B^{\\top} \\otimes I_{2}\\Big)\\, x(t),\n$$\nwhere $I_{k}$ denotes the $k \\times k$ identity, and $\\otimes$ denotes the Kronecker product.\n\nAssume that $A$ and $B$ are diagonalizable over $\\mathbb{C}$ with spectra $\\{\\alpha_{1}, \\alpha_{2}\\}$ and $\\{\\beta_{1}, \\beta_{2}, \\beta_{3}\\}$, respectively, and that each has a complete set of right eigenvectors. Work from first principles, using only the definitions of eigenvalues and eigenvectors, the basic property $(P \\otimes Q)(u \\otimes v) = (P u) \\otimes (Q v)$ of the Kronecker product, and the vectorization identity stated above.\n\n(a) Show that for any right eigenpair $(v_{i}, \\alpha_{i})$ of $A$ and any right eigenpair $(\\tilde{w}_{j}, \\beta_{j})$ of $B^{\\top}$, the Kronecker product $\\tilde{w}_{j} \\otimes v_{i}$ is an eigenvector of $I_{3} \\otimes A + B^{\\top} \\otimes I_{2}$, and determine the corresponding eigenvalue in terms of $\\alpha_{i}$ and $\\beta_{j}$.\n\n(b) Using part (a), list all eigenvalues of $I_{3} \\otimes A + B^{\\top} \\otimes I_{2}$ as a single row vector of length six, ordered lexicographically by the pair of indices $(i,j)$ with the $A$-index $i \\in \\{1,2\\}$ increasing slowest and, for each fixed $i$, the $B$-index $j \\in \\{1,2,3\\}$ increasing fastest.\n\n(c) Interpret how this eigen-decomposition provides a decoupling into normal modes for the ODE $\\dot{x}(t) = \\big(I_{3} \\otimes A + B^{\\top} \\otimes I_{2}\\big) x(t)$, and write the general form of the solution as a linear combination of these modes.\n\nYour final reported answer must be the row vector from part (b). No numerical evaluation is required, and no units are associated with the quantities in this problem. Express the final answer symbolically, as instructed, without additional commentary.", "solution": "The problem statement is a well-posed exercise in linear algebra, specifically concerning the properties of Kronecker products and their application to systems of linear ordinary differential equations. It is scientifically grounded, mathematically consistent, and all necessary information is provided. Therefore, the problem is valid, and a solution will be provided.\n\nLet the system matrix for the vectorized state $x(t)$ be denoted by $K \\in \\mathbb{C}^{6 \\times 6}$. From the problem statement, this matrix is\n$$\nK = I_{3} \\otimes A + B^{\\top} \\otimes I_{2}.\n$$\nThe problem provides the following information:\n1.  The matrix $A \\in \\mathbb{C}^{2 \\times 2}$ is diagonalizable and has a complete set of right eigenvectors. Let $(v_i, \\alpha_i)$ be the eigenpairs for $i \\in \\{1, 2\\}$, such that $A v_i = \\alpha_i v_i$.\n2.  The matrix $B \\in \\mathbb{C}^{3 \\times 3}$ is diagonalizable and has a complete set of right eigenvectors. Its spectrum is $\\{\\beta_{1}, \\beta_{2}, \\beta_{3}\\}$. The eigenvalues of $B^{\\top}$ are the same as the eigenvalues of $B$. The problem states that $B^{\\top}$ has right eigenpairs $(\\tilde{w}_{j}, \\beta_{j})$ for $j \\in \\{1, 2, 3\\}$, so $B^{\\top} \\tilde{w}_{j} = \\beta_{j} \\tilde{w}_{j}$. Since $B$ is diagonalizable, $B^\\top$ is also diagonalizable, and a complete set of such eigenvectors $\\{\\tilde{w}_{j}\\}$ exists.\n\n(a) We must show that $\\tilde{w}_{j} \\otimes v_{i}$ is an eigenvector of $K$ and determine its corresponding eigenvalue. We apply $K$ to the vector $\\tilde{w}_{j} \\otimes v_{i}$ and use the distributive property of matrix multiplication:\n$$\nK (\\tilde{w}_{j} \\otimes v_{i}) = (I_{3} \\otimes A + B^{\\top} \\otimes I_{2}) (\\tilde{w}_{j} \\otimes v_{i}) = (I_{3} \\otimes A)(\\tilde{w}_{j} \\otimes v_{i}) + (B^{\\top} \\otimes I_{2})(\\tilde{w}_{j} \\otimes v_{i}).\n$$\nNow, we use the fundamental property of the Kronecker product given in the problem, $(P \\otimes Q)(u \\otimes v) = (P u) \\otimes (Q v)$.\nFor the first term, let $P = I_{3}$, $Q = A$, $u = \\tilde{w}_{j}$, and $v = v_{i}$. This gives:\n$$\n(I_{3} \\otimes A)(\\tilde{w}_{j} \\otimes v_{i}) = (I_{3} \\tilde{w}_{j}) \\otimes (A v_{i}).\n$$\nUsing the definitions of the identity matrix ($I_{3} \\tilde{w}_{j} = \\tilde{w}_{j}$) and the eigenvector of $A$ ($A v_{i} = \\alpha_{i} v_{i}$), this simplifies to:\n$$\n\\tilde{w}_{j} \\otimes (\\alpha_{i} v_{i}) = \\alpha_{i} (\\tilde{w}_{j} \\otimes v_{i}).\n$$\nFor the second term, let $P = B^{\\top}$, $Q = I_{2}$, $u = \\tilde{w}_{j}$, and $v = v_{i}$. This gives:\n$$\n(B^{\\top} \\otimes I_{2})(\\tilde{w}_{j} \\otimes v_{i}) = (B^{\\top} \\tilde{w}_{j}) \\otimes (I_{2} v_{i}).\n$$\nUsing the definitions of the eigenvector of $B^{\\top}$ ($B^{\\top} \\tilde{w}_{j} = \\beta_{j} \\tilde{w}_{j}$) and the identity matrix ($I_{2} v_{i} = v_{i}$), this simplifies to:\n$$\n(\\beta_{j} \\tilde{w}_{j}) \\otimes v_{i} = \\beta_{j} (\\tilde{w}_{j} \\otimes v_{i}).\n$$\nCombining these two results, we have:\n$$\nK (\\tilde{w}_{j} \\otimes v_{i}) = \\alpha_{i} (\\tilde{w}_{j} \\otimes v_{i}) + \\beta_{j} (\\tilde{w}_{j} \\otimes v_{i}) = (\\alpha_{i} + \\beta_{j}) (\\tilde{w}_{j} \\otimes v_{i}).\n$$\nThis equation is of the form $K z = \\lambda z$, where $z = \\tilde{w}_{j} \\otimes v_{i}$ and $\\lambda = \\alpha_{i} + \\beta_{j}$. Thus, we have shown from first principles that $\\tilde{w}_{j} \\otimes v_{i}$ is an eigenvector of $K = I_{3} \\otimes A + B^{\\top} \\otimes I_{2}$, and the corresponding eigenvalue is $\\lambda_{ij} = \\alpha_{i} + \\beta_{j}$.\n\n(b) The matrix $A$ has $2$ linearly independent eigenvectors, $\\{v_1, v_2\\}$, which form a basis for $\\mathbb{C}^2$. The matrix $B^{\\top}$ has $3$ linearly independent eigenvectors, $\\{\\tilde{w}_1, \\tilde{w}_2, \\tilde{w}_3\\}$, which form a basis for $\\mathbb{C}^3$. The set of all Kronecker products of these basis vectors, $\\{\\tilde{w}_{j} \\otimes v_{i} \\mid i \\in \\{1,2\\}, j \\in \\{1,2,3\\}\\}$, forms a basis of $2 \\times 3 = 6$ linearly independent vectors for the space $\\mathbb{C}^{6}$. These are the $6$ eigenvectors of the $6 \\times 6$ matrix $K$.\nThe corresponding eigenvalues are $\\lambda_{ij} = \\alpha_i + \\beta_j$ for all combinations of $i \\in \\{1,2\\}$ and $j \\in \\{1,2,3\\}$.\nWe are asked to list these $6$ eigenvalues as a single row vector, ordered lexicographically by the pair $(i,j)$, with the $A$-index $i$ increasing slowest.\nFor $i=1$:\n\\begin{itemize}\n    \\item $(i,j) = (1,1) \\implies \\lambda_{11} = \\alpha_1 + \\beta_1$\n    \\item $(i,j) = (1,2) \\implies \\lambda_{12} = \\alpha_1 + \\beta_2$\n    \\item $(i,j) = (1,3) \\implies \\lambda_{13} = \\alpha_1 + \\beta_3$\n\\end{itemize}\nFor $i=2$:\n\\begin{itemize}\n    \\item $(i,j) = (2,1) \\implies \\lambda_{21} = \\alpha_2 + \\beta_1$\n    \\item $(i,j) = (2,2) \\implies \\lambda_{22} = \\alpha_2 + \\beta_2$\n    \\item $(i,j) = (2,3) \\implies \\lambda_{23} = \\alpha_2 + \\beta_3$\n\\end{itemize}\nThe ordered row vector of eigenvalues is therefore $(\\lambda_{11}, \\lambda_{12}, \\lambda_{13}, \\lambda_{21}, \\lambda_{22}, \\lambda_{23})$.\n\n(c) The system of ordinary differential equations is $\\frac{d}{dt}x(t) = K x(t)$. Since we have found a complete set of $6$ eigenvectors for the matrix $K \\in \\mathbb{C}^{6 \\times 6}$, $K$ is diagonalizable. Let the eigenvectors be $u_{ij} = \\tilde{w}_{j} \\otimes v_{i}$ with eigenvalues $\\lambda_{ij} = \\alpha_i + \\beta_j$. Any solution $x(t)$ can be expressed as a linear combination of these eigenvectors with time-dependent coefficients:\n$$\nx(t) = \\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij}(t) u_{ij}.\n$$\nSubstituting this expansion into the ODE gives:\n$$\n\\frac{d}{dt} \\sum_{i,j} c_{ij}(t) u_{ij} = K \\sum_{i,j} c_{ij}(t) u_{ij}.\n$$\nSince the eigenvectors $u_{ij}$ are constant vectors, we have:\n$$\n\\sum_{i,j} \\frac{dc_{ij}(t)}{dt} u_{ij} = \\sum_{i,j} c_{ij}(t) (K u_{ij}) = \\sum_{i,j} c_{ij}(t) \\lambda_{ij} u_{ij}.\n$$\nBy the linear independence of the eigenvectors $u_{ij}$, we can equate the coefficients for each mode:\n$$\n\\frac{dc_{ij}(t)}{dt} = \\lambda_{ij} c_{ij}(t).\n$$\nThis is a decoupled system of scalar first-order linear ODEs. The vectors $u_{ij}$ are the \"normal modes\" of the system, each evolving independently of the others. The solution for each coefficient is $c_{ij}(t) = c_{ij}(0) \\exp(\\lambda_{ij} t)$, where $c_{ij}(0)$ are constants determined by the initial condition $x(0)$.\nThe general solution for $x(t)$ is then a superposition of these independently evolving normal modes:\n$$\nx(t) = \\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij}(0) \\exp(\\lambda_{ij} t) u_{ij} = \\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij}(0) \\exp((\\alpha_{i} + \\beta_{j}) t) (\\tilde{w}_{j} \\otimes v_{i}).\n$$\nThis eigen-decomposition transforms the coupled system of $6$ differential equations into $6$ independent scalar equations, which is the essence of decoupling into normal modes. Each mode corresponds to a specific combination of relaxation dynamics from the \"left\" ($A$) and \"right\" ($B$) processes, and the relaxation rate of that mode is the sum of the individual rates $\\alpha_i$ and $\\beta_j$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\alpha_{1} + \\beta_{1} & \\alpha_{1} + \\beta_{2} & \\alpha_{1} + \\beta_{3} & \\alpha_{2} + \\beta_{1} & \\alpha_{2} + \\beta_{2} & \\alpha_{2} + \\beta_{3} \\end{pmatrix}}\n$$", "id": "2648938"}]}