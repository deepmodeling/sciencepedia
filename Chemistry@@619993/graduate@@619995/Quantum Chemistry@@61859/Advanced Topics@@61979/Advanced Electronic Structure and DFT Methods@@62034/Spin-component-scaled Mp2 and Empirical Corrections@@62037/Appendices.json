{"hands_on_practices": [{"introduction": "This first exercise provides a foundational numerical experiment to build intuition for the spin-component scaling approach. By abstracting the complex machinery of Møller–Plesset perturbation theory into simple weights ($w_k$) and energy denominators ($\\Delta_k$), we can directly observe the impact of re-weighting correlation components. This practice [@problem_id:2926383] demonstrates how standard Unrestricted MP2 (UMP2) can fail in cases of spin symmetry breaking and how the empirical scaling in SCS-MP2 is designed to correct for the systematic overestimation of same-spin correlation.", "problem": "Construct a minimal, self-contained numerical experiment to illustrate how Unrestricted second-order Møller–Plesset perturbation theory (UMP2) can overestimate same-spin correlation when the reference determinant breaks spin symmetry, and how Spin-Component-Scaled Møller–Plesset perturbation theory (SCS-MP2) reduces the resulting error. Your program must implement the following from first principles.\n\nBegin with Rayleigh–Schrödinger perturbation theory for second-order correlation energy. In a spin-orbital basis, second-order correlation energy is a sum over double excitations from the reference determinant, each term being the squared coupling matrix element divided by the excitation energy difference. In a data-driven abstraction suitable for programming, we model this as two disjoint sets of contributions:\n- a same-spin set, where both excited electrons have the same spin, and\n- an opposite-spin set, where the excited electrons have opposite spins.\n\nFor each excitation class, you are given:\n- a list of positive excitation energy differences, denoted by $\\Delta_k$, and\n- a list of nonnegative squared coupling weights, denoted by $w_k = \\lvert\\langle \\Phi_k | \\hat{V} | \\Phi_0 \\rangle\\rvert^2$.\n\nUsing the second-order perturbation energy definition, and taking $\\Delta_k$ as positive excitation energy gaps, the correlation energy contribution from a class $\\mathcal{C}$ is\n$$\nE_{\\mathcal{C}} = - \\sum_{k \\in \\mathcal{C}} \\frac{w_k}{\\Delta_k}.\n$$\nThus, the unrestricted second-order correlation energy is $E_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}$, where $E_{\\text{os}}$ and $E_{\\text{ss}}$ denote the opposite-spin and same-spin sums, respectively.\n\nSpin-Component-Scaled Møller–Plesset perturbation theory (SCS-MP2) replaces the unscaled sum by a linear combination of the spin-resolved components,\n$$\nE_{\\text{SCS}} = c_{\\text{os}} \\, E_{\\text{os}} + c_{\\text{ss}} \\, E_{\\text{ss}},\n$$\nwith fixed empirical coefficients $c_{\\text{os}}$ and $c_{\\text{ss}}$. Use the canonical values $c_{\\text{os}} = 1.20$ and $c_{\\text{ss}} = 0.33$.\n\nYou are provided with a test suite of three cases. Each case specifies:\n- the opposite-spin list of weights $w_k$ and denominators $\\Delta_k$,\n- the same-spin list of weights $w_k$ and denominators $\\Delta_k$, and\n- a reference correlation energy $E_{\\text{ref}}$ representative of a high-level benchmark (e.g., Full Configuration Interaction in a minimal space).\n\nAll energies are to be computed and reported in hartree. Your program must, for each case, compute:\n- the unrestricted second-order correlation energy $E_{\\text{UMP2}}$,\n- the spin-component-scaled correlation energy $E_{\\text{SCS}}$,\n- the absolute error of UMP2, $\\lvert E_{\\text{UMP2}} - E_{\\text{ref}} \\rvert$,\n- the absolute error of SCS-MP2, $\\lvert E_{\\text{SCS}} - E_{\\text{ref}} \\rvert$, and\n- a boolean flag indicating whether SCS-MP2 strictly reduces the absolute error compared to UMP2.\n\nTest suite (each number shown is dimensionless except where noted; all energies to be reported in hartree):\n- Case A (well-behaved, near-equilibrium-like):\n  - Opposite-spin: weights $[0.010, 0.006]$, denominators $[0.50, 0.60]$.\n  - Same-spin: weights $[0.0005]$, denominators $[0.55]$.\n  - Reference correlation energy $E_{\\text{ref}} = -0.0315$ hartree.\n- Case B (spin-broken with inflated same-spin amplitudes):\n  - Opposite-spin: weights $[0.018, 0.014]$, denominators $[0.30, 0.35]$.\n  - Same-spin: weights $[0.012, 0.010]$, denominators $[0.32, 0.36]$.\n  - Reference correlation energy $E_{\\text{ref}} = -0.1100$ hartree.\n- Case C (edge case with near-degeneracy in same-spin channel):\n  - Opposite-spin: weights $[0.010, 0.008]$, denominators $[0.25, 0.30]$.\n  - Same-spin: weights $[0.0020, 0.0015]$, denominators $[0.050, 0.060]$.\n  - Reference correlation energy $E_{\\text{ref}} = -0.0850$ hartree.\n\nAngle units are not applicable. No percentages are used.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sublist of the form $[e_u, e_s, b]$ with $e_u$ equal to the UMP2 absolute error rounded to six decimal places, $e_s$ equal to the SCS-MP2 absolute error rounded to six decimal places, and $b$ equal to the boolean value true if and only if SCS-MP2 strictly reduces the absolute error. For example, the overall format must be\n$[[e_{u,A}, e_{s,A}, b_A],[e_{u,B}, e_{s,B}, b_B],[e_{u,C}, e_{s,C}, b_C]]$,\nprinted on one line with no additional text.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of quantum mechanical perturbation theory, specifically second-order Møller–Plesset theory (MP2), and its empirical variants like Spin-Component-Scaled MP2 (SCS-MP2). The problem is well-posed, providing a complete and consistent set of definitions, formulas, and data necessary for a unique numerical solution. It avoids subjectivity and ambiguity. We shall proceed to the solution.\n\nThe foundation of this analysis is Rayleigh–Schrödinger perturbation theory, which provides an expression for the second-order correction to the energy. Within the framework of electronic structure theory, this correction, known as the correlation energy, accounts for electron-electron interactions beyond the mean-field approximation of a single reference determinant, $\\Phi_0$. For a system described by Unrestricted Hartree-Fock (UHF), the second-order correlation energy, $E_{\\text{UMP2}}$, is given by a sum over all doubly excited determinants $\\Phi_{ij}^{ab}$ from occupied spin-orbitals $i, j$ to virtual spin-orbitals $a, b$:\n$$\nE_{\\text{UMP2}} = \\sum_{ij} \\sum_{ab} \\frac{\\lvert \\langle \\Phi_0 \\lvert \\hat{H} \\rvert \\Phi_{ij}^{ab} \\rangle \\rvert^2}{\\epsilon_i + \\epsilon_j - \\epsilon_a - \\epsilon_b}\n$$\nwhere $\\epsilon_p$ is the energy of spin-orbital $p$.\n\nThe problem provides an abstraction of this formula. We define the positive excitation energy denominator as $\\Delta_k = \\epsilon_a + \\epsilon_b - \\epsilon_i - \\epsilon_j  0$ and the squared coupling weight as $w_k = \\lvert \\langle \\Phi_0 \\lvert \\hat{H} \\rvert \\Phi_{ij}^{ab} \\rangle \\rvert^2$. The correlation energy contribution from a set of excitations $\\mathcal{C}$ is then:\n$$\nE_{\\mathcal{C}} = - \\sum_{k \\in \\mathcal{C}} \\frac{w_k}{\\Delta_k}\n$$\nA critical insight in electronic structure theory is that correlation effects differ for electrons of opposite spin versus those of the same spin. We can therefore partition the total correlation energy into opposite-spin (os) and same-spin (ss) components:\n$$\nE_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}\n$$\nwhere $E_{\\text{os}}$ sums over excitations involving one alpha and one beta electron, and $E_{\\text{ss}}$ sums over excitations involving either two alpha or two beta electrons.\n\nIt is a known deficiency of the UMP2 method that in systems with significant spin contamination—where the UHF reference determinant is not an eigenfunction of the total spin operator $\\hat{S}^2$—the same-spin component $E_{\\text{ss}}$ is often systematically overestimated. This leads to a total correlation energy that is too negative.\n\nSpin-Component-Scaled Møller–Plesset perturbation theory (SCS-MP2) addresses this by introducing empirically determined scaling factors, $c_{\\text{os}}$ and $c_{\\text{ss}}$, for the respective spin components:\n$$\nE_{\\text{SCS}} = c_{\\text{os}} E_{\\text{os}} + c_{\\text{ss}} E_{\\text{ss}}\n$$\nThe canonical parameters, which we are instructed to use, are $c_{\\text{os}} = 1.20$ and $c_{\\text{ss}} = 0.33$. The opposite-spin component, which MP2 tends to underestimate, is scaled up. The same-spin component, which MP2 tends to overestimate (especially in spin-contaminated cases), is scaled down.\n\nThe computational procedure for each test case is as follows:\n1.  For the given set of opposite-spin weights $\\{w_{k, \\text{os}}\\}$ and denominators $\\{\\Delta_{k, \\text{os}}\\}$, calculate the opposite-spin correlation energy component:\n    $$\n    E_{\\text{os}} = - \\sum_{k} \\frac{w_{k, \\text{os}}}{\\Delta_{k, \\text{os}}}\n    $$\n2.  Similarly, for the same-spin data $\\{w_{k, \\text{ss}}\\}$ and $\\{\\Delta_{k, \\text{ss}}\\}$, calculate the same-spin component:\n    $$\n    E_{\\text{ss}} = - \\sum_{k} \\frac{w_{k, \\text{ss}}}{\\Delta_{k, \\text{ss}}}\n    $$\n3.  Calculate the total UMP2 correlation energy:\n    $$\n    E_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}\n    $$\n4.  Calculate the SCS-MP2 correlation energy using the given scaling parameters:\n    $$\n    E_{\\text{SCS}} = (1.20) E_{\\text{os}} + (0.33) E_{\\text{ss}}\n    $$\n5.  With the provided reference correlation energy, $E_{\\text{ref}}$, compute the absolute errors for both methods:\n    $$\n    \\epsilon_{\\text{UMP2}} = \\lvert E_{\\text{UMP2}} - E_{\\text{ref}} \\rvert\n    $$\n    $$\n    \\epsilon_{\\text{SCS}} = \\lvert E_{\\text{SCS}} - E_{\\text{ref}} \\rvert\n    $$\n6.  Finally, evaluate the boolean condition $\\epsilon_{\\text{SCS}}  \\epsilon_{\\text{UMP2}}$ to determine if SCS-MP2 provides a strict improvement over UMP2 for the given case.\n\nThis procedure will be applied to the three distinct cases provided, which are designed to model a well-behaved system (Case A), a spin-contaminated system with artificially large same-spin amplitudes (Case B), and a system with near-degeneracy causing a breakdown of standard perturbation theory (Case C). The constructed numerical experiment correctly tests the hypothesis that down-weighting the same-spin correlation can improve accuracy in problematic cases where UMP2 fails. Case A serves as a control, demonstrating that this empirical scaling is not a universal panacea and can degrade accuracy in well-behaved systems where the original MP2 formulation is more appropriate. Cases B and C demonstrate the scenarios where SCS-MP2 is designed to excel. The implementation must precisely follow these steps.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this problem.\n\ndef solve():\n    \"\"\"\n    Solves the computational chemistry problem comparing UMP2 and SCS-MP2.\n    \"\"\"\n    \n    # Define the scaling coefficients for SCS-MP2.\n    c_os = 1.20\n    c_ss = 0.33\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"os_weights\": [0.010, 0.006],\n            \"os_denominators\": [0.50, 0.60],\n            \"ss_weights\": [0.0005],\n            \"ss_denominators\": [0.55],\n            \"e_ref\": -0.0315\n        },\n        {\n            \"name\": \"Case B\",\n            \"os_weights\": [0.018, 0.014],\n            \"os_denominators\": [0.30, 0.35],\n            \"ss_weights\": [0.012, 0.010],\n            \"ss_denominators\": [0.32, 0.36],\n            \"e_ref\": -0.1100\n        },\n        {\n            \"name\": \"Case C\",\n            \"os_weights\": [0.010, 0.008],\n            \"os_denominators\": [0.25, 0.30],\n            \"ss_weights\": [0.0020, 0.0015],\n            \"ss_denominators\": [0.050, 0.060],\n            \"e_ref\": -0.0850\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Convert lists to numpy arrays for vectorized operations.\n        os_weights = np.array(case[\"os_weights\"])\n        os_denominators = np.array(case[\"os_denominators\"])\n        ss_weights = np.array(case[\"ss_weights\"])\n        ss_denominators = np.array(case[\"ss_denominators\"])\n        e_ref = case[\"e_ref\"]\n\n        # Calculate the opposite-spin and same-spin correlation energy components.\n        # E_C = - sum(w_k / Delta_k)\n        e_os = -np.sum(os_weights / os_denominators)\n        e_ss = -np.sum(ss_weights / ss_denominators)\n        \n        # Calculate the UMP2 correlation energy.\n        # E_UMP2 = E_os + E_ss\n        e_ump2 = e_os + e_ss\n        \n        # Calculate the SCS-MP2 correlation energy.\n        # E_SCS = c_os * E_os + c_ss * E_ss\n        e_scs = c_os * e_os + c_ss * e_ss\n        \n        # Calculate the absolute errors with respect to the reference energy.\n        error_ump2 = abs(e_ump2 - e_ref)\n        error_scs = abs(e_scs - e_ref)\n        \n        # Determine if SCS-MP2 strictly reduces the error.\n        is_scs_better = error_scs  error_ump2\n        \n        # Format the results for this case.\n        # Errors are rounded to six decimal places.\n        # Boolean is converted to lowercase string 'true' or 'false'.\n        case_result = [\n            round(error_ump2, 6),\n            round(error_scs, 6),\n            is_scs_better\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified in the problem.\n    # e.g., [[e_u,A, e_s,A, b_A],[e_u,B, e_s,B, b_B],...]\n    # The string representation of a list of lists in Python is very close to the target.\n    # The only change needed is converting boolean 'True'/'False' to 'true'/'false'.\n    result_strings = []\n    for res in results:\n        # res[0] is e_u, res[1] is e_s, res[2] is boolean b\n        result_strings.append(f\"[{res[0]}, {res[1]}, {str(res[2]).lower()}]\")\n\n    # Join the sublist strings with a comma and enclose in brackets.\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2926383"}, {"introduction": "Moving from theoretical motivation to practical model development, this exercise explores the empirical heart of scaled-component models. The coefficients used in methods like SCS-MP2 are not derived from first principles but are fitted to reproduce high-quality reference data. This practice [@problem_id:2926415] guides you through a regression analysis to determine optimal scaling parameters, demonstrating how their values are sensitive to the chemical diversity of the training set—a critical lesson in the development and application of any data-driven chemical model.", "problem": "You are asked to investigate how expanding a calibration set from only noncovalent interactions to a broader set that includes barrier heights and thermochemistry shifts the optimal coefficients in a spin-component-scaled second-order Møller–Plesset correlation model with an empirical dispersion term. The model uses the following linear ansatz for the correlation contribution to an energy quantity of interest:\n$$\n\\hat{y} = c_{\\mathrm{OS}}\\,x_{\\mathrm{OS}} + c_{\\mathrm{SS}}\\,x_{\\mathrm{SS}} + s_{6}\\,x_{\\mathrm{D3}} + c_{0},\n$$\nwhere $x_{\\mathrm{OS}}$ is the opposite-spin second-order Møller–Plesset (MP2) correlation component, $x_{\\mathrm{SS}}$ is the same-spin MP2 correlation component, $x_{\\mathrm{D3}}$ is a precomputed Grimme third-generation dispersion correction (D3), $c_{\\mathrm{OS}}$ and $c_{\\mathrm{SS}}$ are spin-component-scaling coefficients, $s_{6}$ is an empirical dispersion scaling, and $c_{0}$ is an intercept. All energy-like inputs below are in kilocalories per mole (kcal/mol). You must report the final numerical outputs as unitless floating-point values.\n\nFrom first principles, determine the optimal coefficients by minimizing the sum of squared residuals with an $\\ell_{2}$-type (ridge) penalty on the non-intercept coefficients. Concretely, for a given nonnegative penalty parameter $\\lambda$, minimize\n$$\nJ(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N}\\left(\\hat{y}_{i} - y_{i}\\right)^{2} + \\lambda\\left(c_{\\mathrm{OS}}^{2} + c_{\\mathrm{SS}}^{2} + s_{6}^{2}\\right),\n$$\nsubject to the model above, where $y_{i}$ are the provided reference targets and $\\boldsymbol{\\theta} = \\left[c_{\\mathrm{OS}},\\,c_{\\mathrm{SS}},\\,s_{6},\\,c_{0}\\right]^{\\top}$. The intercept $c_{0}$ must not be penalized. Use ordinary least squares when $\\lambda=0$.\n\nData are provided for three categories: noncovalent interactions (NC), barrier heights (BH), and thermochemistry (TC). Each datum is a quadruple $(x_{\\mathrm{OS}},x_{\\mathrm{SS}},x_{\\mathrm{D3}},y)$, all in kcal/mol. Use the following values:\n\n- NC1: $\\left(-3.2,\\,-1.1,\\,-0.8,\\,-5.92\\right)$\n- NC2: $\\left(-2.5,\\,-0.9,\\,-0.6,\\,-4.62\\right)$\n- NC3: $\\left(-4.1,\\,-1.5,\\,-1.1,\\,-7.64\\right)$\n- NC4: $\\left(-1.8,\\,-0.6,\\,-0.4,\\,-3.31\\right)$\n- NC5: $\\left(-5.0,\\,-1.8,\\,-1.3,\\,-9.27\\right)$\n- NC6: $\\left(-2.9,\\,-1.0,\\,-0.7,\\,-5.35\\right)$\n\n- BH1: $\\left(-12.0,\\,-4.5,\\,-0.1,\\,-15.375\\right)$\n- BH2: $\\left(-9.0,\\,-3.2,\\,0.0,\\,-11.47\\right)$\n- BH3: $\\left(-15.0,\\,-5.6,\\,0.2,\\,-19.21\\right)$\n- BH4: $\\left(-7.5,\\,-2.7,\\,0.0,\\,-9.57\\right)$\n\n- TC1: $\\left(-25.0,\\,-9.0,\\,0.0,\\,-30.30\\right)$\n- TC2: $\\left(-18.0,\\,-6.5,\\,0.0,\\,-21.825\\right)$\n- TC3: $\\left(-30.0,\\,-11.0,\\,0.1,\\,-36.45\\right)$\n- TC4: $\\left(-22.0,\\,-8.0,\\,0.0,\\,-26.70\\right)$\n\nFor each penalty parameter in the test suite given below, compute two sets of optimal coefficients:\n1) Train using only the NC data (NC1–NC6).\n2) Train using the combined dataset including NC, BH, and TC (all fourteen entries).\n\nThen, for each penalty, compute the coefficient shift vector\n$$\n\\Delta(\\lambda) = \\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda) - \\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda),\n$$\nwhere $\\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda)$ is trained on the combined dataset and $\\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$ on NC-only. Report the four components of $\\Delta(\\lambda)$ in the order $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0}\\right]$, and also report the Euclidean norm\n$$\n\\left\\|\\Delta(\\lambda)\\right\\|_{2} = \\sqrt{\\left(\\Delta c_{\\mathrm{OS}}\\right)^{2} + \\left(\\Delta c_{\\mathrm{SS}}\\right)^{2} + \\left(\\Delta s_{6}\\right)^{2} + \\left(\\Delta c_{0}\\right)^{2}}.\n$$\n\nTest suite of ridge penalties:\n- $\\lambda = 0$\n- $\\lambda = 10^{-4}$\n- $\\lambda = 10^{-2}$\n- $\\lambda = 1$\n- $\\lambda = 10^{3}$\n\nYour program must:\n- Construct the design matrices with a column of ones for the intercept.\n- Solve the penalized least-squares problems where only the first three coefficients are penalized.\n- For each $\\lambda$ in the test suite, compute and collect $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0},\\,\\left\\|\\Delta\\right\\|_{2}\\right]$.\n\nFinal output format:\n- Produce a single line of output containing the results as a comma-separated list of lists, one inner list per penalty value, in the order of the test suite. Each inner list must contain five floating-point numbers in the order $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0},\\,\\left\\|\\Delta\\right\\|_{2}\\right]$.\n- All floating-point numbers in the output must be rounded to six decimal places.\n- Example format (illustrative only): $[[a_{1},b_{1},c_{1},d_{1},e_{1}],[a_{2},b_{2},c_{2},d_{2},e_{2}],\\dots]$.", "solution": "The problem statement presented is valid. It describes a well-defined task in computational chemistry, specifically the parameterization of a semi-empirical energy model using ridge regression. The model is physically motivated, the objective function is mathematically sound, and all necessary data and constraints are provided. The problem is self-contained, scientifically grounded, and objective. It is a standard application of penalized linear regression and requires no invalid assumptions.\n\nThe task is to determine the optimal coefficients for a linear energy model by minimizing a sum-of-squared-residuals objective function, which includes an $\\ell_{2}$ penalty (ridge regression). The analysis will be performed for two different training sets and a range of penalty parameters $\\lambda$.\n\nThe linear model for the predicted energy quantity $\\hat{y}$ is given by the ansatz:\n$$\n\\hat{y} = c_{\\mathrm{OS}}\\,x_{\\mathrm{OS}} + c_{\\mathrm{SS}}\\,x_{\\mathrm{SS}} + s_{6}\\,x_{\\mathrm{D3}} + c_{0}\n$$\nHere, $\\boldsymbol{\\theta} = \\begin{bmatrix} c_{\\mathrm{OS}}  c_{\\mathrm{SS}}  s_{6}  c_{0} \\end{bmatrix}^{\\top}$ is the vector of parameters to be determined. For a set of $N$ data points, this can be expressed in matrix form:\n$$\n\\mathbf{\\hat{y}} = \\mathbf{X}\\boldsymbol{\\theta}\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{N}$ is the vector of predicted values, $\\mathbf{X} \\in \\mathbb{R}^{N \\times 4}$ is the design matrix, and $\\boldsymbol{\\theta} \\in \\mathbb{R}^{4}$ is the parameter vector. Each row of $\\mathbf{X}$ corresponds to a data point $(x_{\\mathrm{OS},i}, x_{\\mathrm{SS},i}, x_{\\mathrm{D3},i})$ with a fourth column of $1$ for the intercept term $c_0$. The vector $\\mathbf{y} \\in \\mathbb{R}^{N}$ contains the corresponding reference target values.\n\nThe objective is to find the parameter vector $\\boldsymbol{\\theta}$ that minimizes the cost function $J(\\boldsymbol{\\theta})$:\n$$\nJ(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N}\\left(\\hat{y}_{i} - y_{i}\\right)^{2} + \\lambda\\left(c_{\\mathrm{OS}}^{2} + c_{\\mathrm{SS}}^{2} + s_{6}^{2}\\right)\n$$\nThe intercept $c_{0}$ is not penalized. This cost function can be written in matrix notation as:\n$$\nJ(\\boldsymbol{\\theta}) = (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^{\\top}(\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta}\n$$\nwhere $\\mathbf{y}$ is the vector of reference values and $\\mathbf{\\Gamma}$ is a $4 \\times 4$ diagonal matrix that selects which coefficients to penalize. Since only $c_{\\mathrm{OS}}$, $c_{\\mathrm{SS}}$, and $s_{6}$ are penalized, $\\mathbf{\\Gamma}$ is:\n$$\n\\mathbf{\\Gamma} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  0 \\end{pmatrix}\n$$\n\nTo find the minimum of $J(\\boldsymbol{\\theta})$, we compute its gradient with respect to $\\boldsymbol{\\theta}$ and set it to the zero vector:\n$$\n\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\left( (\\mathbf{X}\\boldsymbol{\\theta})^{\\top}(\\mathbf{X}\\boldsymbol{\\theta}) - 2(\\mathbf{X}\\boldsymbol{\\theta})^{\\top}\\mathbf{y} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda\\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta} \\right) = \\mathbf{0}\n$$\n$$\n\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\left( \\boldsymbol{\\theta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} - 2\\boldsymbol{\\theta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda\\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta} \\right) = \\mathbf{0}\n$$\nUsing standard rules of vector calculus, the gradient is:\n$$\n2\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} - 2\\mathbf{X}^{\\top}\\mathbf{y} + 2\\lambda\\mathbf{\\Gamma}\\boldsymbol{\\theta} = \\mathbf{0}\n$$\nRearranging the terms gives the normal equations for this ridge regression problem:\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThis is a system of linear equations of the form $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{b}$, where $\\mathbf{A} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{\\Gamma})$ and $\\mathbf{b} = \\mathbf{X}^{\\top}\\mathbf{y}$. The optimal parameter vector $\\boldsymbol{\\theta}$ can be found by solving this system. For $\\lambda  0$, the matrix $\\mathbf{A}$ is guaranteed to be invertible, providing a unique solution. For $\\lambda = 0$, the problem reduces to ordinary least squares, which has a unique solution if $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible (i.e., if the columns of $\\mathbf{X}$ are linearly independent).\n\nThe computational procedure is as follows:\n1.  Partition the provided data into two sets: the noncovalent (NC) set with $N_{\\mathrm{NC}} = 6$ points, and the combined (all) set with $N_{\\mathrm{all}} = 14$ points.\n2.  For each data set, construct the design matrix $\\mathbf{X}$ and the target vector $\\mathbf{y}$. For the NC set, this yields $\\mathbf{X}_{\\mathrm{NC}}$ ($6 \\times 4$) and $\\mathbf{y}_{\\mathrm{NC}}$ ($6 \\times 1$). For the combined set, we have $\\mathbf{X}_{\\mathrm{all}}$ ($14 \\times 4$) and $\\mathbf{y}_{\\mathrm{all}}$ ($14 \\times 1$).\n3.  For each specified value of $\\lambda$ in the test suite $\\{0, 10^{-4}, 10^{-2}, 1, 10^{3}\\}$:\n    a.  Solve $(\\mathbf{X}_{\\mathrm{NC}}^{\\top}\\mathbf{X}_{\\mathrm{NC}} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta}_{\\mathrm{NC}} = \\mathbf{X}_{\\mathrm{NC}}^{\\top}\\mathbf{y}_{\\mathrm{NC}}$ to find the optimal parameters $\\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$ for the NC-only data.\n    b.  Solve $(\\mathbf{X}_{\\mathrm{all}}^{\\top}\\mathbf{X}_{\\mathrm{all}} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta}_{\\mathrm{all}} = \\mathbf{X}_{\\mathrm{all}}^{\\top}\\mathbf{y}_{\\mathrm{all}}$ to find the optimal parameters $\\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda)$ for the combined data set.\n    c.  Calculate the coefficient shift vector $\\Delta(\\lambda) = \\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda) - \\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$.\n    d.  Calculate the Euclidean norm of the shift vector, $\\|\\Delta(\\lambda)\\|_{2}$.\n4.  The components of $\\Delta(\\lambda)$ and its norm, rounded to six decimal places, are collected for each $\\lambda$ and formatted into the final output. The following program implements this procedure.", "answer": "```python\nimport numpy as np\n\ndef solve_ridge(X, y, lambda_val):\n    \"\"\"\n    Solves a ridge regression problem with a custom penalty.\n    The penalty is applied only to the first three coefficients.\n    \n    The problem is to find theta that minimizes:\n    ||X @ theta - y||^2 + lambda_val * (theta[0]^2 + theta[1]^2 + theta[2]^2)\n    \n    This corresponds to the normal equations:\n    (X.T @ X + lambda_val * Gamma) @ theta = X.T @ y\n    where Gamma = diag([1, 1, 1, 0]).\n    \"\"\"\n    num_params = X.shape[1]\n    \n    # Penalty matrix Gamma\n    Gamma = np.zeros((num_params, num_params))\n    Gamma[0, 0] = 1.0\n    Gamma[1, 1] = 1.0\n    Gamma[2, 2] = 1.0\n    \n    # Left-hand side of the normal equations\n    A = X.T @ X + lambda_val * Gamma\n    \n    # Right-hand side of the normal equations\n    b = X.T @ y\n    \n    # Solve the linear system A * theta = b\n    theta = np.linalg.solve(A, b)\n    \n    return theta\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis and print the results.\n    \"\"\"\n    # Data provided in the problem statement (x_OS, x_SS, x_D3, y)\n    nc_data_tuples = [\n        (-3.2, -1.1, -0.8, -5.92),\n        (-2.5, -0.9, -0.6, -4.62),\n        (-4.1, -1.5, -1.1, -7.64),\n        (-1.8, -0.6, -0.4, -3.31),\n        (-5.0, -1.8, -1.3, -9.27),\n        (-2.9, -1.0, -0.7, -5.35),\n    ]\n\n    bh_data_tuples = [\n        (-12.0, -4.5, -0.1, -15.375),\n        (-9.0, -3.2, 0.0, -11.47),\n        (-15.0, -5.6, 0.2, -19.21),\n        (-7.5, -2.7, 0.0, -9.57),\n    ]\n\n    tc_data_tuples = [\n        (-25.0, -9.0, 0.0, -30.30),\n        (-18.0, -6.5, 0.0, -21.825),\n        (-30.0, -11.0, 0.1, -36.45),\n        (-22.0, -8.0, 0.0, -26.70),\n    ]\n\n    all_data_tuples = nc_data_tuples + bh_data_tuples + tc_data_tuples\n\n    # Convert tuple data to numpy arrays\n    nc_data = np.array(nc_data_tuples)\n    all_data = np.array(all_data_tuples)\n\n    # Construct design matrices (X) and target vectors (y)\n    # The last column of X is for the intercept c0, so it's a column of ones.\n    X_nc = np.c_[nc_data[:, :3], np.ones(nc_data.shape[0])]\n    y_nc = nc_data[:, 3]\n    \n    X_all = np.c_[all_data[:, :3], np.ones(all_data.shape[0])]\n    y_all = all_data[:, 3]\n\n    # Test suite of ridge penalties\n    test_lambdas = [0.0, 1e-4, 1e-2, 1.0, 1e3]\n    \n    results = []\n    \n    for lambda_val in test_lambdas:\n        # 1) Train using only the NC data\n        theta_nc = solve_ridge(X_nc, y_nc, lambda_val)\n        \n        # 2) Train using the combined dataset\n        theta_all = solve_ridge(X_all, y_all, lambda_val)\n        \n        # 3) Compute the coefficient shift vector Delta\n        delta = theta_all - theta_nc\n        \n        # 4) Compute the Euclidean norm of Delta\n        norm_delta = np.linalg.norm(delta)\n        \n        # Collect the results: [delta_c_OS, delta_c_SS, delta_s6, delta_c0, norm_delta]\n        # Round to six decimal places as required.\n        current_result = [round(val, 6) for val in list(delta) + [norm_delta]]\n        results.append(current_result)\n        \n    # Format the final output string to match the required format exactly.\n    # The str() representation of a list of lists with spaces removed is correct.\n    output_string = str(results).replace(\" \", \"\")\n    \n    print(output_string)\n\nsolve()\n```", "id": "2926415"}, {"introduction": "This final practice addresses a crucial aspect of using any empirical model: quantifying the reliability of its predictions. A single set of \"optimal\" parameters hides the uncertainty inherent in the fitting process, which stems from the limited and noisy training data. This exercise [@problem_id:2926422] introduces the nonparametric bootstrap, a powerful statistical technique to estimate the uncertainty in the fitted SCS-MP2 parameters and propagate it to generate confidence intervals for new predictions, transforming a simple point estimate into a more informative and scientifically honest prediction with error bars.", "problem": "You are given a dataset to calibrate a Spin-Component-Scaled second-order Møller–Plesset perturbation theory (SCS-MP2) empirical model and quantify epistemic uncertainty in its predictions by nonparametric bootstrap. Let the SCS-MP2 correlation energy model be a linear predictor with an intercept:\n$$\nE_{\\mathrm{pred}} = c_0 + c_{\\mathrm{OS}} E_{\\mathrm{OS}} + c_{\\mathrm{SS}} E_{\\mathrm{SS}},\n$$\nwhere $E_{\\mathrm{OS}}$ and $E_{\\mathrm{SS}}$ are the opposite-spin and same-spin second-order Møller–Plesset perturbation theory (MP2) components, respectively, and $E_{\\mathrm{pred}}$ is the predicted correlation energy. You are provided a training set of $N$ molecules with known reference correlation energies $E_{\\mathrm{ref}}$ and corresponding MP2 components, and a test set of $T$ molecules with only the MP2 components. All energies are in Hartree, and you must report all outputs in Hartree. The quantities are:\n- Training set size $N = 10$.\n- Test set size $T = 4$.\n\nTraining set arrays (length $N$, in Hartree):\n- $E_{\\mathrm{OS}}^{\\mathrm{train}} = [-0.040000,-0.120000,-0.220000,-0.350000,-0.500000,-0.180000,-0.270000,-0.080000,-0.420000,-0.300000]$,\n- $E_{\\mathrm{SS}}^{\\mathrm{train}} = [-0.010000,-0.030000,-0.060000,-0.090000,-0.130000,-0.050000,-0.070000,-0.020000,-0.110000,-0.080000]$,\n- $E_{\\mathrm{ref}}^{\\mathrm{train}} = [-0.051600,-0.154500,-0.284000,-0.450600,-0.643300,-0.233000,-0.347800,-0.102800,-0.540900,-0.386700]$.\n\nTest set arrays (length $T$, in Hartree):\n- $E_{\\mathrm{OS}}^{\\mathrm{test}} = [-0.260000,-0.150000,-0.520000,-0.020000]$,\n- $E_{\\mathrm{SS}}^{\\mathrm{test}} = [-0.070000,-0.040000,-0.140000,-0.005000]$.\n\nYour task is to:\n1. Calibrate the linear model parameters $c_0$, $c_{\\mathrm{OS}}$, and $c_{\\mathrm{SS}}$ by minimizing the sum of squared residuals on the training set with Tikhonov (ridge) regularization applied only to the slope parameters to ensure numerical stability. Specifically, estimate parameters by solving a regularized least-squares problem that penalizes $c_{\\mathrm{OS}}$ and $c_{\\mathrm{SS}}$ but not $c_0$. Use the regularization strength $\\lambda = 10^{-8}$ (dimensionless multiplier on the squared $L_2$-norm of the slope parameters).\n2. Quantify epistemic uncertainty in the calibrated model by nonparametric bootstrap resampling of the training set with replacement, using $B = 2000$ bootstrap replicates. In each replicate, resample $N$ training pairs $\\left(E_{\\mathrm{OS}}^{(i)}, E_{\\mathrm{SS}}^{(i)}, E_{\\mathrm{ref}}^{(i)}\\right)$ with replacement, refit the regularized model to obtain parameters $\\left(c_0^{*}, c_{\\mathrm{OS}}^{*}, c_{\\mathrm{SS}}^{*}\\right)$, and compute predicted energies $E_{\\mathrm{pred}}^{*}$ for all $T$ test molecules.\n3. For each test molecule $j \\in \\{1,\\dots,T\\}$, compute:\n   - A point estimate $\\hat{E}_{\\mathrm{pred},j}$ using the parameters fitted on the original (non-resampled) training set.\n   - A two-sided $95$ percent equal-tailed confidence interval for $E_{\\mathrm{pred},j}$ from the bootstrap distribution, i.e., the empirical quantiles at levels $2.5$ and $97.5$ percent. Use the standard linear interpolation rule for quantiles.\n4. Use a fixed pseudo-random number generator seed of $12345$ for bootstrap resampling to ensure reproducibility.\n\nFoundational base you may assume:\n- Linear regression modeling and the principle of least squares are valid approximations for small empirical corrections in electronic structure models.\n- Tikhonov (ridge) regularization stabilizes least-squares estimation by adding a small penalty on coefficient magnitudes.\n- The nonparametric bootstrap approximates the sampling distribution of estimators by resampling with replacement from the empirical distribution of the data.\n\nImportant scientific and numerical requirements:\n- Treat the problem entirely in purely mathematical terms as described, without invoking any external data or domain-specific heuristics beyond the definitions above.\n- All energies must be reported in Hartree.\n- The confidence interval bounds must be computed as described and reported in Hartree.\n\nTest suite and output specification:\n- Use the exact training and test arrays given above.\n- Use $\\lambda = 10^{-8}$, $B = 2000$, and seed $12345$ as specified.\n- Your program must produce a single line of output containing a list of $T$ sublists. Each sublist corresponds to one test molecule in the given order and contains three floating-point numbers rounded to exactly $6$ decimal places: $[\\hat{E}_{\\mathrm{pred},j}, \\mathrm{CI}_{\\mathrm{low},j}, \\mathrm{CI}_{\\mathrm{high},j}]$, where all entries are in Hartree. For example, the overall format must be:\n$[[x_{1},\\ell_{1},u_{1}],[x_{2},\\ell_{2},u_{2}],\\dots,[x_{T},\\ell_{T},u_{T}]]$\nwith no spaces inserted anywhere.\n- Edge cases to cover via the bootstrap include repeated sampling of the same training items; to ensure invertibility of normal equations for any resample, use the stated ridge regularization and do not penalize the intercept.\n\nYour program must be fully deterministic and must not read any input. It must use only the libraries specified by the execution environment. The final line printed must be the only output and must adhere exactly to the formatting stated above.", "solution": "The problem requires the calibration of a Spin-Component-Scaled Møller–Plesset perturbation theory (SCS-MP2) empirical model and the subsequent quantification of epistemic uncertainty in its predictions. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique, verifiable solution. We shall proceed with the derivation and computation.\n\nThe given linear model for the predicted correlation energy, $E_{\\mathrm{pred}}$, is:\n$$\nE_{\\mathrm{pred}} = c_0 + c_{\\mathrm{OS}} E_{\\mathrm{OS}} + c_{\\mathrm{SS}} E_{\\mathrm{SS}}\n$$\nwhere $c_0$, $c_{\\mathrm{OS}}$, and $c_{\\mathrm{SS}}$ are the parameters to be determined. $E_{\\mathrm{OS}}$ and $E_{\\mathrm{SS}}$ are the opposite-spin and same-spin MP2 correlation energy components, respectively.\n\nWe are given a training set of $N=10$ observations. We can express the relationship for the entire training set in matrix notation:\n$$\n\\mathbf{y} = \\mathbf{X}\\mathbf{c} + \\boldsymbol{\\epsilon}\n$$\nHere, $\\mathbf{y}$ is the $N \\times 1$ column vector of reference correlation energies $E_{\\mathrm{ref}}^{\\mathrm{train}}$. $\\mathbf{c}$ is the $3 \\times 1$ column vector of parameters $[c_0, c_{\\mathrm{OS}}, c_{\\mathrm{SS}}]^T$. $\\mathbf{X}$ is the $N \\times 3$ design matrix, where the $i$-th row corresponds to the $i$-th training data point and is given by $[1, E_{\\mathrm{OS}, i}^{\\mathrm{train}}, E_{\\mathrm{SS}, i}^{\\mathrm{train}}]$. The vector $\\boldsymbol{\\epsilon}$ represents the residuals.\n\nThe parameters $\\mathbf{c}$ are estimated by minimizing the sum of squared residuals, augmented with a Tikhonov (ridge) regularization term. The problem specifies that the penalty is applied only to the slope parameters, $c_{\\mathrm{OS}}$ and $c_{\\mathrm{SS}}$, and not to the intercept $c_0$. The objective function to minimize is:\n$$\nL(\\mathbf{c}) = \\sum_{i=1}^{N} (y_i - (\\mathbf{X}\\mathbf{c})_i)^2 + \\lambda (c_{\\mathrm{OS}}^2 + c_{\\mathrm{SS}}^2)\n$$\nThis can be written in matrix form as:\n$$\nL(\\mathbf{c}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{c})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{c}) + \\mathbf{c}^T \\mathbf{\\Lambda} \\mathbf{c}\n$$\nwhere $\\lambda=10^{-8}$ is the regularization strength and $\\mathbf{\\Lambda}$ is a $3 \\times 3$ matrix that selectively applies the penalty:\n$$\n\\mathbf{\\Lambda} = \\begin{pmatrix} 0  0  0 \\\\ 0  \\lambda  0 \\\\ 0  0  \\lambda \\end{pmatrix}\n$$\nThe minimum of this objective function is found by setting its gradient with respect to $\\mathbf{c}$ to zero, which yields the solution in the form of modified normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda}) \\hat{\\mathbf{c}} = \\mathbf{X}^T \\mathbf{y}\n$$\nThe optimal parameter vector $\\hat{\\mathbf{c}}$ is therefore:\n$$\n\\hat{\\mathbf{c}} = (\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nThe regularization term $\\mathbf{\\Lambda}$ ensures that the matrix $(\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda})$ is invertible, even if $\\mathbf{X}^T \\mathbf{X}$ is singular or ill-conditioned, which can occur with certain bootstrap resamples.\n\nOnce the parameters $\\hat{\\mathbf{c}}$ are determined from the full training set, the point estimates for the predicted energies of the $T=4$ test molecules are computed as:\n$$\n\\hat{\\mathbf{y}}_{\\mathrm{pred}} = \\mathbf{X}_{\\mathrm{test}} \\hat{\\mathbf{c}}\n$$\nwhere $\\mathbf{X}_{\\mathrm{test}}$ is the $T \\times 3$ design matrix for the test set, constructed analogously to $\\mathbf{X}$.\n\nTo quantify the epistemic uncertainty in these predictions, we employ a nonparametric bootstrap procedure with $B=2000$ replicates. For each replicate $b \\in \\{1, \\dots, B\\}$:\n1. A bootstrap sample of size $N$ is drawn with replacement from the original training set. This gives a new design matrix $\\mathbf{X}_b$ and a new reference energy vector $\\mathbf{y}_b$.\n2. The regularized least-squares problem is solved for this bootstrap sample to obtain a new set of parameters, $\\hat{\\mathbf{c}}_b$:\n$$\n\\hat{\\mathbf{c}}_b = (\\mathbf{X}_b^T \\mathbf{X}_b + \\mathbf{\\Lambda})^{-1} \\mathbf{X}_b^T \\mathbf{y}_b\n$$\n3. These bootstrap parameters are used to generate a set of predictions for the test molecules:\n$$\n\\hat{\\mathbf{y}}_{\\mathrm{pred}, b} = \\mathbf{X}_{\\mathrm{test}} \\hat{\\mathbf{c}}_b\n$$\nAfter completing all $B$ replicates, we have, for each test molecule $j \\in \\{1, \\dots, T\\}$, a distribution of $B$ predicted energies. The $95\\%$ equal-tailed confidence interval is constructed by finding the empirical $2.5$-th and $97.5$-th percentiles of this distribution. These quantiles are computed using linear interpolation between adjacent values in the sorted distribution. The use of a fixed pseudo-random number generator seed ensures the reproducibility of the bootstrap resampling process and, consequently, the final results.\n\nThe final output for each test molecule $j$ will consist of the point estimate $\\hat{E}_{\\mathrm{pred},j}$ from the original fit, and the lower and upper bounds of its $95\\%$ confidence interval, $[\\mathrm{CI}_{\\mathrm{low},j}, \\mathrm{CI}_{\\mathrm{high},j}]$. All values are reported in Hartree, rounded to six decimal places.\n\nThe implementation will proceed by first constructing the necessary matrices from the provided data. Then, the parameters for the original data are computed to find the point estimates. Following this, the bootstrap loop is executed, storing the predictions for each replicate. Finally, the collection of bootstrap predictions is processed to compute the specified confidence intervals.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calibrates an SCS-MP2 model, predicts energies for a test set, and quantifies\n    uncertainty using nonparametric bootstrap.\n    \"\"\"\n    #\n    # Step 0: Define givens from the problem statement\n    #\n    N = 10  # Training set size\n    T = 4   # Test set size\n    B = 2000 # Number of bootstrap replicates\n    LAMBDA = 1e-8 # Regularization strength\n    SEED = 12345  # PRNG seed\n\n    # Training set data (in Hartree)\n    E_os_train = np.array([-0.040000,-0.120000,-0.220000,-0.350000,-0.500000,-0.180000,-0.270000,-0.080000,-0.420000,-0.300000])\n    E_ss_train = np.array([-0.010000,-0.030000,-0.060000,-0.090000,-0.130000,-0.050000,-0.070000,-0.020000,-0.110000,-0.080000])\n    E_ref_train = np.array([-0.051600,-0.154500,-0.284000,-0.450600,-0.643300,-0.233000,-0.347800,-0.102800,-0.540900,-0.386700])\n\n    # Test set data (in Hartree)\n    E_os_test = np.array([-0.260000,-0.150000,-0.520000,-0.020000])\n    E_ss_test = np.array([-0.070000,-0.040000,-0.140000,-0.005000])\n\n    #\n    # Step 1: Prepare matrices for linear regression\n    #\n    \n    # Design matrix X for training data (N x 3)\n    X_train = np.vstack([np.ones(N), E_os_train, E_ss_train]).T\n    y_train = E_ref_train\n\n    # Design matrix X_test for test data (T x 3)\n    X_test = np.vstack([np.ones(T), E_os_test, E_ss_test]).T\n    \n    # Regularization matrix Lambda (3 x 3)\n    # Penalizes c_os and c_ss, but not the intercept c_0\n    lambda_mat = np.diag([0, LAMBDA, LAMBDA])\n\n    def fit_model(X, y, reg_mat):\n        \"\"\"\n        Solves the regularized least-squares problem to find model parameters.\n        c = (X^T X + Lambda)^(-1) X^T y\n        \"\"\"\n        XTX = X.T @ X\n        XTy = X.T @ y\n        \n        # Add regularization term\n        regularized_XTX = XTX + reg_mat\n        \n        # Solve for coefficients\n        try:\n            inv_matrix = np.linalg.inv(regularized_XTX)\n            coeffs = inv_matrix @ XTy\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if inversion fails, though ridge should prevent this\n            inv_matrix = np.linalg.pinv(regularized_XTX)\n            coeffs = inv_matrix @ XTy\n            \n        return coeffs\n\n    #\n    # Step 2: Calibrate on original training set and get point estimates\n    #\n\n    # Fit model to the original training data\n    c_hat = fit_model(X_train, y_train, lambda_mat)\n\n    # Compute point predictions for the test set\n    point_predictions = X_test @ c_hat\n\n    #\n    # Step 3: Perform nonparametric bootstrap\n    #\n    \n    # Initialize pseudo-random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Store bootstrap predictions for each test molecule\n    bootstrap_predictions = np.zeros((B, T))\n\n    for i in range(B):\n        # Resample N indices with replacement\n        indices = rng.choice(N, size=N, replace=True)\n\n        # Create bootstrap sample\n        X_boot = X_train[indices]\n        y_boot = y_train[indices]\n\n        # Fit model on bootstrap sample\n        c_boot = fit_model(X_boot, y_boot, lambda_mat)\n\n        # Predict on test set and store results\n        bootstrap_predictions[i, :] = X_test @ c_boot\n\n    #\n    # Step 4: Compute confidence intervals and format the final output\n    #\n\n    results = []\n    for j in range(T):\n        # Get predictions for the j-th test molecule from all bootstrap runs\n        pred_dist = bootstrap_predictions[:, j]\n        \n        # Compute 2.5% and 97.5% quantiles for a 95% CI\n        # Using linear interpolation as specified\n        ci_low, ci_high = np.quantile(pred_dist, [0.025, 0.975], interpolation='linear')\n        \n        # Get the point estimate\n        point_est = point_predictions[j]\n        \n        results.append([point_est, ci_low, ci_high])\n\n    # Format the output string exactly as specified: no spaces, 6 decimal places.\n    # e.g., [[x1,l1,u1],[x2,l2,u2]]\n    sublist_strings = []\n    for res in results:\n        sublist_strings.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\")\n    final_output_string = f\"[{','.join(sublist_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "2926422"}]}