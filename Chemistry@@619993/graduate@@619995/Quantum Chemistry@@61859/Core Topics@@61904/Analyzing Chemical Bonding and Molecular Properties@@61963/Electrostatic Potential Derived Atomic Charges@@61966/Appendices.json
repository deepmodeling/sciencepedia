{"hands_on_practices": [{"introduction": "Deriving atomic charges from the electrostatic potential is a classic inverse problem in computational chemistry, where we seek a simple point-charge model that best reproduces a quantum mechanical observable. This exercise [@problem_id:2889373] guides you through the fundamental mathematics of this procedure by implementing both an unconstrained and a constrained least-squares fit. By comparing the results, you will gain a hands-on appreciation for why enforcing the total molecular charge is critical for obtaining physically meaningful and numerically stable results.", "problem": "Consider the task of deriving electrostatic potential (ESP) fitted atomic charges for a monovalent ion from first principles. Work entirely in atomic units: lengths in Bohr radius $a_0$, charges in elementary charge $e$, and electrostatic potential in Hartree per $e$ (so that Coulomb’s law reads simply as $V(\\mathbf{r}) = \\sum_i q_i / \\lVert \\mathbf{r} - \\mathbf{R}_i \\rVert$). The electric dipole moment is therefore $\\boldsymbol{\\mu} = \\sum_i q_i \\mathbf{R}_i$ in units of $e\\,a_0$. Assume measurement noise is additive, independent, and identically distributed, with zero mean and specified standard deviation. The forward model for $M$ grid points is\n$$\nV_j = \\sum_{i=1}^{N} \\frac{q_i}{\\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert} + \\eta_j,\\quad j = 1,\\dots, M,\n$$\nwhere $N$ is the number of atoms, $\\mathbf{R}_i$ are the nuclear positions, $q_i$ are the unknown atomic charges to be fitted, $\\mathbf{r}_j$ are the grid points, and $\\eta_j$ are noise samples.\n\nStarting from Coulomb’s law and the least-squares principle, you must:\n- Implement the unconstrained least-squares fit for $\\mathbf{q}$ that minimizes $\\sum_{j=1}^{M} \\left(V_j - \\sum_{i=1}^N q_i / \\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert\\right)^2$.\n- Implement the equality-constrained least-squares fit that enforces the total charge constraint $\\sum_{i=1}^{N} q_i = Q_{\\text{tot}}$ using a mathematically sound method derived from first principles (e.g., a Lagrange multiplier Karush–Kuhn–Tucker system).\n- For each fit, compute:\n  1. The total charge from the unconstrained fit, $\\sum_i \\hat{q}_i^{(\\text{uncon})}$ (a float).\n  2. The electric dipole vector error for the unconstrained fit, $\\lVert \\sum_i (\\hat{q}_i^{(\\text{uncon})} - q_i^{(\\text{true})}) \\mathbf{R}_i \\rVert_2$ (a float, in $e\\,a_0$).\n  3. The electric dipole vector error for the constrained fit, $\\lVert \\sum_i (\\hat{q}_i^{(\\text{con})} - q_i^{(\\text{true})}) \\mathbf{R}_i \\rVert_2$ (a float, in $e\\,a_0$).\n\nUse the following test suite. In all cases, generate the observed potentials by $V_j = \\sum_i q_i^{(\\text{true})}/\\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert + \\eta_j$, where $\\eta_j$ are independent Gaussian samples with the stated standard deviation, drawn using a fixed random seed for reproducibility. All coordinates are in $a_0$, all charges in $e$, all potentials in atomic units, and all dipole errors must be reported in $e\\,a_0$.\n\n- Test case A (general overdetermined, cation, moderate noise):\n  - $N = 3$, $Q_{\\text{tot}} = +1$.\n  - Nuclear positions $\\mathbf{R}_i$: $[(0,0,0),\\ (1,0,0),\\ (0,1,0)]$.\n  - True charges $\\mathbf{q}^{(\\text{true})}$: $[0.3,\\ 0.5,\\ 0.2]$.\n  - Grid points $\\mathbf{r}_j$: $[(2,0,0),\\ (0,2,0),\\ (2,2,0),\\ (1,1,1),\\ (-1,-1,0.5),\\ (3,1,0),\\ (1,3,0),\\ (1,1,-1)]$.\n  - Noise: Gaussian with standard deviation $\\sigma = 10^{-4}$; random seed $123$.\n\n- Test case B (exactly determined, collinear geometry, very low noise):\n  - $N = 3$, $Q_{\\text{tot}} = +1$.\n  - Nuclear positions $\\mathbf{R}_i$: $[(0,0,0),\\ (1.2,0,0),\\ (2.4,0,0)]$.\n  - True charges $\\mathbf{q}^{(\\text{true})}$: $[0.7,\\ 0.2,\\ 0.1]$.\n  - Grid points $\\mathbf{r}_j$: $[(0,2,0),\\ (1.2,2,0),\\ (2.4,2,0)]$.\n  - Noise: Gaussian with standard deviation $\\sigma = 10^{-6}$; random seed $321$.\n\n- Test case C (overdetermined, anion, stronger noise and a far-field point):\n  - $N = 3$, $Q_{\\text{tot}} = -1$.\n  - Nuclear positions $\\mathbf{R}_i$: $[(0,0,0),\\ (1,0,0),\\ (0,0,1)]$.\n  - True charges $\\mathbf{q}^{(\\text{true})}$: $[-0.6,\\ -0.2,\\ -0.2]$.\n  - Grid points $\\mathbf{r}_j$: $[(2,0,0),\\ (0,2,0),\\ (2,2,0),\\ (1,1,1),\\ (-1,-1,1),\\ (3,0,0),\\ (10,10,10)]$.\n  - Noise: Gaussian with standard deviation $\\sigma = 5\\times 10^{-4}$; random seed $999$.\n\nYour program should:\n- Construct the design matrix with entries $A_{j i} = 1/\\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert$ for each test case.\n- Generate $V_j$ with the specified noise using the given seeds.\n- Compute the unconstrained and constrained fits as specified above.\n- Compute, for each test case, the three required floats in the order described.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three lists, one per test case, in the order A, B, C. Each inner list must be of the form $[\\sum_i \\hat{q}_i^{(\\text{uncon})},\\ \\text{dipole\\_error}^{(\\text{uncon})},\\ \\text{dipole\\_error}^{(\\text{con})}]$, where the dipole errors are in $e\\,a_0$.\n- For example: $[[x_A,y_A,z_A],[x_B,y_B,z_B],[x_C,y_C,z_C]]$ where each $x,y,z$ is a float.", "solution": "The problem of determining atomic charges from electrostatic potential data is a classic inverse problem in computational chemistry. We are given a set of nuclear positions $\\{\\mathbf{R}_i\\}_{i=1}^N$, a set of grid points $\\{\\mathbf{r}_j\\}_{j=1}^M$, and the corresponding electrostatic potentials $\\{V_j\\}_{j=1}^M$ measured at these grid points. The forward model, based on Coulomb's law in atomic units, relates the unknown atomic charges $\\{q_i\\}_{i=1}^N$ to the potentials via a linear system of equations, corrupted by noise:\n$$\nV_j = \\sum_{i=1}^{N} \\frac{q_i}{\\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert} + \\eta_j\n$$\nThis can be expressed in matrix form as:\n$$\n\\mathbf{V} = A \\mathbf{q} + \\boldsymbol{\\eta}\n$$\nHere, $\\mathbf{V}$ is the $M \\times 1$ column vector of observed potentials, $\\mathbf{q}$ is the $N \\times 1$ column vector of unknown charges, and $\\boldsymbol{\\eta}$ is the $M \\times 1$ vector of unknown noise samples. The $M \\times N$ matrix $A$, known as the design matrix, has entries $A_{ji} = 1 / \\lVert \\mathbf{r}_j - \\mathbf{R}_i \\rVert$. Our objective is to estimate $\\mathbf{q}$ from $\\mathbf{V}$ and $A$.\n\nFirst, we consider the unconstrained least-squares fit. The goal is to find the vector of charges $\\hat{\\mathbf{q}}^{(\\text{uncon})}$ that minimizes the sum of squared residuals, $S(\\mathbf{q}) = \\sum_{j=1}^M (V_j - (A\\mathbf{q})_j)^2$. In matrix notation, this is equivalent to minimizing the squared Euclidean norm of the residual vector:\n$$\nS(\\mathbf{q}) = \\lVert \\mathbf{V} - A\\mathbf{q} \\rVert_2^2 = (\\mathbf{V} - A\\mathbf{q})^T (\\mathbf{V} - A\\mathbf{q})\n$$\nTo find the minimum, we compute the gradient of $S(\\mathbf{q})$ with respect to $\\mathbf{q}$ and set it to zero.\n$$\n\\nabla_{\\mathbf{q}} S(\\mathbf{q}) = \\nabla_{\\mathbf{q}} (\\mathbf{V}^T\\mathbf{V} - 2\\mathbf{V}^T A\\mathbf{q} + \\mathbf{q}^T A^T A\\mathbf{q}) = -2 A^T \\mathbf{V} + 2 A^T A \\mathbf{q}\n$$\nSetting $\\nabla_{\\mathbf{q}} S(\\mathbf{q}) = \\mathbf{0}$ yields the well-known normal equations:\n$$\n(A^T A) \\mathbf{q} = A^T \\mathbf{V}\n$$\nAssuming the matrix $A^T A$ is invertible (which is true if $A$ has full column rank, a condition met in non-degenerate molecular geometries), the unconstrained least-squares solution is:\n$$\n\\hat{\\mathbf{q}}^{(\\text{uncon})} = (A^T A)^{-1} A^T \\mathbf{V}\n$$\nThe matrix $(A^T A)^{-1} A^T$ is the Moore-Penrose pseudoinverse of $A$. Computationally, this system is solved using stable numerical methods such as QR decomposition, as implemented in standard linear algebra libraries.\n\nSecond, we address the equality-constrained least-squares fit. We must minimize the same objective function $S(\\mathbf{q})$ subject to the physical constraint that the sum of atomic charges equals the total charge of the ion, $Q_{\\text{tot}}$. This constraint is linear:\n$$\n\\sum_{i=1}^{N} q_i = Q_{\\text{tot}} \\quad \\text{or} \\quad \\mathbf{c}^T \\mathbf{q} = Q_{\\text{tot}}, \\quad \\text{where } \\mathbf{c} = [1, 1, \\dots, 1]^T\n$$\nThis constrained optimization problem is solved using the method of Lagrange multipliers. We define the Lagrangian function $\\mathcal{L}(\\mathbf{q}, \\lambda)$:\n$$\n\\mathcal{L}(\\mathbf{q}, \\lambda) = \\frac{1}{2}\\lVert \\mathbf{V} - A\\mathbf{q} \\rVert_2^2 + \\lambda (\\mathbf{c}^T \\mathbf{q} - Q_{\\text{tot}})\n$$\nThe factor of $1/2$ is for algebraic convenience. The solution $(\\hat{\\mathbf{q}}^{(\\text{con})}, \\lambda^*)$ is found at a stationary point of $\\mathcal{L}$. We set the partial derivatives with respect to $\\mathbf{q}$ and $\\lambda$ to zero.\n$$\n\\nabla_{\\mathbf{q}} \\mathcal{L} = -A^T(\\mathbf{V} - A\\mathbf{q}) + \\lambda \\mathbf{c} = \\mathbf{0} \\implies (A^T A)\\mathbf{q} + \\lambda \\mathbf{c} = A^T \\mathbf{V}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{c}^T \\mathbf{q} - Q_{\\text{tot}} = 0 \\implies \\mathbf{c}^T \\mathbf{q} = Q_{\\text{tot}}\n$$\nThese two equations form a system of $N+1$ linear equations in $N+1$ unknowns ($\\mathbf{q}$ and $\\lambda$). This is known as a Karush-Kuhn-Tucker (KKT) system, which can be expressed in block matrix form:\n$$\n\\begin{pmatrix}\nA^T A & \\mathbf{c} \\\\\n\\mathbf{c}^T & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{q} \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nA^T \\mathbf{V} \\\\\nQ_{\\text{tot}}\n\\end{pmatrix}\n$$\nSolving this system yields the constrained charge vector $\\hat{\\mathbf{q}}^{(\\text{con})}$ and the Lagrange multiplier $\\lambda$.\n\nFor each test case, the procedure is as follows:\n$1$. Construct the $M \\times N$ matrix $A$ from the atomic coordinates $\\mathbf{R}_i$ and grid point coordinates $\\mathbf{r}_j$.\n$2$. Generate the \"observed\" potential vector $\\mathbf{V}$ by computing the true potential $A\\mathbf{q}^{(\\text{true})}$ and adding Gaussian noise with the specified standard deviation $\\sigma$ and random seed.\n$3$. Solve for the unconstrained charges $\\hat{\\mathbf{q}}^{(\\text{uncon})}$ using a numerical least-squares solver.\n$4$. Solve the KKT system for the constrained charges $\\hat{\\mathbf{q}}^{(\\text{con})}$.\n$5$. Calculate the required quantities:\n   a. Total unconstrained charge: $Q_{\\text{uncon}} = \\sum_{i=1}^N \\hat{q}_i^{(\\text{uncon})}$.\n   b. Unconstrained dipole error: $\\lVert \\sum_i (\\hat{q}_i^{(\\text{uncon})} - q_i^{(\\text{true})}) \\mathbf{R}_i \\rVert_2$. The summation represents the error dipole vector $\\boldsymbol{\\mu}_{\\text{err}}^{(\\text{uncon})}$, so we compute its Euclidean norm.\n   c. Constrained dipole error: $\\lVert \\sum_i (\\hat{q}_i^{(\\text{con})} - q_i^{(\\text{true})}) \\mathbf{R}_i \\rVert_2$, the norm of the error dipole vector $\\boldsymbol{\\mu}_{\\text{err}}^{(\\text{con})}$.\nThis rigorous, principle-based approach ensures a correct and robust implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for ESP-fitted charges using unconstrained and constrained least-squares\n    for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"N\": 3, \"Q_tot\": 1.0,\n            \"R\": np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"q_true\": np.array([0.3, 0.5, 0.2]),\n            \"r_grid\": np.array([\n                [2.0, 0.0, 0.0], [0.0, 2.0, 0.0], [2.0, 2.0, 0.0], [1.0, 1.0, 1.0],\n                [-1.0, -1.0, 0.5], [3.0, 1.0, 0.0], [1.0, 3.0, 0.0], [1.0, 1.0, -1.0]\n            ]),\n            \"sigma\": 1e-4, \"seed\": 123\n        },\n        {\n            \"name\": \"B\",\n            \"N\": 3, \"Q_tot\": 1.0,\n            \"R\": np.array([[0.0, 0.0, 0.0], [1.2, 0.0, 0.0], [2.4, 0.0, 0.0]]),\n            \"q_true\": np.array([0.7, 0.2, 0.1]),\n            \"r_grid\": np.array([[0.0, 2.0, 0.0], [1.2, 2.0, 0.0], [2.4, 2.0, 0.0]]),\n            \"sigma\": 1e-6, \"seed\": 321\n        },\n        {\n            \"name\": \"C\",\n            \"N\": 3, \"Q_tot\": -1.0,\n            \"R\": np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"q_true\": np.array([-0.6, -0.2, -0.2]),\n            \"r_grid\": np.array([\n                [2.0, 0.0, 0.0], [0.0, 2.0, 0.0], [2.0, 2.0, 0.0], [1.0, 1.0, 1.0],\n                [-1.0, -1.0, 1.0], [3.0, 0.0, 0.0], [10.0, 10.0, 10.0]\n            ]),\n            \"sigma\": 5e-4, \"seed\": 999\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        R_atoms = case[\"R\"]\n        q_true = case[\"q_true\"]\n        r_grid = case[\"r_grid\"]\n        Q_tot = case[\"Q_tot\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n        N = case[\"N\"]\n        M = r_grid.shape[0]\n\n        # 1. Construct the design matrix A\n        # A_ji = 1 / ||r_j - R_i||\n        # Using broadcasting for efficiency:\n        distances = np.linalg.norm(r_grid[:, np.newaxis, :] - R_atoms[np.newaxis, :, :], axis=2)\n        A = 1.0 / distances\n\n        # 2. Generate the observed potential vector V\n        V_true = A @ q_true\n        rng = np.random.default_rng(seed)\n        eta = rng.normal(loc=0.0, scale=sigma, size=M)\n        V_obs = V_true + eta\n\n        # 3. Unconstrained least-squares fit\n        q_uncon = np.linalg.lstsq(A, V_obs, rcond=None)[0]\n\n        # 4. Constrained least-squares fit using KKT system\n        # | A^T*A   c | |  q_con  | = | A^T*V |\n        # |  c^T    0 | | lambda  |   | Q_tot |\n        \n        ATA = A.T @ A\n        ATV = A.T @ V_obs\n        \n        # Build KKT matrix (N+1 x N+1)\n        KKT_matrix = np.zeros((N + 1, N + 1))\n        KKT_matrix[:N, :N] = ATA\n        c = np.ones(N)\n        KKT_matrix[:N, N] = c\n        KKT_matrix[N, :N] = c\n\n        # Build RHS vector (N+1)\n        rhs = np.zeros(N + 1)\n        rhs[:N] = ATV\n        rhs[N] = Q_tot\n\n        # Solve the KKT system\n        solution_kkt = np.linalg.solve(KKT_matrix, rhs)\n        q_con = solution_kkt[:N]\n\n        # 5. Compute required quantities\n        # a. Total charge from unconstrained fit\n        total_q_uncon = np.sum(q_uncon)\n\n        # b. Dipole error for unconstrained fit\n        dipole_err_uncon_vec = (q_uncon - q_true) @ R_atoms\n        dipole_err_uncon_norm = np.linalg.norm(dipole_err_uncon_vec)\n        \n        # c. Dipole error for constrained fit\n        dipole_err_con_vec = (q_con - q_true) @ R_atoms\n        dipole_err_con_norm = np.linalg.norm(dipole_err_con_vec)\n\n        all_results.append([\n            total_q_uncon,\n            dipole_err_uncon_norm,\n            dipole_err_con_norm\n        ])\n\n    # Final print statement in the exact required format\n    # produces [[...],[...],[...]] without spaces\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2889373"}, {"introduction": "While the constrained least-squares formulation provides a sound theoretical basis, its numerical implementation can be fragile if the grid points are not chosen carefully, leading to ill-conditioned linear systems and unreliable charges. This practice [@problem_id:2889411] tackles this issue head-on by having you implement a grid de-duplication strategy based on kernel correlation. This exercise will provide you with a powerful tool to ensure the numerical robustness of your calculations and a deeper understanding of the connection between data redundancy and matrix conditioning.", "problem": "You are given a standard Electrostatic Potential (ESP) charge fitting setup from quantum chemistry. The total externally computed electrostatic potential at a set of grid points is modeled as the Coulomb potential generated by fixed point charges located at given atomic positions. Let there be $M$ atoms at positions $\\{\\mathbf{R}_A\\}_{A=1}^{M}$ in three-dimensional space, with unknown charges $\\{q_A\\}_{A=1}^{M}$, and let there be $N$ grid points at positions $\\{\\mathbf{r}_i\\}_{i=1}^{N}$. The Coulomb kernel induced feature vector at a grid point is defined by the component-wise mapping $\\phi_A(\\mathbf{r}) = 1 / \\|\\mathbf{r} - \\mathbf{R}_A\\|$, where the norm is the Euclidean norm. Using this definition, the linear system matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times M}$ has entries $A_{iA} = \\phi_A(\\mathbf{r}_i)$. In standard least squares fitting of ESP-derived atomic charges, the normal equations involve the matrix $\\mathbf{A}^\\top \\mathbf{A}$. When rows of $\\mathbf{A}$ are nearly linearly dependent, $\\mathbf{A}^\\top \\mathbf{A}$ becomes ill-conditioned and the fitted charges become numerically unstable.\n\nYour task is to design and implement a principled grid de-duplication strategy that removes grid points that are redundant with respect to the Coulomb kernel. Two grid points $\\mathbf{r}_i$ and $\\mathbf{r}_j$ are considered redundant if the cosine correlation between their kernel feature vectors exceeds a specified threshold. For any two grid points $\\mathbf{r}_i$ and $\\mathbf{r}_j$, define the kernel correlation\n$$\nc(i,j) \\;=\\; \\frac{\\langle \\boldsymbol{\\phi}(\\mathbf{r}_i), \\boldsymbol{\\phi}(\\mathbf{r}_j) \\rangle}{\\|\\boldsymbol{\\phi}(\\mathbf{r}_i)\\|_2 \\, \\|\\boldsymbol{\\phi}(\\mathbf{r}_j)\\|_2} \\;=\\; \\frac{\\sum_{A=1}^{M} \\frac{1}{\\|\\mathbf{r}_i-\\mathbf{R}_A\\|} \\cdot \\frac{1}{\\|\\mathbf{r}_j-\\mathbf{R}_A\\|}}{\\left(\\sum_{A=1}^{M} \\frac{1}{\\|\\mathbf{r}_i-\\mathbf{R}_A\\|^2}\\right)^{1/2}\\left(\\sum_{A=1}^{M} \\frac{1}{\\|\\mathbf{r}_j-\\mathbf{R}_A\\|^2}\\right)^{1/2}} \\,,\n$$\nwhich lies in $[0,1]$ for strictly positive features. For a user-specified threshold $\\tau \\in [0,1]$, the de-duplication strategy must construct a subset $S \\subset \\{1,\\dots,N\\}$ of retained grid indices such that for all distinct $i,j \\in S$ one has $c(i,j) \\le \\tau$, while attempting to retain as many informative points as possible to avoid loss of resolution. You must justify mathematically why imposing an upper bound $\\tau$ on all pairwise correlations controls near-linear dependence among rows of $\\mathbf{A}$ and improves the conditioning of the least squares problem. Your algorithm should be deterministic, and in case of potential ties it should prefer retaining points with larger feature norm $\\|\\boldsymbol{\\phi}(\\mathbf{r}_i)\\|_2$.\n\nFundamental starting points that you may use include: the Coulomb law potential $V(\\mathbf{r}) = \\sum_{A=1}^{M} q_A / \\|\\mathbf{r} - \\mathbf{R}_A\\|$, the Euclidean inner product and norm, properties of the Gram matrix, Gershgorin’s circle theorem, and basic linear least squares conditioning facts. Do not assume any pre-given de-duplication formula; derive the logic from these bases.\n\nYour program must implement the following deterministic greedy selection strategy: sort grid points in descending order of $\\|\\boldsymbol{\\phi}(\\mathbf{r}_i)\\|_2$, then sequentially retain a point if and only if its correlation $c(i,j)$ with every previously retained point $j$ is $\\le \\tau$. If $\\tau \\ge 1$, retain all points. If $\\tau \\le 0$, retain at most one point (specifically, the first in the sorted order). Use a strict inequality $c(i,j) > \\tau$ as the criterion to discard a candidate $i$.\n\nPhysical units: all positions $\\mathbf{R}_A$ and $\\mathbf{r}_i$ are specified in ångström ($\\text{\\AA}$). The algorithm’s output is unitless.\n\nTest suite: use the following atomic centers and grid sets.\n- Atom centers $\\{\\mathbf{R}_A\\}_{A=1}^{M}$ with $M = 3$: \n  - $\\mathbf{R}_1 = (0.0000,\\, 0.0000,\\, 0.0000)$,\n  - $\\mathbf{R}_2 = (0.9572,\\, 0.0000,\\, 0.0000)$,\n  - $\\mathbf{R}_3 = (-0.2390,\\, 0.9270,\\, 0.0000)$.\n- Define five test cases, each a triple $(\\text{grid}, \\tau, \\text{name})$:\n  - Case $1$ (clustered far-field, moderate threshold): grid points $\\mathbf{r}_i$ equal to $(3.00,\\,0.00,\\,0.00)$, $(3.05,\\,0.00,\\,0.00)$, $(3.10,\\,0.00,\\,0.00)$, $(0.00,\\,3.00,\\,0.00)$, $(-3.00,\\,0.00,\\,0.00)$ with $\\tau = 0.95$.\n  - Case $2$ (duplicate points, high threshold): grid points $\\mathbf{r}_i$ equal to $(2.50,\\,0.00,\\,0.00)$, $(2.50,\\,0.00,\\,0.00)$, $(0.00,\\,2.50,\\,0.00)$, $(0.00,\\,2.50,\\,0.00)$, $(0.00,\\,0.00,\\,2.50)$ with $\\tau = 0.99$.\n  - Case $3$ (near-nuclear cluster, stricter threshold): grid points $\\mathbf{r}_i$ equal to $(0.20,\\,0.00,\\,0.00)$, $(0.25,\\,0.00,\\,0.00)$, $(0.30,\\,0.00,\\,0.00)$, $(0.00,\\,0.20,\\,0.00)$, $(0.00,\\,0.00,\\,0.20)$ with $\\tau = 0.90$.\n  - Case $4$ (boundary keep-all): grid points $\\mathbf{r}_i$ equal to $(1.50,\\,0.50,\\,0.00)$, $(-1.50,\\,-0.50,\\,0.00)$, $(0.00,\\,0.00,\\,1.50)$ with $\\tau = 1.00$.\n  - Case $5$ (boundary keep-one): grid points $\\mathbf{r}_i$ equal to $(4.00,\\,0.00,\\,0.00)$, $(0.00,\\,4.00,\\,0.00)$, $(0.00,\\,0.00,\\,4.00)$ with $\\tau = 0.00$.\n\nFor each test case, your program must compute the number of retained grid points after applying the specified de-duplication strategy. Your final program output must be a single line containing the results for cases $1$ through $5$ as a comma-separated list enclosed in square brackets, for example $[n_1,n_2,n_3,n_4,n_5]$, where $n_k$ is the integer number of retained grid points for case $k$.", "solution": "**Mathematical Justification**\n\nThe task requires a justification for why bounding the pairwise correlation $c(i,j)$ improves the conditioning of the least-squares problem. The least-squares solution for the charges $\\mathbf{q}$ is found by solving the normal equations $(\\mathbf{A}^\\top \\mathbf{A}) \\mathbf{q} = \\mathbf{A}^\\top \\mathbf{v}$, where $\\mathbf{v}$ is the vector of electrostatic potentials at the grid points. The numerical stability of this solution is governed by the condition number of the matrix $\\mathbf{A}^\\top \\mathbf{A}$. A large condition number signifies an ill-conditioned problem, where small perturbations in the input can lead to large changes in the solution $\\mathbf{q}$.\n\nThe de-duplication strategy creates a subset of grid points, which corresponds to selecting a subset of rows from the matrix $\\mathbf{A}$ to form a new matrix, let us call it $\\mathbf{A}_S$. The rows of $\\mathbf{A}_S$ are the feature vectors $\\{\\boldsymbol{\\phi}(\\mathbf{r}_i)\\}_{i \\in S}$ for the set of retained grid points $S$. The new normal equations involve the matrix $\\mathbf{A}_S^\\top \\mathbf{A}_S$. We analyze its conditioning.\n\nThe non-zero eigenvalues of $\\mathbf{A}_S^\\top \\mathbf{A}_S$ are identical to the non-zero eigenvalues of the Gram matrix $\\mathbf{G} = \\mathbf{A}_S \\mathbf{A}_S^\\top$. The entries of this Gram matrix are the inner products of the selected feature vectors: $G_{ij} = \\langle \\boldsymbol{\\phi}(\\mathbf{r}_i), \\boldsymbol{\\phi}(\\mathbf{r}_j) \\rangle$ for $i,j \\in S$. The condition number of $\\mathbf{A}_S^\\top \\mathbf{A}_S$ is related to the ratio of the largest to the smallest eigenvalue of $\\mathbf{G}$.\n\nThe prescribed algorithm controls the off-diagonal elements of the *normalized* Gram matrix, $\\mathbf{G}'$, whose entries are $G'_{ij} = c(i,j)$. By construction, for all distinct $i,j \\in S$, we have $|G'_{ij}| = |c(i,j)| \\le \\tau$. The diagonal elements are $G'_{ii} = c(i,i) = 1$.\n\nWe can apply Gershgorin's circle theorem to the matrix $\\mathbf{G}'$. The theorem states that every eigenvalue of $\\mathbf{G}'$ lies within at least one of the Gershgorin disks $D(G'_{ii}, R_i)$ in the complex plane, where the center is $G'_{ii}$ and the radius is $R_i = \\sum_{j \\neq i} |G'_{ij}|$.\nFor our matrix $\\mathbf{G}'$ of size $k \\times k$ (where $k = |S|$ is the number of retained points), we have:\n- Center of the disk: $G'_{ii} = 1$.\n- Radius of the disk: $R_i = \\sum_{j \\in S, j \\neq i} |c(i,j)| \\le \\sum_{j \\in S, j \\neq i} \\tau = (k-1)\\tau$.\n\nSince $\\mathbf{G}'$ is a real symmetric matrix, its eigenvalues $\\lambda$ are real. Thus, all eigenvalues must lie in the union of the real intervals $[1 - R_i, 1 + R_i]$. This implies:\n$$ \\lambda_{\\min}(\\mathbf{G}') \\ge 1 - \\max_i R_i \\ge 1 - (k-1)\\tau $$\n$$ \\lambda_{\\max}(\\mathbf{G}') \\le 1 + \\max_i R_i \\le 1 + (k-1)\\tau $$\nThe condition number of $\\mathbf{G}'$ is $\\kappa(\\mathbf{G}') = \\lambda_{\\max}(\\mathbf{G}') / \\lambda_{\\min}(\\mathbf{G}')$. We can bound it as:\n$$ \\kappa(\\mathbf{G}') \\le \\frac{1 + (k-1)\\tau}{1 - (k-1)\\tau} $$\nThis bound is meaningful if $1 - (k-1)\\tau > 0$, i.e., $\\tau < 1/(k-1)$. By enforcing a small threshold $\\tau$, we ensure that the rows of $\\mathbf{A}_S$ are far from being linearly dependent. This keeps the eigenvalues of the normalized Gram matrix $\\mathbf{G}'$ away from zero, which bounds its condition number. Bounding the condition number of $\\mathbf{G}'$ is a standard method for ensuring the underlying matrix $\\mathbf{G}$ is well-conditioned. Since $\\mathbf{A}_S^\\top \\mathbf{A}_S$ shares its non-zero eigenvalues with $\\mathbf{G}$, this procedure directly improves the conditioning of the normal equations matrix, leading to a numerically stable and reliable computation of the ESP charges.\n\n**Algorithmic Design**\n\nThe implementation follows the prescribed deterministic greedy selection strategy.\n\n1.  **Preprocessing**: For a given test case (atomic coordinates $\\{\\mathbf{R}_A\\}$, grid points $\\{\\mathbf{r}_i\\}$), we first compute the necessary data for each grid point $i \\in \\{1,\\dots,N\\}$.\n    -   The feature vector $\\boldsymbol{\\phi}(\\mathbf{r}_i) \\in \\mathbb{R}^M$, where the $A$-th component is $1/\\|\\mathbf{r}_i - \\mathbf{R}_A\\|_2$.\n    -   The L2-norm of the feature vector, $\\|\\boldsymbol{\\phi}(\\mathbf{r}_i)\\|_2$.\n    This is best done by creating an $N \\times M$ matrix of feature vectors and an $N$-dimensional array of their norms.\n\n2.  **Sorting**: The original indices of the grid points, $\\{0, 1, \\dots, N-1\\}$, are sorted in descending order based on the pre-calculated norms $\\|\\boldsymbol{\\phi}(\\mathbf{r}_i)\\|_2$. This establishes the prioritized order in which points are considered for retention.\n\n3.  **Greedy Selection Loop**:\n    -   An empty list, `retained_indices`, is initialized to store the indices of the grid points that are kept.\n    -   The algorithm iterates through the sorted indices. For each candidate index `current_idx`:\n        a. A flag `is_redundant` is set to `False`.\n        b. The algorithm then iterates through the indices already present in `retained_indices`. Let one such index be `retained_idx`.\n        c. The kernel correlation $c(\\text{current\\_idx}, \\text{retained\\_idx})$ is computed using the pre-calculated feature vectors and norms.\n        $$ c(\\text{current\\_idx}, \\text{retained\\_idx}) = \\frac{\\boldsymbol{\\phi}(\\mathbf{r}_{\\text{current\\_idx}}) \\cdot \\boldsymbol{\\phi}(\\mathbf{r}_{\\text{retained\\_idx}})}{\\|\\boldsymbol{\\phi}(\\mathbf{r}_{\\text{current\\_idx}})\\|_2 \\|\\boldsymbol{\\phi}(\\mathbf{r}_{\\text{retained\\_idx}})\\|_2} $$\n        d. If this correlation is greater than the threshold $\\tau$, i.e., $c(\\text{current\\_idx}, \\text{retained\\_idx}) > \\tau$, the candidate point is deemed redundant with respect to an already-retained point. The `is_redundant` flag is set to `True`, and the inner loop (over `retained_indices`) is terminated.\n        e. After checking against all `retained_indices`, if the `is_redundant` flag remains `False`, the `current_idx` is added to the `retained_indices` list.\n    -   This process is repeated for all candidate indices.\n\n4.  **Result**: The final result for the test case is the size of the `retained_indices` list. This procedure is applied for each of the five test cases, and the results are collected.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the grid de-duplication problem for the given test cases.\n    \"\"\"\n    \n    # Define atomic centers as specified in the problem.\n    atom_centers = np.array([\n        [0.0000, 0.0000, 0.0000],\n        [0.9572, 0.0000, 0.0000],\n        [-0.2390, 0.9270, 0.0000]\n    ])\n\n    # Define the five test cases. Each is a tuple of (grid_points, threshold).\n    test_cases = [\n        (np.array([\n            [3.00, 0.00, 0.00], [3.05, 0.00, 0.00], [3.10, 0.00, 0.00],\n            [0.00, 3.00, 0.00], [-3.00, 0.00, 0.00]\n        ]), 0.95),  # Case 1\n        (np.array([\n            [2.50, 0.00, 0.00], [2.50, 0.00, 0.00],\n            [0.00, 2.50, 0.00], [0.00, 2.50, 0.00],\n            [0.00, 0.00, 2.50]\n        ]), 0.99),  # Case 2\n        (np.array([\n            [0.20, 0.00, 0.00], [0.25, 0.00, 0.00], [0.30, 0.00, 0.00],\n            [0.00, 0.20, 0.00], [0.00, 0.00, 0.20]\n        ]), 0.90),  # Case 3\n        (np.array([\n            [1.50, 0.50, 0.00], [-1.50, -0.50, 0.00], [0.00, 0.00, 1.50]\n        ]), 1.00),  # Case 4\n        (np.array([\n            [4.00, 0.00, 0.00], [0.00, 4.00, 0.00], [0.00, 0.00, 4.00]\n        ]), 0.00)   # Case 5\n    ]\n\n    results = []\n    \n    for grid_points, threshold in test_cases:\n        num_grid_points = grid_points.shape[0]\n        num_atoms = atom_centers.shape[0]\n\n        # Step 1: Preprocessing - Calculate feature vectors and their norms.\n        feature_vectors = np.zeros((num_grid_points, num_atoms))\n        for i in range(num_grid_points):\n            for a in range(num_atoms):\n                dist = np.linalg.norm(grid_points[i] - atom_centers[a])\n                # Handle the case where a grid point is exactly on an atom, though not in test data.\n                if dist > 1e-9:\n                    feature_vectors[i, a] = 1.0 / dist\n                else: # To avoid division by zero, assign a large but finite value.\n                    feature_vectors[i, a] = 1e9\n\n        feature_norms = np.linalg.norm(feature_vectors, axis=1)\n\n        # Step 2: Sorting - Sort indices by descending norm.\n        # Use negative norms for ascending sort to get descending order.\n        # np.argsort is stable, which handles ties correctly based on original order.\n        sorted_indices = np.argsort(-feature_norms, kind='stable')\n\n        # Step 3: Greedy Selection\n        retained_indices = []\n        for i in sorted_indices:\n            is_redundant = False\n            # Check correlation with already retained points.\n            for j in retained_indices:\n                # Handle perfect duplicates to avoid division by zero if norms are zero (not an issue here)\n                if feature_norms[i] == 0 or feature_norms[j] == 0:\n                    correlation = 1.0 if i == j else 0.0\n                else:\n                    dot_product = np.dot(feature_vectors[i], feature_vectors[j])\n                    correlation = dot_product / (feature_norms[i] * feature_norms[j])\n\n                if correlation > threshold:\n                    is_redundant = True\n                    break\n            \n            if not is_redundant:\n                retained_indices.append(i)\n\n        results.append(len(retained_indices))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2889411"}, {"introduction": "Having established a robust framework, we now consider the impact of methodological choices on the final charges and their chemical interpretation. This conceptual problem [@problem_id:2889395] explores how two popular grid-generation protocols, CHELPG and Merz-Kollman (MK), can yield systematically different atomic charges due to their distinct sampling philosophies. By analyzing the case of a planar aromatic molecule, you will learn to reason about how the choice of fitting grid interacts with the molecule's electronic structure, reinforcing the key insight that ESP-derived charges are model-dependent parameters.", "problem": "Consider a neutral, planar aromatic molecule (for concreteness, benzene, $\\text{C}_6\\text{H}_6$) lying in the $xy$-plane with its ring normal along $\\hat{\\mathbf{z}}$. In quantum chemistry, electrostatic-potential-derived (ESP) atomic charges are obtained by least-squares fitting of atom-centered point charges $\\{q_i\\}$ at nuclear positions $\\{\\mathbf{R}_i\\}$ to the molecular electrostatic potential $V(\\mathbf{r})$ computed from the electron density $\\rho(\\mathbf{r})$ and nuclear charges $\\{Z_A\\}$:\n$$\nV(\\mathbf{r}) \\;=\\; \\sum_{A} \\frac{Z_A}{\\lvert \\mathbf{r}-\\mathbf{R}_A\\rvert} \\;-\\; \\int \\frac{\\rho(\\mathbf{r}')}{\\lvert \\mathbf{r}-\\mathbf{r}'\\rvert}\\,d\\mathbf{r}' \\;\\approx\\; \\sum_{i} \\frac{q_i}{\\lvert \\mathbf{r}-\\mathbf{R}_i\\rvert}.\n$$\nTwo widely used ESP charge definitions differ in how the fitting points $\\{\\mathbf{r}_k\\}$ are chosen. Charges from Electrostatic Potentials using a Grid (CHELPG) employs a rectilinear grid in space (excluding a small sphere of radius $d$ around each nucleus to avoid singularities), whereas the Merz–Kollman (MK) scheme distributes points on one or more Connolly surfaces offset from the van der Waals surface by a scale factor $s$ and a probe radius, thereby emphasizing regions near the molecular surface. For a planar aromatic ring, the local electrostatic potential is anisotropic around a ring carbon: along $\\pm \\hat{\\mathbf{z}}$ above and below the ring, $V(\\mathbf{r})$ is dominated by delocalized $\\pi$ density and is relatively negative at modest $z$, whereas in directions within the plane (near the ring periphery), $V(\\mathbf{r})$ is less negative (or even slightly positive very close to nuclei) because the nuclear term is stronger at short range. Assume a typical CHELPG grid spacing $h$ and exclusion radius $d$ that lead to many accessible points at small $\\lvert z\\rvert$ above and below the plane, and an MK construction with $s>1$ that samples near-surface regions densely around the molecular periphery.\n\nWhich statement best predicts how the fitted charges on the ring carbons will differ between CHELPG and MK for such a planar aromatic molecule, and why, based on the anisotropy of $V(\\mathbf{r})$?\n\nA. CHELPG will tend to assign more negative charges to the ring carbons than MK, because the cubic grid includes many points directly above and below the ring plane where $V(\\mathbf{r})$ is relatively negative due to $\\pi$ density, whereas MK emphasizes near-surface points around the ring periphery in-plane where $V(\\mathbf{r})$ is less negative.\n\nB. CHELPG will tend to assign more positive charges to the ring carbons than MK, because the uniform grid overweights near-nuclear, in-plane points where the nuclear term dominates $V(\\mathbf{r})$.\n\nC. In the limit of very fine grids ($h \\to 0$) and large surfaces, CHELPG and MK must converge to the same ring carbon charges for any neutral planar aromatic, because the true $V(\\mathbf{r})$ is unique and determines a unique set of atom-centered point charges.\n\nD. Any difference between CHELPG and MK charges at ring carbons disappears when hydrogens are included in the fit, because hydrogen atoms screen the anisotropy of $V(\\mathbf{r})$ above and below the ring, making the two sampling schemes equivalent.", "solution": "### Derivation\nThe core of the problem is to understand how the choice of fitting points, $\\{\\mathbf{r}_k\\}$, influences the outcome of a least-squares fitting procedure. The procedure minimizes the sum of squared errors, $\\chi^2 = \\sum_k [V(\\mathbf{r}_k) - \\sum_i q_i/\\lvert \\mathbf{r}_k - \\mathbf{R}_i\\rvert]^2$, by adjusting the parameters $\\{q_i\\}$. Consequently, the resulting charges $\\{q_i\\}$ will be optimized to best reproduce the true ESP, $V(\\mathbf{r})$, in the regions of space where the fitting points are most densely concentrated.\n\n1.  **Analysis of the Electrostatic Potential, $V(\\mathbf{r})$**: For a planar aromatic ring such as benzene, the electron density is not isotropic. There is a high concentration of electron density from the delocalized $\\pi$-system in lobes above and below the molecular plane. This electron density makes the electrostatic potential $V(\\mathbf{r})$ significantly negative in these regions. In contrast, in the plane of the molecule ($z=0$) and around its periphery, the shielding of the positive nuclear charges by the electrons is less effective. Specifically, the $\\sigma$-framework and the nuclei dominate, leading to a region where $V(\\mathbf{r})$ is less negative, or can even be positive, particularly near the hydrogen atoms.\n\n2.  **Analysis of the CHELPG Sampling Scheme**: The CHELPG method uses a uniform cubic grid. For a planar molecule, this grid samples space both in the plane and, crucially, above and below the plane. The problem states that the grid is constructed to have \"many accessible points at small $\\lvert z\\rvert$ above and below the plane\". These are precisely the regions where $V(\\mathbf{r})$ is strongly negative due to the $\\pi$-electron cloud. The least-squares fitting procedure, when presented with many points in this negative-potential region, will be heavily biased towards reproducing this feature. To generate a negative potential above and below the carbon ring, the point charges, $q_i$, on the carbon atoms must be assigned more negative values.\n\n3.  **Analysis of the MK Sampling Scheme**: The MK method samples points on Connolly surfaces, which follow the molecular surface (related to the van der Waals surface). For benzene, this surface is 'doughnut' shaped. The problem states this method \"samples near-surface regions densely around the molecular periphery\". This means the MK points are concentrated in the $xy$-plane around the outer edge of the molecule, near the C-H bonds. As established, in this region, $V(\\mathbf{r})$ is \"less negative\" than it is above or below the ring. The fitting procedure is therefore optimizing the charges $\\{q_i\\}$ to match the potential in this peripheral region. To reproduce a less negative potential, the carbon charges, $q_i$, will be fitted to be less negative (or more positive) compared to a fit that emphasizes the $\\pi$-region.\n\n4.  **Conclusion**: By comparing the two schemes, we deduce that CHELPG, by sampling the negative-potential region of the $\\pi$-cloud, will force the carbon charges to become more negative to account for this. MK, by sampling the less-negative peripheral region, will result in less negative (or more positive) carbon charges. Therefore, CHELPG is expected to produce more negative charges on the ring carbons than the MK scheme.\n\n### Evaluation of Options\n\n**A. CHELPG will tend to assign more negative charges to the ring carbons than MK, because the cubic grid includes many points directly above and below the ring plane where $V(\\mathbf{r})$ is relatively negative due to $\\pi$ density, whereas MK emphasizes near-surface points around the ring periphery in-plane where $V(\\mathbf{r})$ is less negative.**\nThis statement is a direct and accurate consequence of the derivation above. It correctly identifies the key sampling regions for each method (above/below plane for CHELPG, periphery for MK) and connects them to the known anisotropy of the ESP, leading to the correct conclusion about the relative values of the fitted charges.\n**Verdict: Correct.**\n\n**B. CHELPG will tend to assign more positive charges to the ring carbons than MK, because the uniform grid overweights near-nuclear, in-plane points where the nuclear term dominates $V(\\mathbf{r})$.**\nThis statement makes an incorrect conclusion and provides faulty reasoning. For a planar molecule, the volumetric sampling of CHELPG leads to a heavy emphasis on the regions above and below the plane, not an overweighting of in-plane points. The MK scheme is the one that concentrates points in-plane at the periphery. The conclusion that CHELPG gives more positive charges is the opposite of the correct deduction.\n**Verdict: Incorrect.**\n\n**C. In the limit of very fine grids ($h \\to 0$) and large surfaces, CHELPG and MK must converge to the same ring carbon charges for any neutral planar aromatic, because the true $V(\\mathbf{r})$ is unique and determines a unique set of atom-centered point charges.**\nThis statement is fundamentally flawed. The representation of a molecule's continuous charge distribution by a discrete set of atom-centered point charges is an approximation and an ill-posed problem. There is no unique set of point charges that can perfectly reproduce the external ESP at all points in space. The charges are artifacts of the fitting procedure, including the choice of fitting points. Different fitting schemes (CHELPG vs. MK) are defined by different spatial domains and will, by definition, converge to different sets of charges, as they are optimizing for different aspects of the true ESP.\n**Verdict: Incorrect.**\n\n**D. Any difference between CHELPG and MK charges at ring carbons disappears when hydrogens are included in the fit, because hydrogen atoms screen the anisotropy of $V(\\mathbf{r})$ above and below the ring, making the two sampling schemes equivalent.**\nThis statement is incorrect. Including hydrogens in the fit is standard and necessary (to satisfy charge neutrality, for example), but it does not remove the fundamental difference between the CHELPG and MK sampling schemes. The hydrogens themselves have an electrostatic signature, but they do not eliminate the strong negative potential from the $\\pi$-system above and below the ring. The anisotropy of $V(\\mathbf{r})$ persists. The geometric difference between a volumetric grid (CHELPG) and a surface-based grid (MK) remains, so they will continue to sample different regions of the ESP and produce different charges.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2889395"}]}