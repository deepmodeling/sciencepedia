{"hands_on_practices": [{"introduction": "This first exercise solidifies the fundamental definition of a bond critical point (BCP) by returning to first principles. Using a simple, analytically tractable model for the electron density of a diatomic molecule, you will locate the BCP by finding where the gradient $\\nabla \\rho$ vanishes. By calculating the Hessian matrix at this point, you will directly observe the signature $(3, -1)$ that defines a BCP, building an intuitive understanding of the local curvature of the electron density that signifies a chemical bond [@problem_id:2876140].", "problem": "Consider a homonuclear diatomic molecule aligned along the $z$-axis and modeled by a cylindrically symmetric, analytic electron density given by\n$$\n\\rho(x,y,z) \\;=\\; A\\left[\\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+\\left(z-\\frac{R}{2}\\right)^{2}\\right)\\right) \\;+\\; \\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+\\left(z+\\frac{R}{2}\\right)^{2}\\right)\\right)\\right],\n$$\nwhere $A0$, $\\alpha0$, and $R0$. Work in atomic units, where lengths are in Bohr $a_{0}$, electron density is in $a_{0}^{-3}$, and Hessian elements of the density have units $a_{0}^{-5}$. Use the following parameter values: $A=1.0\\,a_{0}^{-3}$, $\\alpha=1.0\\,a_{0}^{-2}$, and $R=2.0\\,a_{0}$. In the Quantum Theory of Atoms in Molecules (QTAIM), a bond critical point (BCP) is a point where the gradient of the electron density $\\nabla \\rho(\\mathbf{r})$ vanishes and the Hessian of the density, $H_{ij}=\\partial^{2}\\rho/\\partial x_{i}\\partial x_{j}$, has eigenvalues with two negative curvatures transverse to the bond and one positive curvature along the bond.\n\nStarting from the fundamental definitions of a critical point and the Hessian of a scalar field:\n- A critical point satisfies $\\nabla \\rho(\\mathbf{r})=\\mathbf{0}$.\n- The Hessian is the $3\\times 3$ matrix with entries $H_{ij}=\\partial^{2}\\rho/\\partial x_{i}\\partial x_{j}$.\n- The eigenvalues of $H$ determine the local curvature of $\\rho$.\n\nDo the following:\n1. By symmetry and direct evaluation of first derivatives, locate the bond critical point along the internuclear axis for the given $\\rho(x,y,z)$.\n2. Compute the Hessian matrix $H_{ij}$ at that point.\n3. Determine its three eigenvalues and use their signs to identify the pointâ€™s topological type in the QTAIM sense.\n\nReport the ordered eigenvalues $\\lambda_{1}\\le \\lambda_{2}\\le \\lambda_{3}$ as a single row vector in atomic units. No rounding is required; provide exact symbolic expressions.", "solution": "We begin from the definitions. A critical point of the scalar field $\\rho(\\mathbf{r})$ is a point where the gradient $\\nabla \\rho(\\mathbf{r})=\\mathbf{0}$. The Hessian $H$ is the matrix of second derivatives with entries $H_{ij}=\\partial^{2}\\rho/\\partial x_{i}\\partial x_{j}$. The eigenvalues of $H$ characterize curvature along principal directions.\n\nStep 1: Locate the bond critical point. The electron density is\n$$\n\\rho(x,y,z)=A\\left[\\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+\\left(z-\\frac{R}{2}\\right)^{2}\\right)\\right)+\\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+\\left(z+\\frac{R}{2}\\right)^{2}\\right)\\right)\\right],\n$$\nwhich is invariant under $(x,y)\\mapsto(-x,-y)$ and under $z\\mapsto -z$ because the two terms are centered symmetrically at $z=\\pm R/2$. By cylindrical symmetry, the point $(x,y,z)=(0,0,0)$ is a candidate for a critical point. We verify by computing first derivatives.\n\nLet $z_{0}=R/2$. Define\n$$\n\\rho_{+}(x,y,z)=A\\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+(z-z_{0})^{2}\\right)\\right),\\quad\n\\rho_{-}(x,y,z)=A\\exp\\!\\left(-\\alpha\\left(x^{2}+y^{2}+(z+z_{0})^{2}\\right)\\right),\n$$\nso $\\rho=\\rho_{+}+\\rho_{-}$. The first derivatives are\n$$\n\\frac{\\partial \\rho_{\\pm}}{\\partial x}=-2\\alpha x\\,\\rho_{\\pm},\\quad\n\\frac{\\partial \\rho_{\\pm}}{\\partial y}=-2\\alpha y\\,\\rho_{\\pm},\\quad\n\\frac{\\partial \\rho_{+}}{\\partial z}=-2\\alpha (z-z_{0})\\,\\rho_{+},\\quad\n\\frac{\\partial \\rho_{-}}{\\partial z}=-2\\alpha (z+z_{0})\\,\\rho_{-}.\n$$\nAt $(0,0,0)$, we have $x=0$, $y=0$, and\n$$\n\\left.\\frac{\\partial \\rho}{\\partial x}\\right|_{\\mathbf{0}}=0,\\quad\n\\left.\\frac{\\partial \\rho}{\\partial y}\\right|_{\\mathbf{0}}=0,\n$$\nand\n$$\n\\left.\\frac{\\partial \\rho}{\\partial z}\\right|_{\\mathbf{0}}=-2\\alpha\\left[-z_{0}\\,\\rho_{+}(0,0,0)+z_{0}\\,\\rho_{-}(0,0,0)\\right]=0,\n$$\nbecause $\\rho_{+}(0,0,0)=\\rho_{-}(0,0,0)=A\\exp(-\\alpha z_{0}^{2})$. Thus $\\nabla \\rho(0,0,0)=\\mathbf{0}$, so $(0,0,0)$ is a critical point. In a symmetric diatomic, this is the bond critical point along the bond axis.\n\nStep 2: Compute the Hessian at the BCP. We compute second derivatives of $\\rho_{\\pm}$. For $x$ and $y$ directions,\n$$\n\\frac{\\partial^{2}\\rho_{\\pm}}{\\partial x^{2}}=\\frac{\\partial}{\\partial x}\\left(-2\\alpha x\\,\\rho_{\\pm}\\right)\n=-2\\alpha\\,\\rho_{\\pm}+(-2\\alpha x)\\left(-2\\alpha x\\,\\rho_{\\pm}\\right)\n=\\left(-2\\alpha+4\\alpha^{2}x^{2}\\right)\\rho_{\\pm},\n$$\nand similarly\n$$\n\\frac{\\partial^{2}\\rho_{\\pm}}{\\partial y^{2}}=\\left(-2\\alpha+4\\alpha^{2}y^{2}\\right)\\rho_{\\pm}.\n$$\nFor the $z$ direction,\n$$\n\\frac{\\partial^{2}\\rho_{+}}{\\partial z^{2}}\n=\\frac{\\partial}{\\partial z}\\left(-2\\alpha(z-z_{0})\\,\\rho_{+}\\right)\n=-2\\alpha\\,\\rho_{+}+(-2\\alpha)(z-z_{0})\\left(-2\\alpha(z-z_{0})\\,\\rho_{+}\\right)\n=\\left(-2\\alpha+4\\alpha^{2}(z-z_{0})^{2}\\right)\\rho_{+},\n$$\nand\n$$\n\\frac{\\partial^{2}\\rho_{-}}{\\partial z^{2}}\n=\\left(-2\\alpha+4\\alpha^{2}(z+z_{0})^{2}\\right)\\rho_{-}.\n$$\nThe mixed second derivatives are\n$$\n\\frac{\\partial^{2}\\rho_{\\pm}}{\\partial x\\,\\partial y}\n=\\frac{\\partial}{\\partial x}\\left(-2\\alpha y\\,\\rho_{\\pm}\\right)\n=(-2\\alpha y)\\left(-2\\alpha x\\,\\rho_{\\pm}\\right)\n=4\\alpha^{2}xy\\,\\rho_{\\pm},\n$$\n$$\n\\frac{\\partial^{2}\\rho_{\\pm}}{\\partial x\\,\\partial z}\n=\\frac{\\partial}{\\partial x}\\left(-2\\alpha(z\\mp z_{0})\\,\\rho_{\\pm}\\right)\n=(-2\\alpha)(z\\mp z_{0})\\left(-2\\alpha x\\,\\rho_{\\pm}\\right)\n=4\\alpha^{2}x(z\\mp z_{0})\\,\\rho_{\\pm},\n$$\nand similarly for $\\partial^{2}\\rho_{\\pm}/\\partial y\\,\\partial z$.\n\nEvaluating at $(0,0,0)$, we have $x=0$, $y=0$, and $\\rho_{+}(0,0,0)=\\rho_{-}(0,0,0)=A\\exp(-\\alpha z_{0}^{2})$. Therefore,\n$$\n\\rho(0,0,0)=\\rho_{+}(0,0,0)+\\rho_{-}(0,0,0)=2A\\exp(-\\alpha z_{0}^{2}).\n$$\nThe diagonal second derivatives are\n$$\n\\left.\\frac{\\partial^{2}\\rho}{\\partial x^{2}}\\right|_{\\mathbf{0}}\n=\\sum_{\\pm}\\left(-2\\alpha+4\\alpha^{2}x^{2}\\right)\\rho_{\\pm}\\bigg|_{\\mathbf{0}}\n=\\sum_{\\pm}(-2\\alpha)\\rho_{\\pm}(0,0,0)\n=-2\\alpha\\,\\rho(0,0,0),\n$$\nand identically,\n$$\n\\left.\\frac{\\partial^{2}\\rho}{\\partial y^{2}}\\right|_{\\mathbf{0}}\n=-2\\alpha\\,\\rho(0,0,0).\n$$\nFor $z$,\n$$\n\\left.\\frac{\\partial^{2}\\rho}{\\partial z^{2}}\\right|_{\\mathbf{0}}\n=\\left[\\left(-2\\alpha+4\\alpha^{2}z_{0}^{2}\\right)\\rho_{+}(0,0,0)\\right]+\\left[\\left(-2\\alpha+4\\alpha^{2}z_{0}^{2}\\right)\\rho_{-}(0,0,0)\\right]\n=\\left(-2\\alpha+4\\alpha^{2}z_{0}^{2}\\right)\\rho(0,0,0).\n$$\nAll mixed derivatives vanish at $(0,0,0)$ because each contains a factor of $x$, $y$, or $z$, which is zero there:\n$$\n\\left.\\frac{\\partial^{2}\\rho}{\\partial x\\,\\partial y}\\right|_{\\mathbf{0}}=0,\\quad\n\\left.\\frac{\\partial^{2}\\rho}{\\partial x\\,\\partial z}\\right|_{\\mathbf{0}}=0,\\quad\n\\left.\\frac{\\partial^{2}\\rho}{\\partial y\\,\\partial z}\\right|_{\\mathbf{0}}=0.\n$$\nHence, the Hessian at the BCP is diagonal in the $(x,y,z)$ basis:\n$$\nH(0,0,0)=\\begin{pmatrix}\n-2\\alpha\\,\\rho(0,0,0)  0  0 \\\\\n0  -2\\alpha\\,\\rho(0,0,0)  0 \\\\\n0  0  \\left(-2\\alpha+4\\alpha^{2}z_{0}^{2}\\right)\\rho(0,0,0)\n\\end{pmatrix}\n$$\n\nStep 3: Eigenvalues and their signs for the specified parameters. Using $A=1.0$, $\\alpha=1.0$, and $R=2.0$, we have $z_{0}=R/2=1.0$ and\n$$\n\\rho(0,0,0)=2A\\exp(-\\alpha z_{0}^{2})=2\\exp(-1).\n$$\nTherefore,\n$$\n\\lambda_{x}=\\left.\\frac{\\partial^{2}\\rho}{\\partial x^{2}}\\right|_{\\mathbf{0}}=-2\\alpha\\,\\rho(0,0,0)=-2\\cdot 1\\cdot 2\\exp(-1)=-4\\exp(-1),\n$$\n$$\n\\lambda_{y}=\\left.\\frac{\\partial^{2}\\rho}{\\partial y^{2}}\\right|_{\\mathbf{0}}=-4\\exp(-1),\n$$\n$$\n\\lambda_{z}=\\left.\\frac{\\partial^{2}\\rho}{\\partial z^{2}}\\right|_{\\mathbf{0}}=\\left(-2\\alpha+4\\alpha^{2}z_{0}^{2}\\right)\\rho(0,0,0)=\\left(-2+4\\cdot 1\\cdot 1\\right)\\cdot 2\\exp(-1)=4\\exp(-1).\n$$\nThe ordered eigenvalues are $\\lambda_{1}=\\lambda_{x}=-4\\exp(-1)$, $\\lambda_{2}=\\lambda_{y}=-4\\exp(-1)$, and $\\lambda_{3}=\\lambda_{z}=4\\exp(-1)$. Two are negative (transverse curvatures) and one is positive (along the bond), identifying a $(3,-1)$ bond critical point in the Quantum Theory of Atoms in Molecules sense. All values are in atomic units, with Hessian elements in $a_{0}^{-5}$.", "answer": "$$\\boxed{\\begin{pmatrix}-4\\exp(-1)  -4\\exp(-1)  4\\exp(-1)\\end{pmatrix}}$$", "id": "2876140"}, {"introduction": "Real-world topological analysis requires automated methods for identifying all critical points, not just a single, known BCP. This practice guides you through the design of a robust computational algorithm to locate and classify all types of critical points (nuclear, bond, and cage) from a given density field [@problem_id:2876176]. You will implement a powerful workflow combining a global grid scan with a sophisticated local refinement technique, gaining insight into the engines that drive modern QTAIM software.", "problem": "You are given a family of smooth scalar fields in two dimensions that model electron density in the framework of the Quantum Theory of Atoms in Molecules (QTAIM). Let the electron density be defined by\n$$\n\\rho(x,y) \\equiv \\sum_{i=1}^{M} A_i \\exp\\!\\left(-\\alpha_i\\left[(x - x_i)^2 + (y - y_i)^2\\right]\\right),\n$$\nwhere $A_i  0$ are amplitudes, $\\alpha_i  0$ are exponents, and $(x_i,y_i)$ are centers of isotropic Gaussian contributions. A critical point (CP) is any point $(x^\\ast,y^\\ast)$ such that the gradient vanishes, that is\n$$\n\\nabla \\rho(x^\\ast,y^\\ast) = \\mathbf{0}.\n$$\nIn two dimensions, a CP is classified by the signs of the eigenvalues of the Hessian matrix\n$$\nH(x,y) \\equiv \n\\begin{bmatrix}\n\\frac{\\partial^2 \\rho}{\\partial x^2}  \\frac{\\partial^2 \\rho}{\\partial x \\partial y}\\\n$$4pt]\n\\frac{\\partial^2 \\rho}{\\partial y \\partial x}  \\frac{\\partial^2 \\rho}{\\partial y^2}\n\\end{bmatrix}.\n$$\nIf both eigenvalues are negative, the CP is a local maximum (nuclear-like); if one is negative and one positive, it is a saddle (bond-like); if both are positive, it is a local minimum (cage-like). Your task is to implement and test a robust algorithm that combines:\n- a uniform grid scan to seed candidate CP locations by detecting sign changes of gradient components over grid cells,\n- a Hessian sign-signature classification at converged points,\n- an eigenvector-following refinement step to converge from initial guesses to CPs.\n\nStarting only from fundamental definitions and standard calculus identities, implement the following requirements.\n\nAlgorithmic specifications:\n- Grid scanning: On a regular grid in a bounded square domain $[x_{\\min},x_{\\max}] \\times [y_{\\min},y_{\\max}]$, evaluate $\\nabla \\rho(x,y)$ at each grid node. For each grid cell (spanned by four adjacent nodes), mark it as a candidate if the range of each gradient component over the four corners contains $0$ (that is, the minimum is less than or equal to $0$ and the maximum is greater than or equal to $0$ for both $\\partial \\rho/\\partial x$ and $\\partial \\rho/\\partial y$).\n- Eigenvector-following refinement: For each candidate cell, use its center as an initial guess $(x_0,y_0)$ and perform a fixed-iteration capped-step refinement to solve $\\nabla \\rho(x,y)=\\mathbf{0}$. At an iterate $(x_k,y_k)$ with gradient $\\mathbf{g}_k$ and Hessian $H_k$, compute the symmetric eigendecomposition $H_k = V_k \\Lambda_k V_k^\\mathsf{T}$ with eigenvalues $\\lambda_{k,1},\\lambda_{k,2}$ and orthonormal eigenvectors (columns of $V_k$). Define a regularized step by projecting the gradient into the eigenbasis and scaling by the absolute curvature:\n$$\n\\mathbf{s}_k \\equiv - V_k \\,\\mathrm{diag}\\!\\left(\\frac{1}{\\max(|\\lambda_{k,1}|,\\lambda_{\\mathrm{floor}})},\\frac{1}{\\max(|\\lambda_{k,2}|,\\lambda_{\\mathrm{floor}})}\\right) V_k^\\mathsf{T} \\mathbf{g}_k.\n$$\nThen cap the step by a trust radius $s_{\\max}$: if $\\|\\mathbf{s}_k\\|_2  s_{\\max}$, scale $\\mathbf{s}_k$ to have norm $s_{\\max}$. Optionally backtrack by halving $\\mathbf{s}_k$ repeatedly until $\\|\\nabla \\rho(x_k+\\mathbf{s}_k)\\|_2 \\le \\|\\mathbf{g}_k\\|_2$ or a small backtracking limit is reached. Terminate when $\\|\\mathbf{g}_k\\|_2 \\le \\varepsilon_{\\mathrm{g}}$ or a maximum of $k_{\\max}$ iterations is reached.\n- Classification: At each converged point $(x^\\ast,y^\\ast)$, compute the Hessian eigenvalues and count the negative versus positive signs to classify the CP as nuclear-like (both eigenvalues negative), bond-like (one negative, one positive), or cage-like (both positive). Use a small threshold $\\varepsilon_{\\lambda}$ to decide the sign of a numerically tiny eigenvalue; assume no exactly zero eigenvalues occur for the given test suite. Discard converged points with $\\rho(x^\\ast,y^\\ast) \\le \\rho_{\\min}$ as non-physical.\n- De-duplication: Cluster converged points by Euclidean distance with a merge radius $r_{\\mathrm{merge}}$ to avoid counting the same CP multiple times.\n\nAnalytical base:\n- Use only the definition of $\\rho(x,y)$ above; the gradient and Hessian follow from differentiating the sum of Gaussians. Do not use any library function other than basic linear algebra and exponentials.\n- Angles are not used in this problem. No physical units are required; treat all quantities as dimensionless real numbers.\n\nTest suite:\nImplement your algorithm for the following four cases. In each case, the computational domain is $[x_{\\min},x_{\\max}] \\times [y_{\\min},y_{\\max}] = [-3.0,3.0] \\times [-3.0,3.0]$ and the grid is $N_x \\times N_y = 121 \\times 121$ unless otherwise specified. Use the same algorithmic parameters for all cases: gradient tolerance $\\varepsilon_{\\mathrm{g}} = 10^{-8}$, eigenvalue floor $\\lambda_{\\mathrm{floor}} = 10^{-6}$, trust radius $s_{\\max} = 0.5$, maximum iterations $k_{\\max} = 100$, backtracking limit $n_{\\mathrm{bt}} = 10$, merge radius $r_{\\mathrm{merge}} = 5\\times 10^{-2}$, and density floor $\\rho_{\\min} = 10^{-8}$.\n\n- Case $1$ (two equal centers along the $x$-axis): $M=2$ with $(A_1,\\alpha_1,x_1,y_1) = (1.0,1.0,-1.0,0.0)$ and $(A_2,\\alpha_2,x_2,y_2) = (1.0,1.0,1.0,0.0)$.\n- Case $2$ (single center): $M=1$ with $(A_1,\\alpha_1,x_1,y_1) = (1.0,1.5,0.0,0.0)$.\n- Case $3$ (equilateral triangle of three equal centers): $M=3$ with side length $s=1.2\\sqrt{3}$ arranged at $(A_i,\\alpha_i)=(1.0,1.2)$ for all $i$, and centers $(x_1,y_1)=(0.0,1.2)$, $(x_2,y_2)=(-\\tfrac{\\sqrt{3}}{2}\\cdot 1.2,-0.6)$, $(x_3,y_3)=(\\tfrac{\\sqrt{3}}{2}\\cdot 1.2,-0.6)$.\n- Case $4$ (asymmetric dimer): $M=2$ with $(A_1,\\alpha_1,x_1,y_1) = (1.0,1.0,-1.0,0.0)$ and $(A_2,\\alpha_2,x_2,y_2) = (0.7,0.5,0.5,0.0)$.\n\nRequired output:\n- For each case, report the list $[n_{\\mathrm{nuc}}, n_{\\mathrm{bond}}, n_{\\mathrm{cage}}]$ giving the counts of nuclear-like, bond-like, and cage-like CPs found inside the domain after de-duplication and filtering.\n- Your program should produce a single line of output containing the results as a comma-separated list of these lists in the same order as the cases, enclosed in square brackets, for example $[[1,2,3],[4,5,6],[7,8,9],[10,11,12]]$.\n\nYour implementation must be a complete, runnable program that takes no input and uses only the specified numerical libraries. The output must be exactly one line in the format described above. Ensure scientific realism by directly differentiating the given $\\rho(x,y)$; do not introduce any ad hoc formulas or external heuristics beyond the algorithmic specifications above.", "solution": "The problem requires the implementation of an algorithm to find and classify critical points of a two-dimensional scalar field, which models electron density as a sum of Gaussian functions. The approach must be built from first principles, beginning with the analytical derivation of the gradient and Hessian of the density function, followed by a systematic numerical procedure.\n\nThe electron density $\\rho(x,y)$ is defined as:\n$$\n\\rho(x,y) = \\sum_{i=1}^{M} A_i \\exp\\!\\left(-\\alpha_i\\left[(x - x_i)^2 + (y - y_i)^2\\right]\\right)\n$$\nwhere $A_i  0$, $\\alpha_i  0$, and $(x_i, y_i)$ are parameters of the $i$-th Gaussian. We introduce the shorthand $r_i^2(x,y) = (x - x_i)^2 + (y - y_i)^2$. The density is a sum of contributions $\\rho_i = A_i \\exp(-\\alpha_i r_i^2)$.\n\nFirst, we derive the analytical expressions for the gradient vector $\\mathbf{g} = \\nabla\\rho$ and the Hessian matrix $H$.\n\nThe components of the gradient are the first partial derivatives of $\\rho$.\n$$\ng_x(x,y) = \\frac{\\partial \\rho}{\\partial x} = \\sum_{i=1}^{M} \\frac{\\partial}{\\partial x} \\left[ A_i \\exp(-\\alpha_i r_i^2) \\right] = \\sum_{i=1}^{M} A_i \\exp(-\\alpha_i r_i^2) \\cdot (-\\alpha_i) \\frac{\\partial r_i^2}{\\partial x}\n$$\nSince $\\frac{\\partial r_i^2}{\\partial x} = 2(x - x_i)$, the gradient components are:\n$$\ng_x(x,y) = -2 \\sum_{i=1}^{M} \\alpha_i A_i (x - x_i) \\exp(-\\alpha_i r_i^2)\n$$\nBy symmetry, the $y$-component is:\n$$\ng_y(x,y) = -2 \\sum_{i=1}^{M} \\alpha_i A_i (y - y_i) \\exp(-\\alpha_i r_i^2)\n$$\n\nThe Hessian matrix $H$ consists of the second partial derivatives.\nFor the diagonal element $H_{xx}$:\n$$\nH_{xx}(x,y) = \\frac{\\partial^2 \\rho}{\\partial x^2} = \\frac{\\partial g_x}{\\partial x} = -2 \\sum_{i=1}^{M} \\alpha_i A_i \\frac{\\partial}{\\partial x} \\left[ (x - x_i) \\exp(-\\alpha_i r_i^2) \\right]\n$$\nApplying the product rule to the term in the sum:\n$$\n\\frac{\\partial}{\\partial x}\\left[\\dots\\right] = 1 \\cdot \\exp(-\\alpha_i r_i^2) + (x - x_i) \\cdot \\left[ \\exp(-\\alpha_i r_i^2) \\cdot (-\\alpha_i) \\cdot 2(x-x_i) \\right] = \\left( 1 - 2\\alpha_i(x-x_i)^2 \\right) \\exp(-\\alpha_i r_i^2)\n$$\nSubstituting this back, we get:\n$$\nH_{xx}(x,y) = -2 \\sum_{i=1}^{M} \\alpha_i A_i \\left( 1 - 2\\alpha_i(x-x_i)^2 \\right) \\exp(-\\alpha_i r_i^2) = 2 \\sum_{i=1}^{M} \\alpha_i A_i \\left( 2\\alpha_i(x-x_i)^2 - 1 \\right) \\exp(-\\alpha_i r_i^2)\n$$\nBy symmetry, the other diagonal element $H_{yy}$ is:\n$$\nH_{yy}(x,y) = 2 \\sum_{i=1}^{M} \\alpha_i A_i \\left( 2\\alpha_i(y-y_i)^2 - 1 \\right) \\exp(-\\alpha_i r_i^2)\n$$\nFor the off-diagonal element $H_{xy}$:\n$$\nH_{xy}(x,y) = \\frac{\\partial^2 \\rho}{\\partial x \\partial y} = \\frac{\\partial g_y}{\\partial x} = -2 \\sum_{i=1}^{M} \\alpha_i A_i (y - y_i) \\frac{\\partial}{\\partial x} \\left[ \\exp(-\\alpha_i r_i^2) \\right]\n$$\nThe derivative of the exponential is:\n$$\n\\frac{\\partial}{\\partial x} \\left[ \\exp(-\\alpha_i r_i^2) \\right] = \\exp(-\\alpha_i r_i^2) \\cdot (-\\alpha_i) \\cdot 2(x-x_i)\n$$\nThis gives:\n$$\nH_{xy}(x,y) = -2 \\sum_{i=1}^{M} \\alpha_i A_i (y - y_i) \\left[ -2\\alpha_i(x-x_i) \\exp(-\\alpha_i r_i^2) \\right] = 4 \\sum_{i=1}^{M} \\alpha_i^2 A_i (x-x_i)(y-y_i) \\exp(-\\alpha_i r_i^2)\n$$\nBy Clairaut's theorem for smooth functions, $H_{yx} = H_{xy}$. These analytical formulas are implemented to compute $\\rho$, $\\mathbf{g}$, and $H$ at any point $(x,y)$.\n\nThe numerical algorithm unfolds in four stages:\n\n1.  **Grid Scan:** A uniform grid is constructed over the specified domain. At each node, the gradient vector $\\mathbf{g}(x,y)$ is evaluated. Each elementary cell of the grid (defined by four adjacent nodes) is examined. If the range of values for $g_x$ across the four corners includes zero, and similarly for $g_y$, the cell is considered a candidate for containing a critical point. The geometric center of each such candidate cell serves as an initial guess for the refinement procedure.\n\n2.  **Iterative Refinement:** Each initial guess $\\mathbf{p}_0 = (x_0, y_0)$ is refined using an eigenvector-following method to solve $\\mathbf{g}(\\mathbf{p}) = \\mathbf{0}$. At iteration $k$, given the point $\\mathbf{p}_k$, the gradient $\\mathbf{g}_k$ and Hessian $H_k$ are computed. If $\\|\\mathbf{g}_k\\|_2 \\le \\varepsilon_{\\mathrm{g}}$, the point is considered converged. Otherwise, a search direction is determined. The Hessian is diagonalized, $H_k = V_k \\Lambda_k V_k^\\mathsf{T}$, yielding eigenvalues $\\lambda_{k,j}$ and eigenvectors (columns of $V_k$). The step $\\mathbf{s}_k$ is computed using a regularized inverse Hessian:\n    $$\n    \\mathbf{s}_k = -V_k \\mathrm{diag}\\left(\\frac{1}{\\max(|\\lambda_{k,1}|, \\lambda_{\\mathrm{floor}})}, \\frac{1}{\\max(|\\lambda_{k,2}|, \\lambda_{\\mathrm{floor}})}\\right) V_k^\\mathsf{T} \\mathbf{g}_k\n    $$\n    This formulation ensures stable steps even near saddle points (where an eigenvalue is negative) or in flat regions (where an eigenvalue is near zero). The step length is then capped at a maximum value $s_{\\max}$. A backtracking line search is performed by halving the step until the gradient norm at the new point is not greater than the current gradient norm. The point is updated, $\\mathbf{p}_{k+1} = \\mathbf{p}_k + \\mathbf{s}_k$, and the process is repeated for a maximum of $k_{\\max}$ iterations.\n\n3.  **De-duplication and Filtering:** Since multiple initial guesses can converge to the same critical point, a de-duplication step is necessary. Converged points are collected, and a point is added to a unique list only if its Euclidean distance to all previously accepted points exceeds a merge radius $r_{\\mathrm{merge}}$. Finally, points are filtered. Any point lying outside the computational domain or having a density value $\\rho$ below a threshold $\\rho_{\\min}$ is discarded.\n\n4.  **Classification:** Each unique, valid critical point is classified based on its Hessian eigenvalues $(\\lambda_1, \\lambda_2)$:\n    -   **Nuclear-like (maximum):** $\\lambda_1  0, \\lambda_2  0$.\n    -   **Bond-like (saddle point):** $\\lambda_1 \\cdot \\lambda_2  0$.\n    -   **Cage-like (minimum):** $\\lambda_1  0, \\lambda_2  0$.\n    The counts of each type of critical point, $[n_{\\mathrm{nuc}}, n_{\\mathrm{bond}}, n_{\\mathrm{cage}}]$, are then determined for each test case.\n\nThis systematic procedure, combining analytical rigor with robust numerical techniques, allows for the reliable characterization of the topological features of the given scalar field.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Algorithmic parameters, constant across all cases\n    params = {\n        \"domain\": np.array([[-3.0, 3.0], [-3.0, 3.0]]),\n        \"grid_size\": np.array([121, 121]),\n        \"grad_tol\": 1e-8,\n        \"lambda_floor\": 1e-6,\n        \"trust_radius\": 0.5,\n        \"max_iter\": 100,\n        \"backtrack_limit\": 10,\n        \"merge_radius\": 5e-2,\n        \"rho_min\": 1e-8,\n    }\n\n    # Test cases definition\n    test_cases = [\n        # Case 1: Two equal centers\n        [\n            (1.0, 1.0, -1.0, 0.0),\n            (1.0, 1.0, 1.0, 0.0),\n        ],\n        # Case 2: Single center\n        [\n            (1.0, 1.5, 0.0, 0.0),\n        ],\n        # Case 3: Equilateral triangle\n        [\n            (1.0, 1.2, 0.0, 1.2),\n            (1.0, 1.2, -0.5 * 1.2 * np.sqrt(3), -0.6),\n            (1.0, 1.2, 0.5 * 1.2 * np.sqrt(3), -0.6),\n        ],\n        # Case 4: Asymmetric dimer\n        [\n            (1.0, 1.0, -1.0, 0.0),\n            (0.7, 0.5, 0.5, 0.0),\n        ],\n    ]\n\n    all_results = []\n    for gaussians in test_cases:\n        results = process_case(gaussians, params)\n        all_results.append(results)\n    \n    # Format and print the final output string\n    output_str = \"[\" + \",\".join([str(r) for r in all_results]) + \"]\"\n    print(output_str)\n\ndef process_case(gaussians, params):\n    \"\"\"\n    Processes a single test case: grid scan, refinement, and classification.\n    \"\"\"\n    # Step 1: Grid Scan to find initial guesses\n    initial_guesses = grid_scan(gaussians, params)\n\n    # Step 2: Refine initial guesses to find CPs\n    raw_cps = []\n    for guess in initial_guesses:\n        converged_point = refine_cp(guess, gaussians, params)\n        if converged_point is not None:\n            raw_cps.append(converged_point)\n    \n    # Step 3 and 4: Deduplicate, filter, classify, and count CPs\n    cp_counts = post_process_cps(raw_cps, gaussians, params)\n    \n    return cp_counts\n\ndef compute_properties(point, gaussians):\n    \"\"\"\n    Computes rho, gradient, and Hessian at a given point.\n    \"\"\"\n    x, y = point\n    rho = 0.0\n    grad = np.zeros(2)\n    hess = np.zeros((2, 2))\n\n    for A, alpha, x_i, y_i in gaussians:\n        dx = x - x_i\n        dy = y - y_i\n        r2 = dx**2 + dy**2\n        exp_term = A * np.exp(-alpha * r2)\n\n        rho += exp_term\n\n        common_grad_factor = -2.0 * alpha * exp_term\n        grad[0] += common_grad_factor * dx\n        grad[1] += common_grad_factor * dy\n\n        # Hessian components\n        h_xx_term = 2.0 * alpha * (2.0 * alpha * dx**2 - 1.0) * exp_term\n        h_yy_term = 2.0 * alpha * (2.0 * alpha * dy**2 - 1.0) * exp_term\n        h_xy_term = 4.0 * alpha**2 * dx * dy * exp_term\n        \n        hess[0, 0] += h_xx_term\n        hess[1, 1] += h_yy_term\n        hess[0, 1] += h_xy_term\n        hess[1, 0] += h_xy_term\n        \n    return rho, grad, hess\n\ndef grid_scan(gaussians, params):\n    \"\"\"\n    Performs a grid scan to find candidate cells for CPs.\n    \"\"\"\n    x_min, x_max = params[\"domain\"][0]\n    y_min, y_max = params[\"domain\"][1]\n    nx, ny = params[\"grid_size\"]\n    \n    x_coords = np.linspace(x_min, x_max, nx)\n    y_coords = np.linspace(y_min, y_max, ny)\n\n    grad_x_grid = np.zeros((nx, ny))\n    grad_y_grid = np.zeros((nx, ny))\n\n    for i in range(nx):\n        for j in range(ny):\n            _, grad, _ = compute_properties(np.array([x_coords[i], y_coords[j]]), gaussians)\n            grad_x_grid[i, j] = grad[0]\n            grad_y_grid[i, j] = grad[1]\n\n    initial_guesses = []\n    for i in range(nx - 1):\n        for j in range(ny - 1):\n            gx_vals = [grad_x_grid[i, j], grad_x_grid[i+1, j], grad_x_grid[i, j+1], grad_x_grid[i+1, j+1]]\n            gy_vals = [grad_y_grid[i, j], grad_y_grid[i+1, j], grad_y_grid[i, j+1], grad_y_grid[i+1, j+1]]\n\n            if (min(gx_vals) = 0 = max(gx_vals)) and (min(gy_vals) = 0 = max(gy_vals)):\n                center_x = (x_coords[i] + x_coords[i+1]) / 2.0\n                center_y = (y_coords[j] + y_coords[j+1]) / 2.0\n                initial_guesses.append(np.array([center_x, center_y]))\n\n    return initial_guesses\n\ndef refine_cp(initial_guess, gaussians, params):\n    \"\"\"\n    Refines an initial guess to find a critical point using eigenvector-following.\n    \"\"\"\n    p_k = np.copy(initial_guess)\n    \n    for _ in range(params[\"max_iter\"]):\n        _, g_k, h_k = compute_properties(p_k, gaussians)\n\n        if np.linalg.norm(g_k)  params[\"grad_tol\"]:\n            return p_k\n\n        # Eigendecomposition of the Hessian\n        eigenvalues, eigenvectors = eigh(h_k)\n        \n        # Build regularized inverse Hessian in the eigenbasis\n        d_inv = np.diag([1.0 / max(abs(lam), params[\"lambda_floor\"]) for lam in eigenvalues])\n        \n        # Compute step\n        s_k = -eigenvectors @ d_inv @ eigenvectors.T @ g_k\n        \n        # Cap step size\n        s_norm = np.linalg.norm(s_k)\n        if s_norm > params[\"trust_radius\"]:\n            s_k = s_k * (params[\"trust_radius\"] / s_norm)\n\n        # Backtracking line search\n        g_k_norm = np.linalg.norm(g_k)\n        s_k_current = np.copy(s_k)\n        for _ in range(params[\"backtrack_limit\"]):\n            p_next_try = p_k + s_k_current\n            _, g_next, _ = compute_properties(p_next_try, gaussians)\n            if np.linalg.norm(g_next) = g_k_norm:\n                break\n            s_k_current /= 2.0\n        \n        p_k += s_k_current\n\n    # Final check for convergence after max_iter\n    _, g_final, _ = compute_properties(p_k, gaussians)\n    if np.linalg.norm(g_final)  params[\"grad_tol\"]:\n        return p_k\n    \n    return None\n\ndef post_process_cps(raw_cps, gaussians, params):\n    \"\"\"\n    Deduplicates, filters, classifies, and counts critical points.\n    \"\"\"\n    unique_cps = []\n    \n    # Deduplication\n    for p in raw_cps:\n        is_unique = True\n        for up in unique_cps:\n            if np.linalg.norm(p - up)  params[\"merge_radius\"]:\n                is_unique = False\n                break\n        if is_unique:\n            unique_cps.append(p)\n            \n    # Filtering, classification, and counting\n    counts = {\"nuc\": 0, \"bond\": 0, \"cage\": 0}\n    domain_x, domain_y = params[\"domain\"]\n    \n    for p in unique_cps:\n        # Filter by domain\n        if not (domain_x[0] = p[0] = domain_x[1] and domain_y[0] = p[1] = domain_y[1]):\n            continue\n            \n        rho, _, h = compute_properties(p, gaussians)\n\n        # Filter by density\n        if rho  params[\"rho_min\"]:\n            continue\n            \n        # Classify\n        eigenvalues, _ = eigh(h)\n        neg_count = np.sum(eigenvalues  0)\n        \n        if neg_count == 2:\n            counts[\"nuc\"] += 1\n        elif neg_count == 1:\n            counts[\"bond\"] += 1\n        elif neg_count == 0:\n            counts[\"cage\"] += 1\n\n    return [counts[\"nuc\"], counts[\"bond\"], counts[\"cage\"]]\n\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2876176"}, {"introduction": "A key application of QTAIM is the development of quantitative structure-property relationships (QSPRs), where topological descriptors are used to predict chemical behavior. This final exercise places you in the role of a computational chemist testing such a relationship: the correlation between the electron density at the BCP, $\\rho_b$, and the bond energy [@problem_id:2876039]. You will use regression analysis to build a predictive model and, more importantly, critically assess its transferability across different chemical families, a crucial skill in evaluating the scope and limitations of any theoretical model.", "problem": "You are given three chemical families for which the electron density at the bond critical point (BCP), denoted $ \\rho_b $ in units of electrons per cubic bohr ($\\mathrm{e/bohr^3}$), and reference bond energies $ E $ in kilojoules per mole ($\\mathrm{kJ/mol}$) have been compiled. In the framework of Quantum Theory of Atoms in Molecules (QTAIM), a bond critical point is a point on the bond path where the gradient of the electron density $ \\nabla \\rho(\\mathbf{r}) $ vanishes and the Hessian of $ \\rho $ has signature consistent with a bond saddle point. We focus on the empirical practice of correlating $ \\rho_b $ to $ E $ and want to evaluate the transferability of such correlations across chemical families. Your task is to formalize this evaluation as a mathematically well-posed regression and error analysis problem and implement it as a program.\n\nStart from the following foundations:\n- The electron density $ \\rho(\\mathbf{r}) $ is a scalar field over space, and a bond critical point is a stationary point of $ \\rho(\\mathbf{r}) $ along the bond path.\n- We are given data pairs $ (\\rho_b, E) $ for several bonds within each family.\n- Empirical structureâ€“property models often take the power-law form $ E = k \\, \\rho_b^{\\alpha} $, where $ k  0 $ and $ \\alpha $ are parameters to be learned from data.\n- The method of ordinary least squares for linear regression is a well-tested procedure to estimate parameters when a linear relationship is specified.\n\nTasks:\n1) Adopt the model $ E = k \\, \\rho_b^{\\alpha} $. Estimate the parameters $ k $ and $ \\alpha $ from training data by applying ordinary least squares to the logarithmically transformed variables, i.e., regress $ \\ln E $ against $ \\ln \\rho_b $, so that $ \\ln E = \\ln k + \\alpha \\ln \\rho_b $. Use the unique least-squares solution that minimizes the sum of squared residuals in the transformed space.\n2) Define the root-mean-square error (RMSE) on a test set as $ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (E_i - \\hat{E}_i)^2} $, where $ \\hat{E}_i $ is the predicted energy for $ \\rho_{b,i} $ using the trained model, and $ n $ is the number of test points.\n3) To quantify within-family predictive error (a baseline for noise and model adequacy), compute leave-one-out cross-validation (LOOCV) RMSE within a family: for each point in that family, fit the model on the remaining points and compute the squared prediction error on the left-out point; aggregate across all left-out points to yield the LOOCV RMSE.\n4) To assess transferability across families, train the model on one family and evaluate RMSE on a different family without refitting.\n\nData (each pair is $ (\\rho_b, E) $ with $ \\rho_b $ in $ \\mathrm{e/bohr^3} $ and $ E $ in $ \\mathrm{kJ/mol} $):\n- Family Covalent (C): $[(0.12, 125.2), (0.15, 176.0), (0.18, 227.0), (0.22, 312.0), (0.26, 395.0), (0.30, 497.0), (0.34, 590.0), (0.38, 706.0)]$.\n- Family Ionic/Dative (I): $[(0.06, 78.0), (0.08, 111.0), (0.10, 152.0), (0.12, 189.0), (0.14, 235.0), (0.16, 279.0)]$.\n- Family Hydrogen-bonded (H): $[(0.010, 3.2), (0.015, 5.1), (0.020, 7.4), (0.030, 12.0), (0.040, 16.6), (0.050, 22.3)]$.\n\nTest suite:\n- Cross-family RMSE cases (train family $\\rightarrow$ test family):\n  1) $ \\mathrm{C} \\rightarrow \\mathrm{I} $\n  2) $ \\mathrm{I} \\rightarrow \\mathrm{C} $\n  3) $ \\mathrm{C} \\rightarrow \\mathrm{H} $\n  4) $ \\mathrm{H} \\rightarrow \\mathrm{C} $\n  5) $ \\mathrm{H} \\rightarrow \\mathrm{I} $\n  6) $ \\mathrm{I} \\rightarrow \\mathrm{H} $\n- Within-family LOOCV RMSE cases:\n  7) $ \\mathrm{C} \\rightarrow \\mathrm{C} $ (LOOCV)\n  8) $ \\mathrm{H} \\rightarrow \\mathrm{H} $ (LOOCV)\n  9) $ \\mathrm{I} \\rightarrow \\mathrm{I} $ (LOOCV)\n\nImplementation and output requirements:\n- Use natural logarithms for the regression in the transformed space.\n- All RMSE values must be computed in $ \\mathrm{kJ/mol} $ and reported in $ \\mathrm{kJ/mol} $.\n- Your program must produce a single line of output containing the nine RMSE values corresponding to the test suite above, in the specified order, rounded to three decimal places, as a comma-separated list enclosed in square brackets. For example, an output should look like $[\\dots]$ with exactly nine decimal-formatted numbers.\n- Angles are not involved. No percentages are involved.\n\nYour program must be a complete, runnable implementation that hardcodes the data above and performs the calculations with no external input. The final printed output must be a single line in the exact format described.", "solution": "The problem is scientifically well-posed and provides a complete set of instructions for a computational task in the domain of quantum chemistry and data analysis. It is based on the established practice of correlating topological properties of the electron density, such as the value at a bond critical point ($\\rho_b$), with macroscopic chemical properties like bond energy ($E$). The problem is validated as sound, and a solution is provided below.\n\nThe core of the problem is to quantify the predictive power and transferability of an empirical power-law model, $E = k \\, \\rho_b^{\\alpha}$, which connects the bond energy $E$ to the electron density at the bond critical point, $\\rho_b$. The parameters $k$ and $\\alpha$ are specific to the chemical environment and must be determined from experimental or computational data.\n\nTo estimate these parameters, we linearize the model by taking the natural logarithm of both sides:\n$$ \\ln(E) = \\ln(k \\, \\rho_b^{\\alpha}) = \\ln(k) + \\alpha \\ln(\\rho_b) $$\nThis equation is in the form of a straight line, $y = c + m x$, where the variables are $y = \\ln(E)$ and $x = \\ln(\\rho_b)$. The slope of the line is $m = \\alpha$, and the y-intercept is $c = \\ln(k)$. This transformation allows us to use the method of ordinary least squares (OLS) to find the optimal values for $\\alpha$ and $\\ln(k)$ that minimize the sum of squared differences between the observed and predicted values of $\\ln(E)$.\n\nFor a given training dataset of $n$ points, $\\{(\\rho_{b,i}, E_i)\\}_{i=1}^n$, we first transform it into a set of logarithmic points $\\{x_i, y_i\\}_{i=1}^n$, where $x_i = \\ln(\\rho_{b,i})$ and $y_i = \\ln(E_i)$. The OLS solution for the slope $\\alpha$ and intercept $\\ln(k)$ is given by:\n$$ \\alpha = \\frac{n \\sum_{i=1}^{n} (x_i y_i) - (\\sum_{i=1}^{n} x_i) (\\sum_{i=1}^{n} y_i)}{n \\sum_{i=1}^{n} (x_i^2) - (\\sum_{i=1}^{n} x_i)^2} $$\n$$ \\ln(k) = \\bar{y} - \\alpha \\bar{x} $$\nwhere $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$ are the sample means. The parameter $k$ is then found by exponentiation: $k = e^{\\ln(k)}$.\n\nOnce the model parameters $(\\alpha, k)$ are determined from a training set, we can predict the energy $\\hat{E}_j$ for any new value of electron density $\\rho_{b,j}$ using the original power-law relationship:\n$$ \\hat{E}_j = k \\, (\\rho_{b,j})^{\\alpha} $$\nThe performance of the model is assessed using the Root-Mean-Square Error (RMSE) on a test set of $N$ points, which is defined as:\n$$ \\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} (E_j - \\hat{E}_j)^2} $$\nThis metric provides a measure of the average magnitude of the prediction error in the original units of energy ($\\mathrm{kJ/mol}$).\n\nThe problem requires two types of model evaluation:\n$1$. **Cross-Family Transferability**: This assesses how well a model trained on one chemical family (e.g., Covalent) predicts the energies for a different family (e.g., Ionic). This is a direct test of the model's generality. The procedure involves fitting the model parameters $(\\alpha, k)$ using all data from the training family and then calculating the RMSE on the entire test family.\n\n$2$. **Within-Family Predictive Error**: This is evaluated using Leave-One-Out Cross-Validation (LOOCV). This technique provides a robust estimate of how well the model predicts new data from *within the same family*, serving as a baseline for the model's inherent accuracy and stability. For a family with $n$ data points, the process is repeated $n$ times. In each iteration $i$, the $i$-th data point is held out as a test point, and the model is trained on the remaining $n-1$ points. A prediction $\\hat{E}_i$ is made for the held-out point. The final LOOCV RMSE is calculated over all $n$ such predictions, $\\{ (E_i, \\hat{E}_i) \\}_{i=1}^n$.\n\nThe implementation will systematically perform these calculations for the nine specified test cases. The provided data for the Covalent (C), Ionic/Dative (I), and Hydrogen-bonded (H) families will be used. The regression is performed using `scipy.stats.linregress` on the log-transformed data, which provides a numerically stable implementation of OLS. The resulting parameters are then used to compute predictions and RMSE values as defined.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Solves the QTAIM regression and error analysis problem.\n    \"\"\"\n    # Define the data for the three chemical families.\n    # Each entry is a numpy array of (rho_b, E) pairs.\n    data = {\n        'C': np.array([\n            (0.12, 125.2), (0.15, 176.0), (0.18, 227.0), (0.22, 312.0),\n            (0.26, 395.0), (0.30, 497.0), (0.34, 590.0), (0.38, 706.0)\n        ]),\n        'I': np.array([\n            (0.06, 78.0), (0.08, 111.0), (0.10, 152.0), (0.12, 189.0),\n            (0.14, 235.0), (0.16, 279.0)\n        ]),\n        'H': np.array([\n            (0.010, 3.2), (0.015, 5.1), (0.020, 7.4), (0.030, 12.0),\n            (0.040, 16.6), (0.050, 22.3)\n        ])\n    }\n\n    def fit_model(training_data):\n        \"\"\"\n        Fits the power-law model E = k * rho_b^alpha by linear regression\n        on log-transformed data.\n        \n        Args:\n            training_data (np.ndarray): Array of (rho_b, E) pairs.\n            \n        Returns:\n            tuple: A tuple (alpha, k) representing the model parameters.\n        \"\"\"\n        rho_b = training_data[:, 0]\n        E = training_data[:, 1]\n        \n        log_rho_b = np.log(rho_b)\n        log_E = np.log(E)\n        \n        # OLS on log-transformed data: ln(E) = alpha * ln(rho_b) + ln(k)\n        slope, intercept, _, _, _ = linregress(log_rho_b, log_E)\n        \n        alpha = slope\n        k = np.exp(intercept)\n        \n        return alpha, k\n\n    def predict_E(model, rho_b_values):\n        \"\"\"\n        Predicts bond energies E using a fitted power-law model.\n        \n        Args:\n            model (tuple): A tuple (alpha, k) of model parameters.\n            rho_b_values (np.ndarray): Array of rho_b values.\n            \n        Returns:\n            np.ndarray: Array of predicted E values.\n        \"\"\"\n        alpha, k = model\n        return k * (rho_b_values ** alpha)\n\n    def calculate_rmse(E_true, E_pred):\n        \"\"\"\n        Calculates the Root-Mean-Square Error.\n        \n        Args:\n            E_true (np.ndarray): Array of true energy values.\n            E_pred (np.ndarray): Array of predicted energy values.\n            \n        Returns:\n            float: The RMSE value.\n        \"\"\"\n        return np.sqrt(np.mean((E_true - E_pred)**2))\n\n    def loocv_rmse(family_data):\n        \"\"\"\n        Calculates the Leave-One-Out Cross-Validation RMSE for a family.\n        \n        Args:\n            family_data (np.ndarray): Array of (rho_b, E) pairs for one family.\n            \n        Returns:\n            float: The LOOCV RMSE value.\n        \"\"\"\n        n_points = len(family_data)\n        squared_errors = []\n        \n        for i in range(n_points):\n            # Create leave-one-out training set by deleting the i-th row\n            lo_train_data = np.delete(family_data, i, axis=0)\n            \n            # The left-out point is the test set\n            lo_test_point = family_data[i]\n            \n            # Fit model on the (n-1) training points\n            model = fit_model(lo_train_data)\n            \n            # Predict on the single left-out point\n            rho_b_test = lo_test_point[0]\n            E_true = lo_test_point[1]\n            E_pred = predict_E(model, np.array([rho_b_test]))[0]\n            \n            squared_errors.append((E_true - E_pred)**2)\n            \n        return np.sqrt(np.mean(squared_errors))\n\n    # Define the 9 test cases from the problem statement.\n    test_suite = [\n        # Cross-family RMSE cases (train -> test)\n        ('C', 'I'),  # 1\n        ('I', 'C'),  # 2\n        ('C', 'H'),  # 3\n        ('H', 'C'),  # 4\n        ('H', 'I'),  # 5\n        ('I', 'H'),  # 6\n        # Within-family LOOCV RMSE cases\n        ('C', 'C'),  # 7\n        ('H', 'H'),  # 8\n        ('I', 'I')   # 9\n    ]\n\n    results = []\n    for train_fam, test_fam in test_suite:\n        if train_fam == test_fam:\n            # This is an LOOCV case\n            family_data = data[train_fam]\n            rmse = loocv_rmse(family_data)\n        else:\n            # This is a cross-family transferability case\n            train_data = data[train_fam]\n            test_data = data[test_fam]\n            \n            # Fit model on the entire training family\n            model = fit_model(train_data)\n            \n            # Test model on the entire testing family\n            rho_b_test = test_data[:, 0]\n            E_true_test = test_data[:, 1]\n            E_pred_test = predict_E(model, rho_b_test)\n            \n            rmse = calculate_rmse(E_true_test, E_pred_test)\n            \n        results.append(rmse)\n\n    # Format the results to three decimal places and print in the required format.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2876039"}]}