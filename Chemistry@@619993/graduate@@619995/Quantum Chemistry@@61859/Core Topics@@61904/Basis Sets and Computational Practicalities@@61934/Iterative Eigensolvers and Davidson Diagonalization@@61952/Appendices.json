{"hands_on_practices": [{"introduction": "Before tackling complex implementations, it's essential to understand the engine of the Davidson method: a single iterative step. This practice demystifies the algorithm by guiding you through one full cycle: calculating the residual for a trial vector, using the diagonal preconditioner to generate a correction, expanding the search subspace, and solving the new projected eigenproblem to find an improved eigenvalue estimate [@problem_id:2900262]. Performing this hands-on calculation solidifies the core theoretical concepts of the Rayleigh-Ritz procedure and the role of preconditioning in accelerating convergence.", "problem": "A Hermitian matrix $A$ arises as a small sub-block of an electronic Hamiltonian in an orthonormal atomic-orbital basis. Consider the following real-symmetric $4 \\times 4$ matrix\n$$\nA = \\begin{pmatrix}\n1 & \\frac{1}{5} & 0 & 0 \\\\\n\\frac{1}{5} & 2 & \\frac{3}{10} & 0 \\\\\n0 & \\frac{3}{10} & 3 & \\frac{2}{5} \\\\\n0 & 0 & \\frac{2}{5} & 4\n\\end{pmatrix}.\n$$\nSuppose the current subspace of a Davidson diagonalization contains a single normalized trial vector $u=\\begin{pmatrix}1&0&0&0\\end{pmatrix}^{\\mathsf{T}}$, intended to approximate the ground-state eigenvector. Let the associated Ritz value $\\theta$ be defined by the Rayleigh quotient $\\theta=\\dfrac{u^{\\mathsf{T}}Au}{u^{\\mathsf{T}}u}$. Using first principles of the Rayleigh–Ritz procedure and the definition of the residual $r$, perform one Davidson expansion step that uses the diagonal preconditioner $D=\\mathrm{diag}(A)$ to form a correction vector $t$, orthogonalizes $t$ to the current subspace, and then carries out the Rayleigh–Ritz extraction in the augmented subspace.\n\nYour tasks are:\n- Compute the residual $r$ from the trial pair $(\\theta,u)$.\n- Construct the standard Davidson correction $t$ using the diagonal preconditioner $D=\\mathrm{diag}(A)$ with shift $\\theta$.\n- Orthogonalize $t$ against the current subspace $\\mathrm{span}\\{u\\}$, normalize it to obtain a unit vector $w$, and form the two-column basis $W=[u\\;w]$.\n- Build the projected $2\\times 2$ Rayleigh–Ritz matrix $B=W^{\\mathsf{T}}AW$ and extract its smallest eigenvalue as the updated ground-state Ritz value.\n\nWhat is the updated smallest Ritz value after this single Davidson expansion step? Express your final answer as an exact analytic expression. Do not round.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. It describes a standard application of one step of the Davidson diagonalization algorithm, a well-established iterative method for finding eigenvalues of large matrices, commonly used in quantum chemistry. All necessary data and definitions are provided, and no inconsistencies or ambiguities are present. We shall proceed with the solution.\n\nThe problem requires performing one step of the Davidson iterative procedure to refine the estimate of the lowest eigenvalue of the given matrix $A$. The steps are followed as prescribed.\n\nThe given real-symmetric matrix is\n$$\nA = \\begin{pmatrix}\n1 & \\frac{1}{5} & 0 & 0 \\\\\n\\frac{1}{5} & 2 & \\frac{3}{10} & 0 \\\\\n0 & \\frac{3}{10} & 3 & \\frac{2}{5} \\\\\n0 & 0 & \\frac{2}{5} & 4\n\\end{pmatrix}\n$$\nThe initial subspace is spanned by the single normalized trial vector\n$$\nu = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nSince $u$ is normalized, $u^{\\mathsf{T}}u = 1$.\n\nFirst, we compute the initial Ritz value $\\theta$, which is the Rayleigh quotient for the current trial vector $u$.\n$$\n\\theta = \\frac{u^{\\mathsf{T}}Au}{u^{\\mathsf{T}}u} = u^{\\mathsf{T}}Au\n$$\nWe compute the product $Au$:\n$$\nAu = \\begin{pmatrix}\n1 & \\frac{1}{5} & 0 & 0 \\\\\n\\frac{1}{5} & 2 & \\frac{3}{10} & 0 \\\\\n0 & \\frac{3}{10} & 3 & \\frac{2}{5} \\\\\n0 & 0 & \\frac{2}{5} & 4\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 1 \\\\ \\frac{1}{5} \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThen, the Ritz value $\\theta$ is:\n$$\n\\theta = u^{\\mathsf{T}}(Au) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{1}{5} \\\\ 0 \\\\ 0 \\end{pmatrix} = 1\n$$\nThe initial approximation to the eigenvalue is $\\theta = 1$.\n\nNext, we compute the residual vector $r$ associated with the current Ritz pair $(\\theta, u)$.\n$$\nr = Au - \\theta u = \\begin{pmatrix} 1 \\\\ \\frac{1}{5} \\\\ 0 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nThe next step is to compute the correction vector $t$ by approximately solving the correction equation $(A - \\theta I)t = -r$. In the Davidson method, the matrix $(A - \\theta I)$ is approximated by its diagonal, yielding the preconditioner $(D - \\theta I)$, where $D = \\mathrm{diag}(A)$.\n$$\nD = \\mathrm{diag}(1, 2, 3, 4)\n$$\nThe correction equation becomes $(D - \\theta I)t = -r$.\n$$\n\\left( \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\right) t = -r\n$$\n$$\n\\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} t = -\\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe components of $t = (t_1, t_2, t_3, t_4)^{\\mathsf{T}}$ are found by $t_i = -r_i / (D_{ii} - \\theta)$. For the index $i=1$, the denominator $D_{11}-\\theta = 1-1=0$. Since the corresponding residual component $r_1$ is also $0$, this component is formally undefined. The standard Davidson procedure requires the correction to be orthogonal to the current solution vector $u$. Since $u$ is the first canonical basis vector, this condition implies $t_1 = 0$. For other components:\n$$\nt_2 = \\frac{-r_2}{D_{22}-\\theta} = \\frac{-1/5}{2-1} = -\\frac{1}{5}\n$$\n$$\nt_3 = \\frac{-r_3}{D_{33}-\\theta} = \\frac{-0}{3-1} = 0\n$$\n$$\nt_4 = \\frac{-r_4}{D_{44}-\\theta} = \\frac{-0}{4-1} = 0\n$$\nSo the correction vector is\n$$\nt = \\begin{pmatrix} 0 \\\\ -1/5 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we must orthogonalize the correction vector $t$ against the current basis, which consists only of $u$. We use the Gram-Schmidt procedure. Let the orthogonalized vector be $t'$.\n$$\nt' = t - (u^{\\mathsf{T}}t)u\n$$\nThe inner product is\n$$\nu^{\\mathsf{T}}t = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1/5 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0\n$$\nSince $t$ is already orthogonal to $u$, we have $t' = t$. We then normalize this vector to obtain the new basis vector $w$.\n$$\n\\|t'\\| = \\sqrt{0^2 + \\left(-\\frac{1}{5}\\right)^2 + 0^2 + 0^2} = \\frac{1}{5}\n$$\n$$\nw = \\frac{t'}{\\|t'\\|} = \\frac{1}{1/5} \\begin{pmatrix} 0 \\\\ -1/5 \\\\ 0 \\\\ 0 \\end{pmatrix} = 5 \\begin{pmatrix} 0 \\\\ -1/5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe new, expanded orthonormal basis is $\\{u, w\\}$. We form the matrix $W$ with these vectors as columns.\n$$\nW = \\begin{bmatrix} u & w \\end{bmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nWe now construct the projected $2 \\times 2$ Rayleigh-Ritz matrix $B = W^{\\mathsf{T}}AW$.\nFirst, we compute $AW$:\n$$\nAW = A \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{5} \\\\ \\frac{1}{5} & -2 \\\\ 0 & -\\frac{3}{10} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThen we compute $B$:\n$$\nB = W^{\\mathsf{T}} (AW) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -\\frac{1}{5} \\\\ \\frac{1}{5} & -2 \\\\ 0 & -\\frac{3}{10} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{5} \\\\ -\\frac{1}{5} & 2 \\end{pmatrix}\n$$\nThe final step is to find the eigenvalues of $B$ by solving the characteristic equation $\\det(B - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\lambda & -\\frac{1}{5} \\\\ -\\frac{1}{5} & 2-\\lambda \\end{pmatrix} = (1-\\lambda)(2-\\lambda) - \\left(-\\frac{1}{5}\\right)^2 = 0\n$$\n$$\n\\lambda^2 - 3\\lambda + 2 - \\frac{1}{25} = 0\n$$\n$$\n\\lambda^2 - 3\\lambda + \\frac{50 - 1}{25} = 0\n$$\n$$\n\\lambda^2 - 3\\lambda + \\frac{49}{25} = 0\n$$\nWe solve this quadratic equation for $\\lambda$:\n$$\n\\lambda = \\frac{-(-3) \\pm \\sqrt{(-3)^2 - 4(1)\\left(\\frac{49}{25}\\right)}}{2(1)} = \\frac{3 \\pm \\sqrt{9 - \\frac{196}{25}}}{2}\n$$\n$$\n\\lambda = \\frac{3 \\pm \\sqrt{\\frac{225 - 196}{25}}}{2} = \\frac{3 \\pm \\sqrt{\\frac{29}{25}}}{2} = \\frac{3 \\pm \\frac{\\sqrt{29}}{5}}{2}\n$$\nThe two eigenvalues are $\\lambda_1 = \\frac{3}{2} + \\frac{\\sqrt{29}}{10}$ and $\\lambda_2 = \\frac{3}{2} - \\frac{\\sqrt{29}}{10}$.\nThe problem asks for the updated smallest Ritz value, which corresponds to the smallest eigenvalue of $B$. This is $\\lambda_2$.\n$$\n\\lambda_{\\min} = \\frac{3}{2} - \\frac{\\sqrt{29}}{10}\n$$\nThis is the updated estimate for the ground-state eigenvalue after one Davidson expansion step.", "answer": "$$\\boxed{\\frac{3}{2} - \\frac{\\sqrt{29}}{10}}$$", "id": "2900262"}, {"introduction": "Iterative algorithms, while powerful, can sometimes exhibit pathological behavior in challenging situations. This exercise explores a classic failure mode known as \"root-flipping,\" which often occurs when seeking nearly degenerate eigenvalues using a simple single-vector approach [@problem_id:2900285]. By analytically deriving the iterative map for a model two-level system, you will gain direct insight into why the algorithm may oscillate between states rather than converging, highlighting the critical need for more robust block-Davidson strategies in practical applications.", "problem": "In the context of iterative eigensolvers for quantum chemistry, consider a model Hamiltonian representing a nearly degenerate doublet,\n$$\nH \\;=\\; \\begin{pmatrix}\n-\\delta & \\epsilon \\\\\n\\epsilon & \\delta\n\\end{pmatrix},\n$$\nwith real parameters $\\delta$ and $\\epsilon$, where $|\\epsilon| \\ll |\\delta|$ models weak coupling between two nearly degenerate basis states. The matrix $H$ is real symmetric and thus has real eigenvalues and orthogonal eigenvectors.\n\nWe consider the single-vector Davidson diagonalization with one-dimensional restarts, using the diagonal (Davidson) preconditioner. In this naive variant, starting from a normalized vector $v = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ with $x^{2}+y^{2}=1$, one Davidson iteration proceeds as follows:\n- Compute the Rayleigh quotient $\\theta = v^{\\mathsf{T}} H v$.\n- Compute the residual $r = H v - \\theta v$.\n- With $D = \\mathrm{diag}(H)$, compute the correction $t$ by solving $(D - \\theta I)\\, t = r$.\n- Form the new trial vector as $v^{+} \\propto v + t$ (normalized after addition).\n\nDefine the component ratio $s = y/x$ for the current vector and $s^{+} = y^{+}/x^{+}$ for the updated vector. Starting only from the definitions above of the Rayleigh quotient, the residual, and the Davidson correction, derive the closed-form expression for the rational map $f$ such that $s^{+} = f(s)$ in terms of $s$, $\\delta$, and $\\epsilon$. Your derivation should be explicit and self-contained.\n\nThen, briefly explain (qualitatively, without computing a number) how, for $|\\epsilon| \\ll |\\delta|$ and with $s$ initially near $\\pm 1$, the map $s \\mapsto f(s)$ can produce an oscillatory two-cycle that alternates the iterate between approximations to the two lowest roots when only a single vector is retained each step, and why a block Davidson step with block size $2$ stabilizes convergence for this two-level problem.\n\nProvide as your final answer the closed-form expression for $f(s)$ (i.e., $s^{+}$ as an explicit function of $s$, $\\delta$, and $\\epsilon$). No numerical evaluation is required, and no units are necessary. Do not include any additional commentary in your final answer beyond this single expression.", "solution": "The problem requires the derivation of the iterative map for the component ratio of a trial vector in a single-vector Davidson diagonalization procedure for a specified $2 \\times 2$ Hamiltonian, followed by a qualitative explanation of its convergence behavior.\n\nFirst, the problem is validated.\n\n**Step 1: Extract Givens**\n-   Hamiltonian: $H = \\begin{pmatrix} -\\delta & \\epsilon \\\\ \\epsilon & \\delta \\end{pmatrix}$ with real parameters $\\delta, \\epsilon$ and $|\\epsilon| \\ll |\\delta|$.\n-   Trial vector: $v = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$, with $x^2+y^2=1$.\n-   Component ratio: $s = y/x$.\n-   Davidson iteration:\n    1.  Rayleigh quotient: $\\theta = v^{\\mathsf{T}} H v$.\n    2.  Residual: $r = H v - \\theta v$.\n    3.  Preconditioner: $D = \\mathrm{diag}(H) = \\begin{pmatrix} -\\delta & 0 \\\\ 0 & \\delta \\end{pmatrix}$.\n    4.  Correction: Solve $(D - \\theta I)\\, t = r$ for $t$.\n    5.  Update: $v^{+} \\propto v + t$, with new ratio $s^{+} = y^{+}/x^{+}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, using a standard model from quantum mechanics and a well-known numerical algorithm (Davidson diagonalization). It is well-posed, providing all necessary definitions to derive the requested mathematical expression. The language is objective and precise. The procedure specified in \"solve $(D-\\theta I)t=r$\" becomes ambiguous if $D-\\theta I$ is singular, which occurs if the Rayleigh quotient $\\theta$ equals a diagonal entry of $H$. This happens if the trial vector is one of the basis vectors (e.g., $s=0$ or $s \\to \\infty$). However, assuming the desired map $s \\mapsto f(s)$ is a rational function, its value at these points can be determined by taking the limit from non-singular cases, thus resolving the ambiguity. Under this reasonable interpretation, the problem is valid.\n\n**Derivation of the map $s^{+} = f(s)$**\n\nLet the current normalized trial vector be $v = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$. The Rayleigh quotient is $\\theta = v^{\\mathsf{T}}Hv$.\nThe components of the residual vector $r = Hv - \\theta v$ are:\n$$r_x = (-\\delta x + \\epsilon y) - \\theta x = (-\\delta - \\theta)x + \\epsilon y$$\n$$r_y = (\\epsilon x + \\delta y) - \\theta y = \\epsilon x + (\\delta - \\theta)y$$\nThe diagonal preconditioner matrix is $D = \\begin{pmatrix} -\\delta & 0 \\\\ 0 & \\delta \\end{pmatrix}$. The correction vector $t = \\begin{pmatrix} t_x \\\\ t_y \\end{pmatrix}$ is found by solving $(D - \\theta I)t=r$:\n$$t_x = \\frac{r_x}{-\\delta - \\theta} = \\frac{(-\\delta - \\theta)x + \\epsilon y}{-\\delta - \\theta} = x + \\frac{\\epsilon y}{-\\delta - \\theta} = x - \\frac{\\epsilon y}{\\delta + \\theta}$$\n$$t_y = \\frac{r_y}{\\delta - \\theta} = \\frac{\\epsilon x + (\\delta - \\theta)y}{\\delta - \\theta} = y + \\frac{\\epsilon x}{\\delta - \\theta}$$\nThe new trial vector $v^{+}$ is proportional to $v+t$:\n$$v^{+} \\propto \\begin{pmatrix} x+t_x \\\\ y+t_y \\end{pmatrix} = \\begin{pmatrix} x + (x - \\frac{\\epsilon y}{\\delta + \\theta}) \\\\ y + (y + \\frac{\\epsilon x}{\\delta - \\theta}) \\end{pmatrix} = \\begin{pmatrix} 2x - \\frac{\\epsilon y}{\\delta + \\theta} \\\\ 2y + \\frac{\\epsilon x}{\\delta - \\theta} \\end{pmatrix}$$\nThe new component ratio $s^{+} = y^{+}/x^{+}$ is given by:\n$$s^{+} = \\frac{2y + \\frac{\\epsilon x}{\\delta - \\theta}}{2x - \\frac{\\epsilon y}{\\delta + \\theta}}$$\nTo express this in terms of $s=y/x$, we divide the numerator and denominator by $x$:\n$$s^{+} = \\frac{2s + \\frac{\\epsilon}{\\delta - \\theta}}{2 - \\frac{\\epsilon s}{\\delta + \\theta}}$$\nNow, we express $\\theta$ in terms of $s$. Using an unnormalized vector $\\begin{pmatrix} 1 \\\\ s \\end{pmatrix}$ for the Rayleigh quotient calculation gives:\n$$\\theta = \\frac{1}{1+s^2}\\begin{pmatrix} 1 & s \\end{pmatrix} \\begin{pmatrix} -\\delta & \\epsilon \\\\ \\epsilon & \\delta \\end{pmatrix} \\begin{pmatrix} 1 \\\\ s \\end{pmatrix} = \\frac{-\\delta + \\delta s^2 + 2\\epsilon s}{1+s^2}$$\nWe compute the denominators needed for the $s^{+}$ expression:\n$$\\delta - \\theta = \\delta - \\frac{\\delta s^2 - \\delta + 2\\epsilon s}{1+s^2} = \\frac{\\delta(1+s^2) - (\\delta s^2 - \\delta + 2\\epsilon s)}{1+s^2} = \\frac{2\\delta - 2\\epsilon s}{1+s^2}$$\n$$\\delta + \\theta = \\delta + \\frac{\\delta s^2 - \\delta + 2\\epsilon s}{1+s^2} = \\frac{\\delta(1+s^2) + (\\delta s^2 - \\delta + 2\\epsilon s)}{1+s^2} = \\frac{2\\delta s^2 + 2\\epsilon s}{1+s^2}$$\nSubstituting these into the expression for $s^{+}$:\n$$s^{+} = \\frac{2s + \\frac{\\epsilon(1+s^2)}{2(\\delta - \\epsilon s)}}{2 - \\frac{\\epsilon s(1+s^2)}{2(\\delta s^2 + 2\\epsilon s)}} = \\frac{2s + \\frac{\\epsilon(1+s^2)}{2(\\delta - \\epsilon s)}}{2 - \\frac{\\epsilon(1+s^2)}{2(\\delta s + \\epsilon)}}$$\nThe cancellation of $s$ in the denominator is valid for $s \\neq 0$. The resulting rational function will be continuous, and its value at $s=0$ can be obtained by taking the limit.\nWe simplify the numerator and denominator:\nNumerator: $N = \\frac{4s(\\delta - \\epsilon s) + \\epsilon(1+s^2)}{2(\\delta - \\epsilon s)} = \\frac{4\\delta s - 4\\epsilon s^2 + \\epsilon + \\epsilon s^2}{2(\\delta - \\epsilon s)} = \\frac{4\\delta s + \\epsilon - 3\\epsilon s^2}{2(\\delta - \\epsilon s)}$\nDenominator: $D = \\frac{4(\\delta s + \\epsilon) - \\epsilon(1+s^2)}{2(\\delta s + \\epsilon)} = \\frac{4\\delta s + 4\\epsilon - \\epsilon - \\epsilon s^2}{2(\\delta s + \\epsilon)} = \\frac{4\\delta s + 3\\epsilon - \\epsilon s^2}{2(\\delta s + \\epsilon)}$\nCombining these gives the final map $s^{+} = f(s) = N/D$:\n$$s^{+} = \\frac{\\frac{4\\delta s + \\epsilon - 3\\epsilon s^2}{2(\\delta - \\epsilon s)}}{\\frac{4\\delta s + 3\\epsilon - \\epsilon s^2}{2(\\delta s + \\epsilon)}} = \\frac{(4\\delta s + \\epsilon(1 - 3s^2))}{(4\\delta s + \\epsilon(3 - s^2))} \\cdot \\frac{(\\delta s + \\epsilon)}{(\\delta - \\epsilon s)}$$\nThis is the required closed-form expression for the map $f$.\n\n**Qualitative Explanation of Convergence Behavior**\n\nThe described model has two nearly degenerate eigenstates. Let's call them $v_1$ and $v_2$, with corresponding eigenvalues $\\lambda_1 \\approx -\\delta$ and $\\lambda_2 \\approx \\delta$. In the single-vector Davidson method, starting with a trial vector $v^{(k)}$ that is a good approximation to one of the eigenstates, say $v_1$, the Rayleigh quotient $\\theta$ will be close to the corresponding eigenvalue, $\\theta \\approx \\lambda_1$. The residual $r = (H-\\theta I)v^{(k)}$ is supposed to contain information about the error in $v^{(k)}$. The key step is the preconditioning, which computes the correction $t = (D-\\theta I)^{-1}r$.\n\nFor the nearly degenerate case where $|\\epsilon| \\ll |\\delta|$, the true eigenvectors are small perturbations of the basis vectors $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. If the current iterate $v^{(k)}$ is close to the eigenvector $v_1$ (which is close to $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$), then $\\theta \\approx \\lambda_1 \\approx -\\delta$. The preconditioner matrix becomes $D-\\theta I \\approx \\begin{pmatrix} -\\delta - (-\\delta) & 0 \\\\ 0 & \\delta - (-\\delta) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 2\\delta \\end{pmatrix}$. This preconditioner is nearly singular. Its inverse, $(D-\\theta I)^{-1}$, will have a very large entry corresponding to the direction of the *other* basis state, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The residual $r$ will have a small component pointing towards the other eigenvector $v_2$. The preconditioner will dramatically amplify this component. Consequently, the correction vector $t$ will be dominated by the direction towards $v_2$, which is nearly orthogonal to the current iterate $v^{(k)}$.\n\nThe update step $v^{(k+1)} \\propto v^{(k)} + t$ then produces a vector that is pulled strongly towards $v_2$. The next iteration, starting from an approximation to $v_2$, will similarly generate a correction pointing back towards $v_1$. This results in the iterate oscillating between the two nearly degenerate states, failing to converge to either one. This behavior is called \"root-flipping\" and is a known pathology of single-vector iterative methods when applied to nearly degenerate systems.\n\nA block Davidson method with a block size of $2$ or more resolves this issue. By maintaining a subspace of at least two vectors, the algorithm can build a representation for the entire nearly degenerate manifold. It starts with two initial vectors (e.g., approximations to both eigenstates). At each step, it solves a $2 \\times 2$ eigenvalue problem within the subspace to find the best current approximations (the Ritz vectors) to both eigenstates. It then computes correction vectors for *both* Ritz vectors and uses them to expand the subspace. By working in a vector space that is large enough to describe both nearly degenerate states simultaneously, the algorithm can stably converge to both eigenvectors without the oscillatory behavior caused by the preconditioner pushing the single trial vector from one state to the other.", "answer": "$$\n\\boxed{\\frac{(4\\delta s + \\epsilon(1 - 3s^2))(\\delta s + \\epsilon)}{(4\\delta s + \\epsilon(3 - s^2))(\\delta - \\epsilon s)}}\n$$", "id": "2900285"}, {"introduction": "Quantum chemistry calculations in non-orthogonal atomic orbital bases require solving the *generalized* eigenvalue problem, $H c = \\theta S c$. This practice bridges the gap between textbook examples and real-world application by extending the Davidson method to this more complex scenario [@problem_id:2900312]. You will implement a crucial step involving an $S$-aware preconditioner and learn about numerical stabilization techniques like denominator damping, essential components for building a research-grade electronic structure program.", "problem": "You are given the generalized symmetric eigenvalue problem in a non-orthogonal atomic orbital (AO) basis, where the overlap matrix is denoted by $S$ and the Hamiltonian (or Fock-like) matrix is denoted by $H$. The generalized eigenproblem seeks a pair $(\\theta, c)$ such that\n$$\nH c = \\theta S c,\n$$\nwith $S$ symmetric positive definite and $H$ symmetric. In the Davidson method, a subspace expansion vector is constructed from the residual of the current Ritz pair and a preconditioner that approximates the inverse of the shifted operator. Your task is to derive from first principles the construction of the residual for a generalized eigenproblem, justify the diagonal preconditioner in a non-orthogonal basis, implement an $S$-aware Davidson step using the diagonal preconditioner, and compute the subspace expansion vector for a small set of test cases.\n\nFundamental bases and definitions you must use:\n- The generalized Rayleigh quotient of a vector $c$ with respect to $(H, S)$ is defined as\n$$\n\\theta(c) = \\frac{c^{\\mathsf{T}} H c}{c^{\\mathsf{T}} S c}.\n$$\n- The residual associated with $(\\theta, c)$ for the generalized eigenproblem is\n$$\nr = H c - \\theta S c.\n$$\n- In the Davidson method, one approximates the inverse of the operator $H - \\theta S$ by using a diagonal matrix $D$ that approximates $H$ in the AO basis. In a non-orthogonal AO basis, an $S$-aware diagonal preconditioner approximates $(H - \\theta S)^{-1}$ by the inverse of the diagonal matrix with entries $D_{ii} - \\theta S_{ii}$, applied elementwise.\n\nAlgorithmic tasks to implement:\n1. For each test case, take as input $H \\in \\mathbb{R}^{n \\times n}$ symmetric, $S \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite, a diagonal approximation $D \\in \\mathbb{R}^{n}$ (the diagonal of $H$ unless otherwise provided), and a nonzero vector $c \\in \\mathbb{R}^{n}$.\n2. Compute the generalized Rayleigh quotient\n$$\n\\theta = \\frac{c^{\\mathsf{T}} H c}{c^{\\mathsf{T}} S c}.\n$$\n3. Compute the residual\n$$\nr = H c - \\theta S c.\n$$\n4. Form the elementwise denominators\n$$\n\\delta_i = D_{ii} - \\theta S_{ii}.\n$$\n5. To safely invert near-zero denominators, apply magnitude clamping with a damping parameter $\\tau > 0$: define the effective denominator $\\tilde{\\delta}_i$ by\n- If $|\\delta_i| \\ge \\tau$, set $\\tilde{\\delta}_i = \\delta_i$.\n- If $|\\delta_i| < \\tau$ and $\\delta_i > 0$, set $\\tilde{\\delta}_i = \\tau$.\n- If $|\\delta_i| < \\tau$ and $\\delta_i < 0$, set $\\tilde{\\delta}_i = -\\tau$.\n- If $\\delta_i = 0$, set $\\tilde{\\delta}_i = \\tau$.\n6. Compute the $S$-aware Davidson subspace expansion vector as the preconditioned residual with a minus sign,\n$$\nt_i = - \\frac{r_i}{\\tilde{\\delta}_i}, \\quad i = 1, \\dots, n.\n$$\nDo not perform any orthogonalization in this task; return the raw preconditioned correction $t$.\n\nNumerical stability requirement:\n- Use a damping parameter $\\tau = 10^{-4}$ in all cases unless specified otherwise.\n\nTest suite:\nImplement your program to compute the vector $t$ for each of the following cases. All matrices and vectors are given explicitly. Every number that appears below is to be treated as a real scalar in standard double-precision arithmetic.\n\n- Case A (baseline, size $3 \\times 3$):\n  - $H = \\begin{bmatrix} 1.2 & 0.1 & 0.0 \\\\ 0.1 & 0.9 & 0.05 \\\\ 0.0 & 0.05 & 1.5 \\end{bmatrix}$,\n    $S = \\begin{bmatrix} 1.0 & 0.02 & 0.0 \\\\ 0.02 & 1.0 & 0.01 \\\\ 0.0 & 0.01 & 1.0 \\end{bmatrix}$,\n    $D = \\operatorname{diag}(1.2, 0.9, 1.5)$,\n    $c = \\begin{bmatrix} 0.6 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}$,\n    $\\tau = 10^{-4}$.\n- Case B (near-singular denominator, size $3 \\times 3$):\n  - $H = \\operatorname{diag}(1.0, 2.0, 3.0)$,\n    $S = I_{3}$,\n    $D = \\operatorname{diag}(1.0, 2.0, 3.0)$,\n    $c = \\begin{bmatrix} 0.01 \\\\ 1.0 \\\\ 0.01 \\end{bmatrix}$,\n    $\\tau = 10^{-4}$.\n- Case C (non-orthogonal overlap with off-diagonals, size $3 \\times 3$):\n  - $H = \\begin{bmatrix} 1.8 & 0.3 & 0.2 \\\\ 0.3 & 1.1 & 0.4 \\\\ 0.2 & 0.4 & 1.6 \\end{bmatrix}$,\n    $S = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.0 & 0.1 \\\\ 0.0 & 0.1 & 1.0 \\end{bmatrix}$,\n    $D = \\operatorname{diag}(1.8, 1.1, 1.6)$,\n    $c = \\begin{bmatrix} 0.3 \\\\ -0.4 \\\\ 0.5 \\end{bmatrix}$,\n    $\\tau = 10^{-4}$.\n- Case D (small system with noticeable overlap, size $2 \\times 2$):\n  - $H = \\begin{bmatrix} 0.7 & 0.05 \\\\ 0.05 & 1.9 \\end{bmatrix}$,\n    $S = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.0 \\end{bmatrix}$,\n    $D = \\operatorname{diag}(0.7, 1.9)$,\n    $c = \\begin{bmatrix} 1.0 \\\\ 0.2 \\end{bmatrix}$,\n    $\\tau = 10^{-4}$.\n- Case E (exact eigenvector, zero residual check, size $2 \\times 2$):\n  - $H = \\operatorname{diag}(1.0, 3.0)$,\n    $S = I_{2}$,\n    $D = \\operatorname{diag}(1.0, 3.0)$,\n    $c = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$,\n    $\\tau = 10^{-4}$.\n\nYour program should compute the vector $t$ for each case using the procedure above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list representing the subspace expansion vector $t$ for a given case, in the same order as listed above. For example, the output format should look like\n$[ [t^{(A)}_1, t^{(A)}_2, \\dots], [t^{(B)}_1, \\dots], \\dots ]$.\n- Use standard floating-point decimal notation for all entries. No physical units are involved.", "solution": "The problem presented is a precise and well-formulated exercise in numerical linear algebra, specifically concerning one step of the Davidson iterative method for solving the generalized symmetric eigenvalue problem. It is scientifically sound, logically consistent, and provides all necessary information for a unique solution. We shall proceed with a complete, reasoned solution.\n\nThe central task in many quantum chemistry calculations, such as the Hartree-Fock method, is to solve the generalized eigenvalue problem:\n$$\nH c = \\theta S c\n$$\nHere, $H$ is the Hamiltonian (or Fock) matrix, which is symmetric, and $S$ is the overlap matrix of the non-orthogonal atomic orbital basis, which is symmetric and positive definite. The eigenvalues $\\theta$ correspond to orbital energies, and the eigenvectors $c$ represent the coefficients of the atomic orbitals that form the molecular orbitals.\n\nFor systems of significant size, direct diagonalization of the full matrices is computationally prohibitive. Iterative methods, such as the one proposed by Davidson, are employed instead. These methods operate by building a small subspace, solving the eigenproblem within this subspace, and then systematically expanding the subspace with a new vector that is expected to efficiently improve the approximation of the desired eigenpair.\n\nLet us assume we have a current approximation to an eigenvector, denoted by the vector $c$. The quality of this approximation is assessed through the generalized Rayleigh quotient:\n$$\n\\theta = \\theta(c) = \\frac{c^{\\mathsf{T}} H c}{c^{\\mathsf{T}} S c}\n$$\nThis value, $\\theta$, is the best possible approximation of an eigenvalue for the given trial vector $c$. The corresponding residual vector, $r$, quantifies the error in the eigenvalue equation for this approximate pair $(\\theta, c)$:\n$$\nr = H c - \\theta S c\n$$\nIf $c$ were an exact eigenvector, the residual $r$ would be the zero vector. A non-zero residual indicates that the approximation must be improved. The core idea of the Davidson method is to compute a correction vector, $\\delta c$, such that $c + \\delta c$ is a better approximation to the true eigenvector.\n\nThe exact correction, $\\delta c$, should satisfy the equation $H(c + \\delta c) = \\theta_{\\text{exact}} S(c + \\delta c)$, which is nonlinear and difficult to solve. A linear approximation, known as the correction equation, is derived by requiring that the new vector $c + \\delta c$ satisfies the original equation with the current approximate eigenvalue $\\theta$:\n$$\nH(c + \\delta c) = \\theta S (c + \\delta c)\n$$\nRearranging this gives:\n$$\n(H - \\theta S) \\delta c = - (H c - \\theta S c) = -r\n$$\nThis yields a linear system for the correction: $\\delta c = -(H - \\theta S)^{-1} r$. However, solving this system is computationally as expensive as the original problem. The innovation of the Davidson method is to approximate the operator $(H - \\theta S)^{-1}$ with a simpler, invertible operator, known as a preconditioner.\n\nIn a typical atomic orbital basis, the Hamiltonian matrix $H$ is often diagonally dominant, meaning the diagonal elements are large in magnitude compared to the off-diagonal elements. This suggests that the diagonal of $H$, which we denote as $D$, is a reasonable first-order approximation to $H$. Following this logic, we approximate the operator $(H - \\theta S)$ by only its diagonal components. This leads to the so-called $S$-aware diagonal preconditioner. The matrix $(H - \\theta S)$ is approximated by a diagonal matrix $P$ with entries:\n$$\nP_{ii} = D_{ii} - \\theta S_{ii}\n$$\nThe inverse of this diagonal matrix is trivially computed: $(P^{-1})_{ii} = 1/P_{ii}$. Applying this preconditioner to the residual gives the new subspace expansion vector, which the problem denotes as $t$:\n$$\nt = -P^{-1} r\n$$\nWritten element-wise, this is:\n$$\nt_i = -\\frac{r_i}{D_{ii} - \\theta S_{ii}}\n$$\nThis vector $t$ is the preconditioned residual. It is used to expand the subspace in the next iteration of the Davidson algorithm.\n\nA critical issue arises when a denominator, $\\delta_i = D_{ii} - \\theta S_{ii}$, is close to zero. This occurs when the current Rayleigh quotient $\\theta$ is near one of the approximate eigenvalues $D_{ii}/S_{ii}$. In this situation, the magnitude of the corresponding component $t_i$ can become excessively large, leading to numerical instability in the iterative process. To prevent this, a regularization technique is employed. A small positive damping parameter, $\\tau$, is introduced. The denominator $\\delta_i$ is \"clamped\" such that its magnitude is no smaller than $\\tau$. The effective denominator $\\tilde{\\delta}_i$ is defined as follows:\n- If $|\\delta_i| \\ge \\tau$, $\\tilde{\\delta}_i = \\delta_i$.\n- If $|\\delta_i| < \\tau$, the value is pushed to the boundary of the allowed interval, preserving its sign. For $\\delta_i > 0$, $\\tilde{\\delta}_i = \\tau$; for $\\delta_i < 0$, $\\tilde{\\delta}_i = -\\tau$.\n- The special case $\\delta_i = 0$ is handled by setting $\\tilde{\\delta}_i = \\tau$.\n\nThis clamping procedure ensures that the correction vector $t$ remains of a reasonable magnitude, thereby stabilizing the convergence of the Davidson method.\n\nThe full algorithm for a single Davidson step, as specified, is as follows:\n1.  Given $H$, $S$, $D$, a non-zero vector $c$, and a damping parameter $\\tau$.\n2.  Compute the generalized Rayleigh quotient: $\\theta = (c^{\\mathsf{T}} H c) / (c^{\\mathsf{T}} S c)$.\n3.  Compute the residual vector: $r = H c - \\theta S c$.\n4.  Form the diagonal preconditioner denominators: $\\delta_i = D_{ii} - \\theta S_{ii}$ for each $i$.\n5.  Apply the magnitude clamping with parameter $\\tau$ to obtain the stabilized denominators $\\tilde{\\delta}_i$.\n6.  Compute the subspace expansion vector: $t_i = -r_i / \\tilde{\\delta}_i$.\n\nThis robust procedure forms the core of one iteration of the Davidson eigensolver, and we will now implement it for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the S-aware Davidson subspace expansion\n    vector for a suite of test cases.\n    \"\"\"\n\n    def compute_expansion_vector(H, S, D, c, tau):\n        \"\"\"\n        Computes the S-aware Davidson subspace expansion vector for a single case.\n\n        Args:\n            H (np.ndarray): The Hamiltonian matrix.\n            S (np.ndarray): The overlap matrix.\n            D (np.ndarray): The diagonal approximation of H.\n            c (np.ndarray): The current approximate eigenvector.\n            tau (float): The damping parameter for the preconditioner.\n\n        Returns:\n            np.ndarray: The computed subspace expansion vector t.\n        \"\"\"\n        # 1. Ensure inputs are numpy arrays for consistent operations.\n        H = np.asarray(H, dtype=float)\n        S = np.asarray(S, dtype=float)\n        D = np.asarray(D, dtype=float)\n        c = np.asarray(c, dtype=float)\n\n        # 2. Compute the generalized Rayleigh quotient theta.\n        c_T_H_c = c.T @ H @ c\n        c_T_S_c = c.T @ S @ c\n        \n        # Avoid division by zero if c is the zero vector, though problem states non-zero.\n        if c_T_S_c == 0:\n            # This case is not expected with valid inputs (S is SPD, c is non-zero)\n            # but as a safeguard, we can define theta as 0 or handle the error.\n            # A zero residual will result if Hc is also zero.\n            theta = 0.0\n        else:\n            theta = c_T_H_c / c_T_S_c\n\n        # 3. Compute the residual r.\n        r = H @ c - theta * (S @ c)\n\n        # 4. Form the elementwise denominators for the preconditioner.\n        # S.diagonal() extracts the main diagonal of the overlap matrix S.\n        denominators = D - theta * S.diagonal()\n\n        # 5. Apply magnitude clamping with the damping parameter tau.\n        tilde_denominators = np.zeros_like(denominators)\n        for i, delta in enumerate(denominators):\n            if abs(delta) >= tau:\n                tilde_denominators[i] = delta\n            else:\n                if delta == 0:\n                    tilde_denominators[i] = tau\n                elif delta > 0:\n                    tilde_denominators[i] = tau\n                else: # delta  0\n                    tilde_denominators[i] = -tau\n\n        # 6. Compute the subspace expansion vector t.\n        # We must handle the case where tilde_denominators can still be zero,\n        # although the logic above prevents this for non-zero tau.\n        # The elementwise division is safe.\n        t = -r / tilde_denominators\n        \n        return t\n\n    # Define the test cases from the problem statement.\n    tau_global = 1e-4\n\n    test_cases = [\n        # Case A\n        (\n            [[1.2, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 1.5]],\n            [[1.0, 0.02, 0.0], [0.02, 1.0, 0.01], [0.0, 0.01, 1.0]],\n            [1.2, 0.9, 1.5],\n            [0.6, 0.3, 0.2],\n            tau_global\n        ),\n        # Case B\n        (\n            [[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 3.0]],\n            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n            [1.0, 2.0, 3.0],\n            [0.01, 1.0, 0.01],\n            tau_global\n        ),\n        # Case C\n        (\n            [[1.8, 0.3, 0.2], [0.3, 1.1, 0.4], [0.2, 0.4, 1.6]],\n            [[1.0, 0.2, 0.0], [0.2, 1.0, 0.1], [0.0, 0.1, 1.0]],\n            [1.8, 1.1, 1.6],\n            [0.3, -0.4, 0.5],\n            tau_global\n        ),\n        # Case D\n        (\n            [[0.7, 0.05], [0.05, 1.9]],\n            [[1.0, 0.3], [0.3, 1.0]],\n            [0.7, 1.9],\n            [1.0, 0.2],\n            tau_global\n        ),\n        # Case E\n        (\n            [[1.0, 0.0], [0.0, 3.0]],\n            [[1.0, 0.0], [0.0, 1.0]],\n            [1.0, 3.0],\n            [1.0, 0.0],\n            tau_global\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        H_case, S_case, D_case, c_case, tau_case = case\n        t_vector = compute_expansion_vector(H_case, S_case, D_case, c_case, tau_case)\n        results.append(t_vector.tolist())\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2900312"}]}