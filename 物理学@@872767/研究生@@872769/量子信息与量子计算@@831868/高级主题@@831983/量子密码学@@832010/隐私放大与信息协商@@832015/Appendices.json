{"hands_on_practices": [{"introduction": "信息对和是确保通信双方密钥一致性的核心环节，而低密度奇偶校验码（LDPC）及其迭代解码算法是实现这一目标的强大工具。本练习 ([@problem_id:110643]) 将你置于一个具体的解码场景中：一个由特定坦纳图（Tanner graph）定义的LDPC码，通过置信度传播（Belief Propagation）算法在一个二进制删除信道（BEC）上恢复被删除的比特。通过计算单次迭代后恢复失败的概率，你将亲身体验迭代解码的工作原理，并理解码的图结构如何直接影响其纠错性能。", "problem": "在量子密钥分发 (QKD) 中，信息协商是一个关键步骤，通信双方（Alice 和 Bob）在此步骤中纠正其共享的、经过筛选的密钥中的错误。这通常通过使用经典纠错码来完成，例如低密度奇偶校验 (LDPC) 码。这些码的性能通常在与筛选密钥中观察到的错误相关的简化信道模型上进行分析。\n\nLDPC 码是一种由稀疏二进制奇偶校验矩阵 $H$ 定义的线性分组码。其结构可以方便地用 Tanner 图表示，这是一种二分图，其中变量节点代表码字比特，校验节点代表奇偶校验方程。如果比特 $i$ 参与校验方程 $j$，则在变量节点 $v_i$ 和校验节点 $c_j$ 之间存在一条边。\n\n考虑一个场景，其中共享密钥被建模为在二元擦除信道 (BEC) 上传输的码字，该信道以概率 $p$ 擦除每个比特，并以概率 $1-p$ 正确传输它。为了恢复被擦除的比特（即解码码字），使用了一种称为置信度传播 (BP) 的迭代消息传递算法。在 BEC 的背景下，BP 算法的工作方式如下：\n- 如果一个被擦除的变量节点其相邻的校验节点中至少有一个能提供其正确的值，则该变量节点可以被恢复。\n- 一个校验节点能确定其相邻的某个被擦除变量节点的值，当且仅当参与该同一校验方程的所有其他变量节点都是已知的（即未被擦除）。\n\n考虑一个特定的正则 (2,4) LDPC 码，这意味着每个变量节点的度为 2，每个校验节点的度为 4。该码有 $N=8$ 个变量节点，表示为 $\\{v_1, \\dots, v_8\\}$，以及 $M=4$ 个校验节点，表示为 $\\{c_1, \\dots, c_4\\}$。该码的 Tanner 图由以下连接性定义：\n- $c_1$ 连接到 $\\{v_1, v_2, v_3, v_4\\}$。\n- $c_2$ 连接到 $\\{v_5, v_6, v_7, v_8\\}$。\n- $c_3$ 连接到 $\\{v_1, v_2, v_5, v_6\\}$。\n- $c_4$ 连接到 $\\{v_3, v_4, v_7, v_8\\}$。\n\n由此码生成的一个码字在擦除概率为 $p$ 的 BEC 上发送。您的任务是计算一个随机选择的变量节点，在给定其初始被擦除的情况下，经过单次迭代的置信度传播算法后仍未被恢复的概率。请以关于 $p$ 的闭式解析表达式给出答案。", "solution": "该问题涉及一个具有 $N=8$ 个变量节点和 $M=4$ 个校验节点的正则 (2,4) LDPC 码。其 Tanner 图定义如下：\n- $c_1$ 连接到 $\\{v_1, v_2, v_3, v_4\\}$\n- $c_2$ 连接到 $\\{v_5, v_6, v_7, v_8\\}$\n- $c_3$ 连接到 $\\{v_1, v_2, v_5, v_6\\}$\n- $c_4$ 连接到 $\\{v_3, v_4, v_7, v_8\\}$\n\n每个变量节点的度为 2。目标是找到一个随机选择的变量节点，在给定它在擦除概率为 $p$ 的 BEC 上被初始擦除的情况下，经过一次置信度传播算法迭代后仍未被恢复的概率。\n\n对于一个被擦除的变量节点 $v_i$，如果其相邻的校验节点中至少有一个提供了它的值，它就可以被恢复。与 $v_i$ 相邻的校验节点 $c_j$ 仅当所有连接到 $c_j$ 的其他变量节点都是已知的（未被擦除）时，才能恢复 $v_i$。\n\n考虑一个被擦除的变量节点 $v_i$。设 $c_A$ 和 $c_B$ 是其两个相邻的校验节点。定义：\n- 事件 $A$：$c_A$ 中的另外三个变量节点是已知的。\n- 事件 $B$：$c_B$ 中的另外三个变量节点是已知的。\n\n如果事件 $A$ 或 $B$ 中至少有一个发生，$v_i$ 就会被恢复。$v_i$ 仍未被恢复的概率是：\n$$\nP(\\text{unrecovered} \\mid \\text{erased}) = 1 - P(A \\cup B \\mid \\text{erased})\n$$\n根据容斥原理：\n$$\nP(A \\cup B \\mid \\text{erased}) = P(A \\mid \\text{erased}) + P(B \\mid \\text{erased}) - P(A \\cap B \\mid \\text{erased})\n$$\n\n擦除事件是独立的，并且以 $v_i$ 被擦除为条件不影响其他节点。因此：\n$$\nP(A \\mid \\text{erased}) = P(\\text{three specific nodes known}) = (1-p)^3\n$$\n类似地，$P(B \\mid \\text{erased}) = (1-p)^3$。\n\n事件 $A$ 和 $B$ 所依赖的变量节点集存在交集。以 $v_1$ 为例，其邻居为 $c_1$ 和 $c_3$。恢复 $v_1$ 分别依赖于 $\\{v_2, v_3, v_4\\}$ 和 $\\{v_2, v_5, v_6\\}$ 这两组节点。事件 $A \\cap B$ 意味着这两组节点中的所有节点都是已知的。其并集为 $\\{v_2, v_3, v_4, v_5, v_6\\}$，共5个不同的节点。\n\n因此：\n$$\nP(A \\cap B \\mid \\text{erased}) = (1-p)^5\n$$\n\n代入得：\n$$\nP(A \\cup B \\mid \\text{erased}) = (1-p)^3 + (1-p)^3 - (1-p)^5 = 2(1-p)^3 - (1-p)^5\n$$\n\n所以：\n$$\nP(\\text{unrecovered} \\mid \\text{erased}) = 1 - \\left[ 2(1-p)^3 - (1-p)^5 \\right] = 1 - 2(1-p)^3 + (1-p)^5\n$$\n\n由于码的结构是对称的，该表达式对任何变量节点都成立。", "answer": "$$ \\boxed{1 - 2(1-p)^3 + (1-p)^5} $$", "id": "110643"}, {"introduction": "任何信息对和协议都无法避免一个基本事实：公开信道上的通信会向窃听者泄露关于密钥的信息。为了确保最终密钥的安全性，我们必须精确地量化这种信息泄露。本练习 ([@problem_id:110589]) 建立了一个简洁而深刻的模型，揭示了哪怕是看似无害的一比特公共信息——密钥与一个公共随机串汉明距离的奇偶性——也会如何降低密钥的安全性。通过计算最小熵（min-entropy）的损失，你将掌握量化信息泄露的基本方法，这是后续进行隐私放大的必要前提。", "problem": "在一个量子密钥分发 (QKD) 协议中，经过量子通信、筛选和信息协商等阶段后，Alice 和 Bob 共享一个他们认为完全相同的秘密密钥 $X \\in \\{0,1\\}^n$。从他们的角度来看，密钥 $X$ 是一个长度为 $n$ 的均匀随机比特串。\n\n一个窃听者 Eve 一直在监控整个过程。她对密钥的旁路信息建模如下：她已经完全获知了密钥的前 $k$ 个比特，其中 $0 \\le k  n$。剩下的 $n-k$ 个比特对她来说是完全未知的，这意味着从她的角度来看，这些比特是均匀随机的。最终密钥的安全性由其在给定 Eve 知识下的条件最小熵来量化。给定旁路信息 $E$ 的密钥 $K$ 的最小熵定义为 $H_{\\min}(K|E) = -\\log_2 P_{\\text{guess}}(K|E)$，其中 $P_{\\text{guess}}(K|E)$ 是拥有知识 $E$ 的对手能够猜中 $K$ 值的最大概率。\n\n在进行隐私放大之前，Alice 和 Bob 决定执行一个额外的、但有缺陷的错误验证步骤。他们生成一个公开可访问的随机比特串 $R \\in \\{0,1\\}^n$。然后，Alice 计算她的密钥 $X$ 和公共字符串 $R$ 之间的汉明距离 $d(X, R)$。最后，她公开宣布这个汉明距离的奇偶性，即，她广播单个比特 $Z = d(X, R) \\pmod 2$。\n\n计算在 Eve 整合了公开宣布的比特 $Z$ 的知识之后，从她的角度来看，密钥 $X$ 的最终最小熵。用 $n$ 和 $k$ 表示你的答案。", "solution": "1.  **初始最小熵**\nAlice的密钥 $X \\in \\{0,1\\}^n$ 对她来说是均匀随机的。窃听者Eve的先验知识 $E$ 是密钥的前 $k$ 个比特。这意味着对于Eve来说，密钥的后 $n-k$ 个比特是完全随机的。因此，Eve猜中整个密钥 $X$ 的最大概率是猜中这 $n-k$ 个未知比特的概率，即 $P_{\\text{guess}}(X|E) = 2^{-(n-k)}$。\n泄露前的条件最小熵是：\n$$ H_{\\min}(X|E) = -\\log_2(P_{\\text{guess}}(X|E)) = -\\log_2(2^{-(n-k)}) = n-k $$\n\n2.  **分析信息泄露**\nAlice公开了 $Z = d(X, R) \\pmod 2$。汉明距离 $d(X, R)$ 是 $X \\oplus R$ 中1的个数。因此，$Z$ 是 $X \\oplus R$ 的奇偶校验位。\n$$ Z = \\left( \\sum_{i=1}^n (X_i \\oplus R_i) \\right) \\pmod 2 $$\n由于 $R$ 是公开的，Eve知道 $R_1, \\dots, R_n$。上述表达式可以重写为：\n$$ Z = \\left( \\sum_{i=1}^n X_i + \\sum_{i=1}^n R_i \\right) \\pmod 2 $$\n这意味着Eve可以从公开的 $Z$ 和 $R$ 中推断出 $\\sum_{i=1}^n X_i \\pmod 2$ 的值。换句话说，Eve学到了密钥 $X$ 的奇偶性。\n\n3.  **对未知比特的影响**\nEve学到的信息是关于整个密钥 $X$ 的奇偶性。我们可以把这个和分解为已知部分和未知部分：\n$$ \\left( \\sum_{i=1}^k X_i \\right) \\pmod 2 + \\left( \\sum_{i=k+1}^n X_i \\right) \\pmod 2 = \\text{已知值} $$\n由于Eve知道前 $k$ 个比特 $X_1, \\dots, X_k$，第一项 $\\left( \\sum_{i=1}^k X_i \\right) \\pmod 2$ 对她来说也是一个已知值。因此，她能够精确地推断出后 $n-k$ 个未知比特的奇偶性，即 $\\left( \\sum_{i=k+1}^n X_i \\right) \\pmod 2$ 的值。\n\n4.  **最终最小熵**\n在泄露之前，后 $n-k$ 个比特有 $2^{n-k}$ 种可能性。在得知它们的奇偶性之后，这些可能性的数量减少了一半，变为 $2^{n-k-1}$。\n因此，Eve现在猜中密钥 $X$ 的最大概率变为 $P_{\\text{guess}}(X|E, Z) = \\frac{1}{2^{n-k-1}} = 2^{-(n-k-1)}$。\n最终的条件最小熵是：\n$$ H_{\\min}(X|E, Z) = -\\log_2(P_{\\text{guess}}(X|E, Z)) = -\\log_2(2^{-(n-k-1)}) = n-k-1 $$\n公开一个关于密钥的奇偶校验比特，导致最小熵恰好减少了1比特。", "answer": "$$\\boxed{n-k-1}$$", "id": "110589"}, {"introduction": "隐私放大通常依赖于从一个2-通用哈希函数族中随机选择一个函数来压缩密钥，这个选择过程由一个公共种子（seed）决定。然而，协议的安全性不仅取决于原语（primitive）本身，更取决于其使用方式。本练习 ([@problem_id:110739]) 深入探讨了一种典型且危险的实现缺陷：对两个相关的原始密钥重用同一个哈希种子。通过分析这种场景下的条件最小熵，你将学会如何评估因不当操作（如种子重用）而导致的安全风险，并理解为何在密码学协议设计中，“一次一密”的原则至关重要。", "problem": "在涉及密码学密钥生成的场景中，例如在量子密钥分发（QKD）中，隐私放大是一个至关重要的过程，用于从一个更长的、部分泄露的原始密钥中生成一个短的、高度安全的密钥。这通常通过使用一个公开种子的双重通用哈希函数来完成。本问题探讨了在相关操作中重用相同哈希函数种子的安全影响。\n\n令 $X_1 \\in \\{0,1\\}^n$ 为一个原始密钥，它是一个长度为 $n$ 的均匀随机二进制字符串。第二个二进制字符串 $X_2 \\in \\{0,1\\}^n$ 是通过将 $X_1$ 传递到一个比特翻转概率为 $p \\in (0, 1/2)$ 的二进制对称信道（BSC）生成的。也就是说，$X_2 = X_1 \\oplus \\Delta$，其中 $\\Delta$ 是一个 $n$ 比特的误差向量，其分量是参数为 $p$ 的独立同分布的伯努利随机变量。\n\n通过从一个可能的种子集合中均匀随机地选择一个种子 $S$，来选择一个哈希函数 $h_S: \\{0,1\\}^n \\to \\{0,1\\}^m$。相应的哈希函数族是线性的和双重通用的。这蕴含了两个性质：\n1.  对于任意种子 $s$，$h_s$ 是一个线性映射：$h_s(x \\oplus y) = h_s(x) \\oplus h_s(y)$。\n2.  对于任意非零向量 $z \\in \\{0,1\\}^n$，在随机选择种子 $S$ 的情况下，$h_S(z)=0$ 的概率恰好是 $P_S(h_S(z)=0) = 2^{-m}$。\n\nAlice 计算她的最终密钥为 $K_1 = h_S(X_1)$。一个窃听者 Eve 设法获知了一个相关的密钥 $K_2 = h_S(X_2)$，该密钥是使用相同的种子 $S$ 生成的。Eve 的全部知识包括对 $(K_2, S)$。\n\nAlice 的密钥 $K_1$ 对抗 Eve 的知识的安全性由条件最小熵 $H_{\\min}(K_1|K_2,S)$ 来量化。最小熵是根据对手猜测密钥的最大成功概率 $P_{guess}$ 来定义的：\n$$H_{\\min}(K_1|K_2,S) = -\\log_2 P_{guess}(K_1|K_2,S)$$\n其中\n$$P_{guess}(K_1|K_2,S) = \\mathbb{E}_{(k_2,s) \\sim P_{K_2,S}} \\left[ \\max_{k_1 \\in \\{0,1\\}^m} P(K_1=k_1 | K_2=k_2, S=s) \\right]$$\n\n计算条件最小熵 $H_{\\min}(K_1|K_2,S)$。在本次计算中，你可以假设对于任意给定的种子 $s$ 和值 $k_2$，对手猜测 $K_1$ 的最优策略是猜测 $K_1=k_2$。这意味着你可以使用如下简化：\n$$\\max_{k_1 \\in \\{0,1\\}^m} P(K_1=k_1|K_2=k_2, S=s) = P(K_1=k_2|K_2=k_2, S=s)$$", "solution": "我们的目标是计算条件最小熵 $H_{\\min}(K_1 | K_2, S)$，它由对手的平均最大猜测概率 $P_{\\text{guess}}(K_1 | K_2, S)$ 决定。\n\n1.  **利用线性性质**\n哈希函数 $h_S$ 是线性的，所以我们可以将被哈希的量关联起来：\n$$ K_2 = h_S(X_2) = h_S(X_1 \\oplus \\Delta) = h_S(X_1) \\oplus h_S(\\Delta) = K_1 \\oplus h_S(\\Delta) $$\n这意味着 $K_1$ 和 $K_2$ 通过哈希后的误差向量 $h_S(\\Delta)$ 相关：$K_1 = K_2 \\oplus h_S(\\Delta)$。\n\n2.  **计算条件猜测概率**\n根据问题中的简化，对于给定的 $S=s$ 和 $K_2=k_2$，Eve的最佳猜测是 $K_1=k_2$。我们计算这个猜测的成功概率：\n$$ P(K_1 = k_2 | K_2 = k_2, S = s) $$\n使用上面导出的关系，事件 $K_1 = k_2$ 等价于 $k_2 = k_2 \\oplus h_s(\\Delta)$，这进一步简化为 $h_s(\\Delta) = 0$。\n因此，条件猜测概率变为：\n$$ P(K_1 = k_2 | K_2 = k_2, S = s) = P(h_s(\\Delta) = 0 | K_2=k_2, S=s) $$\n由于误差向量 $\\Delta$ 的生成独立于密钥 $X_1$ 和种子 $S$，所以上述概率简化为 $P(h_s(\\Delta) = 0)$。这个概率仅取决于种子 $s$ 和误差 $\\Delta$ 的分布，而不取决于 $k_2$。\n\n3.  **计算平均猜测概率**\n现在我们计算 $P_{\\text{guess}}(K_1 | K_2, S)$，它是对所有可能的 $(k_2, s)$ 取期望：\n$$ P_{\\text{guess}}(K_1 | K_2, S) = \\mathbb{E}_{(k_2,s)} \\left[ P(h_s(\\Delta) = 0) \\right] $$\n由于内部概率不依赖于 $k_2$，期望可以简化为只对种子 $S$ 取期望：\n$$ P_{\\text{guess}}(K_1 | K_2, S) = \\mathbb{E}_{S} \\left[ P(h_S(\\Delta) = 0) \\right] $$\n现在我们对误差向量 $\\Delta$ 的所有可能值进行求和，并根据其概率加权：\n$$ \\mathbb{E}_{S} \\left[ P(h_S(\\Delta) = 0) \\right] = \\mathbb{E}_S \\left[ \\sum_{\\delta \\in \\{0,1\\}^n} P(\\Delta = \\delta) \\cdot \\mathbf{1}_{\\{h_S(\\delta) = 0\\}} \\right] $$\n其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 是指示函数。由于 $\\Delta$ 和 $S$ 是独立的，我们可以交换期望和求和的顺序：\n$$ = \\sum_{\\delta \\in \\{0,1\\}^n} P(\\Delta = \\delta) \\cdot \\mathbb{E}_S \\left[ \\mathbf{1}_{\\{h_S(\\delta) = 0\\}} \\right] = \\sum_{\\delta \\in \\{0,1\\}^n} P(\\Delta = \\delta) \\cdot P_S(h_S(\\delta) = 0) $$\n\n4.  **利用双重通用性质**\n我们将和式分为两种情况：\n-   当 $\\delta = 0$ (零向量) 时：$P_S(h_S(0) = 0) = 1$，因为 $h_S$ 是线性的。$P(\\Delta = 0) = (1-p)^n$。\n-   当 $\\delta \\neq 0$ 时：根据双重通用性质，$P_S(h_S(\\delta) = 0) = 2^{-m}$。这种情况发生的概率是 $P(\\Delta \\neq 0) = 1 - P(\\Delta=0) = 1 - (1-p)^n$。\n\n将这两部分加起来：\n$$ P_{\\text{guess}} = P(\\Delta=0) \\cdot 1 + \\sum_{\\delta \\neq 0} P(\\Delta = \\delta) \\cdot 2^{-m} $$\n$$ = (1-p)^n + \\left( \\sum_{\\delta \\neq 0} P(\\Delta = \\delta) \\right) \\cdot 2^{-m} $$\n$$ = (1-p)^n + (1 - (1-p)^n) \\cdot 2^{-m} $$\n\n5.  **计算最终最小熵**\n最后，我们根据定义计算最小熵：\n$$ H_{\\min}(K_1 | K_2, S) = -\\log_2(P_{\\text{guess}}(K_1 | K_2, S)) $$\n$$ = -\\log_2 \\left( (1-p)^n + 2^{-m} (1 - (1-p)^n) \\right) $$", "answer": "$$\\boxed{ -\\log_{2} \\left( (1-p)^{n} + 2^{-m} \\cdot \\left(1 - (1-p)^{n}\\right) \\right) }$$", "id": "110739"}]}