{
    "hands_on_practices": [
        {
            "introduction": "The Third Law of Thermodynamics provides a crucial anchor point for entropy, setting its value to zero for a perfect crystal at absolute zero. This practice demonstrates how to build upon this foundation to calculate the absolute molar entropy, $S_m$, of a substance at a standard temperature like $298 \\, \\mathrm{K}$. By systematically integrating heat capacity data ($C_p/T$) and accounting for the entropy changes of phase transitions ($\\Delta H_{\\text{tr}}/T_{\\text{tr}}$), you will trace the accumulation of disorder from $0 \\, \\mathrm{K}$ upwards, transforming abstract theory into a concrete, quantitative skill fundamental to chemical thermodynamics.",
            "id": "2938089",
            "problem": "A hypothetical molecular substance, denoted as Q, is observed at atmospheric pressure to undergo a sequence of equilibrium states from absolute zero to room temperature. Assume the Third Law of Thermodynamics applies without exception to Q, that is, the molar entropy at absolute zero is zero, there is no residual configurational entropy, and all phase transitions occur reversibly at their equilibrium temperatures. Use only fundamental definitions and well-tested relations that follow from reversible transformations to determine the absolute molar entropy at $298\\ \\mathrm{K}$.\n\nThe measured molar heat capacities at constant pressure, $C_p(T)$, for Q are reliably represented by the following expressions over the indicated temperature intervals at $1\\ \\mathrm{bar}$:\n- For the low-temperature solid polymorph $\\alpha$ over $0 \\le T < 100\\ \\mathrm{K}$: $C_{p,\\alpha,s}(T) = \\alpha_s T^{3}$ with $\\alpha_s = 1.00 \\times 10^{-5}\\ \\mathrm{J\\ mol^{-1}\\ K^{-4}}$.\n- At $T = 100\\ \\mathrm{K}$, there is a first-order solidâ€“solid transition $\\alpha \\to \\beta$ with molar enthalpy of transition $\\Delta H_{\\alpha\\to\\beta} = 320\\ \\mathrm{J\\ mol^{-1}}$.\n- For the higher-temperature solid polymorph $\\beta$ over $100 \\le T < 250\\ \\mathrm{K}$: $C_{p,\\beta,s}(T) = a_s + b_s T$ with $a_s = 12.0\\ \\mathrm{J\\ mol^{-1}\\ K^{-1}}$ and $b_s = 0.0300\\ \\mathrm{J\\ mol^{-1}\\ K^{-2}}$.\n- At $T = 250\\ \\mathrm{K}$, Q melts reversibly with molar enthalpy of fusion $\\Delta H_{\\mathrm{fus}} = 6.20 \\times 10^{3}\\ \\mathrm{J\\ mol^{-1}}$.\n- For the liquid over $250 \\le T \\le 298\\ \\mathrm{K}$: $C_{p,l}(T) = c_l + d_l T$ with $c_l = 28.0\\ \\mathrm{J\\ mol^{-1}\\ K^{-1}}$ and $d_l = 0.0200\\ \\mathrm{J\\ mol^{-1}\\ K^{-2}}$.\n\nAll data are given in the International System of Units (SI). Treat $C_p(T)$ as smooth and accurate within the stated ranges, neglect any magnetic or ordering anomalies, and assume equilibrium paths at each step. Compute the absolute molar entropy of Q at $298\\ \\mathrm{K}$.\n\nRound your final answer to four significant figures and express it in $\\mathrm{J\\ mol^{-1}\\ K^{-1}}$.",
            "solution": "The problem requires the calculation of the absolute molar entropy of substance Q at a temperature of $298\\ \\mathrm{K}$. The calculation proceeds from the Third Law of Thermodynamics, which posits that the entropy of a perfect crystalline substance is zero at absolute zero temperature, $S_m(0) = 0$. The absolute entropy at a finite temperature $T$ is then found by integrating the change in entropy from $0\\ \\mathrm{K}$ to $T$.\n\nFor a process at constant pressure, the infinitesimal change in molar entropy $dS_m$ for a reversible temperature change is given by $dS_m = \\frac{C_p(T)}{T} dT$, where $C_p(T)$ is the molar heat capacity at constant pressure. For a reversible phase transition occurring at a constant temperature $T_{tr}$, the change in molar entropy is $\\Delta S_{tr} = \\frac{\\Delta H_{tr}}{T_{tr}}$, where $\\Delta H_{tr}$ is the molar enthalpy of transition.\n\nThe total absolute molar entropy at $298\\ \\mathrm{K}$, $S_m(298\\ \\mathrm{K})$, is the sum of the entropy contributions from heating the substance through its various phases and the entropy changes at each phase transition, starting from $S_m(0) = 0$. The path from $0\\ \\mathrm{K}$ to $298\\ \\mathrm{K}$ is as follows:\n$1$. Heating solid polymorph $\\alpha$ from $0\\ \\mathrm{K}$ to $100\\ \\mathrm{K}$.\n$2$. The solid-solid phase transition $\\alpha \\to \\beta$ at $100\\ \\mathrm{K}$.\n$3$. Heating solid polymorph $\\beta$ from $100\\ \\mathrm{K}$ to $250\\ \\mathrm{K}$.\n$4$. The solid-liquid phase transition (fusion) at $250\\ \\mathrm{K}$.\n$5$. Heating the liquid from $250\\ \\mathrm{K}$ to $298\\ \\mathrm{K}$.\n\nThe total entropy is the sum of the entropy changes for these five steps:\n$$S_m(298\\ \\mathrm{K}) = \\Delta S_1 + \\Delta S_2 + \\Delta S_3 + \\Delta S_4 + \\Delta S_5$$\n\nEach term is calculated as follows:\n\nStep 1: Entropy change from heating solid $\\alpha$ from $T = 0\\ \\mathrm{K}$ to $T = 100\\ \\mathrm{K}$.\nThe heat capacity is given by $C_{p,\\alpha,s}(T) = \\alpha_s T^{3}$, where $\\alpha_s = 1.00 \\times 10^{-5}\\ \\mathrm{J\\ mol^{-1}\\ K^{-4}}$.\n$$\\Delta S_1 = \\int_{0}^{100} \\frac{C_{p,\\alpha,s}(T)}{T} dT = \\int_{0}^{100} \\frac{\\alpha_s T^{3}}{T} dT = \\int_{0}^{100} \\alpha_s T^{2} dT$$\n$$\\Delta S_1 = \\alpha_s \\left[ \\frac{T^3}{3} \\right]_{0}^{100} = (1.00 \\times 10^{-5})\\frac{(100)^3 - 0^3}{3} = \\frac{1.00 \\times 10^{-5} \\times 10^6}{3} = \\frac{10}{3} \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n$$\\Delta S_1 \\approx 3.3333 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nStep 2: Entropy change for the solid-solid transition $\\alpha \\to \\beta$ at $T_{tr} = 100\\ \\mathrm{K}$.\nThe enthalpy of transition is $\\Delta H_{\\alpha\\to\\beta} = 320\\ \\mathrm{J\\ mol^{-1}}$.\n$$\\Delta S_2 = \\frac{\\Delta H_{\\alpha\\to\\beta}}{T_{tr}} = \\frac{320}{100} = 3.20 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nStep 3: Entropy change from heating solid $\\beta$ from $T = 100\\ \\mathrm{K}$ to $T = 250\\ \\mathrm{K}$.\nThe heat capacity is $C_{p,\\beta,s}(T) = a_s + b_s T$, with $a_s = 12.0\\ \\mathrm{J\\ mol^{-1}\\ K^{-1}}$ and $b_s = 0.0300\\ \\mathrm{J\\ mol^{-1}\\ K^{-2}}$.\n$$\\Delta S_3 = \\int_{100}^{250} \\frac{C_{p,\\beta,s}(T)}{T} dT = \\int_{100}^{250} \\frac{a_s + b_s T}{T} dT = \\int_{100}^{250} \\left( \\frac{a_s}{T} + b_s \\right) dT$$\n$$\\Delta S_3 = [a_s \\ln(T) + b_s T]_{100}^{250} = a_s \\ln\\left(\\frac{250}{100}\\right) + b_s(250 - 100)$$\n$$\\Delta S_3 = 12.0 \\ln(2.5) + (0.0300)(150) = 12.0 \\times 0.91629 + 4.5 = 10.9955 + 4.5 = 15.4955 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nStep 4: Entropy change for fusion at $T_{fus} = 250\\ \\mathrm{K}$.\nThe enthalpy of fusion is $\\Delta H_{\\mathrm{fus}} = 6.20 \\times 10^{3}\\ \\mathrm{J\\ mol^{-1}}$.\n$$\\Delta S_4 = \\frac{\\Delta H_{\\mathrm{fus}}}{T_{fus}} = \\frac{6.20 \\times 10^{3}}{250} = 24.8 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nStep 5: Entropy change from heating the liquid from $T = 250\\ \\mathrm{K}$ to $T = 298\\ \\mathrm{K}$.\nThe heat capacity is $C_{p,l}(T) = c_l + d_l T$, with $c_l = 28.0\\ \\mathrm{J\\ mol^{-1}\\ K^{-1}}$ and $d_l = 0.0200\\ \\mathrm{J\\ mol^{-1}\\ K^{-2}}$.\n$$\\Delta S_5 = \\int_{250}^{298} \\frac{C_{p,l}(T)}{T} dT = \\int_{250}^{298} \\frac{c_l + d_l T}{T} dT = \\int_{250}^{298} \\left( \\frac{c_l}{T} + d_l \\right) dT$$\n$$\\Delta S_5 = [c_l \\ln(T) + d_l T]_{250}^{298} = c_l \\ln\\left(\\frac{298}{250}\\right) + d_l(298 - 250)$$\n$$\\Delta S_5 = 28.0 \\ln(1.192) + (0.0200)(48) = 28.0 \\times 0.17565 + 0.96 = 4.9182 + 0.96 = 5.8782 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nTotal absolute molar entropy at $298\\ \\mathrm{K}$:\nSumming the contributions:\n$$S_m(298\\ \\mathrm{K}) = \\Delta S_1 + \\Delta S_2 + \\Delta S_3 + \\Delta S_4 + \\Delta S_5$$\n$$S_m(298\\ \\mathrm{K}) = (3.3333 + 3.20 + 15.4955 + 24.8 + 5.8782) \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n$$S_m(298\\ \\mathrm{K}) = 52.707 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$$\n\nRounding the final answer to four significant figures as required by the problem statement gives $52.71 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$.",
            "answer": "$$\\boxed{52.71}$$"
        },
        {
            "introduction": "One of the most intuitive manifestations of the Second Law is the spontaneous mixing of gases, a process driven not by energy changes but by an increase in entropy. This exercise guides you through a first-principles derivation of the entropy of mixing, $\\Delta S_{\\mathrm{mix}}$, revealing how the increase in available spatial configurations for distinguishable particles drives the process forward. You will not only quantify this change but also confront the famous Gibbs paradox, a thought experiment that arises when the mixing gases are identical, which deepens the understanding of statistical entropy and the crucial quantum mechanical concept of indistinguishability.",
            "id": "2680115",
            "problem": "Two ideal gases, species $A$ and $B$, are contained in a rigid vessel with thermally insulating walls. A thin, impermeable partition divides the vessel into two compartments of volumes $V_{A}^{0}$ and $V_{B}^{0}$. Each compartment contains only one species, with amounts $n_{A} = 1.20$ and $n_{B} = 2.80$ (in moles), respectively. Initially, both compartments are at the same temperature $T$ and pressure $P$, and each compartment satisfies the ideal gas relation at its own $V_{i}^{0}$, $T$, and $P$. The partition is punctured and removed, and the system is allowed to reach internal equilibrium. Starting from fundamental principles, namely the second law of thermodynamics in the form $\\Delta S = \\int \\delta q_{\\mathrm{rev}}/T$ and the defining properties of an ideal gas, derive an expression for the entropy change of the system due to mixing, $\\Delta S_{\\mathrm{mix}}$, and then compute its numerical value for the given amounts. Within the same framework, explain what changes in the derivation when the two gases are microscopically identical (same species) and state the implication for $\\Delta S_{\\mathrm{mix}}$. Round your final numerical answer to four significant figures. Express the final entropy change in joules per kelvin.",
            "solution": "The problem requires the derivation of the entropy of mixing for two ideal gases, a subsequent numerical calculation, and a conceptual analysis of the case where the gases are identical. The derivation must originate from fundamental principles.\n\nThe fundamental thermodynamic relation for a closed system undergoing a reversible process is given by the combined first and second laws of thermodynamics:\n$$dU = \\delta q_{\\mathrm{rev}} - \\delta w_{\\mathrm{rev}} = TdS - PdV$$\nFrom this, the change in entropy $dS$ for a change in state can be expressed as:\n$$dS = \\frac{dU}{T} + \\frac{PdV}{T}$$\nFor an ideal gas, the internal energy $U$ is a function of temperature only, so $dU = C_V dT$, where $C_V$ is the heat capacity at constant volume. The ideal gas equation of state is $PV = nRT$.\n\nThe process described is the removal of a partition between two ideal gases, $A$ and $B$, within a rigid, thermally insulated vessel. This is an irreversible process.\n1. The vessel is rigid, so no work is done on the surroundings: $w = 0$.\n2. The walls are thermally insulating, so no heat is exchanged with the surroundings: $q = 0$.\nAccording to the first law of thermodynamics, the change in the internal energy of the total system is $\\Delta U = q + w = 0$.\nThe total internal energy is the sum of the internal energies of the two gases, $U = U_A + U_B$. Since the gases are ideal, $U_A$ and $U_B$ depend only on their respective temperatures. The total internal energy before mixing is $U_{\\mathrm{initial}} = U_A(T) + U_B(T)$. After mixing, the total internal energy is $U_{\\mathrm{final}}$. Since $\\Delta U = 0$, we have $U_{\\mathrm{final}} = U_{\\mathrm{initial}}$. For ideal gases, this implies that the final equilibrium temperature must be the same as the initial temperature, $T$. Therefore, the mixing process is isothermal.\n\nTo calculate the entropy change for this irreversible process, we must devise a reversible path between the same initial and final states. The initial state consists of gas $A$ in volume $V_{A}^{0}$ and gas $B$ in volume $V_{B}^{0}$, both at temperature $T$. The final state consists of a mixture of gases $A$ and $B$, with each gas occupying the total volume $V_{\\mathrm{total}} = V_{A}^{0} + V_{B}^{0}$ at temperature $T$.\n\nA suitable reversible path treats the expansion of each gas independently. We can imagine gas $A$ isothermally and reversibly expanding from its initial volume $V_{A}^{0}$ to the final volume $V_{\\mathrm{total}}$. Simultaneously, gas $B$ isothermally and reversibly expands from its initial volume $V_{B}^{0}$ to the final volume $V_{\\mathrm{total}}$. The total entropy change, $\\Delta S_{\\mathrm{mix}}$, is the sum of the entropy changes for each gas, $\\Delta S_A$ and $\\Delta S_B$.\n$$\\Delta S_{\\mathrm{mix}} = \\Delta S_A + \\Delta S_B$$\nFor an isothermal process involving an ideal gas, $dT=0$, thus $dU = C_V dT = 0$. The expression for $dS$ simplifies to:\n$$dS = \\frac{P}{T}dV$$\nUsing the ideal gas law, $P/T = nR/V$, we have:\n$$dS = nR \\frac{dV}{V}$$\nIntegrating from an initial volume $V_i$ to a final volume $V_f$ at constant temperature gives the entropy change for the isothermal expansion:\n$$\\Delta S = \\int_{V_i}^{V_f} nR \\frac{dV}{V} = nR \\ln\\left(\\frac{V_f}{V_i}\\right)$$\nWe apply this result to both gases $A$ and $B$.\nFor gas $A$: $\\Delta S_A = n_A R \\ln\\left(\\frac{V_{\\mathrm{total}}}{V_A^0}\\right)$.\nFor gas $B$: $\\Delta S_B = n_B R \\ln\\left(\\frac{V_{\\mathrm{total}}}{V_B^0}\\right)$.\n\nThe problem states that initially, both compartments are at the same temperature $T$ and pressure $P$. Using the ideal gas law:\n$V_{A}^{0} = \\frac{n_A R T}{P}$ and $V_{B}^{0} = \\frac{n_B R T}{P}$.\nThe total volume is $V_{\\mathrm{total}} = V_{A}^{0} + V_{B}^{0} = \\frac{(n_A + n_B)RT}{P}$.\n\nThe volume ratios can be expressed in terms of mole fractions. Let $n_{\\mathrm{total}} = n_A+n_B$. The mole fractions are $x_A = n_A/n_{\\mathrm{total}}$ and $x_B = n_B/n_{\\mathrm{total}}$.\n$$\\frac{V_{\\mathrm{total}}}{V_A^0} = \\frac{(n_A + n_B)RT/P}{n_A RT/P} = \\frac{n_A + n_B}{n_A} = \\frac{1}{x_A}$$\n$$\\frac{V_{\\mathrm{total}}}{V_B^0} = \\frac{(n_A + n_B)RT/P}{n_B RT/P} = \\frac{n_A + n_B}{n_B} = \\frac{1}{x_B}$$\nSubstituting these ratios into the expressions for $\\Delta S_A$ and $\\Delta S_B$:\n$\\Delta S_A = n_A R \\ln(1/x_A) = -n_A R \\ln(x_A)$.\n$\\Delta S_B = n_B R \\ln(1/x_B) = -n_B R \\ln(x_B)$.\n\nThe total entropy of mixing is the sum:\n$$\\Delta S_{\\mathrm{mix}} = \\Delta S_A + \\Delta S_B = -n_A R \\ln(x_A) - n_B R \\ln(x_B)$$\nThis expression can be written in terms of the total number of moles, $n_{\\mathrm{total}}$:\n$$\\Delta S_{\\mathrm{mix}} = -(n_{\\mathrm{total}} x_A) R \\ln(x_A) - (n_{\\mathrm{total}} x_B) R \\ln(x_B) = -n_{\\mathrm{total}} R (x_A \\ln(x_A) + x_B \\ln(x_B))$$\nSince mole fractions $x_i$ are always less than $1$, their logarithms are negative, which ensures that $\\Delta S_{\\mathrm{mix}}$ is positive, consistent with the second law for a spontaneous process.\n\nNow, we compute the numerical value for the given amounts.\nGiven: $n_A = 1.20$ mol and $n_B = 2.80$ mol.\nThe total number of moles is $n_{\\mathrm{total}} = n_A + n_B = 1.20 + 2.80 = 4.00$ mol.\nThe mole fractions are:\n$x_A = \\frac{n_A}{n_{\\mathrm{total}}} = \\frac{1.20}{4.00} = 0.300$.\n$x_B = \\frac{n_B}{n_{\\mathrm{total}}} = \\frac{2.80}{4.00} = 0.700$.\nThe universal gas constant is $R \\approx 8.3145 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$.\nSubstituting these values into the derived expression for $\\Delta S_{\\mathrm{mix}}$:\n$$\\Delta S_{\\mathrm{mix}} = -(4.00 \\, \\mathrm{mol}) \\times (8.3145 \\, \\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}) \\times [0.300 \\ln(0.300) + 0.700 \\ln(0.700)]$$\n$$\\Delta S_{\\mathrm{mix}} = -33.258 \\, \\mathrm{J}\\,\\mathrm{K}^{-1} \\times [0.300 \\times (-1.20397) + 0.700 \\times (-0.35667)]$$\n$$\\Delta S_{\\mathrm{mix}} = -33.258 \\, \\mathrm{J}\\,\\mathrm{K}^{-1} \\times [-0.36119 - 0.24967]$$\n$$\\Delta S_{\\mathrm{mix}} = -33.258 \\, \\mathrm{J}\\,\\mathrm{K}^{-1} \\times [-0.61086]$$\n$$\\Delta S_{\\mathrm{mix}} \\approx 20.3163 \\, \\mathrm{J}\\,\\mathrm{K}^{-1}$$\nRounding to four significant figures as required:\n$$\\Delta S_{\\mathrm{mix}} = 20.32 \\, \\mathrm{J}\\,\\mathrm{K}^{-1}$$\n\nFinally, we consider the case where the two gases are microscopically identical.\nIn this scenario, we are mixing two samples of the same gas, which are initially at the same temperature $T$ and pressure $P$.\nThe initial state consists of $n_A$ moles of a gas in volume $V_A^0$ and $n_B$ moles of the same gas in volume $V_B^0$. Since $T$ and $P$ are the same, the molar volume $V_m = V/n = RT/P$ is the same in both compartments.\nThe final state, after removing the partition, is $n_{\\mathrm{total}} = n_A+n_B$ moles of the gas in a volume $V_{\\mathrm{total}} = V_A^0+V_B^0$. The final molar volume is $V_{\\mathrm{total}}/n_{\\mathrm{total}} = (V_A^0+V_B^0)/(n_A+n_B)$. Since $V_A^0 = n_A V_m$ and $V_B^0 = n_B V_m$, the final molar volume is $(n_A V_m + n_B V_m)/(n_A+n_B) = V_m$.\nThe final temperature is $T$, and the final pressure is $P_{\\mathrm{final}} = n_{\\mathrm{total}}RT/V_{\\mathrm{total}} = n_{\\mathrm{total}}R T / (n_{\\mathrm{total}}V_m) = RT/V_m = P$.\nThus, the final macroscopic state (defined by $T$, $P$, and molar volume $V_m$) is identical to the initial macroscopic state. Removing the partition between two identical gases at the same $T$ and $P$ causes no change in the thermodynamic state of the system.\n\nThe derivation for distinguishable gases is invalid here because it presumes that the particles are identifiable based on their origin (compartment $A$ or $B$). For identical, indistinguishable particles, this distinction is meaningless. We cannot say that the gas from compartment $A$ has \"expanded\" to fill the whole volume, as it is indistinguishable from the gas already present in compartment $B$. The concept of \"mixing\" does not apply.\nSince entropy is a state function, and the initial and final macroscopic states of the system are identical, the change in entropy must be zero.\nThe implication for $\\Delta S_{\\mathrm{mix}}$ is therefore:\n$$\\Delta S_{\\mathrm{mix}} = 0$$\nThis result is known as the Gibbs paradox. The seeming discontinuity (the entropy of mixing does not gradually go to zero as two gases become more and more similar) is resolved by quantum mechanics, which posits that particles of the same species are fundamentally indistinguishable. Within classical thermodynamics, it is taken as a postulate that $\\Delta S_{\\mathrm{mix}} = 0$ for identical gases under these conditions.",
            "answer": "$$\\boxed{20.32}$$"
        },
        {
            "introduction": "The principles of entropy extend beyond equilibrium to describe the dynamics of systems in flux, where processes like heat conduction and mass diffusion are coupled. In this modern, computational exercise, you will step into the role of an experimentalist analyzing data from such non-equilibrium systems using the powerful framework of linear irreversible thermodynamics. This practice involves using numerical methods to estimate the phenomenological matrix $\\mathbf{L}$ that links thermodynamic forces $\\mathbf{X}$ and fluxes $\\mathbf{J}$, and most importantly, to computationally verify that the experimental data is consistent with the Second Law's mandate of non-negative entropy production, $\\sigma \\ge 0$.",
            "id": "2938122",
            "problem": "You are given data from multiple steady-state, near-equilibrium experiments on coupled transport processes in a fluid mixture. Each experiment provides a vector of thermodynamic forces and the corresponding measured vector of fluxes. In the linear irreversible regime, for a system with $m$ coupled processes, the $m$-component flux vector $\\mathbf{J}$ and the $m$-component force vector $\\mathbf{X}$ satisfy the linear relation $\\mathbf{J} = \\mathbf{L}\\,\\mathbf{X}$, where $\\mathbf{L} \\in \\mathbb{R}^{m \\times m}$ is the unknown phenomenological matrix. The entropy production rate per unit volume is $\\sigma = \\mathbf{J}^{\\mathsf{T}} \\mathbf{X}$. The Second Law of Thermodynamics requires $\\sigma \\ge 0$ for all admissible $\\mathbf{X}$ in the linear regime. Under microscopic reversibility, the Onsager reciprocal relations imply that $\\mathbf{L}$ is symmetric, and $\\sigma \\ge 0$ for all $\\mathbf{X}$ implies that $\\mathbf{L}$ is positive semidefinite.\n\nYour task is to design a method that, given a set of measured $\\{ (\\mathbf{X}_k, \\mathbf{J}_k) \\}_{k=1}^N$, estimates $\\mathbf{L}$ in the linear model $\\mathbf{J}_k \\approx \\mathbf{L} \\mathbf{X}_k$ by minimizing the sum of squared residuals $\\sum_{k=1}^N \\lVert \\mathbf{J}_k - \\mathbf{L} \\mathbf{X}_k \\rVert_2^2$. Use the estimated $\\widehat{\\mathbf{L}}$ to produce its symmetric part $\\widehat{\\mathbf{L}}_{\\mathrm{sym}} = \\tfrac{1}{2}(\\widehat{\\mathbf{L}} + \\widehat{\\mathbf{L}}^{\\mathsf{T}})$, and then verify the Second Law in two ways:\n\n1. Verify that $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$ is positive semidefinite by checking that all its eigenvalues are nonnegative within a fixed numerical tolerance $\\tau = 10^{-9}$.\n2. For the given forces $\\{\\mathbf{X}_k\\}$, compute $\\sigma_k = \\mathbf{X}_k^{\\mathsf{T}} \\widehat{\\mathbf{L}}_{\\mathrm{sym}} \\mathbf{X}_k$ and verify that each $\\sigma_k \\ge -\\tau$.\n\nFor each dataset, output a boolean value that is $true$ if and only if both verifications succeed, and $false$ otherwise.\n\nThe input datasets are embedded and fixed. For each dataset, define an $N \\times m$ matrix $\\mathbf{X}$ with rows $\\mathbf{X}_k^{\\mathsf{T}}$ and an $N \\times m$ matrix $\\mathbf{J}$ with rows $\\mathbf{J}_k^{\\mathsf{T}}$. Use the following four datasets (test suite), covering a happy-path case, a boundary case with a zero-force experiment and collinearity, a noisy case, and a violation case:\n\n- Dataset A (happy path, $m = 2$, $N = 4$):\n  - Forces $\\mathbf{X}$:\n    - Row $1$: $[\\,1.0,\\,-0.5\\,]$\n    - Row $2$: $[\\,0.2,\\,0.8\\,]$\n    - Row $3$: $[\\,-1.5,\\,0.7\\,]$\n    - Row $4$: $[\\,0.0,\\,1.2\\,]$\n  - Fluxes $\\mathbf{J}$:\n    - Row $1$: $[\\,1.85,\\,-0.2\\,]$\n    - Row $2$: $[\\,0.64,\\,0.86\\,]$\n    - Row $3$: $[\\,-2.79,\\,0.25\\,]$\n    - Row $4$: $[\\,0.36,\\,1.2\\,]$\n\n- Dataset B (boundary, $m = 3$, $N = 6$):\n  - Forces $\\mathbf{X}$:\n    - Row $1$: $[\\,0.0,\\,0.0,\\,0.0\\,]$\n    - Row $2$: $[\\,1.0,\\,2.0,\\,-1.0\\,]$\n    - Row $3$: $[\\,0.5,\\,1.0,\\,-0.5\\,]$\n    - Row $4$: $[\\,0.0,\\,1.0,\\,1.0\\,]$\n    - Row $5$: $[\\,1.0,\\,0.0,\\,1.0\\,]$\n    - Row $6$: $[\\,1.0,\\,1.0,\\,0.0\\,]$\n  - Fluxes $\\mathbf{J}$:\n    - Row $1$: $[\\,0.0,\\,0.0,\\,0.0\\,]$\n    - Row $2$: $[\\,1.5,\\,2.05,\\,-0.5\\,]$\n    - Row $3$: $[\\,0.75,\\,1.025,\\,-0.25\\,]$\n    - Row $4$: $[\\,0.3,\\,1.05,\\,0.85\\,]$\n    - Row $5$: $[\\,1.7,\\,0.15,\\,1.0\\,]$\n    - Row $6$: $[\\,1.6,\\,1.1,\\,0.25\\,]$\n\n- Dataset C (noisy, $m = 2$, $N = 5$):\n  - Forces $\\mathbf{X}$:\n    - Row $1$: $[\\,0.3,\\,-0.7\\,]$\n    - Row $2$: $[\\,-1.1,\\,0.5\\,]$\n    - Row $3$: $[\\,0.0,\\,-0.9\\,]$\n    - Row $4$: $[\\,1.5,\\,1.2\\,]$\n    - Row $5$: $[\\,-0.4,\\,0.2\\,]$\n  - Fluxes $\\mathbf{J}$:\n    - Row $1$: $[\\,0.02001,\\,-0.72002\\,]$\n    - Row $2$: $[\\,-0.900015,\\,0.16001\\,]$\n    - Row $3$: $[\\, -0.35998,\\,-1.08001\\,]$\n    - Row $4$: $[\\,1.97999,\\,2.040015\\,]$\n    - Row $5$: $[\\,-0.32,\\,0.07999\\,]$\n\n- Dataset D (violation, $m = 2$, $N = 3$):\n  - Forces $\\mathbf{X}$:\n    - Row $1$: $[\\,1.0,\\,0.0\\,]$\n    - Row $2$: $[\\,0.0,\\,1.0\\,]$\n    - Row $3$: $[\\,1.0,\\,-1.0\\,]$\n  - Fluxes $\\mathbf{J}$:\n    - Row $1$: $[\\,0.5,\\,1.0\\,]$\n    - Row $2$: $[\\,1.0,\\,0.2\\,]$\n    - Row $3$: $[\\,-0.5,\\,0.8\\,]$\n\nImplementation requirements:\n\n- Use the principle-based approach described above to estimate $\\widehat{\\mathbf{L}}$ by minimizing the sum of squared residuals over $\\mathbf{L}$, then enforce reciprocity by symmetrizing to $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$, and verify positive semidefiniteness and nonnegativity of $\\sigma_k$ within tolerance $\\tau = 10^{-9}$.\n- For each dataset, return a boolean that is $true$ if and only if the estimated $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$ is positive semidefinite and all computed $\\sigma_k$ are nonnegative within the tolerance.\n- The final output of your program must be a single line containing the results for the four datasets in order A, B, C, D, as a comma-separated list enclosed in square brackets, for example $[\\,\\text{true},\\text{false},\\text{true},\\text{true}\\,]$. Use Python boolean literals, that is, output must be of the form $[\\,\\text{True},\\text{False},\\dots\\,]$.\n\nYour program must be self-contained and must not read any input. There are no units required for the boolean outputs.",
            "solution": "The problem presented is a valid and well-posed exercise in the application of non-equilibrium thermodynamics and linear algebra to experimental data. It is scientifically grounded in the principles of linear irreversible thermodynamics, namely the linear flux-force relationship and the consequences of the Second Law of Thermodynamics combined with Onsager's reciprocal relations. The instructions are clear, the data is provided, and the objective is unambiguous. We shall proceed with a formal solution.\n\nThe fundamental model for coupled transport processes in the linear regime close to equilibrium is given by the linear relation between thermodynamic fluxes $\\mathbf{J}$ and forces $\\mathbf{X}$:\n$$ \\mathbf{J} = \\mathbf{L} \\mathbf{X} $$\nwhere $\\mathbf{J}, \\mathbf{X} \\in \\mathbb{R}^m$ are $m$-component column vectors and $\\mathbf{L} \\in \\mathbb{R}^{m \\times m}$ is the matrix of phenomenological coefficients.\n\nWe are given a set of $N$ experimental measurements $\\{(\\mathbf{X}_k, \\mathbf{J}_k)\\}_{k=1}^N$. Our first task is to estimate the matrix $\\mathbf{L}$ from this data. This is a classic multivariate linear regression problem. We can arrange the data into two matrices: an $N \\times m$ force matrix $\\mathbf{X}_{\\text{data}}$ where the $k$-th row is $\\mathbf{X}_k^{\\mathsf{T}}$, and an $N \\times m$ flux matrix $\\mathbf{J}_{\\text{data}}$ where the $k$-th row is $\\mathbf{J}_k^{\\mathsf{T}}$. The system of equations can be written as:\n$$ \\mathbf{J}_{\\text{data}} = \\mathbf{X}_{\\text{data}} \\mathbf{L}^{\\mathsf{T}} $$\nWe seek the matrix $\\widehat{\\mathbf{L}}$ that minimizes the sum of squared residuals, which is equivalent to minimizing the squared Frobenius norm of the residual matrix:\n$$ S(\\mathbf{L}) = \\sum_{k=1}^N \\lVert \\mathbf{J}_k - \\mathbf{L} \\mathbf{X}_k \\rVert_2^2 = \\lVert \\mathbf{J}_{\\text{data}} - \\mathbf{X}_{\\text{data}} \\mathbf{L}^{\\mathsf{T}} \\rVert_F^2 $$\nThe solution to this linear least-squares problem, which we denote $\\widehat{\\mathbf{L}}^{\\mathsf{T}}$, is found by solving the normal equations. A robust numerical approach is to use a method such as QR decomposition, as implemented in standard numerical libraries. We solve for $\\widehat{\\mathbf{L}}^{\\mathsf{T}}$ in the system $\\mathbf{X}_{\\text{data}} \\widehat{\\mathbf{L}}^{\\mathsf{T}} \\approx \\mathbf{J}_{\\text{data}}$. Transposing the result gives the estimated phenomenological matrix $\\widehat{\\mathbf{L}}$.\n\nThe principle of microscopic reversibility implies the Onsager reciprocal relations, which state that the phenomenological matrix $\\mathbf{L}$ must be symmetric, i.e., $\\mathbf{L} = \\mathbf{L}^{\\mathsf{T}}$. The least-squares estimate $\\widehat{\\mathbf{L}}$ may not be perfectly symmetric due to experimental noise. The prescribed procedure is to enforce this physical constraint by taking the symmetric part of the estimate:\n$$ \\widehat{\\mathbf{L}}_{\\mathrm{sym}} = \\frac{1}{2}(\\widehat{\\mathbf{L}} + \\widehat{\\mathbf{L}}^{\\mathsf{T}}) $$\nThis matrix, $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$, is our final estimate for the true phenomenological matrix.\n\nThe Second Law of Thermodynamics requires that the rate of entropy production per unit volume, $\\sigma$, must be non-negative. In the linear regime, $\\sigma$ is given by:\n$$ \\sigma = \\mathbf{J}^{\\mathsf{T}} \\mathbf{X} = (\\mathbf{L}\\mathbf{X})^{\\mathsf{T}}\\mathbf{X} = \\mathbf{X}^{\\mathsf{T}}\\mathbf{L}^{\\mathsf{T}}\\mathbf{X} $$\nWhen $\\mathbf{L}$ is symmetric, this becomes $\\sigma = \\mathbf{X}^{\\mathsf{T}}\\mathbf{L}\\mathbf{X}$. The condition $\\sigma \\ge 0$ for all possible forces $\\mathbf{X}$ implies that the symmetric matrix $\\mathbf{L}$ must be positive semidefinite.\n\nWe must verify this condition for our estimated matrix $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$ in two ways:\n\n1.  A matrix is positive semidefinite if and only if all its eigenvalues are non-negative. We compute the eigenvalues, $\\lambda_i$, of $\\widehat{\\mathbf{L}}_{\\mathrm{sym}}$ and verify that $\\lambda_i \\ge -\\tau$ for all $i$, where $\\tau = 10^{-9}$ is the specified numerical tolerance.\n\n2.  We compute the entropy production rate for each specific experiment using our estimated matrix: $\\sigma_k = \\mathbf{X}_k^{\\mathsf{T}} \\widehat{\\mathbf{L}}_{\\mathrm{sym}} \\mathbf{X}_k$. We then verify that $\\sigma_k \\ge -\\tau$ for all $k = 1, \\dots, N$.\n\nFor each dataset, the final result is `True` if and only if both verification conditions are satisfied. Otherwise, the result is `False`. The procedure will now be applied to each of the four datasets provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all datasets and print the final results.\n    \"\"\"\n\n    # --- Dataset Definitions ---\n\n    # Dataset A (happy path, m=2, N=4)\n    X_A = np.array([\n        [1.0, -0.5],\n        [0.2, 0.8],\n        [-1.5, 0.7],\n        [0.0, 1.2]\n    ])\n    J_A = np.array([\n        [1.85, -0.2],\n        [0.64, 0.86],\n        [-2.79, 0.25],\n        [0.36, 1.2]\n    ])\n\n    # Dataset B (boundary, m=3, N=6)\n    X_B = np.array([\n        [0.0, 0.0, 0.0],\n        [1.0, 2.0, -1.0],\n        [0.5, 1.0, -0.5],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0]\n    ])\n    J_B = np.array([\n        [0.0, 0.0, 0.0],\n        [1.5, 2.05, -0.5],\n        [0.75, 1.025, -0.25],\n        [0.3, 1.05, 0.85],\n        [1.7, 0.15, 1.0],\n        [1.6, 1.1, 0.25]\n    ])\n\n    # Dataset C (noisy, m=2, N=5)\n    X_C = np.array([\n        [0.3, -0.7],\n        [-1.1, 0.5],\n        [0.0, -0.9],\n        [1.5, 1.2],\n        [-0.4, 0.2]\n    ])\n    J_C = np.array([\n        [0.02001, -0.72002],\n        [-0.900015, 0.16001],\n        [-0.35998, -1.08001],\n        [1.97999, 2.040015],\n        [-0.32, 0.07999]\n    ])\n\n    # Dataset D (violation, m=2, N=3)\n    X_D = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, -1.0]\n    ])\n    J_D = np.array([\n        [0.5, 1.0],\n        [1.0, 0.2],\n        [-0.5, 0.8]\n    ])\n\n    test_cases = [\n        (X_A, J_A),\n        (X_B, J_B),\n        (X_C, J_C),\n        (X_D, J_D)\n    ]\n    \n    tolerance = 1e-9\n    results = []\n\n    for X_data, J_data in test_cases:\n        result = verify_second_law(X_data, J_data, tolerance)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef verify_second_law(X, J, tau):\n    \"\"\"\n    Estimates the phenomenological matrix and verifies the Second Law.\n\n    Args:\n        X (np.ndarray): The N x m matrix of thermodynamic forces.\n        J (np.ndarray): The N x m matrix of thermodynamic fluxes.\n        tau (float): The numerical tolerance for checks.\n\n    Returns:\n        bool: True if both verification conditions are met, False otherwise.\n    \"\"\"\n    # Step 1: Estimate L by solving the least-squares problem J = X @ L_T\n    # np.linalg.lstsq solves for x in a @ x = b. Here, a=X, b=J, x=L_T\n    L_T, _, _, _ = np.linalg.lstsq(X, J, rcond=None)\n    L_hat = L_T.T\n\n    # Step 2: Enforce Onsager reciprocity by symmetrizing L_hat\n    L_sym = 0.5 * (L_hat + L_hat.T)\n\n    # Step 3a: Verify positive semidefiniteness of L_sym\n    # A real symmetric matrix is positive semidefinite iff all its eigenvalues are non-negative.\n    # We use eigvalsh for hermitian (or real symmetric) matrices.\n    eigenvalues = np.linalg.eigvalsh(L_sym)\n    is_positive_semidefinite = np.all(eigenvalues >= -tau)\n\n    # Step 3b: Verify non-negative entropy production for each experiment.\n    # sigma_k = X_k^T @ L_sym @ X_k\n    # This can be computed for all k efficiently.\n    # (X @ L_sym) * X gives element-wise product. Sum along axis 1.\n    sigmas = np.sum((X @ L_sym) * X, axis=1)\n    is_sigma_non_negative = np.all(sigmas >= -tau)\n\n    # The final result is true if and only if both conditions are met.\n    return is_positive_semidefinite and is_sigma_non_negative\n\nsolve()\n```"
        }
    ]
}