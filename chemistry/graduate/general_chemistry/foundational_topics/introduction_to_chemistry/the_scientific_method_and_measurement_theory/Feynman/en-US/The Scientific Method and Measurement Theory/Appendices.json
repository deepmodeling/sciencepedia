{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the scientific method is the principle of dimensional homogeneity, which states that any physically meaningful equation must have consistent dimensions. This exercise grounds your understanding of this principle by applying it to a common power-law model in chemical kinetics. By deriving the units of a rate constant from first principles, you will practice a fundamental skill required to construct and validate any empirical law in the physical sciences .",
            "id": "2961548",
            "problem": "A core requirement in measurement theory and the scientific method is dimensional homogeneity: any proposed empirical law must be dimensionally consistent to be testable and falsifiable by measurement. Consider a homogeneous liquid-phase reaction studied in a well-mixed, isothermal batch reactor. The rate of formation of a product species is modeled by a power-law rate expression of the form $r = k[A]^{m}[B]^{n}$, where $r$ denotes an experimentally measured reaction rate, $k$ is an empirical rate constant, $[A]$ and $[B]$ are molar concentrations of reactants $A$ and $B$, and $m$ and $n$ are reaction orders determined by regression of rate data. The rate $r$ is operationally defined as the time derivative of the amount of product per unit volume, and is reported in the International System of Units (SI) as $\\mathrm{mol\\,m^{-3}\\,s^{-1}}$. Concentrations are reported in SI as $\\mathrm{mol\\,m^{-3}}$. Assume $m$ and $n$ are dimensionless.\n\nUsing only the base SI dimensions of amount of substance, length, and time, and enforcing dimensional homogeneity, derive symbolically the SI units of $k$ as a function of $m$ and $n$. Your derivation must start from the definitions of rate and concentration and the requirement that both sides of the rate law share identical dimensions.\n\nAnswer specification:\n- Express your final result as the ordered triple of exponents on the base units $\\mathrm{mol}$, $\\mathrm{m}$, and $\\mathrm{s}$, respectively, corresponding to the SI units of $k$. That is, if $[k] = \\mathrm{mol}^{\\alpha}\\,\\mathrm{m}^{\\beta}\\,\\mathrm{s}^{\\gamma}$, report the row vector $\\left(\\alpha\\ \\beta\\ \\gamma\\right)$.\n- The final answer must be a single closed-form analytic expression. Do not include units in your final boxed answer.",
            "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of dimensional analysis as applied to chemical kinetics, it is well-posed, objective, and contains all necessary information for a unique solution. The problem is a standard exercise in applying the principle of dimensional homogeneity.\n\nThe fundamental principle to be applied is that of dimensional homogeneity, which dictates that for any physically meaningful equation, the dimensions of the terms on both sides of the equality must be identical. The given rate law is:\n$$r = k[A]^{m}[B]^{n}$$\nTo analyze the dimensions, we denote the dimensions of a physical quantity $X$ as $[X]$. Applying this to the rate law, we have:\n$$[r] = [k[A]^{m}[B]^{n}]$$\nUsing the property that the dimensions of a product are the product of the dimensions, we can separate the terms:\n$$[r] = [k] [A]^{m} [B]^{n}$$\nThe base SI dimensions relevant to this problem are amount of substance ($\\mathrm{mol}$), length ($\\mathrm{m}$), and time ($\\mathrm{s}$). For dimensional analysis, we can represent these base units with the symbols $N$ (amount of substance), $L$ (length), and $T$ (time).\n\nThe problem provides the SI units for the rate, $r$, and concentrations, $[A]$ and $[B]$. We translate these into their fundamental dimensions:\nThe dimension of the reaction rate, $r$, is given as $\\mathrm{mol\\,m^{-3}\\,s^{-1}}$. In terms of our base dimensions, this is:\n$$[r] = N L^{-3} T^{-1}$$\nThe dimension of molar concentration, for both $[A]$ and $[B]$, is given as $\\mathrm{mol\\,m^{-3}}$. In terms of our base dimensions, this is:\n$$[A] = [B] = N L^{-3}$$\nThe reaction orders, $m$ and $n$, are stated to be dimensionless numbers. Therefore, they do not contribute to the dimensional equation themselves, but only act as exponents on the dimensions of the concentrations.\n\nWe now substitute these dimensional expressions back into the primary dimensional equation:\n$$N^{1} L^{-3} T^{-1} = [k] (N L^{-3})^{m} (N L^{-3})^{n}$$\nThe right-hand side can be simplified by combining the concentration terms using the law of exponents:\n$$(N L^{-3})^{m} (N L^{-3})^{n} = (N L^{-3})^{m+n} = N^{m+n} (L^{-3})^{m+n} = N^{m+n} L^{-3(m+n)}$$\nThe dimensional equation now becomes:\n$$N^{1} L^{-3} T^{-1} = [k] N^{m+n} L^{-3(m+n)}$$\nTo find the dimensions of the rate constant, $k$, we must isolate $[k]$ by dividing both sides of the equation by the dimensional group for the concentration terms:\n$$[k] = \\frac{N^{1} L^{-3} T^{-1}}{N^{m+n} L^{-3(m+n)}}$$\nBy applying the rules for division of exponents ($x^{a}/x^{b} = x^{a-b}$), we can determine the net exponent for each base dimension:\nFor the dimension of amount of substance, $N$:\n$$\\text{Exponent} = 1 - (m+n) = 1 - m - n$$\nFor the dimension of length, $L$:\n$$\\text{Exponent} = -3 - (-3(m+n)) = -3 + 3m + 3n = 3(m+n-1)$$\nFor the dimension of time, $T$:\n$$\\text{Exponent} = -1$$\nCombining these results, the dimensions of the rate constant $k$ are:\n$$[k] = N^{1-m-n} L^{3(m+n-1)} T^{-1}$$\nThe problem asks for the result to be expressed as an ordered triple of exponents $(\\alpha, \\beta, \\gamma)$ corresponding to the units $\\mathrm{mol}^{\\alpha}\\,\\mathrm{m}^{\\beta}\\,\\mathrm{s}^{\\gamma}$. By direct comparison with our derived dimensional expression, we identify:\n$$\\alpha = 1 - m - n$$\n$$\\beta = 3(m+n-1)$$\n$$\\gamma = -1$$\nThis result provides the exponents for the SI units of the rate constant $k$ as a function of the reaction orders $m$ and $n$, consistent with the principle of dimensional homogeneity. The requested format is a row vector of these exponents.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - m - n & 3(m+n-1) & -1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While a model must be dimensionally consistent, it must also agree with experimental observations. This practice introduces the statistical lack-of-fit test, a rigorous method for determining if a proposed model, such as the Nernst equation for an electrode, accurately describes the data. You will learn to partition experimental variability into random measurement error and systematic model deviation, a critical skill for validating the functional form of any scientific model .",
            "id": "2961596",
            "problem": "An ion-selective electrode (monovalent cation, charge number $z=+1$) is calibrated in aqueous standards of ionic activity $a$ at temperature $T$ that is effectively constant. The Nernst relation for the mean electrode potential $E$ is the fundamental starting point:\n$$\nE \\;=\\; E^{\\circ} \\;+\\; \\frac{RT}{zF}\\,\\ln a \\;=\\; \\beta_0 \\;+\\; \\beta_1\\,x,\n$$\nwhere $R$ is the gas constant, $F$ is the Faraday constant, and $x=\\log_{10} a$ with $\\beta_1=\\left(\\frac{2.303\\,RT}{zF}\\right)$ in units of millivolts per decade when $E$ is reported in millivolts. Under ideal Nernstian behavior, the mean function $E(x)$ is linear in $x$ with slope fixed by thermodynamics, while realized measurements scatter about the line due to random error.\n\nTo assess model adequacy from first principles of statistical calibration and measurement theory, a residual-based lack-of-fit test partitions the total residual variability into a pure (replicate) error component and a systematic component attributable to model misspecification. Consider $k=5$ distinct standards with equal replicate count $r=3$ at each $x_i\\in\\{-5,-4,-3,-2,-1\\}$, corresponding to $a\\in\\{10^{-5},10^{-4},10^{-3},10^{-2},10^{-1}\\}$. The replicate sample means $\\bar y_i$ (in millivolts) and within-level sample variances $s_i^2$ (in millivolts squared) are:\n- $x_1=-5$: $\\bar y_1=254.2$, $s_1^2=1.21$,\n- $x_2=-4$: $\\bar y_2=295.36$, $s_2^2=0.81$,\n- $x_3=-3$: $\\bar y_3=340.52$, $s_3^2=0.64$,\n- $x_4=-2$: $\\bar y_4=389.68$, $s_4^2=0.64$,\n- $x_5=-1$: $\\bar y_5=442.84$, $s_5^2=0.81$.\n\nYou are to design and execute a residual-based lack-of-fit test for the Nernstian linear-in-$\\log_{10} a$ mean function using these replicated standards. Use only the following base definitions and laws:\n- The Nernst relation implies a linear mean function $E=\\beta_0+\\beta_1 x$ in the predictor $x=\\log_{10} a$ under ideal behavior.\n- An ordinary least squares fit of the linear mean function minimizes $\\sum_{i=1}^{k}\\sum_{j=1}^{r} \\big(y_{ij}-\\hat \\beta_0 - \\hat \\beta_1 x_i\\big)^2$.\n- For replicated $x_i$, the total residual sum of squares can be decomposed as\n$$\n\\mathrm{SSE} \\;=\\; \\underbrace{\\sum_{i=1}^{k}\\sum_{j=1}^{r}\\big(y_{ij}-\\bar y_i\\big)^2}_{\\text{pure error sum of squares, }SS_{\\mathrm{PE}}}\n\\;+\\;\n\\underbrace{\\sum_{i=1}^{k} r\\,\\big(\\bar y_i - \\hat y_i\\big)^2}_{\\text{lack-of-fit sum of squares, }SS_{\\mathrm{LOF}}},\n$$\nwhere $\\hat y_i=\\hat \\beta_0+\\hat \\beta_1 x_i$.\n- The lack-of-fit $F$-statistic is\n$$\nF \\;=\\; \\frac{SS_{\\mathrm{LOF}}/(k-p)}{SS_{\\mathrm{PE}}/(N-k)},\n$$\nwith $p=2$ parameters in the linear mean function, $N=kr$ total observations, and independent, identically distributed normal errors under the null hypothesis that the linear mean function is correct.\n\nTasks:\n- Fit the linear relation $E=\\beta_0+\\beta_1 x$ to the replicated data by first principles, obtaining $\\hat \\beta_0$ and $\\hat \\beta_1$ from the $k$ distinct $x_i$ and $\\bar y_i$ (you may use the fact that all $r$ are equal).\n- Compute $SS_{\\mathrm{PE}}$, $SS_{\\mathrm{LOF}}$, the corresponding degrees of freedom, and the lack-of-fit $F$-statistic and its qualitative $p$-value magnitude.\n- Interpret the pattern of the mean-residuals $\\bar y_i-\\hat y_i$ across $x$ in the context of Nernstian versus non-Nernstian behavior, making explicit reference to what pattern would be expected under ideality.\n\nWhich option best reflects the correct test outcome and interpretation?\n\nA. The linear Nernstian mean function is adequate: the lack-of-fit test is not significant (e.g., $p>0.05$), and the residuals versus $x$ show no systematic pattern.\n\nB. There is a highly significant lack-of-fit (e.g., $p\\ll 0.001$), with a symmetric, curved residual pattern (positive at extreme $x$, negative near the center) consistent with systematic non-Nernstian behavior such as activity-coefficient or junction-potential induced curvature; a higher-order or extended model is warranted.\n\nC. The apparent lack-of-fit arises from heteroscedastic noise (variance increasing with $|x|$) rather than mean-function misspecification; a variance-stabilizing transformation of $E$ removes the issue without changing the mean model.\n\nD. The lack-of-fit is driven by outliers at a single concentration level; exclusion of those replicates restores adequacy of the linear Nernstian mean without evidence of curvature.",
            "solution": "We proceed from first principles of the Nernst relation and the statistical definitions of residuals and sums of squares.\n\nStep 1: Mean function implied by Nernst. For a monovalent ion at fixed $T$, the Nernst equation implies a linear mean function in the predictor $x=\\log_{10} a$,\n$$\nE \\;=\\; \\beta_0 + \\beta_1 x,\n$$\nwith $\\beta_1=\\left(\\frac{2.303\\,RT}{F}\\right)$ under ideal Nernstian behavior. The adequacy question is whether the observed mean response across replicated standards is compatible with this linear mean function.\n\nStep 2: Fit the linear mean function to the $k=5$ distinct $x_i$ values using the $k$ cell means $\\bar y_i$ (this is justified because all $r=3$ are equal; the ordinary least squares estimate using all $N=kr=15$ points equals the weighted least squares fit to the $k$ means with equal weights). Let $\\bar x = \\frac{1}{k}\\sum_{i=1}^{k} x_i$ and $\\bar{\\bar y} = \\frac{1}{k}\\sum_{i=1}^{k} \\bar y_i$.\n\nCompute $\\bar x$:\n$$\n\\bar x \\;=\\; \\frac{(-5)+(-4)+(-3)+(-2)+(-1)}{5} \\;=\\; \\frac{-15}{5} \\;=\\; -3.\n$$\n\nCompute $\\bar{\\bar y}$:\n$$\n\\bar{\\bar y} \\;=\\; \\frac{254.2+295.36+340.52+389.68+442.84}{5} \\;=\\; \\frac{1722.6}{5} \\;=\\; 344.52.\n$$\n\nCompute the slope using $\\hat \\beta_1=\\dfrac{\\sum_{i=1}^{k} (x_i-\\bar x)(\\bar y_i-\\bar{\\bar y})}{\\sum_{i=1}^{k} (x_i-\\bar x)^2}$.\n\nFirst, $(x_i-\\bar x)$ are $\\{-2,-1,0,1,2\\}$, so $\\sum_{i=1}^{k} (x_i-\\bar x)^2 = 4+1+0+1+4 = 10$.\n\nNext, compute $(\\bar y_i-\\bar{\\bar y})$:\n- For $x=-5$: $254.2-344.52=-90.32$.\n- For $x=-4$: $295.36-344.52=-49.16$.\n- For $x=-3$: $340.52-344.52=-4.00$.\n- For $x=-2$: $389.68-344.52=45.16$.\n- For $x=-1$: $442.84-344.52=98.32$.\n\nThen $\\sum_{i=1}^{k} (x_i-\\bar x)(\\bar y_i-\\bar{\\bar y})$ equals\n$$\n(-2)(-90.32) + (-1)(-49.16) + 0(-4.00) + (1)(45.16) + (2)(98.32) \\;=\\; 180.64+49.16+0+45.16+196.64 \\;=\\; 471.6.\n$$\n\nTherefore,\n$$\n\\hat \\beta_1 \\;=\\; \\frac{471.6}{10} \\;=\\; 47.16,\n\\qquad\n\\hat \\beta_0 \\;=\\; \\bar{\\bar y} - \\hat \\beta_1 \\bar x \\;=\\; 344.52 - 47.16(-3) \\;=\\; 344.52 + 141.48 \\;=\\; 486.00.\n$$\n\nThe fitted mean function is thus\n$$\n\\hat y(x) \\;=\\; 486.00 + 47.16\\,x \\quad \\text{(millivolts)}.\n$$\n\nNote that the estimated slope $\\hat \\beta_1=47.16$ millivolts per decade is substantially below the ideal Nernstian value at typical laboratory temperatures (e.g., near $59$ millivolts per decade at $T\\approx 298\\,\\mathrm{K}$), which already suggests potential non-Nernstian behavior; however, the formal adequacy test uses replicated residuals.\n\nStep 3: Pure error sum of squares. By definition,\n$$\nSS_{\\mathrm{PE}} \\;=\\; \\sum_{i=1}^{k}\\sum_{j=1}^{r} (y_{ij}-\\bar y_i)^2.\n$$\nGiven the within-level sample variances $s_i^2$ computed from $r=3$ replicates, the identity $\\sum_{j=1}^{r} (y_{ij}-\\bar y_i)^2 = (r-1)s_i^2$ yields\n$$\nSS_{\\mathrm{PE}} \\;=\\; \\sum_{i=1}^{5} (r-1)\\,s_i^2 \\;=\\; 2\\,(1.21+0.81+0.64+0.64+0.81) \\;=\\; 2\\,(4.11) \\;=\\; 8.22.\n$$\nThe pure error degrees of freedom are $df_{\\mathrm{PE}}=N-k=15-5=10$, so\n$$\nMS_{\\mathrm{PE}} \\;=\\; \\frac{SS_{\\mathrm{PE}}}{df_{\\mathrm{PE}}} \\;=\\; \\frac{8.22}{10} \\;=\\; 0.822.\n$$\n\nStep 4: Lack-of-fit sum of squares. By definition,\n$$\nSS_{\\mathrm{LOF}} \\;=\\; \\sum_{i=1}^{k} r\\,(\\bar y_i - \\hat y_i)^2,\n$$\nwith $\\hat y_i=\\hat y(x_i)$. Compute $\\hat y_i$ and the mean residuals:\n- For $x=-5$: $\\hat y_1 = 486.00 + 47.16(-5) = 486.00 - 235.80 = 250.20$, so $\\bar y_1-\\hat y_1=254.20-250.20=4.00$.\n- For $x=-4$: $\\hat y_2 = 486.00 + 47.16(-4) = 486.00 - 188.64 = 297.36$, so $\\bar y_2-\\hat y_2=295.36-297.36=-2.00$.\n- For $x=-3$: $\\hat y_3 = 486.00 + 47.16(-3) = 486.00 - 141.48 = 344.52$, so $\\bar y_3-\\hat y_3=340.52-344.52=-4.00$.\n- For $x=-2$: $\\hat y_4 = 486.00 + 47.16(-2) = 486.00 - 94.32 = 391.68$, so $\\bar y_4-\\hat y_4=389.68-391.68=-2.00$.\n- For $x=-1$: $\\hat y_5 = 486.00 + 47.16(-1) = 486.00 - 47.16 = 438.84$, so $\\bar y_5-\\hat y_5=442.84-438.84=4.00$.\n\nThus,\n$$\nSS_{\\mathrm{LOF}} \\;=\\; 3\\left(4.00^2 + (-2.00)^2 + (-4.00)^2 + (-2.00)^2 + 4.00^2\\right)\n\\;=\\; 3\\,(16+4+16+4+16)\n\\;=\\; 3\\,(56)\n\\;=\\; 168.\n$$\nThe lack-of-fit degrees of freedom are $df_{\\mathrm{LOF}}=k-p=5-2=3$, so\n$$\nMS_{\\mathrm{LOF}} \\;=\\; \\frac{SS_{\\mathrm{LOF}}}{df_{\\mathrm{LOF}}} \\;=\\; \\frac{168}{3} \\;=\\; 56.0.\n$$\n\nStep 5: Lack-of-fit $F$-statistic and $p$-value. The test statistic comparing the mean-function misspecification to pure measurement error is\n$$\nF \\;=\\; \\frac{MS_{\\mathrm{LOF}}}{MS_{\\mathrm{PE}}} \\;=\\; \\frac{56.0}{0.822} \\;\\approx\\; 68.1,\n$$\nto be referenced to an $F$ distribution with $(df_{\\mathrm{LOF}},df_{\\mathrm{PE}})=(3,10)$ under the null hypothesis that the linear mean function $E=\\beta_0+\\beta_1 x$ is correct. The observed $F\\approx 68.1$ is extremely large; the corresponding $p$-value satisfies $p\\ll 0.001$ (indeed, numerically $p$ is far smaller than $0.001$), leading to a decisive rejection of the linear Nernstian mean function over this range.\n\nStep 6: Interpret the residual pattern. The mean residuals $\\bar y_i-\\hat y_i$ across $x$ are $\\{+4.00,-2.00,-4.00,-2.00,+4.00\\}$ for $x\\in\\{-5,-4,-3,-2,-1\\}$, showing a symmetric, curved pattern: positive at the extremes, most negative near the center. Under ideal Nernstian behavior, the residuals should be centered around zero without systematic structure across $x$. The observed pattern indicates curvature relative to the best-fitting line, consistent with non-Nernstian behavior such as systematic deviations due to activity-coefficient variation at higher ionic strength, liquid junction potentials, or interfering ions described by extended models (e.g., the Nikolskii–Eisenman formulation), and suggests that a higher-order calibration or a restricted linear working range is required. The within-level variances $s_i^2=\\{1.21,0.81,0.64,0.64,0.81\\}$ are comparable, arguing against heteroscedasticity as the primary cause.\n\nOption-by-option analysis:\n- Option A: Claims adequacy with nonsignificant lack-of-fit and no residual pattern. This contradicts the computed $F\\approx 68.1$ with $p\\ll 0.001$ and the evident curved residual structure. Incorrect.\n- Option B: States a highly significant lack-of-fit with a symmetric curved residual pattern indicating non-Nernstian behavior and recommends an extended model. This matches both the statistical outcome and the physical interpretation. Correct.\n- Option C: Attributes the issue to heteroscedasticity that can be addressed by transforming $E$. The within-level variances $s_i^2=\\{1.21,0.81,0.64,0.64,0.81\\}$ are comparable, and the mean residuals show systematic curvature rather than variance trends, so this does not explain the lack-of-fit. Incorrect.\n- Option D: Attributes lack-of-fit to outliers at a single level. The mean residuals are structured across multiple levels, and the lack-of-fit remains after averaging replicates, so a single-level outlier explanation is inconsistent with the data. Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Once a model's form has been validated, it can become a powerful tool for process optimization. This exercise guides you through a complete Response Surface Methodology (RSM) workflow, a key technique in experimental design used to find the optimal settings for a system. By fitting a quadratic model to data and analyzing it to maximize a Signal-to-Noise Ratio, you will gain hands-on experience in the practical application of statistical modeling to improve measurement system performance .",
            "id": "2961521",
            "problem": "You are given the task of implementing Response Surface Methodology (RSM) to locate the maximum Signal-to-Noise Ratio (SNR) in an electrochemical detection system with two control factors and then plan confirmatory runs. The goal is to formalize model building, optimization, and experimental confirmation in a way that is consistent with fundamental principles of the scientific method and measurement theory. The system has two controllable factors measured in natural units: the working electrode potential $E$ (in volts, V) and the supporting electrolyte concentration $C$ (in millimoles per liter, mM). The measured response is the Signal-to-Noise Ratio (SNR) expressed in decibels (dB), which is a logarithmic quantity. Assume that within a specified design region the expected response is well-approximated by a second-order polynomial in the factors expressed in coded units. You will work entirely with coded factors for modeling and optimization, then convert outputs to natural units for reporting.\n\nDefinitions and constraints:\n- Let the coded variables be $x_1$ and $x_2$. Define the linear coding between natural units $(E, C)$ and coded units $(x_1, x_2)$ by\n  - $x_1 = (E - 0.75)/0.15$ and $x_2 = (C - 30)/20$,\n  - equivalently, to map from coded to natural units, $E = 0.75 + 0.15 x_1$ (in V) and $C = 30 + 20 x_2$ (in mM).\n- The design region of interest for optimization is the hyper-rectangle $x_1 \\in [-1, 1]$ and $x_2 \\in [-1, 1]$, which corresponds to $E \\in [0.60, 0.90]$ V and $C \\in [10, 50]$ mM.\n- The structural model for the expected response $\\mathbb{E}[Y]$ in coded units is a full quadratic polynomial in $x_1$ and $x_2$. You must estimate its coefficients by least squares from the provided data.\n- The measurement error is assumed independent and homoscedastic, and the center point replicates provide an internal estimate of pure error; however, you are not asked to perform a formal lack-of-fit test.\n\nOptimization and decision logic:\n- From the fitted quadratic model, compute the stationary point by setting the partial derivatives with respect to $x_1$ and $x_2$ equal to zero and solving the resulting linear system. Determine whether this point is a local maximum of the quadratic model by checking the negative definiteness of the Hessian matrix (both eigenvalues strictly negative) and whether the stationary point lies within the interior of the design region $[-1, 1]^2$.\n- Decision rule:\n  - If the stationary point is inside $[-1, 1]^2$ and the Hessian is negative definite, adopt it as the predicted optimal coded setting $(x_1^\\star, x_2^\\star)$.\n  - Otherwise, perform a constrained search for the maximum predicted response over the grid $\\{ -1, -1 + \\delta, \\dots, 1 - \\delta, 1 \\}^2$ with $\\delta = 0.01$ in coded units, using the fitted model prediction. In the case of ties, choose the maximizer with the smallest $x_1$; if still tied, choose the smallest $x_2$. Denote the selected grid maximizer by $(\\tilde{x}_1, \\tilde{x}_2)$.\n- Confirmatory run planning: use the eigen-decomposition of the symmetric Hessian to find the eigenvector corresponding to the most negative eigenvalue (the direction of largest downward curvature). From the adopted base point $(x_b)$, where $x_b = (x_1^\\star, x_2^\\star)$ in the interior-maximum case or $x_b = (\\tilde{x}_1, \\tilde{x}_2)$ otherwise, plan two axial confirmatory runs at coded offsets $\\pm \\Delta \\mathbf{u}$, where $\\mathbf{u}$ is the unit eigenvector corresponding to the most negative eigenvalue and $\\Delta = 0.5$. If either of the two confirmatory points would lie outside $[-1, 1]^2$, scale $\\Delta$ down uniformly to the largest nonnegative value such that both $x_b + \\Delta \\mathbf{u}$ and $x_b - \\Delta \\mathbf{u}$ lie within $[-1, 1]^2$. Use the fitted model to predict the response at both confirmatory settings.\n\nData and test suite:\n- Use the following fixed set of coded design points, in the specified order:\n  - Factorial points: $(x_1, x_2) \\in \\{ (1, 1), (1, -1), (-1, 1), (-1, -1) \\}$.\n  - Axial points: $(1.5, 0), (-1.5, 0), (0, 1.5), (0, -1.5)$.\n  - Center points (replicated): $(0, 0)$ repeated $5$ times.\n  - In total, there are $13$ runs in the order: $(1, 1)$, $(1, -1)$, $(-1, 1)$, $(-1, -1)$, $(1.5, 0)$, $(-1.5, 0)$, $(0, 1.5)$, $(0, -1.5)$, $(0, 0)$, $(0, 0)$, $(0, 0)$, $(0, 0)$, $(0, 0)$.\n- For each dataset below, the observed SNR values (in dB) correspond to these $13$ coded design points in the listed order.\n\nProvide three datasets:\n- Dataset A (interior local maximum case):\n  - Observed SNRs (dB): $[16.12, 17.92, -21.95, -4.04, 20.03, -25.02, -5.5, 9.56, 20.02, 19.99, 20.03, 20.01, 19.98]$.\n- Dataset B (saddle case; no interior maximum; the constrained maximum lies on the boundary within $[-1, 1]^2$):\n  - Observed SNRs (dB): $[17.07, 26.96, -13.06, 17.02, 15.76, -14.28, 8.30, 38.25, 12.02, 11.98, 12.01, 12.00, 11.99]$.\n- Dataset C (model maximum outside $[-1, 1]^2$; constrained interior maximum on $[-1, 1]^2$ boundary):\n  - Observed SNRs (dB): $[-100.95, -17.03, -208.96, -117.02, -37.23, -193.26, -176.75, -44.72, -94.99, -95.02, -95.00, -94.98, -95.01]$.\n\nImplementation requirements:\n- Fit the quadratic response surface in coded variables by least squares using all $13$ runs.\n- Evaluate optimality and determine the adopted coded optimum as specified.\n- Plan two confirmatory runs along the principal curvature direction as specified.\n- Convert all coded settings used in the final report to natural units using $E = 0.75 + 0.15 x_1$ (V) and $C = 30 + 20 x_2$ (mM).\n\nFinal outputs and units:\n- For each dataset, produce a list in the form\n  - $[E^\\star, C^\\star, \\widehat{\\mathrm{SNR}}^\\star, \\text{is\\_quadratic\\_max}, E_1, C_1, \\widehat{\\mathrm{SNR}}_1, E_2, C_2, \\widehat{\\mathrm{SNR}}_2]$\n  - where:\n    - $E^\\star$ is the adopted optimal $E$ in volts,\n    - $C^\\star$ is the adopted optimal $C$ in millimoles per liter,\n    - $\\widehat{\\mathrm{SNR}}^\\star$ is the fitted-model predicted SNR at the adopted optimum in decibels,\n    - $\\text{is\\_quadratic\\_max}$ is a boolean that is $True$ if and only if the stationary point lies in $[-1, 1]^2$ and the Hessian is negative definite, and $False$ otherwise,\n    - $(E_1, C_1)$ and $(E_2, C_2)$ are the two planned confirmatory settings in natural units,\n    - $\\widehat{\\mathrm{SNR}}_1$ and $\\widehat{\\mathrm{SNR}}_2$ are the fitted-model predicted SNRs at the confirmatory settings in decibels.\n- Express $E$ in volts and $C$ in millimoles per liter, each rounded to three decimal places. Express SNR values in decibels rounded to two decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_A,result\\_B,result\\_C]$), where each $result\\_*$ is the list specified above.\n\nYour program must be self-contained, run without input, and implement the above logic exactly for all three datasets using the given design and observations. The final output must reflect the three datasets in the stated order.",
            "solution": "The problem presented is a standard exercise in the application of Response Surface Methodology (RSM), a collection of statistical and mathematical techniques useful for developing, improving, and optimizing processes. The task is to construct a second-order model for an electrochemical system's Signal-to-Noise Ratio (SNR) as a function of two control factors, find the settings that maximize this response, and plan confirmatory experiments. This is a valid problem, grounded in the principles of experimental design and statistical modeling. We will proceed with a rigorous, step-by-step solution.\n\nThe core of the problem lies in fitting a quadratic model to experimental data and then analyzing this model to find an optimum. The relationship between the natural factor units, potential $E$ (V) and concentration $C$ (mM), and the dimensionless coded units, $x_1$ and $x_2$, is given by the linear transformations:\n$$x_1 = \\frac{E - 0.75}{0.15} \\quad \\text{and} \\quad x_2 = \\frac{C - 30}{20}$$\nThe optimization is conducted within the hypercube defined by $x_1, x_2 \\in [-1, 1]$.\n\nThe assumed structural model for the expected response, $\\mathbb{E}[Y]$, is a full second-order polynomial in the coded variables:\n$$\\mathbb{E}[Y | x_1, x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2$$\nThis can be expressed in matrix notation. For a set of $N$ experimental runs, we have the system $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the $N \\times 1$ vector of observed responses, $\\mathbf{X}$ is the $N \\times 6$ design matrix, $\\boldsymbol{\\beta}$ is the $6 \\times 1$ vector of unknown coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of random errors. The columns of the design matrix for the $i$-th run are $[1, x_{i1}, x_{i2}, x_{i1}^2, x_{i2}^2, x_{i1}x_{i2}]$.\n\nThe coefficients $\\boldsymbol{\\beta}$ are estimated using the method of Ordinary Least Squares (OLS), which minimizes the sum of squared differences between observed and predicted responses. The OLS estimate, denoted by $\\mathbf{b}$, is given by the solution to the normal equations:\n$$\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\nThe fitted response surface is then $\\hat{Y}(\\mathbf{x}) = \\mathbf{x}_{\\text{model}}^T \\mathbf{b}$, where $\\mathbf{x}_{\\text{model}}^T = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$.\n\nTo find the optimum of this fitted surface, we perform a canonical analysis. The first step is to locate the stationary point, $\\mathbf{x}_s = (x_{1s}, x_{2s})^T$, where the gradient of the surface is zero: $\\nabla \\hat{Y}(\\mathbf{x}_s) = \\mathbf{0}$. The gradient is:\n$$\\nabla \\hat{Y} = \\begin{pmatrix} b_1 + 2b_{11}x_1 + b_{12}x_2 \\\\ b_2 + b_{12}x_1 + 2b_{22}x_2 \\end{pmatrix}$$\nSetting this to zero yields a linear system for the stationary point coordinates:\n$$\\begin{pmatrix} 2b_{11} & b_{12} \\\\ b_{12} & 2b_{22} \\end{pmatrix} \\begin{pmatrix} x_{1s} \\\\ x_{s2} \\end{pmatrix} = \\begin{pmatrix} -b_1 \\\\ -b_2 \\end{pmatrix}$$\nThe $2 \\times 2$ matrix is the Hessian matrix, $\\mathbf{H}$, of the quadratic form. The nature of the stationary point is determined by the eigenvalues of $\\mathbf{H}$. If both eigenvalues are negative, the stationary point is a local maximum. If both are positive, it is a local minimum. If they have opposite signs, it is a saddle point.\n\nThe problem defines a specific decision rule. We first determine if the stationary point represents an unconstrained maximum *within the interior* of the design space, i.e., $x_{1s} \\in (-1, 1)$ and $x_{2s} \\in (-1, 1)$, and the Hessian matrix $\\mathbf{H}$ is negative definite. If these conditions are met, this stationary point is adopted as the optimum, $\\mathbf{x}_b = \\mathbf{x}_s$, and the flag $\\text{is\\_quadratic\\_max}$ is set to $True$.\n\nIf these conditions are not met—either because the stationary point is not a maximum, falls outside the region, or does not exist (if $\\mathbf{H}$ is singular)— we must find the maximum of $\\hat{Y}$ on the boundary and interior of the region $[-1, 1]^2$. The problem specifies a constrained search over a discrete grid of points defined by a step size $\\delta=0.01$. The point on this grid yielding the highest predicted response, $\\hat{Y}$, is chosen as the adopted optimum $\\mathbf{x}_b$. The specified tie-breaking rule (smallest $x_1$, then smallest $x_2$) ensures a unique solution. In this case, $\\text{is\\_quadratic\\_max}$ is $False$.\n\nFinally, we must plan two confirmatory experiments. These are positioned symmetrically around the adopted optimum $\\mathbf{x}_b$ along the direction of principal curvature. For a maximum, this corresponds to the direction of most rapid change, which is given by the eigenvector $\\mathbf{u}$ associated with the most negative eigenvalue of the Hessian $\\mathbf{H}$. The experimental points are set at $\\mathbf{x}_b \\pm \\Delta \\mathbf{u}$. The initial step size is $\\Delta = 0.5$. A crucial practical consideration is to ensure these new points lie within the valid experimental region $[-1, 1]^2$. If they do not, $\\Delta$ must be scaled down to the largest possible value that keeps both points inside the cube. This value is determined by the most restrictive constraint among all dimensions, calculated as $\\Delta_{\\text{new}} = \\min(0.5, \\min_{i \\in \\{1,2\\}, u_i \\neq 0} \\frac{1 - |x_{bi}|}{|u_i|})$.\n\nThe final step is to convert the coded coordinates of the optimal point $(\\mathbf{x}_b)$ and the two confirmatory points back to natural units $(E, C)$ and report the predicted responses at these locations, with the specified numerical precision. This procedure will be applied to each of the three datasets provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to process all datasets and print the final result.\n    \"\"\"\n    # Fixed experimental design points in coded units\n    design_points = [\n        # Factorial points\n        (1.0, 1.0), (1.0, -1.0), (-1.0, 1.0), (-1.0, -1.0),\n        # Axial points\n        (1.5, 0.0), (-1.5, 0.0), (0.0, 1.5), (0.0, -1.5),\n        # Center points\n        (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)\n    ]\n\n    # Test cases: Observed SNR values (dB) for the 13 design points\n    test_cases = [\n        # Dataset A\n        [16.12, 17.92, -21.95, -4.04, 20.03, -25.02, -5.5, 9.56, 20.02, 19.99, 20.03, 20.01, 19.98],\n        # Dataset B\n        [17.07, 26.96, -13.06, 17.02, 15.76, -14.28, 8.30, 38.25, 12.02, 11.98, 12.01, 12.00, 11.99],\n        # Dataset C\n        [-100.95, -17.03, -208.96, -117.02, -37.23, -193.26, -176.75, -44.72, -94.99, -95.02, -95.00, -94.98, -95.01]\n    ]\n\n    results = []\n    for y_obs in test_cases:\n        result = process_dataset(design_points, y_obs)\n        results.append(result)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_dataset(design_points, y_obs):\n    \"\"\"\n    Performs Response Surface Methodology analysis for a single dataset.\n    \"\"\"\n    # 1. Construct the design matrix X for the quadratic model\n    X = []\n    for x1, x2 in design_points:\n        X.append([1.0, x1, x2, x1**2, x2**2, x1*x2])\n    X = np.array(X)\n    y = np.array(y_obs)\n\n    # 2. Estimate model coefficients using Ordinary Least Squares\n    # b = [b0, b1, b2, b11, b22, b12]\n    b = np.linalg.lstsq(X, y, rcond=None)[0]\n    b0, b1, b2, b11, b22, b12 = b\n\n    def predict_snr(x1, x2):\n        \"\"\"Fitted quadratic model to predict SNR.\"\"\"\n        return b0 + b1*x1 + b2*x2 + b11*x1**2 + b22*x2**2 + b12*x1*x2\n\n    # 3. Find the stationary point by solvingnabla(Y_hat) = 0\n    Hessian_part = np.array([[2*b11, b12], [b12, 2*b22]])\n    grad_const = np.array([-b1, -b2])\n    \n    try:\n        if np.abs(np.linalg.det(Hessian_part)) < 1e-9:\n             xs = np.array([np.nan, np.nan]) # Singular Hessian\n        else:\n             xs = np.linalg.solve(Hessian_part, grad_const)\n    except np.linalg.LinAlgError:\n        xs = np.array([np.nan, np.nan]) # Should be caught by det check, but for robustness.\n\n    # 4. Analyze the stationary point to check for an interior maximum\n    is_quadratic_max = False\n    if not np.any(np.isnan(xs)):\n        eigenvalues = np.linalg.eigvalsh(Hessian_part)\n        is_neg_definite = np.all(eigenvalues < 0)\n        is_interior = np.all(np.abs(xs) < 1.0)\n        \n        if is_neg_definite and is_interior:\n            is_quadratic_max = True\n\n    # 5. Determine the adopted optimal coded setting xb\n    if is_quadratic_max:\n        xb = xs\n    else:\n        # Perform a constrained grid search\n        delta = 0.01\n        grid_pts = np.round(np.arange(-1.0, 1.0 + delta/2, delta), 2)\n        x1_mesh, x2_mesh = np.meshgrid(grid_pts, grid_pts, indexing='xy')\n        snr_mesh = predict_snr(x1_mesh, x2_mesh)\n\n        # Create candidates and sort to find the best point with tie-breaking\n        candidates = []\n        for i in range(len(grid_pts)):\n            for j in range(len(grid_pts)):\n                candidates.append((snr_mesh[i, j], x1_mesh[i, j], x2_mesh[i, j]))\n        \n        # Sort: 1. descending by SNR, 2. ascending by x1, 3. ascending by x2\n        candidates.sort(key=lambda item: (-item[0], item[1], item[2]))\n        \n        best_cand = candidates[0]\n        xb = np.array([best_cand[1], best_cand[2]])\n\n    # 6. Predict SNR at the adopted optimum\n    snr_star = predict_snr(xb[0], xb[1])\n\n    # 7. Plan confirmatory runs\n    Hessian = Hessian_part\n    eigenvalues, eigenvectors = np.linalg.eigh(Hessian)\n    \n    # Get eigenvector for the most negative eigenvalue\n    min_eig_idx = np.argmin(eigenvalues)\n    u = eigenvectors[:, min_eig_idx]\n\n    # Scale step size delta_conf if needed to stay within [-1, 1]^2\n    delta_conf = 0.5\n    \n    ratios = []\n    for i in range(2):\n        if abs(u[i]) > 1e-9:\n            ratios.append((1.0 - abs(xb[i])) / abs(u[i]))\n\n    if ratios:\n        max_allowable_delta = min(ratios)\n        if max_allowable_delta < delta_conf:\n            delta_conf = max_allowable_delta\n    \n    # Calculate confirmatory points in coded units\n    xc1 = xb + delta_conf * u\n    xc2 = xb - delta_conf * u\n\n    # 8. Predict SNR at confirmatory points\n    snr1 = predict_snr(xc1[0], xc1[1])\n    snr2 = predict_snr(xc2[0], xc2[1])\n\n    # 9. Convert coded settings to natural units and format for output\n    def to_natural(x1, x2):\n        E = 0.75 + 0.15 * x1\n        C = 30.0 + 20.0 * x2\n        return E, C\n\n    E_star, C_star = to_natural(xb[0], xb[1])\n    E1, C1 = to_natural(xc1[0], xc1[1])\n    E2, C2 = to_natural(xc2[0], xc2[1])\n\n    # Round all values as specified in the problem statement\n    return [\n        round(E_star, 3), round(C_star, 3), round(snr_star, 2),\n        is_quadratic_max,\n        round(E1, 3), round(C1, 3), round(snr1, 2),\n        round(E2, 3), round(C2, 3), round(snr2, 2),\n    ]\n\n# Execute the main function when the script is run\nsolve()\n```"
        }
    ]
}