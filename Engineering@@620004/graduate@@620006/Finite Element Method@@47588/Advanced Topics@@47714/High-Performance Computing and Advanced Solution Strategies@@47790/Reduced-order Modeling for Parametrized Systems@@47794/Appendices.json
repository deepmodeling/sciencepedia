{"hands_on_practices": [{"introduction": "Reduced-order modeling begins with identifying the most important features of a complex system. Proper Orthogonal Decomposition (POD) is a cornerstone technique for this task, extracting a hierarchy of \"modes\" that optimally capture the energy or variance within a set of data snapshots. This exercise will guide you through the core mechanics of POD, requiring you to compute the essential modes from a given set of snapshots and quantify the trade-off between model size and accuracy [@problem_id:2593061].", "problem": "Consider a family of linear, coercive, parameterized partial differential equations discretized by the Finite Element Method (FEM). Suppose the solution manifold is sampled at three parameter values $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$, yielding three snapshot vectors in $\\mathbb{R}^{3}$ collected as columns of the snapshot matrix $S \\in \\mathbb{R}^{3 \\times 3}$. Let the finite element mass matrix be the symmetric positive definite matrix $M \\in \\mathbb{R}^{3 \\times 3}$. You will compute a Proper Orthogonal Decomposition (POD) with respect to the $M$-inner product for these snapshots.\n\nUse the following data:\n- The mass matrix is \n$$\nM \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n- The snapshot matrix is \n$$\nS \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nAdopt the $M$-inner product $\\langle u, v \\rangle_{M} \\equiv u^{\\top} M v$ and the corresponding $M$-norm $\\|u\\|_{M} \\equiv \\sqrt{u^{\\top} M u}$. Define the snapshot correlation matrix by the fundamental POD definition\n$$\nC \\;=\\; S^{\\top} M S \\;\\in\\; \\mathbb{R}^{3 \\times 3}.\n$$\nLet the eigenvalues of $C$ be denoted and ordered nonincreasingly as $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3} \\ge 0$. The POD energy captured by the first $r$ modes is defined as\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}.\n$$\n\nYour tasks are:\n1. Given the target energy threshold $\\eta = 0.85$, determine the minimal integer $r$ such that $\\mathcal{E}(r) \\ge \\eta$.\n2. Using the fundamental POD error identity for snapshots, estimate the total snapshot reconstruction truncation error in the $M$-norm when using $r$ modes, defined as\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}.\n$$\n\nReport your final answer as a two-entry row matrix $\\begin{pmatrix} r & E_{\\text{trunc}} \\end{pmatrix}$. No rounding is required, and exact values are expected. No physical units are needed.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the established theory of reduced-order modeling, specifically Proper Orthogonal Decomposition (POD). It is well-posed, with all necessary data and definitions provided to ensure a unique, stable, and meaningful solution. The problem's structure is objective and logically consistent. We proceed with the solution.\n\nThe problem requires the computation of two quantities related to the Proper Orthogonal Decomposition of a given set of snapshots. The first is the minimal number of modes, $r$, required to capture a specified fraction of the total energy. The second is the corresponding truncation error. The computations are to be performed with respect to the inner product induced by the mass matrix $M$.\n\nThe core of the POD method, as described, is the eigenvalue problem for the snapshot correlation matrix $C$. We begin by computing this matrix. The definition provided is\n$$\nC \\;=\\; S^{\\top} M S\n$$\nThe snapshot matrix $S$ is given as the $3 \\times 3$ identity matrix, $S = I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$. The transpose of the identity matrix is the identity matrix itself, $S^{\\top} = I^{\\top} = I$. Substituting these into the definition of $C$ yields:\n$$\nC \\;=\\; I^{\\top} M I \\;=\\; I M I \\;=\\; M\n$$\nTherefore, the correlation matrix $C$ is identical to the mass matrix $M$:\n$$\nC \\;=\\; M \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}\n$$\nThe next step is to determine the eigenvalues of $C$. Since $C$ is a diagonal matrix, its eigenvalues are precisely its diagonal entries. The problem requires that these eigenvalues be ordered nonincreasingly: $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}$. From the diagonal of $C$, we identify the eigenvalues as $2$, $1$, and $\\frac{1}{2}$. Ordering them gives:\n$$\n\\lambda_{1} = 2\n$$\n$$\n\\lambda_{2} = 1\n$$\n$$\n\\lambda_{3} = \\frac{1}{2}\n$$\nThese eigenvalues represent the squared singular values of the snapshot matrix scaled by the $M$-inner product, and their sum represents the total energy, or variance, contained in the snapshots. The total energy is the trace of the correlation matrix, $\\mathrm{Tr}(C)$:\n$$\n\\sum_{i=1}^{3} \\lambda_{i} = \\lambda_{1} + \\lambda_{2} + \\lambda_{3} = 2 + 1 + \\frac{1}{2} = 3.5 = \\frac{7}{2}\n$$\nNow, we address the first task: to find the minimal integer $r$ such that the captured energy $\\mathcal{E}(r)$ meets or exceeds the threshold $\\eta = 0.85$. The captured energy is defined as:\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}\n$$\nWe evaluate $\\mathcal{E}(r)$ for $r = 1, 2, \\dots$ until the condition $\\mathcal{E}(r) \\ge 0.85$ is met. The threshold is $\\eta = 0.85 = \\frac{85}{100} = \\frac{17}{20}$.\n\nFor $r=1$:\n$$\n\\mathcal{E}(1) = \\frac{\\lambda_{1}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2}{\\frac{7}{2}} = \\frac{4}{7}\n$$\nWe compare this to the threshold. Is $\\frac{4}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $4 \\times 20 = 80$ and $7 \\times 17 = 119$. Since $80 < 119$, we have $\\frac{4}{7} < \\frac{17}{20}$. Thus, $\\mathcal{E}(1) < 0.85$. One mode is insufficient.\n\nFor $r=2$:\n$$\n\\mathcal{E}(2) = \\frac{\\lambda_{1} + \\lambda_{2}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2 + 1}{\\frac{7}{2}} = \\frac{3}{\\frac{7}{2}} = \\frac{6}{7}\n$$\nWe compare this to the threshold. Is $\\frac{6}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $6 \\times 20 = 120$ and $7 \\times 17 = 119$. Since $120 > 119$, we have $\\frac{6}{7} > \\frac{17}{20}$. Thus, $\\mathcal{E}(2) > 0.85$. Two modes are sufficient.\n\nSince $r=1$ is insufficient and $r=2$ is sufficient, the minimal integer is $r=2$.\n\nThe second task is to compute the total snapshot reconstruction truncation error, $E_{\\text{trunc}}$, when using $r=2$ modes. The definition provided is:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}\n$$\nFor $r=2$, this sum becomes:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=2+1}^{3} \\lambda_{i} \\;=\\; \\sum_{i=3}^{3} \\lambda_{i} \\;=\\; \\lambda_{3}\n$$\nSubstituting the value for $\\lambda_{3}$:\n$$\nE_{\\text{trunc}} = \\frac{1}{2}\n$$\nThis value represents the energy content of the modes that are truncated, which is exactly the portion of total energy not captured by the first $r$ modes.\n\nThe two required values are $r=2$ and $E_{\\text{trunc}} = \\frac{1}{2}$. The final answer must be reported as a two-entry row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "2593061"}, {"introduction": "While POD excels at compressing existing data, greedy algorithms offer a more proactive approach to building an accurate reduced basis by adaptively exploring the parameter space. This method iteratively finds the parameter for which the current reduced model has the largest error and adds the corresponding solution to the basis. This practice simulates one crucial iteration of a weak greedy algorithm, where you will use a residual-based error indicator to select the next most informative basis vector [@problem_id:2593056].", "problem": "Consider a parametrized, coercive linear elliptic problem discretized by the Finite Element (FE) method, yielding for each parameter value $\\mu \\in \\mathbb{R}_{+}$ the linear system $A(\\mu) u(\\mu) = f$, with an affine decomposition $A(\\mu) = \\Theta_{1}(\\mu) A_{1} + \\Theta_{2}(\\mu) A_{2}$. Suppose that $A_{1} \\in \\mathbb{R}^{3 \\times 3}$ and $A_{2} \\in \\mathbb{R}^{3 \\times 3}$ are symmetric positive-definite, the parameter functions are $\\Theta_{1}(\\mu) = \\mu$ and $\\Theta_{2}(\\mu) = 1$, and the right-hand side is $f \\in \\mathbb{R}^{3}$. The Reduced Basis (RB) greedy procedure starts from an initial seed parameter $\\mu_{1}$ and a training set $\\Xi_{\\mathrm{train}}$. In one greedy iteration, for each $\\mu \\in \\Xi_{\\mathrm{train}}$, one computes the reduced Galerkin solution $u_{N}(\\mu)$ in the current RB space and the residual-based error indicator\n$$\n\\eta(\\mu) \\;=\\; \\frac{\\|r(\\mu)\\|_{V^{*}}}{\\alpha_{\\mathrm{LB}}(\\mu)} \\,,\n$$\nwhere $r(\\mu) = f - A(\\mu) u_{N}(\\mu)$, $\\|\\cdot\\|_{V^{*}}$ is the dual norm induced by the FE trial space norm, and $\\alpha_{\\mathrm{LB}}(\\mu)$ is a reliable lower bound for the coercivity constant of $A(\\mu)$. The greedy update selects the parameter $\\mu^{\\star}$ that maximizes $\\eta(\\mu)$ over $\\Xi_{\\mathrm{train}}$.\n\nYou are given the following concrete data:\n- $A_{1} = \\mathrm{diag}(2,3,4)$, $A_{2} = I_{3}$, and $f = [1, 1, 1]^{T}$.\n- The FE inner product on the trial space is the standard Euclidean inner product on $\\mathbb{R}^{3}$, so that the dual norm $\\|r(\\mu)\\|_{V^{*}}$ equals the Euclidean norm $\\|r(\\mu)\\|_{2}$.\n- The initial seed is $\\mu_{1} = 1$ and the RB space is one-dimensional, spanned by the truth snapshot $\\phi = u_{h}(\\mu_{1})$, where $u_{h}(\\mu_{1})$ is the exact FE solution of $A(\\mu_{1}) u = f$. No normalization of $\\phi$ is performed.\n- The training set is $\\Xi_{\\mathrm{train}} = \\{\\tfrac{1}{2},\\,\\tfrac{3}{2},\\,2\\}$.\n- Use the reliable coercivity lower bound $\\alpha_{\\mathrm{LB}}(\\mu) = 2\\mu + 1$.\n\nUsing only the above information and starting from first principles (Galerkin projection in the current RB space, the definition of the residual, the Euclidean dual norm, and the given coercivity lower bound), carry out one greedy iteration as follows:\n1. For each $\\mu \\in \\Xi_{\\mathrm{train}}$, compute the reduced Galerkin approximation $u_{N}(\\mu)$ in the span of $\\{\\phi\\}$.\n2. Compute the residual $r(\\mu) = f - A(\\mu) u_{N}(\\mu)$ and its Euclidean norm.\n3. Compute $\\eta(\\mu) = \\|r(\\mu)\\|_{2} / \\alpha_{\\mathrm{LB}}(\\mu)$ for each candidate.\n4. Select the parameter $\\mu^{\\star}$ that maximizes $\\eta(\\mu)$ over $\\Xi_{\\mathrm{train}}$.\n\nReport the chosen parameter value $\\mu^{\\star}$ as a single exact rational number. Do not include units. No rounding is required.", "solution": "The problem statement is critically validated and found to be valid. It is a well-posed, scientifically grounded, and objective problem in the field of reduced-order modeling. All necessary data are provided, and there are no internal contradictions or logical flaws. The task is to perform one iteration of a standard Reduced Basis greedy algorithm. I will proceed with the solution.\n\nThe objective is to find the parameter $\\mu^{\\star}$ that maximizes the error indicator $\\eta(\\mu)$ over the training set $\\Xi_{\\mathrm{train}}$.\n$$\n\\mu^{\\star} = \\arg\\max_{\\mu \\in \\Xi_{\\mathrm{train}}} \\eta(\\mu) = \\arg\\max_{\\mu \\in \\{\\frac{1}{2}, \\frac{3}{2}, 2\\}} \\frac{\\|r(\\mu)\\|_{2}}{\\alpha_{\\mathrm{LB}}(\\mu)}\n$$\nThe problem is set in $\\mathbb{R}^{3}$, where the FE trial space inner product is the standard Euclidean inner product. The dual norm $\\| \\cdot \\|_{V^*}$ is therefore the Euclidean norm $\\| \\cdot \\|_2$.\n\nFirst, we determine the one-dimensional Reduced Basis (RB) space, $V_{N} = \\mathrm{span}\\{\\phi\\}$, where $N=1$. The basis vector $\\phi$ is the \"truth\" Finite Element (FE) solution $u_h(\\mu_1)$ for the seed parameter $\\mu_1 = 1$.\nThe parameter-dependent matrix $A(\\mu)$ is given by $A(\\mu) = \\Theta_{1}(\\mu) A_{1} + \\Theta_{2}(\\mu) A_{2} = \\mu A_{1} + A_{2}$.\nGiven $A_{1} = \\mathrm{diag}(2, 3, 4)$ and $A_{2} = I_{3} = \\mathrm{diag}(1, 1, 1)$, we have:\n$$\nA(\\mu) = \\mu \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2\\mu+1 & 0 & 0 \\\\ 0 & 3\\mu+1 & 0 \\\\ 0 & 0 & 4\\mu+1 \\end{pmatrix}\n$$\nFor $\\mu_1 = 1$, the matrix is $A(1) = \\mathrm{diag}(3, 4, 5)$. The basis vector $\\phi$ is the solution to $A(1) \\phi = f$, where $f = [1, 1, 1]^T$.\n$$\n\\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\implies \\phi = \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix}\n$$\nThe RB space is $V_1 = \\mathrm{span}\\{\\phi\\}$.\n\nNext, for each $\\mu \\in \\Xi_{\\mathrm{train}}$, we find the reduced Galerkin solution $u_N(\\mu) \\in V_1$. This solution can be written as $u_N(\\mu) = c(\\mu) \\phi$ for some scalar coefficient $c(\\mu)$. The Galerkin projection condition states that the residual must be orthogonal to the basis of $V_1$. The inner product is the Euclidean dot product.\n$$\n\\phi^T (A(\\mu) u_N(\\mu) - f) = 0\n$$\nSubstituting $u_N(\\mu) = c(\\mu)\\phi$:\n$$\n\\phi^T (A(\\mu) (c(\\mu)\\phi) - f) = 0 \\implies c(\\mu) (\\phi^T A(\\mu) \\phi) - \\phi^T f = 0\n$$\nThis gives the coefficient $c(\\mu)$:\n$$\nc(\\mu) = \\frac{\\phi^T f}{\\phi^T A(\\mu) \\phi}\n$$\nLet us compute the numerator and denominator.\n$$\n\\phi^T f = \\begin{pmatrix} 1/3 & 1/4 & 1/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} = \\frac{20+15+12}{60} = \\frac{47}{60}\n$$\nThe denominator is an affine function of $\\mu$:\n$$\n\\phi^T A(\\mu) \\phi = \\mu (\\phi^T A_1 \\phi) + (\\phi^T A_2 \\phi)\n$$\n$$\n\\phi^T A_1 \\phi = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix} = 2\\left(\\frac{1}{3}\\right)^2 + 3\\left(\\frac{1}{4}\\right)^2 + 4\\left(\\frac{1}{5}\\right)^2 = \\frac{2}{9} + \\frac{3}{16} + \\frac{4}{25} = \\frac{800+675+576}{3600} = \\frac{2051}{3600}\n$$\n$$\n\\phi^T A_2 \\phi = \\phi^T I_3 \\phi = \\|\\phi\\|_2^2 = \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{9} + \\frac{1}{16} + \\frac{1}{25} = \\frac{400+225+144}{3600} = \\frac{769}{3600}\n$$\nSo, $\\phi^T A(\\mu) \\phi = \\mu \\frac{2051}{3600} + \\frac{769}{3600} = \\frac{2051\\mu + 769}{3600}$.\nAnd the coefficient is:\n$$\nc(\\mu) = \\frac{47/60}{(2051\\mu + 769)/3600} = \\frac{47 \\cdot 60}{2051\\mu + 769} = \\frac{2820}{2051\\mu + 769}\n$$\nNow we compute the residual $r(\\mu) = f - A(\\mu) u_N(\\mu) = f - c(\\mu) A(\\mu) \\phi$. A crucial insight is that the residual vector $r(\\mu)$ vanishes at $\\mu = 1$, because $u_N(1) = c(1)\\phi$ and by construction $\\phi = u_h(1)$. Let's check:\n$$\nc(1) = \\frac{47/60}{\\phi^T A(1) \\phi} = \\frac{\\phi^T f}{\\phi^T f} = 1\n$$\nSo $u_N(1) = 1 \\cdot \\phi = u_h(1)$, and $r(1) = f - A(1) u_h(1) = 0$. This implies that each component of the residual vector must have a factor of $(\\mu-1)$. Let's derive the general form for $r(\\mu)$.\nThe $i$-th component is $r_i(\\mu) = f_i - c(\\mu) (A(\\mu)\\phi)_i$. Since $A(\\mu)$ is diagonal, $(A(\\mu)\\phi)_i = A_{ii}(\\mu) \\phi_i = (d_i \\mu + 1) \\phi_i$, where $d_i$ are the diagonal entries of $A_1$.\n$$\nr_i(\\mu) = 1 - \\frac{\\phi^T f}{\\phi^T A(\\mu) \\phi} (d_i \\mu + 1) \\phi_i = \\frac{(\\phi^T A(\\mu) \\phi) - (\\phi^T f)(d_i \\mu + 1)\\phi_i}{\\phi^T A(\\mu) \\phi}\n$$\nThe numerator is a linear function of $\\mu$. As established, it must be zero for $\\mu=1$. Thus, it is proportional to $(\\mu-1)$.\nWe find that $r(\\mu) = \\frac{1-\\mu}{2051\\mu + 769} v$ for a constant vector $v$.\nLet's find $v$.\n$$r(\\mu) = \\frac{1-\\mu}{2051\\mu + 769}\\begin{pmatrix} -171 \\\\ 64 \\\\ 205 \\end{pmatrix}$$\nLet's verify the first component. The numerator is $(\\mu-1)(f_1 (\\phi^T A_1 \\phi) - (\\phi^T f)(A_1\\phi)_1) \\cdot 3600$.\n$(\\phi^T A_1 \\phi) = \\frac{2051}{3600}$, $\\phi^T f = \\frac{47}{60}$, $f_1=1$, $(A_1\\phi)_1 = 2 \\cdot \\frac{1}{3}$.\n$1 \\cdot \\frac{2051}{3600} - \\frac{47}{60} \\cdot \\frac{2}{3} = \\frac{2051 - 94 \\cdot 20}{3600} = \\frac{171}{3600}$.\nSo $r_1(\\mu) = \\frac{171(\\mu-1)}{2051\\mu+769}$, which matches the form $\\frac{1-\\mu}{2051\\mu+769}(-171)$.\nThe norm of the residual is:\n$$\n\\|r(\\mu)\\|_2 = \\frac{|1-\\mu|}{2051\\mu+769} \\left\\| \\begin{pmatrix} -171 \\\\ 64 \\\\ 205 \\end{pmatrix} \\right\\|_2\n$$\nLet's compute the norm of the constant vector $v = [-171, 64, 205]^T$:\n$$\n\\|v\\|_2^2 = (-171)^2 + 64^2 + 205^2 = 29241 + 4096 + 42025 = 75362\n$$\nThus, $\\|r(\\mu)\\|_2 = \\frac{|1-\\mu|\\sqrt{75362}}{2051\\mu+769}$.\nThe error indicator is $\\eta(\\mu) = \\frac{\\|r(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)}$, with $\\alpha_{\\mathrm{LB}}(\\mu) = 2\\mu+1$.\n$$\n\\eta(\\mu) = \\frac{|1-\\mu|\\sqrt{75362}}{(2051\\mu+769)(2\\mu+1)}\n$$\nTo find the maximizer, we can ignore the constant factor $\\sqrt{75362}$ and maximize the function $g(\\mu) = \\frac{|1-\\mu|}{(2051\\mu+769)(2\\mu+1)}$ for $\\mu \\in \\{\\frac{1}{2}, \\frac{3}{2}, 2\\}$.\n\nFor $\\mu = \\frac{1}{2}$:\n$g(\\frac{1}{2}) = \\frac{|1-\\frac{1}{2}|}{(2051(\\frac{1}{2})+769)(2(\\frac{1}{2})+1)} = \\frac{\\frac{1}{2}}{(1025.5+769)(1+1)} = \\frac{0.5}{1794.5 \\cdot 2} = \\frac{0.5}{3589} = \\frac{1}{7178}$.\n\nFor $\\mu = \\frac{3}{2}$:\n$g(\\frac{3}{2}) = \\frac{|1-\\frac{3}{2}|}{(2051(\\frac{3}{2})+769)(2(\\frac{3}{2})+1)} = \\frac{\\frac{1}{2}}{(3076.5+769)(3+1)} = \\frac{0.5}{3845.5 \\cdot 4} = \\frac{0.5}{15382} = \\frac{1}{30764}$.\n\nFor $\\mu = 2$:\n$g(2) = \\frac{|1-2|}{(2051(2)+769)(2(2)+1)} = \\frac{1}{(4102+769)(5)} = \\frac{1}{4871 \\cdot 5} = \\frac{1}{24355}$.\n\nWe must compare the values: $g(\\frac{1}{2}) = \\frac{1}{7178}$, $g(\\frac{3}{2}) = \\frac{1}{30764}$, and $g(2) = \\frac{1}{24355}$.\nThe largest value corresponds to the smallest denominator.\nSince $7178 < 24355 < 30764$, we have $\\frac{1}{7178} > \\frac{1}{24355} > \\frac{1}{30764}$.\nThe maximum value of $g(\\mu)$, and thus $\\eta(\\mu)$, occurs at $\\mu = \\frac{1}{2}$.\nThe selected parameter is $\\mu^{\\star} = \\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2593056"}, {"introduction": "The practical power of reduced-order models for parametrized systems comes from the offline-online computational strategy, which separates expensive, parameter-independent calculations from cheap, parameter-dependent ones. To achieve this efficiency, we rely on an affine decomposition of the system operators. This practice challenges you to formalize this strategy by identifying the minimal set of pre-computed quantities and quantifying the resulting memory footprint, making the cost-benefit analysis of ROMs tangible [@problem_id:2593090].", "problem": "Consider a parameter-dependent finite element method (FEM) linear system arising from a coercive elliptic problem, discretized on a fixed mesh with $n_h$ degrees of freedom. For each parameter vector $\\mu$ in a compact parameter set, the assembled high-fidelity algebraic system is $A_h(\\mu) u_h(\\mu)=b_h(\\mu)$, where $A_h(\\mu)\\in\\mathbb{R}^{n_h\\times n_h}$ and $b_h(\\mu)\\in\\mathbb{R}^{n_h}$.\nAssume there is an exact affine parameter dependence with $Q$ terms for both the operator and the right-hand side,\n$$\nA_h(\\mu)=\\sum_{q=1}^{Q}\\theta_q(\\mu)\\,A_q,\\qquad b_h(\\mu)=\\sum_{q=1}^{Q}\\theta_q(\\mu)\\,b_q,\n$$\nwhere $A_q\\in\\mathbb{R}^{n_h\\times n_h}$ and $b_q\\in\\mathbb{R}^{n_h}$ are parameter-independent coefficient objects and $\\theta_q(\\mu)$ are known scalar functions evaluable at negligible cost compared to linear algebra operations with dimension $n_h$. Let $V_r\\in\\mathbb{R}^{n_h\\times r}$ be a reduced trial basis with $r\\ll n_h$ and $V_r^{T}V_r=I_r$. The reduced Galerkin system for the reduced coordinates $a_r(\\mu)\\in\\mathbb{R}^{r}$ is $A_r(\\mu) a_r(\\mu)=b_r(\\mu)$, with $A_r(\\mu)=V_r^{T}A_h(\\mu)V_r$ and $b_r(\\mu)=V_r^{T}b_h(\\mu)$.\nYou are tasked with many-query evaluations over $\\mu$, with the goal that the online assembly of $A_r(\\mu)$ and $b_r(\\mu)$ is independent of $n_h$. Construct the minimal set of offline, parameter-independent precomputations that enable assembling $A_r(\\mu)$ and $b_r(\\mu)$ online using only operations that scale with $Q$ and $r$. Assume that no symmetry is exploited and that you must be able to recover the high-fidelity approximation $u_h(\\mu)\\approx V_r a_r(\\mu)$ online.\n\nUnder these assumptions, determine a single expression $M(Q,n_h,r)$ giving the total number of floating-point scalars that must be stored after the offline stage to support online assembly of $A_r(\\mu)$ and $b_r(\\mu)$ and recovery of $u_h(\\mu)$, while discarding any high-dimensional objects that are no longer needed. Report $M(Q,n_h,r)$ as a closed-form symbolic expression. Do not include storage for the functions $\\theta_q(\\mu)$, and do not count storage for any quantities that can be recomputed online in $\\mathcal{O}(r^2)$ or less without dependence on $n_h$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, well-posed, and formulated with objective, unambiguous language. The problem addresses a fundamental aspect of the offline-online computational paradigm in reduced-order modeling for parametrized systems, which is a standard topic in computational science and engineering.\n\nThe objective is to derive an expression for the total count of floating-point scalars, $M(Q, n_h, r)$, that must be stored after the offline precomputation phase. This stored data must be sufficient to enable the online assembly of the reduced system, its solution, and the reconstruction of the high-fidelity solution approximation, with the constraints specified. The online assembly of the reduced matrix and vector must be independent of the high-fidelity dimension $n_h$.\n\nWe will analyze the storage requirements for the distinct components needed in the online phase.\n\n1.  **Assembly of the Reduced-Order Matrix $A_r(\\mu)$**:\n    The reduced-order matrix is defined by the Galerkin projection $A_r(\\mu) = V_r^{T}A_h(\\mu)V_r$. Substituting the given affine expansion for $A_h(\\mu)$:\n    $$A_r(\\mu) = V_r^{T}\\left(\\sum_{q=1}^{Q}\\theta_q(\\mu)A_q\\right)V_r$$\n    By the linearity of matrix multiplication, we can rearrange this expression as:\n    $$A_r(\\mu) = \\sum_{q=1}^{Q}\\theta_q(\\mu)\\left(V_r^{T}A_qV_r\\right)$$\n    For the online assembly to be independent of the dimension $n_h$, the parameter-independent matrices $A_{r,q} \\equiv V_r^{T}A_qV_r$ must be computed during the offline stage and stored. The inputs to this computation are the high-fidelity component matrices $A_q \\in \\mathbb{R}^{n_h \\times n_h}$ and the reduced basis $V_r \\in \\mathbb{R}^{n_h \\times r}$. Each resulting matrix $A_{r,q}$ has dimensions $r \\times r$. As the problem states that no symmetry is to be exploited, we must store all $r^2$ entries for each of the $Q$ matrices. The total number of scalars for this component is therefore $Q r^2$.\n\n2.  **Assembly of the Reduced-Order Vector $b_r(\\mu)$**:\n    Similarly, the reduced-order right-hand side is $b_r(\\mu) = V_r^{T}b_h(\\mu)$. We substitute the affine expansion for $b_h(\\mu)$:\n    $$b_r(\\mu) = V_r^{T}\\left(\\sum_{q=1}^{Q}\\theta_q(\\mu)b_q\\right) = \\sum_{q=1}^{Q}\\theta_q(\\mu)\\left(V_r^{T}b_q\\right)$$\n    To ensure an online computation that is independent of $n_h$, the parameter-independent vectors $b_{r,q} \\equiv V_r^{T}b_q$ must be precomputed and stored. Each vector $b_{r,q}$ results from the product of a matrix of size $r \\times n_h$ and a vector of size $n_h \\times 1$, yielding a vector of dimension $r$. There are $Q$ such vectors. The total storage cost for this component is $Q r$.\n\n3.  **Reconstruction of the High-Fidelity Solution $u_h(\\mu)$**:\n    The problem mandates that the high-fidelity solution approximation must be recoverable online using the formula $u_h(\\mu) \\approx V_r a_r(\\mu)$, where $a_r(\\mu) \\in \\mathbb{R}^r$ is the solution to the reduced system. This reconstruction requires the reduced basis matrix $V_r \\in \\mathbb{R}^{n_h \\times r}$. Therefore, this matrix must be stored as part of the offline-computed data. The number of scalars required to store $V_r$ is $n_h r$.\n\nThe high-dimensional objects $\\{A_q\\}_{q=1}^Q$ and $\\{b_q\\}_{q=1}^Q$ are used only during the offline stage to compute $\\{A_{r,q}\\}_{q=1}^Q$ and $\\{b_{r,q}\\}_{q=1}^Q$, and are then discarded as per the problem's instructions. The set of objects to be stored for the online phase is thus $\\{A_{r,q}\\}_{q=1}^Q$, $\\{b_{r,q}\\}_{q=1}^Q$, and $V_r$. This is the minimal set required to fulfill all specified online tasks.\n\nThe total storage, $M(Q, n_h, r)$, is the sum of the scalars required for these three components.\n$$M(Q, n_h, r) = (\\text{storage for } \\{A_{r,q}\\}) + (\\text{storage for } \\{b_{r,q}\\}) + (\\text{storage for } V_r)$$\n$$M(Q, n_h, r) = Qr^2 + Qr + n_hr$$\nThis expression represents the total number of floating-point scalars that must be stored to enable the efficient online execution of the reduced-order model.", "answer": "$$\n\\boxed{Qr^2 + Qr + n_hr}\n$$", "id": "2593090"}]}