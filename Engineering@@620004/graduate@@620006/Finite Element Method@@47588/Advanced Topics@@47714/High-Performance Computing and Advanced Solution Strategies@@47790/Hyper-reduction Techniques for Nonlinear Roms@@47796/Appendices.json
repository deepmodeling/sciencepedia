{"hands_on_practices": [{"introduction": "The primary motivation for hyper-reduction is to achieve computational speedup by breaking the dependence of the online runtime on the full-system size. To truly appreciate this benefit, it is essential to move beyond qualitative descriptions and perform a quantitative analysis of the computational costs. This exercise [@problem_id:2566979] guides you through the process of building a simple yet powerful flop-count model, allowing you to derive the theoretical speedup factor and understand which parameters, such as the full-order dimension $N$ and the reduced basis size $r$, govern the efficiency of a hyper-reduced model.", "problem": "Consider a nonlinear finite element method (FEM) semi-discrete model with $N$ degrees of freedom (DOFs) and internal force vector $f_{\\text{int}}(u) \\in \\mathbb{R}^{N}$ defined by an element-wise integral over the spatial domain. Let a reduced-order model (ROM) with reduced basis $V \\in \\mathbb{R}^{N \\times r}$ and reduced coordinates $q \\in \\mathbb{R}^{r}$ approximate the state as $u \\approx V q$. The internal force evaluation is approximated by numerical quadrature using $n_q$ quadrature points, yielding a sum of $n_q$ local contributions. Assume:\n- Each quadrature-point contribution evaluation and its accumulation to the global internal force (including constitutive updates, strain-displacement application, and assembly) costs a constant $c_e$ floating-point operations (flops) independent of $N$, $r$, and $n_q$.\n- A dense matrix-vector multiplication of size $N \\times r$ costs $2 N r$ flops.\n- In the hyper-reduction setting, a sample-based empirical quadrature or interpolation is used with $s$ sampled quadrature points and a precomputed linear reconstruction operator $T \\in \\mathbb{R}^{N \\times s}$ such that the full internal force is approximated from the $s$ sampled contributions via a linear map. Assume evaluating the $s$ samples costs $s c_e$ flops, applying the precomputed sampled operator $(\\text{size } s \\times r)$ to $q$ costs $2 r s$ flops, and lifting the $s$ contributions to an $N$-vector via $T$ costs $2 N s$ flops.\n- All offline precomputations have been completed and incur no online cost.\n\nStarting from the definition of the internal force as a quadrature sum and the above cost model, derive the online floating-point operation counts for evaluating $f_{\\text{int}}(V q)$:\n- without hyper-reduction, and\n- with hyper-reduction.\n\nThen, provide the closed-form analytic expression for the speedup factor $S$ defined as the ratio of the non-hyper-reduced online cost to the hyper-reduced online cost, expressed in terms of $n_q$, $N$, $r$, $s$, and $c_e$. Your final answer must be a single analytic expression for $S$. Do not simplify by assuming any asymptotic regime. No rounding is required.", "solution": "The problem statement has been rigorously validated.\n\n### Step 1: Extract Givens\n- System: Nonlinear finite element method (FEM) semi-discrete model.\n- Full-order degrees of freedom (DOFs): $N$.\n- Internal force vector: $f_{\\text{int}}(u) \\in \\mathbb{R}^{N}$.\n- Reduced basis: $V \\in \\mathbb{R}^{N \\times r}$.\n- Reduced coordinates: $q \\in \\mathbb{R}^{r}$.\n- Reduced-order model (ROM) state approximation: $u \\approx V q$.\n- Number of quadrature points for full model: $n_q$.\n- Cost per quadrature point evaluation and assembly: $c_e$ flops.\n- Cost of a dense matrix-vector multiplication of size $N \\times r$: $2 N r$ flops.\n- Number of sampled quadrature points for hyper-reduction: $s$.\n- Hyper-reduction reconstruction operator: $T \\in \\mathbb{R}^{N \\times s}$.\n- Cost component 1 (hyper-reduction): Evaluating $s$ samples costs $s c_e$ flops.\n- Cost component 2 (hyper-reduction): Applying a precomputed sampled operator of size $s \\times r$ to $q$ costs $2 r s$ flops.\n- Cost component 3 (hyper-reduction): Lifting $s$ contributions to an $N$-vector via $T$ costs $2 N s$ flops.\n- Offline precomputation cost is zero online.\n- Objective: Derive the floating-point operation counts for evaluating $f_{\\text{int}}(V q)$ with and without hyper-reduction, and then the speedup factor $S$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. Its premises are consistent with the established theory of model order reduction and hyper-reduction techniques for nonlinear problems in computational mechanics. The cost models provided are standard simplifications used for complexity analysis. The problem is self-contained and provides all necessary information to derive the required expressions. There are no contradictions, ambiguities, or factual inaccuracies.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will proceed with a complete derivation.\n\nThe objective is to determine the speedup factor $S$, defined as the ratio of the online computational cost without hyper-reduction to the cost with hyper-reduction. We denote these costs as $C_{\\text{full}}$ and $C_{\\text{hyper}}$, respectively.\n\nFirst, we derive the cost for the non-hyper-reduced evaluation of the internal force vector, $f_{\\text{int}}(V q)$. This evaluation involves two sequential steps:\n1.  Compute the full-order state approximation vector, $u = V q$. This is a matrix-vector product between the reduced basis matrix $V \\in \\mathbb{R}^{N \\times r}$ and the reduced coordinate vector $q \\in \\mathbb{R}^{r}$. Based on the provided cost model, the number of floating-point operations (flops) for this operation is $2 N r$.\n2.  Evaluate the nonlinear internal force vector $f_{\\text{int}}(u)$ for the state $u$. The problem states that this is performed using numerical quadrature over the entire domain, involving $n_q$ quadrature points. The cost for evaluating the contribution of each quadrature point and accumulating it into the global force vector is given as a constant $c_e$. Therefore, the total cost for this step is the number of points multiplied by the cost per point, which is $n_q c_e$.\n\nThe total online cost for the non-hyper-reduced case, $C_{\\text{full}}$, is the sum of the costs of these two steps:\n$$C_{\\text{full}} = n_q c_e + 2 N r$$\n\nNext, we derive the cost for the hyper-reduced evaluation. The problem statement explicitly partitions the online cost of the hyper-reduced evaluation into three distinct components:\n1.  Evaluating the nonlinear function at $s$ sampled quadrature points, which costs $s c_e$ flops.\n2.  Applying a precomputed operator to the reduced coordinates $q$ to obtain necessary inputs for the sampled evaluations, which costs $2 r s$ flops.\n3.  Lifting the $s$ evaluated force contributions to reconstruct the full $N$-dimensional internal force vector, which costs $2 N s$ flops.\n\nThe total online cost for the hyper-reduced case, $C_{\\text{hyper}}$, is the sum of these three defined components.\n$$C_{\\text{hyper}} = s c_e + 2 r s + 2 N s$$\nThis cost is independent of the full-system number of quadrature points $n_q$, which is the central purpose of hyper-reduction.\n\nFinally, the speedup factor $S$ is defined as the ratio of the non-hyper-reduced cost to the hyper-reduced cost.\n$$S = \\frac{C_{\\text{full}}}{C_{\\text{hyper}}}$$\nSubstituting the derived expressions for $C_{\\text{full}}$ and $C_{\\text{hyper}}$ gives the final closed-form expression for the speedup:\n$$S = \\frac{n_q c_e + 2 N r}{s c_e + 2 r s + 2 N s}$$\nThe expression can also be written by factoring out $s$ in the denominator, but this is not required.\n$$S = \\frac{n_q c_e + 2 N r}{s(c_e + 2 r + 2 N)}$$\nThis expression provides the theoretical speedup achieved by the specified hyper-reduction scheme as a function of the problem parameters $n_q$, $N$, $r$, $s$, and the unit cost $c_e$.", "answer": "$$\n\\boxed{\\frac{n_q c_e + 2 N r}{s c_e + 2 r s + 2 N s}}\n$$", "id": "2566979"}, {"introduction": "Having established the computational motivation for hyper-reduction, we now delve into the mechanics of how these methods operate at the solver level. The Gauss-Newton with Approximated Tensors (GNAT) method is a cornerstone technique that constructs a small, inexpensive linear system to solve for the update at each nonlinear iteration. This practice problem [@problem_id:2566919] offers a concrete, step-by-step calculation of a single GNAT update, demystifying the interplay between the trial basis $\\Phi$, residual basis $Z$, and the sampling matrix $P$ to build a tangible understanding of the algorithm.", "problem": "Consider the nonlinear algebraic residual $R(u) \\in \\mathbb{R}^{3}$ arising from a finite element discretization, where $u \\in \\mathbb{R}^{3}$ and\n$$\nR(u) \\equiv \n\\begin{pmatrix}\nu_{1} + u_{2}^{2} - 1 \\\\\n2 u_{1} - u_{3} + \\sin(u_{2}) \\\\\nu_{1} u_{3} - \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\nLet the trial basis for the reduced-order model (ROM) be $\\Phi \\in \\mathbb{R}^{3 \\times 2}$ and the residual basis be $Z \\in \\mathbb{R}^{3 \\times 2}$, given by\n$$\n\\Phi = \n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & -1\n\\end{pmatrix},\n\\qquad\nZ =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nLet the sampling matrix $P \\in \\mathbb{R}^{3 \\times 2}$ select the first two residual entries, so that $P^{T} \\in \\mathbb{R}^{2 \\times 3}$ is\n$$\nP^{T} = \n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nAssume the current full-state iterate is\n$$\nu^{k} = \n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n1\n\\end{pmatrix}.\n$$\nUsing the Gauss–Newton with Approximated Tensors (GNAT) hyper-reduction at $u^{k}$ with the given sampling matrix $P$ and residual basis $Z$ (you may take the reference state to be $0$ so that the reduced correction acts through $\\Phi$), form the hyper-reduced Gauss–Newton update for the reduced coordinates $\\Delta a \\in \\mathbb{R}^{2}$ by minimizing the sampled residual in the Gauss–Newton linearization at $u^{k}$, and compute the resulting reduced correction. Report the reduced correction $\\Delta a$ as a row vector. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**1. Givens Extraction:**\n- Nonlinear residual: $R(u) = \\begin{pmatrix} u_{1} + u_{2}^{2} - 1 \\\\ 2 u_{1} - u_{3} + \\sin(u_{2}) \\\\ u_{1} u_{3} - \\frac{1}{2} \\end{pmatrix} \\in \\mathbb{R}^{3}$ for $u \\in \\mathbb{R}^{3}$.\n- Trial basis: $\\Phi = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$.\n- Residual basis: $Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$.\n- Sampling matrix transpose: $P^{T} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}$.\n- Current state iterate: $u^{k} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{3}$.\n- The task is to compute the reduced correction $\\Delta a \\in \\mathbb{R}^2$ using the Gauss-Newton with Approximated Tensors (GNAT) method.\n\n**2. Validation:**\nThe problem is set within the established field of numerical analysis for model order reduction. All mathematical objects ($R(u)$, $\\Phi$, $Z$, $P$, $u^k$) are well-defined, and their dimensions are consistent for the operations involved. The problem asks for the application of a specific, albeit simplified, version of the GNAT algorithm. The terminology is standard within the discipline. There are no scientific contradictions, no missing information, and no subjective elements. The problem is therefore deemed valid.\n\n**3. Solution:**\nThe Gauss-Newton method is an iterative procedure for solving nonlinear systems of equations, such as $R(u)=0$. At each iteration $k$, it solves a linear system for a correction $\\Delta u$, based on a first-order Taylor expansion of the residual at the current iterate $u^k$:\n$$R(u^k + \\Delta u) \\approx R(u^k) + J(u^k) \\Delta u$$\nwhere $J(u^k)$ is the Jacobian of $R(u)$ evaluated at $u^k$. The update is found by setting the linearized residual to zero: $J(u^k) \\Delta u = -R(u^k)$.\n\nIn a reduced-order model (ROM), the state correction is restricted to the subspace spanned by the columns of the trial basis $\\Phi$, so $\\Delta u = \\Phi \\Delta a$, where $\\Delta a \\in \\mathbb{R}^2$ is the reduced correction vector. The linearized residual becomes:\n$$R_{lin}(\\Delta a) = R(u^k) + J(u^k) \\Phi \\Delta a$$\n\nThe GNAT hyper-reduction technique constructs a smaller, computationally inexpensive system. As specified, this involves using the sampling matrix $P$ and the residual basis $Z$. We apply a Petrov-Galerkin projection to the sampled linearized residual, $P^T R_{lin}(\\Delta a)$. The test basis for this projection is derived from the residual basis $Z$ and the sampling matrix $P$, giving the test operator $(P^T Z)^T$. The resulting hyper-reduced system is:\n$$ (P^T Z)^T \\left( P^T R(u^k) + P^T J(u^k) \\Phi \\Delta a \\right) = 0 $$\nThis can be rearranged into a linear system for the unknown reduced correction $\\Delta a$:\n$$ \\left((P^T Z)^T P^T J(u^k) \\Phi\\right) \\Delta a = - (P^T Z)^T P^T R(u^k) $$\n\nWe now compute the necessary components. First, the residual at the current iterate $u^k = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\n$$ R(u^k) = \\begin{pmatrix} 1 + 0^2 - 1 \\\\ 2(1) - 1 + \\sin(0) \\\\ (1)(1) - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix} $$\nNext, we compute the Jacobian matrix $J(u) = \\frac{\\partial R}{\\partial u}$:\n$$ J(u) = \\begin{pmatrix} 1 & 2u_2 & 0 \\\\ 2 & \\cos(u_2) & -1 \\\\ u_3 & 0 & u_1 \\end{pmatrix} $$\nEvaluating at $u^k$:\n$$ J(u^k) = \\begin{pmatrix} 1 & 2(0) & 0 \\\\ 2 & \\cos(0) & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\n\nNow we compute the matrices for the linear system.\nThe term involving the test basis is:\n$$ P^T Z = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nSince $P^T Z$ is the $2 \\times 2$ identity matrix $I_2$, so is its transpose. The linear system for $\\Delta a$ simplifies to:\n$$ \\left(P^T J(u^k) \\Phi\\right) \\Delta a = - P^T R(u^k) $$\nThis simplification arises because the choice of sampling points and residual basis leads to a collocation condition for the projected test basis. This is also the system one would obtain by finding the exact solution to the sampled linearized system, i.e., minimizing $\\| P^T R(u^k) + P^T J(u^k) \\Phi \\Delta a \\|_2^2$, since the matrix $P^T J(u^k) \\Phi$ is square and invertible.\n\nWe compute the matrix $P^T J(u^k) \\Phi$:\n$$ J(u^k) \\Phi = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 2-1 & 1+1 \\\\ 1+1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\\\ 2 & -1 \\end{pmatrix} $$\n$$ P^T (J(u^k) \\Phi) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\end{pmatrix} $$\nAnd the right-hand side vector $-P^T R(u^k)$:\n$$ P^T R(u^k) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\nSo, the linear system for $\\Delta a = \\begin{pmatrix} \\Delta a_1 \\\\ \\Delta a_2 \\end{pmatrix}$ is:\n$$ \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\Delta a_1 \\\\ \\Delta a_2 \\end{pmatrix} = - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} $$\nThis system is solved by forward substitution. The first equation is:\n$$ 1 \\cdot \\Delta a_1 + 0 \\cdot \\Delta a_2 = 0 \\implies \\Delta a_1 = 0 $$\nSubstituting this into the second equation:\n$$ 1 \\cdot \\Delta a_1 + 2 \\cdot \\Delta a_2 = -1 \\implies 1(0) + 2 \\Delta a_2 = -1 \\implies 2 \\Delta a_2 = -1 \\implies \\Delta a_2 = -\\frac{1}{2} $$\nThe reduced correction vector is $\\Delta a = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$. The problem requires the answer as a row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & -\\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "2566919"}, {"introduction": "Achieving computational speedup is only useful if the resulting reduced model remains physically meaningful and numerically stable. When applying hyper-reduction to dynamic systems, such as in nonlinear elastodynamics, preserving the energy dissipation properties of the original system is paramount to avoid unphysical behavior like spontaneous energy growth. This problem [@problem_id:2566977] critically examines the Energy-Conserving Sampling and Weighting (ECSW) method, challenging you to analyze how certain choices in the approximation (e.g., negative weights $w_e$) can compromise stability and how to design the offline stage to rigorously guarantee a stable reduced-order model.", "problem": "Consider a semi-discrete nonlinear elastodynamic finite element model with proportional Rayleigh damping, defined on nodal displacement vector $\\mathbf{u}(t) \\in \\mathbb{R}^{n}$:\n$$\n\\mathbf{M}\\,\\ddot{\\mathbf{u}} + \\mathbf{C}(\\mathbf{u})\\,\\dot{\\mathbf{u}} + \\mathbf{f}_{\\mathrm{int}}(\\mathbf{u}) = \\mathbf{f}_{\\mathrm{ext}}(t),\n$$\nwhere $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\mathbf{C}(\\mathbf{u}) = \\alpha\\,\\mathbf{M} + \\beta\\,\\mathbf{K}(\\mathbf{u})$ with $\\alpha \\ge 0$ and $\\beta \\ge 0$, and $\\mathbf{K}(\\mathbf{u})$ is the symmetric tangent stiffness associated with the internal force $\\mathbf{f}_{\\mathrm{int}}(\\mathbf{u}) = \\nabla_{\\mathbf{u}} \\Pi(\\mathbf{u})$ for a stored energy $\\Pi(\\mathbf{u})$. The discrete total energy is\n$$\n\\mathcal{E}(\\mathbf{u},\\dot{\\mathbf{u}}) = \\tfrac{1}{2}\\,\\dot{\\mathbf{u}}^{T}\\mathbf{M}\\dot{\\mathbf{u}} + \\Pi(\\mathbf{u}),\n$$\nso that in the unforced case $\\mathbf{f}_{\\mathrm{ext}} = \\mathbf{0}$ one has the energy dissipation identity\n$$\n\\dot{\\mathcal{E}} = -\\,\\dot{\\mathbf{u}}^{T}\\mathbf{C}(\\mathbf{u})\\,\\dot{\\mathbf{u}} \\le 0.\n$$\n\nA Reduced Order Model (ROM) is constructed by a Galerkin projection with trial/test basis $\\mathbf{V} \\in \\mathbb{R}^{n \\times r}$, where $r \\ll n$, so that $\\mathbf{u}(t) \\approx \\mathbf{V}\\mathbf{a}(t)$ with reduced coordinates $\\mathbf{a}(t) \\in \\mathbb{R}^{r}$. The reduced mass is $\\mathbf{M}_{r} = \\mathbf{V}^{T}\\mathbf{M}\\mathbf{V}$ and, if no hyper-reduction is applied to mass, $\\mathbf{M}_{r}$ is symmetric positive definite. To reduce online cost, the Energy-Conserving Sampling and Weighting (ECSW) hyper-reduction approximates nonlinear internal forces and stiffness by a weighted sum over a sampled element set $\\mathcal{S}$:\n$$\n\\mathbf{f}_{\\mathrm{int},r}(\\mathbf{a}) \\approx \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{f}_{e,r}(\\mathbf{a}), \\qquad \n\\mathbf{K}_{r}(\\mathbf{a}) \\approx \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a}),\n$$\nwhere $\\mathbf{f}_{e,r}(\\mathbf{a})$ and $\\mathbf{K}_{e,r}(\\mathbf{a})$ are, respectively, the element-level reduced internal force contribution and tangent stiffness associated with element $e$, and $w_{e} \\in \\mathbb{R}$ are constant weights identified offline. Assume that for the state region of interest each $\\mathbf{K}_{e,r}(\\mathbf{a})$ is symmetric positive semidefinite and each elemental stored energy $\\Pi_{e,r}(\\mathbf{a})$ is convex, so that $\\mathbf{f}_{e,r}(\\mathbf{a}) = \\nabla_{\\mathbf{a}}\\Pi_{e,r}(\\mathbf{a})$. The stiffness-proportional damping in the ROM is hyper-reduced using the same weights:\n$$\n\\mathbf{C}_{r}(\\mathbf{a}) = \\alpha\\,\\mathbf{M}_{r} + \\beta\\,\\mathbf{K}_{r}(\\mathbf{a}) \\approx \\alpha\\,\\mathbf{M}_{r} + \\beta \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a}).\n$$\nThe corresponding reduced energy is\n$$\n\\mathcal{E}_{r}(\\mathbf{a},\\dot{\\mathbf{a}}) = \\tfrac{1}{2}\\,\\dot{\\mathbf{a}}^{T}\\mathbf{M}_{r}\\dot{\\mathbf{a}} + \\tilde{\\Pi}_{r}(\\mathbf{a}), \\qquad \\tilde{\\Pi}_{r}(\\mathbf{a}) := \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\Pi_{e,r}(\\mathbf{a}).\n$$\n\nAnalyze the consequences of allowing some $w_{e} < 0$ on the discrete energy stability of the unforced ROM, and identify principled constraints or regularizations during offline weight identification that enforce positivity while preserving the dissipation inequality. Select all statements that are correct under the assumptions above.\n\nA. If negative weights are used to hyper-reduce the stiffness-proportional part of the Rayleigh damping, the reduced damping matrix can lose positive semidefiniteness, and the unforced reduced model can violate the discrete energy dissipation inequality, i.e., there exist states with $\\dot{\\mathcal{E}}_{r} > 0$.\n\nB. Enforcing $w_{e} \\ge 0$ in the offline identification and using the same nonnegative weights to approximate both $\\mathbf{f}_{\\mathrm{int},r}$ and the stiffness-proportional damping is sufficient to guarantee that, in the unforced case with $\\alpha \\ge 0$ and $\\beta \\ge 0$, one has $\\dot{\\mathcal{E}}_{r} \\le 0$ for all reduced states.\n\nC. Adding only a Tikhonov $\\ell_{2}$ regularization term on the weights in the offline least-squares training, while allowing $w_{e}$ to be negative, guarantees energy stability of the unforced reduced model.\n\nD. Parameterizing the weights as $w_{e} = \\exp(z_{e})$ and solving the offline least-squares problem in the unconstrained variables $z_{e}$ (optionally with an $\\ell_{2}$ penalty) enforces positivity by construction; if the same positive weights are used for the stiffness-proportional damping, the discrete energy dissipation inequality of the unforced ROM is preserved.\n\nE. Orthonormalizing the reduced basis with respect to mass, i.e., choosing $\\mathbf{V}$ such that $\\mathbf{V}^{T}\\mathbf{M}\\mathbf{V} = \\mathbf{I}$, eliminates the possibility of energy growth due to negative weights, so explicit positivity enforcement on $w_{e}$ is unnecessary.", "solution": "The problem statement has been validated as scientifically sound, well-posed, and self-consistent. We may proceed with the analysis.\n\nThe central issue is the stability of the unforced, hyper-reduced Reduced Order Model (ROM), which is governed by the energy dissipation inequality. The equation of motion for the unforced ROM is:\n$$\n\\mathbf{M}_{r}\\,\\ddot{\\mathbf{a}} + \\mathbf{C}_{r}(\\mathbf{a})\\,\\dot{\\mathbf{a}} + \\tilde{\\mathbf{f}}_{\\mathrm{int},r}(\\mathbf{a}) = \\mathbf{0}\n$$\nThe reduced total energy is given by $\\mathcal{E}_{r}(\\mathbf{a},\\dot{\\mathbf{a}}) = \\tfrac{1}{2}\\,\\dot{\\mathbf{a}}^{T}\\mathbf{M}_{r}\\dot{\\mathbf{a}} + \\tilde{\\Pi}_{r}(\\mathbf{a})$. We compute its time derivative:\n$$\n\\dot{\\mathcal{E}}_{r} = \\frac{d}{dt}\\left( \\tfrac{1}{2}\\,\\dot{\\mathbf{a}}^{T}\\mathbf{M}_{r}\\dot{\\mathbf{a}} + \\tilde{\\Pi}_{r}(\\mathbf{a}) \\right) = \\dot{\\mathbf{a}}^{T}\\mathbf{M}_{r}\\ddot{\\mathbf{a}} + (\\nabla_{\\mathbf{a}}\\tilde{\\Pi}_{r}(\\mathbf{a}))^{T}\\dot{\\mathbf{a}}\n$$\nwhere we have used the symmetry of the reduced mass matrix $\\mathbf{M}_{r}$. The problem statement provides a consistent formulation where the hyper-reduced internal force is the gradient of the hyper-reduced potential energy, $\\tilde{\\mathbf{f}}_{\\mathrm{int},r}(\\mathbf{a}) = \\nabla_{\\mathbf{a}}\\tilde{\\Pi}_{r}(\\mathbf{a})$. Substituting this into the energy rate expression yields:\n$$\n\\dot{\\mathcal{E}}_{r} = \\dot{\\mathbf{a}}^{T}\\mathbf{M}_{r}\\ddot{\\mathbf{a}} + \\tilde{\\mathbf{f}}_{\\mathrm{int},r}(\\mathbf{a})^{T}\\dot{\\mathbf{a}} = \\dot{\\mathbf{a}}^{T} \\left( \\mathbf{M}_{r}\\ddot{\\mathbf{a}} + \\tilde{\\mathbf{f}}_{\\mathrm{int},r}(\\mathbf{a}) \\right)\n$$\nFrom the equation of motion, we substitute $\\mathbf{M}_{r}\\ddot{\\mathbf{a}} + \\tilde{\\mathbf{f}}_{\\mathrm{int},r}(\\mathbf{a}) = -\\mathbf{C}_{r}(\\mathbf{a})\\,\\dot{\\mathbf{a}}$. This gives the rate of energy change:\n$$\n\\dot{\\mathcal{E}}_{r} = \\dot{\\mathbf{a}}^{T} \\left( -\\mathbf{C}_{r}(\\mathbf{a})\\,\\dot{\\mathbf{a}} \\right) = -\\dot{\\mathbf{a}}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\dot{\\mathbf{a}}\n$$\nWe have used the fact that $\\mathbf{C}_{r}(\\mathbf{a})$ is symmetric, since $\\mathbf{M}_{r}$ and all $\\mathbf{K}_{e,r}(\\mathbf{a})$ are given as symmetric. The discrete energy dissipation inequality, $\\dot{\\mathcal{E}}_{r} \\le 0$, is satisfied for all possible motions if and only if the quadratic form $\\dot{\\mathbf{a}}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\dot{\\mathbf{a}} \\ge 0$ for any vector $\\dot{\\mathbf{a}} \\in \\mathbb{R}^{r}$. This is the definition of the reduced damping matrix $\\mathbf{C}_{r}(\\mathbf{a})$ being symmetric positive semidefinite (SPSD).\n\nThe structure of the reduced damping matrix is:\n$$\n\\mathbf{C}_{r}(\\mathbf{a}) = \\alpha\\,\\mathbf{M}_{r} + \\beta \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})\n$$\nWe are given that $\\alpha \\ge 0$, $\\beta \\ge 0$, $\\mathbf{M}_{r}$ is symmetric positive definite (SPD), and each elemental reduced stiffness $\\mathbf{K}_{e,r}(\\mathbf{a})$ is SPSD. The matrix $\\alpha\\,\\mathbf{M}_{r}$ is therefore SPSD. The definiteness of $\\mathbf{C}_{r}(\\mathbf{a})$ depends critically on the term $\\beta \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$, and thus on the signs of the weights $w_{e}$.\n\nLet us analyze each option.\n\n**A. If negative weights are used to hyper-reduce the stiffness-proportional part of the Rayleigh damping, the reduced damping matrix can lose positive semidefiniteness, and the unforced reduced model can violate the discrete energy dissipation inequality, i.e., there exist states with $\\dot{\\mathcal{E}}_{r} > 0$.**\n\nThe term $\\mathbf{K}_{\\mathrm{hyp},r}(\\mathbf{a}) := \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$ is a linear combination of SPSD matrices $\\mathbf{K}_{e,r}(\\mathbf{a})$. If one or more weights $w_{e}$ are negative, this combination is not guaranteed to be SPSD. It is possible to construct a state $\\mathbf{a}$ and a vector $\\mathbf{v} \\in \\mathbb{R}^{r}$ such that $\\mathbf{v}^{T}\\mathbf{K}_{\\mathrm{hyp},r}(\\mathbf{a})\\mathbf{v} < 0$. Consequently, the quadratic form for the full damping matrix, $\\mathbf{v}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\mathbf{v} = \\alpha\\,\\mathbf{v}^{T}\\mathbf{M}_{r}\\mathbf{v} + \\beta\\,\\mathbf{v}^{T}\\mathbf{K}_{\\mathrm{hyp},r}(\\mathbf{a})\\mathbf{v}$, can become negative if the second term is negative and large enough in magnitude to overcome the first positive term. If $\\mathbf{C}_{r}(\\mathbf{a})$ is not SPSD, then for some reduced velocity $\\dot{\\mathbf{a}}$, we have $\\dot{\\mathbf{a}}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\dot{\\mathbf{a}} < 0$. This would lead to $\\dot{\\mathcal{E}}_{r} = -\\dot{\\mathbf{a}}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\dot{\\mathbf{a}} > 0$, indicating a spurious generation of energy and instability. The statement is therefore a correct description of the potential consequences.\n\n**Verdict: Correct**\n\n**B. Enforcing $w_{e} \\ge 0$ in the offline identification and using the same nonnegative weights to approximate both $\\mathbf{f}_{\\mathrm{int},r}$ and the stiffness-proportional damping is sufficient to guarantee that, in the unforced case with $\\alpha \\ge 0$ and $\\beta \\ge 0$, one has $\\dot{\\mathcal{E}}_{r} \\le 0$ for all reduced states.**\n\nIf all weights $w_{e} \\ge 0$, then the matrix $\\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$ becomes a non-negative linear combination of SPSD matrices. Such a combination is itself SPSD. The full reduced damping matrix $\\mathbf{C}_{r}(\\mathbf{a}) = \\alpha\\,\\mathbf{M}_{r} + \\beta\\,\\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$ is then the sum of two SPSD matrices (since $\\alpha\\mathbf{M}_r$ is SPSD), which results in an SPSD matrix. Therefore, $\\dot{\\mathbf{a}}^{T}\\mathbf{C}_{r}(\\mathbf{a})\\dot{\\mathbf{a}} \\ge 0$ for all $\\dot{\\mathbf{a}}$, which guarantees $\\dot{\\mathcal{E}}_{r} \\le 0$. The non-negativity of weights is a sufficient condition for preserving the dissipation inequality.\n\n**Verdict: Correct**\n\n**C. Adding only a Tikhonov $\\ell_{2}$ regularization term on the weights in the offline least-squares training, while allowing $w_{e}$ to be negative, guarantees energy stability of the unforced reduced model.**\n\nTikhonov, or $\\ell_{2}$, regularization adds a penalty proportional to the sum of the squares of the weights, $\\lambda \\sum_{e} w_e^2$, to the objective function being minimized. This discourages large weights but does not constrain their sign. The optimal weights $w_e$ can still be negative. As established in the analysis of option A, the presence of negative weights can lead to a violation of the energy dissipation inequality. Therefore, $\\ell_{2}$ regularization alone provides no guarantee of stability.\n\n**Verdict: Incorrect**\n\n**D. Parameterizing the weights as $w_{e} = \\exp(z_{e})$ and solving the offline least-squares problem in the unconstrained variables $z_{e}$ (optionally with an $\\ell_{2}$ penalty) enforces positivity by construction; if the same positive weights are used for the stiffness-proportional damping, the discrete energy dissipation inequality of the unforced ROM is preserved.**\n\nThe parameterization $w_{e} = \\exp(z_{e})$ maps any real variable $z_{e}$ to a strictly positive weight $w_{e}$. By optimizing over the unconstrained variables $z_e$, we inherently enforce the constraint $w_{e} > 0$ for all $e$. Since $w_{e} > 0$ implies $w_{e} \\ge 0$, this method produces weights that satisfy the sufficient condition for stability analyzed in option B. Using these positive weights for both the internal force and the stiffness-proportional damping ensures the reduced damping matrix $\\mathbf{C}_{r}(\\mathbf{a})$ is SPSD, thus preserving the dissipation inequality $\\dot{\\mathcal{E}}_{r} \\le 0$. An optional penalty on $z_e$ would influence the resulting weight values but would not change their positivity.\n\n**Verdict: Correct**\n\n**E. Orthonormalizing the reduced basis with respect to mass, i.e., choosing $\\mathbf{V}$ such that $\\mathbf{V}^{T}\\mathbf{M}\\mathbf{V} = \\mathbf{I}$, eliminates the possibility of energy growth due to negative weights, so explicit positivity enforcement on $w_{e}$ is unnecessary.**\n\nOrthonormalizing the basis with respect to the mass matrix simplifies the reduced mass matrix to the aidentity, $\\mathbf{M}_{r} = \\mathbf{I}$. The reduced damping matrix becomes $\\mathbf{C}_{r}(\\mathbf{a}) = \\alpha\\,\\mathbf{I} + \\beta \\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$. While this simplifies the mass-proportional term, it does not affect the stiffness-proportional term, which is the source of the potential instability. If negative weights $w_e$ are used, the matrix $\\sum_{e \\in \\mathcal{S}} w_{e}\\,\\mathbf{K}_{e,r}(\\mathbf{a})$ can still have negative eigenvalues. If this term has a minimum eigenvalue $\\lambda_{\\min} < 0$, the minimum eigenvalue of $\\mathbf{C}_{r}(\\mathbf{a})$ would be $\\alpha + \\beta\\lambda_{\\min}$, which can be negative. A change of basis does not resolve the fundamental algebraic problem caused by negative coefficients in the linear combination of SPSD matrices. The risk of energy growth remains, and explicit enforcement of weight positivity is still required to guarantee stability.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{ABD}$$", "id": "2566977"}]}