{"hands_on_practices": [{"introduction": "This practice begins at the very foundation of meshfree methods: the construction of shape functions using the Moving Least Squares (MLS) approximation. By working through a concrete one-dimensional example, you will build the moment matrix, compute shape function values, and directly verify the method's ability to exactly reproduce polynomials present in its basis. This hands-on calculation [@problem_id:2576499] is essential for demystifying the abstract theory and building an intuition for why methods like the Element-Free Galerkin (EFG) and Reproducing Kernel Particle Method (RKPM) are powerful and accurate.", "problem": "Consider the one-dimensional moving least squares (MLS) approximation that underlies both the Element-Free Galerkin (EFG) method and the Reproducing Kernel Particle Method (RKPM). Let the nodes be uniformly distributed on the real line at positions $x_I = I h_n$ with spacing $h_n = 0.1$, where $I \\in \\mathbb{Z}$. Use the quadratic polynomial basis $\\mathbf{p}(x) = [1\\ \\ x\\ \\ x^2]^{T}$ and the standard cubic spline weight function with compact support of radius $2 h_n$, defined by\n$$\nW(q) = \n\\begin{cases}\n\\frac{2}{3} - q^2 + \\frac{1}{2} q^3, & 0 \\le q < 1, \\\\\n\\frac{1}{6} (2 - q)^3, & 1 \\le q < 2, \\\\\n0, & q \\ge 2,\n\\end{cases}\n$$\nwhere $q = \\dfrac{|x - x_I|}{h_n}$. For a given evaluation point $x_0 = 0.3$, denote by $\\phi_I(x)$ the MLS shape functions associated with the nodes $\\{x_I\\}$.\n\nStarting from the MLS normal equations and their induced consistency conditions, and without assuming any interpolation property a priori, do the following:\n\n1) Determine the subset of nodes with nonzero weight at $x_0 = 0.3$.\n\n2) Using the MLS construction with the given basis and weight, compute explicitly the values $\\phi_I(x_0)$ for the nodes found in part 1).\n\n3) For the function $u(x) = x^2$, construct the MLS approximation $u_h(x_0) = \\sum_I \\phi_I(x_0) u(x_I)$ and verify quadratic reproduction at $x_0 = 0.3$.\n\nExpress your final answer as a single exact number for $u_h(0.3)$ (no units, no rounding required).", "solution": "The Moving Least Squares (MLS) approximation $u_h(x)$ of a function $u(x)$ is constructed locally at an evaluation point $x$ as $u_h(x) = \\mathbf{p}^T(x)\\mathbf{a}(x)$, where $\\mathbf{p}(x)$ is a basis of polynomials and $\\mathbf{a}(x)$ is a vector of coefficients determined by minimizing a weighted, discrete $L_2$ norm. This leads to the MLS shape functions $\\phi_I(x)$ such that $u_h(x) = \\sum_I \\phi_I(x) u_I$, where $u_I = u(x_I)$ are the nodal values of the function.\n\n1) Determine the subset of nodes with nonzero weight at $x_0 = 0.3$.\n\nThe weight function $W(q)$ has compact support, being non-zero only for $0 \\le q < 2$. The argument $q$ is defined as $q = \\frac{|x - x_I|}{h_n}$. For a given evaluation point $x = x_0 = 0.3$ and node spacing $h_n = 0.1$, the condition for a node $x_I = I h_n = 0.1 I$ to have a non-zero weight is:\n$$\n\\frac{|x_0 - x_I|}{h_n} < 2\n$$\n$$\n\\frac{|0.3 - 0.1 I|}{0.1} < 2\n$$\n$$\n|3 - I| < 2\n$$\nThis inequality is equivalent to $-2 < 3 - I < 2$.\nFrom $3 - I < 2$, we obtain $I > 1$.\nFrom $-2 < 3 - I$, we obtain $I < 5$.\nThus, the integer indices $I$ must satisfy $1 < I < 5$. The set of qualifying indices is $\\{2, 3, 4\\}$. The corresponding nodes, which constitute the support domain for the evaluation point $x_0 = 0.3$, are:\n$$\nx_2 = 2 \\times 0.1 = 0.2\n$$\n$$\nx_3 = 3 \\times 0.1 = 0.3\n$$\n$$\nx_4 = 4 \\times 0.1 = 0.4\n$$\n\n2) Compute the values $\\phi_I(x_0)$ for the nodes found in part 1).\n\nThe MLS shape function for node $I$ is given by the expression:\n$$\n\\phi_I(x) = \\mathbf{p}^T(x) \\mathbf{M}^{-1}(x) \\mathbf{p}(x_I) W_I(x)\n$$\nwhere $W_I(x) = W(\\frac{|x-x_I|}{h_n})$ and the moment matrix $\\mathbf{M}(x)$ is defined as:\n$$\n\\mathbf{M}(x) = \\sum_J W_J(x) \\mathbf{p}(x_J) \\mathbf{p}^T(x_J)\n$$\nThe sum is over all nodes $J$ in the support domain. The given basis is $\\mathbf{p}(x) = [1, x, x^2]^T$. To simplify the inversion of the moment matrix, we introduce a shifted basis centered at the evaluation point $x_0 = 0.3$:\n$$\n\\mathbf{q}(x) = \\begin{pmatrix} 1 \\\\ x - x_0 \\\\ (x - x_0)^2 \\end{pmatrix}\n$$\nThe shape functions are invariant under this change of basis. In the new basis, the shape function is $\\phi_I(x) = \\mathbf{q}^T(x) \\mathbf{A}^{-1}(x) \\mathbf{q}(x_I) W_I(x)$, with the moment matrix $\\mathbf{A}(x) = \\sum_J W_J(x) \\mathbf{q}(x_J) \\mathbf{q}^T(x_J)$.\n\nWe evaluate at $x = x_0 = 0.3$.\nFirst, calculate the weights $W_I(x_0)$ for the active nodes $I \\in \\{2, 3, 4\\}$.\nFor $I=2$: $q_2 = \\frac{|0.3 - 0.2|}{0.1} = 1$. $W_2(x_0) = \\frac{1}{6}(2 - 1)^3 = \\frac{1}{6}$.\nFor $I=3$: $q_3 = \\frac{|0.3 - 0.3|}{0.1} = 0$. $W_3(x_0) = \\frac{2}{3} - 0^2 + \\frac{1}{2}0^3 = \\frac{2}{3}$.\nFor $I=4$: $q_4 = \\frac{|0.3 - 0.4|}{0.1} = 1$. $W_4(x_0) = \\frac{1}{6}(2 - 1)^3 = \\frac{1}{6}$.\n\nNext, we construct the moment matrix $\\mathbf{A}(x_0)$. Let $y_I = x_I - x_0$.\n$y_2 = 0.2 - 0.3 = -0.1$.\n$y_3 = 0.3 - 0.3 = 0$.\n$y_4 = 0.4 - 0.3 = 0.1$.\nThe entries of $\\mathbf{A}(x_0)$ are $A_{ij} = \\sum_{I=2,3,4} W_I(x_0) y_I^{i-1} y_I^{j-1}$. Due to the symmetry of the node distribution and weights around $x_0$, the off-diagonal terms involving odd powers of $y_I$ will be zero.\n$A_{11} = \\sum W_I = \\frac{1}{6} + \\frac{2}{3} + \\frac{1}{6} = 1$.\n$A_{12} = A_{21} = \\sum W_I y_I = \\frac{1}{6}(-0.1) + \\frac{2}{3}(0) + \\frac{1}{6}(0.1) = 0$.\n$A_{13} = A_{31} = A_{22} = \\sum W_I y_I^2 = \\frac{1}{6}(-0.1)^2 + \\frac{2}{3}(0)^2 + \\frac{1}{6}(0.1)^2 = 2 \\times \\frac{1}{6}(0.01) = \\frac{0.01}{3} = \\frac{1}{300}$.\n$A_{23} = A_{32} = \\sum W_I y_I^3 = \\frac{1}{6}(-0.1)^3 + \\frac{2}{3}(0)^3 + \\frac{1}{6}(0.1)^3 = 0$.\n$A_{33} = \\sum W_I y_I^4 = \\frac{1}{6}(-0.1)^4 + \\frac{2}{3}(0)^4 + \\frac{1}{6}(0.1)^4 = 2 \\times \\frac{1}{6}(0.0001) = \\frac{1}{30000}$.\n\nSo, the moment matrix is:\n$$\n\\mathbf{A}(x_0) = \\begin{pmatrix} 1 & 0 & \\frac{1}{300} \\\\ 0 & \\frac{1}{300} & 0 \\\\ \\frac{1}{300} & 0 & \\frac{1}{30000} \\end{pmatrix}\n$$\nIts inverse is:\n$$\n\\mathbf{A}^{-1}(x_0) = \\begin{pmatrix} \\frac{3}{2} & 0 & -150 \\\\ 0 & 300 & 0 \\\\ -150 & 0 & 45000 \\end{pmatrix}\n$$\nThe basis vector evaluated at $x_0$ is $\\mathbf{q}(x_0) = [1, 0, 0]^T$. The shape function values are:\n$$\n\\phi_I(x_0) = \\mathbf{q}^T(x_0) \\mathbf{A}^{-1}(x_0) \\mathbf{q}(x_I) W_I(x_0) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\mathbf{A}^{-1}(x_0) \\begin{pmatrix} 1 \\\\ y_I \\\\ y_I^2 \\end{pmatrix} W_I(x_0)\n$$\n$$\n\\phi_I(x_0) = \\begin{pmatrix} \\frac{3}{2} & 0 & -150 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ y_I \\\\ y_I^2 \\end{pmatrix} W_I(x_0) = \\left(\\frac{3}{2} - 150 y_I^2\\right) W_I(x_0)\n$$\nNow we compute the values for $I \\in \\{2, 3, 4\\}$.\nFor $I=2$: $y_2 = -0.1$. $\\phi_2(0.3) = \\left(\\frac{3}{2} - 150(-0.1)^2\\right) \\frac{1}{6} = (1.5 - 1.5) \\frac{1}{6} = 0$.\nFor $I=3$: $y_3 = 0$. $\\phi_3(0.3) = \\left(\\frac{3}{2} - 150(0)^2\\right) \\frac{2}{3} = \\frac{3}{2} \\times \\frac{2}{3} = 1$.\nFor $I=4$: $y_4 = 0.1$. $\\phi_4(0.3) = \\left(\\frac{3}{2} - 150(0.1)^2\\right) \\frac{1}{6} = (1.5 - 1.5) \\frac{1}{6} = 0$.\nThe shape function values are $\\phi_2(0.3) = 0$, $\\phi_3(0.3) = 1$, and $\\phi_4(0.3) = 0$. This indicates that the MLS approximation possesses the Kronecker delta property at this specific nodal location, $\\phi_I(x_J) = \\delta_{IJ}$.\n\n3) Construct the MLS approximation for $u(x) = x^2$ and verify quadratic reproduction.\n\nThe MLS approximation of $u(x) = x^2$ at $x_0 = 0.3$ is given by:\n$$\nu_h(x_0) = \\sum_{I=2,3,4} \\phi_I(x_0) u(x_I)\n$$\nThe nodal values of the function $u(x) = x^2$ are:\n$u(x_2) = (0.2)^2 = 0.04$.\n$u(x_3) = (0.3)^2 = 0.09$.\n$u(x_4) = (0.4)^2 = 0.16$.\n\nSubstituting the shape function values and nodal values:\n$$\nu_h(0.3) = \\phi_2(0.3) u(x_2) + \\phi_3(0.3) u(x_3) + \\phi_4(0.3) u(x_4)\n$$\n$$\nu_h(0.3) = (0)(0.04) + (1)(0.09) + (0)(0.16) = 0.09\n$$\nTo verify quadratic reproduction, we must compare the approximated value $u_h(x_0)$ with the exact value $u(x_0)$.\nThe exact value is $u(0.3) = (0.3)^2 = 0.09$.\nSince $u_h(0.3) = u(0.3) = 0.09$, quadratic reproduction is verified at the point $x_0 = 0.3$. This result is expected, as the MLS method is constructed to reproduce any polynomial that is present in its basis, a property known as consistency. Given that $u(x)=x^2$ is an element of the chosen quadratic basis $\\mathbf{p}(x)$, its exact reproduction is guaranteed by the formulation.\nThe final required value is $u_h(0.3)$.", "answer": "$$\n\\boxed{0.09}\n$$", "id": "2576499"}, {"introduction": "Building upon the properties of meshfree shape functions, this exercise explores their role within the Element-Free Galerkin (EFG) framework for solving differential equations. You will first discover a remarkable identity satisfied by the stiffness matrix, which arises directly from the consistency of the shape function derivatives. This practice [@problem_id:2576530] bridges the gap between approximation theory and practical implementation, illustrating how theoretical guarantees translate into elegant properties of the discrete system and how numerical integration must be handled with care.", "problem": "Consider the one-dimensional second-order elliptic boundary value problem on the open interval $(0,1)$,\n$$\n-\\frac{d}{dx}\\!\\left(k(x)\\,\\frac{du}{dx}\\right)=f(x),\n$$\ndiscretized by the Element-Free Galerkin (EFG) method (also valid for the Reproducing Kernel Particle Method (RKPM)) using a moving least-squares trial space with linear polynomial basis, hence achieving linear completeness (first-order reproduction). Let $\\{N_i(x)\\}_{i=1}^{n}$ denote the meshfree shape functions associated with nodes at coordinates $\\{x_i\\}_{i=1}^{n}$, which satisfy the linear reproducing conditions\n$$\n\\sum_{i=1}^{n} N_i(x) = 1,\\qquad \\sum_{i=1}^{n} x_i\\,N_i(x) = x,\\qquad x\\in(0,1).\n$$\nThe symmetric stiffness matrix entries are given by\n$$\nK_{ij}=\\int_{0}^{1} k(x)\\,\\frac{dN_i}{dx}(x)\\,\\frac{dN_j}{dx}(x)\\,dx,\\qquad 1\\le i,j\\le n.\n$$\nDefine the coordinate vector $x:=(x_1,\\dots,x_n)^{T}\\in\\mathbb{R}^{n}$. Using only the foregoing reproducing conditions and standard Galerkin principles, first show that under exact integration one has the identity\n$$\nx^{T}K\\,x=\\int_{0}^{1} k(x)\\,dx.\n$$\nNow, approximate the stiffness entries by background cell Gaussian quadrature: partition $(0,1)$ into two equal background cells $[0,\\frac{1}{2}]$ and $[\\frac{1}{2},1]$, and on each cell apply the two-point Gauss rule (with standard abscissae and weights mapped from $[-1,1]$). Denote by $\\widehat{Q}$ the numerically integrated scalar $x^{T}Kx$ obtained when replacing the exact integrals in $K_{ij}$ by this background quadrature. For the polynomial conductivity\n$$\nk(x)=1+2x+3x^{2}+4x^{3}+5x^{4},\n$$\ncompute the signed quadrature error\n$$\n\\widehat{Q}-\\int_{0}^{1} k(x)\\,dx,\n$$\nas an exact value. Express your final answer as a single exact real number (no units and no rounding are required).", "solution": "First, we prove the identity $x^{T}K\\,x=\\int_{0}^{1} k(x)\\,dx$ under exact integration.\nThe quadratic form $x^{T}Kx$ is expressed as:\n$$\nx^{T}K\\,x = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i K_{ij} x_j\n$$\nSubstituting the definition of $K_{ij}$:\n$$\nx^{T}K\\,x = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i \\left( \\int_{0}^{1} k(x)\\,\\frac{dN_i}{dx}(x)\\,\\frac{dN_j}{dx}(x)\\,dx \\right) x_j\n$$\nBy linearity of the integral and summations, we can interchange the order of operations:\n$$\nx^{T}K\\,x = \\int_{0}^{1} k(x) \\left( \\sum_{i=1}^{n} x_i \\frac{dN_i}{dx}(x) \\right) \\left( \\sum_{j=1}^{n} x_j \\frac{dN_j}{dx}(x) \\right) dx = \\int_{0}^{1} k(x) \\left( \\sum_{i=1}^{n} x_i \\frac{dN_i}{dx}(x) \\right)^2 dx\n$$\nThe key to simplifying this expression lies in the derivatives of the reproducing conditions. Differentiating the second condition, $\\sum_{i=1}^{n} x_i N_i(x) = x$, with respect to $x$ yields:\n$$\n\\frac{d}{dx} \\left( \\sum_{i=1}^{n} x_i N_i(x) \\right) = \\frac{d}{dx}(x)\n$$\n$$\n\\sum_{i=1}^{n} x_i \\frac{dN_i}{dx}(x) = 1\n$$\nThis is a fundamental property stemming from linear completeness, often termed a first-order consistency condition for the derivatives of the shape functions. Substituting this result into the integral expression for $x^{T}Kx$:\n$$\nx^{T}K\\,x = \\int_{0}^{1} k(x) (1)^2 dx = \\int_{0}^{1} k(x)\\,dx\n$$\nThis completes the proof of the identity.\n\nNext, we address the quantity $\\widehat{Q}$, which is $x^{T}Kx$ evaluated with numerical quadrature. Let the quadrature rule over the domain $(0,1)$ be represented by a set of quadrature points $\\{x_q\\}$ and corresponding weights $\\{w_q\\}$. The numerically integrated stiffness matrix, denoted $\\widehat{K}$, has entries:\n$$\n\\widehat{K}_{ij} = \\sum_{q} w_q k(x_q) \\frac{dN_i}{dx}(x_q) \\frac{dN_j}{dx}(x_q)\n$$\nThe quantity $\\widehat{Q}$ is then:\n$$\n\\widehat{Q} = x^{T}\\widehat{K}x = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i \\widehat{K}_{ij} x_j = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i \\left( \\sum_{q} w_q k(x_q) \\frac{dN_i}{dx}(x_q) \\frac{dN_j}{dx}(x_q) \\right) x_j\n$$\nAgain, rearranging the summations:\n$$\n\\widehat{Q} = \\sum_{q} w_q k(x_q) \\left( \\sum_{i=1}^{n} x_i \\frac{dN_i}{dx}(x_q) \\right) \\left( \\sum_{j=1}^{n} x_j \\frac{dN_j}{dx}(x_q) \\right)\n$$\nThe consistency condition $\\sum_{i=1}^{n} x_i \\frac{dN_i}{dx}(x) = 1$ holds for any point $x$ in the domain, including the quadrature points $x_q$. Therefore:\n$$\n\\widehat{Q} = \\sum_{q} w_q k(x_q) (1) (1) = \\sum_{q} w_q k(x_q)\n$$\nThis shows that $\\widehat{Q}$ is precisely the numerical approximation of the integral $\\int_{0}^{1} k(x)\\,dx$ using the specified quadrature rule. The problem thus reduces to computing the quadrature error for the integral of $k(x)$ over $(0,1)$. The desired quantity is:\n$$\n\\text{Error} = \\widehat{Q} - \\int_{0}^{1} k(x)\\,dx = \\left( \\sum_{q} w_q k(x_q) \\right) - \\int_{0}^{1} k(x)\\,dx\n$$\nThe function is $k(x)=1+2x+3x^{2}+4x^{3}+5x^{4}$. The two-point Gaussian quadrature rule is exact for polynomials of degree up to $2n-1 = 2(2)-1 = 3$. Our quadrature scheme applies this rule over two subintervals. Any polynomial of degree up to $3$ will be integrated exactly over each subinterval, and thus over the entire domain $(0,1)$.\nThe error calculation simplifies significantly:\n$$\n\\text{Error} = \\left( \\sum_{q} w_q k(x_q) - \\int_{0}^{1} k(x)\\,dx \\right)\n$$\nLet $k(x) = p_3(x) + 5x^4$, where $p_3(x) = 1+2x+3x^2+4x^3$.\n$$\n\\text{Error} = \\left( \\sum_{q} w_q p_3(x_q) - \\int_{0}^{1} p_3(x)\\,dx \\right) + \\left( \\sum_{q} w_q (5x_q^4) - \\int_{0}^{1} 5x^4\\,dx \\right)\n$$\nThe first term is zero due to the exactness of the quadrature rule for polynomials of degree $3$. We are left with:\n$$\n\\text{Error} = 5 \\left( \\left( \\sum_{q} w_q x_q^4 \\right) - \\int_{0}^{1} x^4\\,dx \\right)\n$$\nFirst, the exact integral:\n$$\n\\int_{0}^{1} x^4\\,dx = \\left[ \\frac{x^5}{5} \\right]_{0}^{1} = \\frac{1}{5}\n$$\nNow, the numerical integral. The standard two-point Gauss rule on $[-1,1]$ has points $\\xi_{\\pm} = \\pm 1/\\sqrt{3}$ and weights $w_{\\pm} = 1$.\nCell 1: $[0, \\frac{1}{2}]$. The mapping is $x(\\xi) = \\frac{1}{4}(\\xi+1)$. The Jacobian is $J_1 = 1/4$.\nQuadrature points: $x_1 = \\frac{1}{4}(1 - 1/\\sqrt{3})$, $x_2 = \\frac{1}{4}(1 + 1/\\sqrt{3})$.\nWeights: $W_1 = w_{-} J_1 = 1/4$, $W_2 = w_{+} J_1 = 1/4$.\nCell 2: $[\\frac{1}{2}, 1]$. The mapping is $x(\\xi) = \\frac{1}{4}(\\xi+3)$. The Jacobian is $J_2 = 1/4$.\nQuadrature points: $x_3 = \\frac{1}{4}(3 - 1/\\sqrt{3})$, $x_4 = \\frac{1}{4}(3 + 1/\\sqrt{3})$.\nWeights: $W_3 = w_{-} J_2 = 1/4$, $W_4 = w_{+} J_2 = 1/4$.\n\nThe numerical integral of $x^4$ is:\n$$\n\\sum_{q=1}^{4} W_q x_q^4 = \\frac{1}{4} (x_1^4 + x_2^4 + x_3^4 + x_4^4)\n$$\nLet $s=1/\\sqrt{3}$ so $s^2=1/3$.\n$x_{1,2} = \\frac{1}{4}(1 \\mp s)$. $x_{3,4} = \\frac{1}{4}(3 \\mp s)$.\n$x_1^4+x_2^4 = \\frac{1}{4^4} \\left( (1-s)^4 + (1+s)^4 \\right) = \\frac{2}{256} (1+6s^2+s^4) = \\frac{1}{128} \\left(1 + 6(\\frac{1}{3}) + (\\frac{1}{3})^2 \\right) = \\frac{1}{128} (1+2+\\frac{1}{9}) = \\frac{1}{128} (\\frac{28}{9}) = \\frac{7}{288}$.\n$x_3^4+x_4^4 = \\frac{1}{4^4} \\left( (3-s)^4 + (3+s)^4 \\right) = \\frac{2}{256} (3^4+6(3^2)s^2+s^4) = \\frac{1}{128} (81 + 54(\\frac{1}{3}) + \\frac{1}{9}) = \\frac{1}{128} (81+18+\\frac{1}{9}) = \\frac{1}{128} (99+\\frac{1}{9}) = \\frac{1}{128} (\\frac{891+1}{9}) = \\frac{1}{128} (\\frac{892}{9}) = \\frac{223}{288}$.\nThe sum is:\n$$\n\\sum_{q=1}^{4} x_q^4 = (x_1^4+x_2^4) + (x_3^4+x_4^4) = \\frac{7}{288} + \\frac{223}{288} = \\frac{230}{288} = \\frac{115}{144}\n$$\nThe numerical integral of $x^4$ is:\n$$\n\\sum_{q=1}^{4} W_q x_q^4 = \\frac{1}{4} \\cdot \\frac{115}{144} = \\frac{115}{576}\n$$\nNow we compute the error for the $5x^4$ term:\n$$\n\\text{Error} = 5 \\left( \\frac{115}{576} - \\frac{1}{5} \\right) = 5 \\left( \\frac{115 \\cdot 5 - 576}{576 \\cdot 5} \\right) = \\frac{575 - 576}{576} = -\\frac{1}{576}\n$$\nThis is the final result for the signed quadrature error $\\widehat{Q}-\\int_{0}^{1} k(x)\\,dx$.", "answer": "$$\n\\boxed{-\\frac{1}{576}}\n$$", "id": "2576530"}, {"introduction": "A robust numerical method requires an understanding not only of how it works, but also how it can fail. This exercise presents a classic challenge in meshfree methods: the loss of accuracy and stability due to poor nodal arrangements, leading to an ill-conditioned moment matrix. By analyzing a specific case of near-collinear nodes and evaluating potential remedies, you will develop the critical skills needed to diagnose and resolve practical implementation issues [@problem_id:2576516], ensuring your meshfree simulations are both stable and reliable.", "problem": "Consider the Moving Least Squares (MLS) construction that underlies both the Element-Free Galerkin method (EFG) and the Reproducing Kernel Particle Method (RKPM) for a scalar field $u(\\boldsymbol{x})$ in two spatial dimensions. Let the polynomial basis at a query point $\\boldsymbol{x}^\\ast$ be linear, $p(\\boldsymbol{x}) = \\begin{bmatrix} 1 & x & y \\end{bmatrix}^{\\top}$, and let the MLS weight associated with a neighbor node $a$ be a positive scalar $w_a(\\boldsymbol{x}^\\ast) = w\\!\\left(\\lVert \\boldsymbol{x}^\\ast - \\boldsymbol{x}_a \\rVert\\right)$. The normal equations for the MLS coefficients $a(\\boldsymbol{x}^\\ast) \\in \\mathbb{R}^3$ are\n$$\nM(\\boldsymbol{x}^\\ast)\\, a(\\boldsymbol{x}^\\ast) \\;=\\; b(\\boldsymbol{x}^\\ast),\n$$\nwith the moment matrix and right-hand side defined by\n$$\nM(\\boldsymbol{x}^\\ast) \\;=\\; \\sum_{a} w_a(\\boldsymbol{x}^\\ast)\\, p(\\boldsymbol{x}_a)\\, p(\\boldsymbol{x}_a)^{\\top}, \n\\qquad\nb(\\boldsymbol{x}^\\ast) \\;=\\; \\sum_{a} w_a(\\boldsymbol{x}^\\ast)\\, p(\\boldsymbol{x}_a)\\, u_a,\n$$\nwhere $u_a = u(\\boldsymbol{x}_a)$ are the nodal samples. Exact first-order completeness (exact reproduction of any linear polynomial) at $\\boldsymbol{x}^\\ast$ requires that $M(\\boldsymbol{x}^\\ast)$ be invertible; if $u(\\boldsymbol{x}) = c_0 + c_1 x + c_2 y$ for constants $c_0,c_1,c_2$, then $b(\\boldsymbol{x}^\\ast) = M(\\boldsymbol{x}^\\ast)\\, c$ with $c = \\begin{bmatrix} c_0 & c_1 & c_2 \\end{bmatrix}^{\\top}$, so the unique solution is $a(\\boldsymbol{x}^\\ast)=c$.\n\nConstruct the following specific, scientifically plausible scenario in a neighborhood of $\\boldsymbol{x}^\\ast = (0,0)$ that is prone to geometric degeneracy. There are exactly three neighbor nodes used in the fit:\n$$\n\\boldsymbol{x}_1 = (-1, 0), \n\\qquad\n\\boldsymbol{x}_2 = (0, 0), \n\\qquad\n\\boldsymbol{x}_3 = (1, \\epsilon),\n$$\nwith a small parameter $\\epsilon > 0$. Assume a radial weight with width chosen so that the three neighbors above dominate and have weights\n$$\nw_1(\\boldsymbol{x}^\\ast) \\;=\\; w_3(\\boldsymbol{x}^\\ast) \\;=\\; \\omega, \n\\qquad\nw_2(\\boldsymbol{x}^\\ast) \\;=\\; 1,\n$$\nwith $\\omega \\in (0,1)$ a fixed constant. Throughout, use the linear basis $p(\\boldsymbol{x}) = \\begin{bmatrix} 1 & x & y \\end{bmatrix}^{\\top}$ unless otherwise stated.\n\nBased on the definitions above, one computes\n$$\np(\\boldsymbol{x}_1) = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}, \n\\quad\np(\\boldsymbol{x}_2) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \n\\quad\np(\\boldsymbol{x}_3) = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\epsilon \\end{bmatrix},\n$$\nand hence\n$$\nM(\\boldsymbol{x}^\\ast) \n= \\sum_{a=1}^{3} w_a\\, p(\\boldsymbol{x}_a) p(\\boldsymbol{x}_a)^{\\top}\n= \n\\begin{bmatrix}\n1 + 2\\omega & 0 & \\omega \\epsilon \\\\\n0 & 2\\omega & \\omega \\epsilon \\\\\n\\omega \\epsilon & \\omega \\epsilon & \\omega \\epsilon^2\n\\end{bmatrix}.\n$$\n\nNow consider the following potential remedies in the same neighborhood $\\boldsymbol{x}^\\ast$:\n\n(i) Support enlargement: include one additional neighbor at $\\boldsymbol{x}_4 = (0,\\delta)$ with weight $w_4(\\boldsymbol{x}^\\ast) = \\tilde{\\omega} > 0$ for some $\\delta \\neq 0$, and otherwise keep the setup unchanged.\n\n(ii) Basis reduction: replace the linear basis by the constant basis $p(\\boldsymbol{x}) = \\begin{bmatrix} 1 \\end{bmatrix}$, keeping the original three neighbors only.\n\n(iii) Orthonormal basis change: replace $p(\\boldsymbol{x})$ by $\\tilde{p}(\\boldsymbol{x}) = Q p(\\boldsymbol{x})$ with an orthonormal matrix $Q \\in \\mathbb{R}^{3 \\times 3}$, keeping the original three neighbors and weights.\n\n(iv) Tikhonov regularization: replace the normal equations by $\\left(M(\\boldsymbol{x}^\\ast) + \\lambda I\\right) a_\\lambda(\\boldsymbol{x}^\\ast) = b(\\boldsymbol{x}^\\ast)$ with a small $\\lambda > 0$.\n\nSelect all statements below that are correct, and justify your selection by deriving them from the definitions above.\n\nA. For the given three-node configuration with the linear basis, one has $\\det M(\\boldsymbol{x}^\\ast) = \\omega^{2} \\epsilon^{2}$, so as $\\epsilon \\to 0^{+}$ the moment matrix becomes singular and exact first-order completeness is lost at $\\boldsymbol{x}^\\ast$.\n\nB. Under (i) with the added node at $\\boldsymbol{x}_4 = (0,\\delta)$ of weight $\\tilde{\\omega} > 0$, the determinant at $\\epsilon=0$ equals $2\\omega \\tilde{\\omega} (1+2\\omega)\\, \\delta^{2} > 0$, so the linear basis is admissible and exact first-order completeness is restored at $\\boldsymbol{x}^\\ast$.\n\nC. Under (ii) with the constant basis and the original three neighbors, the resulting moment matrix is invertible and exact first-order completeness is preserved.\n\nD. Under (iii) with an orthonormal change of polynomial basis and the original three neighbors, the rank and condition number of the moment matrix are unchanged, so the near-singularity caused by near collinearity cannot be resolved in this way.\n\nE. Under (iv) with Tikhonov regularization and the original three neighbors, $\\left(M+\\lambda I\\right)$ is invertible but exact first-order completeness of MLS or RKPM at $\\boldsymbol{x}^\\ast$ is not preserved for any $\\lambda>0$.", "solution": "**Analysis of Options**\n\n**A. For the given three-node configuration with the linear basis, one has $\\det M(\\boldsymbol{x}^\\ast) = \\omega^{2} \\epsilon^{2}$, so as $\\epsilon \\to 0^{+}$ the moment matrix becomes singular and exact first-order completeness is lost at $\\boldsymbol{x}^\\ast$.**\n\nWe compute the determinant of the provided moment matrix:\n$$\nM(\\boldsymbol{x}^\\ast) = \n\\begin{bmatrix}\n1 + 2\\omega & 0 & \\omega \\epsilon \\\\\n0 & 2\\omega & \\omega \\epsilon \\\\\n\\omega \\epsilon & \\omega \\epsilon & \\omega \\epsilon^2\n\\end{bmatrix}\n$$\nUsing cofactor expansion along the first row:\n$$\n\\det M(\\boldsymbol{x}^\\ast) = (1 + 2\\omega) \\begin{vmatrix} 2\\omega & \\omega \\epsilon \\\\ \\omega \\epsilon & \\omega \\epsilon^2 \\end{vmatrix} - 0 \\cdot (...) + \\omega \\epsilon \\begin{vmatrix} 0 & 2\\omega \\\\ \\omega \\epsilon & \\omega \\epsilon \\end{vmatrix}\n$$\n$$\n\\det M(\\boldsymbol{x}^\\ast) = (1 + 2\\omega) \\left( (2\\omega)(\\omega \\epsilon^2) - (\\omega \\epsilon)(\\omega \\epsilon) \\right) + \\omega \\epsilon \\left( (0)(\\omega \\epsilon) - (2\\omega)(\\omega \\epsilon) \\right)\n$$\n$$\n\\det M(\\boldsymbol{x}^\\ast) = (1 + 2\\omega) (2\\omega^2 \\epsilon^2 - \\omega^2 \\epsilon^2) - 2\\omega^3 \\epsilon^2\n$$\n$$\n\\det M(\\boldsymbol{x}^\\ast) = (1 + 2\\omega) (\\omega^2 \\epsilon^2) - 2\\omega^3 \\epsilon^2\n$$\n$$\n\\det M(\\boldsymbol{x}^\\ast) = \\omega^2 \\epsilon^2 + 2\\omega^3 \\epsilon^2 - 2\\omega^3 \\epsilon^2 = \\omega^2 \\epsilon^2\n$$\nThe calculation is correct. As $\\epsilon \\to 0^+$, the three nodes $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3$ become collinear. In this limit, $\\det M(\\boldsymbol{x}^\\ast) \\to 0$. A matrix with a zero determinant is singular and thus not invertible. According to the problem definition, invertibility of the moment matrix is required for exact first-order completeness. Therefore, completeness is lost at $\\boldsymbol{x}^\\ast$ as the nodes become collinear.\nVerdict: **Correct**.\n\n**B. Under (i) with the added node at $\\boldsymbol{x}_4 = (0,\\delta)$ of weight $\\tilde{\\omega} > 0$, the determinant at $\\epsilon=0$ equals $2\\omega \\tilde{\\omega} (1+2\\omega)\\, \\delta^{2} > 0$, so the linear basis is admissible and exact first-order completeness is restored at $\\boldsymbol{x}^\\ast$.**\n\nWe consider remedy (i) at the degenerate limit where $\\epsilon=0$. The original moment matrix becomes:\n$$\nM_{\\text{orig}}(\\epsilon=0) = \n\\begin{bmatrix}\n1 + 2\\omega & 0 & 0 \\\\\n0 & 2\\omega & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\nThe new node is $\\boldsymbol{x}_4 = (0, \\delta)$ with weight $w_4 = \\tilde{\\omega}$. The basis vector at this node is $p(\\boldsymbol{x}_4) = \\begin{bmatrix} 1 & 0 & \\delta \\end{bmatrix}^\\top$. The contribution to the moment matrix is:\n$$\nM_{\\text{add}} = w_4 p(\\boldsymbol{x}_4) p(\\boldsymbol{x}_4)^\\top = \\tilde{\\omega} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\delta \\end{bmatrix} \\begin{bmatrix} 1 & 0 & \\delta \\end{bmatrix} = \\begin{bmatrix} \\tilde{\\omega} & 0 & \\tilde{\\omega}\\delta \\\\ 0 & 0 & 0 \\\\ \\tilde{\\omega}\\delta & 0 & \\tilde{\\omega}\\delta^2 \\end{bmatrix}\n$$\nThe new total moment matrix is $M_{\\text{new}} = M_{\\text{orig}}(\\epsilon=0) + M_{\\text{add}}$:\n$$\nM_{\\text{new}} = \\begin{bmatrix}\n1 + 2\\omega + \\tilde{\\omega} & 0 & \\tilde{\\omega}\\delta \\\\\n0 & 2\\omega & 0 \\\\\n\\tilde{\\omega}\\delta & 0 & \\tilde{\\omega}\\delta^2\n\\end{bmatrix}\n$$\nIts determinant is:\n$$\n\\det M_{\\text{new}} = (1 + 2\\omega + \\tilde{\\omega}) \\begin{vmatrix} 2\\omega & 0 \\\\ 0 & \\tilde{\\omega} \\delta^2 \\end{vmatrix} - 0 \\cdot (...) + \\tilde{\\omega}\\delta \\begin{vmatrix} 0 & 2\\omega \\\\ \\tilde{\\omega}\\delta & 0 \\end{vmatrix}\n$$\n$$\n\\det M_{\\text{new}} = (1 + 2\\omega + \\tilde{\\omega})(2\\omega \\tilde{\\omega} \\delta^2) + \\tilde{\\omega}\\delta(-2\\omega \\tilde{\\omega} \\delta)\n$$\n$$\n\\det M_{\\text{new}} = (2\\omega \\tilde{\\omega} \\delta^2)(1+2\\omega) + (2\\omega \\tilde{\\omega} \\delta^2)(\\tilde{\\omega}) - 2\\omega \\tilde{\\omega}^2 \\delta^2\n$$\n$$\n\\det M_{\\text{new}} = 2\\omega \\tilde{\\omega} (1+2\\omega) \\delta^2 + 2\\omega \\tilde{\\omega}^2 \\delta^2 - 2\\omega \\tilde{\\omega}^2 \\delta^2 = 2\\omega \\tilde{\\omega} (1+2\\omega) \\delta^2\n$$\nThe calculation in the statement is correct. Given that $\\omega \\in (0,1)$, $\\tilde{\\omega} > 0$, and $\\delta \\neq 0$ (so $\\delta^2 > 0$), we have $\\det M_{\\text{new}} > 0$. The matrix is invertible. Adding a non-collinear node breaks the geometric degeneracy, allowing the linear basis to be admissible and restoring exact first-order completeness.\nVerdict: **Correct**.\n\n**C. Under (ii) with the constant basis and the original three neighbors, the resulting moment matrix is invertible and exact first-order completeness is preserved.**\n\nUnder remedy (ii), the basis is reduced to the constant basis, $p(\\boldsymbol{x}) = \\begin{bmatrix} 1 \\end{bmatrix}$. The moment matrix becomes a $1 \\times 1$ matrix (a scalar):\n$$\nM(\\boldsymbol{x}^\\ast) = \\sum_{a=1}^3 w_a p(\\boldsymbol{x}_a) p(\\boldsymbol{x}_a)^\\top = w_1 (1)(1)^\\top + w_2 (1)(1)^\\top + w_3 (1)(1)^\\top\n$$\n$$\nM(\\boldsymbol{x}^\\ast) = \\omega + 1 + \\omega = 1+2\\omega\n$$\nSince $\\omega \\in (0,1)$, $M(\\boldsymbol{x}^\\ast) = 1+2\\omega > 1 > 0$, so the matrix is invertible. However, the statement claims that \"exact first-order completeness is preserved\". First-order completeness means the ability to exactly reproduce any linear polynomial. A constant basis can only guarantee the exact reproduction of constant functions (zeroth-order completeness). By reducing the basis from linear to constant, first-order completeness is explicitly lost, not preserved.\nVerdict: **Incorrect**.\n\n**D. Under (iii) with an orthonormal change of polynomial basis and the original three neighbors, the rank and condition number of the moment matrix are unchanged, so the near-singularity caused by near collinearity cannot be resolved in this way.**\n\nUnder remedy (iii), the new basis is $\\tilde{p}(\\boldsymbol{x}) = Q p(\\boldsymbol{x})$ where $Q$ is an orthonormal matrix ($Q Q^\\top = Q^\\top Q = I$). The new moment matrix $\\tilde{M}$ is:\n$$\n\\tilde{M} = \\sum_a w_a \\tilde{p}(\\boldsymbol{x}_a) \\tilde{p}(\\boldsymbol{x}_a)^\\top = \\sum_a w_a (Q p(\\boldsymbol{x}_a)) (Q p(\\boldsymbol{x}_a))^\\top = \\sum_a w_a Q p(\\boldsymbol{x}_a) p(\\boldsymbol{x}_a)^\\top Q^\\top\n$$\n$$\n\\tilde{M} = Q \\left( \\sum_a w_a p(\\boldsymbol{x}_a) p(\\boldsymbol{x}_a)^\\top \\right) Q^\\top = Q M Q^\\top\n$$\nThis is an orthogonal similarity transformation.\n- Rank: Since $Q$ is invertible, $\\text{rank}(\\tilde{M}) = \\text{rank}(M)$. The rank is invariant.\n- Condition Number (with respect to the Euclidean $2$-norm): For any matrix $A$ and orthogonal matrix $Q$, $\\text{cond}_2(Q A Q^\\top) = \\text{cond}_2(A)$. The condition number is also invariant.\nA change of basis is a purely algebraic manipulation. It does not alter the underlying geometric information content of the nodal configuration, which is the source of the ill-conditioning. If the original matrix $M$ is singular or nearly singular (high condition number), the transformed matrix $\\tilde{M}$ will have exactly the same properties. This remedy is therefore ineffective.\nVerdict: **Correct**.\n\n**E. Under (iv) with Tikhonov regularization and the original three neighbors, $\\left(M+\\lambda I\\right)$ is invertible but exact first-order completeness of MLS or RKPM at $\\boldsymbol{x}^\\ast$ is not preserved for any $\\lambda>0$.**\n\nUnder remedy (iv), we solve the regularized system $(M + \\lambda I) a_\\lambda = b$ with $\\lambda > 0$.\n- Invertibility: The matrix $M$ is symmetric and positive semi-definite, as $v^\\top M v = \\sum_a w_a (v^\\top p(\\boldsymbol{x}_a))^2 \\ge 0$. Its eigenvalues $\\mu_i$ are all non-negative. The eigenvalues of the regularized matrix $M+\\lambda I$ are $\\mu_i + \\lambda$. Since $\\mu_i \\ge 0$ and $\\lambda > 0$, all eigenvalues of $M+\\lambda I$ are strictly positive. A symmetric matrix with strictly positive eigenvalues is positive definite and thus invertible. This is true even if $M$ is singular.\n- Completeness: Exact first-order completeness requires that if the nodal values come from a linear polynomial, $u(\\boldsymbol{x}) = p(\\boldsymbol{x})^\\top c$, then the computed coefficients $a_\\lambda$ must be equal to $c$. For such a field, the right-hand side is $b = M c$. The regularized system becomes:\n$$\n(M + \\lambda I) a_\\lambda = M c\n$$\nThe solution for the coefficients is $a_\\lambda = (M + \\lambda I)^{-1} M c$.\nFor completeness, we require $a_\\lambda = c$, which implies $(M + \\lambda I)^{-1} M c = c$ for any vector $c$. This would mean $(M + \\lambda I)^{-1} M = I$. Pre-multiplying by $(M+\\lambda I)$ gives $M = M + \\lambda I$, which simplifies to $\\lambda I = 0$. This is only true if $\\lambda=0$. However, Tikhonov regularization is defined for $\\lambda > 0$. For any $\\lambda > 0$, $a_\\lambda \\neq c$ (unless $c=0$). The regularization scheme stabilizes the matrix inversion at the cost of introducing a bias, thereby losing the exact reproduction property.\nVerdict: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "2576516"}]}