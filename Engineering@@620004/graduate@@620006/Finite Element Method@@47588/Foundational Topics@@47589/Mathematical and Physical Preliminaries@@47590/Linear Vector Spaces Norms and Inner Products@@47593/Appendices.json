{"hands_on_practices": [{"introduction": "The inner product is the foundational structure of a Hilbert space, providing the geometric concepts of angle and orthogonality that are essential for defining projections and best approximations in the Finite Element Method. However, not every norm, or measure of distance, is generated by an inner product. This exercise challenges you to apply the parallelogram law, a crucial identity that serves as the definitive test for whether a norm is induced by an inner product. By working through this problem [@problem_id:2575281], you will develop a practical understanding of the unique geometric properties that distinguish Hilbert spaces from more general Banach spaces.", "problem": "Consider a conforming finite element subspace $V_h \\subset H_0^1(0,1)$ of dimension $2$ with basis functions $\\{\\varphi_1,\\varphi_2\\}$ chosen to be orthonormal with respect to the energy inner product associated with the Poisson operator, that is, $\\langle u,v \\rangle_E := \\int_0^1 u'(x)\\,v'(x)\\,\\mathrm{d}x$. For a coefficient vector $c = (c_1,c_2)^{\\top} \\in \\mathbb{R}^2$ associated with $u_h = c_1 \\varphi_1 + c_2 \\varphi_2$, the discrete energy norm equals the Euclidean norm of $c$, i.e., $\\|u_h\\|_E = \\|c\\|_2$. For a fixed parameter $\\gamma \\ge 0$, define on $\\mathbb{R}^2$ the norm\n$$\n\\|c\\|_{\\gamma} := \\|c\\|_2 + \\gamma \\|c\\|_1,\n$$\nwhere $\\|c\\|_2 := \\sqrt{c_1^2 + c_2^2}$ and $\\|c\\|_1 := |c_1| + |c_2|$. Using only the fundamental definitions of norms and inner products and the characterization that a norm is induced by an inner product if and only if it satisfies the parallelogram law, determine the unique value of $\\gamma \\ge 0$ (if any) for which $\\|\\cdot\\|_{\\gamma}$ is induced by an inner product on $\\mathbb{R}^2$. Additionally, construct an explicit counterexample demonstrating that the $1$-norm $\\|\\cdot\\|_1$ on $\\mathbb{R}^2$ does not satisfy the parallelogram identity. Express your final answer as the required value of $\\gamma$ (no units).", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It is rooted in the established mathematical theory of normed vector spaces. All definitions are provided, and the problem is self-contained and consistent. Therefore, a solution will be provided.\n\nThe initial context relating to the finite element method establishes that the Euclidean norm $\\|c\\|_2$ of the coefficient vector $c \\in \\mathbb{R}^2$ is physically meaningful, as it represents the energy norm $\\|u_h\\|_E$ of the corresponding function $u_h \\in V_h$. The core of the problem, however, is to determine for which value of the parameter $\\gamma \\ge 0$ the modified norm $\\|c\\|_{\\gamma} := \\|c\\|_2 + \\gamma \\|c\\|_1$ is induced by an inner product on $\\mathbb{R}^2$.\n\nAccording to the Jordan-von Neumann theorem, a norm on a vector space is induced by an inner product if and only if it satisfies the parallelogram law. For the norm $\\|\\cdot\\|_{\\gamma}$ on the vector space $\\mathbb{R}^2$, this identity is:\n$$\n\\|c+d\\|_{\\gamma}^2 + \\|c-d\\|_{\\gamma}^2 = 2\\left(\\|c\\|_{\\gamma}^2 + \\|d\\|_{\\gamma}^2\\right)\n$$\nThis must hold for all vectors $c, d \\in \\mathbb{R}^2$. To find a necessary condition on $\\gamma$, we can test this identity with a specific choice of vectors. Let us select the standard basis vectors $c = e_1 = (1, 0)^{\\top}$ and $d = e_2 = (0, 1)^{\\top}$.\n\nFirst, we compute the component norms for these vectors and their sum and difference.\nFor $c = e_1$:\n$\\|c\\|_2 = \\|e_1\\|_2 = \\sqrt{1^2 + 0^2} = 1$\n$\\|c\\|_1 = \\|e_1\\|_1 = |1| + |0| = 1$\n\nFor $d = e_2$:\n$\\|d\\|_2 = \\|e_2\\|_2 = \\sqrt{0^2 + 1^2} = 1$\n$\\|d\\|_1 = \\|e_2\\|_1 = |0| + |1| = 1$\n\nFor $c+d = (1, 1)^{\\top}$:\n$\\|c+d\\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}$\n$\\|c+d\\|_1 = |1| + |1| = 2$\n\nFor $c-d = (1, -1)^{\\top}$:\n$\\|c-d\\|_2 = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}$\n$\\|c-d\\|_1 = |1| + |-1| = 2$\n\nNow, we express the $\\|\\cdot\\|_{\\gamma}$ norms:\n$\\|c\\|_{\\gamma} = \\|c\\|_2 + \\gamma \\|c\\|_1 = 1 + \\gamma$\n$\\|d\\|_{\\gamma} = \\|d\\|_2 + \\gamma \\|d\\|_1 = 1 + \\gamma$\n$\\|c+d\\|_{\\gamma} = \\|c+d\\|_2 + \\gamma \\|c+d\\|_1 = \\sqrt{2} + 2\\gamma$\n$\\|c-d\\|_{\\gamma} = \\|c-d\\|_2 + \\gamma \\|c-d\\|_1 = \\sqrt{2} + 2\\gamma$\n\nSubstitute these into the parallelogram law:\nLeft-hand side (LHS):\n$$\n\\|c+d\\|_{\\gamma}^2 + \\|c-d\\|_{\\gamma}^2 = (\\sqrt{2} + 2\\gamma)^2 + (\\sqrt{2} + 2\\gamma)^2 = 2(\\sqrt{2} + 2\\gamma)^2\n$$\nRight-hand side (RHS):\n$$\n2\\left(\\|c\\|_{\\gamma}^2 + \\|d\\|_{\\gamma}^2\\right) = 2\\left((1+\\gamma)^2 + (1+\\gamma)^2\\right) = 4(1+\\gamma)^2\n$$\nEquating LHS and RHS gives:\n$$\n2(\\sqrt{2} + 2\\gamma)^2 = 4(1+\\gamma)^2\n$$\n$$\n(\\sqrt{2} + 2\\gamma)^2 = 2(1+\\gamma)^2\n$$\nExpanding both sides:\n$$\n2 + 4\\sqrt{2}\\gamma + 4\\gamma^2 = 2(1 + 2\\gamma + \\gamma^2) = 2 + 4\\gamma + 2\\gamma^2\n$$\nRearranging the terms to form an equation for $\\gamma$:\n$$\n(4-2)\\gamma^2 + (4\\sqrt{2}-4)\\gamma + (2-2) = 0\n$$\n$$\n2\\gamma^2 + 4(\\sqrt{2}-1)\\gamma = 0\n$$\n$$\n2\\gamma \\left(\\gamma + 2(\\sqrt{2}-1)\\right) = 0\n$$\nThis equation yields two possible solutions for $\\gamma$: $\\gamma = 0$ or $\\gamma = -2(\\sqrt{2}-1) = 2 - 2\\sqrt{2}$.\nSince $\\sqrt{2} > 1$, the second solution $2 - 2\\sqrt{2}$ is negative. The problem specifies the constraint $\\gamma \\ge 0$. Therefore, the only possible value for $\\gamma$ is $0$.\n\nIf $\\gamma = 0$, the norm becomes $\\|c\\|_0 = \\|c\\|_2 + 0 \\cdot \\|c\\|_1 = \\|c\\|_2$. The norm is the standard Euclidean norm. It is a fundamental result that the Euclidean norm is induced by the standard dot product $\\langle c, d \\rangle = c_1 d_1 + c_2 d_2$. Indeed, for any $c, d \\in \\mathbb{R}^2$:\n$$\n\\|c+d\\|_2^2 + \\|c-d\\|_2^2 = \\langle c+d, c+d \\rangle + \\langle c-d, c-d \\rangle\n$$\n$$\n= (\\langle c,c \\rangle + 2\\langle c,d \\rangle + \\langle d,d \\rangle) + (\\langle c,c \\rangle - 2\\langle c,d \\rangle + \\langle d,d \\rangle)\n$$\n$$\n= 2\\langle c,c \\rangle + 2\\langle d,d \\rangle = 2\\|c\\|_2^2 + 2\\|d\\|_2^2 = 2\\left(\\|c\\|_2^2 + \\|d\\|_2^2\\right)\n$$\nThe parallelogram law holds for all $c, d \\in \\mathbb{R}^2$ when $\\gamma = 0$. Since our initial analysis showed that $\\gamma = 0$ is the only non-negative possibility, it must be the unique solution.\n\nThe second task is to demonstrate that the $1$-norm, $\\|\\cdot\\|_1$, does not satisfy a parallelogram identity. We use the same specific vectors, $c = e_1$ and $d = e_2$. The parallelogram law for $\\|\\cdot\\|_1$ is $\\|c+d\\|_1^2 + \\|c-d\\|_1^2 = 2(\\|c\\|_1^2 + \\|d\\|_1^2)$.\nWe have previously calculated the required $1$-norms:\n$\\|c\\|_1 = 1$\n$\\|d\\|_1 = 1$\n$\\|c+d\\|_1 = 2$\n$\\|c-d\\|_1 = 2$\n\nSubstituting these values into the parallelogram law for the $1$-norm:\nLHS: $\\|c+d\\|_1^2 + \\|c-d\\|_1^2 = 2^2 + 2^2 = 4 + 4 = 8$.\nRHS: $2(\\|c\\|_1^2 + \\|d\\|_1^2) = 2(1^2 + 1^2) = 2(1 + 1) = 4$.\nSince LHS $= 8 \\neq 4 =$ RHS, the parallelogram law is not satisfied for this choice of vectors. This provides the required explicit counterexample and proves that the $1$-norm on $\\mathbb{R}^2$ is not induced by an inner product.", "answer": "$$\n\\boxed{0}\n$$", "id": "2575281"}, {"introduction": "In analyzing partial differential equations, we frequently use the space of square-integrable functions, $L^2(\\Omega)$, and the Sobolev space $H^1(\\Omega)$, which also accounts for the function's derivatives. The choice of inner product defines a space's topology, which in turn governs our notion of convergence. This pivotal hands-on practice [@problem_id:2575288] uses a classic example to demonstrate that a sequence of functions can converge in the $L^2$-norm but fail to converge in the stronger $H^1$-norm. This distinction is not merely academic; it is fundamental to understanding error estimates in FEM and why the energy norm is the natural setting for many physical problems.", "problem": "Let $\\Omega = (0,\\pi)$ and consider the Hilbert spaces $L^{2}(\\Omega)$ and $H^{1}_{0}(\\Omega)$ that arise in the finite element method (FEM). Define the sequence $\\{u_{n}\\}_{n\\in\\mathbb{N}} \\subset H^{1}_{0}(\\Omega)$ by $u_{n}(x) = \\dfrac{\\sin(n x)}{n}$ for $x \\in \\Omega$. Using only the canonical inner products and norms of $L^{2}(\\Omega)$ and $H^{1}(\\Omega)$ as the foundational starting point, do the following:\n- Compute $\\|u_{n}\\|_{L^{2}(\\Omega)}$, $\\|u_{n}'\\|_{L^{2}(\\Omega)}$, and $\\|u_{n}\\|_{H^{1}(\\Omega)}$ in closed form for each $n \\in \\mathbb{N}$.\n- Determine $\\lim_{n\\to\\infty}\\|u_{n}\\|_{L^{2}(\\Omega)}$ and $\\lim_{n\\to\\infty}\\|u_{n}\\|_{H^{1}(\\Omega)}$.\n- Explain, from the viewpoint of inner products, why convergence in $L^{2}(\\Omega)$ does not imply convergence in $H^{1}(\\Omega)$ for this sequence, making explicit the role of the derivative term in the $H^{1}(\\Omega)$ inner product in inducing a strictly stronger topology than that of $L^{2}(\\Omega)$.\n\nYour final answer must be the exact value (no rounding) of $\\lim_{n\\to\\infty}\\|u_{n}\\|_{H^{1}(\\Omega)}$ as a single closed-form expression without units.", "solution": "The problem presented is a standard exercise in functional analysis, specifically concerning the properties of Sobolev spaces. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a full solution will be provided.\n\nLet $\\Omega = (0, \\pi)$. The Hilbert spaces in question are $L^{2}(\\Omega)$ and $H^{1}_{0}(\\Omega)$. The canonical inner product on $L^{2}(\\Omega)$ for real-valued functions $u, v$ is given by\n$$ \\langle u, v \\rangle_{L^{2}} = \\int_{\\Omega} u(x)v(x) \\, dx $$\nThe induced norm is $\\|u\\|_{L^{2}(\\Omega)} = \\sqrt{\\langle u, u \\rangle_{L^{2}}}$.\n\nThe canonical inner product on the Sobolev space $H^{1}(\\Omega)$ is\n$$ \\langle u, v \\rangle_{H^{1}} = \\int_{\\Omega} (u(x)v(x) + u'(x)v'(x)) \\, dx = \\langle u, v \\rangle_{L^{2}} + \\langle u', v' \\rangle_{L^{2}} $$\nwhere $u'$ and $v'$ are the weak derivatives. The induced norm is $\\|u\\|_{H^{1}(\\Omega)} = \\sqrt{\\langle u, u \\rangle_{H^{1}}}$. The space $H^{1}_{0}(\\Omega)$ is the subspace of $H^{1}(\\Omega)$ consisting of functions with zero trace on the boundary $\\partial\\Omega$. The sequence is given by $u_{n}(x) = \\frac{\\sin(n x)}{n}$ for $n \\in \\mathbb{N}$. Since $u_{n}(0) = 0$ and $u_{n}(\\pi) = \\sin(n\\pi)/n = 0$, the condition for being in $H^{1}_{0}(\\Omega)$ is satisfied. The derivative is $u_{n}'(x) = \\cos(nx)$.\n\nFirst, we compute the norms as requested.\n\nThe squared $L^{2}$-norm of $u_{n}$ is:\n$$ \\|u_{n}\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} \\left( \\frac{\\sin(nx)}{n} \\right)^{2} dx = \\frac{1}{n^{2}} \\int_{0}^{\\pi} \\sin^{2}(nx) \\, dx $$\nUsing the trigonometric identity $\\sin^{2}(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$, the integral becomes:\n$$ \\int_{0}^{\\pi} \\sin^{2}(nx) \\, dx = \\int_{0}^{\\pi} \\frac{1 - \\cos(2nx)}{2} \\, dx = \\frac{1}{2} \\left[ x - \\frac{\\sin(2nx)}{2n} \\right]_{0}^{\\pi} = \\frac{1}{2} \\left( (\\pi - \\frac{\\sin(2n\\pi)}{2n}) - (0 - 0) \\right) = \\frac{\\pi}{2} $$\nThus, the squared norm is $\\|u_{n}\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2n^{2}}$. Taking the square root gives the norm:\n$$ \\|u_{n}\\|_{L^{2}(\\Omega)} = \\sqrt{\\frac{\\pi}{2n^{2}}} = \\frac{\\sqrt{\\pi}}{\\sqrt{2} n} $$\n\nNext, we compute the $L^{2}$-norm of the derivative, $u_{n}'(x) = \\cos(nx)$:\n$$ \\|u_{n}'\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} (\\cos(nx))^{2} dx = \\int_{0}^{\\pi} \\cos^{2}(nx) \\, dx $$\nUsing the identity $\\cos^{2}(\\theta) = \\frac{1 + \\cos(2\\theta)}{2}$, the integral becomes:\n$$ \\int_{0}^{\\pi} \\cos^{2}(nx) \\, dx = \\int_{0}^{\\pi} \\frac{1 + \\cos(2nx)}{2} \\, dx = \\frac{1}{2} \\left[ x + \\frac{\\sin(2nx)}{2n} \\right]_{0}^{\\pi} = \\frac{1}{2} \\left( (\\pi + \\frac{\\sin(2n\\pi)}{2n}) - (0 + 0) \\right) = \\frac{\\pi}{2} $$\nThe norm of the derivative is therefore constant for all $n \\in \\mathbb{N}$:\n$$ \\|u_{n}'\\|_{L^{2}(\\Omega)} = \\sqrt{\\frac{\\pi}{2}} $$\n\nNow, we compute the $H^{1}$-norm of $u_{n}$. By definition, the squared $H^{1}$-norm is the sum of the squared $L^{2}$-norms of the function and its derivative:\n$$ \\|u_{n}\\|_{H^{1}(\\Omega)}^{2} = \\|u_{n}\\|_{L^{2}(\\Omega)}^{2} + \\|u_{n}'\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2n^{2}} + \\frac{\\pi}{2} = \\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right) $$\nTaking the square root gives the norm:\n$$ \\|u_{n}\\|_{H^{1}(\\Omega)} = \\sqrt{\\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right)} $$\n\nSecond, we determine the limits as $n \\to \\infty$.\nFor the $L^{2}$-norm:\n$$ \\lim_{n\\to\\infty} \\|u_{n}\\|_{L^{2}(\\Omega)} = \\lim_{n\\to\\infty} \\frac{\\sqrt{\\pi}}{\\sqrt{2} n} = 0 $$\nFor the $H^{1}$-norm:\n$$ \\lim_{n\\to\\infty} \\|u_{n}\\|_{H^{1}(\\Omega)} = \\lim_{n\\to\\infty} \\sqrt{\\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right)} = \\sqrt{\\frac{\\pi}{2} \\left( 1 + 0 \\right)} = \\sqrt{\\frac{\\pi}{2}} $$\n\nThird, we explain the differing convergence behaviors.\nConvergence of a sequence $\\{v_{n}\\}$ to a limit $v$ in a normed space $(V, \\|\\cdot\\|_{V})$ is defined as $\\|v_{n} - v\\|_{V} \\to 0$ as $n \\to \\infty$.\nOur calculations show that $\\|u_{n} - 0\\|_{L^{2}(\\Omega)} \\to 0$. Therefore, the sequence $\\{u_{n}\\}$ converges to the zero function in the space $L^{2}(\\Omega)$.\nHowever, $\\|u_{n} - 0\\|_{H^{1}(\\Omega)} \\to \\sqrt{\\frac{\\pi}{2}} \\neq 0$. The sequence does not converge to the zero function in $H^{1}(\\Omega)$. In fact, the sequence does not converge at all in $H^{1}(\\Omega)$ because it is not a Cauchy sequence. For $n \\neq m$:\n$$ \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} (\\cos(nx) - \\cos(mx))^{2} dx = \\int_{0}^{\\pi} \\cos^{2}(nx) dx - 2\\int_{0}^{\\pi} \\cos(nx)\\cos(mx) dx + \\int_{0}^{\\pi} \\cos^{2}(mx) dx $$\nThe set of functions $\\{\\cos(kx)\\}_{k \\in \\mathbb{N}}$ is an orthogonal system on $(0, \\pi)$, so $\\int_{0}^{\\pi} \\cos(nx)\\cos(mx) dx = 0$ for $n \\neq m$. Thus,\n$$ \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2} - 0 + \\frac{\\pi}{2} = \\pi $$\nThis implies $\\|u_{n} - u_{m}\\|_{H^{1}(\\Omega)}^{2} \\geq \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\pi$, which means $\\|u_{n} - u_{m}\\|_{H^{1}(\\Omega)} \\geq \\sqrt{\\pi}$ for all $n \\neq m$. The sequence is not Cauchy and therefore cannot converge in the complete space $H^{1}_{0}(\\Omega)$.\n\nThe fundamental reason for this discrepancy lies in the definitions of the inner products and the topologies they induce. The $H^{1}$ inner product, $\\langle u, v \\rangle_{H^{1}} = \\langle u, v \\rangle_{L^{2}} + \\langle u', v' \\rangle_{L^{2}}$, contains an additional term that measures the \"energy\" of the derivatives. For convergence in $H^{1}(\\Omega)$, both the functions and their derivatives must converge in the $L^{2}$ sense.\nFor the sequence $u_{n}(x) = \\frac{\\sin(nx)}{n}$:\n- The function values are scaled by $\\frac{1}{n}$, causing $\\|u_{n}\\|_{L^{2}}$ to vanish as $n \\to \\infty$. The functions are \"squashed\" to zero.\n- The derivatives $u_{n}'(x) = \\cos(nx)$ are not scaled down. As $n$ increases, the functions become more oscillatory, and their gradients do not diminish in magnitude. The term $\\|u_{n}'\\|_{L^{2}}$ remains constant at $\\sqrt{\\frac{\\pi}{2}}$.\n\nThe topology of a normed space is determined by its open sets, which are unions of open balls $B_{r}(f) = \\{ g : \\|f-g\\| < r \\}$. The inequality $\\|v\\|_{L^{2}} \\leq \\|v\\|_{H^{1}}$ for any $v \\in H^{1}(\\Omega)$ implies that any open ball in the $H^{1}$-norm is a subset of the corresponding open ball in the $L^{2}$-norm: $B_{r}^{H^{1}}(f) \\subseteq B_{r}^{L^{2}}(f)$. This means that any open set in the $L^{2}$ topology is also an open set in the $H^{1}$ topology. The converse is not true; there are open sets in the $H^{1}$ topology (e.g., $B_{r}^{H^{1}}(0)$ for $r < \\sqrt{\\pi/2}$) that are not open in the $L^{2}$ topology. This makes the $H^{1}$ topology *strictly stronger* (or finer) than the $L^{2}$ topology. Convergence in the stronger topology ($H^{1}$) implies convergence in the weaker topology ($L^{2}$), but the converse is false. This sequence $\\{u_{n}\\}$ serves as the canonical counterexample. It converges in $L^{2}(\\Omega)$ but fails to converge in $H^{1}(\\Omega)$ because the derivative term in the $H^{1}$ inner product imposes a stricter requirement that the sequence does not satisfy.", "answer": "$$\\boxed{\\sqrt{\\frac{\\pi}{2}}}$$", "id": "2575288"}, {"introduction": "A central task in FEM is to find the \"best approximation\" of a function within a given finite-dimensional subspace. The concept of \"best,\" however, is not universal; it is defined by the inner product used to measure the approximation error. This exercise [@problem_id:2575243] provides a clear, computational demonstration of this principle by asking you to find two different \"best\" approximations for the same function—one using the $L^2$ inner product and another using the $H^1$ energy inner product. By deriving and comparing these distinct solutions, you will gain tangible insight into how the choice of inner product directly dictates the outcome of a projection, a direct parallel to the Galerkin method.", "problem": "Consider the interval $\\Omega = (0,1)$ and the conforming piecewise-linear finite element subspace $V_h \\subset H_0^1(\\Omega)$ built on the uniform mesh with nodes at $x_0 = 0$, $x_1 = \\frac{1}{3}$, $x_2 = \\frac{2}{3}$, and $x_3 = 1$, with essential (Dirichlet) boundary conditions enforced at $x_0$ and $x_3$. Let $\\{\\varphi_1,\\varphi_2\\}$ denote the standard hat basis functions associated with the interior nodes $x_1$ and $x_2$, respectively. Define two norms on $V_h$ via inner products:\n- The $L^2$ inner product $(v,w)_{L^2} = \\int_0^1 v(x) w(x)\\,dx$ with norm $\\|v\\|_{L^2} = \\sqrt{(v,v)_{L^2}}$.\n- The $H^1$ energy inner product $a(v,w) = \\int_0^1 v'(x) w'(x)\\,dx$ with induced seminorm $|v|_{H^1} = \\sqrt{a(v,v)}$. On $H_0^1(\\Omega)$, this seminorm is a norm.\n\nLet the target function be $u(x) = x(1-x) \\in H_0^1(\\Omega)$. Define the best approximation $u_h^{L^2} \\in V_h$ to $u$ as the unique element minimizing $\\|u - v_h\\|_{L^2}$ over $v_h \\in V_h$, and the best approximation $u_h^{H^1} \\in V_h$ as the unique element minimizing $|u - v_h|_{H^1}$ over $v_h \\in V_h$.\n\nTasks:\n1) Using only foundational definitions of inner products, norms, and best approximation (orthogonality with respect to the defining inner product), justify that the norms $\\|\\cdot\\|_{L^2}$ and $|\\cdot|_{H^1}$ are equivalent on $V_h$ by relating the generalized Rayleigh quotient of the corresponding matrices. Do not invoke any pre-packaged theorems beyond finite-dimensional norm equivalence and properties of symmetric positive definite matrices; derive the statement directly from first principles.\n2) Derive the linear systems that determine the coefficients of $u_h^{L^2}$ and $u_h^{H^1}$ in the basis $\\{\\varphi_1,\\varphi_2\\}$. That is, write $u_h^{\\star} = c_1^{\\star}\\,\\varphi_1 + c_2^{\\star}\\,\\varphi_2$ for $\\star \\in \\{L^2,H^1\\}$ and derive the $2\\times 2$ systems satisfied by the coefficient vectors from the orthogonality conditions.\n3) Compute the exact coefficient vectors and then compute the $L^2$-norm of the difference of the two best approximations, $\\|u_h^{H^1} - u_h^{L^2}\\|_{L^2}$. Express your final answer as an exact value. No rounding is required and no units are involved.\n\nYour final answer must be a single real number or a closed-form analytic expression for $\\|u_h^{H^1} - u_h^{L^2}\\|_{L^2}$.", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in numerical analysis, specifically within the finite element method, built upon sound mathematical principles. All necessary information is provided, and the tasks are clearly defined. We proceed to the solution.\n\nThe problem asks for three tasks to be completed. We address them in order. The finite-dimensional space is $V_h = \\text{span}\\{\\varphi_1, \\varphi_2\\}$, which is a subspace of $H_0^1(\\Omega)$. Any function $v_h \\in V_h$ can be written as $v_h(x) = c_1 \\varphi_1(x) + c_2 \\varphi_2(x)$ for some real coefficients $c_1, c_2$.\n\n1) Justification of Norm Equivalence\n\nLet $v_h$ be an arbitrary element of $V_h$. We can represent $v_h$ by its coefficient vector $\\mathbf{c} = \\begin{pmatrix} c_1 & c_2 \\end{pmatrix}^T \\in \\mathbb{R}^2$ such that $v_h(x) = \\sum_{j=1}^{2} c_j \\varphi_j(x)$. We express the squares of the two norms in terms of this coefficient vector.\n\nThe square of the $L^2$ norm is given by:\n$$ \\|v_h\\|_{L^2}^2 = (v_h, v_h)_{L^2} = \\left(\\sum_{i=1}^{2} c_i \\varphi_i, \\sum_{j=1}^{2} c_j \\varphi_j\\right)_{L^2} $$\nBy the bilinearity of the inner product, this becomes:\n$$ \\|v_h\\|_{L^2}^2 = \\sum_{i=1}^{2} \\sum_{j=1}^{2} c_i c_j (\\varphi_i, \\varphi_j)_{L^2} = \\mathbf{c}^T M \\mathbf{c} $$\nwhere $M$ is the $2 \\times 2$ mass matrix with entries $M_{ij} = (\\varphi_i, \\varphi_j)_{L^2}$. Since $\\|\\cdot\\|_{L^2}$ is a norm on $V_h$, for any non-zero $v_h$ (which implies a non-zero $\\mathbf{c}$), we have $\\|v_h\\|_{L^2}^2 > 0$. Therefore, $M$ is a symmetric positive definite (SPD) matrix.\n\nSimilarly, the square of the $H^1$ seminorm is:\n$$ |v_h|_{H^1}^2 = a(v_h, v_h) = a\\left(\\sum_{i=1}^{2} c_i \\varphi_i, \\sum_{j=1}^{2} c_j \\varphi_j\\right) $$\nBy bilinearity, this is:\n$$ |v_h|_{H^1}^2 = \\sum_{i=1}^{2} \\sum_{j=1}^{2} c_i c_j a(\\varphi_i, \\varphi_j) = \\mathbf{c}^T K \\mathbf{c} $$\nwhere $K$ is the $2 \\times 2$ stiffness matrix with entries $K_{ij} = a(\\varphi_i, \\varphi_j)$. Since $|\\cdot|_{H^1}$ is a norm on the subspace $V_h \\subset H_0^1(\\Omega)$, for any non-zero $v_h$, we have $|v_h|_{H^1}^2 > 0$. Thus, $K$ is also an SPD matrix.\n\nThe equivalence of the norms requires finding constants $C_1, C_2 > 0$ such that for any $v_h \\in V_h$:\n$$ C_1 \\|v_h\\|_{L^2}^2 \\le |v_h|_{H^1}^2 \\le C_2 \\|v_h\\|_{L^2}^2 $$\nSubstituting the matrix representations, this inequality for any non-zero $\\mathbf{c} \\in \\mathbb{R}^2$ becomes:\n$$ C_1 \\mathbf{c}^T M \\mathbf{c} \\le \\mathbf{c}^T K \\mathbf{c} \\le C_2 \\mathbf{c}^T M \\mathbf{c} $$\nSince $M$ is SPD, $\\mathbf{c}^T M \\mathbf{c} > 0$ for $\\mathbf{c} \\neq \\mathbf{0}$. We can divide by it to obtain:\n$$ C_1 \\le \\frac{\\mathbf{c}^T K \\mathbf{c}}{\\mathbf{c}^T M \\mathbf{c}} \\le C_2 $$\nThe expression $\\mathcal{R}(\\mathbf{c}) = \\frac{\\mathbf{c}^T K \\mathbf{c}}{\\mathbf{c}^T M \\mathbf{c}}$ is the generalized Rayleigh quotient for the matrix pencil $(K, M)$. For SPD matrices $K$ and $M$, the extreme values of this quotient are given by the minimum and maximum eigenvalues of the generalized eigenvalue problem $K\\mathbf{v} = \\lambda M\\mathbf{v}$. Let these eigenvalues be $\\lambda_{\\min}$ and $\\lambda_{\\max}$. Since $K$ and $M$ are SPD, these eigenvalues are real and positive. Thus, we have:\n$$ 0 < \\lambda_{\\min} \\le \\frac{\\mathbf{c}^T K \\mathbf{c}}{\\mathbf{c}^T M \\mathbf{c}} \\le \\lambda_{\\max} $$\nWe can choose $C_1 = \\lambda_{\\min}$ and $C_2 = \\lambda_{\\max}$. Since $V_h$ is a finite-dimensional space, the matrices $K$ and $M$ are finite, and these eigenvalues exist, are finite, and positive. This directly demonstrates the equivalence of the norms $\\|\\cdot\\|_{L^2}$ and $|\\cdot|_{H^1}$ on the space $V_h$.\n\n2) Derivation of Linear Systems\n\nThe best approximation $u_h \\in V_h$ to a function $u$ in a Hilbert space with inner product $(\\cdot, \\cdot)$ is defined by the Galerkin orthogonality condition: $(u - u_h, v_h) = 0$ for all $v_h \\in V_h$. This is equivalent to $(u_h, v_h) = (u, v_h)$ for all $v_h \\in V_h$.\n\nFor the best approximation $u_h^{L^2}$ in the $L^2$ norm, the inner product is $(\\cdot, \\cdot)_{L^2}$. Writing $u_h^{L^2} = \\sum_{j=1}^{2} c_j^{L^2} \\varphi_j$, the orthogonality condition must hold for each basis function $\\varphi_i$, $i=1,2$:\n$$ (u_h^{L^2}, \\varphi_i)_{L^2} = (u, \\varphi_i)_{L^2} \\quad \\text{for } i=1,2 $$\n$$ \\left(\\sum_{j=1}^{2} c_j^{L^2} \\varphi_j, \\varphi_i\\right)_{L^2} = (u, \\varphi_i)_{L^2} $$\n$$ \\sum_{j=1}^{2} (\\varphi_j, \\varphi_i)_{L^2} c_j^{L^2} = (u, \\varphi_i)_{L^2} $$\nThis is a $2 \\times 2$ linear system $M \\mathbf{c}^{L^2} = \\mathbf{f}^{L^2}$, where $M_{ij} = (\\varphi_i, \\varphi_j)_{L^2}$, $\\mathbf{c}^{L^2} = \\begin{pmatrix} c_1^{L^2} & c_2^{L^2} \\end{pmatrix}^T$, and $\\mathbf{f}^{L^2}_i = (u, \\varphi_i)_{L^2}$.\n\nFor the best approximation $u_h^{H^1}$ in the $H^1$ norm, the inner product is $a(\\cdot, \\cdot)$. Writing $u_h^{H^1} = \\sum_{j=1}^{2} c_j^{H^1} \\varphi_j$, the orthogonality condition is:\n$$ a(u_h^{H^1}, \\varphi_i) = a(u, \\varphi_i) \\quad \\text{for } i=1,2 $$\n$$ a\\left(\\sum_{j=1}^{2} c_j^{H^1} \\varphi_j, \\varphi_i\\right) = a(u, \\varphi_i) $$\n$$ \\sum_{j=1}^{2} a(\\varphi_j, \\varphi_i) c_j^{H^1} = a(u, \\varphi_i) $$\nThis is a $2 \\times 2$ linear system $K \\mathbf{c}^{H^1} = \\mathbf{f}^{H^1}$, where $K_{ij} = a(\\varphi_i, \\varphi_j)$, $\\mathbf{c}^{H^1} = \\begin{pmatrix} c_1^{H^1} & c_2^{H^1} \\end{pmatrix}^T$, and $\\mathbf{f}^{H^1}_i = a(u, \\varphi_i)$.\n\n3) Computation of Coefficients and Norm of Difference\n\nThe mesh size is $h=1/3$. The basis functions are:\n$\\varphi_1(x) = \\begin{cases} 3x & x\\in[0, 1/3] \\\\ 2-3x & x\\in[1/3, 2/3] \\\\ 0 & \\text{otherwise} \\end{cases}$ and $\\varphi_2(x) = \\begin{cases} 3x-1 & x\\in[1/3, 2/3] \\\\ 3-3x & x\\in[2/3, 1] \\\\ 0 & \\text{otherwise} \\end{cases}$.\nTheir derivatives are:\n$\\varphi_1'(x) = \\begin{cases} 3 & x\\in(0, 1/3) \\\\ -3 & x\\in(1/3, 2/3) \\end{cases}$ and $\\varphi_2'(x) = \\begin{cases} 3 & x\\in(1/3, 2/3) \\\\ -3 & x\\in(2/3, 1) \\end{cases}$.\n\nWe compute the matrices $M$ and $K$. For a uniform mesh with linear elements, standard formulas give $M_{ii} = 2h/3$, $M_{i,i\\pm 1} = h/6$, $K_{ii} = 2/h$, $K_{i,i\\pm 1} = -1/h$.\nWith $h=1/3$:\n$M_{11} = M_{22} = 2(1/3)/3 = 2/9$.\n$M_{12} = M_{21} = (1/3)/6 = 1/18$. So, $M = \\frac{1}{18} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix}$.\n$K_{11} = K_{22} = 2/(1/3) = 6$.\n$K_{12} = K_{21} = -1/(1/3) = -3$. So, $K = \\begin{pmatrix} 6 & -3 \\\\ -3 & 6 \\end{pmatrix}$.\n\nNext, we compute the load vectors for $u(x) = x(1-x) = x-x^2$ and $u'(x) = 1-2x$.\n$f_1^{L^2} = \\int_0^{1} (x-x^2)\\varphi_1(x)dx = \\int_0^{1/3} (x-x^2)(3x)dx + \\int_{1/3}^{2/3} (x-x^2)(2-3x)dx = \\frac{1}{36} + \\frac{13}{324} = \\frac{9+13}{324} = \\frac{22}{324} = \\frac{11}{162}$.\nBy symmetry of $u(x)$ and the basis functions, $f_2^{L^2} = f_1^{L^2} = 11/162$. So $\\mathbf{f}^{L^2} = \\begin{pmatrix} 11/162 & 11/162 \\end{pmatrix}^T$.\n$f_1^{H^1} = \\int_0^{1} (1-2x)\\varphi_1'(x)dx = \\int_0^{1/3} (1-2x)(3)dx + \\int_{1/3}^{2/3} (1-2x)(-3)dx = \\frac{2}{3}$.\nBy symmetry, $f_2^{H^1} = f_1^{H^1} = 2/3$. So $\\mathbf{f}^{H^1} = \\begin{pmatrix} 2/3 & 2/3 \\end{pmatrix}^T$.\n\nWe solve the systems. For $\\mathbf{c}^{L^2}$:\n$$ \\frac{1}{18} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix} \\begin{pmatrix} c_1^{L^2} \\\\ c_2^{L^2} \\end{pmatrix} = \\begin{pmatrix} 11/162 \\\\ 11/162 \\end{pmatrix} $$\nBy symmetry, $c_1^{L^2} = c_2^{L^2} = c$. The first row gives $\\frac{1}{18}(4c+c) = 11/162 \\implies \\frac{5c}{18} = \\frac{11}{162} \\implies c = \\frac{11 \\cdot 18}{162 \\cdot 5} = \\frac{11}{9 \\cdot 5} = \\frac{11}{45}$.\nSo, $\\mathbf{c}^{L^2} = \\begin{pmatrix} 11/45 & 11/45 \\end{pmatrix}^T$.\n\nFor $\\mathbf{c}^{H^1}$:\n$$ \\begin{pmatrix} 6 & -3 \\\\ -3 & 6 \\end{pmatrix} \\begin{pmatrix} c_1^{H^1} \\\\ c_2^{H^1} \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} $$\nBy symmetry, $c_1^{H^1} = c_2^{H^1} = c$. The first row gives $6c - 3c = 2/3 \\implies 3c = 2/3 \\implies c = 2/9$.\nSo, $\\mathbf{c}^{H^1} = \\begin{pmatrix} 2/9 & 2/9 \\end{pmatrix}^T$. This corresponds to the nodal interpolant of $u(x)$, as $u(1/3) = 1/3 - 1/9 = 2/9$ and $u(2/3) = 2/3 - 4/9 = 2/9$.\n\nThe difference function is $\\delta u_h(x) = u_h^{H^1}(x) - u_h^{L^2}(x) = \\delta c_1 \\varphi_1(x) + \\delta c_2 \\varphi_2(x)$, where the coefficient vector is $\\mathbf{\\delta c} = \\mathbf{c}^{H^1} - \\mathbf{c}^{L^2}$.\n$\\delta c_1 = \\delta c_2 = 2/9 - 11/45 = (10-11)/45 = -1/45$. So $\\mathbf{\\delta c} = \\begin{pmatrix} -1/45 & -1/45 \\end{pmatrix}^T$.\n\nThe norm is computed as $\\|\\delta u_h\\|_{L^2} = \\sqrt{(\\mathbf{\\delta c})^T M (\\mathbf{\\delta c})}$.\n$$ \\|\\delta u_h\\|_{L^2}^2 = \\begin{pmatrix} -1/45 & -1/45 \\end{pmatrix} \\left( \\frac{1}{18} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix} \\right) \\begin{pmatrix} -1/45 \\\\ -1/45 \\end{pmatrix} $$\n$$ = \\left(\\frac{-1}{45}\\right)^2 \\frac{1}{18} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ = \\frac{1}{2025} \\frac{1}{18} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 5 \\end{pmatrix} = \\frac{1}{2025} \\frac{10}{18} = \\frac{1}{2025} \\frac{5}{9} = \\frac{5}{18225} = \\frac{1}{3645} $$\nThe required norm is the square root:\n$$ \\|u_h^{H^1} - u_h^{L^2}\\|_{L^2} = \\sqrt{\\frac{1}{3645}} = \\frac{1}{\\sqrt{3645}} $$\nTo simplify, we factor the denominator: $3645 = 5 \\times 729 = 5 \\times 27^2$.\n$$ \\frac{1}{\\sqrt{27^2 \\cdot 5}} = \\frac{1}{27\\sqrt{5}} = \\frac{\\sqrt{5}}{27 \\cdot 5} = \\frac{\\sqrt{5}}{135} $$\nThis is the final exact value.", "answer": "$$\\boxed{\\frac{\\sqrt{5}}{135}}$$", "id": "2575243"}]}