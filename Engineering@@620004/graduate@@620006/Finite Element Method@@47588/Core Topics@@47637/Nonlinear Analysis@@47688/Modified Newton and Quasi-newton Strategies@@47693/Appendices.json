{"hands_on_practices": [{"introduction": "This exercise provides a direct comparison between the full Newton-Raphson method and its simplest variant, the modified Newton method. By performing iterations for both schemes on a small-scale problem, you will gain a tangible understanding of the trade-off between the computational cost per iteration and the rate of convergence [@problem_id:2580609]. This practice is fundamental to appreciating why more sophisticated quasi-Newton methods were developed.", "problem": "Consider the nonlinear balance equations arising from a reduced two-degree-of-freedom finite element (FE) system, where the residual vector $\\boldsymbol{R}(\\boldsymbol{u}) \\in \\mathbb{R}^{2}$ is defined componentwise by\n$$\nR_{1}(\\boldsymbol{u}) \\equiv 2\\,u_{1} + \\tfrac{1}{2}\\,u_{1}^{2} + 0.1\\,u_{2}^{2} - 1.2, \\quad\nR_{2}(\\boldsymbol{u}) \\equiv -0.3\\,u_{1} + 1.5\\,u_{2} + 0.2\\,u_{1}\\,u_{2} + 0.1\\,u_{2}^{3} - 0.9,\n$$\nwith the initial guess $\\boldsymbol{u}_{0} = (0,0)^{\\mathsf{T}}$. The consistent tangent (Jacobian) matrix $\\boldsymbol{K}(\\boldsymbol{u}) = \\partial \\boldsymbol{R}/\\partial \\boldsymbol{u}$ is\n$$\n\\boldsymbol{K}(\\boldsymbol{u}) = \n\\begin{pmatrix}\n2 + u_{1} & 0.2\\,u_{2} \\\\\n-0.3 + 0.2\\,u_{2} & 1.5 + 0.2\\,u_{1} + 0.3\\,u_{2}^{2}\n\\end{pmatrix},\n$$\nand at the initial guess one has\n$$\n\\boldsymbol{K}(\\boldsymbol{u}_{0}) = \n\\begin{pmatrix}\n2 & 0 \\\\\n-0.3 & 1.5\n\\end{pmatrix}.\n$$\nStarting from $\\boldsymbol{u}_{0}$, perform two iterations for each of the following strategies:\n- Full Newton: $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{k})^{-1}\\,\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n- Modified Newton (a quasi-Newton strategy with frozen tangent): $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{0})^{-1}\\,\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n\nFor each strategy, define the Euclidean residual reduction factor at iteration $k$ as\n$$\n\\rho_{k} \\equiv \\frac{\\|\\boldsymbol{R}(\\boldsymbol{u}_{k+1})\\|_{2}}{\\|\\boldsymbol{R}(\\boldsymbol{u}_{k})\\|_{2}},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Derive the iterations from the Newton linearization principle and compute all residuals and factors from first principles (no line search or damping).\n\nReport, as your final answer, the second-iteration residual reduction factor $\\rho_{1}$ for the modified Newton method. Round your final answer to four significant figures. No physical units are required.", "solution": "The problem requires the calculation of the residual reduction factor $\\rho_{1}$ for the modified Newton method after two iterations. The general principle for solving a nonlinear system of equations $\\boldsymbol{R}(\\boldsymbol{u}) = \\boldsymbol{0}$ via a Newton-type method is based on a first-order Taylor series expansion of the residual vector $\\boldsymbol{R}$ around the current iterate $\\boldsymbol{u}_{k}$:\n$$\n\\boldsymbol{R}(\\boldsymbol{u}_{k+1}) \\approx \\boldsymbol{R}(\\boldsymbol{u}_{k}) + \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{u}}\\bigg|_{\\boldsymbol{u}_{k}} (\\boldsymbol{u}_{k+1} - \\boldsymbol{u}_{k})\n$$\nSetting $\\boldsymbol{R}(\\boldsymbol{u}_{k+1})$ to $\\boldsymbol{0}$ and defining the tangent matrix $\\boldsymbol{K}(\\boldsymbol{u}_{k}) = \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{u}}\\big|_{\\boldsymbol{u}_{k}}$ and the displacement increment $\\Delta \\boldsymbol{u}_{k} = \\boldsymbol{u}_{k+1} - \\boldsymbol{u}_{k}$, we obtain the linear system for the increment:\n$$\n\\boldsymbol{K}(\\boldsymbol{u}_{k}) \\Delta \\boldsymbol{u}_{k} = -\\boldsymbol{R}(\\boldsymbol{u}_{k})\n$$\nThe next iterate is then $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} + \\Delta \\boldsymbol{u}_{k}$.\nFor the full Newton method, the tangent matrix $\\boldsymbol{K}(\\boldsymbol{u}_{k})$ is re-evaluated at each iteration $k$. For the modified Newton method, the tangent matrix is computed only once at the initial guess $\\boldsymbol{u}_{0}$ and then held constant (frozen) for all subsequent iterations. Thus, the iterative update is $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{0})^{-1}\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n\nWe begin with the initial guess $\\boldsymbol{u}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Initial State ($k=0$)**\n\nFirst, compute the initial residual vector $\\boldsymbol{R}(\\boldsymbol{u}_{0})$:\n$$\nR_{1}(\\boldsymbol{u}_{0}) = 2(0) + \\frac{1}{2}(0)^{2} + 0.1(0)^{2} - 1.2 = -1.2\n$$\n$$\nR_{2}(\\boldsymbol{u}_{0}) = -0.3(0) + 1.5(0) + 0.2(0)(0) + 0.1(0)^{3} - 0.9 = -0.9\n$$\nSo, $\\boldsymbol{R}(\\boldsymbol{u}_{0}) = \\begin{pmatrix} -1.2 \\\\ -0.9 \\end{pmatrix}$.\nThe Euclidean norm of the initial residual is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{0})\\|_{2} = \\sqrt{(-1.2)^{2} + (-0.9)^{2}} = \\sqrt{1.44 + 0.81} = \\sqrt{2.25} = 1.5\n$$\nThe tangent matrix at the initial guess is given as:\n$$\n\\boldsymbol{K}_{0} = \\boldsymbol{K}(\\boldsymbol{u}_{0}) = \\begin{pmatrix} 2 & 0 \\\\ -0.3 & 1.5 \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(\\boldsymbol{K}_{0}) = (2)(1.5) - (0)(-0.3) = 3$. The inverse is:\n$$\n\\boldsymbol{K}_{0}^{-1} = \\frac{1}{3} \\begin{pmatrix} 1.5 & 0 \\\\ 0.3 & 2 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0 \\\\ 0.1 & \\frac{2}{3} \\end{pmatrix}\n$$\n\n**First Iteration ($k=0$)**\n\nThis iteration is identical for both full and modified Newton methods. We solve $\\boldsymbol{K}_{0} \\Delta \\boldsymbol{u}_{0} = -\\boldsymbol{R}(\\boldsymbol{u}_{0})$.\n$$\n\\Delta \\boldsymbol{u}_{0} = -\\boldsymbol{K}_{0}^{-1} \\boldsymbol{R}(\\boldsymbol{u}_{0}) = -\\begin{pmatrix} 0.5 & 0 \\\\ 0.1 & \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} -1.2 \\\\ -0.9 \\end{pmatrix} = -\\begin{pmatrix} 0.5(-1.2) \\\\ 0.1(-1.2) + \\frac{2}{3}(-0.9) \\end{pmatrix} = -\\begin{pmatrix} -0.6 \\\\ -0.12 - 0.6 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix}\n$$\nThe first updated displacement vector is:\n$$\n\\boldsymbol{u}_{1} = \\boldsymbol{u}_{0} + \\Delta \\boldsymbol{u}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix}\n$$\nNext, we compute the residual at $\\boldsymbol{u}_{1}$:\n$$\nR_{1}(\\boldsymbol{u}_{1}) = 2(0.6) + \\frac{1}{2}(0.6)^{2} + 0.1(0.72)^{2} - 1.2 = 1.2 + 0.5(0.36) + 0.1(0.5184) - 1.2 = 0.18 + 0.05184 = 0.23184\n$$\n$$\nR_{2}(\\boldsymbol{u}_{1}) = -0.3(0.6) + 1.5(0.72) + 0.2(0.6)(0.72) + 0.1(0.72)^{3} - 0.9 = -0.18 + 1.08 + 0.0864 + 0.1(0.373248) - 0.9 = 0.0864 + 0.0373248 = 0.1237248\n$$\nThus, $\\boldsymbol{R}(\\boldsymbol{u}_{1}) = \\begin{pmatrix} 0.23184 \\\\ 0.1237248 \\end{pmatrix}$. The norm of this residual is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{1})\\|_{2} = \\sqrt{(0.23184)^{2} + (0.1237248)^{2}} = \\sqrt{0.0537498256 + 0.01530782611584} = \\sqrt{0.06905765171584} \\approx 0.2627885994\n$$\nThis value serves as the denominator for the reduction factor $\\rho_{1}$.\n\n**Second Iteration ($k=1$) - Modified Newton Method**\n\nFor the modified Newton method, we use the frozen tangent $\\boldsymbol{K}_{0}$. We solve $\\boldsymbol{K}_{0} \\Delta \\boldsymbol{u}_{1} = -\\boldsymbol{R}(\\boldsymbol{u}_{1})$.\n$$\n\\Delta \\boldsymbol{u}_{1} = -\\boldsymbol{K}_{0}^{-1} \\boldsymbol{R}(\\boldsymbol{u}_{1}) = -\\begin{pmatrix} 0.5 & 0 \\\\ 0.1 & \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 0.23184 \\\\ 0.1237248 \\end{pmatrix} = -\\begin{pmatrix} 0.5(0.23184) \\\\ 0.1(0.23184) + \\frac{2}{3}(0.1237248) \\end{pmatrix} = -\\begin{pmatrix} 0.11592 \\\\ 0.023184 + 0.0824832 \\end{pmatrix} = \\begin{pmatrix} -0.11592 \\\\ -0.1056672 \\end{pmatrix}\n$$\nThe second updated displacement vector is:\n$$\n\\boldsymbol{u}_{2} = \\boldsymbol{u}_{1} + \\Delta \\boldsymbol{u}_{1} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix} + \\begin{pmatrix} -0.11592 \\\\ -0.1056672 \\end{pmatrix} = \\begin{pmatrix} 0.48408 \\\\ 0.6143328 \\end{pmatrix}\n$$\nNow we compute the residual at $\\boldsymbol{u}_{2}$:\n$$\nR_{1}(\\boldsymbol{u}_{2}) = 2(0.48408) + \\frac{1}{2}(0.48408)^{2} + 0.1(0.6143328)^{2} - 1.2\n$$\n$$\nR_{1}(\\boldsymbol{u}_{2}) = 0.96816 + 0.5(0.2343334464) + 0.1(0.3774048590378304) - 1.2 = 0.96816 + 0.1171667232 + 0.0377404859 - 1.2 \\approx -0.0769327909\n$$\n$$\nR_{2}(\\boldsymbol{u}_{2}) = -0.3(0.48408) + 1.5(0.6143328) + 0.2(0.48408)(0.6143328) + 0.1(0.6143328)^{3} - 0.9\n$$\n$$\nR_{2}(\\boldsymbol{u}_{2}) = -0.145224 + 0.9214992 + 0.2(0.297440535936) + 0.1(0.231846067341235...) - 0.9 \\approx -0.145224 + 0.9214992 + 0.0594881072 + 0.0231846067 - 0.9 \\approx -0.0410520861\n$$\nThe residual vector is $\\boldsymbol{R}(\\boldsymbol{u}_{2}) \\approx \\begin{pmatrix} -0.07693279 \\\\ -0.04105209 \\end{pmatrix}$. Its norm is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{2})\\|_{2} \\approx \\sqrt{(-0.07693279)^{2} + (-0.4105209)^{2}} = \\sqrt{0.0059186546 + 0.0016852731} = \\sqrt{0.0076039277} \\approx 0.0872005029\n$$\nFinally, the residual reduction factor at the second iteration ($k=1$) is defined as:\n$$\n\\rho_{1} = \\frac{\\|\\boldsymbol{R}(\\boldsymbol{u}_{2})\\|_{2}}{\\|\\boldsymbol{R}(\\boldsymbol{u}_{1})\\|_{2}} \\approx \\frac{0.0872005029}{0.2627885994} \\approx 0.33182046\n$$\nRounding to four significant figures, we get $0.3318$.", "answer": "$$ \\boxed{0.3318} $$", "id": "2580609"}, {"introduction": "Moving beyond simple tangent approximations, this practice challenges you to implement the powerful Limited-memory BFGS (L-BFGS) algorithm, a workhorse for large-scale nonlinear problems. You will develop the famous \"two-loop recursion\" which avoids forming and storing dense Hessian approximations, making the method practical for systems with thousands or millions of degrees of freedom [@problem_id:2580610]. This coding exercise bridges the gap between the theory of quasi-Newton updates and their efficient, matrix-free implementation.", "problem": "Consider the nonlinear residual of a one-dimensional finite element (FE) model for the scalar field $u(x)$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. The strong form is a nonlinear diffusion equation\n$$-\\frac{d}{dx}\\Big((1+u(x)^2)\\frac{du}{dx}\\Big) - f(x) = 0.$$\nThe weak (variational) form seeks $u \\in H_0^1(0,1)$ such that, for all admissible test functions $v$,\n$$\\int_0^1 (1+u^2)\\, u' \\, v' \\, dx - \\int_0^1 f \\, v \\, dx = 0.$$\nDiscretize the problem with linear shape functions on a uniform mesh of $N_e$ elements (and $N_n=N_e+1$ nodes) over $[0,1]$. The vector of unknowns comprises the $N_n-2$ interior degrees of freedom, denoted by the vector $\\mathbf{u}\\in\\mathbb{R}^{N_n-2}$; the full vector including boundary nodes is denoted $\\tilde{\\mathbf{u}}\\in\\mathbb{R}^{N_n}$ with $\\tilde{u}_0=\\tilde{u}_{N_n-1}=0$ and $\\tilde{u}_i=\\mathbf{u}_{i-1}$ for $i\\in\\{1,\\dots,N_n-2\\}$. Using the two-point Gaussian quadrature rule with points $\\xi=\\pm 1/\\sqrt{3}$ and weights $w=1$ on the reference element $[-1,1]$, and the affine mapping to each physical element of size $h=1/N_e$ with Jacobian $J=h/2$, assemble the discrete nonlinear residual $\\mathbf{R}(\\mathbf{u})\\in\\mathbb{R}^{N_n-2}$ by summing element contributions\n$$r_e^{(j)} \\approx \\sum_{g=1}^{2}\\Big( a(u(\\xi_g))\\, u'(x)\\, \\frac{dN_j}{dx}\\, J - f(x(\\xi_g))\\, N_j(\\xi_g)\\, J\\Big), \\quad j\\in\\{1,2\\},$$\nwhere $a(u)=1+u^2$, $N_j$ are the linear shape functions on the reference element and $dN_j/dx$ are their physical derivatives, which are constant over each element. The element contributions are scattered into the global interior residual at the corresponding interior indices (skipping the Dirichlet boundary nodes).\n\nYou are to implement a single Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) step to compute the quasi-Newton search direction $\\mathbf{p}_k$ without forming any Jacobian or Hessian matrices. Given the current residual (which plays the role of the gradient) $\\mathbf{g}_k=\\mathbf{R}(\\mathbf{u}_k)$, and a history of $m$ vector pairs $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}_{i=k-m}^{k-1}$ with $\\mathbf{s}_i=\\mathbf{u}_{i+1}-\\mathbf{u}_i$ and $\\mathbf{y}_i=\\mathbf{g}_{i+1}-\\mathbf{g}_i$, compute the L-BFGS direction $\\mathbf{p}_k=-\\mathbf{H}_k \\mathbf{g}_k$ using the two-loop recursion with an initial inverse Hessian scaling $\\gamma_k$ applied as $\\mathbf{H}_0=\\gamma_k \\mathbf{I}$, and a curvature safeguard that discards any pair with $\\mathbf{s}_i^\\top \\mathbf{y}_i \\le \\tau$, where $\\tau>0$ is a specified tolerance. No matrices other than identity-scaled vectors may be formed; use only vector inner products, saxpy operations, and scaling.\n\nYour program must implement:\n- Assembly of $\\mathbf{R}(\\mathbf{u})$ for a given $\\mathbf{u}$ and source $f(x)$ as specified above.\n- The L-BFGS two-loop recursion to compute $\\mathbf{p}_k$ from $\\mathbf{g}_k$, $\\{(\\mathbf{s}_i,\\mathbf{y}_i)\\}$, and $\\gamma_k$ without forming any Jacobian or Hessian.\n- A curvature check with tolerance $\\tau$ that discards any $(\\mathbf{s}_i,\\mathbf{y}_i)$ for which $\\mathbf{s}_i^\\top \\mathbf{y}_i \\le \\tau$. The remaining pairs must be used in chronological order.\n\nTest Suite:\nUse the following deterministic test suite to exercise different facets of the implementation. In all tests, set $f(x)=10\\sin(2\\pi x)$, $N_e=16$, so $h=1/16$, and the number of interior unknowns is $N_n-2=15$. Generate reproducible vectors using a fixed random seed.\n\n- Common setup for all tests:\n  - Use a pseudorandom generator with seed $12345$ to generate the vectors.\n  - Let $\\mathbf{u}_0$ be drawn component-wise from a standard normal distribution scaled by $0.1$.\n  - Let $\\mathbf{s}_0$ and $\\mathbf{s}_1$ be drawn component-wise from a standard normal distribution scaled by $10^{-2}$.\n  - Define $\\mathbf{u}_1=\\mathbf{u}_0+\\mathbf{s}_0$, $\\mathbf{u}_2=\\mathbf{u}_1+\\mathbf{s}_1$.\n  - Compute the residuals $\\mathbf{g}_0=\\mathbf{R}(\\mathbf{u}_0)$, $\\mathbf{g}_1=\\mathbf{R}(\\mathbf{u}_1)$, and $\\mathbf{g}_2=\\mathbf{R}(\\mathbf{u}_2)$.\n  - Define the history pairs $(\\mathbf{s}_0,\\mathbf{y}_0)$ with $\\mathbf{y}_0=\\mathbf{g}_1-\\mathbf{g}_0$, and $(\\mathbf{s}_1,\\mathbf{y}_1)$ with $\\mathbf{y}_1=\\mathbf{g}_2-\\mathbf{g}_1$.\n\n- Test $1$ (boundary case $m=0$): Use no history pairs (i.e., $m=0$), set $\\gamma_k=1$. Compute $\\mathbf{p}_2$ for the current state $\\mathbf{u}_2$ using only $\\mathbf{g}_2$. Report the Euclidean norm $\\|\\mathbf{p}_2+\\mathbf{g}_2\\|_2$ as a floating-point number.\n\n- Test $2$ (happy path $m=2$): Use the two history pairs $\\{(\\mathbf{s}_0,\\mathbf{y}_0),(\\mathbf{s}_1,\\mathbf{y}_1)\\}$ that satisfy the curvature condition. Set the initial scaling to the commonly used choice $\\gamma_k = (\\mathbf{s}_{1}^\\top \\mathbf{y}_{1})/(\\mathbf{y}_{1}^\\top \\mathbf{y}_{1})$. Compute $\\mathbf{p}_2$ and report the scalar $\\mathbf{g}_2^\\top \\mathbf{p}_2$ as a floating-point number.\n\n- Test $3$ (edge case with near-singular history): Replace the first pair by $(\\mathbf{s}_0,\\hat{\\mathbf{y}}_0)$ with $\\hat{\\mathbf{y}}_0=\\epsilon\\, \\mathbf{s}_0$ where $\\epsilon=10^{-12}$ to emulate a near-singular secant pair, keep the second pair $(\\mathbf{s}_1,\\mathbf{y}_1)$ unchanged, and set $\\tau=10^{-10}$. Use the curvature safeguard, compute $\\mathbf{p}_2$, and report the descent-indicator integer defined as\n$$d = \\begin{cases}\n1 & \\text{if } \\mathbf{g}_2^\\top \\mathbf{p}_2 < 0,\\\\\n0 & \\text{otherwise.}\n\\end{cases}$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results of Tests $1$, $2$, and $3$ as a comma-separated list enclosed in square brackets, in the order $[\\text{Test 1 result}, \\text{Test 2 result}, \\text{Test 3 result}]$. For example, the output must have the form $[\\text{float},\\text{float},\\text{integer}]$ on a single line. No units are required; all quantities are dimensionless real numbers or integers as specified.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the fields of numerical analysis and computational mechanics, well-posed with all necessary information provided for a unique and reproducible solution, and formulated with objective and precise language. There are no logical contradictions, unrealistic assumptions, or ambiguities that would prevent a rigorous solution. We may therefore proceed with the analysis.\n\nThe task is to compute a quasi-Newton search direction using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm. This requires two main components: first, the assembly of the discrete nonlinear residual vector, which serves as the gradient in the optimization context; and second, the implementation of the L-BFGS two-loop recursion.\n\n### 1. Finite Element Residual Assembly\n\nThe governing nonlinear diffusion equation is\n$$-\\frac{d}{dx}\\left( \\left(1+u(x)^2\\right)\\frac{du}{dx} \\right) - f(x) = 0, \\quad x \\in (0,1)$$\nwith homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. The corresponding weak form is to find $u \\in H_0^1(0,1)$ such that for all test functions $v \\in H_0^1(0,1)$:\n$$ \\int_0^1 (1+u^2) u' v' \\,dx - \\int_0^1 f v \\,dx = 0 $$\nThe domain $[0,1]$ is discretized into $N_e$ uniform elements of length $h=1/N_e$. We use linear Lagrange basis functions (hat functions), denoted $\\phi_i(x)$, for both the trial function $u_h(x) = \\sum_{i=1}^{N_n-2} \\mathbf{u}_{i-1} \\phi_i(x)$ and the test functions $v_h(x) = \\phi_j(x)$. The vector $\\mathbf{u} \\in \\mathbb{R}^{N_n-2}$ represents the unknown solution values at the $N_n-2$ interior nodes.\n\nThe $j$-th component of the nonlinear residual vector $\\mathbf{R}(\\mathbf{u}) \\in \\mathbb{R}^{N_n-2}$ is given by\n$$ R_j(\\mathbf{u}) = \\int_0^1 (1+u_h^2) u_h' \\phi_j' \\,dx - \\int_0^1 f \\phi_j \\,dx $$\nThis integral is computed by summing contributions from each element. For an element $e = [x_i, x_{i+1}]$, the contribution to the residual at its two local nodes is calculated using two-point Gaussian quadrature on the reference element $[-1,1]$.\n\nLet's consider a single element $e$. The solution $u_h(x)$ and its derivative $u_h'(x)$ within this element are\n$$ u_h(x(\\xi)) = \\tilde{u}_i N_1(\\xi) + \\tilde{u}_{i+1} N_2(\\xi) $$\n$$ u_h'(x) = \\frac{\\tilde{u}_{i+1} - \\tilde{u}_i}{h} $$\nwhere $\\tilde{\\mathbf{u}}$ is the full nodal vector including boundaries, $\\xi \\in [-1,1]$ is the coordinate on the reference element, and $N_1(\\xi)=\\frac{1-\\xi}{2}$, $N_2(\\xi)=\\frac{1+\\xi}{2}$ are the linear shape functions. The physical derivatives of the basis functions are constant over the element: $\\phi_i'(x) = -1/h$ and $\\phi_{i+1}'(x)=1/h$.\n\nThe element residual vector $\\mathbf{r}_e \\in \\mathbb{R}^2$ is computed as specified:\n$$ r_{e}^{(j)} = \\int_{x_i}^{x_{i+1}} \\left( (1+u_h^2)u_h' \\phi'_{i+j-1} - f \\phi_{i+j-1} \\right) dx $$\nUsing Gaussian quadrature with points $\\xi_g=\\pm 1/\\sqrt{3}$ and weights $w_g=1$, and Jacobian $J=h/2$, this becomes:\n$$ r_{e}^{(j)} \\approx \\sum_{g=1}^{2} \\left[ \\left(1+u_h(x(\\xi_g))^2\\right) \\left(\\frac{\\tilde{u}_{i+1} - \\tilde{u}_i}{h}\\right) \\frac{dN_j}{dx} - f(x(\\xi_g)) N_j(\\xi_g) \\right] J $$\nwhere $\\frac{dN_1}{dx} = -1/h$ and $\\frac{dN_2}{dx} = 1/h$.\n\nThe global residual vector $\\mathbf{R}(\\mathbf{u})$ is formed by assembling these element contributions. For an interior node $k$, its residual component $R_{k-1}$ receives the second contribution $r^{(2)}$ from element $k-1$ and the first contribution $r^{(1)}$ from element $k$.\n\n### 2. L-BFGS Two-Loop Recursion\n\nThe L-BFGS algorithm provides a matrix-free method to compute the search direction $\\mathbf{p}_k = -\\mathbf{H}_k \\mathbf{g}_k$, where $\\mathbf{H}_k$ is an approximation to the inverse Hessian of the objective function. The gradient $\\mathbf{g}_k$ is our residual vector $\\mathbf{R}(\\mathbf{u}_k)$. The algorithm uses a limited history of $m$ past updates, stored as vector pairs $(\\mathbf{s}_i, \\mathbf{y}_i)$, where $\\mathbf{s}_i = \\mathbf{u}_{i+1} - \\mathbf{u}_i$ is the step in the variables and $\\mathbf{y}_i = \\mathbf{g}_{i+1} - \\mathbf{g}_i$ is the change in the gradient.\n\nThe computation proceeds via the well-known two-loop recursion:\n1.  Initialize $\\mathbf{q} = \\mathbf{g}_k$.\n2.  **First loop** (backward pass): For $i$ from $k-1$ down to $k-m$:\n    -   Compute $\\rho_i = 1 / (\\mathbf{y}_i^\\top \\mathbf{s}_i)$.\n    -   Compute $\\alpha_i = \\rho_i (\\mathbf{s}_i^\\top \\mathbf{q})$.\n    -   Update $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_i \\mathbf{y}_i$.\n3.  **Scaling**: Apply the initial inverse Hessian approximation. This is a diagonal matrix $\\mathbf{H}_0 = \\gamma_k \\mathbf{I}$.\n    -   $\\mathbf{p} \\leftarrow \\gamma_k \\mathbf{q}$.\n4.  **Second loop** (forward pass): For $i$ from $k-m$ to $k-1$:\n    -   Retrieve $\\rho_i$ and $\\alpha_i$ from the first loop.\n    -   Compute $\\beta = \\rho_i (\\mathbf{y}_i^\\top \\mathbf{p})$.\n    -   Update $\\mathbf{p} \\leftarrow \\mathbf{p} + (\\alpha_i - \\beta) \\mathbf{s}_i$.\n5.  The final search direction is $\\mathbf{p}_k = -\\mathbf{p}$.\n\nA critical stability requirement for the BFGS update is the curvature condition $\\mathbf{s}_i^\\top \\mathbf{y}_i > 0$. The problem mandates a safeguard: any pair $(\\mathbf{s}_i, \\mathbf{y}_i)$ for which $\\mathbf{s}_i^\\top \\mathbf{y}_i \\le \\tau$ for a given tolerance $\\tau > 0$ is discarded from the history before the recursion is performed.\n\n### 3. Test Cases Execution\n\nThe provided test suite is executed as follows:\n\n-   **Common Setup**: Generate vectors $\\mathbf{u}_0, \\mathbf{s}_0, \\mathbf{s}_1$ using the specified random seed. Compute $\\mathbf{u}_1, \\mathbf{u}_2$. Assemble the corresponding residuals $\\mathbf{g}_0, \\mathbf{g}_1, \\mathbf{g}_2$ using the FE function. Form the history pairs $(\\mathbf{s}_0, \\mathbf{y}_0=\\mathbf{g}_1-\\mathbf{g}_0)$ and $(\\mathbf{s}_1, \\mathbf{y}_1=\\mathbf{g}_2-\\mathbf{g}_1)$.\n\n-   **Test 1**: With $m=0$, the history is empty. The two-loop recursion simplifies significantly. The first and second loops are not executed. The calculation reduces to $\\mathbf{p} \\leftarrow \\gamma_k \\mathbf{g}_k$ and finally $\\mathbf{p}_k = -\\gamma_k \\mathbf{g}_k$. With $\\gamma_k=1$, we get $\\mathbf{p}_k = -\\mathbf{g}_k$, which is the steepest descent direction. The quantity $\\|\\mathbf{p}_2 + \\mathbf{g}_2\\|_2$ should evaluate to zero, up to machine precision.\n\n-   **Test 2**: With $m=2$, the full history $\\{(\\mathbf{s}_0, \\mathbf{y}_0), (\\mathbf{s}_1, \\mathbf{y}_1)\\}$ is used. The initial scaling is chosen as $\\gamma_k = (\\mathbf{s}_{k-1}^\\top \\mathbf{y}_{k-1}) / (\\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1}) = (\\mathbf{s}_1^\\top \\mathbf{y}_1) / (\\mathbf{y}_1^\\top \\mathbf{y}_1)$. The two-loop recursion is executed fully to find $\\mathbf{p}_2$. The result to report is the inner product $\\mathbf{g}_2^\\top \\mathbf{p}_2$, which must be negative to confirm that $\\mathbf{p}_2$ is a descent direction.\n\n-   **Test 3**: The history is modified: the pair $(\\mathbf{s}_0, \\hat{\\mathbf{y}}_0)$ with $\\hat{\\mathbf{y}}_0 = \\epsilon \\mathbf{s}_0$ is constructed to be nearly singular. The curvature check $\\mathbf{s}_0^\\top \\hat{\\mathbf{y}}_0 = \\epsilon \\|\\mathbf{s}_0\\|^2_2 \\le \\tau=10^{-10}$ will cause this pair to be discarded. The computation proceeds with the remaining valid history, which contains only $(\\mathbf{s}_1, \\mathbf{y}_1)$. The scaling $\\gamma_k$ is computed using this most recent valid pair, as in Test 2. The L-BFGS recursion for $m=1$ is performed, and the sign of $\\mathbf{g}_2^\\top \\mathbf{p}_2$ determines the integer output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing FEM residual assembly and L-BFGS,\n    then running the specified test cases.\n    \"\"\"\n\n    # ------------------\n    # UTILITY FUNCTIONS\n    # ------------------\n\n    def assemble_residual(u_interior, f_func, Ne):\n        \"\"\"\n        Assembles the nonlinear FE residual vector R(u).\n        \"\"\"\n        h = 1.0 / Ne\n        num_dofs = Ne - 1\n        \n        # Prepend and append boundary conditions (u=0)\n        u_full = np.concatenate(([0.0], u_interior, [0.0]))\n        \n        R = np.zeros(num_dofs)\n        \n        # Gaussian quadrature points and weights (for reference element [-1, 1])\n        # Points are +/- 1/sqrt(3), weights are 1.\n        xi_gp = np.array([-1.0 / np.sqrt(3.0), 1.0 / np.sqrt(3.0)])\n        \n        Jacobian = h / 2.0\n        \n        for e in range(Ne):\n            # Element node indices\n            node_i, node_j = e, e + 1\n            \n            # Nodal values for the element\n            u_element = u_full[node_i : node_j + 1]\n            \n            # Element residual vector\n            r_element = np.zeros(2)\n            \n            # Constant gradient over the linear element\n            u_grad_e = (u_element[1] - u_element[0]) / h\n            \n            # Derivatives of shape functions w.r.t physical coordinate x\n            dN_dx = np.array([-1.0 / h, 1.0 / h])\n            \n            # Loop over Gauss points\n            for xi in xi_gp:\n                # Shape functions on reference element\n                N_vals = np.array([(1.0 - xi) / 2.0, (1.0 + xi) / 2.0])\n                \n                # Interpolate u at the Gauss point\n                u_at_gp = np.dot(N_vals, u_element)\n                \n                # Nonlinear coefficient a(u) = 1 + u^2\n                a_u = 1.0 + u_at_gp**2\n                \n                # Map Gauss point to physical coordinate\n                x_nodes = np.array([e * h, (e + 1) * h])\n                x_at_gp = np.dot(N_vals, x_nodes)\n                \n                # Evaluate source term at the Gauss point\n                f_at_gp = f_func(x_at_gp)\n                \n                # Accumulate element residual contribution (weight is 1)\n                stiffness_term = a_u * u_grad_e * dN_dx\n                load_term = f_at_gp * N_vals\n                r_element += (stiffness_term - load_term) * Jacobian\n\n            # Assemble element residual into global residual vector\n            # r_element[0] corresponds to global node `e`\n            # r_element[1] corresponds to global node `e+1`\n            # DOF indices are shifted by -1 from global node indices\n            if e > 0:\n                R[e - 1] += r_element[0]\n            if e + 1 < Ne + 1 - 1: # e+1 < Ne\n                R[e] += r_element[1]\n                \n        return R\n\n    def lbfgs_direction(g_k, history, gamma_k):\n        \"\"\"\n        Computes the L-BFGS search direction using the two-loop recursion.\n        \"\"\"\n        q = g_k.copy()\n        alphas = []\n        rhos = []\n        \n        # First loop: backward pass from k-1 to k-m\n        for s, y in reversed(history):\n            rho = 1.0 / np.dot(y, s)\n            rhos.insert(0, rho)\n            alpha = rho * np.dot(s, q)\n            alphas.insert(0, alpha)\n            q -= alpha * y\n        \n        # Initial Hessian approximation scaling\n        r = gamma_k * q\n        \n        # Second loop: forward pass from k-m to k-1\n        for i, (s, y) in enumerate(history):\n            rho = rhos[i]\n            alpha = alphas[i]\n            beta = rho * np.dot(y, r)\n            r += s * (alpha - beta)\n            \n        return -r\n\n    # ------------------\n    # PROBLEM SETUP\n    # ------------------\n    Ne = 16\n    f_source = lambda x: 10.0 * np.sin(2.0 * np.pi * x)\n    num_dofs = Ne - 1 # N_e+1-2 = N_e-1 = 15\n    seed = 12345\n    \n    rng = np.random.default_rng(seed)\n    \n    # Generate vectors as per problem statement\n    u_0 = 0.1 * rng.standard_normal(num_dofs)\n    s_0 = 1e-2 * rng.standard_normal(num_dofs)\n    s_1 = 1e-2 * rng.standard_normal(num_dofs)\n    \n    u_1 = u_0 + s_0\n    u_2 = u_1 + s_1\n    \n    # Compute residuals (gradients)\n    g_0 = assemble_residual(u_0, f_source, Ne)\n    g_1 = assemble_residual(u_1, f_source, Ne)\n    g_2 = assemble_residual(u_2, f_source, Ne)\n    \n    # Compute history pairs\n    y_0 = g_1 - g_0\n    y_1 = g_2 - g_1\n    \n    history_full = [(s_0, y_0), (s_1, y_1)]\n    \n    results = []\n\n    # ------------------\n    # TEST 1: m=0\n    # ------------------\n    gamma_1 = 1.0\n    history_1 = []\n    p2_test1 = lbfgs_direction(g_2, history_1, gamma_1)\n    result1 = np.linalg.norm(p2_test1 + g_2)\n    results.append(result1)\n\n    # ------------------\n    # TEST 2: m=2\n    # ------------------\n    s1_T_y1 = np.dot(s_1, y_1)\n    y1_T_y1 = np.dot(y_1, y_1)\n    # The problem implies this pair passes curvature, so s1_T_y1 and y1_T_y1 > 0\n    gamma_2 = s1_T_y1 / y1_T_y1 if y1_T_y1 > 0 else 1.0\n    history_2 = history_full # Assume pairs satisfy curvature as per \"happy path\"\n    p2_test2 = lbfgs_direction(g_2, history_2, gamma_2)\n    result2 = np.dot(g_2, p2_test2)\n    results.append(result2)\n\n    # ------------------\n    # TEST 3: Curvature safeguard\n    # ------------------\n    epsilon = 1e-12\n    tau = 1e-10\n    y0_hat = epsilon * s_0\n    history_mod = [(s_0, y0_hat), (s_1, y_1)]\n    \n    history_3 = []\n    for s, y in history_mod:\n        if np.dot(s, y) > tau:\n            history_3.append((s, y))\n            \n    # Scaling based on the most recent valid pair, which is (s_1, y_1)\n    s_last, y_last = history_3[-1] if history_3 else (None, None)\n    if s_last is not None:\n        gamma_3 = np.dot(s_last, y_last) / np.dot(y_last, y_last)\n    else: # Fallback if all pairs are discarded\n        gamma_3 = 1.0\n\n    p2_test3 = lbfgs_direction(g_2, history_3, gamma_3)\n    g2_T_p2_test3 = np.dot(g_2, p2_test3)\n    result3 = 1 if g2_T_p2_test3 < 0 else 0\n    results.append(result3)\n    \n    # ------------------\n    # FINAL OUTPUT\n    # ------------------\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n```", "id": "2580610"}, {"introduction": "In real-world finite element analysis, convergence issues are common, and diagnosing them is a critical skill. This problem presents a convergence log from an Inexact Newton solver and asks you to identify the root cause of its failure to converge [@problem_id:2580751]. Analyzing this scenario will teach you to recognize the signature of insufficient linear solver accuracy, a frequent roadblock in complex simulations, and understand how to correct it.", "problem": "A nonlinear finite element equilibrium problem with internal variables is solved at a single load step using a globalized Newton framework with backtracking line search and an iterative linear solver. The finite element residual is denoted by $R(u)$, and the consistent tangent (Jacobian) is recomputed whenever specified below. The iterative linear solver is the Generalized Minimal Residual (GMRES) method, preconditioned by an incomplete factorization. The line search enforces a sufficient decrease condition; if a full step is accepted, the step length is recorded as $\\alpha_k = 1$. The nonlinear residual norm $\\lVert R_k \\rVert$ (measured in a consistent energy norm) and selected algorithmic diagnostics for each outer iteration $k$ are recorded as follows:\n\n- $k = 0$: $\\lVert R_0 \\rVert = 1.2 \\times 10^{1}$; tangent recomputed; GMRES terminated when $\\lVert J_0 s_0 + R_0 \\rVert / \\lVert R_0 \\rVert \\le 1.0 \\times 10^{-2}$; accepted $\\alpha_0 = 1$; obtained $\\lVert R_1 \\rVert = 2.3$.\n- $k = 1$: tangent recomputed; GMRES termination as above with relative linear residual $\\le 1.0 \\times 10^{-2}$; accepted $\\alpha_1 = 1$; obtained $\\lVert R_2 \\rVert = 4.8 \\times 10^{-1}$.\n- $k = 2$: tangent recomputed; GMRES termination as above; accepted $\\alpha_2 = 1$; obtained $\\lVert R_3 \\rVert = 1.1 \\times 10^{-1}$.\n- $k = 3$: tangent recomputed; GMRES termination as above; accepted $\\alpha_3 = 1$; obtained $\\lVert R_4 \\rVert = 2.7 \\times 10^{-2}$.\n- $k = 4$: tangent recomputed; GMRES termination as above; accepted $\\alpha_4 = 1$; obtained $\\lVert R_5 \\rVert = 2.5 \\times 10^{-2}$.\n- $k = 5$: tangent recomputed; GMRES termination as above; accepted $\\alpha_5 = 1$; obtained $\\lVert R_6 \\rVert = 2.4 \\times 10^{-2}$.\n\nNo backtracking was reported in any iteration, and the number of GMRES iterations per outer step remained nearly constant. The prescribed nonlinear convergence tolerance is $\\lVert R_k \\rVert \\le 1.0 \\times 10^{-8}$.\n\nBased on a first-principles analysis of Newton-type methods in finite element contexts, diagnose the most plausible primary cause of the observed stagnation near $\\lVert R_k \\rVert \\approx 2.4 \\times 10^{-2}$, and select the option that provides a consistent and effective corrective action.\n\nA. Stagnation is due to inexact linear solves with a forcing term bounded below by $10^{-2}$; tighten the linear solve accuracy adaptively so that the relative linear residual $\\eta_k$ decreases with $\\lVert R_k \\rVert$ (for example, using an Eisenstat–Walker forcing sequence), lower any hard floor below $10^{-6}$, and improve preconditioning to allow GMRES to meet stricter tolerances.\n\nB. Stagnation is due to poor tangents from a modified Newton strategy using a frozen Jacobian; switch to full Newton by recomputing the tangent every iteration or to a Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton update to restore superlinear convergence.\n\nC. Stagnation is due to an overly conservative line search that rejects full steps; relax the sufficient decrease parameter to accept larger $\\alpha_k$, or replace line search with a trust-region method to avoid step clipping.\n\nD. Stagnation is due to nonconvexity leading to indefinite tangents and ascent directions; add Levenberg–Marquardt damping to force positive definiteness and ensure descent, even if it increases the number of line search backtracks.", "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\nStep 1: Extract Givens\n- Method: Globalized Newton framework for a nonlinear finite element equilibrium problem.\n- Globalization: Backtracking line search enforcing a sufficient decrease condition. A full step is denoted by $\\alpha_k = 1$.\n- Residual: $R(u)$.\n- Tangent/Jacobian: $J_k$. Recomputed at each specified iteration.\n- Linear Solver: Iterative, Generalized Minimal Residual (GMRES) method with incomplete factorization preconditioning.\n- Linear Solver Termination Criterion: $\\lVert J_k s_k + R_k \\rVert / \\lVert R_k \\rVert \\le 1.0 \\times 10^{-2}$ for all iterations $k$.\n- Nonlinear Convergence Tolerance: $\\lVert R_k \\rVert \\le 1.0 \\times 10^{-8}$.\n- Iteration History:\n  - $k=0$: $\\lVert R_0 \\rVert = 1.2 \\times 10^{1}$; tangent recomputed; linear solve as specified; accepted $\\alpha_0 = 1$; resulting $\\lVert R_1 \\rVert = 2.3$.\n  - $k=1$: Tangent recomputed; linear solve as specified; accepted $\\alpha_1 = 1$; resulting $\\lVert R_2 \\rVert = 4.8 \\times 10^{-1}$.\n  - $k=2$: Tangent recomputed; linear solve as specified; accepted $\\alpha_2 = 1$; resulting $\\lVert R_3 \\rVert = 1.1 \\times 10^{-1}$.\n  - $k=3$: Tangent recomputed; linear solve as specified; accepted $\\alpha_3 = 1$; resulting $\\lVert R_4 \\rVert = 2.7 \\times 10^{-2}$.\n  - $k=4$: Tangent recomputed; linear solve as specified; accepted $\\alpha_4 = 1$; resulting $\\lVert R_5 \\rVert = 2.5 \\times 10^{-2}$.\n  - $k=5$: Tangent recomputed; linear solve as specified; accepted $\\alpha_5 = 1$; resulting $\\lVert R_6 \\rVert = 2.4 \\times 10^{-2}$.\n- Additional Observations: No backtracking reported. Number of GMRES iterations per outer step remained nearly constant.\n\nStep 2: Validate Using Extracted Givens\n- Scientific Grounding: The problem describes a standard Inexact Newton method applied to a nonlinear finite element problem. The use of GMRES, line search, and the observed convergence behavior are scientifically realistic and well-documented phenomena in computational science and engineering.\n- Well-Posedness: The problem provides sufficient numerical data to diagnose a common performance issue and asks for the most plausible cause and corrective action. A unique, logical conclusion can be drawn from the provided information.\n- Objectivity: The problem is stated using precise, objective, and quantitative language.\n\nThe problem is scientifically grounded, well-posed, and objective, with no evident flaws, contradictions, or ambiguities in the core statement. The data provided is consistent and sufficient for analysis.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\nThe core of the algorithm is the Inexact Newton iteration. At each outer iteration $k$, we seek a displacement update $s_k$ by approximately solving the linear system:\n$$ J_k s_k = -R_k $$\nwhere $J_k = J(u_k)$ is the Jacobian matrix at the current displacement iterate $u_k$, and $R_k = R(u_k)$ is the residual vector. The subsequent update is $u_{k+1} = u_k + \\alpha_k s_k$.\n\nThe problem states that the tangent is recomputed at every iteration ($k = 0, 1, 2, 3, 4, 5$). This constitutes a full Newton method, not a modified or quasi-Newton method where the tangent is held constant or approximated.\n\nThe linear system is solved iteratively using GMRES until the relative linear residual satisfies:\n$$ \\frac{\\lVert J_k s_k + R_k \\rVert}{\\lVert R_k \\rVert} \\le \\eta_k $$\nFrom the given data, the forcing term $\\eta_k$ is constant for all $k$:\n$$ \\eta_k = 1.0 \\times 10^{-2} $$\n\nLet us analyze the convergence behavior.\nThe residual norm converges as follows: $12 \\to 2.3 \\to 0.48 \\to 0.11 \\to 0.027 \\to 0.025 \\to 0.024$.\nThe convergence is initially linear:\n- $\\lVert R_1 \\rVert / \\lVert R_0 \\rVert \\approx 2.3/12 \\approx 0.19$\n- $\\lVert R_2 \\rVert / \\lVert R_1 \\rVert \\approx 0.48/2.3 \\approx 0.21$\n- $\\lVert R_3 \\rVert / \\lVert R_2 \\rVert \\approx 0.11/0.48 \\approx 0.23$\nA full Newton method, when close to the solution, should exhibit quadratic convergence. The observed linear convergence is the first sign of a problem.\n\nThe convergence then stagnates. The residual norm hovers around $2.4 \\times 10^{-2}$, far from the desired tolerance of $1.0 \\times 10^{-8}$.\n\nThe theory of Inexact Newton methods provides the explanation. The residual at the next step, $R_{k+1} = R(u_k + \\alpha_k s_k)$, can be approximated by a Taylor series expansion. For a full step ($\\alpha_k = 1$), this is:\n$$ R_{k+1} = R(u_k + s_k) \\approx R_k + J_k s_k $$\nThe term $R_k + J_k s_k$ is precisely the residual of the linear system solved at step $k$. Let us denote it by $r_k^{\\text{lin}}$. Thus, $\\lVert R_{k+1} \\rVert \\approx \\lVert r_k^{\\text{lin}} \\rVert$. The GMRES stopping criterion ensures that $\\lVert r_k^{\\text{lin}} \\rVert \\le \\eta_k \\lVert R_k \\rVert$.\n\nThis implies that $\\lVert R_{k+1} \\rVert \\lesssim \\eta_k \\lVert R_k \\rVert$. With $\\eta_k = 1.0 \\times 10^{-2}$, we expect at best linear convergence with a rate of about $10^{-2}$. The observed initial rates are slower, which is typical when not yet in the asymptotic regime.\n\nCrucially, for the nonlinear iteration to converge to high accuracy, the forcing term $\\eta_k$ must tend to zero as $k \\to \\infty$. A constant $\\eta_k$ imposes a hard limit on the achievable accuracy. The nonlinear residual $\\lVert R_k \\rVert$ cannot be reduced much further once it is on the same order of magnitude as the error introduced from the inexact linear solve. The stagnation level is therefore determined by $\\eta_k$. In this problem, stagnation occurs at $\\lVert R_k \\rVert \\approx 2.4 \\times 10^{-2}$, which is entirely consistent with the fixed linear solver tolerance $\\eta_k = 1.0 \\times 10^{-2}$. The method cannot reduce the nonlinear residual further because the search direction $s_k$ is not accurate enough.\n\nOption-by-Option Analysis:\n\nA. Stagnation is due to inexact linear solves with a forcing term bounded below by $10^{-2}$; tighten the linear solve accuracy adaptively so that the relative linear residual $\\eta_k$ decreases with $\\lVert R_k \\rVert$ (for example, using an Eisenstat–Walker forcing sequence), lower any hard floor below $10^{-6}$, and improve preconditioning to allow GMRES to meet stricter tolerances.\nThis diagnosis is perfectly aligned with the analysis above. The constant forcing term $\\eta_k = 1.0 \\times 10^{-2}$ is the direct cause of the stagnation. The proposed corrective action—making $\\eta_k$ adaptive, for example $\\eta_k \\propto \\lVert R_k \\rVert$ via an Eisenstat–Walker strategy—is the standard and correct way to restore fast (superlinear or quadratic) convergence in an Inexact Newton method. Improving preconditioning is a necessary practical step to achieve the tighter linear tolerances efficiently. This option correctly identifies the cause and proposes a complete and effective solution. **Correct**.\n\nB. Stagnation is due to poor tangents from a modified Newton strategy using a frozen Jacobian; switch to full Newton by recomputing the tangent every iteration or to a Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton update to restore superlinear convergence.\nThis diagnosis is factually incorrect. The problem statement explicitly says, \"tangent recomputed\" for every iteration from $k=0$ to $k=5$. The algorithm employed is already a full Newton method, not a modified Newton method with a frozen Jacobian. Therefore, the proposed correction to \"switch to full Newton\" is nonsensical. **Incorrect**.\n\nC. Stagnation is due to an overly conservative line search that rejects full steps; relax the sufficient decrease parameter to accept larger $\\alpha_k$, or replace line search with a trust-region method to avoid step clipping.\nThis diagnosis is factually incorrect. The problem statement explicitly reports \"accepted $\\alpha_k = 1$\" for all iterations. This means the line search accepted the full Newton step every time and never performed any backtracking or step reduction (\"clipping\"). The line search is behaving as permissively as possible, not conservatively. Proposing to accept larger $\\alpha_k$ is impossible, as $\\alpha_k$ is already at its maximum value of $1$. **Incorrect**.\n\nD. Stagnation is due to nonconvexity leading to indefinite tangents and ascent directions; add Levenberg–Marquardt damping to force positive definiteness and ensure descent, even if it increases the number of line search backtracks.\nThis diagnosis is inconsistent with the provided data. If the Newton direction $s_k$ were not a descent direction due to an indefinite tangent, the line search would fail to satisfy the sufficient decrease condition with a full step. This would force backtracking, resulting in $\\alpha_k < 1$. The data, however, shows $\\alpha_k=1$ for all iterations, which is strong evidence that the computed search directions are effective descent directions. The Globalization strategy (line search) is not the source of the failure; the local convergence properties of the Inexact Newton update are. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2580751"}]}