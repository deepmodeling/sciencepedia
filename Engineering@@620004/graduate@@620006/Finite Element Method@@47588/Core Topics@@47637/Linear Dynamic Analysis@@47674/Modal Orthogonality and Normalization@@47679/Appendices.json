{"hands_on_practices": [{"introduction": "Understanding modal orthogonality begins with recognizing that the 'geometry' of a dynamic system is defined by its mass distribution, not by our standard Euclidean intuition. This first practice provides a concrete, calculation-based exploration of this principle by contrasting the familiar Euclidean inner product with the physically correct mass-weighted ($M$-weighted) inner product. By calculating the Modal Assurance Criterion (MAC) for both cases on a simple two-degree-of-freedom system, you will gain a tangible understanding of why eigenvectors that are orthogonal in the system's natural metric may not appear so in a conventional geometric sense [@problem_id:2578481].", "problem": "Consider the undamped free-vibration generalized eigenproblem from the Finite Element Method (FEM) for a two-degree-of-freedom system,\n$$\n\\mathbf{K}\\,\\boldsymbol{\\phi}=\\lambda\\,\\mathbf{M}\\,\\boldsymbol{\\phi},\n$$\nwith symmetric positive-definite mass and stiffness matrices\n$$\n\\mathbf{M}=\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix},\\qquad\n\\mathbf{K}=\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix}.\n$$\nStart from the fundamental definitions that (i) free vibrations satisfy the generalized eigenproblem above, (ii) the modes of distinct eigenvalues are orthogonal with respect to the $\\mathbf{M}$-weighted inner product, and (iii) for any inner product $\\langle\\cdot,\\cdot\\rangle$ on $\\mathbb{R}^n$, the squared cosine of the angle between vectors $\\mathbf{a}$ and $\\mathbf{b}$ is given by\n$$\n\\frac{|\\langle \\mathbf{a},\\mathbf{b}\\rangle|^2}{\\langle \\mathbf{a},\\mathbf{a}\\rangle\\,\\langle \\mathbf{b},\\mathbf{b}\\rangle},\n$$\nwhich defines the Modal Assurance Criterion (MAC) once an inner product has been chosen.\n\n1) Solve the generalized eigenproblem to obtain analytic eigenpairs $(\\lambda_1,\\boldsymbol{\\phi}_1)$ and $(\\lambda_2,\\boldsymbol{\\phi}_2)$, and verify the $\\mathbf{M}$-orthogonality of the two modes by computing $\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\,\\boldsymbol{\\phi}_2$ and the Euclidean non-orthogonality via $\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\boldsymbol{\\phi}_2$.\n\n2) Suppose a coarse discretization produces an approximate first-mode shape\n$$\n\\tilde{\\boldsymbol{\\phi}}_1=\\begin{pmatrix}1 \\\\ \\tfrac{9}{5}\\end{pmatrix}.\n$$\nCompute the Modal Assurance Criterion (MAC) between the exact $\\boldsymbol{\\phi}_1$ and the approximate $\\tilde{\\boldsymbol{\\phi}}_1$ with respect to:\n- the Euclidean inner product $\\langle \\mathbf{a},\\mathbf{b}\\rangle_{\\mathrm{E}}=\\mathbf{a}^{\\mathsf{T}}\\mathbf{b}$,\n- the $\\mathbf{M}$-weighted inner product $\\langle \\mathbf{a},\\mathbf{b}\\rangle_{\\mathbf{M}}=\\mathbf{a}^{\\mathsf{T}}\\mathbf{M}\\mathbf{b}$.\n\n3) Briefly interpret why the two MAC values differ in this example in light of modal orthogonality and normalization principles.\n\nProvide as your final deliverable a single exact number equal to\n$$\n\\Delta=\\mathrm{MAC}_{\\mathbf{M}}(\\boldsymbol{\\phi}_1,\\tilde{\\boldsymbol{\\phi}}_1)-\\mathrm{MAC}_{\\mathrm{E}}(\\boldsymbol{\\phi}_1,\\tilde{\\boldsymbol{\\phi}}_1),\n$$\nexpressed as a fully reduced fraction. Do not include units. Do not round; give the exact value.", "solution": "The problem statement is first subjected to validation.\n\nGivens are extracted verbatim:\n1.  The governing equation is the generalized eigenproblem $\\mathbf{K}\\,\\boldsymbol{\\phi}=\\lambda\\,\\mathbf{M}\\,\\boldsymbol{\\phi}$.\n2.  The mass matrix is $\\mathbf{M}=\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}$.\n3.  The stiffness matrix is $\\mathbf{K}=\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix}$.\n4.  $\\mathbf{M}$ and $\\mathbf{K}$ are stated to be symmetric positive-definite.\n5.  Modes of distinct eigenvalues are orthogonal with respect to the $\\mathbf{M}$-weighted inner product.\n6.  The squared cosine of the angle between vectors $\\mathbf{a}$ and $\\mathbf{b}$ for an inner product $\\langle\\cdot,\\cdot\\rangle$ is given by $\\frac{|\\langle \\mathbf{a},\\mathbf{b}\\rangle|^2}{\\langle \\mathbf{a},\\mathbf{a}\\rangle\\,\\langle \\mathbf{b},\\mathbf{b}\\rangle}$, which defines the Modal Assurance Criterion (MAC).\n7.  An approximate first-mode shape is given as $\\tilde{\\boldsymbol{\\phi}}_1=\\begin{pmatrix}1 \\\\ \\tfrac{9}{5}\\end{pmatrix}$.\n8.  Two inner products are defined: Euclidean $\\langle \\mathbf{a},\\mathbf{b}\\rangle_{\\mathrm{E}}=\\mathbf{a}^{\\mathsf{T}}\\mathbf{b}$ and $\\mathbf{M}$-weighted $\\langle \\mathbf{a},\\mathbf{b}\\rangle_{\\mathbf{M}}=\\mathbf{a}^{\\mathsf{T}}\\mathbf{M}\\mathbf{b}$.\n9.  The final deliverable is $\\Delta=\\mathrm{MAC}_{\\mathbf{M}}(\\boldsymbol{\\phi}_1,\\tilde{\\boldsymbol{\\phi}}_1)-\\mathrm{MAC}_{\\mathrm{E}}(\\boldsymbol{\\phi}_1,\\tilde{\\boldsymbol{\\phi}}_1)$.\n\nThe problem is validated as follows:\n- **Scientifically Grounded:** The problem pertains to standard modal analysis of a linear time-invariant mechanical system, a core topic in engineering and physics. The matrices are physically plausible. We verify the positive-definiteness. For $\\mathbf{M}$, the eigenvalues are $2$ and $1$, which are positive. For $\\mathbf{K}$, the leading principal minors are $\\det(3)=3>0$ and $\\det(\\mathbf{K})=3(1)-(-1)(-1)=2>0$. Both matrices are indeed symmetric and positive-definite.\n- **Well-Posed:** The problem is a standard generalized eigenvalue problem with symmetric positive-definite matrices, which guarantees real, positive eigenvalues and a set of $\\mathbf{M}$-orthogonal eigenvectors. The tasks are specific and lead to a unique numerical answer.\n- **Objective:** The problem is stated using precise mathematical language without subjectivity.\n\nThe verdict is that the problem is valid. We proceed to the solution.\n\nThe solution is found by following the three parts of the problem description.\n\nPart 1: Solve the generalized eigenproblem and verify orthogonality.\nThe eigenproblem is $(\\mathbf{K} - \\lambda\\mathbf{M})\\boldsymbol{\\phi} = \\mathbf{0}$. For non-trivial solutions, the determinant of the characteristic matrix must be zero:\n$$\n\\det(\\mathbf{K} - \\lambda\\mathbf{M}) = \\det\\left(\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix} - \\lambda \\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\right) = \\det\\begin{pmatrix}3 - 2\\lambda & -1 \\\\ -1 & 1 - \\lambda\\end{pmatrix} = 0\n$$\nThis gives the characteristic equation:\n$$\n(3 - 2\\lambda)(1 - \\lambda) - (-1)^2 = 0\n$$\n$$\n3 - 3\\lambda - 2\\lambda + 2\\lambda^2 - 1 = 0\n$$\n$$\n2\\lambda^2 - 5\\lambda + 2 = 0\n$$\nSolving this quadratic equation for $\\lambda$:\n$$\n\\lambda = \\frac{-(-5) \\pm \\sqrt{(-5)^2 - 4(2)(2)}}{2(2)} = \\frac{5 \\pm \\sqrt{25-16}}{4} = \\frac{5 \\pm 3}{4}\n$$\nThe eigenvalues are $\\lambda_1 = \\frac{5-3}{4} = \\frac{1}{2}$ and $\\lambda_2 = \\frac{5+3}{4} = 2$.\n\nFor each eigenvalue, we find the corresponding eigenvector (mode shape).\nFor $\\lambda_1 = \\frac{1}{2}$:\n$$\n(\\mathbf{K} - \\lambda_1\\mathbf{M})\\boldsymbol{\\phi}_1 = \\begin{pmatrix}3 - 2(\\frac{1}{2}) & -1 \\\\ -1 & 1 - \\frac{1}{2}\\end{pmatrix} \\boldsymbol{\\phi}_1 = \\begin{pmatrix}2 & -1 \\\\ -1 & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}\\phi_{11} \\\\ \\phi_{12}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThis yields the equation $2\\phi_{11} - \\phi_{12} = 0$, or $\\phi_{12} = 2\\phi_{11}$. Choosing $\\phi_{11} = 1$ gives $\\boldsymbol{\\phi}_1 = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$.\n\nFor $\\lambda_2 = 2$:\n$$\n(\\mathbf{K} - \\lambda_2\\mathbf{M})\\boldsymbol{\\phi}_2 = \\begin{pmatrix}3 - 2(2) & -1 \\\\ -1 & 1 - 2\\end{pmatrix} \\boldsymbol{\\phi}_2 = \\begin{pmatrix}-1 & -1 \\\\ -1 & -1\\end{pmatrix} \\begin{pmatrix}\\phi_{21} \\\\ \\phi_{22}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThis yields the equation $-\\phi_{21} - \\phi_{22} = 0$, or $\\phi_{22} = -\\phi_{21}$. Choosing $\\phi_{21} = 1$ gives $\\boldsymbol{\\phi}_2 = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$.\n\nThe eigenpairs are $(\\lambda_1, \\boldsymbol{\\phi}_1) = (\\frac{1}{2}, \\begin{pmatrix}1 \\\\ 2\\end{pmatrix})$ and $(\\lambda_2, \\boldsymbol{\\phi}_2) = (2, \\begin{pmatrix}1 \\\\ -1\\end{pmatrix})$.\n\nWe verify the orthogonality properties:\n$\\mathbf{M}$-orthogonality:\n$$\n\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_2 = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1\\end{pmatrix} = 1(2) + 2(-1) = 0\n$$\nThe modes are $\\mathbf{M}$-orthogonal, as theory dictates.\nEuclidean non-orthogonality:\n$$\n\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\boldsymbol{\\phi}_2 = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} = 1(1) + 2(-1) = -1\n$$\nThe modes are not orthogonal in the Euclidean sense.\n\nPart 2: Compute the MAC values.\nWe compare the exact first mode $\\boldsymbol{\\phi}_1 = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$ with the approximate mode $\\tilde{\\boldsymbol{\\phi}}_1 = \\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix}$.\n\nThe MAC with respect to the Euclidean inner product is:\n$$\n\\mathrm{MAC}_{\\mathrm{E}}(\\boldsymbol{\\phi}_1, \\tilde{\\boldsymbol{\\phi}}_1) = \\frac{|\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\tilde{\\boldsymbol{\\phi}}_1|^2}{(\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\boldsymbol{\\phi}_1)(\\tilde{\\boldsymbol{\\phi}}_1^{\\mathsf{T}}\\tilde{\\boldsymbol{\\phi}}_1)}\n$$\nThe terms are:\n$\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\tilde{\\boldsymbol{\\phi}}_1 = 1(1) + 2(\\frac{9}{5}) = 1 + \\frac{18}{5} = \\frac{23}{5}$.\n$\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\boldsymbol{\\phi}_1 = 1^2 + 2^2 = 5$.\n$\\tilde{\\boldsymbol{\\phi}}_1^{\\mathsf{T}}\\tilde{\\boldsymbol{\\phi}}_1 = 1^2 + (\\frac{9}{5})^2 = 1 + \\frac{81}{25} = \\frac{25+81}{25} = \\frac{106}{25}$.\n$$\n\\mathrm{MAC}_{\\mathrm{E}} = \\frac{(\\frac{23}{5})^2}{5 \\cdot \\frac{106}{25}} = \\frac{\\frac{529}{25}}{\\frac{530}{25}} = \\frac{529}{530}\n$$\n\nThe MAC with respect to the $\\mathbf{M}$-weighted inner product is:\n$$\n\\mathrm{MAC}_{\\mathbf{M}}(\\boldsymbol{\\phi}_1, \\tilde{\\boldsymbol{\\phi}}_1) = \\frac{|\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\tilde{\\boldsymbol{\\phi}}_1|^2}{(\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_1)(\\tilde{\\boldsymbol{\\phi}}_1^{\\mathsf{T}}\\mathbf{M}\\tilde{\\boldsymbol{\\phi}}_1)}\n$$\nThe terms are:\n$\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\tilde{\\boldsymbol{\\phi}}_1 = \\begin{pmatrix}1 & 2\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = \\begin{pmatrix}2 & 2\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = 2(1) + 2(\\frac{9}{5}) = 2 + \\frac{18}{5} = \\frac{28}{5}$.\n$\\boldsymbol{\\phi}_1^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_1 = \\begin{pmatrix}1 & 2\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 2\\end{pmatrix} = 1(2)(1) + 2(1)(2) = 2+4 = 6$.\n$\\tilde{\\boldsymbol{\\phi}}_1^{\\mathsf{T}}\\mathbf{M}\\tilde{\\boldsymbol{\\phi}}_1 = \\begin{pmatrix}1 & \\frac{9}{5}\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = 1(2)(1) + \\frac{9}{5}(1)(\\frac{9}{5}) = 2 + \\frac{81}{25} = \\frac{50+81}{25} = \\frac{131}{25}$.\n$$\n\\mathrm{MAC}_{\\mathbf{M}} = \\frac{(\\frac{28}{5})^2}{6 \\cdot \\frac{131}{25}} = \\frac{\\frac{784}{25}}{\\frac{786}{25}} = \\frac{784}{786} = \\frac{392}{393}\n$$\n\nPart 3: Interpretation.\nThe Modal Assurance Criterion (MAC) is a measure of collinearity, interpreted as the squared cosine of the angle between two vectors. This \"angle\" is defined by the choice of inner product.\nThe Euclidean inner product $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{E}}$ defines a standard geometric angle.\nThe $\\mathbf{M}$-weighted inner product $\\langle\\cdot,\\cdot\\rangle_{\\mathbf{M}}$ defines an angle in a space where coordinates are scaled according to the mass distribution of the physical system. This is the natural inner product for structural dynamics, as it is related to kinetic energy, and it is the one with respect to which the system's modes are orthogonal.\nThe two MAC values differ because the underlying geometric structures used to measure collinearity are different. Unless the mass matrix $\\mathbf{M}$ is a scalar multiple of the identity matrix, the Euclidean and $\\mathbf{M}$-weighted inner products will define different geometries, and thus yield different MAC values for the same pair of vectors.\n\nFinal deliverable calculation:\nWe are required to compute $\\Delta = \\mathrm{MAC}_{\\mathbf{M}} - \\mathrm{MAC}_{\\mathrm{E}}$.\n$$\n\\Delta = \\frac{392}{393} - \\frac{529}{530}\n$$\nTo subtract, we find a common denominator, which is $393 \\times 530$.\n$$\n\\Delta = \\frac{392 \\times 530 - 529 \\times 393}{393 \\times 530}\n$$\nThe numerator is:\n$392 \\times 530 = 207760$.\n$529 \\times 393 = 207897$.\nNumerator $= 207760 - 207897 = -137$.\nThe denominator is:\n$393 \\times 530 = 208290$.\nThus, the difference is:\n$$\n\\Delta = -\\frac{137}{208290}\n$$\nThe number $137$ is prime, and $208290$ is not divisible by $137$. Therefore, this fraction is in its simplest form.", "answer": "$$\n\\boxed{-\\frac{137}{208290}}\n$$", "id": "2578481"}, {"introduction": "While theoretical derivations guarantee that eigenvectors from a symmetric generalized eigenproblem are $M$-orthogonal, numerical solvers operate in finite precision and produce only approximations. This hands-on coding exercise challenges you to bridge the gap between theory and practice by developing a robust procedure to numerically verify $M$-orthogonality for a computed set of eigenvectors [@problem_id:2578532]. You will implement a scale-invariant metric based on the system's Gram matrix, a fundamental tool for assessing the quality of modal analysis results in real-world engineering applications.", "problem": "Consider the generalized symmetric definite eigenvalue problem in the finite element method context: given matrices $K \\in \\mathbb{R}^{n \\times n}$ and $M \\in \\mathbb{R}^{n \\times n}$ with $K = K^T$ and $M = M^T \\succ 0$, an eigenpair $(\\lambda, \\phi)$ satisfies $K \\phi = \\lambda M \\phi$. In exact arithmetic, eigenvectors associated with distinct eigenvalues are orthogonal with respect to the $M$-inner product. For a computed set of column vectors $\\{\\hat{\\phi}_i\\}_{i=1}^m$ assembled as $\\hat{\\Phi} \\in \\mathbb{R}^{n \\times m}$, numerical verification of $M$-orthogonality requires careful handling of scaling and roundoff.\n\nTask: Design and implement a procedure that, given $K$, $M$, a matrix $\\hat{\\Phi}$ whose columns are the computed eigenvectors, and a tolerance $\\epsilon > 0$, returns a boolean indicating whether the set $\\{\\hat{\\phi}_i\\}$ is $M$-orthogonal within tolerance $\\epsilon$. The verification must be based solely on fundamental definitions: the $M$-inner product $\\langle x, y \\rangle_M := x^T M y$ and the definition of orthogonality $\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M = 0$ for $i \\neq j$. Your procedure must be invariant to nonzero rescaling of the columns of $\\hat{\\Phi}$ and must robustly detect invalid inputs such as zero columns (which violate the positive definiteness implication $\\hat{\\phi}_i^T M \\hat{\\phi}_i > 0$ for nonzero $\\hat{\\phi}_i$).\n\nSpecification of the verification procedure:\n- Inputs: $K \\in \\mathbb{R}^{n \\times n}$ with $K = K^T$, $M \\in \\mathbb{R}^{n \\times n}$ with $M = M^T \\succ 0$, $\\hat{\\Phi} \\in \\mathbb{R}^{n \\times m}$, and $\\epsilon \\in \\mathbb{R}$ with $\\epsilon > 0$.\n- Compute the Gram matrix $G := \\hat{\\Phi}^T M \\hat{\\Phi} \\in \\mathbb{R}^{m \\times m}$, with entries $g_{ij} = \\hat{\\phi}_i^T M \\hat{\\phi}_j$.\n- Let $d_i := g_{ii}$ for $i \\in \\{1,\\dots,m\\}$. If any $d_i \\le 0$, declare the set not $M$-orthogonal (return false).\n- Define the normalized correlation coefficients $c_{ij} := \\dfrac{g_{ij}}{\\sqrt{d_i d_j}}$ for $i \\ne j$. This normalization removes sensitivity to column scaling.\n- Let $\\eta := \\max_{i \\ne j} |c_{ij}|$. For the case $m = 1$, define $\\eta := 0$.\n- Return true if and only if $\\eta \\le \\epsilon$; otherwise return false.\n\nYour program must implement this verification as a pure function and apply it to the following deterministic test suite. In each test case, use the provided $M$, $\\hat{\\Phi}$, and $\\epsilon$. You may set $K$ to any symmetric matrix of compatible size; it will not be used by the verification but is included to maintain the problem’s context.\n\nTest suite:\n- Test $1$ (scale invariance, exact $M$-orthogonality):\n  - $M_1 = \\operatorname{diag}(4,3,2,5)$.\n  - $\\hat{\\Phi}_1 = [7 e_1, -2 e_2, 0.5 e_3]$, where $e_i$ denotes the $i$-th standard basis vector in $\\mathbb{R}^4$.\n  - $\\epsilon_1 = 10^{-12}$.\n  - Expected boolean: true.\n- Test $2$ (small violation, strict tolerance):\n  - $M_1$ as above.\n  - $\\hat{\\Phi}_2 = [e_1 + 10^{-3} e_2, e_2, e_3]$ in $\\mathbb{R}^4$.\n  - $\\epsilon_2 = 10^{-6}$.\n  - Expected boolean: false.\n- Test $3$ (same violation, looser tolerance):\n  - $M_1$ as above.\n  - $\\hat{\\Phi}_3 = [e_1 + 10^{-3} e_2, e_2, e_3]$ in $\\mathbb{R}^4$.\n  - $\\epsilon_3 = 10^{-2}$.\n  - Expected boolean: true.\n- Test $4$ (single vector, vacuous orthogonality):\n  - $M_2 = \\begin{bmatrix}2 & 0 & 0 \\\\ 0 & 5 & 1 \\\\ 0 & 1 & 3\\end{bmatrix}$.\n  - $\\hat{\\Phi}_4 = [[1,2,3]^T]$ (a single column in $\\mathbb{R}^3$).\n  - $\\epsilon_4 = 10^{-12}$.\n  - Expected boolean: true.\n- Test $5$ (non-orthogonal pair under $M$, degenerate subspace example):\n  - $M_3 = \\operatorname{diag}(2,2)$.\n  - $\\hat{\\Phi}_5 = [[1,0]^T, [1,1]^T]$ in $\\mathbb{R}^2$.\n  - $\\epsilon_5 = 0.5$.\n  - Expected boolean: false.\n- Test $6$ (invalid input with a zero column):\n  - $M_4 = \\operatorname{diag}(1,1)$.\n  - $\\hat{\\Phi}_6 = [[1,0]^T, [0,0]^T]$ in $\\mathbb{R}^2$.\n  - $\\epsilon_6 = 10^{-12}$.\n  - Expected boolean: false.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of Tests $1$ through $6$, for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$, where each $r_i$ is a boolean value in the programming language’s canonical boolean literal format.", "solution": "The problem requires the design and implementation of a procedure to verify the numerical $M$-orthogonality of a set of vectors. This task is fundamental in the context of the finite element method, particularly in modal analysis where eigenvectors of the generalized eigenvalue problem $K \\phi = \\lambda M \\phi$ must be orthogonal with respect to the mass matrix $M$. We will first validate the problem statement and then construct the solution based on rigorous mathematical principles.\n\nThe problem is valid. It is scientifically grounded in established principles of numerical linear algebra, is well-posed with a clear and deterministic set of rules, and is expressed with objective, unambiguous language. It presents a standard, non-trivial verification task that requires careful implementation. All necessary data for the test cases are provided. We may therefore proceed with the solution.\n\nThe core of the problem lies in the definition of an inner product induced by a symmetric positive definite matrix $M \\in \\mathbb{R}^{n \\times n}$. For any two vectors $x, y \\in \\mathbb{R}^n$, the $M$-inner product is defined as:\n$$\n\\langle x, y \\rangle_M := x^T M y\n$$\nThe positive definiteness of $M$ ($M \\succ 0$) ensures that this definition satisfies all axioms of an inner product. Specifically, $\\langle x, x \\rangle_M = x^T M x > 0$ for all non-zero vectors $x$. The $M$-norm of a vector is then $\\|x\\|_M = \\sqrt{\\langle x, x \\rangle_M}$.\n\nA set of vectors $\\{\\hat{\\phi}_i\\}_{i=1}^m$ is said to be $M$-orthogonal if $\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M = 0$ for all $i \\neq j$. In numerical computation, results are subject to roundoff error, so we must verify this condition within a specified tolerance $\\epsilon$.\n\nThe specified verification procedure correctly formalizes this task. Let us analyze each step.\n\n1.  **Compute the Gram Matrix $G$**:\n    The procedure begins by computing the Gram matrix $G \\in \\mathbb{R}^{m \\times m}$ whose entries are the pairwise $M$-inner products of the columns of $\\hat{\\Phi} = [\\hat{\\phi}_1, \\dots, \\hat{\\phi}_m]$. The entry $g_{ij}$ is given by:\n    $$\n    g_{ij} = \\hat{\\phi}_i^T M \\hat{\\phi}_j = \\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M\n    $$\n    This can be expressed compactly in matrix notation as $G = \\hat{\\Phi}^T M \\hat{\\Phi}$. The diagonal entries are $g_{ii} = \\hat{\\phi}_i^T M \\hat{\\phi}_i = \\|\\hat{\\phi}_i\\|_M^2$, the squared $M$-norms of the vectors. The off-diagonal entries $g_{ij}$ ($i \\neq j$) are the quantities that should be zero for an $M$-orthogonal set.\n\n2.  **Validate Vector Norms**:\n    The next step is to examine the diagonal entries $d_i := g_{ii}$. The condition is to return `false` if any $d_i \\le 0$. This is a critical validity check. Since $M$ is positive definite, $\\hat{\\phi}_i^T M \\hat{\\phi}_i$ can only be zero if $\\hat{\\phi}_i$ is the zero vector, and it cannot be negative. The presence of a zero vector in a set intended to represent eigenvectors is an anomaly; such a vector cannot be part of an orthogonal basis. Therefore, any $\\hat{\\phi}_i$ for which $\\|\\hat{\\phi}_i\\|_M^2 \\le 0$ immediately invalidates the set for the purpose of orthogonality verification. This check also prevents division by zero in the subsequent normalization step.\n\n3.  **Normalize to Achieve Scale Invariance**:\n    Eigenvectors are defined only up to a non-zero scaling factor. A robust verification procedure must be invariant to such scaling. If we replace $\\hat{\\phi}_i$ with $\\alpha_i \\hat{\\phi}_i$ and $\\hat{\\phi}_j$ with $\\alpha_j \\hat{\\phi}_j$ (for $\\alpha_i, \\alpha_j \\neq 0$), the inner product becomes $g_{ij}' = \\alpha_i \\alpha_j g_{ij}$. The raw value of $g_{ij}$ is therefore not a reliable measure of orthogonality.\n    The procedure mandates the computation of normalized correlation coefficients:\n    $$\n    c_{ij} := \\frac{g_{ij}}{\\sqrt{d_i d_j}} = \\frac{\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M}{\\sqrt{\\|\\hat{\\phi}_i\\|_M^2 \\|\\hat{\\phi}_j\\|_M^2}} = \\frac{\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M}{\\|\\hat{\\phi}_i\\|_M \\|\\hat{\\phi}_j\\|_M}\n    $$\n    This quantity $c_{ij}$ is the cosine of the angle between vectors $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$ in the Hilbert space endowed with the $M$-inner product. By the Cauchy-Schwarz inequality for this inner product, we have $|\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M| \\le \\|\\hat{\\phi}_i\\|_M \\|\\hat{\\phi}_j\\|_M$, which implies $|c_{ij}| \\le 1$. The value of $c_{ij}$ is independent of the scaling of $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$, thus satisfying the scale-invariance requirement. For perfect orthogonality, $c_{ij}=0$.\n\n4.  **Quantify Overall Deviation from Orthogonality**:\n    To assess the entire set, we must find the maximum deviation from orthogonality among all distinct pairs of vectors. This is captured by the metric $\\eta$:\n    $$\n    \\eta := \\max_{i \\neq j} |c_{ij}|\n    $$\n    This metric represents the worst-case \"orthogonality error\" in the set. For the special case of a single vector ($m \\le 1$), there are no distinct pairs, so the condition of orthogonality is vacuously satisfied. The procedure correctly defines $\\eta := 0$ in this case.\n\n5.  **Final Verdict**:\n    The final step is to compare the worst-case deviation $\\eta$ against the specified tolerance $\\epsilon > 0$. The set $\\{\\hat{\\phi}_i\\}$ is considered numerically $M$-orthogonal if $\\eta \\le \\epsilon$. This provides a clear, quantitative, and robust criterion for the verification.\n\nThis procedure is a complete and mathematically sound algorithm for the stated task. We will now implement it in code and apply it to the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef is_m_orthogonal(M: np.ndarray, Phi: np.ndarray, epsilon: float) -> bool:\n    \"\"\"\n    Verifies if the columns of Phi are M-orthogonal within a given tolerance.\n\n    Args:\n        M (np.ndarray): A symmetric positive definite matrix (n x n).\n        Phi (np.ndarray): A matrix whose columns are the vectors to be checked (n x m).\n        epsilon (float): A positive tolerance for orthogonality.\n\n    Returns:\n        bool: True if the columns of Phi are M-orthogonal within tolerance, False otherwise.\n    \"\"\"\n    if not isinstance(M, np.ndarray) or not isinstance(Phi, np.ndarray):\n        raise TypeError(\"Inputs M and Phi must be numpy arrays.\")\n    \n    if M.ndim != 2 or Phi.ndim != 2:\n        raise ValueError(\"Inputs M and Phi must be 2D arrays.\")\n\n    n_m, n_m_check = M.shape\n    n_phi, m = Phi.shape\n\n    if n_m != n_m_check or n_m != n_phi:\n        raise ValueError(\"Matrix dimensions are not compatible for multiplication.\")\n\n    if epsilon <= 0:\n        raise ValueError(\"Tolerance epsilon must be positive.\")\n\n    # Per the specification, for m=1, eta is 0 if the vector is valid.\n    # We must still check for a zero vector.\n    if m <= 1:\n        if m == 0:\n            return True # An empty set of vectors is vacuously orthogonal.\n        \n        # Case m = 1\n        phi_1 = Phi[:, 0]\n        d1 = phi_1.T @ M @ phi_1\n        \n        # If d1 <= 0, the vector is a zero vector (since M is positive definite),\n        # which is invalid.\n        if d1 <= 0:\n            return False\n        \n        # For a single valid vector, orthogonality is vacuously true (eta=0).\n        # Since epsilon > 0, eta <= epsilon is always true.\n        return True\n\n    # Compute the Gram matrix: G = Phi^T * M * Phi\n    G = Phi.T @ M @ Phi\n\n    # Extract the diagonal entries d_i = g_ii\n    d = np.diagonal(G)\n\n    # Check for invalid vectors: d_i must be > 0.\n    # If any d_i <= 0, it implies a zero vector or M is not SPD.\n    if np.any(d <= 0):\n        return False\n\n    # Compute the normalized correlation coefficients c_ij = g_ij / sqrt(d_i * d_j)\n    # We do not need to compute the full matrix C. We only need the max of off-diagonals.\n    \n    # An efficient way to compute sqrt(d_i * d_j) matrix is via an outer product.\n    d_sqrt = np.sqrt(d)\n    denom_matrix = np.outer(d_sqrt, d_sqrt)\n    \n    # Avoid division by zero, though the check d <= 0 should already prevent this.\n    # Add a small machine epsilon for safety in floating point arithmetic.\n    denom_matrix[denom_matrix == 0] = np.finfo(float).eps\n    \n    C = np.abs(G / denom_matrix)\n    \n    # Set diagonal elements to 0 to only consider off-diagonal pairs (i != j)\n    np.fill_diagonal(C, 0)\n    \n    # Find the maximum absolute off-diagonal value\n    eta = np.max(C)\n    \n    return eta <= epsilon\n\ndef solve():\n    \"\"\"\n    Runs the deterministic test suite for the M-orthogonality verification procedure.\n    \"\"\"\n    # Define standard basis vectors for convenience\n    e1_4d = np.array([1., 0., 0., 0.])\n    e2_4d = np.array([0., 1., 0., 0.])\n    e3_4d = np.array([0., 0., 1., 0.])\n\n    # Test Case 1: Scale invariance, exact M-orthogonality\n    M1 = np.diag([4., 3., 2., 5.])\n    Phi1 = np.array([7. * e1_4d, -2. * e2_4d, 0.5 * e3_4d]).T\n    epsilon1 = 1e-12\n\n    # Test Case 2: Small violation, strict tolerance\n    Phi2 = np.array([e1_4d + 1e-3 * e2_4d, e2_4d, e3_4d]).T\n    epsilon2 = 1e-6\n\n    # Test Case 3: Same violation, looser tolerance\n    Phi3 = np.copy(Phi2)\n    epsilon3 = 1e-2\n\n    # Test Case 4: Single vector, vacuous orthogonality\n    M2 = np.array([[2., 0., 0.], [0., 5., 1.], [0., 1., 3.]])\n    Phi4 = np.array([[1., 2., 3.]]).T\n    epsilon4 = 1e-12\n\n    # Test Case 5: Non-orthogonal pair under M\n    M3 = np.diag([2., 2.])\n    Phi5 = np.array([[1., 0.], [1., 1.]]).T\n    epsilon5 = 0.5\n\n    # Test Case 6: Invalid input with a zero column\n    M4 = np.diag([1., 1.])\n    Phi6 = np.array([[1., 0.], [0., 0.]]).T\n    epsilon6 = 1e-12\n\n    test_cases = [\n        (M1, Phi1, epsilon1),\n        (M1, Phi2, epsilon2),\n        (M1, Phi3, epsilon3),\n        (M2, Phi4, epsilon4),\n        (M3, Phi5, epsilon5),\n        (M4, Phi6, epsilon6),\n    ]\n\n    results = []\n    for M, Phi, epsilon in test_cases:\n        # K is not needed for the verification procedure L-:)\n        result = is_m_orthogonal(M, Phi, epsilon)\n        results.append(result)\n        \n    # Format the output as a comma-separated list of lowercase boolean literals.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "2578532"}, {"introduction": "Having established how to verify modal orthogonality, we now explore a more subtle question: under what conditions does this property become numerically fragile? This advanced practice delves into the sensitivity of computed eigenvectors, particularly when a system has closely-spaced eigenvalues [@problem_id:2578501]. Using first-order perturbation theory, you will quantify how small modeling or numerical errors can lead to a significant loss of $M$-orthogonality, revealing a crucial relationship between eigenvalue separation and the stability of the corresponding eigenvectors.", "problem": "Consider a symmetric, positive-definite generalized eigenproblem in structural dynamics, given by the matrix pair $(\\mathbf{K}, \\mathbf{M})$ with $\\mathbf{K} \\in \\mathbb{R}^{2 \\times 2}$ and $\\mathbf{M} \\in \\mathbb{R}^{2 \\times 2}$, where modal orthogonality is taken with respect to the $\\mathbf{M}$-inner product. Let\n$$\n\\mathbf{M} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}, \n\\quad \n\\mathbf{K} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2(1+\\delta) \\end{pmatrix},\n$$\nwith a small parameter $\\delta = 10^{-6}$. The exact, $\\mathbf{M}$-orthonormal eigenpairs are $\\lambda_{1} = 1$ with $\\boldsymbol{\\phi}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\lambda_{2} = 1 + \\delta$ with $\\boldsymbol{\\phi}_{2} = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$, satisfying $\\boldsymbol{\\phi}_{i}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{j} = \\delta_{ij}$.\n\nSuppose two computed modes $\\boldsymbol{\\psi}_{1}$ and $\\boldsymbol{\\psi}_{2}$ are obtained by solving two slightly perturbed generalized eigenproblems independently (for the first and second mode, respectively), each mode subsequently normalized so that $\\boldsymbol{\\psi}_{i}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\psi}_{i} = 1$, but without any mutual orthogonalization step. Specifically, assume the perturbations are symmetric and of the form\n$$\n\\Delta \\mathbf{K}^{(1)} = \\varepsilon \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\Delta \\mathbf{M}^{(1)} = \\mathbf{0}, \n\\qquad\n\\Delta \\mathbf{K}^{(2)} = \\varepsilon \\begin{pmatrix} 0 & -2 \\\\ -2 & 0 \\end{pmatrix}, \\quad \\Delta \\mathbf{M}^{(2)} = \\mathbf{0},\n$$\nwith $\\varepsilon = 10^{-8}$. Assume the perturbations are sufficiently small to justify retaining only first-order terms in a perturbation analysis.\n\nStarting from the generalized eigenvalue equation $\\mathbf{K} \\boldsymbol{\\phi} = \\lambda \\mathbf{M} \\boldsymbol{\\phi}$, the symmetry of $\\mathbf{K}$ and $\\mathbf{M}$, and the definition of $\\mathbf{M}$-orthogonality and normalization, carry out a first-principles, first-order perturbation derivation for the contamination coefficients in the expansions\n$$\n\\boldsymbol{\\psi}_{1} = \\boldsymbol{\\phi}_{1} + a\\, \\boldsymbol{\\phi}_{2} + \\text{higher order terms}, \n\\qquad\n\\boldsymbol{\\psi}_{2} = \\boldsymbol{\\phi}_{2} + b\\, \\boldsymbol{\\phi}_{1} + \\text{higher order terms},\n$$\nand hence obtain the leading-order value of the $\\mathbf{M}$-inner product defect $\\boldsymbol{\\psi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\psi}_{2}$ in terms of $\\delta$, $\\varepsilon$, and the given data. Evaluate your final expression numerically for the given values of $\\delta$ and $\\varepsilon$. Round your answer to four significant figures. The final answer must be a single real number.", "solution": "We begin with the symmetric, positive-definite generalized eigenproblem\n$$\n\\mathbf{K} \\boldsymbol{\\phi} = \\lambda \\mathbf{M} \\boldsymbol{\\phi},\n$$\nwhere $\\mathbf{M}$ defines the inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\mathbf{M}} = \\mathbf{u}^{\\mathsf{T}} \\mathbf{M} \\mathbf{v}$. For symmetric definite pairs $(\\mathbf{K}, \\mathbf{M})$, eigenvectors associated with distinct eigenvalues are $\\mathbf{M}$-orthogonal. We are given\n$$\n\\mathbf{M} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}, \n\\quad \n\\mathbf{K} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2(1+\\delta) \\end{pmatrix},\n\\quad\n\\delta = 10^{-6},\n$$\nwith exact $\\mathbf{M}$-orthonormal eigenpairs\n$$\n\\lambda_{1} = 1, \\quad \\boldsymbol{\\phi}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}; \n\\qquad\n\\lambda_{2} = 1+\\delta, \\quad \\boldsymbol{\\phi}_{2} = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix},\n$$\nsatisfying $\\boldsymbol{\\phi}_{i}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{j} = \\delta_{ij}$.\n\nTwo computed modes are obtained by solving two slightly perturbed eigenproblems independently and then normalizing each mode to unit $\\mathbf{M}$-norm, without enforcing mutual orthogonality:\n- For the first mode, the perturbed problem is $(\\mathbf{K} + \\Delta \\mathbf{K}^{(1)}, \\mathbf{M} + \\Delta \\mathbf{M}^{(1)})$ with\n$$\n\\Delta \\mathbf{K}^{(1)} = \\varepsilon \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\Delta \\mathbf{M}^{(1)} = \\mathbf{0}.\n$$\n- For the second mode, the perturbed problem is $(\\mathbf{K} + \\Delta \\mathbf{K}^{(2)}, \\mathbf{M} + \\Delta \\mathbf{M}^{(2)})$ with\n$$\n\\Delta \\mathbf{K}^{(2)} = \\varepsilon \\begin{pmatrix} 0 & -2 \\\\ -2 & 0 \\end{pmatrix}, \\quad \\Delta \\mathbf{M}^{(2)} = \\mathbf{0}.\n$$\nWe take $\\varepsilon = 10^{-8}$ and assume first-order perturbation suffices.\n\nWe expand the computed modes to first order in the unperturbed $\\mathbf{M}$-orthonormal basis $\\{\\boldsymbol{\\phi}_{1}, \\boldsymbol{\\phi}_{2}\\}$ as\n$$\n\\boldsymbol{\\psi}_{1} = \\boldsymbol{\\phi}_{1} + a\\, \\boldsymbol{\\phi}_{2} + \\mathcal{O}(\\varepsilon^{2}),\n\\qquad\n\\boldsymbol{\\psi}_{2} = \\boldsymbol{\\phi}_{2} + b\\, \\boldsymbol{\\phi}_{1} + \\mathcal{O}(\\varepsilon^{2}),\n$$\nwhere $a$ and $b$ are first-order coefficients to be determined. Because $\\boldsymbol{\\phi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{2} = 0$ and $\\boldsymbol{\\phi}_{i}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{i} = 1$, the $\\mathbf{M}$-norms of $\\boldsymbol{\\psi}_{1}$ and $\\boldsymbol{\\psi}_{2}$ remain unity up to $\\mathcal{O}(\\varepsilon^{2})$, so the subsequent normalization does not affect $a$ and $b$ at first order.\n\nTo derive $a$, insert $\\boldsymbol{\\psi}_{1}$ into the first perturbed eigenproblem:\n$$\n\\left(\\mathbf{K} + \\Delta \\mathbf{K}^{(1)}\\right)\\left(\\boldsymbol{\\phi}_{1} + a \\boldsymbol{\\phi}_{2}\\right) \n= \\left(\\lambda_{1} + \\delta \\lambda_{1}\\right) \\mathbf{M} \\left(\\boldsymbol{\\phi}_{1} + a \\boldsymbol{\\phi}_{2}\\right),\n$$\nand retain only first-order terms in $\\varepsilon$ and $a$:\n$$\n\\mathbf{K}\\boldsymbol{\\phi}_{1} + a \\mathbf{K}\\boldsymbol{\\phi}_{2} + \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1}\n\\approx \\lambda_{1} \\mathbf{M}\\boldsymbol{\\phi}_{1} + \\lambda_{1} a \\mathbf{M}\\boldsymbol{\\phi}_{2} + \\delta \\lambda_{1} \\mathbf{M}\\boldsymbol{\\phi}_{1}.\n$$\nUsing the unperturbed relations $\\mathbf{K}\\boldsymbol{\\phi}_{i} = \\lambda_{i}\\mathbf{M}\\boldsymbol{\\phi}_{i}$, subtract $\\mathbf{K}\\boldsymbol{\\phi}_{1} = \\lambda_{1}\\mathbf{M}\\boldsymbol{\\phi}_{1}$ from both sides to obtain\n$$\na\\, \\mathbf{K}\\boldsymbol{\\phi}_{2} + \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1}\n\\approx \\lambda_{1} a\\, \\mathbf{M}\\boldsymbol{\\phi}_{2} + \\delta \\lambda_{1}\\, \\mathbf{M}\\boldsymbol{\\phi}_{1}.\n$$\nPremultiplying by $\\boldsymbol{\\phi}_{2}^{\\mathsf{T}}$ and using symmetry along with $\\boldsymbol{\\phi}_{2}^{\\mathsf{T}}\\mathbf{K}\\boldsymbol{\\phi}_{2} = \\lambda_{2} \\boldsymbol{\\phi}_{2}^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_{2} = \\lambda_{2}$, $\\boldsymbol{\\phi}_{2}^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_{2} = 1$, and $\\boldsymbol{\\phi}_{2}^{\\mathsf{T}}\\mathbf{M}\\boldsymbol{\\phi}_{1} = 0$, we get\n$$\na \\lambda_{2} + \\boldsymbol{\\phi}_{2}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1} \\approx \\lambda_{1} a.\n$$\nHence,\n$$\na (\\lambda_{2} - \\lambda_{1}) \\approx - \\boldsymbol{\\phi}_{2}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1},\n\\qquad\\Rightarrow\\qquad\na \\approx - \\frac{\\boldsymbol{\\phi}_{2}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1}}{\\lambda_{2} - \\lambda_{1}}.\n$$\n\nAnalogously, for the second mode,\n$$\n\\left(\\mathbf{K} + \\Delta \\mathbf{K}^{(2)}\\right)\\left(\\boldsymbol{\\phi}_{2} + b \\boldsymbol{\\phi}_{1}\\right) \n= \\left(\\lambda_{2} + \\delta \\lambda_{2}\\right) \\mathbf{M} \\left(\\boldsymbol{\\phi}_{2} + b \\boldsymbol{\\phi}_{1}\\right),\n$$\nwhich to first order yields\n$$\nb (\\lambda_{1} - \\lambda_{2}) \\approx - \\boldsymbol{\\phi}_{1}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(2)} \\boldsymbol{\\phi}_{2},\n\\qquad\\Rightarrow\\qquad\nb \\approx \\frac{\\boldsymbol{\\phi}_{1}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(2)} \\boldsymbol{\\phi}_{2}}{\\lambda_{2} - \\lambda_{1}}.\n$$\n\nThe $\\mathbf{M}$-orthogonality defect between the computed modes is, to leading order,\n$$\n\\boldsymbol{\\psi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\psi}_{2} \n= \\left(\\boldsymbol{\\phi}_{1} + a \\boldsymbol{\\phi}_{2}\\right)^{\\mathsf{T}} \\mathbf{M} \\left(\\boldsymbol{\\phi}_{2} + b \\boldsymbol{\\phi}_{1}\\right) \n= a\\, \\boldsymbol{\\phi}_{2}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{2} + b\\, \\boldsymbol{\\phi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\phi}_{1} + \\mathcal{O}(\\varepsilon^{2})\n= a + b + \\mathcal{O}(\\varepsilon^{2}).\n$$\n\nWe now evaluate the required numerators. With\n$$\n\\Delta \\mathbf{K}^{(1)} = \\varepsilon \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad\n\\boldsymbol{\\phi}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\boldsymbol{\\phi}_{2} = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix},\n$$\nwe find\n$$\n\\boldsymbol{\\phi}_{2}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(1)} \\boldsymbol{\\phi}_{1} \n= \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \n\\varepsilon \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n\\varepsilon \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n= \\frac{\\varepsilon}{\\sqrt{2}}.\n$$\nSimilarly, with\n$$\n\\Delta \\mathbf{K}^{(2)} = \\varepsilon \\begin{pmatrix} 0 & -2 \\\\ -2 & 0 \\end{pmatrix},\n$$\nwe obtain\n$$\n\\boldsymbol{\\phi}_{1}^{\\mathsf{T}} \\Delta \\mathbf{K}^{(2)} \\boldsymbol{\\phi}_{2} \n= \\begin{pmatrix} 1 & 0 \\end{pmatrix}\n\\varepsilon \\begin{pmatrix} 0 & -2 \\\\ -2 & 0 \\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n= \\begin{pmatrix} 1 & 0 \\end{pmatrix}\n\\varepsilon \\begin{pmatrix} -\\frac{2}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n= - \\frac{2 \\varepsilon}{\\sqrt{2}}.\n$$\nThe eigenvalue gap is\n$$\n\\lambda_{2} - \\lambda_{1} = (1 + \\delta) - 1 = \\delta.\n$$\nTherefore,\n$$\na \\approx - \\frac{\\varepsilon/\\sqrt{2}}{\\delta} = - \\frac{\\varepsilon}{\\sqrt{2}\\delta}, \n\\qquad\nb \\approx \\frac{-2\\varepsilon/\\sqrt{2}}{\\delta} = - \\frac{2\\varepsilon}{\\sqrt{2}\\delta},\n$$\nand the leading-order $\\mathbf{M}$-orthogonality defect is\n$$\n\\boldsymbol{\\psi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\psi}_{2} \\approx a + b = - \\frac{3 \\varepsilon}{\\sqrt{2}\\delta}.\n$$\n\nFinally, substitute $\\varepsilon = 10^{-8}$ and $\\delta = 10^{-6}$:\n$$\n\\boldsymbol{\\psi}_{1}^{\\mathsf{T}} \\mathbf{M} \\boldsymbol{\\psi}_{2} \\approx - \\frac{3 \\times 10^{-8}}{\\sqrt{2} \\times 10^{-6}} = - \\frac{3}{\\sqrt{2}} \\times 10^{-2}.\n$$\nNumerically,\n$$\n- \\frac{3}{\\sqrt{2}} \\times 10^{-2} \\approx - 0.021213203\\ldots\n$$\nRounded to four significant figures, the requested value is $-0.02121$.\n\nThis calculation exhibits the sensitivity of $\\mathbf{M}$-orthogonality to a small spectral gap $\\delta$, with the defect scaling like $\\mathcal{O}(\\varepsilon/\\delta)$ when modes are computed independently, thereby quantifying how small gaps can lead to numerically non-orthogonal computed modes.", "answer": "$$\\boxed{-0.02121}$$", "id": "2578501"}]}