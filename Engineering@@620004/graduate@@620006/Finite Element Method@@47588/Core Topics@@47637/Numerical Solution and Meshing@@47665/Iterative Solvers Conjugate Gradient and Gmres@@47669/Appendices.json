{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is exceptionally efficient for solving linear systems, but its applicability is specifically tied to matrices that are Symmetric Positive-Definite (SPD). This property is characteristic of stiffness matrices arising from many physical systems modeled with the Finite Element Method. This foundational exercise [@problem_id:2570982] helps you build intuition for this crucial requirement by having you verify the SPD property of a matrix and compute the associated \"energy norm,\" $\\|x\\|_A$, which is the very quantity the CG method is designed to minimize.", "problem": "In the Finite Element Method (FEM), stiffness matrices associated with coercive bilinear forms are symmetric positive-definite (SPD), which induces an energy inner product suitable for the Conjugate Gradient method. Consider the matrix\n$$\nA=\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}\n$$\nand the vector\n$$\nx=\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}.\n$$\nUsing only fundamental definitions, do the following:\n1. With the energy inner product induced by $A$, compute the energy norm $\\|x\\|_{A}$.\n2. Verify from first principles that $A$ is symmetric positive-definite by checking symmetry and showing that $z^{\\mathsf{T}}Az>0$ for every nonzero $z\\in\\mathbb{R}^{2}$.\n\nExpress the norm $\\|x\\|_{A}$ exactly; no rounding or approximation is required.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of linear algebra and numerical analysis, it is well-posed, and all provided data is complete and consistent. We will proceed with the solution.\n\nThe problem asks for two tasks to be performed using the given matrix $A$ and vector $x$:\n$$\nA=\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}, \\quad x=\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\n$$\nThe first task is to compute the energy norm $\\|x\\|_{A}$. The energy inner product induced by a symmetric positive-definite matrix $A$ is defined for any vectors $u, v \\in \\mathbb{R}^n$ as:\n$$\n\\langle u, v \\rangle_{A} = u^{\\mathsf{T}}Av\n$$\nThe corresponding energy norm is then defined as:\n$$\n\\|u\\|_{A} = \\sqrt{\\langle u, u \\rangle_{A}} = \\sqrt{u^{\\mathsf{T}}Au}\n$$\n\nTo compute $\\|x\\|_{A}$, we first calculate the product $Ax$:\n$$\nAx = \\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix} \\begin{bmatrix}1 \\\\ 2\\end{bmatrix} = \\begin{bmatrix}2 \\cdot 1 + (-1) \\cdot 2 \\\\ -1 \\cdot 1 + 2 \\cdot 2\\end{bmatrix} = \\begin{bmatrix}2-2 \\\\ -1+4\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 3\\end{bmatrix}\n$$\nNext, we compute the quadratic form $x^{\\mathsf{T}}Ax$, which is the squared energy norm $\\|x\\|_{A}^2$:\n$$\n\\|x\\|_{A}^2 = x^{\\mathsf{T}}(Ax) = \\begin{bmatrix}1 & 2\\end{bmatrix} \\begin{bmatrix}0 \\\\ 3\\end{bmatrix} = 1 \\cdot 0 + 2 \\cdot 3 = 6\n$$\nThe energy norm is the square root of this value:\n$$\n\\|x\\|_{A} = \\sqrt{6}\n$$\n\nThe second task is to verify from first principles that the matrix $A$ is symmetric positive-definite (SPD). A matrix is SPD if it is both symmetric and positive-definite.\n\nFirst, we check for symmetry. A matrix $A$ is symmetric if $A = A^{\\mathsf{T}}$.\n$$\nA^{\\mathsf{T}} = \\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}^{\\mathsf{T}} = \\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}\n$$\nSince $A = A^{\\mathsf{T}}$, the matrix $A$ is symmetric.\n\nSecond, we check for positive-definiteness. A matrix $A$ is positive-definite if the quadratic form $z^{\\mathsf{T}}Az$ is strictly positive for any non-zero vector $z \\in \\mathbb{R}^2$. Let $z=\\begin{bmatrix}z_1 \\\\ z_2\\end{bmatrix}$ be an arbitrary non-zero vector, meaning at least one of $z_1, z_2$ is not zero. We compute $z^{\\mathsf{T}}Az$:\n$$\nz^{\\mathsf{T}}Az = \\begin{bmatrix}z_1 & z_2\\end{bmatrix} \\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix} \\begin{bmatrix}z_1 \\\\ z_2\\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix}z_1 & z_2\\end{bmatrix} \\begin{bmatrix}2z_1 - z_2 \\\\ -z_1 + 2z_2\\end{bmatrix}\n$$\n$$\n= z_1(2z_1 - z_2) + z_2(-z_1 + 2z_2)\n$$\n$$\n= 2z_1^2 - z_1z_2 - z_2z_1 + 2z_2^2 = 2z_1^2 - 2z_1z_2 + 2z_2^2\n$$\nTo demonstrate that this expression is strictly positive for any non-zero $z$, we can complete the square:\n$$\n2z_1^2 - 2z_1z_2 + 2z_2^2 = (z_1^2 - 2z_1z_2 + z_2^2) + z_1^2 + z_2^2 = (z_1 - z_2)^2 + z_1^2 + z_2^2\n$$\nThis expression is a sum of three squared real terms: $(z_1 - z_2)^2$, $z_1^2$, and $z_2^2$. Each term is non-negative. The sum is equal to zero if and only if all individual terms are zero:\n$$\nz_1^2=0 \\implies z_1=0\n$$\n$$\nz_2^2=0 \\implies z_2=0\n$$\nIf $z_1=0$ and $z_2=0$, then $(z_1-z_2)^2=0$ is also satisfied. Thus, $z^{\\mathsf{T}}Az = 0$ if and only if $z = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$. For any non-zero vector $z$, at least one of $z_1$ or $z_2$ is non-zero, which implies that at least one of the terms $z_1^2$ or $z_2^2$ will be strictly positive. Therefore, the sum is strictly positive.\n$$\nz^{\\mathsf{T}}Az > 0 \\quad \\text{for all } z \\in \\mathbb{R}^2, z \\neq \\mathbf{0}\n$$\nThis proves that the matrix $A$ is positive-definite. Since $A$ is both symmetric and positive-definite, it is an SPD matrix. The verification is complete.", "answer": "$$\n\\boxed{\\sqrt{6}}\n$$", "id": "2570982"}, {"introduction": "With the foundational concepts of SPD matrices and the energy norm established, we can now peek under the hood of the Conjugate Gradient (CG) algorithm itself. This practice [@problem_id:2570866] demystifies the iterative process by guiding you step-by-step through its application to a small system. By manually deriving the expressions for the step length $\\alpha_k$ and recurrence coefficient $\\beta_k$ and then executing the calculations, you will gain a deep, practical understanding of how residuals and $A$-conjugate search directions work together to efficiently find the solution.", "problem": "Consider the symmetric positive definite (SPD) linear system arising from a two-degree-of-freedom finite element model, with stiffness matrix $A=\\begin{bmatrix}2&0\\\\0&1\\end{bmatrix}$, right-hand side $b=\\begin{bmatrix}2\\\\1\\end{bmatrix}$, and initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Apply the Conjugate Gradient (CG) method starting from $x_{0}$ for $2$ iterations. Begin from first principles appropriate for SPD systems: the quadratic energy $J(x)=\\tfrac{1}{2}x^{\\mathsf{T}}A x-b^{\\mathsf{T}}x$, the residual $r_{k}=b-Ax_{k}$, the search subspace $x_{k+1}\\in x_{k}+\\operatorname{span}\\{p_{k}\\}$, the $A$-conjugacy of the search directions $p_{i}^{\\mathsf{T}}A p_{j}=0$ for $i\\neq j$, and the orthogonality of residuals to the current search direction induced by minimization of $J$ along $p_{k}$. Using only these foundational facts, first derive the expressions for the step length $\\alpha_{k}$ and the recurrence coefficient $\\beta_{k}$ in terms of $r_{k}$, $p_{k}$, and $A$, and then carry out the computations to obtain, explicitly and in exact rational form, the quantities $\\alpha_{0}$, $\\beta_{0}$, $\\alpha_{1}$, $\\beta_{1}$, the iterates $x_{1}$ and $x_{2}$, the residuals $r_{0}$, $r_{1}$, $r_{2}$, and the search directions $p_{0}$, $p_{1}$, $p_{2}$. \n\nReport your final result as a single row matrix containing, in this exact order,\n$$\\bigl[\\alpha_{0},\\,\\beta_{0},\\,\\alpha_{1},\\,\\beta_{1},\\,x_{0,1},\\,x_{0,2},\\,x_{1,1},\\,x_{1,2},\\,x_{2,1},\\,x_{2,2},\\,r_{0,1},\\,r_{0,2},\\,r_{1,1},\\,r_{1,2},\\,r_{2,1},\\,r_{2,2},\\,p_{0,1},\\,p_{0,2},\\,p_{1,1},\\,p_{1,2},\\,p_{2,1},\\,p_{2,2}\\bigr].$$\n\nExpress all quantities exactly; no rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a canonical exercise in the application of the Conjugate Gradient (CG) method. Therefore, it is deemed valid and a full solution will be provided.\n\nThe problem requires the derivation of the Conjugate Gradient algorithm's parameters from first principles, followed by the execution of two iterations for the given linear system.\n\n**1. Derivation of CG Algorithm Parameters**\n\nLet the linear system be $Ax = b$, where $A$ is an $n \\times n$ symmetric positive definite (SPD) matrix. The solution $x$ uniquely minimizes the quadratic energy functional $J(x) = \\frac{1}{2}x^{\\mathsf{T}}A x - b^{\\mathsf{T}}x$. The CG method is an iterative process that generates a sequence of approximations $x_k$ that converge to the true solution.\n\nThe method is defined by the following update rules:\n$x_{k+1} = x_k + \\alpha_k p_k$\n$r_{k+1} = r_k - \\alpha_k A p_k$\n$p_{k+1} = r_{k+1} + \\beta_k p_k$\nwhere $r_k = b - Ax_k$ is the residual at step $k$, $p_k$ is the search direction, $\\alpha_k$ is the step length, and $\\beta_k$ is a coefficient for combining the new residual with the previous search direction.\n\n**Derivation of the step length $\\alpha_k$:**\nThe step length $\\alpha_k$ is chosen to minimize the energy functional $J(x_{k+1})$ along the search direction $p_k$. That is, we minimize $J(x_k + \\alpha_k p_k)$ with respect to $\\alpha_k$.\n$$J(x_k + \\alpha_k p_k) = \\frac{1}{2}(x_k + \\alpha_k p_k)^{\\mathsf{T}}A(x_k + \\alpha_k p_k) - b^{\\mathsf{T}}(x_k + \\alpha_k p_k)$$\nTo find the minimum, we set the derivative with respect to $\\alpha_k$ to zero:\n$$\\frac{d}{d\\alpha_k} J(x_k + \\alpha_k p_k) = 0$$\nUsing the product rule for matrix calculus and the symmetry of $A$, we get:\n$$p_k^{\\mathsf{T}}A(x_k + \\alpha_k p_k) - p_k^{\\mathsf{T}}b = 0$$\n$$p_k^{\\mathsf{T}}Ax_k + \\alpha_k p_k^{\\mathsf{T}}Ap_k - p_k^{\\mathsf{T}}b = 0$$\n$$\\alpha_k (p_k^{\\mathsf{T}}Ap_k) = p_k^{\\mathsf{T}}(b - Ax_k)$$\nUsing the definition of the residual, $r_k = b - Ax_k$, we obtain:\n$$\\alpha_k = \\frac{p_k^{\\mathsf{T}}r_k}{p_k^{\\mathsf{T}}Ap_k}$$\nThis is a general expression. For the CG algorithm, it can be shown that $p_k^{\\mathsf{T}}r_k = r_k^{\\mathsf{T}}r_k$. Starting with $p_0 = r_0$, this is trivial for $k=0$. For $k>0$, $p_k = r_k + \\beta_{k-1}p_{k-1}$. Thus, $p_k^{\\mathsf{T}}r_k = (r_k + \\beta_{k-1}p_{k-1})^{\\mathsf{T}}r_k = r_k^{\\mathsf{T}}r_k + \\beta_{k-1}p_{k-1}^{\\mathsf{T}}r_k$. The optimality condition which yields $\\alpha_{k-1}$ is equivalent to $r_k^{\\mathsf{T}}p_{k-1}=0$. Therefore, $p_{k-1}^{\\mathsf{T}}r_k=0$, which simplifies the expression to $p_k^{\\mathsf{T}}r_k = r_k^{\\mathsf{T}}r_k$.\nThe standard formula for the step length is therefore:\n$$\\alpha_k = \\frac{r_k^{\\mathsf{T}}r_k}{p_k^{\\mathsf{T}}Ap_k}$$\n\n**Derivation of the recurrence coefficient $\\beta_k$:**\nThe search directions must be $A$-conjugate, i.e., $p_{k+1}^{\\mathsf{T}}A p_j = 0$ for all $j \\le k$. We enforce this for $j=k$:\n$$p_{k+1}^{\\mathsf{T}}A p_k = 0$$\nSubstitute the expression for the new search direction, $p_{k+1} = r_{k+1} + \\beta_k p_k$:\n$$(r_{k+1} + \\beta_k p_k)^{\\mathsf{T}}A p_k = 0$$\n$$r_{k+1}^{\\mathsf{T}}A p_k + \\beta_k p_k^{\\mathsf{T}}A p_k = 0$$\nSolving for $\\beta_k$:\n$$\\beta_k = - \\frac{r_{k+1}^{\\mathsf{T}}A p_k}{p_k^{\\mathsf{T}}A p_k}$$\nThis formula can be simplified. From the residual update formula $r_{k+1} = r_k - \\alpha_k A p_k$, we can write $A p_k = \\frac{1}{\\alpha_k}(r_k - r_{k+1})$. Substituting this into the numerator:\n$$\\beta_k = - \\frac{r_{k+1}^{\\mathsf{T}}\\left(\\frac{1}{\\alpha_k}(r_k - r_{k+1})\\right)}{p_k^{\\mathsf{T}}A p_k} = -\\frac{1}{\\alpha_k} \\frac{r_{k+1}^{\\mathsf{T}}r_k - r_{k+1}^{\\mathsf{T}}r_{k+1}}{p_k^{\\mathsf{T}}A p_k}$$\nA fundamental property of CG is the orthogonality of the residuals, $r_{k+1}^{\\mathsf{T}}r_k = 0$. This can be shown by induction using the $A$-conjugacy of search directions. Thus, the expression becomes:\n$$\\beta_k = \\frac{1}{\\alpha_k} \\frac{r_{k+1}^{\\mathsf{T}}r_{k+1}}{p_k^{\\mathsf{T}}A p_k}$$\nNow, substitute the expression for $\\alpha_k = \\frac{r_k^{\\mathsf{T}}r_k}{p_k^{\\mathsf{T}}Ap_k}$, which implies $\\alpha_k p_k^{\\mathsf{T}}Ap_k = r_k^{\\mathsf{T}}r_k$:\n$$\\beta_k = \\frac{r_{k+1}^{\\mathsf{T}}r_{k+1}}{r_k^{\\mathsf{T}}r_k}$$\n\n**2. Application of CG Algorithm**\n\nThe problem provides:\n$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, and initial guess $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\n**Iteration $k=0$:**\nInitial residual:\n$r_0 = b - A x_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\nInitial search direction:\n$p_0 = r_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\nScalar product of the residual:\n$r_0^{\\mathsf{T}}r_0 = 2^2 + 1^2 = 5$.\nCompute $A p_0$:\n$A p_0 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}$.\nCompute $p_0^{\\mathsf{T}}A p_0$:\n$p_0^{\\mathsf{T}}A p_0 = \\begin{bmatrix} 2 & 1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = 8 + 1 = 9$.\nStep length $\\alpha_0$:\n$\\alpha_0 = \\frac{r_0^{\\mathsf{T}}r_0}{p_0^{\\mathsf{T}}A p_0} = \\frac{5}{9}$.\nUpdate iterate $x_1$:\n$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{5}{9}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix}$.\nUpdate residual $r_1$:\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\frac{5}{9}\\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 18/9 - 20/9 \\\\ 9/9 - 5/9 \\end{bmatrix} = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$.\nScalar product of new residual:\n$r_1^{\\mathsf{T}}r_1 = (-\\frac{2}{9})^2 + (\\frac{4}{9})^2 = \\frac{4}{81} + \\frac{16}{81} = \\frac{20}{81}$.\nRecurrence coefficient $\\beta_0$:\n$\\beta_0 = \\frac{r_1^{\\mathsf{T}}r_1}{r_0^{\\mathsf{T}}r_0} = \\frac{20/81}{5} = \\frac{4}{81}$.\nUpdate search direction $p_1$:\n$p_1 = r_1 + \\beta_0 p_0 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} + \\frac{4}{81}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -18/81 \\\\ 36/81 \\end{bmatrix} + \\begin{bmatrix} 8/81 \\\\ 4/81 \\end{bmatrix} = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$.\n\n**Iteration $k=1$:**\nWe have $r_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$, $p_1 = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$, and $r_1^{\\mathsf{T}}r_1 = \\frac{20}{81}$.\nCompute $A p_1$:\n$Ap_1 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix}$.\nCompute $p_1^{\\mathsf{T}}A p_1$:\n$p_1^{\\mathsf{T}}A p_1 = \\begin{bmatrix} -10/81 & 40/81 \\end{bmatrix} \\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix} = \\frac{200}{81^2} + \\frac{1600}{81^2} = \\frac{1800}{6561}$.\nStep length $\\alpha_1$:\n$\\alpha_1 = \\frac{r_1^{\\mathsf{T}}r_1}{p_1^{\\mathsf{T}}A p_1} = \\frac{20/81}{1800/6561} = \\frac{20}{81} \\cdot \\frac{6561}{1800} = \\frac{20 \\cdot 81}{1800} = \\frac{1620}{1800} = \\frac{9}{10}$.\nUpdate iterate $x_2$:\n$x_2 = x_1 + \\alpha_1 p_1 = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix} + \\frac{9}{10}\\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix} + \\begin{bmatrix} -1/9 \\\\ 4/9 \\end{bmatrix} = \\begin{bmatrix} 9/9 \\\\ 9/9 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nUpdate residual $r_2$:\n$r_2 = r_1 - \\alpha_1 A p_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} - \\frac{9}{10}\\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} - \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nScalar product of new residual:\n$r_2^{\\mathsf{T}}r_2 = 0^2 + 0^2 = 0$.\nRecurrence coefficient $\\beta_1$:\n$\\beta_1 = \\frac{r_2^{\\mathsf{T}}r_2}{r_1^{\\mathsf{T}}r_1} = \\frac{0}{20/81} = 0$.\nUpdate search direction $p_2$:\n$p_2 = r_2 + \\beta_1 p_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + 0 \\cdot \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThe algorithm has converged to the exact solution $x=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ in $2$ steps, as expected for a $2 \\times 2$ system.\n\n**Summary of Quantities:**\n$\\alpha_0 = 5/9$\n$\\beta_0 = 4/81$\n$\\alpha_1 = 9/10$\n$\\beta_1 = 0$\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n$x_1 = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix}$\n$x_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n$r_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n$r_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$\n$r_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n$p_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n$p_1 = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$\n$p_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\nThese will be assembled into the final answer matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{9} & \\frac{4}{81} & \\frac{9}{10} & 0 & 0 & 0 & \\frac{10}{9} & \\frac{5}{9} & 1 & 1 & 2 & 1 & -\\frac{2}{9} & \\frac{4}{9} & 0 & 0 & 2 & 1 & -\\frac{10}{81} & \\frac{40}{81} & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "2570866"}, {"introduction": "While the Conjugate Gradient method is powerful, its reliance on SPD matrices is a significant constraint. For the many problems in science and engineering that result in general, non-symmetric linear systems, we must turn to more broadly applicable solvers like the Generalized Minimal Residual (GMRES) method. This exercise [@problem_id:2570955] provides a hands-on introduction to the core mechanism of GMRES: using the Arnoldi process to build an optimal subspace and then solving a small least-squares problem to find the best possible residual reduction in that step.", "problem": "Consider the linear system $A x = b$ arising from a finite element discretization, with\n$$\nA=\\begin{bmatrix}2 & 1 \\\\ 0 & 3\\end{bmatrix}, \\quad b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}, \\quad x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}.\n$$\nLet $\\|\\cdot\\|$ denote the Euclidean norm. You will perform one step of the Arnoldi process and compute the residual norm produced by one step of the Generalized Minimal Residual (GMRES) method.\n\nStarting from the initial residual $r_{0}=b-A x_{0}$, define the first Arnoldi vector $v_{1}=r_{0}/\\|r_{0}\\|$. Perform one Arnoldi step to obtain scalars $h_{11}$ and $h_{21}$ such that\n$$\nA v_{1}=v_{1} h_{11}+v_{2} h_{21},\n$$\nwhere $v_{2}$ is the next Arnoldi vector if $h_{21}\\neq 0$. Use this to form the $(1+1)\\times 1$ upper Hessenberg matrix $\\hat{H}_{1}=\\begin{bmatrix}h_{11} \\\\ h_{21}\\end{bmatrix}$ and the scalar $\\beta=\\|r_{0}\\|$.\n\nUsing only the defining properties of Krylov subspaces, the Arnoldi relation, and the orthonormality of Arnoldi vectors, derive the $1$-step GMRES least-squares problem and compute the resulting GMRES residual norm after this single step. Provide the exact value of the residual norm as your final answer (no rounding required).", "solution": "The problem requires the computation of the residual norm after one step of the Generalized Minimal Residual (GMRES) method for a given linear system $A x = b$. We must first validate the problem statement, which is found to be well-posed and scientifically sound, before proceeding with the solution.\n\nThe problem is defined by the matrix $A$, the vector $b$, and the initial guess $x_{0}$:\n$$\nA=\\begin{bmatrix}2 & 1 \\\\ 0 & 3\\end{bmatrix}, \\quad b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}, \\quad x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n$$\nThe GMRES algorithm iteratively finds an approximate solution in a Krylov subspace. The process begins with the computation of the initial residual $r_{0}$.\n\nFirst, we compute the initial residual, $r_{0}$:\n$$\nr_{0} = b - A x_{0} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} - \\begin{bmatrix}2 & 1 \\\\ 0 & 3\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\n$$\nThe norm of the initial residual, denoted by $\\beta$, is computed using the Euclidean norm:\n$$\n\\beta = \\|r_{0}\\| = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}\n$$\nThe first step of the Arnoldi process generates an orthonormal basis for the Krylov subspace $\\mathcal{K}_{1}(A, r_{0})$. The first basis vector, $v_{1}$, is the normalized initial residual:\n$$\nv_{1} = \\frac{r_{0}}{\\|r_{0}\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\n$$\nNext, we perform one step of the Arnoldi iteration to find the entries of the Hessenberg matrix $\\hat{H}_{1}$. We compute the product of $A$ and $v_{1}$:\n$$\nw_{1} = A v_{1} = \\begin{bmatrix}2 & 1 \\\\ 0 & 3\\end{bmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}2(1)+1(1) \\\\ 0(1)+3(1)\\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix}\n$$\nThe coefficient $h_{11}$ is the projection of $w_{1}$ onto $v_{1}$. Since we are working with real vectors, $h_{11} = v_{1}^{T} w_{1}$:\n$$\nh_{11} = \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 & 1\\end{bmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} \\right) = \\frac{1}{2} (1 \\cdot 3 + 1 \\cdot 3) = \\frac{6}{2} = 3\n$$\nThe next Arnoldi vector $v_{2}$ is obtained by orthogonalizing $w_{1}$ against $v_{1}$. Let $\\tilde{w}_{1} = w_{1} - h_{11} v_{1}$:\n$$\n\\tilde{w}_{1} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} - 3 \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\left( \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} - \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} \\right) = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n$$\nThe coefficient $h_{21}$ is the norm of this vector:\n$$\nh_{21} = \\|\\tilde{w}_{1}\\| = \\sqrt{0^2 + 0^2} = 0\n$$\nThis situation, where $h_{m+1, m} = 0$, is known as a \"lucky breakdown.\" It implies that the Krylov subspace $\\mathcal{K}_{m}(A, r_{0})$ is an invariant subspace of $A$. In this case, for $m=1$, we have $A v_{1} = h_{11} v_{1}$, meaning $v_{1}$ (and thus $r_{0}$) is an eigenvector of $A$ with eigenvalue $h_{11}=3$.\n\nThe GMRES method finds an approximate solution $x_{m}$ of the form $x_{m} = x_{0} + z_{m}$, where $z_{m}$ is in the Krylov subspace $\\mathcal{K}_{m}(A, r_{0}) = \\text{span}\\{v_{1}, v_{2}, \\ldots, v_{m}\\}$. The vector $z_{m}$ is chosen to minimize the norm of the residual $r_{m} = \\|b - A x_{m}\\|$.\nFor the first step ($m=1$), we seek $x_{1} = x_{0} + y_{1}v_{1}$ that minimizes $\\|r_{1}\\|$. The residual is:\n$$\nr_{1} = b - A x_{1} = b - A(x_{0} + y_{1}v_{1}) = (b - A x_{0}) - y_{1}A v_{1} = r_{0} - y_{1}A v_{1}\n$$\nUsing the Arnoldi relations $r_{0} = \\beta v_{1}$ and $A v_{1} = h_{11}v_{1} + h_{21}v_{2}$, we get:\n$$\nr_{1} = \\beta v_{1} - y_{1}(h_{11}v_{1} + h_{21}v_{2}) = (\\beta - y_{1}h_{11})v_{1} - y_{1}h_{21}v_{2}\n$$\nSince the Arnoldi vectors are orthonormal, the norm of the residual is:\n$$\n\\|r_{1}\\|^{2} = \\|(\\beta - y_{1}h_{11})v_{1} - y_{1}h_{21}v_{2}\\|^{2} = (\\beta - y_{1}h_{11})^{2} + (-y_{1}h_{21})^{2}\n$$\nMinimizing $\\|r_{1}\\|$ is equivalent to solving the following $(m+1) \\times m$ least-squares problem for $y$:\n$$\n\\min_{y \\in \\mathbb{R}^{m}} \\|\\hat{H}_{m} y - \\beta e_{1}\\|\n$$\nwhere $\\hat{H}_{m}$ is the $(m+1) \\times m$ upper Hessenberg matrix from the Arnoldi process, and $e_{1}$ is the first standard basis vector in $\\mathbb{R}^{m+1}$. The norm of the GMRES residual, $\\|r_{m}\\|$, is a direct result of this minimization.\n\nFor $m=1$, we have $y \\in \\mathbb{R}^{1}$ (a scalar $y_1$) and $e_{1} \\in \\mathbb{R}^{2}$. The problem is:\n$$\n\\min_{y_{1}} \\left\\| \\begin{bmatrix} h_{11} \\\\ h_{21} \\end{bmatrix} [y_{1}] - \\beta \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\|\n$$\nSubstituting the computed values $h_{11}=3$, $h_{21}=0$, and $\\beta=\\sqrt{2}$:\n$$\n\\min_{y_{1}} \\left\\| \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} [y_{1}] - \\sqrt{2} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\| = \\min_{y_{1}} \\left\\| \\begin{bmatrix} 3y_{1} - \\sqrt{2} \\\\ 0 \\end{bmatrix} \\right\\|\n$$\nThe norm of this vector is $\\sqrt{(3y_{1} - \\sqrt{2})^{2} + 0^{2}} = |3y_{1} - \\sqrt{2}|$.\nThe minimum value of this expression is achieved when $3y_{1} - \\sqrt{2} = 0$, which means choosing $y_{1} = \\frac{\\sqrt{2}}{3}$. The minimum value itself, which corresponds to the norm of the GMRES residual after one step, is $0$.\n\nThus, GMRES converges to the exact solution in a single step because the initial residual $r_{0}$ is an eigenvector of the matrix $A$. The resulting residual norm, $\\|r_{1}\\|$, is zero.", "answer": "$$\\boxed{0}$$", "id": "2570955"}]}