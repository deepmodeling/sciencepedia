{"hands_on_practices": [{"introduction": "The concept of preconditioning often boils down to finding a good, cheaply invertible approximation of the system matrix $A$. This exercise explores one of the most fundamental ways to construct such an approximation: the Neumann series. By working through this problem [@problem_id:2590440], you will derive a polynomial preconditioner from first principles for a matrix of the form $A = I - E$ and, more importantly, quantify its accuracy, providing a solid theoretical foundation for understanding more complex preconditioning strategies.", "problem": "Consider a linear system arising from a symmetric positive definite finite element method (FEM) discretization that has been diagonally equilibrated so that the coefficient matrix can be written as $A = I - E$ in an operator norm $\\|\\cdot\\|$ induced by an inner product, with $\\|E\\| = \\rho < 1$. A polynomial preconditioner based on the Neumann series is constructed to approximate $A^{-1}$ using only powers of $E$. Starting from the definition of the operator norm, its submultiplicativity, and the absolute convergence of series in norm when dominated by a convergent scalar series, derive a degree-$m$ Neumann series preconditioner $M_m^{-1}$ built from $\\{E^{k}\\}_{k=0}^{m}$ that approximates $A^{-1}$, and show that the corresponding truncation remainder is the tail of a norm-convergent geometric series. Then, using only these principles, obtain an explicit, closed-form operator-norm bound for the truncation error $\\|A^{-1} - M_m^{-1}\\|$ as a function of $m$ and $\\rho$. Express your final answer as a single analytic expression $f_m(\\rho)$ depending only on $m$ and $\\rho$. No numerical evaluation or rounding is required.", "solution": "The problem statement is validated and determined to be scientifically grounded, well-posed, and objective. The analysis proceeds as follows.\n\nThe problem revolves around the approximation of the inverse of a linear operator $A = I - E$, where $I$ is the identity operator and $E$ is an operator for which the operator norm $\\|E\\|$ is equal to a scalar $\\rho < 1$. The existence of $A^{-1}$ and its representation are guaranteed by the theory of Neumann series in a Banach space.\n\nThe Neumann series expansion for $(I-E)^{-1}$ is given by:\n$$A^{-1} = (I - E)^{-1} = \\sum_{k=0}^{\\infty} E^k$$\nThe convergence of this series must be justified as per the problem's constraints. We consider the series of norms, $\\sum_{k=0}^{\\infty} \\|E^k\\|$. The operator norm is submultiplicative, meaning $\\|XY\\| \\le \\|X\\|\\|Y\\|$. Applying this property repeatedly gives $\\|E^k\\| \\le \\|E\\|^k$. Given that $\\|E\\| = \\rho$, we have:\n$$\\|E^k\\| \\le \\rho^k$$\nTherefore, the series of norms is bounded by a scalar geometric series:\n$$\\sum_{k=0}^{\\infty} \\|E^k\\| \\le \\sum_{k=0}^{\\infty} \\rho^k$$\nSince it is given that $\\rho < 1$, the geometric series $\\sum_{k=0}^{\\infty} \\rho^k$ is convergent. By the comparison test, the series of norms $\\sum_{k=0}^{\\infty} \\|E^k\\|$ also converges. This signifies that the operator series $\\sum_{k=0}^{\\infty} E^k$ is absolutely convergent. In a Banach space (the space of bounded linear operators on the underlying vector space is complete), absolute convergence implies convergence. Thus, the Neumann series converges to a unique operator, which is $A^{-1} = (I-E)^{-1}$.\n\nThe problem defines a degree-$m$ polynomial preconditioner, $M_m^{-1}$, constructed from the set of operators $\\{E^k\\}_{k=0}^m$. This is the $m$-th partial sum of the Neumann series:\n$$M_m^{-1} = \\sum_{k=0}^{m} E^k = I + E + E^2 + \\dots + E^m$$\nThis is an approximation to the true inverse $A^{-1}$. The error of this approximation is the truncation error, defined as the difference $A^{-1} - M_m^{-1}$.\n\nSubstituting the series representations, we find:\n$$A^{-1} - M_m^{-1} = \\left( \\sum_{k=0}^{\\infty} E^k \\right) - \\left( \\sum_{k=0}^{m} E^k \\right) = \\sum_{k=m+1}^{\\infty} E^k$$\nThis demonstrates that the truncation remainder is precisely the tail of the norm-convergent Neumann series, starting from the term of degree $m+1$.\n\nThe final step is to derive a closed-form operator-norm bound for this truncation error, $\\|A^{-1} - M_m^{-1}\\|$. We take the norm of the remainder series:\n$$\\|A^{-1} - M_m^{-1}\\| = \\left\\| \\sum_{k=m+1}^{\\infty} E^k \\right\\|$$\nUsing the triangle inequality, which is valid for convergent series in a normed space, the norm of the sum is less than or equal to the sum of the norms:\n$$\\left\\| \\sum_{k=m+1}^{\\infty} E^k \\right\\| \\le \\sum_{k=m+1}^{\\infty} \\|E^k\\|$$\nAgain, we use the submultiplicativity of the norm, $\\|E^k\\| \\le \\|E\\|^k = \\rho^k$, to bound the terms of the series:\n$$\\|A^{-1} - M_m^{-1}\\| \\le \\sum_{k=m+1}^{\\infty} \\rho^k$$\nThe expression on the right is the sum of the tail of a geometric series. This sum can be calculated as:\n$$\\sum_{k=m+1}^{\\infty} \\rho^k = \\rho^{m+1} + \\rho^{m+2} + \\rho^{m+3} + \\dots$$\nBy factoring out the first term, $\\rho^{m+1}$, we get:\n$$\\rho^{m+1} (1 + \\rho + \\rho^2 + \\dots) = \\rho^{m+1} \\sum_{j=0}^{\\infty} \\rho^j$$\nThe sum of the infinite geometric series $\\sum_{j=0}^{\\infty} \\rho^j$ is $\\frac{1}{1-\\rho}$, since $|\\rho| < 1$.\nTherefore, the sum of the tail is:\n$$\\sum_{k=m+1}^{\\infty} \\rho^k = \\frac{\\rho^{m+1}}{1-\\rho}$$\nThis provides the explicit, closed-form operator-norm bound for the truncation error as a function of $m$ and $\\rho$:\n$$\\|A^{-1} - M_m^{-1}\\| \\le \\frac{\\rho^{m+1}}{1-\\rho}$$\nThe required function $f_m(\\rho)$ is this upper bound.", "answer": "$$\\boxed{\\frac{\\rho^{m+1}}{1-\\rho}}$$", "id": "2590440"}, {"introduction": "While theoretically powerful, many workhorse preconditioners like Incomplete Cholesky (IC) can be surprisingly fragile in practice, especially when faced with matrices from real-world physics like high-contrast diffusion. This problem [@problem_id:2590421] challenges you to diagnose why a standard IC($0$) factorization fails for such problems and to evaluate robust scaling techniques that restore its effectiveness. Mastering this material is crucial for moving from textbook examples to solving challenging, industrial-scale finite element problems.", "problem": "You discretize the scalar diffusion equation $-\\nabla \\cdot \\left(k(\\mathbf{x}) \\nabla u(\\mathbf{x})\\right) = f(\\mathbf{x})$ on a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^d$ with homogeneous Dirichlet boundary conditions using a conforming finite element method on a quasi-uniform mesh with continuous, piecewise-linear basis functions $\\{\\phi_i\\}_{i=1}^n$. The assembled stiffness matrix $A \\in \\mathbb{R}^{n \\times n}$ has entries $A_{ij} = \\int_{\\Omega} k(\\mathbf{x}) \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, \\mathrm{d}\\mathbf{x}$ and is symmetric positive definite.\n\nAssume the diffusion coefficient $k(\\mathbf{x})$ is piecewise constant with high contrast: there exist disjoint subregions where $k(\\mathbf{x}) = k_{\\min}$ and others where $k(\\mathbf{x}) = k_{\\max}$, with contrast ratio $J = k_{\\max}/k_{\\min} \\gg 1$. In practice, when solving $A x = b$ using a Preconditioned Conjugate Gradient (PCG) method with an Incomplete Cholesky factorization with zero fill (IC(0)) built on $A$ (or equivalently, Incomplete Lower-Upper factorization with zero fill (ILU(0)) specialized to symmetric positive definite matrices), severe iteration stagnation and, at times, factorization breakdowns (due to nonpositive pivots) are observed for large $J$.\n\nUsing only fundamental finite element method and linear algebra facts, analyze why such failures can occur. Your analysis should begin from the bilinear form $a(u,v) = \\int_{\\Omega} k(\\mathbf{x}) \\nabla u \\cdot \\nabla v \\, \\mathrm{d}\\mathbf{x}$, the structure of the element contributions to $A$, and basic matrix facts such as Gershgorin’s theorem and the role of diagonal dominance and pivot sizes in incomplete factorizations without pivoting. Then, identify which diagonal scaling or equilibration strategies can theoretically improve robustness of IC(0)/ILU(0) for this class of problems while preserving the applicability of PCG to the resulting preconditioned system.\n\nSelect all options that are valid, and for each selection justify briefly in your mind how it follows from first principles rather than relying on black-box results.\n\nA. Apply symmetric diagonal equilibration with $D = \\mathrm{diag}(A)$, forming the scaled system $\\tilde{A} = D^{-1/2} A D^{-1/2}$, compute IC(0) on $\\tilde{A}$, and solve for $\\tilde{y}$ with PCG; then recover $x$ via $x = D^{-1/2} \\tilde{y}$. This preserves symmetry and reduces diagonal variability, which tends to stabilize incomplete pivots.\n\nB. Perform left row equilibration only: let $D_r$ contain positive row norms (e.g., absolute row sums), form $\\hat{A} = D_r^{-1} A$, compute ILU(0) on $\\hat{A}$, and use the same PCG setup for the original symmetric system $A x = b$, claiming improved robustness without affecting PCG’s validity.\n\nC. Use iterative Ruiz equilibration to find diagonal $D_r$ and $D_c$ such that the rows and columns of $D_r A D_c$ have approximately unit $2$-norms; then set $D = \\sqrt{D_r D_c}$ and form the symmetric scaling $\\tilde{A} = D^{-1/2} A D^{-1/2}$ before applying IC(0), keeping PCG applicable.\n\nD. Use a matching-based diagonal scaling: find diagonal $D_r$ and $D_c$ so that the diagonal entries of $D_r A D_c$ are of order $1$ and comparatively large relative to off-diagonals (e.g., via maximal product matching), then define $D = \\sqrt{D_r D_c}$ and form $\\tilde{A} = D^{-1/2} A D^{-1/2}$ before IC(0), preserving symmetry and strengthening pivots.\n\nE. During assembly, scale each element stiffness submatrix $K_e$ by $k_e^{-1}$, where $k_e$ is the element-wise diffusion coefficient, assemble the modified matrix, and then apply IC(0) without further scaling, claiming this neutralizes heterogeneity at the algebraic level without altering the continuum operator in a way that harms correctness.\n\nChoose all that apply.", "solution": "The problem statement is scientifically and mathematically sound. It describes a standard scenario in the numerical solution of partial differential equations using the finite element method, where high-contrast material coefficients lead to ill-conditioned linear systems that are challenging for standard iterative solvers and preconditioners. The observed failures are a well-documented phenomenon.\n\nThe core task is to first analyze the mechanism of this failure for the Incomplete Cholesky factorization with zero fill-in, denoted IC($0$), and then to evaluate several proposed scaling strategies for their ability to mitigate this failure while preserving the necessary matrix properties for the use of the Preconditioned Conjugate Gradient (PCG) method.\n\nFirst, we analyze the source of the numerical difficulty. The stiffness matrix $A$ has entries given by $A_{ij} = \\int_{\\Omega} k(\\mathbf{x}) \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, \\mathrm{d}\\mathbf{x}$. The matrix $A$ is assembled from element stiffness matrices $K_e$, whose entries are $[K_e]_{ab} = \\int_{K_e} k_e \\nabla \\psi_a \\cdot \\nabla \\psi_b \\, \\mathrm{d}\\mathbf{x}$, where $k_e$ is the constant value of $k(\\mathbf{x})$ on element $K_e$. Consequently, the magnitude of the entries of $K_e$ is proportional to $k_e$.\n\nWhen the domain $\\Omega$ contains subregions with high contrast in the diffusion coefficient, i.e., $J = k_{\\max}/k_{\\min} \\gg 1$, the entries of the global matrix $A$ will exhibit vast differences in magnitude. A diagonal entry $A_{ii}$ corresponds to a basis function $\\phi_i$ centered at a node $i$. Its value is a sum of contributions from all elements containing that node: $A_{ii} = \\sum_{e | i \\in e} \\int_{K_e} k_e |\\nabla \\phi_i|^2 \\, \\mathrm{d}\\mathbf{x}$. If node $i$ is deep within a region where $k(\\mathbf{x}) = k_{\\max}$, then $A_{ii}$ will be of order $k_{\\max}$. If it is deep within a region where $k(\\mathbf{x}) = k_{\\min}$, then $A_{ii}$ will be of order $k_{\\min}$. The diagonal entries of $A$ can thus vary by a factor of roughly $J$. Off-diagonal entries $A_{ij}$ are non-zero only if nodes $i$ and $j$ belong to the same element; their magnitude is proportional to the $k_e$ of the shared element(s). This results in a matrix $A$ that is very poorly scaled. Although $A$ is symmetric positive definite (SPD), its condition number $\\kappa(A)$ is typically very large, scaling with the contrast $J$, which explains slow convergence or stagnation of iterative methods.\n\nThe failure of the IC($0$) preconditioner is more subtle. The IC($0$) factorization of an SPD matrix $A$ seeks a lower triangular matrix $L$, with the same sparsity pattern as the lower part of $A$, such that $A \\approx L L^T$. The entries of $L$ are computed via the recurrence:\n$$L_{ii} = \\sqrt{A_{ii} - \\sum_{k<i, L_{ik}\\neq 0} L_{ik}^2}$$\n$$L_{ji} = \\frac{1}{L_{ii}} \\left( A_{ij} - \\sum_{k<i, L_{ik}\\neq 0, L_{jk}\\neq 0} L_{jk} L_{ik} \\right) \\quad \\text{for } j > i$$\nWhile a complete Cholesky factorization is guaranteed to exist for any SPD matrix with positive pivots, the incomplete factorization is not. The terms that are \"dropped\" due to the zero fill-in constraint (the \"fill-in\") are crucial. The failure occurs in the computation of the diagonal pivot $L_{ii}$: the argument of the square root, $d_i = A_{ii} - \\sum_{k<i, L_{ik}\\neq 0} L_{ik}^2$, can become zero or negative. This happens when the matrix lacks sufficient diagonal dominance. The poor scaling caused by high-contrast coefficients is a prime cause. For a node $i$ in a low-conductivity ($k_{\\min}$) region, $A_{ii}$ can be very small. If this node is processed after a neighboring node $k$ in a high-conductivity ($k_{\\max}$) region, the computed entry $L_{ik}$ can be relatively large. The sum $\\sum L_{ik}^2$ can then overwhelm the small $A_{ii}$, leading to a non-positive pivot $d_i$ and factorization breakdown. Even if breakdown is avoided, the resulting preconditioner $M=LL^T$ may be a poor approximation to $A$, leading to iteration stagnation in PCG.\n\nThe remedy is to scale the matrix $A$ before factorization to make it more amenable to IC($0$). For PCG to remain applicable, any scaling must preserve the symmetric positive definite property. If we have a system $Ax=b$ where $A$ is SPD, we can transform it into an equivalent system $\\tilde{A}\\tilde{x} = \\tilde{b}$ where $\\tilde{A} = S A S$ for some non-singular matrix $S$. The solution is recovered via $x = S \\tilde{x}$. If $S$ is chosen to be a positive diagonal matrix, then $\\tilde{A}$ is also SPD. All robust scaling strategies for this problem follow this symmetric scaling pattern.\n\nNow, we evaluate each option:\n\n**A. Apply symmetric diagonal equilibration with $D = \\mathrm{diag}(A)$, forming the scaled system $\\tilde{A} = D^{-1/2} A D^{-1/2}$, compute IC(0) on $\\tilde{A}$, and solve for $\\tilde{y}$ with PCG; then recover $x$ via $x = D^{-1/2} \\tilde{y}$. This preserves symmetry and reduces diagonal variability, which tends to stabilize incomplete pivots.**\nThis describes symmetric Jacobi scaling. Since $A$ is SPD, its diagonal entries $A_{ii}$ are all positive, so $D = \\mathrm{diag}(A)$ is a positive diagonal matrix, and $D^{-1/2}$ is well-defined. The scaled matrix $\\tilde{A} = D^{-1/2} A D^{-1/2}$ is symmetric and positive definite, so PCG is applicable. The diagonal entries of $\\tilde{A}$ are all equal to $1$. This directly counteracts the large variation in diagonal entries, which is the primary source of poor scaling. By making the diagonal uniform, the relative magnitudes of off-diagonal entries are put into a more balanced perspective, which generally improves the numerical properties of the matrix for IC($0$). This is a standard, fundamental, and correct approach.\nVerdict: **Correct**.\n\n**B. Perform left row equilibration only: let $D_r$ contain positive row norms (e.g., absolute row sums), form $\\hat{A} = D_r^{-1} A$, compute ILU(0) on $\\hat{A}$, and use the same PCG setup for the original symmetric system $A x = b$, claiming improved robustness without affecting PCG’s validity.**\nThis strategy is fundamentally flawed. Applying a one-sided scaling $\\hat{A} = D_r^{-1} A$ destroys the symmetry of the original matrix $A$. The resulting matrix $\\hat{A}$ is generally non-symmetric. The PCG algorithm is only applicable to systems with symmetric (or more generally, Hermitian) positive definite operators. Applying PCG to a non-symmetric system is mathematically incorrect and will not converge to the correct solution. The claim that this does not affect PCG's validity is false. While one-sided scaling is used in the context of general non-symmetric solvers like GMRES or BiCGSTAB, it is incompatible with PCG.\nVerdict: **Incorrect**.\n\n**C. Use iterative Ruiz equilibration to find diagonal $D_r$ and $D_c$ such that the rows and columns of $D_r A D_c$ have approximately unit $2$-norms; then set $D = \\sqrt{D_r D_c}$ and form the symmetric scaling $\\tilde{A} = D^{-1/2} A D^{-1/2}$ before applying IC(0), keeping PCG applicable.**\nRuiz equilibration is a powerful scaling technique that iteratively computes diagonal matrices $D_r$ and $D_c$ to equilibrate the row and column norms of a matrix. For a general matrix, $D_r \\neq D_c$. For an SPD matrix $A$, we require a symmetric scaling to preserve the SPD property. The proposed procedure is a valid way to achieve this: after finding $D_r$ and $D_c$ (which will be positive diagonal matrices for an SPD matrix), one constructs a single symmetric scaling matrix $D = \\sqrt{D_r D_c}$. The symmetrically scaled matrix is then $\\tilde{A} = D^{-1/2} A D^{-1/2}$. Following the same logic as in option A, since $D$ is a positive diagonal matrix, $\\tilde{A}$ is SPD and PCG is applicable. This represents a more sophisticated equilibration strategy than simple Jacobi scaling and is known to be effective.\nVerdict: **Correct**.\n\n**D. Use a matching-based diagonal scaling: find diagonal $D_r$ and $D_c$ so that the diagonal entries of $D_r A D_c$ are of order $1$ and comparatively large relative to off-diagonals (e.g., via maximal product matching), then define $D = \\sqrt{D_r D_c}$ and form $\\tilde{A} = D^{-1/2} A D^{-1/2}$ before IC(0), preserving symmetry and strengthening pivots.**\nThis describes another advanced scaling method. Matching-based algorithms are designed to improve the diagonal dominance of a matrix. For an SPD matrix with a known positive diagonal, the permutation part of such algorithms is not needed, and the focus is on finding diagonal scaling matrices $D_r$ and $D_c$ to make the diagonal entries numerically large compared to the off-diagonals. This directly addresses the stability of factorization pivots. As in option C, the resulting non-symmetric scaling ($D_r, D_c$) is symmetrized by defining $D = \\sqrt{D_r D_c}$ and applying the transformation $\\tilde{A} = D^{-1/2} A D^{-1/2}$. This again produces an SPD matrix $\\tilde{A}$ for which PCG can be used. This strategy is theoretically sound and targets the core issue of weak pivots in IC($0$).\nVerdict: **Correct**.\n\n**E. During assembly, scale each element stiffness submatrix $K_e$ by $k_e^{-1}$, where $k_e$ is the element-wise diffusion coefficient, assemble the modified matrix, and then apply IC(0) without further scaling, claiming this neutralizes heterogeneity at the algebraic level without altering the continuum operator in a way that harms correctness.**\nThis proposal is fundamentally incorrect. The original matrix $A$ represents the operator $-\\nabla \\cdot(k \\nabla)$. Scaling each element submatrix $K_e$ by $k_e^{-1}$ means the assembled matrix, let's call it $A_1$, will have entries $A_{1,ij} = \\sum_{e | i,j \\in e} \\int_{K_e} \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, \\mathrm{d}\\mathbf{x} = \\int_{\\Omega} \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, \\mathrm{d}\\mathbf{x}$. This matrix $A_1$ is the stiffness matrix for the Poisson operator $-\\nabla^2$. Thus, this procedure does not scale the original problem; it replaces the original differential operator with a different one. Solving a system with matrix $A_1$ is equivalent to solving a different physical problem, namely one with a constant diffusion coefficient $k=1$. The claim that this does not harm correctness is false. It completely changes the model. While $A_1$ is well-conditioned and IC($0$) works well for it, it is not a valid preconditioning or scaling strategy for the original problem $Ax=b$.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "2590421"}, {"introduction": "An effective preconditioner strikes a delicate balance: it must be a good enough approximation to the inverse to significantly reduce iterations, but not so complex that its own application cost overwhelms the savings. This exercise [@problem_id:2590469] formalizes this trade-off using a performance model for an Incomplete $\\mathrm{LU}$ with Thresholding (ILUT) preconditioner. By finding the optimal drop tolerance $\\tau^{\\ast}$, you will engage in a practical optimization task that lies at the heart of high-performance iterative methods.", "problem": "A linear system $A x = b$ arises from a conforming finite element (FE) discretization of a second-order elliptic boundary value problem on a shape-regular mesh, producing a large, sparse matrix $A$. You solve this system with right-preconditioned Generalized Minimal Residual (GMRES) using an Incomplete $\\mathrm{LU}$ with Threshold (ILUT) preconditioner with drop tolerance $\\tau \\in (0,1]$. Assume the following modeling hypotheses hold over an operating range of $\\tau$ where the preconditioner is stable and the spectrum of the preconditioned operator is well-behaved.\n\n1. One iteration of GMRES requires one Sparse Matrix-Vector product (SpMV) and two triangular solves with the ILUT factors. The SpMV has cost $C_{\\mathrm{spmv}}$ (a positive constant). The total number of nonzeros in the ILUT factors is modeled by $\\phi(\\tau) = \\phi_{0} + \\phi_{1}\\,\\tau^{-1}$, with $\\phi_{0} > 0$ and $\\phi_{1} > 0$. Each triangular solve costs $c_{t}\\,\\phi(\\tau)$ with $c_{t} > 0$. Thus, the per-iteration cost is\n$$\nC_{\\mathrm{it}}(\\tau) = C_{\\mathrm{spmv}} + 2\\,c_{t}\\,\\phi(\\tau).\n$$\n\n2. For a prescribed relative residual reduction target $\\varepsilon \\in (0,1)$, the GMRES iteration count is modeled as\n$$\nk(\\tau) = \\ln\\!\\big(\\varepsilon^{-1}\\big)\\,\\big(\\alpha_{0} + \\alpha_{1}\\,\\tau\\big),\n$$\nwith $\\alpha_{0} > 0$ and $\\alpha_{1} > 0$, reflecting that as the drop tolerance $\\tau$ increases (more dropping), the preconditioner weakens and more iterations are required, whereas as $\\tau \\to 0$ the iteration count approaches a floor proportional to $\\ln(\\varepsilon^{-1})$.\n\n3. The total time-to-solution is the product $T(\\tau) = k(\\tau)\\,C_{\\mathrm{it}}(\\tau)$; ignore other costs.\n\nUsing only these assumptions and standard definitions of time-to-solution and operation costs, derive the drop tolerance $\\tau^{\\ast}$ that minimizes the total time-to-solution $T(\\tau)$ over the admissible range where the above models are valid. Express your final answer as a single closed-form analytic expression in terms of the positive constants $C_{\\mathrm{spmv}}$, $c_{t}$, $\\phi_{0}$, $\\phi_{1}$, $\\alpha_{0}$, and $\\alpha_{1}$, and $\\varepsilon$. No numerical evaluation is required, and no units are needed. The final answer must be a single analytic expression.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard optimization task arising in the performance modeling of iterative linear solvers. All necessary functional forms and constants are provided. There are no contradictions, ambiguities, or violations of physical or mathematical principles. Therefore, the problem is valid, and a solution will be derived.\n\nThe objective is to find the optimal drop tolerance, denoted $\\tau^{\\ast}$, that minimizes the total time-to-solution, $T(\\tau)$. The total time is given as the product of the number of iterations, $k(\\tau)$, and the cost per iteration, $C_{\\mathrm{it}}(\\tau)$.\n$$\nT(\\tau) = k(\\tau) C_{\\mathrm{it}}(\\tau)\n$$\nThe provided models for $k(\\tau)$ and $C_{\\mathrm{it}}(\\tau)$ are:\n$$\nk(\\tau) = \\ln(\\varepsilon^{-1})(\\alpha_{0} + \\alpha_{1}\\tau)\n$$\n$$\nC_{\\mathrm{it}}(\\tau) = C_{\\mathrm{spmv}} + 2c_{t}\\phi(\\tau) = C_{\\mathrm{spmv}} + 2c_{t}(\\phi_{0} + \\phi_{1}\\tau^{-1})\n$$\nwhere $C_{\\mathrm{spmv}}$, $c_{t}$, $\\phi_{0}$, $\\phi_{1}$, $\\alpha_{0}$, $\\alpha_{1}$ are all positive constants, and $\\varepsilon \\in (0,1)$.\n\nCombining these expressions, the total time-to-solution $T(\\tau)$ is a function of the drop tolerance $\\tau$:\n$$\nT(\\tau) = \\ln(\\varepsilon^{-1}) (\\alpha_{0} + \\alpha_{1}\\tau) \\left( C_{\\mathrm{spmv}} + 2c_{t}\\phi_{0} + 2c_{t}\\phi_{1}\\tau^{-1} \\right)\n$$\nTo find the value of $\\tau$ that minimizes $T(\\tau)$, we must use differential calculus. We compute the derivative of $T(\\tau)$ with respect to $\\tau$ and set it to zero. Let us define some composite positive constants to simplify the expression:\nLet $K = \\ln(\\varepsilon^{-1})$.\nLet $C_{A} = C_{\\mathrm{spmv}} + 2c_{t}\\phi_{0}$.\nLet $C_{B} = 2c_{t}\\phi_{1}$.\nThe function to minimize is now:\n$$\nT(\\tau) = K (\\alpha_{0} + \\alpha_{1}\\tau) (C_{A} + C_{B}\\tau^{-1})\n$$\nExpanding the terms within the parentheses, we get:\n$$\nT(\\tau) = K (\\alpha_{0}C_{A} + \\alpha_{0}C_{B}\\tau^{-1} + \\alpha_{1}C_{A}\\tau + \\alpha_{1}C_{B})\n$$\nNow, we differentiate $T(\\tau)$ with respect to $\\tau$:\n$$\n\\frac{dT}{d\\tau} = K \\frac{d}{d\\tau} (\\alpha_{0}C_{A} + \\alpha_{0}C_{B}\\tau^{-1} + \\alpha_{1}C_{A}\\tau + \\alpha_{1}C_{B})\n$$\nThe terms $\\alpha_{0}C_{A}$ and $\\alpha_{1}C_{B}$ are constant with respect to $\\tau$. The derivative is:\n$$\n\\frac{dT}{d\\tau} = K \\left( -\\alpha_{0}C_{B}\\tau^{-2} + \\alpha_{1}C_{A} \\right)\n$$\nTo find the critical point $\\tau^{\\ast}$, we set the derivative equal to zero. Since $K = \\ln(\\varepsilon^{-1}) > 0$ for $\\varepsilon \\in (0,1)$, we must have:\n$$\n-\\alpha_{0}C_{B}(\\tau^{\\ast})^{-2} + \\alpha_{1}C_{A} = 0\n$$\nSolving for $\\tau^{\\ast}$:\n$$\n\\alpha_{1}C_{A} = \\alpha_{0}C_{B}(\\tau^{\\ast})^{-2}\n$$\n$$\n(\\tau^{\\ast})^{2} = \\frac{\\alpha_{0}C_{B}}{\\alpha_{1}C_{A}}\n$$\nSince the drop tolerance $\\tau$ must be positive, we take the positive root:\n$$\n\\tau^{\\ast} = \\sqrt{\\frac{\\alpha_{0}C_{B}}{\\alpha_{1}C_{A}}}\n$$\nTo confirm that this critical point corresponds to a minimum, we examine the second derivative of $T(\\tau)$:\n$$\n\\frac{d^{2}T}{d\\tau^{2}} = \\frac{d}{d\\tau} \\left[ K \\left( -\\alpha_{0}C_{B}\\tau^{-2} + \\alpha_{1}C_{A} \\right) \\right] = K (-(-2)\\alpha_{0}C_{B}\\tau^{-3}) = 2K\\alpha_{0}C_{B}\\tau^{-3}\n$$\nAll constants $K$, $\\alpha_{0}$, and $C_{B}$ are positive. For any admissible $\\tau > 0$, the second derivative $\\frac{d^{2}T}{d\\tau^{2}}$ is strictly positive. This confirms that $\\tau^{\\ast}$ is a local minimum. Given that $T(\\tau) \\to \\infty$ as $\\tau \\to 0^{+}$ and as $\\tau \\to \\infty$, this single critical point must be the global minimum.\n\nFinally, we substitute the original expressions for the composite constants $C_{A}$ and $C_{B}$ back into the equation for $\\tau^{\\ast}$:\n$$\nC_{A} = C_{\\mathrm{spmv}} + 2c_{t}\\phi_{0}\n$$\n$$\nC_{B} = 2c_{t}\\phi_{1}\n$$\nThe optimal drop tolerance $\\tau^{\\ast}$ is therefore:\n$$\n\\tau^{\\ast} = \\sqrt{\\frac{\\alpha_{0}(2c_{t}\\phi_{1})}{\\alpha_{1}(C_{\\mathrm{spmv}} + 2c_{t}\\phi_{0})}}\n$$\nThis is the closed-form analytic expression for the optimal drop tolerance that minimizes the total time-to-solution under the given modeling assumptions.", "answer": "$$\n\\boxed{\\sqrt{\\frac{2 \\alpha_{0} c_{t} \\phi_{1}}{\\alpha_{1} (C_{\\mathrm{spmv}} + 2 c_{t} \\phi_{0})}}}\n$$", "id": "2590469"}]}