{"hands_on_practices": [{"introduction": "Accurate instrument calibration is the bedrock of reliable quantitative measurement. This first practice moves beyond simple least-squares fitting by introducing a fully probabilistic approach to Atomic Force Microscope (AFM) deflection sensitivity calibration [@problem_id:2777620]. You will apply the principles of Bayesian linear regression to infer a posterior distribution for the calibration parameters, allowing you to not only estimate their values but also to rigorously quantify their uncertainty. The exercise further explores the practical challenge of \"calibration transfer,\" providing a framework to model and calculate how uncertainty increases when experimental components are swapped.", "problem": "Atomic Force Microscopy (AFM) deflection sensitivity calibration relates the photodiode voltage to the vertical piezo displacement on a stiff reference surface. You will construct a probabilistic calibration curve using Bayesian linear regression with known Gaussian noise and then quantify calibration transfer uncertainty when switching from a source cantilever to a target cantilever. The modeling assumptions and tasks are as follows.\n\nModeling assumptions:\n- The measurement model is linear: the surface approach displacement in nanometers, denoted $z$ in $\\mathrm{nm}$, is related to the photodiode voltage $V$ in $\\mathrm{V}$ by $z = \\alpha + \\beta V + \\epsilon$ with additive zero-mean Gaussian noise $\\epsilon$ of known variance $\\sigma^2$ in $\\mathrm{nm}^2$. Here, $\\alpha$ is an intercept in $\\mathrm{nm}$, and $\\beta$ is the deflection sensitivity in $\\mathrm{nm/V}$.\n- A Gaussian prior is placed on $\\theta = [\\alpha,\\beta]^T$: $\\theta \\sim \\mathcal{N}(m_0, V_0)$, with $m_0 \\in \\mathbb{R}^2$ and $V_0 \\in \\mathbb{R}^{2 \\times 2}$ positive definite.\n- When switching cantilevers, the target cantilever parameters $\\theta_t$ are modeled as a Gaussian perturbation of the source cantilever’s posterior parameters $\\theta_s$: $\\theta_t = \\theta_s + \\delta$, where $\\delta \\sim \\mathcal{N}(0, T)$ with $T \\in \\mathbb{R}^{2 \\times 2}$ positive semidefinite. This implies a transferred prior for the target cantilever $\\theta_t \\sim \\mathcal{N}(m_s, V_s + T)$, where $(m_s, V_s)$ are the source posterior mean and covariance.\n\nComputational tasks for each test case:\n1. Construct the source cantilever calibration curve by computing the posterior mean and covariance for $(\\alpha,\\beta)$ using the specified prior and the provided source dataset $(V_s, z_s)$. Report the posterior mean of $\\beta$ in $\\mathrm{nm/V}$ as the source deflection sensitivity estimate for the calibration curve.\n2. For the target cantilever, use the transferred prior $\\mathcal{N}(m_s, V_s + T)$ and the target dataset $(V_t, z_t)$ to compute the posterior. For each specified query voltage $V^\\star$, compute the posterior predictive mean for $z$ in $\\mathrm{nm}$.\n3. Quantify calibration transfer uncertainty at each $V^\\star$ as follows. Let $\\mathrm{Var}_T(z^\\star)$ be the posterior predictive variance at $V^\\star$ computed with the transferred prior that includes the switching covariance $T$, and let $\\mathrm{Var}_0(z^\\star)$ be the posterior predictive variance computed with the same target data but with $T$ set to the zero matrix (i.e., no switching uncertainty, using the source posterior as the prior). Define the calibration transfer uncertainty as\n$U(V^\\star) = \\sqrt{\\max\\{0, \\mathrm{Var}_T(z^\\star) - \\mathrm{Var}_0(z^\\star)\\}}$ in $\\mathrm{nm}$.\n\nYour program must implement the above using any modern programming language, with the following test suite and parameters. Treat the known noise standard deviation $\\sigma$ as specified for each case. Use the same prior $(m_0, V_0)$ for all cases. All arrays are given explicitly and must be used as-is without modification.\n\nCommon prior for all cases:\n- $m_0 = [0.0, 40.0]^T$\n- $V_0 = \\mathrm{diag}([100.0, 100.0])$\n\nTest case 1:\n- Source data $(V_s, z_s)$:\n  - $V_s = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]$ in $\\mathrm{V}$\n  - $z_s = [0.52, 10.02, 19.75, 29.27, 38.94, 48.48]$ in $\\mathrm{nm}$\n- Target data $(V_t, z_t)$:\n  - $V_t = [0.15, 0.5, 0.9]$ in $\\mathrm{V}$\n  - $z_t = [7.76, 25.32, 45.29]$ in $\\mathrm{nm}$\n- Known noise: $\\sigma = 0.3$ in $\\mathrm{nm}$\n- Switching covariance $T = \\mathrm{diag}([0.5^2, 3.0^2])$ with units $\\mathrm{nm}^2$ and $(\\mathrm{nm/V})^2$\n- Query voltages: $V^\\star \\in \\{0.3, 1.2\\}$ in $\\mathrm{V}$\n\nTest case 2:\n- Source data $(V_s, z_s)$:\n  - $V_s = [0.0, 0.5, 1.0, 1.5]$ in $\\mathrm{V}$\n  - $z_s = [0.01, 7.52, 15.02, 22.57]$ in $\\mathrm{nm}$\n- Target data $(V_t, z_t)$:\n  - $V_t = [0.2]$ in $\\mathrm{V}$\n  - $z_t = [3.02]$ in $\\mathrm{nm}$\n- Known noise: $\\sigma = 0.2$ in $\\mathrm{nm}$\n- Switching covariance $T = \\mathrm{diag}([0.2^2, 1.5^2])$\n- Query voltages: $V^\\star \\in \\{0.2, 1.0\\}$ in $\\mathrm{V}$\n\nTest case 3:\n- Source data $(V_s, z_s)$:\n  - $V_s = [0.0, 0.25, 0.5, 0.75]$ in $\\mathrm{V}$\n  - $z_s = [-1.00, 12.70, 26.56, 40.21]$ in $\\mathrm{nm}$\n- Target data $(V_t, z_t)$:\n  - $V_t = [0.1, 0.7]$ in $\\mathrm{V}$\n  - $z_t = [5.53, 41.48]$ in $\\mathrm{nm}$\n- Known noise: $\\sigma = 0.25$ in $\\mathrm{nm}$\n- Switching covariance $T = \\mathrm{diag}([0.8^2, 5.0^2])$\n- Query voltages: $V^\\star \\in \\{0.0, 0.5, 1.0\\}$ in $\\mathrm{V}$\n\nRequired outputs and units:\n- For each test case, output a list whose first element is the posterior mean of the source deflection sensitivity $\\beta$ in $\\mathrm{nm/V}$, followed by pairs for each $V^\\star$ of the posterior predictive mean of $z$ in $\\mathrm{nm}$ and the calibration transfer uncertainty $U(V^\\star)$ in $\\mathrm{nm}$, in the order that $V^\\star$ values are provided above.\n- All numerical outputs must be expressed in the units indicated and rounded to exactly six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one bracketed, comma-separated sublist as described above. For example: \"[[case1_item1,case1_item2,...],[case2_item1,...],[case3_item1,...]]\". Ensure each float has exactly six digits after the decimal point.", "solution": "The problem presented is a standard, well-posed exercise in applied Bayesian statistics, specifically concerning linear regression. It has a sound scientific basis in the field of nanomechanics for the calibration of Atomic Force Microscopes. We will proceed with a formal solution.\n\nThe core of the problem lies in the application of Bayesian inference to a linear model. The relationship between the photodiode voltage $V$ and the piezo displacement $z$ is given by the linear equation:\n$$\nz = \\alpha + \\beta V + \\epsilon\n$$\nwhere $\\theta = [\\alpha, \\beta]^T$ are the model parameters (intercept and sensitivity), and $\\epsilon$ is additive Gaussian noise with zero mean and known variance $\\sigma^2$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThis can be expressed in matrix notation. For a set of $N$ observations $(V_i, z_i)$, we can write $z_i = \\phi_i^T \\theta + \\epsilon_i$, where $\\phi_i = [1, V_i]^T$. The likelihood for the entire dataset $D = \\{(V_i, z_i)\\}_{i=1}^N$ is:\n$$\np(z | \\theta, X, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(z - X\\theta)^T(z - X\\theta)\\right)\n$$\nwhere $z = [z_1, ..., z_N]^T$ is the vector of displacement measurements and $X$ is the $N \\times 2$ design matrix with rows $\\phi_i^T$.\n\nA Gaussian prior is placed on the parameters $\\theta \\sim \\mathcal{N}(m_0, V_0)$. According to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\theta|D) \\propto p(z|\\theta)p(\\theta)$. For conjugate Gaussian prior and likelihood, the posterior distribution for $\\theta$ is also a Gaussian, $\\theta | D \\sim \\mathcal{N}(m_N, V_N)$, with posterior mean $m_N$ and covariance $V_N$ given by:\n$$\nV_N = \\left( V_0^{-1} + \\frac{1}{\\sigma^2} X^T X \\right)^{-1}\n$$\n$$\nm_N = V_N \\left( V_0^{-1} m_0 + \\frac{1}{\\sigma^2} X^T z \\right)\n$$\nThese formulas provide the mechanism for updating our belief about the parameters $\\theta$ after observing data.\n\nThe computational tasks are executed as follows:\n\n1.  **Source Cantilever Calibration**: We first compute the posterior distribution for the source cantilever's parameters, $\\theta_s = [\\alpha_s, \\beta_s]^T$. We use the specified common prior $(m_0, V_0)$ and the source dataset $(V_s, z_s)$. We construct the source design matrix $X_s$ from $V_s$ and apply the update equations to find the source posterior mean $m_s$ and covariance $V_s$. The estimated source deflection sensitivity is the second component of the posterior mean vector $m_s$.\n\n2.  **Target Cantilever Posterior Predictive Mean**: When switching to a target cantilever, the parameters are assumed to be a perturbation of the source parameters. This is modeled by a transferred prior for the target parameters $\\theta_t$:\n    $$\n    \\theta_t \\sim \\mathcal{N}(m_s, V_s + T)\n    $$\n    where $T$ is the specified switching covariance matrix. This new prior is then updated using the target dataset $(V_t, z_t)$ to obtain the target posterior distribution, $\\theta_t | D_t \\sim \\mathcal{N}(m_t, V_t)$.\n    For a new query voltage $V^\\star$, we form a design vector $\\phi^\\star = [1, V^\\star]^T$. The posterior predictive distribution for the corresponding displacement $z^\\star$ is a Gaussian. Its mean, which is our prediction, is given by:\n    $$\n    E[z^\\star | D_t, V^\\star] = (\\phi^\\star)^T m_t\n    $$\n\n3.  **Calibration Transfer Uncertainty**: This quantity measures the additional uncertainty introduced by the cantilever switching process, which is captured by the matrix $T$. We compute the posterior predictive variance for $z^\\star$ under two conditions.\n    First, with the full transferred prior including $T$, the variance is:\n    $$\n    \\mathrm{Var}_T(z^\\star) = \\sigma^2 + (\\phi^\\star)^T V_t \\phi^\\star\n    $$\n    Here, $V_t$ is the target posterior covariance computed using the prior covariance $V_s + T$. The term $\\sigma^2$ accounts for the observation noise of the new measurement.\n    Second, we compute a baseline variance, $\\mathrm{Var}_0(z^\\star)$, by setting $T$ to the zero matrix. This corresponds to a situation where there is no additional uncertainty from switching; the source posterior is used directly as the prior for the target. This calculation yields a different target posterior covariance, let us call it $V_{t,0}$, and the variance is:\n    $$\n    \\mathrm{Var}_0(z^\\star) = \\sigma^2 + (\\phi^\\star)^T V_{t,0} \\phi^\\star\n    $$\n    The calibration transfer uncertainty is then defined and computed as:\n    $$\n    U(V^\\star) = \\sqrt{\\max\\{0, \\mathrm{Var}_T(z^\\star) - \\mathrm{Var}_0(z^\\star)\\}} = \\sqrt{\\max\\{0, (\\phi^\\star)^T (V_t - V_{t,0}) \\phi^\\star\\}}\n    $$\nThe implementation will systematically apply these matrix computations for each test case as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the AFM calibration problem using Bayesian linear regression\n    for the provided test cases.\n    \"\"\"\n\n    # Common prior parameters\n    m0 = np.array([0.0, 40.0])\n    V0 = np.diag([100.0, 100.0])\n\n    test_cases = [\n        {\n            \"Vs\": np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n            \"zs\": np.array([0.52, 10.02, 19.75, 29.27, 38.94, 48.48]),\n            \"Vt\": np.array([0.15, 0.5, 0.9]),\n            \"zt\": np.array([7.76, 25.32, 45.29]),\n            \"sigma\": 0.3,\n            \"T\": np.diag([0.5**2, 3.0**2]),\n            \"V_star\": [0.3, 1.2],\n        },\n        {\n            \"Vs\": np.array([0.0, 0.5, 1.0, 1.5]),\n            \"zs\": np.array([0.01, 7.52, 15.02, 22.57]),\n            \"Vt\": np.array([0.2]),\n            \"zt\": np.array([3.02]),\n            \"sigma\": 0.2,\n            \"T\": np.diag([0.2**2, 1.5**2]),\n            \"V_star\": [0.2, 1.0],\n        },\n        {\n            \"Vs\": np.array([0.0, 0.25, 0.5, 0.75]),\n            \"zs\": np.array([-1.00, 12.70, 26.56, 40.21]),\n            \"Vt\": np.array([0.1, 0.7]),\n            \"zt\": np.array([5.53, 41.48]),\n            \"sigma\": 0.25,\n            \"T\": np.diag([0.8**2, 5.0**2]),\n            \"V_star\": [0.0, 0.5, 1.0],\n        },\n    ]\n\n    def bayesian_update(prior_mean, prior_cov, V_data, z_data, sigma2):\n        \"\"\"\n        Computes the posterior mean and covariance for a linear model.\n        \"\"\"\n        X_data = np.vstack((np.ones_like(V_data), V_data)).T\n        prior_cov_inv = np.linalg.inv(prior_cov)\n        \n        # Posterior covariance\n        post_cov_inv = prior_cov_inv + (1.0 / sigma2) * X_data.T @ X_data\n        post_cov = np.linalg.inv(post_cov_inv)\n        \n        # Posterior mean\n        post_mean = post_cov @ (prior_cov_inv @ prior_mean + (1.0 / sigma2) * X_data.T @ z_data)\n        \n        return post_mean, post_cov\n\n    all_results = []\n    for case in test_cases:\n        Vs, zs = case[\"Vs\"], case[\"zs\"]\n        Vt, zt = case[\"Vt\"], case[\"zt\"]\n        sigma = case[\"sigma\"]\n        T = case[\"T\"]\n        V_star_list = case[\"V_star\"]\n        sigma2 = sigma**2\n        \n        case_outputs = []\n\n        # 1. Source cantilever calibration\n        ms_post, Vs_post = bayesian_update(m0, V0, Vs, zs, sigma2)\n        source_beta_mean = ms_post[1]\n        case_outputs.append(source_beta_mean)\n        \n        # 2. & 3. Target cantilever posterior and uncertainty\n        \n        # Compute target posterior with transfer uncertainty T\n        mt_post_T, Vt_post_T = bayesian_update(ms_post, Vs_post + T, Vt, zt, sigma2)\n        \n        # Compute target posterior without transfer uncertainty (T=0)\n        # The prior covariance is just the source posterior covariance Vs_post.\n        _ , Vt_post_0 = bayesian_update(ms_post, Vs_post, Vt, zt, sigma2)\n        \n        for v_star in V_star_list:\n            phi_star = np.array([1.0, v_star])\n            \n            # Posterior predictive mean\n            pred_mean = phi_star @ mt_post_T\n            \n            # Predictive variances for uncertainty calculation\n            pred_var_T_component = phi_star.T @ Vt_post_T @ phi_star\n            pred_var_0_component = phi_star.T @ Vt_post_0 @ phi_star\n            \n            # Calibration transfer uncertainty\n            variance_diff = pred_var_T_component - pred_var_0_component\n            uncertainty = np.sqrt(max(0, variance_diff))\n            \n            case_outputs.append(pred_mean)\n            case_outputs.append(uncertainty)\n            \n        all_results.append(case_outputs)\n\n    # Format output string\n    formatted_results = []\n    for result_list in all_results:\n        formatted_list = [f\"{x:.6f}\" for x in result_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2777620"}, {"introduction": "Many nanomechanical properties, such as friction, arise from complex multi-scale interactions that defy simple analytical description. This practice introduces Gaussian Processes (GPs) as a powerful non-parametric tool to build a data-driven surrogate model of the frictional response of a textured surface [@problem_id:2777669]. By learning the relationship between surface topography and friction from simulated data, you will then compute the analytical gradient of the GP model, a key technique for performing sensitivity analysis and a foundational step towards machine-learning-driven inverse design and optimization of material surfaces.", "problem": "You are given a surrogate modeling task motivated by frictional response of a sinusoidal surface in nanomechanics. Consider a one-dimensional sinusoidal topography defined by $z(x) = A \\sin\\!\\left(2\\pi x / L\\right)$, where $A$ is the texture amplitude and $L$ is the texture wavelength. Under a fixed normal load $N$, the tangential friction force $F$ can be modeled as a function of $(A, L)$, but the mapping is not known in closed form. In practice, one can use a Machine Learning surrogate such as a Gaussian Process (GP) to predict $F$ from $(A, L)$ based on training data. Your task is to implement a Gaussian Process surrogate for $F(A, L)$ using a synthetic but physically plausible data generator and then compute the gradient (sensitivity) of the GP posterior mean prediction with respect to $A$ and $L$. You must verify these sensitivities against central finite differences.\n\nWork in the following physically consistent setting:\n- Input variables: texture amplitude $A$ and wavelength $L$ in nanometers; internally convert to meters before any computation.\n- Normal load: $N = 10\\,\\mu\\text{N}$; internally use $N = 10^{-5}\\,\\text{N}$.\n- Synthetic ground-truth generator used only for constructing the training set (not for test predictions) is defined by\n  $$\n  F_{\\text{true}}(A,L;N) \\;=\\; N \\left[ \\mu_0 \\;+\\; \\alpha \\,\\frac{\\left(\\dfrac{A}{L}\\right)^2}{1 + \\left(\\dfrac{A}{L_c}\\right)^2} \\;+\\; \\beta \\,\\frac{A}{A + A_0}\\,\\frac{L_{\\mathrm{ref}}}{L} \\right],\n  $$\n  where all quantities are in the International System of Units (SI). Use the constants $\\mu_0 = 0.2$, $\\alpha = 0.6$, $\\beta = 0.05$, $L_c = 50\\,\\text{nm}$, $A_0 = 1\\,\\text{nm}$, and $L_{\\mathrm{ref}} = 100\\,\\text{nm}$. This model reflects that increased roughness amplitude and reduced wavelength tend to increase dissipation, while saturating at very large amplitude scales; it is dimensionally consistent because each bracketed term is dimensionless and multiplies $N$.\n\nConstruct a Gaussian Process (GP) surrogate as follows:\n- Use a zero-mean GP prior with an Automatic Relevance Determination (ARD) squared-exponential covariance kernel\n  $$\n  k(\\mathbf{x}, \\mathbf{x}') \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2}\\sum_{j=1}^{2}\\frac{(x_j - x_j')^2}{\\ell_j^2}\\right),\n  $$\n  where $\\mathbf{x} = [A, L]^\\top$ with both entries in meters. Use hyperparameters $\\sigma_f = 2\\times 10^{-6}\\,\\text{N}$, $\\ell_1 = 5\\times 10^{-9}\\,\\text{m}$ for $A$, and $\\ell_2 = 5\\times 10^{-8}\\,\\text{m}$ for $L$. Add independent Gaussian observation noise with standard deviation $\\sigma_n = 10^{-8}\\,\\text{N}$ to the training outputs.\n\nTraining data:\n- Build a grid of training inputs $(A, L)$ with $A \\in \\{2, 5, 10, 15, 18\\}\\,\\text{nm}$ and $L \\in \\{20, 50, 80, 120, 160\\}\\,\\text{nm}$. For each pair, compute the noiseless $F_{\\text{true}}$ at $N = 10\\,\\mu\\text{N}$ and then model the observations as $y = F_{\\text{true}}$ (the noise only enters the GP via $\\sigma_n$ in the covariance matrix).\n\nPrediction and gradient:\n- For a test input $\\mathbf{x}_\\star = [A_\\star, L_\\star]^\\top$ (in meters), the GP posterior mean is $m(\\mathbf{x}_\\star) = \\mathbf{k}_\\star^\\top \\mathbf{\\alpha}$, where $\\mathbf{k}_\\star$ is the vector of kernel evaluations between $\\mathbf{x}_\\star$ and each training input, and $\\mathbf{\\alpha}$ solves $(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}) \\mathbf{\\alpha} = \\mathbf{y}$ with $\\mathbf{K}$ the training kernel matrix. The gradient of the posterior mean with respect to $\\mathbf{x}_\\star$ exists in closed form via differentiation of the kernel,\n  $$\n  \\nabla_{\\mathbf{x}_\\star} m(\\mathbf{x}_\\star) \\;=\\; \\sum_{i} \\alpha_i \\,\\nabla_{\\mathbf{x}_\\star} k(\\mathbf{x}_\\star,\\mathbf{x}_i),\n  $$\n  with each component using the chain rule applied to the ARD squared-exponential. Your program must implement this analytic gradient.\n\nVerification by finite differences:\n- Verify the analytic gradient against a central finite-difference (FD) approximation using step sizes $h_A = 5\\times 10^{-13}\\,\\text{m}$ and $h_L = 5\\times 10^{-12}\\,\\text{m}$. Use the same GP posterior mean $m(\\cdot)$ for the FD evaluations; do not use the ground-truth generator in verification. Compute the finite-difference gradient components by\n  $$\n  \\frac{\\partial m}{\\partial A}(\\mathbf{x}_\\star) \\approx \\frac{m([A_\\star + h_A, L_\\star]) - m([A_\\star - h_A, L_\\star])}{2 h_A}, \\quad\n  \\frac{\\partial m}{\\partial L}(\\mathbf{x}_\\star) \\approx \\frac{m([A_\\star, L_\\star + h_L]) - m([A_\\star, L_\\star - h_L])}{2 h_L}.\n  $$\n\nTest suite:\n- Use normal load $N = 10\\,\\mu\\text{N}$ for all tests.\n- Evaluate the maximum absolute component-wise discrepancy between the analytic gradient and the central finite-difference gradient,\n  $$\n  \\Delta(\\mathbf{x}_\\star) \\;=\\; \\max\\!\\left(\\left|\\frac{\\partial m}{\\partial A}\\Big|_{\\text{analytic}} - \\frac{\\partial m}{\\partial A}\\Big|_{\\text{FD}}\\right|, \\left|\\frac{\\partial m}{\\partial L}\\Big|_{\\text{analytic}} - \\frac{\\partial m}{\\partial L}\\Big|_{\\text{FD}}\\right|\\right),\n  $$\n  in units of $\\text{N}/\\text{m}$.\n- Use the following four test inputs, given in nanometers and internally converted to meters:\n  - Case $1$: $A_\\star = 8\\,\\text{nm}$, $L_\\star = 60\\,\\text{nm}$.\n  - Case $2$: $A_\\star = 1.5\\,\\text{nm}$, $L_\\star = 200\\,\\text{nm}$.\n  - Case $3$: $A_\\star = 18\\,\\text{nm}$, $L_\\star = 30\\,\\text{nm}$.\n  - Case $4$: $A_\\star = 2\\,\\text{nm}$, $L_\\star = 20\\,\\text{nm}$.\n\nRequired output:\n- Your program should produce a single line of output containing a list of $4$ floating-point values, each equal to $\\Delta(\\mathbf{x}_\\star)$ for the corresponding test case, expressed in $\\text{N}/\\text{m}$. The output must be a comma-separated list enclosed in square brackets, for example, $[d_1,d_2,d_3,d_4]$, where each $d_i$ is a floating-point number.", "solution": "We begin from the definition of friction force as $F_t = \\mu N$, where $\\mu$ is an effective friction coefficient determined by surface topography, material response, and loading, and $N$ is the applied normal load. For a sinusoidal surface $z(x) = A \\sin(2\\pi x/L)$, it is well established in nanomechanics and surface science that increased amplitude $A$ and reduced wavelength $L$ typically increase frictional dissipation through enhanced plowing or additional asperity interactions. This motivates a dimensionally consistent synthetic generator $F_{\\text{true}}(A,L;N) = N \\left[ \\mu_0 + \\alpha \\frac{(A/L)^2}{1 + (A/L_c)^2} + \\beta \\frac{A}{A + A_0}\\frac{L_{\\mathrm{ref}}}{L} \\right]$, where each term in brackets is dimensionless and $N$ carries units of force. We use this generator to create training data only; predictions for the test cases are made by a learned Machine Learning surrogate.\n\nTo approximate the unknown mapping $(A, L) \\mapsto F$ from data, we use a Gaussian Process (GP), a Bayesian nonparametric model that places a prior over functions. Under a zero-mean GP prior with an Automatic Relevance Determination (ARD) squared-exponential kernel,\n$$\nk(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2}\\sum_{j=1}^{2}\\frac{(x_j - x_j')^2}{\\ell_j^2}\\right),\n$$\nand independent Gaussian observation noise of variance $\\sigma_n^2$, the joint distribution of training outputs $\\mathbf{y}$ and the function value at a test input $\\mathbf{x}_\\star$ is multivariate Gaussian. Conditioning yields the posterior mean (which we will use as the point prediction)\n$$\nm(\\mathbf{x}_\\star) = \\mathbf{k}_\\star^\\top \\mathbf{\\alpha}, \\quad \\text{where} \\quad \\mathbf{\\alpha} = (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y},\n$$\n$\\mathbf{K}$ is the training covariance matrix with entries $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, and $\\mathbf{k}_\\star$ is the vector with entries $k(\\mathbf{x}_\\star, \\mathbf{x}_i)$. We use a numerically stable linear solve via a Cholesky factorization of $\\mathbf{K} + \\sigma_n^2 \\mathbf{I}$ to compute $\\mathbf{\\alpha}$.\n\nWe require the sensitivity of the GP posterior mean prediction with respect to the input, namely $\\nabla_{\\mathbf{x}_\\star} m(\\mathbf{x}_\\star)$. Using the chain rule and linearity of differentiation, and noting that $m(\\mathbf{x}_\\star)$ is a linear combination of kernel evaluations, we obtain\n$$\n\\nabla_{\\mathbf{x}_\\star} m(\\mathbf{x}_\\star) = \\sum_{i} \\alpha_i \\,\\nabla_{\\mathbf{x}_\\star} k(\\mathbf{x}_\\star,\\mathbf{x}_i).\n$$\nFor the ARD squared-exponential kernel, the partial derivative with respect to the $j$-th component of $\\mathbf{x}_\\star$ is\n$$\n\\frac{\\partial}{\\partial x_{\\star,j}} k(\\mathbf{x}_\\star,\\mathbf{x}_i) = k(\\mathbf{x}_\\star,\\mathbf{x}_i)\\left(-\\frac{x_{\\star,j} - x_{i,j}}{\\ell_j^2}\\right).\n$$\nTherefore, the gradient is a weighted sum of vectors whose components are the above expressions for $j = 1$ (amplitude $A$) and $j = 2$ (wavelength $L$).\n\nTo verify the analytic gradient, we compare it against a central finite-difference (FD) approximation applied directly to the GP posterior mean $m(\\cdot)$ (not to the ground truth). Given step sizes $h_A$ and $h_L$, the central finite-difference approximations are\n$$\n\\frac{\\partial m}{\\partial A}(\\mathbf{x}_\\star) \\approx \\frac{m([A_\\star + h_A, L_\\star]) - m([A_\\star - h_A, L_\\star])}{2 h_A}, \\quad\n\\frac{\\partial m}{\\partial L}(\\mathbf{x}_\\star) \\approx \\frac{m([A_\\star, L_\\star + h_L]) - m([A_\\star, L_\\star - h_L])}{2 h_L}.\n$$\nThese approximations have truncation error of order $\\mathcal{O}(h_A^2)$ and $\\mathcal{O}(h_L^2)$ under smoothness, and the squared-exponential kernel is infinitely differentiable, ensuring high-order smoothness of $m(\\cdot)$. We choose $h_A = 5\\times 10^{-13}\\,\\text{m}$ and $h_L = 5\\times 10^{-12}\\,\\text{m}$, which are small relative to the kernel length scales and the ranges of inputs but not so small as to be dominated by floating-point round-off at double precision, especially since we evaluate $m(\\cdot)$ via stable linear algebra.\n\nAlgorithmic procedure implemented by the program:\n- Convert training grids $A \\in \\{2, 5, 10, 15, 18\\}\\,\\text{nm}$ and $L \\in \\{20, 50, 80, 120, 160\\}\\,\\text{nm}$ to meters, form all pairs, and compute $F_{\\text{true}}$ at $N = 10^{-5}\\,\\text{N}$ for each, yielding the training targets $\\mathbf{y}$.\n- Assemble the covariance matrix $\\mathbf{K}$ using the ARD squared-exponential kernel with $\\sigma_f = 2\\times 10^{-6}\\,\\text{N}$, $\\ell_1 = 5\\times 10^{-9}\\,\\text{m}$, $\\ell_2 = 5\\times 10^{-8}\\,\\text{m}$, and add $\\sigma_n^2 \\mathbf{I}$ with $\\sigma_n = 10^{-8}\\,\\text{N}$.\n- Compute $\\mathbf{\\alpha}$ by solving $(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}) \\mathbf{\\alpha} = \\mathbf{y}$ using a Cholesky factorization.\n- For each test input $(A_\\star, L_\\star)$ in the suite, convert from nanometers to meters and compute:\n  - The analytic gradient $\\nabla_{\\mathbf{x}_\\star} m(\\mathbf{x}_\\star)$ by summing $\\alpha_i \\nabla_{\\mathbf{x}_\\star} k(\\mathbf{x}_\\star,\\mathbf{x}_i)$ over training points.\n  - The finite-difference gradient using central differences with steps $h_A$ and $h_L$ applied to $m(\\cdot)$.\n  - The maximum absolute component-wise discrepancy $\\Delta(\\mathbf{x}_\\star)$ in units of $\\text{N}/\\text{m}$.\n- Output the list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$ corresponding to the four test cases.\n\nScientific realism and consistency:\n- The friction generator is built from a baseline coefficient $\\mu_0$ plus dimensionless modifiers depending on $A/L$ and $L$-scaling; the overall force is proportional to $N$, consistent with $F_t = \\mu N$.\n- The Gaussian Process with squared-exponential kernel is appropriate for smooth mappings. The ARD length scales reflect that friction may vary more rapidly with amplitude than with wavelength at the nanoscale, hence $\\ell_1 < \\ell_2$.\n- Units are consistently in SI for computation. The gradient components have units of $\\text{N}/\\text{m}$ since they are derivatives of force with respect to length.\n\nThe finite-difference verification validates that our analytic sensitivity expressions are implemented correctly. For well-chosen step sizes and numerically stable solves, the discrepancies $\\Delta(\\mathbf{x}_\\star)$ should be very small, limited primarily by floating-point round-off and truncation error of order $\\mathcal{O}(h^2)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ground_truth_F(A_m, L_m, N):\n    \"\"\"\n    Synthetic physically plausible friction force generator.\n    Inputs are in SI units: meters for A, L; Newtons for N.\n    Returns force in Newtons.\n    \"\"\"\n    mu0 = 0.2\n    alpha = 0.6\n    beta = 0.05\n    Lc = 50e-9      # 50 nm in meters\n    A0 = 1e-9       # 1 nm in meters\n    Lref = 100e-9   # 100 nm in meters\n\n    term1 = mu0\n    term2 = alpha * ( (A_m / L_m)**2 ) / (1.0 + (A_m / Lc)**2 )\n    term3 = beta * (A_m / (A_m + A0)) * (Lref / L_m)\n    mu_eff = term1 + term2 + term3\n    return N * mu_eff\n\ndef ard_sqexp_kernel(X1, X2, sigma_f, ell):\n    \"\"\"\n    ARD squared-exponential kernel between two sets of inputs.\n    X1: (n1, d), X2: (n2, d)\n    ell: (d,) lengthscales\n    Returns: (n1, n2) covariance matrix\n    \"\"\"\n    # Scale inputs by lengthscales for ARD\n    X1s = X1 / ell\n    X2s = X2 / ell\n    # Squared Euclidean distances\n    # (x - x')^2 summed over dimensions\n    dists_sq = (\n        np.sum(X1s**2, axis=1, keepdims=True)\n        + np.sum(X2s**2, axis=1, keepdims=True).T\n        - 2.0 * (X1s @ X2s.T)\n    )\n    return (sigma_f**2) * np.exp(-0.5 * dists_sq)\n\ndef gp_train_alpha(X_train, y_train, sigma_f, ell, sigma_n, jitter=1e-18):\n    \"\"\"\n    Compute alpha = (K + sigma_n^2 I)^(-1) y using Cholesky.\n    \"\"\"\n    K = ard_sqexp_kernel(X_train, X_train, sigma_f, ell)\n    K[np.diag_indices_from(K)] += sigma_n**2 + jitter\n    # Cholesky factorization\n    L = np.linalg.cholesky(K)\n    # Solve L * z = y\n    z = np.linalg.solve(L, y_train)\n    # Solve L.T * alpha = z\n    alpha = np.linalg.solve(L.T, z)\n    return alpha, L  # Return L to enable predictive variance if needed\n\ndef gp_predict_mean_and_grad(x_star, X_train, alpha, sigma_f, ell):\n    \"\"\"\n    Compute GP posterior mean and its gradient at x_star.\n    x_star: (d,) in SI units (meters)\n    X_train: (n, d)\n    alpha: (n,)\n    Returns: mean (scalar), grad (d,)\n    \"\"\"\n    # Compute kernel vector k_star and mean\n    # We compute kernel via the same ARD mechanism\n    x_star_2d = x_star[None, :]  # shape (1, d)\n    k_star = ard_sqexp_kernel(x_star_2d, X_train, sigma_f, ell).ravel()  # (n,)\n    mean = float(k_star @ alpha)\n\n    # Gradient of kernel w.r.t x_star for ARD SE:\n    # d/dx_j k(x, x_i) = k(x, x_i) * (-(x_j - x_i_j)/ell_j^2)\n    diff = (x_star - X_train)  # (n, d)\n    grad_components = k_star[:, None] * ( - diff / (ell**2) )  # (n, d)\n    grad = grad_components.T @ alpha  # (d,)\n    return mean, grad\n\ndef central_fd_grad(x_star, fun, h):\n    \"\"\"\n    Central finite difference gradient of scalar function fun at x_star.\n    fun: function that maps x -> scalar\n    h: step sizes array (d,)\n    Returns: grad (d,)\n    \"\"\"\n    d = x_star.size\n    grad = np.zeros(d, dtype=float)\n    for j in range(d):\n        e = np.zeros(d, dtype=float)\n        e[j] = 1.0\n        xp = x_star + h[j] * e\n        xm = x_star - h[j] * e\n        fp = fun(xp)\n        fm = fun(xm)\n        grad[j] = (fp - fm) / (2.0 * h[j])\n    return grad\n\ndef solve():\n    # Constants and hyperparameters\n    N_load = 1e-5  # 10 microNewtons in N\n    sigma_f = 2e-6  # Signal std in N\n    ell = np.array([5e-9, 5e-8], dtype=float)  # lengthscales for A and L in meters\n    sigma_n = 1e-8  # Noise std in N\n\n    # Training grid in nanometers, then convert to meters\n    A_nm_list = np.array([2.0, 5.0, 10.0, 15.0, 18.0], dtype=float)\n    L_nm_list = np.array([20.0, 50.0, 80.0, 120.0, 160.0], dtype=float)\n    A_m_vals = A_nm_list * 1e-9\n    L_m_vals = L_nm_list * 1e-9\n\n    # Build training set\n    X_train = []\n    y_train = []\n    for A_m in A_m_vals:\n        for L_m in L_m_vals:\n            X_train.append([A_m, L_m])\n            y_train.append(ground_truth_F(A_m, L_m, N_load))\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n\n    # Train GP (compute alpha and store Cholesky if needed)\n    alpha, L_factor = gp_train_alpha(X_train, y_train, sigma_f, ell, sigma_n, jitter=1e-18)\n\n    # Define prediction function for finite differences (posterior mean only)\n    def gp_mean_fun(x):\n        mean, _ = gp_predict_mean_and_grad(x, X_train, alpha, sigma_f, ell)\n        return mean\n\n    # Test suite: inputs in nm -> convert to meters\n    test_cases_nm = [\n        (8.0, 60.0),\n        (1.5, 200.0),\n        (18.0, 30.0),\n        (2.0, 20.0),\n    ]\n    test_cases = [(A * 1e-9, L * 1e-9) for (A, L) in test_cases_nm]\n\n    # Finite difference step sizes in meters\n    h_vec = np.array([5e-13, 5e-12], dtype=float)  # for A and L respectively\n\n    results = []\n    for (A_m, L_m) in test_cases:\n        x_star = np.array([A_m, L_m], dtype=float)\n        mean, grad_analytic = gp_predict_mean_and_grad(x_star, X_train, alpha, sigma_f, ell)\n        grad_fd = central_fd_grad(x_star, gp_mean_fun, h_vec)\n        discrepancy = np.max(np.abs(grad_analytic - grad_fd))\n        results.append(discrepancy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2777669"}, {"introduction": "A central goal in nanomechanics is to infer fundamental material properties from experimental measurements. This exercise tackles the non-linear inverse problem of estimating adhesion energy from dynamic pull-off force data by combining contact mechanics theory with a Bayesian inference framework [@problem_id:2777678]. Since the resulting posterior distribution is analytically intractable, you will implement Hamiltonian Monte Carlo (HMC), a powerful Markov Chain Monte Carlo algorithm, to explore the parameter space and generate samples from the posterior. This practice provides hands-on experience with an advanced inference method essential for modern computational science.", "problem": "Implement a complete, runnable program that uses Hamiltonian Monte Carlo (HMC) to infer the posterior distribution of the adhesion energy per unit area from noisy pull-off force data under dynamic loading in a nanoscale contact, and computes the posterior predictive mean of pull-off force at new loading rates. Build the derivation and the algorithm from the following fundamental base.\n\nAssume an axisymmetric nanoscale contact between a spherical tip of radius $R$ and a flat substrate, where the quasi-static pull-off force at zero loading rate follows the Johnson–Kendall–Roberts (JKR) prediction. The JKR contact mechanics gives a zero-rate pull-off force proportional to the work of adhesion $W$ per unit area as\n$$\nF_{0}(W) = \\frac{3}{2}\\pi R W .\n$$\nUnder dynamic loading, introduce a thermally activated correction that scales with the natural logarithm of the loading rate $r$ following the Bell–Evans model of dynamic force spectroscopy, yielding an expected pull-off force\n$$\n\\mu(W, r) = F_{0}(W) + \\frac{k_{\\mathrm{B}} T}{x_{\\mathrm{b}}} \\ln\\!\\left(\\frac{r}{r_{\\mathrm{ref}}}\\right) ,\n$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is absolute temperature, $x_{\\mathrm{b}}$ is an effective bond length, and $r_{\\mathrm{ref}}$ is a reference loading rate. Let the observed pull-off forces $F_{i}$ at loading rates $r_{i}$ be modeled as\n$$\nF_{i} \\mid W \\sim \\mathcal{N}\\!\\big(\\mu(W, r_{i}), \\sigma_{n}^{2}\\big) ,\n$$\nwith independent Gaussian noise of known standard deviation $\\sigma_{n}$.\n\nTo enforce positivity of $W$, reparameterize with $\\theta = \\ln W$. Place a Gaussian prior on $\\theta$,\n$$\n\\theta \\sim \\mathcal{N}(m_{\\theta}, s_{\\theta}^{2}) .\n$$\nYour program must:\n- Derive and implement the negative log-posterior $U(\\theta)$ and its gradient $\\nabla_{\\theta} U(\\theta)$ from the above definitions (do not use automatic differentiation).\n- Implement Hamiltonian Monte Carlo with a leapfrog integrator and unit mass to sample from the posterior $p(\\theta \\mid \\{(r_{i}, F_{i})\\})$.\n- Using the posterior samples of $\\theta$, compute the posterior predictive mean force for specified new loading rates $r_{\\star}$ via the expectation over the posterior. Since the observation noise is zero mean, the posterior predictive mean at $r_{\\star}$ is\n$$\n\\mathbb{E}\\left[F_{\\star} \\mid \\{(r_{i}, F_{i})\\}\\right] = \\mathbb{E}_{\\theta \\mid \\text{data}}\\left[\\mu\\!\\left(e^{\\theta}, r_{\\star}\\right)\\right] .\n$$\n\nUse International System of Units (SI) consistently. All forces must be reported in Newtons. Angles are not involved. No percentages are involved. The final numerical outputs must be floats in Newtons.\n\nConstants to use for all test cases:\n- Boltzmann constant $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}\\ \\mathrm{J/K}$.\n- Temperature $T = 298\\ \\mathrm{K}$.\n- Effective bond length $x_{\\mathrm{b}} = 1.0\\times 10^{-10}\\ \\mathrm{m}$.\n- Reference loading rate $r_{\\mathrm{ref}} = 1.0\\ \\mathrm{N/s}$.\n- Tip radius $R = 3.0\\times 10^{-8}\\ \\mathrm{m}$.\n- Prior mean and standard deviation for $\\theta$: $m_{\\theta} = \\ln(0.25)$ and $s_{\\theta} = 0.5$.\n- HMC hyperparameters: step size $\\epsilon = 0.01$, number of leapfrog steps $L = 25$, burn-in draws $N_{\\mathrm{burn}} = 1000$, posterior draws $N_{\\mathrm{samples}} = 3000$, unit mass.\n\nData generation protocol for each test case:\n- For each case, generate synthetic observed forces by first computing the noise-free mean $\\mu(W_{\\mathrm{true}}, r_{i})$ and then adding independent Gaussian noise with standard deviation $\\sigma_{n}$ using the specified random seed for data generation. Use a fixed random number generator seed as provided for reproducibility.\n\nPosterior sampling protocol:\n- Initialize HMC at $\\theta_{0} = m_{\\theta}$.\n- Use the specified HMC random seed for sampling the momentum variables and accept/reject steps.\n\nPosterior prediction protocol:\n- For each test case, compute the posterior predictive mean at three new loading rates $r_{\\star} \\in \\{10^{0}, 10^{3}, 10^{6}\\}\\ \\mathrm{N/s}$ using the posterior samples of $\\theta$.\n\nTest suite:\n- Case $1$:\n  - True adhesion energy $W_{\\mathrm{true}} = 0.20\\ \\mathrm{J/m^{2}}$.\n  - Noise standard deviation $\\sigma_{n} = 2.0\\times 10^{-10}\\ \\mathrm{N}$.\n  - Loading rates $r_{i} \\in \\{10^{-1}, 10^{0}, 10^{1}, 10^{2}, 10^{3}, 10^{4}, 10^{5}, 10^{6}\\}\\ \\mathrm{N/s}$.\n  - Data-generation seed $s_{\\mathrm{data}} = 12345$.\n  - HMC seed $s_{\\mathrm{hmc}} = 24680$.\n- Case $2$:\n  - True adhesion energy $W_{\\mathrm{true}} = 0.15\\ \\mathrm{J/m^{2}}$.\n  - Noise standard deviation $\\sigma_{n} = 5.0\\times 10^{-10}\\ \\mathrm{N}$.\n  - Loading rates $r_{i} \\in \\{10^{0}, 10^{6}\\}\\ \\mathrm{N/s}$.\n  - Data-generation seed $s_{\\mathrm{data}} = 54321$.\n  - HMC seed $s_{\\mathrm{hmc}} = 13579$.\n- Case $3$:\n  - True adhesion energy $W_{\\mathrm{true}} = 0.35\\ \\mathrm{J/m^{2}}$.\n  - Noise standard deviation $\\sigma_{n} = 3.0\\times 10^{-10}\\ \\mathrm{N}$.\n  - Loading rates $r_{i} \\in \\{10^{-2}, 10^{-1}, 10^{0}, 10^{3}, 10^{5}, 10^{7}\\}\\ \\mathrm{N/s}$.\n  - Data-generation seed $s_{\\mathrm{data}} = 2023$.\n  - HMC seed $s_{\\mathrm{hmc}} = 97531$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a list of lists. For each test case in the order $\\text{Case }1$, $\\text{Case }2$, $\\text{Case }3$, output a list of three floats equal to the posterior predictive means (in Newtons) at new loading rates $r_{\\star} \\in \\{10^{0}, 10^{3}, 10^{6}\\}\\ \\mathrm{N/s}$, in that order. Aggregate the three per-case lists into an outer list. For example, a syntactically correct output looks like\n$$\n[[f_{11},f_{12},f_{13}],[f_{21},f_{22},f_{23}],[f_{31},f_{32},f_{33}]],\n$$\nwhere each $f_{jk}$ is a floating-point number in Newtons. The printed line must contain only this bracketed list with comma separators.", "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in established principles of contact mechanics (Johnson–Kendall–Roberts theory) and dynamic force spectroscopy (Bell–Evans model). The problem is mathematically well-posed, providing a complete and consistent Bayesian inference framework. All parameters, constants, and procedural steps are specified, making the problem unambiguous and self-contained. The task of implementing a Hamiltonian Monte Carlo sampler to infer physical parameters from synthetic data is a standard and non-trivial problem in computational physics and statistics. I will now proceed with the solution.\n\nThe core of the problem is to infer the posterior distribution of the work of adhesion, $W$, from noisy pull-off force measurements, and then use this posterior to predict forces at new loading rates. To ensure positivity, we work with the reparameterized variable $\\theta = \\ln W$.\n\n### 1. Bayesian Model Formulation\n\nThe posterior distribution for $\\theta$ given the observed data $D = \\{(r_i, F_i)\\}_{i=1}^N$ is given by Bayes' theorem:\n$$\np(\\theta \\mid D) \\propto p(D \\mid \\theta) p(\\theta)\n$$\nwhere $p(D \\mid \\theta)$ is the likelihood and $p(\\theta)$ is the prior.\n\n**Likelihood**: The observed forces $F_i$ are modeled as independent Gaussian random variables centered around the mean force $\\mu(W, r_i)$ with a known variance $\\sigma_n^2$. With the substitution $W = e^\\theta$, the likelihood for a single data point $(r_i, F_i)$ is:\n$$\np(F_i \\mid \\theta) = \\mathcal{N}\\big(F_i; \\mu(e^\\theta, r_i), \\sigma_n^2\\big) = \\frac{1}{\\sqrt{2\\pi\\sigma_n^2}} \\exp\\left(-\\frac{(F_i - \\mu(e^\\theta, r_i))^2}{2\\sigma_n^2}\\right)\n$$\nThe mean force $\\mu(e^\\theta, r_i)$ is given by:\n$$\n\\mu(e^\\theta, r_i) = \\frac{3}{2}\\pi R e^\\theta + \\frac{k_{\\mathrm{B}} T}{x_{\\mathrm{b}}} \\ln\\left(\\frac{r_i}{r_{\\mathrm{ref}}}\\right)\n$$\nSince the observations are independent, the total likelihood is the product of individual likelihoods:\n$$\np(D \\mid \\theta) = \\prod_{i=1}^N p(F_i \\mid \\theta)\n$$\n\n**Prior**: A Gaussian prior is placed on $\\theta$:\n$$\np(\\theta) = \\mathcal{N}(\\theta; m_\\theta, s_\\theta^2) = \\frac{1}{\\sqrt{2\\pi s_\\theta^2}} \\exp\\left(-\\frac{(\\theta - m_\\theta)^2}{2s_\\theta^2}\\right)\n$$\n\n**Posterior**: The log-posterior is the sum of the log-likelihood and the log-prior. Dropping constant terms that are independent of $\\theta$:\n$$\n\\ln p(\\theta \\mid D) \\propto -\\sum_{i=1}^N \\frac{(F_i - \\mu(e^\\theta, r_i))^2}{2\\sigma_n^2} - \\frac{(\\theta - m_\\theta)^2}{2s_\\theta^2}\n$$\n\n### 2. Potential Energy and its Gradient for HMC\n\nHamiltonian Monte Carlo (HMC) requires the definition of a potential energy function $U(\\theta)$, which is the negative log-posterior.\n$$\nU(\\theta) = - \\ln p(\\theta \\mid D) = \\frac{1}{2\\sigma_n^2} \\sum_{i=1}^N (F_i - \\mu(e^\\theta, r_i))^2 + \\frac{1}{2s_\\theta^2} (\\theta - m_\\theta)^2\n$$\nThe HMC algorithm also requires the gradient of the potential energy with respect to the parameter $\\theta$, denoted $\\nabla_\\theta U(\\theta) = \\frac{dU}{d\\theta}$. Applying the chain rule:\n$$\n\\frac{dU}{d\\theta} = \\frac{1}{2\\sigma_n^2} \\sum_{i=1}^N 2(F_i - \\mu(e^\\theta, r_i)) \\cdot \\left(-\\frac{d\\mu(e^\\theta, r_i)}{d\\theta}\\right) + \\frac{1}{2s_\\theta^2} \\cdot 2(\\theta - m_\\theta)\n$$\nWe need the derivative of $\\mu(e^\\theta, r_i)$ with respect to $\\theta$:\n$$\n\\frac{d\\mu(e^\\theta, r_i)}{d\\theta} = \\frac{d}{d\\theta} \\left( \\frac{3}{2}\\pi R e^\\theta + \\frac{k_{\\mathrm{B}} T}{x_{\\mathrm{b}}} \\ln\\left(\\frac{r_i}{r_{\\mathrm{ref}}}\\right) \\right) = \\frac{3}{2}\\pi R e^\\theta\n$$\nSubstituting this back into the gradient expression yields:\n$$\n\\frac{dU}{d\\theta} = -\\frac{1}{\\sigma_n^2} \\sum_{i=1}^N (F_i - \\mu(e^\\theta, r_i)) \\left(\\frac{3}{2}\\pi R e^\\theta\\right) + \\frac{\\theta - m_\\theta}{s_\\theta^2}\n$$\nThis can be rewritten as:\n$$\n\\frac{dU}{d\\theta} = \\frac{1}{\\sigma_n^2} \\left[ \\sum_{i=1}^N (\\mu(e^\\theta, r_i) - F_i) \\right] \\left(\\frac{3}{2}\\pi R e^\\theta\\right) + \\frac{\\theta - m_\\theta}{s_\\theta^2}\n$$\nThese expressions for $U(\\theta)$ and $\\frac{dU}{d\\theta}$ are implemented numerically to run the HMC sampler.\n\n### 3. Hamiltonian Monte Carlo Algorithm\n\nHMC is a Markov Chain Monte Carlo method that uses Hamiltonian dynamics to propose moves through the parameter space. The state is described by the position $\\theta$ and a fictitious momentum $p$. The Hamiltonian is $H(\\theta, p) = U(\\theta) + K(p)$, where $K(p) = p^2/(2m)$ is the kinetic energy. For this problem, we use unit mass, $m=1$.\n\nThe algorithm to generate one sample is:\n1.  **Momentum Sampling**: Draw a new momentum value $p$ from a standard normal distribution, $p \\sim \\mathcal{N}(0, 1)$.\n2.  **Leapfrog Integration**: Starting from the current position $\\theta_{\\text{curr}}$ and the new momentum $p$, simulate the dynamics for $L$ steps with a step size $\\epsilon$. The leapfrog integrator discretizes Hamilton's equations:\n    a. Half-step for momentum: $p \\leftarrow p - (\\epsilon/2) \\nabla_\\theta U(\\theta)$\n    b. Full-step for position: $\\theta \\leftarrow \\theta + \\epsilon \\cdot p$\n    c. Repeat $L-1$ times:\n        i. Full-step for momentum: $p \\leftarrow p - \\epsilon \\nabla_\\theta U(\\theta)$\n        ii. Full-step for position: $\\theta \\leftarrow \\theta + \\epsilon \\cdot p$\n    d. Final half-step for momentum: $p \\leftarrow p - (\\epsilon/2) \\nabla_\\theta U(\\theta)$\n    This sequence results in a proposal state $(\\theta_{\\text{prop}}, p_{\\text{prop}})$.\n3.  **Metropolis-Hastings Acceptance**: The proposal is accepted or rejected based on the change in the Hamiltonian to ensure detailed balance. The acceptance probability $\\alpha$ is:\n    $$\n    \\alpha = \\min\\left(1, \\exp\\left(H(\\theta_{\\text{curr}}, p) - H(\\theta_{\\text{prop}}, p_{\\text{prop}})\\right)\\right)\n    $$\n    A random number $u \\in [0, 1]$ is drawn. If $u < \\alpha$, the new state is $\\theta_{\\text{next}} = \\theta_{\\text{prop}}$; otherwise, the state remains $\\theta_{\\text{next}} = \\theta_{\\text{curr}}$.\n\nThis process is repeated for $N_{\\text{burn}} + N_{\\text{samples}}$ iterations. The first $N_{\\text{burn}}$ samples (the burn-in period) are discarded to allow the chain to converge to the stationary distribution, and the subsequent $N_{\\text{samples}}$ are retained as samples from the posterior $p(\\theta \\mid D)$.\n\n### 4. Posterior Predictive Inference\n\nOnce we have the posterior samples $\\{\\theta^{(j)}\\}_{j=1}^{N_{\\text{samples}}}$, we can compute the posterior predictive distribution for a new observation $F_\\star$ at a new loading rate $r_\\star$. The problem asks for the posterior predictive mean, $\\mathbb{E}[F_\\star \\mid D]$. As the observation noise model $\\mathcal{N}$ has zero mean, this expectation simplifies to the posterior expectation of the mean function $\\mu$:\n$$\n\\mathbb{E}[F_\\star \\mid D] = \\mathbb{E}_{\\theta \\mid D}\\left[\\mu(e^\\theta, r_\\star)\\right]\n$$\nThis expectation is approximated using a Monte Carlo average over the posterior samples:\n$$\n\\mathbb{E}[F_\\star \\mid D] \\approx \\frac{1}{N_{\\text{samples}}} \\sum_{j=1}^{N_{\\text{samples}}} \\mu(e^{\\theta^{(j)}}, r_\\star)\n$$\nwhere\n$$\n\\mu(e^{\\theta^{(j)}}, r_\\star) = \\frac{3}{2}\\pi R e^{\\theta^{(j)}} + \\frac{k_{\\mathrm{B}} T}{x_{\\mathrm{b}}} \\ln\\left(\\frac{r_\\star}{r_{\\mathrm{ref}}}\\right)\n$$\nThis calculation is performed for each specified new loading rate $r_\\star$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Global constants (SI units)\n    K_B = 1.380649e-23  # Boltzmann constant [J/K]\n    T = 298.0  # Temperature [K]\n    X_B = 1.0e-10  # Effective bond length [m]\n    R_REF = 1.0  # Reference loading rate [N/s]\n    R_TIP = 3.0e-8  # Tip radius [m]\n    M_THETA = np.log(0.25)  # Prior mean for theta\n    S_THETA = 0.5  # Prior std dev for theta\n    \n    # HMC hyperparameters\n    EPSILON = 0.01\n    L_STEPS = 25\n    N_BURN = 1000\n    N_SAMPLES = 3000\n\n    # Calculated constants\n    THERMAL_FORCE_CONST = K_B * T / X_B\n    JKR_CONST = 1.5 * np.pi * R_TIP\n\n    test_cases = [\n        {\n            \"W_true\": 0.20, \"sigma_n\": 2.0e-10,\n            \"r_i\": np.array([1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]),\n            \"s_data\": 12345, \"s_hmc\": 24680,\n        },\n        {\n            \"W_true\": 0.15, \"sigma_n\": 5.0e-10,\n            \"r_i\": np.array([1e0, 1e6]),\n            \"s_data\": 54321, \"s_hmc\": 13579,\n        },\n        {\n            \"W_true\": 0.35, \"sigma_n\": 3.0e-10,\n            \"r_i\": np.array([1e-2, 1e-1, 1e0, 1e3, 1e5, 1e7]),\n            \"s_data\": 2023, \"s_hmc\": 97531,\n        },\n    ]\n\n    r_predict = np.array([1e0, 1e3, 1e6])\n    all_results = []\n\n    for case in test_cases:\n        # Generate synthetic data\n        rng_data = np.random.default_rng(case[\"s_data\"])\n        \n        def mean_force_model(W, r):\n            return JKR_CONST * W + THERMAL_FORCE_CONST * np.log(r / R_REF)\n\n        mu_true = mean_force_model(case[\"W_true\"], case[\"r_i\"])\n        noise = rng_data.normal(0, case[\"sigma_n\"], size=mu_true.shape)\n        F_obs = mu_true + noise\n\n        # Define potential energy and its gradient\n        F_obs_data, r_obs_data, sigma_n_sq = F_obs, case[\"r_i\"], case[\"sigma_n\"]**2\n        s_theta_sq = S_THETA**2\n        \n        def U_potential(theta):\n            W = np.exp(theta)\n            mu = mean_force_model(W, r_obs_data)\n            log_likelihood_term = np.sum((F_obs_data - mu)**2) / (2.0 * sigma_n_sq)\n            log_prior_term = (theta - M_THETA)**2 / (2.0 * s_theta_sq)\n            return log_likelihood_term + log_prior_term\n\n        def grad_U_potential(theta):\n            W = np.exp(theta)\n            mu = mean_force_model(W, r_obs_data)\n            d_mu_d_theta = JKR_CONST * W\n            \n            grad_log_likelihood = np.sum(mu - F_obs_data) * d_mu_d_theta / sigma_n_sq\n            grad_log_prior = (theta - M_THETA) / s_theta_sq\n            return grad_log_likelihood + grad_log_prior\n\n        # Run HMC\n        rng_hmc = np.random.default_rng(case[\"s_hmc\"])\n        theta_current = M_THETA\n        samples = []\n\n        for i in range(N_BURN + N_SAMPLES):\n            p_current = rng_hmc.normal(0, 1)\n            q_proposal, p_proposal = theta_current, p_current\n\n            # Leapfrog integration\n            p_proposal -= 0.5 * EPSILON * grad_U_potential(q_proposal)\n            for _ in range(L_STEPS - 1):\n                q_proposal += EPSILON * p_proposal\n                p_proposal -= EPSILON * grad_U_potential(q_proposal)\n            q_proposal += EPSILON * p_proposal\n            p_proposal -= 0.5 * EPSILON * grad_U_potential(q_proposal)\n            \n            p_proposal = -p_proposal\n\n            # Metropolis-Hastings acceptance step\n            H_current = U_potential(theta_current) + 0.5 * p_current**2\n            H_proposal = U_potential(q_proposal) + 0.5 * p_proposal**2\n            \n            alpha = min(1.0, np.exp(H_current - H_proposal))\n\n            if rng_hmc.random() < alpha:\n                theta_current = q_proposal\n\n            if i >= N_BURN:\n                samples.append(theta_current)\n        \n        theta_samples = np.array(samples)\n\n        # Posterior predictive mean\n        W_samples = np.exp(theta_samples)\n        case_predictive_means = []\n        for r_p in r_predict:\n            predictive_forces = mean_force_model(W_samples, r_p)\n            case_predictive_means.append(np.mean(predictive_forces))\n        \n        all_results.append(case_predictive_means)\n\n    print(str(all_results).replace(\" \", \"\"))\n\n\nsolve()\n```", "id": "2777678"}]}