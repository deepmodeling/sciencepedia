{"hands_on_practices": [{"introduction": "This first exercise provides a rigorous workout in the fundamental mechanics of polar decomposition. By deriving the closed-form expressions for the rotation tensor $\\mathbf{R}$ and the right stretch tensor $\\mathbf{U}$ for the case of simple shear, you will gain hands-on practice with the core definitions and algebraic manipulations that form the bedrock of finite deformation kinematics. Mastering this canonical example is an essential step toward tackling more complex deformation states [@problem_id:2886430].", "problem": "A homogeneous simple shear deformation in three dimensions is described by the deformation gradient $\\,\\mathbf{F}=\\mathbf{I}+\\gamma\\,\\mathbf{e}_{1}\\otimes\\mathbf{e}_{2}\\,$, where $\\,\\gamma\\in\\mathbb{R}\\,$ is a constant shear parameter and $\\{\\mathbf{e}_{1},\\mathbf{e}_{2},\\mathbf{e}_{3}\\}$ is an orthonormal basis. Using only fundamental definitions from finite deformation kinematics, perform the right polar decomposition of $\\,\\mathbf{F}\\,$ by determining a proper orthogonal tensor $\\,\\mathbf{R}\\in\\mathrm{SO}(3)\\,$ and a symmetric positive-definite right stretch tensor $\\,\\mathbf{U}\\,$ such that $\\,\\mathbf{F}=\\mathbf{R}\\mathbf{U}\\,$. Your derivation must start from the definitions $\\,\\mathbf{C}=\\mathbf{F}^{\\mathsf{T}}\\mathbf{F}\\,$ and $\\,\\mathbf{U}=\\mathbf{C}^{1/2}\\,$ (the unique symmetric positive-definite square root of $\\,\\mathbf{C}\\,$), followed by $\\,\\mathbf{R}=\\mathbf{F}\\mathbf{U}^{-1}\\,$, and should establish closed-form expressions for $\\,\\mathbf{R}\\,$ and $\\,\\mathbf{U}\\,$ in terms of $\\,\\gamma\\,$. Verify that $\\,\\mathbf{U}^{2}=\\mathbf{C}\\,$ and $\\,\\mathbf{R}^{\\mathsf{T}}\\mathbf{R}=\\mathbf{I}\\,$.\n\nFinally, parameterize $\\,\\mathbf{R}\\,$ as a planar rotation about $\\,\\mathbf{e}_{3}\\,$ with signed angle $\\,\\theta(\\gamma)\\,$, namely $\\,\\mathbf{R}=\\mathbf{Q}(\\theta)\\,$ where $\\,\\mathbf{Q}(\\theta)\\,$ restricts in the $\\,\\mathbf{e}_{1}\\text{--}\\mathbf{e}_{2}\\,$ plane to a standard rotation matrix and $\\,\\mathbf{Q}_{33}=1\\,$, and extract an explicit closed-form expression for the angle $\\,\\theta(\\gamma)\\,$.\n\nAs your final submitted answer, report only the function $\\,\\theta(\\gamma)\\,$ in radians as a single closed-form analytic expression (no numerical rounding is required).", "solution": "The given deformation gradient is $\\mathbf{F}=\\mathbf{I}+\\gamma\\,\\mathbf{e}_{1}\\otimes\\mathbf{e}_{2}$, where $\\gamma$ is the shear parameter and $\\{\\mathbf{e}_{1},\\mathbf{e}_{2},\\mathbf{e}_{3}\\}$ is an orthonormal basis. In this basis, the matrix representation of $\\mathbf{F}$ is:\n$$\n[F] = \\begin{pmatrix} 1 & \\gamma & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe right polar decomposition is $\\mathbf{F}=\\mathbf{R}\\mathbf{U}$, where $\\mathbf{R}$ is a proper orthogonal tensor and $\\mathbf{U}$ is a symmetric positive-definite right stretch tensor. The derivation begins with the computation of the right Cauchy-Green deformation tensor $\\mathbf{C}=\\mathbf{F}^{\\mathsf{T}}\\mathbf{F}$.\n$$\n[F^{\\mathsf{T}}] = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\gamma & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\n[C] = [F^{\\mathsf{T}}][F] = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\gamma & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\gamma & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\gamma & 0 \\\\ \\gamma & \\gamma^2 + 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nNext, we find the right stretch tensor $\\mathbf{U}=\\mathbf{C}^{1/2}$. Since $\\mathbf{C}$ is symmetric and positive-definite, $\\mathbf{U}$ is unique. We seek a symmetric matrix $[U]$ such that $[U]^2=[C]$. Based on the structure of $[C]$, we posit that $[U]$ has the form:\n$$\n[U] = \\begin{pmatrix} a & b & 0 \\\\ b & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nwhere the component $U_{33}=1$ because $C_{33}=1$. Squaring this matrix yields:\n$$\n[U]^2 = \\begin{pmatrix} a^2+b^2 & b(a+c) & 0 \\\\ b(a+c) & b^2+c^2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\gamma & 0 \\\\ \\gamma & \\gamma^2 + 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThis provides a system of algebraic equations for $a, b, c$:\n1. $a^2+b^2=1$\n2. $b(a+c)=\\gamma$\n3. $b^2+c^2=\\gamma^2+1$\n\nFor $\\mathbf{U}$ to be positive-definite, its principal minors must be positive. This requires $a>0$ and $ac-b^2>0$. From (2), if $\\gamma \\ne 0$, then $b \\ne 0$. We can assume $b$ has the same sign as $\\gamma$.\nFrom (1) and (3), we find $c^2-a^2 = \\gamma^2$. From (2), $a+c = \\gamma/b$. Combining these gives $c-a = \\gamma b$. Solving for $a$ and $c$ gives $a=\\frac{1}{2}(\\frac{\\gamma}{b}-\\gamma b)$ and $c=\\frac{1}{2}(\\frac{\\gamma}{b}+\\gamma b)$.\nSubstituting $a$ into (1) leads to a quadratic equation for $b^2$: $(\\gamma^2+4)b^4 - 2(\\gamma^2+2)b^2 + \\gamma^2 = 0$.\nThe solutions for $b^2$ are $b^2 = \\frac{\\gamma^2}{\\gamma^2+4}$ and $b^2=1$. The solution $b^2=1$ leads to a matrix for $\\mathbf{U}$ which is not positive-definite, so it is discarded.\nWe take $b^2=\\frac{\\gamma^2}{\\gamma^2+4}$. We choose $b=\\frac{\\gamma}{\\sqrt{\\gamma^2+4}}$ so that $b$ has the same sign as $\\gamma$. This yields:\n$a = \\frac{2}{\\sqrt{\\gamma^2+4}}$\n$c = \\frac{\\gamma^2+2}{\\sqrt{\\gamma^2+4}}$\nThese values satisfy all three equations and the positive-definiteness conditions. Thus, the matrix for $\\mathbf{U}$ is:\n$$\n[U] = \\frac{1}{\\sqrt{\\gamma^2+4}} \\begin{pmatrix} 2 & \\gamma & 0 \\\\ \\gamma & \\gamma^2+2 & 0 \\\\ 0 & 0 & \\sqrt{\\gamma^2+4} \\end{pmatrix}\n$$\nTo find the rotation tensor $\\mathbf{R}$, we compute $\\mathbf{R}=\\mathbf{F}\\mathbf{U}^{-1}$. First, we find $\\mathbf{U}^{-1}$. The determinant of the upper $2\\times2$ block of $[U]$ is $1$. The inverse is found by standard formula:\n$$\n[U^{-1}] = \\frac{1}{\\sqrt{\\gamma^2+4}} \\begin{pmatrix} \\gamma^2+2 & -\\gamma & 0 \\\\ -\\gamma & 2 & 0 \\\\ 0 & 0 & \\sqrt{\\gamma^2+4} \\end{pmatrix}\n$$\nNow we compute $[R]=[F][U^{-1}]$:\n$$\n[R] = \\begin{pmatrix} 1 & \\gamma & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{\\gamma^2+4}} \\begin{pmatrix} \\gamma^2+2 & -\\gamma & 0 \\\\ -\\gamma & 2 & 0 \\\\ 0 & 0 & \\sqrt{\\gamma^2+4} \\end{pmatrix}\n$$\n$$\n[R] = \\frac{1}{\\sqrt{\\gamma^2+4}} \\begin{pmatrix} (1)(\\gamma^2+2)+(\\gamma)(-\\gamma) & (1)(-\\gamma)+(\\gamma)(2) & 0 \\\\ (0)(\\gamma^2+2)+(1)(-\\gamma) & (0)(-\\gamma)+(1)(2) & 0 \\\\ 0 & 0 & \\sqrt{\\gamma^2+4} \\end{pmatrix}\n$$\n$$\n[R] = \\frac{1}{\\sqrt{\\gamma^2+4}} \\begin{pmatrix} 2 & \\gamma & 0 \\\\ -\\gamma & 2 & 0 \\\\ 0 & 0 & \\sqrt{\\gamma^2+4} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{\\sqrt{\\gamma^2+4}} & \\frac{\\gamma}{\\sqrt{\\gamma^2+4}} & 0 \\\\ \\frac{-\\gamma}{\\sqrt{\\gamma^2+4}} & \\frac{2}{\\sqrt{\\gamma^2+4}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nWe verify that $\\mathbf{R}$ is a proper orthogonal tensor.\n$$\n[R^{\\mathsf{T}}R] = \\begin{pmatrix} \\frac{2}{k} & \\frac{-\\gamma}{k} & 0 \\\\ \\frac{\\gamma}{k} & \\frac{2}{k} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{k} & \\frac{\\gamma}{k} & 0 \\\\ \\frac{-\\gamma}{k} & \\frac{2}{k} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\frac{1}{k^2}\\begin{pmatrix} 4+\\gamma^2 & 2\\gamma-2\\gamma & 0 \\\\ 2\\gamma-2\\gamma & \\gamma^2+4 & 0 \\\\ 0 & 0 & k^2 \\end{pmatrix}\n$$\nwhere $k=\\sqrt{\\gamma^2+4}$. Since $k^2=\\gamma^2+4$, this simplifies to the identity matrix $\\mathbf{I}$.\nThe determinant is $\\det[R] = (\\frac{2}{k})^2 - (\\frac{\\gamma}{k})(\\frac{-\\gamma}{k}) = \\frac{4+\\gamma^2}{k^2}=1$. Thus $\\mathbf{R} \\in \\mathrm{SO}(3)$.\n\nFinally, we parameterize $\\mathbf{R}$ as a planar rotation in the $\\mathbf{e}_1$-$\\mathbf{e}_2$ plane. A standard rotation matrix for a counter-clockwise rotation of a vector by a signed angle $\\theta$ is:\n$$\n[Q(\\theta)] = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nComparing $[R]$ with $[Q(\\theta)]$, we identify the components:\n$$\n\\cos\\theta = \\frac{2}{\\sqrt{\\gamma^2+4}}\n$$\n$$\n-\\sin\\theta = \\frac{\\gamma}{\\sqrt{\\gamma^2+4}} \\quad \\implies \\quad \\sin\\theta = -\\frac{\\gamma}{\\sqrt{\\gamma^2+4}}\n$$\nThe tangent of the angle $\\theta$ is the ratio of these components:\n$$\n\\tan\\theta = \\frac{\\sin\\theta}{\\cos\\theta} = \\frac{-\\gamma/\\sqrt{\\gamma^2+4}}{2/\\sqrt{\\gamma^2+4}} = -\\frac{\\gamma}{2}\n$$\nThe angle $\\theta(\\gamma)$ is therefore given by the arctangent function. The range of the standard $\\arctan$ function, $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$, is appropriate here since $\\cos\\theta > 0$ for all $\\gamma$.\n$$\n\\theta(\\gamma) = \\arctan\\left(-\\frac{\\gamma}{2}\\right)\n$$\nThis expression provides the required signed angle of rotation as a function of the shear parameter $\\gamma$.", "answer": "$$\\boxed{\\arctan\\left(-\\frac{\\gamma}{2}\\right)}$$", "id": "2886430"}, {"introduction": "Having practiced the direct calculation of a polar decomposition, we now explore a deeper conceptual question: do finite stretch and rotation operations commute? This practice investigates a deformation constructed by a pure stretch followed by a rigid rotation, a scenario that directly probes the non-commutative nature of finite deformation. You will not only determine the correct polar decomposition but also quantify the \"error\" introduced by switching the order of operations, providing a clear insight into why $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$ is distinct from $\\mathbf{F} = \\mathbf{V}\\mathbf{R}$ in general [@problem_id:2886415].", "problem": "Consider a homogeneous deformation mapping defined by a finite stretch followed by a finite rigid-body rotation. The deformation gradient is prescribed as the product $\\mathbf{F} = \\mathbf{Q}\\,\\mathbf{D}$, where:\n- $\\mathbf{Q} \\in \\mathbb{R}^{3 \\times 3}$ is a proper orthogonal tensor, $\\mathbf{Q}^{\\mathsf{T}}\\mathbf{Q} = \\mathbf{I}$ and $\\det \\mathbf{Q} = 1$, representing a right-handed rotation by angle $\\theta$ (in radians) about the fixed unit vector $\\mathbf{e}_3$,\n- $\\mathbf{D} = \\operatorname{diag}(\\lambda_1,\\lambda_2,\\lambda_3)$ is a symmetric positive-definite (SPD) stretch tensor with principal stretches $\\lambda_i > 0$.\n\nYou may assume the standard orthonormal basis $\\{\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3\\}$ and the explicit rotation\n$$\n\\mathbf{Q} = \\begin{pmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\phantom{-}\\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the definitions of the right Cauchyâ€“Green tensor $\\mathbf{C} = \\mathbf{F}^{\\mathsf{T}}\\mathbf{F}$ and the right stretch tensor $\\mathbf{U}$ as the unique SPD square root of $\\mathbf{C}$, extract the right stretch $\\mathbf{U}$ and the rotation $\\mathbf{R}$ in the right polar decomposition, where $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$ and $\\mathbf{R}$ is proper orthogonal.\n2. Using only first principles and the above definitions, analyze whether the rotation and the stretch generally commute. To quantify the departure from commutativity for this $\\mathbf{Q}$ and $\\mathbf{D}$, define the scalar measure\n$$\n\\Gamma \\equiv \\|\\,[\\mathbf{Q},\\mathbf{U}]\\,\\|_F^{2} = \\|\\mathbf{Q}\\mathbf{U} - \\mathbf{U}\\mathbf{Q}\\|_F^{2},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm. Derive $\\Gamma$ explicitly as a closed-form function of $\\lambda_1,\\lambda_2,\\lambda_3$ and $\\theta$.\n\nYour final submitted answer must be the single closed-form analytical expression for $\\Gamma$. Do not include units. No numerical evaluation is required.", "solution": "**Task 1: Determination of the Right Polar Decomposition**\n\nThe right polar decomposition of the deformation gradient $\\mathbf{F}$ is given by $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$, where $\\mathbf{R}$ is a proper orthogonal tensor and $\\mathbf{U}$ is a symmetric positive-definite (SPD) tensor, the right stretch tensor.\n\nThe right stretch tensor $\\mathbf{U}$ is determined from the right Cauchy-Green tensor $\\mathbf{C}$ by the relation $\\mathbf{U} = \\sqrt{\\mathbf{C}}$. The tensor $\\mathbf{C}$ is defined as $\\mathbf{C} = \\mathbf{F}^{\\mathsf{T}}\\mathbf{F}$.\n\nWe are given $\\mathbf{F} = \\mathbf{Q}\\mathbf{D}$. We compute $\\mathbf{C}$ by substituting this expression:\n$$\n\\mathbf{C} = (\\mathbf{Q}\\mathbf{D})^{\\mathsf{T}}(\\mathbf{Q}\\mathbf{D})\n$$\nUsing the property of the transpose of a matrix product, $(\\mathbf{A}\\mathbf{B})^{\\mathsf{T}} = \\mathbf{B}^{\\mathsf{T}}\\mathbf{A}^{\\mathsf{T}}$, we have:\n$$\n\\mathbf{C} = (\\mathbf{D}^{\\mathsf{T}}\\mathbf{Q}^{\\mathsf{T}})(\\mathbf{Q}\\mathbf{D})\n$$\nBy associativity of matrix multiplication:\n$$\n\\mathbf{C} = \\mathbf{D}^{\\mathsf{T}}(\\mathbf{Q}^{\\mathsf{T}}\\mathbf{Q})\\mathbf{D}\n$$\nThe problem states that $\\mathbf{Q}$ is an orthogonal tensor, thus $\\mathbf{Q}^{\\mathsf{T}}\\mathbf{Q} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity tensor. The expression for $\\mathbf{C}$ simplifies to:\n$$\n\\mathbf{C} = \\mathbf{D}^{\\mathsf{T}}\\mathbf{I}\\mathbf{D} = \\mathbf{D}^{\\mathsf{T}}\\mathbf{D}\n$$\nFurthermore, $\\mathbf{D}$ is given as a symmetric tensor, so $\\mathbf{D}^{\\mathsf{T}} = \\mathbf{D}$. This leads to:\n$$\n\\mathbf{C} = \\mathbf{D}\\mathbf{D} = \\mathbf{D}^2\n$$\nThe right stretch tensor $\\mathbf{U}$ is the unique SPD square root of $\\mathbf{C}$. Therefore:\n$$\n\\mathbf{U} = \\sqrt{\\mathbf{C}} = \\sqrt{\\mathbf{D}^2}\n$$\nSince $\\mathbf{D}$ is given as an SPD tensor with positive principal stretches $\\lambda_i > 0$, its unique SPD square root is $\\mathbf{D}$ itself. Hence:\n$$\n\\mathbf{U} = \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix}\n$$\nNow, we find the rotation tensor $\\mathbf{R}$ from the polar decomposition formula $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$.\n$$\n\\mathbf{R} = \\mathbf{F}\\mathbf{U}^{-1}\n$$\nSubstituting the given $\\mathbf{F} = \\mathbf{Q}\\mathbf{D}$ and the derived $\\mathbf{U} = \\mathbf{D}$:\n$$\n\\mathbf{R} = (\\mathbf{Q}\\mathbf{D})\\mathbf{D}^{-1}\n$$\nSince $\\lambda_i > 0$, $\\mathbf{D}$ is invertible. By associativity:\n$$\n\\mathbf{R} = \\mathbf{Q}(\\mathbf{D}\\mathbf{D}^{-1}) = \\mathbf{Q}\\mathbf{I} = \\mathbf{Q}\n$$\nThus, for the specified deformation, the right polar decomposition is given by $\\mathbf{R} = \\mathbf{Q}$ and $\\mathbf{U} = \\mathbf{D}$.\n\n**Task 2: Derivation of the Non-commutativity Measure $\\Gamma$**\n\nWe must derive the expression for $\\Gamma \\equiv \\|\\mathbf{Q}\\mathbf{U} - \\mathbf{U}\\mathbf{Q}\\|_F^{2}$. Based on the result from Task 1, we have $\\mathbf{U} = \\mathbf{D}$. Therefore, we must compute the commutator of $\\mathbf{Q}$ and $\\mathbf{D}$, denoted as $[\\mathbf{Q}, \\mathbf{D}] = \\mathbf{Q}\\mathbf{D} - \\mathbf{D}\\mathbf{Q}$.\n\nLet us write $c \\equiv \\cos\\theta$ and $s \\equiv \\sin\\theta$. The matrices are:\n$$\n\\mathbf{Q} = \\begin{pmatrix} c & -s & 0 \\\\ s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix}\n$$\nFirst, we compute the product $\\mathbf{Q}\\mathbf{D}$:\n$$\n\\mathbf{Q}\\mathbf{D} = \\begin{pmatrix} c & -s & 0 \\\\ s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix} = \\begin{pmatrix} c\\lambda_1 & -s\\lambda_2 & 0 \\\\ s\\lambda_1 & c\\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix}\n$$\nNext, we compute the product $\\mathbf{D}\\mathbf{Q}$:\n$$\n\\mathbf{D}\\mathbf{Q} = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0 & \\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix} \\begin{pmatrix} c & -s & 0 \\\\ s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} c\\lambda_1 & -s\\lambda_1 & 0 \\\\ s\\lambda_2 & c\\lambda_2 & 0 \\\\ 0 & 0 & \\lambda_3 \\end{pmatrix}\n$$\nThe commutator matrix is the difference between these two products:\n$$\n[\\mathbf{Q},\\mathbf{D}] = \\mathbf{Q}\\mathbf{D} - \\mathbf{D}\\mathbf{Q} = \\begin{pmatrix} c\\lambda_1 - c\\lambda_1 & -s\\lambda_2 - (-s\\lambda_1) & 0 - 0 \\\\ s\\lambda_1 - s\\lambda_2 & c\\lambda_2 - c\\lambda_2 & 0 - 0 \\\\ 0 - 0 & 0 - 0 & \\lambda_3 - \\lambda_3 \\end{pmatrix}\n$$\n$$\n[\\mathbf{Q},\\mathbf{D}] = \\begin{pmatrix} 0 & -s(\\lambda_2 - \\lambda_1) & 0 \\\\ s(\\lambda_1 - \\lambda_2) & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & s(\\lambda_1 - \\lambda_2) & 0 \\\\ -s(\\lambda_1 - \\lambda_2) & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThe quantity $\\Gamma$ is the square of the Frobenius norm of this commutator matrix. The squared Frobenius norm $\\|\\mathbf{A}\\|_F^2$ of a matrix $\\mathbf{A}$ is the sum of the squares of its elements.\n$$\n\\Gamma = \\|\\,[\\mathbf{Q},\\mathbf{D}]\\,\\|_F^{2} = (0)^{2} + (s(\\lambda_1 - \\lambda_2))^{2} + (0)^{2} + (-s(\\lambda_1 - \\lambda_2))^{2} + (0)^{2} + (0)^{2} + (0)^{2} + (0)^{2} + (0)^{2}\n$$\n$$\n\\Gamma = s^2(\\lambda_1 - \\lambda_2)^2 + s^2(\\lambda_1 - \\lambda_2)^2 = 2s^2(\\lambda_1 - \\lambda_2)^2\n$$\nSubstituting $s = \\sin\\theta$ back into the expression gives the final closed-form result for $\\Gamma$:\n$$\n\\Gamma = 2(\\sin\\theta)^{2}(\\lambda_1 - \\lambda_2)^{2}\n$$\nThe commutativity of $\\mathbf{Q}$ and $\\mathbf{U}=\\mathbf{D}$ is achieved if and only if $\\Gamma = 0$. This occurs if $\\sin\\theta = 0$ (i.e., the rotation angle $\\theta$ is a multiple of $\\pi$, corresponding to a trivial or no rotation in the plane) or if $\\lambda_1 = \\lambda_2$ (i.e., the stretch is isotropic in the plane of rotation). This confirms the physical intuition.", "answer": "$$\n\\boxed{2(\\sin(\\theta))^{2}(\\lambda_1 - \\lambda_2)^{2}}\n$$", "id": "2886415"}, {"introduction": "This final practice bridges the gap between analytical theory and robust computational methods, a crucial skill for a modern mechanician. You will derive an algorithm for polar decomposition based on the Singular Value Decomposition (SVD), the standard numerical tool for this task. Furthermore, you will prove a profound result: the rotation matrix $\\mathbf{R}$ is the unique rotation that is \"closest\" to the deformation gradient $\\mathbf{F}$, giving the decomposition a powerful geometric interpretation and a basis in optimization theory [@problem_id:2886399].", "problem": "You are given a square matrix representing a finite deformation gradient in materials mechanics. Let the deformation gradient be a real matrix $\\mathbf{F} \\in \\mathbb{R}^{n \\times n}$ with $\\det(\\mathbf{F}) > 0$. The right polar decomposition seeks a proper rotation $\\mathbf{R} \\in \\mathrm{SO}(n)$ and a symmetric positive-definite stretch $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ such that $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$. All angles in this problem must be interpreted in radians.\n\nYour tasks:\n\n1) Starting only from the fundamental definitions of the singular value decomposition, orthogonal matrices, Frobenius norm, and symmetric positive-definite matrices, derive an algorithm that computes the right polar decomposition of $\\mathbf{F}$ using the singular value decomposition. The algorithm must work for any $\\mathbf{F}$ with $\\det(\\mathbf{F}) > 0$, and it must return $\\mathbf{R} \\in \\mathrm{SO}(n)$ and $\\mathbf{U}$ symmetric positive-definite.\n\n2) Prove that the rotation $\\mathbf{R}$ produced by your algorithm is the unique minimizer of the problem\n$$\n\\min_{\\mathbf{Q} \\in \\mathrm{SO}(n)} \\|\\mathbf{F} - \\mathbf{Q}\\|_F,\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm, and that the associated $\\mathbf{U} = \\mathbf{R}^\\top \\mathbf{F}$ is symmetric positive-definite. Your proof must be principle-based, starting from the unitary invariance of the Frobenius norm and standard spectral inequalities, without assuming the target result.\n\n3) Implement a program that:\n- Computes the singular value decomposition of each given test matrix $\\mathbf{F}$.\n- Uses your derived algorithm to compute $\\mathbf{R}$ and $\\mathbf{U}$ for each case.\n- For each test case, computes a single real-valued diagnostic that quantitatively verifies one of the core properties implied by your derivation and proof.\n\nUse the following test suite of deformation gradients:\n- Case 1 (two-dimensional diagonal stretch): $\\mathbf{F}_1 = \\begin{bmatrix} 2.0 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$. Output the absolute difference between $\\|\\mathbf{F}_1 - \\mathbf{R}_1\\|_F$ and the value $\\sqrt{\\sum_{i=1}^2 (\\sigma_i - 1)^2}$, where $\\sigma_i$ are the singular values of $\\mathbf{F}_1$.\n- Case 2 (two-dimensional rotated stretch): Let $\\theta = 0.4$ (radians), $\\mathbf{R}_{\\mathrm{true}} = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$, and $\\mathbf{U}_{\\mathrm{true}} = \\operatorname{diag}(1.2, 0.8)$. Define $\\mathbf{F}_2 = \\mathbf{R}_{\\mathrm{true}} \\mathbf{U}_{\\mathrm{true}}$. Output $\\|\\mathbf{R}_2 - \\mathbf{R}_{\\mathrm{true}}\\|_F$.\n- Case 3 (three-dimensional general symmetric stretch with nontrivial eigenbasis): Let $\\phi = 0.6$ (radians), $\\mathbf{Q} = \\begin{bmatrix} \\cos\\phi & -\\sin\\phi & 0 \\\\ \\sin\\phi & \\cos\\phi & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$, $\\mathbf{D} = \\operatorname{diag}(1.5, 0.9, 1.1)$, and $\\mathbf{U}_{\\mathrm{true}} = \\mathbf{Q}\\mathbf{D}\\mathbf{Q}^\\top$. Let $\\alpha = -0.7$ (radians) and $\\mathbf{R}_{\\mathrm{true}}^{(x)} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\alpha & -\\sin\\alpha \\\\ 0 & \\sin\\alpha & \\cos\\alpha \\end{bmatrix}$. Define $\\mathbf{F}_3 = \\mathbf{R}_{\\mathrm{true}}^{(x)} \\mathbf{U}_{\\mathrm{true}}$. Output $\\|\\mathbf{U}_3 - \\mathbf{U}_{\\mathrm{true}}\\|_F$.\n- Case 4 (two-dimensional ill-conditioned stretch): Let $\\beta = 1.1$ (radians), $\\mathbf{R}_{\\beta} = \\begin{bmatrix} \\cos\\beta & -\\sin\\beta \\\\ \\sin\\beta & \\cos\\beta \\end{bmatrix}$, and $\\mathbf{U}_{\\mathrm{true}} = \\operatorname{diag}(10^{-6}, 3.0)$. Define $\\mathbf{F}_4 = \\mathbf{R}_{\\beta} \\mathbf{U}_{\\mathrm{true}}$. Output the absolute difference between $\\|\\mathbf{F}_4 - \\mathbf{R}_4\\|_F$ and the value $\\sqrt{\\sum_{i=1}^2 (\\sigma_i - 1)^2}$.\n\nAll computations must assume angles in radians. Your program must aggregate the four diagnostics into a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases 1 through 4. Each diagnostic must be a real number (a float). It is acceptable to round the outputs to a fixed number of decimal places, but you must be consistent across all four outputs.\n\nNotes:\n- The singular values in the expressions above refer to those of the corresponding $\\mathbf{F}$ in each case.\n- Ensure all $\\mathbf{F}_k$ used have strictly positive determinants so that a proper rotation exists in the polar decomposition.", "solution": "The solution is presented in three parts, as requested: first, the derivation of the algorithm for right polar decomposition; second, the proof of the associated minimization property; and third, an explanation of the implementation applied to the test cases.\n\n### 1. Algorithm Derivation for Right Polar Decomposition\n\nThe objective is to find a proper rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(n)$ (i.e., $\\mathbf{R}^\\top\\mathbf{R}=\\mathbf{I}$ and $\\det(\\mathbf{R})=+1$) and a symmetric positive-definite stretch tensor $\\mathbf{U}$ such that for a given deformation gradient $\\mathbf{F}$ with $\\det(\\mathbf{F}) > 0$, the right polar decomposition holds:\n$$ \\mathbf{F} = \\mathbf{R}\\mathbf{U} $$\nWe start from the Singular Value Decomposition (SVD) of $\\mathbf{F}$, which is defined for any real matrix $\\mathbf{F} \\in \\mathbb{R}^{n \\times n}$ as:\n$$ \\mathbf{F} = \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{W}^\\top $$\nHere, $\\mathbf{V}$ and $\\mathbf{W}$ are orthogonal matrices ($\\mathbf{V}^\\top\\mathbf{V} = \\mathbf{I}$, $\\mathbf{W}^\\top\\mathbf{W} = \\mathbf{I}$), and $\\mathbf{\\Sigma}$ is a diagonal matrix containing the singular values $\\sigma_i \\ge 0$ of $\\mathbf{F}$. Since the problem states $\\det(\\mathbf{F}) > 0$, $\\mathbf{F}$ is invertible, which implies that all singular values are strictly positive, $\\sigma_i > 0$.\n\nWe seek to express $\\mathbf{R}$ and $\\mathbf{U}$ in terms of $\\mathbf{V}$, $\\mathbf{\\Sigma}$, and $\\mathbf{W}$. A natural construction is to group the orthogonal and symmetric parts of the SVD. Let us propose the following candidates for $\\mathbf{R}$ and $\\mathbf{U}$:\n$$ \\mathbf{R} = \\mathbf{V}\\mathbf{W}^\\top $$\n$$ \\mathbf{U} = \\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top $$\nWe must verify that these choices satisfy the required properties.\n\nFirst, check if the decomposition is valid:\n$$ \\mathbf{R}\\mathbf{U} = (\\mathbf{V}\\mathbf{W}^\\top)(\\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top) = \\mathbf{V}(\\mathbf{W}^\\top\\mathbf{W})\\mathbf{\\Sigma}\\mathbf{W}^\\top = \\mathbf{V}\\mathbf{I}\\mathbf{\\Sigma}\\mathbf{W}^\\top = \\mathbf{V}\\mathbf{\\Sigma}\\mathbf{W}^\\top = \\mathbf{F} $$\nThe decomposition is correct.\n\nSecond, verify the properties of $\\mathbf{U}$.\nSymmetry:\n$$ \\mathbf{U}^\\top = (\\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top)^\\top = (\\mathbf{W}^\\top)^\\top \\mathbf{\\Sigma}^\\top \\mathbf{W}^\\top = \\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top = \\mathbf{U} $$\nsince $\\mathbf{\\Sigma}$ is diagonal and thus symmetric. $\\mathbf{U}$ is symmetric.\n\nPositive-definiteness: For any non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$, we examine the quadratic form $\\mathbf{x}^\\top\\mathbf{U}\\mathbf{x}$:\n$$ \\mathbf{x}^\\top\\mathbf{U}\\mathbf{x} = \\mathbf{x}^\\top (\\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top) \\mathbf{x} = (\\mathbf{W}^\\top\\mathbf{x})^\\top \\mathbf{\\Sigma} (\\mathbf{W}^\\top\\mathbf{x}) $$\nLet $\\mathbf{y} = \\mathbf{W}^\\top\\mathbf{x}$. Since $\\mathbf{W}$ is orthogonal and $\\mathbf{x} \\neq \\mathbf{0}$, $\\mathbf{y} \\neq \\mathbf{0}$. The expression becomes:\n$$ \\mathbf{y}^\\top \\mathbf{\\Sigma} \\mathbf{y} = \\sum_{i=1}^n \\sigma_i y_i^2 $$\nAs established, $\\sigma_i > 0$ for all $i$. Since $\\mathbf{y} \\neq \\mathbf{0}$, at least one $y_i \\neq 0$. Thus, $\\sum_{i=1}^n \\sigma_i y_i^2 > 0$. This confirms that $\\mathbf{U}$ is positive-definite.\n\nThird, verify the properties of $\\mathbf{R}$.\nOrthogonality:\n$$ \\mathbf{R}^\\top\\mathbf{R} = (\\mathbf{V}\\mathbf{W}^\\top)^\\top(\\mathbf{V}\\mathbf{W}^\\top) = (\\mathbf{W}\\mathbf{V}^\\top)(\\mathbf{V}\\mathbf{W}^\\top) = \\mathbf{W}(\\mathbf{V}^\\top\\mathbf{V})\\mathbf{W}^\\top = \\mathbf{W}\\mathbf{I}\\mathbf{W}^\\top = \\mathbf{W}\\mathbf{W}^\\top = \\mathbf{I} $$\n$\\mathbf{R}$ is orthogonal.\n\nProper rotation: We must ensure $\\det(\\mathbf{R}) = +1$.\n$$ \\det(\\mathbf{R}) = \\det(\\mathbf{V}\\mathbf{W}^\\top) = \\det(\\mathbf{V})\\det(\\mathbf{W}^\\top) = \\det(\\mathbf{V})\\det(\\mathbf{W}) $$\nFrom the SVD, $\\det(\\mathbf{F}) = \\det(\\mathbf{V}\\mathbf{\\Sigma}\\mathbf{W}^\\top) = \\det(\\mathbf{V})\\det(\\mathbf{\\Sigma})\\det(\\mathbf{W}) = (\\det(\\mathbf{V})\\det(\\mathbf{W})) \\prod_{i=1}^n \\sigma_i$.\nGiven $\\det(\\mathbf{F}) > 0$ and knowing $\\prod \\sigma_i > 0$, it must be that $\\det(\\mathbf{V})\\det(\\mathbf{W}) > 0$.\nThe determinants of orthogonal matrices can only be $+1$ or $-1$. Therefore, $\\det(\\mathbf{V})$ and $\\det(\\mathbf{W})$ must have the same sign. Their product is necessarily $+1$.\n$$ \\det(\\mathbf{R}) = \\det(\\mathbf{V})\\det(\\mathbf{W}) = +1 $$\nThus, $\\mathbf{R}$ is a proper rotation matrix, i.e., $\\mathbf{R} \\in \\mathrm{SO}(n)$.\n\nThe algorithm derived from these first principles is as follows:\n1.  For a given matrix $\\mathbf{F}$ with $\\det(\\mathbf{F}) > 0$, compute its SVD: $\\mathbf{F} = \\mathbf{V}\\mathbf{\\Sigma}\\mathbf{W}^\\top$.\n2.  The rotation matrix is $\\mathbf{R} = \\mathbf{V}\\mathbf{W}^\\top$.\n3.  The stretch tensor is $\\mathbf{U} = \\mathbf{W}\\mathbf{\\Sigma}\\mathbf{W}^\\top$. Alternatively, it can be computed via $\\mathbf{U} = \\mathbf{R}^\\top\\mathbf{F}$.\n\n### 2. Proof of Minimality and Uniqueness\n\nWe must prove that $\\mathbf{R} = \\mathbf{V}\\mathbf{W}^\\top$ is the unique minimizer of the problem $\\min_{\\mathbf{Q} \\in \\mathrm{SO}(n)} \\|\\mathbf{F} - \\mathbf{Q}\\|_F$.\n\nThe Frobenius norm squared is $\\|\\mathbf{A}\\|_F^2 = \\operatorname{Tr}(\\mathbf{A}^\\top\\mathbf{A})$. We expand the objective function:\n$$ \\|\\mathbf{F} - \\mathbf{Q}\\|_F^2 = \\operatorname{Tr}((\\mathbf{F} - \\mathbf{Q})^\\top(\\mathbf{F} - \\mathbf{Q})) = \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{F} - \\mathbf{F}^\\top\\mathbf{Q} - \\mathbf{Q}^\\top\\mathbf{F} + \\mathbf{Q}^\\top\\mathbf{Q}) $$\nSince $\\mathbf{Q} \\in \\mathrm{SO}(n)$, $\\mathbf{Q}$ is orthogonal, so $\\mathbf{Q}^\\top\\mathbf{Q} = \\mathbf{I}$ and $\\operatorname{Tr}(\\mathbf{I}) = n$. Also, $\\operatorname{Tr}(\\mathbf{Q}^\\top\\mathbf{F}) = \\operatorname{Tr}((\\mathbf{F}^\\top\\mathbf{Q})^\\top) = \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{Q})$.\n$$ \\|\\mathbf{F} - \\mathbf{Q}\\|_F^2 = \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{F}) + n - 2\\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{Q}) = \\|\\mathbf{F}\\|_F^2 + n - 2\\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{Q}) $$\nMinimizing this expression with respect to $\\mathbf{Q}$ is equivalent to maximizing the term $\\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{Q})$.\n\nSubstitute the SVD of $\\mathbf{F} = \\mathbf{V}\\mathbf{\\Sigma}\\mathbf{W}^\\top$ into the trace:\n$$ \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{Q}) = \\operatorname{Tr}((\\mathbf{V}\\mathbf{\\Sigma}\\mathbf{W}^\\top)^\\top \\mathbf{Q}) = \\operatorname{Tr}(\\mathbf{W}\\mathbf{\\Sigma}\\mathbf{V}^\\top \\mathbf{Q}) $$\nUsing the cyclic property of the trace, $\\operatorname{Tr}(\\mathbf{ABC}) = \\operatorname{Tr}(\\mathbf{CAB})$:\n$$ \\operatorname{Tr}(\\mathbf{W}\\mathbf{\\Sigma}\\mathbf{V}^\\top \\mathbf{Q}) = \\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{V}^\\top \\mathbf{Q}\\mathbf{W}) $$\nLet $\\mathbf{Z} = \\mathbf{V}^\\top\\mathbf{Q}\\mathbf{W}$. Since $\\mathbf{V}$, $\\mathbf{Q}$, and $\\mathbf{W}$ are orthogonal, their product $\\mathbf{Z}$ is also orthogonal. We want to maximize $\\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z})$ over all orthogonal $\\mathbf{Z}$ that can be formed from a $\\mathbf{Q} \\in \\mathrm{SO}(n)$.\n$$ \\det(\\mathbf{Z}) = \\det(\\mathbf{V}^\\top)\\det(\\mathbf{Q})\\det(\\mathbf{W}) = \\det(\\mathbf{V})^{-1} (+1) \\det(\\mathbf{W}) = \\det(\\mathbf{V})\\det(\\mathbf{W}) = +1 $$\nThe last step is because $\\det(\\mathbf{V}) = \\det(\\mathbf{V}^\\top)$ for orthogonal matrices with determinant $\\pm 1$, or simply because $\\det(\\mathbf{V})^2=1$. More rigorously, $\\det(\\mathbf{V}^\\top\\mathbf{V})=1 \\implies \\det(\\mathbf{V}^\\top)\\det(\\mathbf{V})=1 \\implies \\det(\\mathbf{V})^2=1$. So $\\det(\\mathbf{V}^\\top)=\\det(\\mathbf{V})$.\nThus, we must maximize $\\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z})$ over all $\\mathbf{Z} \\in \\mathrm{SO}(n)$.\n\nThe trace is $\\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z}) = \\sum_{i=1}^n \\sigma_i Z_{ii}$. By von Neumann's trace inequality, $\\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z}) \\le \\sum_i \\sigma_i(\\mathbf{\\Sigma})\\sigma_i(\\mathbf{Z})$. The singular values of $\\mathbf{\\Sigma}$ are $\\sigma_i$ and the singular values of an orthogonal matrix $\\mathbf{Z}$ are all $1$.\n$$ \\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z}) \\le \\sum_{i=1}^n \\sigma_i $$\nEquality holds if $\\mathbf{\\Sigma}$ and $\\mathbf{Z}$ can be simultaneously diagonalized by the same pair of orthogonal matrices. Since $\\mathbf{\\Sigma}$ is already diagonal, this condition implies that $\\mathbf{Z}$ must also be diagonal. An orthogonal diagonal matrix must have diagonal entries of $\\pm 1$.\nTo maximize $\\sum_i \\sigma_i Z_{ii}$ where $\\sigma_i > 0$ and $Z_{ii} = \\pm 1$, we must choose $Z_{ii}=+1$ for all $i$. This leads to the unique choice $\\mathbf{Z} = \\mathbf{I}$.\nThe identity matrix $\\mathbf{I}$ is in $\\mathrm{SO}(n)$ since $\\det(\\mathbf{I})=+1$.\nTherefore, the maximum of $\\operatorname{Tr}(\\mathbf{\\Sigma}\\mathbf{Z})$ is achieved uniquely when $\\mathbf{Z} = \\mathbf{I}$.\n\nSetting $\\mathbf{Z} = \\mathbf{I}$ gives the optimal $\\mathbf{Q}$:\n$$ \\mathbf{V}^\\top\\mathbf{Q}\\mathbf{W} = \\mathbf{I} \\implies \\mathbf{Q} = \\mathbf{V}\\mathbf{I}\\mathbf{W}^\\top = \\mathbf{V}\\mathbf{W}^\\top $$\nThis is precisely the matrix $\\mathbf{R}$ derived in the previous section. This proves that $\\mathbf{R} = \\mathbf{V}\\mathbf{W}^\\top$ is the unique minimizer of $\\|\\mathbf{F} - \\mathbf{Q}\\|_F$ for $\\mathbf{Q} \\in \\mathrm{SO}(n)$.\n\nThe proof that the associated stretch tensor $\\mathbf{U} = \\mathbf{R}^\\top\\mathbf{F}$ is symmetric and positive-definite has already been demonstrated in the algorithm derivation section.\n\n### 3. Implementation and Verification\n\nThe Python implementation will follow the derived algorithm. For each test matrix $\\mathbf{F}_k$, we perform the following steps:\n1.  Construct $\\mathbf{F}_k$.\n2.  Compute its SVD using `numpy.linalg.svd`: $\\mathbf{V}$, $\\mathbf{S}$ (vector of singular values), $\\mathbf{W}^\\top$.\n3.  Calculate $\\mathbf{R}_k = \\mathbf{V} \\mathbf{W}^\\top$.\n4.  Calculate the required diagnostic quantity.\n\nFor Cases $1$ and $4$, the diagnostic is $|\\,\\|\\mathbf{F}_k - \\mathbf{R}_k\\|_F - \\sqrt{\\sum (\\sigma_i - 1)^2}\\,|$.\nFrom the proof in Section 2, the minimum value of the objective function is:\n$$ \\|\\mathbf{F} - \\mathbf{R}\\|_F^2 = \\|\\mathbf{F}\\|_F^2 + n - 2 \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{R}) $$\nWe showed that $\\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{R}) = \\sum \\sigma_i$. Also, $\\|\\mathbf{F}\\|_F^2 = \\operatorname{Tr}(\\mathbf{F}^\\top\\mathbf{F}) = \\operatorname{Tr}(\\mathbf{W}\\mathbf{\\Sigma}^2\\mathbf{W}^\\top) = \\operatorname{Tr}(\\mathbf{\\Sigma}^2) = \\sum \\sigma_i^2$.\n$$ \\|\\mathbf{F} - \\mathbf{R}\\|_F^2 = \\sum \\sigma_i^2 + n - 2\\sum \\sigma_i = \\sum_{i=1}^n (\\sigma_i^2 - 2\\sigma_i + 1) = \\sum_{i=1}^n (\\sigma_i - 1)^2 $$\nTaking the square root, we get exactly $\\|\\mathbf{F} - \\mathbf{R}\\|_F = \\sqrt{\\sum (\\sigma_i - 1)^2}$.\nTherefore, the diagnostic for Cases $1$ and $4$ is expected to be zero, up to floating-point precision errors. This serves as a quantitative verification of our minimization proof.\n\nFor Cases $2$ and $3$, the matrices $\\mathbf{F}_k$ are constructed in the form $\\mathbf{F}_k = \\mathbf{R}_{\\mathrm{true}} \\mathbf{U}_{\\mathrm{true}}$, where $\\mathbf{R}_{\\mathrm{true}}$ is a proper rotation and $\\mathbf{U}_{\\mathrm{true}}$ is symmetric positive-definite. This is, by definition, the right polar decomposition. Since the polar decomposition of an invertible matrix is unique, our algorithm must recover $\\mathbf{R}_k = \\mathbf{R}_{\\mathrm{true}}$ and $\\mathbf{U}_k = \\mathbf{U}_{\\mathrm{true}}$. The diagnostics $\\|\\mathbf{R}_2 - \\mathbf{R}_{\\mathrm{true}}\\|_F$ and $\\|\\mathbf{U}_3 - \\mathbf{U}_{\\mathrm{true}}\\|_F$ are therefore expected to be zero, up to numerical precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the right polar decomposition for several deformation gradients and\n    calculates specific diagnostic values to verify the theoretical results.\n    \"\"\"\n\n    # --- Test Case 1: 2D diagonal stretch ---\n    F1 = np.array([[2.0, 0.0], [0.0, 0.5]])\n\n    # --- Test Case 2: 2D rotated stretch ---\n    theta = 0.4\n    R_true2 = np.array([[np.cos(theta), -np.sin(theta)],\n                        [np.sin(theta), np.cos(theta)]])\n    U_true2 = np.diag([1.2, 0.8])\n    F2 = R_true2 @ U_true2\n\n    # --- Test Case 3: 3D general symmetric stretch ---\n    phi = 0.6\n    Q3 = np.array([[np.cos(phi), -np.sin(phi), 0.0],\n                   [np.sin(phi), np.cos(phi), 0.0],\n                   [0.0, 0.0, 1.0]])\n    D3 = np.diag([1.5, 0.9, 1.1])\n    U_true3 = Q3 @ D3 @ Q3.T\n    \n    alpha = -0.7\n    R_true3_x = np.array([[1.0, 0.0, 0.0],\n                          [0.0, np.cos(alpha), -np.sin(alpha)],\n                          [0.0, np.sin(alpha), np.cos(alpha)]])\n    F3 = R_true3_x @ U_true3\n\n    # --- Test Case 4: 2D ill-conditioned stretch ---\n    beta = 1.1\n    R_beta4 = np.array([[np.cos(beta), -np.sin(beta)],\n                        [np.sin(beta), np.cos(beta)]])\n    U_true4 = np.diag([1e-6, 3.0])\n    F4 = R_beta4 @ U_true4\n\n    test_cases = [\n        {'F': F1, 'type': 'diag_check'},\n        {'F': F2, 'type': 'R_check', 'R_true': R_true2},\n        {'F': F3, 'type': 'U_check', 'U_true': U_true3},\n        {'F': F4, 'type': 'diag_check'}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        F = case['F']\n        \n        # Core algorithm: Right Polar Decomposition using SVD\n        # F = V @ Sigma @ Wt\n        V, S_vec, Wt = np.linalg.svd(F)\n        \n        # Check for reflection and correct if necessary.\n        # This is needed for a general algorithm where det(F) could be < 0.\n        # For this problem, det(F) > 0, so det(V)*det(Wt) is always 1.\n        if np.linalg.det(V) * np.linalg.det(Wt) < 0:\n            V_corr = V.copy()\n            V_corr[:, -1] *= -1\n            R = V_corr @ Wt\n        else:\n            R = V @ Wt\n\n        if case['type'] == 'diag_check':\n            # Diagnostic: | ||F - R||_F - sqrt(sum((sigma_i - 1)^2)) |\n            norm_F_minus_R = np.linalg.norm(F - R, 'fro')\n            norm_s_minus_1 = np.sqrt(np.sum((S_vec - 1)**2))\n            diagnostic = np.abs(norm_F_minus_R - norm_s_minus_1)\n            results.append(diagnostic)\n            \n        elif case['type'] == 'R_check':\n            # Diagnostic: ||R - R_true||_F\n            R_true = case['R_true']\n            diagnostic = np.linalg.norm(R - R_true, 'fro')\n            results.append(diagnostic)\n\n        elif case['type'] == 'U_check':\n            # U can be computed as U = R.T @ F\n            U = R.T @ F\n            U_true = case['U_true']\n            # Diagnostic: ||U - U_true||_F\n            diagnostic = np.linalg.norm(U - U_true, 'fro')\n            results.append(diagnostic)\n\n    # Format the output as a comma-separated list of floats in a single line.\n    # Use scientific notation for consistency and to handle very small numbers.\n    formatted_results = [f\"{res:.8e}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2886399"}]}