{"hands_on_practices": [{"introduction": "A foundational challenge in applying machine learning to mechanics is translating tensor-valued physical quantities, like stress and strain, into the vector formats required by standard algorithms. This translation must be done carefully to preserve the underlying geometric and physical structure. This exercise guides you through a formal comparison of two common encoding schemes, Voigt and Mandel notation, revealing why the latter is essential for maintaining a consistent physical metric from tensor space to vector space [@problem_id:2898837]. Mastering this concept is a crucial first step toward building robust and physically meaningful data-driven material models.", "problem": "A data-driven constitutive modeling study uses supervised regression to map microstructural descriptors to a symmetric second-order tensor response (for example, Cauchy stress) in dimension $n \\in \\{2,3\\}$. For each sample $k \\in \\{1,\\dots,N\\}$, the model predicts a tensor $\\hat{\\boldsymbol{T}}^{(k)} \\in \\mathbb{R}^{n \\times n}$ and is compared to the target $\\boldsymbol{T}^{(k)} \\in \\mathbb{R}^{n \\times n}$ via the residual $\\boldsymbol{R}^{(k)} = \\boldsymbol{T}^{(k)} - \\hat{\\boldsymbol{T}}^{(k)}$, which is symmetric. Two encodings are considered for training with a standard Euclidean mean squared error (MSE) in the encoded space:\n- Voigt encoding $v(\\cdot)$, which lists all unique components with off-diagonal entries included once.\n- Mandel encoding $m(\\cdot)$, which is identical to Voigt except that off-diagonal components are multiplied by $\\sqrt{2}$.\n\nThe model is trained by minimizing the empirical MSE using the squared Euclidean norm in the encoded space. Consider the continuous-data limit in which the residual $\\boldsymbol{R}$ is a random symmetric tensor satisfying the following assumptions:\n(i) $R_{ij}$ has zero mean for all $i,j$ with $i \\leq j$;\n(ii) the set $\\{R_{ii}\\}_{i=1}^{n}$ of diagonal components and the set $\\{R_{ij}\\}_{1 \\leq i < j \\leq n}$ of off-diagonal components are mutually independent and internally independent;\n(iii) all $R_{ij}$ with $i \\leq j$ share a common variance $\\sigma^{2}$.\n\nLet $\\ell_{V} = \\|v(\\boldsymbol{R})\\|_{2}^{2}$ denote the per-sample squared loss under Voigt encoding and $\\ell_{M} = \\|m(\\boldsymbol{R})\\|_{2}^{2}$ the per-sample squared loss under Mandel encoding, both computed with the standard Euclidean metric on the encoded vectors. Starting only from the definitions of the Frobenius inner product, the Euclidean norm, and the encodings above, derive an exact expression $\\alpha(n)$ such that the expected losses satisfy $\\mathbb{E}[\\ell_{M}] = \\alpha(n)\\,\\mathbb{E}[\\ell_{V}]$ under assumptions (i)–(iii). Your final answer must be the closed-form analytic expression for $\\alpha(n)$. No numerical rounding is required. Do not include units in your final answer.", "solution": "The problem statement has been subjected to rigorous validation and is deemed scientifically grounded, well-posed, and objective. It presents a formalizable question within the domain of data-driven computational mechanics, based on standard definitions and plausible statistical assumptions. All necessary information is provided, and there are no internal contradictions or ambiguities. Therefore, we proceed with the derivation of a solution.\n\nLet $\\boldsymbol{R}$ be a symmetric second-order tensor in $\\mathbb{R}^{n \\times n}$, where $n$ is the spatial dimension. Its components are denoted by $R_{ij}$, with $R_{ij} = R_{ji}$. The number of unique components in such a tensor is the sum of the $n$ diagonal components and the $\\frac{n(n-1)}{2}$ unique off-diagonal components (where $i < j$), for a total of $d = n + \\frac{n(n-1)}{2} = \\frac{n(n+1)}{2}$ unique components.\n\nThe problem defines two vector encodings for $\\boldsymbol{R}$.\nThe Voigt encoding, $v(\\boldsymbol{R})$, is a vector in $\\mathbb{R}^d$ containing all unique components of $\\boldsymbol{R}$. Without loss of generality, we can express it as:\n$$ v(\\boldsymbol{R}) = \\left( R_{11}, \\dots, R_{nn}, R_{12}, \\dots, R_{n-1,n} \\right)^T $$\nThe Mandel encoding, $m(\\boldsymbol{R})$, is a vector in $\\mathbb{R}^d$ which is identical to the Voigt encoding for diagonal components but multiplies the off-diagonal components by a factor of $\\sqrt{2}$:\n$$ m(\\boldsymbol{R}) = \\left( R_{11}, \\dots, R_{nn}, \\sqrt{2} R_{12}, \\dots, \\sqrt{2} R_{n-1,n} \\right)^T $$\nThe order of components within these vectors is irrelevant for the calculation of the Euclidean norm.\n\nThe per-sample squared losses, $\\ell_V$ and $\\ell_M$, are defined as the squared Euclidean norms of these vectors.\nFor the Voigt encoding, the loss $\\ell_V$ is:\n$$ \\ell_V = \\|v(\\boldsymbol{R})\\|_{2}^{2} = \\sum_{i=1}^{n} R_{ii}^{2} + \\sum_{1 \\le i < j \\le n} R_{ij}^{2} $$\nFor the Mandel encoding, the loss $\\ell_M$ is:\n$$ \\ell_M = \\|m(\\boldsymbol{R})\\|_{2}^{2} = \\sum_{i=1}^{n} R_{ii}^{2} + \\sum_{1 \\le i < j \\le n} \\left(\\sqrt{2} R_{ij}\\right)^{2} = \\sum_{i=1}^{n} R_{ii}^{2} + 2 \\sum_{1 \\le i < j \\le n} R_{ij}^{2} $$\nIt is a fundamental property that the squared norm of the Mandel vector, $\\|m(\\boldsymbol{R})\\|_{2}^{2}$, is equal to the squared Frobenius norm of the tensor $\\boldsymbol{R}$, denoted $\\|\\boldsymbol{R}\\|_{F}^{2} = \\sum_{i,j=1}^{n} R_{ij}^2$. This is the reason for the $\\sqrt{2}$ factor in the Mandel definition.\n\nWe are tasked with finding the relationship between the expected values of these losses, $\\mathbb{E}[\\ell_V]$ and $\\mathbb{E}[\\ell_M]$, where the expectation is taken over the distribution of the random residual tensor $\\boldsymbol{R}$.\nUsing the linearity of the expectation operator, we have:\n$$ \\mathbb{E}[\\ell_V] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} R_{ii}^{2} + \\sum_{1 \\le i < j \\le n} R_{ij}^{2} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[R_{ii}^{2}] + \\sum_{1 \\le i < j \\le n} \\mathbb{E}[R_{ij}^{2}] $$\n$$ \\mathbb{E}[\\ell_M] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} R_{ii}^{2} + 2 \\sum_{1 \\le i < j \\le n} R_{ij}^{2} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[R_{ii}^{2}] + 2 \\sum_{1 \\le i < j \\le n} \\mathbb{E}[R_{ij}^{2}] $$\n\nThe problem provides three assumptions about the random components $R_{ij}$ for $i \\le j$:\n(i) They have zero mean: $\\mathbb{E}[R_{ij}] = 0$.\n(ii) They are mutually independent.\n(iii) They share a common variance: $\\text{Var}(R_{ij}) = \\sigma^2$.\n\nThe variance of a random variable $X$ is defined as $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Using assumption (i), we find the expected value of the square of each component:\n$$ \\mathbb{E}[R_{ij}^2] = \\text{Var}(R_{ij}) + (\\mathbb{E}[R_{ij}])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\nThis holds for all unique components, both diagonal ($i=j$) and off-diagonal ($i<j$).\n\nNow we substitute this result into our expressions for the expected losses.\nFor $\\mathbb{E}[\\ell_V]$, we sum the expected squares of the $n$ diagonal components and the $\\frac{n(n-1)}{2}$ unique off-diagonal components:\n$$ \\mathbb{E}[\\ell_V] = \\sum_{i=1}^{n} \\sigma^2 + \\sum_{1 \\le i < j \\le n} \\sigma^2 = n \\sigma^2 + \\frac{n(n-1)}{2} \\sigma^2 $$\nSimplifying this expression:\n$$ \\mathbb{E}[\\ell_V] = \\left( n + \\frac{n^2 - n}{2} \\right) \\sigma^2 = \\left( \\frac{2n + n^2 - n}{2} \\right) \\sigma^2 = \\frac{n^2+n}{2} \\sigma^2 = \\frac{n(n+1)}{2} \\sigma^2 $$\n\nFor $\\mathbb{E}[\\ell_M]$, we perform a similar substitution:\n$$ \\mathbb{E}[\\ell_M] = \\sum_{i=1}^{n} \\sigma^2 + 2 \\sum_{1 \\le i < j \\le n} \\sigma^2 = n \\sigma^2 + 2 \\left( \\frac{n(n-1)}{2} \\right) \\sigma^2 $$\nSimplifying this expression:\n$$ \\mathbb{E}[\\ell_M] = (n + n(n-1)) \\sigma^2 = (n + n^2 - n) \\sigma^2 = n^2 \\sigma^2 $$\nNote that assumption (ii), the independence of components, is not required for calculating the expectation of the sum, but it is a standard and reasonable assumption for such a model.\n\nThe problem asks for a factor $\\alpha(n)$ such that $\\mathbb{E}[\\ell_M] = \\alpha(n) \\mathbb{E}[\\ell_V]$. We can find this factor by taking the ratio of the two expected values, assuming $\\sigma^2 \\neq 0$ (the trivial case where all residuals are zero):\n$$ \\alpha(n) = \\frac{\\mathbb{E}[\\ell_M]}{\\mathbb{E}[\\ell_V]} = \\frac{n^2 \\sigma^2}{\\frac{n(n+1)}{2} \\sigma^2} $$\nThe common factor $\\sigma^2$ cancels, yielding the final expression for $\\alpha(n)$:\n$$ \\alpha(n) = \\frac{n^2}{\\frac{n(n+1)}{2}} = \\frac{2n^2}{n(n+1)} = \\frac{2n}{n+1} $$\nThis expression gives the exact relationship between the expected losses under the two encoding schemes as a function of the spatial dimension $n$.", "answer": "$$\\boxed{\\frac{2n}{n+1}}$$", "id": "2898837"}, {"introduction": "Building upon correct data representation, we can construct models that inherently respect physical laws. Since many material behaviors, such as plasticity and viscoelasticity, are governed by time-dependent evolution equations, recurrent neural networks (RNNs) are a natural architectural choice. This exercise demonstrates how to create a \"physics-informed\" RNN cell by directly deriving its update rule from the numerical discretization of a continuum mechanics-based ODE for internal variables [@problem_id:2898867]. You will also analyze the numerical stability of your custom cell, a critical skill that bridges the gap between deep learning design and classical computational mechanics.", "problem": "You are asked to build a physics-informed recurrent cell for a Recurrent Neural Network (RNN), grounded in continuum materials mechanics, by explicitly discretizing the internal variable evolution equation. Consider an internal state vector $\\mathbf{z}\\in\\mathbb{R}^2$ whose evolution is governed by the ordinary differential equation (ODE)\n$$\n\\dot{\\mathbf{z}} = g(\\mathbf{F},\\mathbf{z}),\n$$\nwhere $\\mathbf{F}\\in\\mathbb{R}^{2\\times 2}$ is the deformation gradient. Use the following physics-informed choice motivated by dissipative internal variable dynamics in inelasticity:\n$$\ng(\\mathbf{F},\\mathbf{z}) = -\\mathbf{L}\\,\\mathbf{z} + \\gamma\\,\\mathbf{s}(\\mathbf{F}),\n$$\nwhere $\\mathbf{L}\\in\\mathbb{R}^{2\\times 2}$ is a symmetric positive-definite matrix (representing a linear kinetic operator), $\\gamma\\in\\mathbb{R}$ is a scalar gain, and $\\mathbf{s}(\\mathbf{F})\\in\\mathbb{R}^2$ is a source vector derived from the deviatoric logarithmic strain. Define the deviatoric logarithmic strain source as follows: compute the right stretch tensor $\\mathbf{U}$ from the polar decomposition $\\mathbf{F}=\\mathbf{R}\\mathbf{U}$, where $\\mathbf{U}$ is symmetric positive-definite, then form the principal matrix logarithm $\\log\\mathbf{U}$, and its deviatoric part\n$$\n\\operatorname{dev}(\\log\\mathbf{U}) = \\log\\mathbf{U} - \\frac{\\operatorname{tr}(\\log\\mathbf{U})}{2}\\,\\mathbf{I}.\n$$\nFinally, set\n$$\n\\mathbf{s}(\\mathbf{F}) = \\begin{bmatrix} \\operatorname{dev}(\\log\\mathbf{U})_{11} \\\\ \\operatorname{dev}(\\log\\mathbf{U})_{22} \\end{bmatrix}.\n$$\nAll quantities are nondimensional.\n\nYour tasks are:\n\n1) Construct the recurrent cell update by applying an explicit time discretization with uniform time step $\\Delta t>0$ to the ODE, yielding a map $\\mathbf{z}_{k+1}=\\Phi(\\mathbf{F}_k,\\mathbf{z}_k)$ that is consistent with the above physics. Use the explicit Euler method derived from first principles of time discretization.\n\n2) Analyze the local linear stability of this update by linearizing in $\\mathbf{z}$. Use the Jacobian $\\mathbf{J}_z = \\partial g/\\partial \\mathbf{z}$ and the corresponding one-step update Jacobian $\\mathbf{A}_{\\text{step}} = \\mathbf{I} + \\Delta t\\,\\mathbf{J}_z$. Using the spectral radius $\\rho(\\cdot)$, declare the update locally stable if and only if\n$$\n\\rho(\\mathbf{A}_{\\text{step}}) < 1.\n$$\nYour program must compute $\\rho(\\mathbf{A}_{\\text{step}})$ exactly for the provided cases and return a boolean flag indicating whether the above strict inequality holds. Do not use asymptotic or approximate bounds in place of the exact spectral radius of $\\mathbf{A}_{\\text{step}}$.\n\n3) Implement the physics-informed recurrent cell and simulate $\\mathbf{z}_{k+1}=\\mathbf{z}_k + \\Delta t\\,g(\\mathbf{F},\\mathbf{z}_k)$ for a given constant $\\mathbf{F}$ over $N$ steps from a prescribed initial condition $\\mathbf{z}_0$. Return the final state $\\mathbf{z}_N$.\n\nBase your derivations only on fundamental definitions: the explicit Euler discretization of $\\dot{\\mathbf{z}}=g(\\mathbf{F},\\mathbf{z})$, the polar decomposition $\\mathbf{F}=\\mathbf{R}\\mathbf{U}$ with $\\mathbf{U}=\\mathbf{V}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$ computed from the singular value decomposition of $\\mathbf{F}$, the principal matrix logarithm $\\log\\mathbf{U}=\\mathbf{V}\\,\\log(\\boldsymbol{\\Sigma})\\,\\mathbf{V}^\\top$, and the definition of the spectral radius as the maximum absolute eigenvalue.\n\nFor numerical testing, use the following test suite containing three cases. Each case provides $(\\mathbf{L},\\gamma,\\mathbf{F},\\Delta t,N,\\mathbf{z}_0)$, with all numbers nondimensional:\n\n- Case A (stable interior):\n  - $\\mathbf{L}=\\begin{bmatrix} 6 & 0 \\\\ 0 & 10 \\end{bmatrix}$,\n  - $\\gamma=2.0$,\n  - $\\mathbf{F}=\\begin{bmatrix} 1.1 & 0.0 \\\\ 0.0 & 0.95 \\end{bmatrix}$,\n  - $\\Delta t=0.15$,\n  - $N=8$,\n  - $\\mathbf{z}_0=\\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Case B (stability boundary):\n  - $\\mathbf{L}=\\begin{bmatrix} 4 & 1 \\\\ 1 & 4 \\end{bmatrix}$,\n  - $\\gamma=1.5$,\n  - $\\mathbf{F}=\\begin{bmatrix} 1.0 & 0.2 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n  - $\\Delta t=0.4$,\n  - $N=12$,\n  - $\\mathbf{z}_0=\\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n\n- Case C (unstable):\n  - $\\mathbf{L}=\\begin{bmatrix} 8.0 & 0.0 \\\\ 0.0 & 12.0 \\end{bmatrix}$,\n  - $\\gamma=0.5$,\n  - $\\mathbf{F}=\\begin{bmatrix} 0.9 & 0.0 \\\\ 0.0 & 1.05 \\end{bmatrix}$,\n  - $\\Delta t=0.21$,\n  - $N=5$,\n  - $\\mathbf{z}_0=\\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nImplementation requirements:\n\n- Compute $\\mathbf{U}$ and $\\log\\mathbf{U}$ using only linear algebra primitives: if $\\mathbf{F}=\\mathbf{U}_s\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$ is the singular value decomposition, then $\\mathbf{U}=\\mathbf{V}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$ and $\\log\\mathbf{U}=\\mathbf{V}\\,\\log(\\boldsymbol{\\Sigma})\\,\\mathbf{V}^\\top$ with the logarithm applied entrywise to the diagonal of $\\boldsymbol{\\Sigma}$.\n- Use the explicit Euler update $\\mathbf{z}_{k+1}=\\mathbf{z}_k + \\Delta t\\,(-\\mathbf{L}\\,\\mathbf{z}_k + \\gamma\\,\\mathbf{s}(\\mathbf{F}))$.\n- For stability, use $\\mathbf{J}_z=-\\mathbf{L}$ and $\\mathbf{A}_{\\text{step}}=\\mathbf{I}-\\Delta t\\,\\mathbf{L}$, and compute $\\rho(\\mathbf{A}_{\\text{step}})$ as the maximum absolute value of the eigenvalues of $\\mathbf{A}_{\\text{step}}$.\n- For each case, your program must output a list containing the spectral radius, a boolean stability flag (true if strictly less than $1$), and the two components of the final state $\\mathbf{z}_N$. Round all floating-point outputs to exactly $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sub-list of the form $[\\rho,\\text{stable},z_{N,1},z_{N,2}]$. For example, an output with three cases should look like $[[\\rho_1,\\text{True},z_{N,1}^{(1)},z_{N,2}^{(1)}],[\\rho_2,\\text{False},z_{N,1}^{(2)},z_{N,2}^{(2)}],[\\rho_3,\\text{False},z_{N,1}^{(3)},z_{N,2}^{(3)}]]$ with all real numbers rounded to $6$ decimals.\n\nEnsure scientific realism and self-consistency by following the definitions above. Do not use any external data. All computations are nondimensional; no physical unit conversion is required. Angles are not used. All percentages, if any, must be expressed as decimals.", "solution": "The problem presented is valid. It is scientifically grounded in the principles of continuum mechanics and numerical analysis, well-posed with all necessary information provided, and stated with objective, formal language. There are no contradictions, ambiguities, or unsound premises. We shall proceed with the solution.\n\nThe problem requires the development and analysis of a physics-informed recurrent cell based on the discretization of an internal variable evolution equation from continuum materials mechanics. The solution is structured into three parts: derivation of the recurrent update rule, stability analysis of this rule, and its numerical implementation.\n\nThe evolution of the internal state vector $\\mathbf{z} \\in \\mathbb{R}^2$ is governed by the ordinary differential equation (ODE):\n$$\n\\dot{\\mathbf{z}} = g(\\mathbf{F}, \\mathbf{z})\n$$\nwhere $\\mathbf{F} \\in \\mathbb{R}^{2\\times 2}$ is the deformation gradient. The function $g$ is specified as:\n$$\ng(\\mathbf{F}, \\mathbf{z}) = -\\mathbf{L}\\,\\mathbf{z} + \\gamma\\,\\mathbf{s}(\\mathbf{F})\n$$\nHere, $\\mathbf{L} \\in \\mathbb{R}^{2\\times 2}$ is a symmetric positive-definite matrix, $\\gamma \\in \\mathbb{R}$ is a scalar, and $\\mathbf{s}(\\mathbf{F}) \\in \\mathbb{R}^2$ is a source term derived from the deformation.\n\nFirst, we must define the source term $\\mathbf{s}(\\mathbf{F})$. It is based on the deviatoric part of the logarithmic strain. We begin with the polar decomposition of the deformation gradient, $\\mathbf{F} = \\mathbf{R}\\mathbf{U}$, where $\\mathbf{R}$ is a rotation matrix and $\\mathbf{U}$ is the symmetric positive-definite right stretch tensor. The problem specifies computing $\\mathbf{U}$ from the singular value decomposition (SVD) of $\\mathbf{F}$. Let the SVD of $\\mathbf{F}$ be $\\mathbf{F} = \\mathbf{U}_s\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$. Then the right stretch tensor is given by $\\mathbf{U} = \\mathbf{V}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$. The principal matrix logarithm of $\\mathbf{U}$ is then $\\log\\mathbf{U} = \\mathbf{V}\\,(\\log\\boldsymbol{\\Sigma})\\,\\mathbf{V}^\\top$, where the logarithm is applied element-wise to the diagonal matrix of singular values $\\boldsymbol{\\Sigma}$.\n\nThe deviatoric part of the logarithmic strain tensor $\\log\\mathbf{U}$ is defined as:\n$$\n\\operatorname{dev}(\\log\\mathbf{U}) = \\log\\mathbf{U} - \\frac{\\operatorname{tr}(\\log\\mathbf{U})}{2}\\,\\mathbf{I}\n$$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix. The source vector $\\mathbf{s}(\\mathbf{F})$ is constructed from the diagonal components of this deviatoric tensor:\n$$\n\\mathbf{s}(\\mathbf{F}) = \\begin{bmatrix} \\operatorname{dev}(\\log\\mathbf{U})_{11} \\\\ \\operatorname{dev}(\\log\\mathbf{U})_{22} \\end{bmatrix}\n$$\n\nWith all terms defined, we address the three main tasks.\n\n**1. Construction of the Recurrent Cell Update Rule**\n\nWe are tasked to discretize the governing ODE using the explicit (forward) Euler method with a uniform time step $\\Delta t > 0$. The explicit Euler method approximates the time derivative $\\dot{\\mathbf{z}}(t)$ at time $t_k$ as $\\frac{\\mathbf{z}_{k+1} - \\mathbf{z}_k}{\\Delta t}$, where $\\mathbf{z}_k = \\mathbf{z}(t_k)$.\n$$\n\\frac{\\mathbf{z}_{k+1} - \\mathbf{z}_k}{\\Delta t} = g(\\mathbf{F}_k, \\mathbf{z}_k)\n$$\nRearranging for $\\mathbf{z}_{k+1}$ yields the update rule:\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\Delta t\\,g(\\mathbf{F}_k, \\mathbf{z}_k)\n$$\nSubstituting the given expression for $g(\\mathbf{F}_k, \\mathbf{z}_k)$:\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\Delta t\\,(-\\mathbf{L}\\,\\mathbf{z}_k + \\gamma\\,\\mathbf{s}(\\mathbf{F}_k))\n$$\nWe can rearrange this expression by grouping terms involving $\\mathbf{z}_k$:\n$$\n\\mathbf{z}_{k+1} = (\\mathbf{I} - \\Delta t\\,\\mathbf{L})\\,\\mathbf{z}_k + \\Delta t\\,\\gamma\\,\\mathbf{s}(\\mathbf{F}_k)\n$$\nThis equation defines the recurrent map $\\mathbf{z}_{k+1} = \\Phi(\\mathbf{F}_k, \\mathbf{z}_k)$, which is affine in $\\mathbf{z}_k$.\n\n**2. Local Linear Stability Analysis**\n\nThe stability of the update rule is assessed by linearizing the map $\\Phi$ with respect to the state $\\mathbf{z}_k$ for a fixed input $\\mathbf{F}$. This is equivalent to finding the Jacobian of the one-step update. The problem provides the formula for this Jacobian, denoted $\\mathbf{A}_{\\text{step}}$, as $\\mathbf{A}_{\\text{step}} = \\mathbf{I} + \\Delta t\\,\\mathbf{J}_z$, where $\\mathbf{J}_z = \\frac{\\partial g}{\\partial \\mathbf{z}}$.\n\nLet us compute $\\mathbf{J}_z$:\n$$\n\\mathbf{J}_z = \\frac{\\partial}{\\partial \\mathbf{z}} (-\\mathbf{L}\\,\\mathbf{z} + \\gamma\\,\\mathbf{s}(\\mathbf{F}))\n$$\nSince the term $\\gamma\\,\\mathbf{s}(\\mathbf{F})$ does not depend on $\\mathbf{z}$, its derivative with respect to $\\mathbf{z}$ is the zero matrix. The derivative of $-\\mathbf{L}\\,\\mathbf{z}$ with respect to $\\mathbf{z}$ is simply $-\\mathbf{L}$. Thus,\n$$\n\\mathbf{J}_z = -\\mathbf{L}\n$$\nThe one-step update Jacobian is therefore:\n$$\n\\mathbf{A}_{\\text{step}} = \\mathbf{I} + \\Delta t\\,(-\\mathbf{L}) = \\mathbf{I} - \\Delta t\\,\\mathbf{L}\n$$\nThis confirms the matrix derived directly from our update rule $\\mathbf{z}_{k+1} = (\\mathbf{I} - \\Delta t\\,\\mathbf{L})\\,\\mathbf{z}_k + \\dots$.\n\nLocal stability requires that the spectral radius of this Jacobian, $\\rho(\\mathbf{A}_{\\text{step}})$, be strictly less than $1$:\n$$\n\\rho(\\mathbf{A}_{\\text{step}}) = \\max_i |\\lambda_i(\\mathbf{A}_{\\text{step}})| < 1\n$$\nwhere $\\lambda_i(\\mathbf{A}_{\\text{step}})$ are the eigenvalues of $\\mathbf{A}_{\\text{step}}$. Let $\\lambda_j(\\mathbf{L})$ be the eigenvalues of $\\mathbf{L}$. Since $\\mathbf{L}$ is symmetric and positive-definite, its eigenvalues are real and positive. The eigenvalues of $\\mathbf{A}_{\\text{step}} = \\mathbf{I} - \\Delta t\\,\\mathbf{L}$ are given by $1 - \\Delta t\\,\\lambda_j(\\mathbf{L})$. The spectral radius is then:\n$$\n\\rho(\\mathbf{A}_{\\text{step}}) = \\max_j |1 - \\Delta t\\,\\lambda_j(\\mathbf{L})|\n$$\nThe stability check involves computing the eigenvalues of $\\mathbf{L}$ for each case, calculating the eigenvalues of $\\mathbf{A}_{\\text{step}}$, finding their maximum absolute value, and comparing it to $1$.\n\n**3. Numerical Implementation and Simulation**\n\nThe final task is to implement the derived update rule and stability analysis. For a given constant deformation gradient $\\mathbf{F}$, initial state $\\mathbf{z}_0$, and number of steps $N$, we must simulate the evolution of $\\mathbf{z}$ and find the final state $\\mathbf{z}_N$. The simulation proceeds iteratively:\n$$\n\\mathbf{z}_{k+1} = (\\mathbf{I} - \\Delta t\\,\\mathbf{L})\\,\\mathbf{z}_k + \\Delta t\\,\\gamma\\,\\mathbf{s}(\\mathbf{F}) \\quad \\text{for } k = 0, 1, \\dots, N-1\n$$\nThe source term $\\mathbf{s}(\\mathbf{F})$ is constant throughout the simulation as $\\mathbf{F}$ is constant. The algorithm for each test case is:\n\n1.  Given $\\mathbf{L}$ and $\\Delta t$, compute the eigenvalues of $\\mathbf{L}$.\n2.  Compute the spectral radius $\\rho(\\mathbf{A}_{\\text{step}}) = \\max_j |1 - \\Delta t\\,\\lambda_j(\\mathbf{L})|$.\n3.  Determine the stability flag: `True` if $\\rho(\\mathbf{A}_{\\text{step}}) < 1$, `False` otherwise.\n4.  Given $\\mathbf{F}$, compute its SVD: $\\mathbf{F} = \\mathbf{U}_s\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$.\n5.  Construct the right stretch tensor $\\mathbf{U} = \\mathbf{V}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$.\n6.  Compute its logarithm $\\log\\mathbf{U} = \\mathbf{V}\\,(\\log\\boldsymbol{\\Sigma})\\,\\mathbf{V}^\\top$.\n7.  Calculate the trace $\\operatorname{tr}(\\log\\mathbf{U})$.\n8.  Compute the deviatoric tensor $\\operatorname{dev}(\\log\\mathbf{U}) = \\log\\mathbf{U} - \\frac{1}{2}\\operatorname{tr}(\\log\\mathbf{U})\\mathbf{I}$.\n9.  Extract the source vector $\\mathbf{s}(\\mathbf{F}) = [\\operatorname{dev}(\\log\\mathbf{U})_{11}, \\operatorname{dev}(\\log\\mathbf{U})_{22}]^\\top$.\n10. Initialize $\\mathbf{z} = \\mathbf{z}_0$.\n11. Loop $N$ times, applying the update rule: $\\mathbf{z} \\leftarrow (\\mathbf{I} - \\Delta t\\,\\mathbf{L})\\,\\mathbf{z} + \\Delta t\\,\\gamma\\,\\mathbf{s}(\\mathbf{F})$.\n12. The final vector $\\mathbf{z}$ is the result $\\mathbf{z}_N$.\n13. Collate the spectral radius, stability flag, and components of $\\mathbf{z}_N$ into the required output format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the physics-informed recurrent cell problem for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (stable interior)\n        {\n            \"L\": np.array([[6.0, 0.0], [0.0, 10.0]]),\n            \"gamma\": 2.0,\n            \"F\": np.array([[1.1, 0.0], [0.0, 0.95]]),\n            \"dt\": 0.15,\n            \"N\": 8,\n            \"z0\": np.array([0.0, 0.0])\n        },\n        # Case B (stability boundary)\n        {\n            \"L\": np.array([[4.0, 1.0], [1.0, 4.0]]),\n            \"gamma\": 1.5,\n            \"F\": np.array([[1.0, 0.2], [0.0, 1.0]]),\n            \"dt\": 0.4,\n            \"N\": 12,\n            \"z0\": np.array([0.1, -0.1])\n        },\n        # Case C (unstable)\n        {\n            \"L\": np.array([[8.0, 0.0], [0.0, 12.0]]),\n            \"gamma\": 0.5,\n            \"F\": np.array([[0.9, 0.0], [0.0, 1.05]]),\n            \"dt\": 0.21,\n            \"N\": 5,\n            \"z0\": np.array([0.0, 0.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L, gamma, F, dt, N, z0 = \\\n            case[\"L\"], case[\"gamma\"], case[\"F\"], case[\"dt\"], case[\"N\"], case[\"z0\"]\n\n        # 1. Stability Analysis\n        # Eigenvalues of L\n        eigvals_L = np.linalg.eigvalsh(L)\n        # Eigenvalues of the amplification matrix A_step = I - dt*L\n        eigvals_A_step = 1.0 - dt * eigvals_L\n        # Spectral radius\n        spectral_radius = np.max(np.abs(eigvals_A_step))\n        # Stability check (strict inequality)\n        is_stable = spectral_radius  1.0\n\n        # 2. Simulation\n        # Compute the source term s(F)\n        # SVD of F: F = U_svd @ Sigma @ Vh\n        _, s, Vh = np.linalg.svd(F)\n        V = Vh.T\n        Sigma = np.diag(s)\n        \n        # Right stretch tensor U = V @ Sigma @ V.T\n        U_tensor = V @ Sigma @ V.T\n        \n        # Logarithmic strain log(U) = V @ log(Sigma) @ V.T\n        log_Sigma = np.diag(np.log(s))\n        log_U = V @ log_Sigma @ V.T\n        \n        # Deviatoric part of log(U)\n        tr_log_U = np.trace(log_U)\n        dev_log_U = log_U - (tr_log_U / 2.0) * np.identity(2)\n        \n        # Source vector s(F)\n        s_F = np.array([dev_log_U[0, 0], dev_log_U[1, 1]])\n        \n        # Pre-compute matrices for the simulation loop\n        A = np.identity(2) - dt * L\n        b = dt * gamma * s_F\n        \n        # Run simulation\n        z_k = z0.copy()\n        for _ in range(N):\n            z_k = A @ z_k + b\n        \n        z_N = z_k\n\n        # Format results for output\n        # Round all floating-point numbers to 6 decimal places\n        rho_out = round(spectral_radius, 6)\n        zN1_out = round(z_N[0], 6)\n        zN2_out = round(z_N[1], 6)\n        \n        # Append formatted sublist\n        results.append(f\"[{rho_out:.6f},{'True' if is_stable else 'False'},{zN1_out:.6f},{zN2_out:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2898867"}, {"introduction": "The ultimate test of a data-driven constitutive model is its performance in a practical simulation. This capstone practice puts a learned viscoplastic model to work by embedding it within a numerical simulation of a material subjected to cyclic loading [@problem_id:2898921]. Your task is to implement the model's predictive equations, integrate the material's state over time to capture its stress-strain response, and calculate a key physical quantity: the energy dissipated per cycle. By comparing your numerical results with a known analytical solution, you will gain hands-on experience in deploying a surrogate model and validating its physical fidelity.", "problem": "Consider a one-dimensional bar subjected to a prescribed sinusoidal total strain history and modeled with a data-driven viscoplastic surrogate that was learned offline. Let the total strain be $\\epsilon(t)$, the plastic strain be $\\epsilon_p(t)$, and the Cauchy stress be $\\sigma(t)$. Assume small strains so that additive decomposition holds, and linear elasticity for the elastic part. The constitutive relations are:\n- Kinematics and elasticity: $\\sigma(t) = E \\, \\big(\\epsilon(t) - \\epsilon_p(t)\\big)$, where $E$ is the Young’s modulus.\n- Learned viscoplastic flow rule: the plastic strain rate is given by a learned scalar surrogate,\n$$\n\\dot{\\epsilon}_p(t) = \\mathcal{N}(\\sigma(t)) = \\frac{1}{\\eta} \\, \\mathcal{O}(\\sigma(t)) \\, \\mathrm{sign}\\big(\\sigma(t)\\big),\n$$\nwhere $\\eta$ is an effective viscosity learned from data, and $\\mathcal{O}(\\sigma)$ is a smooth approximation of the overstress relative to a learned yield strength $\\sigma_y$,\n$$\n\\mathcal{O}(\\sigma) = s_{\\mathrm{sm}} \\, \\ln\\!\\Big(1 + \\exp\\!\\Big(\\frac{|\\sigma| - \\sigma_y}{s_{\\mathrm{sm}}}\\Big)\\Big).\n$$\nHere $s_{\\mathrm{sm}}$ is a smoothing parameter (in stress units) learned to approximate the ramp function $\\max(|\\sigma|-\\sigma_y,0)$ while keeping differentiability.\n\nStart from the fundamental mechanical dissipation definition given by the Clausius–Duhem inequality: the local inelastic power density is $d(t) = \\sigma(t) \\, \\dot{\\epsilon}_p(t) \\ge 0$. The energy dissipated per cycle (energy density) is then\n$$\nW = \\int_{t_0}^{t_0+T} \\sigma(t) \\, \\dot{\\epsilon}_p(t) \\, dt,\n$$\nwhere $T$ is the period and the integral is taken over one stabilized loading cycle. In uniaxial elastic–perfectly plastic rate-independent theory with yield stress $\\sigma_y$ under strain control, if the strain amplitude $\\epsilon_a$ is below the yield strain $\\epsilon_y = \\sigma_y/E$, the response is elastic and $W=0$. If $\\epsilon_a  \\epsilon_y$, the stabilized stress–strain loop area equals the dissipation per cycle and is\n$$\nW_{\\mathrm{analytical}} = 4 \\, \\sigma_y \\, \\big(\\epsilon_a - \\epsilon_y\\big) \\quad \\text{for} \\quad \\epsilon_a  \\epsilon_y, \\quad \\text{and} \\quad W_{\\mathrm{analytical}} = 0 \\quad \\text{otherwise}.\n$$\nThis result follows from the piecewise linear elastic–perfectly plastic stress–strain path under fully reversed strain control, where plastic flow at $\\pm \\sigma_y$ occurs over a strain range of width $2\\big(\\epsilon_a - \\epsilon_y\\big)$ in each half-cycle.\n\nYour task is to:\n1. Numerically integrate the learned viscoplastic model over several cycles of sinusoidal total strain $\\epsilon(t) = \\epsilon_a \\sin(2\\pi f t)$ to reach a stabilized response, and compute the numerical dissipation per cycle $W_{\\mathrm{learned}} = \\int \\sigma \\, \\dot{\\epsilon}_p \\, dt$ over the last full cycle.\n2. Compute the analytical elastic–perfectly plastic reference $W_{\\mathrm{analytical}}$ from the expression above.\n3. For each test case, return a list of three floats: $[W_{\\mathrm{learned}}, W_{\\mathrm{analytical}}, |W_{\\mathrm{learned}} - W_{\\mathrm{analytical}}|]$.\n\nUse the following fixed learned parameters (obtained from prior training and held constant across test cases):\n- Young’s modulus: $E = 210000 \\ \\mathrm{MPa}$.\n- Yield stress surrogate: $\\sigma_y = 250 \\ \\mathrm{MPa}$.\n- Viscosity surrogate: $\\eta = 1000 \\ \\mathrm{MPa \\cdot s}$.\n- Smoothing parameter: $s_{\\mathrm{sm}} = 5 \\ \\mathrm{MPa}$.\n\nSinusoidal strain input: $\\epsilon(t) = \\epsilon_a \\sin(2\\pi f t)$ with specified amplitude $\\epsilon_a$ and frequency $f$.\n\nEnergy units: Express the dissipation per cycle $W$ in $\\mathrm{MJ/m^3}$ (noting that $1 \\ \\mathrm{MPa} = 1 \\ \\mathrm{MJ/m^3}$), and report each of the three floats in these units.\n\nAngle units: If any angles appear, they must be in radians. In this problem, time–frequency relationships use radians implicitly via $2\\pi f$.\n\nDesign details for numerical integration:\n- Use a uniform time step $\\Delta t$ with at least $4000$ steps per cycle to ensure numerical accuracy.\n- Simulate at least $6$ full cycles and compute the integral only over the last complete cycle to approximate the stabilized response.\n- Time integration of the internal variable should use a consistent explicit method where $\\epsilon_p(t+\\Delta t) \\approx \\epsilon_p(t) + \\Delta t \\, \\mathcal{N}(\\sigma(t))$.\n- Use the trapezoidal rule for the time integral of $\\sigma \\, \\dot{\\epsilon}_p$ over the measured cycle.\n\nTest suite:\n- Case A (elastic regime, below yield): $\\epsilon_a = 0.0005$, $f = 1.0 \\ \\mathrm{Hz}$.\n- Case B (boundary to yielding): $\\epsilon_a = \\epsilon_y = \\sigma_y/E$, $f = 1.0 \\ \\mathrm{Hz}$.\n- Case C (moderate plasticity): $\\epsilon_a = 0.0020$, $f = 1.0 \\ \\mathrm{Hz}$.\n- Case D (dominant plasticity): $\\epsilon_a = 0.0100$, $f = 1.0 \\ \\mathrm{Hz}$.\n\nYour program must produce a single line of output containing the results as a comma-separated Python-style list of the four case results, where each case result is itself a list of three floats: $[W_{\\mathrm{learned}}, W_{\\mathrm{analytical}}, |W_{\\mathrm{learned}} - W_{\\mathrm{analytical}}|]$. For example, the output format must be exactly like:\n\"[ [wA_learned,wA_analytical,wA_absdiff], [wB_learned,wB_analytical,wB_absdiff], [wC_learned,wC_analytical,wC_absdiff], [wD_learned,wD_analytical,wD_absdiff] ]\".", "solution": "The problem presented is a well-posed exercise in computational materials science, specifically in the domain of inelastic constitutive modeling. It is scientifically grounded, internally consistent, and contains all necessary information to proceed with a numerical solution. The model, while representing a \"learned\" surrogate, is given in an explicit functional form, reducing the problem to a standard simulation of a viscoplastic material under cyclic loading. The premises are sound, and the task is to implement the given model, perform a numerical experiment, and compare the result to a known analytical solution for a simplified, reference material model. The problem is therefore valid.\n\nThe objective is to compute the energy dissipated per cycle, $W$, for a one-dimensional bar described by a specific data-driven viscoplastic constitutive law. This computed dissipation, $W_{\\mathrm{learned}}$, is then compared against an analytical benchmark, $W_{\\mathrm{analytical}}$, derived for a classical rate-independent elastic-perfectly plastic model.\n\nThe foundation of the simulation is the time integration of the state evolution law for the plastic strain, $\\epsilon_p$. The governing ordinary differential equation (ODE) is given by the flow rule:\n$$\n\\dot{\\epsilon}_p(t) = \\frac{d\\epsilon_p}{dt} = \\mathcal{N}(\\sigma(t)) = \\frac{1}{\\eta} \\, s_{\\mathrm{sm}} \\, \\ln\\!\\Big(1 + \\exp\\!\\Big(\\frac{|\\sigma(t)| - \\sigma_y}{s_{\\mathrm{sm}}}\\Big)\\Big) \\, \\mathrm{sign}\\big(\\sigma(t)\\big)\n$$\nThe stress, $\\sigma(t)$, which drives the plastic flow, is itself dependent on the plastic strain via the linear elastic relation $\\sigma(t) = E \\, \\big(\\epsilon(t) - \\epsilon_p(t)\\big)$. The total strain, $\\epsilon(t)$, is a prescribed function of time, serving as the external driver for the system: $\\epsilon(t) = \\epsilon_a \\sin(2\\pi f t)$.\n\nTo solve this system numerically, we discretize time with a uniform step $\\Delta t$. The simulation advances from time $t_i$ to $t_{i+1} = t_i + \\Delta t$. The state variable $\\epsilon_p$ is updated using the explicit forward Euler method, as specified:\n$$\n\\epsilon_p(t_{i+1}) = \\epsilon_p(t_i) + \\Delta t \\cdot \\dot{\\epsilon}_p(t_i)\n$$\nwhere the rate $\\dot{\\epsilon}_p(t_i)$ is computed using the state variables known at time $t_i$. The simulation starts from an undeformed state, $\\epsilon_p(0) = 0$, and runs for $6$ complete cycles to ensure that the transient response has decayed and the system has reached a stabilized cyclic state, also known as a limit cycle.\n\nThe primary quantity of interest is the dissipated energy per cycle, which is the mechanical work done by the stress over the plastic strain increment, integrated over one period $T=1/f$ of the stabilized cycle. This is given by the integral:\n$$\nW_{\\mathrm{learned}} = \\int_{t_0}^{t_0+T} \\sigma(t) \\dot{\\epsilon}_p(t) \\, dt\n$$\nIn our numerical implementation, this integral is computed over the final ($6$-th) cycle of the simulation. At each time step $t_i$ of this cycle, we have values for stress $\\sigma(t_i)$ and plastic strain rate $\\dot{\\epsilon}_p(t_i)$. The integral is then approximated numerically using the trapezoidal rule, which provides a second-order accurate estimate:\n$$\nW_{\\mathrm{learned}} \\approx \\sum_{i=\\text{start}}^{\\text{end}-1} \\frac{\\big(\\sigma(t_i)\\dot{\\epsilon}_p(t_i) + \\sigma(t_{i+1})\\dot{\\epsilon}_p(t_{i+1})\\big)}{2} \\Delta t\n$$\nwhere the summation indices span the last cycle.\n\nFor comparison, the analytical dissipation $W_{\\mathrm{analytical}}$ for an ideal elastic-perfectly plastic material with yield stress $\\sigma_y$ is calculated. For a strain amplitude $\\epsilon_a$ less than or equal to the yield strain $\\epsilon_y = \\sigma_y/E$, the response is purely elastic and the dissipation is zero. For strain amplitudes exceeding the yield strain, plastic deformation occurs, and the dissipation per cycle is given by the area of the stabilized rectangular hysteresis loop:\n$$\nW_{\\mathrm{analytical}} = \\begin{cases} 4 \\, \\sigma_y \\left(\\epsilon_a - \\epsilon_y\\right)  \\text{if } \\epsilon_a  \\epsilon_y \\\\ 0  \\text{if } \\epsilon_a \\le \\epsilon_y \\end{cases}\n$$\nThe absolute difference $|W_{\\mathrm{learned}} - W_{\\mathrm{analytical}}|$ quantifies the deviation of the smooth, rate-dependent viscoplastic model from the sharp, rate-independent ideal plastic model. This difference arises from two main sources: the rate-dependence introduced by the viscosity $\\eta$, and the smooth transition from elastic to plastic behavior governed by the smoothing parameter $s_{\\mathrm{sm}}$.\n\nThe procedure is implemented for each of the four specified test cases, using the provided material parameters: $E = 210000 \\ \\mathrm{MPa}$, $\\sigma_y = 250 \\ \\mathrm{MPa}$, $\\eta = 1000 \\ \\mathrm{MPa \\cdot s}$, and $s_{\\mathrm{sm}} = 5 \\ \\mathrm{MPa}$. The numerical integration uses a time step $\\Delta t$ corresponding to $4000$ steps per cycle to ensure sufficient accuracy. The final results, $[W_{\\mathrm{learned}}, W_{\\mathrm{analytical}}, |W_{\\mathrm{learned}} - W_{\\mathrm{analytical}}|]$, are reported in units of $\\mathrm{MJ/m^3}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the data-driven viscoplasticity problem by simulating cyclic loading\n    and computing energy dissipation.\n    \"\"\"\n\n    # Fixed learned parameters and simulation settings\n    E = 210000.0           # Young's modulus in MPa\n    SIGMA_Y = 250.0        # Yield stress surrogate in MPa\n    ETA = 1000.0           # Viscosity surrogate in MPa·s\n    S_SM = 5.0             # Smoothing parameter in MPa\n    NUM_CYCLES = 6         # Number of cycles to simulate\n    STEPS_PER_CYCLE = 4000 # Time steps per cycle\n    \n    def run_simulation(eps_a, f):\n        \"\"\"\n        Performs the numerical integration for a single test case.\n        \n        Args:\n            eps_a (float): Total strain amplitude.\n            f (float): Loading frequency in Hz.\n            \n        Returns:\n            list[float]: A list containing [W_learned, W_analytical, |W_learned - W_analytical|].\n        \"\"\"\n        # Calculate derived simulation parameters\n        period = 1.0 / f\n        delta_t = period / STEPS_PER_CYCLE\n        total_steps = NUM_CYCLES * STEPS_PER_CYCLE\n        time = np.linspace(0, NUM_CYCLES * period, total_steps + 1, dtype=np.float64)\n\n        # Initialize arrays to store time histories\n        epsilon_p = np.zeros(total_steps + 1, dtype=np.float64)\n        sigma = np.zeros(total_steps + 1, dtype=np.float64)\n        epsilon_p_dot = np.zeros(total_steps + 1, dtype=np.float64)\n\n        # Time integration loop using explicit forward Euler method\n        for i in range(total_steps + 1):\n            # Calculate total strain at current time step t_i\n            eps_total_i = eps_a * np.sin(2 * np.pi * f * time[i])\n            \n            # Calculate stress based on plastic strain from previous update\n            # sigma(t_i) = E * (epsilon(t_i) - epsilon_p(t_i))\n            sigma_i = E * (eps_total_i - epsilon_p[i])\n            sigma[i] = sigma_i\n            \n            # Calculate plastic strain rate eps_p_dot(t_i) using the learned flow rule\n            # Overstress function O(sigma)\n            # Use np.longdouble for intermediate exp calculation to avoid overflow warnings,\n            # though numpy's float64 handles it by returning inf.\n            arg_exp = (np.abs(sigma_i) - SIGMA_Y) / S_SM\n            O_sigma_i = S_SM * np.log(1 + np.exp(arg_exp, dtype=np.longdouble))\n            \n            # Plastic strain rate N(sigma)\n            eps_p_dot_i = (1.0 / ETA) * float(O_sigma_i) * np.sign(sigma_i)\n            epsilon_p_dot[i] = eps_p_dot_i\n            \n            # Update plastic strain for the next step: epsilon_p(t_{i+1})\n            if i  total_steps:\n                epsilon_p[i+1] = epsilon_p[i] + delta_t * eps_p_dot_i\n                \n        # Calculate learned dissipation W_learned over the last full cycle\n        start_idx = (NUM_CYCLES - 1) * STEPS_PER_CYCLE\n        end_idx = NUM_CYCLES * STEPS_PER_CYCLE\n        \n        # Integrand for dissipation: sigma(t) * eps_p_dot(t)\n        integrand_slice = (sigma * epsilon_p_dot)[start_idx : end_idx + 1]\n        \n        # Integrate using the trapezoidal rule\n        w_learned = np.trapz(integrand_slice, dx=delta_t)\n        \n        # Calculate analytical dissipation for the reference elastic-perfectly plastic model\n        eps_y = SIGMA_Y / E\n        if eps_a > eps_y:\n            w_analytical = 4.0 * SIGMA_Y * (eps_a - eps_y)\n        else:\n            w_analytical = 0.0\n            \n        # Calculate the absolute difference\n        w_absdiff = np.abs(w_learned - w_analytical)\n        \n        return [w_learned, w_analytical, w_absdiff]\n\n    # Define the test cases from the problem statement.\n    eps_y_val = SIGMA_Y / E\n    test_cases = [\n        (0.0005, 1.0),          # Case A\n        (eps_y_val, 1.0),       # Case B\n        (0.0020, 1.0),          # Case C\n        (0.0100, 1.0)           # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        eps_a_case, f_case = case\n        result = run_simulation(eps_a_case, f_case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation for a list includes spaces, which is fine.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2898921"}]}