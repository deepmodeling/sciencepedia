{"hands_on_practices": [{"introduction": "A fundamental task in state estimation is designing an observer that ensures the estimation error converges to zero. This practice explores two principal philosophies for this task: deterministic pole placement and stochastic optimal estimation via the Kalman filter. By first deriving the conditions for arbitrary pole placement and then contrasting it with the calculation of an optimal Kalman gain, you will gain a deep, practical understanding of when and why each method is used [@problem_id:2748160].", "problem": "Consider a continuous-time, linear, time-invariant system with state-space matrices $A \\in \\mathbb{R}^{n \\times n}$ and $C \\in \\mathbb{R}^{m \\times n}$. The state observer error dynamics under a static output injection gain $L \\in \\mathbb{R}^{n \\times m}$ are governed by the matrix $A - L C$.\n\nPart 1. Starting only from the definitions of observability and controllability and the Popov–Belevitch–Hautus test, derive why, for any observable pair $(A,C)$ and for any monic polynomial $p(\\lambda) = \\lambda^{n} + \\alpha_{n-1} \\lambda^{n-1} + \\cdots + \\alpha_{1} \\lambda + \\alpha_{0}$ with roots strictly in the open left half-plane, there exists an output injection matrix $L$ such that the characteristic polynomial of $A - L C$ equals $p(\\lambda)$. Your derivation must proceed from first principles and fundamental definitions, without invoking pre-packaged formulas.\n\nPart 2. Specialize to the second-order system with\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}.\n$$\nChoose the desired eigenvalues for $A - L C$ to be $\\lambda_{1} = -2$ and $\\lambda_{2} = -5$. Determine explicitly the entries of $L = \\begin{pmatrix} \\ell_{1} \\\\ \\ell_{2} \\end{pmatrix}$ that realize these eigenvalues by direct computation of the characteristic polynomial of $A - L C$ and coefficient matching.\n\nPart 3. For the same system in Part $2$, consider the stochastic state-space model\n$$\n\\dot{x}(t) = A x(t) + w(t), \\quad y(t) = C x(t) + v(t),\n$$\nwith zero-mean, white, Gaussian process noise $w(t)$ and measurement noise $v(t)$, mutually independent, with covariances $\\mathbb{E}[w(t) w(\\tau)^{\\top}] = Q \\delta(t - \\tau)$ and $\\mathbb{E}[v(t) v(\\tau)] = R \\delta(t - \\tau)$, where\n$$\nQ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad R = 1.\n$$\nCompute the steady-state Kalman filter gain $L_{\\mathrm{KF}} \\in \\mathbb{R}^{2 \\times 1}$ by solving the continuous-time algebraic Riccati equation for the error covariance $P \\in \\mathbb{R}^{2 \\times 2}$ and evaluating $L_{\\mathrm{KF}} = P C^{\\top} R^{-1}$. Then, in words, contrast the conceptual basis of the eigenvalue placement gain in Part $2$ with the steady-state Kalman gain derived here.\n\nAnswer specification: Report your final answer as a row matrix containing the four entries $\\ell_{1}$, $\\ell_{2}$, $k_{1}$, $k_{2}$, where $L = \\begin{pmatrix} \\ell_{1} \\\\ \\ell_{2} \\end{pmatrix}$ is the eigenvalue placement gain from Part $2$ and $L_{\\mathrm{KF}} = \\begin{pmatrix} k_{1} \\\\ k_{2} \\end{pmatrix}$ is the Kalman gain from Part $3$. Provide the exact values; do not round.", "solution": "The problem is subjected to validation and found to be well-posed, scientifically sound, and internally consistent. It is a standard problem in linear systems theory and optimal estimation. We shall proceed with the solution.\n\nPart 1: Derivation of Observer Pole Placement\nThe objective is to demonstrate that for an observable pair $(A,C)$, where $A \\in \\mathbb{R}^{n \\times n}$ and $C \\in \\mathbb{R}^{m \\times n}$, it is always possible to find an output injection gain $L \\in \\mathbb{R}^{n \\times m}$ such that the characteristic polynomial of the observer error dynamics matrix, $A-LC$, matches any specified monic polynomial $p(\\lambda)$ of degree $n$. This property is known as pole placement or pole assignment for observers. The derivation will rely on the principle of duality in control theory.\n\nThe poles of the observer are the eigenvalues of the matrix $A-LC$. The characteristic polynomial of $A-LC$ is given by $\\det(\\lambda I - (A-LC))$. A fundamental property of determinants is that $\\det(M) = \\det(M^{\\top})$ for any square matrix $M$. Therefore, the eigenvalues of $A-LC$ are identical to the eigenvalues of its transpose, $(A-LC)^{\\top} = A^{\\top} - C^{\\top}L^{\\top}$.\n\nLet us define a \"dual\" system. For the primal system governed by matrices $(A, C)$, we define a dual system with state matrix $A' = A^{\\top}$ and input matrix $B' = C^{\\top}$. We also define a state feedback gain for this dual system as $K' = L^{\\top}$. The closed-loop matrix for this dual system under state feedback is $A' - B'K' = A^{\\top} - C^{\\top}L^{\\top}$.\n\nFrom this construction, it is evident that the problem of finding an observer gain $L$ to place the eigenvalues of $A-LC$ at desired locations is mathematically equivalent to the problem of finding a state-feedback gain $K' = L^{\\top}$ to place the eigenvalues of $A^{\\top} - C^{\\top}L^{\\top}$ at the same locations. The latter is the standard state-feedback pole placement problem for the system $(A', B') = (A^{\\top}, C^{\\top})$.\n\nA cornerstone theorem of control theory states that arbitrary pole placement via state feedback is possible if and only if the system is controllable. Therefore, if we can prove that the dual system $(A^{\\top}, C^{\\top})$ is controllable, then we can always find a gain $K'$ (and thus an $L$) to achieve any desired characteristic polynomial. The linkage between the observability of the primal system and the controllability of the dual system is established using the Popov–Belevitch–Hautus (PBH) test.\n\nThe PBH test for observability states that the pair $(A, C)$ is observable if and only if the matrix\n$$\n\\begin{pmatrix} A - \\lambda I \\\\ C \\end{pmatrix}\n$$\nhas full column rank $n$ for all complex numbers $\\lambda \\in \\mathbb{C}$.\n\nThe problem statement posits that the pair $(A,C)$ is observable. Thus, for any $\\lambda \\in \\mathbb{C}$,\n$$\n\\mathrm{rank} \\begin{pmatrix} A - \\lambda I \\\\ C \\end{pmatrix} = n.\n$$\nSince the rank of a matrix is equal to the rank of its transpose, we have\n$$\n\\mathrm{rank} \\left( \\begin{pmatrix} A - \\lambda I \\\\ C \\end{pmatrix}^{\\top} \\right) = \\mathrm{rank} \\begin{pmatrix} (A - \\lambda I)^{\\top} & C^{\\top} \\end{pmatrix} = \\mathrm{rank} \\begin{pmatrix} A^{\\top} - \\lambda I & C^{\\top} \\end{pmatrix} = n.\n$$\nThis last condition, $\\mathrm{rank} \\begin{pmatrix} A^{\\top} - \\lambda I & C^{\\top} \\end{pmatrix} = n$ for all $\\lambda \\in \\mathbb{C}$, is precisely the PBH test for controllability of the pair $(A^{\\top}, C^{\\top})$.\n\nTherefore, if the pair $(A,C)$ is observable, the pair $(A^{\\top}, C^{\\top})$ is controllable. By the state-feedback pole placement theorem, for a controllable system, a gain matrix $K'$ can be found to assign the eigenvalues of the closed-loop system $A^{\\top}-C^{\\top}K'$ to any desired set of locations. These locations are the roots of the specified monic polynomial $p(\\lambda)$. By setting $K'=L^{\\top}$, we find the observer gain $L$ that places the poles of the observer dynamics matrix $A-LC$ at these same locations. The requirement that the roots of $p(\\lambda)$ lie in the open left half-plane ensures the stability of the observer, which is the practical objective. This completes the derivation.\n\nPart 2: Computation of Eigenvalue Placement Gain\nWe are given the system with matrices\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}.\n$$\nThe output injection gain is $L = \\begin{pmatrix} \\ell_{1} \\\\ \\ell_{2} \\end{pmatrix}$. The matrix $A-LC$ is computed as:\n$$\nA - LC = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} \\ell_{1} \\\\ \\ell_{2} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} \\ell_{1} & 0 \\\\ \\ell_{2} & 0 \\end{pmatrix} = \\begin{pmatrix} -\\ell_{1} & 1 \\\\ -\\ell_{2} & 0 \\end{pmatrix}.\n$$\nThe characteristic polynomial of $A-LC$ is\n$$\n\\det(\\lambda I - (A-LC)) = \\det \\begin{pmatrix} \\lambda + \\ell_{1} & -1 \\\\ \\ell_{2} & \\lambda \\end{pmatrix} = \\lambda(\\lambda + \\ell_{1}) - (-1)(\\ell_{2}) = \\lambda^{2} + \\ell_{1}\\lambda + \\ell_{2}.\n$$\nThe desired eigenvalues are $\\lambda_{1} = -2$ and $\\lambda_{2} = -5$. The desired characteristic polynomial is\n$$\np(\\lambda) = (\\lambda - \\lambda_{1})(\\lambda - \\lambda_{2}) = (\\lambda + 2)(\\lambda + 5) = \\lambda^{2} + 7\\lambda + 10.\n$$\nBy matching the coefficients of the computed and desired characteristic polynomials, we obtain:\n$$\n\\ell_{1} = 7, \\quad \\ell_{2} = 10.\n$$\nThus, the required gain matrix is $L = \\begin{pmatrix} 7 \\\\ 10 \\end{pmatrix}$.\n\nPart 3: Computation of Steady-State Kalman Gain and Conceptual Contrast\nWe are given process and measurement noise with covariance intensities $Q=I$ and $R=1$. The steady-state Kalman filter gain $L_{\\mathrm{KF}}$ is given by $L_{\\mathrm{KF}} = PC^{\\top}R^{-1}$, where $P$ is the symmetric, positive-definite solution to the continuous-time algebraic Riccati equation (CARE):\n$$\nAP + PA^{\\top} - PC^{\\top}R^{-1}CP + Q = 0.\n$$\nLet $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$. Substituting the given matrices $A, C, Q, R$:\n$$\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} - \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nEvaluating each term:\n$$\n\\begin{pmatrix} p_{12} & p_{22} \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} p_{12} & 0 \\\\ p_{22} & 0 \\end{pmatrix} - \\begin{pmatrix} p_{11}^{2} & p_{11}p_{12} \\\\ p_{11}p_{12} & p_{12}^{2} \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nThis matrix equation yields a system of algebraic equations for the entries of $P$:\n1. $2p_{12} - p_{11}^{2} + 1 = 0$\n2. $p_{22} - p_{11}p_{12} = 0$\n3. $-p_{12}^{2} + 1 = 0$\n\nFrom equation ($3$), we find $p_{12}^{2} = 1$, which gives $p_{12} = \\pm 1$. The eigenvalues of the Kalman filter error dynamics matrix $A - L_{\\mathrm{KF}}C = \\begin{pmatrix} -p_{11} & 1 \\\\ -p_{12} & 0 \\end{pmatrix}$ must lie in the open left half-plane for stability. The characteristic polynomial is $\\lambda^{2} + p_{11}\\lambda + p_{12} = 0$. For stable roots, the Routh-Hurwitz stability criterion requires all coefficients to be positive. Thus, we must have $p_{11} > 0$ and $p_{12} > 0$. We must therefore select $p_{12} = 1$.\n\nSubstituting $p_{12}=1$ into equation ($1$):\n$$\n2(1) - p_{11}^{2} + 1 = 0 \\implies p_{11}^{2} = 3.\n$$\nSince we require $p_{11} > 0$, we have $p_{11} = \\sqrt{3}$.\n\nSubstituting $p_{11}=\\sqrt{3}$ and $p_{12}=1$ into equation ($2$):\n$$\np_{22} - (\\sqrt{3})(1) = 0 \\implies p_{22} = \\sqrt{3}.\n$$\nThe steady-state error covariance is $P = \\begin{pmatrix} \\sqrt{3} & 1 \\\\ 1 & \\sqrt{3} \\end{pmatrix}$. We verify that this matrix is positive definite: $p_{11}=\\sqrt{3} > 0$ and $\\det(P) = (\\sqrt{3})(\\sqrt{3}) - 1^{2} = 3-1=2>0$.\n\nThe steady-state Kalman gain $L_{\\mathrm{KF}}$ is then:\n$$\nL_{\\mathrm{KF}} = PC^{\\top}R^{-1} = \\begin{pmatrix} \\sqrt{3} & 1 \\\\ 1 & \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} \\sqrt{3} \\\\ 1 \\end{pmatrix}.\n$$\nLetting $L_{\\mathrm{KF}} = \\begin{pmatrix} k_{1} \\\\ k_{2} \\end{pmatrix}$, we have $k_{1} = \\sqrt{3}$ and $k_{2} = 1$.\n\nConceptual Contrast:\nThe eigenvalue placement gain $L$ from Part $2$ is chosen deterministically to achieve an arbitrary, user-specified dynamic response for the observer error. The choice of poles (here, $-2$ and $-5$) is dictated by design specifications like desired settling time or damping, but is otherwise arbitrary. This method completely ignores any stochastic aspects of the system, such as process or measurement noise.\n\nIn stark contrast, the steady-state Kalman gain $L_{\\mathrm{KF}}$ from Part $3$ is not arbitrary. It is uniquely determined as the optimal gain that minimizes the variance of the state estimation error, based on the statistical properties of the system, namely the process noise covariance $Q$ and the measurement noise covariance $R$. The resulting observer poles are not directly specified by the designer but are an outcome of this optimization. The Kalman filter provides the best possible estimate in the mean-square sense by systematically balancing the confidence in the system model against the confidence in the measurements. The pole placement method enforces a desired behavior, while the Kalman filter derives an optimal behavior from the underlying stochastic reality of the system.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7 & 10 & \\sqrt{3} & 1 \\end{pmatrix}}\n$$", "id": "2748160"}, {"introduction": "The performance of a Kalman filter is critically dependent on the statistical models of process noise ($Q$) and measurement noise ($R$). This exercise provides a hands-on analysis of how perturbations in these noise covariances affect the steady-state estimation error, governed by the algebraic Riccati equation. By working through a carefully constructed scenario, you will uncover an elegant scaling relationship, deepening your intuition for filter tuning and the fundamental trade-off between trusting the model versus the measurements [@problem_id:2748182].", "problem": "Consider the continuous-time linear time-invariant (LTI) stochastic system\n$$\\dot{x}(t)=A\\,x(t)+w(t),\\quad y(t)=C\\,x(t)+v(t),$$\nwith\n$$A=\\begin{pmatrix}1&0\\\\0&-2\\end{pmatrix},\\quad C=\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix},$$\nand white Gaussian process noise and measurement noise characterized by covariances\n$$Q=q\\,I_2,\\quad R=r\\,I_2,$$\nwith $q=1$ and $r=1$. Assume standard detectability and stabilizability conditions hold so that the steady-state error covariance $P\\succ 0$ of the Kalman-Bucy filter exists, where $P$ is the unique positive-definite solution to the steady-state estimation error covariance equation. Define the principal variances as the eigenvalues of $P$.\n\nTwo perturbations of the noise statistics are considered separately:\n- Doubling the measurement noise covariance to $R_{\\mathrm{d}}=2\\,R$ while keeping $Q$ unchanged.\n- Halving the process noise covariance to $Q_{\\mathrm{h}}=\\tfrac{1}{2}\\,Q$ while keeping $R$ unchanged.\n\nLet $P_{\\mathrm{d}}$ and $P_{\\mathrm{h}}$ denote the corresponding steady-state error covariance matrices under $R_{\\mathrm{d}}$ and $Q_{\\mathrm{h}}$, respectively. Using first principles of continuous-time state estimation and without invoking any shortcut results beyond core definitions, derive how these changes affect the principal variances and compute the scalar ratio\n$$\\Gamma=\\frac{\\det\\!\\big(P_{\\mathrm{d}}\\big)}{\\det\\!\\big(P_{\\mathrm{h}}\\big)}.$$\nExpress your final answer as a single real number without units. No rounding is required.", "solution": "We begin from the continuous-time Kalman-Bucy filter framework. For the stochastic system\n$$\\dot{x}(t)=A\\,x(t)+w(t),\\quad y(t)=C\\,x(t)+v(t),$$\nwith process noise covariance $Q$ and measurement noise covariance $R$, the steady-state estimation error covariance $P$ satisfies the continuous-time algebraic Riccati equation\n$$A\\,P+P\\,A^{\\top}+Q-P\\,C^{\\top}R^{-1}C\\,P=0.$$\nThis is a core, well-tested result in continuous-time state estimation.\n\nWith the given data,\n$$A=\\begin{pmatrix}1&0\\\\0&-2\\end{pmatrix},\\quad C=I_2,\\quad Q=q\\,I_2,\\quad R=r\\,I_2,$$\nand $q=1$, $r=1$. Because $A$, $C$, $Q$, and $R$ are all diagonal (with $C=I_2$), the Riccati equation decouples into two independent scalar equations, one for each coordinate. Let $a_1=1$ and $a_2=-2$ denote the diagonal entries of $A$, and let $p_i$ denote the $i$-th principal variance (eigenvalue) of $P$. For a general scalar mode with drift $a$ and scalar covariances $q$ and $r$, the steady-state scalar Riccati equation reads\n$$2\\,a\\,p+q-\\frac{p^2}{r}=0.$$\nRewriting,\n$$\\frac{p^2}{r}-2\\,a\\,p-q=0,$$\nwhich is a quadratic equation in $p$ with solutions\n$$p=r\\left(a\\pm \\sqrt{a^2+\\frac{q}{r}}\\right).$$\nThe physically relevant (stabilizing) solution is selected by requiring that the estimation error dynamics matrix $A-K\\,C$ be Hurwitz, where the steady-state Kalman gain is $K=P\\,C^{\\top}R^{-1}=\\frac{p}{r}$. In scalar form, this requires $a-\\frac{p}{r}<0$. Choosing the plus sign yields\n$$\\frac{p}{r}=a+\\sqrt{a^2+\\frac{q}{r}},$$\nso that\n$$a-\\frac{p}{r}=a-\\left(a+\\sqrt{a^2+\\frac{q}{r}}\\right)=-\\sqrt{a^2+\\frac{q}{r}}<0,$$\nensuring stability. The minus sign would give $a-\\frac{p}{r}=+\\sqrt{a^2+\\frac{q}{r}}>0$, which is not stabilizing. Therefore, for any scalar mode,\n$$p=r\\left(a+\\sqrt{a^2+\\frac{q}{r}}\\right).$$\n\nWe now apply this to the two perturbations.\n\n1) Doubling the measurement noise covariance: $R_{\\mathrm{d}}=2\\,r\\,I_2$ with $q$ unchanged. For a scalar mode with drift $a$, the corresponding principal variance $p_{\\mathrm{d}}$ satisfies\n$$p_{\\mathrm{d}}=2\\,r\\left(a+\\sqrt{a^2+\\frac{q}{2\\,r}}\\right).$$\n\n2) Halving the process noise covariance: $Q_{\\mathrm{h}}=\\tfrac{1}{2}\\,q\\,I_2$ with $r$ unchanged. For the same scalar mode with drift $a$, the corresponding principal variance $p_{\\mathrm{h}}$ satisfies\n$$p_{\\mathrm{h}}=r\\left(a+\\sqrt{a^2+\\frac{q/2}{r}}\\right)=r\\left(a+\\sqrt{a^2+\\frac{q}{2\\,r}}\\right).$$\n\nComparing these two expressions for an arbitrary scalar mode with the same $a$, we obtain the exact proportionality\n$$p_{\\mathrm{d}}=2\\,p_{\\mathrm{h}}.$$\nThis proportionality holds for each decoupled mode independently and therefore for both principal variances of the $2\\times 2$ covariance matrix. Hence, if $P_{\\mathrm{h}}$ has eigenvalues $p_{\\mathrm{h},1}$ and $p_{\\mathrm{h},2}$, then $P_{\\mathrm{d}}$ has eigenvalues $p_{\\mathrm{d},1}=2\\,p_{\\mathrm{h},1}$ and $p_{\\mathrm{d},2}=2\\,p_{\\mathrm{h},2}$. Consequently,\n$$\\det\\!\\big(P_{\\mathrm{d}}\\big)=(2\\,p_{\\mathrm{h},1})\\,(2\\,p_{\\mathrm{h},2})=4\\,p_{\\mathrm{h},1}\\,p_{\\mathrm{h},2}=4\\,\\det\\!\\big(P_{\\mathrm{h}}\\big).$$\nTherefore, the requested ratio is\n$$\\Gamma=\\frac{\\det\\!\\big(P_{\\mathrm{d}}\\big)}{\\det\\!\\big(P_{\\mathrm{h}}\\big)}=4.$$\n\nThis conclusion is independent of the specific values of $a_1$ and $a_2$ and thus applies to the given pair $A=\\mathrm{diag}(1,-2)$ and $C=I_2$ with $q=1$ and $r=1$.", "answer": "$$\\boxed{4}$$", "id": "2748182"}, {"introduction": "Real-world systems are often modeled with continuous-time dynamics, but their state estimators are implemented on digital computers, requiring a discrete-time representation. This programming practice tackles the essential task of accurately converting a continuous-time stochastic system into its discrete-time equivalent, $(\\Phi, Q_d)$. You will implement the powerful Van Loan's method, which elegantly computes both the state transition matrix and the process noise covariance via a single matrix exponential, providing a cornerstone skill for implementing high-fidelity Kalman filters [@problem_id:2748131].", "problem": "You are given a continuous-time linear time-invariant stochastic system, to be interpreted in the sense of Itô calculus, described by the state equation $\\,\\dot{x}(t) = A x(t) + G\\,w(t)\\,$ where $\\,x(t) \\in \\mathbb{R}^{n}\\,$, $\\,A \\in \\mathbb{R}^{n \\times n}\\,$, $\\,G \\in \\mathbb{R}^{n \\times m}\\,$, and $\\,w(t)\\,$ is a zero-mean Gaussian white noise (GWN) process with spectral density $\\,Q_{c} \\in \\mathbb{R}^{m \\times m}\\,$. Over a uniform sampling period $\\,h > 0\\,$, the exact discrete-time model is $\\,x_{k+1} = \\Phi x_{k} + v_{k}\\,$ where $\\,\\Phi \\in \\mathbb{R}^{n \\times n}\\,$ is the state transition matrix and $\\,v_{k}\\,$ is a zero-mean discrete-time process noise with covariance $\\,Q_{d} \\in \\mathbb{R}^{n \\times n}\\,$. By definition of the stochastic integral for linear systems, $\\,\\Phi = \\exp(A h)\\,$ and $\\,Q_{d} = \\int_{0}^{h} \\exp(A \\tau)\\,S\\,\\exp(A^{\\top} \\tau)\\,d\\tau\\,$ with $\\,S \\triangleq G Q_{c} G^{\\top}\\,$. Your task is to implement Van Loan’s method, which computes $\\,\\Phi\\,$ and $\\,Q_{d}\\,$ via a single block matrix exponential. You must not rely on any formula that directly gives $\\,Q_{d}\\,$ without deriving it from a fundamental base, and you must construct the appropriate block matrix implied by Van Loan’s method and extract the desired blocks from its exponential. Your program must then validate the implementation against analytically known results or high-accuracy numerical references.\n\nImplement a program that:\n- Constructs and evaluates the required block matrix exponential to obtain $\\,(\\Phi, Q_{d})\\,$ for each test system.\n- Validates $\\,\\Phi\\,$ against $\\,\\exp(A h)\\,$ where applicable, and validates $\\,Q_{d}\\,$ against either closed-form analytic expressions (where available) or a high-accuracy numerical quadrature of $\\,\\int_{0}^{h} \\exp(A \\tau)\\,S\\,\\exp(A^{\\top} \\tau)\\,d\\tau\\,$.\n- For each test case, reports two booleans: one for $\\,\\Phi\\,$ agreement and one for $\\,Q_{d}\\,$ agreement, based on relative Frobenius-norm error being below a specified tolerance. The relative error for a matrix $\\,X\\,$ compared to $\\,Y\\,$ must be computed as $\\,\\|X-Y\\|_{F} / \\max(1,\\|Y\\|_{F})\\,$, where $\\,\\|\\cdot\\|_{F}\\,$ is the Frobenius norm.\n\nUse the following test suite. In all items below, all numbers are real and dimensionally consistent. No physical units are required.\n\n- Test $\\,1\\,$ (general stable $\\,2 \\times 2\\,$ system):\n  - $\\,A = \\begin{bmatrix} -1 & 2 \\\\ -3 & -4 \\end{bmatrix}\\,$,\n  - $\\,G = I_{2}\\,$,\n  - $\\,Q_{c} = \\operatorname{diag}(0.5,\\,2.0)\\,$,\n  - $\\,h = 0.1\\,$.\n  - Validate $\\,\\Phi\\,$ against $\\,\\exp(A h)\\,$ and $\\,Q_{d}\\,$ against numerical quadrature of the defining integral.\n\n- Test $\\,2\\,$ (pure integrator boundary case with $\\,A = 0\\,$):\n  - $\\,A = 0_{3 \\times 3}\\,$,\n  - $\\,G = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.5 & 1.0 \\\\ 0.0 & -1.0 \\end{bmatrix}\\,$,\n  - $\\,Q_{c} = \\operatorname{diag}(0.3,\\,0.7)\\,$,\n  - $\\,h = 10^{-6}\\,$.\n  - Validate $\\,\\Phi\\,$ against $\\,I_{3}\\,$ and $\\,Q_{d}\\,$ against $\\,h\\,G Q_{c} G^{\\top}\\,$.\n\n- Test $\\,3\\,$ (scalar analytic check with unstable drift):\n  - $\\,A = [\\,0.7\\,]\\,$,\n  - $\\,G = [\\,1.0\\,]\\,$,\n  - $\\,Q_{c} = [\\,0.3\\,]\\,$,\n  - $\\,h = 0.3\\,$.\n  - Validate $\\,\\Phi\\,$ against $\\,\\exp(0.7 \\cdot 0.3)\\,$ and $\\,Q_{d}\\,$ against the analytic expression $\\,Q_{d} = 0.3\\,\\dfrac{\\exp(2\\cdot 0.7 \\cdot 0.3) - 1}{2\\cdot 0.7}\\,$.\n\n- Test $\\,4\\,$ (oscillatory $\\,2 \\times 2\\,$ system):\n  - $\\,A = \\begin{bmatrix} 0 & \\omega \\\\ -\\omega & 0 \\end{bmatrix}\\,$ with $\\,\\omega = 20.0\\,$,\n  - $\\,G = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}\\,$,\n  - $\\,Q_{c} = \\operatorname{diag}(1.0,\\,9.0)\\,$,\n  - $\\,h = 0.05\\,$.\n  - Validate $\\,\\Phi\\,$ against $\\,\\exp(A h)\\,$ and $\\,Q_{d}\\,$ against numerical quadrature of the defining integral.\n\nTolerances:\n- For $\\,\\Phi\\,$ comparisons: accept if the relative Frobenius-norm error is less than $\\,5 \\times 10^{-13}\\,$.\n- For $\\,Q_{d}\\,$ comparisons: accept if the relative Frobenius-norm error is less than $\\,10^{-10}\\,$.\n\nYour program must produce a single line of output containing the $\\,8\\,$ boolean results for the tests, in this exact order:\n$[\\text{Test 1: } \\Phi \\text{ ok}, \\text{Test 1: } Q_{d} \\text{ ok}, \\text{Test 2: } \\Phi \\text{ ok}, \\text{Test 2: } Q_{d} \\text{ ok}, \\text{Test 3: } \\Phi \\text{ ok}, \\text{Test 3: } Q_{d} \\text{ ok}, \\text{Test 4: } \\Phi \\text{ ok}, \\text{Test 4: } Q_{d} \\text{ ok}]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{true},\\text{false},\\dots\\,]$).", "solution": "The problem requires the computation of the exact discrete-time equivalent model for a continuous-time linear time-invariant (LTI) stochastic system. This process is fundamental in digital control and state estimation, particularly for the implementation of Kalman filters.\n\nFirst, we must meticulously establish the theoretical foundation. The continuous-time system is described by the Itô stochastic differential equation:\n$$\n\\dot{x}(t) = A x(t) + G w(t)\n$$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the state vector, $A \\in \\mathbb{R}^{n \\times n}$ is the dynamics matrix, $G \\in \\mathbb{R}^{n \\times m}$ is the noise input matrix, and $w(t)$ is an $m$-dimensional vector of continuous-time, zero-mean Gaussian white noise with a spectral density matrix $Q_{c} \\in \\mathbb{R}^{m \\times m}$. The covariance of the noise process is formally expressed as $E[w(t) w(\\tau)^T] = Q_{c} \\delta(t-\\tau)$, where $\\delta(\\cdot)$ is the Dirac delta function.\n\nThe solution to this linear SDE over a time interval of duration $h > 0$, from an initial time $t_k$ to a final time $t_{k+1} = t_k + h$, is given by:\n$$\nx(t_{k+1}) = e^{Ah} x(t_k) + \\int_{t_k}^{t_{k+1}} e^{A(t_{k+1}-\\tau)} G w(\\tau) d\\tau\n$$\nBy discretizing the state as $x_k \\triangleq x(t_k)$, we obtain the exact discrete-time model:\n$$\nx_{k+1} = \\Phi x_k + v_k\n$$\nFrom this, we identify the state transition matrix $\\Phi$ and the discrete-time process noise $v_k$:\n$$\n\\Phi = e^{Ah}\n$$\n$$\nv_k = \\int_{0}^{h} e^{A(h-\\sigma)} G w(t_k+\\sigma) d\\sigma\n$$\nThe discrete process noise $v_k$ is a zero-mean Gaussian random vector. Its covariance matrix, $Q_d = E[v_k v_k^T]$, is what we seek. By substituting the expression for $v_k$ and using the properties of white noise, the covariance is found to be:\n$$\nQ_d = \\int_{0}^{h} e^{A\\tau} (G Q_c G^T) e^{A^T\\tau} d\\tau\n$$\nLet us define the constant matrix $S \\triangleq G Q_c G^T \\in \\mathbb{R}^{n \\times n}$. The problem is now to compute $\\Phi = e^{Ah}$ and $Q_d = \\int_{0}^{h} e^{A\\tau} S e^{A^T\\tau} d\\tau$.\n\nA direct numerical quadrature of the integral for $Q_d$ is possible but computationally intensive and must be performed at every time step if $h$ were to change. Van Loan's method offers a more elegant and numerically robust solution by computing both $\\Phi$ and $Q_d$ via a single matrix exponential.\n\nThe method involves constructing a larger, $2n \\times 2n$ block matrix. Consider the matrix $M$ defined as:\n$$\nM = \\begin{bmatrix} A & S \\\\ 0 & -A^T \\end{bmatrix}\n$$\nWe shall compute the exponential of $Mh$, which we denote by $E = \\exp(Mh)$. Let $E(t) = \\exp(Mt)$. This matrix is the solution to the matrix differential equation $\\frac{d}{dt}E(t) = M E(t)$ with the initial condition $E(0) = I_{2n}$. We partition $E(t)$ into $n \\times n$ blocks:\n$$\nE(t) = \\begin{bmatrix} \\mathcal{E}_{11}(t) & \\mathcal{E}_{12}(t) \\\\ \\mathcal{E}_{21}(t) & \\mathcal{E}_{22}(t) \\end{bmatrix}\n$$\nThe system of differential equations for the blocks becomes:\n$$\n\\frac{d}{dt} \\begin{bmatrix} \\mathcal{E}_{11} & \\mathcal{E}_{12} \\\\ \\mathcal{E}_{21} & \\mathcal{E}_{22} \\end{bmatrix} = \\begin{bmatrix} A & S \\\\ 0 & -A^T \\end{bmatrix} \\begin{bmatrix} \\mathcal{E}_{11} & \\mathcal{E}_{12} \\\\ \\mathcal{E}_{21} & \\mathcal{E}_{22} \\end{bmatrix}\n$$\nWith initial conditions $\\mathcal{E}_{11}(0) = I_n$, $\\mathcal{E}_{22}(0) = I_n$, and $\\mathcal{E}_{12}(0) = \\mathcal{E}_{21}(0) = 0$.\n\nSolving the block equations from the bottom row up:\n1.  $\\dot{\\mathcal{E}}_{21}(t) = -A^T \\mathcal{E}_{21}(t)$. With $\\mathcal{E}_{21}(0) = 0$, the unique solution is $\\mathcal{E}_{21}(t) = 0$ for all $t$.\n2.  $\\dot{\\mathcal{E}}_{22}(t) = -A^T \\mathcal{E}_{22}(t)$. With $\\mathcal{E}_{22}(0) = I_n$, the solution is $\\mathcal{E}_{22}(t) = e^{-A^T t}$.\n3.  $\\dot{\\mathcal{E}}_{11}(t) = A \\mathcal{E}_{11}(t) + S \\mathcal{E}_{21}(t)$. Since $\\mathcal{E}_{21}(t) = 0$, this becomes $\\dot{\\mathcal{E}}_{11}(t) = A \\mathcal{E}_{11}(t)$. With $\\mathcal{E}_{11}(0) = I_n$, the solution is $\\mathcal{E}_{11}(t) = e^{At}$.\n4.  $\\dot{\\mathcal{E}}_{12}(t) = A \\mathcal{E}_{12}(t) + S \\mathcal{E}_{22}(t)$. Substituting the solution for $\\mathcal{E}_{22}(t)$, we have a non-homogeneous linear ODE: $\\dot{\\mathcal{E}}_{12}(t) = A \\mathcal{E}_{12}(t) + S e^{-A^T t}$. With $\\mathcal{E}_{12}(0) = 0$, the solution by variation of parameters is $\\mathcal{E}_{12}(t) = e^{At} \\int_0^t e^{-A\\tau} S e^{-A^T\\tau} d\\tau$.\n\nEvaluating these solutions at $t=h$, we find the blocks of $E = \\exp(Mh)$:\n$$\n\\mathcal{E}_{11}(h) = e^{Ah} = \\Phi\n$$\n$$\n\\mathcal{E}_{12}(h) = e^{Ah} \\int_0^h e^{-A\\tau} S e^{-A^T\\tau} d\\tau\n$$\nThe bottom-right block is $\\mathcal{E}_{22}(h) = e^{-A^T h} = (e^{A^T h})^{-1} = (\\Phi^T)^{-1}$. The top-left block $\\mathcal{E}_{11}(h)$ is exactly the state transition matrix $\\Phi$. The top-right block $\\mathcal{E}_{12}(h)$ is not $Q_d$ itself, but is closely related. Let us post-multiply $\\mathcal{E}_{12}(h)$ by $\\Phi^T = e^{A^T h}$:\n$$\n\\mathcal{E}_{12}(h) \\Phi^T = \\left( e^{Ah} \\int_0^h e^{-A\\tau} S e^{-A^T\\tau} d\\tau \\right) e^{A^T h} = \\int_0^h e^{A(h-\\tau)} S e^{-A^T\\tau} e^{A^T h} d\\tau\n$$\n$$\n= \\int_0^h e^{A(h-\\tau)} S e^{A^T(h-\\tau)} d\\tau\n$$\nBy a change of variable $\\sigma = h-\\tau$, where $d\\sigma = -d\\tau$ and the integration limits $[0, h]$ for $\\tau$ become $[h, 0]$ for $\\sigma$:\n$$\n= \\int_h^0 e^{A\\sigma} S e^{A^T\\sigma} (-d\\sigma) = \\int_0^h e^{A\\sigma} S e^{A^T\\sigma} d\\sigma = Q_d\n$$\nThis derivation confirms the correctness of the method. The algorithm is as follows:\n1.  Given $A, G, Q_c, h$, compute $S = G Q_c G^T$.\n2.  Construct the $2n \\times 2n$ matrix $M = \\begin{bmatrix} A & S \\\\ 0 & -A^T \\end{bmatrix}$.\n3.  Compute $E = \\exp(Mh)$ using a reliable matrix exponential algorithm.\n4.  Extract the blocks: $\\Phi = E[1:n, 1:n]$ and let $\\mathcal{E}_{12} = E[1:n, n+1:2n]$.\n5.  Compute the discrete noise covariance as $Q_d = \\mathcal{E}_{12} \\Phi^T$.\n\nThis procedure is implemented and then validated against reference computations. The validation uses either analytical expressions, where they exist, or high-accuracy numerical quadrature for the defining integral of $Q_d$. The comparison is based on the relative Frobenius-norm error: $\\epsilon(X, Y) = \\|X-Y\\|_F / \\max(1, \\|Y\\|_F)$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\nfrom scipy.integrate import quad_vec\n\ndef solve():\n    \"\"\"\n    Implements and validates Van Loan's method for discretizing a continuous-time\n    LTI stochastic system.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # ω for Test 4\n    omega = 20.0\n    \n    test_cases = [\n        {\n            \"id\": 1,\n            \"A\": np.array([[-1.0, 2.0], [-3.0, -4.0]]),\n            \"G\": np.identity(2),\n            \"Qc\": np.diag([0.5, 2.0]),\n            \"h\": 0.1,\n            \"ref_type\": \"quadrature\"\n        },\n        {\n            \"id\": 2,\n            \"A\": np.zeros((3, 3)),\n            \"G\": np.array([[1.0, 0.0], [0.5, 1.0], [0.0, -1.0]]),\n            \"Qc\": np.diag([0.3, 0.7]),\n            \"h\": 1e-6,\n            \"ref_type\": \"analytic_A0\"\n        },\n        {\n            \"id\": 3,\n            \"A\": np.array([[0.7]]),\n            \"G\": np.array([[1.0]]),\n            \"Qc\": np.array([[0.3]]),\n            \"h\": 0.3,\n            \"ref_type\": \"analytic_scalar\"\n        },\n        {\n            \"id\": 4,\n            \"A\": np.array([[0.0, omega], [-omega, 0.0]]),\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"Qc\": np.diag([1.0, 9.0]),\n            \"h\": 0.05,\n            \"ref_type\": \"quadrature\"\n        }\n    ]\n\n    phi_tol = 5e-13\n    qd_tol = 1e-10\n    results = []\n\n    def relative_frobenius_error(X, Y):\n        \"\"\"Computes the relative Frobenius norm error.\"\"\"\n        norm_Y = np.linalg.norm(Y, 'fro')\n        norm_diff = np.linalg.norm(X - Y, 'fro')\n        return norm_diff / max(1.0, norm_Y)\n\n    for case in test_cases:\n        A, G, Qc, h = case[\"A\"], case[\"G\"], case[\"Qc\"], case[\"h\"]\n        n = A.shape[0]\n\n        # --- Van Loan's Method ---\n        # 1. Compute S = G * Qc * G^T\n        S = G @ Qc @ G.T\n\n        # 2. Construct the 2n x 2n matrix M\n        M = np.block([\n            [A, S],\n            [np.zeros((n, n)), -A.T]\n        ])\n\n        # 3. Compute the matrix exponential E = exp(M*h)\n        E = expm(M * h)\n\n        # 4. Extract Phi\n        phi_vl = E[:n, :n]\n\n        # 5. Extract the intermediate block and compute Qd\n        E12 = E[:n, n:]\n        qd_vl = E12 @ phi_vl.T\n        \n        # Symmetrize Qd to enforce theoretical property and improve numerical stability\n        qd_vl = (qd_vl + qd_vl.T) / 2.0\n\n        # --- Reference Calculation ---\n        if case[\"ref_type\"] == \"quadrature\":\n            phi_ref = expm(A * h)\n            \n            # Integrand function for Qd\n            def integrand(tau):\n                mat = expm(A * tau) @ S @ expm(A.T * tau)\n                return mat.flatten()\n\n            # Integrate from 0 to h\n            integral_flat, _ = quad_vec(integrand, 0, h, epsabs=1e-14, epsrel=1e-14)\n            qd_ref = integral_flat.reshape((n, n))\n\n        elif case[\"ref_type\"] == \"analytic_A0\":\n            phi_ref = np.identity(n)\n            qd_ref = S * h\n\n        elif case[\"ref_type\"] == \"analytic_scalar\":\n            a = A[0, 0]\n            phi_ref = np.array([[np.exp(a * h)]])\n            # For a != 0\n            if np.abs(a) > 1e-9:\n                qd_ref = S * (np.exp(2 * a * h) - 1) / (2 * a)\n            else: # Taylor expansion for a -> 0\n                qd_ref = S * h * (1 + a*h + (2/3)*(a*h)**2)\n\n        # --- Validation ---\n        phi_err = relative_frobenius_error(phi_vl, phi_ref)\n        qd_err = relative_frobenius_error(qd_vl, qd_ref)\n\n        phi_ok = phi_err  phi_tol\n        qd_ok = qd_err  qd_tol\n\n        results.extend([phi_ok, qd_ok])\n    \n    # --- Final Output Formatting ---\n    # The problem asks for lowercase boolean strings.\n    print(f\"[{','.join(str(b).lower() for b in results)}]\")\n\nsolve()\n```", "id": "2748131"}]}