{"hands_on_practices": [{"introduction": "We begin with a core application of state feedback control: stabilizing an inherently unstable system. In this exercise [@problem_id:1556713], you will design a controller for a simplified magnetic levitation system, a classic problem in control engineering. Before calculating the feedback gain matrix $K$, this practice emphasizes the critical first step of verifying system controllability, a fundamental prerequisite that guarantees our ability to arbitrarily place the system's poles.", "problem": "A simplified linear model for a single-axis magnetic levitation system is used to control the vertical position of a small object. The state of the system is described by the vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, where $x_1$ is the object's vertical displacement from the desired equilibrium point and $x_2$ is its vertical velocity. The system dynamics are governed by the state-space equation $\\dot{\\mathbf{x}} = A\\mathbf{x} + Bu$, where $u$ is the control input voltage. The system matrices are given by:\n$$A = \\begin{pmatrix} 0 & 1 \\\\ 4 & 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nTo stabilize the object, a state feedback controller of the form $u = -K\\mathbf{x}$ is to be designed, where $K = \\begin{pmatrix} k_1 & k_2 \\end{pmatrix}$ is the feedback gain matrix. The goal is to place the poles of the closed-loop system at $-3$ and $-4$.\n\nDetermine the feedback gain matrix $K$.", "solution": "We use full-state feedback $u=-K\\mathbf{x}$ with $K=\\begin{pmatrix}k_{1} & k_{2}\\end{pmatrix}$. The closed-loop dynamics are given by the principle of state feedback pole placement:\n$$\n\\dot{\\mathbf{x}}=(A-BK)\\mathbf{x}.\n$$\nFirst, verify controllability to ensure arbitrary pole placement is possible. The controllability matrix is\n$$\n\\mathcal{C}=\\begin{pmatrix}B & AB\\end{pmatrix}=\\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix},\n$$\nwhich has full rank $2$, so the system is controllable.\n\nCompute the closed-loop matrix:\n$$\nA-BK=\\begin{pmatrix}0 & 1 \\\\ 4 & 0\\end{pmatrix}-\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\begin{pmatrix}k_{1} & k_{2}\\end{pmatrix}=\\begin{pmatrix}0 & 1 \\\\ 4-k_{1} & -k_{2}\\end{pmatrix}.\n$$\nThe characteristic polynomial of $A-BK$ is\n$$\n\\det\\!\\big(sI-(A-BK)\\big)=\\det\\!\\begin{pmatrix}s & -1 \\\\ k_{1}-4 & s+k_{2}\\end{pmatrix}=s(s+k_{2})-(-1)(k_{1}-4)=s^{2}+k_{2}s+(k_{1}-4).\n$$\nThe design goal is to place the closed-loop poles at $-3$ and $-4$, so the desired characteristic polynomial is\n$$\n(s+3)(s+4)=s^{2}+7s+12.\n$$\nEquate coefficients to solve for $k_{1}$ and $k_{2}$:\n$$\nk_{2}=7,\\qquad k_{1}-4=12 \\;\\Rightarrow\\; k_{1}=16.\n$$\nTherefore, the feedback gain matrix is\n$$\nK=\\begin{pmatrix}16 & 7\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}16 & 7\\end{pmatrix}}$$", "id": "1556713"}, {"introduction": "Having applied the pole placement technique, we now delve deeper into its theoretical underpinnings with a compelling thought experiment. This practice [@problem_id:1556685] challenges you to predict the outcome of Ackermann's formula when the desired closed-loop poles are chosen to be identical to the system's original open-loop poles. Solving this reveals the elegant connection between the pole placement objective, the desired characteristic polynomial $\\phi_d(A)$, and the Cayley-Hamilton theorem.", "problem": "Consider a general $n$-th order, controllable, Linear Time-Invariant (LTI), Single-Input Single-Output (SISO) system described by the state-space representation $\\dot{x}(t) = Ax(t) + Bu(t)$, where $x \\in \\mathbb{R}^{n}$ is the state vector, $u \\in \\mathbb{R}$ is the control input, $A$ is the $n \\times n$ state matrix, and $B$ is the $n \\times 1$ input matrix. The system's open-loop poles are the eigenvalues of the matrix $A$, which are the roots of the open-loop characteristic polynomial, $\\alpha(s) = \\det(sI-A)$.\n\nA state feedback control law of the form $u(t) = -Kx(t)$ is applied, where $K$ is a $1 \\times n$ row vector of feedback gains. The goal of this control law is to place the poles of the closed-loop system, given by the eigenvalues of the matrix $(A - BK)$, at desired locations. The desired pole locations are specified by the roots of a desired characteristic polynomial, $\\alpha_d(s)$.\n\nAckermann's formula provides a method for calculating the gain matrix $K$ that achieves the desired pole placement. The formula is given by:\n$$K = [0 \\ 0 \\ \\dots \\ 1] \\mathcal{C}^{-1} \\alpha_d(A)$$\nwhere $\\mathcal{C}$ is the $n \\times n$ controllability matrix of the pair $(A, B)$, and $\\alpha_d(A)$ is the matrix polynomial found by substituting the matrix $A$ for the scalar variable $s$ in the desired characteristic polynomial $\\alpha_d(s)$.\n\nSuppose that a control designer, for analytical purposes, decides to choose the desired closed-loop poles to be exactly the same as the system's original open-loop poles. What is the state feedback gain matrix $K$ that Ackermann's formula would compute for this specific scenario? Express your answer as a general matrix expression valid for any such $n$-th order system.", "solution": "We are given an $n$-th order controllable LTI SISO system $\\dot{x}(t) = Ax(t) + Bu(t)$ and the state feedback law $u(t) = -Kx(t)$, with Ackermann’s formula\n$$\nK = \\begin{pmatrix}0 & 0 & \\dots & 1\\end{pmatrix} \\mathcal{C}^{-1} \\alpha_{d}(A),\n$$\nwhere $\\mathcal{C}$ is the controllability matrix of $(A,B)$ and $\\alpha_{d}(s)$ is the desired characteristic polynomial.\n\nLet the desired closed-loop polynomial be chosen equal to the open-loop characteristic polynomial:\n$$\n\\alpha_{d}(s) = \\alpha(s) = \\det(sI - A).\n$$\nBy the Cayley–Hamilton theorem, the matrix $A$ satisfies its own characteristic polynomial:\n$$\n\\alpha(A) = 0_{n \\times n}.\n$$\nTherefore, substituting $\\alpha_{d}(A) = \\alpha(A)$ into Ackermann’s formula yields\n$$\nK = \\begin{pmatrix}0 & 0 & \\dots & 1\\end{pmatrix} \\mathcal{C}^{-1} \\alpha(A) = \\begin{pmatrix}0 & 0 & \\dots & 1\\end{pmatrix} \\mathcal{C}^{-1} 0_{n \\times n} = 0_{1 \\times n}.\n$$\nHence, the gain returned by Ackermann’s formula in this scenario is the zero row vector, which leaves the closed-loop matrix $A - BK$ equal to $A$, placing the poles at the original open-loop locations as specified.", "answer": "$$\\boxed{0_{1 \\times n}}$$", "id": "1556685"}, {"introduction": "Theoretical formulas provide the foundation, but professional engineering demands numerically robust implementations. While elegant, the direct computation of Ackermann's formula can be sensitive to numerical errors, especially for high-order or ill-conditioned systems. This advanced practice [@problem_id:2689308] moves beyond the textbook formula, challenging you to implement a numerically stable pole placement algorithm using modern techniques from numerical linear algebra, such as QR factorization and Singular Value Decomposition (SVD).", "problem": "You are given a fully controllable, single-input linear time-invariant state-space model of order $n$ described by the pair $(A,B)$ with $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times 1}$. The objective is to place the closed-loop eigenvalues at a prescribed set of real locations by a static state-feedback law $u = -K x$, while computing the state feedback gain $K \\in \\mathbb{R}^{1 \\times n}$ via a numerically stable method that does not explicitly form any matrix inverse.\n\nStart from the following foundational definitions and facts:\n- The controllability matrix is $\\mathcal{C} = \\big[ B \\;\\; A B \\;\\; \\cdots \\;\\; A^{n-1} B \\big] \\in \\mathbb{R}^{n \\times n}$.\n- The desired real eigenvalues $\\{\\lambda_1,\\dots,\\lambda_n\\}$ define a monic polynomial $p_d(s) = \\prod_{i=1}^n (s - \\lambda_i) = s^n + a_{n-1} s^{n-1} + \\cdots + a_1 s + a_0$ with real coefficients $\\{a_0,\\dots,a_{n-1}\\}$.\n- For a matrix argument, the associated matrix polynomial is $p_d(A) = A^n + a_{n-1} A^{n-1} + \\cdots + a_1 A + a_0 I$, where $I$ is the identity matrix of size $n$.\n- The Cayley–Hamilton theorem states that every square matrix satisfies its own characteristic polynomial.\n- For a controllable system, there exists a static feedback gain $K$ that assigns the closed-loop eigenvalues to the desired set.\n\nYour task is to:\n- Derive, from these foundations and without invoking any pre-packaged formula, an expression for a computable state feedback gain that places the closed-loop eigenvalues at the desired locations using the controllability matrix $\\mathcal{C}$ and the matrix polynomial $p_d(A)$. Your derivation should show how to compute an intermediate quantity of the form $\\mathcal{C}^{-1} p_d(A)$, and how a canonical selector recovers a row vector gain from it. You must not form $\\mathcal{C}^{-1}$ explicitly.\n- Propose and implement a numerically stable computational procedure to evaluate $\\mathcal{C}^{-1} p_d(A)$ using either the orthogonal–triangular (QR) factorization or the Singular Value Decomposition (SVD) of $\\mathcal{C}$. Your implementation must:\n  - Evaluate $p_d(A)$ using a numerically stable matrix polynomial evaluation scheme (such as Horner’s method) to avoid explicitly forming large matrix powers.\n  - Solve the linear matrix equation $\\mathcal{C} X = p_d(A)$ for $X$ using either a QR-based triangular solve or an SVD-based pseudoinverse with a principled tolerance for small singular values, thereby computing $X = \\mathcal{C}^{-1} p_d(A)$ without explicitly inverting $\\mathcal{C}$.\n  - Extract a valid state feedback row vector from $X$ using an appropriate canonical selector.\n- Validate the result by constructing the closed-loop matrix $A_{cl} = A - B K$ and comparing the coefficients of its characteristic polynomial to those of $p_d(s)$. Use the maximum absolute difference between corresponding coefficients as a scalar error metric. Smaller values indicate better eigenvalue placement.\n\nImportant implementation constraints:\n- Do not use any direct matrix inverse. Use either QR factorization or Singular Value Decomposition to solve the linear system for $X$.\n- Use double-precision floating point arithmetic throughout.\n- Angles are not involved in this problem, hence no angle unit is required. No physical units appear in this problem.\n\nTest suite and required outputs:\nImplement your method for the following three test cases. In all cases, $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times 1}$.\n\n- Test case $1$ (happy path):\n  - Dimension $n = 3$.\n  - Use the controllable integrator chain $A = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}$ and $B = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Desired eigenvalues $\\{\\lambda_1,\\lambda_2,\\lambda_3\\} = \\{-1,-2,-3\\}$.\n- Test case $2$ (repeated desired eigenvalues):\n  - Dimension $n = 4$.\n  - Use the controllable integrator chain $A = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$ and $B = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Desired eigenvalues $\\{\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4\\} = \\{-1,-1,-2,-2\\}$.\n- Test case $3$ (ill-conditioned controllability via a similarity transform):\n  - Dimension $n = 5$.\n  - Let $J \\in \\mathbb{R}^{5 \\times 5}$ denote the integrator chain $J = \\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$ and $e_5 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Define a poorly scaled similarity transform $T = \\operatorname{diag}(1,10^2,10^4,10^6,10^8) \\in \\mathbb{R}^{5 \\times 5}$.\n  - Set $A = T J T^{-1}$ and $B = T e_5$.\n  - Desired eigenvalues $\\{\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4,\\lambda_5\\} = \\{-1,-2,-3,-4,-5\\}$.\n\nFor each test case, compute two versions of the feedback gain:\n- One using a QR-based solve for $\\mathcal{C} X = p_d(A)$.\n- One using an SVD-based solve (with a numerically justified cutoff for small singular values) for $\\mathcal{C} X = p_d(A)$.\n\nFor each computed gain $K$, form $A_{cl} = A - B K$ and compute the monic characteristic polynomial coefficients of $A_{cl}$, denoted $\\{1,\\hat{a}_{n-1},\\dots,\\hat{a}_0\\}$. Let the scalar error be $\\max_{k \\in \\{0,\\dots,n-1\\}} |a_k - \\hat{a}_k|$. Finally, report, for each test case, the triple of floats:\n- The error for the QR-based method.\n- The error for the SVD-based method.\n- The Euclidean norm of the difference between the two gains, $\\|K_{\\text{QR}} - K_{\\text{SVD}}\\|_2$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist with its three floats in the order specified above. For example, an output for three test cases must look like $[[\\text{err\\_QR}_1,\\text{err\\_SVD}_1,\\text{dK}_1],[\\text{err\\_QR}_2,\\text{err\\_SVD}_2,\\text{dK}_2],[\\text{err\\_QR}_3,\\text{err\\_SVD}_3,\\text{dK}_3]]$ with no additional text.", "solution": "The problem presented is a standard, well-posed problem in linear control theory concerning eigenvalue assignment by state feedback, commonly known as pole placement. All provided information is scientifically sound, internally consistent, and sufficient for derivation and computation. The problem is therefore valid. We shall proceed with the derivation and solution.\n\nThe objective is to find a state feedback gain matrix $K \\in \\mathbb{R}^{1 \\times n}$ for the controllable single-input linear time-invariant system $\\dot{x} = Ax + Bu$ such that the closed-loop system, under the control law $u = -Kx$, has its eigenvalues at a prescribed set of real locations $\\{\\lambda_1, \\dots, \\lambda_n\\}$. The closed-loop system is described by $\\dot{x} = (A - BK)x = A_{cl}x$.\n\nThe desired eigenvalues are the roots of the monic polynomial $p_d(s) = \\prod_{i=1}^n (s - \\lambda_i) = s^n + a_{n-1}s^{n-1} + \\dots + a_0$. For the eigenvalues of $A_{cl}$ to be $\\{\\lambda_1, \\dots, \\lambda_n\\}$, its characteristic polynomial, $\\det(sI-A_{cl})$, must be equal to $p_d(s)$. By the Cayley-Hamilton theorem, the matrix $A_{cl}$ must satisfy its own characteristic equation, which implies $p_d(A_{cl}) = 0$.\n\nThe derivation will proceed by postulating the form of the gain $K$, known as Ackermann's formula, and then proving that it achieves the desired eigenvalue placement. The formula is given by:\n$$ K = e_n^T \\mathcal{C}^{-1} p_d(A) $$\nwhere $e_n^T = [0, \\dots, 0, 1] \\in \\mathbb{R}^{1 \\times n}$ is a canonical row vector, $\\mathcal{C}$ is the system's controllability matrix, and $p_d(A)$ is the matrix polynomial associated with the desired characteristic polynomial.\n\nLet us define a row vector $q^T = e_n^T \\mathcal{C}^{-1}$. By definition, $q^T$ is the last row of the inverse of the controllability matrix $\\mathcal{C} = [B \\;\\; AB \\;\\; \\dots \\;\\; A^{n-1}B]$. This implies that $q^T \\mathcal{C} = e_n^T$. Expanding this matrix equation yields a set of distinct properties for $q^T$:\n$$ q^T A^i B = 0 \\quad \\text{for } i = 0, 1, \\dots, n-2 $$\n$$ q^T A^{n-1} B = 1 $$\n\nWith $K = q^T p_d(A)$, we must now demonstrate that the eigenvalues of $A_{cl} = A - BK$ are the roots of $p_d(s)$. The eigenvalues $\\lambda$ of $A_{cl}$ are the solutions to $\\det(\\lambda I - A_{cl}) = 0$. Using the matrix determinant lemma, $\\det(M+uv^T) = \\det(M)(1+v^T M^{-1}u)$, we can write:\n$$ \\det(\\lambda I - A_{cl}) = \\det(\\lambda I - (A - BK)) = \\det((\\lambda I - A) + BK) $$\n$$ = \\det(\\lambda I - A) \\left(1 + K(\\lambda I - A)^{-1}B\\right) $$\nSince the term $K(\\lambda I - A)^{-1}B$ is a scalar for a single-input system, the characteristic equation for the closed-loop system's eigenvalues $\\lambda$ (where $\\lambda$ is not an eigenvalue of $A$) becomes:\n$$ 1 + K(\\lambda I - A)^{-1}B = 0 $$\nSubstituting our expression for $K = q^T p_d(A)$:\n$$ 1 + q^T p_d(A) (\\lambda I - A)^{-1} B = 0 $$\nWe can use polynomial division to write $p_d(s) = p_d(\\lambda) + (s-\\lambda)\\hat{p}(s,\\lambda)$, where $\\hat{p}(s,\\lambda)$ is a polynomial in $s$ of degree $n-1$. The leading coefficient of $\\hat{p}(s,\\lambda)$ is $1$. Evaluating this at the matrix $A$ gives:\n$$ p_d(A) = p_d(\\lambda)I + (A-\\lambda I)\\hat{p}(A,\\lambda) $$\nAssuming $\\lambda I - A$ is invertible, we can write:\n$$ p_d(A)(\\lambda I-A)^{-1} = -p_d(A)(A-\\lambda I)^{-1} = -p_d(\\lambda)(A-\\lambda I)^{-1} - \\hat{p}(A,\\lambda) $$\nSubstituting this into the characteristic equation:\n$$ 1 - q^T \\left( p_d(\\lambda)(A-\\lambda I)^{-1} + \\hat{p}(A,\\lambda) \\right) B = 0 $$\n$$ 1 - p_d(\\lambda) q^T(A-\\lambda I)^{-1}B - q^T\\hat{p}(A,\\lambda)B = 0 $$\nNow, we examine the term $q^T\\hat{p}(A,\\lambda)B$. Since $\\hat{p}(s,\\lambda)$ is a polynomial of degree $n-1$, we can write $\\hat{p}(A,\\lambda) = \\sum_{i=0}^{n-1} c_i A^i$, where $c_{n-1}=1$.\n$$ q^T\\hat{p}(A,\\lambda)B = q^T \\left(\\sum_{i=0}^{n-1} c_i A^i \\right) B = \\sum_{i=0}^{n-1} c_i (q^T A^i B) $$\nUsing the properties of $q^T$, this sum collapses to a single term:\n$$ (c_0 \\cdot 0) + \\dots + (c_{n-2} \\cdot 0) + (c_{n-1} \\cdot 1) = c_{n-1} = 1 $$\nThe characteristic equation thus simplifies to:\n$$ 1 - p_d(\\lambda) q^T(A-\\lambda I)^{-1}B - 1 = 0 $$\n$$ -p_d(\\lambda) \\left(q^T(A-\\lambda I)^{-1}B\\right) = 0 $$\nThis equation holds true if $p_d(\\lambda) = 0$. This proves that any root of the desired polynomial $p_d(s)$ is an eigenvalue of the closed-loop matrix $A_{cl}$. Since there are $n$ such roots, this constitutes the complete set of eigenvalues for $A_{cl}$.\n\nThe computational procedure avoids explicit matrix inversion. The gain is $K = e_n^T \\mathcal{C}^{-1} p_d(A)$. Let $X = \\mathcal{C}^{-1} p_d(A)$. Then $K$ is simply the last row of the matrix $X$. We can find $X$ by solving the linear matrix equation $\\mathcal{C}X = p_d(A)$.\nThe algorithm is as follows:\n1.  From the desired eigenvalues $\\{\\lambda_1, \\dots, \\lambda_n\\}$, compute the coefficients $\\{a_0, \\dots, a_{n-1}\\}$ of the target characteristic polynomial $p_d(s) = s^n + a_{n-1}s^{n-1} + \\dots + a_0$.\n2.  Evaluate the matrix polynomial $p_d(A)$ using a numerically stable scheme like Horner's method to avoid forming and storing high powers of $A$:\n    $p_d(A) = A^n + a_{n-1}A^{n-1} + \\dots + a_0I = (\\dots((I\\cdot A + a_{n-1}I)A + a_{n-2}I)A + \\dots + a_0I)$.\n3.  Construct the controllability matrix $\\mathcal{C} = [B \\;\\; AB \\;\\; \\dots \\;\\; A^{n-1}B]$.\n4.  Solve the linear matrix equation $\\mathcal{C}X = p_d(A)$ for $X$ without forming $\\mathcal{C}^{-1}$.\n    -   **Using QR factorization**: Compute $\\mathcal{C}=QR$. The equation becomes $RX = Q^T p_d(A)$. Since $R$ is upper triangular, $X$ can be found efficiently via back substitution.\n    -   **Using SVD**: Compute $\\mathcal{C}=U\\Sigma V^T$. The solution is $X = V \\Sigma^{\\dagger} U^T p_d(A)$, where $\\Sigma^{\\dagger}$ is the pseudoinverse of the diagonal matrix $\\Sigma$. The entries of $\\Sigma^{\\dagger}$ are $1/\\sigma_i$ for singular values $\\sigma_i$ above a certain tolerance, and $0$ otherwise. A principled tolerance is $\\tau = \\max(n,n) \\cdot \\epsilon \\cdot \\sigma_{\\max}$, where $\\epsilon$ is machine precision and $\\sigma_{\\max}$ is the largest singular value.\n5.  Extract the gain vector $K$ as the last row of the computed matrix $X$: $K = X[n-1, :]$.\n6.  For validation, construct $A_{cl}=A-BK$ and compute the coefficients of its characteristic polynomial. The error is the maximum absolute difference between these coefficients and the target coefficients $\\{a_i\\}$.\n\nThis procedure provides a numerically robust method for computing the state feedback gain.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves for the state feedback gain K using QR and SVD based methods for three test cases,\n    and reports the accuracy of eigenvalue placement.\n    \"\"\"\n\n    def compute_ackermann_gain(A, B, desired_eigs, method):\n        \"\"\"\n        Computes the state feedback gain K using Ackermann's formula via QR or SVD.\n        \n        Args:\n            A (np.ndarray): State matrix (n x n).\n            B (np.ndarray): Input matrix (n x 1).\n            desired_eigs (list or np.ndarray): List of desired closed-loop eigenvalues.\n            method (str): 'qr' or 'svd'.\n\n        Returns:\n            np.ndarray: State feedback gain K (1 x n).\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Compute coefficients of the desired characteristic polynomial p_d(s)\n        # np.poly gives [1, a_{n-1}, ..., a_0]\n        p_coeffs = np.poly(desired_eigs)\n\n        # 2. Evaluate the matrix polynomial p_d(A) using Horner's method\n        # p_d(A) = A^n + a_{n-1}A^{n-1} + ... + a_0*I\n        PdA = np.eye(n)\n        for i in range(1, n + 1):\n            PdA = A @ PdA + p_coeffs[i] * np.eye(n)\n\n        # 3. Construct the controllability matrix C\n        C = np.zeros((n, n), dtype=np.float64)\n        C[:, 0] = B.flatten()\n        for i in range(1, n):\n            C[:, i] = A @ C[:, i - 1]\n        \n        # 4. Solve the linear system CX = p_d(A) for X\n        if method == 'qr':\n            Q, R = np.linalg.qr(C)\n            # Solve R @ X = Q.T @ PdA\n            X = scipy.linalg.solve_triangular(R, Q.T @ PdA)\n        elif method == 'svd':\n            U, s, Vh = np.linalg.svd(C)\n            # Set tolerance for pseudoinverse calculation\n            tol = max(C.shape) * np.finfo(s.dtype).eps * s[0]\n            s_inv = np.where(s > tol, 1 / s, 0)\n            # Compute X = V @ S_pinv @ U.T @ PdA\n            C_pinv = Vh.T @ np.diag(s_inv) @ U.T\n            X = C_pinv @ PdA\n        else:\n            raise ValueError(\"Method must be 'qr' or 'svd'\")\n\n        # 5. Extract gain K from the last row of X\n        K = X[-1, :]\n        return K.reshape(1, n)\n\n    test_cases = [\n        {\n            \"n\": 3,\n            \"A\": np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]], dtype=np.float64),\n            \"B\": np.array([[0], [0], [1]], dtype=np.float64),\n            \"eigs\": [-1, -2, -3]\n        },\n        {\n            \"n\": 4,\n            \"A\": np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]], dtype=np.float64),\n            \"B\": np.array([[0], [0], [0], [1]], dtype=np.float64),\n            \"eigs\": [-1, -1, -2, -2]\n        },\n        {\n            \"n\": 5,\n            \"setup\": {\n                \"J\": np.diag(np.ones(4), 1),\n                \"e5\": np.array([[0],[0],[0],[0],[1]], dtype=np.float64),\n                \"T\": np.diag([10**(2*i) for i in range(5)])\n            },\n            \"eigs\": [-1, -2, -3, -4, -5]\n        }\n    ]\n    # Prepare test case 3\n    tc3 = test_cases[2]\n    J = tc3['setup']['J']\n    T = tc3['setup']['T']\n    e5 = tc3['setup']['e5']\n    T_inv = np.linalg.inv(T)\n    tc3['A'] = T @ J @ T_inv\n    tc3['B'] = T @ e5\n\n    results = []\n    \n    for case in test_cases:\n        A, B, eigs = case['A'], case['B'], case['eigs']\n        n = case['n']\n\n        # Desired characteristic polynomial coefficients\n        p_coeffs_desired = np.poly(eigs)\n\n        # QR method\n        K_qr = compute_ackermann_gain(A, B, eigs, 'qr')\n        A_cl_qr = A - B @ K_qr\n        p_coeffs_qr = np.poly(A_cl_qr)\n        err_qr = np.max(np.abs(p_coeffs_desired[1:] - p_coeffs_qr[1:]))\n\n        # SVD method\n        K_svd = compute_ackermann_gain(A, B, eigs, 'svd')\n        A_cl_svd = A - B @ K_svd\n        p_coeffs_svd = np.poly(A_cl_svd)\n        err_svd = np.max(np.abs(p_coeffs_desired[1:] - p_coeffs_svd[1:]))\n\n        # Difference between gains\n        dK = np.linalg.norm(K_qr - K_svd)\n\n        results.append([err_qr, err_svd, dK])\n\n    # Format the output as specified\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2689308"}]}