{"hands_on_practices": [{"introduction": "Calculating the matrix exponential is a cornerstone of linear systems analysis. While the definition as an infinite series is foundational, for practical computations with smaller matrices, more direct methods are invaluable. This exercise guides you through a powerful analytical technique based on the Cayley-Hamilton theorem, which allows you to express $e^{tA}$ as a finite polynomial in $A$, effectively reducing a matrix function problem to solving a system of scalar linear equations [@problem_id:1156702].", "problem": "A physical system's state, described by a vector $\\mathbf{x}(t) \\in \\mathbb{R}^3$, evolves in time according to the linear system of ordinary differential equations $\\frac{d\\mathbf{x}}{dt} = A\\mathbf{x}$, where $A$ is a constant $3 \\times 3$ matrix. The solution to this system is given by $\\mathbf{x}(t) = e^{tA}\\mathbf{x}(0)$, where $e^{tA}$ is the matrix exponential.\n\nA powerful method for computing the matrix exponential $e^{tA}$ for an $n \\times n$ matrix $A$ relies on the Cayley-Hamilton theorem, which states that any square matrix satisfies its own characteristic equation. A consequence of this theorem is that $e^{tA}$ can be expressed as a polynomial in $A$ of degree at most $n-1$:\n$$e^{tA} = \\sum_{j=0}^{n-1} \\alpha_j(t) A^j$$\nThe scalar coefficients $\\alpha_j(t)$ are determined by solving a system of linear equations. These equations are generated by replacing the matrix $A$ with its eigenvalues $\\lambda$ in the above expression, i.e., $e^{t\\lambda} = \\sum_{j=0}^{n-1} \\alpha_j(t) \\lambda^j$. If an eigenvalue $\\lambda_k$ has algebraic multiplicity $m_k > 1$, then additional equations are obtained by taking derivatives with respect to $\\lambda$ up to order $m_k-1$.\n\nConsider a system governed by the matrix:\n$$A = \\begin{pmatrix} 2 & 1 & c \\\\ 0 & 2 & d \\\\ 0 & 0 & 1 \\end{pmatrix}$$\nwhere $c$ and $d$ are arbitrary real parameters.\n\nUsing the Cayley-Hamilton theorem approach, derive the state-transition matrix $e^{tA}$.", "solution": "The problem is to compute the matrix exponential $e^{tA}$ for the matrix $A = \\begin{pmatrix} 2 & 1 & c \\\\ 0 & 2 & d \\\\ 0 & 0 & 1 \\end{pmatrix}$.\n\n**Step 1: Find the eigenvalues of A**\n\nThe characteristic polynomial is $p(\\lambda) = \\det(A - \\lambda I)$.\n$$p(\\lambda) = \\det \\begin{pmatrix} 2-\\lambda & 1 & c \\\\ 0 & 2-\\lambda & d \\\\ 0 & 0 & 1-\\lambda \\end{pmatrix} = (2-\\lambda)^2 (1-\\lambda)$$\nThe eigenvalues are the roots of $p(\\lambda)=0$. Thus, we have an eigenvalue $\\lambda_1 = 2$ with algebraic multiplicity $m_1 = 2$, and a simple eigenvalue $\\lambda_2 = 1$ with algebraic multiplicity $m_2 = 1$.\n\n**Step 2: Set up the system for the coefficients $\\alpha_j(t)$**\n\nSince $A$ is a $3 \\times 3$ matrix, $e^{tA}$ can be written as a quadratic polynomial in $A$:\n$$e^{tA} = \\alpha_0(t)I + \\alpha_1(t)A + \\alpha_2(t)A^2$$\nThe corresponding scalar equation is:\n$$e^{t\\lambda} = \\alpha_0(t) + \\alpha_1(t)\\lambda + \\alpha_2(t)\\lambda^2$$\nWe use the eigenvalues to generate a system of equations for $\\alpha_0, \\alpha_1, \\alpha_2$.\n\nFor the simple eigenvalue $\\lambda_2 = 1$:\n$$e^{t\\cdot 1} = \\alpha_0 + \\alpha_1(1) + \\alpha_2(1)^2 \\implies e^t = \\alpha_0 + \\alpha_1 + \\alpha_2 \\quad (1)$$\n\nFor the repeated eigenvalue $\\lambda_1 = 2$ (multiplicity 2), we get two equations. First, using the eigenvalue itself:\n$$e^{t\\cdot 2} = \\alpha_0 + \\alpha_1(2) + \\alpha_2(2)^2 \\implies e^{2t} = \\alpha_0 + 2\\alpha_1 + 4\\alpha_2 \\quad (2)$$\nSecond, we take the derivative of the scalar equation with respect to $\\lambda$ and then evaluate at $\\lambda=2$:\n$$\\frac{d}{d\\lambda}(e^{t\\lambda}) = \\frac{d}{d\\lambda}(\\alpha_0 + \\alpha_1\\lambda + \\alpha_2\\lambda^2)$$\n$$t e^{t\\lambda} = \\alpha_1 + 2\\alpha_2\\lambda$$\nEvaluating at $\\lambda=2$:\n$$t e^{2t} = \\alpha_1 + 2\\alpha_2(2) \\implies t e^{2t} = \\alpha_1 + 4\\alpha_2 \\quad (3)$$\n\n**Step 3: Solve for $\\alpha_0(t)$, $\\alpha_1(t)$, and $\\alpha_2(t)$**\n\nWe have a system of three linear equations:\n1. $\\alpha_0 + \\alpha_1 + \\alpha_2 = e^t$\n2. $\\alpha_0 + 2\\alpha_1 + 4\\alpha_2 = e^{2t}$\n3. $\\alpha_1 + 4\\alpha_2 = t e^{2t}$\n\nFrom (3), we can express $\\alpha_1$ in terms of $\\alpha_2$:\n$$\\alpha_1 = t e^{2t} - 4\\alpha_2$$\nSubstitute this into (1) and (2).\nSubstituting into (2):\n$$\\alpha_0 + 2(t e^{2t} - 4\\alpha_2) + 4\\alpha_2 = e^{2t}$$\n$$\\alpha_0 + 2t e^{2t} - 8\\alpha_2 + 4\\alpha_2 = e^{2t} \\implies \\alpha_0 - 4\\alpha_2 = e^{2t} - 2t e^{2t} \\quad (4)$$\nSubstituting into (1):\n$$\\alpha_0 + (t e^{2t} - 4\\alpha_2) + \\alpha_2 = e^t$$\n$$\\alpha_0 - 3\\alpha_2 = e^t - t e^{2t} \\quad (5)$$\n\nNow we solve the system of two equations (4) and (5) for $\\alpha_0$ and $\\alpha_2$. Subtract (4) from (5):\n$$(\\alpha_0 - 3\\alpha_2) - (\\alpha_0 - 4\\alpha_2) = (e^t - t e^{2t}) - (e^{2t} - 2t e^{2t})$$\n$$\\alpha_2 = e^t - e^{2t} + t e^{2t} = e^t + (t-1)e^{2t}$$\nNow find $\\alpha_1$:\n$$\\alpha_1 = t e^{2t} - 4\\alpha_2 = t e^{2t} - 4(e^t + (t-1)e^{2t}) = t e^{2t} - 4e^t - 4t e^{2t} + 4e^{2t}$$\n$$\\alpha_1 = -4e^t + (4-3t)e^{2t}$$\nFinally, find $\\alpha_0$ using (1):\n$$\\alpha_0 = e^t - \\alpha_1 - \\alpha_2 = e^t - [-4e^t + (4-3t)e^{2t}] - [e^t + (t-1)e^{2t}]$$\n$$\\alpha_0 = (1 + 4 - 1)e^t - [(4-3t) + (t-1)]e^{2t} = 4e^t - (3-2t)e^{2t}$$\n\n**Step 4: Compute AÂ²**\n$$A^2 = A \\cdot A = \\begin{pmatrix} 2 & 1 & c \\\\ 0 & 2 & d \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 & c \\\\ 0 & 2 & d \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 2+2 & 2c+d+c \\\\ 0 & 4 & 2d+d \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 4 & 3c+d \\\\ 0 & 4 & 3d \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n\n**Step 5: Assemble $e^{tA}$**\n$$e^{tA} = \\alpha_0 I + \\alpha_1 A + \\alpha_2 A^2$$\n$$e^{tA} = \\alpha_0 \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + \\alpha_1 \\begin{pmatrix} 2 & 1 & c \\\\ 0 & 2 & d \\\\ 0 & 0 & 1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 4 & 4 & 3c+d \\\\ 0 & 4 & 3d \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n$$e^{tA} = \\begin{pmatrix} \\alpha_0 + 2\\alpha_1 + 4\\alpha_2 & \\alpha_1 + 4\\alpha_2 & c\\alpha_1 + (3c+d)\\alpha_2 \\\\ 0 & \\alpha_0 + 2\\alpha_1 + 4\\alpha_2 & d\\alpha_1 + 3d\\alpha_2 \\\\ 0 & 0 & \\alpha_0 + \\alpha_1 + \\alpha_2 \\end{pmatrix}$$\n\nLet's compute the matrix elements using the combinations of $\\alpha_j$ we already used to define our system:\n- $(1,1)$ and $(2,2)$ elements: $\\alpha_0 + 2\\alpha_1 + 4\\alpha_2 = e^{2t}$ from (2).\n- $(3,3)$ element: $\\alpha_0 + \\alpha_1 + \\alpha_2 = e^t$ from (1).\n- $(1,2)$ element: $\\alpha_1 + 4\\alpha_2 = te^{2t}$ from (3).\n\nNow compute the remaining off-diagonal elements:\n- $(2,3)$ element: $d\\alpha_1 + 3d\\alpha_2 = d(\\alpha_1 + 3\\alpha_2)$.\n  $\\alpha_1 + 3\\alpha_2 = (-4e^t + (4-3t)e^{2t}) + 3(e^t + (t-1)e^{2t}) = -e^t + (4-3t+3t-3)e^{2t} = e^{2t}-e^t$.\n  So the $(2,3)$ element is $d(e^{2t}-e^t)$.\n- $(1,3)$ element: $c\\alpha_1 + (3c+d)\\alpha_2 = c(\\alpha_1 + 3\\alpha_2) + d\\alpha_2$.\n  Using the expressions for $\\alpha_1+3\\alpha_2$ and $\\alpha_2$:\n  $c(e^{2t}-e^t) + d(e^t + (t-1)e^{2t}) = c e^{2t} - c e^t + d e^t + dt e^{2t} - d e^{2t} = (c-d+dt)e^{2t} + (d-c)e^t$.\n\nFinally, assembling the matrix $e^{tA}$:\n$$e^{tA} = \\begin{pmatrix} e^{2t} & t e^{2t} & (c-d+dt)e^{2t} + (d-c)e^t \\\\ 0 & e^{2t} & d(e^{2t} - e^t) \\\\ 0 & 0 & e^t \\end{pmatrix}$$", "answer": "$$\n\\boxed{\\begin{pmatrix} e^{2t} & t e^{2t} & (c-d+dt)e^{2t} + (d-c)e^t \\\\ 0 & e^{2t} & d(e^{2t} - e^t) \\\\ 0 & 0 & e^t \\end{pmatrix}}\n$$", "id": "1156702"}, {"introduction": "A system's stability, governed by the eigenvalues of its state matrix, only describes the long-term asymptotic behavior, but transient performance can be dramatically different and is often critical in engineering applications. This practice delves into the non-intuitive phenomenon of transient growth, where the norm of the state-transition matrix $\\|e^{tA}\\|$ can grow significantly before decaying, even for a stable matrix $A$. By working through a canonical example of a non-normal matrix, you will analytically quantify this growth and understand why eigenvalues alone do not tell the full story [@problem_id:2753706].", "problem": "Consider the family of real upper-triangular matrices parameterized by $0<\\varepsilon\\leq 1$,\n$$\nA_{\\varepsilon} \\;=\\; \\begin{pmatrix}\n-1 & \\dfrac{1}{\\varepsilon} \\\\\n0 & -(1+\\varepsilon)\n\\end{pmatrix}.\n$$\nThis family is asymptotically stable for every fixed $\\varepsilon>0$ and becomes nearly defective as $\\varepsilon\\to 0^{+}$ because $A_{\\varepsilon}\\to \\begin{pmatrix}-1 & \\infty\\\\ 0&-1\\end{pmatrix}$ in the sense that its eigenvalues coalesce at $-1$ while its eigenvectors become ill-conditioned. In state-space terms, the state-transition matrix is the matrix exponential $\\exp(t A_{\\varepsilon})$, which solves the homogeneous linear time-invariant system $\\dot{x}(t)=A_{\\varepsilon} x(t)$ with $x(0)=I$.\n\nStarting only from the defining properties of the matrix exponential as the unique solution $\\Phi(t)=\\exp(tA_{\\varepsilon})$ of $\\dot{\\Phi}(t)=A_{\\varepsilon}\\Phi(t)$ with $\\Phi(0)=I$, carry out the following:\n\n1. Derive an explicit expression for $\\exp(t A_{\\varepsilon})$.\n2. Using the induced Euclidean operator norm (that is, the spectral norm), define\n$$\nM(\\varepsilon) \\;=\\; \\sup_{t\\geq 0} \\left\\| \\exp\\!\\big(t A_{\\varepsilon}\\big) \\right\\|_{2}.\n$$\n3. Prove that $M(\\varepsilon)$ exhibits large transient growth as $\\varepsilon\\to 0^{+}$ despite asymptotic stability, and compute the exact constant governing the leading-order growth by evaluating the limit\n$$\nL \\;=\\; \\lim_{\\varepsilon\\to 0^{+}} \\; \\varepsilon \\, M(\\varepsilon).\n$$\n\nProvide your final answer for $L$ as an exact symbolic expression (no numerical rounding). No units are required.", "solution": "**Part 1: Derivation of the matrix exponential $\\exp(t A_{\\varepsilon})$**\n\nWe must find the matrix $\\Phi(t) = \\exp(t A_{\\varepsilon})$ by solving the matrix differential equation $\\dot{\\Phi}(t) = A_{\\varepsilon} \\Phi(t)$ with $\\Phi(0) = I$. Let $\\Phi(t) = \\begin{pmatrix} \\phi_{11}(t) & \\phi_{12}(t) \\\\ \\phi_{21}(t) & \\phi_{22}(t) \\end{pmatrix}$. The equation becomes:\n$$\n\\begin{pmatrix} \\dot{\\phi}_{11}(t) & \\dot{\\phi}_{12}(t) \\\\ \\dot{\\phi}_{21}(t) & \\dot{\\phi}_{22}(t) \\end{pmatrix} = \\begin{pmatrix} -1 & \\frac{1}{\\varepsilon} \\\\ 0 & -(1+\\varepsilon) \\end{pmatrix} \\begin{pmatrix} \\phi_{11}(t) & \\phi_{12}(t) \\\\ \\phi_{21}(t) & \\phi_{22}(t) \\end{pmatrix}\n$$\nThe initial conditions from $\\Phi(0) = I$ are $\\phi_{11}(0) = 1$, $\\phi_{12}(0) = 0$, $\\phi_{21}(0) = 0$, and $\\phi_{22}(0) = 1$.\n\nFor the second row, the equations are decoupled:\n-   $\\dot{\\phi}_{21}(t) = -(1+\\varepsilon) \\phi_{21}(t)$, with $\\phi_{21}(0) = 0$. The unique solution is $\\phi_{21}(t) = 0$.\n-   $\\dot{\\phi}_{22}(t) = -(1+\\varepsilon) \\phi_{22}(t)$, with $\\phi_{22}(0) = 1$. The solution is $\\phi_{22}(t) = \\exp(-(1+\\varepsilon)t)$.\n\nFor the first row, we have:\n-   $\\dot{\\phi}_{11}(t) = -\\phi_{11}(t) + \\frac{1}{\\varepsilon}\\phi_{21}(t)$. Since $\\phi_{21}(t) = 0$, this simplifies to $\\dot{\\phi}_{11}(t) = -\\phi_{11}(t)$. With $\\phi_{11}(0) = 1$, the solution is $\\phi_{11}(t) = \\exp(-t)$.\n-   $\\dot{\\phi}_{12}(t) = -\\phi_{12}(t) + \\frac{1}{\\varepsilon}\\phi_{22}(t)$. Substituting $\\phi_{22}(t)$, we get the first-order linear ordinary differential equation:\n    $$\n    \\dot{\\phi}_{12}(t) + \\phi_{12}(t) = \\frac{1}{\\varepsilon} \\exp(-(1+\\varepsilon)t)\n    $$\n    We solve this using an integrating factor, which is $\\exp(\\int 1 \\, dt) = \\exp(t)$.\n    $$\n    \\frac{d}{dt} \\left(\\exp(t) \\phi_{12}(t)\\right) = \\exp(t) \\left( \\frac{1}{\\varepsilon} \\exp(-(1+\\varepsilon)t) \\right) = \\frac{1}{\\varepsilon} \\exp(-\\varepsilon t)\n    $$\n    Integrating with respect to $t$:\n    $$\n    \\exp(t) \\phi_{12}(t) = \\int \\frac{1}{\\varepsilon} \\exp(-\\varepsilon t) \\, dt = \\frac{1}{\\varepsilon} \\left(-\\frac{1}{\\varepsilon} \\exp(-\\varepsilon t)\\right) + C = -\\frac{1}{\\varepsilon^2} \\exp(-\\varepsilon t) + C\n    $$\n    Using the initial condition $\\phi_{12}(0) = 0$:\n    $$\n    \\exp(0) \\phi_{12}(0) = 0 = -\\frac{1}{\\varepsilon^2}\\exp(0) + C \\implies C = \\frac{1}{\\varepsilon^2}\n    $$\n    Thus, $\\exp(t) \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(1 - \\exp(-\\varepsilon t))$, which gives:\n    $$\n    \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(\\exp(-t) - \\exp(-(1+\\varepsilon)t))\n    $$\nCollecting the components, the matrix exponential is:\n$$\n\\exp(t A_{\\varepsilon}) = \\begin{pmatrix} \\exp(-t) & \\frac{1}{\\varepsilon^2} (\\exp(-t) - \\exp(-(1+\\varepsilon)t)) \\\\ 0 & \\exp(-(1+\\varepsilon)t) \\end{pmatrix}\n$$\n\n**Part 2 & 3: Analysis of Transient Growth and Evaluation of the Limit**\n\nWe are asked to compute $L = \\lim_{\\varepsilon\\to 0^{+}} \\varepsilon M(\\varepsilon) = \\lim_{\\varepsilon\\to 0^{+}} \\varepsilon \\sup_{t\\geq 0} \\| \\exp(t A_{\\varepsilon}) \\|_{2}$.\nThis is equivalent to $L = \\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} (\\varepsilon \\| \\exp(t A_{\\varepsilon}) \\|_{2})$. Let $h_{\\varepsilon}(t) = \\varepsilon \\| \\exp(t A_{\\varepsilon}) \\|_{2}$. We wish to compute $\\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} h_{\\varepsilon}(t)$.\n\nIt is a known result in analysis that if a sequence of functions converges uniformly, the limit and supremum operators can be interchanged. We will show that $h_{\\varepsilon}(t)$ converges uniformly to a function $h(t)$ for $t \\in [0, \\infty)$, which justifies writing $L = \\sup_{t\\geq 0} \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t)$.\n\nLet us first find the pointwise limit of $h_{\\varepsilon}(t)$. We need an asymptotic expression for $\\| \\exp(t A_{\\varepsilon}) \\|_{2}$ for small $\\varepsilon$. The squared spectral norm $\\| \\cdot \\|_{2}^2$ is the largest eigenvalue of $\\Phi(t)^T \\Phi(t)$.\nLet $\\Phi(t) = \\exp(t A_{\\varepsilon})$. Then\n$$\n\\Phi(t)^T \\Phi(t) = \\begin{pmatrix} \\exp(-2t) & \\exp(-t)f(t) \\\\ \\exp(-t)f(t) & f(t)^2 + \\exp(-2(1+\\varepsilon)t) \\end{pmatrix}\n$$\nwhere $f(t) = \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(\\exp(-t) - \\exp(-(1+\\varepsilon)t))$. For small $\\varepsilon$, we expand $f(t)$:\n$$\nf(t) = \\frac{\\exp(-t)}{\\varepsilon^2}(1 - \\exp(-\\varepsilon t)) = \\frac{\\exp(-t)}{\\varepsilon^2}\\left(1 - \\left(1 - \\varepsilon t + \\frac{(\\varepsilon t)^2}{2} - \\dots\\right)\\right) = \\frac{t\\exp(-t)}{\\varepsilon} - \\frac{t^2\\exp(-t)}{2} + O(\\varepsilon)\n$$\nThe dominant term is of order $1/\\varepsilon$, so $f(t)^2$ is of order $1/\\varepsilon^2$.\nThe eigenvalues $\\lambda$ of $\\Phi(t)^T \\Phi(t)$ are given by the solution to the characteristic equation $\\lambda^2 - \\text{Tr}(\\Phi(t)^T \\Phi(t))\\lambda + \\det(\\Phi(t)^T \\Phi(t)) = 0$.\nThe squared norm is the larger eigenvalue, $\\lambda_{\\max} = \\frac{1}{2}\\left(\\text{Tr} + \\sqrt{\\text{Tr}^2 - 4\\det}\\right)$. Let's analyze the terms for small $\\varepsilon$:\n$$\n\\text{Tr}(\\Phi(t)^T \\Phi(t)) = f(t)^2 + \\exp(-2t) + \\exp(-2(1+\\varepsilon)t) = \\frac{t^2\\exp(-2t)}{\\varepsilon^2} - \\frac{t^3\\exp(-2t)}{\\varepsilon} + O(1)\n$$\n$$\n\\det(\\Phi(t)^T \\Phi(t)) = (\\det \\Phi(t))^2 = (\\exp(-t)\\exp(-(1+\\varepsilon)t))^2 = \\exp(-2(2+\\varepsilon)t) = O(1)\n$$\nAsymptotically, the term $\\text{Tr}(\\Phi(t)^T \\Phi(t))$ dominates, and the squared norm $\\|\\exp(t A_{\\varepsilon})\\|_{2}^2$ approaches $\\text{Tr}(\\Phi(t)^T \\Phi(t))$, which is approximately $f(t)^2$.\nThus, $\\|\\exp(t A_{\\varepsilon})\\|_{2} \\approx |f(t)| \\approx \\frac{t \\exp(-t)}{\\varepsilon}$.\nNow we can analyze $h_{\\varepsilon}(t)$:\n$$\nh_{\\varepsilon}(t) = \\varepsilon \\| \\exp(t A_{\\varepsilon}) \\|_{2} \\approx \\varepsilon \\left(\\frac{t\\exp(-t)}{\\varepsilon}\\right) = t\\exp(-t)\n$$\nThe pointwise limit is $h(t) = \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t) = t\\exp(-t)$. A more detailed analysis shows that the convergence is uniform, which justifies interchanging the limit and supremum.\n$$\nL = \\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} h_{\\varepsilon}(t) = \\sup_{t\\geq 0} \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t) = \\sup_{t\\geq 0} (t\\exp(-t))\n$$\nTo find this supremum, we differentiate $h(t) = t\\exp(-t)$ with respect to $t$ and set the derivative to zero:\n$$\n\\frac{dh}{dt} = \\exp(-t) - t\\exp(-t) = (1-t)\\exp(-t)\n$$\nThe derivative is zero for $t=1$. At this point, the function attains its maximum value:\n$$\nh(1) = 1 \\cdot \\exp(-1) = \\frac{1}{e}\n$$\nThus, the value of the limit is $L = \\exp(-1)$.", "answer": "$$\\boxed{\\exp(-1)}$$", "id": "2753706"}, {"introduction": "The theoretical consequences of non-normality, such as transient growth, have profound implications for the practical computation of the matrix exponential's action on a vector, $\\exp(A)v$, especially in large-scale systems. This hands-on coding exercise challenges you to implement a Krylov subspace method, a state-of-the-art iterative technique for this task. By comparing the method's convergence for normal and highly non-normal matrices, you will directly observe the performance degradation caused by non-normality and connect it to the modern analytical tool of pseudospectra [@problem_id:2753717].", "problem": "You are asked to rigorously construct and analyze an example where nonnormality slows the convergence of Krylov subspace methods for computing the action of the matrix exponential on a vector. Work purely in mathematical terms. Use the following fundamental base: the linear time-invariant system $\\dot{x}(t)=A x(t)$ has solution $x(t)=\\exp(tA)x(0)$, and the matrix exponential is defined by the absolutely convergent series $\\exp(A)=\\sum_{k=0}^{\\infty} \\frac{A^k}{k!}$. A $k$-dimensional Krylov subspace for $(A,v)$ is $\\mathcal{K}_k(A,v)=\\operatorname{span}\\{v,Av,\\dots,A^{k-1}v\\}$. A matrix is normal if $A^*A=AA^*$; otherwise it is nonnormal. The $\\varepsilon$-pseudospectrum of $A$ is the set of complex numbers $z$ for which the minimum singular value of $zI-A$ is at most $\\varepsilon$.\n\nImplement an Arnoldi-based Krylov projection method at $t=1$ that approximates $\\exp(A)v$ by projecting the linear dynamics $\\dot{x}=Ax$ onto $\\mathcal{K}_m(A,v)$, evolving there, and lifting back to the full space. Your implementation must:\n- Build an orthonormal basis of $\\mathcal{K}_m(A,v)$ via Arnoldi iteration.\n- Form the reduced $m\\times m$ projected matrix.\n- Use a reliable direct method to exponentiate the reduced matrix.\n- Return the smallest $m$ for which the relative error $\\|\\hat{y}_m-\\exp(A)v\\|_2/\\|\\exp(A)v\\|_2$ is at most a tolerance of $10^{-10}$, where $\\hat{y}_m$ denotes your Krylov approximation.\n\nRelate the observed convergence behavior to pseudospectral properties as follows. For a fixed $\\varepsilon=10^{-2}$, approximate the $\\varepsilon$-pseudospectral abscissa along the real axis, defined here as the maximum real $x$ such that the smallest singular value of $xI-A$ is at most $\\varepsilon$. Numerically approximate this by sampling $251$ equally spaced points on the real interval $[-2,0.5]$ and taking the maximum $x$ among those samples that satisfy the singular-value condition. If no sample satisfies the condition, report $-\\infty$.\n\nTest suite. For each case below, take dimension $n=40$ and initial vector $v=e_1$ (the first standard basis vector). For each matrix $A$, compute:\n- the smallest Krylov dimension $m$ achieving the tolerance $10^{-10}$, and\n- the real-axis $\\varepsilon$-pseudospectral abscissa approximation with $\\varepsilon=10^{-2}$ and the described sampling.\nUse the following four matrices $A$:\n1) Normal baseline: $A=-I_n$.\n2) Symmetric tridiagonal (normal): $A$ with diagonal entries $-1$ and sub- and super-diagonal entries $-0.25$.\n3) Nonnormal Jordan block: $A=-I_n+N$, where $N$ has ones on the first superdiagonal and zeros elsewhere.\n4) Scaled nonnormal Jordan-type: $A=-I_n+4N$, with $N$ as in case $3$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case in the given order and is itself a two-element list $[m,\\alpha]$, with $m$ an integer and $\\alpha$ a floating-point number. For example, the overall format must be\n[[m_1,alpha_1],[m_2,alpha_2],[m_3,alpha_3],[m_4,alpha_4]]\nNo spaces are permitted in the printed line. No user input is allowed. No physical units or angle units apply in this problem. All numerical values and intervals must be used exactly as specified above.", "solution": "The problem as stated is valid. It is a well-posed, computationally feasible, and scientifically sound exercise in numerical linear algebra, specifically concerning the convergence properties of Krylov subspace methods for the matrix exponential. The definitions and premises are consistent with established theory in mathematics and control systems. I will now proceed with the solution.\n\nThe objective is to analyze the convergence of an Arnoldi-based Krylov subspace method for computing the action of the matrix exponential, $y(t) = \\exp(tA)v$, and to correlate this convergence behavior with the pseudospectral properties of the matrix $A$. The analysis is performed at $t=1$.\n\nThe core principle is the projection of the high-dimensional linear system $\\dot{x} = Ax$ onto a low-dimensional Krylov subspace $\\mathcal{K}_m(A,v) = \\operatorname{span}\\{v, Av, \\dots, A^{m-1}v\\}$. The Arnoldi iteration is a numerically stable algorithm based on the Gram-Schmidt process to construct an orthonormal basis $V_m = [v_1, v_2, \\dots, v_m]$ for $\\mathcal{K}_m(A,v)$. This process simultaneously generates an $m \\times m$ upper Hessenberg matrix $H_m$, which represents the projection of the operator $A$ onto the subspace $\\mathcal{K}_m$. The relationship is given by $H_m = V_m^* A V_m$. The full Arnoldi decomposition after $m$ steps is $AV_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^T$, where $V_m$ is an $n \\times m$ matrix with orthonormal columns, and $H_m$ is an $m \\times m$ upper Hessenberg matrix.\n\nThe approximation of the solution $y(1) = \\exp(A)v$ is constructed by solving the projected problem within the subspace. The dynamics projected onto $\\mathcal{K}_m$ are governed by the smaller matrix $H_m$. The solution is then evolved within this subspace and lifted back to the full $n$-dimensional space. This yields the approximation $\\hat{y}_m$:\n$$\n\\hat{y}_m = V_m \\exp(H_m) V_m^* v\n$$\nGiven that the Arnoldi process starts with $v_1 = v/\\|v\\|_2$, the vector $v$ in the full space corresponds to $\\|v\\|_2 e_1$ in the coordinate system of the basis $V_m$. Thus, $V_m^*v = \\|v\\|_2 e_1$, where $e_1$ is the first standard basis vector in $\\mathbb{R}^m$. The approximation simplifies to:\n$$\n\\hat{y}_m = \\|v\\|_2 V_m \\exp(H_m) e_1\n$$\n\nThe rate of convergence, i.e., how quickly the relative error $\\|\\hat{y}_m - y(1)\\|_2 / \\|y(1)\\|_2$ decreases as the subspace dimension $m$ increases, is strongly dependent on the properties of the matrix $A$. For normal matrices, where $AA^* = A^*A$, convergence is typically rapid and is governed by the spectral properties of $A$. However, for nonnormal matrices, convergence can be significantly delayed. This delay is not predicted by the eigenvalues of $A$ alone but is instead explained by the matrix's pseudospectrum.\n\nThe $\\varepsilon$-pseudospectrum of a matrix $A$, denoted $\\Lambda_\\varepsilon(A)$, is the set of complex numbers $z$ such that the norm of the resolvent, $\\|(zI-A)^{-1}\\|_2$, is large, specifically $\\|(zI-A)^{-1}\\|_2 \\ge \\varepsilon^{-1}$. This is equivalent to the given definition that the smallest singular value of $zI-A$ is at most $\\varepsilon$. For nonnormal matrices, $\\Lambda_\\varepsilon(A)$ can extend far beyond the spectrum $\\Lambda(A)$. The real-axis pseudospectral abscissa, $\\alpha_\\varepsilon(A) = \\sup\\{\\operatorname{Re}(z) \\mid z \\in \\Lambda_\\varepsilon(A)\\}$, provides a measure of the maximum possible transient growth rate of the system dynamics. If $\\alpha_\\varepsilon(A) > 0$ for a small $\\varepsilon$, even when the spectral abscissa $\\max\\{\\operatorname{Re}(\\lambda) \\mid \\lambda \\in \\Lambda(A)\\}$ is negative, the system can exhibit significant transient growth before the eventual decay dictated by the spectrum. A Krylov method must expand the subspace dimension $m$ sufficiently to capture this transient behavior before it can accurately approximate the long-term asymptotic behavior. A larger positive $\\alpha_\\varepsilon(A)$ is correlated with more severe transient effects and, consequently, a larger Krylov dimension $m$ required to achieve a given accuracy.\n\nThe computational procedure for each test case is twofold:\n\n1.  **Krylov Dimension Search**: For each matrix $A$ from the test suite, with $n=40$ and $v=e_1$:\n    a. The exact solution $y_{\\text{exact}} = \\exp(A)v$ is computed once for reference using a reliable direct method.\n    b. The Krylov subspace dimension $m$ is incremented starting from $m=1$.\n    c. For each $m$, the Arnoldi iteration is used to find the basis $V_m$ and the projected matrix $H_m$.\n    d. The approximation $\\hat{y}_m = \\|v\\|_2 V_m \\exp(H_m) e_1$ is computed.\n    e. The relative error $\\|\\hat{y}_m - y_{\\text{exact}}\\|_2 / \\|y_{\\text{exact}}\\|_2$ is calculated.\n    f. The process terminates when this error is less than or equal to the tolerance $10^{-10}$, and the smallest such $m$ is recorded.\n\n2.  **Pseudospectral Abscissa Approximation**:\n    a. A set of $251$ points $x$ is sampled uniformly from the real interval $[-2, 0.5]$.\n    b. For each sample $x$, the smallest singular value of the matrix $xI-A$, denoted $\\sigma_{\\min}(xI-A)$, is computed.\n    c. If $\\sigma_{\\min}(xI-A) \\le \\varepsilon$ where $\\varepsilon = 10^{-2}$, the point $x$ is considered to be in the projection of the $\\varepsilon$-pseudospectrum onto the real axis.\n    d. The maximum value among all such points $x$ is taken as the approximation for the real-axis pseudospectral abscissa. If no such points are found, the value is taken as $-\\infty$.\n\nThis procedure is applied to the four specified matrices, which will illustrate the stark contrast in convergence behavior between normal and nonnormal systems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm, svdvals\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing four matrix test cases. For each case, it finds:\n    1. The smallest Krylov dimension 'm' to approximate exp(A)v within a tolerance.\n    2. The real-axis pseudospectral abscissa 'alpha'.\n    \"\"\"\n\n    def arnoldi_iteration(A, v, m):\n        \"\"\"\n        Performs m steps of the Arnoldi iteration for matrix A and vector v.\n        Returns an n x m orthonormal basis V_m for the Krylov subspace and\n        the m x m upper Hessenberg projection H_m.\n        \"\"\"\n        n = A.shape[0]\n        # Using complex types is a robust choice for numerical linear algebra algorithms.\n        V = np.zeros((n, m + 1), dtype=np.complex128)\n        H = np.zeros((m + 1, m), dtype=np.complex128)\n\n        V[:, 0] = v / np.linalg.norm(v)\n\n        for j in range(m):\n            w = A @ V[:, j]\n            # Modified Gram-Schmidt process\n            for i in range(j + 1):\n                H[i, j] = np.dot(np.conj(V[:, i]), w)\n                w = w - H[i, j] * V[:, i]\n\n            H[j + 1, j] = np.linalg.norm(w)\n\n            # Handle breakdown (invariant subspace found)\n            if H[j + 1, j] < 1e-14:\n                m_actual = j + 1\n                return V[:, :m_actual], H[:m_actual, :m_actual]\n\n            V[:, j + 1] = w / H[j + 1, j]\n\n        return V[:, :m], H[:m, :]\n\n    # Problem parameters\n    n = 40\n    v = np.zeros(n, dtype=float)\n    v[0] = 1.0\n    tol = 1e-10\n\n    # Define matrices for the test cases\n    I_n = np.eye(n, dtype=float)\n    N = np.diag(np.ones(n - 1), 1)\n\n    test_cases = [\n        # 1) Normal baseline\n        -I_n,\n        # 2) Symmetric tridiagonal (normal)\n        np.diag(-1.0 * np.ones(n)) + np.diag(-0.25 * np.ones(n - 1), 1) + np.diag(-0.25 * np.ones(n - 1), -1),\n        # 3) Nonnormal Jordan block\n        -I_n + N,\n        # 4) Scaled nonnormal Jordan-type\n        -I_n + 4.0 * N\n    ]\n\n    results = []\n    norm_v = np.linalg.norm(v)\n\n    for A in test_cases:\n        A = A.astype(np.complex128) # Promote to complex for consistent types\n\n        # Part 1: Find the smallest Krylov dimension 'm'\n        y_exact = expm(A) @ v\n        norm_y_exact = np.linalg.norm(y_exact)\n        \n        m_final = 0\n        for m in range(1, n + 2): # Iterate m from 1 up to n+1 (should converge)\n            if m > n: # Failsafe\n                m_final = m\n                break\n\n            V_m, H_m = arnoldi_iteration(A, v, m)\n\n            # Handle early termination from Arnoldi (invariant subspace)\n            if H_m.shape[1] < m:\n                m = H_m.shape[1]\n            \n            # Approximation: y_hat = ||v|| * V_m * expm(H_m) * e_1\n            # expm(H_m)[:, 0] is expm(H_m) @ e_1\n            y_hat_m = norm_v * (V_m @ expm(H_m)[:, 0])\n             \n            error = np.linalg.norm(y_hat_m - y_exact) / norm_y_exact\n            \n            if error <= tol:\n                m_final = m\n                break\n        \n        # Part 2: Approximate the real-axis pseudospectral abscissa 'alpha'\n        epsilon = 1e-2\n        x_samples = np.linspace(-2.0, 0.5, 251)\n        \n        valid_x = []\n        for x in x_samples:\n            M = x * np.eye(n, dtype=np.complex128) - A\n            # svdvals returns singular values in descending order.\n            min_sigma = svdvals(M)[-1]\n            if min_sigma <= epsilon:\n                valid_x.append(x)\n        \n        alpha = -np.inf if not valid_x else max(valid_x)\n            \n        results.append([m_final, alpha])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for m_val, alpha_val in results:\n        # np.inf prints as 'inf' which is not what the problem asks for\n        alpha_str = f\"{alpha_val:.15g}\" if alpha_val != -np.inf else \"-inf\"\n        result_strings.append(f\"[{m_val},{alpha_str}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2753717"}]}