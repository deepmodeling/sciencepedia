{"hands_on_practices": [{"introduction": "A crucial first step in analyzing a control system is to determine whether its internal states can be inferred from its outputs. This exercise provides practice with two fundamental methods for assessing observability: the algebraic Kalman rank test and the analytical observability Gramian. By applying both techniques to the same system [@problem_id:2888329], you will solidify your understanding of these core concepts and see how they provide equivalent conclusions about a system's properties.", "problem": "Consider the continuous-time linear time-invariant (LTI) system with state equation $\\dot{x}(t)=A\\,x(t)$ and output equation $y(t)=C\\,x(t)$, where $A=\\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}$ and $C=\\begin{bmatrix}1 & 0\\end{bmatrix}$. \n\na) Using the fundamental definition of observability for LTI systems in terms of the ability to reconstruct the initial condition $x(0)$ from output measurements, construct the observability matrix $\\mathcal{O}$ for this pair and compute its rank.\n\nb) Based on your result in part (a), decide whether the pair $(A,C)$ is observable in the sense of Kalman.\n\nc) Using the definition of the finite-time observability Gramian for continuous-time LTI systems, compute the Gramian over the interval $[0,1]$ and then compute its determinant. Report the scalar value of $\\det(W_{o}(1))$ as your final answer, expressed exactly with no rounding and no units.\n\nYour final submitted answer must be the single scalar value of $\\det(W_{o}(1))$ only. Do not include any intermediate objects in the final answer.", "solution": "The problem presents a standard task in linear systems theory: the analysis of observability for a continuous-time linear time-invariant (LTI) system. All components of the problem are well-defined, scientifically sound, and self-contained. The problem is therefore valid, and I shall proceed with its solution.\n\nThe system is defined by the state-space representation:\n$$ \\dot{x}(t)=A\\,x(t) $$\n$$ y(t)=C\\,x(t) $$\nwith the state matrix $A$ and output matrix $C$ given as:\n$$ A=\\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}, \\quad C=\\begin{bmatrix}1 & 0\\end{bmatrix} $$\nThe dimension of the state vector $x(t)$ is $n=2$.\n\na) The fundamental definition of observability relates to the ability to determine the initial state $x(0)$ from future output measurements $y(t)$ for $t \\ge 0$. The Kalman observability rank condition provides an algebraic test for this property. The observability matrix, $\\mathcal{O}$, for an $n$-dimensional system is constructed as:\n$$ \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\vdots \\\\ CA^{n-1} \\end{bmatrix} $$\nFor this system, $n=2$, so the observability matrix is:\n$$ \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} $$\nWe compute the product $CA$:\n$$ CA = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix} = \\begin{bmatrix} (1)(0)+(0)(0) & (1)(1)+(0)(0) \\end{bmatrix} = \\begin{bmatrix}0 & 1\\end{bmatrix} $$\nSubstituting $C$ and $CA$ into the structure of $\\mathcal{O}$ gives:\n$$ \\mathcal{O} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $$\nThis is the $2 \\times 2$ identity matrix, $I_2$. The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). The rank of the $2 \\times 2$ identity matrix is manifestly $2$.\n$$ \\text{rank}(\\mathcal{O}) = \\text{rank}(I_2) = 2 $$\n\nb) The Kalman observability criterion states that a pair $(A,C)$ is observable if and only if its corresponding observability matrix $\\mathcal{O}$ has full column rank, which is equal to the dimension of the state, $n$.\nIn this case, we have $n=2$ and we found that $\\text{rank}(\\mathcal{O})=2$. Since $\\text{rank}(\\mathcal{O}) = n$, the pair $(A,C)$ is indeed observable.\n\nc) The finite-time observability Gramian for a continuous-time LTI system over the time interval $[0, t_f]$ is defined as:\n$$ W_{o}(t_f) = \\int_0^{t_f} e^{A^T \\tau} C^T C e^{A \\tau} \\, d\\tau $$\nWe are asked to compute this for the interval $[0,1]$, so $t_f=1$.\nFirst, we must compute the matrix exponential $e^{A\\tau}$. The matrix $A$ is nilpotent, as $A^2 = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}\\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix} = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}$. Therefore, all higher powers $A^k$ for $k \\geq 2$ are also the zero matrix. The Taylor series expansion of the matrix exponential truncates:\n$$ e^{A\\tau} = I + A\\tau + \\frac{(A\\tau)^2}{2!} + \\dots = I + A\\tau = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} + \\begin{bmatrix}0 & \\tau \\\\ 0 & 0\\end{bmatrix} = \\begin{bmatrix}1 & \\tau \\\\ 0 & 1\\end{bmatrix} $$\nNext, we find the transpose of $A$, which is $A^T = \\begin{bmatrix}0 & 0 \\\\ 1 & 0\\end{bmatrix}$. The matrix exponential $e^{A^T \\tau}$ is found similarly, or by taking the transpose of $e^{A\\tau}$:\n$$ e^{A^T \\tau} = (e^{A\\tau})^T = \\begin{bmatrix}1 & \\tau \\\\ 0 & 1\\end{bmatrix}^T = \\begin{bmatrix}1 & 0 \\\\ \\tau & 1\\end{bmatrix} $$\nThe term $C^T C$ is computed as:\n$$ C^T C = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\begin{bmatrix}1 & 0\\end{bmatrix} = \\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix} $$\nNow we form the integrand of the Gramian expression, $e^{A^T \\tau} C^T C e^{A \\tau}$:\n$$ \\begin{aligned} e^{A^T \\tau} C^T C e^{A \\tau} &= \\begin{bmatrix}1 & 0 \\\\ \\tau & 1\\end{bmatrix} \\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix} \\begin{bmatrix}1 & \\tau \\\\ 0 & 1\\end{bmatrix} \\\\ &= \\begin{bmatrix}1 & 0 \\\\ \\tau & 0\\end{bmatrix} \\begin{bmatrix}1 & \\tau \\\\ 0 & 1\\end{bmatrix} \\\\ &= \\begin{bmatrix}1 & \\tau \\\\ \\tau & \\tau^2\\end{bmatrix} \\end{aligned} $$\nThe observability Gramian $W_o(1)$ is the integral of this matrix from $\\tau=0$ to $\\tau=1$:\n$$ W_{o}(1) = \\int_0^1 \\begin{bmatrix}1 & \\tau \\\\ \\tau & \\tau^2\\end{bmatrix} d\\tau = \\begin{bmatrix} \\int_0^1 1 \\,d\\tau & \\int_0^1 \\tau \\,d\\tau \\\\ \\int_0^1 \\tau \\,d\\tau & \\int_0^1 \\tau^2 \\,d\\tau \\end{bmatrix} $$\nPerforming the element-wise integration:\n$$ \\int_0^1 1 \\,d\\tau = [\\tau]_0^1 = 1 $$\n$$ \\int_0^1 \\tau \\,d\\tau = \\left[\\frac{\\tau^2}{2}\\right]_0^1 = \\frac{1}{2} $$\n$$ \\int_0^1 \\tau^2 \\,d\\tau = \\left[\\frac{\\tau^3}{3}\\right]_0^1 = \\frac{1}{3} $$\nThus, the observability Gramian is:\n$$ W_{o}(1) = \\begin{bmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{3} \\end{bmatrix} $$\nThe final step is to compute the determinant of this matrix. For a $2 \\times 2$ matrix $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, the determinant is $ad-bc$.\n$$ \\det(W_{o}(1)) = (1)\\left(\\frac{1}{3}\\right) - \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{3} - \\frac{1}{4} $$\nTo find the difference, we use a common denominator of $12$:\n$$ \\det(W_{o}(1)) = \\frac{4}{12} - \\frac{3}{12} = \\frac{1}{12} $$\nThe non-zero value of the determinant of the observability Gramian confirms, once again, that the system is observable over the interval $[0,1]$.", "answer": "$$\\boxed{\\frac{1}{12}}$$", "id": "2888329"}, {"introduction": "When a system is not fully observable, it is not enough to simply label it \"unobservable\"; a deeper structural understanding is required. This practice guides you through the Kalman observable decomposition, a powerful technique that uses a change of basis to separate a system's state-space into its observable and unobservable subspaces. Performing this decomposition [@problem_id:2694863] is a foundational skill that reveals which parts of a system's dynamics influence the output and which remain hidden.", "problem": "Consider the linear time-invariant (LTI) system defined by the state-space realization with state matrix $A \\in \\mathbb{R}^{4 \\times 4}$ and output matrix $C \\in \\mathbb{R}^{1 \\times 4}$,\n$$\n\\dot{x}(t) = A x(t), \\quad y(t) = C x(t),\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}, \\qquad\nC = \\begin{pmatrix}\n1 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\nStarting only from the fundamental definitions of observability for linear time-invariant systems, determine the unobservable subspace, construct a similarity transformation $T \\in \\mathbb{R}^{4 \\times 4}$ whose columns form a basis that brings the pair $(A,C)$ into a Kalman observable/unobservable decomposition, and compute the transformed realization\n$$\nA' = T^{-1} A T, \\qquad C' = C T,\n$$\nexplicitly identifying the block structure that separates the observable and unobservable dynamics. Use a basis ordering in which the observable coordinates come first and the unobservable coordinates come last, and make explicit which blocks are zero.\n\nFinally, let $n_u$ denote the dimension of the unobservable subspace. Report $n_u$ as your final answer. No rounding is required and no units are associated with $n_u$.", "solution": "We begin from the core definition: for a linear time-invariant (LTI) system $(A,C)$ of dimension $n$, the finite-time observability matrix is\n$$\n\\mathcal{O} = \\begin{pmatrix}\nC \\\\\nC A \\\\\n\\vdots \\\\\nC A^{n-1}\n\\end{pmatrix} \\in \\mathbb{R}^{(n \\cdot p) \\times n},\n$$\nwhere $p$ is the number of outputs. The system is observable if and only if $\\operatorname{rank}(\\mathcal{O}) = n$. The unobservable subspace is the largest $A$-invariant subspace contained in the kernel of the output map induced by repeated applications of $A$, which for a time-invariant system is equivalently the kernel of $\\mathcal{O}$:\n$$\n\\mathcal{N}_u \\coloneqq \\ker(\\mathcal{O}) = \\left\\{ x \\in \\mathbb{R}^{n} \\,:\\, C A^{k} x = 0 \\text{ for } k=0,1,\\dots,n-1 \\right\\}.\n$$\nThe Kalman observable/unobservable decomposition is achieved by choosing a basis of $\\mathbb{R}^{n}$ adapted to the direct sum of an observable complement and the unobservable subspace. With the observable coordinates ordered first and the unobservable ones last, the transformed realization satisfies\n$$\nA' = \\begin{pmatrix}\nA_{o} & 0 \\\\\nA_{21} & A_{u}\n\\end{pmatrix}, \\qquad\nC' = \\begin{pmatrix}\nC_{o} & 0\n\\end{pmatrix},\n$$\nwhere $A_{u}$ acts on the unobservable subspace and the upper-right block is zero because the unobservable subspace is $A$-invariant.\n\nStep 1: Compute the observability matrix and its rank. For $n=4$ and $p=1$, we compute\n$$\nC = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}, \\quad\nC A = \\begin{pmatrix} 0 & 1 & 1 & 0 \\end{pmatrix}.\n$$\nWe next compute $A^{2}$:\n$$\nA^{2} = A A = \n\\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\nThus\n$$\nC A^{2} = \\begin{pmatrix} 0 & 0 & 0 & 1 \\end{pmatrix}.\n$$\nFinally $A^{3} = A^{2} A = 0_{4 \\times 4}$, hence $C A^{3} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\end{pmatrix}$. Therefore\n$$\n\\mathcal{O} = \\begin{pmatrix}\nC \\\\\nC A \\\\\nC A^{2} \\\\\nC A^{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\nThe first three rows are linearly independent, and the last row is zero, so\n$$\n\\operatorname{rank}(\\mathcal{O}) = 3.\n$$\nHence the dimension of the unobservable subspace is\n$$\nn_u = n - \\operatorname{rank}(\\mathcal{O}) = 4 - 3 = 1.\n$$\n\nStep 2: Compute the unobservable subspace $\\mathcal{N}_u = \\ker(\\mathcal{O})$. Solve $\\mathcal{O} x = 0$ with $x = \\begin{pmatrix} x_{1} & x_{2} & x_{3} & x_{4} \\end{pmatrix}^{\\top}$. The equations are\n$$\nx_{1} = 0, \\qquad x_{2} + x_{3} = 0, \\qquad x_{4} = 0.\n$$\nThus\n$$\n\\ker(\\mathcal{O}) = \\operatorname{span}\\left\\{ \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}.\n$$\nDenote $v_{u} \\coloneqq \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$. Verify $A$-invariance on this generator:\n$$\nA v_{u} = \n\\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} -1 + 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\in \\operatorname{span}\\{v_{u}\\}.\n$$\nThus $\\mathcal{N}_u$ is indeed $A$-invariant.\n\nStep 3: Choose a basis adapted to the decomposition. We order the basis so that the observable coordinates come first and the unobservable last. Choose three vectors spanning a complementary subspace to $\\mathcal{N}_u$; for convenience take\n$$\nb_{1} = e_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nb_{2} = e_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nb_{3} = e_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\n$$\nand take the unobservable basis vector\n$$\nb_{4} = v_{u} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nThese four vectors are linearly independent, so the similarity transformation matrix formed by these basis vectors as columns is invertible:\n$$\nT = \\begin{pmatrix}\n| & | & | & | \\\\\nb_{1} & b_{2} & b_{3} & b_{4} \\\\\n| & | & | & |\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}.\n$$\nBy construction, $C T = \\begin{pmatrix} C b_{1} & C b_{2} & C b_{3} & C b_{4} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}$, so\n$$\nC' = C T = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} C_{o} & 0 \\end{pmatrix},\n$$\nwith $C_{o} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$, as required for the Kalman decomposition with observable coordinates first and unobservable last.\n\nStep 4: Compute $A' = T^{-1} A T$ via action on the chosen basis. For each basis vector $b_{j}$, express $A b_{j}$ in the chosen basis $\\{b_{1}, b_{2}, b_{3}, b_{4}\\}$; the $j$-th column of $A'$ is that coordinate vector.\n\nCompute:\n- $A b_{1} = A e_{1} = 0$.\n- $A b_{2} = A e_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = b_{1}$.\n- $A b_{3} = A e_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = e_{3}$. Expressing $e_3$ in the new basis yields $e_3 = b_2 + b_4$.\n- $A b_{4} = A v_{u} = 0$.\n\nHence the columns of $A'$, which are the coordinate vectors of the images above, are:\n$$\nA' = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}.\n$$\nPartitioning with the first three coordinates observable and the last one unobservable, we identify the blocks\n$$\nA' = \\begin{pmatrix}\nA_{o} & 0 \\\\\nA_{21} & A_{u}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\begin{matrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 0 \\\\ 0\n\\end{matrix} \\\\\n\\begin{matrix}\n0 & 0 & 1\n\\end{matrix}\n&\n\\begin{matrix}\n0\n\\end{matrix}\n\\end{pmatrix},\n\\qquad\nC' = \\begin{pmatrix} C_{o} & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}.\n$$\nAs expected for the Kalman observable/unobservable decomposition with observable coordinates first and unobservable last, the upper-right block is zero due to $A$-invariance of the unobservable subspace, and the output depends only on the observable coordinates.\n\nFinally, from Step $1$,\n$$\nn_u = \\dim(\\mathcal{N}_u) = 1.\n$$\nThis is the requested integer to report.", "answer": "$$\\boxed{1}$$", "id": "2694863"}, {"introduction": "The concept of observability finds its ultimate purpose in the design of state estimators, which are essential for modern control and monitoring. This exercise bridges theory and practice by tasking you with designing a Luenberger observer for an observable system [@problem_id:2694880]. You will calculate the observer gain vector $L$ to place the eigenvalues of the estimation error dynamics at specific desired locations, a technique known as pole placement, thereby controlling how quickly your state estimate converges to the true state.", "problem": "Consider the linear time-invariant (LTI) state-space model given by\n$$\n\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t),\n$$\nwith\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-6 & -11 & -6\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nYou are told that the pair $\\left(A,C\\right)$ is observable. A full-order Luenberger observer uses an injection gain vector $L \\in \\mathbb{R}^{3}$ so that the observer is\n$$\n\\dot{\\hat{x}}(t) = A \\hat{x}(t) + B u(t) + L\\big(y(t) - C \\hat{x}(t)\\big).\n$$\nStarting from the definition of observability, the structure of the estimation error dynamics, and basic properties of characteristic polynomials, determine a gain vector $L$ such that the estimation error dynamics matrix $A - L C$ has eigenvalues at the prescribed stable locations $s = -2$, $s = -3$, and $s = -4$. Express the final gain vector explicitly as a column vector. No rounding is required.", "solution": "The problem requires the determination of a Luenberger observer gain vector $L$ for a given linear time-invariant (LTI) system. The design criterion is to place the eigenvalues of the observer error dynamics matrix at the specified locations $s = -2$, $s = -3$, and $s = -4$.\n\nThe LTI system is described by the state-space equations:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t)\n$$\nwith matrices\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-6 & -11 & -6\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nThe problem statement asserts that the pair $(A, C)$ is observable. As a matter of scientific rigor, we must first verify this claim. The observability of a system is a fundamental prerequisite for the design of an observer with arbitrary pole placement. An LTI system is observable if and only if the observability matrix $\\mathcal{O}$ has full rank, which for this $3$-dimensional system is rank $3$.\n\nThe observability matrix is defined as\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\end{pmatrix}.\n$$\nWe compute the necessary components:\n$C = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$.\n$$\nCA = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}.\n$$\n$$\nCA^2 = (CA)A = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}.\n$$\nConstructing the observability matrix:\n$$\n\\mathcal{O} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I_3.\n$$\nThe determinant of $\\mathcal{O}$ is $\\det(\\mathcal{O}) = 1 \\neq 0$, which confirms that its rank is $3$. The system is indeed observable. This guarantees that a unique gain vector $L$ exists that can place the observer poles at any desired locations.\n\nNext, we analyze the dynamics of the estimation error. The Luenberger observer for the state estimate $\\hat{x}(t)$ is given by\n$$\n\\dot{\\hat{x}}(t) = A \\hat{x}(t) + B u(t) + L\\big(y(t) - C \\hat{x}(t)\\big).\n$$\nThe estimation error is defined as $e(t) = x(t) - \\hat{x}(t)$. Its derivative is $\\dot{e}(t) = \\dot{x}(t) - \\dot{\\hat{x}}(t)$. Substituting the state and observer equations:\n$$\n\\dot{e}(t) = \\big(A x(t) + B u(t)\\big) - \\Big(A \\hat{x}(t) + B u(t) + L\\big(y(t) - C \\hat{x}(t)\\big)\\Big).\n$$\nSubstituting $y(t) = C x(t)$ and simplifying:\n$$\n\\dot{e}(t) = A x(t) - A \\hat{x}(t) - L\\big(C x(t) - C \\hat{x}(t)\\big) = A\\big(x(t) - \\hat{x}(t)\\big) - LC\\big(x(t) - \\hat{x}(t)\\big).\n$$\nThis leads to the error dynamics equation:\n$$\n\\dot{e}(t) = (A - LC)e(t).\n$$\nThe stability and convergence rate of the estimation error are determined by the eigenvalues of the matrix $A_{obs} = A - LC$. Our goal is to choose $L$ such that these eigenvalues are at $s = -2$, $s = -3$, and $s = -4$.\n\nThe desired characteristic polynomial, $p_{des}(s)$, for these eigenvalues is:\n$$\np_{des}(s) = (s - (-2))(s - (-3))(s - (-4)) = (s+2)(s+3)(s+4).\n$$\nExpanding this polynomial gives:\n$$\np_{des}(s) = (s^2 + 5s + 6)(s+4) = s^3 + 4s^2 + 5s^2 + 20s + 6s + 24 = s^3 + 9s^2 + 26s + 24.\n$$\nNow, we must find the actual characteristic polynomial, $p_{act}(s)$, of the matrix $A - LC$. Let the gain vector be $L = \\begin{pmatrix} l_1 \\\\ l_2 \\\\ l_3 \\end{pmatrix}$.\nThe product $LC$ is:\n$$\nLC = \\begin{pmatrix} l_1 \\\\ l_2 \\\\ l_3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} l_1 & 0 & 0 \\\\ l_2 & 0 & 0 \\\\ l_3 & 0 & 0 \\end{pmatrix}.\n$$\nThe observer error dynamics matrix is:\n$$\nA - LC = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{pmatrix} - \\begin{pmatrix} l_1 & 0 & 0 \\\\ l_2 & 0 & 0 \\\\ l_3 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -l_1 & 1 & 0 \\\\ -l_2 & 0 & 1 \\\\ -6-l_3 & -11 & -6 \\end{pmatrix}.\n$$\nThe characteristic polynomial $p_{act}(s)$ is $\\det(sI - (A - LC))$:\n$$\nsI - (A - LC) = \\begin{pmatrix} s+l_1 & -1 & 0 \\\\ l_2 & s & -1 \\\\ 6+l_3 & 11 & s+6 \\end{pmatrix}.\n$$\nThe determinant is calculated as:\n$$\np_{act}(s) = (s+l_1) \\det\\begin{pmatrix} s & -1 \\\\ 11 & s+6 \\end{pmatrix} - (-1) \\det\\begin{pmatrix} l_2 & -1 \\\\ 6+l_3 & s+6 \\end{pmatrix}\n$$\n$$\np_{act}(s) = (s+l_1)\\big(s(s+6) - (-1)(11)\\big) + \\big(l_2(s+6) - (-1)(6+l_3)\\big)\n$$\n$$\np_{act}(s) = (s+l_1)(s^2 + 6s + 11) + (l_2s + 6l_2 + 6 + l_3)\n$$\n$$\np_{act}(s) = (s^3 + 6s^2 + 11s) + (l_1s^2 + 6l_1s + 11l_1) + l_2s + 6l_2 + 6 + l_3\n$$\nGrouping terms by powers of $s$:\n$$\np_{act}(s) = s^3 + (6+l_1)s^2 + (11+6l_1+l_2)s + (6+11l_1+6l_2+l_3).\n$$\nTo achieve the desired pole placement, we equate the coefficients of $p_{act}(s)$ with those of $p_{des}(s)$:\n$$\ns^3 + (6+l_1)s^2 + (11+6l_1+l_2)s + (6+11l_1+6l_2+l_3) = s^3 + 9s^2 + 26s + 24.\n$$\nThis yields a system of three linear equations for the three unknown gains $l_1, l_2, l_3$:\n1.  Coefficient of $s^2$: $6 + l_1 = 9 \\implies l_1 = 3$.\n2.  Coefficient of $s^1$: $11 + 6l_1 + l_2 = 26$. Substituting $l_1 = 3$:\n    $11 + 6(3) + l_2 = 26 \\implies 11 + 18 + l_2 = 26 \\implies 29 + l_2 = 26 \\implies l_2 = -3$.\n3.  Coefficient of $s^0$: $6 + 11l_1 + 6l_2 + l_3 = 24$. Substituting $l_1 = 3$ and $l_2 = -3$:\n    $6 + 11(3) + 6(-3) + l_3 = 24 \\implies 6 + 33 - 18 + l_3 = 24 \\implies 21 + l_3 = 24 \\implies l_3 = 3$.\n\nThus, the components of the gain vector are $l_1 = 3$, $l_2 = -3$, and $l_3 = 3$. The observer gain vector $L$ is:\n$$\nL = \\begin{pmatrix} 3 \\\\ -3 \\\\ 3 \\end{pmatrix}.\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 3 \\\\ -3 \\\\ 3 \\end{pmatrix}}\n$$", "id": "2694880"}]}