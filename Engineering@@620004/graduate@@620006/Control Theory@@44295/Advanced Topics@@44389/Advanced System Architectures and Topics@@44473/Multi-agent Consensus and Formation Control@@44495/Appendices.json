{"hands_on_practices": [{"introduction": "This first exercise goes to the heart of multi-agent consensus by focusing on the graph Laplacian, a matrix that mathematically represents the network's communication topology. By computing the Laplacian and its eigenvalues for a small system, you will directly engage with the concept of algebraic connectivity. This hands-on calculation is fundamental, as it provides a definitive test for whether a network is connected—a necessary condition for agents to reach an agreement [@problem_id:2726172].", "problem": "Consider a network of $4$ agents with an undirected weighted communication graph. The adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{4 \\times 4}$ is given by\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0 & 1 & \\alpha & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n\\alpha & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{pmatrix},\n$$\nwhere $\\alpha \\ge 0$ is a real scalar weight. Let $\\mathbf{D}$ denote the degree matrix and $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$ denote the (combinatorial) graph Laplacian. Work from first principles, using only the definitions of degree, adjacency, and Laplacian, and standard spectral properties of real symmetric matrices. \n\nTasks:\n- Compute $\\mathbf{D}$ and $\\mathbf{L}$ explicitly.\n- Derive all eigenvalues of $\\mathbf{L}$ analytically in closed form as functions of $\\alpha$.\n- Using the spectral connectivity criterion for undirected graphs, determine whether the graph is connected for $\\alpha \\ge 0$.\n\nAs your final answer, report only the algebraic connectivity, defined as the second-smallest eigenvalue $\\lambda_2(\\mathbf{L})$, as a closed-form expression in $\\alpha$. No units are required, and no rounding is necessary.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard exercise in algebraic graph theory, central to the study of multi-agent consensus. All necessary information is provided, and the definitions are standard. The problem is valid. We will proceed with the solution.\n\nThe problem requires the analysis of a $4$-agent network with a specified weighted, undirected communication graph.\n\nFirst, we are given the adjacency matrix $\\mathbf{A}$:\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0 & 1 & \\alpha & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n\\alpha & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{pmatrix}\n$$\nwhere $\\alpha \\ge 0$ is a real scalar. The graph is undirected, which is consistent with the fact that $\\mathbf{A}$ is a symmetric matrix.\n\nThe first task is to compute the degree matrix $\\mathbf{D}$. The degree $d_i$ of a vertex $i$ is the sum of the weights of all edges connected to it, which corresponds to the sum of the elements in the $i$-th row (or column) of the adjacency matrix $\\mathbf{A}$.\nThe degrees of the four agents (vertices) are:\n$$ d_1 = 0 + 1 + \\alpha + 1 = 2+\\alpha $$\n$$ d_2 = 1 + 0 + 1 + 0 = 2 $$\n$$ d_3 = \\alpha + 1 + 0 + 1 = 2+\\alpha $$\n$$ d_4 = 1 + 0 + 1 + 0 = 2 $$\nThe degree matrix $\\mathbf{D}$ is the diagonal matrix formed by these degrees:\n$$\n\\mathbf{D} = \\text{diag}(d_1, d_2, d_3, d_4) = \n\\begin{pmatrix}\n2+\\alpha & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 2+\\alpha & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\nNext, we compute the graph Laplacian $\\mathbf{L}$ using its definition, $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$.\n$$\n\\mathbf{L} = \n\\begin{pmatrix}\n2+\\alpha & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 2+\\alpha & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n0 & 1 & \\alpha & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n\\alpha & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2+\\alpha & -1 & -\\alpha & -1 \\\\\n-1 & 2 & -1 & 0 \\\\\n-\\alpha & -1 & 2+\\alpha & -1 \\\\\n-1 & 0 & -1 & 2\n\\end{pmatrix}\n$$\n\nThe second task is to derive all eigenvalues of $\\mathbf{L}$. We must solve the characteristic equation $\\det(\\mathbf{L} - \\lambda \\mathbf{I}) = 0$. However, it is more efficient to find the eigenvalues by exploiting the structure of the matrix $\\mathbf{L}$ to find its eigenvectors.\n\nA fundamental property of the Laplacian matrix for any undirected graph is that the sum of each row is zero. This implies that the vector $\\mathbf{1} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^T$ is an eigenvector with a corresponding eigenvalue of $0$.\n$$\n\\mathbf{L} \\mathbf{1} = \n\\begin{pmatrix}\n2+\\alpha - 1 - \\alpha - 1 \\\\\n-1 + 2 - 1 + 0 \\\\\n-\\alpha - 1 + 2+\\alpha - 1 \\\\\n-1 + 0 - 1 + 2\n\\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n= 0 \\cdot \\mathbf{1}\n$$\nThus, one eigenvalue is $\\lambda_1 = 0$.\n\nNow, we observe the symmetries in the graph structure. Vertices $1$ and $3$ are structurally symmetric, as are vertices $2$ and $4$. This suggests eigenvectors that capture these symmetries.\n\nConsider the vector $\\mathbf{v}_2 = \\begin{pmatrix} 1 & 0 & -1 & 0 \\end{pmatrix}^T$. We compute $\\mathbf{L}\\mathbf{v}_2$:\n$$\n\\mathbf{L} \\mathbf{v}_2 = \n\\begin{pmatrix}\n2+\\alpha & -1 & -\\alpha & -1 \\\\\n-1 & 2 & -1 & 0 \\\\\n-\\alpha & -1 & 2+\\alpha & -1 \\\\\n-1 & 0 & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} (2+\\alpha) - (-\\alpha) \\\\ -1 - (-1) \\\\ -\\alpha - (2+\\alpha) \\\\ -1 - (-1) \\end{pmatrix}\n=\n\\begin{pmatrix} 2+2\\alpha \\\\ 0 \\\\ -2-2\\alpha \\\\ 0 \\end{pmatrix}\n= (2+2\\alpha) \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThus, another eigenvalue is $\\lambda_2 = 2+2\\alpha$.\n\nConsider the vector $\\mathbf{v}_3 = \\begin{pmatrix} 0 & 1 & 0 & -1 \\end{pmatrix}^T$. We compute $\\mathbf{L}\\mathbf{v}_3$:\n$$\n\\mathbf{L} \\mathbf{v}_3 = \n\\begin{pmatrix}\n2+\\alpha & -1 & -\\alpha & -1 \\\\\n-1 & 2 & -1 & 0 \\\\\n-\\alpha & -1 & 2+\\alpha & -1 \\\\\n-1 & 0 & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n=\n\\begin{pmatrix} -1 - (-1) \\\\ 2 \\\\ -1 - (-1) \\\\ 2 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ -2 \\end{pmatrix}\n= 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThus, a third eigenvalue is $\\lambda_3 = 2$.\n\nTo find the fourth eigenvalue, we use the property that the sum of the eigenvalues of a matrix is equal to its trace.\n$$ \\text{tr}(\\mathbf{L}) = (2+\\alpha) + 2 + (2+\\alpha) + 2 = 8+2\\alpha $$\nLet the fourth eigenvalue be $\\lambda_4$. Then,\n$$ \\sum_{i=1}^{4} \\lambda_i = \\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 = \\text{tr}(\\mathbf{L}) $$\n$$ 0 + (2+2\\alpha) + 2 + \\lambda_4 = 8+2\\alpha $$\n$$ 4+2\\alpha + \\lambda_4 = 8+2\\alpha $$\n$$ \\lambda_4 = 4 $$\nThe four eigenvalues of $\\mathbf{L}$ as functions of $\\alpha$ are $\\{0, 2, 4, 2+2\\alpha\\}$.\n\nThe third task is to determine if the graph is connected. For an undirected graph, the spectral connectivity criterion states that the graph is connected if and only if the second-smallest eigenvalue of its Laplacian, $\\lambda_2(\\mathbf{L})$, known as the algebraic connectivity, is strictly positive.\n\nWe must order the eigenvalues for the given condition $\\alpha \\ge 0$. The eigenvalues are $\\{0, 2, 4, 2+2\\alpha\\}$.\nThe smallest eigenvalue is always $\\lambda_1(\\mathbf{L}) = 0$.\nThe non-zero eigenvalues are $\\{2, 4, 2+2\\alpha\\}$.\nWe are given $\\alpha \\ge 0$. This implies $2\\alpha \\ge 0$, and therefore $2+2\\alpha \\ge 2$.\nThe second-smallest eigenvalue, $\\lambda_2(\\mathbf{L})$, is the minimum of the set of positive eigenvalues:\n$$ \\lambda_2(\\mathbf{L}) = \\min\\{2, 4, 2+2\\alpha\\} $$\nSince $2+2\\alpha \\ge 2$, the minimum value in this set is $2$.\nTherefore, the algebraic connectivity is $\\lambda_2(\\mathbf{L}) = 2$.\n\nSince $\\lambda_2(\\mathbf{L}) = 2 > 0$ for all $\\alpha \\ge 0$, the graph is connected for all non-negative values of $\\alpha$.\n\nThe problem asks for the algebraic connectivity, $\\lambda_2(\\mathbf{L})$, as the final answer.\nBased on our derivation, this value is a constant.\n$$ \\lambda_2(\\mathbf{L}) = 2 $$", "answer": "$$\n\\boxed{2}\n$$", "id": "2726172"}, {"introduction": "Having established how to verify connectivity, we now explore a critical implication: network robustness. This practice presents a scenario where consensus fails not due to a lack of simple connectivity, but because the topology is not resilient to adversarial influence. By explicitly calculating the agent trajectories, you will demonstrate how a single, non-cooperative agent can prevent the system from converging, highlighting the crucial link between graph structure and the security of multi-agent systems [@problem_id:2726171].", "problem": "Consider a discrete-time multi-agent system on a directed graph with three agents labeled $1$, $2$, and $3$. Agents $1$ and $2$ are normal, and agent $3$ is adversarial. The graph has directed edges $3 \\to 1$ and $1 \\to 2$ and no other edges. Each normal agent applies a row-stochastic linear averaging update over its in-neighbors including itself at each time $k \\in \\mathbb{N}$. Specifically, the normal agents evolve according to\n$$\nx_{1}(k+1) \\;=\\; \\tfrac{1}{2}\\,x_{1}(k) \\;+\\; \\tfrac{1}{2}\\,x_{3}(k),\\qquad\nx_{2}(k+1) \\;=\\; \\tfrac{1}{3}\\,x_{2}(k) \\;+\\; \\tfrac{2}{3}\\,x_{1}(k),\n$$\nwith initial conditions $x_{1}(0)=0$ and $x_{2}(0)=0$. The adversary $3$ is free to choose its trajectory, and here it applies the explicit open-loop choice\n$$\nx_{3}(k) \\;=\\; (-1)^{k},\\quad k=0,1,2,\\dots\n$$\nwhich is a bounded, alternating signal.\n\nUse only fundamental definitions of consensus (all normal agents’ states converge to a common limit), linear time-invariant (LTI) difference equations, and explicit trajectory computation to demonstrate how this single adversary prevents consensus on this graph. In particular, compute the exact value of\n$$\n\\limsup_{k \\to \\infty}\\,\\big|\\,x_{2}(k) - x_{1}(k)\\,\\big|.\n$$\nProvide your final answer as a single exact number in simplest fractional form. No rounding is required and no units are involved.", "solution": "We start from the definitions of the dynamics and the notion of consensus. Consensus for the normal agents means that $|x_{2}(k)-x_{1}(k)| \\to 0$ as $k \\to \\infty$. To demonstrate that the adversary prevents consensus, it suffices to construct an explicit valid adversarial trajectory that yields a persistent nonzero disagreement in the limit, i.e., a positive $\\limsup_{k \\to \\infty} |x_{2}(k)-x_{1}(k)|$.\n\nThe normal-agent dynamics are linear time-invariant (LTI) difference equations driven by the adversarial input $x_{3}(k)$. Define $u(k) \\equiv x_{3}(k) = (-1)^{k}$ for all $k$. The update equations are\n$$\nx_{1}(k+1) \\;=\\; \\tfrac{1}{2}\\,x_{1}(k) \\;+\\; \\tfrac{1}{2}\\,u(k),\n\\qquad\nx_{2}(k+1) \\;=\\; \\tfrac{1}{3}\\,x_{2}(k) \\;+\\; \\tfrac{2}{3}\\,x_{1}(k),\n$$\nwith $x_{1}(0)=0$ and $x_{2}(0)=0$.\n\nWe analyze the disagreement $d(k) \\equiv x_{2}(k) - x_{1}(k)$. First, characterize $x_{1}(k)$ in response to the alternating input. Introduce the tracking error\n$$\ne(k) \\;\\equiv\\; x_{1}(k) - u(k).\n$$\nThen, using the $x_{1}$-dynamics,\n\n$$\n\\begin{aligned}\ne(k+1) \\;&=\\; x_{1}(k+1) - u(k+1) \\\\\n&=\\; \\tfrac{1}{2}\\,x_{1}(k) + \\tfrac{1}{2}\\,u(k) - u(k+1) \\\\\n&=\\; \\tfrac{1}{2}\\,\\big(e(k)+u(k)\\big) + \\tfrac{1}{2}\\,u(k) - u(k+1) \\\\\n&=\\; \\tfrac{1}{2}\\,e(k) + u(k) - u(k+1).\n\\end{aligned}\n$$\n\nSince $u(k)=(-1)^{k}$, we have $u(k+1)-u(k) = (-1)^{k+1} - (-1)^{k} = -2(-1)^{k}$, so\n$$\ne(k+1) \\;=\\; \\tfrac{1}{2}\\,e(k) + 2\\,(-1)^{k}.\n$$\nThis is an LTI difference equation with a periodic input. A particular solution of the form $e_{p}(k) = C\\,(-1)^{k}$ satisfies\n\n$$\n-C \\;=\\; \\tfrac{1}{2}\\,C + 2 \\quad\\Longrightarrow\\quad -\\tfrac{3}{2}\\,C = 2 \\quad\\Longrightarrow\\quad C = -\\tfrac{4}{3}.\n$$\n\nThus $e_{p}(k) = -\\tfrac{4}{3}\\,(-1)^{k}$. The homogeneous solution is $e_{h}(k) = \\left(\\tfrac{1}{2}\\right)^{k} e(0)$. From $x_{1}(0)=0$ and $u(0)=1$, we have $e(0)=-1$. Therefore the complete solution is\n$$\ne(k) \\;=\\; -\\tfrac{4}{3}\\,(-1)^{k} \\;+\\; \\left(\\tfrac{1}{2}\\right)^{k}\\Big(e(0) - e_{p}(0)\\Big)\n\\;=\\; -\\tfrac{4}{3}\\,(-1)^{k} \\;+\\; \\left(\\tfrac{1}{2}\\right)^{k}\\Big(-1 - \\big(-\\tfrac{4}{3}\\big)\\Big)\n\\;=\\; -\\tfrac{4}{3}\\,(-1)^{k} \\;+\\; \\left(\\tfrac{1}{2}\\right)^{k}\\,\\tfrac{1}{3}.\n$$\nHence\n$$\nx_{1}(k) \\;=\\; u(k) + e(k) \\;=\\; (-1)^{k} \\;-\\; \\tfrac{4}{3}\\,(-1)^{k} \\;+\\; \\left(\\tfrac{1}{2}\\right)^{k}\\,\\tfrac{1}{3}\n\\;=\\; -\\tfrac{1}{3}\\,(-1)^{k} \\;+\\; \\left(\\tfrac{1}{2}\\right)^{k}\\,\\tfrac{1}{3}.\n$$\nTherefore, as $k \\to \\infty$, the transient term $\\left(\\tfrac{1}{2}\\right)^{k}\\tfrac{1}{3}\\to 0$ and $x_{1}(k)$ approaches the bounded alternating sequence $-\\tfrac{1}{3}(-1)^{k}$.\n\nNext, derive the disagreement dynamics. Using the updates,\n\n$$\n\\begin{aligned}\nd(k+1) \\;&=\\; x_{2}(k+1) - x_{1}(k+1) \\\\\n&=\\; \\tfrac{1}{3}\\,x_{2}(k) + \\tfrac{2}{3}\\,x_{1}(k) \\;-\\; \\Big(\\tfrac{1}{2}\\,x_{1}(k) + \\tfrac{1}{2}\\,u(k)\\Big) \\\\\n&=\\; \\tfrac{1}{3}\\,x_{2}(k) + \\Big(\\tfrac{2}{3} - \\tfrac{1}{2}\\Big) x_{1}(k) \\;-\\; \\tfrac{1}{2}\\,u(k) \\\\\n&=\\; \\tfrac{1}{3}\\,\\big(x_{1}(k)+d(k)\\big) + \\tfrac{1}{6}\\,x_{1}(k) - \\tfrac{1}{2}\\,u(k) \\\\\n&=\\; \\tfrac{1}{3}\\,d(k) + \\Big(\\tfrac{1}{3}+\\tfrac{1}{6}\\Big) x_{1}(k) - \\tfrac{1}{2}\\,u(k) \\\\\n&=\\; \\tfrac{1}{3}\\,d(k) + \\tfrac{1}{2}\\,x_{1}(k) - \\tfrac{1}{2}\\,u(k) \\\\\n&=\\; \\tfrac{1}{3}\\,d(k) + \\tfrac{1}{2}\\,\\big(x_{1}(k) - u(k)\\big).\n\\end{aligned}\n$$\n\nThus\n$$\nd(k+1) \\;=\\; \\tfrac{1}{3}\\,d(k) \\;+\\; \\tfrac{1}{2}\\,e(k).\n$$\nSubstitute the particular component $e_{p}(k) = -\\tfrac{4}{3}(-1)^{k}$ to find a steady-state particular solution for $d(k)$ of the form $d_{p}(k) = D\\,(-1)^{k}$. Plugging into the difference equation gives\n\n$$\n- D \\;=\\; \\tfrac{1}{3}\\,D \\;+\\; \\tfrac{1}{2}\\,\\Big(-\\tfrac{4}{3}\\Big)\n\\quad\\Longrightarrow\\quad\n-\\tfrac{4}{3}\\,D \\;=\\; -\\tfrac{2}{3}\n\\quad\\Longrightarrow\\quad\nD \\;=\\; \\tfrac{1}{2}.\n$$\n\nTherefore the steady-state disagreement oscillates as $d_{p}(k) = \\tfrac{1}{2}\\,(-1)^{k}$. The full solution includes transients that decay geometrically (with factors $\\left(\\tfrac{1}{2}\\right)^{k}$ and $\\left(\\tfrac{1}{3}\\right)^{k}$) due to the stability of the LTI homogeneous dynamics, so the asymptotic behavior is governed by $d_{p}(k)$.\n\nConsequently,\n$$\n\\limsup_{k \\to \\infty}\\,|x_{2}(k) - x_{1}(k)| \\;=\\; \\limsup_{k \\to \\infty}\\,|d(k)| \\;=\\; \\big|\\tfrac{1}{2}\\big| \\;=\\; \\tfrac{1}{2}.\n$$\nSince this limit superior is strictly positive, the disagreement does not vanish and the normal agents do not achieve consensus. This explicit trajectory construction shows that a single adversarial agent on this not-robust directed graph prevents consensus, and the exact asymptotic disagreement amplitude is $\\tfrac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2726171"}, {"introduction": "Our final practice transitions from theoretical analysis to a practical, computational application that synthesizes consensus with estimation theory. You are tasked with implementing a distributed Kalman filter, where agents must first agree on their collective sensor information before they can estimate the state of a dynamic process. This coding exercise demonstrates how consensus algorithms function as essential building blocks in sophisticated, real-world tasks like cooperative sensing and tracking, providing a powerful example of distributed intelligence in action [@problem_id:2726151].", "problem": "Design and implement a program that computes the steady-state error covariance for a distributed estimator over a simple network using consensus on information contributions. The agents observe a common discrete-time Linear Time-Invariant (LTI) system. The derivation must start from first principles: Bayesian estimation for linear Gaussian systems and average consensus over connected undirected graphs.\n\nFundamental base and setting:\n- The system state evolves according to the linear Gaussian model $x_{k+1} = A x_k + w_k$, where $x_k \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{n \\times n}$, and the process noise $w_k$ is zero-mean Gaussian with covariance $Q \\in \\mathbb{R}^{n \\times n}$, with $Q \\succ 0$.\n- Agent $i$ measures $y_{i,k} = H_i x_k + v_{i,k}$, where $H_i \\in \\mathbb{R}^{m_i \\times n}$, and $v_{i,k}$ is zero-mean Gaussian with covariance $R_i \\in \\mathbb{R}^{m_i \\times m_i}$, with $R_i \\succ 0$.\n- The network is modeled as a connected undirected graph with adjacency matrix $A_{\\text{graph}} \\in \\{0,1\\}^{N \\times N}$, where $A_{\\text{graph}}[i,j] = 1$ if there is an undirected edge between agents $i$ and $j$ and $0$ otherwise, and $A_{\\text{graph}}[i,i] = 0$. Let $N \\in \\mathbb{N}$ denote the number of agents.\n\nPrinciples to use:\n- For a linear Gaussian prior and measurement, the posterior covariance after a measurement update depends only on the prior covariance and the information contribution. Specifically, if the prior covariance is $P^- \\in \\mathbb{R}^{n \\times n}$ and the stacked measurement model is $C \\in \\mathbb{R}^{m \\times n}$ with measurement covariance $R \\in \\mathbb{R}^{m \\times m}$, then the posterior covariance is $P^+ = \\left((P^-)^{-1} + C^\\top R^{-1} C\\right)^{-1}$.\n- For distributed sensing, each agent $i$ contributes a local information matrix $M_i = H_i^\\top R_i^{-1} H_i \\in \\mathbb{R}^{n \\times n}$. Over a connected undirected graph, average consensus with a doubly stochastic weight matrix $W \\in \\mathbb{R}^{N \\times N}$ asymptotically yields agreement on the average $\\bar{M} = \\frac{1}{N} \\sum_{i=1}^N M_i$. The Metropolis weighting rule is one standard choice that yields a symmetric doubly stochastic $W$ for undirected graphs: for $i \\neq j$, if $A_{\\text{graph}}[i,j] = 1$ then $W[i,j] = \\frac{1}{1+\\max\\{d_i,d_j\\}}$, where $d_i$ is the degree of node $i$, and $W[i,i] = 1 - \\sum_{j \\neq i} W[i,j]$.\n\nDistributed estimator recursion to implement:\n- Let $P_k \\in \\mathbb{R}^{n \\times n}$ be the estimation error covariance after the measurement update at time $k$ (assume identical across agents due to exact agreement on information via consensus).\n- The prediction step is $P_k^- = A P_k A^\\top + Q$.\n- The distributed measurement fusion proceeds by consensus on $M_i = H_i^\\top R_i^{-1} H_i$ to compute the average $\\bar{M} = \\lim_{\\ell \\to \\infty} W^\\ell M$, where $M \\in \\mathbb{R}^{N \\times (n n)}$ stacks the vectorized $M_i$ row-wise. Recover the sum $\\sum_{i=1}^N M_i = N \\bar{M}$ by rescaling. The measurement update in information form is then $P_{k+1} = \\left( (P_k^-)^{-1} + \\sum_{i=1}^N M_i \\right)^{-1}$.\n- At steady state, the covariance $P_\\star$ is the fixed point of the time-invariant recursion $P \\mapsto \\left( \\left( A P A^\\top + Q \\right)^{-1} + \\sum_{i=1}^N M_i \\right)^{-1}$. Assume detectability and stabilizability conditions are satisfied so that a unique positive definite fixed point exists.\n\nTask:\n- Implement the above distributed estimator equations with explicit consensus on the local information matrices using the Metropolis weights. Use repeated multiplication by the weight matrix to numerically reach agreement to a tight tolerance on the average of the vectorized $M_i$, then rescale to obtain $\\sum_{i=1}^N M_i$.\n- Iterate the covariance recursion until convergence in Frobenius norm with a specified tolerance, and then report the trace of the steady-state covariance $P_\\star$.\n\nTest suite:\nImplement your program to solve the following four test cases. Each test case specifies $(N, n, A, Q, \\{H_i, R_i\\}_{i=1}^N, A_{\\text{graph}})$.\n\n- Test case $1$ (scalar random walk, two-agent line, one informative agent):\n  - $N = 2$, $n = 1$.\n  - $A = [1.0]$, $Q = [0.5]$.\n  - $H_1 = [1.0]$, $R_1 = [2.0]$.\n  - $H_2 = [0.0]$, $R_2 = [1.0]$.\n  - $A_{\\text{graph}} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\n\n- Test case $2$ (complementary sensing of two-dimensional stable system, two-agent line):\n  - $N = 2$, $n = 2$.\n  - $A = \\operatorname{diag}(0.9, 0.8)$, $Q = \\operatorname{diag}(0.1, 0.1)$.\n  - $H_1 = [1.0, 0.0]$, $R_1 = [0.5]$.\n  - $H_2 = [0.0, 1.0]$, $R_2 = [0.5]$.\n  - $A_{\\text{graph}} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\n\n- Test case $3$ (nearly marginal dynamics, three-agent chain with redundant sensing):\n  - $N = 3$, $n = 2$.\n  - $A = \\begin{bmatrix} 1.0 & 0.1 \\\\ 0.0 & 1.0 \\end{bmatrix}$, $Q = \\operatorname{diag}(0.01, 0.01)$.\n  - $H_1 = [1.0, 0.0]$, $R_1 = [0.2]$.\n  - $H_2 = [0.0, 1.0]$, $R_2 = [0.2]$.\n  - $H_3 = [1.0, 1.0]$, $R_3 = [0.3]$.\n  - $A_{\\text{graph}} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}$.\n\n- Test case $4$ (one unstable mode with weak measurements, three-agent complete graph):\n  - $N = 3$, $n = 2$.\n  - $A = \\operatorname{diag}(0.95, 1.02)$, $Q = \\operatorname{diag}(0.05, 0.05)$.\n  - $H_1 = [1.0, 0.0]$, $R_1 = [10.0]$.\n  - $H_2 = [0.0, 1.0]$, $R_2 = [10.0]$.\n  - $H_3 = [1.0, 1.0]$, $R_3 = [20.0]$.\n  - $A_{\\text{graph}} = \\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$.\n\nNumerical specifications:\n- Use a consensus tolerance of $\\varepsilon_{\\text{cons}} = 10^{-10}$ and a maximum of $L_{\\max} = 2000$ consensus iterations. If not converged by $L_{\\max}$, use the direct sum $\\sum_{i=1}^N M_i$ as a fallback to guarantee correctness.\n- Use a covariance fixed-point tolerance of $\\varepsilon_{\\text{cov}} = 10^{-12}$ and a maximum of $K_{\\max} = 10000$ iterations.\n- Initialize $P_0 = I_n$.\n\nWhat to output:\n- For each test case, compute the steady-state covariance $P_\\star$ and return $\\operatorname{trace}(P_\\star)$ rounded to exactly $6$ digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_j$ is a decimal string with exactly $6$ digits after the decimal point.", "solution": "The problem presented is a standard, well-posed exercise in the field of distributed state estimation for linear time-invariant systems. It requires the synthesis of concepts from Bayesian estimation, specifically the information filter formulation of the Kalman filter, and multi-agent consensus theory. The validation confirms that the problem statement is scientifically grounded, internally consistent, and contains all necessary information for a unique solution. We shall therefore proceed with the derivation and algorithmic design.\n\nThe objective is to compute the steady-state error covariance, denoted $P_\\star$, for a network of $N$ agents observing a common process. The derivation must originate from first principles.\n\nLet the state of the system at discrete time step $k$ be the random vector $x_k \\in \\mathbb{R}^n$. Its evolution is described by the linear stochastic difference equation:\n$$\nx_{k+1} = A x_k + w_k\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix and $w_k$ is a zero-mean Gaussian process noise with covariance $Q = \\mathbb{E}[w_k w_k^\\top] \\succ 0$.\n\nEach agent $i \\in \\{1, \\dots, N\\}$ acquires a local measurement $y_{i,k} \\in \\mathbb{R}^{m_i}$ according to the model:\n$$\ny_{i,k} = H_i x_k + v_{i,k}\n$$\nwhere $H_i \\in \\mathbb{R}^{m_i \\times n}$ is the measurement matrix for agent $i$ and $v_{i,k}$ is a zero-mean Gaussian measurement noise with covariance $R_i = \\mathbb{E}[v_{i,k} v_{i,k}^\\top] \\succ 0$. All noise processes are assumed to be mutually uncorrelated.\n\nThe core of the estimation problem is to produce an estimate $\\hat{x}_k$ of $x_k$ that is optimal in the minimum mean squared error sense. For linear Gaussian systems, the Kalman filter provides the recursive solution for the conditional mean and covariance of the state.\n\nLet $P_k$ be the error covariance after the measurement update at time $k$. The filter proceeds in two steps: prediction and update.\n\n1.  **Prediction**: The state and covariance are propagated forward in time using the system model.\n    $$\n    P_k^- = A P_{k-1} A^\\top + Q\n    $$\n    Here, $P_k^-$ denotes the *a priori* error covariance at time $k$ before the measurement $y_k$ is processed.\n\n2.  **Update**: The measurement is used to correct the predicted state. In the information filter formulation, this is expressed as an addition of information matrices. The information matrix is the inverse of the covariance matrix, $Y = P^{-1}$. A measurement $y_{i,k}$ provides a local information contribution given by the matrix $M_i$:\n    $$\n    M_i = H_i^\\top R_i^{-1} H_i\n    $$\n    If all measurements from all agents were available at a central location, the total information gain from all measurements at time $k$ would be $S = \\sum_{i=1}^N M_i$. The update step in information form is then:\n    $$\n    Y_k = Y_k^- + S\n    $$\n    where $Y_k = P_k^{-1}$ and $Y_k^- = (P_k^-)^{-1}$.\n\nCombining these steps gives the full recursion for the error covariance from one measurement update to the next:\n$$\nP_k = \\left( (P_k^-)^{-1} + S \\right)^{-1} = \\left( (A P_{k-1} A^\\top + Q)^{-1} + \\sum_{i=1}^N M_i \\right)^{-1}\n$$\nThis is a recursive equation for the posterior covariance. We seek the steady-state solution $P_\\star = \\lim_{k \\to \\infty} P_k$, which is the fixed point of this recursion. If it exists, $P_\\star$ must satisfy the Discrete Algebraic Riccati Equation (DARE):\n$$\nP_\\star = \\left( (A P_\\star A^\\top + Q)^{-1} + \\sum_{i=1}^N M_i \\right)^{-1}\n$$\nThe problem asserts that the necessary conditions of stabilizability for $(A, \\sqrt{Q})$ and detectability for $(A, H_{total})$, where $H_{total}$ corresponds to the stacked measurement matrices, are satisfied, guaranteeing a unique positive definite solution $P_\\star$.\n\nIn a distributed setting, the total information matrix $S = \\sum_{i=1}^N M_i$ is not centrally available. The agents must compute it collaboratively. This is achieved via a consensus algorithm. Specifically, agents can reach an agreement on the average information matrix $\\bar{M} = \\frac{1}{N} \\sum_{i=1}^N M_i$. Once $\\bar{M}$ is known to all agents, the total information can be found by scaling: $S = N \\bar{M}$.\n\nAverage consensus is achieved via a linear iterative process. Let $z_i(l)$ be the state of agent $i$ at consensus iteration $l$, initialized with $z_i(0) = M_i$. The update rule is:\n$$\nz_i(l+1) = \\sum_{j=1}^N W_{ij} z_j(l)\n$$\nwhere $W$ is a doubly stochastic weight matrix compatible with the network graph. For a connected, undirected graph, the Metropolis weighting scheme provides such a matrix:\n$$\nW_{ij} = \\begin{cases}\n\\frac{1}{1 + \\max\\{d_i, d_j\\}} & \\text{if } (i, j) \\text{ is an edge} \\\\\n1 - \\sum_{k \\neq i} W_{ik} & \\text{if } i=j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nwhere $d_i$ is the degree of node $i$. As $l \\to \\infty$, $z_i(l) \\to \\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i(0)$ for all $i$. In our case, the \"state\" is the vectorized information matrix, $z_i = \\text{vec}(M_i)$.\n\nThe overall algorithm is as follows:\n\n1.  **Initialization**: Each agent $i$ computes its local information matrix $M_i = H_i^\\top R_i^{-1} H_i$. The network-wide Metropolis weight matrix $W$ is computed based on the graph adjacency matrix $A_{\\text{graph}}$.\n\n2.  **Information Fusion via Consensus**:\n    a. The vectorized local information matrices, $\\text{vec}(M_i)$, are stacked into a matrix $Z_0 \\in \\mathbb{R}^{N \\times n^2}$.\n    b. The consensus iteration $Z_{l+1} = W Z_l$ is performed until the rows of $Z_l$ converge to a common vector, which represents $\\text{vec}(\\bar{M})$.\n    c. The total information matrix is recovered: $S = N \\bar{M}$. If consensus fails to converge numerically, $S$ is computed by the non-distributed direct sum as a fallback.\n\n3.  **DARE Solution**:\n    a. Initialize the covariance $P_0 = I_n$.\n    b. Iterate the DARE recursion for $k=0, 1, 2, \\dots$:\n       $$\n       P_{k+1} = \\left( (A P_k A^\\top + Q)^{-1} + S \\right)^{-1}\n       $$\n    c. The iteration terminates when the change in covariance is negligible, i.e., $\\|P_{k+1} - P_k\\|_{\\text{F}} < \\varepsilon_{\\text{cov}}$, where $\\|\\cdot\\|_{\\text{F}}$ is the Frobenius norm. The resulting matrix is the steady-state covariance $P_\\star$.\n\n4.  **Result**: The final output is the trace of the steady-state covariance, $\\operatorname{trace}(P_\\star)$, which represents the total mean squared estimation error.\n\nThis procedure is implemented for each of the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef compute_steady_state_trace(N, n, A, Q, H_list, R_list, A_graph, P0, tol_cons, l_max, tol_cov, k_max):\n    \"\"\"\n    Computes the trace of the steady-state error covariance for a distributed estimator.\n\n    Args:\n        N (int): Number of agents.\n        n (int): Dimension of the state.\n        A (np.ndarray): State transition matrix.\n        Q (np.ndarray): Process noise covariance.\n        H_list (list): List of measurement matrices H_i.\n        R_list (list): List of measurement noise covariances R_i.\n        A_graph (np.ndarray): Adjacency matrix of the communication graph.\n        P0 (np.ndarray): Initial covariance matrix.\n        tol_cons (float): Tolerance for consensus convergence.\n        l_max (int): Maximum number of consensus iterations.\n        tol_cov (float): Tolerance for DARE convergence.\n        k_max (int): Maximum number of DARE iterations.\n\n    Returns:\n        float: The trace of the steady-state covariance matrix P_star.\n    \"\"\"\n    # Step 1: Compute local information matrices and consensus weight matrix\n    M_list = [H.T @ np.linalg.inv(R) @ H for H, R in zip(H_list, R_list)]\n\n    # Metropolis-Hastings weights for consensus\n    W = np.zeros((N, N))\n    degrees = np.sum(A_graph, axis=1)\n    for i in range(N):\n        for j in range(i + 1, N):\n            if A_graph[i, j] == 1:\n                weight = 1.0 / (1.0 + max(degrees[i], degrees[j]))\n                W[i, j] = weight\n                W[j, i] = weight\n    for i in range(N):\n        W[i, i] = 1.0 - np.sum(W[i, :])\n\n    # Step 2: Information fusion via average consensus\n    M_vec_stack = np.array([m.flatten() for m in M_list])\n    \n    consensus_converged = False\n    for l in range(l_max):\n        M_vec_stack = W @ M_vec_stack\n        max_diff = np.max(np.max(M_vec_stack, axis=0) - np.min(M_vec_stack, axis=0))\n        if max_diff < tol_cons:\n            consensus_converged = True\n            break\n            \n    if consensus_converged:\n        # Averages are in each row now\n        M_avg_vec = M_vec_stack[0, :]\n        M_avg = M_avg_vec.reshape((n, n))\n        S = N * M_avg\n    else:\n        # Fallback: direct summation (centralized equivalent)\n        S = np.sum(M_list, axis=0)\n\n    # Step 3: DARE iteration to find steady-state covariance P_star\n    P = P0.copy()\n    for k in range(k_max):\n        P_prev = P\n        \n        # Prediction step\n        P_pred = A @ P_prev @ A.T + Q\n        \n        # Update step (in information form)\n        P = np.linalg.inv(np.linalg.inv(P_pred) + S)\n        \n        # Check for convergence\n        if np.linalg.norm(P - P_prev, 'fro') < tol_cov:\n            break\n            \n    P_star = P\n    \n    # Step 4: Return the trace\n    return np.trace(P_star)\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    # Numerical specifications from the problem\n    tol_cons = 1e-10\n    l_max = 2000\n    tol_cov = 1e-12\n    k_max = 10000\n\n    test_cases = [\n        # Test case 1\n        {\n            \"N\": 2, \"n\": 1,\n            \"A\": np.array([[1.0]]),\n            \"Q\": np.array([[0.5]]),\n            \"H_list\": [np.array([[1.0]]), np.array([[0.0]])],\n            \"R_list\": [np.array([[2.0]]), np.array([[1.0]])],\n            \"A_graph\": np.array([[0, 1], [1, 0]])\n        },\n        # Test case 2\n        {\n            \"N\": 2, \"n\": 2,\n            \"A\": np.diag([0.9, 0.8]),\n            \"Q\": np.diag([0.1, 0.1]),\n            \"H_list\": [np.array([[1.0, 0.0]]), np.array([[0.0, 1.0]])],\n            \"R_list\": [np.array([[0.5]]), np.array([[0.5]])],\n            \"A_graph\": np.array([[0, 1], [1, 0]])\n        },\n        # Test case 3\n        {\n            \"N\": 3, \"n\": 2,\n            \"A\": np.array([[1.0, 0.1], [0.0, 1.0]]),\n            \"Q\": np.diag([0.01, 0.01]),\n            \"H_list\": [\n                np.array([[1.0, 0.0]]),\n                np.array([[0.0, 1.0]]),\n                np.array([[1.0, 1.0]])\n            ],\n            \"R_list\": [\n                np.array([[0.2]]),\n                np.array([[0.2]]),\n                np.array([[0.3]])\n            ],\n            \"A_graph\": np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n        },\n        # Test case 4\n        {\n            \"N\": 3, \"n\": 2,\n            \"A\": np.diag([0.95, 1.02]),\n            \"Q\": np.diag([0.05, 0.05]),\n            \"H_list\": [\n                np.array([[1.0, 0.0]]),\n                np.array([[0.0, 1.0]]),\n                np.array([[1.0, 1.0]])\n            ],\n            \"R_list\": [\n                np.array([[10.0]]),\n                np.array([[10.0]]),\n                np.array([[20.0]])\n            ],\n            \"A_graph\": np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        P0 = np.eye(n)\n        trace_p_star = compute_steady_state_trace(\n            case[\"N\"], case[\"n\"], case[\"A\"], case[\"Q\"],\n            case[\"H_list\"], case[\"R_list\"], case[\"A_graph\"],\n            P0, tol_cons, l_max, tol_cov, k_max\n        )\n        results.append(trace_p_star)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2726151"}]}