{"hands_on_practices": [{"introduction": "The first step in any fault-tolerant system is reliably detecting when an anomaly has occurred. This is often achieved by monitoring a \"residual\" signal, which should be near zero during normal operation. This exercise [@problem_id:2707656] provides hands-on practice in the fundamental task of setting a statistically meaningful detection threshold, formalizing the trade-off between sensitivity to faults and the rate of false alarms. You will use the properties of multivariate Gaussian vectors to derive the exact probability distribution of a common decision statistic, a core skill in designing robust fault detection systems.", "problem": "A parity-space residual generator is used for fault detection in a sensor-redundant linear time-invariant system. At each discrete time index $k$, the residual vector $r_k \\in \\mathbb{R}^2$ under the no-fault hypothesis is modeled as a zero-mean multivariate Gaussian random vector with covariance matrix $\\Sigma_r \\in \\mathbb{R}^{2 \\times 2}$, where\n$$\n\\Sigma_r \\;=\\; \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nA quadratic decision statistic is formed as\n$$\nJ_k \\;=\\; r_k^{\\top}\\,\\Sigma_r^{-1}\\,r_k,\n$$\nand an alarm is declared if $J_k > \\gamma$. \n\nStarting from the fundamental definitions of the multivariate Gaussian distribution and linear transformations of Gaussian random vectors, derive the probability distribution of $J_k$ under the no-fault hypothesis. Then, using this distribution, determine the detection threshold $\\gamma$ such that the false-alarm probability equals $\\alpha = 0.05$, that is,\n$$\n\\mathbb{P}\\!\\left(J_k > \\gamma \\,\\middle|\\, \\text{no fault}\\right) \\;=\\; \\alpha.\n$$\nRound your final numerical answer for $\\gamma$ to four significant figures. State the final answer as a single real number without units.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The residual vector under the no-fault hypothesis is $r_k \\in \\mathbb{R}^2$.\n- The distribution of the residual vector is a zero-mean multivariate Gaussian: $r_k \\sim \\mathcal{N}(0, \\Sigma_r)$.\n- The covariance matrix is $\\Sigma_r = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$.\n- The quadratic decision statistic is $J_k = r_k^{\\top}\\,\\Sigma_r^{-1}\\,r_k$.\n- A fault is declared if $J_k > \\gamma$.\n- The specified false-alarm probability is $\\mathbb{P}(J_k > \\gamma \\,|\\, \\text{no fault}) = \\alpha = 0.05$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, a standard application of statistical decision theory in fault detection. The given covariance matrix $\\Sigma_r$ is symmetric. Its determinant is $\\det(\\Sigma_r) = (3)(2) - (1)(1) = 5 > 0$, and its diagonal entries are positive. Thus, $\\Sigma_r$ is a positive-definite matrix, which is a valid requirement for a covariance matrix of a non-degenerate distribution. The problem is well-posed, as all information required to determine the distribution of $J_k$ and subsequently the threshold $\\gamma$ is provided. The language is objective and precise. The problem is not trivial, as it requires knowledge of the properties of quadratic forms of Gaussian vectors. The problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nThe objective is to first derive the probability distribution of the statistic $J_k = r_k^{\\top}\\Sigma_r^{-1}r_k$ and then to compute the threshold $\\gamma$ corresponding to a false-alarm probability of $\\alpha = 0.05$.\n\nUnder the no-fault hypothesis, the residual vector $r_k$ is a $2$-dimensional multivariate Gaussian random vector with mean vector $0$ and covariance matrix $\\Sigma_r$. We write this as $r_k \\sim \\mathcal{N}(0, \\Sigma_r)$.\n\nThe fundamental theory of multivariate Gaussian vectors states that any linear transformation of a Gaussian vector results in another Gaussian vector. Let us define a transformation to standardize the vector $r_k$. Since $\\Sigma_r$ is a symmetric positive-definite matrix, it admits a Cholesky decomposition or, more generally, a matrix square root. Let $\\Sigma_r^{-1/2}$ be the symmetric positive-definite square root of the inverse covariance matrix $\\Sigma_r^{-1}$. We define a new random vector $z_k \\in \\mathbb{R}^2$ as:\n$$\nz_k = \\Sigma_r^{-1/2} r_k\n$$\nThis is a linear transformation of $r_k$. We can determine the distribution of $z_k$ by computing its mean and covariance matrix.\n\nThe mean of $z_k$ is:\n$$\n\\mathbb{E}[z_k] = \\mathbb{E}[\\Sigma_r^{-1/2} r_k] = \\Sigma_r^{-1/2} \\mathbb{E}[r_k] = \\Sigma_r^{-1/2} \\cdot 0 = 0\n$$\nThe covariance matrix of $z_k$ is:\n$$\n\\text{Cov}(z_k) = \\mathbb{E}[(z_k - \\mathbb{E}[z_k])(z_k - \\mathbb{E}[z_k])^{\\top}] = \\mathbb{E}[z_k z_k^{\\top}]\n$$\nSubstituting the expression for $z_k$:\n$$\n\\text{Cov}(z_k) = \\mathbb{E}[(\\Sigma_r^{-1/2} r_k)(\\Sigma_r^{-1/2} r_k)^{\\top}] = \\mathbb{E}[\\Sigma_r^{-1/2} r_k r_k^{\\top} (\\Sigma_r^{-1/2})^{\\top}]\n$$\nSince $\\Sigma_r^{-1/2}$ is a constant matrix and symmetric, $(\\Sigma_r^{-1/2})^{\\top} = \\Sigma_r^{-1/2}$. We can write:\n$$\n\\text{Cov}(z_k) = \\Sigma_r^{-1/2} \\mathbb{E}[r_k r_k^{\\top}] \\Sigma_r^{-1/2}\n$$\nBy definition, $\\mathbb{E}[r_k r_k^{\\top}]$ is the covariance matrix of $r_k$, which is $\\Sigma_r$. Therefore:\n$$\n\\text{Cov}(z_k) = \\Sigma_r^{-1/2} \\Sigma_r \\Sigma_r^{-1/2} = \\Sigma_r^{-1/2} (\\Sigma_r^{1/2} \\Sigma_r^{1/2}) \\Sigma_r^{-1/2} = (\\Sigma_r^{-1/2} \\Sigma_r^{1/2}) (\\Sigma_r^{1/2} \\Sigma_r^{-1/2}) = I_2 \\cdot I_2 = I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix.\nThus, $z_k$ is a multivariate Gaussian vector with zero mean and an identity covariance matrix, $z_k \\sim \\mathcal{N}(0, I_2)$. This means that the components of $z_k$, let us denote them $z_{k,1}$ and $z_{k,2}$, are independent and identically distributed standard normal random variables, with $z_{k,i} \\sim \\mathcal{N}(0, 1)$ for $i=1, 2$.\n\nNow, let us examine the decision statistic $J_k$:\n$$\nJ_k = r_k^{\\top}\\Sigma_r^{-1}r_k\n$$\nSince $\\Sigma_r^{-1} = (\\Sigma_r^{-1/2})^{\\top} \\Sigma_r^{-1/2}$ (as $\\Sigma_r^{-1/2}$ is symmetric), we can rewrite $J_k$ as:\n$$\nJ_k = r_k^{\\top} (\\Sigma_r^{-1/2})^{\\top} \\Sigma_r^{-1/2} r_k = (\\Sigma_r^{-1/2} r_k)^{\\top} (\\Sigma_r^{-1/2} r_k) = z_k^{\\top} z_k\n$$\nExpanding this expression gives:\n$$\nJ_k = \\begin{pmatrix} z_{k,1} & z_{k,2} \\end{pmatrix} \\begin{pmatrix} z_{k,1} \\\\ z_{k,2} \\end{pmatrix} = z_{k,1}^2 + z_{k,2}^2\n$$\nBy the fundamental definition of the chi-squared ($\\chi^2$) distribution, the sum of the squares of $n$ independent standard normal random variables is a chi-squared distributed random variable with $n$ degrees of freedom. In this case, $n=2$.\nTherefore, the statistic $J_k$ follows a chi-squared distribution with $2$ degrees of freedom:\n$$\nJ_k \\sim \\chi_2^2\n$$\nThis concludes the first part of the problem.\n\nFor the second part, we must find the threshold $\\gamma$ such that the false-alarm probability is $\\alpha = 0.05$. The condition is:\n$$\n\\mathbb{P}(J_k > \\gamma) = \\alpha = 0.05\n$$\nThis means that $\\gamma$ is the value such that the area under the probability density function (PDF) of the $\\chi_2^2$ distribution from $\\gamma$ to infinity is $0.05$. This is equivalent to finding the value $\\gamma$ for which the cumulative distribution function (CDF) is $1 - \\alpha = 0.95$.\nLet $F_{\\chi_2^2}(x)$ be the CDF of the $\\chi_2^2$ distribution. We need to solve $F_{\\chi_2^2}(\\gamma) = 0.95$.\n\nA chi-squared distribution with $2$ degrees of freedom is a special case that is equivalent to an exponential distribution. The PDF of a $\\chi_n^2$ distribution is $f(x; n) = \\frac{1}{2^{n/2}\\Gamma(n/2)} x^{n/2 - 1} \\exp(-x/2)$ for $x \\ge 0$. For $n=2$:\n$$\nf(x; 2) = \\frac{1}{2^{2/2}\\Gamma(2/2)} x^{2/2 - 1} \\exp(-x/2) = \\frac{1}{2\\Gamma(1)} x^0 \\exp(-x/2) = \\frac{1}{2}\\exp(-x/2)\n$$\nThis is the PDF of an exponential distribution with rate parameter $\\lambda = 1/2$. The CDF of this distribution is $F(x) = 1 - \\exp(-\\lambda x) = 1 - \\exp(-x/2)$.\n\nWe set the CDF equal to $1-\\alpha$:\n$$\nF_{\\chi_2^2}(\\gamma) = 1 - \\exp(-\\gamma/2) = 1 - 0.05 = 0.95\n$$\nSolving for $\\gamma$:\n$$\n\\exp(-\\gamma/2) = 0.05\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{\\gamma}{2} = \\ln(0.05)\n$$\n$$\n\\gamma = -2 \\ln(0.05) = 2 \\ln\\left(\\frac{1}{0.05}\\right) = 2 \\ln(20)\n$$\nNow, we calculate the numerical value:\n$$\n\\gamma \\approx 2 \\times 2.99573227... = 5.99146454...\n$$\nRounding to four significant figures, we get:\n$$\n\\gamma \\approx 5.991\n$$\nThis is the required detection threshold.", "answer": "$$\n\\boxed{5.991}\n$$", "id": "2707656"}, {"introduction": "After detecting a fault, the next critical challenge is to identify its specific source, a process known as fault isolation. This practice [@problem_id:2707683] explores the crucial distinction between fault detectability (knowing something is wrong) and isolability (knowing what is wrong). By deriving and analyzing a fault-to-residual transfer matrix for an observer-based system, you will gain a deep, structural understanding of why some faults, despite being detectable, may be mathematically impossible to distinguish from one another.", "problem": "Consider a continuous-time Linear Time-Invariant (LTI) plant with two additive actuator faults and full-state measurement:\n$$\n\\dot{x}(t) = A x(t) + E f(t), \\quad y(t) = C x(t),\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$, $y(t) \\in \\mathbb{R}^{2}$, and $f(t) = \\begin{bmatrix} f_{1}(t) \\\\ f_{2}(t) \\end{bmatrix} \\in \\mathbb{R}^{2}$. The matrices are\n$$\nA = \\begin{bmatrix} -1 & 1 \\\\ 0 & -2 \\end{bmatrix}, \\quad C = I_{2}, \\quad E = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\end{bmatrix}.\n$$\nDesign a full-order Luenberger observer-based residual generator\n$$\n\\dot{\\hat{x}}(t) = A \\hat{x}(t) + L \\big(y(t) - C \\hat{x}(t)\\big), \\quad r(t) = y(t) - C \\hat{x}(t),\n$$\nwith observer gain\n$$\nL = \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix}.\n$$\nStart from the state estimation error dynamics definition and first principles to derive the fault-to-residual transfer matrix $T_{f}(s)$ such that in the Laplace domain $r(s) = T_{f}(s) f(s)$ for zero initial conditions. Use your derivation to address the following tasks:\n\n- Using only the derived expressions and linear-algebraic reasoning, determine whether each fault $f_{1}$ and $f_{2}$ is detectable and whether they are isolable from one another in the sense of Fault Detection and Isolation (FDI). Base your conclusion on the structural properties of the columns of $T_{f}(s)$.\n\n- Compute explicitly the two residual signature vectors, i.e., the columns of $T_{f}(s)$, and specialize them at the frequency $s = 1$. Normalize each specialized column to unit $2$-norm to obtain two residual directions $d_{1}$ and $d_{2}$ in $\\mathbb{R}^{2}$.\n\n- Compute the principal angle $\\theta$ between these two directions using $\\theta = \\arccos\\big(d_{1}^{\\top} d_{2}\\big)$ in radians.\n\nReport as your final answer the angle $\\theta$ in radians. No rounding is required. All angles must be expressed in radians.", "solution": "The problem as stated is well-posed, scientifically sound, and contains all necessary information for a unique solution. It is a standard problem in fault diagnosis theory. I shall proceed with the derivation and analysis.\n\nThe state estimation error is defined as $e(t) = x(t) - \\hat{x}(t)$. We must first derive the dynamics governing this error. The time derivative is $\\dot{e}(t) = \\dot{x}(t) - \\dot{\\hat{x}}(t)$. By substituting the given system and observer dynamics, we obtain:\n$$\n\\dot{e}(t) = \\big(A x(t) + E f(t)\\big) - \\Big(A \\hat{x}(t) + L \\big(y(t) - C \\hat{x}(t)\\big)\\Big)\n$$\nGiven that the output is $y(t) = C x(t)$, we can rewrite the observer dynamics substitution as:\n$$\n\\dot{\\hat{x}}(t) = A \\hat{x}(t) + L \\big(C x(t) - C \\hat{x}(t)\\big) = A \\hat{x}(t) + LC \\big(x(t) - \\hat{x}(t)\\big) = A \\hat{x}(t) + LC e(t)\n$$\nSubstituting this back into the expression for $\\dot{e}(t)$:\n$$\n\\dot{e}(t) = \\big(A x(t) + E f(t)\\big) - \\big(A \\hat{x}(t) + LC e(t)\\big) = A \\big(x(t) - \\hat{x}(t)\\big) - LC e(t) + E f(t)\n$$\nThis simplifies to the fundamental error dynamics equation:\n$$\n\\dot{e}(t) = (A - LC) e(t) + E f(t)\n$$\nThe residual vector is defined as $r(t) = y(t) - C \\hat{x}(t)$. Using $y(t) = C x(t)$, this becomes:\n$$\nr(t) = C x(t) - C \\hat{x}(t) = C\\big(x(t) - \\hat{x}(t)\\big) = C e(t)\n$$\nTo find the fault-to-residual transfer matrix $T_f(s)$, we apply the Laplace transform to the error dynamics and the residual equation, assuming zero initial conditions ($e(0)=0$).\n$$\ns e(s) = (A - LC) e(s) + E f(s)\n$$\n$$\n\\big(sI - A + LC\\big) e(s) = E f(s) \\implies e(s) = \\big(sI - A + LC\\big)^{-1} E f(s)\n$$\nThe residual in the Laplace domain is $r(s) = C e(s)$. Substituting the expression for $e(s)$, we find:\n$$\nr(s) = C \\big(sI - A + LC\\big)^{-1} E f(s)\n$$\nFrom this, the fault-to-residual transfer matrix is identified as:\n$$\nT_f(s) = C \\big(sI - A + LC\\big)^{-1} E\n$$\nNow, we substitute the specified matrices: $A = \\begin{bmatrix} -1 & 1 \\\\ 0 & -2 \\end{bmatrix}$, $C = I_{2}$, $E = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\end{bmatrix}$, and $L = \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix} = 3I_2$.\nFirst, we compute the matrix $A - LC$:\n$$\nA - LC = \\begin{bmatrix} -1 & 1 \\\\ 0 & -2 \\end{bmatrix} - \\left(\\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right) = \\begin{bmatrix} -1 & 1 \\\\ 0 & -2 \\end{bmatrix} - \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} -4 & 1 \\\\ 0 & -5 \\end{bmatrix}\n$$\nThe eigenvalues of this matrix are $-4$ and $-5$, which are in the open left-half complex plane, confirming observer stability. Next, we find the matrix to be inverted:\n$$\nsI - (A - LC) = \\begin{bmatrix} s & 0 \\\\ 0 & s \\end{bmatrix} - \\begin{bmatrix} -4 & 1 \\\\ 0 & -5 \\end{bmatrix} = \\begin{bmatrix} s+4 & -1 \\\\ 0 & s+5 \\end{bmatrix}\n$$\nThe inverse is:\n$$\n\\big(sI - A + LC\\big)^{-1} = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} s+5 & 1 \\\\ 0 & s+4 \\end{bmatrix}\n$$\nSince $C = I_2$, $T_f(s) = \\big(sI - A + LC\\big)^{-1} E$. We compute this product:\n$$\nT_f(s) = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} s+5 & 1 \\\\ 0 & s+4 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\end{bmatrix} = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} (s+5)(1) + (1)(1) & (s+5)(2) + (1)(2) \\\\ (0)(1) + (s+4)(1) & (0)(2) + (s+4)(2) \\end{bmatrix}\n$$\n$$\nT_f(s) = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} s+6 & 2s+12 \\\\ s+4 & 2s+8 \\end{bmatrix} = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} s+6 & 2(s+6) \\\\ s+4 & 2(s+4) \\end{bmatrix}\n$$\nThe columns of $T_f(s)$, which represent the transfer functions from each fault to the residual vector, are the fault signature vectors:\n$T_{f1}(s) = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} s+6 \\\\ s+4 \\end{bmatrix}$ and $T_{f2}(s) = \\frac{1}{(s+4)(s+5)} \\begin{bmatrix} 2(s+6) \\\\ 2(s+4) \\end{bmatrix}$.\n\nFor fault detectability and isolability, we analyze these columns. A fault $f_i$ is detectable if $T_{fi}(s)$ is not identically zero. Both $T_{f1}(s)$ and $T_{f2}(s)$ are non-zero transfer functions, so both faults $f_1$ and $f_2$ are detectable. Two faults are isolable if their signature vectors are linearly independent over the field of rational functions of $s$. Here, it is immediately apparent that $T_{f2}(s) = 2 T_{f1}(s)$. The columns are linearly dependent. This means the direction of the residual vector is identical for both faults. Consequently, faults $f_1$ and $f_2$ are not isolable from one another. This is an unavoidable consequence of the columns of the fault distribution matrix $E$ being linearly dependent, as $E = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\end{bmatrix}$.\n\nNext, we evaluate these signature vectors at the frequency $s=1$. Let these specialized vectors be $v_1$ and $v_2$.\n$$\nv_1 = T_{f1}(1) = \\frac{1}{(1+4)(1+5)} \\begin{bmatrix} 1+6 \\\\ 1+4 \\end{bmatrix} = \\frac{1}{30} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}\n$$\n$$\nv_2 = T_{f2}(1) = \\frac{1}{30} \\begin{bmatrix} 2(1+6) \\\\ 2(1+4) \\end{bmatrix} = \\frac{1}{30} \\begin{bmatrix} 14 \\\\ 10 \\end{bmatrix}\n$$\nWe normalize these vectors to obtain the residual directions $d_1$ and $d_2$. For $d_1$:\n$$\n\\|v_1\\|_2 = \\left\\| \\frac{1}{30} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix} \\right\\|_2 = \\frac{1}{30} \\sqrt{7^2 + 5^2} = \\frac{\\sqrt{49+25}}{30} = \\frac{\\sqrt{74}}{30}\n$$\n$$\nd_1 = \\frac{v_1}{\\|v_1\\|_2} = \\frac{\\frac{1}{30} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}}{\\frac{\\sqrt{74}}{30}} = \\frac{1}{\\sqrt{74}} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}\n$$\nFor $d_2$, since $v_2 = 2v_1$, the normalized direction must be the same:\n$$\nd_2 = \\frac{v_2}{\\|v_2\\|_2} = \\frac{2v_1}{\\|2v_1\\|_2} = \\frac{2v_1}{2\\|v_1\\|_2} = \\frac{v_1}{\\|v_1\\|_2} = d_1 = \\frac{1}{\\sqrt{74}} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}\n$$\nThe principal angle $\\theta$ between these directions is given by $\\theta = \\arccos(d_1^\\top d_2)$.\n$$\nd_1^\\top d_2 = \\left(\\frac{1}{\\sqrt{74}} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}\\right)^\\top \\left(\\frac{1}{\\sqrt{74}} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix}\\right) = \\frac{1}{74} \\begin{bmatrix} 7 & 5 \\end{bmatrix} \\begin{bmatrix} 7 \\\\ 5 \\end{bmatrix} = \\frac{1}{74}(49+25) = \\frac{74}{74} = 1\n$$\nFinally, we compute the angle $\\theta$:\n$$\n\\theta = \\arccos(1) = 0 \\text{ radians}\n$$\nThis result quantitatively confirms that the fault directions are identical, which is the mathematical basis for non-isolability.", "answer": "$$\n\\boxed{0}\n$$", "id": "2707683"}, {"introduction": "Once a fault is detected and identified, the control system must adapt its strategy to maintain performance and stabilityâ€”a process called fault accommodation. This final exercise [@problem_id:2707720] introduces a powerful, optimization-based approach to this challenge. By formulating the control allocation problem as a quadratic program, you will use the Karush-Kuhn-Tucker (KKT) conditions to find the optimal actuator commands in the presence of failures and physical saturation limits, providing a glimpse into modern, resilient control design.", "problem": "Consider a fault-tolerant control allocation problem within a one-step Model Predictive Control (MPC) framework for a system with three actuators subject to saturation and partial failures. The generalized control effort to be tracked at the current sampling instant is the vector $v \\in \\mathbb{R}^{3}$. Let the actuator effectiveness (health) matrix be the diagonal matrix $E \\in \\mathbb{R}^{3 \\times 3}$, where $E_{ii} \\in [0,1]$ models the fraction of effectiveness of actuator $i$ (with $E_{ii} = 0$ indicating total failure). The allocator determines the commanded actuator input $u \\in \\mathbb{R}^{3}$ by solving the strictly convex quadratic program\n$$\n\\min_{u \\in \\mathbb{R}^{3}} \\;\\; \\frac{1}{2}\\,(E u - v)^{\\top} W (E u - v) \\;+\\; \\frac{1}{2}\\, u^{\\top} R u\n$$\nsubject to the box constraints\n$$\n\\ell \\le u \\le h,\n$$\nwhere $W \\in \\mathbb{R}^{3 \\times 3}$ and $R \\in \\mathbb{R}^{3 \\times 3}$ are positive definite diagonal weighting matrices, and the inequalities are interpreted componentwise with lower and upper bounds $\\ell, h \\in \\mathbb{R}^{3}$.\n\n1) Starting from the definitions of a convex quadratic program, the Lagrangian, and the Karush-Kuhn-Tucker (KKT) conditions, derive the necessary and sufficient KKT optimality conditions for the allocator problem above. Your derivation must be fully symbolic and should clearly identify the stationarity, primal feasibility, dual feasibility, and complementary slackness conditions in terms of the problem data $E$, $W$, $R$, $v$, $\\ell$, and $h$.\n\n2) Using your KKT conditions, compute the optimal allocation $u^{\\star}$ for the specific numerical instance\n$$\nE = \\mathrm{diag}(1,\\;0.7,\\;0), \\quad W = \\mathrm{diag}(2,\\;1,\\;3), \\quad R = \\mathrm{diag}(0.5,\\;1,\\;0.2),\n$$\n$$\nv = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}, \\quad \\ell = \\begin{pmatrix} -0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}, \\quad h = \\begin{pmatrix} 0.6 \\\\ 0.1 \\\\ 0.15 \\end{pmatrix}.\n$$\nReport the optimal allocation vector $u^{\\star}$, rounding each component to four significant figures. Express your final answer as a row vector without units.", "solution": "The problem as stated is a standard, well-posed convex optimization problem frequently encountered in control engineering. It is scientifically grounded, self-contained, and possesses a unique solution. We shall proceed with the derivation and calculation.\n\nThe problem is to find the actuator commands $u \\in \\mathbb{R}^{3}$ that solve the following strictly convex quadratic program (QP):\n$$\n\\min_{u \\in \\mathbb{R}^{3}} \\;\\; J(u) = \\frac{1}{2}\\,(E u - v)^{\\top} W (E u - v) \\;+\\; \\frac{1}{2}\\, u^{\\top} R u\n$$\nsubject to the componentwise constraints $\\ell \\le u \\le h$. The objective function $J(u)$ is strictly convex because its Hessian, $\\nabla^2_u J(u) = E^{\\top}WE + R$, is the sum of a positive definite matrix $R$ and a positive semidefinite matrix $E^{\\top}WE$, rendering the sum positive definite. The constraint set is a non-empty, closed, and convex polyhedron (a box). The minimization of a strictly convex function over such a set guarantees the existence of a unique global minimum. Therefore, the Karush-Kuhn-Tucker (KKT) conditions are both necessary and sufficient for optimality.\n\n**1) Derivation of the KKT Conditions**\n\nTo derive the KKT conditions, we first formulate the Lagrangian. The box constraints $\\ell \\le u \\le h$ are equivalent to two sets of inequality constraints: $u - h \\le 0$ and $\\ell - u \\le 0$. We introduce corresponding Lagrange multiplier vectors $\\mu \\in \\mathbb{R}^{3}$ and $\\lambda \\in \\mathbb{R}^{3}$. The Lagrangian function $\\mathcal{L}(u, \\lambda, \\mu)$ is:\n$$\n\\mathcal{L}(u, \\lambda, \\mu) = J(u) + \\lambda^{\\top}(\\ell - u) + \\mu^{\\top}(u - h)\n$$\nThe KKT conditions for optimality are:\n\n**a) Stationarity:** The gradient of the Lagrangian with respect to the primal variable $u$ must be zero.\n$$\n\\nabla_u \\mathcal{L}(u, \\lambda, \\mu) = \\nabla_u J(u) - \\lambda + \\mu = 0\n$$\nFirst, we compute the gradient of the objective function $J(u)$:\n$$\n\\nabla_u J(u) = \\nabla_u \\left( \\frac{1}{2}(u^{\\top}E^{\\top}WEu - 2v^{\\top}WEu + v^{\\top}Wv) + \\frac{1}{2}u^{\\top}Ru \\right) = E^{\\top}WEu - E^{\\top}Wv + Ru\n$$\nThe stationarity condition is therefore:\n$$\n(E^{\\top}WE + R)u - E^{\\top}Wv - \\lambda + \\mu = 0\n$$\n\n**b) Primal Feasibility:** The original constraints on $u$ must be satisfied.\n$$\n\\ell_i \\le u_i \\le h_i, \\quad \\text{for } i=1, 2, 3\n$$\n\n**c) Dual Feasibility:** The Lagrange multipliers must be non-negative.\n$$\n\\lambda_i \\ge 0, \\quad \\mu_i \\ge 0, \\quad \\text{for } i=1, 2, 3\n$$\n\n**d) Complementary Slackness:** The product of each multiplier and its corresponding constraint function must be zero.\n$$\n\\lambda_i (\\ell_i - u_i) = 0, \\quad \\mu_i (u_i - h_i) = 0, \\quad \\text{for } i=1, 2, 3\n$$\nThis implies that a multiplier can be positive only if the corresponding constraint is active (i.e., holds with equality).\n\nSince the matrices $E$, $W$, and $R$ are diagonal, the objective function and constraints are decoupled for each component $u_i$. The total objective function is a sum of component-wise objectives, $J(u) = \\sum_{i=1}^3 J_i(u_i)$, where\n$$\nJ_i(u_i) = \\frac{1}{2} W_{ii}(E_{ii} u_i - v_i)^2 + \\frac{1}{2} R_{ii} u_i^2\n$$\nand we solve $\\min J_i(u_i)$ subject to $\\ell_i \\le u_i \\le h_i$ independently for each $i \\in \\{1, 2, 3\\}$. The KKT conditions can be written for each component $i$:\n\\begin{align*}\n\\text{Stationarity: } & (E_{ii}^2 W_{ii} + R_{ii}) u_i - E_{ii} W_{ii} v_i - \\lambda_i + \\mu_i = 0 \\\\\n\\text{Primal Feasibility: } & \\ell_i \\le u_i \\le h_i \\\\\n\\text{Dual Feasibility: } & \\lambda_i \\ge 0, \\; \\mu_i \\ge 0 \\\\\n\\text{Complementary Slackness: } & \\lambda_i(\\ell_i - u_i) = 0, \\; \\mu_i(u_i - h_i) = 0\n\\end{align*}\n\n**2) Computation of the Optimal Allocation**\n\nWe apply the component-wise KKT conditions to the given numerical instance. For each component $i$, the optimal solution $u_i^{\\star}$ is found by first computing the unconstrained minimum of $J_i(u_i)$ and then projecting it onto the feasible interval $[\\ell_i, h_i]$. The unconstrained minimum, denoted $u_i^{\\text{unc}}$, is found by setting $\\lambda_i = 0$ and $\\mu_i = 0$, which yields from the stationarity condition:\n$$\nu_i^{\\text{unc}} = \\frac{E_{ii} W_{ii} v_i}{E_{ii}^2 W_{ii} + R_{ii}}\n$$\nThe optimal solution is then $u_i^{\\star} = \\max(\\ell_i, \\min(h_i, u_i^{\\text{unc}}))$.\n\nThe given data is:\n$E = \\mathrm{diag}(1, 0.7, 0)$, $W = \\mathrm{diag}(2, 1, 3)$, $R = \\mathrm{diag}(0.5, 1, 0.2)$\n$v = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}$, $\\ell = \\begin{pmatrix} -0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}$, $h = \\begin{pmatrix} 0.6 \\\\ 0.1 \\\\ 0.15 \\end{pmatrix}$.\n\n**Component $i=1$:**\n$E_{11}=1$, $W_{11}=2$, $R_{11}=0.5$, $v_1=1.0$, $\\ell_1=-0.2$, $h_1=0.6$.\nThe unconstrained solution is:\n$$\nu_1^{\\text{unc}} = \\frac{1 \\cdot 2 \\cdot 1.0}{1^2 \\cdot 2 + 0.5} = \\frac{2.0}{2.5} = 0.8\n$$\nThe feasible interval is $[-0.2, 0.6]$. Since $u_1^{\\text{unc}} = 0.8 > h_1 = 0.6$, the solution is saturated at the upper bound.\n$$\nu_1^{\\star} = h_1 = 0.6\n$$\n\n**Component $i=2$:**\n$E_{22}=0.7$, $W_{22}=1$, $R_{22}=1$, $v_2=-0.5$, $\\ell_2=-0.3$, $h_2=0.1$.\nThe unconstrained solution is:\n$$\nu_2^{\\text{unc}} = \\frac{0.7 \\cdot 1 \\cdot (-0.5)}{0.7^2 \\cdot 1 + 1} = \\frac{-0.35}{0.49 + 1} = \\frac{-0.35}{1.49} \\approx -0.234899...\n$$\nThe feasible interval is $[-0.3, 0.1]$. Since $\\ell_2 = -0.3 < u_2^{\\text{unc}} < h_2 = 0.1$, the unconstrained solution is feasible.\n$$\nu_2^{\\star} = \\frac{-0.35}{1.49}\n$$\n\n**Component $i=3$:**\n$E_{33}=0$, $W_{33}=3$, $R_{33}=0.2$, $v_3=0.2$, $\\ell_3=-0.1$, $h_3=0.15$.\nThe actuator has completely failed. The unconstrained solution is:\n$$\nu_3^{\\text{unc}} = \\frac{0 \\cdot 3 \\cdot 0.2}{0^2 \\cdot 3 + 0.2} = \\frac{0}{0.2} = 0\n$$\nAlternatively, note that the cost component $J_3(u_3)$ is $\\frac{1}{2}W_{33}(0 \\cdot u_3 - v_3)^2 + \\frac{1}{2}R_{33}u_3^2 = \\frac{1}{2}W_{33}v_3^2 + \\frac{1}{2}R_{33}u_3^2$.\nTo minimize this cost, we must minimize $\\frac{1}{2}R_{33}u_3^2$ (as the first term is constant), which for $R_{33} > 0$ means driving $u_3$ as close to $0$ as possible. The feasible interval is $[-0.1, 0.15]$, which contains $0$.\nTherefore, the optimal solution is:\n$$\nu_3^{\\star} = 0\n$$\n\nThe final optimal allocation vector $u^{\\star} = (u_1^{\\star}, u_2^{\\star}, u_3^{\\star})^{\\top}$ must be reported with each component rounded to four significant figures.\n$u_1^{\\star} = 0.6$. Rounded to four significant figures, this is $0.6000$.\n$u_2^{\\star} = -0.35/1.49 \\approx -0.234899...$. Rounded to four significant figures, this is $-0.2349$.\n$u_3^{\\star} = 0$. Represented to be consistent with the other components, this is $0.0000$.\n\nThe final optimal allocation is $u^{\\star} = (0.6000, -0.2349, 0.0000)^{\\top}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.6000 & -0.2349 & 0.0000 \\end{pmatrix}}\n$$", "id": "2707720"}]}