{"hands_on_practices": [{"introduction": "The first step in understanding networked control is to grapple with its most fundamental limitation: how packet dropouts can render a deterministically controllable system impossible to stabilize. This exercise provides a stark counterexample by asking you to derive the critical packet success probability needed to stabilize an unstable scalar system in the mean-square sense. By working through this problem [@problem_id:2727000], you will gain a crucial intuition for the trade-off between the system's inherent instability during dropouts and the corrective action of the controller during successful transmissions.", "problem": "Consider a scalar discrete-time plant in a Networked Control System (NCS) with actuation packet dropouts modeled as a multiplicative Bernoulli process. The plant evolves as $x_{k+1} = a\\,x_k + b\\,\\gamma_k\\,u_k$, where $x_k \\in \\mathbb{R}$ is the state, $u_k \\in \\mathbb{R}$ is the control input, $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}\\setminus\\{0\\}$ are fixed parameters, and $\\{\\gamma_k\\}_{k \\ge 0}$ is an independent and identically distributed (i.i.d.) sequence with $\\gamma_k \\in \\{0,1\\}$ representing whether the control packet sent at time $k$ is successfully applied at the actuator. The success probability is $\\mathbb{P}\\{\\gamma_k = 1\\} = p$ and the dropout probability is $\\mathbb{P}\\{\\gamma_k = 0\\} = 1-p$, with $0 \\le p \\le 1$. The controller has perfect, instantaneous state information and applies static state feedback $u_k = K x_k$ with a constant gain $K \\in \\mathbb{R}$. The goal is mean-square stabilization, namely $\\lim_{k \\to \\infty} \\mathbb{E}[x_k^2] = 0$.\n\nYou are given the concrete plant with $a = 2$ and $b = 1$. In the deterministic sense (no losses), the pair $(a,b)$ is controllable because $b \\ne 0$. However, over the lossy actuation channel, mean-square stabilizability may fail for some values of $p$ despite deterministic controllability.\n\nTasks:\n- Starting from the definition of mean-square stability and the law of total expectation, derive the one-step second-moment recursion for $\\mathbb{E}[x_{k+1}^2]$ under the feedback $u_k = K x_k$ and the i.i.d. Bernoulli packet model described above. Show how the resulting multiplicative factor on $\\mathbb{E}[x_k^2]$ depends on $a$, $b$, $K$, and $p$, and determine the choice of $K$ that minimizes this factor.\n- From this derivation, obtain the condition on $p$ (in terms of $a$ and $b$) that is necessary and sufficient for the existence of a static gain $K$ that achieves mean-square stability.\n- Specialize your expression to the given plant with $a = 2$ and $b = 1$, and compute the critical success probability $p^{\\star}$ (the infimum of $p$ for which mean-square stabilization is possible). Then use $p = 0.70$ to argue briefly why this provides a counterexample where deterministic controllability holds but mean-square stabilizability fails, and explain the underlying reason in terms of the competing effects of open-loop growth during dropouts and control action during successful transmissions.\n\nGive your final answer as the exact value of the critical success probability $p^{\\star}$ for $a = 2$ and $b = 1$. No rounding is required. Do not include units in your final answer.", "solution": "The problem statement is first validated.\n\n**Step 1: Extract Givens**\n- Plant dynamics: $x_{k+1} = a\\,x_k + b\\,\\gamma_k\\,u_k$\n- State: $x_k \\in \\mathbb{R}$\n- Control input: $u_k \\in \\mathbb{R}$\n- Plant parameters: $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}\\setminus\\{0\\}$\n- Packet dropout model: $\\{\\gamma_k\\}_{k \\ge 0}$ is an independent and identically distributed (i.i.d.) Bernoulli sequence with $\\gamma_k \\in \\{0,1\\}$.\n- Probabilities: $\\mathbb{P}\\{\\gamma_k = 1\\} = p$ and $\\mathbb{P}\\{\\gamma_k = 0\\} = 1-p$, where $0 \\le p \\le 1$.\n- Controller: Static state feedback $u_k = K x_k$ with a constant gain $K \\in \\mathbb{R}$.\n- Objective: Mean-square stabilization, i.e., $\\lim_{k \\to \\infty} \\mathbb{E}[x_k^2] = 0$.\n- Specific parameters: $a = 2$ and $b = 1$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard, fundamental problem in the theory of Networked Control Systems (NCS) and stochastic control. The Bernoulli model for packet dropouts is a widely used and accepted simplification.\n- **Well-Posed:** The problem is well-defined, with a clear objective and sufficient information to derive a unique solution for the critical probability.\n- **Objective:** The problem is stated using precise mathematical language, free from subjective or ambiguous terms.\n- **Conclusion:** The problem is valid as it is scientifically sound, well-posed, objective, and contains no internal contradictions or missing information.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Second-Moment Recursion**\nWe begin with the closed-loop system dynamics, substituting the control law $u_k = K x_k$ into the plant equation:\n$$x_{k+1} = a\\,x_k + b\\,\\gamma_k\\,(K x_k) = (a + b K \\gamma_k) x_k$$\nThe goal is mean-square stability, which concerns the evolution of the second moment of the state, $\\mathbb{E}[x_k^2]$. We derive the one-step recursion for this quantity.\n$$x_{k+1}^2 = ((a + b K \\gamma_k) x_k)^2 = (a + b K \\gamma_k)^2 x_k^2$$\nTo compute the expectation $\\mathbb{E}[x_{k+1}^2]$, we use the law of total expectation, conditioning on the information available at time $k$, which we denote by the filtration $\\mathcal{F}_k$. The state $x_k$ is $\\mathcal{F}_k$-measurable, while the random variable $\\gamma_k$ is independent of $\\mathcal{F}_k$ due to the i.i.d. assumption.\n$$\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[\\mathbb{E}[x_{k+1}^2 | \\mathcal{F}_k]] = \\mathbb{E}[\\mathbb{E}[(a + b K \\gamma_k)^2 x_k^2 | \\mathcal{F}_k]]$$\nSince $x_k$ is known given $\\mathcal{F}_k$, it can be treated as a constant inside the inner expectation:\n$$\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[x_k^2 \\mathbb{E}[(a + b K \\gamma_k)^2 | \\mathcal{F}_k]]$$\nBecause $\\gamma_k$ is independent of $\\mathcal{F}_k$, the conditioning does not affect its expectation:\n$$\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[x_k^2 \\mathbb{E}[(a + b K \\gamma_k)^2]] = \\mathbb{E}[(a + b K \\gamma_k)^2] \\mathbb{E}[x_k^2]$$\nThe term $\\mathbb{E}[(a + b K \\gamma_k)^2]$ is the multiplicative factor for the second moment. We can expand this expectation using the definition of the Bernoulli random variable $\\gamma_k$:\n\\begin{align*} \\mathbb{E}[(a + b K \\gamma_k)^2] &= (a + b K \\cdot 1)^2 \\mathbb{P}\\{\\gamma_k = 1\\} + (a + b K \\cdot 0)^2 \\mathbb{P}\\{\\gamma_k = 0\\} \\\\ &= (a + b K)^2 p + a^2 (1-p) \\end{align*}\nThus, the one-step second-moment recursion is:\n$$\\mathbb{E}[x_{k+1}^2] = \\left[ p(a + bK)^2 + (1-p)a^2 \\right] \\mathbb{E}[x_k^2]$$\nFor the system to be mean-square stable, we require $\\lim_{k \\to \\infty} \\mathbb{E}[x_k^2] = 0$, which holds if and only if the multiplicative factor is strictly less than $1$:\n$$p(a + bK)^2 + (1-p)a^2 < 1$$\n\n**Optimal Gain and Condition for Stabilizability**\nTo find the condition for the *existence* of a stabilizing gain $K$, we must find the gain $K$ that minimizes this factor and check if that minimum value can be made less than $1$. Let $f(K) = p(a + bK)^2 + (1-p)a^2$. This is a quadratic function of $K$. We find the minimum by taking the derivative with respect to $K$ and setting it to zero.\n$$\\frac{df}{dK} = \\frac{d}{dK} \\left[ p(a^2 + 2abK + b^2K^2) + (1-p)a^2 \\right] = p(2ab + 2b^2K)$$\nAssuming $p > 0$ and $b \\ne 0$ (as given), we set the derivative to $0$:\n$$2ab + 2b^2K = 0 \\implies K = -\\frac{2ab}{2b^2} = -\\frac{a}{b}$$\nThis optimal gain, let's call it $K^{\\star} = -a/b$, minimizes the factor. The second derivative $\\frac{d^2f}{dK^2} = 2pb^2 > 0$, confirming it is a minimum.\nSubstituting $K^{\\star}$ back into the factor gives the minimum possible value:\n$$\\min_K f(K) = p\\left(a + b\\left(-\\frac{a}{b}\\right)\\right)^2 + (1-p)a^2 = p(a-a)^2 + (1-p)a^2 = a^2(1-p)$$\nThe system is mean-square stabilizable if and only if this minimum factor is less than $1$. Note that if the open-loop plant is already mean-square stable (i.e., $|a|<1$), it is stable for any $p \\in [0,1]$ (with $K=0$, the factor is $a^2 < 1$). The interesting case is when $|a| \\ge 1$. The necessary and sufficient condition for stabilizability is:\n$$a^2(1-p) < 1$$\nSolving for the success probability $p$:\n$$1-p < \\frac{1}{a^2} \\implies p > 1 - \\frac{1}{a^2}$$\n\n**Specialization and Counterexample**\nWe are given the specific plant parameters $a=2$ and $b=1$. Since $|a| = 2 > 1$, the open-loop system is unstable. The condition for mean-square stabilizability is:\n$$p > 1 - \\frac{1}{2^2} = 1 - \\frac{1}{4} = \\frac{3}{4}$$\nThe critical success probability $p^{\\star}$ is the infimum of the set of probabilities $p$ for which stabilization is possible. Therefore:\n$$p^{\\star} = \\frac{3}{4} = 0.75$$\nNow we consider the case where $p=0.70$.\nSince $0.70 < 0.75$, our derived condition $p > p^{\\star}$ is not met. This implies that for a success probability of $p=0.70$, the system is not mean-square stabilizable by any static state feedback gain $K$.\nThis provides a counterexample to the notion that deterministic controllability implies stabilizability under stochastic losses. The pair $(a,b)=(2,1)$ is controllable since the controllability matrix $[b]$ has rank $1$. If there were no packet losses ($p=1$), we could choose $K=-2$ to make the closed-loop pole $a+bK = 2-2=0$, achieving deadbeat stability.\nHowever, with $p=0.70$, the minimum possible value of the second-moment amplification factor is $a^2(1-p) = 2^2(1-0.70) = 4(0.30) = 1.2$. Since this minimum factor is greater than $1$, $\\mathbb{E}[x_k^2]$ will diverge for any choice of $K$.\nThe underlying reason is a competition between two effects. During a packet dropout, which occurs with probability $1-p$, the system evolves according to its unstable open-loop dynamics $x_{k+1} = ax_k$. The state's squared magnitude is amplified by $a^2$. During a successful transmission, which occurs with probability $p$, the controller can act to reduce the state's magnitude via $x_{k+1} = (a+bK)x_k$. The most effective control action (with $K^{\\star}=-a/b$) nullifies the state completely. Mean-square stability depends on the weighted average of these two outcomes. The condition $a^2(1-p)<1$ shows that even with a perfect controller that is active with probability $p$, the expected growth during dropouts, $a^2(1-p)$, must itself be less than unity. If packet dropouts are too frequent (i.e., $p$ is too small), the explosive growth during the uncontrolled steps overwhelms any stabilizing action taken during the controlled steps.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "2727000"}, {"introduction": "Having established the concept of mean-square stability, we now explore a more subtle but equally important topic: the difference between various forms of stochastic stability. This problem [@problem_id:2726974] challenges you to distinguish between sample-path stability, where typical trajectories converge, and second-moment stability, which concerns the behavior of the system's average energy. You will discover that rare but large disturbances from packet losses can cause the system's expected energy to grow infinitely even while almost every individual trajectory converges to zero, a critical insight for designing robust and predictable systems.", "problem": "Consider a scalar Networked Control System (NCS) in discrete time with an actuator channel subject to independent and identically distributed (i.i.d.) packet losses. The plant is sampled and controlled at each integer time step $k \\in \\mathbb{Z}_{\\ge 0}$. When the control packet arrives successfully, the closed-loop update is a stable contraction; when it is lost, the plant evolves in open loop and expands. This results in the multiplicative stochastic recursion\n$$\nx_{k+1} = g_k \\, x_k,\n$$\nwhere $\\{g_k\\}_{k \\ge 0}$ are i.i.d. random variables taking values $g_c \\in (0,1)$ with probability $1-p$ (successful control update) and $g_o \\in (1,\\infty)$ with probability $p$ (packet dropout), for a fixed dropout probability $p \\in [0,1]$. There is no exogenous noise.\n\nTwo notions of stability are of interest:\n- Sample-path stability, understood as almost sure exponential convergence to zero, meaning that $\\lim_{k \\to \\infty} |x_k|^{1/k} = \\rho$ exists almost surely with $\\rho < 1$ (equivalently, the top Lyapunov exponent is negative).\n- Second-moment boundedness, understood as boundedness of the second moment over time, meaning $\\sup_{k \\ge 0} \\mathbb{E}[x_k^2] < \\infty$, given $x_0$ deterministic and finite.\n\nStarting from first principles of probability for products of i.i.d. random variables, do the following:\n1. Derive a condition on $p$, $g_c$, and $g_o$ that is necessary and sufficient for sample-path stability (almost sure exponential stability).\n2. Derive a condition on $p$, $g_c$, and $g_o$ that is necessary and sufficient for second-moment boundedness.\n3. Specialize to $g_c = \\tfrac{1}{2}$ and $g_o = 3$. Compute the two critical dropout probabilities $p_{\\mathrm{as}}$ and $p_{\\mathrm{ms}}$ that delimit, respectively, the largest $p$ values for which sample-path stability and second-moment boundedness hold.\n4. Explain why there exists a nonempty interval of dropout probabilities $p$ for which sample-path stability holds while second-moment boundedness fails, and exhibit one specific value of $p$ in that interval.\n\nYour final reported quantity must be the exact symbolic expression for the difference\n$$\n\\Delta \\equiv p_{\\mathrm{as}} - p_{\\mathrm{ms}}\n$$\nfor the case $g_c = \\tfrac{1}{2}$ and $g_o = 3$. Do not approximate; provide the exact analytic expression using natural logarithms. No units are required for the final answer.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of stochastic control theory, addressing the fundamental distinction between sample-path stability and moment stability for systems with multiplicative noise. We shall proceed with a formal derivation.\n\nThe system dynamics are given by the stochastic recursion $x_{k+1} = g_k x_k$, where the initial state $x_0$ is a finite deterministic value. By recursive substitution, the state at time $k$ is given by\n$$\nx_k = \\left( \\prod_{i=0}^{k-1} g_i \\right) x_0.\n$$\nThe random variables $\\{g_k\\}_{k \\ge 0}$ are independent and identically distributed (i.i.d.), taking the value $g_c \\in (0,1)$ with probability $1-p$ and $g_o \\in (1,\\infty)$ with probability $p$. Since $g_c > 0$ and $g_o > 0$, we have $|g_k| = g_k$ for all $k$.\n\n1. Derivation of the condition for sample-path stability:\nSample-path stability is defined as $\\lim_{k \\to \\infty} |x_k|^{1/k} < 1$ almost surely. Let us examine this limit:\n$$\n|x_k|^{1/k} = \\left| \\left( \\prod_{i=0}^{k-1} g_i \\right) x_0 \\right|^{1/k} = |x_0|^{1/k} \\left( \\prod_{i=0}^{k-1} g_i \\right)^{1/k}.\n$$\nAs $k \\to \\infty$, $|x_0|^{1/k} \\to 1$ since $x_0$ is a finite non-zero constant. Thus, the stability is determined by the asymptotic behavior of the geometric mean of the gains. To analyze this, we consider its logarithm:\n$$\n\\ln\\left( \\left( \\prod_{i=0}^{k-1} g_i \\right)^{1/k} \\right) = \\frac{1}{k} \\sum_{i=0}^{k-1} \\ln(g_i).\n$$\nThe terms $\\ln(g_i)$ are i.i.d. random variables. By the Strong Law of Large Numbers, the sample average converges almost surely to the true mean as $k \\to \\infty$:\n$$\n\\lim_{k \\to \\infty} \\frac{1}{k} \\sum_{i=0}^{k-1} \\ln(g_i) = \\mathbb{E}[\\ln(g)] \\quad \\text{a.s.}\n$$\nThis limit is the top Lyapunov exponent of the system. For the system to be almost surely exponentially stable, we require that $\\lim_{k \\to \\infty} |x_k|^{1/k} < 1$, which is equivalent to its logarithm being negative. Thus, the necessary and sufficient condition is $\\mathbb{E}[\\ln(g)] < 0$.\n\nThe expectation is computed over the distribution of $g$:\n$$\n\\mathbb{E}[\\ln(g)] = (1-p) \\ln(g_c) + p \\ln(g_o).\n$$\nTherefore, the condition for sample-path stability is $(1-p) \\ln(g_c) + p \\ln(g_o) < 0$.\n\n2. Derivation of the condition for second-moment boundedness:\nSecond-moment boundedness requires that $\\sup_{k \\ge 0} \\mathbb{E}[x_k^2] < \\infty$. Let us derive the dynamics of the second moment, $\\mathbb{E}[x_k^2]$.\n$$\nx_{k+1}^2 = (g_k x_k)^2 = g_k^2 x_k^2.\n$$\nTaking the expectation of both sides, we get:\n$$\n\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[g_k^2 x_k^2].\n$$\nBecause $g_k$ is independent of $x_k$ (which is a function of $g_0, g_1, \\dots, g_{k-1}$), we can separate the expectations:\n$$\n\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[g_k^2] \\mathbb{E}[x_k^2].\n$$\nThis is a deterministic linear recurrence relation for the sequence of second moments $m_k \\equiv \\mathbb{E}[x_k^2]$. The solution is\n$$\nm_k = (\\mathbb{E}[g^2])^k m_0 = (\\mathbb{E}[g^2])^k x_0^2,\n$$\nwhere $\\mathbb{E}[g^2]$ is the second moment of the generic gain $g$. For the sequence $\\{m_k\\}_{k \\ge 0}$ to remain bounded for any finite initial condition $x_0 \\neq 0$, it is necessary and sufficient that the multiplier be less than or equal to one, i.e., $\\mathbb{E}[g^2] \\le 1$.\n\nThe expectation is computed over the distribution of $g$:\n$$\n\\mathbb{E}[g^2] = (1-p) g_c^2 + p g_o^2.\n$$\nTherefore, the condition for second-moment boundedness is $(1-p) g_c^2 + p g_o^2 \\le 1$.\n\n3. Computation of critical probabilities for $g_c = \\frac{1}{2}$ and $g_o = 3$:\nThe critical probability for sample-path stability, $p_{\\mathrm{as}}$, is the supremum of values of $p$ for which stability holds. This corresponds to the boundary case where the Lyapunov exponent is zero:\n$$\n(1-p_{\\mathrm{as}}) \\ln\\left(\\frac{1}{2}\\right) + p_{\\mathrm{as}} \\ln(3) = 0\n$$\n$$\n-(1-p_{\\mathrm{as}}) \\ln(2) + p_{\\mathrm{as}} \\ln(3) = 0\n$$\n$$\np_{\\mathrm{as}} (\\ln(3) + \\ln(2)) = \\ln(2)\n$$\n$$\np_{\\mathrm{as}} \\ln(6) = \\ln(2) \\implies p_{\\mathrm{as}} = \\frac{\\ln(2)}{\\ln(6)}.\n$$\nThe critical probability for second-moment boundedness, $p_{\\mathrm{ms}}$, corresponds to the boundary case where the second moment growth factor is one:\n$$\n(1-p_{\\mathrm{ms}}) \\left(\\frac{1}{2}\\right)^2 + p_{\\mathrm{ms}} (3)^2 = 1\n$$\n$$\n(1-p_{\\mathrm{ms}}) \\frac{1}{4} + 9p_{\\mathrm{ms}} = 1\n$$\n$$\n\\frac{1}{4} - \\frac{1}{4}p_{\\mathrm{ms}} + 9p_{\\mathrm{ms}} = 1\n$$\n$$\n\\left(9 - \\frac{1}{4}\\right) p_{\\mathrm{ms}} = 1 - \\frac{1}{4}\n$$\n$$\n\\frac{35}{4} p_{\\mathrm{ms}} = \\frac{3}{4} \\implies p_{\\mathrm{ms}} = \\frac{3}{35}.\n$$\n\n4. Explanation of the stability gap:\nThe existence of a non-empty interval $(p_{\\mathrm{ms}}, p_{\\mathrm{as}})$ where sample-path stability holds but second-moment boundedness fails is a consequence of Jensen's inequality. The condition for sample-path stability pertains to the geometric mean of the gains, while the condition for second-moment stability pertains to the arithmetic mean of the squared gains.\nLet $Z = \\ln(g^2) = 2 \\ln(g)$. The function $\\phi(z) = \\exp(z)$ is strictly convex. By Jensen's inequality:\n$$\n\\mathbb{E}[g^2] = \\mathbb{E}[\\exp(Z)] > \\exp(\\mathbb{E}[Z]) = \\exp(\\mathbb{E}[2 \\ln(g)]) = (\\exp(\\mathbb{E}[\\ln(g)]))^2.\n$$\nThe inequality is strict because the random variable $g$ is not deterministic.\nIf a system possesses second-moment stability, then $1 \\ge \\mathbb{E}[g^2]$. From the inequality, this implies $1 > (\\exp(\\mathbb{E}[\\ln(g)]))^2$, which means $1 > \\exp(\\mathbb{E}[\\ln(g)])$, and taking logarithms gives $0 > \\mathbb{E}[\\ln(g)]$. Thus, second-moment stability implies sample-path stability.\nHowever, the converse is not true. The strict inequality allows for a regime where $\\mathbb{E}[\\ln(g)] < 0$ (so $\\exp(\\mathbb{E}[\\ln(g)]) < 1$) while $\\mathbb{E}[g^2] > 1$. This occurs for $p \\in (p_{\\mathrm{ms}}, p_{\\mathrm{as}})$. For the given values, this interval is $(\\frac{3}{35}, \\frac{\\ln(2)}{\\ln(6)})$. This interval is non-empty, as $p_{\\mathrm{ms}} \\approx 0.0857$ and $p_{\\mathrm{as}} \\approx 0.3869$.\nPhysically, this means that while the state trajectory $x_k$ converges to zero almost surely, rare but large increases caused by packet dropouts (multiplication by $g_o$) are heavily weighted by the squaring operation in the second moment, causing $\\mathbb{E}[x_k^2]$ to diverge to infinity.\nA specific value of $p$ in this interval is, for instance, $p = \\frac{1}{5} = 0.2$. This value satisifes $\\frac{3}{35} < \\frac{1}{5} < \\frac{\\ln(2)}{\\ln(6)}$.\n\nFinally, we compute the required difference $\\Delta \\equiv p_{\\mathrm{as}} - p_{\\mathrm{ms}}$:\n$$\n\\Delta = \\frac{\\ln(2)}{\\ln(6)} - \\frac{3}{35}.\n$$\nThis is the final symbolic expression.", "answer": "$$\\boxed{\\frac{\\ln(2)}{\\ln(6)} - \\frac{3}{35}}$$", "id": "2726974"}, {"introduction": "Real-world networks suffer from more than just packet loss; variable delays, or jitter, also degrade control performance. This final practice moves from analytical derivations to a powerful computational framework for analyzing systems with multiple concurrent impairments. By modeling the system as a jump linear system and deriving a quadratic stability condition, you will learn to formulate the stability problem as a Linear Matrix Inequality (LMI) [@problem_id:2726946]. This hands-on coding exercise will equip you with a modern method to precisely compute the stability boundaries for both jitter and dropout probabilities using convex optimization techniques.", "problem": "Consider a discrete-time Networked Control System (NCS) with state dynamics given by the linear time-invariant plant\n$$\nx_{k+1} = A x_k + B u_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$, $u_k \\in \\mathbb{R}^m$, $A \\in \\mathbb{R}^{n \\times n}$, and $B \\in \\mathbb{R}^{n \\times m}$. A static state-feedback controller computes $u_k^{\\mathrm{cmd}} = K x_k$ with a given gain $K \\in \\mathbb{R}^{m \\times n}$. Due to the communication network, the control input applied to the plant at time step $k$, denoted $u_k^{\\mathrm{app}}$, is affected by both actuation jitter and packet dropout as follows:\n- With probability $r \\in [0,1]$ (jitter), the input applied is one-step delayed, i.e., $u_k^{\\mathrm{app}} = K x_{k-1}$.\n- With probability $q \\in [0,1]$ (dropout), the input applied is zero, i.e., $u_k^{\\mathrm{app}} = 0$.\n- Otherwise, with probability $1 - r - q$, the input is applied immediately, i.e., $u_k^{\\mathrm{app}} = K x_k$.\n\nAssume the events are independent and identically distributed over time and mutually exclusive at each step, and assume $r \\ge 0$, $q \\ge 0$, and $r + q \\le 1$. Introduce the augmented state $s_k = \\begin{bmatrix} x_k^\\top & x_{k-1}^\\top \\end{bmatrix}^\\top \\in \\mathbb{R}^{2n}$ to write the closed-loop dynamics as a random jump linear system with three modes, corresponding to the three network outcomes defined above.\n\nYour task is to determine, for each given $(A,B,K)$, the maximum allowable jitter probability $r_{\\max}$ when there are no dropouts (i.e., $q=0$), and the maximum allowable dropout probability $q_{\\max}$ when there is no jitter (i.e., $r=0$), such that the closed-loop NCS is quadratically mean-square stable. Specifically, define quadratic mean-square stability as the existence of a positive definite matrix $P \\in \\mathbb{R}^{2n \\times 2n}$ for which the expected one-step decrease condition of a quadratic Lyapunov function holds strictly for all nonzero states. Your program must solve, for each case, the largest $r \\in [0,1]$ with $q=0$ and the largest $q \\in [0,1]$ with $r=0$ that satisfy a correct Linear Matrix Inequality (LMI) implied by this quadratic stability condition, derived from first principles starting with the fundamental definitions of discrete-time Lyapunov stability and the law of total expectation applied to the jump system.\n\nUse the following test suite of plants and feedback gains:\n- Test Case 1 (happy path):\n  - $A = \\begin{bmatrix} 1.05 & 0.1 \\\\ 0 & 0.98 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$,\n    $K = \\begin{bmatrix} -1.2 & -0.4 \\end{bmatrix}$.\n- Test Case 2 (near-boundary dynamics):\n  - $A = \\begin{bmatrix} 1.2 & 0.15 \\\\ 0 & 1.05 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix}$,\n    $K = \\begin{bmatrix} -2.0 & -0.8 \\end{bmatrix}$.\n- Test Case 3 (scalar edge case):\n  - $A = \\begin{bmatrix} 1.1 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 1.0 \\end{bmatrix}$,\n    $K = \\begin{bmatrix} -0.5 \\end{bmatrix}$.\n\nOutput specification:\n- For each test case, compute $r_{\\max}$ with $q=0$ and $q_{\\max}$ with $r=0$.\n- Report each value as a floating-point number rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n  $[r_{\\max}^{(1)}, q_{\\max}^{(1)}, r_{\\max}^{(2)}, q_{\\max}^{(2)}, r_{\\max}^{(3)}, q_{\\max}^{(3)}]$,\n  where the superscript indicates the test case number.\n\nNotes:\n- You must treat the problem in purely mathematical terms. Do not assume any physical units.\n- Angles are not involved.\n- The final numerical outputs must be floats.", "solution": "The problem requires an analysis of the mean-square stability of a discrete-time Networked Control System (NCS) subject to random control input jitter and packet dropouts. The analysis must yield the maximum tolerable probabilities for each of these network-induced impairments individually.\n\nFirst, the problem statement is validated. The givens are a linear time-invariant plant model, a state-feedback control law, and a probabilistic model for network effects. The task is to find stability boundaries using a specific, well-defined criterion. The problem is scientifically grounded in the theory of jump linear systems, a standard framework for modeling systems with random parameter variations. It is well-posed, objective, and contains all necessary information to proceed. All terms are standard in control theory, and the provided numerical data are consistent. The problem is deemed valid.\n\nThe solution proceeds from first principles as follows.\n\nThe plant dynamics are given by:\n$$\nx_{k+1} = A x_k + B u_k^{\\mathrm{app}}\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state, $u_k^{\\mathrm{app}} \\in \\mathbb{R}^m$ is the applied control input, and the matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$ define the plant model. The controller computes a commanded input $u_k^{\\mathrm{cmd}} = K x_k$ using a static gain $K \\in \\mathbb{R}^{m \\times n}$.\n\nThe network introduces uncertainty in the applied input $u_k^{\\mathrm{app}}$. Let $\\theta_k$ be a random variable representing the network state at time step $k$. The problem describes three mutually exclusive outcomes for $u_k^{\\mathrm{app}}$:\n$1$. **Mode $1$ (Normal)**: $u_k^{\\mathrm{app}} = K x_k$. This occurs with probability $p_1 = 1-r-q$.\n$2$. **Mode $2$ (Jitter)**: $u_k^{\\mathrm{app}} = K x_{k-1}$. This occurs with probability $p_2 = r$.\n$3$. **Mode $3$ (Dropout)**: $u_k^{\\mathrm{app}} = 0$. This occurs with probability $p_3 = q$.\n\nSince the input at time $k$ can depend on the state at time $k-1$, the system has memory. To formulate a Markovian state model, we define an augmented state vector $s_k \\in \\mathbb{R}^{2n}$ as prescribed:\n$$\ns_k = \\begin{bmatrix} x_k \\\\ x_{k-1} \\end{bmatrix}\n$$\nThe dynamics of this augmented state can be written as a jump linear system, $s_{k+1} = \\mathcal{A}_{\\theta_k} s_k$, where $\\mathcal{A}_{\\theta_k}$ is one of three system matrices corresponding to the network modes.\n\nFor each mode, we derive the corresponding matrix $\\mathcal{A}_i$:\n- **Mode $1$ (Normal, $\\theta_k=1$):**\n  $x_{k+1} = A x_k + B(K x_k) = (A+BK)x_k$. The augmented dynamics are:\n  $$\n  s_{k+1} = \\begin{bmatrix} x_{k+1} \\\\ x_k \\end{bmatrix} = \\begin{bmatrix} (A+BK)x_k \\\\ I_n x_k \\end{bmatrix} = \\begin{bmatrix} A+BK & 0_{n \\times n} \\\\ I_n & 0_{n \\times n} \\end{bmatrix} \\begin{bmatrix} x_k \\\\ x_{k-1} \\end{bmatrix} = \\mathcal{A}_1 s_k\n  $$\n- **Mode $2$ (Jitter, $\\theta_k=2$):**\n  $x_{k+1} = A x_k + B(K x_{k-1})$. The augmented dynamics are:\n  $$\n  s_{k+1} = \\begin{bmatrix} x_{k+1} \\\\ x_k \\end{bmatrix} = \\begin{bmatrix} A x_k + BK x_{k-1} \\\\ I_n x_k \\end{bmatrix} = \\begin{bmatrix} A & BK \\\\ I_n & 0_{n \\times n} \\end{bmatrix} \\begin{bmatrix} x_k \\\\ x_{k-1} \\end{bmatrix} = \\mathcal{A}_2 s_k\n  $$\n- **Mode $3$ (Dropout, $\\theta_k=3$):**\n  $x_{k+1} = A x_k + B(0) = A x_k$. The augmented dynamics are:\n  $$\n  s_{k+1} = \\begin{bmatrix} x_{k+1} \\\\ x_k \\end{bmatrix} = \\begin{bmatrix} A x_k \\\\ I_n x_k \\end{bmatrix} = \\begin{bmatrix} A & 0_{n \\times n} \\\\ I_n & 0_{n \\times n} \\end{bmatrix} \\begin{bmatrix} x_k \\\\ x_{k-1} \\end{bmatrix} = \\mathcal{A}_3 s_k\n  $$\nHere, $I_n$ is the $n \\times n$ identity matrix and $0_{n \\times n}$ is the $n \\times n$ zero matrix.\n\nThe system is defined to be mean-square stable if $\\lim_{k \\to \\infty} \\mathbb{E}[s_k] = 0$ and $\\lim_{k \\to \\infty} \\mathbb{E}[s_k s_k^\\top] = 0$. The second condition is stricter and implies the first for zero-mean initial conditions. Let $S_k = \\mathbb{E}[s_k s_k^\\top]$ be the second-moment (covariance) matrix. Its dynamics are:\n$$\nS_{k+1} = \\mathbb{E}[s_{k+1} s_{k+1}^\\top] = \\mathbb{E}[\\mathcal{A}_{\\theta_k} s_k s_k^\\top \\mathcal{A}_{\\theta_k}^\\top] = \\sum_{i=1}^3 p_i \\mathcal{A}_i S_k \\mathcal{A}_i^\\top\n$$\nThis is a linear recurrence for the covariance matrix. The system is mean-square stable if and only if this recurrence converges to zero for any initial covariance $S_0$. This is equivalent to the condition that the spectral radius of the linear operator $\\mathcal{L}(S) = \\sum p_i \\mathcal{A}_i S \\mathcal{A}_i^\\top$ is strictly less than 1.\nTo compute this spectral radius, we can represent the operator as a large matrix acting on the vectorized covariance, $\\mathrm{vec}(S)$. Using the identity $\\mathrm{vec}(CXD) = (D^\\top \\otimes C)\\mathrm{vec}(X)$, the vectorized dynamics are:\n$$\n\\mathrm{vec}(S_{k+1}) = \\left( \\sum_{i=1}^3 p_i (\\mathcal{A}_i \\otimes \\mathcal{A}_i) \\right) \\mathrm{vec}(S_k)\n$$\nLet $\\mathbb{L} = \\sum_{i=1}^3 p_i (\\mathcal{A}_i \\otimes \\mathcal{A}_i)$. The condition for mean-square stability is $\\rho(\\mathbb{L})  1$.\n\nWe are tasked with finding the maximum probabilities $r_{\\max}$ (with $q=0$) and $q_{\\max}$ (with $r=0$). This can be achieved by a bisection search over the interval $[0,1]$ for each parameter.\n\n**Case 1: Jitter analysis ($r_{\\max}$ with $q=0$)**\nThe probabilities are $p_1 = 1-r$, $p_2 = r$, $p_3 = 0$. The vectorized operator matrix is:\n$$\n\\mathbb{L}_r = (1-r)(\\mathcal{A}_1 \\otimes \\mathcal{A}_1) + r(\\mathcal{A}_2 \\otimes \\mathcal{A}_2)\n$$\nWe must find $r_{\\max} = \\sup\\{ r \\in [0,1] \\mid \\rho(\\mathbb{L}_r)  1 \\}$. A bisection search is performed for $r$ on $[0,1]$. For each candidate $r$, we compute $\\mathbb{L}_r$, find its eigenvalues, and check if their maximum magnitude is less than $1$.\n\n**Case 2: Dropout analysis ($q_{\\max}$ with $r=0$)**\nThe probabilities are $p_1 = 1-q$, $p_2 = 0$, $p_3 = q$. The vectorized operator matrix is:\n$$\n\\mathbb{L}_q = (1-q)(\\mathcal{A}_1 \\otimes \\mathcal{A}_1) + q(\\mathcal{A}_3 \\otimes \\mathcal{A}_3)\n$$\nSimilarly, we find $q_{\\max} = \\sup\\{ q \\in [0,1] \\mid \\rho(\\mathbb{L}_q)  1 \\}$ via bisection search.\n\nThe algorithm for both cases is:\n$1$. Check for stability at the nominal point ($r=0$ or $q=0$). This requires $\\rho(\\mathcal{A}_1 \\otimes \\mathcal{A}_1)  1$, which simplifies to $\\rho(\\mathcal{A}_1)^2  1$, or $\\rho(\\mathcal{A}_1)  1$. If the nominal system is unstable, the maximum probability is $0$. The eigenvalues of $\\mathcal{A}_1$ are the union of the eigenvalues of $A+BK$ and $n$ eigenvalues at zero, so this check reduces to $\\rho(A+BK)  1$.\n$2$. If the nominal system is stable, perform a bisection search. Set a search interval $[low, high] = [0, 1]$.\n$3$. In each iteration, take the midpoint `mid = (low + high) / 2`. Construct the corresponding matrix $\\mathbb{L}_{mid}$ and compute its spectral radius $\\rho(\\mathbb{L}_{mid})$.\n$4$. If $\\rho(\\mathbb{L}_{mid})  1$, the system is stable for this probability. We can potentially tolerate a higher value, so we set $low = mid$.\n$5$. If $\\rho(\\mathbb{L}_{mid}) \\ge 1$, the system is unstable. The probability must be reduced, so we set $high = mid$.\n$6$. After a sufficient number of iterations (e.g., $100$), the value of $low$ is a highly accurate approximation of the maximum allowable probability.\nThis procedure is implemented for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the NCS stability problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        (np.array([[1.05, 0.1], [0, 0.98]]),\n         np.array([[0.1], [0.05]]),\n         np.array([[-1.2, -0.4]])),\n        # Test Case 2\n        (np.array([[1.2, 0.15], [0, 1.05]]),\n         np.array([[0.2], [0.1]]),\n         np.array([[-2.0, -0.8]])),\n        # Test Case 3 (scalar)\n        (np.array([[1.1]]),\n         np.array([[1.0]]),\n         np.array([[-0.5]]))\n    ]\n\n    results = []\n    for A, B, K in test_cases:\n        r_max, q_max = analyze_stability(A, B, K)\n        results.extend([r_max, q_max])\n\n    # Format the final output as a comma-separated list of floats rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef analyze_stability(A, B, K):\n    \"\"\"\n    Analyzes the mean-square stability to find r_max and q_max for a given system.\n    \n    Args:\n        A (np.ndarray): State matrix.\n        B (np.ndarray): Input matrix.\n        K (np.ndarray): Feedback gain matrix.\n        \n    Returns:\n        tuple[float, float]: A tuple containing (r_max, q_max).\n    \"\"\"\n    n = A.shape[0]\n    I_n = np.eye(n)\n    Z_nn = np.zeros((n, n))\n\n    # --- Construct augmented system matrices ---\n    # Mode 1: No delay, u_k = K*x_k\n    A_cl = A + B @ K\n    A1_aug = np.block([[A_cl, Z_nn], [I_n, Z_nn]])\n\n    # Mode 2: Jitter (one-step delay), u_k = K*x_{k-1}\n    A2_aug = np.block([[A, B @ K], [I_n, Z_nn]])\n\n    # Mode 3: Packet dropout, u_k = 0\n    A3_aug = np.block([[A, Z_nn], [I_n, Z_nn]])\n\n    # --- Stability check for the nominal system (r=0, q=0) ---\n    # Stability condition is rho(A1_aug)  1.\n    # eigvals(A1_aug) are eigvals(A+BK) union with n zeros.\n    nominal_eigs = np.linalg.eigvals(A_cl)\n    if np.max(np.abs(nominal_eigs)) >= 1.0:\n        return 0.0, 0.0\n\n    # --- Construct vectorized operator matrices using Kronecker product for MSS ---\n    # The MSS stability condition is rho(L)  1 for the operator L(S) = sum(p_i * A_i * S * A_i^T).\n    # Its matrix representation using vec(S) is L_vec = sum(p_i * kron(A_i, A_i)).\n    M1 = np.kron(A1_aug, A1_aug)\n    M2 = np.kron(A2_aug, A2_aug)\n    M3 = np.kron(A3_aug, A3_aug)\n\n    # --- Bisection search for r_max (with q=0) ---\n    low_r, high_r = 0.0, 1.0\n    for _ in range(100):  # 100 iterations provide ample precision.\n        mid_r = (low_r + high_r) / 2\n        if mid_r == low_r or mid_r == high_r:\n            break\n        L_r = (1 - mid_r) * M1 + mid_r * M2\n        rho = np.max(np.abs(np.linalg.eigvals(L_r)))\n        if rho  1.0:\n            low_r = mid_r  # Stable, try for a larger r\n        else:\n            high_r = mid_r # Unstable, must reduce r\n    r_max = low_r\n\n    # --- Bisection search for q_max (with r=0) ---\n    low_q, high_q = 0.0, 1.0\n    for _ in range(100):\n        mid_q = (low_q + high_q) / 2\n        if mid_q == low_q or mid_q == high_q:\n            break\n        L_q = (1 - mid_q) * M1 + mid_q * M3\n        rho = np.max(np.abs(np.linalg.eigvals(L_q)))\n        if rho  1.0:\n            low_q = mid_q  # Stable, try for a larger q\n        else:\n            high_q = mid_q # Unstable, must reduce q\n    q_max = low_q\n\n    return r_max, q_max\n\nsolve()\n```", "id": "2726946"}]}