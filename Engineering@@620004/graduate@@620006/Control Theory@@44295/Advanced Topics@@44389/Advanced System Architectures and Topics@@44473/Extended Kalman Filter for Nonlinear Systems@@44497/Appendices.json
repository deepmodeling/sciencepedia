{"hands_on_practices": [{"introduction": "Many physical systems are naturally described by continuous-time differential equations, but digital filters like the EKF operate in discrete time steps. This exercise [@problem_id:2705962] bridges that gap by focusing on the crucial first step of the EKF prediction cycle: discretizing the system dynamics. You will practice linearizing the nonlinear dynamics around a nominal state to find the Jacobian matrix $F$ and then apply the forward-Euler method to derive the discrete-time state transition matrix $F_d$ and process noise covariance $Q_d$, which are fundamental inputs for propagating the state estimate forward in time.", "problem": "Consider the continuous-time nonlinear system intended for use with the Extended Kalman Filter (EKF) for nonlinear systems given by\n$$\n\\dot{x}(t) = f(x(t)) + L\\,w(t), \\quad f(x) \\triangleq \\begin{bmatrix} x_2 \\\\ -\\omega^{2}\\,\\sin(x_1) \\end{bmatrix},\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$ with $x_1$ an angle in radians, $\\omega \\in \\mathbb{R}_{>0}$, $L \\in \\mathbb{R}^{2\\times 1}$, and $w(t)$ is zero-mean white Gaussian process noise with continuous-time spectral density $Q_c \\in \\mathbb{R}$ (a scalar). Assume $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and $\\omega = 1$. The EKF uses the linearization of the drift $f(x)$ about the mean state $\\mu \\in \\mathbb{R}^{2}$ through the Jacobian $F(\\mu) \\triangleq \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=\\mu}$. For a small sampling interval $\\Delta t$, adopt a first-order forward-Euler discretization of the linearized dynamics and a first-order approximation of the discrete process noise covariance under a zero-order hold on the diffusion.\n\nLet the numerical values be $\\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$, $Q_c = q = 1/5$, and $\\Delta t = 0.05$. Compute $F(\\mu)$, then obtain the first-order discrete-time approximations of the state transition matrix and process noise covariance, denoted $F_d$ and $Q_d$, respectively, consistent with the above modeling assumptions. Finally, evaluate the scalar\n$$\ns \\triangleq \\operatorname{tr}(F_d) + [Q_d]_{22}.\n$$\nProvide your final result for $s$ as an exact value with no rounding and no units. Angles are in radians. Do not report any intermediate matrices; only report the final value of $s$.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for its resolution. It is a standard exercise in the discretization of a continuous-time nonlinear system for the application of an Extended Kalman Filter. We proceed with the solution.\n\nThe problem requires the computation of a scalar value $s \\triangleq \\operatorname{tr}(F_d) + [Q_d]_{22}$, where $F_d$ and $Q_d$ are the discrete-time state transition matrix and process noise covariance, respectively. These are derived from a continuous-time nonlinear system model.\n\nThe continuous-time dynamics are given by $\\dot{x}(t) = f(x(t)) + L\\,w(t)$, with the drift function defined as:\n$$\nf(x) = \\begin{bmatrix} x_2 \\\\ -\\omega^{2}\\,\\sin(x_1) \\end{bmatrix}\n$$\nThe problem specifies the values $\\omega=1$ and $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Thus, the drift function simplifies to:\n$$\nf(x) = \\begin{bmatrix} x_2 \\\\ -\\sin(x_1) \\end{bmatrix}\n$$\n\nThe first step is to linearize this drift function about the provided mean state $\\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$. This is done by computing the Jacobian matrix $F(\\mu) \\triangleq \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=\\mu}$. The partial derivatives of the components of $f(x)$ are:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(x_2) = 0\n$$\n$$\n\\frac{\\partial f_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(x_2) = 1\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(-\\sin(x_1)) = -\\cos(x_1)\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(-\\sin(x_1)) = 0\n$$\nAssembling these into the Jacobian matrix gives:\n$$\nF(x) = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} 0 & 1 \\\\ -\\cos(x_1) & 0 \\end{bmatrix}\n$$\nWe evaluate this matrix at the point $x = \\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$:\n$$\nF(\\mu) = \\begin{bmatrix} 0 & 1 \\\\ -\\cos(\\pi/6) & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -\\frac{\\sqrt{3}}{2} & 0 \\end{bmatrix}\n$$\n\nThe second step is to obtain the discrete-time state transition matrix, $F_d$. The problem specifies a first-order forward-Euler discretization of the linearized dynamics over a sampling interval $\\Delta t = 0.05$. The formula for this is $F_d \\approx I + \\Delta t F(\\mu)$, where $I$ is the $2 \\times 2$ identity matrix.\nSubstituting the values $\\Delta t = 0.05 = \\frac{1}{20}$ and the computed $F(\\mu)$:\n$$\nF_d = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\frac{1}{20} \\begin{bmatrix} 0 & 1 \\\\ -\\frac{\\sqrt{3}}{2} & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & \\frac{1}{20} \\\\ -\\frac{\\sqrt{3}}{40} & 1 \\end{bmatrix}\n$$\n\nThe third step is to compute the discrete-time process noise covariance matrix, $Q_d$. The problem asks for a first-order approximation consistent with a zero-order hold on the process noise diffusion. For a continuous-time system with process noise input matrix $L$ and spectral density $Q_c$, the simplest and most common first-order approximation for small $\\Delta t$ is:\n$$\nQ_d \\approx (L Q_c L^T) \\Delta t\n$$\nWe are given $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $Q_c = q = 1/5$, and $\\Delta t = 0.05 = \\frac{1}{20}$.\nFirst, we compute the continuous-time covariance matrix term $L Q_c L^T$:\n$$\nL Q_c L^T = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\left(\\frac{1}{5}\\right) \\begin{bmatrix} 0 & 1 \\end{bmatrix} = \\frac{1}{5} \\begin{bmatrix} 0 \\cdot 0 & 0 \\cdot 1 \\\\ 1 \\cdot 0 & 1 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{5} \\end{bmatrix}\n$$\nNow, we multiply by $\\Delta t$ to get $Q_d$:\n$$\nQ_d = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{5} \\end{bmatrix} \\cdot \\left(\\frac{1}{20}\\right) = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{100} \\end{bmatrix}\n$$\n\nThe final step is to compute the scalar $s = \\operatorname{tr}(F_d) + [Q_d]_{22}$.\nThe trace of $F_d$ is the sum of its diagonal elements:\n$$\n\\operatorname{tr}(F_d) = 1 + 1 = 2\n$$\nThe element at the second row and second column of $Q_d$, denoted $[Q_d]_{22}$, is:\n$$\n[Q_d]_{22} = \\frac{1}{100}\n$$\nSumming these two values gives the final result for $s$:\n$$\ns = 2 + \\frac{1}{100} = \\frac{200}{100} + \\frac{1}{100} = \\frac{201}{100}\n$$\nAs a decimal, this is $2.01$. The value is exact.", "answer": "$$\\boxed{\\frac{201}{100}}$$", "id": "2705962"}, {"introduction": "The heart of the Kalman filter is the measurement update, where a prediction is corrected using new information from sensors. The Extended Kalman Filter adapts this process for nonlinear systems by linearizing the measurement model at the point of the predicted state. This practice [@problem_id:2705967] offers a complete, step-by-step walkthrough of this core mechanism for a realistic bearing-only tracking scenario. You will compute the measurement Jacobian $H_k$, the innovation covariance $S_k$, the optimal Kalman gain $K_k$, and the final corrected state and covariance, building a concrete understanding of how the EKF fuses information.", "problem": "Consider a planar bearing-only sensor that measures the bearing angle of a target at state $x_k = \\begin{bmatrix}x_{1,k} & x_{2,k}\\end{bmatrix}^{\\top}$ relative to the sensor colocated at the origin. The measurement model is the nonlinear function $h:\\mathbb{R}^2 \\to \\mathbb{R}$ given by $h(x) = \\arctan\\!\\big(x_2/x_1\\big)$, with angle measured in radians. Assume additive zero-mean Gaussian measurement noise with covariance $R = 0.01$. At time $k$, an Extended Kalman Filter (EKF) performs a measurement update using a single scalar measurement $y_k = 0.3$ radians, a predicted state mean $\\mu_k^{-} = \\begin{bmatrix}1.0 & 0.5\\end{bmatrix}^{\\top}$, and a predicted covariance\n$$\nP_k^{-} = \\begin{bmatrix} 0.1 & 0.02 \\\\ 0.02 & 0.05 \\end{bmatrix}.\n$$\nStarting from the core definition that the EKF linearizes the measurement model about the predicted mean and applies the linear Kalman filter update to the linearized model, carry out the following tasks:\n1) Derive the measurement Jacobian $H_k = \\left.\\dfrac{\\partial h}{\\partial x}\\right|_{x=\\mu_k^{-}}$ from first principles and evaluate it at $\\mu_k^{-}$.\n2) Using first principles for the linearized Bayesian update, compute the innovation covariance $S_k$, the Kalman gain $K_k$, the updated mean $\\mu_k$, and the updated covariance $P_k$. You may use any algebraically equivalent form of the covariance update that preserves positive semidefiniteness.\nExpress all intermediate quantities exactly whenever possible (for example, using rational numbers and elementary constants), and keep angles in radians. As your final answer, report only the first component of the updated mean, i.e., the first entry of $\\mu_k$, as a single closed-form analytic expression without numerical rounding.", "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. It provides all necessary components for executing a single measurement update step of an Extended Kalman Filter. We shall proceed with the derivation and computation from first principles.\n\nThe state vector is $x_k = \\begin{bmatrix} x_{1,k} & x_{2,k} \\end{bmatrix}^{\\top}$. The nonlinear measurement function is given by $h(x) = \\arctan(x_2/x_1)$. To perform the EKF update, we must linearize this function about the predicted state mean, $\\mu_k^{-} = \\begin{bmatrix}1.0 & 0.5\\end{bmatrix}^{\\top}$.\n\n**1) Measurement Jacobian $H_k$**\n\nThe measurement Jacobian $H$ is the matrix of first-order partial derivatives of the measurement function $h(x)$. For a scalar function of a vector $x = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}^{\\top}$, the Jacobian is a row vector.\n\nThe partial derivative of $h(x)$ with respect to $x_1$ is:\n$$\n\\frac{\\partial h}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left( \\arctan\\left(\\frac{x_2}{x_1}\\right) \\right) = \\frac{1}{1 + \\left(\\frac{x_2}{x_1}\\right)^2} \\cdot \\left(-\\frac{x_2}{x_1^2}\\right) = \\frac{-x_2}{x_1^2 + x_2^2}\n$$\nThe partial derivative of $h(x)$ with respect to $x_2$ is:\n$$\n\\frac{\\partial h}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left( \\arctan\\left(\\frac{x_2}{x_1}\\right) \\right) = \\frac{1}{1 + \\left(\\frac{x_2}{x_1}\\right)^2} \\cdot \\left(\\frac{1}{x_1}\\right) = \\frac{x_1}{x_1^2 + x_2^2}\n$$\nThus, the Jacobian is the row vector:\n$$\nH(x) = \\frac{\\partial h}{\\partial x} = \\begin{bmatrix} \\frac{-x_2}{x_1^2 + x_2^2} & \\frac{x_1}{x_1^2 + x_2^2} \\end{bmatrix}\n$$\nWe evaluate this Jacobian at the predicted mean $\\mu_k^{-} = \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix}$. Let $\\mu_k^{-} = \\begin{bmatrix} \\mu_1^{-} & \\mu_2^{-} \\end{bmatrix}^{\\top}$.\nThe denominator is $(\\mu_1^{-})^2 + (\\mu_2^{-})^2 = 1^2 + (\\frac{1}{2})^2 = 1 + \\frac{1}{4} = \\frac{5}{4}$.\nThe components of the Jacobian $H_k = H(\\mu_k^{-})$ are:\n$$\n\\frac{\\partial h}{\\partial x_1}\\bigg|_{x=\\mu_k^{-}} = \\frac{-\\mu_2^{-}}{(\\mu_1^{-})^2 + (\\mu_2^{-})^2} = \\frac{-1/2}{5/4} = -\\frac{1}{2} \\cdot \\frac{4}{5} = -\\frac{2}{5}\n$$\n$$\n\\frac{\\partial h}{\\partial x_2}\\bigg|_{x=\\mu_k^{-}} = \\frac{\\mu_1^{-}}{(\\mu_1^{-})^2 + (\\mu_2^{-})^2} = \\frac{1}{5/4} = \\frac{4}{5}\n$$\nSo, the evaluated measurement Jacobian is:\n$$\nH_k = \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix}\n$$\n\n**2) EKF Measurement Update**\n\nThe EKF measurement update equations for the mean and covariance are:\n$$\n\\mu_k = \\mu_k^{-} + K_k (y_k - h(\\mu_k^{-}))\n$$\n$$\nP_k = (I - K_k H_k) P_k^{-}\n$$\nwhere $K_k$ is the Kalman gain, defined as:\n$$\nK_k = P_k^{-} H_k^{\\top} S_k^{-1}\n$$\nand $S_k$ is the innovation covariance:\n$$\nS_k = H_k P_k^{-} H_k^{\\top} + R\n$$\nWe are given the following quantities, which we express with rational numbers for precision:\n$$\n\\mu_k^{-} = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix}, \\quad P_k^{-} = \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix}, \\quad y_k = \\frac{3}{10}, \\quad R = \\frac{1}{100}\n$$\n\nFirst, we compute the **innovation covariance $S_k$**.\n$$\nH_k P_k^{-} = \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{50}+\\frac{4}{250} & -\\frac{2}{250}+\\frac{4}{100} \\end{bmatrix} = \\begin{bmatrix} -\\frac{10}{250}+\\frac{4}{250} & -\\frac{2}{250}+\\frac{10}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{250} & \\frac{8}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{125} & \\frac{4}{125} \\end{bmatrix}\n$$\n$$\nH_k P_k^{-} H_k^{\\top} = \\begin{bmatrix} -\\frac{3}{125} & \\frac{4}{125} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} \\\\ \\frac{4}{5} \\end{bmatrix} = \\frac{6}{625} + \\frac{16}{625} = \\frac{22}{625}\n$$\n$$\nS_k = \\frac{22}{625} + R = \\frac{22}{625} + \\frac{1}{100} = \\frac{88}{2500} + \\frac{25}{2500} = \\frac{113}{2500}\n$$\n\nNext, we compute the **Kalman gain $K_k$**.\n$$\nP_k^{-} H_k^{\\top} = \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} \\\\ \\frac{4}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{50} + \\frac{4}{250} \\\\ -\\frac{2}{250} + \\frac{4}{100} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{250} \\\\ \\frac{8}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{125} \\\\ \\frac{4}{125} \\end{bmatrix}\n$$\n$$\nK_k = P_k^{-} H_k^{\\top} S_k^{-1} = \\begin{bmatrix} -\\frac{3}{125} \\\\ \\frac{4}{125} \\end{bmatrix} \\left(\\frac{2500}{113}\\right) = \\begin{bmatrix} -\\frac{3 \\cdot 20}{113} \\\\ \\frac{4 \\cdot 20}{113} \\end{bmatrix} = \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix}\n$$\n\nNow, we compute the **updated mean $\\mu_k$**.\nThe innovation (or residual) is $\\tilde{y}_k = y_k - h(\\mu_k^{-})$.\nThe predicted measurement is $h(\\mu_k^{-}) = \\arctan(\\frac{1/2}{1}) = \\arctan(\\frac{1}{2})$.\nThe innovation is $\\tilde{y}_k = \\frac{3}{10} - \\arctan(\\frac{1}{2})$.\n$$\n\\mu_k = \\mu_k^{-} + K_k \\tilde{y}_k = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix} + \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix} \\left(\\frac{3}{10} - \\arctan\\left(\\frac{1}{2}\\right)\\right)\n$$\nThe first component of the updated mean, $\\mu_{1,k}$, is:\n$$\n\\mu_{1,k} = 1 - \\frac{60}{113} \\left(\\frac{3}{10} - \\arctan\\left(\\frac{1}{2}\\right)\\right) = 1 - \\frac{18}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right) = \\frac{113-18}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right)\n$$\n$$\n\\mu_{1,k} = \\frac{95}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right) = \\frac{95 + 60 \\arctan(\\frac{1}{2})}{113}\n$$\nThe second component is $\\mu_{2,k} = \\frac{1}{2} + \\frac{80}{113} (\\frac{3}{10} - \\arctan(\\frac{1}{2})) = \\frac{1}{2} + \\frac{24}{113} - \\frac{80}{113}\\arctan(\\frac{1}{2}) = \\frac{113+48}{226} - \\frac{80}{113}\\arctan(\\frac{1}{2}) = \\frac{161}{226} - \\frac{80}{113}\\arctan(\\frac{1}{2})$.\nThe updated state mean is $\\mu_k = \\begin{bmatrix} \\frac{95 + 60 \\arctan(\\frac{1}{2})}{113} \\\\ \\frac{161 - 160 \\arctan(\\frac{1}{2})}{226} \\end{bmatrix}$.\n\nFinally, we compute the **updated covariance $P_k$**.\n$$\nK_k H_k = \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} (-60)(-\\frac{2}{5}) & (-60)(\\frac{4}{5}) \\\\ (80)(-\\frac{2}{5}) & (80)(\\frac{4}{5}) \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 24 & -48 \\\\ -32 & 64 \\end{bmatrix}\n$$\n$$\nI - K_k H_k = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\frac{1}{113}\\begin{bmatrix} 24 & -48 \\\\ -32 & 64 \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 113-24 & 48 \\\\ 32 & 113-64 \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix}\n$$\n$$\nP_k = (I - K_k H_k) P_k^{-} = \\frac{1}{113} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} = \\frac{1}{11300} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix} \\begin{bmatrix} 10 & 2 \\\\ 2 & 5 \\end{bmatrix}\n$$\n$$\nP_k = \\frac{1}{11300} \\begin{bmatrix} 890+96 & 178+240 \\\\ 320+98 & 64+245 \\end{bmatrix} = \\frac{1}{11300} \\begin{bmatrix} 986 & 418 \\\\ 418 & 309 \\end{bmatrix}\n$$\nThis completes all required computations. The final answer requested is the first component of the updated mean, $\\mu_{1,k}$.", "answer": "$$\n\\boxed{\\frac{95 + 60 \\arctan\\left(\\frac{1}{2}\\right)}{113}}\n$$", "id": "2705967"}, {"introduction": "The EKF is built upon a first-order linear approximation, which is both its strength and its fundamental limitation. Understanding the magnitude of the error introduced by this approximation is key to diagnosing filter performance and knowing when a more advanced filter might be necessary. This exercise [@problem_id:2705986] moves beyond the mechanics of the EKF to a deeper analysis of its underlying assumptions. By using a second-order Taylor expansion, you will derive and quantify the leading error term that the standard EKF neglects, providing critical insight into the filter's accuracy and stability.", "problem": "Consider a nonlinear transformation in the context of the Extended Kalman Filter (EKF), where the state is Gaussian. Let $x \\in \\mathbb{R}^{2}$ be distributed as $x \\sim \\mathcal{N}(\\mu, P)$ with mean $\\mu = [\\,0.1,\\,0.1\\,]^{\\top}$ and covariance $P = \\mathrm{diag}(0.04,\\,0.09)$. Define $f:\\mathbb{R}^{2}\\to \\mathbb{R}^{2}$ componentwise by $f_{1}(x) = \\sin(x_{1})$ and $f_{2}(x) = \\exp(x_{2})$. In an EKF prediction, the first-order mean update uses a linearization of $f$ at $x=\\mu$, thereby neglecting higher-order terms.\n\nUsing a second-order Taylor expansion about $x=\\mu$ and the standard moment identities for a Gaussian random vector (namely, $\\mathbb{E}[x-\\mu] = 0$ and $\\mathbb{E}[(x-\\mu)(x-\\mu)^{\\top}] = P$), derive an expression for the second-order correction to the mean of $f(x)$ in terms of the Hessian of each component $f_{i}$ evaluated at $\\mu$. Then, using only the diagonal entries of each Hessian (i.e., neglect cross-derivative terms), compute the Euclidean norm of the neglected second-order mean correction vector for the given $\\mu$ and $P$.\n\nAll angles are in radians. Round your final numerical answer to four significant figures. Report a single real number with no units.", "solution": "The objective is to find the second-order correction to the mean of $f(x)$ when $x \\sim \\mathcal{N}(\\mu, P)$. The first-order approximation of the mean used in the EKF is $\\mathbb{E}[f(x)] \\approx f(\\mu)$. We seek to improve this approximation using a second-order Taylor expansion.\n\nLet us consider a single component $f_{i}(x)$ of the vector function $f(x)$. The second-order Taylor expansion of $f_{i}(x)$ around the mean $\\mu$ is given by:\n$$ f_{i}(x) \\approx f_{i}(\\mu) + \\nabla f_{i}(\\mu)^{\\top}(x-\\mu) + \\frac{1}{2}(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu) $$\nwhere $\\nabla f_{i}(\\mu)$ is the gradient of $f_{i}$ and $H_{f_{i}}(\\mu)$ is the Hessian matrix of $f_{i}$, both evaluated at $x=\\mu$.\n\nTo find the expected value of $f_{i}(x)$, we take the expectation of the expansion:\n$$ \\mathbb{E}[f_{i}(x)] \\approx \\mathbb{E}\\left[f_{i}(\\mu) + \\nabla f_{i}(\\mu)^{\\top}(x-\\mu) + \\frac{1}{2}(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] $$\nBy the linearity of the expectation operator:\n$$ \\mathbb{E}[f_{i}(x)] \\approx \\mathbb{E}[f_{i}(\\mu)] + \\mathbb{E}[\\nabla f_{i}(\\mu)^{\\top}(x-\\mu)] + \\mathbb{E}\\left[\\frac{1}{2}(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] $$\nSince $f_{i}(\\mu)$, $\\nabla f_{i}(\\mu)$, and $H_{f_{i}}(\\mu)$ are constants evaluated at $\\mu$, we can write:\n$$ \\mathbb{E}[f_{i}(x)] \\approx f_{i}(\\mu) + \\nabla f_{i}(\\mu)^{\\top}\\mathbb{E}[x-\\mu] + \\frac{1}{2}\\mathbb{E}\\left[(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] $$\nUsing the given identity $\\mathbb{E}[x-\\mu] = 0$, the first-order term vanishes:\n$$ \\mathbb{E}[f_{i}(x)] \\approx f_{i}(\\mu) + \\frac{1}{2}\\mathbb{E}\\left[(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] $$\nThe term $(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)$ is a scalar. We can use the trace property, $\\mathrm{Tr}(A) = A$ for a scalar $A$, and the cyclic property of the trace, $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(BCA)$.\n$$ \\mathbb{E}\\left[(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] = \\mathbb{E}\\left[\\mathrm{Tr}\\left((x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right)\\right] = \\mathbb{E}\\left[\\mathrm{Tr}\\left(H_{f_{i}}(\\mu) (x-\\mu) (x-\\mu)^{\\top}\\right)\\right] $$\nMoving the expectation inside the trace operator:\n$$ = \\mathrm{Tr}\\left(\\mathbb{E}\\left[H_{f_{i}}(\\mu) (x-\\mu) (x-\\mu)^{\\top}\\right]\\right) = \\mathrm{Tr}\\left(H_{f_{i}}(\\mu) \\mathbb{E}\\left[(x-\\mu) (x-\\mu)^{\\top}\\right]\\right) $$\nUsing the second identity, $\\mathbb{E}[(x-\\mu)(x-\\mu)^{\\top}] = P$, we have:\n$$ \\mathbb{E}\\left[(x-\\mu)^{\\top} H_{f_{i}}(\\mu) (x-\\mu)\\right] = \\mathrm{Tr}\\left(H_{f_{i}}(\\mu) P\\right) $$\nThus, the second-order approximation of the mean is:\n$$ \\mathbb{E}[f_{i}(x)] \\approx f_{i}(\\mu) + \\frac{1}{2}\\mathrm{Tr}\\left(H_{f_{i}}(\\mu) P\\right) $$\nThe term $\\frac{1}{2}\\mathrm{Tr}\\left(H_{f_{i}}(\\mu) P\\right)$ is the second-order correction to the mean of the $i$-th component. The correction vector, which we denote by $\\delta\\mu_{f}$, is:\n$$ \\delta\\mu_{f} = \\begin{pmatrix} \\frac{1}{2}\\mathrm{Tr}\\left(H_{f_{1}}(\\mu) P\\right) \\\\ \\frac{1}{2}\\mathrm{Tr}\\left(H_{f_{2}}(\\mu) P\\right) \\end{pmatrix} $$\nNow, we compute this for the given functions and parameters.\nThe state is $x=[x_{1}, x_{2}]^{\\top}$. The mean is $\\mu = [\\,0.1,\\,0.1\\,]^{\\top}$. The covariance is $P = \\begin{pmatrix} 0.04 & 0 \\\\ 0 & 0.09 \\end{pmatrix}$.\n\nFor the first component, $f_{1}(x) = \\sin(x_{1})$:\nThe first partial derivatives are $\\frac{\\partial f_{1}}{\\partial x_{1}} = \\cos(x_{1})$ and $\\frac{\\partial f_{1}}{\\partial x_{2}} = 0$.\nThe second partial derivatives are $\\frac{\\partial^{2} f_{1}}{\\partial x_{1}^{2}} = -\\sin(x_{1})$, $\\frac{\\partial^{2} f_{1}}{\\partial x_{2}^{2}} = 0$, and $\\frac{\\partial^{2} f_{1}}{\\partial x_{1} \\partial x_{2}} = 0$.\nThe Hessian matrix for $f_{1}$ is:\n$$ H_{f_{1}}(x) = \\begin{pmatrix} -\\sin(x_{1}) & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nEvaluated at $\\mu=[0.1, 0.1]^{\\top}$:\n$$ H_{f_{1}}(\\mu) = \\begin{pmatrix} -\\sin(0.1) & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n\nFor the second component, $f_{2}(x) = \\exp(x_{2})$:\nThe first partial derivatives are $\\frac{\\partial f_{2}}{\\partial x_{1}} = 0$ and $\\frac{\\partial f_{2}}{\\partial x_{2}} = \\exp(x_{2})$.\nThe second partial derivatives are $\\frac{\\partial^{2} f_{2}}{\\partial x_{1}^{2}} = 0$, $\\frac{\\partial^{2} f_{2}}{\\partial x_{2}^{2}} = \\exp(x_{2})$, and $\\frac{\\partial^{2} f_{2}}{\\partial x_{1} \\partial x_{2}} = 0$.\nThe Hessian matrix for $f_{2}$ is:\n$$ H_{f_{2}}(x) = \\begin{pmatrix} 0 & 0 \\\\ 0 & \\exp(x_{2}) \\end{pmatrix} $$\nEvaluated at $\\mu=[0.1, 0.1]^{\\top}$:\n$$ H_{f_{2}}(\\mu) = \\begin{pmatrix} 0 & 0 \\\\ 0 & \\exp(0.1) \\end{pmatrix} $$\nThe problem states to neglect cross-derivative terms, which is already satisfied as both Hessians are diagonal.\n\nNow we compute the correction for each component.\nFor the first component:\n$$ [\\delta\\mu_{f}]_{1} = \\frac{1}{2}\\mathrm{Tr}(H_{f_{1}}(\\mu) P) = \\frac{1}{2}\\mathrm{Tr}\\left(\\begin{pmatrix} -\\sin(0.1) & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0.04 & 0 \\\\ 0 & 0.09 \\end{pmatrix}\\right) $$\n$$ [\\delta\\mu_{f}]_{1} = \\frac{1}{2}\\mathrm{Tr}\\begin{pmatrix} -0.04\\sin(0.1) & 0 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{2}(-0.04\\sin(0.1)) = -0.02\\sin(0.1) $$\n\nFor the second component:\n$$ [\\delta\\mu_{f}]_{2} = \\frac{1}{2}\\mathrm{Tr}(H_{f_{2}}(\\mu) P) = \\frac{1}{2}\\mathrm{Tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 0 & \\exp(0.1) \\end{pmatrix} \\begin{pmatrix} 0.04 & 0 \\\\ 0 & 0.09 \\end{pmatrix}\\right) $$\n$$ [\\delta\\mu_{f}]_{2} = \\frac{1}{2}\\mathrm{Tr}\\begin{pmatrix} 0 & 0 \\\\ 0 & 0.09\\exp(0.1) \\end{pmatrix} = \\frac{1}{2}(0.09\\exp(0.1)) = 0.045\\exp(0.1) $$\nThe second-order mean correction vector is:\n$$ \\delta\\mu_{f} = \\begin{pmatrix} -0.02\\sin(0.1) \\\\ 0.045\\exp(0.1) \\end{pmatrix} $$\nThe final step is to compute the Euclidean norm of this vector, $||\\delta\\mu_{f}||_{2}$:\n$$ ||\\delta\\mu_{f}||_{2} = \\sqrt{\\left(-0.02\\sin(0.1)\\right)^{2} + \\left(0.045\\exp(0.1)\\right)^{2}} $$\n$$ ||\\delta\\mu_{f}||_{2} = \\sqrt{0.0004\\sin^{2}(0.1) + 0.002025\\exp(0.2)} $$\nNow, we compute the numerical value.\n$\\sin(0.1) \\approx 0.0998334166$\n$\\exp(0.1) \\approx 1.105170918$\n$$ [\\delta\\mu_{f}]_{1} \\approx -0.02 \\times 0.0998334166 = -0.0019966683 $$\n$$ [\\delta\\mu_{f}]_{2} \\approx 0.045 \\times 1.105170918 = 0.0497326913 $$\n$$ ||\\delta\\mu_{f}||_{2} \\approx \\sqrt{(-0.0019966683)^{2} + (0.0497326913)^{2}} $$\n$$ ||\\delta\\mu_{f}||_{2} \\approx \\sqrt{0.00000398668 + 0.00247334058} = \\sqrt{0.00247732726} \\approx 0.049772755 $$\nRounding the result to four significant figures gives $0.04977$.", "answer": "$$\n\\boxed{0.04977}\n$$", "id": "2705986"}]}