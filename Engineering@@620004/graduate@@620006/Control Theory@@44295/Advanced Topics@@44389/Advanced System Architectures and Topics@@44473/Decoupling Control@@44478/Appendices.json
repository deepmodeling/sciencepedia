{"hands_on_practices": [{"introduction": "Ideal input-output decoupling often requires a 'square' system, where the number of control inputs matches the number of outputs. This practice explores the common engineering scenario where there are fewer inputs than outputs, making full decoupling structurally impossible. By strategically combining system outputs into a single virtual output, you will learn how to achieve partial decoupling, isolating a specific subsystem from unwanted influence and normalizing its response [@problem_id:2699019]. This exercise builds foundational skills in translating design specifications into algebraic constraints on a system's transfer function.", "problem": "Consider the linear time-invariant multiple-input multiple-output plant with transfer matrix\n$$\nG(s)=\\begin{pmatrix}\n\\dfrac{s+3}{s+1} & \\dfrac{2}{s+1} \\\\\n\\dfrac{2s+1}{s+1} & \\dfrac{4}{s+1} \\\\\n\\dfrac{s+2}{s+1} & -\\dfrac{2}{s+1}\n\\end{pmatrix},\n$$\nwhich maps the input vector $u(s)=\\begin{pmatrix}u_1(s) \\\\ u_2(s)\\end{pmatrix}$ to the output vector $y(s)=\\begin{pmatrix}y_1(s) \\\\ y_2(s) \\\\ y_3(s)\\end{pmatrix}$ via $y(s)=G(s)u(s)$. Because the number of outputs exceeds the number of inputs (i.e., $p=3$ and $m=2$), full decoupling is not achievable by square inversion. A common partial-decoupling approach is to define a virtual output $y_v(s)=a^{\\top}y(s)$ with a constant vector $a=\\begin{pmatrix}1 \\\\ \\alpha \\\\ \\beta\\end{pmatrix}$ chosen to remove the influence of the input $u_2$ on $y_v$ for all complex values $s$, while normalizing the steady-state gain from $u_1$ to $y_v$ to unity.\n\nUsing only the definitions of linear time-invariant input-output maps and steady-state (direct current) gain, determine the value of the scalar $\\alpha$ such that:\n- $y_v(s)$ is completely decoupled from $u_2(s)$ for all $s$ (that is, the transfer from $u_2$ to $y_v$ is identically zero), and\n- the steady-state gain from $u_1$ to $y_v$ equals $1$ (that is, for a constant input $u_1(t)=U$ and $u_2(t)=0$, the steady-state value of $y_v(t)$ equals $U$).\n\nProvide your answer for $\\alpha$ as an exact value (no rounding). No physical units are required.", "solution": "The problem is first validated. The givens are the transfer matrix $G(s)$, the input vector $u(s)$, the output vector $y(s)$, the input-output relationship $y(s)=G(s)u(s)$, the definition of a virtual output $y_v(s) = a^\\top y(s)$ with $a=\\begin{pmatrix}1 \\\\ \\alpha \\\\ \\beta\\end{pmatrix}$, and two conditions on the resulting system. The problem is scientifically grounded in linear control theory, well-posed, objective, and contains all necessary information to determine the value of the parameter $\\alpha$. There are no contradictions, ambiguities, or scientifically unsound premises. The problem is therefore deemed valid and a solution will be constructed.\n\nThe relationship between the system inputs $u(s)$ and the virtual output $y_v(s)$ is established by substituting the system dynamics into the definition of the virtual output.\nThe input-output relationship is given by:\n$$\ny(s) = G(s)u(s)\n$$\nThe virtual output is defined as:\n$$\ny_v(s) = a^\\top y(s)\n$$\nwhere $a = \\begin{pmatrix} 1 & \\alpha & \\beta \\end{pmatrix}^\\top$. Combining these gives:\n$$\ny_v(s) = a^\\top G(s) u(s)\n$$\nThis defines a new $1 \\times 2$ transfer matrix from $u(s)$ to $y_v(s)$, which we will denote as $G_v(s) = a^\\top G(s)$. Let us compute this matrix.\n$$\nG_v(s) = \\begin{pmatrix} 1 & \\alpha & \\beta \\end{pmatrix}\n\\begin{pmatrix}\n\\dfrac{s+3}{s+1} & \\dfrac{2}{s+1} \\\\\n\\dfrac{2s+1}{s+1} & \\dfrac{4}{s+1} \\\\\n\\dfrac{s+2}{s+1} & -\\dfrac{2}{s+1}\n\\end{pmatrix}\n$$\nThe resulting $1 \\times 2$ matrix $G_v(s)$ has two components, $G_{v1}(s)$ and $G_{v2}(s)$, such that $y_v(s) = G_{v1}(s)u_1(s) + G_{v2}(s)u_2(s)$. The components are:\n$$\nG_{v1}(s) = 1 \\cdot \\left(\\dfrac{s+3}{s+1}\\right) + \\alpha \\cdot \\left(\\dfrac{2s+1}{s+1}\\right) + \\beta \\cdot \\left(\\dfrac{s+2}{s+1}\\right) = \\dfrac{(s+3) + \\alpha(2s+1) + \\beta(s+2)}{s+1}\n$$\n$$\nG_{v2}(s) = 1 \\cdot \\left(\\dfrac{2}{s+1}\\right) + \\alpha \\cdot \\left(\\dfrac{4}{s+1}\\right) + \\beta \\cdot \\left(\\dfrac{-2}{s+1}\\right) = \\dfrac{2 + 4\\alpha - 2\\beta}{s+1}\n$$\n\nNow, we apply the two conditions given in the problem statement.\n\nThe first condition is that the virtual output $y_v(s)$ is completely decoupled from the input $u_2(s)$ for all $s$. This means that the transfer function from $u_2(s)$ to $y_v(s)$, which is $G_{v2}(s)$, must be identically zero.\n$$\nG_{v2}(s) = \\dfrac{2 + 4\\alpha - 2\\beta}{s+1} = 0\n$$\nFor this rational function to be zero for all $s$ (except at the pole $s=-1$), its numerator must be zero.\n$$\n2 + 4\\alpha - 2\\beta = 0\n$$\nDividing by $2$ gives our first equation relating $\\alpha$ and $\\beta$:\n$$\n1 + 2\\alpha - \\beta = 0 \\quad \\implies \\quad \\beta = 1 + 2\\alpha\n$$\n\nThe second condition is that the steady-state gain from $u_1(s)$ to $y_v(s)$ must be unity, i.e., $1$. The steady-state (or direct current, DC) gain of a stable linear time-invariant system is obtained by evaluating its transfer function at $s=0$. The transfer function from $u_1(s)$ to $y_v(s)$ is $G_{v1}(s)$. The poles of the overall system are at $s=-1$, which is in the left-half plane, so the system is stable and the steady-state gain concept is applicable.\nWe set $G_{v1}(0) = 1$.\n$$\nG_{v1}(0) = \\left. \\dfrac{(s+3) + \\alpha(2s+1) + \\beta(s+2)}{s+1} \\right|_{s=0}\n$$\n$$\nG_{v1}(0) = \\dfrac{(0+3) + \\alpha(2 \\cdot 0+1) + \\beta(0+2)}{0+1} = \\dfrac{3 + \\alpha + 2\\beta}{1} = 3+\\alpha+2\\beta\n$$\nSetting this gain to $1$:\n$$\n3 + \\alpha + 2\\beta = 1\n$$\nThis gives our second equation:\n$$\n\\alpha + 2\\beta = -2\n$$\n\nWe now have a system of two linear equations with two unknowns, $\\alpha$ and $\\beta$:\n1. $\\beta = 1 + 2\\alpha$\n2. $\\alpha + 2\\beta = -2$\n\nWe can solve for $\\alpha$ by substituting the expression for $\\beta$ from the first equation into the second equation.\n$$\n\\alpha + 2(1 + 2\\alpha) = -2\n$$\n$$\n\\alpha + 2 + 4\\alpha = -2\n$$\n$$\n5\\alpha + 2 = -2\n$$\n$$\n5\\alpha = -4\n$$\n$$\n\\alpha = -\\dfrac{4}{5}\n$$\nThe value of $\\alpha$ is determined to be $-\\frac{4}{5}$. The value of $\\beta$ is not requested, but can be found as $\\beta = 1 + 2(-\\frac{4}{5}) = 1 - \\frac{8}{5} = -\\frac{3}{5}$. The solution for $\\alpha$ is unique and consistently derived from the problem's premises.", "answer": "$$\\boxed{-\\frac{4}{5}}$$", "id": "2699019"}, {"introduction": "Moving beyond the domain of linear systems, this exercise delves into the powerful techniques of nonlinear decoupling control. You will apply the machinery of differential geometry, including Lie derivatives and the concept of relative degree, to a control-affine nonlinear system [@problem_id:2698987]. This practice is essential for understanding how to achieve partial feedback linearization, which reveals the system's inherent 'zero dynamics' and the ultimate limitations of feedback control.", "problem": "Consider the nonlinear multiple-input multiple-output system in control-affine form with state $x \\in \\mathbb{R}^{4}$, inputs $u = (u_{1},u_{2}) \\in \\mathbb{R}^{2}$, and outputs $y = (y_{1},y_{2}) \\in \\mathbb{R}^{2}$:\n- State dynamics:\n$$\n\\begin{aligned}\n\\dot{x}_{1} &= x_{2} + x_{1} x_{4},\\\\\n\\dot{x}_{2} &= u_{1} + x_{3},\\\\\n\\dot{x}_{3} &= u_{2},\\\\\n\\dot{x}_{4} &= -x_{1} + x_{2} x_{3} + x_{4}^{3}.\n\\end{aligned}\n$$\n- Outputs:\n$$\ny_{1} = x_{1}, \\qquad y_{2} = x_{3}.\n$$\nStarting from the foundational definitions of Lie derivatives, relative degree, and the concept of input-output decoupling for nonlinear systems, perform the following:\n- Determine the vector of relative degrees $(r_{1},r_{2})$ associated with $(y_{1},y_{2})$ and the corresponding decoupling matrix $A(x)$ whose $(i,j)$ entry is given by $L_{g_{j}} L_{f}^{r_{i}-1} h_{i}(x)$, where $h_{1}(x)=x_{1}$ and $h_{2}(x)=x_{3}$.\n- Show that the system admits partial input-output linearization into a triangular form with relative degree vector $(r_{1},r_{2})$ and internal dynamics of dimension $n - (r_{1}+r_{2})$, where $n$ is the state dimension.\n- Choose normal form coordinates $(z_{1},z_{2},z_{3},\\eta)$ defined by\n$$\nz_{1} = y_{1}, \\quad z_{2} = \\dot{y}_{1}, \\quad z_{3} = y_{2}, \\quad \\eta = x_{4},\n$$\nand express their dynamics explicitly using only the system definition and the concept of Lie derivatives.\n- Using the input transformation implied by partial decoupling, define the virtual inputs $v_{1} = \\ddot{y}_{1}$ and $v_{2} = \\dot{y}_{2}$ and identify the corresponding feedback of the form $u = \\alpha(x) + \\beta(x) v$ that renders the input-output map triangular.\n- Consider the zero dynamics manifold defined by $z_{1}=0$, $z_{2}=0$, and $z_{3}=0$, and enforce invariance of this manifold by choosing $v_{1}=0$ and $v_{2}=0$. Compute the instantaneous internal dynamics rate $\\dot{\\eta}$ at the state\n$$\nx = (x_{1},x_{2},x_{3},x_{4}) = (0,0,0,2).\n$$\nProvide your final answer as a single real number. No rounding is required.", "solution": "The problem statement is a standard exercise in nonlinear control theory, specifically concerning input-output linearization and decoupling. It is scientifically grounded, well-posed, objective, and self-contained, presenting a valid system of equations and a set of clear, formalizable tasks. There are no contradictions, ambiguities, or factual unsoundness. Therefore, the problem is valid, and we proceed with the solution.\n\nThe system is given by the state-space representation $\\dot{x} = f(x) + g(x)u$, where $x \\in \\mathbb{R}^{4}$, $u \\in \\mathbb{R}^{2}$, and the outputs are $y=h(x)$. The components are explicitly:\n$$\nf(x) = \\begin{pmatrix} x_{2} + x_{1} x_{4} \\\\ x_{3} \\\\ 0 \\\\ -x_{1} + x_{2} x_{3} + x_{4}^{3} \\end{pmatrix}, \\quad g(x) = \\begin{bmatrix} g_{1}(x) & g_{2}(x) \\end{bmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nand the output functions are $h_{1}(x) = y_{1} = x_{1}$ and $h_{2}(x) = y_{2} = x_{3}$.\n\nFirst, we determine the vector of relative degrees $(r_{1}, r_{2})$. The relative degree $r_{i}$ is the smallest integer for which the Lie derivative of the $i$-th output function $h_{i}$ with respect to at least one of the input vector fields $g_{j}$ is non-zero, after having been differentiated $r_{i}-1$ times along the drift vector field $f$. That is, $L_{g_{j}}L_{f}^{k}h_{i}(x) = 0$ for all $k < r_{i}-1$ and for all $j \\in \\{1, 2\\}$, and for some $j$, $L_{g_{j}}L_{f}^{r_{i}-1}h_{i}(x) \\neq 0$.\n\nFor the first output, $h_{1}(x) = x_{1}$:\nThe Lie derivative with respect to $f$ is $\\dot{y}_{1}$ in the absence of control.\n$L_{f}h_{1}(x) = \\nabla h_{1}(x) \\cdot f(x) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} f(x) = x_{2} + x_{1} x_{4}$.\nNow, we compute the Lie derivatives of $h_{1}(x)$ with respect to the input vector fields:\n$L_{g_{1}}h_{1}(x) = \\nabla h_{1}(x) \\cdot g_{1}(x) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} g_{1}(x) = 0$.\n$L_{g_{2}}h_{1}(x) = \\nabla h_{1}(x) \\cdot g_{2}(x) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} g_{2}(x) = 0$.\nSince both are zero, the relative degree $r_{1}$ is greater than $1$. We must differentiate further. We compute the Lie derivatives of $L_{f}h_{1}(x)$ with respect to $g_{j}$:\n$L_{g_{1}}L_{f}h_{1}(x) = \\nabla(L_{f}h_{1}(x)) \\cdot g_{1}(x) = \\nabla(x_{2} + x_{1} x_{4}) \\cdot g_{1}(x) = \\begin{pmatrix} x_{4} & 1 & 0 & x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1$.\nSince $L_{g_{1}}L_{f}^{1}h_{1}(x) = 1 \\neq 0$, the relative degree for the first output is $r_{1} = 2$.\nFor completeness, $L_{g_{2}}L_{f}h_{1}(x) = \\begin{pmatrix} x_{4} & 1 & 0 & x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = 0$.\n\nFor the second output, $h_{2}(x) = x_{3}$:\nWe compute the Lie derivatives with respect to $g_{j}$:\n$L_{g_{1}}h_{2}(x) = \\nabla h_{2}(x) \\cdot g_{1}(x) = \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix} g_{1}(x) = 0$.\n$L_{g_{2}}h_{2}(x) = \\nabla h_{2}(x) \\cdot g_{2}(x) = \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix} g_{2}(x) = 1$.\nSince $L_{g_{2}}h_{2}(x) \\neq 0$, the relative degree for the second output is $r_{2} = 1$.\nThe vector of relative degrees is therefore $(r_{1}, r_{2}) = (2, 1)$.\n\nThe decoupling matrix $A(x)$ is defined by its entries $A_{ij}(x) = L_{g_{j}} L_{f}^{r_{i}-1} h_{i}(x)$.\n$A_{11}(x) = L_{g_{1}} L_{f}^{2-1} h_{1}(x) = L_{g_{1}} L_{f} h_{1}(x) = 1$.\n$A_{12}(x) = L_{g_{2}} L_{f}^{2-1} h_{1}(x) = L_{g_{2}} L_{f} h_{1}(x) = 0$.\n$A_{21}(x) = L_{g_{1}} L_{f}^{1-1} h_{2}(x) = L_{g_{1}} h_{2}(x) = 0$.\n$A_{22}(x) = L_{g_{2}} L_{f}^{1-1} h_{2}(x) = L_{g_{2}} h_{2}(x) = 1$.\nThus, the decoupling matrix is the constant identity matrix:\n$$\nA(x) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe system admits partial input-output linearization because the decoupling matrix $A(x)$ is non-singular for all $x$. A diagonal matrix is a special case of a triangular matrix, so the condition is met. The total relative degree is $r_{1} + r_{2} = 2 + 1 = 3$, which is less than the state dimension $n=4$. This implies the existence of internal dynamics of dimension $n - (r_{1}+r_{2}) = 4 - 3 = 1$.\n\nNext, we define the normal form coordinates:\n$z_{1} = y_{1} = h_{1}(x) = x_{1}$.\n$z_{2} = \\dot{y}_{1} = L_{f}h_{1}(x) = x_{2} + x_{1} x_{4}$.\n$z_{3} = y_{2} = h_{2}(x) = x_{3}$.\n$\\eta = x_{4}$.\nThe coordinate transformation $\\Phi(x) = (z_{1}, z_{2}, z_{3}, \\eta)^T$ has Jacobian determinant equal to $1$, making it a global diffeomorphism. The dynamics of these new coordinates are found by differentiation:\n$\\dot{z}_{1} = \\frac{d}{dt}(x_{1}) = \\dot{x}_{1} = x_{2} + x_{1} x_{4} = z_{2}$.\n$\\dot{z}_{3} = \\frac{d}{dt}(x_{3}) = \\dot{x}_{3} = u_{2}$.\nThe time derivative of $z_{2}$ corresponds to the second derivative of $y_1$:\n$\\dot{z}_{2} = \\ddot{y}_{1} = L_{f}^{2}h_{1}(x) + L_{g_{1}}L_{f}h_{1}(x) u_{1} + L_{g_{2}}L_{f}h_{1}(x) u_{2}$.\nWe require $L_{f}^{2}h_{1}(x) = \\nabla(L_{f}h_{1}(x)) \\cdot f(x) = \\begin{pmatrix} x_{4} & 1 & 0 & x_{1} \\end{pmatrix} f(x) = x_{4}(x_{2} + x_{1}x_{4}) + x_{3} + x_{1}(-x_{1} + x_{2}x_{3} + x_{4}^{3}) = x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}$.\nSo, $\\dot{z}_{2} = (x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}) + u_{1}$.\nThe dynamics of the internal state $\\eta$ are:\n$\\dot{\\eta} = \\dot{x}_{4} = -x_{1} + x_{2}x_{3} + x_{4}^{3}$.\n\nWe now define the virtual inputs $v_{1} = \\ddot{y}_{1}$ and $v_{2} = \\dot{y}_{2}$. The input-output dynamics are:\n$\\ddot{y}_{1} = L_{f}^{2}h_{1}(x) + u_{1}$.\n$\\dot{y}_{2} = L_{f}h_{2}(x) + u_{2} = 0 + u_{2} = u_{2}$.\nSetting $\\ddot{y}_{1} = v_{1}$ and $\\dot{y}_{2} = v_{2}$ yields the linearizing feedback law.\n$v_{1} = L_{f}^{2}h_{1}(x) + u_{1} \\implies u_{1} = -L_{f}^{2}h_{1}(x) + v_{1}$.\n$v_{2} = u_{2}$.\nIn the form $u = \\alpha(x) + \\beta(x)v$, we have:\n$\\alpha(x) = \\begin{pmatrix} -L_{f}^{2}h_{1}(x) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -(x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}) \\\\ 0 \\end{pmatrix}$\n$\\beta(x) = A(x)^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe triangular structure is evident as the input transformation is diagonal (a special case of triangular).\n\nFinally, we analyze the zero dynamics. The zero dynamics manifold is defined by keeping the outputs and their derivatives at zero: $y_{1}=0$, $\\dot{y}_{1}=0$, and $y_{2}=0$.\n$z_{1} = y_{1} = x_{1} = 0$.\n$z_{2} = \\dot{y}_{1} = x_{2} + x_{1}x_{4} = 0$. Since $x_{1}=0$, we have $x_{2}=0$.\n$z_{3} = y_{2} = x_{3} = 0$.\nThe zero dynamics manifold is the set of states where $x_{1}=x_{2}=x_{3}=0$.\nTo enforce invariance of this manifold, we must set the virtual inputs to zero: $v_{1}=0$ and $v_{2}=0$. This implies $\\ddot{y}_{1}=0$ and $\\dot{y}_{2}=0$, ensuring the outputs remain at zero.\nThe internal dynamics are described by the evolution of $\\eta = x_{4}$ on this manifold. The governing equation is $\\dot{\\eta} = \\dot{x}_{4} = -x_{1} + x_{2}x_{3} + x_{4}^{3}$.\nSubstituting the conditions of the zero dynamics manifold ($x_{1}=0, x_{2}=0, x_{3}=0$) into this equation gives the zero dynamics:\n$\\dot{\\eta} = -0 + (0)(0) + x_{4}^{3} = x_{4}^{3}$.\nIn terms of the internal coordinate $\\eta$, this is $\\dot{\\eta} = \\eta^{3}$.\n\nWe are asked to compute the instantaneous internal dynamics rate $\\dot{\\eta}$ at the state $x = (0, 0, 0, 2)$. This state lies on the zero dynamics manifold. At this point, the internal coordinate has the value $\\eta = x_{4} = 2$.\nSubstituting this value into the zero dynamics equation provides the rate:\n$\\dot{\\eta} = \\eta^{3} = (2)^{3} = 8$.", "answer": "$$\\boxed{8}$$", "id": "2698987"}, {"introduction": "Even when a plant is square, perfect theoretical decoupling can be a fragile solution that amplifies noise and performs poorly in practice. This computational exercise explores the practical challenge of approximate decoupling, where a controller's bandwidth, $\\omega_c$, must be carefully tuned to balance conflicting objectives [@problem_id:2699011]. By minimizing a composite cost function, $J_{\\alpha}(\\omega_c)$, that weighs low-frequency decoupling against high-frequency control effort, you will gain hands-on experience with the design trade-offs central to modern control engineering.", "problem": "Design a program that evaluates approximate decoupling and the associated performance tradeoffs for a square, continuous-time, linear time-invariant multivariable plant under a static precompensator followed by identical diagonal first-order loop filters. The plant is two-input and two-output. The standard negative feedback architecture is used, where the control input is given by $u=K(r-y)$ and the output is $y=G(s)u$, with $r$ the reference input. The plant transfer matrix $G(s)$ is strictly proper and stable, and each entry is a first-order rational function of the form $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$ with $k_{ij}\\in\\mathbb{R}$ and $\\tau_{ij}>0$. The decoupling controller is constructed as follows: a static precompensator $W=G(0)^{-1}$ is used to approximately invert the plant at steady state, and identical diagonal loop filters are applied per channel as $F(s)=\\mathrm{diag}\\!\\left(\\dfrac{\\omega_c}{s+\\omega_c},\\dfrac{\\omega_c}{s+\\omega_c}\\right)$, with $\\omega_c>0$ the tunable bandwidth parameter. The resulting controller is $K(s)=F(s)W$. The closed-loop map from $r$ to $y$ under unity feedback is $T(s)$, defined by the standard negative feedback relation based on $G(s)$ and $K(s)$. The approximate-decoupling quality is measured at low frequency by the maximum off-diagonal magnitude of $T(\\mathrm{j}\\omega)$ over a prescribed low-frequency band, while the performance penalty is measured by the high-frequency gain of $K(\\mathrm{j}\\omega)$ over a prescribed high-frequency band. The scalarized tradeoff cost to be minimized over $\\omega_c$ is\n$$\nJ_{\\alpha}(\\omega_c)=\\alpha\\,E_{\\mathrm{dec}}(\\omega_c)+(1-\\alpha)\\,E_{\\mathrm{noise}}(\\omega_c),\n$$\nwith $0<\\alpha<1$.\n\nYour program must implement the following precise definitions and computations.\n\n- Low-frequency decoupling error:\n$$\nE_{\\mathrm{dec}}(\\omega_c)=\\max_{\\omega\\in\\Omega_L}\\left\\|\\mathrm{OffDiag}\\!\\left(T(\\mathrm{j}\\omega)\\right)\\right\\|_F,\n$$\nwhere $\\mathrm{OffDiag}(M)$ is the matrix with the same off-diagonal elements as $M$ and zeros on the diagonal, and $\\|\\cdot\\|_F$ denotes the Frobenius norm. The low-frequency grid is $\\Omega_L=\\{\\omega_{\\ell,m}\\}_{m=1}^{N_L}$, logarithmically spaced from $\\omega_{\\ell,\\min}$ to $\\omega_{\\ell,\\max}$.\n\n- High-frequency noise amplification proxy:\n$$\nE_{\\mathrm{noise}}(\\omega_c)=\\sup_{\\omega\\in\\Omega_H}\\,\\bar{\\sigma}\\!\\left(K(\\mathrm{j}\\omega)\\right),\n$$\nwhere $\\bar{\\sigma}(\\cdot)$ is the largest singular value and $\\Omega_H=\\{\\omega_{h,m}\\}_{m=1}^{N_H}$ is logarithmically spaced from $\\omega_{h,\\min}$ to $\\omega_{h,\\max}$.\n\n- Controller construction details:\n    - Compute $G(0)$ entrywise from $g_{ij}(s)$ and set $W=G(0)^{-1}$.\n    - For each frequency $\\omega$, evaluate $G(\\mathrm{j}\\omega)$ and $F(\\mathrm{j}\\omega)$, then $K(\\mathrm{j}\\omega)=F(\\mathrm{j}\\omega)W$.\n    - The closed-loop map is $T(\\mathrm{j}\\omega)=(I+G(\\mathrm{j}\\omega)K(\\mathrm{j}\\omega))^{-1}G(\\mathrm{j}\\omega)K(\\mathrm{j}\\omega)$.\n\n- Optimization over controller bandwidth:\n    - Search over $\\omega_c$ in the grid $\\Omega_c=\\{\\omega_{c,n}\\}_{n=1}^{N_c}$, logarithmically spaced between $\\omega_{c,\\min}$ and $\\omega_{c,\\max}$.\n    - Choose the minimizing $\\omega_c$ for $J_{\\alpha}(\\omega_c)$; in case of ties within numerical tolerance, select the smallest $\\omega_c$ achieving the minimum.\n\nUse the following globally fixed parameters for all test cases:\n- Tradeoff weight $\\alpha=0.6$.\n- Low-frequency band: $\\omega_{\\ell,\\min}=10^{-3}$ in rad/s and $\\omega_{\\ell,\\max}=1.0$ in rad/s, with $N_L=80$ logarithmically spaced points.\n- High-frequency band: $\\omega_{h,\\min}=20.0$ in rad/s and $\\omega_{h,\\max}=200.0$ in rad/s, with $N_H=80$ logarithmically spaced points.\n- Controller bandwidth search: $\\omega_{c,\\min}=0.1$ in rad/s and $\\omega_{c,\\max}=100.0$ in rad/s, with $N_c=160$ logarithmically spaced points.\n\nTest suite. For each test, the plant is specified by the matrices $K=[k_{ij}]$ and $\\Tau=[\\tau_{ij}]$ that define each entry $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$.\n\n- Test $1$ (well-conditioned steady-state gain with moderate coupling):\n    - $K=\\begin{bmatrix}2.0&0.3\\\\0.2&1.2\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}1.0&0.5\\\\0.7&0.8\\end{bmatrix}$.\n\n- Test $2$ (nearly singular steady-state gain):\n    - $K=\\begin{bmatrix}1.0&0.95\\\\0.95&0.91\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}0.3&1.2\\\\0.9&0.4\\end{bmatrix}$.\n\n- Test $3$ (mixed-sign coupling and disparate time constants):\n    - $K=\\begin{bmatrix}1.0&-0.8\\\\0.6&1.1\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}0.5&0.6\\\\0.4&1.5\\end{bmatrix}$.\n\nAngle and frequency units must be in radians per second. Your program must compute, for each test, the $\\omega_c$ in rad/s that minimizes $J_{\\alpha}(\\omega_c)$ over the specified grid, and report the three optimal $\\omega_c$ values.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places in rad/s, ordered by tests $1$, $2$, $3$, for example, $[1.234000,5.678000,9.101000]$.", "solution": "The problem presented is valid. It is a well-posed optimization problem in the domain of multivariable control theory, grounded in established principles of decoupling and performance trade-offs. All constants, functions, and metrics are defined with sufficient rigor to permit a unique, verifiable solution. We shall proceed with the design of the required computational procedure.\n\nThe task is to find the optimal controller bandwidth $\\omega_c$ that minimizes a weighted cost function $J_{\\alpha}(\\omega_c)$, which represents a trade-off between low-frequency decoupling performance and high-frequency noise sensitivity for a given two-input, two-output (TITO) LTI plant.\n\nThe plant's transfer function matrix is given by $G(s)$, with entries $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$. The matrices of gains $K=[k_{ij}]$ and time constants $\\Tau=[\\tau_{ij}]$ are provided for each test case.\n\nThe controller $K(s)$ is composed of a static precompensator $W$ and a diagonal filter matrix $F(s)$, such that $K(s) = F(s)W$.\nThe precompensator $W$ is designed to achieve perfect steady-state decoupling. This is accomplished by setting $W$ to be the inverse of the plant's steady-state gain matrix, $G(0)$. Since $g_{ij}(0) = k_{ij}$, we have $G(0) = K$. Therefore, the static precompensator is $W=K^{-1}$. This requires that $K$ is invertible, which is true for all test cases provided.\n\nThe filter matrix is $F(s)=\\mathrm{diag}\\!\\left(\\dfrac{\\omega_c}{s+\\omega_c},\\dfrac{\\omega_c}{s+\\omega_c}\\right) = \\dfrac{\\omega_c}{s+\\omega_c} I$, where $I$ is the $2 \\times 2$ identity matrix and $\\omega_c > 0$ is the tunable controller bandwidth.\n\nThe total cost to be minimized is $J_{\\alpha}(\\omega_c)=\\alpha\\,E_{\\mathrm{dec}}(\\omega_c)+(1-\\alpha)\\,E_{\\mathrm{noise}}(\\omega_c)$ with $\\alpha=0.6$. The optimization is performed by searching over a discrete grid of $\\omega_c$ values, $\\Omega_c$. We now analyze the two components of the cost function.\n\nFirst, the low-frequency decoupling error, $E_{\\mathrm{dec}}(\\omega_c)$, is defined as:\n$$\nE_{\\mathrm{dec}}(\\omega_c)=\\max_{\\omega\\in\\Omega_L}\\left\\|\\mathrm{OffDiag}\\!\\left(T(\\mathrm{j}\\omega)\\right)\\right\\|_F\n$$\nHere, $T(\\mathrm{j}\\omega)$ is the closed-loop transfer function matrix evaluated at frequency $\\omega$. It is given by the standard formula for negative feedback systems:\n$$\nT(s) = (I+L(s))^{-1}L(s)\n$$\nwhere $L(s) = G(s)K(s)$ is the open-loop transfer function. Substituting the expressions for $G(s)$ and $K(s)$, we evaluate this at $s=\\mathrm{j}\\omega$ for each $\\omega$ in the low-frequency grid $\\Omega_L$. For a given matrix $M$, $\\mathrm{OffDiag}(M)$ sets the diagonal elements to zero, and $\\|\\cdot\\|_F$ is the Frobenius norm, calculated as the square root of the sum of the squared magnitudes of the matrix elements. The computation of $E_{\\mathrm{dec}}(\\omega_c)$ requires a numerical sweep across the frequency grid $\\Omega_L$. For a $2 \\times 2$ matrix $T = \\begin{bmatrix} T_{11} & T_{12} \\\\ T_{21} & T_{22} \\end{bmatrix}$, this norm is $\\sqrt{|T_{12}|^2 + |T_{21}|^2}$.\n\nSecond, the high-frequency noise amplification proxy, $E_{\\mathrm{noise}}(\\omega_c)$, is defined as:\n$$\nE_{\\mathrm{noise}}(\\omega_c)=\\sup_{\\omega\\in\\Omega_H}\\,\\bar{\\sigma}\\!\\left(K(\\mathrm{j}\\omega)\\right)\n$$\nwhere $\\bar{\\sigma}(\\cdot)$ denotes the largest singular value. The controller is $K(s) = \\left(\\dfrac{\\omega_c}{s+\\omega_c}\\right)W$. Evaluating at $s=\\mathrm{j}\\omega$, we have $K(\\mathrm{j}\\omega) = \\left(\\dfrac{\\omega_c}{\\mathrm{j}\\omega+\\omega_c}\\right)W$.\nUsing the property of singular values that $\\bar{\\sigma}(cA) = |c|\\bar{\\sigma}(A)$ for a complex scalar $c$ and matrix $A$, we can write:\n$$\n\\bar{\\sigma}(K(\\mathrm{j}\\omega)) = \\left|\\dfrac{\\omega_c}{\\mathrm{j}\\omega+\\omega_c}\\right|\\bar{\\sigma}(W) = \\dfrac{\\omega_c}{\\sqrt{\\omega^2+\\omega_c^2}}\\bar{\\sigma}(W)\n$$\nThe term $\\bar{\\sigma}(W)$ is a constant for a given plant. The function $f(\\omega) = \\dfrac{\\omega_c}{\\sqrt{\\omega^2+\\omega_c^2}}$ is a strictly decreasing function of $\\omega$ for $\\omega > 0$. The high-frequency evaluation band is $\\Omega_H = [\\omega_{h,\\min}, \\omega_{h,\\max}]$, where $\\omega_{h,min} > 0$. Therefore, the supremum of $f(\\omega)$ over this band occurs at its lowest frequency, $\\omega_{h,min}$. This provides a simplified analytical expression for $E_{\\mathrm{noise}}(\\omega_c)$:\n$$\nE_{\\mathrm{noise}}(\\omega_c) = \\dfrac{\\omega_c}{\\sqrt{\\omega_{h,\\min}^2+\\omega_c^2}}\\bar{\\sigma}(W)\n$$\nThis simplification avoids a numerical sweep over the high-frequency grid $\\Omega_H$ and will be used in the implementation.\n\nThe optimization algorithm proceeds as follows:\n1. For each test case defined by matrices $K$ and $\\Tau$:\n2. Pre-compute the static precompensator $W = K^{-1}$ and its largest singular value $\\bar{\\sigma}(W)$.\n3. Iterate through each candidate bandwidth $\\omega_c$ in the specified grid $\\Omega_c$.\n4. For each $\\omega_c$:\n    a. Calculate $E_{\\mathrm{noise}}(\\omega_c)$ using the simplified analytical formula.\n    b. Calculate $E_{\\mathrm{dec}}(\\omega_c)$ by iterating through each frequency $\\omega$ in the grid $\\Omega_L$, computing $T(\\mathrm{j}\\omega)$, its off-diagonal Frobenius norm, and finding the maximum value.\n    c. Compute the total cost $J_{\\alpha}(\\omega_c) = \\alpha E_{\\mathrm{dec}}(\\omega_c) + (1-\\alpha) E_{\\mathrm{noise}}(\\omega_c)$.\n5. After evaluating the cost for all $\\omega_c \\in \\Omega_c$, find the minimum cost value.\n6. Identify the $\\omega_c$ that produces this minimum cost. If there is a tie (within numerical tolerance), select the smallest $\\omega_c$. This value is the solution for the test case.\n7. Repeat for all test cases and format the results as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multivariable controller optimization problem for three test cases.\n    \"\"\"\n    # Globally fixed parameters\n    ALPHA = 0.6\n    OMEGA_L_MIN, OMEGA_L_MAX, N_L = 1e-3, 1.0, 80\n    OMEGA_H_MIN = 20.0  # OMEGA_H_MAX and N_H are not needed due to analytical simplification\n    OMEGA_C_MIN, OMEGA_C_MAX, N_C = 0.1, 100.0, 160\n\n    # Define frequency grids\n    omega_l_grid = np.logspace(np.log10(OMEGA_L_MIN), np.log10(OMEGA_L_MAX), N_L)\n    omega_c_grid = np.logspace(np.log10(OMEGA_C_MIN), np.log10(OMEGA_C_MAX), N_C)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"K\": np.array([[2.0, 0.3], [0.2, 1.2]]),\n            \"Tau\": np.array([[1.0, 0.5], [0.7, 0.8]]),\n        },\n        {\n            \"K\": np.array([[1.0, 0.95], [0.95, 0.91]]),\n            \"Tau\": np.array([[0.3, 1.2], [0.9, 0.4]]),\n        },\n        {\n            \"K\": np.array([[1.0, -0.8], [0.6, 1.1]]),\n            \"Tau\": np.array([[0.5, 0.6], [0.4, 1.5]]),\n        },\n    ]\n\n    optimal_omegas = []\n\n    for case in test_cases:\n        K_plant = case[\"K\"]\n        Tau_plant = case[\"Tau\"]\n\n        # Step 1: Compute static precompensator W and its largest singular value\n        try:\n            W = np.linalg.inv(K_plant)\n        except np.linalg.LinAlgError:\n            # This case should not be reached with the given valid problems\n            # but is good practice.\n            optimal_omegas.append(np.nan)\n            continue\n        \n        s_bar_W = np.linalg.svd(W, compute_uv=False)[0]\n\n        costs = []\n        # Step 2: Iterate through each candidate bandwidth omega_c\n        for omega_c in omega_c_grid:\n            # Step 2a: Calculate E_noise using the analytical simplification\n            e_noise = (omega_c / np.sqrt(omega_c**2 + OMEGA_H_MIN**2)) * s_bar_W\n\n            # Step 2b: Calculate E_dec by sweeping over the low-frequency grid\n            max_off_diag_norm = 0.0\n            for omega in omega_l_grid:\n                s = 1j * omega\n                \n                # Evaluate plant transfer matrix G(s)\n                G_s = K_plant / (Tau_plant * s + 1)\n                \n                # Evaluate controller K(s) and loop gain L(s)\n                f_scalar = omega_c / (s + omega_c)\n                K_s = f_scalar * W\n                L_s = G_s @ K_s\n                \n                # Evaluate closed-loop transfer function T(s)\n                I = np.eye(2)\n                T_s = np.linalg.inv(I + L_s) @ L_s\n                \n                # Calculate Frobenius norm of off-diagonal part\n                off_diag_T = T_s.copy()\n                off_diag_T[0, 0] = 0\n                off_diag_T[1, 1] = 0\n                current_norm = np.linalg.norm(off_diag_T, 'fro')\n                \n                if current_norm > max_off_diag_norm:\n                    max_off_diag_norm = current_norm\n            \n            e_dec = max_off_diag_norm\n            \n            # Step 2c: Compute total cost\n            j_cost = ALPHA * e_dec + (1 - ALPHA) * e_noise\n            costs.append(j_cost)\n\n        # Step 3: Find the optimal omega_c that minimizes the cost\n        costs_arr = np.array(costs)\n        min_cost = np.min(costs_arr)\n        \n        # Find all indices where cost is close to the minimum (handles numerical ties)\n        min_indices = np.where(np.isclose(costs_arr, min_cost))[0]\n        \n        # Per tie-breaking rule, choose the smallest omega_c, which corresponds to the first index\n        optimal_index = min_indices[0]\n        optimal_omega_c = omega_c_grid[optimal_index]\n        \n        optimal_omegas.append(optimal_omega_c)\n\n    # Final formatting and printing\n    formatted_results = [f\"{val:.6f}\" for val in optimal_omegas]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2699011"}]}