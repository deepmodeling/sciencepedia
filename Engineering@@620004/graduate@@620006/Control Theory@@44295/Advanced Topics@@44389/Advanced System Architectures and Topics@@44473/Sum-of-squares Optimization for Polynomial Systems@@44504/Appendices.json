{"hands_on_practices": [{"introduction": "This first practice grounds the abstract theory of sum-of-squares decomposition in a concrete calculation. In this exercise [@problem_id:2751111], you will use the Gram matrix method from first principles to determine if a given polynomial can be written as a single square, providing a hands-on opportunity to master the foundational mechanics of setting up a monomial basis and matching coefficients. This is the first step toward understanding how algebraic problems about polynomials are transformed into the geometric language of semidefinite programming.", "problem": "Consider the bivariate quartic polynomial $p(x,y)=x^{4}+y^{4}+2x^{2}y^{2}+2x^{2}+2y^{2}+1$. A real multivariate polynomial is called a sum of squares (SOS) if it can be written as a finite sum of squared real polynomials. Using only the definition of sum of squares and the Gram matrix characterization of SOS polynomials as your fundamental starting point, determine whether $p(x,y)$ admits a single-square sum-of-squares representation of the form $p(x,y)=q(x,y)^{2}$ with $q(x,y)$ a real polynomial of total degree at most $2$. If such a representation exists, construct it explicitly; if it does not, prove that no such representation is possible. Your derivation must begin from first principles—namely, the SOS definition and the Gram matrix viewpoint—and proceed by equating coefficients or an equivalent logically rigorous approach to rule in or out the required structure.\n\nProvide the final answer as the explicit expression for $q(x,y)$ chosen with a positive constant term. The final answer must be a single closed-form expression with no units. If you find no such $q(x,y)$ exists, your derivation should logically establish impossibility, but your final answer must still be a single expression (in that case, give the zero polynomial). No rounding is required.", "solution": "The problem statement is subjected to validation before any attempt at a solution.\n\nGivens are extracted verbatim:\n1.  The polynomial is $p(x,y)=x^{4}+y^{4}+2x^{2}y^{2}+2x^{2}+2y^{2}+1$.\n2.  A real multivariate polynomial is a sum of squares (SOS) if it can be written as a finite sum of squared real polynomials.\n3.  The task is to determine if $p(x,y)$ can be written as $p(x,y)=q(x,y)^{2}$, where $q(x,y)$ is a real polynomial of total degree at most $2$.\n4.  The derivation must use the definition of SOS and the Gram matrix characterization.\n5.  If a solution for $q(x,y)$ exists, the one with a positive constant term must be provided.\n6.  If no such $q(x,y)$ exists, impossibility must be proven and the final answer must be the zero polynomial.\n\nValidation Verdict:\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in polynomial optimization and control theory, specifically concerning the sum-of-squares decomposition of a polynomial. The givens are complete and consistent. The degree of $p(x,y)$ is $4$, which is compatible with it being the square of a polynomial of degree $2$. The problem is not trivial, metaphorical, or outside the scope of mathematical verification. Therefore, the problem is deemed **valid**.\n\nWe now proceed to the solution. The problem asks whether the polynomial $p(x,y)=x^{4}+y^{4}+2x^{2}y^{2}+2x^{2}+2y^{2}+1$ can be expressed as a single square of a polynomial $q(x,y)$ of total degree at most $2$.\n\nAccording to the Gram matrix characterization, a polynomial $p(x)$ of total degree $2d$ is a sum of squares (SOS) if and only if there exists a positive semidefinite symmetric matrix $Q$, known as the Gram matrix, such that $p(x) = z(x)^T Q z(x)$, where $z(x)$ is a vector of monomials of degree up to $d$. For $p(x,y)$ to be a single square, $p(x,y) = q(x,y)^2$, the Gram matrix $Q$ must not only be positive semidefinite but also have a rank of $1$. A rank-$1$ positive semidefinite matrix can be written as $Q = vv^T$ for some vector $v$. In this case, $p(x,y) = z(x,y)^T (vv^T) z(x,y) = (v^T z(x,y))^2$. The polynomial $q(x,y)$ is then given by $v^T z(x,y)$.\n\nThe given polynomial $p(x,y)$ has a total degree of $2d=4$, thus $d=2$. The vector of monomials $z(x,y)$ of degree up to $d=2$ in variables $x$ and $y$ is:\n$$ z(x,y) = \\begin{pmatrix} 1  x  y  x^2  xy  y^2 \\end{pmatrix}^T $$\nThe associated Gram matrix $Q$ is a $6 \\times 6$ symmetric matrix. We must find such a matrix $Q$ that satisfies $p(x,y) = z^T Q z$. Let the elements of $Q$ be $q_{ij}$. The expansion of $z^T Q z$ is:\n$$ z^T Q z = q_{11} + 2q_{12}x + 2q_{13}y + (q_{22}+2q_{14})x^2 + (2q_{15}+2q_{23})xy + (q_{33}+2q_{16})y^2 + 2q_{24}x^3 + (2q_{25}+2q_{34})x^2y + (2q_{26}+2q_{35})xy^2 + 2q_{36}y^3 + q_{44}x^4 + 2q_{45}x^3y + (q_{55}+2q_{46})x^2y^2 + 2q_{56}xy^3 + q_{66}y^4 $$\nWe equate the coefficients of this expansion with the coefficients of $p(x,y)=x^{4}+y^{4}+2x^{2}y^{2}+2x^{2}+2y^{2}+1$:\n\\begin{itemize}\n    \\item $x^4$: $q_{44} = 1$\n    \\item $y^4$: $q_{66} = 1$\n    \\item $x^2y^2$: $q_{55} + 2q_{46} = 2$\n    \\item $x^2$: $q_{22} + 2q_{14} = 2$\n    \\item $y^2$: $q_{33} + 2q_{16} = 2$\n    \\item $1$: $q_{11} = 1$\n\\end{itemize}\nAll other coefficients must be zero:\n\\begin{itemize}\n    \\item $x^3, y^3, x^3y, xy^3, x, y$: $2q_{24}=0, 2q_{36}=0, 2q_{45}=0, 2q_{56}=0, 2q_{12}=0, 2q_{13}=0$. This implies $q_{24}=q_{36}=q_{45}=q_{56}=q_{12}=q_{13}=0$.\n    \\item $x^2y$: $2q_{25}+2q_{34}=0 \\implies q_{25}+q_{34}=0$.\n    \\item $xy^2$: $2q_{26}+2q_{35}=0 \\implies q_{26}+q_{35}=0$.\n    \\item $xy$: $2q_{15}+2q_{23}=0 \\implies q_{15}+q_{23}=0$.\n\\end{itemize}\nWe now seek a rank-$1$ matrix $Q=vv^T$ that satisfies these constraints. Let $v = \\begin{pmatrix} v_1  v_2  v_3  v_4  v_5  v_6 \\end{pmatrix}^T$. The elements of $Q$ are $q_{ij} = v_i v_j$. Substituting this into the constraints:\n\\begin{itemize}\n    \\item $q_{11} = v_1^2 = 1 \\implies v_1 = \\pm 1$\n    \\item $q_{44} = v_4^2 = 1 \\implies v_4 = \\pm 1$\n    \\item $q_{66} = v_6^2 = 1 \\implies v_6 = \\pm 1$\n\\end{itemize}\nFrom the zero-coefficient constraints:\n\\begin{itemize}\n    \\item $q_{12} = v_1 v_2 = 0$. Since $v_1 \\neq 0$, we must have $v_2 = 0$.\n    \\item $q_{13} = v_1 v_3 = 0$. Since $v_1 \\neq 0$, we must have $v_3 = 0$.\n    \\item $q_{45} = v_4 v_5 = 0$. Since $v_4 \\neq 0$, we must have $v_5 = 0$.\n\\end{itemize}\nThe other zero constraints are automatically satisfied: $q_{24}=v_2v_4=0$, $q_{36}=v_3v_6=0$, $q_{56}=v_5v_6=0$, $q_{25}=v_2v_5=0$, $q_{34}=v_3v_4=0$, etc. The vector $v$ has the form $v = \\begin{pmatrix} v_1  0  0  v_4  0  v_6 \\end{pmatrix}^T$.\n\nNow we check the remaining non-zero coefficient constraints:\n\\begin{itemize}\n    \\item $x^2$: $q_{22} + 2q_{14} = v_2^2 + 2(v_1 v_4) = 0 + 2v_1v_4 = 2 \\implies v_1v_4 = 1$.\n    \\item $y^2$: $q_{33} + 2q_{16} = v_3^2 + 2(v_1 v_6) = 0 + 2v_1v_6 = 2 \\implies v_1v_6 = 1$.\n    \\item $x^2y^2$: $q_{55} + 2q_{46} = v_5^2 + 2(v_4 v_6) = 0 + 2v_4v_6 = 2 \\implies v_4v_6 = 1$.\n\\end{itemize}\nWe must solve for $v_1, v_4, v_6$ using the conditions:\n$v_1^2=1$, $v_4^2=1$, $v_6^2=1$, $v_1v_4=1$, $v_1v_6=1$, and $v_4v_6=1$.\nThe product conditions imply that $v_1, v_4, v_6$ must all have the same sign. Since their squares must be $1$, the only two possibilities are:\n1. $v_1=1, v_4=1, v_6=1$.\n2. $v_1=-1, v_4=-1, v_6=-1$.\n\nThis confirms that a rank-$1$ Gram matrix representation exists. The polynomial $q(x,y)$ is given by $v^T z(x,y)$:\n$$ q(x,y) = v_1 \\cdot 1 + v_2 \\cdot x + v_3 \\cdot y + v_4 \\cdot x^2 + v_5 \\cdot xy + v_6 \\cdot y^2 $$\nSubstituting the values of $v_i$:\n$$ q(x,y) = v_1 + v_4 x^2 + v_6 y^2 $$\nFor Case 1 ($v_1=1, v_4=1, v_6=1$):\n$$ q(x,y) = 1 + x^2 + y^2 $$\nFor Case 2 ($v_1=-1, v_4=-1, v_6=-1$):\n$$ q(x,y) = -1 - x^2 - y^2 = -(1 + x^2 + y^2) $$\nThe problem requires the solution for $q(x,y)$ with a positive constant term. The constant term of $q(x,y)$ is $v_1$. We must therefore choose Case 1, where $v_1=1$.\n\nThe unique solution under the given constraint is $q(x,y) = x^2 + y^2 + 1$.\nA direct check confirms the result:\n$$ (x^2+y^2+1)^2 = (x^2)^2+(y^2)^2+1^2 + 2(x^2)(y^2) + 2(x^2)(1) + 2(y^2)(1) = x^4+y^4+1+2x^2y^2+2x^2+2y^2 $$\nThis matches the given polynomial $p(x,y)$. The derivation from the first principles of the Gram matrix method is complete and successful.", "answer": "$$ \\boxed{x^{2} + y^{2} + 1} $$", "id": "2751111"}, {"introduction": "Moving from manual calculation to automated formulation is a key step in applying SOS methods to real-world problems. This practice [@problem_id:2751036] challenges you to translate the theoretical steps of the Gram matrix method—generating monomial bases and forming coefficient-matching equations—into a computational algorithm. By implementing this procedure, you will gain insight into how SOS parsers translate high-level polynomial optimization problems into the standard matrix inequalities required by SDP solvers.", "problem": "You are given a polynomial in three variables of total degree $4$ and asked to design a computational procedure to formulate the corresponding Semidefinite Program (SDP) for testing whether the polynomial is a Sum-of-Squares (SOS). The final program must implement the Gram-matrix formulation, identify the associated linear equality constraints arising from coefficient matching, and report the size of the symmetric matrix variable. Start from the foundational definitions: a real multivariate polynomial $p(\\mathbf{x})$ is SOS if there exist real multivariate polynomials $f_k(\\mathbf{x})$ such that $p(\\mathbf{x}) = \\sum_k f_k(\\mathbf{x})^2$. The Gram-matrix method asserts that if $\\deg(p) \\le 2d$, then $p(\\mathbf{x})$ is SOS if and only if there exists a symmetric positive semidefinite matrix $Q \\succeq 0$ and a vector $v(\\mathbf{x})$ of all monomials of total degree at most $d$ such that $p(\\mathbf{x}) = v(\\mathbf{x})^\\top Q v(\\mathbf{x})$. Coefficient matching yields linear equality constraints on the entries of $Q$; the positivity constraint $Q \\succeq 0$ is represented by a matrix inequality. Your program must build these constraints symbolically and report sizes, not solve the SDP.\n\nConstruct the monomial basis $v(\\mathbf{x})$ in graded lexicographic order (total degree nondecreasing; ties broken lexicographically on exponent tuples). For a given number of variables $n$ and integer $d \\ge 0$, define $v(\\mathbf{x})$ as all monomials in $n$ variables with total degree at most $d$. Then $Q$ has dimension $m \\times m$ where $m$ is the length of $v(\\mathbf{x})$. The coefficient-matching constraints are built by expanding $v(\\mathbf{x})^\\top Q v(\\mathbf{x})$ and equating coefficients of each monomial of total degree up to $2d$ with those of $p(\\mathbf{x})$. Using the symmetric variable $Q$, the contribution of off-diagonal entries $Q_{ij}$ with $i \\ne j$ appears with factor $2$ in the coefficient matching.\n\nYour task is to implement the following in code:\n- Given a polynomial specified by its coefficient dictionary on monomial exponents, compute its total degree and set $d = \\deg(p)/2$. Assume $\\deg(p)$ is even in all test cases.\n- Generate the monomial exponent lists for $v(\\mathbf{x})$ (total degree $\\le d$) and for all monomials up to degree $2d$. Use graded lexicographic order.\n- Build the linear map $A$ from the upper-triangular entries of $Q$ (vectorized) to the coefficient vector of $v(\\mathbf{x})^\\top Q v(\\mathbf{x})$ ordered by monomials up to degree $2d$. Each row corresponds to one monomial exponent $\\alpha$ with $|\\alpha| \\le 2d$, and each column corresponds to one upper-triangular position $(i,j)$ with $i \\le j$ in $Q$. For each pair $(i,j)$ with $i \\le j$, add $1$ to the row corresponding to $\\alpha = \\beta_i + \\beta_j$ if $i=j$, and add $2$ if $ij$, where $\\beta_k$ is the exponent tuple of the $k$-th monomial in $v(\\mathbf{x})$.\n- Report the following integers for each test case:\n  1. The dimension $m$ of $Q$.\n  2. The number of scalar decision variables $t$ in the symmetric matrix $Q$, i.e., the number of distinct upper-triangular entries $t = m(m+1)/2$.\n  3. The number of equality constraints $p$ from coefficient matching, which equals the number of monomials of degree at most $2d$ resulting from the expansion.\n  4. The rank $r$ of the matrix $A$.\n\nUse the following test suite of polynomials, each provided as a dictionary mapping exponent tuples to real coefficients. For the tuple $(a_1,\\dots,a_n)$, the monomial is $x_1^{a_1}\\cdots x_n^{a_n}$.\n- Test $1$ (main case, $n=3$, $\\deg(p)=4$):\n  - Variables: $x_1, x_2, x_3$.\n  - Polynomial $p(x_1,x_2,x_3)$ with coefficients:\n    - $(4,0,0) \\mapsto 1.0$, $(0,4,0) \\mapsto 1.0$, $(0,0,4) \\mapsto 1.0$,\n    - $(2,2,0) \\mapsto 2.0$, $(0,2,2) \\mapsto 1.0$, $(2,0,2) \\mapsto 1.5$,\n    - $(3,1,0) \\mapsto 0.5$, $(1,0,3) \\mapsto -0.25$, $(1,1,2) \\mapsto -1.0$,\n    - $(2,0,0) \\mapsto 0.75$, $(0,1,0) \\mapsto 1.2$, $(0,0,0) \\mapsto 3.0$.\n- Test $2$ (boundary case, constant polynomial, $n=3$, $\\deg(p)=0$):\n  - Variables: $x_1, x_2, x_3$.\n  - Polynomial $p(x_1,x_2,x_3)$ with coefficients: $(0,0,0) \\mapsto 5.0$.\n- Test $3$ (single variable, $n=1$, $\\deg(p)=4$):\n  - Variable: $x_1$.\n  - Polynomial $p(x_1)$ with coefficients: $(4,) \\mapsto 2.0$, $(3,) \\mapsto -1.0$, $(2,) \\mapsto 0.5$, $(0,) \\mapsto 1.0$.\n- Test $4$ (higher degree for coverage, $n=2$, $\\deg(p)=6$):\n  - Variables: $x_1, x_2$.\n  - Polynomial $p(x_1,x_2)$ with coefficients:\n    - $(6,0) \\mapsto 3.0$, $(0,6) \\mapsto 1.0$, $(3,3) \\mapsto 1.0$,\n    - $(5,1) \\mapsto -2.0$, $(2,4) \\mapsto 1.5$, $(1,1) \\mapsto 1.0$,\n    - $(0,0) \\mapsto -4.0$.\n\nYour program should, for each test, construct the monomial bases, assemble the matrix $A$, and output a list of lists, where each inner list contains the four integers $[m, t, p, r]$ in this order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, for example, $[[1,1,1,1],[2,3,4,5]]$. No physical units are involved. Angles are not involved. All reported quantities are integers and must be printed exactly as integers in the specified format.", "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed problem in the field of computational algebraic geometry and optimization, specifically concerning Sum-of-Squares (SOS) programming, a key technique in modern control theory. The problem is scientifically grounded, requires no subjective interpretation, and provides all necessary data and definitions for a unique, verifiable solution. We shall proceed with a complete, reasoned solution.\n\nThe objective is to formulate the Semidefinite Program (SDP) for testing whether a given multivariate polynomial $p(\\mathbf{x})$ is a Sum-of-Squares (SOS). The core of this formulation is the Gram-matrix method. We are tasked with constructing the components of this formulation and reporting key dimensional parameters for several test cases. The procedure involves the following steps.\n\nFirst, for a given polynomial $p(\\mathbf{x})$ in $n$ variables, we determine its total degree, which we denote as $\\deg(p)$. The theory of SOS decomposition guarantees that if $p(\\mathbf{x})$ is an SOS polynomial, then $p(\\mathbf{x}) = \\sum_k f_k(\\mathbf{x})^2$, where the degree of each $f_k(\\mathbf{x})$ is at most $\\deg(p)/2$. The problem states that $\\deg(p)$ is an even integer, so we define $d = \\deg(p)/2$.\n\nThe Gram-matrix method provides an equivalent characterization: a polynomial $p(\\mathbf{x})$ with total degree at most $2d$ is an SOS if and only if there exists a symmetric positive semidefinite matrix $Q \\succeq 0$ such that $p(\\mathbf{x}) = v(\\mathbf{x})^\\top Q v(\\mathbf{x})$. Here, $v(\\mathbf{x})$ is a vector containing all monomials in $n$ variables of total degree up to $d$.\n\nThe first quantity to determine is $m$, the dimension of the matrix $Q$. This is equal to the number of monomials in $n$ variables with total degree at most $d$. This is a classic combinatorial problem, and the number is given by the multiset coefficient, or \"stars and bars\" formula:\n$$m = \\binom{n+d}{d} = \\frac{(n+d)!}{n!d!}$$\nThe monomial basis vector $v(\\mathbf{x})$ must be constructed in a specific, fixed order. The problem specifies graded lexicographic order. This means monomials are ordered first by non-decreasing total degree, and ties are broken by lexicographic comparison of their exponent tuples.\n\nThe second quantity is $t$, the number of scalar decision variables. The matrix $Q$ is symmetric, so we only need to consider the entries on or above the main diagonal. For an $m \\times m$ matrix, the number of such entries is:\n$$t = \\frac{m(m+1)}{2}$$\n\nThe third quantity is $p$, the number of linear equality constraints. These constraints arise from equating the coefficients of $p(\\mathbf{x})$ with the coefficients of the polynomial resulting from the expansion $v(\\mathbf{x})^\\top Q v(\\mathbf{x})$. The highest possible degree of a monomial in this expansion is $d+d=2d$. Therefore, the number of constraints $p$ is equal to the number of monomials in $n$ variables with total degree at most $2d$. Similar to $m$, this is given by:\n$$p = \\binom{n+2d}{2d} = \\frac{(n+2d)!}{n!(2d)!}$$\nLet the vector of monomials of degree up to $d$ be $v(\\mathbf{x}) = [v_1(\\mathbf{x}), \\dots, v_m(\\mathbf{x})]^\\top$, where $v_k(\\mathbf{x}) = \\mathbf{x}^{\\beta_k}$ and $\\beta_k$ is the exponent tuple. The expansion is:\n$$v(\\mathbf{x})^\\top Q v(\\mathbf{x}) = \\sum_{i=1}^m \\sum_{j=1}^m Q_{ij} v_i(\\mathbf{x}) v_j(\\mathbf{x}) = \\sum_{i=1}^m \\sum_{j=1}^m Q_{ij} \\mathbf{x}^{\\beta_i + \\beta_j}$$\nDue to the symmetry $Q_{ij} = Q_{ji}$, this sum can be rewritten in terms of the unique variables in $Q$:\n$$ \\sum_{i=1}^m Q_{ii} \\mathbf{x}^{2\\beta_i} + \\sum_{1 \\le i  j \\le m} 2 Q_{ij} \\mathbf{x}^{\\beta_i + \\beta_j} $$\nFor each monomial exponent $\\alpha$ with total degree $|\\alpha| \\le 2d$, its coefficient in the expansion is a linear combination of the entries of $Q$. Specifically, the coefficient of $\\mathbf{x}^{\\alpha}$ is $\\sum_{i,j : \\beta_i+\\beta_j=\\alpha} Q_{ij}$. This gives us a system of $p$ linear equations in $t$ variables. These equations can be represented by a matrix $A$ of size $p \\times t$, such that $A q_{\\text{vec}} = c_p$, where $q_{\\text{vec}}$ is the vector of the $t$ unique entries of $Q$ and $c_p$ is the vector of coefficients of the target polynomial $p(\\mathbf{x})$.\n\nThe construction of the matrix $A$ proceeds as follows:\n1.  Generate the list of $m$ monomial exponent tuples $\\{\\beta_k\\}_{k=1}^m$ for $v(\\mathbf{x})$ (degree $\\le d$) in graded lexicographic order.\n2.  Generate the list of $p$ monomial exponent tuples $\\{\\alpha_l\\}_{l=1}^p$ for the full polynomial (degree $\\le 2d$) in graded lexicographic order. Create a map from each $\\alpha_l$ to its index $l$.\n3.  Create a map from each pair of indices $(i,j)$ with $1 \\le i \\le j \\le m$ to a unique column index from $1$ to $t$.\n4.  Initialize a zero matrix $A$ of size $p \\times t$.\n5.  Iterate through each pair $(i,j)$ with $1 \\le i \\le j \\le m$. Let its corresponding column index in $A$ be $k$.\n    a. Compute the sum of exponents $\\alpha = \\beta_i + \\beta_j$.\n    b. Find the row index $l$ corresponding to this exponent $\\alpha$.\n    c. If $i = j$, the contribution to the coefficient of $\\mathbf{x}^{\\alpha}$ is $Q_{ii}$. Set the entry $A_{l,k} = 1$.\n    d. If $i  j$, the contribution is $2Q_{ij}$. Set the entry $A_{l,k} = 2$.\n\nThe fourth and final quantity to report is $r$, the rank of the matrix $A$. The rank of $A$ represents the number of linearly independent equality constraints. It is not necessarily equal to $p$ because certain sums of exponents $\\beta_i + \\beta_j$ may coincide for different pairs $(i,j)$, leading to relationships between the coefficients. More formally, the mapping from the symmetric matrix $Q$ to the coefficients of $v(\\mathbf{x})^\\top Q v(\\mathbf{x})$ is not always surjective onto the space of all polynomials of degree $2d$. The rank $r$ is the dimension of the image of this linear map, which can be computed using standard numerical methods such as Singular Value Decomposition or QR factorization.\n\nThe computational procedure will execute these steps for each test case provided, calculating and reporting the four integer values $[m, t, p, r]$.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def generate_monomial_exponents(n_vars, max_degree):\n        \"\"\"\n        Generates a list of exponent tuples for n_vars variables up to max_degree.\n        The list is sorted in graded lexicographic order.\n\n        Example: n_vars=2, max_degree=2 - [(0,0), (1,0), (0,1), (2,0), (1,1), (0,2)]\n        \"\"\"\n        exponents = []\n        if n_vars == 0:\n            if max_degree = 0:\n                return [()]\n            else:\n                return []\n        \n        for deg in range(max_degree + 1):\n            # Generate partitions of deg into n_vars parts\n            # This is equivalent to placing n_vars-1 bars in deg+n_vars-1 slots\n            if n_vars == 1:\n                exponents.append((deg,))\n                continue\n\n            current_degree_exps = []\n            \n            # Recursive helper to generate partitions\n            def find_partitions(target, num_parts, current_partition):\n                if num_parts == 1:\n                    current_degree_exps.append(tuple(current_partition + [target]))\n                    return\n\n                for i in range(target + 1):\n                    find_partitions(target - i, num_parts - 1, current_partition + [i])\n            \n            # We want graded lexicographic order, so we need partitions sorted lexicographically.\n            # Building them in reverse order of components gives the correct final sort.\n            \n            def find_lex_partitions(target, num_parts, current_partition):\n                if num_parts == 1:\n                    current_degree_exps.append(tuple(current_partition + [target]))\n                    return\n                # Iterate from highest possible value for the current position downwards\n                for i in range(target, -1, -1):\n                    find_lex_partitions(target - i, num_parts - 1, current_partition + [i])\n            \n            find_lex_partitions(deg, n_vars, [])\n            exponents.extend(current_degree_exps)\n        \n        return exponents\n\n    def process_case(n_vars, poly_coeffs):\n        \"\"\"\n        Processes a single test case to compute m, t, p, r.\n        \"\"\"\n        if not poly_coeffs:\n            # Handle polynomials with no terms, like the zero polynomial\n            max_poly_deg = 0\n        else:\n            max_poly_deg = max(sum(exp) for exp in poly_coeffs.keys())\n        \n        if max_poly_deg % 2 != 0:\n            # As per problem, assume degree is even. Defensive check.\n            raise ValueError(\"Polynomial degree must be even.\")\n            \n        d = max_poly_deg // 2\n        \n        # 1. Generate monomial basis for v(x) (degree = d) and calculate m\n        v_exponents = generate_monomial_exponents(n_vars, d)\n        m = len(v_exponents)\n\n        # 2. Calculate t, the number of decision variables in Q\n        t = m * (m + 1) // 2\n\n        # 3. Generate monomial basis for the full polynomial (degree = 2d) and calculate p\n        p_exponents = generate_monomial_exponents(n_vars, 2 * d)\n        p = len(p_exponents)\n\n        # 4. Build the constraint matrix A and calculate its rank r\n        p_exp_to_row = {exp: i for i, exp in enumerate(p_exponents)}\n        \n        A = np.zeros((p, t))\n        \n        col_idx = 0\n        for i in range(m):\n            for j in range(i, m):\n                beta_i = v_exponents[i]\n                beta_j = v_exponents[j]\n                \n                # Sum of exponents\n                alpha = tuple(b1 + b2 for b1, b2 in zip(beta_i, beta_j))\n                \n                if alpha in p_exp_to_row:\n                    row_idx = p_exp_to_row[alpha]\n                    value = 1.0 if i == j else 2.0\n                    A[row_idx, col_idx] = value\n                \n                col_idx += 1\n        \n        r = np.linalg.matrix_rank(A)\n        \n        return [m, t, p, int(np.round(r))]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, {\n            (4, 0, 0): 1.0, (0, 4, 0): 1.0, (0, 0, 4): 1.0,\n            (2, 2, 0): 2.0, (0, 2, 2): 1.0, (2, 0, 2): 1.5,\n            (3, 1, 0): 0.5, (1, 0, 3): -0.25, (1, 1, 2): -1.0,\n            (2, 0, 0): 0.75, (0, 1, 0): 1.2, (0, 0, 0): 3.0,\n        }),\n        (3, {\n            (0, 0, 0): 5.0,\n        }),\n        (1, {\n            (4,): 2.0, (3,): -1.0, (2,): 0.5, (0,): 1.0,\n        }),\n        (2, {\n            (6, 0): 3.0, (0, 6): 1.0, (3, 3): 1.0,\n            (5, 1): -2.0, (2, 4): 1.5, (1, 1): 1.0,\n            (0, 0): -4.0,\n        }),\n    ]\n\n    results = []\n    for n, coeffs in test_cases:\n        result = process_case(n, coeffs)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "2751036"}, {"introduction": "After mastering the formulation of SOS programs, it is crucial to understand their fundamental scope and limitations. This exercise [@problem_id:2751105] explores the famous Motzkin polynomial, a canonical example of a function that is non-negative everywhere but is not a sum of squares of polynomials. By analyzing this counterexample and its matrix-valued extension, you will grasp the concept of \"conservatism\" in SOS relaxations, a vital piece of knowledge for any researcher or practitioner in the field.", "problem": "Consider the bivariate polynomial \n$$\nm(x_{1},x_{2}) = x_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1 - 3x_{1}^{2}x_{2}^{2},\n$$\nand define the polynomial matrix \n$$\nM(x_{1},x_{2}) = \\begin{pmatrix} m(x_{1},x_{2})  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nWork in the setting of control theory for polynomial systems with Sum of Squares (SOS) optimization. Use only fundamental definitions and well-tested facts to address the following.\n\na) Starting from the definition of pointwise positive semidefiniteness of a polynomial matrix and the arithmetic mean–geometric mean inequality, show that $M(x_{1},x_{2}) \\succeq 0$ for all $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$.\n\nb) Using the definition that a polynomial matrix is a matrix-Sum of Squares (matrix-SOS) if there exists a polynomial matrix $S(x_{1},x_{2})$ with $M(x_{1},x_{2}) = S(x_{1},x_{2})^{\\top} S(x_{1},x_{2})$, and using the well-tested fact that $m(x_{1},x_{2})$ is nonnegative but not a Sum of Squares polynomial, prove that $M(x_{1},x_{2})$ is not matrix-SOS.\n\nc) Briefly discuss the implication of parts (a)–(b) for controller synthesis via SOS optimization, particularly in the context of certifying polynomial matrix inequalities that arise in Lyapunov or dissipation-based designs for nonlinear systems. Your discussion should be conceptually precise and grounded in first principles and standard facts about Sum of Squares Programming (SOSP) and Semidefinite Programming (SDP), but does not need to cite specific theorems beyond common knowledge in the field.\n\nFinally, to produce a calculation-based answer, evaluate the integral\n$$\nI = \\int_{0}^{2\\pi} \\operatorname{tr}\\!\\big(M(\\cos\\theta,\\sin\\theta)\\big)\\, d\\theta,\n$$\nand express your final result in exact closed form with no rounding. No units are required, and angles are in radians.", "solution": "The problem as stated is well-posed and scientifically sound. It is a standard, in fact canonical, example in the study of sum of squares optimization, based on the Motzkin polynomial. We proceed directly to the solution.\n\na) We are tasked to demonstrate that the polynomial matrix $M(x_{1},x_{2}) \\succeq 0$ for all $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$. The matrix is given by\n$$\nM(x_{1},x_{2}) = \\begin{pmatrix} m(x_{1},x_{2})  0 \\\\ 0  0 \\end{pmatrix},\n$$\nwhere $m(x_{1},x_{2}) = x_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1 - 3x_{1}^{2}x_{2}^{2}$.\n\nA symmetric matrix is positive semidefinite if and only if its eigenvalues are all non-negative. The eigenvalues of the diagonal matrix $M(x_{1},x_{2})$ are its diagonal entries, which are $m(x_{1},x_{2})$ and $0$. Therefore, $M(x_{1},x_{2})$ is positive semidefinite if and only if $m(x_{1},x_{2}) \\ge 0$.\n\nWe must prove the non-negativity of the Motzkin polynomial $m(x_{1},x_{2})$ using the arithmetic mean–geometric mean (AM-GM) inequality. The AM-GM inequality for $n$ non-negative real numbers $a_{1}, a_{2}, \\dots, a_{n}$ states that\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} a_{i} \\ge \\sqrt[n]{\\prod_{i=1}^{n} a_{i}}.\n$$\nConsider the three non-negative terms in $m(x_{1},x_{2})$: $x_{1}^{4}x_{2}^{2}$, $x_{1}^{2}x_{2}^{4}$, and $1$. For any $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$, these terms are indeed non-negative. Applying the AM-GM inequality for $n=3$ with $a_{1} = x_{1}^{4}x_{2}^{2}$, $a_{2} = x_{1}^{2}x_{2}^{4}$, and $a_{3} = 1$, we obtain:\n$$\n\\frac{x_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1}{3} \\ge \\sqrt[3]{(x_{1}^{4}x_{2}^{2})(x_{1}^{2}x_{2}^{4})(1)}.\n$$\nThe product inside the cube root simplifies as:\n$$\n(x_{1}^{4}x_{2}^{2})(x_{1}^{2}x_{2}^{4})(1) = x_{1}^{4+2}x_{2}^{2+4} = x_{1}^{6}x_{2}^{6} = (x_{1}^{2}x_{2}^{2})^{3}.\n$$\nThus, the geometric mean is:\n$$\n\\sqrt[3]{(x_{1}^{2}x_{2}^{2})^{3}} = |x_{1}^{2}x_{2}^{2}| = x_{1}^{2}x_{2}^{2},\n$$\nsince $x_{1}^{2}x_{2}^{2}$ is always non-negative. The AM-GM inequality therefore becomes:\n$$\n\\frac{x_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1}{3} \\ge x_{1}^{2}x_{2}^{2}.\n$$\nMultiplying both sides by $3$ yields:\n$$\nx_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1 \\ge 3x_{1}^{2}x_{2}^{2}.\n$$\nRearranging the terms, we arrive at the desired inequality:\n$$\nm(x_{1},x_{2}) = x_{1}^{4}x_{2}^{2} + x_{1}^{2}x_{2}^{4} + 1 - 3x_{1}^{2}x_{2}^{2} \\ge 0.\n$$\nSince $m(x_{1},x_{2}) \\ge 0$ for all $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$, the eigenvalues of $M(x_{1},x_{2})$ are non-negative, and thus $M(x_{1},x_{2}) \\succeq 0$ for all $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$.\n\nb) We are asked to prove that $M(x_{1},x_{2})$ is not a matrix-Sum of Squares (matrix-SOS). The definition of a matrix-SOS is that there exists a polynomial matrix $S(x_{1},x_{2})$ such that $M(x_{1},x_{2}) = S(x_{1},x_{2})^{\\top} S(x_{1},x_{2})$.\n\nLet us assume, for the sake of contradiction, that $M(x_{1},x_{2})$ is a matrix-SOS. Then there exists a polynomial matrix $S(x_{1},x_{2})$, which must be of size $k \\times 2$ for some integer $k \\ge 1$. Let its columns be $s_{1}(x)$ and $s_{2}(x)$, where $x = (x_{1},x_{2})$.\n$$\nS(x) = \\begin{pmatrix} s_{11}(x)  s_{12}(x) \\\\ s_{21}(x)  s_{22}(x) \\\\ \\vdots  \\vdots \\\\ s_{k1}(x)  s_{k2}(x) \\end{pmatrix}.\n$$\nThe product $S(x)^{\\top}S(x)$ is then:\n$$\nS(x)^{\\top}S(x) = \\begin{pmatrix} \\sum_{i=1}^{k} s_{i1}(x)^{2}  \\sum_{i=1}^{k} s_{i1}(x)s_{i2}(x) \\\\ \\sum_{i=1}^{k} s_{i1}(x)s_{i2}(x)  \\sum_{i=1}^{k} s_{i2}(x)^{2} \\end{pmatrix}.\n$$\nEquating this with $M(x) = \\begin{pmatrix} m(x)  0 \\\\ 0  0 \\end{pmatrix}$, we obtain a system of polynomial identities:\n1. $m(x) = \\sum_{i=1}^{k} s_{i1}(x)^{2}$\n2. $0 = \\sum_{i=1}^{k} s_{i1}(x)s_{i2}(x)$\n3. $0 = \\sum_{i=1}^{k} s_{i2}(x)^{2}$\n\nFrom identity (3), we have a sum of squares of polynomials that is identically zero. For real variables, a sum of squares is zero if and only if each term is zero. This means $s_{i2}(x)^{2}$ must be the zero polynomial for each $i \\in \\{1, \\dots, k\\}$. This in turn implies that each polynomial $s_{i2}(x)$ must be the zero polynomial, i.e., $s_{i2}(x) \\equiv 0$ for all $i$.\n\nSubstituting $s_{i2}(x) \\equiv 0$ into identity (2) yields $0=0$, which is consistent.\n\nNow, we examine identity (1). It states that $m(x) = \\sum_{i=1}^{k} s_{i1}(x)^{2}$. This is precisely the definition of $m(x)$ being a Sum of Squares (SOS) polynomial, as it is expressed as a sum of squares of other polynomials, namely the $s_{i1}(x)$.\n\nHowever, the problem provides the well-tested fact that the Motzkin polynomial $m(x_{1},x_{2})$ is non-negative but is *not* a sum of squares of polynomials. Our assumption that $M(x_{1},x_{2})$ is matrix-SOS has led to the conclusion that $m(x_{1},x_{2})$ is an SOS polynomial. This is a direct contradiction of the given fact.\n\nTherefore, the initial assumption must be false. We conclude that $M(x_{1},x_{2})$ is not a matrix-SOS.\n\nc) The results of parts (a) and (b) illustrate a fundamental aspect of Sum of Squares (SOS) optimization in control theory: the conservatism of SOS-based relaxations.\n\nIn controller synthesis and stability analysis for polynomial systems (e.g., via Lyapunov's second method), one frequently encounters constraints of the form $P(x) \\ge 0$ (for scalar polynomials) or $P(x) \\succeq 0$ (for polynomial matrices), where $P(x)$ might depend on system dynamics and unknown controller or Lyapunov function coefficients. Verifying such non-negativity or positive semidefiniteness conditions is computationally intractable in general (NP-hard).\n\nSOS optimization provides a tractable approach by replacing these conditions with stronger, sufficient conditions:\n- For a scalar polynomial $p(x)$, the condition $p(x) \\ge 0$ is replaced by the condition that $p(x)$ is an SOS.\n- For a polynomial matrix $P(x)$, the condition $P(x) \\succeq 0$ is replaced by the condition that $P(x)$ is a matrix-SOS.\n\nThese SOS conditions can be efficiently checked by casting them as Semidefinite Programs (SDPs).\n\nThe matrix $M(x_{1},x_{2})$ from this problem is a concrete example of the gap between semidefiniteness and matrix-SOS. We proved $M(x_{1},x_{2}) \\succeq 0$ for all $x \\in \\mathbb{R}^{2}$ (part a), but also that $M(x_{1},x_{2})$ is not matrix-SOS (part b).\n\nThe implication is as follows: Suppose a control design problem involved a polynomial matrix inequality constraint $P(x, k) \\succeq 0$, where $k$ represents the controller parameters to be found. If, for a certain choice of parameters $k_{0}$, the resulting matrix $P(x, k_{0})$ was precisely our matrix $M(x)$, this would represent a valid solution to the design problem since the semidefiniteness condition holds. However, an SOS-based optimization routine would fail to certify this solution. The corresponding SDP would be infeasible because $M(x)$ is not matrix-SOS. The solver would report that no such controller exists (within the search space), even though a valid one does. This phenomenon is known as the *conservatism* of the SOS relaxation. The set of matrix-SOS polynomials is a strict subset of the set of positive semidefinite polynomial matrices. SOS programming only searches within the smaller, more structured subset, and can therefore miss valid solutions that lie outside it.\n\nFinally, we perform the requested calculation. We must evaluate the integral\n$$\nI = \\int_{0}^{2\\pi} \\operatorname{tr}\\!\\big(M(\\cos\\theta,\\sin\\theta)\\big)\\, d\\theta.\n$$\nThe trace of $M(x_{1},x_{2})$ is $\\operatorname{tr}(M(x_{1},x_{2})) = m(x_{1},x_{2}) + 0 = m(x_{1},x_{2})$.\nWe substitute $x_{1} = \\cos\\theta$ and $x_{2} = \\sin\\theta$ into $m(x_{1},x_{2})$:\n$$\nm(\\cos\\theta, \\sin\\theta) = \\cos^{4}\\theta \\sin^{2}\\theta + \\cos^{2}\\theta \\sin^{4}\\theta + 1 - 3\\cos^{2}\\theta \\sin^{2}\\theta.\n$$\nWe can factor the first two terms:\n$$\n\\cos^{2}\\theta \\sin^{2}\\theta (\\cos^{2}\\theta + \\sin^{2}\\theta) = \\cos^{2}\\theta \\sin^{2}\\theta (1) = \\cos^{2}\\theta \\sin^{2}\\theta.\n$$\nSubstituting this back into the expression for $m(\\cos\\theta, \\sin\\theta)$:\n$$\nm(\\cos\\theta, \\sin\\theta) = (\\cos^{2}\\theta \\sin^{2}\\theta) + 1 - 3\\cos^{2}\\theta \\sin^{2}\\theta = 1 - 2\\cos^{2}\\theta \\sin^{2}\\theta.\n$$\nTo integrate this expression, we use trigonometric identities. Recall that $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta$, so $\\sin^{2}\\theta\\cos^{2}\\theta = \\frac{1}{4}\\sin^{2}(2\\theta)$.\nThe integrand becomes:\n$$\n1 - 2\\left(\\frac{1}{4}\\sin^{2}(2\\theta)\\right) = 1 - \\frac{1}{2}\\sin^{2}(2\\theta).\n$$\nNext, we use the power-reduction formula $\\sin^{2}\\alpha = \\frac{1 - \\cos(2\\alpha)}{2}$. With $\\alpha=2\\theta$, this gives:\n$$\n1 - \\frac{1}{2}\\left(\\frac{1 - \\cos(4\\theta)}{2}\\right) = 1 - \\frac{1}{4} + \\frac{1}{4}\\cos(4\\theta) = \\frac{3}{4} + \\frac{1}{4}\\cos(4\\theta).\n$$\nThe integral is now straightforward to evaluate:\n$$\nI = \\int_{0}^{2\\pi} \\left(\\frac{3}{4} + \\frac{1}{4}\\cos(4\\theta)\\right) d\\theta.\n$$\n$$\nI = \\left[ \\frac{3}{4}\\theta + \\frac{1}{4}\\left(\\frac{\\sin(4\\theta)}{4}\\right) \\right]_{0}^{2\\pi} = \\left[ \\frac{3}{4}\\theta + \\frac{1}{16}\\sin(4\\theta) \\right]_{0}^{2\\pi}.\n$$\nEvaluating at the limits of integration:\n$$\nI = \\left(\\frac{3}{4}(2\\pi) + \\frac{1}{16}\\sin(8\\pi)\\right) - \\left(\\frac{3}{4}(0) + \\frac{1}{16}\\sin(0)\\right).\n$$\n$$\nI = \\left(\\frac{3\\pi}{2} + 0\\right) - (0 + 0) = \\frac{3\\pi}{2}.\n$$\nThe value of the integral is $\\frac{3\\pi}{2}$.", "answer": "$$\\boxed{\\frac{3\\pi}{2}}$$", "id": "2751105"}]}