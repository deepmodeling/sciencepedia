{"hands_on_practices": [{"introduction": "A cornerstone of proving nominal stability for Model Predictive Control is the use of a terminal cost that acts as a local Lyapunov function, ensuring the value function decreases at each step. This exercise [@problem_id:2746597] provides a hands-on opportunity to compute the quadratic terminal cost matrix $P$ by solving the discrete-time Lyapunov equation. This practice solidifies the essential link between this algebraic tool and the fundamental decrease condition required for stability.", "problem": "Consider the discrete-time linear time-invariant system used in Model Predictive Control (MPC) stability analysis,\n$$x_{k+1} = A x_k + B u_k,$$\nwith state feedback control law\n$$u_k = K x_k,$$\nand quadratic stage cost\n$$\\ell(x_k,u_k) = x_k^{\\top} Q x_k + u_k^{\\top} R u_k.$$\nAssume that the closed-loop matrix $A_{K} \\coloneqq A + B K$ is Schur (all eigenvalues lie strictly inside the unit disk). A standard terminal cost candidate for MPC is the quadratic Lyapunov function\n$$V(x) = x^{\\top} P x,$$\nwhere $P \\succ 0$ solves the discrete-time Lyapunov equation\n$$P = A_{K}^{\\top} P A_{K} + \\left(Q + K^{\\top} R K\\right).$$\nThis choice is used to guarantee recursive feasibility and closed-loop stability by ensuring the Lyapunov decrease condition.\n\nFor the specific data\n$$A=\\begin{bmatrix}1 & 1\\\\ 0 & 1\\end{bmatrix},\\quad B=\\begin{bmatrix}0\\\\ 1\\end{bmatrix},\\quad K=\\begin{bmatrix}-1 & -2\\end{bmatrix},\\quad Q=I,\\quad R=I,$$\nperform the following:\n1) Compute the unique symmetric positive definite matrix $P$ that solves the discrete-time Lyapunov equation above.\n2) Using your computed $P$, verify the Lyapunov decrease inequality for all $x$,\n$$V(x^{+})-V(x) \\leq -\\left(x^{\\top} Q x + x^{\\top} K^{\\top} R K x\\right),$$\nwhere $x^{+} = A_{K} x$.\n\nProvide your final answer as the exact $2 \\times 2$ matrix $P$ with rational entries (no rounding). Do not include any inequality in the final answer.", "solution": "The problem presented is a standard exercise in the stability analysis of linear discrete-time systems under feedback control, specifically concerning the construction of a quadratic Lyapunov function. The problem is well-posed and scientifically sound. I will proceed with the solution.\n\nThe task is to solve for the symmetric positive definite matrix $P$ from the discrete-time algebraic Lyapunov equation:\n$$P = A_{K}^{\\top} P A_{K} + \\left(Q + K^{\\top} R K\\right)$$\nand subsequently to verify the Lyapunov decrease condition.\n\nFirst, we must compute the closed-loop system matrix $A_K = A + BK$. The given matrices are:\n$$A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad K = \\begin{bmatrix} -1 & -2 \\end{bmatrix}$$\nThe product $BK$ is:\n$$BK = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} -1 & -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\times (-1) & 0 \\times (-2) \\\\ 1 \\times (-1) & 1 \\times (-2) \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ -1 & -2 \\end{bmatrix}$$\nTherefore, the closed-loop matrix $A_K$ is:\n$$A_K = A + BK = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 0 & 0 \\\\ -1 & -2 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ -1 & -1 \\end{bmatrix}$$\nThe problem states that $A_K$ is Schur. Let us confirm this. The characteristic equation is $\\det(A_K - \\lambda I) = 0$.\n$$\\det \\left( \\begin{bmatrix} 1-\\lambda & 1 \\\\ -1 & -1-\\lambda \\end{bmatrix} \\right) = (1-\\lambda)(-1-\\lambda) - (1)(-1) = -1 - \\lambda + \\lambda + \\lambda^2 + 1 = \\lambda^2 = 0$$\nThe eigenvalues are $\\lambda_{1,2} = 0$. Since all eigenvalues have magnitude $|\\lambda| = 0 < 1$, the matrix $A_K$ is indeed Schur, which guarantees a unique positive definite solution $P$ to the Lyapunov equation for any positive definite weighting matrix $Q + K^{\\top} R K$.\n\nNext, we compute the total weighting matrix for the stage cost, which we denote as $Q_{cl} = Q + K^{\\top} R K$. We are given $Q = I$ and $R = I$. Since the control input $u_k = Kx_k$ is a scalar, $R$ is the scalar $1$.\n$$K^{\\top} R K = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} (1) \\begin{bmatrix} -1 & -2 \\end{bmatrix} = \\begin{bmatrix} (-1)(-1) & (-1)(-2) \\\\ (-2)(-1) & (-2)(-2) \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}$$\nThus, $Q_{cl}$ is:\n$$Q_{cl} = Q + K^{\\top} R K = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}$$\nThe Lyapunov equation is $P = A_K^{\\top} P A_K + Q_{cl}$. Let $P$ be a symmetric matrix $P = \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix}$. We must solve for its elements.\nFirst, we compute the term $A_K^{\\top} P A_K$:\n$$A_K^{\\top} P A_K = \\begin{bmatrix} 1 & -1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ -1 & -1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} p_{11}-p_{12} & p_{12}-p_{22} \\\\ p_{11}-p_{12} & p_{12}-p_{22} \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ -1 & -1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} (p_{11}-p_{12}) - (p_{12}-p_{22}) & (p_{11}-p_{12}) - (p_{12}-p_{22}) \\\\ (p_{11}-p_{12}) - (p_{12}-p_{22}) & (p_{11}-p_{12}) - (p_{12}-p_{22}) \\end{bmatrix}$$\n$$= \\begin{bmatrix} p_{11}-2p_{12}+p_{22} & p_{11}-2p_{12}+p_{22} \\\\ p_{11}-2p_{12}+p_{22} & p_{11}-2p_{12}+p_{22} \\end{bmatrix}$$\nLet the scalar value $c = p_{11}-2p_{12}+p_{22}$. Then $A_K^{\\top} P A_K = c \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$.\nSubstituting this into the Lyapunov equation:\n$$\\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} = \\begin{bmatrix} c & c \\\\ c & c \\end{bmatrix} + \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix} = \\begin{bmatrix} c+2 & c+2 \\\\ c+2 & c+5 \\end{bmatrix}$$\nBy comparing the elements, we obtain a system of equations:\n1) $p_{11} = c+2$\n2) $p_{12} = c+2$\n3) $p_{22} = c+5$\nFrom equations 1) and 2), we find $p_{11} = p_{12}$.\nNow we substitute these into the definition of $c$:\n$$c = p_{11} - 2p_{12} + p_{22} = (c+2) - 2(c+2) + (c+5)$$\n$$c = c+2 - 2c-4 + c+5 = (c-2c+c) + (2-4+5) = 0 \\cdot c + 3$$\n$$c = 3$$\nUsing this value, we find the elements of $P$:\n$$p_{11} = c+2 = 3+2 = 5$$\n$$p_{12} = c+2 = 3+2 = 5$$\n$$p_{22} = c+5 = 3+5 = 8$$\nSo, the solution is the matrix:\n$$P = \\begin{bmatrix} 5 & 5 \\\\ 5 & 8 \\end{bmatrix}$$\nThis matrix is symmetric. We check for positive definiteness using Sylvester's criterion. The leading principal minors are $5 > 0$ and $\\det(P) = 5 \\times 8 - 5 \\times 5 = 40 - 25 = 15 > 0$. Both are positive, so $P$ is positive definite. This completes the first part of the problem.\n\nFor the second part, we must verify the Lyapunov decrease inequality:\n$$V(x^{+}) - V(x) \\leq -\\left(x^{\\top} Q x + x^{\\top} K^{\\top} R K x\\right)$$\nwhere $V(x) = x^{\\top} P x$ and $x^{+} = A_K x$.\nThe left-hand side (LHS) is:\n$$V(x^{+}) - V(x) = (A_K x)^{\\top} P (A_K x) - x^{\\top} P x = x^{\\top} (A_K^{\\top} P A_K - P) x$$\nThe right-hand side (RHS) is:\n$$-\\left(x^{\\top} Q x + x^{\\top} K^{\\top} R K x\\right) = -x^{\\top} (Q + K^{\\top} R K) x$$\nThe inequality is equivalent to showing $x^{\\top} (A_K^{\\top} P A_K - P) x \\leq -x^{\\top} (Q + K^{\\top} R K) x$.\nBy the very definition of $P$ from the Lyapunov equation $P = A_K^{\\top} P A_K + Q + K^{\\top} R K$, we can rearrange it to:\n$$A_K^{\\top} P A_K - P = -(Q + K^{\\top} R K)$$\nThis means that the matrices defining the quadratic forms on both sides of the inequality are not just related by an inequality, but are in fact equal. Substituting this equality into the LHS of the inequality yields:\n$$x^{\\top} (-(Q + K^{\\top} R K)) x = -x^{\\top} (Q + K^{\\top} R K) x$$\nThis is precisely the RHS. Therefore, for a matrix $P$ that solves the Lyapunov equation, the relation holds with equality:\n$$V(x^{+}) - V(x) = -\\left(x^{\\top} Q x + x^{\\top} K^{\\top} R K x\\right)$$\nSince equality implies the \"less than or equal to\" condition, the inequality is trivially verified by the construction of $P$. To be thorough, we can re-insert our computed values to show the matrix equality $A_K^{\\top} P A_K - P = -(Q + K^{\\top} R K)$ holds.\nWe calculated $A_K^{\\top} P A_K = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\end{bmatrix}$ and $P = \\begin{bmatrix} 5 & 5 \\\\ 5 & 8 \\end{bmatrix}$.\n$$A_K^{\\top} P A_K - P = \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\end{bmatrix} - \\begin{bmatrix} 5 & 5 \\\\ 5 & 8 \\end{bmatrix} = \\begin{bmatrix} -2 & -2 \\\\ -2 & -5 \\end{bmatrix}$$\nAnd we calculated $Q_{cl} = Q + K^{\\top} R K = \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}$.\n$$-(Q + K^{\\top} R K) = -\\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix} = \\begin{bmatrix} -2 & -2 \\\\ -2 & -5 \\end{bmatrix}$$\nThe matrices are identical, which confirms our solution for $P$ is correct and verifies the condition.", "answer": "$$\\boxed{\\begin{pmatrix} 5 & 5 \\\\ 5 & 8 \\end{pmatrix}}$$", "id": "2746597"}, {"introduction": "Moving from ideal models to real-world applications requires accounting for uncertainties and disturbances, making constraint satisfaction a primary concern. Robust MPC schemes often guarantee safety by tightening constraints according to a \"back-off\" margin derived from the system's dynamics and disturbance bounds. This practice [@problem_id:2746575] guides you through calculating the minimal Robust Positively Invariant (RPI) set, which quantifies the cumulative effect of disturbances and directly determines this necessary safety margin.", "problem": "Consider a discrete-time Linear Time-Invariant (LTI) closed-loop error system under Model Predictive Control (MPC) with additive bounded disturbance given by the dynamics $e_{k+1} = A_{K} e_{k} + w_{k}$, where $A_{K} \\in \\mathbb{R}^{n \\times n}$ and $w_{k} \\in W$ for all $k \\in \\mathbb{N}$. A set $\\mathcal{E} \\subset \\mathbb{R}^{n}$ is called a Robust Positively Invariant (RPI) set for this system if $A_{K} \\mathcal{E} \\oplus W \\subseteq \\mathcal{E}$, where $\\oplus$ denotes Minkowski sum. In robust MPC, nominal state constraints are typically tightened by the RPI set so that the nominal state remains feasible despite disturbances; specifically, for a symmetric convex constraint set $X$, a common tightening is $X_{\\text{tight}} = X \\ominus \\mathcal{E}$, where $\\ominus$ denotes the Pontryagin difference.\n\nSuppose $A_{K} = 0.5 I$ and the disturbance set is the axis-aligned hypercube $W = \\{ w \\in \\mathbb{R}^{n} : \\|w\\|_{\\infty} \\le 0.1 \\}$. The nominal state constraint set is $X = \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 \\}$. Using only fundamental definitions of RPI sets, Minkowski sums, and limits of set-valued series, derive the minimal RPI set $\\mathcal{E}$ as the limit of the Minkowski sum series generated by repeated propagation of $W$ through the closed-loop dynamics. Then, from first principles, quantify the induced infinity-norm constraint tightening, defined as the scalar $\\tau$ that must be subtracted from each bound of $X$ so that $X_{\\text{tight}} = \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 - \\tau \\}$ satisfies $X_{\\text{tight}} \\oplus \\mathcal{E} \\subseteq X$.\n\nProvide the exact value of the tightening magnitude $\\tau$ as your final answer. No rounding is required. The final answer must be a single real number.", "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Closed-loop error dynamics: $e_{k+1} = A_{K} e_{k} + w_{k}$.\n- System matrix: $A_{K} \\in \\mathbb{R}^{n \\times n}$.\n- Disturbance: $w_{k} \\in W$ for all $k \\in \\mathbb{N}$.\n- Robust Positively Invariant (RPI) set $\\mathcal{E}$: $A_{K} \\mathcal{E} \\oplus W \\subseteq \\mathcal{E}$.\n- Minkowski sum operator: $\\oplus$.\n- Pontryagin difference operator: $\\ominus$.\n- Constraint tightening: $X_{\\text{tight}} = X \\ominus \\mathcal{E}$.\n- Specific system matrix: $A_{K} = 0.5 I$.\n- Specific disturbance set: $W = \\{ w \\in \\mathbb{R}^{n} : \\|w\\|_{\\infty} \\le 0.1 \\}$.\n- Specific nominal state constraint set: $X = \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 \\}$.\n- Task 1: Derive the minimal RPI set $\\mathcal{E}$ as the limit of the Minkowski sum series.\n- Task 2: Quantify the tightening $\\tau$ such that $X_{\\text{tight}} = \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 - \\tau \\}$ satisfies $X_{\\text{tight}} \\oplus \\mathcal{E} \\subseteq X$.\n\nThe problem is scientifically grounded in the established principles of robust control theory and Model Predictive Control (MPC). It is well-posed, as the concepts of minimal RPI sets for stable LTI systems with bounded disturbances are standard and lead to a unique solution. The problem statement is objective, complete, and contains no contradictions. All terms are mathematically defined and consistent with literature in the field. Therefore, the problem is deemed valid and a solution will be furnished.\n\nThe first step is to derive the minimal Robust Positively Invariant (RPI) set, $\\mathcal{E}$, for the given system. The minimal RPI set is the set of all states reachable from the origin ($e_0 = 0$) under the action of all possible disturbance sequences. It is constructed as the infinite Minkowski sum of the disturbance set propagated by the system dynamics.\nThe state at time step $k$, starting from $e_0 = 0$, is given by:\n$$ e_k = \\sum_{i=0}^{k-1} A_K^i w_{k-1-i} $$\nThe set of all possible states at any time is the union of all reachable sets. The minimal RPI set $\\mathcal{E}$ is the limit of these reachable sets as $k \\to \\infty$:\n$$ \\mathcal{E} = \\bigoplus_{i=0}^{\\infty} A_K^i W $$\nThis series converges because the system matrix $A_K$ is Schur stable. The eigenvalues of $A_K = 0.5 I$ are all $0.5$, which have a magnitude less than $1$.\nSubstituting the given $A_K = 0.5 I$:\n$$ A_K^i = (0.5 I)^i = (0.5)^i I $$\nThe minimal RPI set is therefore:\n$$ \\mathcal{E} = \\bigoplus_{i=0}^{\\infty} (0.5)^i I W = \\bigoplus_{i=0}^{\\infty} (0.5)^i W $$\nThe disturbance set $W = \\{ w \\in \\mathbb{R}^{n} : \\|w\\|_{\\infty} \\le 0.1 \\}$ is a convex, centrally symmetric set (an origin-centered hypercube). For such sets, scaling and Minkowski addition have the property that $\\alpha S \\oplus \\beta S = (\\alpha+\\beta)S$ for non-negative scalars $\\alpha, \\beta$. Extending this to the infinite sum, we can write:\n$$ \\mathcal{E} = \\left( \\sum_{i=0}^{\\infty} (0.5)^i \\right) W $$\nThe sum is a geometric series with ratio $r = 0.5$:\n$$ \\sum_{i=0}^{\\infty} (0.5)^i = \\frac{1}{1 - 0.5} = \\frac{1}{0.5} = 2 $$\nThus, the minimal RPI set is:\n$$ \\mathcal{E} = 2 W = 2 \\{ w \\in \\mathbb{R}^{n} : \\|w\\|_{\\infty} \\le 0.1 \\} $$\nTo characterize this set, let $e \\in \\mathcal{E}$. Then $e = 2w$ for some $w \\in W$. The norm of $e$ is $\\|e\\|_{\\infty} = \\|2w\\|_{\\infty} = 2\\|w\\|_{\\infty}$. Since $\\|w\\|_{\\infty} \\le 0.1$, it follows that $\\|e\\|_{\\infty} \\le 2 \\times 0.1 = 0.2$.\nSo, the minimal RPI set is the hypercube:\n$$ \\mathcal{E} = \\{ e \\in \\mathbb{R}^{n} : \\|e\\|_{\\infty} \\le 0.2 \\} $$\n\nThe second step is to determine the tightening magnitude $\\tau$. The tightened constraint set is given as $X_{\\text{tight}} = \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 - \\tau \\}$. This set must satisfy the condition $X_{\\text{tight}} \\oplus \\mathcal{E} \\subseteq X$. This condition ensures that if the nominal state is in $X_{\\text{tight}}$, the true state (nominal plus error) remains within the original constraint set $X$.\n\nLet us analyze the Minkowski sum $X_{\\text{tight}} \\oplus \\mathcal{E}$. Both $X_{\\text{tight}}$ and $\\mathcal{E}$ are origin-centered hypercubes, defined by infinity-norm bounds.\n- $X_{\\text{tight}}$ is a hypercube of infinity-norm radius $1 - \\tau$.\n- $\\mathcal{E}$ is a hypercube of infinity-norm radius $0.2$.\nThe Minkowski sum of two origin-centered hypercubes is another origin-centered hypercube whose radius is the sum of their radii.\nLet $z \\in X_{\\text{tight}} \\oplus \\mathcal{E}$. Then $z = x + e$ for some $x \\in X_{\\text{tight}}$ and $e \\in \\mathcal{E}$. The infinity norm of $z$ is bounded by:\n$$ \\|z\\|_{\\infty} = \\|x+e\\|_{\\infty} \\le \\|x\\|_{\\infty} + \\|e\\|_{\\infty} $$\nThe maximum possible value is attained, so the resulting set is characterized by:\n$$ \\sup_{z \\in X_{\\text{tight}} \\oplus \\mathcal{E}} \\|z\\|_{\\infty} = \\sup_{x \\in X_{\\text{tight}}} \\|x\\|_{\\infty} + \\sup_{e \\in \\mathcal{E}} \\|e\\|_{\\infty} = (1 - \\tau) + 0.2 = 1.2 - \\tau $$\nSo, $X_{\\text{tight}} \\oplus \\mathcal{E} = \\{ z \\in \\mathbb{R}^{n} : \\|z\\|_{\\infty} \\le 1.2 - \\tau \\}$.\nThe condition $X_{\\text{tight}} \\oplus \\mathcal{E} \\subseteq X$ becomes:\n$$ \\{ z \\in \\mathbb{R}^{n} : \\|z\\|_{\\infty} \\le 1.2 - \\tau \\} \\subseteq \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le 1 \\} $$\nFor this set inclusion to hold, the radius of the inner set must be less than or equal to the radius of the outer set:\n$$ 1.2 - \\tau \\le 1 $$\nThis implies $0.2 \\le \\tau$. To ensure feasibility while being minimally conservative, we must choose the smallest possible value for the tightening $\\tau$. The minimal tightening that satisfies the condition is $\\tau = 0.2$.\n\nThis is equivalent to computing the Pontryagin difference $X \\ominus \\mathcal{E}$. By definition, $X \\ominus \\mathcal{E} = \\{ x \\in \\mathbb{R}^n \\mid x \\oplus \\mathcal{E} \\subseteq X \\}$.\nFor an element $x$ to be in this set, we must have $x+e \\in X$ for all $e \\in \\mathcal{E}$. This means $\\|x+e\\|_{\\infty} \\le 1$ for all $\\|e\\|_{\\infty} \\le 0.2$.\nThe maximal value of $\\|x+e\\|_{\\infty}$ for a fixed $x$ is $\\|x\\|_{\\infty} + \\sup_{e \\in \\mathcal{E}}\\|e\\|_{\\infty}$.\nSo, the condition is $\\|x\\|_{\\infty} + 0.2 \\le 1$, which simplifies to $\\|x\\|_{\\infty} \\le 0.8$.\nThis defines the maximally large tightened set: $X_{\\text{tight}} = \\{ x \\in \\mathbb{R}^n \\mid \\|x\\|_{\\infty} \\le 0.8 \\}$.\nComparing this to the given form $X_{\\text{tight}} = \\{ x \\in \\mathbb{R}^n \\mid \\|x\\|_{\\infty} \\le 1 - \\tau \\}$, we equate the bounds:\n$$ 1 - \\tau = 0.8 $$\nSolving for $\\tau$ gives $\\tau = 1 - 0.8 = 0.2$.\nThe tightening magnitude is $\\tau = 0.2$.", "answer": "$$\n\\boxed{0.2}\n$$", "id": "2746575"}, {"introduction": "The theoretical definitions of N-step feasible sets and recursive feasibility are central to MPC, but can feel abstract. A powerful way to build intuition is to explore these concepts computationally. This coding exercise [@problem_id:2746631] challenges you to translate theory into practice by implementing an algorithm that numerically approximates the set of initial states for which a feasible control solution exists, building a deep, practical understanding of how constraints and dynamics define the operational domain of an MPC controller.", "problem": "Consider the discrete-time linear time-invariant system used in Model Predictive Control (MPC), defined by the state-update equation $x_{k+1} = A x_k + B u_k$, where $x_k \\in \\mathbb{R}^2$ and $u_k \\in \\mathbb{R}$. Let $A = \\begin{bmatrix}1 & 1 \\\\ 0 & 1\\end{bmatrix}$ and $B = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. The state constraint set is $X = \\{ x \\in \\mathbb{R}^2 : \\|x\\|_{\\infty} \\le 1 \\}$ and the input constraint set is $U = \\{ u \\in \\mathbb{R} : |u| \\le u_{\\max} \\}$. The $N$-step feasible set $X_N$ is defined recursively from the fundamental admissible predecessor operator as follows: for any set $S \\subseteq \\mathbb{R}^2$, define $\\mathrm{Pre}(S) = \\{ x \\in X : \\exists u \\in U \\text{ such that } A x + B u \\in S \\}$. Then $X_0 = X$ and $X_{k+1} = \\mathrm{Pre}(X_k)$ for $k \\ge 0$. The set $X_N$ contains the initial states $x_0$ for which there exists an input sequence $(u_0,\\dots,u_{N-1})$ with each $u_i \\in U$ such that the predicted states remain in $X$ for $N$ steps.\n\nYour tasks are:\n\n1) Using only the fundamental definitions above and without invoking any precomputed formulas, construct a nonempty inner approximation of $X_3$ by gridding $X$ and discretizing $U$. Specifically:\n- Use a uniform rectangular grid on $X$ with spacing $\\Delta$ in each coordinate. For each grid point $x_0$, decide membership in an inner approximation $\\widehat{X}_N$ by checking if there exists a sequence $(u_0,\\dots,u_{N-1})$ drawn from a finite discretization $U_{\\mathrm{disc}} \\subset U$ such that the $N$-step forward simulation under $x_{k+1} = A x_k + B u_k$ remains inside $X$ at each predicted step. Because $U_{\\mathrm{disc}} \\subset U$ and the grid on $X$ is finite, the resulting set is a computable inner approximation of $X_N$.\n\n2) Verify one-step recursive feasibility via the standard MPC candidate sequence argument: for each $x_0 \\in \\widehat{X}_N$, take any feasible input sequence $(u_0,\\dots,u_{N-1})$ that certifies membership. Form the candidate sequence at the next time by shifting-and-appending, namely $(u_1,\\dots,u_{N-1}, u_N)$. Show feasibility at the next time by finding some $u_N \\in U_{\\mathrm{disc}}$ such that the additional predicted state $x_{N+1} \\in X$. Conclude that the shifted sequence is a feasible candidate for the horizon-$N$ problem starting from $x_1$ with the same constraint sets. Your program should output a boolean indicating whether this one-step recursive feasibility check succeeds for every $x_0 \\in \\widehat{X}_N$.\n\nYou must implement the above using a finite grid over $X$ and a finite discretization of $U$. Use the following test suite (each case specifies $u_{\\max}$, horizon $N$, and grid spacing $\\Delta$; use the same $A$, $B$, and $X$ for all cases):\n- Test 1 (happy path): $u_{\\max} = 0.5$, $N = 3$, $\\Delta = 0.5$.\n- Test 2 (tighter input bound): $u_{\\max} = 0.25$, $N = 3$, $\\Delta = 0.5$.\n- Test 3 (boundary horizon): $u_{\\max} = 0.5$, $N = 1$, $\\Delta = 0.5$.\n- Test 4 (longer horizon): $u_{\\max} = 0.5$, $N = 4$, $\\Delta = 0.5$.\n\nFor all cases, use the discrete input set $U_{\\mathrm{disc}} = \\{-u_{\\max}, 0, u_{\\max}\\}$ and the state grid defined by $\\{ x \\in \\mathbb{R}^2 : x_i \\in \\{-1, -1+\\Delta, \\dots, 1-\\Delta, 1\\}, i=1,2 \\}$.\n\nYour program must:\n- For each test case, compute the cardinality of the inner approximation $\\widehat{X}_N \\subseteq X$ (i.e., the number of grid points $x_0$ from which at least one feasible $N$-step sequence from $U_{\\mathrm{disc}}$ exists that keeps the predicted states in $X$).\n- For each test case, output a boolean that is true if and only if $\\widehat{X}_N$ is nonempty and the one-step recursive feasibility check via the shifted candidate sequence succeeds for every $x_0 \\in \\widehat{X}_N$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a Python-like list of lists. Each inner list corresponds to a test case and contains exactly two elements: first, the integer cardinality of $\\widehat{X}_N$, and second, a boolean indicating whether the candidate-sequence recursive feasibility check passed for all points in $\\widehat{X}_N$ of that case. Do not print any extra text besides this single line.", "solution": "The problem requires the numerical investigation of the finite-horizon feasible set for a discrete-time linear time-invariant system, a fundamental task in the analysis and implementation of Model Predictive Control (MPC). The system is described by the state-update equation $x_{k+1} = A x_k + B u_k$, with given matrices $A = \\begin{bmatrix}1 & 1 \\\\ 0 & 1\\end{bmatrix}$ and $B = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. The state $x_k$ and input $u_k$ are constrained to polytopic sets $X = \\{ x \\in \\mathbb{R}^2 : \\|x\\|_{\\infty} \\le 1 \\}$ and $U = \\{ u \\in \\mathbb{R} : |u| \\le u_{\\max} \\}$, respectively.\n\nThe solution proceeds in two stages as specified. First, we construct a computable inner approximation, denoted $\\widehat{X}_N$, of the true $N$-step feasible set $X_N$. Second, we verify a key property related to recursive feasibility for all points within this computed set.\n\nThe foundation of our numerical approach is the discretization of the continuous state and input spaces. The state constraint set $X$, which is the square $[-1, 1] \\times [-1, 1]$, is replaced by a finite grid of points. For a given spacing $\\Delta$, this grid consists of all points $(x_1, x_2)$ where $x_1, x_2 \\in \\{-1, -1+\\Delta, \\dots, 1-\\Delta, 1\\}$. The input constraint set $U$ is discretized into a finite set $U_{\\mathrm{disc}} = \\{-u_{\\max}, 0, u_{\\max}\\}$. These finite representations transform the problem from one of infinite search to one of combinatorial, albeit potentially large, evaluation.\n\n**Part 1: Construction of the Inner Approximation $\\widehat{X}_N$**\n\nThe $N$-step feasible set $X_N$ is the set of all initial states $x_0 \\in X$ for which there exists an admissible input sequence $(u_0, \\dots, u_{N-1})$ that keeps the entire state trajectory within $X$ for $N$ steps. Our inner approximation $\\widehat{X}_N$ is constructed by restricting both the set of initial states and the set of available inputs to their discrete counterparts.\n\nThe algorithm is as follows:\n1.  Iterate through each initial state $x_0$ on the defined grid over $X$.\n2.  For each $x_0$, systematically test every possible input sequence $(u_0, \\dots, u_{N-1})$ where each $u_k$ is drawn from $U_{\\mathrm{disc}}$. The total number of such sequences is $|U_{\\mathrm{disc}}|^N=3^N$.\n3.  For each candidate sequence, simulate the system forward for $N$ steps using the dynamics $x_{k+1} = A x_k + B u_k$.\n4.  At each step $k \\in \\{1, \\dots, N\\}$, check if the resulting state $x_k$ satisfies the state constraint, i.e., $\\|x_k\\|_{\\infty} \\le 1$.\n5.  If the entire state trajectory $(x_1, \\dots, x_N)$ remains within $X$, the input sequence is deemed \"certifying\" or feasible for the initial state $x_0$.\n6.  An initial state $x_0$ is included in the set $\\widehat{X}_N$ if at least one such certifying sequence is found.\n7.  All certifying sequences for each $x_0 \\in \\widehat{X}_N$ must be stored for use in the second part of the analysis.\n8.  The cardinality of $\\widehat{X}_N$ is the total count of grid points that meet this criterion.\n\nThis forward-simulation or \"brute-force\" method guarantees that $\\widehat{X}_N$ is an inner approximation, i.e., $\\widehat{X}_N \\subseteq X_N$, because it is constructed using a more restrictive set of initial states (a finite grid) and inputs ($U_{\\mathrm{disc}} \\subset U$).\n\n**Part 2: Verification of One-Step Recursive Feasibility**\n\nRecursive feasibility is a cornerstone of MPC stability proofs. It ensures that if a feasible solution to the control problem exists at the current time, a feasible solution will also exist at the next time step. A common method to guarantee this is to show that a candidate solution, constructed from the previous optimal solution, is always available and feasible. The problem asks us to verify a condition that enables this argument.\n\nThe candidate input sequence at time $k+1$ is often formed by taking the previously optimal sequence $(u_1^*, \\dots, u_{N-1}^*)$, shifting it, and appending some new input $u_N$. For this to be a valid strategy, we must ensure that such a $u_N$ can always be found that maintains feasibility.\n\nThe verification algorithm is as follows:\n1.  The overall check passes only if $\\widehat{X}_N$ is non-empty. If $\\widehat{X}_N$ is empty, the check fails.\n2.  If $\\widehat{X}_N$ is non-empty, we must iterate through every point $x_0 \\in \\widehat{X}_N$.\n3.  For each such $x_0$, we must then iterate through *every* one of its certifying sequences $(u_0^*, \\dots, u_{N-1}^*)$ that were found in Part 1. The condition must hold universally.\n4.  For a given pair of $x_0$ and a certifying sequence, compute the terminal state of the corresponding trajectory: $x_N = A x_{N-1} + B u_{N-1}^*$.\n5.  Check if there exists at least one input $u_N \\in U_{\\mathrm{disc}}$ such that the state at the next step, $x_{N+1} = A x_N + B u_N$, satisfies the state constraint $\\|x_{N+1}\\|_{\\infty} \\le 1$. This is done by testing each of the three values in $U_{\\mathrm{disc}}$.\n6.  If, for any $x_0 \\in \\widehat{X}_N$ and any of its certifying sequences, no such $u_N$ can be found, the recursive feasibility property fails. The overall result for the test case is `False`, and the process can be terminated.\n7.  If the condition holds for all points in $\\widehat{X}_N$ and all their respective certifying sequences, the overall result for the test case is `True`.\n\nThis exhaustive check confirms whether the chosen discretization supports the shift-and-append candidate solution strategy, which is a discrete analogue of the properties of the true continuous-space feasible sets. The provided Python code implements this complete validation and verification procedure for each specified test case.", "answer": "```python\nimport numpy as np\nfrom itertools import product\nimport sys\n\ndef solve():\n    \"\"\"\n    Solves the MPC feasibility problem for the given test cases.\n    \"\"\"\n    # The problem statement requests output to be Python-like, which may be interpreted\n    # as having spaces. To be safe and unambiguous, printing with an f-string is robust.\n    # The default boolean representation ('True', 'False') is standard.\n    # Disabling the default formatter's behavior for strictness.\n    # pylint: disable=f-string-without-interpolation, consider-using-f-string\n    \n    # Define matrix A and B from the problem statement\n    A = np.array([[1.0, 1.0], [0.0, 1.0]])\n    B = np.array([[0.0], [1.0]])\n\n    # Define the test cases\n    test_cases = [\n        (0.5, 3, 0.5),   # Test 1 (happy path)\n        (0.25, 3, 0.5),  # Test 2 (tighter input bound)\n        (0.5, 1, 0.5),   # Test 3 (boundary horizon)\n        (0.5, 4, 0.5),   # Test 4 (longer horizon)\n    ]\n\n    all_results = []\n\n    for u_max, N, delta in test_cases:\n        # Discretize input space U\n        U_disc = np.array([-u_max, 0.0, u_max])\n\n        # Generate grid for state space X\n        grid_coords = np.arange(-1.0, 1.0 + delta / 2.0, delta)\n        x0_grid_tuples = list(product(grid_coords, grid_coords))\n\n        # Generate all possible input sequences of length N\n        input_sequences = list(product(U_disc, repeat=N))\n\n        # Dictionary to store initial states in hat_X_N and their certifying sequences\n        hat_X_N_data = {}\n\n        # --- Part 1: Construct inner approximation hat_X_N ---\n        for x0_tuple in x0_grid_tuples:\n            x0 = np.array(x0_tuple).reshape(2, 1)\n            certifying_sequences = []\n\n            for u_seq in input_sequences:\n                is_traj_feasible = True\n                x_k = x0\n                \n                # Simulate system for N steps\n                for k in range(N):\n                    u_k = u_seq[k]\n                    # State update: x_{k+1} = A*x_k + B*u_k\n                    x_k_plus_1 = A @ x_k + B * u_k\n                    # Check state constraints: ||x_{k+1}||_inf <= 1\n                    # A small tolerance is used for robust floating point comparison.\n                    if np.any(np.abs(x_k_plus_1) > 1.0 + 1e-9):\n                        is_traj_feasible = False\n                        break\n                    x_k = x_k_plus_1\n                \n                if is_traj_feasible:\n                    certifying_sequences.append(u_seq)\n            \n            if certifying_sequences:\n                hat_X_N_data[x0_tuple] = certifying_sequences\n\n        cardinality = len(hat_X_N_data)\n\n        # --- Part 2: Verify one-step recursive feasibility ---\n        recursive_feasibility_ok = True\n        if cardinality == 0:\n            # If hat_X_N is empty, the condition is not met.\n            recursive_feasibility_ok = False\n        else:\n            # Check must hold for every x0 in hat_X_N and every one of its certifying sequences\n            for x0_tuple, c_seqs in hat_X_N_data.items():\n                x0 = np.array(x0_tuple).reshape(2, 1)\n                \n                for u_seq in c_seqs:\n                    # Find the terminal state x_N for this trajectory\n                    x_N = x0\n                    for u_k in u_seq:\n                        x_N = A @ x_N + B * u_k\n                        \n                    # Check if there exists a u_N in U_disc s.t. A*x_N + B*u_N is in X\n                    terminal_condition_met = False\n                    for u_N in U_disc:\n                        x_N_plus_1 = A @ x_N + B * u_N\n                        if np.all(np.abs(x_N_plus_1) <= 1.0 + 1e-9):\n                            terminal_condition_met = True\n                            break\n                            \n                    if not terminal_condition_met:\n                        # If check fails for even one sequence, the overall check fails\n                        recursive_feasibility_ok = False\n                        break  # Break from c_seqs loop\n                \n                if not recursive_feasibility_ok:\n                    break  # Break from hat_X_N_data loop\n\n        all_results.append([cardinality, recursive_feasibility_ok])\n\n    # Print results in the required format\n    # Example: [[5, True], [2, False]]\n    print(str(all_results))\n\n# Execute the main function\nsolve()\n```", "id": "2746631"}]}