{"hands_on_practices": [{"introduction": "To truly understand the Unscented Kalman Filter, we must first master its core engine: the Unscented Transform (UT). This exercise guides you through a manual, step-by-step calculation of a single UKF measurement update for a simple nonlinear system. By working through the sigma point generation, propagation, and recombination by hand, you will build a concrete intuition for how the UKF approximates the posterior distribution without resorting to analytical linearization [@problem_id:691357].", "problem": "An Unscented Kalman Filter (UKF) is being used to estimate the state of a two-dimensional dynamic system. The UKF is a powerful Bayesian filtering technique that approximates a probability distribution by a set of deterministically chosen sample points, known as sigma points. These points, when propagated through a non-linear function, can provide a more accurate estimate of the resulting mean and covariance compared to linearization-based methods like the Extended Kalman Filter.\n\nAt a discrete time step $k$, the system state is represented by a vector $x_k = [x_{1,k}, x_{2,k}]^T$. The filter has already completed its prediction step, resulting in a prior (predicted) state estimate $\\hat{x}_{k|k-1}$ and its corresponding error covariance matrix $P_{k|k-1}$. This prior distribution is assumed to be Gaussian, i.e., $x_k \\sim \\mathcal{N}(\\hat{x}_{k|k-1}, P_{k|k-1})$.\n\nYou are given the following:\n1.  The prior state estimate is $\\hat{x}_{k|k-1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n2.  The prior error covariance is a diagonal matrix $P_{k|k-1} = \\begin{pmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{pmatrix}$.\n3.  A scalar measurement $z_k$ is obtained from a sensor. The measurement model is non-linear and is given by:\n    $$z_k = h(x_k) + v_k$$\n    where the function $h: \\mathbb{R}^2 \\to \\mathbb{R}$ is $h(x_k) = x_{1,k} + \\gamma x_{2,k}^2$, and $v_k$ is a zero-mean Gaussian measurement noise with variance $R$, i.e., $v_k \\sim \\mathcal{N}(0, R)$.\n4.  The UKF uses a standard set of parameters for generating sigma points: the state dimension is $n=2$, the primary scaling parameter is $\\alpha_{ukf}=1$, the parameter for incorporating prior knowledge of the distribution is $\\beta=2$ (optimal for Gaussians), and the secondary scaling parameter is $\\kappa=1$.\n\nThe sigma point selection follows the standard algorithm. For a state dimension $n$, a composite scaling parameter $\\lambda$ is defined as $\\lambda = \\alpha_{ukf}^2(n+\\kappa) - n$. The $2n+1$ sigma points $\\mathcal{X}_i$ and their corresponding weights for the mean ($W_i^{(m)}$) and covariance ($W_i^{(c)}$) are given by:\n-   $\\mathcal{X}_0 = \\hat{x}$\n-   $\\mathcal{X}_i = \\hat{x} + (\\sqrt{(n+\\lambda)P})_i, \\quad i=1, \\dots, n$\n-   $\\mathcal{X}_{i+n} = \\hat{x} - (\\sqrt{(n+\\lambda)P})_i, \\quad i=1, \\dots, n$\nwhere $(\\sqrt{M})_i$ is the $i$-th column of the matrix square root of $M$.\n-   $W_0^{(m)} = \\frac{\\lambda}{n+\\lambda}$\n-   $W_0^{(c)} = \\frac{\\lambda}{n+\\lambda} + (1 - \\alpha_{ukf}^2 + \\beta)$\n-   $W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(n+\\lambda)}, \\quad i=1, \\dots, 2n$\n\nYour task is to perform the measurement update step of the UKF to compute the posterior (updated) state covariance matrix $P_{k|k}$.\n\n**Problem:**\nCalculate the top-left element, $(P_{k|k})_{11}$, of the posterior state covariance matrix.", "solution": "The solution involves performing a standard UKF measurement update.\n1.  **Parameters and Weights:** Given $n=2, \\alpha_{ukf}=1, \\kappa=1, \\beta=2$, the composite scaling parameter is $\\lambda = \\alpha_{ukf}^2(n+\\kappa)-n = 1^2(2+1)-2 = 1$. The weights are:\n    *   $W_0^{(m)} = \\frac{\\lambda}{n+\\lambda} = \\frac{1}{3}$\n    *   $W_0^{(c)} = \\frac{\\lambda}{n+\\lambda} + (1-\\alpha_{ukf}^2+\\beta) = \\frac{1}{3} + (1-1+2) = \\frac{7}{3}$\n    *   $W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(n+\\lambda)} = \\frac{1}{6}$ for $i=1,\\dots,4$.\n\n2.  **Sigma Points and Propagation:** With a prior mean $\\hat{x}_{k|k-1}=0$ and covariance $P_{k|k-1}=\\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$, the sigma points $\\mathcal{X}_i$ are propagated through the measurement function $h(x) = x_1 + \\gamma x_2^2$ to yield measurement points $\\mathcal{Z}_i$:\n    *   $\\mathcal{X}_0=\\begin{pmatrix}0\\\\0\\end{pmatrix} \\implies \\mathcal{Z}_0 = 0$\n    *   $\\mathcal{X}_{1,3}=\\begin{pmatrix}\\pm\\sqrt{3}\\sigma_1\\\\0\\end{pmatrix} \\implies \\mathcal{Z}_{1,3} = \\pm\\sqrt{3}\\sigma_1$\n    *   $\\mathcal{X}_{2,4}=\\begin{pmatrix}0\\\\\\pm\\sqrt{3}\\sigma_2\\end{pmatrix} \\implies \\mathcal{Z}_{2,4} = \\gamma(\\pm\\sqrt{3}\\sigma_2)^2 = 3\\gamma\\sigma_2^2$\n\n3.  **Predicted Measurement and Covariances:**\n    *   Predicted measurement: $\\hat{z}_k = \\sum W_i^{(m)}\\mathcal{Z}_i = \\frac{1}{6}(3\\gamma\\sigma_2^2) + \\frac{1}{6}(3\\gamma\\sigma_2^2) = \\gamma\\sigma_2^2$.\n    *   Innovation covariance: $S_k = \\sum W_i^{(c)}(\\mathcal{Z}_i-\\hat{z}_k)^2 + R$. This sum evaluates to $\\sigma_1^2 + 4\\gamma^2\\sigma_2^4$. Thus, $S_k = \\sigma_1^2 + 4\\gamma^2\\sigma_2^4 + R$.\n    *   Cross-covariance: $P_{xz} = \\sum W_i^{(c)}(\\mathcal{X}_i-\\hat{x}_{k|k-1})(\\mathcal{Z}_i-\\hat{z}_k)$. This sum evaluates to $\\begin{pmatrix}\\sigma_1^2 \\\\ 0\\end{pmatrix}$.\n\n4.  **Posterior Covariance Update:** The Kalman gain is $K=P_{xz}S_k^{-1}$. The posterior covariance is $P_{k|k}=P_{k|k-1} - K S_k K^T = P_{k|k-1} - P_{xz}S_k^{-1}P_{xz}^T$. The top-left element $(P_{k|k})_{11}$ is:\n    $(P_{k|k-1})_{11} - \\frac{(P_{xz})_1^2}{S_k} = \\sigma_1^2 - \\frac{(\\sigma_1^2)^2}{\\sigma_1^2 + 4\\gamma^2\\sigma_2^4 + R}$", "answer": "$$\\boxed{\\sigma_1^2-\\frac{\\sigma_1^4}{\\sigma_1^2+4\\gamma^2\\sigma_2^4+R}}$$", "id": "691357"}, {"introduction": "With a mechanical understanding of the UKF in place, we now turn to the theoretical justification for its superior performance over the Extended Kalman Filter (EKF). This analytical problem asks you to derive the prediction biases of both filters for a nonlinear system [@problem_id:2886769]. This exercise reveals that the UKF's prediction mean is accurate to the second order of the state covariance $P$, a significant improvement over the EKF's first-order accuracy, and demonstrates why the UKF is often the preferred choice for highly nonlinear problems.", "problem": "Consider the scalar, discrete-time, nonlinear stochastic system\n$$\nx_{k+1} \\;=\\; f(x_k) \\;+\\; w_k, \n\\qquad f(x) \\;=\\; x \\;+\\; \\frac{1}{2}\\sin(x),\n$$\nwith a nonlinear measurement\n$$\ny_k \\;=\\; h(x_k) \\;+\\; v_k, \n\\qquad h(x) \\;=\\; x^2,\n$$\nwhere $w_k$ and $v_k$ are mutually independent, zero-mean, Gaussian random variables independent of $x_k$. Suppose that, conditioned on past data, the prior for $x_k$ is Gaussian,\n$$\nx_k \\mid \\mathcal{Y}_{k-1} \\;\\sim\\; \\mathcal{N}(\\mu, P),\n$$\nwith small variance $P>0$. Define the true one-step predicted state mean and the true predicted measurement mean as\n$$\n\\bar{x}_{k+1}^{\\mathrm{true}} \\;=\\; \\mathbb{E}\\!\\left[f(x_k)\\right], \n\\qquad \n\\bar{y}_k^{\\mathrm{true}} \\;=\\; \\mathbb{E}\\!\\left[h(x_k)\\right],\n$$\nwhere expectations are with respect to the Gaussian prior of $x_k$.\n\nDefine the predicted means under the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) as follows. For the EKF, use first-order linearization about the prior mean so that\n$$\n\\bar{x}_{k+1}^{\\mathrm{EKF}} \\;=\\; f(\\mu), \n\\qquad \n\\bar{y}_k^{\\mathrm{EKF}} \\;=\\; h(\\mu).\n$$\nFor the UKF, use the standard Unscented Transform (UT) in one dimension ($n=1$) with scaling parameter $\\alpha>0$ and secondary scaling $\\kappa \\in \\mathbb{R}$, and define\n$$\n\\lambda \\;=\\; \\alpha^2 (n+\\kappa) \\;-\\; n \\;=\\; \\alpha^2(1+\\kappa)\\;-\\;1,\n\\quad\\text{assume } 1+\\lambda>0.\n$$\nUse the sigma points and mean-weights\n$$\n\\chi_0 \\;=\\; \\mu, \n\\qquad \n\\chi_{\\pm} \\;=\\; \\mu \\,\\pm\\, \\sqrt{(1+\\lambda)\\,P},\n$$\n$$\nW_0^{(m)} \\;=\\; \\frac{\\lambda}{1+\\lambda}, \n\\qquad \nW_{+}^{(m)} \\;=\\; W_{-}^{(m)} \\;=\\; \\frac{1}{2(1+\\lambda)}.\n$$\nThe UKF predicted means are then\n$$\n\\bar{x}_{k+1}^{\\mathrm{UKF}} \\;=\\; \\sum_{i\\in\\{0,+,-\\}} W_i^{(m)}\\, f(\\chi_i),\n\\qquad\n\\bar{y}_k^{\\mathrm{UKF}} \\;=\\; \\sum_{i\\in\\{0,+,-\\}} W_i^{(m)}\\, h(\\chi_i).\n$$\n\nFor each mapping $g \\in \\{f,h\\}$, define the prediction bias of a method as the difference between the method’s predicted mean and the true predicted mean,\n$$\nb_g^{\\mathrm{method}} \\;=\\; \\bar{g}^{\\mathrm{method}} \\;-\\; \\bar{g}^{\\mathrm{true}}.\n$$\nCompute, to first nonvanishing order in the small variance $P$, the four biases\n$$\nb_f^{\\mathrm{EKF}},\\quad b_f^{\\mathrm{UKF}},\\quad b_h^{\\mathrm{EKF}},\\quad b_h^{\\mathrm{UKF}},\n$$\nas explicit analytic expressions in terms of $\\mu$, $P$, and $\\lambda$ (if present). Report your final result as a single $1\\times 4$ row vector\n$$\n\\bigl[\\, b_f^{\\mathrm{EKF}} \\;\\; b_f^{\\mathrm{UKF}} \\;\\; b_h^{\\mathrm{EKF}} \\;\\; b_h^{\\mathrm{UKF}} \\,\\bigr].\n$$\nNo rounding is required, and no units are needed. Express any angles in radians.", "solution": "To find the biases, we first find the true expected value of a generic function $g(x_k)$ where $x_k \\sim \\mathcal{N}(\\mu, P)$ using a Taylor series expansion around $\\mu$. The expectation of the expansion yields:\n$$ \\bar{g}^{\\mathrm{true}} = \\mathbb{E}[g(x_k)] = g(\\mu) + \\frac{g''(\\mu)}{2}P + \\frac{g^{(4)}(\\mu)}{8}P^2 + O(P^3) $$\n\n**EKF Biases:** The EKF prediction is $\\bar{g}^{\\mathrm{EKF}} = g(\\mu)$. The bias is $b_g^{\\mathrm{EKF}} = \\bar{g}^{\\mathrm{EKF}} - \\bar{g}^{\\mathrm{true}} = -\\frac{g''(\\mu)}{2}P + O(P^2)$.\n- For $f(x) = x + \\frac{1}{2}\\sin(x)$, $f''(\\mu) = -\\frac{1}{2}\\sin(\\mu)$, so $b_f^{\\mathrm{EKF}} = \\frac{1}{4}\\sin(\\mu)P$.\n- For $h(x) = x^2$, $h''(\\mu) = 2$, so $b_h^{\\mathrm{EKF}} = -P$.\n\n**UKF Biases:** The UKF prediction for the mean, after expanding the sigma point propagations, is:\n$$ \\bar{g}^{\\mathrm{UKF}} = g(\\mu) + \\frac{g''(\\mu)}{2}P + \\frac{(1+\\lambda)g^{(4)}(\\mu)}{24}P^2 + O(P^3) $$\nThe bias is $b_g^{\\mathrm{UKF}} = \\bar{g}^{\\mathrm{UKF}} - \\bar{g}^{\\mathrm{true}} = \\left(\\frac{1+\\lambda}{24} - \\frac{1}{8}\\right)g^{(4)}(\\mu)P^2 + O(P^3) = \\frac{\\lambda-2}{24}g^{(4)}(\\mu)P^2 + O(P^3)$.\n- For $f(x)$, $f^{(4)}(\\mu) = \\frac{1}{2}\\sin(\\mu)$, so $b_f^{\\mathrm{UKF}} = \\frac{\\lambda-2}{48}\\sin(\\mu)P^2$.\n- For $h(x)$, $h^{(4)}(\\mu) = 0$, so $b_h^{\\mathrm{UKF}} = 0$.\n\nCombining these results gives the final vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4}P\\sin(\\mu) & \\frac{(\\lambda-2)}{48}P^2\\sin(\\mu) & -P & 0 \\end{pmatrix}}\n$$", "id": "2886769"}, {"introduction": "Moving from theory to practice, this exercise involves implementing a complete Unscented Kalman Filter to track the state of a nonlinear pendulum. You will build the full prediction-update loop and test it against a ground-truth trajectory, applying your knowledge in a realistic simulation environment [@problem_id:2756735]. This practice is essential for developing practical skills, as it also introduces standard filter consistency checks—such as the Normalized Innovation Squared (NIS) and Normalized Estimation Error Squared (NEES)—which are critical for validating and debugging any Kalman filter implementation.", "problem": "Consider a discrete-time, nonlinear state-estimation problem for a planar pendulum with state vector $x_k = [\\theta_k,\\ \\omega_k]^\\top$, where $\\theta_k$ is the angular displacement and $\\omega_k$ is the angular velocity at discrete time $k$. The continuous-time dynamics are approximated over a sampling interval $\\Delta t$ by the forward-Euler scheme to yield the discrete-time process model\n$$\n\\begin{aligned}\n\\theta_{k+1} &= \\theta_k + \\Delta t\\, \\omega_k,\\\\\n\\omega_{k+1} &= \\omega_k + \\Delta t\\left(-\\frac{g}{L}\\sin(\\theta_k)\\right) + w_k,\n\\end{aligned}\n$$\nand the measurement model\n$$\nz_k = \\sin(\\theta_k) + v_k.\n$$\nHere $g$ is the gravitational acceleration, $L$ is the pendulum length, $w_k$ is a scalar process noise added to the angular velocity channel, and $v_k$ is a scalar measurement noise. All angles must be treated in radians. The Unscented Kalman Filter (UKF) must be used to estimate the state $x_k$ from the measurements $z_k$.\n\nYou must implement the UKF starting from the Bayesian filtering recursion and the definition of the unscented transform for Gaussian priors, using sigma points and weights determined from parameters $\\alpha$, $\\beta$, and $\\kappa$. Do not assume any linearization; you must propagate sigma points through the nonlinear process and measurement maps. The process noise covariance is $Q = \\operatorname{diag}(0,\\ q)$, where $q$ is a scalar variance applied only to the $\\omega$ channel, and the measurement noise variance is $R = r$. The UKF must use the additive noise formulation.\n\nTo enable deterministic and reproducible testing without stochastic sampling, the ground-truth trajectory and measurements must be generated using the following deterministic “noise” sequences, which you must treat as the actual disturbances acting on the system:\n- For each step $k \\in \\{1,2,\\dots,T\\}$, define\n$w_k = \\sqrt{q}\\, \\frac{1}{2}\\left(\\sin(k) + \\cos(2k)\\right)$, and $v_k = \\sqrt{r}\\, \\frac{1}{2}\\left(\\sin(0.3k) - \\cos(0.7k)\\right)$.\n- The ground-truth state is propagated as\n$$\n\\begin{aligned}\n\\theta_{k} &= \\theta_{k-1} + \\Delta t\\, \\omega_{k-1},\\\\\n\\omega_{k} &= \\omega_{k-1} + \\Delta t\\left(-\\frac{g}{L}\\sin(\\theta_{k-1})\\right) + w_k,\n\\end{aligned}\n$$\nwith initial condition $x_0 = [\\theta_0,\\ \\omega_0]^\\top$.\n- The measurement is\n$z_k = \\sin(\\theta_k) + v_k$.\n\nAt each step $k$, let $\\hat{x}_{k|k-1}$ and $P_{k|k-1}$ denote the predicted state mean and covariance, $\\hat{z}_{k|k-1}$ and $S_k$ denote the predicted measurement mean and innovation covariance, and let the innovation (also called the residual) be\n$$\n\\nu_k = z_k - \\hat{z}_{k|k-1}.\n$$\nLet $\\hat{x}_{k|k}$ and $P_{k|k}$ denote the posterior state mean and covariance after incorporating $z_k$. Define the normalized innovation squared (NIS) and normalized estimation error squared (NEES) at step $k$ as\n$$\n\\operatorname{NIS}_k = \\nu_k^\\top S_k^{-1} \\nu_k, \\qquad \\operatorname{NEES}_k = e_k^\\top P_{k|k}^{-1} e_k,\n$$\nwhere $e_k = x_k - \\hat{x}_{k|k}$ is the estimation error relative to ground truth $x_k$. For a scalar measurement, the whitened innovation is defined as\n$$\n\\tilde{\\nu}_k = \\frac{\\nu_k}{\\sqrt{S_k}}.\n$$\nGiven a sequence $\\{\\tilde{\\nu}_k\\}_{k=1}^T$, define the sample mean\n$$\n\\bar{\\nu} = \\frac{1}{T} \\sum_{k=1}^T \\tilde{\\nu}_k,\n$$\nand for each lag $\\ell \\in \\{1,2,3\\}$, the sample autocorrelation coefficient\n$$\n\\rho(\\ell) = \\frac{\\sum_{k=\\ell+1}^{T} (\\tilde{\\nu}_k - \\bar{\\nu})(\\tilde{\\nu}_{k-\\ell} - \\bar{\\nu})}{\\sum_{k=1}^{T} (\\tilde{\\nu}_k - \\bar{\\nu})^2}.\n$$\nYour program must:\n- Implement the Unscented Kalman Filter (UKF) using sigma points and weights derived from $(\\alpha,\\ \\beta,\\ \\kappa)$ for a state dimension of $n = 2$ and a scalar measurement.\n- Simulate the deterministic ground-truth trajectory and measurements using the formulas above.\n- Compute the sequences $\\{\\operatorname{NIS}_k\\}_{k=1}^T$, $\\{\\tilde{\\nu}_k\\}_{k=1}^T$, and $\\{\\operatorname{NEES}_k\\}_{k=1}^T$.\n- Report for each test case:\n  1. The time average of $\\operatorname{NIS}_k$, i.e., $\\frac{1}{T}\\sum_{k=1}^T \\operatorname{NIS}_k$.\n  2. The maximum absolute sample autocorrelation over lags $\\ell \\in \\{1,2,3\\}$, i.e., $\\max_{\\ell \\in \\{1,2,3\\}} |\\rho(\\ell)|$.\n  3. The time average of $\\operatorname{NEES}_k$, i.e., $\\frac{1}{T}\\sum_{k=1}^T \\operatorname{NEES}_k$.\n\nAll angles must be treated in radians. The physical constants are $g = 9.81$ in $\\mathrm{m/s^2}$ and $L = 1.0$ in $\\mathrm{m}$. Time is in $\\mathrm{s}$. The outputs are dimensionless real numbers and must be reported as floating-point numbers rounded to six decimal places. No other units are required in the output.\n\nTest Suite. Your program must run the following three test cases and aggregate their results in the order specified. For each case, use the given parameters and initial conditions:\n- Case A (nominal):\n  - $\\Delta t = 0.05$,\n  - $T = 60$,\n  - $q = (0.05)^2$,\n  - $r = (0.05)^2$,\n  - $x_0 = [0.3,\\ 0.0]^\\top$,\n  - $P_0 = \\operatorname{diag}((0.1)^2,\\ (0.1)^2)$,\n  - $\\alpha = 0.8$, $\\beta = 2.0$, $\\kappa = 0.0$.\n- Case B (low noise, fast sampling):\n  - $\\Delta t = 0.02$,\n  - $T = 80$,\n  - $q = (0.01)^2$,\n  - $r = (0.001)^2$,\n  - $x_0 = [0.25,\\ 0.05]^\\top$,\n  - $P_0 = \\operatorname{diag}((0.05)^2,\\ (0.05)^2)$,\n  - $\\alpha = 0.7$, $\\beta = 2.0$, $\\kappa = 0.0$.\n- Case C (high noise, slower sampling):\n  - $\\Delta t = 0.10$,\n  - $T = 50$,\n  - $q = (0.10)^2$,\n  - $r = (0.20)^2$,\n  - $x_0 = [0.4,\\ -0.1]^\\top$,\n  - $P_0 = \\operatorname{diag}((0.2)^2,\\ (0.2)^2)$,\n  - $\\alpha = 0.5$, $\\beta = 2.0$, $\\kappa = 0.0$.\n\nFinal Output Format. Your program should produce a single line of output containing the nine results (three per case, in the order listed above) as a comma-separated list enclosed in square brackets. Each floating-point number must be rounded to six decimal places. The output ordering is:\n$[\\overline{\\operatorname{NIS}}_{\\mathrm{A}},\\ \\max_{\\ell \\in \\{1,2,3\\}}|\\rho_{\\mathrm{A}}(\\ell)|,\\ \\overline{\\operatorname{NEES}}_{\\mathrm{A}},\\ \\overline{\\operatorname{NIS}}_{\\mathrm{B}},\\ \\max_{\\ell \\in \\{1,2,3\\}}|\\rho_{\\mathrm{B}}(\\ell)|,\\ \\overline{\\operatorname{NEES}}_{\\mathrm{B}},\\ \\overline{\\operatorname{NIS}}_{\\mathrm{C}},\\ \\max_{\\ell \\in \\{1,2,3\\}}|\\rho_{\\mathrm{C}}(\\ell)|,\\ \\overline{\\operatorname{NEES}}_{\\mathrm{C}}]$.\nFor example, a syntactically correct output looks like $[1.234567,0.012345,1.987654, \\dots]$.", "solution": "The solution involves implementing the full Unscented Kalman Filter prediction-update loop for the given nonlinear pendulum system.\n\n**1. Initialization:**\nThe filter is initialized with the prior state estimate $\\hat{x}_{0|0}$ and covariance $P_{0|0}$ provided in each test case. UKF parameters $(\\alpha, \\beta, \\kappa)$ are used to compute the constant sigma point weights $W^{(m)}$ and $W^{(c)}$.\n\n**2. Prediction-Update Loop:**\nFor each time step $k=1, \\dots, T$:\n*   **Prediction:**\n    1.  A set of $2n+1=5$ sigma points is generated based on the posterior estimate from the previous step, $\\hat{x}_{k-1|k-1}$ and $P_{k-1|k-1}$.\n    2.  Each sigma point is propagated through the nonlinear discrete-time dynamics function $f(x) = [\\theta + \\Delta t\\,\\omega, \\omega - \\Delta t(g/L)\\sin(\\theta)]^\\top$.\n    3.  The propagated sigma points are recombined using weights $W^{(m)}$ and $W^{(c)}$ to compute the predicted state mean $\\hat{x}_{k|k-1}$ and predicted state covariance.\n    4.  The process noise covariance $Q = \\operatorname{diag}(0, q)$ is added to the predicted state covariance to get the final prior covariance $P_{k|k-1}$.\n*   **Update:**\n    1.  The sigma points from the prediction step are propagated through the nonlinear measurement function $h(x) = \\sin(\\theta)$.\n    2.  The resulting measurement points are recombined to compute the predicted measurement mean $\\hat{z}_{k|k-1}$ and the innovation covariance $S_k$. The measurement noise variance $R=r$ is added to $S_k$.\n    3.  The cross-covariance $P_{xz}$ between the state and measurement is calculated.\n    4.  The Kalman gain $K_k = P_{xz} S_k^{-1}$ is computed.\n    5.  The state estimate is updated using the actual measurement $z_k$ (from the deterministic ground-truth simulation): $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (z_k - \\hat{z}_{k|k-1})$.\n    6.  The state covariance is updated: $P_{k|k} = P_{k|k-1} - K_k S_k K_k^\\top$.\n\n**3. Analysis:**\nAfter the loop completes, the stored sequences of NIS, NEES, and whitened innovations are used to compute the required statistics: the time average of NIS and NEES, and the maximum absolute autocorrelation of the whitened innovations for lags 1, 2, and 3. This process is repeated for all three test cases to generate the final output vector.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the UKF simulation for all test cases.\n    \"\"\"\n    # Physical and system constants\n    g = 9.81\n    L = 1.0\n    n = 2  # State dimension: [theta, omega]\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Case A: nominal\n        {'name': 'A', 'dt': 0.05, 'T': 60, 'q': 0.05**2, 'r': 0.05**2,\n         'x0': np.array([[0.3], [0.0]]), 'P0': np.diag([0.1**2, 0.1**2]),\n         'alpha': 0.8, 'beta': 2.0, 'kappa': 0.0},\n        # Case B: low noise, fast sampling\n        {'name': 'B', 'dt': 0.02, 'T': 80, 'q': 0.01**2, 'r': 0.001**2,\n         'x0': np.array([[0.25], [0.05]]), 'P0': np.diag([0.05**2, 0.05**2]),\n         'alpha': 0.7, 'beta': 2.0, 'kappa': 0.0},\n        # Case C: high noise, slower sampling\n        {'name': 'C', 'dt': 0.10, 'T': 50, 'q': 0.10**2, 'r': 0.20**2,\n         'x0': np.array([[0.4], [-0.1]]), 'P0': np.diag([0.2**2, 0.2**2]),\n         'alpha': 0.5, 'beta': 2.0, 'kappa': 0.0},\n    ]\n\n    # Process models as functions\n    def f_process(x, dt):\n        theta, omega = x[0, 0], x[1, 0]\n        theta_next = theta + dt * omega\n        omega_next = omega + dt * (-g / L * np.sin(theta))\n        return np.array([[theta_next], [omega_next]])\n\n    def h_measure(x):\n        theta = x[0, 0]\n        return np.sin(theta)\n\n    def run_ukf_simulation(params):\n        \"\"\"\n        Runs the UKF simulation for a single test case.\n        \"\"\"\n        dt, T, q, r = params['dt'], params['T'], params['q'], params['r']\n        alpha, beta, kappa = params['alpha'], params['beta'], params['kappa']\n        x0, P0 = params['x0'], params['P0']\n        \n        Q = np.diag([0, q])\n        R_scalar = r\n\n        # --- UKF Weights and Parameters ---\n        lambda_param = alpha**2 * (n + kappa) - n\n        \n        W_m = np.full(2 * n + 1, 1 / (2 * (n + lambda_param)))\n        W_c = np.full(2 * n + 1, 1 / (2 * (n + lambda_param)))\n        \n        W_m[0] = lambda_param / (n + lambda_param)\n        W_c[0] = lambda_param / (n + lambda_param) + (1 - alpha**2 + beta)\n\n        # --- Initialization ---\n        x_est = x0.copy()  # Posterior estimate at k=0\n        P_est = P0.copy()  # Posterior covariance at k=0\n        x_true = x0.copy() # Ground truth at k=0\n\n        nis_history = []\n        nees_history = []\n        whitened_nu_history = []\n        \n        # --- Main Filter Loop ---\n        for k in range(1, T + 1):\n            # 1. Ground Truth Generation\n            w_k = np.sqrt(q) * 0.5 * (np.sin(k) + np.cos(2 * k))\n            v_k = np.sqrt(r) * 0.5 * (np.sin(0.3 * k) - np.cos(0.7 * k))\n            \n            x_true = f_process(x_true, dt) + np.array([[0], [w_k]])\n            z_k = h_measure(x_true) + v_k\n\n            # 2. UKF Prediction\n            # Generate sigma points from estimate at k-1\n            sqrt_P = np.linalg.cholesky((n + lambda_param) * P_est)\n            sigma_points = np.zeros((2 * n + 1, n, 1))\n            sigma_points[0] = x_est\n            for i in range(n):\n                sigma_points[i + 1]       = x_est + sqrt_P[:, i:i+1]\n                sigma_points[i + 1 + n] = x_est - sqrt_P[:, i:i+1]\n            \n            # Propagate sigma points through process model\n            propagated_sigma_points = np.array([f_process(sp, dt) for sp in sigma_points])\n\n            # Predicted state mean and covariance\n            x_pred = np.sum(W_m[:, np.newaxis, np.newaxis] * propagated_sigma_points, axis=0)\n            \n            P_pred = np.zeros((n, n))\n            for i in range(2 * n + 1):\n                diff = propagated_sigma_points[i] - x_pred\n                P_pred += W_c[i] * (diff @ diff.T)\n            P_pred += Q\n\n            # 3. UKF Update\n            # Transform sigma points through measurement model\n            measurement_sigma_points = np.array([h_measure(sp) for sp in propagated_sigma_points])\n            \n            # Predicted measurement mean\n            z_pred = np.sum(W_m * measurement_sigma_points)\n            \n            # Innovation covariance\n            S_k = np.sum(W_c * (measurement_sigma_points - z_pred)**2) + R_scalar\n            \n            # Cross-covariance\n            P_xz = np.zeros((n, 1))\n            for i in range(2 * n + 1):\n                diff_x = propagated_sigma_points[i] - x_pred\n                diff_z = measurement_sigma_points[i] - z_pred\n                P_xz += W_c[i] * diff_x * diff_z\n                \n            # Kalman Gain\n            K_k = P_xz / S_k\n            \n            # Innovation\n            nu_k = z_k - z_pred\n            \n            # Update state estimate and covariance\n            x_est = x_pred + K_k * nu_k\n            P_est = P_pred - K_k * S_k * K_k.T\n            \n            # 4. Metrics Calculation\n            nis_k = nu_k**2 / S_k\n            nis_history.append(nis_k)\n\n            whitened_nu_k = nu_k / np.sqrt(S_k)\n            whitened_nu_history.append(whitened_nu_k)\n            \n            error = x_true - x_est\n            # Ensure P_est is well-conditioned before inverting\n            try:\n                P_inv = np.linalg.inv(P_est)\n                nees_k = (error.T @ P_inv @ error).item()\n                nees_history.append(nees_k)\n            except np.linalg.LinAlgError:\n                # In case of singularity, we cannot compute NEES for this step.\n                # Skip this data point; this is a sign of filter divergence.\n                pass\n\n        # 5. Final Statistics Calculation\n        avg_nis = np.mean(nis_history)\n        avg_nees = np.mean(nees_history) if nees_history else float('nan')\n        \n        # Autocorrelation\n        whitened_nu_arr = np.array(whitened_nu_history)\n        nu_mean = np.mean(whitened_nu_arr)\n        nu_mean_subtracted = whitened_nu_arr - nu_mean\n        \n        autocov = np.correlate(nu_mean_subtracted, nu_mean_subtracted, mode='full')\n        \n        lags = [1, 2, 3]\n        autocorr_values = []\n        denominator = autocov[T - 1] # Lag-0 autocovariance\n        if denominator > 1e-9: # Avoid division by zero\n            for lag in lags:\n                numerator = autocov[T - 1 + lag]\n                autocorr_values.append(np.abs(numerator / denominator))\n        else:\n             autocorr_values = [0.0, 0.0, 0.0]\n\n        max_abs_rho = max(autocorr_values) if autocorr_values else 0.0\n        \n        return avg_nis, max_abs_rho, avg_nees\n\n    results = []\n    for case in test_cases:\n        avg_nis, max_abs_rho, avg_nees = run_ukf_simulation(case)\n        results.extend([avg_nis, max_abs_rho, avg_nees])\n    \n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2756735"}]}