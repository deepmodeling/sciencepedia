{"hands_on_practices": [{"introduction": "The 'Model' in the Internal Model Principle is a precise mathematical description of the external signals a control system must accommodate. This first practice focuses on constructing this model, known as an exosystem. By translating a family of common signals—polynomials and sinusoids—into a state-space representation, you will master the foundational step of any IMP-based design and understand how the dynamics of references and disturbances are captured by the eigenvalues of the exosystem matrix $S$ [@problem_id:2752883].", "problem": "Consider the Internal Model Principle (IMP) in Linear Time-Invariant (LTI) control design. Let the reference/disturbance signal family be\n$$\\{1,\\, t,\\, \\ldots,\\, t^{k},\\, \\sin(\\omega_{1} t),\\, \\cos(\\omega_{1} t),\\, \\ldots,\\, \\sin(\\omega_{m} t),\\, \\cos(\\omega_{m} t)\\},$$\nwhere $k \\in \\mathbb{N}_{0}$, $m \\in \\mathbb{N}$, and $\\omega_{1},\\ldots,\\omega_{m} \\in \\mathbb{R}_{>0}$ are pairwise distinct. You are to construct a finite-dimensional LTI exosystem\n$$\\dot{w} = S w,\\quad r = F w,$$\nwith $w \\in \\mathbb{R}^{n}$, that generates exactly the linear span of the signals above as its output $r(t)$ for suitable initial conditions $w(0)$. Provide the matrices $S$ and $F$ explicitly in block-diagonal form using only constant real entries and blocks of minimal sizes that realize the required signals. Then, determine the minimal possible state dimension $n$ (i.e., the minimal dimension of $w$) so that such an exosystem exists. Express your final answer as a closed-form symbolic expression in terms of $k$ and $m$. The final answer must be a single analytic expression with no units.", "solution": "The problem statement will first be subjected to a rigorous validation procedure.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- **Topic**: Internal Model Principle (IMP) in Linear Time-Invariant (LTI) control design.\n- **Signal Family**: The linear span of $\\{1,\\, t,\\, \\ldots,\\, t^{k},\\, \\sin(\\omega_{1} t),\\, \\cos(\\omega_{1} t),\\, \\ldots,\\, \\sin(\\omega_{m} t),\\, \\cos(\\omega_{m} t)\\}$.\n- **Parameters**: $k \\in \\mathbb{N}_{0}$, $m \\in \\mathbb{N}$, and $\\omega_{1},\\ldots,\\omega_{m} \\in \\mathbb{R}_{>0}$ are pairwise distinct positive real numbers.\n- **Exosystem Model**: A finite-dimensional LTI system given by $\\dot{w} = S w$, $r = F w$, with state $w \\in \\mathbb{R}^{n}$.\n- **Task 1**: Construct the matrices $S$ and $F$ explicitly. $S$ must be in block-diagonal form, using constant real entries and blocks of minimal sizes.\n- **Task 2**: Determine the minimal possible state dimension $n$ as a closed-form symbolic expression in terms of $k$ and $m$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is analyzed against the established criteria.\n\n1.  **Scientifically Grounded**: The problem is a standard exercise in modern control theory. The construction of an exosystem to generate a specified class of signals is a fundamental concept underlying the Internal Model Principle. The signals—polynomials and sinusoids—are canonical examples used in this context. The problem is entirely grounded in the established principles of linear systems and differential equations.\n2.  **Well-Posed**: The problem is well-posed. It asks for the minimal realization of a linear system that generates a specific, well-defined function space. The theory of linear systems guarantees that such a minimal realization exists and its dimension is unique. The constraints on the matrices ($S$ being real and block-diagonal) are standard and achievable.\n3.  **Objective**: The problem is stated in precise mathematical language, free from any subjectivity, ambiguity, or opinion.\n4.  **Completeness and Consistency**: The problem is self-contained. All parameters ($k, m, \\omega_j$) are defined with their respective domains. The conditions that the frequencies $\\omega_j$ are positive and distinct are crucial and correctly specified to avoid unnecessary complexity from repeated or zero-frequency sinusoidal modes. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is a valid problem of control systems engineering. I will proceed with the solution.\n\n## Solution\n\nThe core principle for constructing an exosystem $\\dot{w} = Sw$ that generates a family of signals is that the eigenvalues of the matrix $S$ must correspond to the modes of the signals. The modes are determined by the poles of the Laplace transform of the basis functions of the signal space.\n\nThe given signal family is the linear span of two types of functions:\n1.  Polynomials: $\\{1, t, \\ldots, t^{k}\\}$\n2.  Sinusoids: $\\{\\sin(\\omega_{j} t), \\cos(\\omega_{j} t)\\}$ for $j \\in \\{1, \\ldots, m\\}$\n\nBy the principle of superposition for linear systems, we can construct the total exosystem by combining the minimal-order subsystems that generate each of these signal types.\n\n**Part 1: Generating Polynomial Signals**\nThe set of signals $\\{1, t, \\ldots, t^{k}\\}$ forms the solution space of the homogeneous linear differential equation with constant coefficients:\n$$ \\frac{d^{k+1}r}{dt^{k+1}} = 0 $$\nThe characteristic equation is $\\lambda^{k+1} = 0$, which has a single root $\\lambda = 0$ with algebraic multiplicity $k+1$. For an LTI system to generate these signals, its dynamics matrix must have an eigenvalue at $0$ with a corresponding Jordan block of at least size $k+1$. A minimal realization is achieved with a single Jordan block of size exactly $k+1$.\nWe can represent this in state-space form. Let the state vector for this subsystem be $w_p \\in \\mathbb{R}^{k+1}$. A minimal realization is given by the controllable canonical form:\n$$ \\dot{w}_p = S_p w_p $$\nwhere $S_p$ is the $(k+1) \\times (k+1)$ Jordan block associated with the eigenvalue $\\lambda=0$:\n$$ S_p = \\begin{pmatrix} 0 & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 \\\\ 0 & 0 & 0 & \\cdots & 0 \\end{pmatrix} $$\nTo ensure that the output can span the entire polynomial space, we must choose an output matrix $F_p$ such that the pair $(F_p, S_p)$ is observable. A standard choice is $F_p = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\end{pmatrix}$, which makes the output $r_p(t)$ equal to the first state variable $w_{p,1}(t)$. With this choice, $w_{p,2} = \\dot{w}_{p,1}$, $w_{p,3} = \\ddot{w}_{p,1}$, and so on, up to $w_{p, k+1} = w_{p,1}^{(k)}$. The dynamics $\\dot{w}_{p,k+1}=0$ implies $w_{p,1}^{(k+1)}=0$, which is the desired differential equation. By selecting appropriate initial conditions for $w_p(0)$, the output $r_p(t) = w_{p,1}(t)$ can be any polynomial of degree up to $k$. The minimal dimension for this polynomial-generating subsystem is $n_p = k+1$.\n\n**Part 2: Generating Sinusoidal Signals**\nFor each $j \\in \\{1, \\ldots, m\\}$, the pair of signals $\\{\\sin(\\omega_j t), \\cos(\\omega_j t)\\}$ constitutes the solution space of the differential equation:\n$$ \\frac{d^2 r}{dt^2} + \\omega_j^2 r = 0 $$\nThe characteristic equation is $\\lambda^2 + \\omega_j^2 = 0$, with roots $\\lambda = \\pm i\\omega_j$. A minimal LTI system that generates these signals must have a pair of complex conjugate eigenvalues $\\pm i\\omega_j$. For a system with real-valued matrices, this requires a state of at least dimension $2$.\nLet the state sub-vector for the $j$-th sinusoidal pair be $w_j \\in \\mathbb{R}^2$. A minimal real-valued realization is given by:\n$$ \\dot{w}_j = S_j w_j $$\nwhere $S_j$ is the $2 \\times 2$ matrix in real canonical form corresponding to the eigenvalues $\\pm i\\omega_j$:\n$$ S_j = \\begin{pmatrix} 0 & \\omega_j \\\\ -\\omega_j & 0 \\end{pmatrix} $$\nThe solution to this system is of the form $w_{j,1}(t) = A\\cos(\\omega_j t) + B\\sin(\\omega_j t)$. To generate the required signal space, we can choose the output matrix $F_j = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. The pair $(F_j, S_j)$ is observable because $\\omega_j > 0$. The minimal dimension for each such sinusoidal subsystem is $n_j = 2$.\n\n**Part 3: Assembling the Total Exosystem**\nThe signal basis functions are polynomials and sinusoids with distinct non-zero frequencies. These functions are linearly independent. The eigenvalues corresponding to these signals are $\\{0\\}$ for the polynomials and $\\{\\pm i\\omega_1, \\ldots, \\pm i\\omega_m\\}$ for the sinusoids. Since all $\\omega_j$ are positive and distinct, these eigenvalues are all distinct.\nTherefore, the total minimal exosystem can be constructed as a direct sum of the individual minimal subsystems. The total state vector is $w = (w_p^T, w_1^T, \\ldots, w_m^T)^T$. The system matrices $S$ and $F$ take a block structure.\n\nThe matrix $S$ is a block-diagonal matrix:\n$$ S = \\text{diag}(S_p, S_1, S_2, \\ldots, S_m) = \\begin{pmatrix} S_p & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\ \\mathbf{0} & S_1 & & \\mathbf{0} \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\mathbf{0} & \\mathbf{0} & \\cdots & S_m \\end{pmatrix} $$\nwhere $S_p$ is the $(k+1) \\times (k+1)$ matrix defined previously, and each $S_j$ is the $2 \\times 2$ matrix for $j=1, \\ldots, m$. The zero matrices $\\mathbf{0}$ are of appropriate dimensions.\n\nThe output matrix $F$ is a row vector that combines the outputs from each subsystem:\n$$ F = \\begin{pmatrix} F_p & F_1 & F_2 & \\cdots & F_m \\end{pmatrix} $$\nwhere $F_p = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\end{pmatrix}$ is $1 \\times (k+1)$ and each $F_j = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ is $1 \\times 2$.\nThe output $r(t) = F w(t)$ is the sum of outputs from each block. Since the blocks are uncoupled and each is observable from its corresponding part of the output matrix $F$, any signal in the span of the basis functions can be generated by selecting the appropriate initial condition vector $w(0)$.\n\n**Part 4: Determining the Minimal Dimension**\nThe minimal dimension $n$ of the state vector $w$ is the sum of the dimensions of the minimal subsystems.\n- The dimension for the polynomial part $\\{1, t, \\ldots, t^k\\}$ is $n_p = k+1$.\n- The dimension for each sinusoidal pair $\\{\\sin(\\omega_j t), \\cos(\\omega_j t)\\}$ is $n_j=2$. Since there are $m$ such pairs, their total contribution to the dimension is $2m$.\n\nThe total minimal dimension $n$ is therefore:\n$$ n = n_p + \\sum_{j=1}^{m} n_j = (k+1) + 2m $$\n\nThis expression gives the minimal possible state dimension for an LTI exosystem that generates the specified signal family.", "answer": "$$\n\\boxed{k + 1 + 2m}\n$$", "id": "2752883"}, {"introduction": "After modeling the external signals, the next step is to determine the ideal steady-state behavior required of the plant and controller to achieve perfect regulation. This hands-on practice guides you through solving the algebraic regulator equations for the key matrices $\\Pi$ and $\\Gamma$, which define the necessary feedforward control action [@problem_id:2752873]. This calculation is the computational core of output regulation theory, revealing exactly how the control input must counteract the effect of disturbances and generate the desired reference trajectory.", "problem": "Consider the linear time-invariant plant and exosystem in the standard output regulation setting:\n- Plant dynamics: $\\dot{x} = A x + B u + P w$, with measured/regulated error $e = C x + D u + Q w$.\n- Exosystem dynamics: $\\dot{w} = S w$.\nHere $x \\in \\mathbb{R}^{2}$, $u \\in \\mathbb{R}$, and $w \\in \\mathbb{R}$. The matrices are\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1 & 0 \\end{pmatrix}, \\quad\nD = \\begin{pmatrix} 0 \\end{pmatrix},\n$$\n$$\nP = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nQ = \\begin{pmatrix} -1 \\end{pmatrix}, \\quad\nS = \\begin{pmatrix} 0 \\end{pmatrix}.\n$$\nStart from the foundational definitions of steady-state tracking and the internal model principle (IMP): assume steady-state signals of the form $x_{\\mathrm{ss}}(t) = \\Pi w(t)$ and $u_{\\mathrm{ss}}(t) = \\Gamma w(t)$, with $e_{\\mathrm{ss}}(t) \\equiv 0$ for all trajectories $w(\\cdot)$ generated by the exosystem $\\dot{w} = S w$. Use these definitions and the plant and exosystem equations to derive the regulator equations that relate the unknown matrices $\\Pi \\in \\mathbb{R}^{2 \\times 1}$ and $\\Gamma \\in \\mathbb{R}^{1 \\times 1}$ to the given data $(A,B,C,D,P,Q,S)$. Then, for the concrete numerical data above, solve for $\\Pi$ and $\\Gamma$ and verify directly that the regulator equations hold.\n\nReport the value of $\\Gamma$ as your final answer. Express your answer as an exact real number (no rounding needed). No physical units are required.", "solution": "The problem requires the derivation of the regulator equations for output regulation and their solution for a given set of system matrices. We begin by validating the problem statement.\n\n**Problem Validation**\n\nStep 1: Extract Givens\nThe given system is a linear time-invariant plant with an associated exosystem, described by the following equations and matrices:\nPlant dynamics: $\\dot{x} = A x + B u + P w$\nRegulated error: $e = C x + D u + Q w$\nExosystem dynamics: $\\dot{w} = S w$\n\nThe dimensions of the state, input, and exosystem state are $x \\in \\mathbb{R}^{2}$, $u \\in \\mathbb{R}$, and $w \\in \\mathbb{R}$. The matrices are given as:\n$A = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}$, $B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, $D = \\begin{pmatrix} 0 \\end{pmatrix}$.\n$P = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $Q = \\begin{pmatrix} -1 \\end{pmatrix}$, $S = \\begin{pmatrix} 0 \\end{pmatrix}$.\n\nThe problem assumes the existence of steady-state solutions of the form $x_{\\mathrm{ss}}(t) = \\Pi w(t)$ and $u_{\\mathrm{ss}}(t) = \\Gamma w(t)$ for some constant matrices $\\Pi \\in \\mathbb{R}^{2 \\times 1}$ and $\\Gamma \\in \\mathbb{R}^{1 \\times 1}$, such that the steady-state error $e_{\\mathrm{ss}}(t)$ is identically zero for all trajectories $w(t)$ generated by the exosystem.\n\nStep 2: Validate Using Extracted Givens\nThe problem is formulated within the standard mathematical framework of control theory, specifically the theory of output regulation.\n- **Scientifically Grounded:** The problem uses standard state-space representations and concepts central to the internal model principle. It is free of scientific fallacies or pseudoscience.\n- **Well-Posed:** The problem provides a complete set of matrices and asks for the derivation and solution of algebraic equations. The existence and uniqueness of the solution for $\\Pi$ and $\\Gamma$ depend on the invertibility of a specific matrix constructed from the given system matrices. A preliminary check indicates this condition is met, thus the problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical language and is devoid of subjective or ambiguous terminology.\nThe problem is self-contained, consistent, and scientifically sound.\n\nStep 3: Verdict and Action\nThe problem is deemed **valid**. We will now proceed with the solution.\n\n**Derivation of the Regulator Equations**\n\nThe objective is to find matrices $\\Pi$ and $\\Gamma$ such that when $x(t) = x_{\\mathrm{ss}}(t) = \\Pi w(t)$ and $u(t) = u_{\\mathrm{ss}}(t) = \\Gamma w(t)$, the plant dynamics are satisfied and the error $e(t)$ is zero for any $w(t)$ generated by $\\dot{w} = S w$.\n\nFirst, we substitute the steady-state signals into the plant dynamics equation:\n$\\dot{x}_{\\mathrm{ss}} = A x_{\\mathrm{ss}} + B u_{\\mathrm{ss}} + P w$.\nThe time derivative of $x_{\\mathrm{ss}}(t)$ is given by the chain rule: $\\dot{x}_{\\mathrm{ss}}(t) = \\frac{d}{dt} (\\Pi w(t)) = \\Pi \\dot{w}(t)$.\nUsing the exosystem dynamics, $\\dot{w} = S w$, we have $\\dot{x}_{\\mathrm{ss}} = \\Pi S w$.\nSubstituting all steady-state expressions into the plant equation yields:\n$\\Pi S w = A (\\Pi w) + B (\\Gamma w) + P w$.\nThis equation must hold for all $w(t)$ generated by the exosystem. We can therefore write:\n$(\\Pi S - A \\Pi - B \\Gamma - P)w = 0$.\nFor this to be true for any non-trivial $w$, the matrix multiplying $w$ must be zero. This gives the first regulator equation:\n$\\Pi S = A \\Pi + B \\Gamma + P$, or, rearranged:\n$A \\Pi + B \\Gamma - \\Pi S = -P$.\n\nSecond, we enforce the condition that the steady-state error is zero, $e_{\\mathrm{ss}}(t) \\equiv 0$. The error is given by $e = C x + D u + Q w$. In steady state:\n$e_{\\mathrm{ss}} = C x_{\\mathrm{ss}} + D u_{\\mathrm{ss}} + Q w = C(\\Pi w) + D(\\Gamma w) + Q w = (C \\Pi + D \\Gamma + Q)w$.\nFor $e_{\\mathrm{ss}}(t)$ to be zero for any $w(t)$, the matrix factor must be zero. This gives the second regulator equation:\n$C \\Pi + D \\Gamma + Q = 0$, or, rearranged:\n$C \\Pi + D \\Gamma = -Q$.\n\nThe pair of matrix equations\n$$\n\\begin{cases}\nA \\Pi + B \\Gamma - \\Pi S = -P \\\\\nC \\Pi + D \\Gamma = -Q\n\\end{cases}\n$$\nare the general regulator equations.\n\n**Solution for the Given Data**\n\nWe now solve these equations using the provided numerical matrices.\nThe matrix $S$ is given as $S = \\begin{pmatrix} 0 \\end{pmatrix}$, which simplifies the first regulator equation to:\n$A \\Pi + B \\Gamma = -P$.\nThe matrix $D$ is given as $D = \\begin{pmatrix} 0 \\end{pmatrix}$, which simplifies the second regulator equation to:\n$C \\Pi = -Q$.\n\nLet the unknown matrices be represented as $\\Pi = \\begin{pmatrix} \\pi_1 \\\\ \\pi_2 \\end{pmatrix}$ and $\\Gamma = \\begin{pmatrix} \\gamma_1 \\end{pmatrix}$.\n\nWe begin with the second, simpler equation: $C \\Pi = -Q$.\nSubstituting the matrices:\n$$\n\\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\pi_1 \\\\ \\pi_2 \\end{pmatrix} = - \\begin{pmatrix} -1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication gives:\n$$\n\\begin{pmatrix} \\pi_1 \\end{pmatrix} = \\begin{pmatrix} 1 \\end{pmatrix}\n$$\nThus, we find $\\pi_1 = 1$.\n\nNext, we use the first equation: $A \\Pi + B \\Gamma = -P$.\nSubstituting the matrices and the known value of $\\pi_1$:\n$$\n\\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\pi_2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\gamma_1 \\end{pmatrix} = - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nExpanding the matrix products:\n$$\n\\begin{pmatrix} \\pi_2 \\\\ -2 - 3\\pi_2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\gamma_1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nThis vector equation yields a system of two linear equations:\n1. $\\pi_2 + 0 = 0 \\implies \\pi_2 = 0$.\n2. $-2 - 3\\pi_2 + \\gamma_1 = -1$.\n\nSubstituting $\\pi_2 = 0$ into the second equation:\n$-2 - 3(0) + \\gamma_1 = -1$\n$-2 + \\gamma_1 = -1$\n$\\gamma_1 = 1$.\n\nThe solution is therefore $\\Pi = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\Gamma = \\begin{pmatrix} 1 \\end{pmatrix}$.\n\n**Verification**\n\nAs required, we verify this solution by substituting $\\Pi$ and $\\Gamma$ back into the regulator equations.\nEquation 1: $A \\Pi + B \\Gamma = -P$.\nLeft-hand side (LHS):\n$$\n\\text{LHS} = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nRight-hand side (RHS):\n$$\n\\text{RHS} = -P = -\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nLHS = RHS. The first equation holds.\n\nEquation 2: $C \\Pi = -Q$.\nLeft-hand side (LHS):\n$$\n\\text{LHS} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\end{pmatrix}\n$$\nRight-hand side (RHS):\n$$\n\\text{RHS} = -Q = -\\begin{pmatrix} -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\end{pmatrix}\n$$\nLHS = RHS. The second equation holds.\nThe verification is successful. The value of $\\Gamma$ is $1$.", "answer": "$$\\boxed{1}$$", "id": "2752873"}, {"introduction": "The ultimate justification for the Internal Model Principle is its promise of robustness—a quality that is not a mere academic curiosity but a crucial requirement for real-world systems. This exercise provides a striking demonstration of why the IMP is essential, contrasting it with a non-robust method that relies on a fragile cancellation of plant zeros [@problem_id:2752895]. By calculating how an arbitrarily small plant perturbation can destroy perfect disturbance rejection in a controller lacking an internal model, you will gain a concrete understanding of the robustness that makes the IMP a cornerstone of modern control.", "problem": "Consider a stable Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) plant subjected to an input disturbance that is a pure sinusoid of known frequency. The nominal plant is given by the strictly proper transfer function\n$$\nP_0(s)=\\frac{s^2+\\omega_0^2}{(s+1)(s+2)},\n$$\nwhere $\\omega_0>0$ is fixed and the denominator polynomial is Hurwitz. The controller is a static gain\n$$\nC(s)=k,\n$$\nwith real $k$ chosen so that the nominal closed loop with unity feedback is internally stable. The disturbance enters additively at the plant input and is\n$$\nd(t)=D\\sin(\\omega_0 t),\n$$\nwith amplitude $D>0$ known. The output is denoted $y(t)$.\n\nYou will examine the effect of an arbitrarily small coprime perturbation of the nominal plant on steady-state disturbance rejection when the controller has no internal model of the sinusoid, that is, when $C(s)$ has no poles at $s=\\pm \\mathrm{i}\\omega_0$.\n\nUse only the following as your starting points: the definition of sinusoidal steady-state response of an LTI system via the frequency response, the unity-feedback loop input–output relations, and the notion of a left coprime factorization in which a plant $P(s)$ is written as $P(s)=N(s)D(s)^{-1}$ with $N(s)$ and $D(s)$ stable and coprime.\n\n1. Show that for the nominal plant $P_0(s)$ and controller $C(s)$ the steady-state sinusoidal component of $y(t)$ due to $d(t)$ at frequency $\\omega_0$ is identically zero.\n\n2. Construct an arbitrarily small left coprime perturbation of $P_0(s)$ by setting\n$$\nP_\\varepsilon(s)=\\frac{N_0(s)+\\varepsilon\\,\\Delta N(s)}{D_0(s)+\\varepsilon\\,\\Delta D(s)},\n$$\nwith $N_0(s)=s^2+\\omega_0^2$, $D_0(s)=(s+1)(s+2)$, $\\Delta N(s)=1$, $\\Delta D(s)=0$, and $\\varepsilon\\in\\mathbb{R}$ nonzero but arbitrarily small. Argue that for sufficiently small $|\\varepsilon|$ the closed loop with $C(s)=k$ remains internally stable.\n\n3. Denote by $T_{dy,\\varepsilon}(s)$ the transfer function from the disturbance input $d$ to the output $y$ in the perturbed loop with $P_\\varepsilon(s)$ and $C(s)=k$. Using the frequency response at $s=\\mathrm{i}\\omega_0$, derive the exact closed-form expression for the steady-state amplitude of the sinusoidal component of $y(t)$ at frequency $\\omega_0$ as a function of $\\varepsilon$, $\\omega_0$, $k$, and $D$.\n\nYour final answer must be this steady-state amplitude as a single closed-form analytic expression in $\\varepsilon$, $\\omega_0$, $k$, and $D$. Do not simplify by asymptotic approximations and do not include any units in the final boxed answer. Do not round your result.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard scenario in control theory to analyze the robustness of disturbance rejection. A minor terminological inconsistency is noted: the problem refers to a \"left coprime factorization\" $P(s)=N(s)D(s)^{-1}$ with stable $N(s)$ and $D(s)$, which is technically the form of a right coprime factorization for scalar systems, and then proceeds to define $N_0(s)$ as a polynomial that is not stable. However, the calculation steps are unambiguously defined by the provided equations for the plant perturbation, using a ratio of polynomials. This imprecision does not invalidate the problem, as the intended mathematical operations are clear. We shall proceed with the specified polynomial representations. The overall structure asks for a demonstration of a fundamental concept: disturbance rejection based on plant zeros is not robust to parameter uncertainty, unlike rejection based on the internal model principle.\n\nThe unity-feedback loop with a plant $P(s)$, a controller $C(s)$, and an additive disturbance $d(t)$ at the plant input is described by the equation for the output $y(t)$. With a zero reference signal, the control signal is $u = d - C y$. The plant output is $y = P u = P(d - C y)$. Solving for $y$ gives $Y(s)(1+P(s)C(s)) = P(s)D(s)$. The transfer function from the disturbance $d$ to the output $y$ is given by:\n$$T_{dy}(s) = \\frac{Y(s)}{D(s)} = \\frac{P(s)}{1+P(s)C(s)}$$\nFor a sinusoidal input $d(t)=D\\sin(\\omega_0 t)$, the steady-state output of a stable LTI system is a sinusoid of the same frequency, $y_{ss}(t) = A \\sin(\\omega_0 t + \\phi)$, where the amplitude is $A = D |T_{dy}(i\\omega_0)|$ and the phase is $\\phi = \\angle T_{dy}(i\\omega_0)$.\n\n1. Nominal System Disturbance Rejection.\n\nFor the nominal plant $P_0(s) = \\frac{s^2+\\omega_0^2}{(s+1)(s+2)}$ and controller $C(s)=k$, the disturbance-to-output transfer function is:\n$$T_{dy,0}(s) = \\frac{P_0(s)}{1+k P_0(s)} = \\frac{\\frac{s^2+\\omega_0^2}{(s+1)(s+2)}}{1+k\\frac{s^2+\\omega_0^2}{(s+1)(s+2)}} = \\frac{s^2+\\omega_0^2}{(s+1)(s+2) + k(s^2+\\omega_0^2)}$$\nTo find the steady-state response to $d(t)=D\\sin(\\omega_0 t)$, we evaluate $T_{dy,0}(s)$ at $s=i\\omega_0$:\n$$T_{dy,0}(i\\omega_0) = \\frac{(i\\omega_0)^2+\\omega_0^2}{(i\\omega_0+1)(i\\omega_0+2) + k((i\\omega_0)^2+\\omega_0^2)}$$\nThe numerator is $(i\\omega_0)^2+\\omega_0^2 = -\\omega_0^2+\\omega_0^2 = 0$.\nThe denominator at $s=i\\omega_0$ is $(i\\omega_0+1)(i\\omega_0+2) + k(0) = -\\omega_0^2+3i\\omega_0+2$. Since $\\omega_0>0$, this denominator is non-zero. The problem states the nominal closed-loop is internally stable, which guarantees that the denominator of $T_{dy,0}(s)$, which is the characteristic polynomial $(s+1)(s+2)+k(s^2+\\omega_0^2)$, has no roots at $s=\\pm i\\omega_0$.\nThus, the frequency response at $\\omega_0$ is:\n$$T_{dy,0}(i\\omega_0) = \\frac{0}{2-\\omega_0^2 + 3i\\omega_0} = 0$$\nThe amplitude of the steady-state sinusoidal component of $y(t)$ is $A_0 = D |T_{dy,0}(i\\omega_0)| = D \\cdot 0 = 0$. This demonstrates perfect steady-state rejection of the disturbance for the nominal system. This rejection is achieved because the plant has transmission zeros at the disturbance frequencies $\\pm i\\omega_0$.\n\n2. Perturbed System and Stability.\n\nThe perturbed plant is given by:\n$$P_\\varepsilon(s) = \\frac{N_0(s)+\\varepsilon\\Delta N(s)}{D_0(s)+\\varepsilon\\Delta D(s)} = \\frac{s^2+\\omega_0^2+\\varepsilon \\cdot 1}{(s+1)(s+2)+\\varepsilon \\cdot 0} = \\frac{s^2+\\omega_0^2+\\varepsilon}{(s+1)(s+2)}$$\nThe characteristic polynomial of the closed-loop system with the perturbed plant $P_\\varepsilon(s)$ is given by the numerator of $1+kP_\\varepsilon(s)$:\n$$\\chi_\\varepsilon(s) = (s+1)(s+2) + k(s^2+\\omega_0^2+\\varepsilon) = (1+k)s^2 + 3s + (2+k\\omega_0^2+k\\varepsilon)$$\nThe nominal characteristic polynomial (for $\\varepsilon=0$) is $\\chi_0(s) = (1+k)s^2 + 3s + (2+k\\omega_0^2)$. By the problem statement, the nominal closed loop is internally stable, which means all roots of $\\chi_0(s)$ lie in the open left-half complex plane.\nThe roots of a polynomial are continuous functions of its coefficients. The coefficients of $\\chi_\\varepsilon(s)$ are continuous (in fact, linear) functions of $\\varepsilon$. Since the roots of $\\chi_0(s)$ are strictly in the left-half plane (not on the imaginary axis), there exists a neighborhood of $\\varepsilon=0$ such that for any $\\varepsilon$ in this neighborhood, the roots of $\\chi_\\varepsilon(s)$ also lie in the left-half plane. Therefore, for sufficiently small $|\\varepsilon|$, the perturbed closed-loop system remains internally stable.\n\n3. Steady-State Amplitude for the Perturbed System.\n\nThe disturbance-to-output transfer function for the perturbed system is:\n$$T_{dy,\\varepsilon}(s) = \\frac{P_\\varepsilon(s)}{1+k P_\\varepsilon(s)} = \\frac{\\frac{s^2+\\omega_0^2+\\varepsilon}{(s+1)(s+2)}}{1+k\\frac{s^2+\\omega_0^2+\\varepsilon}{(s+1)(s+2)}} = \\frac{s^2+\\omega_0^2+\\varepsilon}{(s+1)(s+2) + k(s^2+\\omega_0^2+\\varepsilon)}$$\nWe evaluate this transfer function at $s=i\\omega_0$ to find the steady-state response to the disturbance.\nThe numerator becomes:\n$$(i\\omega_0)^2+\\omega_0^2+\\varepsilon = -\\omega_0^2+\\omega_0^2+\\varepsilon = \\varepsilon$$\nThe denominator becomes:\n$$(i\\omega_0+1)(i\\omega_0+2) + k((i\\omega_0)^2+\\omega_0^2+\\varepsilon) = (i\\omega_0)^2+3i\\omega_0+2 + k\\varepsilon = -\\omega_0^2+3i\\omega_0+2+k\\varepsilon$$\nGrouping real and imaginary parts, the denominator is $(2-\\omega_0^2+k\\varepsilon) + i(3\\omega_0)$.\nThe frequency response at $\\omega_0$ is:\n$$T_{dy,\\varepsilon}(i\\omega_0) = \\frac{\\varepsilon}{(2-\\omega_0^2+k\\varepsilon) + i(3\\omega_0)}$$\nThe magnitude of this complex number is:\n$$|T_{dy,\\varepsilon}(i\\omega_0)| = \\frac{|\\varepsilon|}{|(2-\\omega_0^2+k\\varepsilon) + i(3\\omega_0)|} = \\frac{|\\varepsilon|}{\\sqrt{(2-\\omega_0^2+k\\varepsilon)^2 + (3\\omega_0)^2}} = \\frac{|\\varepsilon|}{\\sqrt{(2-\\omega_0^2+k\\varepsilon)^2 + 9\\omega_0^2}}$$\nThe steady-state amplitude of the sinusoidal component of the output $y(t)$ is this magnitude multiplied by the disturbance amplitude $D$.\n$$A_\\varepsilon = D |T_{dy,\\varepsilon}(i\\omega_0)| = D \\frac{|\\varepsilon|}{\\sqrt{(2-\\omega_0^2+k\\varepsilon)^2 + 9\\omega_0^2}}$$\nThis is the exact closed-form expression for the steady-state output amplitude. This result shows that for any non-zero perturbation $\\varepsilon$, however small, the perfect rejection is lost and a sinusoidal component appears in the output. The amplitude of this component is proportional to $|\\varepsilon|$ for small $\\varepsilon$, highlighting the non-robust nature of disturbance rejection based on plant zero cancellation.", "answer": "$$\n\\boxed{D \\frac{|\\varepsilon|}{\\sqrt{(2-\\omega_0^2+k\\varepsilon)^2 + 9\\omega_0^2}}}\n$$", "id": "2752895"}]}