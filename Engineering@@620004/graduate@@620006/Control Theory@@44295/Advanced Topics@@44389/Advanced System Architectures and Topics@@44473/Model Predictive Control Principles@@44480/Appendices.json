{"hands_on_practices": [{"introduction": "At the heart of Model Predictive Control lies the repetitive solution of a constrained optimal control problem. This first practice exercise [@problem_id:2724762] strips MPC down to a minimal, one-dimensional example, allowing for a fully analytical solution. By applying the Karush-Kuhn-Tucker (KKT) conditions, you will gain a foundational understanding of how cost functions, system dynamics, and active constraints interact to define the optimal control move, a process typically abstracted away by numerical solvers.", "problem": "Consider a one-dimensional discrete-time linear system used in Model Predictive Control (MPC) with dynamics given by $x_{k+1}=a x_k + b u_k$, where $x_k \\in \\mathbb{R}$ and $u_k \\in \\mathbb{R}$. Let the prediction horizon be $N=2$ with decision variables $u_0$ and $u_1$. The quadratic performance index to be minimized is\n$$J = q x_1^2 + r u_0^2 + p x_2^2 + r u_1^2,$$\nsubject to the dynamics\n$$x_1 = a x_0 + b u_0, \\quad x_2 = a x_1 + b u_1,$$\nand a single affine inequality constraint at the terminal step,\n$$x_2 \\ge \\underline{x},$$\nwith all other constraints absent. Assume that at the optimum this single inequality constraint is active and all other constraints are inactive.\n\nUse the Karush–Kuhn–Tucker (KKT) conditions for convex quadratic programming to derive and solve the resulting system analytically for the optimal first control move $u_0^{\\star}$. You must start from the definitions of the dynamics, the cost, and the active inequality, construct the Lagrangian, write down the stationarity, primal feasibility, dual feasibility, and complementarity conditions, and solve them to obtain $u_0^{\\star}$.\n\nUse the following numerically specified and scientifically consistent parameters: $a=1.2$, $b=1$, $q=1$, $r=0.5$, $p=2$, $x_0=1$, and $\\underline{x}=0.8$.\n\nExpress your final answer as a real number (unitless) rounded to four significant figures.", "solution": "The problem provided is a well-posed, scientifically grounded optimal control problem that can be solved using the Karush–Kuhn–Tucker (KKT) conditions for constrained optimization. The problem is valid and we proceed with the analytical solution.\n\nThe system dynamics are given by the linear difference equation $x_{k+1}=a x_k + b u_k$. With an initial state $x_0$ and a prediction horizon of $N=2$, the states are expressed in terms of the control inputs $u_0$ and $u_1$ as:\n$$x_1 = a x_0 + b u_0$$\n$$x_2 = a x_1 + b u_1 = a(a x_0 + b u_0) + b u_1 = a^2 x_0 + ab u_0 + b u_1$$\n\nThe objective is to minimize the quadratic performance index $J$:\n$$J(u_0, u_1) = q x_1^2 + r u_0^2 + p x_2^2 + r u_1^2$$\nSubstituting the expressions for $x_1$ and $x_2$ in terms of $u_0$ and $u_1$, the cost function becomes:\n$$J(u_0, u_1) = q(a x_0 + b u_0)^2 + r u_0^2 + p(a^2 x_0 + ab u_0 + b u_1)^2 + r u_1^2$$\n\nThe optimization is subject to the terminal state constraint $x_2 \\ge \\underline{x}$, which can be written in the standard form $g(u_0, u_1) \\le 0$ as:\n$$g(u_0, u_1) = \\underline{x} - x_2 = \\underline{x} - (a^2 x_0 + ab u_0 + b u_1) \\le 0$$\n\nThis is a convex quadratic program, as the cost function $J$ is a sum of squared terms with positive coefficients ($q, r, p > 0$), making it strictly convex, and the constraint is affine. We construct the Lagrangian $\\mathcal{L}$ by introducing a Lagrange multiplier $\\lambda$:\n$$\\mathcal{L}(u_0, u_1, \\lambda) = J(u_0, u_1) + \\lambda g(u_0, u_1)$$\n$$\\mathcal{L}(u_0, u_1, \\lambda) = q(a x_0 + b u_0)^2 + r u_0^2 + p(a^2 x_0 + ab u_0 + b u_1)^2 + r u_1^2 + \\lambda(\\underline{x} - (a^2 x_0 + ab u_0 + b u_1))$$\n\nThe KKT conditions for optimality are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the decision variables must be zero.\n    $$\\frac{\\partial \\mathcal{L}}{\\partial u_0} = 2q(a x_0 + b u_0)b + 2r u_0 + 2p(a^2 x_0 + ab u_0 + b u_1)ab - \\lambda ab = 0$$\n    $$\\frac{\\partial \\mathcal{L}}{\\partial u_1} = 2p(a^2 x_0 + ab u_0 + b u_1)b + 2r u_1 - \\lambda b = 0$$\n2.  **Primal Feasibility**: The constraint must be satisfied.\n    $$\\underline{x} - (a^2 x_0 + ab u_0 + b u_1) \\le 0$$\n3.  **Dual Feasibility**: The Lagrange multiplier for an inequality of the form $g \\le 0$ must be non-negative.\n    $$\\lambda \\ge 0$$\n4.  **Complementary Slackness**: The product of the multiplier and the constraint function must be zero.\n    $$\\lambda(\\underline{x} - (a^2 x_0 + ab u_0 + b u_1)) = 0$$\n\nThe problem states that the inequality constraint is active at the optimum. This implies:\n$$x_2 = a^2 x_0 + ab u_0 + b u_1 = \\underline{x}$$\nThis satisfies the primal feasibility condition. It also satisfies the complementary slackness condition, leaving $\\lambda$ to be determined. The stationarity equations can be simplified using $x_2 = \\underline{x}$ and dividing by non-zero factors ($b=1 \\ne 0$).\n\nThe first stationarity equation becomes, after dividing by $2b$:\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - \\frac{\\lambda a}{2} = 0 \\quad (1)$$\nThe second stationarity equation becomes, after dividing by $2b$:\n$$p\\underline{x} + \\frac{r}{b}u_1 - \\frac{\\lambda}{2} = 0 \\quad (2)$$\n\nFrom equation $(2)$, we express $\\frac{\\lambda}{2}$ in terms of $u_1$:\n$$\\frac{\\lambda}{2} = p\\underline{x} + \\frac{r}{b}u_1$$\nWe substitute this expression for $\\frac{\\lambda}{2}$ into equation $(1)$:\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - a\\left(p\\underline{x} + \\frac{r}{b}u_1\\right) = 0$$\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - pa\\underline{x} - \\frac{ar}{b}u_1 = 0$$\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 - \\frac{ar}{b}u_1 = 0$$\nMultiplying by $b$ and expanding yields:\n$$qabx_0 + qb^2 u_0 + ru_0 - aru_1 = 0$$\n$$u_0(qb^2+r) + qabx_0 - aru_1 = 0 \\quad (A)$$\n\nWe now have a system of two linear equations in the two unknowns $u_0$ and $u_1$:\n$$(A): \\quad u_0(qb^2+r) + qabx_0 - aru_1 = 0$$\n$$(B): \\quad a^2 x_0 + ab u_0 + b u_1 = \\underline{x}$$\nFrom $(B)$, we solve for $u_1$:\n$$b u_1 = \\underline{x} - a^2 x_0 - ab u_0 \\implies u_1 = \\frac{1}{b}(\\underline{x} - a^2 x_0 - ab u_0)$$\nSubstitute this into $(A)$:\n$$u_0(qb^2+r) + qabx_0 - ar \\left(\\frac{1}{b}(\\underline{x} - a^2 x_0 - ab u_0)\\right) = 0$$\n$$u_0(qb^2+r) + qabx_0 - \\frac{ar}{b}(\\underline{x} - a^2 x_0) + a^2 r u_0 = 0$$\nGroup terms containing $u_0$:\n$$u_0(qb^2+r+a^2r) = \\frac{ar}{b}(\\underline{x} - a^2 x_0) - qabx_0$$\nFinally, we solve for the optimal first control move, $u_0^{\\star}$:\n$$u_0^{\\star} = \\frac{\\frac{ar}{b}(\\underline{x} - a^2 x_0) - qabx_0}{qb^2 + r + a^2 r}$$\n\nNow we substitute the given numerical values: $a=1.2$, $b=1$, $q=1$, $r=0.5$, $x_0=1$, and $\\underline{x}=0.8$.\nThe numerator is:\n$$\\frac{(1.2)(0.5)}{1}(0.8 - (1.2)^2(1)) - (1)(1.2)(1)(1) = 0.6(0.8 - 1.44) - 1.2 = 0.6(-0.64) - 1.2 = -0.384 - 1.2 = -1.584$$\nThe denominator is:\n$$(1)(1)^2 + 0.5 + (1.2)^2(0.5) = 1 + 0.5 + (1.44)(0.5) = 1.5 + 0.72 = 2.22$$\nThus, the optimal control input is:\n$$u_0^{\\star} = \\frac{-1.584}{2.22} \\approx -0.7135135...$$\nRounding to four significant figures, we obtain $u_0^{\\star} = -0.7135$.\n\nAs a consistency check, we can compute the value of $\\lambda$.\n$u_0^{\\star} \\approx -0.7135135$\n$u_1^{\\star} = \\frac{1}{1}(0.8 - (1.2)^2(1) - (1.2)(1)u_0^{\\star}) = -0.64 - 1.2(-0.7135135) \\approx 0.2162162$\n$\\lambda^{\\star} = 2\\left(p\\underline{x} + \\frac{r}{b}u_1^{\\star}\\right) = 2\\left((2)(0.8) + \\frac{0.5}{1}(0.2162162)\\right) = 2(1.6 + 0.1081081) \\approx 3.416$\nSince $\\lambda^{\\star} > 0$, the dual feasibility condition is met, confirming the correctness of assuming the constraint is active.", "answer": "$$\\boxed{-0.7135}$$", "id": "2724762"}, {"introduction": "While MPC solves an optimization at each step, ensuring long-term stability and recursive feasibility is paramount. This coding practice [@problem_id:2724632] focuses on the computation of a control invariant set, a key component often used as a terminal region in MPC to provide these guarantees. You will implement an algorithm based on the iterative application of the one-step preimage operator, gaining hands-on experience with the computational geometry fundamental to constrained control synthesis.", "problem": "Consider the discrete-time linear time-invariant system given by the state-update equation $x^{+} = A x + B u$, where $x \\in \\mathbb{R}^{n}$ is the state and $u \\in \\mathbb{R}^{m}$ is the control input. A set $\\mathcal{C} \\subset \\mathbb{R}^{n}$ is called a controlled invariant set if for every $x \\in \\mathcal{C}$ there exists an input $u \\in \\mathcal{U}$ such that the successor state satisfies $A x + B u \\in \\mathcal{C}$ while also respecting state constraints $x \\in \\mathcal{X}$. The one-step preimage operator for a target set $\\mathcal{S} \\subset \\mathbb{R}^{n}$ under polytopic input constraints $\\mathcal{U} \\subset \\mathbb{R}^{m}$ is defined as $\\operatorname{Pre}(\\mathcal{S}) = \\{ x \\in \\mathbb{R}^{n} \\mid \\exists u \\in \\mathcal{U} \\ \\text{s.t.} \\ A x + B u \\in \\mathcal{S} \\}$. When $\\mathcal{S}$ is polyhedral in half-space (H-) representation, the set $\\operatorname{Pre}(\\mathcal{S})$ can be computed exactly by projecting, onto the $x$-space, the polyhedron in the joint variables $(x,u)$ that encodes $A x + B u \\in \\mathcal{S}$ and $u \\in \\mathcal{U}$. The maximal controlled invariant subset of $\\mathcal{X}$ relative to $\\mathcal{U}$ can be obtained by iterating the one-step preimage operator and intersecting with $\\mathcal{X}$, starting from $\\mathcal{K}^{0} = \\mathcal{X}$, via $\\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X}$, until convergence.\n\nYour task is to implement a program that, for each of the following test cases, computes the polyhedral set sequence $\\{ \\mathcal{K}^{k} \\}_{k \\ge 0}$ as above, by representing each set in H-representation and computing $\\operatorname{Pre}(\\cdot)$ exactly using Fourier–Motzkin elimination to eliminate the input variables $u$. At each iteration, represent $\\mathcal{K}^{k}$ in H-representation, compute $\\mathcal{K}^{k+1}$ exactly, simplify by removing redundant inequalities, and stop when the H-representation is unchanged up to an absolute tolerance $\\varepsilon = 10^{-8}$ (after normalizing inequality rows by a positive scalar to unit Euclidean norm), or after at most $N_{\\max}$ iterations, whichever occurs first. If the set becomes empty at any iteration, treat all subsequent iterates as empty. For the final iterate $\\mathcal{K}^{\\star}$ returned by this termination rule, compute the area of $\\mathcal{K}^{\\star}$ in $\\mathbb{R}^{2}$ by enumerating all vertices of its polygon (if fewer than three vertices exist or the set is empty, the area is zero). Express the area as a real number rounded to five decimal places.\n\nAll sets are polytopes defined by linear inequalities. The state constraint set is a hyper-rectangle $\\mathcal{X}$ with potentially different bounds per coordinate, encoded by $G x \\le g$ with $G = \\begin{bmatrix} 1 & 0 \\\\ -1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{bmatrix}$ and $g = \\begin{bmatrix} r_{1} \\\\ r_{1} \\\\ r_{2} \\\\ r_{2} \\end{bmatrix}$. The input constraint set is a one-dimensional interval $\\mathcal{U} = \\{ u \\in \\mathbb{R} \\mid |u| \\le u_{\\max} \\}$ encoded by $H_{u} u \\le h_{u}$ with $H_{u} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and $h_{u} = \\begin{bmatrix} u_{\\max} \\\\ u_{\\max} \\end{bmatrix}$. The system data and bounds for the test suite are:\n\n- Test case $1$ (happy path):\n  - $A = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.0 & 0.95 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.0 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 2.0$, $r_{2} = 2.0$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.5$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\n- Test case $2$ (boundary where invariance may collapse):\n  - $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 1.1 \\end{bmatrix}$, $B = \\begin{bmatrix} 0.1 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 0.2$, $r_{2} = 0.2$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.05$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\n- Test case $3$ (coupled dynamics):\n  - $A = \\begin{bmatrix} 0.8 & 0.3 \\\\ -0.2 & 0.9 \\end{bmatrix}$, $B = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 1.5$, $r_{2} = 1.5$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.7$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\nImplementation requirements and definitions:\n\n- Represent any polyhedron $\\{ z \\mid H z \\le h \\}$ by the pair $(H,h)$, where $H \\in \\mathbb{R}^{p \\times d}$, $h \\in \\mathbb{R}^{p}$, $p \\in \\mathbb{N}$, $d \\in \\mathbb{N}$. Intersection corresponds to stacking inequalities.\n- The exact one-step preimage $\\operatorname{Pre}(\\mathcal{S})$ for $\\mathcal{S} = \\{ x \\mid H_{S} x \\le h_{S} \\}$ under $u \\in \\mathcal{U}$ is the projection onto $x$ of $\\{ (x,u) \\mid H_{S} (A x + B u) \\le h_{S}, \\ H_{u} u \\le h_{u} \\}$. Compute this projection by Fourier–Motzkin elimination eliminating $u$ exactly. For a single scalar $u$, if an inequality has coefficient $\\beta$ on $u$, classify it as positive if $\\beta > 0$, negative if $\\beta < 0$, or zero if $\\beta = 0$. The projected inequalities on $x$ are all zero-$\\beta$ inequalities (with $u$ removed) together with all pairwise combinations of one positive and one negative inequality, yielding $(\\alpha^{+}/\\beta^{+} - \\alpha^{-}/\\beta^{-})^{\\top} x \\le d^{+}/\\beta^{+} - d^{-}/\\beta^{-}$, where $\\alpha^{\\pm}$ are the $x$-coefficients and $d^{\\pm}$ the right-hand sides.\n- After each preimage and intersection, remove redundant inequalities: an inequality $h_{i}^{\\top} x \\le b_{i}$ is redundant if the optimal value of $\\max \\{ h_{i}^{\\top} x \\mid H x \\le h, \\ (h_{i}, b_{i}) \\ \\text{removed} \\}$ is less than or equal to $b_{i}$ up to $\\varepsilon$.\n- Detect emptiness by solving a linear program for feasibility. If the set is empty, its area is defined as $0$.\n- Compute the polygon area of $\\mathcal{K}^{\\star}$ by enumerating all vertices as pairwise intersections of active constraints (in $\\mathbb{R}^{2}$), discarding those that violate any inequality, ordering the unique feasible vertices by polar angle about their centroid, and applying the shoelace formula. If fewer than three vertices exist, the area is $0$.\n\nYour program should run the three test cases above in order, compute the area of the final iterate $\\mathcal{K}^{\\star}$ for each, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[a\\_1,a\\_2,a\\_3]$), where each $a\\_i$ is the area rounded to five decimal places.", "solution": "The problem posed is to compute the maximal controlled invariant subset of a given state constraint set $\\mathcal{X}$ for a discrete-time linear time-invariant (LTI) system. This is a fundamental problem in control theory, particularly in the synthesis of controllers for constrained systems and in the domain of Model Predictive Control (MPC), where such sets often serve as terminal sets to guarantee stability and recursive feasibility. The solution requires a sequence of polyhedral computations.\n\nThe system is described by the state-update equation:\n$$ x^{+} = A x + B u $$\nwhere $x \\in \\mathbb{R}^{n}$ is the state vector and $u \\in \\mathbb{R}^{m}$ is the control input vector. For this problem, $n=2$ and $m=1$. The state and control inputs are subject to polytopic constraints: $x \\in \\mathcal{X}$ and $u \\in \\mathcal{U}$.\n\nThe algorithm to compute the maximal controlled invariant set, denoted $\\mathcal{K}^{\\star}$, is an iterative procedure based on the one-step preimage operator, $\\operatorname{Pre}(\\cdot)$. Starting with an initial set $\\mathcal{K}^{0} = \\mathcal{X}$, we generate a sequence of sets $\\{ \\mathcal{K}^{k} \\}_{k \\ge 0}$ using the recursion:\n$$ \\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X} $$\nThis sequence is guaranteed to be monotonically non-increasing, $\\mathcal{K}^{k+1} \\subseteq \\mathcal{K}^{k}$, and converges to the maximal controlled invariant subset of $\\mathcal{X}$.\n\nAll sets are represented as polyhedra in half-space (H-) representation, $\\{ z \\mid H z \\le h \\}$. The core of each iteration involves three main steps: preimage calculation, intersection, and simplification.\n\nStep 1: Preimage Calculation\nGiven a polyhedral set $\\mathcal{K}^{k} = \\{ x \\in \\mathbb{R}^{n} \\mid H_{k} x \\le h_{k} \\}$, the preimage $\\operatorname{Pre}(\\mathcal{K}^{k})$ is the set of all states $x$ for which there exists an admissible control $u \\in \\mathcal{U}$ such that the successor state $x^{+} = A x + B u$ lies in $\\mathcal{K}^{k}$. Formally:\n$$ \\operatorname{Pre}(\\mathcal{K}^{k}) = \\{ x \\in \\mathbb{R}^{n} \\mid \\exists u \\in \\mathcal{U} \\text{ s.t. } A x + B u \\in \\mathcal{K}^{k} \\} $$\nWith polytopic constraints $\\mathcal{U} = \\{ u \\in \\mathbb{R}^{m} \\mid H_{u} u \\le h_{u} \\}$, this can be computed exactly. The conditions define a polyhedron in the joint space of $(x, u)$:\n$$ H_{k} (A x + B u) \\le h_{k} $$\n$$ H_{u} u \\le h_{u} $$\nThe first set of inequalities can be rewritten as $(H_{k}A)x + (H_{k}B)u \\le h_{k}$. To obtain $\\operatorname{Pre}(\\mathcal{K}^{k})$, we must project this joint polyhedron onto the $x$-space by eliminating the variables $u$. For this problem, with a single input $u \\in \\mathbb{R}$, this projection is performed using Fourier-Motzkin elimination.\n\nLet the set of all inequalities involving $u$ be written as $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i}$ for $i=1, \\dots, p$. These inequalities are partitioned based on the sign of the scalar coefficient $\\beta_{i}$:\n1.  Positive coefficient ($\\beta_{i} > 0$): $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i} \\implies u \\le (d_{i} - \\alpha_{i}^{\\top} x) / \\beta_{i}$. Let this be a \"positive\" inequality, denoted by superscripts $(+)$.\n2.  Negative coefficient ($\\beta_{i}  0$): $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i} \\implies u \\ge (d_{i} - \\alpha_{i}^{\\top} x) / \\beta_{i}$. Let this be a \"negative\" inequality, denoted by superscripts $(-)$.\n3.  Zero coefficient ($\\beta_{i} = 0$): $\\alpha_{i}^{\\top} x \\le d_{i}$. These inequalities are already in the $x$-space and are part of the projection.\n\nFor a value of $u$ to exist, every lower bound on $u$ must be less than or equal to every upper bound. Combining each negative inequality with each positive one yields a new set of inequalities in $x$:\n$$ \\frac{d^{-} - (\\alpha^{-})^{\\top} x}{\\beta^{-}} \\le \\frac{d^{+} - (\\alpha^{+})^{\\top} x}{\\beta^{+}} $$\nRearranging this gives the formula specified in the problem statement:\n$$ \\left( \\frac{\\alpha^{+}}{\\beta^{+}} - \\frac{\\alpha^{-}}{\\beta^{-}} \\right)^{\\top} x \\le \\frac{d^{+}}{\\beta^{+}} - \\frac{d^{-}}{\\beta^{-}} $$\nThe resulting set $\\operatorname{Pre}(\\mathcal{K}^{k})$ is described by the collection of all such derived inequalities and the original zero-coefficient inequalities.\n\nStep 2: Intersection and Simplification\nThe intersection $\\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X}$ is straightforward in H-representation: the matrices and vectors defining the inequalities of $\\operatorname{Pre}(\\mathcal{K}^{k})$ and $\\mathcal{X}$ are vertically concatenated. This operation, however, introduces many redundant inequalities. To maintain a minimal representation and for the convergence check to be reliable, these redundancies must be removed.\n\nAn inequality $H_{i} x \\le h_{i}$ from a system $H x \\le h$ is redundant if it does not define a facet of the polyhedron. This is tested by solving a linear program (LP):\n$$ v^{\\star} = \\max_{x} \\left\\{ H_{i} x \\mid H_{j} x \\le h_{j} \\text{ for all } j \\neq i \\right\\} $$\nIf $v^{\\star} \\le h_{i} + \\varepsilon$ for a small tolerance $\\varepsilon$, the inequality is redundant and can be removed. Before this, a feasibility LP is solved to check if the set $\\mathcal{K}^{k+1}$ is empty. If it is, the process terminates for this branch, and all subsequent iterates are considered empty.\n\nStep 3: Termination\nThe iterative process is terminated under one of two conditions:\n1.  Convergence: The H-representation of $\\mathcal{K}^{k+1}$ is equivalent to that of $\\mathcal{K}^{k}$. To check this, the non-redundant H-representations $(H_{k}, h_{k})$ and $(H_{k+1}, h_{k+1})$ are converted to a canonical form. This involves normalizing each inequality row (e.g., making the normal vector $H_{i}$ a unit vector) and then sorting the rows lexicographically. The canonical forms are then compared element-wise up to a tolerance $\\varepsilon$.\n2.  Maximum Iterations: The number of iterations reaches a predefined limit $N_{\\max}$.\n\nFinal Step: Area Calculation\nUpon termination, the final iterate $\\mathcal{K}^{\\star}$ is represented by $(H^{\\star}, h^{\\star})$. To compute its area in $\\mathbb{R}^{2}$, we first enumerate its vertices. A vertex is the unique intersection of two boundary-defining hyperplanes, i.e., the solution to a system $H^{\\star}_{\\{i,j\\}, :} x = h^{\\star}_{\\{i,j\\}}$ for two rows $i,j$, provided this solution is feasible with respect to all other inequalities in the system. After finding all unique, feasible vertices, they are sorted by polar angle around their centroid. The area of the resulting polygon is then calculated using the shoelace formula. If the set is empty or has fewer than three vertices, its area is $0$.\n\nThe implementation proceeds by meticulously following these steps for each test case provided.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[0.9, 0.1], [0.0, 0.95]]),\n            \"B\": np.array([[1.0], [0.2]]),\n            \"r1\": 2.0,\n            \"r2\": 2.0,\n            \"umax\": 0.5,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"A\": np.array([[1.2, 0.0], [0.0, 1.1]]),\n            \"B\": np.array([[0.1], [0.0]]),\n            \"r1\": 0.2,\n            \"r2\": 0.2,\n            \"umax\": 0.05,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"A\": np.array([[0.8, 0.3], [-0.2, 0.9]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"r1\": 1.5,\n            \"r2\": 1.5,\n            \"umax\": 0.7,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        area = run_iteration_scheme(params)\n        results.append(f\"{area:.5f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_iteration_scheme(params):\n    \"\"\"\n    Computes the maximal controlled invariant set and its area for a given test case.\n    \"\"\"\n    A, B, r1, r2, umax, Nmax, epsilon = (\n        params[\"A\"], params[\"B\"], params[\"r1\"], params[\"r2\"],\n        params[\"umax\"], params[\"Nmax\"], params[\"epsilon\"]\n    )\n    n, m = A.shape[1], B.shape[1]\n\n    # State constraints X = {x | Gx = g}\n    G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n    g = np.array([r1, r1, r2, r2])\n\n    # Input constraints U = {u | Hu*u = hu}\n    Hu = np.array([[1], [-1]])\n    hu = np.array([umax, umax])\n\n    H_k_prev, h_k_prev = None, None\n    H_k, h_k = G, g\n\n    H_k, h_k, is_empty = remove_redundant_inequalities(H_k, h_k, n, epsilon)\n    if is_empty:\n        return 0.0\n\n    for _ in range(Nmax):\n        H_k_prev, h_k_prev = H_k, h_k\n\n        # Step 1: Compute Pre(K^k)\n        H_pre, h_pre = fourier_motzkin_preimage(H_k, h_k, A, B, Hu, hu, n, m, epsilon)\n\n        # Step 2: Intersect with X\n        H_k1 = np.vstack([H_pre, G])\n        h_k1 = np.hstack([h_pre, g])\n\n        # Step 3: Simplify and check for emptiness\n        H_k, h_k, is_empty = remove_redundant_inequalities(H_k1, h_k1, n, epsilon)\n\n        if is_empty:\n            return 0.0\n\n        # Step 4: Check for convergence\n        if are_sets_equal(H_k, h_k, H_k_prev, h_k_prev, epsilon):\n            break\n\n    return calculate_area(H_k, h_k, n, epsilon)\n\ndef fourier_motzkin_preimage(H_k, h_k, A, B, Hu, hu, n, m, epsilon):\n    \"\"\"\n    Computes the preimage of a polytope using Fourier-Motzkin elimination.\n    \"\"\"\n    # System inequalities: H_k*(A*x + B*u) = h_k  = (H_k*A)*x + (H_k*B)*u = h_k\n    Hk_A = H_k @ A\n    Hk_B = (H_k @ B).flatten()\n\n    # Input inequalities: Hu*u = hu\n    Hu_x = np.zeros((Hu.shape[0], n))\n\n    # Combine all inequalities involving u\n    alpha_coeffs = np.vstack([Hk_A, Hu_x])\n    beta_coeffs = np.hstack([Hk_B, Hu.flatten()])\n    d_rhs = np.hstack([h_k, hu])\n\n    # Partition inequalities based on sign of beta\n    pos_indices = np.where(beta_coeffs  epsilon)[0]\n    neg_indices = np.where(beta_coeffs  -epsilon)[0]\n    zero_indices = np.where(np.abs(beta_coeffs) = epsilon)[0]\n\n    new_inequalities = []\n    # Add zero-coefficient inequalities\n    if len(zero_indices)  0:\n        new_inequalities.append((alpha_coeffs[zero_indices], d_rhs[zero_indices]))\n\n    # Add cross-combinations of positive and negative inequalities\n    for p_idx in pos_indices:\n        for n_idx in neg_indices:\n            alpha_p, beta_p, d_p = alpha_coeffs[p_idx], beta_coeffs[p_idx], d_rhs[p_idx]\n            alpha_n, beta_n, d_n = alpha_coeffs[n_idx], beta_coeffs[n_idx], d_rhs[n_idx]\n            \n            # (alpha_p/beta_p - alpha_n/beta_n) * x = d_p/beta_p - d_n / beta_n\n            new_H_row = alpha_p / beta_p - alpha_n / beta_n\n            new_h_val = d_p / beta_p - d_n / beta_n\n            new_inequalities.append((new_H_row.reshape(1, -1), np.array([new_h_val])))\n\n    if not new_inequalities:\n        # This case happens if the set is unbounded in a way that allows any preimage.\n        # This shouldn't occur in this problem setting with compact sets.\n        # Fallback to a representation of the whole space, will be clipped by X.\n        return np.zeros((0, n)), np.zeros(0)\n\n    H_pre = np.vstack([ineq[0] for ineq in new_inequalities])\n    h_pre = np.concatenate([ineq[1] for ineq in new_inequalities])\n    return H_pre, h_pre\n\ndef remove_redundant_inequalities(H, h, n, epsilon):\n    \"\"\"\n    Removes redundant inequalities from an H-representation {x | Hx = h}.\n    Also returns a flag indicating if the set is empty.\n    \"\"\"\n    if H.shape[0] == 0:\n        return H, h, False\n\n    # Check for feasibility first\n    res = linprog(c=np.zeros(n), A_ub=H, b_ub=h, bounds=(None, None), method='highs-ds')\n    if res.status == 2:  # Infeasible\n        return np.zeros((0, n)), np.zeros(0), True\n    \n    non_redundant_indices = []\n    for i in range(H.shape[0]):\n        # Objective: max H[i] @ x  = min -H[i] @ x\n        c = -H[i]\n        A_ub = np.delete(H, i, axis=0)\n        b_ub = np.delete(h, i, axis=0)\n\n        # In case the remaining constraints describe an empty set\n        res_feasibility = linprog(c=np.zeros(n), A_ub=A_ub, b_ub=b_ub, bounds=(None, None), method='highs-ds')\n        if res_feasibility.status == 2:\n            # If remaining set is empty, this constraint is not redundant\n            # as it's required to define the empty set.\n            non_redundant_indices.append(i)\n            continue\n        \n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(None, None), method='highs-ds')\n        \n        is_redundant = False\n        if res.success:\n            max_val = -res.fun\n            if max_val = h[i] + epsilon:\n                is_redundant = True\n        elif res.status == 3: # Unbounded\n            # max is infinity, so constraint is not redundant\n            is_redundant = False\n\n        if not is_redundant:\n            non_redundant_indices.append(i)\n\n    return H[non_redundant_indices], h[non_redundant_indices], False\n\n\ndef get_canonical_representation(H, h, epsilon):\n    \"\"\"\n    Normalizes and sorts an H-representation to get a canonical form.\n    \"\"\"\n    if H.shape[0] == 0:\n        return H, h\n\n    # Normalize rows\n    norms = np.linalg.norm(H, axis=1)\n    # Avoid division by zero for zero-rows (should not happen for valid inequality)\n    valid_rows = norms  epsilon\n    H_norm = H[valid_rows] / norms[valid_rows, np.newaxis]\n    h_norm = h[valid_rows] / norms[valid_rows]\n    \n    # Sort lexicographically\n    combined = np.hstack([H_norm, h_norm[:, np.newaxis]])\n    sorted_indices = np.lexsort(combined.T[::-1])\n    return H_norm[sorted_indices], h_norm[sorted_indices]\n\n\ndef are_sets_equal(H1, h1, H2, h2, epsilon):\n    \"\"\"\n    Checks if two H-representations define the same set.\n    \"\"\"\n    if H1 is None or H2 is None:\n        return False\n    if H1.shape != H2.shape:\n        return False\n    if H1.shape[0] == 0:\n        return True\n\n    H1_canon, h1_canon = get_canonical_representation(H1, h1, epsilon)\n    H2_canon, h2_canon = get_canonical_representation(H2, h2, epsilon)\n    \n    if H1_canon.shape != H2_canon.shape:\n        return False\n\n    return np.allclose(H1_canon, H2_canon, atol=epsilon) and \\\n           np.allclose(h1_canon, h2_canon, atol=epsilon)\n\n\ndef calculate_area(H, h, n, epsilon):\n    \"\"\"\n    Calculates the area of a 2D polygon defined by Hx = h.\n    \"\"\"\n    if H.shape[0]  n or n != 2:\n        return 0.0\n\n    # Enumerate vertices\n    vertices = []\n    num_constraints = H.shape[0]\n    for i in range(num_constraints):\n        for j in range(i + 1, num_constraints):\n            H_sub = H[[i, j], :]\n            if abs(np.linalg.det(H_sub))  epsilon:\n                continue\n\n            try:\n                vertex = np.linalg.solve(H_sub, h[[i, j]])\n            except np.linalg.LinAlgError:\n                continue\n            \n            # Check feasibility\n            if np.all(H @ vertex = h + epsilon):\n                is_duplicate = False\n                for v in vertices:\n                    if np.linalg.norm(vertex - v)  epsilon:\n                        is_duplicate = True\n                        break\n                if not is_duplicate:\n                    vertices.append(vertex)\n\n    if len(vertices)  3:\n        return 0.0\n    \n    vertices = np.array(vertices)\n    \n    # Sort vertices by angle around the centroid\n    centroid = np.mean(vertices, axis=0)\n    angles = np.arctan2(vertices[:, 1] - centroid[1], vertices[:, 0] - centroid[0])\n    sorted_indices = np.argsort(angles)\n    sorted_vertices = vertices[sorted_indices]\n\n    # Shoelace formula\n    x = sorted_vertices[:, 0]\n    y = sorted_vertices[:, 1]\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n    \n    return area\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2724632"}, {"introduction": "Many real-world systems exhibit nonlinear behavior, posing a significant challenge for control design. This final practice exercise [@problem_id:2724668] provides a window into Nonlinear MPC (NMPC) by guiding you through a single iteration of the Sequential Quadratic Programming (SQP) algorithm. You will linearize nonlinear dynamics, formulate a local Quadratic Programming (QP) approximation, and solve for the optimal control update, demystifying one of the most powerful and widely used methods for controlling nonlinear systems.", "problem": "Consider a discrete-time nonlinear prediction model used in Model Predictive Control (MPC). MPC is an optimization-based feedback control strategy that solves, at each sampling instant, a finite-horizon optimal control problem, and Sequential Quadratic Programming (SQP) is a method that iteratively solves Quadratic Programming (QP) subproblems obtained by local linearization and quadratization. For a scalar state $x \\in \\mathbb{R}$ and scalar input $u \\in \\mathbb{R}$, the model is\n$$\nx_{k+1} \\;=\\; f(x_k,u_k) \\;=\\; x_k \\;+\\; h\\big(\\sin(x_k) + u_k^3\\big),\n$$\nwith sampling period $h0$. Let the finite-horizon cost be\n$$\nJ(x_{0},U) \\;=\\; \\sum_{k=0}^{N-1} \\Big((x_k - x_{\\mathrm{ref}})^2 Q \\;+\\; u_k^2 R\\Big) \\;+\\; (x_N - x_{\\mathrm{ref}})^2 P,\n$$\nwhere $U \\triangleq [u_0,\\dots,u_{N-1}]$, and $Q \\ge 0$, $R  0$, $P \\ge 0$, and $x_{\\mathrm{ref}} \\in \\mathbb{R}$ are given. Assume a direct single-shooting formulation: given an initial state $x_0$ and an initial input sequence guess $U^{(0)}$, simulate the nominal trajectory $\\{x_k^{(0)}\\}_{k=0}^{N}$ using the true nonlinear dynamics $x_{k+1}^{(0)} = f(x_k^{(0)},u_k^{(0)})$.\n\nOne Sequential Quadratic Programming (SQP) iteration constructs a convex Quadratic Programming (QP) subproblem in the perturbations $\\delta x_k \\triangleq x_k - x_k^{(0)}$ and $\\delta u_k \\triangleq u_k - u_k^{(0)}$ by linearizing the dynamics and using the quadratic cost as a local model. Use the following fundamental bases:\n\n- The discrete-time system linearization is given by the first-order Taylor expansion,\n$$\n\\delta x_{k+1} \\;=\\; A_k \\,\\delta x_k \\;+\\; B_k \\,\\delta u_k \\;+\\; r_k,\n$$\nwhere $A_k \\triangleq \\frac{\\partial f}{\\partial x}(x_k^{(0)},u_k^{(0)})$, $B_k \\triangleq \\frac{\\partial f}{\\partial u}(x_k^{(0)},u_k^{(0)})$, and $r_k \\triangleq f(x_k^{(0)},u_k^{(0)}) - x_{k+1}^{(0)}$. Because the nominal trajectory is generated by the true dynamics, you must take $r_k = 0$ for all $k$.\n\n- The quadratic cost expansion around the nominal $(x_k^{(0)},u_k^{(0)})$ yields, up to an additive constant independent of $(\\delta x,\\delta u)$,\n$$\n\\begin{aligned}\n\\tilde{J}(\\delta x,\\delta u) \\;=\\; \\sum_{k=0}^{N-1} \\Big( Q\\,\\delta x_k^2 \\;+\\; 2Q\\,e_k\\,\\delta x_k \\;+\\; R\\,\\delta u_k^2 \\;+\\; 2R\\,u_k^{(0)}\\,\\delta u_k \\Big) \\\\\n+\\; P\\,\\delta x_N^2 \\;+\\; 2P\\,e_N\\,\\delta x_N,\n\\end{aligned}\n$$\nwhere $e_k \\triangleq x_k^{(0)} - x_{\\mathrm{ref}}$. In the direct single-shooting SQP, the initial state is fixed, so $\\delta x_0 = 0$ is not a decision variable.\n\nDefine the QP decision vector by stacking the state and input perturbations\n$$\nv \\;\\triangleq\\; [\\delta x_1,\\dots,\\delta x_N,\\,\\delta u_0,\\dots,\\delta u_{N-1}]^\\top \\in \\mathbb{R}^{2N}.\n$$\nWrite the QP in the standard form\n$$\n\\min_{v \\in \\mathbb{R}^{2N}} \\;\\; \\tfrac{1}{2} v^\\top H v \\;+\\; h^\\top v \\quad \\text{subject to} \\quad E v = b,\n$$\nwith the following ingredients:\n\n- Equality constraints are the linearized dynamics with $\\delta x_0 = 0$,\n$$\n\\delta x_{k+1} - A_k \\delta x_k - B_k \\delta u_k = 0, \\quad k = 0,\\dots,N-1,\n$$\nwhere for $k=0$ the term $A_0 \\delta x_0$ is identically $0$ and therefore omitted. Collect these into $E v = b$ with $b = 0$.\n\n- The quadratic term matrix $H$ and linear term vector $h$ come from $\\tilde{J}$ expressed in $v$ with the convention that the objective is $\\tfrac{1}{2} v^\\top H v + h^\\top v$:\nfor the entries corresponding to $\\delta x_k$ with $k=1,\\dots,N-1$ use the diagonal coefficient $2Q$, for $\\delta x_N$ use $2P$, and for each $\\delta u_k$ use $2R$. The linear term entries are $2Q e_k$ for $\\delta x_k$ with $k=1,\\dots,N-1$, $2P e_N$ for $\\delta x_N$, and $2R u_k^{(0)}$ for $\\delta u_k$.\n\nFormally,\n$$\nH \\;=\\; \\mathrm{diag}(\\underbrace{2Q,\\dots,2Q}_{N-1\\text{ times}},\\,2P,\\,\\underbrace{2R,\\dots,2R}_{N\\text{ times}}), \\quad\nh \\;=\\; [2Q e_1,\\dots,2Q e_{N-1},\\,2P e_N,\\,2R u_0^{(0)},\\dots,2R u_{N-1}^{(0)}]^\\top.\n$$\n\nThe Karush–Kuhn–Tucker (KKT) system for the equality-constrained QP is\n$$\n\\begin{bmatrix}\nH  E^\\top \\\\\nE  0\n\\end{bmatrix}\n\\begin{bmatrix}\nv^\\star \\\\ \\lambda^\\star\n\\end{bmatrix}\n\\;=\\;\n\\begin{bmatrix}\n-\\,h \\\\ b\n\\end{bmatrix},\n$$\nwhose solution yields the unique optimal step $v^\\star$; the component corresponding to $\\delta u_k$ is the desired step $\\delta u_k^\\star$. Report the step for the input sequence as\n$$\n\\delta U^\\star \\;\\triangleq\\; [\\delta u_0^\\star,\\dots,\\delta u_{N-1}^\\star].\n$$\n\nYou must carry out exactly one SQP iteration (build the nominal, compute $A_k$ and $B_k$, assemble the QP, solve the KKT system) for each test case below. Use the fundamental derivatives\n$$\nA_k \\;=\\; \\frac{\\partial f}{\\partial x}(x_k^{(0)},u_k^{(0)}) \\;=\\; 1 + h \\cos\\!\\big(x_k^{(0)}\\big), \\quad\nB_k \\;=\\; \\frac{\\partial f}{\\partial u}(x_k^{(0)},u_k^{(0)}) \\;=\\; 3h \\big(u_k^{(0)}\\big)^2.\n$$\n\nTest suite. For each parameter set, compute $\\delta U^\\star$ as described. All numbers listed are real scalars; vectors are real row vectors.\n\n- Case A: $N = 3$, $h = 0.1$, $Q = 1.0$, $R = 0.1$, $P = 1.0$, $x_0 = 0.5$, $x_{\\mathrm{ref}} = 0.0$, $U^{(0)} = [0.2,\\,0.2,\\,0.2]$.\n- Case B: $N = 3$, $h = 0.1$, $Q = 10.0$, $R = 1.0$, $P = 10.0$, $x_0 = 1.5$, $x_{\\mathrm{ref}} = 1.0$, $U^{(0)} = [-0.1,\\,-0.1,\\,-0.1]$.\n- Case C: $N = 3$, $h = 0.1$, $Q = 1.0$, $R = 0.01$, $P = 1.0$, $x_0 = 0.1$, $x_{\\mathrm{ref}} = 0.0$, $U^{(0)} = [0.0,\\,0.0,\\,0.0]$.\n- Case D: $N = 3$, $h = 0.2$, $Q = 0.5$, $R = 0.05$, $P = 0.5$, $x_0 = -0.7$, $x_{\\mathrm{ref}} = 0.0$, $U^{(0)} = [0.5,\\,-0.5,\\,0.5]$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four vectors $\\delta U^\\star$ enclosed in square brackets, with each vector printed as a comma-separated list enclosed in square brackets and each scalar rounded to six digits after the decimal point. For example, an output with two cases would look like $[[0.123456,-0.000001],[0.000000,0.000000]]$. In this problem there are four cases, so the output must be of the form $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$.", "solution": "The problem presented is a well-defined and standard exercise in computational control theory, specifically concerning the application of Sequential Quadratic Programming (SQP) to a nonlinear Model Predictive Control (MPC) problem. The problem is scientifically grounded, self-contained, and algorithmically specified. It is therefore valid, and a solution will be provided.\n\nThe task is to compute a single SQP step, which involves finding the optimal perturbation to a nominal control sequence, $\\delta U^\\star = [\\delta u_0^\\star, \\dots, \\delta u_{N-1}^\\star]^\\top$. This is achieved by first linearizing the system dynamics and quadratizing the cost function around a nominal trajectory, creating a Quadratic Programming (QP) subproblem. This QP is then solved via its Karush-Kuhn-Tucker (KKT) conditions. The process for each test case consists of four main steps.\n\n**Step 1: Generation of the Nominal Trajectory**\n\nGiven an initial state $x_0$ and a nominal control sequence $U^{(0)} = [u_0^{(0)}, \\dots, u_{N-1}^{(0)}]$, we must first compute the corresponding nominal state trajectory $\\{x_k^{(0)}\\}_{k=0}^{N}$. We begin with $x_0^{(0)} = x_0$ and iteratively apply the nonlinear dynamics:\n$$\nx_{k+1}^{(0)} = f(x_k^{(0)}, u_k^{(0)}) = x_k^{(0)} + h\\big(\\sin(x_k^{(0)}) + (u_k^{(0)})^3\\big) \\quad \\text{for } k=0, \\dots, N-1.\n$$\nThis provides the reference trajectory $(X^{(0)}, U^{(0)})$ around which we will construct the QP.\n\n**Step 2: Linearization of System Dynamics**\n\nThe nonlinear dynamics are linearized around the nominal trajectory. The evolution of the perturbations, $\\delta x_k = x_k - x_k^{(0)}$ and $\\delta u_k = u_k - u_k^{(0)}$, is approximated by the first-order Taylor expansion:\n$$\n\\delta x_{k+1} = A_k \\delta x_k + B_k \\delta u_k,\n$$\nwhere $A_k$ and $B_k$ are the Jacobians of the dynamics function $f(x, u)$ evaluated at each point $(x_k^{(0)}, u_k^{(0)})$ of the nominal trajectory. The partial derivatives are given as:\n$$\nA_k = \\frac{\\partial f}{\\partial x}\\bigg|_{(x_k^{(0)}, u_k^{(0)})} = 1 + h \\cos(x_k^{(0)})\n$$\n$$\nB_k = \\frac{\\partial f}{\\partial u}\\bigg|_{(x_k^{(0)}, u_k^{(0)})} = 3h (u_k^{(0)})^2\n$$\nThese matrices are computed for $k=0, \\dots, N-1$.\n\n**Step 3: Formulation of the QP Subproblem**\n\nThe QP subproblem is formulated in terms of the decision vector $v \\triangleq [\\delta x_1, \\dots, \\delta x_N, \\delta u_0, \\dots, \\delta u_{N-1}]^\\top \\in \\mathbb{R}^{2N}$. The problem is to minimize a quadratic objective subject to linear equality constraints:\n$$\n\\min_{v \\in \\mathbb{R}^{2N}} \\;\\; \\tfrac{1}{2} v^\\top H v \\;+\\; h^\\top v \\quad \\text{subject to} \\quad E v = b.\n$$\nThe components of this QP are constructed as follows:\n\n-   **Objective Function:** The matrices $H$ and $h$ are derived from the second-order Taylor expansion of the cost function $J$ around the nominal trajectory. The problem specifies them directly.\n    The Hessian matrix $H \\in \\mathbb{R}^{2N \\times 2N}$ is a diagonal matrix:\n    $$\n    H = \\mathrm{diag}(\\underbrace{2Q, \\dots, 2Q}_{N-1 \\text{ times}}, 2P, \\underbrace{2R, \\dots, 2R}_{N \\text{ times}}).\n    $$\n    The gradient vector $h \\in \\mathbb{R}^{2N}$ is constructed from the tracking errors $e_k = x_k^{(0)} - x_{\\mathrm{ref}}$ and the nominal inputs $u_k^{(0)}$:\n    $$\n    h = [2Q e_1, \\dots, 2Q e_{N-1}, 2P e_N, 2R u_0^{(0)}, \\dots, 2R u_{N-1}^{(0)}]^\\top.\n    $$\n\n-   **Equality Constraints:** The constraints enforce the linearized dynamics. For $k=0, \\dots, N-1$, we have $\\delta x_{k+1} - A_k \\delta x_k - B_k \\delta u_k = 0$. Since the initial state is fixed, $\\delta x_0 = 0$. These $N$ linear equations are assembled into the matrix equation $E v = b$.\n    The constraint matrix $E \\in \\mathbb{R}^{N \\times 2N}$ populates the coefficients of the decision variables in each equation. For $N=3$, the structure is:\n    $$\n    E = \\begin{bmatrix}\n    1  0  0  -B_0  0  0 \\\\\n    -A_1  1  0  0  -B_1  0 \\\\\n    0  -A_2  1  0  0  -B_2\n    \\end{bmatrix}.\n    $$\n    The right-hand side vector $b$ is the zero vector, $b=0 \\in \\mathbb{R}^N$.\n\n**Step 4: Solution of the KKT System**\n\nThe unique solution to the equality-constrained QP is found by solving the Karush-Kuhn-Tucker (KKT) system of linear equations. This system combines the optimality condition (gradient of the Lagrangian is zero) and the feasibility condition (constraints are met):\n$$\n\\begin{bmatrix}\nH  E^\\top \\\\\nE  0\n\\end{bmatrix}\n\\begin{bmatrix}\nv^\\star \\\\ \\lambda^\\star\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-h \\\\ b\n\\end{bmatrix},\n$$\nwhere $v^\\star$ is the optimal perturbation vector and $\\lambda^\\star \\in \\mathbb{R}^N$ is the vector of Lagrange multipliers. For our problem, $b=0$. The KKT matrix is a square, symmetric matrix of size $(3N \\times 3N)$. Solving this system yields $v^\\star$.\n\nThe desired result, $\\delta U^\\star$, is extracted from the solution vector $v^\\star$. Specifically, the components of $v^\\star$ corresponding to the input perturbations, $[\\delta u_0^\\star, \\dots, \\delta u_{N-1}^\\star]^\\top$, constitute the final answer for one SQP iteration. This procedure is executed for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the SQP step for each test case provided in the problem statement.\n    \"\"\"\n\n    test_cases = [\n        # Case A: N, h, Q, R, P, x0, xref, U0\n        (3, 0.1, 1.0, 0.1, 1.0, 0.5, 0.0, np.array([0.2, 0.2, 0.2])),\n        # Case B: N, h, Q, R, P, x0, xref, U0\n        (3, 0.1, 10.0, 1.0, 10.0, 1.5, 1.0, np.array([-0.1, -0.1, -0.1])),\n        # Case C: N, h, Q, R, P, x0, xref, U0\n        (3, 0.1, 1.0, 0.01, 1.0, 0.1, 0.0, np.array([0.0, 0.0, 0.0])),\n        # Case D: N, h, Q, R, P, x0, xref, U0\n        (3, 0.2, 0.5, 0.05, 0.5, -0.7, 0.0, np.array([0.5, -0.5, 0.5])),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N, h_param, Q, R, P, x0, xref, U0 = case\n\n        # Step 1: Generate nominal trajectory\n        x_nominal = np.zeros(N + 1)\n        x_nominal[0] = x0\n        for k in range(N):\n            x_nominal[k+1] = x_nominal[k] + h_param * (np.sin(x_nominal[k]) + U0[k]**3)\n        \n        # Step 2: Linearize system dynamics\n        A = np.zeros(N)\n        B = np.zeros(N)\n        for k in range(N):\n            A[k] = 1 + h_param * np.cos(x_nominal[k])\n            B[k] = 3 * h_param * U0[k]**2\n            \n        # Step 3: Formulate QP subproblem (H, h, E, b)\n        # Decision vector v = [dx1, ..., dxN, du0, ..., du(N-1)]\n        \n        # Hessian H\n        diag_H = np.zeros(2*N)\n        diag_H[0:N-1] = 2 * Q\n        diag_H[N-1] = 2 * P\n        diag_H[N:2*N] = 2 * R\n        H_mat = np.diag(diag_H)\n\n        # Gradient h\n        h_vec = np.zeros(2*N)\n        errors = x_nominal - xref\n        for k in range(1, N + 1):\n            if k  N:\n                 h_vec[k-1] = 2 * Q * errors[k]\n            else: # k == N\n                 h_vec[k-1] = 2 * P * errors[k]\n        for k in range(N):\n            h_vec[N+k] = 2 * R * U0[k]\n            \n        # Constraint matrix E (for Ev = b)\n        E_mat = np.zeros((N, 2*N))\n        # k=0: dx1 - B0 du0 = 0\n        E_mat[0, 0] = 1.0 \n        E_mat[0, N] = -B[0] \n        # k=1..N-1: dx_{k+1} - Ak dxk - Bk duk = 0\n        for k in range(1, N):\n            E_mat[k, k] = 1.0 \n            E_mat[k, k-1] = -A[k] \n            E_mat[k, N+k] = -B[k] \n        \n        # b is zero vector\n        b_vec = np.zeros(N)\n\n        # Step 4: Solve the KKT system\n        kkt_mat_size = 3 * N\n        kkt_mat = np.zeros((kkt_mat_size, kkt_mat_size))\n        \n        kkt_mat[0:2*N, 0:2*N] = H_mat\n        kkt_mat[0:2*N, 2*N:3*N] = E_mat.T\n        kkt_mat[2*N:3*N, 0:2*N] = E_mat\n        \n        rhs = np.concatenate([-h_vec, b_vec])\n        \n        # Solve the linear system\n        solution = np.linalg.solve(kkt_mat, rhs)\n\n        # Extract delta_U^*\n        v_star = solution[0:2*N]\n        delta_u_star = v_star[N:]\n        \n        all_results.append(delta_u_star)\n\n    # Format the final output string\n    result_strings = []\n    for res in all_results:\n        res_str = '[' + ','.join([f'{val:.6f}' for val in res]) + ']'\n        result_strings.append(res_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "2724668"}]}