{"hands_on_practices": [{"introduction": "Before designing an estimator for a networked system, we must answer a fundamental question: is it even possible to determine the system's state from the available measurements? This practice explores the concept of collective observability, which assesses whether the union of all sensor measurements across the network is sufficient to reconstruct the full state. By applying the Popov-Belevitch-Hautus (PBH) test, you will develop your skill in identifying unobservable modes and determining the minimal sensor additions needed to ensure full state visibility [@problem_id:2701991].", "problem": "Consider a linear time-invariant networked system with state dimension $n = 4$ governed by $\\dot{x}(t) = A x(t)$, where\n$$\nA = \\begin{pmatrix}\n1  0  0  0\\\\\n0  1  0  0\\\\\n0  0  2  0\\\\\n0  0  0  3\n\\end{pmatrix}.\n$$\nThere are $3$ sensing agents (nodes), indexed by $i \\in \\{1,2,3\\}$, each with a local measurement $y_i(t) = C_i x(t)$. The local sensing matrices are\n$$\nC_1 = \\begin{pmatrix}1  0  0  0\\end{pmatrix},\\quad\nC_2 = \\begin{pmatrix}0  0  1  0\\end{pmatrix},\\quad\nC_3 = \\begin{pmatrix}0  0  0  1\\end{pmatrix}.\n$$\nThe directed communication graph among the $3$ agents is strongly connected, with edges $\\{(1,2),(2,3),(3,1)\\}$. The agents can exchange information over this graph.\n\nUsing only foundational definitions from linear systems and networked estimation, determine whether the system is collectively observable under the union of all available sensors (that is, with the stacked measurement matrix $C = \\begin{pmatrix} C_1^{\\vphantom{\\top}} \\\\ C_2^{\\vphantom{\\top}} \\\\ C_3^{\\vphantom{\\top}} \\end{pmatrix}$). If it is not observable, compute the minimal number $m$ of additional scalar sensors of the form $y_{\\text{add}}(t) = e_j^{\\top} x(t)$, where $e_j$ is a standard basis vector in $\\mathbb{R}^{4}$ and $j \\in \\{1,2,3,4\\}$, that must be added (and can be placed at any nodes) so that the unioned sensing renders the pair $(A,C_{\\text{aug}})$ observable. Provide your final answer as the integer $m$. If the system is already observable, give $m=0$. The final answer must be a single integer; no rounding instruction is necessary.", "solution": "The problem asks for an analysis of the collective observability of a given linear time-invariant system and for the determination of the minimal number of additional standard sensors required to achieve observability if it is not already present.\n\nFirst, we must formalize the notion of \"collective observability\" as specified. The problem states this is observability under the \"union of all available sensors.\" For a linear system $\\dot{x}(t) = Ax(t)$, this is equivalent to checking the observability of the pair $(A, C)$, where $A$ is the state matrix and $C$ is the stacked matrix of all individual sensor matrices $C_i$. The information regarding the communication graph topology is relevant for *distributed* observability, where each agent must estimate the state using local measurements and information from its neighbors. However, for collective observability, we consider a centralized scenario where all measurements are available at once.\n\nThe state matrix is given as:\n$$\nA = \\begin{pmatrix}\n1  0  0  0\\\\\n0  1  0  0\\\\\n0  0  2  0\\\\\n0  0  0  3\n\\end{pmatrix}\n$$\nThe individual sensor matrices are:\n$$\nC_1 = \\begin{pmatrix}1  0  0  0\\end{pmatrix},\\quad\nC_2 = \\begin{pmatrix}0  0  1  0\\end{pmatrix},\\quad\nC_3 = \\begin{pmatrix}0  0  0  1\\end{pmatrix}\n$$\nThe collective measurement matrix $C$ is formed by stacking these individual matrices:\n$$\nC = \\begin{pmatrix} C_1 \\\\ C_2 \\\\ C_3 \\end{pmatrix} = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\n\\end{pmatrix}\n$$\nThe state dimension is $n=4$. To determine if the pair $(A, C)$ is observable, we will employ the Popov-Belevitch-Hautus (PBH) test. The PBH test states that the pair $(A, C)$ is observable if and only if the matrix\n$$\n\\begin{pmatrix} A - \\lambda I \\\\ C \\end{pmatrix}\n$$\nhas full column rank, which is $n=4$, for every eigenvalue $\\lambda$ of $A$.\n\nSince $A$ is a diagonal matrix, its eigenvalues are its diagonal entries. The set of eigenvalues of $A$, denoted $\\sigma(A)$, is $\\{1, 1, 2, 3\\}$. We must check the PBH rank condition for each distinct eigenvalue: $\\lambda=1$, $\\lambda=2$, and $\\lambda=3$.\n\nCase 1: $\\lambda = 2$\nThe eigenvector corresponding to this eigenvalue is $v_2 = e_3 = (0, 0, 1, 0)^{\\top}$. A mode is observable if and only if $Cv \\neq 0$ for any eigenvector $v$ associated with that mode.\n$$\nC v_2 = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\neq 0\n$$\nSince $C v_2 \\neq 0$, the mode associated with $\\lambda=2$ is observable.\n\nCase 2: $\\lambda = 3$\nThe eigenvector corresponding to this eigenvalue is $v_3 = e_4 = (0, 0, 0, 1)^{\\top}$. We check:\n$$\nC v_3 = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\neq 0\n$$\nSince $C v_3 \\neq 0$, the mode associated with $\\lambda=3$ is observable.\n\nCase 3: $\\lambda = 1$\nThis eigenvalue has an algebraic multiplicity of $2$. The matrix $A$ is diagonal, so the geometric multiplicity is also $2$. The eigenspace associated with $\\lambda=1$ is spanned by the eigenvectors $v_{1a} = e_1 = (1, 0, 0, 0)^{\\top}$ and $v_{1b} = e_2 = (0, 1, 0, 0)^{\\top}$.\nThe pair $(A,C)$ is unobservable if there exists a non-zero eigenvector $v$ of $A$ in the kernel of $C$, i.e., $Cv=0$. We check this for the basis vectors of the eigenspace for $\\lambda=1$.\nFor $v_{1a} = e_1$:\n$$\nC e_1 = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\neq 0\n$$\nFor $v_{1b} = e_2$:\n$$\nC e_2 = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0\n$$\nWe have found a non-zero eigenvector, $v_{1b} = e_2$, such that $C v_{1b} = 0$. This means the mode associated with this eigenvector is unobservable. The unobservable subspace is the span of this eigenvector, which is $\\text{span}\\{e_2\\}$. Consequently, the system is not collectively observable.\n\nThe problem now is to find the minimum number $m$ of additional sensors of the form $y_{\\text{add}}(t) = e_j^{\\top} x(t)$ for $j \\in \\{1,2,3,4\\}$ required to make the system observable. Adding such a sensor corresponds to appending the row vector $e_j^{\\top}$ to the measurement matrix $C$. Let the augmented matrix be $C_{\\text{aug}}$.\nTo render the unobservable mode corresponding to $e_2$ observable, the augmented measurement matrix $C_{\\text{aug}}$ must satisfy $C_{\\text{aug}}e_2 \\neq 0$. This requires that at least one of the added rows $e_j^{\\top}$ satisfies $e_j^{\\top} e_2 \\neq 0$.\nWe evaluate the product $e_j^{\\top} e_2$ for each possible $j$:\n\\begin{itemize}\n    \\item $j=1$: $e_1^{\\top} e_2 = \\begin{pmatrix}1  0  0  0\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} = 0$.\n    \\item $j=2$: $e_2^{\\top} e_2 = \\begin{pmatrix}0  1  0  0\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} = 1 \\neq 0$.\n    \\item $j=3$: $e_3^{\\top} e_2 = \\begin{pmatrix}0  0  1  0\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} = 0$.\n    \\item $j=4$: $e_4^{\\top} e_2 = \\begin{pmatrix}0  0  0  1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} = 0$.\n\\end{itemize}\nThe only choice of sensor that can observe the state component $x_2$, and thus the unobservable mode, is the one corresponding to $j=2$, i.e., $y_{\\text{add}}(t) = e_2^{\\top}x(t) = x_2(t)$.\nAdding this single sensor is sufficient. Let us form the new augmented matrix $C_{\\text{aug}}$ with this additional sensor:\n$$\nC_{\\text{aug}} = \\begin{pmatrix} C \\\\ e_2^{\\top} \\end{pmatrix} = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\\\\\n0  1  0  0\n\\end{pmatrix}\n$$\nThis matrix is a permutation matrix and thus has full rank $4$. Now we must check the observability of the pair $(A, C_{\\text{aug}})$. The modes for $\\lambda=2$ and $\\lambda=3$ remain observable since adding a sensor cannot remove observability. We need only re-examine the eigenspace for $\\lambda=1$.\nAny vector $v$ in this eigenspace is of the form $v = \\alpha e_1 + \\beta e_2 = (\\alpha, \\beta, 0, 0)^{\\top}$. The mode is unobservable if $C_{\\text{aug}}v=0$ for some non-zero $v$ (i.e., $\\alpha$ or $\\beta$ is non-zero).\n$$\nC_{\\text{aug}}v = \\begin{pmatrix}\n1  0  0  0\\\\\n0  0  1  0\\\\\n0  0  0  1\\\\\n0  1  0  0\n\\end{pmatrix}\n\\begin{pmatrix} \\alpha \\\\ \\beta \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} \\alpha \\\\ 0 \\\\ 0 \\\\ \\beta \\end{pmatrix}\n$$\nFor $C_{\\text{aug}}v = 0$, we require $\\alpha=0$ and $\\beta=0$, which implies $v=0$. Thus, there are no non-zero unobservable eigenvectors in this eigenspace. The system $(A, C_{\\text{aug}})$ is now fully observable.\nSince the original system was unobservable, at least one sensor must be added. We have demonstrated that adding one specific sensor is sufficient. Therefore, the minimal number of additional sensors required is $m=1$.\nThe final answer must be a single integer, which is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2701991"}, {"introduction": "A cornerstone of distributed coordination is the consensus algorithm, which enables agents to agree on a common value using only local information exchange. This exercise investigates an output-feedback consensus protocol, where agents rely on local measurements to update their states. You will derive the conditions for convergence by analyzing the spectral properties of the system's iteration matrix, which combines the network's graph Laplacian with local sensing capabilities via the Kronecker product [@problem_id:2702015]. This practice is crucial for understanding how network topology, sensing, and algorithm parameters collectively determine the stability of distributed protocols.", "problem": "Consider a network of $n$ agents with identical local state dimension $p \\in \\mathbb{N}$. Stack the local states into a global vector $x_k \\in \\mathbb{R}^{np}$ at discrete time $k \\in \\mathbb{N}$. Let $L \\in \\mathbb{R}^{n \\times n}$ be the graph Laplacian of an undirected graph, and let $I_p \\in \\mathbb{R}^{p \\times p}$ be the identity. Consider the distributed output-feedback consensus iteration\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\alpha \\, (L \\otimes I_p) \\, y_k, \n\\qquad y_k \\;=\\; C \\, x_k,\n$$\nwith constant step size $\\alpha \\in \\mathbb{R}$ and output matrix $C \\in \\mathbb{R}^{np \\times np}$. Assume $C$ is block diagonal and identical across agents, i.e., $C = I_n \\otimes C_{\\mathrm{loc}}$ with $C_{\\mathrm{loc}} \\in \\mathbb{R}^{p \\times p}$ symmetric. Denote the consensus subspace by $\\mathcal{S} \\triangleq \\{ \\mathbf{1}_n \\otimes v : v \\in \\mathbb{R}^p \\}$, where $\\mathbf{1}_n \\in \\mathbb{R}^n$ is the vector of all ones.\n\nYour tasks are:\n\n1) Starting only from core definitions and well-tested facts about graph Laplacians and Kronecker products, derive the spectrum of the linear iteration matrix $M \\triangleq I_{np} - \\alpha \\,(L \\otimes I_p) \\, C$. Then, using this spectrum, derive necessary and sufficient conditions on $\\alpha$, on the eigenvalues of $L$, and on the eigenvalues of $C_{\\mathrm{loc}}$ to guarantee that for every initial condition $x_0 \\in \\mathbb{R}^{np}$ the sequence $(x_k)_{k \\in \\mathbb{N}}$ converges to the consensus subspace $\\mathcal{S}$. Express these conditions in a detectability form for the pair $(C, I_{np})$ that ensures that the only unit-modulus eigenvalues of $M$ correspond to $\\mathcal{S}$. Provide your conditions in terms of:\n- the connectivity of the graph encoded by $L$,\n- the definiteness of $C_{\\mathrm{loc}}$,\n- and the admissible range of $\\alpha$ as a function of the largest eigenvalues of $L$ and $C_{\\mathrm{loc}}$.\n\n2) Implement a program that, for each test case below, returns a boolean indicating whether your derived conditions are satisfied so that every trajectory converges to consensus. The program must not simulate trajectories; it must decide using only spectral conditions derived in Part 1. The agreement notion is strict global consensus across all agents, i.e., convergence to $\\mathcal{S}$ for every initial $x_0$.\n\nFoundational facts you may use:\n- For an undirected connected graph, the Laplacian $L$ is symmetric positive semidefinite with a simple zero eigenvalue, i.e., $\\lambda_1(L) = 0$ and $\\lambda_i(L)  0$ for all $i \\in \\{2,\\dots,n\\}$.\n- For symmetric matrices $A \\in \\mathbb{R}^{a \\times a}$ and $B \\in \\mathbb{R}^{b \\times b}$, the eigenvalues of the Kronecker product $A \\otimes B$ are the pairwise products of the eigenvalues of $A$ and $B$.\n- The sequence $x_{k+1} = M x_k$ converges for all initial conditions to the eigenspace associated with eigenvalues of unit magnitude if and only if all other eigenvalues lie strictly inside the unit disk and the eigenvalues on the unit circle are semisimple.\n\nTest suite:\n- Test $1$ (happy path): $n = 4$, $p = 2$, \n  $$\n  L = \\begin{bmatrix}\n  1  -1  0  0 \\\\\n  -1  2  -1  0 \\\\\n  0  -1  2  -1 \\\\\n  0  0  -1  1\n  \\end{bmatrix}, \n  \\quad \n  C_{\\mathrm{loc}} = \\begin{bmatrix} 1  0 \\\\ 0  2 \\end{bmatrix}, \n  \\quad \n  \\alpha = 0.1464466094067262.\n  $$\n- Test $2$ (boundary step size): same $L$ and $C_{\\mathrm{loc}}$ as Test $1$, with \n  $$\n  \\alpha = 0.2928932188134524.\n  $$\n- Test $3$ (too large step size): same $L$ and $C_{\\mathrm{loc}}$ as Test $1$, with \n  $$\n  \\alpha = 0.3661165235168155.\n  $$\n- Test $4$ (undetectable output directions): same $L$ as Test $1$, \n  $$\n  C_{\\mathrm{loc}} = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}, \n  \\quad \n  \\alpha = 0.1.\n  $$\n- Test $5$ (disconnected graph): $n = 4$, $p = 1$, \n  $$\n  L = \\begin{bmatrix}\n  1  -1  0  0 \\\\\n  -1  1  0  0 \\\\\n  0  0  1  -1 \\\\\n  0  0  -1  1\n  \\end{bmatrix}, \n  \\quad \n  C_{\\mathrm{loc}} = \\begin{bmatrix} 1 \\end{bmatrix}, \n  \\quad \n  \\alpha = 0.5.\n  $$\n\nFinal output specification:\n- Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, i.e., of the form $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is the boolean outcome for Test $i$ in the order listed above.", "solution": "The validity of the problem statement is hereby confirmed. It is a well-posed, scientifically grounded problem within the domain of distributed control theory. We may proceed to the solution.\n\nThe system dynamics are described by the linear time-invariant iteration\n$$x_{k+1} = M x_k$$\nwhere the iteration matrix $M \\in \\mathbb{R}^{np \\times np}$ is given by\n$$M \\triangleq I_{np} - \\alpha \\, (L \\otimes I_p) \\, C.$$\nWe are given that $C = I_n \\otimes C_{\\mathrm{loc}}$. Substituting this into the expression for $M$ and using the mixed-product property of the Kronecker product, $(A \\otimes B)(D \\otimes F) = (AD \\otimes BF)$, yields\n$$M = I_n \\otimes I_p - \\alpha (L \\otimes I_p) (I_n \\otimes C_{\\mathrm{loc}}) = I_n \\otimes I_p - \\alpha (L I_n \\otimes I_p C_{\\mathrm{loc}}) = I_n \\otimes I_p - \\alpha (L \\otimes C_{\\mathrm{loc}}).$$\nThe problem requires that for any initial condition $x_0 \\in \\mathbb{R}^{np}$, the state trajectory $x_k$ converges to the consensus subspace $\\mathcal{S} \\triangleq \\{ \\mathbf{1}_n \\otimes v : v \\in \\mathbb{R}^p \\}$. For a linear system, this means that the disagreement component of the state must converge to zero. This is governed by the spectral properties of the matrix $M$. Specifically, convergence to the subspace $\\mathcal{S}$ is guaranteed if and only if two conditions on the spectrum of $M$ are met:\n1. The eigenspace of $M$ corresponding to eigenvalues of unit magnitude is precisely the consensus subspace $\\mathcal{S}$.\n2. All other eigenvalues of $M$ are strictly inside the open unit disk in the complex plane (i.e., their magnitude is strictly less than $1$).\n\nThe problem states that both $L$ (for an undirected graph) and $C_{\\mathrm{loc}}$ are symmetric matrices. Consequently, they are diagonalizable and have real eigenvalues. Let the eigenvalues of $L$ be $\\lambda_1(L), \\dots, \\lambda_n(L)$ with corresponding eigenvectors $v_1, \\dots, v_n$. Let the eigenvalues of $C_{\\mathrm{loc}}$ be $\\mu_1(C_{\\mathrm{loc}}), \\dots, \\mu_p(C_{\\mathrm{loc}})$ with corresponding eigenvectors $u_1, \\dots, u_p$.\nThe eigenvalues of the Kronecker product $L \\otimes C_{\\mathrm{loc}}$ are all the pairwise products of the eigenvalues of $L$ and $C_{\\mathrm{loc}}$, i.e., $\\{\\lambda_i(L)\\mu_j(C_{\\mathrm{loc}})\\}_{i=1,\\dots,n, j=1,\\dots,p}$. The corresponding eigenvectors are $\\{v_i \\otimes u_j\\}_{i,j}$.\nThe matrix $M$ is of the form $I - \\alpha A$ where $A = L \\otimes C_{\\mathrm{loc}}$. The eigenvalues of $M$, denoted $\\gamma_{i,j}$, are therefore given by\n$$\\gamma_{i,j} = 1 - \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}}), \\quad \\forall i \\in \\{1,\\dots,n\\}, \\forall j \\in \\{1,\\dots,p\\}.$$\nSince $L$ and $C_{\\mathrm{loc}}$ are symmetric, $L \\otimes C_{\\mathrm{loc}}$ is symmetric, and thus $M$ is symmetric. This guarantees that all eigenvalues of $M$ are real and semisimple, satisfying the condition from the problem statement.\n\nWe now derive the necessary and sufficient conditions by analyzing the eigenvalues $\\gamma_{i,j}$. We sort the eigenvalues of $L$ such that $0 \\le \\lambda_1(L) \\le \\lambda_2(L) \\le \\dots \\le \\lambda_n(L)$.\n\n**Condition 1: Graph Connectivity**\nThe graph Laplacian $L$ of any undirected graph is positive semidefinite, so $\\lambda_1(L) = 0$. The corresponding eigenvector is $v_1 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n$. For this mode ($i=1$), the eigenvalues of $M$ are:\n$$\\gamma_{1,j} = 1 - \\alpha \\lambda_1(L) \\mu_j(C_{\\mathrm{loc}}) = 1 - \\alpha \\cdot 0 \\cdot \\mu_j(C_{\\mathrm{loc}}) = 1, \\quad \\forall j \\in \\{1, \\dots, p\\}.$$\nThe eigenvectors corresponding to these $p$ eigenvalues are $\\{v_1 \\otimes u_j\\}_{j=1,\\dots,p} = \\{\\frac{1}{\\sqrt{n}}\\mathbf{1}_n \\otimes u_j\\}_{j=1,\\dots,p}$. The span of these eigenvectors is precisely the consensus subspace $\\mathcal{S}$, since the eigenvectors $\\{u_j\\}$ of $C_{\\mathrm{loc}}$ form a basis for $\\mathbb{R}^p$.\nFor convergence to $\\mathcal{S}$, these must be the only unit-magnitude eigenvalues. If the graph is disconnected, $L$ has multiple zero eigenvalues. Let's say $\\lambda_2(L) = 0$. Then, for $i=2$, we would have $\\gamma_{2,j} = 1 - \\alpha \\cdot 0 \\cdot \\mu_j(C_{\\mathrm{loc}}) = 1$ for all $j$. The eigenvector $v_2$ corresponding to $\\lambda_2(L)$ is orthogonal to $v_1 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n$. Thus, the eigenvectors $\\{v_2 \\otimes u_j\\}$ represent disagreement states which do not decay, preventing convergence to global consensus. Therefore, a simple zero eigenvalue for $L$ is required. This is true if and only if the graph is connected.\n**Condition I:** The graph must be connected, which is equivalent to $\\lambda_2(L)  0$.\n\n**Condition 2: Detectability and Definiteness of $C_{\\mathrm{loc}}$**\nFor all disagreement modes (i.e., for all $i \\in \\{2, \\dots, n\\}$ and $j \\in \\{1, \\dots, p\\}$), we require the corresponding eigenvalues of $M$ to have a magnitude strictly less than $1$:\n$$|\\gamma_{i,j}| = |1 - \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})|  1.$$\nThis is equivalent to the two-sided inequality:\n$$-1  1 - \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})  1.$$\nThe right-hand side implies $0  \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})$. If any $\\mu_j(C_{\\mathrm{loc}})$ were zero, this condition would be violated. The corresponding eigenvalue would be $\\gamma_{i,j}=1$, leading to a non-decaying disagreement mode. This corresponds to a failure of detectability: the mode is in the nullspace of the output matrix $C=I_n \\otimes C_{\\mathrm{loc}}$ and is thus unobservable to the control feedback. Therefore, all eigenvalues of $C_{\\mathrm{loc}}$ must be non-zero. Assuming the standard gradient-descent form with $\\alpha  0$, and knowing $\\lambda_i(L)  0$ for $i  1$ from Condition I, we must have $\\mu_j(C_{\\mathrm{loc}})  0$ for all $j$. This means $C_{\\mathrm{loc}}$ must be positive definite.\n**Condition II:** The matrix $C_{\\mathrm{loc}}$ must be positive definite, which is equivalent to its smallest eigenvalue satisfying $\\mu_{\\min}(C_{\\mathrm{loc}})  0$.\n\n**Condition 3: Admissible Range for Step Size $\\alpha$**\nWith Conditions I and II satisfied and assuming $\\alpha  0$, the inequality $0  \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})$ holds for all disagreement modes. We are left with the left-hand side of the inequality:\n$$1 - \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})  -1 \\implies \\alpha \\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})  2.$$\nThis must hold for all $i \\in \\{2, \\dots, n\\}$ and $j \\in \\{1, \\dots, p\\}$. To guarantee this, the condition must hold for the maximum possible value of the product $\\lambda_i(L) \\mu_j(C_{\\mathrm{loc}})$. This maximum is $\\lambda_{\\max}(L) \\mu_{\\max}(C_{\\mathrm{loc}})$, where $\\lambda_{\\max}(L) = \\lambda_n(L)$ and $\\mu_{\\max}(C_{\\mathrm{loc}})$ are the largest eigenvalues of their respective matrices.\nThus, we must have $\\alpha \\lambda_n(L) \\mu_{\\max}(C_{\\mathrm{loc}})  2$, which gives the upper bound for $\\alpha$:\n$$\\alpha  \\frac{2}{\\lambda_n(L) \\mu_{\\max}(C_{\\mathrm{loc}})}.$$\nCombined with the assumption $\\alpha0$, we have derived the full range for the step size.\n**Condition III:** The step size must satisfy $0  \\alpha  \\frac{2}{\\lambda_n(L) \\mu_{\\max}(C_{\\mathrm{loc}})}$. The strict inequalities are crucial, as equality would result in an eigenvalue of $-1$, which corresponds to an oscillating, non-converging disagreement mode.\n\nIn summary, the necessary and sufficient conditions for every trajectory to converge to the consensus subspace $\\mathcal{S}$ are:\n1.  **Connectivity:** The graph is connected ($\\lambda_2(L)  0$).\n2.  **Detectability:** The local output matrix $C_{\\mathrm{loc}}$ is positive definite ($\\mu_{\\min}(C_{\\mathrm{loc}})  0$).\n3.  **Stability:** The step size $\\alpha$ is in the range $0  \\alpha  \\frac{2}{\\lambda_n(L) \\mu_{\\max}(C_{\\mathrm{loc}})}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef check_convergence_conditions(L, C_loc, alpha):\n    \"\"\"\n    Checks if the necessary and sufficient conditions for convergence to consensus are met.\n\n    Args:\n        L (np.ndarray): The graph Laplacian matrix.\n        C_loc (np.ndarray): The local output matrix.\n        alpha (float): The step size.\n\n    Returns:\n        bool: True if conditions are met, False otherwise.\n    \"\"\"\n    TOL = 1e-9  # Tolerance for floating point comparisons\n\n    # Condition 1: Graph Connectivity\n    # The graph must be connected, which means the Laplacian L has a simple zero eigenvalue.\n    # np.linalg.eigvalsh is used for symmetric matrices. It is faster and more stable.\n    try:\n        lambda_L = np.linalg.eigvalsh(L)\n    except np.linalg.LinAlgError:\n        return False  # Matrix is not valid\n    \n    n = L.shape[0]\n    if n  1:\n        # The second smallest eigenvalue must be strictly positive.\n        if lambda_L[1] = TOL:\n            return False  # Graph is not connected\n    \n    # Condition 2: C_loc Definiteness\n    # C_loc must be positive definite, meaning all its eigenvalues are strictly positive.\n    try:\n        mu_C = np.linalg.eigvalsh(C_loc)\n    except np.linalg.LinAlgError:\n        return False  # Matrix is not valid\n        \n    mu_min = mu_C.min()\n    if mu_min = TOL:\n        return False  # C_loc is not positive definite\n    \n    # Condition 3: Step Size Range\n    # The step size alpha must be in the range 0  alpha  2 / (lambda_max(L) * mu_max(C_loc)).\n    lambda_max = lambda_L[-1]\n    mu_max = mu_C.max()\n    \n    # If the graph has only one node, lambda_max is 0.\n    # In this case, consensus is trivial, and any positive alpha works\n    # assuming C_loc is PD. However, all test cases have n  1.\n    if lambda_max = TOL:\n        # This case is handled by Condition 1 for n  1.\n        # This is just a safeguard.\n        return True\n        \n    alpha_upper_bound = 2.0 / (lambda_max * mu_max)\n    \n    # The inequalities must be strict.\n    is_alpha_valid = (alpha  TOL) and (alpha  alpha_upper_bound)\n    \n    if not is_alpha_valid:\n        return False\n        \n    # All three conditions are satisfied.\n    return True\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test 1: Happy path\n    L1 = np.array([\n        [1, -1, 0, 0],\n        [-1, 2, -1, 0],\n        [0, -1, 2, -1],\n        [0, 0, -1, 1]\n    ], dtype=float)\n    C_loc1 = np.array([\n        [1, 0],\n        [0, 2]\n    ], dtype=float)\n    alpha1 = 0.1464466094067262\n    \n    # Test 2: Boundary step size\n    L2 = L1\n    C_loc2 = C_loc1\n    alpha2 = 0.2928932188134524\n    \n    # Test 3: Too large step size\n    L3 = L1\n    C_loc3 = C_loc1\n    alpha3 = 0.3661165235168155\n    \n    # Test 4: Undetectable output directions\n    L4 = L1\n    C_loc4 = np.array([\n        [1, 0],\n        [0, 0]\n    ], dtype=float)\n    alpha4 = 0.1\n    \n    # Test 5: Disconnected graph\n    L5 = np.array([\n        [1, -1, 0, 0],\n        [-1, 1, 0, 0],\n        [0, 0, 1, -1],\n        [0, 0, -1, 1]\n    ], dtype=float)\n    C_loc5 = np.array([[1]], dtype=float)\n    alpha5 = 0.5\n    \n    test_cases = [\n        (L1, C_loc1, alpha1),\n        (L2, C_loc2, alpha2),\n        (L3, C_loc3, alpha3),\n        (L4, C_loc4, alpha4),\n        (L5, C_loc5, alpha5)\n    ]\n    \n    results = []\n    for L, C_loc, alpha in test_cases:\n        result = check_convergence_conditions(L, C_loc, alpha)\n        results.append(str(result).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2702015"}, {"introduction": "In distributed control design, we often face a critical trade-off between global performance and the practical constraints of a distributed architecture. This hands-on problem introduces the Localized Linear Quadratic Regulator (LLQR), a structured controller that respects the network's communication graph by restricting feedback to local neighborhoods. You will synthesize this controller using a locality-preserving method and quantitatively compare its performance against the optimal centralized LQR, providing insight into the performance cost of decentralization [@problem_id:2702021].", "problem": "Consider a discrete-time linear time-invariant networked system composed of a one-dimensional chain of $n$ nodes with nearest-neighbor coupling. The state vector is $x_{t} \\in \\mathbb{R}^{n}$, the control input is $u_{t} \\in \\mathbb{R}^{n}$, and the dynamics are given by $x_{t+1} = A x_{t} + B u_{t}$, where $A \\in \\mathbb{R}^{n \\times n}$ is tridiagonal with $a$ on the diagonal and $c$ on the first sub- and super-diagonals, and $B = I_{n}$. The performance is measured by the infinite-horizon quadratic cost $J = \\sum_{t=0}^{\\infty} \\left( x_{t}^{\\top} Q x_{t} + u_{t}^{\\top} R u_{t} \\right)$ with $Q = q I_{n}$ and $R = r I_{n}$, where $q  0$ and $r  0$.\n\nDefine the centralized Linear Quadratic Regulator (LQR) as the optimal static state-feedback $u_{t} = -K_{\\mathrm{cen}} x_{t}$ that minimizes $J$ over all static gains. Define the Localized Linear Quadratic Regulator (LLQR) with locality radius $1$ as any static feedback $u_{t} = -K_{\\mathrm{loc}} x_{t}$ where the gain matrix $K_{\\mathrm{loc}} \\in \\mathbb{R}^{n \\times n}$ is banded with half-bandwidth $1$, that is, $\\left(K_{\\mathrm{loc}}\\right)_{ij} = 0$ whenever $\\lvert i - j \\rvert  1$.\n\nYou must compute $K_{\\mathrm{cen}}$ and construct $K_{\\mathrm{loc}}$ using the following locality-preserving synthesis rule:\n- For each node index $i \\in \\{1,2,\\dots,n\\}$, form the local index set $S_{i} = \\{j \\in \\{1,2,\\dots,n\\} : \\lvert j - i \\rvert \\le 1\\}$.\n- Extract the local subsystem matrices $A_{S_{i}} \\in \\mathbb{R}^{\\lvert S_{i} \\rvert \\times \\lvert S_{i} \\rvert}$ and $B_{S_{i}} = I_{\\lvert S_{i} \\rvert}$ by restricting $A$ and $B$ to rows and columns indexed by $S_{i}$.\n- On this local subsystem, synthesize the centralized LQR gain for the local cost with $Q_{S_{i}} = q I_{\\lvert S_{i} \\rvert}$ and $R_{S_{i}} = r I_{\\lvert S_{i} \\rvert}$, obtaining a local gain matrix $K_{S_{i}} \\in \\mathbb{R}^{\\lvert S_{i} \\rvert \\times \\lvert S_{i} \\rvert}$.\n- Let $p$ be the position of the index $i$ within the ordered set $S_{i}$, and set the $i$-th row of $K_{\\mathrm{loc}}$ by placing the $p$-th row of $K_{S_{i}}$ into columns indexed by $S_{i}$, with all other entries in that row equal to $0$.\n\nTo compare the two controllers quantitatively, use the standard infinite-horizon cost matrices. For the centralized LQR, let $P_{\\mathrm{cen}} \\in \\mathbb{R}^{n \\times n}$ denote the optimal cost matrix associated with the minimal cost $J = x_{0}^{\\top} P_{\\mathrm{cen}} x_{0}$ for initial state $x_{0}$. For the localized controller $K_{\\mathrm{loc}}$, define the closed-loop matrix $A_{\\mathrm{cl,loc}} = A - B K_{\\mathrm{loc}}$ and the corresponding cost matrix $P_{\\mathrm{loc}}$ as the unique solution to the discrete-time Lyapunov equation $P_{\\mathrm{loc}} = A_{\\mathrm{cl,loc}}^{\\top} P_{\\mathrm{loc}} A_{\\mathrm{cl,loc}} + \\left(Q + K_{\\mathrm{loc}}^{\\top} R K_{\\mathrm{loc}}\\right)$, provided that $A_{\\mathrm{cl,loc}}$ is Schur stable (all eigenvalues strictly inside the unit disk). If $A_{\\mathrm{cl,loc}}$ is not Schur stable, treat the cost as unbounded.\n\nYour program must, for each test case, compute the scalar ratio $\\rho = \\operatorname{trace}(P_{\\mathrm{loc}}) / \\operatorname{trace}(P_{\\mathrm{cen}})$, which equals the ratio of the expected infinite-horizon costs when the initial state $x_{0}$ is distributed with zero mean and identity covariance. If $A_{\\mathrm{cl,loc}}$ is not Schur stable, output $+\\infty$ for that test case.\n\nUse the following test suite of parameter sets, where each case specifies $(n, a, c, q, r)$:\n- Case $1$: $n = 5$, $a = 0.6$, $c = 0.15$, $q = 1.0$, $r = 0.1$.\n- Case $2$: $n = 8$, $a = 0.7$, $c = 0.10$, $q = 1.0$, $r = 0.5$.\n- Case $3$: $n = 3$, $a = 0.95$, $c = 0.02$, $q = 1.0$, $r = 0.01$.\n\nImplementation requirements:\n- Construct $A$ as the tridiagonal matrix with entries $A_{ii} = a$ for all $i$, $A_{i,i+1} = c$ for all $i \\in \\{1,2,\\dots,n-1\\}$, and $A_{i+1,i} = c$ for all $i \\in \\{1,2,\\dots,n-1\\}$, and set $B = I_{n}$.\n- Compute $K_{\\mathrm{cen}}$ using the standard centralized LQR approach for discrete-time systems, and obtain $P_{\\mathrm{cen}}$ consistently from the same solution.\n- Build $K_{\\mathrm{loc}}$ using the locality-preserving synthesis rule given above, then evaluate $P_{\\mathrm{loc}}$ via the discrete-time Lyapunov equation if $A_{\\mathrm{cl,loc}}$ is Schur stable.\n- For each case, output the ratio $\\rho$ as a floating-point number.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above. For example, the output should look like $[\\rho_{1},\\rho_{2},\\rho_{3}]$.\n- No physical units are involved.\n- Angles do not arise.\n- Express each $\\rho$ as a floating-point decimal number.", "solution": "The problem presented is a well-posed and standard exercise in modern control theory, specifically concerning the trade-off between optimality and decentralization in controller design for networked systems. We shall proceed with a rigorous analysis. The objective is to compute a performance ratio $\\rho$ that quantifies the suboptimality of a structured, localized controller relative to the globally optimal centralized controller.\n\nThe system dynamics are given by the discrete-time linear time-invariant equation:\n$$x_{t+1} = A x_{t} + B u_{t}$$\nwhere $x_{t} \\in \\mathbb{R}^{n}$ is the state and $u_{t} \\in \\mathbb{R}^{n}$ is the control input. The system matrices are $A \\in \\mathbb{R}^{n \\times n}$, a tridiagonal matrix with $a$ on the main diagonal and $c$ on the first off-diagonals, and $B = I_{n}$, the $n \\times n$ identity matrix. The performance objective is to minimize the infinite-horizon quadratic cost:\n$$J = \\sum_{t=0}^{\\infty} \\left( x_{t}^{\\top} Q x_{t} + u_{t}^{\\top} R u_{t} \\right)$$\nwith weighting matrices $Q = q I_{n}$ for $q  0$ and $R = r I_{n}$ for $r  0$.\n\nFirst, we analyze the centralized Linear Quadratic Regulator (LQR). The optimal control law is a static state-feedback $u_{t} = -K_{\\mathrm{cen}} x_{t}$. The optimal gain $K_{\\mathrm{cen}}$ and the associated cost-to-go matrix $P_{\\mathrm{cen}}$ are found by solving the Discrete Algebraic Riccati Equation (DARE). For a general system $(A, B)$ and cost $(Q, R)$, the DARE is:\n$$P = A^{\\top} P A - (A^{\\top} P B)(R + B^{\\top} P B)^{-1}(B^{\\top} P A) + Q$$\nThe solution $P$ to this equation is the optimal cost matrix, thus $P_{\\mathrm{cen}} = P$. This means the minimal cost from an initial state $x_{0}$ is $J^{*} = x_{0}^{\\top} P_{\\mathrm{cen}} x_{0}$. The pair $(A, B)$ is controllable since $B=I_{n}$, and the pair $(A, Q^{1/2})$ is observable since $Q=qI_{n}$ with $q0$. These conditions guarantee the existence of a unique positive definite solution $P_{\\mathrm{cen}}$ that yields a stabilizing feedback gain.\n\nThe optimal centralized feedback gain is then given by:\n$$K_{\\mathrm{cen}} = (R + B^{\\top} P_{\\mathrm{cen}} B)^{-1}(B^{\\top} P_{\\mathrm{cen}} A)$$\nSubstituting $B = I_{n}$ and $R = r I_{n}$, this simplifies to:\n$$K_{\\mathrm{cen}} = (r I_{n} + P_{\\mathrm{cen}})^{-1}(P_{\\mathrm{cen}} A)$$\nWe will solve the DARE numerically to find $P_{\\mathrm{cen}}$ and then compute $K_{\\mathrm{cen}}$.\n\nNext, we construct the localized controller, $K_{\\mathrm{loc}}$, following the prescribed synthesis rule. This is a bottom-up design procedure that builds a global controller from locally optimal solutions. For each node $i \\in \\{1, 2, \\dots, n\\}$, we define a local neighborhood $S_{i} = \\{j \\in \\{1, \\dots, n\\} : |j - i| \\le 1\\}$. The size of this set, $|S_{i}|$, is $2$ for the boundary nodes ($i=1$ and $i=n$) and $3$ for the interior nodes ($1  i  n$, assuming $n  2$).\n\nFor each $i$, we extract the local subsystem matrices. $A_{S_{i}}$ is the submatrix of $A$ formed by selecting rows and columns with indices in $S_{i}$. The local input and cost matrices are $B_{S_{i}} = I_{|S_{i}|}$, $Q_{S_{i}} = q I_{|S_{i}|}$, and $R_{S_{i}} = r I_{|S_{i}|}$. We then solve the DARE for this smaller, local LQR problem:\n$$P_{S_{i}} = A_{S_{i}}^{\\top} P_{S_{i}} A_{S_{i}} - (A_{S_{i}}^{\\top} P_{S_{i}} B_{S_{i}})(R_{S_{i}} + B_{S_{i}}^{\\top} P_{S_{i}} B_{S_{i}})^{-1}(B_{S_{i}}^{\\top} P_{S_{i}} A_{S_{i}}) + Q_{S_{i}}$$\nThe corresponding local optimal gain is $K_{S_{i}} = (R_{S_{i}} + B_{S_{i}}^{\\top} P_{S_{i}} B_{S_{i}})^{-1}(B_{S_{i}}^{\\top} P_{S_{i}} A_{S_{i}})$.\n\nThe global localized gain matrix $K_{\\mathrm{loc}}$ is assembled row by row. For each node $i$, let $p$ be the position of index $i$ within the ordered set $S_{i}$. The $i$-th row of $K_{\\mathrm{loc}}$ is constructed by taking the $p$-th row of the local gain matrix $K_{S_{i}}$ and placing its elements into the columns of the $i$-th row of $K_{\\mathrm{loc}}$ that correspond to the indices in $S_{i}$. All other entries in the $i$-th row of $K_{\\mathrm{loc}}$ are zero. This construction ensures that $(K_{\\mathrm{loc}})_{ij} = 0$ if $|i-j|  1$, enforcing the desired locality structure.\n\nWith the localized controller $u_{t} = -K_{\\mathrm{loc}} x_{t}$, the closed-loop system dynamics become:\n$$x_{t+1} = (A - B K_{\\mathrm{loc}}) x_{t} = (A - K_{\\mathrm{loc}}) x_{t}$$\nLet $A_{\\mathrm{cl,loc}} = A - K_{\\mathrm{loc}}$. The performance of this controller is determined by the cost matrix $P_{\\mathrm{loc}}$, which is the solution to the discrete-time Lyapunov equation, provided that $A_{\\mathrm{cl,loc}}$ is Schur stable (all its eigenvalues lie strictly inside the unit disk in the complex plane). The Lyapunov equation is:\n$$P_{\\mathrm{loc}} = A_{\\mathrm{cl,loc}}^{\\top} P_{\\mathrm{loc}} A_{\\mathrm{cl,loc}} + Q + K_{\\mathrm{loc}}^{\\top} R K_{\\mathrm{loc}}$$\nIf $A_{\\mathrm{cl,loc}}$ is not Schur stable, the sum defining the cost $J$ diverges, and the cost is considered infinite. If it is stable, a unique positive definite solution $P_{\\mathrm{loc}}$ exists. We will solve this linear matrix equation numerically. A standard method is to vectorize the equation: let $p_{\\mathrm{loc}} = \\operatorname{vec}(P_{\\mathrm{loc}})$ and $M = Q + K_{\\mathrm{loc}}^{\\top} R K_{\\mathrm{loc}}$. The equation becomes $(I_{n^2} - A_{\\mathrm{cl,loc}}^{\\top} \\otimes A_{\\mathrm{cl,loc}}^{\\top}) p_{\\mathrm{loc}} = \\operatorname{vec}(M)$, which is a standard linear system.\n\nFinally, we compute the desired ratio $\\rho$. This ratio compares the expected cost under the two control schemes, where the expectation is taken over initial states $x_{0}$ drawn from a distribution with zero mean and identity covariance, $\\mathbb{E}[x_{0}x_{0}^{\\top}] = I_{n}$. The expected cost is $\\mathbb{E}[x_{0}^{\\top} P x_{0}] = \\operatorname{trace}(P \\mathbb{E}[x_{0}x_{0}^{\\top}]) = \\operatorname{trace}(P)$. Therefore, the performance ratio is:\n$$\\rho = \\frac{\\operatorname{trace}(P_{\\mathrm{loc}})}{\\operatorname{trace}(P_{\\mathrm{cen}})}$$\nIf $P_{\\mathrm{loc}}$ is unbounded, we set $\\rho = +\\infty$. We will now proceed with the computation for each specified test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_discrete_are, solve_lyapunov, LinAlgError\n\ndef solve():\n    \"\"\"\n    Solves the control theory problem for centralized and localized LQR controllers.\n    \"\"\"\n    test_cases = [\n        # (n, a, c, q, r)\n        (5, 0.6, 0.15, 1.0, 0.1),\n        (8, 0.7, 0.10, 1.0, 0.5),\n        (3, 0.95, 0.02, 1.0, 0.01),\n    ]\n\n    results = []\n    for n, a, c, q, r in test_cases:\n        # Construct system matrices A, B, Q, R\n        A = np.diag(np.full(n, a)) + np.diag(np.full(n - 1, c), k=1) + np.diag(np.full(n - 1, c), k=-1)\n        B = np.eye(n)\n        Q = q * np.eye(n)\n        R = r * np.eye(n)\n\n        # 1. Centralized LQR\n        # Solve Discrete Algebraic Riccati Equation (DARE) for P_cen\n        # X = A'XA - (A'XB)(R+B'XB)^-1(B'XA) + Q\n        try:\n            P_cen = solve_discrete_are(A, B, Q, R)\n        except LinAlgError:\n            # Should not happen for these parameters as system is controllable and observable\n            results.append(np.inf)\n            continue\n        \n        # 2. Localized LQR (LLQR) Synthesis\n        K_loc = np.zeros((n, n))\n        for i in range(n):\n            # Define local index set Si (using 0-based indexing)\n            s_i_indices = []\n            if i  0:\n                s_i_indices.append(i - 1)\n            s_i_indices.append(i)\n            if i  n - 1:\n                s_i_indices.append(i + 1)\n            \n            s_i_size = len(s_i_indices)\n            \n            # Extract local subsystem matrices\n            A_si = A[np.ix_(s_i_indices, s_i_indices)]\n            B_si = np.eye(s_i_size)\n            Q_si = q * np.eye(s_i_size)\n            R_si = r * np.eye(s_i_size)\n            \n            # Solve local DARE\n            P_si = solve_discrete_are(A_si, B_si, Q_si, R_si)\n\n            # Compute local gain K_si\n            # K = (R + B'PB)^-1 B'PA\n            K_si = np.linalg.inv(R_si + B_si.T @ P_si @ B_si) @ (B_si.T @ P_si @ A_si)\n            \n            # Find the position 'p' of index 'i' in S_i\n            p = s_i_indices.index(i)\n            \n            # Place the p-th row of K_si into the i-th row of K_loc\n            K_loc[i, s_i_indices] = K_si[p, :]\n\n        # 3. Evaluate LLQR performance\n        A_cl_loc = A - B @ K_loc\n        \n        # Check Schur stability\n        eigvals = np.linalg.eigvals(A_cl_loc)\n        if np.any(np.abs(eigvals) = 1.0):\n            results.append(np.inf)\n            continue\n\n        # Solve discrete-time Lyapunov equation for P_loc\n        # P = A_cl' * P * A_cl + M, where M = Q + K_loc' * R * K_loc\n        M = Q + K_loc.T @ R @ K_loc\n        \n        # Solving P = A'PA + Q using Kronecker product:\n        # (I - A' kron A')vec(P) = vec(Q)\n        # In our case A is A_cl_loc and Q is M\n        kron_mat = np.kron(A_cl_loc.T, A_cl_loc.T)\n        I_n2 = np.eye(n * n)\n        \n        try:\n            vec_Ploc = np.linalg.solve(I_n2 - kron_mat, M.flatten())\n            P_loc = vec_Ploc.reshape((n, n))\n        except LinAlgError:\n            # Can happen if (I - kron) is singular, related to eigenvalues on unit circle\n            results.append(np.inf)\n            continue\n            \n        # 4. Compute the ratio rho\n        trace_P_cen = np.trace(P_cen)\n        trace_P_loc = np.trace(P_loc)\n        \n        if trace_P_cen == 0: # Avoid division by zero\n            rho = np.inf if trace_P_loc  0 else 1.0\n        else:\n            rho = trace_P_loc / trace_P_cen\n        \n        results.append(rho)\n\n    # Format output as requested\n    formatted_results = [f\"{r:.7f}\" if r != np.inf else \"inf\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2702021"}]}