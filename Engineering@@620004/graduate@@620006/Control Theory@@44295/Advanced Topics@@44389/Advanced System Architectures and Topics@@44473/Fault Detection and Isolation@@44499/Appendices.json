{"hands_on_practices": [{"introduction": "A core task in fault detection is designing a residual generator that is sensitive to faults but robust against known inputs and unknown initial conditions. The parity space method provides a direct, data-driven way to achieve this using a finite history of input-output measurements. This practice offers a concrete exercise in transforming a system's state-space representation into a functional residual generator, grounding the theory in tangible matrix algebra [@problem_id:2706941].", "problem": "Consider the discrete-time single-input single-output (SISO) linear time-invariant system with an additive sensor fault given by\n$$\nx_{k+1}=A x_k + B u_k, \\quad y_k = C x_k + D u_k + f_k,\n$$\nwhere\n$$\nA=\\begin{pmatrix}0.6 & 1.0\\\\ 0 & 0.3\\end{pmatrix},\\quad\nB=\\begin{pmatrix}1\\\\ 0\\end{pmatrix},\\quad\nC=\\begin{pmatrix}1 & 0\\end{pmatrix},\\quad\nD=0.2,\n$$\nand $f_k$ is an unknown sensor fault. The pair $\\left(A,C\\right)$ is observable. Define the stacked horizon-$N$ vectors\n$$\nY=\\begin{pmatrix}y_0\\\\ y_1\\\\ y_2\\\\ y_3\\end{pmatrix},\\quad\nU=\\begin{pmatrix}u_0\\\\ u_1\\\\ u_2\\\\ u_3\\end{pmatrix},\\quad\nf=\\begin{pmatrix}f_0\\\\ f_1\\\\ f_2\\\\ f_3\\end{pmatrix},\n$$\nwith $N=4$. Starting from first principles (state recursion and output equations), explicitly assemble the stacked data equation\n$$\nY=\\mathcal{T}_U\\,U+\\mathcal{T}_X\\,x_0+\\mathcal{T}_F\\,F\\,f,\n$$\nby constructing the $4\\times 4$ lower block Toeplitz input matrix $\\mathcal{T}_U$, the $4\\times 2$ extended observability matrix $\\mathcal{T}_X$, and the $4\\times 4$ fault distribution matrix $\\mathcal{T}_F$ together with the fault signature $F$. Then, by exploiting only linear-algebraic properties of these matrices, derive an explicit nontrivial parity matrix that eliminates both the unknown initial condition $x_0$ and the measured input $U$. Specifically, construct matrices $Q_y\\in\\mathbb{R}^{2\\times 4}$ and $Q_u\\in\\mathbb{R}^{2\\times 4}$ such that\n$$\nQ_y\\,\\mathcal{T}_X=0,\\qquad Q_y\\,\\mathcal{T}_U+Q_u=0,\n$$\nand hence the residual\n$$\nr = Q_y\\,Y + Q_u\\,U\n$$\ndepends only on the fault. Provide one explicit numerical choice of the block parity matrix $\\Pi=\\big[\\,Q_y\\;\\;Q_u\\,\\big]\\in\\mathbb{R}^{2\\times 8}$. Your final answer must be this $\\Pi$ as a single analytic expression. No rounding is required and no units are involved.", "solution": "The starting point is the state recursion and the output map,\n$$\nx_{k+1}=A x_k + B u_k,\\qquad y_k = C x_k + D u_k + f_k,\n$$\ntogether with the definition of the stacked vectors for a horizon $N=4$,\n$$\nY=\\begin{pmatrix}y_0\\\\ y_1\\\\ y_2\\\\ y_3\\end{pmatrix},\\quad\nU=\\begin{pmatrix}u_0\\\\ u_1\\\\ u_2\\\\ u_3\\end{pmatrix},\\quad\nf=\\begin{pmatrix}f_0\\\\ f_1\\\\ f_2\\\\ f_3\\end{pmatrix}.\n$$\nFrom the recursion, one obtains the well-known expressions for the outputs over the horizon by repeatedly substituting the state update into the output equation:\n$$\n\\begin{aligned}\ny_0 &= C x_0 + D u_0 + f_0,\\\\\ny_1 &= C A x_0 + C B u_0 + D u_1 + f_1,\\\\\ny_2 &= C A^2 x_0 + C A B u_0 + C B u_1 + D u_2 + f_2,\\\\\ny_3 &= C A^3 x_0 + C A^2 B u_0 + C A B u_1 + C B u_2 + D u_3 + f_3.\n\\end{aligned}\n$$\nStacking these four equations yields\n$$\nY=\\underbrace{\\begin{pmatrix}\nD & 0 & 0 & 0\\\\\nC B & D & 0 & 0\\\\\nC A B & C B & D & 0\\\\\nC A^2 B & C A B & C B & D\n\\end{pmatrix}}_{\\mathcal{T}_U}\\,U+\n\\underbrace{\\begin{pmatrix}\nC\\\\\nC A\\\\\nC A^2\\\\\nC A^3\n\\end{pmatrix}}_{\\mathcal{T}_X}\\,x_0+\n\\underbrace{I_4}_{\\mathcal{T}_F}\\,\\underbrace{1}_{F}\\,f.\n$$\nThus $\\mathcal{T}_F=I_4$ and $F=1$ for an additive sensor fault.\n\nNext, compute the needed Markov parameters and powers of $A$ to explicitly construct $\\mathcal{T}_U$ and $\\mathcal{T}_X$. With\n$$\nA=\\begin{pmatrix}0.6 & 1.0\\\\ 0 & 0.3\\end{pmatrix},\\quad\nB=\\begin{pmatrix}1\\\\ 0\\end{pmatrix},\\quad\nC=\\begin{pmatrix}1 & 0\\end{pmatrix},\\quad\nD=0.2,\n$$\none finds\n$$\nA^2=\\begin{pmatrix}0.36 & 0.9\\\\ 0 & 0.09\\end{pmatrix},\\qquad\nA^3=\\begin{pmatrix}0.216 & 0.63\\\\ 0 & 0.027\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\begin{aligned}\nC &= \\begin{pmatrix}1 & 0\\end{pmatrix},\\\\\nC A &= \\begin{pmatrix}0.6 & 1\\end{pmatrix},\\\\\nC A^2 &= \\begin{pmatrix}0.36 & 0.9\\end{pmatrix},\\\\\nC A^3 &= \\begin{pmatrix}0.216 & 0.63\\end{pmatrix},\n\\end{aligned}\n$$\nso that\n$$\n\\mathcal{T}_X=\\begin{pmatrix}\n1 & 0\\\\\n0.6 & 1\\\\\n0.36 & 0.9\\\\\n0.216 & 0.63\n\\end{pmatrix}.\n$$\nFor the input Toeplitz matrix, the Markov parameters are\n$$\ng_0 = D = 0.2,\\quad g_1 = C B = 1,\\quad g_2 = C A B = 0.6,\\quad g_3 = C A^2 B = 0.36,\n$$\nyielding\n$$\n\\mathcal{T}_U=\\begin{pmatrix}\n0.2 & 0   & 0   & 0\\\\\n1   & 0.2 & 0   & 0\\\\\n0.6 & 1   & 0.2 & 0\\\\\n0.36& 0.6 & 1   & 0.2\n\\end{pmatrix}.\n$$\n\nTo eliminate the dependence on the unknown initial condition $x_0$ and on the measured inputs $U$, we construct a parity residual of the form\n$$\nr = Q_y\\,Y + Q_u\\,U,\n$$\nwith $Q_y\\in\\mathbb{R}^{2\\times 4}$ and $Q_u\\in\\mathbb{R}^{2\\times 4}$ chosen to satisfy\n$$\nQ_y\\,\\mathcal{T}_X=0,\\qquad Q_y\\,\\mathcal{T}_U+Q_u=0.\n$$\nThe first condition requires that the rows of $Q_y$ lie in the left nullspace of $\\mathcal{T}_X$. Solve $Q_y\\,\\mathcal{T}_X=0$ by finding $q\\in\\mathbb{R}^4$ such that $\\mathcal{T}_X^{\\top} q=0$. Writing $q=\\begin{pmatrix}q_0 & q_1 & q_2 & q_3\\end{pmatrix}^{\\top}$, the equations\n$$\n\\begin{aligned}\nq_0\\cdot 1 + q_1\\cdot 0.6 + q_2\\cdot 0.36 + q_3\\cdot 0.216 &= 0,\\\\\nq_0\\cdot 0 + q_1\\cdot 1 + q_2\\cdot 0.9 + q_3\\cdot 0.63 &= 0,\n\\end{aligned}\n$$\nyield a two-parameter family\n$$\n\\begin{pmatrix}q_0\\\\ q_1\\\\ q_2\\\\ q_3\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.18\\,c + 0.162\\,d\\\\\n-0.9\\,c - 0.63\\,d\\\\\nc\\\\\nd\n\\end{pmatrix},\n\\quad c,d\\in\\mathbb{R}.\n$$\nChoosing the basis vectors corresponding to $(c,d)=(1,0)$ and $(c,d)=(0,1)$, we obtain two independent left-null vectors\n$$\nq^{(1)}=\\begin{pmatrix}0.18\\\\ -0.9\\\\ 1\\\\ 0\\end{pmatrix},\\qquad\nq^{(2)}=\\begin{pmatrix}0.162\\\\ -0.63\\\\ 0\\\\ 1\\end{pmatrix}.\n$$\nStacking these as rows gives\n$$\nQ_y=\\begin{pmatrix}\n0.18 & -0.9 & 1 & 0\\\\\n0.162 & -0.63 & 0 & 1\n\\end{pmatrix},\n$$\nwhich by construction satisfies $Q_y\\,\\mathcal{T}_X=0$.\n\nThe second condition $Q_y\\,\\mathcal{T}_U+Q_u=0$ determines $Q_u$ uniquely for this $Q_y$:\n$$\nQ_u = -\\,Q_y\\,\\mathcal{T}_U.\n$$\nCompute $Q_y\\,\\mathcal{T}_U$ explicitly. For the first row of $Q_y$,\n$$\n\\begin{aligned}\n\\begin{pmatrix}0.18 & -0.9 & 1 & 0\\end{pmatrix}\\mathcal{T}_U\n&= \\begin{pmatrix}\n0.18\\cdot 0.2 + (-0.9)\\cdot 1 + 1\\cdot 0.6 + 0\\cdot 0.36 &\n0.18\\cdot 0 + (-0.9)\\cdot 0.2 + 1\\cdot 1 + 0\\cdot 0.6\\\\\n0.18\\cdot 0 + (-0.9)\\cdot 0 + 1\\cdot 0.2 + 0\\cdot 1 &\n0.18\\cdot 0 + (-0.9)\\cdot 0 + 1\\cdot 0 + 0\\cdot 0.2\n\\end{pmatrix}\\\\\n&= \\begin{pmatrix}-0.264 & 0.82 & 0.2 & 0\\end{pmatrix}.\n\\end{aligned}\n$$\nFor the second row of $Q_y$,\n$$\n\\begin{aligned}\n\\begin{pmatrix}0.162 & -0.63 & 0 & 1\\end{pmatrix}\\mathcal{T}_U\n&= \\begin{pmatrix}\n0.162\\cdot 0.2 + (-0.63)\\cdot 1 + 0\\cdot 0.6 + 1\\cdot 0.36 &\n0.162\\cdot 0 + (-0.63)\\cdot 0.2 + 0\\cdot 1 + 1\\cdot 0.6\\\\\n0.162\\cdot 0 + (-0.63)\\cdot 0 + 0\\cdot 0.2 + 1\\cdot 1 &\n0.162\\cdot 0 + (-0.63)\\cdot 0 + 0\\cdot 0 + 1\\cdot 0.2\n\\end{pmatrix}\\\\\n&= \\begin{pmatrix}-0.2376 & 0.474 & 1 & 0.2\\end{pmatrix}.\n\\end{aligned}\n$$\nTherefore,\n$$\nQ_u=-Q_y\\,\\mathcal{T}_U=\n\\begin{pmatrix}\n0.264 & -0.82 & -0.2 & 0\\\\\n0.2376 & -0.474 & -1 & -0.2\n\\end{pmatrix}.\n$$\nWith these choices, the residual\n$$\nr = Q_y\\,Y + Q_u\\,U\n= Q_y\\left(\\mathcal{T}_U\\,U+\\mathcal{T}_X\\,x_0+\\mathcal{T}_F\\,F\\,f\\right)+Q_u\\,U\n= \\underbrace{Q_y\\,\\mathcal{T}_X}_{0}\\,x_0+\\underbrace{\\big(Q_y\\,\\mathcal{T}_U+Q_u\\big)}_{0}\\,U+Q_y\\,\\mathcal{T}_F\\,F\\,f,\n$$\nreduces to\n$$\nr = Q_y\\,f,\n$$\nsince $\\mathcal{T}_F=I_4$ and $F=1$. Thus the block parity matrix acting on the extended data vector $\\begin{pmatrix}Y^{\\top} & U^{\\top}\\end{pmatrix}^{\\top}$ is\n$$\n\\Pi=\\big[\\,Q_y\\;\\;Q_u\\,\\big]\n=\n\\begin{pmatrix}\n0.18 & -0.9 & 1 & 0 & \\;\\;0.264 & -0.82 & -0.2 & 0\\\\\n0.162 & -0.63 & 0 & 1 & \\;\\;0.2376 & -0.474 & -1 & -0.2\n\\end{pmatrix},\n$$\nwhich eliminates both the unknown initial state $x_0$ and the measured inputs $U$ from the residual.", "answer": "$$\\boxed{\\begin{pmatrix}\n0.18 & -0.9 & 1 & 0 & 0.264 & -0.82 & -0.2 & 0\\\\\n0.162 & -0.63 & 0 & 1 & 0.2376 & -0.474 & -1 & -0.2\n\\end{pmatrix}}$$", "id": "2706941"}, {"introduction": "Once a residual signal is generated, a systematic method is needed to decide whether a fault has occurred. This practice introduces the principles of statistical hypothesis testing, specifically the Neyman-Pearson framework, which formalizes the trade-off between false alarms and missed detections. You will practice the essential skill of setting a detection threshold $\\gamma$ to meet a specific false alarm rate and then determining the detector's sensitivity by calculating the minimal detectable fault magnitude [@problem_id:2706769].", "problem": "A residual generator for Fault Detection and Isolation (FDI) in a discrete-time Linear Time-Invariant system produces a scalar residual sequence $ r_k $. Under the no-fault hypothesis $ \\mathcal{H}_0 $, assume $ r_k $ are independent and identically distributed as Gaussian with zero mean and unknown variance $ \\sigma^2 $. A calibration experiment yields an unbiased variance estimate $ \\hat{\\sigma}^2 = 0.09 $, which you may treat as known for detector design due to a large calibration dataset.\n\nYou are to design a one-sided instantaneous detector aimed at a positive-mean bias fault in the residual, modeled under the fault hypothesis $ \\mathcal{H}_1 $ by $ r_k \\sim \\mathcal{N}(f, \\sigma^2) $ with $ f > 0 $ and the same variance as under $ \\mathcal{H}_0 $. The decision rule is of the Neyman–Pearson type: declare a fault at time $ k $ if $ r_k > \\gamma $, where $ \\gamma $ is a threshold to be determined.\n\nThe design requirements are:\n- The per-sample false alarm probability must equal $ \\alpha = 1.0 \\times 10^{-3} $ under $ \\mathcal{H}_0 $.\n- The per-sample detection probability must equal $ \\beta = 0.95 $ under $ \\mathcal{H}_1 $ when the mean shift is at its minimal detectable magnitude $ f_{\\min} $.\n\nStarting from first principles for Gaussian hypothesis testing and without using any pre-stated target formulas, derive:\n1. The threshold $ \\gamma $ that satisfies the false alarm requirement in terms of $ \\hat{\\sigma} $ and $ \\alpha $, and then compute its numerical value.\n2. The minimal detectable positive mean shift $ f_{\\min} $ that achieves the target detection probability $ \\beta $ for the threshold $ \\gamma $, and then compute its numerical value.\n\nExpress both $ \\gamma $ and $ f_{\\min} $ in the same unitless normalized residual units as $ r_k $. Report your final answer as a row vector $ [\\gamma, f_{\\min}] $. Round each entry to four significant figures. All probabilities must be treated as decimals, not percentages.", "solution": "The problem presented is a standard exercise in hypothesis testing, a core component of statistical signal processing for Fault Detection and Isolation (FDI). It is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We shall proceed with the derivation from first principles.\n\nThe problem defines two hypotheses for the observed residual $r_k$ at any time step $k$:\nThe null hypothesis, $\\mathcal{H}_0$, corresponds to the no-fault condition:\n$$r_k \\sim \\mathcal{N}(0, \\sigma^2)$$\nThe alternative hypothesis, $\\mathcal{H}_1$, corresponds to the presence of a positive-mean bias fault:\n$$r_k \\sim \\mathcal{N}(f, \\sigma^2), \\quad \\text{with } f > 0$$\nThe variance $\\sigma^2$ is given as the known value $\\hat{\\sigma}^2 = 0.09$. Therefore, the standard deviation is $\\sigma = \\sqrt{0.09} = 0.3$.\nThe decision rule is to declare a fault if the observed residual $r_k$ exceeds a threshold $\\gamma$, i.e., $r_k > \\gamma$.\n\nFirst, we determine the threshold $\\gamma$.\nThe false alarm probability, $\\alpha$, is the probability of declaring a fault when none exists (i.e., when $\\mathcal{H}_0$ is true). The design specification requires $\\alpha = 1.0 \\times 10^{-3}$.\nBy definition,\n$$\\alpha = P(\\text{declare fault} | \\mathcal{H}_0) = P(r_k > \\gamma | r_k \\sim \\mathcal{N}(0, \\sigma^2))$$\nTo evaluate this probability, we standardize the random variable $r_k$. Let $Z = \\frac{r_k - \\mu}{\\sigma}$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. Under $\\mathcal{H}_0$, the mean $\\mu$ is $0$.\nThe inequality $r_k > \\gamma$ can be rewritten in terms of the standard normal variable $Z$:\n$$\\frac{r_k}{\\sigma} > \\frac{\\gamma}{\\sigma} \\implies Z > \\frac{\\gamma}{\\sigma}$$\nTherefore, the false alarm probability is $\\alpha = P(Z > \\frac{\\gamma}{\\sigma})$.\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = P(Z \\le z)$. Then, $P(Z > z) = 1 - \\Phi(z)$.\nSo, we have the relation:\n$$\\alpha = 1 - \\Phi\\left(\\frac{\\gamma}{\\sigma}\\right)$$\nRearranging for the CDF gives $\\Phi(\\frac{\\gamma}{\\sigma}) = 1 - \\alpha$.\nTo find the argument of $\\Phi$, we use the inverse cumulative distribution function, $\\Phi^{-1}$:\n$$\\frac{\\gamma}{\\sigma} = \\Phi^{-1}(1 - \\alpha)$$\nThis gives the analytical expression for the threshold $\\gamma$:\n$$\\gamma = \\sigma \\Phi^{-1}(1 - \\alpha)$$\nWe are given $\\sigma = 0.3$ and $\\alpha = 0.001$. We substitute these values:\n$$\\gamma = 0.3 \\times \\Phi^{-1}(1 - 0.001) = 0.3 \\times \\Phi^{-1}(0.999)$$\nThe value $\\Phi^{-1}(0.999)$ corresponds to the z-score leaving a tail probability of $0.001$. From standard normal tables or a calculator, $\\Phi^{-1}(0.999) \\approx 3.09023$.\n$$\\gamma \\approx 0.3 \\times 3.09023 \\approx 0.927069$$\nRounding to four significant figures, the threshold is $\\gamma \\approx 0.9271$.\n\nNext, we determine the minimal detectable positive mean shift $f_{\\min}$.\nThe detection probability, $\\beta$, is the probability of correctly declaring a fault when one exists (i.e., when $\\mathcal{H}_1$ is true). For the minimal detectable magnitude $f_{\\min}$, the problem specifies $\\beta = 0.95$.\nBy definition,\n$$\\beta = P(\\text{declare fault} | \\mathcal{H}_1 \\text{ with } f=f_{\\min}) = P(r_k > \\gamma | r_k \\sim \\mathcal{N}(f_{\\min}, \\sigma^2))$$\nAgain, we standardize the random variable $r_k$, this time under $\\mathcal{H}_1$. The mean is now $\\mu=f_{\\min}$.\n$$Z = \\frac{r_k - f_{\\min}}{\\sigma} \\sim \\mathcal{N}(0, 1)$$\nThe inequality $r_k > \\gamma$ becomes:\n$$r_k - f_{\\min} > \\gamma - f_{\\min} \\implies \\frac{r_k - f_{\\min}}{\\sigma} > \\frac{\\gamma - f_{\\min}}{\\sigma} \\implies Z > \\frac{\\gamma - f_{\\min}}{\\sigma}$$\nThe detection probability is thus:\n$$\\beta = P\\left(Z > \\frac{\\gamma - f_{\\min}}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\gamma - f_{\\min}}{\\sigma}\\right)$$\nRearranging for the CDF gives $\\Phi(\\frac{\\gamma - f_{\\min}}{\\sigma}) = 1 - \\beta$.\nApplying the inverse CDF:\n$$\\frac{\\gamma - f_{\\min}}{\\sigma} = \\Phi^{-1}(1 - \\beta)$$\nNow, we solve for $f_{\\min}$:\n$$\\gamma - f_{\\min} = \\sigma \\Phi^{-1}(1 - \\beta)$$\n$$f_{\\min} = \\gamma - \\sigma \\Phi^{-1}(1 - \\beta)$$\nWe can substitute the expression for $\\gamma$ we derived earlier, $\\gamma = \\sigma \\Phi^{-1}(1 - \\alpha)$:\n$$f_{\\min} = \\sigma \\Phi^{-1}(1 - \\alpha) - \\sigma \\Phi^{-1}(1 - \\beta) = \\sigma \\left( \\Phi^{-1}(1 - \\alpha) - \\Phi^{-1}(1 - \\beta) \\right)$$\nNote the property that for the standard normal distribution, $\\Phi^{-1}(p) = -\\Phi^{-1}(1-p)$. This allows us to write $\\Phi^{-1}(1-\\beta) = -\\Phi^{-1}(\\beta)$.\n$$f_{\\min} = \\sigma \\left( \\Phi^{-1}(1 - \\alpha) + \\Phi^{-1}(\\beta) \\right)$$\nWe are given $\\sigma = 0.3$, $\\alpha = 0.001$, and $\\beta = 0.95$.\n$$f_{\\min} = 0.3 \\times \\left( \\Phi^{-1}(0.999) + \\Phi^{-1}(0.95) \\right)$$\nUsing the value $\\Phi^{-1}(0.999) \\approx 3.09023$ from before, and finding $\\Phi^{-1}(0.95) \\approx 1.64485$:\n$$f_{\\min} \\approx 0.3 \\times (3.09023 + 1.64485) = 0.3 \\times 4.73508$$\n$$f_{\\min} \\approx 1.420524$$\nRounding to four significant figures, the minimal detectable fault is $f_{\\min} \\approx 1.421$.\n\nThe required values are $\\gamma \\approx 0.9271$ and $f_{\\min} \\approx 1.421$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9271 & 1.421\n\\end{pmatrix}\n}\n$$", "id": "2706769"}, {"introduction": "Before optimizing a fault detection system, it's crucial to understand its ultimate performance limitations. This exercise explores the Cramér-Rao Lower Bound (CRLB), a fundamental concept from estimation theory that establishes the best possible precision for any unbiased fault estimator. By deriving the CRLB, you will gain a deep appreciation for how factors like measurement noise variance $\\sigma^2$ and the number of data samples $N$ fundamentally constrain our ability to estimate faults accurately [@problem_id:2706749].", "problem": "In a fault detection and isolation (FDI) setting for a linear time-invariant system, a residual generator outputs a scalar residual sequence modeled as an additive constant bias plus noise. Specifically, assume that for sampling instants indexed by $k \\in \\{1,\\dots,N\\}$ the residuals satisfy\n$r_k = b + v_k$,\nwhere the fault bias $b$ is an unknown constant and the noise $v_k$ are independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and known variance $\\sigma^2$. The design objective is to estimate $b$ from the $N$ residual samples.\n\nStarting from the joint probability density function of the $N$ residuals conditioned on $b$, use the definition of Fisher information and the Cramér–Rao lower bound (CRLB) to derive the fundamental lower bound on the variance of any unbiased estimator of $b$. Present your final answer as a closed-form expression in terms of $N$ and $\\sigma^2$. Briefly interpret how the bound scales with $N$ in your reasoning, but express the final answer solely as an analytic expression without units or commentary. No numerical rounding is required.", "solution": "The problem requires the derivation of the Cramér–Rao lower bound (CRLB) for the variance of any unbiased estimator of a constant bias $b$ in an additive noise model. The model for the scalar residual sequence $\\{r_k\\}_{k=1}^N$ is given by\n$$r_k = b + v_k$$\nfor sampling instants $k \\in \\{1, \\dots, N\\}$. The noise terms $v_k$ are specified as independent and identically distributed (i.i.d.) Gaussian random variables with mean $E[v_k] = 0$ and a known variance $Var(v_k) = \\sigma^2$. From this definition, it follows that each residual $r_k$ is also a Gaussian random variable with mean $E[r_k] = b$ and variance $Var(r_k) = \\sigma^2$. The probability density function (PDF) of a single residual $r_k$, conditioned on the unknown parameter $b$, is therefore\n$$P(r_k | b) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r_k-b)^2}{2\\sigma^2}\\right).$$\nGiven that the noise terms $v_k$ are independent, the residuals $r_k$ are also independent. Consequently, the joint PDF of the entire observation vector $R = (r_1, \\dots, r_N)$ is the product of the individual PDFs:\n$$P(R | b) = \\prod_{k=1}^N P(r_k | b) = \\left(2\\pi\\sigma^2\\right)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{k=1}^N (r_k-b)^2\\right).$$\nTo find the Fisher information, we begin by constructing the log-likelihood function, $L(b) = \\ln P(R | b)$:\n$$L(b) = \\ln\\left[\\left(2\\pi\\sigma^2\\right)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{k=1}^N (r_k-b)^2\\right)\\right]$$\n$$L(b) = -\\frac{N}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{k=1}^N (r_k-b)^2.$$\nThe Fisher information, $I(b)$, for a single parameter is defined as $I(b) = -E\\left[\\frac{\\partial^2 L(b)}{\\partial b^2}\\right]$. We must first compute the partial derivatives of the log-likelihood function with respect to the parameter $b$. The first derivative is:\n$$\\frac{\\partial L(b)}{\\partial b} = -\\frac{1}{2\\sigma^2} \\sum_{k=1}^N \\frac{\\partial}{\\partial b}(r_k-b)^2 = -\\frac{1}{2\\sigma^2} \\sum_{k=1}^N 2(r_k-b)(-1) = \\frac{1}{\\sigma^2} \\sum_{k=1}^N (r_k-b).$$\nThe second derivative is then:\n$$\\frac{\\partial^2 L(b)}{\\partial b^2} = \\frac{\\partial}{\\partial b}\\left(\\frac{1}{\\sigma^2} \\sum_{k=1}^N (r_k-b)\\right) = \\frac{1}{\\sigma^2} \\sum_{k=1}^N (-1) = -\\frac{N}{\\sigma^2}.$$\nThis second derivative is a deterministic constant; it does not depend on the random variables $r_k$. Therefore, its expectation is simply the constant itself:\n$$E\\left[\\frac{\\partial^2 L(b)}{\\partial b^2}\\right] = -\\frac{N}{\\sigma^2}.$$\nThe Fisher information is subsequently calculated as:\n$$I(b) = -E\\left[\\frac{\\partial^2 L(b)}{\\partial b^2}\\right] = -\\left(-\\frac{N}{\\sigma^2}\\right) = \\frac{N}{\\sigma^2}.$$\nThe Cramér–Rao lower bound states that for any unbiased estimator $\\hat{b}$ of $b$, its variance is bounded from below by the reciprocal of the Fisher information:\n$$Var(\\hat{b}) \\ge \\frac{1}{I(b)}.$$\nSubstituting the derived expression for $I(b)$, we obtain the fundamental lower bound on the variance of the estimator:\n$$Var(\\hat{b}) \\ge \\frac{\\sigma^2}{N}.$$\nThis result is the required closed-form expression. It represents the fundamental limit on the precision with which the bias $b$ can be estimated from $N$ samples. The bound $\\frac{\\sigma^2}{N}$ shows that the minimum achievable variance of an unbiased estimator is directly proportional to the noise variance $\\sigma^2$ and inversely proportional to the number of samples $N$. This inverse relationship with $N$ is a critical insight, signifying that the quality of the estimate improves as more data is collected. Specifically, the standard deviation of the estimate, being the square root of the variance, scales as $\\frac{1}{\\sqrt{N}}$, which is a foundational principle in statistical estimation.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{N}}\n$$", "id": "2706749"}]}