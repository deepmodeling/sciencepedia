{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a fundamental case: a scalar nonlinear system. This exercise [@problem_id:2738256] is valuable because, unlike more complex systems, the regions of attraction for all equilibria can be determined exactly using first principles. By analyzing the sign of the dynamics, you will build a complete picture of the state space, gaining a clear and intuitive understanding of how unstable equilibria create the boundaries that separate different basins of attraction.", "problem": "Consider the scalar autonomous nonlinear system defined on $\\mathbb{R}$ by the ordinary differential equation $\\dot{x}=x-x^{3}$. Work from first principles of dynamical systems and control to do the following: identify all equilibria, classify their local stability using linearization, and then rigorously justify the global region of attraction for each equilibrium using only foundational tools such as invariance, uniqueness of solutions for Lipschitz vector fields, and Lyapunov arguments derived from first principles. Explicitly describe the basin boundaries and explain why no other boundaries exist. Conclude by determining the region of attraction (ROA) of each equilibrium as subsets of $\\mathbb{R}$. Finally, report as your final answer the Lebesgue measure of the intersection of the region of attraction of the equilibrium $x^{\\star}=1$ with the interval $[-2,2]$. Express the final answer as a single real number. An exact value is expected; no rounding is necessary.", "solution": "The problem requires a complete analysis of the scalar autonomous system given by the ordinary differential equation $\\dot{x} = f(x) = x - x^{3}$ on the domain $\\mathbb{R}$. This analysis will proceed from first principles, beginning with the identification and classification of equilibria, followed by a rigorous determination of their respective regions of attraction, and culminating in the calculation of a specified Lebesgue measure.\n\nFirst, we identify the equilibrium points of the system by solving the equation $\\dot{x} = 0$.\n$$x - x^{3} = 0$$\n$$x(1 - x^{2}) = 0$$\n$$x(1 - x)(1 + x) = 0$$\nThis yields three distinct equilibrium points: $x_{1}^{\\star} = -1$, $x_{2}^{\\star} = 0$, and $x_{3}^{\\star} = 1$.\n\nNext, we investigate the local stability of each equilibrium by linearization. The vector field is $f(x) = x - x^{3}$. The Jacobian of this one-dimensional system is the derivative of $f(x)$ with respect to $x$:\n$$J(x) = \\frac{df}{dx} = 1 - 3x^{2}$$\nAccording to the Hartman-Grobman theorem, the stability of a hyperbolic equilibrium point (where the real part of the eigenvalue is non-zero) is determined by the sign of the eigenvalue of the Jacobian evaluated at that point.\n- For the equilibrium $x_{1}^{\\star} = -1$:\n$$J(-1) = 1 - 3(-1)^{2} = 1 - 3 = -2$$\nSince the eigenvalue $\\lambda = -2 < 0$, the equilibrium at $x^{\\star} = -1$ is locally asymptotically stable (a sink).\n- For the equilibrium $x_{2}^{\\star} = 0$:\n$$J(0) = 1 - 3(0)^{2} = 1$$\nSince the eigenvalue $\\lambda = 1 > 0$, the equilibrium at $x^{\\star} = 0$ is unstable (a source).\n- For the equilibrium $x_{3}^{\\star} = 1$:\n$$J(1) = 1 - 3(1)^{2} = 1 - 3 = -2$$\nSince the eigenvalue $\\lambda = -2 < 0$, the equilibrium at $x^{\\star} = 1$ is locally asymptotically stable (a sink).\n\nWe now proceed to a global analysis to determine the regions of attraction. The vector field $f(x) = x - x^{3}$ is a polynomial and thus is continuously differentiable ($C^{\\infty}$) on $\\mathbb{R}$. This ensures that $f(x)$ is locally Lipschitz on any compact interval of $\\mathbb{R}$, which by the Picard-LindelÃ¶f theorem guarantees the existence and uniqueness of solutions for any given initial condition $x(0) = x_{0}$.\n\nThe behavior of trajectories can be qualitatively understood by examining the sign of $\\dot{x} = x(1-x)(1+x)$ on the intervals defined by the equilibria:\n- For $x \\in (1, \\infty)$, let's test $x=2$: $\\dot{x} = 2(1-2)(1+2) = -6 < 0$. Trajectories move to the left, toward $x=1$.\n- For $x \\in (0, 1)$, let's test $x=0.5$: $\\dot{x} = 0.5(1-0.5)(1+0.5) > 0$. Trajectories move to the right, toward $x=1$.\n- For $x \\in (-1, 0)$, let's test $x=-0.5$: $\\dot{x} = -0.5(1-(-0.5))(1+(-0.5)) < 0$. Trajectories move to the left, toward $x=-1$.\n- For $x \\in (-\\infty, -1)$, let's test $x=-2$: $\\dot{x} = -2(1-(-2))(1+(-2)) > 0$. Trajectories move to the right, toward $x=-1$.\n\nThis phase-line analysis suggests that the region of attraction for $x^{\\star}=1$ is $(0, \\infty)$, the region of attraction for $x^{\\star}=-1$ is $(-\\infty, 0)$, and the point $x^{\\star}=0$ is the boundary separating these two basins of attraction.\n\nWe must rigorously justify these conclusions.\nLet $A(x^{\\star})$ denote the region of attraction (or basin of attraction) of an equilibrium $x^{\\star}$.\nFor the equilibrium $x^{\\star}=1$:\nLet an initial condition $x(0) = x_{0} \\in (0, 1)$. Since $\\dot{x} > 0$ in this interval, the solution $x(t)$ is a monotonically increasing function of time. As $x=1$ is an equilibrium point, uniqueness of solutions prevents $x(t)$ from crossing $1$. Thus, $x(t)$ is a monotonic increasing function bounded above by $1$. By the monotone convergence theorem, the limit $\\lim_{t \\to \\infty} x(t)$ must exist and must be an equilibrium point. The only equilibrium in the interval $(0, 1]$ is $x=1$. Therefore, $\\lim_{t \\to \\infty} x(t) = 1$.\nNow, let $x(0) = x_{0} \\in (1, \\infty)$. Since $\\dot{x} < 0$ in this interval, the solution $x(t)$ is monotonically decreasing and bounded below by $1$. By a similar argument, $\\lim_{t \\to \\infty} x(t) = 1$.\nTrivially, if $x(0)=1$, then $x(t)=1$ for all $t \\geq 0$.\nCombining these cases, we have rigorously established that the region of attraction for $x^{\\star}=1$ is $A(1) = (0, \\infty)$.\n\nFor the equilibrium $x^{\\star}=-1$:\nThe vector field $f(x)=x-x^3$ is an odd function, i.e., $f(-x)=-f(x)$. This implies that if $x(t)$ is a solution to $\\dot{x}=f(x)$, then $y(t) = -x(t)$ is also a solution because $\\dot{y}(t) = -\\dot{x}(t) = -f(x(t)) = -f(-y(t)) = f(y(t))$. Consequently, the phase portrait is symmetric with respect to the origin. The region of attraction for $x^{\\star}=-1$ must be the negative of the region of attraction for $x^{\\star}=1$. Thus, $A(-1) = (-\\infty, 0)$.\n\nFor the equilibrium $x^{\\star}=0$:\nThis is an unstable equilibrium. Any initial condition $x_{0} \\neq 0$, no matter how close to $0$, will result in a trajectory that moves away from $0$ and converges to either $1$ or $-1$. Therefore, the region of attraction for $x^{\\star}=0$ consists only of the point itself: $A(0) = \\{0\\}$.\n\nThe basin boundaries are the interfaces between the regions of attraction. The state space $\\mathbb{R}$ is partitioned into three disjoint sets: $A(-1) = (-\\infty, 0)$, $A(0) = \\{0\\}$, and $A(1) = (0, \\infty)$. The boundary of $A(1)$ is $\\partial A(1) = \\{0\\}$, and the boundary of $A(-1)$ is $\\partial A(-1) = \\{0\\}$. The point $x=0$ is an unstable equilibrium, and its stable manifold (which is just the point itself in this scalar case) forms the boundary between the basins of the two stable equilibria. Since for any large $|x|$, $\\dot{x} \\approx -x^3$, which pushes the trajectory towards the origin, no trajectory can escape to infinity. Thus, every point in $\\mathbb{R}$ must converge to one of the three equilibria. The partition is complete, and no other boundaries exist.\n\nFinally, we are asked to compute the Lebesgue measure of the intersection of the region of attraction of the equilibrium $x^{\\star}=1$ with the interval $[-2, 2]$.\nThe region of attraction of $x^{\\star}=1$ is $A(1) = (0, \\infty)$.\nThe interval is $I = [-2, 2]$.\nThe intersection of these two sets is:\n$$S = A(1) \\cap I = (0, \\infty) \\cap [-2, 2] = (0, 2]$$\nThe Lebesgue measure, $\\mu$, of an interval $(a, b]$ is given by $b-a$.\nTherefore, the measure of the intersection is:\n$$\\mu(S) = \\mu((0, 2]) = 2 - 0 = 2$$\nThe final answer is $2$.", "answer": "$$\\boxed{2}$$", "id": "2738256"}, {"introduction": "For higher-dimensional systems, finding the exact boundary of the Region of Attraction (ROA) is rarely feasible, so we must instead turn to powerful estimation techniques. This practice [@problem_id:2738264] introduces the cornerstone of ROA estimation: using a quadratic Lyapunov function to find a certified \"safe\" region around an equilibrium. Your task will be to analyze the behavior of the Lyapunov derivative, $\\dot{V}(x)$, to determine the largest possible sublevel set of $V(x)$ that can be guaranteed as a subset of the true ROA.", "problem": "Consider the autonomous nonlinear system given by the ordinary differential equation (ODE) $\\dot{x}=f(x)$ with state $x=\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}\\in\\mathbb{R}^{2}$ and dynamics\n$$\n\\dot{x}=\\begin{bmatrix}-x_{1}+x_{1}x_{2}\\\\ -x_{2}\\end{bmatrix}.\n$$\nLet the continuously differentiable, positive definite function candidate $V:\\mathbb{R}^{2}\\to\\mathbb{R}_{\\ge 0}$ be\n$$\nV(x)=x_{1}^{2}+x_{2}^{2}.\n$$\nStarting from the fundamental definition of the Lyapunov derivative $\\dot{V}(x)=\\nabla V(x)^{\\top}f(x)$, first compute $\\dot{V}(x)$ along trajectories of the system. Next, using only first principles (optimization under a norm constraint and comparison inequalities), determine the largest $c>0$ such that the sublevel set\n$$\n\\Omega_{c}\\coloneqq\\{x\\in\\mathbb{R}^{2}:V(x)\\le c\\}\n$$\nis an inner estimate of the region of attraction of the equilibrium $x=0$ certified by the direct method of Lyapunov; that is, $\\dot{V}(x)\\le 0$ for all $x$ with $V(x)=c$ and $\\dot{V}(x)<0$ for all $x\\neq 0$ with $V(x)<c$.\n\nReport your final answer as the ordered pair $(\\dot{V}(x),\\,c)$ in exact form (no rounding).", "solution": "The first step is to compute the time derivative of the Lyapunov function candidate, $V(x)$, along the trajectories of the system. The system dynamics are $\\dot{x} = f(x) = \\begin{bmatrix} -x_{1}+x_{1}x_{2} \\\\ -x_{2} \\end{bmatrix}$. The gradient of $V(x) = x_{1}^{2}+x_{2}^{2}$ is:\n$$\n\\nabla V(x) = \\begin{bmatrix} \\frac{\\partial V}{\\partial x_{1}} \\\\ \\frac{\\partial V}{\\partial x_{2}} \\end{bmatrix} = \\begin{bmatrix} 2x_{1} \\\\ 2x_{2} \\end{bmatrix}\n$$\nThe Lyapunov derivative is then computed as $\\dot{V}(x) = \\nabla V(x)^{\\top}f(x)$:\n$$\n\\dot{V}(x) = \\begin{bmatrix} 2x_{1} & 2x_{2} \\end{bmatrix} \\begin{bmatrix} -x_{1}+x_{1}x_{2} \\\\ -x_{2} \\end{bmatrix}\n$$\n$$\n\\dot{V}(x) = 2x_{1}(-x_{1}+x_{1}x_{2}) + 2x_{2}(-x_{2})\n$$\n$$\n\\dot{V}(x) = -2x_{1}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2}\n$$\nThis is the expression for $\\dot{V}(x)$.\n\nThe second step is to find the largest constant $c>0$ such that the sublevel set $\\Omega_c=\\{x : V(x) \\le c\\}$ is an inner estimate of the region of attraction. According to the direct method of Lyapunov, this requires that $\\dot{V}(x)$ be negative semi-definite within $\\Omega_c$, becoming zero only at the equilibrium point $x=0$.\n\nThe region of attraction estimate is compromised in any region of the state space where $\\dot{V}(x) > 0$. Let us analyze this condition:\n$$\n-2x_{1}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2} > 0\n$$\n$$\n2x_{1}^{2}(x_{2}-1) > 2x_{2}^{2}\n$$\n$$\nx_{1}^{2}(x_{2}-1) > x_{2}^{2}\n$$\nFor this inequality to hold, since $x_{1}^{2} \\ge 0$ and $x_{2}^{2} \\ge 0$, it is necessary that $x_{2}-1 > 0$, which implies $x_{2} > 1$. If $x_{2} \\le 1$, the left-hand side is non-positive while the right-hand side is non-negative, so the inequality cannot be satisfied. Therefore, the \"unsafe\" region where $\\dot{V}(x)$ can be positive exists only where $x_{2} > 1$.\n\nWe seek the largest sublevel set $\\Omega_c$, which is a disk of radius $\\sqrt{c}$ centered at the origin, that does not enter this unsafe region. This means that for any $x$ such that $V(x) \\le c$, we must have $\\dot{V}(x) \\le 0$. The largest such $c$ is determined by the minimum value of $V(x)$ on the boundary of the unsafe region. This boundary is the set of points where $\\dot{V}(x) = 0$ (excluding the origin $x=0$, where $\\dot{V}(0)=0$ trivially).\n\nWe formulate this as a constrained optimization problem:\nMinimize $V(x) = x_{1}^{2}+x_{2}^{2}$ subject to the constraint $\\dot{V}(x) = -2x_{1}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2} = 0$, for $x \\neq 0$.\nThe constraint simplifies to:\n$$\nx_{1}^{2}(x_{2}-1) = x_{2}^{2}\n$$\nAs established, a non-trivial solution ($x_1 \\neq 0$) requires $x_{2} > 1$. From the constraint, we can express $x_{1}^{2}$ in terms of $x_{2}$:\n$$\nx_{1}^{2} = \\frac{x_{2}^{2}}{x_{2}-1}\n$$\nNow, substitute this expression into the function $V(x)$ to obtain a function of a single variable, $x_2$:\n$$\nV(x_{2}) = \\frac{x_{2}^{2}}{x_{2}-1} + x_{2}^{2} = x_{2}^{2} \\left( \\frac{1}{x_{2}-1} + 1 \\right) = x_{2}^{2} \\left( \\frac{1 + x_{2}-1}{x_{2}-1} \\right) = \\frac{x_{2}^{3}}{x_{2}-1}\n$$\nTo find the minimum value of this function for $x_{2} > 1$, we compute its derivative with respect to $x_2$ and set it to zero:\n$$\n\\frac{dV}{dx_{2}} = \\frac{d}{dx_{2}}\\left(\\frac{x_{2}^{3}}{x_{2}-1}\\right) = \\frac{(3x_{2}^{2})(x_{2}-1) - (x_{2}^{3})(1)}{(x_{2}-1)^{2}} = \\frac{3x_{2}^{3} - 3x_{2}^{2} - x_{2}^{3}}{(x_{2}-1)^{2}} = \\frac{2x_{2}^{3} - 3x_{2}^{2}}{(x_{2}-1)^{2}}\n$$\nSetting the derivative to zero to find critical points:\n$$\n\\frac{x_{2}^{2}(2x_{2}-3)}{(x_{2}-1)^{2}} = 0\n$$\nSince we are in the domain $x_{2} > 1$, the only solution is $2x_{2}-3 = 0$, which yields $x_{2} = \\frac{3}{2}$.\nThe second derivative test or analysis of the sign of the first derivative confirms this is a minimum. For $1 < x_2 < \\frac{3}{2}$, the derivative is negative, and for $x_2 > \\frac{3}{2}$, it is positive.\nThe minimum value of $V(x)$ on the set $\\dot{V}(x)=0$ is found by evaluating the function at $x_{2} = \\frac{3}{2}$:\n$$\nc = V\\left(x_{2}=\\frac{3}{2}\\right) = \\frac{\\left(\\frac{3}{2}\\right)^{3}}{\\frac{3}{2}-1} = \\frac{\\frac{27}{8}}{\\frac{1}{2}} = \\frac{27}{4}\n$$\nThis value $c = \\frac{27}{4}$ is the largest value for which the sublevel set $\\Omega_c$ is guaranteed to be a subset of the region of attraction using this Lyapunov function. For any $c \\le \\frac{27}{4}$, all points $x\\neq 0$ inside the set $\\Omega_c$ have $\\dot{V}(x) < 0$, and on the boundary $V(x)=c$, we have $\\dot{V}(x) \\le 0$.\n\nThe final answer consists of the pair $(\\dot{V}(x), c)$.\n$\\dot{V}(x) = -2x_{1}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2}$\n$c = \\frac{27}{4}$", "answer": "$$\n\\boxed{\\begin{pmatrix} -2x_{1}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2} & \\frac{27}{4} \\end{pmatrix}}\n$$", "id": "2738264"}, {"introduction": "Lyapunov-based estimates are powerful because they provide a rigorous guarantee, but they are often conservative. This final exercise bridges the gap between theoretical analysis and practical implementation. In this problem [@problem_id:2721989], you will first derive an analytical inner estimate of the ROA and then design a systematic numerical experiment to probe the region beyond this conservative bound. This practice showcases a standard engineering workflow: using theory to establish a baseline for safety and then using computation to explore the system's actual performance limits.", "problem": "Consider the autonomous nonlinear ordinary differential equation (ODE) in continuous time given by $\\dot{x} = f(x)$ for the state $x \\in \\mathbb{R}^2$, where\n$$\nf(x) = A x + g(x), \\quad A = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix},\n$$\nand\n$$\ng(x) = \\begin{bmatrix}\nc_1 x_1^3 + c_2 x_1 x_2^2 \\\\\nc_3 x_1^2 x_2 + c_4 x_2^3\n\\end{bmatrix}.\n$$\nWork from the following foundational base in control theory:\n- The Jacobian linearization of $\\dot{x} = f(x)$ at the equilibrium $x = 0$ is given by $A = \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=0}$, and Lyapunov's indirect method states: if $A$ is Hurwitz (all eigenvalues have strictly negative real parts), then the equilibrium is locally asymptotically stable.\n- For any symmetric positive definite matrix $Q \\in \\mathbb{S}_{++}^2$, the continuous-time Lyapunov equation $A^\\top P + P A = -Q$ has a unique solution $P \\in \\mathbb{S}_{++}^2$ when $A$ is Hurwitz.\n- A quadratic Lyapunov function candidate can be taken as $V(x) = x^\\top P x$, and along trajectories $\\dot{V}(x) = \\nabla V(x)^\\top f(x)$.\n- Use Euclidean norms and the CauchyâSchwarz inequality, together with elementary bounds such as $\\lvert x_i \\rvert \\le \\lVert x \\rVert_2$ for $i \\in \\{1,2\\}$.\n\nYour task is to formalize and implement a numerical experiment that estimates a local domain of attraction around the origin for the above system by simulating trajectories that start on the boundary of quadratic level sets. You must relate the numerical outcomes to a theoretically derived sufficient bound that guarantees negativity of $\\dot{V}$ within a quadratic level set.\n\nCarry out the following, without using any shortcut formulas given in advance.\n\n- Show that $A$ is Hurwitz and choose $Q = I_2$, then compute $P \\succ 0$ from $A^\\top P + P A = -Q$.\n- Define $V(x) = x^\\top P x$. For the nonlinear term $g(x)$, derive a norm bound of the form $\\lVert g(x) \\rVert_2 \\le c \\lVert x \\rVert_2^3$ with a computable constant $c$ expressed only in terms of the coefficients $c_1, c_2, c_3, c_4$.\n- Using only the above foundational tools, derive a sufficient condition that guarantees $\\dot{V}(x) < 0$ for all $x$ in the quadratic sublevel set $\\{x \\in \\mathbb{R}^2 \\mid V(x) \\le \\rho\\}$. Express your condition as an explicit upper bound $\\rho_\\star$ such that if $\\rho \\in (0,\\rho_\\star)$ is chosen, then $\\dot{V}(x) < 0$ holds for all $x$ in that sublevel set.\n- Design a numerical experiment that, for a given $\\rho$, samples initial conditions on the ellipse $\\{x \\in \\mathbb{R}^2 \\mid V(x) = \\rho\\}$, simulates the trajectories under $\\dot{x} = f(x)$ for a fixed time horizon, and labels a trial as successful if the state approaches the origin numerically. Use the following specifications:\n  - Use $Q = I_2$.\n  - Use a fixed time horizon $T = 20$.\n  - Use $N = 24$ initial conditions, taken uniformly in angle on the ellipse $V(x) = \\rho$ via an exact transformation from the Euclidean circle to that ellipse.\n  - Use absolute tolerance $\\mathrm{atol} = 10^{-12}$ and relative tolerance $\\mathrm{rtol} = 10^{-9}$ in the numerical integrator.\n  - Declare numerical convergence if the Euclidean norm satisfies $\\lVert x(T) \\rVert_2 \\le 10^{-3}$ or if an event detects crossing into the ball of radius $10^{-3}$ around the origin at any earlier time.\n  - Declare numerical divergence if an event detects that the Euclidean norm exceeds $10$ at any time.\n- For each test case below, first compute $\\rho_\\star$ from your derived sufficient condition. Then set $\\rho = s \\rho_\\star$ using the scale factor $s$ from the test case. Run the simulation experiment described above using this $\\rho$. Return a boolean for each test case equal to $\\mathrm{True}$ if and only if all simulated trajectories from the $N$ initial conditions converge numerically as defined above; otherwise return $\\mathrm{False}$.\n\nUse the following test suite of parameter values:\n- Test case $1$: $(c_1,c_2,c_3,c_4) = (0.5,0.5,0.5,0.5)$ and $s = 0.5$.\n- Test case $2$: $(c_1,c_2,c_3,c_4) = (1,1,1,1)$ and $s = 0.99$.\n- Test case $3$: $(c_1,c_2,c_3,c_4) = (2,2,2,2)$ and $s = 20$.\n\nAll quantities are nondimensional and unit-free.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[r_1,r_2,r_3]$ where each $r_i$ is a boolean. No additional text must be printed.", "solution": "The problem demands a two-part analysis: first, a theoretical estimation of a region of attraction for a given nonlinear system, and second, a numerical experiment to test the boundaries of this theoretical estimate.\n\nThe system dynamics are given by $\\dot{x} = f(x)$, where $x \\in \\mathbb{R}^2$, with $f(x) = A x + g(x)$,\n$$\nA = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix}, \\quad g(x) = \\begin{bmatrix}\nc_1 x_1^3 + c_2 x_1 x_2^2 \\\\\nc_3 x_1^2 x_2 + c_4 x_2^3\n\\end{bmatrix}.\n$$\nThe origin $x=0$ is an equilibrium point, as $f(0) = 0$. The Jacobian of $f(x)$ at the origin is the matrix $A$.\n\nFirst, we establish the local asymptotic stability of the equilibrium point for the linearized system $\\dot{x} = Ax$. This is determined by the eigenvalues of $A$. The characteristic equation is $\\det(A - \\lambda I) = 0$, which is\n$$\n\\det \\begin{bmatrix} -\\lambda & 1 \\\\ -2 & -3-\\lambda \\end{bmatrix} = (-\\lambda)(-3-\\lambda) - (1)(-2) = \\lambda^2 + 3\\lambda + 2 = 0.\n$$\nThis factors as $(\\lambda+1)(\\lambda+2) = 0$, yielding eigenvalues $\\lambda_1 = -1$ and $\\lambda_2 = -2$. Since both eigenvalues have strictly negative real parts, the matrix $A$ is Hurwitz. By Lyapunov's indirect method, the equilibrium $x=0$ of the nonlinear system is locally asymptotically stable.\n\nNext, we construct a quadratic Lyapunov function $V(x) = x^\\top P x$ to analyze the domain of attraction. We must find the symmetric positive definite matrix $P$ by solving the continuous-time Lyapunov equation $A^\\top P + P A = -Q$. The problem specifies using $Q=I_2$, the $2 \\times 2$ identity matrix. Let $P = \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix}$. The equation becomes:\n$$\n\\begin{bmatrix} 0 & -2 \\\\ 1 & -3 \\end{bmatrix} \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} + \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n$$\nPerforming the matrix multiplication and summation yields:\n$$\n\\begin{bmatrix} -4p_{12} & p_{11}-3p_{12}-2p_{22} \\\\ p_{11}-3p_{12}-2p_{22} & 2p_{12}-6p_{22} \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n$$\nEquating the elements gives a system of linear equations for the entries of $P$:\n1. $-4p_{12} = -1 \\implies p_{12} = \\frac{1}{4}$.\n2. $2p_{12}-6p_{22} = -1 \\implies 2(\\frac{1}{4}) - 6p_{22} = -1 \\implies \\frac{1}{2} - 6p_{22} = -1 \\implies p_{22} = \\frac{1}{4}$.\n3. $p_{11}-3p_{12}-2p_{22} = 0 \\implies p_{11} = 3(\\frac{1}{4}) + 2(\\frac{1}{4}) = \\frac{5}{4}$.\nThus, the solution is $P = \\frac{1}{4} \\begin{bmatrix} 5 & 1 \\\\ 1 & 1 \\end{bmatrix}$. To confirm $P$ is positive definite, we check its principal minors: $p_{11} = \\frac{5}{4} > 0$ and $\\det(P) = (\\frac{5}{4})(\\frac{1}{4}) - (\\frac{1}{4})^2 = \\frac{4}{16} = \\frac{1}{4} > 0$. Both are positive, so $P \\succ 0$.\n\nNow, we derive a sufficient condition for the time derivative of the Lyapunov function, $\\dot{V}(x)$, to be negative. The derivative along the trajectories of the nonlinear system is:\n$$\n\\dot{V}(x) = \\nabla V(x)^\\top f(x) = (2Px)^\\top (Ax + g(x)) = x^\\top(A^\\top P + PA)x + 2x^\\top P g(x).\n$$\nUsing $A^\\top P + PA = -I_2$, this simplifies to:\n$$\n\\dot{V}(x) = -x^\\top I_2 x + 2x^\\top P g(x) = -\\lVert x \\rVert_2^2 + 2x^\\top P g(x).\n$$\nFor $\\dot{V}(x)$ to be negative, we require $2x^\\top P g(x)  \\lVert x \\rVert_2^2$. We bound the term involving $g(x)$. First, we find a bound on the norm of $g(x)$. Using the inequality $|x_i| \\le \\lVert x \\rVert_2$:\n$$\n\\lvert g_1(x) \\rvert = \\lvert c_1 x_1^3 + c_2 x_1 x_2^2 \\rvert \\le \\lvert c_1 \\rvert \\lvert x_1 \\rvert^3 + \\lvert c_2 \\rvert \\lvert x_1 \\rvert \\lvert x_2 \\rvert^2 \\le (\\lvert c_1 \\rvert + \\lvert c_2 \\rvert) \\lVert x \\rVert_2^3.\n$$\n$$\n\\lvert g_2(x) \\rvert = \\lvert c_3 x_1^2 x_2 + c_4 x_2^3 \\rvert \\le \\lvert c_3 \\rvert \\lvert x_1 \\rvert^2 \\lvert x_2 \\rvert + \\lvert c_4 \\rvert \\lvert x_2 \\rvert^3 \\le (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert) \\lVert x \\rVert_2^3.\n$$\nThe Euclidean norm of $g(x)$ is therefore bounded by:\n$$\n\\lVert g(x) \\rVert_2 = \\sqrt{\\lvert g_1(x) \\rvert^2 + \\lvert g_2(x) \\rvert^2} \\le \\sqrt{(\\lvert c_1 \\rvert + \\lvert c_2 \\rvert)^2 \\lVert x \\rVert_2^6 + (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert)^2 \\lVert x \\rVert_2^6} = c \\lVert x \\rVert_2^3,\n$$\nwhere $c = \\sqrt{(\\lvert c_1 \\rvert + \\lvert c_2 \\rvert)^2 + (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert)^2}$.\n\nUsing this bound and the Cauchy-Schwarz inequality, we bound $2x^\\top P g(x)$:\n$$\n2x^\\top P g(x) \\le 2 \\lvert x^\\top P g(x) \\rvert \\le 2 \\lVert Px \\rVert_2 \\lVert g(x) \\rVert_2.\n$$\nSince $P$ is symmetric positive definite, its induced $2$-norm is its largest eigenvalue, $\\lVert P \\rVert_2 = \\lambda_{\\max}(P)$. Thus, $\\lVert Px \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert x \\rVert_2 = \\lambda_{\\max}(P) \\lVert x \\rVert_2$. Substituting the bounds:\n$$\n2x^\\top P g(x) \\le 2 (\\lambda_{\\max}(P) \\lVert x \\rVert_2) (c \\lVert x \\rVert_2^3) = 2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^4.\n$$\nThe condition $\\dot{V}(x)  0$ is therefore guaranteed if $-\\lVert x \\rVert_2^2 + 2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^4  0$. For $x \\ne 0$, this simplifies to $2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^2  1$, or $\\lVert x \\rVert_2^2  \\frac{1}{2c\\lambda_{\\max}(P)}$.\n\nWe need to find an upper bound $\\rho_\\star$ on $\\rho$ such that for any $x$ in the sublevel set $\\{x \\mid V(x) \\le \\rho\\}$, the condition $\\dot{V}(x)  0$ holds. For any $x$ in this set, we have the inequality $V(x) \\ge \\lambda_{\\min}(P) \\lVert x \\rVert_2^2$, which implies $\\lVert x \\rVert_2^2 \\le \\frac{V(x)}{\\lambda_{\\min}(P)} \\le \\frac{\\rho}{\\lambda_{\\min}(P)}$. To satisfy the negativity condition for all $x$ in the sublevel set, we require its boundary to be within the region of guaranteed negativity. We impose the strict inequality:\n$$\n\\frac{\\rho}{\\lambda_{\\min}(P)}  \\frac{1}{2c\\lambda_{\\max}(P)} \\implies \\rho  \\frac{\\lambda_{\\min}(P)}{2c\\lambda_{\\max}(P)}.\n$$\nThis gives the desired explicit upper bound $\\rho_\\star = \\frac{\\lambda_{\\min}(P)}{2c\\lambda_{\\max}(P)}$. The eigenvalues of $P$ are found from the characteristic equation of $4P$, which is $\\mu^2 - 6\\mu + 4 = 0$, yielding $\\mu = 3 \\pm \\sqrt{5}$. Thus, the eigenvalues of $P$ are $\\lambda_{\\min}(P) = \\frac{3-\\sqrt{5}}{4}$ and $\\lambda_{\\max}(P) = \\frac{3+\\sqrt{5}}{4}$.\nThe bound is $\\rho_\\star = \\frac{(3-\\sqrt{5})/4}{2c(3+\\sqrt{5})/4} = \\frac{3-\\sqrt{5}}{2c(3+\\sqrt{5})} = \\frac{(3-\\sqrt{5})^2}{2c(9-5)} = \\frac{14 - 6\\sqrt{5}}{8c} = \\frac{7 - 3\\sqrt{5}}{4c}$.\n\nFor the numerical experiment, we generate initial conditions on the ellipse $x^\\top P x = \\rho$. A point $x$ on this ellipse can be generated from a point $y$ on a circle of radius $\\sqrt{\\rho}$ through a linear transformation. Let $P=LL^\\top$ be the Cholesky decomposition of $P$. Then $x^\\top P x = x^\\top L L^\\top x = (L^\\top x)^\\top (L^\\top x) = \\lVert L^\\top x \\rVert_2^2 = \\rho$. We define $y = L^\\top x$, so that $\\lVert y \\rVert_2 = \\sqrt{\\rho}$. The transformation is $x = (L^\\top)^{-1}y$. We will sample $N=24$ points $y_k = \\sqrt{\\rho} [\\cos \\theta_k, \\sin \\theta_k]^\\top$ with $\\theta_k = 2\\pi k/N$ for $k=0, \\dots, N-1$, and transform them to initial conditions $x_k$ for numerical integration.\n\nThe simulation of $\\dot{x}=f(x)$ for each $x_k$ will be performed over a time horizon $T=20$ using a numerical integrator with specified tolerances. Convergence is declared if the trajectory enters a small ball of radius $10^{-3}$ around the origin. Divergence is declared if the trajectory norm exceeds $10$. If all $N=24$ trajectories from the initial conditions on the ellipse $V(x) = \\rho = s \\rho_\\star$ converge, the test case is successful.\n\nThis completes the formal analytical derivation and the design of the numerical experiment. The implementation will proceed based on these results.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the specified numerical experiment and print results.\n    \"\"\"\n    # Define test cases: ((c1, c2, c3, c4), s)\n    test_cases = [\n        ((0.5, 0.5, 0.5, 0.5), 0.5),\n        ((1.0, 1.0, 1.0, 1.0), 0.99),\n        ((2.0, 2.0, 2.0, 2.0), 20.0),\n    ]\n\n    results = []\n    for c_coeffs, s in test_cases:\n        results.append(run_experiment(c_coeffs, s))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_experiment(c_coeffs, s_factor):\n    \"\"\"\n    Computes the theoretical bound rho_star, sets up the simulation radius rho,\n    and runs the simulation for N initial conditions.\n\n    Returns:\n        bool: True if all trajectories converge, False otherwise.\n    \"\"\"\n    # --- Theoretical Analysis ---\n    # Matrix A\n    A = np.array([[0, 1], [-2, -3]])\n    \n    # Solution P to A^T P + P A = -I\n    P = np.array([[5/4, 1/4], [1/4, 1/4]])\n    \n    # Eigenvalues of P\n    lambda_min_P = (3 - np.sqrt(5)) / 4\n    lambda_max_P = (3 + np.sqrt(5)) / 4\n    \n    # Constant c for the bound on g(x)\n    c1, c2, c3, c4 = c_coeffs\n    c = np.sqrt((abs(c1) + abs(c2))**2 + (abs(c3) + abs(c4))**2)\n    \n    # Theoretical bound rho_star\n    rho_star = (7 - 3 * np.sqrt(5)) / (4 * c)\n    \n    # Simulation level set parameter rho\n    rho = s_factor * rho_star\n\n    # --- Numerical Experiment ---\n    N_samples = 24\n    time_horizon = 20.0\n    \n    # Transformation matrix to map circle to ellipse V(x)=rho\n    # P = L L^T, x = (L^T)^-1 y\n    # L_inv_T = inv(L.T) where L is the Cholesky factor of P\n    L_inv_T = np.array([[2/np.sqrt(5), -1/np.sqrt(5)], [0, np.sqrt(5)]])\n    \n    # Generate initial conditions on the ellipse x^T P x = rho\n    thetas = np.linspace(0, 2 * np.pi, N_samples, endpoint=False)\n    radius = np.sqrt(rho)\n    y_coords = radius * np.vstack([np.cos(thetas), np.sin(thetas)])\n    initial_conditions = L_inv_T @ y_coords\n    \n    # --- Simulation Loop ---\n    all_converged = True\n    for i in range(N_samples):\n        x0 = initial_conditions[:, i]\n        converged = simulate_trajectory(A, c_coeffs, x0, time_horizon)\n        if not converged:\n            all_converged = False\n            break\n            \n    return all_converged\n\ndef simulate_trajectory(A, c_coeffs, x0, T):\n    \"\"\"\n    Integrates the ODE for one initial condition and checks for convergence.\n    \"\"\"\n    # Define the nonlinear system dynamics: dx/dt = f(x)\n    def ode_func(t, x, A_mat, coeffs):\n        c1, c2, c3, c4 = coeffs\n        gx = np.array([\n            c1 * x[0]**3 + c2 * x[0] * x[1]**2,\n            c3 * x[0]**2 * x[1] + c4 * x[1]**3\n        ])\n        return A_mat @ x + gx\n\n    # Event for numerical convergence\n    def convergence_event(t, x, A_mat, coeffs):\n        return np.linalg.norm(x) - 1e-3\n    convergence_event.terminal = True\n    convergence_event.direction = -1\n\n    # Event for numerical divergence\n    def divergence_event(t, x, A_mat, coeffs):\n        return np.linalg.norm(x) - 10.0\n    divergence_event.terminal = True\n    divergence_event.direction = 1\n\n    sol = solve_ivp(\n        fun=ode_func,\n        t_span=(0, T),\n        y0=x0,\n        method='RK45',\n        args=(A, c_coeffs),\n        rtol=1e-9,\n        atol=1e-12,\n        events=[convergence_event, divergence_event]\n    )\n    \n    # Check for convergence\n    # 1. Did the convergence event trigger?\n    if sol.t_events[0].size > 0:\n        return True\n    # 2. Did the simulation finish at T? Check final state norm.\n    if sol.status == 0 and np.linalg.norm(sol.y[:, -1]) = 1e-3:\n        return True\n    \n    # If neither of the above, it's considered not to have converged.\n    # This includes divergence event or finishing at T with norm > 1e-3.\n    return False\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "2721989"}]}