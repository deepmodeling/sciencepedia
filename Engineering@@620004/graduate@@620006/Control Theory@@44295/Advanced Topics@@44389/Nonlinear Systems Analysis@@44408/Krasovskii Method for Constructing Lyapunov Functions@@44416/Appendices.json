{"hands_on_practices": [{"introduction": "Mastering any new technique begins with the fundamentals. This first exercise provides a direct, hands-on application of the Krasovskii method for a simple nonlinear system. By constructing the Lyapunov candidate $V(x) = \\|f(x)\\|^2$ and calculating its time derivative step-by-step, you will solidify your understanding of the core mechanics and build a foundation for more complex analyses [@problem_id:2715988].", "problem": "Consider the autonomous nonlinear planar system defined by the ordinary differential equation (ODE) $\\dot{x}=f(x)$ with\n$$\nf(x)=\\begin{bmatrix} f_{1}(x) \\\\ f_{2}(x)\\end{bmatrix}=\\begin{bmatrix}-x_{1}+x_{1}x_{2}\\\\ -x_{2}\\end{bmatrix},\n$$\nwhere $x=\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix}^{\\top}\\in\\mathbb{R}^{2}$. Using Krasovskii's method for constructing Lyapunov functions, consider the candidate\n$$\nV(x)=\\|f(x)\\|^{2}=f(x)^{\\top}f(x).\n$$\nYour tasks are:\n- Analyze the definiteness of $V(x)$ about the origin $x=0$ (that is, determine whether $V$ is positive definite, negative definite, positive semidefinite, or negative semidefinite, and whether it is radially unbounded).\n- Compute the orbital derivative $\\dot{V}(x)$ along trajectories of the system, starting from core definitions and the chain rule (do not use any pre-stated shortcut formulas).\n\nExpress the final $\\dot{V}(x)$ as a single simplified analytic expression in terms of $x_{1}$ and $x_{2}$. No numerical rounding is required and no units are involved. The final answer you provide must be only the explicit expression for $\\dot{V}(x)$.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- System: Autonomous nonlinear planar ODE $\\dot{x}=f(x)$.\n- State vector: $x=\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix}^{\\top}\\in\\mathbb{R}^{2}$.\n- Vector field: $f(x)=\\begin{bmatrix}-x_{1}+x_{1}x_{2}\\\\ -x_{2}\\end{bmatrix}$.\n- Lyapunov candidate function: $V(x)=\\|f(x)\\|^{2}=f(x)^{\\top}f(x)$.\n- Tasks:\n    1. Analyze the definiteness of $V(x)$ about the origin $x=0$.\n    2. Compute the orbital derivative $\\dot{V}(x)$ along trajectories, starting from the chain rule.\n- Final Answer Requirement: Provide the explicit expression for $\\dot{V}(x)$.\n\nValidation against criteria:\nThe problem is scientifically grounded in the established field of Lyapunov stability theory for nonlinear dynamical systems. It is well-posed, providing all necessary definitions and a clear, objective set of tasks that lead to a unique mathematical solution. The problem is free of factual errors, contradictions, and ambiguities. It is a standard, non-trivial exercise in control theory.\n\nVerdict:\nThe problem is valid. We proceed with the solution.\n\nFirst, we determine the equilibrium point(s) of the system by setting $\\dot{x}=f(x)=0$.\n$$\n\\begin{bmatrix}-x_{1}+x_{1}x_{2}\\\\ -x_{2}\\end{bmatrix} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}\n$$\nFrom the second equation, $-x_{2}=0$, we have $x_{2}=0$. Substituting this into the first equation gives $-x_{1}+x_{1}(0) = -x_{1}=0$, which implies $x_{1}=0$. The only equilibrium point is the origin, $x=0$.\n\nThe first task is to analyze the definiteness of the Lyapunov candidate function $V(x)$. The function is defined as $V(x)=f(x)^{\\top}f(x)$.\n$$\nV(x) = \\begin{bmatrix}-x_{1}+x_{1}x_{2} & -x_{2}\\end{bmatrix} \\begin{bmatrix}-x_{1}+x_{1}x_{2}\\\\ -x_{2}\\end{bmatrix}\n$$\n$$\nV(x) = (-x_{1}+x_{1}x_{2})^{2} + (-x_{2})^{2} = x_{1}^{2}(x_{2}-1)^{2} + x_{2}^{2}\n$$\nTo analyze its definiteness about the origin:\n1.  Evaluate $V(x)$ at the origin $x=0$:\n    $V(0) = 0^{2}(0-1)^{2} + 0^{2} = 0$.\n2.  Evaluate $V(x)$ for any $x \\neq 0$. The function $V(x)$ is a sum of two non-negative terms, $x_{1}^{2}(x_{2}-1)^{2} \\ge 0$ and $x_{2}^{2} \\ge 0$. Thus, $V(x) \\ge 0$ for all $x \\in \\mathbb{R}^{2}$. $V(x)=0$ if and only if both terms are zero simultaneously:\n    - $x_{2}^{2} = 0 \\implies x_{2}=0$.\n    - $x_{1}^{2}(x_{2}-1)^{2} = 0$. With $x_{2}=0$, this becomes $x_{1}^{2}(-1)^{2} = x_{1}^{2} = 0 \\implies x_{1}=0$.\n    Therefore, $V(x)=0$ if and only if $x=0$. For all $x \\neq 0$, $V(x) > 0$. The function $V(x)$ is positive definite.\n\nFor radial unboundedness, we check if $V(x) \\to \\infty$ as $\\|x\\| \\to \\infty$.\nLet $\\|x\\| = \\sqrt{x_{1}^{2}+x_{2}^{2}} \\to \\infty$.\n$V(x) = x_{1}^{2}(x_{2}-1)^{2} + x_{2}^{2} = x_{1}^{2}(x_{2}^{2}-2x_{2}+1) + x_{2}^{2} = x_{1}^{2}x_{2}^{2} - 2x_{1}^{2}x_{2} + x_{1}^{2} + x_{2}^{2}$.\nThe term $x_{1}^{2}x_{2}^{2}$ dominates unless $x_{1}$ or $x_{2}$ is small. If either $x_{1} \\to \\infty$ or $x_{2} \\to \\infty$ (or both), the terms $x_{1}^{2}$ or $x_{2}^{2}$ or $x_{1}^{2}x_{2}^{2}$ will ensure $V(x) \\to \\infty$. For example, if we move along any ray $x_{1}=r\\cos\\theta$, $x_{2}=r\\sin\\theta$ for fixed $\\theta$ as $r \\to \\infty$, $V(x)$ grows at least as $r^{2}$. Thus, $V(x)$ is radially unbounded.\n\nThe second task is to compute the orbital derivative $\\dot{V}(x)$. The problem mandates derivation from first principles, specifically the chain rule. The orbital derivative is the time derivative of $V(x)$ along the system's trajectories:\n$$\n\\dot{V}(x) = \\frac{d}{dt}V(x(t))\n$$\nBy the chain rule for multivariable functions:\n$$\n\\dot{V}(x) = \\nabla V(x)^{\\top}\\dot{x} = \\nabla V(x)^{\\top}f(x) = \\frac{\\partial V}{\\partial x_{1}}f_{1}(x) + \\frac{\\partial V}{\\partial x_{2}}f_{2}(x)\n$$\nWe compute the partial derivatives of $V(x) = x_{1}^{2}(x_{2}-1)^{2} + x_{2}^{2}$:\n$$\n\\frac{\\partial V}{\\partial x_{1}} = \\frac{\\partial}{\\partial x_{1}} \\left( x_{1}^{2}(x_{2}-1)^{2} + x_{2}^{2} \\right) = 2x_{1}(x_{2}-1)^{2}\n$$\n$$\n\\frac{\\partial V}{\\partial x_{2}} = \\frac{\\partial}{\\partial x_{2}} \\left( x_{1}^{2}(x_{2}-1)^{2} + x_{2}^{2} \\right) = x_{1}^{2} \\cdot 2(x_{2}-1) \\cdot 1 + 2x_{2} = 2x_{1}^{2}(x_{2}-1) + 2x_{2}\n$$\nNow, substitute these partial derivatives and the components of $f(x)$ into the expression for $\\dot{V}(x)$:\n$$\n\\dot{V}(x) = \\left( 2x_{1}(x_{2}-1)^{2} \\right) (-x_{1}+x_{1}x_{2}) + \\left( 2x_{1}^{2}(x_{2}-1) + 2x_{2} \\right) (-x_{2})\n$$\nWe simplify the expression. First, factor $x_{1}$ from the term $(-x_{1}+x_{1}x_{2}) = x_{1}(x_{2}-1)$:\n$$\n\\dot{V}(x) = \\left( 2x_{1}(x_{2}-1)^{2} \\right) \\left( x_{1}(x_{2}-1) \\right) - x_{2}\\left( 2x_{1}^{2}(x_{2}-1) + 2x_{2} \\right)\n$$\n$$\n\\dot{V}(x) = 2x_{1}^{2}(x_{2}-1)^{3} - 2x_{1}^{2}x_{2}(x_{2}-1) - 2x_{2}^{2}\n$$\nTo obtain the final simplified analytic expression, we expand the polynomial terms:\nThe term $(x_{2}-1)^{3}$ expands to $x_{2}^{3} - 3x_{2}^{2} + 3x_{2} - 1$.\nThe term $x_{2}(x_{2}-1)$ expands to $x_{2}^{2}-x_{2}$.\nSubstituting these expansions:\n$$\n\\dot{V}(x) = 2x_{1}^{2}(x_{2}^{3} - 3x_{2}^{2} + 3x_{2} - 1) - 2x_{1}^{2}(x_{2}^{2}-x_{2}) - 2x_{2}^{2}\n$$\nDistribute the factors $2x_{1}^{2}$ and $-2x_{1}^{2}$:\n$$\n\\dot{V}(x) = (2x_{1}^{2}x_{2}^{3} - 6x_{1}^{2}x_{2}^{2} + 6x_{1}^{2}x_{2} - 2x_{1}^{2}) - (2x_{1}^{2}x_{2}^{2}-2x_{1}^{2}x_{2}) - 2x_{2}^{2}\n$$\n$$\n\\dot{V}(x) = 2x_{1}^{2}x_{2}^{3} - 6x_{1}^{2}x_{2}^{2} + 6x_{1}^{2}x_{2} - 2x_{1}^{2} - 2x_{1}^{2}x_{2}^{2} + 2x_{1}^{2}x_{2} - 2x_{2}^{2}\n$$\nFinally, collect like terms to arrive at the simplified expression:\n$$\n\\dot{V}(x) = 2x_{1}^{2}x_{2}^{3} + (-6-2)x_{1}^{2}x_{2}^{2} + (6+2)x_{1}^{2}x_{2} - 2x_{1}^{2} - 2x_{2}^{2}\n$$\n$$\n\\dot{V}(x) = 2x_{1}^{2}x_{2}^{3} - 8x_{1}^{2}x_{2}^{2} + 8x_{1}^{2}x_{2} - 2x_{1}^{2} - 2x_{2}^{2}\n$$\nThis is the final, simplified expression for the orbital derivative $\\dot{V}(x)$.", "answer": "$$\n\\boxed{2x_{1}^{2}x_{2}^{3} - 8x_{1}^{2}x_{2}^{2} + 8x_{1}^{2}x_{2} - 2x_{1}^{2} - 2x_{2}^{2}}\n$$", "id": "2715988"}, {"introduction": "Moving beyond direct computation, this practice delves into a more powerful and general aspect of the Krasovskii method. Here, we analyze stability by examining the properties of the system's Jacobian matrix $J(x)$, encapsulated in the symmetric matrix $S(x) = J(x)^{\\top} + J(x)$. This exercise challenges you to use matrix analysis to prove stability conditions, offering deeper insight into how the local linearization of the system governs its behavior [@problem_id:2716042].", "problem": "Consider the autonomous ordinary differential equation (ODE) in $\\mathbb{R}^{2}$ given by\n$$\n\\dot{x} = f(x) \\triangleq \\begin{bmatrix} -x_{1} + \\sin(x_{2}) \\\\ -x_{2} \\end{bmatrix}, \\quad x \\in \\mathbb{R}^{2}.\n$$\nKrasovskii's method constructs a Lyapunov candidate of the form $V(x) = \\dot{x}^{\\top} P \\dot{x}$ with a symmetric positive definite matrix $P \\in \\mathbb{R}^{2 \\times 2}$. Let $P = I$, where $I$ is the identity matrix. Starting from first principles (the chain rule and the definition of the Jacobian matrix), compute $\\dot{V}(x)$ and express it in the quadratic form\n$$\n\\dot{V}(x) = \\dot{x}^{\\top} S(x) \\dot{x},\n$$\nwhere $S(x)$ is a symmetric matrix depending on $x$. Then, on the closed Euclidean ball $B_{\\rho} \\triangleq \\{ x \\in \\mathbb{R}^{2} : \\|x\\| \\le \\rho \\}$ with any given radius $\\rho > 0$, determine whether there exists a constant $\\alpha > 0$ such that the matrix inequality $S(x) \\preceq -\\alpha I$ holds for all $x \\in B_{\\rho}$, and if so, find the largest such $\\alpha$ (independent of $\\rho$). Provide your final answer as a single exact number (no units).", "solution": "The problem is well-posed, scientifically grounded, and objective. It is a standard exercise in control theory involving the application of Krasovskii's method for constructing a Lyapunov function to analyze the stability of a nonlinear system. All necessary data and definitions are provided, and no contradictions or ambiguities are present. Therefore, the problem is valid, and a solution will be constructed.\n\nThe autonomous system is described by the ordinary differential equation $\\dot{x} = f(x)$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2$ and the vector field is given by\n$$ f(x) = \\begin{bmatrix} -x_{1} + \\sin(x_{2}) \\\\ -x_{2} \\end{bmatrix}. $$\nWe are asked to use Krasovskii's method with the Lyapunov candidate function $V(x) = \\dot{x}^{\\top} P \\dot{x}$. Substituting $\\dot{x} = f(x)$ and $P = I$ (the $2 \\times 2$ identity matrix), the function becomes\n$$ V(x) = f(x)^{\\top} I f(x) = f(x)^{\\top} f(x). $$\nTo compute the time derivative of $V(x)$ along the trajectories of the system, $\\dot{V}(x)$, we apply the chain rule. Let $x(t)$ be a solution to $\\dot{x} = f(x)$. Then $V(x(t)) = f(x(t))^{\\top} f(x(t))$. Differentiating with respect to time $t$ gives\n$$ \\dot{V}(x) = \\frac{d}{dt} \\left( f(x(t))^{\\top} f(x(t)) \\right) = \\left(\\frac{d}{dt} f(x(t))\\right)^{\\top} f(x(t)) + f(x(t))^{\\top} \\left(\\frac{d}{dt} f(x(t))\\right). $$\nThe term $\\frac{d}{dt} f(x(t))$ is found by applying the multivariable chain rule:\n$$ \\frac{d}{dt} f(x(t)) = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} = J(x) \\dot{x} = J(x) f(x), $$\nwhere $J(x)$ is the Jacobian matrix of $f(x)$ with respect to $x$. Substituting this back into the expression for $\\dot{V}(x)$:\n$$ \\dot{V}(x) = (J(x)f(x))^{\\top} f(x) + f(x)^{\\top} (J(x)f(x)) = f(x)^{\\top} J(x)^{\\top} f(x) + f(x)^{\\top} J(x) f(x). $$\nThis can be factored into the quadratic form\n$$ \\dot{V}(x) = f(x)^{\\top} \\left( J(x)^{\\top} + J(x) \\right) f(x). $$\nSince $\\dot{x} = f(x)$, we have\n$$ \\dot{V}(x) = \\dot{x}^{\\top} \\left( J(x)^{\\top} + J(x) \\right) \\dot{x}. $$\nThe problem asks for this to be in the form $\\dot{V}(x) = \\dot{x}^{\\top} S(x) \\dot{x}$, where $S(x)$ is a symmetric matrix. We can identify $S(x) = J(x)^{\\top} + J(x)$. For any matrix $J(x)$, the matrix $J(x)^{\\top} + J(x)$ is symmetric, as $(J(x)^{\\top} + J(x))^{\\top} = (J(x)^{\\top})^{\\top} + J(x)^{\\top} = J(x) + J(x)^{\\top}$.\n\nNext, we compute the Jacobian matrix $J(x)$ for the given $f(x)$:\n$$ J(x) = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}(-x_1 + \\sin(x_2)) & \\frac{\\partial}{\\partial x_2}(-x_1 + \\sin(x_2)) \\\\ \\frac{\\partial}{\\partial x_1}(-x_2) & \\frac{\\partial}{\\partial x_2}(-x_2) \\end{bmatrix} = \\begin{bmatrix} -1 & \\cos(x_{2}) \\\\ 0 & -1 \\end{bmatrix}. $$\nNow we construct the matrix $S(x)$:\n$$ S(x) = J(x)^{\\top} + J(x) = \\begin{bmatrix} -1 & 0 \\\\ \\cos(x_{2}) & -1 \\end{bmatrix} + \\begin{bmatrix} -1 & \\cos(x_{2}) \\\\ 0 & -1 \\end{bmatrix} = \\begin{bmatrix} -2 & \\cos(x_{2}) \\\\ \\cos(x_{2}) & -2 \\end{bmatrix}. $$\nThe problem requires us to determine if there exists a constant $\\alpha > 0$ such that the matrix inequality $S(x) \\preceq -\\alpha I$ holds for all $x$ in any closed ball $B_{\\rho} = \\{ x \\in \\mathbb{R}^{2} : \\|x\\| \\le \\rho \\}$ with radius $\\rho > 0$. Since this must hold for any $\\rho>0$, it must hold for all $x \\in \\mathbb{R}^2$. The inequality $S(x) \\preceq -\\alpha I$ is equivalent to $S(x) + \\alpha I \\preceq 0$, which means the matrix $S(x) + \\alpha I$ must be negative semi-definite.\nLet's analyze the matrix $M(x, \\alpha) = S(x) + \\alpha I$:\n$$ M(x, \\alpha) = \\begin{bmatrix} -2 & \\cos(x_{2}) \\\\ \\cos(x_{2}) & -2 \\end{bmatrix} + \\begin{bmatrix} \\alpha & 0 \\\\ 0 & \\alpha \\end{bmatrix} = \\begin{bmatrix} -2 + \\alpha & \\cos(x_{2}) \\\\ \\cos(x_{2}) & -2 + \\alpha \\end{bmatrix}. $$\nA symmetric matrix is negative semi-definite if and only if all its eigenvalues are less than or equal to zero. The eigenvalues $\\lambda$ of $M(x, \\alpha)$ are the roots of the characteristic equation $\\det(M(x, \\alpha) - \\lambda I) = 0$:\n$$ \\det\\left(\\begin{bmatrix} -2 + \\alpha - \\lambda & \\cos(x_{2}) \\\\ \\cos(x_{2}) & -2 + \\alpha - \\lambda \\end{bmatrix}\\right) = 0 $$\n$$ (-2 + \\alpha - \\lambda)^2 - \\cos^2(x_2) = 0 $$\n$$ -2 + \\alpha - \\lambda = \\pm \\cos(x_2) $$\n$$ \\lambda = -2 + \\alpha \\mp \\cos(x_2). $$\nThe two eigenvalues are $\\lambda_1 = -2 + \\alpha - \\cos(x_2)$ and $\\lambda_2 = -2 + \\alpha + \\cos(x_2)$. For $M(x, \\alpha)$ to be negative semi-definite, we need $\\lambda_1 \\le 0$ and $\\lambda_2 \\le 0$ for all $x \\in \\mathbb{R}^2$. Since $S(x)$ only depends on $x_2$, this condition must hold for all $x_2 \\in \\mathbb{R}$.\n\n1.  Condition on $\\lambda_1$: $-2 + \\alpha - \\cos(x_2) \\le 0 \\implies \\alpha \\le 2 + \\cos(x_2)$.\n    This inequality must hold for all values of $x_2$. To ensure this, we must satisfy the most restrictive case, which occurs when $\\cos(x_2)$ is at its minimum value, $-1$. This gives $\\alpha \\le 2 + (-1)$, so $\\alpha \\le 1$.\n\n2.  Condition on $\\lambda_2$: $-2 + \\alpha + \\cos(x_2) \\le 0 \\implies \\alpha \\le 2 - \\cos(x_2)$.\n    This inequality must also hold for all values of $x_2$. The most restrictive case occurs when $\\cos(x_2)$ is at its maximum value, $1$. This gives $\\alpha \\le 2 - 1$, so $\\alpha \\le 1$.\n\nBoth conditions require that $\\alpha \\le 1$. The problem asks for the existence of an $\\alpha > 0$. Any value of $\\alpha$ in the interval $(0, 1]$ satisfies the conditions. Thus, such a positive constant $\\alpha$ exists. The problem then asks for the largest such $\\alpha$, which is the supremum of the set $(0, 1]$.\n\nThe largest possible value for $\\alpha$ is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2716042"}, {"introduction": "Real-world analysis often presents scenarios that don't fit perfectly into textbook theorems. This problem explores a crucial situation where the time derivative of the Lyapunov function, $\\dot{V}(x)$, is only negative semi-definite, meaning we can only conclude that trajectories do not diverge. To determine the system's final asymptotic behavior, we must pair the Krasovskii method with the powerful LaSalle's invariance principle, demonstrating how to reach precise conclusions even when the conditions for asymptotic stability are not strictly met [@problem_id:2715968].", "problem": "Consider the autonomous nonlinear system in two dimensions defined by\n$$\n\\dot{x} = f(x), \\quad x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad f(x) = \\begin{pmatrix} -x_{1} - x_{1}^{3} \\\\ 0 \\end{pmatrix}.\n$$\nThis system has an equilibrium at the origin. You will analyze its stability using Krasovskii's method and the LaSalle invariance principle. Assume a constant symmetric positive definite matrix $P = I_{2}$ in the construction.\n\nTasks:\n- Construct a Lyapunov candidate using Krasovskii's method with the given $P$, and compute the associated symmetric part\n$$\nS(x) = \\left(\\frac{\\partial f}{\\partial x}(x)\\right)^{\\top} P + P \\frac{\\partial f}{\\partial x}(x).\n$$\n- Prove that $S(x) \\preceq 0$ for all $x$ and identify the manifold on which the time derivative $\\dot{V}(x)$ of your constructed Lyapunov function is equal to zero.\n- Using the LaSalle invariance principle, determine the asymptotic behavior of solutions that start at an arbitrary initial condition $x(0) = \\begin{pmatrix} x_{1}(0) \\\\ x_{2}(0) \\end{pmatrix}$.\n- Provide the limit $\\lim_{t \\to \\infty} x(t)$ as a row matrix whose entries are analytical expressions in terms of $x_{1}(0)$ and $x_{2}(0)$. No rounding is required, and no physical units are involved.", "solution": "The problem statement is scientifically grounded, self-contained, and well-posed. It presents a standard problem in nonlinear control theory, providing all necessary information for a complete analysis. No flaws are detected. We proceed with the solution.\n\nThe autonomous nonlinear system is given by $\\dot{x} = f(x)$, where $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ and $f(x) = \\begin{pmatrix} -x_{1} - x_{1}^{3} \\\\ 0 \\end{pmatrix}$. The system possesses a continuum of equilibrium points defined by $f(x) = 0$, which occurs when $-x_{1}(1+x_{1}^{2}) = 0$. This condition is satisfied if and only if $x_{1}=0$. Thus, the set of equilibria is the entire $x_{2}$-axis.\n\nFirst, we construct the Lyapunov function candidate using Krasovskii's method. The method suggests a function of the form $V(x) = f(x)^{\\top} P f(x)$, where $P$ is a symmetric positive definite matrix. The problem specifies $P = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe Lyapunov candidate is therefore:\n$$\nV(x) = \\begin{pmatrix} -x_{1} - x_{1}^{3} & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -x_{1} - x_{1}^{3} \\\\ 0 \\end{pmatrix} = (-x_{1} - x_{1}^{3})^{2} = (x_{1}(1+x_{1}^{2}))^{2}\n$$\nThis function $V(x)$ is positive semi-definite. It is zero if and only if $x_{1}=0$, which is the set of equilibria.\n\nNext, we compute the symmetric matrix $S(x) = \\left(\\frac{\\partial f}{\\partial x}(x)\\right)^{\\top} P + P \\frac{\\partial f}{\\partial x}(x)$. First, we find the Jacobian matrix $J(x) = \\frac{\\partial f}{\\partial x}(x)$:\n$$\nf(x) = \\begin{pmatrix} f_{1}(x_{1}, x_{2}) \\\\ f_{2}(x_{1}, x_{2}) \\end{pmatrix} = \\begin{pmatrix} -x_{1} - x_{1}^{3} \\\\ 0 \\end{pmatrix}\n$$\n$$\nJ(x) = \\begin{pmatrix} \\frac{\\partial f_{1}}{\\partial x_{1}} & \\frac{\\partial f_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial f_{2}}{\\partial x_{1}} & \\frac{\\partial f_{2}}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} -1 - 3x_{1}^{2} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nWith $P = I_{2}$, the matrix $S(x)$ simplifies to $S(x) = J(x)^{\\top} + J(x)$.\n$$\nS(x) = \\begin{pmatrix} -1 - 3x_{1}^{2} & 0 \\\\ 0 & 0 \\end{pmatrix}^{\\top} + \\begin{pmatrix} -1 - 3x_{1}^{2} & 0 \\\\ 0 & 0 \\end{pmatrix} = 2 \\begin{pmatrix} -1 - 3x_{1}^{2} & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -2(1 + 3x_{1}^{2}) & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nTo prove that $S(x)$ is negative semi-definite ($S(x) \\preceq 0$), we examine its eigenvalues. As $S(x)$ is a diagonal matrix, its eigenvalues are its diagonal entries: $\\lambda_{1} = -2(1 + 3x_{1}^{2})$ and $\\lambda_{2} = 0$. Since $x_{1}^{2} \\ge 0$, we have $1 + 3x_{1}^{2} \\ge 1$, which implies $\\lambda_{1} \\le -2$. As both eigenvalues are non-positive ($\\lambda_1 \\le 0$ and $\\lambda_2 = 0$) for all $x \\in \\mathbb{R}^{2}$, the matrix $S(x)$ is indeed negative semi-definite.\n\nThe time derivative of the Lyapunov function is given by $\\dot{V}(x) = f(x)^{\\top} S(x) f(x)$.\n$$\n\\dot{V}(x) = \\begin{pmatrix} -x_{1} - x_{1}^{3} & 0 \\end{pmatrix} \\begin{pmatrix} -2(1 + 3x_{1}^{2}) & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} -x_{1} - x_{1}^{3} \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\dot{V}(x) = (-x_{1} - x_{1}^{3})(-2(1 + 3x_{1}^{2}))(-x_{1} - x_{1}^{3}) = -2(1 + 3x_{1}^{2})(-x_{1}(1+x_{1}^{2}))^{2}\n$$\n$$\n\\dot{V}(x) = -2(1 + 3x_{1}^{2})x_{1}^{2}(1+x_{1}^{2})^{2}\n$$\nSince $1+3x_{1}^{2} > 0$ and $(1+x_{1}^{2})^{2} > 0$, we have $\\dot{V}(x) \\le 0$ for all $x$. The equality $\\dot{V}(x) = 0$ holds if and only if $x_{1}^{2}=0$, which means $x_{1}=0$. The manifold on which $\\dot{V}(x)$ is zero is the set $E = \\{x \\in \\mathbb{R}^{2} \\mid x_{1}=0\\}$, which is the $x_{2}$-axis.\n\nSince $\\dot{V}(x)$ is only negative semi-definite, we apply the LaSalle invariance principle to determine the asymptotic behavior. The principle states that all trajectories converge to the largest invariant set $M$ contained in $E$. A set is invariant if any trajectory starting in it remains in it for all future time.\nLet us examine the dynamics for any point in $E$, i.e., for any point $x$ with $x_{1}=0$. The system dynamics are:\n$$\n\\dot{x}_{1} = -x_{1} - x_{1}^{3} \\implies \\dot{x}_{1} = -0 - 0^{3} = 0\n$$\n$$\n\\dot{x}_{2} = 0\n$$\nThis shows that if a trajectory is at a point $(0, c)$ on the $x_{2}$-axis, its velocity vector is $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The trajectory remains at $(0, c)$ for all time. Therefore, every point on the $x_{2}$-axis is an equilibrium point. Consequently, any subset of $E$ is an invariant set, and the largest invariant set $M$ contained in $E$ is $E$ itself.\nAccording to LaSalle's principle, any solution $x(t)$ must approach the set $M=E$ as $t \\to \\infty$. This implies that the distance from the point $x(t)=(x_{1}(t), x_{2}(t))$ to the set $E$ (the $x_{2}$-axis) must go to zero. This distance is $|x_{1}(t)|$. Thus, we conclude that $\\lim_{t \\to \\infty} x_{1}(t) = 0$.\n\nHowever, this only describes the behavior of the first component. To find the limit of the entire state vector, we must analyze the dynamics of the second component, $x_{2}$. The system equation for $x_{2}$ is $\\dot{x}_{2}(t) = 0$. Integrating this equation with respect to time from $t=0$ to an arbitrary $t$ yields:\n$$\n\\int_{0}^{t} \\dot{x}_{2}(\\tau) d\\tau = \\int_{0}^{t} 0 d\\tau \\implies x_{2}(t) - x_{2}(0) = 0\n$$\nThis means $x_{2}(t) = x_{2}(0)$ for all $t \\ge 0$. The value of $x_{2}$ remains constant throughout the evolution of the system. Therefore, its limit as $t \\to \\infty$ is simply its initial value:\n$$\n\\lim_{t \\to \\infty} x_{2}(t) = x_{2}(0)\n$$\nCombining the limits for both components, the asymptotic behavior of the solution $x(t)$ is convergence to a specific equilibrium point on the $x_{2}$-axis:\n$$\n\\lim_{t \\to \\infty} x(t) = \\begin{pmatrix} \\lim_{t \\to \\infty} x_{1}(t) \\\\ \\lim_{t \\to \\infty} x_{2}(t) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ x_{2}(0) \\end{pmatrix}\n$$\nThe problem requests the answer as a row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & x_{2}(0) \\end{pmatrix}}\n$$", "id": "2715968"}]}