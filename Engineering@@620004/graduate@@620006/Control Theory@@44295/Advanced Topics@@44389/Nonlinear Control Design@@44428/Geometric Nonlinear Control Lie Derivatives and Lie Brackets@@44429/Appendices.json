{"hands_on_practices": [{"introduction": "The Lie bracket is a central operator in geometric control, measuring the extent to which the flows of two vector fields fail to commute. This exercise provides foundational practice in computing the Lie bracket directly from its Jacobian-based definition. By explicitly calculating both $[f,g]$ and $[g,f]$ and confirming the antisymmetry property $[f,g] = -[g,f]$, you will solidify your understanding of this crucial algebraic structure.", "problem": "Consider smooth, time-invariant vector fields on $\\mathbb{R}^{3}$ written in the standard coordinate basis. Let $f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}$ and $g:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}$ be given by $f(x)=\\big(x_{2},0,0\\big)$ and $g(x)=\\big(0,x_{3},0\\big)$ for $x=\\big(x_{1},x_{2},x_{3}\\big)$. The Lie bracket of two smooth vector fields is defined pointwise by the difference of directional derivatives (equivalently, the commutator of Lie derivatives), namely\n$$\n[f,g](x)\\coloneqq Dg(x)\\,f(x)\\;-\\;Df(x)\\,g(x),\n$$\nwhere $Df(x)$ and $Dg(x)$ denote the Jacobian matrices of $f$ and $g$ at $x$, respectively. Starting from this definition and the coordinate expression of the Jacobian, compute $[f,g](x)$ and $[g,f](x)$ explicitly at a generic point $x\\in\\mathbb{R}^{3}$ without invoking any pre-known symmetry properties. Then verify antisymmetry directly by showing that $[f,g](x)+[g,f](x)=0$ holds for all $x\\in\\mathbb{R}^{3}$.\n\nProvide your final answer as a single row matrix containing the components of $[f,g](x)$ followed by those of $[g,f](x)$, in that order. No numerical approximation is required, and no units are involved.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique and verifiable solution. The definitions and objects are standard in the field of differential geometry and control theory. We shall proceed with the derivation.\n\nThe state vector is $x = (x_{1}, x_{2}, x_{3}) \\in \\mathbb{R}^{3}$. The vector fields $f: \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$ and $g: \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$ are provided as:\n$$ f(x) = \\begin{pmatrix} x_{2} \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad g(x) = \\begin{pmatrix} 0 \\\\ x_{3} \\\\ 0 \\end{pmatrix} $$\nThe Lie bracket is defined by $[f,g](x) = Dg(x)f(x) - Df(x)g(x)$, where $Df(x)$ and $Dg(x)$ are the Jacobian matrices of $f$ and $g$ respectively.\n\nFirst, we compute the Jacobian matrices.\nThe components of $f(x)$ are $f_{1}(x) = x_{2}$, $f_{2}(x) = 0$, and $f_{3}(x) = 0$. The Jacobian matrix $Df(x)$ is:\n$$ Df(x) = \\frac{\\partial f}{\\partial x} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_1}{\\partial x_3} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_3} \\\\ \\frac{\\partial f_3}{\\partial x_1} & \\frac{\\partial f_3}{\\partial x_2} & \\frac{\\partial f_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(x_{2}) & \\frac{\\partial}{\\partial x_2}(x_{2}) & \\frac{\\partial}{\\partial x_3}(x_{2}) \\\\ \\frac{\\partial}{\\partial x_1}(0) & \\frac{\\partial}{\\partial x_2}(0) & \\frac{\\partial}{\\partial x_3}(0) \\\\ \\frac{\\partial}{\\partial x_1}(0) & \\frac{\\partial}{\\partial x_2}(0) & \\frac{\\partial}{\\partial x_3}(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nThe components of $g(x)$ are $g_{1}(x) = 0$, $g_{2}(x) = x_{3}$, and $g_{3}(x) = 0$. The Jacobian matrix $Dg(x)$ is:\n$$ Dg(x) = \\frac{\\partial g}{\\partial x} = \\begin{pmatrix} \\frac{\\partial g_1}{\\partial x_1} & \\frac{\\partial g_1}{\\partial x_2} & \\frac{\\partial g_1}{\\partial x_3} \\\\ \\frac{\\partial g_2}{\\partial x_1} & \\frac{\\partial g_2}{\\partial x_2} & \\frac{\\partial g_2}{\\partial x_3} \\\\ \\frac{\\partial g_3}{\\partial x_1} & \\frac{\\partial g_3}{\\partial x_2} & \\frac{\\partial g_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(0) & \\frac{\\partial}{\\partial x_2}(0) & \\frac{\\partial}{\\partial x_3}(0) \\\\ \\frac{\\partial}{\\partial x_1}(x_{3}) & \\frac{\\partial}{\\partial x_2}(x_{3}) & \\frac{\\partial}{\\partial x_3}(x_{3}) \\\\ \\frac{\\partial}{\\partial x_1}(0) & \\frac{\\partial}{\\partial x_2}(0) & \\frac{\\partial}{\\partial x_3}(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nWith the Jacobians computed, we can evaluate the Lie bracket $[f,g](x)$. We compute each term separately.\n$$ Dg(x)f(x) = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n$$ Df(x)g(x) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ x_{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nSubtracting the second term from the first yields the Lie bracket $[f,g](x)$:\n$$ [f,g](x) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNext, we compute $[g,f](x)$ explicitly, as required. The definition is $[g,f](x) = Df(x)g(x) - Dg(x)f(x)$. The terms have already been computed above.\n$$ [g,f](x) = Df(x)g(x) - Dg(x)f(x) = \\begin{pmatrix} x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nFinally, we verify the antisymmetry property by direct computation:\n$$ [f,g](x) + [g,f](x) = \\begin{pmatrix} -x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} x_{3} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -x_{3} + x_{3} \\\\ 0+0 \\\\ 0+0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe sum is the zero vector for any $x \\in \\mathbb{R}^{3}$, which confirms the antisymmetry property $[f,g] = -[g,f]$.\n\nThe components of $[f,g](x)$ are $(-x_{3}, 0, 0)$ and the components of $[g,f](x)$ are $(x_{3}, 0, 0)$. These are concatenated to form the final answer.", "answer": "$$\n\\boxed{\\begin{pmatrix} -x_{3} & 0 & 0 & x_{3} & 0 & 0 \\end{pmatrix}}\n$$", "id": "2710247"}, {"introduction": "Lie derivatives provide the essential language for analyzing the input-output behavior of nonlinear systems. This practice problem moves from abstract calculation to concrete system analysis, demonstrating how to determine the effect of a control input $u$ on an output $y$. You will see how the term $L_{g}L_{f} h(x)$ reveals a \"hidden\" connection, defining the system's relative degree and playing a key role in assessing observability.", "problem": "Consider the control-affine system with state $x \\in \\mathbb{R}^{2}$, input $u \\in \\mathbb{R}$, and output $y \\in \\mathbb{R}$ given by\n$$\n\\dot{x} = f(x) + g(x)\\,u, \\quad y = h(x),\n$$\nwhere\n$$\nf(x) = \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix}, \\quad g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad h(x) = x_{1}^{2}.\n$$\nUsing only the core definitions of Lie derivatives for smooth vector fields and outputs, perform the following tasks:\n- Compute $L_{g} h(x)$ and $L_{f} h(x)$ directly from their definitions.\n- Compute $L_{g}L_{f} h(x)$ and verify that $L_{g} h(x) \\equiv 0$ but $L_{g} L_{f} h(x) \\not\\equiv 0$, thereby showing that the input $u$ affects the output $y$ at a higher derivative order.\n- By differentiating the output along trajectories, derive $\\dot{y}$ and $\\ddot{y}$ in terms of Lie derivatives and the input $u$, and identify where $u$ first appears.\n- Analyze local weak observability using the Hermann–Krener local observability rank condition by constructing the observability codistribution generated by $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$, and determine for which $x$ the rank is full.\n\nProvide the exact analytic expression for $L_{g} L_{f} h(x)$ as your final answer. No rounding is required. There are no physical units involved. Angles are not present in this problem.", "solution": "The posed problem is subject to rigorous validation.\n\n**Step 1: Extract Givens**\nThe control-affine system is defined by:\n- State dynamics: $\\dot{x} = f(x) + g(x)\\,u$, where $x \\in \\mathbb{R}^{2}$ and $u \\in \\mathbb{R}$.\n- Output equation: $y = h(x)$, where $y \\in \\mathbb{R}$.\n- Drift vector field: $f(x) = \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix}$.\n- Control vector field: $g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n- Output function: $h(x) = x_{1}^{2}$.\n\nThe tasks are:\n1.  Compute $L_{g} h(x)$ and $L_{f} h(x)$ using their definitions.\n2.  Compute $L_{g}L_{f} h(x)$ and verify that $L_{g} h(x) \\equiv 0$ but $L_{g} L_{f} h(x) \\not\\equiv 0$.\n3.  Derive the time derivatives $\\dot{y}$ and $\\ddot{y}$ and identify where the input $u$ first appears.\n4.  Analyze local weak observability using the Hermann–Krener rank condition on the codistribution generated by $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in nonlinear control theory. The system, functions, and tasks are well-defined and mathematically sound. The vector fields $f(x)$ and $g(x)$, and the output function $h(x)$, are smooth (infinitely differentiable) functions on $\\mathbb{R}^{2}$. The concepts of Lie derivatives and observability are central to the specified topic. The problem is self-contained, scientifically grounded, and objective. It does not violate any of the specified invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe state vector is $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$. The output function is $h(x) = x_{1}^{2}$. The Lie derivative of a scalar function $\\phi(x)$ with respect to a vector field $v(x)$ is defined as $L_{v}\\phi(x) = \\frac{\\partial \\phi}{\\partial x} v(x) = \\nabla \\phi(x) \\cdot v(x)$.\n\nFirst, we must compute the gradient of the output function $h(x)$:\n$$\n\\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_{1}} & \\frac{\\partial h}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} & 0 \\end{pmatrix}\n$$\n\n**Task 1: Computation of $L_{g} h(x)$ and $L_{f} h(x)$**\n\nUsing the definition of the Lie derivative, we compute $L_{f} h(x)$:\n$$\nL_{f} h(x) = \\nabla h(x) \\cdot f(x) = \\begin{pmatrix} 2x_{1} & 0 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = (2x_{1})(x_{2}) + (0)(0) = 2x_{1}x_{2}\n$$\nNext, we compute $L_{g} h(x)$:\n$$\nL_{g} h(x) = \\nabla h(x) \\cdot g(x) = \\begin{pmatrix} 2x_{1} & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (2x_{1})(0) + (0)(1) = 0\n$$\n\n**Task 2: Computation of $L_{g}L_{f} h(x)$ and Verification**\n\nTo compute $L_{g}L_{f} h(x)$, we treat $L_{f}h(x)$ as a new scalar function, which we will call $\\psi(x) = L_{f}h(x) = 2x_{1}x_{2}$. We must find the Lie derivative of $\\psi(x)$ with respect to $g(x)$. First, we compute the gradient of $\\psi(x)$:\n$$\n\\nabla \\psi(x) = \\nabla (L_{f}h(x)) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_{1}}(2x_{1}x_{2}) & \\frac{\\partial}{\\partial x_{2}}(2x_{1}x_{2}) \\end{pmatrix} = \\begin{pmatrix} 2x_{2} & 2x_{1} \\end{pmatrix}\n$$\nNow, we compute $L_{g}L_{f} h(x) = L_{g}\\psi(x)$:\n$$\nL_{g}L_{f} h(x) = \\nabla(L_{f}h(x)) \\cdot g(x) = \\begin{pmatrix} 2x_{2} & 2x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (2x_{2})(0) + (2x_{1})(1) = 2x_{1}\n$$\nWe have shown that $L_{g} h(x) = 0$ for all $x \\in \\mathbb{R}^{2}$, which is an identity. We have also shown that $L_{g}L_{f} h(x) = 2x_{1}$, which is not identically zero (it is only zero on the subspace where $x_{1}=0$). This demonstrates that the input $u$ does not have an instantaneous effect on the output $y$, but it does affect a higher-order derivative of $y$.\n\n**Task 3: Derivation of Output Derivatives**\n\nThe first time derivative of the output $y = h(x(t))$ is given by the chain rule:\n$$\n\\dot{y} = \\frac{d}{dt}h(x(t)) = \\frac{\\partial h}{\\partial x} \\frac{dx}{dt} = \\nabla h(x) \\cdot \\dot{x}\n$$\nSubstituting the system dynamics $\\dot{x} = f(x) + g(x)u$:\n$$\n\\dot{y} = \\nabla h(x) \\cdot (f(x) + g(x)u) = \\nabla h(x) \\cdot f(x) + (\\nabla h(x) \\cdot g(x))u\n$$\nUsing the Lie derivative notation:\n$$\n\\dot{y} = L_{f} h(x) + (L_{g} h(x))u\n$$\nFrom our previous calculations, $L_{f} h(x) = 2x_{1}x_{2}$ and $L_{g} h(x) = 0$. Thus:\n$$\n\\dot{y} = 2x_{1}x_{2}\n$$\nThe input $u$ does not appear in the expression for $\\dot{y}$. We proceed to the second derivative, $\\ddot{y}$:\n$$\n\\ddot{y} = \\frac{d}{dt} \\dot{y} = \\frac{d}{dt} (L_{f} h(x(t))) = \\frac{\\partial (L_{f} h(x))}{\\partial x} \\frac{dx}{dt} = \\nabla(L_{f}h(x)) \\cdot \\dot{x}\n$$\nAgain, substituting the dynamics $\\dot{x} = f(x) + g(x)u$:\n$$\n\\ddot{y} = \\nabla(L_{f}h(x)) \\cdot (f(x) + g(x)u) = \\nabla(L_{f}h(x)) \\cdot f(x) + (\\nabla(L_{f}h(x)) \\cdot g(x))u\n$$\nThis is precisely the definition of iterated Lie derivatives:\n$$\n\\ddot{y} = L_{f}^{2}h(x) + (L_{g}L_{f}h(x))u\n$$\nWe need to compute $L_{f}^{2}h(x) = L_{f}(L_{f}h(x))$. We already have $\\nabla(L_{f}h(x)) = \\begin{pmatrix} 2x_{2} & 2x_{1} \\end{pmatrix}$.\n$$\nL_{f}^{2}h(x) = \\nabla(L_{f}h(x)) \\cdot f(x) = \\begin{pmatrix} 2x_{2} & 2x_{1} \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = 2x_{2}^{2}\n$$\nSubstituting this and our previous result for $L_{g}L_{f}h(x)$ into the expression for $\\ddot{y}$:\n$$\n\\ddot{y} = 2x_{2}^{2} + (2x_{1})u\n$$\nThe input $u$ first appears in the second time derivative of the output, $\\ddot{y}$. This implies the system has a relative degree of $r=2$ at all points where $L_{g}L_{f}h(x) = 2x_{1} \\neq 0$.\n\n**Task 4: Local Weak Observability Analysis**\n\nThe Hermann–Krener observability rank condition states that the system is locally weakly observable at a point $x$ if the rank of the observability matrix, formed by the gradients of successive Lie derivatives of $h(x)$ along the system's vector fields, is equal to the dimension of the state space, which is $n=2$. The problem specifies constructing the codistribution from the covectors $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$. In this context, $\\mathrm{d} \\phi(x)$ is the gradient $\\nabla \\phi(x)$.\n\nThe covectors are:\n- $\\mathrm{d} h(x) = \\nabla h(x) = \\begin{pmatrix} 2x_{1} & 0 \\end{pmatrix}$\n- $\\mathrm{d} L_{f} h(x) = \\nabla(L_{f}h(x)) = \\begin{pmatrix} 2x_{2} & 2x_{1} \\end{pmatrix}$\n\nWe form the observability matrix $M_{O}(x)$ by stacking these covectors:\n$$\nM_{O}(x) = \\begin{pmatrix} \\mathrm{d} h(x) \\\\ \\mathrm{d} L_{f} h(x) \\end{pmatrix} = \\begin{pmatrix} 2x_{1} & 0 \\\\ 2x_{2} & 2x_{1} \\end{pmatrix}\n$$\nThe rank of this $2 \\times 2$ matrix is full (equal to $2$) if and only if its determinant is non-zero.\n$$\n\\det(M_{O}(x)) = (2x_{1})(2x_{1}) - (0)(2x_{2}) = 4x_{1}^{2}\n$$\nFor the rank to be full, we require $\\det(M_{O}(x)) \\neq 0$, which implies $4x_{1}^{2} \\neq 0$. This condition holds if and only if $x_{1} \\neq 0$.\n\nTherefore, the system is locally weakly observable for all $x \\in \\mathbb{R}^{2}$ such that $x_{1} \\neq 0$. The observability is lost on the set of points where $x_{1} = 0$, which is the $x_{2}$-axis. This is logical, as for any state on this axis, $x=(0, x_2)$, the output is $y=h(x)=0^2=0$ and its first derivative is $\\dot{y}=L_f h(x) = 2(0)x_2=0$, making it impossible to distinguish between different values of $x_2$ from the output and its derivatives.", "answer": "$$\n\\boxed{2x_{1}}\n$$", "id": "2710283"}, {"introduction": "The true power of the Lie bracket in control theory lies in its geometric interpretation: it generates new directions of motion. This exercise demonstrates how iterated Lie brackets can span dimensions of the state space that are not directly accessible through the primary control vector fields. By checking the Lie Algebra Rank Condition (LARC), you will gain hands-on experience with a fundamental tool for determining the controllability of nonlinear systems.", "problem": "Consider the control-affine system on $\\mathbb{R}^{4}$ with state $x=(x_{1},x_{2},x_{3},x_{4})$ and control vector fields $f_{1}$, $f_{2}$, $f_{3}$ given in standard coordinates by $f_{1}(x)=(1,0,0,0)$, $f_{2}(x)=(0,1,0,0)$, and $f_{3}(x)=(0,0,x_{2},1)$. Let the dynamics be $\\dot{x}=u_{1}f_{1}(x)+u_{2}f_{2}(x)+u_{3}f_{3}(x)$, where $u_{1},u_{2},u_{3}$ are measurable and locally bounded inputs. Use the following fundamental definitions as your starting point:\n\n- A smooth vector field on $\\mathbb{R}^{n}$ is a smooth map $f:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$.\n- The Lie bracket of smooth vector fields $X,Y:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ is defined by $[X,Y](x)=\\mathrm{D}Y(x)\\,X(x)-\\mathrm{D}X(x)\\,Y(x)$, where $\\mathrm{D}Y(x)$ denotes the Jacobian matrix of $Y$ at $x$.\n- The Lie Algebra Rank Condition (LARC) at a point $x_{0}$ holds if the linear span of the evaluations at $x_{0}$ of all iterated Lie brackets of the control vector fields equals the tangent space $T_{x_{0}}\\mathbb{R}^{n}$, which is naturally identified with $\\mathbb{R}^{n}$.\n\nCompute the vector subspace of $T_{0}\\mathbb{R}^{4}$ spanned by $\\{f_{i}(0)\\}$ together with all their iterated Lie brackets evaluated at $0$. From this, determine whether the Lie Algebra Rank Condition holds at $0$. For your final numeric answer, report only the dimension of this span at $0$. No rounding is needed. The answer is unitless and should be reported as a single integer.", "solution": "The problem as stated is a well-posed exercise in geometric nonlinear control theory, specifically concerning the verification of the Lie Algebra Rank Condition (LARC) for a control-affine system. The provided definitions and data are complete, consistent, and scientifically sound. I will proceed with the analysis.\n\nThe system dynamics are defined on the state space $\\mathbb{R}^{4}$ by $\\dot{x}=u_{1}f_{1}(x)+u_{2}f_{2}(x)+u_{3}f_{3}(x)$, where the state is $x=(x_{1},x_{2},x_{3},x_{4})$ and the control vector fields are given by:\n$$f_{1}(x) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{2}(x) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{3}(x) = \\begin{pmatrix} 0 \\\\ 0 \\\\ x_{2} \\\\ 1 \\end{pmatrix}$$\nThe task is to determine the dimension of the subspace of the tangent space $T_{0}\\mathbb{R}^{4}$ spanned by the set of vector fields $\\{f_1, f_2, f_3\\}$ and all their iterated Lie brackets, evaluated at the origin $x=0$. The tangent space $T_{0}\\mathbb{R}^{4}$ is identified with $\\mathbb{R}^{4}$.\n\nFirst, we evaluate the given vector fields at the point $x=0$:\n$$f_{1}(0) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{2}(0) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{3}(0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\nLet $\\mathcal{S}_{0} = \\mathrm{span}\\{f_{1}(0), f_{2}(0), f_{3}(0)\\}$. These three vectors are evidently linearly independent. They correspond to the standard basis vectors $e_{1}$, $e_{2}$, and $e_{4}$ in $\\mathbb{R}^{4}$. Thus, the dimension of this initial subspace is $\\dim(\\mathcal{S}_{0}) = 3$.\nSince the dimension of the state space is $4$, we must compute Lie brackets to determine if additional linearly independent directions can be generated.\n\nThe Lie bracket of two vector fields $X$ and $Y$ is defined as $[X,Y](x) = \\mathrm{D}Y(x)X(x) - \\mathrm{D}X(x)Y(x)$, where $\\mathrm{D}F(x)$ represents the Jacobian matrix of a vector field $F$ at $x$. We compute the Jacobian matrices for $f_1$, $f_2$, and $f_3$:\nSince $f_1$ and $f_2$ are constant vector fields, their Jacobian matrices are the $4 \\times 4$ zero matrix, $\\mathbf{0}$:\n$$\\mathrm{D}f_{1}(x) = \\mathbf{0}, \\quad \\mathrm{D}f_{2}(x) = \\mathbf{0}$$\nFor $f_{3}(x)$, we have components $f_{3,1}=0$, $f_{3,2}=0$, $f_{3,3}=x_2$, $f_{3,4}=1$. The only non-zero partial derivative is $\\frac{\\partial f_{3,3}}{\\partial x_{2}} = 1$. The Jacobian matrix is:\n$$\\mathrm{D}f_{3}(x) = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$$\nThis Jacobian is constant with respect to $x$.\n\nNow, we compute the first-order Lie brackets:\n1.  $[f_{1}, f_{2}](x) = \\mathrm{D}f_{2}(x)f_{1}(x) - \\mathrm{D}f_{1}(x)f_{2}(x) = \\mathbf{0} \\cdot f_{1}(x) - \\mathbf{0} \\cdot f_{2}(x) = 0$. This bracket yields the zero vector field and provides no new information.\n2.  $[f_{1}, f_{3}](x) = \\mathrm{D}f_{3}(x)f_{1}(x) - \\mathrm{D}f_{1}(x)f_{3}(x) = \\mathrm{D}f_{3}(x)f_{1}(x) - \\mathbf{0} \\cdot f_{3}(x) = \\mathrm{D}f_{3}(x)f_{1}(x)$.\n    $$\\mathrm{D}f_{3}(x)f_{1}(x) = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n    So, $[f_{1}, f_{3}](x) = 0$. This bracket is also trivial.\n3.  $[f_{2}, f_{3}](x) = \\mathrm{D}f_{3}(x)f_{2}(x) - \\mathrm{D}f_{2}(x)f_{3}(x) = \\mathrm{D}f_{3}(x)f_{2}(x) - \\mathbf{0} \\cdot f_{3}(x) = \\mathrm{D}f_{3}(x)f_{2}(x)$.\n    $$\\mathrm{D}f_{3}(x)f_{2}(x) = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n    This yields a non-zero, constant vector field: $[f_2, f_3](x) = (0, 0, 1, 0)^{T}$.\n\nLet us evaluate this new vector at $x=0$:\n$$[f_2, f_3](0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThis vector corresponds to the standard basis vector $e_{3}$ in $\\mathbb{R}^{4}$. We check if this vector is in the span $\\mathcal{S}_{0}$. A generic vector in $\\mathcal{S}_{0}$ has the form $c_{1}f_{1}(0) + c_{2}f_{2}(0) + c_{3}f_{3}(0) = (c_{1}, c_{2}, 0, c_{3})^{T}$. It is impossible for this to equal $(0, 0, 1, 0)^{T}$ for any choice of scalars $c_{1}, c_{2}, c_{3}$. Therefore, $[f_2, f_3](0)$ is linearly independent of $\\{f_{1}(0), f_{2}(0), f_{3}(0)\\}$.\n\nWe augment our set of vectors with this new vector and define the new subspace $\\mathcal{S}_{1} = \\mathrm{span}\\{f_{1}(0), f_{2}(0), f_{3}(0), [f_2, f_3](0)\\}$. The set of vectors is:\n$$\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}$$\nThis is the set of standard basis vectors $\\{e_1, e_2, e_4, e_3\\}$ for $\\mathbb{R}^{4}$. These four vectors are linearly independent and form a basis for $\\mathbb{R}^{4}$.\nThe dimension of the subspace spanned by these vectors is $\\dim(\\mathcal{S}_{1}) = 4$.\n\nSince the dimension of this subspace is $4$, which is equal to the dimension of the state space $\\mathbb{R}^{4}$, the span of the evaluated vector fields and their brackets is the entire tangent space $T_{0}\\mathbb{R}^{4}$. Any further iterated Lie brackets will produce vectors that are already in this span. For instance, any bracket involving the constant vector $[f_2, f_3]$ with one of the constant vector fields $f_1$ or $f_2$ will be zero. A bracket with $f_3$ will also result in a zero vector. The process terminates.\n\nThe vector subspace of $T_{0}\\mathbb{R}^{4}$ spanned by the control vector fields and all their iterated Lie brackets is $\\mathbb{R}^{4}$ itself. The dimension of this subspace is $4$. Consequently, the Lie Algebra Rank Condition is satisfied at the origin $x=0$. The question asks for the dimension of this span.", "answer": "$$\\boxed{4}$$", "id": "2710214"}]}