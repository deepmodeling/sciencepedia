{"hands_on_practices": [{"introduction": "The first step in applying flatness-based control is to verify if the system possesses this property. This practice provides a hands-on exercise in computing the relative degree for a canonical nonlinear system using the fundamental tool of Lie derivatives. Successfully determining the relative degree not only confirms the possibility of input-output linearization but is also a crucial check for differential flatness [@problem_id:2700623].", "problem": "Consider the single-input single-output (SISO) nonlinear system with state vector $x=\\begin{pmatrix}x_{1} & x_{2}\\end{pmatrix}^{\\top}$, control input $u$, and output $y$ given by\n$$\n\\dot{x}_{1}=x_{2},\\qquad \\dot{x}_{2}=\\sin(x_{1})+u,\\qquad y=x_{1}.\n$$\nStarting from first principles and core definitions used in nonlinear control, use the concept of Lie derivatives to determine the relative degree $r$ of the system with respect to the output $y$, and use that determination to decide whether static input-output linearization is possible around all states for which the relative degree is well defined. Your reasoning should begin from the definition of the Lie derivative of a scalar field along a vector field and the definition of relative degree for a SISO system, and proceed step by step without invoking any unproven shortcut formulas.\n\nAnswer specification:\n- Express your final answer as a $1\\times 2$ row matrix $\\begin{pmatrix}r & I\\end{pmatrix}$, where $r$ is the relative degree and $I$ is the indicator of feasibility of static input-output linearization with $I=1$ if it is possible (for all states where the relative degree is well defined) and $I=0$ otherwise.\n- Do not include any units.\n- No rounding is required.", "solution": "The problem provided is a standard exercise in nonlinear control theory. It is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We shall proceed with the analysis.\n\nThe system is given by the state equations:\n$$\n\\dot{x}_{1} = x_{2}\n$$\n$$\n\\dot{x}_{2} = \\sin(x_{1}) + u\n$$\nwith the state vector $x = \\begin{pmatrix} x_{1} & x_{2} \\end{pmatrix}^{\\top}$ and output $y = x_{1}$. This system can be written in the standard affine form $\\dot{x} = f(x) + g(x)u$ with the output equation $y = h(x)$, where:\n$$\nf(x) = \\begin{pmatrix} x_{2} \\\\ \\sin(x_{1}) \\end{pmatrix}, \\quad g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad h(x) = x_{1}\n$$\nThe problem requires us to determine the relative degree $r$ using Lie derivatives and then to assess the possibility of static input-output linearization.\n\nWe begin from the first principles, as instructed. The Lie derivative of a scalar field $h(x)$ along a vector field $f(x)$ is defined as the directional derivative of $h(x)$ in the direction of $f(x)$:\n$$\nL_f h(x) = \\frac{\\partial h}{\\partial x} f(x) = \\sum_{i=1}^{n} \\frac{\\partial h}{\\partial x_i} f_i(x)\n$$\nwhere $n$ is the dimension of the state space, which is $2$ in this case.\n\nThe relative degree $r$ of a single-input single-output (SISO) system at a point $x_0$ is defined as the smallest integer $r \\ge 1$ such that:\n$1.$ $L_g L_f^k h(x) = 0$ for all $x$ in a neighborhood of $x_0$ and for all integers $k < r-1$.\n$2.$ $L_g L_f^{r-1} h(x_0) \\neq 0$.\nHere, $L_f^k h$ denotes the $k$-th iterated Lie derivative, where $L_f^0 h = h$ and $L_f^k h = L_f(L_f^{k-1} h)$.\n\nTo find the relative degree, we differentiate the output $y$ with respect to time until the input $u$ explicitly appears. The number of differentiations required is the relative degree.\n\nThe first time derivative of the output $y$ is:\n$$\n\\dot{y} = \\frac{d}{dt}h(x) = \\frac{\\partial h}{\\partial x} \\dot{x} = \\frac{\\partial h}{\\partial x} (f(x) + g(x)u) = L_f h(x) + L_g h(x) u\n$$\nWe must compute the terms $L_g h(x)$ and $L_f h(x)$. The gradient of $h(x) = x_1$ is:\n$$\n\\frac{\\partial h}{\\partial x} = \\begin{pmatrix} \\frac{\\partial x_1}{\\partial x_1} & \\frac{\\partial x_1}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}\n$$\nNow we compute $L_g h(x)$:\n$$\nL_g h(x) = \\frac{\\partial h}{\\partial x} g(x) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (0)(1) = 0\n$$\nSince $L_g h(x) = L_g L_f^0 h(x) = 0$, the relative degree must be $r > 1$. The expression for $\\dot{y}$ simplifies to $\\dot{y} = L_f h(x)$. Let us compute this term:\n$$\nL_f h(x) = \\frac{\\partial h}{\\partial x} f(x) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ \\sin(x_{1}) \\end{pmatrix} = (1)(x_{2}) + (0)(\\sin(x_{1})) = x_{2}\n$$\nSo, we have $\\dot{y} = x_2$. The input $u$ does not appear, confirming that $r > 1$.\n\nWe proceed to the second time derivative of the output:\n$$\n\\ddot{y} = \\frac{d}{dt} \\dot{y} = \\frac{d}{dt} (L_f h(x)) = \\frac{\\partial (L_f h(x))}{\\partial x} \\dot{x} = \\frac{\\partial (L_f h(x))}{\\partial x} (f(x) + g(x)u)\n$$\nThis expression can be written using Lie derivatives as:\n$$\n\\ddot{y} = L_f(L_f h(x)) + L_g(L_f h(x)) u = L_f^2 h(x) + L_g L_f^1 h(x) u\n$$\nWe need to check if the coefficient of $u$, which is $L_g L_f h(x)$, is non-zero. Let $\\psi(x) = L_f h(x) = x_2$. The gradient of $\\psi(x)$ is:\n$$\n\\frac{\\partial \\psi}{\\partial x} = \\begin{pmatrix} \\frac{\\partial x_2}{\\partial x_1} & \\frac{\\partial x_2}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}\n$$\nNow, we compute $L_g L_f h(x)$:\n$$\nL_g L_f h(x) = L_g \\psi(x) = \\frac{\\partial \\psi}{\\partial x} g(x) = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (0)(0) + (1)(1) = 1\n$$\nSince $L_g L_f^{2-1} h(x) = 1$, which is non-zero for all states $x \\in \\mathbb{R}^2$, we conclude from the definition that the relative degree of the system is $r=2$.\n\nNext, we determine if static input-output linearization is possible. This procedure aims to find a static state feedback control law $u = \\alpha(x) + \\beta(x)v$, where $v$ is a new external input, that renders the input-output map linear. The input-output dynamics are given by the $r$-th derivative of $y$:\n$$\ny^{(r)} = L_f^r h(x) + L_g L_f^{r-1} h(x) u\n$$\nFor our system with $r=2$, this is:\n$$\n\\ddot{y} = L_f^2 h(x) + L_g L_f h(x) u\n$$\nWe have already found $L_g L_f h(x) = 1$. We now compute $L_f^2 h(x)$:\n$$\nL_f^2 h(x) = L_f(\\psi(x)) = \\frac{\\partial \\psi}{\\partial x} f(x) = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ \\sin(x_{1}) \\end{pmatrix} = (0)(x_{2}) + (1)(\\sin(x_{1})) = \\sin(x_{1})\n$$\nSubstituting these results into the equation for $\\ddot{y}$ gives:\n$$\n\\ddot{y} = \\sin(x_{1}) + (1)u\n$$\nTo achieve a linear input-output response, for example $\\ddot{y} = v$, we set:\n$$\nv = \\sin(x_{1}) + u\n$$\nSolving for the control input $u$ yields the static state feedback law:\n$$\nu = v - \\sin(x_{1})\n$$\nThis control law makes the system behave as $\\ddot{y} = v$, which is a linear relationship between the new input $v$ and the output $y$.\n\nThe feasibility of static input-output linearization depends on whether this feedback law is well-defined. The general form of the law is $u = \\frac{1}{L_g L_f^{r-1} h(x)}(v - L_f^r h(x))$. This law is well-defined at all states $x$ for which the relative degree $r$ is well-defined, i.e., where $L_g L_f^{r-1} h(x) \\neq 0$.\nIn our case, $r=2$, and the critical term is $L_g L_f h(x) = 1$. Since this term is a non-zero constant, it is non-zero for all states $x \\in \\mathbb{R}^2$. This means the relative degree $r=2$ is well-defined over the entire state space. Consequently, the feedback law $u = v - \\sin(x_{1})$ is well-defined for all $x \\in \\mathbb{R}^2$.\nTherefore, static input-output linearization is possible for all states. This corresponds to an indicator value of $I=1$.\n\nCombining our findings, the relative degree is $r=2$ and the indicator of linearization feasibility is $I=1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 1 \\end{pmatrix}}\n$$", "id": "2700623"}, {"introduction": "Once a system is identified as differentially flat, the daunting task of generating dynamically feasible trajectories simplifies dramatically. This exercise demonstrates the core benefit of flatness: planning a complex maneuver boils down to a straightforward algebraic problem of fitting a polynomial to a set of boundary conditions on the flat output and its derivatives [@problem_id:2700537]. This technique forms the basis for generating smooth, point-to-point motions in systems ranging from robotics to aerospace.", "problem": "Consider a single-input single-output system known to be differentially flat with scalar flat output $z(t)$. In trajectory generation for differentially flat systems, one often prescribes endpoint constraints on the flat output and its time derivatives and then fits a polynomial of minimal degree that satisfies those constraints. Let $r=4$, so that the constraints are on $z$ and its first three derivatives. You are given the boundary data\n$$\nz^{(k)}(0)=z_{0}^{(k)},\\quad z^{(k)}(T)=z_{T}^{(k)},\\quad k=0,1,2,3,\n$$\nwith $T>0$. Using only the definitions of derivatives and the fact that a polynomial of degree $n$ has $n+1$ free coefficients, address the following:\n\n1. Argue from first principles why the minimal-degree polynomial that can satisfy all $8$ constraints must have degree $7$.\n\n2. Construct a polynomial trajectory $z(t)$ of degree $7$,\n$$\nz(t)=a_{0}+a_{1}t+a_{2}t^{2}+a_{3}t^{3}+a_{4}t^{4}+a_{5}t^{5}+a_{6}t^{6}+a_{7}t^{7},\n$$\nthat satisfies the specific boundary conditions\n$$\nz_{0}^{(0)}=0,\\quad z_{0}^{(1)}=0,\\quad z_{0}^{(2)}=0,\\quad z_{0}^{(3)}=0,\\qquad z_{T}^{(0)}=1,\\quad z_{T}^{(1)}=0,\\quad z_{T}^{(2)}=0,\\quad z_{T}^{(3)}=0.\n$$\nCompute the coefficients $\\{a_{i}\\}_{i=0}^{7}$ explicitly as functions of $T$.\n\nAnswer specification: Express your final answer as the $1\\times 8$ row vector of coefficients $(a_{0},a_{1},a_{2},a_{3},a_{4},a_{5},a_{6},a_{7})$. No numerical rounding is required. Do not include units in your answer.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard, solvable problem in control theory and applied mathematics without internal contradictions or missing information.\n\nPart 1: Determination of the minimal polynomial degree.\n\nA polynomial of degree $n$ is specified by $n+1$ independent coefficients. Let the polynomial be $z(t) = \\sum_{i=0}^{n} a_i t^i$. The coefficients $\\{ a_i \\}_{i=0}^n$ constitute the $n+1$ degrees of freedom available to satisfy a set of constraints.\n\nThe problem statement prescribes $8$ boundary constraints. These are constraints on the value of the function and its first three derivatives at two distinct points in time, $t=0$ and $t=T$. Specifically, for $k \\in \\{0, 1, 2, 3\\}$, we have $4$ constraints at $t=0$:\n$$z^{(k)}(0) = z_0^{(k)}$$\nand $4$ constraints at $t=T$:\n$$z^{(k)}(T) = z_T^{(k)}$$\nThis is a total of $4+4=8$ linear constraints on the polynomial coefficients.\n\nIn order to satisfy $8$ independent linear constraints, one must have at least $8$ free parameters. Therefore, the number of coefficients, $n+1$, must be at least $8$.\n$$n+1 \\ge 8 \\implies n \\ge 7$$\nThe minimal integer degree $n$ that satisfies this condition is $n=7$. A polynomial of degree $7$ provides exactly $8$ coefficients ($a_0, \\dots, a_7$), which is the precise number required to satisfy the $8$ given constraints. This leads to a system of $8$ linear equations in $8$ unknowns, which, if the constraint matrix is non-singular, has a unique solution. A polynomial of degree $6$ or less would have $7$ or fewer coefficients and would be insufficient to satisfy $8$ arbitrary boundary conditions. Hence, the minimal degree is $7$.\n\nPart 2: Construction of the polynomial trajectory.\n\nWe seek a polynomial of degree $7$:\n$$z(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5 + a_6 t^6 + a_7 t^7$$\nThe derivatives up to the third order are:\n$$z^{(1)}(t) = a_1 + 2 a_2 t + 3 a_3 t^2 + 4 a_4 t^3 + 5 a_5 t^4 + 6 a_6 t^5 + 7 a_7 t^6$$\n$$z^{(2)}(t) = 2 a_2 + 6 a_3 t + 12 a_4 t^2 + 20 a_5 t^3 + 30 a_6 t^4 + 42 a_7 t^5$$\n$$z^{(3)}(t) = 6 a_3 + 24 a_4 t + 60 a_5 t^2 + 120 a_6 t^3 + 210 a_7 t^4$$\n\nThe specified boundary conditions are:\n$z^{(k)}(0) = 0$ for $k=0, 1, 2, 3$.\n$z^{(0)}(T) = 1$ and $z^{(k)}(T) = 0$ for $k=1, 2, 3$.\n\nFirst, we apply the constraints at $t=0$:\n$z^{(0)}(0) = a_0 = 0$\n$z^{(1)}(0) = a_1 = 0$\n$z^{(2)}(0) = 2 a_2 = 0 \\implies a_2 = 0$\n$z^{(3)}(0) = 6 a_3 = 0 \\implies a_3 = 0$\nThese four conditions immediately determine the first four coefficients to be zero. The polynomial thus simplifies to:\n$$z(t) = a_4 t^4 + a_5 t^5 + a_6 t^6 + a_7 t^7$$\n\nNext, we apply the constraints at $t=T$, where $T>0$.\nThe simplified polynomial and its derivatives are:\n$z(t) = a_4 t^4 + a_5 t^5 + a_6 t^6 + a_7 t^7$\n$z^{(1)}(t) = 4 a_4 t^3 + 5 a_5 t^4 + 6 a_6 t^5 + 7 a_7 t^6$\n$z^{(2)}(t) = 12 a_4 t^2 + 20 a_5 t^3 + 30 a_6 t^4 + 42 a_7 t^5$\n$z^{(3)}(t) = 24 a_4 t + 60 a_5 t^2 + 120 a_6 t^3 + 210 a_7 t^4$\n\nEvaluating these at $t=T$ gives the following system of four linear equations for the remaining four coefficients $a_4, a_5, a_6, a_7$:\n1. $z(T) = a_4 T^4 + a_5 T^5 + a_6 T^6 + a_7 T^7 = 1$\n2. $z^{(1)}(T) = 4 a_4 T^3 + 5 a_5 T^4 + 6 a_6 T^5 + 7 a_7 T^6 = 0$\n3. $z^{(2)}(T) = 12 a_4 T^2 + 20 a_5 T^3 + 30 a_6 T^4 + 42 a_7 T^5 = 0$\n4. $z^{(3)}(T) = 24 a_4 T + 60 a_5 T^2 + 120 a_6 T^3 + 210 a_7 T^4 = 0$\n\nWe can write this system in matrix form. Let the vector of unknowns be $\\mathbf{a} = \\begin{pmatrix} a_4 & a_5 & a_6 & a_7 \\end{pmatrix}^T$.\n$$\n\\begin{pmatrix}\nT^4 & T^5 & T^6 & T^7 \\\\\n4T^3 & 5T^4 & 6T^5 & 7T^6 \\\\\n12T^2 & 20T^3 & 30T^4 & 42T^5 \\\\\n24T & 60T^2 & 120T^3 & 210T^4\n\\end{pmatrix}\n\\begin{pmatrix}\na_4 \\\\ a_5 \\\\ a_6 \\\\ a_7\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}\n$$\nTo solve this system, we introduce new variables $b_i$ to simplify the algebra:\n$b_4 = a_4 T^4$, $b_5 = a_5 T^5$, $b_6 = a_6 T^6$, $b_7 = a_7 T^7$.\nThe system becomes:\n1. $b_4 + b_5 + b_6 + b_7 = 1$\n2. $4 b_4 + 5 b_5 + 6 b_6 + 7 b_7 = 0$\n3. $12 b_4 + 20 b_5 + 30 b_6 + 42 b_7 = 0$\n4. $24 b_4 + 60 b_5 + 120 b_6 + 210 b_7 = 0$\n\nFrom (1), $b_4 = 1 - b_5 - b_6 - b_7$. Substituting this into the other three equations gives:\n2'. $4(1-b_5-b_6-b_7) + 5b_5 + 6b_6 + 7b_7 = 0 \\implies 4+b_5+2b_6+3b_7 = 0$\n3'. $12(1-b_5-b_6-b_7) + 20b_5 + 30b_6 + 42b_7 = 0 \\implies 12+8b_5+18b_6+30b_7=0 \\implies 6+4b_5+9b_6+15b_7=0$\n4'. $24(1-b_5-b_6-b_7) + 60b_5 + 120b_6 + 210b_7 = 0 \\implies 24+36b_5+96b_6+186b_7=0 \\implies 4+6b_5+16b_6+31b_7=0$\n\nWe now have a $3 \\times 3$ system for $b_5, b_6, b_7$:\nA. $b_5 + 2b_6 + 3b_7 = -4$\nB. $4b_5 + 9b_6 + 15b_7 = -6$\nC. $6b_5 + 16b_6 + 31b_7 = -4$\n\nFrom (A), $b_5 = -4 - 2b_6 - 3b_7$. Substituting into (B) and (C):\nB'. $4(-4-2b_6-3b_7) + 9b_6 + 15b_7 = -6 \\implies -16-8b_6-12b_7+9b_6+15b_7=-6 \\implies b_6+3b_7 = 10$\nC'. $6(-4-2b_6-3b_7) + 16b_6 + 31b_7 = -4 \\implies -24-12b_6-18b_7+16b_6+31b_7=-4 \\implies 4b_6+13b_7 = 20$\n\nFrom (B'), $b_6=10-3b_7$. Substituting into (C'):\n$4(10-3b_7)+13b_7 = 20 \\implies 40-12b_7+13b_7=20 \\implies b_7 = -20$.\n\nBack-substituting, we find:\n$b_6 = 10 - 3(-20) = 10 + 60 = 70$.\n$b_5 = -4 - 2(70) - 3(-20) = -4 - 140 + 60 = -84$.\n$b_4 = 1 - b_5 - b_6 - b_7 = 1 - (-84) - (70) - (-20) = 1 + 84 - 70 + 20 = 35$.\n\nFinally, we solve for the original coefficients $a_i$ using their definitions:\n$a_4 = \\frac{b_4}{T^4} = \\frac{35}{T^4}$\n$a_5 = \\frac{b_5}{T^5} = -\\frac{84}{T^5}$\n$a_6 = \\frac{b_6}{T^6} = \\frac{70}{T^6}$\n$a_7 = \\frac{b_7}{T^7} = -\\frac{20}{T^7}$\n\nThe complete set of coefficients for the polynomial $z(t)$ is:\n$a_0=0$, $a_1=0$, $a_2=0$, $a_3=0$, $a_4 = \\frac{35}{T^4}$, $a_5 = -\\frac{84}{T^5}$, $a_6 = \\frac{70}{T^6}$, $a_7 = -\\frac{20}{T^7}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 0 & 0 & 0 & \\frac{35}{T^4} & -\\frac{84}{T^5} & \\frac{70}{T^6} & -\\frac{20}{T^7} \\end{pmatrix}}\n$$", "id": "2700537"}, {"introduction": "Real-world tasks often require traversing a sequence of waypoints, not just a single point-to-point motion. This practice scales up the previous concept to generating complex, multi-segment trajectories by creating piecewise polynomial splines that ensure smoothness at the waypoints [@problem_id:2700549]. By translating continuity constraints into a solvable linear system, this computational exercise provides a direct blueprint for implementing practical trajectory planners for differentially flat systems.", "problem": "Consider a differentially flat, single-input single-output system whose flat output $y(t)$ is planned on a time grid $t_0 < t_1 < \\dots < t_K$. On each interval $\\left[t_i,t_{i+1}\\right]$, $i \\in \\{0,\\dots,K-1\\}$, approximate the flat output by a degree-$n$ polynomial in the locally shifted time $\\tau = t - t_i$,\n$$\ny_i(t) \\triangleq \\sum_{j=0}^{n} c_{i,j}\\,\\tau^j,\n$$\nwhere the unknown coefficients $c_{i,j}$ are to be determined. Let $h_i \\triangleq t_{i+1}-t_i$. Denote by $y^{(m)}(t)$ the $m$-th time derivative of $y(t)$, with $m \\in \\mathbb{N}_0$. The fundamental rule for differentiating monomials yields\n$$\n\\frac{d^m}{dt^m}\\left(\\tau^j\\right) = \\begin{cases}\n\\dfrac{j!}{(j-m)!}\\,\\tau^{j-m}, & j \\ge m, \\\\\n0, & j  m.\n\\end{cases}\n$$\nTherefore, at the left endpoint $\\tau=0$ of segment $i$,\n$$\ny_i^{(m)}(t_i) = \\sum_{j=m}^{n} c_{i,j} \\frac{j!}{(j-m)!} \\cdot 0^{j-m} = m!\\,c_{i,m},\n$$\nand at the right endpoint $\\tau=h_i$ of segment $i$,\n$$\ny_i^{(m)}(t_{i+1}) = \\sum_{j=m}^{n} c_{i,j} \\frac{j!}{(j-m)!} h_i^{\\,j-m}.\n$$\n\nYou are tasked to set up a linear system that enforces endpoint value interpolation, continuity up to order $r-1$ at internal waypoints, and boundary derivative constraints at $t_0$ and $t_K$, and then solve for all polynomial coefficients across all segments.\n\nUse the following foundational requirements, which are the only constraints to be enforced:\n\n- Value interpolation on each segment’s endpoints:\n  - For every segment $i \\in \\{0,\\dots,K-1\\}$,\n    - left endpoint value: $y_i^{(0)}(t_i) = y_i$,\n    - right endpoint value: $y_i^{(0)}(t_{i+1}) = y_{i+1}$,\n    where the list $\\{y_0, y_1, \\dots, y_K\\}$ is given.\n\n- Continuity up to order $r-1$ at internal waypoints for derivatives of order $m \\in \\{1,\\dots,r-1\\}$:\n  - For every internal waypoint $i \\in \\{1,\\dots,K-1\\}$,\n    $$\n    y_{i-1}^{(m)}(t_i) = y_{i}^{(m)}(t_i).\n    $$\n\n- Boundary derivative constraints at $t_0$ and $t_K$ for derivatives of order $m \\in \\{1,\\dots,r-1\\}$:\n  $$\n  y_0^{(m)}(t_0) = \\alpha_m, \\quad y_{K-1}^{(m)}(t_K) = \\beta_m,\n  $$\n  where $\\{\\alpha_m\\}$ and $\\{\\beta_m\\}$ are given.\n\nNotes:\n- These constraints are all linear in the unknown coefficients $\\{c_{i,j}\\}$. Stack them to form a linear system $A c = b$, where $c$ is the vector stacking all $c_{i,j}$ by increasing segment index $i$ and, within each segment, by increasing power $j$.\n- Depending on the tuple $(K,n,r)$, the system may be square, overdetermined, or underdetermined. You must solve it in the least-squares sense: find $c^\\star$ that minimizes $\\|A c - b\\|_2$, and when multiple minimizers exist, select the minimum Euclidean norm solution.\n\nScientific realism and units:\n- Time must be interpreted in seconds, i.e., $t$ and all $t_i$ are in seconds. The values $y_i$ are dimensionless. The $j$-th coefficient $c_{i,j}$ has units of “$y$-units per second$^j$.” Your program must compute coefficients in those implied units and output the numeric values as floating-point numbers (unit-free numerals).\n- Angle units are not involved in this problem.\n\nYour program must:\n- Implement a general routine to assemble $A$ and $b$ from the constraints above for arbitrary $(K,n,r)$, time grid $\\{t_i\\}$, waypoint values $\\{y_i\\}$, and boundary derivatives $\\{\\alpha_m\\}, \\{\\beta_m\\}$ for $m \\in \\{1,\\dots,r-1\\}$.\n- Solve for $c^\\star$ using a numerically stable least-squares method.\n\nTest suite:\nProvide results for the following three cases. In each case, return the flattened list of coefficients $[c_{0,0},\\dots,c_{0,n},c_{1,0},\\dots]$:\n\n- Case A (square system, unique solution):\n  - $r = 3$, $n = 5$, $K = 1$.\n  - Times: $\\left[t_0,t_1\\right] = \\left[0,2\\right]$ (seconds).\n  - Waypoint values: $\\left[y_0,y_1\\right] = \\left[0,1\\right]$.\n  - Boundary derivatives: $y^{(1)}(t_0) = \\alpha_1 = 0$, $y^{(2)}(t_0) = \\alpha_2 = 0$, $y^{(1)}(t_1) = \\beta_1 = 0$, $y^{(2)}(t_1) = \\beta_2 = 0$.\n\n- Case B (rectangular system, minimum-norm least-squares solution):\n  - $r = 2$, $n = 3$, $K = 2$.\n  - Times: $\\left[t_0,t_1,t_2\\right] = \\left[0,1,3\\right]$ (seconds).\n  - Waypoint values: $\\left[y_0,y_1,y_2\\right] = \\left[0,1,0\\right]$.\n  - Boundary derivatives: $y^{(1)}(t_0) = \\alpha_1 = 0$, $y^{(1)}(t_2) = \\beta_1 = 0$.\n\n- Case C (rectangular system, minimum-norm least-squares solution; nonuniform segments):\n  - $r = 2$, $n = 3$, $K = 3$.\n  - Times: $\\left[t_0,t_1,t_2,t_3\\right] = \\left[0,0.5,1.5,3.0\\right]$ (seconds).\n  - Waypoint values: $\\left[y_0,y_1,y_2,y_3\\right] = \\left[0,1,-1,0\\right]$.\n  - Boundary derivatives: $y^{(1)}(t_0) = \\alpha_1 = 0.5$, $y^{(1)}(t_3) = \\beta_1 = -0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with three elements, one per case, where each element is itself a flat list of floating-point coefficients ordered by increasing segment and, within each segment, by increasing power $j$ from $0$ to $n$. For example, an output with two cases would look like $\\left[\\left[c_{0,0},\\dots\\right],\\left[c_{0,0},\\dots\\right]\\right]$. Use standard Python list syntax without units.", "solution": "We begin from the fundamental definition of a polynomial segment on $\\left[t_i,t_{i+1}\\right]$ in locally shifted time $\\tau=t-t_i$,\n$$\ny_i(t) = \\sum_{j=0}^{n} c_{i,j}\\,\\tau^j,\n$$\nand use the standard differentiation rule for monomials,\n$$\n\\frac{d^m}{dt^m}\\left(\\tau^j\\right) = \\begin{cases}\n\\dfrac{j!}{(j-m)!}\\,\\tau^{j-m},  j \\ge m, \\\\\n0,  j  m.\n\\end{cases}\n$$\nEvaluating derivatives at the endpoints of the segment leverages the special values $\\tau=0$ (left) and $\\tau=h_i$ (right), with $h_i \\triangleq t_{i+1}-t_i$. At $\\tau=0$,\n$$\ny_i^{(m)}(t_i) = \\sum_{j=m}^{n} c_{i,j} \\frac{j!}{(j-m)!} 0^{j-m} = m! c_{i,m}.\n$$\nAt $\\tau=h_i$,\n$$\ny_i^{(m)}(t_{i+1}) = \\sum_{j=m}^{n} c_{i,j} \\frac{j!}{(j-m)!} h_i^{\\,j-m}.\n$$\n\nThese identities are the base to encode constraints as linear equations in the unknown coefficients $c_{i,j}$. We now translate the three groups of requirements into rows of a linear system $A c = b$.\n\n1) Endpoint value interpolation on each segment. For each segment $i \\in \\{0,\\dots,K-1\\}$:\n- Left endpoint value $y_i^{(0)}(t_i) = y_i$ becomes:\n  $$\n  \\sum_{j=0}^{n} c_{i,j}\\,0^j = c_{i,0} = y_i.\n  $$\n  This contributes one linear equation with coefficient $1$ at the position of $c_{i,0}$ and right-hand side $y_i$.\n- Right endpoint value $y_i^{(0)}(t_{i+1}) = y_{i+1}$ becomes:\n  $$\n  \\sum_{j=0}^{n} c_{i,j}\\,h_i^{\\,j} = y_{i+1}.\n  $$\n  This contributes one equation with entries $h_i^{\\,j}$ at the positions of $c_{i,j}$.\n\n2) Continuity up to order $r-1$ at internal waypoints for derivatives $m \\in \\{1,\\dots,r-1\\}$. For each internal waypoint $i \\in \\{1,\\dots,K-1\\}$ and each derivative order $m$:\n- Enforce $y_{i-1}^{(m)}(t_i) = y_{i}^{(m)}(t_i)$. Using the endpoint formulas,\n  $$\n  \\sum_{j=m}^{n} c_{i-1,j} \\frac{j!}{(j-m)!} h_{i-1}^{\\,j-m} \\;=\\; m!\\,c_{i,m}.\n  $$\n  This contributes one linear equation per $(i,m)$, with the left-hand side linking the coefficients of segment $i-1$ and the right-hand side involving $c_{i,m}$.\n\n3) Boundary derivative constraints at $t_0$ and $t_K$ for $m \\in \\{1,\\dots,r-1\\}$:\n- At $t_0$:\n  $$\n  y_0^{(m)}(t_0) = m!\\,c_{0,m} = \\alpha_m,\n  $$\n  yielding one equation per $m$ with a single nonzero entry $m!$ at the position of $c_{0,m}$ and right-hand side $\\alpha_m$.\n- At $t_K$:\n  $$\n  y_{K-1}^{(m)}(t_K) = \\sum_{j=m}^{n} c_{K-1,j} \\frac{j!}{(j-m)!} h_{K-1}^{\\,j-m} = \\beta_m,\n  $$\n  yielding one equation per $m$ tying coefficients of the last segment.\n\nCollect all equations into a matrix $A \\in \\mathbb{R}^{M_c \\times M}$ and a vector $b \\in \\mathbb{R}^{M_c}$, where $M = K (n+1)$ is the total number of unknown coefficients and $M_c$ is the number of constraints. The coefficients are stacked as\n$$\nc \\triangleq \\begin{bmatrix} c_{0,0}  c_{0,1}  \\dots  c_{0,n}  c_{1,0}  \\dots  c_{K-1,n} \\end{bmatrix}^{\\top}.\n$$\n\nSolving strategy. Depending on $(K,n,r)$, $A$ may be square, tall, or wide. The problem posed here does not include an optimality criterion beyond satisfying constraints; thus the appropriate approach is the Euclidean least-squares solution:\n$$\nc^\\star \\triangleq \\arg\\min_{c} \\|A c - b\\|_2,\n$$\nwith the additional requirement to select the minimum Euclidean norm solution among minimizers when the solution is not unique. A robust numerical method is to use a linear least-squares solver based on the Singular Value Decomposition (SVD), which computes $c^\\star$ satisfying both requirements.\n\nAlgorithmic design:\n- Compute segment durations $h_i = t_{i+1}-t_i$.\n- Initialize empty lists for rows of $A$ and entries of $b$.\n- Add value interpolation constraints:\n  - For each segment $i$:\n    - Left value: row with a single $1$ at the index of $c_{i,0}$, right-hand side $y_i$.\n    - Right value: row with entries $h_i^{\\,j}$ for $j \\in \\{0,\\dots,n\\}$ at the indices of $c_{i,j}$, right-hand side $y_{i+1}$.\n- Add continuity constraints for $m \\in \\{1,\\dots,r-1\\}$:\n  - For each internal waypoint $i \\in \\{1,\\dots,K-1\\}$:\n    - Form a row with entries $\\frac{j!}{(j-m)!} h_{i-1}^{\\,j-m}$ at the indices of $c_{i-1,j}$ for $j \\in \\{m,\\dots,n\\}$, and place $-m!$ at the index of $c_{i,m}$; right-hand side $0$.\n- Add boundary derivative constraints for $m \\in \\{1,\\dots,r-1\\}$:\n  - At $t_0$: row with entry $m!$ at $c_{0,m}$, right-hand side $\\alpha_m$.\n  - At $t_K$: row with entries $\\frac{j!}{(j-m)!} h_{K-1}^{\\,j-m}$ at $c_{K-1,j}$ for $j \\in \\{m,\\dots,n\\}$, right-hand side $\\beta_m$.\n- Stack rows into $A$ and $b$.\n- Solve the least-squares problem using a numerically stable routine (for example, via Singular Value Decomposition), obtaining the minimum-norm solution $c^\\star$.\n- Return, for each test case, the flattened list of coefficients segment-by-segment and power-by-power.\n\nWhy this works. The constraints derive directly from the definitions of derivatives of polynomials and the requirement of continuity of the flat output and its derivatives up to order $r-1$. Because the constraints are linear in the coefficients, stacking them yields a linear system. When the system is square and nonsingular, the unique solution satisfies all constraints exactly. When rectangular, the least-squares solution minimizes the violation in the Euclidean sense and, with SVD-based solvers, also selects the minimum Euclidean norm among all minimizers, which is a canonical choice in the absence of an additional optimality criterion such as minimizing an integrated higher-order derivative (e.g., snap). This approach is consistent with trajectory generation under differential flatness, where the flat output and its derivatives up to the relative degree define the full system state and input, provided the flatness map is smooth at waypoints.\n\nThe test suite includes:\n- A square system (Case A) where $n=2r-1$ and a single segment $K=1$ makes the constraints exactly match the number of unknowns.\n- Two rectangular systems (Cases B and C) with continuity up to order $r-1$ and endpoint derivative constraints that leave degrees of freedom, resolved by the minimum-norm least-squares solution.\nThe outputs are the computed coefficients in the units implied by the model: the coefficient $c_{i,j}$ has units of $y$-units per second$^j$, consistent with $y(t)$ being dimensionless and $t$ in seconds.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef factorial(n: int) - int:\n    return math.factorial(n)\n\ndef coeff_index(seg: int, power: int, n: int) - int:\n    return seg * (n + 1) + power\n\ndef assemble_system(times, y_vals, r, n, alpha_dict, beta_dict):\n    \"\"\"\n    Assemble A and b for the linear system enforcing:\n    - Endpoint value interpolation on every segment\n    - Continuity up to order r-1 at internal waypoints for derivatives m=1..r-1\n    - Boundary derivative constraints at t0 and tK for derivatives m=1..r-1\n\n    times: list of K+1 time points [t0,...,tK]\n    y_vals: list of K+1 waypoint values [y0,...,yK]\n    r: continuity order parameter (enforce up to r-1)\n    n: degree of polynomial per segment\n    alpha_dict: dict mapping m in 1..r-1 to alpha_m at t0\n    beta_dict: dict mapping m in 1..r-1 to beta_m at tK\n\n    Returns: (A, b)\n    \"\"\"\n    times = np.asarray(times, dtype=float)\n    y_vals = np.asarray(y_vals, dtype=float)\n    K = len(times) - 1\n    assert K = 1, \"At least one segment is required.\"\n    assert len(y_vals) == len(times)\n    # Segment durations\n    h = np.diff(times)\n\n    rows = []\n    rhs = []\n\n    # 1) Endpoint value interpolation on each segment\n    # For each segment i: y_i(t_i) = y_i - c_{i,0} = y_i\n    # and y_i(t_{i+1}) = y_{i+1} - sum_j c_{i,j} h_i^j = y_{i+1}\n    for i in range(K):\n        # Left endpoint value\n        row = np.zeros(K * (n + 1))\n        row[coeff_index(i, 0, n)] = 1.0\n        rows.append(row)\n        rhs.append(y_vals[i])\n\n        # Right endpoint value\n        row = np.zeros(K * (n + 1))\n        for j in range(n + 1):\n            row[coeff_index(i, j, n)] = (h[i] ** j)\n        rows.append(row)\n        rhs.append(y_vals[i + 1])\n\n    # 2) Continuity up to order r-1 at internal waypoints for derivatives m=1..r-1\n    for i in range(1, K):\n        hi_1 = h[i - 1]\n        for m in range(1, min(r, n + 1)):  # If n  r-1, only enforce up to n\n            row = np.zeros(K * (n + 1))\n            # Left side: derivatives at end of segment i-1\n            for j in range(m, n + 1):\n                row[coeff_index(i - 1, j, n)] += factorial(j) / factorial(j - m) * (hi_1 ** (j - m))\n            # Right side: derivatives at start of segment i: m! * c_{i,m}\n            row[coeff_index(i, m, n)] += - factorial(m)\n            rows.append(row)\n            rhs.append(0.0)\n\n    # 3) Boundary derivative constraints at t0 and tK for m=1..r-1\n    # At t0: m! * c_{0,m} = alpha_m\n    for m in range(1, min(r, n + 1)):\n        if m in alpha_dict:\n            row = np.zeros(K * (n + 1))\n            row[coeff_index(0, m, n)] = factorial(m)\n            rows.append(row)\n            rhs.append(float(alpha_dict[m]))\n    # At tK: sum_{j=m..n} c_{K-1,j} * j!/(j-m)! * h_{K-1}^{j-m} = beta_m\n    for m in range(1, min(r, n + 1)):\n        if m in beta_dict:\n            row = np.zeros(K * (n + 1))\n            h_last = h[-1]\n            for j in range(m, n + 1):\n                row[coeff_index(K - 1, j, n)] = factorial(j) / factorial(j - m) * (h_last ** (j - m))\n            rows.append(row)\n            rhs.append(float(beta_dict[m]))\n\n    A = np.vstack(rows) if rows else np.zeros((0, K * (n + 1)))\n    b = np.asarray(rhs, dtype=float)\n    return A, b\n\ndef solve_case(times, y_vals, r, n, alpha_dict, beta_dict):\n    A, b = assemble_system(times, y_vals, r, n, alpha_dict, beta_dict)\n    # Least-squares solve; numpy returns minimum-norm solution for underdetermined via SVD\n    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n    return x\n\ndef format_solution(x, decimals=6):\n    x_round = np.round(x.astype(float), decimals=decimals)\n    return [float(v) for v in x_round.tolist()]\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A\n    r_A = 3\n    n_A = 5\n    times_A = [0.0, 2.0]\n    y_A = [0.0, 1.0]\n    alpha_A = {1: 0.0, 2: 0.0}\n    beta_A = {1: 0.0, 2: 0.0}\n\n    # Case B\n    r_B = 2\n    n_B = 3\n    times_B = [0.0, 1.0, 3.0]\n    y_B = [0.0, 1.0, 0.0]\n    alpha_B = {1: 0.0}\n    beta_B = {1: 0.0}\n\n    # Case C\n    r_C = 2\n    n_C = 3\n    times_C = [0.0, 0.5, 1.5, 3.0]\n    y_C = [0.0, 1.0, -1.0, 0.0]\n    alpha_C = {1: 0.5}\n    beta_C = {1: -0.5}\n\n    test_cases = [\n        (times_A, y_A, r_A, n_A, alpha_A, beta_A),\n        (times_B, y_B, r_B, n_B, alpha_B, beta_B),\n        (times_C, y_C, r_C, n_C, alpha_C, beta_C),\n    ]\n\n    results = []\n    for times, y_vals, r, n, alpha, beta in test_cases:\n        x = solve_case(times, y_vals, r, n, alpha, beta)\n        results.append(format_solution(x, decimals=6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```", "id": "2700549"}]}