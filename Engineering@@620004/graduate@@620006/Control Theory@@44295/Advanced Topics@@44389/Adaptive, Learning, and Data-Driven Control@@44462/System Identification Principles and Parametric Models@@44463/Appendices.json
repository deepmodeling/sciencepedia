{"hands_on_practices": [{"introduction": "Real-world systems are rarely disturbed by pure white noise; more often, the disturbances are \"colored,\" exhibiting correlation over time. Parametric models like ARMAX capture this structure by modeling the noise as the output of a filter, often denoted by a polynomial $C(q^{-1})$. This exercise demonstrates the fundamental process of deriving this filter by performing spectral factorization on a given power spectral density (PSD), bridging the gap between frequency-domain observations and a time-domain parametric model. Mastering this technique [@problem_id:2751633] is crucial for understanding and constructing accurate noise models, with a key focus on the minimum-phase constraint that ensures model invertibility.", "problem": "A zero-mean, wide-sense stationary discrete-time noise process with power spectral density (PSD) $\\Phi_{v}(\\omega)$ has been empirically estimated over $\\omega \\in [-\\pi,\\pi]$ (radians) and is modeled by the real, even trigonometric polynomial\n$$\n\\Phi_{v}(\\omega) \\;=\\; 15 \\;+\\; 15.4 \\cos(\\omega) \\;+\\; 2 \\cos(2\\omega).\n$$\nAssume an Autoregressive Moving-Average with Exogenous input (ARMAX) noise model in which the disturbance $v_{t}$ is generated by passing a white noise sequence $e_{t}$ of variance $\\lambda>0$ through a causal, monic Moving Average (MA) polynomial $C(q^{-1})$ of degree $2$, where $q^{-1}$ denotes the unit delay. Use only fundamental principles of linear time-invariant (LTI) systems and the Wiener–Khinchin theorem to justify that the PSD of $v_{t}$ admits a spectral factorization of the form\n$$\n\\Phi_{v}(\\omega) \\;=\\; \\lambda \\,\\big|C\\!\\left(e^{-j\\omega}\\right)\\big|^{2}.\n$$\nThen, enforce the minimum-phase constraint (all zeros of $C(z^{-1})$ strictly inside the unit disk) to obtain the unique invertible factor. Determine the monic second-order $C(q^{-1})$ and the corresponding $\\lambda>0$ that exactly reproduce the given $\\Phi_{v}(\\omega)$. Discuss the invertibility constraint in terms of the location of the zeros of $C(z^{-1})$ and verify it for your solution.\n\nReport only the polynomial $C(q^{-1})$ as your final answer. No rounding is required, and the angles are in radians.", "solution": "The user-provided problem statement has been evaluated and is deemed valid. It is a well-posed problem in the field of time series analysis and system identification, grounded in established scientific principles such as the Wiener–Khinchin theorem and spectral factorization. The data provided are consistent and sufficient for deriving a unique solution under the specified constraints.\n\nThe problem requires the determination of a Moving Average (MA) noise model for a stochastic process $v_t$ with a given Power Spectral Density (PSD), $\\Phi_{v}(\\omega)$. The disturbance $v_t$ is modeled as the output of a linear time-invariant (LTI) filter with transfer function $C(q^{-1})$ driven by a white noise process $e_t$ with variance $\\lambda$. The transfer function corresponds to a monic, second-order polynomial $C(q^{-1}) = 1 + c_1 q^{-1} + c_2 q^{-2}$.\n\nFirst, we justify the spectral factorization form. For a discrete-time LTI system with frequency response $H(e^{-j\\omega})$, the relationship between the PSD of the input signal, $\\Phi_{in}(\\omega)$, and the PSD of the output signal, $\\Phi_{out}(\\omega)$, is given by\n$$\n\\Phi_{out}(\\omega) = |H(e^{-j\\omega})|^2 \\Phi_{in}(\\omega).\n$$\nIn this case, the system is the MA filter, so $H(e^{-j\\omega}) = C(e^{-j\\omega})$, and the input is a white noise sequence $e_t$ with zero mean and variance $\\lambda$. The PSD of a white noise process is constant across all frequencies, equal to its variance, so $\\Phi_{e}(\\omega) = \\lambda$. Therefore, the PSD of the output process $v_t$ is\n$$\n\\Phi_{v}(\\omega) = |C(e^{-j\\omega})|^2 \\lambda,\n$$\nwhich justifies the form provided in the problem statement.\n\nThe given PSD is\n$$\n\\Phi_{v}(\\omega) = 15 + 15.4 \\cos(\\omega) + 2 \\cos(2\\omega).\n$$\nTo perform spectral factorization, we express $\\Phi_{v}(\\omega)$ as a rational function in the complex variable $z$, by substituting $z = e^{j\\omega}$ and using Euler's formula: $\\cos(k\\omega) = \\frac{e^{jk\\omega} + e^{-jk\\omega}}{2} = \\frac{z^k + z^{-k}}{2}$.\n$$\n\\Phi_{v}(z) = 15 + 15.4 \\left(\\frac{z + z^{-1}}{2}\\right) + 2 \\left(\\frac{z^2 + z^{-2}}{2}\\right)\n$$\n$$\n\\Phi_{v}(z) = 15 + 7.7(z + z^{-1}) + (z^2 + z^{-2}) = z^2 + 7.7z + 15 + 7.7z^{-1} + z^{-2}.\n$$\nThis is a Laurent polynomial. To find its roots, we can analyze the associated polynomial $P(z) = z^2 \\Phi_{v}(z)$:\n$$\nP(z) = z^4 + 7.7z^3 + 15z^2 + 7.7z + 1.\n$$\nThe factorization of the PSD takes the form $\\Phi_{v}(z) = \\lambda C(z^{-1}) C(z)$. For a monic second-order polynomial $C(z^{-1}) = 1+c_1z^{-1}+c_2z^{-2}$, we have $C(z) = 1+c_1z+c_2z^2$. The product is:\n$$\n\\lambda C(z^{-1}) C(z) = \\lambda(1+c_1z^{-1}+c_2z^{-2})(1+c_1z+c_2z^2) = \\frac{\\lambda}{z^2}(z^2+c_1z+c_2)(1+c_1z+c_2z^2).\n$$\nMultiplying by $z^2$ gives $z^2\\Phi_v(z) = \\lambda(z^2+c_1z+c_2)(1+c_1z+c_2z^2)$. The roots of this polynomial must be the roots of $P(z)$.\nSince $P(z)$ is a reciprocal polynomial, we can solve $P(z)=0$ by dividing by $z^2$ (as $z=0$ is not a root) and substituting $y = z + z^{-1}$.\n$$\n(z^2 + z^{-2}) + 7.7(z + z^{-1}) + 15 = 0\n$$\nWith $y = z+z^{-1}$, it follows that $y^2 = z^2+2+z^{-2}$, so $z^2+z^{-2} = y^2-2$. The equation becomes:\n$$\n(y^2-2) + 7.7y + 15 = 0 \\implies y^2 + 7.7y + 13 = 0.\n$$\nWe solve this quadratic equation for $y$:\n$$\ny = \\frac{-7.7 \\pm \\sqrt{7.7^2 - 4(1)(13)}}{2} = \\frac{-7.7 \\pm \\sqrt{59.29 - 52}}{2} = \\frac{-7.7 \\pm \\sqrt{7.29}}{2}.\n$$\nSince $\\sqrt{7.29}=2.7$, the roots are:\n$$\ny_1 = \\frac{-7.7 - 2.7}{2} = -5.2, \\quad y_2 = \\frac{-7.7 + 2.7}{2} = -2.5.\n$$\nNow, we find the roots of $P(z)$ by solving $z+z^{-1}=y$ for each value of $y$, which is equivalent to solving the quadratic equation $z^2-yz+1=0$.\nFor $y_1 = -5.2$:\n$$\nz^2 + 5.2z + 1 = 0 \\implies z = \\frac{-5.2 \\pm \\sqrt{5.2^2 - 4}}{2} = \\frac{-5.2 \\pm \\sqrt{23.04}}{2} = \\frac{-5.2 \\pm 4.8}{2}.\n$$\nThis gives roots $z_1 = -5$ and $z_2 = -0.2$.\n\nFor $y_2 = -2.5$:\n$$\nz^2 + 2.5z + 1 = 0 \\implies z = \\frac{-2.5 \\pm \\sqrt{2.5^2 - 4}}{2} = \\frac{-2.5 \\pm \\sqrt{2.25}}{2} = \\frac{-2.5 \\pm 1.5}{2}.\n$$\nThis gives roots $z_3 = -2$ and $z_4 = -0.5$.\n\nThe four roots of $P(z)$ are $\\{-5, -0.2, -2, -0.5\\}$. These are the zeros of the overall spectral function $\\Phi_v(z)$. These zeros appear in reciprocal pairs $(\\rho, 1/\\rho)$: $(-5, -0.2)$ and $(-2, -0.5)$.\n\nThe problem requires an \"invertible factor\" $C(q^{-1})$, which is ensured by the minimum-phase constraint that all zeros of the transfer function $C(z^{-1})$ must lie strictly inside the unit disk $|z|<1$. The zeros of $C(z^{-1})=1+c_1z^{-1}+c_2z^{-2}$ are the roots of the polynomial $z^2+c_1z+c_2=0$.\nTo construct the minimum-phase filter, we must choose the roots from the set $\\{-5, -0.2, -2, -0.5\\}$ that lie inside the unit circle. These are $z=-0.2$ and $z=-0.5$.\nThe polynomial whose roots are these minimum-phase zeros is:\n$$\n(z - (-0.2))(z - (-0.5)) = (z+0.2)(z+0.5) = z^2 + 0.7z + 0.1.\n$$\nBy comparing this to the form $z^2+c_1z+c_2$, we identify the coefficients of the MA polynomial:\n$$\nc_1 = 0.7, \\quad c_2 = 0.1.\n$$\nThus, the desired monic, second-order MA polynomial is:\n$$\nC(q^{-1}) = 1 + 0.7q^{-1} + 0.1q^{-2}.\n$$\nThe invertibility constraint is satisfied because the zeros of $C(x)=1+0.7x+0.1x^2$ where $x=z^{-1}$ are $x=-2$ and $x=-5$. Since both zeros are outside the unit circle, the filter $1/C(q^{-1})$ is stable and causal, making the MA model $v_t=C(q^{-1})e_t$ invertible.\n\nFinally, we determine the variance $\\lambda$ of the white noise process $e_t$. We equate the expression for the factored PSD with the given one.\n$$\n\\Phi_{v}(z) = \\lambda C(z^{-1})C(z) = \\lambda(1+c_1z^{-1}+c_2z^{-2})(1+c_1z+c_2z^2)\n$$\n$$\n= \\lambda(c_2z^{-2} + (c_1+c_1c_2)z^{-1} + (1+c_1^2+c_2^2) + (c_1+c_1c_2)z + c_2z^2).\n$$\nIn trigonometric form, this is:\n$$\n\\Phi_{v}(\\omega) = \\lambda(1+c_1^2+c_2^2) + 2\\lambda c_1(1+c_2)\\cos(\\omega) + 2\\lambda c_2 \\cos(2\\omega).\n$$\nComparing this with the given $\\Phi_v(\\omega) = 15 + 15.4\\cos(\\omega) + 2\\cos(2\\omega)$, we compare the coefficients of $\\cos(2\\omega)$:\n$$\n2\\lambda c_2 = 2 \\implies \\lambda c_2 = 1.\n$$\nSubstituting $c_2=0.1$:\n$$\n\\lambda (0.1) = 1 \\implies \\lambda = 10.\n$$\nSince $\\lambda=10 > 0$, this is a valid variance. We can verify this with the other coefficients:\n- Constant term: $\\lambda(1+c_1^2+c_2^2) = 10(1+0.7^2+0.1^2) = 10(1+0.49+0.01) = 10(1.5) = 15$. Correct.\n- $\\cos(\\omega)$ term: $2\\lambda c_1(1+c_2) = 2(10)(0.7)(1+0.1) = 14(1.1) = 15.4$. Correct.\n\nThe solution is consistent. The unique, invertible, monic, second-order polynomial is $C(q^{-1})=1+0.7q^{-1}+0.1q^{-2}$.", "answer": "$$\n\\boxed{1 + 0.7q^{-1} + 0.1q^{-2}}\n$$", "id": "2751633"}, {"introduction": "The Prediction Error Method (PEM) is a cornerstone of system identification, where model parameters are found by minimizing the variance of the one-step-ahead prediction errors. At the heart of any PEM algorithm is the iterative computation of these prediction errors, $\\varepsilon(t, \\theta)$, for a given candidate parameter vector $\\theta$. This practice [@problem_id:2751638] moves from theory to implementation, showing you how to construct an efficient computational method for these errors using a cascade of digital filters. By analyzing the process for an ARMAX model, you will gain a practical understanding of how these powerful estimation algorithms work under the hood and appreciate the computational complexity involved.", "problem": "Consider an AutoRegressive Moving Average with eXogenous input (ARMAX) model of a scalar, discrete-time, linear time-invariant system given by\n$$\nA(q^{-1})\\,y(t) \\;=\\; B(q^{-1})\\,q^{-n_k}\\,u(t) \\;+\\; C(q^{-1})\\,e(t),\n$$\nwhere $q^{-1}$ is the backward shift operator, $A(q^{-1}) = 1 + a_{1} q^{-1} + \\cdots + a_{n_a} q^{-n_a}$ and $C(q^{-1}) = 1 + c_{1} q^{-1} + \\cdots + c_{n_c} q^{-n_c}$ are monic, stable polynomials with all zeros strictly inside the unit circle, $B(q^{-1}) = b_{1} q^{-1} + \\cdots + b_{n_b} q^{-n_b}$ is a strictly proper polynomial, and $n_k \\in \\mathbb{Z}_{\\ge 0}$ is a known input delay. You are given a finite dataset $\\{u(t),y(t)\\}_{t=1}^{N}$ and an initial parameter vector $\\theta^{(0)} = \\{a_i^{(0)}\\}_{i=1}^{n_a} \\cup \\{b_j^{(0)}\\}_{j=1}^{n_b} \\cup \\{c_\\ell^{(0)}\\}_{\\ell=1}^{n_c}$ such that $A(\\cdot)$ and $C(\\cdot)$ are invertible as described.\n\nStarting from the defining equation above and fundamental properties of linear time-invariant filtering and shift operators, do the following:\n\n- Construct a computational method that, given $\\theta^{(0)}$, produces the $1$-step-ahead prediction errors $\\varepsilon(t,\\theta^{(0)})$ for all $t \\in \\{1,\\dots,N\\}$ by cascading stable causal filters and shifts, without forming any dense regression matrices or performing any explicit deconvolutions by polynomial long division.\n- Under a cost model that counts only scalar multiplications and treats additions/subtractions and memory accesses as free, and that implements each causal filter in direct form with fixed loop bounds independent of $t$, derive the exact total number of scalar multiplications required to compute $\\{\\varepsilon(t,\\theta^{(0)})\\}_{t=1}^{N}$ as a function of $N$, $n_a$, $n_b$, and $n_c$. Assume zero initial conditions outside the measurement window, and that multiplications by the monic coefficients in $A(\\cdot)$ and $C(\\cdot)$ are not performed.\n\nProvide your final result as a single closed-form expression in $N$, $n_a$, $n_b$, and $n_c$. No numerical evaluation is required, and no rounding is needed. The final answer must be this expression alone.", "solution": "The problem requires the construction of a computational method to find the $1$-step-ahead prediction errors for an AutoRegressive Moving Average with eXogenous input (ARMAX) model and the derivation of its computational cost in terms of scalar multiplications. The analysis will proceed from first principles.\n\nThe ARMAX model is given by the equation:\n$$\nA(q^{-1})\\,y(t) \\;=\\; B(q^{-1})\\,q^{-n_k}\\,u(t) \\;+\\; C(q^{-1})\\,e(t)\n$$\nwhere $y(t)$ is the system output, $u(t)$ is the exogenous input, and $e(t)$ is a white noise sequence. The polynomials $A(q^{-1})$, $B(q^{-1})$, and $C(q^{-1})$ are defined as:\n$$\nA(q^{-1}) = 1 + a_{1} q^{-1} + \\cdots + a_{n_a} q^{-n_a}\n$$\n$$\nB(q^{-1}) = b_{1} q^{-1} + \\cdots + b_{n_b} q^{-n_b}\n$$\n$$\nC(q^{-1}) = 1 + c_{1} q^{-1} + \\cdots + c_{n_c} q^{-n_c}\n$$\nThe $1$-step-ahead prediction error, denoted $\\varepsilon(t, \\theta)$, is the value of the innovation $e(t)$ computed using a given parameter vector $\\theta$. For this problem, we use the specified initial parameter vector $\\theta^{(0)}$. For notational simplicity in the derivation, we will write $\\varepsilon(t)$ for $\\varepsilon(t, \\theta^{(0)})$ and use the coefficients $\\{a_i\\}, \\{b_j\\}, \\{c_\\ell\\}$ from $\\theta^{(0)}$.\n\nTo find an expression for $\\varepsilon(t)$, we rearrange the model equation to solve for $e(t)$:\n$$\nC(q^{-1})\\,e(t) \\;=\\; A(q^{-1})\\,y(t) \\;-\\; B(q^{-1})\\,q^{-n_k}\\,u(t)\n$$\nThe problem states that the polynomial $C(q^{-1})$ is stable, which means all its roots are strictly inside the unit circle. This guarantees that its inverse, $C(q^{-1})^{-1}$, represents a stable and causal linear time-invariant filter. Thus, we can formally write:\n$$\n\\varepsilon(t) = e(t) = \\frac{1}{C(q^{-1})} \\left( A(q^{-1})\\,y(t) - B(q^{-1})\\,q^{-n_k}\\,u(t) \\right)\n$$\nThis expression describes the computation of $\\varepsilon(t)$ as a filtering operation. The problem specifies a method based on cascading stable filters. A direct and computationally efficient implementation of this expression is to first compute the signal inside the parentheses and then apply the filter $1/C(q^{-1})$. This exploits the linearity of the operators to avoid redundant computations, specifically applying the filter $1/C(q^{-1})$ to $y(t)$ and $u(t)$ separately.\n\nThe computational method is structured as a two-stage cascade:\nStage 1: Compute an intermediate signal, let us call it $v(t)$, which is the result of applying the FIR filters $A(q^{-1})$ and $B(q^{-1})$ to the signals $y(t)$ and $u(t-n_k)$ respectively.\n$$\nv(t) = A(q^{-1})\\,y(t) - B(q^{-1})\\,u(t-n_k)\n$$\nStage 2: Compute the prediction error $\\varepsilon(t)$ by applying the IIR filter $1/C(q^{-1})$ to the intermediate signal $v(t)$.\n$$\n\\varepsilon(t) = \\frac{1}{C(q^{-1})} v(t)\n$$\nWe now expand these two stages into their explicit recursive forms to analyze the computational cost.\n\nFor Stage 1, we expand the polynomial operators:\n$$\nv(t) = \\left(1 + \\sum_{i=1}^{n_a} a_i q^{-i}\\right) y(t) - \\left(\\sum_{j=1}^{n_b} b_j q^{-j}\\right) u(t-n_k)\n$$\nApplying the definition of the backward shift operator $q^{-k}x(t) = x(t-k)$, we get the computational formula for $v(t)$:\n$$\nv(t) = y(t) + \\sum_{i=1}^{n_a} a_i y(t-i) - \\sum_{j=1}^{n_b} b_j u(t-n_k-j)\n$$\nFor Stage 2, the relation $\\varepsilon(t) = v(t)/C(q^{-1})$ is equivalent to $C(q^{-1})\\varepsilon(t) = v(t)$. Expanding this gives:\n$$\n\\left(1 + \\sum_{\\ell=1}^{n_c} c_\\ell q^{-\\ell}\\right) \\varepsilon(t) = v(t)\n$$\nSolving for $\\varepsilon(t)$ provides the recursive formula for the second stage:\n$$\n\\varepsilon(t) = v(t) - \\sum_{\\ell=1}^{n_c} c_\\ell \\varepsilon(t-\\ell)\n$$\nNow, we determine the computational cost based on the provided model which counts only scalar multiplications. We assume zero initial conditions for $t \\le 0$ and that filter implementations use fixed loop bounds for $t \\in \\{1, \\dots, N\\}$.\n\nAt each time step $t$, the cost is as follows:\n1.  **Cost to compute $v(t)$**: The expression is $v(t) = y(t) + \\sum_{i=1}^{n_a} a_i y(t-i) - \\sum_{j=1}^{n_b} b_j u(t-n_k-j)$.\n    - The first summation, $\\sum_{i=1}^{n_a} a_i y(t-i)$, involves $n_a$ products of the form $a_i \\times y(t-i)$. This requires $n_a$ scalar multiplications. The term $y(t)$ corresponds to the monic part of $A(q^{-1})$ and, as per the rules, multiplication by $1$ is not counted.\n    - The second summation, $\\sum_{j=1}^{n_b} b_j u(t-n_k-j)$, involves $n_b$ products of the form $b_j \\times u(t-n_k-j)$. This requires $n_b$ scalar multiplications.\n    The total number of multiplications to compute $v(t)$ is $n_a + n_b$.\n\n2.  **Cost to compute $\\varepsilon(t)$**: The expression is $\\varepsilon(t) = v(t) - \\sum_{\\ell=1}^{n_c} c_\\ell \\varepsilon(t-\\ell)$.\n    - The summation $\\sum_{\\ell=1}^{n_c} c_\\ell \\varepsilon(t-\\ell)$ involves $n_c$ products of the form $c_\\ell \\times \\varepsilon(t-\\ell)$. This requires $n_c$ scalar multiplications. The term $v(t)$ is related to the monic part of $C(q^{-1})$ and involves no multiplication in this step.\n    The total number of multiplications to compute $\\varepsilon(t)$ from $v(t)$ is $n_c$.\n\nThe total number of scalar multiplications per time step $t$ is the sum of the costs from both stages:\n$$\n\\text{Multiplications per step} = (n_a + n_b) + n_c = n_a + n_b + n_c\n$$\nThis cost is constant for each time step from $t=1$ to $t=N$ because the problem specifies fixed loop bounds for the filter implementation. The input delay $n_k$ influences which data points of $u(t)$ are used but does not change the number of multiplications.\n\nTo find the total number of scalar multiplications required to compute the entire sequence $\\{\\varepsilon(t)\\}_{t=1}^{N}$, we multiply the per-step cost by the total number of steps, $N$.\n$$\n\\text{Total Multiplications} = N \\times (n_a + n_b + n_c)\n$$\nThis is the final closed-form expression for the total computational cost.", "answer": "$$\n\\boxed{N (n_a + n_b + n_c)}\n$$", "id": "2751638"}, {"introduction": "Arriving at a parameter estimate is a significant step, but it is incomplete without a measure of its reliability. To build robust controllers or make reliable predictions, we must quantify the uncertainty of our model. This practice [@problem_id:2751642] delves into the statistical properties of the widely used least-squares estimator for ARX models. You will derive the asymptotic covariance matrix of the parameter estimates, a result that forms the basis for calculating confidence intervals and understanding how factors like noise variance and input signal properties affect the final precision of your identified model.", "problem": "Consider the AutoRegressive with eXogenous input (ARX) model of order $\\left(2,1\\right)$ given by\n$$\ny(k) \\;=\\; -a_{1}\\,y(k-1)\\;-\\;a_{2}\\,y(k-2)\\;+\\;b_{1}\\,u(k-1)\\;+\\;e(k),\n$$\nwhere $e(k)$ is a zero-mean, independent and identically distributed (i.i.d.) disturbance with variance $\\sigma_{e}^{2}$ and is independent of the regressor process. Define the parameter vector $\\theta_{0}=\\begin{pmatrix}a_{1} & a_{2} & b_{1}\\end{pmatrix}^{\\top}$ and the regressor $\\varphi(k)=\\begin{pmatrix}-y(k-1) & -y(k-2) & u(k-1)\\end{pmatrix}^{\\top}$ so that $y(k)=\\varphi(k)^{\\top}\\theta_{0}+e(k)$. The least-squares (LS) estimator based on $N$ samples is\n$$\n\\hat{\\theta}_{N} \\;=\\; \\arg\\min_{\\theta}\\sum_{k=1}^{N}\\big(y(k)-\\varphi(k)^{\\top}\\theta\\big)^{2}.\n$$\nAssume the standard regularity conditions for consistency and asymptotic normality of least squares in linear regression with stochastic regressors hold: the input $u(k)$ is stationary and persistently exciting of order at least $3$, the joint process $\\{\\varphi(k),e(k)\\}$ is stationary and ergodic, $\\mathbb{E}\\big[\\varphi(k)\\,e(k)\\big]=0$, and $\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}=R_{\\varphi}$ exists and is positive definite.\n\n1) Starting from the definition of the least-squares estimator and these assumptions, derive the asymptotic covariance of the scaled estimation error $\\sqrt{N}\\big(\\hat{\\theta}_{N}-\\theta_{0}\\big)$ and show that it equals\n$$\n\\Sigma_{\\theta} \\;=\\; \\sigma_{e}^{2}\\,R_{\\varphi}^{-1}.\n$$\nExplain how to estimate $\\sigma_{e}^{2}$ from the residuals $\\hat{e}(k)=y(k)-\\varphi(k)^{\\top}\\hat{\\theta}_{N}$ using an unbiased variance estimator that accounts for the number of estimated parameters.\n\n2) In an experiment with $N=1000$ samples and $n_{p}=3$ estimated parameters, suppose the limit $R_{\\varphi}=\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}$ is known to be\n$$\nR_{\\varphi} \\;=\\;\n\\begin{pmatrix}\n2.0 & 0.5 & 0.0\\\\\n0.5 & 1.5 & 0.2\\\\\n0.0 & 0.2 & 3.0\n\\end{pmatrix},\n$$\nand the residual sum of squares is $\\sum_{k=1}^{N}\\hat{e}(k)^{2}=997$. Using the unbiased residual variance estimator you described, compute the numerical matrix for the asymptotic covariance $\\Sigma_{\\theta}$ of the scaled error $\\sqrt{N}\\big(\\hat{\\theta}_{N}-\\theta_{0}\\big)$. Round each entry of your final matrix to four significant figures. Provide your final answer as a single matrix without units.", "solution": "The problem presented is a standard exercise in the asymptotic analysis of least-squares estimators for dynamic systems, specifically an ARX model. It is scientifically grounded, well-posed, and contains all necessary information. The problem is valid. We proceed with the solution in two parts.\n\nPart 1: Derivation of the asymptotic covariance and the unbiased variance estimator.\n\nThe least-squares estimator $\\hat{\\theta}_{N}$ minimizes the sum of squared errors, $J(\\theta) = \\sum_{k=1}^{N}(y(k)-\\varphi(k)^{\\top}\\theta)^{2}$. The first-order condition for the minimum is $\\nabla_{\\theta}J(\\theta)|_{\\theta=\\hat{\\theta}_{N}} = 0$. This gives the normal equations:\n$$\n\\sum_{k=1}^{N}\\varphi(k)\\big(y(k)-\\varphi(k)^{\\top}\\hat{\\theta}_{N}\\big) \\;=\\; 0\n$$\n$$\n\\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)\\hat{\\theta}_{N} \\;=\\; \\sum_{k=1}^{N}\\varphi(k)y(k)\n$$\nThe true system is given by $y(k)=\\varphi(k)^{\\top}\\theta_{0}+e(k)$. Substituting this into the normal equations:\n$$\n\\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)\\hat{\\theta}_{N} \\;=\\; \\sum_{k=1}^{N}\\varphi(k)\\big(\\varphi(k)^{\\top}\\theta_{0}+e(k)\\big)\n$$\n$$\n\\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)\\hat{\\theta}_{N} \\;=\\; \\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)\\theta_{0} \\;+\\; \\sum_{k=1}^{N}\\varphi(k)e(k)\n$$\nRearranging the terms to isolate the estimation error $\\hat{\\theta}_{N}-\\theta_{0}$:\n$$\n\\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)(\\hat{\\theta}_{N}-\\theta_{0}) \\;=\\; \\sum_{k=1}^{N}\\varphi(k)e(k)\n$$\n$$\n\\hat{\\theta}_{N}-\\theta_{0} \\;=\\; \\left(\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)^{-1}\\left(\\sum_{k=1}^{N}\\varphi(k)e(k)\\right)\n$$\nTo analyze the asymptotic distribution, we scale the error by $\\sqrt{N}$:\n$$\n\\sqrt{N}(\\hat{\\theta}_{N}-\\theta_{0}) \\;=\\; \\left(\\frac{1}{N}\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)^{-1}\\left(\\frac{1}{\\sqrt{N}}\\sum_{k=1}^{N}\\varphi(k)e(k)\\right)\n$$\nWe analyze the two terms on the right-hand side as $N\\to\\infty$.\nFor the first term, by the assumption that the process is ergodic and the limit exists, the Law of Large Numbers implies:\n$$\n\\frac{1}{N}\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top} \\;\\xrightarrow{p}\\; \\mathbb{E}\\big[\\varphi(k)\\varphi(k)^{\\top}\\big] \\;=\\; R_{\\varphi}\n$$\nwhere $\\xrightarrow{p}$ denotes convergence in probability. Since matrix inversion is a continuous function, the Continuous Mapping Theorem gives:\n$$\n\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\varphi(k)\\varphi(k)^{\\top}\\right)^{-1} \\;\\xrightarrow{p}\\; R_{\\varphi}^{-1}\n$$\nFor the second term, we consider the sum $S_{N} = \\frac{1}{\\sqrt{N}}\\sum_{k=1}^{N}\\varphi(k)e(k)$. The vector process $v(k)=\\varphi(k)e(k)$ has zero mean, as $\\mathbb{E}[v(k)] = \\mathbb{E}[\\varphi(k)e(k)] = 0$ by assumption. Furthermore, $v(k)$ is a martingale difference sequence with respect to the filtration $\\mathcal{F}_{k-1}$ generated by $\\{e(j), u(j)\\}_{j \\le k-1}$, since $\\mathbb{E}[v(k) | \\mathcal{F}_{k-1}] = \\varphi(k)\\mathbb{E}[e(k) | \\mathcal{F}_{k-1}] = \\varphi(k)\\cdot 0 = 0$.\nThe Central Limit Theorem for martingale difference sequences states that $S_{N}$ converges in distribution to a normally distributed random vector with zero mean and covariance matrix $Q$:\n$$\n\\frac{1}{\\sqrt{N}}\\sum_{k=1}^{N}\\varphi(k)e(k) \\;\\xrightarrow{d}\\; \\mathcal{N}(0, Q)\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and the covariance matrix $Q$ is given by:\n$$\nQ \\;=\\; \\lim_{N\\to\\infty} \\frac{1}{N}\\sum_{k=1}^{N}\\mathbb{E}\\big[(\\varphi(k)e(k))(\\varphi(k)e(k))^{\\top}\\big] \\;=\\; \\mathbb{E}\\big[\\varphi(k)\\varphi(k)^{\\top}e(k)^{2}\\big]\n$$\nSince $e(k)$ is independent of the regressor $\\varphi(k)$ (which depends on past data), we can separate the expectation:\n$$\nQ \\;=\\; \\mathbb{E}\\big[\\varphi(k)\\varphi(k)^{\\top}\\big]\\,\\mathbb{E}\\big[e(k)^{2}\\big] \\;=\\; R_{\\varphi}\\,\\sigma_{e}^{2}\n$$\nCombining the two results using Slutsky's theorem, we find the asymptotic distribution of the scaled estimation error:\n$$\n\\sqrt{N}(\\hat{\\theta}_{N}-\\theta_{0}) \\;\\xrightarrow{d}\\; R_{\\varphi}^{-1} \\cdot \\mathcal{N}(0, \\sigma_{e}^{2}R_{\\varphi})\n$$\nThe asymptotic covariance of $\\sqrt{N}(\\hat{\\theta}_{N}-\\theta_{0})$ is the covariance of the resulting normal distribution. Let $X \\sim \\mathcal{N}(0, \\sigma_{e}^{2}R_{\\varphi})$. The covariance of $R_{\\varphi}^{-1}X$ is:\n$$\n\\text{Cov}(R_{\\varphi}^{-1}X) \\;=\\; R_{\\varphi}^{-1}\\,\\text{Cov}(X)\\,(R_{\\varphi}^{-1})^{\\top} \\;=\\; R_{\\varphi}^{-1}(\\sigma_{e}^{2}R_{\\varphi})(R_{\\varphi}^{-1})^{\\top}\n$$\nSince $R_{\\varphi}$ is symmetric, $R_{\\varphi}^{-1}$ is also symmetric, so $(R_{\\varphi}^{-1})^{\\top} = R_{\\varphi}^{-1}$.\n$$\n\\Sigma_{\\theta} \\;=\\; \\sigma_{e}^{2}R_{\\varphi}^{-1}R_{\\varphi}R_{\\varphi}^{-1} \\;=\\; \\sigma_{e}^{2}R_{\\varphi}^{-1}\n$$\nThis completes the first part of the derivation.\n\nFor the second part, we must find an unbiased estimator for the noise variance $\\sigma_{e}^{2}$. The sum of squared residuals (SSR) is $\\sum_{k=1}^{N}\\hat{e}(k)^{2}$, where $\\hat{e}(k)=y(k)-\\varphi(k)^{\\top}\\hat{\\theta}_{N}$. A naive estimator $\\frac{1}{N}\\sum_{k=1}^{N}\\hat{e}(k)^{2}$ is biased. The unbiased estimator corrects for the number of parameters, $n_{p}$, estimated from the data. The expectation of the SSR is $\\mathbb{E}\\big[\\sum_{k=1}^{N}\\hat{e}(k)^{2}\\big] = (N-n_{p})\\sigma_{e}^{2}$. This is a standard result from linear regression theory, where $n_{p}$ represents the degrees of freedom consumed by fitting the model parameters. Therefore, an unbiased estimator for $\\sigma_{e}^{2}$ is:\n$$\n\\hat{\\sigma}_{e}^{2} \\;=\\; \\frac{1}{N-n_{p}}\\sum_{k=1}^{N}\\hat{e}(k)^{2}\n$$\n\nPart 2: Numerical Calculation.\n\nWe are given the following numerical values:\nNumber of samples $N=1000$.\nNumber of parameters $n_{p}=3$.\nResidual sum of squares $\\sum_{k=1}^{N}\\hat{e}(k)^{2} = 997$.\nThe regressor covariance matrix is given as:\n$$\nR_{\\varphi} \\;=\\;\n\\begin{pmatrix}\n2.0 & 0.5 & 0.0\\\\\n0.5 & 1.5 & 0.2\\\\\n0.0 & 0.2 & 3.0\n\\end{pmatrix}\n$$\nFirst, we compute the unbiased estimate of the noise variance $\\sigma_{e}^{2}$ using the formula derived above:\n$$\n\\hat{\\sigma}_{e}^{2} \\;=\\; \\frac{\\sum_{k=1}^{N}\\hat{e}(k)^{2}}{N-n_{p}} \\;=\\; \\frac{997}{1000-3} \\;=\\; \\frac{997}{997} \\;=\\; 1\n$$\nNow, we compute the asymptotic covariance matrix $\\Sigma_{\\theta} = \\hat{\\sigma}_{e}^{2}R_{\\varphi}^{-1} = 1 \\cdot R_{\\varphi}^{-1}$. We need to find the inverse of $R_{\\varphi}$. For a $3 \\times 3$ matrix $A$, the inverse is $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$.\nThe determinant of $R_{\\varphi}$ is:\n$$\n\\det(R_{\\varphi}) \\;=\\; 2.0\\big((1.5)(3.0)-(0.2)(0.2)\\big) - 0.5\\big((0.5)(3.0)-(0.2)(0.0)\\big) + 0.0\\big(\\dots\\big)\n$$\n$$\n\\det(R_{\\varphi}) \\;=\\; 2.0(4.5-0.04) - 0.5(1.5) \\;=\\; 2.0(4.46) - 0.75 \\;=\\; 8.92 - 0.75 \\;=\\; 8.17\n$$\nThe adjugate matrix, $\\text{adj}(R_{\\varphi})$, is the transpose of the cofactor matrix. Since $R_{\\varphi}$ is symmetric, its cofactor matrix is also symmetric, thus $\\text{adj}(R_{\\varphi})$ is the cofactor matrix itself.\nThe cofactors are:\n$C_{11} = +(1.5 \\cdot 3.0 - 0.2 \\cdot 0.2) = 4.46$\n$C_{12} = -(0.5 \\cdot 3.0 - 0.2 \\cdot 0.0) = -1.5$\n$C_{13} = +(0.5 \\cdot 0.2 - 1.5 \\cdot 0.0) = 0.1$\n$C_{21} = C_{12} = -1.5$\n$C_{22} = +(2.0 \\cdot 3.0 - 0.0 \\cdot 0.0) = 6.0$\n$C_{23} = -(2.0 \\cdot 0.2 - 0.5 \\cdot 0.0) = -0.4$\n$C_{31} = C_{13} = 0.1$\n$C_{32} = C_{23} = -0.4$\n$C_{33} = +(2.0 \\cdot 1.5 - 0.5 \\cdot 0.5) = 2.75$\nSo, the adjugate matrix is:\n$$\n\\text{adj}(R_{\\varphi}) \\;=\\;\n\\begin{pmatrix}\n4.46 & -1.5 & 0.1\\\\\n-1.5 & 6.0 & -0.4\\\\\n0.1 & -0.4 & 2.75\n\\end{pmatrix}\n$$\nNow, we find $R_{\\varphi}^{-1}$:\n$$\nR_{\\varphi}^{-1} \\;=\\; \\frac{1}{8.17}\n\\begin{pmatrix}\n4.46 & -1.5 & 0.1\\\\\n-1.5 & 6.0 & -0.4\\\\\n0.1 & -0.4 & 2.75\n\\end{pmatrix}\n$$\n$$\nR_{\\varphi}^{-1} \\;=\\;\n\\begin{pmatrix}\n\\frac{4.46}{8.17} & \\frac{-1.5}{8.17} & \\frac{0.1}{8.17}\\\\\n\\frac{-1.5}{8.17} & \\frac{6.0}{8.17} & \\frac{-0.4}{8.17}\\\\\n\\frac{0.1}{8.17} & \\frac{-0.4}{8.17} & \\frac{2.75}{8.17}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0.5458996\\dots & -0.1835985\\dots & 0.0122399\\dots\\\\\n-0.1835985\\dots & 0.7343941\\dots & -0.0489596\\dots\\\\\n0.0122399\\dots & -0.0489596\\dots & 0.3366009\\dots\n\\end{pmatrix}\n$$\nRounding each element to four significant figures gives the final matrix for $\\Sigma_{\\theta}$:\n$$\n\\Sigma_{\\theta} \\;=\\;\n\\begin{pmatrix}\n0.5459 & -0.1836 & 0.01224\\\\\n-0.1836 & 0.7344 & -0.04896\\\\\n0.01224 & -0.04896 & 0.3366\n\\end{pmatrix}\n$$\nThis is the final numerical result for the asymptotic covariance matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5459 & -0.1836 & 0.01224 \\\\\n-0.1836 & 0.7344 & -0.04896 \\\\\n0.01224 & -0.04896 & 0.3366\n\\end{pmatrix}\n}\n$$", "id": "2751642"}]}