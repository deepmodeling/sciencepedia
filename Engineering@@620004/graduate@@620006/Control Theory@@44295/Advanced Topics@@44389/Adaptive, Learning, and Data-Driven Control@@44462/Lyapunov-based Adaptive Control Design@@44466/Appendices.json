{"hands_on_practices": [{"introduction": "The foundation of many adaptive control schemes is the gradient-based update law, designed to minimize a prediction error. This first exercise guides you through the process of deriving this fundamental law from a steepest-descent principle and proving its stability using a Lyapunov argument. By analyzing the relationship between the adaptation gain $\\gamma$, the properties of the regressor signal, and the resulting convergence rate, you will develop a practical understanding of how to tune an adaptive estimator to meet performance specifications [@problem_id:2722772].", "problem": "Consider a scalar linear-in-parameters regression model used inside an adaptive estimator for a plant with unknown constant parameter. The measured scalar output is modeled as $y(t) = \\theta \\, \\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter and $\\phi(t) \\in \\mathbb{R}$ is a known, time-varying regressor that is continuous and bounded. An estimator produces the prediction $\\hat{y}(t) = \\hat{\\theta}(t) \\, \\phi(t)$ with scalar estimate $\\hat{\\theta}(t) \\in \\mathbb{R}$. Define the prediction error as $e(t) = \\hat{y}(t) - y(t)$. Starting from the steepest-descent principle applied to the instantaneous squared loss $J(t) = \\frac{1}{2} e^{2}(t)$, derive the scalar gradient update law for $\\hat{\\theta}(t)$ and the corresponding parameter error dynamics for $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$. Then, using the direct method of Lyapunov, construct a smooth positive definite Lyapunov candidate depending only on $\\tilde{\\theta}(t)$ and a constant adaptation gain $\\gamma > 0$, and prove that the resulting adaptive law is Lyapunov stable for any $\\gamma > 0$.\n\nNext, assume that the regressor satisfies a uniform pointwise lower bound on its instantaneous energy: there exists a known constant $m > 0$ such that $\\phi^{2}(t) \\ge m$ for all $t \\ge 0$. Using only this bound and your error dynamics, derive a bound of the form $\\lvert \\tilde{\\theta}(t) \\rvert \\le \\lvert \\tilde{\\theta}(0) \\rvert \\exp\\!\\big(-\\alpha t\\big)$ for some rate $\\alpha > 0$ expressed in terms of $\\gamma$ and $m$. You are tasked with meeting a prescribed adaptation rate $\\lambda_{d} > 0$, meaning that the exponential bound is no slower than $\\exp(-\\lambda_{d} t)$ for all $t \\ge 0$. Determine a constant adaptation gain $\\gamma$ that guarantees this requirement while preserving Lyapunov stability.\n\nProvide your final answer as a single, closed-form analytical expression in terms of $\\lambda_{d}$ and $m$. No numerical approximation is required. No units are needed.", "solution": "The problem posed is a standard exercise in the fundamentals of adaptive control theory. It is well-posed, scientifically sound, and contains all necessary information for its resolution. We will proceed with the derivation in a logical, step-by-step manner.\n\nThe system is described by the scalar model $y(t) = \\theta \\, \\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter and $\\phi(t) \\in \\mathbb{R}$ is a known, bounded, and continuous regressor signal. The parameter estimate is $\\hat{\\theta}(t)$, which forms the prediction $\\hat{y}(t) = \\hat{\\theta}(t) \\, \\phi(t)$. The prediction error is $e(t) = \\hat{y}(t) - y(t)$.\n\nFirst, we derive the update law for $\\hat{\\theta}(t)$ using the steepest-descent method to minimize the instantaneous squared error cost function, $J(t) = \\frac{1}{2} e^{2}(t)$. The gradient update law is given by\n$$\n\\dot{\\hat{\\theta}}(t) = - \\gamma \\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)}\n$$\nwhere $\\gamma > 0$ is the constant adaptation gain.\n\nWe compute the partial derivative of $J(t)$ with respect to $\\hat{\\theta}(t)$:\n$$\n\\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)} = \\frac{\\partial}{\\partial \\hat{\\theta}(t)} \\left( \\frac{1}{2} e^{2}(t) \\right) = e(t) \\frac{\\partial e(t)}{\\partial \\hat{\\theta}(t)}\n$$\nThe error $e(t)$ can be expressed as $e(t) = \\hat{\\theta}(t)\\phi(t) - y(t)$. Its partial derivative with respect to $\\hat{\\theta}(t)$ is:\n$$\n\\frac{\\partial e(t)}{\\partial \\hat{\\theta}(t)} = \\frac{\\partial}{\\partial \\hat{\\theta}(t)} (\\hat{\\theta}(t)\\phi(t) - y(t)) = \\phi(t)\n$$\nSubstituting this back, we find the gradient:\n$$\n\\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)} = e(t) \\phi(t)\n$$\nThus, the gradient update law for the parameter estimate is:\n$$\n\\dot{\\hat{\\theta}}(t) = - \\gamma e(t) \\phi(t)\n$$\nNext, we derive the dynamics of the parameter error, defined as $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$. Since $\\theta$ is a constant, its time derivative is zero, so $\\dot{\\tilde{\\theta}}(t) = \\dot{\\hat{\\theta}}(t)$. Substituting the update law and expressing the prediction error in terms of the parameter error, $e(t) = (\\hat{\\theta}(t) - \\theta)\\phi(t) = \\tilde{\\theta}(t)\\phi(t)$, we obtain the parameter error dynamics:\n$$\n\\dot{\\tilde{\\theta}}(t) = - \\gamma \\left( \\tilde{\\theta}(t)\\phi(t) \\right) \\phi(t) = - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t)\n$$\nThis is a linear time-varying ordinary differential equation governing the evolution of the parameter error.\n\nNow, we prove Lyapunov stability. As suggested, we choose a smooth, positive definite Lyapunov function candidate $V$ that depends on $\\tilde{\\theta}(t)$ and $\\gamma$. A suitable choice is:\n$$\nV(\\tilde{\\theta}) = \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t)\n$$\nSince $\\gamma > 0$, $V(\\tilde{\\theta}) > 0$ for all $\\tilde{\\theta} \\neq 0$ and $V(0) = 0$. Thus, $V$ is positive definite. We analyze its time derivative along the trajectories of the error dynamics:\n$$\n\\dot{V}(t) = \\frac{d}{dt} \\left( \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\right) = \\frac{1}{2\\gamma} \\left( 2 \\tilde{\\theta}(t) \\dot{\\tilde{\\theta}}(t) \\right) = \\frac{1}{\\gamma} \\tilde{\\theta}(t) \\dot{\\tilde{\\theta}}(t)\n$$\nSubstituting the error dynamics $\\dot{\\tilde{\\theta}}(t) = - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t)$:\n$$\n\\dot{V}(t) = \\frac{1}{\\gamma} \\tilde{\\theta}(t) \\left( - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t) \\right) = - \\phi^{2}(t) \\tilde{\\theta}^{2}(t)\n$$\nSince $\\phi^{2}(t) \\ge 0$ for all $t$, the time derivative $\\dot{V}(t) \\le 0$ for all $t$. This shows that $V(t)$ is a non-increasing function of time, which implies that $\\tilde{\\theta}(t)$ is bounded. Specifically, $V(t) \\le V(0)$, which means $\\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\le \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(0)$, or $|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)|$. This implies that the equilibrium $\\tilde{\\theta} = 0$ is globally uniformly stable, which satisfies the requirement of Lyapunov stability for any $\\gamma > 0$.\n\nThe second part of the problem assumes a uniform pointwise lower bound on the regressor's energy, $\\phi^{2}(t) \\ge m$ for some known constant $m > 0$. This is a persistence of excitation (PE) condition. Using this condition, we can establish an exponential convergence rate. From our Lyapunov analysis, we have $\\dot{V}(t) = - \\phi^{2}(t) \\tilde{\\theta}^{2}(t)$. Applying the PE condition, we get:\n$$\n\\dot{V}(t) \\le -m \\tilde{\\theta}^{2}(t)\n$$\nWe relate $\\tilde{\\theta}^{2}(t)$ back to $V(t)$ using the definition $V(t) = \\frac{1}{2\\gamma}\\tilde{\\theta}^{2}(t)$, which gives $\\tilde{\\theta}^{2}(t) = 2\\gamma V(t)$. Substituting this into the inequality for $\\dot{V}(t)$:\n$$\n\\dot{V}(t) \\le -m (2\\gamma V(t)) = -2\\gamma m V(t)\n$$\nThis is a standard differential inequality. By the Comparison Lemma (a version of GrÃ¶nwall's inequality), we can conclude that:\n$$\nV(t) \\le V(0) \\exp(-2\\gamma m t)\n$$\nSubstituting the definition of $V$ back in terms of $\\tilde{\\theta}$:\n$$\n\\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\le \\left( \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(0) \\right) \\exp(-2\\gamma m t)\n$$\nMultiplying both sides by $2\\gamma$ and taking the square root, we obtain the desired exponential bound on the parameter error:\n$$\n|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)| \\exp(-\\gamma m t)\n$$\nThis bound has the form $|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)| \\exp(-\\alpha t)$, with the exponential decay rate being $\\alpha = \\gamma m$.\n\nFinally, we are tasked with choosing the adaptation gain $\\gamma$ to meet a prescribed adaptation rate $\\lambda_{d} > 0$. This means the decay must be at least as fast as $\\exp(-\\lambda_{d} t)$, which requires the rate $\\alpha$ to satisfy $\\alpha \\ge \\lambda_{d}$.\n$$\n\\gamma m \\ge \\lambda_{d}\n$$\nTo guarantee this condition, we may simply select $\\gamma$ such that the rates are equal. This provides a specific choice for the gain:\n$$\n\\gamma m = \\lambda_{d}\n$$\nSolving for $\\gamma$ yields:\n$$\n\\gamma = \\frac{\\lambda_{d}}{m}\n$$\nSince it is given that $\\lambda_{d} > 0$ and $m > 0$, the resulting adaptation gain $\\gamma$ is guaranteed to be positive. Therefore, this choice of $\\gamma$ preserves the Lyapunov stability established earlier. This expression for $\\gamma$ is the solution.", "answer": "$$\\boxed{\\frac{\\lambda_{d}}{m}}$$", "id": "2722772"}, {"introduction": "While the previous practice demonstrated that sufficient excitation guarantees parameter convergence, it is equally important to understand what happens when this condition is not met. This exercise presents a classic counterexample where the regressor signal's energy decays over time, violating the Persistence of Excitation (PE) condition. By solving for the long-term behavior of the system, you will uncover the crucial subtlety that prediction error convergence to zero does not necessarily imply parameter convergence, cementing the central role of PE in adaptive control theory [@problem_id:2722709].", "problem": "Consider the scalar adaptive estimation problem arising in Lyapunov-based adaptive control design. A measurable regressor signal $\\,\\phi(t)\\,$ and an unknown constant parameter $\\,\\theta \\in \\mathbb{R}\\,$ generate the scalar output $\\,y(t) = \\theta\\,\\phi(t)\\,$. An estimator uses the parametric model $\\,\\hat{y}(t) = \\hat{\\theta}(t)\\,\\phi(t)\\,$ and the standard gradient update law derived from a quadratic Lyapunov function:\n$$\n\\dot{\\hat{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,\\big(\\hat{y}(t) - y(t)\\big), \\quad \\gamma > 0.\n$$\nDefine the prediction error $\\,e(t) \\coloneqq \\hat{y}(t) - y(t)\\,$ and the parameter error $\\,\\tilde{\\theta}(t) \\coloneqq \\hat{\\theta}(t) - \\theta\\,$. Suppose the regressor is $\\,\\phi(t) = \\exp(-t)\\,$ for all $\\,t \\ge 0\\,$, which is not persistently exciting in the sense of persistence of excitation (PE), namely, there do not exist $\\,\\alpha > 0\\,$ and $\\,T > 0\\,$ such that for all $\\,t \\ge 0\\,$ one has $\\,\\int_{t}^{t+T} \\phi(\\tau)^2\\,\\mathrm{d}\\tau \\ge \\alpha\\,$.\n\nStarting from the fundamental definitions of $\\,e(t)\\,$ and $\\,\\tilde{\\theta}(t)\\,$ and the given update law, analyze the closed-loop error dynamics implied by the Lyapunov approach. Show that $\\,e(t) \\to 0\\,$ as $\\,t \\to \\infty\\,$ even though the parameter error need not converge to $\\,0\\,$ due to the lack of persistence of excitation. Compute the closed-form limit\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t)\n$$\nas a function of the initial parameter error $\\,\\tilde{\\theta}(0)\\,$ and the adaptation gain $\\,\\gamma\\,$. Your final answer must be a single analytic expression without units. Do not round or approximate your result.", "solution": "The validity of the problem statement must first be established.\n\nStep 1: Extract Givens.\nThe problem provides the following definitions and equations:\n- The output signal is given by $y(t) = \\theta\\,\\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter.\n- The measurable regressor signal is $\\phi(t) = \\exp(-t)$ for $t \\ge 0$.\n- The parametric model for the estimator is $\\hat{y}(t) = \\hat{\\theta}(t)\\,\\phi(t)$.\n- The prediction error is defined as $e(t) \\coloneqq \\hat{y}(t) - y(t)$.\n- The parameter error is defined as $\\tilde{\\theta}(t) \\coloneqq \\hat{\\theta}(t) - \\theta$.\n- The update law for the parameter estimate $\\hat{\\theta}(t)$ is given by the gradient rule $\\dot{\\hat{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,e(t)$, where the adaptation gain $\\gamma > 0$.\n- It is stated that the regressor $\\phi(t)$ is not persistently exciting.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. It presents a canonical scenario in adaptive control theory, specifically parameter estimation using a gradient descent algorithm. The system model, estimator structure, and update law are standard. The choice of regressor, $\\phi(t) = \\exp(-t)$, is a classic example used to illustrate the consequences of the lack of the persistence of excitation (PE) condition. The PE condition is a fundamental concept in system identification and adaptive control that guarantees parameter convergence. The problem is well-posed, providing all necessary information to derive the dynamics of the parameter error and analyze its asymptotic behavior. The language is objective and precise. The problem is a standard exercise in the analysis of adaptive systems and does not violate any criteria for invalidity.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A solution will be provided.\n\nThe objective is to analyze the error dynamics and compute the limit of the parameter error, $\\lim_{t \\to \\infty} \\tilde{\\theta}(t)$.\n\nFirst, we express the prediction error $e(t)$ in terms of the parameter error $\\tilde{\\theta}(t)$. From the definitions provided:\n$$\ne(t) = \\hat{y}(t) - y(t) = \\hat{\\theta}(t)\\phi(t) - \\theta\\phi(t) = \\left(\\hat{\\theta}(t) - \\theta\\right)\\phi(t)\n$$\nUsing the definition of the parameter error, $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$, we have:\n$$\ne(t) = \\tilde{\\theta}(t)\\phi(t)\n$$\nNext, we derive the differential equation governing the parameter error $\\tilde{\\theta}(t)$. We differentiate the definition of $\\tilde{\\theta}(t)$ with respect to time $t$:\n$$\n\\dot{\\tilde{\\theta}}(t) = \\frac{d}{dt}\\left(\\hat{\\theta}(t) - \\theta\\right) = \\dot{\\hat{\\theta}}(t) - \\dot{\\theta}(t)\n$$\nSince $\\theta$ is a constant parameter, its time derivative is zero, $\\dot{\\theta}(t) = 0$. Therefore, $\\dot{\\tilde{\\theta}}(t) = \\dot{\\hat{\\theta}}(t)$.\nSubstituting the given update law for $\\dot{\\hat{\\theta}}(t)$:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,e(t)\n$$\nNow, we substitute the expression for $e(t)$ into this equation to obtain the closed-loop error dynamics for $\\tilde{\\theta}(t)$:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,\\left(\\tilde{\\theta}(t)\\phi(t)\\right) = -\\gamma\\,\\phi(t)^2\\,\\tilde{\\theta}(t)\n$$\nThis is a first-order linear time-varying ordinary differential equation for the parameter error.\n\nThe problem specifies the regressor signal as $\\phi(t) = \\exp(-t)$. We substitute this into the differential equation:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\left(\\exp(-t)\\right)^2\\,\\tilde{\\theta}(t) = -\\gamma\\,\\exp(-2t)\\,\\tilde{\\theta}(t)\n$$\nThis ODE is separable and can be written as:\n$$\n\\frac{d\\tilde{\\theta}(t)}{\\tilde{\\theta}(t)} = -\\gamma\\,\\exp(-2t)\\,dt\n$$\nWe integrate both sides of the equation from the initial time $t=0$ to an arbitrary time $t > 0$:\n$$\n\\int_{\\tilde{\\theta}(0)}^{\\tilde{\\theta}(t)} \\frac{1}{\\tau} d\\tau = \\int_{0}^{t} -\\gamma\\,\\exp(-2\\xi)\\,d\\xi\n$$\nEvaluating the integrals:\n$$\n\\left[ \\ln|\\tau| \\right]_{\\tilde{\\theta}(0)}^{\\tilde{\\theta}(t)} = -\\gamma \\left[ -\\frac{1}{2}\\exp(-2\\xi) \\right]_{0}^{t}\n$$\n$$\n\\ln|\\tilde{\\theta}(t)| - \\ln|\\tilde{\\theta}(0)| = \\frac{\\gamma}{2} \\left[ \\exp(-2t) - \\exp(0) \\right]\n$$\n$$\n\\ln\\left(\\frac{|\\tilde{\\theta}(t)|}{|\\tilde{\\theta}(0)|}\\right) = \\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\n$$\nExponentiating both sides gives the solution for the magnitude of the parameter error:\n$$\n|\\tilde{\\theta}(t)| = |\\tilde{\\theta}(0)|\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right)\n$$\nSince the differential equation is linear, the solution $\\tilde{\\theta}(t)$ will not change sign if $\\tilde{\\theta}(0) \\ne 0$. If $\\tilde{\\theta}(0) = 0$, then $\\tilde{\\theta}(t) = 0$ for all $t$. Thus, we can remove the absolute value signs:\n$$\n\\tilde{\\theta}(t) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right)\n$$\nThis is the closed-form solution for the parameter error $\\tilde{\\theta}(t)$.\n\nThe problem also requires showing that $e(t) \\to 0$ as $t \\to \\infty$. We use the relationship $e(t) = \\tilde{\\theta}(t)\\phi(t)$:\n$$\ne(t) = \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right) \\exp(-t)\n$$\nWe take the limit as $t \\to \\infty$:\n$$\n\\lim_{t \\to \\infty} e(t) = \\lim_{t \\to \\infty} \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right) \\cdot \\lim_{t \\to \\infty} \\exp(-t)\n$$\nAs $t \\to \\infty$, we have $\\exp(-2t) \\to 0$ and $\\exp(-t) \\to 0$.\nThe first term converges to a constant:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}(0 - 1)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\n$$\nThe second term converges to zero:\n$$\n\\lim_{t \\to \\infty} \\exp(-t) = 0\n$$\nTherefore, the limit of the prediction error is:\n$$\n\\lim_{t \\to \\infty} e(t) = \\left(\\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\\right) \\cdot 0 = 0\n$$\nThis demonstrates that the prediction error converges to zero, even though, as we will see, the parameter error does not.\n\nFinally, we compute the limit of the parameter error $\\tilde{\\theta}(t)$ as $t \\to \\infty$:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t) = \\lim_{t \\to \\infty} \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right)\n$$\nAs established, $\\lim_{t \\to \\infty} \\exp(-2t) = 0$. Substituting this into the expression:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}(0 - 1)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\n$$\nThe final result shows that the parameter error does not converge to $0$ in general, but to a constant value that depends on its initial condition $\\tilde{\\theta}(0)$ and the adaptation gain $\\gamma$. This is a direct consequence of the regressor $\\phi(t)$ not being persistently exciting. The total \"energy\" of the regressor, $\\int_{0}^{\\infty} \\phi(\\tau)^2 d\\tau = \\int_{0}^{\\infty} \\exp(-2\\tau) d\\tau = \\frac{1}{2}$, is finite, which is insufficient to drive the parameter estimate to its true value.", "answer": "$$\n\\boxed{\\tilde{\\theta}(0)\\exp\\left(-\\frac{\\gamma}{2}\\right)}\n$$", "id": "2722709"}, {"introduction": "Having explored the core mechanics of parameter estimation, we now apply these principles to the design of a complete closed-loop control system. This final practice tasks you with synthesizing an indirect Model Reference Adaptive Controller (MRAC) using the certainty equivalence principle, where unknown plant parameters in the control law are replaced by their online estimates. You will construct both the parameter estimator and the control law, and then use a composite Lyapunov function to prove that the interconnected system is stable and that the plant output successfully tracks the desired reference model, providing a capstone experience in Lyapunov-based adaptive design [@problem_id:2722775].", "problem": "Consider a single-input single-output linear time-invariant plant with unknown constant parameters described by the differential equation $\\dot{y}(t)=-a\\,y(t)+b\\,u(t)$, where $a>0$ and $b>0$ are unknown constants, $u(t)$ is the control input, and $y(t)$ is the measured output. Assume that $\\dot{y}(t)$ is measurable and that the reference signal $r(t)$ is bounded and piecewise continuously differentiable. The desired behavior is specified by a first-order reference model $\\dot{y}_{m}(t)=-a_{m}\\,y_{m}(t)+b_{m}\\,r(t)$, where $a_{m}>0$ and $b_{m}>0$ are known design parameters.\n\nYour tasks are as follows.\n\n1. Starting from the fundamental definition of a Lyapunov function and the parameter estimation error, formulate a prediction model that is linear in the unknown parameters and derive a parameter estimator for $(a,b)$ using only measurable signals and a Lyapunov-based argument. You may use a constant, symmetric positive definite adaptation gain matrix, but you must explicitly construct a candidate Lyapunov function and its time derivative to justify stability of the estimator. Ensure that your estimator keeps all internal signals bounded for bounded regressors, and discuss how to guarantee that the estimate of $b$ remains bounded away from zero without breaking the Lyapunov nonincreasing property.\n\n2. Using the certainty equivalence principle, synthesize a control law that achieves exact model matching when the estimates equal the true parameters. Express your control input in terms of the current estimates $\\hat{a}(t)$ and $\\hat{b}(t)$, the measured output $y(t)$, and the reference $r(t)$, together with the known model parameters $a_{m}$ and $b_{m}$.\n\n3. Define the tracking error $e(t)=y(t)-y_{m}(t)$ and argue, using first principles and without invoking any prepackaged theorems beyond Lyapunov stability and properties of linear systems, that the certainty-equivalent controller from part 2 stabilizes the tracking error dynamics in the presence of bounded parameter estimation errors generated by your estimator. Clearly state any additional mild conditions you need for well-posedness and stability.\n\nProvide the closed-form analytical expression for the certainty-equivalent control law $u(t)$ from part 2 as your final answer. No numerical computation is required. Do not include any units. The final answer must be a single analytical expression.", "solution": "The problem as stated is a standard exercise in Lyapunov-based model reference adaptive control for a linear time-invariant system. All provided information is self-contained and scientifically sound. The problem is well-posed and objective, free of contradictions or ambiguities. It represents a valid theoretical problem in control engineering. We may therefore proceed with the solution.\n\nThe problem is addressed in three parts as requested.\n\n1. Parameter Estimator Formulation\n\nThe plant dynamics are given by the differential equation\n$$\n\\dot{y}(t) = -a y(t) + b u(t)\n$$\nwhere $a > 0$ and $b > 0$ are unknown constants. This equation is linear in the parameters $a$ and $b$. We define a parameter vector $\\theta$ and a regressor vector $\\phi(t)$ as\n$$\n\\theta = \\begin{pmatrix} a \\\\ b \\end{pmatrix}, \\quad \\phi(t) = \\begin{pmatrix} -y(t) \\\\ u(t) \\end{pmatrix}\n$$\nWith these definitions, the plant dynamics can be written in the linear parametric form\n$$\n\\dot{y}(t) = \\phi^T(t) \\theta\n$$\nThe problem states that $y(t)$ and $\\dot{y}(t)$ are measurable, and $u(t)$ is the known control input. Therefore, the regressor vector $\\phi(t)$ is directly measurable.\n\nWe formulate a prediction model for $\\dot{y}(t)$ using the parameter estimates $\\hat{a}(t)$ and $\\hat{b}(t)$, which are components of the estimated parameter vector $\\hat{\\theta}(t) = [\\hat{a}(t), \\hat{b}(t)]^T$. The predicted value $\\hat{\\dot{y}}(t)$ is\n$$\n\\hat{\\dot{y}}(t) = -\\hat{a}(t) y(t) + \\hat{b}(t) u(t) = \\phi^T(t) \\hat{\\theta}(t)\n$$\nThe prediction error, or equation error, $\\epsilon_1(t)$ is defined as\n$$\n\\epsilon_1(t) = \\hat{\\dot{y}}(t) - \\dot{y}(t) = \\phi^T(t) (\\hat{\\theta}(t) - \\theta) = -\\phi^T(t) \\tilde{\\theta}(t)\n$$\nwhere $\\tilde{\\theta}(t) = \\theta - \\hat{\\theta}(t)$ is the parameter estimation error vector.\n\nTo derive the parameter update law, we consider a Lyapunov function candidate for the parameter error dynamics:\n$$\nV_e(\\tilde{\\theta}) = \\frac{1}{2} \\tilde{\\theta}^T \\Gamma^{-1} \\tilde{\\theta}\n$$\nwhere $\\Gamma$ is a constant, symmetric, positive definite adaptation gain matrix. Since $\\theta$ is constant, the time derivative of the parameter error is $\\dot{\\tilde{\\theta}} = -\\dot{\\hat{\\theta}}$. The time derivative of $V_e$ is\n$$\n\\dot{V}_e = \\tilde{\\theta}^T \\Gamma^{-1} \\dot{\\tilde{\\theta}} = - \\tilde{\\theta}^T \\Gamma^{-1} \\dot{\\hat{\\theta}}\n$$\nWe choose a standard gradient-based update law for $\\hat{\\theta}(t)$:\n$$\n\\dot{\\hat{\\theta}}(t) = - \\Gamma \\phi(t) \\epsilon_1(t)\n$$\nSubstituting this into the expression for $\\dot{V}_e$, we obtain\n$$\n\\dot{V}_e = - \\tilde{\\theta}^T \\Gamma^{-1} (- \\Gamma \\phi \\epsilon_1) = \\tilde{\\theta}^T \\phi \\epsilon_1\n$$\nSince $\\epsilon_1 = -\\phi^T \\tilde{\\theta}$, this becomes\n$$\n\\dot{V}_e = -(\\tilde{\\theta}^T \\phi)^2 = - \\epsilon_1^2 \\leq 0\n$$\nBecause $\\dot{V}_e$ is negative semi-definite, $V_e(t)$ is non-increasing. This implies that $V_e(t) \\leq V_e(0)$ for all $t \\geq 0$, and therefore the parameter error $\\tilde{\\theta}(t)$ is bounded. Since $\\theta$ is a bounded constant, the estimate $\\hat{\\theta}(t)$ is also bounded. This confirms the stability of the estimator itself.\n\nThe control law, to be derived next, will involve division by $\\hat{b}(t)$. To prevent this estimate from becoming zero or negative (since we know $b > 0$), we must ensure it remains bounded away from zero. This is achieved by using a projection modification to the update law. We define a convex set for the parameters, for instance, $\\mathcal{D} = \\{ (\\hat{a}, \\hat{b}) \\mid \\hat{b} \\geq b_{\\min} \\}$, where $b_{\\min}$ is a small positive constant chosen based on a priori knowledge about the possible range of $b$. The projected update law, $\\dot{\\hat{\\theta}} = \\text{Proj}(\\hat{\\theta}, -\\Gamma \\phi \\epsilon_1)$, ensures that if $\\hat{\\theta}$ is on the boundary of $\\mathcal{D}$ (i.e., $\\hat{b} = b_{\\min}$), the update will not point outside the set. This modification preserves the non-increasing property of the Lyapunov function, i.e., $\\dot{V}_e \\leq 0$.\n\n2. Certainty-Equivalent Control Law Synthesis\n\nThe objective is to make the plant output $y(t)$ track the reference model output $y_m(t)$, whose dynamics are given by $\\dot{y}_m = -a_m y_m + b_m r$. The ideal scenario is \"exact model matching,\" where the plant dynamics for $y(t)$ become identical to the reference model dynamics:\n$$\n\\dot{y}(t) = -a_m y(t) + b_m r(t)\n$$\nEquating the plant equation with this desired behavior, we get\n$$\n-a y(t) + b u(t) = -a_m y(t) + b_m r(t)\n$$\nSolving for the ideal control input $u(t)$ gives:\n$$\nb u(t) = (a - a_m) y(t) + b_m r(t) \\implies u(t) = \\frac{1}{b} \\left( (a - a_m) y(t) + b_m r(t) \\right)\n$$\nThe certainty equivalence principle dictates that we replace the unknown true parameters $a$ and $b$ with their current estimates $\\hat{a}(t)$ and $\\hat{b}(t)$. This yields the adaptive control law:\n$$\nu(t) = \\frac{1}{\\hat{b}(t)} \\left( (\\hat{a}(t) - a_m) y(t) + b_m r(t) \\right)\n$$\nThe projection mechanism discussed previously ensures that $\\hat{b}(t) \\geq b_{\\min} > 0$, so the control law is always well-defined.\n\n3. Closed-Loop Stability Analysis\n\nDefine the tracking error as $e(t) = y(t) - y_m(t)$. Its time derivative is $\\dot{e}(t) = \\dot{y}(t) - \\dot{y}_m(t)$. We derive the tracking error dynamics:\n$$\n\\dot{e} = (-ay+bu) - (-a_m y_m + b_m r)\n$$\nAdding and subtracting $a_m y(t)$ gives\n$$\n\\dot{e} = -a_m(y - y_m) + (a_m - a)y + bu - b_m r = -a_m e + (a_m - a)y + bu - b_m r\n$$\nFrom the control law in part 2, we have $\\hat{b}u = (\\hat{a}-a_m)y + b_m r$, which can be rearranged to $b_m r = \\hat{b}u - (\\hat{a}-a_m)y$. Substituting this into the expression for $\\dot{e}$:\n$$\n\\dot{e} = -a_m e + (a_m - a)y + bu - (\\hat{b}u - (\\hat{a}-a_m)y)\n$$\n$$\n\\dot{e} = -a_m e + (a_m - a + \\hat{a} - a_m)y + (b - \\hat{b})u\n$$\n$$\n\\dot{e} = -a_m e + (\\hat{a} - a)y + (b - \\hat{b})u = -a_m e - (a - \\hat{a})y + (b - \\hat{b})u\n$$\nUsing the definitions $\\tilde{a} = a - \\hat{a}$, $\\tilde{b} = b - \\hat{b}$, and $\\phi^T = [-y, u]$, we can write this compactly:\n$$\n\\dot{e}(t) = -a_m e(t) + \\tilde{b}u - \\tilde{a}y = -a_m e(t) + \\phi^T(t)\\tilde{\\theta}(t)\n$$\nNote that the equation error is $\\epsilon_1 = -\\phi^T\\tilde{\\theta}$. Therefore, the tracking error dynamics are:\n$$\n\\dot{e}(t) = -a_m e(t) - \\epsilon_1(t)\n$$\nTo analyze the stability of the coupled system ($\\dot{e}$, $\\dot{\\tilde{\\theta}}$), consider the composite Lyapunov function candidate:\n$$\nV(e, \\tilde{\\theta}) = \\frac{1}{2} e^2 + \\frac{1}{2}\\tilde{\\theta}^T \\Gamma^{-1} \\tilde{\\theta}\n$$\nIts time derivative is $\\dot{V} = e\\dot{e} + \\tilde{\\theta}^T\\Gamma^{-1}\\dot{\\tilde{\\theta}}$. From part 1, we know $\\tilde{\\theta}^T\\Gamma^{-1}\\dot{\\tilde{\\theta}} = -\\epsilon_1^2$. Substituting this and the expression for $\\dot{e}$:\n$$\n\\dot{V} = e(-a_m e - \\epsilon_1) - \\epsilon_1^2 = -a_m e^2 - e\\epsilon_1 - \\epsilon_1^2\n$$\nBy completing the square for the terms involving $\\epsilon_1$:\n$$\n\\dot{V} = -a_m e^2 - \\left(\\epsilon_1^2 + e\\epsilon_1 + \\frac{e^2}{4}\\right) + \\frac{e^2}{4} = -a_m e^2 - \\left(\\epsilon_1 + \\frac{e}{2}\\right)^2 + \\frac{e^2}{4}\n$$\n$$\n\\dot{V} = -\\left(a_m - \\frac{1}{4}\\right)e^2 - \\left(\\epsilon_1 + \\frac{e}{2}\\right)^2\n$$\nFor $\\dot{V}$ to be negative semi-definite, we need $a_m - 1/4 \\ge 0$, which means $a_m \\ge 1/4$. This is a mild condition on a known design parameter. If this condition is met, $\\dot{V} \\le 0$, which implies that $V(t)$ is bounded. This in turn proves that the tracking error $e(t)$ and the parameter error $\\tilde{\\theta}(t)$ are bounded.\n\nFinally, we argue that all signals in the closed loop are bounded. The reference signal $r(t)$ is bounded, and the reference model is stable, so $y_m(t)$ is bounded. Since $e(t)$ is bounded, the plant output $y(t) = e(t) + y_m(t)$ is also bounded. The parameter estimates $\\hat{\\theta}(t)$ are bounded. The control input $u(t)$ is a function of bounded signals ($y, r, \\hat{a}, \\hat{b}$) and is well-defined since $\\hat{b}(t)$ is bounded away from zero by projection. Thus, $u(t)$ is bounded. Since $y(t)$ and $u(t)$ are bounded, the regressor $\\phi(t)$ is bounded. This shows that the certainty-equivalent controller stabilizes the system in the sense that all signals remain bounded (Uniformly Ultimately Bounded). The additional mild conditions are that the sign of $b$ is known (to set the projection boundary $b_{\\min}>0$) and that the reference model is chosen such that $a_m \\ge 1/4$.", "answer": "$$\\boxed{\\frac{1}{\\hat{b}(t)} \\left( \\left(\\hat{a}(t) - a_{m}\\right) y(t) + b_{m} r(t) \\right)}$$", "id": "2722775"}]}