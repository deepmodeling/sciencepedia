{"hands_on_practices": [{"introduction": "The cornerstone of any data-driven control method is the quality of the data itself. For linear systems, the concept of Persistent Excitation (PE) provides a rigorous mathematical definition of a \"sufficiently rich\" input signal, ensuring that the collected data can reveal the system's dynamics. This first exercise [@problem_id:2698781] takes you back to first principles, tasking you with constructing a PE signal from scratch to satisfy a specific rank condition on its Hankel matrix. Mastering this fundamental construction is key to understanding what makes data truly informative.", "problem": "Consider a discrete-time, linear time-invariant (LTI) system with a scalar input (that is, input dimension $m = 1$). In data-driven control, an input sequence $\\{u_k\\}_{k=0}^{T-1}$ is called persistently exciting of order $L$ if the block Hankel matrix of depth $L$ built from $\\{u_k\\}$ has full row rank. The block Hankel matrix of depth $L$ associated with a scalar sequence $\\{u_k\\}_{k=0}^{T-1}$ is the $L \\times (T-L+1)$ matrix\n$$\nH_L(u) \\;=\\;\n\\begin{bmatrix}\nu_0 & u_1 & \\cdots & u_{T-L} \\\\\nu_1 & u_2 & \\cdots & u_{T-L+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_{L-1} & u_{L} & \\cdots & u_{T-1}\n\\end{bmatrix}.\n$$\nUsing only the core definition above and first principles, do the following:\n- Let $L=4$ and $T=19$. Design an explicit binary input sequence $\\{u_k\\}_{k=0}^{18}$ with $u_k \\in \\{0,1\\}$ that is persistently exciting of order $L$.\n- Justify, from first principles, why your design guarantees that the rank condition implied by the definition is satisfied. Your justification must proceed by identifying a structurally sufficient set of linearly independent columns of $H_L(u)$ and explaining why they exist for your design.\n- Compute the rank of $H_4(u)$ for your constructed sequence, and report this rank as your final answer.\n\nYour final answer must be a single real number. No rounding is required and no units are involved.", "solution": "The problem requires the design of a binary input sequence $\\{u_k\\}_{k=0}^{18}$ with $u_k \\in \\{0, 1\\}$ that is persistently exciting of order $L=4$. The total sequence length is $T=19$.\n\nFirst, we establish the properties of the Hankel matrix based on the given parameters. The depth is $L=4$ and the sequence length is $T=19$. The block Hankel matrix $H_L(u)$, which we shall denote as $H_4(u)$, is an $L \\times (T-L+1)$ matrix.\n$$\n\\text{Number of rows} = L = 4\n$$\n$$\n\\text{Number of columns} = T - L + 1 = 19 - 4 + 1 = 16\n$$\nSo, $H_4(u)$ is a $4 \\times 16$ matrix of the form:\n$$\nH_4(u) =\n\\begin{bmatrix}\nu_0 & u_1 & u_2 & \\cdots & u_{15} \\\\\nu_1 & u_2 & u_3 & \\cdots & u_{16} \\\\\nu_2 & u_3 & u_4 & \\cdots & u_{17} \\\\\nu_3 & u_4 & u_5 & \\cdots & u_{18}\n\\end{bmatrix}\n$$\nThe definition of persistent excitation of order $L$ states that the matrix $H_L(u)$ must have full row rank. For $H_4(u)$, this means its rank must be $4$.\n\nOur task is to construct a sequence $\\{u_k\\}_{k=0}^{18}$ that guarantees $\\text{rank}(H_4(u)) = 4$. We will proceed by construction, from first principles. The rank of a matrix is the dimension of its column space, which is equivalent to the maximum number of linearly independent columns. To demonstrate that the rank is $4$, it is sufficient to identify a set of $4$ linearly independent columns within $H_4(u)$.\n\nLet the columns of $H_4(u)$ be denoted by $c_j$ for $j \\in \\{0, 1, \\dots, 15\\}$, where\n$$\nc_j = \\begin{pmatrix} u_j \\\\ u_{j+1} \\\\ u_{j+2} \\\\ u_{j+3} \\end{pmatrix}\n$$\nWe will design the sequence $\\{u_k\\}$ such that a specific set of four columns forms an invertible $4 \\times 4$ matrix. The simplest choice for such a matrix is the identity matrix, $I_4$. Let us select the columns $c_0$, $c_4$, $c_8$, and $c_{12}$ and force them to be the standard basis vectors $e_1, e_2, e_3, e_4$ respectively, where $e_1 = [1,0,0,0]^T$, $e_2 = [0,1,0,0]^T$, and so on.\n\nThis imposes the following conditions on the sequence $\\{u_k\\}$:\n1. $c_0 = e_1 \\implies [u_0, u_1, u_2, u_3]^T = [1, 0, 0, 0]^T$. This sets $u_0=1, u_1=0, u_2=0, u_3=0$.\n2. $c_4 = e_2 \\implies [u_4, u_5, u_6, u_7]^T = [0, 1, 0, 0]^T$. This sets $u_4=0, u_5=1, u_6=0, u_7=0$.\n3. $c_8 = e_3 \\implies [u_8, u_9, u_{10}, u_{11}]^T = [0, 0, 1, 0]^T$. This sets $u_8=0, u_9=0, u_{10}=1, u_{11}=0$.\n4. $c_{12} = e_4 \\implies [u_{12}, u_{13}, u_{14}, u_{15}]^T = [0, 0, 0, 1]^T$. This sets $u_{12}=0, u_{13}=0, u_{14}=0, u_{15}=1$.\n\nThese conditions define the sequence elements $\\{u_k\\}$ for $k \\in \\{0, 1, \\dots, 15\\}$. The index intervals for these definitions, $[0,3]$, $[4,7]$, $[8,11]$, and $[12,15]$, are disjoint, so the definitions are consistent and do not create contradictions.\n\nCompiling these values, we obtain the first $16$ elements of our input sequence:\n$$\n\\{u_k\\}_{k=0}^{15} = \\{1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1\\}\n$$\nThe problem requires a sequence of length $T=19$, i.e., $\\{u_k\\}_{k=0}^{18}$. The elements $u_{16}, u_{17}, u_{18}$ are not constrained by our construction. For simplicity, we set them to $0$:\n$$\nu_{16} = 0, \\quad u_{17} = 0, \\quad u_{18} = 0\n$$\nThus, the explicit binary input sequence is:\n$$\n\\{u_k\\}_{k=0}^{18} = \\{1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0\\}\n$$\nNow, we must justify that this sequence meets the rank condition. By our design, the submatrix of $H_4(u)$ formed by columns $c_0, c_4, c_8, c_{12}$ is:\n$$\n[c_0, c_4, c_8, c_{12}] =\n\\begin{bmatrix}\nu_0 & u_4 & u_8 & u_{12} \\\\\nu_1 & u_5 & u_9 & u_{13} \\\\\nu_2 & u_6 & u_{10} & u_{14} \\\\\nu_3 & u_7 & u_{11} & u_{15}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n= I_4\n$$\nThe determinant of this $4 \\times 4$ submatrix is $\\det(I_4) = 1$, which is non-zero. Therefore, the columns $c_0, c_4, c_8, c_{12}$ are linearly independent.\n\nSince $H_4(u)$ contains a set of $4$ linearly independent columns, the dimension of its column space, which is its rank, must be at least $4$.\n$$\n\\text{rank}(H_4(u)) \\ge 4\n$$\nFurthermore, the rank of any matrix cannot exceed the number of its rows or columns. The matrix $H_4(u)$ has $4$ rows.\n$$\n\\text{rank}(H_4(u)) \\le \\min(\\text{rows}, \\text{columns}) = \\min(4, 16) = 4\n$$\nCombining these two inequalities, we have $4 \\le \\text{rank}(H_4(u)) \\le 4$. This forces the rank to be exactly $4$.\n$$\n\\text{rank}(H_4(u)) = 4\n$$\nSince the rank equals the number of rows $L=4$, the matrix $H_4(u)$ has full row rank, and the constructed sequence is, by definition, persistently exciting of order $4$. The computed rank is $4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "2698781"}, {"introduction": "While the concept of persistent excitation is straightforward for single-input systems, its extension to multi-input (MIMO) systems introduces critical subtleties. It is not enough for each input channel to be independently exciting; the relationships between the channels also matter. This practice [@problem_id:2698812] demonstrates a common pitfall in experimental design for MIMO systems: input correlation, which can render data uninformative even when all inputs are non-zero. By constructing a rank-deficient Hankel matrix from correlated inputs, you will gain a deeper appreciation for the stringent requirements of data informativity in a multivariable context.", "problem": "Consider a discrete-time, multi-input sequence with input dimension $m=2$. Let the input at time $k$ be $u_k \\in \\mathbb{R}^2$. For a positive integer window length $L$, define the block Hankel matrix $H_L(u)$ as the matrix obtained by stacking $L$ time-shifted blocks of the input sequence into $L$ block rows of height $m$, with consecutive columns built from consecutive time indices. Specifically, if the total sequence length is $T$ with $T \\ge L$, then $H_L(u) \\in \\mathbb{R}^{(mL)\\times (T-L+1)}$ and its $j$-th column is $\\operatorname{col}(u_j,u_{j+1},\\dots,u_{j+L-1})$. The input is said to be persistently exciting of order $L$ if $H_L(u)$ has full row rank $mL$.\n\nUsing only the core linear algebra facts that (i) the rank of a product satisfies $\\operatorname{rank}(AB)\\leq \\min\\{\\operatorname{rank}(A),\\operatorname{rank}(B)\\}$ and (ii) full row rank of $H_L(u)$ is necessary for persistency of excitation of order $L$, construct an explicit length-$T$ sequence with $m=2$, $L=3$, and $T=8$ such that every input $u_k$ is nonzero but the resulting $H_3(u)$ is rank-deficient. Your construction must be based on creating correlation across the two input channels and should be explained from first principles in terms of linear dependence. Then, compute the rank of $H_3(u)$ for your constructed sequence.\n\nProvide the rank of $H_3(u)$ as your final answer. No units are required. The final answer must be a single real number. Rounding is not applicable to this problem.", "solution": "The problem requires the construction of a discrete-time input sequence $u_k \\in \\mathbb{R}^2$ of length $T=8$ that is rank-deficient in the sense of persistency of excitation of order $L=3$. Specifically, the associated block Hankel matrix $H_3(u)$ must have a rank less than its full row rank of $mL=6$. The construction must ensure every input vector $u_k$ is non-zero and must be based on creating a correlation between the two input channels.\n\nFirst, we establish the structure of the Hankel matrix. Given the parameters $m=2$, $L=3$, and $T=8$, the Hankel matrix $H_3(u)$ is of size $(mL) \\times (T-L+1)$, which evaluates to $(2 \\times 3) \\times (8 - 3 + 1) = 6 \\times 6$. A $6 \\times 6$ matrix is rank-deficient if its rank is less than $6$.\n\nThe $j$-th column of $H_3(u)$ is given by $\\operatorname{col}(u_{j-1}, u_j, u_{j+1})$ for $j=1, \\dots, 6$. Let the input vector at time $k$ be $u_k = \\begin{pmatrix} u_{k,1} \\\\ u_{k,2} \\end{pmatrix}$. The full matrix is:\n$$ H_3(u) = \\begin{pmatrix}\nu_{0,1} & u_{1,1} & u_{2,1} & u_{3,1} & u_{4,1} & u_{5,1} \\\\\nu_{0,2} & u_{1,2} & u_{2,2} & u_{3,2} & u_{4,2} & u_{5,2} \\\\\nu_{1,1} & u_{2,1} & u_{3,1} & u_{4,1} & u_{5,1} & u_{6,1} \\\\\nu_{1,2} & u_{2,2} & u_{3,2} & u_{4,2} & u_{5,2} & u_{6,2} \\\\\nu_{2,1} & u_{3,1} & u_{4,1} & u_{5,1} & u_{6,1} & u_{7,1} \\\\\nu_{2,2} & u_{3,2} & u_{4,2} & u_{5,2} & u_{6,2} & u_{7,2}\n\\end{pmatrix} $$\nThe rows of this matrix, which we denote as $R_1, \\dots, R_6$, determine its rank.\n$R_1 = (u_{0,1}, u_{1,1}, u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1})$\n$R_2 = (u_{0,2}, u_{1,2}, u_{2,2}, u_{3,2}, u_{4,2}, u_{5,2})$\n$R_3 = (u_{1,1}, u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1}, u_{6,1})$\n$R_4 = (u_{1,2}, u_{2,2}, u_{3,2}, u_{4,2}, u_{5,2}, u_{6,2})$\n$R_5 = (u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1}, u_{6,1}, u_{7,1})$\n$R_6 = (u_{2,2}, u_{3,2}, u_{4,2}, u_{5,2}, u_{6,2}, u_{7,2})$\n\nTo introduce rank deficiency via correlation between channels, we impose a linear relationship between the two components of the input, $u_{k,1}$ and $u_{k,2}$, for all time steps $k=0, \\dots, 7$. The simplest such non-trivial relationship is direct proportionality. Let us set $u_{k,2} = c \\cdot u_{k,1}$ for all $k$, where $c$ is a non-zero constant. For maximum simplicity, we choose $c=1$, which means $u_{k,2} = u_{k,1}$ for all $k$.\n\nThis relationship has a direct consequence on the rows of $H_3(u)$. For any time shift $i$, the sequence of the second channel's inputs is identical to the sequence of the first channel's inputs. This translates to the following linear dependencies between the rows of $H_3(u)$:\n\\begin{itemize}\n    \\item $R_2$, which is formed from $\\{u_{k,2}\\}$ starting at $k=0$, becomes identical to $R_1$, which is formed from $\\{u_{k,1}\\}$ starting at $k=0$. Thus, $R_2 = R_1$.\n    \\item $R_4$, formed from $\\{u_{k,2}\\}$ starting at $k=1$, becomes identical to $R_3$, formed from $\\{u_{k,1}\\}$ starting at $k=1$. Thus, $R_4 = R_3$.\n    \\item $R_6$, formed from $\\{u_{k,2}\\}$ starting at $k=2$, becomes identical to $R_5$, formed from $\\{u_{k,1}\\}$ starting at $k=2$. Thus, $R_6 = R_5$.\n\\end{itemize}\nThese three linear dependencies ($R_2 - R_1 = 0$, $R_4 - R_3 = 0$, $R_6 - R_5 = 0$) imply that the row space of $H_3(u)$ is spanned by only three rows: $R_1$, $R_3$, and $R_5$. Therefore, the rank of $H_3(u)$ is, at most, $3$.\n$$ \\operatorname{rank}(H_3(u)) = \\operatorname{rank} \\begin{pmatrix} R_1 \\\\ R_3 \\\\ R_5 \\end{pmatrix} $$\nTo complete the construction, we must define the scalar sequence $\\{u_{k,1}\\}_{k=0}^7$ such that two conditions are met:\n1. The inputs $u_k = (u_{k,1}, u_{k,1})^T$ are non-zero for all $k$. This is satisfied if $u_{k,1} \\neq 0$ for all $k$.\n2. The matrix formed by rows $R_1, R_3, R_5$ has rank $3$. This ensures that the rank of $H_3(u)$ is exactly $3$, making it rank-deficient.\n\nLet us construct such a sequence. We need the three vectors $R_1$, $R_3$, and $R_5$ to be linearly independent. These vectors are time-shifted versions of the sequence $\\{u_{k,1}\\}$. We choose a sequence that is not overly simple (like an arithmetic progression, which would cause dependencies). A short, repeating, non-trivial sequence will suffice.\nLet the sequence for the first channel be $\\{u_{k,1}\\}_{k=0}^7 = (1, 2, 3, 1, 2, 3, 1, 2)$. All elements are non-zero.\nThe sequence for the second channel is identical: $\\{u_{k,2}\\}_{k=0}^7 = (1, 2, 3, 1, 2, 3, 1, 2)$.\nThe input vectors $u_k$ are $(1,1)^T, (2,2)^T, \\dots, (2,2)^T$. None are the zero vector.\n\nNow we form the matrix $M = \\begin{pmatrix} R_1 \\\\ R_3 \\\\ R_5 \\end{pmatrix}$ and compute its rank.\n$R_1 = (u_{0,1}, u_{1,1}, u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1}) = (1, 2, 3, 1, 2, 3)$\n$R_3 = (u_{1,1}, u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1}, u_{6,1}) = (2, 3, 1, 2, 3, 1)$\n$R_5 = (u_{2,1}, u_{3,1}, u_{4,1}, u_{5,1}, u_{6,1}, u_{7,1}) = (3, 1, 2, 3, 1, 2)$\nSo, we must find the rank of:\n$$ M = \\begin{pmatrix}\n1 & 2 & 3 & 1 & 2 & 3 \\\\\n2 & 3 & 1 & 2 & 3 & 1 \\\\\n3 & 1 & 2 & 3 & 1 & 2\n\\end{pmatrix} $$\nWe use Gaussian elimination to find the rank.\nPerform row operations: $R_3 \\rightarrow R_3 - 3R_1$ and $R_2 \\rightarrow R_2 - 2R_1$.\n$R_2 - 2R_1 = (2-2(1), 3-2(2), 1-2(3), 2-2(1), 3-2(2), 1-2(3)) = (0, -1, -5, 0, -1, -5)$\n$R_3 - 3R_1 = (3-3(1), 1-3(2), 2-3(3), 3-3(1), 1-3(2), 2-3(3)) = (0, -5, -7, 0, -5, -7)$\nThe matrix becomes:\n$$ \\begin{pmatrix}\n1 & 2 & 3 & 1 & 2 & 3 \\\\\n0 & -1 & -5 & 0 & -1 & -5 \\\\\n0 & -5 & -7 & 0 & -5 & -7\n\\end{pmatrix} $$\nNow, perform the operation $R_3 \\rightarrow R_3 - 5R_2$:\n$R_3 - 5R_2 = (0, -5-5(-1), -7-5(-5), 0, -5-5(-1), -7-5(-5)) = (0, 0, -7+25, 0, 0, -7+25) = (0, 0, 18, 0, 0, 18)$\nThe matrix in row-echelon form is:\n$$ \\begin{pmatrix}\n1 & 2 & 3 & 1 & 2 & 3 \\\\\n0 & -1 & -5 & 0 & -1 & -5 \\\\\n0 & 0 & 18 & 0 & 0 & 18\n\\end{pmatrix} $$\nThis matrix has three non-zero rows, and hence three pivots. Its rank is $3$.\nSince $\\operatorname{rank}(H_3(u)) = \\operatorname{rank}(M)$, the rank of the Hankel matrix for our constructed sequence is $3$. This is less than the full rank of $6$, so the input is not persistently exciting of order $3$.\n\nThe constructed sequence is $u_0=(1,1)^T$, $u_1=(2,2)^T$, $u_2=(3,3)^T$, $u_3=(1,1)^T$, $u_4=(2,2)^T$, $u_5=(3,3)^T$, $u_6=(1,1)^T$, $u_7=(2,2)^T$.\n\nThe rank of $H_3(u)$ for this sequence is $3$.", "answer": "$$\\boxed{3}$$", "id": "2698812"}, {"introduction": "Moving from the theory of data informativity to practical implementation, we often face uncertainty. Instead of identifying a single system model, set-membership methods use data to define a whole set of models consistent with observations and known noise bounds. This computational exercise [@problem_id:2698754] guides you through a complete data-to-control workflow: you will construct this feasible parameter set, compute its geometric center for a nominal controller design, and then perform a robust analysis to find the worst-case performance under the identified uncertainty. This practice bridges the gap between data-driven identification and robust control, a central theme in modern control engineering.", "problem": "Consider a Single-Input Single-Output (SISO) AutoRegressive with eXogenous input (ARX) one-step-ahead prediction model written in regression form as $y_k = \\varphi_k^\\top \\theta + e_k$, where $y_k \\in \\mathbb{R}$ is the measured output at time index $k$, $\\varphi_k \\in \\mathbb{R}^p$ is the known regressor vector, $\\theta \\in \\mathbb{R}^p$ is the unknown parameter vector, and $e_k \\in \\mathbb{R}$ is bounded disturbance satisfying $|e_k| \\le \\varepsilon$. Under the assumption of a known uniform bound $\\varepsilon \\ge 0$, the Feasible Parameter Set (FPS) is the polyhedron\n$$\n\\Theta = \\left\\{ \\theta \\in \\mathbb{R}^p \\,\\middle|\\, -\\varepsilon \\le y_k - \\varphi_k^\\top \\theta \\le \\varepsilon \\text{ for all available indices } k \\right\\}.\n$$\nEquivalently, letting $A \\in \\mathbb{R}^{m \\times p}$ and $b \\in \\mathbb{R}^m$ collect the linear inequalities derived from the data as\n$$\n\\begin{aligned}\n\\varphi_k^\\top \\theta &\\le y_k + \\varepsilon,\\\\\n(-\\varphi_k)^\\top \\theta &\\le -y_k + \\varepsilon,\n\\end{aligned}\n$$\nfor each index $k$, we write $\\Theta = \\{ \\theta \\in \\mathbb{R}^p \\mid A \\theta \\le b \\}$.\n\nThe Chebyshev center of $\\Theta$ is defined as the point $\\theta_c \\in \\mathbb{R}^p$ that maximizes the radius $r \\ge 0$ of the largest Euclidean ball $\\mathbb{B}_2(\\theta_c, r) = \\{\\theta \\in \\mathbb{R}^p \\mid \\|\\theta - \\theta_c\\|_2 \\le r\\}$ contained in $\\Theta$. For a polyhedron given by $A \\theta \\le b$, the Chebyshev center can be computed by solving the Linear Programming (LP) problem\n$$\n\\begin{aligned}\n\\max_{\\theta \\in \\mathbb{R}^p,\\, r \\in \\mathbb{R}}~& r\\\\\n\\text{s.t.}~& A_i^\\top \\theta + \\|A_i\\|_2 \\, r \\le b_i,\\quad i = 1,\\dots,m,\\\\\n& r \\ge 0,\n\\end{aligned}\n$$\nwhere $A_i^\\top$ is the $i$-th row of $A$ and $b_i$ is the corresponding element of $b$.\n\nFor a static state-feedback control law $u_{k-1} = k_{\\text{nom}} y_{k-1}$ designed using the nominal parameter estimate $\\hat{\\theta} = \\theta_c$ from the Chebyshev center, the nominal closed-loop pole for the first-order SISO ARX plant model $y_k = a\\, y_{k-1} + b\\, u_{k-1} + e_k$ equals $a + b\\,k_{\\text{nom}}$. If one specifies a desired nominal closed-loop pole $\\alpha_{\\text{des}} \\in \\mathbb{R}$, a certainty-equivalence design chooses\n$$\nk_{\\text{nom}} = \n\\begin{cases}\n\\dfrac{\\alpha_{\\text{des}} - \\hat{a}}{\\hat{b}}, & \\text{if } |\\hat{b}| \\ge \\delta,\\\n$$1ex]\n0, & \\text{otherwise},\n\\end{cases}\n$$\nfor a small threshold $\\delta > 0$ to avoid division by very small numbers. The resulting worst-case closed-loop pole magnitude over the feasible parameter set is then\n$$\n\\rho_{\\max} = \\max_{\\theta \\in \\Theta} \\left| a + b\\,k_{\\text{nom}} \\right|,\n$$\nwhich quantifies a fundamental conservatism induced by using a single nominal gain on an entire feasible set. This quantity can be computed by two LPs because it is the maximum of a piecewise-linear function:\n$$\n\\begin{aligned}\nz_{\\max} &= \\max_{\\theta \\in \\Theta}~ c^\\top \\theta,\\\\\nz_{\\min} &= \\min_{\\theta \\in \\Theta}~ c^\\top \\theta,\\\\\n\\rho_{\\max} &= \\max\\{\\, z_{\\max},\\, -z_{\\min}\\,\\},\n\\end{aligned}\n$$\nwhere $c = \\begin{bmatrix} 1 & k_{\\text{nom}} \\end{bmatrix}^\\top$ and $\\theta = \\begin{bmatrix} a & b \\end{bmatrix}^\\top$.\n\nYour task is to write a program that:\n- Constructs the FPS $\\Theta$ from given regressor-output pairs $(\\varphi_k, y_k)$ and noise bound $\\varepsilon$.\n- Computes the Chebyshev center $(\\theta_c, r)$ using the LP above.\n- Uses $\\hat{\\theta} = \\theta_c$ to synthesize $k_{\\text{nom}}$ for a specified desired pole $\\alpha_{\\text{des}}$ with a small safety threshold $\\delta = 10^{-8}$.\n- Computes the worst-case closed-loop pole magnitude $\\rho_{\\max}$ via two LPs and the conservatism ratio\n$$\n\\Gamma = \\frac{\\rho_{\\max}}{\\max\\{|\\alpha_{\\text{des}}|, 10^{-12}\\}},\n$$\nwhich is a nonnegative, dimensionless indicator: values larger than $1$ reflect potential conservatism or even instability relative to the design target, while values near $1$ indicate low conservatism.\n\nUse the following test suite. In all cases, take $\\alpha_{\\text{des}} = 0.2$ and $\\delta = 10^{-8}$. For each case, the true parameter vector $\\theta^\\star$ is provided to compute the identification error norm $\\|\\theta_c - \\theta^\\star\\|_2$.\n\n- Case 1 (well-excited data, small uncertainty):\n    - Dimension: $p = 2$ with $\\theta = [a, b]^\\top$.\n    - Data:\n        - $\\varphi_1 = [0.0,~1.0]^\\top$, $y_1 = 1.0$.\n        - $\\varphi_2 = [1.0,~0.0]^\\top$, $y_2 = 0.5$.\n        - $\\varphi_3 = [0.5,~1.0]^\\top$, $y_3 = 1.25$.\n        - $\\varphi_4 = [1.25,~-1.0]^\\top$, $y_4 = -0.375$.\n        - $\\varphi_5 = [-0.375,~0.5]^\\top$, $y_5 = 0.3125$.\n        - $\\varphi_6 = [0.3125,~-0.5]^\\top$, $y_6 = -0.34375$.\n    - Noise bound: $\\varepsilon = 0.05$.\n    - True parameter: $\\theta^\\star = [0.5,~1.0]^\\top$.\n\n- Case 2 (weakly informative but bounded, moderate uncertainty):\n    - Dimension: $p = 2$ with $\\theta = [a, b]^\\top$.\n    - Data:\n        - $\\varphi_1 = [1.0,~1.0]^\\top$, $y_1 = 1.0$.\n        - $\\varphi_2 = [2.0,~2.01]^\\top$, $y_2 = 2.006$.\n        - $\\varphi_3 = [3.0,~3.02]^\\top$, $y_3 = 3.012$.\n        - $\\varphi_4 = [1.5,~1.49]^\\top$, $y_4 = 1.494$.\n    - Noise bound: $\\varepsilon = 0.05$.\n    - True parameter: $\\theta^\\star = [0.4,~0.6]^\\top$.\n\n- Case 3 (well-conditioned, very tight uncertainty):\n    - Dimension: $p = 2$ with $\\theta = [a, b]^\\top$.\n    - Data:\n        - $\\varphi_1 = [1.0,~-1.0]^\\top$, $y_1 = 1.1$.\n        - $\\varphi_2 = [0.5,~2.0]^\\top$, $y_2 = -1.7$.\n        - $\\varphi_3 = [-1.5,~0.5]^\\top$, $y_3 = -0.75$.\n        - $\\varphi_4 = [2.0,~-0.5]^\\top$, $y_4 = 0.85$.\n        - $\\varphi_5 = [-0.5,~-1.5]^\\top$, $y_5 = 1.25$.\n    - Noise bound: $\\varepsilon = 0.001$.\n    - True parameter: $\\theta^\\star = [0.2,~-0.9]^\\top$.\n\nProgram output specification:\n- For each test case, compute and return a list containing, in order:\n    1) the Chebyshev radius $r$,\n    2) the identification error norm $\\|\\theta_c - \\theta^\\star\\|_2$,\n    3) the worst-case closed-loop pole magnitude $\\rho_{\\max}$, and\n    4) the conservatism ratio $\\Gamma$.\n- Express all four quantities as real numbers rounded to six decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of lists enclosed in square brackets, for example:\n\"[ [r1,err1,rho1,gamma1], [r2,err2,rho2,gamma2], [r3,err3,rho3,gamma3] ]\".", "solution": "The problem presented is a valid and well-posed exercise in data-driven control, specifically concerning set-membership identification and robust control analysis for a simple linear system. The theoretical background is sound, the data is complete and consistent, and the objectives are computationally tractable via linear programming. We shall proceed with a systematic solution.\n\nThe core of the problem lies in characterizing the uncertainty in the parameters of an AutoRegressive with eXogenous input (ARX) model and analyzing the consequences for a simple feedback controller. The ARX model is given by\n$$y_k = \\varphi_k^\\top \\theta + e_k$$\nwhere $\\theta = [a, b]^\\top \\in \\mathbb{R}^2$ is the unknown parameter vector, $\\varphi_k \\in \\mathbb{R}^2$ is the regressor vector, $y_k \\in \\mathbb{R}$ is the output, and $e_k$ is a disturbance bounded by $|e_k| \\le \\varepsilon$. This bounded-error assumption allows us to define a Feasible Parameter Set (FPS), denoted $\\Theta$, which is the set of all parameter vectors $\\theta$ consistent with the observed data $(\\varphi_k, y_k)$ and the noise bound $\\varepsilon$.\n\nThe FPS is a polyhedron defined by a set of linear inequalities:\n$$\\Theta = \\left\\{ \\theta \\in \\mathbb{R}^2 \\,\\middle|\\, |y_k - \\varphi_k^\\top \\theta| \\le \\varepsilon \\text{ for all } k \\right\\}$$\nThis is equivalent to the system of linear inequalities $A \\theta \\le b$, where for each data-point $k$, we have two constraints:\n$$\n\\begin{aligned}\n\\varphi_k^\\top \\theta &\\le y_k + \\varepsilon \\\\\n-\\varphi_k^\\top \\theta &\\le -y_k + \\varepsilon\n\\end{aligned}\n$$\nThe matrix $A$ and vector $b$ are constructed by stacking these constraints for all available data points.\n\nThe first step is to compute the Chebyshev center of $\\Theta$. This provides a nominal parameter estimate $\\theta_c$ that is, in a geometric sense, maximally distant from the boundary of the uncertainty set. The center $\\theta_c$ and radius $r$ of the largest inscribed hypersphere are found by solving the following linear program (LP):\n$$\n\\begin{aligned}\n\\max_{\\theta \\in \\mathbb{R}^2,\\, r \\in \\mathbb{R}}~& r\\\\\n\\text{s.t.}~& A_i^\\top \\theta + \\|A_i\\|_2 \\, r \\le b_i,\\quad i = 1,\\dots,m \\\\\n& r \\ge 0\n\\end{aligned}\n$$\nHere, $A_i^\\top$ is the $i$-th row of the matrix $A$ (which is either $\\varphi_k^\\top$ or $-\\varphi_k^\\top$), and $m$ is the total number of inequalities. The decision variables for this LP are the components of the center $\\theta$ and the radius $r$.\n\nOnce the Chebyshev center $\\theta_c = [\\hat{a}, \\hat{b}]^\\top$ is found, we use it as the nominal parameter estimate $\\hat{\\theta}$ to design a state-feedback controller $u_{k-1} = k_{\\text{nom}} y_{k-1}$. The gain $k_{\\text{nom}}$ is chosen using a certainty-equivalence pole-placement strategy to place the nominal closed-loop pole $a+bk_{\\text{nom}}$ at a desired location $\\alpha_{\\text{des}}$. The formula for the gain is:\n$$\nk_{\\text{nom}} = \n\\begin{cases}\n\\dfrac{\\alpha_{\\text{des}} - \\hat{a}}{\\hat{b}}, & \\text{if } |\\hat{b}| \\ge \\delta \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nwhere $\\delta = 10^{-8}$ is a small positive constant to prevent division by a near-zero estimate of the input gain $\\hat{b}$.\n\nWith the controller gain $k_{\\text{nom}}$ fixed, we must evaluate its worst-case performance over the entire feasible parameter set $\\Theta$. The performance metric is the magnitude of the closed-loop pole, $|a + b k_{\\text{nom}}|$. We seek to compute its maximum value:\n$$\\rho_{\\max} = \\max_{\\theta \\in \\Theta} |a + b k_{\\text{nom}}|$$\nThis is the maximum of a linear function over a polyhedron. Let $c = [1, k_{\\text{nom}}]^\\top$. Then the expression inside the absolute value is $c^\\top \\theta$. The problem is equivalent to finding the maximum and minimum values of $c^\\top \\theta$ over $\\Theta$:\n$$\n\\begin{aligned}\nz_{\\max} &= \\max_{\\theta \\in \\Theta}~ c^\\top \\theta \\\\\nz_{\\min} &= \\min_{\\theta \\in \\Theta}~ c^\\top \\theta\n\\end{aligned}\n$$\nThese two values are found by solving two additional LPs. The final worst-case pole magnitude is then $\\rho_{\\max} = \\max\\{z_{\\max}, -z_{\\min}\\}$.\n\nFinally, we compute two summary metrics: the identification error $\\|\\theta_c - \\theta^\\star\\|_2$, where $\\theta^\\star$ is the true parameter vector, and the conservatism ratio $\\Gamma = \\frac{\\rho_{\\max}}{\\max\\{|\\alpha_{\\text{des}}|, 10^{-12}\\}}$. This ratio compares the worst-case achieved pole magnitude to the design target, providing a measure of performance degradation due to parameter uncertainty.\n\nWe will now execute this procedure for each test case provided. The LPs will be solved using `scipy.optimize.linprog`.\n\nFor each test case with $N$ data points $(\\varphi_k, y_k)$, we construct a matrix $A \\in \\mathbb{R}^{2N \\times 2}$ and a vector $b \\in \\mathbb{R}^{2N}$.\n\n1.  **LP for Chebyshev Center**: We solve for $x = [\\theta_1, \\theta_2, r]^\\top = [a, b, r]^\\top$. The objective is to minimize $c^\\top x$ with $c = [0, 0, -1]^\\top$. The constraints $A_{ub} x \\le b_{ub}$ are constructed where each row of $A_{ub}$ is $[(A_i)_1, (A_i)_2, \\|A_i\\|_2]$ and $b_{ub}$ is the original $b$ vector. The bounds are set such that $r \\ge 0$.\n\n2.  **LPs for Worst-Case Pole**: We solve for $x = \\theta = [a, b]^\\top$. The constraints are $A \\theta \\le b$.\n    - To find $z_{\\max}$, we minimize $(-c)^\\top \\theta$ and negate the result.\n    - To find $z_{\\min}$, we minimize $c^\\top \\theta$.\n\nThis systematic application of linear programming provides all the quantities requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"p\": 2,\n            \"data\": [\n                (np.array([0.0, 1.0]), 1.0),\n                (np.array([1.0, 0.0]), 0.5),\n                (np.array([0.5, 1.0]), 1.25),\n                (np.array([1.25, -1.0]), -0.375),\n                (np.array([-0.375, 0.5]), 0.3125),\n                (np.array([0.3125, -0.5]), -0.34375),\n            ],\n            \"epsilon\": 0.05,\n            \"theta_star\": np.array([0.5, 1.0]),\n        },\n        {\n            \"name\": \"Case 2\",\n            \"p\": 2,\n            \"data\": [\n                (np.array([1.0, 1.0]), 1.0),\n                (np.array([2.0, 2.01]), 2.006),\n                (np.array([3.0, 3.02]), 3.012),\n                (np.array([1.5, 1.49]), 1.494),\n            ],\n            \"epsilon\": 0.05,\n            \"theta_star\": np.array([0.4, 0.6]),\n        },\n        {\n            \"name\": \"Case 3\",\n            \"p\": 2,\n            \"data\": [\n                (np.array([1.0, -1.0]), 1.1),\n                (np.array([0.5, 2.0]), -1.7),\n                (np.array([-1.5, 0.5]), -0.75),\n                (np.array([2.0, -0.5]), 0.85),\n                (np.array([-0.5, -1.5]), 1.25),\n            ],\n            \"epsilon\": 0.001,\n            \"theta_star\": np.array([0.2, -0.9]),\n        },\n    ]\n\n    alpha_des = 0.2\n    delta = 1e-8\n\n    all_results = []\n    \n    for case in test_cases:\n        p = case[\"p\"]\n        data = case[\"data\"]\n        epsilon = case[\"epsilon\"]\n        theta_star = case[\"theta_star\"]\n        \n        # 1. Construct the FPS polyhedron A_poly * theta <= b_poly\n        num_data_points = len(data)\n        A_poly = np.zeros((2 * num_data_points, p))\n        b_poly = np.zeros(2 * num_data_points)\n        \n        for i, (phi_k, y_k) in enumerate(data):\n            A_poly[2 * i, :] = phi_k\n            b_poly[2 * i] = y_k + epsilon\n            A_poly[2 * i + 1, :] = -phi_k\n            b_poly[2 * i + 1] = -y_k + epsilon\n\n        # 2. Compute the Chebyshev center (theta_c, r)\n        # LP variables: [theta_1, ..., theta_p, r]\n        c_chebyshev = np.zeros(p + 1)\n        c_chebyshev[-1] = -1  # Maximize r by minimizing -r\n\n        A_chebyshev = np.zeros((2 * num_data_points, p + 1))\n        for i in range(2 * num_data_points):\n            A_chebyshev[i, :p] = A_poly[i, :]\n            A_chebyshev[i, p] = np.linalg.norm(A_poly[i, :])\n        \n        b_chebyshev = b_poly\n        \n        bounds_chebyshev = [(None, None)] * p + [(0, None)]\n        \n        res_chebyshev = linprog(c_chebyshev, A_ub=A_chebyshev, b_ub=b_chebyshev, bounds=bounds_chebyshev, method='highs')\n\n        if not res_chebyshev.success:\n            raise RuntimeError(f\"Chebyshev center LP failed for {case['name']}\")\n            \n        theta_c = res_chebyshev.x[:p]\n        r = res_chebyshev.x[p]\n        \n        # 3. Synthesize controller gain k_nom\n        a_hat, b_hat = theta_c\n        if abs(b_hat) >= delta:\n            k_nom = (alpha_des - a_hat) / b_hat\n        else:\n            k_nom = 0.0\n            \n        # 4. Compute worst-case pole magnitude rho_max\n        c_pole = np.array([1.0, k_nom])\n        bounds_pole = [(None, None)] * p\n\n        # Find z_max = max c_pole^T * theta\n        res_z_max = linprog(-c_pole, A_ub=A_poly, b_ub=b_poly, bounds=bounds_pole, method='highs')\n        if not res_z_max.success:\n            raise RuntimeError(f\"z_max LP failed for {case['name']}\")\n        z_max = -res_z_max.fun\n\n        # Find z_min = min c_pole^T * theta\n        res_z_min = linprog(c_pole, A_ub=A_poly, b_ub=b_poly, bounds=bounds_pole, method='highs')\n        if not res_z_min.success:\n            raise RuntimeError(f\"z_min LP failed for {case['name']}\")\n        z_min = res_z_min.fun\n\n        rho_max = max(z_max, -z_min)\n\n        # 5. Compute additional metrics\n        id_error_norm = np.linalg.norm(theta_c - theta_star)\n        conservatism_ratio = rho_max / max(abs(alpha_des), 1e-12)\n\n        # 6. Format results\n        case_results = [\n            round(r, 6),\n            round(id_error_norm, 6),\n            round(rho_max, 6),\n            round(conservatism_ratio, 6)\n        ]\n        all_results.append(case_results)\n\n    # Final print statement\n    results_str = \", \".join([str(res) for res in all_results])\n    print(f\"[{results_str}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "2698754"}]}