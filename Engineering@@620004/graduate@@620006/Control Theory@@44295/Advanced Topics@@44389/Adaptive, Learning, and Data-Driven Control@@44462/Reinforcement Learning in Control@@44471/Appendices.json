{"hands_on_practices": [{"introduction": "Before diving into model-free reinforcement learning, it is crucial to master the foundational principles of dynamic programming, which assume a perfect model of the environment. This first practice focuses on value iteration, a cornerstone algorithm that computes the optimal state-value function by repeatedly applying the Bellman optimality operator [@problem_id:2738630]. By manually performing these calculations, you will gain a concrete understanding of how value functions converge and how policies are systematically improved, providing the theoretical bedrock for more advanced RL techniques.", "problem": "Consider a finite Markov Decision Process (MDP) with state space $\\mathcal{S}=\\{1,2,3\\}$, action set $\\mathcal{A}=\\{a,b\\}$ available at every state, discount factor $\\gamma=\\frac{4}{5}$, reward function $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$, and transition probabilities $P(s' \\mid s,a)$. The data are:\n- At state $1$:\n  - Action $a$: reward $r(1,a)=2$, transitions $P(1 \\mid 1,a)=\\frac{1}{2}$, $P(2 \\mid 1,a)=\\frac{1}{2}$.\n  - Action $b$: reward $r(1,b)=1$, transitions $P(3 \\mid 1,b)=1$.\n- At state $2$:\n  - Action $a$: reward $r(2,a)=0$, transitions $P(1 \\mid 2,a)=1$.\n  - Action $b$: reward $r(2,b)=3$, transitions $P(2 \\mid 2,b)=\\frac{7}{10}$, $P(3 \\mid 2,b)=\\frac{3}{10}$.\n- At state $3$:\n  - Action $a$: reward $r(3,a)=1$, transitions $P(3 \\mid 3,a)=1$.\n  - Action $b$: reward $r(3,b)=0$, transitions $P(1 \\mid 3,b)=1$.\n\nLet the initial value function be $V_{0}$ given by the column vector $V_{0}=\\begin{pmatrix}2\\\\2\\\\1\\end{pmatrix}$. Let the Bellman optimality operator $T$ act on any value function $V$ by\n$$\n(TV)(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Big[\\, r(s,a) \\;+\\; \\gamma \\sum_{s'\\in\\mathcal{S}} P(s' \\mid s,a)\\, V(s') \\,\\Big].\n$$\nFor any value function $V$, define the greedy policy $\\pi=\\operatorname{Greedy}(V)$ by selecting in each state $s$ any maximizer of the right-hand side inside the maximum above. For any stationary policy $\\pi$, define its value function $V^{\\pi}$ as the unique solution of the Bellman equations $V^{\\pi} = r^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$, and define the optimal value $V^{\\star}$ as the unique fixed point of $T$, that is, $V^{\\star}=T V^{\\star}$. For any policy $\\pi$, define its greedy-policy suboptimality as\n$$\n\\Delta_{\\infty}(\\pi) \\;=\\; \\| V^{\\star} - V^{\\pi} \\|_{\\infty} \\;=\\; \\max_{s\\in\\mathcal{S}} \\big| V^{\\star}(s) - V^{\\pi}(s) \\big|.\n$$\n\nTasks:\n- Starting from $V_{0}$, compute one step of value iteration $V_{1}=T V_{0}$.\n- Compute the greedy policies $\\pi_{0}=\\operatorname{Greedy}(V_{0})$ and $\\pi_{1}=\\operatorname{Greedy}(V_{1})$.\n- Exactly evaluate $V^{\\pi_{0}}$ and $V^{\\pi_{1}}$ by solving their respective Bellman equations.\n- Exactly compute $V^{\\star}$ from first principles using the definition of $T$ and a consistent selection of maximizing actions, and verify optimality by checking action-wise optimality at each state.\n- Finally, compute the improvement\n$$\n\\Delta \\;=\\; \\Delta_{\\infty}(\\pi_{0}) \\;-\\; \\Delta_{\\infty}(\\pi_{1}),\n$$\nand report $\\Delta$ as an exact value (no rounding required).", "solution": "We begin from the standard definitions of a Markov Decision Process (MDP), the Bellman optimality operator $T$, and policy evaluation. The value iteration update is $V_{k+1} = T V_{k}$, where\n$$\n(TV)(s) \\;=\\; \\max_{a \\in \\{a,b\\}} \\left[ r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) V(s') \\right],\n$$\nwith $\\gamma=\\frac{4}{5}$.\n\nStep $1$: Compute $V_{1} = T V_{0}$ from $V_{0}=\\begin{pmatrix}2\\\\2\\\\1\\end{pmatrix}$.\n\nFor each state $s$, define the action-value for $V_{0}$:\n- At $s=1$:\n  - $Q(1,a;V_{0}) \\;=\\; 2 + \\frac{4}{5}\\left( \\frac{1}{2}\\cdot 2 + \\frac{1}{2}\\cdot 2 \\right) \\;=\\; 2 + \\frac{4}{5}\\cdot 2 \\;=\\; 2 + \\frac{8}{5} \\;=\\; \\frac{18}{5}$.\n  - $Q(1,b;V_{0}) \\;=\\; 1 + \\frac{4}{5}\\cdot 1 \\;=\\; 1 + \\frac{4}{5} \\;=\\; \\frac{9}{5}$.\n  Hence $(T V_{0})(1) \\;=\\; \\max\\left\\{ \\frac{18}{5}, \\frac{9}{5} \\right\\} \\;=\\; \\frac{18}{5}$.\n- At $s=2$:\n  - $Q(2,a;V_{0}) \\;=\\; 0 + \\frac{4}{5}\\cdot 2 \\;=\\; \\frac{8}{5}$.\n  - $Q(2,b;V_{0}) \\;=\\; 3 + \\frac{4}{5} \\left( \\frac{7}{10}\\cdot 2 + \\frac{3}{10}\\cdot 1 \\right) \\;=\\; 3 + \\frac{4}{5}\\cdot \\frac{17}{10} \\;=\\; 3 + \\frac{34}{25} \\;=\\; \\frac{109}{25}$.\n  Hence $(T V_{0})(2) \\;=\\; \\max\\left\\{ \\frac{8}{5}, \\frac{109}{25} \\right\\} \\;=\\; \\frac{109}{25}$.\n- At $s=3$:\n  - $Q(3,a;V_{0}) \\;=\\; 1 + \\frac{4}{5}\\cdot 1 \\;=\\; \\frac{9}{5}$.\n  - $Q(3,b;V_{0}) \\;=\\; 0 + \\frac{4}{5}\\cdot 2 \\;=\\; \\frac{8}{5}$.\n  Hence $(T V_{0})(3) \\;=\\; \\max\\left\\{ \\frac{9}{5}, \\frac{8}{5} \\right\\} \\;=\\; \\frac{9}{5}$.\n\nTherefore,\n$$\nV_{1} \\;=\\; T V_{0} \\;=\\; \\begin{pmatrix} \\frac{18}{5} \\\\[4pt] \\frac{109}{25} \\\\[4pt] \\frac{9}{5} \\end{pmatrix}.\n$$\n\nStep $2$: Compute $\\pi_{0}=\\operatorname{Greedy}(V_{0})$ and $\\pi_{1}=\\operatorname{Greedy}(V_{1})$.\n\nFrom the $Q$-values above for $V_{0}$:\n- At $s=1$, $\\max\\{Q(1,a;V_{0}), Q(1,b;V_{0})\\} = \\frac{18}{5}$, attained by action $a$.\n- At $s=2$, $\\max\\{Q(2,a;V_{0}), Q(2,b;V_{0})\\} = \\frac{109}{25}$, attained by action $b$.\n- At $s=3$, $\\max\\{Q(3,a;V_{0}), Q(3,b;V_{0})\\} = \\frac{9}{5}$, attained by action $a$.\nThus $\\pi_{0}=(a,b,a)$.\n\nFor $\\pi_{1}=\\operatorname{Greedy}(V_{1})$, compute $Q(\\cdot,\\cdot;V_{1})$:\n- At $s=1$:\n  $$\n  Q(1,a;V_{1}) \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{18}{5} + \\frac{109}{25} }{2} \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{199}{50} \\;=\\; 2 + \\frac{398}{250} \\;=\\; \\frac{648}{125},\n  $$\n  $$\n  Q(1,b;V_{1}) \\;=\\; 1 + \\frac{4}{5}\\cdot \\frac{9}{5} \\;=\\; 1 + \\frac{36}{25} \\;=\\; \\frac{61}{25}.\n  $$\n  Since $\\frac{648}{125} > \\frac{61}{25}$, choose $a$.\n- At $s=2$:\n  $$\n  Q(2,a;V_{1}) \\;=\\; 0 + \\frac{4}{5}\\cdot \\frac{18}{5} \\;=\\; \\frac{72}{25}, \\quad\n  Q(2,b;V_{1}) \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{7}{10}\\cdot \\frac{109}{25} + \\frac{3}{10}\\cdot \\frac{9}{5} \\right).\n  $$\n  Compute the bracket:\n  $$\n  \\frac{7}{10}\\cdot \\frac{109}{25} + \\frac{3}{10}\\cdot \\frac{9}{5} \\;=\\; \\frac{763}{250} + \\frac{27}{50} \\;=\\; \\frac{763}{250} + \\frac{135}{250} \\;=\\; \\frac{898}{250} \\;=\\; \\frac{449}{125}.\n  $$\n  Thus\n  $$\n  Q(2,b;V_{1}) \\;=\\; 3 + \\frac{4}{5}\\cdot \\frac{449}{125} \\;=\\; 3 + \\frac{1796}{625} \\;=\\; \\frac{1875}{625} + \\frac{1796}{625} \\;=\\; \\frac{3671}{625}.\n  $$\n  Since $\\frac{3671}{625} > \\frac{72}{25}$, choose $b$.\n- At $s=3$:\n  $$\n  Q(3,a;V_{1}) \\;=\\; 1 + \\frac{4}{5}\\cdot \\frac{9}{5} \\;=\\; \\frac{61}{25}, \\quad\n  Q(3,b;V_{1}) \\;=\\; 0 + \\frac{4}{5}\\cdot \\frac{18}{5} \\;=\\; \\frac{72}{25}.\n  $$\n  Since $\\frac{72}{25} > \\frac{61}{25}$, choose $b$.\n\nHence $\\pi_{1}=(a,b,b)$.\n\nStep $3$: Exactly evaluate $V^{\\pi_{0}}$ and $V^{\\pi_{1}}$ by solving $V^{\\pi} = r^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$.\n\nWrite $V^{\\pi} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{pmatrix}$.\n\nFor $\\pi_{0}=(a,b,a)$:\n- State $1$ with action $a$: $x_{1} = 2 + \\frac{4}{5}\\left( \\frac{1}{2} x_{1} + \\frac{1}{2} x_{2} \\right) \\;\\Rightarrow\\; \\frac{3}{5} x_{1} - \\frac{2}{5} x_{2} = 2 \\;\\Rightarrow\\; 3 x_{1} - 2 x_{2} = 10$.\n- State $2$ with action $b$: $x_{2} = 3 + \\frac{4}{5}\\left( \\frac{7}{10} x_{2} + \\frac{3}{10} x_{3} \\right) \\;\\Rightarrow\\; \\frac{11}{25} x_{2} - \\frac{6}{25} x_{3} = 3 \\;\\Rightarrow\\; 11 x_{2} - 6 x_{3} = 75$.\n- State $3$ with action $a$: $x_{3} = 1 + \\frac{4}{5} x_{3} \\;\\Rightarrow\\; \\frac{1}{5} x_{3} = 1 \\;\\Rightarrow\\; x_{3}=5$.\n\nSubstitute $x_{3}=5$ into $11 x_{2} - 6 x_{3} = 75$ to get $11 x_{2} = 105$, so $x_{2} = \\frac{105}{11}$. Then $3 x_{1} - 2 x_{2} = 10$ gives $3 x_{1} = 10 + \\frac{210}{11} = \\frac{320}{11}$, hence $x_{1} = \\frac{320}{33}$. Therefore\n$$\nV^{\\pi_{0}} \\;=\\; \\begin{pmatrix} \\frac{320}{33} \\\\[4pt] \\frac{105}{11} \\\\[4pt] 5 \\end{pmatrix}.\n$$\n\nFor $\\pi_{1}=(a,b,b)$:\n- State $1$ with action $a$: $x_{1} = 2 + \\frac{4}{5}\\left( \\frac{1}{2} x_{1} + \\frac{1}{2} x_{2} \\right) \\;\\Rightarrow\\; 3 x_{1} - 2 x_{2} = 10$.\n- State $2$ with action $b$: $11 x_{2} - 6 x_{3} = 75$.\n- State $3$ with action $b$: $x_{3} = \\frac{4}{5} x_{1}$.\n\nUse $x_{3}=\\frac{4}{5} x_{1}$ in $11 x_{2} - 6 x_{3} = 75$ to get $11 x_{2} - \\frac{24}{5} x_{1} = 75 \\;\\Rightarrow\\; 55 x_{2} - 24 x_{1} = 375$. From $3 x_{1} - 2 x_{2} = 10$, express $x_{2} = \\frac{3}{2} x_{1} - 5$ and substitute:\n$$\n55\\left( \\frac{3}{2} x_{1} - 5 \\right) - 24 x_{1} = 375 \\;\\Rightarrow\\; \\frac{165}{2} x_{1} - 275 - 24 x_{1} = 375 \\;\\Rightarrow\\; \\frac{117}{2} x_{1} = 650,\n$$\nso $x_{1} = \\frac{1300}{117} = \\frac{100}{9}$, $x_{3} = \\frac{4}{5} x_{1} = \\frac{80}{9}$, and $x_{2} = \\frac{3}{2}\\cdot \\frac{100}{9} - 5 = \\frac{50}{3} - 5 = \\frac{35}{3}$. Therefore\n$$\nV^{\\pi_{1}} \\;=\\; \\begin{pmatrix} \\frac{100}{9} \\\\[4pt] \\frac{35}{3} \\\\[4pt] \\frac{80}{9} \\end{pmatrix}.\n$$\n\nStep $4$: Compute the optimal value $V^{\\star}$ and verify optimality.\n\nBy definition, $V^{\\star}$ satisfies $V^{\\star} = T V^{\\star}$. A candidate optimal stationary policy is $\\pi^{\\star}=(a,b,b)$, which we already analyzed; its value is $V^{\\pi^{\\star}}=\\begin{pmatrix} \\frac{100}{9} \\\\ \\frac{35}{3} \\\\ \\frac{80}{9} \\end{pmatrix}$. To verify optimality, check that at each state, the selected action maximizes the action-value computed with $V^{\\pi^{\\star}}$:\n- At $s=1$:\n  $$\n  Q(1,a;V^{\\pi^{\\star}}) \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{100}{9} + \\frac{35}{3} }{2}\n  \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{100}{9} + \\frac{105}{9} }{2}\n  \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{205}{18}\n  \\;=\\; \\frac{100}{9},\n  $$\n  while $Q(1,b;V^{\\pi^{\\star}}) = 1 + \\frac{4}{5}\\cdot \\frac{80}{9} = \\frac{73}{9} < \\frac{100}{9}$, so action $a$ is optimal.\n- At $s=2$:\n  $$\n  Q(2,b;V^{\\pi^{\\star}}) \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{7}{10}\\cdot \\frac{35}{3} + \\frac{3}{10}\\cdot \\frac{80}{9} \\right)\n  \\;=\\; 3 + \\frac{26}{3}\n  \\;=\\; \\frac{35}{3},\n  $$\n  while $Q(2,a;V^{\\pi^{\\star}}) = \\frac{4}{5}\\cdot \\frac{100}{9} = \\frac{80}{9} < \\frac{35}{3}$, so action $b$ is optimal.\n- At $s=3$:\n  $$\n  Q(3,b;V^{\\pi^{\\star}}) \\;=\\; \\frac{4}{5}\\cdot \\frac{100}{9} \\;=\\; \\frac{80}{9},\n  $$\n  while $Q(3,a;V^{\\pi^{\\star}}) = 1 + \\frac{4}{5}\\cdot \\frac{80}{9} = \\frac{73}{9} < \\frac{80}{9}$, so action $b$ is optimal.\n\nThus $\\pi^{\\star}=(a,b,b)$ is greedy with respect to its own value and hence optimal. Therefore $V^{\\star} = V^{\\pi^{\\star}} = \\begin{pmatrix} \\frac{100}{9} \\\\ \\frac{35}{3} \\\\ \\frac{80}{9} \\end{pmatrix}$.\n\nStep $5$: Compute the greedy-policy suboptimalities and the improvement $\\Delta$.\n\nBy definition,\n$$\n\\Delta_{\\infty}(\\pi_{0}) \\;=\\; \\| V^{\\star} - V^{\\pi_{0}} \\|_{\\infty}, \\quad\n\\Delta_{\\infty}(\\pi_{1}) \\;=\\; \\| V^{\\star} - V^{\\pi_{1}} \\|_{\\infty}.\n$$\nWe have $V^{\\pi_{1}}=V^{\\star}$, so $\\Delta_{\\infty}(\\pi_{1})=0$. For $\\pi_{0}$,\n$$\nV^{\\star} - V^{\\pi_{0}} \\;=\\; \\begin{pmatrix}\n\\frac{100}{9} - \\frac{320}{33} \\\\\n\\frac{35}{3} - \\frac{105}{11} \\\\\n\\frac{80}{9} - 5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{140}{99} \\\\\n\\frac{70}{33} \\\\\n\\frac{35}{9}\n\\end{pmatrix},\n$$\nso\n$$\n\\Delta_{\\infty}(\\pi_{0}) \\;=\\; \\max\\left\\{ \\frac{140}{99}, \\frac{70}{33}, \\frac{35}{9} \\right\\} \\;=\\; \\frac{35}{9}.\n$$\nTherefore, the improvement is\n$$\n\\Delta \\;=\\; \\Delta_{\\infty}(\\pi_{0}) - \\Delta_{\\infty}(\\pi_{1}) \\;=\\; \\frac{35}{9} - 0 \\;=\\; \\frac{35}{9}.\n$$\n\nThis strictly positive value verifies the improvement in greedy policy suboptimality after one step of value iteration and policy improvement.", "answer": "$$\\boxed{\\frac{35}{9}}$$", "id": "2738630"}, {"introduction": "In most control applications, a perfect model of the system dynamics is unavailable, necessitating model-free approaches. This exercise introduces Q-learning, a hallmark temporal-difference (TD) algorithm that learns the optimal action-value function directly from interaction with the environment [@problem_id:2738645]. Working through a sequence of updates will provide you with a hands-on feel for how an agent bootstraps value estimates from experienced rewards and subsequent state values, a core mechanism of many modern RL algorithms.", "problem": "Consider a finite Markov Decision Process (MDP) with state set $\\mathcal{S}=\\{s_{0},s_{1},s_{2}\\}$, where $s_{2}$ is terminal, and action set $\\mathcal{A}=\\{a_{0},a_{1}\\}$. The one-step transition dynamics and rewards are deterministic and given by the following descriptions, which are consistent with the controlled Markov property and an additive reward signal:\n- From $s_{0}$:\n  - Taking $a_{0}$ leads to $s_{1}$ with reward $2$.\n  - Taking $a_{1}$ leads to $s_{2}$ with reward $0$.\n- From $s_{1}$:\n  - Taking $a_{0}$ leads to $s_{2}$ with reward $0$.\n  - Taking $a_{1}$ leads to $s_{1}$ with reward $-1$.\n- From $s_{2}$: the episode terminates immediately. For any evaluation that involves $\\max_{a'}$ in a terminal state, use the convention that the maximum over the empty action set equals $0$.\n\nYou will approximate the optimal state-action value function (the $Q$-function) using off-policy temporal-difference updates arising from the Bellman optimality operator, as in the standard Q-learning method in reinforcement learning (RL). The discount factor is $\\gamma=\\tfrac{1}{2}$. The initial action-value table over the nonterminal states is uniform: $Q_{0}(s,a)=1$ for all $(s,a)\\in\\{s_{0},s_{1}\\}\\times\\{a_{0},a_{1}\\}$. The step size (learning rate) is constant and equal to $\\alpha=\\tfrac{1}{2}$ for each update. You observe the following ordered sequence of transitions generated by some behavior policy (which need not be optimal):\n1. $(s,a,r,s')=(s_{0},a_{0},2,s_{1})$,\n2. $(s,a,r,s')=(s_{1},a_{1},-1,s_{1})$,\n3. $(s,a,r,s')=(s_{1},a_{0},0,s_{2})$.\n\nStarting from $Q_{0}$ and applying three off-policy Q-learning updates in the order of the transitions above, compute the resulting action-value table $Q_{3}$ over the nonterminal state-action pairs $(s_{0},a_{0})$, $(s_{0},a_{1})$, $(s_{1},a_{0})$, $(s_{1},a_{1})$. Report your final answer as a single row matrix in that order. No rounding is required, and exact fractional values are expected.", "solution": "The Q-learning update rule for a transition from state $s$ to state $s'$ with action $a$ and reward $r$ is given by:\n$$Q_{k+1}(s,a) = Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') - Q_{k}(s,a) \\right]$$\nwhere $k$ is the update index, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. Given the problem parameters, we have $\\alpha=\\frac{1}{2}$ and $\\gamma=\\frac{1}{2}$. The update rule can be rewritten as:\n$$Q_{k+1}(s,a) = \\left(1-\\alpha\\right)Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') \\right]$$\n$$Q_{k+1}(s,a) = \\frac{1}{2} Q_{k}(s,a) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{k}(s', a') \\right]$$\n\nThe initial action-value table for nonterminal states is given as $Q_{0}(s, a) = 1$ for all $(s, a) \\in \\{s_{0}, s_{1}\\} \\times \\{a_{0}, a_{1}\\}$. So we begin with:\n$$Q_{0}(s_{0}, a_{0}) = 1$$\n$$Q_{0}(s_{0}, a_{1}) = 1$$\n$$Q_{0}(s_{1}, a_{0}) = 1$$\n$$Q_{0}(s_{1}, a_{1}) = 1$$\nFor the terminal state $s_{2}$, we use the convention that $\\max_{a'} Q(s_{2}, a') = 0$.\n\nWe now proceed with the updates in the specified order.\n\n**Update 1:**\nThe first transition is $(s, a, r, s') = (s_{0}, a_{0}, 2, s_{1})$. We update $Q(s_{0}, a_{0})$. The other values in the table remain unchanged for this step. The next state is $s_{1}$. We must first compute the maximum Q-value from state $s_{1}$ using the current table, $Q_{0}$.\n$$\\max_{a'} Q_{0}(s_{1}, a') = \\max\\{Q_{0}(s_{1}, a_{0}), Q_{0}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\nNow we apply the update rule for $Q_{1}(s_{0}, a_{0})$:\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2} Q_{0}(s_{0}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{0}(s_{1}, a') \\right]$$\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 2 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ 2 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( \\frac{5}{2} \\right) = \\frac{1}{2} + \\frac{5}{4} = \\frac{2}{4} + \\frac{5}{4} = \\frac{7}{4}$$\nAfter this first update, the action-value table $Q_{1}$ is:\n$$Q_{1}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{1}(s_{0}, a_{1}) = 1$$\n$$Q_{1}(s_{1}, a_{0}) = 1$$\n$$Q_{1}(s_{1}, a_{1}) = 1$$\n\n**Update 2:**\nThe second transition is $(s, a, r, s') = (s_{1}, a_{1}, -1, s_{1})$. We update $Q(s_{1}, a_{1})$ using the values from $Q_{1}$. The next state is $s_{1}$.\n$$\\max_{a'} Q_{1}(s_{1}, a') = \\max\\{Q_{1}(s_{1}, a_{0}), Q_{1}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\nNow we apply the update rule for $Q_{2}(s_{1}, a_{1})$:\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2} Q_{1}(s_{1}, a_{1}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{1}(s_{1}, a') \\right]$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ -1 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ -1 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( -\\frac{1}{2} \\right) = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$$\nAfter this second update, the action-value table $Q_{2}$ is:\n$$Q_{2}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{2}(s_{0}, a_{1}) = 1$$\n$$Q_{2}(s_{1}, a_{0}) = 1$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{4}$$\n\n**Update 3:**\nThe third transition is $(s, a, r, s') = (s_{1}, a_{0}, 0, s_{2})$. We update $Q(s_{1}, a_{0})$ using the values from $Q_{2}$. The next state $s_{2}$ is terminal.\n$$\\max_{a'} Q_{2}(s_{2}, a') = 0$$\nNow we apply the update rule for $Q_{3}(s_{1}, a_{0})$:\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2} Q_{2}(s_{1}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{2}(s_{2}, a') \\right]$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 0 + \\frac{1}{2}(0) \\right] = \\frac{1}{2} + 0 = \\frac{1}{2}$$\nAfter this final update, the action-value table $Q_{3}$ is:\n$$Q_{3}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{3}(s_{0}, a_{1}) = 1$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}$$\n$$Q_{3}(s_{1}, a_{1}) = \\frac{1}{4}$$\nThe problem requires reporting these final values as a single row matrix in the order $(s_{0}, a_{0})$, $(s_{0}, a_{1})$, $(s_{1}, a_{0})$, $(s_{1}, a_{1})$. The values are $\\frac{7}{4}$, $1$, $\\frac{1}{2}$, and $\\frac{1}{4}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{4} & 1 & \\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}}$$", "id": "2738645"}, {"introduction": "While value-based methods learn a value function and derive a policy from it, policy gradient methods directly parameterize and optimize the policy itself. This practice explores the REINFORCE algorithm, a foundational policy gradient technique, by having you compute both the exact policy gradient and a Monte Carlo estimate from sampled episodes [@problem_id:2738661]. This comparison provides crucial intuition into the nature of policy optimization and the inherent stochasticity of sample-based gradient estimation, a key concept in designing advanced actor-critic controllers.", "problem": "Consider the following episodic Markov Decision Process (MDP) used in Reinforcement Learning (RL) for control. The state set is $\\{s_{0}, s_{1}, s_{2}\\}$ with $s_{0}$ as the unique start state. The action set is $\\{0,1\\}$. The episode length is $2$ time steps ($t=0,1$) and the discount factor is $\\gamma=1$. The transition and reward structure is:\n- At $s_{0}$: taking action $a=1$ deterministically moves to $s_{1}$ with immediate reward $0$; taking action $a=0$ deterministically moves to $s_{2}$ with immediate reward $0$.\n- At $s_{1}$ (terminal after acting): taking action $a=1$ yields reward $2$; taking action $a=0$ yields reward $0$.\n- At $s_{2}$ (terminal after acting): taking action $a=1$ yields reward $1$; taking action $a=0$ yields reward $3$.\n\nThe stochastic policy is a state-wise independent Bernoulli with logistic parameterization. For each $s \\in \\{s_{0}, s_{1}, s_{2}\\}$, the policy chooses action $a=1$ with probability $\\pi_{\\theta}(1 \\mid s)=\\sigma(\\theta_{s})$ and action $a=0$ with probability $\\pi_{\\theta}(0 \\mid s)=1-\\sigma(\\theta_{s})$, where $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ and $\\theta=\\big(\\theta_{s_{0}},\\theta_{s_{1}},\\theta_{s_{2}}\\big)$. The performance objective is the expected return $J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\big[\\sum_{t=0}^{1} r_{t}\\big]$.\n\nUse the concrete parameter vector $\\theta=\\big(0.3,-0.4,0.8\\big)$.\n\nTask:\n1. Compute the exact policy gradient $\\nabla_{\\theta} J(\\theta)$ by explicitly enumerating all possible trajectories and taking the gradient of the expected return with respect to $\\theta$.\n2. A Monte Carlo (MC) estimate of the policy gradient is formed using the likelihood-ratio (REINFORCE) estimator without a baseline over $K$ sampled episodes, as $\\widehat{g}=\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{t=0}^{1}\\nabla_{\\theta}\\ln \\pi_{\\theta}(a_{t}^{(k)} \\mid s_{t}^{(k)})\\,G_{t}^{(k)}$, where $G_{t}^{(k)}=\\sum_{t' = t}^{1} r_{t'}^{(k)}$. Suppose that, at the given $\\theta$, you observed $K=5$ episodes with the following state-action-reward sequences:\n   - Episode $1$: at $s_{0}$ choose $a=1$ then at $s_{1}$ choose $a=0$; terminal reward $0$.\n   - Episode $2$: at $s_{0}$ choose $a=0$ then at $s_{2}$ choose $a=0$; terminal reward $3$.\n   - Episode $3$: at $s_{0}$ choose $a=1$ then at $s_{1}$ choose $a=1$; terminal reward $2$.\n   - Episode $4$: at $s_{0}$ choose $a=0$ then at $s_{2}$ choose $a=1$; terminal reward $1$.\n   - Episode $5$: at $s_{0}$ choose $a=1$ then at $s_{1}$ choose $a=1$; terminal reward $2$.\n\nCompute the MC estimate $\\widehat{g}$ from these episodes.\n\nFinally, report the Euclidean norm of the difference between the exact gradient and the MC estimate, that is $\\|\\nabla_{\\theta} J(\\theta)-\\widehat{g}\\|_{2}$. Round your answer to four significant figures. Provide a unitless real number as your final answer.", "solution": "We begin from first principles in Reinforcement Learning (RL). The return for a trajectory is the sum of rewards $\\sum_{t=0}^{1} r_{t}$ (with $\\gamma=1$). The expected return under policy $\\pi_{\\theta}$ is $J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{t=0}^{1} r_{t}\\right]$. For this small tabular environment, we can exactly enumerate all possible trajectories and compute $J(\\theta)$ as a function of $\\theta$, then differentiate.\n\nDefine $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ and for brevity let $p_{0}=\\sigma(\\theta_{s_{0}})$, $p_{1}=\\sigma(\\theta_{s_{1}})$, and $p_{2}=\\sigma(\\theta_{s_{2}})$. Under the environment dynamics:\n- At time $t=0$ in $s_{0}$, with probability $p_{0}$ the agent goes to $s_{1}$ (after taking $a=1$), and with probability $1-p_{0}$ it goes to $s_{2}$ (after taking $a=0$). There is no reward at $t=0$.\n- At time $t=1$, if in $s_{1}$ the expected reward is $2 \\cdot p_{1} + 0 \\cdot (1-p_{1})=2p_{1}$. If in $s_{2}$ the expected reward is $1 \\cdot p_{2} + 3 \\cdot (1-p_{2})=3-2p_{2}$.\n\nTherefore, by enumerating the two-step trajectories and aggregating, the expected return is\n$$\nJ(\\theta)=p_{0}\\cdot (2p_{1}) + (1-p_{0})\\cdot (3-2p_{2}).\n$$\nWe differentiate $J(\\theta)$ with respect to each component of $\\theta$. Using $\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\sigma(\\theta)=\\sigma(\\theta)\\big(1-\\sigma(\\theta)\\big)$ and the chain rule, we obtain\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{0}}}\n= \\frac{\\mathrm{d}p_{0}}{\\mathrm{d}\\theta_{s_{0}}}\\,\\big(2p_{1}-(3-2p_{2})\\big)\n= p_{0}\\big(1-p_{0}\\big)\\,\\big(2p_{1}-3+2p_{2}\\big),\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{1}}}\n= p_{0}\\cdot 2\\,\\frac{\\mathrm{d}p_{1}}{\\mathrm{d}\\theta_{s_{1}}}\n= 2p_{0}\\,p_{1}\\big(1-p_{1}\\big),\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{2}}}\n= (1-p_{0})\\cdot \\big(-2\\big)\\,\\frac{\\mathrm{d}p_{2}}{\\mathrm{d}\\theta_{s_{2}}}\n= -2(1-p_{0})\\,p_{2}\\big(1-p_{2}\\big).\n$$\n\nNow substitute the given $\\theta=\\big(0.3,-0.4,0.8\\big)$:\n$$\np_{0}=\\sigma(0.3)=\\frac{1}{1+\\exp(-0.3)}\\approx 0.574442516,\\quad\np_{1}=\\sigma(-0.4)=\\frac{1}{1+\\exp(0.4)}\\approx 0.401312339,\\quad\np_{2}=\\sigma(0.8)=\\frac{1}{1+\\exp(-0.8)}\\approx 0.689974481.\n$$\nAlso,\n$$\np_{0}\\big(1-p_{0}\\big)\\approx 0.244458311,\\quad\np_{1}\\big(1-p_{1}\\big)\\approx 0.240260745,\\quad\np_{2}\\big(1-p_{2}\\big)\\approx 0.213909696.\n$$\nThus, the exact policy gradient is\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{0}}}\n\\approx 0.244458311\\cdot\\big(2\\cdot 0.401312339 - 3 + 2\\cdot 0.689974481\\big)\n= 0.244458311\\cdot(-0.81742636)\\approx -0.199826659,\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{1}}}\n\\approx 2\\cdot 0.574442516\\cdot 0.240260745\\approx 0.276031974,\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{2}}}\n\\approx -2\\cdot 0.425557484\\cdot 0.213909696\\approx -0.182061746.\n$$\nTherefore,\n$$\n\\nabla_{\\theta} J(\\theta)\\approx\n\\begin{pmatrix}\n-0.199826659 & 0.276031974 & -0.182061746\n\\end{pmatrix}.\n$$\n\nNext, compute the Monte Carlo (MC) estimate using the likelihood-ratio (REINFORCE) form without a baseline. For a Bernoulli policy with $\\pi_{\\theta}(1 \\mid s)=\\sigma(\\theta_{s})$ and $\\pi_{\\theta}(0 \\mid s)=1-\\sigma(\\theta_{s})$, the log-probability gradient for an action $a\\in\\{0,1\\}$ at state $s$ is\n$$\n\\nabla_{\\theta_{s}}\\ln \\pi_{\\theta}(a \\mid s)\n= \\frac{\\partial}{\\partial \\theta_{s}}\\Big(a\\ln \\sigma(\\theta_{s}) + (1-a)\\ln\\big(1-\\sigma(\\theta_{s})\\big)\\Big)\n= a - \\sigma(\\theta_{s}),\n$$\nand is $0$ with respect to $\\theta_{s'}$ for $s'\\neq s$.\n\nBecause the only nonzero reward occurs at $t=1$, the return-to-go at both time steps is the terminal reward, i.e., $G_{0}=G_{1}=r_{\\text{terminal}}$. For each episode, the per-episode gradient contribution is\n$$\ng^{(k)} = \\sum_{t=0}^{1} \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_{t}^{(k)} \\mid s_{t}^{(k)})\\,G_{t}^{(k)}\n= r^{(k)}\\cdot\n\\begin{pmatrix}\na_{0}^{(k)}-p_{0} \\\\\n\\mathbb{I}\\{s_{1}\\text{ visited}\\}\\cdot(a_{1}^{(k)}-p_{1}) \\\\\n\\mathbb{I}\\{s_{2}\\text{ visited}\\}\\cdot(a_{1}^{(k)}-p_{2})\n\\end{pmatrix}.\n$$\nWe use $p_{0}\\approx 0.574442516$, $p_{1}\\approx 0.401312339$, $p_{2}\\approx 0.689974481$.\n\nCompute each episode’s contribution:\n- Episode $1$ ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=0$, $r=0$):\n$$\ng^{(1)}=0\\cdot\n\\begin{pmatrix}\n1-p_{0} \\\\\n0-p_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n- Episode $2$ ($s_{0}\\to s_{2}$, $a_{0}=0$, $a_{1}=0$, $r=3$):\n$$\ng^{(2)}=3\\cdot\n\\begin{pmatrix}\n0-p_{0} \\\\\n0 \\\\\n0-p_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1.723327548 \\\\ 0 \\\\ -2.069923444\n\\end{pmatrix}.\n$$\n- Episode $3$ ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=1$, $r=2$):\n$$\ng^{(3)}=2\\cdot\n\\begin{pmatrix}\n1-p_{0} \\\\\n1-p_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.851114968 \\\\ 1.197375322 \\\\ 0\n\\end{pmatrix}.\n$$\n- Episode $4$ ($s_{0}\\to s_{2}$, $a_{0}=0$, $a_{1}=1$, $r=1$):\n$$\ng^{(4)}=1\\cdot\n\\begin{pmatrix}\n0-p_{0} \\\\\n0 \\\\\n1-p_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.574442516 \\\\ 0 \\\\ 0.310025519\n\\end{pmatrix}.\n$$\n- Episode $5$ ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=1$, $r=2$):\n$$\ng^{(5)}=\n\\begin{pmatrix}\n0.851114968 \\\\ 1.197375322 \\\\ 0\n\\end{pmatrix}\n\\quad\\text{(same as Episode $3$)}.\n$$\n\nAverage over $K=5$ to obtain the MC estimate:\n$$\n\\widehat{g}\n=\\frac{1}{5}\\sum_{k=1}^{5} g^{(k)}\n=\\frac{1}{5}\n\\begin{pmatrix}\n-0.595540128 \\\\\n2.394750644 \\\\\n-1.759897925\n\\end{pmatrix}\n\\approx\n\\begin{pmatrix}\n-0.119108026 & 0.478950129 & -0.351979585\n\\end{pmatrix}.\n$$\n\nCompute the Euclidean norm of the difference:\n$$\n\\Delta = \\widehat{g} - \\nabla_{\\theta} J(\\theta)\n\\approx\n\\begin{pmatrix}\n-0.119108026 - (-0.199826659) \\\\\n0.478950129 - 0.276031974 \\\\\n-0.351979585 - (-0.182061746)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.080718633 \\\\\n0.202918155 \\\\\n-0.169917839\n\\end{pmatrix}.\n$$\nThen\n$$\n\\|\\Delta\\|_{2}\n=\\sqrt{(0.080718633)^{2}+(0.202918155)^{2}+(-0.169917839)^{2}}\n\\approx \\sqrt{0.076567355}\\approx 0.276708.\n$$\n\nRounding to four significant figures yields $0.2767$, as required.", "answer": "$$\\boxed{0.2767}$$", "id": "2738661"}]}