{"hands_on_practices": [{"introduction": "The normal equations offer a direct analytical solution to the least-squares problem, but their implementation is fraught with numerical peril, especially with ill-conditioned data. This exercise guides you through deriving a more robust solution using QR factorization, a cornerstone of professional numerical software. By exploring the link between the matrix condition number and numerical error amplification, you will understand why this method is essential for reliable parameter estimation [@problem_id:2718842].", "problem": "Consider the overdetermined linear model in control-oriented system identification, where a measured output vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ is related to a regressor matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ and an unknown parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}$ by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, with $m \\geq n$ and $\\operatorname{rank}(\\mathbf{X}) = n$. The least-squares (LS) estimate $\\widehat{\\boldsymbol{\\theta}}$ minimizes the Euclidean norm of the residual, that is, $\\widehat{\\boldsymbol{\\theta}} \\in \\arg\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$. Assume a thin orthogonal-triangular factorization $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ with $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ satisfying $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$ and $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ upper triangular with positive diagonal entries.\n\nUsing only the definition of the least-squares criterion $\\min_{\\boldsymbol{\\theta}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$, the fact that multiplication by an orthogonal matrix preserves the Euclidean norm, and standard properties of upper triangular systems, derive an explicit expression for the unique minimizer $\\widehat{\\boldsymbol{\\theta}}$ in terms of $\\mathbf{Q}$, $\\mathbf{R}$, and $\\mathbf{y}$.\n\nThen, justify from first principles of numerical linear algebra and finite-precision arithmetic why solving the least-squares problem via the $\\mathbf{Q}\\mathbf{R}$ factorization is numerically more stable than forming and solving the normal equations $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. Your justification must be grounded in the Euclidean norm, conditioning, and the effect of squaring the condition number.\n\nFinally, compute the least-squares estimate for the specific data\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n0  1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\n2 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\nUse your $\\mathbf{Q}\\mathbf{R}$-based derivation to obtain $\\widehat{\\boldsymbol{\\theta}}$ without forming $\\mathbf{X}^{\\top}\\mathbf{X}$. Report the final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete solution. We shall proceed in three parts as requested.\n\nFirst, we derive the least-squares estimator $\\widehat{\\boldsymbol{\\theta}}$ using the provided thin orthogonal-triangular ($\\mathbf{Q}\\mathbf{R}$) factorization of the regressor matrix $\\mathbf{X}$. The objective is to find $\\widehat{\\boldsymbol{\\theta}}$ that minimizes the squared Euclidean norm of the residual vector $\\mathbf{r} = \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}$, which is given by the cost function $J(\\boldsymbol{\\theta}) = \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}$.\n\nWe substitute the thin $\\mathbf{Q}\\mathbf{R}$ factorization $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ into the cost function:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}\n$$\nHere, $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns, meaning $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$. To utilize the norm-preserving property of orthogonal matrices, we consider a full orthogonal matrix $\\mathbf{Q}_{\\text{full}} \\in \\mathbb{R}^{m \\times m}$, which can be constructed by appending $m-n$ orthonormal columns $\\mathbf{Q}_{\\perp}$ to $\\mathbf{Q}$, such that $\\mathbf{Q}_{\\text{full}} = \\begin{pmatrix} \\mathbf{Q}  \\mathbf{Q}_{\\perp} \\end{pmatrix}$. Since $\\mathbf{Q}_{\\text{full}}$ is an orthogonal matrix, $\\mathbf{Q}_{\\text{full}}^{\\top}\\mathbf{Q}_{\\text{full}} = \\mathbf{Q}_{\\text{full}}\\mathbf{Q}_{\\text{full}}^{\\top} = \\mathbf{I}_{m}$.\n\nMultiplication of a vector by an orthogonal matrix preserves its Euclidean norm. Thus, we have:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y})\\|_{2}^{2}\n$$\nWe expand the expression inside the norm using the partitioned form of $\\mathbf{Q}_{\\text{full}}^{\\top} = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}$:\n$$\n\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nUsing the property $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$, this simplifies to:\n$$\n\\begin{pmatrix} \\mathbf{I}_{n}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nThe squared norm of this partitioned vector is the sum of the squared norms of its components:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} + \\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}\n$$\nTo minimize $J(\\boldsymbol{\\theta})$, we must choose $\\boldsymbol{\\theta}$ to minimize this sum. The second term, $\\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}$, is the squared norm of the residual component orthogonal to the column space of $\\mathbf{X}$ and is independent of $\\boldsymbol{\\theta}$. The minimization is therefore achieved by minimizing the first term. The minimum possible value for a squared norm is $0$. This minimum is attainable, so we set the first term to zero:\n$$\n\\|\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} = 0 \\implies \\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\nThis yields the upper triangular system of linear equations:\n$$\n\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}\n$$\nThe problem states that $\\operatorname{rank}(\\mathbf{X}) = n$ and that $\\mathbf{R}$ has positive diagonal entries. This ensures that $\\mathbf{R}$ is invertible. The unique least-squares estimate $\\widehat{\\boldsymbol{\\theta}}$ is therefore given by:\n$$\n\\widehat{\\boldsymbol{\\theta}} = \\mathbf{R}^{-1}\\mathbf{Q}^{\\top}\\mathbf{y}\n$$\nThis system is efficiently solved by back substitution without explicitly computing $\\mathbf{R}^{-1}$.\n\nSecond, we justify the superior numerical stability of the $\\mathbf{Q}\\mathbf{R}$ method over solving the normal equations $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. The stability of solving a linear system is related to the condition number of the system matrix. The condition number of a matrix $\\mathbf{A}$ with respect to the Euclidean norm is $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$. For a full-rank rectangular matrix $\\mathbf{X}$, the condition number of the least-squares problem is $\\kappa_2(\\mathbf{X}) = \\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $\\mathbf{X}$.\n\nThe normal equations method involves forming the matrix $\\mathbf{A} = \\mathbf{X}^{\\top}\\mathbf{X}$ and then solving the $n \\times n$ system $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. The condition number of this system is $\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$. The eigenvalues of the symmetric positive definite matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ are the squares of the singular values of $\\mathbf{X}$. Thus, $\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\max}(\\mathbf{X})^2$ and $\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\min}(\\mathbf{X})^2$. The condition number of $\\mathbf{X}^{\\top}\\mathbf{X}$ is:\n$$\n\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X}) = \\frac{\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X})}{\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X})} = \\frac{\\sigma_{\\max}(\\mathbf{X})^2}{\\sigma_{\\min}(\\mathbf{X})^2} = \\left(\\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}\\right)^2 = (\\kappa_2(\\mathbf{X}))^2\n$$\nThis shows that forming the normal equations squares the condition number of the original problem. If $\\mathbf{X}$ is ill-conditioned (i.e., $\\kappa_2(\\mathbf{X})$ is large), $\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$ will be much larger, severely amplifying the effects of finite-precision arithmetic and round-off errors. Information can be lost in the very formation of $\\mathbf{X}^{\\top}\\mathbf{X}$ before solving the system.\n\nIn contrast, the $\\mathbf{Q}\\mathbf{R}$ method solves the system $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}$. The condition number of this system is $\\kappa_2(\\mathbf{R})$. Since $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ and $\\mathbf{Q}$ has orthonormal columns, $\\mathbf{Q}$ acts as an isometry. It can be shown that $\\kappa_2(\\mathbf{X}) = \\kappa_2(\\mathbf{Q}\\mathbf{R}) = \\kappa_2(\\mathbf{R})$. Therefore, the $\\mathbf{Q}\\mathbf{R}$ method solves a system with a condition number of $\\kappa_2(\\mathbf{X})$, not its square. By avoiding the formation of $\\mathbf{X}^{\\top}\\mathbf{X}$, the $\\mathbf{Q}\\mathbf{R}$ method bypasses the numerical instability associated with squaring the condition number, making it a more robust and accurate method for solving least-squares problems, especially for ill-conditioned matrices $\\mathbf{X}$.\n\nFinally, we compute the least-squares estimate for the given data:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nWe first compute the thin $\\mathbf{Q}\\mathbf{R}$ factorization of $\\mathbf{X}$ using the Gram-Schmidt process on its columns, $\\mathbf{x}_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^\\top$ and $\\mathbf{x}_2 = \\begin{pmatrix} 1  -1  1 \\end{pmatrix}^\\top$.\nFor the first column:\n$r_{11} = \\|\\mathbf{x}_1\\|_2 = \\sqrt{1^2+1^2+0^2} = \\sqrt{2}$.\n$\\mathbf{q}_1 = \\frac{\\mathbf{x}_1}{r_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor the second column:\n$r_{12} = \\mathbf{q}_1^\\top \\mathbf{x}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1-1) = 0$.\n$\\mathbf{u}_2 = \\mathbf{x}_2 - r_{12}\\mathbf{q}_1 = \\mathbf{x}_2 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n$r_{22} = \\|\\mathbf{u}_2\\|_2 = \\sqrt{1^2+(-1)^2+1^2} = \\sqrt{3}$.\n$\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{r_{22}} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n\nThus, we have:\n$$\n\\mathbf{Q} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{3}} \\\\ 0  \\frac{1}{\\sqrt{3}} \\end{pmatrix}, \\quad\n\\mathbf{R} = \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix}\n$$\nNext, we compute the vector $\\mathbf{Q}^\\top\\mathbf{y}$:\n$$\n\\mathbf{Q}^\\top\\mathbf{y} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{3}}  -\\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{\\sqrt{2}} \\\\ \\frac{2}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\frac{3}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\nNow we solve the system $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^\\top\\mathbf{y}$:\n$$\n\\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\widehat{\\theta}_1 \\\\ \\widehat{\\theta}_2 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\nThis diagonal system yields the solution directly:\n$\\sqrt{2} \\cdot \\widehat{\\theta}_1 = \\sqrt{2} \\implies \\widehat{\\theta}_1 = 1$.\n$\\sqrt{3} \\cdot \\widehat{\\theta}_2 = \\sqrt{3} \\implies \\widehat{\\theta}_2 = 1$.\nThe least-squares estimate is $\\widehat{\\boldsymbol{\\theta}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1\n\\end{pmatrix}\n}\n$$", "id": "2718842"}, {"introduction": "While batch least-squares and Recursive Least Squares (RLS) appear different in formulation, they are fundamentally two sides of the same coin when using a forgetting factor of $\\lambda=1$. This hands-on coding practice solidifies this equivalence by having you implement both methods and verify that they yield numerically identical parameter estimates. Tackling various scenarios, including rank-deficient and ill-conditioned data, will demonstrate the robustness of this fundamental relationship in estimation theory [@problem_id:2718829].", "problem": "Consider a linear regression measurement model with a deterministic regressor sequence and a Gaussian prior on the unknown parameter vector. Let the parameter be a vector $\\theta \\in \\mathbb{R}^n$, and for each sample index $k \\in \\{1,\\dots,N\\}$, the scalar measurement $y_k \\in \\mathbb{R}$ is related to the regressor vector $\\varphi_k \\in \\mathbb{R}^n$ by the linear model $y_k = \\varphi_k^\\top \\theta + v_k$, where $v_k$ is a zero-mean noise with known variance $R_k  0$. Assume a Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$ with known mean $\\theta_0 \\in \\mathbb{R}^n$ and symmetric positive definite covariance $P_0 \\in \\mathbb{R}^{n \\times n}$. The fundamental basis for this problem is the definition of the weighted least-squares criterion with a quadratic prior, together with the linear Gaussian modeling assumption. The batch estimator is defined as the minimizer of the strictly convex quadratic objective\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\nA Recursive Least Squares (RLS) estimator processes the same data sequentially in the given order, starting from the same prior $(\\theta_0, P_0)$ and using the same measurement variances $\\{R_k\\}$, and produces an updated estimate $\\hat{\\theta}_N$ after processing all $N$ samples.\n\nTask: Write a complete, runnable program that, for each test case listed below, computes:\n- the batch estimator $\\hat{\\theta}_{\\text{batch}}$ as the unique minimizer of $J(\\theta)$, and\n- the final RLS estimator $\\hat{\\theta}_{N}$ after processing all samples in the given order, initialized with the same prior $(\\theta_0, P_0)$.\n\nThen, for each test case, compute the Euclidean norm $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$, compare it against the tolerance $\\tau = 10^{-7}$, and return a boolean result that is $true$ if and only if the norm is less than or equal to $\\tau$, and $false$ otherwise.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,true,false]\"). The list must contain one boolean per test case, in the same order as presented below.\n\nTest suite (each case specifies $n$, $N$, the regressor matrix $\\Phi \\in \\mathbb{R}^{N \\times n}$ whose $k$-th row is $\\varphi_k^\\top$, the measurement vector $y \\in \\mathbb{R}^N$, the measurement variances $R \\in \\mathbb{R}^N$ with entries $R_k$, the prior mean $\\theta_0 \\in \\mathbb{R}^n$, and the prior covariance $P_0 \\in \\mathbb{R}^{n \\times n}$):\n\n- Case $1$ (happy path, full rank, heterogeneous variances):\n  - $n = 3$, $N = 6$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0  0.5  -0.3 \\\\\n  0.2  -1.0  0.7 \\\\\n  1.5  0.0  0.5 \\\\\n  -0.3  0.8  1.2 \\\\\n  0.0  -0.4  2.0 \\\\\n  1.0  1.0  1.0\n  \\end{bmatrix}$.\n  - $y = [0.7, -1.2, 1.5, 0.3, 0.0, 2.0]$.\n  - $R = [0.5, 1.2, 0.8, 1.0, 0.3, 2.0]$.\n  - $\\theta_0 = [0.1, -0.2, 0.3]$.\n  - $P_0 = \\mathrm{diag}([1.0, 2.0, 0.5])$.\n\n- Case $2$ (rank-deficient design, regularization from prior ensures uniqueness):\n  - $n = 4$, $N = 5$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0  0.0  0.0  0.0 \\\\\n  0.0  1.0  0.0  0.0 \\\\\n  1.0  0.0  0.0  0.0 \\\\\n  0.0  1.0  0.0  0.0 \\\\\n  1.0  1.0  0.0  0.0\n  \\end{bmatrix}$.\n  - $y = [1.0, 2.0, 1.5, 1.8, 3.1]$.\n  - $R = [0.1, 0.2, 0.3, 0.4, 0.5]$.\n  - $\\theta_0 = [0.0, 0.0, 0.0, 0.0]$.\n  - $P_0 = \\mathrm{diag}([10.0, 10.0, 1.0, 1.0])$.\n\n- Case $3$ (single-sample boundary case):\n  - $n = 2$, $N = 1$.\n  - $\\Phi = \\begin{bmatrix} 2.0  -1.0 \\end{bmatrix}$.\n  - $y = [0.5]$.\n  - $R = [0.05]$.\n  - $\\theta_0 = [1.0, -1.0]$.\n  - $P_0 = \\begin{bmatrix} 0.2  0.05 \\\\ 0.05  0.5 \\end{bmatrix}$.\n\n- Case $4$ (ill-conditioned scaling, heterogeneous prior variances):\n  - $n = 3$, $N = 4$.\n  - $\\Phi = \\begin{bmatrix}\n  10^{6}  10^{-6}  1.0 \\\\\n  10^{6}  -10^{-6}  -1.0 \\\\\n  2\\cdot 10^{6}  0.0  0.5 \\\\\n  -10^{6}  10^{-6}  -0.5\n  \\end{bmatrix}$.\n  - $y = [1.0, -1.0, 0.5, -0.2]$.\n  - $R = [1.0, 2.0, 1.5, 0.7]$.\n  - $\\theta_0 = [0.0, 0.0, 0.0]$.\n  - $P_0 = \\mathrm{diag}([10^{-12}, 10^{12}, 1.0])$.\n\n- Case $5$ (unequal measurement variances emphasizing weighting behavior):\n  - $n = 3$, $N = 3$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0  2.0  3.0 \\\\\n  4.0  5.0  6.0 \\\\\n  7.0  8.0  10.0\n  \\end{bmatrix}$.\n  - $y = [14.0, 32.0, 50.0]$.\n  - $R = [0.5, 100.0, 0.1]$.\n  - $\\theta_0 = [0.0, 0.0, 0.0]$.\n  - $P_0 = I_3$ (the $3 \\times 3$ identity matrix).\n\nAngle units are not applicable. No physical units are involved. The tolerance is $\\tau = 10^{-7}$, and the Euclidean norm must be compared to this tolerance. The final output must be a single line containing a list of boolean values corresponding to the five cases, in order.", "solution": "The problem statement is critically examined for validity before any solution is attempted.\n\n**Step 1: Extract Givens**\n\nThe measurement model is given by $y_k = \\varphi_k^\\top \\theta + v_k$ for $k \\in \\{1, \\dots, N\\}$, where $\\theta \\in \\mathbb{R}^n$ is the unknown parameter vector, $y_k \\in \\mathbb{R}$ is the scalar measurement, $\\varphi_k \\in \\mathbb{R}^n$ is the regressor vector, and $v_k$ is a zero-mean noise with known variance $R_k  0$.\nThe prior on the parameter is a Gaussian distribution, $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$, with known mean $\\theta_0 \\in \\mathbb{R}^n$ and known symmetric positive definite covariance $P_0 \\in \\mathbb{R}^{n \\times n}$.\nThe batch estimator $\\hat{\\theta}_{\\text{batch}}$ is defined as the unique minimizer of the objective function:\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\nThe Recursive Least Squares (RLS) estimator $\\hat{\\theta}_N$ is defined as the result of sequential processing of the measurements $\\{y_k\\}_{k=1}^N$, starting from the same prior $(\\theta_0, P_0)$.\nThe task requires a comparison of the final estimators from both methods, $\\hat{\\theta}_{\\text{batch}}$ and $\\hat{\\theta}_N$, by computing the Euclidean norm of their difference, $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$, and checking if this norm is less than or equal to a tolerance $\\tau = 10^{-7}$.\nFive test cases are provided, each specifying the parameters $n$, $N$, $\\Phi$, $y$, $R$, $\\theta_0$, and $P_0$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is analyzed against the specified validation criteria.\n\n1.  **Scientifically Grounded**: The problem is a standard exercise in Bayesian estimation for a linear-Gaussian system. The concepts of weighted least-squares with a quadratic prior (which corresponds to a Gaussian prior), batch estimation, and recursive estimation (specifically, the Kalman filter for a static parameter) are fundamental and well-established principles in statistical signal processing and control theory. The problem is scientifically sound.\n\n2.  **Well-Posed**: The objective function $J(\\theta)$ is a sum of quadratic terms. The prior term, $(\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0)$, is a strictly convex function of $\\theta$ because $P_0$ is given as symmetric positive definite, which implies its inverse $P_0^{-1}$ is also symmetric positive definite. The measurement terms, $\\sum_{k=1}^N \\frac{(y_k - \\varphi_k^\\top \\theta)^2}{R_k}$, are convex (positive semi-definite quadratic forms). The sum of a strictly convex function and one or more convex functions is strictly convex. A strictly convex function has a unique minimum. Therefore, a unique solution $\\hat{\\theta}_{\\text{batch}}$ that minimizes $J(\\theta)$ always exists. This holds even in cases where the information from the measurements alone is rank-deficient (as in Case 2), as the prior term serves to regularize the problem. The problem is well-posed.\n\n3.  **Objective**: The problem is formulated in precise mathematical language. All variables and objectives are defined unambiguously. It is free from subjective or opinion-based statements.\n\n4.  **Completeness**: All necessary data ($n$, $N$, $\\Phi$, $y$, $R$, $\\theta_0$, $P_0$) are provided for each of the five test cases. The problem is self-contained.\n\n5.  **Conclusion**: The problem is valid. It is a well-posed, scientifically sound, and objective task from the field of estimation theory. It is a verification of the fundamental algebraic equivalence between batch and recursive Bayesian least-squares estimation.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be provided.\n\nThe core of the problem is the demonstration that the batch and recursive estimators for a linear-Gaussian model yield identical results. We will first derive the closed-form expression for the batch estimator and then the iterative equations for the recursive estimator.\n\n**Batch Estimator Derivation**\n\nThe batch estimator $\\hat{\\theta}_{\\text{batch}}$ minimizes the cost function $J(\\theta)$. To find the minimum, we compute the gradient of $J(\\theta)$ with respect to $\\theta$ and set it to zero.\n$$\n\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left( (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} (y_k - \\varphi_k^\\top \\theta)^2 \\right) = 0\n$$\nUsing standard rules for vector calculus, the gradient is:\n$$\n\\nabla_\\theta J(\\theta) = 2 P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} \\cdot 2(y_k - \\varphi_k^\\top \\theta)(-\\varphi_k) = 0\n$$\nDividing by $2$ and rearranging the terms:\n$$\nP_0^{-1} \\theta - P_0^{-1} \\theta_0 - \\sum_{k=1}^N R_k^{-1} \\varphi_k y_k + \\sum_{k=1}^N R_k^{-1} \\varphi_k \\varphi_k^\\top \\theta = 0\n$$\nGrouping the terms containing $\\theta$:\n$$\n\\left( P_0^{-1} + \\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} \\right) \\theta = P_0^{-1} \\theta_0 + \\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k}\n$$\nLet the regressor matrix be $\\Phi \\in \\mathbb{R}^{N \\times n}$ with rows $\\varphi_k^\\top$, the measurement vector be $y \\in \\mathbb{R}^N$, and the weighting matrix be $W = \\mathrm{diag}(R_1^{-1}, \\dots, R_N^{-1})$. The sums can be expressed in matrix form: $\\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} = \\Phi^\\top W \\Phi$ and $\\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k} = \\Phi^\\top W y$.\nThe equation becomes:\n$$\n\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right) \\hat{\\theta}_{\\text{batch}} = P_0^{-1} \\theta_0 + \\Phi^\\top W y\n$$\nThe solution for the batch estimator is therefore:\n$$\n\\hat{\\theta}_{\\text{batch}} = \\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)^{-1} \\left( P_0^{-1} \\theta_0 + \\Phi^\\top W y \\right)\n$$\nThe matrix $\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)$ is the inverse of the posterior covariance, $P_N^{-1}$, and is guaranteed to be invertible.\n\n**Recursive Least Squares (RLS) Estimator Derivation**\n\nThe RLS estimator updates the estimate of $\\theta$ sequentially as each measurement $y_k$ becomes available. This is a direct application of the Kalman filter equations for a static parameter system ($\\theta_{k} = \\theta_{k-1}$). The estimate at step $k$, $\\hat{\\theta}_k$, and its associated covariance, $P_k$, are computed based on the estimate at step $k-1$ and the new measurement $y_k$.\n\nThe process starts with the prior information:\n- Initial estimate: $\\hat{\\theta}_0 = \\theta_0$\n- Initial covariance: $P_0 = P_0$\n\nFor each measurement $k = 1, 2, \\dots, N$, the following update steps are performed:\n1.  **Innovation (Prediction Error)**: The difference between the actual measurement and the predicted measurement.\n    $$e_k = y_k - \\varphi_k^\\top \\hat{\\theta}_{k-1}$$\n2.  **Innovation Covariance**: The variance of the innovation.\n    $$S_k = \\varphi_k^\\top P_{k-1} \\varphi_k + R_k$$\n3.  **Kalman Gain**: The weight given to the innovation. It balances the uncertainty of the current estimate and the new measurement.\n    $$K_k = P_{k-1} \\varphi_k S_k^{-1}$$\n4.  **State (Parameter) Update**: The new estimate is the old estimate plus a correction proportional to the innovation.\n    $$\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$$\n5.  **Covariance Update**: The uncertainty of the estimate is reduced. A numerically stable form is used.\n    $$P_k = (I - K_k \\varphi_k^\\top) P_{k-1}$$\n\nAfter iterating through all $N$ measurements, the final RLS estimate is $\\hat{\\theta}_N$ and the final covariance is $P_N$. It is a fundamental result in estimation theory that, under ideal arithmetic, $\\hat{\\theta}_N$ is algebraically identical to $\\hat{\\theta}_{\\text{batch}}$. The numerical implementation will verify this equivalence within a specified floating-point tolerance.\n\n**Numerical Implementation Plan**\n\nA program will be written to perform the following for each test case:\n1.  Compute $\\hat{\\theta}_{\\text{batch}}$ using the derived matrix formula. A linear system solver will be used for numerical stability instead of explicit matrix inversion: solving $A x = b$ for $x$ is preferred over computing $A^{-1}b$.\n2.  Compute $\\hat{\\theta}_N$ by implementing the RLS loop, starting with $(\\theta_0, P_0)$ and iterating through the $N$ measurements. To maintain numerical stability and correctness, the covariance matrix $P_k$ can be explicitly symmetrized at each step to counteract potential floating-point inaccuracies.\n3.  Calculate the Euclidean norm of the difference: $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$.\n4.  Compare this norm against the tolerance $\\tau = 10^{-7}$ and generate the boolean result.\n\nThe final output will be a list of these boolean results, one for each test case, formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Tolerance for comparison of the Euclidean norm.\n    tau = 1e-7\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, full rank, heterogeneous variances)\n        {\n            \"n\": 3, \"N\": 6,\n            \"Phi\": np.array([\n                [1.0, 0.5, -0.3], [0.2, -1.0, 0.7], [1.5, 0.0, 0.5],\n                [-0.3, 0.8, 1.2], [0.0, -0.4, 2.0], [1.0, 1.0, 1.0]\n            ]),\n            \"y\": np.array([0.7, -1.2, 1.5, 0.3, 0.0, 2.0]),\n            \"R\": np.array([0.5, 1.2, 0.8, 1.0, 0.3, 2.0]),\n            \"theta0\": np.array([0.1, -0.2, 0.3]),\n            \"P0\": np.diag([1.0, 2.0, 0.5]),\n        },\n        # Case 2 (rank-deficient design, regularization from prior ensures uniqueness)\n        {\n            \"n\": 4, \"N\": 5,\n            \"Phi\": np.array([\n                [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]\n            ]),\n            \"y\": np.array([1.0, 2.0, 1.5, 1.8, 3.1]),\n            \"R\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]),\n            \"theta0\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"P0\": np.diag([10.0, 10.0, 1.0, 1.0]),\n        },\n        # Case 3 (single-sample boundary case)\n        {\n            \"n\": 2, \"N\": 1,\n            \"Phi\": np.array([[2.0, -1.0]]),\n            \"y\": np.array([0.5]),\n            \"R\": np.array([0.05]),\n            \"theta0\": np.array([1.0, -1.0]),\n            \"P0\": np.array([[0.2, 0.05], [0.05, 0.5]]),\n        },\n        # Case 4 (ill-conditioned scaling, heterogeneous prior variances)\n        {\n            \"n\": 3, \"N\": 4,\n            \"Phi\": np.array([\n                [1e6, 1e-6, 1.0], [1e6, -1e-6, -1.0],\n                [2e6, 0.0, 0.5], [-1e6, 1e-6, -0.5]\n            ]),\n            \"y\": np.array([1.0, -1.0, 0.5, -0.2]),\n            \"R\": np.array([1.0, 2.0, 1.5, 0.7]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.diag([1e-12, 1e12, 1.0]),\n        },\n        # Case 5 (unequal measurement variances emphasizing weighting behavior)\n        {\n            \"n\": 3, \"N\": 3,\n            \"Phi\": np.array([\n                [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 10.0]\n            ]),\n            \"y\": np.array([14.0, 32.0, 50.0]),\n            \"R\": np.array([0.5, 100.0, 0.1]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.eye(3),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters\n        n, N = case[\"n\"], case[\"N\"]\n        Phi, y = case[\"Phi\"], case[\"y\"]\n        R, theta0, P0 = case[\"R\"], case[\"theta0\"], case[\"P0\"]\n\n        # 1. Batch Estimator\n        # theta_batch = (P0_inv + Phi^T W Phi)^-1 (P0_inv theta0 + Phi^T W y)\n        # where W = diag(1/R_k)\n        P0_inv = np.linalg.inv(P0)\n        W = np.diag(1.0 / R)\n        \n        # Information matrix: I = P0_inv + Phi^T W Phi\n        info_matrix = P0_inv + Phi.T @ W @ Phi\n        # Information vector: i = P0_inv theta0 + Phi^T W y\n        info_vector = P0_inv @ theta0 + Phi.T @ W @ y\n        \n        # Solve I * theta = i for theta. Numerically more stable than inv(I) @ i.\n        theta_batch = np.linalg.solve(info_matrix, info_vector)\n\n        # 2. Recursive Least Squares (RLS) Estimator\n        theta_rls = theta0.copy().astype(np.float64)\n        P_rls = P0.copy().astype(np.float64)\n        \n        for k in range(N):\n            # Get k-th measurement and regressor\n            phi_k = Phi[k, :].reshape(n, 1)\n            y_k = y[k]\n            R_k = R[k]\n            \n            # Innovation\n            e_k = y_k - phi_k.T @ theta_rls\n            \n            # Innovation covariance\n            S_k = phi_k.T @ P_rls @ phi_k + R_k\n            \n            # Kalman Gain\n            K_k = (P_rls @ phi_k) / S_k\n            \n            # State update\n            theta_rls = theta_rls + (K_k * e_k).flatten()\n            \n            # Covariance update (Joseph form is most stable, but this is simpler and sufficient)\n            P_rls = (np.eye(n) - K_k @ phi_k.T) @ P_rls\n            # Enforce symmetry to prevent numerical drift\n            P_rls = 0.5 * (P_rls + P_rls.T)\n\n        # 3. Comparison\n        # Calculate the Euclidean norm of the difference vector\n        norm_diff = np.linalg.norm(theta_batch - theta_rls)\n        \n        # Compare against the tolerance\n        is_equivalent = norm_diff = tau\n        results.append(str(is_equivalent).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2718829"}, {"introduction": "A naive application of least-squares to data from a closed-loop system often leads to biased and inconsistent estimates, a critical pitfall arising from endogeneity. This practice challenges you to first prove this inconsistency theoretically and then demonstrate it through simulation, revealing how feedback corrupts the estimation process. You will then implement the Instrumental Variables (IV) method to see firsthand how a carefully chosen instrument can break the problematic correlation and restore estimator consistency, a vital technique for accurate closed-loop identification [@problem_id:2718799].", "problem": "Consider a closed-loop control setting with a static plant and instantaneous feedback. The scalar plant is given by the equation $y_t = b\\,u_t + e_t$, where $y_t$ is the plant output, $u_t$ is the control input, $b$ is the true plant gain to be identified, and $e_t$ is a zero-mean disturbance. The controller uses the current plant output in a static feedback law $u_t = g\\,r_t - k\\,y_t$, where $r_t$ is an exogenous reference signal, $g$ is a known reference gain, and $k$ is a known feedback gain. Assume $\\{r_t\\}$ and $\\{e_t\\}$ are mutually independent sequences of independent and identically distributed Gaussian random variables, $r_t \\sim \\mathcal{N}(0,\\sigma_r^2)$ and $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$, with $\\sigma_r^2  0$ and $\\sigma_e^2  0$. All quantities are real-valued. The goal is to estimate $b$ from a batch of $N$ samples $\\{(u_t,y_t)\\}_{t=1}^N$ using both Ordinary Least Squares (OLS) and an Instrumental Variables (IV) estimator with instrument $z_t = r_t$, and to compare these with a Recursive Least Squares (RLS) estimate.\n\nYour tasks are as follows.\n\n1) Using only fundamental definitions (linearity of expectation, the Law of Large Numbers, and the definitions of Ordinary Least Squares (OLS), Instrumental Variables (IV), and Recursive Least Squares (RLS)), derive the probability limit of the OLS estimator that regresses $y_t$ on $u_t$ under the specified feedback, and determine whether it equals $b$ when $k \\neq 0$. Do not use any pre-quoted final results; start from $y_t = b\\,u_t + e_t$ and $u_t = g\\,r_t - k\\,y_t$ and reason about endogeneity induced by feedback. Conclude under what condition on $k$ OLS is consistent for $b$.\n\n2) Argue, again from first principles, that the IV estimator using $z_t = r_t$ as an instrument is consistent for $b$ under the stated assumptions. Explicitly identify the relevance and exogeneity conditions in terms of the joint statistics of $r_t$, $u_t$, and $e_t$.\n\n3) Implement the following estimators on simulated data:\n- Batch OLS estimate $\\hat{b}_{\\mathrm{OLS}}$ from $\\{(u_t,y_t)\\}$.\n- Batch IV estimate $\\hat{b}_{\\mathrm{IV}}$ using instrument $z_t = r_t$ from $\\{(z_t,u_t,y_t)\\}$.\n- Recursive Least Squares (RLS) estimate $\\hat{b}_{\\mathrm{RLS}}$ updated sequentially over the same data with forgetting factor $\\lambda = 1$, scalar covariance initialization $P_0 = 10^6$, and initial parameter $\\hat{b}_0 = 0$.\n\n4) For each test case below, compute the following truth-valued (boolean) assertions using explicit numerical tolerances:\n- Define a tolerance for truth as $\\varepsilon_{\\text{true}} = 10^{-2}$, for the OLS probability limit as $\\varepsilon_{\\text{lim}} = 10^{-2}$, and for RLS agreement as $\\varepsilon_{\\text{rls}} = 5 \\times 10^{-3}$.\n- Let $b_{\\infty,\\mathrm{OLS}}$ denote the theoretical OLS probability limit you derived in Task 1, expressed in terms of $(b,k,g,\\sigma_r,\\sigma_e)$.\n- For each test case, report three booleans in this order:\n    a) $\\text{OLS\\_inconsistent}$: this is true if $|\\hat{b}_{\\mathrm{OLS}} - b|  \\varepsilon_{\\text{true}}$ and $|\\hat{b}_{\\mathrm{OLS}} - b_{\\infty,\\mathrm{OLS}}| \\le \\varepsilon_{\\text{lim}}$.\n    b) $\\text{IV\\_eliminates\\_bias}$: this is true if $|\\hat{b}_{\\mathrm{IV}} - b| \\le \\varepsilon_{\\text{true}}$.\n    c) $\\text{RLS\\_matches\\_OLS}$: this is true if $|\\hat{b}_{\\mathrm{RLS}} - \\hat{b}_{\\mathrm{OLS}}| \\le \\varepsilon_{\\text{rls}}$.\n\nSimulation details:\n- Use a fixed pseudorandom seed equal to $12345$ for reproducibility.\n- For each test case, generate $N$ independent samples of $r_t$ and $e_t$ and then form $u_t$ and $y_t$ according to the equations above.\n- You may assume that $1 + k\\,b \\neq 0$ for the test cases below.\n\nTest suite:\n- Case A (feedback present): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.5,1.0,1.0,0.7,200000)$.\n- Case B (no feedback): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.0,1.0,1.0,0.7,200000)$.\n- Case C (stronger feedback): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,1.5,1.0,1.0,0.7,200000)$.\n\nFinal output format:\n- Your program should produce a single line of output containing all results for the three cases concatenated in order A, B, C, each case contributing the three booleans $(\\text{OLS\\_inconsistent}, \\text{IV\\_eliminates\\_bias}, \\text{RLS\\_matches\\_OLS})$, as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\ldots]$.\n- No other output should be produced.", "solution": "The problem presented requires a rigorous analysis of parameter estimation for a linear static plant operating within a closed-loop feedback system. We must first validate the problem's integrity. The problem is a canonical exercise in system identification, grounded in established principles of control theory and statistical estimation. It is well-posed, objective, and contains all necessary information. Its parameters are scientifically plausible, and it poses a non-trivial question regarding estimator consistency in the presence of feedback. The problem is therefore deemed valid and a formal solution will be provided.\n\nThe system is described by the following set of equations:\nPlant model: $y_t = b u_t + e_t$\nFeedback controller: $u_t = g r_t - k y_t$\n\nHere, $y_t$ is the output, $u_t$ is the control input, $b$ is the unknown parameter to estimate, $e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$ is a stochastic disturbance, $r_t \\sim \\mathcal{N}(0, \\sigma_r^2)$ is an exogenous reference signal, and $g$ and $k$ are known controller gains. The sequences $\\{r_t\\}$ and $\\{e_t\\}$ are independent. We assume $1+kb \\neq 0$ to ensure the closed-loop system is well-defined.\n\nFirst, we must express the endogenous variables $u_t$ and $y_t$ solely in terms of the exogenous inputs $r_t$ and $e_t$. Substituting the plant equation into the controller equation yields:\n$u_t = g r_t - k (b u_t + e_t) \\implies u_t(1+kb) = g r_t - k e_t \\implies u_t = \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t$\nSubstituting this back into the plant equation yields:\n$y_t = b \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) + e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\left(1 - \\frac{bk}{1+kb}\\right) e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\frac{1}{1+kb} e_t$\nThese are the reduced-form equations for the system.\n\n**1. Analysis of the Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator for the parameter $b$ in the regression of $y_t$ on $u_t$ is given by:\n$$ \\hat{b}_{\\mathrm{OLS}} = \\left( \\sum_{t=1}^N u_t^2 \\right)^{-1} \\left( \\sum_{t=1}^N u_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N u_t y_t}{\\frac{1}{N}\\sum_{t=1}^N u_t^2} $$\nBy the Law of Large Numbers, the probability limit (plim) of this estimator as $N \\to \\infty$ is the ratio of expectations:\n$$ b_{\\infty,\\mathrm{OLS}} = \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{OLS}} = \\frac{\\mathrm{E}[u_t y_t]}{\\mathrm{E}[u_t^2]} $$\nThe fundamental condition for OLS consistency ($b_{\\infty,\\mathrm{OLS}} = b$) is that the regressor, $u_t$, must be uncorrelated with the error term, $e_t$. This is the property of exogeneity. Let us examine this condition. We compute the covariance $\\mathrm{E}[u_t e_t]$:\n$$ \\mathrm{E}[u_t e_t] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) e_t \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t e_t] - \\frac{k}{1+kb}\\mathrm{E}[e_t^2] $$\nGiven that $r_t$ and $e_t$ are independent and zero-mean, $\\mathrm{E}[r_t e_t] = \\mathrm{E}[r_t]\\mathrm{E}[e_t] = 0$. With $\\mathrm{E}[e_t^2] = \\sigma_e^2$, we find:\n$$ \\mathrm{E}[u_t e_t] = -\\frac{k \\sigma_e^2}{1+kb} $$\nThis covariance is non-zero if and only if $k \\neq 0$ (as $\\sigma_e^2  0$ and $1+kb \\neq 0$). The feedback gain $k$ creates a direct algebraic link from the disturbance $e_t$ to the input $u_t$. This phenomenon, where the regressor is correlated with the error term, is known as endogeneity. It violates the core assumption of OLS and leads to a biased and inconsistent estimator.\n\nTo find the asymptotic value of the OLS estimator, we proceed to calculate the necessary moments.\n$$ \\mathrm{E}[u_t^2] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right)^2 \\right] = \\frac{1}{(1+kb)^2} (g^2 \\mathrm{E}[r_t^2] + k^2 \\mathrm{E}[e_t^2]) = \\frac{g^2 \\sigma_r^2 + k^2 \\sigma_e^2}{(1+kb)^2} $$\nThe numerator of the plim is $\\mathrm{E}[u_t y_t] = \\mathrm{E}[u_t(b u_t + e_t)] = b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]$.\nThus, the probability limit is:\n$$ b_{\\infty,\\mathrm{OLS}} = \\frac{b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} = b + \\frac{\\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} $$\nSubstituting our derived moments:\n$$ b_{\\infty,\\mathrm{OLS}} = b + \\frac{-k \\sigma_e^2 / (1+kb)}{(g^2 \\sigma_r^2 + k^2 \\sigma_e^2) / (1+kb)^2} = b - \\frac{k \\sigma_e^2 (1+kb)}{g^2 \\sigma_r^2 + k^2 \\sigma_e^2} $$\nThis expression confirms that the OLS estimator is inconsistent, converging to a value different from the true parameter $b$ whenever feedback is present ($k \\neq 0$). Consistency is achieved only in the open-loop case, where $k=0$.\n\n**2. Analysis of the Instrumental Variables (IV) Estimator**\n\nThe IV method is designed to overcome endogeneity. It requires an instrumental variable, $z_t$, that satisfies two conditions:\n1.  **Instrument Relevance**: The instrument must be correlated with the endogenous regressor $u_t$. Formally, $\\mathrm{E}[z_t u_t] \\neq 0$.\n2.  **Instrument Exogeneity**: The instrument must be uncorrelated with the model's error term $e_t$. Formally, $\\mathrm{E}[z_t e_t] = 0$.\n\nThe proposed instrument is the exogenous reference signal, $z_t=r_t$. Let us verify the two conditions.\nThe exogeneity condition is satisfied by definition: $r_t$ and $e_t$ are from mutually independent sequences, so $\\mathrm{E}[r_t e_t] = 0$.\nFor the relevance condition, we compute the covariance $\\mathrm{E}[r_t u_t]$:\n$$ \\mathrm{E}[r_t u_t] = \\mathrm{E}\\left[ r_t \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t^2] - \\frac{k}{1+kb}\\mathrm{E}[r_t e_t] = \\frac{g \\sigma_r^2}{1+kb} $$\nAssuming $g \\neq 0$ and $\\sigma_r^2  0$ (which are true for any meaningful control objective and are satisfied in the test cases), this covariance is non-zero. Thus, $r_t$ is a valid instrument.\n\nThe IV estimator is constructed as:\n$$ \\hat{b}_{\\mathrm{IV}} = \\left( \\sum_{t=1}^N z_t u_t \\right)^{-1} \\left( \\sum_{t=1}^N z_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N r_t y_t}{\\frac{1}{N}\\sum_{t=1}^N r_t u_t} $$\nIts probability limit is:\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{\\mathrm{E}[r_t y_t]}{\\mathrm{E}[r_t u_t]} $$\nThe numerator is $\\mathrm{E}[r_t y_t] = \\mathrm{E}[r_t(b u_t + e_t)] = b\\mathrm{E}[r_t u_t] + \\mathrm{E}[r_t e_t]$. Due to exogeneity, $\\mathrm{E}[r_t e_t]=0$. Therefore, $\\mathrm{E}[r_t y_t] = b\\mathrm{E}[r_t u_t]$.\nSubstituting this into the plim expression:\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{b\\mathrm{E}[r_t u_t]}{\\mathrm{E}[r_t u_t]} = b $$\nThe IV estimator is consistent for $b$, correctly accounting for the endogeneity induced by the feedback loop.\n\n**3. Recursive Least Squares (RLS) Estimator**\n\nThe RLS algorithm provides a sequential update for the least-squares estimate. When the forgetting factor $\\lambda$ is set to $1$, RLS gives exactly the same result as the batch OLS estimator over the same dataset, assuming a non-informative prior (i.e., initial covariance $P_0$ is large). Therefore, we expect $\\hat{b}_{\\mathrm{RLS}}$ to be numerically very close to $\\hat{b}_{\\mathrm{OLS}}$ and to suffer from the same asymptotic bias when $k \\neq 0$. The simulation will verify this equivalence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the system identification problem for OLS, IV, and RLS estimators.\n    \"\"\"\n    # Set the fixed pseudorandom seed for reproducibility.\n    np.random.seed(12345)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (b, k, g, sigma_r, sigma_e, N)\n        (0.8, 0.5, 1.0, 1.0, 0.7, 200000),  # Case A: feedback present\n        (0.8, 0.0, 1.0, 1.0, 0.7, 200000),  # Case B: no feedback\n        (0.8, 1.5, 1.0, 1.0, 0.7, 200000),  # Case C: stronger feedback\n    ]\n\n    # Define numerical tolerances for asserting truth values.\n    eps_true = 1e-2    # Tolerance for proximity to the true parameter b\n    eps_lim = 1e-2     # Tolerance for proximity to the OLS theoretical limit\n    eps_rls = 5e-3     # Tolerance for agreement between RLS and OLS estimates\n\n    all_results = []\n\n    for case in test_cases:\n        b, k, g, sigma_r, sigma_e, N = case\n\n        # Generate exogenous signals r_t and e_t.\n        r = np.random.normal(0, sigma_r, N)\n        e = np.random.normal(0, sigma_e, N)\n\n        # Compute endogenous signals u_t and y_t using the reduced-form equations.\n        # This is more efficient than a step-by-step simulation loop.\n        denom = 1.0 + k * b\n        if denom == 0:\n            # This case is excluded by the problem statement.\n            # Handle defensively to avoid division by zero.\n            u = np.full(N, np.nan)\n            y = np.full(N, np.nan)\n        else:\n            u = (g / denom) * r - (k / denom) * e\n            y = (b * g / denom) * r + (1.0 / denom) * e\n\n        # Task 3.1: Batch Ordinary Least Squares (OLS) estimate.\n        # This is the standard formula for scalar regression.\n        b_ols = np.dot(u, y) / np.dot(u, u)\n\n        # Task 3.2: Batch Instrumental Variables (IV) estimate using z_t = r_t.\n        # The instrument r_t replaces the regressor u_t in the \"X'X\" part of the projection.\n        b_iv = np.dot(r, y) / np.dot(r, u)\n\n        # Task 3.3: Recursive Least Squares (RLS) estimate.\n        # Initialize parameter and covariance matrix (scalar in this case).\n        b_rls = 0.0\n        P = 1e6\n        lambda_rls = 1.0  # Forgetting factor set to 1.\n        \n        # Sequentially update the estimate for each data point.\n        for i in range(N):\n            phi = u[i]  # The regressor at step i\n            y_i = y[i]  # The observation at step i\n            \n            # Kalman gain calculation\n            gain = (P * phi) / (lambda_rls + P * phi * phi)\n            \n            # Prediction error\n            prediction_error = y_i - phi * b_rls\n            \n            # Update parameter estimate\n            b_rls = b_rls + gain * prediction_error\n            \n            # Update covariance\n            P = (1.0 / lambda_rls) * (1.0 - gain * phi) * P\n            \n        # Task 4: Compute boolean assertions.\n        # Calculate the theoretical probability limit of the OLS estimator.\n        if k == 0:\n            b_inf_ols = b\n        else:\n            b_inf_ols = b - (k * sigma_e**2 * (1.0 + k * b)) / (g**2 * sigma_r**2 + k**2 * sigma_e**2)\n\n        # a) OLS_inconsistent: True if OLS is far from true 'b' but close to its theoretical limit.\n        ols_inconsistent = (abs(b_ols - b) > eps_true) and (abs(b_ols - b_inf_ols) = eps_lim)\n        \n        # b) IV_eliminates_bias: True if IV estimate is close to the true 'b'.\n        iv_eliminates_bias = (abs(b_iv - b) = eps_true)\n        \n        # c) RLS_matches_OLS: True if final RLS estimate is close to batch OLS estimate.\n        rls_matches_ols = (abs(b_rls - b_ols) = eps_rls)\n\n        all_results.extend([ols_inconsistent, iv_eliminates_bias, rls_matches_ols])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly converts Python booleans (True/False) to strings (\"True\"/\"False\").\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2718799"}]}