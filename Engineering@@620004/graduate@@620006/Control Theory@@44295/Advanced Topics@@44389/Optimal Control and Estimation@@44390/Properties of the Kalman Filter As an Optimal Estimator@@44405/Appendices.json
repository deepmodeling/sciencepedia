{"hands_on_practices": [{"introduction": "Our first practice problem takes us to the heart of the Kalman filter's long-term behavior. For many time-invariant systems, the filter's error covariance and gain converge to constant values after an initial transient phase. This exercise [@problem_id:2733975] challenges you to derive these steady-state values from first principles for a simple yet ubiquitous scalar autoregressive model. By solving the algebraic Riccati equation, you will gain a fundamental understanding of how the filter optimally balances trust between the system model and incoming measurements, and how this balance depends on the system dynamics ($a$), process noise variance ($Q$), and measurement noise variance ($R$).", "problem": "Consider the scalar linear time-invariant stochastic system\n$$x_{k+1}=a\\,x_k+w_k,\\qquad y_k=x_k+v_k,$$\nwhere $a\\in\\mathbb{R}$, the process noise $w_k$ is zero-mean, white, Gaussian with variance $Q>0$, the measurement noise $v_k$ is zero-mean, white, Gaussian with variance $R>0$, and $\\{w_k\\}$, $\\{v_k\\}$, and $x_0$ are mutually independent. Under the standard assumptions guaranteeing existence of a unique stabilizing steady-state solution for the discrete-time Kalman filter (Kalman filter (KF)), there exists a steady-state posterior error variance $P_\\infty$ and steady-state Kalman gain $K_\\infty$.\n\nStarting from the foundational definitions of linear minimum-variance estimation for Gaussian systems and the associated error covariance recursion, derive the closed-form expressions for the steady-state posterior error variance $P_\\infty$ and the steady-state Kalman gain $K_\\infty$ as functions of $a$, $Q$, and $R$. Your derivation should explicitly justify the selection of the stabilizing root. Then, analyze from first principles how $P_\\infty$ and $K_\\infty$ change when $a$, $Q$, and $R$ are varied, explaining the direction of change and any limiting behaviors for extreme parameter values. Finally, provide your answer as a single row matrix containing $P_\\infty$ and $K_\\infty$ in closed form, expressed symbolically in terms of $a$, $Q$, and $R$ only. No numerical evaluation is required, and no rounding is needed. The final answer must not include units.", "solution": "The problem requires the derivation of the steady-state posterior error variance $P_\\infty$ and Kalman gain $K_\\infty$ for a scalar linear time-invariant system, along with a parametric analysis. The derivation must proceed from first principles.\n\nThe system is defined by the state and measurement equations:\n$$x_{k+1} = a x_k + w_k$$\n$$y_k = x_k + v_k$$\nwhere $w_k \\sim \\mathcal{N}(0, Q)$ and $v_k \\sim \\mathcal{N}(0, R)$, with $Q > 0$ and $R > 0$. The system matrices are $F=a$ and $H=1$.\n\nThe Kalman filter equations for the error covariance are given by the prediction and update steps. Let $P_{k|k}$ be the posterior error variance at step $k$, and $P_{k|k-1}$ be the prior error variance.\n\nThe prediction step for the covariance is:\n$$P_{k|k-1} = F P_{k-1|k-1} F^T + Q = a^2 P_{k-1|k-1} + Q$$\n\nThe update step equations for the Kalman gain $K_k$ and posterior covariance $P_{k|k}$ are:\n$$K_k = P_{k|k-1} H^T (H P_{k|k-1} H^T + R)^{-1} = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$$\n$$P_{k|k} = (I - K_k H) P_{k|k-1} = (1 - K_k) P_{k|k-1}$$\n\nSubstituting the expression for $K_k$ into the covariance update equation gives:\n$$P_{k|k} = \\left(1 - \\frac{P_{k|k-1}}{P_{k|k-1} + R}\\right) P_{k|k-1} = \\frac{R}{P_{k|k-1} + R} P_{k|k-1}$$\nThis relationship can be expressed via the information filter form $P_{k|k}^{-1} = P_{k|k-1}^{-1} + R^{-1}$.\n\nTo find a single recursion for the posterior variance, we substitute the prediction equation into the update equation. Let $P_k \\equiv P_{k|k}$:\n$$P_k = \\frac{R (a^2 P_{k-1} + Q)}{a^2 P_{k-1} + Q + R}$$\nThis is the discrete-time Riccati equation for the posterior variance.\n\nIn steady state, the variance converges to a constant value, $P_k = P_{k-1} = P_\\infty$. The Riccati equation becomes the Discrete Algebraic Riccati Equation (DARE):\n$$P_\\infty = \\frac{R (a^2 P_\\infty + Q)}{a^2 P_\\infty + Q + R}$$\nRearranging this equation yields a quadratic equation for $P_\\infty$:\n$$P_\\infty (a^2 P_\\infty + Q + R) = R (a^2 P_\\infty + Q)$$\n$$a^2 P_\\infty^2 + Q P_\\infty + R P_\\infty = a^2 R P_\\infty + Q R$$\n$$a^2 P_\\infty^2 + (Q + R - a^2 R) P_\\infty - Q R = 0$$\n\nWe solve this quadratic equation for $P_\\infty$ using the quadratic formula $x = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with $A = a^2$, $B = Q + R - a^2 R$, and $C = -Q R$.\n$$P_\\infty = \\frac{-(Q + R - a^2 R) \\pm \\sqrt{(Q + R - a^2 R)^2 - 4(a^2)(-Q R)}}{2a^2}$$\n$$P_\\infty = \\frac{a^2 R - Q - R \\pm \\sqrt{(a^2 R - (Q+R))^2 + 4a^2 Q R}}{2a^2}$$\nThe discriminant $\\Delta = (a^2 R - (Q+R))^2 + 4a^2 Q R$ is strictly positive since $Q > 0, R > 0$. Thus, there are two distinct real roots. Since $A=a^2>0$ and $C=-QR<0$, the product of the roots is $C/A < 0$, meaning one root is positive and one is negative. As $P_\\infty$ is an error variance, it must be non-negative, $P_\\infty \\ge 0$. Therefore, we must choose the positive root, which requires selecting the $+$ sign in the numerator, because $\\sqrt{(B)^2-4AC} > |B|$.\n\nThe unique, physically meaningful solution is:\n$$P_\\infty = \\frac{a^2 R - Q - R + \\sqrt{(Q + R - a^2 R)^2 + 4a^2 Q R}}{2a^2}$$\nThe problem guarantees a unique stabilizing solution exists. This non-negative root is that solution. The stability of the filter is determined by the pole of the error dynamics, $\\lambda_{\\text{cl}} = a(1-K_\\infty)$. A stable filter requires $|\\lambda_{\\text{cl}}| < 1$. Choosing the positive root for $P_\\infty$ ensures this stability condition is met.\n\nNext, we derive the steady-state Kalman gain $K_\\infty$. From the update equations, we have the relations $P_\\infty^{-1} = P_{\\infty|\\infty-1}^{-1} + R^{-1}$ and $K_\\infty = P_{\\infty|\\infty-1}/(P_{\\infty|\\infty-1}+R)$. From the first relation, $P_{\\infty|\\infty-1} = \\frac{P_\\infty R}{R-P_\\infty}$. Substituting this into the second yields:\n$$K_\\infty = \\frac{\\frac{P_\\infty R}{R-P_\\infty}}{\\frac{P_\\infty R}{R-P_\\infty} + R} = \\frac{P_\\infty R}{P_\\infty R + R(R-P_\\infty)} = \\frac{P_\\infty R}{R^2} = \\frac{P_\\infty}{R}$$\nThis simple relation $K_\\infty = P_\\infty / R$ holds for this scalar system. Using the expression for $P_\\infty$:\n$$K_\\infty = \\frac{a^2 R - Q - R + \\sqrt{(Q + R - a^2 R)^2 + 4a^2 Q R}}{2a^2 R}$$\n\nFinally, we analyze the behavior of $P_\\infty$ and $K_\\infty$ with respect to the parameters $a$, $Q$, and $R$.\n\n1.  **Dependence on Process Noise Variance $Q$**:\n    -   $P_\\infty$: As $Q$ increases, the uncertainty in the state prediction grows. The filter must rely more on the measurement, but the overall estimation error variance $P_\\infty$ increases. Thus, $\\frac{\\partial P_\\infty}{\\partial Q} > 0$.\n        In the limit $Q \\to 0$, for a stable open-loop system ($|a|<1$), the state converges to zero and can be estimated perfectly, so $P_\\infty \\to 0$. For an unstable system ($|a| \\ge 1$), estimation remains necessary, and $P_\\infty \\to R(1-1/a^2)$.\n        In the limit $Q \\to \\infty$, the state prediction becomes useless. The posterior variance is limited only by the measurement quality, so $P_\\infty \\to R$.\n    -   $K_\\infty$: An increase in $Q$ signifies that the model prediction is less reliable. Consequently, the filter assigns more weight to the measurement innovation, so the gain increases. $\\frac{\\partial K_\\infty}{\\partial Q} > 0$. As $Q \\to \\infty$, $K_\\infty \\to 1$.\n\n2.  **Dependence on Measurement Noise Variance $R$**:\n    -   $P_\\infty$: As $R$ increases, the measurements become less reliable. This directly degrades the filter's ability to correct its estimate, leading to a higher posterior error variance. Thus, $\\frac{\\partial P_\\infty}{\\partial R} > 0$.\n        In the limit $R \\to 0$ (perfect measurement), the error variance is driven to zero, $P_\\infty \\to 0$.\n        In the limit $R \\to \\infty$ (no useful measurement), the filter performs open-loop prediction. For a stable system ($|a|<1$), the error variance converges to the process variance, $P_\\infty \\to \\frac{Q}{1-a^2}$. For an unstable system ($|a| \\ge 1$), the error variance grows without bound.\n    -   $K_\\infty$: As $R$ increases, the filter trusts the noisy measurements less, so the gain decreases. $\\frac{\\partial K_\\infty}{\\partial R} < 0$.\n        In the limit $R \\to 0$, $K_\\infty \\to 1$ (complete trust in the perfect measurement).\n        In the limit $R \\to \\infty$, $K_\\infty \\to 0$ (no trust in the useless measurement).\n\n3.  **Dependence on System Parameter $a$**:\n    -   $P_\\infty$: As $|a|$ increases, the system dynamics become faster or more unstable, making the state harder to predict and estimate. This results in a larger estimation error. Thus, $\\frac{\\partial P_\\infty}{\\partial |a|} > 0$.\n        In the limit $a \\to 0$, the system becomes $x_{k+1}=w_k$, and $P_\\infty \\to \\frac{QR}{Q+R}$.\n        In the limit $|a| \\to \\infty$, the prediction $a\\hat{x}_{k-1|k-1}$ is extremely large and uncertain. The prior variance $P_{\\infty|\\infty-1}$ becomes effectively infinite, causing the filter to discard the prediction and rely solely on the measurement. The posterior error becomes the measurement error, so $P_\\infty \\to R$.\n    -   $K_\\infty$: Since $K_\\infty=P_\\infty/R$, its dependence on $a$ follows that of $P_\\infty$. $\\frac{\\partial K_\\infty}{\\partial |a|} > 0$. As $|a| \\to \\infty$, $P_\\infty \\to R$, so $K_\\infty \\to 1$.\n\nThe closed-form expressions for $P_\\infty$ and $K_\\infty$ constitute the final answer.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{a^2 R - Q - R + \\sqrt{(Q + R - a^2 R)^2 + 4a^2 Q R}}{2a^2} & \\frac{a^2 R - Q - R + \\sqrt{(Q + R - a^2 R)^2 + 4a^2 Q R}}{2a^2 R} \\end{pmatrix}}$$", "id": "2733975"}, {"introduction": "Having analyzed the steady-state filtered estimate, we now broaden our perspective to compare the three principal types of state estimates: prediction, filtering, and smoothing. This practice problem [@problem_id:2733966] focuses on a random walk model, a cornerstone of time-series analysis, by setting the system dynamic parameter $a=1$. Your task is to derive the steady-state error variances for the one-step-ahead prediction, the current filtered estimate, and the optimal smoothed estimate, which uses a full history of measurements. By comparing these three variances, you will develop a quantitative intuition for the value of information at each stage of the estimation process.", "problem": "Consider a discrete-time scalar random walk observed in additive noise. The state evolves as $x_{k+1} = x_{k} + w_{k}$, where $w_{k} \\sim \\mathcal{N}(0, Q)$ are independent and identically distributed Gaussian process disturbances with variance $Q \\in \\mathbb{R}_{>0}$. The measurement is $y_{k} = x_{k} + v_{k}$, where $v_{k} \\sim \\mathcal{N}(0, R)$ are independent and identically distributed Gaussian measurement disturbances with variance $R \\in \\mathbb{R}_{>0}$, independent of $\\{w_{k}\\}$. Assume the system is operated for a sufficiently long time so that steady-state error covariances exist.\n\nUsing only the defining recursions of the Kalman filter and the Rauch–Tung–Striebel (RTS) smoother, derive from first principles the steady-state error variances for:\n- the a priori one-step prediction (the variance of the prediction error before the measurement update),\n- the a posteriori filtering estimate (the variance of the estimation error immediately after the measurement update at the same time index),\n- the fixed-interval infinite-horizon smoothing estimate at any interior time (the variance of the estimation error given the entire bi-infinite sequence of measurements).\n\nExpress each of these steady-state variances exclusively as a closed-form analytic function of $Q$ and $R$. Report your final answer as a single row matrix whose entries, in order, are the steady-state prediction variance, filtering variance, and smoothing variance. No numerical approximation is required; provide exact symbolic expressions. Do not include units in your final answer.", "solution": "The system is a discrete-time, linear time-invariant (LTI) scalar system described by:\nState equation: $x_{k+1} = A x_{k} + w_{k}$\nMeasurement equation: $y_{k} = H x_{k} + v_{k}$\nFor the given random walk model, the system matrices are $A=1$ and $H=1$. The process noise $w_k$ and measurement noise $v_k$ are zero-mean, white, Gaussian, and mutually independent, with variances $E[w_k^2] = Q$ and $E[v_k^2] = R$, respectively, where $Q>0$ and $R>0$.\n\nThe Kalman filter provides optimal estimates of the state $x_k$. The error variances associated with these estimates are governed by the following recursions:\nLet $P_{k|k-1}$ be the a priori error variance (prediction variance) of the estimate $\\hat{x}_{k|k-1} = E[x_k | y_1, ..., y_{k-1}]$.\nLet $P_{k|k}$ be the a posteriori error variance (filtering variance) of the estimate $\\hat{x}_{k|k} = E[x_k | y_1, ..., y_k]$.\n\nThe filter recursions for these variances are:\n1. Time Update (Prediction): $P_{k|k-1} = A P_{k-1|k-1} A^T + Q$. For our system, this is $P_{k|k-1} = P_{k-1|k-1} + Q$.\n2. Measurement Update (Correction):\n   The Kalman gain is $K_k = P_{k|k-1} H^T (H P_{k|k-1} H^T + R)^{-1}$. For our system, $K_k = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$.\n   The a posteriori variance is $P_{k|k} = (I - K_k H) P_{k|k-1}$. For our system, $P_{k|k} = \\left(1 - \\frac{P_{k|k-1}}{P_{k|k-1} + R}\\right) P_{k|k-1} = \\frac{R}{P_{k|k-1} + R} P_{k|k-1}$.\n\nThe problem assumes the system has reached steady state, meaning the variances are constant over time. Let $P_{pred} = \\lim_{k\\to\\infty} P_{k|k-1}$ and $P_{filt} = \\lim_{k\\to\\infty} P_{k|k}$. The recursions become algebraic equations:\n(1) $P_{pred} = P_{filt} + Q$\n(2) $P_{filt} = \\frac{R}{P_{pred} + R} P_{pred}$\n\nBy substituting (1) into (2), we could solve for $P_{filt}$, but it is more direct to solve the Discrete Algebraic Riccati Equation (DARE) for $P_{pred}$. Combining the two update steps into one equation for $P_{k|k-1}$:\n$P_{k|k-1} = A \\left((I - K_{k-1}H)P_{k-1|k-2}\\right) A^T + Q$.\nIn steady state, $P_{k|k-1} = P_{k-1|k-2} = P_{pred}$, so the DARE for $P_{pred}$ is:\n$P_{pred} = A \\left( (I - K H) P_{pred} \\right) A^T + Q$, with $K = \\frac{P_{pred}}{P_{pred}+R}$.\nSubstituting the system parameters $A=1$, $H=1$:\n$P_{pred} = 1 \\cdot \\left( \\left(1 - \\frac{P_{pred}}{P_{pred}+R}\\right) P_{pred} \\right) \\cdot 1 + Q$\n$P_{pred} = \\left( \\frac{R}{P_{pred}+R} \\right) P_{pred} + Q$\n$P_{pred} (P_{pred}+R) = R P_{pred} + Q (P_{pred}+R)$\n$P_{pred}^2 + R P_{pred} = R P_{pred} + Q P_{pred} + QR$\n$P_{pred}^2 - Q P_{pred} - QR = 0$\n\nThis is a quadratic equation for $P_{pred}$. The solution is given by the quadratic formula:\n$P_{pred} = \\frac{-(-Q) \\pm \\sqrt{(-Q)^2 - 4(1)(-QR)}}{2(1)} = \\frac{Q \\pm \\sqrt{Q^2 + 4QR}}{2}$.\nSince variance must be non-negative ($P_{pred} \\ge 0$), and given $Q>0, R>0$, we must select the positive root:\n$P_{pred} = \\frac{Q + \\sqrt{Q^2 + 4QR}}{2}$.\nThis is the steady-state prediction error variance.\n\nNow, we find the steady-state filtering variance $P_{filt}$ using equation (1):\n$P_{filt} = P_{pred} - Q = \\frac{Q + \\sqrt{Q^2 + 4QR}}{2} - Q = \\frac{-Q + \\sqrt{Q^2 + 4QR}}{2}$.\nThis is the steady-state filtering error variance.\n\nNext, we derive the steady-state smoothing error variance. The Rauch-Tung-Striebel (RTS) smoother provides the optimal estimate $\\hat{x}_{k|N}$ given a batch of measurements up to time $N > k$. The corresponding error variance $P_{k|N}$ is given by the recursion:\n$P_{k|N} = P_{k|k} + C_k (P_{k+1|N} - P_{k+1|k}) C_k^T$, where $C_k = P_{k|k} A^T P_{k+1|k}^{-1}$.\n\nFor an infinite-horizon smoother operating on a system in steady state, we consider the limit as $N \\to \\infty$. All variances become constant: $P_{k|N} \\to P_{smooth}$, $P_{k|k} \\to P_{filt}$, $P_{k+1|k} \\to P_{pred}$. The smoother gain becomes $C_s = P_{filt} A^T P_{pred}^{-1} = \\frac{P_{filt}}{P_{pred}}$.\nThe RTS recursion for the variance becomes an algebraic equation for $P_{smooth}$:\n$P_{smooth} = P_{filt} + C_s (P_{smooth} - P_{pred}) C_s^T = P_{filt} + \\left(\\frac{P_{filt}}{P_{pred}}\\right)^2 (P_{smooth} - P_{pred})$.\nWe solve for $P_{smooth}$:\n$P_{smooth} \\left(1 - \\frac{P_{filt}^2}{P_{pred}^2}\\right) = P_{filt} - \\frac{P_{filt}^2}{P_{pred}}$\n$P_{smooth} \\left(\\frac{P_{pred}^2 - P_{filt}^2}{P_{pred}^2}\\right) = \\frac{P_{filt} P_{pred} - P_{filt}^2}{P_{pred}}$\n$P_{smooth} \\frac{(P_{pred}-P_{filt})(P_{pred}+P_{filt})}{P_{pred}^2} = \\frac{P_{filt}(P_{pred}-P_{filt})}{P_{pred}}$.\nFrom (1), we have $P_{pred}-P_{filt} = Q$. Since $Q>0$, this term is non-zero and can be cancelled:\n$P_{smooth} \\frac{P_{pred}+P_{filt}}{P_{pred}^2} = \\frac{P_{filt}}{P_{pred}}$.\n$P_{smooth} = \\frac{P_{filt} P_{pred}}{P_{pred} + P_{filt}}$.\n\nTo express this in terms of $Q$ and $R$, we substitute our expressions for $P_{pred}$ and $P_{filt}$.\nThe product $P_{filt} P_{pred}$ can be obtained from the DARE $P_{pred}^2 - Q P_{pred} - QR = 0$, which implies $P_{pred}(P_{pred}-Q) = QR$. Since $P_{filt}=P_{pred}-Q$, we have $P_{filt}P_{pred} = QR$.\nThe sum is $P_{pred} + P_{filt} = \\left(\\frac{Q + \\sqrt{Q^2 + 4QR}}{2}\\right) + \\left(\\frac{-Q + \\sqrt{Q^2 + 4QR}}{2}\\right) = \\frac{2\\sqrt{Q^2 + 4QR}}{2} = \\sqrt{Q^2 + 4QR}$.\nSubstituting these results:\n$P_{smooth} = \\frac{QR}{\\sqrt{Q^2 + 4QR}}$.\nThis is the steady-state smoothing error variance.\n\nThe three requested variances are:\nPrediction variance: $\\frac{Q + \\sqrt{Q^2 + 4QR}}{2}$\nFiltering variance: $\\frac{-Q + \\sqrt{Q^2 + 4QR}}{2}$\nSmoothing variance: $\\frac{QR}{\\sqrt{Q^2 + 4QR}}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{Q + \\sqrt{Q^2 + 4QR}}{2} & \\frac{-Q + \\sqrt{Q^2 + 4QR}}{2} & \\frac{QR}{\\sqrt{Q^2 + 4QR}}\n\\end{pmatrix}\n}\n$$", "id": "2733966"}, {"introduction": "Theory and practice are not always the same, especially in the world of numerical computation. This final exercise [@problem_id:2733957] moves from the ideal world of infinite-precision arithmetic to the practical challenges of implementing a Kalman filter on a computer. You will investigate a scenario where a poor choice of coordinates makes a theoretically correct covariance update formula numerically unstable, potentially leading to a non-positive definite covariance matrix. By analyzing this failure mode and contrasting it with the robust 'Joseph form' of the update, you will learn a critical lesson in designing and implementing numerically sound estimation algorithms.", "problem": "Consider the following linear-Gaussian estimation problem that illustrates how a poor choice of coordinates can induce numerical loss of positive semidefiniteness (PSD) in a naive covariance update. Let the prior state $x \\in \\mathbb{R}^{2}$ have zero mean and prior covariance\n$$\nP^{-} \\;=\\; \\begin{pmatrix}\n1 & 1-\\varepsilon\\\\\n1-\\varepsilon & 1\n\\end{pmatrix},\n$$\nwith $0 < \\varepsilon \\ll 1$. A single scalar measurement is taken,\n$$\ny \\;=\\; H x + v,\\quad H \\;=\\; \\begin{pmatrix} 1 & -1 \\end{pmatrix},\\quad v \\sim \\mathcal{N}(0,\\rho),\n$$\nwith $0 < \\rho \\ll 1$. All quantities are dimensionless. The coordinate choice in which the two state components are nearly equal makes the measurement extract the small difference $x_{1}-x_{2}$, resulting in a highly ill-conditioned innovation.\n\nWork from first principles of the linear minimum mean-square error estimator and the orthogonality principle (do not assume any textbook covariance “update formulas” a priori). Proceed as follows.\n\n1) Derive the unique linear estimator $\\hat{x}^{+} = \\hat{x}^{-} + K\\,(y - H \\hat{x}^{-})$ that minimizes the posterior mean-square error, and express the resulting posterior error covariance $P^{+}$ solely in terms of $P^{-}$, $H$, $K$, and $\\rho$ in a form that makes its PSD property manifest.\n\n2) Specialize your result to the given $P^{-}$, $H$, and $\\rho$, and diagonalize $P^{-}$ in the orthonormal basis $u = \\frac{1}{\\sqrt{2}}(1,1)^{\\top}$ and $v=\\frac{1}{\\sqrt{2}}(1,-1)^{\\top}$. Using only the definitions above, compute the posterior variance along the $v$-direction, denoted $\\lambda_{v}^{+}$.\n\n3) Now model a naive covariance update performed in the original coordinates by $P_{\\mathrm{naive}}^{+} = (I - K H) P^{-}$ and suppose that, due to finite-precision rounding in this ill-conditioned basis, the posterior variance along the $v$-direction is perturbed by a symmetric reduction of magnitude $\\delta > 0$. In other words, the variance along the $v$-direction becomes $\\lambda_{v}^{+} - \\delta$. Determine the smallest perturbation magnitude $\\delta_{c}$ that makes $P_{\\mathrm{naive}}^{+}$ lose PSD, i.e., the threshold at which the posterior variance along $v$ becomes nonpositive.\n\n4) Briefly justify, based on your expression from part 1), why the same roundoff effects do not destroy PSD when the covariance is updated in a form that is explicitly a sum of positive semidefinite terms, and explain the role of a coordinate change in exacerbating the ill-conditioning of the innovation computation.\n\nEvaluate your final expression for $\\delta_{c}$ for the concrete values $\\varepsilon = 10^{-8}$ and $\\rho = 10^{-12}$, and report the numerical value rounded to four significant figures. Express your final answer as a pure number (no units).", "solution": "1) Derivation of the optimal estimator and posterior covariance.\n\nWe are tasked to find the linear estimator $\\hat{x}^{+} = \\hat{x}^{-} + K(y - H\\hat{x}^{-})$ that minimizes the posterior mean-square error, which is the trace of the posterior error covariance $P^{+} = E[(x-\\hat{x}^{+})(x-\\hat{x}^{+})^\\top]$. The prior mean is given as $\\hat{x}^{-} = E[x] = 0$. The estimator form simplifies to $\\hat{x}^{+} = Ky$.\n\nThe posterior estimation error is $\\tilde{x}^{+} = x - \\hat{x}^{+} = x - Ky$. Substituting the measurement model $y = Hx+v$, we get:\n$$\n\\tilde{x}^{+} = x - K(Hx+v) = (I-KH)x - Kv\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The state $x$ and noise $v$ are uncorrelated, so $E[xv^\\top] = 0$. The posterior covariance is then:\n$$\nP^{+} = E[\\tilde{x}^{+}(\\tilde{x}^{+})^\\top] = E[((I-KH)x - Kv)((I-KH)x - Kv)^\\top]\n$$\n$$\nP^{+} = E[(I-KH)xx^\\top(I-KH)^\\top - (I-KH)xv^\\top K^\\top - Kvx^\\top(I-KH)^\\top + Kvv^\\top K^\\top]\n$$\nUsing the linearity of expectation and the uncorrelation of $x$ and $v$:\n$$\nP^{+} = (I-KH)E[xx^\\top](I-KH)^\\top - 0 - 0 + KE[vv^\\top]K^\\top\n$$\nGiven the definitions $P^{-} = E[xx^\\top]$ and $E[vv^\\top] = \\rho$ (since $v$ is a scalar), we obtain:\n$$\nP^{+} = (I-KH)P^{-}(I-KH)^\\top + K\\rho K^\\top\n$$\nThis is the Joseph form of the covariance update. This form makes the Positive Semidefinite (PSD) property manifest, as it is a sum of two PSD matrices. $P^{-}$ is PSD, so $(I-KH)P^{-}(I-KH)^\\top$ is PSD. Since $\\rho > 0$, $K\\rho K^\\top$ is also PSD. Their sum is therefore guaranteed to be PSD.\n\nTo find the optimal gain matrix $K$ that minimizes the mean-square error, we apply the orthogonality principle, which states that the estimation error must be orthogonal to the data. For a linear estimator, this means $E[\\tilde{x}^{+}y^\\top] = 0$. Since $y$ is a scalar, this is $E[\\tilde{x}^{+}y] = 0$.\n$$\nE[(x-Ky)y] = 0 \\implies E[xy] - KE[y^2] = 0\n$$\nThis gives the optimal gain $K = E[xy](E[y^2])^{-1}$. We compute the two expectations:\n$$\nE[xy] = E[x(Hx+v)^\\top] = E[xx^\\top H^\\top + xv^\\top] = E[xx^\\top]H^\\top = P^{-}H^\\top\n$$\n$$\nE[y^2] = E[(Hx+v)^2] = E[Hxx^\\top H^\\top + 2Hxv + v^2] = HE[xx^\\top]H^\\top + E[v^2] = HP^{-}H^\\top + \\rho\n$$\nThe term $S = HP^{-}H^\\top + \\rho$ is the innovation covariance. The optimal gain, known as the Kalman gain, is:\n$$\nK = P^{-}H^\\top (HP^{-}H^\\top + \\rho)^{-1}\n$$\n\n2) Specialization to the problem and calculation of posterior variance.\n\nWe are given $P^{-} = \\begin{pmatrix} 1 & 1-\\varepsilon\\\\ 1-\\varepsilon & 1 \\end{pmatrix}$, $H = \\begin{pmatrix} 1 & -1 \\end{pmatrix}$, and $\\rho$.\nFirst, we compute the components needed for the gain $K$:\n$$\nHP^{-} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 1-\\varepsilon\\\\ 1-\\varepsilon & 1 \\end{pmatrix} = \\begin{pmatrix} 1-(1-\\varepsilon) & (1-\\varepsilon)-1 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon & -\\varepsilon \\end{pmatrix}\n$$\n$$\nHP^{-}H^\\top = \\begin{pmatrix} \\varepsilon & -\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\varepsilon - (-\\varepsilon) = 2\\varepsilon\n$$\nThe innovation covariance is $S = 2\\varepsilon + \\rho$.\n$$\nP^{-}H^\\top = (HP^{-})^\\top = \\begin{pmatrix} \\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\nThe optimal gain is:\n$$\nK = \\frac{1}{2\\varepsilon+\\rho} \\begin{pmatrix} \\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\nThe orthonormal vectors are $u = \\frac{1}{\\sqrt{2}}(1,1)^{\\top}$ and $v=\\frac{1}{\\sqrt{2}}(1,-1)^{\\top}$. Let us verify they are eigenvectors of $P^{-}$:\n$$\nP^{-}u = \\begin{pmatrix} 1 & 1-\\varepsilon\\\\ 1-\\varepsilon & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1+1-\\varepsilon \\\\ 1-\\varepsilon+1 \\end{pmatrix} = (2-\\varepsilon)\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = (2-\\varepsilon)u\n$$\n$$\nP^{-}v = \\begin{pmatrix} 1 & 1-\\varepsilon\\\\ 1-\\varepsilon & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1-(1-\\varepsilon) \\\\ 1-\\varepsilon-1 \\end{pmatrix} = \\varepsilon\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\varepsilon v\n$$\nThe eigenvalues are $\\lambda_{u} = 2-\\varepsilon$ and $\\lambda_{v} = \\varepsilon$. These are the prior variances along the $u$ and $v$ directions.\nThe posterior variance along the $v$-direction is $\\lambda_{v}^{+} = v^\\top P^{+} v$. We use the standard update formula $P^{+} = P^{-} - KHP^{-}$, which is algebraically equivalent to the Joseph form.\n$$\n\\lambda_{v}^{+} = v^\\top (P^{-} - KHP^{-}) v = v^\\top P^{-} v - v^\\top K H P^{-} v\n$$\nSince $v$ is an eigenvector of $P^{-}$ with eigenvalue $\\varepsilon$, we have $P^{-}v = \\varepsilon v$. Thus, $v^\\top P^{-} v = v^\\top(\\varepsilon v) = \\varepsilon (v^\\top v) = \\varepsilon$.\nThe second term is $v^\\top K (H P^{-} v) = v^\\top K H (\\varepsilon v) = \\varepsilon (v^\\top K) (Hv)$.\nLet's compute the components:\n$$\nHv = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1+1) = \\sqrt{2}\n$$\n$$\nv^\\top K = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & -1 \\end{pmatrix} \\left( \\frac{1}{2\\varepsilon+\\rho} \\begin{pmatrix} \\varepsilon \\\\ -\\varepsilon \\end{pmatrix} \\right) = \\frac{\\varepsilon}{2\\varepsilon+\\rho} \\frac{1}{\\sqrt{2}} (1 - (-1)) = \\frac{\\varepsilon\\sqrt{2}}{2\\varepsilon+\\rho}\n$$\nSubstituting these into the expression for $\\lambda_v^+$:\n$$\n\\lambda_{v}^{+} = \\varepsilon - \\left(\\frac{\\varepsilon\\sqrt{2}}{2\\varepsilon+\\rho}\\right) (\\sqrt{2}\\varepsilon) = \\varepsilon - \\frac{2\\varepsilon^2}{2\\varepsilon+\\rho} = \\frac{\\varepsilon(2\\varepsilon+\\rho) - 2\\varepsilon^2}{2\\varepsilon+\\rho} = \\frac{2\\varepsilon^2 + \\varepsilon\\rho - 2\\varepsilon^2}{2\\varepsilon+\\rho} = \\frac{\\varepsilon\\rho}{2\\varepsilon+\\rho}\n$$\n\n3) Naive update and loss of PSD.\n\nThe naive update formula $P_{\\mathrm{naive}}^{+} = (I - K H) P^{-}$ is algebraically correct but can be numerically unstable. The problem states that rounding errors in this computation cause the posterior variance along the $v$-direction to be reduced by $\\delta > 0$. The resulting matrix, let's call it $P_{\\text{pert}}$, would have a variance of $\\lambda_{v}^{+} - \\delta$ in the $v$ direction. For $P_{\\text{pert}}$ to be PSD, all its eigenvalues must be non-negative.\n\nThe eigenvectors of $P^{+}$ are also $u$ and $v$. The eigenvalue corresponding to $u$ is $\\lambda_u^{+} = u^\\top P^{+} u = u^\\top (P^{-} - KHP^{-}) u$. Since $Hu=0$, the second term vanishes, leaving $\\lambda_u^{+} = u^\\top P^{-} u = \\lambda_u = 2-\\varepsilon$. This eigenvalue is positive for $\\varepsilon \\ll 1$. The eigenvalues of $P^{+}$ are $2-\\varepsilon$ and $\\frac{\\varepsilon\\rho}{2\\varepsilon+\\rho}$.\n\nThe perturbed matrix effectively has its eigenvalue along the $v$-direction reduced. The matrix loses PSD when its smallest eigenvalue becomes non-positive. Since $2-\\varepsilon > 0$, the critical condition is on the other eigenvalue:\n$$\n\\lambda_{v}^{+} - \\delta \\le 0\n$$\nThe smallest perturbation $\\delta_c$ that makes the matrix lose PSD (i.e., makes the eigenvalue exactly zero) is therefore:\n$$\n\\delta_{c} = \\lambda_{v}^{+} = \\frac{\\varepsilon\\rho}{2\\varepsilon+\\rho}\n$$\n\n4) Justification and role of coordinates.\n\nAs derived in part 1), the Joseph form of the covariance update is $P^{+} = (I-KH)P^{-}(I-KH)^\\top + K\\rho K^\\top$. This expression is a sum of two matrices that are themselves guaranteed to be PSD. The sum of PSD matrices is always PSD. A numerical implementation following this structure will preserve the PSD property, as small rounding errors are unlikely to make either of the summed terms non-PSD with a large negative eigenvalue. This form is numerically robust.\n\nIn contrast, the form $P_{\\mathrm{naive}}^{+} = (I - K H) P^{-}$ involves matrix multiplication where catastrophic cancellation can occur. The term $I-KH$ can be calculated with some precision, but when it multiplies $P^{-}$, which is nearly singular (its determinant is $\\varepsilon(2-\\varepsilon)$), rounding errors can be magnified. Specifically, for $\\rho \\ll \\varepsilon$, we have $K \\approx \\frac{1}{2}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, and $KH \\approx \\frac{1}{2}\\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$. Then $I-KH \\approx \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$. Multiplying this by $P^{-} = \\begin{pmatrix} 1 & 1-\\varepsilon \\\\ 1-\\varepsilon & 1 \\end{pmatrix}$ gives approximately $\\begin{pmatrix} 1-\\varepsilon/2 & 1-\\varepsilon/2 \\\\ 1-\\varepsilon/2 & 1-\\varepsilon/2 \\end{pmatrix}$, a singular matrix with an eigenvalue of $0$. The true eigenvalue $\\lambda_{v}^{+}$ is a very small positive number, $\\frac{\\varepsilon\\rho}{2\\varepsilon+\\rho} \\approx \\rho/2$. The naive computation, which involves subtraction of large, nearly equal numbers, can easily result in a final value of $0$ or even negative due to finite precision, thereby destroying the PSD property.\n\nThe coordinate system choice exacerbates this ill-conditioning. The original state components $x_1$ and $x_2$ are highly correlated (correlation coefficient $1-\\varepsilon \\approx 1$). The measurement $y = x_1 - x_2 + v$ extracts a small difference between these two large, nearly equal states. This is a classic setup for numerical sensitivity. By changing to the coordinate system defined by the eigenvectors $u$ and $v$ of $P^{-}$, the state becomes decoupled. In these coordinates, the measurement $y = \\sqrt{2}x_v + v$ only provides information about the $x_v$ component, and the update becomes clean, affecting only the variance of $x_v$ without any large cancellations.\n\nFinally, we evaluate $\\delta_c$ for $\\varepsilon = 10^{-8}$ and $\\rho = 10^{-12}$:\n$$\n\\delta_{c} = \\frac{(10^{-8})(10^{-12})}{2(10^{-8}) + 10^{-12}} = \\frac{10^{-20}}{2 \\times 10^{-8} + 0.0001 \\times 10^{-8}} = \\frac{10^{-20}}{(2.0001) \\times 10^{-8}}\n$$\n$$\n\\delta_{c} = \\frac{1}{2.0001} \\times 10^{-12} \\approx 0.499975001 \\times 10^{-12} = 4.99975001 \\times 10^{-13}\n$$\nRounding to four significant figures, we get $5.000 \\times 10^{-13}$.", "answer": "$$\\boxed{5.000 \\times 10^{-13}}$$", "id": "2733957"}]}