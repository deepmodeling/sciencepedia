{"hands_on_practices": [{"introduction": "Our journey into the practical application of the calculus of variations begins with a classic and fundamental problem. This exercise connects the abstract Euler-Lagrange equation to the familiar physics of the simple harmonic oscillator [@problem_id:1260]. By finding the function that extremizes a functional analogous to the action (kinetic minus potential energy), you will derive the system's equation of motion from a more fundamental standpoint, a cornerstone technique in both classical and quantum mechanics.", "problem": "A functional is a mapping from a set of functions to the real numbers. In the calculus of variations, we seek a function $y(x)$ that extremizes (i.e., makes stationary) a given functional. A fundamental tool for this is the Euler-Lagrange equation.\n\nConsider the functional $J[y]$ defined by the integral:\n$$J[y] = \\int_{x_1}^{x_2} L(x, y, y') \\, dx = \\int_{x_1}^{x_2} (y'(x)^2 - k^2 y(x)^2) \\, dx$$\nwhere $y(x)$ is a twice-differentiable function, $y'(x) = \\frac{dy}{dx}$, and $k$ is a non-zero real constant. The function $y(x)$ must satisfy the fixed boundary conditions $y(x_1) = y_1$ and $y(x_2) = y_2$.\n\nThe function $y(x)$ that extremizes this functional must satisfy the Euler-Lagrange equation:\n$$ \\frac{\\partial L}{\\partial y} - \\frac{d}{dx}\\left(\\frac{\\partial L}{\\partial y'}\\right) = 0 $$\nAssume that $\\sin(k(x_2 - x_1)) \\neq 0$.\n\nDerive the explicit functional form of $y(x)$ that extremizes $J[y]$ subject to the given boundary conditions. Your final expression should be in terms of $x$, $k$, $x_1$, $x_2$, $y_1$, and $y_2$.", "solution": "Applying the Euler-Lagrange equation to the Lagrangian\n$$L(x,y,y')=y'^2-k^2y^2$$\nwe compute the partial derivatives:\n$$\\frac{\\partial L}{\\partial y}=-2k^2y,\\qquad \\frac{\\partial L}{\\partial y'}=2y'$$\nSubstituting these into the Euler-Lagrange equation gives:\n$$-2k^2y - \\frac{d}{dx}(2y') = 0 \\quad\\Longrightarrow\\quad y''+k^2y=0$$\nThis is the equation for a simple harmonic oscillator. The general solution is:\n$$y(x)=A\\cos(kx)+B\\sin(kx)$$\nWe enforce the boundary conditions $y(x_1)=y_1$ and $y(x_2)=y_2$ to solve for the constants $A$ and $B$:\n$$A\\cos(kx_1)+B\\sin(kx_1)=y_1$$\n$$A\\cos(kx_2)+B\\sin(kx_2)=y_2$$\nThe determinant of this linear system is $\\sin(k(x_2-x_1))$, which is non-zero by assumption. Solving this system yields the unique solution. A convenient explicit form of the solution is:\n$$\ny(x)=\\frac{y_1\\,\\sin\\bigl(k(x_2-x)\\bigr)+y_2\\,\\sin\\bigl(k(x-x_1)\\bigr)}\n{\\sin\\bigl(k(x_2-x_1)\\bigr)}\n$$\nThis form manifestly satisfies both boundary conditions and the ODE.", "answer": "$$\\boxed{\\frac{y_1\\,\\sin\\bigl(k(x_2-x)\\bigr)+y_2\\,\\sin\\bigl(k(x-x_1)\\bigr)}{\\sin\\bigl(k(x_2-x_1)\\bigr)}}$$", "id": "1260"}, {"introduction": "Real-world optimization problems are seldom unconstrained. This practice explores how to handle such scenarios by minimizing the bending energy of an elastic beam subject to an integral constraint on its deflection [@problem_id:1151573]. You will employ the method of Lagrange multipliers to incorporate the constraint and work with a functional involving second derivatives, leading to the higher-order Euler-Poisson equation, a vital tool for problems in structural mechanics and optimal control.", "problem": "A flexible, thin elastic beam of length $L$ is modeled by a function $y(x)$ for $x \\in [0, L]$, representing its deflection from the equilibrium position. The bending energy of the beam is proportional to the functional $J[y] = \\int_0^L (y''(x))^2 dx$.\n\nThe beam is simply supported at its ends, which imposes the boundary conditions $y(0) = 0$, $y''(0) = 0$, $y(L) = 0$, and $y''(L) = 0$.\n\nWe are interested in the shape of the beam, i.e., the function $y(x)$, that minimizes this bending energy, subject to a normalization constraint on its deflection given by $\\int_0^L y(x)^2 dx = 1$.\n\nUsing the calculus of variations, determine the minimum possible value of the bending energy functional $J[y]$ under these conditions.", "solution": "We minimize $J[y]=\\int_0^L (y''(x))^2\\,dx$ subject to $\\int_0^L y(x)^2\\,dx=1$ and boundary conditions $y(0)=y(L)=y''(0)=y''(L)=0$.\nWe use a Lagrange multiplier $\\lambda$ for the augmented Lagrangian $\\mathcal L=(y'')^2-\\lambda y^2$. The Euler-Poisson equation is $\\frac{d^2}{dx^2}(\\frac{\\partial\\mathcal{L}}{\\partial y''}) + \\frac{\\partial\\mathcal{L}}{\\partial y} = 0$, which gives $y''''-\\lambda y=0$.\nSetting $\\lambda=k^4$, the general solution is $y(x)=C_1\\cosh kx+C_2\\sinh kx+C_3\\cos kx+C_4\\sin kx$.\nThe boundary conditions $y(0)=y''(0)=0$ imply $C_1=C_3=0$.\nThe conditions $y(L)=y''(L)=0$ then require $C_2\\sinh kL + C_4\\sin kL = 0$ and $C_2\\sinh kL - C_4\\sin kL = 0$. For a non-trivial solution, this forces $C_2=0$ and $\\sin(kL)=0$.\nThus, $kL=n\\pi$ for $n\\in\\mathbb N$. The eigenfunctions are proportional to $\\sin(n\\pi x/L)$.\nThe eigenvalues are $\\lambda_n=k^4=(n\\pi/L)^4$.\nThe value of the functional is $J[y_n] = \\int_0^L (y_n'')^2 dx = \\int_0^L y_n'''' y_n dx = \\int_0^L \\lambda_n y_n^2 dx = \\lambda_n$, where we used integration by parts and the boundary conditions, and finally the constraint $\\int_0^L y_n^2 dx = 1$.\nThe minimum value of $J$ is the smallest eigenvalue, which occurs for $n=1$.\n$$J_{\\min}=\\left(\\frac{\\pi}{L}\\right)^4$$", "answer": "$$\\boxed{\\left(\\frac{\\pi}{L}\\right)^4}$$", "id": "1151573"}, {"introduction": "To achieve a true mastery of variational principles, we must look beyond simply applying established formulas and understand their origins. This exercise guides you back to first principles to derive the necessary conditions for an extremal from the ground up [@problem_id:2691366]. By analyzing the first variation of a functional with second derivatives, you will not only derive the Euler-Poisson equation but also uncover the \"natural boundary conditions\" that arise when endpoint derivatives are free, a subtle but critical concept in advanced mechanics and control system design.", "problem": "Consider the functional $J[y] = \\int_{0}^{1} \\tfrac{1}{2}\\, \\big(y''(x)\\big)^{2} \\, dx$ over the class of twice continuously differentiable functions $y:[0,1]\\to\\mathbb{R}$ with prescribed boundary values $y(0)=0$ and $y(1)=1$. The endpoint derivatives are free, meaning $y'(0)$ and $y'(1)$ are not prescribed. Starting from the first principles of the calculus of variations and the definition of the first variation, derive the necessary condition satisfied by an extremal, namely the Eulerâ€“Poisson equation for a functional that depends on second derivatives. Then determine the natural boundary conditions that arise from the free endpoint derivatives and solve the resulting boundary value problem, which must be an ordinary differential equation (ODE), to obtain the minimizing function in closed form. Provide your final answer as a single closed-form analytic expression for the minimizer $y(x)$ on $[0,1]$. No rounding is required.", "solution": "Let $y(x)$ be the extremal function and consider a varied path $y(x) + \\epsilon \\eta(x)$, where $\\eta(x)$ is an admissible variation function. The fixed boundary conditions $y(0)=0$ and $y(1)=1$ require that $\\eta(0)=0$ and $\\eta(1)=0$.\n\nFor $y(x)$ to be an extremal, the first variation $\\delta J$ must be zero.\n$$\\delta J = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\int_{0}^{1} \\frac{1}{2} (y'' + \\epsilon\\eta'')^2 dx = \\int_{0}^{1} y''(x) \\eta''(x) dx = 0$$\nTo isolate $\\eta(x)$, we integrate by parts twice:\n\\begin{align*} \\int_{0}^{1} y''(x) \\eta''(x) dx = \\left[ y''(x) \\eta'(x) \\right]_{0}^{1} - \\int_{0}^{1} y'''(x) \\eta'(x) dx \\\\ = \\left[ y''(x) \\eta'(x) \\right]_{0}^{1} - \\left[ y'''(x) \\eta(x) \\right]_{0}^{1} + \\int_{0}^{1} y''''(x) \\eta(x) dx \\end{align*}\nSince $\\eta(0)=\\eta(1)=0$, the second boundary term vanishes. The condition for an extremum becomes:\n$$\\int_{0}^{1} y''''(x) \\eta(x) dx + y''(1)\\eta'(1) - y''(0)\\eta'(0) = 0$$\nThis must hold for all admissible variations $\\eta$. If we first consider only variations where $\\eta'(0)=\\eta'(1)=0$, the boundary terms vanish. The condition $\\int_{0}^{1} y''''(x) \\eta(x) dx = 0$ for all such $\\eta(x)$ implies, by the fundamental lemma of calculus of variations, that the Euler-Poisson equation must hold:\n$$y''''(x) = 0$$\nWith this result, the integral term is always zero. The condition reduces to $y''(1)\\eta'(1) - y''(0)\\eta'(0) = 0$. Since the endpoint derivatives are free, $\\eta'(0)$ and $\\eta'(1)$ can be chosen arbitrarily. This forces their coefficients to be zero, which gives the natural boundary conditions:\n$$y''(0) = 0 \\quad \\text{and} \\quad y''(1) = 0$$\nWe now solve the ODE $y''''(x)=0$ with the four boundary conditions: $y(0)=0$, $y(1)=1$, $y''(0)=0$, and $y''(1)=0$.\nThe general solution to the ODE is a cubic polynomial: $y(x) = Ax^3 + Bx^2 + Cx + D$.\nApplying the conditions:\n1. $y(0) = 0 \\implies D=0$.\n2. $y''(x) = 6Ax + 2B$.\n3. $y''(0) = 0 \\implies 2B=0 \\implies B=0$.\n4. $y''(1) = 0 \\implies 6A+2B=0 \\implies A=0$.\n5. $y(1) = 1 \\implies A+B+C+D=1 \\implies C=1$.\nThe unique solution is $y(x) = x$. This is the extremal. Since the functional $J[y] = \\int_{0}^{1} \\frac{1}{2}(y''(x))^2 dx \\ge 0$, and for $y(x)=x$ we have $y''(x)=0$ so $J[x]=0$, this extremal is the absolute minimizer.", "answer": "$$\\boxed{x}$$", "id": "2691366"}]}