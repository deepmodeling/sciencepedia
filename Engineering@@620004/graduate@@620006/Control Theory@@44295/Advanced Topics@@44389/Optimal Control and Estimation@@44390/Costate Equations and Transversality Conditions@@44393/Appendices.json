{"hands_on_practices": [{"introduction": "Transversality conditions can often appear as a set of rules to be memorized. This exercise demystifies them by guiding you back to first principles, connecting the algebraic conditions of optimality to the underlying geometry of the terminal constraints. By considering a terminal state confined to a unit sphere, you will derive the condition that the terminal costate vector, $\\lambda(T)$, must be orthogonal to the tangent space of this constraint manifold, providing a deep, intuitive understanding of its origin [@problem_id:2698222].", "problem": "Consider a finite-horizon optimal control problem with state $x(t) \\in \\mathbb{R}^{n}$ and control $u(t) \\in \\mathbb{R}^{m}$ governed by a smooth control-affine system $\\dot{x}(t) = f(x(t),u(t),t)$ on the interval $t \\in [0,T]$, where $T0$ is fixed. The performance index is a pure integral cost $J[u(\\cdot)] = \\int_{0}^{T} L(x(t),u(t),t)\\, \\mathrm{d}t$, where $L$ and $f$ are continuously differentiable in all arguments. The initial state $x(0)$ is fixed, and the terminal state $x(T)$ is constrained to lie on the unit sphere defined by $\\|x(T)\\| = 1$. There is no terminal cost and no inequality endpoint constraints. Assume all standard regularity and qualification conditions required by the Pontryagin Maximum Principle (PMP) hold, including the existence of an absolutely continuous costate $ \\lambda(t) \\in \\mathbb{R}^{n}$ and a maximized Hamiltonian.\n\nStarting from the fundamental variational structure of the PMP and the geometry of endpoint constraints, derive the geometric transversality condition at the terminal time $T$ associated with the equality constraint $\\|x(T)\\|=1$ by characterizing the tangent space of the constraint manifold and enforcing the vanishing of the boundary term for all admissible endpoint variations. Compute the admissible form of the terminal costate $ \\lambda(T)$ implied by this geometric transversality condition, expressing your final answer solely in terms of $x(T)$ and a scalar Lagrange multiplier. Provide your final answer as a single closed-form symbolic expression with no units and no numerical approximation required.", "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed, scientifically grounded problem in the field of optimal control theory, containing all necessary information for a rigorous derivation.\n\nThe objective is to minimize the performance index $J[u(\\cdot)] = \\int_{0}^{T} L(x(t),u(t),t)\\, \\mathrm{d}t$ for a system governed by $\\dot{x}(t) = f(x(t),u(t),t)$. The initial state $x(0)$ is fixed, and the terminal state $x(T)$ is constrained to lie on a specific manifold, the unit sphere in $\\mathbb{R}^{n}$. This terminal constraint is given by the equality $g(x(T)) = 0$, where $g(x) = \\|x\\|^2 - 1 = x^T x - 1$. There is no terminal cost component in the performance index.\n\nAccording to the Pontryagin Maximum Principle (PMP), we define the Hamiltonian $H(x, u, \\lambda, t) = L(x, u, t) + \\lambda^T f(x, u, t)$. The application of the calculus of variations to the cost functional, coupled with the system dynamics, yields the necessary conditions for optimality. The first variation of the cost functional, after incorporating the costate dynamics $\\dot{\\lambda}^T = -\\frac{\\partial H}{\\partial x}$ and the optimality condition $\\frac{\\partial H}{\\partial u} = 0$, reduces to the boundary terms:\n$$\n\\delta J = \\lambda(T)^T \\delta x(T) - \\lambda(0)^T \\delta x(0)\n$$\nwhere $\\delta x(t)$ is the variation of the state trajectory.\n\nThe initial state $x(0)$ is specified as fixed. Therefore, any admissible variation must satisfy $\\delta x(0) = 0$. This simplifies the boundary term condition to:\n$$\n\\delta J = \\lambda(T)^T \\delta x(T)\n$$\nFor an optimal trajectory, the first variation of the cost $\\delta J$ must be non-negative for any admissible control perturbation. In the context of terminal constraints, this implies that the boundary term must vanish for all *admissible* variations of the terminal state, $\\delta x(T)$. Thus, we must have:\n$$\n\\lambda(T)^T \\delta x(T) = 0\n$$\nAn admissible variation $\\delta x(T)$ is one that is consistent with the terminal constraint. The terminal state $x(T)$ is constrained to lie on the manifold $\\mathcal{S} = \\{x \\in \\mathbb{R}^{n} \\mid \\|x\\| = 1 \\}$. Any infinitesimal variation $\\delta x(T)$ from a point $x(T)$ on this manifold must lie in the tangent space to the manifold at that point, which we denote by $T_{x(T)}\\mathcal{S}$.\n\nThe manifold $\\mathcal{S}$ is defined as a level set of the function $g(x) = x^T x - 1$. The tangent space $T_{x(T)}\\mathcal{S}$ is the set of all vectors that are orthogonal to the gradient of the defining function $g(x)$ at the point $x(T)$. The gradient of $g(x)$ is:\n$$\n\\nabla g(x) = \\frac{\\partial}{\\partial x} (x^T x - 1) = 2x\n$$\nTherefore, the tangent space at $x(T)$ is the set of all vectors $\\delta x(T)$ that satisfy the condition:\n$$\n(\\nabla g(x(T)))^T \\delta x(T) = 0\n$$\nSubstituting the expression for the gradient, we get:\n$$\n(2x(T))^T \\delta x(T) = 0 \\implies x(T)^T \\delta x(T) = 0\n$$\nThis equation geometrically states that any admissible variation $\\delta x(T)$ must be orthogonal to the position vector $x(T)$ itself. This is an intuitive result for a sphere centered at the origin: the tangent plane at any point is orthogonal to the radius vector to that point.\n\nThe transversality condition requires that $\\lambda(T)^T \\delta x(T) = 0$ for all vectors $\\delta x(T)$ belonging to the tangent space $T_{x(T)}\\mathcal{S}$. In other words, $\\lambda(T)^T \\delta x(T) = 0$ for all $\\delta x(T)$ such that $x(T)^T \\delta x(T) = 0$.\n\nThis is a fundamental result from linear algebra. If a vector $\\lambda(T)$ is orthogonal to every vector in a subspace (the tangent space $T_{x(T)}\\mathcal{S}$), then $\\lambda(T)$ must lie in the orthogonal complement of that subspace. The orthogonal complement of the tangent space is the normal space to the manifold at $x(T)$.\n\nThe normal space to the manifold $\\mathcal{S}$ at $x(T)$ is the space spanned by the gradient vector $\\nabla g(x(T))$. In this case, the normal space is a one-dimensional space spanned by the vector $2x(T)$, or equivalently, by the vector $x(T)$.\n$$\n\\text{Normal Space} = \\text{span}\\{\\nabla g(x(T))\\} = \\text{span}\\{x(T)\\}\n$$\nTherefore, the terminal costate vector $\\lambda(T)$ must be an element of this normal space. This implies that $\\lambda(T)$ must be collinear with the terminal state vector $x(T)$. We can express this relationship as:\n$$\n\\lambda(T) = \\nu \\, x(T)\n$$\nwhere $\\nu$ is a scalar constant of proportionality. This scalar, $\\nu$, is interpreted as a Lagrange multiplier associated with the terminal equality constraint $g(x(T))=0$. Its specific value is not determined by the transversality condition alone but would be found by solving the complete two-point boundary value problem.\n\nThis expression provides the form of the terminal costate $\\lambda(T)$ dictated by the geometric transversality condition associated with the constraint that the final state lies on the unit sphere.", "answer": "$$\n\\boxed{\\nu \\, x(T)}\n$$", "id": "2698222"}, {"introduction": "Many real-world optimization problems require finding not just the optimal path, but also the optimal duration. This practice addresses such scenarios by tackling a problem with a free terminal time, $T$. You will apply the corresponding transversality condition, which dictates that the Hamiltonian must be zero along the optimal trajectory, to explicitly solve for the optimal time needed for a minimum-fuel transfer [@problem_id:2698226]. This is a crucial skill for analyzing time-optimal and resource-optimal control strategies.", "problem": "Consider the minimum-fuel transfer for the scalar control-affine system with bounded control\n$$\\dot{x}(t) = u(t), \\quad |u(t)| \\leq U,$$\nwith fixed initial condition\n$$x(0) = x_{0} \\quad \\text{with} \\quad x_{0}  0,$$\nand fixed terminal state\n$$x(T) = 0,$$\nwhere the terminal time $T$ is free and all quantities are dimensionless. The running cost is a smooth approximation of the $L_{1}$-norm,\n$$L_{\\varepsilon}(u) = \\sqrt{u(t)^{2} + \\varepsilon^{2}}, \\quad \\varepsilon  0.$$\nThe performance index to be minimized is\n$$J_{\\varepsilon}[u, T] = \\int_{0}^{T} \\sqrt{u(t)^{2} + \\varepsilon^{2}} \\, dt.$$\nUsing only first principles and core definitions of the Pontryagin Maximum Principle (PMP), derive the necessary conditions of optimality, including the adjoint (costate) dynamics and the appropriate transversality condition associated with the free terminal time. Then, determine the optimal terminal time $T^{\\star}$ by enforcing the transversality condition at the optimum together with the state dynamics and control constraint. Your final answer must be a single, closed-form analytic expression for $T^{\\star}$ in terms of $x_{0}$ and $U$.", "solution": "The problem is to find the optimal terminal time $T^{\\star}$ for a scalar system with dynamics $\\dot{x}(t) = u(t)$ and bounded control $|u(t)| \\leq U$. The state must be transferred from the initial condition $x(0) = x_{0}$ where $x_{0}  0$ to the fixed terminal state $x(T) = 0$, where the terminal time $T$ is free. The objective is to minimize the performance index $J_{\\varepsilon}[u, T] = \\int_{0}^{T} \\sqrt{u(t)^{2} + \\varepsilon^{2}} \\, dt$, where $\\varepsilon  0$. We will use the Pontryagin Maximum Principle (PMP) to derive the necessary conditions for optimality.\n\nFirst, we define the Hamiltonian, $H$. For a normal extremal, the Hamiltonian is given by the sum of the running cost and the inner product of the costate vector and the state dynamics function. Here, the state is $x$, the control is $u$, the costate is $p$, the running cost is $L(u) = \\sqrt{u^{2} + \\varepsilon^{2}}$, and the state dynamics are $f(x, u) = u$.\n$$H(x, p, u) = L(u) + p \\cdot f(x, u) = \\sqrt{u^{2} + \\varepsilon^{2}} + p u$$\n\nAccording to the PMP, for an optimal control $u^{\\star}(t)$ and corresponding state trajectory $x^{\\star}(t)$, there must exist a costate trajectory $p^{\\star}(t)$ such that the following necessary conditions are satisfied:\n\n1.  The state equation: $\\dot{x}^{\\star}(t) = \\frac{\\partial H}{\\partial p} = u^{\\star}(t)$. This is the given system dynamics.\n\n2.  The costate (adjoint) equation: The dynamics of the costate are given by $\\dot{p}^{\\star}(t) = -\\frac{\\partial H}{\\partial x}$.\n    $$\\dot{p}^{\\star}(t) = -\\frac{\\partial}{\\partial x} \\left(\\sqrt{(u^{\\star}(t))^{2} + \\varepsilon^{2}} + p^{\\star}(t) u^{\\star}(t)\\right) = 0$$\n    This implies that the costate is constant along the optimal trajectory: $p^{\\star}(t) = p_{c}$ for some constant $p_{c}$.\n\n3.  The Hamiltonian minimization condition: For almost every $t \\in [0, T^{\\star}]$, the optimal control $u^{\\star}(t)$ must minimize the Hamiltonian over the set of admissible controls, $U_{adm} = \\{u \\in \\mathbb{R} : |u| \\leq U\\}$.\n    $$u^{\\star}(t) = \\arg\\min_{|u| \\leq U} H(x^{\\star}(t), p_{c}, u) = \\arg\\min_{|u| \\leq U} \\left(\\sqrt{u^{2} + \\varepsilon^{2}} + p_{c} u\\right)$$\n\n4.  The transversality condition for a free terminal time $T$: For a problem with a fixed terminal state and a free terminal time, the Hamiltonian evaluated at the terminal time along the optimal trajectory must be zero.\n    $$H(x^{\\star}(T^{\\star}), p_{c}, u^{\\star}(T^{\\star})) = 0$$\n    Since the Hamiltonian does not explicitly depend on time, and both $\\dot{x}^{\\star}$ and $\\dot{p}^{\\star}$ are functions only of $x, p, u$, the Hamiltonian is constant along the optimal trajectory. Therefore, $H(t) = 0$ for all $t \\in [0, T^{\\star}]$.\n\nNow, we analyze the Hamiltonian minimization to determine the optimal control $u^{\\star}$. Let $h(u) = \\sqrt{u^{2} + \\varepsilon^{2}} + p_{c} u$. This function is strictly convex since its second derivative, $h''(u) = \\frac{\\varepsilon^{2}}{(u^{2}+\\varepsilon^{2})^{3/2}}$, is strictly positive for $\\varepsilon  0$. The minimum of $h(u)$ over the compact interval $[-U, U]$ must exist and is either at a point where the derivative is zero (an interior solution) or at the boundaries $u = \\pm U$.\n\nLet us first consider the case of an interior (unsaturated) solution, where $|u^{\\star}|  U$. This requires the derivative of $h(u)$ to be zero at $u=u^{\\star}$:\n$$\\frac{\\partial h}{\\partial u} \\bigg|_{u=u^{\\star}} = \\frac{u^{\\star}}{\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}}} + p_{c} = 0 \\quad (1)$$\nThis implies $p_{c} = -\\frac{u^{\\star}}{\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}}}$. From this, we see that $|p_{c}| = \\frac{|u^{\\star}|}{\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}}}  1$.\nNow we apply the transversality condition, $H=0$:\n$$\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}} + p_{c} u^{\\star} = 0 \\quad (2)$$\nSubstituting $p_c$ from $(1)$ into $(2)$:\n$$\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}} + \\left(-\\frac{u^{\\star}}{\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}}}\\right) u^{\\star} = 0$$\n$$\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}} = \\frac{(u^{\\star})^{2}}{\\sqrt{(u^{\\star})^{2} + \\varepsilon^{2}}}$$\n$$(u^{\\star})^{2} + \\varepsilon^{2} = (u^{\\star})^{2}$$\n$$\\varepsilon^{2} = 0$$\nThis contradicts the given condition $\\varepsilon  0$. Therefore, an unsaturated optimal control is not possible. The optimal control must be at the boundary of the admissible set.\n\nThe optimal control must be $u^{\\star}(t) = U$ or $u^{\\star}(t) = -U$. Since the state must be driven from $x(0) = x_{0}  0$ to $x(T) = 0$, the velocity $\\dot{x} = u$ must be negative on average. As $p_c$ is constant, the control $u^{\\star}$ that minimizes the Hamiltonian is also constant. Thus, the optimal control must be $u^{\\star}(t) = -U$ for all $t \\in [0, T^{\\star}]$.\n\nNow we enforce this control strategy on the system dynamics and boundary conditions to find the optimal time $T^{\\star}$.\n$$\\dot{x}(t) = -U$$\nIntegrating from $t=0$ to $t=T^{\\star}$:\n$$\\int_{0}^{T^{\\star}} \\dot{x}(t) \\, dt = \\int_{0}^{T^{\\star}} -U \\, dt$$\n$$x(T^{\\star}) - x(0) = -U T^{\\star}$$\nSubstituting the boundary conditions $x(0) = x_{0}$ and $x(T^{\\star}) = 0$:\n$$0 - x_{0} = -U T^{\\star}$$\n$$T^{\\star} = \\frac{x_{0}}{U}$$\n\nFinally, we must verify that this solution is consistent with all the necessary conditions of the PMP. The control $u^{\\star} = -U$ must be the minimizer of $H(u)$ for the value of $p_c$ determined by the transversality condition.\nThe transversality condition $H=0$ with $u^{\\star} = -U$ gives:\n$$H = \\sqrt{(-U)^{2} + \\varepsilon^{2}} + p_{c}(-U) = 0$$\n$$\\sqrt{U^{2} + \\varepsilon^{2}} - p_{c} U = 0$$\n$$p_{c} = \\frac{\\sqrt{U^{2} + \\varepsilon^{2}}}{U}$$\nFor $u^{\\star} = -U$ to be the minimizer of the convex function $h(u)$, the derivative at $u=-U$ must be non-negative:\n$$h'(-U) = \\frac{-U}{\\sqrt{(-U)^{2} + \\varepsilon^{2}}} + p_{c} \\geq 0$$\n$$p_{c} \\geq \\frac{U}{\\sqrt{U^{2} + \\varepsilon^{2}}}$$\nSubstituting the value of $p_c$ we found:\n$$\\frac{\\sqrt{U^{2} + \\varepsilon^{2}}}{U} \\geq \\frac{U}{\\sqrt{U^{2} + \\varepsilon^{2}}}$$\nMultiplying both sides by $U\\sqrt{U^{2} + \\varepsilon^{2}}$ (a positive quantity, since $U0$ is required to solve the problem and $\\varepsilon0$):\n$$(\\sqrt{U^{2} + \\varepsilon^{2}})^{2} \\geq U^{2}$$\n$$U^{2} + \\varepsilon^{2} \\geq U^{2}$$\n$$\\varepsilon^{2} \\geq 0$$\nThis inequality is true since $\\varepsilon0$. All the necessary conditions of the PMP are satisfied by the control $u^{\\star}(t)=-U$ with the terminal time $T^{\\star} = x_0/U$. This is the unique candidate for the optimal solution.", "answer": "$$\\boxed{\\frac{x_{0}}{U}}$$", "id": "2698226"}, {"introduction": "The Pontryagin Maximum Principle provides a powerful theoretical framework, but its application often results in a two-point boundary value problem that cannot be solved by hand. This final practice bridges the gap between theory and computation by outlining the architecture of a shooting method, a fundamental numerical technique in optimal control. You will explore how to treat the initial costate, $\\lambda(0)$, as the unknown in a root-finding algorithm, iteratively adjusting it to satisfy the terminal conditions, thus translating abstract necessary conditions into a concrete, powerful algorithm for finding solutions [@problem_id:2698225].", "problem": "You are asked to implement, analyze, and test a Newton-type shooting method that updates the initial costate to enforce a terminal equality constraint in a finite-horizon optimal control problem. The method must be derived from first principles using Pontryagin’s Minimum Principle (PMP), the adjoint (costate) equations, and a sensitivity analysis of the terminal constraint with respect to the initial costate. The implementation must use numerical integration of the corresponding ordinary differential equations (ODEs) and iteratively update the initial costate using Newton’s method with a computed Jacobian from variational (sensitivity) equations.\n\nConsider the following finite-horizon optimal control problem:\n- Minimize the integral cost\n$$\nJ(u) \\;=\\; \\int_{0}^{T} \\tfrac{1}{2}\\,\\big(x(t)^{\\top} Q\\,x(t) \\;+\\; u(t)^{\\top} R \\, u(t)\\big)\\, dt,\n$$\nsubject to the linear time-invariant dynamics\n$$\n\\dot{x}(t) \\;=\\; A\\,x(t) \\;+\\; B\\,u(t), \\quad x(0) \\;=\\; x_{0},\n$$\nand the terminal equality constraint\n$$\n\\psi\\big(x(T)\\big) \\;=\\; C\\,x(T) \\;-\\; x_{T} \\;=\\; 0.\n$$\nHere, $x(t) \\in \\mathbb{R}^{n}$, $u(t) \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $Q \\in \\mathbb{R}^{n \\times n}$ is positive semidefinite, $R \\in \\mathbb{R}^{m \\times m}$ is positive definite, $C \\in \\mathbb{R}^{r \\times n}$, $x_{0} \\in \\mathbb{R}^{n}$ is given, $x_{T} \\in \\mathbb{R}^{r}$ is given, and $T  0$ is fixed. Assume $m = 1$ and $R$ is a scalar (i.e., $R \\in \\mathbb{R}$ with $R  0$).\n\nFundamental base and derivation requirements:\n- Use Pontryagin’s Minimum Principle (PMP). Define the Hamiltonian\n$$\nH(x,u,\\lambda) \\;=\\; \\tfrac{1}{2}\\,\\big(x^{\\top} Q\\,x \\;+\\; u^{\\top} R\\,u\\big) \\;+\\; \\lambda^{\\top}(A x + B u),\n$$\nwith costate $\\lambda(t) \\in \\mathbb{R}^{n}$. The PMP yields the following necessary conditions:\n    - State equation: $\\dot{x}(t) \\;=\\; A\\,x(t) \\;+\\; B\\,u(t)$ with $x(0)=x_{0}$.\n    - Costate equation: $\\dot{\\lambda}(t) \\;=\\; -Q\\,x(t) \\;-\\; A^{\\top}\\lambda(t)$.\n    - Optimal control (first-order condition): $\\partial H/\\partial u \\;=\\; R u \\;+\\; B^{\\top}\\lambda \\;=\\; 0$, hence $u(t) \\;=\\; -R^{-1} B^{\\top}\\lambda(t)$.\n    - Terminal transversality for equality constraint: there exists a Lagrange multiplier $\\nu \\in \\mathbb{R}^{r}$ such that $\\lambda(T) \\;=\\; C^{\\top}\\nu$ and $\\psi\\big(x(T)\\big)=0$.\n- To implement a single-shooting Newton method on the initial costate, treat $\\lambda(0)$ as the decision vector. For any guess $\\lambda(0)$, integrate the coupled state–costate ODE forward in time using $u(t) = -R^{-1} B^{\\top}\\lambda(t)$ to obtain $x(T)$ and the terminal residual\n$$\nr\\big(\\lambda(0)\\big) \\;=\\; \\psi\\big(x(T)\\big) \\;=\\; C\\,x(T) \\;-\\; x_{T}.\n$$\n- Derive and integrate the variational (sensitivity) ODEs with respect to $\\lambda(0)$ to obtain the Jacobian needed for Newton’s method. Let $S(t) = \\dfrac{\\partial x(t)}{\\partial \\lambda(0)} \\in \\mathbb{R}^{n \\times n}$ and $M(t) = \\dfrac{\\partial \\lambda(t)}{\\partial \\lambda(0)} \\in \\mathbb{R}^{n \\times n}$. Using the definitions of $f(x,\\lambda) = A x + B u$ and $u(\\lambda) = -R^{-1} B^{\\top}\\lambda$, and the costate dynamics, derive the linear time-varying matrix ODEs\n$$\n\\begin{aligned}\n\\dot{S}(t) \\;=\\; A\\,S(t) \\;-\\; B\\,R^{-1} B^{\\top}\\,M(t), \\\\\n\\dot{M}(t) \\;=\\; -Q\\,S(t) \\;-\\; A^{\\top} M(t),\n\\end{aligned}\n$$\nwith initial conditions $S(0) = 0$ and $M(0)=I_{n}$. The terminal sensitivity of the constraint is then\n$$\nJ\\big(\\lambda(0)\\big) \\;=\\; \\frac{\\partial \\psi(x(T))}{\\partial \\lambda(0)} \\;=\\; C\\,S(T).\n$$\n- Implement Newton’s method on $\\lambda(0)$ using the update\n$$\n\\lambda(0) \\leftarrow \\lambda(0) \\;-\\; \\Delta, \\quad \\text{where } J\\,\\Delta \\;=\\; r.\n$$\nIf $J$ is not square or not invertible, solve in the least-squares sense. Iterate until $\\lVert r \\rVert_{2}$ is below a specified tolerance or until a maximum number of iterations is reached.\n\nProgramming task:\n- Implement a program that, for each given test case, performs the above Newton-type shooting method:\n    - Numerically integrate the coupled ODE system for $\\big(x(t),\\lambda(t),S(t),M(t)\\big)$ over $[0,T]$.\n    - Compute the terminal residual $r$ and Jacobian $J$.\n    - Update $\\lambda(0)$ via a linear solve in the Newton step.\n    - Repeat until convergence.\n- Set the stopping tolerance to $\\varepsilon = 10^{-9}$ in Euclidean norm for $r$, with a maximum of $10$ iterations.\n- Initialize $\\lambda(0)$ to the zero vector.\n- Use absolute and relative ODE solver tolerances of $10^{-10}$.\n\nTest suite:\nProvide the numerical results for the following four cases. In each case, $n = 2$, $m = 1$.\n\n- Case $1$ (happy path, square terminal constraint):\n    - $A = \\begin{bmatrix} 0  1 \\\\ -1  -\\tfrac{1}{2}\\end{bmatrix}$,\n      $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$,\n      $Q = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n      $R = 1$,\n      $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n      $x_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n      $x_{T} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n      $T = 2$.\n- Case $2$ (underdetermined terminal constraint, least-squares Newton step):\n    - $A = \\begin{bmatrix} 0  1 \\\\ -1  -\\tfrac{1}{2}\\end{bmatrix}$,\n      $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$,\n      $Q = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n      $R = 1$,\n      $C = \\begin{bmatrix} 1  0 \\end{bmatrix}$,\n      $x_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n      $x_{T} = \\begin{bmatrix} 0 \\end{bmatrix}$,\n      $T = 2$.\n- Case $3$ (double integrator, square terminal constraint):\n    - $A = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix}$,\n      $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$,\n      $Q = \\begin{bmatrix} 1  0 \\\\ 0  0.1 \\end{bmatrix}$,\n      $R = 0.1$,\n      $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n      $x_{0} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$,\n      $x_{T} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n      $T = 1.5$.\n- Case $4$ (different damping and constraint on a single component):\n    - $A = \\begin{bmatrix} 0  1 \\\\ -2  -0.2 \\end{bmatrix}$,\n      $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$,\n      $Q = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n      $R = 10$,\n      $C = \\begin{bmatrix} 0  1 \\end{bmatrix}$,\n      $x_{0} = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix}$,\n      $x_{T} = \\begin{bmatrix} 0 \\end{bmatrix}$,\n      $T = 1.2$.\n\nRequired output:\n- For each case, after the Newton iterations terminate, return the Euclidean norm $\\lVert r \\rVert_{2}$ of the terminal residual.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each residual rounded to $10$ decimal places, in the order of Cases $1$ through $4$ (for example, $[0.0,0.0,0.0,0.0]$ would indicate exact satisfaction within rounding).\n\nConstraints and notes:\n- Angles do not appear; no angle unit is needed.\n- There are no explicit physical units to report.\n- Use numerically stable linear algebra operations for the Newton step (least-squares for possibly non-square $J$).\n- The implementation must be self-contained, require no input, and print exactly one line in the specified format.", "solution": "The problem statement has been subjected to rigorous validation and is found to be valid. It is scientifically grounded in the principles of optimal control theory, specifically Pontryagin's Minimum Principle for a linear-quadratic system with terminal constraints. The problem is well-posed, objective, and provides all necessary information to construct a unique and meaningful numerical solution. No logical contradictions, factual unsoundness, or ambiguities are present. We may therefore proceed with the derivation and implementation of the solution.\n\nThe problem requires solving a finite-horizon optimal control problem defined by the minimization of a quadratic cost functional:\n$$\nJ(u) = \\int_{0}^{T} \\tfrac{1}{2}\\,\\big(x(t)^{\\top} Q\\,x(t) + u(t)^{\\top} R \\, u(t)\\big)\\, dt\n$$\nsubject to linear time-invariant dynamics:\n$$\n\\dot{x}(t) = A\\,x(t) + B\\,u(t), \\quad x(0) = x_{0}\n$$\nand a terminal equality constraint:\n$$\n\\psi\\big(x(T)\\big) = C\\,x(T) - x_{T} = 0\n$$\nHere, $x(t) \\in \\mathbb{R}^{n}$, $u(t) \\in \\mathbb{R}^{m}$, and all matrices and vectors have compatible dimensions. We are given $n=2$ and $m=1$.\n\nAccording to Pontryagin's Minimum Principle, we introduce the Hamiltonian $H(x, u, \\lambda)$:\n$$\nH(x, u, \\lambda) = \\tfrac{1}{2}\\,\\big(x^{\\top} Q\\,x + u^{\\top} R\\,u\\big) + \\lambda^{\\top}(A x + B u)\n$$\nwhere $\\lambda(t) \\in \\mathbb{R}^{n}$ is the costate vector. The necessary conditions for optimality are:\n1.  State Equation: $\\dot{x} = \\nabla_{\\lambda}H = Ax + Bu$.\n2.  Costate Equation: $\\dot{\\lambda} = -\\nabla_{x}H = -Qx - A^{\\top}\\lambda$.\n3.  Optimality Condition: $\\nabla_{u}H = Ru + B^{\\top}\\lambda = 0$. Since $R$ is positive definite, this gives the optimal control law in terms of the costate:\n    $$\n    u^{*}(t) = -R^{-1}B^{\\top}\\lambda(t)\n    $$\n4.  Transversality Condition: For the terminal constraint $\\psi(x(T))=0$, there must exist a vector of Lagrange multipliers $\\nu \\in \\mathbb{R}^{r}$ such that $\\lambda(T) = \\frac{\\partial}{\\partial x(T)} [\\nu^\\top \\psi(x(T))]^\\top = (C^\\top \\nu)^\\top{}^\\top = C^\\top \\nu$.\n\nSubstituting the optimal control $u^*(t)$ into the state and costate equations yields a coupled system of linear ordinary differential equations (ODEs):\n$$\n\\begin{aligned}\n\\dot{x}(t) = A\\,x(t) - B\\,R^{-1}B^{\\top}\\lambda(t) \\\\\n\\dot{\\lambda}(t) = -Q\\,x(t) - A^{\\top}\\lambda(t)\n\\end{aligned}\n$$\nThis can be written in block matrix form:\n$$\n\\frac{d}{dt}\\begin{pmatrix} x(t) \\\\ \\lambda(t) \\end{pmatrix} = \\begin{pmatrix} A  -BR^{-1}B^{\\top} \\\\ -Q  -A^{\\top} \\end{pmatrix} \\begin{pmatrix} x(t) \\\\ \\lambda(t) \\end{pmatrix}\n$$\nThis system constitutes a two-point boundary value problem (TPBVP) with the boundary conditions $x(0) = x_0$ and the terminal condition $C x(T) - x_T = 0$. The initial costate $\\lambda(0)$ is unknown.\n\nThe shooting method transforms this TPBVP into an initial value problem. We treat the unknown initial costate $\\lambda(0)$ as a decision variable. For a given guess of $\\lambda(0)$, we can integrate the ODE system forward in time from $t=0$ to $t=T$. This yields a terminal state $x(T)$ that is a function of the initial guess, i.e., $x(T; \\lambda(0))$. The problem is then to find $\\lambda(0)$ such that the terminal constraint is satisfied. This is a root-finding problem for the residual function $r(\\lambda(0))$:\n$$\nr\\big(\\lambda(0)\\big) = C\\,x(T; \\lambda(0)) - x_{T} = 0\n$$\nWe employ Newton's method to solve this nonlinear system of equations for $\\lambda(0)$. The iterative update is:\n$$\n\\lambda(0)_{k+1} = \\lambda(0)_{k} - \\Delta_k\n$$\nwhere $\\Delta_k$ is the solution to the linear system $J_k \\Delta_k = r_k$. Here, $r_k = r(\\lambda(0)_k)$ is the residual at iteration $k$, and $J_k$ is the Jacobian matrix of the residual function evaluated at $\\lambda(0)_k$:\n$$\nJ(\\lambda(0)) = \\frac{\\partial r(\\lambda(0))}{\\partial \\lambda(0)} = C \\frac{\\partial x(T)}{\\partial \\lambda(0)}\n$$\nTo compute the Jacobian, we need the sensitivity of the terminal state with respect to the initial costate. We define the sensitivity matrices $S(t) = \\frac{\\partial x(t)}{\\partial \\lambda(0)} \\in \\mathbb{R}^{n \\times n}$ and $M(t) = \\frac{\\partial \\lambda(t)}{\\partial \\lambda(0)} \\in \\mathbb{R}^{n \\times n}$. By differentiating the coupled $(x, \\lambda)$ ODE system with respect to $\\lambda(0)$, we obtain the variational (or sensitivity) equations:\n$$\n\\frac{d}{dt}\\begin{pmatrix} S(t) \\\\ M(t) \\end{pmatrix} = \\begin{pmatrix} A  -BR^{-1}B^{\\top} \\\\ -Q  -A^{\\top} \\end{pmatrix} \\begin{pmatrix} S(t) \\\\ M(t) \\end{pmatrix}\n$$\nThe initial conditions for these sensitivity matrices are derived from the initial conditions of the original system. Since $x(0) = x_0$ is fixed and does not depend on $\\lambda(0)$, its derivative is zero: $S(0) = \\frac{\\partial x(0)}{\\partial \\lambda(0)} = 0$. The initial costate $\\lambda(0)$ is the independent variable, so its derivative with respect to itself is the identity matrix: $M(0) = \\frac{\\partial \\lambda(0)}{\\partial \\lambda(0)} = I_n$.\n\nThe complete numerical procedure is as follows. We form an augmented state vector $Y(t)$ that concatenates the state, costate, and vectorized sensitivity matrices:\n$$\nY(t) = \\begin{pmatrix} x(t) \\\\ \\lambda(t) \\\\ \\text{vec}(S(t)) \\\\ \\text{vec}(M(t)) \\end{pmatrix} \\in \\mathbb{R}^{2n+2n^2}\n$$\nThe dynamics of $Y(t)$ are given by the combined system of four coupled matrix ODEs. The numerical algorithm for the Newton-based shooting method is:\n1.  Initialize the guess for the initial costate, $\\lambda(0)_0 = 0 \\in \\mathbb{R}^n$.\n2.  For each iteration $k = 0, 1, \\dots, \\text{max\\_iter}-1$:\n    a. Construct the initial condition $Y(0)$ for the augmented system using $x_0$, the current guess $\\lambda(0)_k$, $S(0) = 0$, and $M(0) = I_n$.\n    b. Integrate the augmented ODE system from $t=0$ to $t=T$ using a numerical solver (e.g., Runge-Kutta) with high precision.\n    c. From the terminal solution $Y(T)$, extract the terminal state $x(T)$ and the terminal sensitivity matrix $S(T)$.\n    d. Compute the residual $r_k = C\\,x(T) - x_T$ and its Euclidean norm $\\|r_k\\|_2$.\n    e. If $\\|r_k\\|_2$ is below the specified tolerance $\\varepsilon = 10^{-9}$, convergence is achieved, and the iteration terminates.\n    f. Compute the Jacobian $J_k = C\\,S(T)$.\n    g. Solve the linear system $J_k \\Delta_k = r_k$ for the update step $\\Delta_k$. A least-squares solver (`numpy.linalg.lstsq`) is used to robustly handle both square ($r=n$) and non-square ($r \\neq n$) Jacobians.\n    h. Update the initial costate: $\\lambda(0)_{k+1} = \\lambda(0)_{k} - \\Delta_k$.\n3.  After the loop terminates (either by convergence or reaching the maximum number of iterations), the final residual norm $\\|r_k\\|_2$ is reported as the result.\n\nThis procedure is implemented for each of the four test cases provided. The state vector for the ODE solver is a flattened one-dimensional array containing all components of $x$, $\\lambda$, $S$, and $M$. The ODE system function correctly unpacks this vector, computes the derivatives, and returns them as a flattened vector.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the shooting method for each.\n    \"\"\"\n    \n    # Case 1: happy path, square terminal constraint\n    case1 = {\n        \"A\": np.array([[0, 1], [-1, -0.5]]),\n        \"B\": np.array([[0], [1]]),\n        \"Q\": np.array([[1, 0], [0, 1]]),\n        \"R\": 1.0,\n        \"C\": np.array([[1, 0], [0, 1]]),\n        \"x0\": np.array([1, 0]),\n        \"xT\": np.array([0, 0]),\n        \"T\": 2.0\n    }\n\n    # Case 2: underdetermined terminal constraint\n    case2 = {\n        \"A\": np.array([[0, 1], [-1, -0.5]]),\n        \"B\": np.array([[0], [1]]),\n        \"Q\": np.array([[1, 0], [0, 1]]),\n        \"R\": 1.0,\n        \"C\": np.array([[1, 0]]),\n        \"x0\": np.array([1, 0]),\n        \"xT\": np.array([0]),\n        \"T\": 2.0\n    }\n\n    # Case 3: double integrator, square terminal constraint\n    case3 = {\n        \"A\": np.array([[0, 1], [0, 0]]),\n        \"B\": np.array([[0], [1]]),\n        \"Q\": np.array([[1, 0], [0, 0.1]]),\n        \"R\": 0.1,\n        \"C\": np.array([[1, 0], [0, 1]]),\n        \"x0\": np.array([1, -1]),\n        \"xT\": np.array([0, 0]),\n        \"T\": 1.5\n    }\n\n    # Case 4: different damping and single component constraint\n    case4 = {\n        \"A\": np.array([[0, 1], [-2, -0.2]]),\n        \"B\": np.array([[0], [1]]),\n        \"Q\": np.array([[1, 0], [0, 1]]),\n        \"R\": 10.0,\n        \"C\": np.array([[0, 1]]),\n        \"x0\": np.array([0.5, -0.5]),\n        \"xT\": np.array([0]),\n        \"T\": 1.2\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    results = []\n\n    for case in test_cases:\n        final_residual_norm = run_shooting_method(\n            case[\"A\"], case[\"B\"], case[\"Q\"], case[\"R\"], case[\"C\"],\n            case[\"x0\"], case[\"xT\"], case[\"T\"]\n        )\n        results.append(f\"{final_residual_norm:.10f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_shooting_method(A, B, Q, R, C, x0, xT, T):\n    \"\"\"\n    Performs the Newton-type shooting method for a single optimal control problem.\n\n    Args:\n        A, B, Q, R, C: System matrices\n        x0, xT: Initial state and target vector for the terminal constraint\n        T: Final time\n\n    Returns:\n        The Euclidean norm of the final terminal residual.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Since R is a scalar m=1, R_inv is simple.\n    R_inv = 1.0 / R\n    \n    # Pre-compute matrix products for efficiency in the ODE function.\n    BRB_T = B @ B.T * R_inv\n\n    def ode_system(t, y):\n        \"\"\"\n        Defines the augmented ODE system for [x, lambda, S, M].\n        y is a flattened vector of size (2n + 2n^2).\n        \"\"\"\n        # Unpack state vector y\n        x = y[0:n]\n        lam = y[n:2*n]\n        S_flat = y[2*n : 2*n + n*n]\n        M_flat = y[2*n + n*n : 2*n + 2*n*n]\n        \n        S = S_flat.reshape((n, n))\n        M = M_flat.reshape((n, n))\n        \n        # State and Costate ODEs\n        dx_dt = A @ x - BRB_T @ lam\n        dlam_dt = -Q @ x - A.T @ lam\n        \n        # Sensitivity ODEs\n        dS_dt = A @ S - BRB_T @ M\n        dM_dt = -Q @ S - A.T @ M\n        \n        # Pack derivatives into a flat vector for the solver\n        dy_dt = np.concatenate((\n            dx_dt,\n            dlam_dt,\n            dS_dt.flatten(),\n            dM_dt.flatten()\n        ))\n        return dy_dt\n\n    # Newton's method settings\n    lambda0 = np.zeros(n)\n    max_iter = 10\n    conv_tol = 1e-9\n    ode_tol = 1e-10\n\n    residual_norm = -1.0\n\n    for i in range(max_iter):\n        # Set initial conditions for the augmented system\n        S0 = np.zeros((n, n))\n        M0 = np.eye(n)\n        y0 = np.concatenate((\n            x0,\n            lambda0,\n            S0.flatten(),\n            M0.flatten()\n        ))\n        \n        # Integrate the augmented ODEs\n        sol = solve_ivp(\n            ode_system,\n            [0, T],\n            y0,\n            method='RK45', \n            rtol=ode_tol,\n            atol=ode_tol\n        )\n        \n        # Extract terminal values from the integration result\n        y_T = sol.y[:, -1]\n        x_T_sim = y_T[0:n]\n        S_T = y_T[2*n : 2*n + n*n].reshape((n, n))\n        \n        # Compute the terminal residual and its norm\n        residual = C @ x_T_sim - xT\n        residual_norm = np.linalg.norm(residual)\n        \n        # Check for convergence\n        if residual_norm  conv_tol:\n            break\n            \n        # Compute the Jacobian for the Newton step\n        jacobian = C @ S_T\n        \n        # Solve for the Newton update delta: J * delta = r\n        # Use least squares to handle both square and non-square Jacobians\n        delta, _, _, _ = np.linalg.lstsq(jacobian, residual, rcond=None)\n        \n        # Update the initial costate guess\n        lambda0 = lambda0 - delta\n        \n    return residual_norm\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2698225"}]}