{"hands_on_practices": [{"introduction": "This exercise serves as a fundamental workout in dynamic programming. By manually solving a small-scale Markov Decision Process with a finite horizon, you will directly apply the principle of optimality through backward recursion, building the optimal value function $V_t$ and policy $\\pi_t^{\\star}$ from the terminal time step to the initial one. This hands-on calculation is crucial for demystifying the Bellman equation and solidifying your understanding of its core iterative structure.", "problem": "Consider a finite-horizon, time-indexed stochastic control problem modeled as a Markov Decision Process (MDP) with state space $\\mathcal{X}=\\{0,1\\}$, action space $\\mathcal{U}=\\{a,b\\}$, horizon $N=3$ with decision times $t\\in\\{0,1,2\\}$, and a terminal cost at $t=3$. The controlled state evolution is time-homogeneous and Markovian: for each $t$, the next state $x_{t+1}$ depends only on the current state $x_t$ and the applied control $u_t$. The decision-maker aims to minimize the expected sum of stage costs plus terminal cost.\n\nThe transition probabilities are:\n- From $x=0$: under $u=a$, $x_{t+1}=0$ with probability $\\frac{2}{3}$ and $x_{t+1}=1$ with probability $\\frac{1}{3}$; under $u=b$, $x_{t+1}=1$ with probability $1$.\n- From $x=1$: under $u=a$, $x_{t+1}=0$ with probability $\\frac{1}{2}$ and $x_{t+1}=1$ with probability $\\frac{1}{2}$; under $u=b$, $x_{t+1}=1$ with probability $1$.\n\nThe stage cost at time $t\\in\\{0,1,2\\}$ for taking action $u$ in state $x$ is time-invariant and given by $c(x,u)$ with\n- $c(0,a)=0$, $c(0,b)=\\frac{3}{5}$,\n- $c(1,a)=\\frac{3}{2}$, $c(1,b)=0$.\nThe terminal cost at $t=3$ is $g(x)$ with $g(0)=\\frac{7}{10}$ and $g(1)=0$.\n\nStarting from the foundational definition of optimal control for a finite-horizon MDP (expected total cost minimization over admissible policies) and Bellman’s principle of optimality, first derive the backward recursion that characterizes the optimal cost-to-go functions $\\{V_t\\}_{t=0}^{3}$ without appealing to any pre-stated “shortcut” formulas. Then compute explicitly the value functions $V_3$, $V_2$, $V_1$, and $V_0$ for all states $x\\in\\{0,1\\}$, and determine an optimal policy $\\pi_t^{\\star}(x)\\in\\{a,b\\}$ at each time $t\\in\\{0,1,2\\}$ and each state $x\\in\\{0,1\\}$. Finally, verify the optimal substructure (Bellman optimality) by checking that, for each subproblem starting at $(t,x)\\in\\{1,2\\}\\times\\{0,1\\}$, the restriction of the optimal policy from earlier times is indeed optimal for that subproblem by explicitly evaluating the competing actions.\n\nLet the initial condition be $x_0=0$. Report as your final answer the optimal initial expected total cost $V_0(x_0)$ as a single reduced rational number. Do not round; express your answer exactly as a fraction.", "solution": "The problem shall first be validated for scientific soundness and consistency.\n\n### Step 1: Extract Givens\n\n- **State Space**: $\\mathcal{X}=\\{0,1\\}$\n- **Action Space**: $\\mathcal{U}=\\{a,b\\}$\n- **Horizon**: $N=3$, with decision times $t \\in \\{0, 1, 2\\}$\n- **Terminal Time**: $t=3$\n- **Transition Probabilities**, $P(x_{t+1}=y | x_t=x, u_t=u)$:\n  - From $x=0$:\n    - $u=a$: $P(x_{t+1}=0|0,a) = \\frac{2}{3}$, $P(x_{t+1}=1|0,a) = \\frac{1}{3}$\n    - $u=b$: $P(x_{t+1}=1|0,b) = 1$\n  - From $x=1$:\n    - $u=a$: $P(x_{t+1}=0|1,a) = \\frac{1}{2}$, $P(x_{t+1}=1|1,a) = \\frac{1}{2}$\n    - $u=b$: $P(x_{t+1}=1|1,b) = 1$\n- **Stage Cost**, $c(x,u)$:\n  - $c(0,a)=0$, $c(0,b)=\\frac{3}{5}$\n  - $c(1,a)=\\frac{3}{2}$, $c(1,b)=0$\n- **Terminal Cost**, $g(x)$:\n  - $g(0)=\\frac{7}{10}$, $g(1)=0$\n- **Objective**: Minimize the expected total cost.\n- **Initial Condition**: $x_0=0$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is a standard, well-formulated Markov Decision Process (MDP), a cornerstone of stochastic control theory and operations research. All concepts are based on established mathematical principles.\n- **Well-Posed**: The problem is a finite-horizon, finite-state, finite-action MDP. For such problems, an optimal policy and a unique optimal cost-to-go function are guaranteed to exist.\n- **Objective**: The problem statement uses precise, unambiguous mathematical language. The costs and probabilities are given as objective numerical values.\n- **Completeness**: All necessary information (states, actions, horizon, costs, transitions) is provided. The problem is self-contained.\n- **Consistency**: There are no internal contradictions in the provided data.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A solution will be provided.\n\n---\n\nThe objective is to find a policy $\\pi = (\\mu_0, \\mu_1, \\mu_2)$, where each $\\mu_t: \\mathcal{X} \\to \\mathcal{U}$ is a decision rule, that minimizes the expected total cost functional $J_{\\pi}(x_0)$:\n$$\nJ_{\\pi}(x_0) = \\mathbb{E}_{\\pi} \\left[ g(x_3) + \\sum_{t=0}^{2} c(x_t, u_t) \\mid x_0 \\right]\n$$\nwhere $u_t = \\mu_t(x_t)$. The optimal cost-to-go, or value function, at time $t$ from state $x$ is defined as the minimum possible expected cost from time $t$ to the horizon $N=3$:\n$$\nV_t(x) = \\min_{\\mu_t, ..., \\mu_{N-1}} \\mathbb{E} \\left[ g(x_N) + \\sum_{k=t}^{N-1} c(x_k, u_k) \\mid x_t = x \\right]\n$$\nThe optimal cost for the entire problem is $V_0(x_0)$.\n\nBellman's principle of optimality states that an optimal policy has the property that whatever the current state and control action are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the current action. This principle gives rise to a backward recursive relation for the optimal cost-to-go function, known as the Bellman equation.\n\nFor the final time step $t=N=3$, there are no more decisions to make, so the cost-to-go is simply the terminal cost:\n$$\nV_3(x) = g(x)\n$$\nFor any time $t \\in \\{0, 1, 2\\}$, if we are in state $x_t=x$ and choose action $u_t=u$, we incur an immediate stage cost $c(x,u)$ and transition to a new state $x_{t+1}$. If we assume that from time $t+1$ onwards we follow an optimal policy, the expected future cost is $\\mathbb{E}[V_{t+1}(x_{t+1}) \\mid x_t=x, u_t=u]$. To act optimally at time $t$, we must choose the action $u$ that minimizes the sum of the immediate cost and the expected future optimal cost. This establishes the backward recursion:\n$$\nV_t(x) = \\min_{u \\in \\mathcal{U}} \\left\\{ c(x,u) + \\mathbb{E}[V_{t+1}(x_{t+1}) \\mid x_t=x, u_t=u] \\right\\}\n$$\nThe expectation is computed over the probability distribution of the next state $x_{t+1}$:\n$$\n\\mathbb{E}[V_{t+1}(x_{t+1}) \\mid x_t=x, u_t=u] = \\sum_{y \\in \\mathcal{X}} P(x_{t+1}=y \\mid x_t=x, u_t=u) V_{t+1}(y)\n$$\nThe optimal policy at time $t$, $\\pi_t^{\\star}(x) = \\mu_t^{\\star}(x)$, is the action $u$ that achieves this minimum. We now apply this recursion, starting from $t=3$ and working backwards to $t=0$.\n\n**Step `t=3` (Terminal Step)**\nThe cost-to-go is the terminal cost $g(x)$:\n$V_3(0) = g(0) = \\frac{7}{10}$\n$V_3(1) = g(1) = 0$\n\n**Step `t=2` (Backward Recursion)**\nWe compute $V_2(x)$ for each state $x \\in \\{0,1\\}$ using $V_3$.\nFor $x=0$:\n$$\nV_2(0) = \\min \\left\\{\n\\begin{array}{ll}\n\\text{cost for } u=a: & c(0,a) + P(x_3=0|0,a)V_3(0) + P(x_3=1|0,a)V_3(1) \\\\\n\\text{cost for } u=b: & c(0,b) + P(x_3=0|0,b)V_3(0) + P(x_3=1|0,b)V_3(1)\n\\end{array}\n\\right.\n$$\n$$\nV_2(0) = \\min \\left\\{ 0 + \\frac{2}{3} \\cdot \\frac{7}{10} + \\frac{1}{3} \\cdot 0, \\quad \\frac{3}{5} + 0 \\cdot \\frac{7}{10} + 1 \\cdot 0 \\right\\} = \\min \\left\\{ \\frac{14}{30}, \\frac{3}{5} \\right\\} = \\min \\left\\{ \\frac{7}{15}, \\frac{9}{15} \\right\\} = \\frac{7}{15}\n$$\nThe minimum is achieved for $u=a$. Thus, $V_2(0) = \\frac{7}{15}$ and $\\pi_2^{\\star}(0)=a$.\n\nFor $x=1$:\n$$\nV_2(1) = \\min \\left\\{\n\\begin{array}{ll}\n\\text{cost for } u=a: & c(1,a) + P(x_3=0|1,a)V_3(0) + P(x_3=1|1,a)V_3(1) \\\\\n\\text{cost for } u=b: & c(1,b) + P(x_3=0|1,b)V_3(0) + P(x_3=1|1,b)V_3(1)\n\\end{array}\n\\right.\n$$\n$$\nV_2(1) = \\min \\left\\{ \\frac{3}{2} + \\frac{1}{2} \\cdot \\frac{7}{10} + \\frac{1}{2} \\cdot 0, \\quad 0 + 0 \\cdot \\frac{7}{10} + 1 \\cdot 0 \\right\\} = \\min \\left\\{ \\frac{3}{2} + \\frac{7}{20}, 0 \\right\\} = \\min \\left\\{ \\frac{30}{20} + \\frac{7}{20}, 0 \\right\\} = \\min \\left\\{ \\frac{37}{20}, 0 \\right\\} = 0\n$$\nThe minimum is achieved for $u=b$. Thus, $V_2(1) = 0$ and $\\pi_2^{\\star}(1)=b$.\n\n**Step `t=1` (Backward Recursion)**\nWe compute $V_1(x)$ for each state $x \\in \\{0,1\\}$ using $V_2(0)=\\frac{7}{15}$ and $V_2(1)=0$.\nFor $x=0$:\n$$\nV_1(0) = \\min \\left\\{ 0 + \\frac{2}{3}V_2(0) + \\frac{1}{3}V_2(1), \\quad \\frac{3}{5} + 0 \\cdot V_2(0) + 1 \\cdot V_2(1) \\right\\}\n$$\n$$\nV_1(0) = \\min \\left\\{ \\frac{2}{3} \\cdot \\frac{7}{15} + \\frac{1}{3} \\cdot 0, \\quad \\frac{3}{5} + 0 \\right\\} = \\min \\left\\{ \\frac{14}{45}, \\frac{3}{5} \\right\\} = \\min \\left\\{ \\frac{14}{45}, \\frac{27}{45} \\right\\} = \\frac{14}{45}\n$$\nThe minimum is achieved for $u=a$. Thus, $V_1(0) = \\frac{14}{45}$ and $\\pi_1^{\\star}(0)=a$.\n\nFor $x=1$:\n$$\nV_1(1) = \\min \\left\\{ \\frac{3}{2} + \\frac{1}{2}V_2(0) + \\frac{1}{2}V_2(1), \\quad 0 + 0 \\cdot V_2(0) + 1 \\cdot V_2(1) \\right\\}\n$$\n$$\nV_1(1) = \\min \\left\\{ \\frac{3}{2} + \\frac{1}{2} \\cdot \\frac{7}{15} + \\frac{1}{2} \\cdot 0, \\quad 0 \\right\\} = \\min \\left\\{ \\frac{3}{2} + \\frac{7}{30}, 0 \\right\\} = \\min \\left\\{ \\frac{45}{30} + \\frac{7}{30}, 0 \\right\\} = \\min \\left\\{ \\frac{52}{30}, 0 \\right\\} = 0\n$$\nThe minimum is achieved for $u=b$. Thus, $V_1(1) = 0$ and $\\pi_1^{\\star}(1)=b$.\n\n**Step `t=0` (Backward Recursion)**\nWe compute $V_0(x)$ for each state $x \\in \\{0,1\\}$ using $V_1(0)=\\frac{14}{45}$ and $V_1(1)=0$.\nFor $x=0$:\n$$\nV_0(0) = \\min \\left\\{ 0 + \\frac{2}{3}V_1(0) + \\frac{1}{3}V_1(1), \\quad \\frac{3}{5} + 0 \\cdot V_1(0) + 1 \\cdot V_1(1) \\right\\}\n$$\n$$\nV_0(0) = \\min \\left\\{ \\frac{2}{3} \\cdot \\frac{14}{45} + \\frac{1}{3} \\cdot 0, \\quad \\frac{3}{5} + 0 \\right\\} = \\min \\left\\{ \\frac{28}{135}, \\frac{3}{5} \\right\\} = \\min \\left\\{ \\frac{28}{135}, \\frac{81}{135} \\right\\} = \\frac{28}{135}\n$$\nThe minimum is achieved for $u=a$. Thus, $V_0(0) = \\frac{28}{135}$ and $\\pi_0^{\\star}(0)=a$.\n\nFor completeness, we compute $V_0(1)$:\n$$\nV_0(1) = \\min \\left\\{ \\frac{3}{2} + \\frac{1}{2}V_1(0) + \\frac{1}{2}V_1(1), \\quad 0 + 0 \\cdot V_1(0) + 1 \\cdot V_1(1) \\right\\}\n$$\n$$\nV_0(1) = \\min \\left\\{ \\frac{3}{2} + \\frac{1}{2} \\cdot \\frac{14}{45} + \\frac{1}{2} \\cdot 0, \\quad 0 \\right\\} = \\min \\left\\{ \\frac{3}{2} + \\frac{7}{45}, 0 \\right\\} = \\min \\left\\{ \\frac{135}{90} + \\frac{14}{90}, 0 \\right\\} = \\min \\left\\{ \\frac{149}{90}, 0 \\right\\} = 0\n$$\nThe minimum is achieved for $u=b$. Thus, $V_0(1) = 0$ and $\\pi_0^{\\star}(1)=b$.\n\n**Optimal Policy Summary**\nThe optimal policy $\\pi^{\\star} = (\\pi_0^{\\star}, \\pi_1^{\\star}, \\pi_2^{\\star})$ is:\n- $\\pi_0^{\\star}(0) = a, \\quad \\pi_0^{\\star}(1) = b$\n- $\\pi_1^{\\star}(0) = a, \\quad \\pi_1^{\\star}(1) = b$\n- $\\pi_2^{\\star}(0) = a, \\quad \\pi_2^{\\star}(1) = b$\n\n**Verification of Optimal Substructure**\nThe problem requests verification of the optimal substructure. The very method of dynamic programming, by construction, enforces this property. At each step $(t,x)$, we compute $V_t(x)$ by explicitly minimizing the sum of current cost and expected future optimal costs. This ensures that the policy chosen for any subproblem starting at time $t > 0$ is optimal for that subproblem.\nFor example, let's verify that for the subproblem starting at $(t,x) = (2,0)$, the policy $\\pi_2^{\\star}(0)=a$ is indeed optimal. The cost of this action is $c(0,a) + \\mathbb{E}[V_3(x_3)|x_2=0, u_2=a] = \\frac{7}{15}$. The cost of the alternative action $u=b$ is $c(0,b) + \\mathbb{E}[V_3(x_3)|x_2=0, u_2=b] = \\frac{3}{5}$. As $\\frac{7}{15} < \\frac{3}{5}$, the choice $\\pi_2^{\\star}(0)=a$ is optimal for the subproblem starting at $(2,0)$. Our computation of $V_2(0)$ is precisely this verification. The same logic applies to all other states and times. The backward recursion is the algorithmic implementation of the Bellman optimality principle.\n\nThe problem asks for the optimal initial expected total cost starting from $x_0=0$. This is the value $V_0(0)$.\nBased on the calculations, $V_0(0) = \\frac{28}{135}$. This fraction is already in simplest form since $28 = 2^2 \\cdot 7$ and $135 = 3^3 \\cdot 5$.", "answer": "$$\n\\boxed{\\frac{28}{135}}\n$$", "id": "2703371"}, {"introduction": "Building on the fundamentals, this practice transitions to continuous systems and introduces a critical real-world challenge: actuator saturation. You will apply dynamic programming to a system with linear dynamics and quadratic costs, where the control input $u_k$ is constrained, resulting in a non-linear optimal policy. This problem hones the skill of deriving and analyzing the piecewise-analytic structure of the value function that arises from such constraints.", "problem": "Consider the discrete-time, one-dimensional system with actuator saturation\n$$\nx_{k+1} \\;=\\; x_{k} + u_{k}, \\quad k \\in \\{0,1\\},\n$$\nsubject to the hard input constraint\n$$\n|u_{k}| \\;\\le\\; 1 \\quad \\text{for all } k.\n$$\nThe performance index over the finite horizon of length $2$ is the sum of quadratic stage costs plus a terminal cost,\n$$\nJ \\;=\\; \\sum_{k=0}^{1} \\big( x_{k}^{2} + u_{k}^{2} \\big) \\;+\\; x_{2}^{2}.\n$$\nUsing Bellman’s principle of optimality and Dynamic Programming (DP), start from the definition of the value function and derive, from first principles, the Bellman recursion over the horizon. Then, solve the step-$1$ problem to obtain $V_{1}(x_{1})$ and the optimal policy $u_{1}^{\\star}(x_{1})$, carefully handling the input saturation. Next, solve the step-$0$ problem to obtain $V_{0}(x_{0})$ and the optimal policy $u_{0}^{\\star}(x_{0})$, again handling the input saturation consistently with the step-$1$ solution. Identify precisely the regions of the state space where the optimal control saturates at each step, and justify that your solution is globally optimal by convexity arguments that are valid under the given problem data.\n\nProvide as your final answer the exact, closed-form expression of the value function $V_{0}(x_{0})$ as a piecewise analytic function of $x_{0}$, simplified as much as possible. Do not approximate; no rounding is required.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n- System dynamics: $x_{k+1} = x_{k} + u_{k}$, $k \\in \\{0, 1\\}$.\n- State space: $x_k \\in \\mathbb{R}$.\n- Control input constraint: $|u_{k}| \\le 1$.\n- Horizon: $N=2$.\n- Performance index: $J = \\sum_{k=0}^{1} ( x_{k}^{2} + u_{k}^{2} ) + x_{2}^{2}$.\nThis can be written with stage cost $L(x_k, u_k) = x_k^2 + u_k^2$ and terminal cost $\\Phi(x_2) = x_2^2$.\n- Method: Dynamic Programming (DP).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard finite-horizon discrete-time optimal control problem. The system is a linear integrator, the cost function is quadratic (and thus strictly convex in the controls), and the control constraints define a compact, convex set. The problem is scientifically grounded, well-posed, completely specified, and objective. There are no violations of the validation criteria.\n\n**Verdict**\nThe problem is valid. A solution will be provided.\n\n**Derivation of the Solution using Dynamic Programming**\n\nThe solution proceeds by backward induction in time from $k=N=2$ to $k=0$. The value function $V_k(x_k)$ represents the optimal cost-to-go from state $x_k$ at time $k$.\n\nThe Bellman recursion is defined as:\n$$ V_k(x_k) = \\min_{|u_k| \\le 1} \\left\\{ L(x_k, u_k) + V_{k+1}(x_{k+1}) \\right\\}, \\quad k=0, 1 $$\nwith the terminal condition:\n$$ V_2(x_2) = \\Phi(x_2) = x_2^2 $$\n\n**Step for k=1 (Final Stage)**\n\nThe value function at stage $k=1$ is:\n$$ V_1(x_1) = \\min_{|u_1| \\le 1} \\left\\{ x_1^2 + u_1^2 + V_2(x_2) \\right\\} $$\nSubstituting $x_2 = x_1 + u_1$ and $V_2(x_2) = x_2^2$:\n$$ V_1(x_1) = \\min_{|u_1| \\le 1} \\left\\{ x_1^2 + u_1^2 + (x_1 + u_1)^2 \\right\\} $$\nLet $J_1(u_1)$ be the quantity to minimize:\n$$ J_1(u_1) = x_1^2 + u_1^2 + x_1^2 + 2x_1 u_1 + u_1^2 = 2x_1^2 + 2x_1 u_1 + 2u_1^2 $$\nThis is a strictly convex quadratic function of $u_1$. The unconstrained minimum is found by setting the derivative to zero:\n$$ \\frac{\\partial J_1}{\\partial u_1} = 2x_1 + 4u_1 = 0 \\implies u_{1,\\text{unc}} = -\\frac{1}{2}x_1 $$\nThe optimal control $u_1^\\star(x_1)$ is found by projecting this unconstrained solution onto the feasible set $[-1, 1]$:\n$$ u_1^\\star(x_1) = \\text{sat}\\left(-\\frac{1}{2}x_1\\right) = \\begin{cases} 1 & \\text{if } -\\frac{1}{2}x_1 > 1 \\iff x_1 < -2 \\\\ -\\frac{1}{2}x_1 & \\text{if } -1 \\le -\\frac{1}{2}x_1 \\le 1 \\iff |x_1| \\le 2 \\\\ -1 & \\text{if } -\\frac{1}{2}x_1 < -1 \\iff x_1 > 2 \\end{cases} $$\nThe control saturates for $|x_1| \\ge 2$. We now compute the value function $V_1(x_1)$ by substituting $u_1^\\star(x_1)$ into $J_1(u_1)$, which results in a piecewise function:\n\n1.  For $|x_1| \\le 2$ (linear region), $u_1^\\star = -\\frac{1}{2}x_1$:\n    $$ V_1(x_1) = 2x_1^2 + 2x_1\\left(-\\frac{1}{2}x_1\\right) + 2\\left(-\\frac{1}{2}x_1\\right)^2 = 2x_1^2 - x_1^2 + \\frac{1}{2}x_1^2 = \\frac{3}{2}x_1^2 $$\n2.  For $x_1 > 2$ (saturation region), $u_1^\\star = -1$:\n    $$ V_1(x_1) = 2x_1^2 + 2x_1(-1) + 2(-1)^2 = 2x_1^2 - 2x_1 + 2 $$\n3.  For $x_1  -2$ (saturation region), $u_1^\\star = 1$:\n    $$ V_1(x_1) = 2x_1^2 + 2x_1(1) + 2(1)^2 = 2x_1^2 + 2x_1 + 2 $$\n\nThus, the value function at $k=1$ is:\n$$ V_1(x_1) = \\begin{cases} 2x_1^2 + 2x_1 + 2  \\text{if } x_1 \\le -2 \\\\ \\frac{3}{2}x_1^2  \\text{if } -2  x_1  2 \\\\ 2x_1^2 - 2x_1 + 2  \\text{if } x_1 \\ge 2 \\end{cases} $$\nThis function is continuous and convex. Its derivative is also continuous and is given piecewise by:\n$$ V_1'(x_1) = \\begin{cases} 4x_1+2  \\text{if } x_1 \\le -2 \\\\ 3x_1  \\text{if } -2  x_1  2 \\\\ 4x_1-2  \\text{if } x_1 \\ge 2 \\end{cases} $$\n\n**Step for k=0 (Initial Stage)**\n\nThe value function at stage $k=0$ is:\n$$ V_0(x_0) = \\min_{|u_0| \\le 1} \\left\\{ x_0^2 + u_0^2 + V_1(x_0 + u_0) \\right\\} $$\nLet $J_0(u_0) = x_0^2 + u_0^2 + V_1(x_0 + u_0)$. The function $V_1$ is convex, and $u_0 \\mapsto x_0^2+u_0^2$ is strictly convex. Therefore, $J_0(u_0)$ is a strictly convex function of $u_0$. The optimal control $u_0^\\star(x_0)$ is the unique minimizer.\n\nThe unconstrained minimizer $u_{0, \\text{unc}}$ satisfies $\\frac{dJ_0}{du_0} = 0$:\n$$ 2u_0 + V_1'(x_0+u_0) = 0 $$\nThe optimal constrained control is $u_0^\\star = \\text{sat}(u_{0,\\text{unc}})$. The control saturates at $u_0^\\star=-1$ if the derivative $\\frac{dJ_0}{du_0} \\ge 0$ at $u_0=-1$, and at $u_0^\\star=1$ if $\\frac{dJ_0}{du_0} \\le 0$ at $u_0=1$.\n\n- Saturation at $u_0^\\star = -1$: this occurs if $2(-1) + V_1'(x_0-1) \\ge 0 \\implies V_1'(x_0-1) \\ge 2$.\n  Using the expression for $V_1'(x_1)$, we find that $V_1'(x_1) \\ge 2$ when $x_1 \\ge 2/3$. Thus, saturation at $u_0^\\star=-1$ occurs if $x_0-1 \\ge 2/3$, which means $x_0 \\ge 5/3$.\n\n- Saturation at $u_0^\\star = 1$: this occurs if $2(1) + V_1'(x_0+1) \\le 0 \\implies V_1'(x_0+1) \\le -2$.\n  This is true for $x_1 \\le -2/3$. Thus, saturation at $u_0^\\star=1$ occurs if $x_0+1 \\le -2/3$, which means $x_0 \\le -5/3$.\n\n- Linear region: for $|x_0|  5/3$, the control is unsaturated. The optimality condition is based on $x_1 = x_0+u_0$ being in the linear region of $V_1$, i.e., $|x_1|  2$.\n  In this case, $V_1'(x_1) = 3x_1$. The optimality condition $2u_0 + 3(x_0+u_0) = 0$ gives $5u_0+3x_0=0$, so $u_{0,\\text{unc}} = -\\frac{3}{5}x_0$.\n  For $|x_0|  5/3$, $|u_{0,\\text{unc}}| = \\frac{3}{5}|x_0|  1$, so the control is indeed unsaturated.\n  The state at the next step is $x_1 = x_0 + u_0^\\star = x_0 - \\frac{3}{5}x_0 = \\frac{2}{5}x_0$. For $|x_0|  5/3$, we have $|x_1|  \\frac{2}{5} \\cdot \\frac{5}{3} = 2/3  2$, so $x_1$ is indeed in the quadratic region of $V_1$.\n\nThe optimal policy is $u_0^\\star(x_0)$:\n$$ u_0^\\star(x_0) = \\begin{cases} 1  \\text{if } x_0 \\le -5/3 \\\\ -\\frac{3}{5}x_0  \\text{if } -5/3  x_0  5/3 \\\\ -1  \\text{if } x_0 \\ge 5/3 \\end{cases} $$\nWe now compute the value function $V_0(x_0)$ piecewise:\n\n1.  For $|x_0| \\le 5/3$ (linear control region), $u_0^\\star = -3x_0/5$. The next state is $x_1 = 2x_0/5$, which lies in $(-2,2)$.\n    $$ V_0(x_0) = x_0^2 + \\left(-\\frac{3}{5}x_0\\right)^2 + V_1\\left(\\frac{2}{5}x_0\\right) = x_0^2 + \\frac{9}{25}x_0^2 + \\frac{3}{2}\\left(\\frac{2}{5}x_0\\right)^2 $$\n    $$ V_0(x_0) = x_0^2 + \\frac{9}{25}x_0^2 + \\frac{3}{2}\\frac{4}{25}x_0^2 = \\left(1 + \\frac{9}{25} + \\frac{6}{25}\\right)x_0^2 = \\left(\\frac{25+9+6}{25}\\right)x_0^2 = \\frac{40}{25}x_0^2 = \\frac{8}{5}x_0^2 $$\n\n2.  For $x_0 > 5/3$ (saturated control), $u_0^\\star = -1$. The next state is $x_1 = x_0-1 > 2/3$.\n    $$ V_0(x_0) = x_0^2 + (-1)^2 + V_1(x_0-1) = x_0^2 + 1 + V_1(x_0-1) $$\n    The form of $V_1$ depends on whether $x_1 = x_0-1$ is greater or less than $2$.\n\n    - For $5/3 \\le x_0  3$: $x_1=x_0-1 \\in [2/3, 2)$, so $V_1(x_1) = \\frac{3}{2}x_1^2$.\n      $$ V_0(x_0) = x_0^2 + 1 + \\frac{3}{2}(x_0-1)^2 = x_0^2 + 1 + \\frac{3}{2}(x_0^2 - 2x_0 + 1) = \\frac{5}{2}x_0^2 - 3x_0 + \\frac{5}{2} $$\n    - For $x_0 \\ge 3$: $x_1=x_0-1 \\ge 2$, so $V_1(x_1) = 2x_1^2-2x_1+2$.\n      $$ V_0(x_0) = x_0^2 + 1 + 2(x_0-1)^2 - 2(x_0-1) + 2 = x_0^2 + 1 + 2(x_0^2-2x_0+1) - 2x_0 + 2 + 2 = 3x_0^2 - 6x_0 + 7 $$\n\n3.  For $x_0  -5/3$ (saturated control), $u_0^\\star = 1$. By symmetry with the case $x_0>5/3$. The next state is $x_1=x_0+1  -2/3$.\n\n    - For $-3  x_0 \\le -5/3$: $x_1=x_0+1 \\in (-2, -2/3]$, so $V_1(x_1) = \\frac{3}{2}x_1^2$.\n      $$ V_0(x_0) = x_0^2+1+\\frac{3}{2}(x_0+1)^2 = x_0^2+1+\\frac{3}{2}(x_0^2+2x_0+1) = \\frac{5}{2}x_0^2 + 3x_0 + \\frac{5}{2} $$\n    - For $x_0 \\le -3$: $x_1=x_0+1 \\le -2$, so $V_1(x_1) = 2x_1^2+2x_1+2$.\n      $$ V_0(x_0) = x_0^2+1+2(x_0+1)^2+2(x_0+1)+2 = x_0^2+1+2(x_0^2+2x_0+1)+2x_0+2+2 = 3x_0^2+6x_0+7 $$\n\nThe final expression for $V_0(x_0)$ is a continuous, piecewise analytic function composed of five parts.\n\n**Justification of Global Optimality**\n\nThe cost function $J = x_0^2 + u_0^2 + (x_0+u_0)^2 + u_1^2 + (x_0+u_0+u_1)^2$ is a strictly convex function of the control vector $(u_0, u_1)$, as its Hessian matrix $H = \\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix}$ is positive definite. The feasible control set $\\mathcal{U} = \\{(u_0, u_1) \\in \\mathbb{R}^2 \\mid |u_0|\\le 1, |u_1|\\le 1\\}$ is compact and convex. The minimization of a strictly convex function over a compact convex set has a unique global minimum. The DP algorithm correctly finds this minimum. At each stage $k$, the minimization problem for $u_k$ is convex, since the stage cost is convex and the value function $V_{k+1}$ is convex (by induction, starting from the convex terminal cost $V_2(x_2)=x_2^2$). Therefore, the policy derived at each step is optimal for that subproblem. By Bellman's principle of optimality, the composition of these optimal sub-policies yields the globally optimal policy. The derived solution is unique and globally optimal.\n\nThe final expression for the value function $V_0(x_0)$ is:\n$$ V_0(x_0) = \\begin{cases}\n3x_0^2 + 6x_0 + 7  \\text{if } x_0 \\le -3 \\\\\n\\frac{5}{2}x_0^2 + 3x_0 + \\frac{5}{2}  \\text{if } -3  x_0 \\le -5/3 \\\\\n\\frac{8}{5}x_0^2  \\text{if } -5/3  x_0  5/3 \\\\\n\\frac{5}{2}x_0^2 - 3x_0 + \\frac{5}{2}  \\text{if } 5/3 \\le x_0  3 \\\\\n3x_0^2 - 6x_0 + 7  \\text{if } x_0 \\ge 3\n\\end{cases} $$", "answer": "$$\n\\boxed{\nV_0(x_0) = \\begin{cases}\n3x_0^2 + 6x_0 + 7  \\text{if } x_0 \\le -3 \\\\\n\\frac{5}{2}x_0^2 + 3x_0 + \\frac{5}{2}  \\text{if } -3  x_0 \\le -\\frac{5}{3} \\\\\n\\frac{8}{5}x_0^2  \\text{if } -\\frac{5}{3}  x_0  \\frac{5}{3} \\\\\n\\frac{5}{2}x_0^2 - 3x_0 + \\frac{5}{2}  \\text{if } \\frac{5}{3} \\le x_0  3 \\\\\n3x_0^2 - 6x_0 + 7  \\text{if } x_0 \\ge 3\n\\end{cases}\n}\n$$", "id": "2703354"}, {"introduction": "This exercise expands the application of the Bellman principle to a different, yet equally important, class of sequential decision problems: optimal stopping. You will first be guided to derive the general form of the optimal stopping Bellman equation, which involves a choice between terminating the process for a reward $\\psi(x)$ or continuing. You will then apply this framework to determine the optimal stopping region for a specific problem, a key skill in areas from finance to resource management.", "problem": "Consider a discrete-time optimal stopping problem for a real-valued state process $\\{x_{t}\\}_{t \\in \\mathbb{N}_{0}}$ with initial condition $x_{0}=x \\in \\mathbb{R}$. At each time $t$, the decision-maker chooses either to stop and receive an immediate reward, or to continue and receive a running reward plus the option to decide again at the next time. The system evolves according to a time-homogeneous transition law when the decision is to continue. The objective is to maximize the expected total discounted reward. Assume that the discount factor is $\\gamma \\in (0,1)$, the immediate stopping reward is given by a measurable function $\\psi:\\mathbb{R}\\to\\mathbb{R}$, the running reward is given by a measurable function $\\ell:\\mathbb{R}\\to\\mathbb{R}$, and that the transition from $x$ to the next state $x'$ under continuation admits a conditional distribution such that $\\mathbb{E}[\\cdot \\mid x]$ is well-defined.\n\n- Using only the principle of optimality and the tower property of conditional expectation, derive the dynamic programming equation that characterizes the optimal value function $V:\\mathbb{R}\\to\\mathbb{R}$ for this optimal stopping problem. Clearly state any regularity conditions you impose to justify the derivation.\n\n- Now specialize to a one-dimensional example with deterministic linear dynamics under continuation, given by $x_{t+1}=\\lambda x_{t}$ for a fixed $\\lambda \\in \\mathbb{R}$, running reward $\\ell(x)=-q x^{2}$ with $q0$, and stopping reward $\\psi(x)=A - B x^{2}$ with $A0$ and $B0$. Assume $\\gamma \\lambda^{2}  1$ and $B  \\frac{q}{1-\\gamma \\lambda^{2}}$.\n\n  Using your dynamic programming equation, solve for the value function in the continuation region by positing an appropriate functional form that is invariant under the dynamics and rewards. Then determine the threshold $x^{\\star}0$ such that the optimal stopping region is $\\{x \\in \\mathbb{R} : |x| \\leq x^{\\star}\\}$. Express $x^{\\star}$ in closed form in terms of $A$, $B$, $q$, $\\gamma$, and $\\lambda$.\n\n- Finally, evaluate $x^{\\star}$ for the specific parameter values $A=3$, $B=5$, $q=1$, $\\gamma=\\frac{1}{2}$, and $\\lambda=\\frac{1}{2}$. Report only the exact value of $x^{\\star}$ as your final answer. Do not round or approximate.", "solution": "We begin with a rigorous validation of the problem statement.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A discrete-time optimal stopping problem for a real-valued state process $\\{x_{t}\\}_{t \\in \\mathbb{N}_{0}}$ with initial condition $x_{0}=x \\in \\mathbb{R}$.\n- Decision at each time $t$: stop or continue.\n- Discount factor: $\\gamma \\in (0,1)$.\n- Immediate stopping reward function: $\\psi:\\mathbb{R}\\to\\mathbb{R}$.\n- Running reward function (for continuing): $\\ell:\\mathbb{R}\\to\\mathbb{R}$.\n- State transition: a time-homogeneous conditional distribution from $x$ to $x'$ such that the conditional expectation $\\mathbb{E}[\\cdot \\mid x]$ is well-defined.\n- Objective: Maximize the expected total discounted reward.\n- Part 1 task: Derive the dynamic programming equation for the optimal value function $V:\\mathbb{R}\\to\\mathbb{R}$ using the principle of optimality and the tower property of conditional expectation.\n- Part 2 specialization:\n  - Deterministic linear dynamics: $x_{t+1}=\\lambda x_{t}$ for $\\lambda \\in \\mathbb{R}$.\n  - Running reward: $\\ell(x)=-q x^{2}$ with $q0$.\n  - Stopping reward: $\\psi(x)=A - B x^{2}$ with $A0$ and $B0$.\n  - Parameter constraints: $\\gamma \\lambda^{2}  1$ and $B  \\frac{q}{1-\\gamma \\lambda^{2}}$.\n- Part 2 task: Solve for the value function in the continuation region using an invariant functional form and determine the stopping threshold $x^{\\star}0$ for the stopping region $\\{x \\in \\mathbb{R} : |x| \\leq x^{\\star}\\}$.\n- Part 3 task: Evaluate $x^{\\star}$ for the specific parameter values $A=3$, $B=5$, $q=1$, $\\gamma=\\frac{1}{2}$, and $\\lambda=\\frac{1}{2}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scrutinized against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard formulation of an infinite-horizon, discrete-time optimal stopping problem, a fundamental topic in control theory and sequential decision theory. It relies on established concepts such as Bellman's principle of optimality, dynamic programming, and conditional expectation. It is free of any scientific or mathematical fallacies.\n- **Well-Posed**: The problem is well-posed. The first part asks for a standard derivation. The second part provides specific functional forms and parameter constraints ($ \\gamma \\lambda^2  1 $, $ B  \\frac{q}{1-\\gamma \\lambda^2} $) that are precisely those needed to ensure a well-defined, non-trivial, and unique solution exists for the value function and the stopping threshold.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Completeness**: The problem is self-contained. It provides all necessary functions, parameters, and constraints to perform the required derivations and calculations.\n- **Other Flaws**: The problem exhibits no other flaws. It is not trivial, metaphorical, or unrealistic within the mathematical context of control theory.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed to provide a complete solution.\n\n### Solution Derivation\n\n**Part 1: Derivation of the Dynamic Programming Equation**\n\nLet $V(x)$ be the optimal value function, representing the maximum expected total discounted reward starting from state $x_0 = x$. The decision-maker chooses a policy, which is a sequence of decisions (stop or continue) for each state and time, to maximize this value.\n\nAt any state $x$, the decision-maker can choose one of two actions:\n1.  **Stop**: The process terminates, and the decision-maker receives a terminal reward of $\\psi(x)$. The total discounted reward is simply $\\psi(x)$.\n2.  **Continue**: The decision-maker receives an immediate running reward of $\\ell(x)$. The system then transitions to a new state $x'$ according to its transition law. The process continues from $x'$, and by the principle of optimality, the subsequent decisions must constitute an optimal policy for the problem starting from $x'$. The expected discounted value of all future rewards, seen from the current time, is $\\gamma \\mathbb{E}[V(x') \\mid x]$. The tower property of expectation, $\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]$, ensures this formulation is consistent over time. The total expected reward for continuing is thus $\\ell(x) + \\gamma \\mathbb{E}[V(x') \\mid x]$.\n\nBellman's principle of optimality states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. Mathematically, this implies that the value function $V(x)$ must be the maximum of the values associated with each possible initial action.\n\nTherefore, the dynamic programming equation, or Bellman equation, for this optimal stopping problem is:\n$$V(x) = \\max \\left\\{ \\psi(x), \\quad \\ell(x) + \\gamma \\mathbb{E}[V(x') \\mid x] \\right\\}$$\nThis equation is a functional equation for $V(x)$. To guarantee the existence and uniqueness of a solution $V(x)$ to this equation and justify the derivation, certain regularity conditions are required. Typically, one assumes that the reward functions $\\psi(x)$ and $\\ell(x)$ are bounded, and the operator $T(f)(x) = \\max\\{\\psi(x), \\ell(x) + \\gamma \\mathbb{E}[f(x') \\mid x]\\}$ is a contraction mapping on the space of bounded continuous functions with the supremum norm. The condition $\\gamma \\in (0,1)$ is crucial for this, making $T$ a contraction and ensuring convergence of value iteration to a unique fixed point, by the Banach fixed-point theorem.\n\n**Part 2: Solution for the Specific Example**\n\nWe are given the following specializations:\n- Dynamics: $x_{t+1} = \\lambda x_t$. This is deterministic, so for any function $f$, $\\mathbb{E}[f(x_{t+1}) | x_t] = f(\\lambda x_t)$.\n- Running reward: $\\ell(x) = -q x^2$.\n- Stopping reward: $\\psi(x) = A - B x^2$.\n\nThe dynamic programming equation simplifies to:\n$$V(x) = \\max \\left\\{ A - Bx^2, \\quad -qx^2 + \\gamma V(\\lambda x) \\right\\}$$\nThe space is partitioned into a stopping region $S$ and a continuation region $C$.\n- In the stopping region $S$, $V(x) = \\psi(x) = A - Bx^2$.\n- In the continuation region $C$, $V(x) = \\ell(x) + \\gamma V(\\lambda x) = -qx^2 + \\gamma V(\\lambda x)$.\n\nThe problem posits an invariant functional form for the value function in the continuation region. Given the quadratic forms of the reward functions, we hypothesize a quadratic form for the value function in the continuation region, $V_c(x) = K - Px^2$, for some constants $K$ and $P$.\n\nSubstituting this ansatz into the equation for the continuation region:\n$$K - Px^2 = -qx^2 + \\gamma (K - P(\\lambda x)^2)$$\n$$K - Px^2 = -qx^2 + \\gamma K - \\gamma P \\lambda^2 x^2$$\nFor this equation to hold for all $x$ in the continuation region, we must match coefficients of the powers of $x$.\n- Matching constant terms: $K = \\gamma K \\implies K(1-\\gamma) = 0$. Since $\\gamma \\in (0,1)$, we must have $K=0$.\n- Matching $x^2$ terms: $-P = -q - \\gamma P \\lambda^2 \\implies P(1 - \\gamma \\lambda^2) = q$.\nThe condition $\\gamma \\lambda^2  1$ ensures that $1 - \\gamma \\lambda^2  0$. Since $q0$, we can solve for $P$:\n$$P = \\frac{q}{1 - \\gamma \\lambda^2}$$\nThus, the value function in the continuation region is $V_c(x) = -Px^2 = -\\frac{q}{1 - \\gamma \\lambda^2}x^2$.\n\nThe optimal policy is to stop when the stopping reward is at least as large as the value of continuing. The stopping region is $\\{x \\in \\mathbb{R} : \\psi(x) \\geq V_c(x)\\}$. The problem states this region is of the form $\\{x \\in \\mathbb{R} : |x| \\leq x^{\\star}\\}$. The boundary of this region is defined by the value matching condition $\\psi(x) = V_c(x)$.\n$$A - Bx^2 = -\\frac{q}{1 - \\gamma \\lambda^2}x^2$$\n$$A = Bx^2 - \\frac{q}{1 - \\gamma \\lambda^2}x^2$$\n$$A = \\left( B - \\frac{q}{1 - \\gamma \\lambda^2} \\right) x^2$$\nThe problem provides the constraint $B  \\frac{q}{1-\\gamma \\lambda^2}$, which ensures the coefficient of $x^2$ is positive. Let this positive coefficient be $C = B - \\frac{q}{1-\\gamma \\lambda^2}$.\n$$A = C x^2 \\implies x^2 = \\frac{A}{C}$$\nThe thresholds are $x = \\pm \\sqrt{\\frac{A}{C}}$. Since $x^\\star  0$, we have:\n$$x^{\\star} = \\sqrt{\\frac{A}{B - \\frac{q}{1-\\gamma \\lambda^2}}}$$\nThis is the closed-form expression for the stopping threshold. The stopping region is indeed $|x| \\leq x^\\star$, as for any $x$ in this interval, $x^2 \\leq (x^\\star)^2 = A/C$, which implies $Cx^2 \\leq A$, and rearranging gives $\\psi(x) \\geq V_c(x)$.\n\n**Part 3: Numerical Evaluation**\n\nWe are given the parameters: $A=3$, $B=5$, $q=1$, $\\gamma=\\frac{1}{2}$, and $\\lambda=\\frac{1}{2}$.\nFirst, we verify the constraints:\n- $\\gamma \\lambda^2 = \\frac{1}{2} \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2} \\cdot \\frac{1}{4} = \\frac{1}{8}$. Since $\\frac{1}{8}  1$, the first condition is satisfied.\n- We must check if $B  \\frac{q}{1-\\gamma \\lambda^2}$. The right side is $\\frac{1}{1 - 1/8} = \\frac{1}{7/8} = \\frac{8}{7}$. The condition is $5  \\frac{8}{7}$, which is true since $5 = \\frac{35}{7}$.\n\nNow, we substitute these values into the expression for $x^{\\star}$:\n$$x^{\\star} = \\sqrt{\\frac{A}{B - \\frac{q}{1 - \\gamma\\lambda^2}}}$$\n$$x^{\\star} = \\sqrt{\\frac{3}{5 - \\frac{1}{1 - \\frac{1}{8}}}}$$\n$$x^{\\star} = \\sqrt{\\frac{3}{5 - \\frac{8}{7}}}$$\n$$x^{\\star} = \\sqrt{\\frac{3}{\\frac{35}{7} - \\frac{8}{7}}}$$\n$$x^{\\star} = \\sqrt{\\frac{3}{\\frac{27}{7}}}$$\n$$x^{\\star} = \\sqrt{\\frac{3 \\cdot 7}{27}} = \\sqrt{\\frac{7}{9}}$$\n$$x^{\\star} = \\frac{\\sqrt{7}}{3}$$\n\nThe exact value of the stopping threshold is $\\frac{\\sqrt{7}}{3}$.", "answer": "$$\\boxed{\\frac{\\sqrt{7}}{3}}$$", "id": "2703363"}]}