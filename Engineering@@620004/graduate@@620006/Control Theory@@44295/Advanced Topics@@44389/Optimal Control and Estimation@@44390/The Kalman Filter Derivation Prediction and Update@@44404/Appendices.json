{"hands_on_practices": [{"introduction": "The update step is where the Kalman filter masterfully synthesizes new measurement information with its model-based prediction. While multiple algebraic forms exist for the posterior covariance update, they are not all equivalent in terms of numerical robustness, a critical consideration in practical implementations. This exercise [@problem_id:2753294] guides you through a first-principles derivation to demonstrate the equivalence of the standard and the numerically superior Joseph form, highlighting why the latter is essential for guaranteeing a symmetric and positive semi-definite covariance matrix in finite-precision arithmetic.", "problem": "Consider a single measurement update step in a linear Gaussian state-estimation setting that underlies the Kalman filter (KF). Let the prior state estimation error covariance be $P \\in \\mathbb{R}^{2 \\times 2}$, the measurement matrix be $H \\in \\mathbb{R}^{1 \\times 2}$, and the measurement noise covariance be $R \\in \\mathbb{R}^{1 \\times 1}$. Assume a linear estimator of the form $x^{+} = x^{-} + K \\left(y - H x^{-}\\right)$, where $K \\in \\mathbb{R}^{2 \\times 1}$ is a gain to be determined by minimizing the posterior mean-square error in the sense of Linear Minimum Mean Square Error (LMMSE). The measurement model is $y = H x + v$, where $v$ is zero-mean Gaussian noise, independent of the prior state error, with covariance $R$. Work from first principles, beginning with the orthogonality condition for LMMSE estimators and the definitions of the innovation and posterior error, to derive the gain that minimizes the posterior error covariance. Then, for the specific data\n$$\nP = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1  1 \\end{bmatrix}, \\quad R = 1,\n$$\ncompute:\n- the optimal gain $K$;\n- the updated covariance $P^{+}$ computed using the simple left-factor update form;\n- the updated covariance $P^{+}$ computed using the symmetric Joseph stabilized form.\n\nDiscuss, in exact arithmetic, whether both updates preserve symmetry and whether they coincide, and justify your conclusion from the derivation. Finally, compute the Frobenius norm of the difference between the two updated covariances and report that norm as your final answer. Express your final answer as a single real number without units. No rounding is required.", "solution": "The problem is subjected to rigorous validation and is found to be valid. It is a well-posed problem in linear estimation theory, containing all necessary information and being free of scientific fallacies or inconsistencies. We shall proceed with a complete solution.\n\nThe problem asks for the derivation of the optimal gain for a linear estimator, followed by a comparison of two forms of the posterior covariance update for a specific numerical case.\n\nLet the true state be $x$, the prior estimate be $x^{-}$, and the posterior estimate be $x^{+}$. The prior error is $e^{-} = x - x^{-}$, with covariance $P = E[e^{-} (e^{-})^{T}]$. The measurement is $y = Hx + v$, where the measurement noise $v$ is zero-mean, has covariance $R = E[v v^{T}]$, and is uncorrelated with the prior error, i.e., $E[e^{-} v^{T}] = 0$.\n\nThe posterior estimate is given by the linear estimator form:\n$$x^{+} = x^{-} + K (y - H x^{-})$$\nwhere $y - H x^{-} = \\tilde{y}$ is the innovation or measurement residual. The gain matrix $K$ is to be determined.\n\nThe posterior error is $e^{+} = x - x^{+}$. We can express this in terms of the prior error and the measurement noise:\n$$e^{+} = x - \\left(x^{-} + K (y - H x^{-})\\right)$$\nSubstituting $y = Hx + v$:\n$$e^{+} = x - \\left(x^{-} + K (Hx + v - H x^{-})\\right)$$\n$$e^{+} = (x - x^{-}) - K(H(x - x^{-}) + v)$$\n$$e^{+} = e^{-} - K(H e^{-} + v)$$\n$$e^{+} = (I - KH)e^{-} - Kv$$\n\nWe will now derive the optimal gain $K$ using the orthogonality principle of Linear Minimum Mean Square Error (LMMSE) estimation. This principle states that for the estimate to be optimal, the estimation error must be orthogonal to the data used. In this update step, the new data is the innovation $\\tilde{y}$. Thus, we require the expectation of the cross-correlation between the posterior error $e^{+}$ and the innovation $\\tilde{y}$ to be zero:\n$$E[e^{+} \\tilde{y}^{T}] = 0$$\n\nThe innovation is $\\tilde{y} = y - Hx^{-} = Hx + v - Hx^{-} = H(x - x^{-}) + v = He^{-} + v$. Substituting the expressions for $e^{+}$ and $\\tilde{y}$:\n$$E[\\left((I - KH)e^{-} - Kv\\right) (He^{-} + v)^{T}] = 0$$\nExpanding the product:\n$$E[\\left((I - KH)e^{-} - Kv\\right) ((e^{-})^{T}H^{T} + v^{T})] = 0$$\n$$E[(I - KH)e^{-}(e^{-})^{T}H^{T}] + E[(I - KH)e^{-}v^{T}] - E[Kv(e^{-})^{T}H^{T}] - E[Kvv^{T}] = 0$$\nUsing the definitions $P = E[e^{-}(e^{-})^{T}]$ and $R = E[vv^{T}]$, and the fact that $e^{-}$ and $v$ are uncorrelated ($E[e^{-}v^{T}] = E[v(e^{-})^{T}] = 0$), the equation simplifies to:\n$$(I - KH)E[e^{-}(e^{-})^{T}]H^{T} - K E[vv^{T}] = 0$$\n$$(I - KH)PH^{T} - KR = 0$$\n$$PH^{T} - KHPH^{T} - KR = 0$$\n$$PH^{T} = K(HPH^{T} + R)$$\nSolving for the optimal gain $K$:\n$$K = P H^{T} (H P H^{T} + R)^{-1}$$\n\nNext, we derive the posterior covariance matrix $P^{+} = E[e^{+}(e^{+})^{T}]$. Using the expression for $e^{+}$:\n$$P^{+} = E[\\left((I - KH)e^{-} - Kv\\right) \\left((I - KH)e^{-} - Kv\\right)^{T}]$$\n$$P^{+} = E[\\left((I - KH)e^{-} - Kv\\right) \\left((e^{-})^{T}(I-KH)^{T} - v^{T}K^{T}\\right)]$$\nExpanding this and again using the uncorrelation of $e^{-}$ and $v$:\n$$P^{+} = E[(I - KH)e^{-}(e^{-})^{T}(I-KH)^{T}] + E[Kvv^{T}K^{T}]$$\n$$P^{+} = (I - KH)P(I - KH)^{T} + KRK^{T}$$\nThis is the Joseph stabilized form of the covariance update. It is guaranteed to produce a symmetric and positive semi-definite matrix, provided $P$ and $R$ are symmetric and positive semi-definite, because it is of the form $A P A^{T} + B R B^{T}$.\n\nWhen the optimal gain $K$ is used, this expression can be simplified. From the derivation of $K$, we have $K(HPH^{T} + R) = PH^{T}$.\nLet us expand the Joseph form:\n$$P^{+} = P(I - KH)^{T} - KHP(I - KH)^{T} + KRK^{T}$$\n$$P^{+} = P - PH^{T}K^{T} - KHP + KHPH^{T}K^{T} + KRK^{T}$$\n$$P^{+} = P - PH^{T}K^{T} - KHP + K(HPH^{T} + R)K^{T}$$\nSubstituting $K(HPH^{T} + R) = PH^{T}$:\n$$P^{+} = P - PH^{T}K^{T} - KHP + PH^{T}K^{T}$$\n$$P^{+} = P - KHP$$\nThis can be factored as:\n$$P^{+} = (I - KH)P$$\nThis is the \"simple left-factor update form\". This derivation shows that the simple form is algebraically equivalent to the Joseph form if and only if the optimal gain $K$ is used.\n\nNow, we perform the computations with the given data:\n$$P = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1  1 \\end{bmatrix}, \\quad R = 1$$\n\nFirst, we compute the optimal gain $K$. We need the innovation covariance $S = HPH^{T} + R$.\n$$H P H^{T} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1(1) + 1(1) = 2$$\n$$S = 2 + 1 = 3$$\nThe gain $K$ is:\n$$K = P H^{T} S^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (3)^{-1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\frac{1}{3} = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\end{bmatrix}$$\n\nNow, we compute the updated covariance $P^{+}$ using both forms.\n1. Simple left-factor update form: $P_{\\text{simple}}^{+} = (I - KH)P$.\nFirst, calculate $I - KH$:\n$$K H = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\end{bmatrix} \\begin{bmatrix} 1  1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{3}  \\frac{1}{3} \\\\ \\frac{1}{3}  \\frac{1}{3} \\end{bmatrix}$$\n$$I - KH = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} \\frac{1}{3}  \\frac{1}{3} \\\\ \\frac{1}{3}  \\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{1}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  1 - \\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}$$\nSince $P = I$, we have:\n$$P_{\\text{simple}}^{+} = (I - KH)I = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}$$\n\n2. Symmetric Joseph stabilized form: $P_{\\text{Joseph}}^{+} = (I - KH)P(I - KH)^{T} + KRK^{T}$.\nThe first term is, with $P = I$:\n$$(I - KH)I(I - KH)^{T} = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}^{T} = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}$$\n$$= \\begin{bmatrix} (\\frac{2}{3})^2 + (-\\frac{1}{3})^2  (\\frac{2}{3})(-\\frac{1}{3}) + (-\\frac{1}{3})(\\frac{2}{3}) \\\\ (-\\frac{1}{3})(\\frac{2}{3}) + (\\frac{2}{3})(-\\frac{1}{3})  (-\\frac{1}{3})^2 + (\\frac{2}{3})^2 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{9} + \\frac{1}{9}  -\\frac{2}{9} - \\frac{2}{9} \\\\ -\\frac{2}{9} - \\frac{2}{9}  \\frac{1}{9} + \\frac{4}{9} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{9}  -\\frac{4}{9} \\\\ -\\frac{4}{9}  \\frac{5}{9} \\end{bmatrix}$$\nThe second term is, with $R=1$:\n$$K R K^{T} = K(1)K^{T} = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{3}  \\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{9}  \\frac{1}{9} \\\\ \\frac{1}{9}  \\frac{1}{9} \\end{bmatrix}$$\nAdding the two terms:\n$$P_{\\text{Joseph}}^{+} = \\begin{bmatrix} \\frac{5}{9}  -\\frac{4}{9} \\\\ -\\frac{4}{9}  \\frac{5}{9} \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{9}  \\frac{1}{9} \\\\ \\frac{1}{9}  \\frac{1}{9} \\end{bmatrix} = \\begin{bmatrix} \\frac{5+1}{9}  \\frac{-4+1}{9} \\\\ \\frac{-4+1}{9}  \\frac{5+1}{9} \\end{bmatrix} = \\begin{bmatrix} \\frac{6}{9}  -\\frac{3}{9} \\\\ -\\frac{3}{9}  \\frac{6}{9} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}$$\n\nDiscussion:\nIn exact arithmetic, both update computations result in the symmetric matrix $\\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix}$. Thus, both forms preserve symmetry and they coincide. This outcome is expected and is justified by our prior derivation, which demonstrated that the simple form $P^{+} = (I - KH)P$ is an algebraic simplification of the Joseph form $P^{+} = (I - KH)P(I - KH)^{T} + KRK^{T}$ under the condition that the optimal gain $K$ is used. Since we computed and used the optimal gain, the two forms must yield identical results. The Joseph form is manifestly symmetric by its structure, while the symmetry of the simplified form depends on the optimality of $K$.\n\nFinally, we compute the Frobenius norm of the difference between the two updated covariances. The difference matrix $\\Delta P$ is:\n$$\\Delta P = P_{\\text{simple}}^{+} - P_{\\text{Joseph}}^{+} = \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix} - \\begin{bmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$$\nThe Frobenius norm is $\\|\\Delta P\\|_{F} = \\sqrt{\\sum_{i,j} |\\Delta P_{ij}|^2}$.\n$$\\|\\Delta P\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0$$\nThe norm of the difference is $0$, confirming their equality in exact arithmetic.", "answer": "$$\n\\boxed{0}\n$$", "id": "2753294"}, {"introduction": "The power of the Kalman filter lies in its recursive application, where the posterior variance from one step becomes the prior for the next, governed by the discrete-time algebraic Riccati equation. The long-term behavior of this recursion determines the filter's steady-state accuracy and performance. Through this hands-on coding practice [@problem_id:2753301], you will implement the scalar Riccati recursion to observe its evolution across various scenarios, building crucial intuition for how system parameters dictate the convergence, divergence, or steady-state value of the estimation error variance.", "problem": "Consider a one-dimensional linear-Gaussian discrete-time state-space model defined by the pair of equations\n$$x_{k+1} = a\\,x_k + w_k,\\quad y_k = c\\,x_k + v_k,$$\nwhere $x_k$ is the state, $y_k$ is the observation, $w_k \\sim \\mathcal{N}(0,q)$ and $v_k \\sim \\mathcal{N}(0,r)$ are independent zero-mean Gaussian noises with variances $q \\ge 0$ and $r  0$, respectively, and $a, c \\in \\mathbb{R}$. Assume a Gaussian posterior at time index $k$ with mean and variance $(\\mu_k, P_k)$, and consider the standard Bayesian estimation framework for linear-Gaussian systems.\n\nTask 1 (derivation): Starting from the fundamental properties of linear transformations of Gaussian random variables and conditioning in jointly Gaussian distributions, derive the one-step prediction and update rules for the posterior variance $P_k$ in scalar form, expressed explicitly in terms of $a$, $c$, $q$, $r$, and $P_{k-1}$. Your derivation must not assume any pre-quoted Kalman filter formulas and must rely only on the laws of total expectation and variance, and the conditional distribution of jointly Gaussian vectors.\n\nTask 2 (algorithm): Using the derived scalar variance recursion, implement an iterative procedure that, given an initial posterior variance $P_0  0$, computes a sequence $\\{P_k\\}_{k=1}^N$ by alternating prediction and update. For each test case below, perform $N$ iterations and report:\n- The approximate steady-state posterior variance, taken as $P_N$.\n- The absolute difference between the last two posterior iterates, $\\Delta_N = |P_N - P_{N-1}|$.\n- A boolean convergence flag defined as $\\mathrm{converged} = \\big(\\Delta_N \\le \\mathrm{tol}\\big)$.\n\nTask 3 (test suite and output): Use the following five test cases. In all cases, the initial posterior variance is $P_0 = 1$.\n- Case A (unstable but detectable): $a = 1.2$, $c = 1$, $q = 1$, $r = 1$, $N = 100$, $\\mathrm{tol} = 10^{-10}$.\n- Case B (stable with large measurement noise): $a = 0.9$, $c = 1$, $q = 0.1$, $r = 10$, $N = 100$, $\\mathrm{tol} = 10^{-10}$.\n- Case C (no process noise, identity dynamics): $a = 1$, $c = 1$, $q = 0$, $r = 1$, $N = 100$, $\\mathrm{tol} = 10^{-10}$.\n- Case D (unobservable, diverging prediction): $a = 1.2$, $c = 0$, $q = 1$, $r = 1$, $N = 50$, $\\mathrm{tol} = 10^{-10}$.\n- Case E (near noise-free measurement): $a = 1.2$, $c = 1$, $q = 1$, $r = 10^{-6}$, $N = 100$, $\\mathrm{tol} = 10^{-10}$.\n\nYour program must produce a single line of output containing a JSON-like array of five entries, one per case, in the same order A through E. Each entry is the triple\n$$[P_N,\\ \\Delta_N,\\ \\mathrm{converged}],$$\nwhere $P_N$ and $\\Delta_N$ are floating-point numbers rounded to $6$ decimal places, and $\\mathrm{converged}$ is a boolean literal written as either $true$ or $false$ in lowercase. The entire output must be a single line of the form\n$$\\big[[P_N^{(A)},\\Delta_N^{(A)},\\mathrm{converged}^{(A)}],\\ldots,[P_N^{(E)},\\Delta_N^{(E)},\\mathrm{converged}^{(E)}]\\big].$$\nNo additional text should be printed. All quantities are dimensionless, so no physical units are required.", "solution": "The problem requires the derivation of the scalar Kalman filter variance update equations from first principles, followed by a numerical implementation to analyze the filter's behavior under different parameter regimes. The analysis will proceed in two main stages: first, the theoretical derivation, and second, the implementation and execution for the specified test cases.\n\nThe state-space model is defined by the following one-dimensional linear-Gaussian equations:\nState transition process:\n$$x_{k+1} = a\\,x_k + w_k, \\quad w_k \\sim \\mathcal{N}(0, q)$$\nMeasurement process:\n$$y_k = c\\,x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, r)$$\nHere, $x_k \\in \\mathbb{R}$ is the hidden state at time $k$, and $y_k \\in \\mathbb{R}$ is the corresponding measurement. The process noise $w_k$ and measurement noise $v_k$ are independent, zero-mean Gaussian random variables with variances $q \\ge 0$ and $r  0$, respectively. The parameters $a$ and $c$ are real-valued constants.\n\nThe core of Bayesian filtering is to recursively compute the posterior distribution of the state, $p(x_k | y_{1:k})$, where $y_{1:k} = \\{y_1, y_2, \\dots, y_k\\}$ denotes the sequence of all measurements up to time $k$. For a linear-Gaussian system, if the initial state distribution is Gaussian, all subsequent posteriors will also be Gaussian. We denote the posterior at time $k$ by $\\mathcal{N}(\\mu_k, P_k)$. The problem asks for a recursion for the posterior variance $P_k$ in terms of the posterior variance at the previous step, $P_{k-1}$. This recursion consists of two steps: prediction and update.\n\n**1. Prediction Step**\n\nThe prediction step aims to compute the prior distribution for the state at time $k$, conditioned on all evidence up to time $k-1$. This is the distribution $p(x_k | y_{1:k-1})$. Let us assume the posterior at time $k-1$ is known and is Gaussian:\n$$p(x_{k-1} | y_{1:k-1}) = \\mathcal{N}(x_{k-1} | \\mu_{k-1}, P_{k-1})$$\nThe state $x_k$ is generated from $x_{k-1}$ via the state transition equation $x_k = a\\,x_{k-1} + w_{k-1}$. Conditioned on $y_{1:k-1}$, the variable $x_{k-1}$ is a Gaussian random variable with mean $\\mu_{k-1}$ and variance $P_{k-1}$. The process noise $w_{k-1}$ is independent of $x_{k-1}$ and all past measurements $y_{1:k-1}$, and follows $\\mathcal{N}(0, q)$.\nSince $x_k$ is a linear combination of two independent Gaussian random variables, its distribution $p(x_k | y_{1:k-1})$ is also Gaussian. We denote its mean and variance as $\\mu_k^-$ and $P_k^-$, respectively (the superscript '$-$' denotes a predicted or *a priori* quantity).\n\nThe mean of the predicted distribution is found using the law of total expectation:\n$$ \\mu_k^- = E[x_k | y_{1:k-1}] = E[a\\,x_{k-1} + w_{k-1} | y_{1:k-1}] $$\nBy linearity of expectation:\n$$ \\mu_k^- = a\\,E[x_{k-1} | y_{1:k-1}] + E[w_{k-1} | y_{1:k-1}] $$\nSince $w_{k-1}$ is independent of past measurements, $E[w_{k-1} | y_{1:k-1}] = E[w_{k-1}] = 0$. The first term is $a\\,\\mu_{k-1}$. Thus,\n$$ \\mu_k^- = a\\,\\mu_{k-1} $$\nThe variance of the predicted distribution is found using the law of total variance. Since $w_{k-1}$ is independent of $x_{k-1}$ (conditioned on $y_{1:k-1}$), the variance of their sum is the sum of their variances:\n$$ P_k^- = \\mathrm{Var}(x_k | y_{1:k-1}) = \\mathrm{Var}(a\\,x_{k-1} + w_{k-1} | y_{1:k-1}) = \\mathrm{Var}(a\\,x_{k-1} | y_{1:k-1}) + \\mathrm{Var}(w_{k-1} | y_{1:k-1}) $$\nUsing the variance property $\\mathrm{Var}(bZ) = b^2\\mathrm{Var}(Z)$ and the independence of $w_{k-1}$ from past data:\n$$ P_k^- = a^2\\,\\mathrm{Var}(x_{k-1} | y_{1:k-1}) + \\mathrm{Var}(w_{k-1}) = a^2\\,P_{k-1} + q $$\nThis is the prediction equation for the variance.\n\n**2. Update Step**\n\nThe update step incorporates the new measurement $y_k$ to refine the predicted distribution $p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k | \\mu_k^-, P_k^-)$ into the posterior distribution $p(x_k | y_{1:k}) = \\mathcal{N}(x_k | \\mu_k, P_k)$. To do this, we consider the joint distribution of the state $x_k$ and the measurement $y_k$, conditioned on the past data $y_{1:k-1}$.\n\nThe measurement model is $y_k = c\\,x_k + v_k$. Both variables upon which $y_k$ depends, $x_k$ and $v_k$, are Gaussian (conditioned on $y_{1:k-1}$). Thus, the pair $(x_k, y_k)$ is jointly Gaussian under the conditioning on $y_{1:k-1}$. We need to find the parameters of this joint distribution:\n$$ \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} \\Bigg| y_{1:k-1} \\sim \\mathcal{N} \\left( \\begin{pmatrix} E[x_k] \\\\ E[y_k] \\end{pmatrix}, \\begin{pmatrix} \\mathrm{Var}(x_k)  \\mathrm{Cov}(x_k, y_k) \\\\ \\mathrm{Cov}(y_k, x_k)  \\mathrm{Var}(y_k) \\end{pmatrix} \\right) $$\nwhere all expectations and covariances are implicitly conditioned on $y_{1:k-1}$.\n\nThe mean vector is:\n$$ E[x_k] = \\mu_k^- $$\n$$ E[y_k] = E[c\\,x_k + v_k] = c\\,E[x_k] + E[v_k] = c\\,\\mu_k^- $$\n(since $v_k$ is zero-mean and independent of all other variables).\n\nThe covariance matrix is:\n$$ \\mathrm{Var}(x_k) = P_k^- $$\n$$ \\mathrm{Cov}(x_k, y_k) = \\mathrm{Cov}(x_k, c\\,x_k + v_k) = c\\,\\mathrm{Cov}(x_k, x_k) + \\mathrm{Cov}(x_k, v_k) = c\\,\\mathrm{Var}(x_k) = c\\,P_k^- $$\n(since $v_k$ is independent of $x_k$).\n$$ \\mathrm{Var}(y_k) = \\mathrm{Var}(c\\,x_k + v_k) = c^2\\,\\mathrm{Var}(x_k) + \\mathrm{Var}(v_k) = c^2\\,P_k^- + r $$\n(since $x_k$ and $v_k$ are independent).\n\nSo, the joint distribution is:\n$$ \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} \\Bigg| y_{1:k-1} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mu_k^- \\\\ c\\,\\mu_k^- \\end{pmatrix}, \\begin{pmatrix} P_k^-  c\\,P_k^- \\\\ c\\,P_k^-  c^2\\,P_k^- + r \\end{pmatrix} \\right) $$\nThe posterior distribution $p(x_k | y_k, y_{1:k-1})$ is the conditional distribution of a jointly Gaussian vector. Using the standard formula for conditioning, the variance of $x_k$ given $y_k$ is:\n$$ P_k = \\mathrm{Var}(x_k | y_k, y_{1:k-1}) = \\mathrm{Var}(x_k) - \\mathrm{Cov}(x_k, y_k) \\mathrm{Var}(y_k)^{-1} \\mathrm{Cov}(y_k, x_k) $$\nSubstituting the terms we derived:\n$$ P_k = P_k^- - (c\\,P_k^-)(c^2\\,P_k^- + r)^{-1}(c\\,P_k^-) = P_k^- - \\frac{c^2(P_k^-)^2}{c^2\\,P_k^- + r} $$\nThis is the variance update equation. It can be simplified by finding a common denominator:\n$$ P_k = \\frac{P_k^-(c^2\\,P_k^- + r) - c^2(P_k^-)^2}{c^2\\,P_k^- + r} = \\frac{c^2(P_k^-)^2 + r\\,P_k^- - c^2(P_k^-)^2}{c^2\\,P_k^- + r} = \\frac{r\\,P_k^-}{c^2\\,P_k^- + r} $$\n\n**3. Complete Variance Recursion**\n\nCombining the prediction and update steps, we obtain a single recursion for the posterior variance $P_k$ in terms of $P_{k-1}$:\n1. Predict: $P_k^- = a^2\\,P_{k-1} + q$\n2. Update: $P_k = \\frac{r\\,P_k^-}{c^2\\,P_k^- + r}$\n\nSubstituting the first equation into the second gives the complete recursion, also known as the scalar discrete-time algebraic Riccati equation:\n$$ P_k = \\frac{r(a^2\\,P_{k-1} + q)}{c^2(a^2\\,P_{k-1} + q) + r} $$\nThis equation does not depend on the measurements $y_k$ or the state means $\\mu_k$. It can be computed offline before any data is processed. The algorithm for the numerical task will iterate this equation starting from $P_0$.\n\nThe implementation will consist of a loop that, for each of the five test cases, starts with $P_0=1$ and applies the derived recursion $N$ times. At the end of the iterations, it will compute the final variance $P_N$, the change from the previous iterate $\\Delta_N = |P_N - P_{N-1}|$, and a boolean flag indicating if this change is below the tolerance $\\mathrm{tol}$.\n\nA special case arises when $c = 0$. In this situation, the measurement provides no information about the state. The update equation becomes:\n$$ P_k = \\frac{r\\,P_k^-}{0^2\\,P_k^- + r} = \\frac{r\\,P_k^-}{r} = P_k^- $$\nThis shows that when the system is unobservable, the posterior variance is simply the predicted variance, which will grow unboundedly if $|a| \\ge 1$ and $q  0$. This behavior is expected in Case D.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Kalman filter variance recursion problem for five test cases.\n\n    This function implements the derived scalar Riccati recursion for the\n    posterior variance of a 1D linear-Gaussian state-space model. It iterates\n    the recursion for N steps for each test case and reports the final variance,\n    the absolute difference between the last two iterates, and a convergence flag.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Format: (a, c, q, r, N, tol, P0)\n    test_cases = [\n        # Case A: unstable but detectable\n        (1.2, 1.0, 1.0, 1.0, 100, 1e-10, 1.0),\n        # Case B: stable with large measurement noise\n        (0.9, 1.0, 0.1, 10.0, 100, 1e-10, 1.0),\n        # Case C: no process noise, identity dynamics\n        (1.0, 1.0, 0.0, 1.0, 100, 1e-10, 1.0),\n        # Case D: unobservable, diverging prediction\n        (1.2, 0.0, 1.0, 1.0, 50, 1e-10, 1.0),\n        # Case E: near noise-free measurement\n        (1.2, 1.0, 1.0, 1e-6, 100, 1e-10, 1.0),\n    ]\n\n    results = []\n    for a, c, q, r, N, tol, P0 in test_cases:\n        p_current = float(P0)\n        p_previous = 0.0  # Initialize to a different value\n\n        for _ in range(N):\n            p_previous = p_current\n\n            # Prediction step for variance\n            p_predicted = a * a * p_previous + q\n\n            # Update step for variance\n            # The formula is P_k = (r * P_k^-) / (c^2 * P_k^- + r)\n            # This is numerically stable and handles c=0 correctly.\n            denominator = c * c * p_predicted + r\n            if denominator == 0:\n                # This case should not happen given r > 0.\n                # If c=0 and r=0, this would be an issue, but problem states r > 0.\n                # If p_predicted is somehow negative infinity, this could happen\n                # but variance must be non-negative.\n                p_current = np.inf\n            else:\n                p_current = (r * p_predicted) / denominator\n\n        # After N iterations, calculate final values\n        P_N = p_current\n        # The value of p_previous is from the (N-1)-th iteration\n        Delta_N = abs(P_N - p_previous)\n        converged = (Delta_N = tol)\n\n        results.append((P_N, Delta_N, converged))\n\n    # Format the output as a single JSON-like string\n    formatted_results = []\n    for p, d, c in results:\n        # Convert boolean to lowercase 'true' or 'false'\n        c_str = 'true' if c else 'false'\n        # Format floats to 6 decimal places\n        formatted_results.append(f'[{p:.6f},{d:.6f},{c_str}]')\n\n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2753301"}, {"introduction": "A fundamental question in filter design is whether the estimation error will remain bounded or diverge over time, potentially leading to filter failure. The answer lies not in the filter algorithm itself, but in the inherent properties of the system model, specifically the concept of detectability. This problem [@problem_id:2753278] provides a striking example of a non-detectable system, where you will mathematically prove that the variance corresponding to an unobservable and unstable state grows without bound, a critical lesson in pre-emptive filter stability analysis.", "problem": "Consider the discrete-time, linear time-invariant (LTI) stochastic state-space model\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad y_{k} = H x_{k} + v_{k},\n$$\nwhere $x_{k} \\in \\mathbb{R}^{2}$, $y_{k} \\in \\mathbb{R}$, and\n$$\nA = \\begin{pmatrix} \\tfrac{3}{2}  0 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix}, \\quad H = \\begin{pmatrix} 0  1 \\end{pmatrix}.\n$$\nAssume the process noise $w_{k} \\sim \\mathcal{N}(0,Q)$ with $Q = \\operatorname{diag}(1,1)$, and the measurement noise $v_{k} \\sim \\mathcal{N}(0,R)$ with $R = 1$. Suppose $w_{k}$ and $v_{k}$ are mutually independent and independent over time, and that the prior is Gaussian with $x_{0} \\sim \\mathcal{N}(\\hat{x}_{0|0}, P_{0|0})$, where $P_{0|0} = I_{2}$.\n\nUsing only fundamental properties of linear Gaussian models, Bayes’ rule, and the laws of total expectation and covariance (without assuming any formula specific to the Kalman filter a priori), do the following:\n\n1) Using the definition of observability and detectability for discrete-time systems, argue whether the pair $(A,H)$ is detectable. Your argument must explicitly identify any unobservable eigenmodes and compare their eigenvalue magnitudes to $1$.\n\n2) Starting from the joint Gaussian structure implied by the model and the definition of conditional mean and covariance, derive the prediction and update recursions for the state-estimation error covariance, and specialize them to the given $(A,H,Q,R)$. Show that for these data the covariance recursion decouples along the coordinate axes and that the first coordinate’s covariance is unaffected by the measurement update.\n\n3) Solve the resulting scalar recursion for the first coordinate’s a posteriori covariance $P_{11,k|k}$ and express it in closed form as a function of the discrete time index $k \\in \\mathbb{N}$, given $P_{11,0|0} = 1$.\n\nProvide the exact, simplified analytic expression for $P_{11,k|k}$ as your final answer. No rounding is required. Treat the covariance as unitless.", "solution": "The problem statement is evaluated for validity before any attempt at a solution. The givens are a discrete-time LTI stochastic state-space model:\n$x_{k+1} = A x_{k} + w_{k}$\n$y_{k} = H x_{k} + v_{k}$\nwhere $x_{k} \\in \\mathbb{R}^{2}$, $y_{k} \\in \\mathbb{R}$. The matrices and noise statistics are:\n$A = \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$, $H = \\begin{pmatrix} 0  1 \\end{pmatrix}$.\nProcess noise $w_{k} \\sim \\mathcal{N}(0,Q)$ with $Q = \\operatorname{diag}(1,1) = I_{2}$.\nMeasurement noise $v_{k} \\sim \\mathcal{N}(0,R)$ with $R = 1$.\nThe noises $w_k$ and $v_k$ are independent and white.\nThe prior is $x_{0} \\sim \\mathcal{N}(\\hat{x}_{0|0}, P_{0|0})$ with $P_{0|0} = I_{2}$.\n\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard exercise in linear systems theory and state estimation, requiring derivation from fundamental principles. All necessary information is provided, and the problem is internally consistent and free from ambiguity. Therefore, we proceed with the solution.\n\n1) Analysis of Detectability\nA discrete-time linear time-invariant system is defined as detectable if and only if every unobservable mode is stable. A mode is stable if its associated eigenvalue has a magnitude less than $1$. We first determine the unobservable subspace of the pair $(A,H)$.\nThe observability matrix for a system with state dimension $n=2$ is $\\mathcal{O} = \\begin{pmatrix} H \\\\ HA \\end{pmatrix}$.\nWe compute the product $HA$:\n$$ HA = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0  \\frac{1}{2} \\end{pmatrix} $$\nThe observability matrix is:\n$$ \\mathcal{O} = \\begin{pmatrix} 0  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix} $$\nThe rank of $\\mathcal{O}$ is $1$, which is less than the state dimension $n=2$. The system is therefore not observable.\nThe unobservable subspace is the null space of $\\mathcal{O}$. Let $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ be a vector in this subspace. The condition $\\mathcal{O}v = 0$ implies:\n$$ \\begin{pmatrix} 0  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} v_2 \\\\ \\frac{1}{2}v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies v_2 = 0 $$\nThe null space is spanned by the vector $e_1 = \\begin{pmatrix} 1  0 \\end{pmatrix}^T$, which corresponds to the first state variable, $x_{1,k}$. This is the unobservable subspace.\n\nNext, we identify the modes associated with this subspace. A mode is unobservable if its corresponding eigenvector lies in the unobservable subspace. The matrix $A$ is diagonal, so its eigenvectors are the standard basis vectors. The eigenvector associated with the unobservable subspace is $e_1$. We examine its corresponding eigenvalue:\n$$ A e_1 = \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{3}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $$\nThe eigenvalue for this unobservable mode is $\\lambda_1 = \\frac{3}{2}$. For a discrete-time system, a mode is unstable if the magnitude of its eigenvalue is greater than or equal to $1$. Here, $|\\lambda_1| = |\\frac{3}{2}| > 1$.\nSince the system possesses an unobservable mode that is unstable, the pair $(A,H)$ is not detectable.\n\n2) Derivation of Covariance Recursions\nWe derive the prediction and update equations for the state-estimation error covariance $P_{k|k} = E[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T]$ from first principles.\n\nPrediction Step: The a priori covariance at time $k+1$ is $P_{k+1|k} = E[(x_{k+1} - \\hat{x}_{k+1|k})(x_{k+1} - \\hat{x}_{k+1|k})^T]$. The prediction error is $x_{k+1} - \\hat{x}_{k+1|k} = (A x_k + w_k) - A \\hat{x}_{k|k} = A(x_k - \\hat{x}_{k|k}) + w_k$.\n$$ P_{k+1|k} = E[ (A(x_k - \\hat{x}_{k|k}) + w_k) (A(x_k - \\hat{x}_{k|k}) + w_k)^T ] $$\nThe cross-term $E[A(x_k - \\hat{x}_{k|k})w_k^T]$ is zero because the error $x_k - \\hat{x}_{k|k}$ is a function of information up to time $k$, while $w_k$ is independent of the past. Thus:\n$$ P_{k+1|k} = A E[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T] A^T + E[w_k w_k^T] = A P_{k|k} A^T + Q $$\n\nUpdate Step: The update relies on the properties of jointly Gaussian distributions. The predicted state $x_{k+1}$ and the new measurement $y_{k+1}$ are jointly Gaussian, conditioned on past data. The posterior covariance $P_{k+1|k+1}$ is given by the standard formula for conditional Gaussian distributions:\n$P_{k+1|k+1} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma_{yy}^{-1} \\Sigma_{yx}$, where $\\Sigma_{xx} = P_{k+1|k}$, $\\Sigma_{xy} = P_{k+1|k}H^T$, and $\\Sigma_{yy} = HP_{k+1|k}H^T + R$.\n$$ P_{k+1|k+1} = P_{k+1|k} - P_{k+1|k} H^T (H P_{k+1|k} H^T + R)^{-1} H P_{k+1|k} $$\n\nSpecialization to the given system: Since $A$ and $Q$ are diagonal, and the initial covariance $P_{0|0}=I_2$ is diagonal, the covariance matrices $P_{k|k}$ and $P_{k+1|k}$ remain diagonal for all $k \\ge 0$. Let $P_{k|k} = \\text{diag}(P_{11,k|k}, P_{22,k|k})$.\nThe prediction recursion is:\n$$ P_{k+1|k} = \\begin{pmatrix} \\frac{9}{4}  0 \\\\ 0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} P_{11,k|k}  0 \\\\ 0  P_{22,k|k} \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{4}P_{11,k|k} + 1  0 \\\\ 0  \\frac{1}{4}P_{22,k|k} + 1 \\end{pmatrix} $$\nFor the update step, we compute the Kalman gain components. The innovation covariance $S_{k+1} = H P_{k+1|k} H^T + R$ is:\n$$ S_{k+1} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} P_{11,k+1|k}  0 \\\\ 0  P_{22,k+1|k} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + 1 = P_{22,k+1|k} + 1 $$\nThe Kalman gain is $K_{k+1} = P_{k+1|k} H^T S_{k+1}^{-1}$:\n$$ K_{k+1} = \\begin{pmatrix} P_{11,k+1|k}  0 \\\\ 0  P_{22,k+1|k} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (P_{22,k+1|k} + 1)^{-1} = \\begin{pmatrix} 0 \\\\ \\frac{P_{22,k+1|k}}{P_{22,k+1|k} + 1} \\end{pmatrix} $$\nThe update for $P_{k+1|k+1} = (I - K_{k+1}H)P_{k+1|k}$ is:\n$$ P_{k+1|k+1} = \\left( I_2 - \\begin{pmatrix} 0 \\\\ \\frac{P_{22,k+1|k}}{P_{22,k+1|k} + 1} \\end{pmatrix} \\begin{pmatrix} 0  1 \\end{pmatrix} \\right) P_{k+1|k} = \\begin{pmatrix} 1  0 \\\\ 0  1-\\frac{P_{22,k+1|k}}{P_{22,k+1|k}+1} \\end{pmatrix} P_{k+1|k} $$\n$$ P_{k+1|k+1} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{P_{22,k+1|k}+1} \\end{pmatrix} \\begin{pmatrix} P_{11,k+1|k}  0 \\\\ 0  P_{22,k+1|k} \\end{pmatrix} = \\begin{pmatrix} P_{11,k+1|k}  0 \\\\ 0  \\frac{P_{22,k+1|k}}{P_{22,k+1|k}+1} \\end{pmatrix} $$\nThis shows the covariance recursion is decoupled. The first coordinate's covariance is unaffected by the measurement update ($P_{11,k+1|k+1} = P_{11,k+1|k}$), as $H$ only provides information about the second state. This directly corresponds to the unobservability of the first state.\n\n3) Closed-form Solution for $P_{11,k|k}$\nWe must solve the scalar recursion for the first coordinate's a posteriori covariance. Combining the prediction and update steps for the $(1,1)$ component yields:\n$$ P_{11,k+1|k+1} = P_{11,k+1|k} = \\frac{9}{4} P_{11,k|k} + 1 $$\nLet $p_k = P_{11,k|k}$. The recurrence is $p_{k+1} = \\frac{9}{4} p_k + 1$, with initial condition $p_0 = P_{11,0|0} = 1$.\nThis is a linear first-order non-homogeneous recurrence relation. The solution is the sum of a particular solution and the solution to the homogeneous equation.\nThe homogeneous solution is $p_h(k) = C(\\frac{9}{4})^k$.\nFor a particular solution, we assume a constant $p^*$, which is the fixed point:\n$$ p^* = \\frac{9}{4} p^* + 1 \\implies (1 - \\frac{9}{4})p^* = 1 \\implies -\\frac{5}{4}p^* = 1 \\implies p^* = -\\frac{4}{5} $$\nThe general solution is $p_k = C(\\frac{9}{4})^k - \\frac{4}{5}$.\nWe use the initial condition $p_0 = 1$ to find the constant $C$:\n$$ p_0 = 1 = C \\left(\\frac{9}{4}\\right)^0 - \\frac{4}{5} = C - \\frac{4}{5} \\implies C = 1 + \\frac{4}{5} = \\frac{9}{5} $$\nSubstituting $C$ back, the closed-form expression for $P_{11,k|k}$ is:\n$$ P_{11,k|k} = \\frac{9}{5} \\left(\\frac{9}{4}\\right)^k - \\frac{4}{5} $$\nThis variance grows exponentially, consistent with the fact that the associated state is driven by noise, unstable, and unobservable.", "answer": "$$\\boxed{\\frac{9}{5} \\left(\\frac{9}{4}\\right)^{k} - \\frac{4}{5}}$$", "id": "2753278"}]}