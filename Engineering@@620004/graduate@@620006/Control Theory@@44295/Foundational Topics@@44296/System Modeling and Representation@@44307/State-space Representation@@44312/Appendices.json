{"hands_on_practices": [{"introduction": "A crucial skill in modern control theory is the ability to translate system descriptions between different domains. This first exercise provides fundamental practice in converting a frequency-domain transfer function into a time-domain state-space model using the controllable companion form. By then verifying the controllability and observability of the resulting realization [@problem_id:2749421], you will solidify your understanding of what constitutes a minimal, and therefore efficient, system representation.", "problem": "Consider the strictly proper single-input single-output (SISO) transfer function of a linear time-invariant (LTI) system\n$$\nG(s) \\;=\\; \\frac{1}{s^{4} + 7 s^{3} + 16 s^{2} + 15 s + 5}.\n$$\nUsing only the foundational relationships between state-space models and transfer functions (namely, the Laplace-domain relation $G(s) = C\\,(sI - A)^{-1} B + D$ under zero initial conditions), and the definitions of controllability and observability, perform the following:\n\n1) Construct a controllable companion-form realization $\\big(A,B,C,D\\big)$ of $G(s)$ with minimal state dimension.\n\n2) Verify the minimality of your realization by showing that it is both controllable and observable, starting from the definitions of the controllability and observability matrices.\n\n3) Let the observability matrix be defined as\n$$\n\\mathcal{O} \\;=\\; \\begin{bmatrix}\nC \\\\\nC A \\\\\nC A^{2} \\\\\nC A^{3}\n\\end{bmatrix}.\n$$\nCompute the determinant of $\\mathcal{O}$ and report it as your final answer. Express your final answer as an exact integer. No rounding is required.", "solution": "The analysis and solution of the presented problem will proceed in three stages: first, a validation of the problem statement to ensure its scientific and logical integrity; second, the construction of the required state-space realization; and third, the verification of its properties and computation of the final requested value.\n\nThe problem statement has been validated and found to be scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in linear control theory that is free of contradictions, ambiguities, or factual unsoundness. We may therefore proceed with the solution.\n\nThe given transfer function is\n$$\nG(s) = \\frac{1}{s^{4} + 7 s^{3} + 16 s^{2} + 15 s + 5}\n$$\nThis is a single-input single-output (SISO) system. We denote the numerator and denominator polynomials as $N(s)$ and $D(s)$, respectively.\n$$\nN(s) = 1\n$$\n$$\nD(s) = s^{4} + 7 s^{3} + 16 s^{2} + 15 s + 5\n$$\nThe order of the system, $n$, is the degree of the denominator polynomial, which is $n=4$. Since the degree of the numerator ($0$) is less than the degree of the denominator ($4$), the system is strictly proper. This implies that the direct feedthrough matrix $D$ is zero. For a SISO system, $D$ is a scalar, so $D=0$.\n\nPart 1: Construction of a controllable companion-form realization.\nA state-space realization is a set of matrices $(A, B, C, D)$ such that $G(s) = C(sI-A)^{-1}B+D$. We seek a controllable companion form (also known as the controller canonical form) for this system. For a general $n$-th order denominator $D(s) = s^n + a_{n-1}s^{n-1} + \\dots + a_1s + a_0$ and numerator $N(s) = b_{n-1}s^{n-1} + \\dots + b_1s + b_0$, the controllable companion form is given by:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & \\dots & 0 \\\\\n0 & 0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 \\\\\n-a_0 & -a_1 & -a_2 & \\dots & -a_{n-1}\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n1\n\\end{pmatrix}\n$$\n$$\nC = \\begin{pmatrix} b_0 & b_1 & \\dots & b_{n-1} \\end{pmatrix}, \\quad D = [0]\n$$\nFrom the given $G(s)$, we have $n=4$ and the coefficients are:\n$a_3 = 7$, $a_2 = 16$, $a_1 = 15$, $a_0 = 5$.\n$b_3 = 0$, $b_2 = 0$, $b_1 = 0$, $b_0 = 1$.\n\nSubstituting these values into the canonical form yields the state-space matrices:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-5 & -15 & -16 & -7\n\\end{pmatrix}\n$$\n$$\nB = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix}\n$$\n$$\nC = \\begin{pmatrix}\n1 & 0 & 0 & 0\n\\end{pmatrix}\n$$\n$$\nD = [0]\n$$\nThis completes the construction of the controllable companion-form realization. The state dimension is $n=4$.\n\nPart 2: Verification of minimality.\nA realization is minimal if and only if it is both controllable and observable. The dimension of a minimal realization is equal to the order of the transfer function, provided there are no pole-zero cancellations. Here, $N(s)=1$, so there are no zeros to cancel the poles of the system. The denominator is $D(s)=(s+1)^2(s^2+5s+5)$, which has four roots. Thus, the minimal dimension is $4$. Our realization has dimension $4$, and we must verify its minimality by checking controllability and observability from their fundamental definitions.\n\nControllability:\nThe controllability matrix is $\\mathcal{C} = \\begin{bmatrix} B & AB & A^2B & A^3B \\end{bmatrix}$. The system is controllable if $\\operatorname{rank}(\\mathcal{C}) = n = 4$.\nWe compute the necessary vectors:\n$B = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n$AB = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ -5 & -15 & -16 & -7 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -7 \\end{pmatrix}$\n$A^2B = A(AB) = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ -5 & -15 & -16 & -7 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -7 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -7 \\\\ 33 \\end{pmatrix}$\n$A^3B = A(A^2B) = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ -5 & -15 & -16 & -7 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -7 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -7 \\\\ 33 \\\\ -134 \\end{pmatrix}$\n\nThe controllability matrix is:\n$$\n\\mathcal{C} = \\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & -7 \\\\\n0 & 1 & -7 & 33 \\\\\n1 & -7 & 33 & -134\n\\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{C})$. By cofactor expansion, one finds $\\det(\\mathcal{C}) = -1$. Since $\\det(\\mathcal{C}) \\neq 0$, the matrix has full rank, $\\operatorname{rank}(\\mathcal{C}) = 4$. The realization is controllable, as expected from this canonical form.\n\nObservability:\nThe observability matrix is $\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\end{pmatrix}$. The system is observable if $\\operatorname{rank}(\\mathcal{O}) = n = 4$.\nWe compute the necessary row vectors:\n$C = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}$\n$CA = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} A = \\begin{pmatrix} 0 & 1 & 0 & 0 \\end{pmatrix}$\n$CA^2 = (CA)A = \\begin{pmatrix} 0 & 1 & 0 & 0 \\end{pmatrix} A = \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix}$\n$CA^3 = (CA^2)A = \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix} A = \\begin{pmatrix} 0 & 0 & 0 & 1 \\end{pmatrix}$\n\nThe observability matrix is:\n$$\n\\mathcal{O} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} = I_4\n$$\nwhere $I_4$ is the $4 \\times 4$ identity matrix. The determinant is $\\det(\\mathcal{O}) = \\det(I_4) = 1$. Since $\\det(\\mathcal{O}) \\neq 0$, the matrix has full rank, $\\operatorname{rank}(\\mathcal{O}) = 4$. The realization is observable.\n\nSince the realization is both controllable and observable, it is a minimal realization.\n\nPart 3: Computation of the determinant of the observability matrix.\nAs calculated in the course of the observability verification, the observability matrix is the identity matrix $I_4$.\n$$\n\\mathcal{O} = I_4\n$$\nThe determinant is therefore:\n$$\n\\det(\\mathcal{O}) = \\det(I_4) = 1\n$$\nThe problem requires this value to be reported as the final answer.", "answer": "$$\n\\boxed{1}\n$$", "id": "2749421"}, {"introduction": "Once a state-space model is established, its primary purpose is to predict the system's dynamic behavior over time. This practice problem focuses on applying the fundamental solution to linear state equations, also known as the variation of constants formula, to compute the exact state trajectory for a system with a given initial condition and a piecewise constant input [@problem_id:2749393]. Tackling this problem will build your intuition for how the state-transition matrix and the convolution integral work together to govern system response.", "problem": "Consider the linear time-invariant (LTI) state-space system with state $\\;x(t)\\in\\mathbb{R}^{2}\\;$ and output $\\;y(t)\\in\\mathbb{R}\\;$ defined by\n$$\\dot{x}(t)=A\\,x(t)+B\\,u(t),\\qquad y(t)=C\\,x(t),$$\nwith\n$$A=\\begin{pmatrix}-1 & 1\\\\ 0 & -2\\end{pmatrix},\\quad B=\\begin{pmatrix}0\\\\ 1\\end{pmatrix},\\quad C=\\begin{pmatrix}1 & 0\\end{pmatrix},\\quad x(0)=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}.$$\nThe input is piecewise constant over the interval $\\;[0,2]\\;$,\n$$u(t)=\\begin{cases}2,& 0\\le t<1,\\\\ 0,& 1\\le t\\le 2.\\end{cases}$$\nStarting from first principles for linear systems, including the homogeneous solution and the integral representation that follows from the definition of the matrix exponential, derive closed-form expressions for $\\;x(t)\\;$ and $\\;y(t)\\;$ on $\\;[0,1]\\;$ and on $\\;[1,2]\\;$, accounting for the discontinuity of $\\;u(t)\\;$ at $\\;t=1\\;$. Finally, provide the exact closed-form value of $\\;y(2)\\;$ as your final answer. Do not approximate; report the exact analytic expression.", "solution": "The problem presented is a standard exercise in linear control theory. It is well-posed, scientifically sound, and complete. Therefore, a solution can be derived.\n\nThe system is described by the linear time-invariant state-space equations:\n$$ \\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t) + B u(t) $$\n$$ y(t) = C \\mathbf{x}(t) $$\nwith given matrices $A$, $B$, $C$, initial state $\\mathbf{x}(0)$, and a piecewise constant input $u(t)$. The solution for the state vector $\\mathbf{x}(t)$ starting from an initial state $\\mathbf{x}(t_0)$ at time $t_0$ is given by the variation of parameters formula:\n$$ \\mathbf{x}(t) = e^{A(t-t_0)} \\mathbf{x}(t_0) + \\int_{t_0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau $$\nwhere $e^{At}$ is the state-transition matrix, also known as the matrix exponential.\n\nFirst, we must compute the matrix exponential $e^{At}$ for the given matrix $A = \\begin{pmatrix} -1 & 1 \\\\ 0 & -2 \\end{pmatrix}$. The matrix $A$ is upper triangular, so its eigenvalues are its diagonal entries, $\\lambda_1 = -1$ and $\\lambda_2 = -2$. Since the eigenvalues are distinct, $A$ is diagonalizable. We can compute $e^{At}$ using the formula $e^{At} = P e^{Dt} P^{-1}$, where $D$ is the diagonal matrix of eigenvalues and $P$ is the matrix of corresponding eigenvectors.\nThe eigenvector for $\\lambda_1 = -1$ is found by solving $(A - \\lambda_1 I)v_1 = 0$:\n$$ \\begin{pmatrix} 0 & 1 \\\\ 0 & -1 \\end{pmatrix} v_1 = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $$\nThe eigenvector for $\\lambda_2 = -2$ is found by solving $(A - \\lambda_2 I)v_2 = 0$:\n$$ \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} v_2 = 0 \\implies v_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nThe modal matrix $P$ and its inverse $P^{-1}$ are:\n$$ P = \\begin{pmatrix} 1 & 1 \\\\ 0 & -1 \\end{pmatrix}, \\quad P^{-1} = \\frac{1}{-1} \\begin{pmatrix} -1 & -1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & -1 \\end{pmatrix} $$\nThe diagonal matrix of eigenvalues is $D = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$, so $e^{Dt} = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix}$.\nThe matrix exponential is then:\n$$ e^{At} = P e^{Dt} P^{-1} = \\begin{pmatrix} 1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & -1 \\end{pmatrix} $$\n$$ e^{At} = \\begin{pmatrix} e^{-t} & e^{-2t} \\\\ 0 & -e^{-2t} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} e^{-t} & e^{-t} - e^{-2t} \\\\ 0 & e^{-2t} \\end{pmatrix} $$\n\nThe solution is determined in two parts, corresponding to the piecewise definition of the input $u(t)$.\n\nPart 1: Solution on the interval $t \\in [0, 1)$\nFor this interval, $t_0 = 0$, $\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and the input is constant, $u(t) = 2$.\nThe solution is $\\mathbf{x}(t) = e^{At} \\mathbf{x}(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau$.\nThe homogeneous part is:\n$$ e^{At} \\mathbf{x}(0) = \\begin{pmatrix} e^{-t} & e^{-t} - e^{-2t} \\\\ 0 & e^{-2t} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} e^{-t} - e^{-2t} \\\\ e^{-2t} \\end{pmatrix} $$\nThe particular part (convolution integral) is:\n$$ \\int_{0}^{t} e^{A(t-\\tau)} B (2) d\\tau = 2 \\int_{0}^{t} \\begin{pmatrix} e^{-(t-\\tau)} & e^{-(t-\\tau)} - e^{-2(t-\\tau)} \\\\ 0 & e^{-2(t-\\tau)} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} d\\tau $$\n$$ = 2 \\int_{0}^{t} \\begin{pmatrix} e^{-(t-\\tau)} - e^{-2(t-\\tau)} \\\\ e^{-2(t-\\tau)} \\end{pmatrix} d\\tau = 2 \\int_{0}^{t} \\begin{pmatrix} e^{-t}e^{\\tau} - e^{-2t}e^{2\\tau} \\\\ e^{-2t}e^{2\\tau} \\end{pmatrix} d\\tau $$\n$$ = 2 \\begin{pmatrix} e^{-t}[e^{\\tau}]_{0}^{t} - e^{-2t}[\\frac{1}{2}e^{2\\tau}]_{0}^{t} \\\\ e^{-2t}[\\frac{1}{2}e^{2\\tau}]_{0}^{t} \\end{pmatrix} = 2 \\begin{pmatrix} e^{-t}(e^{t} - 1) - \\frac{1}{2} e^{-2t}(e^{2t} - 1) \\\\ \\frac{1}{2} e^{-2t}(e^{2t} - 1) \\end{pmatrix} $$\n$$ = \\begin{pmatrix} 2(1 - e^{-t}) - (1 - e^{-2t}) \\\\ 1 - e^{-2t} \\end{pmatrix} = \\begin{pmatrix} 1 - 2e^{-t} + e^{-2t} \\\\ 1 - e^{-2t} \\end{pmatrix} $$\nCombining the homogeneous and particular solutions for $t \\in [0, 1)$:\n$$ \\mathbf{x}(t) = \\begin{pmatrix} e^{-t} - e^{-2t} \\\\ e^{-2t} \\end{pmatrix} + \\begin{pmatrix} 1 - 2e^{-t} + e^{-2t} \\\\ 1 - e^{-2t} \\end{pmatrix} = \\begin{pmatrix} 1 - e^{-t} \\\\ 1 \\end{pmatrix} $$\nThe output for this interval is $y(t) = C \\mathbf{x}(t)$:\n$$ y(t) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 - e^{-t} \\\\ 1 \\end{pmatrix} = 1 - e^{-t} $$\n\nPart 2: Solution on the interval $t \\in [1, 2]$\nFor this interval, the initial time is $t_0 = 1$. The state $\\mathbf{x}(t)$ must be continuous, so the initial condition for this interval is the value of the state at $t=1$, calculated from the solution in Part 1.\n$$ \\mathbf{x}(1) = \\lim_{t \\to 1^{-}} \\mathbf{x}(t) = \\begin{pmatrix} 1 - e^{-1} \\\\ 1 \\end{pmatrix} $$\nIn this interval, the input is $u(t) = 0$, so the system is homogeneous: $\\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t)$. The solution is:\n$$ \\mathbf{x}(t) = e^{A(t-1)} \\mathbf{x}(1) $$\nSubstituting the expressions for $e^{A(t-1)}$ and $\\mathbf{x}(1)$:\n$$ \\mathbf{x}(t) = \\begin{pmatrix} e^{-(t-1)} & e^{-(t-1)} - e^{-2(t-1)} \\\\ 0 & e^{-2(t-1)} \\end{pmatrix} \\begin{pmatrix} 1 - e^{-1} \\\\ 1 \\end{pmatrix} $$\n$$ \\mathbf{x}(t) = \\begin{pmatrix} (1 - e^{-1})e^{-(t-1)} + (e^{-(t-1)} - e^{-2(t-1)}) \\\\ e^{-2(t-1)} \\end{pmatrix} $$\nThe first component, $x_1(t)$, is:\n$$ x_1(t) = e^{-t+1} - e^{-t} + e^{-t+1} - e^{-2t+2} = 2e^{-t+1} - e^{-t} - e^{-2t+2} $$\n$$ x_1(t) = (2e - 1)e^{-t} - e^2 e^{-2t} $$\nSo the state vector for $t \\in [1, 2]$ is:\n$$ \\mathbf{x}(t) = \\begin{pmatrix} (2e - 1)e^{-t} - e^2 e^{-2t} \\\\ e^2 e^{-2t} \\end{pmatrix} $$\nThe output for this interval is $y(t) = C\\mathbf{x}(t) = x_1(t)$:\n$$ y(t) = (2e - 1)e^{-t} - e^2 e^{-2t} $$\n\nFinally, we must find the exact value of $y(2)$. We substitute $t=2$ into the expression for $y(t)$ on the interval $[1, 2]$:\n$$ y(2) = (2e - 1)e^{-2} - e^2 e^{-2(2)} $$\n$$ y(2) = (2e - 1)e^{-2} - e^2 e^{-4} $$\n$$ y(2) = 2e \\cdot e^{-2} - e^{-2} - e^2 \\cdot e^{-4} $$\n$$ y(2) = 2e^{-1} - e^{-2} - e^{-2} $$\n$$ y(2) = 2e^{-1} - 2e^{-2} $$\nUsing the exponential function notation as per convention:\n$$ y(2) = 2\\exp(-1) - 2\\exp(-2) $$", "answer": "$$ \\boxed{2\\exp(-1) - 2\\exp(-2)} $$", "id": "2749393"}, {"introduction": "Not all state-space realizations are created equal; many may contain states that are either disconnected from the input (uncontrollable) or hidden from the output (unobservable). This exercise guides you through the Kalman decomposition, a powerful technique for systematically analyzing the structure of any linear system [@problem_id:2749383]. By partitioning the state space into its four fundamental subspaces, you will learn how to identify and isolate the minimal core of the system that fully captures its input-output behavior.", "problem": "Consider the continuous-time linear time-invariant (LTI) single-input single-output (SISO) state-space system with a known nonminimal realization given by the quadruple $\\left(A,B,C,D\\right)$:\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n-2 & -3 & 0 & 0 & 0 \\\\\n0 & 0 & -4 & 0 & 0 \\\\\n0 & 0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 0 & 2\n\\end{bmatrix},\\quad\nB \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nC \\;=\\; \\begin{bmatrix} 1 & 0 & 0 & 1 & 0 \\end{bmatrix},\\quad\nD \\;=\\; 0.\n$$\nStarting only from first principles in control theory, namely the definitions of the controllable subspace generated by the iterates of $A$ acting on $B$, the observable subspace generated by iterates of $A^{\\top}$ acting on $C^{\\top}$, and the Kalman decomposition that simultaneously decomposes the state-space into controllable/uncontrollable and observable/unobservable components via a similarity transformation, carry out the following:\n\n- Identify the controllable subspace and the observable subspace of the given realization without invoking any pre-packaged theorems beyond these definitions. Justify which state coordinates are uncontrollable, unobservable, both, or neither.\n- Explain how a similarity transformation can be chosen to obtain the Kalman canonical form, and use this to remove all uncontrollable and unobservable states to obtain a minimal realization $\\left(A_{\\min},B_{\\min},C_{\\min},D_{\\min}\\right)$ that preserves the input-output map.\n\nFinally, let $G(s)$ be the transfer function of the minimal realization. Compute the zero-frequency gain $G(0)$ as an exact value. Report only the scalar value of $G(0)$ as your final answer. Do not round; provide the exact rational number.", "solution": "The problem presented is a standard exercise in linear systems theory and is well-posed, scientifically grounded, and internally consistent. It provides all necessary information to proceed with a rigorous analysis. Therefore, the problem is valid, and a full solution can be derived.\n\nThe state-space representation is given by the matrices:\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n-2 & -3 & 0 & 0 & 0 \\\\\n0 & 0 & -4 & 0 & 0 \\\\\n0 & 0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 0 & 2\n\\end{bmatrix},\\quad\nB \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nC \\;=\\; \\begin{bmatrix} 1 & 0 & 0 & 1 & 0 \\end{bmatrix},\\quad\nD \\;=\\; 0\n$$\nThe state vector is $\\mathbf{x}(t) \\in \\mathbb{R}^5$.\n\n**1. Controllability Analysis**\n\nAccording to first principles, the controllable subspace, $\\mathcal{C}$, is the range of the controllability matrix $\\mathcal{M}_c$, defined as $\\mathcal{M}_c = \\begin{bmatrix} B & AB & A^2B & A^3B & A^4B \\end{bmatrix}$. We compute the first few columns to determine the rank of $\\mathcal{M}_c$.\n$$\nB = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\n$$\nAB = \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n-2 & -3 & 0 & 0 & 0 \\\\\n0 & 0 & -4 & 0 & 0 \\\\\n0 & 0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 0 & 2\n\\end{bmatrix}\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ -3 \\\\ -4 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\n$$\nA^2B = A(AB) = \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n-2 & -3 & 0 & 0 & 0 \\\\\n0 & 0 & -4 & 0 & 0 \\\\\n0 & 0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 0 & 2\n\\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ -3 \\\\ -4 \\\\ 0 \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} -3 \\\\ 7 \\\\ 16 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\nThe first three columns of $\\mathcal{M}_c$ are:\n$$\n\\begin{bmatrix} B & AB & A^2B \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & -3 \\\\ 1 & -3 & 7 \\\\ 1 & -4 & 16 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n$$\nThe determinant of the upper $3 \\times 3$ submatrix is $0( -3 \\cdot 16 - 7 \\cdot (-4) ) - 1(1 \\cdot 16 - 7 \\cdot 1) + (-3)(1 \\cdot (-4) - (-3) \\cdot 1) = -9 -3(-1) = -6 \\neq 0$. Thus, these three vectors are linearly independent. All subsequent vectors $A^k B$ for $k \\ge 0$ will have zero entries in their last two components because of the block-diagonal structure of $A$. Therefore, the rank of $\\mathcal{M}_c$ is $3$.\nThe controllable subspace $\\mathcal{C}$ is the span of these vectors, which is the subspace of $\\mathbb{R}^5$ where the fourth and fifth components are zero.\n$$\n\\mathcal{C} = \\text{span}\\left\\{\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n\\right\\} = \\text{span}\\{e_1, e_2, e_3\\}\n$$\nThe dimension of the controllable subspace is $n_c = 3$. The states associated with the directions $e_4$ and $e_5$ are uncontrollable.\n\n**2. Observability Analysis**\n\nThe observable subspace, $\\mathcal{O}$, is defined as the orthogonal complement of the unobservable subspace, $\\mathcal{U}$. The unobservable subspace $\\mathcal{U}$ is the null space of the observability matrix $\\mathcal{M}_o$:\n$$\n\\mathcal{M}_o = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\\\ CA^4 \\end{bmatrix}\n$$\nA state vector $\\mathbf{v} \\in \\mathbb{R}^5$ is unobservable if and only if $\\mathbf{v} \\in \\mathcal{N}(\\mathcal{M}_o)$, which means $CA^k \\mathbf{v} = 0$ for all $k \\ge 0$. We check the conditions for $\\mathbf{v} = [v_1, v_2, v_3, v_4, v_5]^\\top$.\n$C = \\begin{bmatrix} 1 & 0 & 0 & 1 & 0 \\end{bmatrix}$, so $C\\mathbf{v} = v_1 + v_4 = 0$.\nNext, we compute $CA$:\n$$\nCA = \\begin{bmatrix} 1 & 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n-2 & -3 & 0 & 0 & 0 \\\\\n0 & 0 & -4 & 0 & 0 \\\\\n0 & 0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 0 & 2\n\\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & -5 & 0 \\end{bmatrix}\n$$\nSo, $CA\\mathbf{v} = v_2 - 5v_4 = 0$.\nNext, $CA^2$:\n$$\nCA^2 = (CA)A = \\begin{bmatrix} 0 & 1 & 0 & -5 & 0 \\end{bmatrix} A = \\begin{bmatrix} -2 & -3 & 0 & 25 & 0 \\end{bmatrix}\n$$\nSo, $CA^2\\mathbf{v} = -2v_1 - 3v_2 + 25v_4 = 0$.\nSubstituting $v_1 = -v_4$ and $v_2 = 5v_4$ into the third equation gives:\n$$\n-2(-v_4) - 3(5v_4) + 25v_4 = 2v_4 - 15v_4 + 25v_4 = 12v_4 = 0 \\implies v_4 = 0\n$$\nThis implies $v_1=0$ and $v_2=0$. The equations place no constraints on $v_3$ and $v_5$. A vector $\\mathbf{v}$ is unobservable if and only if $v_1=v_2=v_4=0$.\nThus, the unobservable subspace $\\mathcal{U}$ is spanned by $e_3$ and $e_5$:\n$$\n\\mathcal{U} = \\text{span}\\left\\{\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\right\\} = \\text{span}\\{e_3, e_5\\}\n$$\nThe dimension of the unobservable subspace is $n_{\\bar{o}} = 2$.\nThe observable subspace $\\mathcal{O}$ is the orthogonal complement of $\\mathcal{U}$, hence $\\mathcal{O} = \\mathcal{U}^\\perp = \\text{span}\\{e_1, e_2, e_4\\}$. The dimension of the observable subspace is $n_o = 3$. The states associated with the directions $e_3$ and $e_5$ are unobservable.\n\n**3. Kalman Decomposition and Minimal Realization**\n\nThe state space $\\mathbb{R}^5$ can be decomposed into a direct sum of four subspaces: controllable and observable ($\\mathcal{S}_{co}$), controllable and unobservable ($\\mathcal{S}_{c\\bar{o}}$), uncontrollable and observable ($\\mathcal{S}_{\\bar{c}o}$), and uncontrollable and unobservable ($\\mathcal{S}_{\\bar{c}\\bar{o}}$).\n\nUsing the subspaces found above:\n- $\\mathcal{S}_{co} = \\mathcal{C} \\cap \\mathcal{O} = \\text{span}\\{e_1, e_2, e_3\\} \\cap \\text{span}\\{e_1, e_2, e_4\\} = \\text{span}\\{e_1, e_2\\}$. This part is 2-dimensional.\n- $\\mathcal{S}_{c\\bar{o}} = \\mathcal{C} \\cap \\mathcal{U} = \\text{span}\\{e_1, e_2, e_3\\} \\cap \\text{span}\\{e_3, e_5\\} = \\text{span}\\{e_3\\}$. This part is 1-dimensional.\n- $\\mathcal{S}_{\\bar{c}o} = \\mathcal{C}^\\perp \\cap \\mathcal{O} = \\text{span}\\{e_4, e_5\\} \\cap \\text{span}\\{e_1, e_2, e_4\\} = \\text{span}\\{e_4\\}$. This part is 1-dimensional.\n- $\\mathcal{S}_{\\bar{c}\\bar{o}} = \\mathcal{C}^\\perp \\cap \\mathcal{U} = \\text{span}\\{e_4, e_5\\} \\cap \\text{span}\\{e_3, e_5\\} = \\text{span}\\{e_5\\}$. This part is 1-dimensional.\n\nSummary of states:\n- $x_1, x_2$ (directions $e_1, e_2$): controllable and observable.\n- $x_3$ (direction $e_3$): controllable but unobservable.\n- $x_4$ (direction $e_4$): uncontrollable but observable.\n- $x_5$ (direction $e_5$): uncontrollable and unobservable.\n\nA similarity transformation $T$ is used to transform the state coordinates $\\mathbf{x}$ to $\\mathbf{z}=T^{-1}\\mathbf{x}$ such that the system matrices $(\\bar{A}, \\bar{B}, \\bar{C}) = (T^{-1}AT, T^{-1}B, CT)$ are in a block-triangular form known as the Kalman canonical form. The columns of $T$ are constructed by forming an ordered basis for the state space, with basis vectors chosen sequentially from $\\mathcal{S}_{co}$, $\\mathcal{S}_{c\\bar{o}}$, $\\mathcal{S}_{\\bar{c}o}$, and $\\mathcal{S}_{\\bar{c}\\bar{o}}$.\n\nFor this problem, the standard basis vectors $\\{e_1, \\dots, e_5\\}$ almost align with this decomposition. We reorder them to match the Kalman structure: $\\{e_1, e_2, e_3, e_4, e_5\\}$. The system is already in the required order. We can choose the transformation matrix to be the identity matrix, $T=I_5$. This means the given system is already in Kalman canonical form.\nThe transfer function of the system is $G(s) = C(sI-A)^{-1}B+D$. Since the system is already decomposed, we have:\n$$\nG(s) = \\begin{bmatrix} C_{co} & C_{c\\bar{o}} & C_{\\bar{c}o} & C_{\\bar{c}\\bar{o}} \\end{bmatrix} \\begin{bmatrix} (sI-A_{co})^{-1} & \\dots \\\\ 0 & (sI-A_{c\\bar{o}})^{-1} & \\dots \\\\ 0 & 0 & (sI-A_{\\bar{c}o})^{-1} & \\dots \\\\ 0 & 0 & 0 & (sI-A_{\\bar{c}\\bar{o}})^{-1} \\end{bmatrix} \\begin{bmatrix} B_{co} \\\\ B_{c\\bar{o}} \\\\ B_{\\bar{c}o} \\\\ B_{\\bar{c}\\bar{o}} \\end{bmatrix}\n$$\nThe structure of these matrices (specifically, the zero blocks in $\\bar{B}$ and $\\bar{C}$ for certain subspaces) guarantees that contributions from the unobservable and uncontrollable parts cancel out in the input-output map. Multiplication yields:\n$$\nG(s) = C_{co} (sI - A_{co})^{-1} B_{co}\n$$\nThe minimal realization consists only of the controllable and observable part of the system. The corresponding state matrices are obtained by partitioning the original matrices according to the subspaces:\n$$\nA_{\\min} = A_{co} = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix}\n$$\n$$\nB_{\\min} = B_{co} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n$$\nC_{\\min} = C_{co} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\n$$\n$$\nD_{\\min} = D = 0\n$$\n\n**4. Zero-Frequency Gain Calculation**\n\nThe zero-frequency gain $G(0)$ is obtained by setting $s=0$ in the transfer function of the minimal realization:\n$$\nG(0) = C_{\\min}(0 \\cdot I - A_{\\min})^{-1}B_{\\min} = -C_{\\min}A_{\\min}^{-1}B_{\\min}\n$$\nFirst, we find the inverse of $A_{\\min}$:\n$$\nA_{\\min} = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix}\n$$\nThe determinant is $\\det(A_{\\min}) = (0)(-3) - (1)(-2) = 2$.\nThe inverse is:\n$$\nA_{\\min}^{-1} = \\frac{1}{\\det(A_{\\min})} \\begin{bmatrix} -3 & -1 \\\\ 2 & 0 \\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix} -3 & -1 \\\\ 2 & 0 \\end{bmatrix}\n$$\nNow, we compute $G(0)$:\n$$\nG(0) = - \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\left( \\frac{1}{2} \\begin{bmatrix} -3 & -1 \\\\ 2 & 0 \\end{bmatrix} \\right) \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n$$\nG(0) = -\\frac{1}{2} \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} -3 & -1 \\\\ 2 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n$$\nG(0) = -\\frac{1}{2} \\begin{bmatrix} -3 & -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n$$\nG(0) = -\\frac{1}{2} ((-3)(0) + (-1)(1)) = -\\frac{1}{2}(-1) = \\frac{1}{2}\n$$\nThe zero-frequency gain is an exact value of $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2749383"}]}