{"hands_on_practices": [{"introduction": "The negative feedback loop is the cornerstone of control theory. This exercise takes you back to first principles, guiding you to derive the canonical closed-loop transfer function not from memory, but from the fundamental algebraic rules of block diagram components. By then calculating the system's poles as a function of gain $k$, you will directly connect the abstract algebra of block diagrams to the concrete analysis of system stability and dynamic behavior [@problem_id:2690577].", "problem": "Consider a single-input single-output negative-unity-feedback configuration of a linear time-invariant (LTI) system. The forward-path plant has transfer function $G(s)=\\dfrac{k}{s(s+1)}$, where $k \\in \\mathbb{R}$ is a real design parameter, the feedback path is the identity $H(s)=1$, and $s$ is the complex Laplace variable. Starting only from the definitions of a summing junction, the meaning of a transfer function for an LTI block, and the algebraic rules for series and feedback interconnections in block diagrams, perform the equivalent block reduction of the loop to obtain the closed-loop transfer function $T(s)=\\dfrac{Y(s)}{R(s)}$ from the reference $R(s)$ to the output $Y(s)$ as a function of $k$ and $s$. Then, determine the closed-loop pole locations as functions of $k$ by exact symbolic calculation. Report the two pole locations as an ordered pair in your final answer. No numerical approximation is required, and no units are needed in the final answer.", "solution": "The problem statement is evaluated and found to be valid. It is a standard, well-posed problem in classical control theory, free of scientific or logical inconsistencies. We shall proceed with the derivation as requested.\n\nThe system is a negative-unity-feedback configuration. Let $R(s)$ be the Laplace transform of the reference input signal and $Y(s)$ be the Laplace transform of the output signal. The error signal, $E(s)$, is produced by a summing junction which subtracts the feedback signal, $B(s)$, from the reference, $R(s)$. The algebraic relationship is:\n$$E(s) = R(s) - B(s)$$\nThe forward path consists of a plant with transfer function $G(s)$. The relationship between its input $E(s)$ and its output $Y(s)$ is:\n$$Y(s) = G(s) E(s)$$\nThe feedback path has a transfer function $H(s)$. Its input is the system output $Y(s)$ and its output is the feedback signal $B(s)$:\n$$B(s) = H(s) Y(s)$$\nTo derive the closed-loop transfer function $T(s) = \\dfrac{Y(s)}{R(s)}$, we must combine these fundamental equations to eliminate the internal signals $E(s)$ and $B(s)$. We start by substituting the expression for $B(s)$ into the equation for $E(s)$:\n$$E(s) = R(s) - H(s) Y(s)$$\nNext, we substitute this expression for $E(s)$ into the equation for the output $Y(s)$:\n$$Y(s) = G(s) [R(s) - H(s) Y(s)]$$\nWe distribute $G(s)$ on the right-hand side:\n$$Y(s) = G(s) R(s) - G(s) H(s) Y(s)$$\nTo solve for the ratio $\\dfrac{Y(s)}{R(s)}$, we collect all terms involving $Y(s)$ on one side of the equation:\n$$Y(s) + G(s) H(s) Y(s) = G(s) R(s)$$\nFactoring out $Y(s)$ leads to:\n$$Y(s) [1 + G(s) H(s)] = G(s) R(s)$$\nFinally, dividing by $R(s)$ and by the term in the brackets yields the general expression for the closed-loop transfer function of a negative feedback system:\n$$T(s) = \\dfrac{Y(s)}{R(s)} = \\dfrac{G(s)}{1 + G(s) H(s)}$$\nThis derivation is rigorous and based only on the defined algebraic rules for the block diagram components.\n\nNow, we substitute the specific transfer functions given in the problem: $G(s) = \\dfrac{k}{s(s+1)}$ and $H(s) = 1$.\n$$T(s) = \\dfrac{\\dfrac{k}{s(s+1)}}{1 + \\left(\\dfrac{k}{s(s+1)}\\right)(1)}$$\nTo simplify this complex fraction, we find a common denominator for the terms in the main denominator:\n$$T(s) = \\dfrac{\\dfrac{k}{s(s+1)}}{\\dfrac{s(s+1)}{s(s+1)} + \\dfrac{k}{s(s+1)}} = \\dfrac{\\dfrac{k}{s(s+1)}}{\\dfrac{s(s+1) + k}{s(s+1)}}$$\nSimplifying the fraction by canceling the common term $s(s+1)$ from the numerator's denominator and the denominator's denominator gives:\n$$T(s) = \\dfrac{k}{s(s+1) + k}$$\nExpanding the polynomial in the denominator results in the final form of the closed-loop transfer function:\n$$T(s) = \\dfrac{k}{s^2 + s + k}$$\nThe closed-loop poles are, by definition, the roots of the characteristic equation, which is obtained by setting the denominator of the closed-loop transfer function to zero. The characteristic equation for this system is:\n$$s^2 + s + k = 0$$\nThis is a quadratic equation in the complex variable $s$ of the form $as^2 + bs + c = 0$, with coefficients $a=1$, $b=1$, and $c=k$. The roots are found using the quadratic formula:\n$$s = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\nSubstituting the coefficients, we obtain the locations of the two poles:\n$$s = \\dfrac{-(1) \\pm \\sqrt{(1)^2 - 4(1)(k)}}{2(1)}$$\n$$s = \\dfrac{-1 \\pm \\sqrt{1 - 4k}}{2}$$\nThe two distinct symbolic expressions for the pole locations are therefore $s_1 = \\dfrac{-1 + \\sqrt{1 - 4k}}{2}$ and $s_2 = \\dfrac{-1 - \\sqrt{1 - 4k}}{2}$. These are the required poles as functions of the real parameter $k$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{-1 + \\sqrt{1 - 4k}}{2} & \\frac{-1 - \\sqrt{1 - 4k}}{2} \\end{pmatrix}}$$", "id": "2690577"}, {"introduction": "For systems with intricate interconnections, step-by-step block reduction can become cumbersome and error-prone. This practice introduces Mason’s Gain Formula, a powerful and systematic method for determining the transfer function of any complex linear signal-flow graph [@problem_id:2690600]. By applying the formula to a system with multiple non-touching loops and then verifying the result via traditional block reduction, you will develop a deeper appreciation for the structure of feedback systems and gain proficiency in a versatile analytical tool.", "problem": "Consider a causal linear time-invariant (LTI) single-input single-output system described by the following Signal Flow Graph (SFG) structure. There is a source node carrying the Laplace-domain input signal $R(s)$ that feeds a summing node $n_0$. From $n_0$, two strictly parallel forward branches carry signals to the output summing node $n_5$, which produces the Laplace-domain output $Y(s)$. The two branches are as follows.\n\n- Branch 1 (upper branch): The signal at $n_0$ passes through a block with transfer function $G_1(s)$ to node $n_1$, then through a block with transfer function $G_2(s)$ to node $n_2$. Node $n_2$ connects directly to the output summing node $n_5$. There is a local negative feedback from $n_2$ back to $n_1$ through a block with transfer function $H_1(s)$, entering the summing junction at $n_1$ with negative sign. Thus the algebraic sum at $n_1$ is the forward signal from $G_1(s)$ minus the feedback signal from $H_1(s)$.\n\n- Branch 2 (lower branch): The signal at $n_0$ passes through a block with transfer function $G_3(s)$ to node $n_3$, then through a block with transfer function $G_4(s)$ to node $n_4$. Node $n_4$ connects directly to the output summing node $n_5$. There is a local negative feedback from $n_4$ back to $n_3$ through a block with transfer function $H_2(s)$, entering the summing junction at $n_3$ with negative sign. Thus the algebraic sum at $n_3$ is the forward signal from $G_3(s)$ minus the feedback signal from $H_2(s)$.\n\nThere are no other interconnections. All transfer functions $G_1(s)$, $G_2(s)$, $G_3(s)$, $G_4(s)$, $H_1(s)$, and $H_2(s)$ are proper rational functions with real coefficients, and the graph contains exactly two loops that are non-touching: the loop within Branch 1 formed by $G_2(s)$ and $H_1(s)$, and the loop within Branch 2 formed by $G_4(s)$ and $H_2(s)$. The output node $n_5$ simply sums the contributions from $n_2$ and $n_4$.\n\nTasks:\n- Using the definitions of Signal Flow Graph (SFG) loops, forward paths, and non-touching loops, compute the overall transfer function $T(s) = \\dfrac{Y(s)}{R(s)}$ by applying Mason’s Gain Formula (MGF). Do not assume any simplifications beyond the structural description given.\n- Independently verify your result by direct block-diagram reduction using only series, parallel, and negative feedback interconnection identities justified from the linearity of the nodes and the superposition principle.\n\nExpress your final answer as a single closed-form symbolic expression for $T(s)$ in terms of $G_1(s)$, $G_2(s)$, $G_3(s)$, $G_4(s)$, $H_1(s)$, and $H_2(s)$. No numerical evaluation or rounding is required, and no physical units are to be reported with the final expression.", "solution": "The analysis will be conducted in two parts as required: first using Mason's Gain Formula, and second using algebraic block-diagram reduction.\n\n**Method 1: Mason’s Gain Formula (MGF)**\n\nMason's Gain Formula is given by $T(s) = \\frac{1}{\\Delta(s)} \\sum_{k} P_k(s) \\Delta_k(s)$, where $P_k$ are the forward path gains, $\\Delta$ is the graph determinant, and $\\Delta_k$ are the path cofactors.\n\n1.  **Forward Paths ($P_k$)**: A forward path is a path from the input node $n_0$ to the output node $n_5$ that does not traverse any node more than once.\n    -   Path 1 ($P_1$): Through the upper branch, $n_0 \\rightarrow n_1 \\rightarrow n_2 \\rightarrow n_5$. The path gain is the product of the branch gains: $P_1(s) = G_1(s) G_2(s) \\cdot 1 = G_1(s)G_2(s)$.\n    -   Path 2 ($P_2$): Through the lower branch, $n_0 \\rightarrow n_3 \\rightarrow n_4 \\rightarrow n_5$. The path gain is: $P_2(s) = G_3(s) G_4(s) \\cdot 1 = G_3(s)G_4(s)$.\n    There are two forward paths.\n\n2.  **Individual Loops ($L_i$)**: A loop is a path that starts and ends at the same node.\n    -   Loop 1 ($L_1$): The feedback loop in the upper branch, $n_1 \\rightarrow n_2 \\rightarrow n_1$. The gain of this loop is $L_1(s) = G_2(s) \\cdot (-H_1(s)) = -G_2(s)H_1(s)$. The negative sign comes from the specified negative feedback.\n    -   Loop 2 ($L_2$): The feedback loop in the lower branch, $n_3 \\rightarrow n_4 \\rightarrow n_3$. The gain of this loop is $L_2(s) = G_4(s) \\cdot (-H_2(s)) = -G_4(s)H_2(s)$.\n    There are two loops. As stated in the problem, these loops are non-touching because they do not share any nodes. Loop $L_1$ involves nodes $\\{n_1, n_2\\}$ and loop $L_2$ involves nodes $\\{n_3, n_4\\}$.\n\n3.  **Graph Determinant ($\\Delta$)**: The determinant is calculated as $\\Delta = 1 - (\\sum L_i) + (\\sum L_i L_j) - \\dots$.\n    -   Sum of individual loop gains: $\\sum L_i = L_1(s) + L_2(s) = -G_2(s)H_1(s) - G_4(s)H_2(s)$.\n    -   Sum of gain products of non-touching loop pairs: There is only one pair of non-touching loops, ($L_1, L_2$). The product is $L_1(s)L_2(s) = (-G_2(s)H_1(s))(-G_4(s)H_2(s)) = G_2(s)H_1(s)G_4(s)H_2(s)$.\n    -   Therefore, the determinant is:\n        $$ \\Delta(s) = 1 - (L_1(s) + L_2(s)) + (L_1(s)L_2(s)) $$\n        $$ \\Delta(s) = 1 - (-G_2(s)H_1(s) - G_4(s)H_2(s)) + G_2(s)H_1(s)G_4(s)H_2(s) $$\n        $$ \\Delta(s) = 1 + G_2(s)H_1(s) + G_4(s)H_2(s) + G_2(s)H_1(s)G_4(s)H_2(s) $$\n        This can be factored as $\\Delta(s) = (1 + G_2(s)H_1(s))(1 + G_4(s)H_2(s))$.\n\n4.  **Path Cofactors ($\\Delta_k$)**: $\\Delta_k$ is the determinant of the subgraph formed by removing all loops that touch the $k$-th forward path.\n    -   For path $P_1$ (nodes $n_0, n_1, n_2, n_5$): This path touches loop $L_1$ (at nodes $n_1, n_2$) but does not touch loop $L_2$. Therefore, $\\Delta_1$ is calculated with only loop $L_2$.\n        $$ \\Delta_1(s) = 1 - L_2(s) = 1 - (-G_4(s)H_2(s)) = 1 + G_4(s)H_2(s) $$\n    -   For path $P_2$ (nodes $n_0, n_3, n_4, n_5$): This path touches loop $L_2$ (at nodes $n_3, n_4$) but does not touch loop $L_1$. Therefore, $\\Delta_2$ is calculated with only loop $L_1$.\n        $$ \\Delta_2(s) = 1 - L_1(s) = 1 - (-G_2(s)H_1(s)) = 1 + G_2(s)H_1(s) $$\n\n5.  **Total Transfer Function ($T(s)$)**:\n    $$ T(s) = \\frac{P_1(s)\\Delta_1(s) + P_2(s)\\Delta_2(s)}{\\Delta(s)} $$\n    $$ T(s) = \\frac{G_1(s)G_2(s)(1 + G_4(s)H_2(s)) + G_3(s)G_4(s)(1 + G_2(s)H_1(s))}{(1 + G_2(s)H_1(s))(1 + G_4(s)H_2(s))} $$\n\n**Method 2: Block-Diagram Reduction**\n\nThe system structure consists of two independent branches connected in parallel. The input $R(s)$ is fed to both branches, and their outputs are summed at node $n_5$ to produce $Y(s)$. By the principle of superposition, the overall transfer function $T(s)$ is the sum of the transfer functions of the two parallel branches, $T_1(s)$ and $T_2(s)$.\n$$ T(s) = T_1(s) + T_2(s) $$\n\n1.  **Transfer Function of Branch 1 ($T_1(s)$)**:\n    This branch has a block $G_1(s)$ in series with a feedback structure. The feedback loop involves the forward transfer function $G_2(s)$ and the feedback transfer function $H_1(s)$. The standard formula for a negative feedback loop is $\\frac{G}{1+GH}$.\n    -   The transfer function of the feedback portion (from node $n_1$ to node $n_2$) is $\\frac{G_2(s)}{1+G_2(s)H_1(s)}$.\n    -   This feedback system is in series with the preceding block $G_1(s)$. The transfer function of series components is their product.\n    -   Therefore, the transfer function of the entire upper branch is:\n        $$ T_1(s) = G_1(s) \\cdot \\frac{G_2(s)}{1 + G_2(s)H_1(s)} = \\frac{G_1(s)G_2(s)}{1 + G_2(s)H_1(s)} $$\n\n2.  **Transfer Function of Branch 2 ($T_2(s)$)**:\n    This branch is structurally identical to the first. By analogy, we replace $G_1, G_2, H_1$ with $G_3, G_4, H_2$.\n    -   The transfer function of the feedback portion is $\\frac{G_4(s)}{1+G_4(s)H_2(s)}$.\n    -   This is in series with $G_3(s)$.\n    -   Therefore, the transfer function of the entire lower branch is:\n        $$ T_2(s) = G_3(s) \\cdot \\frac{G_4(s)}{1 + G_4(s)H_2(s)} = \\frac{G_3(s)G_4(s)}{1 + G_4(s)H_2(s)} $$\n\n3.  **Total Transfer Function ($T(s)$)**:\n    Summing the transfer functions of the two parallel branches gives:\n    $$ T(s) = T_1(s) + T_2(s) = \\frac{G_1(s)G_2(s)}{1 + G_2(s)H_1(s)} + \\frac{G_3(s)G_4(s)}{1 + G_4(s)H_2(s)} $$\n    To verify this against the MGF result, we can place this expression over a common denominator:\n    $$ T(s) = \\frac{G_1(s)G_2(s)(1 + G_4(s)H_2(s))}{(1 + G_2(s)H_1(s))(1 + G_4(s)H_2(s))} + \\frac{G_3(s)G_4(s)(1 + G_2(s)H_1(s))}{(1 + G_2(s)H_1(s))(1 + G_4(s)H_2(s))} $$\n    $$ T(s) = \\frac{G_1(s)G_2(s)(1 + G_4(s)H_2(s)) + G_3(s)G_4(s)(1 + G_2(s)H_1(s))}{(1 + G_2(s)H_1(s))(1 + G_4(s)H_2(s))} $$\nThis expression is identical to the one derived using Mason's Gain Formula. The independent verification is successful. The structure of the final expression confirms that the total system response is the sum of the responses of two independent, parallel subsystems.", "answer": "$$ \\boxed{\\frac{G_{1}(s)G_{2}(s)(1 + G_{4}(s)H_{2}(s)) + G_{3}(s)G_{4}(s)(1 + G_{2}(s)H_{1}(s))}{(1 + G_{2}(s)H_{1}(s))(1 + G_{4}(s)H_{2}(s))}} $$", "id": "2690600"}, {"introduction": "Moving from single-variable (SISO) to multi-variable (MIMO) systems requires an elevation in mathematical rigor, as signals become vectors and blocks become matrices. This exercise challenges you to apply the principles of block diagram algebra in a MIMO context, where dimensional consistency is paramount [@problem_id:2690565]. You will learn to meticulously track signal dimensions through the system, identify incompatibilities at summing junctions, and engineer corrective embedding matrices, a critical skill for the formal modeling of complex, real-world systems.", "problem": "Consider a linear time-invariant interconnection of multiple-input multiple-output (MIMO) blocks represented by constant real matrices. Signals are real column vectors, and each summing junction requires that all incident signals have the same dimension. Each pickoff point duplicates a signal without altering its dimension. For a matrix block $G \\in \\mathbb{R}^{p \\times q}$, the input must be a vector in $\\mathbb{R}^{q}$ and the output is a vector in $\\mathbb{R}^{p}$. The following blocks and signals are interconnected:\n\n- External reference $r \\in \\mathbb{R}^{2}$ is pickoff-split into $r_{a}$ and $r_{b}$, each in $\\mathbb{R}^{2}$.\n- Block $G_{1} \\in \\mathbb{R}^{3 \\times 2}$ produces $y_{1} = G_{1} r_{a} \\in \\mathbb{R}^{3}$.\n- An error signal $e$ is formed by a negative-feedback summing junction attempting to compute $e = r_{b} - f$.\n- Block $G_{2} \\in \\mathbb{R}^{2 \\times 3}$ produces the control $u = G_{2} e \\in \\mathbb{R}^{2}$.\n- Plant $P \\in \\mathbb{R}^{3 \\times 2}$ produces $y_{2} = P u \\in \\mathbb{R}^{3}$.\n- Sensor block $H \\in \\mathbb{R}^{3 \\times 3}$ produces $f = H y_{2} \\in \\mathbb{R}^{3}$.\n- A pickoff of $u$ excites an auxiliary block $W \\in \\mathbb{R}^{2 \\times 2}$ producing $w = W u \\in \\mathbb{R}^{2}$.\n- The final output is formed by a summing junction attempting to compute $y = y_{1} + w$.\n\nTo preserve a canonical interpretation that the $2$-dimensional reference lives in the first two components of the $3$-dimensional ambient space, define the fixed selection matrix $S \\in \\mathbb{R}^{2 \\times 3}$ by\n$$S = \\begin{bmatrix} I_{2} & 0_{2 \\times 1} \\end{bmatrix}$$\nwhich selects the first two components of a $3$-vector. You may insert constant linear embedding matrices $E_{r} \\in \\mathbb{R}^{3 \\times 2}$ and $E_{w} \\in \\mathbb{R}^{3 \\times 2}$ only at the inputs of the two summing junctions to resolve any dimension mismatches, subject to the constraints\n- $S E_{r} = I_{2}$ and $S E_{w} = I_{2}$ (the embeddings preserve the first two components when selected), and\n- the embeddings must not inject any artificial third-component content; that is, the third row of each embedding must be the zero row.\n\nFrom first principles of block diagram algebra and matrix compatibility, determine which option correctly\n(i) formalizes all signal dimensions,\n(ii) identifies every dimension mismatch, and\n(iii) provides embedding matrices that resolve all mismatches while satisfying the stated constraints.\n\nChoose the single best option.\n\nA. Signal dimensions are $r, r_{a}, r_{b}, u, w \\in \\mathbb{R}^{2}$ and $y_{1}, y_{2}, f, e, y \\in \\mathbb{R}^{3}$. The only mismatches occur at the error junction $e = r_{b} - f$ and the output junction $y = y_{1} + w$. Insert embeddings $E_{r}, E_{w} \\in \\mathbb{R}^{3 \\times 2}$ to form $e = E_{r} r_{b} - f$ and $y = y_{1} + E_{w} w$ with\n$$\nE_{r} = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix}, \n\\quad\nE_{w} = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix}.\n$$\nThese satisfy $S E_{r} = I_{2}$ and $S E_{w} = I_{2}$ and do not inject third-component content.\n\nB. Signal dimensions are $r, r_{a}, r_{b}, u, w, e \\in \\mathbb{R}^{2}$ and $y_{1}, y_{2}, f, y \\in \\mathbb{R}^{3}$. There is no mismatch at the error junction because define $e = r_{b} - S f$, but a mismatch exists at the output junction $y = y_{1} + w$. Resolve only the output mismatch by $y = y_{1} + E_{w} w$ with $E_{w} = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix}$. No embedding is needed at the error junction.\n\nC. Signal dimensions are $r, r_{a}, r_{b}, u, w \\in \\mathbb{R}^{2}$ and $y_{1}, y_{2}, f, e, y \\in \\mathbb{R}^{3}$. The only mismatch is at the error junction. Resolve it with $e = E_{r} r_{b} - f$ using\n$$\nE_{r} = \\begin{bmatrix} 0_{1 \\times 2} \\\\ I_{2} \\end{bmatrix},\n$$\nwhich preserves two components but in the last two rows. No embedding is needed at the output junction.\n\nD. Signal dimensions are $r, r_{a}, r_{b}, u, w \\in \\mathbb{R}^{2}$ and $y_{1}, y_{2}, f, e, y \\in \\mathbb{R}^{3}$. Mismatches occur at both summing junctions. Resolve the error junction by $e = E_{r} r_{b} - f$ with\n$$\nE_{r} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix},\n$$\nand resolve the output junction by $y = y_{1} + E_{w} w$ with\n$$\nE_{w} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nThese embeddings permute the two input components into the first two coordinates of the $3$-dimensional space.", "solution": "### Derivation of Solution\n\nThe solution requires a three-step process: (i) determine the dimension of every signal in the diagram, (ii) identify any dimension mismatches at the summing junctions, and (iii) find the specific embedding matrices that resolve the mismatches while satisfying all given constraints.\n\n**Part (i): Formalize Signal Dimensions**\n\nWe trace the dimensions of each signal through the block diagram, starting from the input $r$.\n\n1.  The reference signal is given as $r \\in \\mathbb{R}^{2}$.\n2.  The pickoff points create copies, so $r_{a} \\in \\mathbb{R}^{2}$ and $r_{b} \\in \\mathbb{R}^{2}$.\n3.  The signal $y_{1}$ is the output of block $G_{1} \\in \\mathbb{R}^{3 \\times 2}$ with input $r_{a} \\in \\mathbb{R}^{2}$. The resulting dimension is $(3 \\times 2) \\times (2 \\times 1) \\rightarrow (3 \\times 1)$. Thus, $y_{1} \\in \\mathbb{R}^{3}$.\n4.  To determine the dimension of other signals in the feedback loop, we must work from the known dimensions of the blocks. The block $G_{2} \\in \\mathbb{R}^{2 \\times 3}$ produces the output $u = G_{2} e$. For this matrix-vector product to be defined, the input signal $e$ must have a dimension of $3 \\times 1$. Thus, $e \\in \\mathbb{R}^{3}$.\n5.  With $e \\in \\mathbb{R}^{3}$, the control signal $u = G_{2} e$ has dimension $(2 \\times 3) \\times (3 \\times 1) \\rightarrow (2 \\times 1)$. Thus, $u \\in \\mathbb{R}^{2}$.\n6.  The plant output $y_{2} = P u$ is produced by block $P \\in \\mathbb{R}^{3 \\times 2}$ with input $u \\in \\mathbb{R}^{2}$. The dimension is $(3 \\times 2) \\times (2 \\times 1) \\rightarrow (3 \\times 1)$. Thus, $y_{2} \\in \\mathbb{R}^{3}$.\n7.  The sensor output $f = H y_{2}$ is produced by block $H \\in \\mathbb{R}^{3 \\times 3}$ with input $y_{2} \\in \\mathbb{R}^{3}$. The dimension is $(3 \\times 3) \\times (3 \\times 1) \\rightarrow (3 \\times 1)$. Thus, $f \\in \\mathbb{R}^{3}$.\n8.  The signal $w = W u$ is produced by block $W \\in \\mathbb{R}^{2 \\times 2}$ with input $u \\in \\mathbb{R}^{2}$ (from a pickoff). The dimension is $(2 \\times 2) \\times (2 \\times 1) \\rightarrow (2 \\times 1)$. Thus, $w \\in \\mathbb{R}^{2}$.\n9.  The final output $y$ is formed by summing $y_{1} \\in \\mathbb{R}^{3}$ and a signal derived from $w \\in \\mathbb{R}^{2}$. For the sum to be possible, both operands must have the same dimension. Hence, $y$ will be in $\\mathbb{R}^{3}$.\n\n**Summary of Dimensions:**\n*   Signals in $\\mathbb{R}^{2}$: $r, r_{a}, r_{b}, u, w$.\n*   Signals in $\\mathbb{R}^{3}$: $y_{1}, y_{2}, f, e, y$.\n\n**Part (ii): Identify Dimension Mismatches**\n\nWe examine the two summing junctions based on the dimensions derived above.\n\n1.  **Error Junction**: The operation is defined as $e = r_{b} - f$. We found $r_{b} \\in \\mathbb{R}^{2}$ and $f \\in \\mathbb{R}^{3}$. Vector subtraction is only defined for vectors of the same dimension. A dimension mismatch exists at this junction.\n2.  **Output Junction**: The operation is defined as $y = y_{1} + w$. We found $y_{1} \\in \\mathbb{R}^{3}$ and $w \\in \\mathbb{R}^{2}$. Vector addition is only defined for vectors of the same dimension. A dimension mismatch also exists at this junction.\n\nTherefore, mismatches occur at both the error junction and the output junction.\n\n**Part (iii): Provide and Validate Embedding Matrices**\n\nTo resolve the mismatches, we introduce embedding matrices $E_{r}$ and $E_{w}$, both in $\\mathbb{R}^{3 \\times 2}$, as specified.\nThe junction equations become:\n*   $e = E_{r} r_{b} - f$\n*   $y = y_{1} + E_{w} w$\nIn both cases, the embedding matrix maps a vector from $\\mathbb{R}^{2}$ to $\\mathbb{R}^{3}$, resolving the mismatch.\n\nThese matrices must satisfy two constraints:\n1.  The third row of the matrix must be $0_{1 \\times 2}$.\n2.  $S E = I_{2}$, where $S = \\begin{bmatrix} I_{2} & 0_{2 \\times 1} \\end{bmatrix}$.\n\nLet $E \\in \\mathbb{R}^{3 \\times 2}$ be a generic embedding matrix. We can write it as $E = \\begin{bmatrix} E_{top} \\\\ E_{bottom} \\end{bmatrix}$ where $E_{top}$ is a $2 \\times 2$ matrix and $E_{bottom}$ is a $1 \\times 2$ row vector.\n\nFrom constraint 1, we must have $E_{bottom} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}$.\nSo, $E = \\begin{bmatrix} E_{top} \\\\ 0_{1 \\times 2} \\end{bmatrix}$.\n\nNow we apply constraint 2:\n$$ S E = \\begin{bmatrix} I_{2} & 0_{2 \\times 1} \\end{bmatrix} \\begin{bmatrix} E_{top} \\\\ 0_{1 \\times 2} \\end{bmatrix} = I_{2} E_{top} + 0_{2 \\times 1} 0_{1 \\times 2} = E_{top} $$\nThe constraint requires $S E = I_{2}$. Therefore, we must have $E_{top} = I_{2}$.\n\nCombining these results, the only possible matrix $E$ that satisfies all constraints is:\n$$ E = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} $$\nThis result must hold for both $E_{r}$ and $E_{w}$. Thus, the correct resolution is:\n$$ E_{r} = E_{w} = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix} $$\n\n### Option-by-Option Analysis\n\n*   **Option A**:\n    *   (i) States signal dimensions are $r, r_{a}, r_{b}, u, w \\in \\mathbb{R}^{2}$ and $y_{1}, y_{2}, f, e, y \\in \\mathbb{R}^{3}$. This is **Correct**.\n    *   (ii) States mismatches occur at the error junction ($e = r_b - f$) and the output junction ($y = y_1 + w$). This is **Correct**.\n    *   (iii) Proposes resolving with $e = E_{r} r_{b} - f$ and $y = y_{1} + E_{w} w$ using $E_{r} = E_{w} = \\begin{bmatrix} I_{2} \\\\ 0_{1 \\times 2} \\end{bmatrix}$. This matches our derivation precisely. It also correctly notes that these matrices satisfy the constraints. This is **Correct**.\n    *   **Verdict**: **Correct**.\n\n*   **Option B**:\n    *   States $e \\in \\mathbb{R}^{2}$. This is fundamentally incorrect, as the input to $G_{2} \\in \\mathbb{R}^{2 \\times 3}$ must be in $\\mathbb{R}^{3}$. The suggestion to redefine the error as $e = r_{b} - S f$ is an arbitrary change to the diagram structure, not an embedding at a junction input, and it leads to a subsequent dimension mismatch between the new $e \\in \\mathbb{R}^2$ and the input of block $G_2$.\n    *   **Verdict**: **Incorrect**.\n\n*   **Option C**:\n    *   States that the only mismatch is at the error junction. This is incorrect; a mismatch also exists at the output junction.\n    *   Proposes an embedding matrix $E_{r} = \\begin{bmatrix} 0_{1 \\times 2} \\\\ I_{2} \\end{bmatrix}$, which is $\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. This matrix violates the constraint that the third row must be zero. It also violates the constraint $S E_r = I_2$, since $S E_r = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\end{bmatrix} \\neq I_2$.\n    *   **Verdict**: **Incorrect**.\n\n*   **Option D**:\n    *   Correctly identifies the signal dimensions and the locations of the two mismatches.\n    *   Proposes the embedding matrix $E = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$. While this matrix satisfies the zero third-row constraint, it fails the second constraint: $$S E = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\neq I_2$$. This matrix permutes the input vector components, which is not what the constraints demand. The constraints demand a canonical embedding.\n    *   **Verdict**: **Incorrect**.\n\nBased on rigorous analysis, only option A provides a completely correct description of the system, its dimensional conflicts, and the constrained resolution.", "answer": "$$\\boxed{A}$$", "id": "2690565"}]}