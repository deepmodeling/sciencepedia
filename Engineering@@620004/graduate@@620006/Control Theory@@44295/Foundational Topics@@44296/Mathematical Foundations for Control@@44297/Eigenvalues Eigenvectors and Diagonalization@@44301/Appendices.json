{"hands_on_practices": [{"introduction": "Understanding a non-diagonalizable system begins with its most fundamental building block, the nilpotent matrix. This exercise [@problem_id:2704108] guides you through the analysis of a simple $3 \\times 3$ nilpotent matrix, which is itself a Jordan block. By constructing its Jordan chain from first principles, you will gain a concrete intuition for generalized eigenvectors and see how the matrix's structure leads to a finite polynomial form for its state-transition matrix, $\\exp(At)$.", "problem": "Consider the state matrix $A \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nThis matrix arises naturally in controllable canonical forms and nilpotent approximations in linear time-invariant (LTI) systems. Work with the following steps, grounding your arguments in first principles: the definitions of eigenvalue, eigenvector, algebraic and geometric multiplicity, and the power-series definition of the matrix exponential.\n\n1. Using the characteristic polynomial and the definition of eigenvectors, verify that $A$ is defective with a single eigenvalue $\\lambda = 0$ of algebraic multiplicity $3$ and geometric multiplicity $1$.\n\n2. Determine the Jordan canonical form of $A$ and construct an explicit Jordan chain of generalized eigenvectors $\\{v_1, v_2, v_3\\}$ satisfying $A v_1 = 0$, $A v_2 = v_1$, and $A v_3 = v_2$.\n\n3. In the context of the homogeneous state equation $\\dot{x}(t) = A x(t)$, the state transition matrix is given by the matrix exponential $\\exp(A t)$, defined by its power series. Using your results from parts $1$ and $2$, derive a closed-form expression for the entry in row $1$, column $3$ of $\\exp(A t)$ as a function of $t \\ge 0$.\n\nAnswer specification: Provide only the final closed-form analytical expression for the entry in row $1$, column $3$ of $\\exp(A t)$ as your final answer. No units are required. Do not round.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard problem in linear systems theory and is based on fundamental principles of linear algebra. All necessary information is provided, and the problem is internally consistent. We shall proceed with the solution.\n\nThe problem asks for a three-part analysis of the state matrix $A$ and the corresponding state transition matrix $\\exp(At)$. We will address each part in sequence, adhering strictly to first principles.\n\nFirst, we verify that the matrix $A$ is defective with a single eigenvalue $\\lambda = 0$. The eigenvalues are the roots of the characteristic polynomial, $p(\\lambda) = \\det(A - \\lambda I)$.\nFor the given matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nthe characteristic matrix is\n$$\nA - \\lambda I \\;=\\; \\begin{pmatrix}\n-\\lambda & 1 & 0 \\\\\n0 & -\\lambda & 1 \\\\\n0 & 0 & -\\lambda\n\\end{pmatrix}.\n$$\nThis is an upper triangular matrix, so its determinant is the product of its diagonal entries:\n$$\np(\\lambda) = \\det(A - \\lambda I) = (-\\lambda)(-\\lambda)(-\\lambda) = -\\lambda^3.\n$$\nSetting the characteristic polynomial to zero, $p(\\lambda) = 0$, gives $-\\lambda^3 = 0$, which has a single root $\\lambda = 0$ with multiplicity $3$. Therefore, the algebraic multiplicity of the eigenvalue $\\lambda = 0$ is $m_a(0) = 3$.\n\nNext, we determine the geometric multiplicity, $m_g(0)$, which is the dimension of the eigenspace corresponding to $\\lambda = 0$. The eigenspace is the null space of $(A - 0I) = A$. We seek all vectors $v = \\begin{pmatrix} v_x \\\\ v_y \\\\ v_z \\end{pmatrix}$ such that $Av = 0$:\n$$\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\nv_z\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThis matrix-vector equation is equivalent to the system of linear equations $v_y = 0$ and $v_z = 0$. The variable $v_x$ is unconstrained, so it is a free variable. Any eigenvector must be of the form\n$$\nv \\;=\\; \\begin{pmatrix} v_x \\\\ 0 \\\\ 0 \\end{pmatrix} \\;=\\; v_x \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe eigenspace for $\\lambda=0$ is the span of the single vector $\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}^T$. The dimension of this space is $1$. Thus, the geometric multiplicity is $m_g(0) = 1$. Since $m_g(0) = 1 < m_a(0) = 3$, the matrix $A$ is defective, as stated.\n\nSecond, we determine the Jordan canonical form $J$ and a corresponding Jordan chain. Since there is one eigenvalue $\\lambda = 0$ with algebraic multiplicity $3$ and geometric multiplicity $1$, there must be a single Jordan block of size $3 \\times 3$ corresponding to this eigenvalue. The Jordan form is therefore\n$$\nJ \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nWe observe that the matrix $A$ is already in its Jordan canonical form, so $A=J$. We must now construct a Jordan chain of generalized eigenvectors $\\{v_1, v_2, v_3\\}$ satisfying the relations $Av_1 = 0$, $Av_2 = v_1$, and $Av_3 = v_2$. These are equivalent to $(A-\\lambda I)v_1 = 0$, $(A-\\lambda I)v_2 = v_1$, and $(A-\\lambda I)v_3 = v_2$ for $\\lambda=0$. The vector $v_1$ is a true eigenvector. The vector $v_3$ is a generalized eigenvector of rank $3$, meaning $(A-0I)^2 v_3 \\neq 0$ but $(A-0I)^3 v_3 = 0$. Let's compute the powers of $A$:\n$$\nA^2 \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\n$$\nA^3 \\;=\\; A^2 A \\;=\\; \\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nWe need to choose $v_3$ such that $A^2 v_3 \\neq 0$. Let $v_3 = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix}$. The condition becomes $A^2 v_3 = \\begin{pmatrix} c_3 \\\\ 0 \\\\ 0 \\end{pmatrix} \\neq 0$, which requires $c_3 \\neq 0$. For simplicity, let's choose $v_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nNow we generate the rest of the chain:\n$$\nv_2 = Av_3 = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{pmatrix}.\n$$\n$$\nv_1 = Av_2 = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nWe verify that $v_1$ is an eigenvector: $Av_1 = A \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\cdot v_1$. The chain $\\{v_1, v_2, v_3\\}$ is explicitly $\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\}$.\n\nThird, we derive the closed-form expression for the entry in row $1$, column $3$ of $\\exp(At)$. The state transition matrix is defined by the matrix exponential power series:\n$$\n\\exp(At) = \\sum_{k=0}^{\\infty} \\frac{(At)^k}{k!} = I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\dots\n$$\nSince $A$ is nilpotent with $A^3=0$, all powers $A^k$ for $k \\ge 3$ are the zero matrix. Consequently, the infinite series for $\\exp(At)$ truncates:\n$$\n\\exp(At) = I + At + \\frac{A^2 t^2}{2!}.\n$$\nSubstituting the matrices $I$, $A$, and $A^2$:\n$$\n\\exp(At) = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n+ t \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n+ \\frac{t^2}{2} \\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nSumming these matrices term by term gives the final expression for the state transition matrix:\n$$\n\\exp(At) = \\begin{pmatrix}\n1 & t & \\frac{t^2}{2} \\\\\n0 & 1 & t \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThe entry in row $1$, column $3$ of $\\exp(At)$ is therefore $\\frac{t^2}{2}$.", "answer": "$$\n\\boxed{\\frac{t^2}{2}}\n$$", "id": "2704108"}, {"introduction": "The distinction between a diagonalizable matrix and a defective one has profound consequences for a system's dynamic behavior. This practice [@problem_id:2704026] makes this abstract concept tangible by comparing the step responses of two systems with identical eigenvalues but different eigenstructures. You will derive how the Jordan block structure introduces a polynomial-in-time mode, of the form $t\\exp(\\lambda t)$, which is absent in the diagonalizable case, providing a clear 'dynamic signature' of defectiveness.", "problem": "Consider two continuous-time Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) systems with state-space realization given by $\\dot{x}(t)=A x(t)+B u(t)$ and $y(t)=C x(t)$, where $x(t)\\in\\mathbb{R}^{2}$, $u(t)\\in\\mathbb{R}$, and $y(t)\\in\\mathbb{R}$. Both systems share the same input and output matrices $B=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ and $C=\\begin{pmatrix}1 & 1\\end{pmatrix}$, zero initial condition $x(0)=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, and a unit-step input $u(t)=1$ for $t\\ge 0$. The systems differ only in their state matrix $A$:\n- System $\\mathcal{J}$ has a double eigenvalue at $-1$ with a size-$2$ Jordan block, $A_{\\mathcal{J}}=\\begin{pmatrix}-1 & 1 \\\\ 0 & -1\\end{pmatrix}$.\n- System $\\mathcal{D}$ is diagonalizable with the same eigenvalues, $A_{\\mathcal{D}}=\\begin{pmatrix}-1 & 0 \\\\ 0 & -1\\end{pmatrix}$.\n\nStarting from the definition of the matrix exponential as the uniformly convergent series $ \\exp(A t)=\\sum_{k=0}^{\\infty}\\frac{(A t)^{k}}{k!}$ and the state-transition formula for the zero-initial-condition forced response $x(t)=\\int_{0}^{t}\\exp\\!\\big(A (t-\\tau)\\big) B u(\\tau)\\,d\\tau$, derive explicit expressions for the step responses $y_{\\mathcal{J}}(t)$ and $y_{\\mathcal{D}}(t)$ for $t\\ge 0$, and explain, using only these foundational definitions, why any difference you observe arises from the eigenstructure of $A$. \n\nProvide, as your final answer, a single closed-form analytic expression for the difference $y_{\\mathcal{J}}(t)-y_{\\mathcal{D}}(t)$ for $t\\ge 0$. No rounding is required. The signals are dimensionless; do not include units in your final answer.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of linear time-invariant systems theory, is well-posed with all necessary information provided, and is formulated objectively. We will proceed with the derivation as required.\n\nThe task is to compute and compare the step responses of two systems, $\\mathcal{D}$ and $\\mathcal{J}$, which share the same eigenvalues but differ in their eigenstructure. We must derive the results from the foundational definitions of the matrix exponential and the state-transition formula.\n\nThe state response for a system with zero initial condition is given by\n$$x(t) = \\int_{0}^{t} \\exp(A(t-\\tau)) B u(\\tau) \\,d\\tau$$\nThe output is $y(t)=Cx(t)$. The input is a unit step, so $u(\\tau)=1$ for $\\tau \\ge 0$. The matrices $B$ and $C$ are given as $B=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ and $C=\\begin{pmatrix}1 & 1\\end{pmatrix}$.\n\nFirst, we analyze System $\\mathcal{D}$, which is diagonalizable.\nThe state matrix is $A_{\\mathcal{D}} = \\begin{pmatrix}-1 & 0 \\\\ 0 & -1\\end{pmatrix} = -I$, where $I$ is the $2 \\times 2$ identity matrix.\nWe compute the matrix exponential $\\exp(A_{\\mathcal{D}}t)$ using its series definition:\n$$ \\exp(A_{\\mathcal{D}}t) = \\sum_{k=0}^{\\infty} \\frac{(A_{\\mathcal{D}}t)^k}{k!} $$\nSince $A_{\\mathcal{D}} = -I$, we have $(A_{\\mathcal{D}}t)^k = ((-I)t)^k = (-t)^k I^k = (-t)^k I$.\nThe series becomes:\n$$ \\exp(A_{\\mathcal{D}}t) = \\sum_{k=0}^{\\infty} \\frac{(-t)^k I}{k!} = I \\left( \\sum_{k=0}^{\\infty} \\frac{(-t)^k}{k!} \\right) = I \\exp(-t) = \\begin{pmatrix} \\exp(-t) & 0 \\\\ 0 & \\exp(-t) \\end{pmatrix} $$\nNow we find the state vector $x_{\\mathcal{D}}(t)$:\n$$ x_{\\mathcal{D}}(t) = \\int_{0}^{t} \\exp(A_{\\mathcal{D}}(t-\\tau)) B \\cdot 1 \\,d\\tau = \\int_{0}^{t} \\begin{pmatrix} \\exp(-(t-\\tau)) & 0 \\\\ 0 & \\exp(-(t-\\tau)) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\,d\\tau $$\n$$ x_{\\mathcal{D}}(t) = \\int_{0}^{t} \\begin{pmatrix} 0 \\\\ \\exp(-(t-\\tau)) \\end{pmatrix} \\,d\\tau = \\begin{pmatrix} \\int_{0}^{t} 0 \\,d\\tau \\\\ \\int_{0}^{t} \\exp(-t+\\tau) \\,d\\tau \\end{pmatrix} $$\nThe lower component is computed as:\n$$ \\int_{0}^{t} \\exp(-t)\\exp(\\tau) \\,d\\tau = \\exp(-t) [\\exp(\\tau)]_{0}^{t} = \\exp(-t) (\\exp(t) - \\exp(0)) = \\exp(-t)(\\exp(t) - 1) = 1 - \\exp(-t) $$\nSo, the state vector is $x_{\\mathcal{D}}(t) = \\begin{pmatrix} 0 \\\\ 1 - \\exp(-t) \\end{pmatrix}$.\nThe output response $y_{\\mathcal{D}}(t)$ is:\n$$ y_{\\mathcal{D}}(t) = C x_{\\mathcal{D}}(t) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 - \\exp(-t) \\end{pmatrix} = 0 + (1 - \\exp(-t)) = 1 - \\exp(-t) $$\n\nNext, we analyze System $\\mathcal{J}$, which has a Jordan block structure.\nThe state matrix is $A_{\\mathcal{J}} = \\begin{pmatrix}-1 & 1 \\\\ 0 & -1\\end{pmatrix}$. We can write this as a sum of a diagonal and a nilpotent matrix: $A_{\\mathcal{J}} = -I + N$, where $N = \\begin{pmatrix}0 & 1 \\\\ 0 & 0\\end{pmatrix}$.\nThe matrix $N$ is nilpotent of order $2$, as $N^2 = \\begin{pmatrix}0 & 0 \\\\ 0 & 0\\end{pmatrix} = O$. All higher powers $N^k$ for $k \\ge 2$ are also the zero matrix.\nTo compute $\\exp(A_{\\mathcal{J}}t)$ from the series definition, we first find an expression for $(A_{\\mathcal{J}}t)^k$:\n$$ (A_{\\mathcal{J}}t)^k = ((-I+N)t)^k = t^k(-I+N)^k $$\nSince $-I$ and $N$ commute, we can apply the binomial theorem:\n$$ (-I+N)^k = \\sum_{j=0}^{k} \\binom{k}{j} (-I)^{k-j} N^j = \\binom{k}{0}(-I)^k N^0 + \\binom{k}{1}(-I)^{k-1} N^1 + \\sum_{j=2}^{k} \\binom{k}{j} (-I)^{k-j} N^j $$\nAs $N^j=O$ for $j \\ge 2$, the sum truncates:\n$$ (-I+N)^k = 1 \\cdot (-1)^k I + k \\cdot (-1)^{k-1} I^{k-1} N = (-1)^k I + k(-1)^{k-1}N $$\nSubstituting into the exponential series:\n$$ \\exp(A_{\\mathcal{J}}t) = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} ((-1)^k I + k(-1)^{k-1}N) = \\sum_{k=0}^{\\infty} \\frac{(-t)^k}{k!}I + \\sum_{k=0}^{\\infty} \\frac{t^k k(-1)^{k-1}}{k!}N $$\nThe first sum is $I \\exp(-t)$. The second sum is:\n$$ \\sum_{k=1}^{\\infty} \\frac{t^k (-1)^{k-1}}{(k-1)!}N = t \\sum_{k=1}^{\\infty} \\frac{t^{k-1} (-1)^{k-1}}{(k-1)!}N $$\nLet $j=k-1$. The sum becomes:\n$$ t \\sum_{j=0}^{\\infty} \\frac{(-t)^j}{j!}N = t \\exp(-t) N $$\nTherefore, the matrix exponential is:\n$$ \\exp(A_{\\mathcal{J}}t) = \\exp(-t)I + t\\exp(-t)N = \\exp(-t)(I+tN) = \\exp(-t) \\left( \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} + t\\begin{pmatrix}0 & 1 \\\\ 0 & 0\\end{pmatrix} \\right) = \\begin{pmatrix} \\exp(-t) & t\\exp(-t) \\\\ 0 & \\exp(-t) \\end{pmatrix} $$\nNow we find the state vector $x_{\\mathcal{J}}(t)$:\n$$ x_{\\mathcal{J}}(t) = \\int_{0}^{t} \\exp(A_{\\mathcal{J}}(t-\\tau)) B \\,d\\tau = \\int_{0}^{t} \\begin{pmatrix} \\exp(-(t-\\tau)) & (t-\\tau)\\exp(-(t-\\tau)) \\\\ 0 & \\exp(-(t-\\tau)) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\,d\\tau $$\n$$ x_{\\mathcal{J}}(t) = \\int_{0}^{t} \\begin{pmatrix} (t-\\tau)\\exp(-(t-\\tau)) \\\\ \\exp(-(t-\\tau)) \\end{pmatrix} \\,d\\tau $$\nLet the components of the state vector be $x_{\\mathcal{J}}(t) = \\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}$.\nThe second component $x_2(t)$ is identical to the integral solved for System $\\mathcal{D}$:\n$$ x_2(t) = \\int_{0}^{t} \\exp(-(t-\\tau)) \\,d\\tau = 1 - \\exp(-t) $$\nThe first component $x_1(t)$ is:\n$$ x_1(t) = \\int_{0}^{t} (t-\\tau)\\exp(-(t-\\tau)) \\,d\\tau $$\nLet $s = t-\\tau$, so $ds = -d\\tau$. The limits of integration change from $\\tau=0 \\rightarrow s=t$ and $\\tau=t \\rightarrow s=0$.\n$$ x_1(t) = \\int_{t}^{0} s \\exp(-s) (-ds) = \\int_{0}^{t} s \\exp(-s) \\,ds $$\nWe use integration by parts, $\\int u \\,dv = uv - \\int v \\,du$, with $u=s$ and $dv=\\exp(-s)ds$. This gives $du=ds$ and $v=-\\exp(-s)$.\n$$ x_1(t) = [-s\\exp(-s)]_{0}^{t} - \\int_{0}^{t} (-\\exp(-s)) \\,ds = -t\\exp(-t) + \\int_{0}^{t} \\exp(-s) \\,ds $$\n$$ x_1(t) = -t\\exp(-t) + [-\\exp(-s)]_{0}^{t} = -t\\exp(-t) - \\exp(-t) - (-\\exp(0)) = 1 - \\exp(-t) - t\\exp(-t) $$\nThe state vector is $x_{\\mathcal{J}}(t) = \\begin{pmatrix} 1 - \\exp(-t) - t\\exp(-t) \\\\ 1 - \\exp(-t) \\end{pmatrix}$.\nThe output response $y_{\\mathcal{J}}(t)$ is:\n$$ y_{\\mathcal{J}}(t) = C x_{\\mathcal{J}}(t) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 - \\exp(-t) - t\\exp(-t) \\\\ 1 - \\exp(-t) \\end{pmatrix} $$\n$$ y_{\\mathcal{J}}(t) = (1 - \\exp(-t) - t\\exp(-t)) + (1 - \\exp(-t)) = 2 - 2\\exp(-t) - t\\exp(-t) $$\nThe difference between the two responses is:\n$$ y_{\\mathcal{J}}(t) - y_{\\mathcal{D}}(t) = (2 - 2\\exp(-t) - t\\exp(-t)) - (1 - \\exp(-t)) = 1 - \\exp(-t) - t\\exp(-t) $$\n\nThe reason for this difference lies entirely in the eigenstructure of the matrix $A$.\nFor the diagonalizable system $\\mathcal{D}$, the matrix $A_{\\mathcal{D}}$ is a scalar multiple of the identity. The matrix exponential $\\exp(A_{\\mathcal{D}}t)$ contains only the mode $\\exp(-t)$. The state variables are dynamically uncoupled.\nFor the non-diagonalizable system $\\mathcal{J}$, the matrix $A_{\\mathcal{J}}$ cannot be reduced to a diagonal form. Its Jordan form contains a non-zero off-diagonal element, which is captured by the nilpotent matrix $N$ in the decomposition $A_{\\mathcal{J}}=-I+N$. This structural property, an algebraic multiplicity of $2$ but a geometric multiplicity of $1$ for the eigenvalue $-1$, is the critical distinction. As demonstrated through the series expansion, the nilpotency of $N$ causes the series for $\\exp(A_{\\mathcal{J}}t)$ to generate an additional term, $t\\exp(-t)N$. This term, which is absent for the diagonalizable system, introduces the mode $t\\exp(-t)$ into the state-transition matrix. This mode represents a coupling between the states. This additional term propagates through the convolution integral and the output mapping to produce the polynomial-exponential term $-t\\exp(-t)$ in the final output $y_{\\mathcal{J}}(t)$ and contributes to the difference between the two responses. This term is a direct mathematical consequence of the Jordan block structure, which is determined by the eigenstructure of $A$.\nThus, the difference in responses is a direct manifestation of the difference in the geometric multiplicity of the eigenvalues of the state matrices.", "answer": "$$\n\\boxed{1 - \\exp(-t) - t\\exp(-t)}\n$$", "id": "2704026"}, {"introduction": "Moving from system analysis to system design is a cornerstone of control engineering. This hands-on practice [@problem_id:2704124] focuses on the powerful technique of pole placement, where you will actively shape a system's dynamics by assigning its closed-loop eigenvalues. By working with a system in controllable canonical form, you will learn to compute the state-feedback gain $K$ necessary to place the system poles at desired locations, thereby dictating its stability and transient response.", "problem": "Consider a single-input linear time-invariant (LTI) system in controllable canonical form. Let the desired closed-loop characteristic polynomial be the monic cubic\n$$p_{d}(s) = s^{3} + 6 s^{2} + 11 s + 6.$$\nTask A. Starting from the definition of the characteristic polynomial of a matrix, construct the companion (also called controllable canonical) matrix $A_{d} \\in \\mathbb{R}^{3 \\times 3}$ whose characteristic polynomial is exactly $p_{d}(s)$.\nTask B. Consider the third-order single-input single-output (SISO) system in controllable canonical form\n$$\\dot{x}(t) = A_{0}\\,x(t) + B\\,u(t), \\quad A_{0} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -1 & -3 & -3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\nUsing only fundamental definitions (characteristic polynomial, eigenvalues of a matrix, and the effect of state feedback on the system matrix), derive the state-feedback gain $K \\in \\mathbb{R}^{1 \\times 3}$ in the control law $u(t) = -K\\,x(t)$ that makes the closed-loop system matrix $A_{0} - B K$ have characteristic polynomial $p_{d}(s)$. Report $K$ as your final answer in row-vector form. No numerical rounding is required.", "solution": "The problem is divided into two parts. First, we construct the target companion matrix from the desired characteristic polynomial. Second, we derive the state-feedback gain vector that transforms the given open-loop system into the desired closed-loop system.\n\nTask A: Construction of the Desired Companion Matrix $A_{d}$\n\nA system in controllable canonical form with a state matrix $A \\in \\mathbb{R}^{n \\times n}$ has a specific structure. For $n=3$, the companion matrix is given by:\n$$ A = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -a_{0} & -a_{1} & -a_{2} \\end{bmatrix} $$\nThe characteristic polynomial of such a matrix is defined as $p(s) = \\det(sI - A)$, where $s$ is a complex variable and $I$ is the $3 \\times 3$ identity matrix. We compute this determinant:\n$$ sI - A = \\begin{bmatrix} s & -1 & 0 \\\\ 0 & s & -1 \\\\ a_{0} & a_{1} & s+a_{2} \\end{bmatrix} $$\nThe determinant is found by cofactor expansion along the first column:\n$$ \\det(sI - A) = s \\begin{vmatrix} s & -1 \\\\ a_{1} & s+a_{2} \\end{vmatrix} - 0 \\cdot \\begin{vmatrix} -1 & 0 \\\\ a_{1} & s+a_{2} \\end{vmatrix} + a_{0} \\begin{vmatrix} -1 & 0 \\\\ s & -1 \\end{vmatrix} $$\n$$ \\det(sI - A) = s(s(s+a_{2}) - (-1)a_{1}) + a_{0}((-1)(-1) - (0)s) $$\n$$ \\det(sI - A) = s(s^{2} + a_{2}s + a_{1}) + a_{0} $$\n$$ p(s) = s^{3} + a_{2}s^{2} + a_{1}s + a_{0} $$\nThe desired characteristic polynomial is given as $p_{d}(s) = s^{3} + 6s^{2} + 11s + 6$.\nBy comparing the coefficients of $p(s)$ and $p_{d}(s)$, we identify the values of $a_{0}$, $a_{1}$, and $a_{2}$:\n$a_{2} = 6$\n$a_{1} = 11$\n$a_{0} = 6$\nSubstituting these values back into the general form of the companion matrix, we construct the desired matrix $A_{d}$:\n$$ A_{d} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{bmatrix} $$\nThis matrix $A_{d}$ has the characteristic polynomial $p_{d}(s)$.\n\nTask B: Derivation of the State-Feedback Gain $K$\n\nThe open-loop system is described by $\\dot{x}(t) = A_{0}x(t) + Bu(t)$, with\n$$ A_{0} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -1 & -3 & -3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} $$\nWe apply a state-feedback control law $u(t) = -Kx(t)$, where $K \\in \\mathbb{R}^{1 \\times 3}$ is the gain vector. Let $K = \\begin{bmatrix} k_{1} & k_{2} & k_{3} \\end{bmatrix}$. The closed-loop system dynamics are:\n$$ \\dot{x}(t) = A_{0}x(t) + B(-Kx(t)) = (A_{0} - BK)x(t) $$\nThe closed-loop system matrix is $A_{cl} = A_{0} - BK$. Our objective is for the characteristic polynomial of $A_{cl}$ to be $p_{d}(s)$. Since the companion matrix representation for a given characteristic polynomial is unique, this is equivalent to requiring that the closed-loop matrix $A_{cl}$ is identical to the desired companion matrix $A_{d}$ from Task A.\n$$ A_{cl} = A_{d} $$\n$$ A_{0} - BK = A_{d} $$\nThis allows us to solve for the product $BK$:\n$$ BK = A_{0} - A_{d} $$\nWe first compute the matrix product $BK$:\n$$ BK = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} k_{1} & k_{2} & k_{3} \\end{bmatrix} = \\begin{bmatrix} 0 \\cdot k_{1} & 0 \\cdot k_{2} & 0 \\cdot k_{3} \\\\ 0 \\cdot k_{1} & 0 \\cdot k_{2} & 0 \\cdot k_{3} \\\\ 1 \\cdot k_{1} & 1 \\cdot k_{2} & 1 \\cdot k_{3} \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ k_{1} & k_{2} & k_{3} \\end{bmatrix} $$\nNext, we compute the matrix difference $A_{0} - A_{d}$:\n$$ A_{0} - A_{d} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -1 & -3 & -3 \\end{bmatrix} - \\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{bmatrix} $$\n$$ A_{0} - A_{d} = \\begin{bmatrix} 0-0 & 1-1 & 0-0 \\\\ 0-0 & 0-0 & 1-1 \\\\ -1-(-6) & -3-(-11) & -3-(-6) \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 5 & 8 & 3 \\end{bmatrix} $$\nBy equating the two expressions for $BK$ and $A_{0}-A_{d}$:\n$$ \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ k_{1} & k_{2} & k_{3} \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 5 & 8 & 3 \\end{bmatrix} $$\nBy comparing the elements of these matrices, we directly find the components of the gain vector $K$:\n$k_{1} = 5$\n$k_{2} = 8$\n$k_{3} = 3$\nTherefore, the state-feedback gain vector is $K = \\begin{bmatrix} 5 & 8 & 3 \\end{bmatrix}$. This method is valid only because the system is given in controllable canonical form, which simplifies the structure of the $BK$ product, affecting only the last row of the system matrix.\n\nTo verify, the characteristic polynomial of $A_0$ is $s^3 + 3s^2 + 3s + 1 = (s+1)^3$. The open-loop eigenvalues are $\\{-1, -1, -1\\}$. The desired characteristic polynomial is $p_d(s) = s^3+6s^2+11s+6 = (s+1)(s+2)(s+3)$. The desired closed-loop eigenvalues are $\\{-1, -2, -3\\}$. The feedback law successfully moves the system's poles.", "answer": "$$ \\boxed{\\begin{pmatrix} 5 & 8 & 3 \\end{pmatrix}} $$", "id": "2704124"}]}