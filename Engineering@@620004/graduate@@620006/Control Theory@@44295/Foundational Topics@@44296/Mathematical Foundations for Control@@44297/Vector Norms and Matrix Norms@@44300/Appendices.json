{"hands_on_practices": [{"introduction": "The spectral radius $\\rho(A)$ famously governs the long-term stability of a linear system, but what does a matrix norm tell us about its behavior? This exercise explores the subtle yet critical difference between asymptotic behavior and transient performance. By constructing a matrix where the spectral radius is zero while a norm is not, you will uncover the nature of nilpotent systems and see how significant transient effects can occur even when a system is guaranteed to be stable in the long run [@problem_id:2449584].", "id": "2449584", "problem": "In a linear fixed-point iteration used in computational engineering, the iteration matrix controls stability. Construct a $2 \\times 2$ real matrix $A$ that satisfies the two properties $\\rho(A) = 0$ and $\\|A\\|_{F} = 1$, where $\\rho(A)$ denotes the spectral radius and $\\|A\\|_{F}$ denotes the Frobenius norm. Then determine the exact value of the limit\n$$\nL = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F}.\n$$\nProvide your answer as a single real number. No rounding is necessary.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It presents a solvable problem in linear algebra with direct relevance to computational engineering. We shall proceed with a rigorous solution.\n\nThe problem requires us to first understand the implications of the given properties for a $2 \\times 2$ real matrix $A$, and then to compute a limit involving its powers. The properties are:\n$1$. The spectral radius $\\rho(A) = 0$.\n$2$. The Frobenius norm $\\|A\\|_{F} = 1$.\n\nThe spectral radius $\\rho(A)$ of a matrix $A$ is defined as the maximum of the absolute values of its eigenvalues.\n$$\n\\rho(A) = \\max_{i} |\\lambda_i|\n$$\nwhere $\\lambda_i$ are the eigenvalues of $A$. For the given $2 \\times 2$ matrix $A$, let its eigenvalues be $\\lambda_1$ and $\\lambda_2$. The condition $\\rho(A) = 0$ implies that $\\max(|\\lambda_1|, |\\lambda_2|) = 0$. This is only possible if $|\\lambda_1| = 0$ and $|\\lambda_2| = 0$, which means both eigenvalues must be zero: $\\lambda_1 = \\lambda_2 = 0$.\n\nA matrix whose eigenvalues are all zero is a nilpotent matrix. The characteristic polynomial of $A$, denoted $p(\\lambda)$, is given by $p(\\lambda) = \\det(A - \\lambda I)$. Since the roots of the characteristic polynomial are the eigenvalues of the matrix, for $A$ we have:\n$$\np(\\lambda) = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = (\\lambda - 0)(\\lambda - 0) = \\lambda^2\n$$\nAccording to the Cayley-Hamilton theorem, every square matrix satisfies its own characteristic equation. Therefore, substituting the matrix $A$ into its characteristic polynomial yields the zero matrix:\n$$\np(A) = A^2 = O\n$$\nwhere $O$ is the $2 \\times 2$ zero matrix. Thus, any $2 \\times 2$ matrix $A$ with $\\rho(A)=0$ must satisfy $A^2 = O$. The second condition, $\\|A\\|_{F}=1$, ensures that $A$ is not itself the zero matrix, since $\\|O\\|_F = 0$. Therefore, $A$ is a non-zero nilpotent matrix of index $2$.\n\nAn explicit construction confirms the existence of such a matrix. Consider the matrix\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis is an upper triangular matrix, so its eigenvalues are its diagonal entries, which are $\\lambda_1 = 0$ and $\\lambda_2 = 0$. Thus, $\\rho(A) = 0$. The Frobenius norm is calculated as\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{2} \\sum_{j=1}^{2} |a_{ij}|^2} = \\sqrt{0^2 + 1^2 + 0^2 + 0^2} = \\sqrt{1} = 1\n$$\nBoth conditions are satisfied. Computing its square gives:\n$$\nA^2 = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = O\n$$\nThis confirms our deduction from the Cayley-Hamilton theorem.\n\nThe problem requires the evaluation of the limit $L = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F}$. We must analyze the sequence of matrix powers $A^k$ for integers $k \\ge 1$.\nFor $k=1$: $A^1 = A$.\nFor $k=2$: $A^2 = O$.\nFor $k=3$: $A^3 = A^2 \\cdot A = O \\cdot A = O$.\nBy induction, for any integer $k \\ge 2$, the matrix power $A^k$ is the zero matrix:\n$$\nA^k = O \\quad \\forall k \\ge 2\n$$\nNow, we consider the sequence of the Frobenius norms of these matrix powers, $\\{ \\|A^k\\|_F \\}_{k=1}^\\infty$.\nFor $k=1$, we have $\\|A^1\\|_F = \\|A\\|_F = 1$.\nFor any integer $k \\ge 2$, we have $\\|A^k\\|_F = \\|O\\|_F$. The Frobenius norm of the zero matrix is:\n$$\n\\|O\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0\n$$\nTherefore, the sequence of real numbers is $\\{1, 0, 0, 0, \\dots\\}$. The limit of this sequence as $k \\to \\infty$ is the value the terms eventually take and remain at. For any integer $k > 1$, the term in the sequence is $0$. By the formal definition of a limit, for any chosen $\\epsilon > 0$, we can select an integer $N=1$. Then, for all $k > N$, we have $\\|A^k\\|_F = 0$, and thus $|\\|A^k\\|_F - 0| = 0 < \\epsilon$.\n\nThe limit is unequivocally $0$.\n$$\nL = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F} = 0\n$$\nThe result is independent of the specific choice of matrix $A$, provided it satisfies the stated properties. The structure imposed by $\\rho(A)=0$ on a $2 \\times 2$ matrix is what dictates the result.", "answer": "$$\n\\boxed{0}\n$$"}, {"introduction": "For non-normal systems, standard norms can be much larger than the spectral radius, potentially giving a pessimistic view of system dynamics. This practice guides you through constructing a custom-weighted norm, $\\| \\cdot \\|_{P}$, that achieves the theoretical minimum value by equaling the spectral radius $\\rho(A)$. In doing so, you will confront a fundamental trade-off in control theory: the tension between achieving an analytically optimal measure and maintaining a well-conditioned, numerically stable problem representation [@problem_id:2757370].", "id": "2757370", "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ be diagonalizable, so that $A = V \\Lambda V^{-1}$ with $V$ invertible and $\\Lambda = \\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{n})$. For any symmetric positive definite matrix $P \\succ 0$, define the $P$-weighted vector norm by $\\|x\\|_{P} = \\sqrt{x^{*} P x}$ and the induced matrix norm by $\\|A\\|_{P} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{P}}{\\|x\\|_{P}}$. Using only these definitions and standard properties of quadratic forms and eigenvalues, do the following.\n\n1. Construct a family of symmetric positive definite matrices $P \\succ 0$ that are aligned with the eigenvector basis of $A$ in the sense that the columns of $V$ are mutually orthogonal with respect to the inner product $\\langle x, y \\rangle_{P} = x^{*} P y$. Prove from first principles that for any such $P$, the induced norm satisfies $\\|A\\|_{P} = \\max_{i} |\\lambda_{i}|$. Justify that this value is the minimum possible over all choices of $P$ that are aligned in this way.\n\n2. Consider the specific non-normal matrix\n$$\nA = \\begin{bmatrix}\n3 & 100 \\\\\n0 & 1\n\\end{bmatrix}.\n$$\nCompute an explicit $P \\succ 0$ aligned with the eigenvector basis of $A$ using the simplest admissible choice in your construction from part 1, and then evaluate the corresponding $\\|A\\|_{P}$.\n\n3. Discuss, using the singular values of $V$, how the spectral condition number $\\kappa_{2}(P) = \\|P\\|_{2} \\,\\|P^{-1}\\|_{2}$ depends on $V$. Explain the trade-off between minimizing $\\|A\\|_{P}$ and the conditioning of $P$ for the above $A$.\n\nYour final answer should be the minimal achievable value of $\\|A\\|_{P}$ for the given $A$ in part 2, expressed as an exact real number without units. Do not round your final answer.", "solution": "The problem is subjected to validation. The givens are: a diagonalizable matrix $A \\in \\mathbb{C}^{n \\times n}$ with $A = V \\Lambda V^{-1}$, where $\\Lambda = \\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{n})$; a symmetric positive definite matrix $P \\succ 0$; a $P$-weighted vector norm $\\|x\\|_{P} = \\sqrt{x^{*} P x}$; an induced matrix norm $\\|A\\|_{P} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{P}}{\\|x\\|_{P}}$; an inner product $\\langle x, y \\rangle_{P} = x^{*} P y$; a condition of alignment where the columns of $V$ are mutually orthogonal with respect to this inner product; a specific matrix $A = \\begin{bmatrix} 3 & 100 \\\\ 0 & 1 \\end{bmatrix}$; and the definition of the spectral condition number $\\kappa_{2}(P) = \\|P\\|_{2} \\,\\|P^{-1}\\|_{2}$. The problem is scientifically grounded in linear algebra and matrix theory, well-posed with clear definitions and objectives, and expressed in objective mathematical language. It is self-contained and does not violate any fundamental principles. Therefore, the problem is deemed valid and a solution will be provided.\n\nPart 1: Construction of $P$ and proof of the norm property.\nLet the columns of the invertible matrix $V$ be the eigenvectors $v_{1}, \\dots, v_{n}$ of $A$. The condition that these vectors are mutually orthogonal with respect to the inner product $\\langle x, y \\rangle_{P} = x^{*} P y$ means that for $i \\neq j$, we have $\\langle v_{i}, v_{j} \\rangle_{P} = v_{i}^{*} P v_{j} = 0$. This implies that the matrix product $V^{*}PV$ must be a diagonal matrix. Let us denote this diagonal matrix by $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$.\nFor $P$ to be positive definite ($P \\succ 0$), we must have $x^{*} P x > 0$ for any non-zero vector $x \\in \\mathbb{C}^{n}$. Since $V$ is invertible, any $x \\neq 0$ can be written as $x = Vy$ for some non-zero $y \\in \\mathbb{C}^{n}$. Substituting this into the quadratic form gives:\n$$x^{*} P x = (Vy)^{*} P (Vy) = y^{*} V^{*} P V y = y^{*} D y = \\sum_{i=1}^{n} d_{i} |y_{i}|^{2}$$\nFor this expression to be strictly positive for any non-zero $y$, all diagonal entries of $D$ must be strictly positive, i.e., $d_{i} > 0$ for all $i=1, \\dots, n$.\nFrom the relation $V^{*} P V = D$, we can express $P$ in terms of $V$ and $D$. Since $V$ is invertible, so is $V^{*}$. We can pre-multiply by $(V^{*})^{-1}$ and post-multiply by $V^{-1}$ to obtain $P = (V^{*})^{-1} D V^{-1}$.\nThis defines the family of symmetric positive definite matrices $P$ that are aligned with the eigenvector basis of $A$: they are of the form $P = (V^{*})^{-1} D V^{-1}$ where $D$ is any diagonal matrix with strictly positive entries on its diagonal.\n\nNow, we prove that for any such $P$, the induced norm $\\|A\\|_{P}$ is equal to the spectral radius of $A$, $\\rho(A) = \\max_{i} |\\lambda_{i}|$. The squared norm is given by:\n$$\\|A\\|_{P}^{2} = \\left(\\sup_{x \\neq 0} \\frac{\\|A x\\|_{P}}{\\|x\\|_{P}}\\right)^2 = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{P}^{2}}{\\|x\\|_{P}^{2}} = \\sup_{x \\neq 0} \\frac{x^{*} A^{*} P A x}{x^{*} P x}$$\nWe use the diagonalization $A = V \\Lambda V^{-1}$, which implies $A^{*} = (V^{-1})^{*} \\Lambda^{*} V^{*}$. Substituting this into the numerator:\n$$x^{*} A^{*} P A x = x^{*} (V^{-1})^{*} \\Lambda^{*} V^{*} P V \\Lambda V^{-1} x$$\nUsing the alignment property $V^{*} P V = D$, the expression simplifies to:\n$$x^{*} (V^{-1})^{*} \\Lambda^{*} D \\Lambda V^{-1} x$$\nLet us perform a change of variables $y = V^{-1}x$. Since $V$ is invertible, the supremum over all non-zero $x$ is equivalent to the supremum over all non-zero $y$. The numerator becomes $y^{*} \\Lambda^{*} D \\Lambda y$.\nThe denominator becomes $x^{*} P x = y^{*} V^{*} P V y = y^{*} D y$.\nThus, the expression for the squared norm becomes:\n$$\\|A\\|_{P}^{2} = \\sup_{y \\neq 0} \\frac{y^{*} \\Lambda^{*} D \\Lambda y}{y^{*} D y}$$\nGiven that $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$ and $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$, we have $\\Lambda^{*} = \\mathrm{diag}(\\bar{\\lambda}_{1}, \\dots, \\bar{\\lambda}_{n})$ and $\\Lambda^{*} D \\Lambda = \\mathrm{diag}(d_{1}|\\lambda_{1}|^{2}, \\dots, d_{n}|\\lambda_{n}|^{2})$.\nThe ratio can be written in terms of the components of $y$:\n$$\\frac{\\sum_{i=1}^{n} d_{i} |\\lambda_{i}|^{2} |y_{i}|^{2}}{\\sum_{i=1}^{n} d_{i} |y_{i}|^{2}}$$\nThis is a weighted average of the values $|\\lambda_{i}|^{2}$, where the non-negative weights are $w_{i} = d_{i}|y_{i}|^{2}$. The supremum of such a weighted average is the maximum of the values being averaged. Let $M = \\max_{i} |\\lambda_{i}|^{2}$. Then $\\sum_{i} d_{i} |\\lambda_{i}|^{2} |y_{i}|^{2} \\le \\sum_{i} d_{i} M |y_{i}|^{2} = M \\sum_{i} d_{i} |y_{i}|^{2}$. The ratio is therefore always less than or equal to $M$. To show that the supremum is $M$, let $j$ be an index for which $|\\lambda_{j}|^{2} = M$. By choosing $y$ to be the standard basis vector $e_{j}$, the ratio becomes $\\frac{d_{j}|\\lambda_{j}|^{2}}{d_{j}} = |\\lambda_{j}|^{2} = M$.\nTherefore, $\\|A\\|_{P}^{2} = \\max_{i} |\\lambda_{i}|^{2} = (\\rho(A))^{2}$. Taking the square root gives $\\|A\\|_{P} = \\rho(A) = \\max_{i} |\\lambda_{i}|$.\nFor any choice of $P$ from this aligned family, the value of the induced norm is constant and equal to $\\rho(A)$. Therefore, the minimum possible value over this family is trivially $\\rho(A)$. It is also a fundamental result in matrix analysis that for any induced norm $\\|\\cdot\\|$, $\\rho(A) \\le \\|A\\|$. Since our construction provides a norm for which this lower bound is achieved, $\\rho(A)$ is the infimum over all possible induced norms, making it the minimal possible value.\n\nPart 2: Explicit calculation for the given matrix $A$.\nWe are given the matrix $A = \\begin{bmatrix} 3 & 100 \\\\ 0 & 1 \\end{bmatrix}$. Since $A$ is upper triangular, its eigenvalues are its diagonal entries: $\\lambda_{1} = 3$ and $\\lambda_{2} = 1$. The spectral radius is $\\rho(A) = \\max(|3|, |1|) = 3$.\nWe find the eigenvectors:\nFor $\\lambda_{1}=3$: $(A-3I)v_{1} = \\begin{bmatrix} 0 & 100 \\\\ 0 & -2 \\end{bmatrix}v_{1} = 0$, which gives $v_{1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nFor $\\lambda_{2}=1$: $(A-1I)v_{2} = \\begin{bmatrix} 2 & 100 \\\\ 0 & 0 \\end{bmatrix}v_{2} = 0$, which implies $2x+100y=0$. We can choose $v_{2} = \\begin{bmatrix} -50 \\\\ 1 \\end{bmatrix}$.\nThe eigenvector matrix is $V = \\begin{bmatrix} 1 & -50 \\\\ 0 & 1 \\end{bmatrix}$.\nThe family of aligned matrices is $P=(V^*)^{-1} D V^{-1}$, where $V^* = V^T$ as $V$ is real. The problem asks for the simplest admissible choice, which we take to be $D=I$, the identity matrix, meaning $d_1=1$ and $d_2=1$.\nFirst, we find $V^{-1}$: $\\det(V) = 1$, so $V^{-1} = \\begin{bmatrix} 1 & 50 \\\\ 0 & 1 \\end{bmatrix}$.\nNext, we find $(V^*)^{-1} = (V^T)^{-1} = (V^{-1})^T = \\begin{bmatrix} 1 & 0 \\\\ 50 & 1 \\end{bmatrix}$.\nNow we compute $P$ with $D=I$:\n$$P = (V^*)^{-1} I V^{-1} = \\begin{bmatrix} 1 & 0 \\\\ 50 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 50 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 50 \\\\ 50 & 2501 \\end{bmatrix}$$\nTo verify, $P$ is symmetric. For positive definiteness, the leading principal minors are $1 > 0$ and $\\det(P) = 1(2501) - 50^2 = 2501 - 2500 = 1 > 0$. So $P \\succ 0$.\nAccording to the result from Part 1, for any aligned $P$, including this specific one, the induced norm is given by the spectral radius of $A$.\nTherefore, $\\|A\\|_{P} = \\rho(A) = 3$.\n\nPart 3: Discussion of condition number and trade-off.\nThe spectral condition number of $P$ is $\\kappa_{2}(P) = \\|P\\|_{2}\\|P^{-1}\\|_{2}$. Since $P$ is symmetric positive definite, its $2$-norm is its largest eigenvalue, $\\|P\\|_{2} = \\lambda_{\\max}(P)$, and $\\|P^{-1}\\|_{2} = \\lambda_{\\max}(P^{-1}) = 1/\\lambda_{\\min}(P)$. Thus, $\\kappa_{2}(P) = \\frac{\\lambda_{\\max}(P)}{\\lambda_{\\min}(P)}$.\nWe investigate the dependency of $\\kappa_{2}(P)$ on $V$. Let's use the simplest choice $D=I$. Then $P = (V^*)^{-1}V^{-1} = (VV^*)^{-1}$. Consequently, $P^{-1} = VV^*$.\nThe matrix $VV^*$ is Hermitian positive definite. Its eigenvalues are the squares of the singular values of $V$. Let the singular values of $V$ be $\\sigma_{1} \\ge \\dots \\ge \\sigma_{n} > 0$. The eigenvalues of $P^{-1}=VV^*$ are $\\lambda_{i}(P^{-1}) = \\sigma_{i}(V)^2$.\nThen $\\lambda_{\\max}(P^{-1}) = \\sigma_{\\max}(V)^2$ and $\\lambda_{\\min}(P^{-1}) = \\sigma_{\\min}(V)^2$.\nThe condition number of $P$ is the same as that of $P^{-1}$:\n$$\\kappa_{2}(P) = \\kappa_{2}(P^{-1}) = \\frac{\\lambda_{\\max}(P^{-1})}{\\lambda_{\\min}(P^{-1})} = \\frac{\\sigma_{\\max}(V)^2}{\\sigma_{\\min}(V)^2} = \\left(\\frac{\\sigma_{\\max}(V)}{\\sigma_{\\min}(V)}\\right)^2 = (\\kappa_{2}(V))^2$$\nThis shows that the condition number of the desired metric $P$ is the square of the condition number of the eigenvector matrix $V$.\n\nFor the specific matrix $A$, $V = \\begin{bmatrix} 1 & -50 \\\\ 0 & 1 \\end{bmatrix}$. The eigenvectors $v_1$ and $v_2$ are nearly parallel, which suggests $V$ is ill-conditioned. The singular values of $V$ are the square roots of the eigenvalues of $V^T V = \\begin{bmatrix} 1 & -50 \\\\ -50 & 2501 \\end{bmatrix}$. The characteristic equation is $\\lambda^2 - 2502\\lambda + 1 = 0$. The eigenvalues are $\\lambda(V^TV) = 1251 \\pm \\sqrt{1251^2-1}$.\nSo, $\\sigma_{\\max}(V)^2 = 1251 + \\sqrt{1251^2-1}$ and $\\sigma_{\\min}(V)^2 = 1251 - \\sqrt{1251^2-1}$.\nThe condition number of $V$ is $\\kappa_2(V) = \\frac{\\sigma_{\\max}(V)}{\\sigma_{\\min}(V)} = \\sqrt{\\frac{1251 + \\sqrt{1251^2-1}}{1251 - \\sqrt{1251^2-1}}} = 1251 + \\sqrt{1251^2-1} \\approx 2502$.\nThe condition number of our $P$ is $\\kappa_{2}(P) = (\\kappa_{2}(V))^2 \\approx (2502)^2 \\approx 6.26 \\times 10^6$, which is extremely large.\n\nThe trade-off is as follows:\nTo obtain a matrix norm $\\|A\\|_{P}$ that equals the spectral radius $\\rho(A)$ (the smallest possible value for any induced norm), one must use a metric defined by a matrix $P$ from the \"aligned\" family. For a non-normal matrix $A$ whose eigenvectors are nearly linearly dependent, the eigenvector matrix $V$ is ill-conditioned ($\\kappa_{2}(V) \\gg 1$). As shown, this leads to an extremely ill-conditioned matrix $P$ ($\\kappa_{2}(P) = (\\kappa_{2}(V))^2 \\gg 1$). Using such a $P$ is problematic in numerical computations as it amplifies errors. The level sets of the norm, $x^*Px=c$, become highly eccentric ellipsoids, reflecting a distorted geometry.\nIn contrast, one could use a well-conditioned metric like the standard Euclidean one ($P=I$, $\\kappa_2(P)=1$). However, the corresponding norm $\\|A\\|_{2} = \\sigma_{\\max}(A)$ can be much larger than $\\rho(A)$. For our $A$, $\\|A\\|_{2} = \\sigma_{\\max}(A) = \\sqrt{5005 + \\sqrt{5005^2-9}} \\approx 100.05$, which is significantly larger than $\\rho(A)=3$.\nThis illustrates the fundamental trade-off for non-normal matrices: one can either achieve the optimal norm value $\\|A\\|_{P}=\\rho(A)$ at the cost of using a numerically unstable, ill-conditioned metric $P$, or use a well-conditioned metric like $P=I$ at the cost of a norm value $\\|A\\|_{2}$ that can be drastically larger than the spectral radius. The severity of this trade-off is dictated by the non-normality of $A$, as quantified by the condition number of its eigenvector matrix $V$.", "answer": "$$\\boxed{3}$$"}, {"introduction": "While the previous exercise explored a theoretically optimal but potentially ill-conditioned solution, many practical situations call for simpler methods to improve a matrix's numerical properties. This problem introduces diagonal scaling, a powerful and widely-used pre-conditioning technique. By designing a diagonal transformation $D$ to equilibrate the column norms of a matrix, you will directly investigate how this straightforward scaling can substantially reduce the condition number $\\kappa_{1}(DAD^{-1})$, enhancing the robustness of numerical computations involving the system [@problem_id:2757387].", "id": "2757387", "problem": "In linear time-invariant state-space control, a change of state coordinates $x = D z$ with a positive diagonal matrix $D \\in \\mathbb{R}^{n \\times n}$ induces a similarity transformation $A \\mapsto B = D A D^{-1}$. The sensitivity of computations involving the state matrix is often reflected by the condition number in the induced $1$-norm. The induced $1$-norm of a matrix $M$ is defined by $\\|M\\|_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{n} |m_{ij}|$, and the $1$-norm condition number is $\\kappa_{1}(M) = \\|M\\|_{1} \\|M^{-1}\\|_{1}$ for invertible $M$.\n\nConsider the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with strictly positive entries\n$$\nA = \\begin{pmatrix}\n1 & 3 \\\\\n\\frac{1}{5} & 2\n\\end{pmatrix}.\n$$\nYou will investigate whether a diagonal similarity scaling can reduce the $1$-norm condition number and design a scaling that equilibrates the $1$-norms of the columns of the scaled matrix.\n\nTasks:\n1. Using only the definitions above, compute $\\|A\\|_{1}$, $\\|A^{-1}\\|_{1}$, and $\\kappa_{1}(A)$.\n2. Let $D = \\operatorname{diag}(d_{1}, d_{2})$ with $d_{1} > 0$ and $d_{2} > 0$, and define $B = D A D^{-1}$. Using the definition of the induced $1$-norm, derive the condition on the ratio $r = d_{2}/d_{1}$ that makes the two column $1$-norms of $B$ equal. Solve explicitly for $r > 0$.\n3. With this choice of $D$, compute $\\|B\\|_{1}$ and $\\|B^{-1}\\|_{1}$ exactly, and hence $\\kappa_{1}(B)$. Conclude whether $\\kappa_{1}(B) < \\kappa_{1}(A)$ holds for this design.\n\nProvide your final answer as the exact closed-form expression for $\\kappa_{1}(B)$ obtained in Task $3$. Do not round. No units are required.", "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. All definitions are standard in linear algebra and control theory. The given matrix $A$ is invertible, allowing for the computation of its condition number. We proceed with the solution.\n\nThe problem is divided into three consecutive tasks. We shall address them in order.\n\nTask 1: Compute $\\|A\\|_{1}$, $\\|A^{-1}\\|_{1}$, and $\\kappa_{1}(A)$.\n\nThe given matrix $A$ is\n$$\nA = \\begin{pmatrix}\n1 & 3 \\\\\n\\frac{1}{5} & 2\n\\end{pmatrix}\n$$\nThe induced $1$-norm of a matrix is the maximum absolute column sum. For matrix $A$, the entries are all positive, so we do not need to take absolute values.\nThe sum of the first column is $1 + \\frac{1}{5} = \\frac{6}{5}$.\nThe sum of the second column is $3 + 2 = 5$.\nThe $1$-norm of $A$ is the maximum of these two values:\n$$\n\\|A\\|_{1} = \\max\\left(\\frac{6}{5}, 5\\right) = 5\n$$\nNext, we compute the inverse of $A$. The determinant of $A$ is\n$$\n\\det(A) = (1)(2) - (3)\\left(\\frac{1}{5}\\right) = 2 - \\frac{3}{5} = \\frac{10-3}{5} = \\frac{7}{5}\n$$\nSince $\\det(A) \\neq 0$, the matrix is invertible. The inverse is given by\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix}\n2 & -3 \\\\\n-\\frac{1}{5} & 1\n\\end{pmatrix} = \\frac{5}{7} \\begin{pmatrix}\n2 & -3 \\\\\n-\\frac{1}{5} & 1\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{10}{7} & -\\frac{15}{7} \\\\\n-\\frac{1}{7} & \\frac{5}{7}\n\\end{pmatrix}\n$$\nNow, we compute the $1$-norm of $A^{-1}$. We must sum the absolute values of the entries in each column.\nThe sum for the first column is $\\left|\\frac{10}{7}\\right| + \\left|-\\frac{1}{7}\\right| = \\frac{10}{7} + \\frac{1}{7} = \\frac{11}{7}$.\nThe sum for the second column is $\\left|-\\frac{15}{7}\\right| + \\left|\\frac{5}{7}\\right| = \\frac{15}{7} + \\frac{5}{7} = \\frac{20}{7}$.\nThe $1$-norm of $A^{-1}$ is the maximum of these two values:\n$$\n\\|A^{-1}\\|_{1} = \\max\\left(\\frac{11}{7}, \\frac{20}{7}\\right) = \\frac{20}{7}\n$$\nThe $1$-norm condition number of $A$ is the product of these norms:\n$$\n\\kappa_{1}(A) = \\|A\\|_{1} \\|A^{-1}\\|_{1} = 5 \\cdot \\frac{20}{7} = \\frac{100}{7}\n$$\n\nTask 2: Find the condition on the ratio $r = d_{2}/d_{1}$ that makes the two column $1$-norms of $B=DAD^{-1}$ equal.\n\nLet $D = \\operatorname{diag}(d_{1}, d_{2}) = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix}$. Its inverse is $D^{-1} = \\begin{pmatrix} 1/d_{1} & 0 \\\\ 0 & 1/d_{2} \\end{pmatrix}$.\nThe similarity transformation gives the matrix $B$:\n$$\nB = DAD^{-1} = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix} \\begin{pmatrix} 1 & 3 \\\\ \\frac{1}{5} & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{d_{1}} & 0 \\\\ 0 & \\frac{1}{d_{2}} \\end{pmatrix}\n$$\n$$\nB = \\begin{pmatrix} d_{1} & 3d_{1} \\\\ \\frac{1}{5}d_{2} & 2d_{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{d_{1}} & 0 \\\\ 0 & \\frac{1}{d_{2}} \\end{pmatrix} = \\begin{pmatrix} (d_{1})(\\frac{1}{d_{1}}) & (3d_{1})(\\frac{1}{d_{2}}) \\\\ (\\frac{1}{5}d_{2})(\\frac{1}{d_{1}}) & (2d_{2})(\\frac{1}{d_{2}}) \\end{pmatrix} = \\begin{pmatrix} 1 & 3\\frac{d_{1}}{d_{2}} \\\\ \\frac{1}{5}\\frac{d_{2}}{d_{1}} & 2 \\end{pmatrix}\n$$\nUsing the definition $r = d_{2}/d_{1}$, we have $1/r = d_{1}/d_{2}$. The matrix $B$ can be written as:\n$$\nB = \\begin{pmatrix} 1 & \\frac{3}{r} \\\\ \\frac{r}{5} & 2 \\end{pmatrix}\n$$\nSince $d_{1} > 0$ and $d_{2} > 0$, we have $r > 0$. The entries of $B$ are all positive.\nThe absolute column sums (column norms) of $B$ are:\nColumn 1: $C_{1} = 1 + \\frac{r}{5}$\nColumn 2: $C_{2} = \\frac{3}{r} + 2$\nTo equilibrate the column norms, we set $C_{1} = C_{2}$:\n$$\n1 + \\frac{r}{5} = \\frac{3}{r} + 2\n$$\nRearranging the terms to solve for $r$:\n$$\n\\frac{r}{5} - 1 = \\frac{3}{r}\n$$\nMultiplying by $5r$ (which is non-zero as $r>0$):\n$$\nr^{2} - 5r = 15 \\implies r^{2} - 5r - 15 = 0\n$$\nThis is a quadratic equation for $r$. We use the quadratic formula $r = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\nr = \\frac{-(-5) \\pm \\sqrt{(-5)^{2} - 4(1)(-15)}}{2(1)} = \\frac{5 \\pm \\sqrt{25 + 60}}{2} = \\frac{5 \\pm \\sqrt{85}}{2}\n$$\nSince we must have $r > 0$, and $\\sqrt{85} > \\sqrt{25} = 5$, the solution $r = \\frac{5 - \\sqrt{85}}{2}$ is negative. We must choose the positive root:\n$$\nr = \\frac{5 + \\sqrt{85}}{2}\n$$\n\nTask 3: Compute $\\|B\\|_{1}$, $\\|B^{-1}\\|_{1}$, $\\kappa_{1}(B)$ and compare with $\\kappa_{1}(A)$.\n\nWith the value of $r$ from Task $2$, the column norms of $B$ are equal. We can compute this common value to find $\\|B\\|_{1}$. Using the expression for the first column norm:\n$$\n\\|B\\|_{1} = 1 + \\frac{r}{5} = 1 + \\frac{1}{5}\\left(\\frac{5 + \\sqrt{85}}{2}\\right) = 1 + \\frac{5 + \\sqrt{85}}{10} = \\frac{10 + 5 + \\sqrt{85}}{10} = \\frac{15 + \\sqrt{85}}{10}\n$$\nNext, we compute $B^{-1}$. The determinant of $B$ is $\\det(B) = (1)(2) - (\\frac{3}{r})(\\frac{r}{5}) = 2 - \\frac{3}{5} = \\frac{7}{5}$. This is equal to $\\det(A)$, as expected for a similarity transformation.\n$$\nB^{-1} = \\frac{1}{\\det(B)} \\begin{pmatrix} 2 & -\\frac{3}{r} \\\\ -\\frac{r}{5} & 1 \\end{pmatrix} = \\frac{5}{7} \\begin{pmatrix} 2 & -\\frac{3}{r} \\\\ -\\frac{r}{5} & 1 \\end{pmatrix} = \\begin{pmatrix}\n\\frac{10}{7} & -\\frac{15}{7r} \\\\ -\\frac{r}{7} & \\frac{5}{7}\n\\end{pmatrix}\n$$\nThe absolute column sums of $B^{-1}$ are:\nColumn 1: $C'_{1} = \\left|\\frac{10}{7}\\right| + \\left|-\\frac{r}{7}\\right| = \\frac{10+r}{7}$\nColumn 2: $C'_{2} = \\left|-\\frac{15}{7r}\\right| + \\left|\\frac{5}{7}\\right| = \\frac{15}{7r} + \\frac{5}{7} = \\frac{15+5r}{7r}$\nWe substitute $r = \\frac{5 + \\sqrt{85}}{2}$:\n$C'_{1} = \\frac{1}{7} \\left(10 + \\frac{5 + \\sqrt{85}}{2}\\right) = \\frac{20 + 5 + \\sqrt{85}}{14} = \\frac{25 + \\sqrt{85}}{14}$.\nFor $C'_{2}$, we can use the relation from the quadratic equation for $r$: $r-5 = \\frac{15}{r}$.\n$C'_{2} = \\frac{1}{7}\\left(\\frac{15}{r} + 5 \\right) = \\frac{1}{7}\\left((r-5) + 5\\right) = \\frac{r}{7} = \\frac{1}{7}\\left(\\frac{5 + \\sqrt{85}}{2}\\right) = \\frac{5 + \\sqrt{85}}{14}$.\nWe take the maximum of these two sums for $\\|B^{-1}\\|_{1}$:\n$$\n\\|B^{-1}\\|_{1} = \\max\\left(\\frac{25 + \\sqrt{85}}{14}, \\frac{5 + \\sqrt{85}}{14}\\right) = \\frac{25 + \\sqrt{85}}{14}\n$$\nThe condition number of $B$ is $\\kappa_{1}(B) = \\|B\\|_{1}\\|B^{-1}\\|_{1}$:\n$$\n\\kappa_{1}(B) = \\left(\\frac{15 + \\sqrt{85}}{10}\\right) \\left(\\frac{25 + \\sqrt{85}}{14}\\right) = \\frac{(15 + \\sqrt{85})(25 + \\sqrt{85})}{140}\n$$\nExpanding the numerator:\n$(15 + \\sqrt{85})(25 + \\sqrt{85}) = 15 \\cdot 25 + 15\\sqrt{85} + 25\\sqrt{85} + (\\sqrt{85})^{2} = 375 + 40\\sqrt{85} + 85 = 460 + 40\\sqrt{85}$.\nSo,\n$$\n\\kappa_{1}(B) = \\frac{460 + 40\\sqrt{85}}{140} = \\frac{46 + 4\\sqrt{85}}{14} = \\frac{23 + 2\\sqrt{85}}{7}\n$$\nFinally, we compare $\\kappa_{1}(B)$ with $\\kappa_{1}(A) = \\frac{100}{7}$. We must compare $23 + 2\\sqrt{85}$ with $100$. This is equivalent to comparing $2\\sqrt{85}$ with $77$, or $\\sqrt{85}$ with $38.5$. Squaring both sides: $(\\sqrt{85})^{2} = 85$ and $(38.5)^{2} = 1482.25$. Since $85 < 1482.25$, it follows that $\\sqrt{85} < 38.5$. Therefore, $23 + 2\\sqrt{85} < 100$.\nWe conclude that $\\kappa_{1}(B) < \\kappa_{1}(A)$, so the diagonal scaling has indeed reduced the condition number.\nThe question asks for the exact expression for $\\kappa_{1}(B)$.", "answer": "$$\n\\boxed{\\frac{23 + 2\\sqrt{85}}{7}}\n$$"}]}