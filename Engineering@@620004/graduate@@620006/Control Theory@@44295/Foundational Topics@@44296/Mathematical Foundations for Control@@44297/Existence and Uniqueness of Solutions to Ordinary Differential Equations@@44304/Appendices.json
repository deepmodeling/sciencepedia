{"hands_on_practices": [{"introduction": "The Picard–Lindelöf theorem is the foundational result guaranteeing the existence and uniqueness of solutions to ordinary differential equations. This practice exercise challenges you to move from abstract theory to direct application by verifying the theorem's hypotheses for a nonlinear, time-varying system. By explicitly calculating the minimal uniform Lipschitz constant for the given function [@problem_id:2705679], you will develop a core skill in ODE analysis: confirming that a system is well-behaved and quantifying its properties.", "problem": "Consider the scalar state equation from control theory, given as the time-varying nonlinear ordinary differential equation\n$$\\frac{dx}{dt} = f(t,x), \\quad f(t,x) \\triangleq \\sin(t)\\,x + \\cos(t)\\,\\arctan\\!\\big(x^{3}\\big),$$\nwith initial condition $x(0)=x_{0}$. Let the domain of interest be the compact set\n$$K \\triangleq [0,\\pi/2] \\times [-1,1].$$\nUsing only foundational principles (continuity of elementary functions, the Mean Value Theorem, and the Picard–Lindelöf hypothesis that continuity in time and a Lipschitz condition in the state on a rectangle imply existence and uniqueness of a local solution), verify explicitly that the Picard–Lindelöf hypotheses hold for $f$ on $K$. In particular, determine the exact minimal uniform Lipschitz constant $L$ in the state variable $x$ for $f$ on $K$, namely a constant $L$ such that\n$$|f(t,x_{1})-f(t,x_{2})| \\le L\\,|x_{1}-x_{2}| \\quad \\text{for all } (t,x_{1}),(t,x_{2}) \\in K.$$\nReport your final answer as the exact analytical expression for this constant $L$. No numerical approximation is required.", "solution": "The problem requires validation of the Picard–Lindelöf hypotheses for the function $f(t,x) = \\sin(t)x + \\cos(t)\\arctan(x^3)$ on the compact domain $K \\triangleq [0,\\pi/2] \\times [-1,1]$, and the calculation of the minimal uniform Lipschitz constant $L$ with respect to the state variable $x$.\n\nThe Picard–Lindelöf theorem for the existence and uniqueness of a local solution to the initial value problem $\\frac{dx}{dt} = f(t,x)$, $x(t_0)=x_0$, on a rectangular domain $R = [t_0-a, t_0+a] \\times [x_0-b, x_0+b]$ requires two conditions on $f(t,x)$:\n$1$. $f(t,x)$ is continuous on $R$.\n$2$. $f(t,x)$ is Lipschitz continuous in the variable $x$, uniformly in $t$, on $R$.\n\nWe verify these conditions for the given function $f(t,x)$ on the domain $K$.\n\nFirst, we verify the continuity of $f(t,x)$. The function is constructed from elementary functions. The functions $\\sin(t)$, $\\cos(t)$, $x$, and $x^3$ are continuous for all real numbers. The function $\\arctan(y)$ is also continuous for all real $y$. Since sums, products, and compositions of continuous functions are continuous, the function $f(t,x) = \\sin(t)x + \\cos(t)\\arctan(x^3)$ is continuous on its entire domain $\\mathbb{R}^2$. Consequently, it is continuous on the compact subset $K = [0,\\pi/2] \\times [-1,1]$. The first hypothesis is therefore satisfied.\n\nSecond, we verify the Lipschitz condition. A sufficient condition for $f(t,x)$ to be Lipschitz continuous in $x$ on a convex domain is that its partial derivative with respect to $x$, $\\frac{\\partial f}{\\partial x}$, is bounded on that domain. The interval $[-1,1]$ for $x$ is convex. By the Mean Value Theorem, for any fixed $t \\in [0, \\pi/2]$ and any $x_1, x_2 \\in [-1,1]$, there exists a $c$ between $x_1$ and $x_2$ such that\n$$f(t,x_1) - f(t,x_2) = \\frac{\\partial f}{\\partial x}(t,c) (x_1 - x_2).$$\nTaking the absolute value, we get\n$$|f(t,x_1) - f(t,x_2)| = \\left| \\frac{\\partial f}{\\partial x}(t,c) \\right| |x_1 - x_2|.$$\nA uniform Lipschitz constant $L$ must satisfy this inequality for all $(t,x_1), (t,x_2) \\in K$. The minimal such constant $L$ is given by the supremum of $|\\frac{\\partial f}{\\partial x}|$ over the domain $K$.\n$$L = \\sup_{(t,x) \\in K} \\left| \\frac{\\partial f}{\\partial x}(t,x) \\right|.$$\nThe partial derivative of $f(t,x)$ with respect to $x$ is:\n$$\\frac{\\partial f}{\\partial x}(t,x) = \\frac{\\partial}{\\partial x} \\left[ \\sin(t)x + \\cos(t)\\arctan(x^3) \\right] = \\sin(t) + \\cos(t) \\frac{1}{1+(x^3)^2} \\cdot (3x^2),$$\nwhich simplifies to\n$$\\frac{\\partial f}{\\partial x}(t,x) = \\sin(t) + \\cos(t) \\frac{3x^2}{1+x^6}.$$\nThis partial derivative is a continuous function on the compact set $K$. By the Extreme Value Theorem, it must attain its maximum and minimum values on $K$. Thus, the supremum is a maximum, and we can find $L$ by maximizing $|\\frac{\\partial f}{\\partial x}(t,x)|$.\n\nOn the domain $K$, we have $t \\in [0, \\pi/2]$ and $x \\in [-1,1]$. For these values:\n- $\\sin(t) \\ge 0$\n- $\\cos(t) \\ge 0$\n- The term $\\frac{3x^2}{1+x^6}$ is always non-negative.\nSince all terms are non-negative, their sum is non-negative. Therefore, on $K$, $\\frac{\\partial f}{\\partial x}(t,x) \\ge 0$, and we have $\\left| \\frac{\\partial f}{\\partial x}(t,x) \\right| = \\frac{\\partial f}{\\partial x}(t,x)$.\nThe problem reduces to finding the maximum value of $g(t,x) \\triangleq \\frac{\\partial f}{\\partial x}(t,x)$ on $K$.\n$$L = \\max_{(t,x) \\in [0,\\pi/2] \\times [-1,1]} \\left( \\sin(t) + \\cos(t) \\frac{3x^2}{1+x^6} \\right).$$\nTo find this maximum, we can adopt a two-stage approach. For any fixed $t \\in [0, \\pi/2]$, we first maximize the expression with respect to $x$. Let $h(x) = \\frac{3x^2}{1+x^6}$. Since $\\cos(t) \\ge 0$, the expression $g(t,x) = \\sin(t) + \\cos(t)h(x)$ is maximized when $h(x)$ is maximized. We find the maximum of $h(x)$ on $x \\in [-1,1]$.\nThe derivative of $h(x)$ is:\n$$h'(x) = \\frac{(6x)(1+x^6) - (3x^2)(6x^5)}{(1+x^6)^2} = \\frac{6x+6x^7-18x^7}{(1+x^6)^2} = \\frac{6x-12x^7}{(1+x^6)^2} = \\frac{6x(1-2x^6)}{(1+x^6)^2}.$$\nThe critical points are at $x=0$ and where $1-2x^6=0$, which implies $x^6 = 1/2$, so $x = \\pm (1/2)^{1/6}$. These points are within the interval $[-1,1]$. We evaluate $h(x)$ at these critical points and at the endpoints $x = \\pm 1$:\n- $h(0) = 0$.\n- $h(\\pm 1) = \\frac{3(1)^2}{1+1^6} = \\frac{3}{2}$.\n- For $x^6=1/2$, $x^2 = (1/2)^{1/3} = 4^{-1/3}$.\n$h\\left(\\pm(1/2)^{1/6}\\right) = \\frac{3(1/2)^{1/3}}{1+1/2} = \\frac{3 \\cdot 2^{-1/3}}{3/2} = 2 \\cdot 2^{-1/3} = 2^{2/3} = \\sqrt[3]{4}$.\nTo compare $3/2$ and $\\sqrt[3]{4}$, we can cube both values: $(3/2)^3 = 27/8 = 3.375$ and $(\\sqrt[3]{4})^3 = 4$. Since $4 > 3.375$, we have $\\sqrt[3]{4} > 3/2$.\nThus, the maximum value of $h(x)$ on $[-1,1]$ is $\\sqrt[3]{4}$.\n\nNow we substitute this maximum value back into the expression for $g(t,x)$:\n$$L = \\max_{t \\in [0, \\pi/2]} \\left( \\sin(t) + \\cos(t) \\cdot \\sqrt[3]{4} \\right).$$\nThis is a standard problem of maximizing an expression of the form $A\\sin(t) + B\\cos(t)$, where $A=1$ and $B=\\sqrt[3]{4}$. The maximum value is given by $\\sqrt{A^2+B^2}$.\n$$L = \\sqrt{1^2 + (\\sqrt[3]{4})^2} = \\sqrt{1 + (4^{1/3})^2} = \\sqrt{1 + 4^{2/3}}.$$\nThis maximum occurs for an angle $t_{max} \\in (0, \\pi/2)$ such that $\\tan(t_{max}) = A/B = 1/\\sqrt[3]{4}$. Since this angle is in the required interval, this is the true maximum.\n\nBoth hypotheses of the Picard–Lindelöf theorem are satisfied. The function $f(t,x)$ is continuous on $K$, and it is Lipschitz continuous in $x$ with the minimal Lipschitz constant $L = \\sqrt{1+4^{2/3}}$.", "answer": "$$\\boxed{\\sqrt{1+4^{2/3}}}$$", "id": "2705679"}, {"introduction": "The guarantee of the Picard–Lindelöf theorem is fundamentally local, a subtlety with profound practical implications. This exercise explores this concept through the classic example of an ODE whose solutions exhibit finite-time blow-up. By finding the explicit solution and determining its maximal interval of existence [@problem_id:2705691], you will gain a concrete understanding of how solutions can cease to exist and why the distinction between local and global solutions is critical in nonlinear dynamics.", "problem": "Consider the scalar Ordinary Differential Equation (ODE) that arises as the scalar Riccati differential equation for a Linear Quadratic Regulator with data matrices specialized to $A=0$, $B=1$, $Q=0$, and $R=1$, namely\n$$\n\\dot{x}(t)=x(t)^{2}, \\quad x(0)=x_{0},\n$$\nwith $x_{0}>0$. A function $x:[0,T)\\to\\mathbb{R}$ is called a solution on $[0,T)$ if it is absolutely continuous and satisfies the integral form\n$$\nx(t)=x_{0}+\\int_{0}^{t}x(s)^{2}\\,ds \\quad \\text{for all } t\\in[0,T).\n$$\nDefine the maximal forward existence time as\n$$\nT^{\\star}(x_{0})=\\sup\\{T>0:\\text{ there exists a unique solution on }[0,T)\\}.\n$$\nUsing only standard definitions and the existence and uniqueness theorem for ODEs (Picard–Lindelöf theorem), do the following:\n- Establish local existence and uniqueness near $t=0$.\n- Derive an explicit expression for $x(t)$ on its interval of existence by separation of variables, use it to demonstrate finite-time blow-up in forward time, and justify that no continuation past the blow-up time is possible in the sense of the maximal forward solution.\n- Identify the exact analytic expression of $T^{\\star}(x_{0})$ as a function of $x_{0}$.\n\nYour final answer must be the exact analytic expression you obtain for $T^{\\star}(x_{0})$. Do not provide any units. Do not round.", "solution": "The problem presented is a standard initial value problem from the theory of ordinary differential equations, specifically concerning the maximal interval of existence of a solution. It is well-posed, scientifically sound, and contains all necessary information. We may therefore proceed with a rigorous analysis.\n\nThe problem is to analyze the initial value problem (IVP) given by the scalar autonomous ordinary differential equation:\n$$\n\\dot{x}(t) = x(t)^{2}, \\quad x(0) = x_{0}\n$$\nwhere $x_{0} > 0$.\n\nFirst, we establish local existence and uniqueness by applying the Picard–Lindelöf theorem. The theorem requires the function $f(t,x)$ in the ODE $\\dot{x} = f(t,x)$ to be continuous in $t$ and locally Lipschitz continuous in $x$. In our case, $f(t,x) = x^{2}$, which is independent of $t$. The function $f(x) = x^{2}$ is continuously differentiable with respect to $x$ for all $x \\in \\mathbb{R}$, with $\\frac{\\partial f}{\\partial x} = 2x$.\n\nConsider any compact interval for $x$, say $I = [x_{0}-a, x_{0}+a]$ for some $a > 0$. For any $x \\in I$, the partial derivative is bounded: $|\\frac{\\partial f}{\\partial x}| = |2x| \\le 2(|x_{0}|+a)$. By the Mean Value Theorem, for any $x_{1}, x_{2} \\in I$, there exists a $\\xi$ between $x_{1}$ and $x_{2}$ such that\n$$\n|f(x_{1}) - f(x_{2})| = \\left| \\frac{\\partial f}{\\partial x}(\\xi) \\right| |x_{1} - x_{2}|\n$$\nSince $\\xi \\in I$, we have $|\\frac{\\partial f}{\\partial x}(\\xi)| = |2\\xi| \\le 2(|x_{0}|+a)$. Let $L = 2(|x_{0}|+a)$, which is a finite constant. Then, $|f(x_{1}) - f(x_{2})| \\le L|x_{1}-x_{2}|$. This demonstrates that $f(x) = x^{2}$ is locally Lipschitz continuous in $x$ on any neighborhood of $x_{0}$. Since $f$ is also continuous, the Picard–Lindelöf theorem guarantees the existence of a unique solution to the IVP on some open interval containing $t=0$.\n\nNext, we derive the explicit form of this unique solution. The equation $\\frac{dx}{dt} = x^{2}$ is a separable nonlinear ODE. Since $x_{0} > 0$, the solution $x(t)$ will be positive for small $t$, so we can divide by $x^{2}$ without issue.\n$$\n\\frac{dx}{x^{2}} = dt\n$$\nIntegrating both sides from the initial time $t=0$ to a later time $t$ gives:\n$$\n\\int_{x(0)}^{x(t)} \\frac{1}{s^{2}} \\, ds = \\int_{0}^{t} 1 \\, d\\tau\n$$\n$$\n\\left[ -\\frac{1}{s} \\right]_{x_{0}}^{x(t)} = [\\tau]_{0}^{t}\n$$\n$$\n-\\frac{1}{x(t)} - \\left(-\\frac{1}{x_{0}}\\right) = t - 0\n$$\n$$\n\\frac{1}{x_{0}} - \\frac{1}{x(t)} = t\n$$\nRearranging to solve for $x(t)$:\n$$\n\\frac{1}{x(t)} = \\frac{1}{x_{0}} - t = \\frac{1 - x_{0}t}{x_{0}}\n$$\n$$\nx(t) = \\frac{x_{0}}{1 - x_{0}t}\n$$\nThis expression is the unique local solution. We now analyze its domain. Since $x_{0} > 0$ is given, the solution is defined and continuously differentiable for all $t$ such that the denominator is non-zero, i.e., $1 - x_{0}t \\neq 0$, which means $t \\neq \\frac{1}{x_{0}}$. The initial condition is at $t=0$. The interval of definition containing $t=0$ is therefore $(-\\infty, \\frac{1}{x_{0}})$. We are concerned with the forward existence time, so we consider the interval $[0, \\frac{1}{x_{0}})$.\n\nAs $t$ approaches $\\frac{1}{x_{0}}$ from below, denoted $t \\to (\\frac{1}{x_{0}})^{-}$, the denominator $1 - x_{0}t \\to 0^{+}$. Consequently, the solution $x(t)$ exhibits a vertical asymptote:\n$$\n\\lim_{t \\to (1/x_{0})^{-}} x(t) = \\lim_{t \\to (1/x_{0})^{-}} \\frac{x_{0}}{1 - x_{0}t} = +\\infty\n$$\nThis phenomenon is known as finite-time blow-up. The solution cannot be extended to include $t = \\frac{1}{x_{0}}$ as a real-valued function.\n\nThe theory of maximal solutions for ODEs states that if a solution $x(t)$ to $\\dot{x} = f(t,x)$ exists on a maximal interval of existence $(a,b)$ with $b < \\infty$, then as $t \\to b^{-}$, the graph of the solution $(t, x(t))$ must leave every compact subset of the domain of $f$. In our case, the domain of $f(t,x)=x^{2}$ is all of $\\mathbb{R}^{2}$. As $t \\to (\\frac{1}{x_{0}})^{-}$, we have shown that $|x(t)| \\to \\infty$. This means the trajectory $(t, x(t))$ leaves any compact subset of $\\mathbb{R}^{2}$. Therefore, the solution cannot be continued beyond $t = \\frac{1}{x_{0}}$ in forward time.\n\nThe maximal forward interval of existence is thus $[0, \\frac{1}{x_{0}})$. The quantity $T^{\\star}(x_{0})$ is defined as the supremum of all $T>0$ for which a unique solution exists on $[0,T)$. Based on our derivation, this supremum is precisely the time of blow-up.\n$$\nT^{\\star}(x_{0}) = \\sup \\{ T>0 : \\text{the unique solution } x(t) = \\frac{x_{0}}{1-x_{0}t} \\text{ exists on } [0,T) \\} = \\frac{1}{x_{0}}\n$$\nThe problem asks for this exact analytic expression.", "answer": "$$\n\\boxed{\\frac{1}{x_{0}}}\n$$", "id": "2705691"}, {"introduction": "Many systems in control engineering, such as switched systems, involve dynamics that change abruptly, resulting in an ODE whose right-hand side is discontinuous in time. This exercise demonstrates how to handle such cases by moving beyond the classical framework to a more general one that accommodates measurable time-dependence. You will first justify the existence of a unique solution for a linear time-varying system with piecewise-constant dynamics and then compute the overall state transition matrix by \"stitching\" together solutions across the intervals of continuity [@problem_id:2705649], a fundamental technique for analyzing switched and hybrid systems.", "problem": "Consider the linear time-varying (LTV) ordinary differential equation (ODE) in control theory, given by $\\dot{x}(t)=A(t)\\,x(t)$ on the finite horizon $t\\in[0,3]$ with jumps in the matrix $A(t)$ at $t=1$ and $t=2$. Specifically, let\n$$\nA(t)=\n\\begin{cases}\nA_{0}, & t\\in[0,1),\\\\\nA_{1}, & t\\in[1,2),\\\\\nA_{2}, & t\\in[2,3],\n\\end{cases}\n$$\nwhere\n$$\nA_{0}=\\begin{pmatrix}0 & 1\\\\ 0 & 0\\end{pmatrix},\\quad\nA_{1}=\\begin{pmatrix}0 & 0\\\\ -2 & 0\\end{pmatrix},\\quad\nA_{2}=\\begin{pmatrix}\\ln 2 & 0\\\\ 0 & -\\ln 3\\end{pmatrix}.\n$$\nAnswer the following.\n1) Using only foundational results for existence and uniqueness of solutions to ODEs based on measurability and local Lipschitz conditions in the state (for example, the Carathéodory framework), justify rigorously that for every initial condition $x(0)=x_{0}\\in\\mathbb{R}^{2}$ there exists a unique absolutely continuous solution $x:[0,3]\\to\\mathbb{R}^{2}$ that satisfies $\\dot{x}(t)=A(t)\\,x(t)$ for almost every $t\\in[0,3]$.\n2) Let $\\Phi(t,t_{0})$ denote the state transition matrix (STM), defined as the unique matrix solution to $\\frac{d}{dt}\\Phi(t,t_{0})=A(t)\\,\\Phi(t,t_{0})$ with $\\Phi(t_{0},t_{0})=I$. Compute the exact closed-form expression for $\\Phi(3,0)$ by stitching together the fundamental solutions across the subintervals partitioned by the jump times.\n\nProvide your final answer as the explicit $2\\times 2$ matrix for $\\Phi(3,0)$. No numerical rounding is required, and no units are involved. The final answer must be a single closed-form expression.", "solution": "The problem consists of two parts. First, we must justify the existence and uniqueness of a solution. Second, we must compute the state transition matrix $\\Phi(3,0)$.\n\n**Part 1: Existence and Uniqueness of the Solution**\nWe consider the initial value problem $\\dot{x}(t) = A(t)x(t)$, with $x(0) = x_0$. This can be written in the form $\\dot{x} = f(t,x)$ where $f(t,x) = A(t)x$. We turn to the Carathéodory existence theorem for solutions. A function $x(t)$ is a solution in the sense of Carathéodory if it is absolutely continuous and satisfies the integral equation $x(t) = x_0 + \\int_{0}^{t} f(\\tau, x(\\tau)) d\\tau$. This implies that $x(t)$ is differentiable for almost every $t$ and satisfies the differential equation $\\dot{x}(t) = f(t, x(t))$ almost everywhere.\n\nThe conditions for the existence of a unique Carathéodory solution for $\\dot{x}=f(t,x)$ on an interval $[t_0, t_f]$ are:\n1. For each fixed $x$, the function $t \\mapsto f(t,x)$ is measurable on $[t_0, t_f]$.\n2. For almost every fixed $t \\in [t_0, t_f]$, the function $x \\mapsto f(t,x)$ is continuous.\n3. For any compact set of states, there exists a locally integrable function $m(t)$ such that $\\|f(t,x)\\| \\leq m(t)$.\n\nFor the given problem, $f(t,x) = A(t)x$ and the interval is $[0,3]$. Let us verify these conditions.\n1. The matrix function $A(t)$ is piecewise constant. A piecewise constant function is a simple function, which is Borel measurable, and therefore Lebesgue measurable. For any fixed vector $x \\in \\mathbb{R}^2$, the product $A(t)x$ is a linear combination of the columns of $A(t)$ with coefficients from $x$. The components of $A(t)x$ are therefore measurable functions of $t$. Thus, $t \\mapsto A(t)x$ is a measurable vector-valued function.\n2. For any fixed $t \\in [0,3]$, the function $x \\mapsto A(t)x$ is a linear transformation, which is continuous for all $x \\in \\mathbb{R}^2$.\n3. The function $f(t,x)$ satisfies a stronger condition than local boundedness; it satisfies a global Lipschitz condition with respect to $x$, uniformly in $t$. Let any matrix norm be chosen. Then $\\|f(t,x_1) - f(t,x_2)\\| = \\|A(t)(x_1-x_2)\\| \\leq \\|A(t)\\| \\|x_1-x_2\\|$. The function $\\|A(t)\\|$ is piecewise constant on $[0,3]$, with values $\\|A_0\\|$, $\\|A_1\\|$, and $\\|A_2\\|$. Therefore, $\\|A(t)\\|$ is bounded on $[0,3]$ by some constant $L = \\sup_{t \\in [0,3]} \\|A(t)\\|$. This means that $f(t,x)$ is globally Lipschitz in $x$, uniformly in $t$.\n\nThe combination of measurability in $t$ and the uniform Lipschitz condition in $x$ is sufficient to guarantee the existence of a unique absolutely continuous solution $x(t)$ for any initial condition $x(0) = x_0$, and this solution is defined on the entire interval $[0,3]$. This provides the required rigorous justification.\n\n**Part 2: Computation of the State Transition Matrix $\\Phi(3,0)$**\nThe state transition matrix $\\Phi(t, t_0)$ propagates the state from time $t_0$ to $t$ via $x(t) = \\Phi(t, t_0)x(t_0)$. For a piecewise constant system, the STM over a composite interval can be found by composing the STMs of the subintervals in the correct time order. Specifically, for a sequence of times $t_0 < t_1 < \\dots < t_k$, we have $\\Phi(t_k, t_0) = \\Phi(t_k, t_{k-1})\\Phi(t_{k-1}, t_{k-2})\\cdots\\Phi(t_1, t_0)$.\n\nIn this problem, the jumps occur at $t=1$ and $t=2$. Therefore, the total state transition matrix from $t=0$ to $t=3$ is given by the product:\n$$ \\Phi(3,0) = \\Phi(3,2)\\Phi(2,1)\\Phi(1,0) $$\nOn each subinterval $[t_k, t_{k+1}]$, the system is linear time-invariant (LTI), $\\dot{x}(t) = A_k x(t)$. The STM for an LTI system is given by the matrix exponential, $\\Phi(t_{k+1}, t_k) = \\exp(A_k(t_{k+1}-t_k))$.\n\nWe compute each matrix factor:\n\n- For $t \\in [0,1)$, $A(t) = A_0 = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$. The time duration is $1-0=1$.\nThe STM is $\\Phi(1,0) = \\exp(A_0(1-0)) = \\exp(A_0)$.\nThe matrix $A_0$ is nilpotent: $A_0^2 = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\nThe Taylor series for the exponential thus truncates: $\\exp(A_0) = I + A_0 + \\frac{A_0^2}{2!} + \\dots = I+A_0$.\n$$ \\Phi(1,0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\n\n- For $t \\in [1,2)$, $A(t) = A_1 = \\begin{pmatrix} 0 & 0 \\\\ -2 & 0 \\end{pmatrix}$. The time duration is $2-1=1$.\nThe STM is $\\Phi(2,1) = \\exp(A_1(2-1)) = \\exp(A_1)$.\nThe matrix $A_1$ is also nilpotent: $A_1^2 = \\begin{pmatrix} 0 & 0 \\\\ -2 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 0 \\\\ -2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\nSimilarly, $\\exp(A_1) = I + A_1$.\n$$ \\Phi(2,1) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ -2 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -2 & 1 \\end{pmatrix} $$\n\n- For $t \\in [2,3]$, $A(t) = A_2 = \\begin{pmatrix} \\ln 2 & 0 \\\\ 0 & -\\ln 3 \\end{pmatrix}$. The time duration is $3-2=1$.\nThe STM is $\\Phi(3,2) = \\exp(A_2(3-2)) = \\exp(A_2)$.\nThe matrix $A_2$ is diagonal. The exponential of a diagonal matrix is the diagonal matrix of the exponentials of its elements.\n$$ \\exp(A_2) = \\begin{pmatrix} \\exp(\\ln 2) & 0 \\\\ 0 & \\exp(-\\ln 3) \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3^{-1} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix} $$\n\nNow, we multiply these matrices to find $\\Phi(3,0)$.\n$$ \\Phi(3,0) = \\Phi(3,2)\\Phi(2,1)\\Phi(1,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\nFirst, we compute the product of the rightmost two matrices:\n$$ \\Phi(2,1)\\Phi(1,0) = \\begin{pmatrix} 1 & 0 \\\\ -2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot 1 + 0\\cdot 0 & 1\\cdot 1 + 0\\cdot 1 \\\\ -2\\cdot 1 + 1\\cdot 0 & -2\\cdot 1 + 1\\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ -2 & -1 \\end{pmatrix} $$\nFinally, we multiply by the first matrix:\n$$ \\Phi(3,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ -2 & -1 \\end{pmatrix} = \\begin{pmatrix} 2\\cdot 1 + 0\\cdot (-2) & 2\\cdot 1 + 0\\cdot (-1) \\\\ 0\\cdot 1 + \\frac{1}{3}\\cdot (-2) & 0\\cdot 1 + \\frac{1}{3}\\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2 & 2 \\\\ -\\frac{2}{3} & -\\frac{1}{3} \\end{pmatrix} $$\nThis is the final expression for the state transition matrix from $t=0$ to $t=3$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 2 & 2 \\\\ -\\frac{2}{3} & -\\frac{1}{3} \\end{pmatrix} } $$", "id": "2705649"}]}