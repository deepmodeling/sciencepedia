{"hands_on_practices": [{"introduction": "One of the cornerstones of modern control theory is the deep connection between a system's dynamic properties and the structure of its state space. This exercise builds that bridge by starting with the concept of controllability and showing how it guarantees a spanning set for the entire state space, $\\mathbb{R}^{n}$. By working through the construction of a basis from the columns of the controllability matrix, you will gain a concrete understanding of the celebrated controllability rank condition and its implications for both single-input and multi-input systems [@problem_id:2757663].", "problem": "Let $\\dot{x}(t) = A x(t) + B u(t)$ be a finite-dimensional linear time-invariant (LTI) system with $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$. The reachable subspace from the origin over any nontrivial time interval equals the linear span of vectors of the form $A^{k} b_{j}$ where $k \\in \\{0,1,\\dots,n-1\\}$ and $b_{j}$ is the $j$-th column of $B$. The pair $(A,B)$ is called controllable if the reachable subspace equals $\\mathbb{R}^{n}$.\n\n1) Starting from the definitions of reachability and linear span in vector spaces, and using only fundamental properties of linearity, show that when $(A,B)$ is controllable, the set of columns of the controllability matrix\n$$\n\\mathcal{C}(A,B) \\coloneqq \\big[\\, B \\;\\; A B \\;\\; A^{2} B \\;\\; \\dots \\;\\; A^{n-1} B \\,\\big]\n$$\nspans $\\mathbb{R}^{n}$, and therefore any choice of $n$ linearly independent columns from $\\mathcal{C}(A,B)$ forms a basis of $\\mathbb{R}^{n}$.\n\n2) Describe, in terms of linear independence and the graded sequence of subspaces $\\mathcal{R}_{k} \\coloneqq \\operatorname{span}\\{B,AB,\\dots,A^{k}B\\}$, conditions under which a basis selected from columns of $\\mathcal{C}(A,B)$ is unique. Your discussion should identify structural features (e.g., the number of inputs and the pattern of rank increments $\\operatorname{rank}(\\mathcal{R}_{k}) - \\operatorname{rank}(\\mathcal{R}_{k-1})$) that either enforce uniqueness or create non-uniqueness, explicitly citing how ordering and normalization choices affect the outcome.\n\n3) Consider the concrete $4$-state, $2$-input system with\n$$\nA \\,=\\, \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix},\n\\qquad\nB \\,=\\, \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix}.\n$$\nForm the controllability matrix $\\mathcal{C}(A,B)$, and, proceeding left-to-right, select the first $n=4$ columns that are linearly independent to assemble a basis matrix $T \\in \\mathbb{R}^{4 \\times 4}$ (i.e., $T$ has those four columns, in the same order, as its columns). Compute the determinant $\\det(T)$. Express the final answer as an exact integer (no rounding is needed).", "solution": "The problem as stated is scientifically grounded, well-posed, and complete. It presents a standard set of questions in linear control theory. I shall proceed to solve it.\n\n### Part 1: Proof of the Controllability Rank Condition\n\nThe problem defines a linear time-invariant (LTI) system $\\dot{x}(t) = A x(t) + B u(t)$, with $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{n \\times n}$, and $B \\in \\mathbb{R}^{n \\times m}$. Let the columns of the input matrix $B$ be denoted by $b_j$ for $j \\in \\{1, 2, \\dots, m\\}$.\n\nThe problem provides two key definitions as premises:\n1.  The reachable subspace from the origin, denoted $\\mathcal{R}$, is defined as the linear span of the set of vectors $S = \\{A^k b_j \\mid k \\in \\{0, 1, \\dots, n-1\\}, j \\in \\{1, \\dots, m\\}\\}$. In mathematical notation, $\\mathcal{R} \\coloneqq \\operatorname{span}(S)$.\n2.  The pair $(A, B)$ is controllable if and only if the reachable subspace is the entire state space, i.e., $\\mathcal{R} = \\mathbb{R}^n$.\n\nThe controllability matrix is defined as\n$$\n\\mathcal{C}(A,B) \\coloneqq \\big[\\, B \\;\\; A B \\;\\; A^{2} B \\;\\; \\dots \\;\\; A^{n-1} B \\,\\big]\n$$\nThis matrix is formed by concatenating the blocks $A^k B$ for $k = 0, \\dots, n-1$. Each block $A^k B$ consists of the columns $A^k b_1, A^k b_2, \\dots, A^k b_m$.\n\nThe set of all columns of the controllability matrix $\\mathcal{C}(A,B)$ is precisely the set $S$ defined in the premise. That is, the columns of $\\mathcal{C}(A,B)$ are the vectors $\\{A^k b_j\\}$ for all $k \\in \\{0, \\dots, n-1\\}$ and $j \\in \\{1, \\dots, m\\}$.\n\nBy the definition of linear span, the space spanned by the columns of $\\mathcal{C}(A,B)$ is $\\operatorname{span}(S)$. From the problem's first premise, this is the reachable subspace $\\mathcal{R}$.\n$$\n\\operatorname{span}(\\text{columns of } \\mathcal{C}(A,B)) = \\operatorname{span}(S) = \\mathcal{R}\n$$\n\nThe problem states that we are to assume the pair $(A,B)$ is controllable. By the second premise, this means $\\mathcal{R} = \\mathbb{R}^n$.\nSubstituting this into our previous finding, we arrive at the conclusion:\n$$\n\\operatorname{span}(\\text{columns of } \\mathcal{C}(A,B)) = \\mathbb{R}^n\n$$\nThis demonstrates that when $(A,B)$ is controllable, the columns of the controllability matrix $\\mathcal{C}(A,B)$ span the state space $\\mathbb{R}^n$.\n\nA basis for an $n$-dimensional vector space like $\\mathbb{R}^n$ is defined as any set of $n$ linearly independent vectors that span the space. Since the columns of $\\mathcal{C}(A,B)$ form a spanning set for $\\mathbb{R}^n$, it follows from the definition of a basis that any subset of these columns containing exactly $n$ linearly independent vectors constitutes a basis for $\\mathbb{R}^n$.\n\n### Part 2: Uniqueness of Basis from Controllability Matrix\n\nThe uniqueness of a basis selected from the columns of the controllability matrix $\\mathcal{C}(A,B)$ depends critically on the number of inputs, $m$.\n\nFirst, consider the statement \"a basis selected from columns of $\\mathcal{C}(A,B)$\". This implies we choose a subset of the column vectors of $\\mathcal{C}(A,B)$ as they are given, without allowing for arbitrary scaling (normalization) or linear combinations. We are discussing the uniqueness of this chosen *set* of vectors.\n\nCase 1: Single-Input System ($m=1$)\nIf the system has a single input, $B$ is an $n \\times 1$ column vector, which we denote as $b$. The controllability matrix is\n$$\n\\mathcal{C}(A,b) = \\big[\\, b \\;\\; Ab \\;\\; A^2 b \\;\\; \\dots \\;\\; A^{n-1}b \\,\\big]\n$$\nThis is an $n \\times n$ matrix. For the pair $(A,b)$ to be controllable, the columns of $\\mathcal{C}(A,b)$ must span $\\mathbb{R}^n$. Since there are exactly $n$ columns, for them to span the $n$-dimensional space $\\mathbb{R}^n$, they must be linearly independent.\nIn this situation, there are precisely $n$ columns in $\\mathcal{C}(A,b)$, and all $n$ of them are required to form a basis. Therefore, there is only one possible set of vectors to choose: the entire set of columns of $\\mathcal{C}(A,b)$. The choice of basis vectors is unique.\nOrdering and normalization: If the order of vectors in the basis matters, then the basis $[b, Ab, \\dots, A^{n-1}b]$ is also unique. If permutations are allowed, there are $n!$ possible ordered bases. Since the problem phrasing does not mention normalization, we assume the columns are taken as is.\n\nCase 2: Multi-Input System ($m>1$)\nIf the system has multiple inputs, the controllability matrix $\\mathcal{C}(A,B)$ is an $n \\times (nm)$ matrix. Since $m>1$, the number of columns $nm$ is strictly greater than the dimension of the state space $n$.\nFor the pair $(A,B)$ to be controllable, the rank of this $n \\times (nm)$ matrix must be $n$. This means that the set of $nm$ columns spans $\\mathbb{R}^n$. Since there are more than $n$ vectors in this spanning set, the set must be linearly dependent.\nThe existence of linear dependencies among the columns of $\\mathcal{C}(A,B)$ implies that there are multiple distinct ways to select a subset of $n$ linearly independent columns. For a simple example, if columns $c_1, c_2, \\dots, c_n$ form a basis, and another column $c_{n+1}$ can be written as a linear combination of some of these, say $c_{n+1} = \\alpha_1 c_1 + \\dots + \\alpha_n c_n$ with $\\alpha_1 \\neq 0$, then we can replace $c_1$ with $c_{n+1}$ to form a new basis $\\{c_{n+1}, c_2, \\dots, c_n\\}$. As long as $c_{n+1}$ is not a scalar multiple of $c_1$ and is a distinct column in $\\mathcal{C}(A,B)$, this new set of basis vectors is different from the original one. Such dependencies are guaranteed to exist when $nm > n$. Therefore, for any controllable multi-input system, the basis selected from the columns of $\\mathcal{C}(A,B)$ is **never** unique.\n\nThe graded sequence of subspaces $\\mathcal{R}_k = \\operatorname{span}\\{B, AB, \\dots, A^k B\\}$ and the rank increments $\\nu_k = \\operatorname{rank}(\\mathcal{R}_k) - \\operatorname{rank}(\\mathcal{R}_{k-1})$ (with $\\mathcal{R}_{-1}=\\{0\\}$) illuminate this non-uniqueness. At each step $k$, we must find $\\nu_k$ new basis vectors from the columns of $A^k B$ that are linearly independent of $\\mathcal{R}_{k-1}$. For $m>1$, it is generally possible that the number of columns in $A^k B$ that are candidates for providing these new directions is greater than $\\nu_k$, creating a choice. For instance, at step $k=0$, we must choose $\\nu_0 = \\operatorname{rank}(B)$ vectors. If $m > \\operatorname{rank}(B)$, there are multiple ways to choose a basis for the image of $B$ from its columns.\n\nIn summary, the structural feature determining uniqueness is the number of inputs $m$:\n-   Uniqueness of the set of basis vectors is guaranteed if and only if $m=1$.\n-   Non-uniqueness is guaranteed if $m>1$.\n\n### Part 3: Concrete Example and Determinant Calculation\n\nWe are given the system with $n=4$ states and $m=2$ inputs:\n$$\nA \\,=\\, \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix},\n\\qquad\nB \\,=\\, \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix} \\,=\\, [b_1, b_2]\n$$\nThe controllability matrix is $\\mathcal{C}(A,B) = [B, AB, A^2 B, A^3 B]$. We need to find the first $4$ linearly independent columns by searching from left to right.\n\nThe first two columns are the columns of $B$:\n$c_1 = b_1 = [0, 0, 1, 0]^T$\n$c_2 = b_2 = [0, 0, 0, 1]^T$\nThese two vectors are linearly independent. Our basis so far is $\\{c_1, c_2\\}$.\n\nNext, we compute the columns of $AB$:\n$$\nAB = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix} = \\begin{bmatrix}\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n-3 & -4\n\\end{bmatrix}\n$$\nThe next two columns of $\\mathcal{C}(A,B)$ are:\n$c_3 = Ab_1 = [0, 1, 0, -3]^T$\n$c_4 = Ab_2 = [0, 0, 1, -4]^T$\n\nWe check if $c_3$ is in the span of $\\{c_1, c_2\\}$. Any vector in $\\operatorname{span}\\{c_1, c_2\\}$ has the form $[\\alpha \\cdot 0 + \\beta \\cdot 0, \\alpha \\cdot 0 + \\beta \\cdot 0, \\alpha \\cdot 1, \\beta \\cdot 1]^T = [0, 0, \\alpha, \\beta]^T$. Since $c_3$ has a non-zero second component, it is linearly independent of $c_1$ and $c_2$. We add $c_3$ to our basis, which is now $\\{c_1, c_2, c_3\\}$.\n\nNext, we check if $c_4$ is in the span of $\\{c_1, c_2, c_3\\}$. We test if there exist scalars $\\alpha, \\beta, \\gamma$ such that $c_4 = \\alpha c_1 + \\beta c_2 + \\gamma c_3$.\n$$\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -4 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\gamma \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\gamma \\\\ \\alpha \\\\ \\beta - 3\\gamma \\end{bmatrix}\n$$\nComparing components gives $\\gamma=0$, $\\alpha=1$, and $\\beta-3\\gamma = -4 \\Rightarrow \\beta = -4$. This is consistent. We find $c_4 = 1 \\cdot c_1 - 4 \\cdot c_2$. So, $c_4$ is linearly dependent on the preceding columns. We must skip it.\n\nWe need a fourth linearly independent vector. We proceed to the next column of $\\mathcal{C}(A,B)$, which is the first column of $A^2 B$, i.e., $A^2 b_1 = A(Ab_1) = Ac_3$.\n$c_5 = A c_3 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n0 \\\\\n-3\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n-3 \\\\\n-2+12\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n-3 \\\\\n10\n\\end{bmatrix}$\n\nWe check if $c_5$ is in the span of $\\{c_1, c_2, c_3\\}$.\n$$\n\\begin{bmatrix} 1 \\\\ 0 \\\\ -3 \\\\ 10 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\gamma \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\gamma \\\\ \\alpha \\\\ \\beta - 3\\gamma \\end{bmatrix}\n$$\nComparing the first component yields $1=0$, a contradiction. Thus, $c_5$ is linearly independent of $\\{c_1, c_2, c_3\\}$.\n\nWe have found our four linearly independent columns, selected in order by searching left-to-right: $\\{c_1, c_2, c_3, c_5\\}$. The basis matrix $T$ is formed by these columns:\n$$\nT = [c_1, c_2, c_3, c_5] = \\begin{bmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & -3 \\\\\n0 & 1 & -3 & 10\n\\end{bmatrix}\n$$\nWe compute the determinant of $T$. We use cofactor expansion along the first row:\n$$\n\\det(T) = (-1)^{1+1} \\cdot 0 \\cdot M_{11} + (-1)^{1+2} \\cdot 0 \\cdot M_{12} + (-1)^{1+3} \\cdot 0 \\cdot M_{13} + (-1)^{1+4} \\cdot 1 \\cdot \\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix}\n$$\n$$\n\\det(T) = -1 \\cdot \\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix}\n$$\nTo compute the determinant of the $3 \\times 3$ matrix, we expand along its first row:\n$$\n\\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix} = (-1)^{1+3} \\cdot 1 \\cdot \\det \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = 1 \\cdot (1 \\cdot 1 - 0 \\cdot 0) = 1\n$$\nSubstituting this back, we find the determinant of $T$:\n$$\n\\det(T) = -1 \\cdot 1 = -1\n$$\nThe determinant is an exact integer.", "answer": "$$\n\\boxed{-1}\n$$", "id": "2757663"}, {"introduction": "The power of linear algebra lies in its abstract nature, allowing us to unify seemingly disparate concepts. This practice challenges you to move beyond the familiar vector space $\\mathbb{R}^{n}$ and apply the same rigorous principles of bases and coordinate changes to a space of rational transfer functions. By framing partial fraction expansion as a change of basis, you will see how a standard engineering tool is elegantly described by the formal language of vector spaces, reinforcing the universality of these mathematical structures [@problem_id:2757652].", "problem": "Consider the real vector space of single-input single-output transfer functions in the Laplace variable $s$, defined as\n$$\n\\mathcal{V} \\;=\\; \\left\\{\\, \\frac{P(s)}{Q(s)} \\;:\\; Q(s) \\,=\\, (s+1)^{2}(s+2),\\; P(s)\\in\\mathbb{R}[s],\\; \\deg P \\leq 2 \\,\\right\\}.\n$$\nBy construction, $\\mathcal{V}$ is a finite-dimensional subspace of the space of real-rational transfer functions commonly used in Linear Time-Invariant (LTI) control theory. Let $Q(s)=(s+1)^{2}(s+2)$.\n\nDefine the two ordered bases of $\\mathcal{V}$:\n- The monomial-denominator basis $\\mathcal{B}_{\\mathrm{mon}}=\\{\\, b_{1}(s),\\, b_{2}(s),\\, b_{3}(s)\\,\\}$ where\n$$\nb_{1}(s)=\\frac{1}{Q(s)},\\quad b_{2}(s)=\\frac{s}{Q(s)},\\quad b_{3}(s)=\\frac{s^{2}}{Q(s)}.\n$$\n- The partial-fraction basis $\\mathcal{B}_{\\mathrm{pf}}=\\{\\, c_{1}(s),\\, c_{2}(s),\\, c_{3}(s)\\,\\}$ where\n$$\nc_{1}(s)=\\frac{1}{s+1},\\quad c_{2}(s)=\\frac{1}{(s+1)^{2}},\\quad c_{3}(s)=\\frac{1}{s+2}.\n$$\n\nLet the transfer function $G(s)\\in\\mathcal{V}$ be\n$$\nG(s)\\;=\\;\\frac{5s^{2}-s+4}{(s+1)^{2}(s+2)}.\n$$\n\nUsing only the linear-algebraic definitions of vector space, linear independence, basis, coordinates, and change of basis:\n1) Compute the coordinate vector of $G(s)$ with respect to $\\mathcal{B}_{\\mathrm{mon}}$.\n2) Construct explicitly, from first principles, the change-of-basis map that sends coordinates in $\\mathcal{B}_{\\mathrm{mon}}$ to coordinates in $\\mathcal{B}_{\\mathrm{pf}}$, by expressing the basis elements of $\\mathcal{B}_{\\mathrm{pf}}$ in the basis $\\mathcal{B}_{\\mathrm{mon}}$ and inverting the resulting linear relation.\n3) Apply this change of basis to obtain the coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{pf}}$.\n\nReport only the final coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{pf}}$ as your answer, formatted as a row vector. No rounding is required; give exact values.", "solution": "The problem presented is a standard exercise in linear algebra, applied to a vector space of rational functions relevant to control theory. The problem is well-posed, self-contained, and scientifically sound. We shall proceed with its resolution.\n\nThe vector space is $\\mathcal{V} = \\left\\{\\, \\frac{P(s)}{Q(s)} \\;:\\; Q(s) = (s+1)^{2}(s+2),\\; P(s)\\in\\mathbb{R}[s],\\; \\deg P \\leq 2 \\,\\right\\}$. The dimension of this space is $3$, as the numerator polynomial is determined by $3$ coefficients. We are given two bases for this space:\nThe monomial-denominator basis $\\mathcal{B}_{\\mathrm{mon}}=\\{\\, b_{1}(s),\\, b_{2}(s),\\, b_{3}(s)\\,\\}$ with $b_{1}(s)=\\frac{1}{Q(s)}$, $b_{2}(s)=\\frac{s}{Q(s)}$, $b_{3}(s)=\\frac{s^{2}}{Q(s)}$.\nThe partial-fraction basis $\\mathcal{B}_{\\mathrm{pf}}=\\{\\, c_{1}(s),\\, c_{2}(s),\\, c_{3}(s)\\,\\}$ with $c_{1}(s)=\\frac{1}{s+1}$, $c_{2}(s)=\\frac{1}{(s+1)^{2}}$, $c_{3}(s)=\\frac{1}{s+2}$.\nThe vector of interest is $G(s) = \\frac{5s^{2}-s+4}{(s+1)^{2}(s+2)}$.\n\nOur objective is to find the coordinate vector of $G(s)$ with respect to the basis $\\mathcal{B}_{\\mathrm{pf}}$. We will follow the three prescribed steps.\n\nFirst, we compute the coordinate vector of $G(s)$ with respect to $\\mathcal{B}_{\\mathrm{mon}}$, denoted by $[G]_{\\mathcal{B}_{\\mathrm{mon}}}$. By definition, we must find scalars $k_{1}$, $k_{2}$, $k_{3}$ such that $G(s) = k_{1}b_{1}(s) + k_{2}b_{2}(s) + k_{3}b_{3}(s)$.\nSubstituting the definitions of $G(s)$ and the basis vectors $b_{i}(s)$:\n$$\n\\frac{5s^{2}-s+4}{Q(s)} = k_{1}\\frac{1}{Q(s)} + k_{2}\\frac{s}{Q(s)} + k_{3}\\frac{s^{2}}{Q(s)} = \\frac{k_{1} + k_{2}s + k_{3}s^{2}}{Q(s)}.\n$$\nFor these two rational functions to be equal, their numerators must be equal. By comparing the coefficients of the polynomials in the numerators, we have:\n$$\nk_{3}s^{2} + k_{2}s + k_{1} \\;=\\; 5s^{2} - 1s + 4.\n$$\nThis equality must hold for all $s$, so we identify the coefficients of like powers of $s$:\n$k_{1} = 4$, $k_{2} = -1$, $k_{3} = 5$.\nThe coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{mon}}$ is therefore\n$$\n[G]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 4 \\\\ -1 \\\\ 5 \\end{pmatrix}.\n$$\n\nSecond, we construct the change-of-basis matrix from $\\mathcal{B}_{\\mathrm{mon}}$ to $\\mathcal{B}_{\\mathrm{pf}}$, which we denote as $P_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}}$. The problem specifies a method: express the basis vectors of $\\mathcal{B}_{\\mathrm{pf}}$ in terms of $\\mathcal{B}_{\\mathrm{mon}}$, which gives the matrix $P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}}$, and then invert it.\nLet us write each vector $c_{i}(s)$ from $\\mathcal{B}_{\\mathrm{pf}}$ as a linear combination of the vectors from $\\mathcal{B}_{\\mathrm{mon}}$:\nFor $c_{1}(s)$:\n$$\nc_{1}(s) = \\frac{1}{s+1} = \\frac{(s+1)(s+2)}{(s+1)^{2}(s+2)} = \\frac{s^{2}+3s+2}{Q(s)} = 2\\frac{1}{Q(s)} + 3\\frac{s}{Q(s)} + 1\\frac{s^{2}}{Q(s)} = 2b_{1}(s) + 3b_{2}(s) + 1b_{3}(s).\n$$\nThe coordinate vector is $[c_{1}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}$.\nFor $c_{2}(s)$:\n$$\nc_{2}(s) = \\frac{1}{(s+1)^{2}} = \\frac{s+2}{(s+1)^{2}(s+2)} = \\frac{s+2}{Q(s)} = 2\\frac{1}{Q(s)} + 1\\frac{s}{Q(s)} + 0\\frac{s^{2}}{Q(s)} = 2b_{1}(s) + 1b_{2}(s) + 0b_{3}(s).\n$$\nThe coordinate vector is $[c_{2}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor $c_{3}(s)$:\n$$\nc_{3}(s) = \\frac{1}{s+2} = \\frac{(s+1)^{2}}{(s+1)^{2}(s+2)} = \\frac{s^{2}+2s+1}{Q(s)} = 1\\frac{1}{Q(s)} + 2\\frac{s}{Q(s)} + 1\\frac{s^{2}}{Q(s)} = 1b_{1}(s) + 2b_{2}(s) + 1b_{3}(s).\n$$\nThe coordinate vector is $[c_{3}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$.\nThe change-of-basis matrix from $\\mathcal{B}_{\\mathrm{pf}}$ to $\\mathcal{B}_{\\mathrm{mon}}$ is formed by taking these coordinate vectors as its columns:\n$$\nP_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}} = \\begin{pmatrix} [c_{1}]_{\\mathcal{B}_{\\mathrm{mon}}} & [c_{2}]_{\\mathcal{B}_{\\mathrm{mon}}} & [c_{3}]_{\\mathcal{B}_{\\mathrm{mon}}} \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 1 \\\\ 3 & 1 & 2 \\\\ 1 & 0 & 1 \\end{pmatrix}.\n$$\nThe mapping of coordinates is given by $[v]_{\\mathcal{B}_{\\mathrm{mon}}} = P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}} [v]_{\\mathcal{B}_{\\mathrm{pf}}}$. To find the map from $\\mathcal{B}_{\\mathrm{mon}}$-coordinates to $\\mathcal{B}_{\\mathrm{pf}}$-coordinates, we must compute the inverse of this matrix:\n$$\nP_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} = (P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}})^{-1}.\n$$\nLet $A = P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}}$. The determinant of $A$ is:\n$$\n\\det(A) = 2(1\\cdot 1 - 2\\cdot 0) - 2(3\\cdot 1 - 2\\cdot 1) + 1(3\\cdot 0 - 1\\cdot 1) = 2(1) - 2(1) - 1 = -1.\n$$\nSince the determinant is non-zero, the matrix is invertible. The inverse is $A^{-1} = \\frac{1}{\\det(A)}\\mathrm{adj}(A)$. The adjugate matrix, $\\mathrm{adj}(A)$, is the transpose of the cofactor matrix.\nThe cofactor matrix is:\n$$\nC = \\begin{pmatrix} +\\begin{vmatrix} 1 & 2 \\\\ 0 & 1 \\end{vmatrix} & -\\begin{vmatrix} 3 & 2 \\\\ 1 & 1 \\end{vmatrix} & +\\begin{vmatrix} 3 & 1 \\\\ 1 & 0 \\end{vmatrix} \\\\ -\\begin{vmatrix} 2 & 1 \\\\ 0 & 1 \\end{vmatrix} & +\\begin{vmatrix} 2 & 1 \\\\ 1 & 1 \\end{vmatrix} & -\\begin{vmatrix} 2 & 2 \\\\ 1 & 0 \\end{vmatrix} \\\\ +\\begin{vmatrix} 2 & 1 \\\\ 1 & 2 \\end{vmatrix} & -\\begin{vmatrix} 2 & 1 \\\\ 3 & 2 \\end{vmatrix} & +\\begin{vmatrix} 2 & 2 \\\\ 3 & 1 \\end{vmatrix} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 \\\\ -2 & 1 & 2 \\\\ 3 & -1 & -4 \\end{pmatrix}.\n$$\nThe adjugate matrix is $\\mathrm{adj}(A) = C^{T} = \\begin{pmatrix} 1 & -2 & 3 \\\\ -1 & 1 & -1 \\\\ -1 & 2 & -4 \\end{pmatrix}$.\nThe inverse matrix is therefore:\n$$\nP_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} = \\frac{1}{-1} \\begin{pmatrix} 1 & -2 & 3 \\\\ -1 & 1 & -1 \\\\ -1 & 2 & -4 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 & -3 \\\\ 1 & -1 & 1 \\\\ 1 & -2 & 4 \\end{pmatrix}.\n$$\n\nThird, we apply this change-of-basis matrix to the coordinate vector $[G]_{\\mathcal{B}_{\\mathrm{mon}}}$ to obtain $[G]_{\\mathcal{B}_{\\mathrm{pf}}}$.\nThe transformation rule is $[G]_{\\mathcal{B}_{\\mathrm{pf}}} = P_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} [G]_{\\mathcal{B}_{\\mathrm{mon}}}$.\n$$\n[G]_{\\mathcal{B}_{\\mathrm{pf}}} = \\begin{pmatrix} -1 & 2 & -3 \\\\ 1 & -1 & 1 \\\\ 1 & -2 & 4 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -1 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (-1)(4) + (2)(-1) + (-3)(5) \\\\ (1)(4) + (-1)(-1) + (1)(5) \\\\ (1)(4) + (-2)(-1) + (4)(5) \\end{pmatrix} = \\begin{pmatrix} -4 - 2 - 15 \\\\ 4 + 1 + 5 \\\\ 4 + 2 + 20 \\end{pmatrix} = \\begin{pmatrix} -21 \\\\ 10 \\\\ 26 \\end{pmatrix}.\n$$\nThus, the coordinate vector of $G(s)$ with respect to the basis $\\mathcal{B}_{\\mathrm{pf}}$ is $\\begin{pmatrix} -21 \\\\ 10 \\\\ 26 \\end{pmatrix}$. This corresponds to the partial fraction expansion $G(s) = \\frac{-21}{s+1} + \\frac{10}{(s+1)^{2}} + \\frac{26}{s+2}$.\n\nThe final answer, formatted as a row vector as requested, is the transpose of this column vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} -21 & 10 & 26 \\end{pmatrix}}\n$$", "id": "2757652"}, {"introduction": "Not all parts of a system's state are necessarily visible to its output, a concept captured by the notion of observability. This practice explores the structure of the unobservable subspace, a fundamental component of the Kalman decomposition, by connecting its algebraic definition to a clear geometric interpretation using orthogonal projection. Calculating how the system's output depends only on the 'observable' part of the initial state provides a profound insight into how a system's internal structure dictates its external behavior [@problem_id:2757686].", "problem": "Consider a continuous-time Linear Time-Invariant (LTI) system with state-space realization given by the state matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and output matrix $C \\in \\mathbb{R}^{1 \\times 3}$,\n$$\nA = \\begin{pmatrix}\n-1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}, \n\\qquad\nC = \\begin{pmatrix}\n1 & 0 & -1\n\\end{pmatrix}.\n$$\nLet the state space be equipped with the standard inner product $\\langle x, y \\rangle = x^{\\top} y$ on $\\mathbb{R}^{3}$. The output is $y(t) = C x(t)$ and the state trajectory is $x(t) = \\exp(At) x(0)$ for initial condition $x(0) \\in \\mathbb{R}^{3}$. Using only the foundational definitions that the unobservable subspace is \n$$\n\\mathcal{N} = \\bigcap_{k=0}^{n-1} \\ker\\!\\big(C A^{k}\\big)\n$$\nand that orthogonal projection onto a subspace is defined by inner products with an orthonormal basis for that subspace, perform the following:\n\n- Derive $\\mathcal{N}$ explicitly and construct an orthonormal basis for it with respect to the standard inner product.\n- Derive the orthogonal projection of a general initial state $x(0) = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}$ onto $\\mathcal{N}$.\n- Using the state solution, determine the analytic expression of the resulting output $y(t)$ and interpret how the projection you derived affects $y(t)$ for all $t \\ge 0$.\n\nProvide, as your final answer, the closed-form analytic expression for $y(t)$ in terms of $\\alpha$, $\\beta$, $\\gamma$, and $t$. No rounding is required. Do not include units.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in linear control theory with a complete and consistent setup, devoid of any scientific or logical flaws. We may therefore proceed with the solution.\n\nThe problem requires the determination of the system output $y(t)$ and an interpretation of how it is affected by the unobservable subspace, $\\mathcal{N}$. We begin by explicitly determining $\\mathcal{N}$. The state-space dimension is $n=3$. The unobservable subspace is defined as\n$$\n\\mathcal{N} = \\bigcap_{k=0}^{n-1} \\ker\\!\\big(C A^{k}\\big) = \\ker(C) \\cap \\ker(CA) \\cap \\ker(CA^2)\n$$\nThe state and output matrices are given as:\n$$\nA = \\begin{pmatrix}\n-1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}, \n\\qquad\nC = \\begin{pmatrix}\n1 & 0 & -1\n\\end{pmatrix}\n$$\nSince $A$ is a diagonal matrix, its powers are computed by taking powers of its diagonal elements.\nFor $k=0$:\n$$ A^0 = I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\implies C A^0 = C = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} $$\nFor $k=1$:\n$$ A^1 = A = \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\implies C A^1 = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 & -2 \\end{pmatrix} $$\nFor $k=2$:\n$$ A^2 = \\begin{pmatrix} (-1)^2 & 0 & 0 \\\\ 0 & 0^2 & 0 \\\\ 0 & 0 & 2^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\implies C A^2 = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & -4 \\end{pmatrix} $$\nNow, we find the kernel of each matrix $C A^k$. A vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} \\in \\mathbb{R}^3$ is in $\\mathcal{N}$ if and only if it satisfies $C A^k v = 0$ for $k=0, 1, 2$. This gives the following system of linear equations:\n\\begin{enumerate}\n    \\item $v_1 - v_3 = 0$\n    \\item $-v_1 - 2v_3 = 0$\n    \\item $v_1 - 4v_3 = 0$\n\\end{enumerate}\nFrom the first equation, we have $v_1 = v_3$. Substituting this into the second equation gives $-v_3 - 2v_3 = -3v_3 = 0$, which implies $v_3 = 0$. Consequently, $v_1 = 0$. The third equation, $v_1 - 4v_3 = 0$, is also satisfied since $0 - 4(0) = 0$. The variable $v_2$ is not constrained by any of the equations. Therefore, any vector in $\\mathcal{N}$ must be of the form $\\begin{pmatrix} 0 \\\\ v_2 \\\\ 0 \\end{pmatrix}$ where $v_2 \\in \\mathbb{R}$. The unobservable subspace is thus spanned by a single vector:\n$$\n\\mathcal{N} = \\text{span} \\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}\n$$\nTo construct an orthonormal basis for $\\mathcal{N}$ with respect to the standard inner product, we take the spanning vector $b = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and normalize it. Its norm is $\\|b\\| = \\sqrt{0^2 + 1^2 + 0^2} = 1$. Thus, the vector $u_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ itself forms an orthonormal basis for $\\mathcal{N}$.\n\nNext, we derive the orthogonal projection of a general initial state $x(0) = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}$ onto $\\mathcal{N}$. The projection is given by $\\text{proj}_{\\mathcal{N}}(x(0)) = \\langle x(0), u_1 \\rangle u_1$. The inner product is:\n$$\n\\langle x(0), u_1 \\rangle = x(0)^{\\top} u_1 = \\begin{pmatrix} \\alpha & \\beta & \\gamma \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\beta\n$$\nTherefore, the projection of $x(0)$ onto $\\mathcal{N}$ is:\n$$\n\\text{proj}_{\\mathcal{N}}(x(0)) = \\beta u_1 = \\begin{pmatrix} 0 \\\\ \\beta \\\\ 0 \\end{pmatrix}\n$$\nNow, we determine the output $y(t) = C x(t) = C \\exp(At) x(0)$. First, we compute the matrix exponential $\\exp(At)$. Since $A$ is diagonal, $\\exp(At)$ is found by exponentiating the diagonal elements of $At$:\n$$\nAt = \\begin{pmatrix} -t & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 2t \\end{pmatrix} \\implies \\exp(At) = \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & \\exp(0) & 0 \\\\ 0 & 0 & \\exp(2t) \\end{pmatrix} = \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\exp(2t) \\end{pmatrix}\n$$\nThe state trajectory $x(t)$ is:\n$$\nx(t) = \\exp(At) x(0) = \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\exp(2t) \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} \\alpha \\exp(-t) \\\\ \\beta \\\\ \\gamma \\exp(2t) \\end{pmatrix}\n$$\nFinally, the output $y(t)$ is:\n$$\ny(t) = C x(t) = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\exp(-t) \\\\ \\beta \\\\ \\gamma \\exp(2t) \\end{pmatrix} = \\alpha \\exp(-t) - \\gamma \\exp(2t)\n$$\nFor the interpretation, we decompose the initial state $x(0)$ into a component in the unobservable subspace, $x_{\\mathcal{N}} = \\text{proj}_{\\mathcal{N}}(x(0))$, and a component in its orthogonal complement (the observable subspace $\\mathcal{N}^{\\perp}$), $x_{\\mathcal{O}} = x(0) - x_{\\mathcal{N}}$.\n$$\nx_{\\mathcal{N}} = \\begin{pmatrix} 0 \\\\ \\beta \\\\ 0 \\end{pmatrix}, \\qquad x_{\\mathcal{O}} = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\beta \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ 0 \\\\ \\gamma \\end{pmatrix}\n$$\nBy linearity of the system, the output is $y(t) = C\\exp(At)(x_{\\mathcal{O}} + x_{\\mathcal{N}}) = C\\exp(At)x_{\\mathcal{O}} + C\\exp(At)x_{\\mathcal{N}}$. The term $C\\exp(At)x_{\\mathcal{N}}$ represents the output generated by the unobservable component of the initial state. By definition of the unobservable subspace, this term must be identically zero for all $t \\ge 0$. Let us verify this:\n$$\nC\\exp(At)x_{\\mathcal{N}} = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\exp(2t) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\beta \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\beta \\\\ 0 \\end{pmatrix} = 0\n$$\nThis confirms that the output is generated solely by the observable component of the initial state, $x_{\\mathcal{O}}$:\n$$\ny(t) = C\\exp(At)x_{\\mathcal{O}} = \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\exp(2t) \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ 0 \\\\ \\gamma \\end{pmatrix} = \\alpha \\exp(-t) - \\gamma \\exp(2t)\n$$\nThe result is identical. The interpretation is that the component of the initial state lying in the unobservable subspace, which is the component along the second standard basis vector with magnitude $\\beta$, is \"invisible\" to the output $y(t)$. The output signal depends only on the components $\\alpha$ and $\\gamma$, which define the projection of the initial state onto the observable subspace $\\mathcal{N}^{\\perp}$.\n\nThe final expression for the output is:\n$$\ny(t) = \\alpha \\exp(-t) - \\gamma \\exp(2t)\n$$", "answer": "$$\\boxed{\\alpha \\exp(-t) - \\gamma \\exp(2t)}$$", "id": "2757686"}]}