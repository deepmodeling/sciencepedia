{"hands_on_practices": [{"introduction": "The Cholesky factorization is a cornerstone for working with positive definite matrices in optimization and control. This exercise moves beyond a black-box algorithmic view, guiding you to derive the factorization for a $2 \\times 2$ matrix from the elementary technique of completing the square [@problem_id:2735086]. This process builds fundamental intuition by revealing the deep structural equivalence between the algebraic representation of a quadratic form and its matrix factorization, solidifying why the conditions for positive definiteness emerge naturally.", "problem": "Consider a continuous-time linear time-invariant (LTI) system with state vector $x \\in \\mathbb{R}^{2}$ and a quadratic Lyapunov function candidate $V(x) = x^{\\top} P x$, where $P \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite. Let $P$ have the general form\n$$\nP \\;=\\; \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix},\n$$\nwith $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, $c \\in \\mathbb{R}$, and assume $a>0$ and $ac-b^2>0$ so that $P \\succ 0$. The factorization $P = R^{\\top} R$ with $R$ upper triangular and with positive diagonal entries is the Cholesky factorization and is useful for coordinate changes that render $V(x)$ a Euclidean norm.\n\nUsing only the following foundations: (i) the definition of positive definiteness of a quadratic form, (ii) completion of squares in a quadratic form in two variables, and (iii) the definition of the Cholesky factorization $P = R^{\\top} R$ with $R$ upper triangular and diagonal entries positive, perform the following:\n\n1. By completing the square on the quadratic form $V(x) = x^{\\top} P x$, construct explicitly an upper triangular matrix $R$ with positive diagonal entries such that $P = R^{\\top} R$.\n2. Compare your construction step-by-step with the standard Cholesky recursion specialized to the $2 \\times 2$ case, and argue their equivalence.\n\nProvide as your final answer a single closed-form analytic expression for the $(2,2)$ entry of $R$ in terms of $a$, $b$, and $c$. No numerical evaluation is required. Do not include any units. The final answer must be a single expression, not an inequality or an equation.", "solution": "The problem presented is a well-defined exercise in linear algebra, specifically concerning the relationship between a positive definite quadratic form and the Cholesky factorization of its associated symmetric matrix. The problem is scientifically grounded, self-contained, and objective. It is valid and warrants a full solution.\n\nLet the state vector be $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$. The quadratic form $V(x)$ is given by $V(x) = x^{\\top} P x$, where $P = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$. The matrix $P$ is symmetric and positive definite, which implies that its leading principal minors are positive. For a $2 \\times 2$ matrix, these conditions are $a > 0$ and $\\det(P) = ac-b^2 > 0$.\n\nExpanding the quadratic form, we have:\n$$ V(x) = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = a x_1^2 + 2b x_1 x_2 + c x_2^2 $$\n\n**Part 1: Construction of $R$ by Completing the Square**\n\nWe will restructure the expression for $V(x)$ as a sum of squares. Since $a > 0$, we can proceed by completing the square for the terms involving $x_1$:\n$$ V(x) = a \\left( x_1^2 + \\frac{2b}{a} x_1 x_2 \\right) + c x_2^2 $$\nTo complete the binomial square $(x_1 + k x_2)^2 = x_1^2 + 2k x_1 x_2 + k^2 x_2^2$, we identify $k = b/a$. We add and subtract the term $a(b/a)^2 x_2^2 = (b^2/a) x_2^2$:\n$$ V(x) = a \\left( x_1^2 + \\frac{2b}{a} x_1 x_2 + \\left(\\frac{b}{a}\\right)^2 x_2^2 \\right) - a \\left(\\frac{b}{a}\\right)^2 x_2^2 + c x_2^2 $$\n$$ V(x) = a \\left( x_1 + \\frac{b}{a} x_2 \\right)^2 + \\left( c - \\frac{b^2}{a} \\right) x_2^2 $$\nCombining the terms multiplying $x_2^2$ gives:\n$$ V(x) = a \\left( x_1 + \\frac{b}{a} x_2 \\right)^2 + \\left( \\frac{ac - b^2}{a} \\right) x_2^2 $$\nThe problem requires finding an upper triangular matrix $R$ such that $V(x) = (Rx)^{\\top}(Rx)$. Let $R$ be of the form $R = \\begin{pmatrix} r_{11} & r_{12} \\\\ 0 & r_{22} \\end{pmatrix}$. Then the vector $Rx$ is:\n$$ Rx = \\begin{pmatrix} r_{11} & r_{12} \\\\ 0 & r_{22} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} r_{11} x_1 + r_{12} x_2 \\\\ r_{22} x_2 \\end{pmatrix} $$\nThe squared Euclidean norm of this vector is:\n$$ (Rx)^{\\top}(Rx) = \\left\\| Rx \\right\\|_2^2 = (r_{11} x_1 + r_{12} x_2)^2 + (r_{22} x_2)^2 $$\nWe equate this expression with our completed square form of $V(x)$:\n$$ (r_{11} x_1 + r_{12} x_2)^2 + (r_{22} x_2)^2 = a \\left( x_1 + \\frac{b}{a} x_2 \\right)^2 + \\left( \\frac{ac - b^2}{a} \\right) x_2^2 $$\nThis can be rewritten as:\n$$ (r_{11} x_1 + r_{12} x_2)^2 + (r_{22})^2 x_2^2 = (\\sqrt{a})^2 \\left( x_1 + \\frac{b}{a} x_2 \\right)^2 + \\left( \\sqrt{\\frac{ac - b^2}{a}} \\right)^2 x_2^2 $$\n$$ (r_{11} x_1 + r_{12} x_2)^2 + (r_{22})^2 x_2^2 = \\left( \\sqrt{a} x_1 + \\frac{b}{\\sqrt{a}} x_2 \\right)^2 + \\left( \\sqrt{\\frac{ac - b^2}{a}} \\right)^2 x_2^2 $$\nBy comparing the terms, we can identify the entries of $R$. The problem requires positive diagonal entries. Given $a > 0$ and $ac-b^2 > 0$, the square roots of $a$ and $(ac-b^2)/a$ are well-defined real and positive numbers.\n$$ r_{11} x_1 + r_{12} x_2 = \\sqrt{a} x_1 + \\frac{b}{\\sqrt{a}} x_2 \\quad \\implies \\quad r_{11} = \\sqrt{a}, \\quad r_{12} = \\frac{b}{\\sqrt{a}} $$\n$$ r_{22}^2 x_2^2 = \\left( \\frac{ac - b^2}{a} \\right) x_2^2 \\quad \\implies \\quad r_{22} = \\sqrt{\\frac{ac - b^2}{a}} $$\nThus, the constructed matrix $R$ is:\n$$ R = \\begin{pmatrix} \\sqrt{a} & \\frac{b}{\\sqrt{a}} \\\\ 0 & \\sqrt{\\frac{ac-b^2}{a}} \\end{pmatrix} $$\nWe verify that $P = R^{\\top}R$:\n$$ R^{\\top}R = \\begin{pmatrix} \\sqrt{a} & 0 \\\\ \\frac{b}{\\sqrt{a}} & \\sqrt{\\frac{ac-b^2}{a}} \\end{pmatrix} \\begin{pmatrix} \\sqrt{a} & \\frac{b}{\\sqrt{a}} \\\\ 0 & \\sqrt{\\frac{ac-b^2}{a}} \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ b & \\frac{b^2}{a} + \\frac{ac-b^2}{a} \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} = P $$\nThis confirms the correctness of the construction.\n\n**Part 2: Comparison with Standard Cholesky Recursion**\n\nThe standard Cholesky factorization algorithm calculates the entries of $R$ from the equation $P = R^{\\top}R$. For the $2 \\times 2$ case, let $P=\\begin{pmatrix} p_{11} & p_{12} \\\\ p_{21} & p_{22} \\end{pmatrix}$ and $R=\\begin{pmatrix} r_{11} & r_{12} \\\\ 0 & r_{22} \\end{pmatrix}$.\n$$ \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{21} & p_{22} \\end{pmatrix} = \\begin{pmatrix} r_{11} & 0 \\\\ r_{12} & r_{22} \\end{pmatrix} \\begin{pmatrix} r_{11} & r_{12} \\\\ 0 & r_{22} \\end{pmatrix} = \\begin{pmatrix} r_{11}^2 & r_{11}r_{12} \\\\ r_{11}r_{12} & r_{12}^2 + r_{22}^2 \\end{pmatrix} $$\nFrom our problem, $p_{11}=a$, $p_{12}=b$, and $p_{22}=c$. The algorithm proceeds by equating matrix elements:\n1.  For element $(1,1)$: $p_{11} = r_{11}^2 \\implies a = r_{11}^2$. Since $r_{11}>0$, we get $r_{11} = \\sqrt{a}$.\n    This corresponds to factoring out $\\sqrt{a}$ from the first term in the completed square expression, $(\\sqrt{a}x_1 + \\dots)^2$. The requirement $a>0$ for positive definiteness is precisely what ensures $r_{11}$ is a real positive number.\n\n2.  For element $(1,2)$: $p_{12} = r_{11} r_{12} \\implies b = \\sqrt{a} r_{12}$. Solving for $r_{12}$ gives $r_{12} = b/\\sqrt{a}$.\n    This step is equivalent to selecting the coefficient of $x_2$ inside the parentheses during completion of the square. The term $2bx_1x_2$ in $V(x)$ is algebraically captured by the product $r_{11}r_{12}$, and solving for $r_{12}$ is equivalent to determining the mix term in $(r_{11}x_1 + r_{12}x_2)^2$.\n\n3.  For element $(2,2)$: $p_{22} = r_{12}^2 + r_{22}^2 \\implies c = (b/\\sqrt{a})^2 + r_{22}^2$. Solving for $r_{22}$ gives:\n    $r_{22}^2 = c - (b/\\sqrt{a})^2 = c - b^2/a = (ac-b^2)/a$.\n    Since $r_{22}>0$, we have $r_{22} = \\sqrt{(ac-b^2)/a}$.\n    This corresponds to the coefficient of the remaining $x_2^2$ term after completing the square. The operation $c - r_{12}^2$ is precisely the subtraction step in completion of squares that isolates the final squared term, $\\left(c - b^2/a\\right)x_2^2$. The positive definiteness condition $ac-b^2 > 0$ ensures that $r_{22}$ is real and positive.\n\nIn conclusion, the method of completing the square on the quadratic form $x^{\\top}Px$ is a conceptual instantiation of the Cholesky factorization algorithm. Each algebraic step in completing the square has a direct counterpart in the element-wise matrix calculation of the Cholesky recursion. Both methods rely on the same sequence of operations and the same conditions for positive definiteness to construct the unique upper triangular matrix $R$ with positive diagonal entries.\n\nThe $(2,2)$ entry of the matrix $R$ is $r_{22}$, which was derived as:\n$$ r_{22} = \\sqrt{\\frac{ac - b^2}{a}} $$", "answer": "$$\\boxed{\\sqrt{\\frac{ac - b^{2}}{a}}}$$", "id": "2735086"}, {"introduction": "The continuous-time Lyapunov equation, $A^{\\top} P + P A = -Q$, is the bedrock of stability analysis for linear systems. This practice explores a crucial limitation: why can we not find a quadratic Lyapunov function to certify the stability of an inherently unstable system? By applying first principles to the quadratic forms defined by this equation, you will derive the fundamental obstruction that an unstable eigenvalue of $A$ presents to finding a positive definite solution $P$, linking matrix definiteness directly to system dynamics [@problem_id:2735050].", "problem": "Consider the continuous-time Lyapunov equation for a real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a symmetric matrix $Q \\in \\mathbb{R}^{n \\times n}$,\n$$\nA^{\\top} P + P A = -Q,\n$$\nwhere $P \\in \\mathbb{R}^{n \\times n}$ is required to be symmetric positive definite. Let\n$$\nA \\triangleq \\begin{pmatrix} 1 & 1 \\\\ 0 & -2 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}.\n$$\nThis $A$ has an unstable eigenvalue. Using only the foundational definitions of positive definiteness of quadratic forms, eigenvalues and eigenvectors, and the bilinear form identity $x^{\\top} B y = y^{\\top} B^{\\top} x$ for conformable matrices $B$, do the following:\n\n1) Derive a necessary sign condition imposed by the equation on the scalar quantity $x^{\\top} \\left( A^{\\top} P + P A \\right) x$ when $x$ is a real eigenvector of $A$ associated with a real eigenvalue $\\lambda$. Conclude that, for the given $A$, any choice of symmetric $Q \\succ 0$ leads to an obstruction to the existence of a symmetric $P \\succ 0$ that satisfies the Lyapunov equation.\n\n2) Based on your reasoning in part $1)$, explicitly construct a symmetric $Q \\succ 0$ for which the Lyapunov equation with the given $A$ has no symmetric positive definite solution $P$.\n\nExpress your final answer as the explicit matrix you choose for $Q$, written in closed form. No rounding is required.", "solution": "The problem requires an analysis of the continuous-time Lyapunov equation, $A^{\\top} P + P A = -Q$, under specific constraints on the matrices involved. We are given the matrix $A \\triangleq \\begin{pmatrix} 1 & 1 \\\\ 0 & -2 \\end{pmatrix}$ and are tasked to investigate the existence of a symmetric positive definite solution $P \\in \\mathbb{R}^{2 \\times 2}$ when $Q \\in \\mathbb{R}^{2 \\times 2}$ is also symmetric and positive definite. The analysis must be based on foundational principles.\n\nFirst, we derive a necessary condition on the real eigenvalues of the matrix $A$ for such a solution $P$ to exist. Let us assume that a symmetric positive definite matrix $P$ and a symmetric positive definite matrix $Q$ exist that satisfy the equation $A^{\\top} P + P A = -Q$.\n\nLet $\\lambda$ be a real eigenvalue of $A$ and let $x \\in \\mathbb{R}^{n}$ be a corresponding non-zero eigenvector, so that $Ax = \\lambda x$. We form a quadratic form by pre-multiplying the Lyapunov equation by $x^{\\top}$ and post-multiplying by $x$:\n$$\nx^{\\top} \\left( A^{\\top} P + P A \\right) x = -x^{\\top} Q x\n$$\nThe left-hand side (LHS) can be expanded:\n$$\n\\text{LHS} = x^{\\top} A^{\\top} P x + x^{\\top} P A x\n$$\nWe analyze the first term, $x^{\\top} A^{\\top} P x$. This can be rewritten as $(A x)^{\\top} P x$. This transpose property, $(B y)^{\\top} = y^{\\top} B^{\\top}$, is a consequence of the provided identity $y^{\\top} B z = z^{\\top} B^{\\top} y$. By letting $z$ be an arbitrary vector, we have $(By)^{\\top}z = z^{\\top}(By) = y^{\\top} B^{\\top} z$, which implies $(By)^{\\top} = y^{\\top} B^{\\top}$ since $z$ is arbitrary.\nUsing this property and the eigenvector relation $Ax = \\lambda x$, the first term becomes:\n$$\nx^{\\top} A^{\\top} P x = (A x)^{\\top} P x = (\\lambda x)^{\\top} P x = \\lambda x^{\\top} P x\n$$\nThe second term, $x^{\\top} P A x$, is analyzed similarly:\n$$\nx^{\\top} P (A x) = x^{\\top} P (\\lambda x) = \\lambda x^{\\top} P x\n$$\nCombining these results, the LHS simplifies to:\n$$\n\\text{LHS} = \\lambda x^{\\top} P x + \\lambda x^{\\top} P x = 2 \\lambda (x^{\\top} P x)\n$$\nEquating the simplified LHS with the right-hand side (RHS) of the quadratic form equation, we obtain the key relationship:\n$$\n2 \\lambda (x^{\\top} P x) = -x^{\\top} Q x\n$$\nNow, we inspect the signs of the terms in this equation.\nBy definition, a symmetric matrix $P$ is positive definite ($P \\succ 0$) if and only if $v^{\\top} P v > 0$ for all non-zero vectors $v \\in \\mathbb{R}^{n}$. Since an eigenvector $x$ is, by definition, a non-zero vector, it follows that $x^{\\top} P x > 0$.\nSimilarly, as $Q$ is given to be symmetric and positive definite ($Q \\succ 0$), we must have $x^{\\top} Q x > 0$. Consequently, the term $-x^{\\top} Q x$ must be negative.\n\nOur equation can thus be written in terms of signs:\n$$\n2 \\lambda (\\text{positive number}) = (\\text{negative number})\n$$\nFor this equality to hold, the eigenvalue $\\lambda$ must be negative, i.e., $\\lambda < 0$. This constitutes a necessary condition: for the Lyapunov equation $A^{\\top} P + P A = -Q$ to have a symmetric positive definite solution $P$ for a given symmetric positive definite $Q$, any real eigenvalue of the matrix $A$ must be strictly negative.\n\nNow we apply this finding to the given matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & -2 \\end{pmatrix}$.\nThe matrix $A$ is upper triangular. The eigenvalues of a triangular matrix are its diagonal entries. Therefore, the eigenvalues of $A$ are $\\lambda_1 = 1$ and $\\lambda_2 = -2$.\nOne of these eigenvalues, $\\lambda_1 = 1$, is positive. This violates the necessary condition $\\lambda < 0$ that we have just derived.\nLet's make this more concrete. The eigenvector $x_1$ corresponding to $\\lambda_1 = 1$ is found by solving $(A - I)x_1 = 0$:\n$$\n\\begin{pmatrix} 1-1 & 1 \\\\ 0 & -2-1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & -3 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields $v_2 = 0$, while $v_1$ can be any non-zero real number. We choose a representative eigenvector $x_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nIf a symmetric positive definite solution $P$ were to exist for some symmetric positive definite $Q$, then substituting $\\lambda = 1$ and $x = x_1$ into our derived relation $2 \\lambda (x^{\\top} P x) = -x^{\\top} Q x$, we would get:\n$$\n2 (1) (x_1^{\\top} P x_1) = -x_1^{\\top} Q x_1\n$$\n$$\n2 (x_1^{\\top} P x_1) = -x_1^{\\top} Q x_1\n$$\nSince $P \\succ 0$ and $x_1 \\neq 0$, the term $x_1^{\\top} P x_1$ must be positive. Thus, the left side, $2 (x_1^{\\top} P x_1)$, is positive.\nSince $Q \\succ 0$ and $x_1 \\neq 0$, the term $x_1^{\\top} Q x_1$ must be positive. Thus, the right side, $-x_1^{\\top} Q x_1$, is negative.\nThis leads to the contradiction: a strictly positive number must be equal to a strictly negative number. This contradiction is independent of the particular choice of $Q$, as long as $Q$ is positive definite. Therefore, the initial assumption that a symmetric positive definite solution $P$ exists must be false. We conclude that for the given matrix $A$, any choice of a symmetric positive definite matrix $Q$ leads to an obstruction to the existence of a symmetric positive definite solution $P$.\n\nFor the second part of the problem, we must explicitly construct a symmetric matrix $Q \\succ 0$ for which the Lyapunov equation has no symmetric positive definite solution $P$. Based on the comprehensive reasoning above, any symmetric positive definite matrix $Q$ will suffice. The simplest and most canonical choice for a positive definite matrix is the identity matrix $I$.\nLet us choose $Q = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThis matrix is symmetric. Its eigenvalues are both $1$, which are positive, so $Q$ is positive definite.\nWith this choice, our argument has shown that the equation $A^{\\top} P + P A = -I$ cannot have a symmetric positive definite solution $P$. This explicitly constructed $Q$ satisfies the conditions of the problem.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}}\n$$", "id": "2735050"}, {"introduction": "Theoretical elegance does not always guarantee practical accuracy; numerical implementation can introduce significant errors. This problem confronts this reality by presenting a scenario where the direct evaluation of a quadratic form $x^{\\top} Q x$ suffers from catastrophic cancellation in finite-precision arithmetic, yielding a completely wrong result [@problem_id:2735080]. You will diagnose this failure and then apply the Cholesky factorization to restructure the calculation, demonstrating how a deeper understanding of matrix properties leads to numerically stable and reliable algorithms.", "problem": "In Lyapunov stability analysis for a linear time-invariant system, the candidate Lyapunov function is often a quadratic form $V(x) = x^{\\top} Q x$ with a symmetric positive definite matrix $Q \\in \\mathbb{R}^{n \\times n}$. In finite-precision arithmetic, evaluating $x^{\\top} Q x$ can suffer from catastrophic cancellation when $Q$ is nearly singular. Consider the specific instance with\n$$\nQ = \\begin{pmatrix}\n1 & 1 - \\delta \\\\\n1 - \\delta & 1\n\\end{pmatrix}, \\quad x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nwhere $\\delta \\in (0,1)$ is a small positive parameter. Assume arithmetic conforms to the Institute of Electrical and Electronics Engineers (IEEE) 754 model with rounding to nearest and unit roundoff $u$, and assume $0  \\delta  \\frac{u}{2}$. No fused multiply-add or compensated summation is available.\n\na) Using only core definitions and properties of symmetric positive definiteness, establish that $Q$ is symmetric positive definite for $\\delta \\in (0,1)$ and derive the exact analytic expression for $x^{\\top} Q x$ as a function of $\\delta$.\n\nb) Consider the naive finite-precision evaluation that first forms $y = Q x$ in floating point using the standard matrix-vector multiplication algorithm and then computes $x^{\\top} y$ as a dot product. Under the stated floating-point model and the assumption $0  \\delta  \\frac{u}{2}$, determine the value returned by this naive computation. Then propose a numerically stable restructuring that expresses $x^{\\top} Q x$ via either a symmetric matrix square root $Q^{1/2}$ or a Cholesky factorization $Q = R^{\\top} R$, and write the resulting exact analytic expression for $x^{\\top} Q x$ in that restructured form.\n\nc) Finally, under the same floating-point assumptions, compute the relative forward error of the naive computation in part b), defined as\n$$\n\\frac{\\left| \\widehat{V} - V \\right|}{V},\n$$\nwhere $V$ is the exact value of $x^{\\top} Q x$ from part a), and $\\widehat{V}$ is the finite-precision value returned by the naive computation in part b). Provide your final answer as a single exact real number with no units.", "solution": "This problem requires an analysis of a quadratic form's evaluation, both analytically and in finite-precision arithmetic. We will first validate the problem statement.\n\nThe givens are:\n- A quadratic form $V(x) = x^{\\top} Q x$.\n- A symmetric matrix $Q = \\begin{pmatrix} 1  1 - \\delta \\\\ 1 - \\delta  1 \\end{pmatrix}$.\n- A vector $x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n- A parameter $\\delta \\in (0,1)$.\n- A floating-point arithmetic model conforming to IEEE 754 with rounding to nearest and unit roundoff $u$.\n- A specific condition on the parameter: $0  \\delta  \\frac{u}{2}$.\n- Computational constraint: No fused multiply-add or compensated summation.\n\nThe problem is scientifically grounded in control theory and numerical linear algebra, discussing the well-known issue of catastrophic cancellation. It is well-posed, objective, and self-contained, with all necessary data and conditions provided. The assumptions, particularly regarding the floating-point model and the magnitude of $\\delta$, are standard for analyzing such numerical phenomena. The problem is non-trivial and requires careful application of definitions from linear algebra and numerical analysis. The verdict is that the problem is valid. We proceed to the solution.\n\na) First, we must establish that $Q$ is symmetric positive definite (SPD) for $\\delta \\in (0,1)$ and derive the exact expression for $V(x) = x^{\\top} Q x$.\n\nSymmetry: The matrix $Q$ is given as $Q = \\begin{pmatrix} 1  1 - \\delta \\\\ 1 - \\delta  1 \\end{pmatrix}$. By inspection, $Q_{12} = Q_{21} = 1 - \\delta$, so $Q = Q^{\\top}$. The matrix is symmetric.\n\nPositive Definiteness: We apply Sylvester's criterion, which states that a symmetric matrix is positive definite if and only if all its leading principal minors are positive.\nThe first leading principal minor is $\\Delta_1 = Q_{11} = 1$. This is positive.\nThe second leading principal minor is $\\Delta_2 = \\det(Q)$.\n$$\n\\det(Q) = (1)(1) - (1-\\delta)(1-\\delta) = 1 - (1-\\delta)^2 = 1 - (1 - 2\\delta + \\delta^2) = 2\\delta - \\delta^2 = \\delta(2-\\delta).\n$$\nFor the given range $\\delta \\in (0,1)$, we have $\\delta  0$ and $2-\\delta  1  0$. Therefore, $\\det(Q) = \\delta(2-\\delta)  0$.\nSince all leading principal minors are positive, the matrix $Q$ is positive definite for all $\\delta \\in (0,1)$.\n\nExact evaluation of $V(x)$: We compute $V(x) = x^{\\top} Q x$ with $x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n$$\nV(x) = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  1 - \\delta \\\\ 1 - \\delta  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nFirst, we compute the product $Qx$:\n$$\nQx = \\begin{pmatrix} 1  1 - \\delta \\\\ 1 - \\delta  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + (1-\\delta)(-1) \\\\ (1-\\delta)(1) + 1(-1) \\end{pmatrix} = \\begin{pmatrix} 1 - (1-\\delta) \\\\ (1-\\delta) - 1 \\end{pmatrix} = \\begin{pmatrix} \\delta \\\\ -\\delta \\end{pmatrix}\n$$\nNow, we compute the dot product $x^{\\top}(Qx)$:\n$$\nV(x) = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} \\delta \\\\ -\\delta \\end{pmatrix} = (1)(\\delta) + (-1)(-\\delta) = \\delta + \\delta = 2\\delta.\n$$\nThe exact analytic expression for $x^{\\top} Q x$ is $V = 2\\delta$.\n\nb) Next, we analyze the naive finite-precision evaluation and then propose a stable alternative.\n\nNaive computation: The process is to first compute $\\widehat{y} = \\text{fl}(Qx)$ and then $\\widehat{V} = \\text{fl}(x^{\\top}\\widehat{y})$. The floating-point operation $\\text{fl}(z)$ rounds a real number $z$ to the nearest number in the set of machine-representable numbers. The unit roundoff $u$ is the maximum relative error of this operation.\nLet $\\epsilon_m$ be the machine epsilon. For IEEE 754, $u = \\frac{\\epsilon_m}{2}$. The floating-point number preceding $1$ is $1-\\frac{\\epsilon_m}{2}$. The midpoint between $1-\\frac{\\epsilon_m}{2}$ and $1$ is $1-\\frac{\\epsilon_m}{4}$.\nThe condition $0  \\delta  \\frac{u}{2}$ means $0  \\delta  \\frac{\\epsilon_m}{4}$.\nThis implies $1 - \\frac{\\epsilon_m}{4}  1-\\delta  1$. Because $1-\\delta$ lies closer to $1$ than to the next smaller floating-point number, rounding to nearest gives:\n$$\n\\text{fl}(1-\\delta) = 1.\n$$\nNow we trace the computation of $\\widehat{y} = \\text{fl}(Qx)$. The multiplications by $1$ or $-1$ are exact.\n$$\n\\widehat{y}_1 = \\text{fl}(1 - (1-\\delta)) = \\text{fl}(1 - \\text{fl}(1-\\delta)) = \\text{fl}(1-1) = 0.\n$$\n$$\n\\widehat{y}_2 = \\text{fl}((1-\\delta) - 1) = \\text{fl}(\\text{fl}(1-\\delta) - 1) = \\text{fl}(1-1) = 0.\n$$\nThe computed intermediate vector is $\\widehat{y} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe final step is the dot product:\n$$\n\\widehat{V} = \\text{fl}(x^{\\top}\\widehat{y}) = \\text{fl}(1 \\cdot 0 + (-1) \\cdot 0) = \\text{fl}(0) = 0.\n$$\nThe naive computation yields a value of $0$. This is a result of catastrophic cancellation in the subtraction of nearly equal numbers.\n\nNumerically stable restructuring: The problem suggests using a Cholesky factorization $Q=R^{\\top}R$, where $R$ is an upper triangular matrix. Let $R = \\begin{pmatrix} r_{11}  r_{12} \\\\ 0  r_{22} \\end{pmatrix}$.\n$$\nR^{\\top}R = \\begin{pmatrix} r_{11}  0 \\\\ r_{12}  r_{22} \\end{pmatrix} \\begin{pmatrix} r_{11}  r_{12} \\\\ 0  r_{22} \\end{pmatrix} = \\begin{pmatrix} r_{11}^2  r_{11}r_{12} \\\\ r_{11}r_{12}  r_{12}^2 + r_{22}^2 \\end{pmatrix}.\n$$\nEquating this with $Q = \\begin{pmatrix} 1  1-\\delta \\\\ 1-\\delta  1 \\end{pmatrix}$:\n$r_{11}^2 = 1 \\implies r_{11}=1$ (by convention, diagonal elements are positive).\n$r_{11}r_{12} = 1-\\delta \\implies r_{12} = 1-\\delta$.\n$r_{12}^2 + r_{22}^2 = 1 \\implies r_{22}^2 = 1-r_{12}^2 = 1-(1-\\delta)^2 = 2\\delta-\\delta^2 = \\delta(2-\\delta)$.\nSo, $r_{22} = \\sqrt{\\delta(2-\\delta)}$. Note that this expression for $r_{22}$ is numerically stable for small $\\delta$, as it does not involve subtraction of nearly equal numbers.\nThe Cholesky factor is $R = \\begin{pmatrix} 1  1-\\delta \\\\ 0  \\sqrt{\\delta(2-\\delta)} \\end{pmatrix}$.\nThe quadratic form can be restructured as $V(x) = x^{\\top}Qx = x^{\\top}R^{\\top}Rx = (Rx)^{\\top}(Rx) = \\|Rx\\|_2^2$.\nLet $z=Rx$. The components of $z$ are:\n$z_1 = r_{11}x_1 + r_{12}x_2 = 1(1) + (1-\\delta)(-1) = 1 - (1-\\delta) = \\delta$.\n$z_2 = r_{22}x_2 = \\sqrt{\\delta(2-\\delta)}(-1) = -\\sqrt{\\delta(2-\\delta)}$.\nThe restructured expression for $x^\\top Q x$ in terms of these components is $V(x) = z_1^2 + z_2^2$. Substituting the expressions for $z_1$ and $z_2$ yields:\n$$\nV(x) = \\delta^2 + \\left(-\\sqrt{\\delta(2-\\delta)}\\right)^2 = \\delta^2 + \\delta(2-\\delta).\n$$\nThis expression simplifies to $\\delta^2 + 2\\delta - \\delta^2 = 2\\delta$, confirming the analytical result. This form is numerically stable because the catastrophic cancellation is avoided algebraically. A direct computation of $\\delta^2 + \\delta(2-\\delta)$ in floating-point arithmetic would yield an accurate result.\n\nc) Finally, we compute the relative forward error of the naive computation.\nThe relative forward error is defined as $\\frac{|\\widehat{V}-V|}{V}$.\nFrom part a), the exact value is $V = 2\\delta$.\nFrom part b), the value from the naive finite-precision computation is $\\widehat{V} = 0$.\nSubstituting these values into the error formula:\n$$\n\\text{Relative Error} = \\frac{|0 - 2\\delta|}{2\\delta}\n$$\nSince $\\delta  0$, we have $|-2\\delta| = 2\\delta$.\n$$\n\\text{Relative Error} = \\frac{2\\delta}{2\\delta} = 1.\n$$\nThe naive computation suffers from a complete loss of relative accuracy, resulting in a relative error of $1$, which corresponds to a $100\\%$ error.", "answer": "$$\\boxed{1}$$", "id": "2735080"}]}