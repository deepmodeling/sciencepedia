{"hands_on_practices": [{"introduction": "At the heart of Polynomial Chaos Expansion for Gaussian uncertainties lies the elegant structure of Hermite polynomials. This exercise guides you through a foundational derivation of their three-term recurrence relation, a crucial property for both theoretical understanding and efficient numerical implementation. Mastering this derivation will provide you with a deeper appreciation for the mathematical machinery that makes PCE a powerful tool for uncertainty quantification. [@problem_id:2671707]", "problem": "A one-dimensional linear elastic bar of length $L$ is subjected to a prescribed axial displacement at $x=L$, while the Young's modulus $E$ is modeled as a lognormal random field with a single dominant standard normal mode $Z \\sim \\mathcal{N}(0,1)$. In a non-intrusive Polynomial Chaos Expansion (PCE) analysis of the bar’s axial displacement $u(L;Z)$, one employs a basis of orthonormal polynomials $\\{\\psi_{n}(x)\\}_{n \\geq 0}$ with respect to the standard normal probability measure, defined by the inner product\n$$\n\\langle f,g \\rangle = \\int_{-\\infty}^{\\infty} f(x)\\,g(x)\\,\\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)\\,dx = \\mathbb{E}[f(Z)\\,g(Z)].\n$$\nThese orthonormal polynomials are the orthonormal Hermite polynomials associated with the standard normal measure. A fundamental tool for assembling the PCE numerically is the three-term recurrence relation satisfied by the orthonormal basis. Starting only from the definition of orthonormality under the standard normal measure, the Rodrigues representation of the (probabilists’) Hermite polynomials,\n$$\n\\mathrm{He}_{n}(x) = (-1)^{n}\\exp\\!\\left(\\frac{x^{2}}{2}\\right)\\frac{d^{n}}{dx^{n}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right),\n$$\nand standard properties of integration by parts, derive the three-term recurrence\n$$\nx\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x), \\quad n \\geq 0,\n$$\nfor the orthonormal Hermite basis $\\{\\psi_{n}\\}$ under the standard normal weight, and specify the recurrence coefficients $a_{n}$ and $b_{n}$ as explicit functions of $n$. You may assume $\\psi_{-1} \\equiv 0$ by convention. Express your final answer as the row matrix $[\\,a_{n}\\;\\;b_{n}\\,]$. No numerical rounding is required, and no physical units apply.", "solution": "The problem as stated is valid. It is a well-posed mathematical exercise in the theory of orthogonal polynomials, which is a cornerstone of Polynomial Chaos Expansions in uncertainty quantification. All necessary definitions and properties are provided to derive a unique and verifiable result. We shall proceed with the derivation.\n\nThe objective is to derive the coefficients $a_n$ and $b_n$ for the three-term recurrence relation satisfied by the orthonormal Hermite polynomials $\\{\\psi_n(x)\\}_{n \\ge 0}$:\n$$\nx\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x)\n$$\nThe polynomials are orthonormal with respect to the standard normal weight function $w(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$. The inner product is $\\langle f,g \\rangle = \\int_{-\\infty}^{\\infty} f(x)g(x)w(x)dx$.\n\nThe polynomials $\\{\\psi_n(x)\\}$ are related to the orthogonal (but not orthonormal) probabilists' Hermite polynomials $\\{\\mathrm{He}_n(x)\\}$ by a normalization constant. Let $\\psi_n(x) = C_n \\mathrm{He}_n(x)$, where $C_n = 1/\\|\\mathrm{He}_n\\|$. The norm is defined as $\\|\\mathrm{He}_n\\| = \\sqrt{\\langle \\mathrm{He}_n, \\mathrm{He}_n \\rangle}$.\n\nFirst, we must calculate the squared norm, $\\|\\mathrm{He}_n\\|^2$. We use the provided Rodrigues' formula for $\\mathrm{He}_n(x)$:\n$$\n\\mathrm{He}_{n}(x) = (-1)^{n}\\exp\\left(\\frac{x^{2}}{2}\\right)\\frac{d^{n}}{dx^{n}}\\exp\\left(-\\frac{x^{2}}{2}\\right)\n$$\nThe squared norm is:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\langle \\mathrm{He}_n, \\mathrm{He}_n \\rangle = \\int_{-\\infty}^{\\infty} \\mathrm{He}_n(x) \\left[ (-1)^{n} \\exp\\left(\\frac{x^2}{2}\\right) \\frac{d^n}{dx^n}\\exp\\left(-\\frac{x^2}{2}\\right) \\right] \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThis simplifies to:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{(-1)^n}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\mathrm{He}_n(x) \\left( \\frac{d^n}{dx^n}\\exp\\left(-\\frac{x^2}{2}\\right) \\right) dx\n$$\nWe apply integration by parts $n$ times. For each step, the boundary terms of the form $[\\dots \\exp(-x^2/2)]_{-\\infty}^\\infty$ vanish because the exponential term decays to zero faster than any polynomial can grow. After $n$ applications, the derivatives are transferred from the exponential term to $\\mathrm{He}_n(x)$:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{(-1)^n(-1)^n}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\left( \\frac{d^n}{dx^n} \\mathrm{He}_n(x) \\right) \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThe polynomial $\\mathrm{He}_n(x)$ is of degree $n$ with a leading coefficient of $1$. This can be shown by induction from the Rodrigues formula or by using the known property $\\mathrm{He}_n'(x) = n \\mathrm{He}_{n-1}(x)$. It follows that the $n$-th derivative is a constant: $\\frac{d^n}{dx^n} \\mathrm{He}_n(x) = n!$.\nSubstituting this into the integral:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{n!}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{2}\\right) dx = n! \\times 1 = n!\n$$\nThe normalization constant is therefore $C_n = 1/\\|\\mathrm{He}_n\\| = 1/\\sqrt{n!}$. The orthonormal polynomial is $\\psi_n(x) = \\frac{1}{\\sqrt{n!}} \\mathrm{He}_n(x)$.\n\nNext, we establish the three-term recurrence for the orthogonal polynomials $\\mathrm{He}_n(x)$. The polynomial $x \\mathrm{He}_n(x)$ has degree $n+1$ and can be expressed in the basis of Hermite polynomials:\n$$\nx \\mathrm{He}_n(x) = \\sum_{k=0}^{n+1} c_k \\mathrm{He}_k(x)\n$$\nThe coefficient $c_k$ is found using orthogonality: $c_k = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_k \\rangle}{\\|\\mathrm{He}_k\\|^2} = \\frac{\\langle \\mathrm{He}_n, x \\mathrm{He}_k \\rangle}{\\|\\mathrm{He}_k\\|^2}$. Since $x\\mathrm{He}_k(x)$ is a polynomial of degree $k+1$, the inner product $\\langle \\mathrm{He}_n, x \\mathrm{He}_k \\rangle$ is zero if $n > k+1$, i.e., $k  n-1$. Thus, only the coefficients for $k=n+1, n, n-1$ can be non-zero. The recurrence has the form:\n$$\nx \\mathrm{He}_n(x) = c_{n+1} \\mathrm{He}_{n+1}(x) + c_n \\mathrm{He}_n(x) + c_{n-1} \\mathrm{He}_{n-1}(x)\n$$\nThe coefficients are:\n1. $c_{n+1}$: The leading term of $\\mathrm{He}_n(x)$ is $x^n$. Thus, the leading term of $x \\mathrm{He}_n(x)$ is $x^{n+1}$. The leading term of the right-hand side is $c_{n+1}x^{n+1}$. Equating the leading coefficients gives $c_{n+1}=1$.\n2. $c_n$: $c_n = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_n \\rangle}{\\|\\mathrm{He}_n\\|^2}$. The integrand for the inner product is $x (\\mathrm{He}_n(x))^2 w(x)$. The polynomial $\\mathrm{He}_n(x)$ has parity $(-1)^n$. Thus $(\\mathrm{He}_n(x))^2$ is an even function. The weight function $w(x)$ is also even. The term $x$ is odd. The total integrand is an odd function integrated over a symmetric domain $(-\\infty, \\infty)$, so the integral is zero. Hence, $c_n=0$.\n3. $c_{n-1}$: $c_{n-1} = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_{n-1} \\rangle}{\\|\\mathrm{He}_{n-1}\\|^2} = \\frac{\\langle \\mathrm{He}_n, x \\mathrm{He}_{n-1} \\rangle}{\\|\\mathrm{He}_{n-1}\\|^2}$. We know $x \\mathrm{He}_{n-1}(x) = \\mathrm{He}_n(x) + c_{n-2} \\mathrm{He}_{n-2}(x) + \\dots$. Using orthogonality: $\\langle \\mathrm{He}_n, x \\mathrm{He}_{n-1} \\rangle = \\langle \\mathrm{He}_n, \\mathrm{He}_n(x) \\rangle = \\|\\mathrm{He}_n\\|^2 = n!$.\nSo, $c_{n-1} = \\frac{n!}{\\|\\mathrm{He}_{n-1}\\|^2} = \\frac{n!}{(n-1)!} = n$.\n\nThe recurrence relation for the orthogonal polynomials $\\mathrm{He}_n(x)$ is:\n$$\nx \\mathrm{He}_n(x) = \\mathrm{He}_{n+1}(x) + n \\mathrm{He}_{n-1}(x)\n$$\nNow, substitute $\\mathrm{He}_k(x) = \\sqrt{k!} \\psi_k(x)$ into this relation:\n$$\nx \\sqrt{n!} \\psi_n(x) = \\sqrt{(n+1)!} \\psi_{n+1}(x) + n \\sqrt{(n-1)!} \\psi_{n-1}(x)\n$$\nDivide the entire equation by $\\sqrt{n!}$:\n$$\nx \\psi_n(x) = \\frac{\\sqrt{(n+1)!}}{\\sqrt{n!}} \\psi_{n+1}(x) + \\frac{n\\sqrt{(n-1)!}}{\\sqrt{n!}} \\psi_{n-1}(x)\n$$\n$$\nx \\psi_n(x) = \\sqrt{n+1} \\psi_{n+1}(x) + \\frac{n}{\\sqrt{n}} \\psi_{n-1}(x)\n$$\n$$\nx \\psi_n(x) = \\sqrt{n+1} \\psi_{n+1}(x) + \\sqrt{n} \\psi_{n-1}(x)\n$$\nWe compare this result with the target form $x\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x)$.\n- The coefficient of $\\psi_{n+1}(x)$ gives $a_{n+1} = \\sqrt{n+1}$. This implies $a_k = \\sqrt{k}$ for $k \\ge 1$.\n- The coefficient of $\\psi_n(x)$ is zero, so $b_n = 0$ for all $n \\ge 0$.\n- The coefficient of $\\psi_{n-1}(x)$ gives $a_n = \\sqrt{n}$ for $n \\ge 1$.\nFor $n=0$, the relation is $x\\psi_0(x) = a_1 \\psi_1(x) + b_0 \\psi_0(x)$ given $\\psi_{-1}=0$. Our derived relation for $n=0$ is $x\\psi_0(x) = \\sqrt{1}\\psi_1(x) + \\sqrt{0}\\psi_{-1}(x) = \\psi_1(x)$, which implies $a_1=1$ and $b_0=0$. This is consistent with our general formulas. The formula $a_n=\\sqrt{n}$ can be extended to $n=0$, where $a_0=0$. With the convention $\\psi_{-1}=0$, the term $a_0 \\psi_{-1}(x)$ vanishes, making the value of $a_0$ arbitrary, but $a_0=0$ is the consistent choice.\n\nThus, the recurrence coefficients are $a_n = \\sqrt{n}$ and $b_n = 0$ for $n \\ge 0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{n}  0\n\\end{pmatrix}\n}\n$$", "id": "2671707"}, {"introduction": "Once a Polynomial Chaos Expansion has been constructed, it becomes more than just an approximation; it becomes a powerful analytical tool. This practice demonstrates one of the most significant benefits of PCE: the ability to efficiently compute variance-based sensitivity indices. You will derive the direct relationship between PCE coefficients and first-order Sobol indices, transforming the abstract expansion into concrete insights about the drivers of model uncertainty. [@problem_id:2671662]", "problem": "A straight, prismatic axial bar in linear elasticity is subjected to a deterministic axial load, producing a tip displacement denoted by $Y$. The uncertainty in the bar’s material and geometric parameters is modeled by two independent standardized inputs $\\boldsymbol{\\xi}=(\\xi_{1},\\xi_{2})$, and the response $Y$ is approximated by a Polynomial Chaos Expansion (PCE) constructed on an orthonormal tensor-product polynomial basis $\\{\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\}$ indexed by multi-indices $\\alpha=(\\alpha_{1},\\alpha_{2})\\in\\mathbb{N}_{0}^{2}$, such that $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}]=\\delta_{\\alpha\\beta}$ and $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for all $\\alpha\\neq (0,0)$. The resulting truncated PCE has the form\n$$\nY \\approx \\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\quad\\text{with}\\quad c_{\\alpha}\\in\\mathbb{R}.\n$$\nYou may assume the inputs are independent and that the orthonormal basis is consistent with their probability laws.\n\nTask 1. Starting from the definition of variance and the Analysis of Variance (ANOVA) decomposition underlying variance-based Sobol sensitivity indices, and using only the orthonormality of the polynomial chaos basis and independence of inputs, derive an expression for the first-order Sobol index $S_{i}$ of input $\\xi_{i}$ solely in terms of the PCE coefficients $\\{c_{\\alpha}\\}$.\n\nTask 2. For the specific two-dimensional expansion\n$$\nY \\approx c_{(0,0)}\\Psi_{(0,0)} + c_{(1,0)}\\Psi_{(1,0)} + c_{(0,1)}\\Psi_{(0,1)} + c_{(2,0)}\\Psi_{(2,0)} + c_{(0,2)}\\Psi_{(0,2)} + c_{(1,1)}\\Psi_{(1,1)},\n$$\nwith coefficients\n- $c_{(0,0)} = 1.0$,\n- $c_{(1,0)} = 0.8$,\n- $c_{(0,1)} = 0.6$,\n- $c_{(2,0)} = 0.3$,\n- $c_{(0,2)} = 0.1$,\n- $c_{(1,1)} = 0.4$,\ncompute the first-order Sobol index $S_{1}$ for $\\xi_{1}$ using your derived expression.\n\nRound your final numeric answer to four significant figures. Express the result as a pure decimal (unitless).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard task in the field of uncertainty quantification, specifically connecting Polynomial Chaos Expansion (PCE) with variance-based sensitivity analysis (Sobol indices). All necessary definitions, such as the orthonormality of the basis and independence of the inputs, are provided. The numerical values are consistent. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is divided into a derivation and a subsequent calculation.\n\nTask 1: Derivation of the First-Order Sobol Index $S_i$\n\nThe analysis begins from the fundamental definitions of variance and the Sobol indices. The total variance of the model output $Y$ is denoted by $D = \\text{Var}(Y)$. The first-order Sobol index for an input parameter $\\xi_i$ is defined as the fraction of the total variance contributed by $\\xi_i$ alone:\n$$\nS_i = \\frac{D_i}{D}\n$$\nwhere $D_i = \\text{Var}(\\mathbb{E}[Y | \\xi_i])$ is the first-order partial variance associated with $\\xi_i$.\n\nWe must express both $D$ and $D_i$ in terms of the PCE coefficients $\\{c_{\\alpha}\\}$. The PCE of the response $Y$ is given as:\n$$\nY(\\boldsymbol{\\xi}) \\approx \\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nThe basis functions $\\{\\Psi_{\\alpha}\\}$ are orthonormal with respect to the probability measure of the inputs $\\boldsymbol{\\xi}$, meaning $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$. Also, $\\mathbb{E}[\\Psi_{\\alpha}] = 0$ for $\\alpha \\neq \\mathbf{0}$, and $\\Psi_{\\mathbf{0}}$ is a constant, which we normalize to $\\Psi_{\\mathbf{0}} = 1$.\n\nFirst, we compute the mean and total variance of $Y$. The mean of $Y$ is:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\right] = \\sum_{\\alpha} c_{\\alpha}\\,\\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})]\n$$\nDue to the property $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for $\\alpha\\neq\\mathbf{0}$ and $\\mathbb{E}[\\Psi_{\\mathbf{0}}]=1$, the sum reduces to a single term:\n$$\n\\mathbb{E}[Y] = c_{\\mathbf{0}}\\,\\mathbb{E}[\\Psi_{\\mathbf{0}}] = c_{\\mathbf{0}}\n$$\nThe total variance $D$ is then:\n$$\nD = \\text{Var}(Y) = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2] = \\mathbb{E}\\left[\\left(\\sum_{\\alpha} c_{\\alpha}\\Psi_{\\alpha} - c_{\\mathbf{0}}\\right)^2\\right]\n$$\nSubstituting $c_{\\mathbf{0}} = c_{\\mathbf{0}}\\Psi_{\\mathbf{0}}$:\n$$\nD = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}\\Psi_{\\alpha}\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}\\Psi_{\\alpha}\\right) \\left(\\sum_{\\beta \\neq \\mathbf{0}} c_{\\beta}\\Psi_{\\beta}\\right)\\right]\n$$\nExpanding the product and applying the expectation operator:\n$$\nD = \\sum_{\\alpha \\neq \\mathbf{0}} \\sum_{\\beta \\neq \\mathbf{0}} c_{\\alpha} c_{\\beta} \\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}]\n$$\nBy orthonormality, $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$, so the expression simplifies to:\n$$\nD = \\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}^2\n$$\nThis is a fundamental result: the total variance of the PCE model is the sum of the squares of all non-constant coefficients.\n\nNext, we compute the partial variance $D_i$. We begin by evaluating the conditional expectation $\\mathbb{E}[Y | \\xi_i]$. The basis functions are of a tensor-product form, $\\Psi_{\\alpha}(\\boldsymbol{\\xi}) = \\prod_{k=1}^{d} \\psi_{\\alpha_k}(\\xi_k)$, where $d$ is the dimension of $\\boldsymbol{\\xi}$.\n$$\n\\mathbb{E}[Y | \\xi_i] = \\mathbb{E}\\left[\\sum_{\\alpha} c_{\\alpha} \\prod_{k=1}^{d} \\psi_{\\alpha_k}(\\xi_k) \\Bigg| \\xi_i \\right]\n$$\nBecause the inputs $\\{\\xi_k\\}$ are independent, the expectation with respect to $\\boldsymbol{\\xi}_{\\sim i}$ (all variables except $\\xi_i$) can be taken inside the sum and product:\n$$\n\\mathbb{E}[Y | \\xi_i] = \\sum_{\\alpha} c_{\\alpha} \\psi_{\\alpha_i}(\\xi_i) \\prod_{k \\neq i} \\mathbb{E}[\\psi_{\\alpha_k}(\\xi_k)]\n$$\nThe univariate polynomials are also orthonormal, implying $\\mathbb{E}[\\psi_{j}(\\xi_k)] = \\delta_{j0}$. The product term $\\prod_{k \\neq i} \\mathbb{E}[\\psi_{\\alpha_k}(\\xi_k)]$ is therefore equal to $1$ if and only if $\\alpha_k = 0$ for all $k \\neq i$, and it is $0$ otherwise. This restricts the sum to multi-indices $\\alpha$ that have non-zero components only at the $i$-th position (plus the zero vector $\\mathbf{0}$).\nLet's define the set of such indices as $\\mathcal{I}_i^* = \\{\\alpha \\in \\mathbb{N}_0^d \\mid \\alpha_j = 0 \\text{ for all } j \\neq i\\}$. The conditional expectation becomes:\n$$\n\\mathbb{E}[Y | \\xi_i] = \\sum_{\\alpha \\in \\mathcal{I}_i^*} c_{\\alpha} \\Psi_{\\alpha}(\\boldsymbol{\\xi}) = c_{\\mathbf{0}}\\Psi_{\\mathbf{0}} + \\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nwhere $\\mathcal{I}_i = \\{\\alpha \\in \\mathbb{N}_0^d \\mid \\alpha_i > 0 \\text{ and } \\alpha_j = 0 \\text{ for all } j \\neq i\\}$. The functions $\\Psi_{\\alpha}$ for $\\alpha \\in \\mathcal{I}_i$ depend only on $\\xi_i$.\n\nNow we compute the variance of this quantity, $D_i = \\text{Var}(\\mathbb{E}[Y | \\xi_i])$. The constant term $c_{\\mathbf{0}}\\Psi_{\\mathbf{0}}$ does not contribute to the variance.\n$$\nD_i = \\text{Var}\\left(\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right) = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right)^2\\right] - \\left(\\mathbb{E}\\left[\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right]\\right)^2\n$$\nThe second term is zero because $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for all $\\alpha \\in \\mathcal{I}_i$. Using the same logic as for the total variance $D$, the orthonormality of the basis functions leads to:\n$$\nD_i = \\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha}^2\n$$\nThis demonstrates that the partial variance due to $\\xi_i$ is the sum of the squares of the coefficients corresponding to basis functions that depend only on $\\xi_i$.\n\nFinally, the first-order Sobol index $S_i$ is the ratio of the partial variance $D_i$ to the total variance $D$:\n$$\nS_i = \\frac{D_i}{D} = \\frac{\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha}^2}{\\sum_{\\beta \\neq \\mathbf{0}} c_{\\beta}^2}\n$$\nThis is the required expression.\n\nTask 2: Calculation of $S_1$\n\nFor the specific two-dimensional case, $\\boldsymbol{\\xi}=(\\xi_1, \\xi_2)$, the PCE is given as:\n$$\nY \\approx c_{(0,0)}\\Psi_{(0,0)} + c_{(1,0)}\\Psi_{(1,0)} + c_{(0,1)}\\Psi_{(0,1)} + c_{(2,0)}\\Psi_{(2,0)} + c_{(0,2)}\\Psi_{(0,2)} + c_{(1,1)}\\Psi_{(1,1)}\n$$\nThe provided coefficients are:\n$c_{(0,0)} = 1.0$, $c_{(1,0)} = 0.8$, $c_{(0,1)} = 0.6$, $c_{(2,0)} = 0.3$, $c_{(0,2)} = 0.1$, $c_{(1,1)} = 0.4$.\n\nWe must calculate $S_1$. The denominator is the total variance $D$, which is the sum of squares of all coefficients except $c_{(0,0)}$:\n$$\nD = c_{(1,0)}^2 + c_{(0,1)}^2 + c_{(2,0)}^2 + c_{(0,2)}^2 + c_{(1,1)}^2\n$$\nSubstituting the values:\n$$\nD = (0.8)^2 + (0.6)^2 + (0.3)^2 + (0.1)^2 + (0.4)^2\n$$\n$$\nD = 0.64 + 0.36 + 0.09 + 0.01 + 0.16 = 1.26\n$$\nThe numerator is the partial variance $D_1$. According to our derived formula, this is the sum of squares of coefficients whose multi-indices are in $\\mathcal{I}_1$. For this problem, $\\mathcal{I}_1$ consists of indices of the form $(\\alpha_1, 0)$ with $\\alpha_1 > 0$. The relevant indices from the given expansion are $(1,0)$ and $(2,0)$.\n$$\nD_1 = c_{(1,0)}^2 + c_{(2,0)}^2\n$$\nSubstituting the values:\n$$\nD_1 = (0.8)^2 + (0.3)^2 = 0.64 + 0.09 = 0.73\n$$\nThe first-order Sobol index for $\\xi_1$ is therefore:\n$$\nS_1 = \\frac{D_1}{D} = \\frac{0.73}{1.26} \\approx 0.579365079...\n$$\nRounding to four significant figures, we obtain $S_1 = 0.5794$.", "answer": "$$\n\\boxed{0.5794}\n$$", "id": "2671662"}, {"introduction": "While PCE provides an elegant framework, its practical implementation is governed by computational cost, a challenge that grows dramatically with the number of uncertain parameters. This exercise confronts the 'curse of dimensionality' head-on by calculating the number of model evaluations required for a standard non-intrusive method based on tensor-product quadrature. This analysis will give you a crucial, practical perspective on the scalability of PCE and the motivation behind more advanced integration and sampling schemes. [@problem_id:2671736]", "problem": "Consider a linear elastic solid modeled by the small-strain equations with an uncertain Young’s modulus field represented by a Karhunen–Loève (KL) expansion truncated to $d$ independent standardized random variables $\\boldsymbol{\\xi} = (\\xi_{1},\\dots,\\xi_{d})$. You seek an intrusive Polynomial Chaos Expansion (PCE) of total degree $p$ for the displacement field within a Stochastic Galerkin Method (SGM). To assemble the Galerlin system, you must evaluate expectations of products of basis functions and model terms, which, under standard assumptions of independence and use of orthonormal polynomial bases adapted to the marginals of $\\xi_{i}$, reduce to integrals of multivariate polynomials of total degree up to $2p$ with respect to the product measure of $\\boldsymbol{\\xi}$. \n\nAssume you choose a tensor-product Gaussian quadrature rule formed from a one-dimensional Gaussian quadrature with $m$ points per dimension that is appropriate to the orthogonal polynomial family associated with each $\\xi_{i}$ (for example, Gauss–Hermite for Gaussian or Gauss–Legendre for uniform). Starting from the exactness property of Gaussian quadrature and the orthogonality of polynomial chaos bases, determine the minimal $m$ such that the resulting $d$-dimensional tensor-product quadrature exactly integrates every polynomial integrand arising from products of PCE basis functions up to total degree $2p$. Then, derive a formula $N(d,p)$ giving the total number of quadrature points required by this tensor-product rule. Finally, evaluate $N(d,p)$ for $d=6$ and $p=4$. \n\nProvide your final answer as a single real-valued number. No rounding is necessary. Also, briefly discuss in words (no calculation required) whether this number of points is practically feasible for assembling stochastic Galerkin systems in solid mechanics with large-scale finite element discretizations, assuming each quadrature evaluation requires a deterministic residual and tangent computation at the sample.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique solution based on established principles of numerical analysis for uncertainty quantification.\n\nThe problem asks for the minimal number of one-dimensional quadrature points, $m$, required for a $d$-dimensional tensor-product quadrature rule to exactly integrate multivariate polynomials of total degree up to $2p$. It then asks for the total number of quadrature points, $N(d,p)$, and its value for $d=6$ and $p=4$.\n\nFirst, let us establish the exactness property of a one-dimensional Gaussian quadrature rule. A rule with $m$ points and corresponding weights is constructed to be exact for polynomials of the highest possible degree. For a given weight function, the $m$-point Gaussian quadrature rule integrates polynomials of degree up to $2m-1$ exactly.\n\nThe problem requires the numerical integration of multivariate polynomials $P(\\boldsymbol{\\xi})$, where $\\boldsymbol{\\xi} = (\\xi_{1}, \\dots, \\xi_{d})$, of total degree up to $2p$. The total degree of a multivariate polynomial is the maximum sum of the exponents of the variables in any of its monomial terms. A tensor-product quadrature rule is employed, which is formed by taking the product of $d$ one-dimensional quadrature rules. The integral of a function $f(\\xi_1, \\dots, \\xi_d)$ with respect to the product measure $d\\mu(\\boldsymbol{\\xi}) = d\\mu_1(\\xi_1) \\dots d\\mu_d(\\xi_d)$ is approximated as:\n$$\n\\int_{\\mathbb{R}^d} f(\\boldsymbol{\\xi}) d\\mu(\\boldsymbol{\\xi}) \\approx \\sum_{i_1=1}^{m} \\dots \\sum_{i_d=1}^{m} f(\\xi_{1,i_1}, \\dots, \\xi_{d,i_d}) w_{1,i_1} \\dots w_{d,i_d}\n$$\nThis tensor-product rule is exact for a multivariate polynomial if each one-dimensional rule is exact for the corresponding one-dimensional polynomials that constitute the multivariate polynomial.\n\nTo determine the required exactness for the one-dimensional rule, we must find the maximum polynomial degree that can appear for any single variable, $\\xi_j$, within a multivariate polynomial of total degree $2p$. Consider a monomial term of the form $\\xi_1^{\\alpha_1} \\xi_2^{\\alpha_2} \\dots \\xi_d^{\\alpha_d}$ with total degree $\\sum_{i=1}^d \\alpha_i \\le 2p$. The highest possible degree for a single variable, say $\\xi_j$, occurs when all other variables have degree $0$. For instance, the monomial $\\xi_j^{2p}$ has a total degree of $2p$. To integrate such a term exactly, the one-dimensional quadrature rule for the variable $\\xi_j$ must be exact for polynomials of degree up to $2p$.\n\nTherefore, the one-dimensional $m$-point Gaussian quadrature must satisfy the condition:\n$$\n2m - 1 \\ge 2p\n$$\nTo find the minimal integer $m$ that satisfies this inequality, we rearrange it:\n$$\n2m \\ge 2p + 1\n$$\n$$\nm \\ge p + \\frac{1}{2}\n$$\nSince $m$ must be an integer, the minimal value for $m$ is the smallest integer greater than or equal to $p + \\frac{1}{2}$, which is:\n$$\nm = p + 1\n$$\n\nNext, we derive the formula for the total number of quadrature points, $N(d,p)$. The quadrature rule is a tensor product of $d$ one-dimensional rules, each using $m$ points. The total number of points in the resulting $d$-dimensional grid is the product of the number of points in each dimension.\n$$\nN = m \\times m \\times \\dots \\times m \\quad (d \\text{ times})\n$$\n$$\nN = m^d\n$$\nSubstituting the derived expression for $m$, we obtain the formula for $N$ as a function of $d$ and $p$:\n$$\nN(d,p) = (p+1)^d\n$$\n\nFinally, we evaluate this formula for the specific case given in the problem: $d=6$ (number of random dimensions) and $p=4$ (total degree of the polynomial chaos expansion).\n$$\nN(6,4) = (4+1)^6 = 5^6\n$$\nCalculating the value:\n$$\n5^2 = 25\n$$\n$$\n5^3 = 125\n$$\n$$\n5^4 = 625\n$$\n$$\n5^5 = 3125\n$$\n$$\n5^6 = 15625\n$$\nSo, a total of $15625$ quadrature points are required.\n\nRegarding the practical feasibility of this number of points: Assembling the stochastic Galerkin system requires evaluating the deterministic model (in this context, solving the system of equations from the finite element discretization of the solid mechanics problem) at each of the $N(d,p)$ quadrature points. With $15625$ points, this implies that $15625$ separate, full-scale deterministic simulations must be performed. For any large-scale, non-trivial engineering problem, where a single deterministic simulation can take from minutes to many hours on high-performance computing resources, executing this many simulations is computationally prohibitive. This explosive growth in the number of required points with increasing dimension $d$ and polynomial degree $p$ is a manifestation of the \"curse of dimensionality\". Consequently, for problems with even a moderate number of random variables, the full tensor-product quadrature approach is practically infeasible. This has motivated the development and widespread use of more efficient techniques, such as sparse grid quadrature (e.g., Smolyak's algorithm), which provides a much more favorable scaling of computational cost with dimensionality.", "answer": "$$\n\\boxed{15625}\n$$", "id": "2671736"}]}