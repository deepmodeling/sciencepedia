{"hands_on_practices": [{"introduction": "A common method for enforcing boundary conditions in PINNs is to add a penalty term to the loss function, a technique known as \"soft\" enforcement. However, a more robust and often more effective approach is to design the neural network's architecture to satisfy the boundary conditions by construction. This \"hard\" enforcement can eliminate the boundary loss term and its associated weighting hyperparameter, often leading to more stable and efficient training. This practice [@problem_id:2126300] challenges you to devise such a transformation, providing a foundational skill for building reliable PINN models.", "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.", "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.", "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$", "id": "2126300"}, {"introduction": "The core of any Physics-Informed Neural Network is the \"physics\" itself, encoded as the residual of the governing partial differential equations. For problems in solid mechanics, this means translating the fundamental principles of equilibrium, kinematic compatibility, and constitutive behavior into a computable loss term. This exercise [@problem_id:2668927] provides essential hands-on practice by guiding you through the derivation of the equilibrium residual for 2D linear elasticity, starting from first principles and culminating in a concrete evaluation for a simple neural network ansatz.", "problem": "Consider a two-dimensional, small-strain, linear elastic solid in a quasi-static setting with body force per unit volume $\\mathbf{b} = (b_x, b_y)$. Let the displacement field be $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$. Starting only from the balance of linear momentum, the small-strain kinematic relation, and the linear isotropic constitutive law in terms of the Lamé parameters $(\\lambda, \\mu)$, do the following:\n\n1) Write the constitutive law for the Cauchy stress tensor $\\boldsymbol{\\sigma}$ in terms of the strain tensor $\\boldsymbol{\\varepsilon}$ and $(\\lambda,\\mu)$, and define the small-strain tensor in terms of the displacement field. Express all objects in Cartesian coordinates.\n\n2) Using your definitions, expand the interior equilibrium residual components that a Physics-Informed Neural Network (PINN) would enforce at interior collocation points, namely the two components of $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$, explicitly in Cartesian coordinates in terms of spatial derivatives of $u_x$ and $u_y$ and the Lamé parameters $(\\lambda,\\mu)$.\n\n3) Now consider a one-hidden-neuron neural network ansatz for the displacement field with hyperbolic tangent activation,\n$$\nz(x,y) = w_1 x + w_2 y + b_1,\\quad \\mathbf{u}(x,y) = \\mathbf{W}_2 \\,\\tanh\\!\\big(z(x,y)\\big) + \\mathbf{b}_2,\n$$\nwith $\\mathbf{W}_2 \\in \\mathbb{R}^{2 \\times 1}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$. Take the specific parameters\n$$\nw_1 = 1,\\quad w_2 = 2,\\quad b_1 = 0,\\quad \\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad \\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\nand assume zero body force $\\mathbf{b}=\\mathbf{0}$. Evaluate the two components of the interior equilibrium residual vector at the point $(x_0,y_0)=(1,0)$.\n\nAssume a nondimensionalized formulation so that all quantities are dimensionless. Express the final result as a single row vector containing the two residual components, written as a closed-form analytic expression. Do not approximate or round any constants; leave hyperbolic functions unevaluated (for example, write $\\tanh(1)$, $\\cosh(1)$ explicitly).", "solution": "The problem is validated as well-posed, scientifically grounded, and containing all necessary information for a unique solution. We proceed with the derivation.\n\nThe problem requires a step-by-step derivation, starting from fundamental principles of continuum mechanics, to evaluate the equilibrium residual for a given neural network ansatz for the displacement field.\n\nFirst, we address Task 1: defining the relevant kinematic and constitutive relations in Cartesian coordinates.\n\nIn a two-dimensional domain with coordinates $(x,y)$, the displacement field is $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$. The small-strain (or infinitesimal strain) tensor $\\boldsymbol{\\varepsilon}$ is defined in terms of the gradient of the displacement field as:\n$$\n\\boldsymbol{\\varepsilon} = \\frac{1}{2} \\left[ \\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T \\right]\n$$\nIn Cartesian component form, this gives:\n$$\n\\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x}, \\quad \\varepsilon_{yy} = \\frac{\\partial u_y}{\\partial y}, \\quad \\varepsilon_{xy} = \\varepsilon_{yx} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\nThe trace of the strain tensor, which represents the volumetric strain, is $\\text{tr}(\\boldsymbol{\\varepsilon}) = \\varepsilon_{kk} = \\varepsilon_{xx} + \\varepsilon_{yy}$.\n\nFor a linear, isotropic, and elastic material, the constitutive law relating the Cauchy stress tensor $\\boldsymbol{\\sigma}$ to the strain tensor $\\boldsymbol{\\varepsilon}$ is given by Hooke's Law, which can be expressed using the Lamé parameters $\\lambda$ and $\\mu$ (also known as the first and second Lamé parameters, where $\\mu$ is the shear modulus):\n$$\n\\boldsymbol{\\sigma} = \\lambda \\, \\text{tr}(\\boldsymbol{\\varepsilon}) \\, \\mathbf{I} + 2\\mu \\, \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{I}$ is the second-order identity tensor. In Cartesian component form, the stress components are:\n$$\n\\sigma_{xx} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{xx} = (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{yy} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{yy} = \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{xy} = \\sigma_{yx} = 2\\mu \\varepsilon_{xy} = \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n\nNext, we address Task 2: deriving the explicit form of the interior equilibrium residual components. The balance of linear momentum in a quasi-static setting is the equilibrium equation $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b} = \\mathbf{0}$. A Physics-Informed Neural Network (PINN) seeks to minimize the residual of this equation, which is $\\mathbf{R} = \\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$. The components of this residual vector in Cartesian coordinates are:\n$$\nR_x = \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xy}}{\\partial y} + b_x\n$$\n$$\nR_y = \\frac{\\partial \\sigma_{yx}}{\\partial x} + \\frac{\\partial \\sigma_{yy}}{\\partial y} + b_y\n$$\nSubstituting the expressions for the stress components in terms of displacement derivatives yields the Navier-Cauchy equations. We assume the material is homogeneous, so $\\lambda$ and $\\mu$ are constants.\nFor the $x$-component of the residual:\n$$\nR_x = \\frac{\\partial}{\\partial x} \\left( (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + b_x\n$$\n$$\nR_x = (\\lambda + 2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_y}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_x}{\\partial y^2} + \\mu\\frac{\\partial^2 u_y}{\\partial y \\partial x} + b_x\n$$\nBy equality of mixed partials ($\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial^2 u_y}{\\partial y \\partial x}$), we can group the terms:\n$$\nR_x = (\\lambda+2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\mu \\frac{\\partial^2 u_x}{\\partial y^2} + (\\lambda+\\mu) \\frac{\\partial^2 u_y}{\\partial x \\partial y} + b_x\n$$\nFor the $y$-component of the residual:\n$$\nR_y = \\frac{\\partial}{\\partial x} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + \\frac{\\partial}{\\partial y} \\left( \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y} \\right) + b_y\n$$\n$$\nR_y = \\mu\\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_y}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_x}{\\partial y \\partial x} + (\\lambda + 2\\mu)\\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\nAgain, grouping terms:\n$$\nR_y = (\\lambda+\\mu) \\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu \\frac{\\partial^2 u_y}{\\partial x^2} + (\\lambda+2\\mu) \\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\nThese are the explicit expressions for the residual components that a PINN would enforce.\n\nFinally, we address Task 3: evaluating these residuals for the specified neural network ansatz and parameters. The ansatz is given by:\n$z(x,y) = w_1 x + w_2 y + b_1$ and $\\mathbf{u}(x,y) = \\mathbf{W}_2 \\tanh(z(x,y)) + \\mathbf{b}_2$.\nWith the given parameters $w_1 = 1$, $w_2 = 2$, $b_1 = 0$, $\\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, and $\\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we have:\n$z(x,y) = x + 2y$\nAnd the displacement components are:\n$$\nu_x(x,y) = 2 \\tanh(x+2y)\n$$\n$$\nu_y(x,y) = -1 \\tanh(x+2y)\n$$\nWe must compute the second-order partial derivatives of $u_x$ and $u_y$. Let $T(x,y) = \\tanh(x+2y)$ and $S(x,y) = \\text{sech}^2(x+2y) = 1-\\tanh^2(x+2y)$. We use the chain rule and the derivative identities $\\frac{d}{d\\alpha}\\tanh(\\alpha) = \\text{sech}^2(\\alpha)$ and $\\frac{d}{d\\alpha}\\text{sech}^2(\\alpha) = -2\\tanh(\\alpha)\\text{sech}^2(\\alpha)$.\n\nDerivatives of $u_x = 2T$:\n$\\frac{\\partial u_x}{\\partial x} = 2 \\frac{\\partial T}{\\partial x} = 2S \\cdot 1 = 2S$\n$\\frac{\\partial u_x}{\\partial y} = 2 \\frac{\\partial T}{\\partial y} = 2S \\cdot 2 = 4S$\n$\\frac{\\partial^2 u_x}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2S) = 2(-2TS) \\cdot 1 = -4TS$\n$\\frac{\\partial^2 u_x}{\\partial y^2} = \\frac{\\partial}{\\partial y}(4S) = 4(-2TS) \\cdot 2 = -16TS$\n$\\frac{\\partial^2 u_x}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2S) = 2(-2TS) \\cdot 2 = -8TS$\n\nDerivatives of $u_y = -T$:\n$\\frac{\\partial u_y}{\\partial x} = -S \\cdot 1 = -S$\n$\\frac{\\partial u_y}{\\partial y} = -S \\cdot 2 = -2S$\n$\\frac{\\partial^2 u_y}{\\partial x^2} = \\frac{\\partial}{\\partial x}(-S) = -(-2TS) \\cdot 1 = 2TS$\n$\\frac{\\partial^2 u_y}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2S) = -2(-2TS) \\cdot 2 = 8TS$\n$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(-S) = -(-2TS) \\cdot 2 = 4TS$\n\nWe are given zero body force, so $\\mathbf{b}=\\mathbf{0}$. We substitute these derivatives into the residual expressions:\n$$\nR_x = (\\lambda+2\\mu)(-4TS) + \\mu(-16TS) + (\\lambda+\\mu)(4TS)\n$$\n$$\nR_x = [-4(\\lambda+2\\mu) - 16\\mu + 4(\\lambda+\\mu)]TS = [-4\\lambda - 8\\mu - 16\\mu + 4\\lambda + 4\\mu]TS = -20\\mu TS\n$$\n$$\nR_y = (\\lambda+\\mu)(-8TS) + \\mu(2TS) + (\\lambda+2\\mu)(8TS)\n$$\n$$\nR_y = [-8(\\lambda+\\mu) + 2\\mu + 8(\\lambda+2\\mu)]TS = [-8\\lambda - 8\\mu + 2\\mu + 8\\lambda + 16\\mu]TS = 10\\mu TS\n$$\nThe residual vector is $\\mathbf{R}(x,y) = \\begin{pmatrix} -20\\mu & 10\\mu \\end{pmatrix} \\tanh(x+2y)\\text{sech}^2(x+2y)$.\n\nWe must evaluate this at the point $(x_0, y_0) = (1, 0)$. At this point, the argument of the hyperbolic functions is $z_0 = 1 + 2(0) = 1$.\nThe term $TS$ becomes $\\tanh(1)\\text{sech}^2(1)$. Using the identity $\\text{sech}^2(\\alpha) = 1/\\cosh^2(\\alpha)$, this is $\\frac{\\tanh(1)}{\\cosh^2(1)}$.\nThe residual components at $(1, 0)$ are:\n$$\nR_x(1,0) = -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n$$\nR_y(1,0) = 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\nThe problem asks for the result as a single row vector.\n$$\n\\mathbf{R}(1,0) = \\begin{pmatrix} -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} \\end{pmatrix}\n$$\nThis is the final analytical expression.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n\\end{pmatrix}\n}\n$$", "id": "2668927"}, {"introduction": "A theoretically sound PINN formulation can still face significant challenges during training due to numerical ill-conditioning inherent in the physical model. One of the most classic and important examples in solid mechanics is \"volumetric locking,\" which occurs in nearly incompressible materials as Poisson's ratio $\\nu$ approaches $0.5$. This advanced problem [@problem_id:2668960] asks you to analyze the mathematical origin of this instability and explore the use of a mixed formulation, a powerful technique used across computational mechanics to create more robust models for such challenging physical regimes.", "problem": "Consider small-strain, static, isotropic linear elasticity in spatial dimension $d \\in \\{2,3\\}$. The Cauchy stress tensor $\\boldsymbol{\\sigma}$ and the small-strain tensor $\\boldsymbol{\\varepsilon}(\\mathbf{u}) = \\tfrac{1}{2}\\left(\\nabla \\mathbf{u} + \\nabla \\mathbf{u}^{\\top}\\right)$ are related by the isotropic Hooke law $\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}$, where $(\\lambda,\\mu)$ are the Lamé parameters. The Young’s modulus $E$ and the Poisson’s ratio $\\nu$ are defined by a uniaxial test: prescribe a uniaxial Cauchy stress $\\sigma_{11} = \\sigma$ with lateral stresses $\\sigma_{22}=\\sigma_{33}=0$ (for $d=3$; adapt accordingly for $d=2$ plane stress), and define $E = \\sigma_{11}/\\varepsilon_{11}$ and $\\nu = -\\varepsilon_{22}/\\varepsilon_{11} = -\\varepsilon_{33}/\\varepsilon_{11}$ under this loading. Starting from these definitions and the constitutive law, derive $(\\lambda,\\mu)$ in terms of $(E,\\nu)$.\n\nNow consider training a Physics-Informed Neural Network (PINN) that outputs the displacement field $\\mathbf{u}(\\mathbf{x})$ to solve the equilibrium partial differential equation (PDE) $\\nabla \\cdot \\boldsymbol{\\sigma}(\\mathbf{u}) = \\mathbf{0}$ with given boundary conditions, using the above constitutive model. Discuss, from first principles, how the limit $\\nu \\to 0.5$ (nearly incompressible) impacts the conditioning of the PDE residual and the learnability of $\\mathbf{u}$ in a displacement-only PINN, and what modeling or loss-design changes can remedy this.\n\nWhich of the following statements are correct?\n\nA. The derived mapping yields $\\mu = \\dfrac{E}{2(1+\\nu)}$ and $\\lambda = \\dfrac{E\\,\\nu}{(1+\\nu)(1-2\\nu)}$.\n\nB. As $\\nu \\to 0.5$, the parameter $\\lambda$ remains bounded while $\\mu \\to \\infty$, so the shear term dominates the PDE residual; therefore near-incompressibility does not cause ill-conditioning in displacement-only PINNs.\n\nC. In the nearly incompressible regime, a displacement-only PINN commonly suffers from an ill-conditioned loss landscape dominated by volumetric contributions. Simply rescaling inputs and outputs or reweighting loss terms is, by itself, sufficient to fully resolve training pathologies for all $\\nu$ arbitrarily close to $0.5$.\n\nD. A robust remedy is a mixed displacement–pressure formulation in which one introduces $p$ as an additional unknown and writes $\\boldsymbol{\\sigma}(\\mathbf{u},p) = 2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u}) - p\\,\\mathbf{I}$, together with a volumetric closure $p + \\kappa\\,\\nabla \\cdot \\mathbf{u} = 0$ where $\\kappa = \\lambda + \\tfrac{2}{3}\\mu$ is the bulk modulus. The momentum equation becomes $\\nabla \\cdot \\left(2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u})\\right) - \\nabla p = \\mathbf{0}$, which avoids unbounded coefficients in the momentum residual as $\\nu \\to 0.5$ and can be stably enforced in a PINN by separately scaling the constraint.\n\nE. Adaptive loss balancing based on the Neural Tangent Kernel (NTK) guarantees that a displacement-only PINN trained on the original momentum residual with $(\\lambda,\\mu)$ remains well-conditioned as $\\nu \\to 0.5$, obviating the need for mixed formulations.", "solution": "The problem statement will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Problem Domain**: Small-strain, static, isotropic linear elasticity.\n-   **Spatial Dimension**: $d \\in \\{2,3\\}$.\n-   **Strain Tensor Definition**: $\\boldsymbol{\\varepsilon}(\\mathbf{u}) = \\tfrac{1}{2}\\left(\\nabla \\mathbf{u} + \\nabla \\mathbf{u}^{\\top}\\right)$.\n-   **Constitutive Law (Hooke's Law)**: $\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}$, where $(\\lambda,\\mu)$ are Lamé parameters.\n-   **Material Properties from Uniaxial Test**:\n    -   Loading condition: $\\sigma_{11} = \\sigma$, with lateral stresses $\\sigma_{22}=\\sigma_{33}=0$ (for $d=3$).\n    -   Young’s Modulus, $E$: $E = \\sigma_{11}/\\varepsilon_{11}$.\n    -   Poisson’s Ratio, $\\nu$: $\\nu = -\\varepsilon_{22}/\\varepsilon_{11} = -\\varepsilon_{33}/\\varepsilon_{11}$.\n-   **Task 1**: Derive $(\\lambda,\\mu)$ in terms of $(E,\\nu)$.\n-   **PINN Context**: Solve the equilibrium equation $\\nabla \\cdot \\boldsymbol{\\sigma}(\\mathbf{u}) = \\mathbf{0}$ using a physics-informed neural network (PINN) that outputs the displacement field $\\mathbf{u}(\\mathbf{x})$.\n-   **Task 2**: Analyze the impact of the nearly incompressible limit $\\nu \\to 0.5$ on a displacement-only PINN and discuss remedies.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the fundamental theory of linear elasticity, a cornerstone of continuum mechanics. All equations and definitions—Cauchy stress, small-strain tensor, isotropic Hooke's law, equilibrium equation, and the definitions of $E$ and $\\nu$—are standard and factually correct.\n2.  **Well-Posed**: The derivation of Lamé parameters from engineering constants is a standard, well-posed algebraic problem. The analysis of numerical ill-conditioning (volumetric locking) as $\\nu \\to 0.5$ is a classic topic in computational mechanics, and its application to PINNs is a relevant and formalizable modern research question.\n3.  **Objective**: The language is precise, technical, and free of ambiguity or subjective claims.\n\nThe problem statement is self-contained, scientifically sound, and well-posed. There are no contradictions, missing data, or other invalidating flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation and Analysis\n\n**Part 1: Derivation of $(\\lambda, \\mu)$ in terms of $(E, \\nu)$**\n\nWe start with the isotropic Hooke's law:\n$$\n\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}\n$$\nWe need to invert this relation to express $\\boldsymbol{\\varepsilon}$ as a function of $\\boldsymbol{\\sigma}$. Taking the trace of the equation (for dimension $d=3$):\n$$\n\\mathrm{tr}(\\boldsymbol{\\sigma}) = \\mathrm{tr}(2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathbf{I}) = 2\\,\\mu\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon}) + \\lambda\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\,\\mathrm{tr}(\\mathbf{I}) = (2\\mu + 3\\lambda)\\,\\mathrm{tr}(\\boldsymbol{\\varepsilon})\n$$\nThus, the volumetric strain is:\n$$\n\\mathrm{tr}(\\boldsymbol{\\varepsilon}) = \\frac{\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\n$$\nSubstituting this back into the first equation:\n$$\n\\boldsymbol{\\sigma} = 2\\,\\mu\\,\\boldsymbol{\\varepsilon} + \\lambda \\left(\\frac{\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\\right)\\mathbf{I}\n$$\nSolving for $\\boldsymbol{\\varepsilon}$:\n$$\n2\\,\\mu\\,\\boldsymbol{\\varepsilon} = \\boldsymbol{\\sigma} - \\frac{\\lambda\\,\\mathrm{tr}(\\boldsymbol{\\sigma})}{2\\mu + 3\\lambda}\\,\\mathbf{I} \\implies \\boldsymbol{\\varepsilon} = \\frac{1}{2\\mu}\\boldsymbol{\\sigma} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\mathrm{tr}(\\boldsymbol{\\sigma})\\,\\mathbf{I}\n$$\nNow, we apply the conditions of the uniaxial test: $\\sigma_{11} = \\sigma$ and all other $\\sigma_{ij}=0$. This gives $\\mathrm{tr}(\\boldsymbol{\\sigma}) = \\sigma$.\nThe strain components are:\n$$\n\\varepsilon_{11} = \\frac{1}{2\\mu}\\sigma_{11} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\sigma = \\frac{\\sigma}{2\\mu}\\left(1 - \\frac{\\lambda}{2\\mu + 3\\lambda}\\right) = \\frac{\\sigma}{2\\mu}\\left(\\frac{2\\mu + 3\\lambda - \\lambda}{2\\mu + 3\\lambda}\\right) = \\sigma\\frac{\\mu+\\lambda}{\\mu(2\\mu+3\\lambda)}\n$$\n$$\n\\varepsilon_{22} = \\frac{1}{2\\mu}\\sigma_{22} - \\frac{\\lambda}{2\\mu(2\\mu + 3\\lambda)}\\sigma = 0 - \\frac{\\lambda\\sigma}{2\\mu(2\\mu + 3\\lambda)}\n$$\nUsing the definitions of $E$ and $\\nu$:\n$$\nE = \\frac{\\sigma_{11}}{\\varepsilon_{11}} = \\frac{\\sigma}{\\sigma\\frac{\\mu+\\lambda}{\\mu(2\\mu+3\\lambda)}} = \\frac{\\mu(2\\mu+3\\lambda)}{\\mu+\\lambda}\n$$\n$$\n\\nu = -\\frac{\\varepsilon_{22}}{\\varepsilon_{11}} = -\\frac{-\\lambda\\sigma / [2\\mu(2\\mu+3\\lambda)]}{\\sigma(\\mu+\\lambda) / [\\mu(2\\mu+3\\lambda)]} = \\frac{\\lambda}{2(\\mu+\\lambda)}\n$$\nWe now solve the system of two equations for $\\mu$ and $\\lambda$. From the equation for $\\nu$:\n$2\\nu(\\mu+\\lambda) = \\lambda \\implies 2\\nu\\mu = \\lambda(1-2\\nu) \\implies \\lambda = \\frac{2\\nu\\mu}{1-2\\nu}$.\nSubstitute this into the equation for $E$:\n$E = 2\\mu(1+\\nu)$. To see this: $E = \\frac{\\mu(2\\mu+3\\lambda)}{\\mu+\\lambda} = \\frac{\\mu(2\\mu+3\\frac{2\\nu\\mu}{1-2\\nu})}{\\mu+\\frac{2\\nu\\mu}{1-2\\nu}} = \\mu \\frac{2(1-2\\nu)+6\\nu}{1-2\\nu+2\\nu} = \\mu(2-4\\nu+6\\nu) = 2\\mu(1+\\nu)$.\nFrom $E = 2\\mu(1+\\nu)$, we find the shear modulus $\\mu$:\n$$\n\\mu = \\frac{E}{2(1+\\nu)}\n$$\nNow substitute this $\\mu$ back into the expression for $\\lambda$:\n$$\n\\lambda = \\frac{2\\nu}{1-2\\nu}\\mu = \\frac{2\\nu}{1-2\\nu} \\frac{E}{2(1+\\nu)} = \\frac{E\\nu}{(1+\\nu)(1-2\\nu)}\n$$\nThe derivation is complete.\n\n**Part 2: Analysis of PINNs for Nearly Incompressible Elasticity ($\\nu \\to 0.5$)**\n\nThe equilibrium PDE in terms of displacement is the Navier-Cauchy equation:\n$$\n\\nabla \\cdot \\boldsymbol{\\sigma} = \\mu \\nabla^2 \\mathbf{u} + (\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u}) = \\mathbf{0}\n$$\nA displacement-only PINN minimizes a loss based on the residual of this equation. We examine the behavior of the coefficients as $\\nu \\to 0.5$:\n-   $\\mu = \\frac{E}{2(1+\\nu)} \\to \\frac{E}{2(1.5)} = \\frac{E}{3}$. The coefficient $\\mu$ remains bounded and well-behaved.\n-   $\\lambda = \\frac{E\\nu}{(1+\\nu)(1-2\\nu)} \\to \\infty$ because the denominator $(1-2\\nu) \\to 0$.\nThe coefficient of the volumetric term, $(\\lambda+\\mu)$, therefore also diverges: $(\\lambda+\\mu) \\to \\infty$.\n\nThe PDE becomes dominated by the term $(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u})$. For the equation to hold, the network must learn a displacement field $\\mathbf{u}$ such that $\\nabla \\cdot \\mathbf{u} \\approx 0$ with extreme precision. The loss function becomes\n$\\mathcal{L}_{PDE} \\approx ||(\\lambda+\\mu) \\nabla(\\nabla \\cdot \\mathbf{u})||^2$. The extremely large pre-factor $(\\lambda+\\mu)$ creates an ill-conditioned loss landscape with enormous gradients related to any non-zero volumetric strain. This phenomenon, known as **volumetric locking**, makes it extremely difficult for gradient-based optimizers to find a good solution. The training stagnates, unable to resolve the shear behavior because the loss is completely dominated by the penalty on volumetric deformation.\n\n### Evaluation of Options\n\n**A. The derived mapping yields $\\mu = \\dfrac{E}{2(1+\\nu)}$ and $\\lambda = \\dfrac{E\\,\\nu}{(1+\\nu)(1-2\\nu)}$.**\nOur derivation in Part 1 confirms these expressions precisely.\n**Verdict: Correct.**\n\n**B. As $\\nu \\to 0.5$, the parameter $\\lambda$ remains bounded while $\\mu \\to \\infty$, so the shear term dominates the PDE residual; therefore near-incompressibility does not cause ill-conditioning in displacement-only PINNs.**\nThis statement is incorrect on multiple counts. As derived, when $\\nu \\to 0.5$, $\\mu$ remains bounded while $\\lambda \\to \\infty$. The term that dominates the PDE is the volumetric term containing $\\lambda$, not the shear term containing $\\mu$. Consequently, near-incompressibility is a classic cause of severe ill-conditioning (locking).\n**Verdict: Incorrect.**\n\n**C. In the nearly incompressible regime, a displacement-only PINN commonly suffers from an ill-conditioned loss landscape dominated by volumetric contributions. Simply rescaling inputs and outputs or reweighting loss terms is, by itself, sufficient to fully resolve training pathologies for all $\\nu$ arbitrarily close to $0.5$.**\nThe first part of the statement, describing the ill-conditioned loss landscape, is correct. However, the second part makes an overly strong claim. While simple techniques like reweighting loss terms can provide some relief for moderate values of $\\nu$, they are not a \"sufficient\" remedy to \"fully resolve\" the pathology for $\\nu$ arbitrarily close to $0.5$. The problem is fundamental to the displacement-only formulation, which becomes singular in the limit. Such simple fixes do not change the formulation and cannot robustly handle the singularity. A more profound change, such as a mixed formulation, is required for robust performance in the severe incompressible regime.\n**Verdict: Incorrect.**\n\n**D. A robust remedy is a mixed displacement–pressure formulation in which one introduces $p$ as an additional unknown and writes $\\boldsymbol{\\sigma}(\\mathbf{u},p) = 2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u}) - p\\,\\mathbf{I}$, together with a volumetric closure $p + \\kappa\\,\\nabla \\cdot \\mathbf{u} = 0$ where $\\kappa = \\lambda + \\tfrac{2}{3}\\mu$ is the bulk modulus. The momentum equation becomes $\\nabla \\cdot \\left(2\\,\\mu\\,\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}(\\mathbf{u})\\right) - \\nabla p = \\mathbf{0}$, which avoids unbounded coefficients in the momentum residual as $\\nu \\to 0.5$ and can be stably enforced in a PINN by separately scaling the constraint.**\nThis accurately describes the standard mixed $u-p$ formulation. The decomposition of stress into deviatoric and hydrostatic parts, $\\boldsymbol{\\sigma} = 2\\mu\\boldsymbol{\\varepsilon}^{\\mathrm{dev}} - p\\mathbf{I}$, with pressure $p = -\\kappa \\nabla \\cdot \\mathbf{u}$, is correct. The resulting equilibrium equation $\\nabla \\cdot (2\\mu\\boldsymbol{\\varepsilon}^{\\mathrm{dev}}) - \\nabla p = \\mathbf{0}$ involves only the bounded shear modulus $\\mu$. The unboundedness is isolated in the scalar constitutive relation for pressure, $p + \\kappa \\nabla \\cdot \\mathbf{u} = 0$, where the bulk modulus $\\kappa = E/(3(1-2\\nu)) \\to \\infty$. By treating the momentum equation and the pressure constraint as two separate residuals in the PINN loss, one can apply separate scaling or weighting to manage the stiff constraint, leading to a much more stable training process. This is the standard and effective approach to overcome volumetric locking.\n**Verdict: Correct.**\n\n**E. Adaptive loss balancing based on the Neural Tangent Kernel (NTK) guarantees that a displacement-only PINN trained on the original momentum residual with $(\\lambda,\\mu)$ remains well-conditioned as $\\nu \\to 0.5$, obviating the need for mixed formulations.**\nThis is an overstatement. NTK-based weighting typically balances different loss *terms* (e.g., PDE residual vs. boundary condition residual). It does not address the ill-conditioning *within* the single PDE residual term caused by the diverging ratio of material parameters $(\\lambda+\\mu)/\\mu$. The gradients of the loss function would still be pathologically dominated by the volumetric component. Furthermore, no adaptive weighting scheme can \"guarantee\" a well-conditioned problem in a singular limit without changing the underlying physical formulation. Mixed formulations represent a change in the formulation itself, which is a more fundamental and robust solution than re-weighting the loss of an ill-posed system.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "2668960"}]}