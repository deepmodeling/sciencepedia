{"hands_on_practices": [{"introduction": "A fundamental task in uncertainty quantification is to analyze data from experiments or simulations to make inferences about a system's behavior. This practice builds a foundational understanding of how to quantify the uncertainty in an estimated quantity, such as a mean response, directly from a sample of data. By deriving the confidence interval for the mean compliance of a beam with random stiffness, you will see firsthand how the powerful Central Limit Theorem provides a rigorous basis for statistical inference in mechanics [@problem_id:2707385].", "problem": "A linear elastic cantilever beam with deterministic geometry and loading is made of a material whose Young’s modulus is random due to manufacturing variability. Let the compliance, defined as the work conjugate to the applied load, be the scalar quantity $C = \\mathbf{f}^{\\top}\\mathbf{u}$, where $\\mathbf{f}$ is the applied nodal force vector and $\\mathbf{u}$ is the resulting nodal displacement vector obtained by solving the deterministic equilibrium equations for a realization of the Young’s modulus field. Suppose that $C$ is a real-valued random variable with finite mean $\\mu_{C} = \\mathbb{E}[C]$ and finite, strictly positive variance $\\sigma_{C}^{2} = \\mathrm{Var}(C)$, and that independent and identically distributed realizations $\\{C_{i}\\}_{i=1}^{n}$ are obtained by repeated experiments or simulations under the same load and geometry.\n\nDefine the sample mean estimator of the compliance by $\\bar{C}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} C_{i}$ and the sample variance by $s_{n}^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (C_{i} - \\bar{C}_{n})^{2}$.\n\nStarting from the assumptions of independence, identical distribution, and the finiteness of $\\sigma_{C}^{2}$, derive the limiting distribution of the standardized sample mean $\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)$ as $n \\to \\infty$. Then, use this limiting distribution together with appropriate probability limit arguments to construct an asymptotic $(1-\\alpha)$ confidence interval for $\\mu_{C}$ in terms of $\\bar{C}_{n}$, $s_{n}$, $n$, and the $(1-\\alpha/2)$ quantile of the standard normal distribution. Clearly state any regularity condition you invoke.\n\nReport the final confidence interval endpoints as a row matrix with two entries in the following order: $[$lower endpoint, upper endpoint$]$. Do not include units. If you choose to use the standard normal quantile, denote it by $z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the unique real number such that $\\Phi(z_{1-\\alpha/2}) = 1-\\alpha/2$ and $\\Phi$ is the cumulative distribution function of the standard normal distribution. No numerical rounding is required; provide a closed-form analytic expression.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   The quantity of interest is compliance, $C = \\mathbf{f}^{\\top}\\mathbf{u}$.\n-   The applied nodal force vector $\\mathbf{f}$ and the beam geometry are deterministic.\n-   The Young's modulus is a random variable, making the displacement vector $\\mathbf{u}$ and thus the compliance $C$ random variables.\n-   $C$ is a real-valued random variable with a finite mean $\\mu_{C} = \\mathbb{E}[C]$ and a finite, strictly positive variance $\\sigma_{C}^{2} = \\mathrm{Var}(C) > 0$.\n-   A set of observations $\\{C_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) realizations of $C$.\n-   The sample mean estimator is $\\bar{C}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} C_{i}$.\n-   The sample variance estimator is $s_{n}^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (C_{i} - \\bar{C}_{n})^{2}$.\n-   The objective is to derive the limiting distribution of the standardized sample mean $\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)$ as $n \\to \\infty$.\n-   The second objective is to construct an asymptotic $(1-\\alpha)$ confidence interval for $\\mu_{C}$ using $\\bar{C}_{n}$, $s_{n}$, $n$, and the $(1-\\alpha/2)$ quantile of the standard normal distribution, denoted $z_{1-\\alpha/2}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, arising from the common practice of uncertainty quantification in mechanics using stochastic models. It is well-posed, providing the necessary and sufficient conditions to solve the problem using foundational statistical theorems. The language is objective and precise. The problem is self-contained, consistent, and does not contain any of the flaws listed in the validation criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be constructed.\n\n**Derivation**\n\nThe solution proceeds in two parts. First, we derive the limiting distribution of the standardized sample mean. Second, we use this result to construct the asymptotic confidence interval for the true mean $\\mu_{C}$.\n\n**Part 1: Limiting Distribution of the Standardized Sample Mean**\n\nWe are given a sequence of random variables $\\{C_{i}\\}_{i=1}^{n}$ that are independent and identically distributed (i.i.d.). The problem statement specifies that their common mean is $\\mathbb{E}[C_{i}] = \\mu_{C}$ and their common variance is $\\mathrm{Var}(C_{i}) = \\sigma_{C}^{2}$. Crucially, it is given that the mean $\\mu_{C}$ is finite and the variance $\\sigma_{C}^{2}$ is finite and strictly positive ($0 < \\sigma_{C}^{2} < \\infty$).\n\nThese are the precise hypotheses of the Lindeberg-Lévy Central Limit Theorem (CLT). The theorem states that for a sequence of i.i.d. random variables $\\{C_{i}\\}$ with finite mean $\\mu_{C}$ and finite non-zero variance $\\sigma_{C}^{2}$, the standardized sample mean converges in distribution to a standard normal distribution as the sample size $n$ approaches infinity. Mathematically, this is expressed as:\n$$\n\\frac{\\bar{C}_{n} - \\mu_{C}}{\\sigma_{C} / \\sqrt{n}} = \\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{\\sigma_{C}} \\xrightarrow{d} N(0, 1) \\quad \\text{as } n \\to \\infty\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and $N(0, 1)$ is the standard normal distribution with mean $0$ and variance $1$.\n\nThe problem asks for the limiting distribution of the quantity $\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)$. Using the properties of convergence in distribution, if a random variable $X_{n} \\xrightarrow{d} X$, then for any constant $c$, $c X_{n} \\xrightarrow{d} c X$. Applying this with the constant $\\sigma_C$, we have:\n$$\n\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right) = \\sigma_{C} \\left( \\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{\\sigma_{C}} \\right) \\xrightarrow{d} \\sigma_{C} \\cdot N(0, 1)\n$$\nA random variable $Y = cZ$, where $Z \\sim N(0, 1)$, is normally distributed with mean $\\mathbb{E}[Y] = c \\mathbb{E}[Z] = 0$ and variance $\\mathrm{Var}(Y) = c^2 \\mathrm{Var}(Z) = c^2$. Therefore, the limiting distribution is a normal distribution with mean $0$ and variance $\\sigma_{C}^{2}$.\n$$\n\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right) \\xrightarrow{d} N(0, \\sigma_{C}^{2})\n$$\n\n**Part 2: Construction of the Asymptotic Confidence Interval**\n\nTo construct a confidence interval for $\\mu_{C}$, we need a pivotal quantity whose distribution is known and independent of unknown parameters. From Part 1, the quantity $\\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{\\sigma_{C}}$ converges to a standard normal distribution. However, the population standard deviation $\\sigma_{C}$ is unknown. We must replace it with a consistent estimator.\n\nThe problem provides the sample variance $s_{n}^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (C_{i} - \\bar{C}_{n})^{2}$. For i.i.d. random variables with finite variance $\\sigma_{C}^{2}$, the sample variance $s_{n}^{2}$ is a consistent estimator of the population variance $\\sigma_{C}^{2}$. This is a standard result derived from the Law of Large Numbers. Formally:\n$$\ns_{n}^{2} \\xrightarrow{p} \\sigma_{C}^{2} \\quad \\text{as } n \\to \\infty\n$$\nwhere $\\xrightarrow{p}$ denotes convergence in probability.\n\nSince the square root function $g(x)=\\sqrt{x}$ is continuous for $x \\ge 0$, and $\\sigma_C^2 > 0$, the Continuous Mapping Theorem ensures that convergence in probability is preserved. Therefore, the sample standard deviation $s_{n}$ is a consistent estimator for the population standard deviation $\\sigma_{C}$:\n$$\ns_{n} = \\sqrt{s_{n}^{2}} \\xrightarrow{p} \\sqrt{\\sigma_{C}^{2}} = \\sigma_{C} \\quad \\text{as } n \\to \\infty\n$$\n\nWe now construct the studentized statistic by replacing the unknown $\\sigma_{C}$ with its consistent estimator $s_{n}$:\n$$\nT_{n} = \\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{s_{n}}\n$$\nTo find the limiting distribution of $T_{n}$, we invoke Slutsky's Theorem. The theorem states that if $X_{n} \\xrightarrow{d} X$ and $Y_{n} \\xrightarrow{p} c$ for some constant $c \\neq 0$, then $\\frac{X_{n}}{Y_{n}} \\xrightarrow{d} \\frac{X}{c}$. Let $X_{n} = \\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)$ and $Y_{n} = s_{n}$. We have established that:\n1.  $X_{n} \\xrightarrow{d} N(0, \\sigma_{C}^{2})$\n2.  $Y_{n} \\xrightarrow{p} \\sigma_{C}$ (a non-zero constant, since $\\sigma_C^2 > 0$)\n\nApplying Slutsky's Theorem:\n$$\nT_{n} = \\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{s_{n}} \\xrightarrow{d} \\frac{N(0, \\sigma_{C}^{2})}{\\sigma_{C}}\n$$\nThe resulting distribution is $\\frac{1}{\\sigma_C}$ times a $N(0, \\sigma_C^2)$ random variable, which is $N(0, (\\frac{1}{\\sigma_C})^2 \\sigma_C^2) = N(0, 1)$. Thus, the studentized statistic converges in distribution to the standard normal distribution:\n$$\n\\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{s_{n}} \\xrightarrow{d} N(0, 1)\n$$\nFor a large sample size $n$, we can approximate the distribution of $T_{n}$ by the standard normal distribution. An asymptotic $(1-\\alpha)$ confidence interval for $\\mu_{C}$ is constructed by finding values that bound the middle $(1-\\alpha)$ probability mass of the standard normal distribution. Let $z_{1-\\alpha/2}$ be the $(1-\\alpha/2)$ quantile of the standard normal distribution, such that $P(Z \\le z_{1-\\alpha/2}) = 1-\\alpha/2$ where $Z \\sim N(0, 1)$. By the symmetry of the normal distribution, the lower quantile is $-z_{1-\\alpha/2}$.\n\nWe can write the following approximate probability statement for large $n$:\n$$\nP\\left(-z_{1-\\alpha/2} \\le \\frac{\\sqrt{n}\\left(\\bar{C}_{n} - \\mu_{C}\\right)}{s_{n}} \\le z_{1-\\alpha/2}\\right) \\approx 1-\\alpha\n$$\nTo obtain the confidence interval for $\\mu_{C}$, we isolate $\\mu_{C}$ in the inequality:\n$$\n-z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}} \\le \\bar{C}_{n} - \\mu_{C} \\le z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}\n$$\nSubtracting $\\bar{C}_{n}$ from all parts:\n$$\n-\\bar{C}_{n} - z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}} \\le - \\mu_{C} \\le -\\bar{C}_{n} + z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}\n$$\nMultiplying by $-1$ and reversing the direction of the inequalities gives:\n$$\n\\bar{C}_{n} - z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}} \\le \\mu_{C} \\le \\bar{C}_{n} + z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}\n$$\nThis defines the endpoints of the asymptotic $(1-\\alpha)$ confidence interval for $\\mu_{C}$. The lower endpoint is $\\bar{C}_{n} - z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}$ and the upper endpoint is $\\bar{C}_{n} + z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\bar{C}_{n} - z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}} & \\bar{C}_{n} + z_{1-\\alpha/2} \\frac{s_{n}}{\\sqrt{n}}\n\\end{pmatrix}\n}\n$$", "id": "2707385"}, {"introduction": "While direct sampling is powerful, it can be computationally expensive, motivating the use of analytical approximation methods. This exercise introduces the First-Order Second-Moment (FOSM) technique, which uses linearization to propagate uncertainty through a model. By explicitly calculating the estimation bias for a common nonlinear mechanical response, you will gain critical insight into the accuracy of first-order methods and learn about variance-stabilizing transformations as a potential remedy [@problem_id:2707476].", "problem": "A straight prismatic bar of length $L$ is loaded axially by a deterministic force $F>0$. The bar is modeled as a linear elastic spring with global axial stiffness $K>0$ that is uncertain due to material and geometric variability; the stiffness $K$ is modeled as a lognormal random variable with mean $\\mu_K$ and coefficient of variation $\\delta_K \\in (0,1)$. The axial displacement response is the nonlinear function $u(K)=F/K$. You will analyze the First-Order Second-Moment (FOSM) linearization bias for the mean of $u$ when $K$ is lognormal, and then design a variance-stabilizing transform to remove this bias.\n\nWork from first principles and well-tested facts only: the definition of expectation, properties of the normal distribution, the definition of the lognormal distribution via $K=\\exp(Y)$ with $Y$ normal, and Taylor linearization. Do not assume any specialized uncertainty quantification formulas beyond these bases.\n\nTasks:\n- Using $K=\\exp(Y)$ with $Y \\sim \\mathcal{N}(m,s^2)$ and the definitions of the mean $\\mu_K$ and coefficient of variation $\\delta_K$ of a lognormal variable, derive a closed-form expression for the exact mean $\\mathbb{E}[u]$ in terms of $\\mu_K$ and $\\delta_K$ (do not leave $m$ or $s^2$ in the final expression).\n- Construct the FOSM mean estimate by linearizing $u(K)$ about $K=\\mu_K$ and retaining terms through first order. Denote this estimate by $u_{\\text{FOSM}}$.\n- Quantify the mean bias $b=\\mathbb{E}[u]-u_{\\text{FOSM}}$ and the relative bias $r=b/u_{\\text{FOSM}}$ as closed-form expressions in terms of $\\delta_K$ and parameters appearing in the problem.\n- Propose a variance-stabilizing transform $T(u)$ so that, to leading order in $\\delta_K$, $\\mathrm{Var}[T(u)]$ is independent of $\\mu_K$. Justify your choice from first principles by explicit calculation. Using $T$, construct a bias-corrected mean estimate by computing $\\mathbb{E}[T(u)]$ and $\\mathrm{Var}[T(u)]$ and then back-transforming to an estimate of $\\mathbb{E}[u]$; express the result in terms of $\\mu_K$ and $\\delta_K$.\n- Assume all quantities are expressed in consistent International System of Units (SI). No numerical substitution is required.\n\nProvide your final answer as the single closed-form expression for the bias $b$.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, and objective. It is self-contained and free from logical contradictions or ambiguities. It presents a standard, verifiable problem in uncertainty quantification in mechanics. Therefore, a full solution is warranted.\n\nThe problem requires the analysis of uncertainty in the axial displacement $u$ of a prismatic bar, where $u(K) = F/K$. The stiffness $K$ is a lognormal random variable. We are asked to quantify the bias of the first-order approximation of the mean displacement and to explore a variance-stabilizing transformation.\n\nFirst, we establish the properties of the lognormal random variable $K$. It is defined by $K = \\exp(Y)$, where $Y$ is a normally distributed random variable, $Y \\sim \\mathcal{N}(m, s^2)$. The mean $\\mu_K$ and variance $\\sigma_K^2$ of $K$ are related to the parameters $m$ and $s^2$ of the underlying normal distribution by:\n$$ \\mu_K = \\mathbb{E}[K] = \\exp\\left(m + \\frac{s^2}{2}\\right) $$\n$$ \\sigma_K^2 = \\mathrm{Var}(K) = \\left(\\exp(s^2) - 1\\right) \\exp\\left(2m + s^2\\right) = \\left(\\exp(s^2) - 1\\right) \\mu_K^2 $$\nThe coefficient of variation $\\delta_K$ is defined as $\\delta_K = \\sigma_K / \\mu_K$. From the variance expression, we find the relationship between $\\delta_K$ and $s^2$:\n$$ \\delta_K^2 = \\frac{\\sigma_K^2}{\\mu_K^2} = \\exp(s^2) - 1 $$\nThis allows us to express $s^2$ in terms of $\\delta_K$:\n$$ s^2 = \\ln(1 + \\delta_K^2) $$\nFrom the expression for the mean $\\mu_K$, we can express $m$ in terms of $\\mu_K$ and $s^2$:\n$$ \\ln(\\mu_K) = m + \\frac{s^2}{2} \\implies m = \\ln(\\mu_K) - \\frac{s^2}{2} $$\nSubstituting the expression for $s^2$:\n$$ m = \\ln(\\mu_K) - \\frac{1}{2}\\ln(1 + \\delta_K^2) $$\n\nNow, we proceed with the tasks specified in the problem.\n\nThe first task is to derive the exact mean of the displacement, $\\mathbb{E}[u]$. The displacement is $u(K) = F/K$. Since the force $F$ is a deterministic constant, the expectation is:\n$$ \\mathbb{E}[u] = \\mathbb{E}\\left[\\frac{F}{K}\\right] = F \\cdot \\mathbb{E}\\left[\\frac{1}{K}\\right] $$\nWe need to compute the expectation of the reciprocal of $K$. Using the definition $K = \\exp(Y)$, we have $1/K = \\exp(-Y)$. The variable $-Y$ is also normally distributed: if $Y \\sim \\mathcal{N}(m, s^2)$, then $-Y \\sim \\mathcal{N}(-m, s^2)$. The expectation of $\\exp(-Y)$ is the value of the moment-generating function of $Y$ at $-1$, or equivalently, the expectation of a lognormal variable with underlying normal parameters $-m$ and $s^2$.\n$$ \\mathbb{E}\\left[\\frac{1}{K}\\right] = \\mathbb{E}[\\exp(-Y)] = \\exp\\left(-m + \\frac{s^2}{2}\\right) $$\nSubstituting the expression for $m$:\n$$ \\mathbb{E}\\left[\\frac{1}{K}\\right] = \\exp\\left(-\\left(\\ln(\\mu_K) - \\frac{s^2}{2}\\right) + \\frac{s^2}{2}\\right) = \\exp(-\\ln(\\mu_K) + s^2) $$\nNow, substituting the expression for $s^2$:\n$$ \\mathbb{E}\\left[\\frac{1}{K}\\right] = \\exp(-\\ln(\\mu_K) + \\ln(1 + \\delta_K^2)) = \\exp\\left(\\ln\\left(\\frac{1 + \\delta_K^2}{\\mu_K}\\right)\\right) = \\frac{1 + \\delta_K^2}{\\mu_K} $$\nTherefore, the exact mean displacement is:\n$$ \\mathbb{E}[u] = F \\left(\\frac{1 + \\delta_K^2}{\\mu_K}\\right) $$\n\nThe second task is to construct the First-Order Second-Moment (FOSM) mean estimate, $u_{\\text{FOSM}}$. This is obtained by taking the expectation of the first-order Taylor series expansion of $u(K)$ around the mean of $K$, which is $\\mu_K$.\n$$ u(K) \\approx u(\\mu_K) + u'(K)|_{K=\\mu_K} (K - \\mu_K) $$\nThe expectation of this linearized function is:\n$$ u_{\\text{FOSM}} = \\mathbb{E}[u(\\mu_K) + u'(\\mu_K) (K - \\mu_K)] = u(\\mu_K) + u'(\\mu_K) \\mathbb{E}[K - \\mu_K] $$\nBy definition, $\\mathbb{E}[K - \\mu_K] = \\mathbb{E}[K] - \\mu_K = 0$. Thus, the FOSM estimate for the mean is simply the function evaluated at the mean of the input variable:\n$$ u_{\\text{FOSM}} = u(\\mu_K) = \\frac{F}{\\mu_K} $$\n\nThe third task is to quantify the mean bias $b = \\mathbb{E}[u] - u_{\\text{FOSM}}$ and the relative bias $r = b/u_{\\text{FOSM}}$.\nUsing the expressions derived above:\n$$ b = \\frac{F(1 + \\delta_K^2)}{\\mu_K} - \\frac{F}{\\mu_K} = \\frac{F}{\\mu_K} (1 + \\delta_K^2 - 1) = \\frac{F \\delta_K^2}{\\mu_K} $$\nThe bias is positive, which is expected from Jensen's inequality because $u(K) = F/K$ is a convex function for $K>0$.\nThe relative bias is:\n$$ r = \\frac{b}{u_{\\text{FOSM}}} = \\frac{F \\delta_K^2 / \\mu_K}{F / \\mu_K} = \\delta_K^2 $$\n\nThe fourth task is to propose and justify a variance-stabilizing transform $T(u)$. Such a transform $T$ is chosen so that $\\mathrm{Var}[T(u)]$ is, to first order, independent of the mean of $u$. Using the Delta method, the variance of the transformed variable is approximated as:\n$$ \\mathrm{Var}[T(u)] \\approx (T'(\\mu_u))^2 \\mathrm{Var}(u) $$\nwhere $\\mu_u = \\mathbb{E}[u]$. The first-order approximation for the variance of $u$ is:\n$$ \\mathrm{Var}(u) \\approx (u'(\\mu_K))^2 \\mathrm{Var}(K) $$\nWe have $u'(K) = -F/K^2$, so $u'(\\mu_K) = -F/\\mu_K^2$. The variance of $K$ is $\\mathrm{Var}(K) = \\sigma_K^2 = (\\delta_K \\mu_K)^2$.\n$$ \\mathrm{Var}(u) \\approx \\left(-\\frac{F}{\\mu_K^2}\\right)^2 (\\delta_K \\mu_K)^2 = \\frac{F^2}{\\mu_K^4} \\delta_K^2 \\mu_K^2 = \\frac{F^2 \\delta_K^2}{\\mu_K^2} $$\nApproximating $\\mu_u \\approx u_{\\text{FOSM}} = F/\\mu_K$, we get $\\mu_K \\approx F/\\mu_u$. Substituting this into the variance expression:\n$$ \\mathrm{Var}(u) \\approx \\frac{F^2 \\delta_K^2}{(F/\\mu_u)^2} = \\mu_u^2 \\delta_K^2 $$\nThis shows that the standard deviation of $u$ is proportional to its mean. To make $\\mathrm{Var}[T(u)]$ constant, we need $(T'(\\mu_u))^2 \\mu_u^2$ to be constant. This implies $T'(\\mu_u) \\propto 1/\\mu_u$. The simplest functional form is $T'(u) = 1/u$, which upon integration yields the logarithmic transformation:\n$$ T(u) = \\ln(u) $$\nTo justify this choice from first principles, we consider the random variable $Z = T(u) = \\ln(u) = \\ln(F/K) = \\ln(F) - \\ln(K)$. Since $\\ln(K) = Y \\sim \\mathcal{N}(m, s^2)$, and $\\ln(F)$ is a constant, $Z$ is a normal random variable:\n$$ Z = \\ln(F) - Y \\sim \\mathcal{N}(\\ln(F) - m, s^2) $$\nThe variance of $Z$ is exact:\n$$ \\mathrm{Var}[Z] = \\mathrm{Var}[\\ln(u)] = s^2 = \\ln(1 + \\delta_K^2) $$\nThis variance depends only on $\\delta_K$ and is independent of $\\mu_K$, confirming that $T(u) = \\ln(u)$ is a variance-stabilizing transform.\n\nThe final task is to use this transform to construct a bias-corrected mean estimate. The procedure is to find the exact moments of the transformed variable $Z = \\ln(u)$ and then back-transform to find the mean of $u$. We have $u = \\exp(Z)$, where $Z$ is normal.\nThe mean of $u$ is the expectation of a lognormal variable $\\exp(Z)$:\n$$ \\mathbb{E}[u] = \\mathbb{E}[\\exp(Z)] = \\exp(\\mathbb{E}[Z] + \\frac{1}{2}\\mathrm{Var}[Z]) $$\nThe moments of $Z$ are $\\mathbb{E}[Z] = \\ln(F) - m$ and $\\mathrm{Var}[Z] = s^2$.\n$$ \\mathbb{E}[u] = \\exp\\left((\\ln(F) - m) + \\frac{s^2}{2}\\right) = \\exp(\\ln(F)) \\exp\\left(-m + \\frac{s^2}{2}\\right) = F \\exp\\left(-m + \\frac{s^2}{2}\\right) $$\nAs established earlier, $\\exp(-m + s^2/2) = (1 + \\delta_K^2)/\\mu_K$.\nSo the mean estimate obtained via this procedure is:\n$$ \\mathbb{E}[u] = \\frac{F(1 + \\delta_K^2)}{\\mu_K} $$\nThis result is the exact mean of the displacement. This procedure effectively corrects the bias of the FOSM approximation by exactly accounting for the nonlinearity of the function $u(K)$ through the properties of the lognormal distribution. The bias-corrected estimate is the exact analytical answer.\n\nThe problem specifically asks for the expression for the bias $b$ as the final answer.\n$$ b = \\mathbb{E}[u] - u_{\\text{FOSM}} = \\frac{F \\delta_K^2}{\\mu_K} $$", "answer": "$$ \\boxed{\\frac{F \\delta_K^2}{\\mu_K}} $$", "id": "2707476"}, {"introduction": "To overcome the limitations of simple linearization about the mean, more advanced reliability methods are used. The First-Order Reliability Method (FORM) is a cornerstone of modern structural reliability, which improves accuracy by linearizing the problem in a transformed standard normal space. This practice provides essential hands-on experience with the core mechanism of FORM: constructing the isoprobabilistic map and computing the limit-state gradient, which are the necessary steps to finding the most probable failure point [@problem_id:2707508].", "problem": "A straight prismatic bar of length $L$ and cross-sectional area $A$ is subjected to an axial tensile force $P$. The axial displacement at the loaded end under linear elasticity is $\\delta = \\frac{P L}{A E}$, where $E$ is the Young’s modulus. Consider the serviceability limit-state function $g(E) = \\delta_{\\max} - \\frac{P L}{A E}$ with deterministic threshold $\\delta_{\\max}$. The material Young’s modulus $E$ is uncertain and modeled as a lognormal random variable with mean $m_{E}$ and coefficient of variation $c_{E}$. Assume there is no correlation with any other variable.\n\nThe First-Order Reliability Method (FORM) is to be applied in the standard normal space. The isoprobabilistic mapping must be constructed from the fundamental definition of a lognormal distribution via its underlying normal variable and the probability integral transform. In particular, start from the definition that $Y = \\ln E$ is normally distributed and use the relationships implied by $m_{E} = \\mathbb{E}[E]$ and $\\mathrm{Var}[E] = c_{E}^{2} m_{E}^{2}$ to determine the parameters of $Y$. Then form the mapping to a standard normal variable $U$.\n\nParameters:\n- $L = 2\\,\\mathrm{m}$,\n- $A = 1.0 \\times 10^{-3}\\,\\mathrm{m}^{2}$,\n- $P = 1.0 \\times 10^{5}\\,\\mathrm{N}$,\n- $\\delta_{\\max} = 2.0 \\times 10^{-3}\\,\\mathrm{m}$,\n- $m_{E} = 2.10 \\times 10^{11}\\,\\mathrm{Pa}$,\n- $c_{E} = 0.10$.\n\nTasks:\n1) Using only the definition of a lognormal variable $E = \\exp(Y)$ with $Y$ normal and the properties $m_{E} = \\mathbb{E}[E]$ and $\\mathrm{Var}[E] = \\mathbb{E}[E^{2}] - (\\mathbb{E}[E])^{2}$, derive the isoprobabilistic mapping $U \\mapsto E(U)$ that transforms the standard normal variable $U \\sim \\mathcal{N}(0,1)$ to $E$. Clearly identify the intermediate normal parameters of $Y$ in terms of $m_{E}$ and $c_{E}$, and write the final mapping $E(U)$.\n\n2) Define $G(U) = g(E(U))$ and compute the scalar gradient mapping $\\dfrac{\\mathrm{d}G}{\\mathrm{d}U}$ evaluated at $U = 0$. Express your final numerical answer in millimeters and round to four significant figures. State the value as a single number as instructed. The angle unit is not applicable here.", "solution": "The problem statement has been evaluated and is found to be scientifically grounded, well-posed, and objective. It contains a complete and consistent set of givens, allowing for a unique and meaningful solution. The problem is a standard exercise in structural reliability and uncertainty quantification, based on fundamental principles of solid mechanics and probability theory. Therefore, I will proceed with the solution.\n\nThe task is divided into two parts. First, we must derive the isoprobabilistic transformation for the Young's modulus, $E$. Second, we must compute the gradient of the limit-state function in the standard normal space at the origin.\n\nPart 1: Derivation of the Isoprobabilistic Mapping $U \\mapsto E(U)$\n\nThe Young's modulus, $E$, is a lognormal random variable. This means its natural logarithm, $Y = \\ln E$, follows a normal distribution, denoted as $Y \\sim \\mathcal{N}(\\mu_{Y}, \\sigma_{Y}^{2})$, where $\\mu_{Y}$ and $\\sigma_{Y}$ are the mean and standard deviation of $Y$. The problem requires us to determine $\\mu_{Y}$ and $\\sigma_{Y}$ from the given mean $m_{E} = \\mathbb{E}[E]$ and coefficient of variation $c_{E}$ of $E$.\n\nThe fundamental relationships between the parameters of a lognormal variable $E$ and its underlying normal variable $Y$ are:\n$$\nm_{E} = \\mathbb{E}[E] = \\exp\\left(\\mu_{Y} + \\frac{1}{2}\\sigma_{Y}^{2}\\right)\n$$\n$$\n\\mathrm{Var}[E] = \\mathbb{E}[E^2] - (\\mathbb{E}[E])^2 = \\left(\\exp(\\sigma_{Y}^{2}) - 1\\right) \\exp\\left(2\\mu_{Y} + \\sigma_{Y}^{2}\\right) = \\left(\\exp(\\sigma_{Y}^{2}) - 1\\right) m_{E}^{2}\n$$\nThe coefficient of variation, $c_{E}$, is defined as the ratio of the standard deviation of $E$ to its mean:\n$$\nc_{E} = \\frac{\\sqrt{\\mathrm{Var}[E]}}{m_{E}}\n$$\nSquaring this expression and substituting the variance gives:\n$$\nc_{E}^{2} = \\frac{\\mathrm{Var}[E]}{m_{E}^{2}} = \\frac{\\left(\\exp(\\sigma_{Y}^{2}) - 1\\right) m_{E}^{2}}{m_{E}^{2}} = \\exp(\\sigma_{Y}^{2}) - 1\n$$\nFrom this, we solve for the parameter $\\sigma_{Y}^{2}$:\n$$\n\\sigma_{Y}^{2} = \\ln(c_{E}^{2} + 1)\n$$\nAnd consequently, the standard deviation $\\sigma_{Y}$ is:\n$$\n\\sigma_{Y} = \\sqrt{\\ln(c_{E}^{2} + 1)}\n$$\nNow, we use the expression for the mean $m_{E}$ to find $\\mu_{Y}$:\n$$\n\\ln(m_{E}) = \\mu_{Y} + \\frac{1}{2}\\sigma_{Y}^{2}\n$$\nSolving for $\\mu_{Y}$:\n$$\n\\mu_{Y} = \\ln(m_{E}) - \\frac{1}{2}\\sigma_{Y}^{2}\n$$\nSubstituting the expression for $\\sigma_{Y}^{2}$:\n$$\n\\mu_{Y} = \\ln(m_{E}) - \\frac{1}{2}\\ln(c_{E}^{2} + 1) = \\ln\\left(\\frac{m_{E}}{\\sqrt{c_{E}^{2} + 1}}\\right)\n$$\nThe isoprobabilistic mapping transforms a standard normal variable $U \\sim \\mathcal{N}(0,1)$ to the physical variable $E$. This is achieved by first transforming $U$ to the intermediate normal variable $Y$ and then to $E$. The transformation from $U$ to $Y$ is:\n$$\nY(U) = \\mu_{Y} + \\sigma_{Y} U\n$$\nThe transformation from $Y$ to $E$ is given by the definition of the lognormal distribution:\n$$\nE(U) = \\exp(Y(U)) = \\exp(\\mu_{Y} + \\sigma_{Y} U)\n$$\nThis is the final form of the isoprobabilistic mapping $U \\mapsto E(U)$, with the parameters $\\mu_{Y}$ and $\\sigma_{Y}$ fully defined in terms of the initial givens $m_{E}$ and $c_{E}$.\n\nPart 2: Computation of the Gradient $\\dfrac{\\mathrm{d}G}{\\mathrm{d}U}$ at $U=0$\n\nThe limit-state function in the physical space is $g(E) = \\delta_{\\max} - \\dfrac{P L}{A E}$. We transform this into the standard normal space by substituting the mapping $E(U)$:\n$$\nG(U) = g(E(U)) = \\delta_{\\max} - \\frac{P L}{A E(U)} = \\delta_{\\max} - \\frac{P L}{A \\exp(\\mu_{Y} + \\sigma_{Y} U)}\n$$\nTo facilitate differentiation, we rewrite the expression as:\n$$\nG(U) = \\delta_{\\max} - \\frac{PL}{A} \\exp(-\\mu_{Y} - \\sigma_{Y} U)\n$$\nNow, we compute the derivative of $G(U)$ with respect to $U$ using the chain rule:\n$$\n\\frac{\\mathrm{d}G}{\\mathrm{d}U} = \\frac{\\mathrm{d}}{\\mathrm{d}U} \\left( \\delta_{\\max} - \\frac{PL}{A} \\exp(-\\mu_{Y} - \\sigma_{Y} U) \\right)\n$$\n$$\n\\frac{\\mathrm{d}G}{\\mathrm{d}U} = 0 - \\frac{PL}{A} \\left[ \\exp(-\\mu_{Y} - \\sigma_{Y} U) \\cdot (-\\sigma_{Y}) \\right] = \\frac{PL\\sigma_{Y}}{A} \\exp(-\\mu_{Y} - \\sigma_{Y} U)\n$$\nWe must evaluate this gradient at $U=0$:\n$$\n\\left. \\frac{\\mathrm{d}G}{\\mathrm{d}U} \\right|_{U=0} = \\frac{PL\\sigma_{Y}}{A} \\exp(-\\mu_{Y})\n$$\nWe can substitute the expression for $\\exp(-\\mu_{Y})$ derived from $\\mu_{Y} = \\ln\\left(\\frac{m_{E}}{\\sqrt{c_{E}^{2} + 1}}\\right)$:\n$$\n\\exp(-\\mu_{Y}) = \\exp\\left(-\\ln\\left(\\frac{m_{E}}{\\sqrt{c_{E}^{2} + 1}}\\right)\\right) = \\frac{\\sqrt{c_{E}^{2} + 1}}{m_{E}}\n$$\nSubstituting this and the expression for $\\sigma_{Y}$ into the gradient equation:\n$$\n\\left. \\frac{\\mathrm{d}G}{\\mathrm{d}U} \\right|_{U=0} = \\frac{PL}{A} \\left( \\sqrt{\\ln(c_{E}^{2} + 1)} \\right) \\left( \\frac{\\sqrt{c_{E}^{2} + 1}}{m_{E}} \\right) = \\frac{PL}{A m_{E}} \\sqrt{(c_{E}^{2} + 1)\\ln(c_{E}^{2} + 1)}\n$$\nNow, we substitute the numerical values provided:\n$P = 1.0 \\times 10^{5}\\,\\mathrm{N}$, $L = 2\\,\\mathrm{m}$, $A = 1.0 \\times 10^{-3}\\,\\mathrm{m}^{2}$, $m_{E} = 2.10 \\times 10^{11}\\,\\mathrm{Pa}$, $c_{E} = 0.10$.\n\nFirst, compute the parameters for the underlying normal distribution:\n$$\nc_{E}^{2} = (0.10)^{2} = 0.01\n$$\n$$\n\\sigma_{Y} = \\sqrt{\\ln((0.10)^{2} + 1)} = \\sqrt{\\ln(1.01)} \\approx 0.0997513\\,\\text{(dimensionless)}\n$$\nNext, calculate the terms in the gradient expression:\n$$\n\\frac{PL}{A m_{E}} = \\frac{(1.0 \\times 10^{5}\\,\\mathrm{N}) (2\\,\\mathrm{m})}{(1.0 \\times 10^{-3}\\,\\mathrm{m}^2) (2.10 \\times 10^{11}\\,\\mathrm{N/m}^2)} = \\frac{2.0 \\times 10^{5}}{2.10 \\times 10^{8}}\\,\\mathrm{m} \\approx 9.52381 \\times 10^{-4}\\,\\mathrm{m}\n$$\nThe evaluation of $\\exp(-\\mu_Y)$ gives:\n$$\n\\exp(-\\mu_{Y}) = \\frac{\\sqrt{(0.10)^2 + 1}}{2.10 \\times 10^{11}\\,\\mathrm{Pa}} = \\frac{\\sqrt{1.01}}{2.10 \\times 10^{11}\\,\\mathrm{Pa}} \\approx \\frac{1.00498756}{2.10 \\times 10^{11}}\\,\\mathrm{Pa}^{-1} \\approx 4.785655 \\times 10^{-12}\\,\\mathrm{Pa}^{-1}\n$$\nCombining these results to find the gradient:\n$$\n\\left. \\frac{\\mathrm{d}G}{\\mathrm{d}U} \\right|_{U=0} = \\frac{PL\\sigma_{Y}}{A} \\exp(-\\mu_{Y})\n$$\nThe term $\\frac{PL}{A}$ is:\n$$\n\\frac{PL}{A} = \\frac{(1.0 \\times 10^{5}\\,\\mathrm{N}) (2\\,\\mathrm{m})}{1.0 \\times 10^{-3}\\,\\mathrm{m}^2} = 2.0 \\times 10^8\\,\\mathrm{N/m}\n$$\nSo the gradient is:\n$$\n(2.0 \\times 10^8\\,\\mathrm{N/m}) \\cdot (0.0997513) \\cdot (4.785655 \\times 10^{-12}\\,\\mathrm{Pa}^{-1})\n$$\nSince $1\\,\\mathrm{Pa} = 1\\,\\mathrm{N/m}^2$, the units are $\\mathrm{(N/m) \\cdot (m^2/N) = m}$.\n$$\n\\left. \\frac{\\mathrm{d}G}{\\mathrm{d}U} \\right|_{U=0} \\approx (2.0 \\times 10^8) \\cdot (0.0997513) \\cdot (4.785655 \\times 10^{-12}) \\,\\mathrm{m} \\approx 9.54756 \\times 10^{-5}\\,\\mathrm{m}\n$$\nThe problem requires the answer in millimeters. $1\\,\\mathrm{mm} = 10^{-3}\\,\\mathrm{m}$.\n$$\n9.54756 \\times 10^{-5}\\,\\mathrm{m} = 0.0954756\\,\\mathrm{mm}\n$$\nRounding to four significant figures gives $0.09548\\,\\mathrm{mm}$.", "answer": "$$\n\\boxed{0.09548}\n$$", "id": "2707508"}]}