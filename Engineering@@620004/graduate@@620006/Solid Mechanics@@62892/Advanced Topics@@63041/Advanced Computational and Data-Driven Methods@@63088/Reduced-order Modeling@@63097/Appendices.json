{"hands_on_practices": [{"introduction": "The foundation of any reduced-order model is the choice of basis vectors that can efficiently represent the system's behavior. This practice guides you through the fundamental process of Proper Orthogonal Decomposition (POD), a powerful technique for extracting an optimal basis from simulation data. By implementing the algorithm in the context of elastodynamics with a weighted energy norm, you will learn how to make a principled, quantitative decision on the size of your reduced model by controlling the amount of energy discarded by the truncation [@problem_id:2679837].", "problem": "Consider reduced-order modeling for transient elastodynamics where the energetic content of a set of displacement or velocity snapshots is assessed under a symmetric positive definite (SPD) weighting that represents a discretized energy inner product. Let $X \\in \\mathbb{R}^{n \\times m}$ be a snapshot matrix whose columns are state snapshots, and let $W \\in \\mathbb{R}^{n \\times n}$ be an SPD matrix defining the weighted inner product $\\langle u,v \\rangle_W = u^\\top W v$. The goal is to determine how much of the total energy content of $X$ is captured by the first $r$ modes of an optimal rank-$r$ subspace under the $W$-inner product, and to select the smallest $r$ such that the discarded energy fraction is at most a specified tolerance $\\varepsilon$. All quantities are dimensionless fractions; no physical units are required. Angles do not appear; no angle units are required.\n\nStarting only from fundamental linear algebra principles for inner products, orthogonal projections, and energy in a weighted norm, derive the measure of cumulative energy content captured by the best $r$-dimensional subspace associated with $X$ under the $W$-inner product. Use that derivation to design an algorithm that, for each test case $(X,W,\\varepsilon)$, computes:\n- the smallest integer $r \\ge 0$ such that the discarded energy fraction is less than or equal to $\\varepsilon$, and\n- the achieved discarded energy fraction at that $r$.\n\nIf the total energy of $X$ under the $W$-inner product is zero, define $r = 0$ and the discarded energy fraction to be $0$. Express the discarded energy fraction as a decimal rounded to $10$ decimal places.\n\nYour program must solve the following test suite. Each case provides $X$, $W$, and $\\varepsilon$ explicitly.\n\nTest case $\\#1$:\n- $n = 5$, $m = 4$.\n- $W_1 = I_5$.\n- $X_1 = \\begin{bmatrix}\n1.5 & 0.0 & 0.0 & 0.0 \\\\\n0.0 & 1.2 & 0.0 & 0.0 \\\\\n0.1 & 0.0 & 0.9 & 0.0 \\\\\n0.0 & 0.1 & 0.0 & 0.6 \\\\\n0.05 & 0.02 & 0.01 & 0.01\n\\end{bmatrix}$.\n- $\\varepsilon_1 = 0.1$.\n\nTest case $\\#2$:\n- $n = 5$, $m = 3$.\n- $W_2 = \\mathrm{diag}(2.0, 1.0, 0.5, 1.5, 1.0)$.\n- $X_2 = \\begin{bmatrix}\n1.0 & 0.5 & 0.2 \\\\\n0.0 & 0.4 & 0.0 \\\\\n0.0 & 0.0 & 0.3 \\\\\n0.2 & 0.0 & 0.1 \\\\\n0.0 & 0.0 & 0.05\n\\end{bmatrix}$.\n- $\\varepsilon_2 = 0.01$.\n\nTest case $\\#3$ (rank-deficient snapshots):\n- $n = 5$, $m = 3$.\n- $W_3 = I_5$.\n- $X_3 = \\begin{bmatrix}\n1.0 & 1.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 \\\\\n0.5 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 0.0 \\\\\n0.2 & 0.2 & 0.0\n\\end{bmatrix}$.\n- $\\varepsilon_3 = 0$.\n\nTest case $\\#4$ (full SPD weighting):\n- $n = 5$, $m = 2$.\n- $W_4 = \\begin{bmatrix}\n2.0 & 0.3 & 0.0 & 0.0 & 0.0 \\\\\n0.3 & 1.5 & 0.2 & 0.0 & 0.0 \\\\\n0.0 & 0.2 & 1.2 & 0.1 & 0.0 \\\\\n0.0 & 0.0 & 0.1 & 1.1 & 0.05 \\\\\n0.0 & 0.0 & 0.0 & 0.05 & 1.0\n\\end{bmatrix}$.\n- $X_4 = \\begin{bmatrix}\n0.5 & 0.1 \\\\\n0.1 & 0.4 \\\\\n0.0 & 0.3 \\\\\n0.2 & 0.0 \\\\\n0.0 & 0.1\n\\end{bmatrix}$.\n- $\\varepsilon_4 = 0.5$.\n\nFor each test case, your program must:\n- compute the minimal $r$ such that the discarded energy fraction is less than or equal to $\\varepsilon$,\n- compute the discarded energy fraction at that $r$, rounded to $10$ decimal places,\n- output the result for that case as a list of the form $[r, \\text{discarded}]$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a bracketed list. For example, the output must have the form $[[r_1,\\text{discarded}_1],[r_2,\\text{discarded}_2],[r_3,\\text{discarded}_3],[r_4,\\text{discarded}_4]]$ with no spaces anywhere in the line.", "solution": "The problem statement is analyzed for validity.\n\nStep 1: Extract Givens\n- A snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ whose columns are state snapshots.\n- A symmetric positive definite (SPD) weighting matrix $W \\in \\mathbb{R}^{n \\times n}$.\n- A weighted inner product defined as $\\langle u,v \\rangle_W = u^\\top W v$.\n- A tolerance for discarded energy fraction, $\\varepsilon$.\n- The goal is to find the smallest integer $r \\ge 0$ such that the discarded energy fraction by an optimal rank-$r$ subspace is $\\le \\varepsilon$.\n- The output for each test case is a pair: $[r, \\text{achieved discarded fraction}]$.\n- A special condition: If the total energy of $X$ is zero, then $r=0$ and the discarded fraction is $0$.\n- Four test cases are provided with explicit matrices $X_i, W_i$ and tolerances $\\varepsilon_i$.\n\nTest Case 1: $X_1 \\in \\mathbb{R}^{5 \\times 4}$, $W_1 = I_5$, $\\varepsilon_1 = 0.1$.\nTest Case 2: $X_2 \\in \\mathbb{R}^{5 \\times 3}$, $W_2 = \\mathrm{diag}(2.0, 1.0, 0.5, 1.5, 1.0)$, $\\varepsilon_2 = 0.01$.\nTest Case 3: $X_3 \\in \\mathbb{R}^{5 \\times 3}$, $W_3 = I_5$, $\\varepsilon_3 = 0$.\nTest Case 4: $X_4 \\in \\mathbb{R}^{5 \\times 2}$, $W_4$ is a full SPD matrix, $\\varepsilon_4 = 0.5$.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in linear algebra and its application to reduced-order modeling (specifically, Proper Orthogonal Decomposition or POD) in solid mechanics. The use of a weighted inner product corresponding to system energy (e.g., kinetic or potential) is standard practice.\n- **Well-Posed**: The problem is well-posed. The existence of an optimal subspace is guaranteed, and the method to find it (related to singular value or eigenvalue decomposition) is established. The provided data is sufficient to find a unique solution for $r$ and the corresponding fraction.\n- **Objective**: The problem is stated objectively with precise mathematical definitions and no subjective language.\n- The problem does not violate any of the invalidity criteria. It is formalizable, complete, realistic, and requires non-trivial reasoning based on fundamental principles.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n**Derivation of the Solution**\n\nWe are tasked with finding an optimal rank-$r$ subspace that best approximates a set of snapshots $\\{x_i\\}_{i=1}^m$, which are the columns of the matrix $X \\in \\mathbb{R}^{n \\times m}$. The quality of the approximation is measured by the energy content captured, where energy is defined through the weighted inner product $\\langle u, v \\rangle_W = u^\\top W v$ and its induced norm $\\|u\\|_W = \\sqrt{u^\\top W u}$. The matrix $W \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite (SPD).\n\n$1$. Total Energy Content:\nThe energy of a single snapshot vector $x_i$ is its squared norm, $\\|x_i\\|_W^2 = x_i^\\top W x_i$. The total energy content of the entire snapshot set is the sum of the energies of individual snapshots:\n$$ E_{total} = \\sum_{i=1}^{m} \\|x_i\\|_W^2 = \\sum_{i=1}^{m} x_i^\\top W x_i $$\nThis sum can be expressed using the trace operator. The $i$-th diagonal element of the matrix product $X^\\top W X$ is precisely $x_i^\\top W x_i$. Therefore, the total energy is the trace of this product:\n$$ E_{total} = \\mathrm{Tr}(X^\\top W X) $$\n\n$2$. Transformation to a Standard Euclidean Space:\nThe problem of finding an optimal subspace under a general weighted inner product can be simplified by transforming it into an equivalent problem under the standard Euclidean inner product, where $\\langle u,v \\rangle = u^\\top v$.\nSince $W$ is SPD, it admits a unique Cholesky factorization $W = L L^\\top$, where $L$ is a real, lower-triangular matrix with positive diagonal entries. The weighted inner product can be rewritten as:\n$$ \\langle u, v \\rangle_W = u^\\top W v = u^\\top (L L^\\top) v = (L^\\top u)^\\top (L^\\top v) $$\nThis expression shows that the $W$-inner product of vectors $u$ and $v$ is equivalent to the standard Euclidean inner product of transformed vectors $\\tilde{u} = L^\\top u$ and $\\tilde{v} = L^\\top v$.\n\nConsequently, finding the optimal representation for the original snapshots $X = [x_1, \\dots, x_m]$ in the $W$-norm is equivalent to finding the optimal representation for the transformed snapshots $\\tilde{X} = L^\\top X = [L^\\top x_1, \\dots, L^\\top x_m]$ in the standard Euclidean norm. The total energy remains invariant under this transformation:\n$$ E_{total} = \\mathrm{Tr}(X^\\top W X) = \\mathrm{Tr}(X^\\top L L^\\top X) = \\mathrm{Tr}((L^\\top X)^\\top(L^\\top X)) = \\mathrm{Tr}(\\tilde{X}^\\top \\tilde{X}) = \\|\\tilde{X}\\|_F^2 $$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n\n$3$. Optimal Subspace and Singular Value Decomposition (SVD):\nAccording to the Eckart-Young-Mirsky theorem, the best rank-$r$ approximation of a matrix $\\tilde{X}$ in the Frobenius norm is given by its truncated Singular Value Decomposition (SVD). Let the SVD of $\\tilde{X}$ be $\\tilde{X} = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$.\nThe optimal rank-$r$ subspace is spanned by the first $r$ columns of $U$. The energy captured by this subspace is the squared Frobenius norm of the projection of $\\tilde{X}$ onto this subspace. This is equal to the sum of the squares of the first $r$ singular values:\n$$ E_r = \\sum_{i=1}^{r} \\sigma_i^2 $$\n\n$4$. Relation to the Eigenvalue Problem:\nComputing the full SVD of $\\tilde{X}$ can be computationally intensive, especially if $n$ is large. A more direct method is available. The squares of the singular values of $\\tilde{X}$ are the eigenvalues of the matrix $\\tilde{X}^\\top \\tilde{X}$. This smaller matrix, known as the correlation matrix, has dimensions $m \\times m$:\n$$ C = \\tilde{X}^\\top \\tilde{X} = (L^\\top X)^\\top(L^\\top X) = X^\\top L L^\\top X = X^\\top W X $$\nLet $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m \\ge 0$ be the eigenvalues of the symmetric positive semi-definite matrix $C = X^\\top W X$. Then, $\\lambda_i = \\sigma_i^2$ for $i=1, \\dots, m$.\n\n$5$. Discarded Energy Fraction:\nThe total energy is the sum of all eigenvalues of $C$: $E_{total} = \\sum_{i=1}^{m} \\lambda_i$.\nThe energy captured by the first $r$ modes is $E_r = \\sum_{i=1}^{r} \\lambda_i$.\nThe energy discarded is the remainder: $E_{discarded}(r) = E_{total} - E_r = \\sum_{i=r+1}^{m} \\lambda_i$.\nThe discarded energy fraction for a rank-$r$ approximation is therefore:\n$$ f_d(r) = \\frac{E_{discarded}(r)}{E_{total}} = \\frac{\\sum_{i=r+1}^{m} \\lambda_i}{\\sum_{i=1}^{m} \\lambda_i} $$\n\n$6$. Algorithmic Design:\nBased on the derivation, the algorithm to find the smallest integer $r \\ge 0$ such that $f_d(r) \\le \\varepsilon$ is as follows:\n1.  Given matrices $X$, $W$ and tolerance $\\varepsilon$.\n2.  Construct the correlation matrix $C = X^\\top W X$.\n3.  Compute the eigenvalues of $C$. Since $C$ is symmetric, all its eigenvalues are real. Since it is also positive semi-definite, all eigenvalues are non-negative.\n4.  Sort the eigenvalues in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m \\ge 0$.\n5.  Calculate the total energy $E_{total} = \\sum_{i=1}^{m} \\lambda_i$.\n6.  If $E_{total}$ is numerically zero, the problem statement specifies to return $r=0$ and a discarded fraction of $0.0$.\n7.  Iterate through possible ranks $r$ from $0$ to $m$. For each $r$:\n    a. Calculate the captured energy $E_r = \\sum_{i=1}^{r} \\lambda_i$. (For $r=0$, $E_0=0$).\n    b. Calculate the discarded energy fraction $f_d(r) = (E_{total} - E_r) / E_{total}$.\n    c. If $f_d(r) \\le \\varepsilon$, then this value of $r$ is the minimal rank required. The algorithm terminates and returns this $r$ and the computed $f_d(r)$.\nThis iteration guarantees finding the smallest $r$, as the discarded energy fraction is a monotonically non-increasing function of $r$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test case #1\n    X1 = np.array([\n        [1.5, 0.0, 0.0, 0.0],\n        [0.0, 1.2, 0.0, 0.0],\n        [0.1, 0.0, 0.9, 0.0],\n        [0.0, 0.1, 0.0, 0.6],\n        [0.05, 0.02, 0.01, 0.01]\n    ])\n    W1 = np.identity(5)\n    epsilon1 = 0.1\n\n    # Test case #2\n    X2 = np.array([\n        [1.0, 0.5, 0.2],\n        [0.0, 0.4, 0.0],\n        [0.0, 0.0, 0.3],\n        [0.2, 0.0, 0.1],\n        [0.0, 0.0, 0.05]\n    ])\n    W2 = np.diag([2.0, 1.0, 0.5, 1.5, 1.0])\n    epsilon2 = 0.01\n\n    # Test case #3\n    X3 = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.5, 0.5, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.2, 0.2, 0.0]\n    ])\n    W3 = np.identity(5)\n    epsilon3 = 0.0\n\n    # Test case #4\n    X4 = np.array([\n        [0.5, 0.1],\n        [0.1, 0.4],\n        [0.0, 0.3],\n        [0.2, 0.0],\n        [0.0, 0.1]\n    ])\n    W4 = np.array([\n        [2.0, 0.3, 0.0, 0.0, 0.0],\n        [0.3, 1.5, 0.2, 0.0, 0.0],\n        [0.0, 0.2, 1.2, 0.1, 0.0],\n        [0.0, 0.0, 0.1, 1.1, 0.05],\n        [0.0, 0.0, 0.0, 0.05, 1.0]\n    ])\n    epsilon4 = 0.5\n\n    test_cases = [\n        (X1, W1, epsilon1),\n        (X2, W2, epsilon2),\n        (X3, W3, epsilon3),\n        (X4, W4, epsilon4)\n    ]\n\n    results = []\n    for X, W, epsilon in test_cases:\n        result = compute_optimal_rank(X, W, epsilon)\n        results.append(result)\n\n    # Format the final output string without spaces.\n    inner_results_str = [f\"[{r},{d}]\" for r, d in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    \n    print(final_output)\n\ndef compute_optimal_rank(X, W, epsilon):\n    \"\"\"\n    Computes the smallest rank r and the corresponding discarded energy fraction.\n\n    Args:\n        X (np.ndarray): Snapshot matrix of size n x m.\n        W (np.ndarray): SPD weighting matrix of size n x n.\n        epsilon (float): Tolerance for the discarded energy fraction.\n\n    Returns:\n        list: A list containing [r, discarded_fraction].\n    \"\"\"\n    # Form the m x m correlation matrix C = X^T * W * X\n    C = X.T @ W @ X\n\n    # The eigenvalues of C are the squared singular values of L^T * X.\n    # We use eigh for symmetric matrices, which is numerically stable and efficient.\n    # It returns eigenvalues in ascending order.\n    eigenvalues = linalg.eigh(C, eigvals_only=True)\n    \n    # Sort eigenvalues in descending order\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    \n    # Total energy is the sum of all eigenvalues.\n    total_energy = np.sum(eigenvalues)\n\n    # Handle the case where total energy is zero.\n    if np.isclose(total_energy, 0.0):\n        return [0, 0.0]\n\n    # Number of snapshots (and maximum possible rank)\n    m = X.shape[1]\n\n    # Iterate from r=0 to m to find the smallest r satisfying the condition.\n    cumulative_energy = 0.0\n    for r in range(m + 1):\n        if r == 0:\n            retained_energy = 0.0\n        else:\n            # For a given r, we retain the first r largest eigenvalues.\n            retained_energy = np.sum(eigenvalues[:r])\n\n        discarded_fraction = (total_energy - retained_energy) / total_energy\n        \n        # Check against the tolerance.\n        if discarded_fraction <= epsilon:\n            # Found the minimal r. Round the fraction and return.\n            return [r, round(discarded_fraction, 10)]\n    \n    # This part should not be reached for epsilon >= 0, as for r=m,\n    # the discarded fraction is 0. It's included for robustness.\n    return [m, 0.0]\n\nsolve()\n```", "id": "2679837"}, {"introduction": "Once a basis is constructed, Galerkin projection is used to create the reduced-order model (ROM), but this process is not without its pitfalls. This exercise presents a crucial cautionary tale, demonstrating how a ROM for a perfectly stable full-order model can become unstable and produce unphysical, diverging results. By constructing such a case for a non-normal system, you will gain a deep appreciation for the subtleties of ROM stability and the importance of verifying the dynamics of your reduced model [@problem_id:2432128].", "problem": "You are asked to implement a complete numerical experiment in reduced-order modeling that demonstrates the following phenomenon: a Proper Orthogonal Decomposition (POD) basis can be excellent for reconstructing training snapshots of a stable full-order linear time-invariant system, yet the Galerkin-projected reduced-order model (ROM) can produce unstable dynamics that blow up when integrated in time.\n\nYour implementation must start from the full-order ordinary differential equation\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b},\n$$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{b}\\in\\mathbb{R}^{n}$ are constant, and $\\mathbf{x}(t)\\in\\mathbb{R}^{n}$ is the state. All computations are over the real numbers with the standard Euclidean inner product. You will use $n=2$ throughout.\n\nFundamental definitions and requirements:\n- Proper Orthogonal Decomposition (POD) basis: Given a snapshot matrix\n$$\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}(t_1) & \\mathbf{x}(t_2) & \\cdots & \\mathbf{x}(t_m)\\end{bmatrix}\\in\\mathbb{R}^{n\\times m},\n$$\ncompute its singular value decomposition (SVD) $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The rank-$r$ POD basis $\\mathbf{Q}\\in\\mathbb{R}^{n\\times r}$ is taken as the first $r$ columns of $\\mathbf{U}$.\n- Galerkin projection: The reduced operator and reduced forcing are\n$$\n\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}\\in\\mathbb{R}^{r\\times r},\\qquad \\mathbf{b}_r=\\mathbf{Q}^\\top\\mathbf{b}\\in\\mathbb{R}^{r}.\n$$\nThe reduced state $\\mathbf{z}(t)\\in\\mathbb{R}^{r}$ evolves as\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r \\mathbf{z} + \\mathbf{b}_r,\\qquad \\mathbf{x}_r(t)=\\mathbf{Q}\\mathbf{z}(t).\n$$\n- Time integration: Use the classical fourth-order Runge–Kutta method with a fixed time step $h>0$ for both the full-order model and the ROM. Set the initial condition to $\\mathbf{x}(0)=\\mathbf{0}$ and $\\mathbf{z}(0)=\\mathbf{Q}^\\top\\mathbf{x}(0)=\\mathbf{0}$.\n- Snapshot collection: Integrate the full-order model over a training horizon $[0,T_{\\text{train}}]$ with a constant time step $h$, sampling the state at every step to form $\\mathbf{X}$.\n- Reconstruction error: Measure the relative POD reconstruction error of the training snapshots as\n$$\n\\varepsilon_{\\text{rec}} = \\frac{\\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F}{\\lVert \\mathbf{X}\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- Blow-up detection: Evolve both the full-order model and the ROM over a test horizon $[0,T_{\\text{test}}]$ with the same $h$. Declare a solution “blown up” if at any time step the Euclidean norm of the current state exceeds a threshold $M$, or if any component becomes not-a-number or infinite. Use the threshold $M=10^6$.\n\nConstructed forcing to target instability under ROM:\n- For each test, you must construct the constant forcing $\\mathbf{b}$ as follows. Compute the symmetric part $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$ and its dominant unit eigenvector $\\mathbf{q}\\in\\mathbb{R}^{n}$ associated with the largest eigenvalue of $\\mathbf{S}$ (break ties arbitrarily but deterministically). Set\n$$\n\\mathbf{b}=-\\mathbf{A}\\mathbf{q}.\n$$\nThis construction ensures that the full-order steady state is $\\mathbf{x}_\\infty = -\\mathbf{A}^{-1}\\mathbf{b}=\\mathbf{q}$. When $\\mathbf{A}$ is highly non-normal and the largest eigenvalue of $\\mathbf{S}$ is positive, the scalar ROM obtained with $r=1$ and $\\mathbf{Q}=\\mathbf{q}$ has reduced dynamics $\\frac{dz}{dt} = a_r z + b_r$ with $a_r=\\mathbf{q}^\\top\\mathbf{A}\\mathbf{q}>0$ and $b_r=-a_r$, which is unstable and diverges from $z(0)=0$.\n\nNumerical specification common to all tests:\n- Use $n=2$.\n- Use $h=10^{-3}$.\n- Use classical fourth-order Runge–Kutta.\n- Use the Euclidean norm for all vector norms.\n- Use $\\mathbf{x}(0)=\\mathbf{0}$.\n\nTest suite:\nImplement the above for the following parameter sets. In each case, define $\\mathbf{A}$, compute $\\mathbf{q}$ and $\\mathbf{b}$ as specified, collect snapshots over $[0,T_{\\text{train}}]$ to form $\\mathbf{Q}$, then form the ROM and run both models over $[0,T_{\\text{test}}]$.\n\n- Test $1$ (highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $2$ (highly non-normal, rank-$2$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=2$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $3$ (symmetric negative definite, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-1.0 & 0.0 \\\\ 0.0 & -2.0\\end{bmatrix}$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $4$ (more highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=120.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n\nRequired outputs:\n- For each test, output a list of three entries:\n  - the scalar $\\varepsilon_{\\text{rec}}$ rounded to six decimal places,\n  - a boolean indicating whether the ROM blew up on $[0,T_{\\text{test}}]$,\n  - a boolean indicating whether the full-order model blew up on $[0,T_{\\text{test}}]$.\n- Aggregate the results from all tests into a single line as a comma-separated list enclosed in square brackets, in the same order as the tests. Example format:\n$[\\,[\\varepsilon_{\\text{rec}}^{(1)},\\,\\text{ROM}^{(1)}\\_\\text{blowup},\\,\\text{FOM}^{(1)}\\_\\text{blowup}],\\,[\\varepsilon_{\\text{rec}}^{(2)},\\,\\text{ROM}^{(2)}\\_\\text{blowup},\\,\\text{FOM}^{(2)}\\_\\text{blowup}],\\,\\ldots\\,]$.", "solution": "The user has presented a problem in computational engineering, specifically concerning the stability of reduced-order models (ROMs) derived from Proper Orthogonal Decomposition (POD) and Galerkin projection. The task is to demonstrate via numerical experiment a known failure mode where a ROM can be unstable despite the full-order model (FOM) being stable. This phenomenon is characteristic of systems governed by highly non-normal operators.\n\nThe problem statement has been validated and found to be scientifically sound, well-posed, and complete. All definitions, parameters, and procedures are specified with sufficient clarity to permit a unique and verifiable solution. We will proceed with the analysis and implementation.\n\nThe core of the problem lies in the distinction between the spectrum of a matrix $\\mathbf{A}$ and its numerical range (or field of values), defined as $W(\\mathbf{A}) = \\{\\mathbf{v}^\\dagger\\mathbf{A}\\mathbf{v} : \\mathbf{v} \\in \\mathbb{C}^n, \\lVert\\mathbf{v}\\rVert_2 = 1\\}$. For a linear time-invariant system $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x}$, stability is determined by the eigenvalues of $\\mathbf{A}$ (the spectrum, $\\sigma(\\mathbf{A})$). If all eigenvalues have negative real parts, the system is stable, and $\\lVert\\mathbf{x}(t)\\rVert \\to 0$ as $t\\to\\infty$. However, transient growth is possible if $\\mathbf{A}$ is non-normal (i.e., $\\mathbf{A}\\mathbf{A}^\\top \\neq \\mathbf{A}^\\top\\mathbf{A}$). The numerical range provides insight into this transient behavior. The real part of the numerical range is governed by the symmetric part of the matrix, $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$, since $\\text{Re}(\\mathbf{v}^\\top\\mathbf{A}\\mathbf{v}) = \\mathbf{v}^\\top\\mathbf{S}\\mathbf{v}$. A positive eigenvalue of $\\mathbf{S}$ implies that the numerical range of $\\mathbf{A}$ extends into the right half-plane, indicating potential for transient energy growth.\n\nA Galerkin projection with a rank-$r$ POD basis $\\mathbf{Q}$ transforms the FOM $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$ into the ROM $\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r\\mathbf{z} + \\mathbf{b}_r$, where $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. The stability of the ROM is determined by the eigenvalues of the reduced matrix $\\mathbf{A}_r$. Crucially, the eigenvalues of $\\mathbf{A}_r$ are contained within the numerical range of $\\mathbf{A}$, but not necessarily within the convex hull of its spectrum. If the numerical range $W(\\mathbf{A})$ crosses into the right half-plane, it is possible to find a projection subspace (basis $\\mathbf{Q}$) such that $\\mathbf{A}_r$ has eigenvalues with positive real parts, rendering the ROM unstable.\n\nThe problem's construction is designed to expose this pathology. The FOM is stable (eigenvalues of $\\mathbf{A}$ are $\\{-0.1, -1.0\\}$). The forcing term $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$ is chosen such that the FOM steady state is $\\mathbf{x}_{\\infty} = \\mathbf{q}$, where $\\mathbf{q}$ is the eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}$. This drives the system dynamics towards the direction of maximum transient growth. The resulting snapshots will be dominated by this direction, causing the primary POD mode (the first column of $\\mathbf{Q}$) to align with $\\mathbf{q}$. For a rank-$1$ ROM ($r=1$), the reduced matrix $\\mathbf{A}_r$ becomes a scalar $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. If $\\mathbf{Q} \\approx \\mathbf{q}$, then $a_r \\approx \\mathbf{q}^\\top\\mathbf{A}\\mathbf{q} = \\mathbf{q}^\\top\\mathbf{S}\\mathbf{q} = \\lambda_{\\max}(\\mathbf{S})$. For the non-normal matrices in Tests $1$ and $4$, $\\lambda_{\\max}(\\mathbf{S}) > 0$, leading to an unstable ROM.\n\nThe computational procedure for each test case is as follows:\n$1$. Define system parameters: matrix $\\mathbf{A}$, ROM rank $r$, and time horizons $T_{\\text{train}}$ and $T_{\\text{test}}$. The dimension is $n=2$ and the time step is $h=10^{-3}$.\n$2$. Construct the forcing term: Compute the symmetric part $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$. Find its eigenvalues and eigenvectors. Let $\\mathbf{q}$ be the normalized eigenvector corresponding to the largest eigenvalue. Set $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$.\n$3$. Generate training data: Integrate the FOM, $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$, from $\\mathbf{x}(0)=\\mathbf{0}$ over the time interval $[0, T_{\\text{train}}]$ using the classical fourth-order Runge-Kutta method. The states at each time step are collected into the snapshot matrix $\\mathbf{X}$.\n$4$. Compute the POD basis: Perform a singular value decomposition (SVD) on the snapshot matrix, $\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The POD basis $\\mathbf{Q}$ of rank $r$ is formed by the first $r$ columns of $\\mathbf{U}$.\n$5$. Calculate reconstruction error: The relative Frobenius norm error is computed as $\\varepsilon_{\\text{rec}} = \\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F / \\lVert \\mathbf{X}\\rVert_F$.\n$6$. Construct the ROM: The reduced system matrices are $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{b}_r = \\mathbf{Q}^\\top\\mathbf{b}$.\n$7$. Perform time integration for testing: Both the FOM and the ROM are integrated from zero initial conditions ($\\mathbf{x}(0)=\\mathbf{0}$, $\\mathbf{z}(0)=\\mathbf{0}$) over the interval $[0, T_{\\text{test}}]$. During integration, at each step, the Euclidean norm of the state vector is checked against the blow-up threshold $M=10^6$.\n$8$. Record results: The final outputs for the test are the computed $\\varepsilon_{\\text{rec}}$, a boolean indicating if the ROM blew up, and a boolean indicating if the FOM blew up.\n\nExpected outcomes for the tests:\n- **Test 1**: ($\\mathbf{A}$ non-normal, $r=1$): $\\mathbf{A}$ is stable. The construction of $\\mathbf{b}$ and the choice of $r=1$ are designed to produce an unstable ROM. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.\n- **Test 2**: ($\\mathbf{A}$ non-normal, $r=2$): Here, $r=n=2$. The POD basis $\\mathbf{Q}$ will be a complete orthonormal basis for $\\mathbb{R}^2$. Thus, $\\mathbf{Q}\\mathbf{Q}^\\top = \\mathbf{I}$, meaning the reconstruction error $\\varepsilon_{\\text{rec}}$ will be zero (or of the order of machine precision). The ROM is dynamically equivalent to the FOM, simply expressed in a different basis. Since the FOM is stable, the ROM will also be stable. We expect $\\varepsilon_{\\text{rec}} \\approx 0$, no ROM blow-up, and no FOM blow-up.\n- **Test 3**: ($\\mathbf{A}$ symmetric, $r=1$): $\\mathbf{A}$ is a normal matrix. Its numerical range is the convex hull of its eigenvalues, which are $\\{-1.0, -2.0\\}$. Thus, the numerical range is the interval $[-2.0, -1.0]$ on the real axis. The reduced operator $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ must be negative. The ROM will be stable. We expect no blow-up for either model.\n- **Test 4**: ($\\mathbf{A}$ more non-normal, $r=1$): Similar to Test $1$, but with a larger off-diagonal term $\\alpha=120.0$. This increases the non-normality, leading to a larger positive eigenvalue for $\\mathbf{S}$. The ROM instability should be even more pronounced. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.\n\nThe aformentioned logic will now be implemented.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the full numerical experiment to demonstrate ROM instability\n    for a stable FOM.\n    \"\"\"\n\n    def rk4_step(f, y, h, A, b):\n        \"\"\"A single step of the classical fourth-order Runge-Kutta method.\"\"\"\n        k1 = f(y, A, b)\n        k2 = f(y + h / 2 * k1, A, b)\n        k3 = f(y + h / 2 * k2, A, b)\n        k4 = f(y + h * k3, A, b)\n        return y + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    def lti_rhs(y, A, b):\n        \"\"\"RHS of the LTI system dy/dt = Ay + b.\"\"\"\n        return A @ y + b\n\n    def simulate(A, b, y0, T, h, M):\n        \"\"\"\n        Simulates an LTI system and returns snapshots and blow-up status.\n        \"\"\"\n        num_steps = int(T / h)\n        y = y0.copy()\n        snapshots = [y0.copy()]\n        blew_up = False\n        \n        for _ in range(num_steps):\n            y = rk4_step(lti_rhs, y, h, A, b)\n            if np.linalg.norm(y) > M or not np.all(np.isfinite(y)):\n                blew_up = True\n                # Continue collecting snapshots to see the blow-up, if needed for X\n                # But stop checking once blown up.\n                while len(snapshots) < num_steps + 1:\n                    snapshots.append(y.copy()) # Append the diverging state\n                    y = rk4_step(lti_rhs, y, h, A, b) # Could become inf/nan\n                return np.array(snapshots).T, True\n\n            snapshots.append(y.copy())\n            \n        return np.array(snapshots).T, blew_up\n\n    # General parameters\n    n = 2\n    h = 1e-3\n    M = 1e6\n    x0 = np.zeros(n)\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (A_params, r, T_train, T_test)\n        ({\"alpha\": 50.0}, 1, 4.0, 1.0),\n        ({\"alpha\": 50.0}, 2, 4.0, 1.0),\n        ({\"alpha\": None}, 1, 4.0, 1.0), # Symmetric case\n        ({\"alpha\": 120.0}, 1, 4.0, 1.0),\n    ]\n\n    results = []\n    \n    for i, (params, r, T_train, T_test) in enumerate(test_cases):\n        # 1. Define A\n        if i == 2: # Test 3: Symmetric case\n            A = np.array([[-1.0, 0.0], [0.0, -2.0]])\n        else: # Tests 1, 2, 4: Non-normal case\n            alpha = params[\"alpha\"]\n            A = np.array([[-0.1, alpha], [0.0, -1.0]])\n\n        # 2. Construct b\n        S = 0.5 * (A + A.T)\n        eigvals, eigvecs = eigh(S)\n        q = eigvecs[:, -1] # Dominant eigenvector (eigh sorts eigenvalues)\n        b = -A @ q\n\n        # 3. Generate FOM snapshots for training\n        X, _ = simulate(A, b, x0, T_train, h, M)\n\n        # 4. Compute POD basis Q\n        U, s, _ = np.linalg.svd(X, full_matrices=False)\n        Q = U[:, :r]\n\n        # 5. Calculate reconstruction error\n        # eps_rec = norm(X - Q @ Q.T @ X) / norm(X)\n        # Using singular values is more direct: sqrt(sum(s_i^2 for i>r)) / sqrt(sum(s_i^2))\n        if X.shape[1]>1:\n         norm_X_sq = np.sum(s**2)\n         if norm_X_sq > 0:\n            norm_err_sq = np.sum(s[r:]**2)\n            eps_rec = np.sqrt(norm_err_sq / norm_X_sq)\n         else:\n            eps_rec = 0.0\n        else:\n            eps_rec = 0.0\n\n\n        # 6. Form the ROM\n        Ar = Q.T @ A @ Q\n        br = Q.T @ b\n        z0 = np.zeros(r)\n\n        # 7. Simulate FOM and ROM for testing, check blow-up\n        _, fom_blew_up = simulate(A, b, x0, T_test, h, M)\n        _, rom_blew_up = simulate(Ar, br, z0, T_test, h, M)\n\n        # 8. Record results\n        results.append([round(eps_rec, 6), rom_blew_up, fom_blew_up])\n\n    # Final print statement in the exact required format.\n    # Convert bools to lowercase 'true'/'false' for JS-like format\n    formatted_results = []\n    for res in results:\n        eps_str = f\"{res[0]:.6f}\"\n        rom_bool_str = str(res[1]).lower()\n        fom_bool_str = str(res[2]).lower()\n        formatted_results.append(f\"[{eps_str},{rom_bool_str},{fom_bool_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2432128"}, {"introduction": "While Galerkin projection provides a compact representation of the state, it is often not enough to achieve significant speed-up in nonlinear systems, as the cost of evaluating the nonlinear terms can dominate. This practice introduces the concept of hyper-reduction, a critical technique for creating truly fast nonlinear ROMs. You will implement a method based on the Discrete Empirical Interpolation Method (DEIM) to approximate a high-dimensional internal force vector from a small number of \"gappy\" samples, a cornerstone of modern reduced-order modeling for nonlinear solid mechanics [@problem_id:2679826].", "problem": "You are given the task of constructing and evaluating a reduced-order model for reconstructing a high-dimensional internal force vector using gappy sampling within the context of the Finite Element Method (FEM). The goal is to design a sampling set using a pivoted factorization of a reduced basis of internal forces and to compute the reconstruction error as a function of the number of samples for multiple basis sizes. The final outputs are dimensionless relative errors and must be reported as floats.\n\nStart from the following fundamental base of solid mechanics and numerical linear algebra:\n\n- The internal force vector arises from the principle of minimum potential energy. Consider a discrete potential energy given by\n$$\n\\Pi(u) = \\tfrac{1}{2} u^{\\mathsf{T}} K u + \\sum_{i=1}^{n} \\tfrac{\\alpha_i}{4} u_i^4 - f^{\\mathsf{T}} u,\n$$\nwhere $u \\in \\mathbb{R}^{n}$ is the displacement vector, $K \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite stiffness matrix representing a linear elastic discretization, $\\alpha_i > 0$ are nonlinear stiffness coefficients, and $f \\in \\mathbb{R}^{n}$ is the external load vector. The internal force vector is the gradient of the strain energy,\n$$\nf_{\\mathrm{int}}(u) = \\nabla_u \\left( \\tfrac{1}{2} u^{\\mathsf{T}} K u + \\sum_{i=1}^{n} \\tfrac{\\alpha_i}{4} u_i^4 \\right) = K u + \\alpha \\odot u^{\\odot 3},\n$$\nwhere $\\alpha = [\\alpha_1,\\dots,\\alpha_n]^{\\mathsf{T}}$, $\\odot$ denotes the Hadamard product, and $u^{\\odot 3}$ is the element-wise cube. Static equilibrium satisfies $f_{\\mathrm{int}}(u) = f$.\n\n- Use Newton's method to solve the equilibrium equations from first principles. Define the residual $r(u) = K u + \\alpha \\odot u^{\\odot 3} - f$ and the Jacobian $J(u) = K + \\operatorname{diag}(3 \\alpha \\odot u^{\\odot 2})$. The Newton iteration is\n$$\nJ(u^{(k)}) \\, \\Delta u^{(k)} = - r(u^{(k)}), \\quad u^{(k+1)} = u^{(k)} + \\Delta u^{(k)}.\n$$\n\n- Construct a reduced basis for internal forces by Proper Orthogonal Decomposition (POD), which is obtained by the Singular Value Decomposition (SVD) of a snapshot matrix. Given a training snapshot matrix $F_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times N_{\\mathrm{train}}}$ whose columns are internal force snapshots, compute its SVD and retain the first $r$ left singular vectors to form a basis $U_r \\in \\mathbb{R}^{n \\times r}$.\n\n- Design a gappy sampling set using a pivoted factorization of the reduced basis. Use pivoted QR on $U_r^{\\mathsf{T}}$ to select informative sample indices (also known as QDEIM, a special case of the Discrete Empirical Interpolation Method (DEIM)). Let the permutation returned by pivoted QR define an ordered list of sample indices $\\mathcal{P}_r = [p_1, p_2, \\dots, p_n]$. For a specified number of samples $m$, use the first $m$ indices $\\{p_1, \\dots, p_m\\}$.\n\n- For a test internal force $f \\in \\mathbb{R}^n$, reconstruct it from $m$ sampled entries using the Gappy Proper Orthogonal Decomposition (gappy POD) least-squares system\n$$\n\\min_{c \\in \\mathbb{R}^r} \\left\\| S U_r c - S f \\right\\|_2,\n$$\nwhere $S \\in \\mathbb{R}^{m \\times n}$ selects the sampled rows. Use the Moore-Penrose Pseudoinverse (MPP) to compute $c = (S U_r)^{+} S f$ and set the reconstruction $\\widehat{f} = U_r c$. Report the relative error\n$$\ne = \\frac{\\| \\widehat{f} - f \\|_2}{\\| f \\|_2},\n$$\nwhich is dimensionless.\n\nImplement the following concrete, fully specified instance:\n\n- Dimension: $n = 60$.\n- Stiffness: $K \\in \\mathbb{R}^{60 \\times 60}$ is the standard tridiagonal discrete Laplacian with Dirichlet boundary conditions on both ends, i.e., $(K)_{ii} = 2$ for $i=1,\\dots,60$, $(K)_{i,i+1} = (K)_{i+1,i} = -1$ for $i=1,\\dots,59$, and zeros elsewhere.\n- Nonlinear coefficients: for $i=1,\\dots,60$, define $\\alpha_i = 1 + \\tfrac{1}{2} \\sin^2\\!\\left( \\tfrac{2 \\pi i}{61} \\right)$.\n- Newton solver: initialize $u^{(0)} = 0$, use absolute convergence criterion $\\|r(u^{(k)})\\|_2 \\leq \\varepsilon$ with $\\varepsilon = 10^{-10}$ and maximum iterations $k_{\\max} = 50$. If convergence is not reached within $k_{\\max}$ iterations, use the last iterate.\n- Training data: number of snapshots $N_{\\mathrm{train}} = 80$. Generate independent and identically distributed external loads $f \\sim \\mathcal{N}(0,\\sigma^2 I)$ with $\\sigma = 0.1$ using a fixed random seed $s_{\\mathrm{train}} = 2025$. For each load, solve equilibrium to obtain $u$ and then compute $f_{\\mathrm{int}}(u)$, which forms a column of $F_{\\mathrm{train}}$.\n- Testing data: number of snapshots $N_{\\mathrm{test}} = 30$. Generate loads $f \\sim \\mathcal{N}(0,\\sigma^2 I)$ with the same $\\sigma = 0.1$ using a fixed random seed $s_{\\mathrm{test}} = 1337$. For each, compute the corresponding internal force snapshot as in training to form $F_{\\mathrm{test}} \\in \\mathbb{R}^{n \\times N_{\\mathrm{test}}}$.\n- Reduced bases: consider POD force bases with sizes $r \\in \\{4, 8, 12\\}$.\n- Sampling design: for each $r$, compute the pivot ordering $\\mathcal{P}_r$ from pivoted QR of $U_r^{\\mathsf{T}}$. For each specified sample count $m$, use the first $m$ indices in $\\mathcal{P}_r$ to define the sampling operator $S$. If $m &lt; r$, still solve the least-squares problem using the Moore-Penrose Pseudoinverse. If $m &gt; r$, the system is overdetermined and is solved in the least-squares sense.\n- Error metric: for each pair $(r,m)$, compute the average relative error $\\bar{e}(r,m)$ over all $N_{\\mathrm{test}}$ test snapshots. Each $\\bar{e}(r,m)$ is a float.\n\nTest suite specification:\n\n- Basis sizes: $r \\in \\{4, 8, 12\\}$.\n- For $r = 4$, sample counts $m \\in \\{3, 4, 8\\}$.\n- For $r = 8$, sample counts $m \\in \\{4, 8, 12\\}$.\n- For $r = 12$, sample counts $m \\in \\{6, 12, 20\\}$.\n- The required evaluation order for output aggregation is\n$$\n[(4,3), (4,4), (4,8), (8,4), (8,8), (8,12), (12,6), (12,12), (12,20)].\n$$\n\nProgram requirements:\n\n- Implement the full pipeline as specified: model definition, Newton solves, snapshot collection, POD basis computation, sampling index design via pivoted QR on $U_r^{\\mathsf{T}}$, gappy reconstruction via the Moore-Penrose Pseudoinverse, and average relative error calculation.\n- All computations are dimensionless; there are no physical unit conversions required. The final outputs are dimensionless floats.\n- Your program should produce a single line of output containing the nine average relative errors in the evaluation order above, as a comma-separated list enclosed in square brackets. Each float should be rounded to six digits after the decimal point. For example, an acceptable format is\n$$\n[\\bar{e}_{1},\\bar{e}_{2},\\dots,\\bar{e}_{9}],\n$$\nwhere each $\\bar{e}_{i}$ is printed with exactly six digits after the decimal point.", "solution": "The problem presented is valid, scientifically grounded, and well-posed. It outlines a complete numerical experiment in the field of reduced-order modeling for nonlinear solid mechanics. We shall proceed with a systematic solution.\n\nThe overarching goal is to evaluate the efficacy of a gappy reconstruction technique for a high-dimensional internal force vector, which is a key quantity in computational solid mechanics. The methodology hinges on a sequence of standard, yet non-trivial, numerical procedures: solving a nonlinear system via Newton's method, generating a reduced basis via Proper Orthogonal Decomposition (POD), selecting optimal sampling locations using a QR-based algorithm, and finally, reconstructing the full vector from sparse measurements to compute the error.\n\nFirst, we define the full-order model (FOM). The system's state is described by the displacement vector $u \\in \\mathbb{R}^n$. The internal force $f_{\\mathrm{int}}(u)$ is derived as the gradient of the strain energy, a component of the total potential energy $\\Pi(u)$. Given the potential energy\n$$\n\\Pi(u) = \\tfrac{1}{2} u^{\\mathsf{T}} K u + \\sum_{i=1}^{n} \\tfrac{\\alpha_i}{4} u_i^4 - f^{\\mathsf{T}} u,\n$$\nthe internal force is\n$$\nf_{\\mathrm{int}}(u) = \\nabla_u \\left( \\tfrac{1}{2} u^{\\mathsf{T}} K u + \\sum_{i=1}^{n} \\tfrac{\\alpha_i}{4} u_i^4 \\right) = K u + \\alpha \\odot u^{\\odot 3}.\n$$\nHere, $K \\in \\mathbb{R}^{n \\times n}$ is the linear stiffness matrix, $\\alpha \\in \\mathbb{R}^n$ is the vector of nonlinear material coefficients, and $\\odot$ denotes the element-wise Hadamard product. The static equilibrium state for a given external load vector $f \\in \\mathbb{R}^n$ is found by solving the nonlinear system of equations $f_{\\mathrm{int}}(u) = f$.\n\nTo solve this nonlinear system, we employ Newton's method. We define the residual function $r(u) = f_{\\mathrm{int}}(u) - f = K u + \\alpha \\odot u^{\\odot 3} - f$. The iterative update rule for Newton's method is given by solving a linear system for the displacement correction $\\Delta u^{(k)}$ at each step $k$:\n$$\nJ(u^{(k)}) \\, \\Delta u^{(k)} = - r(u^{(k)}),\n$$\nfollowed by an update to the displacement vector:\n$$\nu^{(k+1)} = u^{(k)} + \\Delta u^{(k)}.\n$$\nThe Jacobian matrix $J(u)$, which is the tangent stiffness matrix, is the derivative of the residual with respect to $u$:\n$$\nJ(u) = \\frac{\\partial r(u)}{\\partial u} = K + \\operatorname{diag}(3 \\alpha \\odot u^{\\odot 2}).\n$$\nThe iteration starts from an initial guess, specified as $u^{(0)} = 0$, and continues until the Euclidean norm of the residual falls below a tolerance $\\varepsilon = 10^{-10}$.\n\nThe next stage is data generation for the reduced-order model. We generate two sets of data: a training set to build the model and a testing set to evaluate it. For each set, we generate a number of random external load vectors $f$ from a specified normal distribution. For each $f$, we solve the FOM equilibrium equation $f_{\\mathrm{int}}(u) = f$ using the Newton solver to find the corresponding displacement $u$. The resulting internal force vector $f_{\\mathrm{int}}(u)$ is computed and stored as a column in a snapshot matrix. This yields the training snapshot matrix $F_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times N_{\\mathrm{train}}}$ and the testing snapshot matrix $F_{\\mathrm{test}} \\in \\mathbb{R}^{n \\times N_{\\mathrm{test}}}$.\n\nWith the training data collected, we construct the reduced basis. The method of Proper Orthogonal Decomposition (POD) is used to find a low-dimensional basis that optimally captures the variance in the training data. This is achieved by computing the Singular Value Decomposition (SVD) of the training matrix:\n$$\nF_{\\mathrm{train}} = U \\Sigma V^{\\mathsf{T}}.\n$$\nThe left singular vectors, the columns of the matrix $U \\in \\mathbb{R}^{n \\times n}$, form an orthonormal basis for the space of internal forces. We truncate this basis by retaining only the first $r$ columns, which correspond to the largest singular values, to form the reduced basis $U_r \\in \\mathbb{R}^{n \\times r}$.\n\nA crucial step in gappy reconstruction is the selection of an optimal set of $m$ measurement locations (or sensor placements). We use a method known as Q-DEIM, which employs pivoted QR factorization. We compute the pivoted QR factorization of the transpose of the reduced basis, $U_r^{\\mathsf{T}}$:\n$$\nU_r^{\\mathsf{T}} P = Q R,\n$$\nwhere $P$ is a permutation matrix representing the pivot order. The permutation identifies the columns of $U_r^{\\mathsf{T}}$ (and thus the corresponding rows of $U_r$) in decreasing order of linear independence. We select the first $m$ indices from this permutation, $\\{p_1, \\dots, p_m\\}$, as our set of sampling indices. These indices define a sampling operator $S \\in \\mathbb{R}^{m \\times n}$, which is a matrix that extracts the $m$ corresponding rows from a vector.\n\nFinally, we evaluate the reconstruction performance on the test data. For each test internal force vector $f_{\\mathrm{test}} \\in F_{\\mathrm{test}}$, we first sample it to obtain its measured entries, $S f_{\\mathrm{test}}$. The gappy POD method assumes that the full vector can be approximated as a linear combination of the basis vectors, $f_{\\mathrm{test}} \\approx \\widehat{f} = U_r c$, where $c \\in \\mathbb{R}^r$ are the unknown coefficients. Applying the sampling operator yields $S f_{\\mathrm{test}} \\approx S U_r c$. We solve for the coefficients $c$ by minimizing the discrepancy in the sampled entries in a least-squares sense:\n$$\n\\min_{c \\in \\mathbb{R}^r} \\| S U_r c - S f_{\\mathrm{test}} \\|_2.\n$$\nThe solution is given by the Moore-Penrose Pseudoinverse (MPP):\n$$\nc = (S U_r)^{+} S f_{\\mathrm{test}}.\n$$\nThe reconstructed full-state vector is then $\\widehat{f} = U_r c$. We quantify the accuracy of the reconstruction using the relative error:\n$$\ne = \\frac{\\| \\widehat{f} - f_{\\mathrm{test}} \\|_2}{\\| f_{\\mathrm{test}} \\|_2}.\n$$\nThis process is repeated for all test snapshots, and the average relative error $\\bar{e}(r, m)$ is computed for each specified pair of basis size $r$ and number of samples $m$. The calculations are performed for the pairs $(r,m)$ specified in the problem statement, yielding nine final error values.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, pinv\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline for constructing and evaluating a reduced-order model \n    for internal force reconstruction using gappy POD.\n    \"\"\"\n    \n    # --- 1. Problem Specification ---\n    n = 60\n    alpha_coeffs = 1.0 + 0.5 * np.sin(2 * np.pi * np.arange(1, n + 1) / (n + 1))**2\n    \n    # Newton solver parameters\n    u0 = np.zeros(n)\n    tol = 1e-10\n    max_iter = 50\n    \n    # Data generation parameters\n    n_train = 80\n    n_test = 30\n    sigma = 0.1\n    seed_train = 2025\n    seed_test = 1337\n    \n    # Test cases: (basis_size_r, num_samples_m)\n    test_cases = [\n        (4, 3), (4, 4), (4, 8),\n        (8, 4), (8, 8), (8, 12),\n        (12, 6), (12, 12), (12, 20)\n    ]\n    \n    # --- 2. Full-Order Model (FOM) Definition ---\n    def construct_stiffness_matrix(dim):\n        K = np.diag(np.full(dim, 2.0))\n        K += np.diag(np.full(dim - 1, -1.0), k=1)\n        K += np.diag(np.full(dim - 1, -1.0), k=-1)\n        return K\n\n    K = construct_stiffness_matrix(n)\n\n    def newton_solve(f, u_init, K_mat, alpha_vec, tolerance, max_iterations):\n        \"\"\"Solves the nonlinear system K*u + alpha*(u**3) = f using Newton's method.\"\"\"\n        u = u_init.copy()\n        for _ in range(max_iterations):\n            u_sq = u**2\n            u_cubed = u_sq * u\n            \n            # Residual\n            r = K_mat @ u + alpha_vec * u_cubed - f\n            if np.linalg.norm(r) < tolerance:\n                break\n            \n            # Jacobian\n            J = K_mat + np.diag(3.0 * alpha_vec * u_sq)\n            \n            # Solve linear system for update\n            delta_u = np.linalg.solve(J, -r)\n            u += delta_u\n        \n        # Return the internal force f_int = K*u + alpha*(u**3)\n        return K_mat @ u + alpha_vec * (u**3)\n\n    # --- 3. Snapshot Generation ---\n    def generate_snapshots(num_snapshots, rand_seed):\n        \"\"\"Generates load vectors and solves FOM to get internal force snapshots.\"\"\"\n        rng = np.random.default_rng(rand_seed)\n        f_snapshots = np.zeros((n, num_snapshots))\n        \n        for i in range(num_snapshots):\n            f_ext = rng.normal(0, sigma, n)\n            f_int = newton_solve(f_ext, u0, K, alpha_coeffs, tol, max_iter)\n            f_snapshots[:, i] = f_int\n            \n        return f_snapshots\n\n    F_train = generate_snapshots(n_train, seed_train)\n    F_test = generate_snapshots(n_test, seed_test)\n    \n    # --- 4. ROM Construction: POD Basis ---\n    # Compute SVD of the training data matrix\n    U, _, _ = np.linalg.svd(F_train, full_matrices=False)\n    \n    results = []\n    \n    # --- 5. Evaluation Loop ---\n    for r, m in test_cases:\n        # a. Get reduced basis\n        Ur = U[:, :r]\n        \n        # b. Select sampling indices via pivoted QR on Ur.T (Q-DEIM)\n        _, _, p = qr(Ur.T, pivoting=True)\n        sample_indices = p[:m]\n        \n        # c. Evaluate reconstruction error on test data\n        errors_for_case = []\n        for j in range(F_test.shape[1]):\n            f_test = F_test[:, j]\n            \n            # Sample the test vector and the basis\n            S_Ur = Ur[sample_indices, :]\n            S_f_test = f_test[sample_indices]\n            \n            # Solve for coefficients via Moore-Penrose Pseudoinverse\n            # c = (S*Ur)^+ * (S*f_test)\n            c = pinv(S_Ur) @ S_f_test\n            \n            # Reconstruct the full vector\n            f_recon = Ur @ c\n            \n            # Calculate relative error\n            error = np.linalg.norm(f_recon - f_test) / np.linalg.norm(f_test)\n            errors_for_case.append(error)\n        \n        # d. Compute average error for the (r, m) pair\n        avg_error = np.mean(errors_for_case)\n        results.append(avg_error)\n        \n    # --- 6. Final Output Formatting ---\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2679826"}]}