{"hands_on_practices": [{"introduction": "Before applying the Stochastic Finite Element Method, it is crucial to understand its fundamental components: the polynomial basis functions. This exercise focuses on constructing the multivariate Hermite polynomial basis, which is the standard choice for problems involving Gaussian random variables [@problem_id:2686929]. By explicitly enumerating the basis functions and deriving a formula for the size of the basis set, you will develop a concrete intuition for how the complexity of a Polynomial Chaos Expansion grows with the number of random dimensions ($m$) and the chosen polynomial order ($p$).", "problem": "In the stochastic finite element method, a random response is often represented by a Polynomial Chaos Expansion (PCE) built from an orthogonal basis adapted to the input distribution. Consider an $m$-dimensional vector of independent standard normal random variables $\\boldsymbol{\\Xi} = (\\Xi_{1},\\ldots,\\Xi_{m})$, and the associated Hermite chaos basis constructed from the probabilists’ Hermite polynomials, which are orthogonal in $L^{2}$ with respect to the standard normal density. The univariate probabilists’ Hermite polynomials are defined by the Rodrigues formula\n$$\n\\mathrm{He}_{n}(\\xi) = (-1)^{n} \\exp\\!\\left(\\frac{\\xi^{2}}{2}\\right) \\frac{d^{n}}{d\\xi^{n}} \\exp\\!\\left(-\\frac{\\xi^{2}}{2}\\right),\n$$\nwith the first three given by $\\mathrm{He}_{0}(\\xi)=1$, $\\mathrm{He}_{1}(\\xi)=\\xi$, and $\\mathrm{He}_{2}(\\xi)=\\xi^{2}-1$.\n\nDefine the multivariate Hermite chaos basis functions by tensor products indexed with a multi-index $\\boldsymbol{\\alpha}=(\\alpha_{1},\\ldots,\\alpha_{m}) \\in \\mathbb{N}_{0}^{m}$ and total order $|\\boldsymbol{\\alpha}|=\\sum_{i=1}^{m}\\alpha_{i}$:\n$$\n\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) \\equiv \\prod_{i=1}^{m} \\mathrm{He}_{\\alpha_{i}}(\\Xi_{i}).\n$$\n\nTasks:\n- Write explicitly, in terms of $\\mathrm{He}_{0}$, $\\mathrm{He}_{1}$, and $\\mathrm{He}_{2}$, the set of multivariate basis functions $\\{\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) : |\\boldsymbol{\\alpha}| \\leq p\\}$ for total order $p=2$ and general dimension $m$, organized by total order $|\\boldsymbol{\\alpha}|=0$, $|\\boldsymbol{\\alpha}|=1$, and $|\\boldsymbol{\\alpha}|=2$. Your description should clearly indicate all distinct types of terms without omitting any class.\n- Using only first-principles combinatorial reasoning based on counting nonnegative integer solutions, derive the total number of distinct multivariate basis functions of total order at most $p=2$ as a closed-form function of $m$.\n\nProvide the final answer as a single analytic expression in terms of $m$. No rounding is needed and no units are required.", "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It contains no scientific or factual unsoundness, is formally structured, and provides all necessary definitions and constraints for a unique solution. We will therefore proceed with the derivation.\n\nThe problem is divided into two tasks: first, to explicitly describe the set of multivariate Hermite basis functions for a total order $p \\leq 2$, and second, to derive a closed-form expression for the total number of such functions as a function of the dimension $m$.\n\n**Part 1: Enumeration of Basis Functions for $|\\boldsymbol{\\alpha}| \\leq 2$**\n\nWe seek to identify all distinct multi-indices $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{m}) \\in \\mathbb{N}_{0}^{m}$ such that the total order $|\\boldsymbol{\\alpha}| = \\sum_{i=1}^{m} \\alpha_{i}$ is less than or equal to $2$. The basis functions $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi})$ are constructed as tensor products of the univariate Hermite polynomials $\\mathrm{He}_{\\alpha_{i}}(\\Xi_{i})$. We organize the basis functions by their total order.\n\n**Case 1: Total order $|\\boldsymbol{\\alpha}| = 0$**\nThe condition $\\sum_{i=1}^{m} \\alpha_{i} = 0$, with the constraint that all $\\alpha_{i} \\in \\mathbb{N}_{0}$, has only one possible solution: $\\alpha_{i} = 0$ for all $i \\in \\{1, \\ldots, m\\}$.\nThis corresponds to a single multi-index, $\\boldsymbol{\\alpha} = (0, 0, \\ldots, 0)$.\nThe resulting basis function is:\n$$ \\Psi_{(0, \\ldots, 0)}(\\boldsymbol{\\Xi}) = \\prod_{i=1}^{m} \\mathrm{He}_{0}(\\Xi_{i}) = \\prod_{i=1}^{m} 1 = 1 $$\nThere is $1$ basis function of total order $0$.\n\n**Case 2: Total order $|\\boldsymbol{\\alpha}| = 1$**\nThe condition is $\\sum_{i=1}^{m} \\alpha_{i} = 1$. For non-negative integers $\\alpha_{i}$, this implies that exactly one component, say $\\alpha_{j}$, must be equal to $1$, and all other components $\\alpha_{k}$ (for $k \\neq j$) must be $0$.\nThe index $j$ can be any integer from $1$ to $m$. Thus, there are $m$ distinct multi-indices of this form. For each $j \\in \\{1, \\ldots, m\\}$, the multi-index is $\\boldsymbol{\\alpha} = (0, \\ldots, 1, \\ldots, 0)$, where the $1$ is in the $j$-th position.\nThe corresponding basis functions are:\n$$ \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) = \\mathrm{He}_{1}(\\Xi_{j}) \\prod_{k \\neq j} \\mathrm{He}_{0}(\\Xi_{k}) = \\mathrm{He}_{1}(\\Xi_{j}) $$\nThe set of basis functions for total order $1$ is $\\{\\mathrm{He}_{1}(\\Xi_{j}) : j=1, \\ldots, m\\}$. There are $m$ such functions.\n\n**Case 3: Total order $|\\boldsymbol{\\alpha}| = 2$**\nThe condition is $\\sum_{i=1}^{m} \\alpha_{i} = 2$. For non-negative integers $\\alpha_{i}$, there are two distinct types of solutions:\n- **Type 3a:** One component is $2$, and all others are $0$. Let $\\alpha_{j} = 2$ for some $j \\in \\{1, \\ldots, m\\}$, and $\\alpha_{k} = 0$ for all $k \\neq j$. There are $m$ such multi-indices. The corresponding basis functions are of the form:\n$$ \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) = \\mathrm{He}_{2}(\\Xi_{j}) \\prod_{k \\neq j} \\mathrm{He}_{0}(\\Xi_{k}) = \\mathrm{He}_{2}(\\Xi_{j}) $$\nThis gives a set of $m$ \"pure\" second-order basis functions: $\\{\\mathrm{He}_{2}(\\Xi_{j}) : j=1, \\ldots, m\\}$.\n\n- **Type 3b:** Two components are $1$, and all others are $0$. Let $\\alpha_{j} = 1$ and $\\alpha_{k} = 1$ for two distinct indices $j, k \\in \\{1, \\ldots, m\\}$ with $j \\neq k$. All other components are $0$. The number of ways to choose two distinct indices from a set of $m$ is given by the binomial coefficient $\\binom{m}{2}$. The corresponding basis functions are of the form:\n$$ \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) = \\mathrm{He}_{1}(\\Xi_{j}) \\mathrm{He}_{1}(\\Xi_{k}) \\prod_{l \\neq j,k} \\mathrm{He}_{0}(\\Xi_{l}) = \\mathrm{He}_{1}(\\Xi_{j})\\mathrm{He}_{1}(\\Xi_{k}) $$\nThis gives a set of $\\binom{m}{2} = \\frac{m(m-1)}{2}$ \"mixed\" second-order basis functions: $\\{\\mathrm{He}_{1}(\\Xi_{j})\\mathrm{He}_{1}(\\Xi_{k}) : 1 \\leq j < k \\leq m\\}$.\n\nThe total number of basis functions of order $2$ is the sum of the counts from these two disjoint types: $m + \\binom{m}{2}$.\n\n**Part 2: Total Number of Basis Functions for $|\\boldsymbol{\\alpha}| \\leq 2$**\n\nWe must derive the total number of distinct basis functions for total order at most $p=2$. This is equivalent to finding the cardinality of the set of multi-indices $S = \\{\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{m} : |\\boldsymbol{\\alpha}| \\leq 2\\}$. This is a standard combinatorial problem.\n\nThe problem of counting the number of non-negative integer solutions to the inequality $\\sum_{i=1}^{m} \\alpha_{i} \\leq p$ can be transformed into counting solutions to an equality. We introduce an auxiliary \"slack\" variable, $\\alpha_{m+1} \\in \\mathbb{N}_{0}$, such that the inequality becomes an equality in $m+1$ variables:\n$$ \\sum_{i=1}^{m} \\alpha_{i} + \\alpha_{m+1} = p $$\nThe number of non-negative integer solutions to this equation is given by the \"stars and bars\" formula. The problem is equivalent to distributing $p$ indistinguishable items (stars) into $m+1$ distinguishable bins (the variables $\\alpha_{1}, \\ldots, \\alpha_{m+1}$). The number of ways to do this is given by:\n$$ N_{m,p} = \\binom{p + (m+1) - 1}{(m+1) - 1} = \\binom{p+m}{m} = \\binom{p+m}{p} $$\nFor the specific case of this problem, we have $p=2$. The total number of basis functions, $N$, is:\n$$ N = \\binom{2+m}{m} = \\binom{m+2}{2} $$\nWe expand this binomial coefficient to obtain the final closed-form expression:\n$$ N = \\frac{(m+2)!}{2!(m+2-2)!} = \\frac{(m+2)!}{2 \\cdot m!} = \\frac{(m+2)(m+1)m!}{2m!} = \\frac{(m+1)(m+2)}{2} $$\nThis result can be verified by summing the counts derived in Part 1 for each order:\n- Order $0$: $1$ function.\n- Order $1$: $m$ functions.\n- Order $2$: $m + \\binom{m}{2}$ functions.\nTotal number $N$:\n$$ N = 1 + m + \\left(m + \\binom{m}{2}\\right) = 1 + 2m + \\frac{m(m-1)}{2} $$\n$$ N = \\frac{2 + 4m + m^2 - m}{2} = \\frac{m^2 + 3m + 2}{2} = \\frac{(m+1)(m+2)}{2} $$\nThe verification confirms the result obtained from the stars and bars method. The final expression is the required closed-form function of $m$.", "answer": "$$\\boxed{\\frac{(m+1)(m+2)}{2}}$$", "id": "2686929"}, {"introduction": "With an understanding of the polynomial basis, the next logical step is to apply it to a tangible engineering problem and verify the results. This practice guides you through a classic case study: a one-dimensional elastic bar with an uncertain Young's modulus [@problem_id:2686984]. You will derive the analytical Polynomial Chaos coefficients for the bar's tip displacement, use these coefficients to efficiently compute statistical moments like the mean and variance, and—most importantly—validate your sophisticated Galerkin-based results against a straightforward Monte Carlo simulation, a cornerstone practice in uncertainty quantification.", "problem": "Consider a straight one-dimensional elastic bar of length $L$ (in m), constant cross-sectional area $A$ (in $\\mathrm{m}^2$), fixed at $x=0$ (zero displacement) and subjected to a tensile end load $P$ (in N) at $x=L$. The Young’s modulus is a strictly positive random field modeled by a lognormal parametric form with a single standard Gaussian random variable $\\xi \\sim \\mathcal{N}(0,1)$: \n$$\nE(\\xi) = E_0 \\exp\\left(\\sigma \\,\\xi\\right),\n$$\nwhere $E_0$ (in Pa) is the median modulus and $\\sigma \\ge 0$ is the lognormal standard deviation parameter. Assume small strains, linear elasticity, and quasistatic conditions. Using the definitions of strain $\\varepsilon(x) = \\dfrac{du}{dx}$, Hooke’s law $\\sigma_{xx}(x,\\xi) = E(\\xi)\\varepsilon(x)$, and axial equilibrium $\\dfrac{d}{dx}\\left(A\\,\\sigma_{xx}\\right) = 0$, it follows that the axial stress is uniform and equal to $\\sigma_{xx} = \\dfrac{P}{A}$, leading to a linear displacement field \n$$\nu(x,\\xi) = \\frac{P\\,x}{A\\,E(\\xi)}.\n$$\nDefine the end displacement $u_L(\\xi) \\equiv u(L,\\xi) = \\dfrac{P\\,L}{A\\,E(\\xi)}$.\n\nLet the stochastic solution be approximated by a Stochastic Galerkin (SG) Polynomial Chaos expansion using the orthonormal probabilists’ Hermite polynomials $\\{\\psi_n(\\xi)\\}_{n=0}^p$ (orthonormal with respect to the standard normal weight), truncated at total order $p \\in \\mathbb{N}$. That is,\n$$\nu_L(\\xi) \\approx \\sum_{n=0}^{p} a_n \\,\\psi_n(\\xi).\n$$\nThe SG coefficients $\\{a_n\\}$ are the expansion coefficients of $u_L(\\xi)$ in this orthonormal basis. For such an orthonormal basis, the mean and variance of the truncated SG solution are given by\n$$\n\\mathbb{E}\\left[u_L\\right] \\approx a_0, \n\\qquad\n\\mathrm{Var}\\left[u_L\\right] \\approx \\sum_{n=1}^{p} a_n^2.\n$$\nYour task is to do all of the following from first principles:\n- Use the bar mechanics and the given stochastic model to express $u_L(\\xi)$ in terms of a deterministic scalar and a purely random scalar function of $\\xi$.\n- From the definition of the Stochastic Galerkin Polynomial Chaos expansion with an orthonormal probabilists’ Hermite basis, determine the SG coefficients $\\{a_n\\}_{n=0}^{p}$ for $u_L(\\xi)$.\n- From these coefficients, compute the SG-approximated mean and variance.\n- Independently estimate the mean and variance by direct Monte Carlo sampling of $\\xi$ with $N_{\\text{MC}}$ independent draws, evaluating the corresponding $u_L(\\xi)$ for each draw.\n- Verify correctness by comparing the SG mean and variance to the Monte Carlo estimates using relative errors; declare a test as “pass” only if both the mean and variance relative errors are within their specified tolerances.\n\nImplementation requirements:\n- Units: Use $L$ in m, $A$ in $\\mathrm{m}^2$, $P$ in N, $E_0$ in Pa; the mean of $u_L$ must be in m, and the variance of $u_L$ must be in $\\mathrm{m}^2$. The program’s final outputs are dimensionless booleans.\n- Angle units are not used in this problem.\n- Percentages are not to be used; all tolerances are to be interpreted as decimal fractions.\n\nTest suite:\nFor each test case $k$, you are given $(L, A, P, E_0, \\sigma, p, N_{\\text{MC}}, \\mathrm{tol}_{\\mu}, \\mathrm{tol}_{\\nu})$. Compute the SG mean and variance from the SG coefficients, compute Monte Carlo estimates of the mean and variance using $N_{\\text{MC}}$ samples, and check if both relative errors are within tolerances. The relative error for a quantity $q$ is $\\left|\\dfrac{q_{\\mathrm{SG}} - q_{\\mathrm{MC}}}{q_{\\mathrm{MC}}}\\right|$.\n\nUse the following three test cases:\n1. Case 1 (moderate variability, moderate order):\n   - $L = 2.0$, $A = 0.01$, $P = 1000.0$, $E_0 = 210\\times 10^{9}$, $\\sigma = 0.10$, $p = 4$, $N_{\\text{MC}} = 120000$, $\\mathrm{tol}_{\\mu} = 0.01$, $\\mathrm{tol}_{\\nu} = 0.03$.\n2. Case 2 (near-deterministic, very low order):\n   - $L = 1.0$, $A = 0.005$, $P = 500.0$, $E_0 = 70\\times 10^{9}$, $\\sigma = 0.01$, $p = 1$, $N_{\\text{MC}} = 60000$, $\\mathrm{tol}_{\\mu} = 0.01$, $\\mathrm{tol}_{\\nu} = 0.03$.\n3. Case 3 (higher variability, higher order):\n   - $L = 1.5$, $A = 0.02$, $P = 1500.0$, $E_0 = 100\\times 10^{9}$, $\\sigma = 0.40$, $p = 8$, $N_{\\text{MC}} = 200000$, $\\mathrm{tol}_{\\mu} = 0.01$, $\\mathrm{tol}_{\\nu} = 0.03$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \n  $[r_1,r_2,r_3]$, \n  where $r_k$ is a boolean that is $\\mathrm{True}$ if and only if both the mean and variance relative errors for test case $k$ are less than or equal to their respective tolerances, and $\\mathrm{False}$ otherwise.\n- No additional text should be printed.\n- Use independent random seeds for each test case to ensure reproducibility.\n\nThe final program must be self-contained and runnable as-is, with no user input or external files. It must implement the full workflow described above: derive expressions for the SG coefficients appropriate to the chosen basis, compute mean and variance from those coefficients, perform Monte Carlo estimates, and compare within the specified tolerances.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n\n- **Physical System**: A one-dimensional elastic bar of length $L$, constant cross-sectional area $A$.\n- **Boundary Conditions**: Fixed at $x=0$, axial load $P$ applied at $x=L$.\n- **Material Model**: Young's modulus $E$ is a random variable, $E(\\xi) = E_0 \\exp(\\sigma \\xi)$, where $\\xi \\sim \\mathcal{N}(0,1)$, $E_0 > 0$ is the median modulus, and $\\sigma \\ge 0$ is a parameter.\n- **Governing Equations**:\n    - Strain: $\\varepsilon(x) = \\frac{du}{dx}$.\n    - Hooke's Law: $\\sigma_{xx}(x,\\xi) = E(\\xi)\\varepsilon(x)$.\n    - Equilibrium: $\\frac{d}{dx}(A\\sigma_{xx}) = 0$.\n- **Derived Displacement**: The problem states the end displacement is $u_L(\\xi) \\equiv u(L,\\xi) = \\frac{PL}{A E(\\xi)}$.\n- **Stochastic Approximation**: A Stochastic Galerkin (SG) method using a Polynomial Chaos (PC) expansion with orthonormal probabilists' Hermite polynomials $\\{\\psi_n(\\xi)\\}_{n=0}^p$, truncated at order $p$.\n    - Approximation: $u_L(\\xi) \\approx \\sum_{n=0}^{p} a_n \\psi_n(\\xi)$.\n- **Statistical Moments (SG)**:\n    - Mean: $\\mathbb{E}[u_L] \\approx a_0$.\n    - Variance: $\\mathrm{Var}[u_L] \\approx \\sum_{n=1}^{p} a_n^2$.\n- **Verification**: Direct Monte Carlo (MC) simulation with $N_{\\text{MC}}$ samples.\n- **Comparison Metric**: Relative error $\\left|\\frac{q_{\\mathrm{SG}} - q_{\\mathrm{MC}}}{q_{\\mathrm{MC}}}\\right|$ for mean ($\\mu$) and variance ($\\nu$) against tolerances $\\mathrm{tol}_{\\mu}$ and $\\mathrm{tol}_{\\nu}$.\n- **Test Cases**:\n    1.  $(L, A, P, E_0, \\sigma, p, N_{\\text{MC}}, \\mathrm{tol}_{\\mu}, \\mathrm{tol}_{\\nu}) = (2.0, 0.01, 1000.0, 210\\times 10^{9}, 0.10, 4, 120000, 0.01, 0.03)$.\n    2.  $(L, A, P, E_0, \\sigma, p, N_{\\text{MC}}, \\mathrm{tol}_{\\mu}, \\mathrm{tol}_{\\nu}) = (1.0, 0.005, 500.0, 70\\times 10^{9}, 0.01, 1, 60000, 0.01, 0.03)$.\n    3.  $(L, A, P, E_0, \\sigma, p, N_{\\text{MC}}, \\mathrm{tol}_{\\mu}, \\mathrm{tol}_{\\nu}) = (1.5, 0.02, 1500.0, 100\\times 10^{9}, 0.40, 8, 200000, 0.01, 0.03)$.\n- **Output**: A list of booleans indicating if each test passes.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is examined for validity.\n- **Scientific Grounding**: The physical model is a fundamental problem in linear elastostatics. The stochastic model for the Young's modulus (lognormal distribution) is standard and appropriate for a strictly positive physical quantity. The method of Polynomial Chaos expansion is a well-established and mathematically rigorous technique for uncertainty propagation in computational science and engineering. The problem is scientifically sound.\n- **Well-Posedness**: All required parameters are provided. The formulation is unambiguous. The function to be approximated, $u_L(\\xi)$, is well-behaved. The existence and uniqueness of the Polynomial Chaos coefficients are guaranteed for square-integrable functions, which $u_L(\\xi)$ is with respect to the Gaussian measure. The problem is well-posed.\n- **Objectivity**: The problem is stated in objective, mathematical terms, free of subjective or non-formalizable content.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe derivation proceeds from first principles as required.\n\n**1. Expression for End Displacement**\n\nThe end displacement $u_L(\\xi)$ is given as a function of the standard Gaussian random variable $\\xi$. By substituting the expression for the random Young's modulus $E(\\xi)$ into the displacement formula, we obtain:\n$$\nu_L(\\xi) = \\frac{PL}{A E(\\xi)} = \\frac{PL}{A \\left(E_0 \\exp(\\sigma \\xi)\\right)} = \\frac{PL}{A E_0} \\exp(-\\sigma \\xi)\n$$\nThis expression can be separated into a deterministic scalar part and a purely random scalar function of $\\xi$. Let the deterministic constant be $C$:\n$$\nC = \\frac{PL}{AE_0}\n$$\nAnd let the random part be $g(\\xi)$:\n$$\ng(\\xi) = \\exp(-\\sigma \\xi)\n$$\nThus, the quantity of interest is $u_L(\\xi) = C \\cdot g(\\xi)$.\n\n**2. Stochastic Galerkin Coefficients**\n\nThe goal is to find the coefficients $\\{a_n\\}$ in the approximation:\n$$\nu_L(\\xi) \\approx \\sum_{n=0}^{p} a_n \\psi_n(\\xi)\n$$\nwhere $\\{\\psi_n(\\xi)\\}$ is the orthonormal basis of probabilists' Hermite polynomials. Since $u_L(\\xi) = C \\cdot g(\\xi)$, we only need to find the PC expansion of $g(\\xi) = \\exp(-\\sigma \\xi)$. The coefficients for $u_L(\\xi)$ will then be $C$ times the coefficients for $g(\\xi)$.\n\nWe utilize the generating function for probabilists' Hermite polynomials, $H_n(\\xi)$:\n$$\n\\exp\\left(\\alpha \\xi - \\frac{\\alpha^2}{2}\\right) = \\sum_{n=0}^{\\infty} \\frac{H_n(\\xi)}{n!} \\alpha^n\n$$\nRearranging gives the expansion of an exponential function $\\exp(\\alpha \\xi)$:\n$$\n\\exp(\\alpha \\xi) = \\exp\\left(\\frac{\\alpha^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{H_n(\\xi)}{n!} \\alpha^n\n$$\nThe orthonormal polynomials $\\psi_n(\\xi)$ are related to $H_n(\\xi)$ by $\\psi_n(\\xi) = \\frac{H_n(\\xi)}{\\sqrt{n!}}$. Substituting $H_n(\\xi) = \\sqrt{n!} \\psi_n(\\xi)$ into the expansion gives:\n$$\n\\exp(\\alpha \\xi) = \\exp\\left(\\frac{\\alpha^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{\\sqrt{n!} \\psi_n(\\xi)}{n!} \\alpha^n = \\exp\\left(\\frac{\\alpha^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} \\psi_n(\\xi)\n$$\nWe seek the expansion for $g(\\xi) = \\exp(-\\sigma \\xi)$, which corresponds to setting $\\alpha = -\\sigma$:\n$$\n\\exp(-\\sigma \\xi) = \\exp\\left(\\frac{(-\\sigma)^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{(-\\sigma)^n}{\\sqrt{n!}} \\psi_n(\\xi) = \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{(-\\sigma)^n}{\\sqrt{n!}} \\psi_n(\\xi)\n$$\nThe PC expansion of $u_L(\\xi)$ is then:\n$$\nu_L(\\xi) = C \\cdot \\exp(-\\sigma \\xi) = \\left(\\frac{PL}{AE_0}\\right) \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{(-\\sigma)^n}{\\sqrt{n!}} \\psi_n(\\xi)\n$$\nBy comparing this with $u_L(\\xi) \\approx \\sum_{n=0}^{p} a_n \\psi_n(\\xi)$, we identify the analytical expression for the SG coefficients $a_n$:\n$$\na_n = \\frac{PL}{AE_0} \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\frac{(-\\sigma)^n}{\\sqrt{n!}}\n$$\n\n**3. SG-Approximated Mean and Variance**\n\nUsing the provided formulas, we compute the approximate mean and variance from the coefficients $\\{a_n\\}_{n=0}^p$.\n\nThe mean is approximated by $a_0$:\n$$\n\\mu_{\\text{SG}} = a_0 = \\frac{PL}{AE_0} \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\frac{(-\\sigma)^0}{\\sqrt{0!}} = \\frac{PL}{AE_0} \\exp\\left(\\frac{\\sigma^2}{2}\\right)\n$$\nThe variance is approximated by the sum of squares of the higher-order coefficients:\n$$\n\\nu_{\\text{SG}} = \\sum_{n=1}^{p} a_n^2 = \\sum_{n=1}^{p} \\left( \\frac{PL}{AE_0} \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\frac{(-\\sigma)^n}{\\sqrt{n!}} \\right)^2\n$$\n$$\n\\nu_{\\text{SG}} = \\left(\\frac{PL}{AE_0}\\right)^2 \\exp(\\sigma^2) \\sum_{n=1}^{p} \\frac{(\\sigma^2)^n}{n!}\n$$\nThese formulas provide a direct way to compute the statistical moments from the problem parameters without requiring numerical integration.\n\n**4. Monte Carlo Estimation**\n\nFor verification, we perform a standard Monte Carlo simulation. The procedure is as follows:\n1.  Generate a large number, $N_{\\text{MC}}$, of independent samples of the random variable $\\xi$, denoted $\\{\\xi_i\\}_{i=1}^{N_{\\text{MC}}}$, from the standard normal distribution $\\mathcal{N}(0,1)$.\n2.  For each sample $\\xi_i$, evaluate the corresponding outcome of the end displacement:\n    $$\n    u_{L,i} = u_L(\\xi_i) = \\frac{PL}{AE_0} \\exp(-\\sigma \\xi_i)\n    $$\n3.  Estimate the mean ($\\hat{\\mu}_{\\text{MC}}$) and variance ($\\hat{\\nu}_{\\text{MC}}$) from the set of outcomes $\\{u_{L,i}\\}$:\n    $$\n    \\hat{\\mu}_{\\text{MC}} = \\frac{1}{N_{\\text{MC}}} \\sum_{i=1}^{N_{\\text{MC}}} u_{L,i}\n    $$\n    $$\n    \\hat{\\nu}_{\\text{MC}} = \\frac{1}{N_{\\text{MC}}-1} \\sum_{i=1}^{N_{\\text{MC}}} (u_{L,i} - \\hat{\\mu}_{\\text{MC}})^2\n    $$\nThe use of $N_{\\text{MC}}-1$ in the denominator for the variance provides the unbiased sample variance estimate.\n\n**5. Verification and Implementation Logic**\n\nFor each test case, the following steps are executed:\n1.  Calculate the deterministic constant $C = PL/(AE_0)$.\n2.  Compute the SG-approximated mean $\\mu_{\\text{SG}}$ and variance $\\nu_{\\text{SG}}$ using the derived analytical formulas and the given truncation order $p$.\n3.  Perform a Monte Carlo simulation with $N_{\\text{MC}}$ samples to obtain estimates $\\hat{\\mu}_{\\text{MC}}$ and $\\hat{\\nu}_{\\text{MC}}$.\n4.  Calculate the relative errors for the mean and variance:\n    $$\n    \\text{err}_{\\mu} = \\left| \\frac{\\mu_{\\text{SG}} - \\hat{\\mu}_{\\text{MC}}}{\\hat{\\mu}_{\\text{MC}}} \\right|, \\qquad \\text{err}_{\\nu} = \\left| \\frac{\\nu_{\\text{SG}} - \\hat{\\nu}_{\\text{MC}}}{\\hat{\\nu}_{\\text{MC}}} \\right|\n    $$\n5.  A test case passes if and only if both error conditions are met: $\\text{err}_{\\mu} \\le \\mathrm{tol}_{\\mu}$ and $\\text{err}_{\\nu} \\le \\mathrm{tol}_{\\nu}$. The result is recorded as a boolean value.\nThis entire procedure is implemented in the provided Python code.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the stochastic bar problem using Stochastic Galerkin and Monte Carlo methods,\n    and verifies the results based on specified tolerances.\n    \"\"\"\n    \n    # Test cases: (L, A, P, E0, sigma, p, N_MC, tol_mu, tol_nu)\n    test_cases = [\n        # Case 1 (moderate variability, moderate order)\n        (2.0, 0.01, 1000.0, 210e9, 0.10, 4, 120000, 0.01, 0.03),\n        # Case 2 (near-deterministic, very low order)\n        (1.0, 0.005, 500.0, 70e9, 0.01, 1, 60000, 0.01, 0.03),\n        # Case 3 (higher variability, higher order)\n        (1.5, 0.02, 1500.0, 100e9, 0.40, 8, 200000, 0.01, 0.03),\n    ]\n\n    results = []\n    \n    for i, case in enumerate(test_cases):\n        L, A, P, E0, sigma, p, N_MC, tol_mu, tol_nu = case\n\n        # --- Stochastic Galerkin (SG) Part ---\n\n        # Deterministic constant C = PL / (A * E0)\n        C = (P * L) / (A * E0)\n\n        # Calculate SG coefficients a_n for n=0 to p\n        # a_n = C * exp(sigma^2 / 2) * (-sigma)^n / sqrt(n!)\n        coeffs_a = np.zeros(p + 1)\n        common_factor = C * math.exp(sigma**2 / 2.0)\n        \n        for n in range(p + 1):\n            # Calculate factorial(n) iteratively if it were needed for large numbers,\n            # but math.factorial is fine for small p.\n            try:\n                # Use math.sqrt and math.factorial for scalar math\n                a_n = common_factor * ((-sigma)**n) / math.sqrt(math.factorial(n))\n                coeffs_a[n] = a_n\n            except OverflowError:\n                # Handle potential overflow for large n, though not expected here\n                coeffs_a[n] = 0.0\n\n        # SG Mean: mu_SG = a_0\n        mu_sg = coeffs_a[0]\n\n        # SG Variance: nu_SG = sum_{n=1 to p} a_n^2\n        # Use vectorized operation on the slice of coefficients from index 1.\n        nu_sg = np.sum(coeffs_a[1:]**2)\n\n        # --- Monte Carlo (MC) Part ---\n\n        # Set a unique random seed for each test case for reproducibility\n        np.random.seed(i)\n\n        # Generate N_MC standard normal random samples for xi\n        xi_samples = np.random.randn(N_MC)\n\n        # Calculate the end displacement u_L for each sample\n        # u_L(xi) = C * exp(-sigma * xi)\n        u_L_samples = C * np.exp(-sigma * xi_samples)\n\n        # MC Mean estimate\n        mu_mc = np.mean(u_L_samples)\n        \n        # MC Variance estimate (unbiased, ddof=1)\n        nu_mc = np.var(u_L_samples, ddof=1)\n\n        # --- Comparison and Verification Part ---\n\n        # Calculate relative errors\n        # Handle the case where the MC estimate is near zero to avoid division by zero\n        if abs(mu_mc) > 1e-15:\n            rel_err_mu = abs((mu_sg - mu_mc) / mu_mc)\n        else: # If MC mean is virtually zero, only an exact match is acceptable\n            rel_err_mu = abs(mu_sg - mu_mc)\n\n        if abs(nu_mc) > 1e-15:\n            rel_err_nu = abs((nu_sg - nu_mc) / nu_mc)\n        else: # If MC variance is virtually zero\n            rel_err_nu = abs(nu_sg - nu_mc)\n            \n        # Check if both errors are within tolerance\n        test_passed = (rel_err_mu <= tol_mu) and (rel_err_nu <= tol_nu)\n        results.append(test_passed)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2686984"}, {"introduction": "A primary challenge in applying SFEM to complex, real-world systems is the \"curse of dimensionality,\" where the number of required basis functions becomes computationally prohibitive. This advanced practice introduces a powerful, modern technique to overcome this hurdle by leveraging the concept of sparsity [@problem_id:2686980]. You will learn to reframe the task of computing PCE coefficients as a sparse regression problem, enabling accurate solution recovery from a surprisingly small number of simulation runs using $\\ell_1$-regularization, a method at the heart of compressed sensing.", "problem": "A two-dimensional linear elastic cantilever domain with spatial coordinate $x \\in \\Omega \\subset \\mathbb{R}^{2}$ is clamped on the left edge and subjected to a deterministic surface traction $\\boldsymbol{t}(x)$ on the right edge. The displacement field $\\boldsymbol{u}(x,\\boldsymbol{\\xi})$ depends on a random vector $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ that parametrizes a spatially varying Young’s modulus field $E(x,\\boldsymbol{\\xi})$. Assume small strains and linear elasticity so that the weak form of balance of linear momentum yields, after spatial discretization by a conforming finite element method, a linear system $K(\\boldsymbol{\\xi}) \\boldsymbol{u}(\\boldsymbol{\\xi}) = \\boldsymbol{f}$ for each realization of $\\boldsymbol{\\xi}$, where $K(\\boldsymbol{\\xi}) \\in \\mathbb{R}^{n_{u} \\times n_{u}}$ is the stiffness matrix and $\\boldsymbol{f} \\in \\mathbb{R}^{n_{u}}$ is the deterministic load vector. Consider a scalar quantity of interest $q(\\boldsymbol{\\xi}) = L \\boldsymbol{u}(\\boldsymbol{\\xi})$ for a fixed linear functional $L \\in \\mathbb{R}^{1 \\times n_{u}}$.\n\nThe input random vector is modeled as $d$ independent standard Gaussian variables, and $q(\\boldsymbol{\\xi})$ is approximated by a truncated Polynomial Chaos Expansion (PCE) of total degree $p$ in $d$ variables using multivariate Hermite polynomials $\\{\\psi_{j}(\\boldsymbol{\\xi})\\}_{j=1}^{P}$ that are orthonormal with respect to the Gaussian measure. The truncation size is $P = \\binom{p+d}{p}$, and the truncated representation is $q(\\boldsymbol{\\xi}) \\approx \\sum_{j=1}^{P} c_{j} \\psi_{j}(\\boldsymbol{\\xi})$, where the coefficient vector $\\boldsymbol{c} \\in \\mathbb{R}^{P}$ is assumed to be $s$-sparse.\n\nYou perform $N$ independent realizations $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^{N}$, and, for each, solve the deterministic finite element problem to obtain $q^{(i)} = q(\\boldsymbol{\\xi}^{(i)})$. Due to numerical error and modeling discrepancies, the observed values are $y^{(i)} = q^{(i)} + \\varepsilon^{(i)}$, where $\\varepsilon^{(i)}$ are independent Gaussian perturbations with zero mean and variance $\\sigma^{2}$.\n\nTasks:\n- Using only fundamental definitions from linear elasticity, the finite element method, and orthonormal polynomial expansions, derive the linear regression model that links the observation vector $\\boldsymbol{y} \\in \\mathbb{R}^{N}$ to the PCE coefficient vector $\\boldsymbol{c} \\in \\mathbb{R}^{P}$ via a design matrix built from evaluations of $\\{\\psi_{j}\\}_{j=1}^{P}$ at the samples $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^{N}$. Then, formulate the recovery of $\\boldsymbol{c}$ as an $\\ell_{1}$-regularized regression problem suitable for the underdetermined regime $N < P$.\n- State conditions on the design matrix under which exact recovery of an $s$-sparse $\\boldsymbol{c}$ is possible in the noiseless case and stable recovery is possible in the noisy case. Your answer should reference a property that quantifies near-isometry of $s$-sparse vectors and may optionally include a bound in terms of a pairwise correlation metric between columns.\n- Assume now $d = 6$ and $p = 4$, so that $P = \\binom{p+d}{p}$. Suppose the objective uses the empirical least-squares loss scaled by $1/(2N)$ and an $\\ell_{1}$ penalty, and suppose the columns of the design matrix are normalized so that for each column $\\boldsymbol{\\Psi}_{j}$ one has $(1/N) \\lVert \\boldsymbol{\\Psi}_{j} \\rVert_{2}^{2} = 1$. Under independent Gaussian noise of standard deviation $\\sigma$, adopt the widely used high-probability choice of the regularization level as a function of $(N,P,\\sigma)$ that controls the dual feasibility in the Karush–Kuhn–Tucker optimality conditions. For $N = 120$ and $\\sigma = 5 \\times 10^{-6}$, compute this regularization parameter and express your final numerical answer in meters, rounded to four significant figures.", "solution": "The problem statement is parsed and validated against the required criteria.\n\nGivens:\n- A linear elastic system discretized by the finite element method, resulting in the algebraic system $K(\\boldsymbol{\\xi}) \\boldsymbol{u}(\\boldsymbol{\\xi}) = \\boldsymbol{f}$.\n- The stiffness matrix $K$ depends on a random vector $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ of $d$ independent standard Gaussian variables.\n- A scalar quantity of interest (QoI) is defined as $q(\\boldsymbol{\\xi}) = L \\boldsymbol{u}(\\boldsymbol{\\xi})$.\n- The QoI is approximated by a truncated Polynomial Chaos Expansion (PCE) of total degree $p$: $q(\\boldsymbol{\\xi}) \\approx \\sum_{j=1}^{P} c_{j} \\psi_{j}(\\boldsymbol{\\xi})$, where $P=\\binom{p+d}{p}$ and $\\{\\psi_j\\}$ are orthonormal multivariate Hermite polynomials.\n- The coefficient vector $\\boldsymbol{c} \\in \\mathbb{R}^{P}$ is assumed to be $s$-sparse.\n- $N$ independent samples $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^{N}$ are drawn.\n- The corresponding observations are noisy: $y^{(i)} = q(\\boldsymbol{\\xi}^{(i)}) + \\varepsilon^{(i)}$, where $\\varepsilon^{(i)}$ are independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$.\n- Task 1: Derive the linear regression model for $\\boldsymbol{y} \\in \\mathbb{R}^{N}$ and formulate the $\\ell_1$-regularized recovery problem for $\\boldsymbol{c} \\in \\mathbb{R}^{P}$.\n- Task 2: State conditions on the design matrix for sparse recovery.\n- Task 3: For $d=6$, $p=4$, $N=120$, $\\sigma=5 \\times 10^{-6}$, and a column-normalized design matrix, compute the standard high-probability regularization parameter $\\lambda$ for an objective function with a loss term of $\\frac{1}{2N} \\text{RSS}$.\n\nValidation:\nThe problem is scientifically grounded in the fields of computational solid mechanics, uncertainty quantification, and high-dimensional statistics. The formulation uses standard and consistent mathematical models: linear elasticity, FEM, PCE with Hermite basis for Gaussian inputs, and $\\ell_1$-regularized regression (LASSO) for sparse recovery. The parameters provided are for a well-defined computational problem within this context. The problem is objective, complete, and well-posed. The request for units of \"meters\" for the regularization parameter is dimensionally consistent with the problem setup, as demonstrated by analysis of the objective function. Therefore, the problem is deemed valid.\n\nSolution:\n\n**Part 1: Linear Model and $\\ell_1$-Regularized Formulation**\n\nThe relationship between the observed data and the unknown PCE coefficients is established starting from the observation model for a single sample $i$. The observed value $y^{(i)}$ is given by:\n$$y^{(i)} = q(\\boldsymbol{\\xi}^{(i)}) + \\varepsilon^{(i)}$$\nThe quantity of interest $q(\\boldsymbol{\\xi}^{(i)})$ is approximated by its truncated PCE:\n$$q(\\boldsymbol{\\xi}^{(i)}) \\approx \\sum_{j=1}^{P} c_j \\psi_j(\\boldsymbol{\\xi}^{(i)})$$\nSubstituting the PCE approximation into the observation model, we obtain:\n$$y^{(i)} = \\left(\\sum_{j=1}^{P} c_j \\psi_j(\\boldsymbol{\\xi}^{(i)})\\right) + \\varepsilon^{(i)}$$\nThis equation holds for each of the $N$ observations, $i=1, \\dots, N$. We can assemble these $N$ scalar equations into a single matrix-vector equation. Let $\\boldsymbol{y} \\in \\mathbb{R}^{N}$ be the vector of observations, where $(\\boldsymbol{y})_i = y^{(i)}$. Let $\\boldsymbol{c} \\in \\mathbb{R}^{P}$ be the vector of unknown PCE coefficients, where $(\\boldsymbol{c})_j = c_j$. Let $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{N}$ be the vector of noise terms, where $(\\boldsymbol{\\varepsilon})_i = \\varepsilon^{(i)}$.\n\nWe define the design matrix $\\boldsymbol{\\Psi} \\in \\mathbb{R}^{N \\times P}$ whose elements are the evaluations of the orthonormal polynomial basis functions at the sample points. The element at the $i$-th row and $j$-th column is:\n$$\\Psi_{ij} = \\psi_j(\\boldsymbol{\\xi}^{(i)})$$\nWith these definitions, the system of $N$ equations can be written compactly as:\n$$\\boldsymbol{y} = \\boldsymbol{\\Psi} \\boldsymbol{c} + \\boldsymbol{\\varepsilon}$$\nThis is the linear regression model that links the observation vector $\\boldsymbol{y}$ to the PCE coefficient vector $\\boldsymbol{c}$.\n\nTo recover the sparse vector $\\boldsymbol{c}$ in the underdetermined regime where the number of samples $N$ is less than the number of coefficients $P$, we solve an optimization problem that balances data fidelity with sparsity. The data fidelity is measured by the sum of squared residuals, $\\|\\boldsymbol{y} - \\boldsymbol{\\Psi} \\boldsymbol{c}\\|_2^2$. Sparsity is promoted by penalizing the $\\ell_1$-norm of the coefficient vector, $\\|\\boldsymbol{c}\\|_1 = \\sum_{j=1}^P |c_j|$. The problem specifies scaling the least-squares loss by $1/(2N)$. This leads to the formulation of an $\\ell_1$-regularized regression problem, also known as the LASSO (Least Absolute Shrinkage and Selection Operator):\n$$\\hat{\\boldsymbol{c}} = \\arg\\min_{\\boldsymbol{c} \\in \\mathbb{R}^{P}} \\left( \\frac{1}{2N} \\|\\boldsymbol{y} - \\boldsymbol{\\Psi} \\boldsymbol{c}\\|_2^2 + \\lambda \\|\\boldsymbol{c}\\|_1 \\right)$$\nwhere $\\lambda > 0$ is the regularization parameter that controls the trade-off between data fidelity and sparsity.\n\n**Part 2: Conditions for Sparse Recovery**\n\nThe ability to recover an $s$-sparse vector $\\boldsymbol{c}$ depends on properties of the design matrix $\\boldsymbol{\\Psi}$. The most common and fundamental condition is the Restricted Isometry Property (RIP). For clarity, let us consider the normalized matrix $\\boldsymbol{\\Phi} = \\frac{1}{\\sqrt{N}}\\boldsymbol{\\Psi}$. The matrix $\\boldsymbol{\\Phi}$ is said to satisfy the RIP of order $s$ with constant $\\delta_s \\in (0, 1)$ if for all $s$-sparse vectors $\\boldsymbol{v} \\in \\mathbb{R}^{P}$ (i.e., vectors with at most $s$ non-zero entries), the following inequality holds:\n$$(1 - \\delta_s) \\|\\boldsymbol{v}\\|_2^2 \\le \\|\\boldsymbol{\\Phi} \\boldsymbol{v}\\|_2^2 \\le (1 + \\delta_s) \\|\\boldsymbol{v}\\|_2^2$$\nThis property means that the matrix $\\boldsymbol{\\Phi}$ acts as a near-isometry on the subspace of $s$-sparse vectors.\n\n- **Noiseless Recovery ($\\boldsymbol{\\varepsilon = 0}$):** If the matrix $\\boldsymbol{\\Phi}$ satisfies the RIP of order $2s$ with $\\delta_{2s} < 1$, then any $s$-sparse signal $\\boldsymbol{c}$ can be recovered exactly by solving the convex optimization problem $\\min \\|\\boldsymbol{z}\\|_1$ subject to $\\boldsymbol{\\Phi z} = \\boldsymbol{\\Phi c}$. Slightly stronger conditions, such as $\\delta_{2s} < \\sqrt{2}-1$, guarantee that the LASSO formulation also provides the exact solution.\n\n- **Noisy Recovery:** In the presence of noise $\\boldsymbol{\\varepsilon}$, stable recovery is possible. If $\\boldsymbol{\\Phi}$ satisfies the RIP of order $2s$ with a sufficiently small constant $\\delta_{2s}$ (e.g., $\\delta_{2s} < 1/3$), and the regularization parameter $\\lambda$ is chosen appropriately (e.g., $\\lambda \\approx \\sigma \\sqrt{2\\ln(P)/N}$), then the solution $\\hat{\\boldsymbol{c}}$ of the LASSO problem is close to the true solution $\\boldsymbol{c}$. For a strictly $s$-sparse vector $\\boldsymbol{c}$, the recovery error is bounded as:\n$$\\|\\hat{\\boldsymbol{c}} - \\boldsymbol{c}\\|_2 \\le C_1 \\lambda \\sqrt{s}$$\nfor some constant $C_1$ that depends on $\\delta_{2s}$. This shows that the recovery error is proportional to the noise level (via $\\lambda$) and the sparsity level $s$.\n\nAn alternative, more restrictive but simpler-to-check condition is based on the mutual coherence of the columns of the normalized matrix $\\boldsymbol{\\Phi}$. The coherence $\\mu$ is defined as the maximum absolute inner product between distinct normalized columns:\n$$\\mu = \\max_{j \\neq k} |\\boldsymbol{\\phi}_j^T \\boldsymbol{\\phi}_k|$$\nwhere $\\boldsymbol{\\phi}_j$ are the columns of $\\boldsymbol{\\Phi}$. Exact recovery in the noiseless case is guaranteed if the sparsity $s$ satisfies the condition $s < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$.\n\n**Part 3: Calculation of the Regularization Parameter**\n\nThe problem specifies the objective function $\\min_{\\boldsymbol{c}} \\frac{1}{2N} \\|\\boldsymbol{y} - \\boldsymbol{\\Psi} \\boldsymbol{c}\\|_2^2 + \\lambda \\|\\boldsymbol{c}\\|_1$. A standard choice for the regularization parameter $\\lambda$ is derived from the Karush-Kuhn-Tucker (KKT) optimality conditions. Specifically, $\\lambda$ is chosen to be just large enough to dominate the noise term in the dual feasibility condition. This leads to the condition $\\lambda \\ge \\frac{1}{N} \\|\\boldsymbol{\\Psi}^T \\boldsymbol{\\varepsilon}\\|_\\infty$.\nWe are given that the columns $\\boldsymbol{\\Psi}_j$ of the design matrix are normalized such that $\\frac{1}{N}\\|\\boldsymbol{\\Psi}_j\\|_2^2=1$, which implies $\\|\\boldsymbol{\\Psi}_j\\|_2 = \\sqrt{N}$. The term $(\\boldsymbol{\\Psi}^T \\boldsymbol{\\varepsilon})_j = \\boldsymbol{\\Psi}_j^T \\boldsymbol{\\varepsilon}$ is a random variable. Since $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are i.i.d., $\\boldsymbol{\\Psi}_j^T \\boldsymbol{\\varepsilon}$ is a Gaussian random variable with mean $0$ and variance $\\sigma^2 \\|\\boldsymbol{\\Psi}_j\\|_2^2 = N\\sigma^2$.\nWe need to bound $\\max_{j=1,\\dots,P} |\\boldsymbol{\\Psi}_j^T \\boldsymbol{\\varepsilon}|$. Let $Z_j = \\frac{\\boldsymbol{\\Psi}_j^T \\boldsymbol{\\varepsilon}}{\\sqrt{N}\\sigma}$, which is a standard normal variable $\\mathcal{N}(0, 1)$. We want to set $N\\lambda$ to be an upper bound on $\\max_j |\\boldsymbol{\\Psi}_j^T \\boldsymbol{\\varepsilon}|$ that holds with high probability. This is equivalent to setting $\\frac{\\sqrt{N}\\lambda}{\\sigma}$ to be an upper bound on $\\max_j |Z_j|$. A common choice for this bound, based on extreme value theory for Gaussian variables, is $\\sqrt{2\\ln(P)}$.\nSetting $\\frac{\\sqrt{N}\\lambda}{\\sigma} = \\sqrt{2\\ln(P)}$ gives the widely used theoretical choice for $\\lambda$:\n$$\\lambda = \\sigma \\sqrt{\\frac{2\\ln(P)}{N}}$$\nWe are given the parameters $d=6$, $p=4$, $N=120$, and $\\sigma=5 \\times 10^{-6}$. First, we compute the number of PCE coefficients $P$:\n$$P = \\binom{p+d}{p} = \\binom{4+6}{4} = \\binom{10}{4} = \\frac{10 \\cdot 9 \\cdot 8 \\cdot 7}{4 \\cdot 3 \\cdot 2 \\cdot 1} = 10 \\cdot 3 \\cdot 7 = 210$$\nNow we substitute the values into the formula for $\\lambda$:\n$$\\lambda = (5 \\times 10^{-6}) \\sqrt{\\frac{2 \\ln(210)}{120}}$$\nWe compute the components:\n$$\\ln(210) \\approx 5.3471075$$\n$$\\lambda = (5 \\times 10^{-6}) \\sqrt{\\frac{2 \\times 5.3471075}{120}} = (5 \\times 10^{-6}) \\sqrt{\\frac{10.694215}{120}}$$\n$$\\lambda = (5 \\times 10^{-6}) \\sqrt{0.08911846} \\approx (5 \\times 10^{-6}) \\times 0.2985271$$\n$$\\lambda \\approx 1.4926355 \\times 10^{-6}$$\nAs instructed, the units of $\\lambda$ are meters, consistent with the units of $\\sigma$. Rounding the numerical result to four significant figures gives:\n$$\\lambda \\approx 1.493 \\times 10^{-6}$$", "answer": "$$\\boxed{1.493 \\times 10^{-6}}$$", "id": "2686980"}]}