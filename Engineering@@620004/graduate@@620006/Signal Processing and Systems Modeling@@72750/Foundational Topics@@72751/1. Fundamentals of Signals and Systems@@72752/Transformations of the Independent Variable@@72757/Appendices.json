{"hands_on_practices": [{"introduction": "Transformations of the independent variable do more than just shift or scale a signal; they can also alter its fundamental properties. This exercise invites you to move beyond procedural manipulation and think structurally about the consequences of an affine time transformation. By determining the conditions under which odd symmetry is preserved, you will engage with the crucial concept of invariance, a cornerstone of both signal processing and theoretical physics. [@problem_id:1771619]", "problem": "In signal processing, operations on the independent variable (time, in this case) are fundamental. Consider a continuous-time signal processing system that applies an affine time transformation to an input signal. Let $x(t)$ be an arbitrary non-zero continuous-time signal that is odd, meaning it satisfies the property $x(-t) = -x(t)$ for all real numbers $t$. This signal is fed into the system, which produces an output signal $y(t)$ defined by the transformation $y(t) = x(at+b)$, where $a$ and $b$ are real constants.\n\nFor the system to be useful in certain symmetry-preserving applications, it is required that the output signal $y(t)$ is also an odd signal for *any* non-zero odd input signal $x(t)$.\n\nWhich of the following options represents the necessary and sufficient conditions on the constants $a$ and $b$ for this requirement to be met?\n\nA. $a  0$ and $b=0$\n\nB. $a$ is any real number and $b=0$\n\nC. $a$ is any non-zero real number and $b=0$\n\nD. $a=1$ and $b$ is any real number\n\nE. $a=0$ and $b=0$", "solution": "The problem asks for the necessary and sufficient conditions on the real constants $a$ and $b$ such that the transformed signal $y(t) = x(at+b)$ is odd for any arbitrary non-zero odd signal $x(t)$.\n\nFirst, let's state the given properties and the required property.\n1.  The input signal $x(t)$ is odd. By definition, this means $x(-\\tau) = -x(\\tau)$ for all $\\tau \\in \\mathbb{R}$. A direct consequence of this for any odd signal is that at $\\tau=0$, we have $x(0) = -x(0)$, which implies $2x(0) = 0$, so $x(0)=0$.\n2.  The output signal $y(t)$ is defined as $y(t) = x(at+b)$.\n3.  We require the output signal $y(t)$ to be odd. By definition, this means $y(-t) = -y(t)$ for all $t \\in \\mathbb{R}$.\n\nLet's substitute the definition of $y(t)$ into the condition for it to be odd.\nThe left-hand side (LHS) is:\n$y(-t) = x(a(-t)+b) = x(-at+b)$\n\nThe right-hand side (RHS) is:\n$-y(t) = -x(at+b)$\n\nFor $y(t)$ to be odd, we must have LHS = RHS:\n$x(-at+b) = -x(at+b)$\n\nNow, we use the fact that $x(t)$ is an odd signal. For any argument $\\tau$, we have $-x(\\tau) = x(-\\tau)$. Let's set $\\tau = at+b$. Then,\n$-x(at+b) = x(-(at+b)) = x(-at-b)$\n\nSubstituting this back into our required condition, we get:\n$x(-at+b) = x(-at-b)$\n\nThis equation must hold for all $t \\in \\mathbb{R}$ and for *any* odd signal $x(t)$. This is a very strong constraint. To find the conditions on $a$ and $b$, we analyze this equation by considering two cases for the constant $a$.\n\n**Case 1: $a \\neq 0$**\n\nIf $a \\neq 0$, we can define a new variable $z = -at$. As $t$ spans all real numbers, $z$ also spans all real numbers. The equation becomes:\n$x(z+b) = x(z-b)$ for all $z \\in \\mathbb{R}$.\n\nThis can be rewritten by letting $z' = z-b$, which means $z = z'+b$. Since $z$ spans $\\mathbb{R}$, $z'$ also spans $\\mathbb{R}$. The equation becomes:\n$x(z'+2b) = x(z')$ for all $z' \\in \\mathbb{R}$.\n\nThis equation states that the signal $x(t)$ must be periodic with a period of $2b$. However, the problem statement requires that our condition holds for *any* non-zero odd signal. We can easily find non-zero odd signals that are not periodic. For instance, $x_1(t) = t^3$ is odd but not periodic. For $x_1(t)$ to satisfy $x_1(z'+2b) = x_1(z')$, we would need $(z'+2b)^3 = (z')^3$. This is not true for a general $z'$ unless $2b=0$.\n\nTherefore, for the condition to be satisfied universally for all odd signals, including non-periodic ones, the required \"period\" $2b$ must be zero.\n$2b = 0 \\implies b=0$.\n\nSo, for $a \\neq 0$, the condition is $b=0$.\n\n**Case 2: $a = 0$**\n\nIf $a = 0$, the transformation becomes $y(t) = x(0 \\cdot t + b) = x(b)$. The output signal $y(t)$ is a constant.\nFor this constant-valued signal to be odd, it must satisfy $y(-t) = -y(t)$.\nSubstituting $y(t) = x(b)$, we get:\n$x(b) = -x(b)$\nThis implies $2x(b) = 0$, which means $x(b)=0$.\n\nThis condition, $x(b)=0$, must hold for *any* non-zero odd signal $x(t)$.\nIf we choose a non-zero value for $b$, say $b=b_0 \\neq 0$, can we find an odd signal $x(t)$ for which $x(b_0) \\neq 0$?\nYes. For example, consider the signal $x(t) = t$. This signal is odd, and $x(b_0) = b_0 \\neq 0$.\nAnother example is $x(t) = \\sin(t)$. This signal is odd. If $b_0$ is not a multiple of $\\pi$, then $x(b_0) = \\sin(b_0) \\neq 0$.\nSince the condition $x(b)=0$ must hold for *all* odd signals, it must hold for $x(t)=t$. This requires $b=0$.\n\nLet's verify if $b=0$ works. The condition becomes $x(0)=0$. As we established at the beginning, any odd signal $x(t)$ must satisfy $x(0)=-x(0)$, which means $x(0)=0$. So, the condition $x(0)=0$ is universally true for all odd signals.\nTherefore, if $a=0$, the only value of $b$ that works is $b=0$. In this case, $y(t) = x(0) = 0$. The zero signal is indeed an odd signal.\n\n**Conclusion**\n\nCombining the results from both cases:\n-   If $a \\neq 0$, we must have $b=0$.\n-   If $a = 0$, we must have $b=0$.\n\nIn all possible scenarios, we must have $b=0$. The value of $a$ can be any real number (zero or non-zero). A non-zero $a$ corresponds to scaling and/or reflection of the time axis, while $a=0$ corresponds to a degenerate transformation that maps the entire signal to its value at $t=0$. Both scenarios result in an odd output signal if and only if $b=0$.\n\nThus, the necessary and sufficient condition is $a \\in \\mathbb{R}$ and $b=0$. This corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1771619"}, {"introduction": "In the digital domain, signals are represented as finite sequences of numbers, and time transformations often manifest as permutations of the sample indices. This practice explores this idea within the practical context of a digital data scrambler, which reorders signal samples for purposes like encryption or transmission. You will investigate a special class of \"self-inverting\" scramblers, a task that connects signal processing concepts to the elegant and powerful framework of modular arithmetic and number theory. [@problem_id:1771646]", "problem": "A digital data scrambler is designed to reorder the samples of a discrete-time signal $x[n]$ defined for the time interval $n \\in \\{0, 1, \\dots, N-1\\}$. The scrambling operation produces a new signal $y[n]$ by creating an affine transformation of the time index, such that $y[n] = x[(an+b)\\pmod{N}]$. The parameters $a$ and $b$ are integers chosen from the set $\\{0, 1, \\dots, N-1\\}$, which together define a specific scrambling key $(a, b)$.\n\nFor the scrambling process to be reversible, the index mapping $f(n) = (an+b)\\pmod{N}$ must be a permutation of the set of indices $\\{0, 1, \\dots, N-1\\}$.\n\nA particularly efficient type of scrambler is \"self-inverting\". A scrambler is self-inverting if applying the exact same transformation twice restores the original signal. That is, if $y[n]$ is the scrambled version of $x[n]$, then applying the same transformation to $y[n]$ produces the original signal $x[n]$ for any arbitrary input signal $x[n]$.\n\nFor a system with signal length $N = 72$, determine the total number of distinct self-inverting scrambling keys $(a, b)$ that exist.", "solution": "We seek the number of affine maps $f(n) = an + b \\pmod{N}$ with $N=72$ that are self-inverting, i.e., $f(f(n)) \\equiv n \\pmod{72}$ for all $n$. Compute\n$$\nf(f(n)) \\equiv a(an + b) + b \\equiv a^{2} n + (ab + b) \\pmod{72}.\n$$\nThus self-inversion requires the simultaneous congruences\n$$\na^{2} \\equiv 1 \\pmod{72}, \\quad b(a+1) \\equiv 0 \\pmod{72}.\n$$\nIf $a^{2} \\equiv 1 \\pmod{72}$, then $a$ is a unit modulo $72$ because $a^{2} \\equiv 1$ implies $\\gcd(a,72)=1$. Therefore these two conditions are necessary and sufficient.\n\nFor a fixed $a$ with $a^{2} \\equiv 1 \\pmod{72}$, the number of $b \\pmod{72}$ satisfying $b(a+1) \\equiv 0 \\pmod{72}$ equals $\\gcd(72, a+1)$. To see this, let $c=a+1$, $d=\\gcd(72,c)$, write $72=dN'$, $c=dc'$, with $\\gcd(c',N')=1$. Then $cb \\equiv 0 \\pmod{72}$ reduces to $c'b \\equiv 0 \\pmod{N'}$, which implies $b \\equiv 0 \\pmod{N'}$ since $c'$ is invertible modulo $N'$. Hence $b$ must be a multiple of $N' = 72/d$, yielding exactly $d$ distinct solutions modulo $72$.\n\nTherefore, the total number of self-inverting keys is\n$$\n\\sum_{\\substack{a \\ (\\mathrm{mod}\\ 72)\\\\ a^{2}\\equiv 1 \\ (\\mathrm{mod}\\ 72)}} \\gcd(72, a+1).\n$$\nWe count these via the Chinese remainder theorem. Factor $72=8\\cdot 9$. The solutions to $a^{2} \\equiv 1 \\pmod{8}$ are $a \\equiv 1,3,5,7 \\pmod{8}$ (four solutions), and the solutions to $a^{2} \\equiv 1 \\pmod{9}$ are $a \\equiv 1,8 \\pmod{9}$ (two solutions). Thus there are $4 \\times 2 = 8$ solutions $a \\pmod{72}$.\n\nFor each such $a$, we compute $\\gcd(72, a+1)$. Since $\\gcd(8,9)=1$, we have\n$$\n\\gcd(72, a+1) = \\gcd(8, a+1)\\,\\gcd(9, a+1).\n$$\nThese depend only on the residues of $a$ modulo $8$ and $9$:\n- If $a \\equiv 1,3,5,7 \\pmod{8}$, then $a+1 \\equiv 2,4,6,0 \\pmod{8}$, giving $\\gcd(8,a+1)=2,4,2,8$, whose sum is $2+4+2+8=16$.\n- If $a \\equiv 1 \\pmod{9}$, then $\\gcd(9,a+1)=\\gcd(9,2)=1$; if $a \\equiv 8 \\pmod{9}$, then $\\gcd(9,a+1)=\\gcd(9,0)=9$, and the sum over the two residues is $1+9=10$.\n\nBy independence across the Chinese remainder factors, the total number of pairs $(a,b)$ is the product of these sums:\n$$\n16 \\times 10 = 160.\n$$\nThus there are $160$ distinct self-inverting scrambling keys $(a,b)$ modulo $72$.", "answer": "$$\\boxed{160}$$", "id": "1771646"}, {"introduction": "Thus far, we have focused on transforming a signal's independent variable. This final practice takes the concept to a deeper level: transforming the independent variable of the differential equations that define a system's dynamics. By reparameterizing time within a linear time-invariant (LTI) state-space model, you will derive how the system's governing equations change, a crucial skill for analyzing systems with non-uniform time scales or applying advanced theoretical methods. [@problem_id:2914968]", "problem": "Consider a continuous-time linear time-invariant (LTI) state-space system with state vector $x(t) \\in \\mathbb{R}^{2}$, input $u(t) \\in \\mathbb{R}$, and output $y(t) \\in \\mathbb{R}$, described by\n$$\\dot{x}(t)=A x(t)+B u(t), \\quad y(t)=C x(t)+D u(t),$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$, $B \\in \\mathbb{R}^{2 \\times 1}$, $C \\in \\mathbb{R}^{1 \\times 2}$, and $D \\in \\mathbb{R}$. Assume a strictly increasing reparameterization of time $t=\\phi(\\tau)$ with $\\phi'(\\tau)0$ for all $\\tau$ in its domain. Starting only from the definition of the state-space model and the chain rule for differentiating a composition, derive the state and output equations expressed with respect to the independent variable $\\tau$.\n\nNow specialize to the data\n$$A=\\begin{pmatrix}-1  0 \\\\ 0  -2\\end{pmatrix}, \\quad B=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad C=\\begin{pmatrix}1  1\\end{pmatrix}, \\quad D=0,$$\nwith zero input $u(t)\\equiv 0$, initial condition $x(0)=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, and the time reparameterization $\\phi(\\tau)=\\tau^{2}$ for $\\tau \\ge 0$.\n\nUsing your derived transformed equations, compute the output value $y$ at $\\tau=1$. Express your final answer as a single exact analytic expression. No rounding is required.", "solution": "The problem requires the derivation of a transformed state-space model under a change of the independent variable and subsequent solution for a specific case. We proceed methodically.\n\nFirst, we derive the general form of the state-space equations with respect to a new independent variable $\\tau$, where the original time variable $t$ is given by a strictly increasing function $t = \\phi(\\tau)$. The condition $\\phi'(\\tau)  0$ ensures that the mapping is one-to-one and preserves the direction of time.\n\nLet the original system be\n$$\n\\frac{dx}{dt}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nWe define new functions for the state, input, and output in terms of the variable $\\tau$:\n$$\n\\tilde{x}(\\tau) = x(\\phi(\\tau)), \\quad \\tilde{u}(\\tau) = u(\\phi(\\tau)), \\quad \\tilde{y}(\\tau) = y(\\phi(\\tau))\n$$\nTo find the new state equation, we differentiate $\\tilde{x}(\\tau)$ with respect to $\\tau$ using the chain rule:\n$$\n\\frac{d\\tilde{x}}{d\\tau}(\\tau) = \\frac{d}{d\\tau} \\left( x(\\phi(\\tau)) \\right) = \\frac{dx}{dt}\\bigg|_{t=\\phi(\\tau)} \\cdot \\frac{d\\phi}{d\\tau}(\\tau)\n$$\nFrom the original state equation, we substitute the expression for $\\frac{dx}{dt}$:\n$$\n\\frac{d\\tilde{x}}{d\\tau}(\\tau) = \\left( A x(\\phi(\\tau)) + B u(\\phi(\\tau)) \\right) \\cdot \\phi'(\\tau)\n$$\nReplacing the terms with their definitions in the $\\tau$ domain, we obtain the transformed state equation:\n$$\n\\frac{d\\tilde{x}}{d\\tau}(\\tau) = \\phi'(\\tau) \\left( A \\tilde{x}(\\tau) + B \\tilde{u}(\\tau) \\right)\n$$\nThis is a linear, but generally time-varying, system in the variable $\\tau$, as the coefficient $\\phi'(\\tau)$ multiplies the system matrices.\n\nThe output equation is transformed by a direct substitution of variables:\n$$\n\\tilde{y}(\\tau) = y(\\phi(\\tau)) = C x(\\phi(\\tau)) + D u(\\phi(\\tau)) = C \\tilde{x}(\\tau) + D \\tilde{u}(\\tau)\n$$\nThe matrices $C$ and $D$ are unaffected by the transformation of the independent variable.\n\nNow, we specialize these results to the given data:\n$$\nA=\\begin{pmatrix}-1  0 \\\\ 0  -2\\end{pmatrix}, \\quad B=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad C=\\begin{pmatrix}1  1\\end{pmatrix}, \\quad D=0\n$$\nThe input is zero, $u(t) \\equiv 0$, which implies $\\tilde{u}(\\tau) \\equiv 0$. The time reparameterization is $t = \\phi(\\tau) = \\tau^2$ for $\\tau \\ge 0$. The derivative is $\\phi'(\\tau) = 2\\tau$. Note that the premise $\\phi'(\\tau)  0$ is violated at the single point $\\tau = 0$, but the resulting differential equation is nonetheless well-defined and solvable.\n\nSubstituting these into the derived transformed equations gives:\n$$\n\\frac{d\\tilde{x}}{d\\tau}(\\tau) = (2\\tau) \\left( A \\tilde{x}(\\tau) + B \\cdot 0 \\right) = 2\\tau A \\tilde{x}(\\tau)\n$$\n$$\n\\tilde{y}(\\tau) = C \\tilde{x}(\\tau) + D \\cdot 0 = C \\tilde{x}(\\tau)\n$$\nLet $\\tilde{x}(\\tau) = \\begin{pmatrix} \\tilde{x}_1(\\tau) \\\\ \\tilde{x}_2(\\tau) \\end{pmatrix}$. The state equation becomes:\n$$\n\\frac{d}{d\\tau} \\begin{pmatrix} \\tilde{x}_1(\\tau) \\\\ \\tilde{x}_2(\\tau) \\end{pmatrix} = 2\\tau \\begin{pmatrix}-1  0 \\\\ 0  -2\\end{pmatrix} \\begin{pmatrix} \\tilde{x}_1(\\tau) \\\\ \\tilde{x}_2(\\tau) \\end{pmatrix} = \\begin{pmatrix} -2\\tau \\tilde{x}_1(\\tau) \\\\ -4\\tau \\tilde{x}_2(\\tau) \\end{pmatrix}\n$$\nSince the matrix $A$ is diagonal, the system decouples into two independent scalar ordinary differential equations:\n$1$. $\\frac{d\\tilde{x}_1}{d\\tau} = -2\\tau \\tilde{x}_1(\\tau)$\n$2$. $\\frac{d\\tilde{x}_2}{d\\tau} = -4\\tau \\tilde{x}_2(\\tau)$\n\nThese are separable equations. For the first equation:\n$$\n\\frac{d\\tildex_1}{\\tilde{x}_1} = -2\\tau d\\tau \\implies \\int \\frac{1}{\\tilde{x}_1} d\\tilde{x}_1 = \\int -2\\tau d\\tau \\implies \\ln|\\tilde{x}_1| = -\\tau^2 + K_1\n$$\nThus, $\\tilde{x}_1(\\tau) = C_1 \\exp(-\\tau^2)$ for some constant $C_1$.\nFor the second equation:\n$$\n\\frac{d\\tildex_2}{\\tilde{x}_2} = -4\\tau d\\tau \\implies \\int \\frac{1}{\\tilde{x}_2} d\\tilde{x}_2 = \\int -4\\tau d\\tau \\implies \\ln|\\tilde{x}_2| = -2\\tau^2 + K_2\n$$\nThus, $\\tilde{x}_2(\\tau) = C_2 \\exp(-2\\tau^2)$ for some constant $C_2$.\n\nWe determine the constants $C_1$ and $C_2$ using the initial condition. The initial condition is given as $x(0) = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ at time $t=0$. The corresponding value of $\\tau$ is found from $t = \\tau^2$, so $0 = \\tau^2$, which gives $\\tau=0$ (since $\\tau \\ge 0$). Therefore, the initial condition for the transformed system is $\\tilde{x}(0) = x(\\phi(0)) = x(0) = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n\nAt $\\tau=0$:\n$$\n\\tilde{x}_1(0) = C_1 \\exp(-0^2) = C_1 \\cdot 1 = 1 \\implies C_1 = 1\n$$\n$$\n\\tilde{x}_2(0) = C_2 \\exp(-2 \\cdot 0^2) = C_2 \\cdot 1 = 1 \\implies C_2 = 1\n$$\nThe solution for the state vector in the $\\tau$ domain is:\n$$\n\\tilde{x}(\\tau) = \\begin{pmatrix} \\exp(-\\tau^2) \\\\ \\exp(-2\\tau^2) \\end{pmatrix}\n$$\nNow, we compute the output $\\tilde{y}(\\tau)$ using the transformed output equation:\n$$\n\\tilde{y}(\\tau) = C \\tilde{x}(\\tau) = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix} \\exp(-\\tau^2) \\\\ \\exp(-2\\tau^2) \\end{pmatrix} = \\exp(-\\tau^2) + \\exp(-2\\tau^2)\n$$\nThe problem asks for the output value $y$ at $\\tau=1$, which corresponds to $\\tilde{y}(1)$. We evaluate the expression for $\\tilde{y}(\\tau)$ at $\\tau=1$:\n$$\n\\tilde{y}(1) = \\exp(-(1)^2) + \\exp(-2(1)^2) = \\exp(-1) + \\exp(-2)\n$$\nThis is the exact analytical value of the output.", "answer": "$$\\boxed{\\exp(-1) + \\exp(-2)}$$", "id": "2914968"}]}