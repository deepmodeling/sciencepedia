{"hands_on_practices": [{"introduction": "The first step in digital signal processing is converting a continuous-time signal into a discrete-time sequence through sampling. This exercise builds a crucial theoretical bridge between these two domains by exploring how fundamental quantities like signal energy and power are preserved or scaled across the conversion. By applying Parseval's relation in both continuous and discrete settings, you will derive the precise scaling factors that link the $L^{2}(\\mathbb{R})$ norm of a bandlimited signal to the $\\ell^{2}(\\mathbb{Z})$ norm of its alias-free samples, providing a rigorous foundation for energy-based analysis in the digital domain. [@problem_id:2904635]", "problem": "Consider a real-valued continuous-time signal $x_{\\mathrm{c}}(t)$ belonging to the Hilbert space $L^{2}(\\mathbb{R})$ and additionally bandlimited to $|\\omega| \\leq \\omega_{\\mathrm{B}}$ in the sense that its continuous-time Fourier transform (CTFT) $X_{\\mathrm{c}}(\\omega)$ vanishes for $|\\omega| > \\omega_{\\mathrm{B}}$. Let the signal be uniformly sampled at sampling period $T_{\\mathrm{s}} > 0$ to form the discrete-time sequence $x[n] = x_{\\mathrm{c}}(n T_{\\mathrm{s}})$, and assume the Nyquist condition $\\omega_{\\mathrm{s}} = \\frac{2\\pi}{T_{\\mathrm{s}}} > 2 \\,\\omega_{\\mathrm{B}}$ so that ideal sampling is alias-free. Denote by $X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\Omega})$ the discrete-time Fourier transform (DTFT) of $x[n]$.\n\nThe continuous-time energy of $x_{\\mathrm{c}}(t)$ is defined as $E_{\\mathrm{c}} = \\int_{-\\infty}^{\\infty} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t$, and the discrete-time $\\ell^{2}$-energy of $x[n]$ is $E_{\\mathrm{d}} = \\sum_{n=-\\infty}^{\\infty} |x[n]|^{2}$. The continuous-time time-average power of a (not necessarily square-integrable) signal $x_{\\mathrm{c}}(t)$ is $P_{\\mathrm{c}} = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{T} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t$ when this limit exists, and the discrete-time sample-average power of $x[n]$ is $P_{\\mathrm{d}} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} |x[n]|^{2}$ when this limit exists.\n\nStarting only from standard definitions of the continuous-time Fourier transform and the discrete-time Fourier transform, the ideal sampling model (multiplication by an impulse train in time), and Parseval’s relation in each domain, derive how continuous-time energy and power notions translate to discrete-time under alias-free sampling. In particular, show that there exist positive scaling constants $c_{E}(T_{\\mathrm{s}})$ and $c_{P}(T_{\\mathrm{s}})$, depending only on $T_{\\mathrm{s}}$, such that\n$\\int_{-\\infty}^{\\infty} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t = c_{E}(T_{\\mathrm{s}}) \\sum_{n=-\\infty}^{\\infty} |x[n]|^{2}$ for all bandlimited $x_{\\mathrm{c}} \\in L^{2}(\\mathbb{R})$,\nand for any wide-sense time-average-power signal $x_{\\mathrm{c}}(t)$ that is bandlimited and sampled alias-free,\n$\\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{T} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t = c_{P}(T_{\\mathrm{s}}) \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} |x[n]|^{2}$.\n\nCompute the explicit analytic expressions of $c_{E}(T_{\\mathrm{s}})$ and $c_{P}(T_{\\mathrm{s}})$ in terms of $T_{\\mathrm{s}}$. Report your final answer as a row matrix $\\begin{pmatrix} c_{E}(T_{\\mathrm{s}}) & c_{P}(T_{\\mathrm{s}}) \\end{pmatrix}$. No numerical evaluation is required and no units should be included in the final answer.", "solution": "The problem is to establish a quantitative relationship between the energy and power of a continuous-time bandlimited signal and its alias-free discrete-time counterpart obtained through uniform sampling. We are asked to find the scaling constants $c_{E}(T_{\\mathrm{s}})$ and $c_{P}(T_{\\mathrm{s}})$. The entire derivation must proceed from first principles.\n\nLet us begin by establishing the fundamental relationship between the continuous-time Fourier transform (CTFT) of the signal $x_{\\mathrm{c}}(t)$ and the discrete-time Fourier transform (DTFT) of the sequence $x[n] = x_{\\mathrm{c}}(n T_{\\mathrm{s}})$.\n\nWe adopt the following standard definitions for the Fourier transforms:\nThe CTFT of a signal $g(t)$ is $G(\\omega) = \\int_{-\\infty}^{\\infty} g(t) \\exp(-\\mathrm{j}\\omega t)\\,\\mathrm{d}t$.\nThe inverse CTFT is $g(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} G(\\omega) \\exp(\\mathrm{j}\\omega t)\\,\\mathrm{d}\\omega$.\nThe DTFT of a sequence $h[n]$ is $H(\\mathrm{e}^{\\mathrm{j}\\Omega}) = \\sum_{n=-\\infty}^{\\infty} h[n] \\exp(-\\mathrm{j}\\Omega n)$.\nThe inverse DTFT is $h[n] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} H(\\mathrm{e}^{\\mathrm{j}\\Omega}) \\exp(\\mathrm{j}\\Omega n)\\,\\mathrm{d}\\Omega$.\n\nThe process of ideal sampling can be modeled by multiplying the continuous-time signal $x_{\\mathrm{c}}(t)$ by an impulse train, resulting in the sampled signal $x_{\\mathrm{s}}(t) = x_{\\mathrm{c}}(t) \\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{\\mathrm{s}}) = \\sum_{n=-\\infty}^{\\infty} x_{\\mathrm{c}}(n T_{\\mathrm{s}}) \\delta(t - n T_{\\mathrm{s}})$. The discrete sequence is $x[n] = x_{\\mathrm{c}}(n T_{\\mathrm{s}})$.\n\nThe CTFT of $x_{\\mathrm{s}}(t)$, denoted $X_{\\mathrm{s}}(\\omega)$, can be found in two ways. First, using the time-domain multiplication property, which corresponds to convolution in the frequency domain:\n$X_{\\mathrm{s}}(\\omega) = \\frac{1}{2\\pi} \\left( X_{\\mathrm{c}}(\\omega) * \\mathcal{F}\\left\\{\\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{\\mathrm{s}})\\right\\} \\right)$.\nThe Fourier transform of the impulse train is another impulse train: $\\mathcal{F}\\left\\{\\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{\\mathrm{s}})\\right\\} = \\omega_{\\mathrm{s}} \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega - k \\omega_{\\mathrm{s}})$, where $\\omega_{\\mathrm{s}} = \\frac{2\\pi}{T_{\\mathrm{s}}}$.\nConvolution with this impulse train yields periodic replication of $X_{\\mathrm{c}}(\\omega)$:\n$X_{\\mathrm{s}}(\\omega) = \\frac{1}{2\\pi} X_{\\mathrm{c}}(\\omega) * \\left( \\frac{2\\pi}{T_{\\mathrm{s}}} \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega - k \\omega_{\\mathrm{s}}) \\right) = \\frac{1}{T_{\\mathrm{s}}} \\sum_{k=-\\infty}^{\\infty} X_{\\mathrm{c}}(\\omega - k \\omega_{\\mathrm{s}})$.\n\nSecond, we can compute the CTFT of $x_{\\mathrm{s}}(t)$ directly from its definition as a sum of scaled impulses:\n$X_{\\mathrm{s}}(\\omega) = \\mathcal{F}\\left\\{\\sum_{n=-\\infty}^{\\infty} x[n] \\delta(t - n T_{\\mathrm{s}})\\right\\} = \\sum_{n=-\\infty}^{\\infty} x[n] \\exp(-\\mathrm{j}\\omega n T_{\\mathrm{s}})$.\n\nComparing this with the definition of the DTFT, $X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\Omega})$, we see that $X_{\\mathrm{s}}(\\omega)$ is precisely the DTFT of $x[n]$ evaluated at $\\Omega = \\omega T_{\\mathrm{s}}$.\nThus, we arrive at the fundamental relation:\n$$X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}}) = \\frac{1}{T_{\\mathrm{s}}} \\sum_{k=-\\infty}^{\\infty} X_{\\mathrm{c}}(\\omega - k \\omega_{\\mathrm{s}})$$\n\nNow we derive the scaling constant for energy, $c_{E}(T_{\\mathrm{s}})$.\nThe energy of the continuous-time signal $x_{\\mathrm{c}}(t)$ is given by Parseval's relation for the CTFT:\n$E_{\\mathrm{c}} = \\int_{-\\infty}^{\\infty} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |X_{\\mathrm{c}}(\\omega)|^{2}\\,\\mathrm{d}\\omega$.\nThe energy of the discrete-time sequence $x[n]$ is given by Parseval's relation for the DTFT:\n$E_{\\mathrm{d}} = \\sum_{n=-\\infty}^{\\infty} |x[n]|^{2} = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\Omega})|^{2}\\,\\mathrm{d}\\Omega$.\n\nThe problem states that $x_{\\mathrm{c}}(t)$ is bandlimited to $|\\omega| \\leq \\omega_{\\mathrm{B}}$, meaning $X_{\\mathrm{c}}(\\omega)=0$ for $|\\omega| > \\omega_{\\mathrm{B}}$. The sampling is alias-free, which means $\\omega_{\\mathrm{s}} > 2 \\omega_{\\mathrm{B}}$. This condition ensures that the spectral replicas $X_{\\mathrm{c}}(\\omega - k \\omega_{\\mathrm{s}})$ do not overlap.\nFor the principal frequency interval $|\\omega| < \\omega_{\\mathrm{s}}/2$, the sum $\\sum_{k=-\\infty}^{\\infty} X_{\\mathrm{c}}(\\omega - k \\omega_{\\mathrm{s}})$ simplifies. The term for $k=0$ is $X_{\\mathrm{c}}(\\omega)$. For any $k \\neq 0$, the center of the replica $k\\omega_s$ is outside the interval $(-\\omega_s, \\omega_s)$. Since $|\\omega|<\\omega_s/2$, the frequency argument $\\omega-k\\omega_s$ will have magnitude greater than $\\omega_s/2$, which is in turn greater than $\\omega_{\\mathrm{B}}$. Therefore, $X_{\\mathrm{c}}(\\omega - k \\omega_{\\mathrm{s}}) = 0$ for all $k \\ne 0$ when $|\\omega| < \\omega_{\\mathrm{s}}/2$.\nThe relationship simplifies to $X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}}) = \\frac{1}{T_{\\mathrm{s}}} X_{\\mathrm{c}}(\\omega)$ for $|\\omega| < \\omega_{\\mathrm{s}}/2$.\nThis implies $|X_{\\mathrm{c}}(\\omega)|^{2} = T_{\\mathrm{s}}^{2} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}})|^{2}$ for $|\\omega| < \\omega_{\\mathrm{s}}/2$.\nWe can now relate the energy expressions. Since $X_{\\mathrm{c}}(\\omega) = 0$ for $|\\omega| > \\omega_{\\mathrm{B}}$ and $\\omega_{\\mathrm{B}} < \\omega_{\\mathrm{s}}/2$:\n$E_{\\mathrm{c}} = \\frac{1}{2\\pi} \\int_{-\\omega_{\\mathrm{B}}}^{\\omega_{\\mathrm{B}}} |X_{\\mathrm{c}}(\\omega)|^{2}\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi} \\int_{-\\omega_{\\mathrm{B}}}^{\\omega_{\\mathrm{B}}} T_{\\mathrm{s}}^{2} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}})|^{2}\\,\\mathrm{d}\\omega$.\nSince $X_{\\mathrm{c}}(\\omega)$ is zero for $\\omega_{\\mathrm{B}} < |\\omega| \\leq \\omega_{\\mathrm{s}}/2$, so is $X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}})$. We can extend the integration limits to $[-\\omega_{\\mathrm{s}}/2, \\omega_{\\mathrm{s}}/2] = [-\\pi/T_{\\mathrm{s}}, \\pi/T_{\\mathrm{s}}]$ without changing the value:\n$E_{\\mathrm{c}} = \\frac{T_{\\mathrm{s}}^{2}}{2\\pi} \\int_{-\\pi/T_{\\mathrm{s}}}^{\\pi/T_{\\mathrm{s}}} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\omega T_{\\mathrm{s}}})|^{2}\\,\\mathrm{d}\\omega$.\nLet us perform a change of variables in the integral: $\\Omega = \\omega T_{\\mathrm{s}}$, which implies $\\mathrm{d}\\omega = \\frac{1}{T_{\\mathrm{s}}}\\mathrm{d}\\Omega$. The integration limits become $[-\\pi, \\pi]$.\n$E_{\\mathrm{c}} = \\frac{T_{\\mathrm{s}}^{2}}{2\\pi} \\int_{-\\pi}^{\\pi} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\Omega})|^{2} \\frac{1}{T_{\\mathrm{s}}}\\,\\mathrm{d}\\Omega = \\frac{T_{\\mathrm{s}}}{2\\pi} \\int_{-\\pi}^{\\pi} |X_{\\mathrm{d}}(\\mathrm{e}^{\\mathrm{j}\\Omega})|^{2}\\,\\mathrm{d}\\Omega$.\nThe integral on the right is equal to $2\\pi E_{\\mathrm{d}}$. Thus:\n$E_{\\mathrm{c}} = T_{\\mathrm{s}} E_{\\mathrm{d}} = T_{\\mathrm{s}} \\sum_{n=-\\infty}^{\\infty} |x[n]|^{2}$.\nBy comparison with the problem statement $E_{\\mathrm{c}} = c_{E}(T_{\\mathrm{s}}) E_{\\mathrm{d}}$, we find $c_{E}(T_{\\mathrm{s}}) = T_{\\mathrm{s}}$.\n\nNext, we derive the scaling constant for power, $c_{P}(T_{\\mathrm{s}})$.\n$P_{\\mathrm{c}} = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{T} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t$.\n$P_{\\mathrm{d}} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} |x[n]|^{2}$.\nLet us define the signal $y(t) = |x_{\\mathrm{c}}(t)|^{2}$. The problem is to relate the time-average of $y(t)$ to the sample-average of its samples, $y[n] = y(nT_{\\mathrm{s}}) = |x_{\\mathrm{c}}(nT_{\\mathrm{s}})|^{2} = |x[n]|^{2}$.\nThe signal $x_{\\mathrm{c}}(t)$ is bandlimited to $\\omega_{\\mathrm{B}}$. The squaring operation in the time domain corresponds to convolution in the frequency domain. If $X_{\\mathrm{c}}(\\omega)$ is the CTFT of $x_{\\mathrm{c}}(t)$, then the CTFT of $y(t)=|x_c(t)|^2$ is $Y(\\omega) = \\frac{1}{2\\pi} (X_{\\mathrm{c}}(\\omega) * X_{\\mathrm{c}}^{*}(-\\omega))$. Since $x_{\\mathrm{c}}(t)$ is real, $X_{\\mathrm{c}}^{*}(-\\omega)=X_{\\mathrm{c}}(\\omega)$, so $Y(\\omega) = \\frac{1}{2\\pi} (X_{\\mathrm{c}}(\\omega) * X_{\\mathrm{c}}(\\omega))$. The convolution of two functions supported on $[-\\omega_{\\mathrm{B}}, \\omega_{\\mathrm{B}}]$ is a function supported on $[-2\\omega_{\\mathrm{B}}, 2\\omega_{\\mathrm{B}}]$. Thus, $y(t)$ is bandlimited to $2\\omega_{\\mathrm{B}}$.\nThere exists a theorem stating that for a signal $g(t)$ with time-average $\\bar{g}_{\\mathrm{c}}$, the time-average of its samples $g[n]=g(nT_s)$, denoted $\\bar{g}_{\\mathrm{d}}$, is equal to $\\bar{g}_{\\mathrm{c}}$ if and only if the CTFT of the signal, $G(\\omega)$, is zero at all non-zero integer multiples of the sampling frequency, i.e., $G(k \\omega_{\\mathrm{s}}) = 0$ for all integers $k \\neq 0$.\nIn our case, the signal is $y(t)$, with CTFT $Y(\\omega)$. We have established that $Y(\\omega)=0$ for $|\\omega| > 2\\omega_{\\mathrm{B}}$. The sampling frequency is $\\omega_{\\mathrm{s}} = \\frac{2\\pi}{T_{\\mathrm{s}}}$. The condition is that $Y(k \\omega_{\\mathrm{s}}) = 0$ for $k \\in \\mathbb{Z}, k \\neq 0$.\nFrom the alias-free condition on $x_{\\mathrm{c}}(t)$, we have $\\omega_{\\mathrm{s}} > 2\\omega_{\\mathrm{B}}$.\nFor any non-zero integer $k$, we have $|k \\omega_{\\mathrm{s}}| \\geq \\omega_{\\mathrm{s}} > 2\\omega_{\\mathrm{B}}$.\nSince $Y(\\omega)$ is zero for frequencies with magnitude greater than $2\\omega_{\\mathrm{B}}$, it is certain that $Y(k \\omega_{\\mathrm{s}}) = 0$ for all $k \\neq 0$.\nThe condition is satisfied. Therefore, the average value of the continuous-time signal $y(t)$ is equal to the average value of its discrete-time samples $y[n]$.\n$\\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{T} y(t)\\,\\mathrm{d}t = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} y[n]$.\nSubstituting back $y(t) = |x_{\\mathrm{c}}(t)|^{2}$ and $y[n] = |x[n]|^{2}$:\n$\\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{T} |x_{\\mathrm{c}}(t)|^{2}\\,\\mathrm{d}t = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} |x[n]|^{2}$.\nThis is precisely the statement that $P_{\\mathrm{c}} = P_{\\mathrm{d}}$.\nTherefore, the scaling constant $c_{P}(T_{\\mathrm{s}})$ is $1$.\n\nThe two constants are $c_{E}(T_{\\mathrm{s}}) = T_{\\mathrms}}$ and $c_{P}(T_{\\mathrm{s}}) = 1$.", "answer": "$$\\boxed{\\begin{pmatrix} T_{\\mathrm{s}} & 1 \\end{pmatrix}}$$", "id": "2904635"}, {"introduction": "While sampling connects continuous and discrete signals, discretization bridges continuous and discrete systems. This practice challenges the intuitive assumption that a stable discrete-time system must originate from a stable continuous-time counterpart by examining the specific case of the backward Euler method. You will derive the pole mapping for this common discretization and construct a counterexample to demonstrate that stability is not always preserved, a vital insight for the design and analysis of digital control systems and IIR filters. [@problem_id:2904661]", "problem": "An engineer wishes to relate discrete-time and continuous-time stability properties through a particular discretization. Consider the scalar linear time-invariant (LTI) continuous-time system defined by the differential equation $\\dot{x}(t) = s_{\\mathrm{c}}\\, x(t)$, where $s_{\\mathrm{c}} \\in \\mathbb{C}$ is the continuous-time pole. The engineer uses the backward Euler discretization of the derivative, which approximates $\\dot{x}(t)$ at time $(k+1)T$ by $\\left[x((k+1)T) - x(kT)\\right]/T$ with sampling period $T>0$. This yields a discrete-time recursion $x[k+1] = z\\, x[k]$ with discrete-time pole $z \\in \\mathbb{C}$ that depends on $s_{\\mathrm{c}}$ and $T$.\n\nStarting only from the definitions of stability for continuous time (all poles $s_{\\mathrm{c}}$ satisfy $\\operatorname{Re}(s_{\\mathrm{c}}) < 0$) and for discrete time (all poles $z$ satisfy $|z| < 1$) and from the backward Euler approximation as stated above, do the following:\n\n1. Derive the exact mapping from $s_{\\mathrm{c}}$ to $z$ induced by backward Euler discretization, and invert it to express $s_{\\mathrm{c}}$ as a function of $z$ and $T$.\n2. Using the inverted mapping, rigorously demonstrate that there exist discrete-time stable poles $z$ with $|z|<1$ that correspond to continuous-time poles $s_{\\mathrm{c}}$ with $\\operatorname{Re}(s_{\\mathrm{c}})>0$ under this discretization, hence giving a counterexample to stability preservation in this direction.\n3. As an explicit counterexample, take $T = 1\\,\\mathrm{s}$ and a discrete-time pole $z = -\\tfrac{1}{2}$, which is stable in discrete time. Compute the corresponding continuous-time pole $s_{\\mathrm{c}}$ in $\\mathrm{s}^{-1}$. No rounding is required; provide the exact value.\n\nYour final answer should be the single real number that equals the computed $s_{\\mathrm{c}}$, expressed in $\\mathrm{s}^{-1}$ (do not include units in your final boxed answer).", "solution": "We begin with the scalar continuous-time linear time-invariant (LTI) system $\\dot{x}(t) = s_{\\mathrm{c}}\\, x(t)$, where $s_{\\mathrm{c}} \\in \\mathbb{C}$ is the continuous-time pole. The backward Euler approximation of the derivative at time $(k+1)T$ is given by\n$$\n\\dot{x}((k+1)T) \\approx \\frac{x((k+1)T) - x(kT)}{T}.\n$$\nBy substituting $\\dot{x}((k+1)T) \\approx s_{\\mathrm{c}}\\, x((k+1)T)$ into the differential equation, we obtain\n$$\ns_{\\mathrm{c}}\\, x((k+1)T) = \\frac{x((k+1)T) - x(kT)}{T}.\n$$\nRearranging terms yields\n$$\nx((k+1)T) - T\\, s_{\\mathrm{c}}\\, x((k+1)T) = x(kT),\n$$\nso that\n$$\n\\bigl(1 - T\\, s_{\\mathrm{c}}\\bigr)\\, x((k+1)T) = x(kT).\n$$\nAssuming $1 - T\\, s_{\\mathrm{c}} \\neq 0$, it follows that\n$$\nx((k+1)T) = \\frac{1}{1 - T\\, s_{\\mathrm{c}}}\\, x(kT).\n$$\nThus the discrete-time recursion is $x[k+1] = z\\, x[k]$ with discrete-time pole\n$$\nz = \\frac{1}{1 - T\\, s_{\\mathrm{c}}}.\n$$\nThis is the forward mapping from the continuous-time pole $s_{\\mathrm{c}}$ to the discrete-time pole $z$ induced by backward Euler discretization.\n\nTo invert this relation and express $s_{\\mathrm{c}}$ in terms of $z$ and $T$, we solve for $s_{\\mathrm{c}}$:\n$$\nz = \\frac{1}{1 - T\\, s_{\\mathrm{c}}}\n\\;\\;\\Longrightarrow\\;\\;\n1 - T\\, s_{\\mathrm{c}} = \\frac{1}{z}\n\\;\\;\\Longrightarrow\\;\\;\nT\\, s_{\\mathrm{c}} = 1 - \\frac{1}{z}\n\\;\\;\\Longrightarrow\\;\\;\ns_{\\mathrm{c}}(z,T) = \\frac{1 - \\frac{1}{z}}{T} = \\frac{z - 1}{z\\, T}.\n$$\n\nWe now show that there exist discrete-time stable $z$ with $|z|<1$ for which the corresponding $s_{\\mathrm{c}}(z,T)$ has positive real part. Consider the case where $z$ is real and satisfies $-1 < z < 0$, which lies strictly inside the unit disk and is therefore discrete-time stable. For such $z$, the expression simplifies to\n$$\ns_{\\mathrm{c}}(z,T) = \\frac{z - 1}{z\\, T}.\n$$\nWhen $z \\in (-1,0)$ and $T>0$, we have $z - 1 < 0$ and $z < 0$, so the ratio $(z - 1)/z$ is positive. More explicitly,\n$$\n\\frac{z - 1}{z} = 1 - \\frac{1}{z}.\n$$\nBecause $z \\in (-1,0)$, the term $\\frac{1}{z}$ is negative with magnitude greater than $1$, implying $1 - \\frac{1}{z} > 0$. Therefore, for any $T>0$,\n$$\n\\operatorname{Re}\\bigl(s_{\\mathrm{c}}(z,T)\\bigr) = \\frac{z - 1}{z\\, T} > 0.\n$$\nHence, there exist discrete-time poles $z$ with $|z|<1$ that map, under backward Euler inversion, to continuous-time poles with positive real part. This furnishes the requested counterexample to stability preservation from discrete time to continuous time under this discretization.\n\nFor an explicit numerical instance, take $T = 1$ and $z = -\\tfrac{1}{2}$. Then\n$$\ns_{\\mathrm{c}} = \\frac{z - 1}{z\\, T} = \\frac{-\\tfrac{1}{2} - 1}{\\left(-\\tfrac{1}{2}\\right)\\cdot 1} = \\frac{-\\tfrac{3}{2}}{-\\tfrac{1}{2}} = 3.\n$$\nThis value is positive, confirming that the corresponding continuous-time system is unstable even though the discrete-time pole $z = -\\tfrac{1}{2}$ is stable. The requested final answer is the single real number $3$ in $\\mathrm{s}^{-1}$.", "answer": "$$\\boxed{3}$$", "id": "2904661"}, {"introduction": "The conversion from an analog to a digital signal involves not just sampling in time but also quantization in amplitude. This exercise moves beyond idealized models by investigating the conditions under which the common additive white noise (AWN) model for quantization error breaks down. By constructing a scenario with a low-amplitude input, you will demonstrate that quantization error can be highly structured and correlated with the signal, a crucial lesson for understanding the true performance of ADCs and the potential for non-linear artifacts in digital systems. [@problem_id:2904665]", "problem": "Consider an ideal mid-tread uniform quantizer used inside an ideal Analog-to-Digital Converter (ADC) with no dither. The quantizer maps any real input $x$ to the output $Q(x)$ defined by\n$$\nQ(x) = \\Delta \\,\\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2} \\right\\rfloor,\n$$\nwhere $\\Delta$ is the quantization step size and $\\lfloor \\cdot \\rfloor$ denotes the floor function. Assume the quantizer has a sufficiently large dynamic range so that no overload occurs.\n\nAn analog sinusoid $x_{a}(t) = A \\cos\\!\\left(2\\pi f_{0} t\\right)$ with amplitude $A$ is sampled at sampling frequency $f_{s}$ to form the discrete-time input $x[n] = x_{a}(n/f_{s}) = A \\cos\\!\\left(\\omega_{0} n\\right)$, where $\\omega_{0} = 2\\pi f_{0}/f_{s}$. Assume that $\\omega_{0} = 2\\pi p/M$ with integers $p$ and $M$ that are coprime, and that $A$ and $\\Delta$ satisfy $0 < A < \\Delta/2$. Let the quantization error be $e[n] = Q\\!\\left(x[n]\\right) - x[n]$.\n\nStarting from the definitions above (quantizer mapping, sampling, and quantization error), determine the exact time-average mean-square quantization error,\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(e[n]\\right)^{2},\n$$\nas a closed-form expression in terms of $A$ and $\\Delta$. Your construction should make clear why, under these conditions, the additive white noise (AWN) model of quantization fails by producing an error that is correlated with the input and exhibits visible spectral lines. \n\nExpress your final answer as a single closed-form analytic expression with no units. No rounding is required.", "solution": "The problem requires the calculation of the time-average mean-square quantization error for a specific case of quantizing a sinusoidal signal. The problem is well-posed and scientifically sound, so we may proceed with the solution.\n\nThe givens are:\n-   A mid-tread uniform quantizer defined by $Q(x) = \\Delta \\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2} \\right\\rfloor$.\n-   A discrete-time input signal $x[n] = A \\cos(\\omega_{0} n)$, which is periodic.\n-   A critical condition on the signal amplitude $A$ and quantization step size $\\Delta$: $0 < A < \\frac{\\Delta}{2}$.\n-   The quantization error is $e[n] = Q(x[n]) - x[n]$.\n-   The objective is to find the mean-square error (MSE), defined as $\\text{MSE} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} (e[n])^{2}$.\n\nFirst, we must analyze the behavior of the quantizer under the given constraint on the amplitude. The input signal is $x[n] = A \\cos(\\omega_{0} n)$. The range of values taken by $x[n]$ is $[-A, A]$. The condition $0 < A < \\frac{\\Delta}{2}$ implies that for all integer values of $n$, the input sample $x[n]$ is strictly bounded within the interval $(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2})$. Let us examine the argument of the floor function in the quantizer definition for any input value $x$ in this range:\n$$\n-\\frac{\\Delta}{2} < x < \\frac{\\Delta}{2}\n$$\nDividing by $\\Delta$ (since $\\Delta > 0$), we get:\n$$\n-\\frac{1}{2} < \\frac{x}{\\Delta} < \\frac{1}{2}\n$$\nAdding $\\frac{1}{2}$ to all parts of the inequality gives:\n$$\n0 < \\frac{x}{\\Delta} + \\frac{1}{2} < 1\n$$\nThe floor function $\\lfloor z \\rfloor$ yields the greatest integer less than or equal to $z$. For any value $z$ such that $0 < z < 1$, the floor function $\\lfloor z \\rfloor$ is equal to $0$. Therefore, for any input $x$ in the range $(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2})$, the quantizer output is:\n$$\nQ(x) = \\Delta \\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2} \\right\\rfloor = \\Delta \\times 0 = 0\n$$\nSince for all $n$, the input sample $x[n]$ lies within this range, the quantizer output is consistently zero:\n$$\nQ(x[n]) = 0 \\quad \\text{for all } n\n$$\nThis is a critical consequence of the small-signal condition $A < \\frac{\\Delta}{2}$. The signal is too small to cross any quantization threshold, so it is always mapped to the central quantization level, which is zero for a mid-tread quantizer.\n\nNow we can determine the quantization error sequence $e[n]$:\n$$\ne[n] = Q(x[n]) - x[n] = 0 - x[n] = -x[n]\n$$\nSubstituting the expression for $x[n]$, we have:\n$$\ne[n] = -A \\cos(\\omega_{0} n)\n$$\nThe problem asks for the time-average mean-square error. We substitute our expression for $e[n]$ into the definition of MSE:\n$$\n\\text{MSE} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(-A \\cos(\\omega_{0} n)\\right)^{2} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} A^{2} \\cos^{2}(\\omega_{0} n)\n$$\nThis is precisely the definition of the average power of the signal $x[n]$. To evaluate this limit, we use the trigonometric identity $\\cos^{2}(\\theta) = \\frac{1 + \\cos(2\\theta)}{2}$:\n$$\n\\text{MSE} = A^{2} \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\left( \\frac{1 + \\cos(2\\omega_{0} n)}{2} \\right)\n$$\n$$\n\\text{MSE} = \\frac{A^{2}}{2} \\left( \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} 1 + \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\cos(2\\omega_{0} n) \\right)\n$$\nThe first limit is straightforward:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} 1 = \\lim_{N \\to \\infty} \\frac{N}{N} = 1\n$$\nThe second limit is the time-average of a cosine function. The time average of a sinusoid $\\cos(\\Omega n)$ is zero, unless its frequency $\\Omega$ is an integer multiple of $2\\pi$ (i.e., it is a DC signal). Here, the frequency is $2\\omega_{0} = 4\\pi p/M$. This frequency is an integer multiple of $2\\pi$ if and only if $2p/M$ is an integer. Given that $p$ and $M$ are coprime, this occurs only if $M=1$ or $M=2$. These correspond to the special cases of a DC input or an input at the Nyquist frequency, respectively. For any other case (i.e., for $M>2$), the time average of $\\cos(2\\omega_{0} n)$ is zero. The problem formulation implies generality, not these edge cases. Thus, we take the standard result:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\cos(2\\omega_{0} n) = 0\n$$\nSubstituting these results back into the expression for the MSE, we obtain:\n$$\n\\text{MSE} = \\frac{A^{2}}{2} (1 + 0) = \\frac{A^{2}}{2}\n$$\nThis concludes the derivation of the mean-square error.\n\nThe problem also requires an explanation of why the additive white noise (AWN) model of quantization fails under these conditions. The AWN model is a common approximation where the quantization error $e[n]$ is modeled as a random noise source with specific statistical properties. These properties are:\n1.  The error is uncorrelated with the input signal $x[n]$.\n2.  The error has a uniform probability distribution over the interval $(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2})$.\n3.  The error power spectrum is flat (white).\n\nUnder these assumptions, the mean-square error is predicted to be $\\frac{\\Delta^{2}}{12}$. Our derived result is $\\frac{A^{2}}{2}$, which is clearly different. Our rigorous construction demonstrates the failure of the AWN model by violating every one of its core assumptions:\n\n-   **Violation of Uncorrelation**: We found that $e[n] = -x[n]$. The error is a scaled version of the input signal. The cross-correlation between the error and the input is $R_{ex}[k] = E[e[n]x[n-k]] = E[(-x[n])x[n-k]] = -R_{xx}[k]$, where $R_{xx}[k]$ is the autocorrelation of the input. The error is perfectly anti-correlated with the input, not uncorrelated.\n\n-   **Violation of Uniform Distribution**: The error signal $e[n] = -A \\cos(\\omega_{0} n)$ is a sinusoid. The probability density function of a sinusoidal signal is not uniform; it is given by $p(y) = \\frac{1}{\\pi\\sqrt{A^2 - y^2}}$ for $|y| < A$. This distribution is heavily concentrated near the peaks at $\\pm A$, which is the opposite of a uniform distribution.\n\n-   **Violation of White Spectrum**: A white noise signal has a flat power spectral density, meaning its power is distributed across all frequencies. Our error signal $e[n] = -A \\cos(\\omega_0 n)$ is a pure sinusoid. Its power spectrum consists of two impulses (discrete spectral lines) at the frequencies $\\pm\\omega_{0}$. This is a tonal, highly structured spectrum, not a white spectrum. This is precisely what the problem refers to as \"visible spectral lines\"—the error's spectrum is a replica of the input's spectrum.\n\nIn summary, for a low-amplitude input ($A < \\frac{\\Delta}{2}$), the quantizer behaves as a non-linear \"dead-zone\" device, and the resulting error is entirely determined by, and correlated with, the input signal. The statistical assumptions that underpin the AWN model and its $\\frac{\\Delta^{2}}{12}$ power formula are completely invalid in this regime.", "answer": "$$\\boxed{\\frac{A^{2}}{2}}$$", "id": "2904665"}]}