{"hands_on_practices": [{"introduction": "Understanding how a linear time-invariant (LTI) system responds to different inputs is central to signal processing. This exercise explores the fundamental relationship between a system's stability and its maximum possible gain, a concept captured by the induced $L^2$ norm. By proving this general property and then calculating the specific norm for a given filter [@problem_id:2864797], you will bridge the gap between abstract system theory and practical filter analysis, developing essential skills for characterizing system performance.", "problem": "Consider a continuous-time Linear Time-Invariant (LTI) system with impulse response $h(t)$ and frequency response $H(\\omega) = \\int_{-\\infty}^{\\infty} h(t)\\exp(-j \\omega t)\\,dt$, where $j$ is the imaginary unit. Assume the system is bounded-input bounded-output (BIBO) stable so that $h \\in L^{1}(\\mathbb{R})$. For any deterministic input $x \\in L^{2}(\\mathbb{R})$, the output is $y = h * x$, where $*$ denotes convolution. The Fourier transform is defined by $X(\\omega) = \\int_{-\\infty}^{\\infty} x(t)\\exp(-j \\omega t)\\,dt$ with the inverse transform $x(t) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} X(\\omega)\\exp(j \\omega t)\\,d\\omega$. The $L^{2}$ norm is $\\|x\\|_{2} = \\left(\\int_{-\\infty}^{\\infty} |x(t)|^{2} dt\\right)^{1/2}$ and the essential supremum norm is $\\|H\\|_{\\infty} = \\operatorname*{ess\\,sup}_{\\omega \\in \\mathbb{R}} |H(\\omega)|$.\n\nTasks:\na) Starting only from the definitions above and well-tested facts about the Fourier transform on $L^{1}$ and $L^{2}$, show that $H(\\omega) \\in L^{\\infty}(\\mathbb{R})$ and that the induced $L^{2}$ inequality $\\|y\\|_{2} \\le \\|H\\|_{\\infty}\\,\\|x\\|_{2}$ holds for all $x \\in L^{2}(\\mathbb{R})$.\n\nb) For the specific, stable rational transfer function $H(s) = \\dfrac{s + 2}{s^{2} + 2 s + 2}$, compute the exact value of the norm $\\|H\\|_{\\infty} = \\sup_{\\omega \\in \\mathbb{R}} |H(j\\omega)|$. Express the final answer as a single closed-form expression. Do not round.", "solution": "The problem statement is first subjected to validation.\n\nStep 1: Extracted Givens.\n- System type: Continuous-time Linear Time-Invariant (LTI).\n- Impulse response: $h(t)$.\n- Frequency response: $H(\\omega) = \\int_{-\\infty}^{\\infty} h(t)\\exp(-j \\omega t)\\,dt$.\n- Stability: Bounded-input bounded-output (BIBO) stable, which implies $h \\in L^{1}(\\mathbb{R})$.\n- Input: $x \\in L^{2}(\\mathbb{R})$.\n- Output: $y = h * x$.\n- Fourier Transform: $X(\\omega) = \\int_{-\\infty}^{\\infty} x(t)\\exp(-j \\omega t)\\,dt$.\n- Inverse Fourier Transform: $x(t) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} X(\\omega)\\exp(j \\omega t)\\,d\\omega$.\n- $L^{2}$ norm: $\\|x\\|_{2} = \\left(\\int_{-\\infty}^{\\infty} |x(t)|^{2} dt\\right)^{1/2}$.\n- $L^{\\infty}$ norm: $\\|H\\|_{\\infty} = \\operatorname*{ess\\,sup}_{\\omega \\in \\mathbb{R}} |H(\\omega)|$.\n- Task (a): Show $H(\\omega) \\in L^{\\infty}(\\mathbb{R})$ and $\\|y\\|_{2} \\le \\|H\\|_{\\infty}\\,\\|x\\|_{2}$.\n- Task (b): For $H(s) = \\frac{s + 2}{s^{2} + 2 s + 2}$, compute $\\|H\\|_{\\infty} = \\sup_{\\omega \\in \\mathbb{R}} |H(j\\omega)|$.\n\nStep 2: Validation Using Extracted Givens.\nThe problem is scientifically grounded in the established theory of LTI systems and Fourier analysis. All definitions and notations are standard in signal processing and systems theory. The problem is well-posed; part (a) is a fundamental theorem regarding the induced $L^2$ norm of an LTI system, and part (b) is a well-defined optimization problem. The language is objective and precise. The stability of the filter in part (b) can be confirmed by finding the poles of $H(s)$, which are the roots of $s^2+2s+2=0$. The poles are $s = \\frac{-2 \\pm \\sqrt{4-8}}{2} = -1 \\pm j$. Since both poles have negative real parts, the system is indeed stable, confirming the premise. The problem is self-contained, consistent, and requires no external, unstated assumptions.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. We shall proceed with the solution.\n\nThe solution is presented in two parts as requested.\n\nPart (a):\nWe are to show that $H(\\omega) \\in L^{\\infty}(\\mathbb{R})$ and that $\\|y\\|_{2} \\le \\|H\\|_{\\infty}\\,\\|x\\|_{2}$.\n\nFirst, we prove that $H(\\omega) \\in L^{\\infty}(\\mathbb{R})$. By definition, $H(\\omega) = \\int_{-\\infty}^{\\infty} h(t)\\exp(-j \\omega t)\\,dt$. We take the absolute value of both sides:\n$$\n|H(\\omega)| = \\left| \\int_{-\\infty}^{\\infty} h(t)\\exp(-j \\omega t)\\,dt \\right|\n$$\nUsing the triangle inequality for integrals, which states that $|\\int f(z) dz| \\le \\int |f(z)| dz$, we obtain:\n$$\n|H(\\omega)| \\le \\int_{-\\infty}^{\\infty} |h(t)\\exp(-j \\omega t)|\\,dt\n$$\nSince $|\\exp(-j \\omega t)| = \\sqrt{\\cos^2(-\\omega t) + \\sin^2(-\\omega t)} = 1$ for all real $\\omega$ and $t$, the inequality simplifies to:\n$$\n|H(\\omega)| \\le \\int_{-\\infty}^{\\infty} |h(t)|\\,dt = \\|h\\|_{1}\n$$\nThe problem states the system is BIBO stable, which is equivalent to the condition that the impulse response is absolutely integrable, i.e., $h \\in L^{1}(\\mathbb{R})$. This means $\\|h\\|_{1}$ is a finite positive constant. Thus, $|H(\\omega)|$ is bounded for all $\\omega \\in \\mathbb{R}$ by $\\|h\\|_{1}$. The essential supremum of $|H(\\omega)|$ must therefore be finite:\n$$\n\\|H\\|_{\\infty} = \\operatorname*{ess\\,sup}_{\\omega \\in \\mathbb{R}} |H(\\omega)| \\le \\|h\\|_{1} < \\infty\n$$\nA function whose essential supremum is finite is, by definition, a member of $L^{\\infty}(\\mathbb{R})$. This completes the first part of the proof.\n\nSecond, we prove the inequality $\\|y\\|_{2} \\le \\|H\\|_{\\infty}\\,\\|x\\|_{2}$. The output is given by the convolution $y(t) = (h * x)(t)$. Applying the Fourier transform and using the convolution theorem, we get $Y(\\omega) = H(\\omega)X(\\omega)$.\nWe use Parseval's theorem, which for the given Fourier transform convention is:\n$$\n\\|f\\|_{2}^{2} = \\int_{-\\infty}^{\\infty} |f(t)|^2\\,dt = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |F(\\omega)|^2\\,d\\omega\n$$\nApplying this theorem to the output $y(t)$, we have:\n$$\n\\|y\\|_{2}^{2} = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |Y(\\omega)|^2\\,d\\omega = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |H(\\omega)X(\\omega)|^2\\,d\\omega = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |H(\\omega)|^2 |X(\\omega)|^2\\,d\\omega\n$$\nBy the definition of the essential supremum norm, $|H(\\omega)| \\le \\|H\\|_{\\infty}$ for almost every $\\omega \\in \\mathbb{R}$. Consequently, $|H(\\omega)|^2 \\le \\|H\\|_{\\infty}^2$ for almost every $\\omega \\in \\mathbb{R}$. We can use this to bound the integral:\n$$\n\\|y\\|_{2}^{2} \\le \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\|H\\|_{\\infty}^2 |X(\\omega)|^2\\,d\\omega\n$$\nSince $\\|H\\|_{\\infty}$ is a constant, it can be factored out of the integral:\n$$\n\\|y\\|_{2}^{2} \\le \\|H\\|_{\\infty}^2 \\left( \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |X(\\omega)|^2\\,d\\omega \\right)\n$$\nRecognizing the term in the parentheses as $\\|x\\|_{2}^2$ via Parseval's theorem, we arrive at:\n$$\n\\|y\\|_{2}^{2} \\le \\|H\\|_{\\infty}^2 \\|x\\|_{2}^2\n$$\nSince norms are non-negative, we can take the square root of both sides to obtain the desired inequality:\n$$\n\\|y\\|_{2} \\le \\|H\\|_{\\infty} \\|x\\|_{2}\n$$\nThis completes the proof for part (a).\n\nPart (b):\nWe are asked to compute $\\|H\\|_{\\infty} = \\sup_{\\omega \\in \\mathbb{R}} |H(j\\omega)|$ for the transfer function $H(s) = \\frac{s + 2}{s^{2} + 2 s + 2}$.\nFirst, we find the frequency response $H(j\\omega)$ by substituting $s = j\\omega$:\n$$\nH(j\\omega) = \\frac{j\\omega + 2}{(j\\omega)^{2} + 2(j\\omega) + 2} = \\frac{2 + j\\omega}{-\\omega^2 + 2j\\omega + 2} = \\frac{2 + j\\omega}{(2 - \\omega^2) + j(2\\omega)}\n$$\nNext, we determine the magnitude $|H(j\\omega)|$:\n$$\n|H(j\\omega)| = \\frac{|2 + j\\omega|}{|(2 - \\omega^2) + j(2\\omega)|} = \\frac{\\sqrt{2^2 + \\omega^2}}{\\sqrt{(2 - \\omega^2)^2 + (2\\omega)^2}} = \\frac{\\sqrt{4 + \\omega^2}}{\\sqrt{4 - 4\\omega^2 + \\omega^4 + 4\\omega^2}} = \\frac{\\sqrt{4 + \\omega^2}}{\\sqrt{\\omega^4 + 4}}\n$$\nTo find the supremum of $|H(j\\omega)|$, it is computationally simpler to find the maximum of its square, $G(\\omega) = |H(j\\omega)|^2$. Let $u = \\omega^2$. Since $\\omega \\in \\mathbb{R}$, we have $u \\ge 0$. The problem reduces to finding the maximum of the function $f(u)$ for $u \\ge 0$:\n$$\nf(u) = \\frac{4 + u}{u^2 + 4}\n$$\nWe use differential calculus to find the maximum. We compute the derivative of $f(u)$ with respect to $u$ using the quotient rule:\n$$\nf'(u) = \\frac{(1)(u^2 + 4) - (4 + u)(2u)}{(u^2 + 4)^2} = \\frac{u^2 + 4 - 8u - 2u^2}{(u^2 + 4)^2} = \\frac{-u^2 - 8u + 4}{(u^2 + 4)^2}\n$$\nTo find the critical points, we set the numerator to zero:\n$$\n-u^2 - 8u + 4 = 0 \\quad \\implies \\quad u^2 + 8u - 4 = 0\n$$\nSolving for $u$ using the quadratic formula $u = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\nu = \\frac{-8 \\pm \\sqrt{8^2 - 4(1)(-4)}}{2(1)} = \\frac{-8 \\pm \\sqrt{64 + 16}}{2} = \\frac{-8 \\pm \\sqrt{80}}{2} = \\frac{-8 \\pm 4\\sqrt{5}}{2} = -4 \\pm 2\\sqrt{5}\n$$\nSince $u = \\omega^2$ must be non-negative, we discard the negative root $u = -4 - 2\\sqrt{5}$. The only valid critical point is $u = -4 + 2\\sqrt{5} > 0$. The second derivative test or analysis of the sign of $f'(u)$ confirms this point is a maximum.\nNow we evaluate $f(u)$ at this value. From the equation $u^2 + 8u - 4 = 0$, we have $u^2 = 4 - 8u$. We substitute this into the expression for $f(u)$:\n$$\nf(u_{\\text{max}}) = \\frac{4 + u}{u^2 + 4} = \\frac{4 + u}{(4 - 8u) + 4} = \\frac{4 + u}{8 - 8u} = \\frac{4 + u}{8(1 - u)}\n$$\nSubstituting $u = -4 + 2\\sqrt{5}$:\n$$\nf(u_{\\text{max}}) = \\frac{4 + (-4 + 2\\sqrt{5})}{8(1 - (-4 + 2\\sqrt{5}))} = \\frac{2\\sqrt{5}}{8(5 - 2\\sqrt{5})} = \\frac{\\sqrt{5}}{4(5 - 2\\sqrt{5})}\n$$\nTo simplify, we rationalize the denominator:\n$$\nf(u_{\\text{max}}) = \\frac{\\sqrt{5}}{4(5 - 2\\sqrt{5})} \\cdot \\frac{5 + 2\\sqrt{5}}{5 + 2\\sqrt{5}} = \\frac{\\sqrt{5}(5 + 2\\sqrt{5})}{4(5^2 - (2\\sqrt{5})^2)} = \\frac{5\\sqrt{5} + 10}{4(25 - 20)} = \\frac{5\\sqrt{5} + 10}{4(5)} = \\frac{5(2 + \\sqrt{5})}{20} = \\frac{2 + \\sqrt{5}}{4}\n$$\nThis is the maximum value of $|H(j\\omega)|^2$. The norm $\\|H\\|_{\\infty}$ is the square root of this value:\n$$\n\\|H\\|_{\\infty} = \\sup_{\\omega \\in \\mathbb{R}} |H(j\\omega)| = \\sqrt{\\frac{2 + \\sqrt{5}}{4}} = \\frac{\\sqrt{2 + \\sqrt{5}}}{2}\n$$\nThis is the final exact closed-form expression.", "answer": "$$\n\\boxed{\\frac{\\sqrt{2 + \\sqrt{5}}}{2}}\n$$", "id": "2864797"}, {"introduction": "When analyzing random signals, estimating the power spectral density is a primary objective, and the periodogram is the most direct tool for this task. This practice delves into the statistical DNA of the periodogram by examining its behavior when applied to the most fundamental random signal: Gaussian white noise. By deriving the exact probability distribution of the periodogram values [@problem_id:2864867], you will gain critical insight into the inherent variability of spectral estimates, a cornerstone for correctly interpreting frequency-domain data and designing more robust estimation techniques.", "problem": "Let $\\{x[n]\\}_{n=0}^{N-1}$ be a real-valued, zero-mean, independent and identically distributed Gaussian sequence with variance $\\sigma^{2}$, that is, $x[n] \\sim \\mathcal{N}(0,\\sigma^{2})$ and the samples are independent across $n$. Consider the Discrete Fourier Transform (DFT) defined by $X[k] \\triangleq \\sum_{n=0}^{N-1} x[n] \\exp(-\\mathrm{j} 2\\pi k n / N)$ for integer $k$, and the rectangular-window periodogram at the Fourier frequency $\\omega_{k} \\triangleq 2\\pi k / N$ given by\n$$\nI_{N}(\\omega_{k}) \\triangleq \\frac{1}{N} \\left| X[k] \\right|^{2}.\n$$\nFocus on $k$ such that $1 \\leq k \\leq \\lfloor N/2 \\rfloor - 1$ so that $\\omega_{k} \\in (0,\\pi)$ and avoid the direct current ($k=0$) and Nyquist ($k=N/2$ when $N$ is even) cases.\n\nStarting only from the definitions of the Discrete Fourier Transform, properties of linear transformations of Gaussian vectors, and the definition of the periodogram, derive the probability distribution of $I_{N}(\\omega_{k})$ and justify why $I_{N}(\\omega_{k_{1}})$ and $I_{N}(\\omega_{k_{2}})$ for distinct $k_{1} \\neq k_{2}$ become independent in the sense of asymptotics as $N$ increases. Provide a clear chain of reasoning that connects these foundational definitions to the limiting distribution and the independence statement, explaining any necessary orthogonality and covariance properties that you use. You may assume standard facts about jointly Gaussian random vectors and chi-square distributions, but you must derive any scale factors from first principles.\n\nExpress your final answer as a single closed-form analytical expression for the limiting probability density function (PDF) of $I_{N}(\\omega_{k})$ as a function of a nonnegative scalar argument $i$ and the variance $\\sigma^{2}$. No numerical approximation is required, and no units are needed. The final answer must consist only of this PDF expression.", "solution": "The user has submitted a problem for validation and solution.\n\n**Step 1: Extract Givens**\n-   Signal model: $\\{x[n]\\}_{n=0}^{N-1}$ is a real-valued, zero-mean, independent and identically distributed (i.i.d.) Gaussian sequence.\n-   Distribution of samples: $x[n] \\sim \\mathcal{N}(0,\\sigma^{2})$.\n-   Statistical properties: $E[x[n]] = 0$ and $E[x[n]x[m]] = \\sigma^{2}\\delta[n-m]$, where $\\delta[\\cdot]$ is the Kronecker delta.\n-   Discrete Fourier Transform (DFT): $X[k] \\triangleq \\sum_{n=0}^{N-1} x[n] \\exp(-\\mathrm{j} 2\\pi k n / N)$.\n-   Periodogram: $I_{N}(\\omega_{k}) \\triangleq \\frac{1}{N} |X[k]|^{2}$.\n-   Frequency: $\\omega_{k} \\triangleq 2\\pi k / N$.\n-   Constraint on index $k$: $1 \\leq k \\leq \\lfloor N/2 \\rfloor - 1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is subjected to rigorous validation.\n-   **Scientific Grounding**: The problem is well-grounded in the theory of statistical signal processing. It concerns the statistical properties of the periodogram, a fundamental tool for spectral estimation of random processes. The assumptions (i.i.d. Gaussian noise) are standard in this field.\n-   **Well-Posedness**: The problem is well-posed. The input process is fully specified, the transformations (DFT, periodogram) are mathematically defined, and the task is to derive a specific probability distribution and justify an independence property. The constraints on $k$ are precise and necessary to avoid the special cases of DC ($k=0$) and Nyquist ($k=N/2$) frequencies, which have different statistical properties.\n-   **Objectivity**: The problem is stated in objective, mathematical language, free from ambiguity or subjective content.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or logical contradiction. It is a standard, non-trivial problem in signal processing theory.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\n**Solution Derivation**\n\nThe problem requires the derivation of the probability distribution of the periodogram $I_{N}(\\omega_{k})$ and a justification for the independence of periodogram ordinates at different frequencies.\n\nFirst, we analyze the statistical properties of the DFT coefficients $X[k]$. The DFT is a linear transformation. Since the input sequence $\\{x[n]\\}$ consists of jointly Gaussian random variables, the DFT coefficients $\\{X[k]\\}$ are jointly complex Gaussian random variables.\n\nWe express the complex variable $X[k]$ in terms of its real and imaginary parts.\n$$\nX[k] = \\sum_{n=0}^{N-1} x[n] \\exp(-\\mathrm{j} \\frac{2\\pi kn}{N}) = \\sum_{n=0}^{N-1} x[n] \\left( \\cos\\left(\\frac{2\\pi kn}{N}\\right) - \\mathrm{j} \\sin\\left(\\frac{2\\pi kn}{N}\\right) \\right)\n$$\nLet $X_R[k] = \\text{Re}\\{X[k]\\}$ and $X_I[k] = \\text{Im}\\{X[k]\\}$.\n$$\nX_R[k] = \\sum_{n=0}^{N-1} x[n] \\cos\\left(\\frac{2\\pi kn}{N}\\right)\n$$\n$$\nX_I[k] = -\\sum_{n=0}^{N-1} x[n] \\sin\\left(\\frac{2\\pi kn}{N}\\right)\n$$\nAs $X_R[k]$ and $X_I[k]$ are linear combinations of the Gaussian variables $x[n]$, they are themselves jointly Gaussian random variables. We now determine the parameters of their distribution.\n\nThe mean of $X_R[k]$ and $X_I[k]$ is zero because $E[x[n]]=0$:\n$$\nE[X_R[k]] = \\sum_{n=0}^{N-1} E[x[n]] \\cos\\left(\\frac{2\\pi kn}{N}\\right) = 0\n$$\n$$\nE[X_I[k]] = -\\sum_{n=0}^{N-1} E[x[n]] \\sin\\left(\\frac{2\\pi kn}{N}\\right) = 0\n$$\nNext, we compute their variances and covariance.\nThe variance of $X_R[k]$ is:\n$$\n\\text{Var}(X_R[k]) = E[X_R[k]^2] = E\\left[ \\left( \\sum_{n=0}^{N-1} x[n] \\cos\\left(\\frac{2\\pi kn}{N}\\right) \\right) \\left( \\sum_{m=0}^{N-1} x[m] \\cos\\left(\\frac{2\\pi km}{N}\\right) \\right) \\right]\n$$\n$$\n= \\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} E[x[n]x[m]] \\cos\\left(\\frac{2\\pi kn}{N}\\right) \\cos\\left(\\frac{2\\pi km}{N}\\right)\n$$\nUsing $E[x[n]x[m]] = \\sigma^{2}\\delta[n-m]$, the double summation collapses to a single summation:\n$$\n\\text{Var}(X_R[k]) = \\sum_{n=0}^{N-1} \\sigma^{2} \\cos^{2}\\left(\\frac{2\\pi kn}{N}\\right) = \\sigma^{2} \\sum_{n=0}^{N-1} \\frac{1 + \\cos(4\\pi kn/N)}{2}\n$$\nFor $1 \\leq k \\leq \\lfloor N/2 \\rfloor - 1$, the integer $2k$ is not a multiple of $N$. Therefore, the discrete-time sinusoid has a zero sum over one period: $\\sum_{n=0}^{N-1} \\cos(4\\pi kn/N) = 0$. This gives:\n$$\n\\text{Var}(X_R[k]) = \\sigma^{2} \\frac{N}{2}\n$$\nSimilarly, for the variance of $X_I[k]$:\n$$\n\\text{Var}(X_I[k]) = E[X_I[k]^2] = E\\left[ \\left( -\\sum_{n=0}^{N-1} x[n] \\sin\\left(\\frac{2\\pi kn}{N}\\right) \\right)^2 \\right] = \\sigma^{2} \\sum_{n=0}^{N-1} \\sin^{2}\\left(\\frac{2\\pi kn}{N}\\right)\n$$\n$$\n= \\sigma^{2} \\sum_{n=0}^{N-1} \\frac{1 - \\cos(4\\pi kn/N)}{2} = \\sigma^{2} \\frac{N}{2}\n$$\nThe covariance between $X_R[k]$ and $X_I[k]$ is:\n$$\n\\text{Cov}(X_R[k], X_I[k]) = E[X_R[k]X_I[k]] = E\\left[ \\left( \\sum_{n=0}^{N-1} x[n] \\cos\\left(\\frac{2\\pi kn}{N}\\right) \\right) \\left( -\\sum_{m=0}^{N-1} x[m] \\sin\\left(\\frac{2\\pi km}{N}\\right) \\right) \\right]\n$$\n$$\n= -\\sum_{n=0}^{N-1} \\sigma^{2} \\cos\\left(\\frac{2\\pi kn}{N}\\right) \\sin\\left(\\frac{2\\pi kn}{N}\\right) = -\\frac{\\sigma^{2}}{2} \\sum_{n=0}^{N-1} \\sin\\left(\\frac{4\\pi kn}{N}\\right) = 0\n$$\nSince $X_R[k]$ and $X_I[k]$ are jointly Gaussian and their covariance is zero, they are independent.\n$$\nX_R[k] \\sim \\mathcal{N}\\left(0, \\frac{N\\sigma^{2}}{2}\\right) \\quad \\text{and} \\quad X_I[k] \\sim \\mathcal{N}\\left(0, \\frac{N\\sigma^{2}}{2}\\right)\n$$\nNow we can find the distribution of the periodogram $I_{N}(\\omega_{k}) = \\frac{1}{N}|X[k]|^2 = \\frac{1}{N}(X_R[k]^2 + X_I[k]^2)$.\nLet us define two standardized normal variables $Z_1$ and $Z_2$:\n$$\nZ_1 = \\frac{X_R[k]}{\\sqrt{N\\sigma^{2}/2}} \\sim \\mathcal{N}(0,1) \\quad \\text{and} \\quad Z_2 = \\frac{X_I[k]}{\\sqrt{N\\sigma^{2}/2}} \\sim \\mathcal{N}(0,1)\n$$\n$Z_1$ and $Z_2$ are independent. The periodogram can be expressed as:\n$$\nI_{N}(\\omega_{k}) = \\frac{1}{N} \\left( \\left(\\sqrt{\\frac{N\\sigma^{2}}{2}}Z_1\\right)^2 + \\left(\\sqrt{\\frac{N\\sigma^{2}}{2}}Z_2\\right)^2 \\right) = \\frac{1}{N} \\left( \\frac{N\\sigma^{2}}{2} (Z_1^2 + Z_2^2) \\right) = \\frac{\\sigma^{2}}{2} (Z_1^2 + Z_2^2)\n$$\nThe sum of the squares of two independent standard normal variables follows a chi-squared distribution with two degrees of freedom, $\\chi^2_2$. Thus, $Z_1^2 + Z_2^2 \\sim \\chi^2_2$.\nThe probability density function (PDF) of a $\\chi^2_k$ distribution is $f(y;k) = \\frac{1}{2^{k/2}\\Gamma(k/2)} y^{k/2 - 1} \\exp(-y/2)$ for $y \\ge 0$. For $k=2$:\n$$\nf_{\\chi^2_2}(y) = \\frac{1}{2^{1}\\Gamma(1)} y^{1-1} \\exp(-y/2) = \\frac{1}{2}\\exp(-y/2)\n$$\nThis is an exponential distribution with rate parameter $1/2$.\nThe periodogram $I_{N}(\\omega_{k})$ is a scaled version of this random variable: $I = cY$, where $c = \\sigma^2/2$ and $Y \\sim \\chi^2_2$. The PDF of $I$, let us call it $p_I(i)$, is given by the change of variables formula:\n$$\np_I(i) = \\frac{1}{c} f_{\\chi^2_2}\\left(\\frac{i}{c}\\right) = \\frac{1}{\\sigma^2/2} \\cdot \\frac{1}{2} \\exp\\left(-\\frac{i/\\left(\\sigma^2/2\\right)}{2}\\right) = \\frac{2}{\\sigma^2} \\cdot \\frac{1}{2} \\exp\\left(-\\frac{i}{\\sigma^2}\\right)\n$$\n$$\np_I(i) = \\frac{1}{\\sigma^{2}} \\exp\\left(-\\frac{i}{\\sigma^{2}}\\right), \\quad i \\geq 0\n$$\nThis is the PDF of an exponential distribution with mean $\\sigma^2$. Note that this distribution does not depend on $N$ or $k$ for the specified range of $k$.\n\nNext, we address the independence of $I_{N}(\\omega_{k_{1}})$ and $I_{N}(\\omega_{k_{2}})$ for distinct $k_1, k_2$ in the range $1 \\leq k \\leq \\lfloor N/2 \\rfloor - 1$.\nThe problem states this independence is asymptotic, but for an i.i.d. sequence, it is an exact property for finite $N$. We shall prove this stronger result.\nIndependence of $I_{N}(\\omega_{k_1})$ and $I_{N}(\\omega_{k_2})$ follows from the independence of the vectors $(X_R[k_1], X_I[k_1])$ and $(X_R[k_2], X_I[k_2])$. Since these four variables are jointly Gaussian, we only need to show they are pairwise uncorrelated. We have already shown that $X_R[k]$ and $X_I[k]$ are uncorrelated for the same $k$. We now check the cross-frequency covariances.\n$$\nE[X_R[k_1]X_R[k_2]] = \\sum_{n=0}^{N-1} \\sigma^2 \\cos\\left(\\frac{2\\pi k_1 n}{N}\\right) \\cos\\left(\\frac{2\\pi k_2 n}{N}\\right)\n$$\n$$\n= \\frac{\\sigma^2}{2} \\sum_{n=0}^{N-1} \\left[ \\cos\\left(\\frac{2\\pi (k_1-k_2) n}{N}\\right) + \\cos\\left(\\frac{2\\pi (k_1+k_2) n}{N}\\right) \\right]\n$$\nFor $k_1 \\neq k_2$ in the given range, neither $k_1-k_2$ nor $k_1+k_2$ is a non-zero multiple of $N$. Therefore, both sums are zero due to the orthogonality property of discrete-time sinusoids. $E[X_R[k_1]X_R[k_2]] = 0$.\nBy identical reasoning, using the trigonometric identity for $\\sin A \\sin B$:\n$$\nE[X_I[k_1]X_I[k_2]] = \\frac{\\sigma^2}{2} \\sum_{n=0}^{N-1} \\left[ \\cos\\left(\\frac{2\\pi (k_1-k_2) n}{N}\\right) - \\cos\\left(\\frac{2\\pi (k_1+k_2) n}{N}\\right) \\right] = 0\n$$\nAnd for the cross-component covariance, using the identity for $\\cos A \\sin B$:\n$$\nE[X_R[k_1]X_I[k_2]] = -\\frac{\\sigma^2}{2} \\sum_{n=0}^{N-1} \\left[ \\sin\\left(\\frac{2\\pi (k_1+k_2) n}{N}\\right) - \\sin\\left(\\frac{2\\pi (k_2-k_1) n}{N}\\right) \\right] = 0\n$$\nAll cross-covariance terms between the components at frequency $k_1$ and those at $k_2$ are zero. This establishes that the random vector $(X_R[k_1], X_I[k_1])$ is independent of $(X_R[k_2], X_I[k_2])$. Since $I_{N}(\\omega_{k_1})$ is a function of the first vector and $I_{N}(\\omega_{k_2})$ is a function of the second, $I_{N}(\\omega_{k_1})$ and $I_{N}(\\omega_{k_2})$ are independent for $k_1 \\ne k_2$.\nThis result is exact for any finite $N$. The asymptotic independence mentioned in the problem is a more general result that holds for a broader class of stationary processes, not just i.i.d. sequences. For the i.i.d. case, the orthogonality is exact, not asymptotic.\n\nThe final answer required is the PDF of $I_N(\\omega_k)$.", "answer": "$$\n\\boxed{\\frac{1}{\\sigma^{2}} \\exp\\left(-\\frac{i}{\\sigma^{2}}\\right)}\n$$", "id": "2864867"}, {"introduction": "Parametric modeling, such as using an autoregressive (AR) process, is a powerful approach for representing random signals, but it poses a critical question: how complex should the model be? This exercise tackles the crucial problem of model order selection by applying Akaike's Information Criterion (AIC), a foundational concept that balances goodness-of-fit with model parsimony. By first deriving the AIC rule for AR models and then implementing it to test its performance on synthetic data [@problem_id:2864830], you will develop both the theoretical understanding and practical programming skills needed for rigorous time series modeling.", "problem": "You are asked to formalize a model order selection rule for AutoRegressive (AR) models using Akaike’s Information Criterion (AIC) and to implement a program that applies this rule to synthetic data with known ground truth. The problem must be solved from first principles by starting from core definitions, and it is entirely self-contained.\n\nThe fundamental setting is the following. A zero-mean AutoRegressive model of order $k$ (AR($k$)) is defined by the recursion\n$$\nx_t = \\sum_{i=1}^{k} a_i\\, x_{t-i} + e_t,\n$$\nwhere $\\{e_t\\}$ is an independent and identically distributed sequence of Gaussian random variables with zero mean and variance $\\sigma^2$, written as $e_t \\sim \\mathcal{N}(0,\\sigma^2)$. For such a model, assume that the data $\\{x_t\\}_{t=1}^{n}$ are generated by a stationary AR process.\n\nAkaike’s Information Criterion (AIC) is defined for a parametric model with parameter vector of dimension $p$ and maximized likelihood $L(\\hat{\\theta})$ as\n$$\n\\mathrm{AIC} = 2p - 2 \\ln L(\\hat{\\theta}).\n$$\nYou must start from this definition and the Gaussian likelihood of the AR model to derive a computable order selection rule that depends only on the sample size $n$, the candidate order $k$, and an estimate of the innovation variance $\\hat{\\sigma}_k^2$ obtained after fitting an AR($k$) model to the data. Your derivation must make explicit which terms depend on $k$ and which do not, and it must justify the final simplified form used for comparing different $k$.\n\nFor parameter estimation at each candidate order $k$, you must use the Yule–Walker (YW) equations solved from the biased sample autocorrelation estimates\n$$\n\\hat{r}(\\ell) = \\frac{1}{n}\\sum_{t=\\ell+1}^{n} x_t \\, x_{t-\\ell}, \\quad \\ell = 0,1,\\dots,k,\n$$\nto obtain the AR coefficient vector $\\hat{\\mathbf{a}}^{(k)} = [\\hat{a}_1,\\dots,\\hat{a}_k]^\\top$ as the solution of the Toeplitz linear system\n$$\n\\mathbf{R}_k \\, \\hat{\\mathbf{a}}^{(k)} = \\mathbf{r}_k,\n$$\nwhere $\\mathbf{R}_k$ is the $k\\times k$ Toeplitz matrix with entries $[\\mathbf{R}_k]_{i,j} = \\hat{r}(|i-j|)$ and $\\mathbf{r}_k = [\\hat{r}(1), \\dots, \\hat{r}(k)]^\\top$. The corresponding Yule–Walker estimate of the innovation variance is\n$$\n\\hat{\\sigma}_k^2 = \\hat{r}(0) - \\big(\\hat{\\mathbf{a}}^{(k)}\\big)^\\top \\mathbf{r}_k,\n$$\nwith the convention for $k=0$ that $\\hat{\\sigma}_0^2 = \\hat{r}(0)$.\n\nYour tasks:\n- Derive, from the definition of Akaike’s Information Criterion and the Gaussian likelihood of the AR model, a $k$-dependent selection rule that yields a computable score $\\mathrm{AIC}(k)$ based on $n$, $k$, and $\\hat{\\sigma}_k^2$. The derivation must begin from the likelihood and the definition of AIC and justify any constants that can be dropped without affecting order selection.\n- Implement a program that:\n  1. Generates synthetic zero-mean AR($k$) time series from specified ground-truth parameters, using a Gaussian innovation sequence of variance $\\sigma^2$ and a burn-in to reach stationarity.\n  2. Fits AR($k$) models for candidate orders $k$ from $0$ up to a specified maximum using the Yule–Walker method and computes the AIC-based score for each $k$ using your derived rule.\n  3. Selects the order $\\hat{k}$ that minimizes the AIC score. In the event of ties to within numerical tolerance, choose the smallest such $k$.\n- Data generation details:\n  - The AR recursion must be applied as given above, with $x_t$ computed causally from past samples and independent Gaussian noise.\n  - For the case where parameters are specified as “poles” $r_1,\\dots,r_p$ with $|r_i|<1$, form the AR polynomial\n    $$\n    A(z) = \\prod_{i=1}^{p} \\big(1 - r_i z^{-1}\\big) = 1 - a_1 z^{-1} - a_2 z^{-2} - \\cdots - a_p z^{-p},\n    $$\n    and extract the AR coefficients via the elementary symmetric sums so that $a_1 = \\sum_i r_i$, $a_2 = -\\sum_{i<j} r_i r_j$, $a_3 = \\sum_{i<j<k} r_i r_j r_k$, and in general signs alternate, i.e., $a_m = (-1)^{m+1}\\,S_m$ where $S_m$ is the $m$-th elementary symmetric sum of $\\{r_i\\}$.\n- Angle units are not applicable. There are no physical units; all outputs are unitless.\n\nTest suite and required inputs to your program:\n- For each test, you must simulate a time series of length $n$ after discarding a burn-in segment of length $B$. Use $B = 500$ in all tests.\n- Use a fixed random seed for each test for reproducibility. The Gaussian innovation variance is $\\sigma^2$ as specified. Unless otherwise noted, use $\\sigma^2 = 1$.\n- For each test, search over candidate orders $k \\in \\{0,1,2,\\dots,k_{\\max}\\}$ and report the selected order $\\hat{k}$.\n\nProvide the following six tests:\n1. Test A (happy path): Ground truth AR($2$) with coefficients $[0.7, -0.4]$, $n=2000$, $\\sigma^2=1$, seed $123$, $k_{\\max}=7$.\n2. Test B (moderate sample): Ground truth AR($1$) with coefficients $[0.8]$, $n=300$, $\\sigma^2=1$, seed $456$, $k_{\\max}=7$.\n3. Test C (white noise): Ground truth AR($0$), i.e., $x_t = e_t$, $n=1000$, $\\sigma^2=1$, seed $789$, $k_{\\max}=7$.\n4. Test D (higher order with pole specification): Ground truth AR($4$) specified by poles $[0.8, 0.5, -0.3, 0.2]$ (all magnitudes strictly less than $1$), $n=4000$, $\\sigma^2=1$, seed $101112$, $k_{\\max}=10$. The AR coefficients are defined implicitly via $A(z)=\\prod_i (1 - r_i z^{-1})$ as above.\n5. Test E (near-boundary dynamics, small $n$): Ground truth AR($2$) with coefficients $[0.95, -0.3]$, $n=120$, $\\sigma^2=1$, seed $202122$, $k_{\\max}=6$.\n6. Test F (search boundary case): Ground truth AR($3$) with coefficients $[0.6, -0.2, 0.1]$, $n=500$, $\\sigma^2=1$, seed $303132$, $k_{\\max}=2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the six selected orders $\\hat{k}$ for Tests A–F, each as an integer, for example, $[2,1,0,4,2,2]$. The output must contain only this list and no additional text.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, and self-contained. It presents a standard, albeit rigorous, task in time series analysis: the selection of an AutoRegressive (AR) model order using Akaike's Information Criterion (AIC). All definitions, parameters, and procedures are specified without ambiguity. The problem is therefore deemed **valid**.\n\nThe solution proceeds in two parts. First, a formal derivation of the AIC-based selection rule from first principles, as required. Second, an explanation of the methodology for the numerical implementation that adheres to the provided specifications.\n\n**Part 1: Derivation of the AIC Selection Rule for AR Models**\n\nThe objective is to derive a computable formula for AIC for an AR($k$) model, starting from its definition and the statistical properties of the model.\n\n1.  **AR Model and Gaussian Log-Likelihood**\n    A zero-mean AR process of order $k$ is defined by the stochastic difference equation:\n    $$x_t = \\sum_{i=1}^{k} a_i x_{t-i} + e_t$$\n    where $\\{e_t\\}$ is a sequence of independent and identically distributed (i.i.d.) random variables from a Gaussian distribution with zero mean and variance $\\sigma^2$, denoted $e_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n    For a given set of observations $\\{x_t\\}_{t=1}^n$, the likelihood function is the joint probability density function of the data, viewed as a function of the model parameters $\\theta = (\\mathbf{a}, \\sigma^2)$, where $\\mathbf{a} = [a_1, \\dots, a_k]^\\top$. A common and practical approach is to use the conditional likelihood, which is conditioned on the first $k$ observations. This simplifies the problem by treating $x_1, \\dots, x_k$ as fixed. The innovation term can be expressed as $e_t = x_t - \\sum_{i=1}^{k} a_i x_{t-i}$. The conditional probability density of $x_t$ given the past is that of a Gaussian random variable with mean $\\mu_t = \\sum_{i=1}^{k} a_i x_{t-i}$ and variance $\\sigma^2$.\n\n    The conditional log-likelihood function for the observations $x_{k+1}, \\dots, x_n$ is:\n    $$\\ln L(\\mathbf{a}, \\sigma^2) = \\sum_{t=k+1}^{n} \\ln p(x_t | x_{t-1}, \\dots, x_{t-k})$$\n    $$ \\ln L(\\mathbf{a}, \\sigma^2) = \\sum_{t=k+1}^{n} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{\\left(x_t - \\sum_{i=1}^{k} a_i x_{t-i}\\right)^2}{2\\sigma^2} \\right] $$\n    $$ \\ln L(\\mathbf{a}, \\sigma^2) = -\\frac{n-k}{2}\\ln(2\\pi) - \\frac{n-k}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=k+1}^{n} e_t^2 $$\n    For large sample sizes ($n \\gg k$), it is standard practice to approximate the number of effective observations $n-k$ by $n$. This yields the large-sample approximate log-likelihood:\n    $$ \\ln L(\\mathbf{a}, \\sigma^2) \\approx -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=1}^{n} e_t^2 $$\n\n2.  **Maximized Log-Likelihood**\n    Akaike's criterion requires the maximized log-likelihood, $\\ln L(\\hat{\\theta})$, where $\\hat{\\theta}$ represents the maximum likelihood estimates (MLEs) of the parameters. The parameter vector for an AR($k$) model is $\\theta = (a_1, \\dots, a_k, \\sigma^2)$, so the total number of parameters is $p = k+1$.\n\n    The MLE for the variance, $\\hat{\\sigma}_k^2$, which maximizes the approximate log-likelihood, is the average of the squared residuals:\n    $$ \\hat{\\sigma}_k^2 = \\frac{1}{n}\\sum_{t=1}^{n} \\hat{e}_t^2 = \\frac{1}{n}\\sum_{t=1}^{n} \\left(x_t - \\sum_{i=1}^{k} \\hat{a}_i x_{t-i}\\right)^2 $$\n    Here, $\\hat{a}_i$ are the estimated AR coefficients. The problem specifies that these parameters, and subsequently $\\hat{\\sigma}_k^2$, are to be obtained via the Yule-Walker equations. Substituting the MLE $\\hat{\\sigma}_k^2$ back into the log-likelihood expression gives:\n    $$ \\ln L(\\hat{\\theta}) \\approx -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\hat{\\sigma}_k^2) - \\frac{1}{2\\hat{\\sigma}_k^2} (n \\hat{\\sigma}_k^2) $$\n    $$ \\ln L(\\hat{\\theta}) \\approx -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln(\\hat{\\sigma}_k^2) + 1 \\right) $$\n\n3.  **Akaike's Information Criterion Expression**\n    The definition of AIC is $\\mathrm{AIC} = 2p - 2\\ln L(\\hat{\\theta})$. Substituting $p = k+1$ and the expression for $\\ln L(\\hat{\\theta})$:\n    $$ \\mathrm{AIC}(k) = 2(k+1) - 2 \\left[ -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln(\\hat{\\sigma}_k^2) + 1 \\right) \\right] $$\n    $$ \\mathrm{AIC}(k) = 2(k+1) + n\\ln(2\\pi) + n\\ln(\\hat{\\sigma}_k^2) + n $$\n\n4.  **Final Computable Form for Order Selection**\n    The optimal model order $\\hat{k}$ is found by minimizing $\\mathrm{AIC}(k)$ over a set of candidate orders. When comparing different values of $k$, terms in the AIC expression that do not depend on $k$ can be discarded, as they do not influence the location of the minimum. The terms $n\\ln(2\\pi)$ and $n$ are constant for a given dataset of size $n$. Dropping these constant terms yields a simplified, but equivalent, criterion for model selection, which we will denote $\\mathrm{AIC}(k)$:\n    $$ \\mathrm{AIC}(k) = n\\ln(\\hat{\\sigma}_k^2) + 2(k+1) $$\n    This is the final, computable form of the selection rule. For a given candidate order $k$, it requires only the sample size $n$ and the estimated innovation variance $\\hat{\\sigma}_k^2$. The optimal order $\\hat{k}$ is selected as:\n    $$ \\hat{k} = \\arg\\min_{k \\in \\{0, 1, \\dots, k_{\\max}\\}} \\left\\{ n\\ln(\\hat{\\sigma}_k^2) + 2(k+1) \\right\\} $$\n    For the case $k=0$ (white noise), the model is $x_t = e_t$, and the only parameter is $\\sigma^2$, so $p=1$. The variance estimate is $\\hat{\\sigma}_0^2 = \\hat{r}(0)$, and the formula correctly evaluates to $\\mathrm{AIC}(0) = n \\ln(\\hat{\\sigma}_0^2) + 2$.\n\n**Part 2: Methodology of Numerical Implementation**\n\nThe program implements the derived rule by executing the following steps for each test case.\n\n1.  **Data Generation**:\n    -   A time series of length $n+B$ is generated, where $n$ is the final sample size and $B=500$ is the burn-in period to ensure the process reaches a stationary state.\n    -   First, an i.i.d. Gaussian noise sequence $\\{e_t\\}$ of length $n+B$ with variance $\\sigma^2$ is produced using a specific random seed for reproducibility.\n    -   The AR time series $\\{x_t\\}$ is then generated via the causal recursion $x_t = \\sum_{i=1}^{k} a_i x_{t-i} + e_t$. Initial values $x_t$ for $t \\le 0$ are taken as zero.\n    -   The first $B$ samples are discarded, yielding the final time series of length $n$.\n    -   For tests where AR parameters are given as poles $\\{r_i\\}$, the AR coefficients $\\{a_m\\}$ are first computed. The characteristic polynomial is $A(z) = \\prod_i (1 - r_i z^{-1}) = 1 - \\sum_m a_m z^{-m}$. The coefficients are found via $a_m = -c_m$ where $c_m$ are the coefficients of the polynomial $\\prod_i(z-r_i)$ (excluding the leading term). This is efficiently implemented using `numpy.poly`.\n\n2.  **Yule-Walker Parameter Estimation**:\n    -   For each candidate order $k \\in \\{0, 1, \\dots, k_{\\max}\\}$, the AR model parameters are estimated from the synthetic data.\n    -   The biased sample autocorrelations $\\hat{r}(\\ell)$ for lags $\\ell=0, \\dots, k$ are computed as specified: $\\hat{r}(\\ell) = \\frac{1}{n} \\sum_{t=\\ell+1}^{n} x_t x_{t-\\ell}$.\n    -   For $k>0$, the $k \\times k$ Toeplitz matrix $\\mathbf{R}_k$ with entries $[\\mathbf{R}_k]_{i,j} = \\hat{r}(|i-j|)$ and the vector $\\mathbf{r}_k = [\\hat{r}(1), \\dots, \\hat{r}(k)]^\\top$ are constructed.\n    -   The system of Yule-Walker equations $\\mathbf{R}_k \\hat{\\mathbf{a}}^{(k)} = \\mathbf{r}_k$ is solved for the AR coefficient vector $\\hat{\\mathbf{a}}^{(k)}$.\n    -   The innovation variance is estimated using the formula $\\hat{\\sigma}_k^2 = \\hat{r}(0) - (\\hat{\\mathbf{a}}^{(k)})^\\top \\mathbf{r}_k$.\n    -   For the base case $k=0$, the estimate is simply $\\hat{\\sigma}_0^2 = \\hat{r}(0)$.\n\n3.  **AIC-Based Order Selection**:\n    -   For each candidate order $k$, the score $\\mathrm{AIC}(k) = n\\ln(\\hat{\\sigma}_k^2) + 2(k+1)$ is computed.\n    -   The order $\\hat{k}$ that yields the minimum AIC score is selected. A strict inequality is used for comparison, which naturally handles ties by selecting the smallest order $k$ that achieves the minimum, as it is encountered first in the ascending loop over $k$.\n\nThis structured approach ensures a correct and reproducible implementation of the specified model selection task.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\nimport collections\n\n# Define the structure for a test case for clarity.\nTestCase = collections.namedtuple(\n    'TestCase',\n    ['name', 'params', 'n', 'sigma_sq', 'seed', 'k_max']\n)\n\ndef poles_to_coeffs(poles: list[float]) -> np.ndarray:\n    \"\"\"\n    Converts AR model poles to AR coefficients.\n    The AR polynomial is defined as A(z) = product(1 - r_i * z^-1) = 1 - sum(a_k * z^-k).\n    The roots of the polynomial P(x) = product(x - r_i) are the poles r_i.\n    np.poly(poles) returns coefficients [1, c_1, ..., c_p] of P(x).\n    It can be shown that the AR coefficients are a_k = -c_k.\n    \"\"\"\n    if not poles:\n        return np.array([])\n    poly_coeffs = np.poly(poles)\n    # The first coefficient from np.poly is 1, corresponding to the highest power.\n    # The rest are c_1, c_2, ..., c_p. We negate them to get a_1, a_2, ..., a_p.\n    return -poly_coeffs[1:]\n\ndef generate_ar_data(\n    coeffs: np.ndarray, n: int, sigma_sq: float, seed: int, B: int\n) -> np.ndarray:\n    \"\"\"\n    Generates a synthetic AR(k) time series.\n    \n    Args:\n        coeffs: Ground-truth AR coefficients [a_1, a_2, ...].\n        n: Length of the final time series.\n        sigma_sq: Variance of the Gaussian innovation noise.\n        seed: Random seed for reproducibility.\n        B: Length of the burn-in period to discard.\n    \n    Returns:\n        A numpy array of length n representing the AR time series.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    total_len = n + B\n    noise = rng.normal(loc=0.0, scale=np.sqrt(sigma_sq), size=total_len)\n    x = np.zeros(total_len)\n    \n    k = len(coeffs)\n    if k == 0:\n        x = noise\n    else:\n        for t in range(k, total_len):\n            # x[t-k:t] gives [x_{t-k}, ..., x_{t-1}]\n            # We want dot product <a_1,...a_k>, <x_{t-1},...,x_{t-k}>\n            past_values = x[t-k:t]\n            x[t] = np.dot(coeffs, past_values[::-1]) + noise[t]\n            \n    return x[B:]\n\ndef yule_walker_fit(x: np.ndarray, k: int) -> tuple[np.ndarray, float]:\n    \"\"\"\n    Fits an AR(k) model using the Yule-Walker equations with biased autocorrelation.\n    \n    Args:\n        x: The time series data.\n        k: The candidate AR model order.\n    \n    Returns:\n        A tuple containing the estimated AR coefficients and the estimated innovation variance.\n    \"\"\"\n    n = len(x)\n    \n    if k == 0:\n        # For AR(0), variance is simply the mean square value.\n        r0 = np.dot(x, x) / n\n        return np.array([]), r0\n        \n    # Compute biased sample autocorrelation for lags 0 to k\n    r_hat = np.zeros(k + 1)\n    r_hat[0] = np.dot(x, x) / n\n    for l in range(1, k + 1):\n        r_hat[l] = np.dot(x[l:], x[:-l]) / n\n    \n    # Construct the Toeplitz matrix R_k and vector r_k\n    R_k_col = r_hat[:k]  # First column of R_k is [r(0), r(1), ..., r(k-1)]\n    R_k = toeplitz(R_k_col)\n    r_k_vec = r_hat[1:k+1] # Vector r_k is [r(1), ..., r(k)]\n    \n    # Solve the Yule-Walker equations: R_k * a_hat = r_k\n    # A singular matrix can occur for highly periodic or pathological data.\n    try:\n        a_hat = np.linalg.solve(R_k, r_k_vec)\n    except np.linalg.LinAlgError:\n        # If solver fails, return non-finite variance to avoid selection.\n        return np.array([np.nan]*k), np.inf\n        \n    # Estimate innovation variance: sigma_k^2 = r(0) - a_hat^T * r_k\n    sigma_sq_hat = r_hat[0] - np.dot(a_hat, r_k_vec)\n    \n    return a_hat, sigma_sq_hat\n\ndef select_ar_order(test_case: TestCase, B: int) -> int:\n    \"\"\"\n    Selects the best AR model order for a given test case using AIC.\n    \"\"\"\n    # 1. Unpack parameters and generate coefficients if needed\n    if 'poles' in test_case.params:\n        coeffs = poles_to_coeffs(test_case.params['poles'])\n    else:\n        coeffs = np.array(test_case.params.get('coeffs', []))\n        \n    # 2. Generate synthetic data\n    x = generate_ar_data(\n        coeffs, test_case.n, test_case.sigma_sq, test_case.seed, B\n    )\n    \n    # 3. Iterate through candidate orders and compute AIC\n    min_aic = np.inf\n    best_k = -1\n\n    for k in range(test_case.k_max + 1):\n        # Fit AR(k) model\n        _, sigma_sq_hat = yule_walker_fit(x, k)\n        \n        # Check for valid variance estimate\n        if sigma_sq_hat <= 0 or not np.isfinite(sigma_sq_hat):\n            continue\n\n        # Compute AIC: AIC(k) = n*ln(sigma_k^2) + 2*(k+1)\n        aic = test_case.n * np.log(sigma_sq_hat) + 2 * (k + 1)\n\n        if aic < min_aic:\n            min_aic = aic\n            best_k = k\n            \n    return best_k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    # Global burn-in period\n    B = 500\n\n    test_cases = [\n        TestCase('A', {'coeffs': [0.7, -0.4]}, n=2000, sigma_sq=1, seed=123, k_max=7),\n        TestCase('B', {'coeffs': [0.8]}, n=300, sigma_sq=1, seed=456, k_max=7),\n        TestCase('C', {'coeffs': []}, n=1000, sigma_sq=1, seed=789, k_max=7),\n        TestCase('D', {'poles': [0.8, 0.5, -0.3, 0.2]}, n=4000, sigma_sq=1, seed=101112, k_max=10),\n        TestCase('E', {'coeffs': [0.95, -0.3]}, n=120, sigma_sq=1, seed=202122, k_max=6),\n        TestCase('F', {'coeffs': [0.6, -0.2, 0.1]}, n=500, sigma_sq=1, seed=303132, k_max=2),\n    ]\n\n    results = []\n    for case in test_cases:\n        selected_order = select_ar_order(case, B)\n        results.append(selected_order)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2864830"}]}