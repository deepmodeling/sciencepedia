{"hands_on_practices": [{"introduction": "The cornerstone of s-domain analysis is the transformation of a system's time-domain behavior into an algebraic representation. This first exercise grounds our practice in this fundamental process, starting with the impulse response $h(t)$, which is the system's unique signature in the time domain. By applying the unilateral Laplace transform, you will derive the system's transfer function $H(s)$ and identify its poles and zeros, translating a description based on differential equations and distributions into a rational function in the complex variable $s$ [@problem_id:2880787].", "problem": "Consider a causal linear, time-invariant (LTI) system with impulse response given by the tempered distribution $$h(t)=\\delta(t)+2 \\exp(-3 t)\\,u(t),$$ where $\\delta(t)$ is the Dirac delta distribution and $u(t)$ is the unit step function. Let the system function (transfer function) be defined by the unilateral Laplace transform $$H(s)=\\int_{0^{-}}^{\\infty} h(t)\\,\\exp(-s t)\\,dt,$$ for complex $s$ in the region where this integral converges in the sense of distributions. Starting from the definition of the Laplace transform, together with the defining properties of $\\delta(t)$ as a distribution and $u(t)$ as an indicator of support on $[0,\\infty)$, derive a closed-form expression for $H(s)$ valid on its region of convergence, and then identify all finite poles and zeros of $H(s)$ with their multiplicities. Provide your final answer as a single row matrix with entries in the order: $H(s)$, the list of zeros (each repeated by multiplicity, in any order), and the list of poles (each repeated by multiplicity, in any order). No numerical rounding is required, and no physical units are involved.", "solution": "We begin from first principles. The unilateral Laplace transform of an impulse response $h(t)$ is defined as\n$$\nH(s)=\\int_{0^{-}}^{\\infty} h(t)\\,\\exp(-s t)\\,dt,\n$$\nfor complex $s$ in the region where the integral converges. For the given $h(t)=\\delta(t)+2 \\exp(-3 t)\\,u(t)$, linearity of the integral yields\n$$\nH(s)=\\int_{0^{-}}^{\\infty} \\delta(t)\\,\\exp(-s t)\\,dt+2\\int_{0}^{\\infty} \\exp(-3 t)\\,u(t)\\,\\exp(-s t)\\,dt.\n$$\nWe evaluate each term using foundational properties.\n\nFor the Dirac delta distribution, the defining sifting property against a test function $\\varphi(t)$ is\n$$\n\\int_{-\\infty}^{\\infty} \\delta(t)\\,\\varphi(t)\\,dt=\\varphi(0).\n$$\nSince the unilateral Laplace integral starts at $0^{-}$, the contribution of $\\delta(t)$ at $t=0$ is included, hence\n$$\n\\int_{0^{-}}^{\\infty} \\delta(t)\\,\\exp(-s t)\\,dt=\\exp(0)=1,\n$$\nfor all complex $s$.\n\nFor the second term, $u(t)$ confines the integration to $t\\geq 0$, so\n$$\n2\\int_{0}^{\\infty} \\exp(-3 t)\\,u(t)\\,\\exp(-s t)\\,dt=2\\int_{0}^{\\infty} \\exp\\!\\big(-(s+3)t\\big)\\,dt.\n$$\nThis is a standard improper integral that converges if and only if $\\Re(s+3)0$, that is, $\\Re(s)-3$. Under this condition, we evaluate\n$$\n\\int_{0}^{\\infty} \\exp\\!\\big(-(s+3)t\\big)\\,dt=\\left[-\\frac{1}{s+3}\\exp\\!\\big(-(s+3)t\\big)\\right]_{t=0}^{t=\\infty}=\\frac{1}{s+3}.\n$$\nTherefore, for $\\Re(s)-3$, we obtain\n$$\nH(s)=1+2\\,\\frac{1}{s+3}=\\frac{s+3}{s+3}+\\frac{2}{s+3}=\\frac{s+5}{s+3}.\n$$\nThe expression $\\dfrac{s+5}{s+3}$ is a rational function that is analytic on $\\mathbb{C}\\setminus\\{-3\\}$, with a simple pole at $s=-3$. The region of convergence for the unilateral Laplace transform is $\\Re(s)-3$, which is consistent with the pole location being on the boundary of that half-plane.\n\nTo identify poles and zeros with multiplicities, we factor the numerator and denominator. The numerator is $s+5$, which vanishes at $s=-5$ with multiplicity $1$. The denominator is $s+3$, which vanishes at $s=-3$ with multiplicity $1$ and produces a simple pole of $H(s)$ at that location. There are no common factors between numerator and denominator, hence there are no pole-zero cancellations.\n\nIn summary, the transfer function is\n$$\nH(s)=\\frac{s+5}{s+3}\\quad\\text{for}\\quad \\Re(s)-3,\n$$\nwith a single zero at $s=-5$ of multiplicity $1$ and a single pole at $s=-3$ of multiplicity $1$.\n\nPer the requested final answer format, we present the row matrix containing $H(s)$, followed by the list of zeros (each repeated by multiplicity), and the list of poles (each repeated by multiplicity).", "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{s+5}{s+3}  -5  -3\\end{pmatrix}}$$", "id": "2880787"}, {"introduction": "With the transfer function in hand, our primary analytical task is often to assess system stability. While finding the exact pole locations by factoring the denominator polynomial can be difficult for high-order systems, the Routh-Hurwitz stability criterion provides a powerful shortcut. This practice guides you through the construction of a Routh array, a systematic method that allows you to determine the number of unstable poles (those in the right-half plane) simply by inspecting the signs of the coefficients in the characteristic polynomial [@problem_id:2880788].", "problem": "Consider a causal, single-input single-output linear time-invariant (LTI) system with real-rational transfer function $G(s) = \\frac{N(s)}{D(s)}$, where the characteristic polynomial (the denominator) is given by\n$$\nD(s) \\;=\\; s^{4} \\;+\\; 2 s^{3} \\;+\\; 3 s^{2} \\;+\\; 2 s \\;+\\; 1.\n$$\nStarting from first principles that internal stability for an LTI system is determined entirely by the locations of the poles (the roots of $D(s)$) in the complex plane, and that the Routh–Hurwitz stability criterion provides the count of roots with positive real parts without computing the roots explicitly by constructing a Routh array from the coefficients of $D(s)$, do the following:\n- Construct the Routh array associated with $D(s)$ from its coefficients.\n- From the first-column signs, deduce the number of roots of $D(s)$ with strictly positive real parts.\n\nDo not compute any roots of $D(s)$ explicitly. Report as your final answer a single integer equal to the number of roots of $D(s)$ in the open right-half plane. No rounding is needed and no units are required.", "solution": "The problem requires the determination of the number of roots of the characteristic polynomial $D(s)$ with strictly positive real parts. The specified method is the application of the Routh-Hurwitz stability criterion, which relates the number of these roots to the number of sign changes in the first column of the Routh array constructed from the polynomial's coefficients.\n\n**Step 1: Problem Validation**\nThe givens are:\n- A causal, single-input single-output linear time-invariant (LTI) system.\n- A real-rational transfer function $G(s) = \\frac{N(s)}{D(s)}$.\n- The characteristic polynomial is $D(s) = s^{4} + 2 s^{3} + 3 s^{2} + 2 s + 1$.\nThe problem is to construct the Routh array for $D(s)$ and determine the number of its roots in the open right-half complex plane.\n\nThe problem is scientifically grounded, being a standard application of the Routh-Hurwitz criterion in LTI system analysis. It is well-posed, as the polynomial is explicitly given and the procedure for constructing the Routh array is deterministic, leading to a unique result. The problem statement is objective and contains all necessary information. It is therefore deemed valid.\n\n**Step 2: Solution Construction**\nThe characteristic polynomial is of degree $n=4$:\n$$D(s) = a_{4}s^{4} + a_{3}s^{3} + a_{2}s^{2} + a_{1}s + a_{0}$$\nThe coefficients are $a_{4}=1$, $a_{3}=2$, $a_{2}=3$, $a_{1}=2$, and $a_{0}=1$.\n\nThe Routh array is constructed sequentially. The first two rows are populated with the coefficients of $D(s)$.\nThe first row, for the $s^4$ term, uses coefficients $a_4, a_2, a_0$:\n$$\ns^4 \\quad | \\quad 1 \\quad 3 \\quad 1\n$$\nThe second row, for the $s^3$ term, uses coefficients $a_3, a_1$:\n$$\ns^3 \\quad | \\quad 2 \\quad 2 \\quad 0\n$$\nSubsequent rows are calculated based on the two immediately preceding rows.\nThe elements of the $s^2$ row, denoted as $b_1, b_2, \\dots$, are calculated as follows:\n$$\nb_1 = \\frac{a_3 a_2 - a_4 a_1}{a_3} = \\frac{(2)(3) - (1)(2)}{2} = \\frac{6-2}{2} = 2\n$$\n$$\nb_2 = \\frac{a_3 a_0 - a_4 \\cdot 0}{a_3} = \\frac{(2)(1) - (1)(0)}{2} = \\frac{2-0}{2} = 1\n$$\nThus, the $s^2$ row is:\n$$\ns^2 \\quad | \\quad 2 \\quad 1 \\quad 0\n$$\nThe elements of the $s^1$ row, denoted as $c_1, c_2, \\dots$, are calculated from the $s^3$ row (with coefficients $a_3=2, a_1=2$) and the $s^2$ row (with coefficients $b_1=2, b_2=1$):\n$$\nc_1 = \\frac{b_1 a_1 - a_3 b_2}{b_1} = \\frac{(2)(2) - (2)(1)}{2} = \\frac{4-2}{2} = 1\n$$\n$$\nc_2 = \\frac{b_1 \\cdot 0 - a_3 \\cdot 0}{b_1} = \\frac{(2)(0) - (2)(0)}{2} = 0\n$$\nThus, the $s^1$ row is:\n$$\ns^1 \\quad | \\quad 1 \\quad 0 \\quad 0\n$$\nFinally, the element of the $s^0$ row, denoted as $d_1$, is calculated from the $s^2$ and $s^1$ rows:\n$$\nd_1 = \\frac{c_1 b_2 - b_1 c_2}{c_1} = \\frac{(1)(1) - (2)(0)}{1} = \\frac{1-0}{1} = 1\n$$\nThus, the $s^0$ row is:\n$$\ns^0 \\quad | \\quad 1\n$$\nThe construction did not encounter any special cases, such as a zero first-column element or an entire row of zeros.\n\nThe complete Routh array is assembled as:\n$$\n\\begin{array}{c|ccc}\ns^4  1  3  1 \\\\\ns^3  2  2  0 \\\\\ns^2  2  1  0 \\\\\ns^1  1  0  \\\\\ns^0  1  \n\\end{array}\n$$\n\n**Step 3: Conclusion from the Routh Array**\nWe examine the signs of the elements in the first column of the array:\n$$\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 1\n\\end{pmatrix}\n$$\nAll elements in the first column are positive. There are no sign changes in this column. According to the Routh-Hurwitz criterion, the number of roots of the polynomial with strictly positive real parts is equal to the number of sign changes in the first column.\n\nSince there are zero sign changes, the polynomial $D(s)$ has zero roots in the open right-half of the complex plane.", "answer": "$$\\boxed{0}$$", "id": "2880788"}, {"introduction": "The transfer function superbly describes a system's input-output relationship, but does it tell the whole story about the system's internal dynamics? This advanced exercise delves into the relationship between the external transfer function view and the internal state-space representation. You will analyze a non-minimal state-space model to see firsthand how 'hidden modes'—those that are unobservable from the output—can vanish from the transfer function, a crucial insight for understanding minimal realizations and the subtleties of internal stability [@problem_id:2880765].", "problem": "Consider a continuous-time linear time-invariant (LTI) single-input single-output system in state-space form with state vector $x \\in \\mathbb{R}^{4}$, input $u \\in \\mathbb{R}$, and output $y \\in \\mathbb{R}$:\n$$\n\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t) + D u(t).\n$$\nThe system matrices are given by a block-diagonal structure\n$$\nA = \\begin{pmatrix}\nA_{o}  0 \\\\\n0  A_{u}\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\nB_{o} \\\\\nB_{u}\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\nC_{o}  0\n\\end{pmatrix}, \\quad\nD = 0,\n$$\nwhere\n$$\nA_{o} = \\begin{pmatrix}\n0  1 \\\\\n-2  -3\n\\end{pmatrix}, \\quad\nB_{o} = \\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix}, \\quad\nC_{o} = \\begin{pmatrix}\n1  0\n\\end{pmatrix},\n$$\nand\n$$\nA_{u} = \\begin{pmatrix}\n-4  1 \\\\\n0  -5\n\\end{pmatrix}, \\quad\nB_{u} = \\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}.\n$$\nStarting from the state-space definition and the Laplace transform of the LTI system, perform the following:\n\n1. Using the definition of the transfer function $H(s)$ as the map from input $U(s)$ to output $Y(s)$ under zero initial conditions, derive $H(s)$ in terms of the matrices $A$, $B$, $C$, and $D$ without invoking any realization-specific simplifications.\n\n2. Compute $H(s)$ explicitly for the given matrices, simplify it to a strictly proper rational function, and identify its poles. Compare these poles with the eigenvalues of $A$ to determine whether any poles associated with $A$ do not appear in $H(s)$.\n\n3. Using the definition of observability for a pair $(C,A)$, identify the unobservable subspace of the given realization and argue, from first principles, why modes confined to this subspace do not contribute to $H(s)$. Explain how a minimal realization emerges by restricting to the observable portion.\n\n4. Provide the transfer function of the minimal realization, $H_{\\min}(s)$, as a simplified rational function. Your final answer must be only the expression $H_{\\min}(s)$.\n\nNo numerical rounding is required. Express the final answer as a single simplified rational function of $s$ with real coefficients.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard exercise in linear systems theory concerning the relationship between state-space representations and transfer functions, specifically addressing concepts of observability and minimal realizations. All necessary information is provided, and the problem is free of contradictions or ambiguities. It is a valid problem.\n\nWe begin with a rigorous analysis based on fundamental principles.\n\nThe state-space representation of the continuous-time linear time-invariant (LTI) system is given by:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nwhere $x(t)$ is the state vector, $u(t)$ is the input, and $y(t)$ is the output.\n\n**1. Derivation of the Transfer Function $H(s)$**\n\nTo derive the transfer function $H(s)$, we apply the unilateral Laplace transform to the state-space equations, under the specified condition of zero initial state, i.e., $x(0) = 0$. Let $X(s) = \\mathcal{L}\\{x(t)\\}$, $U(s) = \\mathcal{L}\\{u(t)\\}$, and $Y(s) = \\mathcal{L}\\{y(t)\\}$.\n\nThe Laplace transform of the state equation is:\n$$\ns X(s) - x(0) = A X(s) + B U(s)\n$$\nApplying the zero initial condition $x(0)=0$:\n$$\ns X(s) = A X(s) + B U(s)\n$$\nWe must now solve for the state vector in the frequency domain, $X(s)$.\n$$\ns X(s) - A X(s) = B U(s)\n$$\n$$\n(sI - A) X(s) = B U(s)\n$$\nwhere $I$ is the identity matrix of appropriate dimension ($4 \\times 4$ in this case). Assuming $(sI - A)$ is invertible, which holds for any $s$ that is not an eigenvalue of $A$, we can write:\n$$\nX(s) = (sI - A)^{-1} B U(s)\n$$\nNext, we take the Laplace transform of the output equation:\n$$\nY(s) = C X(s) + D U(s)\n$$\nSubstituting the expression for $X(s)$ into the output equation yields:\n$$\nY(s) = C \\left( (sI - A)^{-1} B U(s) \\right) + D U(s)\n$$\nBy factoring out $U(s)$, we obtain the relationship between the output $Y(s)$ and the input $U(s)$:\n$$\nY(s) = \\left[ C(sI - A)^{-1} B + D \\right] U(s)\n$$\nThe transfer function $H(s)$ is defined as the ratio $Y(s)/U(s)$. Therefore, the general expression for the transfer function is:\n$$\nH(s) = C(sI - A)^{-1} B + D\n$$\n\n**2. Computation of $H(s)$ and Pole Analysis**\n\nThe problem provides block-structured matrices and specifies $D=0$. The transfer function simplifies to $H(s) = C(sI - A)^{-1} B$. The matrix $(sI - A)$ is:\n$$\nsI - A = \\begin{pmatrix} sI_{2} - A_{o}  0 \\\\ 0  sI_{2} - A_{u} \\end{pmatrix}\n$$\nIts inverse is also block-diagonal:\n$$\n(sI - A)^{-1} = \\begin{pmatrix} (sI_{2} - A_{o})^{-1}  0 \\\\ 0  (sI_{2} - A_{u})^{-1} \\end{pmatrix}\n$$\nSubstituting the block matrices for $A$, $B$, and $C$ into the formula for $H(s)$:\n$$\nH(s) = \\begin{pmatrix} C_{o}  0 \\end{pmatrix} \\begin{pmatrix} (sI_{2} - A_{o})^{-1}  0 \\\\ 0  (sI_{2} - A_{u})^{-1} \\end{pmatrix} \\begin{pmatrix} B_{o} \\\\ B_{u} \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nH(s) = \\begin{pmatrix} C_{o}(sI_{2} - A_{o})^{-1}  0 \\end{pmatrix} \\begin{pmatrix} B_{o} \\\\ B_{u} \\end{pmatrix}\n$$\n$$\nH(s) = C_{o}(sI_{2} - A_{o})^{-1} B_{o} + 0 \\cdot (sI_{2} - A_{u})^{-1} B_{u}\n$$\nThe calculation simplifies to $H(s) = C_{o}(sI_{2} - A_{o})^{-1} B_{o}$. This demonstrates that the subsystem described by $(A_u, B_u)$ is disconnected from the output. We now compute this expression explicitly.\nFirst, we find $(sI_{2} - A_{o})^{-1}$ for $A_{o} = \\begin{pmatrix} 0  1 \\\\ -2  -3 \\end{pmatrix}$:\n$$\nsI_{2} - A_{o} = \\begin{pmatrix} s  -1 \\\\ 2  s+3 \\end{pmatrix}\n$$\nThe determinant is $\\det(sI_{2} - A_{o}) = s(s+3) - (-1)(2) = s^{2} + 3s + 2 = (s+1)(s+2)$.\nThe inverse is:\n$$\n(sI_{2} - A_{o})^{-1} = \\frac{1}{s^{2} + 3s + 2} \\begin{pmatrix} s+3  1 \\\\ -2  s \\end{pmatrix}\n$$\nNow, we compute $H(s)$ using $C_{o} = \\begin{pmatrix} 1  0 \\end{pmatrix}$ and $B_{o} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$\nH(s) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\left( \\frac{1}{s^{2} + 3s + 2} \\begin{pmatrix} s+3  1 \\\\ -2  s \\end{pmatrix} \\right) \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\nH(s) = \\frac{1}{s^{2} + 3s + 2} \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ s \\end{pmatrix}\n$$\n$$\nH(s) = \\frac{1}{s^{2} + 3s + 2}\n$$\nThe poles of $H(s)$ are the roots of the denominator, $s^{2} + 3s + 2 = (s+1)(s+2) = 0$. The poles are at $s = -1$ and $s = -2$.\n\nNow we compare these poles to the eigenvalues of the full state matrix $A$. Since $A$ is block-diagonal, its eigenvalues are the union of the eigenvalues of $A_o$ and $A_u$.\nThe eigenvalues of $A_o$ are the roots of $\\det(sI_2 - A_o) = s^2+3s+2=0$, which are $\\lambda_1 = -1$ and $\\lambda_2 = -2$.\nThe matrix $A_{u} = \\begin{pmatrix} -4  1 \\\\ 0  -5 \\end{pmatrix}$ is upper triangular, so its eigenvalues are its diagonal entries: $\\lambda_3 = -4$ and $\\lambda_4 = -5$.\nThe set of eigenvalues of $A$ is $\\{-1, -2, -4, -5\\}$. The set of poles of $H(s)$ is $\\{-1, -2\\}$.\nThe poles associated with the eigenvalues of $A_u$ ($-4$ and $-5$) are absent from the transfer function $H(s)$. This phenomenon is known as pole-zero cancellation and is a direct consequence of the system's structure, specifically its lack of observability.\n\n**3. Observability and Minimal Realization**\n\nA system $(C, A)$ is observable if and only if its observability matrix $\\mathcal{O}$ has full rank, i.e., $\\text{rank}(\\mathcal{O}) = n$, where $n$ is the dimension of the state space. The observability matrix for this $n=4$ system is:\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\end{pmatrix}\n$$\nGiven $C = \\begin{pmatrix} C_o  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}$ and $A = \\begin{pmatrix} A_o  0 \\\\ 0  A_u \\end{pmatrix}$.\n$$\nCA = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 0  1  0  0 \\\\ -2  -3  0  0 \\\\ 0  0  -4  1 \\\\ 0  0  0  -5 \\end{pmatrix} = \\begin{pmatrix} 0  1  0  0 \\end{pmatrix}\n$$\n$$\nCA^2 = (CA)A = \\begin{pmatrix} 0  1  0  0 \\end{pmatrix} A = \\begin{pmatrix} -2  -3  0  0 \\end{pmatrix}\n$$\nThe observability matrix is:\n$$\n\\mathcal{O} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ -2  -3  0  0 \\\\ \\dots  \\dots  0  0 \\end{pmatrix}\n$$\nIt is immediately obvious that the last two columns are zero vectors. The rank of $\\mathcal{O}$ is the rank of its first two columns, which is $2$. Since $\\text{rank}(\\mathcal{O}) = 2  4$, the system is not observable.\n\nThe unobservable subspace is the null space of $\\mathcal{O}$. For a vector $x = (x_1, x_2, x_3, x_4)^T$ to be in this subspace, $\\mathcal{O}x=0$. This requires $x_1=0$ and $x_2=0$. Thus, the unobservable subspace is the set of all vectors of the form $(0, 0, x_3, x_4)^T$, which is precisely the state subspace associated with the block $A_u$. Let the state vector be partitioned as $x(t) = \\begin{pmatrix} x_o(t) \\\\ x_u(t) \\end{pmatrix}$. The unobservable subspace corresponds to states where $x_o(t) = 0$.\n\nThe reason these unobservable modes do not contribute to $H(s)$ is found in the output equation itself. The output is $y(t) = C x(t) = \\begin{pmatrix} C_o  0 \\end{pmatrix} \\begin{pmatrix} x_o(t) \\\\ x_u(t) \\end{pmatrix} = C_o x_o(t)$. The output $y(t)$ depends exclusively on the state sub-vector $x_o(t)$. Even though dynamics of the subsystem $x_u(t)$ are excited by the input $u(t)$ via $\\dot{x}_u(t) = A_u x_u(t) + B_u u(t)$, the state $x_u(t)$ is never \"seen\" by the output. The modes of this subsystem, which are the eigenvalues of $A_u$, are therefore hidden from the input-output relationship, which is what the transfer function represents.\n\nA minimal realization of a transfer function is a state-space representation that is both controllable and observable. The given realization is not minimal because it is not observable. By restricting the system to its observable part, we obtain a minimal realization. The block-diagonal structure of $A$ and the specific forms of $B$ and $C$ provide a Kalman decomposition into an observable part and an unobservable part. The observable part is defined by $(A_o, B_o, C_o)$. This subsystem corresponds to a realization of order $2$. We must verify that it is minimal. Observability of $(C_o, A_o)$ is confirmed by $\\mathcal{O}_o = \\begin{pmatrix}C_o \\\\ C_o A_o \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, which has rank $2$. Controllability of $(A_o, B_o)$ is confirmed by $\\mathcal{C}_o = \\begin{pmatrix} B_o  A_o B_o \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 1  -3 \\end{pmatrix}$, which also has rank $2$. Since the $(A_o, B_o, C_o)$ subsystem is both controllable and observable, it constitutes a minimal realization.\n\n**4. Transfer Function of the Minimal Realization, $H_{\\min}(s)$**\n\nThe transfer function of the minimal realization, $H_{\\min}(s)$, is the transfer function of the observable and controllable subsystem $(A_o, B_o, C_o)$. This is precisely the function $H(s)$ we calculated in part 2, where the unobservable dynamics were canceled out.\n$$\nH_{\\min}(s) = C_{o}(sI_{2} - A_{o})^{-1} B_{o} = \\frac{1}{s^{2} + 3s + 2}\n$$\nThis is the required simplified rational function.", "answer": "$$\n\\boxed{\\frac{1}{s^{2} + 3s + 2}}\n$$", "id": "2880765"}]}