{"hands_on_practices": [{"introduction": "The decomposition of a system's response into its zero-input (ZIR) and zero-state (ZSR) components is a direct consequence of linearity. This first practice provides a foundational, hands-on opportunity to apply these definitions directly to a continuous-time system described in state-space form, a ubiquitous model in modern control theory. By calculating the state-transition matrix and performing the convolution integral, you will solidify your understanding of how a system's natural evolution (ZIR) and its reaction to external stimuli (ZSR) are computed and combine to form the total response [@problem_id:2900660].", "problem": "Consider a continuous-time, single-input single-output, linear time-invariant (LTI) state-space system defined by the equations $\\,\\dot{x}(t)=A\\,x(t)+B\\,u(t)\\,$ and $\\,y(t)=C\\,x(t)+D\\,u(t)\\,$, where $\\,A=\\begin{bmatrix}0 & 1\\\\ -2 & -3\\end{bmatrix}\\,$, $\\,B=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\,$, $\\,C=\\begin{bmatrix}1 & 0\\end{bmatrix}\\,$, and $\\,D=0\\,$. The zero-input response $\\,y_{\\text{zi}}(t)\\,$ is defined as the output that results when the input is identically zero and the initial condition is nonzero, while the zero-state response $\\,y_{\\text{zs}}(t)\\,$ is defined as the output that results when the initial condition is zero and the input is applied. Using only fundamental properties of linearity, superposition, and the fundamental solution of linear systems, compute:\n- the zero-input response $\\,y_{\\text{zi}}(t)\\,$ for the initial condition $\\,x(0)=x_0=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\,$ with $\\,u(t)\\equiv 0\\,$, and\n- the zero-state response $\\,y_{\\text{zs}}(t)\\,$ for the input $\\,u(t)=\\sin t\\,$ with $\\,x(0)=\\begin{bmatrix}0\\\\ 0\\end{bmatrix}\\,$.\n\nGive your answers as exact closed-form expressions in terms of $\\,t\\,$, valid for $\\,t\\ge 0\\,$. Interpret all trigonometric arguments in radians. No rounding is required. Your final answer must consist of the pair $\\,(y_{\\text{zi}}(t),\\,y_{\\text{zs}}(t))\\,$ as explicit functions of $\\,t\\,$.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in the analysis of linear time-invariant (LTI) systems, providing all necessary matrices, initial conditions, and inputs to determine a unique solution. The required calculations are based on fundamental principles of linear differential equations and matrix algebra. Therefore, the problem is valid, and we proceed with the solution.\n\nThe total response $y(t)$ of an LTI system can be decomposed into the sum of the zero-input response $y_{\\text{zi}}(t)$ and the zero-state response $y_{\\text{zs}}(t)$, a direct consequence of the linearity property.\nThe state vector $x(t)$ is given by the general solution to the state equation:\n$$x(t) = e^{At}x(0) + \\int_{0}^{t} e^{A(t-\\tau)}B u(\\tau) \\,d\\tau$$\nwhere $e^{At}$ is the state-transition matrix. The output is then $y(t) = C x(t) + D u(t)$.\nSubstituting the expression for $x(t)$ gives:\n$$y(t) = C e^{At}x(0) + C \\int_{0}^{t} e^{A(t-\\tau)}B u(\\tau) \\,d\\tau + D u(t)$$\nFrom this expression, we identify the zero-input and zero-state components.\n\n1.  **Zero-Input Response, $y_{\\text{zi}}(t)$**\n    The zero-input response occurs when the input $u(t) \\equiv 0$ and the initial condition is $x(0) = x_0$. The formula simplifies to:\n    $$y_{\\text{zi}}(t) = C e^{At} x_0$$\n    First, we must compute the state-transition matrix $e^{At}$ for $A=\\begin{bmatrix}0 & 1\\\\ -2 & -3\\end{bmatrix}$. We find the eigenvalues by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n    $$ \\det\\begin{pmatrix}-\\lambda & 1\\\\ -2 & -3-\\lambda\\end{pmatrix} = (-\\lambda)(-3-\\lambda) - (1)(-2) = \\lambda^2 + 3\\lambda + 2 = 0 $$\n    Factoring the polynomial gives $(\\lambda+1)(\\lambda+2) = 0$, so the eigenvalues are $\\lambda_1 = -1$ and $\\lambda_2 = -2$.\n    Since the eigenvalues are distinct, the matrix $A$ is diagonalizable. We find the corresponding eigenvectors.\n    For $\\lambda_1 = -1$: $(A+I)v_1 = \\begin{bmatrix}1 & 1\\\\ -2 & -2\\end{bmatrix}v_1 = 0$. An eigenvector is $v_1 = \\begin{bmatrix}1\\\\ -1\\end{bmatrix}$.\n    For $\\lambda_2 = -2$: $(A+2I)v_2 = \\begin{bmatrix}2 & 1\\\\ -2 & -1\\end{bmatrix}v_2 = 0$. An eigenvector is $v_2 = \\begin{bmatrix}1\\\\ -2\\end{bmatrix}$.\n    The modal matrix $P$ and its inverse $P^{-1}$ are:\n    $$ P = \\begin{bmatrix}v_1 & v_2\\end{bmatrix} = \\begin{bmatrix}1 & 1\\\\ -1 & -2\\end{bmatrix} $$\n    $$ P^{-1} = \\frac{1}{(1)(-2) - (1)(-1)}\\begin{bmatrix}-2 & -1\\\\ 1 & 1\\end{bmatrix} = \\begin{bmatrix}2 & 1\\\\ -1 & -1\\end{bmatrix} $$\n    The state-transition matrix is $e^{At} = P e^{Jt} P^{-1}$, where $J = \\begin{bmatrix}\\lambda_1 & 0\\\\ 0 & \\lambda_2\\end{bmatrix}$.\n    $$ e^{At} = \\begin{bmatrix}1 & 1\\\\ -1 & -2\\end{bmatrix} \\begin{bmatrix}e^{-t} & 0\\\\ 0 & e^{-2t}\\end{bmatrix} \\begin{bmatrix}2 & 1\\\\ -1 & -1\\end{bmatrix} $$\n    $$ e^{At} = \\begin{bmatrix}e^{-t} & e^{-2t}\\\\ -e^{-t} & -2e^{-2t}\\end{bmatrix} \\begin{bmatrix}2 & 1\\\\ -1 & -1\\end{bmatrix} $$\n    $$ e^{At} = \\begin{bmatrix} 2e^{-t}-e^{-2t} & e^{-t}-e^{-2t} \\\\ -2e^{-t}+2e^{-2t} & -e^{-t}+2e^{-2t} \\end{bmatrix} $$\n    Now we can compute $y_{\\text{zi}}(t)$ with $C=\\begin{bmatrix}1 & 0\\end{bmatrix}$ and $x_0=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}$.\n    $$ y_{\\text{zi}}(t) = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix} 2e^{-t}-e^{-2t} & e^{-t}-e^{-2t} \\\\ -2e^{-t}+2e^{-2t} & -e^{-t}+2e^{-2t} \\end{bmatrix} \\begin{bmatrix}1\\\\ 0\\end{bmatrix} $$\n    $$ y_{\\text{zi}}(t) = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix} 2e^{-t}-e^{-2t} \\\\ -2e^{-t}+2e^{-2t} \\end{bmatrix} = 2e^{-t}-e^{-2t} $$\n\n2.  **Zero-State Response, $y_{\\text{zs}}(t)$**\n    The zero-state response occurs when the initial condition $x(0) = 0$. The formula is:\n    $$y_{\\text{zs}}(t) = C \\int_{0}^{t} e^{A(t-\\tau)}B u(\\tau) \\,d\\tau + D u(t)$$\n    Since $D=0$, the second term vanishes. This expression is the convolution of the impulse response $h(t) = C e^{At} B$ with the input $u(t) = \\sin(t)$.\n    First, we calculate the impulse response $h(t)$.\n    $$ h(t) = C e^{At} B = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix} 2e^{-t}-e^{-2t} & e^{-t}-e^{-2t} \\\\ -2e^{-t}+2e^{-2t} & -e^{-t}+2e^{-2t} \\end{bmatrix} \\begin{bmatrix}0\\\\ 1\\end{bmatrix} $$\n    $$ h(t) = \\begin{bmatrix} 2e^{-t}-e^{-2t} & e^{-t}-e^{-2t} \\end{bmatrix} \\begin{bmatrix}0\\\\ 1\\end{bmatrix} = e^{-t}-e^{-2t} $$\n    Now we compute the convolution integral $y_{\\text{zs}}(t) = \\int_{0}^{t} h(t-\\tau)u(\\tau)\\,d\\tau$.\n    $$ y_{\\text{zs}}(t) = \\int_{0}^{t} (e^{-(t-\\tau)} - e^{-2(t-\\tau)}) \\sin(\\tau) \\,d\\tau $$\n    $$ y_{\\text{zs}}(t) = e^{-t} \\int_{0}^{t} e^{\\tau} \\sin(\\tau) \\,d\\tau - e^{-2t} \\int_{0}^{t} e^{2\\tau} \\sin(\\tau) \\,d\\tau $$\n    We use the standard integral formula $\\int e^{a\\tau}\\sin(b\\tau) \\,d\\tau = \\frac{e^{a\\tau}}{a^2+b^2}(a\\sin(b\\tau) - b\\cos(b\\tau))$.\n    For the first integral, $a=1, b=1$:\n    $$ \\int_{0}^{t} e^{\\tau} \\sin(\\tau) \\,d\\tau = \\left[ \\frac{e^{\\tau}}{1^2+1^2}(\\sin(\\tau) - \\cos(\\tau)) \\right]_{0}^{t} = \\frac{e^{t}}{2}(\\sin(t) - \\cos(t)) - \\frac{1}{2}(0-1) = \\frac{1}{2}e^{t}(\\sin(t) - \\cos(t)) + \\frac{1}{2} $$\n    For the second integral, $a=2, b=1$:\n    $$ \\int_{0}^{t} e^{2\\tau} \\sin(\\tau) \\,d\\tau = \\left[ \\frac{e^{2\\tau}}{2^2+1^2}(2\\sin(\\tau) - \\cos(\\tau)) \\right]_{0}^{t} = \\frac{e^{2t}}{5}(2\\sin(t) - \\cos(t)) - \\frac{1}{5}(0-1) = \\frac{1}{5}e^{2t}(2\\sin(t) - \\cos(t)) + \\frac{1}{5} $$\n    Substituting these results back into the expression for $y_{\\text{zs}}(t)$:\n    $$ y_{\\text{zs}}(t) = e^{-t}\\left(\\frac{1}{2}e^{t}(\\sin(t) - \\cos(t)) + \\frac{1}{2}\\right) - e^{-2t}\\left(\\frac{1}{5}e^{2t}(2\\sin(t) - \\cos(t)) + \\frac{1}{5}\\right) $$\n    $$ y_{\\text{zs}}(t) = \\left(\\frac{1}{2}\\sin(t) - \\frac{1}{2}\\cos(t) + \\frac{1}{2}e^{-t}\\right) - \\left(\\frac{2}{5}\\sin(t) - \\frac{1}{5}\\cos(t) + \\frac{1}{5}e^{-2t}\\right) $$\n    Grouping terms, we obtain the final expression:\n    $$ y_{\\text{zs}}(t) = \\left(\\frac{1}{2} - \\frac{2}{5}\\right)\\sin(t) + \\left(-\\frac{1}{2} + \\frac{1}{5}\\right)\\cos(t) + \\frac{1}{2}e^{-t} - \\frac{1}{5}e^{-2t} $$\n    $$ y_{\\text{zs}}(t) = \\frac{1}{10}\\sin(t) - \\frac{3}{10}\\cos(t) + \\frac{1}{2}e^{-t} - \\frac{1}{5}e^{-2t} $$\n\nThe required pair of responses is $(y_{\\text{zi}}(t), y_{\\text{zs}}(t))$.\nFor $t \\ge 0$, the zero-input response is $y_{\\text{zi}}(t) = 2e^{-t}-e^{-2t}$.\nFor $t \\ge 0$, the zero-state response is $y_{\\text{zs}}(t) = \\frac{1}{10}\\sin(t) - \\frac{3}{10}\\cos(t) + \\frac{1}{2}e^{-t} - \\frac{1}{5}e^{-2t}$.", "answer": "$$\\boxed{\\begin{pmatrix} 2\\exp(-t) - \\exp(-2t) & \\frac{1}{10}\\sin(t) - \\frac{3}{10}\\cos(t) + \\frac{1}{2}\\exp(-t) - \\frac{1}{5}\\exp(-2t) \\end{pmatrix}}$$", "id": "2900660"}, {"introduction": "While state-space models provide a complete internal picture, many systems are initially described by a transfer function, which relates input to output in the frequency domain. This exercise bridges the frequency-domain perspective with our time-domain decomposition, demonstrating how to recover the total response from a transfer function $G(s)$ when initial conditions are non-zero. You will learn how the Laplace transform elegantly separates the effects of the initial state (ZIR) from the effects of the input (ZSR), providing a powerful algebraic framework for solving the governing differential equation [@problem_id:2900694].", "problem": "Consider a single-input single-output continuous-time linear time-invariant (LTI) system with transfer function $$G(s)=\\frac{s+1}{s^{2}+2s+2}.$$ The input is the unit step $$u(t)=1(t),$$ where $$1(t)=\\begin{cases}0, & t<0,\\\\ 1, & t\\ge 0.\\end{cases}$$ The initial state at time $$t=0^{-}$$ corresponds to the output initial conditions $$y(0^{-})=1,\\quad \\dot{y}(0^{-})=0.$$ Using only first principles for linear systems (superposition, the definition of the Laplace transform, and the input-output relation in the Laplace domain), do the following:\n- Derive a differential equation in the time domain that is consistent with the given transfer function.\n- Decompose the total response into the Zero-Input Response (ZIR) and the Zero-State Response (ZSR), compute each component explicitly for the given input and initial conditions, and then sum them to obtain the total response.\n\nProvide your final result as a single closed-form analytic expression for the total output $$y(t)$$ valid for $$t\\ge 0.$$ No numerical rounding is required. Do not include units in your answer.", "solution": "The problem statement is evaluated for validity.\n\nGivens are extracted verbatim as follows:\n- System: single-input single-output continuous-time linear time-invariant (LTI) system.\n- Transfer function: $G(s)=\\frac{s+1}{s^{2}+2s+2}$.\n- Input: $u(t)=1(t)$, where $1(t)=\\begin{cases}0, & t<0,\\\\ 1, & t\\ge 0.\\end{cases}$.\n- Initial conditions: $y(0^{-})=1$, $\\dot{y}(0^{-})=0$.\n- Tasks:\n    - Derive a differential equation.\n    - Decompose the total response into Zero-Input Response (ZIR) and Zero-State Response (ZSR).\n    - Compute each component.\n    - Sum them to obtain the total response $y(t)$ for $t\\ge 0$.\n- Constraints: Use only first principles for linear systems.\n\nThe problem is subjected to validation.\n- **Scientific Groundedness**: The problem is a standard exercise in linear systems theory, utilizing well-established principles such as Laplace transforms, transfer functions, and response decomposition (ZIR/ZSR). It is scientifically sound.\n- **Well-Posedness**: The system is a second-order LTI system, for which two initial conditions are provided. The system is stable, as the poles of $G(s)$ ($s = -1 \\pm j$) are in the left-half of the complex plane. The input is a standard test signal. The problem is well-posed and admits a unique, stable solution.\n- **Objectivity**: The problem is stated using precise, unambiguous mathematical and engineering terminology. It is objective.\n- **Completeness and Consistency**: All necessary information (system description, input, initial conditions) is provided. There are no contradictions.\n\nThe verdict is that the problem is **valid**. We proceed with the solution.\n\nThe problem requires the derivation of the system's time-domain differential equation, the calculation of its zero-input and zero-state responses, and finally, the total response.\n\nFirst, we derive the differential equation from the transfer function $G(s)$. The transfer function relates the Laplace transform of the output, $Y(s)$, to the Laplace transform of the input, $U(s)$, under the assumption of zero initial conditions:\n$$G(s) = \\frac{Y(s)}{U(s)} = \\frac{s+1}{s^{2}+2s+2}$$\nBy cross-multiplication, we obtain the input-output relationship in the Laplace domain:\n$$(s^{2}+2s+2)Y(s) = (s+1)U(s)$$\n$$s^{2}Y(s) + 2sY(s) + 2Y(s) = sU(s) + U(s)$$\nUsing the differentiation property of the unilateral Laplace transform, where $\\mathcal{L}\\{\\frac{d^n f(t)}{dt^n}\\} = s^n F(s) - \\sum_{k=1}^{n} s^{n-k} f^{(k-1)}(0^{-})$, a term like $s^n F(s)$ corresponds to the $n$-th derivative of $f(t)$, ignoring initial condition terms. Thus, we can perform an inverse transformation to find the differential equation governing the system:\n$$\\frac{d^2 y(t)}{dt^2} + 2\\frac{d y(t)}{dt} + 2y(t) = \\frac{d u(t)}{dt} + u(t)$$\nThis is the required differential equation.\n\nThe total response $y(t)$ of a linear system can be decomposed by the principle of superposition into two components: the zero-input response $y_{\\text{ZIR}}(t)$, which is the response due to the initial conditions alone with zero input, and the zero-state response $y_{\\text{ZSR}}(t)$, which is the response to the input alone with zero initial conditions.\n$$y(t) = y_{\\text{ZIR}}(t) + y_{\\text{ZSR}}(t)$$\nTo find these components, we take the Laplace transform of the full differential equation, now including the non-zero initial conditions at $t=0^{-}$:\n$$\\mathcal{L}\\left\\{\\frac{d^2 y}{dt^2}\\right\\} = s^{2}Y(s) - sy(0^{-}) - \\dot{y}(0^{-})$$\n$$\\mathcal{L}\\left\\{\\frac{d y}{dt}\\right\\} = sY(s) - y(0^{-})$$\n$$\\mathcal{L}\\left\\{\\frac{d u}{dt}\\right\\} = sU(s) - u(0^{-})$$\nSubstituting these into the differential equation yields:\n$$(s^{2}Y(s) - sy(0^{-}) - \\dot{y}(0^{-})) + 2(sY(s) - y(0^{-})) + 2Y(s) = (sU(s) - u(0^{-})) + U(s)$$\nWe group terms involving $Y(s)$ on the left side and all other terms on the right side:\n$$(s^{2}+2s+2)Y(s) = (s+2)y(0^{-}) + \\dot{y}(0^{-}) + (s+1)U(s) - u(0^{-})$$\nSolving for $Y(s)$:\n$$Y(s) = \\frac{(s+2)y(0^{-}) + \\dot{y}(0^{-})}{s^{2}+2s+2} + \\frac{(s+1)U(s) - u(0^{-})}{s^{2}+2s+2}$$\nThe first term on the right-hand side depends only on the initial conditions and represents the Laplace transform of the zero-input response, $Y_{\\text{ZIR}}(s)$. The second term depends only on the input and represents the Laplace transform of the zero-state response, $Y_{\\text{ZSR}}(s)$.\n\nWe now calculate the ZIR. Given $y(0^{-})=1$ and $\\dot{y}(0^{-})=0$:\n$$Y_{\\text{ZIR}}(s) = \\frac{(s+2)(1) + 0}{s^{2}+2s+2} = \\frac{s+2}{s^{2}+2s+2}$$\nTo find the inverse Laplace transform, we complete the square in the denominator: $s^{2}+2s+2 = (s+1)^{2} + 1^{2}$. We then rewrite the numerator to match the standard transform pairs for damped sinusoids:\n$$Y_{\\text{ZIR}}(s) = \\frac{(s+1) + 1}{(s+1)^{2}+1^{2}} = \\frac{s+1}{(s+1)^{2}+1^{2}} + \\frac{1}{(s+1)^{2}+1^{2}}$$\nUsing the transform pairs $\\mathcal{L}^{-1}\\left\\{\\frac{s+a}{(s+a)^2+\\omega^2}\\right\\} = \\exp(-at)\\cos(\\omega t)$ and $\\mathcal{L}^{-1}\\left\\{\\frac{\\omega}{(s+a)^2+\\omega^2}\\right\\} = \\exp(-at)\\sin(\\omega t)$, with $a=1$ and $\\omega=1$:\n$$y_{\\text{ZIR}}(t) = \\exp(-t)\\cos(t) + \\exp(-t)\\sin(t) = \\exp(-t)(\\cos(t) + \\sin(t)), \\quad \\text{for } t \\ge 0$$\n\nNext, we calculate the ZSR. The input is the unit step function $u(t)=1(t)$, so its Laplace transform is $U(s) = \\frac{1}{s}$ and its initial value is $u(0^{-})=0$.\n$$Y_{\\text{ZSR}}(s) = \\frac{(s+1)U(s)}{s^{2}+2s+2} = \\frac{s+1}{s(s^{2}+2s+2)}$$\nWe use partial fraction expansion to simplify this expression:\n$$Y_{\\text{ZSR}}(s) = \\frac{A}{s} + \\frac{Bs+C}{s^{2}+2s+2}$$\nThe coefficient $A$ is found by the residue method:\n$$A = \\lim_{s\\to 0} s Y_{\\text{ZSR}}(s) = \\lim_{s\\to 0} \\frac{s+1}{s^{2}+2s+2} = \\frac{1}{2}$$\nTo find $B$ and $C$, we equate the numerators:\n$$s+1 = A(s^{2}+2s+2) + (Bs+C)s$$\n$$s+1 = \\frac{1}{2}(s^{2}+2s+2) + Bs^{2} + Cs = \\left(\\frac{1}{2}+B\\right)s^{2} + (1+C)s + 1$$\nComparing coefficients of powers of $s$:\n- For $s^2$: $0 = \\frac{1}{2} + B \\implies B = -\\frac{1}{2}$\n- For $s^1$: $1 = 1 + C \\implies C = 0$\nThus, the expansion is:\n$$Y_{\\text{ZSR}}(s) = \\frac{1/2}{s} - \\frac{1}{2}\\frac{s}{s^{2}+2s+2}$$\nWe rewrite the second term for inverse transformation:\n$$Y_{\\text{ZSR}}(s) = \\frac{1}{2s} - \\frac{1}{2}\\frac{s}{(s+1)^{2}+1^{2}} = \\frac{1}{2s} - \\frac{1}{2}\\left(\\frac{s+1-1}{(s+1)^{2}+1^{2}}\\right)$$\n$$Y_{\\text{ZSR}}(s) = \\frac{1}{2s} - \\frac{1}{2}\\left(\\frac{s+1}{(s+1)^{2}+1^{2}} - \\frac{1}{(s+1)^{2}+1^{2}}\\right)$$\nTaking the inverse Laplace transform term by term:\n$$y_{\\text{ZSR}}(t) = \\frac{1}{2} - \\frac{1}{2}(\\exp(-t)\\cos(t) - \\exp(-t)\\sin(t)) = \\frac{1}{2}(1 - \\exp(-t)\\cos(t) + \\exp(-t)\\sin(t)), \\quad \\text{for } t \\ge 0$$\n\nFinally, the total response $y(t)$ is the sum of the ZIR and ZSR:\n$$y(t) = y_{\\text{ZIR}}(t) + y_{\\text{ZSR}}(t)$$\n$$y(t) = \\exp(-t)(\\cos(t) + \\sin(t)) + \\frac{1}{2}(1 - \\exp(-t)\\cos(t) + \\exp(-t)\\sin(t))$$\nWe combine like terms:\n$$y(t) = \\frac{1}{2} + \\exp(-t)\\cos(t)\\left(1 - \\frac{1}{2}\\right) + \\exp(-t)\\sin(t)\\left(1 + \\frac{1}{2}\\right)$$\n$$y(t) = \\frac{1}{2} + \\frac{1}{2}\\exp(-t)\\cos(t) + \\frac{3}{2}\\exp(-t)\\sin(t)$$\nThis expression can be written more compactly as:\n$$y(t) = \\frac{1}{2}\\left(1 + \\exp(-t)(\\cos(t) + 3\\sin(t))\\right)$$\nThis is the total response of the system for $t \\ge 0$.", "answer": "$$\\boxed{\\frac{1}{2}\\left(1 + \\exp(-t)(\\cos(t) + 3\\sin(t))\\right)}$$", "id": "2900694"}, {"introduction": "This final practice moves beyond pure computation to explore a profound conceptual insight at the intersection of stability, observability, and response decomposition. By constructing and analyzing a system where unstable internal modes are hidden from the output, you will uncover the crucial difference between internal stability and input-output stability. This exercise [@problem_id:2900691] reveals that the zero-input response at the output can remain perfectly well-behaved and bounded, even as the system's internal states diverge, offering a compelling reason why observability is a cornerstone of system analysis and control design.", "problem": "Consider a continuous-time linear time-invariant (LTI) state-space model with state equation $\\dot{x}(t)=A\\,x(t)+B\\,u(t)$ and output equation $y(t)=C\\,x(t)$, where $x(t)\\in\\mathbb{R}^{n}$, $u(t)\\in\\mathbb{R}^{m}$, and $y(t)\\in\\mathbb{R}$. The total response at the output decomposes into the zero-input response (ZIR) due to initial conditions with $u(t)\\equiv 0$ and the zero-state response (ZSR) due to input with $x(0)=0$. Work only from fundamental definitions of stability, observability, and the decomposition into zero-input and zero-state responses.\n\nYour tasks are:\n- Using the definition of observability and the notion of the unobservable subspace as the largest $A$-invariant subspace contained in $\\ker C$, justify rigorously why if every unstable eigenmode of $A$ lies entirely in the unobservable subspace of $(C,A)$, then the zero-input response at the output must remain bounded for all initial conditions, even if the internal state diverges.\n- Exhibit a concrete $3$-state construction that meets these conditions and analyze it. Let\n$$\nA=\\mathrm{diag}(2,\\,1,\\,-1),\\quad C=\\begin{pmatrix}0&0&1\\end{pmatrix},\\quad B=0,\n$$\nand consider the zero-input case $u(t)\\equiv 0$ with arbitrary initial condition\n$$\nx(0)=x_{0}=\\begin{pmatrix}\\alpha\\\\ \\beta\\\\ \\gamma\\end{pmatrix},\\quad \\text{where }\\alpha,\\beta,\\gamma\\in\\mathbb{R}.\n$$\nUsing only the definitions, verify that the unstable eigenmodes of $A$ are unobservable at the output, explain why internal divergence can still occur, and determine the closed-form expression for the zero-input output $y_{\\mathrm{ZI}}(t)$ as a function of $t$, $\\alpha$, $\\beta$, and $\\gamma$.\n\nAnswer specification:\n- Provide your final answer as a single closed-form analytic expression for $y_{\\mathrm{ZI}}(t)$ in terms of $t$, $\\alpha$, $\\beta$, and $\\gamma$.\n- No rounding is required and no units are needed.", "solution": "The problem posed is validated as scientifically grounded, well-posed, and objective. It is a standard problem in linear systems theory concerning the fundamental concepts of stability and observability. All necessary data and definitions are provided, and the problem is free of contradictions or ambiguities. We may proceed with the solution.\n\nThe problem consists of two parts. First, a general justification is required, followed by an analysis of a specific system.\n\nPart 1: General Justification\n\nWe are given a linear time-invariant (LTI) system described by the state-space equations:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t)\n$$\nWe consider the zero-input response (ZIR), where the input $u(t)$ is identically zero for all $t \\ge 0$. The state equation becomes:\n$$\n\\dot{x}(t) = A x(t)\n$$\nFor a given initial condition $x(0) = x_0$, the solution for the state vector is:\n$$\nx(t) = e^{At} x_0\n$$\nConsequently, the zero-input output response, $y_{\\mathrm{ZI}}(t)$, is given by:\n$$\ny_{\\mathrm{ZI}}(t) = C x(t) = C e^{At} x_0\n$$\nTo analyze the behavior of this response, we express the initial state $x_0$ as a linear combination of the eigenvectors of the matrix $A$. Assuming $A$ is diagonalizable with $n$ linearly independent eigenvectors $v_1, v_2, \\ldots, v_n$ corresponding to eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$, we can write:\n$$\nx_0 = \\sum_{i=1}^{n} c_i v_i\n$$\nwhere $c_i$ are scalar coefficients. The action of the state transition matrix $e^{At}$ on an eigenvector $v_i$ is given by the well-known identity $e^{At}v_i = e^{\\lambda_i t} v_i$. Substituting the expansion of $x_0$ into the expression for $y_{\\mathrm{ZI}}(t)$:\n$$\ny_{\\mathrm{ZI}}(t) = C e^{At} \\left( \\sum_{i=1}^{n} c_i v_i \\right) = C \\left( \\sum_{i=1}^{n} c_i e^{At} v_i \\right) = C \\left( \\sum_{i=1}^{n} c_i e^{\\lambda_i t} v_i \\right)\n$$\nBy linearity of the matrix-vector product, this becomes:\n$$\ny_{\\mathrm{ZI}}(t) = \\sum_{i=1}^{n} c_i e^{\\lambda_i t} (C v_i)\n$$\nAn eigenmode, associated with the eigenvalue $\\lambda_i$ and eigenvector $v_i$, is defined as unobservable if the eigenvector $v_i$ lies in the unobservable subspace. The problem defines the unobservable subspace as the largest $A$-invariant subspace contained in the kernel of $C$, denoted $\\ker C$. An eigenspace is, by definition, an $A$-invariant subspace. Thus, if an eigenvector $v_i$ belongs to $\\ker C$, the eigenspace it spans is an $A$-invariant subspace within $\\ker C$, making the mode unobservable. The condition for the mode $(\\lambda_i, v_i)$ to be unobservable is therefore $v_i \\in \\ker C$, which is equivalent to the condition $C v_i = 0$.\n\nThe problem posits that every unstable eigenmode of $A$ is unobservable. An eigenmode is unstable if its corresponding eigenvalue $\\lambda_i$ has a positive real part, $\\operatorname{Re}(\\lambda_i) > 0$. According to the hypothesis, for every such unstable mode, we have $C v_i = 0$.\n\nLet us partition the set of indices $\\{1, 2, \\ldots, n\\}$ into two disjoint sets: $S$ for stable and marginally stable modes ($\\operatorname{Re}(\\lambda_i) \\le 0$) and $U$ for unstable modes ($\\operatorname{Re}(\\lambda_i) > 0$). The output response can then be written as:\n$$\ny_{\\mathrm{ZI}}(t) = \\sum_{i \\in S} c_i e^{\\lambda_i t} (C v_i) + \\sum_{j \\in U} c_j e^{\\lambda_j t} (C v_j)\n$$\nFrom the given condition, for every $j \\in U$, the mode is unobservable, so $C v_j = 0$. This forces the second summation to be identically zero.\n$$\n\\sum_{j \\in U} c_j e^{\\lambda_j t} (C v_j) = \\sum_{j \\in U} c_j e^{\\lambda_j t} (0) = 0\n$$\nThe output response thus reduces to a sum over only the stable and marginally stable modes:\n$$\ny_{\\mathrm{ZI}}(t) = \\sum_{i \\in S} c_i e^{\\lambda_i t} (C v_i)\n$$\nFor each term in this sum, $\\operatorname{Re}(\\lambda_i) \\le 0$. This implies that the magnitude of the exponential term, $|e^{\\lambda_i t}| = e^{\\operatorname{Re}(\\lambda_i) t}$, is either decaying to zero (if $\\operatorname{Re}(\\lambda_i) < 0$) or is constant (if $\\operatorname{Re}(\\lambda_i) = 0$ and the eigenvalue is simple, resulting in bounded oscillations). In either case, each term in the summation is bounded for all $t \\ge 0$. A finite sum of bounded functions is itself a bounded function. Therefore, $y_{\\mathrm{ZI}}(t)$ must remain bounded for all initial conditions $x_0$.\n\nSimultaneously, the internal state $x(t) = \\sum_{i=1}^{n} c_i e^{\\lambda_i t} v_i$ can diverge. If the initial condition $x_0$ has a non-zero component along any unstable eigenvector $v_j$ (i.e., $c_j \\ne 0$ for some $j \\in U$), the term $c_j e^{\\lambda_j t} v_j$ will grow without bound as $t \\to \\infty$ because $\\operatorname{Re}(\\lambda_j) > 0$. This demonstrates that the internal state can diverge while the output remains bounded, a phenomenon known as internal instability that is hidden from the output.\n\nPart 2: Concrete Example Analysis\n\nWe are given the system matrices:\n$$\nA = \\mathrm{diag}(2, 1, -1) = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}\n$$\nThe initial condition is $x(0) = x_0 = \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}$.\n\nThe eigenvalues of the diagonal matrix $A$ are its diagonal elements: $\\lambda_1 = 2$, $\\lambda_2 = 1$, and $\\lambda_3 = -1$. The corresponding eigenvectors are the standard basis vectors:\n$$\nv_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad v_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe unstable modes are those with eigenvalues having a positive real part, which are $\\lambda_1 = 2$ and $\\lambda_2 = 1$.\n\nTo verify that these unstable modes are unobservable, we check if their eigenvectors lie in the kernel of $C$.\nFor $\\lambda_1 = 2$:\n$$\nC v_1 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0\n$$\nSince $C v_1 = 0$, the first mode is unobservable.\nFor $\\lambda_2 = 1$:\n$$\nC v_2 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = 0\n$$\nSince $C v_2 = 0$, the second mode is also unobservable.\nThe stable mode corresponds to $\\lambda_3 = -1$. We check its observability:\n$$\nC v_3 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = 1 \\ne 0\n$$\nThe stable mode is observable. The conditions of the problem are satisfied: all unstable modes are unobservable.\n\nThe internal state vector $x(t)$ is given by $x(t) = e^{At} x_0$. For a diagonal matrix $A$, the matrix exponential is the diagonal matrix of the exponentials of the diagonal elements:\n$$\ne^{At} = \\begin{pmatrix} e^{2t} & 0 & 0 \\\\ 0 & e^{t} & 0 \\\\ 0 & 0 & e^{-t} \\end{pmatrix}\n$$\nThe state evolution is then:\n$$\nx(t) = \\begin{pmatrix} e^{2t} & 0 & 0 \\\\ 0 & e^{t} & 0 \\\\ 0 & 0 & e^{-t} \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} \\alpha e^{2t} \\\\ \\beta e^{t} \\\\ \\gamma e^{-t} \\end{pmatrix}\n$$\nIf $\\alpha \\ne 0$ or $\\beta \\ne 0$, the first or second component of the state vector $x(t)$ will grow exponentially, causing the norm of the state vector $\\|x(t)\\|$ to diverge as $t \\to \\infty$. This is the internal divergence.\n\nFinally, we determine the zero-input output $y_{\\mathrm{ZI}}(t)$:\n$$\ny_{\\mathrm{ZI}}(t) = C x(t) = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\alpha e^{2t} \\\\ \\beta e^{t} \\\\ \\gamma e^{-t} \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\ny_{\\mathrm{ZI}}(t) = (0)(\\alpha e^{2t}) + (0)(\\beta e^{t}) + (1)(\\gamma e^{-t}) = \\gamma e^{-t}\n$$\nThis result confirms the theoretical derivation. The output response depends only on the initial condition component $\\gamma$ associated with the stable, observable mode. The contributions from the unstable modes, which grow as $e^{2t}$ and $e^t$, are completely masked from the output because these modes are unobservable. For any finite values of $\\alpha$, $\\beta$, and $\\gamma$, the output $y_{\\mathrm{ZI}}(t)$ decays to zero as $t \\to \\infty$, remaining bounded for all time, despite the potential for internal state divergence.\nThe closed-form expression for the zero-input output is $y_{\\mathrm{ZI}}(t) = \\gamma e^{-t}$.", "answer": "$$\n\\boxed{\\gamma \\exp(-t)}\n$$", "id": "2900691"}]}