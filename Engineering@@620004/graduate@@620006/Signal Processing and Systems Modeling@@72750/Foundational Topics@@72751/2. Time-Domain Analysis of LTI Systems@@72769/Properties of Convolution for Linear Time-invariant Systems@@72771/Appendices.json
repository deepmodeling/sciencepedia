{"hands_on_practices": [{"introduction": "This exercise connects the fundamental calculus operation of integration to the Linear Time-Invariant (LTI) system framework. You will demonstrate that a repeated integrator is an LTI system and derive its impulse response, $h_k(t)$, through autoconvolution, building a crucial bridge between system theory and integral equation models. This practice [@problem_id:2894646] sharpens skills in both formal proof and practical calculation of system responses to polynomial inputs.", "problem": "Consider the causal repeated-integration operator defined recursively on a signal $x(t)$ by $y_{0}(t)=x(t)$ and $y_{i}(t)=\\int_{0}^{t} y_{i-1}(\\tau)\\, \\mathrm{d}\\tau$ for each integer $i \\geq 1$. Let $T_{k}$ denote the $k$-fold integrator mapping $x$ to $y_{k}$, where $k \\in \\mathbb{N}$. The unit-step function is $u(t)$ and the Dirac delta is $\\delta(t)$.\n\nTasks:\n- Using only the fundamental definitions of linearity, time-invariance, and convolution for Linear Time-Invariant (LTI) systems, show that $T_{k}$ is Linear Time-Invariant (LTI) and that its impulse response $h_{k}(t)$ equals the $k$-fold autoconvolution of the unit-step function. Derive a closed-form expression for $h_{k}(t)$ from first principles.\n- Then, for $k=3$ and input $x(t)=\\big(5 t^{3}-2 t+4\\big)\\,u(t)$, compute the output $y(t)=(T_{3}x)(t)$ and simplify it to a single analytic expression in $t$ using $u(t)$.\n\nState your final answer as a single closed-form expression in $t$; no rounding is required and no units are involved. Angles, if any appear, must be in radians.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n\n-   The repeated-integration operator is defined by $y_{0}(t)=x(t)$ and $y_{i}(t)=\\int_{0}^{t} y_{i-1}(\\tau)\\, \\mathrm{d}\\tau$ for $i \\geq 1$.\n-   $T_{k}$ denotes the $k$-fold integrator, mapping an input signal $x(t)$ to an output signal $y_{k}(t) = (T_k x)(t)$. The domain for $k$ is the set of natural numbers $\\mathbb{N}$.\n-   Standard signals are the unit-step function $u(t)$ and the Dirac delta function $\\delta(t)$.\n-   Task 1: Demonstrate that $T_{k}$ is a Linear Time-Invariant (LTI) system. Show its impulse response $h_{k}(t)$ is the $k$-fold autoconvolution of $u(t)$. Derive a closed-form expression for $h_{k}(t)$ from first principles.\n-   Task 2: For $k=3$ and input $x(t)=\\big(5 t^{3}-2 t+4\\big)\\,u(t)$, compute the output $y(t)=(T_{3}x)(t)$ and present it as a single simplified analytical expression.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in linear systems theory, addressing fundamental properties like linearity, time-invariance, impulse response, and convolution. The definitions are precise, and the tasks are mathematically formalizable and solvable. The provided operator is a causal integrator, and the specified input signal is a causal polynomial, which is standard. There are no scientific or logical contradictions, no missing information, and no\nambiguities that would prevent a unique, meaningful solution.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A solution will be provided.\n\nWe address the two parts of the problem sequentially.\n\n**Part 1: Analysis of the Operator $T_k$**\n\nFirst, we must prove that the operator $T_k$ is Linear Time-Invariant (LTI).\n\n**Linearity:**\nWe will prove linearity by induction on $k$.\nA system $T$ is linear if $T(a_1 x_1(t) + a_2 x_2(t)) = a_1 (T x_1)(t) + a_2 (T x_2)(t)$.\nBase case ($k=1$): The operator is $T_1$, defined as $(T_1 x)(t) = \\int_{0}^{t} x(\\tau) d\\tau$.\nLet the input be $z(t) = a_1 x_1(t) + a_2 x_2(t)$. The output is:\n$$ (T_1 z)(t) = \\int_{0}^{t} (a_1 x_1(\\tau) + a_2 x_2(\\tau)) d\\tau $$\nBy the linearity property of the integral:\n$$ (T_1 z)(t) = a_1 \\int_{0}^{t} x_1(\\tau) d\\tau + a_2 \\int_{0}^{t} x_2(\\tau) d\\tau = a_1 (T_1 x_1)(t) + a_2 (T_1 x_2)(t) $$\nThus, $T_1$ is a linear operator.\nInductive step: Assume that $T_{k-1}$ is linear for some $k-1 \\ge 1$. We must show that $T_k$ is linear.\nThe operator $T_k$ is defined as $(T_k x)(t) = \\int_{0}^{t} (T_{k-1} x)(\\tau) d\\tau = (T_1 (T_{k-1} x))(t)$.\nLet the input be $z(t) = a_1 x_1(t) + a_2 x_2(t)$.\n$$ (T_k z)(t) = (T_1 (T_{k-1} z))(t) = \\int_{0}^{t} (T_{k-1} z)(\\tau) d\\tau $$\nBy the inductive hypothesis, $T_{k-1}$ is linear, so $(T_{k-1} z)(\\tau) = a_1 (T_{k-1} x_1)(\\tau) + a_2 (T_{k-1} x_2)(\\tau)$.\nSubstituting this into the integral:\n$$ (T_k z)(t) = \\int_{0}^{t} (a_1 (T_{k-1} x_1)(\\tau) + a_2 (T_{k-1} x_2)(\\tau)) d\\tau $$\n$$ = a_1 \\int_{0}^{t} (T_{k-1} x_1)(\\tau) d\\tau + a_2 \\int_{0}^{t} (T_{k-1} x_2)(\\tau) d\\tau = a_1 (T_k x_1)(t) + a_2 (T_k x_2)(t) $$\nThe operator $T_k$ is linear for all $k \\in \\mathbb{N}$ by induction.\n\n**Time-Invariance:**\nA system $T$ is time-invariant if for an output $y(t) = (Tx)(t)$, the output to a shifted input $x(t-t_0)$ is the shifted output $y(t-t_0)$. The operator $(T_1 x)(t) = \\int_0^t x(\\tau)d\\tau$ is only time-invariant under the constraint of causal signals, i.e., signals $x(t)$ such that $x(t)=0$ for $t<0$. This is a standard convention in system analysis involving such causal integrators.\nLet $x(t)$ be a causal signal. Let $y_k(t) = (T_k x)(t)$. We must show that $(T_k x(t-t_0)) = y_k(t-t_0)$ for any $t_0 > 0$. We proceed by induction.\nBase case ($k=1$): Let $y_1(t) = \\int_0^t x(\\tau)d\\tau$. The shifted output is $y_1(t-t_0) = \\int_0^{t-t_0} x(\\tau)d\\tau$. The output to the shifted input $x_d(t) = x(t-t_0)$ is $y_{1,d}(t) = \\int_0^t x(\\tau-t_0)d\\tau$. Since $x(t)$ is causal, $x(\\tau-t_0)$ is zero for $\\tau < t_0$. Thus, for $t \\ge t_0$, we can change the lower limit of integration to $t_0$: $y_{1,d}(t) = \\int_{t_0}^t x(\\tau-t_0)d\\tau$. Let $u=\\tau-t_0$, so $du=d\\tau$. The limits become $u=0$ and $u=t-t_0$. Then $y_{1,d}(t) = \\int_0^{t-t_0} x(u)du = y_1(t-t_0)$. For $t<t_0$, $y_{1,d}(t)=0$ and $y_1(t-t_0)=0$. Thus $T_1$ is time-invariant for causal signals.\nInductive Step: Assume $T_{k-1}$ is time-invariant for causal signals. Let $y_k(t) = (T_k x)(t)$ and let $z(t) = (T_{k-1} x)(t)$. If $x(t)$ is causal, then $z(t)$ is also causal.\nThe output to the shifted input $x(t-t_0)$ is $(T_k x(t-t_0)) = (T_1 (T_{k-1} x(t-t_0)))(t)$.\nBy the inductive hypothesis, $(T_{k-1} x(t-t_0)) = z(t-t_0)$.\nSo, $(T_k x(t-t_0)) = (T_1 z(t-t_0))(t)$. From the base case, we know $T_1$ is time-invariant for causal signals like $z(t)$, hence $(T_1 z(t-t_0))(t) = y_k(t-t_0)$.\nThus, $T_k$ is time-invariant for all $k \\in \\mathbb{N}$ for causal signals.\nSince $T_k$ is both linear and time-invariant, it is an LTI system (for causal inputs).\n\n**Impulse Response and relation to Convolution:**\nThe impulse response $h_k(t)$ is the output of $T_k$ when the input is the Dirac delta function, $x(t)=\\delta(t)$.\nFor $k=1$, $h_1(t) = \\int_0^t \\delta(\\tau) d\\tau$. This integral is $0$ for $t<0$ and $1$ for $t>0$. This is the definition of the unit-step function $u(t)$. So, $h_1(t) = u(t)$.\nThe operator $T_k$ is a cascade of $k$ identical operators $T_1$. For LTI systems, the impulse response of a cascade is the convolution of the individual impulse responses.\nTherefore, $h_k(t) = \\underbrace{h_1(t) * h_1(t) * \\dots * h_1(t)}_{k \\text{ times}}$.\nSince $h_1(t) = u(t)$, we have shown that $h_k(t)$ is the $k$-fold autoconvolution of the unit-step function:\n$$ h_k(t) = \\underbrace{u(t) * u(t) * \\dots * u(t)}_{k \\text{ times}} $$\n\n**Closed-form expression for $h_k(t)$:**\nWe derive the expression from first principles, i.e., from the recursive definition of the operator with $x(t)=\\delta(t)$.\n$h_k(t) = \\int_0^t h_{k-1}(\\tau)d\\tau$.\nWe have $h_1(t) = u(t)$.\n$h_2(t) = \\int_0^t h_1(\\tau)d\\tau = \\int_0^t u(\\tau)d\\tau = \\int_0^t 1 \\cdot d\\tau = t$ for $t>0$. So, $h_2(t) = t u(t)$.\n$h_3(t) = \\int_0^t h_2(\\tau)d\\tau = \\int_0^t \\tau u(\\tau)d\\tau = \\int_0^t \\tau d\\tau = \\frac{t^2}{2}$ for $t>0$. So, $h_3(t) = \\frac{t^2}{2} u(t)$.\nWe hypothesize the general form $h_k(t) = \\frac{t^{k-1}}{(k-1)!} u(t)$.\nWe prove this by induction.\nBase case ($k=1$): $h_1(t) = \\frac{t^{1-1}}{(1-1)!} u(t) = \\frac{t^0}{0!} u(t) = u(t)$. Correct.\nInductive step: Assume $h_{k-1}(t) = \\frac{t^{k-2}}{(k-2)!} u(t)$.\nThen $h_k(t) = \\int_0^t h_{k-1}(\\tau)d\\tau = \\int_0^t \\frac{\\tau^{k-2}}{(k-2)!} u(\\tau)d\\tau$.\nFor $t > 0$, this becomes:\n$$ h_k(t) = \\frac{1}{(k-2)!} \\int_0^t \\tau^{k-2} d\\tau = \\frac{1}{(k-2)!} \\left[ \\frac{\\tau^{k-1}}{k-1} \\right]_0^t = \\frac{1}{(k-2)!} \\frac{t^{k-1}}{k-1} = \\frac{t^{k-1}}{(k-1)!} $$\nFor $t < 0$, the integral is $0$. Thus, the expression $h_k(t) = \\frac{t^{k-1}}{(k-1)!} u(t)$ is correct for all $k \\ge 1$.\n\n**Part 2: Output Calculation**\n\nWe need to compute $y(t) = (T_3 x)(t)$ for the input $x(t)=(5t^3-2t+4)u(t)$.\nThis is equivalent to integrating the input signal three times from $0$ to $t$.\nLet $x_p(t) = 5t^3-2t+4$. The input is $x(t)=x_p(t)u(t)$.\n\nFirst integration:\n$y_1(t) = \\int_0^t x(\\tau)d\\tau = \\int_0^t (5\\tau^3-2\\tau+4)d\\tau$ for $t \\ge 0$.\n$$ y_1(t) = \\left[ 5\\frac{\\tau^4}{4} - 2\\frac{\\tau^2}{2} + 4\\tau \\right]_0^t = \\frac{5}{4}t^4 - t^2 + 4t $$\nThis result is for $t \\ge 0$, so we write $y_1(t) = (\\frac{5}{4}t^4 - t^2 + 4t)u(t)$.\n\nSecond integration:\n$y_2(t) = \\int_0^t y_1(\\tau)d\\tau = \\int_0^t (\\frac{5}{4}\\tau^4 - \\tau^2 + 4\\tau)d\\tau$ for $t \\ge 0$.\n$$ y_2(t) = \\left[ \\frac{5}{4}\\frac{\\tau^5}{5} - \\frac{\\tau^3}{3} + 4\\frac{\\tau^2}{2} \\right]_0^t = \\frac{1}{4}t^5 - \\frac{1}{3}t^3 + 2t^2 $$\nSo, $y_2(t) = (\\frac{1}{4}t^5 - \\frac{1}{3}t^3 + 2t^2)u(t)$.\n\nThird integration:\n$y_3(t) = \\int_0^t y_2(\\tau)d\\tau = \\int_0^t (\\frac{1}{4}\\tau^5 - \\frac{1}{3}\\tau^3 + 2\\tau^2)d\\tau$ for $t \\ge 0$.\n$$ y_3(t) = \\left[ \\frac{1}{4}\\frac{\\tau^6}{6} - \\frac{1}{3}\\frac{\\tau^4}{4} + 2\\frac{\\tau^3}{3} \\right]_0^t = \\frac{1}{24}t^6 - \\frac{1}{12}t^4 + \\frac{2}{3}t^3 $$\nThe final output is $y(t) = y_3(t)$, and since the process guarantees a causal output for a causal input, we must include the unit-step function.\n$$ y(t) = \\left(\\frac{1}{24}t^6 - \\frac{1}{12}t^4 + \\frac{2}{3}t^3\\right) u(t) $$\nThis is the simplified analytic expression for the output signal.", "answer": "$$\\boxed{\\left(\\frac{1}{24}t^{6} - \\frac{1}{12}t^{4} + \\frac{2}{3}t^{3}\\right)u(t)}$$", "id": "2894646"}, {"introduction": "In practice, linear convolution is often implemented efficiently using algorithms like the Fast Fourier Transform (FFT), which mathematically compute *circular* convolution. This exercise explores the precise relationship between the two, revealing the \"wrap-around\" or aliasing effect that occurs when the circular convolution length $N$ is insufficient. By deriving the time-domain aliasing formula from first principles [@problem_id:2894647], you will gain an essential understanding for correctly implementing FFT-based filtering and avoiding common artifacts in digital signal processing.", "problem": "Consider two finite-length discrete-time signals in a linear time-invariant setting, and let $y[n]$ denote their linear convolution. Let $\\tilde{y}[n]$ denote the $N$-point circular convolution of the same signals, obtained by evaluating the convolution sum using period-$N$ extensions of the signals (as is done when one uses the $N$-point Discrete Fourier Transform (DFT) without sufficient zero-padding). Work from first principles: use only the definitions of linear convolution and $N$-point circular convolution as your starting point, and reason from those definitions.\n\nPart 1 (derivation): Starting from the definitions of linear convolution and $N$-point circular convolution in the time domain, derive an explicit identity that shows how circular-convolution aliasing arises from wrap-around, i.e., show that for $n \\in \\{0,1,\\dots,N-1\\}$, the $N$-point circular convolution can be written as a sum of shifted copies of the linear convolution $y[n]$. Your derivation must not invoke any transform-domain convolution theorems; it must proceed from the time-domain definitions.\n\nPart 2 (computation of the aliasing term): Let $x[n]$ and $h[n]$ be the finite-length sequences\n$$\nx[n] = \\begin{cases}\n1, & 0 \\le n \\le 4,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad\nh[n] = \\begin{cases}\n1, & 0 \\le n \\le 3,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nLet $N=6$. Define the aliasing term $a[n]$ for $n \\in \\{0,1,\\dots,N-1\\}$ by\n$$\na[n] \\triangleq \\tilde{y}[n] - y[n],\n$$\nwhere by convention $y[n]=0$ outside its natural support. Using your identity from Part $1$, compute $a[n]$ exactly for $n \\in \\{0,1,\\dots,5\\}$, and present your result as a length-$6$ row vector in the order $n=0,1,2,3,4,5$. No rounding is required, and no units are involved.", "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in the field of digital signal processing, grounded in established mathematical definitions and free of scientific or logical flaws. We may proceed with the solution.\n\nThis problem demands a rigorous, time-domain derivation of the relationship between linear and circular convolution, followed by a direct computation based on this derived identity.\n\nPart 1: Derivation of the Aliasing Identity\n\nWe begin from first principles, namely the definitions of linear and $N$-point circular convolution.\n\nLet $x[n]$ and $h[n]$ be two discrete-time sequences of finite length. Let the support of $x[n]$ be $\\{0, 1, \\dots, L_x-1\\}$ and the support of $h[n]$ be $\\{0, 1, \\dots, L_h-1\\}$.\n\nThe linear convolution, denoted $y[n]$, is defined as:\n$$\ny[n] = (x * h)[n] = \\sum_{k=-\\infty}^{\\infty} x[k] h[n-k]\n$$\nGiven the finite, causal nature of the signals, the effective summation range for $k$ is from $0$ to $n$. The resulting sequence $y[n]$ has a finite support of length $L_y = L_x + L_h - 1$, specifically for $n \\in \\{0, 1, \\dots, L_x+L_h-2\\}$.\n\nThe $N$-point circular convolution, denoted $\\tilde{y}[n]$, is defined for $n \\in \\{0, 1, \\dots, N-1\\}$. The problem specifies it is obtained using period-$N$ extensions of the signals. This is equivalent to performing the convolution operation in the ring of integers modulo $N$. A direct time-domain definition is:\n$$\n\\tilde{y}[n] = \\sum_{k=0}^{N-1} x_N[k] h_N[(n-k) \\pmod N]\n$$\nwhere $x_N[n]$ and $h_N[n]$ are the sequences $x[n]$ and $h[n]$ respectively, considered over the interval $n \\in \\{0, 1, \\dots, N-1\\}$. If the original sequences have lengths less than or equal to $N$, as is the case in Part 2, then $x_N[n] = x[n]$ and $h_N[n] = h[n]$ within this interval and are zero otherwise where relevant.\n\nThe expression $h_N[(n-k) \\pmod N]$ represents the value of the periodic extension of $h_N[n]$ at time index $n-k$. Let us denote this periodic sequence as $h_{N,p}[m]$. This sequence can be constructed from the finite-length sequence $h_N[m]$ by summation:\n$$\nh_{N,p}[m] = \\sum_{r=-\\infty}^{\\infty} h_N[m - rN]\n$$\nThen, we can write $h_N[(n-k) \\pmod N] = h_{N,p}[n-k]$. Substituting this into the definition of circular convolution:\n$$\n\\tilde{y}[n] = \\sum_{k=0}^{N-1} x_N[k] h_{N,p}[n-k]\n$$\nNow, substitute the summation form of $h_{N,p}[n-k]$:\n$$\n\\tilde{y}[n] = \\sum_{k=0}^{N-1} x_N[k] \\left( \\sum_{r=-\\infty}^{\\infty} h_N[(n-k)-rN] \\right)\n$$\nThe summations are over finite and absolutely convergent series, so we may interchange their order:\n$$\n\\tilde{y}[n] = \\sum_{r=-\\infty}^{\\infty} \\sum_{k=0}^{N-1} x_N[k] h_N[n-k-rN]\n$$\nLet us examine the inner sum. Since the support of $x_N[k]$ is contained within $\\{0, 1, \\dots, N-1\\}$, the limits of summation over $k$ can be extended to $\\pm\\infty$ without changing the value:\n$$\n\\sum_{k=0}^{N-1} x_N[k] h_N[n-k-rN] = \\sum_{k=-\\infty}^{\\infty} x_N[k] h_N[n-k-rN]\n$$\nThis expression is precisely the linear convolution of $x_N[n]$ and $h_N[n]$, evaluated at the time index $n-rN$. Let us call this linear convolution $y_N[n] = (x_N * h_N)[n]$.\n$$\n\\sum_{k=-\\infty}^{\\infty} x_N[k] h_N[n-k-rN] = y_N[n-rN]\n$$\nIf we assume, as is true in Part 2, that the lengths of the original signals are less than $N$ (i.e., $L_x \\le N$ and $L_h \\le N$), then $x_N[n]=x[n]$ and $h_N[n]=h[n]$. Consequently, their linear convolution is identical to the original, $y_N[n] = y[n]$.\n\nSubstituting this back into our expression for $\\tilde{y}[n]$ gives the final identity:\n$$\n\\tilde{y}[n] = \\sum_{r=-\\infty}^{\\infty} y[n-rN]\n$$\nFor consistency with common literature, we can change the summation index by letting $l = -r$:\n$$\n\\tilde{y}[n] = \\sum_{l=-\\infty}^{\\infty} y[n+lN]\n$$\nThis identity demonstrates that the $N$-point circular convolution $\\tilde{y}[n]$ for $n \\in \\{0, 1, \\dots, N-1\\}$ is formed by summing the linear convolution $y[n]$ and all its versions shifted by integer multiples of $N$. This phenomenon is the time-domain aliasing or \"wrap-around\" effect.\n\nPart 2: Computation of the Aliasing Term\n\nWe are given the signals:\n$$\nx[n] = \\begin{cases}\n1, & 0 \\le n \\le 4,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad\nh[n] = \\begin{cases}\n1, & 0 \\le n \\le 3,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe lengths are $L_x=5$ and $L_h=4$. The circular convolution is performed with $N=6$.\n\nFirst, we compute the linear convolution $y[n] = (x * h)[n]$. This is the convolution of two rectangular pulses of amplitudes $1$ and lengths $5$ and $4$. The result is a trapezoidal sequence of length $L_y = L_x+L_h-1 = 5+4-1=8$. The support is $n \\in \\{0, 1, \\dots, 7\\}$.\nThe values are found by evaluating the convolution sum $y[n] = \\sum_{k=\\max(0, n-3)}^{\\min(4, n)} 1$.\nFor $n=0$: $y[0] = 1$.\nFor $n=1$: $y[1] = 2$.\nFor $n=2$: $y[2] = 3$.\nFor $n=3$: $y[3] = 4$.\nFor $n=4$: $y[4] = 4$.\nFor $n=5$: $y[5] = 3$.\nFor $n=6$: $y[6] = 2$.\nFor $n=7$: $y[7] = 1$.\nFor all other $n$, $y[n]=0$. In sequence form: $y[n] = \\{1, 2, 3, 4, 4, 3, 2, 1\\}$ for $n \\in \\{0, \\dots, 7\\}$.\n\nThe aliasing term is defined as $a[n] = \\tilde{y}[n] - y[n]$ for $n \\in \\{0, 1, \\dots, N-1\\}$. Using our derived identity:\n$$\na[n] = \\left( \\sum_{l=-\\infty}^{\\infty} y[n+lN] \\right) - y[n]\n$$\nThe $l=0$ term is $y[n]$, which cancels.\n$$\na[n] = \\sum_{l \\neq 0, l=-\\infty}^{\\infty} y[n+lN]\n$$\nThis can be split into sums for positive and negative $l$:\n$$\na[n] = \\sum_{l=1}^{\\infty} y[n+lN] + \\sum_{l=1}^{\\infty} y[n-lN]\n$$\nSince $y[n]$ is causal ($y[m]=0$ for $m<0$), and we are evaluating $a[n]$ for $n \\in \\{0, 1, \\dots, N-1\\}$, the term $n-lN$ will always be negative for $l \\ge 1$. For example, with $N=6$ and $l=1$, $n-6$ is negative for $n \\in \\{0, \\dots, 5\\}$. Thus, the second sum is zero.\n$$\na[n] = \\sum_{l=1}^{\\infty} y[n+lN] = y[n+N] + y[n+2N] + \\dots\n$$\nWe have $N=6$. The support of $y[n]$ is $\\{0, \\dots, 7\\}$.\nFor any $n \\ge 0$, $n+2N = n+12$ is greater than $7$, so $y[n+12]$ and all higher terms are zero.\nThe aliasing term simplifies to:\n$$\na[n] = y[n+6]\n$$\nWe compute this for $n \\in \\{0, 1, 2, 3, 4, 5\\}$:\n$a[0] = y[0+6] = y[6] = 2$.\n$a[1] = y[1+6] = y[7] = 1$.\n$a[2] = y[2+6] = y[8] = 0$.\n$a[3] = y[3+6] = y[9] = 0$.\n$a[4] = y[4+6] = y[10] = 0$.\n$a[5] = y[5+6] = y[11] = 0$.\n\nThe resulting aliasing term $a[n]$ for $n \\in \\{0, \\dots, 5\\}$ is the sequence $\\{2, 1, 0, 0, 0, 0\\}$. As a row vector, this is $\\begin{pmatrix} 2 & 1 & 0 & 0 & 0 & 0 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 1 & 0 & 0 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "2894647"}, {"introduction": "Ideal convolution assumes infinite-length signals, but real-world applications provide only finite data segments, for example on an interval $[0, T]$. This practice examines the consequences of this limitation by analyzing the errors introduced by different \"padding\" strategies used to define the signal outside its measured domain. By quantifying the boundary errors for zero, periodic, and reflective padding [@problem_id:2894680], you will develop a practical intuition for the trade-offs involved, a critical skill in fields like image processing and numerical simulation.", "problem": "Consider a continuous-time, linear time-invariant (LTI) system with impulse response $h(t)$ and input $x(t)$. The output $y(t)$ for an input $x(t)$ is given by the continuous-time convolution integral. Let the true input be $x(t)=t$ for $t \\ge 0$ and $x(t)=0$ for $t<0$. Let the impulse response be the symmetric moving-average kernel of half-width $L>0$, defined by $h(t)=\\frac{1}{2L}$ for $|t|\\le L$ and $h(t)=0$ otherwise. Suppose one only has access to the finite segment of the input on the interval $[0,T]$, with $T>L>0$. To approximate the system output at the right endpoint $t=T$ from this finite segment, three padding strategies are used to extend $x(t)$ outside $[0,T]$ before performing convolution with $h(t)$:\n\n- Zero padding: define $x_{0}(t)=x(t)$ for $t \\in [0,T]$ and $x_{0}(t)=0$ otherwise.\n- Periodic padding: define $x_{p}(t)=x(t-kT)$ where $k \\in \\mathbb{Z}$ is the unique integer such that $t-kT \\in [0,T)$.\n- Reflective padding: define $x_{r}(t)=x(t)$ for $t \\in [0,T]$ and $x_{r}(t)=x(2T-t)$ for $t \\in [T,2T]$, with the understanding that $T>L$ ensures $t \\in [T,2T]$ suffices for evaluating the convolution at $t=T$.\n\nLet $y(t)=(x*h)(t)$ denote the ideal infinite-interval output, and let $y_{0}(t)=(x_{0}*h)(t)$, $y_{p}(t)=(x_{p}*h)(t)$, and $y_{r}(t)=(x_{r}*h)(t)$ denote the outputs obtained after zero, periodic, and reflective padding, respectively. Define the pointwise absolute errors at the boundary time $t=T$ as $E_{0}=|y_{0}(T)-y(T)|$, $E_{p}=|y_{p}(T)-y(T)|$, and $E_{r}=|y_{r}(T)-y(T)|$. Compute the scalar quantity\n$$\nR \\equiv E_{0}+E_{p}+E_{r}\n$$\nas a closed-form analytic expression in terms of $T$ and $L$. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and free from contradiction or ambiguity. It constitutes a standard exercise in the analysis of linear time-invariant systems. We shall proceed directly to a formal derivation of the solution.\n\nThe output $y(t)$ of the system is given by the convolution integral $y(t) = (x*h)(t) = \\int_{-\\infty}^{\\infty} x(\\tau) h(t-\\tau) d\\tau$. The impulse response is specified as $h(t) = \\frac{1}{2L}$ for $|t| \\le L$ and $h(t)=0$ otherwise. The support of $h(t-\\tau)$ as a function of $\\tau$ is for $|t-\\tau| \\le L$, which is equivalent to $t-L \\le \\tau \\le t+L$. Consequently, the convolution becomes a moving average operation:\n$$\ny(t) = \\frac{1}{2L} \\int_{t-L}^{t+L} x(\\tau) d\\tau\n$$\nWe are required to evaluate the system outputs at the time $t=T$. For the true input signal $x(t)$, the ideal output is:\n$$\ny(T) = \\frac{1}{2L} \\int_{T-L}^{T+L} x(\\tau) d\\tau\n$$\nFor an input signal $x_i(t)$ obtained through some padding strategy, the corresponding output is:\n$$\ny_i(T) = \\frac{1}{2L} \\int_{T-L}^{T+L} x_i(\\tau) d\\tau\n$$\nThe absolute error for the $i$-th padding strategy at $t=T$ is $E_i = |y_i(T) - y(T)|$. By substituting the integral expressions, we obtain a general formula for the error:\n$$\nE_i = \\left| \\frac{1}{2L} \\int_{T-L}^{T+L} x_i(\\tau) d\\tau - \\frac{1}{2L} \\int_{T-L}^{T+L} x(\\tau) d\\tau \\right| = \\frac{1}{2L} \\left| \\int_{T-L}^{T+L} (x_i(\\tau) - x(\\tau)) d\\tau \\right|\n$$\nAll padding strategies are defined such that $x_i(\\tau) = x(\\tau)$ for $\\tau \\in [0,T]$. The problem specifies that $T > L > 0$, from which it follows that $T-L > 0$. Therefore, on the subinterval $[T-L, T]$, the integrand $(x_i(\\tau) - x(\\tau))$ is identically zero. The error integral thus reduces to an integral over the remaining subinterval, $(T, T+L]$:\n$$\nE_i = \\frac{1}{2L} \\left| \\int_{T}^{T+L} (x_i(\\tau) - x(\\tau)) d\\tau \\right|\n$$\nThe true input signal is given by $x(t) = t$ for $t \\ge 0$. So, for the integration domain $\\tau \\in (T, T+L]$, we have $x(\\tau)=\\tau$. We now compute the error for each padding scheme.\n\n1.  Zero Padding ($E_0$):\n    The zero-padded signal is defined as $x_0(t) = 0$ for $t > T$. For $\\tau \\in (T, T+L]$, we have $x_0(\\tau)=0$. The error integral is:\n    $$\n    \\int_{T}^{T+L} (x_0(\\tau) - x(\\tau)) d\\tau = \\int_{T}^{T+L} (0 - \\tau) d\\tau = - \\left[ \\frac{\\tau^2}{2} \\right]_{T}^{T+L} = -\\frac{1}{2}((T+L)^2 - T^2) = -\\frac{1}{2}(2TL + L^2) = -(TL + \\frac{L^2}{2})\n    $$\n    The absolute error is:\n    $$\n    E_0 = \\frac{1}{2L} \\left| -(TL + \\frac{L^2}{2}) \\right| = \\frac{TL + \\frac{1}{2}L^2}{2L} = \\frac{T}{2} + \\frac{L}{4}\n    $$\n\n2.  Periodic Padding ($E_p$):\n    The periodic-padded signal is $x_p(t) = x(t-kT)$ for the unique integer $k$ such that $t-kT \\in [0,T)$. For $\\tau \\in (T, T+L]$, we choose $k=1$, which gives $\\tau-T \\in (0, L]$. Since $L<T$, this range is within $[0,T)$, so the definition applies. Thus, $x_p(\\tau)=x(\\tau-T)$. As $\\tau-T>0$, $x(\\tau-T)=\\tau-T$. The error integral becomes:\n    $$\n    \\int_{T}^{T+L} (x_p(\\tau) - x(\\tau)) d\\tau = \\int_{T}^{T+L} ((\\tau-T)-\\tau) d\\tau = \\int_{T}^{T+L} (-T) d\\tau = -T[\\tau]_{T}^{T+L} = -TL\n    $$\n    The absolute error is:\n    $$\n    E_p = \\frac{1}{2L} |-TL| = \\frac{TL}{2L} = \\frac{T}{2}\n    $$\n\n3.  Reflective Padding ($E_r$):\n    The reflectively-padded signal is $x_r(t) = x(2T-t)$ for $t \\in [T, 2T]$. The condition $T>L$ implies $T+L < 2T$, so the interval of interest $(T, T+L]$ is contained within this domain. For $\\tau \\in (T, T+L]$, we have $x_r(\\tau)=x(2T-\\tau)$. The argument $2T-\\tau$ is in the range $[T-L, T)$, which is a subset of $[0,T]$. Therefore, $x(2T-\\tau)=2T-\\tau$. The error integral is:\n    $$\n    \\int_{T}^{T+L} (x_r(\\tau) - x(\\tau)) d\\tau = \\int_{T}^{T+L} ((2T-\\tau)-\\tau) d\\tau = \\int_{T}^{T+L} (2T - 2\\tau) d\\tau\n    $$\n    $$\n    = \\left[ 2T\\tau - \\tau^2 \\right]_{T}^{T+L} = (2T(T+L) - (T+L)^2) - (2T^2 - T^2) = (2T^2+2TL-T^2-2TL-L^2) - T^2 = -L^2\n    $$\n    The absolute error is:\n    $$\n    E_r = \\frac{1}{2L} |-L^2| = \\frac{L^2}{2L} = \\frac{L}{2}\n    $$\n\nThe total quantity $R$ is the sum of these absolute errors:\n$$\nR = E_0 + E_p + E_r = \\left(\\frac{T}{2} + \\frac{L}{4}\\right) + \\frac{T}{2} + \\frac{L}{2} = T + \\frac{3L}{4}\n$$\nThis is the final analytical expression.", "answer": "$$\n\\boxed{T + \\frac{3L}{4}}\n$$", "id": "2894680"}]}