{"hands_on_practices": [{"introduction": "A core tenet of system identification is that the input signal must be sufficiently \"rich\" to allow the unique determination of all model parameters. This property, known as persistent excitation (PE), is essential for the least squares regression matrix to be invertible. This exercise provides a hands-on demonstration of what happens when the PE condition is not met, using the common case of a single-tone sinusoidal input. By deriving from first principles the dimension of the resulting unidentifiable parameter subspace, you will gain a tangible understanding of why input signal design is a critical first step for any identification experiment [@problem_id:2880081].", "problem": "Consider a discrete-time, linear time-invariant finite impulse response (FIR) model of order $m$ with input-output relation $y[k] = \\sum_{\\ell=0}^{m-1} h_{\\ell} u[k-\\ell] + v[k]$, where $h_{\\ell}$ are unknown real coefficients, $u[k]$ is a known input, and $v[k]$ is additive, zero-mean measurement noise uncorrelated with $u[k]$. You collect $N$ consecutive samples $\\{(u[k], y[k])\\}_{k=0}^{N-1}$ with $N \\geq m+1$. Let the regressor vector be $x[k] \\triangleq [u[k], u[k-1], \\dots, u[k-m+1]]^{\\top} \\in \\mathbb{R}^{m}$, and the least-squares normal matrix be $S_N \\triangleq \\sum_{k=m-1}^{N-1} x[k] x[k]^{\\top} \\in \\mathbb{R}^{m \\times m}$. The least-squares estimator of $h = [h_0, \\dots, h_{m-1}]^{\\top}$ requires $S_N$ to be invertible, and persistent excitation of order $m$ is equivalent (in the noise-free limit) to $S_N$ being positive definite. Suppose you drive the system with a single-tone sinusoidal input $u[k] = A \\cos(\\omega k + \\phi)$ of nonzero amplitude $A \\neq 0$, with frequency $\\omega \\in (0, \\pi)$ and arbitrary phase $\\phi \\in \\mathbb{R}$. Assume $m \\geq 3$, and interpret $S_N$ in the large-sample sense $N \\to \\infty$ so that sample averages converge to their expectations under stationarity of $u[k]$.\n\nStarting only from the above setup, the linearity of the FIR model, the definition of the normal matrix $S_N$, and trigonometric angle-sum identities, determine the exact dimension (as a function of $m$) of the coefficient subspace that is unidentifiable by least squares from $\\{(u[k], y[k])\\}$ when $u[k]$ is a single-tone sinusoid as specified. Express your final answer as a single closed-form expression in terms of $m$. No units are required, and no rounding is necessary.", "solution": "The problem asks for the dimension of the unidentifiable subspace of the coefficient vector $h = [h_0, h_1, \\dots, h_{m-1}]^{\\top}$ for a finite impulse response (FIR) model of order $m$ when the input is a single-tone sinusoid. The FIR model is given by the relation $y[k] = \\sum_{\\ell=0}^{m-1} h_{\\ell} u[k-\\ell] + v[k]$.\n\nA coefficient vector $h$ is unidentifiable if its contribution to the noise-free output is identically zero. This is the condition under which different parameter vectors can produce the same output, making them indistinguishable from the input-output data. Mathematically, this corresponds to the set of all vectors $h \\in \\mathbb{R}^{m}$ such that the regressor-parameter product is zero for all time $k$. The space of such vectors $h$ is precisely the null space of the asymptotic least-squares normal matrix $S = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k} x[k]x[k]^{\\top}$, where $x[k]$ is the regressor vector. We are therefore tasked with finding the dimension of this null space.\n\nThe condition for a vector $h$ to be in the unidentifiable subspace is:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} u[k-\\ell] = 0 \\quad \\text{for all } k\n$$\nThe input signal is given as $u[k] = A \\cos(\\omega k + \\phi)$, where the amplitude $A$ is non-zero, the frequency $\\omega \\in (0, \\pi)$, and the phase $\\phi$ is some real constant. Substituting this into the condition, we obtain:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} A \\cos(\\omega(k-\\ell) + \\phi) = 0\n$$\nSince $A \\neq 0$, we can divide by it:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} \\cos(\\omega k - \\omega \\ell + \\phi) = 0\n$$\nWe apply the trigonometric angle-sum identity, $\\cos(\\alpha - \\beta) = \\cos(\\alpha)\\cos(\\beta) + \\sin(\\alpha)\\sin(\\beta)$. Let $\\alpha = \\omega k + \\phi$ and $\\beta = \\omega \\ell$. The equation becomes:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} \\left( \\cos(\\omega k + \\phi)\\cos(\\omega \\ell) + \\sin(\\omega k + \\phi)\\sin(\\omega \\ell) \\right) = 0\n$$\nWe can rearrange the summation by factoring out the terms that depend on $k$:\n$$\n\\cos(\\omega k + \\phi) \\left( \\sum_{\\ell=0}^{m-1} h_{\\ell} \\cos(\\omega \\ell) \\right) + \\sin(\\omega k + \\phi) \\left( \\sum_{\\ell=0}^{m-1} h_{\\ell} \\sin(\\omega \\ell) \\right) = 0\n$$\nThis equation must hold for all time steps $k$. The functions of time, $f_1(k) = \\cos(\\omega k + \\phi)$ and $f_2(k) = \\sin(\\omega k + \\phi)$, are linearly independent for $\\omega \\in (0, \\pi)$. For a linear combination of linearly independent functions to be zero for all values of the independent variable, the coefficients of the functions must all be zero. This yields two independent linear constraints on the coefficient vector $h = [h_0, h_1, \\dots, h_{m-1}]^{\\top}$:\n\nConstraint 1:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} \\cos(\\omega \\ell) = 0\n$$\nConstraint 2:\n$$\n\\sum_{\\ell=0}^{m-1} h_{\\ell} \\sin(\\omega \\ell) = 0\n$$\nThe unidentifiable subspace is the solution space of this system of two linear equations for the $m$ unknowns $\\{h_0, h_1, \\dots, h_{m-1}\\}$. To find the dimension of this solution space, we must verify that the two constraints are indeed linearly independent.\n\nWe can write these constraints in vector form. Let $c = [\\cos(0), \\cos(\\omega), \\dots, \\cos((m-1)\\omega)]^{\\top}$ and $s = [\\sin(0), \\sin(\\omega), \\dots, \\sin((m-1)\\omega)]^{\\top}$. The system of equations is:\n$$\nc^{\\top} h = 0\n$$\n$$\ns^{\\top} h = 0\n$$\nThe linear independence of these two constraints is equivalent to the linear independence of the vectors $c$ and $s$. Let us assume a linear combination of $c$ and $s$ is the zero vector: $\\alpha c + \\beta s = 0$, for some scalars $\\alpha, \\beta \\in \\mathbb{R}$.\n\nWriting this out component-wise for the element at index $\\ell=0$:\n$$\n\\alpha \\cos(0) + \\beta \\sin(0) = 0 \\implies \\alpha \\cdot 1 + \\beta \\cdot 0 = 0 \\implies \\alpha = 0\n$$\nWith $\\alpha=0$, the condition reduces to $\\beta s = 0$. The vector $s$ is the zero vector if and only if $\\sin(\\omega \\ell) = 0$ for all $\\ell = 1, \\dots, m-1$. This would require $\\omega \\ell$ to be a multiple of $\\pi$ for each such $\\ell$. For $\\ell=1$, this implies $\\omega$ must be a multiple of $\\pi$. However, the problem specifies that $\\omega \\in (0, \\pi)$, which explicitly excludes $\\omega$ being a multiple of $\\pi$. Therefore, the vector $s$ is not the zero vector.\nSince $\\beta s = 0$ and $s \\neq 0$, it must be that $\\beta = 0$.\nThe only solution is $\\alpha=0$ and $\\beta=0$, which proves that the vectors $c$ and $s$ are linearly independent.\n\nWe have a system of two linearly independent homogeneous equations in $m$ variables. The dimension of the solution space (the unidentifiable subspace) is given by the number of variables minus the number of independent constraints.\n$$\n\\text{Dimension of unidentifiable subspace} = m - 2\n$$\nThis result holds for $m \\geq 2$. The problem condition $m \\geq 3$ is a specific case where this is valid. The dimension of the subspace of coefficient vectors that are unidentifiable by least squares from the given sinusoidal input is therefore $m-2$.", "answer": "$$\n\\boxed{m-2}\n$$", "id": "2880081"}, {"introduction": "While the standard least squares formulation assumes uncorrelated, or \"white,\" measurement noise, many real-world processes are better described by models with colored noise, such as the ARMAX structure. This coloration introduces a correlation between the regressors and the error term, which violates a key assumption and leads to biased parameter estimates. This practice guides you through the fundamental technique of prefiltering, showing how to design a \"whitening\" filter that mathematically restores the conditions necessary for least squares to yield consistent estimates [@problem_id:2880109].", "problem": "Consider a single-input single-output, discrete-time Autoregressive Moving-Average with eXogenous input (ARMAX) model driven by a zero-mean, independent and identically distributed Gaussian sequence,\n$$A(q) \\, y(t) \\;=\\; q^{-n_k} B(q) \\, u(t) \\;+\\; C(q) \\, e(t),$$\nwhere $q^{-1}$ is the unit-delay shift operator, $A(q) = 1 + a_{1} q^{-1} + \\cdots + a_{n_{a}} q^{-n_{a}}$, $B(q) = b_{0} + b_{1} q^{-1} + \\cdots + b_{n_{b}} q^{-n_{b}}$, $C(q) = 1 + c_{1} q^{-1} + \\cdots + c_{n_{c}} q^{-n_{c}}$ is known, monic, stable, and minimum-phase, $n_k \\in \\mathbb{N}$ is the known input delay, the input $u(t)$ is independent of $e(t)$ and persistently exciting of a sufficiently high order, and the data $\\{u(t),y(t)\\}$ are generated by a stable, time-invariant system such that standard regularity conditions for consistency of least squares hold (bounded moments, ergodicity, and identifiability). The goal is to estimate the plant parameter vector $\\theta_{p} = \\begin{pmatrix} a_{1} & \\cdots & a_{n_{a}} & b_{0} & \\cdots & b_{n_{b}} \\end{pmatrix}^{\\top}$ by ordinary least squares on a linear regression.\n\nDerive, from first principles, a causal, stable, linear time-invariant prefilter $L(q)$, expressed explicitly in terms of $C(q)$ only and implementable from the known $C(q)$, such that when $L(q)$ is applied to both sides of the ARMAX equation, the resulting linear regression\n$$y_{L}(t) \\;=\\; \\varphi_{L}(t)^{\\top} \\theta_{p} \\;+\\; \\varepsilon_{L}(t)$$\nhas a white zero-mean disturbance $\\varepsilon_{L}(t)$ that is uncorrelated with the regressor $\\varphi_{L}(t)$, thereby yielding a consistent ordinary least squares estimate of $\\theta_{p}$ under the stated assumptions. Your derivation must start from the model definition and the stated assumptions, and must justify why the disturbance becomes white and why it is uncorrelated with the regressor after prefiltering.\n\nThe final answer must be the analytic expression for the prefilter $L(q)$ in terms of $C(q)$. Do not provide any intermediate expressions or multiple expressions as your final answer; give only the single operator $L(q)$ as a closed-form analytic expression. No numerical rounding is required for this problem.", "solution": "The starting point is the ARMAX model equation:\n$$A(q) \\, y(t) = q^{-n_k} B(q) \\, u(t) + C(q) \\, e(t)$$\nOur goal is to estimate the parameters of $A(q)$ and $B(q)$, which are contained in the vector $\\theta_{p}$. We can rewrite the model to resemble a standard linear regression form. Using the fact that $A(q) = 1 + (A(q) - 1)$, we have:\n$$y(t) + (A(q) - 1)y(t) = q^{-n_k} B(q) u(t) + C(q) e(t)$$\nIsolating $y(t)$ yields:\n$$y(t) = -(A(q) - 1)y(t) + q^{-n_k} B(q) u(t) + C(q) e(t)$$\nThis equation has the structure of a linear regression, which can be expressed as:\n$$y(t) = \\varphi(t)^{\\top} \\theta_{p} + v(t)$$\nwhere the regressor vector $\\varphi(t)$ and the parameter vector $\\theta_p$ are given by:\n$$\\varphi(t)^{\\top} = \\begin{pmatrix} -y(t-1) & \\cdots & -y(t-n_a) & u(t-n_k) & \\cdots & u(t-n_k-n_b) \\end{pmatrix}$$\n$$\\theta_{p} = \\begin{pmatrix} a_{1} & \\cdots & a_{n_{a}} & b_{0} & \\cdots & b_{n_{b}} \\end{pmatrix}^{\\top}$$\nand the equation error, or disturbance, is:\n$$v(t) = C(q) e(t) = (1 + c_{1} q^{-1} + \\cdots + c_{n_{c}} q^{-n_{c}}) e(t) = e(t) + c_{1} e(t-1) + \\cdots + c_{n_{c}} e(t-n_{c})$$\nFor the Ordinary Least Squares (OLS) estimator of $\\theta_p$ to be consistent, a necessary condition is that the disturbance $v(t)$ must be uncorrelated with the regressor vector $\\varphi(t)$, i.e., $E[\\varphi(t) v(t)] = \\mathbf{0}$.\n\nLet us examine this condition. The disturbance $v(t)$ is a moving average of the white noise sequence $e(t)$. Since $C(q)$ is not equal to $1$ (in the general case), $v(t)$ is a colored noise sequence. The regressor $\\varphi(t)$ contains past values of the output, $y(t-k)$ for $k \\ge 1$. The output $y(t)$ is a function of all past inputs $u(s)$ and noises $e(s)$ for $s \\le t$. Therefore, a regressor component like $y(t-1)$ depends on $e(t-1), e(t-2), \\dots$. The disturbance $v(t)$ also depends on $e(t-1), e(t-2), \\dots$ due to the terms $c_1 e(t-1), c_2 e(t-2), \\dots$. Consequently, $y(t-1)$ and $v(t)$ are correlated. This leads to $E[\\varphi(t) v(t)] \\neq \\mathbf{0}$, which violates the consistency condition for OLS. The OLS estimate of $\\theta_p$ will be biased and inconsistent.\n\nTo resolve this issue, we must prefilter the system equation such that the new disturbance term is white and uncorrelated with the new regressor. Let $L(q)$ be a causal, stable, linear time-invariant filter. Applying this prefilter to both sides of the original ARMAX equation gives:\n$$L(q) A(q) y(t) = L(q) q^{-n_k} B(q) u(t) + L(q) C(q) e(t)$$\nThe new disturbance term is $\\varepsilon_{L}(t) = L(q) C(q) e(t)$. To make this disturbance a white noise sequence, we must choose $L(q)$ such that the operator $L(q) C(q)$ does not introduce any temporal correlation. Since $e(t)$ is already white noise, the simplest choice for $L(q) C(q)$ is the identity operator, $1$. Thus, we require:\n$$L(q) C(q) = 1$$\nThis implies that the prefilter must be the inverse of the noise dynamics operator $C(q)$:\n$$L(q) = C(q)^{-1} = \\frac{1}{C(q)}$$\nThis choice is feasible because the problem states that $C(q)$ is known. Furthermore, the problem states that $C(q)$ is a stable and monic polynomial operator. In system identification, a polynomial operator $P(q)$ is called stable if all roots of the polynomial $P(z)=0$ lie strictly inside the unit circle $|z|<1$. The stability of the filter $L(q) = 1/C(q)$ is determined by its poles, which are the roots of $C(z)=0$. Since $C(q)$ is stable, the filter $L(q)$ is guaranteed to be stable. The monic property of $C(q)$ (i.e., $C(q) = 1 + \\sum_{i=1}^{n_c} c_i q^{-i}$) ensures that $L(q)$ is causal. The property of being minimum-phase is related and consistent with this stability requirement.\n\nWith this choice of $L(q) = C(q)^{-1}$, the prefiltered equation becomes:\n$$C(q)^{-1} A(q) y(t) = C(q)^{-1} q^{-n_k} B(q) u(t) + e(t)$$\nLet us define the prefiltered output and input signals as $y_{L}(t) = C(q)^{-1} y(t)$ and $u_{L}(t) = C(q)^{-1} u(t)$. The equation can be written as:\n$$A(q) y_{L}(t) = q^{-n_k} B(q) u_{L}(t) + e(t)$$\nRearranging this into a regression form gives:\n$$y_{L}(t) = -(A(q) - 1) y_{L}(t) + q^{-n_k} B(q) u_{L}(t) + e(t)$$\nThis matches the desired regression model $y_{L}(t) = \\varphi_{L}(t)^{\\top} \\theta_{p} + \\varepsilon_{L}(t)$, where:\n- The new disturbance is $\\varepsilon_{L}(t) = e(t)$. By definition, $e(t)$ is a zero-mean white noise sequence.\n- The new regressor is $\\varphi_{L}(t)^{\\top} = \\begin{pmatrix} -y_{L}(t-1) & \\cdots & -y_{L}(t-n_a) & u_{L}(t-n_k) & \\cdots & u_{L}(t-n_k-n_b) \\end{pmatrix}$.\n\nWe must now verify that the new regressor $\\varphi_{L}(t)$ is uncorrelated with the new disturbance $\\varepsilon_{L}(t) = e(t)$. The components of $\\varphi_{L}(t)$ are $y_L(t-j)$ and $u_L(t-k-j)$ for $j \\ge 1$. Since $L(q)=C(q)^{-1}$ is a causal filter, $y_L(t-j)$ and $u_L(t-k-j)$ are functions of current and past values of $y$ and $u$ up to time $t-j$. Specifically, $y_{L}(t-j)$ depends on $\\{y(s) | s \\le t-j\\}$ and $u_{L}(t-k-j)$ depends on $\\{u(s) | s \\le t-k-j\\}$. Because the overall system is causal and stable, the output $y(s)$ depends only on inputs $u(\\tau)$ and noises $e(\\tau)$ for $\\tau \\le s$.\nTherefore, all components of the regressor $\\varphi_{L}(t)$ are functions of $\\{u(\\tau), e(\\tau) | \\tau \\le t-1\\}$. The disturbance is $e(t)$. Since $e(t)$ is an i.i.d. sequence, it is by definition uncorrelated with all its past values $e(\\tau)$ for $\\tau < t$. The input $u(t)$ is also specified to be independent of $e(t)$. Consequently, $e(t)$ is uncorrelated with any function of past inputs and past noises.\nThis establishes that $E[\\varphi_{L}(t) \\varepsilon_{L}(t)] = \\mathbf{0}$.\n\nThe transformed regression model satisfies the crucial condition for the consistency of OLS. Therefore, applying OLS to this prefiltered model will yield a consistent estimate for $\\theta_p$. The required prefilter $L(q)$ that achieves this is the inverse of the noise polynomial model $C(q)$.", "answer": "$$\\boxed{\\frac{1}{C(q)}}$$", "id": "2880109"}, {"introduction": "Moving from theory to practice, the numerical stability of our computations becomes paramount. A least squares problem may be theoretically well-posed but practically difficult to solve if the data matrix $\\Phi$ is \"ill-conditioned,\" meaning its columns are nearly linearly dependent. This exercise uses a concrete numerical example to demonstrate the concept of ill-conditioning, quantified by the condition number $\\kappa_{2}(\\Phi)$, and reveals why numerically robust algorithms based on QR or SVD factorization are indispensable tools for the practicing engineer, ensuring reliable solutions where more naive methods might fail [@problem_id:2880119].", "problem": "Consider the identification of a discrete-time, finite impulse response linear time-invariant system with two unknown tap gains. With input sequence $u[k]$ and output sequence $y[k]$, the parametric model is $y[k] \\approx \\theta_{0} u[k] + \\theta_{1} u[k-1]$ for $k \\in \\{1,2,3\\}$. Let the input and output data be as follows: $u[0] = 1$, $u[1] = 1$, $u[2] = 1$, $u[3] = 1 + \\delta$ with $\\delta = 1 \\times 10^{-8}$, and $y[1] = 2$, $y[2] = 2 + \\varepsilon$, $y[3] = 2 + \\varepsilon$ with $\\varepsilon = 1 \\times 10^{-4}$. Form the least squares data matrix $\\Phi \\in \\mathbb{R}^{3 \\times 2}$ and vector $\\mathbf{y} \\in \\mathbb{R}^{3}$ by\n$$\n\\Phi \\triangleq \\begin{bmatrix}\nu[1] & u[0] \\\\\nu[2] & u[1] \\\\\nu[3] & u[2]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 + \\delta & 1\n\\end{bmatrix},\n\\quad\n\\mathbf{y} \\triangleq \\begin{bmatrix}\ny[1] \\\\\ny[2] \\\\\ny[3]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\\\\n2 + \\varepsilon \\\\\n2 + \\varepsilon\n\\end{bmatrix}.\n$$\nYou will estimate $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ by minimizing the least squares (LS) criterion $J(\\boldsymbol{\\theta}) \\triangleq \\|\\Phi \\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}$. Starting only from core definitions of the least squares problem and the definitions of orthogonal-triangular (QR) factorization and singular value decomposition (SVD), do the following in exact arithmetic: (i) characterize the LS minimizer and the minimal residual norm using a QR factorization of $\\Phi$; (ii) characterize the LS minimizer and the minimal residual norm using an SVD of $\\Phi$; (iii) explain why the residual norms from (i) and (ii) must coincide; (iv) compute the $2$-norm condition number $\\kappa_{2}(\\Phi)$ symbolically in terms of $\\delta$ and then evaluate it at $\\delta = 1 \\times 10^{-8}$ to justify that the problem is severely ill-conditioned; and (v) finally, report the numerical value of the minimal residual $2$-norm $\\|\\Phi \\boldsymbol{\\theta}_{\\star} - \\mathbf{y}\\|_{2}$ for the given $\\varepsilon = 1 \\times 10^{-4}$, rounded to four significant figures. Your final reported answer must be the single scalar minimal residual $2$-norm, with no units, rounded to four significant figures as specified.", "solution": "The problem is to find $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1]^T$ that minimizes the least squares criterion $J(\\boldsymbol{\\theta}) = \\|\\Phi \\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}$, where\n$$\n\\Phi =\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 + \\delta & 1\n\\end{bmatrix}\n, \\quad\n\\mathbf{y} =\n\\begin{bmatrix}\n2 \\\\\n2 + \\varepsilon \\\\\n2 + \\varepsilon\n\\end{bmatrix}.\n$$\nThe vector $\\Phi\\boldsymbol{\\theta}_{\\star}$ that minimizes this criterion is the orthogonal projection of $\\mathbf{y}$ onto the column space of $\\Phi$, denoted $\\mathcal{C}(\\Phi)$. The minimal residual vector is $\\mathbf{r}_{\\star} = \\mathbf{y} - \\Phi\\boldsymbol{\\theta}_{\\star}$, which is the projection of $\\mathbf{y}$ onto the orthogonal complement of the column space, $\\mathcal{C}(\\Phi)^{\\perp}$. The minimal residual norm is $\\|\\mathbf{r}_{\\star}\\|_2$.\n\n(i) Characterization using QR Factorization\nLet the full QR factorization of $\\Phi \\in \\mathbb{R}^{3 \\times 2}$ be $\\Phi = QR$, where $Q \\in \\mathbb{R}^{3 \\times 3}$ is an orthogonal matrix ($Q^T Q = I_3$) and $R \\in \\mathbb{R}^{3 \\times 2}$ is upper triangular. We partition $Q$ as $Q = [Q_1 | Q_2]$, where $Q_1 \\in \\mathbb{R}^{3 \\times 2}$ has columns forming an orthonormal basis for $\\mathcal{C}(\\Phi)$, and $Q_2 \\in \\mathbb{R}^{3 \\times 1}$ has columns forming an orthonormal basis for $\\mathcal{C}(\\Phi)^{\\perp}$. The matrix $R$ is partitioned as $R = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}$, where $R_1 \\in \\mathbb{R}^{2 \\times 2}$ is upper triangular and invertible since $\\Phi$ has full column rank.\n\nThe objective function becomes:\n$$ J(\\boldsymbol{\\theta}) = \\|\\Phi\\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2 = \\|QR\\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2 = \\|Q^T(QR\\boldsymbol{\\theta} - \\mathbf{y})\\|_2^2 = \\|R\\boldsymbol{\\theta} - Q^T\\mathbf{y}\\|_2^2 $$\nUsing the partitioned forms:\n$$ J(\\boldsymbol{\\theta}) = \\left\\| \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix} \\boldsymbol{\\theta} - \\begin{bmatrix} Q_1^T \\mathbf{y} \\\\ Q_2^T \\mathbf{y} \\end{bmatrix} \\right\\|_2^2 = \\|R_1\\boldsymbol{\\theta} - Q_1^T\\mathbf{y}\\|_2^2 + \\|Q_2^T\\mathbf{y}\\|_2^2 $$\nThe first term can be made zero by choosing $\\boldsymbol{\\theta}$ appropriately, while the second term is independent of $\\boldsymbol{\\theta}$. Thus, the minimum value of $J(\\boldsymbol{\\theta})$ is achieved when $R_1\\boldsymbol{\\theta}_{\\star} = Q_1^T\\mathbf{y}$. The LS minimizer is $\\boldsymbol{\\theta}_{\\star} = R_1^{-1}Q_1^T\\mathbf{y}$.\nThe minimal squared residual norm is $\\|Q_2^T\\mathbf{y}\\|_2^2$. The minimal residual norm is therefore $\\|\\Phi\\boldsymbol{\\theta}_{\\star} - \\mathbf{y}\\|_2 = \\|Q_2^T\\mathbf{y}\\|_2$.\n\n(ii) Characterization using Singular Value Decomposition (SVD)\nLet the SVD of $\\Phi$ be $\\Phi = U\\Sigma V^T$, where $U \\in \\mathbb{R}^{3 \\times 3}$ and $V \\in \\mathbb{R}^{2 \\times 2}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{3 \\times 2}$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 > 0$. We partition $U$ as $U = [\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3]$. The columns $\\{\\mathbf{u}_1, \\mathbf{u}_2\\}$ form an orthonormal basis for $\\mathcal{C}(\\Phi)$, and $\\mathbf{u}_3$ forms an orthonormal basis for $\\mathcal{C}(\\Phi)^{\\perp}$ (which is the null space of $\\Phi^T$).\n\nThe LS minimizer is given by $\\boldsymbol{\\theta}_{\\star} = \\Phi^+\\mathbf{y}$, where $\\Phi^+ = V\\Sigma^+U^T$ is the Moore-Penrose pseudoinverse. Here, $\\Sigma^+ = \\begin{pmatrix} 1/\\sigma_1 & 0 & 0 \\\\ 0 & 1/\\sigma_2 & 0 \\end{pmatrix}$.\nThe projection of $\\mathbf{y}$ onto $\\mathcal{C}(\\Phi)$ is $\\Phi\\boldsymbol{\\theta}_{\\star} = \\Phi\\Phi^+\\mathbf{y} = U\\Sigma V^T V\\Sigma^+ U^T \\mathbf{y} = U(\\Sigma\\Sigma^+)U^T\\mathbf{y}$.\nSince $\\Sigma\\Sigma^+ = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$, the projection is $(\\mathbf{u}_1\\mathbf{u}_1^T + \\mathbf{u}_2\\mathbf{u}_2^T)\\mathbf{y}$.\nThe minimal residual vector is $\\mathbf{r}_{\\star} = \\mathbf{y} - \\Phi\\boldsymbol{\\theta}_{\\star} = \\mathbf{y} - (\\mathbf{u}_1\\mathbf{u}_1^T + \\mathbf{u}_2\\mathbf{u}_2^T)\\mathbf{y}$.\nSince $I = U U^T = \\mathbf{u}_1\\mathbf{u}_1^T + \\mathbf{u}_2\\mathbf{u}_2^T + \\mathbf{u}_3\\mathbf{u}_3^T$, we have $\\mathbf{r}_{\\star} = \\mathbf{u}_3\\mathbf{u}_3^T\\mathbf{y} = (\\mathbf{u}_3^T\\mathbf{y})\\mathbf{u}_3$.\nThe minimal residual norm is $\\|\\mathbf{r}_{\\star}\\|_2 = \\|(\\mathbf{u}_3^T\\mathbf{y})\\mathbf{u}_3\\|_2 = |\\mathbf{u}_3^T\\mathbf{y}| \\|\\mathbf{u}_3\\|_2 = |\\mathbf{u}_3^T\\mathbf{y}|$.\n\n(iii) Coincidence of Residual Norms\nThe least squares solution $\\Phi\\boldsymbol{\\theta}_{\\star}$ is geometrically defined as the unique orthogonal projection of the vector $\\mathbf{y}$ onto the subspace $\\mathcal{C}(\\Phi)$. The minimal residual vector $\\mathbf{r}_{\\star} = \\mathbf{y} - \\Phi\\boldsymbol{\\theta}_{\\star}$ is consequently the unique orthogonal projection of $\\mathbf{y}$ onto the orthogonal complement subspace $\\mathcal{C}(\\Phi)^{\\perp}$. The minimal residual norm $\\|\\mathbf{r}_{\\star}\\|_2$ is the length of this unique projection.\nBoth the QR and SVD methods provide a way to construct an orthonormal basis for $\\mathcal{C}(\\Phi)^{\\perp}$. For QR, this basis is given by the columns of $Q_2$. For SVD, it is given by the columns of $U$ corresponding to zero singular values, here $\\mathbf{u}_3$. While the bases themselves might differ (up to sign), the subspace they span is the same, and the length of the projection of $\\mathbf{y}$ onto this subspace is unique. Therefore, the minimal residual norms derived from both methods must be identical.\n\n(iv) Condition Number\nThe $2$-norm condition number is $\\kappa_2(\\Phi) = \\sigma_{\\max}/\\sigma_{\\min} = \\sigma_1/\\sigma_2$. The squared singular values $\\sigma_1^2, \\sigma_2^2$ are the eigenvalues of $\\Phi^T\\Phi$.\n$$ \\Phi^T\\Phi = \\begin{bmatrix} 1 & 1 & 1+\\delta \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1+\\delta & 1 \\end{bmatrix} = \\begin{bmatrix} 3+2\\delta+\\delta^2 & 3+\\delta \\\\ 3+\\delta & 3 \\end{bmatrix} $$\nThe characteristic equation $\\det(\\Phi^T\\Phi - \\lambda I) = 0$ is $\\lambda^2 - (6+2\\delta+\\delta^2)\\lambda + 2\\delta^2 = 0$.\nThe eigenvalues are $\\lambda_{1,2} = \\frac{1}{2}(6+2\\delta+\\delta^2 \\pm \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2})$.\nSo $\\sigma_1^2 = \\lambda_1$ and $\\sigma_2^2 = \\lambda_2$.\n$$ \\kappa_2(\\Phi)^2 = \\frac{\\sigma_1^2}{\\sigma_2^2} = \\frac{6+2\\delta+\\delta^2 + \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2}}{6+2\\delta+\\delta^2 - \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2}} $$\nMultiplying the numerator and denominator by the numerator gives:\n$$ \\kappa_2(\\Phi)^2 = \\frac{\\left(6+2\\delta+\\delta^2 + \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2}\\right)^2}{(6+2\\delta+\\delta^2)^2 - ((6+2\\delta+\\delta^2)^2 - 8\\delta^2)} = \\frac{\\left(6+2\\delta+\\delta^2 + \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2}\\right)^2}{8\\delta^2} $$\nTaking the square root, and assuming $\\delta>0$:\n$$ \\kappa_2(\\Phi) = \\frac{6+2\\delta+\\delta^2 + \\sqrt{(6+2\\delta+\\delta^2)^2 - 8\\delta^2}}{2\\sqrt{2}\\delta} $$\nFor $\\delta = 1 \\times 10^{-8}$, $\\delta^2$ is negligible. $\\kappa_2(\\Phi) \\approx \\frac{6+2\\delta + \\sqrt{36+24\\delta}}{2\\sqrt{2}\\delta} \\approx \\frac{6+2\\delta + 6(1+\\frac{2}{3}\\delta)^{1/2}}{2\\sqrt{2}\\delta} \\approx \\frac{6+2\\delta + 6(1+\\frac{1}{3}\\delta)}{2\\sqrt{2}\\delta} = \\frac{12+4\\delta}{2\\sqrt{2}\\delta} \\approx \\frac{12}{2\\sqrt{2}\\delta} = \\frac{3\\sqrt{2}}{\\delta}$.\n$$ \\kappa_2(\\Phi) \\approx \\frac{3\\sqrt{2}}{1 \\times 10^{-8}} \\approx 4.2426 \\times 10^8 $$\nA condition number of this magnitude ($ \\sim 10^8$) signifies extreme sensitivity of the solution $\\boldsymbol{\\theta}$ to small perturbations in $\\Phi$ or $\\mathbf{y}$. This is characteristic of a severely ill-conditioned problem, which arises here because the columns of $\\Phi$ are nearly linearly dependent for small $\\delta$.\n\n(v) Minimal Residual Norm Calculation\nTo compute the minimal residual norm, we find a basis for $\\mathcal{C}(\\Phi)^\\perp$, which is the null space of $\\Phi^T$. Let $\\mathbf{x}=[x_1, x_2, x_3]^T$ be in this null space.\n$$ \\Phi^T\\mathbf{x} = \\begin{bmatrix} 1 & 1 & 1+\\delta \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nThis gives the system of equations $x_1+x_2+(1+\\delta)x_3=0$ and $x_1+x_2+x_3=0$. Subtracting the second from the first yields $\\delta x_3 = 0$, which implies $x_3=0$ since $\\delta \\neq 0$. The equations reduce to $x_1+x_2=0$, or $x_1=-x_2$. A basis vector for the null space is $[1, -1, 0]^T$.\nThe orthonormal basis for $\\mathcal{C}(\\Phi)^\\perp$ is the normalized vector, which corresponds to $\\mathbf{u}_3$ from the SVD or the column of $Q_2$ from the QR factorization.\n$$ \\mathbf{u}_3 = \\frac{1}{\\sqrt{1^2+(-1)^2+0^2}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} $$\nThe minimal residual norm is $|\\mathbf{u}_3^T\\mathbf{y}|$.\n$$ \\|\\mathbf{r}_{\\star}\\|_2 = \\left| \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 2+\\varepsilon \\\\ 2+\\varepsilon \\end{bmatrix} \\right| = \\left| \\frac{1}{\\sqrt{2}}(2 - (2+\\varepsilon)) \\right| = \\left| \\frac{-\\varepsilon}{\\sqrt{2}} \\right| = \\frac{|\\varepsilon|}{\\sqrt{2}} $$\nGiven $\\varepsilon = 1 \\times 10^{-4}$, the numerical value is:\n$$ \\|\\mathbf{r}_{\\star}\\|_2 = \\frac{1 \\times 10^{-4}}{\\sqrt{2}} \\approx 0.70710678 \\times 10^{-4} = 7.0710678 \\times 10^{-5} $$\nRounding to four significant figures, we get $7.071 \\times 10^{-5}$.", "answer": "$$\n\\boxed{7.071 \\times 10^{-5}}\n$$", "id": "2880119"}]}