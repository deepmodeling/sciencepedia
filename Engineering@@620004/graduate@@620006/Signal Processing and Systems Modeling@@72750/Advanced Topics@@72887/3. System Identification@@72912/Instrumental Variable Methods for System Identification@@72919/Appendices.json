{"hands_on_practices": [{"introduction": "Before we can estimate the parameters of a system, we must ensure the experiment provides enough information to uniquely distinguish them. In system identification, this requirement is known as persistent excitation. For instrumental variable methods, this condition mathematically translates to the matrix $Z^{\\top}\\Phi$ having full column rank. This practice problem [@problem_id:2878418] provides a clear and concrete illustration of what happens when this condition fails, allowing you to directly calculate how an insufficiently 'rich' input signal leads to a rank-deficient matrix and makes certain parameter combinations impossible to identify.", "problem": "Consider a single-input single-output linear time-invariant system modeled for identification by a finite impulse response structure of second order. The measurement equation is\n$$\ny(t) \\;=\\; b_{1}\\,u(t-1) \\;+\\; b_{2}\\,u(t-2) \\;+\\; v(t),\n$$\nwhere $u(t)$ is a deterministic input, $v(t)$ is a zero-mean disturbance uncorrelated with all past inputs, and $\\theta \\triangleq \\begin{pmatrix} b_{1} & b_{2} \\end{pmatrix}^{\\top}$ is the unknown parameter vector to be identified. Define the regressor\n$$\n\\varphi(t) \\;\\triangleq\\; \\begin{pmatrix} u(t-1) \\\\ u(t-2) \\end{pmatrix},\n$$\nso that $y(t) = \\varphi(t)^{\\top}\\theta + v(t)$. To avoid endogeneity from $v(t)$, an instrumental variable method is used with instrument vector\n$$\nz(t) \\;\\triangleq\\; \\begin{pmatrix} u(t-2) \\\\ u(t-3) \\end{pmatrix},\n$$\nand stacked data matrices\n$$\n\\Phi \\;\\triangleq\\; \\begin{pmatrix} \\varphi(3)^{\\top} \\\\ \\varphi(4)^{\\top} \\\\ \\varphi(5)^{\\top} \\\\ \\varphi(6)^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 2}, \n\\qquad\nZ \\;\\triangleq\\; \\begin{pmatrix} z(3)^{\\top} \\\\ z(4)^{\\top} \\\\ z(5)^{\\top} \\\\ z(6)^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 2}.\n$$\nData are collected for times $t \\in \\{3,4,5,6\\}$ under the experiment $u(t) \\equiv c$ for all integers $t \\leq 6$, with $c$ a known nonzero constant $c = 3$. The instrumental variable normal equations are\n$$\n\\big(Z^{\\top}\\Phi\\big)\\,\\theta \\;=\\; Z^{\\top} y,\n$$\nand identifiability of $\\theta$ requires that $Z^{\\top}\\Phi$ have full column rank.\n\nStarting only from the regression form and the instrumental variable construction above, and using the given excitation, explicitly form $Z^{\\top}\\Phi$, determine its rank, and characterize the set of parameter combinations that are not identifiable from this experiment in terms of the null space of $Z^{\\top}\\Phi$. Then, report a single vector that quantitatively identifies the unidentifiable direction by the following convention: provide the unique unit-norm vector $v \\in \\mathbb{R}^{2}$ with first component positive that spans $\\operatorname{Null}(Z^{\\top}\\Phi)$.\n\nYour final answer must be the row vector $v^{\\top}$ written as a single $1 \\times 2$ matrix. No rounding is required and no units are involved.", "solution": "The problem as stated is valid. It is scientifically sound, well-posed, and objective, presenting a standard scenario in system identification. All necessary information is provided, and the task is to perform a concrete analysis of parameter identifiability under a specific experimental condition.\n\nThe objective is to characterize the unidentifiable directions for the parameter vector $\\theta = \\begin{pmatrix} b_{1} & b_{2} \\end{pmatrix}^{\\top}$. In the context of instrumental variable estimation, identifiability requires the matrix $Z^{\\top}\\Phi$ to be of full column rank. If this condition is not met, the matrix is rank-deficient, and its null space, $\\operatorname{Null}(Z^{\\top}\\Phi)$, defines the subspace of parameter variations that cannot be detected by the experiment. Any vector in this null space represents a combination of parameters that yields no change in the observed instrumental variable normal equations. The task is to find a specific basis vector for this null space.\n\nFirst, we must construct the data matrices $\\Phi$ and $Z$ based on the given input signal. The problem specifies that the input $u(t)$ is a constant, $u(t) = c = 3$, for all relevant times $t \\leq 6$.\n\nThe regressor vector is $\\varphi(t) = \\begin{pmatrix} u(t-1) \\\\ u(t-2) \\end{pmatrix}$. Given the constant input, for any $t$ used in the data matrices, we have $\\varphi(t) = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$. The stacked regressor matrix $\\Phi$ is defined over $t \\in \\{3,4,5,6\\}$:\n$$\n\\Phi = \\begin{pmatrix} \\varphi(3)^{\\top} \\\\ \\varphi(4)^{\\top} \\\\ \\varphi(5)^{\\top} \\\\ \\varphi(6)^{\\top} \\end{pmatrix} = \\begin{pmatrix} u(2) & u(1) \\\\ u(3) & u(2) \\\\ u(4) & u(3) \\\\ u(5) & u(4) \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\n\nThe instrumental variable vector is $z(t) = \\begin{pmatrix} u(t-2) \\\\ u(t-3) \\end{pmatrix}$. Similarly, with a constant input, we have $z(t) = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$ for any $t$. The stacked instrument matrix $Z$ is:\n$$\nZ = \\begin{pmatrix} z(3)^{\\top} \\\\ z(4)^{\\top} \\\\ z(5)^{\\top} \\\\ z(6)^{\\top} \\end{pmatrix} = \\begin{pmatrix} u(1) & u(0) \\\\ u(2) & u(1) \\\\ u(3) & u(2) \\\\ u(4) & u(3) \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\n\nNext, we compute the matrix product $Z^{\\top}\\Phi$. The transpose of $Z$ is:\n$$\nZ^{\\top} = \\begin{pmatrix} 3 & 3 & 3 & 3 \\\\ 3 & 3 & 3 & 3 \\end{pmatrix}.\n$$\nThe product is then:\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 3 & 3 & 3 & 3 \\\\ 3 & 3 & 3 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\nPerforming the matrix multiplication:\nThe element at position $(1,1)$ is $(3)(3)+(3)(3)+(3)(3)+(3)(3) = 4 \\times 9 = 36$.\nDue to the structure of the matrices, all elements of the resulting product are identical.\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 36 & 36 \\\\ 36 & 36 \\end{pmatrix}.\n$$\n\nNow we determine the rank of this matrix. A $2 \\times 2$ matrix has full column rank if its rank is $2$. The rank of $Z^{\\top}\\Phi$ can be determined by observing that its columns are linearly dependent; specifically, the second column is identical to the first. The determinant is $\\det(Z^{\\top}\\Phi) = (36)(36) - (36)(36) = 0$. Since the matrix is non-zero and singular, its rank is $1$. This is not full column rank, which confirms that the parameter vector $\\theta$ is not identifiable with this experimental setup.\n\nThe unidentifiable directions are given by the null space of $Z^{\\top}\\Phi$. We seek a vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ such that $(Z^{\\top}\\Phi)v=0$.\n$$\n\\begin{pmatrix} 36 & 36 \\\\ 36 & 36 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis matrix equation reduces to the single linear equation $36v_1 + 36v_2 = 0$, which simplifies to $v_1 + v_2 = 0$, or $v_1 = -v_2$. Any vector in the null space is therefore of the form $\\begin{pmatrix} -v_2 \\\\ v_2 \\end{pmatrix} = v_2 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ for some scalar $v_2$. The null space is a one-dimensional subspace of $\\mathbb{R}^2$ spanned by the vector $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n\nThe problem requires a specific vector: the unique unit-norm vector $v$ with a positive first component that spans this null space. Our current spanning vector, $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, has a negative first component. To obtain a spanning vector with a positive first component, we can multiply by any negative scalar. Multiplying by $-1$ gives the vector $w = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nFinally, we normalize $w$ to have unit norm. The norm is $\\|w\\| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}$. The required unit-norm vector $v$ is:\n$$\nv = \\frac{w}{\\|w\\|} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nRationalizing the denominator gives:\n$$\nv = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{pmatrix}.\n$$\nThis vector satisfies all conditions: it spans $\\operatorname{Null}(Z^{\\top}\\Phi)$, its first component is positive, and it has unit norm. The question asks for the row vector $v^{\\top}$.\n$$\nv^{\\top} = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\end{pmatrix}.\n$$\nThis vector quantitatively identifies the unidentifiable direction in the parameter space $\\{b_1, b_2\\}$. Any change to the parameter vector $\\theta$ along this direction is undetectable.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\end{pmatrix}}\n$$", "id": "2878418"}, {"introduction": "Once identifiability is established, the next step is to compute the parameter estimate. The Instrumental Variable (IV) method can be viewed from two powerful perspectives: geometrically as a projection of the regressor data onto the space spanned by the instruments, and algorithmically as the well-known Two-Stage Least Squares (2SLS) procedure. This exercise [@problem_id:2878432] demystifies the connection between these views by guiding you through a direct numerical computation, demonstrating step-by-step how these two seemingly different paths lead to the exact same estimate.", "problem": "Consider the linear regression form of a discrete-time, finite-sample system identification problem with potentially endogenous regressors: for sample index $k \\in \\{1,\\dots,n\\}$, the measured output $y_k$ and regressor row vector $\\varphi_k^{\\top}$ satisfy $y_k = \\varphi_k^{\\top}\\theta + v_k$, where the disturbance $v_k$ may be correlated with components of $\\varphi_k$, but there exists an instrument vector $z_k$ such that $\\mathbb{E}[z_k v_k] = 0$ and $\\mathbb{E}[z_k \\varphi_k^{\\top}]$ has full column rank. Collecting $n$ samples into the stacked vectors and matrices yields $y \\in \\mathbb{R}^{n}$, $\\Phi \\in \\mathbb{R}^{n \\times p}$, $Z \\in \\mathbb{R}^{n \\times m}$, and an unknown parameter $\\theta \\in \\mathbb{R}^{p}$. The instrumental variable (IV) method and the Two-Stage Least Squares (2SLS) method are both based on projecting the regressors into the instrument space to remove endogeneity.\n\nYou are given a concrete dataset with $n=4$, $p=2$, $m=2$:\n$$\n\\Phi=\\begin{bmatrix}\n1 & 2\\\\\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\nZ=\\begin{bmatrix}\n1 & 0\\\\\n1 & 0\\\\\n0 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny=\\begin{bmatrix}\n5\\\\\n4\\\\\n3\\\\\n2\n\\end{bmatrix}.\n$$\nUsing only definitions and linear-algebraic facts about orthogonal projectors and the orthogonal-triangular (QR) decomposition, proceed as follows:\n\n1) Compute a QR decomposition of $Z$, write the associated orthogonal projector onto the column space of $Z$ as $P_Z$, and construct $P_Z$ explicitly from the $Q$ factor via $P_Z = QQ^{\\top}$.\n\n2) Using the instrumental variable principle that instruments enforce orthogonality of the residual to the instrument space, form the normal equations in the instrument space and use the projector $P_Z$ to obtain an explicit estimator $\\hat{\\theta}_{\\mathrm{IV}}$ for this dataset.\n\n3) Separately, perform two-stage least squares: first project the regressors into the instrument space to obtain fitted regressors, then regress $y$ onto these fitted regressors by ordinary least squares. Derive the resulting estimator $\\hat{\\theta}_{\\mathrm{2SLS}}$.\n\n4) Prove, using only properties of orthogonal projectors and the orthonormality of $Q$, that for full-column-rank $Z$ one has $\\hat{\\theta}_{\\mathrm{IV}}=\\hat{\\theta}_{\\mathrm{2SLS}}$, and verify numerically that they coincide for the given data.\n\nReport the final estimator vector $\\hat{\\theta}_{\\mathrm{IV}}$ as a single $1\\times 2$ row vector. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary data for its resolution. It constitutes a standard exercise in linear system identification theory. We may therefore proceed with the solution, which is executed in four parts as requested.\n\nThe given data are the regressor matrix $\\Phi$, the instrument matrix $Z$, and the output vector $y$:\n$$\n\\Phi=\\begin{bmatrix}\n1 & 2\\\\\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\nZ=\\begin{bmatrix}\n1 & 0\\\\\n1 & 0\\\\\n0 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny=\\begin{bmatrix}\n5\\\\\n4\\\\\n3\\\\\n2\n\\end{bmatrix}\n$$\nThe dimensions are $n=4$, $p=2$, and $m=2$.\n\nFirst, we compute an orthogonal-triangular (QR) decomposition of the instrument matrix $Z$. The columns of $Z$, denoted $z_1$ and $z_2$, are:\n$$\nz_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad z_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\nWe observe that these columns are already orthogonal, as their inner product is $z_1^{\\top}z_2 = 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 1 = 0$. The Gram-Schmidt procedure thus simplifies to normalization. The squared norms are $\\|z_1\\|^2 = 1^2+1^2 = 2$ and $\\|z_2\\|^2 = 1^2+1^2 = 2$.\nThe orthonormal basis for the column space of $Z$ is given by the columns of the matrix $Q$:\n$$\nq_1 = \\frac{z_1}{\\|z_1\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad q_2 = \\frac{z_2}{\\|z_2\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\n$$\nQ = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix}\n$$\nThe upper triangular matrix $R$ is found via $R = Q^{\\top}Z$:\n$$\nR = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{2} \\end{bmatrix}\n$$\nThe orthogonal projector $P_Z$ onto the column space of $Z$ is constructed from $Q$ as $P_Z = QQ^{\\top}$:\n$$\nP_Z = \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix}\n$$\n\nSecond, we derive the instrumental variable (IV) estimator $\\hat{\\theta}_{\\mathrm{IV}}$. The IV principle imposes orthogonality of the residual, $y-\\Phi\\hat{\\theta}$, to the space spanned by the instruments. This is expressed as $Z^{\\top}(y-\\Phi\\hat{\\theta}_{\\mathrm{IV}}) = 0$. This condition is equivalent to requiring the projection of the residual onto the column space of $Z$ to be zero: $P_Z(y-\\Phi\\hat{\\theta}_{\\mathrm{IV}}) = 0$. This leads to the system of equations $P_Z \\Phi \\hat{\\theta}_{\\mathrm{IV}} = P_Z y$. The least-squares solution for $\\hat{\\theta}_{\\mathrm{IV}}$ is $\\hat{\\theta}_{\\mathrm{IV}} = ((P_Z\\Phi)^{\\top} P_Z\\Phi)^{-1} (P_Z\\Phi)^{\\top} P_Z y$. Since $P_Z$ is an orthogonal projector, it is symmetric ($P_Z^{\\top}=P_Z$) and idempotent ($P_Z^2=P_Z$). The expression simplifies to:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (\\Phi^{\\top}P_Z\\Phi)^{-1} \\Phi^{\\top}P_Z y\n$$\nWe compute the required matrices:\n$$\nP_Z \\Phi = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2\\\\ 2 & 1\\\\ 1 & 1\\\\ 0 & 1 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\n$$\n$$\nP_Z y = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 2 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 9 \\\\ 9 \\\\ 5 \\\\ 5 \\end{bmatrix}\n$$\nNext, we form the products with $\\Phi^{\\top}$:\n$$\n\\Phi^{\\top}P_Z \\Phi = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 1 & 1 & 1 \\end{bmatrix} \\left(\\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 3+6+1 & 3+6+2 \\\\ 6+3+1+1 & 6+3+2+2 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\n$$\n$$\n\\Phi^{\\top}P_Z y = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 1 & 1 & 1 \\end{bmatrix} \\left(\\frac{1}{2}\\begin{bmatrix} 9 \\\\ 9 \\\\ 5 \\\\ 5 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 9+18+5 \\\\ 18+9+5+5 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nNow, we solve for $\\hat{\\theta}_{\\mathrm{IV}}$:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\left(\\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\\right)^{-1} \\left(\\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\\right) = \\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}^{-1} \\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nThe determinant of the matrix is $10 \\cdot 13 - 11 \\cdot 11 = 130 - 121 = 9$. Its inverse is $\\frac{1}{9}\\begin{bmatrix} 13 & -11 \\\\ -11 & 10 \\end{bmatrix}$.\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\frac{1}{9}\\begin{bmatrix} 13 & -11 \\\\ -11 & 10 \\end{bmatrix} \\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 13 \\cdot 32 - 11 \\cdot 37 \\\\ -11 \\cdot 32 + 10 \\cdot 37 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 416 - 407 \\\\ -352 + 370 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 9 \\\\ 18 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n$$\n\nThird, we derive the Two-Stage Least Squares (2SLS) estimator $\\hat{\\theta}_{\\mathrm{2SLS}}$.\nIn Stage 1, the possibly endogenous regressors $\\Phi$ are projected onto the space of instruments. The fitted regressors are $\\hat{\\Phi} = P_Z \\Phi$. We have already computed this matrix:\n$$\n\\hat{\\Phi} = \\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\n$$\nIn Stage 2, the output $y$ is regressed onto the fitted regressors $\\hat{\\Phi}$ using ordinary least squares (OLS):\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = (\\hat{\\Phi}^{\\top}\\hat{\\Phi})^{-1}\\hat{\\Phi}^{\\top}y\n$$\nWe compute the components:\n$$\n\\hat{\\Phi}^{\\top}\\hat{\\Phi} = (P_Z\\Phi)^{\\top}(P_Z\\Phi) = \\Phi^{\\top}P_Z^{\\top}P_Z\\Phi = \\Phi^{\\top}P_Z\\Phi = \\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\n$$\n$$\n\\hat{\\Phi}^{\\top}y = (P_Z\\Phi)^{\\top}y = \\Phi^{\\top}P_Z^{\\top}y = \\Phi^{\\top}P_Z y = \\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nThe expressions for $\\hat{\\Phi}^{\\top}\\hat{\\Phi}$ and $\\hat{\\Phi}^{\\top}y$ are identical to those computed for the IV estimator. Consequently, the resulting estimator must be the same:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = \\left(\\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\\right)^{-1} \\left(\\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n$$\n\nFourth, we prove the general equivalence $\\hat{\\theta}_{\\mathrm{IV}}=\\hat{\\theta}_{\\mathrm{2SLS}}$ and verify it numerically.\nThe 2SLS estimator is defined as $\\hat{\\theta}_{\\mathrm{2SLS}} = (\\hat{\\Phi}^{\\top}\\hat{\\Phi})^{-1}\\hat{\\Phi}^{\\top}y$, with $\\hat{\\Phi}=P_Z\\Phi$. Using $P_Z=QQ^{\\top}$ and the property $Q^{\\top}Q=I_m$:\n$$\n\\hat{\\Phi}^{\\top}\\hat{\\Phi} = (QQ^{\\top}\\Phi)^{\\top}(QQ^{\\top}\\Phi) = \\Phi^{\\top}QQ^{\\top}QQ^{\\top}\\Phi = \\Phi^{\\top}Q(Q^{\\top}Q)Q^{\\top}\\Phi = \\Phi^{\\top}Q I_m Q^{\\top}\\Phi = (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)\n$$\nAnd\n$$\n\\hat{\\Phi}^{\\top}y = (QQ^{\\top}\\Phi)^{\\top}y = \\Phi^{\\top}QQ^{\\top}y = (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y)\n$$\nSubstituting these into the 2SLS formula gives:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = [(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)]^{-1} (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y)\n$$\nThis is the OLS solution for $\\theta$ in the transformed model $Q^{\\top}y = (Q^{\\top}\\Phi)\\theta + Q^{\\top}v$.\n\nThe IV estimator $\\hat{\\theta}_{\\mathrm{IV}}$ is rooted in the orthogonality condition $Z^{\\top}(y-\\Phi\\hat{\\theta})=0$. Since the columns of $Q$ form an orthonormal basis for the column space of $Z$, this is equivalent to $Q^{\\top}(y-\\Phi\\hat{\\theta})=0$, which yields the set of equations $Q^{\\top}\\Phi\\hat{\\theta}_{\\mathrm{IV}}=Q^{\\top}y$.\n\nIn the just-identified case, where the number of instruments $m$ equals the number of parameters $p$, the matrix $Q^{\\top}\\Phi$ is square ($p \\times p$) and, assuming instrument relevance, invertible. The unique solution is:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (Q^{\\top}\\Phi)^{-1}Q^{\\top}y\n$$\nIn this same case ($m=p$), the 2SLS expression simplifies because $(Q^{\\top}\\Phi)$ is square and invertible:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = [(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)]^{-1} (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y) = (Q^{\\top}\\Phi)^{-1}((Q^{\\top}\\Phi)^{\\top})^{-1}(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y) = (Q^{\\top}\\Phi)^{-1}Q^{\\top}y\n$$\nThus, we have proven that $\\hat{\\theta}_{\\mathrm{IV}} = \\hat{\\theta}_{\\mathrm{2SLS}}$ when $m=p$.\nThe provided dataset has $m=p=2$, so this equivalence holds. Our numerical calculations confirm this, as we found $\\hat{\\theta}_{\\mathrm{IV}} = \\hat{\\theta}_{\\mathrm{2SLS}} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nThe final estimator vector is reported as a $1 \\times 2$ row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 \\end{pmatrix}}\n$$", "id": "2878432"}, {"introduction": "Obtaining a parameter estimate is only half the battle; we must then validate the assumptions that grant the estimate its desirable properties, such as consistency. For IV methods, the most critical assumption is instrument exogeneity—the idea that our chosen instruments are uncorrelated with the system's underlying disturbances. This hands-on exercise [@problem_id:2878466] moves from theory to verification, tasking you with constructing and applying a quantitative statistical test to check for this crucial orthogonality condition in the post-estimation residuals.", "problem": "Consider a single-input single-output linear time-invariant system estimated by an instrumental variable method, yielding a residual sequence $\\{\\hat{e}(t)\\}_{t=1}^{T}$ and an instrument sequence $\\{z(t)\\}_{t=1}^{T}$. Under correct specification and instrument exogeneity, a fundamental orthogonality condition states that $\\mathbb{E}[\\hat{e}(t) z(t-\\ell)] = 0$ for all nonnegative integers $\\ell$. You are tasked with validating this condition using a residual–instrument correlation test across several lags.\n\nStarting from the orthogonality condition $\\mathbb{E}[\\hat{e}(t) z(t-\\ell)] = 0$, and using only core definitions of expectation, covariance, and correlation for stochastic processes, construct a test statistic that aggregates, for a given finite positive integer $L$, the sample evidence against these moment conditions across lags $\\ell = 0, 1, \\dots, L$. Your construction should be dimensionless and should be justified as a sample analog of testing the null hypothesis that all these moment conditions hold simultaneously.\n\nThen, for $T = 12$ and $L = 3$, suppose that the residuals and instruments have been demeaned so that $\\frac{1}{T}\\sum_{t=1}^{T}\\hat{e}(t) = 0$ and $\\frac{1}{T}\\sum_{t=1}^{T}z(t) = 0$, and that the following sample energy and cross-sum summaries have been computed from the data:\n$$\n\\sum_{t=1}^{T}\\hat{e}(t)^{2} = 24,\\quad \\sum_{t=1}^{T}z(t)^{2} = 36,\n$$\n$$\n\\sum_{t=1}^{T} \\hat{e}(t) z(t) = 2,\\quad \\sum_{t=2}^{T} \\hat{e}(t) z(t-1) = -1,\\quad \\sum_{t=3}^{T} \\hat{e}(t) z(t-2) = 0,\\quad \\sum_{t=4}^{T} \\hat{e}(t) z(t-3) = 1.\n$$\nUsing your constructed statistic, compute its numerical value for these data. Provide your final answer as a single scalar equal to the value of your test statistic for $L = 3$. If you choose to present a numerical approximation, round your answer to four significant figures; otherwise, present the exact value.", "solution": "The instrumental variable orthogonality condition for a correctly specified model with a valid instrument is that $\\mathbb{E}[\\hat{e}(t) z(t-\\ell)] = 0$ for all $\\ell \\geq 0$. This condition states that the instrument at time $t-\\ell$ is uncorrelated with the post-estimation residuals at time $t$. To assess this empirically, one replaces population expectations with their sample analogs and normalizes to obtain dimensionless quantities comparable across scales.\n\nA standard way to quantify the strength of a linear relationship between two zero-mean sequences $\\{x(t)\\}$ and $\\{y(t)\\}$ is the sample cross-correlation at lag $\\ell$:\n$$\n\\hat{\\rho}_{xy}(\\ell) \\triangleq \\frac{\\sum_{t=\\ell+1}^{T} x(t)\\, y(t-\\ell)}{\\left(\\sum_{t=1}^{T} x(t)^{2}\\right)^{1/2}\\left(\\sum_{t=1}^{T} y(t)^{2}\\right)^{1/2}}.\n$$\nWhen $x(t) = \\hat{e}(t)$ and $y(t) = z(t)$ and both sequences are demeaned, $\\hat{\\rho}_{\\hat{e}z}(\\ell)$ is a dimensionless measure of the sample correlation between $\\hat{e}(t)$ and $z(t-\\ell)$. Under the null hypothesis of instrument exogeneity, $\\hat{\\rho}_{\\hat{e}z}(\\ell)$ should be close to zero for each nonnegative lag $\\ell$.\n\nTo aggregate deviations from zero across several lags simultaneously, one uses a portmanteau-type statistic defined as\n$$\nQ_{L} \\triangleq T \\sum_{\\ell=0}^{L} \\hat{\\rho}_{\\hat{e}z}(\\ell)^{2}.\n$$\nThis statistic is dimensionless, nonnegative, and increases as the collection of sample cross-correlations departs from zero. Under standard regularity conditions and large $T$, $Q_{L}$ has an asymptotic chi-squared distribution with $L+1$ degrees of freedom under the null of instrument exogeneity, which justifies its use as a global test across lags.\n\nWe now compute $Q_{3}$ from the provided summaries. First, compute the common normalization\n$$\nD \\triangleq \\left(\\sum_{t=1}^{T} \\hat{e}(t)^{2}\\right)^{1/2}\\left(\\sum_{t=1}^{T} z(t)^{2}\\right)^{1/2} = \\left(24\\right)^{1/2}\\left(36\\right)^{1/2} = \\sqrt{24 \\cdot 36} = \\sqrt{864} = 12\\sqrt{6}.\n$$\nFor each lag $\\ell \\in \\{0,1,2,3\\}$, using the given overlapping cross-sums:\n- For $\\ell = 0$,\n$$\n\\hat{\\rho}_{\\hat{e}z}(0) = \\frac{\\sum_{t=1}^{T} \\hat{e}(t) z(t)}{D} = \\frac{2}{12\\sqrt{6}}.\n$$\n- For $\\ell = 1$,\n$$\n\\hat{\\rho}_{\\hat{e}z}(1) = \\frac{\\sum_{t=2}^{T} \\hat{e}(t) z(t-1)}{D} = \\frac{-1}{12\\sqrt{6}}.\n$$\n- For $\\ell = 2$,\n$$\n\\hat{\\rho}_{\\hat{e}z}(2) = \\frac{\\sum_{t=3}^{T} \\hat{e}(t) z(t-2)}{D} = \\frac{0}{12\\sqrt{6}} = 0.\n$$\n- For $\\ell = 3$,\n$$\n\\hat{\\rho}_{\\hat{e}z}(3) = \\frac{\\sum_{t=4}^{T} \\hat{e}(t) z(t-3)}{D} = \\frac{1}{12\\sqrt{6}}.\n$$\n\nCompute the sum of squared sample cross-correlations:\n$$\n\\sum_{\\ell=0}^{3} \\hat{\\rho}_{\\hat{e}z}(\\ell)^{2} = \\left(\\frac{2}{12\\sqrt{6}}\\right)^{2} + \\left(\\frac{-1}{12\\sqrt{6}}\\right)^{2} + 0^{2} + \\left(\\frac{1}{12\\sqrt{6}}\\right)^{2}.\n$$\nEvaluating term by term,\n$$\n\\left(\\frac{2}{12\\sqrt{6}}\\right)^{2} = \\frac{4}{(12)^{2}\\cdot 6} = \\frac{4}{864},\\quad \\left(\\frac{-1}{12\\sqrt{6}}\\right)^{2} = \\frac{1}{864},\\quad \\left(\\frac{1}{12\\sqrt{6}}\\right)^{2} = \\frac{1}{864}.\n$$\nTherefore,\n$$\n\\sum_{\\ell=0}^{3} \\hat{\\rho}_{\\hat{e}z}(\\ell)^{2} = \\frac{4 + 1 + 1}{864} = \\frac{6}{864} = \\frac{1}{144}.\n$$\n\nFinally, multiply by $T = 12$ to obtain the test statistic\n$$\nQ_{3} = T \\sum_{\\ell=0}^{3} \\hat{\\rho}_{\\hat{e}z}(\\ell)^{2} = 12 \\cdot \\frac{1}{144} = \\frac{1}{12}.\n$$\nThis is an exact value, so no rounding is required.", "answer": "$$\\boxed{\\frac{1}{12}}$$", "id": "2878466"}]}