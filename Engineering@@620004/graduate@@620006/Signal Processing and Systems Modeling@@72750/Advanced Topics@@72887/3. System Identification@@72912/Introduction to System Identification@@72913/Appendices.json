{"hands_on_practices": [{"introduction": "System identification often begins with analyzing frequency response data, a cornerstone of classical control and signal processing. The asymptotic behavior of a system's Bode plot, particularly for simple systems, directly reveals key parameters like static gain and time constants. This exercise builds the fundamental skill of translating graphical, frequency-domain information into a concrete mathematical model, a process that is both intuitive and powerful for a first-pass analysis. [@problem_id:1585873]", "problem": "An engineer is tasked with identifying a stable, minimum-phase first-order system from experimental data. After conducting a frequency response analysis, the asymptotes of the system's Bode magnitude plot have been precisely characterized.\n\nAt very low frequencies, the magnitude response is observed to approach a constant horizontal asymptote at a level of $15.0$ dB. At very high frequencies, the magnitude response is well-approximated by an asymptote which is a straight line with a slope of $-20$ dB per decade of frequency. This high-frequency asymptote is known to pass through the point where the magnitude is $-18.5$ dB at an angular frequency of $250$ rad/s.\n\nThe system is to be modeled with a transfer function of the form $G(s) = \\frac{K}{\\tau s + 1}$, where $K$ is the static gain and $\\tau$ is the time constant. Based on the described asymptotic behavior, determine the values for the static gain $K$ and the corner frequency $\\omega_c = 1/\\tau$.\n\nProvide your answer as a pair of numerical values for the static gain $K$ and the corner frequency $\\omega_c$. The static gain $K$ is a dimensionless quantity. Express the corner frequency $\\omega_c$ in units of rad/s. Round both numerical values to three significant figures.", "solution": "For a first-order low-pass system $G(s)=\\dfrac{K}{\\tau s+1}$ the magnitude is\n$$\n|G(j\\omega)|=\\frac{K}{\\sqrt{1+(\\tau\\omega)^{2}}},\n$$\nso the Bode magnitude in decibels is\n$$\nM(\\omega)=20\\log_{10}|G(j\\omega)|=20\\log_{10}K-10\\log_{10}\\!\\big(1+(\\tau\\omega)^{2}\\big).\n$$\n\nLow-frequency asymptote: as $\\omega\\to 0$, $M(\\omega)\\to 20\\log_{10}K$. Given the low-frequency level is $15.0$ dB,\n$$\n20\\log_{10}K=15.0 \\quad\\Rightarrow\\quad \\log_{10}K=\\frac{15.0}{20}=0.75 \\quad\\Rightarrow\\quad K=10^{0.75}.\n$$\n\nHigh-frequency asymptote: for $\\omega\\gg \\omega_{c}$ with $\\omega_{c}=\\dfrac{1}{\\tau}$,\n$$\n|G(j\\omega)|\\approx \\frac{K}{\\tau\\,\\omega},\\quad M_{\\text{asymp}}(\\omega)=20\\log_{10}\\!\\left(\\frac{K}{\\tau}\\right)-20\\log_{10}\\omega.\n$$\nThe given asymptote passes through $M=-18.5$ dB at $\\omega=250$ rad/s, hence\n$$\n-18.5=20\\log_{10}\\!\\left(\\frac{K}{\\tau}\\right)-20\\log_{10}(250)\n\\;\\Rightarrow\\;\n20\\log_{10}\\!\\left(\\frac{K}{\\tau}\\right)=-18.5+20\\log_{10}(250).\n$$\nSince the asymptotes intersect at $\\omega=\\omega_{c}$ and both equal $20\\log_{10}K$ there,\n$$\n20\\log_{10}\\omega_{c}\n=20\\log_{10}\\!\\left(\\frac{K}{\\tau}\\right)-20\\log_{10}K\n=-18.5-20\\log_{10}K+20\\log_{10}(250).\n$$\nSubstituting $20\\log_{10}K=15.0$ gives\n$$\n20\\log_{10}\\omega_{c}= -18.5-15.0+20\\log_{10}(250)=14.4588001734,\n$$\nso\n$$\n\\log_{10}\\omega_{c}=0.72294000867,\\quad \\omega_{c}=10^{0.72294000867}.\n$$\n\nNumerical values rounded to three significant figures:\n$$\nK=10^{0.75}\\approx 5.62,\\qquad \\omega_{c}\\approx 5.28\\ \\text{rad/s}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}5.62 & 5.28\\end{pmatrix}}$$", "id": "1585873"}, {"introduction": "While Ordinary Least Squares (OLS) is a foundational estimation method, its core assumption of uncorrelated noise is often violated in practice, leading to biased parameter estimates. This is especially true for systems with feedback or colored disturbances, which are common in real-world applications. This practice introduces the Instrumental Variables (IV) method, a powerful technique that uses auxiliary variables to break the correlation between regressors and noise, thereby yielding consistent estimates where OLS fails. [@problem_id:2878920]", "problem": "Consider a single-input single-output discrete-time system excited by a known scalar input signal $u(t)$ and producing a measured scalar output $y(t)$. You adopt the following predictor form with an autoregressive with exogenous input (ARX) structure and a colored disturbance:\n$$\ny(t) \\;=\\; a\\,y(t-1)\\;+\\;b\\,u(t-1)\\;+\\;v(t),\\qquad v(t)\\;=\\;e(t)\\;+\\;c\\,e(t-1),\n$$\nwhere $\\{e(t)\\}$ is an independently and identically distributed (i.i.d.) zero-mean sequence with finite variance, and $\\{e(t)\\}$ is statistically independent of the input $\\{u(t)\\}$. Because the disturbance $v(t)$ is colored, ordinary least squares is inconsistent for estimating $\\theta \\triangleq \\begin{pmatrix} a & b \\end{pmatrix}^{\\top}$. To address this, you will use Instrumental Variables (IV), where a valid instrument matrix $Z$ must satisfy two properties: (i) instrument relevance (nonzero correlation with the regressors), and (ii) instrument exogeneity (zero correlation with $v(t)$).\n\nYou have access to the following measured data generated from a stable system with initial conditions $y(0)=0$ and $u(0)=0$:\n- Input: $u(1)=1$, $u(2)=0$, $u(3)=1$, $u(4)=0$, $u(5)=1$, $u(6)=0$.\n- Output: $y(1)=1$, $y(2)=1$, $y(3)=0$, $y(4)=2$, $y(5)=1.5$, $y(6)=0.75$.\n\nForm the regressor matrix $\\Phi$ and the data vector $y$ by stacking rows for $t\\in\\{3,4,5,6\\}$, with each row of $\\Phi$ given by $\\begin{pmatrix} y(t-1) & u(t-1) \\end{pmatrix}$ and the corresponding entry of $y$ given by $y(t)$. Construct a valid instrument matrix $Z$ using only lagged inputs such that each instrument is correlated with the columns of $\\Phi$ but uncorrelated with $v(t)$ for the above data. Then, using the instrumental variables method and the sample moment conditions implied by instrument exogeneity and relevance, compute the instrumental variables estimate $\\hat{\\theta}_{\\mathrm{IV}}$.\n\nExpress your final estimate as a single row vector $\\begin{pmatrix} \\hat{a} & \\hat{b} \\end{pmatrix}$. No rounding is required.", "solution": "The system equations can be written in a linear regression form for a set of time indices. We are instructed to use $t \\in \\{3, 4, 5, 6\\}$. This gives us a system of $4$ equations:\n$$\n\\begin{pmatrix} y(3) \\\\ y(4) \\\\ y(5) \\\\ y(6) \\end{pmatrix} = \\begin{pmatrix} y(2) & u(2) \\\\ y(3) & u(3) \\\\ y(4) & u(4) \\\\ y(5) & u(5) \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} + \\begin{pmatrix} v(3) \\\\ v(4) \\\\ v(5) \\\\ v(6) \\end{pmatrix}\n$$\nThis is expressed in matrix form as $\\mathbf{y} = \\Phi\\theta + \\mathbf{v}$. Using the provided data, we construct the output vector $\\mathbf{y}$ and the regressor matrix $\\Phi$:\n$$\n\\mathbf{y} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1.5 \\\\ 0.75 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ \\frac{3}{2} \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n$$\n\\Phi = \\begin{pmatrix} y(2) & u(2) \\\\ y(3) & u(3) \\\\ y(4) & u(4) \\\\ y(5) & u(5) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ 1.5 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ \\frac{3}{2} & 1 \\end{pmatrix}\n$$\nThe OLS estimator would be biased because the regressor $y(t-1)$ is correlated with the noise $v(t)$ via the term $e(t-1)$. We must choose instruments that are correlated with the regressors in $\\Phi$ but uncorrelated with the noise vector $\\mathbf{v}$. The problem directs us to use lagged inputs. A standard and valid choice for the instrument vector $z_t$ corresponding to the regressor vector $\\phi_t^{\\top} = \\begin{pmatrix} y(t-1) & u(t-1) \\end{pmatrix}$ is to replace the endogenous regressor $y(t-1)$ with a lagged input, such as $u(t-2)$, while keeping the exogenous regressor $u(t-1)$ as its own instrument. This yields the instrument vector $z_t^{\\top} = \\begin{pmatrix} u(t-2) & u(t-1) \\end{pmatrix}$. This choice satisfies the exogeneity condition, as inputs $\\{u(\\tau)\\}$ are independent of the noise process $\\{e(t)\\}$, and hence uncorrelated with $v(t) = e(t) + c e(t-1)$ for $\\tau < t$.\n\nWe construct the instrument matrix $Z$ for $t \\in \\{3, 4, 5, 6\\}$:\nFor $t=3$, $z_3^{\\top} = \\begin{pmatrix} u(1) & u(2) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\nFor $t=4$, $z_4^{\\top} = \\begin{pmatrix} u(2) & u(3) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$.\nFor $t=5$, $z_5^{\\top} = \\begin{pmatrix} u(3) & u(4) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\nFor $t=6$, $z_6^{\\top} = \\begin{pmatrix} u(4) & u(5) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$.\nStacking these row vectors gives the instrument matrix $Z$:\n$$\nZ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe Instrumental Variables estimator $\\hat{\\theta}_{\\mathrm{IV}}$ is given by the solution to the sample moment conditions $Z^{\\top}(\\mathbf{y} - \\Phi\\hat{\\theta}) = 0$, which is:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (Z^{\\top}\\Phi)^{-1}Z^{\\top}\\mathbf{y}\n$$\nWe compute the required matrix products. First, $Z^{\\top}\\Phi$:\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ \\frac{3}{2} & 1 \\end{pmatrix} = \\begin{pmatrix} (1 \\cdot 1 + 1 \\cdot 2) & (1 \\cdot 0 + 1 \\cdot 0) \\\\ (1 \\cdot 0 + 1 \\cdot \\frac{3}{2}) & (1 \\cdot 1 + 1 \\cdot 1) \\end{pmatrix} = \\begin{pmatrix} 3 & 0 \\\\ \\frac{3}{2} & 2 \\end{pmatrix}\n$$\nThe determinant of this matrix is $(3)(2) - (0)(\\frac{3}{2}) = 6$, which is non-zero. Thus, the matrix is invertible, and our choice of instruments is valid for this data set. The inverse is:\n$$\n(Z^{\\top}\\Phi)^{-1} = \\frac{1}{6} \\begin{pmatrix} 2 & 0 \\\\ -\\frac{3}{2} & 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{6} & 0 \\\\ -\\frac{3}{12} & \\frac{3}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ -\\frac{1}{4} & \\frac{1}{2} \\end{pmatrix}\n$$\nNext, we compute $Z^{\\top}\\mathbf{y}$:\n$$\nZ^{\\top}\\mathbf{y} = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ \\frac{3}{2} \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + 1 \\cdot \\frac{3}{2} \\\\ 1 \\cdot 2 + 1 \\cdot \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ 2 + \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{11}{4} \\end{pmatrix}\n$$\nFinally, we compute the IV estimate $\\hat{\\theta}_{\\mathrm{IV}}$:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\begin{pmatrix} \\hat{a} \\\\ \\hat{b} \\end{pmatrix} = (Z^{\\top}\\Phi)^{-1} (Z^{\\top}\\mathbf{y}) = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ -\\frac{1}{4} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{11}{4} \\end{pmatrix}\n$$\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\begin{pmatrix} (\\frac{1}{3})(\\frac{3}{2}) + (0)(\\frac{11}{4}) \\\\ (-\\frac{1}{4})(\\frac{3}{2}) + (\\frac{1}{2})(\\frac{11}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{3}{8} + \\frac{11}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{8}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}\n$$\nThe estimated parameters are $\\hat{a} = \\frac{1}{2} = 0.5$ and $\\hat{b} = 1$. The problem asks for the result as a single row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5 & 1 \\end{pmatrix}}\n$$", "id": "2878920"}, {"introduction": "A central challenge in system identification is choosing an appropriate model complexity; an overly simple model underfits the data, while an overly complex one overfits the noise, leading to poor predictive performance. Information criteria, such as the Bayesian Information Criterion (BIC), provide a principled way to navigate this crucial bias-variance trade-off. By working through this exercise, you will learn to apply BIC to penalize model complexity and select the most plausible model order from a set of candidates, a critical skill for building parsimonious and generalizable models. [@problem_id:2878934]", "problem": "You are given a family of discrete-time single-input single-output AutoRegressive with eXogenous input (ARX) models indexed by an order parameter $p \\in \\{1,2,3,4,5\\}$. For each candidate order $p$, an identification experiment on a data record of length $N=1000$ produced a one-step-ahead residual sequence that is well-modeled as independent and identically distributed Gaussian noise with zero mean and unknown variance. The corresponding estimated residual variances $\\hat{\\sigma}_{p}^{2}$ (computed as the empirical mean of squared residuals) and the number of free model parameters $k_{p}$ are listed below:\n- $p=1$: $\\hat{\\sigma}_{1}^{2} = 1.05$, $k_{1} = 3$.\n- $p=2$: $\\hat{\\sigma}_{2}^{2} = 0.98$, $k_{2} = 5$.\n- $p=3$: $\\hat{\\sigma}_{3}^{2} = 0.94$, $k_{3} = 7$.\n- $p=4$: $\\hat{\\sigma}_{4}^{2} = 0.93$, $k_{4} = 9$.\n- $p=5$: $\\hat{\\sigma}_{5}^{2} = 0.929$, $k_{5} = 11$.\n\nStarting from the Gaussian likelihood for the residuals and the definition of the Bayesian Information Criterion (BIC) as a large-sample approximation to the negative twice log model evidence with a complexity penalty proportional to the number of free parameters and the logarithm of the sample size, derive a selection score that depends only on $N$, $\\hat{\\sigma}_{p}^{2}$, and $k_{p}$ and is sufficient to compare these models. Then, using that score, compute the value for each $p \\in \\{1,2,3,4,5\\}$ and identify the order $p^{\\star}$ that should be selected.\n\nReport only the minimizing order index $p^{\\star}$ as your final answer. No rounding instruction is necessary because $p^{\\star}$ is an integer.", "solution": "Let the one-step-ahead prediction errors, or residuals, for a model of order $p$ be denoted by the sequence $\\{\\epsilon_p(t)\\}_{t=1}^{N}$. The problem states that these residuals are well-modeled as independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and variance $\\sigma_p^2$. The probability density function (PDF) of a single residual $\\epsilon_p(t)$ is given by:\n$$ f(\\epsilon_p(t) | \\sigma_p^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) $$\nGiven a data record of length $N$, the likelihood function $L_p$ for the model of order $p$ is the joint probability of observing the entire sequence of residuals. Due to the i.i.d. assumption, this is the product of the individual PDFs:\n$$ L_p(\\{\\epsilon_p(t)\\} | \\sigma_p^2) = \\prod_{t=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) $$\nIt is more convenient to work with the log-likelihood function, $\\ln L_p$:\n$$ \\ln L_p = \\sum_{t=1}^{N} \\ln\\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) \\right] = \\sum_{t=1}^{N} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_p^2) - \\frac{\\epsilon_p(t)^2}{2\\sigma_p^2} \\right] $$\n$$ \\ln L_p = -\\frac{N}{2}\\ln(2\\pi\\sigma_p^2) - \\frac{1}{2\\sigma_p^2}\\sum_{t=1}^{N} \\epsilon_p(t)^2 $$\nThe problem provides the estimated residual variance $\\hat{\\sigma}_p^2$, which is computed as the empirical mean of the squared residuals. This corresponds to the maximum likelihood estimate (MLE) of the variance. The sum of squared residuals is thus $\\sum_{t=1}^{N} \\epsilon_p(t)^2 = N\\hat{\\sigma}_p^2$.\n\nTo find the BIC, we need the maximized value of the log-likelihood, $\\ln L_{max, p}$. This is obtained by substituting the MLE $\\hat{\\sigma}_p^2$ into the log-likelihood expression:\n$$ \\ln L_{max, p} = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}_p^2) - \\frac{1}{2\\hat{\\sigma}_p^2}(N\\hat{\\sigma}_p^2) = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}_p^2) - \\frac{N}{2} $$\n$$ \\ln L_{max, p} = -\\frac{N}{2} \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] $$\nThe Bayesian Information Criterion (BIC) is generally defined as:\n$$ \\text{BIC}_p = -2 \\ln L_{max, p} + k_p \\ln(N) $$\nwhere $k_p$ is the number of free parameters in the model of order $p$ and $N$ is the sample size. Substituting our expression for $\\ln L_{max, p}$:\n$$ \\text{BIC}_p = -2 \\left( -\\frac{N}{2} \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] \\right) + k_p \\ln(N) $$\n$$ \\text{BIC}_p = N \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] + k_p \\ln(N) $$\n$$ \\text{BIC}_p = N \\ln(\\hat{\\sigma}_p^2) + k_p \\ln(N) + N(\\ln(2\\pi) + 1) $$\nThe goal is to select the model order $p$ that minimizes $\\text{BIC}_p$. When comparing the BIC values for different orders $p$, any term that is constant across all models can be discarded. The term $N(\\ln(2\\pi) + 1)$ is such a constant. Therefore, a sufficient selection score $S_p$ for comparing the models is:\n$$ S_p = N \\ln(\\hat{\\sigma}_p^2) + k_p \\ln(N) $$\nThis expression depends only on the given quantities $N$, $\\hat{\\sigma}_p^2$, and $k_p$, as required. We now compute this score for each candidate order $p \\in \\{1, 2, 3, 4, 5\\}$, using the provided data: $N=1000$. The natural logarithm of the sample size is $\\ln(1000) \\approx 6.907755$.\n\nFor $p=1$: $k_1=3$, $\\hat{\\sigma}_1^2=1.05$.\n$S_1 = 1000 \\ln(1.05) + 3 \\ln(1000) \\approx 1000(0.048790) + 3(6.907755) \\approx 48.790 + 20.723 = 69.513$\n\nFor $p=2$: $k_2=5$, $\\hat{\\sigma}_2^2=0.98$.\n$S_2 = 1000 \\ln(0.98) + 5 \\ln(1000) \\approx 1000(-0.020203) + 5(6.907755) \\approx -20.203 + 34.539 = 14.336$\n\nFor $p=3$: $k_3=7$, $\\hat{\\sigma}_3^2=0.94$.\n$S_3 = 1000 \\ln(0.94) + 7 \\ln(1000) \\approx 1000(-0.061875) + 7(6.907755) \\approx -61.875 + 48.354 = -13.521$\n\nFor $p=4$: $k_4=9$, $\\hat{\\sigma}_4^2=0.93$.\n$S_4 = 1000 \\ln(0.93) + 9 \\ln(1000) \\approx 1000(-0.072571) + 9(6.907755) \\approx -72.571 + 62.170 = -10.401$\n\nFor $p=5$: $k_5=11$, $\\hat{\\sigma}_5^2=0.929$.\n$S_5 = 1000 \\ln(0.929) + 11 \\ln(1000) \\approx 1000(-0.073651) + 11(6.907755) \\approx -73.651 + 75.985 = 2.334$\n\nComparing the computed scores:\n$S_1 \\approx 69.513$\n$S_2 \\approx 14.336$\n$S_3 \\approx -13.521$\n$S_4 \\approx -10.401$\n$S_5 \\approx 2.334$\n\nThe minimum score is achieved for $p=3$. This indicates that the model of order $3$ provides the best trade-off between goodness of fit (low residual variance) and model complexity (low number of parameters), according to the Bayesian Information Criterion. Therefore, the selected model order is $p^{\\star}=3$.", "answer": "$$\\boxed{3}$$", "id": "2878934"}]}