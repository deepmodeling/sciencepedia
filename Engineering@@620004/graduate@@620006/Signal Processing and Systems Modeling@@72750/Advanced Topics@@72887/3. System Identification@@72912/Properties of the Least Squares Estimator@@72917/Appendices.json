{"hands_on_practices": [{"introduction": "The least squares method can be elegantly understood through the lens of linear algebra. The fitting process is equivalent to an orthogonal projection of the data vector onto the subspace spanned by the model's regressors. This exercise explores the properties of the projection matrix, often called the \"hat matrix,\" which formalizes this geometric view. By deriving its trace, you will uncover a fundamental quantity that measures the effective number of parameters or \"degrees of freedom\" consumed by the model fit [@problem_id:2897104].", "problem": "Consider a linear signal model in which an observed deterministic signal vector $y \\in \\mathbb{R}^{n}$ is modeled as a linear combination of $p$ known regressors collected as the columns of a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with full column rank $p$ and $p \\leq n$. The least squares estimator (LS) chooses coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^{p}$ to minimize the squared error $\\|y - X \\beta\\|_{2}^{2}$. Let the fitted signal be $\\hat{y} = X \\hat{\\beta}$, and define the linear mapping $P \\in \\mathbb{R}^{n \\times n}$ via $\\hat{y} = P y$ for every $y \\in \\mathbb{R}^{n}$.\n\nStarting from the optimality condition that characterizes the least squares solution and the orthogonality principle stating that the residual is orthogonal to the model subspace, derive an explicit formula for $P$, and prove from first principles that $P$ is symmetric and idempotent. Then, using only structural properties of orthogonal projectors and without assuming any particular basis for $\\mathbb{R}^{n}$, determine $\\operatorname{tr}(P)$ in terms of intrinsic properties of $X$. Interpret this trace as the effective degrees of freedom used by least squares in this model.\n\nProvide as your final answer the analytical value of $\\operatorname{tr}(P)$ expressed in terms of $p$. Do not include any units. If you introduce any additional notation, define it clearly. Your final answer must be a single closed-form expression.", "solution": "We begin from the definition of the least squares estimator: $\\hat{\\beta}$ minimizes $\\|y - X \\beta\\|_{2}^{2}$ over $\\beta \\in \\mathbb{R}^{p}$. The fundamental optimality condition is the normal equations, derived by differentiating the objective with respect to $\\beta$ and setting the derivative to zero:\n$$\nX^{\\top}\\bigl(y - X \\hat{\\beta}\\bigr) = 0.\n$$\nUnder the assumption that $X$ has full column rank $p$, the Gram matrix $X^{\\top} X \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite and hence invertible. Therefore,\n$$\n\\hat{\\beta} = \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\nThe fitted vector is then\n$$\n\\hat{y} = X \\hat{\\beta} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\nBy definition of the linear mapping $P$ with $\\hat{y} = P y$ for all $y \\in \\mathbb{R}^{n}$, we identify\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}.\n$$\n\nWe now establish that $P$ is symmetric. Using the properties of transpose and symmetry of $X^{\\top} X$,\n$$\nP^{\\top} = \\bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\bigr)^{\\top} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} = P.\n$$\nNext, we verify idempotence, i.e., $P^{2} = P$. Compute\n$$\nP^{2} = \\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\n= X \\bigl(X^{\\top} X\\bigr)^{-1} \\underbrace{X^{\\top} X}_{\\text{invertible}} \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= P.\n$$\nThus $P$ is a symmetric idempotent matrix, i.e., an orthogonal projector onto the model subspace $\\mathcal{R}(X)$, the range (column space) of $X$. This also follows from the orthogonality principle: the residual $r = y - \\hat{y}$ satisfies $X^{\\top} r = 0$, which states $r \\in \\mathcal{R}(X)^{\\perp}$ and $\\hat{y} \\in \\mathcal{R}(X)$, hence $P$ is the orthogonal projector onto $\\mathcal{R}(X)$.\n\nTo determine $\\operatorname{tr}(P)$ from intrinsic properties of $X$, observe that any real symmetric idempotent matrix has eigenvalues in $\\{0, 1\\}$. Indeed, if $P v = \\lambda v$ for some nonzero $v$, then $P^{2} v = \\lambda^{2} v$ but also $P^{2} v = P v = \\lambda v$, hence $\\lambda^{2} = \\lambda$, so $\\lambda \\in \\{0, 1\\}$. Moreover, the rank of $P$ equals the number of eigenvalues equal to $1$ and equals $\\dim\\bigl(\\mathcal{R}(P)\\bigr)$. Since $P$ is the orthogonal projector onto $\\mathcal{R}(X)$, we have $\\mathcal{R}(P) = \\mathcal{R}(X)$ and therefore\n$$\n\\operatorname{rank}(P) = \\dim\\bigl(\\mathcal{R}(X)\\bigr) = \\operatorname{rank}(X).\n$$\nBecause the trace of a matrix equals the sum of its eigenvalues, for a symmetric idempotent matrix $P$ this trace equals the number of ones among its eigenvalues, which is the rank of $P$. Therefore,\n$$\n\\operatorname{tr}(P) = \\operatorname{rank}(P) = \\operatorname{rank}(X).\n$$\nUnder the given assumption that $X$ has full column rank $p$, we have $\\operatorname{rank}(X) = p$, and hence\n$$\n\\operatorname{tr}(P) = p.\n$$\n\nAn alternative derivation that makes the structure explicit uses the Singular Value Decomposition (SVD). Let $X = U \\Sigma V^{\\top}$ be a compact Singular Value Decomposition (SVD) with $U \\in \\mathbb{R}^{n \\times n}$ orthogonal, $V \\in \\mathbb{R}^{p \\times p}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ having the form $\\Sigma = \\begin{bmatrix} D \\\\ 0 \\end{bmatrix}$ where $D \\in \\mathbb{R}^{p \\times p}$ is diagonal with strictly positive diagonal entries because $X$ has full column rank $p$. Then\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= U \\Sigma V^{\\top} \\bigl(V \\Sigma^{\\top} \\Sigma V^{\\top}\\bigr)^{-1} V \\Sigma^{\\top} U^{\\top}\n= U \\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top} U^{\\top}.\n$$\nSince $\\Sigma^{\\top} \\Sigma = D^{2}$, we obtain\n$$\n\\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top}\n= \\begin{bmatrix} D \\\\ 0 \\end{bmatrix} D^{-2} \\begin{bmatrix} D & 0 \\end{bmatrix}\n= \\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix},\n$$\nso\n$$\nP = U \\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix} U^{\\top}.\n$$\nThe trace is invariant under similarity by an orthogonal matrix $U$, hence\n$$\n\\operatorname{tr}(P) = \\operatorname{tr}\\!\\left(\\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix}\\right) = p.\n$$\n\nInterpretation as effective degrees of freedom: In least squares fitting, $P$ maps any data vector $y$ to its orthogonal projection onto the $p$-dimensional subspace $\\mathcal{R}(X)$. The trace $\\operatorname{tr}(P)$ equals $p$, which represents the effective number of parameters or linear constraints that the fit adapts to the data. Consequently, the residual subspace has dimension $n - p$, often interpreted as the remaining degrees of freedom available for assessing the discrepancy between the model and the data (e.g., for variance estimation in the presence of additive noise). Thus, $\\operatorname{tr}(P) = p$ quantifies the effective degrees of freedom used by least squares.", "answer": "$$\\boxed{p}$$", "id": "2897104"}, {"introduction": "A well-known property of ordinary least squares (OLS) with an intercept is that the sum of the residuals is exactly zero. This is a direct consequence of the orthogonality principle, which dictates that the residual vector is orthogonal to the model's subspace. This practice challenges you to investigate whether this property is a universal feature of linear estimators or specific to OLS, by examining how it behaves under different estimation frameworks like weighted least squares, generalized least squares, and ridge regression [@problem_id:2897144]. This deepens your understanding of the underlying assumptions of each method.", "problem": "Consider the linear measurement model $y \\in \\mathbb{R}^n$ with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and coefficient vector $\\beta \\in \\mathbb{R}^p$. Denote the residual vector by $r = y - X \\hat{\\beta}$, where $\\hat{\\beta}$ is an estimator determined by minimizing a specified objective. Let $\\mathbf{1} \\in \\mathbb{R}^n$ denote the vector of all ones. The following estimation problems are defined from first principles by their optimization objectives:\n- Ordinary least squares (OLS): minimize $\\|y - X \\beta\\|_2^2$.\n- Weighted least squares (WLS) with positive diagonal weights $W = \\mathrm{diag}(w_1,\\dots,w_n) \\succ 0$: minimize $(y - X \\beta)^\\top W (y - X \\beta)$.\n- Generalized least squares (GLS) with known, symmetric positive definite covariance matrix $\\Sigma \\succ 0$: minimize $(y - X \\beta)^\\top \\Sigma^{-1} (y - X \\beta)$.\n- Ridge regression with penalty parameter $\\lambda > 0$: minimize $\\|y - X \\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2$.\n\nAssume that whenever we say “$X$ includes an intercept,” the matrix $X$ has a column equal to $\\mathbf{1}$. Select all statements that are necessarily true:\n\nA. In ordinary least squares with a column of ones in $X$, the residual vector $r$ satisfies $\\mathbf{1}^\\top r = 0$ for any data $y$.\n\nB. In weighted least squares with $W = \\mathrm{diag}(w_1,\\dots,w_n) \\succ 0$ and a column of ones in $X$, the unweighted sum of residuals is zero: $\\mathbf{1}^\\top r = 0$.\n\nC. In weighted least squares with $W = \\mathrm{diag}(w_1,\\dots,w_n) \\succ 0$ and a column of ones in $X$, the weighted sum of residuals is zero: $\\mathbf{1}^\\top W r = 0$.\n\nD. In ridge regression that penalizes all coefficients including the intercept, with a column of ones in $X$, the residuals sum to zero: $\\mathbf{1}^\\top r = 0$.\n\nE. In generalized least squares with $\\Sigma \\succ 0$ and a column of ones in $X$, the condition $\\mathbf{1}^\\top \\Sigma^{-1} r = 0$ holds at the optimum.\n\nF. If the intercept is omitted but both the response $y$ and all columns of $X$ are centered to have sample mean $0$, then the ordinary least squares residuals still satisfy $\\mathbf{1}^\\top r = 0$.\n\nG. In ordinary least squares with an intercept, the residuals have zero sum only in expectation when the noise has mean $0$, but for a fixed dataset the sum need not be exactly $0$.\n\nChoose all that apply and justify your choices by reasoning from the definitions of the objectives above, without assuming any distributional model for the noise beyond what is explicitly stated.", "solution": "We derive each statement from the defining optimization problems by using first-order optimality (normal equations) and the geometry of projections. Let $r = y - X \\hat{\\beta}$ denote the residual vector at the solution in each case.\n\nFoundational facts used:\n- For a differentiable quadratic objective in $\\beta$, stationarity at the minimum requires the gradient with respect to $\\beta$ to be $0$.\n- In ordinary least squares, $\\hat{y} = X \\hat{\\beta}$ is the orthogonal projection of $y$ onto the column space of $X$, so $r$ is orthogonal to every column of $X$ in the standard Euclidean inner product.\n- In weighted and generalized least squares, orthogonality is with respect to the inner product induced by the weight matrix.\n\nAnalyze each case:\n\nOrdinary least squares (OLS). The objective is $J(\\beta) = \\|y - X \\beta\\|_2^2 = (y - X \\beta)^\\top (y - X \\beta)$. The gradient is\n$$\n\\nabla_\\beta J(\\beta) = -2 X^\\top (y - X \\beta) = -2 X^\\top r.\n$$\nAt the minimizer, $X^\\top r = 0$. This means $r$ is orthogonal (in the standard inner product) to every column of $X$. If $X$ contains a column equal to $\\mathbf{1}$, then $r$ is orthogonal to $\\mathbf{1}$, i.e., $\\mathbf{1}^\\top r = 0$. This is an algebraic identity that holds for any data $y$.\n\nWeighted least squares (WLS). The objective is $J(\\beta) = (y - X \\beta)^\\top W (y - X \\beta)$ with $W \\succ 0$ diagonal. The gradient is\n$$\n\\nabla_\\beta J(\\beta) = -2 X^\\top W (y - X \\beta) = -2 X^\\top W r.\n$$\nAt the minimizer, $X^\\top W r = 0$. Thus $r$ is $W$-orthogonal to each column of $X$, meaning $\\langle r, x_j \\rangle_W = x_j^\\top W r = 0$. If $X$ contains a column $\\mathbf{1}$, then $\\mathbf{1}^\\top W r = 0$. There is no implication that $\\mathbf{1}^\\top r = 0$ unless $W$ is proportional to the identity.\n\nGeneralized least squares (GLS). The objective is $J(\\beta) = (y - X \\beta)^\\top \\Sigma^{-1} (y - X \\beta)$ with $\\Sigma \\succ 0$. The gradient is\n$$\n\\nabla_\\beta J(\\beta) = -2 X^\\top \\Sigma^{-1} (y - X \\beta) = -2 X^\\top \\Sigma^{-1} r.\n$$\nAt the minimizer, $X^\\top \\Sigma^{-1} r = 0$. Therefore, with an intercept column, $\\mathbf{1}^\\top \\Sigma^{-1} r = 0$. Again, there is no guarantee that $\\mathbf{1}^\\top r = 0$ unless $\\Sigma$ is proportional to the identity.\n\nRidge regression. The objective is $J(\\beta) = \\|y - X \\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2$ with $\\lambda > 0$. The gradient is\n$$\n\\nabla_\\beta J(\\beta) = -2 X^\\top (y - X \\beta) + 2 \\lambda \\beta = -2 X^\\top r + 2 \\lambda \\beta.\n$$\nStationarity gives $X^\\top r = \\lambda \\beta$. This generally breaks the orthogonality of $r$ to the columns of $X$. If the intercept coefficient is penalized (the intercept is one component of $\\beta$ and is also multiplied by $\\lambda$), then along the intercept column we have $\\mathbf{1}^\\top r = \\lambda \\beta_0$, which is not necessarily $0$. Only if the intercept is not penalized (i.e., excluded from the penalty) do we recover $\\mathbf{1}^\\top r = 0$.\n\nCentered OLS without an intercept. Suppose the intercept column is omitted, but every column of $X$ satisfies $\\mathbf{1}^\\top x_j = 0$ (sample mean $0$), and $y$ is centered so that $\\mathbf{1}^\\top y = 0$. The OLS fitted values are $\\hat{y} = X \\hat{\\beta}$, which is a linear combination of the centered columns of $X$, hence also centered: $\\mathbf{1}^\\top \\hat{y} = \\mathbf{1}^\\top X \\hat{\\beta} = (\\mathbf{1}^\\top X) \\hat{\\beta} = 0$. Therefore, the residual sum is\n$$\n\\mathbf{1}^\\top r = \\mathbf{1}^\\top (y - \\hat{y}) = \\mathbf{1}^\\top y - \\mathbf{1}^\\top \\hat{y} = 0 - 0 = 0.\n$$\nThis is an algebraic identity under the centering conditions.\n\nOption-by-option evaluation:\n\nA. OLS with an intercept implies $X^\\top r = 0$ and, in particular, $\\mathbf{1}^\\top r = 0$. Verdict: Correct.\n\nB. WLS orthogonality is with respect to the $W$-inner product. It yields $\\mathbf{1}^\\top W r = 0$, not necessarily $\\mathbf{1}^\\top r = 0$ unless $W$ is proportional to the identity. Verdict: Incorrect.\n\nC. This is exactly the WLS orthogonality condition along the intercept direction. Verdict: Correct.\n\nD. Ridge with a penalized intercept satisfies $X^\\top r = \\lambda \\beta$, which along the intercept gives $\\mathbf{1}^\\top r = \\lambda \\beta_0 \\neq 0$ in general. Verdict: Incorrect.\n\nE. GLS first-order optimality gives $X^\\top \\Sigma^{-1} r = 0$, hence with an intercept $\\mathbf{1}^\\top \\Sigma^{-1} r = 0$. Verdict: Correct.\n\nF. With centered $y$ and centered regressors but no intercept, any fitted $\\hat{y}$ is centered, so $\\mathbf{1}^\\top r = \\mathbf{1}^\\top y - \\mathbf{1}^\\top \\hat{y} = 0$. Verdict: Correct.\n\nG. In OLS with an intercept, $\\mathbf{1}^\\top r = 0$ holds deterministically for any fixed $y$, independent of noise distributional assumptions. Verdict: Incorrect.\n\nTherefore, the correct statements are A, C, E, and F.", "answer": "$$\\boxed{ACEF}$$", "id": "2897144"}, {"introduction": "While understanding the algebraic and geometric properties of an estimator is crucial, its ultimate value lies in its ability to generalize to new, unseen data. This exercise shifts the focus from in-sample fit to out-of-sample prediction performance. You will derive the expected squared prediction risk, a key metric for generalization error, and decompose it into its bias and variance components [@problem_id:2897085]. This practice provides a rigorous link between the statistical properties of the OLS estimator and its practical utility in real-world prediction tasks.", "problem": "A wide-sense stationary discrete-time linear time-invariant system with unknown finite impulse response vector $\\beta_{0} \\in \\mathbb{R}^{p}$ is identified from $n$ measurements using ordinary least squares (OLS). The training data are modeled as $y = X \\beta_{0} + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is a fixed full-column-rank design matrix constructed from the input sequence, $\\varepsilon \\in \\mathbb{R}^{n}$ is zero-mean noise with covariance $\\sigma^{2} I_{n}$, and $I_{n}$ is the $n \\times n$ identity matrix. The OLS estimator $\\hat{\\beta}$ is defined as the minimizer of the empirical sum of squared residuals, and is used to predict the noiseless output for a new input feature vector $x \\in \\mathbb{R}^{p}$ via the predictor $x^{\\top} \\hat{\\beta}$. Assume that, at deployment, new inputs $x$ are independent of the training noise and are drawn from a distribution with zero mean and covariance $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$.\n\nUsing only the stated modeling assumptions and first principles, derive the expected out-of-sample squared prediction risk for the noiseless target $x^{\\top} \\beta_{0}$, defined by\n$$\nR_{\\mathrm{out}} \\triangleq \\mathbb{E}\\!\\left[\\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X \\right],\n$$\nwhere the expectation is over the training noise and the new input distribution for $x$. Express $R_{\\mathrm{out}}$ explicitly in terms of $X$, $\\Sigma_{x}$, and $\\sigma^{2}$, and separate it into its squared bias and variance contributions with respect to the distribution of $\\hat{\\beta}$. Provide the final answer as analytic expressions; no numerical approximation is required.", "solution": "The problem requires the derivation of the expected out-of-sample squared prediction risk, $R_{\\mathrm{out}}$, for an ordinary least squares (OLS) estimator and its decomposition into squared bias and variance components.\n\nThe OLS estimator $\\hat{\\beta}$ that minimizes the sum of squared residuals $\\|y - X\\beta\\|_{2}^{2}$ is given by the normal equations, yielding:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\nThe existence of $(X^{\\top}X)^{-1}$ is guaranteed because $X$ is full column rank. We substitute the model $y = X \\beta_{0} + \\varepsilon$ into this expression:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X \\beta_{0} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon\n$$\nThis equation expresses the estimator $\\hat{\\beta}$ in terms of the true parameter vector $\\beta_{0}$ and the noise $\\varepsilon$.\n\nThe risk $R_{\\mathrm{out}}$ is defined as the expectation of the squared prediction error, $(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0})^{2}$, over the distributions of the training noise $\\varepsilon$ and the new input $x$. Since $x$ and $\\varepsilon$ are independent, and the expectation is conditioned on $X$, we can write:\n$$\nR_{\\mathrm{out}} = \\mathbb{E}_{x, \\varepsilon} \\left[ \\left(x^{\\top} (\\hat{\\beta} - \\beta_{0})\\right)^{2} \\,\\middle|\\, X \\right] = \\mathbb{E}_{x} \\left[ \\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] \\right]\n$$\nThe inner expectation is the conditional Mean Squared Error (MSE) of the predictor $x^{\\top}\\hat{\\beta}$ for a fixed new input $x$. We can decompose this conditional MSE into its squared bias and variance components with respect to the distribution of $\\varepsilon$:\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] = \\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\top} \\beta_{0} \\right)^{2} + \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right)\n$$\nLet us analyze each term.\n\nFirst, the bias term. The expectation of the predictor $x^{\\top}\\hat{\\beta}$ over $\\varepsilon$ is:\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] = x^{\\top} \\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right]\n$$\nWe find the expectation of the estimator $\\hat{\\beta}$:\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\mathbb{E}_{\\varepsilon} \\left[ \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X \\right] = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}_{\\varepsilon}[\\varepsilon]\n$$\nSince $\\mathbb{E}[\\varepsilon] = 0$, we have $\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\beta_{0}$. The OLS estimator $\\hat{\\beta}$ is unbiased for $\\beta_{0}$. Consequently, the predictor $x^{\\top}\\hat{\\beta}$ is unbiased for $x^{\\top}\\beta_{0}$ for any given $x$:\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] = x^{\\top}\\beta_{0}\n$$\nThe squared bias contribution for a fixed $x$ is therefore zero:\n$$\n\\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\top} \\beta_{0} \\right)^{2} = (x^{\\top}\\beta_{0} - x^{\\top}\\beta_{0})^{2} = 0\n$$\nThe average squared bias contribution to $R_{\\mathrm{out}}$ is the expectation of this term over $x$, which is $\\mathbb{E}_{x}[0]=0$.\n\nNext, the variance term. The variance of the predictor $x^{\\top}\\hat{\\beta}$ over $\\varepsilon$ for a fixed $x$ is:\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}\\beta_{0} + x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X, x \\right)\n$$\nSince $x^{\\top}\\beta_{0}$ is a constant with respect to $\\varepsilon$, this simplifies to:\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X, x \\right)\n$$\nLet the row vector $c^{\\top} = x^{\\top}(X^{\\top}X)^{-1}X^{\\top}$. Using the property $\\mathrm{Var}(c^{\\top}\\varepsilon) = c^{\\top}\\mathrm{Cov}(\\varepsilon)c$, and given $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2}I_{n}$, we get:\n$$\n\\mathrm{Var}_{\\varepsilon} (c^{\\top}\\varepsilon) = c^{\\top}(\\sigma^{2}I_{n})c = \\sigma^{2}c^{\\top}c\n$$\nSubstituting back $c^{\\top}$:\n$$\nc^{\\top}c = \\left(x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\right) \\left(X(X^{\\top}X)^{-1}x\\right) = x^{\\top}(X^{\\top}X)^{-1}(X^{\\top}X)(X^{\\top}X)^{-1}x = x^{\\top}(X^{\\top}X)^{-1}x\n$$\nThus, the variance contribution for a fixed $x$ is $\\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x$.\n\nTo find the total variance contribution to $R_{\\mathrm{out}}$, we must average this quantity over the distribution of $x$:\n$$\n\\text{Variance Contribution} = \\mathbb{E}_{x} \\left[ \\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x \\right] = \\sigma^{2} \\mathbb{E}_{x} \\left[ x^{\\top}(X^{\\top}X)^{-1}x \\right]\n$$\nThe term inside the expectation is a quadratic form in the random vector $x$. For a random vector $x$ with mean $\\mu_{x}$ and covariance $\\Sigma_{x}$, and a constant matrix $M$, the expectation of the quadratic form $x^{\\top}Mx$ is given by $\\mathbb{E}[x^{\\top}Mx] = \\mathrm{Tr}(M\\Sigma_{x}) + \\mu_{x}^{\\top}M\\mu_{x}$. In our case, $M=(X^{\\top}X)^{-1}$, $\\mu_{x}=0$, and the covariance is $\\Sigma_{x}$. Therefore:\n$$\n\\mathbb{E}_{x} \\left[ x^{\\top}(X^{\\top}X)^{-1}x \\right] = \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right) + 0^{\\top}(X^{\\top}X)^{-1}0 = \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)\n$$\nThe final variance contribution to the risk is $\\sigma^{2} \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)$.\n\nIn summary, the total risk $R_{\\mathrm{out}}$ is the sum of the squared bias and variance contributions:\n$$\nR_{\\mathrm{out}} = \\text{Squared Bias Contribution} + \\text{Variance Contribution} = 0 + \\sigma^{2} \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)\n$$\nThe squared bias contribution is $0$, and the variance contribution is $\\sigma^{2} \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & \\sigma^{2} \\mathrm{Tr}((X^{\\top}X)^{-1} \\Sigma_{x}) \\end{pmatrix}}\n$$", "id": "2897085"}]}