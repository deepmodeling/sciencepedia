{"hands_on_practices": [{"introduction": "A cornerstone of model validation is ensuring that the residuals are indistinguishable from white noise, meaning they are serially uncorrelated. This exercise has you apply the Ljung-Box test, a critical \"portmanteau\" tool for assessing the overall significance of autocorrelations in a residual series. By working from first principles, you will derive the correct degrees of freedom for the test's reference distribution, a crucial adjustment when residuals originate from a fitted parametric model. [@problem_id:2885037]", "problem": "A discrete-time, wide-sense stationary stochastic process is modeled in the linear time series framework and fitted by an Autoregressive Moving-Average (ARMA) model. You estimate an Autoregressive Moving-Average (ARMA) model of order $(2,1)$ to a scalar time series of length $N=500$ using standard maximum likelihood techniques. Let $\\{e_t\\}_{t=1}^{N}$ denote the resulting one-step-ahead prediction residuals. To assess residual whiteness, you plan to use the Ljung–Box (LB) portmanteau statistic $Q_{\\mathrm{LB}}(m)$ based on autocorrelations up to lag $m=20$.\n\nStarting from first principles about residual whiteness and large-sample behavior of sample autocorrelations under the null hypothesis that $\\{e_t\\}$ are independent and identically distributed (i.i.d.) with zero mean and finite variance, and using the well-tested asymptotic fact that, when no dynamic parameters are estimated, the sum of $m$ squared, appropriately scaled residual autocorrelations is approximately chi-squared distributed with $m$ degrees of freedom, derive the appropriate degrees of freedom to be used for the reference chi-squared distribution of $Q_{\\mathrm{LB}}(m)$ when an ARMA$(2,1)$ model is estimated from the same data.\n\nThen, using this reference, state a decision rule at significance level $\\alpha=0.05$ for testing the null hypothesis that the residuals are i.i.d. (that is, white noise), in terms of the upper quantile of the chi-squared distribution. Do not compute any numerical quantiles.\n\nReport the degrees of freedom as your final answer. No rounding is required, and no units should be included in the final answer.", "solution": "The problem requires the derivation of the degrees of freedom for a Ljung-Box test applied to the residuals of an estimated Autoregressive Moving-Average (ARMA) model. The derivation must proceed from first principles.\n\nLet the observed time series be $\\{y_t\\}$. We have fitted an ARMA model of order $(p,q)$, which is specified as:\n$$ y_t - \\sum_{i=1}^p \\phi_i y_{t-i} = \\epsilon_t + \\sum_{j=1}^q \\theta_j \\epsilon_{t-j} $$\nwhere $\\{\\epsilon_t\\}$ is assumed to be a white noise process with zero mean and constant variance $\\sigma^2$. A white noise process is a sequence of independent and identically distributed (i.i.d.) random variables. In this problem, the model order is specified as ARMA($2,1$), so we have $p=2$ and $q=1$.\n\nThe problem states to begin with the asymptotic behavior of sample autocorrelations for a true white noise process. If we have a sequence of $N$ observations from a white noise process, $\\{\\epsilon_t\\}$, the sample autocorrelation at lag $k$, denoted $\\hat{\\rho}_k$, is for large $N$ approximately normally distributed with mean $0$ and variance $1/N$. Furthermore, the sample autocorrelations at different lags are approximately independent.\n$$ \\hat{\\rho}_k \\stackrel{\\text{approx}}{\\sim} N(0, 1/N) \\quad \\text{for } k>0 $$\nFrom this, it follows that the quantity $N\\hat{\\rho}_k^2$ is approximately distributed as a chi-squared random variable with $1$ degree of freedom, $\\chi^2(1)$. The sum of $m$ such independent quantities, a test statistic known as the Box-Pierce statistic, is given by $Q_{\\mathrm{BP}}(m) = N \\sum_{k=1}^m \\hat{\\rho}_k^2$. This statistic is asymptotically distributed as a chi-squared random variable with $m$ degrees of freedom, $\\chi^2(m)$.\n\nThe Ljung-Box statistic, $Q_{\\mathrm{LB}}(m)$, is a modification of the Box-Pierce statistic that provides a better approximation in finite samples:\n$$ Q_{\\mathrm{LB}}(m) = N(N+2) \\sum_{k=1}^m \\frac{\\hat{\\rho}_k^2}{N-k} $$\nUnder the null hypothesis that the series is white noise, $Q_{\\mathrm{LB}}(m)$ also converges to a $\\chi^2(m)$ distribution as $N \\to \\infty$. This is the baseline case where no model parameters are estimated from the data.\n\nHowever, the problem concerns the residuals, $\\{e_t\\}$, obtained after estimating the parameters of the ARMA($2,1$) model. The parameters are the $p=2$ autoregressive coefficients, $\\{\\phi_1, \\phi_2\\}$, and the $q=1$ moving-average coefficient, $\\{\\theta_1\\}$. The total number of estimated dynamic parameters is $p+q$. Let the estimates be $\\{\\hat{\\phi}_1, \\hat{\\phi}_2\\}$ and $\\{\\hat{\\theta}_1\\}$. The residuals are computed as:\n$$ e_t = y_t - \\hat{\\phi}_1 y_{t-1} - \\hat{\\phi}_2 y_{t-2} - \\hat{\\theta}_1 e_{t-1} $$\nThe estimation procedure, such as maximum likelihood, is designed to find parameter values that make the residuals $\\{e_t\\}$ behave as closely as possible to a white noise sequence. This process imposes constraints on the residuals. Essentially, the estimation process forces the sum of squared residuals to be minimized, which in turn reduces the magnitude of the sample autocorrelations of the residuals, which we denote by $\\hat{r}_k$. This effect is most pronounced for small lags.\n\nBecause the estimation of $p+q$ parameters introduces dependencies and constraints, the resulting residual autocorrelations $\\{\\hat{r}_k\\}_{k=1}^m$ are no longer asymptotically independent. The theory developed by Box and Pierce (1970) demonstrates that the estimation of $p+q$ parameters reduces the degrees of freedom of the chi-squared distribution by exactly $p+q$. Each estimated parameter can be thought of as \"explaining\" one degree of freedom in the data's correlation structure, leaving fewer degrees of freedom for the test statistic.\n\nTherefore, the asymptotic distribution of the Ljung-Box statistic when calculated on the residuals of a correctly specified ARMA($p,q$) model is not $\\chi^2(m)$, but rather a chi-squared distribution with degrees of freedom ($df$) given by:\n$$ df = m - (p+q) $$\nThis result holds provided that the number of lags, $m$, is sufficiently large compared to $p$ and $q$.\n\nFor the specific problem given:\n- The model is ARMA($2,1$), so $p=2$ and $q=1$.\n- The number of estimated parameters is $p+q = 2+1 = 3$.\n- The Ljung-Box test is performed up to lag $m=20$.\n\nSubstituting these values into the formula for degrees of freedom:\n$$ df = 20 - (2+1) = 20 - 3 = 17 $$\nThe reference distribution for the test statistic $Q_{\\mathrm{LB}}(20)$ is therefore a chi-squared distribution with $17$ degrees of freedom.\n\nThe problem also requests a decision rule for testing the null hypothesis $H_0$ that the residuals are i.i.d. (white noise) at a significance level of $\\alpha=0.05$. The alternative hypothesis $H_1$ is that the residuals are not white noise, implying serial correlation. High values of the $Q_{\\mathrm{LB}}(m)$ statistic suggest the presence of significant autocorrelation, providing evidence against $H_0$. Thus, a one-sided, upper-tail test is appropriate.\n\nThe decision rule is to reject $H_0$ if the computed test statistic $Q_{\\mathrm{LB}}(20)$ is greater than the critical value from the $\\chi^2(17)$ distribution. The critical value is the upper $(1-\\alpha)$ quantile of this distribution, denoted $\\chi^2_{1-\\alpha, df}$.\nWith $\\alpha=0.05$ and $df=17$, the decision rule is formally stated as:\nReject $H_0$ if $Q_{\\mathrm{LB}}(20) > \\chi^2_{0.95, 17}$.\nIf the calculated statistic does not exceed this value, we fail to reject the null hypothesis, and the model is considered adequate with respect to residual whiteness. The final answer is the number of degrees of freedom derived.", "answer": "$$\n\\boxed{17}\n$$", "id": "2885037"}, {"introduction": "When a model fails the whiteness test, the structure of its residuals contains vital clues about the underlying misspecification. This practice explores a classic scenario—over-differencing—and asks you to derive the theoretical properties of the resulting non-white residuals. Connecting the time-domain signature (autocorrelation) with the frequency-domain signature (power spectrum) will equip you with powerful skills for diagnosing and correcting structural model errors. [@problem_id:2885077]", "problem": "An engineer is validating a time-series model for a scalar discrete-time process. The true data-generating mechanism is an Autoregressive Integrated Moving Average (ARIMA) process of order $(0,1,0)$, defined by the fundamental difference equation $(1 - B) x_t = w_t$, where $B$ is the backshift operator and $w_t$ is a zero-mean white-noise process with variance $\\sigma^{2}$. Unaware of the true differencing order, the engineer fits an ARIMA$(0,2,0)$ model without a deterministic constant and computes residuals $\\hat{e}_t$ equal to the twice-differenced series $\\hat{e}_t = (1 - B)^{2} x_t$.\n\nStarting from the fundamental definitions of the differencing operator, white noise, linear time-invariant filtering, and the Wiener–Khinchin relation between autocovariance and power spectral density, derive the expected residual process implied by this over-differenced specification. In particular:\n- Express $\\hat{e}_t$ in terms of $w_t$ and identify the corresponding linear filter.\n- Derive the autocovariance function $\\gamma_{\\hat{e}}(k)$ and the power spectral density $S_{\\hat{e}}(\\omega)$ of the residuals $\\hat{e}_t$ in closed form, in terms of $\\sigma^{2}$.\n- Explain the expected residual pattern in both the time and frequency domains, and propose rigorous diagnostics that would detect over-differencing, including how the residual autocorrelation at lag $1$ and the behavior of the spectrum near zero frequency would be expected to appear under over-differencing.\n\nProvide as your final answer the closed-form expression for the residual power spectral density $S_{\\hat{e}}(\\omega)$ as a function of $\\omega \\in [-\\pi,\\pi]$ and $\\sigma^{2}$. No rounding is required, and you should not include any units in your final answer.", "solution": "First, we express the residual process $\\hat{e}_t$ in terms of the underlying white-noise process $w_t$.\nThe definition of the residual is given as:\n$$\n\\hat{e}_t = (1 - B)^{2} x_t\n$$\nWe can rewrite this expression by factoring the operator:\n$$\n\\hat{e}_t = (1 - B) \\left[ (1 - B) x_t \\right]\n$$\nThe true data-generating process is given by the equation $(1 - B) x_t = w_t$. Substituting this into the expression for $\\hat{e}_t$, we obtain:\n$$\n\\hat{e}_t = (1 - B) w_t\n$$\nExpanding this expression gives the explicit form of the residual process:\n$$\n\\hat{e}_t = w_t - w_{t-1}\n$$\nThis demonstrates that the residual process $\\hat{e}_t$ is not white noise. Instead, it is a Moving Average process of order $1$, or MA($1$). The linear time-invariant filter that transforms the input white noise $w_t$ into the output residual $\\hat{e}_t$ is represented by the polynomial in the backshift operator $H(B) = 1 - B$.\n\nSecond, we derive the autocovariance function $\\gamma_{\\hat{e}}(k)$ for the residual process $\\hat{e}_t$. The autocovariance at lag $k$ is defined as $\\gamma_{\\hat{e}}(k) = E[\\hat{e}_t \\hat{e}_{t-k}]$. We use the expression $\\hat{e}_t = w_t - w_{t-1}$.\n\nFor lag $k=0$, we compute the variance:\n$$\n\\gamma_{\\hat{e}}(0) = E[\\hat{e}_t^2] = E[(w_t - w_{t-1})^2] = E[w_t^2 - 2w_t w_{t-1} + w_{t-1}^2]\n$$\nBy linearity of expectation and the properties of white noise ($E[w_t^2] = \\sigma^2$ and $E[w_t w_s] = 0$ for $t \\neq s$):\n$$\n\\gamma_{\\hat{e}}(0) = E[w_t^2] - 2E[w_t w_{t-1}] + E[w_{t-1}^2] = \\sigma^2 - 2(0) + \\sigma^2 = 2\\sigma^2\n$$\n\nFor lag $k=1$:\n$$\n\\gamma_{\\hat{e}}(1) = E[\\hat{e}_t \\hat{e}_{t-1}] = E[(w_t - w_{t-1})(w_{t-1} - w_{t-2})] = E[w_t w_{t-1} - w_t w_{t-2} - w_{t-1}^2 + w_{t-1} w_{t-2}]\n$$\nAgain, applying the properties of white noise:\n$$\n\\gamma_{\\hat{e}}(1) = E[w_t w_{t-1}] - E[w_t w_{t-2}] - E[w_{t-1}^2] + E[w_{t-1} w_{t-2}] = 0 - 0 - \\sigma^2 + 0 = -\\sigma^2\n$$\nDue to the symmetry of the autocovariance function, $\\gamma_{\\hat{e}}(-1) = \\gamma_{\\hat{e}}(1) = -\\sigma^2$.\n\nFor lags $|k| \\geq 2$:\n$$\n\\gamma_{\\hat{e}}(k) = E[\\hat{e}_t \\hat{e}_{t-k}] = E[(w_t - w_{t-1})(w_{t-k} - w_{t-k-1})]\n$$\nFor $|k| \\geq 2$, the indices of the white-noise terms in the first factor ($t$, $t-1$) are distinct from the indices in the second factor ($t-k$, $t-k-1$). Therefore, the expectation of any cross-product is zero.\n$$\n\\gamma_{\\hat{e}}(k) = 0 \\quad \\text{for } |k| \\geq 2\n$$\nIn summary, the autocovariance function of the residual process is:\n$$\n\\gamma_{\\hat{e}}(k) =\n\\begin{cases}\n2\\sigma^2 & k=0 \\\\\n-\\sigma^2 & |k|=1 \\\\\n0 & |k| \\ge 2\n\\end{cases}\n$$\n\nThird, we derive the power spectral density (PSD) $S_{\\hat{e}}(\\omega)$ of the residuals. We use the property that for a process $y_t = H(B)w_t$, where $w_t$ is white noise with PSD $S_w(\\omega)=\\sigma^2$, the PSD of $y_t$ is $S_y(\\omega) = |H(e^{-i\\omega})|^2 S_w(\\omega)$.\nHere, $\\hat{e}_t = (1-B)w_t$, so the filter is $H(B) = 1 - B$. The frequency response is found by substituting $B = e^{-i\\omega}$:\n$$\nH(e^{-i\\omega}) = 1 - e^{-i\\omega}\n$$\nThe squared magnitude of the frequency response is:\n$$\n|H(e^{-i\\omega})|^2 = |1 - e^{-i\\omega}|^2 = (1 - \\cos(\\omega))^2 + (\\sin(\\omega))^2 = 1 - 2\\cos(\\omega) + \\cos^2(\\omega) + \\sin^2(\\omega)\n$$\nUsing the identity $\\cos^2(\\omega) + \\sin^2(\\omega) = 1$:\n$$\n|H(e^{-i\\omega})|^2 = 1 - 2\\cos(\\omega) + 1 = 2 - 2\\cos(\\omega) = 2(1 - \\cos(\\omega))\n$$\nThe PSD of the input white noise $w_t$ is constant, $S_w(\\omega) = \\sigma^2$. Therefore, the PSD of the residual process $\\hat{e}_t$ is:\n$$\nS_{\\hat{e}}(\\omega) = |H(e^{-i\\omega})|^2 S_w(\\omega) = 2\\sigma^2(1 - \\cos(\\omega))\n$$\nThis expression is valid for the frequency interval $\\omega \\in [-\\pi, \\pi]$.\n\nFinally, we explain the diagnostic implications. A correctly specified time-series model should produce residuals that are indistinguishable from white noise.\n- **Time-Domain Diagnostics**: The autocovariance function of white noise is zero for all non-zero lags. The derived autocovariance for $\\hat{e}_t$ is non-zero at lag $k=1$. The theoretical autocorrelation function (ACF) of the residuals is:\n$$\n\\rho_{\\hat{e}}(k) = \\frac{\\gamma_{\\hat{e}}(k)}{\\gamma_{\\hat{e}}(0)}\n$$\nSpecifically, for lag $1$:\n$$\n\\rho_{\\hat{e}}(1) = \\frac{-\\sigma^2}{2\\sigma^2} = -0.5\n$$\nFor a model diagnostic, the engineer would inspect the sample ACF of the residuals $\\hat{e}_t$. In this case of over-differencing, the ACF plot will exhibit a single, large, statistically significant negative spike at lag $1$, with a value near $-0.5$. The ACF for all lags greater than $1$ will be insignificant. This is the characteristic signature of superfluous differencing.\n\n- **Frequency-Domain Diagnostics**: The power spectral density of white noise is constant (flat) for all frequencies, $S(\\omega) = \\sigma^2$. The derived PSD for $\\hat{e}_t$ is $S_{\\hat{e}}(\\omega) = 2\\sigma^2(1 - \\cos(\\omega))$. This function is not flat. Crucially, let us evaluate the spectrum at zero frequency:\n$$\nS_{\\hat{e}}(0) = 2\\sigma^2(1 - \\cos(0)) = 2\\sigma^2(1 - 1) = 0\n$$\nThe differencing operator $(1-B)$ acts as a high-pass filter, attenuating low frequencies. Applying it twice results in excessive attenuation of power near zero frequency, creating a \"spectral hole\" or dip at $\\omega=0$. For model diagnostics, an engineer would compute the periodogram or a smoothed spectral estimate of the residuals. A spectrum that shows a significant dip toward zero at the origin is a clear indication of over-differencing.", "answer": "$$\n\\boxed{2\\sigma^{2}(1 - \\cos(\\omega))}\n$$", "id": "2885077"}, {"introduction": "Beyond being uncorrelated, a model's residuals are often assumed to be normally distributed, an assumption that underpins statistical inference procedures like the construction of prediction intervals. This exercise introduces the Jarque-Bera test, an elegant method for checking normality based on the moments of skewness and kurtosis. You will compute the test statistic and use its asymptotic distribution to make a formal decision, solidifying your understanding of moment-based hypothesis testing in model diagnostics. [@problem_id:2885047]", "problem": "You have identified a linear time-invariant (LTI) discrete-time system using a parametric model. Let the one-step-ahead prediction residuals be the sequence $\\{\\varepsilon_{t}\\}_{t=1}^{n}$, computed from a validation dataset of size $n$, and suppose the standard assumptions for residual analysis hold, namely that under a well-specified model with Gaussian innovations the residuals should be independent and identically distributed with a normal distribution. You aim to validate the normality of $\\{\\varepsilon_{t}\\}$ via a moment-based omnibus test rooted in the asymptotic joint behavior of the sample skewness and kurtosis under the normality hypothesis.\n\nFor the given residuals, the observed sample size is $n=432$, the observed sample skewness is $\\hat{\\gamma}=\\frac{1}{4}$, and the observed sample kurtosis (not excess) is $\\hat{\\kappa}=\\frac{19}{6}$. Use fundamental definitions of sample moments and the large-sample behavior of these statistics under the normality hypothesis to construct the appropriate test statistic for normality based on the joint contribution of skewness and kurtosis, determine its asymptotic reference distribution, compute its value for the given data, and make a decision at significance level $\\alpha=0.05$.\n\nReport the decision using the encoding $d=1$ if the null hypothesis of normal residuals is rejected and $d=0$ otherwise. Express your final answer as a single row matrix $\\begin{pmatrix}\\text{JB} & d\\end{pmatrix}$, where $\\text{JB}$ is the computed test statistic and $d$ is the encoded decision. Do not round; express the result exactly, without units.", "solution": "The problem requires us to perform a normality test on a sequence of model residuals, $\\{\\varepsilon_{t}\\}_{t=1}^{n}$, using a joint test on the sample skewness and kurtosis. This is known as the Jarque-Bera test. The null hypothesis, $H_0$, is that the residuals are drawn from a normal distribution. For a normal distribution, the population skewness, $\\gamma$, is $0$ and the population kurtosis, $\\kappa$, is $3$.\n\nThe Jarque-Bera test statistic, denoted here as $JB$, is constructed based on the large-sample properties of the sample skewness, $\\hat{\\gamma}$, and the sample excess kurtosis, $\\hat{\\kappa}-3$. Under the null hypothesis of normality, for a large sample size $n$, the sample statistics have the following asymptotic distributions:\n$$ \\sqrt{n} \\hat{\\gamma} \\xrightarrow{d} N(0, 6) $$\n$$ \\sqrt{n} (\\hat{\\kappa}-3) \\xrightarrow{d} N(0, 24) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. Furthermore, these two statistics are asymptotically independent.\n\nFrom these properties, we can construct two standardized variables that asymptotically follow the standard normal distribution, $N(0,1)$:\n$$ Z_{\\gamma} = \\frac{\\hat{\\gamma}}{\\sqrt{6/n}} = \\sqrt{\\frac{n}{6}}\\hat{\\gamma} $$\n$$ Z_{\\kappa} = \\frac{\\hat{\\kappa}-3}{\\sqrt{24/n}} = \\sqrt{\\frac{n}{24}}(\\hat{\\kappa}-3) $$\nThe Jarque-Bera statistic is the sum of the squares of these two independent standard normal variables. The sum of squares of $k$ independent standard normal variables follows a chi-squared distribution with $k$ degrees of freedom, $\\chi^2(k)$. Therefore, the $JB$ statistic is given by:\n$$ JB = Z_{\\gamma}^2 + Z_{\\kappa}^2 = \\frac{n}{6}\\hat{\\gamma}^2 + \\frac{n}{24}(\\hat{\\kappa}-3)^2 $$\nUnder $H_0$, this statistic asymptotically follows a chi-squared distribution with $2$ degrees of freedom, $JB \\sim \\chi^2(2)$.\n\nThe problem provides the following data:\n- Sample size: $n=432$\n- Sample skewness: $\\hat{\\gamma}=\\frac{1}{4}$\n- Sample kurtosis (not excess kurtosis): $\\hat{\\kappa}=\\frac{19}{6}$\n\nWe first compute the value of the $JB$ statistic.\nThe first term, related to skewness, is:\n$$ \\frac{n}{6}\\hat{\\gamma}^2 = \\frac{432}{6} \\left(\\frac{1}{4}\\right)^2 = 72 \\times \\frac{1}{16} = \\frac{72}{16} = \\frac{9 \\times 8}{2 \\times 8} = \\frac{9}{2} $$\nThe second term, related to kurtosis, requires the sample excess kurtosis:\n$$ \\hat{\\kappa}-3 = \\frac{19}{6} - 3 = \\frac{19}{6} - \\frac{18}{6} = \\frac{1}{6} $$\nNow, we compute the kurtosis term for the $JB$ statistic:\n$$ \\frac{n}{24}(\\hat{\\kappa}-3)^2 = \\frac{432}{24} \\left(\\frac{1}{6}\\right)^2 = 18 \\times \\frac{1}{36} = \\frac{18}{36} = \\frac{1}{2} $$\nSumming the two terms gives the value of the test statistic:\n$$ JB = \\frac{9}{2} + \\frac{1}{2} = \\frac{10}{2} = 5 $$\nThe computed value of the test statistic is $JB=5$.\n\nNext, we must make a decision at the significance level $\\alpha=0.05$. We reject the null hypothesis $H_0$ if the computed $JB$ value is greater than the critical value from the $\\chi^2(2)$ distribution, denoted $\\chi^2_{2, \\alpha}$. The critical value $c$ is defined by $P(X > c) = \\alpha$, where $X \\sim \\chi^2(2)$.\nThe cumulative distribution function (CDF) for a $\\chi^2(2)$ distribution is $F(x) = 1 - \\exp(-x/2)$ for $x \\ge 0$. The critical value $c$ satisfies $F(c) = 1 - \\alpha$.\n$$ 1 - \\exp(-c/2) = 1 - 0.05 = 0.95 $$\n$$ \\exp(-c/2) = 0.05 = \\frac{1}{20} $$\nTaking the natural logarithm of both sides:\n$$ -\\frac{c}{2} = \\ln\\left(\\frac{1}{20}\\right) = -\\ln(20) $$\n$$ c = 2\\ln(20) $$\nThe rejection rule is to reject $H_0$ if $JB > 2\\ln(20)$. We must compare our test statistic $JB=5$ with the critical value $c=2\\ln(20)$. This is equivalent to comparing $\\frac{5}{2}$ with $\\ln(20)$.\nWe know that the base of the natural logarithm, $e$, satisfies $2 < e < 3$. Let us establish a bound for $e^{5/2}$:\n$$ e^{5/2} = e^2 \\cdot e^{1/2} = e^2 \\sqrt{e} $$\nSince $e < 3$, we have $e^2 < 3^2=9$ and $\\sqrt{e} < \\sqrt{3} < 2$.\nTherefore, $e^{5/2} < 9 \\times 2 = 18$.\nSince $e^{5/2} < 18$ and $18 < 20$, we have $e^{5/2} < 20$.\nThe natural logarithm function $\\ln(x)$ is strictly increasing. Applying it to the inequality gives:\n$$ \\ln(e^{5/2}) < \\ln(20) $$\n$$ \\frac{5}{2} < \\ln(20) $$\nMultiplying by $2$:\n$$ 5 < 2\\ln(20) $$\nThis shows that our test statistic $JB=5$ is less than the critical value $c=2\\ln(20)$.\nTherefore, we fail to reject the null hypothesis $H_0$ at the $\\alpha=0.05$ significance level. The data do not provide sufficient evidence to conclude that the residuals are non-normal.\nAccording to the problem's encoding, the decision is $d=0$.\n\nThe final answer is a row matrix containing the computed test statistic $JB$ and the decision code $d$.\n$$ \\begin{pmatrix} JB & d \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\end{pmatrix} $$", "answer": "$$ \\boxed{\\begin{pmatrix} 5 & 0 \\end{pmatrix}} $$", "id": "2885047"}]}