{"hands_on_practices": [{"introduction": "The journey into Prediction Error Methods (PEM) begins with the fundamental task of translating a system model into a quantifiable cost function. This first practice grounds you in this core skill by focusing on the intuitive Finite Impulse Response (FIR) model, a special case of the ARX structure. By deriving the one-step-ahead predictor and the corresponding PEM cost from first principles, you will build a solid foundation for tackling more complex models [@problem_id:2892835].", "problem": "Consider a single-input single-output finite-impulse-response model with order $m \\in \\mathbb{N}$ given by $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$, where $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ and $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$ are the input and output sequences, and $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ is a zero-mean, white sequence with finite variance, independent of the input. Assume that a batch of $N \\in \\mathbb{N}$ samples $\\{(u_{t},y_{t})\\}_{t=1}^{N}$ is available. The unknown parameter vector is $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$. Let $\\mathcal{F}_{t-1}$ denote the $\\sigma$-algebra generated by the past input-output data up to time $t-1$. Starting from the definition of the one-step-ahead predictor as the conditional expectation of $y_{t}$ given $\\mathcal{F}_{t-1}$ under the stated model and assumptions, derive an explicit expression for the predictor $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$ for all time indices $t$ such that all regressors are available from the observed data. Then, using the definition of the Prediction Error Method (PEM) cost as the sample average of squared one-step-ahead prediction errors for white innovations, write the explicit finite-sample PEM cost $V_{N}(\\theta)$ to be minimized over $b_{1},\\dots,b_{m}$, clearly specifying the index range used to avoid unknown pre-sample quantities. Express your final answer as explicit formulas for $\\hat{y}_{t}(\\theta)$ and $V_{N}(\\theta)$. No numerical evaluation is required, and no rounding is needed.", "solution": "The problem requires the derivation of the one-step-ahead predictor and the associated Prediction Error Method (PEM) cost function for a specified Finite Impulse Response (FIR) model. A rigorous validation of the problem statement is a mandatory prerequisite.\n\nFirst, we must extract the given information verbatim.\nThe model is given by $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$, where the order is $m \\in \\mathbb{N}$.\nThe parameter vector is $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$.\nThe signals involved are the input sequence $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ and the output sequence $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$.\nThe noise sequence $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ is stipulated to be a zero-mean, white sequence with finite variance, which is also independent of the input sequence.\nThe available data consists of a batch of $N \\in \\mathbb{N}$ samples, denoted as $\\{(u_{t},y_{t})\\}_{t=1}^{N}$.\nThe information set $\\mathcal{F}_{t-1}$ is defined as the $\\sigma$-algebra generated by the past input-output data up to time $t-1$.\nThe one-step-ahead predictor is defined as $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$.\nThe PEM cost function, $V_{N}(\\theta)$, is defined as the sample average of squared one-step-ahead prediction errors.\n\nThe problem statement is scrutinized for validity. It is scientifically grounded, situated firmly within the standard theory of system identification. The model and assumptions are canonical. The problem is well-posed, providing all necessary definitions and data to arrive at a unique, meaningful solution. The requirement to specify the index range to handle the finite data set is an integral part of the problem, not an indicator of incompleteness. The problem is a fundamental exercise in applying first principles of prediction theory and statistical signal processing. Therefore, the problem is deemed valid and a solution will be constructed.\n\nWe proceed with the derivation of the one-step-ahead predictor, $\\hat{y}_{t}(\\theta)$. By its definition:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]\n$$\nSubstituting the given model for $y_{t}$:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right]\n$$\nUsing the linearity of conditional expectation, this expression is split into two terms:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] + \\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]\n$$\nLet us analyze the first term. The parameters $b_{k}$ are deterministic components of the vector $\\theta$. The inputs $u_{t-k}$ for $k \\in \\{1, 2, \\dots, m\\}$ are, by definition, events that occurred at or before time $t-1$. Thus, they are measurable with respect to the $\\sigma$-algebra $\\mathcal{F}_{t-1}$. Consequently, the entire sum $\\sum_{k=1}^{m} b_{k}\\,u_{t-k}$ is an $\\mathcal{F}_{t-1}$-measurable quantity. The conditional expectation of an $\\mathcal{F}_{t-1}$-measurable variable given $\\mathcal{F}_{t-1}$ is the variable itself.\n$$\n\\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nNow, consider the second term, $\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]$. The noise $e_{t}$ is assumed to be a white sequence, meaning it is uncorrelated with its past values $\\{e_{s}\\}_{s<t}$. It is also independent of the entire input sequence $\\{u_{s}\\}_{s \\in \\mathbb{Z}}$. The information set $\\mathcal{F}_{t-1}$ is generated by past inputs $\\{u_{s}\\}_{s \\le t-1}$ and past outputs $\\{y_{s}\\}_{s \\le t-1}$. Since each past output $y_{s}$ is a function of past inputs and the past noise term $e_{s}$, the $\\sigma$-algebra $\\mathcal{F}_{t-1}$ contains information only about $\\{u_{s}\\}_{s \\le t-1}$ and $\\{e_{s}\\}_{s \\le t-1}$. Because $e_{t}$ is independent of both of these sets of random variables, it is independent of the $\\sigma$-algebra $\\mathcal{F}_{t-1}$. The conditional expectation of a random variable given an independent $\\sigma$-algebra is its unconditional expectation.\n$$\n\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,] = \\mathbb{E}[\\,e_{t}\\,]\n$$\nAs stated in the problem, $\\{e_{t}\\}$ is a zero-mean sequence, so $\\mathbb{E}[\\,e_{t}\\,] = 0$.\nCombining these results gives the final expression for the predictor:\n$$\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nNext, we derive the PEM cost function $V_{N}(\\theta)$. The one-step-ahead prediction error is defined as $\\varepsilon_{t}(\\theta) = y_{t} - \\hat{y}_{t}(\\theta)$. Using our derived predictor, we have:\n$$\n\\varepsilon_{t}(\\theta) = y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nThe problem defines the cost function as the sample average of the squared prediction errors. The summation must be performed over the time indices $t$ for which all quantities ($y_{t}$ and the regressors $u_{t-1}, \\dots, u_{t-m}$) are available within the given data set $\\{(u_{t},y_{t})\\}_{t=1}^{N}$. The regressor with the lowest time index is $u_{t-m}$. For this to be within our data record, we must have $t-m \\ge 1$, which implies $t \\ge m+1$. The latest time for which we have an output $y_{t}$ is $t=N$. Thus, the summation must range from $t=m+1$ to $t=N$. This is valid only if $N \\ge m+1$. The number of terms in this sum is $N - (m+1) + 1 = N-m$.\nThe cost function $V_N(\\theta)$ is therefore:\n$$\nV_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( \\varepsilon_{t}(\\theta) \\right)^2 = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\right)^2\n$$\nThis completes the derivation. The results provide the explicit formulas requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}u_{t-k} & V_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}u_{t-k} \\right)^{2}\n\\end{pmatrix}\n}\n$$", "id": "2892835"}, {"introduction": "While simple models are instructive, real-world systems often feature disturbances that are correlated in time, a scenario not captured by white noise. This exercise advances your skills to the more powerful and general ARMAX model, where the noise itself has dynamics. You will learn to employ the Diophantine equation, a key algebraic tool, to derive the optimal predictor and implement it recursively to compute prediction errors, bridging advanced theory with practical computation [@problem_id:2892827].", "problem": "You are given a single-input single-output AutoRegressive Moving Average with eXogenous input (ARMAX) model written in the backward-shift operator as\n$$\nA(q,\\theta)\\,y_t \\;=\\; B(q,\\theta)\\,u_t \\;+\\; C(q,\\theta)\\,e_t,\n$$\nwhere $q^{-1}$ denotes the unit-delay operator, $A(q,\\theta)$ and $C(q,\\theta)$ are monic polynomials in $q^{-1}$ with all zeros strictly inside the unit disk, $B(q,\\theta)$ is a finite polynomial in $q^{-1}$, and $\\{e_t\\}$ is a zero-mean white-noise sequence of finite variance that is independent of $\\{u_t\\}$. Assume $A(q,\\theta)$ and $C(q,\\theta)$ are coprime. The one-step-ahead predictor $\\hat{y}_{t|t-1}(\\theta)$ is defined as the conditional expectation $\\mathbb{E}\\!\\left[y_t \\mid \\mathcal{F}_{t-1}\\right]$, where $\\mathcal{F}_{t-1}$ is the $\\sigma$-algebra generated by $\\{y_{t-k},u_{t-k}\\}_{k\\ge 1}$.\n\nTasks:\n1) Starting from the above model and the definition of conditional expectation, use a polynomial Diophantine equation to derive a strictly causal one-step-ahead predictor $\\hat{y}_{t|t-1}(\\theta)$ that depends only on past measured data and previously computed prediction errors. In particular, find polynomials $\\Gamma(q,\\theta)$ and $\\Delta(q,\\theta)$ in $q^{-1}$ satisfying a suitable Diophantine relation between $A(q,\\theta)$ and $C(q,\\theta)$, and use it to express $\\hat{y}_{t|t-1}(\\theta)$ in terms of $u_t$, $y_t$, and past innovations. Clearly state the degree conditions you impose to ensure causality and uniqueness.\n2) From your predictor, show how to compute the prediction error $\\epsilon_t(\\theta) \\equiv y_t - \\hat{y}_{t|t-1}(\\theta)$ recursively using only past $\\epsilon$-values together with current and past inputs and outputs; explicitly write this recursion in terms of the coefficients of $A(q,\\theta)$, $B(q,\\theta)$, and $C(q,\\theta)$, assuming they are parameterized as $A(q,\\theta)=1+\\sum_{k=1}^{n_a} a_k q^{-k}$, $B(q,\\theta)=\\sum_{k=0}^{n_b} b_k q^{-k}$, and $C(q,\\theta)=1+\\sum_{k=1}^{n_c} c_k q^{-k}$.\n3) Consider the specific parameter vector $\\theta$ and short data record given by\n- Model: $A(q,\\theta)=1-1.2\\,q^{-1}+0.32\\,q^{-2}$, $B(q,\\theta)=0.5+0.1\\,q^{-1}$, $C(q,\\theta)=1+0.5\\,q^{-1}$.\n- Inputs: $u_{-1}=0$, $u_0=1.0$, $u_1=-0.5$, $u_2=0.0$, $u_3=0.25$.\n- Outputs: $y_{-2}=0$, $y_{-1}=0$, $y_0=0.8$, $y_1=-0.4$, $y_2=0.5$, $y_3=0.1$.\nAssume initial rest for the predictor recursion, i.e., $\\epsilon_t(\\theta)=0$ for all $t<0$. Using your recursion from part 2), compute the numerical value of $\\epsilon_3(\\theta)$. Round your answer to four significant figures.", "solution": "This problem demands a rigorous derivation of the one-step-ahead predictor for an ARMAX model, followed by a numerical calculation. The problem is well-posed and scientifically sound. I will address each part in sequence. For clarity, the dependence of polynomials on the parameter vector $\\theta$ will be suppressed in the notation, e.g., $A(q)$ instead of $A(q,\\theta)$, unless ambiguity arises.\n\n**Part 1: Derivation of the One-Step-Ahead Predictor**\n\nThe ARMAX model is given by the stochastic difference equation:\n$$\nA(q) y_t = B(q) u_t + C(q) e_t\n$$\nwhere $A(q)$ and $C(q)$ are monic polynomials in the backward-shift operator $q^{-1}$ with zeros inside the unit disk, and $\\{e_t\\}$ is a zero-mean white noise process. We seek the one-step-ahead predictor $\\hat{y}_{t|t-1}$, defined as the conditional expectation $\\mathbb{E}[y_t | \\mathcal{F}_{t-1}]$, where $\\mathcal{F}_{t-1}$ is the $\\sigma$-algebra generated by past inputs and outputs $\\{y_{t-k}, u_{t-k}\\}_{k \\ge 1}$. We assume the input sequence $\\{u_t\\}$ is known, which is standard practice.\n\nFirst, we express $y_t$ in terms of the driving signals $u_t$ and $e_t$. Since $A(q)$ has its zeros inside the unit disk, its inverse $A(q)^{-1}$ corresponds to a stable and causal filter.\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{C(q)}{A(q)} e_t\n$$\nTo separate the predictable part of $y_t$ from the unpredictable innovation $e_t$, we decompose the rational function $C(q)/A(q)$ using a polynomial Diophantine equation. For a one-step-ahead predictor, this equation is:\n$$\nC(q) = A(q) \\Gamma(q) + q^{-1} \\Delta(q)\n$$\nHere, $\\Gamma(q)$ and $\\Delta(q)$ are polynomials in $q^{-1}$. To ensure a unique solution, we must impose degree constraints. For a one-step prediction horizon ($d=1$), we require the degree of $\\Gamma(q)$ to be $\\deg(\\Gamma) = 1-1=0$. Since both $A(q)$ and $C(q)$ are monic (having a leading coefficient of $1$), it is necessary that $\\Gamma(q) = 1$. This implies the Diophantine equation simplifies to:\n$$\nC(q) = A(q) \\cdot 1 + q^{-1} \\Delta(q)\n$$\nFrom this, we solve for $\\Delta(q)$:\n$$\n\\Delta(q) = q(C(q) - A(q))\n$$\nSince $A(q)$ and $C(q)$ are monic, the polynomial $C(q)-A(q)$ has a constant term of $1-1=0$, meaning it is of the form $(c_1-a_1)q^{-1} + (c_2-a_2)q^{-2} + \\dots$. Consequently, $\\Delta(q) = q(C(q)-A(q))$ is a finite polynomial in $q^{-1}$. The degree of $\\Delta(q)$ is $\\max(\\deg A, \\deg C) - 1$. This confirms the causality and uniqueness of our decomposition.\n\nNow, we substitute the Diophantine identity back into the expression for $y_t$:\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{A(q)\\Gamma(q) + q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nWith $\\Gamma(q)=1$, this becomes:\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + e_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nThe term $e_t$ is the innovation at time $t$. By definition, it is unpredictable given the past information $\\mathcal{F}_{t-1}$, so $\\mathbb{E}[e_t | \\mathcal{F}_{t-1}] = 0$. The other terms are functions of past values of $e_t$ (i.e., $\\{e_{t-k}\\}_{k \\ge 1}$) and current and past values of $u_t$. Since the sequence $\\{u_t\\}$ is assumed known, and past innovations $\\{e_{t-k}\\}_{k \\ge 1}$ are measurable with respect to $\\mathcal{F}_{t-1}$ (as they can be constructed from past inputs and outputs, assuming invertibility of $C(q)$), these terms are predictable.\n\nTherefore, taking the conditional expectation gives the predictor:\n$$\n\\hat{y}_{t|t-1} = \\mathbb{E}[y_t | \\mathcal{F}_{t-1}] = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nThis expression depends on the unmeasurable noise sequence $\\{e_t\\}$. To obtain a practical form, we must express $e_t$ in terms of measurable quantities. From the original ARMAX model, $C(q)e_t = A(q)y_t - B(q)u_t$. Since $C(q)$ is stable and invertible, we have $e_t = C(q)^{-1}(A(q)y_t - B(q)u_t)$. Substituting this into the predictor equation yields:\n$$\n\\hat{y}_{t|t-1} = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} \\left( \\frac{A(q)}{C(q)}y_t - \\frac{B(q)}{C(q)}u_t \\right) = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} u_t\n$$\nCombining the terms involving $u_t$:\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)}{A(q)} - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} \\right) u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t\n$$\nSubstituting $q^{-1}\\Delta(q) = C(q) - A(q)$:\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)C(q) - (C(q)-A(q))B(q)}{A(q)C(q)} \\right) u_t + \\frac{C(q)-A(q)}{C(q)} y_t\n$$\nThe term in the parenthesis simplifies: $B(q)C(q) - C(q)B(q) + A(q)B(q) = A(q)B(q)$.\n$$\n\\hat{y}_{t|t-1} = \\frac{A(q)B(q)}{A(q)C(q)} u_t + \\frac{C(q)-A(q)}{C(q)} y_t = \\frac{B(q)}{C(q)} u_t + \\left(1 - \\frac{A(q)}{C(q)}\\right) y_t\n$$\nThis is the final expression for the one-step-ahead predictor. It can be implemented recursively as:\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\nThis form depends only on past outputs, current and past inputs, and past predictor values, making it strictly causal in its use of measured outputs.\n\n**Part 2: Recursive Formula for the Prediction Error**\n\nThe prediction error is defined as $\\epsilon_t(\\theta) = y_t - \\hat{y}_{t|t-1}(\\theta)$. From the predictor derived in Part 1, we have:\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\nSubstitute $\\hat{y}_{t|t-1} = y_t - \\epsilon_t$:\n$$\nC(q)(y_t - \\epsilon_t) = B(q)u_t + C(q)y_t - A(q)y_t\n$$\nExpanding the left side:\n$$\nC(q)y_t - C(q)\\epsilon_t = B(q)u_t + C(q)y_t - A(q)y_t\n$$\nThe term $C(q)y_t$ cancels from both sides, leaving:\n$$\n-C(q)\\epsilon_t = B(q)u_t - A(q)y_t\n$$\nor\n$$\nC(q)\\epsilon_t = A(q)y_t - B(q)u_t\n$$\nThis demonstrates that the prediction error $\\epsilon_t$ can be obtained by filtering the input $u_t$ and output $y_t$. Note that if parameters $\\theta$ are the true ones, $\\epsilon_t(\\theta)$ becomes the true innovation $e_t$, and this equation is simply a rearrangement of the original ARMAX model.\n\nTo obtain an explicit recursive formula, we write out the polynomials:\n$A(q) = 1+\\sum_{k=1}^{n_a} a_k q^{-k}$, $B(q) = \\sum_{k=0}^{n_b} b_k q^{-k}$, and $C(q) = 1+\\sum_{k=1}^{n_c} c_k q^{-k}$.\nThe equation $C(q)\\epsilon_t = A(q)y_t - B(q)u_t$ becomes:\n$$\n\\left(1+\\sum_{k=1}^{n_c} c_k q^{-k}\\right)\\epsilon_t = \\left(1+\\sum_{k=1}^{n_a} a_k q^{-k}\\right)y_t - \\left(\\sum_{k=0}^{n_b} b_k q^{-k}\\right)u_t\n$$\nApplying the shift operator:\n$$\n\\epsilon_t + \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k} = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k}\n$$\nIsolating $\\epsilon_t$ yields the desired recursion:\n$$\n\\epsilon_t = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k} - \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k}\n$$\nThis formula allows for the computation of $\\epsilon_t$ using the current output $y_t$, current and past inputs $u_t, u_{t-1}, \\dots$, past outputs $y_{t-1}, \\dots$, and past prediction errors $\\epsilon_{t-1}, \\dots$.\n\n**Part 3: Numerical Computation**\n\nWe are given the specific model parameters:\n$A(q) = 1-1.2\\,q^{-1}+0.32\\,q^{-2} \\implies n_a=2, a_1 = -1.2, a_2 = 0.32$\n$B(q) = 0.5+0.1\\,q^{-1} \\implies n_b=1, b_0 = 0.5, b_1 = 0.1$\n$C(q) = 1+0.5\\,q^{-1} \\implies n_c=1, c_1 = 0.5$\n\nThe recursion for $\\epsilon_t$ is:\n$$\n\\epsilon_t = y_t + a_1 y_{t-1} + a_2 y_{t-2} - (b_0 u_t + b_1 u_{t-1}) - c_1 \\epsilon_{t-1}\n$$\n$$\n\\epsilon_t = y_t - 1.2 y_{t-1} + 0.32 y_{t-2} - (0.5 u_t + 0.1 u_{t-1}) - 0.5 \\epsilon_{t-1}\n$$\nWe are provided with data and the initial condition $\\epsilon_t=0$ for $t<0$. We must compute $\\epsilon_3$. This requires computing $\\epsilon_0, \\epsilon_1, \\epsilon_2$ sequentially.\n\nFor $t=0$:\n$\\epsilon_0 = y_0 - 1.2 y_{-1} + 0.32 y_{-2} - (0.5 u_0 + 0.1 u_{-1}) - 0.5 \\epsilon_{-1}$\n$\\epsilon_0 = 0.8 - 1.2(0) + 0.32(0) - (0.5(1.0) + 0.1(0)) - 0.5(0)$\n$\\epsilon_0 = 0.8 - 0.5 = 0.3$\n\nFor $t=1$:\n$\\epsilon_1 = y_1 - 1.2 y_{0} + 0.32 y_{-1} - (0.5 u_1 + 0.1 u_{0}) - 0.5 \\epsilon_{0}$\n$\\epsilon_1 = -0.4 - 1.2(0.8) + 0.32(0) - (0.5(-0.5) + 0.1(1.0)) - 0.5(0.3)$\n$\\epsilon_1 = -0.4 - 0.96 - (-0.25 + 0.1) - 0.15$\n$\\epsilon_1 = -1.36 - (-0.15) - 0.15 = -1.36$\n\nFor $t=2$:\n$\\epsilon_2 = y_2 - 1.2 y_{1} + 0.32 y_{0} - (0.5 u_2 + 0.1 u_{1}) - 0.5 \\epsilon_{1}$\n$\\epsilon_2 = 0.5 - 1.2(-0.4) + 0.32(0.8) - (0.5(0.0) + 0.1(-0.5)) - 0.5(-1.36)$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 - (-0.05) + 0.68$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 + 0.05 + 0.68 = 1.966$\n\nFinally, for $t=3$:\n$\\epsilon_3 = y_3 - 1.2 y_{2} + 0.32 y_{1} - (0.5 u_3 + 0.1 u_{2}) - 0.5 \\epsilon_{2}$\n$\\epsilon_3 = 0.1 - 1.2(0.5) + 0.32(-0.4) - (0.5(0.25) + 0.1(0.0)) - 0.5(1.966)$\n$\\epsilon_3 = 0.1 - 0.6 - 0.128 - (0.125) - 0.983$\n$\\epsilon_3 = -0.5 - 0.128 - 0.125 - 0.983$\n$\\epsilon_3 = -0.628 - 0.125 - 0.983$\n$\\epsilon_3 = -0.753 - 0.983 = -1.736$\n\nThe computed value is $\\epsilon_3 = -1.736$. This result has four significant figures as requested.", "answer": "$$\n\\boxed{-1.736}\n$$", "id": "2892827"}, {"introduction": "A crucial aspect of system identification is understanding the consequences of your modeling assumptions. What happens if you choose a simpler model structure, like ARX, when the true system's noise is colored, as in an ARMAX process? This practice challenges you to think critically about model mismatch, exploring the pivotal concepts of asymptotic bias and consistency in PEM estimates, and revealing why a careful choice of the noise model is essential for accurate results [@problem_id:2892852].", "problem": "An unknown discrete-time, single-input single-output, linear time-invariant plant is driven by a measurable input $u(t)$ and a stochastic disturbance $e(t)$. The data $\\{y(t),u(t)\\}$ are generated by a true AutoRegressive Moving-Average with eXogenous input (ARMAX) system\n$$\nA^0(q)\\,y(t) \\;=\\; B^0(q)\\,u(t) \\;+\\; C^0(q)\\,e(t),\n$$\nwhere $q^{-1}$ is the delay operator, $A^0(q)$, $B^0(q)$, $C^0(q)$ are stable, coprime polynomials with $A^0(0)=1$, $C^0(0)=1$, and $e(t)$ is zero-mean, independent and identically distributed white noise with variance $\\sigma_e^2$, independent of $u(t)$. An engineer estimates an AutoRegressive with eXogenous input (ARX) model using the Prediction Error Method (PEM):\n$$\nA(q)\\,y(t) \\;=\\; B(q)\\,u(t) \\;+\\; \\epsilon(t,\\theta),\n$$\nwith parameter vector $\\theta$ collecting the coefficients of $A(q)$ and $B(q)$, and with the PEM criterion\n$$\nV_N(\\theta) \\;=\\; \\frac{1}{N}\\sum_{t=1}^{N} \\epsilon(t,\\theta)^2,\n$$\nwhere $\\epsilon(t,\\theta)=y(t)-\\hat{y}(t\\!\\mid\\!t-1,\\theta)$ is the one-step-ahead prediction error implied by the ARX model. Assume that the chosen ARX orders satisfy $n_a \\ge n_a^0$, $n_b \\ge n_b^0$, and that any pure input delay $n_k$ is correctly known. Also assume that $A(q)$ is constrained to be stable and $N\\to\\infty$.\n\nWhich of the following statements correctly identifies the conditions under which the ARX PEM estimates of the plant dynamics are asymptotically unbiased and consistent for the true ARMAX system when $C^0(q)=1$, and correctly explains the bias that arises otherwise when $C^0(q)\\neq 1$?\n\nA. If $C^0(q)=1$, $u(t)$ is persistently exciting of order at least $n_a+n_b$, and $e(t)$ is zero-mean white noise independent of $u(t)$ with finite variance, then the ARX model class contains the true system, the PEM residual at the true parameters equals the innovation $e(t)$, and the orthogonality conditions $E\\{\\varphi(t)\\,e(t)\\}=0$ hold with $\\varphi(t)$ the ARX regressor; hence the PEM minimizer is unique and equals the true parameters (asymptotically unbiased and consistent). If instead $C^0(q)\\neq 1$, the ARX noise model is misspecified, the residual becomes colored and correlated with the regressors (because past $y(t)$ contain filtered noise), violating $E\\{\\varphi(t)\\,\\epsilon(t,\\theta^0)\\}=0$; the PEM solution converges to a pseudo-true parameter that partially absorbs $C^0(q)$ into $A(q)$ and $B(q)$, yielding biased plant estimates.\n\nB. Even if $C^0(q)\\neq 1$, as long as $u(t)$ is persistently exciting and $N\\to\\infty$, ARX PEM estimates of $A^0(q)$ and $B^0(q)$ are asymptotically unbiased; only the noise variance estimate is biased, because PEM decouples plant and noise estimation.\n\nC. If $C^0(q)=1$, ARX PEM is asymptotically unbiased even when $u(t)$ is constant (not persistently exciting), because PEM only requires white noise and stability for consistency.\n\nD. If $C^0(q)=1$, ARX PEM is asymptotically unbiased only when $e(t)$ is colored and $C^0(q)$ is minimum phase; if $e(t)$ is white, ARX PEM is biased because the innovation cannot be predicted.\n\nE. Regardless of $C^0(q)$, choosing sufficiently large ARX orders $n_a,n_b$ and letting them grow with $N$ guarantees asymptotically unbiased ARX PEM plant estimates, because high-order ARX can represent any ARMAX model without bias in the plant dynamics.", "solution": "The problem statement must first be validated for correctness and solvability.\n\nStep 1: Extract Givens\n-   True system is a single-input single-output (SISO) AutoRegressive Moving-Average with eXogenous input (ARMAX) model:\n    $$\n    A^0(q)\\,y(t) \\;=\\; B^0(q)\\,u(t) \\;+\\; C^0(q)\\,e(t)\n    $$\n-   $q^{-1}$ is the delay operator.\n-   $A^0(q)$, $B^0(q)$, $C^0(q)$ are stable, coprime polynomials in $q^{-1}$.\n-   The polynomials are monic: $A^0(0)=1$ and $C^0(0)=1$.\n-   $e(t)$ is a zero-mean, independent and identically distributed (i.i.d.) white noise process with variance $\\sigma_e^2$.\n-   The noise $e(t)$ is independent of the input $u(t)$.\n-   The estimated model is an AutoRegressive with eXogenous input (ARX) model:\n    $$\n    A(q)\\,y(t) \\;=\\; B(q)\\,u(t) \\;+\\; \\epsilon(t,\\theta)\n    $$\n-   $\\theta$ is the parameter vector containing the coefficients of the polynomials $A(q)$ and $B(q)$.\n-   The estimation method is the Prediction Error Method (PEM), which minimizes the criterion:\n    $$\n    V_N(\\theta) \\;=\\; \\frac{1}{N}\\sum_{t=1}^{N} \\epsilon(t,\\theta)^2\n    $$\n-   $\\epsilon(t,\\theta)$ is the one-step-ahead prediction error, $\\epsilon(t,\\theta)=y(t)-\\hat{y}(t\\!\\mid\\!t-1,\\theta)$.\n-   The ARX model orders are sufficient: $n_a \\ge n_a^0$ and $n_b \\ge n_b^0$.\n-   The input delay $n_k$ is known and correctly incorporated.\n-   The estimated polynomial $A(q)$ is constrained to be stable.\n-   The analysis is for the asymptotic case, $N\\to\\infty$.\n-   The question is to identify the conditions for asymptotic unbiasedness and consistency when $C^0(q)=1$, and the source of bias when $C^0(q)\\neq 1$.\n\nStep 2: Validate Using Extracted Givens\nThe problem statement is a standard formulation from the field of system identification.\n-   **Scientifically Grounded:** The concepts of ARMAX and ARX models, PEM, white noise, persistence of excitation, asymptotic unbiasedness, and consistency are fundamental and well-established principles in control theory and signal processing. The problem is scientifically sound.\n-   **Well-Posed:** The problem is clearly defined. It provides all necessary assumptions (model structures, properties of noise and input, estimation criterion, asymptotic analysis) to derive a unique and meaningful conclusion regarding the properties of the estimator.\n-   **Objective:** The language is formal and devoid of subjectivity or ambiguity.\n\nStep 3: Verdict and Action\nThe problem is valid. I will proceed with a full derivation and analysis.\n\nThe core of the Prediction Error Method is to find the parameter vector $\\theta$ that minimizes the variance of the one-step-ahead prediction error. In the asymptotic limit as the number of data points $N \\to \\infty$, the PEM estimate $\\hat{\\theta}_N$ converges to the parameter vector $\\theta^*$ that minimizes the expected squared prediction error:\n$$\n\\theta^* = \\arg \\min_\\theta E\\left\\{\\epsilon(t,\\theta)^2\\right\\}\n$$\nThe necessary condition for this minimum is that the gradient of the criterion with respect to $\\theta$ is zero at $\\theta^*$. This leads to the orthogonality condition:\n$$\nE\\left\\{ \\left. \\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta} \\right|_{\\theta=\\theta^*} \\cdot \\epsilon(t,\\theta^*) \\right\\} = 0\n$$\nFor the ARX model structure, the one-step-ahead predictor is given by:\n$$\n\\hat{y}(t|t-1,\\theta) = [1-A(q)]y(t) + B(q)u(t)\n$$\nThe prediction error is therefore:\n$$\n\\epsilon(t,\\theta) = y(t) - \\hat{y}(t|t-1,\\theta) = A(q)y(t) - B(q)u(t)\n$$\nThis can be written in the linear regression form $\\epsilon(t,\\theta) = y(t) - \\varphi(t)^T \\theta$, where the regressor vector $\\varphi(t)$ consists of past inputs and outputs:\n$$\n\\varphi(t) = [-y(t-1), \\dots, -y(t-n_a), u(t-n_k), \\dots, u(t-n_k-n_b+1)]^T\n$$\nThe gradient of the prediction error is $\\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta} = -\\varphi(t)$. The orthogonality condition thus simplifies to:\n$$\nE\\{\\varphi(t)\\,\\epsilon(t,\\theta^*)\\} = 0\n$$\nThe PEM estimate converges to a value $\\theta^*$ such that the corresponding prediction error $\\epsilon(t,\\theta^*)$ is orthogonal to the regressor vector $\\varphi(t)$. We now analyze two distinct cases.\n\nCase 1: True system has $C^0(q)=1$.\nThe true system is an ARX process:\n$$\nA^0(q)y(t) = B^0(q)u(t) + e(t)\n$$\nSince the model orders are sufficient ($n_a \\ge n_a^0, n_b \\ge n_b^0$), the true parameter vector $\\theta^0$ (corresponding to the coefficients of $A^0(q)$ and $B^0(q)$, padded with zeros if necessary) lies within the set of parameters being searched. Let us evaluate the prediction error at this true parameter vector, $\\theta = \\theta^0$:\n$$\n\\epsilon(t,\\theta^0) = A^0(q)y(t) - B^0(q)u(t)\n$$\nSubstituting the true system dynamics into this expression, we find that the prediction error at the true parameters is exactly the system's innovation:\n$$\n\\epsilon(t,\\theta^0) = e(t)\n$$\nNow, we must check if the orthogonality condition $E\\{\\varphi(t)\\,\\epsilon(t,\\theta^0)\\} = 0$ is satisfied. This becomes $E\\{\\varphi(t)\\,e(t)\\} = 0$. The regressor $\\varphi(t)$ contains terms $y(t-k)$ and $u(t-j)$ for $k \\ge 1$ and $j \\ge n_k \\ge 1$. The output $y(t-k)$ is a function of past inputs $u(\\tau)$ and past noise values $e(\\tau)$ for $\\tau \\le t-k$. Since $e(t)$ is i.i.d. white noise, it is by definition uncorrelated with all its past values $e(\\tau)$ for $\\tau < t$. Furthermore, it is given that $e(t)$ is independent of the input sequence $u(t)$. Consequently, $e(t)$ is uncorrelated with every component of $\\varphi(t)$. The orthogonality condition $E\\{\\varphi(t)\\,e(t)\\} = 0$ is therefore satisfied at the true parameter vector $\\theta^0$.\n\nIf the input $u(t)$ is persistently exciting of a sufficiently high order (specifically, $n_a+n_b$), the covariance matrix of the regressor, $E\\{\\varphi(t)\\varphi(t)^T\\}$, is positive definite. This ensures that the quadratic cost function $E\\{\\epsilon(t,\\theta)^2\\}$ has a unique minimum. Since the orthogonality condition is met at $\\theta^0$, and the minimum is unique, the PEM estimate must converge to the true value: $\\hat{\\theta}_N \\to \\theta^0$ as $N \\to \\infty$. Thus, the ARX PEM estimate is asymptotically unbiased and consistent.\n\nCase 2: True system has $C^0(q) \\neq 1$.\nThe true system is an ARMAX process:\n$$\nA^0(q)y(t) = B^0(q)u(t) + C^0(q)e(t)\n$$\nThe ARX model structure, $A(q)y(t) = B(q)u(t) + \\epsilon(t,\\theta)$, is now a misspecification of the true noise dynamics. Let us again evaluate the prediction error at the parameters corresponding to the true plant dynamics, which we denote $\\theta_p^0$ (with $A=A^0, B=B^0$).\n$$\n\\epsilon(t,\\theta_p^0) = A^0(q)y(t) - B^0(q)u(t)\n$$\nSubstituting from the true ARMAX dynamics, we find the residual is no longer white noise:\n$$\n\\epsilon(t,\\theta_p^0) = C^0(q)e(t)\n$$\nLet this residual be $v(t) = C^0(q)e(t)$. Since $C^0(q) = 1 + c_1^0 q^{-1} + \\dots$ and we assume $C^0(q) \\neq 1$, $v(t)$ is a moving average process. It is colored noise, meaning its values are correlated in time.\n\nNow, we check the orthogonality condition $E\\{\\varphi(t)\\,\\epsilon(t,\\theta_p^0)\\} = E\\{\\varphi(t)\\,v(t)\\} = 0$. The regressor $\\varphi(t)$ includes past outputs, such as $y(t-1)$. The output $y(t)$ is given by:\n$$\ny(t) = \\frac{B^0(q)}{A^0(q)}u(t) + \\frac{C^0(q)}{A^0(q)}e(t)\n$$\nThus, $y(t-1)$ depends on the entire history of the noise process $e(\\tau)$ up to time $t-1$. The residual $v(t) = e(t) + c_1^0 e(t-1) + \\dots$ also depends on past noise. There will be a non-zero correlation between $y(t-1)$ and $v(t)$, for instance through the common term $e(t-1)$ (as $y(t-1)$ depends on $e(t-1)$ and $v(t)$ contains $c_1^0e(t-1)$). In general, $E\\{y(t-k)v(t)\\} \\neq 0$ for $k \\ge 1$.\nBecause at least one component of the regressor $\\varphi(t)$ is correlated with the residual $v(t)$, the orthogonality condition is violated at the true plant parameters:\n$$\nE\\{\\varphi(t)\\,\\epsilon(t,\\theta_p^0)\\} \\neq 0\n$$\nSince the condition is not met at $\\theta_p^0$, the minimizer $\\theta^*$ of the cost function will be different from $\\theta_p^0$. The PEM algorithm will converge to a biased parameter vector $\\theta^* \\neq \\theta_p^0$. The estimated polynomials $A(q)$ and $B(q)$ will be distorted to compensate for the misspecified noise model, effectively absorbing some of the dynamics of $C^0(q)$. This results in biased estimates of the plant dynamics.\n\nNow, we evaluate the provided options against this analysis.\n\nA. If $C^0(q)=1$, the statement claims that with a persistently exciting input, the ARX model class contains the true system, the residual at true parameters equals the innovation $e(t)$, and the orthogonality condition $E\\{\\varphi(t)\\,e(t)\\}=0$ holds, leading to unique, asymptotically unbiased, and consistent estimates. This aligns perfectly with our analysis for Case 1. If $C^0(q)\\neq 1$, it states the ARX noise model is misspecified, the residual is colored and correlated with the regressors, violating the orthogonality condition, and the solution converges to a pseudo-true parameter that absorbs noise dynamics, yielding biased plant estimates. This aligns perfectly with our analysis for Case 2. This statement is a complete and correct description. **Correct**.\n\nB. This statement claims that even if $C^0(q)\\neq 1$, the estimates of $A^0(q)$ and $B^0(q)$ are asymptotically unbiased. This is false, as demonstrated in Case 2. The correlation between the regressors and the colored noise induces bias in all parameters, not just the noise model. The claim that PEM decouples plant and noise estimation is incorrect for ARX models. **Incorrect**.\n\nC. This statement claims that if $C^0(q)=1$, ARX PEM is asymptotically unbiased even without persistence of excitation ($u(t)$ is constant). This is false. Persistence of excitation is a necessary condition to ensure the regressor covariance matrix $E\\{\\varphi(t)\\varphi(t)^T\\}$ is invertible, which guarantees a unique solution. Without it, parameters are not identifiable. **Incorrect**.\n\nD. This statement claims unbiasedness occurs for colored noise ($e(t)$ colored) and requires $C^0(q)$ to be minimum phase, while white noise leads to bias. This is the exact opposite of the correct conditions for an ARX model. ARX PEM is unbiased if the system noise is white, corresponding to $C^0(q)=1$. The line \"if $e(t)$ is white, ARX PEM is biased because the innovation cannot be predicted\" is nonsensical. The innovation is by definition unpredictable, and a correct model's prediction error should be the innovation. **Incorrect**.\n\nE. This statement proposes an alternative estimation scheme where model orders $n_a, n_b$ grow with $N$. While it is true that such non-parametric methods can provide a consistent estimate of the system's *transfer function* $G^0(q)=B^0(q)/A^0(q)$, it is not true that the estimates of the *polynomials* $A(q)$ and $B(q)$ are asymptotically unbiased for $A^0(q)$ and $B^0(q)$. The high-order polynomial $A(q)$ will approximate a function related to $A^0(q)/C^0(q)$, not $A^0(q)$ itself. Furthermore, this option changes the fundamental premise of the problem from a fixed-order parametric estimation to a non-parametric one. Within the context of the fixed-order problem as stated, this statement is misleading and not the correct answer. Option A provides the precise answer for the specified problem setup. **Incorrect**.\n\nBased on a rigorous analysis, statement A is the only one that correctly and completely describes the asymptotic properties of the ARX PEM estimator under the given conditions.", "answer": "$$\\boxed{A}$$", "id": "2892852"}]}