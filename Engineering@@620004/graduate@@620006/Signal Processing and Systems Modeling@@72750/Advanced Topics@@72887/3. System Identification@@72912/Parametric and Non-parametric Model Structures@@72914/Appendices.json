{"hands_on_practices": [{"introduction": "The first step in many system identification tasks is to estimate the parameters of a chosen model structure from experimental data. This exercise provides hands-on practice with one of the most fundamental parametric estimation techniques: the method of least squares. You will apply this method to an AutoRegressive with eXogenous input (ARX) model, a widely used structure in control and signal processing, by setting up and solving the associated normal equations for a given noise-free dataset [@problem_id:2889301]. This practice is essential for building a concrete understanding of how abstract model parameters are determined from concrete measurements.", "problem": "Consider the linear time-invariant AutoRegressive with eXogenous input (ARX) model specified in the prediction-error form\n$$\ny(k) \\;=\\; -a_{1}\\,y(k-1)\\;-\\;a_{2}\\,y(k-2)\\;+\\;b_{1}\\,u(k-1)\\;+\\;b_{2}\\,u(k-2)\\;+\\;e(k),\n$$\nwhere $y(k)$ is the system output, $u(k)$ is a known input, $e(k)$ is a zero-mean disturbance, and the unknown parameter vector is $\\theta \\equiv \\begin{pmatrix} a_{1} & a_{2} & b_{1} & b_{2} \\end{pmatrix}^{\\top}$. You are given measured input-output data and initial conditions\n$$\nu(-1)=0,\\quad u(0)=0,\\quad y(-1)=0,\\quad y(0)=0,\n$$\nand the following finite sequences:\n$$\n\\begin{aligned}\n&u(1)=1,\\; u(2)=2,\\; u(3)=0,\\; u(4)=-1,\\; u(5)=1,\\; u(6)=0,\\\\\n&y(1)=0,\\; y(2)=1,\\; y(3)=2.5,\\; y(4)=1.05,\\; y(5)=-0.975,\\; y(6)=0.3025,\\; y(7)=0.34625.\n\\end{aligned}\n$$\nAssume that for this dataset the disturbance is identically zero, i.e., $e(k)=0$ for all $k$ used below. Formulate the batch least-squares estimation problem that minimizes the sum of squared one-step-ahead prediction errors over the data indices $k=2,3,4,5,6,7$ and construct the corresponding linear normal equations in terms of the data above. Then compute the least-squares estimate of the parameter $b_{1}$.\n\nExpress your final reported value for the scalar $b_{1}$ as an exact number. No rounding is required. The final answer must be a single real number.", "solution": "We begin from the model definition and the least-squares principle. The one-step-ahead prediction error at time $k$ is\n$$\n\\varepsilon(k;\\theta)\\;=\\;y(k)\\;-\\;\\big(-a_{1}\\,y(k-1)-a_{2}\\,y(k-2)+b_{1}\\,u(k-1)+b_{2}\\,u(k-2)\\big).\n$$\nDefine the stacked regression vector\n$$\n\\varphi(k)\\;\\equiv\\;\\begin{pmatrix}-y(k-1)\\\\ -y(k-2)\\\\ u(k-1)\\\\ u(k-2)\\end{pmatrix},\\qquad \\theta\\;\\equiv\\;\\begin{pmatrix}a_{1}\\\\ a_{2}\\\\ b_{1}\\\\ b_{2}\\end{pmatrix},\n$$\nso that the error is $\\varepsilon(k;\\theta)=y(k)-\\varphi(k)^{\\top}\\theta$. The batch Least Squares (LS) criterion over the index set $\\mathcal{K}=\\{2,3,4,5,6,7\\}$ is\n$$\nJ(\\theta)\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varepsilon(k;\\theta)^{2}\\;=\\;\\sum_{k\\in\\mathcal{K}}\\big(y(k)-\\varphi(k)^{\\top}\\theta\\big)^{2}.\n$$\nMinimizing $J(\\theta)$ with respect to $\\theta$ by setting its gradient to zero yields the normal equations\n$$\n\\left(\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,\\varphi(k)^{\\top}\\right)\\theta\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,y(k).\n$$\nEquivalently, if we define the data matrix $\\Phi\\in\\mathbb{R}^{6\\times 4}$ with rows $\\varphi(k)^{\\top}$ for $k=2,\\dots,7$ and the data vector $Y\\in\\mathbb{R}^{6}$ with entries $y(k)$ for $k=2,\\dots,7$, we have the compact form\n$$\n\\Phi^{\\top}\\Phi\\,\\theta\\;=\\;\\Phi^{\\top}Y.\n$$\n\nWe now build $\\Phi$ and $Y$ explicitly from the provided data. For each $k\\in\\{2,3,4,5,6,7\\}$ we compute $\\varphi(k)$:\n$$\n\\begin{aligned}\n&k=2:\\;\\; \\varphi(2)^{\\top}=\\begin{pmatrix}-y(1)&-y(0)&u(1)&u(0)\\end{pmatrix}=\\begin{pmatrix}0&0&1&0\\end{pmatrix},\\;\\; y(2)=1,\\\\\n&k=3:\\;\\; \\varphi(3)^{\\top}=\\begin{pmatrix}-y(2)&-y(1)&u(2)&u(1)\\end{pmatrix}=\\begin{pmatrix}-1&0&2&1\\end{pmatrix},\\;\\; y(3)=2.5,\\\\\n&k=4:\\;\\; \\varphi(4)^{\\top}=\\begin{pmatrix}-y(3)&-y(2)&u(3)&u(2)\\end{pmatrix}=\\begin{pmatrix}-2.5&-1&0&2\\end{pmatrix},\\;\\; y(4)=1.05,\\\\\n&k=5:\\;\\; \\varphi(5)^{\\top}=\\begin{pmatrix}-y(4)&-y(3)&u(4)&u(3)\\end{pmatrix}=\\begin{pmatrix}-1.05&-2.5&-1&0\\end{pmatrix},\\;\\; y(5)=-0.975,\\\\\n&k=6:\\;\\; \\varphi(6)^{\\top}=\\begin{pmatrix}-y(5)&-y(4)&u(5)&u(4)\\end{pmatrix}=\\begin{pmatrix}0.975&-1.05&1&-1\\end{pmatrix},\\;\\; y(6)=0.3025,\\\\\n&k=7:\\;\\; \\varphi(7)^{\\top}=\\begin{pmatrix}-y(6)&-y(5)&u(6)&u(5)\\end{pmatrix}=\\begin{pmatrix}-0.3025&0.975&0&1\\end{pmatrix},\\;\\; y(7)=0.34625.\n\\end{aligned}\n$$\nThus\n$$\n\\Phi=\\begin{pmatrix}\n0 & 0 & 1 & 0\\\\\n-1 & 0 & 2 & 1\\\\\n-2.5 & -1 & 0 & 2\\\\\n-1.05 & -2.5 & -1 & 0\\\\\n0.975 & -1.05 & 1 & -1\\\\\n-0.3025 & 0.975 & 0 & 1\n\\end{pmatrix},\\qquad\nY=\\begin{pmatrix}\n1\\\\\n2.5\\\\\n1.05\\\\\n-0.975\\\\\n0.3025\\\\\n0.34625\n\\end{pmatrix}.\n$$\nThe normal equations take the explicit form\n$$\n\\underbrace{\\Phi^{\\top}\\Phi}_{G}\\,\\theta\\;=\\;\\underbrace{\\Phi^{\\top}Y}_{g},\n$$\nwith\n$$\nG=\\begin{pmatrix}\n9.39463125 & 3.8063125 & 0.025 & -7.2775\\\\\n3.8063125 & 9.303125 & 1.45 & 0.025\\\\\n0.025 & 1.45 & 7 & 1\\\\\n-7.2775 & 0.025 & 1 & 7\n\\end{pmatrix},\\qquad\ng=\\begin{pmatrix}\n-3.911053125\\\\\n1.40746875\\\\\n7.2775\\\\\n4.64375\n\\end{pmatrix}.\n$$\nThis completes the construction of the normal equations.\n\nTo compute the least-squares estimate, we note that the LS solution satisfies $\\Phi\\,\\hat{\\theta}=Y$ exactly if the data are noise-free and $\\Phi$ has full column rank. From the first regression row (corresponding to $k=2$),\n$$\ny(2)\\;=\\;-a_{1}\\,y(1)\\;-\\;a_{2}\\,y(0)\\;+\\;b_{1}\\,u(1)\\;+\\;b_{2}\\,u(0)\\;=\\;0\\;+\\;0\\;+\\;b_{1}\\cdot 1\\;+\\;0,\n$$\nwhich implies immediately\n$$\nb_{1}\\;=\\;y(2)\\;=\\;1.\n$$\nFor completeness, we can verify that there exist $(a_{1},a_{2},b_{2})$ making all residuals zero with this $b_{1}$. Using three subsequent equations (for $k=3,4,5$) with $b_{1}=1$,\n$$\n\\begin{aligned}\n&k=3:\\;\\;2.5=-a_{1}\\cdot 1-a_{2}\\cdot 0+1\\cdot 2+b_{2}\\cdot 1\\;\\;\\Rightarrow\\;\\;-a_{1}+b_{2}=0.5,\\\\\n&k=4:\\;\\;1.05=-a_{1}\\cdot 2.5-a_{2}\\cdot 1+1\\cdot 0+b_{2}\\cdot 2\\;\\;\\Rightarrow\\;\\;-2.5a_{1}-a_{2}+2b_{2}=1.05,\\\\\n&k=5:\\;\\;-0.975=-a_{1}\\cdot 1.05-a_{2}\\cdot 2.5+1\\cdot(-1)+b_{2}\\cdot 0\\;\\;\\Rightarrow\\;\\;-1.05a_{1}-2.5a_{2}=-0.975+1=0.025,\n\\end{aligned}\n$$\nwhich solve to $a_{1}=-0.5$, $a_{2}=0.2$, and $b_{2}=0$. With these values, all six equations are satisfied exactly, and thus the residual vector is identically zero. Consequently the LS estimator coincides with this solution, and in particular the least-squares estimate of $b_{1}$ is\n$$\n\\hat{b}_{1}\\;=\\;1.\n$$\nThis value also satisfies the third component of the normal equations, since substituting $\\theta=\\begin{pmatrix}-0.5&0.2&1&0\\end{pmatrix}^{\\top}$ gives\n$$\n\\begin{pmatrix}0.025 & 1.45 & 7 & 1\\end{pmatrix}\\theta\\;=\\;0.025(-0.5)+1.45(0.2)+7(1)+1(0)\\;=\\;-0.0125+0.29+7\\;=\\;7.2775\\;=\\;g_{3}.\n$$\nTherefore, the least-squares estimate of $b_{1}$ for the given dataset is exactly $1$.", "answer": "$$\\boxed{1}$$", "id": "2889301"}, {"introduction": "In contrast to parametric methods that assume a specific underlying model, non-parametric techniques estimate spectral properties directly from data. This practice explores Welch's method, a robust non-parametric approach for Power Spectral Density (PSD) estimation, focusing on the crucial concept of frequency resolution. By deriving the resolution from the Equivalent Noise Bandwidth (ENBW) of the chosen window function, you will gain a hands-on appreciation for the fundamental trade-off between resolution and statistical variance that is governed by the analyst's choice of segment length [@problem_id:2889322].", "problem": "A real, wide-sense stationary, zero-mean discrete-time process $x[n]$ is sampled at a sampling frequency $f_{s} = 48\\,\\text{kHz}$. You are to estimate its power spectral density (PSD) using Welch’s method, which is a non-parametric spectral estimator, and contrast its notion of frequency resolution with that of parametric approaches, without computing any parametric estimate. The total record length is $N_{\\text{tot}} = 1{,}228{,}800$ samples. You choose the following Welch parameters: segment length $L = 4096$ samples, $50\\%$ overlap between adjacent segments, and the periodic Hann window $w[n]$ defined for $n=0,1,\\dots,L-1$ as\n$$\nw[n] = \\tfrac{1}{2}\\Big(1 - \\cos\\!\\big(\\tfrac{2\\pi n}{L}\\big)\\Big).\n$$\nAssume the Discrete Fourier Transform (DFT) length is $K=L$ for each segment (no zero-padding).\n\nIn this setting, define the frequency resolution of the Welch PSD estimate as the Equivalent Noise Bandwidth (ENBW) of the spectral window, expressed in hertz. Using only fundamental definitions of Welch’s method and ENBW, and properties of discrete-time sums of cosine sequences, compute the resulting frequency resolution in hertz. Provide your final answer as an exact value with no rounding, and express the result in $\\text{Hz}$ (do not include units in the final boxed answer).", "solution": "The frequency resolution for a non-parametric spectral estimator like Welch's method is determined by the spectral characteristics of the window function applied to each data segment. The problem defines this resolution as the Equivalent Noise Bandwidth (ENBW) of the window, expressed in hertz.\n\nThe ENBW of a discrete-time window $w[n]$ of length $L$ is defined in normalized frequency (cycles/sample) as the ratio of the sum of the squared window coefficients to the square of the sum of the window coefficients:\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{\\sum_{n=0}^{L-1} w^2[n]}{\\left(\\sum_{n=0}^{L-1} w[n]\\right)^2}\n$$\nTo convert this normalized bandwidth to a physical frequency in hertz, one must multiply by the sampling frequency, $f_s$:\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\text{ENBW}_{\\text{norm}} \\times f_s\n$$\nThe task reduces to calculating the two sums for the given periodic Hann window, $w[n] = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)$, for $n=0, 1, \\dots, L-1$.\n\nFirst, we compute the sum of the window coefficients, which we denote $S_1$:\n$$\nS_1 = \\sum_{n=0}^{L-1} w[n] = \\sum_{n=0}^{L-1} \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\nBy linearity of summation:\n$$\nS_1 = \\frac{1}{2} \\left[ \\left(\\sum_{n=0}^{L-1} 1\\right) - \\left(\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]\n$$\nThe first term is $\\sum_{n=0}^{L-1} 1 = L$. The second term is the sum of a cosine over one full cycle ($L$ samples). This sum is zero for $L > 1$. This can be shown by considering the sum of the complex exponential $\\sum_{n=0}^{L-1} \\exp\\left(j\\frac{2\\pi k n}{L}\\right)$, which equals $0$ for any integer $k$ not a multiple of $L$. Here, $k=1$. Taking the real part confirms $\\sum_{n=0}^{L-1} \\cos(\\frac{2\\pi n}{L}) = 0$. Given $L = 4096$, this condition holds.\nThus,\n$$\nS_1 = \\frac{1}{2} (L - 0) = \\frac{L}{2}\n$$\n\nNext, we compute the sum of the squared window coefficients, which we denote $S_2$:\n$$\nS_2 = \\sum_{n=0}^{L-1} w^2[n] = \\sum_{n=0}^{L-1} \\left[ \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]^2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)^2\n$$\nExpanding the square:\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\cos^2\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\nWe use the power-reduction identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\\right)\n$$\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(\\frac{3}{2} - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\n$$\nDistributing the summation:\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3}{2}\\sum_{n=0}^{L-1} 1 - 2\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\sum_{n=0}^{L-1} \\cos\\left(\\frac{4\\pi n}{L}\\right) \\right]\n$$\nAs established, $\\sum \\cos(\\frac{2\\pi n}{L}) = 0$. Similarly, $\\sum \\cos(\\frac{4\\pi n}{L}) = 0$ for $L>2$, which is true for $L=4096$.\nThis leaves:\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3L}{2} - 2(0) + \\frac{1}{2}(0) \\right] = \\frac{3L}{8}\n$$\n\nNow, we can compute the normalized ENBW:\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{S_2}{S_1^2} = \\frac{\\frac{3L}{8}}{\\left(\\frac{L}{2}\\right)^2} = \\frac{\\frac{3L}{8}}{\\frac{L^2}{4}} = \\frac{3L \\cdot 4}{8 \\cdot L^2} = \\frac{12L}{8L^2} = \\frac{3}{2L}\n$$\nThis is the well-known ENBW for the Hann window. The frequency resolution depends inversely on the window length $L$.\n\nFinally, we compute the resolution in hertz using the given values $L=4096$ and $f_s = 48000\\,\\text{Hz}$:\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\frac{3}{2L} \\times f_s = \\frac{3}{2 \\times 4096} \\times 48000 = \\frac{3 \\times 48000}{8192} = \\frac{144000}{8192}\n$$\nTo simplify the fraction:\n$$\n\\frac{144000}{8192} = \\frac{144 \\times 1000}{8 \\times 1024} = \\frac{18 \\times 1000}{1024} = \\frac{18000}{1024} = \\frac{9000}{512} = \\frac{4500}{256} = \\frac{2250}{128} = \\frac{1125}{64}\n$$\nThe exact value is $1125/64$, which in decimal form is $17.578125$.\n\nThe problem also requires a contrast with parametric approaches. In non-parametric methods like Welch's, the frequency resolution, here defined as $1.5 \\frac{f_s}{L}$, is fixed by the analyst's choice of the window length $L$. A longer window provides better resolution (narrower main lobe) but at the cost of increased estimator variance for a fixed total data record, as fewer segments can be averaged. This resolution is independent of the signal content. In contrast, parametric methods (e.g., Autoregressive (AR), Moving Average (MA), or ARMA models) postulate that the signal is generated by a linear time-invariant system driven by white noise. The PSD is then a function of the model parameters. The \"resolution\" in this context is not a fixed bandwidth but rather the ability to represent sharp spectral features. If the underlying data conforms to the chosen model structure, parametric methods can achieve superior resolution, resolving closely spaced sinusoids even with very short data records, far exceeding the $1/L$ limitation of non-parametric methods. However, this performance is critically dependent on the correctness of the model assumption; a model mismatch can lead to a spectral estimate that is grossly inaccurate and misleading.", "answer": "$$\\boxed{17.578125}$$", "id": "2889322"}, {"introduction": "Parametric models, while powerful, can be prone to overfitting, yielding poor predictive performance on new data. This advanced practice addresses this challenge by introducing ridge regression, a regularization technique that stabilizes parameter estimates by penalizing their magnitude. You will derive the Generalized Cross-Validation (GCV) criterion from first principles, a powerful method for selecting the optimal regularization parameter $\\lambda$ directly from data, and apply it to a concrete identification problem [@problem_id:2889347]. This exercise demonstrates how to systematically manage the bias-variance trade-off to build more robust and generalizable models.", "problem": "Consider identification of a finite impulse response (FIR) model of order $m$ from input-output data $\\{(u(t),y(t))\\}_{t=1}^{T}$. Let the regression matrix be $\\Phi \\in \\mathbb{R}^{T \\times m}$ whose $t$-th row is $\\varphi(t)^{\\top} \\coloneqq \\big(u(t),u(t-1),\\dots,u(t-m+1)\\big)$, and the parametric model be $y(t) \\approx \\varphi(t)^{\\top}\\theta$ for $\\theta \\in \\mathbb{R}^{m}$. Ridge-regularized identification estimates $\\theta$ by minimizing the penalized least-squares criterion $\\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$ with regularization parameter $\\lambda \\ge 0$, where $\\mathbf{y} \\in \\mathbb{R}^{T}$ stacks $y(t)$.\n\nStarting only from the normal equations and fundamental linear algebraic identities, derive the generalized cross-validation (GCV) criterion for ridge-regularized FIR identification. Your derivation must:\n- Explicitly identify the linear smoother (hat) matrix $S(\\lambda)$ that maps $\\mathbf{y}$ to its fitted values $\\widehat{\\mathbf{y}}$,\n- Express the leave-one-out cross-validation (LOOCV) residuals in terms of the in-sample residuals and diagonal entries of $S(\\lambda)$, and\n- Obtain the GCV functional $V_{\\mathrm{GCV}}(\\lambda)$ by replacing the individual diagonal entries by their average.\n\nThen specialize your expression to the case $m=1$ (a single FIR tap), so that $\\Phi=\\mathbf{u}\\in\\mathbb{R}^{T\\times 1}$ with $\\mathbf{u} \\coloneqq \\big(u(1),\\dots,u(T)\\big)^{\\top}$. Show that $V_{\\mathrm{GCV}}(\\lambda)$ reduces to a one-dimensional function of $\\lambda$ that depends on the singular value of $\\Phi$ and the projections of $\\mathbf{y}$ onto the column space and its orthogonal complement.\n\nFinally, for the concrete dataset with $T=5$, input sequence $\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$ and output sequence $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$, compute the unique value $\\lambda^{\\star}\\ge 0$ that minimizes the derived $V_{\\mathrm{GCV}}(\\lambda)$. Give your final $\\lambda^{\\star}$ as an exact value. No rounding is required. The final answer must be a single real number.", "solution": "The objective is to find the ridge regression parameter estimate $\\widehat{\\theta}$ that minimizes the cost function\n$$J(\\theta) = \\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{T}$ is the output vector, $\\Phi \\in \\mathbb{R}^{T \\times m}$ is the regression matrix, $\\theta \\in \\mathbb{R}^{m}$ is the parameter vector, and $\\lambda \\ge 0$ is the regularization parameter.\n\nFirst, we derive the normal equations. The cost function can be written as $J(\\theta) = (\\mathbf{y}-\\Phi\\theta)^{\\top}(\\mathbf{y}-\\Phi\\theta)+\\lambda\\theta^{\\top}\\theta$. To find the minimum, we compute the gradient with respect to $\\theta$ and set it to zero:\n$$ \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} (\\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\Phi\\theta + \\theta^{\\top}\\Phi^{\\top}\\Phi\\theta + \\lambda\\theta^{\\top}\\theta) = -2\\Phi^{\\top}\\mathbf{y} + 2\\Phi^{\\top}\\Phi\\theta + 2\\lambda\\theta $$\nSetting the gradient to zero, we obtain the normal equations for ridge regression:\n$$ (\\Phi^{\\top}\\Phi + \\lambda I)\\theta = \\Phi^{\\top}\\mathbf{y} $$\nFor any $\\lambda > 0$, the matrix $(\\Phi^{\\top}\\Phi + \\lambda I)$ is positive definite and thus invertible. The solution is unique:\n$$ \\widehat{\\theta}(\\lambda) = (\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\n\nWe now derive the generalized cross-validation (GCV) criterion.\n\n**Step 1: Identify the Smoother (Hat) Matrix**\nThe vector of fitted values $\\widehat{\\mathbf{y}}$ is given by $\\widehat{\\mathbf{y}} = \\Phi\\widehat{\\theta}(\\lambda)$. Substituting the expression for $\\widehat{\\theta}(\\lambda)$:\n$$ \\widehat{\\mathbf{y}} = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\nBy definition, the smoother matrix, or hat matrix, $S(\\lambda)$ maps the observed data $\\mathbf{y}$ to the fitted values $\\widehat{\\mathbf{y}}$, i.e., $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$. Therefore, we identify the hat matrix as:\n$$ S(\\lambda) = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top} $$\nThis matrix is a linear operator that depends on the regularization parameter $\\lambda$.\n\n**Step 2: Express Leave-One-Out Cross-Validation (LOOCV) Residuals**\nThe leave-one-out cross-validation error is $V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}^{(-k)}(k))^2$, where $\\widehat{y}^{(-k)}(k)$ is the prediction for the $k$-th data point using a model trained on all data except the $k$-th point. A fundamental result in linear regression analysis, known as the PRESS (Prediction Error Sum of Squares) identity, relates the LOOCV residual $y(k) - \\widehat{y}^{(-k)}(k)$ to the ordinary in-sample residual $e(k) = y(k) - \\widehat{y}(k)$, where $\\widehat{y}(k)$ is the $k$-th component of $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$. The identity is:\n$$ y(k) - \\widehat{y}^{(-k)}(k) = \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} $$\nwhere $S_{kk}(\\lambda)$ is the $k$-th diagonal element of the hat matrix $S(\\lambda)$. The in-sample residual vector is $\\mathbf{e} = \\mathbf{y} - \\widehat{\\mathbf{y}} = (I-S(\\lambda))\\mathbf{y}$.\n\n**Step 3: Obtain the GCV Functional**\nThe LOOCV criterion is formed by summing the squares of these LOOCV residuals:\n$$ V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} \\right)^2 $$\nThe GCV criterion provides an approximation to LOOCV by replacing the individual diagonal elements $S_{kk}(\\lambda)$ in the denominator with their average value. The average is given by the trace of the hat matrix divided by $T$:\n$$ \\frac{1}{T} \\sum_{k=1}^{T} S_{kk}(\\lambda) = \\frac{1}{T} \\mathrm{tr}(S(\\lambda)) $$\nSubstituting this average into the LOOCV expression yields the GCV functional $V_{\\mathrm{GCV}}(\\lambda)$:\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))} \\right)^2 = \\frac{\\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}(k))^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\nThis is typically written as:\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\|\\mathbf{y} - \\widehat{\\mathbf{y}}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} = \\frac{\\frac{1}{T}\\|(I-S(\\lambda))\\mathbf{y}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\nThis completes the general derivation.\n\n**Specialization to the $m=1$ case**\nFor $m=1$, the regression matrix is a single column vector, $\\Phi = \\mathbf{u} \\in \\mathbb{R}^{T \\times 1}$. The term $\\Phi^{\\top}\\Phi$ becomes a scalar: $\\mathbf{u}^{\\top}\\mathbf{u} = \\|\\mathbf{u}\\|_2^2$. This is the squared singular value of $\\Phi$, which we denote by $\\sigma_1^2 = \\sigma^2 = \\|\\mathbf{u}\\|_2^2$.\n\nThe trace of the hat matrix becomes:\n$$ \\mathrm{tr}(S(\\lambda)) = \\mathrm{tr}(\\mathbf{u}(\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}) = \\mathrm{tr}((\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}\\mathbf{u}) = \\frac{\\mathbf{u}^{\\top}\\mathbf{u}}{\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda} = \\frac{\\sigma^2}{\\sigma^2 + \\lambda} $$\nThe denominator of $V_{\\mathrm{GCV}}(\\lambda)$ is $(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda})^2$.\n\nNext, we analyze the numerator. The hat matrix simplifies to:\n$$ S(\\lambda) = \\mathbf{u}(\\sigma^2+\\lambda)^{-1}\\mathbf{u}^{\\top} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2+\\lambda} $$\nLet $P_{\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2}$ be the projection matrix onto the column space of $\\mathbf{u}$. Then $S(\\lambda) = \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}$.\nThe residual sum of squares term is $\\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2$. We decompose $\\mathbf{y}$ into the sum of its projection onto the column space of $\\mathbf{u}$, $\\mathbf{y}_{\\parallel} = P_{\\mathbf{u}}\\mathbf{y}$, and its projection onto the orthogonal complement, $\\mathbf{y}_{\\perp} = (I-P_{\\mathbf{u}})\\mathbf{y}$.\nThe residual vector is:\n$$ \\mathbf{y} - S(\\lambda)\\mathbf{y} = \\mathbf{y} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}\\mathbf{y} = \\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} = \\left(1-\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} = \\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} $$\nSince $\\mathbf{y}_{\\parallel}$ and $\\mathbf{y}_{\\perp}$ are orthogonal, the squared norm is the sum of their squared norms:\n$$ \\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2 = \\left\\|\\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel}\\right\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 $$\nCombining these results, the GCV functional is:\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\left[ \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 \\right]}{\\left(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)^2} $$\nThis expression clearly depends on the singular value of $\\Phi$ (via $\\sigma^2$) and the projections of $\\mathbf{y}$.\n\n**Computation for the specific dataset**\nGiven $T=5$, $\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$, and $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$, we compute the required quantities.\nThe squared singular value is $\\sigma^2 = \\|\\mathbf{u}\\|_2^2 = 1^2+2^2+(-1)^2+0^2+2^2 = 1+4+1+0+4=10$.\nThe projection of $\\mathbf{y}$ onto $\\mathbf{u}$ requires the inner product $\\mathbf{u}^{\\top}\\mathbf{y}$:\n$$ \\mathbf{u}^{\\top}\\mathbf{y} = (1)(5) + (2)(4) + (-1)(1) + (0)(0) + (2)(4) = 5+8-1+0+8 = 20 $$\nThe squared norm of the parallel component of $\\mathbf{y}$ is:\n$$ \\|\\mathbf{y}_{\\parallel}\\|_2^2 = \\|P_{\\mathbf{u}}\\mathbf{y}\\|_2^2 = \\frac{(\\mathbf{u}^{\\top}\\mathbf{y})^2}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{20^2}{10} = \\frac{400}{10} = 40 $$\nThe squared norm of the original output vector is $\\|\\mathbf{y}\\|_2^2 = 5^2+4^2+1^2+0^2+4^2 = 25+16+1+0+16 = 58$.\nThe squared norm of the orthogonal component is found by the Pythagorean theorem:\n$$ \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\|\\mathbf{y}\\|_2^2 - \\|\\mathbf{y}_{\\parallel}\\|_2^2 = 58 - 40 = 18 $$\nNow, we substitute $T=5$, $\\sigma^2=10$, $\\|\\mathbf{y}_{\\parallel}\\|_2^2=40$, and $\\|\\mathbf{y}_{\\perp}\\|_2^2=18$ into the expression for $V_{\\mathrm{GCV}}(\\lambda)$:\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5}\\left[ \\left(\\frac{\\lambda}{10+\\lambda}\\right)^2(40) + 18 \\right]}{\\left(1 - \\frac{1}{5}\\frac{10}{10+\\lambda}\\right)^2} = \\frac{\\frac{1}{5}\\left[ \\frac{40\\lambda^2}{(10+\\lambda)^2} + 18 \\right]}{\\left(\\frac{5(10+\\lambda)-10}{5(10+\\lambda)}\\right)^2} $$\nSimplifying:\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5} \\frac{40\\lambda^2 + 18(10+\\lambda)^2}{(10+\\lambda)^2}}{\\frac{(40+5\\lambda)^2}{25(10+\\lambda)^2}} = \\frac{5[40\\lambda^2 + 18(100+20\\lambda+\\lambda^2)]}{(40+5\\lambda)^2} $$\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{5[40\\lambda^2 + 1800+360\\lambda+18\\lambda^2]}{25(8+\\lambda)^2} = \\frac{58\\lambda^2+360\\lambda+1800}{5(8+\\lambda)^2} $$\nTo find the value $\\lambda^{\\star} \\ge 0$ that minimizes $V_{\\mathrm{GCV}}(\\lambda)$, we differentiate with respect to $\\lambda$ and set the derivative to zero. We only need to consider the function $f(\\lambda) = \\frac{58\\lambda^2+360\\lambda+1800}{(\\lambda+8)^2}$.\nUsing the quotient rule, $\\frac{d}{d\\lambda}\\left(\\frac{N(\\lambda)}{D(\\lambda)}\\right) = \\frac{N'(\\lambda)D(\\lambda)-N(\\lambda)D'(\\lambda)}{D(\\lambda)^2}$:\n$$ N'(\\lambda) = 116\\lambda + 360 $$\n$$ D(\\lambda) = (\\lambda+8)^2 \\implies D'(\\lambda) = 2(\\lambda+8) $$\nSetting the numerator of the derivative to zero for $\\lambda \\neq -8$:\n$$ (116\\lambda+360)(\\lambda+8) - (58\\lambda^2+360\\lambda+1800)(2) = 0 $$\n$$ 116\\lambda^2 + 928\\lambda + 360\\lambda + 2880 - 116\\lambda^2 - 720\\lambda - 3600 = 0 $$\nThe $\\lambda^2$ terms cancel. We collect the remaining terms:\n$$ (928+360-720)\\lambda + (2880-3600) = 0 $$\n$$ 568\\lambda - 720 = 0 $$\n$$ 568\\lambda = 720 $$\n$$ \\lambda^{\\star} = \\frac{720}{568} $$\nWe simplify the fraction. Both numerator and denominator are divisible by $8$: $720/8 = 90$, $568/8 = 71$.\n$$ \\lambda^{\\star} = \\frac{90}{71} $$\nSince $\\lambda^{\\star} = \\frac{90}{71} > 0$, it is a valid candidate. The second derivative test or analysis of the first derivative's sign confirms this is a minimum. Thus, the unique value minimizing $V_{\\mathrm{GCV}}(\\lambda)$ for $\\lambda \\ge 0$ is $\\frac{90}{71}$.", "answer": "$$\\boxed{\\frac{90}{71}}$$", "id": "2889347"}]}