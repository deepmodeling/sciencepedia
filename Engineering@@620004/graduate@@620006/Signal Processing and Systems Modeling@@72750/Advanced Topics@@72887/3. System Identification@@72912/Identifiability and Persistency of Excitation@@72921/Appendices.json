{"hands_on_practices": [{"introduction": "A common intuition in system identification is that a 'stronger' input signal should yield better results. This exercise challenges that notion by analyzing a simple, high-energy sinusoidal input [@problem_id:2876722]. By studying the asymptotic regressor correlation matrix, you will prove that maximizing input energy alone is insufficient for identifying systems of order $n \\ge 3$, providing a stark demonstration that spectral richness, not sheer power, is the true key to persistency of excitation.", "problem": "Consider a single-input single-output discrete-time finite impulse response system of known order $n \\geq 3$ but unknown coefficients. Let the regressor at time $k$ be the vector $\\varphi_{k} \\in \\mathbb{R}^{n}$ defined by $\\varphi_{k} = \\big[u(k), u(k-1), \\dots, u(k-n+1)\\big]^{\\top}$. A sequence $\\{u(k)\\}$ is said to be persistently exciting (PE) of order $n$ if there exist constants $\\alpha > 0$ and an integer $N_{0} \\geq n$ such that for all integers $t$, the finite-horizon regressor Gramian $G_{t} = \\sum_{k=t}^{t+N_{0}-1} \\varphi_{k} \\varphi_{k}^{\\top}$ satisfies $G_{t} \\succeq \\alpha I_{n}$, where $I_{n}$ is the $n \\times n$ identity matrix. One equivalent way to assess this property is via the asymptotic time-average correlation matrix $R = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\varphi_{k} \\varphi_{k}^{\\top}$, where positive definiteness of $R$ is necessary for persistency of excitation of order $n$.\n\nYou are given an input sequence of the form $u(k) = A \\cos(\\omega k)$ with $A > 0$ and frequency $\\omega \\in (0, \\pi)$ fixed. This class of inputs allows the average input energy per sample, $\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} u(k)^{2}$, to be made arbitrarily large by increasing $A$, thereby meeting any prescribed energy maximization objective that only depends on total or average energy.\n\nUsing only the above definitions and fundamental trigonometric identities, compute the exact value of the smallest eigenvalue $\\lambda_{\\min}$ of the matrix $R$ associated with this input, as a function of $A$, $n$, and $\\omega$. Your final answer must be a single closed-form expression. No rounding is required, and no units are involved. This computation will establish a counterexample demonstrating that maximizing input energy alone does not guarantee persistency of excitation of order $n \\geq 3$ and highlights the necessity of spectral richness in the input.", "solution": "The asymptotic time-average correlation matrix $R$ is defined as\n$$R = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\varphi_{k} \\varphi_{k}^{\\top}$$\nwhere $\\varphi_{k} = \\big[u(k), u(k-1), \\dots, u(k-n+1)\\big]^{\\top}$ is the regressor vector of dimension $n$. The matrix $R$ is an $n \\times n$ matrix whose elements, denoted $R_{ij}$ for $i, j \\in \\{1, 2, \\dots, n\\}$, are given by\n$$R_{ij} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} u(k-i+1) u(k-j+1)$$\nThe input sequence is given by $u(k) = A \\cos(\\omega k)$ with $A > 0$ and $\\omega \\in (0, \\pi)$. Substituting this into the expression for $R_{ij}$:\n$$R_{ij} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\left[ A \\cos(\\omega(k-i+1)) \\right] \\left[ A \\cos(\\omega(k-j+1)) \\right]$$\n$$R_{ij} = A^2 \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\omega k - \\omega(i-1)) \\cos(\\omega k - \\omega(j-1))$$\nWe employ the product-to-sum trigonometric identity $\\cos(X)\\cos(Y) = \\frac{1}{2}[\\cos(X-Y) + \\cos(X+Y)]$.\nLet $X = \\omega k - \\omega(i-1)$ and $Y = \\omega k - \\omega(j-1)$. Then,\n$$X-Y = \\omega(j-1) - \\omega(i-1) = \\omega(j-i)$$\n$$X+Y = 2\\omega k - \\omega(i-1) - \\omega(j-1) = 2\\omega k - \\omega(i+j-2)$$\nThe expression for $R_{ij}$ becomes\n$$R_{ij} = A^2 \\lim_{N \\to \\infty} \\frac{1}{2N} \\sum_{k=0}^{N-1} \\left[ \\cos(\\omega(j-i)) + \\cos(2\\omega k - \\omega(i+j-2)) \\right]$$\nWe can separate the limit of the sum into the sum of the limits:\n$$R_{ij} = \\frac{A^2}{2} \\left[ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\omega(j-i)) + \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(2\\omega k - \\omega(i+j-2)) \\right]$$\nThe first term, $\\cos(\\omega(j-i))$, is a constant with respect to the summation index $k$. Its time average is simply the constant itself.\nFor the second term, we must evaluate the time average of a sinusoid. For any non-zero frequency $\\nu$ that is not a multiple of $2\\pi$, the time average is zero: $\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\nu k + \\phi) = 0$. In our case, the frequency is $2\\omega$. Since $\\omega \\in (0, \\pi)$, we have $2\\omega \\in (0, 2\\pi)$. Thus, the frequency is not a multiple of $2\\pi$, and the limit is zero.\n$$ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(2\\omega k - \\omega(i+j-2)) = 0$$\nTherefore, the elements of the matrix $R$ are\n$$R_{ij} = \\frac{A^2}{2} \\cos(\\omega(j-i))$$\nNow, we must analyze the structure of the matrix $R$. We can express the entry $R_{ij}$ using the cosine angle subtraction formula, $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$:\n$$R_{ij} = \\frac{A^2}{2} \\left[ \\cos(\\omega(i-1)) \\cos(\\omega(j-1)) + \\sin(\\omega(i-1)) \\sin(\\omega(j-1)) \\right]$$\nThis reveals that the matrix $R$ can be written as the sum of two outer products. Let us define two vectors $c, s \\in \\mathbb{R}^n$:\n$$c = \\begin{pmatrix} \\cos(0\\omega) \\\\ \\cos(1\\omega) \\\\ \\vdots \\\\ \\cos((n-1)\\omega) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\cos(\\omega) \\\\ \\vdots \\\\ \\cos((n-1)\\omega) \\end{pmatrix}$$\n$$s = \\begin{pmatrix} \\sin(0\\omega) \\\\ \\sin(1\\omega) \\\\ \\vdots \\\\ \\sin((n-1)\\omega) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\sin(\\omega) \\\\ \\vdots \\\\ \\sin((n-1)\\omega) \\end{pmatrix}$$\nThen, the matrix $R$ can be expressed as\n$$R = \\frac{A^2}{2} \\left( c c^{\\top} + s s^{\\top} \\right)$$\nThe matrix $R$ is a linear combination of two rank-one matrices, $c c^{\\top}$ and $s s^{\\top}$. The rank of a sum of matrices is less than or equal to the sum of their ranks. Thus, $\\mathrm{rank}(R) \\le \\mathrm{rank}(c c^{\\top}) + \\mathrm{rank}(s s^{\\top})$. Since $A>0$, both $c$ and $s$ are non-zero vectors, so $\\mathrm{rank}(c c^{\\top})=1$ and $\\mathrm{rank}(s s^{\\top})=1$. This implies $\\mathrm{rank}(R) \\le 2$.\n\nTo determine the exact rank, we check if the vectors $c$ and $s$ are linearly independent. Consider a linear combination $\\alpha c + \\beta s = 0$. This vector equation implies a set of $n$ scalar equations:\n$$\\alpha \\cos(k\\omega) + \\beta \\sin(k\\omega) = 0 \\quad \\text{for } k = 0, 1, \\dots, n-1$$\nFor $k=0$, the equation becomes $\\alpha \\cos(0) + \\beta \\sin(0) = \\alpha \\cdot 1 + \\beta \\cdot 0 = \\alpha = 0$.\nSubstituting $\\alpha=0$ into the equation for $k=1$, we get $\\beta \\sin(\\omega) = 0$. Since $\\omega \\in (0, \\pi)$, $\\sin(\\omega) \\ne 0$, which implies $\\beta=0$.\nThus, the only solution is $\\alpha=\\beta=0$, which proves that the vectors $c$ and $s$ are linearly independent. The column space of $R$ is spanned by $c$ and $s$, so $\\mathrm{rank}(R) = 2$.\n\nWe are given that the order of the system is $n \\ge 3$. According to the rank-nullity theorem, for an $n \\times n$ matrix $R$, we have $\\mathrm{rank}(R) + \\mathrm{nullity}(R) = n$.\nWith $\\mathrm{rank}(R)=2$, the nullity of $R$ is $n-2$.\nSince $n \\ge 3$, the nullity is $n-2 \\ge 1$. A nullity of at least $1$ means that there exists at least one non-zero vector $v$ such that $Rv=0v$. By definition, this means that $\\lambda=0$ is an eigenvalue of $R$.\n\nFurthermore, the matrix $R$ is a Gramian-type matrix, constructed as a limit of sums of positive semi-definite matrices $\\varphi_k \\varphi_k^{\\top}$. Therefore, $R$ itself must be positive semi-definite. All eigenvalues of a positive semi-definite matrix must be non-negative.\nWe have established that $0$ is an eigenvalue of $R$, and that all eigenvalues of $R$ are greater than or equal to $0$. Consequently, the smallest eigenvalue of $R$, $\\lambda_{\\min}$, must be $0$. This result holds for any $A > 0$ and any $\\omega \\in (0, \\pi)$, given $n \\ge 3$.", "answer": "$$\\boxed{0}$$", "id": "2876722"}, {"introduction": "Having established that spectral richness is crucial, we now turn to a constructive problem: how do we systematically design an input that guarantees it? This practice guides you through the design of a multi-channel Pseudo-Random Binary Sequence (PRBS), a workhorse signal in system identification [@problem_id:2876747]. You will derive the minimal data length required from first principles and select appropriate signal generators to satisfy the algebraic conditions for persistency of excitation, bridging the gap between abstract theory and practical signal engineering.", "problem": "Consider a discrete-time, linear time-invariant, multiple-input system to be identified using least-squares with a block regressor of length $L$. An input sequence $\\{u(k)\\}$ with $m$ input channels is said to be persistently exciting of order $L$ if the block Hankel matrix built from $L$ successive delays of the input has full row rank, which guarantees the identifiability of all parameters that enter linearly through those $L$ delays. A widely used deterministic choice for excitation is a Pseudo-Random Binary Sequence (PRBS) generated by a Linear Feedback Shift Register (LFSR). A maximum-length LFSR sequence (an $m$-sequence) of register degree $n$ has period $P = 2^{n}-1$ and satisfies a linear recurrence of order $n$.\n\nYou are to design a two-channel PRBS input that is persistently exciting of order $L$ and verify the minimal data length requirement from first principles of rank and recurrence. Use the following specifications:\n- Number of inputs $m = 2$.\n- Block size $L = 4$.\n- Each channel is produced by an independent LFSR of degree $n$, run without wrap-around over the used data segment, and mapped to $\\{-1, +1\\}$ by $v(k) = 2x(k)-1$ from binary LFSR output $x(k) \\in \\{0,1\\}$.\n\nTasks:\n1. Starting from the definition of persistency of excitation of order $L$, construct the block Hankel matrix of the inputs and derive a necessary lower bound on the usable data length $N$ that makes full row rank possible in terms of $m$ and $L$. Do not assume any probabilistic model; argue purely from matrix dimensions and rank.\n2. Argue an additional necessary condition imposed by LFSR-generated PRBS due to their linear recurrence: relate $L$ and the LFSR degree $n$ so that the row blocks built from successive delays can be linearly independent within each channel.\n3. Choose the smallest LFSR degree $n$ that satisfies your condition in item 2 for the given $L$, and select two distinct primitive characteristic polynomials of that degree to define the two independent channels. State one valid primitive polynomial for each channel and specify a nonzero initial state for each LFSR. Define the two-channel PRBS $u(k) = \\begin{pmatrix}u_{1}(k) \\\\ u_{2}(k)\\end{pmatrix}$ explicitly in terms of the two LFSRs and the mapping to $\\{-1,+1\\}$.\n4. Using well-tested properties of maximum-length sequences (period and near-white autocorrelation over windows shorter than a full period), justify that for your construction and for your minimal $N$, the block Hankel matrix with $L$ delays has full row rank and thus the input is persistently exciting of order $L$. You may assume the data segment length $N$ does not exceed the period of either channel so that no wrap-around occurs.\n5. Report, as your final answer, the minimal usable data length $N_{\\min}$ you derived in item 1 for the given $m$ and $L$. Provide only the numerical value of $N_{\\min}$.\n\nProvide the requested construction and justification. Your final answer must be a single number with no units and no rounding instructions are necessary for this integer result.", "solution": "1. Derivation of the minimal data length $N$.\nThe problem defines persistency of excitation of order $L$ as the condition that the block Hankel matrix of the inputs has full row rank. Let the input signal be a sequence of $m$-dimensional vectors $\\{u(k)\\}$, where $u(k) \\in \\mathbb{R}^{m}$. We consider a segment of this sequence of length $N$, from $k=0$ to $k=N-1$.\n\nThe block Hankel matrix, denoted $\\mathcal{H}$, built from $L$ successive delays of the input sequence $\\{u(k)\\}_{k=0}^{N-1}$ is structured as follows:\n$$\n\\mathcal{H} = \\begin{pmatrix}\nu(0) & u(1) & u(2) & \\dots & u(N-L) \\\\\nu(1) & u(2) & u(3) & \\dots & u(N-L+1) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nu(L-1) & u(L) & u(L+1) & \\dots & u(N-1)\n\\end{pmatrix}\n$$\nIn this matrix, each element $u(k)$ is a column vector of size $m \\times 1$. The matrix is composed of $L$ block rows and $N-L+1$ columns. Consequently, the total dimensions of $\\mathcal{H}$ are $(m \\times L) \\times (N-L+1)$.\n\nFor this matrix to have full row rank, its rank must be equal to the number of its rows, which is $mL$. A fundamental theorem of linear algebra states that the rank of a matrix cannot exceed its number of rows or its number of columns. For the rank to be $mL$, it is a necessary condition that the number of columns is greater than or equal to the number of rows.\n$$\n\\text{Number of columns} \\ge \\text{Number of rows}\n$$\n$$\nN - L + 1 \\ge mL\n$$\nThis inequality provides a lower bound on the required data length $N$. Rearranging for $N$, we get:\n$$\nN \\ge mL + L - 1\n$$\nSubstituting the given values $m=2$ and $L=4$:\n$$\nN \\ge (2)(4) + 4 - 1\n$$\n$$\nN \\ge 8 + 4 - 1 = 11\n$$\nThus, the minimal usable data length required to potentially satisfy the full rank condition is $N_{\\min} = 11$.\n\n2.  Condition on LFSR degree $n$.\nEach input channel $u_i(k)$ is derived from a binary sequence $\\{x_i(k)\\}$ generated by a Linear Feedback Shift Register (LFSR) of degree $n$. The binary sequence satisfies a linear recurrence relation of order $n$ over the Galois field $GF(2)$. The real-valued sequence $u_i(k) = 2x_i(k) - 1$ also satisfies a linear recurrence relation, but over the field of real numbers $\\mathbb{R}$. The minimal order of such a recurrence is known as the linear complexity of the sequence. For a maximum-length sequence (m-sequence) of degree $n$, its linear complexity over $\\mathbb{R}$ is also $n$.\n\nThe rows of the block Hankel matrix $\\mathcal{H}$ that correspond to a single channel $i$ are:\n$$\n\\begin{pmatrix}\nu_i(0) & u_i(1) & \\dots & u_i(N-L) \\\\\nu_i(1) & u_i(2) & \\dots & u_i(N-L+1) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_i(L-1) & u_i(L) & \\dots & u_i(N-1)\n\\end{pmatrix}\n$$\nThese $L$ rows are vectors in $\\mathbb{R}^{N-L+1}$. If these rows were to be linearly dependent, there would exist coefficients $\\alpha_0, \\alpha_1, \\dots, \\alpha_{L-1}$, not all zero, such that:\n$$\n\\sum_{j=0}^{L-1} \\alpha_j u_i(k+j) = 0 \\quad \\text{for } k=0, 1, \\dots, N-L\n$$\nThis equation implies that the sequence $\\{u_i(k)\\}$ satisfies a linear recurrence of order at most $L-1$ over this data window. If this dependency is inherent to the sequence generation itself, it will prevent the full set of $mL$ rows of $\\mathcal{H}$ from being linearly independent.\n\nSince the sequence $\\{u_i(k)\\}$ has a linear complexity of $n$, it satisfies a minimal recurrence of order $n$. If we choose an LFSR degree $n < L$, the sequence $\\{u_i(k)\\}$ would satisfy a linear recurrence of order $n$. This means a linear dependency among any $n+1$ consecutive time-shifted versions of the sequence is guaranteed to exist. As $n+1 \\le L$, this would imply a linear dependency among the first $n+1$ (and thus among all $L$) rows corresponding to channel $i$. This would make it impossible for the matrix $\\mathcal{H}$ to achieve full row rank $mL$.\n\nTo avoid this structural linear dependency, we must ensure that the linear complexity of each channel's sequence is at least $L$. Therefore, a necessary condition is:\n$$\nn \\ge L\n$$\n\n3.  Choice of LFSR degree and polynomials.\nGiven the condition $n \\ge L$ from the previous step and the problem parameter $L=4$, the smallest possible integer value for the LFSR degree is $n=4$.\n\nWe must select two distinct primitive characteristic polynomials of degree $n=4$ over $GF(2)$ to generate two independent m-sequences. The number of primitive polynomials of degree $n$ is given by $\\frac{\\phi(2^n-1)}{n}$, where $\\phi$ is Euler's totient function. For $n=4$, this is $\\frac{\\phi(15)}{4} = \\frac{\\phi(3)\\phi(5)}{4} = \\frac{(2)(4)}{4} = 2$.\nThe two primitive polynomials of degree $4$ are:\n$p_1(z) = z^4 + z + 1$\n$p_2(z) = z^4 + z^3 + 1$\n\nWe define the two LFSRs based on these polynomials:\nChannel 1: The sequence $\\{x_1(k)\\}$ is generated by the recurrence relation corresponding to $p_1(z)$, which is $x_1(k) = x_1(k-3) + x_1(k-4) \\pmod{2}$.\nChannel 2: The sequence $\\{x_2(k)\\}$ is generated by the recurrence relation corresponding to $p_2(z)$, which is $x_2(k) = x_2(k-1) + x_2(k-4) \\pmod{2}$.\n\nTo start the sequences, we must specify non-zero initial states. A simple choice is:\nInitial state for LFSR 1 (contents of registers for $k-4, k-3, k-2, k-1$): $(1, 0, 0, 0)$.\nInitial state for LFSR 2: $(1, 0, 0, 0)$.\n\nThe two-channel PRBS input $u(k)$ is then constructed by mapping the binary outputs $\\{x_1(k), x_2(k)\\}$ from $\\{0, 1\\}$ to $\\{-1, +1\\}$:\n$$\nu(k) = \\begin{pmatrix} u_1(k) \\\\ u_2(k) \\end{pmatrix} = \\begin{pmatrix} 2x_1(k) - 1 \\\\ 2x_2(k) - 1 \\end{pmatrix}\n$$\n\n4.  Justification of full rank.\nWe must justify that for the constructed input and the minimal data length $N=N_{\\min}=11$, the Hankel matrix $\\mathcal{H}$ has full row rank.\nFrom Task 1, for $m=2, L=4, N=11$, the matrix $\\mathcal{H}$ has dimensions $(mL) \\times (N-L+1) = 8 \\times 8$. To have full rank, this square matrix must be non-singular. This requires its $8$ rows to be linearly independent.\n\nA linear combination of the rows of $\\mathcal{H}$ can be written as a pair of filtered sequences. Let constants $c_{i,j}$ for $i \\in \\{1,2\\}, j \\in \\{0, \\dots, 3\\}$ be the coefficients of the linear combination. A zero linear combination implies:\n$$\n\\sum_{j=0}^{3} c_{1,j} u_1(k+j) + \\sum_{j=0}^{3} c_{2,j} u_2(k+j) = 0, \\quad \\text{for } k = 0, 1, \\dots, 7\n$$\nLet $P_1(D) = \\sum_{j=0}^{3} c_{1,j} D^j$ and $P_2(D) = \\sum_{j=0}^{3} c_{2,j} D^j$ be polynomials in the shift operator $D$. The condition is $P_1(D)u_1(k) + P_2(D)u_2(k) = 0$.\n\nThe sequences $\\{u_1(k)\\}$ and $\\{u_2(k)\\}$ have minimal polynomials $p_1(z)$ and $p_2(z)$ respectively, both of degree $n=4$. Since $p_1(z)$ and $p_2(z)$ are distinct irreducible (and hence coprime) polynomials, the sequences $\\{u_1(k)\\}$ and $\\{u_2(k)\\}$ are linearly independent over the field of rational functions. If the relation $P_1(D)u_1(k) + P_2(D)u_2(k) = 0$ were to hold for all $k$, it would imply that $p_1(D)$ divides $P_2(D)$ and $p_2(D)$ divides $P_1(D)$.\nHowever, the degrees of $P_1(D)$ and $P_2(D)$ are at most $L-1=3$, while the degrees of $p_1(D)$ and $p_2(D)$ are $n=4$. A polynomial of degree $4$ cannot divide a non-zero polynomial of degree $3$. This forces $P_1(D)=0$ and $P_2(D)=0$, which in turn means all coefficients $c_{i,j}$ must be zero.\n\nThis proves the linear independence of rows for an infinitely long sequence. For our finite data window of length $N-L+1=8$, which is greater than the degree $n=4$ of the minimal polynomials but shorter than the sequence period $P=15$, the algebraic properties of the m-sequences are strong enough to prevent such a linear dependency from occurring. The \"near-white\" autocorrelation property implies that shifted versions of a sequence are nearly orthogonal, and the use of independent generators ensures near-orthogonality between channels. These properties ensure that the $8 \\times 8$ matrix $\\mathcal{H}$ is non-singular. Therefore, the constructed input is persistently exciting of order $L=4$.\n\n5.  Minimal usable data length $N_{\\min}$.\nAs derived in Task 1, the minimal usable data length $N_{\\min}$ is the smallest integer $N$ satisfying $N \\ge mL + L - 1$. With $m=2$ and $L=4$, this value is $11$.", "answer": "$$\\boxed{11}$$", "id": "2876747"}, {"introduction": "Persistency of excitation becomes more nuanced in multi-input systems, where the relationships between input channels are as important as the properties of the channels themselves. This exercise presents a critical case study where each input is individually PE, but a simple time-delay relationship between them creates a fatal linear dependency in the joint regressor [@problem_id:2876761]. By analyzing this scenario, you will uncover how inter-channel correlation can lead to a loss of identifiability, demonstrating that inputs must be collectively, not just individually, exciting.", "problem": "Consider a two-input single-output discrete-time linear time-invariant finite impulse response system of order $n=2$ per input channel. Let the unknown parameter vector be $\\theta \\in \\mathbb{R}^{4}$, stacking the two-tap coefficients from each input channel. The standard least-squares regressor at time $k$ is\n$$\n\\varphi(k) \\triangleq \\begin{pmatrix} u_{1}(k) \\\\ u_{1}(k-1) \\\\ u_{2}(k) \\\\ u_{2}(k-1) \\end{pmatrix} \\in \\mathbb{R}^{4}.\n$$\nFor a finite horizon $N \\geq 3$, define the data Gramian (information matrix)\n$$\nR_{N} \\triangleq \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top} \\in \\mathbb{R}^{4 \\times 4}.\n$$\nA scalar input sequence $u(k)$ is called persistently exciting (PE) of order $2$ if the $2 \\times 2$ Gramian formed from the regressor $\\psi(k) \\triangleq \\begin{pmatrix} u(k) \\\\ u(k-1) \\end{pmatrix}$ is positive definite over sufficiently long windows, equivalently if its asymptotic time-average Gramian is positive definite. Identifiability of $\\theta$ from least squares over a window requires $R_{N}$ to be positive definite.\n\nNow consider the following constructed input pair. Fix two incommensurate frequencies $\\omega_{1}, \\omega_{2} \\in (0,\\pi)$, for example $\\omega_{1}=\\pi/\\sqrt{2}$ and $\\omega_{2}=\\pi/\\sqrt{3}$. Let\n$$\nu_{1}(k) \\triangleq \\sin(\\omega_{1} k) + \\sin(\\omega_{2} k), \\qquad u_{2}(k) \\triangleq u_{1}(k-1), \\quad \\text{for all integers } k.\n$$\nTasks:\n- Using only fundamental definitions and properties of linear time-invariant systems and least-squares regression, argue why each scalar channel $u_{1}$ and $u_{2}$, considered individually with its own order-$2$ regressor $\\psi_{i}(k) \\triangleq \\begin{pmatrix} u_{i}(k) \\\\ u_{i}(k-1) \\end{pmatrix}$, is persistently exciting of order $2$.\n- Explain why, despite the individual persistency of excitation of each channel, the joint regressor $\\varphi(k)$ fails to be persistently exciting of order $4$, and characterize the resulting loss of identifiability for $\\theta$ in terms of linear dependencies in the data.\n- Compute the determinant $\\det(R_{N})$ as a function of $N$ for the above input pair. Express your final answer as an exact value with no units. No rounding is required.", "solution": "First, we must establish that the individual input channels $u_{1}(k)$ and $u_{2}(k)$ are persistently exciting (PE) of order $2$. According to the definition, a signal $u(k)$ is PE of order $n$ if its asymptotic time-average Gramian matrix is positive definite. For a scalar signal $u_i(k)$ and order $n=2$, the regressor is $\\psi_{i}(k) = \\begin{pmatrix} u_{i}(k) & u_{i}(k-1) \\end{pmatrix}^{\\top}$, and we must analyze the definiteness of the matrix $M_{i} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} \\psi_{i}(k) \\psi_{i}(k)^{\\top}$.\n\nFor the first channel, $u_{1}(k) = \\sin(\\omega_{1} k) + \\sin(\\omega_{2} k)$. The matrix $M_{1}$ is given by:\n$$\nM_{1} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} \\begin{pmatrix} u_{1}(k)^{2} & u_{1}(k) u_{1}(k-1) \\\\ u_{1}(k) u_{1}(k-1) & u_{1}(k-1)^{2} \\end{pmatrix}\n$$\nThe entries are the signal's autocorrelation values at lags $0$ and $1$. For a quasi-periodic signal composed of sinusoids with incommensurate frequencies, the time-average of cross-products of terms with different frequencies is zero. The time-average of $\\sin^{2}(\\omega k + \\phi)$ is $\\frac{1}{2}$. Therefore, the autocorrelation function $r_{1}(\\tau) = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} u_{1}(k) u_{1}(k+\\tau)$ is:\n$$\nr_{1}(\\tau) = \\frac{1}{2} \\cos(\\omega_{1} \\tau) + \\frac{1}{2} \\cos(\\omega_{2} \\tau)\n$$\nThe matrix $M_{1}$ is thus:\n$$\nM_{1} = \\begin{pmatrix} r_{1}(0) & r_{1}(1) \\\\ r_{1}(1) & r_{1}(0) \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2}(\\cos(\\omega_{1}) + \\cos(\\omega_{2})) \\\\ \\frac{1}{2}(\\cos(\\omega_{1}) + \\cos(\\omega_{2})) & 1 \\end{pmatrix}\n$$\nFor $M_{1}$ to be positive definite, its principal minors must be positive. The first minor is $1 > 0$. The second minor is its determinant, $\\det(M_{1}) = 1 - \\frac{1}{4}(\\cos(\\omega_{1}) + \\cos(\\omega_{2}))^{2}$. Since $\\omega_{1}, \\omega_{2} \\in (0, \\pi)$, we have $|\\cos(\\omega_{i})| < 1$. By the triangle inequality, $|\\cos(\\omega_{1}) + \\cos(\\omega_{2})| < 2$, which implies $(\\cos(\\omega_{1}) + \\cos(\\omega_{2}))^{2} < 4$. Consequently, $\\det(M_{1}) > 1 - \\frac{4}{4} = 0$. Since $M_1$ is symmetric with positive principal minors, it is positive definite. Thus, $u_{1}(k)$ is PE of order $2$. For the second channel, $u_{2}(k) = u_{1}(k-1)$. This is merely a time-shift of $u_{1}(k)$. The autocorrelation function of a stationary or cyclo-stationary process is invariant to a time shift of the signal. Therefore, the asymptotic Gramian matrix $M_{2}$ for $u_{2}(k)$ is identical to $M_{1}$, i.e., $M_{2} = M_{1}$. As $M_{1}$ is positive definite, so is $M_{2}$, and $u_{2}(k)$ is also PE of order $2$.\n\nSecond, we analyze why the joint regressor $\\varphi(k)$ fails to be persistently exciting of order $4$. The regressor is defined as $\\varphi(k) = \\begin{pmatrix} u_{1}(k) & u_{1}(k-1) & u_{2}(k) & u_{2}(k-1) \\end{pmatrix}^{\\top}$. The crucial condition is the linear relationship between the two input signals: $u_{2}(k) = u_{1}(k-1)$ for all $k$. We substitute this into the regressor definition:\n$$\n\\varphi(k) = \\begin{pmatrix} u_{1}(k) \\\\ u_{1}(k-1) \\\\ u_{1}(k-1) \\\\ u_{1}(k-2) \\end{pmatrix}\n$$\nIt is immediately apparent that the second and third components of the vector $\\varphi(k)$ are identical for all $k$. This introduces a linear dependency among the columns of the data matrix over any time horizon. Specifically, let $c = \\begin{pmatrix} 0 & 1 & -1 & 0 \\end{pmatrix}^{\\top}$. Then for any time instant $k$, the inner product $c^{\\top} \\varphi(k)$ is:\n$$\nc^{\\top} \\varphi(k) = 0 \\cdot u_{1}(k) + 1 \\cdot u_{1}(k-1) - 1 \\cdot u_{1}(k-1) + 0 \\cdot u_{1}(k-2) = 0\n$$\nThe information matrix $R_{N} = \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top}$ is therefore singular. To prove this, we multiply $R_{N}$ by the non-zero vector $c$:\n$$\nR_{N} c = \\left( \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top} \\right) c = \\sum_{k=2}^{N} \\varphi(k) (\\varphi(k)^{\\top} c) = \\sum_{k=2}^{N} \\varphi(k) (c^{\\top} \\varphi(k))^{\\top} = \\sum_{k=2}^{N} \\varphi(k) \\cdot 0 = 0\n$$\nSince there exists a non-zero vector $c$ in the null space of $R_{N}$, the matrix $R_{N}$ is singular for any $N \\geq 2$. A singular matrix cannot be positive definite, so the joint regressor is not PE of order $4$. This singularity leads to a loss of identifiability. The system model is $y(k) = \\varphi(k)^{\\top} \\theta$. Consider an alternative parameter vector $\\theta' = \\theta + \\alpha c$ for any non-zero scalar $\\alpha$. The system output produced by $\\theta'$ is $\\varphi(k)^{\\top} \\theta' = \\varphi(k)^{\\top} (\\theta + \\alpha c) = \\varphi(k)^{\\top} \\theta + \\alpha \\varphi(k)^{\\top} c = \\varphi(k)^{\\top} \\theta$. The output is identical, meaning that the data cannot be used to distinguish between $\\theta$ and $\\theta'$. Writing $\\theta = \\begin{pmatrix} \\theta_{11} & \\theta_{12} & \\theta_{21} & \\theta_{22} \\end{pmatrix}^{\\top}$, the indistinguishable parameters are of the form $\\theta' = \\begin{pmatrix} \\theta_{11} & \\theta_{12} + \\alpha & \\theta_{21} - \\alpha & \\theta_{22} \\end{pmatrix}^{\\top}$. This shows that only the sum $\\theta_{12} + \\theta_{21}$ is identifiable, but not the individual parameters $\\theta_{12}$ and $\\theta_{21}$.\n\nThird, we are to compute the determinant of $R_{N}$. The information matrix is given by $R_{N} = \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top}$. Its $(i,j)$-th element is $(R_{N})_{ij} = \\sum_{k=2}^{N} \\varphi_{i}(k) \\varphi_{j}(k)$, where $\\varphi_{i}(k)$ is the $i$-th component of $\\varphi(k)$. As established, $\\varphi_{2}(k) = u_{1}(k-1)$ and $\\varphi_{3}(k) = u_{2}(k) = u_{1}(k-1)$. Therefore, $\\varphi_{2}(k) = \\varphi_{3}(k)$ for all $k$.\nLet us examine the second and third rows of the matrix $R_{N}$.\nThe $j$-th element of the second row is $(R_{N})_{2j} = \\sum_{k=2}^{N} \\varphi_{2}(k) \\varphi_{j}(k)$.\nThe $j$-th element of the third row is $(R_{N})_{3j} = \\sum_{k=2}^{N} \\varphi_{3}(k) \\varphi_{j}(k)$.\nSince $\\varphi_{2}(k) = \\varphi_{3}(k)$, it follows trivially that $(R_{N})_{2j} = (R_{N})_{3j}$ for all $j \\in \\{1, 2, 3, 4\\}$.\nThis means that the second row of the matrix $R_{N}$ is identical to its third row. A fundamental property of determinants is that if a square matrix has two identical rows (or columns), its determinant is zero. This holds for any value of $N \\geq 2$. Therefore, the determinant of $R_{N}$ is exactly zero, independent of $N$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2876761"}]}