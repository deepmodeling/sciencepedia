{"hands_on_practices": [{"introduction": "Real-world dynamical systems often exhibit oscillatory behavior, which corresponds to complex conjugate eigenvalues in their state-space models. While the system matrix $A$ is real, its eigenvectors are complex. This practice guides you through the fundamental technique of constructing a real similarity transformation from a complex eigenpair, allowing you to convert the system into a more intuitive real block-canonical form [@problem_id:2905099]. Mastering this process is key to understanding and analyzing oscillatory modes directly within the real domain.", "problem": "Consider a continuous-time linear time-invariant (LTI) state-space model with real state matrix $A \\in \\mathbb{R}^{n \\times n}$, where $n \\geq 3$. Assume $A$ has exactly one complex conjugate eigenpair $\\alpha \\pm \\mathrm{j}\\beta$ with $\\beta > 0$ (where $\\mathrm{j}^{2} = -1$), and all remaining eigenvalues are real and simple. Let $v \\in \\mathbb{C}^{n}$ be a right eigenvector of $A$ associated with $\\alpha + \\mathrm{j}\\beta$, and write $v = x + \\mathrm{j} y$ with $x, y \\in \\mathbb{R}^{n}$, where $x$ and $y$ are linearly independent.\n\nUsing only the definitions of eigenvalues and eigenvectors, similarity transformation, and invariant subspaces, do the following:\n\n- Construct a real, invertible similarity transformation $T \\in \\mathbb{R}^{n \\times n}$ whose first two columns are $x$ and $y$ and whose remaining columns complete $\\{x, y\\}$ to a real basis of $\\mathbb{R}^{n}$, so that $T^{-1} A T$ is block upper triangular with a leading $2 \\times 2$ real block that captures the dynamics associated with $\\alpha \\pm \\mathrm{j}\\beta$.\n- Derive the exact real $2 \\times 2$ matrix $C \\in \\mathbb{R}^{2 \\times 2}$ such that $A (\\, x \\;\\; y \\,) = (\\, x \\;\\; y \\,) C$.\n\nFrom this construction, determine the monic characteristic polynomial $\\phi_{2}(\\lambda)$ of the leading $2 \\times 2$ block $C$ as a closed-form expression in terms of $\\alpha$ and $\\beta$. Provide your final answer as a single analytic expression in $\\lambda$ with coefficients expressed only in terms of $\\alpha$ and $\\beta$. No numerical approximation is required or permitted.", "solution": "We begin from fundamental definitions. An eigenvalue $\\lambda \\in \\mathbb{C}$ and eigenvector $v \\in \\mathbb{C}^{n} \\setminus \\{0\\}$ of a matrix $A \\in \\mathbb{R}^{n \\times n}$ satisfy\n$$\nA v = \\lambda v.\n$$\nA similarity transformation by an invertible matrix $T \\in \\mathbb{R}^{n \\times n}$ is the map $A \\mapsto T^{-1} A T$. If a subspace $\\mathcal{V} \\subset \\mathbb{R}^{n}$ is invariant under $A$ (that is, $A \\mathcal{V} \\subseteq \\mathcal{V}$), then in a basis that starts with a basis of $\\mathcal{V}$, the matrix representation of $A$ becomes block upper triangular, and the leading block represents the restriction of $A$ to $\\mathcal{V}$.\n\nBecause $A$ is real and has a complex eigenpair $\\alpha \\pm \\mathrm{j}\\beta$ with $\\beta > 0$, let $v = x + \\mathrm{j} y$ be an eigenvector for $\\alpha + \\mathrm{j}\\beta$, where $x, y \\in \\mathbb{R}^{n}$ are linearly independent. From the eigen-equation,\n$$\nA v = (\\alpha + \\mathrm{j}\\beta) v.\n$$\nWriting $v = x + \\mathrm{j} y$ and using linearity of $A$ over $\\mathbb{R}$,\n$$\nA (x + \\mathrm{j} y) = A x + \\mathrm{j} A y = (\\alpha + \\mathrm{j}\\beta)(x + \\mathrm{j} y) = (\\alpha x - \\beta y) + \\mathrm{j}(\\beta x + \\alpha y).\n$$\nBy equating real and imaginary parts in $\\mathbb{C}^{n}$, we obtain two coupled real equations:\n$$\nA x = \\alpha x - \\beta y, \\qquad A y = \\beta x + \\alpha y.\n$$\nStacking these relations columnwise gives\n$$\nA \\begin{pmatrix} x  y \\end{pmatrix} = \\begin{pmatrix} A x  A y \\end{pmatrix} = \\begin{pmatrix} \\alpha x - \\beta y  \\beta x + \\alpha y \\end{pmatrix} = \\begin{pmatrix} x  y \\end{pmatrix} \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}.\n$$\nDefine the real matrix\n$$\nX \\coloneqq \\begin{pmatrix} x  y \\end{pmatrix} \\in \\mathbb{R}^{n \\times 2}, \\qquad C \\coloneqq \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}.\n$$\nThen the above relation is compactly\n$$\nA X = X C.\n$$\nHence the real $2$-dimensional subspace $\\mathcal{V} \\coloneqq \\operatorname{span}\\{x, y\\}$ is invariant under $A$, because $A X$ is a linear combination of the columns of $X$. Choose any real matrix $W \\in \\mathbb{R}^{n \\times (n-2)}$ whose columns complete $\\{x, y\\}$ to a real basis of $\\mathbb{R}^{n}$, and define the invertible matrix\n$$\nT \\coloneqq \\begin{pmatrix} X  W \\end{pmatrix} \\in \\mathbb{R}^{n \\times n}.\n$$\nIn this basis, the matrix of $A$ becomes block upper triangular, because $\\mathcal{V}$ is invariant:\n$$\nT^{-1} A T = \\begin{pmatrix} C  * \\\\ 0  A_{22} \\end{pmatrix},\n$$\nwhere $C \\in \\mathbb{R}^{2 \\times 2}$ is as above and $A_{22} \\in \\mathbb{R}^{(n-2) \\times (n-2)}$ represents the action of $A$ on the complementary invariant subspace spanned by the columns of $W$.\n\nIt remains to compute the monic characteristic polynomial $\\phi_{2}(\\lambda)$ of the leading $2 \\times 2$ block $C$. By definition,\n$$\n\\phi_{2}(\\lambda) \\coloneqq \\det(\\lambda I_{2} - C) = \\det\\!\\left(\\begin{pmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{pmatrix} - \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}\\right) = \\det\\!\\left(\\begin{pmatrix} \\lambda - \\alpha  -\\beta \\\\ \\beta  \\lambda - \\alpha \\end{pmatrix}\\right).\n$$\nEvaluating the determinant gives\n$$\n\\phi_{2}(\\lambda) = (\\lambda - \\alpha)^{2} + \\beta^{2} = \\lambda^{2} - 2 \\alpha \\lambda + \\alpha^{2} + \\beta^{2}.\n$$\nThis polynomial is monic and has real coefficients expressed only in terms of $\\alpha$ and $\\beta$, as required. It is also the minimal polynomial of the restriction of $A$ to $\\mathcal{V}$ provided $\\beta \\neq 0$, which holds by assumption.\n\nTherefore, the requested closed-form expression is\n$$\n\\phi_{2}(\\lambda) = \\lambda^{2} - 2 \\alpha \\lambda + \\left(\\alpha^{2} + \\beta^{2}\\right).\n$$", "answer": "$$\\boxed{\\lambda^{2} - 2 \\alpha \\lambda + \\left(\\alpha^{2} + \\beta^{2}\\right)}$$", "id": "2905099"}, {"introduction": "While diagonalization is a powerful tool, many systems are not diagonalizable due to having insufficient eigenvectors for their repeated eigenvalues. This exercise tackles this important case by guiding you through the construction of the Jordan Canonical Form (JCF) [@problem_id:2905080]. You will build a basis from generalized eigenvectors to form the similarity transformation that reveals the system's underlying Jordan block structure, a critical skill for analyzing the behavior of non-diagonalizable systems.", "problem": "An engineer is modeling a single-input single-output linear time-invariant (LTI) state-space system whose state-transition matrix has a repeated pole. Consider the real matrix $A \\in \\mathbb{R}^{5 \\times 5}$ given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  0  0  1  0 \\\\\n0  2  0  0  0 \\\\\n0  0  2  0  0 \\\\\n0  1  0  2  0 \\\\\n0  0  1  0  2\n\\end{pmatrix}.\n$$\nStarting from the definitions of eigenvalues, eigenvectors, and generalized eigenvectors, as well as the definition of the Jordan canonical form (JCF), carry out the following:\n\n- Determine the eigenvalue(s) of $A$ together with their algebraic and geometric multiplicities. Using the root subspaces, construct a basis of generalized eigenvectors that yields a Jordan chain decomposition and hence a Jordan canonical form $J$ of $A$.\n- Form the similarity transform $S$ whose columns are the generalized eigenvectors you construct so that $S^{-1} A S = J$.\n- Using the similarity and the definition of the matrix exponential, compute $\\exp(A)$.\n\nFinally, report the value of the scalar entry of $\\exp(A)$ in position $(1,2)$, expressed exactly as a closed-form analytic expression. Do not round; no units are required.", "solution": "The problem requires finding the entry in position $(1,2)$ of the matrix exponential $\\exp(A)$ for a given matrix $A$. The solution must be derived by first finding the Jordan canonical form of $A$.\n\nThe given matrix is:\n$$\nA =\n\\begin{pmatrix}\n2  0  0  1  0 \\\\\n0  2  0  0  0 \\\\\n0  0  2  0  0 \\\\\n0  1  0  2  0 \\\\\n0  0  1  0  2\n\\end{pmatrix}\n$$\n\nFirst, we determine the eigenvalues of $A$. The characteristic equation is $\\det(A - \\lambda I) = 0$.\n$$\nA - \\lambda I =\n\\begin{pmatrix}\n2-\\lambda  0  0  1  0 \\\\\n0  2-\\lambda  0  0  0 \\\\\n0  0  2-\\lambda  0  0 \\\\\n0  1  0  2-\\lambda  0 \\\\\n0  0  1  0  2-\\lambda\n\\end{pmatrix}\n$$\nThe determinant is computed by cofactor expansion along the first row:\n$$\n\\det(A - \\lambda I) = (2-\\lambda) \\det \\begin{pmatrix} 2-\\lambda  0  0  0 \\\\ 0  2-\\lambda  0  0 \\\\ 1  0  2-\\lambda  0 \\\\ 0  1  0  2-\\lambda \\end{pmatrix} - 1 \\cdot \\det \\begin{pmatrix} 0  2-\\lambda  0  0 \\\\ 0  0  2-\\lambda  0 \\\\ 0  1  0  0 \\\\ 0  0  1  2-\\lambda \\end{pmatrix}\n$$\nThe second determinant is zero because its first column is all zeros. The remaining $4 \\times 4$ matrix is lower triangular, so its determinant is the product of its diagonal elements.\n$$\n\\det(A - \\lambda I) = (2-\\lambda) (2-\\lambda)^4 = (2-\\lambda)^5\n$$\nThus, there is a single eigenvalue $\\lambda = 2$ with algebraic multiplicity $m_a = 5$.\n\nNext, we find the geometric multiplicity $m_g$, which is the dimension of the eigenspace $E_2 = \\ker(A - 2I)$.\n$$\nA - 2I = \\begin{pmatrix}\n0  0  0  1  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0\n\\end{pmatrix}\n$$\nAn eigenvector $v = (x_1, x_2, x_3, x_4, x_5)^T$ satisfies $(A-2I)v = 0$, which gives the system of equations $x_4=0$, $x_2=0$, and $x_3=0$. The variables $x_1$ and $x_5$ are free. The eigenspace is spanned by the vectors $(1, 0, 0, 0, 0)^T$ and $(0, 0, 0, 0, 1)^T$. The dimension of the eigenspace is $2$, so the geometric multiplicity is $m_g=2$.\nSince $m_g  m_a$, the matrix $A$ is not diagonalizable. The number of Jordan blocks for $\\lambda=2$ is $m_g=2$.\n\nTo determine the sizes of these two Jordan blocks, we analyze the nullities of the powers of $(A-2I)$. Let $N = A-2I$.\n$d_1 = \\dim(\\ker(N)) = 2$.\n$$\nN^2 = (A-2I)^2 = \\begin{pmatrix}\n0  0  0  1  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  0  0  1  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0  1  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0\n\\end{pmatrix}\n$$\nThe null space of $N^2$ is defined by $x_2=0$, which has dimension $4$. So, $d_2 = \\dim(\\ker(N^2)) = 4$.\n$$\nN^3 = N^2 N = \\begin{pmatrix}\n0  1  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  0  0  1  0 \\\\\n0  0  0  0  0 \\\\\n0  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0\n\\end{pmatrix} = 0_{5 \\times 5}\n$$\nThe null space of $N^3$ is $\\mathbb{R}^5$. So, $d_3 = \\dim(\\ker(N^3))=5$. The number of Jordan blocks of size $k$ is given by $N_k = (d_k - d_{k-1}) - (d_{k+1} - d_k) = 2d_k - d_{k-1} - d_{k+1}$.\n$N_3 = 2d_3 - d_2 - d_4 = 2(5) - 4 - 5 = 1$. (since $d_4=5$)\n$N_2 = 2d_2 - d_1 - d_3 = 2(4) - 2 - 5 = 1$.\n$N_1 = 2d_1 - d_0 - d_2 = 2(2) - 0 - 4 = 0$.\nSo there is one block of size $3$ and one block of size $2$. The Jordan canonical form $J$ is:\n$$\nJ = \\begin{pmatrix}\n2  1  0  0  0 \\\\\n0  2  1  0  0 \\\\\n0  0  2  0  0 \\\\\n0  0  0  2  1 \\\\\n0  0  0  0  2\n\\end{pmatrix}\n$$\nNow, we construct the basis of generalized eigenvectors. We need one chain of length $3$ and one of length $2$.\nFor the chain of length $3$, we need a vector $u_3 \\in \\ker(N^3) \\setminus \\ker(N^2)$. $\\ker(N^2)$ consists of vectors with $x_2=0$. We can choose $u_3 = (0,1,0,0,0)^T = e_2$.\nThe chain is then $\\{u_1, u_2, u_3\\}$, where $u_2 = N u_3$ and $u_1 = N u_2 = N^2 u_3$.\n$u_2 = N u_3 = (A-2I)e_2 = (0,0,0,1,0)^T = e_4$.\n$u_1 = N u_2 = (A-2I)e_4 = (1,0,0,0,0)^T = e_1$.\nThe first chain is $\\{e_1, e_4, e_2\\}$.\n\nFor the chain of length $2$, we need a vector $w_2 \\in \\ker(N^2) \\setminus \\ker(N)$ that is linearly independent of the vectors already in the first chain projected onto $\\ker(N^2)/\\ker(N)$. A basis for $\\ker(N^2)$ is $\\{e_1, e_3, e_4, e_5\\}$. A basis for $\\ker(N)$ is $\\{e_1, e_5\\}$. The vector $u_2=e_4$ from the first chain is in $\\ker(N^2) \\setminus \\ker(N)$. We can choose $w_2=e_3=(0,0,1,0,0)^T$, which is in $\\ker(N^2) \\setminus \\ker(N)$ and is linearly independent of $e_4$ modulo $\\ker(N)$.\nThe chain is $\\{w_1, w_2\\}$, where $w_1 = N w_2$.\n$w_1 = N w_2 = (A-2I)e_3 = (0,0,0,0,1)^T = e_5$.\nThe second chain is $\\{e_5, e_3\\}$.\n\nThe similarity matrix $S$ is formed by these generalized eigenvectors as columns, ordered by chains: $S = [u_1 | u_2 | u_3 | w_1 | w_2]$.\n$$\nS = [e_1 | e_4 | e_2 | e_5 | e_3] = \\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  0  1 \\\\\n0  1  0  0  0 \\\\\n0  0  0  1  0\n\\end{pmatrix}\n$$\nThis is a permutation matrix, so it is orthogonal, and its inverse is its transpose: $S^{-1} = S^T$.\n$$\nS^{-1} = S^T = \\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  0  0  1  0 \\\\\n0  1  0  0  0 \\\\\n0  0  0  0  1 \\\\\n0  0  1  0  0\n\\end{pmatrix}\n$$\nWe want to compute $\\exp(A) = S \\exp(J) S^{-1}$. First, $\\exp(J)$.\n$J$ is block diagonal, so $\\exp(J) = \\text{diag}(\\exp(J_3(2)), \\exp(J_2(2)))$.\nFor a Jordan block $J_k(\\lambda)$, $\\exp(J_k(\\lambda)) = \\exp(\\lambda t) \\sum_{i=0}^{k-1} \\frac{t^i}{i!} N_k^i$ evaluated at $t=1$.\n$$\n\\exp(J_3(2)) = \\exp(2) \\left(I + N_3 + \\frac{1}{2}N_3^2\\right) = \\exp(2) \\begin{pmatrix} 1  1  1/2 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\n\\exp(J_2(2)) = \\exp(2) \\left(I + N_2\\right) = \\exp(2) \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}\n$$\nThus,\n$$\n\\exp(J) = \\exp(2) \\begin{pmatrix}\n1  1  1/2  0  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  1 \\\\\n0  0  0  0  1\n\\end{pmatrix}\n$$\nNext, we compute $S \\exp(J)$:\n$$\nS \\exp(J) = \\exp(2) \\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  0  1 \\\\\n0  1  0  0  0 \\\\\n0  0  0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  1/2  0  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  1 \\\\\n0  0  0  0  1\n\\end{pmatrix}\n= \\exp(2) \\begin{pmatrix}\n1  1  1/2  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  0  1 \\\\\n0  1  1  0  0 \\\\\n0  0  0  1  1\n\\end{pmatrix}\n$$\nFinally, we compute $\\exp(A) = (S \\exp(J)) S^{-1}$:\n$$\n\\exp(A) = \\exp(2) \\begin{pmatrix}\n1  1  1/2  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  0  1 \\\\\n0  1  1  0  0 \\\\\n0  0  0  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  0  0  1  0 \\\\\n0  1  0  0  0 \\\\\n0  0  0  0  1 \\\\\n0  0  1  0  0\n\\end{pmatrix}\n$$\nWe only need the entry $(\\exp(A))_{1,2}$. This corresponds to the dot product of the first row of $S \\exp(J)$ and the second column of $S^{-1}$.\nThe first row of $S \\exp(J)$ (without the $\\exp(2)$ factor) is $(1, 1, 1/2, 0, 0)$.\nThe second column of $S^{-1}$ is $(0, 0, 1, 0, 0)^T$.\n$$\n(\\exp(A))_{1,2} = \\exp(2) \\cdot \\left( 1 \\cdot 0 + 1 \\cdot 0 + \\frac{1}{2} \\cdot 1 + 0 \\cdot 0 + 0 \\cdot 0 \\right) = \\frac{1}{2}\\exp(2)\n$$\nAs a check, since $A=2I+N$ with $N=A-2I$, and $2I$ commutes with $N$, we have $\\exp(A) = \\exp(2I+N)=\\exp(2I)\\exp(N) = \\exp(2) \\exp(N)$.\n$\\exp(N) = I + N + \\frac{1}{2}N^2$, since $N^3=0$.\n$$\n\\exp(N) = \\begin{pmatrix} 1  0  0  0  0 \\\\ 0  1  0  0  0 \\\\ 0  0  1  0  0 \\\\ 0  0  0  1  0 \\\\ 0  0  0  0  1 \\end{pmatrix} + \\begin{pmatrix} 0  0  0  1  0 \\\\ 0  0  0  0  0 \\\\ 0  0  0  0  0 \\\\ 0  1  0  0  0 \\\\ 0  0  1  0  0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0  1  0  0  0 \\\\ 0  0  0  0  0 \\\\ 0  0  0  0  0 \\\\ 0  0  0  0  0 \\\\ 0  0  0  0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1/2  0  1  0 \\\\ 0  1  0  0  0 \\\\ 0  0  1  0  0 \\\\ 0  1  0  1  0 \\\\ 0  0  1  0  1 \\end{pmatrix}\n$$\nSo $\\exp(A) = \\exp(2) \\exp(N)$, and the entry $(\\exp(A))_{1,2}$ is $\\exp(2) \\cdot (\\exp(N))_{1,2} = \\exp(2) \\cdot \\frac{1}{2}$. The result is validated.", "answer": "$$\\boxed{\\frac{1}{2}\\exp(2)}$$", "id": "2905080"}, {"introduction": "A key insight in systems theory is that a state-space model can contain internal dynamics that are \"hidden\" from the input-output perspective. This practice explores the concept of non-minimal realizations and the effect of similarity transformations on them [@problem_id:2905021]. By computing the transfer function of a given system, you will directly observe how internal modes can be uncontrollable or unobservable, and confirm that these properties—and the resulting pole-zero cancellations—are invariant under any change of state coordinates.", "problem": "In a single-input single-output linear time-invariant state-space model, the transfer function between input and output is defined by $G(s) = C\\,(sI - A)^{-1} B + D$, where $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times 1}$, $C \\in \\mathbb{R}^{1 \\times n}$, and $D \\in \\mathbb{R}$. A similarity transformation is any change of coordinates of the form $x = T \\,\\bar{x}$, where $T \\in \\mathbb{R}^{n \\times n}$ is invertible, yielding the transformed realization $(\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}) = (T^{-1}AT, T^{-1}B, CT, D)$. A realization is called minimal if and only if it is both controllable and observable, which is equivalent to the transfer function having McMillan degree equal to the state dimension.\n\nConsider the realization with\n$A = \\mathrm{diag}(-1,-2,-3)$, $B = \\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}$, $C = \\begin{pmatrix}1  0  0\\end{pmatrix}$, $D = 0$.\nUsing only the definition of the transfer function and the definition of similarity transformation, and reasoning from properties of controllability and observability, determine which of the following statements are correct.\n\nA. The transfer function of the given realization is $G(s) = \\dfrac{1}{s+1} + \\dfrac{1}{s+3}$, so there is no internal cancellation.\n\nB. For any invertible $T$, the transfer function of $(T^{-1}AT, T^{-1}B, CT, D)$ equals $G(s)$; therefore any pole-zero cancellations present in $G(s)$ for the given realization cannot be made to disappear by similarity, nor can canceled poles be made to reappear.\n\nC. There exists an invertible $T$ such that the controllability rank increases and the transformed realization yields a third-order transfer function, thereby eliminating internal cancellation.\n\nD. There exists an invertible $T$ that puts the realization into a decomposition that separates a minimal controllable-and-observable part from a complementary uncontrollable and/or unobservable part, and the transfer function equals that of the minimal part only.\n\nE. For any realization, the McMillan degree of $G(s)$ equals the dimension of $A$, hence the transfer function associated with the above $A$ must be third order.\n\nSelect all that apply. Your reasoning must rely on the stated definitions and well-established invariance properties of similarity transformations, and you should use the given realization to illustrate where internal cancellation occurs.", "solution": "We begin from the fundamental definition of the transfer function:\n$$\nG(s) = C\\,(sI - A)^{-1} B + D.\n$$\nA similarity transformation with invertible $T$ maps $(A,B,C,D)$ to $(\\bar{A},\\bar{B},\\bar{C},\\bar{D}) = (T^{-1}AT, T^{-1}B, CT, D)$. Using the identity\n$$\n(sI - T^{-1}AT)^{-1} = T\\,(sI - A)^{-1} T^{-1},\n$$\nwhich follows from multiplying both sides by $(sI - T^{-1}AT)$ and using $TT^{-1}=I$, we compute the transformed transfer function:\n$$\n\\bar{G}(s) = \\bar{C}\\,(sI - \\bar{A})^{-1} \\bar{B} + \\bar{D}\n= (CT) \\bigl( T^{-1} (sI - A)^{-1} T \\bigr) (T^{-1} B) + D\n= C\\,(sI - A)^{-1} B + D = G(s).\n$$\nThus, the transfer function is invariant under similarity.\n\nNext, we compute the transfer function for the given realization to identify internal cancellation. Since $A$ is diagonal, we have\n$$\n(sI - A)^{-1} = \\mathrm{diag}\\Bigl(\\frac{1}{s+1},\\,\\frac{1}{s+2},\\,\\frac{1}{s+3}\\Bigr).\n$$\nTherefore,\n$$\nG(s) = C\\,(sI - A)^{-1} B\n= \\begin{pmatrix}1  0  0\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{s+1}  0  0 \\\\\n0  \\frac{1}{s+2}  0 \\\\\n0  0  \\frac{1}{s+3}\n\\end{pmatrix}\n\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\n= \\begin{pmatrix}1  0  0\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{s+1} \\\\\n0 \\\\\n\\frac{1}{s+3}\n\\end{pmatrix}\n= \\frac{1}{s+1}.\n$$\nHence the transfer function has a single pole at $s=-1$. The nominal state dimension is $3$, but the McMillan degree of $G(s)$ is $1$, which indicates internal cancellation. Specifically, the mode at $s=-2$ is unreachable because the second entry of $B$ is $0$, and the mode at $s=-3$ is unobservable because the third entry of $C$ is $0$. These internal modes do not appear as poles of $G(s)$.\n\nWe also recall fundamental invariance properties under similarity:\n- The rank of the controllability matrix $[\\,B\\; AB\\; A^{2}B\\;\\cdots\\; A^{n-1}B\\,]$ is invariant under similarity because replacing $(A,B)$ by $(T^{-1}AT,T^{-1}B)$ premultiplies each block column $A^{k}B$ by $T^{-1}$, which does not change rank.\n- The rank of the observability matrix $\\begin{pmatrix} C^{\\top}  (CA)^{\\top}  \\cdots  (CA^{n-1})^{\\top} \\end{pmatrix}^{\\top}$ is likewise invariant because replacing $(A,C)$ by $(T^{-1}AT,CT)$ postmultiplies $CA^{k}$ by $T$, which does not change rank.\n\nTherefore, similarity transformations cannot alter controllability or observability, cannot change minimality, and cannot change the McMillan degree or the pole-zero cancellation structure in the transfer function. What similarity can do is to reveal the structure via a decomposition, such as the Kalman decomposition, that separates controllable/observable dynamics from uncontrollable and/or unobservable dynamics. In such a form, the transfer function coincides with that of the controllable-and-observable block.\n\nNow, we evaluate each option.\n\nOption A: It claims $G(s) = \\dfrac{1}{s+1} + \\dfrac{1}{s+3}$. Our direct computation gives $G(s) = \\dfrac{1}{s+1}$. The calculation for this option would require either B or C to have a 1 in the third position, which they don't. Hence this statement is Incorrect.\n\nOption B: We proved that $\\bar{G}(s) = G(s)$ for any invertible $T$. Thus, any pole-zero cancellations present in $G(s)$ remain; one cannot make canceled poles reappear or disappear by similarity. This statement is Correct.\n\nOption C: This asserts that some $T$ can increase controllability rank and yield a third-order transfer function. Controllability and observability ranks are invariant under similarity, and so is the McMillan degree. Therefore, no similarity can change the order of $G(s)$ from $1$ to $3$. This statement is Incorrect.\n\nOption D: There exists a similarity transformation that transforms the realization into a decomposition separating the controllable-and-observable part from the rest (the Kalman decomposition). In that form, the transfer function equals that of the controllable-and-observable block. This is a well-established result and aligns with the example, in which two modes are internally canceled. This statement is Correct.\n\nOption E: It claims that the McMillan degree always equals the state dimension. That is only true for minimal realizations. Our example has state dimension $3$ and McMillan degree $1$. Hence this statement is Incorrect.\n\nTherefore, the correct options are B and D.", "answer": "$$\\boxed{BD}$$", "id": "2905021"}]}