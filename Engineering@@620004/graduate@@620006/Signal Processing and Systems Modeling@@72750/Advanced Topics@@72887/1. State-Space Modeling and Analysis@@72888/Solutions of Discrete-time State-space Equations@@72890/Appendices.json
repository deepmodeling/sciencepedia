{"hands_on_practices": [{"introduction": "Understanding the long-term behavior of a discrete-time system starts with mastering its response to an initial condition, which involves computing powers of the state transition matrix $A$. This first exercise [@problem_id:2905338] guides you through the most fundamental and elegant method for this task: eigendecomposition. By transforming the system into a basis of eigenvectors, you will see how the complex dynamics decouple into simple, independent scalar modes, making the calculation of $A^k x_0$ a straightforward process.", "problem": "Consider a discrete-time linear time-invariant (LTI) state-space model defined by the recursion $x[k+1] = A x[k]$ with the initial condition $x[0] = b$ and a scalar output $y[k] = c^{\\top} x[k]$. Assume that the state transition matrix $A$ has three distinct eigenvalues and is given in factored form $A = V \\Lambda V^{-1}$, where\n$$\nV = \\begin{pmatrix}\n1  1  1 \\\\\n0  1  2 \\\\\n1  0  1\n\\end{pmatrix}, \\quad\n\\Lambda = \\operatorname{diag}\\!\\left(2,\\, -1,\\, \\frac{1}{2}\\right), \\quad\nb = \\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix}, \\quad\nc = \\begin{pmatrix}1 \\\\ -1 \\\\ 2\\end{pmatrix}.\n$$\nStarting from first principles for solving linear time-invariant discrete-time state recursions and without invoking any pre-stated closed-form expressions for $A^{k}$, derive a closed-form expression for $y[k]$ as an explicit function of the nonnegative integer $k$. Your derivation should proceed from the definition of the solution to the state recursion and use only the provided system description. Express the final answer as a single closed-form analytic expression in terms of $k$. No numerical rounding is required.", "solution": "The problem requires the derivation of a closed-form expression for the output $y[k]$ of a discrete-time linear time-invariant system. The derivation must proceed from first principles.\n\nThe system is described by the state-space equations:\n$$x[k+1] = A x[k]$$\n$$y[k] = c^{\\top} x[k]$$\nwith the initial state $x[0] = b$.\n\nFirst, we establish the general solution for the state vector $x[k]$. The state equation is a first-order homogeneous linear recurrence relation. We can find its solution by direct iteration starting from the initial condition $x[0]$:\nFor $k=1$, we have $x[1] = A x[0]$.\nFor $k=2$, we have $x[2] = A x[1] = A (A x[0]) = A^2 x[0]$.\nFor $k=3$, we have $x[3] = A x[2] = A (A^2 x[0]) = A^3 x[0]$.\nBy mathematical induction, it is straightforward to demonstrate that the state vector at any time step $k \\ge 0$ is given by:\n$$x[k] = A^k x[0]$$\nGiven the initial condition $x[0] = b$, the state vector is $x[k] = A^k b$.\n\nThe output $y[k]$ is then given by substituting the expression for $x[k]$ into the output equation:\n$$y[k] = c^{\\top} x[k] = c^{\\top} A^k b$$\n\nThe central task is to compute the term $A^k$. We are given the eigendecomposition of the matrix $A$ as $A = V \\Lambda V^{-1}$. We can use this factorization to find an expression for $A^k$. Again, from first principles:\n$$A^2 = (V \\Lambda V^{-1})(V \\Lambda V^{-1}) = V \\Lambda (V^{-1}V) \\Lambda V^{-1} = V \\Lambda I \\Lambda V^{-1} = V \\Lambda^2 V^{-1}$$\nwhere $I$ is the identity matrix. Extending this by induction, we find the general expression for the $k$-th power of $A$:\n$$A^k = V \\Lambda^k V^{-1}$$\nThis is not a pre-stated formula but a direct consequence of the properties of matrix multiplication and inversion.\n\nSubstituting this into our expression for $y[k]$:\n$$y[k] = c^{\\top} (V \\Lambda^k V^{-1}) b$$\nTo compute this scalar value efficiently, we group the terms as follows:\n$$y[k] = (c^{\\top} V) \\Lambda^k (V^{-1} b)$$\nThis approach avoids the full computation of the matrix $A^k$ and instead requires only vector and matrix-vector multiplications.\n\nThe provided matrices and vectors are:\n$$V = \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\\\ 1  0  1 \\end{pmatrix}, \\quad \\Lambda = \\begin{pmatrix} 2  0  0 \\\\ 0  -1  0 \\\\ 0  0  \\frac{1}{2} \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\nWe will now compute the components of the expression for $y[k]$.\n\nStep 1: Compute the inverse of the matrix $V$.\nThe determinant of $V$ is:\n$$\\det(V) = 1(1 \\cdot 1 - 2 \\cdot 0) - 1(0 \\cdot 1 - 2 \\cdot 1) + 1(0 \\cdot 0 - 1 \\cdot 1) = 1(1) - 1(-2) + 1(-1) = 1 + 2 - 1 = 2$$\nSince $\\det(V) = 2 \\neq 0$, the matrix is invertible. The inverse is given by $V^{-1} = \\frac{1}{\\det(V)}\\operatorname{adj}(V)$, where $\\operatorname{adj}(V)$ is the adjugate matrix of $V$.\nThe matrix of cofactors is:\n$$C = \\begin{pmatrix} \\left|\\begin{matrix} 1  2 \\\\ 0  1 \\end{matrix}\\right|  -\\left|\\begin{matrix} 0  2 \\\\ 1  1 \\end{matrix}\\right|  \\left|\\begin{matrix} 0  1 \\\\ 1  0 \\end{matrix}\\right| \\\\ -\\left|\\begin{matrix} 1  1 \\\\ 0  1 \\end{matrix}\\right|  \\left|\\begin{matrix} 1  1 \\\\ 1  1 \\end{matrix}\\right|  -\\left|\\begin{matrix} 1  1 \\\\ 1  0 \\end{matrix}\\right| \\\\ \\left|\\begin{matrix} 1  1 \\\\ 1  2 \\end{matrix}\\right|  -\\left|\\begin{matrix} 1  1 \\\\ 0  2 \\end{matrix}\\right|  \\left|\\begin{matrix} 1  1 \\\\ 0  1 \\end{matrix}\\right| \\end{pmatrix} = \\begin{pmatrix} 1  2  -1 \\\\ -1  0  1 \\\\ 1  -2  1 \\end{pmatrix}$$\nThe adjugate matrix is the transpose of the cofactor matrix:\n$$\\operatorname{adj}(V) = C^{\\top} = \\begin{pmatrix} 1  -1  1 \\\\ 2  0  -2 \\\\ -1  1  1 \\end{pmatrix}$$\nThus, the inverse is:\n$$V^{-1} = \\frac{1}{2}\\begin{pmatrix} 1  -1  1 \\\\ 2  0  -2 \\\\ -1  1  1 \\end{pmatrix}$$\n\nStep 2: Compute the vector $V^{-1}b$.\nThis vector represents the initial condition $b$ in the basis of eigenvectors of $A$.\n$$V^{-1}b = \\frac{1}{2}\\begin{pmatrix} 1  -1  1 \\\\ 2  0  -2 \\\\ -1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 1(1) + (-1)(0) + 1(-1) \\\\ 2(1) + 0(0) + (-2)(-1) \\\\ -1(1) + 1(0) + 1(-1) \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\n\nStep 3: Compute the vector $c^{\\top}V$.\nThis vector determines how the system modes contribute to the output.\n$$c^{\\top}V = \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\\\ 1  0  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+(-1)(0)+2(1)  1(1)+(-1)(1)+2(0)  1(1)+(-1)(2)+2(1) \\end{pmatrix}$$\n$$c^{\\top}V = \\begin{pmatrix} 1+0+2  1-1+0  1-2+2 \\end{pmatrix} = \\begin{pmatrix} 3  0  1 \\end{pmatrix}$$\n\nStep 4: Compute $\\Lambda^k$.\nSince $\\Lambda$ is a diagonal matrix, its $k$-th power is found by raising its diagonal elements to the $k$-th power.\n$$\\Lambda^k = \\begin{pmatrix} 2^k  0  0 \\\\ 0  (-1)^k  0 \\\\ 0  0  (\\frac{1}{2})^k \\end{pmatrix} = \\begin{pmatrix} 2^k  0  0 \\\\ 0  (-1)^k  0 \\\\ 0  0  2^{-k} \\end{pmatrix}$$\n\nStep 5: Assemble the final expression for $y[k]$.\nNow we combine the results from the previous steps:\n$$y[k] = (c^{\\top} V) \\Lambda^k (V^{-1} b) = \\begin{pmatrix} 3  0  1 \\end{pmatrix} \\begin{pmatrix} 2^k  0  0 \\\\ 0  (-1)^k  0 \\\\ 0  0  2^{-k} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\nFirst, we multiply the row vector by the matrix:\n$$\\begin{pmatrix} 3  0  1 \\end{pmatrix} \\begin{pmatrix} 2^k  0  0 \\\\ 0  (-1)^k  0 \\\\ 0  0  2^{-k} \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 2^k  0 \\cdot (-1)^k  1 \\cdot 2^{-k} \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 2^k  0  2^{-k} \\end{pmatrix}$$\nThen, we complete the product with the column vector:\n$$y[k] = \\begin{pmatrix} 3 \\cdot 2^k  0  2^{-k} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\end{pmatrix} = (3 \\cdot 2^k)(0) + (0)(2) + (2^{-k})(-1)$$\n$$y[k] = 0 + 0 - 2^{-k} = -2^{-k}$$\nThis represents the modal expansion of the output. The initial condition has no component in the direction of the eigenvector corresponding to the eigenvalue $\\lambda_1 = 2$. The output vector $c$ is orthogonal to the eigenvector corresponding to the eigenvalue $\\lambda_2 = -1$ in the transformed coordinate system. Consequently, only the mode associated with the eigenvalue $\\lambda_3 = 1/2$ is excited and observed in the output. The final closed-form expression for the output is $y[k] = -2^{-k}$ for any non-negative integer $k$.", "answer": "$$\\boxed{-2^{-k}}$$", "id": "2905338"}, {"introduction": "While eigendecomposition is powerful, many real-world systems are not diagonalizable, particularly those with repeated eigenvalues. This practice [@problem_id:2905370] delves into this crucial scenario by analyzing a system governed by a Jordan block. You will derive the solution from first principles and discover how the interaction between an eigenvalue and the block's nilpotent structure can lead to transient growth, where the state norm initially increases even though the system is asymptotically stable.", "problem": "Consider the discrete-time linear time-invariant state-space system defined by the recursion $x[k+1] = J x[k]$ where $x[k] \\in \\mathbb{R}^{2}$ and the state transition matrix $J \\in \\mathbb{R}^{2 \\times 2}$ is a single $2 \\times 2$ Jordan block associated with the eigenvalue $\\lambda = \\frac{9}{10}$:\n$$\nJ = \\begin{pmatrix}\n\\frac{9}{10}  1 \\\\\n0  \\frac{9}{10}\n\\end{pmatrix}.\n$$\nStarting from first principles appropriate for discrete-time state-space solutions and the structure of Jordan blocks, derive an explicit expression for $J^{k}$ for arbitrary integer $k \\geq 0$. Then, with the initial condition $x[0] = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, use your expression for $J^{k}$ to obtain a closed-form expression for the first component of the state $x[k]$ as a function of $k$. Finally, analyze the asymptotic decay rate of the state and explain the effect of the nilpotent part of $J$ on transient behavior before the asymptotic decay dominates.\n\nYour final reported answer must be a single closed-form analytic expression for the first component of $x[k]$ in terms of $k$. No numerical rounding is required.", "solution": "The solution to the discrete-time system $x[k+1] = J x[k]$ is given by $x[k] = J^k x[0]$. The primary task is to compute the matrix power $J^k$. The matrix $J$ can be decomposed into the sum of its diagonal part and its nilpotent part. This decomposition is $J = D + N$, where $D$ is the diagonal matrix containing the eigenvalues and $N$ is a strictly upper triangular matrix.\n\nFor the given Jordan block, we have:\n$$\nJ = \\lambda I + N = \\begin{pmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nwhere $\\lambda = \\frac{9}{10}$, $I$ is the $2 \\times 2$ identity matrix, and $N = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$.\n\nThe matrix $D = \\lambda I$ is a scalar matrix, which commutes with any matrix. Specifically, $DN = (\\lambda I)N = \\lambda N$ and $ND = N(\\lambda I) = \\lambda N$, so $DN=ND$. Because $D$ and $N$ commute, we can apply the binomial theorem to compute $J^k$:\n$$\nJ^k = (D+N)^k = \\sum_{i=0}^{k} \\binom{k}{i} D^{k-i} N^i\n$$\nWe must compute the powers of the nilpotent matrix $N$:\n$$\nN^0 = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nN^1 = N = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\nN^2 = N \\cdot N = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = \\mathbf{0}\n$$\nSince $N^2$ is the zero matrix, all higher powers $N^i$ for $i \\ge 2$ are also the zero matrix. This property simplifies the binomial expansion, which now truncates after the term for $i=1$:\n$$\nJ^k = \\binom{k}{0} D^k N^0 + \\binom{k}{1} D^{k-1} N^1\n$$\nThis formula is valid for $k \\ge 1$. For $k=0$, $J^0=I$. The formula gives $\\binom{0}{0} D^0 N^0 = I$. For $k=1$, it gives $\\binom{1}{0}D^1 N^0 + \\binom{1}{1}D^0 N^1 = D+N=J$. The formula is valid for all $k \\ge 0$.\nSubstituting the terms:\n- $\\binom{k}{0} = 1$\n- $\\binom{k}{1} = k$\n- $D^j = (\\lambda I)^j = \\lambda^j I$\nThus, the expression for $J^k$ becomes:\n$$\nJ^k = 1 \\cdot (\\lambda^k I) \\cdot I + k \\cdot (\\lambda^{k-1} I) \\cdot N = \\lambda^k I + k \\lambda^{k-1} N\n$$\nSubstituting the matrix forms for $I$ and $N$:\n$$\nJ^k = \\lambda^k \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + k \\lambda^{k-1} \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} \\lambda^k  0 \\\\ 0  \\lambda^k \\end{pmatrix} + \\begin{pmatrix} 0  k \\lambda^{k-1} \\\\ 0  0 \\end{pmatrix}\n$$\nThis results in the explicit expression for $J^k$:\n$$\nJ^k = \\begin{pmatrix} \\lambda^k  k \\lambda^{k-1} \\\\ 0  \\lambda^k \\end{pmatrix}\n$$\nNow, we use this result to find the state vector $x[k] = J^k x[0]$ with the initial condition $x[0] = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$:\n$$\nx[k] = \\begin{pmatrix} \\lambda^k  k \\lambda^{k-1} \\\\ 0  \\lambda^k \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\lambda^k \\cdot 1 + k \\lambda^{k-1} \\cdot 1 \\\\ 0 \\cdot 1 + \\lambda^k \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} \\lambda^k + k \\lambda^{k-1} \\\\ \\lambda^k \\end{pmatrix}\n$$\nThe problem requires the closed-form expression for the first component of $x[k]$, which is $x_1[k]$.\n$$\nx_1[k] = \\lambda^k + k \\lambda^{k-1}\n$$\nSubstituting the value $\\lambda = \\frac{9}{10}$:\n$$\nx_1[k] = \\left(\\frac{9}{10}\\right)^k + k \\left(\\frac{9}{10}\\right)^{k-1}\n$$\nThis expression can be factored for a more compact representation:\n$$\nx_1[k] = \\left(\\frac{9}{10}\\right)^{k-1} \\left(\\frac{9}{10} + k\\right) = \\left(\\frac{9}{10}\\right)^k \\left(1 + \\frac{k}{9/10}\\right) = \\left(1 + \\frac{10}{9}k\\right) \\left(\\frac{9}{10}\\right)^k\n$$\nThis is the required analytical expression.\n\nFinally, we analyze the system's behavior. The state vector is $x[k] = \\begin{pmatrix} (\\lambda^k + k \\lambda^{k-1}) \\\\ \\lambda^k \\end{pmatrix}$. As $k \\to \\infty$, both components of $x[k]$ approach $0$ because $|\\lambda| = \\frac{9}{10}  1$, and exponential decay dominates polynomial growth. The asymptotic decay rate of the system is determined by the magnitude of the eigenvalue, $\\lambda$. Specifically, $\\lim_{k\\to\\infty} \\frac{\\|x[k+1]\\|}{\\|x[k]\\|} = |\\lambda| = \\frac{9}{10}$.\n\nThe effect of the nilpotent part $N$ is captured by the term $k \\lambda^{k-1} N x[0]$ in the solution $x[k] = (\\lambda^k I + k \\lambda^{k-1}N)x[0]$. If the system were diagonalizable ($N = \\mathbf{0}$), the state would be $x[k] = \\lambda^k x[0]$, and its magnitude would decrease monotonically for all $k \\ge 0$. However, the presence of the term $k \\lambda^{k-1}$ introduces a transient behavior characterized by initial growth. The function $f(k) = k \\lambda^{k-1}$ does not decay monotonically from $k=0$. It initially increases, reaches a maximum (for continuous $k$, near $k = -1/\\ln(\\lambda) \\approx 9.49$), and then decays. This means the state vector's magnitude first grows, exhibiting a transient \"hump\", before the asymptotic exponential decay, governed by $\\lambda^k$, becomes the dominant behavior. This transient growth is a direct consequence of the non-diagonalizable nature of the system, encapsulated by the nilpotent component of the Jordan block.", "answer": "$$\n\\boxed{\\left(1 + \\frac{10}{9}k\\right)\\left(\\frac{9}{10}\\right)^k}\n$$", "id": "2905370"}, {"introduction": "The phenomenon of transient growth, first seen with a Jordan block, is a general feature of non-normal systems where $A A^* \\neq A^* A$. This advanced practice [@problem_id:2905357] challenges you to build such a system from the ground up and rigorously quantify its behavior. By deriving a closed-form expression for the matrix norm $\\|A^k\\|_2$ over time, you will not only witness the transient amplification but also use calculus to pinpoint the time at which this growth reaches its peak, providing a powerful link between a matrix's structure and its dynamic response.", "problem": "Consider a discrete-time, linear time-invariant state-space model $x[k+1]=A x[k]$ with a constant state-transition matrix $A \\in \\mathbb{R}^{2\\times 2}$. You are told only that the spectral radius $\\rho(A)$ is strictly less than $1$, yet the system exhibits a large transient amplification in the induced Euclidean operator norm $\\|A^{k}\\|_{2}$ for some finite time index $k$. \n\nYour tasks are:\n\n1) Construct an explicit $2\\times 2$ non-normal matrix $A$ with $\\rho(A)1$ that produces large transient amplification in $\\|A^{k}\\|_{2}$ for some $k$. The construction must be based on first principles: start from the definition of the discrete-time solution $x[k]=A^{k}x[0]$ and the definition of the induced Euclidean operator norm $\\|A\\|_{2}$ as the square root of the largest eigenvalue of $A^{\\top}A$. Do not invoke any pre-packaged “black-box” transient growth results.\n\n2) Derive a closed-form expression for $A^{k}$ using only algebraic identities that follow from the defining properties of your $A$.\n\n3) Using your closed-form for $A^{k}$ and the definition of the induced Euclidean operator norm, obtain an exact expression for $\\|A^{k}\\|_{2}$ as a function of $k$ and your construction parameters.\n\n4) Treating the time index $k$ as a real variable and assuming a regime of parameters in which the transient contribution dominates the asymptotic decay (you must make this regime precise from your exact norm formula), determine analytically the real-valued time index $k_{\\star}$ at which the peak transient amplification occurs. Your derivation must start from the exact norm formula you obtained in step 3 and use only standard calculus and algebra. \n\nProvide your final answer as a single closed-form analytic expression for $k_{\\star}$ in terms of a single stability parameter $r \\in (0,1)$, where $r$ is the magnitude of the eigenvalue(s) of your $A$. No numerical rounding is required. Do not include any units. The final answer must be a single expression.", "solution": "The solution is developed in four parts as stipulated.\n\n1) Construction of the matrix $A$.\nTransient amplification in a stable system is a hallmark of non-normal matrices, for which $\\|A^k\\|_2$ can grow significantly before the asymptotic decay dictated by $\\rho(A)$ takes over. A canonical example of a non-normal matrix is an upper-triangular Jordan block. We construct our matrix $A$ based on this structure to facilitate large transient effects through a tunable off-diagonal element.\nLet the matrix $A$ be defined as:\n$$ A = \\begin{pmatrix} r  \\alpha \\\\ 0  r \\end{pmatrix} $$\nwhere $r \\in (0,1)$ is a real parameter representing the eigenvalue magnitude and $\\alpha > 0$ is a real parameter controlling the degree of non-normality.\nThis matrix has the following properties:\n- The eigenvalues of $A$ are its diagonal entries, $\\lambda_1 = \\lambda_2 = r$. Therefore, the spectral radius is $\\rho(A) = |r| = r$. Since $r \\in (0,1)$, the condition $\\rho(A)  1$ is satisfied, ensuring asymptotic stability.\n- The matrix $A$ is non-normal if it does not commute with its conjugate transpose, $A^{\\top}$.\n$A A^{\\top} = \\begin{pmatrix} r  \\alpha \\\\ 0  r \\end{pmatrix} \\begin{pmatrix} r  0 \\\\ \\alpha  r \\end{pmatrix} = \\begin{pmatrix} r^2 + \\alpha^2  \\alpha r \\\\ \\alpha r  r^2 \\end{pmatrix}$\n$A^{\\top} A = \\begin{pmatrix} r  0 \\\\ \\alpha  r \\end{pmatrix} \\begin{pmatrix} r  \\alpha \\\\ 0  r \\end{pmatrix} = \\begin{pmatrix} r^2  \\alpha r \\\\ \\alpha r  \\alpha^2 + r^2 \\end{pmatrix}$\nFor $\\alpha  0$, $A A^{\\top} \\neq A^{\\top} A$, hence $A$ is non-normal.\n- \"Large transient amplification\" can be achieved by choosing a large value for $\\alpha$, which increases the initial norm $\\|A\\|_2$ far above $\\rho(A)$. This construction is based on the fundamental principle of using non-normality to generate transient growth.\n\n2) Closed-form expression for $A^k$.\nTo find the $k$-th power of $A$, we decompose $A$ into a sum of a scalar matrix and a nilpotent matrix:\n$$ A = rI + N, \\quad \\text{where} \\quad I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\quad \\text{and} \\quad N = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} $$\nThe matrices $rI$ and $N$ commute. We can thus apply the binomial theorem:\n$$ A^k = (rI + N)^k = \\sum_{j=0}^{k} \\binom{k}{j} (rI)^{k-j} N^j = \\sum_{j=0}^{k} \\binom{k}{j} r^{k-j} N^j $$\nWe compute the powers of $N$:\n$N^1 = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix}$\n$N^2 = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = O$\nAll higher powers $N^j$ for $j \\ge 2$ are the zero matrix. The binomial expansion thus truncates:\n$$ A^k = \\binom{k}{0} r^k N^0 + \\binom{k}{1} r^{k-1} N^1 $$\nUsing $N^0 = I$:\n$$ A^k = (1) r^k I + k r^{k-1} N = r^k \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + k r^{k-1} \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} $$\nThis yields the closed-form expression for $A^k$:\n$$ A^k = \\begin{pmatrix} r^k  k r^{k-1} \\alpha \\\\ 0  r^k \\end{pmatrix} $$\n\n3) Exact expression for $\\|A^k\\|_2$.\nThe induced Euclidean norm $\\|A^k\\|_2$ is defined as the square root of the largest eigenvalue of the matrix $(A^k)^{\\top}A^k$.\nFirst, we compute $(A^k)^{\\top}A^k$:\n$$ (A^k)^{\\top}A^k = \\begin{pmatrix} r^k  0 \\\\ k r^{k-1} \\alpha  r^k \\end{pmatrix} \\begin{pmatrix} r^k  k r^{k-1} \\alpha \\\\ 0  r^k \\end{pmatrix} = \\begin{pmatrix} r^{2k}  k r^{2k-1} \\alpha \\\\ k r^{2k-1} \\alpha  k^2 r^{2k-2} \\alpha^2 + r^{2k} \\end{pmatrix} $$\nNext, we find the eigenvalues $\\mu$ of this matrix by solving the characteristic equation $\\det((A^k)^{\\top}A^k - \\mu I) = 0$:\n$$ (r^{2k} - \\mu)(k^2 r^{2k-2} \\alpha^2 + r^{2k} - \\mu) - (k r^{2k-1} \\alpha)^2 = 0 $$\n$$ \\mu^2 - \\mu(2r^{2k} + k^2 r^{2k-2} \\alpha^2) + r^{2k}(k^2 r^{2k-2} \\alpha^2 + r^{2k}) - k^2 r^{4k-2} \\alpha^2 = 0 $$\n$$ \\mu^2 - \\mu(2r^{2k} + k^2 r^{2k-2} \\alpha^2) + r^{4k} = 0 $$\nThe solutions for $\\mu$ from the quadratic formula are:\n$$ \\mu = \\frac{(2r^{2k} + k^2 r^{2k-2} \\alpha^2) \\pm \\sqrt{(2r^{2k} + k^2 r^{2k-2} \\alpha^2)^2 - 4r^{4k}}}{2} $$\nThe term under the square root simplifies to:\n$$ (2r^{2k} + k^2 r^{2k-2} \\alpha^2)^2 - 4r^{4k} = 4r^{4k} + 4k^2r^{4k-2}\\alpha^2 + k^4r^{4k-4}\\alpha^4 - 4r^{4k} = k^2 r^{4k-4} \\alpha^2 (4r^2 + k^2 \\alpha^2) $$\nThe largest eigenvalue, $\\lambda_{\\max}((A^k)^{\\top}A^k)$, corresponds to the positive root:\n$$ \\lambda_{\\max}((A^k)^{\\top}A^k) = \\frac{1}{2} \\left[ 2r^{2k} + k^2 r^{2k-2} \\alpha^2 + \\sqrt{k^2 r^{4k-4} \\alpha^2 (4r^2 + k^2 \\alpha^2)} \\right] $$\n$$ \\lambda_{\\max}((A^k)^{\\top}A^k) = \\frac{r^{2k-2}}{2} \\left[ 2r^2 + k^2 \\alpha^2 + k\\alpha \\sqrt{4r^2 + k^2 \\alpha^2} \\right] $$\nThe exact expression for the norm is the square root of this value:\n$$ \\|A^k\\|_2 = r^{k-1} \\sqrt{\\frac{1}{2} \\left( 2r^2 + k^2 \\alpha^2 + k\\alpha \\sqrt{4r^2 + k^2 \\alpha^2} \\right)} $$\n\n4) Analytical determination of the peak amplification time $k_\\star$.\nWe are asked to find the time $k_\\star$ that maximizes $\\|A^k\\|_2$, assuming a regime where the transient contribution dominates.\nThe transient behavior is driven by the non-normality parameter $\\alpha$. The regime of dominant transient contribution is therefore one where terms involving $\\alpha$ are much larger than those without. This is precisely the case when $k \\alpha \\gg r$. In this regime, the system exhibits large transient amplification.\nWe analyze the term inside the square root from the expression for $\\|A^k\\|_2^2$:\n$$ 2r^2 + k^2 \\alpha^2 + k\\alpha \\sqrt{4r^2 + k^2 \\alpha^2} = 2r^2 + k^2 \\alpha^2 + k\\alpha \\left( k\\alpha \\sqrt{1 + \\frac{4r^2}{k^2\\alpha^2}} \\right) $$\nUnder the condition $k\\alpha \\gg r$, we can use the Taylor expansion $\\sqrt{1+x} \\approx 1 + x/2$ for small $x$:\n$$ \\approx 2r^2 + k^2 \\alpha^2 + k^2\\alpha^2 \\left( 1 + \\frac{2r^2}{k^2\\alpha^2} \\right) = 2r^2 + k^2\\alpha^2 + k^2\\alpha^2 + 2r^2 = 2k^2\\alpha^2 + 4r^2 $$\nThe condition $k\\alpha \\gg r$ implies $k^2\\alpha^2 \\gg r^2$, so we can further approximate this as $2k^2\\alpha^2$.\nSubstituting this back into the expression for the squared norm:\n$$ \\|A^k\\|_2^2 \\approx r^{2k-2} \\left( \\frac{1}{2} \\left( 2k^2\\alpha^2 \\right) \\right) = r^{2k-2} k^2 \\alpha^2 $$\nLet this approximate function be $f(k) = k^2 \\alpha^2 r^{2k-2}$. To find the value of $k$ that maximizes $f(k)$, we can equivalently maximize its natural logarithm, treating $k$ as a continuous real variable:\n$$ g(k) = \\ln(f(k)) = \\ln(k^2 \\alpha^2 r^{2k-2}) = 2\\ln k + \\ln(\\alpha^2) + (2k-2)\\ln r $$\nWe find the critical point by setting the derivative with respect to $k$ to zero:\n$$ \\frac{dg}{dk} = \\frac{2}{k} + 2\\ln r = 0 $$\nSolving for $k$ gives the time of peak amplification, $k_\\star$:\n$$ \\frac{2}{k} = -2\\ln r \\implies k_\\star = -\\frac{1}{\\ln r} $$\nThis result for $k_\\star$ is independent of the non-normality parameter $\\alpha$, as required by the problem statement, and depends only on the stability parameter $r$. The condition for this approximation to be valid at the peak is $k_\\star\\alpha \\gg r$, or $-\\alpha/(r\\ln r) \\gg 1$. This specifies the required degree of non-normality for a given stability margin.", "answer": "$$ \\boxed{-\\frac{1}{\\ln(r)}} $$", "id": "2905357"}]}