{"hands_on_practices": [{"introduction": "To build a strong foundation in the Alternating Direction Method of Multipliers (ADMM), we begin with its core mechanical step: the variable update. This first exercise focuses on the $x$-minimization step for a case where the objective function, $f(x)$, is a simple quadratic. By working through this derivation [@problem_id:2153727], you will gain hands-on experience with minimizing the augmented Lagrangian and see how a seemingly complex optimization step can yield a clean, closed-form solution, making the abstract algorithm much more concrete.", "problem": "The Alternating Direction Method of Multipliers (ADMM) is an algorithm that solves optimization problems of the form:\n$$ \\min_{x, z} f(x) + g(z) $$\n$$ \\text{subject to } Ax + Bz = b $$\nwhere variables are $x \\in \\mathbb{R}^n$ and $z \\in \\mathbb{R}^m$, and the problem data are given by matrices $A \\in \\mathbb{R}^{p \\times n}$, $B \\in \\mathbb{R}^{p \\times m}$, a vector $b \\in \\mathbb{R}^p$, and convex functions $f: \\mathbb{R}^n \\to \\mathbb{R}$ and $g: \\mathbb{R}^m \\to \\mathbb{R}$.\n\nThe algorithm is based on the augmented Lagrangian:\n$$ L_\\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - b) + \\frac{\\rho}{2}\\|Ax + Bz - b\\|_2^2 $$\nwhere $y \\in \\mathbb{R}^p$ is the dual variable (or Lagrange multiplier) and $\\rho > 0$ is a penalty parameter. At each iteration $k$, ADMM performs the following updates sequentially:\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, y^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, y^k)$\n3.  $y^{k+1} := y^k + \\rho(Ax^{k+1} + Bz^{k+1} - b)$\n\nConsider a specific instance of this problem where the function $f(x)$ is defined as a quadratic function:\n$$ f(x) = \\frac{1}{2}\\|x - c\\|_2^2 $$\nfor a given constant vector $c \\in \\mathbb{R}^n$.\n\nDerive a closed-form analytical expression for the $x$-update step, $x^{k+1}$. Your expression should be in terms of the problem data $A, B, b, c$, the penalty parameter $\\rho$, and the values from the previous iteration, $z^k$ and $y^k$. In your derivation, let $I$ denote the $n \\times n$ identity matrix and assume the matrix $(I + \\rho A^T A)$ is invertible.", "solution": "We derive the $x$-update by minimizing the augmented Lagrangian with respect to $x$ while holding $z^{k}$ and $y^{k}$ fixed. The $x$-subproblem is\n$$\nx^{k+1} := \\arg\\min_{x}\\left\\{\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(Ax + B z^{k} - b) + \\frac{\\rho}{2}\\|Ax + B z^{k} - b\\|_{2}^{2}\\right\\}.\n$$\nDefine $d := B z^{k} - b$, so the objective becomes\n$$\n\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(A x + d) + \\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}.\n$$\nTerms independent of $x$ do not affect the minimizer, so we focus on the $x$-dependent part. Taking the gradient with respect to $x$ and setting it to zero gives\n$$\n\\nabla_{x}\\left(\\frac{1}{2}\\|x-c\\|_{2}^{2}\\right) + \\nabla_{x}\\left((y^{k})^{T}A x\\right) + \\nabla_{x}\\left(\\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}\\right) = 0,\n$$\nwhich simplifies to\n$$\n(x - c) + A^{T} y^{k} + \\rho A^{T}(A x + d) = 0.\n$$\nCollecting the terms in $x$ yields\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T} d.\n$$\nSubstituting $d = B z^{k} - b$, we have\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b).\n$$\nUnder the assumption that $\\left(I + \\rho A^{T}A\\right)$ is invertible, the unique minimizer is\n$$\nx^{k+1} = \\left(I + \\rho A^{T}A\\right)^{-1}\\left(c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b)\\right).\n$$", "answer": "$$\\boxed{\\left(I+\\rho A^{T}A\\right)^{-1}\\left(c-A^{T}y^{k}-\\rho A^{T}\\left(Bz^{k}-b\\right)\\right)}$$", "id": "2153727"}, {"introduction": "Many practical optimization problems involve hard constraints, such as requiring a signal to be non-negative or to lie within a specific feasible region. This practice problem [@problem_id:2852062] demonstrates how ADMM elegantly handles such scenarios by incorporating constraints through indicator functions. You will derive the update for the variable $z$ when its corresponding function $g(z)$ enforces membership in a convex set $C$, revealing a beautiful and fundamental connection between the ADMM update and the geometric operation of Euclidean projection.", "problem": "Consider a discrete-time signal reconstruction problem in which the estimate $x \\in \\mathbb{R}^{n}$ must satisfy known convex constraints modeling prior knowledge about feasible signals. Formulate the reconstruction as the splitting problem\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\nwhere $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ is a proper, closed, convex function encoding the data fidelity and regularization of the signal model, and $g:\\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ is the indicator of a nonempty, closed, convex set $C \\subset \\mathbb{R}^{n}$, defined by\n$$\ng(z) = \\begin{cases}\n0, & \\text{if } z \\in C,\\\\\n+\\infty, & \\text{if } z \\notin C.\n\\end{cases}\n$$\nThe Alternating Direction Method of Multipliers (ADMM) in its scaled form is to be applied to this problem. Let $\\rho > 0$ be the penalty parameter and $u \\in \\mathbb{R}^{n}$ denote the scaled dual variable. The scaled augmented Lagrangian is\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}.\n$$\nThe Euclidean projection of a point $v \\in \\mathbb{R}^{n}$ onto $C$ is defined by\n$$\n\\Pi_{C}(v) := \\arg\\min_{z \\in C} \\;\\|z - v\\|_{2}.\n$$\nStarting from these definitions alone, derive the closed-form expression for the $z$-update in the ADMM iteration,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\nExpress your final result explicitly as the Euclidean projection of an affine argument onto $C$. Your final answer must be a single symbolic expression containing only $\\Pi_{C}$, $x^{k+1}$, and $u^{k}$. Do not include any equality sign in the final answer. No numerical approximation is required, and no units are involved.", "solution": "We begin from the problem structure and the scaled augmented Lagrangian. The problem is\n$$\n\\min_{x,z} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\nwith $g$ as the indicator of a nonempty, closed, convex set $C \\subset \\mathbb{R}^{n}$. In the scaled Alternating Direction Method of Multipliers (ADMM), one alternately minimizes the scaled augmented Lagrangian\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}\n$$\nwith respect to $x$ and $z$, and then updates $u$. We focus on the $z$-update, which for fixed $x^{k+1}$ and $u^{k}$ is given by\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\nBy the definition of the indicator function $g$, minimizing $g(z)$ plus any other function over $z \\in \\mathbb{R}^{n}$ is equivalent to minimizing the other function subject to $z \\in C$. Therefore, the $z$-update reduces to the constrained quadratic minimization\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nSince $\\rho > 0$ is a positive constant, it does not change the location of the minimizer. Thus, equivalently,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nWe rewrite the squared norm to exhibit it as a projection objective:\n$$\n\\|x^{k+1} - z + u^{k}\\|_{2}^{2} = \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nTherefore,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nBy the definition of Euclidean projection onto a nonempty, closed, convex set $C$, the minimizer is the projection of the point $x^{k+1} + u^{k}$ onto $C$. In particular, since $C$ is nonempty, closed, and convex, the projection is uniquely defined, and we have\n$$\nz^{k+1} = \\Pi_{C}\\!\\big(x^{k+1} + u^{k}\\big).\n$$\nThis expresses the $z$-update in closed form as the Euclidean projection of the affine argument $x^{k+1} + u^{k}$ onto the convex set $C$, derived directly from the definitions of the indicator function, the scaled augmented Lagrangian, and Euclidean projection.", "answer": "$$\\boxed{\\Pi_{C}\\!\\left(x^{k+1} + u^{k}\\right)}$$", "id": "2852062"}, {"introduction": "A deep understanding of any algorithm includes knowing its limitations. While ADMM's convergence for two-block problems is well-established, a natural impulse is to assume it works for problems split into three or more blocks. This hands-on exercise [@problem_id:2153784] explores a famous counterexample that proves this is not the case. By deriving the linear iteration matrix for a specific three-block system, you will witness firsthand why the direct, naive extension of ADMM can fail, providing a crucial insight into the theoretical guarantees of the method.", "problem": "The Alternating Direction Method of Multipliers (ADMM) is a powerful algorithm for solving convex optimization problems. While its convergence is well-understood for problems split into two blocks, the direct extension to three or more blocks is not guaranteed to converge. This problem explores a classic counterexample that demonstrates this failure.\n\nConsider the feasibility problem of finding a triplet of scalars $(x, y, z) \\in \\mathbb{R}^3$ that satisfies the linear constraint\n$$\nAx + By + Cz = 0\n$$\nwhere $A, B, C$ are constant matrices (in this case, column vectors in $\\mathbb{R}^2$) given by:\n$$\nA = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis problem can be solved using a direct extension of ADMM by minimizing an empty objective function subject to the constraint. The algorithm iteratively updates the variables $x, y, z$ and a dual variable $\\lambda \\in \\mathbb{R}^2$ using the augmented Lagrangian:\n$$\n\\mathcal{L}_{\\rho}(x,y,z,\\lambda) = \\lambda^T(Ax+By+Cz) + \\frac{\\rho}{2} \\|Ax+By+Cz\\|^2_2\n$$\nwhere $\\rho > 0$ is a penalty parameter. The updates at iteration $k$ are performed sequentially:\n1.  $x^{k+1} = \\arg\\min_x \\mathcal{L}_{\\rho}(x, y^k, z^k, \\lambda^k)$\n2.  $y^{k+1} = \\arg\\min_y \\mathcal{L}_{\\rho}(x^{k+1}, y, z^k, \\lambda^k)$\n3.  $z^{k+1} = \\arg\\min_z \\mathcal{L}_{\\rho}(x^{k+1}, y^{k+1}, z, \\lambda^k)$\n4.  $\\lambda^{k+1} = \\lambda^k + \\rho(Ax^{k+1} + By^{k+1} + Cz^{k+1})$\n\nThese updates define a linear recurrence relation. Let us define a state vector $v^k \\in \\mathbb{R}^4$ consisting of a subset of the primal and scaled dual variables:\n$$\nv^k = \\begin{pmatrix} y^k \\\\ z^k \\\\ \\lambda_1^k/\\rho \\\\ \\lambda_2^k/\\rho \\end{pmatrix}\n$$\nThe iteration can be expressed as a linear transformation $v^{k+1} = M v^k$ for a $4 \\times 4$ matrix $M$.\n\nYour task is to derive this iteration matrix $M$ and determine the value of its entry at row 3, column 1 (i.e., $M_{31}$). Express your answer as a decimal.", "solution": "We work with the scaled dual variable $s^{k} = \\lambda^{k}/\\rho \\in \\mathbb{R}^{2}$. The augmented Lagrangian minimizations reduce to least-squares problems in each block, and the first-order optimality conditions for each subproblem give\n$$\nA^{T}\\big(s^{k} + A x + B y^{k} + C z^{k}\\big) = 0,\\quad\nB^{T}\\big(s^{k} + A x^{k+1} + B y + C z^{k}\\big) = 0,\\quad\nC^{T}\\big(s^{k} + A x^{k+1} + B y^{k+1} + C z\\big) = 0,\n$$\nwith the scaled dual update\n$$\ns^{k+1} = s^{k} + A x^{k+1} + B y^{k+1} + C z^{k+1}.\n$$\nUsing\n$$\nA = \\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad B = \\begin{pmatrix}1\\\\1\\end{pmatrix},\\quad C = \\begin{pmatrix}0\\\\1\\end{pmatrix},\n$$\nand writing $s^{k} = \\begin{pmatrix}s_{1}^{k}\\\\ s_{2}^{k}\\end{pmatrix}$, we compute each update.\n\n1) $x$-update:\n$$\nA^{T}A\\,x^{k+1} = -A^{T}\\big(s^{k} + B y^{k} + C z^{k}\\big).\n$$\nSince $A^{T}A = 1$, $A^{T}s^{k} = s_{1}^{k}$, $A^{T}B = 1$, $A^{T}C = 0$, we obtain\n$$\nx^{k+1} = -\\big(s_{1}^{k} + y^{k}\\big).\n$$\n\n2) $y$-update:\n$$\nB^{T}B\\,y^{k+1} = -B^{T}\\big(s^{k} + A x^{k+1} + C z^{k}\\big).\n$$\nSince $B^{T}B = 2$ and $B^{T}(s^{k} + A x^{k+1} + C z^{k}) = s_{1}^{k} + s_{2}^{k} + x^{k+1} + z^{k}$, we get\n$$\n2\\,y^{k+1} = -\\big(s_{1}^{k} + s_{2}^{k} + x^{k+1} + z^{k}\\big).\n$$\nSubstituting $x^{k+1} = - s_{1}^{k} - y^{k}$ yields\n$$\n2\\,y^{k+1} = -\\big(s_{1}^{k} + s_{2}^{k} - s_{1}^{k} - y^{k} + z^{k}\\big) = -\\big(s_{2}^{k} - y^{k} + z^{k}\\big),\n$$\nhence\n$$\ny^{k+1} = \\frac{1}{2}\\big(y^{k} - z^{k} - s_{2}^{k}\\big).\n$$\n\n3) $z$-update:\n$$\nC^{T}C\\,z^{k+1} = -C^{T}\\big(s^{k} + A x^{k+1} + B y^{k+1}\\big).\n$$\nSince $C^{T}C = 1$ and $C^{T}(s^{k} + A x^{k+1} + B y^{k+1}) = s_{2}^{k} + y^{k+1}$, we obtain\n$$\nz^{k+1} = -\\big(s_{2}^{k} + y^{k+1}\\big).\n$$\n\n4) Scaled dual update:\n$$\ns^{k+1} = s^{k} + A x^{k+1} + B y^{k+1} + C z^{k+1}.\n$$\nThus\n$$\ns_{1}^{k+1} = s_{1}^{k} + x^{k+1} + y^{k+1},\\qquad s_{2}^{k+1} = s_{2}^{k} + y^{k+1} + z^{k+1}.\n$$\nUsing $x^{k+1} = - s_{1}^{k} - y^{k}$ and $y^{k+1}$ above,\n$$\ns_{1}^{k+1} = s_{1}^{k} + \\big(- s_{1}^{k} - y^{k}\\big) + y^{k+1} = y^{k+1} - y^{k} = \\frac{1}{2}\\big(y^{k} - z^{k} - s_{2}^{k}\\big) - y^{k} = -\\frac{1}{2}y^{k} - \\frac{1}{2}z^{k} - \\frac{1}{2}s_{2}^{k}.\n$$\nTherefore, in the linear recurrence $v^{k+1} = M v^{k}$ with $v^{k} = \\begin{pmatrix}y^{k}\\\\ z^{k}\\\\ s_{1}^{k}\\\\ s_{2}^{k}\\end{pmatrix}$, the third component is\n$$\ns_{1}^{k+1} = \\left(-\\frac{1}{2}\\right) y^{k} + \\left(-\\frac{1}{2}\\right) z^{k} + 0\\cdot s_{1}^{k} + \\left(-\\frac{1}{2}\\right) s_{2}^{k}.\n$$\nHence the iteration matrix $M$ has entry\n$$\nM_{31} = -\\frac{1}{2}.\n$$\nAs a decimal, this is $-0.5$.", "answer": "$$\\boxed{-0.5}$$", "id": "2153784"}]}