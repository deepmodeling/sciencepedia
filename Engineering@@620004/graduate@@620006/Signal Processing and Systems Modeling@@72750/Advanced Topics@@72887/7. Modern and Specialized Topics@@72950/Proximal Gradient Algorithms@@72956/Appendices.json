{"hands_on_practices": [{"introduction": "To truly master proximal gradient algorithms, we must begin with their fundamental building block: a single iteration. This first practice deconstructs the algorithm into its two core components: a standard gradient descent step on the smooth part of the objective, and a proximal step on the non-smooth regularizer. By implementing this process from first principles for the canonical basis pursuit denoising (LASSO) problem, you will gain a concrete understanding of how the soft-thresholding operator naturally arises and performs its function of inducing sparsity [@problem_id:2897782].", "problem": "You are given the composite convex optimization problem in finite-dimensional real space: minimize the objective $F(x) = f(x) + g(x)$ where $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$ and $g(x) = \\lambda \\lVert x \\rVert_1$, with $x \\in \\mathbb{R}^n$, data $b \\in \\mathbb{R}^n$, and regularization parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$. Starting from the fundamental definitions of the gradient of a smooth function and of the proximal operator of a convex function, derive the single-step update mapping used by the proximal gradient method for $F(x)$, including the step-size admissibility condition based on the Lipschitz continuity of $\\nabla f(x)$. Then, implement a program that applies exactly one proximal gradient iteration to several deterministic test instances of noisy sparse denoising and quantifies the immediate effect on the estimate relative to a known sparse ground truth.\n\nYour program must:\n- Use only one proximal gradient iteration with a user-specified step size $t \\in \\mathbb{R}_{0}$ and initialization $x^{(0)} \\in \\mathbb{R}^n$, derived from the definitions (do not assume any pre-packaged form for the update; it must be explicitly derived).\n- For each test case, compute:\n  1. The mean squared error after one iteration, defined as $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$.\n  2. The support recall as a fraction, defined as $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$, where $\\mathrm{supp}(z) = \\{ i : |z_i|  \\varepsilon \\}$ with threshold $\\varepsilon = 10^{-12}$.\n  3. The number of nonzero entries in $x^{(1)}$ under the same threshold $\\varepsilon$, an integer.\n- Round the $\\mathrm{MSE}$ and $\\mathrm{recall}$ to $6$ decimal places in the output. The nonzero count must be an integer.\n- Satisfy the step-size admissibility $t \\in (0, 2/L)$, where $L$ is the Lipschitz constant of $\\nabla f$ that you must determine from first principles.\n\nTest suite:\n- Case $1$ (happy path, exact shrinkage from zero initialization):\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0, 1.5, 0, 0, -2.0, 0, 0.7, 0]$\n  - $\\mathrm{noise} = [0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- Case $2$ (boundary condition at the shrinkage threshold):\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0.4, -0.4, 0, 0, 0, 0, 0, 0]$\n  - $\\mathrm{noise} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- Case $3$ (nonzero warm start and partial step-size):\n  - $n = 6$\n  - $x_{\\mathrm{true}} = [0, 0, 1.0, 0, -1.2, 0]$\n  - $\\mathrm{noise} = [0.0, 0.05, -0.02, 0.00, 0.03, 0.10]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0.2, 0, 0.5, 0, -0.8, 0]$\n  - $\\lambda = 0.6$\n  - $t = 0.5$\n- Case $4$ (large but admissible step-size close to the stability limit):\n  - $n = 5$\n  - $x_{\\mathrm{true}} = [0, 0, 0, 2.0, 0]$\n  - $\\mathrm{noise} = [0.02, -0.01, 0.00, -0.05, 0.01]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0]$\n  - $\\lambda = 0.2$\n  - $t = 1.9$\n\nImplementation requirements:\n- Derive and implement the proximal gradient one-step update specialized to $f$ and $g$.\n- Implement the proximal operator required by your derivation directly from its definition for $g$, applied element-wise when specialized.\n- For each case, compute $x^{(1)}$ and then compute the triple $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$ where $\\mathrm{nnz}$ is the integer count of indices $i$ with $\\lvert x^{(1)}_i \\rvert  \\varepsilon$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$ for one test case, with the $\\mathrm{MSE}$ and $\\mathrm{recall}$ rounded to $6$ decimal places. For example, an output with two cases should look like $[[0.123456,0.500000,3],[0.010000,1.000000,2]]$.\n\nAngle units, physical units, and percentages are not applicable. All reported fractions must appear as decimals rounded to $6$ places, not percentages.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extracted Givens**\n- **Objective Function**: minimize $F(x) = f(x) + g(x)$ for $x \\in \\mathbb{R}^n$.\n- **Smooth Component**: $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$, where $b \\in \\mathbb{R}^n$.\n- **Non-smooth Component**: $g(x) = \\lambda \\lVert x \\rVert_1$, where $\\lambda \\in \\mathbb{R}_{\\ge 0}$.\n- **Algorithm**: One iteration of the proximal gradient method starting from $x^{(0)}$ with step size $t  0$.\n- **Step-size Condition**: $t \\in (0, 2/L)$, where $L$ is the Lipschitz constant of $\\nabla f(x)$.\n- **Metrics**:\n    1. $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$.\n    2. $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$.\n    3. $\\mathrm{supp}(z) = \\{ i : |z_i|  \\varepsilon \\}$ with $\\varepsilon = 10^{-12}$.\n    4. $\\mathrm{nnz}$: number of nonzero entries in $x^{(1)}$.\n- **Test Cases**: Four specific instances with given parameters $n$, $x_{\\mathrm{true}}$, noise, $x^{(0)}$, $\\lambda$, and $t$.\n\n**Step 2: Validation**\nThe problem is subjected to rigorous validation.\n- **Scientific Grounding**: The problem describes the LASSO formulation for sparse signal recovery, a canonical problem in signal processing and statistics. The functions $f(x)$ and $g(x)$ are convex. The proximal gradient method is the standard and appropriate algorithm for this class of composite convex optimization problems. The premises are scientifically sound.\n- **Well-Posedness**: The objective function $F(x)$ is strictly convex and coercive, which guarantees the existence of a unique minimizer. The task is to execute a single, well-defined algorithmic step and compute deterministic metrics. The problem is well-posed.\n- **Objectivity**: The language is precise and mathematical, free of ambiguity or subjective claims.\n- **Consistency**: The problem is self-contained. The Lipschitz constant $L$ of $\\nabla f(x)$ must be determined. For $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$, the gradient is $\\nabla f(x) = x-b$ and the Hessian is $\\nabla^2 f(x) = I_n$, an identity matrix. The Lipschitz constant of the gradient is the maximum eigenvalue of the Hessian, so $L=1$. The required step-size condition is $t \\in (0, 2/1) = (0, 2)$. The provided step sizes for all test cases ($t=1.0, 1.0, 0.5, 1.9$) are within this interval. The problem setup is consistent and complete.\n\n**Step 3: Verdict**\nThe problem is valid. It is a well-defined and standard exercise in optimization for signal processing. Proceeding to solution.\n\n---\n\nThe problem is to minimize the composite convex function $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\lVert x-b \\rVert_2^2$ and $g(x) = \\lambda \\lVert x \\rVert_1$. The proximal gradient method is an iterative algorithm designed for such problems. Each iteration consists of two steps: a gradient descent step on the smooth part $f(x)$, followed by a proximal step involving the non-smooth part $g(x)$.\n\nThe iterative update is derived by minimizing a quadratic approximation of $f(x)$ around the current estimate $x^{(k)}$, plus the non-smooth term $g(x)$. The Taylor expansion of $f(x)$ around $x^{(k)}$ is $f(x) \\approx f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2}(x - x^{(k)})^T \\nabla^2 f(x^{(k)}) (x - x^{(k)})$. For a general step size $t  0$, we replace the Hessian term with $\\frac{1}{t}I$, which leads to the majorization-minimization step:\n$$x^{(k+1)} = \\arg\\min_x \\left( f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2t}\\lVert x - x^{(k)} \\rVert_2^2 + g(x) \\right)$$\nThe terms $f(x^{(k)})$ and other constants with respect to $x$ can be ignored in the minimization. By completing the square, the problem is equivalent to:\n$$x^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2t}\\lVert x - (x^{(k)} - t\\nabla f(x^{(k)})) \\rVert_2^2 + g(x) \\right)$$\nThis is the definition of the proximal operator of the function $tg(x)$ applied to the point resulting from a gradient descent step on $f(x)$. The general form of the proximal gradient update is:\n$$x^{(k+1)} = \\mathrm{prox}_{tg} \\left( x^{(k)} - t\\nabla f(x^{(k)}) \\right)$$\n\nFor convergence of this method, the step size $t$ must be chosen such that $t \\in (0, 2/L)$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. First, we compute the gradient of $f(x) = \\frac{1}{2}(x-b)^T(x-b)$:\n$$\\nabla f(x) = x - b$$\nThe Lipschitz constant $L$ must satisfy $\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 \\le L \\lVert x_1 - x_2 \\rVert_2$ for all $x_1, x_2$.\n$$\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 = \\lVert (x_1 - b) - (x_2 - b) \\rVert_2 = \\lVert x_1 - x_2 \\rVert_2$$\nThus, $L=1$. The step-size condition is $t \\in (0, 2)$.\n\nNext, we must derive the proximal operator for $g(x) = \\lambda \\lVert x \\rVert_1$. The proximal operator $\\mathrm{prox}_{\\alpha g}(v)$ is defined as:\n$$\\mathrm{prox}_{\\alpha g}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\lVert x - v \\rVert_2^2 + \\alpha g(x) \\right)$$\nIn our case, the function is $tg(x) = t\\lambda\\lVert x \\rVert_1$. Let $\\gamma = t\\lambda$. The optimization becomes:\n$$\\mathrm{prox}_{\\gamma \\lVert \\cdot \\rVert_1}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\sum_{i=1}^n (x_i - v_i)^2 + \\gamma \\sum_{i=1}^n |x_i| \\right)$$\nThe objective function is separable, meaning we can solve for each component $x_i$ independently:\n$$x_i^* = \\arg\\min_{x_i} \\left( \\frac{1}{2} (x_i - v_i)^2 + \\gamma |x_i| \\right)$$\nThe first-order optimality condition from subgradient calculus requires that $0$ is in the subdifferential of the objective at $x_i^*$:\n$$0 \\in x_i^* - v_i + \\gamma \\cdot \\partial|x_i^*|$$\nwhere $\\partial|\\cdot|$ is the subdifferential of the absolute value function.\nIf $x_i^*  0$, $\\partial|x_i^*| = \\{1\\}$, so $x_i^* - v_i + \\gamma = 0 \\implies x_i^* = v_i - \\gamma$. This is valid only if $v_i - \\gamma  0$, i.e., $v_i  \\gamma$.\nIf $x_i^*  0$, $\\partial|x_i^*| = \\{-1\\}$, so $x_i^* - v_i - \\gamma = 0 \\implies x_i^* = v_i + \\gamma$. This is valid only if $v_i + \\gamma  0$, i.e., $v_i  -\\gamma$.\nIf $x_i^* = 0$, $\\partial|x_i^*| = [-1, 1]$, so $v_i \\in \\gamma[-1, 1]$, which is $|v_i| \\le \\gamma$.\nCombining these cases gives the solution, known as the soft-thresholding operator $S_\\gamma(\\cdot)$:\n$$x_i^* = S_\\gamma(v_i) = \\begin{cases} v_i - \\gamma  \\text{if } v_i  \\gamma \\\\ v_i + \\gamma  \\text{if } v_i  -\\gamma \\\\ 0  \\text{if } |v_i| \\le \\gamma \\end{cases}$$\nThis can be written compactly as $S_\\gamma(v_i) = \\mathrm{sign}(v_i) \\max(|v_i| - \\gamma, 0)$.\n\nTo perform one iteration of the proximal gradient method starting from $x^{(0)}$, we first compute the gradient descent update:\n$$z^{(0)} = x^{(0)} - t\\nabla f(x^{(0)}) = x^{(0)} - t(x^{(0)} - b) = (1-t)x^{(0)} + tb$$\nThen, we apply the proximal operator to $z^{(0)}$:\n$$x^{(1)} = \\mathrm{prox}_{t g}(z^{(0)}) = S_{t\\lambda}(z^{(0)})$$\nThe update is applied element-wise: $x_i^{(1)} = S_{t\\lambda}(z_i^{(0)})$ for $i=1, \\dots, n$. This is the single-step update mapping to be implemented.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the sparse denoising problem for multiple test cases using one\n    iteration of the proximal gradient method.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0, 1.5, 0, 0, -2.0, 0, 0.7, 0]),\n            \"noise\": np.array([0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0.4, -0.4, 0, 0, 0, 0, 0, 0]),\n            \"noise\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 6,\n            \"x_true\": np.array([0, 0, 1.0, 0, -1.2, 0]),\n            \"noise\": np.array([0.0, 0.05, -0.02, 0.00, 0.03, 0.10]),\n            \"x0\": np.array([0.2, 0, 0.5, 0, -0.8, 0]),\n            \"lambda\": 0.6,\n            \"t\": 0.5,\n        },\n        {\n            \"n\": 5,\n            \"x_true\": np.array([0, 0, 0, 2.0, 0]),\n            \"noise\": np.array([0.02, -0.01, 0.00, -0.05, 0.01]),\n            \"x0\": np.array([0, 0, 0, 0, 0]),\n            \"lambda\": 0.2,\n            \"t\": 1.9,\n        },\n    ]\n\n    results = []\n    \n    # Epsilon for support calculation\n    epsilon = 1e-12\n\n    def soft_thresholding(v, gamma):\n        \"\"\"\n        Implementation of the soft-thresholding operator S_gamma(v).\n        This is the proximal operator of the L1 norm.\n        \"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - gamma, 0)\n\n    def get_support(z, eps):\n        \"\"\"\n        Computes the support of a vector z.\n        supp(z) = { i : |z_i|  eps }\n        \"\"\"\n        return set(np.where(np.abs(z)  eps)[0])\n\n    for case in test_cases:\n        n = case[\"n\"]\n        x_true = case[\"x_true\"]\n        noise = case[\"noise\"]\n        x0 = case[\"x0\"]\n        lam = case[\"lambda\"]\n        t = case[\"t\"]\n        \n        b = x_true + noise\n\n        # Step 1: Gradient descent step on the smooth part f(x)\n        # z = x0 - t * grad_f(x0) = x0 - t*(x0 - b)\n        z = (1 - t) * x0 + t * b\n\n        # Step 2: Proximal step on the non-smooth part g(x)\n        # x1 = prox_{t*g}(z) = prox_{t*lambda*||.||_1}(z)\n        # This is the soft-thresholding operator\n        gamma = t * lam\n        x1 = soft_thresholding(z, gamma)\n\n        # Calculate metrics\n        # 1. MSE\n        mse = np.mean((x1 - x_true)**2)\n        \n        # 2. Support Recall\n        supp_x_true = get_support(x_true, epsilon)\n        supp_x1 = get_support(x1, epsilon)\n        \n        if len(supp_x_true) == 0:\n            # If true support is empty, recall is 1.0 if estimated support is also empty, 0.0 otherwise.\n            recall = 1.0 if len(supp_x1) == 0 else 0.0\n        else:\n            intersection_size = len(supp_x_true.intersection(supp_x1))\n            recall = intersection_size / len(supp_x_true)\n            \n        # 3. Number of nonzeros (nnz)\n        nnz = len(supp_x1)\n        \n        results.append([mse, recall, nnz])\n\n    # Format the final output string exactly as specified.\n    results_str_list = []\n    for res in results:\n        mse_val, recall_val, nnz_val = res\n        results_str_list.append(f\"[{mse_val:.6f},{recall_val:.6f},{nnz_val}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2897782"}, {"introduction": "An iterative algorithm is only as practical as its stopping rule. While a single proximal gradient step provides insight, a real implementation requires a principled method for termination. This exercise [@problem_id:2897755] moves from the single-step mechanism to the complete iterative process by exploring three common stopping criteria: relative objective decrease, the norm of the gradient mapping, and the duality gap. Analyzing their respective advantages and limitations will equip you with the ability to choose an appropriate convergence diagnostic for your specific application, balancing computational cost with theoretical rigor.", "problem": "You are implementing a proximal gradient method with backtracking for sparse recovery using the Least Absolute Shrinkage and Selection Operator (LASSO), that is, you solve the composite convex optimization problem \n$$\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\triangleq f(x) + g(x), \\quad f(x) \\triangleq \\tfrac{1}{2}\\|A x - y\\|_2^2, \\quad g(x) \\triangleq \\lambda \\|x\\|_1,$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$, and $\\lambda  0$. Assume $f$ is convex and continuously differentiable with a Lipschitz continuous gradient, and $g$ is convex, proper, and lower semicontinuous. You wish to choose a practical stopping criterion that is theoretically meaningful and computationally efficient. Consider criteria built on: relative objective decrease, the gradient mapping norm, and the duality gap (when it can be computed). Select all options that simultaneously give a correct and implementable definition of the criterion and correctly state an essential advantage and a limitation in the context of sparse recovery with the proximal gradient method.\n\nA. Relative objective decrease: stop at the first iterate index $k$ such that \n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}, \\quad \\varepsilon_{\\mathrm{rel}} \\in (0,1).$$ \nAdvantage: it is inexpensive to evaluate per iteration because $F(x^{k})$ and $F(x^{k+1})$ are already available under backtracking line search. Limitation: it can declare convergence prematurely when $F$ flattens due to strong regularization or ill-conditioning, even if the first-order optimality residual is not yet small.\n\nB. Gradient mapping norm: with a backtracking estimate $L_k  0$ of the Lipschitz constant of $\\nabla f$, define the gradient mapping \n$$G_{L_k}(x^{k}) \\triangleq L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}\\,A^\\top(Ax^{k} - y)\\right)\\right).$$ \nStop when \n$$\\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}} \\quad \\text{or} \\quad \\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{rel}} \\max\\{1,\\|x^{k}\\|_2\\},$$ \nwith tolerances $\\varepsilon_{\\mathrm{abs}}, \\varepsilon_{\\mathrm{rel}}  0$. Advantage: $\\|G_{L_k}(x^{k})\\|_2 = 0$ if and only if the composite first-order optimality condition $0 \\in \\nabla f(x) + \\partial g(x)$ holds, so it directly measures stationarity for the nonsmooth problem. Limitation: it depends on the local choice of $L_k$ and incurs an extra proximal evaluation unless reused from the iteration, which can slightly increase per-iteration cost.\n\nC. Duality gap: form a dual-feasible vector by rescaling the residual $\\theta^{k} \\triangleq \\tau^{k}(A x^{k} - y)$ where \n$$\\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top(Ax^{k} - y)\\|_\\infty}\\right\\}$$ \nso that $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$. Define the dual objective \n$$d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta,$$ \nwhich is valid for all $\\theta$ with $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$, and compute the duality gap \n$$\\mathrm{gap}(x^{k},\\theta^{k}) \\triangleq F(x^{k}) - d(\\theta^{k}) = \\tfrac{1}{2}\\|A x^{k} - y\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}.$$ \nStop when $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{abs}}$ or $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{rel}} F(x^{k})$. Advantage: the duality gap is a scale-aware certificate of $\\varepsilon$-optimality for the primal objective. Limitation: obtaining $\\theta^{k}$ requires computing $A^\\top(Ax^{k} - y)$ and maintaining dual feasibility, which may be unavailable or relatively costly if only a gradient oracle is provided or if $A$ is accessed implicitly.\n\nD. Gradient-norm test for the smooth surrogate: stop when $\\|\\nabla f(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}}$. Advantage: it is the classical smooth optimality test and is cheap; the nonsmooth term $g$ does not alter first-order stationarity. Limitation: none essential in this setting.\n\nE. Constrained variant shortcut: for basis pursuit $\\min_{x}\\|x\\|_1 \\;\\text{subject to}\\; A x = y$, when solved by adding a quadratic penalty and applying proximal gradient, one can stop using the surrogate “duality gap” $\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ without constructing a dual-feasible point; this quantity upper-bounds the true duality gap of the constrained problem and thus certifies suboptimality. Advantage: no dual computation. Limitation: only the penalty parameter must be chosen.", "solution": "We start from composite convex optimization with $F(x) \\triangleq f(x) + g(x)$ where $f$ is convex, differentiable, and its gradient is $L$-Lipschitz continuous for some $L  0$, and $g$ is convex, proper, and lower semicontinuous. The proximal gradient method uses the proximal operator defined by $\\operatorname{prox}_{\\alpha g}(v) \\triangleq \\arg\\min_{x}\\{g(x) + \\tfrac{1}{2\\alpha}\\|x - v\\|_2^2\\}$ with steps $\\alpha_k = 1/L_k$ to generate iterates\n$$x^{k+1} = \\operatorname{prox}_{\\alpha_k g}\\!\\left(x^{k} - \\alpha_k \\nabla f(x^{k})\\right).$$\nFor $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ and $g(x) = \\lambda\\|x\\|_1$, we have $\\nabla f(x) = A^\\top(Ax - y)$ and $\\operatorname{prox}_{\\alpha \\lambda \\|\\cdot\\|_1}$ is the soft-thresholding operator at level $\\alpha \\lambda$.\n\nA principle-based stopping design must relate to either (i) monotonic decrease of $F$, (ii) first-order optimality of the composite problem, or (iii) primal-dual optimality via strong duality. We now justify the three families.\n\nRelative objective decrease. Under backtracking line search that enforces the descent condition, we have a nonincreasing sequence $F(x^{k})$ for all $k$. A relative decrease test uses\n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}.$$\nThe numerator reflects the tangible progress, and the denominator normalizes magnitude to prevent division by a very small value; the use of $\\max\\{F(x^{k}),1\\}$ provides numerical stability when $F(x^{k})$ is small. This criterion is cheap because the quantities $f(x^{k})$ and $g(x^{k})$ are already evaluated to verify the backtracking condition. However, small relative decrease may occur far from stationarity in ill-conditioned or heavily regularized problems where $F$ flattens, and it offers no guarantee about first-order residuals or primal-dual gaps. Therefore, it may stop early even when the optimality residual is not small.\n\nGradient mapping norm. The composite first-order optimality condition is\n$$0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star),$$\nwhere $\\partial g$ is the subdifferential of $g$. The gradient mapping at $x$ with parameter $L  0$ is defined as\n$$G_L(x) \\triangleq L\\left(x - \\operatorname{prox}_{g/L}\\!\\left(x - \\tfrac{1}{L}\\nabla f(x)\\right)\\right).$$\nBy the Moreau decomposition and first-order optimality for the proximal step, one can show that $G_L(x) = 0$ if and only if $0 \\in \\nabla f(x) + \\partial g(x)$, that is, $x$ is a first-order stationary point (here, a minimizer due to convexity). In our LASSO setting, with $g(x) = \\lambda\\|x\\|_1$, this becomes\n$$G_{L_k}(x^{k}) = L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}A^\\top(Ax^{k} - y)\\right)\\right).$$\nHence, $\\|G_{L_k}(x^{k})\\|_2$ serves as a principled residual. It directly measures violation of stationarity in the composite sense, unlike $\\|\\nabla f(x^{k})\\|_2$ which ignores $g$. Since the proximal gradient update already computes $\\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}$ at $x^{k}$ to form $x^{k+1}$, the mapping can be obtained at essentially no extra cost by reusing that computation, though if implemented separately it would incur an extra proximal evaluation. It does depend on the choice of $L_k$; however, the zero set of $G_{L_k}$ is independent of $L_k  0$, so it remains a valid residual.\n\nDuality gap. For LASSO, we derive the convex dual from Fenchel duality. Using the representation\n$$\\tfrac{1}{2}\\|A x - y\\|_2^2 = \\sup_{\\theta \\in \\mathbb{R}^m}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\},$$\nwe write the Lagrangian-like saddle form\n$$\\min_{x}\\left[\\sup_{\\theta}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\} + \\lambda\\|x\\|_1\\right] = \\sup_{\\theta}\\left\\{-\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta + \\min_{x}\\left[\\theta^\\top A x + \\lambda\\|x\\|_1\\right]\\right\\}.$$\nThe inner minimum is finite if and only if $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$, in which case it equals $0$; otherwise it is $-\\infty$. Therefore, the dual problem is\n$$\\max_{\\theta \\in \\mathbb{R}^m}\\; d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta \\quad \\text{subject to} \\quad \\|A^\\top \\theta\\|_\\infty \\le \\lambda.$$\nStrong duality holds because $f$ and $g$ are closed, proper, convex and Slater’s condition is satisfied. For any primal $x$ and dual-feasible $\\theta$ we have $d(\\theta) \\le F(x)$, and the duality gap \n$$\\mathrm{gap}(x,\\theta) \\triangleq F(x) - d(\\theta) \\ge 0$$ \nquantifies suboptimality and equals $0$ at optimality. Given $x^{k}$, we can produce a feasible $\\theta^{k}$ by rescaling the residual $r^{k} \\triangleq A x^{k} - y$:\n$$\\theta^{k} \\triangleq \\tau^{k} r^{k}, \\quad \\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top r^{k}\\|_\\infty}\\right\\},$$\nwhich guarantees $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$. The gap then evaluates to\n$$\\mathrm{gap}(x^{k},\\theta^{k}) = \\tfrac{1}{2}\\|r^{k}\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}.$$\nStopping when this is below $\\varepsilon_{\\mathrm{abs}}$ gives an $\\varepsilon_{\\mathrm{abs}}$-accurate primal objective in a certificate sense. Computing $\\theta^{k}$ requires $A^\\top r^{k}$ in addition to $r^{k}$, typically one more multiplication by $A^\\top$ beyond what is needed for $\\nabla f(x^{k}) = A^\\top r^{k}$. If only $\\nabla f$ is provided as a black box without separate access to $A$ and $A^\\top$, or if the model is generalized beyond least-squares, a simple dual construction may not be available.\n\nWe now evaluate each option.\n\nOption A. The formula uses the monotonicity of $F(x^{k})$ under backtracking. The advantage is accurate: it reuses computed quantities and is inexpensive. The limitation is also accurate: a small relative decrease can occur before true stationarity, especially with strong regularization or poor conditioning, and it does not bound any optimality residual. Verdict: Correct.\n\nOption B. The mapping $G_{L_k}(x^{k})$ is correctly defined for the composite model, and its norm is a principled stationarity residual with zero if and only if the composite optimality condition holds. The stated advantage is correct. The limitation is reasonable: it depends on the local $L_k$ and may involve an extra proximal computation unless reused. Verdict: Correct.\n\nOption C. The dual is correctly stated for LASSO, the feasibility condition $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$ is correct, and the rescaling $\\theta^{k} = \\tau^{k}(A x^{k} - y)$ yields a feasible dual point. The duality gap expression $\\mathrm{gap}(x^{k},\\theta^{k}) = F(x^{k}) - d(\\theta^{k})$ expands to the given form and is a valid certificate: if it is $\\le \\varepsilon$, then $F(x^{k}) - F^\\star \\le \\varepsilon$. The advantage is accurate. The limitation is accurate: computing and enforcing feasibility of $\\theta^{k}$ requires access to $A^\\top(Ax^{k} - y)$ and may not be trivial in generalized models. Verdict: Correct.\n\nOption D. Using $\\|\\nabla f(x^{k})\\|_2$ as a stopping test ignores the nonsmooth term $g(x) = \\lambda\\|x\\|_1$. In general, the composite optimality condition is $0 \\in \\nabla f(x) + \\partial g(x)$, not $\\nabla f(x) = 0$. It is common that at a LASSO optimum $\\nabla f(x^\\star) \\neq 0$ while $-\\nabla f(x^\\star) \\in \\partial g(x^\\star)$ holds via the subgradient of the $\\ell_1$ norm. Therefore, this test can be misleading and declare nonconvergence at the true solution or convergence away from it. The claim that “the nonsmooth term does not alter first-order stationarity” is false. Verdict: Incorrect.\n\nOption E. For the constrained basis pursuit $\\min_{x}\\|x\\|_1 \\;\\text{s.t.}\\; A x = y$, the true dual involves constraints on a dual vector and strong duality; $\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ is merely the squared feasibility residual for the constraint and is not a duality gap. In penalty methods, this residual relates to constraint violation but does not certify optimality of the constrained problem without a dual-feasible point and appropriate Lagrange multipliers. Therefore, the statement that it “upper-bounds the true duality gap” and “certifies suboptimality” is incorrect. Verdict: Incorrect.\n\nTherefore, the correct options are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "2897755"}, {"introduction": "While the standard proximal gradient method provides guaranteed convergence, its speed can be a bottleneck in large-scale applications. This has led to the development of accelerated variants like FISTA, which incorporate an inertial term to often achieve significantly faster convergence rates. However, this acceleration is not without its subtleties; it can lead to non-monotone behavior where the objective function temporarily increases. This advanced practice [@problem_id:2897800] challenges you to explore this trade-off between speed and stability, analyze the source of oscillations, and consider principled restart strategies to harness the power of acceleration while maintaining robust performance.", "problem": "Consider the composite objective $F(x) = f(x) + g(x)$ where $f$ is convex and continuously differentiable with $L$-Lipschitz continuous gradient (that is, $\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|$ for all $x,y$), and $g$ is proper, closed, and convex. The Iterative Shrinkage-Thresholding Algorithm (ISTA) is the proximal gradient method with no inertia, while the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) adds an extrapolation (inertial) step driven by a momentum parameter sequence. Assume a fixed stepsize $s \\in (0, 1/L]$ is used in both methods. Using only the descent lemma for $L$-smooth functions and the definition of the proximal operator, analyze the trade-off between acceleration and robustness by constructing a concrete case in which the FISTA iterates and/or objective values exhibit oscillations (non-monotonicity) whereas the ISTA objective values are monotone nonincreasing. Then, propose a principled diagnostic to trigger restarts that curbs these oscillations without sacrificing the benefits of acceleration when it is helpful. Select all options that correctly realize this program.\n\nA. Take $g(x) \\equiv 0$ and $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ with \n$$\nQ = \\begin{bmatrix} 2  1.5 \\\\ 1.5  2 \\end{bmatrix},\n$$\nso that $Q$ is symmetric positive definite with eigenvalues $\\lambda_{\\max} = 3.5$ and $\\lambda_{\\min} = 0.5$. Choose stepsize $s = 0.2 \\le 1/\\lambda_{\\max}$ and initialize $x^{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $x^{-1} = x^{0}$. By the descent lemma and proximal optimality, ISTA yields a monotone nonincreasing sequence $\\{F(x^{k})\\}_{k \\ge 0}$. In contrast, along the slow eigendirection associated with $\\lambda_{\\min}$, the FISTA scalar mode obeys a second-order linear recurrence $u_{k+1} = r \\big(u_k + \\beta_k (u_k - u_{k-1})\\big)$ with $r = 1 - s \\lambda_{\\min} = 0.9$ and $\\beta_k$ the Nesterov momentum parameter. For sufficiently large $k$ (so that $\\beta_k$ is large), this mode becomes underdamped (oscillatory), resulting in non-monotone objective values, while the ISTA values remain monotone. A simple restart diagnostic that prevents sustained oscillation is to reset the momentum (set $\\beta_k = 0$ and $y^k = x^k$) whenever $F(x^{k+1})  F(x^k)$.\n\nB. For any convex $f$ with $L$-Lipschitz gradient and any convex $g$, both FISTA and ISTA produce sequences $\\{F(x^k)\\}$ that are monotone nonincreasing for every stepsize $s \\le 1/L$, hence no restart is needed; oscillations cannot occur.\n\nC. Consider $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ with an ill-conditioned $A \\in \\mathbb{R}^{m \\times n}$ and $g(x) = \\lambda \\|x\\|_1$ with $\\lambda  0$, and choose stepsize $s = 1/L$ with $L = \\|A^{\\top} A\\|_2$. For $x^0 = 0$, ISTA may increase $F(x)$ for some iterations, whereas FISTA is guaranteed monotone in $F(x)$ and thus cannot oscillate.\n\nD. Define the proximal gradient mapping at the extrapolated point $y^k$ by \n$$\nG_s(y^k) \\triangleq \\frac{1}{s}\\left(y^k - \\operatorname{prox}_{s g}\\!\\big(y^k - s \\nabla f(y^k)\\big)\\right).\n$$\nA principled restart diagnostic is to trigger a restart whenever the inertial displacement and the composite descent direction are misaligned, detected by the condition \n$$\n\\langle G_s(y^k),\\, x^{k} - x^{k-1} \\rangle  0,\n$$\nwhich indicates that the momentum is injecting energy in a direction that opposes local descent. This curbs oscillations while preserving acceleration when the two directions align.\n\nE. A reliable restart diagnostic is to trigger a restart whenever \n$$\n\\langle x^{k} - x^{k-1},\\, x^{k+1} - x^{k} \\rangle  0,\n$$\nbecause a negative inner product means the momentum and the descent steps are aligned, so restarting at such times helps avoid oscillations without harming convergence speed.\n\nSelect all that apply.", "solution": "We proceed from first principles: the descent lemma for $L$-smooth functions and the definition of the proximal operator. The descent lemma states that for any $L$-smooth convex $f$,\n$$\nf(y) \\le f(x) + \\langle \\nabla f(x),\\, y - x \\rangle + \\frac{L}{2}\\|y - x\\|^2.\n$$\nFor the composite objective $F(x) = f(x) + g(x)$ and stepsize $s \\in (0, 1/L]$, the ISTA update $x^{k+1}$ is the minimizer of the surrogate \n$$\nQ_s(z; x^k) \\triangleq f(x^k) + \\langle \\nabla f(x^k),\\, z - x^k \\rangle + \\frac{1}{2s}\\|z - x^k\\|^2 + g(z),\n$$\nwhich, by the descent lemma and $s \\le 1/L$, majorizes $F$ at $x^k$, i.e., $Q_s(z; x^k) \\ge F(z)$ for all $z$ and $Q_s(x^k; x^k) = F(x^k)$. Hence,\n$$\nF(x^{k+1}) \\le Q_s(x^{k+1}; x^k) \\le Q_s(x^k; x^k) = F(x^k),\n$$\nso the ISTA objective sequence $\\{F(x^k)\\}$ is monotone nonincreasing. This conclusion hinges only on the descent lemma and proximal optimality and does not depend on a particular $f$ or $g$ beyond the stated assumptions.\n\nIn contrast, FISTA uses an extrapolated point $y^k = x^k + \\beta_k (x^k - x^{k-1})$ with a Nesterov momentum parameter sequence $\\{\\beta_k\\} \\subset [0,1)$, and then applies the same proximal-gradient step at $y^k$. Because the majorization argument that guarantees monotonicity in ISTA involves $x^k$ while FISTA evaluates the surrogate at $y^k \\ne x^k$, the monotonicity of $\\{F(x^k)\\}$ is not guaranteed, and non-monotone oscillations can arise, especially in ill-conditioned problems where the inertial term can temporarily inject energy into slowly decaying modes.\n\nWe now analyze option-by-option.\n\nOption A. Construction and analysis. Take $g \\equiv 0$ and $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ with \n$$\nQ = \\begin{bmatrix} 2  1.5 \\\\ 1.5  2 \\end{bmatrix}.\n$$\nThe symmetric matrix $Q$ has eigenvalues $\\lambda_{\\max} = 3.5$ and $\\lambda_{\\min} = 0.5$, with corresponding orthonormal eigenvectors $u_1 = \\tfrac{1}{\\sqrt{2}}(1,1)^{\\top}$ and $u_2 = \\tfrac{1}{\\sqrt{2}}(1,-1)^{\\top}$. Let $s = 0.2 \\le 1/\\lambda_{\\max}$. ISTA reduces to gradient descent for $g \\equiv 0$, and with $s \\le 1/L$ the descent lemma ensures monotone decrease of $f(x) = F(x)$:\n$$\nf(x^{k+1}) \\le f(x^k) - \\frac{1}{2s}\\|x^{k+1} - x^k\\|^2,\n$$\na standard consequence of the smoothness inequality and optimality of the gradient step. Thus ISTA is monotone.\n\nTo reveal where FISTA can oscillate, decompose $x^k$ in the eigenbasis of $Q$: $x^k = \\alpha_k u_1 + \\beta_k u_2$ (we overload the symbol $\\beta_k$ for the coordinate here only temporarily; to avoid confusion, rename the FISTA momentum as $\\theta_k \\in [0,1)$). The FISTA update for $g \\equiv 0$ is\n$$\ny^k = x^k + \\theta_k (x^k - x^{k-1}), \\quad x^{k+1} = y^k - s Q y^k.\n$$\nProjecting onto the eigenvectors yields decoupled scalar recurrences\n$$\n\\alpha_{k+1} = \\underbrace{(1 - s \\lambda_{\\max})}_{r_1} \\big(\\alpha_k + \\theta_k (\\alpha_k - \\alpha_{k-1})\\big), \\qquad\n\\beta_{k+1} = \\underbrace{(1 - s \\lambda_{\\min})}_{r_2} \\big(\\beta_k + \\theta_k (\\beta_k - \\beta_{k-1})\\big),\n$$\nwith $r_1 = 1 - s \\lambda_{\\max} = 1 - 0.2 \\cdot 3.5 = 0.3$ and $r_2 = 1 - s \\lambda_{\\min} = 1 - 0.2 \\cdot 0.5 = 0.9$. The fast mode $\\alpha_k$ decays rapidly because $|r_1| \\ll 1$. The slow mode $\\beta_k$ is the potential source of oscillations. For a constant momentum $\\theta$ (a standard surrogate for analyzing time-varying momentum sequences), the slow-mode characteristic polynomial is\n$$\nt^2 - r_2 (1 + \\theta)\\, t + r_2 \\theta = 0.\n$$\nThis mode is underdamped (i.e., oscillatory) if and only if the discriminant is negative:\n$$\n\\Delta = r_2^2 (1 + \\theta)^2 - 4 r_2 \\theta  0 \\quad \\Longleftrightarrow \\quad r_2 (1 + \\theta)^2  4 \\theta.\n$$\nWriting $\\theta = \\tau^2$ with $\\tau \\ge 0$, this becomes\n$$\n\\sqrt{r_2}\\, (1 + \\tau^2)  2 \\tau \\quad \\Longleftrightarrow \\quad \\sqrt{r_2}\\, \\tau^2 - 2 \\tau + \\sqrt{r_2}  0,\n$$\nwhich holds when\n$$\n\\tau \\in \\left( \\frac{1 - \\sqrt{1 - r_2}}{\\sqrt{r_2}},\\; \\frac{1 + \\sqrt{1 - r_2}}{\\sqrt{r_2}} \\right).\n$$\nFor $r_2 = 0.9$ we obtain the interval\n$$\n\\tau \\in \\left( \\frac{1 - \\sqrt{0.1}}{\\sqrt{0.9}},\\; \\frac{1 + \\sqrt{0.1}}{\\sqrt{0.9}} \\right) \\approx (0.72,\\; 1.39),\n$$\nso the underdamped regime begins once $\\tau = \\sqrt{\\theta} \\gtrsim 0.72$, i.e., $\\theta \\gtrsim 0.518$. The classical FISTA momentum satisfies $\\theta_k = \\frac{t_{k-1} - 1}{t_k}$ with $t_{k+1} = \\tfrac{1}{2}\\big(1 + \\sqrt{1 + 4 t_k^2}\\big)$, which increases toward $1$; consequently, after a modest number of iterations $\\theta_k$ exceeds $0.518$ and the slow mode becomes underdamped. Under such underdamping, the sequence $\\{|\\beta_k|\\}$ exhibits oscillations (e.g., sign changes and local increases), and therefore the quadratic objective $f(x^k) = \\tfrac{1}{2}\\big(\\lambda_{\\max} \\alpha_k^2 + \\lambda_{\\min} \\beta_k^2\\big)$ is non-monotone, even though it still converges at the accelerated rate. A simple and effective diagnostic to arrest unhelpful oscillations is to trigger a restart when $f(x^{k+1})  f(x^k)$ by resetting $\\theta_k \\leftarrow 0$ and $y^k \\leftarrow x^k$. This preserves acceleration when the inertial direction helps (no restart) and restores robustness when it hurts (restart). Therefore, the construction and the restart rule in Option A are correct.\n\nVerdict for A: Correct.\n\nOption B. The claim that both FISTA and ISTA produce monotone objective sequences for every $s \\le 1/L$ is false. As shown by the analysis above, FISTA’s extrapolation point $y^k \\ne x^k$ breaks the majorization argument that yields $F(x^{k+1}) \\le F(x^k)$, and standard accelerated schemes are known to be non-monotone in general. In contrast, ISTA is monotone by the surrogate minimization argument. Therefore, Option B is incorrect.\n\nVerdict for B: Incorrect.\n\nOption C. For $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$, $g(x) = \\lambda \\|x\\|_1$, and $s = 1/L$ with $L = \\|A^{\\top} A\\|_2$, the proximal-gradient (ISTA) step is the unique minimizer of a quadratic upper bound of $f$ plus $g$, hence $F(x^{k+1}) \\le F(x^k)$ holds for ISTA as in the general argument above, regardless of conditioning. Conversely, FISTA is not guaranteed monotone and can oscillate, especially for ill-conditioned $A$. Thus the statement that ISTA may increase $F$ while FISTA is guaranteed monotone is the reverse of the truth.\n\nVerdict for C: Incorrect.\n\nOption D. The proximal gradient mapping $G_s(y^k) = \\tfrac{1}{s}\\big(y^k - \\operatorname{prox}_{s g}(y^k - s \\nabla f(y^k))\\big)$ generalizes the gradient for the composite setting: when $g \\equiv 0$, it reduces to $\\nabla f(y^k)$. The FISTA inertial displacement is $x^k - x^{k-1}$. If the inner product\n$$\n\\langle G_s(y^k),\\, x^k - x^{k-1} \\rangle  0,\n$$\nthen the extrapolation direction $x^k - x^{k-1}$ is positively aligned with the negative composite descent direction $-G_s(y^k)$ (equivalently, it is misaligned with $-G_s(y^k)$’s opposite), signaling that the inertia term is pushing against the direction favored by the local composite model; this is indicative of overshoot and is a principled signal to restart by zeroing the momentum. This diagnostic has the advantage of not requiring objective evaluation and is consistent with the energy-based rationale for adaptive restarts in accelerated methods. Therefore, Option D presents a correct, principled restart criterion.\n\nVerdict for D: Correct.\n\nOption E. The proposal to restart when $\\langle x^k - x^{k-1},\\, x^{k+1} - x^k \\rangle  0$ is misguided. A negative inner product indicates that the current step and the previous step point in directions that are obtuse, which is typically a sign of curvature and a reduction in step alignment that can be consistent with good descent (e.g., the method is turning toward the minimizer). Restarting when steps are anti-aligned would often discard helpful momentum. Conversely, a positive inner product between the momentum and the composite descent direction (as in Option D, but expressed via $G_s$) is the more appropriate indicator of harmful misalignment. Thus Option E’s criterion restarts under the wrong condition.\n\nVerdict for E: Incorrect.\n\nIn summary, Option A provides a concrete construction where ISTA is monotone and FISTA exhibits oscillations in a slow eigendirection, together with a sensible function-increase restart rule; Option D proposes a principled, composite-aware diagnostic via the proximal gradient mapping. Both are correct, and the remaining options are incorrect for the reasons given.", "answer": "$$\\boxed{AD}$$", "id": "2897800"}]}