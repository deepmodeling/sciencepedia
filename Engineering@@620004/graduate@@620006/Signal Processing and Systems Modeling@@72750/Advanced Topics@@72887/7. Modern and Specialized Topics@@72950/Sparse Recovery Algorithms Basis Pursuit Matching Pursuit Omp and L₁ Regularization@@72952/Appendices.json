{"hands_on_practices": [{"introduction": "The concept of sparsity is central to modern signal processing and machine learning, but quantifying it requires careful thought. This first exercise invites you to explore the different ways we can measure a vector's \"size\" or \"complexity\" using the $\\ell_0$, $\\ell_1$, and $\\ell_2$ norms. By calculating these values for a simple vector and interpreting their meanings, you will build a foundational understanding of why the $\\ell_1$-norm serves as a powerful and computationally tractable proxy for true sparsity [@problem_id:2906040].", "problem": "In sparse recovery, a typical cost function involves norms that quantify different attributes of a vector. Consider the vector $x \\in \\mathbb{R}^{4}$ given by $x = (3,-4,0,2)^{\\top}$. Using the standard definitions of the zero “norm,” one norm, and two norm, compute the values of $\\|x\\|_{0}$, $\\|x\\|_{1}$, and $\\|x\\|_{2}$. Then, explain from first principles in the context of sparse recovery and systems modeling what each quantity measures and why the one norm is used in Basis Pursuit (BP) and $\\ell_{1}$-regularized formulations instead of the zero “norm,” and how this differs from what the two norm measures. Your numerical answers for the norms must be exact. Provide the interpretation in words, and provide the final numerical values as your final answer. No rounding is required. Express angles, if any, in radians. Do not include units.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, and all necessary information is provided. We will proceed with the solution.\n\nThe problem requires the computation of three quantities for the vector $x \\in \\mathbb{R}^{4}$ given by $x = (3, -4, 0, 2)^{\\top}$, and a subsequent explanation of their roles in sparse recovery.\n\nFirst, we will compute the values of the $\\ell_{0}$ \"norm\", the $\\ell_{1}$ norm, and the $\\ell_{2}$ norm. The definitions for a general vector $v \\in \\mathbb{R}^{n}$ are as follows:\nThe $\\ell_{0}$ \"norm\", denoted $\\|v\\|_{0}$, is not a true mathematical norm as it violates the property of positive homogeneity. It is a count of the non-zero elements in the vector:\n$$ \\|v\\|_{0} = \\sum_{i=1}^{n} I(v_i \\neq 0) $$\nwhere $I(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\nThe $\\ell_{1}$ norm, or Manhattan norm, is the sum of the absolute values of the components:\n$$ \\|v\\|_{1} = \\sum_{i=1}^{n} |v_i| $$\n\nThe $\\ell_{2}$ norm, or Euclidean norm, is the square root of the sum of the squares of the components:\n$$ \\|v\\|_{2} = \\sqrt{\\sum_{i=1}^{n} v_i^2} $$\n\nFor the given vector $x = (3, -4, 0, 2)^{\\top}$:\nThe $\\ell_{0}$ \"norm\" is the count of its non-zero elements. The elements $3$, $-4$, and $2$ are non-zero.\n$$ \\|x\\|_{0} = I(3 \\neq 0) + I(-4 \\neq 0) + I(0 \\neq 0) + I(2 \\neq 0) = 1 + 1 + 0 + 1 = 3 $$\n\nThe $\\ell_{1}$ norm is the sum of the absolute values of its components:\n$$ \\|x\\|_{1} = |3| + |-4| + |0| + |2| = 3 + 4 + 0 + 2 = 9 $$\n\nThe $\\ell_{2}$ norm is the square root of the sum of the squares of its components:\n$$ \\|x\\|_{2} = \\sqrt{3^2 + (-4)^2 + 0^2 + 2^2} = \\sqrt{9 + 16 + 0 + 4} = \\sqrt{29} $$\n\nNow, we will explain from first principles the meaning of each quantity in the context of sparse recovery. The archetypal problem in sparse recovery is to find the \"sparsest\" solution $x \\in \\mathbb{R}^{n}$ to an underdetermined system of linear equations $y = Ax$, where $y \\in \\mathbb{R}^{m}$ is a vector of measurements and $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, with $m \\ll n$.\n\nThe $\\ell_{0}$ \"norm\", $\\|x\\|_{0}$, provides a direct measure of the sparsity of the vector $x$. It counts precisely how many components of $x$ are non-zero. In signal processing, if $x$ represents the coefficients of a signal in a basis (e.g., Fourier or wavelet basis), $\\|x\\|_{0}$ is the number of basis elements required to represent the signal. The goal of finding the \"sparsest\" solution is thus mathematically formulated as the optimization problem:\n$$ \\min_{x} \\|x\\|_{0} \\quad \\text{subject to} \\quad y = Ax $$\nThis problem is computationally intractable. It is NP-hard because it requires a combinatorial search over all possible subsets of columns of $A$ to find the one that can represent $y$ with the fewest elements.\n\nThe $\\ell_{1}$ norm, $\\|x\\|_{1}$, is the sum of the magnitudes of the components of $x$. Its critical role in sparse recovery stems from the fact that it is the tightest convex relaxation of the $\\ell_{0}$ \"norm\". This means that the unit ball for the $\\ell_{1}$ norm, $\\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1\\}$, is the largest convex set contained within the $\\ell_{0}$ \"unit ball\" $\\{x \\in \\mathbb{R}^{n} : \\|x\\|_{0} \\leq 1\\}$. Because the $\\ell_{1}$ norm is a convex function, its minimization is a convex optimization problem, which is computationally tractable. Basis Pursuit (BP) replaces the intractable $\\ell_{0}$ minimization with $\\ell_{1}$ minimization:\n$$ \\min_{x} \\|x\\|_{1} \\quad \\text{subject to} \\quad y = Ax $$\nThis can be cast as a linear program and solved efficiently. The reason this works relates to geometry. The level sets of the $\\ell_{1}$ norm are hyper-pyramids (cross-polytopes), which have sharp corners located on the coordinate axes. When seeking an intersection between the affine subspace of solutions $\\{x : y = Ax\\}$ and an expanding $\\ell_{1}$-ball, the first point of contact is very likely to be at one of these corners. A point on a corner has many zero coordinates, thus producing a sparse solution.\n\nThe $\\ell_{2}$ norm, $\\|x\\|_{2}$, measures the Euclidean length of the vector, which corresponds to the signal's energy. Minimizing the $\\ell_{2}$ norm subject to the constraint $y = Ax$ finds the solution with the minimum energy:\n$$ \\min_{x} \\|x\\|_{2} \\quad \\text{subject to} \\quad y = Ax $$\nThis problem has a well-known analytical solution, $x = A^{\\top}(AA^{\\top})^{-1}y$, which is the minimum-norm least-squares solution. However, this solution is almost never sparse. Geometrically, the level sets of the $\\ell_{2}$ norm are hyperspheres, which are perfectly smooth and rotationally invariant. They have no \"corners\" or preference for coordinate axes. The intersection of the solution subspace with an expanding $\\ell_{2}$-ball will typically occur at a point where the energy is distributed among all components of $x$, resulting in a dense vector. Therefore, while useful for other purposes, $\\ell_{2}$ minimization is antithetical to the goal of sparse recovery.\n\nIn summary, $\\ell_{0}$ perfectly measures sparsity but is computationally infeasible to optimize. $\\ell_{2}$ measures energy, is easy to optimize, but yields dense solutions. $\\ell_{1}$ provides the crucial compromise: it is a computationally tractable convex function that, due to its geometric properties, effectively promotes sparse solutions, serving as a practical surrogate for the $\\ell_{0}$ \"norm\".", "answer": "$$ \\boxed{ \\begin{pmatrix} 3 & 9 & \\sqrt{29} \\end{pmatrix} } $$", "id": "2906040"}, {"introduction": "Having established the role of different norms, we now turn to an algorithmic approach for finding sparse solutions. Orthogonal Matching Pursuit (OMP) is a classic greedy algorithm that constructs a solution iteratively by making locally optimal choices at each step. This hands-on simulation will guide you through the core mechanics of OMP, demonstrating how to sequentially identify the most important atoms from a dictionary, project the signal onto the selected subspace, and update the residual until the signal is accurately represented [@problem_id:2905980].", "problem": "Consider a linear model $y = A x^\\star$ with a sensing matrix $A \\in \\mathbb{R}^{3 \\times 4}$ whose columns are unit-norm and a $2$-sparse vector $x^\\star \\in \\mathbb{R}^{4}$. Let the columns of $A$ be\n$$\na_{1}=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\quad\na_{2}=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},\\quad\na_{3}=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix},\\quad\na_{4}=\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix},\n$$\nand let\n$$\nx^\\star=\\begin{bmatrix}0\\\\2\\\\0\\\\1\\end{bmatrix},\\qquad\ny = A x^\\star=\\begin{bmatrix}0\\\\2\\\\1\\end{bmatrix}.\n$$\nUsing the core definition of Orthogonal Matching Pursuit (OMP), which at each iteration selects the column with the largest absolute inner product with the current residual, augments the support, and then orthogonally projects $y$ onto the span of the selected columns via a least-squares solve, carry out exactly three OMP iterations starting from residual $r^{(0)}=y$ and empty support. In case of ties in the selection step, break ties by choosing the smallest column index not yet selected.\n\nAfter the third iteration, form the full coefficient estimate $x^{(3)} \\in \\mathbb{R}^{4}$ by placing the least-squares coefficients on the selected indices (in their natural positions) and zeros elsewhere. Provide the final $x^{(3)}$ as your answer, written as a single row vector. No rounding is required.", "solution": "The problem statement is scrutinized and found to be valid. It is self-contained, mathematically consistent, and well-posed. All provided data, including the matrix $A$, the vector $y$, and the sparse vector $x^\\star$, are consistent with the model $y = A x^\\star$. The Orthogonal Matching Pursuit (OMP) algorithm is a standard procedure in sparse signal recovery. The instructions, including the number of iterations and the tie-breaking rule, are unambiguous. We proceed with the solution.\n\nThe OMP algorithm is initialized with the support set $\\mathcal{S}^{(0)} = \\emptyset$, the estimated coefficient vector $x^{(0)} = \\mathbf{0}$, and the residual $r^{(0)} = y = \\begin{bmatrix}0\\\\2\\\\1\\end{bmatrix}$. We are to perform exactly $3$ iterations.\n\n**Iteration 1:**\nThe first step is to find the column of $A$ that is most correlated with the current residual $r^{(0)}$. We compute the absolute inner products for each column $a_j$, where $j \\in \\{1, 2, 3, 4\\}$.\n$$|\\langle r^{(0)}, a_1 \\rangle| = \\left| \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right| = |0| = 0$$\n$$|\\langle r^{(0)}, a_2 \\rangle| = \\left| \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right| = |2| = 2$$\n$$|\\langle r^{(0)}, a_3 \\rangle| = \\left| \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right| = \\frac{1}{\\sqrt{2}} |0+0+1| = \\frac{1}{\\sqrt{2}} \\approx 0.707$$\n$$|\\langle r^{(0)}, a_4 \\rangle| = \\left| \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right| = |1| = 1$$\nThe maximum correlation is $2$, corresponding to index $j_1 = 2$.\nThe support is updated: $\\mathcal{S}^{(1)} = \\mathcal{S}^{(0)} \\cup \\{2\\} = \\{2\\}$.\nNext, we solve a least-squares problem to find the coefficient for the selected atom. Let $A_{\\mathcal{S}^{(1)}} = [a_2]$. The coefficient vector $z^{(1)}$ is found by solving $\\min_z \\| y - A_{\\mathcal{S}^{(1)}} z \\|_2^2$. The solution is $z^{(1)} = (A_{\\mathcal{S}^{(1)}}^T A_{\\mathcal{S}^{(1)}})^{-1} A_{\\mathcal{S}^{(1)}}^T y$.\n$$A_{\\mathcal{S}^{(1)}}^T A_{\\mathcal{S}^{(1)}} = a_2^T a_2 = \\|a_2\\|_2^2 = 1$$\n$$A_{\\mathcal{S}^{(1)}}^T y = a_2^T y = \\langle y, a_2 \\rangle = 2$$\nSo, $z^{(1)} = (1)^{-1} (2) = 2$.\nThe new residual $r^{(1)}$ is calculated:\n$$r^{(1)} = y - A_{\\mathcal{S}^{(1)}} z^{(1)} = y - a_2 z^{(1)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} (2) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n\n**Iteration 2:**\nWe repeat the selection process with the residual $r^{(1)}$ and the remaining atoms, i.e., for $j \\in \\{1, 3, 4\\}$.\n$$|\\langle r^{(1)}, a_1 \\rangle| = \\left| \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right| = |0| = 0$$\n$$|\\langle r^{(1)}, a_3 \\rangle| = \\left| \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right| = \\frac{1}{\\sqrt{2}} |1| = \\frac{1}{\\sqrt{2}}$$\n$$|\\langle r^{(1)}, a_4 \\rangle| = \\left| \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right| = |1| = 1$$\nThe maximum correlation is $1$, corresponding to index $j_2 = 4$.\nThe support is updated: $\\mathcal{S}^{(2)} = \\mathcal{S}^{(1)} \\cup \\{4\\} = \\{2, 4\\}$.\nWe form the submatrix $A_{\\mathcal{S}^{(2)}} = [a_2, a_4]$ and solve the least-squares problem for $z^{(2)} = (A_{\\mathcal{S}^{(2)}}^T A_{\\mathcal{S}^{(2)}})^{-1} A_{\\mathcal{S}^{(2)}}^T y$.\nFirst, compute $A_{\\mathcal{S}^{(2)}}^T A_{\\mathcal{S}^{(2)}}$:\n$$A_{\\mathcal{S}^{(2)}}^T A_{\\mathcal{S}^{(2)}} = \\begin{pmatrix} a_2^T \\\\ a_4^T \\end{pmatrix} \\begin{pmatrix} a_2 & a_4 \\end{pmatrix} = \\begin{pmatrix} a_2^T a_2 & a_2^T a_4 \\\\ a_4^T a_2 & a_4^T a_4 \\end{pmatrix}$$\nSince $\\|a_2\\|_2^2=1$, $\\|a_4\\|_2^2=1$, and $a_2^T a_4 = 0$, the matrix is the identity matrix: $A_{\\mathcal{S}^{(2)}}^T A_{\\mathcal{S}^{(2)}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThen, compute $A_{\\mathcal{S}^{(2)}}^T y$:\n$$A_{\\mathcal{S}^{(2)}}^T y = \\begin{pmatrix} a_2^T y \\\\ a_4^T y \\end{pmatrix} = \\begin{pmatrix} \\langle y, a_2 \\rangle \\\\ \\langle y, a_4 \\rangle \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nThe coefficient vector is $z^{(2)} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}^{-1} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nThe new residual $r^{(2)}$ is:\n$$r^{(2)} = y - A_{\\mathcal{S}^{(2)}} z^{(2)} = y - (z_1^{(2)} a_2 + z_2^{(2)} a_4) = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\left( 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe residual is now the zero vector. Normally, the algorithm would terminate. However, the problem mandates a third iteration.\n\n**Iteration 3:**\nWe compute correlations with the residual $r^{(2)} = \\mathbf{0}$ for atoms not in the support, i.e., $j \\in \\{1, 3\\}$.\n$$|\\langle r^{(2)}, a_1 \\rangle| = |\\langle \\mathbf{0}, a_1 \\rangle| = 0$$\n$$|\\langle r^{(2)}, a_3 \\rangle| = |\\langle \\mathbf{0}, a_3 \\rangle| = 0$$\nWe have a tie. The tie-breaking rule is to choose the smallest column index not yet selected. The available indices are $\\{1, 3\\}$. The smallest is $1$. So, we select $j_3 = 1$.\nThe support is updated: $\\mathcal{S}^{(3)} = \\mathcal{S}^{(2)} \\cup \\{1\\} = \\{1, 2, 4\\}$.\nWe form the submatrix $A_{\\mathcal{S}^{(3)}} = [a_1, a_2, a_4]$ and solve the least-squares problem for $z^{(3)}$.\n$$A_{\\mathcal{S}^{(3)}} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I_3$$\nThe solution $z^{(3)}$ to $\\min_z \\| y - A_{\\mathcal{S}^{(3)}} z \\|_2^2$ is given by $z^{(3)} = (A_{\\mathcal{S}^{(3)}}^T A_{\\mathcal{S}^{(3)}})^{-1} A_{\\mathcal{S}^{(3)}}^T y$.\n$$A_{\\mathcal{S}^{(3)}}^T A_{\\mathcal{S}^{(3)}} = I_3^T I_3 = I_3$$\n$$A_{\\mathcal{S}^{(3)}}^T y = \\begin{pmatrix} a_1^T y \\\\ a_2^T y \\\\ a_4^T y \\end{pmatrix} = \\begin{pmatrix} \\langle y, a_1 \\rangle \\\\ \\langle y, a_2 \\rangle \\\\ \\langle y, a_4 \\rangle \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\nSo, $z^{(3)} = (I_3)^{-1} \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$.\nThese are the coefficients corresponding to the columns in $A_{\\mathcal{S}^{(3)}}$, which are $a_1, a_2, a_4$ in order.\nThe final coefficient vector $x^{(3)} \\in \\mathbb{R}^4$ is constructed by placing these coefficients at the selected indices and zeros elsewhere.\nThe coefficient for index $1$ is $z^{(3)}_1 = 0$.\nThe coefficient for index $2$ is $z^{(3)}_2 = 2$.\nThe index $3$ is not in the support, so its coefficient is $0$.\nThe coefficient for index $4$ is $z^{(3)}_3 = 1$.\nThus, the final estimate is:\n$$x^{(3)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\nThe problem requests the answer as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 2 & 0 & 1\n\\end{pmatrix}\n}\n$$", "id": "2905980"}, {"introduction": "In contrast to the iterative, step-by-step nature of greedy methods, Basis Pursuit (BP) seeks a globally optimal sparse solution by recasting the problem within the framework of convex optimization. This practice challenges you to solve a BP problem from first principles, connecting the geometric intuition of $\\ell_1$-minimization to the rigorous analytical tools of convex analysis. By deriving the solution and using the Karush-Kuhn-Tucker (KKT) conditions to formally certify its optimality, you will connect theory and practice, understanding not just how to find an answer, but why it is the provably correct one [@problem_id:2906045].", "problem": "Consider the Basis Pursuit (BP) problem from sparse recovery: minimize the $\\ell_{1}$-norm subject to exact linear measurements. Let $A \\in \\mathbb{R}^{2 \\times 3}$ and $y \\in \\mathbb{R}^{2}$ be given by\n$$\nA \\;=\\; \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}, \n\\qquad \ny \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nFormally, the BP problem is\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\nStarting only from the following fundamental bases:\n- the definition of the $\\ell_{1}$-norm $\\|x\\|_{1} = \\sum_{i=1}^{3} |x_{i}|$,\n- linear algebra properties of solutions to $A x = y$,\n- convexity of the $\\ell_{1}$-norm and subdifferential calculus for $\\ell_{1}$,\n- the Lagrangian formalism for equality-constrained convex optimization and the Karush–Kuhn–Tucker (KKT) optimality conditions,\n\ndo the following:\n- Derive the feasible set $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$ explicitly and reduce the problem to a one-dimensional convex minimization.\n- Solve the BP problem exactly to obtain the unique minimizer $x^{\\star}$.\n- Derive the KKT optimality system for this problem by forming the Lagrangian and using the subdifferential of the $\\ell_{1}$-norm. Exhibit an explicit Lagrange multiplier $ \\lambda^{\\star} \\in \\mathbb{R}^{2}$ that certifies optimality of your $x^{\\star}$.\n- Conclude optimality by checking all KKT conditions and, if applicable, comment briefly on uniqueness using a dual certificate argument.\n\nYour final answer must be the optimizer $x^{\\star}$ given as a single $1 \\times 3$ row vector. No rounding is required.", "solution": "The problem presented is a standard instance of Basis Pursuit, a convex optimization problem formulated as\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\nwhere the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and vector $y \\in \\mathbb{R}^{2}$ are given by\n$$\nA \\;=\\; \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}, \n\\qquad \ny \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe problem is well-posed, scientifically sound, and contains all necessary information for a rigorous solution. We shall proceed with the derivation.\n\nFirst, we characterize the feasible set, which is the set of all solutions to the linear system $A x = y$. In component form, the system is\n$$\n\\begin{cases}\nx_1 + x_3 = 1 \\\\\nx_2 + x_3 = 1\n\\end{cases}\n$$\nFrom this system, we can express $x_1$ and $x_2$ in terms of $x_3$:\n$$\nx_1 = 1 - x_3\n$$\n$$\nx_2 = 1 - x_3\n$$\nLet us introduce a free parameter $t \\in \\mathbb{R}$ by setting $x_3 = t$. The entire feasible set can then be parametrized as an affine line in $\\mathbb{R}^{3}$:\n$$\nx(t) = \\begin{pmatrix} 1-t \\\\ 1-t \\\\ t \\end{pmatrix}\n$$\nSubstituting this parametrization into the objective function $\\|x\\|_1 = |x_1| + |x_2| + |x_3|$ reduces the problem to a one-dimensional unconstrained convex minimization:\n$$\n\\min_{t \\in \\mathbb{R}} f(t) \\quad \\text{where} \\quad f(t) = \\|x(t)\\|_1 = |1-t| + |1-t| + |t| = 2|1-t| + |t|\n$$\nThe function $f(t)$ is a sum of absolute value functions, and is therefore convex. The points of non-differentiability are at $t=0$ and $t=1$. We analyze the function by examining its derivative in the intervals defined by these points.\n\\begin{itemize}\n    \\item For $t < 0$: $f(t) = 2(1-t) - t = 2 - 3t$. The slope is $f'(t) = -3$.\n    \\item For $0 < t < 1$: $f(t) = 2(1-t) + t = 2 - t$. The slope is $f'(t) = -1$.\n    \\item For $t > 1$: $f(t) = 2(t-1) + t = 3t - 2$. The slope is $f'(t) = 3$.\n\\end{itemize}\nThe slope of the function $f(t)$ changes from negative to positive at $t=1$. Specifically, $\\lim_{t \\to 1^-} f'(t) = -1$ and $\\lim_{t \\to 1^+} f'(t) = 3$. This indicates that the global minimum of $f(t)$ is attained at $t=1$.\nThe optimal parameter is $t^{\\star} = 1$.\nSubstituting $t^{\\star}$ back into the parametrization for $x$ gives the unique minimizer $x^{\\star}$:\n$$\nx^{\\star} = x(1) = \\begin{pmatrix} 1-1 \\\\ 1-1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe minimum value of the objective is $\\|x^{\\star}\\|_1 = |0| + |0| + |1| = 1$.\n\nNext, we derive the Karush–Kuhn–Tucker (KKT) conditions to certify the optimality of $x^{\\star}$. The Lagrangian for this problem is\n$$\nL(x, \\lambda) = \\|x\\|_1 + \\lambda^{T}(Ax - y)\n$$\nwhere $\\lambda \\in \\mathbb{R}^{2}$ is the vector of Lagrange multipliers. The KKT conditions for an optimal pair $(x^{\\star}, \\lambda^{\\star})$ are:\n1. Primal feasibility: $A x^{\\star} = y$.\n2. Stationarity: $0 \\in \\partial_x L(x^{\\star}, \\lambda^{\\star})$, which simplifies to $-A^T\\lambda^{\\star} \\in \\partial \\|x^{\\star}\\|_1$.\n\nThe subdifferential of the $\\ell_1$-norm at a point $x$ is the set\n$$\n\\partial \\|x\\|_1 = \\{g \\in \\mathbb{R}^3 : g_i = \\text{sign}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } g_i \\in [-1, 1] \\text{ if } x_i=0 \\}\n$$\nFor our candidate solution $x^{\\star} = (0, 0, 1)^T$, the support is the set of indices where the components are non-zero, $T = \\{3\\}$. The components $x^{\\star}_1$ and $x^{\\star}_2$ are zero. The subdifferential at $x^{\\star}$ is thus the set of vectors $g=(g_1, g_2, g_3)^T$ such that:\n$$\ng_1 \\in [-1, 1], \\quad g_2 \\in [-1, 1], \\quad g_3 = \\text{sign}(x^{\\star}_3) = 1\n$$\nThe stationarity condition requires finding a Lagrange multiplier $\\lambda^{\\star} = (\\lambda_1, \\lambda_2)^T$ such that $g = -A^T\\lambda^{\\star}$ satisfies these subgradient conditions. We have\n$$\nA^T = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\text{so} \\quad -A^T\\lambda^{\\star} = \\begin{pmatrix} -\\lambda_1 \\\\ -\\lambda_2 \\\\ -(\\lambda_1+\\lambda_2) \\end{pmatrix}\n$$\nThe conditions on $\\lambda^{\\star}$ are:\n1. $-\\lambda_1 \\in [-1, 1] \\implies |\\lambda_1| \\leq 1$.\n2. $-\\lambda_2 \\in [-1, 1] \\implies |\\lambda_2| \\leq 1$.\n3. $-(\\lambda_1 + \\lambda_2) = 1 \\implies \\lambda_1 + \\lambda_2 = -1$.\n\nWe must find an explicit $\\lambda^{\\star}$ that satisfies these constraints. A suitable choice is $\\lambda^{\\star} = \\begin{pmatrix} -1/2 \\\\ -1/2 \\end{pmatrix}$. Let us verify it:\n\\begin{itemize}\n    \\item $|\\lambda_1| = |-1/2| = 1/2 \\leq 1$. This is satisfied.\n    \\item $|\\lambda_2| = |-1/2| = 1/2 \\leq 1$. This is satisfied.\n    \\item $\\lambda_1 + \\lambda_2 = -1/2 - 1/2 = -1$. This is satisfied.\n\\end{itemize}\nThus, $\\lambda^{\\star} = (-1/2, -1/2)^T$ is a valid Lagrange multiplier. With this $\\lambda^{\\star}$, the corresponding subgradient vector is $g = -A^T\\lambda^{\\star} = (1/2, 1/2, 1)^T$. This vector $g$ lies in $\\partial \\|x^{\\star}\\|_1$ since $|g_1|=1/2 \\le 1$, $|g_2|=1/2 \\le 1$, and $g_3 = 1 = \\text{sign}(x^{\\star}_3)$. The primal feasibility condition $Ax^{\\star}=y$ is also satisfied, as $A x^{\\star} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$.\nSince all KKT conditions hold for the pair $(x^{\\star}, \\lambda^{\\star})$, the optimality of $x^{\\star}$ is confirmed.\n\nFinally, we comment on uniqueness. The uniqueness of the primal solution $x^{\\star}$ is guaranteed if there exists a dual certificate (a subgradient $g = -A^T \\lambda^{\\star}$) such that its components are strictly less than $1$ in magnitude on the zero-support of $x^{\\star}$. The zero-support is $T^c = \\{1, 2\\}$. For our chosen $g = (1/2, 1/2, 1)^T$, we check this condition:\n$$\n|g_1| = 1/2 < 1\n$$\n$$\n|g_2| = 1/2 < 1\n$$\nThe condition is strictly satisfied. This, combined with the fact that the columns of $A$ corresponding to the support $T=\\{3\\}$ are linearly independent (the column $A_3=(1,1)^T$ is non-zero), guarantees that $x^{\\star}$ is the unique solution to the Basis Pursuit problem.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}}\n$$", "id": "2906045"}]}