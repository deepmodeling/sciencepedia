{"hands_on_practices": [{"introduction": "This practice returns to the fundamental definition of the graph Laplacian, constructing it from the node-edge incidence matrix $B$. This first-principles derivation for a path graph, a ubiquitous structure, solidifies the connection between a graph's topology and the operator $L = B^{\\top} B$ [@problem_id:2903901]. You will then determine the algebraic connectivity, a crucial eigenvalue that quantifies graph connectivity, by solving the resulting linear recurrence relationâ€”a classic and powerful problem-solving technique.", "problem": "Consider the undirected, unweighted path graph on $n \\geq 2$ labeled nodes $\\{1,2,\\dots,n\\}$, with edges $\\{(1,2),(2,3),\\dots,(n-1,n)\\}$. Impose an arbitrary orientation by directing each edge from the lower-indexed node to the higher-indexed node. Let $B \\in \\mathbb{R}^{(n-1)\\times n}$ denote the node-edge incidence matrix determined by this orientation, where each row corresponds to an edge and each column to a node, and entries $-1$, $+1$, and $0$ indicate the tail, head, and non-incidence, respectively.\n\nUsing only the fundamental definitions of the incidence matrix and the unnormalized graph Laplacian $L$ as $L = B^{\\top} B$, do the following:\n\n1. Construct $B$ explicitly for the given orientation of the path graph and derive the explicit tridiagonal structure of $L$ from $B^{\\top} B$. Your derivation must start from the definition of $B$ and $L$ and use only linear-algebraic operations, without invoking any pre-memorized formulas for $L$.\n\n2. From the obtained tridiagonal form of $L$, derive the closed-form expression of the algebraic connectivity, defined as the second-smallest eigenvalue of $L$. Your derivation must start from the eigenvalue equation for the tridiagonal matrix and proceed by first principles (e.g., solving the associated linear recurrence with appropriate boundary conditions).\n\nProvide your final answer as a single exact analytic expression in terms of $n$. Do not approximate, and do not include any units. If you introduce any angles, they must be in radians. The final answer must be a single expression, not an inequality or an equation.", "solution": "The problem as stated is mathematically well-defined, self-contained, and scientifically sound. It presents a standard problem in spectral graph theory that admits a unique, verifiable solution. Thus, we proceed with the derivation.\n\nThe problem requires a two-part derivation. First, the construction of the graph Laplacian matrix $L$ from the incidence matrix $B$. Second, the derivation of the algebraic connectivity of $L$ by solving its eigenvalue problem.\n\nPart 1: Derivation of the Laplacian Matrix $L$\n\nThe graph is an unweighted path graph on $n$ nodes, which we label $\\{1, 2, \\dots, n\\}$. The edges are $\\{(k, k+1) \\mid k = 1, 2, \\dots, n-1\\}$. There are $m = n-1$ edges. An orientation is imposed by directing each edge from node $k$ to node $k+1$.\n\nThe incidence matrix $B \\in \\mathbb{R}^{(n-1) \\times n}$ has its rows indexed by edges and columns by nodes. Let the $k$-th edge be $e_k = (k, k+1)$, where $k$ is the tail and $k+1$ is the head. The entries of $B$ are defined as:\n$B_{k,j} = -1$ if node $j$ is the tail of edge $e_k$,\n$B_{k,j} = +1$ if node $j$ is the head of edge $e_k$,\n$B_{k,j} = 0$ otherwise.\n\nFor the $k$-th row, corresponding to edge $e_k = (k, k+1)$, the non-zero entries are at columns $j=k$ and $j=k+1$. Specifically, for $k \\in \\{1, \\dots, n-1\\}$:\n$$\nB_{k,j} = \\begin{cases}\n-1 & \\text{if } j=k \\\\\n+1 & \\text{if } j=k+1 \\\\\n0  & \\text{otherwise}\n\\end{cases}\n$$\nThe unnormalized graph Laplacian is defined as $L = B^{\\top}B$. $L$ is an $n \\times n$ matrix whose element $L_{ij}$ is the dot product of the $i$-th and $j$-th columns of $B$. Let $\\mathbf{b}_j \\in \\mathbb{R}^{n-1}$ be the $j$-th column of $B$. Then $L_{ij} = \\mathbf{b}_i^{\\top}\\mathbf{b}_j$.\n\nWe now determine the structure of the columns of $B$:\n- The first column, $\\mathbf{b}_1$, corresponds to node $1$. Node $1$ is the tail only of edge $e_1=(1,2)$. So, $\\mathbf{b}_1$ has a $-1$ in the first row and zeros elsewhere.\n- A generic column $\\mathbf{b}_j$ for $j \\in \\{2, \\dots, n-1\\}$ corresponds to an internal node $j$. Node $j$ is the head of edge $e_{j-1}=(j-1, j)$ and the tail of edge $e_j=(j, j+1)$. So, $\\mathbf{b}_j$ has a $+1$ in row $j-1$, a $-1$ in row $j$, and zeros elsewhere.\n- The last column, $\\mathbf{b}_n$, corresponds to node $n$. Node $n$ is the head only of edge $e_{n-1}=(n-1, n)$. So, $\\mathbf{b}_n$ has a $+1$ in the last row ($n-1$) and zeros elsewhere.\n\nNow we compute the entries of $L = (L_{ij})$:\n\n1.  Diagonal elements ($i=j$): $L_{ii} = \\mathbf{b}_i^{\\top}\\mathbf{b}_i$.\n    - For $i=1$: $L_{11} = \\mathbf{b}_1^{\\top}\\mathbf{b}_1 = (-1)^2 = 1$.\n    - For $i \\in \\{2, \\dots, n-1\\}$: $L_{ii} = \\mathbf{b}_i^{\\top}\\mathbf{b}_i = (+1)^2 + (-1)^2 = 2$.\n    - For $i=n$: $L_{nn} = \\mathbf{b}_n^{\\top}\\mathbf{b}_n = (+1)^2 = 1$.\n    The diagonal of $L$ is $(1, 2, 2, \\dots, 2, 1)$.\n\n2.  Off-diagonal elements ($i \\neq j$): $L_{ij} = \\mathbf{b}_i^{\\top}\\mathbf{b}_j$.\n    - For adjacent nodes, $j=i+1$: We compute $L_{i, i+1} = \\mathbf{b}_i^{\\top}\\mathbf{b}_{i+1}$.\n        - For $i=1$: $\\mathbf{b}_1$ has a $-1$ at row $1$. $\\mathbf{b}_2$ has a $+1$ at row $1$. Their supports overlap only at row $1$. $L_{12} = (-1)(+1) = -1$.\n        - For $i \\in \\{2, \\dots, n-2\\}$: $\\mathbf{b}_i$ has non-zero entries at rows $i-1$ and $i$. $\\mathbf{b}_{i+1}$ has non-zero entries at rows $i$ and $i+1$. Their supports overlap only at row $i$. The entry of $\\mathbf{b}_i$ at row $i$ is $-1$ and the entry of $\\mathbf{b}_{i+1}$ at row $i$ is $+1$. So, $L_{i, i+1} = (-1)(+1) = -1$.\n        - For $i=n-1$: $\\mathbf{b}_{n-1}$ has non-zero entries at rows $n-2$ and $n-1$. $\\mathbf{b}_n$ has a non-zero entry at row $n-1$. Their supports overlap only at row $n-1$. The entry of $\\mathbf{b}_{n-1}$ at row $n-1$ is $-1$ and the entry of $\\mathbf{b}_n$ is $+1$. So, $L_{n-1,n} = (-1)(+1) = -1$.\n    - Since $L$ is symmetric, $L_{i+1, i} = L_{i, i+1} = -1$.\n    - For non-adjacent nodes, $|i-j| > 1$: The columns $\\mathbf{b}_i$ and $\\mathbf{b}_j$ have non-zero entries at disjoint sets of rows. Thus, their dot product is zero. $L_{ij} = 0$.\n\nCombining these results, the Laplacian matrix $L$ is the $n \\times n$ symmetric tridiagonal matrix:\n$$\nL = \\begin{pmatrix}\n1 & -1 & 0 & \\cdots & 0 & 0 \\\\\n-1 & 2 & -1 & \\cdots & 0 & 0 \\\\\n0 & -1 & 2 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 2 & -1 \\\\\n0 & 0 & 0 & \\cdots & -1 & 1\n\\end{pmatrix}\n$$\n\nPart 2: Derivation of the Algebraic Connectivity\n\nThe algebraic connectivity is the second-smallest eigenvalue of $L$. We must solve the eigenvalue equation $L\\mathbf{v} = \\lambda\\mathbf{v}$, where $\\mathbf{v} = [v_1, v_2, \\dots, v_n]^{\\top}$ is an eigenvector and $\\lambda$ is its corresponding eigenvalue. This matrix equation represents a system of $n$ linear equations.\n\n- Row $1$: $v_1 - v_2 = \\lambda v_1 \\implies (1-\\lambda)v_1 - v_2 = 0$\n- Row $k$ ($1 < k < n$): $-v_{k-1} + 2v_k - v_{k+1} = \\lambda v_k \\implies -v_{k-1} + (2-\\lambda)v_k - v_{k+1} = 0$\n- Row $n$: $-v_{n-1} + v_n = \\lambda v_n \\implies -v_{n-1} + (1-\\lambda)v_n = 0$\n\nThe equation for internal nodes defines a second-order linear homogeneous recurrence relation: $v_{k+1} - (2-\\lambda)v_k + v_{k-1} = 0$. The first and last equations act as boundary conditions.\n\nWe can unify these equations by defining \"ghost points\" $v_0$ and $v_{n+1}$.\nThe boundary equation at $k=1$ is $v_2 = (1-\\lambda)v_1$. The general recurrence for $k=1$ would be $v_2 - (2-\\lambda)v_1 + v_0 = 0$. Substituting $v_2$, we get $(1-\\lambda)v_1 - (2-\\lambda)v_1 + v_0 = 0$, which simplifies to $-v_1 + v_0 = 0$, or $v_0 = v_1$.\nSimilarly, the boundary equation at $k=n$ is $v_{n-1} = (1-\\lambda)v_n$. The general recurrence for $k=n$ would be $v_{n+1} - (2-\\lambda)v_n + v_{n-1} = 0$. Substituting $v_{n-1}$, we get $v_{n+1} - (2-\\lambda)v_n + (1-\\lambda)v_n = 0$, which simplifies to $v_{n+1} - v_n = 0$, or $v_{n+1} = v_n$.\n\nThus, the problem is equivalent to solving the recurrence relation $v_{k+1} - (2-\\lambda)v_k + v_{k-1} = 0$ for $k=1, \\dots, n$ subject to the Neumann-type boundary conditions $v_0 = v_1$ and $v_{n+1} = v_n$.\n\nLet $2-\\lambda = 2\\cos(\\theta)$ for some $\\theta$. The recurrence becomes $v_{k+1} - 2\\cos(\\theta)v_k + v_{k-1} = 0$. The characteristic equation is $r^2 - 2\\cos(\\theta)r + 1 = 0$, with roots $r = e^{\\pm i\\theta}$. The general solution is of the form $v_k = A \\cos(k\\theta) + B \\sin(k\\theta)$.\nFor boundary conditions of the form $v_0=v_1$ and $v_n=v_{n+1}$, a more convenient form of the solution is $v_k = C\\cos((k-1/2)\\theta)$.\nLet's verify the first condition: $v_1 = C\\cos(\\theta/2)$ and $v_0 = C\\cos(-\\theta/2) = C\\cos(\\theta/2)$. So $v_0=v_1$ is satisfied for any $\\theta$.\n\nNow we apply the second boundary condition, $v_{n+1} = v_n$:\n$$ C\\cos\\left(\\left(n+1-\\frac{1}{2}\\right)\\theta\\right) = C\\cos\\left(\\left(n-\\frac{1}{2}\\right)\\theta\\right) $$\n$$ \\cos\\left(\\left(n+\\frac{1}{2}\\right)\\theta\\right) = \\cos\\left(\\left(n-\\frac{1}{2}\\right)\\theta\\right) $$\nThis equality holds if $\\left(n+\\frac{1}{2}\\right)\\theta = \\pm \\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j$ for some integer $j$.\n- Case 1: $\\left(n+\\frac{1}{2}\\right)\\theta = \\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j \\implies \\theta = 2\\pi j$. This yields $\\lambda = 2 - 2\\cos(2\\pi j) = 0$. This corresponds to the smallest eigenvalue $\\lambda_1 = 0$, whose eigenvector is the constant vector $[1, 1, \\dots, 1]^{\\top}$.\n- Case 2: $\\left(n+\\frac{1}{2}\\right)\\theta = -\\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j \\implies 2n\\theta = 2\\pi j \\implies \\theta = \\frac{\\pi j}{n}$.\n\nThis gives a set of $n$ distinct eigenvalues by taking $j = 0, 1, \\dots, n-1$. The eigenvalues $\\lambda_j$ are given by:\n$$ \\lambda_j = 2 - 2\\cos(\\theta_j) = 2 - 2\\cos\\left(\\frac{\\pi j}{n}\\right) \\quad \\text{for } j=0, 1, \\dots, n-1. $$\nThe function $\\cos(x)$ is strictly decreasing for $x \\in [0, \\pi]$. Our arguments $\\frac{\\pi j}{n}$ lie in the range $[0, \\pi - \\pi/n]$. Therefore, as $j$ increases, $\\cos(\\frac{\\pi j}{n})$ decreases, and $\\lambda_j$ increases. The eigenvalues are naturally ordered by the index $j$.\n\nThe smallest eigenvalue corresponds to $j=0$: $\\lambda_0 = 2 - 2\\cos(0) = 0$.\nThe second-smallest eigenvalue, which is the algebraic connectivity, corresponds to $j=1$:\n$$ \\lambda_1 = 2 - 2\\cos\\left(\\frac{\\pi}{n}\\right) $$\nThis is the required expression.", "answer": "$$\n\\boxed{2 - 2\\cos\\left(\\frac{\\pi}{n}\\right)}\n$$", "id": "2903901"}, {"introduction": "Computing the full spectrum of a large graph's Laplacian can be prohibitively expensive. This exercise introduces a powerful technique using Gershgorin's Circle Theorem to estimate the spectral range of both the unnormalized ($L$) and normalized ($L_{\\mathrm{sym}}$) Laplacians without calculating a single eigenvalue [@problem_id:2903926]. Mastering this skill allows for a rapid assessment of the potential behavior of graph filters and dynamics on any given graph, using only local information about node degrees.", "problem": "Consider a finite, undirected, weighted graph with vertex set $\\{1,2,\\dots,n\\}$, symmetric nonnegative weights $w_{ij} = w_{ji} \\ge 0$ with $w_{ii} = 0$, weighted adjacency matrix $A = [w_{ij}]$, degree sequence defined by $d_i = \\sum_{j=1}^{n} w_{ij}$, and degree matrix $D = \\mathrm{diag}(d_1,\\dots,d_n)$. Define the unnormalized graph Laplacian $L = D - A$ and the symmetric normalized Laplacian $L_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}$, where $I$ is the identity matrix. Let $d_{\\max} = \\max_{1 \\le i \\le n} d_i$.\n\nStarting only from the Gershgorin Circle Theorem (no other spectral bounds or specific Laplacian eigenvalue facts may be assumed), and basic linear algebraic facts about similarity transformations, derive tight uniform Gershgorin-based upper bounds $B_{L}$ and $B_{L_{\\mathrm{sym}}}$ on the largest eigenvalues of $L$ and $L_{\\mathrm{sym}}$, respectively, expressed in terms of $\\{w_{ij}\\}$ and $\\{d_i\\}$. Then, provide a single simplified analytic expression for the sum $B_{L} + B_{L_{\\mathrm{sym}}}$ in terms of $d_{\\max}$ only.\n\nYour final answer must be a single closed-form expression. Do not include any inequalities or equations in your final answer. No numerical rounding is required.", "solution": "The problem as stated is valid and well-posed, contingent upon the standard assumption that the graph contains no isolated vertices, i.e., $d_i = \\sum_{j=1}^{n} w_{ij} > 0$ for all vertices $i \\in \\{1, 2, \\dots, n\\}$. This is necessary for the degree matrix $D$ to be invertible and for $D^{-1/2}$ to be well-defined. We proceed under this assumption.\n\nThe foundation of this derivation is the Gershgorin Circle Theorem. For any square matrix $M = [m_{ij}] \\in \\mathbb{C}^{n \\times n}$, the spectrum $\\sigma(M)$ is contained within the union of $n$ disks in the complex plane, $\\sigma(M) \\subseteq \\bigcup_{i=1}^{n} C_i$, where each disk $C_i$ is centered at the diagonal element $m_{ii}$ and has a radius $R_i = \\sum_{j \\ne i} |m_{ij}|$. Since both Laplacian matrices $L$ and $L_{\\mathrm{sym}}$ are real and symmetric, their eigenvalues are real. Consequently, the eigenvalues must lie in the union of the real intervals defined by the intersection of the Gershgorin disks with the real axis.\n\nFirst, we derive the upper bound for the largest eigenvalue of the unnormalized graph Laplacian, $L = D - A$. The entries of $L$ are given by:\n$$L_{ij} = \\begin{cases} d_i & \\text{if } i=j \\\\ -w_{ij} & \\text{if } i \\ne j \\end{cases}$$\nFor the $i$-th Gershgorin circle, the center is the diagonal entry $c_i = L_{ii} = d_i$. The radius is the sum of the absolute values of the off-diagonal entries in the $i$-th row:\n$$R_i = \\sum_{j \\ne i} |L_{ij}| = \\sum_{j \\ne i} |-w_{ij}| = \\sum_{j \\ne i} w_{ij}$$\nBy the definition of the vertex degree $d_i$ and the given condition $w_{ii}=0$, we have $d_i = \\sum_{j=1}^{n} w_{ij} = w_{ii} + \\sum_{j \\ne i} w_{ij} = \\sum_{j \\ne i} w_{ij}$.\nTherefore, the radius of the $i$-th circle is exactly equal to its center: $R_i = d_i$.\n\nFor any eigenvalue $\\lambda$ of $L$, there exists an index $i$ such that $|\\lambda - c_i| \\le R_i$, which is $|\\lambda - d_i| \\le d_i$. Since $\\lambda$ must be real, this inequality is equivalent to $-d_i \\le \\lambda - d_i \\le d_i$, or $0 \\le \\lambda \\le 2d_i$.\nThe entire spectrum of $L$ must lie within the union of these intervals: $\\sigma(L) \\subseteq \\bigcup_{i=1}^{n} [0, 2d_i]$. The largest eigenvalue, $\\lambda_{\\max}(L)$, is therefore bounded by the maximum value in this union:\n$$\\lambda_{\\max}(L) \\le \\max_{1 \\le i \\le n} (2d_i) = 2 \\max_{1 \\le i \\le n} d_i = 2d_{\\max}$$\nThis bound is tight; for instance, a complete graph on two vertices with weight $w_{12}=1$ has $d_{\\max}=1$ and eigenvalues $\\{0, 2\\}$, where $\\lambda_{\\max}(L)=2=2d_{\\max}$. Thus, the tightest uniform Gershgorin-based bound is $B_L = 2d_{\\max}$.\n\nNext, we derive the upper bound for the largest eigenvalue of the symmetric normalized Laplacian, $L_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}$. A direct application of the Gershgorin theorem to $L_{\\mathrm{sym}}$ is possible, but it does not yield the tightest possible bound obtainable through this method. The problem correctly allows the use of similarity transformations, which is the proper path.\n\nConsider the random walk Laplacian, $L_{\\mathrm{rw}} = D^{-1}L = I - D^{-1}A$. The matrix $L_{\\mathrm{sym}}$ is similar to $L_{\\mathrm{rw}}$:\n$$L_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2} = D^{1/2} (I - D^{-1}A) D^{-1/2} = D^{1/2} L_{\\mathrm{rw}} D^{-1/2}$$\nSince $L_{\\mathrm{sym}}$ and $L_{\\mathrm{rw}}$ are similar, they share the same set of eigenvalues. We can therefore analyze the spectrum of $L_{\\mathrm{rw}}$ to bound the eigenvalues of $L_{\\mathrm{sym}}$. The entries of $L_{\\mathrm{rw}}$ are:\n$$(L_{\\mathrm{rw}})_{ij} = \\delta_{ij} - \\frac{w_{ij}}{d_i}$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The diagonal entries are $(L_{\\mathrm{rw}})_{ii} = 1 - w_{ii}/d_i = 1$. The off-diagonal entries are $(L_{\\mathrm{rw}})_{ij} = -w_{ij}/d_i$ for $i \\ne j$.\n\nNow, we apply the Gershgorin theorem to $L_{\\mathrm{rw}}$. For the $i$-th row, the center of the disk is $c_i = (L_{\\mathrm{rw}})_{ii} = 1$. The radius is:\n$$R_i = \\sum_{j \\ne i} |(L_{\\mathrm{rw}})_{ij}| = \\sum_{j \\ne i} \\left|-\\frac{w_{ij}}{d_i}\\right| = \\frac{1}{d_i} \\sum_{j \\ne i} w_{ij}$$\nSince $\\sum_{j \\ne i} w_{ij} = d_i$, the radius is $R_i = d_i/d_i = 1$.\nThis holds for every row $i=1, \\dots, n$. Therefore, all Gershgorin disks for $L_{\\mathrm{rw}}$ are identical: they are centered at $1$ with a radius of $1$. Any eigenvalue $\\lambda$ of $L_{\\mathrm{rw}}$ must satisfy $|\\lambda - 1| \\le 1$.\nThe eigenvalues of $L_{\\mathrm{sym}}$ are the same as those of $L_{\\mathrm{rw}}$ and are known to be real because $L_{\\mathrm{sym}}$ is symmetric. Thus, the eigenvalues must lie in the intersection of the disk $|\\lambda - 1| \\le 1$ with the real axis, which is the interval $[0, 2]$.\nThis implies that for any eigenvalue $\\lambda$ of $L_{\\mathrm{sym}}$, we have $0 \\le \\lambda \\le 2$. The largest eigenvalue is therefore bounded by:\n$$\\lambda_{\\max}(L_{\\mathrm{sym}}) \\le 2$$\nThis bound is tight, as shown again by the two-vertex graph with $w_{12}=1$, for which $L_{\\mathrm{sym}}$ has eigenvalues $\\{0, 2\\}$. The bound is $B_{L_{\\mathrm{sym}}} = 2$.\n\nFinally, the problem requires the sum of these two bounds, $B_L + B_{L_{\\mathrm{sym}}}$, expressed in terms of $d_{\\max}$.\n$$B_L + B_{L_{\\mathrm{sym}}} = 2d_{\\max} + 2$$\nThis expression is the required simplified analytic form.", "answer": "$$\\boxed{2d_{\\max} + 2}$$", "id": "2903926"}, {"introduction": "The power of the graph Laplacian is most evident when used to process signals, a task central to graph signal processing. This exercise guides you through a cornerstone technique: approximating a complex spectral filter $g(L)$ with a computationally efficient polynomial using a Chebyshev series [@problem_id:2903956]. You will learn the critical role of spectral rescaling for numerical stability and derive the general form of the polynomial approximation, a method that underpins many large-scale graph learning and processing algorithms.", "problem": "Consider a weighted, undirected graph with $N$ nodes and combinatorial graph Laplacian $L \\in \\mathbb{R}^{N \\times N}$ defined by $L = D - W$, where $W$ is the symmetric weight matrix and $D$ is the degree matrix. The matrix $L$ is symmetric positive semidefinite with real eigenvalues in $[0,\\lambda_{\\max}]$. Let $g : [0,\\lambda_{\\max}] \\to \\mathbb{R}$ be a continuous function. The graph filter $g(L)$ is defined by the spectral functional calculus: if $L = U \\Lambda U^{\\top}$ with $\\Lambda = \\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{N})$ and $U$ orthogonal, then $g(L) = U \\, g(\\Lambda) \\, U^{\\top}$ with $g(\\Lambda) = \\mathrm{diag}(g(\\lambda_{1}),\\dots,g(\\lambda_{N}))$.\n\nYour goal is to obtain a numerically stable, degree-$K$ polynomial approximation of $g(L)$ using Chebyshev polynomials of the first kind. Starting only from the properties that (i) $L$ is diagonalizable with spectrum in $[0,\\lambda_{\\max}]$, and (ii) Chebyshev polynomials of the first kind $\\{T_{k}\\}_{k \\ge 0}$ form an orthogonal basis on $[-1,1]$ with respect to the weight $(1-x^{2})^{-1/2}$, derive:\n\n1. An affine rescaling of the spectrum that maps $[0,\\lambda_{\\max}]$ to $[-1,1]$ and the corresponding rescaled operator $\\widetilde{L}$ expressed in terms of $L$ and $\\lambda_{\\max}$.\n2. The degree-$K$ Chebyshev approximation $p_{K}(L)$ of $g(L)$ as a sum of Chebyshev polynomials evaluated at $\\widetilde{L}$, with coefficients written explicitly as integrals involving $g$ on $[0,\\lambda_{\\max}]$ via the inverse of your affine rescaling. Your derivation should make clear how the orthogonality on $[-1,1]$ determines these coefficients.\n3. A brief justification of why this rescaling promotes numerical stability when evaluating the approximation via the three-term Chebyshev recurrence.\n\nProvide your final answer as a single, closed-form analytic expression for $p_{K}(L)$ in terms of $g$, $\\lambda_{\\max}$, $K$, and $L$. No numerical evaluation is required, and no units are needed. The final answer must be a single expression, not an equation or inequality.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of graph signal processing, based on established principles of linear algebra and approximation theory. All necessary data and definitions are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nThe objective is to construct a degree-$K$ polynomial approximation of a graph filter $g(L)$ using Chebyshev polynomials. We will address the three required parts of the derivation in sequence.\n\n1. Affine Rescaling of the Spectrum\n\nThe graph Laplacian $L$ is a symmetric positive semidefinite matrix, and its spectrum, denoted $\\sigma(L)$, lies within the interval $[0, \\lambda_{\\max}]$, where $\\lambda_{\\max}$ is the largest eigenvalue of $L$. Chebyshev polynomials $T_k(x)$ are defined and possess their desirable orthogonality and stability properties on the interval $[-1, 1]$. Therefore, a mapping from the spectral domain of $L$ to this interval is required.\n\nLet $\\lambda \\in [0, \\lambda_{\\max}]$ be an eigenvalue of $L$. We seek an affine transformation $\\tilde{\\lambda} = f(\\lambda) = a\\lambda + b$ such that the interval $[0, \\lambda_{\\max}]$ is mapped to $[-1, 1]$. This requires that the endpoints of the interval are mapped accordingly:\n- $f(0) = -1$\n- $f(\\lambda_{\\max}) = 1$\n\nSubstituting these conditions into the affine form yields a system of two linear equations for the coefficients $a$ and $b$:\n$$a(0) + b = -1 \\implies b = -1$$\n$$a(\\lambda_{\\max}) + b = 1 \\implies a\\lambda_{\\max} - 1 = 1 \\implies a\\lambda_{\\max} = 2 \\implies a = \\frac{2}{\\lambda_{\\max}}$$\nThe required affine transformation for the eigenvalues is thus $\\tilde{\\lambda} = \\frac{2}{\\lambda_{\\max}}\\lambda - 1$.\n\nBy the principles of functional calculus for matrices, applying this same transformation to the operator $L$ yields the rescaled operator $\\widetilde{L}$. If $L\\mathbf{u} = \\lambda\\mathbf{u}$ for an eigenvector $\\mathbf{u}$, then:\n$$\\left(\\frac{2}{\\lambda_{\\max}}L - I\\right)\\mathbf{u} = \\frac{2}{\\lambda_{\\max}}(L\\mathbf{u}) - I\\mathbf{u} = \\frac{2}{\\lambda_{\\max}}(\\lambda\\mathbf{u}) - \\mathbf{u} = \\left(\\frac{2\\lambda}{\\lambda_{\\max}} - 1\\right)\\mathbf{u} = \\tilde{\\lambda}\\mathbf{u}$$\nwhere $I$ is the $N \\times N$ identity matrix. This confirms that the eigenvalues of the new operator $\\widetilde{L}$ are the rescaled eigenvalues $\\tilde{\\lambda}$.\n\nThe rescaled operator is therefore:\n$$\\widetilde{L} = \\frac{2}{\\lambda_{\\max}}L - I$$\nThe spectrum of $\\widetilde{L}$, $\\sigma(\\widetilde{L})$, lies entirely within the interval $[-1, 1]$, as required.\n\n2. Derivation of the Chebyshev Approximation\n\nWe wish to approximate the function $g(\\lambda)$ for $\\lambda \\in [0, \\lambda_{\\max}]$. Using the inverse of the affine rescaling, $\\lambda(x) = \\frac{\\lambda_{\\max}}{2}(x+1)$, we define a corresponding function $\\tilde{g}(x)$ on the interval $[-1, 1]$:\n$$\\tilde{g}(x) = g(\\lambda(x)) = g\\left(\\frac{\\lambda_{\\max}}{2}(x+1)\\right)$$\nWe approximate $\\tilde{g}(x)$ using a truncated Chebyshev series of degree $K$. The Chebyshev series expansion is given by $\\tilde{g}(x) = \\sum_{k=0}^{\\infty} c_k T_k(x)$, where the coefficients $c_k$ are determined by exploiting the orthogonality of the Chebyshev polynomials $\\{T_k(x)\\}_{k \\ge 0}$ with respect to the weight function $w(x) = (1-x^2)^{-1/2}$ on $[-1, 1]$. The orthogonality relation is:\n$$\\int_{-1}^{1} T_k(x) T_j(x) \\frac{dx}{\\sqrt{1-x^2}} = \\begin{cases} 0 & \\text{if } k \\neq j \\\\ \\pi & \\text{if } k=j=0 \\\\ \\frac{\\pi}{2} & \\text{if } k=j > 0 \\end{cases}$$\nThe coefficients $c_k$ are the projections of $\\tilde{g}(x)$ onto the basis functions $T_k(x)$:\n$$c_k = \\frac{\\langle \\tilde{g}, T_k \\rangle_w}{\\langle T_k, T_k \\rangle_w} = \\frac{\\int_{-1}^{1} \\tilde{g}(x) T_k(x) (1-x^2)^{-1/2} dx}{\\int_{-1}^{1} T_k(x)^2 (1-x^2)^{-1/2} dx}$$\nThis gives separate expressions for $k=0$ and $k>0$:\n$$c_0 = \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{\\tilde{g}(x)}{\\sqrt{1-x^2}} dx$$\n$$c_k = \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{\\tilde{g}(x) T_k(x)}{\\sqrt{1-x^2}} dx \\quad \\text{for } k \\ge 1$$\nTo express these coefficients in terms of the original function $g$ on the interval $[0, \\lambda_{\\max}]$, we perform a change of variables in the integrals. Let $\\lambda' = \\frac{\\lambda_{\\max}}{2}(x+1)$, which implies $x = \\frac{2\\lambda'}{\\lambda_{\\max}}-1$. The differential is $d\\lambda' = \\frac{\\lambda_{\\max}}{2}dx$, so $dx = \\frac{2}{\\lambda_{\\max}}d\\lambda'$. The weight function term becomes:\n$$\\sqrt{1-x^2} = \\sqrt{1 - \\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)^2} = \\sqrt{\\frac{4\\lambda'(\\lambda_{\\max}-\\lambda')}{\\lambda_{\\max}^2}} = \\frac{2}{\\lambda_{\\max}}\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}$$\nSubstituting these into the integral for $c_k$ (for $k \\ge 1$):\n$$c_k = \\frac{2}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\frac{2}{\\lambda_{\\max}}\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} \\left(\\frac{2}{\\lambda_{\\max}} d\\lambda'\\right) = \\frac{2}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda'$$\nA similar substitution for $c_0$ yields:\n$$c_0 = \\frac{1}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda')}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda'$$\nThe degree-$K$ polynomial approximation of $g(\\lambda)$ is then the truncated series evaluated at the rescaled eigenvalue:\n$$p_K(\\lambda) = \\sum_{k=0}^{K} c_k T_k\\left(\\frac{2\\lambda}{\\lambda_{\\max}}-1\\right)$$\nBy functional calculus, the matrix polynomial approximation $p_K(L)$ of the graph filter $g(L)$ is obtained by substituting the operator $L$ for the scalar $\\lambda$:\n$$p_K(L) = \\sum_{k=0}^{K} c_k T_k\\left(\\frac{2}{\\lambda_{\\max}}L - I\\right) = \\sum_{k=0}^{K} c_k T_k(\\widetilde{L})$$\nSubstituting the integral expressions for the coefficients $c_k$ yields the final analytical form for the approximation.\n\n3. Justification for Numerical Stability\n\nThe use of the rescaled operator $\\widetilde{L}$ is critical for the numerical stability of the computation. The polynomial approximation $p_K(L)$ is evaluated in practice not by forming powers of $\\widetilde{L}$ but by using the three-term recurrence relation for Chebyshev polynomials:\n$$T_{k+1}(x) = 2x T_k(x) - T_{k-1}(x), \\quad \\text{with } T_0(x)=1, T_1(x)=x$$\nThis recurrence can be applied to the operator $\\widetilde{L}$ to generate the matrices $T_k(\\widetilde{L})$ or, more efficiently, used in algorithms like the Clenshaw algorithm to compute the action of $p_K(L)$ on a vector.\n\nThe stability of this recurrence hinges on the magnitude of its argument. For an argument $x$ such that $|x| \\le 1$, the Chebyshev polynomials are bounded: $|T_k(x)| \\le 1$ for all $k \\ge 0$. By rescaling the operator $L$ to $\\widetilde{L}$, we ensure all its eigenvalues $\\tilde{\\lambda}_i$ are in $[-1, 1]$. Consequently, the eigenvalues of each matrix $T_k(\\widetilde{L})$, which are $T_k(\\tilde{\\lambda}_i)$, are also bounded in magnitude by $1$. Since $T_k(\\widetilde{L})$ is a symmetric matrix, its spectral norm is equal to its spectral radius, so $\\|T_k(\\widetilde{L})\\|_2 \\le 1$. This prevents the magnitudes of the matrices and vectors involved in the recurrence from growing exponentially, thus avoiding numerical overflow and instability.\n\nWithout rescaling, we would evaluate $T_k(\\lambda)$ for $\\lambda \\in [0, \\lambda_{\\max}]$. If $\\lambda_{\\max} > 1$, for any eigenvalue $\\lambda_i > 1$, $T_k(\\lambda_i)$ grows exponentially with $k$, leading to catastrophic numerical errors. The affine rescaling is therefore not merely a convenience but a fundamental requirement for a stable computation.", "answer": "$$\\boxed{\\sum_{k=0}^{K} \\left( \\frac{2-\\delta_{k0}}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda' \\right) T_k\\left(\\frac{2}{\\lambda_{\\max}}L-I\\right)}$$", "id": "2903956"}]}