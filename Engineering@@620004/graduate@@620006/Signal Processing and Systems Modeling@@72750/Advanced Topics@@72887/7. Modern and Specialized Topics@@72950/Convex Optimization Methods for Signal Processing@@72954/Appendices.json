{"hands_on_practices": [{"introduction": "First-order optimization methods form the bedrock of modern large-scale signal processing. Projected Gradient Descent (PGD) is a canonical example, providing a straightforward blueprint for solving constrained problems by iteratively taking a step in the negative gradient direction and projecting the result back onto the feasible set. This practice [@problem_id:2861550] guides you through deriving the PGD algorithm from first principles for a constrained least-squares problem, a task that reinforces key concepts such as gradient computation, Lipschitz continuity, and the derivation of a projection operator via Karush-Kuhn-Tucker (KKT) conditions. Mastering these steps is essential for tackling more complex constrained optimization scenarios.", "problem": "Consider the constrained least-squares problem in a Euclidean signal model: minimize the residual energy subject to an energy budget on the signal,\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{2} \\le R,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a known sensing matrix, $b \\in \\mathbb{R}^{m}$ is observed data, and $R \\in \\mathbb{R}_{++}$ is a prescribed radius. Let $f(x) \\triangleq \\|A x - b\\|_{2}^{2}$. Work from first principles of convex analysis, differentiability, and the Karush–Kuhn–Tucker (KKT) conditions, and do not assume any pre-derived optimization formulas.\n\n- Using the definitions of convexity and gradient Lipschitz continuity, derive the gradient $\\nabla f(x)$ and a valid global Lipschitz constant $L$ for $\\nabla f$ expressed in terms of the spectral norm of $A$.\n\n- Formulate the Projected Gradient Descent (PGD) method, defined as gradient descent followed by projection onto the feasible set, with a constant step size $t \\in (0, 1/L]$. Write the PGD iteration in terms of the Euclidean projection $\\Pi_{\\mathbb{B}_{2}(R)}(\\cdot)$ onto the closed $\\ell_{2}$-ball $\\mathbb{B}_{2}(R) \\triangleq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{2} \\le R\\}$.\n\n- Starting from the definition of Euclidean projection and the KKT conditions applied to a norm-ball constraint, derive the closed-form expression for $\\Pi_{\\mathbb{B}_{2}(R)}(y)$ for an arbitrary $y \\in \\mathbb{R}^{n}$. Your derivation must begin from the optimization problem that defines the projection and proceed via stationarity, complementary slackness, and feasibility.\n\n- Combine your results to produce a single closed-form analytic expression (no case splits) for one PGD update $x^{k+1}$ in terms of $A$, $b$, $R$, $t$, and $x^{k}$. Express your final update using only standard arithmetic operations, the Euclidean norm, and the $\\max$ or $\\min$ operator. Provide this explicit expression as your final answer. Do not include an equality sign in the final answer. No numerical evaluation is required.", "solution": "The problem presented is a standard constrained quadratic program fundamental to signal processing and optimization theory. It is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The problem is therefore valid. We proceed with the derivation as requested, from first principles.\n\nThe objective function is $f(x) \\triangleq \\|A x - b\\|_{2}^{2}$, where $x \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^{m}$. The constraint is $\\|x\\|_{2} \\le R$ for some $R > 0$.\n\nFirst, we derive the gradient of $f(x)$ and its Lipschitz constant.\nThe function can be written as $f(x) = (A x - b)^{T}(A x - b) = x^{T}A^{T}Ax - 2b^{T}Ax + b^{T}b$.\nTo find the gradient, we consider a small perturbation $h \\in \\mathbb{R}^{n}$:\n$$\n\\begin{aligned}\nf(x+h) &= (A(x+h) - b)^{T}(A(x+h) - b) \\\\\n&= ((Ax - b) + Ah)^{T}((Ax - b) + Ah) \\\\\n&= (Ax - b)^{T}(Ax - b) + 2(Ax - b)^{T}(Ah) + (Ah)^{T}(Ah) \\\\\n&= f(x) + 2(A^{T}(Ax - b))^{T}h + \\|Ah\\|_{2}^{2}\n\\end{aligned}\n$$\nThe first-order approximation of $f(x+h)$ is $f(x) + \\langle \\nabla f(x), h \\rangle$. By comparison, the gradient is the vector that satisfies this relation. Thus, we identify the gradient as:\n$$\n\\nabla f(x) = 2 A^{T}(A x - b)\n$$\nNext, we find a global Lipschitz constant $L$ for this gradient. The gradient $\\nabla f(x)$ is Lipschitz continuous if there exists a constant $L$ such that for any $x, y \\in \\mathbb{R}^{n}$, the inequality $\\|\\nabla f(x) - \\nabla f(y)\\|_{2} \\le L \\|x - y\\|_{2}$ holds.\nLet us compute the difference:\n$$\n\\begin{aligned}\n\\nabla f(x) - \\nabla f(y) &= \\left(2 A^{T}(A x - b)\\right) - \\left(2 A^{T}(A y - b)\\right) \\\\\n&= 2 A^{T}A x - 2 A^{T}b - 2 A^{T}A y + 2 A^{T}b \\\\\n&= 2 A^{T}A (x - y)\n\\end{aligned}\n$$\nNow we take the Euclidean norm of both sides:\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_{2} = \\|2 A^{T}A (x - y)\\|_{2}\n$$\nBy the definition of an induced matrix norm (specifically, the spectral norm $\\|\\cdot\\|_{2}$), we have $\\|M z\\|_{2} \\le \\|M\\|_{2} \\|z\\|_{2}$. Applying this inequality:\n$$\n\\|2 A^{T}A (x - y)\\|_{2} \\le 2 \\|A^{T}A\\|_{2} \\|x - y\\|_{2}\n$$\nA property of the spectral norm is that $\\|A^{T}A\\|_{2} = \\|A\\|_{2}^{2}$. Therefore,\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_{2} \\le 2 \\|A\\|_{2}^{2} \\|x - y\\|_{2}\n$$\nFrom this inequality, we can identify a valid global Lipschitz constant for $\\nabla f(x)$ as $L = 2 \\|A\\|_{2}^{2}$.\n\nSecond, we formulate the Projected Gradient Descent (PGD) iteration. PGD consists of a standard gradient descent step followed by a projection onto the feasible set. Given a step size $t \\in (0, 1/L]$, the update rule is:\n1. Gradient descent step: $y^{k+1} = x^{k} - t \\nabla f(x^{k})$.\n2. Projection step: $x^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)}(y^{k+1})$.\nHere, $\\mathbb{B}_{2}(R) = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{2} \\le R\\}$ is the closed Euclidean ball of radius $R$. Substituting the expression for the gradient, the iteration is:\n$$\nx^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)} \\left( x^{k} - 2t A^{T}(A x^{k} - b) \\right)\n$$\n\nThird, we derive the closed-form expression for the Euclidean projection $\\Pi_{\\mathbb{B}_{2}(R)}(y)$ for an arbitrary point $y \\in \\mathbb{R}^{n}$. The projection is defined as the solution to the following convex optimization problem:\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = \\arg \\min_{z \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|z - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|z\\|_{2} \\le R\n$$\nThe constraint can be written as $g(z) = \\|z\\|_{2}^{2} - R^{2} \\le 0$. This is a convex problem, so the Karush–Kuhn–Tucker (KKT) conditions are necessary and sufficient for optimality. The Lagrangian is:\n$$\n\\mathcal{L}(z, \\lambda) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda (\\|z\\|_{2}^{2} - R^{2})\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier. The KKT conditions are:\n1.  **Stationarity**: $\\nabla_z \\mathcal{L}(z, \\lambda) = 0$.\n    $$\n    \\nabla_z \\left( \\frac{1}{2} (z^{T}z - 2y^{T}z + y^{T}y) + \\lambda (z^{T}z - R^{2}) \\right) = z - y + 2\\lambda z = 0\n    $$\n    This implies $(1 + 2\\lambda)z = y$, or $z = \\frac{y}{1 + 2\\lambda}$.\n2.  **Primal Feasibility**: $\\|z\\|_{2}^{2} - R^{2} \\le 0$.\n3.  **Dual Feasibility**: $\\lambda \\ge 0$.\n4.  **Complementary Slackness**: $\\lambda (\\|z\\|_{2}^{2} - R^{2}) = 0$.\n\nWe analyze the complementary slackness condition. Two cases arise:\nCase 1: $\\|y\\|_{2} \\le R$. We can test if $\\lambda = 0$ provides a valid solution. If $\\lambda = 0$, stationarity gives $z = y$. Primal feasibility is satisfied, as $\\|z\\|_{2} = \\|y\\|_{2} \\le R$. Dual feasibility ($\\lambda \\ge 0$) is satisfied. Complementary slackness is also satisfied, as $\\lambda=0$. Thus, if $y$ is already inside or on the boundary of the ball, the projection is $y$ itself.\n\nCase 2: $\\|y\\|_{2} > R$. In this case, the solution $z = y$ is not feasible. Therefore, the constraint must be active, meaning $\\|z\\|_{2} = R$. By complementary slackness, this implies $\\lambda > 0$. From stationarity, we have $z = y / (1 + 2\\lambda)$. Taking the norm:\n$$\n\\|z\\|_{2} = \\left\\| \\frac{y}{1 + 2\\lambda} \\right\\|_{2} = \\frac{\\|y\\|_{2}}{1 + 2\\lambda}\n$$\nSince $\\|z\\|_{2} = R$, we have $R = \\frac{\\|y\\|_{2}}{1 + 2\\lambda}$, which gives $1 + 2\\lambda = \\frac{\\|y\\|_{2}}{R}$. Substituting this back into the expression for $z$:\n$$\nz = \\frac{y}{\\|y\\|_{2} / R} = R \\frac{y}{\\|y\\|_{2}}\n$$\nThis solution corresponds to radially scaling the point $y$ back to the boundary of the ball.\n\nCombining these two cases, the projection operator is:\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = \\begin{cases} y & \\text{if } \\|y\\|_{2} \\le R \\\\ R \\frac{y}{\\|y\\|_{2}} & \\text{if } \\|y\\|_{2} > R \\end{cases}\n$$\nThis can be written in a single closed-form expression using the $\\max$ operator, which is well-defined even for $y=0$ (since $R>0$):\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = y \\frac{R}{\\max(R, \\|y\\|_{2})}\n$$\n\nFinally, we combine all results to produce the full PGD update expression. Let $y^{k+1} = x^{k} - 2t A^{T}(A x^{k} - b)$. The next iterate $x^{k+1}$ is the projection of $y^{k+1}$ onto $\\mathbb{B}_{2}(R)$:\n$$\nx^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)}(y^{k+1}) = y^{k+1} \\frac{R}{\\max(R, \\|y^{k+1}\\|_{2})}\n$$\nSubstituting the expression for $y^{k+1}$, we obtain the final explicit update rule:\n$$\n(x^{k} - 2tA^{T}(Ax^{k} - b)) \\frac{R}{\\max(R, \\|x^{k} - 2tA^{T}(Ax^{k} - b)\\|_{2})}\n$$\nThis expression depends only on the current iterate $x^{k}$, the problem data $A, b, R$, and the step size $t$, and uses only standard matrix-vector operations, norms, and the scalar $\\max$ function as required.", "answer": "$$\n\\boxed{\n(x^{k} - 2tA^{T}(Ax^{k} - b)) \\frac{R}{\\max(R, \\|x^{k} - 2tA^{T}(Ax^{k} - b)\\|_{2})}\n}\n$$", "id": "2861550"}, {"introduction": "Moving beyond simple constraints, many signal processing problems involve minimizing a smooth data fidelity term plus a non-smooth regularizer. Proximal gradient methods generalize Projected Gradient Descent to this setting, replacing the projection with a more powerful tool: the proximal operator. This exercise [@problem_id:2861514] focuses on deriving the proximal operator for the group lasso penalty, a regularizer that promotes structured, group-level sparsity. By using subdifferential calculus to derive the closed-form \"block soft-thresholding\" operator, you will gain hands-on experience with the core mathematical machinery behind many state-of-the-art optimization algorithms.", "problem": "Consider a linear inverse problem in signal processing where one seeks a structured sparse estimate $x \\in \\mathbb{R}^{n}$ partitioned into $G$ nonoverlapping groups $\\{x_{g}\\}_{g=1}^{G}$ according to a fixed index partition of $\\{1,\\dots,n\\}$. Let the regularizer be the weighted group-lasso penalty $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$ with strictly positive weights $w_{g} > 0$. In many first-order convex optimization methods based on proximal splitting, one repeatedly evaluates the proximal operator (prox) of $R$, defined for any $\\lambda > 0$ and any $y \\in \\mathbb{R}^{n}$ by\n$$\n\\operatorname{prox}_{\\lambda R}(y) \\triangleq \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}.\n$$\nStarting from the definition of the proximal operator and the subdifferential optimality condition for convex functions, derive the block soft-thresholding operator associated with $R$, and give its closed-form expression for each group $g \\in \\{1,\\dots,G\\}$ in terms of $y_{g}$, $\\lambda$, and $w_{g}$. Your final answer must be a single closed-form analytic expression for $[\\operatorname{prox}_{\\lambda R}(y)]_{g}$ that is valid for all $y_{g} \\in \\mathbb{R}^{|g|}$ and all $g \\in \\{1,\\dots,G\\}$. Do not report an inequality or an equation to be solved; provide the explicit expression. No numerical rounding is required.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in convex optimization for signal processing. No flaws are identified. The derivation of the solution can proceed.\n\nThe objective is to find the closed-form expression for the proximal operator of the weighted group-lasso penalty $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$. The proximal operator is defined as the solution $z^*$ to the following minimization problem:\n$$\nz^{*} = \\operatorname{prox}_{\\lambda R}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\nLet the objective function be $F(z) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda R(z)$. The squared Euclidean norm can be decomposed over the non-overlapping groups:\n$$\n\\|z - y\\|_{2}^{2} = \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2\n$$\nSubstituting this into the objective function, we obtain:\n$$\nF(z) = \\frac{1}{2} \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2 + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} = \\sum_{g=1}^{G} \\left( \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right)\n$$\nThe total objective function $F(z)$ is a sum of functions, where each term only depends on a single group of variables $z_g$. This property is known as separability. Consequently, the minimization of $F(z)$ over $z \\in \\mathbb{R}^n$ can be performed by minimizing each term of the sum independently over the corresponding group variables $z_g \\in \\mathbb{R}^{|g|}$.\n\nFor each group $g \\in \\{1,\\dots,G\\}$, the solution for the corresponding block, denoted $z_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g$, is given by:\n$$\nz_g^* = \\arg\\min_{z_g \\in \\mathbb{R}^{|g|}} \\left\\{ J(z_g) \\triangleq \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\nThe function $J(z_g)$ is convex, being the sum of a strictly convex function (the quadratic term) and a convex function (the $\\ell_2$-norm term). Thus, a unique minimizer $z_g^*$ exists. The optimality condition for a convex function states that $z_g^*$ is a minimizer if and only if the zero vector is an element of the subdifferential of $J$ at $z_g^*$, i.e., $0 \\in \\partial J(z_g^*)$.\n\nThe function $J(z_g)$ is composed of two terms. The first term, $\\frac{1}{2}\\|z_g - y_g\\|_2^2$, is differentiable, and its gradient is $z_g - y_g$. The second term is $\\lambda w_g \\|z_g\\|_2$. The subdifferential of the $\\ell_2$-norm, $\\|z_g\\|_2$, is:\n$$\n\\partial \\|z_g\\|_2 =\n\\begin{cases}\n\\{ \\frac{z_g}{\\|z_g\\|_2} \\} & \\text{if } z_g \\neq 0 \\\\\n\\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\} & \\text{if } z_g = 0\n\\end{cases}\n$$\nUsing the sum rule for subdifferentials (as one function is differentiable), we have $\\partial J(z_g) = (z_g - y_g) + \\lambda w_g \\partial \\|z_g\\|_2$. The optimality condition $0 \\in \\partial J(z_g^*)$ becomes:\n$$\n0 \\in (z_g^* - y_g) + \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\nThis can be rewritten as:\n$$\ny_g - z_g^* \\in \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\nWe now analyze two cases based on the value of $z_g^*$.\n\nCase 1: $z_g^* \\neq 0$.\nIn this case, the subdifferential $\\partial \\|z_g^*\\|_2$ is the singleton set $\\{ \\frac{z_g^*}{\\|z_g^*\\|_2} \\}$. The optimality condition becomes an equality:\n$$\ny_g - z_g^* = \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2}\n$$\nRearranging the terms to solve for $y_g$:\n$$\ny_g = z_g^* + \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2} = z_g^* \\left( 1 + \\frac{\\lambda w_g}{\\|z_g^*\\|_2} \\right)\n$$\nFrom this equation, we see that $y_g$ is a positive scaling of $z_g^*$. This implies that $z_g^*$ must be collinear with $y_g$ and point in the same direction. Therefore, we have $\\frac{z_g^*}{\\|z_g^*\\|_2} = \\frac{y_g}{\\|y_g\\|_2}$. Note that this requires $y_g \\neq 0$. Substituting this back into the equation for $y_g - z_g^*$:\n$$\nz_g^* = y_g - \\lambda w_g \\frac{y_g}{\\|y_g\\|_2} = \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g\n$$\nSince we assumed $z_g^* \\neq 0$, its norm must be positive, $\\|z_g^*\\|_2 > 0$. Taking the norm of the expression for $z_g^*$:\n$$\n\\|z_g^*\\|_2 = \\left\\| \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g \\right\\|_2 = \\left| 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right| \\|y_g\\|_2\n$$\nAs $z_g^*$ and $y_g$ are in the same direction, the scalar factor must be positive: $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} > 0$. This implies $\\|y_g\\|_2 > \\lambda w_g$. If this condition holds, our assumption $z_g^* \\neq 0$ is consistent, and the solution is as derived.\n\nCase 2: $z_g^* = 0$.\nIn this case, the subdifferential is the closed unit ball: $\\partial \\|z_g^*\\|_2 = \\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\}$. The optimality condition becomes:\n$$\ny_g - 0 \\in \\lambda w_g \\{ v \\mid \\|v\\|_2 \\le 1 \\}\n$$\nThis is equivalent to stating that $y_g$ is in the ball of radius $\\lambda w_g$:\n$$\n\\|y_g\\|_2 \\le \\lambda w_g\n$$\nIf this condition is met, the minimizer is $z_g^* = 0$.\n\nCombining the two cases, we have:\n$$\nz_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g =\n\\begin{cases}\n\\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g & \\text{if } \\|y_g\\|_2 > \\lambda w_g \\\\\n0 & \\text{if } \\|y_g\\|_2 \\le \\lambda w_g\n\\end{cases}\n$$\nThis piecewise expression can be written compactly as a single formula using the maximum function. The shrinkage factor $\\left(1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right)$ is thresholded at $0$.\n$$\nz_g^* = \\max\\left\\{0, 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right\\} y_g\n$$\nThis expression is the closed-form representation of the block soft-thresholding operator. It is well-defined for all $y_g \\in \\mathbb{R}^{|g|}$. Specifically, if $y_g = 0$, then $\\|y_g\\|_2 = 0$. The term $\\frac{\\lambda w_g}{\\|y_g\\|_2}$ becomes infinite since $\\lambda w_g > 0$. The argument of the max function, $1 - \\infty$, evaluates to $-\\infty$. Thus, $\\max\\{0, -\\infty\\} = 0$, and the result is $z_g^* = 0 \\cdot y_g = 0$, which is consistent with our derivation.\nFor any non-zero $y_g$ such that $\\|y_g\\|_2 \\le \\lambda w_g$, the term $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}$ is non-positive, so the max function returns $0$, resulting in $z_g^*=0$. For $\\|y_g\\|_2 > \\lambda w_g$, the scaling factor is positive, and the expression yields the correct shrinkage.", "answer": "$$\n\\boxed{\\max\\left\\{0, 1 - \\frac{\\lambda w_{g}}{\\|y_{g}\\|_{2}}\\right\\} y_{g}}\n$$", "id": "2861514"}, {"introduction": "While gradient-based methods update all variables simultaneously, an alternative and powerful strategy is to optimize the objective one coordinate at a time. This approach, known as coordinate descent, can be remarkably efficient for problems with separable regularization, such as the celebrated LASSO. This hands-on practice [@problem_id:2861565] challenges you to both derive the coordinate-wise update rule for LASSO from first principles and implement the complete algorithm. By moving from theoretical derivation to a working, verifiable solver, you will solidify your understanding of this important class of optimizers and their practical application in sparse signal recovery.", "problem": "You are tasked with deriving and implementing a cyclic coordinate descent algorithm for the Least Absolute Shrinkage and Selection Operator (LASSO) problem using principles from convex optimization and signal processing.\n\nConsider the LASSO objective for a design matrix $A \\in \\mathbb{R}^{m \\times n}$ and observation vector $b \\in \\mathbb{R}^{m}$:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a given regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm.\n\nYour tasks are:\n1. Derive, from first principles, the single-coordinate minimization update rule used in a cyclic coordinate descent method for $f(x)$. Start from the definition of $f(x)$ and the subgradient optimality condition for the $\\ell_1$ norm, and reason about the minimization of $f(x)$ with respect to a single coordinate $x_i$ while holding all other coordinates fixed. Use only foundational facts including properties of convex functions, subgradients of the absolute value, and basic linear algebra. Do not assume any specific closed-form update a priori.\n2. Show that the coordinate-wise minimizer is obtained by applying the soft-thresholding operator to an affine function of the current iterate and the residual. Clearly define all quantities introduced in your derivation.\n3. Implement a cyclic coordinate descent algorithm that uses the derived update. Your implementation must:\n   - Maintain the residual $r \\triangleq b - A x$ and update it incrementally after each coordinate update to achieve $\\mathcal{O}(m)$ cost per coordinate update.\n   - Use the soft-thresholding operator defined by $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$.\n   - Terminate when either the maximum absolute change in any coordinate during a full cycle is less than a tolerance $\\varepsilon$ or a maximum number of epochs is reached.\n   - Return the final iterate $x$ and, when requested, the sequence of objective values at the end of each epoch to assess monotonicity.\n\nFoundational base you may use:\n- Convexity of $\\|\\cdot\\|_2^2$ and $\\|\\cdot\\|_1$ and properties of their subgradients.\n- Subgradient optimality condition: $0 \\in \\partial f(x^\\star)$ at an optimum $x^\\star$ of a convex function $f$.\n- The subdifferential of the absolute value: for $t \\in \\mathbb{R}$, $\\partial |t| = \\{\\mathrm{sign}(t)\\}$ if $t \\ne 0$, and $\\partial |t| = [-1,1]$ if $t = 0$.\n- Linear algebra identities for residual updates.\n\nDefine the objective value as:\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\nTest suite:\nImplement your program to run the following five test cases and aggregate the results into a single output line.\n\n- Test 1 (orthonormal columns, analytical check): Set $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, $\\lambda = 0.7$. Run your coordinate descent to obtain $x_{\\mathrm{cd}}$. The known analytical solution for orthonormal columns is $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$. Output the scalar\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- Test 2 (general tall system, Karush–Kuhn–Tucker (KKT) check): Generate $A \\in \\mathbb{R}^{60 \\times 30}$ with independent standard normal entries and then normalize each column to have unit $\\ell_2$ norm. Use a fixed pseudorandom seed $0$ to make the instance deterministic. Define $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ with nonzero entries at indices $0,5,10,15,20$ with values $[2.5,-1.7,1.2,-0.9,1.8]$ respectively, and zeros elsewhere. Set $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\in \\mathbb{R}^{60}$ has independent normal entries with standard deviation $0.01$ generated with the same seed $0$. Let $\\lambda = 0.05$. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Verify the KKT conditions for the LASSO: letting $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$,\n  - If $x_{\\mathrm{cd},i} \\ne 0$, then $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$.\n  - If $x_{\\mathrm{cd},i} = 0$, then $|g_i| \\le \\lambda$.\n  Because of numerical error, implement a tolerance of $10^{-4}$ in these checks. Output the boolean $b_2$ indicating whether all coordinates satisfy the KKT conditions within tolerance.\n\n- Test 3 (large regularization drives solution to zero): Use $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, and $\\lambda = 10^6$. Output the boolean $b_3$ indicating whether the returned solution is the zero vector within an absolute tolerance of $10^{-12}$.\n\n- Test 4 (zero regularization reduces to least squares): Generate $A \\in \\mathbb{R}^{40 \\times 10}$ with independent standard normal entries using pseudorandom seed $1$. Generate $b \\in \\mathbb{R}^{40}$ with independent standard normal entries using seed $2$. Let $\\lambda = 0$. Let $x_{\\mathrm{ls}}$ denote the least-squares solution minimizing $\\frac{1}{2}\\|A x - b\\|_2^2$, computed by the standard linear least-squares method. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Output the scalar\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- Test 5 (monotone descent of objective across epochs): Generate $A \\in \\mathbb{R}^{30 \\times 15}$ and $b \\in \\mathbb{R}^{30}$ with independent standard normal entries using pseudorandom seed $3$. Let $\\lambda = 0.1$. Record the objective after each full pass over all coordinates and verify that the sequence is nonincreasing up to numerical tolerance $10^{-10}$. Output the boolean $b_5$ indicating whether monotonicity holds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[e_1, b_2, b_3, e_4, b_5]$. No physical units are involved in this problem, and no angle units are relevant. All numeric outputs should be real numbers or booleans as specified, with no percentage signs. Your implementation must be robust to the given instances and should not require any user input.", "solution": "We start from the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$. The function $f$ is convex because it is the sum of the convex function $\\frac{1}{2}\\|A x - b\\|_2^2$ and the convex function $\\lambda \\|x\\|_1$.\n\nCyclic coordinate descent minimizes $f$ with respect to one coordinate at a time while holding the others fixed. Fix an index $i \\in \\{1,\\dots,n\\}$ and write $a_i \\in \\mathbb{R}^m$ for the $i$-th column of $A$. Let $x \\in \\mathbb{R}^n$ be the current iterate, and define the residual\n$$\nr \\triangleq b - A x.\n$$\nBecause $A x = \\sum_{j=1}^n a_j x_j$, changing only $x_i$ to a new value $t \\in \\mathbb{R}$ leads to a new vector $x^{(i \\leftarrow t)}$ and residual\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\nThe objective as a function of $t$ (with other coordinates fixed) becomes\n\\begin{align*}\n\\phi_i(t) &\\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n&= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{constant independent of } t \\\\\n&= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{constant}.\n\\end{align*}\nExpanding the squared norm using $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$, we get\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n&= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\nDiscarding the terms independent of $t$, the coordinate-wise objective reduces to the univariate convex function\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\nCompleting the square, define $d_i \\triangleq \\|a_i\\|_2^2$ and\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{when } d_i > 0.\n$$\nThen\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n&= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n&= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\nIgnoring the constant $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$, the minimization of $\\tilde{\\phi}_i(t)$ over $t$ is equivalent to minimizing\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\nThe subgradient optimality condition for this one-dimensional convex problem is\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\nwhere the subdifferential of the absolute value is $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$ if $t^\\star \\ne 0$, and $\\partial |t^\\star| = [-1, 1]$ if $t^\\star = 0$.\n\nConsider two cases.\n\nCase 1: $t^\\star \\ne 0$. Then the subgradient condition is\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\nThis implies $|c_i| > \\lambda/d_i$, and the solution is obtained by shrinking $c_i$ towards zero by $\\lambda/d_i$ while preserving sign:\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\nCase 2: $t^\\star = 0$. Then the subgradient condition becomes\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\nCombining both cases yields the soft-thresholding form\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\nEquivalently, using the residual definition $r = b - A x$, we have\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\nso the coordinate-wise minimizer is\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\nIf $d_i = \\|a_i\\|_2^2 = 0$ (a zero column), any change in $x_i$ does not affect the quadratic term; the minimizer of $\\lambda |t|$ is $t^\\star = 0$ for $\\lambda > 0$. In our implementation, we set $x_i \\leftarrow 0$ if $d_i = 0$ and $\\lambda > 0$; if $\\lambda = 0$ and $d_i = 0$, the coordinate is irrelevant and can be left unchanged.\n\nEfficient residual update: If $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$, then\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\nwhich costs $\\mathcal{O}(m)$ operations.\n\nConvergence and monotonicity: Each coordinate update exactly minimizes $f$ over that coordinate, so $f$ is nonincreasing after each coordinate update, and hence after each epoch (full pass over all coordinates). The algorithm terminates when the maximum absolute coordinate change in an epoch is below a tolerance or when a maximum number of epochs is reached.\n\nOptimality verification via Karush–Kuhn–Tucker (KKT) conditions: Let $g(x) \\triangleq A^\\top (A x - b)$ be the gradient of the smooth part. The KKT condition for optimality of $x^\\star$ in the LASSO is\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\nwhich is equivalent to the component-wise conditions\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0, & \\text{if } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda, & \\text{if } x_i^\\star = 0.\n\\end{cases}\n$$\nIn practice, we check these equalities and inequalities within a small numerical tolerance.\n\nTest cases and outputs: We implement the five test cases specified and compute\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ for orthonormal columns,\n- $b_2$ indicating KKT satisfaction within tolerance for the tall system,\n- $b_3$ indicating that the solution is zero for very large $\\lambda$,\n- $e_4$ the relative error to the least-squares solution when $\\lambda = 0$,\n- $b_5$ indicating monotone nonincreasing objective values across epochs.\n\nThe final program outputs the results as a single list $[e_1, b_2, b_3, e_4, b_5]$ on one line.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau <= 0:\n        return z\n    abs_z = abs(z)\n    if abs_z <= tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # r = b - A x, initially x=0\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd>0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # c_i = x_i + (a_i^T r) / d_i = (a_i^T r + d_i x_i) / d_i\n                ci = xi_old + float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n                max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta < tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| <= lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12:\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else:\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs <= 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}]}