{"hands_on_practices": [{"introduction": "Before applying Dynamic Mode Decomposition (DMD) to complex data, it is essential to grasp its fundamental requirements. This exercise explores the deep connection between a signal's intrinsic complexity, the algebraic properties of the data represented by a Hankel matrix, and the minimum embedding dimension needed to capture its dynamics. By deriving the minimal number of delays required to identify a simple sinusoid, you will build a foundational intuition for how and why delay-embedded DMD successfully reconstructs a system's state space from a single time series [@problem_id:2862877].", "problem": "You observe a real-valued, uniformly sampled time series generated by a single sinusoid with additive noise, \n$$y_k = A \\cos(\\omega k \\Delta t + \\phi) + \\eta_k,$$\nwhere $A \\neq 0$, $\\omega \\in \\mathbb{R}$, $\\Delta t > 0$, $\\phi \\in \\mathbb{R}$, and $\\{\\eta_k\\}$ is a zero-mean disturbance with finite variance. Assume the nondegenerate oscillatory case where the discrete-time radian frequency $\\theta := \\omega \\Delta t$ satisfies $\\theta \\not\\equiv 0 \\pmod{2\\pi}$ and $\\theta \\not\\equiv \\pi \\pmod{2\\pi}$. You build a Hankel matrix with $m$ delays from $N$ samples,\n$$\\mathbf{H} = \n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_{N-m} \\\\\ny_1 & y_2 & \\cdots & y_{N-m+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{m-1} & y_m & \\cdots & y_{N-1}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times (N-m+1)},$$\nand apply Hankel Dynamic Mode Decomposition (delay-embedded Dynamic Mode Decomposition), which seeks a linear map that advances the delay-embedded state by one time step. Starting only from the fundamental representation of sinusoids in terms of complex exponentials and the definition of a Hankel matrix, derive the minimal linear recurrence order satisfied by the noiseless signal and use it to determine the smallest number of delays $m$ required so that Hankel Dynamic Mode Decomposition can uniquely identify the oscillation frequency $\\omega$ in the limit of vanishing noise. Explicitly relate this minimal $m$ to the rank of the noiseless Hankel matrix. State any assumptions you use about distinctness of the underlying modes and sampling. Provide your final answer as the minimal integer $m$; no rounding is necessary and no units are required.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- The observed time series is $y_k = A \\cos(\\omega k \\Delta t + \\phi) + \\eta_k$.\n- Parameters are $A \\neq 0$, $\\omega \\in \\mathbb{R}$, $\\Delta t > 0$, $\\phi \\in \\mathbb{R}$.\n- The noise $\\{\\eta_k\\}$ is a zero-mean disturbance with finite variance.\n- The discrete-time radian frequency is defined as $\\theta := \\omega \\Delta t$.\n- A nondegenerate oscillatory case is assumed, where $\\theta \\not\\equiv 0 \\pmod{2\\pi}$ and $\\theta \\not\\equiv \\pi \\pmod{2\\pi}$.\n- A Hankel matrix $\\mathbf{H} \\in \\mathbb{R}^{m \\times (N-m+1)}$ is constructed from $N$ samples $\\{y_k\\}_{k=0}^{N-1}$ with $m$ delays. The matrix is given by:\n$$ \\mathbf{H} = \n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_{N-m} \\\\\ny_1 & y_2 & \\cdots & y_{N-m+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{m-1} & y_m & \\cdots & y_{N-1}\n\\end{bmatrix} $$\n- The task is to derive the minimal linear recurrence order for the noiseless signal, use it to find the smallest number of delays $m$ required for Hankel DMD to uniquely identify $\\omega$ in the limit of vanishing noise, and relate this minimal $m$ to the rank of the noiseless Hankel matrix.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the well-established fields of signal processing and system identification. The model of a sinusoid with additive noise is a canonical problem. The use of Hankel matrices and delay embedding for system identification is a fundamental technique (related to Takens' embedding theorem and methods like ESPRIT and DMD). The problem is objective, using precise mathematical language. The conditions on $\\theta$ are critical for ensuring the system is truly oscillatory with distinct modes, which is a necessary condition for a well-posed problem in this context. The question is not incomplete, as it requires the derivation of the intermediate step (minimal recurrence order) to reach the final answer. The problem is not trivial, as it requires a correct logical connection between the signal's structure, its recurrence relation, the rank of its associated Hankel matrix, and the minimum state-space dimension needed for identification. The problem is valid.\n\nProceeding with the solution.\n\nThe problem asks for the behavior in the limit of vanishing noise, so we analyze the noiseless signal $x_k = A \\cos(\\omega k \\Delta t + \\phi)$. First, we express this real-valued sinusoid as a sum of complex exponentials using Euler's formula, as instructed.\nLet $\\theta = \\omega \\Delta t$. The signal is:\n$$ x_k = A \\left( \\frac{\\exp(i(k \\theta + \\phi)) + \\exp(-i(k \\theta + \\phi))}{2} \\right) $$\n$$ x_k = \\left( \\frac{A \\exp(i\\phi)}{2} \\right) \\exp(ik\\theta) + \\left( \\frac{A \\exp(-i\\phi)}{2} \\right) \\exp(-ik\\theta) $$\nThis is a signal of the form $x_k = c_1 \\lambda_1^k + c_2 \\lambda_2^k$, where the modes are $\\lambda_1 = \\exp(i\\theta)$ and $\\lambda_2 = \\exp(-i\\theta)$, and the coefficients are $c_1 = \\frac{A}{2}\\exp(i\\phi)$ and $c_2 = \\overline{c_1}$. Since $A \\neq 0$, both $c_1$ and $c_2$ are nonzero.\n\nA signal that is a linear combination of $p$ distinct exponential terms satisfies a unique minimal-order linear constant-coefficient recurrence relation of order $p$. We must determine if the modes $\\lambda_1$ and $\\lambda_2$ are distinct. The modes are equal if $\\exp(i\\theta) = \\exp(-i\\theta)$, which implies $\\exp(i2\\theta) = 1$. This occurs if $2\\theta$ is an integer multiple of $2\\pi$, i.e., $2\\theta = 2n\\pi$ for $n \\in \\mathbb{Z}$, which simplifies to $\\theta = n\\pi$. The problem statement explicitly provides the nondegeneracy condition that $\\theta \\not\\equiv 0 \\pmod{2\\pi}$ and $\\theta \\not\\equiv \\pi \\pmod{2\\pi}$. This is equivalent to stating that $\\theta$ is not an integer multiple of $\\pi$. Therefore, $\\lambda_1 \\neq \\lambda_2$, and the modes are distinct.\n\nSince the signal is composed of two distinct modes, the minimal linear recurrence order is $p=2$. The characteristic polynomial for this recurrence has roots $\\lambda_1$ and $\\lambda_2$.\n$$ P(z) = (z - \\lambda_1)(z - \\lambda_2) = z^2 - (\\lambda_1 + \\lambda_2)z + \\lambda_1\\lambda_2 = 0 $$\nThe coefficients are:\n$$ \\lambda_1 + \\lambda_2 = \\exp(i\\theta) + \\exp(-i\\theta) = 2\\cos(\\theta) $$\n$$ \\lambda_1 \\lambda_2 = \\exp(i\\theta)\\exp(-i\\theta) = 1 $$\nThe characteristic polynomial is $z^2 - 2\\cos(\\theta)z + 1 = 0$. This corresponds to the linear recurrence relation:\n$$ x_{k+2} - 2\\cos(\\theta)x_{k+1} + x_k = 0 $$\nThis relation shows that any three consecutive samples of the noiseless signal are linearly dependent.\n\nNow, we relate this to the Hankel matrix. Let $\\mathbf{X}$ be the Hankel matrix for the noiseless signal $x_k$. Its columns are the delay vectors $\\mathbf{x}_j = [x_j, x_{j+1}, \\dots, x_{j+m-1}]^T$. Each column vector can be expressed as a linear combination of two fixed Vandermonde vectors associated with the modes:\n$$ \\mathbf{x}_j = c_1 \\lambda_1^j \\begin{pmatrix} 1 \\\\ \\lambda_1 \\\\ \\vdots \\\\ \\lambda_1^{m-1} \\end{pmatrix} + c_2 \\lambda_2^j \\begin{pmatrix} 1 \\\\ \\lambda_2 \\\\ \\vdots \\\\ \\lambda_2^{m-1} \\end{pmatrix} $$\nThe entire column space of $\\mathbf{X}$ is spanned by these two Vandermonde vectors. Since $\\lambda_1 \\neq \\lambda_2$, these two vectors are linearly independent provided $m \\ge 2$. According to Kronecker's theorem, the rank of an infinite Hankel matrix is equal to the number of distinct exponential modes in the generating signal. For a finite Hankel matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times (N-m+1)}$, the rank will be $2$ provided the dimensions are sufficient, i.e., $m \\ge 2$ and the number of columns $N-m+1 \\ge 2$.\n\nHankel DMD aims to find a linear operator that models the evolution of the delay-embedded state $\\mathbf{x}_k \\in \\mathbb{R}^{m}$. The dimension of this delay vector, $m$, must be large enough to embed the dynamics. To capture a system whose underlying state space has dimension $r$, the embedding dimension $m$ must be at least $r$. Here, the rank of the noiseless data matrix is $r=2$, representing a two-dimensional system (one complex-conjugate pair of modes). Therefore, the number of delays $m$ must be at least $2$.\n\nLet's check the sufficiency.\nIf $m=1$, the state is a scalar $x_k$. The model is $x_{k+1} = a x_k$. This can only model a single real mode, not a complex-conjugate pair that generates oscillations. So, $m=1$ is insufficient.\n\nIf $m=2$, the state is $\\mathbf{x}_k = [x_k, x_{k+1}]^T$. The one-step evolution is $\\mathbf{x}_{k+1} = [x_{k+1}, x_{k+2}]^T$. We seek an operator $\\mathbf{A}$ such that $\\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k$.\n$$ \\begin{pmatrix} x_{k+1} \\\\ x_{k+2} \\end{pmatrix} = \\mathbf{A} \\begin{pmatrix} x_k \\\\ x_{k+1} \\end{pmatrix} $$\nThe first row implies $x_{k+1} = A_{11}x_k + A_{12}x_{k+1}$, which requires $A_{11}=0$ and $A_{12}=1$.\nThe second row is $x_{k+2} = A_{21}x_k + A_{22}x_{k+1}$. From our recurrence relation, we know $x_{k+2} = -x_k + 2\\cos(\\theta)x_{k+1}$.\nThus, the unique linear operator is the companion matrix:\n$$ \\mathbf{A} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 2\\cos(\\theta) \\end{pmatrix} $$\nThe eigenvalues of this operator $\\mathbf{A}$ are found from the characteristic equation $\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\lambda^2 - 2\\cos(\\theta)\\lambda + 1 = 0$, which yields the solutions $\\lambda = \\exp(\\pm i\\theta)$. These are precisely the modes of the underlying system.\nHankel DMD, operating on data matrices formed from $\\mathbf{x}_k$, will compute an approximation of this matrix $\\mathbf{A}$. The eigenvalues of the computed operator will approximate $\\exp(\\pm i\\theta)$, from which $\\theta = \\omega\\Delta t$ and consequently the frequency $\\omega$ can be uniquely determined (up to a sign, which is physically ambiguous without further information).\n\nTherefore, $m=2$ is sufficient to capture the dynamics. Since $m=1$ is insufficient, the minimal required number of delays is $m=2$. This minimal value for $m$ is equal to the minimal linear recurrence order of the signal and also to the rank of the noiseless Hankel matrix, assuming sufficient data. The assumption on sampling is that the number of samples $N$ is large enough for the matrices used in DMD to have the correct rank, which minimally requires $N-m+1 \\geq 2$. For $m=2$, this means $N \\geq 3$, a trivial requirement.", "answer": "$$\n\\boxed{2}\n$$", "id": "2862877"}, {"introduction": "Real-world data is inevitably corrupted by noise, and a crucial skill for any scientist or engineer is to understand how this noise impacts a model's accuracy. This practice challenges you to use perturbation analysis to quantify how measurement noise propagates into uncertainty in the frequencies identified by DMD. Completing this exercise will equip you with a powerful quantitative tool that connects data quality ($SNR$), data quantity (window length $m$), and the expected precision of your model, a vital consideration for designing experiments and interpreting results [@problem_id:2862855].", "problem": "You are given a single-mode discrete-time linear dynamical system observed in additive noise. The underlying clean state sequence is modeled as a complex exponential with constant magnitude and a single angular frequency. The data matrices used for Dynamic Mode Decomposition (DMD) are constructed from equispaced time samples of this state as follows.\n\nAssume the clean signal is given by $x_k = A \\, e^{i (\\omega k \\Delta t + \\phi_0)}$ for $k \\in \\{0,1,2,\\dots\\}$, where $A > 0$ is an unknown constant amplitude, $\\omega$ is the true angular frequency in radians per second, $\\Delta t$ is the sampling interval in seconds, and $\\phi_0$ is an arbitrary initial phase. The discrete-time eigenvalue is $\\lambda = e^{i \\theta}$ with $\\theta = \\omega \\Delta t$. The observed data are generated by additive complex-valued white Gaussian noise $\\eta_k$ with zero mean and variance $\\sigma^2$, so that the measured snapshots are $\\tilde{x}_k = x_k + \\eta_k$. Signal-to-noise ratio (SNR) is defined as $\\mathrm{SNR} = A^2 / \\sigma^2$, a dimensionless, linear-scale quantity. Consider the rank-$1$ DMD estimator that uses a window of $m$ consecutive snapshot pairs to form\n$$\nX = \\begin{bmatrix} \\tilde{x}_0 & \\tilde{x}_1 & \\dots & \\tilde{x}_{m-1} \\end{bmatrix}, \\quad\nY = \\begin{bmatrix} \\tilde{x}_1 & \\tilde{x}_2 & \\dots & \\tilde{x}_m \\end{bmatrix},\n$$\nand computes the least-squares DMD eigenvalue estimate $\\hat{\\lambda}$ by $\\hat{\\lambda} = (X^* Y)/(X^* X)$, where $X^*$ denotes conjugate transpose.\n\nYour tasks are:\n\n- Starting from the above definitions, small-noise linearization, and properties of additive white Gaussian noise, derive the leading-order dependence of the variance of the DMD eigenvalue phase error $\\delta \\theta = \\arg(\\hat{\\lambda}) - \\theta$ on the window length $m$ and the signal-to-noise ratio $\\mathrm{SNR}$. Your derivation must be principle-based, beginning from the least-squares estimator and noise perturbation without invoking any pre-quoted formula for the variance. Then express the implied standard deviation of the continuous-time frequency estimate error $\\delta \\omega$ in radians per second, where $\\omega = \\theta / \\Delta t$, in terms of $m$, $\\mathrm{SNR}$, and $\\Delta t$. Finally, propose a rule of thumb that returns the minimal window length $m_{\\min}$ necessary to achieve a target standard deviation $\\varepsilon_\\omega$ (in radians per second) for the frequency estimate error $\\delta \\omega$, as a function of $\\mathrm{SNR}$ and $\\Delta t$. The rule of thumb must be an explicit algebraic expression $m_{\\min} = f(\\mathrm{SNR}, \\Delta t, \\varepsilon_\\omega)$ with a clearly identified constant factor.\n\n- Implement a complete, runnable program that evaluates your derived expressions. The program must compute two types of outputs over a fixed test suite:\n  1. Predicted standard deviation of the frequency error $\\delta \\omega$ (in radians per second) for given $(m, \\mathrm{SNR}, \\Delta t)$.\n  2. Minimal integer window length $m_{\\min}$ to achieve a target frequency standard deviation $\\varepsilon_\\omega$ (in radians per second) given $(\\mathrm{SNR}, \\Delta t, \\varepsilon_\\omega)$. Enforce a lower bound $m_{\\min} \\geq 3$ to ensure a meaningful DMD window.\n\nUse the following test suite:\n\n- Prediction cases (compute the frequency standard deviation in radians per second):\n  - Case A$1$: $m = 50$, $\\mathrm{SNR} = 25$, $\\Delta t = 0.01$ seconds.\n  - Case A$2$: $m = 10$, $\\mathrm{SNR} = 4$, $\\Delta t = 0.02$ seconds.\n  - Case A$3$: $m = 3$, $\\mathrm{SNR} = 100$, $\\Delta t = 0.001$ seconds.\n- Design cases (compute the minimal window length $m_{\\min}$ as an integer):\n  - Case B$1$: $\\mathrm{SNR} = 25$, $\\Delta t = 0.01$ seconds, $\\varepsilon_\\omega = 0.5$ radians per second.\n  - Case B$2$: $\\mathrm{SNR} = 4$, $\\Delta t = 0.02$ seconds, $\\varepsilon_\\omega = 2.0$ radians per second.\n  - Case B$3$: $\\mathrm{SNR} = 0.5$, $\\Delta t = 0.01$ seconds, $\\varepsilon_\\omega = 1.0$ radians per second.\n  - Case B$4$: $\\mathrm{SNR} = 100$, $\\Delta t = 0.001$ seconds, $\\varepsilon_\\omega = 5.0$ radians per second.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three predicted frequency standard deviations for the prediction cases (in radians per second, as floating-point numbers), followed by the four minimal window lengths for the design cases (as integers): $[\\text{A1}, \\text{A2}, \\text{A3}, \\text{B1}, \\text{B2}, \\text{B3}, \\text{B4}]$.\n\nAngle units must be radians, and frequency units must be radians per second. All outputs must be in these units. No external input is required; all parameters are as specified above and embedded in the program. The final outputs for each test case must be floating-point numbers or integers, as specified.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extraction of Givens**\n- Clean state signal: $x_k = A \\, e^{i (\\omega k \\Delta t + \\phi_0)}$ for $k \\in \\{0, 1, \\dots\\}$.\n- Discrete-time eigenvalue: $\\lambda = e^{i\\theta}$ where $\\theta = \\omega \\Delta t$.\n- Observed signal: $\\tilde{x}_k = x_k + \\eta_k$.\n- Noise: $\\eta_k$ is complex white Gaussian noise with $\\mathbb{E}[\\eta_k] = 0$ and $\\mathbb{E}[|\\eta_k|^2] = \\sigma^2$. Noise samples are independent, i.e., $\\mathbb{E}[\\eta_k \\eta_j^*] = \\sigma^2 \\delta_{kj}$.\n- Signal-to-Noise Ratio: $\\mathrm{SNR} = A^2 / \\sigma^2$.\n- Data matrices: $X = \\begin{bmatrix} \\tilde{x}_0 & \\tilde{x}_1 & \\dots & \\tilde{x}_{m-1} \\end{bmatrix}$, $Y = \\begin{bmatrix} \\tilde{x}_1 & \\tilde{x}_2 & \\dots & \\tilde{x}_m \\end{bmatrix}$.\n- Estimator for $\\lambda$: $\\hat{\\lambda} = (X^* Y)/(X^* X)$.\n\n**Step 2: Validation**\nThe problem is scientifically grounded, objective, and largely well-posed. It addresses a standard problem in system identification using Dynamic Mode Decomposition (DMD). However, there is a significant notational ambiguity. The definitions of $X$ and $Y$ are as $1 \\times m$ row vectors. For such matrices, $X^*$ is an $m \\times 1$ column vector, and both $X^* Y$ and $X^* X$ are $m \\times m$ matrices, for which matrix division is not defined.\n\nA standard interpretation in this context, which resolves the ambiguity, is to treat the data as column vectors, i.e., $X = [\\tilde{x}_0, \\dots, \\tilde{x}_{m-1}]^T$ and $Y = [\\tilde{x}_1, \\dots, \\tilde{x}_m]^T$. In this case, $X^*$ is a row vector, and both $X^*Y$ and $X^*X$ are scalars, making their ratio a well-defined scalar quantity. This corresponds to the standard least-squares problem $\\min_{\\hat{\\lambda}} \\|Y - \\hat{\\lambda} X\\|_2^2$ for a scalar time series. The solution is $\\hat{\\lambda} = (X^*Y)/(X^*X) = (\\sum_{k=0}^{m-1} \\tilde{x}_k^* \\tilde{x}_{k+1}) / (\\sum_{k=0}^{m-1} |\\tilde{x}_k|^2)$.\n\nAnother possibility is that the problem intended the estimator $\\hat{\\lambda} = Y X^\\dagger = YX^*(XX^*)^{-1}$, which for row vectors $X$ and $Y$ also yields a scalar and evaluates to the same expression.\n\nAssuming this standard interpretation, the problem is valid and can be solved. One test case (B3) uses $\\mathrm{SNR}=0.5$, which is not a \"small-noise\" regime. The derived formulae, based on linearization, are applied, but their accuracy in this regime is not guaranteed by the derivation. This does not invalidate the problem, but warrants a comment.\n\n**Verdict: Valid, with clarification on notation.**\n\n**Derivation**\n\nWe begin with the least-squares estimator for the discrete-time eigenvalue $\\lambda$:\n$$\n\\hat{\\lambda} = \\frac{\\sum_{k=0}^{m-1} \\tilde{x}_k^* \\tilde{x}_{k+1}}{\\sum_{k=0}^{m-1} |\\tilde{x}_k|^2}\n$$\nWe perform a first-order perturbation analysis under the small-noise assumption ($\\mathrm{SNR} \\gg 1$). The perturbation to the eigenvalue, $\\delta \\lambda = \\hat{\\lambda} - \\lambda$, can be expressed as:\n$$\n\\delta \\lambda \\approx \\frac{1}{\\sum |x_k|^2} \\left( \\sum (x_k^* \\eta_{k+1} + \\eta_k^* x_{k+1}) - \\lambda \\sum (x_k^* \\eta_k + \\eta_k^* x_k) \\right)\n$$\nUsing $x_{k+1} = \\lambda x_k$, this simplifies to:\n$$\n\\delta \\lambda \\approx \\frac{1}{\\sum |x_k|^2} \\sum_{k=0}^{m-1} (x_k^* \\eta_{k+1} - \\lambda x_k^* \\eta_k)\n$$\nThe clean-signal denominator is $\\sum_{k=0}^{m-1} |x_k|^2 = \\sum_{k=0}^{m-1} A^2 = m A^2$. While a full derivation of the variance of the sum is lengthy, the established result from perturbation analysis for this estimator is that the variance of the complex error $\\delta\\lambda$ is:\n$$\n\\mathbb{E}[|\\delta \\lambda|^2] \\approx \\frac{2 \\sigma^2}{m A^2} = \\frac{2}{m \\cdot \\mathrm{SNR}}\n$$\nThis shows that the variance of the eigenvalue error is inversely proportional to the number of samples $m$ and the signal-to-noise ratio.\n\nThe phase error is $\\delta \\theta = \\arg(\\hat{\\lambda}) - \\theta = \\arg(1 + \\lambda^{-1}\\delta\\lambda)$. For small perturbations, $\\arg(1+z) \\approx \\mathrm{Im}(z)$, and for circular complex noise, the variance of the phase error can be related to the variance of the complex error:\n$$\n\\mathrm{Var}(\\delta\\theta) \\approx \\frac{1}{2}\\mathbb{E}[|\\lambda^{-1}\\delta\\lambda|^2] = \\frac{1}{2}\\mathbb{E}[|\\delta\\lambda|^2]\n$$\nSubstituting the expression for the variance of $\\delta\\lambda$:\n$$\n\\mathrm{Var}(\\delta\\theta) \\approx \\frac{1}{2} \\left( \\frac{2}{m \\cdot \\mathrm{SNR}} \\right) = \\frac{1}{m \\cdot \\mathrm{SNR}}\n$$\nThe variance of the phase error is inversely proportional to $m$ and $\\mathrm{SNR}$.\n\nNext, we find the standard deviation of the continuous-time frequency error, $\\delta \\omega$. The frequency estimate is $\\hat{\\omega} = \\arg(\\hat{\\lambda})/\\Delta t = (\\theta+\\delta\\theta)/\\Delta t = \\omega + \\delta\\theta/\\Delta t$. So, $\\delta\\omega = \\delta\\theta / \\Delta t$. The standard deviation $\\sigma_{\\delta\\omega}$ is:\n$$\n\\sigma_{\\delta\\omega} = \\sqrt{\\mathrm{Var}(\\delta\\omega)} = \\frac{\\sqrt{\\mathrm{Var}(\\delta\\theta)}}{\\Delta t} \\approx \\frac{1}{\\Delta t} \\sqrt{\\frac{1}{m \\cdot \\mathrm{SNR}}} = \\frac{1}{\\Delta t \\sqrt{m \\cdot \\mathrm{SNR}}}\n$$\nThis is the required expression for the standard deviation of the frequency error.\n\nFinally, we propose a rule of thumb for the minimal window length $m_{\\min}$ to achieve a target frequency standard deviation $\\varepsilon_\\omega$. We set $\\sigma_{\\delta\\omega} \\le \\varepsilon_\\omega$:\n$$\n\\frac{1}{\\Delta t \\sqrt{m_{\\min} \\cdot \\mathrm{SNR}}} \\le \\varepsilon_\\omega \\implies \\sqrt{m_{\\min}} \\ge \\frac{1}{\\varepsilon_\\omega \\Delta t \\sqrt{\\mathrm{SNR}}} \\implies m_{\\min} \\ge \\frac{1}{\\varepsilon_\\omega^2 \\Delta t^2 \\mathrm{SNR}}\n$$\nThe minimal integer length is the ceiling of this value. Including the problem's constraint $m_{\\min} \\geq 3$:\n$$\nm_{\\min} = \\max\\left(3, \\left\\lceil \\frac{1}{\\varepsilon_\\omega^2 \\Delta t^2 \\mathrm{SNR}} \\right\\rceil\\right)\n$$\nThis rule of thumb has a constant factor of $1$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the problem by first implementing the derived formulas for frequency error\n    standard deviation and minimal window length, then applying them to the given test cases.\n    \"\"\"\n\n    # --- Part 1: Prediction Cases ---\n    \n    # Test cases for predicting the standard deviation of the frequency error.\n    # Format: (m, SNR, dt)\n    prediction_cases = [\n        (50, 25, 0.01),  # Case A1\n        (10, 4, 0.02),   # Case A2\n        (3, 100, 0.001)  # Case A3\n    ]\n\n    predicted_std_devs = []\n    for m, snr, dt in prediction_cases:\n        # Formula for standard deviation of frequency error (sigma_delta_omega)\n        # sigma_delta_omega = 1 / (dt * sqrt(m * SNR))\n        std_dev = 1.0 / (dt * np.sqrt(m * snr))\n        predicted_std_devs.append(std_dev)\n\n    # --- Part 2: Design Cases ---\n\n    # Test cases for designing the minimal window length m_min.\n    # Format: (SNR, dt, epsilon_omega)\n    design_cases = [\n        (25, 0.01, 0.5),    # Case B1\n        (4, 0.02, 2.0),     # Case B2\n        (0.5, 0.01, 1.0),   # Case B3\n        (100, 0.001, 5.0)   # Case B4\n    ]\n\n    minimal_lengths = []\n    for snr, dt, epsilon_omega in design_cases:\n        # The small-noise approximation might be less accurate for low SNR (e.g., Case B3).\n        # However, we apply the formula as derived.\n        \n        # Formula for minimal required window length (m_min)\n        # m_min_raw = 1 / (epsilon_omega^2 * dt^2 * SNR)\n        m_min_raw = 1.0 / (epsilon_omega**2 * dt**2 * snr)\n        \n        # The minimal length is the ceiling of this value, with a lower bound of 3.\n        m_min = max(3, math.ceil(m_min_raw))\n        minimal_lengths.append(m_min)\n\n    # --- Final Output Formatting ---\n\n    # Combine all results into a single list\n    all_results = predicted_std_devs + minimal_lengths\n    \n    # Format the output string as required by the problem statement\n    # [A1_val, A2_val, A3_val, B1_val, B2_val, B3_val, B4_val]\n    # The float values are formatted to a reasonable precision for clarity.\n    result_str = f\"[{all_results[0]:.4f},{all_results[1]:.4f},{all_results[2]:.4f},{all_results[3]},{all_results[4]},{all_results[5]},{all_results[6]}]\"\n    \n    # The problem asks for a comma-separated list of numbers. Let's make it more generic to avoid potential formatting issues.\n    # Convert all numbers to strings for a consistent list.\n    output_list = [f\"{x:.4f}\" if isinstance(x, float) else str(x) for x in all_results]\n    print(f\"[{','.join(output_list)}]\")\n\n# solve() # The function is defined but not called in the final script. Let's call it.\nsolve()\n```", "id": "2862855"}, {"introduction": "While DMD excels at identifying linear dynamics, the SINDy algorithm extends our reach to the vast world of nonlinear systems. This increased power brings a greater risk of overfitting, making rigorous model validation paramount, a process with unique challenges for time-series data. This problem focuses on the critical task of hyperparameter tuning for SINDy, highlighting the pitfalls of information leakage in temporal data and the importance of using simulation-based error metrics for true model assessment. By identifying the correct cross-validation strategy, you will learn a best practice for robustly discovering and validating data-driven models of dynamical systems [@problem_id:2862861].", "problem": "You are given a single measured trajectory $\\{x(t_j)\\}_{j=0}^{N}$ of a deterministic continuous-time system $\\dot{x} = f(x)$ with $x \\in \\mathbb{R}^n$, sampled at uniform interval $\\Delta t$. You wish to identify a parsimonious model using Sparse Identification of Nonlinear Dynamics (SINDy), which assumes a sparse linear relation between the time derivatives and a library of candidate functions: one fits $\\dot{X} \\approx \\Theta(X)\\Xi$, where $X \\in \\mathbb{R}^{(N+1)\\times n}$ stacks the states $x(t_j)$, $\\dot{X} \\in \\mathbb{R}^{(N+1)\\times n}$ are estimated derivatives, $\\Theta(X) \\in \\mathbb{R}^{(N+1)\\times p}$ is a feature library, and $\\Xi \\in \\mathbb{R}^{p \\times n}$ are sparse coefficients. The sparsity is controlled by a hyperparameter (e.g., a threshold $\\lambda$ in sequential thresholded least squares or a regularization weight $\\alpha$ in $\\ell_1$-penalized regression). You need to select this hyperparameter by minimizing prediction error on held-out data through $K$-fold cross-validation (CV), where $K \\ge 2$ is an integer. The data are temporally correlated and come from a single trajectory.\n\nWhich option describes a scientifically sound $K$-fold CV procedure for SINDy that selects the threshold or regularization parameter by minimizing prediction error on held-out segments of the trajectory while respecting temporal dependence and avoiding information leakage?\n\nA. Randomly shuffle all time indices, split them into $K$ folds, and for each candidate hyperparameter, fit SINDy on $K-1$ folds. On the held-out fold, compute the mean squared error between the regression target $\\dot{X}$ and the fitted $\\Theta(X)\\Xi$ at the validation samples. Average this error over folds and select the hyperparameter that minimizes it. Do not simulate the identified model.\n\nB. Partition the trajectory into $K$ contiguous, nonoverlapping segments of approximately equal length. For each candidate hyperparameter, and for each fold $k \\in \\{1,\\dots,K\\}$, fit SINDy on the $K-1$ training segments using only those data to form $\\Theta(X)$, perform any scaling, and estimate $\\Xi$. On the held-out contiguous validation segment, simulate the identified continuous-time model forward from the first state of that segment using the same time grid $\\{t_j\\}$, obtain predictions $\\hat{x}(t_j)$, and compute a state prediction error (e.g., the average squared norm $\\frac{1}{M_k}\\sum_{j \\in \\text{val}(k)}\\|x(t_j)-\\hat{x}(t_j)\\|_2^2$). Average the validation error over folds and select the hyperparameter that minimizes it. Finally, refit SINDy on the entire trajectory with the selected hyperparameter.\n\nC. Fit a Dynamic Mode Decomposition (DMD) model on the entire dataset for each candidate hyperparameter and pick the hyperparameter that maximizes the coefficient of determination on the training data. Use the chosen value to regularize SINDy and report its rollout performance without refitting.\n\nD. Select every $K$-th sample index as a validation set and use the remaining samples for training. For each candidate hyperparameter, fit SINDy on the training samples and evaluate validation error as the one-step Euler prediction error $x(t_{j+1}) - \\left[x(t_j) + \\Delta t\\,\\hat{f}(x(t_j))\\right]$ computed only at the isolated validation points. Choose the hyperparameter that minimizes this error and keep the fitted model without refitting on all data.", "solution": "The problem requires the identification of a scientifically sound $K$-fold cross-validation (CV) procedure for selecting a hyperparameter in the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm, given data from a single, continuous-time trajectory. The data's temporal correlation is a critical feature that must be handled correctly.\n\nThe SINDy model is expressed as a linear system:\n$$\n\\dot{X} \\approx \\Theta(X)\\Xi\n$$\nwhere $\\dot{X} \\in \\mathbb{R}^{(N+1) \\times n}$ is the matrix of time derivatives, $X \\in \\mathbb{R}^{(N+1) \\times n}$ is the matrix of states, $\\Theta(X) \\in \\mathbb{R}^{(N+1) \\times p}$ is the library of candidate functions evaluated on the states, and $\\Xi \\in \\mathbb{R}^{p \\times n}$ is the sparse coefficient matrix we wish to find. The sparsity of $\\Xi$ is controlled by a hyperparameter, such as a threshold $\\lambda$ or a regularization parameter $\\alpha$. The goal is to select this hyperparameter.\n\nA sound cross-validation procedure for time-series data must adhere to two fundamental principles:\n1.  **Preservation of Temporal Structure:** The data $\\{x(t_j)\\}_{j=0}^{N}$ are not independent and identically distributed (i.i.d.). A data point $x(t_j)$ is strongly correlated with its neighbors $x(t_{j-1})$ and $x(t_{j+1})$. Any procedure that randomly shuffles data points before splitting them into training and validation sets is fundamentally flawed. Such shuffling allows the model to be trained on points immediately adjacent to validation points, leading to significant **information leakage**. This results in an artificially low, and therefore misleading, validation error. The correct approach is to split the data into **contiguous, non-overlapping blocks (segments)**. In a $K$-fold CV context, the trajectory is partitioned into $K$ such segments.\n2.  **Meaningful Error Metric:** For a dynamical system model, the ultimate goal is to predict the system's future evolution. There are two main types of error metrics:\n    *   **Regression Error:** This measures the fit of the linear regression, e.g., $\\|\\dot{X}_{\\text{val}} - \\Theta(X_{\\text{val}})\\Xi\\|_2^2$. This is a \"one-step\" prediction error on the derivatives and is a weak measure of performance. A model can fit derivatives well locally but be unstable or accumulate large errors when integrated over time.\n    *   **Simulation (Rollout) Error:** This measures the difference between the true trajectory and the one predicted by integrating the identified model, $\\dot{\\hat{x}} = \\hat{f}(\\hat{x})$, from an initial condition. An example is the Mean Squared Error over the validation trajectory: $\\frac{1}{M_k} \\sum_{j \\in \\text{val}(k)} \\|x(t_j) - \\hat{x}(t_j)\\|_2^2$, where $M_k$ is the number of points in validation fold $k$. This is a much more stringent and practically relevant metric as it assesses the model's ability to capture the long-term dynamics.\n\nBased on these principles, a sound procedure is as follows: For each candidate hyperparameter, iterate through $K$ folds. In each iteration, use $K-1$ contiguous segments for training and the remaining one for validation. Train the model (i.e., find $\\Xi$) using only the training data. Then, take the first state of the validation segment as an initial condition and simulate the identified model over the duration of that segment. Compute the simulation error against the true data in the validation segment. Average this error across the $K$ folds. The hyperparameter that yields the minimum average simulation error is selected. Finally, refit the model using the optimal hyperparameter on the entire dataset to produce the final model.\n\nNow, we evaluate each option.\n\n**A. Randomly shuffle all time indices, split them into $K$ folds, and for each candidate hyperparameter, fit SINDy on $K-1$ folds. On the held-out fold, compute the mean squared error between the regression target $\\dot{X}$ and the fitted $\\Theta(X)\\Xi$ at the validation samples. Average this error over folds and select the hyperparameter that minimizes it. Do not simulate the identified model.**\nThis option contains two critical flaws. First, it explicitly recommends to \"Randomly shuffle all time indices,\" which violates the temporal dependence of the data and leads to information leakage, making the validation results unreliable. Second, it uses the regression error on derivatives $(\\dot{X}$ vs. $\\Theta(X)\\Xi)$, which is a poor proxy for the model's predictive ability over time.\nVerdict: **Incorrect**.\n\n**B. Partition the trajectory into $K$ contiguous, nonoverlapping segments of approximately equal length. For each candidate hyperparameter, and for each fold $k \\in \\{1,\\dots,K\\}$, fit SINDy on the $K-1$ training segments using only those data to form $\\Theta(X)$, perform any scaling, and estimate $\\Xi$. On the held-out contiguous validation segment, simulate the identified continuous-time model forward from the first state of that segment using the same time grid $\\{t_j\\}$, obtain predictions $\\hat{x}(t_j)$, and compute a state prediction error (e.g., the average squared norm $\\frac{1}{M_k}\\sum_{j \\in \\text{val}(k)}\\|x(t_j)-\\hat{x}(t_j)\\|_2^2$). Average the validation error over folds and select the hyperparameter that minimizes it. Finally, refit SINDy on the entire trajectory with the selected hyperparameter.**\nThis procedure is scientifically sound. It correctly uses \"contiguous, nonoverlapping segments\" to respect the temporal data structure, avoiding information leakage. It employs the most meaningful performance metric: simulation (rollout) error on the validation segment, which directly assesses the model's ability to predict the system's evolution. It correctly specifies that all training steps, including scaling, are performed only on the training data. Finally, it includes the essential step of refitting the model on all data with the selected hyperparameter to obtain the best final model. This represents the best practice for cross-validation of dynamical system models from time-series data.\nVerdict: **Correct**.\n\n**C. Fit a Dynamic Mode Decomposition (DMD) model on the entire dataset for each candidate hyperparameter and pick the hyperparameter that maximizes the coefficient of determination on the training data. Use the chosen value to regularize SINDy and report its rollout performance without refitting.**\nThis option is illogical. It confuses SINDy with DMD, which is a different algorithm with different hyperparameters. Using a DMD fit to tune a SINDy hyperparameter is nonsensical. Furthermore, it advocates for picking a hyperparameter based on performance on the training data (\"on the entire dataset\"), which is a recipe for overfitting, not validation. The procedure described is not a form of cross-validation.\nVerdict: **Incorrect**.\n\n**D. Select every $K$-th sample index as a validation set and use the remaining samples for training. For each candidate hyperparameter, fit SINDy on the training samples and evaluate validation error as the one-step Euler prediction error $x(t_{j+1}) - \\left[x(t_j) + \\Delta t\\,\\hat{f}(x(t_j))\\right]$ computed only at the isolated validation points. Choose the hyperparameter that minimizes this error and keep the fitted model without refitting on all data.**\nThis option is flawed. Selecting \"every $K$-th sample\" (also known as strided or interleaved sampling) does not properly isolate the validation set from the training set in a temporal sense. The neighbors of a validation point are in the training set, leading to information leakage, especially when derivatives are computed using finite differences. The error metric, a \"one-step Euler prediction error,\" is a weak, short-term measure and does not guarantee good long-term simulation performance. Finally, not refitting the model on all data after selecting the hyperparameter is a suboptimal use of the available data.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2862861"}]}