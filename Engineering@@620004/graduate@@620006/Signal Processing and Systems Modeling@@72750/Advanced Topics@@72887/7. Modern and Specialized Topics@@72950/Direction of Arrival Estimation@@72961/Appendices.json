{"hands_on_practices": [{"introduction": "Before we can estimate the directions of incoming signals, we must first determine how many signals are present, a task known as model order selection. This exercise introduces a powerful, modern method for this task based on Random Matrix Theory, which provides a theoretical prediction for the distribution of eigenvalues from noise alone. By comparing the eigenvalues of your received signal's covariance matrix to the prediction of the Marchenko-Pastur law, you will learn to distinguish signal-bearing eigenvalues from those of noise, a fundamental skill in modern signal processing [@problem_id:2866479].", "problem": "Consider a narrowband far-field array processing scenario with a Uniform Linear Array (ULA) of $M$ sensors and inter-element spacing equal to half the wavelength. Let the received complex baseband data over $N$ temporal snapshots be modeled as\n$$\n\\mathbf{X} = \\mathbf{A}(\\boldsymbol{\\theta}) \\mathbf{S} + \\mathbf{W},\n$$\nwhere $\\mathbf{X} \\in \\mathbb{C}^{M \\times N}$ is the data matrix, $\\mathbf{A}(\\boldsymbol{\\theta}) \\in \\mathbb{C}^{M \\times K}$ is the array manifold (steering) matrix for $K$ uncorrelated far-field plane-wave sources impinging from directions of arrival $\\boldsymbol{\\theta} = [\\theta_1,\\dots,\\theta_K]$ (in degrees), $\\mathbf{S} \\in \\mathbb{C}^{K \\times N}$ is the source signal matrix whose entries are independent and identically distributed zero-mean circularly symmetric complex Gaussian random variables, and $\\mathbf{W} \\in \\mathbb{C}^{M \\times N}$ is additive noise with independent and identically distributed zero-mean circularly symmetric complex Gaussian entries with variance $\\sigma^2$ per sensor. The ULA steering vector for angle $\\theta$ in degrees is defined elementwise by\n$$\na_m(\\theta) = \\exp\\left(-j \\pi (m-1)\\sin\\left(\\theta \\frac{\\pi}{180}\\right)\\right), \\quad m=1,\\dots,M,\n$$\nand the $m$-th column of $\\mathbf{A}(\\boldsymbol{\\theta})$ is $[a_1(\\theta_m),\\dots,a_M(\\theta_m)]^\\top$. Define the sample covariance matrix as\n$$\n\\widehat{\\mathbf{R}} = \\frac{1}{N}\\mathbf{X}\\mathbf{X}^H \\in \\mathbb{C}^{M \\times M}.\n$$\n\nAssume the following:\n- The noise variance is $\\sigma^2 = 1$ (dimensionless).\n- The total signal-to-noise ratio (SNR) per sensor is prescribed as $\\mathrm{SNR}$ in decibels, interpreted as the ratio of the total signal power per sensor to the noise power per sensor, i.e., if $\\mathrm{SNR}_{\\mathrm{lin}} = 10^{\\mathrm{SNR}/10}$ is the linear SNR, then the per-source variance is set to $\\mathrm{SNR}_{\\mathrm{lin}}/K$ so that the sum of per-source powers equals $\\mathrm{SNR}_{\\mathrm{lin}}$.\n- All angles $\\theta_k$ are provided in degrees.\n\nUnder the noise-only hypothesis ($K=0$), the eigenvalue distribution of $\\widehat{\\mathbf{R}}$ is governed, in the large system limit, by the Marchenko–Pastur law for aspect ratio $c = M/N$. Use this law to set a decision boundary at the upper edge of the noise-only eigenvalue support and decide the number of sources by counting how many empirical eigenvalues of $\\widehat{\\mathbf{R}}$ exceed this upper edge. To robustify against finite-sample fluctuations, multiply the upper edge by a conservative factor of $1.1$ to form the decision threshold. The goal is to infer the number of sources by comparing the empirical eigenvalue spectrum to this threshold.\n\nTasks:\n- For each test case below, synthesize $\\mathbf{X}$ according to the model, compute $\\widehat{\\mathbf{R}}$, obtain its eigenvalues, and estimate the number of sources by counting those eigenvalues strictly larger than the decision threshold derived from the Marchenko–Pastur law multiplied by $1.1$.\n- Angles must be treated in degrees as specified; no other physical units are required for the computations.\n- The final output for each test case must be an integer equal to the estimated number of sources.\n\nTest suite (each case specifies $(M,N,K,\\mathrm{SNR}\\ \\mathrm{dB}, \\text{angles in degrees})$):\n- Case $1$: $(M,N,K,\\mathrm{SNR}, \\boldsymbol{\\theta}) = (32, 64, 0, 0, [\\,])$.\n- Case $2$: $(M,N,K,\\mathrm{SNR}, \\boldsymbol{\\theta}) = (32, 64, 1, 10, [10])$.\n- Case $3$: $(M,N,K,\\mathrm{SNR}, \\boldsymbol{\\theta}) = (32, 64, 3, 15, [-20, 5, 30])$.\n- Case $4$: $(M,N,K,\\mathrm{SNR}, \\boldsymbol{\\theta}) = (32, 33, 0, 0, [\\,])$.\n- Case $5$: $(M,N,K,\\mathrm{SNR}, \\boldsymbol{\\theta}) = (64, 32, 0, 0, [\\,])$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\texttt{[result1,result2,result3]}$). Each entry must be the inferred number of sources for the corresponding test case, in the same order as listed above, and expressed as an integer with no units. No additional text should be printed. Angles provided in the test suite are in degrees and must be interpreted as such; no angle computations need to be output. The noise variance is fixed at $\\sigma^2=1$ for all cases.", "solution": "The problem requires the estimation of the number of incident signal sources, $K$, on a uniform linear array (ULA) using a method based on the eigenvalue distribution of the sample covariance matrix. The procedure must be validated for its scientific and mathematical integrity before a solution is attempted.\n\nThe problem formulation is found to be valid. It is grounded in established principles of statistical signal processing and array processing. The model for the received signal, $\\mathbf{X} = \\mathbf{A}(\\boldsymbol{\\theta}) \\mathbf{S} + \\mathbf{W}$, is a standard representation for narrowband far-field sources. The assumptions regarding the statistical properties of the signal matrix $\\mathbf{S}$ and noise matrix $\\mathbf{W}$ (i.i.d. zero-mean circularly symmetric complex Gaussian entries) are conventional. The method proposed for estimating the number of sources, which relies on the Marchenko-Pastur law from Random Matrix Theory, is a well-known technique for model order selection in this context. The parameters provided in the test suite are physically and computationally reasonable. The ambiguity in defining the per-source signal variance for the $K=0$ case is resolved by the standard interpretation that no signal is present, i.e., the term $\\mathbf{A}(\\boldsymbol{\\theta})\\mathbf{S}$ is zero. Therefore, the problem is well-posed and a rigorous solution can be constructed.\n\nThe solution proceeds as follows: First, we will detail the synthesis of the received data matrix $\\mathbf{X}$ according to the specified model. Second, we will describe the algorithm for estimating the number of sources by applying the Marchenko-Pastur (MP) thresholding criterion to the eigenvalues of the sample covariance matrix $\\widehat{\\mathbf{R}}$.\n\nThe received data at an array of $M$ sensors over $N$ snapshots is given by the linear model:\n$$\n\\mathbf{X} = \\mathbf{A}(\\boldsymbol{\\theta}) \\mathbf{S} + \\mathbf{W}\n$$\nHere, $\\mathbf{X} \\in \\mathbb{C}^{M \\times N}$ is the matrix of observed data. The matrix $\\mathbf{W} \\in \\mathbb{C}^{M \\times N}$ represents additive white Gaussian noise, with its entries being i.i.d. complex random variables from a circularly symmetric Gaussian distribution with mean $0$ and variance $\\sigma^2=1$. The signal component is $\\mathbf{A}(\\boldsymbol{\\theta}) \\mathbf{S}$, where $\\mathbf{A}(\\boldsymbol{\\theta}) \\in \\mathbb{C}^{M \\times K}$ is the array steering matrix for $K$ sources arriving from directions $\\boldsymbol{\\theta} = [\\theta_1, \\dots, \\theta_K]$. The matrix $\\mathbf{S} \\in \\mathbb{C}^{K \\times N}$ contains the complex amplitudes of the source signals.\n\nThe steering matrix $\\mathbf{A}(\\boldsymbol{\\theta})$ has columns $\\mathbf{a}(\\theta_k)$, which are steering vectors. For a ULA with inter-element spacing of half a wavelength, the $m$-th element of the steering vector for an angle $\\theta$ (in degrees) is:\n$$\na_m(\\theta) = \\exp\\left(-j \\pi (m-1)\\sin\\left(\\theta \\frac{\\pi}{180}\\right)\\right), \\quad m=1,\\dots,M\n$$\nwhere $j = \\sqrt{-1}$.\n\nThe source signal matrix $\\mathbf{S}$ is populated with i.i.d. zero-mean circularly symmetric complex Gaussian random variables. The variance of these variables, $\\sigma_s^2$, is determined by the signal-to-noise ratio ($\\mathrm{SNR}$). The linear SNR is $\\mathrm{SNR}_{\\mathrm{lin}} = 10^{\\mathrm{SNR_{dB}}/10}$. As the problem states, this represents the ratio of total signal power to noise power per sensor. The total signal power per sensor is $K \\sigma_s^2$, and noise power is $\\sigma^2=1$. Thus, $K \\sigma_s^2 / \\sigma^2 = \\mathrm{SNR}_{\\mathrm{lin}}$, leading to a per-source signal variance of $\\sigma_s^2 = \\mathrm{SNR}_{\\mathrm{lin}} / K$. This applies only when $K0$. If $K=0$, the signal term $\\mathbf{A}(\\boldsymbol{\\theta})\\mathbf{S}$ is absent, and the received data is purely noise: $\\mathbf{X} = \\mathbf{W}$.\n\nTo synthesize the data for a given test case $(M, N, K, \\mathrm{SNR_{dB}}, \\boldsymbol{\\theta})$:\n$1$. The noise matrix $\\mathbf{W}$ is generated by creating an $M \\times N$ matrix of complex values whose real and imaginary parts are drawn from an i.i.d. real Gaussian distribution $\\mathcal{N}(0, \\sigma^2/2)$. Since $\\sigma^2=1$, the distribution is $\\mathcal{N}(0, 1/2)$.\n$2$. If $K  0$:\n    a. Calculate $\\mathrm{SNR}_{\\mathrm{lin}} = 10^{\\mathrm{SNR_{dB}}/10}$ and the source variance $\\sigma_s^2 = \\mathrm{SNR}_{\\mathrm{lin}} / K$.\n    b. The signal matrix $\\mathbf{S}$ is generated as a $K \\times N$ matrix of complex values whose real and imaginary parts are from $\\mathcal{N}(0, \\sigma_s^2/2)$.\n    c. The steering matrix $\\mathbf{A}(\\boldsymbol{\\theta}) \\in \\mathbb{C}^{M \\times K}$ is constructed using the given angles $\\boldsymbol{\\theta}$.\n    d. The final data matrix is computed as $\\mathbf{X} = \\mathbf{A}(\\boldsymbol{\\theta})\\mathbf{S} + \\mathbf{W}$.\n$3$. If $K = 0$, the data matrix is simply $\\mathbf{X} = \\mathbf{W}$.\n\nOnce $\\mathbf{X}$ is synthesized, we compute the sample covariance matrix:\n$$\n\\widehat{\\mathbf{R}} = \\frac{1}{N}\\mathbf{X}\\mathbf{X}^H\n$$\nwhere $(\\cdot)^H$ denotes the conjugate transpose (Hermitian) operator. $\\widehat{\\mathbf{R}}$ is an $M \\times M$ Hermitian matrix, and its eigenvalues are real.\n\nThe core of the detection method lies in Random Matrix Theory. For a matrix $\\mathbf{X}=\\mathbf{W}$ consisting of only noise, in the large system limit where $M, N \\to \\infty$ such that the aspect ratio $c = M/N$ is constant, the empirical spectral distribution of $\\widehat{\\mathbf{R}} = \\frac{1}{N}\\mathbf{W}\\mathbf{W}^H$ converges to the Marchenko-Pastur distribution. The support of this distribution, i.e., the interval containing the eigenvalues, is given by $[\\lambda_{min}, \\lambda_{max}]$, where:\n$$\n\\lambda_{min} = \\sigma^2 (1 - \\sqrt{c})^2\n\\quad \\text{and} \\quad\n\\lambda_{max} = \\sigma^2 (1 + \\sqrt{c})^2\n$$\nWhen signal sources are present and are sufficiently strong, their corresponding eigenvalues will separate from the noise-only spectrum and appear as outliers above $\\lambda_{max}$. The problem prescribes a detection threshold based on this observation. The threshold, $T$, is set to be a factor of $1.1$ larger than the theoretical upper edge of the noise eigenvalue support to provide robustness against finite-sample effects:\n$$\nT = 1.1 \\times \\lambda_{max} = 1.1 \\times \\sigma^2 (1 + \\sqrt{M/N})^2\n$$\nWith $\\sigma^2=1$, this simplifies to $T = 1.1 (1 + \\sqrt{M/N})^2$.\n\nThe estimated number of sources, $\\hat{K}$, is then found by computing all $M$ eigenvalues of the synthesized $\\widehat{\\mathbf{R}}$ and counting how many are strictly greater than this threshold $T$. This procedure is applied to each of the five test cases specified in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the source enumeration for all test cases.\n    \"\"\"\n\n    # Test suite: (M, N, K, SNR_dB, angles_degrees)\n    test_cases = [\n        (32, 64, 0, 0, []),\n        (32, 64, 1, 10, [10]),\n        (32, 64, 3, 15, [-20, 5, 30]),\n        (32, 33, 0, 0, []),\n        (64, 32, 0, 0, []),\n    ]\n\n    # Global noise variance as per problem statement\n    noise_variance = 1.0\n\n    results = []\n    # Seed for reproducibility, though not required by the problem\n    # np.random.seed(42)  \n    \n    for case in test_cases:\n        M, N, K, snr_db, angles_deg = case\n        estimated_K = estimate_sources(M, N, K, snr_db, angles_deg, noise_variance)\n        results.append(estimated_K)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef estimate_sources(M, N, K, snr_db, angles_deg, noise_variance):\n    \"\"\"\n    Synthesizes the data matrix and estimates the number of sources.\n\n    Args:\n        M (int): Number of sensors.\n        N (int): Number of snapshots.\n        K (int): True number of sources.\n        snr_db (float): Signal-to-noise ratio in dB.\n        angles_deg (list): List of source angles in degrees.\n        noise_variance (float): Variance of the additive noise.\n\n    Returns:\n        int: Estimated number of sources.\n    \"\"\"\n    \n    # Step 1: Synthesize the received data matrix X.\n\n    # Generate the additive white Gaussian noise matrix W\n    # The variance of a complex variable z = x+jy is Var(x) + Var(y).\n    # For a circularly symmetric complex Gaussian with variance sigma^2,\n    # the real and imaginary parts are i.i.d. N(0, sigma^2/2).\n    w_real = np.random.randn(M, N)\n    w_imag = np.random.randn(M, N)\n    W = (w_real + 1j * w_imag) * np.sqrt(noise_variance / 2.0)\n\n    if K == 0:\n        # Noise-only case\n        X = W\n    else:\n        # Signal + Noise case\n        \n        # Construct the steering matrix A\n        A = np.zeros((M, K), dtype=np.complex128)\n        angles_rad = np.deg2rad(angles_deg)\n        m_indices = np.arange(M).reshape(-1, 1)\n        for k_idx, angle_rad in enumerate(angles_rad):\n             # ULA steering vector with d = lambda/2\n            A[:, k_idx] = np.exp(-1j * np.pi * m_indices * np.sin(angle_rad)).flatten()\n\n        # Construct the source signal matrix S\n        snr_lin = 10**(snr_db / 10.0)\n        source_variance = snr_lin / K\n        \n        s_real = np.random.randn(K, N)\n        s_imag = np.random.randn(K, N)\n        S = (s_real + 1j * s_imag) * np.sqrt(source_variance / 2.0)\n\n        # Compute the final data matrix X = AS + W\n        X = A @ S + W\n\n    # Step 2: Compute the sample covariance matrix R_hat.\n    R_hat = (1/N) * (X @ X.conj().T)\n\n    # Step 3: Compute the eigenvalues of R_hat.\n    # eigvalsh is used for Hermitian matrices, which is faster and more stable.\n    eigenvalues = np.linalg.eigvalsh(R_hat)\n\n    # Step 4: Calculate the Marchenko-Pastur decision threshold.\n    c = M / N  # Aspect ratio\n    lambda_max = noise_variance * (1 + np.sqrt(c))**2\n    threshold = 1.1 * lambda_max\n\n    # Step 5: Count eigenvalues strictly greater than the threshold.\n    estimated_K = np.sum(eigenvalues > threshold)\n\n    return estimated_K\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2866479"}, {"introduction": "Once the number of sources, $K$, is known, the next step is to pinpoint their exact angles. This practice delves into the mechanics of the elegant and efficient Root-MUSIC algorithm, which transforms the search for spectral peaks into a polynomial root-finding problem for Uniform Linear Arrays (ULAs). By working through a carefully constructed hypothetical scenario where the noise subspace is known, you will gain deep insight into the algebraic structure that underpins this powerful technique and see how the orthogonality principle translates directly into solutions for the directions of arrival [@problem_id:2866423].", "problem": "An eight-sensor Uniform Linear Array (ULA) with inter-element spacing $d=\\lambda/2$ receives $K=2$ narrowband, far-field plane waves in spatially white noise. Sensor indices are $n=0,1,\\dots,7$. Let the array manifold (steering) vector be defined for a complex number $z$ on the unit circle by\n$$\n\\mathbf{a}(z) \\triangleq \\begin{bmatrix} 1  z^{-1}  z^{-2}  \\dots  z^{-7} \\end{bmatrix}^{\\mathsf{T}},\n$$\nwith the spatial frequency–angle mapping given by $z=\\exp\\!\\big(j \\psi\\big)$ and $\\psi=\\pi \\sin(\\theta)$ for direction-of-arrival (DOA) angle $\\theta \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]$ (radians).\n\nYou are given the noise-subspace matrix $\\mathbf{E}_n \\in \\mathbb{C}^{8 \\times 6}$ whose columns form an orthonormal basis equal to the Discrete Fourier Transform (DFT) columns with frequency indices $\\{0,2,3,4,5,6\\}$; that is,\n$$\n\\mathbf{E}_n \\triangleq \\begin{bmatrix} \\mathbf{e}_0  \\mathbf{e}_2  \\mathbf{e}_3  \\mathbf{e}_4  \\mathbf{e}_5  \\mathbf{e}_6 \\end{bmatrix},\n$$\nwhere the $k$-th DFT column is\n$$\n\\mathbf{e}_k[n] \\triangleq \\frac{1}{\\sqrt{8}} \\exp\\!\\Big(-j \\frac{2\\pi}{8} k n\\Big), \\quad n=0,1,\\dots,7.\n$$\nThus the signal subspace is spanned by the remaining two DFT columns with indices $\\{1,7\\}$.\n\nStarting only from the orthogonality principle underlying Multiple Signal Classification (MUSIC)—namely, that the true steering vectors are orthogonal to the noise subspace—and from the definition of the projection $\\mathbf{P}_n \\triangleq \\mathbf{E}_n \\mathbf{E}_n^{\\mathsf{H}}$, perform the following:\n\n1. Construct the root-MUSIC polynomial $p(z)$ of degree $2(8-1)$ in the form\n$$\np(z) \\triangleq \\mathbf{a}^{\\mathsf{H}}(z)\\, \\mathbf{P}_n \\, \\mathbf{a}(z) \\;=\\; \\sum_{k=-7}^{7} c_k\\, z^{-k},\n$$\nby deriving a closed-form expression for the coefficients $c_k$ in terms of $\\mathbf{P}_n$ and the array size $8$.\n\n2. Determine the two roots of $p(z)$ that lie on the unit circle and correspond to the two DOAs in the visible region, and map them to DOA angles $\\theta_1$ and $\\theta_2$ using $\\psi=\\arg(z)$ and $\\theta=\\arcsin(\\psi/\\pi)$.\n\nExpress the final DOA estimates in degrees, rounded to four significant figures, and report them as a two-entry row vector. The final answer must be in degrees; do not include units in the final boxed answer.", "solution": "The problem statement has been validated and is determined to be scientifically grounded, well-posed, objective, and self-contained. It represents a standard problem in array signal processing, specifically Direction-of-Arrival (DOA) estimation using the root-MUSIC algorithm. The provided information is sufficient and consistent for deriving a unique solution.\n\nThe task is to find the DOAs of $K=2$ signals impinging on an $N=8$ sensor Uniform Linear Array (ULA). The core of the method lies in the orthogonality between the signal steering vectors and the noise subspace. The root-MUSIC algorithm operationalizes this by finding the roots of a polynomial derived from the projection onto the noise subspace.\n\nFirst, we derive the coefficients $c_k$ of the root-MUSIC polynomial $p(z)$. The problem defines this polynomial as $p(z) = \\mathbf{a}^{\\mathsf{H}}(z) \\mathbf{P}_n \\mathbf{a}(z)$. For the purpose of finding roots, we form the standard root-MUSIC polynomial which is analytic in the complex plane, by replacing the conjugate transpose $\\mathbf{a}^{\\mathsf{H}}(z)$ with the standard transpose $\\mathbf{a}^{\\mathsf{T}}(z^{-1})$. This is equivalent on the unit circle where $z^{-1} = z^*$.\nThe polynomial is thus formulated as:\n$$\np(z) \\triangleq \\mathbf{a}^{\\mathsf{T}}(z^{-1}) \\mathbf{P}_n \\mathbf{a}(z)\n$$\nwhere $\\mathbf{a}(z) = \\begin{bmatrix} 1  z^{-1}  \\dots  z^{-7} \\end{bmatrix}^{\\mathsf{T}}$. The vector $\\mathbf{a}^{\\mathsf{T}}(z^{-1})$ is $\\begin{bmatrix} 1  z  \\dots  z^7 \\end{bmatrix}$. Expanding the polynomial expression:\n$$\np(z) = \\sum_{m=0}^{7} \\sum_{l=0}^{7} z^{m} (\\mathbf{P}_n)_{ml} z^{-l} = \\sum_{m=0}^{7} \\sum_{l=0}^{7} (\\mathbf{P}_n)_{ml} z^{m-l}\n$$\nTo match the requested form $p(z) = \\sum_{k=-7}^{7} c_k z^{-k}$, we group terms by powers of $z$. Let's first define a polynomial $P(z) = \\sum_{j=-7}^{7} C_j z^j$. Comparing with the expression above, the coefficient $C_j$ for the term $z^j$ is obtained by setting $j=m-l$:\n$$\nC_j = \\sum_{m-l=j, \\, 0 \\le m,l \\le 7} (\\mathbf{P}_n)_{ml}\n$$\nThe problem specifies that the columns of the noise subspace projection matrix $\\mathbf{E}_n$ are DFT vectors. The projection matrix is $\\mathbf{P}_n = \\mathbf{E}_n \\mathbf{E}_n^{\\mathsf{H}} = \\sum_{i \\in S_n} \\mathbf{e}_i \\mathbf{e}_i^{\\mathsf{H}}$, where $S_n=\\{0,2,3,4,5,6\\}$. The $(m,l)$-th element of $\\mathbf{P}_n$ is:\n$$\n(\\mathbf{P}_n)_{ml} = \\sum_{i \\in S_n} (\\mathbf{e}_i)_m (\\mathbf{e}_i)_l^* = \\sum_{i \\in S_n} \\frac{1}{\\sqrt{8}} \\exp\\left(-j \\frac{2\\pi}{8} i m\\right) \\frac{1}{\\sqrt{8}} \\exp\\left(j \\frac{2\\pi}{8} i l\\right) = \\frac{1}{8} \\sum_{i \\in S_n} \\exp\\left(j \\frac{\\pi}{4} i (l-m)\\right)\n$$\nThis shows that $(\\mathbf{P}_n)_{ml}$ depends only on the difference $l-m$. Thus, $\\mathbf{P}_n$ is a Toeplitz matrix. Let $r_{l-m} = (\\mathbf{P}_n)_{ml}$. Then $C_j$, the coefficient of $z^j$, is the sum of elements on the $(-j)$-th diagonal of $\\mathbf{P}_n$. Since all elements on this diagonal are equal to $r_{-j}$, and there are $8-|j|$ such elements:\n$$\nC_j = (8-|j|) r_{-j}\n$$\nThe requested polynomial is $p(z) = \\sum_{k=-7}^{7} c_k z^{-k}$. By setting $z^{-k} = z^j$, we have $j=-k$. The coefficient $c_k$ is then equal to $C_{-k}$:\n$$\nc_k = C_{-k} = (8-|-k|)r_{-(-k)} = (8-|k|)r_k\n$$\nwhere $r_k = \\frac{1}{8} \\sum_{i \\in S_n} \\exp(j \\frac{\\pi}{4} i k)$.\n\nThe DFT vectors form a complete orthonormal basis, so the projection onto the noise subspace $\\mathbf{P}_n$ and the projection onto the signal subspace $\\mathbf{P}_s$ sum to the identity matrix: $\\mathbf{P}_n + \\mathbf{P}_s = \\mathbf{I}$. The signal subspace is spanned by DFT vectors with indices in $S_s=\\{1,7\\}$.\nThus, $(\\mathbf{P}_n)_{ml} = \\delta_{ml} - (\\mathbf{P}_s)_{ml}$, where $\\delta_{ml}$ is the Kronecker delta. The diagonal elements $r_k$ of $\\mathbf{P}_n$ are:\n$$\nr_k = \\delta_{k,0} - \\frac{1}{8} \\sum_{i \\in S_s} \\exp\\left(j \\frac{\\pi}{4} i k\\right)\n$$\nSubstituting $S_s = \\{1,7\\}$:\n$$\nr_k = \\delta_{k,0} - \\frac{1}{8} \\left( \\exp\\left(j \\frac{\\pi}{4} k\\right) + \\exp\\left(j \\frac{7\\pi}{4} k\\right) \\right) = \\delta_{k,0} - \\frac{1}{8} \\left( \\exp\\left(j \\frac{\\pi}{4} k\\right) + \\exp\\left(-j \\frac{\\pi}{4} k\\right) \\right) = \\delta_{k,0} - \\frac{2}{8} \\cos\\left(\\frac{\\pi k}{4}\\right)\n$$\nSo, $r_k = \\delta_{k,0} - \\frac{1}{4}\\cos\\left(\\frac{\\pi k}{4}\\right)$. The closed-form expression for the coefficients $c_k$ is:\n$$\nc_k = (8-|k|) \\left( \\delta_{k,0} - \\frac{1}{4}\\cos\\left(\\frac{\\pi k}{4}\\right) \\right), \\quad k \\in \\{-7, \\dots, 7\\}\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must find the two roots of $p(z)$ on the unit circle. The MUSIC principle states that for a true DOA angle $\\theta$, its corresponding steering vector $\\mathbf{a}(z)$ (with $z=\\exp(j\\pi\\sin\\theta)$) is orthogonal to the noise subspace. This means $\\mathbf{a}(z)$ must lie in the signal subspace.\nThe problem states that the signal subspace is spanned by the DFT vectors $\\mathbf{e}_1$ and $\\mathbf{e}_7$. A DFT vector $\\mathbf{e}_i$ has elements $(\\mathbf{e}_i)_n = \\frac{1}{\\sqrt{8}}\\exp(-j\\frac{2\\pi i n}{8})$. This is proportional to a steering vector $\\mathbf{a}(z_i)$ where $z_i=\\exp(j\\frac{2\\pi i}{8})$, since $(\\mathbf{a}(z_i))_n = (z_i)^{-n} = \\exp(-j\\frac{2\\pi i n}{8})$.\nTherefore, the steering vectors corresponding to the true DOAs must be proportional to the basis vectors of the signal subspace, which are $\\mathbf{e}_1$ and $\\mathbf{e}_7$. This implies that the true signal steering vectors are precisely those corresponding to the DFT indices $1$ and $7$. The roots on the unit circle are therefore:\n$$\nz_1 = \\exp\\left(j\\frac{2\\pi \\cdot 1}{8}\\right) = \\exp\\left(j\\frac{\\pi}{4}\\right)\n$$\n$$\nz_2 = \\exp\\left(j\\frac{2\\pi \\cdot 7}{8}\\right) = \\exp\\left(j\\frac{7\\pi}{4}\\right) = \\exp\\left(-j\\frac{\\pi}{4}\\right)\n$$\nThese roots must nullify the polynomial $p(z)$, a fact which can be verified by substituting them into the expression $\\sum c_k z^{-k}$ with the coefficients derived above.\n\nFinally, we map these roots to the DOA angles $\\theta_1$ and $\\theta_2$ using the given relations $z=\\exp(j\\psi)$ and $\\psi=\\pi\\sin(\\theta)$.\nFor the root $z_1 = \\exp(j\\frac{\\pi}{4})$:\nThe phase is $\\psi_1 = \\arg(z_1) = \\frac{\\pi}{4}$.\n$$\n\\frac{\\pi}{4} = \\pi \\sin(\\theta_1) \\implies \\sin(\\theta_1) = \\frac{1}{4}\n$$\n$$\n\\theta_1 = \\arcsin\\left(\\frac{1}{4}\\right) = \\arcsin(0.25)\n$$\nFor the root $z_2 = \\exp(-j\\frac{\\pi}{4})$:\nThe phase is $\\psi_2 = \\arg(z_2) = -\\frac{\\pi}{4}$.\n$$\n-\\frac{\\pi}{4} = \\pi \\sin(\\theta_2) \\implies \\sin(\\theta_2) = -\\frac{1}{4}\n$$\n$$\n\\theta_2 = \\arcsin\\left(-\\frac{1}{4}\\right) = -\\arcsin(0.25)\n$$\nTo provide the answer in degrees, we convert from radians:\n$$\n\\theta_1 = \\arcsin(0.25) \\times \\frac{180}{\\pi} \\approx 14.47751...^{\\circ}\n$$\n$$\n\\theta_2 = -\\arcsin(0.25) \\times \\frac{180}{\\pi} \\approx -14.47751...^{\\circ}\n$$\nRounding to four significant figures, the DOAs are $\\theta_1 = 14.48^{\\circ}$ and $\\theta_2 = -14.48^{\\circ}$. Presented as a two-entry row vector, this is $\\begin{pmatrix} -14.48  14.48 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -14.48  14.48 \\end{pmatrix}}\n$$", "id": "2866423"}, {"introduction": "While Root-MUSIC offers an elegant solution for ULAs, many scenarios require the more general spectral MUSIC algorithm, which relies on a grid search. This exercise explores the critical engineering trade-offs inherent in designing that grid, balancing computational cost against estimation accuracy and the risk of missing a signal entirely. Through this optimization problem, you will develop a deeper appreciation for the practical challenges of implementing spectral estimation methods and the design principles that ensure reliable performance [@problem_id:2866495].", "problem": "A uniform linear array (ULA) with $N$ identical sensors and inter-element spacing $d=\\lambda/2$ observes a single narrowband far-field source with direction-of-arrival (DOA) angle $\\theta_{0} \\in [-\\Theta_{\\max}, \\Theta_{\\max}]$. The Multiple Signal Classification (MUSIC) pseudospectrum is evaluated on a uniform angular grid $\\{\\theta_{m}\\}$ with spacing $\\Delta \\theta$ across $[-\\Theta_{\\max}, \\Theta_{\\max}]$, and the DOA estimate is taken as the grid point that maximizes the sampled pseudospectrum.\n\nYou are to design the grid spacing $\\Delta \\theta$ to balance computational cost and accuracy, and to quantify the impact of grid density on estimation error and missed peaks, under the following modeling assumptions:\n\n1. Use the standard ULA steering vector $a(\\theta) = [1,\\exp(j\\pi \\sin \\theta),\\dots,\\exp(j\\pi(N-1)\\sin \\theta)]^{\\top}$ and the known envelope of the conventional ULA array factor in spatial frequency $u=\\sin \\theta$ to determine the mainlobe width to the first null in $u$, and relate it to an angular width in $\\theta$ via the local mapping $u(\\theta)=\\sin \\theta$.\n2. Suppose the true peak location relative to the nearest grid point is uniformly distributed in the interval $[-\\Delta \\theta/2,\\Delta \\theta/2]$. Under this quantization model, derive the mean squared estimation error $E(\\Delta \\theta)$ due solely to grid discretization (ignore estimator bias).\n3. Define a missed-peak event as occurring when no grid point falls within the mainlobe capture interval of the pseudospectrum peak. Using the width you obtained in item 1 and the uniform offset model in item 2, derive the missed-peak probability $P_{\\mathrm{miss}}(\\Delta \\theta)$ as a function of $\\Delta \\theta$ and $N$, expressed in terms of the worst-case over $\\theta \\in [-\\Theta_{\\max},\\Theta_{\\max}]$.\n4. Let the computational cost per pseudospectrum evaluation be a positive constant $c_{0}$, so that the total cost of scanning a grid with spacing $\\Delta \\theta$ over the symmetric interval of width $2\\Theta_{\\max}$ is $C(\\Delta \\theta)=c_{0} \\left\\lceil \\frac{2\\Theta_{\\max}}{\\Delta \\theta}\\right\\rceil$. Consider the following penalized objective that trades off cost and accuracy while enforcing a strict no-missed-peak requirement:\n   $$J(\\Delta \\theta) \\triangleq \\frac{2 c_{0}\\,\\Theta_{\\max}}{\\Delta \\theta} + \\frac{E(\\Delta \\theta)}{1}, \\quad \\text{subject to } P_{\\mathrm{miss}}(\\Delta \\theta)=0,$$\n   where you may replace $\\left\\lceil \\cdot \\right\\rceil$ by its argument for tractability in the continuous optimization.\n5. Given $N=64$, $\\Theta_{\\max}=\\frac{\\pi}{3}$, and $c_{0}=10^{-6}$, minimize $J(\\Delta \\theta)$ over $\\Delta \\theta0$ subject to the no-missed-peak constraint from item 3, and report the optimal uniform grid spacing $\\Delta \\theta^{\\star}$.\n\nExpress your final answer for $\\Delta \\theta^{\\star}$ in radians and round to four significant figures.", "solution": "The problem presented is a valid exercise in signal processing system design, addressing the trade-off between computational cost and estimation accuracy in Direction of Arrival (DOA) estimation. It is scientifically grounded, well-posed, and all necessary parameters and models for its solution are provided. We shall proceed with a step-by-step derivation.\n\nThe problem requires us to find an optimal grid spacing $\\Delta \\theta$ for a DOA search by minimizing a cost function that balances computational load and grid-induced estimation error, subject to a constraint of zero missed peaks.\n\nStep 1: Mainlobe Width of the Capture Interval\n\nFirst, we establish the width of the mainlobe of the conventional beamformer, which the problem designates as the \"mainlobe capture interval\". For a Uniform Linear Array (ULA) with $N$ sensors and inter-element spacing $d=\\lambda/2$, the steering vector towards an angle $\\theta$ is given by\n$$ a(\\theta) = [1, \\exp(j k d \\sin\\theta), \\dots, \\exp(j(N-1)kd \\sin\\theta)]^{\\top} $$\nwhere $k = 2\\pi/\\lambda$ is the wavenumber. Substituting $d=\\lambda/2$, we have $kd = \\pi$. The spatial frequency is defined as $u = \\sin\\theta$. The steering vector becomes\n$$ a(u) = [1, \\exp(j\\pi u), \\dots, \\exp(j\\pi(N-1) u)]^{\\top} $$\nThe conventional beamformer output power, or array factor, for a source at $u_0$ when steered to $u$ is proportional to $|a(u)^\\dagger a(u_0)|^2$. For analyzing the mainlobe shape, we steer to broadside ($u=0$) and consider a source at $u_0$. The array factor is\n$$ P(u_0) = \\left| \\sum_{n=0}^{N-1} \\exp(j n \\pi u_0) \\right|^2 $$\nThis is a geometric series, which sums to\n$$ P(u_0) = \\left| \\frac{1 - \\exp(j N \\pi u_0)}{1 - \\exp(j \\pi u_0)} \\right|^2 = \\left| \\frac{\\sin(N\\pi u_0/2)}{\\sin(\\pi u_0/2)} \\right|^2 $$\nThe mainlobe is centered at $u_0=0$. The first nulls occur when the numerator is zero, provided the denominator is non-zero. This happens when $N\\pi u_0/2 = \\pm\\pi$, which gives $u_0 = \\pm 2/N$.\nThe full width of the mainlobe between the first nulls in the spatial frequency domain is therefore $W_u = 4/N$. We interpret the \"mainlobe capture interval\" to be this full null-to-null width.\n\nTo convert this width into the angular domain $\\theta$, we use the local relationship $u = \\sin\\theta$, which gives $\\mathrm{d}u = \\cos\\theta \\, \\mathrm{d}\\theta$. The angular width $W_\\theta$ corresponding to a width $W_u$ at an angle $\\theta$ is approximately\n$$ W_\\theta(\\theta) \\approx \\frac{W_u}{|\\mathrm{d}u/\\mathrm{d}\\theta|} = \\frac{4/N}{|\\cos\\theta|} $$\nThis width depends on the direction $\\theta$, becoming wider as $\\theta$ approaches $\\pm\\pi/2$ (endfire).\n\nStep 2: Mean Squared Estimation Error\n\nThe problem states that the estimation error due to grid discretization, $\\epsilon = \\hat{\\theta} - \\theta_0$, is modeled as a random variable uniformly distributed over the interval $[-\\Delta\\theta/2, \\Delta\\theta/2]$. The probability density function is $p(\\epsilon) = 1/\\Delta\\theta$ for $\\epsilon \\in [-\\Delta\\theta/2, \\Delta\\theta/2]$ and $0$ otherwise.\nThe Mean Squared Error (MSE), denoted $E(\\Delta\\theta)$, is the second moment of this error distribution. Since the distribution is zero-mean, the MSE is equal to the variance.\n$$ E(\\Delta\\theta) = E[\\epsilon^2] = \\int_{-\\Delta\\theta/2}^{\\Delta\\theta/2} \\epsilon^2 p(\\epsilon) \\, d\\epsilon = \\frac{1}{\\Delta\\theta} \\int_{-\\Delta\\theta/2}^{\\Delta\\theta/2} \\epsilon^2 \\, d\\epsilon $$\n$$ E(\\Delta\\theta) = \\frac{1}{\\Delta\\theta} \\left[ \\frac{\\epsilon^3}{3} \\right]_{-\\Delta\\theta/2}^{\\Delta\\theta/2} = \\frac{1}{3\\Delta\\theta} \\left( \\left(\\frac{\\Delta\\theta}{2}\\right)^3 - \\left(-\\frac{\\Delta\\theta}{2}\\right)^3 \\right) = \\frac{1}{3\\Delta\\theta} \\left( 2 \\frac{(\\Delta\\theta)^3}{8} \\right) = \\frac{(\\Delta\\theta)^2}{12} $$\n\nStep 3: Missed-Peak Probability and Constraint\n\nA \"missed-peak event\" is defined as having no grid points fall within the mainlobe capture interval of width $W_\\theta(\\theta)$. This can only happen if the grid spacing $\\Delta\\theta$ is larger than the interval width $W_\\theta(\\theta)$. Assuming the interval's center is uniformly distributed relative to the grid, the probability of a miss is given by\n$$ P_{\\text{miss}}(\\Delta\\theta, \\theta) = \\frac{\\max(0, \\Delta\\theta - W_\\theta(\\theta))}{\\Delta\\theta} = \\max\\left(0, 1 - \\frac{W_\\theta(\\theta)}{\\Delta\\theta}\\right) $$\nThe problem requires finding the probability in the \"worst-case over $\\theta \\in [-\\Theta_{\\max}, \\Theta_{\\max}]$\". The probability of a miss increases as the mainlobe width $W_\\theta(\\theta)$ decreases. The narrowest mainlobe occurs where $|\\cos\\theta|$ is maximum, which is at $\\theta=0$ (broadside).\nThe minimum mainlobe width is $W_{\\theta, \\min} = W_\\theta(0) = 4/N$.\nThe worst-case (maximum) miss probability is therefore\n$$ P_{\\text{miss, wc}}(\\Delta\\theta) = \\sup_{\\theta \\in [-\\Theta_{\\max}, \\Theta_{\\max}]} P_{\\text{miss}}(\\Delta\\theta, \\theta) = \\max\\left(0, 1-\\frac{W_{\\theta, \\min}}{\\Delta\\theta}\\right) = \\max\\left(0, 1-\\frac{4/N}{\\Delta\\theta}\\right) $$\nThe \"no-missed-peak requirement\" is $P_{\\text{miss, wc}}(\\Delta\\theta) = 0$. This imposes the constraint:\n$$ 1-\\frac{4/N}{\\Delta\\theta} \\le 0 \\implies \\Delta\\theta \\le \\frac{4}{N} $$\n\nStep 4: Optimization Problem\n\nWe are tasked to minimize the objective function $J(\\Delta\\theta)$ subject to the derived constraint. The problem is:\n$$ \\min_{\\Delta\\theta  0} J(\\Delta\\theta) = \\frac{2c_0\\Theta_{\\max}}{\\Delta\\theta} + \\frac{(\\Delta\\theta)^2}{12} \\quad \\text{subject to} \\quad \\Delta\\theta \\le \\frac{4}{N} $$\nFirst, we find the unconstrained minimum by setting the derivative of $J(\\Delta\\theta)$ with respect to $\\Delta\\theta$ to zero:\n$$ \\frac{\\mathrm{d}J}{\\mathrm{d}(\\Delta\\theta)} = -\\frac{2c_0\\Theta_{\\max}}{(\\Delta\\theta)^2} + \\frac{2\\Delta\\theta}{12} = -\\frac{2c_0\\Theta_{\\max}}{(\\Delta\\theta)^2} + \\frac{\\Delta\\theta}{6} $$\nSetting the derivative to zero gives:\n$$ \\frac{\\Delta\\theta}{6} = \\frac{2c_0\\Theta_{\\max}}{(\\Delta\\theta)^2} \\implies (\\Delta\\theta)^3 = 12c_0\\Theta_{\\max} $$\nThe unconstrained optimal spacing is $\\Delta\\theta_{\\text{unc}} = (12c_0\\Theta_{\\max})^{1/3}$.\nThe second derivative, $\\frac{\\mathrm{d}^2J}{\\mathrm{d}(\\Delta\\theta)^2} = \\frac{4c_0\\Theta_{\\max}}{(\\Delta\\theta)^3} + \\frac{1}{6}$, is positive for $\\Delta\\theta  0$, confirming this is a minimum.\n\nThe function $J(\\Delta\\theta)$ decreases for $\\Delta\\theta  \\Delta\\theta_{\\text{unc}}$ and increases for $\\Delta\\theta  \\Delta\\theta_{\\text{unc}}$. The solution to the constrained problem is thus found by comparing $\\Delta\\theta_{\\text{unc}}$ with the constraint boundary $4/N$.\n$$ \\Delta\\theta^\\star = \\min\\left(\\Delta\\theta_{\\text{unc}}, \\frac{4}{N}\\right) = \\min\\left( (12c_0\\Theta_{\\max})^{1/3}, \\frac{4}{N} \\right) $$\n\nStep 5: Numerical Calculation\n\nWe substitute the given values: $N=64$, $\\Theta_{\\max}=\\pi/3$, and $c_0=10^{-6}$.\nFirst, calculate the constraint boundary:\n$$ \\frac{4}{N} = \\frac{4}{64} = \\frac{1}{16} = 0.0625 \\text{ radians} $$\nNext, calculate the unconstrained minimizer:\n$$ \\Delta\\theta_{\\text{unc}} = (12 \\times 10^{-6} \\times \\frac{\\pi}{3})^{1/3} = (4\\pi \\times 10^{-6})^{1/3} $$\nUsing $\\pi \\approx 3.14159265$:\n$$ \\Delta\\theta_{\\text{unc}} = (4 \\times 3.14159265 \\times 10^{-6})^{1/3} = (12.56637 \\times 10^{-6})^{1/3} \\approx 0.023248 \\text{ radians} $$\nComparing the two values:\n$$ \\Delta\\theta_{\\text{unc}} \\approx 0.023248  0.0625 = \\frac{4}{N} $$\nSince the unconstrained minimum lies within the feasible region defined by the no-miss-peak constraint, it is the optimal solution.\n$$ \\Delta\\theta^\\star = \\Delta\\theta_{\\text{unc}} \\approx 0.023248 \\text{ radians} $$\nThe problem requires rounding the result to four significant figures.\n$$ \\Delta\\theta^\\star \\approx 0.02325 \\text{ radians} $$\nThis is the optimal grid spacing that minimizes the combined cost of computation and discretization error while guaranteeing that no spectral peak is missed by the grid search, under the given modeling assumptions.", "answer": "$$\\boxed{0.02325}$$", "id": "2866495"}]}