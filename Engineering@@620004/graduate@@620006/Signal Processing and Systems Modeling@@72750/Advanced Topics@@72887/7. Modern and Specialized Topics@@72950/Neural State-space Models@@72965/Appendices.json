{"hands_on_practices": [{"introduction": "Many powerful neural state-space models, such as the S4 family, are founded upon the well-understood principles of continuous-time linear time-invariant (LTI) systems. However, to be applied to discrete sequence data, these continuous models must be transformed into a discrete-time representation. This exercise guides you through the fundamental process of discretization using the Zero-Order Hold (ZOH) assumption, a crucial bridge between continuous-time theory and practical, digital implementation [@problem_id:2886125].", "problem": "You are given continuous-time linear time-invariant state-space models as the linear backbone of a neural state-space modeling pipeline. The backbone is specified by matrices $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{p \\times n}$, and $D \\in \\mathbb{R}^{p \\times m}$, together with a sampling period $\\Delta \\in \\mathbb{R}_{>0}$. Assuming a Zero-Order Hold (ZOH) on the input, derive from first principles a correct discrete-time state update and output equation that map sampling instants $k \\in \\mathbb{Z}_{\\ge 0}$ to $k+1$. Use only the fundamental solution of linear time-invariant dynamics and the ZOH assumption as your starting point. Then, implement a program that computes the discrete-time matrices $(A_d,B_d,C_d,D_d)$ and simulates the discrete-time output sequence for a given piecewise constant input.\n\nThe continuous-time model is\n$x'(t) = A x(t) + B u(t)$ and $y(t) = C x(t) + D u(t)$.\nUnder the Zero-Order Hold (ZOH) assumption, the input $u(t)$ is held constant over each sampling interval $[k\\Delta, (k+1)\\Delta)$; denote that constant value by $u_k \\in \\mathbb{R}^m$. The discrete-time model is of the form\n$x_{k+1} = A_d x_k + B_d u_k$ and $y_k = C_d x_k + D_d u_k$,\nfor $k \\in \\mathbb{Z}_{\\ge 0}$.\n\nYour program must:\n- Compute $(A_d,B_d,C_d,D_d)$ implied by the ZOH assumption, without using any shortcut formulas stated a priori in this problem statement, but following from the fundamental solution of linear time-invariant systems and the ZOH input model.\n- Simulate the output sequence $\\{y_k\\}_{k=0}^{N-1}$ for each test case, given an initial state $x_0$ and a piecewise constant input schedule specified over $N$ steps.\n- For each test case, return a single real number equal to the sum of squares of the output sequence, that is $\\sum_{k=0}^{N-1} \\lVert y_k \\rVert_2^2$, expressed as a real scalar (no physical units are involved).\n- Round each returned scalar to exactly $6$ decimal places.\n\nInput schedules are given as a list of segments $(k_{\\mathrm{start}}, k_{\\mathrm{end}}, v)$, where $k_{\\mathrm{start}}$ and $k_{\\mathrm{end}}$ are integers with $0 \\le k_{\\mathrm{start}} \\le k_{\\mathrm{end}} \\le N-1$, and $v \\in \\mathbb{R}^m$ is the constant input applied for all integer $k$ in the inclusive range $\\{k_{\\mathrm{start}}, \\dots, k_{\\mathrm{end}}\\}$.\n\nTest Suite:\n- Test Case $1$ (happy path, second-order, strictly proper):\n  - $A = \\begin{bmatrix} 0 & 1 \\\\ -2 & -0.5 \\end{bmatrix}$, $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$, $D = \\begin{bmatrix} 0 \\end{bmatrix}$.\n  - $\\Delta = 0.1$.\n  - $x_0 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n  - $N = 50$.\n  - Input segments: $(0, 19, [1.0])$, $(20, 49, [0.0])$.\n- Test Case $2$ (boundary case with $A$ equal to zero and nonzero direct feedthrough):\n  - $A = \\begin{bmatrix} 0 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 \\end{bmatrix}$, $D = \\begin{bmatrix} 0.5 \\end{bmatrix}$.\n  - $\\Delta = 0.25$.\n  - $x_0 = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n  - $N = 8$.\n  - Input segments: $(0, 3, [2.0])$, $(4, 7, [-2.0])$.\n- Test Case $3$ (oscillatory marginal dynamics):\n  - $A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$, $B = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$, $C = \\begin{bmatrix} 0 & 1 \\end{bmatrix}$, $D = \\begin{bmatrix} 0 \\end{bmatrix}$.\n  - $\\Delta = 0.2$.\n  - $x_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$.\n  - $N = 20$.\n  - Input segments: $(0, 9, [0.3])$, $(10, 19, [0.0])$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with each number rounded to exactly six decimal places and without any additional text. For example: \"[1.234000,5.678900,0.000123]\".", "solution": "The problem requires the derivation and implementation of a method to discretize a continuous-time linear time-invariant (LTI) state-space model under the Zero-Order Hold (ZOH) assumption.\n\nThe continuous-time model is given by the state and output equations:\n$$\nx'(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nwhere $x(t) \\in \\mathbb{R}^n$ is the state vector, $u(t) \\in \\mathbb{R}^m$ is the input vector, and $y(t) \\in \\mathbb{R}^p$ is the output vector. The matrices $A$, $B$, $C$, and $D$ have compatible dimensions.\n\nThe objective is to find the equivalent discrete-time model sampled with a period $\\Delta > 0$, of the form:\n$$\nx_{k+1} = A_d x_k + B_d u_k\n$$\n$$\ny_k = C_d x_k + D_d u_k\n$$\nwhere $x_k = x(k\\Delta)$, $y_k = y(k\\Delta)$, and $u_k$ is the constant input value over the interval $[k\\Delta, (k+1)\\Delta)$.\n\nThe derivation proceeds from the fundamental solution of the continuous-time LTI state equation, which is given by the variation of constants formula. For an initial time $t_0$ and a final time $t$, the state is:\n$$\nx(t) = e^{A(t-t_0)} x(t_0) + \\int_{t_0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau\n$$\n\nWe apply this formula over a single sampling interval, from $t_0 = k\\Delta$ to $t = (k+1)\\Delta$. The state at the beginning of the interval is $x(k\\Delta) = x_k$. The state at the end is $x((k+1)\\Delta) = x_{k+1}$. The ZOH assumption states that the input $u(\\tau)$ is constant for $\\tau \\in [k\\Delta, (k+1)\\Delta)$, with the value $u_k$.\n\nSubstituting these into the solution formula:\n$$\nx_{k+1} = e^{A((k+1)\\Delta - k\\Delta)} x_k + \\int_{k\\Delta}^{(k+1)\\Delta} e^{A((k+1)\\Delta - \\tau)} B u_k d\\tau\n$$\n\nWe can simplify this expression. The first term gives the state transition matrix for the discrete system:\n$$\ne^{A\\Delta} x_k\n$$\nBy inspection, we identify the discrete-time state matrix $A_d$:\n$$\nA_d = e^{A\\Delta}\n$$\nThe second term involves the integral. Since $u_k$ is a constant vector over the integration interval, it can be factored out of the integral:\n$$\n\\left( \\int_{k\\Delta}^{(k+1)\\Delta} e^{A((k+1)\\Delta - \\tau)} d\\tau \\right) B u_k\n$$\nTo evaluate the integral, we perform a change of variables. Let $\\sigma = (k+1)\\Delta - \\tau$. Then $d\\sigma = -d\\tau$. The integration limits change from $\\tau=k\\Delta \\rightarrow \\sigma=\\Delta$ and $\\tau=(k+1)\\Delta \\rightarrow \\sigma=0$.\n$$\n\\int_{\\Delta}^{0} e^{A\\sigma} (-d\\sigma) = \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma\n$$\nThus, the second term is $\\left(\\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B u_k$.\nBy comparing with the discrete-time model form, we identify the discrete-time input matrix $B_d$:\n$$\nB_d = \\left( \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B\n$$\nThe complete discrete-time state update equation is therefore:\n$$\nx_{k+1} = (e^{A\\Delta}) x_k + \\left( \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B u_k\n$$\nWhile the integral can be computed as $A^{-1}(e^{A\\Delta}-I)$ if $A$ is invertible, a more general and numerically robust method is required for the implementation. This method, known as the matrix exponential method or van Loan's method, computes $A_d$ and $B_d$ simultaneously. Consider the augmented matrix $M$ of size $(n+m) \\times (n+m)$:\n$$\nM = \\begin{bmatrix} A & B \\\\ 0 & 0 \\end{bmatrix}\n$$\nThe matrix exponential of $M\\Delta$ can be expressed using a block matrix formulation. The series expansion of the exponential shows that:\n$$\ne^{M\\Delta} = \\begin{bmatrix} e^{A\\Delta} & \\left(\\int_0^\\Delta e^{A\\sigma} d\\sigma\\right) B \\\\ 0 & I_m \\end{bmatrix} = \\begin{bmatrix} A_d & B_d \\\\ 0 & I_m \\end{bmatrix}\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix. This allows for the computation of both $A_d$ and $B_d$ using a single matrix exponential calculation, which is robust even for singular $A$.\n\nNext, we derive the discrete-time output matrices, $C_d$ and $D_d$. The discrete output $y_k$ is the sampled value of the continuous output at time $t=k\\Delta$:\n$$\ny_k = y(k\\Delta) = C x(k\\Delta) + D u(k\\Delta)\n$$\nUsing the definitions $x_k = x(k\\Delta)$ and the ZOH property $u(k\\Delta) = u_k$, we have:\n$$\ny_k = C x_k + D u_k\n$$\nComparing this to the generic form $y_k = C_d x_k + D_d u_k$, we directly identify:\n$$\nC_d = C\n$$\n$$\nD_d = D\n$$\nThis completes the derivation. The implementation will construct the augmented matrix $M$, compute its exponential to find $A_d$ and $B_d$, and then use all four discrete matrices $(A_d, B_d, C_d, D_d)$ to simulate the system response over $N$ steps. For each step $k$ from $0$ to $N-1$, the output $y_k$ is computed, and the sum of its squared Euclidean norms, $\\sum_{k=0}^{N-1} \\lVert y_k \\rVert_2^2$, is accumulated.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the problem by discretizing continuous-time state-space models\n    and simulating their response to compute a metric.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[0.0, 1.0], [-2.0, -0.5]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"C\": np.array([[1.0, 0.0]]),\n            \"D\": np.array([[0.0]]),\n            \"Delta\": 0.1,\n            \"x0\": np.array([1.0, 0.0]),\n            \"N\": 50,\n            \"input_segments\": [(0, 19, [1.0]), (20, 49, [0.0])],\n        },\n        {\n            \"A\": np.array([[0.0]]),\n            \"B\": np.array([[1.0]]),\n            \"C\": np.array([[1.0]]),\n            \"D\": np.array([[0.5]]),\n            \"Delta\": 0.25,\n            \"x0\": np.array([0.0]),\n            \"N\": 8,\n            \"input_segments\": [(0, 3, [2.0]), (4, 7, [-2.0])],\n        },\n        {\n            \"A\": np.array([[0.0, -1.0], [1.0, 0.0]]),\n            \"B\": np.array([[0.0], [0.5]]),\n            \"C\": np.array([[0.0, 1.0]]),\n            \"D\": np.array([[0.0]]),\n            \"Delta\": 0.2,\n            \"x0\": np.array([0.0, 1.0]),\n            \"N\": 20,\n            \"input_segments\": [(0, 9, [0.3]), (10, 19, [0.0])],\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, B, C, D = case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"]\n        Delta, x0, N = case[\"Delta\"], case[\"x0\"], case[\"N\"]\n        input_segments = case[\"input_segments\"]\n\n        n, m = A.shape[1], B.shape[1]\n\n        # Construct the augmented matrix M for ZOH discretization.\n        M = np.zeros((n + m, n + m))\n        M[:n, :n] = A\n        M[:n, n:] = B\n        \n        # Compute the matrix exponential.\n        E = expm(M * Delta)\n        \n        # Extract discrete-time matrices Ad and Bd.\n        Ad = E[:n, :n]\n        Bd = E[:n, n:]\n        \n        # Cd and Dd are the same as C and D for ZOH sampled at t_k.\n        Cd = C\n        Dd = D\n\n        # Construct the full input sequence u_k for k=0...N-1.\n        u_sequence = np.zeros((N, m))\n        for k_start, k_end, v in input_segments:\n            for k in range(k_start, k_end + 1):\n                u_sequence[k, :] = v\n\n        # Simulate the discrete-time system.\n        x_current = x0.copy().reshape(-1, 1)\n        sum_of_squares = 0.0\n\n        for k in range(N):\n            u_k = u_sequence[k].reshape(-1, 1)\n\n            # Calculate output y_k = C_d * x_k + D_d * u_k\n            y_k = Cd @ x_current + Dd @ u_k\n\n            # Accumulate the sum of squares of the L2 norm of the output.\n            sum_of_squares += np.sum(y_k**2)\n\n            # Update state for next step: x_{k+1} = A_d * x_k + B_d * u_k\n            x_current = Ad @ x_current + Bd @ u_k\n            \n        results.append(sum_of_squares)\n\n    # Format the final output string as required.\n    output_str = \"[\" + \",\".join([f\"{r:.6f}\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2886125"}, {"introduction": "Once a discrete-time state-space model is obtained, its output can be computed either through a step-by-step state recursion or as a global convolution with the system's impulse response. Modern architectures like S4 prefer the convolution viewpoint for its remarkable computational efficiency, especially on parallel hardware. This practice delves into this powerful technique, using the Fast Fourier Transform (FFT) to first generate the model's convolution kernel and then to compute the system's output for long input sequences [@problem_id:2886012].", "problem": "You are given a discrete-time Structured State Space Sequence model (S4) in diagonal form (often called S4D), which is a linear time-invariant neural state-space model. The model evolves according to the equations\n$$\nx_{n+1} = A x_n + B u_n,\\quad y_n = C x_n + D u_n,\n$$\nwhere $x_n \\in \\mathbb{C}^d$ is the hidden state, $u_n \\in \\mathbb{R}$ is the input, $y_n \\in \\mathbb{R}$ is the output, $A \\in \\mathbb{C}^{d \\times d}$ is diagonal with entries $a_i$ that satisfy $|a_i| < 1$ for all $i$, $B \\in \\mathbb{C}^{d}$, $C \\in \\mathbb{C}^{d}$, and $D \\in \\mathbb{R}$. Assume the initial condition $x_0 = 0$.\n\nFor this system, the impulse response (convolution kernel) $\\{k_n\\}_{n\\ge 0}$ is defined by\n$$\nk_0 = D \\quad \\text{and} \\quad k_n = C A^{n-1} B \\quad \\text{for } n \\ge 1.\n$$\nThe $z$-transform (transfer function) of the system on the unit circle is the rational function $H(\\zeta) = \\sum_{n=0}^\\infty k_n \\zeta^{-n}$, which for this system is equivalent to\n$$\nH(\\zeta) = D + C(\\zeta I - A)^{-1}B = D + \\zeta^{-1} C(I - \\zeta^{-1}A)^{-1}B, \\quad \\text{for } \\zeta \\in \\mathbb{C},\\ |\\zeta| = 1.\n$$\n\nYour task is to compute the first $N$ coefficients of the kernel $\\{k_0, k_1, \\dots, k_{N-1}\\}$ by evaluating the rational function $H(\\zeta)$ on the $L$-th roots of unity, where $L$ is the smallest power of two greater than or equal to $N$, and then applying an inverse Discrete Fourier Transform (DFT). After obtaining the kernel, compute the causal output sequence for a given input by linear convolution. Use Fast Fourier Transform (FFT) algorithms to implement both steps efficiently.\n\nFundamental definitions to use:\n- The Discrete Fourier Transform (DFT) of a length-$L$ sequence $\\{x_n\\}_{n=0}^{L-1}$ is $X_j = \\sum_{n=0}^{L-1} x_n e^{-2\\pi \\mathrm{i} j n / L}$ for $j \\in \\{0,\\dots,L-1\\}$, and the inverse DFT is $x_n = \\frac{1}{L} \\sum_{j=0}^{L-1} X_j e^{2\\pi \\mathrm{i} j n / L}$.\n- The $L$-th roots of unity are $\\zeta_j = e^{2\\pi \\mathrm{i} j / L}$ for $j \\in \\{0,\\dots,L-1\\}$.\n- For the diagonal $A = \\mathrm{diag}(a_1,\\dots,a_d)$, the transfer function evaluates as\n$$\nH(\\zeta) = D + \\zeta^{-1} \\sum_{i=1}^d \\frac{c_i b_i}{1 - \\zeta^{-1} a_i},\n$$\nwhere $b_i$ and $c_i$ are the $i$-th entries of $B$ and $C$, respectively.\n\nRequired algorithm:\n1. Given $N$, set $L$ to the smallest power of two with $L \\ge N$.\n2. For each $j \\in \\{0,\\dots,L-1\\}$, set $\\zeta_j = e^{2\\pi \\mathrm{i} j / L}$ and compute $H_j = H(\\zeta_j)$ using the diagonal evaluation formula.\n3. Compute the inverse DFT of $\\{H_j\\}_{j=0}^{L-1}$ to obtain a length-$L$ sequence whose first $N$ entries are the desired kernel $\\{k_0,\\dots,k_{N-1}\\}$.\n4. Given an input sequence $\\{u_n\\}_{n=0}^{T-1}$, compute the causal output $\\{y_n\\}_{n=0}^{T-1}$ by linear convolution:\n$$\ny_n = \\sum_{m=0}^{n} k_m u_{n-m}.\n$$\nImplement the convolution efficiently by zero-padding to length $P$ where $P$ is a power of two satisfying $P \\ge T + N - 1$, computing DFTs of the zero-padded sequences, multiplying in the frequency domain, and applying the inverse DFT, then truncating to the first $T$ samples.\n\nDesign a program that, for each specified test case below, computes:\n- The kernel via rational evaluation and inverse DFT as above.\n- The output via FFT-based convolution with the given input.\n- The output via direct state-space recursion with $x_{n+1} = A x_n + B u_n$ and $y_n = C x_n + D u_n$ from $n=0$ to $n=T-1$.\n- The maximum absolute difference between the two outputs, reported as a real-valued float.\n\nAngles, when present in trigonometric functions, must be in radians.\n\nTest suite (five cases covering general, complex conjugate, near-boundary, degenerate, and mixed scenarios):\n- Case $1$ (general real diagonal):\n  - $N = 16$, $T = 16$.\n  - $A = \\mathrm{diag}(0.9, 0.5)$, $B = [1.0, -0.3]^\\top$, $C = [0.7, 0.2]^\\top$, $D = 0.1$.\n  - Input: $u_n = \\sin\\!\\big(2\\pi n / 8\\big)$ for $n \\in \\{0,\\dots,15\\}$, where the angle is in radians.\n- Case $2$ (complex conjugate diagonal ensuring real outputs):\n  - $N = 32$, $T = 32$.\n  - $A = \\mathrm{diag}\\!\\big(0.8 e^{\\mathrm{i}\\pi/6},\\, 0.8 e^{-\\mathrm{i}\\pi/6}\\big)$,\n    $B = [0.5 + 0.2\\mathrm{i},\\, 0.5 - 0.2\\mathrm{i}]^\\top$,\n    $C = [1.0 - 0.1\\mathrm{i},\\, 1.0 + 0.1\\mathrm{i}]^\\top$,\n    $D = 0.0$.\n  - Input: $u_n = 1$ for all $n \\in \\{0,\\dots,31\\}$.\n- Case $3$ (near stability boundary, long tail):\n  - $N = 64$, $T = 64$.\n  - $A = \\mathrm{diag}(0.999)$, $B = [1.0]^\\top$, $C = [1.0]^\\top$, $D = 0.0$.\n  - Input: $u_n = 1$ for all $n \\in \\{0,\\dots,63\\}$.\n- Case $4$ (degenerate dynamics and nonzero feedthrough):\n  - $N = 8$, $T = 8$.\n  - $A = \\mathrm{diag}(0.0, 0.0)$, $B = [0.3, -0.1]^\\top$, $C = [0.4, 0.5]^\\top$, $D = 0.25$.\n  - Input: $u_0 = 1$ and $u_n = 0$ for $n \\in \\{1,\\dots,7\\}$.\n- Case $5$ (mixed real poles with oscillatory input):\n  - $N = 20$, $T = 20$.\n  - $A = \\mathrm{diag}(-0.3, 0.2, -0.8)$, $B = [1.0, -0.5, 0.25]^\\top$, $C = [0.5, 0.6, -0.4]^\\top$, $D = -0.2$.\n  - Input: $u_n = \\cos\\!\\big(2\\pi n / 5\\big)$ for $n \\in \\{0,\\dots,19\\}$, where the angle is in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5]\"), where each $r_i$ is the maximum absolute difference (a real-valued float) for Case $i$ between the FFT-based convolution output and the direct state-space recursion output.", "solution": "The problem requires the implementation and verification of two equivalent methods for computing the output of a discrete-time linear time-invariant (LTI) system: direct state-space recursion and FFT-based convolution. The goal is to confirm that both methods produce the same output, with any discrepancies attributable to numerical floating-point precision errors.\n\nFirst, we analyze the state-space representation provided:\n$$\n\\begin{align*}\nx_{n+1} &= A x_n + B u_n \\\\\ny_n &= C x_n + D u_n\n\\end{align*}\n$$\nwith initial condition $x_0 = 0$. The output $y_n$ can be expressed as a function of the input sequence $\\{u_m\\}_{m=0}^n$ by unrolling the recurrence. The state at time $n$ is given by $x_n = \\sum_{m=0}^{n-1} A^{n-1-m} B u_m$. Substituting this into the output equation yields:\n$$\ny_n = C \\left( \\sum_{m=0}^{n-1} A^{n-1-m} B u_m \\right) + D u_n\n$$\nThis is a linear convolution of the input $u_n$ with the system's impulse response, which we denote $\\{k_n\\}$. By setting the input to a discrete-time impulse ($u_n = \\delta_n$, i.e., $u_0=1$ and $u_n=0$ for $n>0$), we find the impulse response sequence to be:\n$$\nk_0 = D, \\quad k_n = C A^{n-1} B \\quad \\text{for } n \\ge 1\n$$\nThis is the kernel implicitly simulated by the direct state-space recursion method.\n\nNext, we analyze the convolution-based method. This relies on the equivalence between convolution in the time domain and multiplication in the frequency domain. The frequency-domain representation of the system is its transfer function, $H(\\zeta)$, which is the $z$-transform of its impulse response, $H(\\zeta) = \\sum_{n=0}^\\infty k_n \\zeta^{-n}$. For the given state-space system, the transfer function is:\n$$\nH(\\zeta) = D + C(\\zeta I - A)^{-1}B\n$$\nFor a stable system evaluated on the unit circle ($|\\zeta|=1$), this can be rewritten using the Neumann series expansion:\n$$\nH(\\zeta) = D + C(\\zeta(I - \\zeta^{-1}A))^{-1}B = D + \\zeta^{-1}C(I - \\zeta^{-1}A)^{-1}B = D + \\zeta^{-1}C\\left(\\sum_{j=0}^{\\infty} (\\zeta^{-1}A)^j \\right)B\n$$\n$$\nH(\\zeta) = D + \\sum_{j=0}^{\\infty} (CA^j B)\\zeta^{-(j+1)} = D + \\sum_{n=1}^{\\infty} (CA^{n-1}B)\\zeta^{-n}\n$$\nThis confirms that the coefficients of the Laurent series of $H(\\zeta)$ are precisely the impulse response terms $\\{k_n\\}$ derived earlier. Thus, both methods represent the same LTI system. The problem asks to verify this equivalence computationally.\n\nThe implementation proceeds in four steps for each test case:\n\n1.  **Kernel Computation via Inverse FFT**: Given a desired kernel length $N$, we find the smallest power of two, $L$, such that $L \\ge N$. We then evaluate the transfer function $H(\\zeta)$ at the $L$-th roots of unity, $\\zeta_j = e^{2\\pi \\mathrm{i} j / L}$ for $j=0, \\dots, L-1$. This gives a sequence of frequency-domain samples $H_j = H(\\zeta_j)$. The formula for a diagonal matrix $A = \\mathrm{diag}(a_1, \\dots, a_d)$ facilitates this computation:\n    $$\n    H_j = D + \\zeta_j^{-1} \\sum_{i=1}^d \\frac{c_i b_i}{1 - \\zeta_j^{-1} a_i}\n    $$\n    The inverse DFT (implemented via IFFT) of the sequence $\\{H_j\\}$ yields a length-$L$ sequence, which is an aliased version of the true kernel. We truncate this to its first $N$ elements to obtain the convolution kernel. For problem configurations with conjugate-symmetric parameters and real inputs, the resulting kernel is guaranteed to be real, so we take the real part of the IFFT output to discard numerical floating-point noise.\n\n2.  **Output via FFT-based Convolution**: Linear convolution of the computed kernel of length $N$ and an input signal of length $T$ is implemented efficiently using the FFT. Both kernel and input sequences are zero-padded to a length $P$, where $P$ is a power of two satisfying $P \\ge T+N-1$. Their respective FFTs are computed, multiplied element-wise in the frequency domain, and the result is transformed back to the time domain via IFFT. The resulting sequence is truncated to the first $T$ samples to get the final output.\n\n3.  **Output via State-Space Recursion**: This is a direct time-domain simulation. The state vector $x$ is initialized to zero. The state and output equations are iterated from $n=0$ to $T-1$:\n    - $y_n = \\mathrm{Re}(C x_n) + D u_n$\n    - $x_{n+1} = A x_n + B u_n$\n    Since $A$ is diagonal, the state update is an element-wise operation.\n\n4.  **Comparison**: The two output vectors, one from FFT convolution ($y_{\\text{conv}}$) and one from recursion ($y_{\\text{recur}}$), are compared. As the methods are theoretically equivalent, the difference should be close to zero. The final metric is the maximum absolute difference over the sequence length $T$:\n    $$\n    \\max_{0 \\le n < T} | y_{\\text{conv}}[n] - y_{\\text{recur}}[n] |\n    $$\nThis procedure is applied to each test case, and the resulting maximum differences, which reflect the limits of floating-point precision, are reported.", "answer": "```python\nimport numpy as np\nfrom scipy import fft\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases specified in the problem.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 16, \"T\": 16,\n            \"A\": np.array([0.9, 0.5]),\n            \"B\": np.array([1.0, -0.3]),\n            \"C\": np.array([0.7, 0.2]),\n            \"D\": 0.1,\n            \"u_func\": lambda n: np.sin(2 * np.pi * n / 8)\n        },\n        {\n            \"N\": 32, \"T\": 32,\n            \"A\": np.array([0.8 * np.exp(1j * np.pi / 6), 0.8 * np.exp(-1j * np.pi / 6)]),\n            \"B\": np.array([0.5 + 0.2j, 0.5 - 0.2j]),\n            \"C\": np.array([1.0 - 0.1j, 1.0 + 0.1j]),\n            \"D\": 0.0,\n            \"u_func\": lambda n: 1.0\n        },\n        {\n            \"N\": 64, \"T\": 64,\n            \"A\": np.array([0.999]),\n            \"B\": np.array([1.0]),\n            \"C\": np.array([1.0]),\n            \"D\": 0.0,\n            \"u_func\": lambda n: 1.0\n        },\n        {\n            \"N\": 8, \"T\": 8,\n            \"A\": np.array([0.0, 0.0]),\n            \"B\": np.array([0.3, -0.1]),\n            \"C\": np.array([0.4, 0.5]),\n            \"D\": 0.25,\n            \"u_func\": lambda n: 1.0 if n == 0 else 0.0\n        },\n        {\n            \"N\": 20, \"T\": 20,\n            \"A\": np.array([-0.3, 0.2, -0.8]),\n            \"B\": np.array([1.0, -0.5, 0.25]),\n            \"C\": np.array([0.5, 0.6, -0.4]),\n            \"D\": -0.2,\n            \"u_func\": lambda n: np.cos(2 * np.pi * n / 5)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_case(case_params):\n    \"\"\"\n    Runs a single test case and returns the maximum absolute difference.\n    \"\"\"\n    N = case_params[\"N\"]\n    T = case_params[\"T\"]\n    A = case_params[\"A\"]\n    B = case_params[\"B\"]\n    C = case_params[\"C\"]\n    D = case_params[\"D\"]\n    u_func = case_params[\"u_func\"]\n    \n    u = np.array([u_func(n) for n in range(T)])\n\n    kernel = compute_kernel_fft(A, B, C, D, N)\n    y_conv = compute_output_fft_conv(kernel, u, N, T)\n    y_recur = compute_output_recursion(A, B, C, D, u, T)\n    \n    max_diff = np.max(np.abs(y_conv - y_recur))\n    return max_diff\n\ndef compute_kernel_fft(A, B, C, D, N):\n    \"\"\"\n    Computes the kernel using the transfer function and IFFT.\n    \"\"\"\n    if N == 0:\n        return np.array([])\n    L = 1  (N - 1).bit_length()\n    \n    zeta_inv = np.exp(-2j * np.pi * np.arange(L) / L)\n    \n    C_d_1 = C.reshape(-1, 1)\n    B_d_1 = B.reshape(-1, 1)\n    A_d_1 = A.reshape(-1, 1)\n\n    # H(zeta) = D + zeta_inv * C * (I - zeta_inv * A)^-1 * B\n    sum_term = np.sum((C_d_1 * B_d_1) / (1 - A_d_1 * zeta_inv), axis=0)\n    H_freq_domain = D + zeta_inv * sum_term\n    \n    kernel_L = fft.ifft(H_freq_domain)\n    return np.real(kernel_L[:N])\n\ndef compute_output_fft_conv(kernel, u, N, T):\n    \"\"\"\n    Computes the output using FFT-based linear convolution.\n    \"\"\"\n    if T == 0:\n        return np.array([])\n    conv_len = N + T - 1\n    P = 1  (conv_len - 1).bit_length()\n    \n    K_fft = fft.fft(kernel, n=P)\n    U_fft = fft.fft(u, n=P)\n    \n    Y_fft = K_fft * U_fft\n    y_conv_P = fft.ifft(Y_fft)\n    \n    return np.real(y_conv_P[:T])\n\ndef compute_output_recursion(A, B, C, D, u, T):\n    \"\"\"\n    Computes the output by iterating the state-space equations.\n    \"\"\"\n    if T == 0:\n        return np.array([])\n    d = A.shape[0]\n    x = np.zeros(d, dtype=np.complex128)\n    y_recur = np.zeros(T, dtype=np.float64)\n    \n    for n in range(T):\n        y_recur[n] = np.real(np.dot(C, x)) + D * u[n]\n        x = A * x + B * u[n]\n        \n    return y_recur\n\nsolve()\n```", "id": "2886012"}, {"introduction": "Having established how to model and efficiently compute with state-space systems, we address the core question of learning: how are the system matrices $(A, B, C, D)$ determined from data? This final practice explores the field of system identification, where you will recover a state-space realization from raw input-output observations. Crucially, this process illuminates the concept of a similarity transformation, revealing that the internal state representation $x_k$ is not uniqueâ€”a fundamental insight for understanding and interpreting the latent spaces learned by neural state-space models [@problem_id:2886056].", "problem": "You are given a sequence of linear time-invariant state-space models that instantiate a special case of a neural state-space model with affine state and output mappings, namely $f(\\mathbf{x},\\mathbf{u}) = A\\mathbf{x} + B\\mathbf{u}$ and $g(\\mathbf{x},\\mathbf{u}) = C\\mathbf{x} + D\\mathbf{u}$. For each model, you must generate input-output data, fit a realization from data only, and then compute a similarity transformation that aligns the estimated realization to the ground-truth.\n\nFundamental base. Use the standard linear time-invariant state-space equations and definitions: for discrete time index $k \\in \\mathbb{Z}_{\\ge 0}$, the state update and output equations are\n$$\n\\mathbf{x}_{k+1} = A \\mathbf{x}_k + B \\mathbf{u}_k, \\quad \\mathbf{y}_k = C \\mathbf{x}_k + D \\mathbf{u}_k,\n$$\nwith state $\\mathbf{x}_k \\in \\mathbb{R}^n$, input $\\mathbf{u}_k \\in \\mathbb{R}^m$, and output $\\mathbf{y}_k \\in \\mathbb{R}^p$. The impulse response (also called Markov parameters) $\\{H_i\\}_{i\\ge 0}$ satisfies $H_0 = D$ and $H_i = C A^{i-1} B$ for $i \\ge 1$. Any two minimal realizations that produce the same input-output behavior are related by a similarity transformation: if $(\\hat{A},\\hat{B},\\hat{C},\\hat{D})$ is another realization of the same transfer behavior, then there exists an invertible matrix $T \\in \\mathbb{R}^{n \\times n}$ such that $\\hat{A} = T^{-1} A T$, $\\hat{B} = T^{-1} B$, and $\\hat{C} = C T$, with $\\hat{D} = D$.\n\nTask. For each test case below, perform the following steps purely from input-output data generated from the given ground-truth $(A,B,C,D)$:\n1. Data generation. Using a fixed pseudorandom seed as specified in each test case, generate a zero-mean independent and identically distributed Gaussian input sequence $\\{\\mathbf{u}_k\\}_{k=0}^{N-1}$ with identity covariance in $\\mathbb{R}^m$. Simulate the system with initial state $\\mathbf{x}_0 = \\mathbf{0}$ to produce outputs $\\{\\mathbf{y}_k\\}_{k=0}^{N-1}$ using the exact state-space dynamics with no added noise.\n2. Markov parameter estimation. From the input-output data alone, estimate a finite sequence of Markov parameters $\\{\\hat{H}_i\\}_{i=0}^{L}$ by solving a linear least-squares problem that enforces the convolutional relation between outputs and past inputs over the horizon $L$, using $L = s + r + 10$, where $s$ and $r$ are specified per test case.\n3. Realization recovery via block Hankel factorization. Using the estimated $\\{\\hat{H}_i\\}$, build block Hankel matrices with $s$ block rows and $r$ block columns, then factor them using singular value decomposition (SVD) to obtain a realization $(\\hat{A},\\hat{B},\\hat{C},\\hat{D})$ of the specified state dimension $n$.\n4. Similarity alignment. Compute a similarity matrix $T \\in \\mathbb{R}^{n \\times n}$ that best aligns the estimated realization to the ground truth by enforcing the linear alignment constraints $T \\hat{B} \\approx B$, $C T \\approx \\hat{C}$, and $A T \\approx T \\hat{A}$ in the least-squares sense. Solve for $T$ as a single linear least-squares problem in the vectorized unknown entries of $T$.\n5. Quantitative error. Report the maximum of the three normalized Frobenius-norm residuals\n$$\n\\varepsilon_A = \\frac{\\lVert A T - T \\hat{A} \\rVert_F}{\\lVert A \\rVert_F + 10^{-12}}, \\quad\n\\varepsilon_B = \\frac{\\lVert T \\hat{B} - B \\rVert_F}{\\lVert B \\rVert_F + 10^{-12}}, \\quad\n\\varepsilon_C = \\frac{\\lVert C T - \\hat{C} \\rVert_F}{\\lVert C \\rVert_F + 10^{-12}},\n$$\nnamely output $e = \\max\\{\\varepsilon_A,\\varepsilon_B,\\varepsilon_C\\}$ for each test case.\n\nTest suite. Use the following three test cases. Each case specifies $(A,B,C,D)$, model dimensions $(n,m,p)$, simulation length $N$, Hankel design parameters $(s,r)$, and the pseudorandom seed for input generation.\n\n- Case 1 (single-input single-output):\n$$\nA = \\begin{bmatrix} 0.7  0.2 \\\\ -0.1  0.9 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 1.0  -0.3 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.1 \\end{bmatrix}.\n$$\nDimensions: $n = 2$, $m = 1$, $p = 1$. Use $N = 800$, $s = 5$, $r = 5$, seed $= 123$.\n\n- Case 2 (multi-input multi-output):\n$$\nA = \\begin{bmatrix} 0.6  0.2  0.0 \\\\ 0.0  0.7  0.1 \\\\ 0.0  -0.2  0.8 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\\\ 0.5  -0.2 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 1.0  0.0  0.3 \\\\ 0.2  0.8  -0.1 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}.\n$$\nDimensions: $n = 3$, $m = 2$, $p = 2$. Use $N = 1500$, $s = 6$, $r = 6$, seed $= 456$.\n\n- Case 3 (direct feedthrough emphasized):\n$$\nA = \\begin{bmatrix} 0.95 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.2 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 0.8 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.4 \\end{bmatrix}.\n$$\nDimensions: $n = 1$, $m = 1$, $p = 1$. Use $N = 400$, $s = 4$, $r = 4$, seed $= 789$.\n\nFinal output format. Your program should produce a single line of output containing the three scalar errors $e$ for the three cases, in order, rounded to exactly six digits after the decimal point, as a comma-separated list enclosed in square brackets, for example $[0.000123,0.001234,0.012345]$. No other text should be printed.", "solution": "The problem requires the identification of linear time-invariant (LTI) state-space models from input-output data and the subsequent alignment of the identified model with a ground-truth realization. The procedure is a standard multi-step process in system identification, which I will execute with methodical precision. Each step is grounded in fundamental principles of linear systems theory.\n\nThe system dynamics are described by the discrete-time state-space equations:\n$$\n\\mathbf{x}_{k+1} = A \\mathbf{x}_k + B \\mathbf{u}_k, \\quad \\mathbf{y}_k = C \\mathbf{x}_k + D \\mathbf{u}_k\n$$\nwhere $\\mathbf{x}_k \\in \\mathbb{R}^n$ is the state, $\\mathbf{u}_k \\in \\mathbb{R}^m$ is the input, and $\\mathbf{y}_k \\in \\mathbb{R}^p$ is the output at time step $k$. The matrices $(A, B, C, D)$ define the system.\n\nThe procedure is executed for each test case as follows:\n\n**Step 1: Data Generation**\nFirst, we simulate the system to generate a finite history of input-output pairs. For a given test case with parameters $(A, B, C, D, n, m, p, N, \\text{seed})$, we proceed as follows:\nA pseudorandom number generator is initialized with the specified seed to ensure reproducibility. An input sequence $\\{\\mathbf{u}_k\\}_{k=0}^{N-1}$ is generated, where each $\\mathbf{u}_k$ is drawn independently from a multivariate standard normal distribution, $\\mathcal{N}(\\mathbf{0}, I_m)$.\nWith the initial state set to the zero vector, $\\mathbf{x}_0 = \\mathbf{0}$, the system is simulated for $k = 0, 1, \\dots, N-1$ using the state-space equations to produce the output sequence $\\{\\mathbf{y}_k\\}_{k=0}^{N-1}$. No process or measurement noise is added, as per the problem statement.\n\n**Step 2: Markov Parameter Estimation**\nThe input-output relationship of an LTI system can be expressed via the convolution sum involving the system's impulse response, also known as Markov parameters $\\{H_i\\}_{i \\ge 0}$. The relationship is given by:\n$$\n\\mathbf{y}_k = \\sum_{i=0}^{\\infty} H_i \\mathbf{u}_{k-i}\n$$\nwhere $H_0 = D$ and $H_i = C A^{i-1} B$ for $i \\ge 1$. For practical estimation from finite data, we truncate this sum at a horizon $L$:\n$$\n\\mathbf{y}_k \\approx \\sum_{i=0}^{L} \\hat{H}_i \\mathbf{u}_{k-i}\n$$\nWe can estimate the sequence of Markov parameters $\\{\\hat{H}_i\\}_{i=0}^{L}$ by solving a linear least-squares problem. We form a set of linear equations for time steps $k = L, L+1, \\dots, N-1$. Let $\\mathcal{M} = [\\hat{H}_0, \\hat{H}_1, \\dots, \\hat{H}_L] \\in \\mathbb{R}^{p \\times (L+1)m}$ be the matrix of unknown parameters. Let $\\mathbf{\\Phi}_k = [\\mathbf{u}_k^T, \\mathbf{u}_{k-1}^T, \\dots, \\mathbf{u}_{k-L}^T]^T \\in \\mathbb{R}^{(L+1)m}$. The equations are $\\mathbf{y}_k^T = \\mathbf{\\Phi}_k^T \\mathcal{M}^T$. We stack these equations for $k=L, \\dots, N-1$ to form a large linear system $\\mathbf{Y} = \\mathbf{U}_{\\text{reg}} \\mathcal{M}^T$, where $\\mathbf{U}_{\\text{reg}} \\in \\mathbb{R}^{(N-L) \\times (L+1)m}$ is the regressor matrix whose rows are $\\mathbf{\\Phi}_k^T$, and $\\mathbf{Y} \\in \\mathbb{R}^{(N-L) \\times p}$ is the matrix of corresponding outputs.\nThe least-squares estimate for $\\mathcal{M}^T$ is obtained by solving this overdetermined system. From the resulting $\\hat{\\mathcal{M}}$, we extract the individual estimated Markov parameters $\\hat{H}_0, \\hat{H}_1, \\dots, \\hat{H}_L$. The horizon is set to $L = s + r + 10$, which is sufficiently long for the subsequent realization step.\n\n**Step 3: Realization Recovery via Block Hankel Factorization**\nThe Ho-Kalman algorithm provides a method to obtain a state-space realization $(\\hat{A}, \\hat{B}, \\hat{C})$ from the strictly proper part of the impulse response, i.e., from $\\{\\hat{H}_i\\}_{i \\ge 1}$. The direct feedthrough term is simply $\\hat{D} = \\hat{H}_0$.\nWe construct a block Hankel matrix of size $(s \\cdot p) \\times (r \\cdot m)$ using the estimated Markov parameters:\n$$\n\\mathcal{H}_{s,r} = \\begin{bmatrix}\n\\hat{H}_1  \\hat{H}_2  \\dots  \\hat{H}_r \\\\\n\\hat{H}_2  \\hat{H}_3  \\dots  \\hat{H}_{r+1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n\\hat{H}_s  \\hat{H}_{s+1}  \\dots  \\hat{H}_{s+r-1}\n\\end{bmatrix}\n$$\nThis matrix can be factored as the product of the extended observability matrix $\\mathcal{O}_s$ and the extended controllability matrix $\\mathcal{C}_r$: $\\mathcal{H}_{s,r} = \\mathcal{O}_s \\mathcal{C}_r$.\nWe perform a singular value decomposition (SVD) of $\\mathcal{H}_{s,r} = U \\Sigma V^T$. For a minimal realization of rank $n$, we truncate the SVD to the $n$ largest singular values: $U_n \\in \\mathbb{R}^{sp \\times n}$, $\\Sigma_n \\in \\mathbb{R}^{n \\times n}$, $V_n \\in \\mathbb{R}^{rm \\times n}$. A balanced realization is then given by choosing $\\mathcal{O}_s = U_n \\Sigma_n^{1/2}$ and $\\mathcal{C}_r = \\Sigma_n^{1/2} V_n^T$.\nFrom these factors, the system matrices are extracted:\n- $\\hat{C}$ is the first $p$ rows of $\\mathcal{O}_s$.\n- $\\hat{B}$ is the first $m$ columns of $\\mathcal{C}_r$.\n- $\\hat{A}$ is found using the shifted block Hankel matrix $\\mathcal{H}'_{s,r}$ (where all indices are incremented by 1), which satisfies $\\mathcal{H}'_{s,r} = \\mathcal{O}_s \\hat{A} \\mathcal{C}_r$. This gives $\\hat{A} = (\\mathcal{O}_s^\\dagger) \\mathcal{H}'_{s,r} (\\mathcal{C}_r^\\dagger) = \\Sigma_n^{-1/2} U_n^T \\mathcal{H}'_{s,r} V_n \\Sigma_n^{-1/2}$.\n\n**Step 4: Similarity Alignment**\nThe realization $(\\hat{A}, \\hat{B}, \\hat{C})$ is correct up to a similarity transformation. To compare it to the ground-truth $(A, B, C)$, we must find an invertible matrix $T \\in \\mathbb{R}^{n \\times n}$ that aligns them. The ideal transformation satisfies $A = T \\hat{A} T^{-1}$, $B = T \\hat{B}$, and $C = \\hat{C} T^{-1}$. Rearranging gives the set of linear equations for $T$:\n1. $A T - T \\hat{A} = 0$\n2. $T \\hat{B} - B = 0$\n3. $C T - \\hat{C} = 0$\nWe find the matrix $T$ that best satisfies these three equations simultaneously in a least-squares sense. To formulate this as a standard linear least-squares problem, we vectorize the matrix equations using the Kronecker product ($\\otimes$):\n1. $(I_n \\otimes A - \\hat{A}^T \\otimes I_n) \\text{vec}(T) = \\text{vec}(0)$\n2. $(\\hat{B}^T \\otimes I_n) \\text{vec}(T) = \\text{vec}(B)$\n3. $(I_n \\otimes C) \\text{vec}(T) = \\text{vec}(\\hat{C})$\nThese three systems are stacked into a single large system $\\mathbf{M} \\mathbf{t} \\approx \\mathbf{v}$, where $\\mathbf{t} = \\text{vec}(T)$. This overdetermined system is solved for $\\mathbf{t}$ using least-squares, and the solution is reshaped into the $n \\times n$ matrix $T$.\n\n**Step 5: Quantitative Error**\nWith the computed alignment matrix $T$, we quantify the alignment error using the normalized Frobenius norm for each of the three matrix equations:\n$$\n\\varepsilon_A = \\frac{\\lVert A T - T \\hat{A} \\rVert_F}{\\lVert A \\rVert_F + \\epsilon}, \\quad\n\\varepsilon_B = \\frac{\\lVert T \\hat{B} - B \\rVert_F}{\\lVert B \\rVert_F + \\epsilon}, \\quad\n\\varepsilon_C = \\frac{\\lVert C T - \\hat{C} \\rVert_F}{\\lVert C \\rVert_F + \\epsilon}\n$$\nwhere $\\epsilon = 10^{-12}$ is added for numerical stability. The final error for the test case is the maximum of these three values, $e = \\max\\{\\varepsilon_A, \\varepsilon_B, \\varepsilon_C\\}$. This entire procedure is repeated for all specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve_case(A, B, C, D, n, m, p, N, s, r, seed):\n    \"\"\"\n    Solves a single test case for LTI system identification and alignment.\n    \"\"\"\n    # Step 1: Data Generation\n    rng = np.random.default_rng(seed)\n    u = rng.standard_normal(size=(N, m))\n    x = np.zeros((N + 1, n))\n    y = np.zeros((N, p))\n\n    for k in range(N):\n        y[k, :] = (C @ x[k, :]) + (D @ u[k, :])\n        x[k + 1, :] = (A @ x[k, :]) + (B @ u[k, :])\n\n    # Step 2: Markov Parameter Estimation\n    L = s + r + 10\n    num_eqs = N - L\n    \n    # Form regressor matrix U_reg and target Y_target\n    # U_reg has rows [u_k^T, u_{k-1}^T, ..., u_{k-L}^T] for k = L..N-1\n    U_reg = np.zeros((num_eqs, (L + 1) * m))\n    for i in range(L + 1):\n        U_reg[:, i * m:(i + 1) * m] = u[L - i:N - i, :]\n    \n    Y_target = y[L:N, :]\n    \n    # Solve U_reg @ M_hat_T = Y_target for M_hat_T\n    M_hat_T, _, _, _ = np.linalg.lstsq(U_reg, Y_target, rcond=None)\n    \n    M_hat = M_hat_T.T  # Shape: (p, (L + 1) * m)\n    \n    H_hat = [M_hat[:, i * m:(i + 1) * m] for i in range(L + 1)]\n    D_hat = H_hat[0]\n\n    # Step 3: Realization Recovery (Ho-Kalman)\n    H_sys = H_hat[1:]  # Strictly proper part\n    \n    # Build block Hankel matrix\n    H_sr = np.zeros((s * p, r * m))\n    for i in range(s):\n        for j in range(r):\n            if i + j  len(H_sys):\n                H_sr[i * p:(i + 1) * p, j * m:(j + 1) * m] = H_sys[i + j]\n\n    # SVD and truncation\n    U, S_vec, Vt = np.linalg.svd(H_sr, full_matrices=False)\n    U_n = U[:, :n]\n    S_n_vec = S_vec[:n]\n    Vt_n = Vt[:n, :]\n    \n    # Balanced realization factors\n    sqrt_Sigma_n = np.diag(np.sqrt(S_n_vec))\n    Obs = U_n @ sqrt_Sigma_n\n    Ctrl = sqrt_Sigma_n @ Vt_n\n    \n    C_hat = Obs[:p, :]\n    B_hat = Ctrl[:, :m]\n\n    # Shifted Hankel matrix for A_hat\n    H_prime_sr = np.zeros((s * p, r * m))\n    for i in range(s):\n        for j in range(r):\n            if i + j + 1  len(H_sys):\n                H_prime_sr[i * p:(i + 1) * p, j * m:(j + 1) * m] = H_sys[i + j + 1]\n\n    inv_sqrt_Sigma_n = np.diag(1.0 / np.sqrt(S_n_vec))\n    V_n = Vt_n.T\n    A_hat = inv_sqrt_Sigma_n @ U_n.T @ H_prime_sr @ V_n @ inv_sqrt_Sigma_n\n\n    # Step 4: Similarity Alignment\n    # Form the stacked least-squares problem to solve for vec(T)\n    # A T - T A_hat = 0\n    M_A = np.kron(np.eye(n), A) - np.kron(A_hat.T, np.eye(n))\n    # T B_hat - B = 0  - (B_hat.T kron I) vec(T) = vec(B)\n    M_B = np.kron(B_hat.T, np.eye(n))\n    # C T - C_hat = 0 - (I kron C) vec(T) = vec(C_hat)\n    M_C = np.kron(np.eye(n), C)\n    \n    M_stack = np.vstack([M_A, M_B, M_C])\n    \n    v_A = np.zeros(n * n)\n    v_B = B.flatten('F')\n    v_C = C_hat.flatten('F')\n    v_stack = np.concatenate([v_A, v_B, v_C])\n    \n    t_vec, _, _, _ = np.linalg.lstsq(M_stack, v_stack, rcond=None)\n    T = t_vec.reshape((n, n), order='F')\n\n    # Step 5: Quantitative Error\n    eps = 1e-12\n    norm_A = np.linalg.norm(A, 'fro') + eps\n    norm_B = np.linalg.norm(B, 'fro') + eps\n    norm_C = np.linalg.norm(C, 'fro') + eps\n    \n    err_A = np.linalg.norm(A @ T - T @ A_hat, 'fro') / norm_A\n    err_B = np.linalg.norm(T @ B_hat - B, 'fro') / norm_B\n    err_C = np.linalg.norm(C @ T - C_hat, 'fro') / norm_C\n    \n    return max(err_A, err_B, err_C)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"A\": np.array([[0.7, 0.2], [-0.1, 0.9]]),\n            \"B\": np.array([[1.0], [0.5]]),\n            \"C\": np.array([[1.0, -0.3]]),\n            \"D\": np.array([[0.1]]),\n            \"dims\": {\"n\": 2, \"m\": 1, \"p\": 1},\n            \"params\": {\"N\": 800, \"s\": 5, \"r\": 5, \"seed\": 123},\n        },\n        # Case 2\n        {\n            \"A\": np.array([[0.6, 0.2, 0.0], [0.0, 0.7, 0.1], [0.0, -0.2, 0.8]]),\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0], [0.5, -0.2]]),\n            \"C\": np.array([[1.0, 0.0, 0.3], [0.2, 0.8, -0.1]]),\n            \"D\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"dims\": {\"n\": 3, \"m\": 2, \"p\": 2},\n            \"params\": {\"N\": 1500, \"s\": 6, \"r\": 6, \"seed\": 456},\n        },\n        # Case 3\n        {\n            \"A\": np.array([[0.95]]),\n            \"B\": np.array([[1.2]]),\n            \"C\": np.array([[0.8]]),\n            \"D\": np.array([[0.4]]),\n            \"dims\": {\"n\": 1, \"m\": 1, \"p\": 1},\n            \"params\": {\"N\": 400, \"s\": 4, \"r\": 4, \"seed\": 789},\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        error = solve_case(\n            case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"],\n            case[\"dims\"][\"n\"], case[\"dims\"][\"m\"], case[\"dims\"][\"p\"],\n            case[\"params\"][\"N\"], case[\"params\"][\"s\"], case[\"params\"][\"r\"], case[\"params\"][\"seed\"]\n        )\n        results.append(error)\n    \n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2886056"}]}