{"hands_on_practices": [{"introduction": "To truly grasp the Restricted Isometry Property (RIP), it is essential to move from its abstract definition to concrete computation. This first practice invites you to do just that by analyzing a matrix with a special structure known as an equiangular tight frame. By directly calculating the mutual coherence and the RIP constants, you will build a tangible understanding of how a matrix's geometry dictates its suitability for sparse recovery and explore the relationship between different metrics that quantify this property [@problem_id:2905693].", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 4}$ be the matrix with columns\n$$\n\\frac{1}{\\sqrt{3}}\n\\begin{bmatrix}\n1\\\\\n1\\\\\n1\n\\end{bmatrix},\\quad\n\\frac{1}{\\sqrt{3}}\n\\begin{bmatrix}\n1\\\\\n-1\\\\\n-1\n\\end{bmatrix},\\quad\n\\frac{1}{\\sqrt{3}}\n\\begin{bmatrix}\n-1\\\\\n1\\\\\n-1\n\\end{bmatrix},\\quad\n\\frac{1}{\\sqrt{3}}\n\\begin{bmatrix}\n-1\\\\\n-1\\\\\n1\n\\end{bmatrix}.\n$$\nThe columns of $A$ form an equiangular tight frame (ETF). Using the foundational definitions of mutual coherence and the Restricted Isometry Property (RIP), proceed as follows:\n\n1) From first principles, verify that all columns of $A$ have unit norm and have pairwise inner products of constant magnitude. Use this to compute the mutual coherence $\\mu(A)$, defined as $\\mu(A) \\triangleq \\max_{i \\neq j} | \\langle a_i, a_j \\rangle |$, where $a_i$ denotes the $i$-th column of $A$.\n\n2) For $k \\in \\{2,3\\}$, compute the exact restricted isometry constant $\\delta_k$ of $A$, where $\\delta_k$ is the smallest $\\delta \\in [0,1]$ such that for every index set $S \\subset \\{1,\\dots,4\\}$ with $|S| = k$, all eigenvalues of the Gram submatrix $G_S \\triangleq A_S^{\\top} A_S$ lie in the interval $[1-\\delta,\\,1+\\delta]$. Your computation must start from the definition of $G_S$ and proceed via standard spectral arguments.\n\n3) Starting from the definition of mutual coherence and using standard eigenvalue localization arguments, derive an upper bound on $\\delta_k$ in terms of $k$ and $\\mu(A)$, and evaluate this bound for $k=2$ and $k=3$. Briefly compare the bound to the exact $\\delta_k$ you found.\n\nReport your final answer as the row vector $\\big[\\mu(A),\\, \\delta_2,\\, \\delta_3\\big]$. No rounding is required. No units are involved. Angles, if any, should be expressed in radians, although none are needed here.", "solution": "The provided problem is subjected to validation and is found to be valid. It is scientifically grounded in the established theory of compressive sensing and linear algebra, well-posed with all necessary information, and stated in objective, formal language. There are no contradictions, ambiguities, or unsound premises. We may therefore proceed with the solution.\n\nThe problem is addressed in three parts as requested. The matrix $A \\in \\mathbb{R}^{3 \\times 4}$ has columns denoted by $a_1, a_2, a_3, a_4$.\n\nPart 1: Mutual Coherence $\\mu(A)$\n\nFirst, we verify that the columns of $A$ are of unit $\\ell_2$-norm. We compute the squared norm for the first column, $a_1$:\n$$\n\\|a_1\\|_2^2 = \\left\\langle \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right\\rangle = \\frac{1}{3} (1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1) = \\frac{3}{3} = 1.\n$$\nThe calculation is identical for the other columns $a_2, a_3, a_4$, as they are composed of elements with magnitude $1$. For example, for $a_2$:\n$$\n\\|a_2\\|_2^2 = \\frac{1}{3} (1^2 + (-1)^2 + (-1)^2) = \\frac{3}{3} = 1.\n$$\nThus, all columns have unit norm.\n\nNext, we compute the pairwise inner products $\\langle a_i, a_j \\rangle$ for $i \\neq j$.\n$$\n\\langle a_1, a_2 \\rangle = \\frac{1}{3} (1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot (-1)) = \\frac{1}{3} (1 - 1 - 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_1, a_3 \\rangle = \\frac{1}{3} (1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1)) = \\frac{1}{3} (-1 + 1 - 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_2, a_3 \\rangle = \\frac{1}{3} (1 \\cdot (-1) + (-1) \\cdot 1 + (-1) \\cdot (-1)) = \\frac{1}{3} (-1 - 1 + 1) = -\\frac{1}{3}.\n$$\nBy symmetry, all other pairwise inner products also evaluate to $-\\frac{1}{3}$. The magnitude of these inner products is constant:\n$$\n|\\langle a_i, a_j \\rangle| = \\left|-\\frac{1}{3}\\right| = \\frac{1}{3} \\quad \\text{for all } i \\neq j.\n$$\nThe mutual coherence $\\mu(A)$ is defined as the maximum of these magnitudes. Since they are all equal, the maximum is this constant value.\n$$\n\\mu(A) \\triangleq \\max_{i \\neq j} |\\langle a_i, a_j \\rangle| = \\frac{1}{3}.\n$$\n\nPart 2: Restricted Isometry Constants $\\delta_2$ and $\\delta_3$\n\nThe restricted isometry constant $\\delta_k$ is the smallest non-negative $\\delta$ such that for any index set $S \\subset \\{1, 2, 3, 4\\}$ with $|S|=k$, all eigenvalues of the Gram submatrix $G_S = A_S^\\top A_S$ lie in the interval $[1-\\delta, 1+\\delta]$. The matrix $G_S$ is a $k \\times k$ matrix whose entries are $(G_S)_{ij} = \\langle a_{s_i}, a_{s_j} \\rangle$ where $S=\\{s_1, \\dots, s_k\\}$.\n\nFor $k=2$:\nLet $S$ be any set of two indices, for instance $S=\\{1,2\\}$. The Gram submatrix $G_S$ is:\n$$\nG_S = \\begin{pmatrix} \\langle a_1, a_1 \\rangle & \\langle a_1, a_2 \\rangle \\\\ \\langle a_2, a_1 \\rangle & \\langle a_2, a_2 \\rangle \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{3} \\\\ -\\frac{1}{3} & 1 \\end{pmatrix}.\n$$\nDue to the equiangular property, any $2 \\times 2$ Gram submatrix will have this form. The eigenvalues $\\lambda$ are solutions to the characteristic equation $\\det(G_S - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} 1-\\lambda & -\\frac{1}{3} \\\\ -\\frac{1}{3} & 1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - \\left(-\\frac{1}{3}\\right)^2 = 0.\n$$\nThis gives $(1-\\lambda)^2 = \\frac{1}{9}$, so $1-\\lambda = \\pm \\frac{1}{3}$. The eigenvalues are $\\lambda_1 = 1 - \\frac{1}{3} = \\frac{2}{3}$ and $\\lambda_2 = 1 + \\frac{1}{3} = \\frac{4}{3}$.\nThe eigenvalues must lie in $[1-\\delta_2, 1+\\delta_2]$. This requires $1-\\delta_2 \\le \\frac{2}{3}$ and $\\frac{4}{3} \\le 1+\\delta_2$. Both inequalities imply $\\delta_2 \\ge \\frac{1}{3}$. The smallest such value is $\\delta_2 = \\frac{1}{3}$.\n\nFor $k=3$:\nLet $S$ be any set of three indices, for instance $S=\\{1,2,3\\}$. The Gram submatrix is:\n$$\nG_S = \\begin{pmatrix} 1 & -\\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & 1 & -\\frac{1}{3} \\\\ -\\frac{1}{3} & -\\frac{1}{3} & 1 \\end{pmatrix}.\n$$\nThis matrix can be written as $G_S = (1-c)I + cJ$, where $c=-\\frac{1}{3}$, $I$ is the $3 \\times 3$ identity matrix, and $J$ is the $3 \\times 3$ matrix of all ones. So, $G_S = \\frac{4}{3}I - \\frac{1}{3}J$.\nThe eigenvalues of a matrix of the form $aI+bJ$ of size $k \\times k$ are $a$ with multiplicity $k-1$ and $a+kb$ with multiplicity $1$. Here, $k=3$, $a=\\frac{4}{3}$, and $b=-\\frac{1}{3}$.\nThe eigenvalues are:\n$\\lambda_{1,2} = a = \\frac{4}{3}$ (multiplicity $2$).\n$\\lambda_3 = a + 3b = \\frac{4}{3} + 3\\left(-\\frac{1}{3}\\right) = \\frac{4}{3} - 1 = \\frac{1}{3}$ (multiplicity $1$).\nThe set of eigenvalues is $\\{\\frac{1}{3}, \\frac{4}{3}\\}$.\nThese eigenvalues must lie in $[1-\\delta_3, 1+\\delta_3]$. This requires $1-\\delta_3 \\le \\frac{1}{3}$ and $\\frac{4}{3} \\le 1+\\delta_3$.\nThe first inequality gives $\\delta_3 \\ge 1 - \\frac{1}{3} = \\frac{2}{3}$.\nThe second inequality gives $\\delta_3 \\ge \\frac{4}{3} - 1 = \\frac{1}{3}$.\nTo satisfy both, we must have $\\delta_3 \\ge \\max(\\frac{2}{3}, \\frac{1}{3}) = \\frac{2}{3}$. The smallest such value is $\\delta_3 = \\frac{2}{3}$.\n\nPart 3: Bound on $\\delta_k$ and Comparison\n\nWe derive an upper bound on $\\delta_k$ using the Gershgorin Circle Theorem. Let $G_S$ be a $k \\times k$ Gram submatrix for an arbitrary index set $S$ with $|S|=k$. The diagonal entries are $(G_S)_{ii} = \\|a_{s_i}\\|_2^2 = 1$. The off-diagonal entries satisfy $|(G_S)_{ij}| = |\\langle a_{s_i}, a_{s_j} \\rangle| \\le \\mu(A)$ for $i \\neq j$.\n\nThe Gershgorin Circle Theorem states that every eigenvalue $\\lambda$ of $G_S$ lies in at least one of the disks $D_i = \\{z \\in \\mathbb{C} : |z - (G_S)_{ii}| \\le R_i\\}$, where $R_i = \\sum_{j \\neq i} |(G_S)_{ij}|$.\nFor $G_S$, the center of each disk is $(G_S)_{ii}=1$. The radius is $R_i = \\sum_{j=1, j\\neq i}^{k} |\\langle a_{s_i}, a_{s_j} \\rangle| \\le \\sum_{j=1, j\\neq i}^{k} \\mu(A) = (k-1)\\mu(A)$.\nSince $G_S$ is real and symmetric, its eigenvalues are real. Thus, for any eigenvalue $\\lambda$:\n$$\n|\\lambda - 1| \\le (k-1)\\mu(A).\n$$\nThis implies that all eigenvalues lie in the interval $[1 - (k-1)\\mu(A), 1 + (k-1)\\mu(A)]$.\nBy the definition of $\\delta_k$, we must have $[1 - \\delta_k, 1 + \\delta_k]$ contain this interval, which gives the bound:\n$$\n\\delta_k \\le (k-1)\\mu(A).\n$$\n\nNow we evaluate this bound and compare. We have $\\mu(A) = \\frac{1}{3}$.\nFor $k=2$: The bound is $\\delta_2 \\le (2-1)\\mu(A) = \\mu(A) = \\frac{1}{3}$. Our exact calculation yielded $\\delta_2 = \\frac{1}{3}$. The bound is tight.\nFor $k=3$: The bound is $\\delta_3 \\le (3-1)\\mu(A) = 2\\mu(A) = \\frac{2}{3}$. Our exact calculation yielded $\\delta_3 = \\frac{2}{3}$. The bound is also tight.\n\nThe tightness of the Gershgorin bound in this specific case is a direct consequence of the matrix having equiangular columns. For such matrices, the inequality $|\\langle a_i, a_j \\rangle| \\le \\mu(A)$ becomes an equality $|\\langle a_i, a_j \\rangle| = \\mu(A)$ for all $i \\neq j$. This makes the radius of each Gershgorin disk exactly $(k-1)\\mu(A)$, rendering the bound sharp.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "2905693"}, {"introduction": "The RIP provides a powerful *sufficient* condition for the *uniform* recovery of all sparse signals, but is it a *necessary* condition for recovering a *specific* sparse signal? This exercise delves into this critical distinction through a carefully constructed example. You will demonstrate a scenario where a matrix fails the RIP condition, yet exact recovery is still achieved for a particular sparse vector, illustrating that the absence of a uniform guarantee does not preclude instance-specific success [@problem_id:2905643].", "problem": "Consider the following concrete construction designed to contrast instance-specific exact recovery against uniform guarantees based on the Restricted Isometry Property (RIP).\n\nLet $A \\in \\mathbb{R}^{3 \\times 4}$ have columns $a_{1},a_{2},a_{3},a_{4}$ given by $a_{1} = (1,0,0)^{\\top}$, $a_{2} = (0,1,0)^{\\top}$, $a_{3} = (1,0,0)^{\\top}$, and $a_{4} = (0,0,1)^{\\top}$. Thus $A = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$. Fix sparsity order $k=2$ and the $k$-sparse vector $x_{0} \\in \\mathbb{R}^{4}$ defined by $x_{0} = (0,1,0,1)^{\\top}$. Let $y = A x_{0}$.\n\nYou will analyze two properties:\n\n1) Instance-specific exact recovery via the Basis Pursuit (BP) program. Basis Pursuit (BP) seeks $x^{\\star}$ that solves the convex optimization problem $\\min \\|x\\|_{1}$ subject to $A x = y$. Compute the unique solution $x^{\\star}$ to this BP problem for the given $A$ and $y$.\n\n2) Uniform behavior via the Restricted Isometry Property (RIP). The Restricted Isometry Property (RIP) of order $k$ with constant $\\delta_{k} \\in [0,1)$ requires that for every $k$-sparse vector $v \\in \\mathbb{R}^{4}$,\n$$(1-\\delta_{k}) \\, \\|v\\|_{2}^{2} \\le \\|A v\\|_{2}^{2} \\le (1+\\delta_{k}) \\, \\|v\\|_{2}^{2}.$$\nDetermine the infimum value $\\delta_{2}^{\\star} \\ge 0$ such that the above inequalities hold for all $2$-sparse vectors $v$. Your answer should reflect whether $A$ can satisfy RIP of order $2$ with any $\\delta_{2} \\in [0,1)$.\n\nAnswer format: Provide the final answer as a single row matrix containing the four coordinates of $x^{\\star}$ followed by the scalar $\\delta_{2}^{\\star}$, in the order $\\big(x_{1}^{\\star}, x_{2}^{\\star}, x_{3}^{\\star}, x_{4}^{\\star}, \\delta_{2}^{\\star}\\big)$. No rounding is required.", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in the field of compressive sensing, free of scientific or logical flaws, and all necessary information is provided.\n\nThe problem consists of two parts. The first part requires solving a specific Basis Pursuit (BP) instance. The second part requires determining the Restricted Isometry Property (RIP) constant of order $2$ for the given matrix.\n\nPart 1: Basis Pursuit Solution\n\nFirst, we are given the matrix $A \\in \\mathbb{R}^{3 \\times 4}$ as\n$$A = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$$\nand the $2$-sparse vector $x_{0} \\in \\mathbb{R}^{4}$ as\n$$x_{0} = (0, 1, 0, 1)^{\\top}$$\nThe measurement vector $y$ is computed as $y = A x_{0}$.\n$$y = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot 1 \\\\ 0 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe Basis Pursuit problem is to find a vector $x^{\\star}$ that solves the convex optimization problem:\n$$\\min_{x \\in \\mathbb{R}^{4}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y$$\nThe constraint $A x = y$ expands to a system of linear equations:\n$$\n\\begin{cases}\n1 \\cdot x_{1} + 0 \\cdot x_{2} + 1 \\cdot x_{3} + 0 \\cdot x_{4} = 0 \\\\\n0 \\cdot x_{1} + 1 \\cdot x_{2} + 0 \\cdot x_{3} + 0 \\cdot x_{4} = 1 \\\\\n0 \\cdot x_{1} + 0 \\cdot x_{2} + 0 \\cdot x_{3} + 1 \\cdot x_{4} = 1\n\\end{cases}\n$$\nThis simplifies to:\n$$\n\\begin{cases}\nx_{1} + x_{3} = 0 \\\\\nx_{2} = 1 \\\\\nx_{4} = 1\n\\end{cases}\n$$\nFrom the first equation, we have $x_{3} = -x_{1}$. Any feasible solution $x$ must be of the form $x = (c, 1, -c, 1)^{\\top}$ for some scalar $c \\in \\mathbb{R}$.\n\nNow, we must minimize the $\\ell_{1}$-norm of this family of solutions:\n$$\\|x\\|_{1} = |x_{1}| + |x_{2}| + |x_{3}| + |x_{4}| = |c| + |1| + |-c| + |1| = 2|c| + 2$$\nThe objective function $f(c) = 2|c| + 2$ is minimized when $|c|$ is minimized. The minimum value of $|c|$ is $0$, which is achieved at $c=0$. This minimum is unique.\nSubstituting $c=0$ into the general form of the solution gives the unique BP solution $x^{\\star}$:\n$$x^{\\star} = (0, 1, 0, 1)^{\\top}$$\nWe observe that $x^{\\star} = x_{0}$, meaning exact recovery is achieved for this specific instance.\n\nPart 2: Restricted Isometry Property Constant\n\nThe RIP of order $k$ with constant $\\delta_{k}$ requires that for every $k$-sparse vector $v$, the following holds:\n$$(1-\\delta_{k}) \\|v\\|_{2}^{2} \\le \\|A v\\|_{2}^{2} \\le (1+\\delta_{k}) \\|v\\|_{2}^{2}$$\nThis is equivalent to the condition that for any submatrix $A_{S}$ of $A$ formed by a set $S$ of $k$ columns, all eigenvalues $\\lambda$ of the Gramian matrix $A_{S}^{\\top}A_{S}$ must satisfy $1-\\delta_{k} \\le \\lambda \\le 1+\\delta_{k}$.\nThis implies that $\\delta_{k}$ must satisfy:\n$$\\delta_{k} \\ge 1 - \\min_{S, |S|=k} \\lambda_{\\min}(A_{S}^{\\top}A_{S})$$\n$$\\delta_{k} \\ge \\max_{S, |S|=k} \\lambda_{\\max}(A_{S}^{\\top}A_{S}) - 1$$\nThe smallest such constant, $\\delta_{k}^{\\star}$, is therefore given by:\n$$\\delta_{k}^{\\star} = \\max \\left( 1 - \\min_{S, |S|=k} \\lambda_{\\min}(A_{S}^{\\top}A_{S}), \\max_{S, |S|=k} \\lambda_{\\max}(A_{S}^{\\top}A_{S}) - 1 \\right)$$\nFor this problem, $k=2$. The number of columns is $n=4$. We must examine all $\\binom{4}{2} = 6$ submatrices of size $3 \\times 2$. The columns of $A$ are $a_{1}=(1,0,0)^{\\top}, a_{2}=(0,1,0)^{\\top}, a_{3}=(1,0,0)^{\\top}, a_{4}=(0,0,1)^{\\top}$. Note that $a_{1}=a_{3}$.\n\n1.  $S=\\{1,2\\}$: $A_{\\{1,2\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}$. $A_{\\{1,2\\}}^{\\top}A_{\\{1,2\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. Eigenvalues are $\\{1, 1\\}$.\n2.  $S=\\{1,3\\}$: $A_{\\{1,3\\}} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$. $A_{\\{1,3\\}}^{\\top}A_{\\{1,3\\}} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$. The characteristic equation is $(1-\\lambda)^{2}-1=0$, which yields eigenvalues $\\lambda=0, 2$.\n3.  $S=\\{1,4\\}$: $A_{\\{1,4\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix}$. $A_{\\{1,4\\}}^{\\top}A_{\\{1,4\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. Eigenvalues are $\\{1, 1\\}$.\n4.  $S=\\{2,3\\}$: $A_{\\{2,3\\}} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$. $A_{\\{2,3\\}}^{\\top}A_{\\{2,3\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. Eigenvalues are $\\{1, 1\\}$.\n5.  $S=\\{2,4\\}$: $A_{\\{2,4\\}} = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. $A_{\\{2,4\\}}^{\\top}A_{\\{2,4\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$. Eigenvalues are $\\{1, 1\\}$.\n6.  $S=\\{3,4\\}$: $A_{\\{3,4\\}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix}$. This is identical to $A_{\\{1,4\\}}$. Eigenvalues are $\\{1, 1\\}$.\n\nFrom this enumeration, the minimum eigenvalue found across all $2 \\times 2$ Gramians is $0$ (from $S=\\{1,3\\}$), and the maximum eigenvalue is $2$ (also from $S=\\{1,3\\}$).\n$$\\min_{S, |S|=2} \\lambda_{\\min}(A_{S}^{\\top}A_{S}) = 0$$\n$$\\max_{S, |S|=2} \\lambda_{\\max}(A_{S}^{\\top}A_{S}) = 2$$\nWe can now compute the infimum value $\\delta_{2}^{\\star}$:\n$$\\delta_{2}^{\\star} = \\max(1-0, 2-1) = \\max(1, 1) = 1$$\nThe RIP condition is conventionally defined for $\\delta_{k} \\in [0,1)$. Since $\\delta_{2}^{\\star}=1$, the matrix $A$ does not satisfy the RIP of order $2$ for any $\\delta_{2} < 1$. The infimum value required by the definition is indeed $1$. This is directly confirmed by considering the $2$-sparse vector $v=(1,0,-1,0)^{\\top}$. For this $v$, we have $\\|v\\|_{2}^{2} = 2$, but $Av = a_{1}-a_{3} = 0$, so $\\|Av\\|_{2}^{2}=0$. The RIP inequality $(1-\\delta_{2})\\|v\\|_{2}^{2} \\le \\|Av\\|_{2}^{2}$ becomes $(1-\\delta_{2}) \\cdot 2 \\le 0$, which implies $1-\\delta_{2} \\le 0$, or $\\delta_{2} \\ge 1$.\n\nThe final answer requires the four coordinates of $x^{\\star}$ and the value $\\delta_{2}^{\\star}$.\n$$x^{\\star} = (0, 1, 0, 1)^{\\top}$$\n$$\\delta_{2}^{\\star} = 1$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1 & 0 & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "2905643"}, {"introduction": "The standard RIP is tailored for the \"synthesis\" model, where a signal is a sparse combination of dictionary atoms. This final practice challenges you to look beyond this framework to the more general \"analysis\" model, where a signal is sparse under a transformation $\\Omega$. You will construct a counterexample where a measurement matrix $A$ satisfies the standard RIP, yet fails to guarantee recovery in the analysis setting, revealing the geometric reasons—the intersection of the measurement nullspace $\\operatorname{ker}(A)$ with the analysis objective's descent cone—why a stronger recovery condition is required [@problem_id:2905725].", "problem": "Consider the following setting in sparse modeling and compressive sensing. Let the measurement operator be the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 & \\frac{1}{\\sqrt{2}}\\\\\n0 & 1 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\nLet the analysis operator be $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ defined as\n$$\n\\Omega \\;=\\; \\begin{pmatrix}\n1 & 0 & 1\\\\\n0 & 1 & -1\n\\end{pmatrix}.\n$$\nChoose $x_{0} \\in \\mathbb{R}^{3}$ such that $\\Omega x_{0} = \\begin{pmatrix}-1\\\\ 1\\end{pmatrix}$, for example $x_{0} = \\begin{pmatrix}-1\\\\ 1\\\\ 0\\end{pmatrix}$. Let $g(x) = \\|\\Omega x\\|_{1}$ denote the analysis $\\ell_{1}$ objective. Define the tangent (descent) cone of $g$ at $x_{0}$ as the set of all directions $h \\in \\mathbb{R}^{3}$ for which the first-order directional derivative is nonpositive:\n$$\n\\mathcal{T}(x_{0}) \\;=\\; \\left\\{\\, h \\in \\mathbb{R}^{3} \\,:\\, \\langle \\operatorname{sign}((\\Omega x_{0})_{S}),\\, (\\Omega h)_{S} \\rangle + \\|(\\Omega h)_{S^{c}}\\|_{1} \\,\\le\\, 0 \\,\\right\\},\n$$\nwhere $S = \\operatorname{supp}(\\Omega x_{0})$, the sign is taken elementwise, and $\\langle \\cdot,\\cdot \\rangle$ is the Euclidean inner product.\n\nStarting only from the definitions of the Restricted Isometry Property (RIP) for $s$-sparse vectors, the analysis $\\ell_{1}$ objective, and the descent cone, perform the following:\n\n- Determine whether $A$ satisfies the standard $s$-sparse Restricted Isometry Property for $s = 1$, and identify the associated restricted isometry constant $\\delta_{1}$.\n- Identify a nonzero direction $h \\in \\mathcal{T}(x_{0})$ such that $A h = 0$, thereby constructing a counterexample where the standard RIP holds but an analysis-type isometry over the descent cone fails. Explain the geometric reason in terms of the intersection of the measurement nullspace with the tangent cone.\n- Compute the Rayleigh quotient\n$$\nr \\;=\\; \\frac{\\|A h\\|_{2}^{2}}{\\|h\\|_{2}^{2}}\n$$\nfor your identified nonzero $h \\in \\mathcal{T}(x_{0})$.\n\nGive your final answer as the single real number $r$. No rounding is required.", "solution": "The posed problem is subjected to validation before a solution is attempted.\n\n**Step 1: Extract Givens**\n- Measurement operator: $A \\in \\mathbb{R}^{2 \\times 3}$, $A = \\begin{pmatrix} 1 & 0 & \\frac{1}{\\sqrt{2}}\\\\ 0 & 1 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n- Analysis operator: $\\Omega \\in \\mathbb{R}^{2 \\times 3}$, $\\Omega = \\begin{pmatrix} 1 & 0 & 1\\\\ 0 & 1 & -1 \\end{pmatrix}$.\n- A specific vector: $x_{0} \\in \\mathbb{R}^{3}$ such that $\\Omega x_{0} = \\begin{pmatrix}-1\\\\ 1\\end{pmatrix}$. An example is given: $x_{0} = \\begin{pmatrix}-1\\\\ 1\\\\ 0\\end{pmatrix}$.\n- Analysis objective function: $g(x) = \\|\\Omega x\\|_{1}$.\n- Tangent (descent) cone at $x_0$: $\\mathcal{T}(x_{0}) = \\left\\{\\, h \\in \\mathbb{R}^{3} \\,:\\, \\langle \\operatorname{sign}((\\Omega x_{0})_{S}),\\, (\\Omega h)_{S} \\rangle + \\|(\\Omega h)_{S^{c}}\\|_{1} \\,\\le\\, 0 \\,\\right\\}$, where $S = \\operatorname{supp}(\\Omega x_{0})$.\n- Rayleigh quotient definition: $r = \\frac{\\|A h\\|_{2}^{2}}{\\|h\\|_{2}^{2}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within the field of signal processing, specifically compressive sensing and sparse modeling. All terms like Restricted Isometry Property (RIP), analysis $\\ell_{1}$ objective, and descent cone are standard and well-defined. The problem is well-posed, providing all necessary matrices, vectors, and definitions to perform the required tasks. The language is objective and formal. The problem is a concrete computational exercise demonstrating a key concept: the insufficiency of the standard RIP for analysis-model recovery. There are no contradictions, ambiguities, or factual unsoundness.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe solution proceeds by addressing the three tasks in order.\n\nFirst, we determine if the matrix $A$ satisfies the standard $s$-sparse Restricted Isometry Property for $s=1$. A matrix $A$ satisfies the $s$-RIP with constant $\\delta_s$ if for every $s$-sparse vector $x$ (i.e., a vector with at most $s$ non-zero entries, $\\|x\\|_0 \\le s$), the following inequality holds:\n$$ (1 - \\delta_s) \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_s) \\|x\\|_{2}^{2} $$\nFor $s=1$, an $1$-sparse vector $x \\in \\mathbb{R}^3$ has the form $x = c e_j$ for some scalar $c \\in \\mathbb{R}$ and some standard basis vector $e_j \\in \\{e_1, e_2, e_3\\}$. For such a vector, $\\|x\\|_2^2 = \\|c e_j\\|_2^2 = c^2 \\|e_j\\|_2^2 = c^2$. The term $\\|Ax\\|_2^2$ becomes $\\|A(c e_j)\\|_2^2 = \\|c A_j\\|_2^2 = c^2 \\|A_j\\|_2^2$, where $A_j$ is the $j$-th column of $A$. The RIP inequality is equivalent to finding the minimum and maximum of $\\|A_j\\|_2^2$ over all columns $j$.\nLet us compute the squared $\\ell_2$-norm of each column of $A$:\n- For $j=1$: $\\|A_1\\|_2^2 = \\left\\|\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\right\\|_2^2 = 1^2 + 0^2 = 1$.\n- For $j=2$: $\\|A_2\\|_2^2 = \\left\\|\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right\\|_2^2 = 0^2 + 1^2 = 1$.\n- For $j=3$: $\\|A_3\\|_2^2 = \\left\\|\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\right\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{\\sqrt{2}}\\right)^2 = \\frac{1}{2} + \\frac{1}{2} = 1$.\nSince $\\|A_j\\|_2^2 = 1$ for all columns $j=1, 2, 3$, it follows that for any $1$-sparse vector $x$, $\\|Ax\\|_2^2 = \\|x\\|_2^2$. The RIP inequality becomes $(1 - \\delta_1) \\|x\\|_2^2 \\le \\|x\\|_2^2 \\le (1 + \\delta_1) \\|x\\|_2^2$. The smallest non-negative constant $\\delta_1$ satisfying $1-\\delta_1 \\le 1$ and $1 \\le 1+\\delta_1$ is $\\delta_1 = 0$. Thus, $A$ satisfies the RIP for $s=1$ with restricted isometry constant $\\delta_1=0$.\n\nSecond, we must identify a nonzero direction $h \\in \\mathcal{T}(x_0)$ such that $Ah=0$. This involves characterizing the descent cone $\\mathcal{T}(x_0)$ and the nullspace of $A$.\nWe are given $x_0 = \\begin{pmatrix}-1\\\\ 1\\\\ 0\\end{pmatrix}$ and $\\Omega = \\begin{pmatrix} 1 & 0 & 1\\\\ 0 & 1 & -1 \\end{pmatrix}$.\nFirst, compute $\\Omega x_0$:\n$$ \\Omega x_0 = \\begin{pmatrix} 1 & 0 & 1\\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix}-1\\\\ 1\\\\ 0\\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (0)(1) + (1)(0) \\\\ (0)(-1) + (1)(1) + (-1)(0) \\end{pmatrix} = \\begin{pmatrix}-1\\\\ 1\\end{pmatrix} $$\nThe support set is $S = \\operatorname{supp}(\\Omega x_0) = \\{1, 2\\}$, since both components are non-zero. The complement set $S^c$ is empty, so $\\|(\\Omega h)_{S^c}\\|_1 = 0$. The sign vector is $\\operatorname{sign}(\\Omega x_0) = \\begin{pmatrix}-1\\\\ 1\\end{pmatrix}$.\nThe condition for a vector $h = \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix}$ to be in the descent cone $\\mathcal{T}(x_0)$ simplifies to:\n$$ \\langle \\operatorname{sign}(\\Omega x_0), \\Omega h \\rangle \\le 0 $$\nLet's compute $\\Omega h$:\n$$ \\Omega h = \\begin{pmatrix} 1 & 0 & 1\\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} h_1 + h_3 \\\\ h_2 - h_3 \\end{pmatrix} $$\nThe inner product condition is:\n$$ \\left\\langle \\begin{pmatrix}-1\\\\ 1\\end{pmatrix}, \\begin{pmatrix} h_1 + h_3 \\\\ h_2 - h_3 \\end{pmatrix} \\right\\rangle = (-1)(h_1 + h_3) + (1)(h_2 - h_3) = -h_1 - h_3 + h_2 - h_3 = -h_1 + h_2 - 2h_3 \\le 0 $$\nSo, $\\mathcal{T}(x_0) = \\{h \\in \\mathbb{R}^3 \\mid -h_1 + h_2 - 2h_3 \\le 0 \\}$.\n\nNext, we characterize the nullspace of $A$, $\\operatorname{ker}(A) = \\{h \\in \\mathbb{R}^3 \\mid Ah=0\\}$.\n$$ A h = \\begin{pmatrix} 1 & 0 & \\frac{1}{\\sqrt{2}}\\\\ 0 & 1 & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis yields the linear system:\n$h_1 + \\frac{1}{\\sqrt{2}} h_3 = 0 \\implies h_1 = -\\frac{1}{\\sqrt{2}} h_3$\n$h_2 + \\frac{1}{\\sqrt{2}} h_3 = 0 \\implies h_2 = -\\frac{1}{\\sqrt{2}} h_3$\nAny vector in $\\operatorname{ker}(A)$ must be of the form $h = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} t \\\\ -\\frac{1}{\\sqrt{2}} t \\\\ t \\end{pmatrix}$ for some scalar $t \\in \\mathbb{R}$.\n\nWe seek a nonzero $h \\in \\mathcal{T}(x_0) \\cap \\operatorname{ker}(A)$. We take a generic vector $h$ from $\\operatorname{ker}(A)$ and enforce the condition for it to be in $\\mathcal{T}(x_0)$:\n$$ -h_1 + h_2 - 2h_3 \\le 0 $$\n$$ -(-\\frac{1}{\\sqrt{2}} t) + (-\\frac{1}{\\sqrt{2}} t) - 2(t) \\le 0 $$\n$$ \\frac{t}{\\sqrt{2}} - \\frac{t}{\\sqrt{2}} - 2t \\le 0 $$\n$$ -2t \\le 0 \\implies t \\ge 0 $$\nTo find a nonzero vector, we can choose any $t > 0$. Let us choose $t=1$.\nThen, a valid direction is $h = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 1 \\end{pmatrix}$. This vector is nonzero, belongs to $\\operatorname{ker}(A)$ by construction, and satisfies the descent cone condition.\n\nThe geometric reason for this counterexample is that the nullspace of the measurement operator $A$ has a non-trivial intersection with the descent cone $\\mathcal{T}(x_0)$. The standard RIP guarantees that $\\operatorname{ker}(A)$ contains no sparse vectors (here, for $s=1$). However, for recovery in the analysis model, one requires a stronger condition, often called the analysis-RIP or D-RIP, which ensures that $\\|Ah\\|_2$ is bounded below for all $h$ in the descent cone. The vector $h$ we found is a descent direction for the analysis objective function $g(x) = \\|\\Omega x\\|_1$ at $x_0$, but it is completely annihilated by the measurement matrix ($Ah=0$). Such a direction is \"invisible\" to the measurements, which compromises the ability to uniquely recover the signal $x$ through minimization of $g(x)$.\n\nThird, we compute the Rayleigh quotient $r = \\frac{\\|A h\\|_{2}^{2}}{\\|h\\|_{2}^{2}}$ for the identified nonzero vector $h$.\nWe chose $h = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 1 \\end{pmatrix}$, which resides in the nullspace of $A$. By definition, for any nonzero $h \\in \\operatorname{ker}(A)$, we have $A h = 0$.\nTherefore, the numerator is $\\|A h\\|_{2}^{2} = \\|0\\|_{2}^{2} = 0$.\nThe denominator is $\\|h\\|_{2}^{2} = \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 + 1^2 = \\frac{1}{2} + \\frac{1}{2} + 1 = 2$.\nSince $h$ is nonzero, its norm is non-zero.\nThe Rayleigh quotient is:\n$$ r = \\frac{0}{2} = 0 $$\nThis result is independent of the specific choice of $t > 0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2905725"}]}