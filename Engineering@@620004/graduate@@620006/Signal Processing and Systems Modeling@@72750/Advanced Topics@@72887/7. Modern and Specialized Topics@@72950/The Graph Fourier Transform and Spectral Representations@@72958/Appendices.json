{"hands_on_practices": [{"introduction": "This first exercise provides a fundamental workout, guiding you through the essential mechanics of the Graph Fourier Transform (GFT). By starting with a simple 4-node graph, you will compute the graph Laplacian, find its complete eigen-decomposition, and use this basis to transform a signal into the spectral domain. This hands-on calculation is invaluable for demystifying the abstract definitions and building a concrete understanding of how a graph's structure gives rise to a set of spectral frequencies [@problem_id:2912992].", "problem": "Consider a weighted, undirected graph on $4$ nodes with adjacency matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 & 1\\\\\n0 & 0 & 1 & 1\\\\\n1 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0\n\\end{pmatrix}.\n$$\nLet $D$ be the degree matrix and let $L$ be the combinatorial Laplacian defined by $L \\equiv D - A$. Using only standard definitions from spectral graph theory and linear algebra for real symmetric matrices, do the following:\n- Derive $L$ from $A$.\n- Compute the eigen-decomposition of $L$ as $L = U \\Lambda U^{\\top}$, where $U$ is an orthonormal matrix whose columns are eigenvectors of $L$ and $\\Lambda$ is diagonal with the eigenvalues of $L$ in nondecreasing order.\n- For the graph signal $x \\in \\mathbb{R}^{4}$ given by\n$$\nx \\;=\\; \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\ncompute its Graph Fourier Transform (GFT), defined as $\\widehat{x} \\equiv U^{\\top} x$.\n\nAs your final answer, provide the single real number equal to the total spectral energy contained in the eigenspace of $L$ associated with eigenvalue $2$, that is, the sum of the squared magnitudes of the GFT coefficients corresponding to eigenvalue $2$. No rounding is required. Report only this number as your final answer.", "solution": "The problem statement is subject to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n-   A weighted, undirected graph on $4$ nodes.\n-   Adjacency matrix: $A = \\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\end{pmatrix}$.\n-   Degree matrix: $D$.\n-   Combinatorial Laplacian: $L \\equiv D - A$.\n-   Eigen-decomposition: $L = U \\Lambda U^{\\top}$, with $U$ being an orthonormal matrix of eigenvectors and $\\Lambda$ a diagonal matrix of eigenvalues in nondecreasing order.\n-   Graph signal: $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n-   Graph Fourier Transform (GFT): $\\widehat{x} \\equiv U^{\\top} x$.\n-   Objective: Compute the total spectral energy in the eigenspace of $L$ associated with eigenvalue $2$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard definitions from spectral graph theory. It is well-posed, providing all necessary information for a unique solution to the specified quantity. The adjacency matrix is symmetric, consistent with an undirected graph. All terms are defined with mathematical precision. The problem is a standard exercise in applying linear algebra to graph theory.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be derived.\n\nThe solution proceeds systematically.\n\nFirst, the degree matrix $D$ is derived from the adjacency matrix $A$. The degree of a node is the sum of its row (or column) entries in $A$.\nThe degrees of the nodes are:\n$d_1 = A_{11} + A_{12} + A_{13} + A_{14} = 0 + 0 + 1 + 1 = 2$.\n$d_2 = A_{21} + A_{22} + A_{23} + A_{24} = 0 + 0 + 1 + 1 = 2$.\n$d_3 = A_{31} + A_{32} + A_{33} + A_{34} = 1 + 1 + 0 + 0 = 2$.\n$d_4 = A_{41} + A_{42} + A_{43} + A_{44} = 1 + 1 + 0 + 0 = 2$.\nThe degree matrix $D$ is the diagonal matrix of these degrees:\n$$\nD = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} = 2I.\n$$\nThe combinatorial Laplacian $L$ is defined as $L = D - A$.\n$$\nL = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & -1 & -1 \\\\ 0 & 2 & -1 & -1 \\\\ -1 & -1 & 2 & 0 \\\\ -1 & -1 & 0 & 2 \\end{pmatrix}.\n$$\nNext, we compute the eigen-decomposition of $L$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(L - \\lambda I) = 0$.\n$$\n\\det(L - \\lambda I) = \\det \\begin{pmatrix} 2-\\lambda & 0 & -1 & -1 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nBy subtracting the second row from the first, we get:\n$$\n\\det \\begin{pmatrix} 2-\\lambda & -(2-\\lambda) & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix} = (2-\\lambda) \\det \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nAdding the first row to the third and fourth rows gives:\n$$\n(2-\\lambda) \\det \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ 0 & -2 & 2-\\lambda & 0 \\\\ 0 & -2 & 0 & 2-\\lambda \\end{pmatrix} = (2-\\lambda) \\det \\begin{pmatrix} 2-\\lambda & -1 & -1 \\\\ -2 & 2-\\lambda & 0 \\\\ -2 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nThe determinant of the $3 \\times 3$ matrix is $(2-\\lambda)(2-\\lambda)^{2} - (-1)(-2(2-\\lambda)) + (-1)(0 - (-2)(2-\\lambda)) = (2-\\lambda)^{3} - 2(2-\\lambda) - 2(2-\\lambda) = (2-\\lambda)^{3} - 4(2-\\lambda) = (2-\\lambda)((2-\\lambda)^{2}-4)$.\nSo, the characteristic polynomial is $(2-\\lambda)(2-\\lambda-2)(2-\\lambda+2) = (2-\\lambda)(-\\lambda)(4-\\lambda) = \\lambda(\\lambda-2)^{2}(4-\\lambda)$.\nThe eigenvalues are $\\lambda=0$, $\\lambda=2$ (with multiplicity $2$), and $\\lambda=4$. In non-decreasing order: $\\lambda_1=0$, $\\lambda_2=2$, $\\lambda_3=2$, $\\lambda_4=4$.\nThe diagonal matrix of eigenvalues is $\\Lambda = \\text{diag}(0, 2, 2, 4)$.\n\nNow we find the corresponding orthonormal eigenvectors.\nFor $\\lambda_1 = 0$: $Lv=0$. This yields $v_1=v_2=v_3=v_4$. A normalized eigenvector is $u_1 = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^{\\top}$.\nFor $\\lambda_2=\\lambda_3 = 2$: $(L-2I)v=0$. This system reduces to $v_1+v_2=0$ and $v_3+v_4=0$. The eigenspace is two-dimensional. We can choose an orthonormal basis for this space. One such basis is:\n$u_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 & 0 & 0 \\end{pmatrix}^{\\top}$ and $u_3 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 & 0 & 1 & -1 \\end{pmatrix}^{\\top}$. These are orthogonal and have unit norm.\nFor $\\lambda_4 = 4$: $(L-4I)v=0$. This yields $v_1=v_2$ and $v_3=v_4=-v_1$. A normalized eigenvector is $u_4 = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & -1 & -1 \\end{pmatrix}^{\\top}$.\n\nThe matrix of eigenvectors $U$ is formed by these column vectors:\n$$\nU = \\begin{pmatrix} u_1 & u_2 & u_3 & u_4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{2} \\\\ \\frac{1}{2} & 0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{2} \\\\ \\frac{1}{2} & 0 & -\\frac{1}{\\sqrt{2}} & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe Graph Fourier Transform of the signal $x = \\begin{pmatrix} 1 & 2 & -1 & 0 \\end{pmatrix}^{\\top}$ is $\\widehat{x} = U^{\\top}x$. The components of $\\widehat{x}$ are $\\widehat{x}_i = u_i^{\\top}x$.\n$\\widehat{x}_1 = u_1^{\\top}x = \\frac{1}{2}(1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot (-1) + 1 \\cdot 0) = \\frac{1}{2}(2) = 1$.\n$\\widehat{x}_2 = u_2^{\\top}x = \\frac{1}{\\sqrt{2}}(1 \\cdot 1 - 1 \\cdot 2 + 0 \\cdot (-1) + 0 \\cdot 0) = \\frac{1}{\\sqrt{2}}(-1) = -\\frac{1}{\\sqrt{2}}$.\n$\\widehat{x}_3 = u_3^{\\top}x = \\frac{1}{\\sqrt{2}}(0 \\cdot 1 + 0 \\cdot 2 + 1 \\cdot (-1) - 1 \\cdot 0) = \\frac{1}{\\sqrt{2}}(-1) = -\\frac{1}{\\sqrt{2}}$.\n$\\widehat{x}_4 = u_4^{\\top}x = \\frac{1}{2}(1 \\cdot 1 + 1 \\cdot 2 - 1 \\cdot (-1) - 1 \\cdot 0) = \\frac{1}{2}(1+2+1) = \\frac{4}{2} = 2$.\nThe GFT is $\\widehat{x} = \\begin{pmatrix} 1 & -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 2 \\end{pmatrix}^{\\top}$.\n\nThe problem requires the total spectral energy in the eigenspace associated with eigenvalue $2$. This corresponds to the GFT coefficients $\\widehat{x}_2$ and $\\widehat{x}_3$. The energy is the sum of their squared magnitudes.\nLet $E_2$ be the energy for $\\lambda=2$.\n$$\nE_2 = |\\widehat{x}_2|^2 + |\\widehat{x}_3|^2.\n$$\nSubstituting the computed values:\n$$\nE_2 = \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 = \\frac{1}{2} + \\frac{1}{2} = 1.\n$$\nThis quantity is unique regardless of the specific choice of orthonormal basis for the eigenspace of the repeated eigenvalue.", "answer": "$$\\boxed{1}$$", "id": "2912992"}, {"introduction": "Moving beyond a single, fixed graph, this practice explores how the choice of operator fundamentally shapes the spectral representation. You will analytically derive the spectra for both the combinatorial Laplacian $L$ and the symmetric normalized Laplacian $L_{\\text{sym}}$ on a star graph, a canonical example of a graph with non-uniform node degrees. This comparative analysis is crucial for understanding why different Laplacians offer distinct notions of \"frequency\" and how they capture signal properties differently, a critical consideration in any graph signal processing task [@problem_id:2912998].", "problem": "Consider the undirected star graph with $N \\geq 3$ nodes and unit weights, where node $1$ is the hub connected to nodes $2,3,\\dots,N$. Let $A$ be the adjacency matrix, $D$ the diagonal degree matrix, $L = D - A$ the combinatorial graph Laplacian, and $L_{\\text{sym}} = D^{-1/2} L D^{-1/2}$ the symmetric normalized Laplacian. Work from the definitions of $A$, $D$, $L$, and $L_{\\text{sym}}$, and the spectral theorem for real symmetric matrices.\n\nTasks:\n- Derive the spectra (all eigenvalues with algebraic multiplicities) of $L$ and $L_{\\text{sym}}$. For each, identify an orthonormal eigenbasis that makes explicit the symmetry between the hub and the leaves.\n- Using your eigenbases, explain how the eigenvectors associated with the nontrivial extreme eigenvalues (the largest eigenvalue of $L$ and the largest eigenvalue of $L_{\\text{sym}}$) distribute mass between the hub and the leaves, and contrast this with the eigenvectors for the zero eigenvalue for each Laplacian.\n- Define the Graph Fourier Transform (GFT) as the expansion of a graph signal in the orthonormal eigenbasis of the chosen Laplacian. Consider the graph signal $x \\in \\mathbb{R}^{N}$ that is an impulse at the hub, i.e., $x = e_{1}$ where $e_{1}$ is the first standard basis vector. Let $\\alpha_{\\max}$ denote the GFT coefficient of $x$ onto the eigenvector corresponding to the largest eigenvalue of $L$, and let $\\beta_{\\max}$ denote the GFT coefficient of $x$ onto the eigenvector corresponding to the largest eigenvalue of $L_{\\text{sym}}$. Define the scalar\n$$\nR(N) \\triangleq \\frac{|\\alpha_{\\max}|^{2}}{|\\beta_{\\max}|^{2}}.\n$$\nCompute $R(N)$ in closed form as a function of $N$.\n\nState your final answer as a single exact expression in terms of $N$. No units are required and no rounding is necessary.", "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. It is a standard exercise in spectral graph theory and presents no inconsistencies or ambiguities. We shall proceed with the derivation.\n\nThe star graph on $N$ nodes has a central hub, node $1$, connected to $N-1$ peripheral leaf nodes, nodes $2, \\dots, N$. All edge weights are unity. The degree of the hub is $d_1 = N-1$ and the degree of each leaf is $d_i = 1$ for $i \\in \\{2, \\dots, N\\}$.\n\nThe diagonal degree matrix $D$ is given by:\n$$ D = \\text{diag}(N-1, 1, 1, \\dots, 1) $$\nThe adjacency matrix $A$ is given by:\n$$ A = \\begin{pmatrix} 0 & 1 & \\cdots & 1 \\\\ 1 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 0 & \\cdots & 0 \\end{pmatrix} $$\nThe combinatorial Laplacian is $L = D - A$:\n$$ L = \\begin{pmatrix} N-1 & -1 & \\cdots & -1 \\\\ -1 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -1 & 0 & \\cdots & 1 \\end{pmatrix} $$\nThe symmetric normalized Laplacian is $L_{\\text{sym}} = D^{-1/2} L D^{-1/2}$. First, $D^{-1/2} = \\text{diag}(\\frac{1}{\\sqrt{N-1}}, 1, \\dots, 1)$. Then:\n$$ L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2} = \\begin{pmatrix} 1 & -\\frac{1}{\\sqrt{N-1}} & \\cdots & -\\frac{1}{\\sqrt{N-1}} \\\\ -\\frac{1}{\\sqrt{N-1}} & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -\\frac{1}{\\sqrt{N-1}} & 0 & \\cdots & 1 \\end{pmatrix} $$\nBoth $L$ and $L_{\\text{sym}}$ are real symmetric matrices, and thus possess an orthonormal basis of eigenvectors.\n\n**Part 1: Spectra and Eigenbases**\n\n**Spectrum of the Combinatorial Laplacian $L$**\nWe solve the eigenvalue equation $Lv = \\lambda v$ for $v \\in \\mathbb{R}^N$.\n1.  **Eigenvalue $\\lambda_1 = 0$**: For any connected graph, the combinatorial Laplacian has a simple eigenvalue of $0$ with a corresponding eigenvector of all ones.\n    The unnormalized eigenvector is $[1, 1, \\dots, 1]^T$. The normalized eigenvector is:\n    $$ u_1 = \\frac{1}{\\sqrt{N}} [1, 1, \\dots, 1]^T $$\n    The multiplicity is $1$.\n\n2.  **Eigenvalue $\\lambda = 1$**: We seek $v$ such that $Lv = v$. Writing this out by components:\n    -   For the hub node ($i=1$): $(N-1)v_1 - \\sum_{j=2}^N v_j = v_1 \\implies (N-2)v_1 = \\sum_{j=2}^N v_j$.\n    -   For leaf nodes ($i \\in \\{2, \\dots, N\\}$): $-v_1 + v_i = v_i \\implies v_1 = 0$.\n    Substituting $v_1=0$ into the first equation yields $\\sum_{j=2}^N v_j = 0$. Thus, any vector with a zero at the hub and whose leaf components sum to zero is an eigenvector for $\\lambda=1$. This defines a vector space of dimension $(N-1) - 1 = N-2$.\n    The eigenspace is $\\{ v \\in \\mathbb{R}^N \\mid v_1=0 \\text{ and } \\sum_{i=2}^N v_i = 0 \\}$. This subspace has dimension $N-2$, so $\\lambda=1$ is an eigenvalue with multiplicity $N-2$. An orthonormal basis for this space can be constructed, for instance, by applying the Gram-Schmidt process to vectors of the form $e_i - e_{i+1}$ for $i=2, \\dots, N-1$. These eigenvectors are entirely supported on the leaves, reflecting the permutation symmetry of the leaf nodes.\n\n3.  **Eigenvalue $\\lambda_N = N$**: The sum of the eigenvalues equals the trace of the matrix, $\\text{Tr}(L) = \\sum d_i = (N-1) + (N-1) = 2N-2$. We have found one eigenvalue of $0$ and $N-2$ eigenvalues of $1$. The final eigenvalue must be $\\lambda_N = (2N-2) - (0 \\cdot 1 + 1 \\cdot (N-2)) = N$.\n    Let $v_N$ be the corresponding eigenvector. From $Lv_N = Nv_N$:\n    -   For the hub node: $(N-1)v_1 - \\sum_{j=2}^N v_j = Nv_1 \\implies -\\sum_{j=2}^N v_j = v_1$.\n    -   For leaf nodes: $-v_1 + v_i = N v_i \\implies -v_1 = (N-1)v_i$, so $v_i = -v_1/(N-1)$ for $i \\ge 2$.\n    This is consistent, as $\\sum_{j=2}^N v_j = (N-1) \\cdot (-v_1/(N-1)) = -v_1$. An unnormalized eigenvector is $[N-1, -1, \\dots, -1]^T$.\n    The squared norm is $(N-1)^2 + (N-1)(-1)^2 = (N-1)^2 + (N-1) = N(N-1)$.\n    The normalized eigenvector is:\n    $$ u_N = \\frac{1}{\\sqrt{N(N-1)}} [N-1, -1, \\dots, -1]^T $$\n\nThe spectrum of $L$ is $\\{0, 1, \\dots, 1, N\\}$ with multiplicities $\\{1, N-2, 1\\}$.\n\n**Spectrum of the Symmetric Normalized Laplacian $L_{\\text{sym}}$**\nWe solve the eigenvalue equation $L_{\\text{sym}} v = \\lambda v$.\n1.  **Eigenvalue $\\lambda'_1 = 0$**: For any graph, the zero eigenvector of $L_{\\text{sym}}$ is $D^{1/2}\\mathbf{1}$.\n    $D^{1/2}\\mathbf{1} = [\\sqrt{N-1}, 1, \\dots, 1]^T$.\n    The squared norm is $(\\sqrt{N-1})^2 + (N-1)(1)^2 = 2(N-1)$.\n    The normalized eigenvector is:\n    $$ u'_1 = \\frac{1}{\\sqrt{2(N-1)}} [\\sqrt{N-1}, 1, \\dots, 1]^T $$\n    The multiplicity is $1$.\n\n2.  **Eigenvalue $\\lambda' = 1$**: We seek $v$ such that $L_{\\text{sym}}v = v$. This implies $(I - D^{-1/2}AD^{-1/2})v = v$, which simplifies to $D^{-1/2}AD^{-1/2}v = 0$. Let $M=D^{-1/2}AD^{-1/2}$.\n    $$ Mv = \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{N-1}} & \\cdots & \\frac{1}{\\sqrt{N-1}} \\\\ \\frac{1}{\\sqrt{N-1}} & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{1}{\\sqrt{N-1}} & 0 & \\cdots & 0 \\end{pmatrix} v = 0 $$\n    This gives two conditions: $\\frac{1}{\\sqrt{N-1}} \\sum_{j=2}^N v_j = 0$ and $\\frac{1}{\\sqrt{N-1}} v_1 = 0$. These simplify to $v_1=0$ and $\\sum_{j=2}^N v_j = 0$. This is precisely the same eigenspace found for $L$ corresponding to $\\lambda=1$. Thus, $\\lambda'=1$ is an eigenvalue of $L_{\\text{sym}}$ with multiplicity $N-2$. The eigenbasis for this space is the same as for $L$.\n\n3.  **Eigenvalue $\\lambda'_N = 2$**: The trace of $L_{\\text{sym}}$ is $N$. So far, we have eigenvalues with sum $0 \\cdot 1 + 1 \\cdot (N-2) = N-2$. The last eigenvalue must be $\\lambda'_N = N-(N-2)=2$.\n    To find the eigenvector, we can look for the other non-zero eigenvalue of $M$. Let us seek an eigenvector of the form $[a, b, \\dots, b]^T$ for $M$.\n    $M[a, b, \\dots, b]^T = [\\sqrt{N-1}b, a/\\sqrt{N-1}, \\dots, a/\\sqrt{N-1}]^T$.\n    For this to be $\\mu [a, b, \\dots, b]^T$, we need $\\sqrt{N-1}b = \\mu a$ and $a/\\sqrt{N-1} = \\mu b$.\n    This gives $\\mu^2=1$, so $\\mu=\\pm 1$. The eigenvalues of $L_{\\text{sym}}=I-M$ are $\\lambda = 1-\\mu$.\n    For $\\mu=1$, we get $\\lambda=0$. For $\\mu=-1$, we get $\\lambda=2$.\n    For $\\mu=-1$, we have $a = \\mu b\\sqrt{N-1} = -b\\sqrt{N-1}$.\n    An unnormalized eigenvector is $[-\\sqrt{N-1}, 1, \\dots, 1]^T$.\n    The squared norm is $(-\\sqrt{N-1})^2 + (N-1)(1)^2 = 2(N-1)$.\n    The normalized eigenvector is:\n    $$ u'_N = \\frac{1}{\\sqrt{2(N-1)}} [-\\sqrt{N-1}, 1, \\dots, 1]^T $$\n\nThe spectrum of $L_{\\text{sym}}$ is $\\{0, 1, \\dots, 1, 2\\}$ with multiplicities $\\{1, N-2, 1\\}$.\n\n**Part 2: Eigenvector Mass Distribution**\n\n- **For $L$**:\n    - The eigenvector for $\\lambda_1=0$, $u_1 = \\frac{1}{\\sqrt{N}}[1, \\dots, 1]^T$, represents a constant signal. The squared-mass is distributed uniformly: $\\frac{1}{N}$ on the hub and $\\frac{1}{N}$ on each leaf. Total leaf mass is $\\frac{N-1}{N}$.\n    - The eigenvector for $\\lambda_N=N$, $u_N = \\frac{1}{\\sqrt{N(N-1)}}[N-1, -1, \\dots, -1]^T$, shows a strong dichotomy. The hub component is large and positive, while leaf components are small and negative.\n        - Mass on hub: $|(u_N)_1|^2 = \\frac{(N-1)^2}{N(N-1)} = \\frac{N-1}{N}$.\n        - Mass on leaves (total): $(N-1) \\times \\left|\\frac{-1}{\\sqrt{N(N-1)}}\\right|^2 = (N-1) \\frac{1}{N(N-1)} = \\frac{1}{N}$.\n    - Contrast: For $L$, the zero-eigenvector distributes mass almost evenly, while the largest-eigenvalue eigenvector concentrates most of the mass ($\\frac{N-1}{N}$) on the high-degree hub.\n\n- **For $L_{\\text{sym}}$**:\n    - The eigenvector for $\\lambda'_1=0$, $u'_1 = \\frac{1}{\\sqrt{2(N-1)}}[\\sqrt{N-1}, 1, \\dots, 1]^T$.\n        - Mass on hub: $|(u'_1)_1|^2 = \\frac{N-1}{2(N-1)} = \\frac{1}{2}$.\n        - Mass on leaves (total): $(N-1) \\times \\left|\\frac{1}{\\sqrt{2(N-1)}}\\right|^2 = (N-1) \\frac{1}{2(N-1)} = \\frac{1}{2}$.\n    - The eigenvector for $\\lambda'_N=2$, $u'_N = \\frac{1}{\\sqrt{2(N-1)}}[-\\sqrt{N-1}, 1, \\dots, 1]^T$.\n        - Mass on hub: $|(u'_N)_1|^2 = \\frac{(-\\sqrt{N-1})^2}{2(N-1)} = \\frac{1}{2}$.\n        - Mass on leaves (total): $(N-1) \\times \\left|\\frac{1}{\\sqrt{2(N-1)}}\\right|^2 = \\frac{1}{2}$.\n    - Contrast: For $L_{\\text{sym}}$, both the zero-eigenvector and the largest-eigenvalue eigenvector partition the total mass exactly in half between the hub and the collection of leaves. This normalization removes the bias toward high-degree nodes seen in the combinatorial Laplacian's eigenvectors.\n\n**Part 3: Computation of $R(N)$**\n\nThe Graph Fourier Transform (GFT) of a signal $x \\in \\mathbb{R}^N$ is its expansion in an orthonormal eigenbasis $\\{u_k\\}$ of a Laplacian. The coefficient corresponding to eigenvector $u_k$ is $\\hat{x}_k = u_k^T x$. The problem considers the signal $x = e_1 = [1, 0, \\dots, 0]^T$, an impulse at the hub.\n\n- For the combinatorial Laplacian $L$, the largest eigenvalue is $\\lambda_N = N$, with eigenvector $u_N$. The GFT coefficient is $\\alpha_{\\max}$:\n$$ \\alpha_{\\max} = u_N^T x = u_N^T e_1 = (u_N)_1 = \\frac{N-1}{\\sqrt{N(N-1)}} $$\nThe squared magnitude is:\n$$ |\\alpha_{\\max}|^2 = \\left(\\frac{N-1}{\\sqrt{N(N-1)}}\\right)^2 = \\frac{(N-1)^2}{N(N-1)} = \\frac{N-1}{N} $$\n\n- For the symmetric normalized Laplacian $L_{\\text{sym}}$, the largest eigenvalue is $\\lambda'_N = 2$, with eigenvector $u'_N$. The GFT coefficient is $\\beta_{\\max}$:\n$$ \\beta_{\\max} = (u'_N)^T x = (u'_N)^T e_1 = (u'_N)_1 = \\frac{-\\sqrt{N-1}}{\\sqrt{2(N-1)}} = -\\frac{1}{\\sqrt{2}} $$\nThe squared magnitude is:\n$$ |\\beta_{\\max}|^2 = \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 = \\frac{1}{2} $$\n\n- Finally, we compute the ratio $R(N)$:\n$$ R(N) = \\frac{|\\alpha_{\\max}|^2}{|\\beta_{\\max}|^2} = \\frac{\\frac{N-1}{N}}{\\frac{1}{2}} = \\frac{2(N-1)}{N} $$\n\nThis is the required closed-form expression.", "answer": "$$\\boxed{\\frac{2(N-1)}{N}}$$", "id": "2912998"}, {"introduction": "The true power of graph signal processing is realized in its application to large, real-world networks where direct eigen-decomposition is computationally infeasible. This practice bridges the gap between theory and application by tasking you with designing a filter pipeline using Chebyshev polynomial approximation. Mastering the ability to apply a graph filter using only efficient, sparse matrix-vector products is the key to unlocking graph signal processing for graphs with millions of nodes [@problem_id:2913014].", "problem": "Design and implement a complete, runnable program that constructs a scalable pipeline to apply a Chebyshev filter of order $K$ to a graph signal $x$ on a large, sparse, undirected graph, using only sparse matrix-vector multiplications and constant working memory per iteration. The derivation must start from the following fundamental base: the unnormalized combinatorial graph Laplacian $L = D - A$ for an undirected graph with adjacency matrix $A$ and degree matrix $D$, the spectral representation of linear graph filters via the Graph Fourier Transform (GFT), and the defining properties of Chebyshev polynomials as an orthogonal basis on the interval $\\left[-1, 1\\right]$. From these base definitions, derive a numerically stable three-term recurrence that enables evaluating a polynomial graph filter of degree $K$ on $x$ without computing any eigen-decomposition, and justify how to rescale the spectrum of $L$ to the interval $\\left[-1, 1\\right]$ using an estimate of the largest eigenvalue $\\lambda_{\\max}$. Your pipeline must:\n- Use only sparse matrix-vector products with $L$ and $O(N)$ auxiliary memory for vectors in $\\mathbb{R}^N$, where $N$ is the number of nodes.\n- Accept a filter specified by coefficients $\\{c_k\\}_{k=0}^K$ and output the filtered signal $y \\in \\mathbb{R}^N$.\n- Be robust to boundary cases such as $K = 0$.\n- Justify how $\\lambda_{\\max}$ is to be obtained without dense factorizations on large graphs, while still permitting exactly computed references for small test graphs.\n\nYour program must implement the pipeline and validate it against a spectral reference computed by dense eigen-decomposition on small graphs. For each test case, compute the filtered signal $y$ in two ways:\n- The scalable Chebyshev pipeline using the three-term recurrence on the rescaled Laplacian, with the largest eigenvalue $\\lambda_{\\max}$ provided exactly for the test graphs.\n- A dense spectral reference using the GFT, by applying the same polynomial in the eigen-domain and transforming back.\n\nFor each test case, report a boolean indicating whether the relative error $\\|y_{\\text{cheb}} - y_{\\text{spec}}\\|_2 / \\max\\{\\|y_{\\text{spec}}\\|_2, \\epsilon\\}$ is less than a tolerance $\\tau$, where $\\epsilon = 10^{-14}$ and $\\tau = 10^{-8}$. All Euclidean norms are the standard $\\ell_2$ norm.\n\nTest suite specification:\n- In all tests, the Laplacian is the unnormalized combinatorial Laplacian $L = D - A$ of an undirected, unweighted graph with no self-loops, and the adjacency matrix $A$ is symmetric with binary entries. The graph signal $x$ must be generated as independent and identically distributed (IID) standard normal entries, with the specified pseudo-random seed per test.\n- Test case $1$ (happy path):\n  - Graph: path graph with $n = 12$ nodes.\n  - Order: $K = 5$.\n  - Coefficients: $c = [0.7, -0.4, 0.3, 0.0, -0.05, 0.01]$.\n  - Signal: seed $0$.\n- Test case $2$ (boundary case $K = 0$):\n  - Graph: star graph with $n = 37$ nodes (one hub connected to all others).\n  - Order: $K = 0$.\n  - Coefficients: $c = [2.3]$.\n  - Signal: seed $1$.\n- Test case $3$ (spectrum with multiplicities):\n  - Graph: disjoint union of two cycles (rings) with sizes $n_1 = 15$ and $n_2 = 15$ (total $n = 30$).\n  - Order: $K = 4$.\n  - Coefficients: $c = [1.0, -0.2, 0.05, -0.01, 0.002]$.\n  - Signal: seed $2$.\n- Test case $4$ (high-degree hub, larger $K$):\n  - Graph: star graph with $n = 60$ nodes.\n  - Order: $K = 7$.\n  - Coefficients: $c = [0.5, -0.3, 0.2, -0.1, 0.05, -0.02, 0.01, -0.005]$.\n  - Signal: seed $3$.\n- Test case $5$ (random sparse graph):\n  - Graph: Erdős–Rényi random graph $G(n, p)$ with $n = 120$ nodes and edge probability $p = 0.05$, undirected, no self-loops, generated deterministically with the specified seed.\n  - Order: $K = 6$.\n  - Coefficients: $c = [0.4, 0.1, -0.08, 0.06, -0.04, 0.02, -0.01]$.\n  - Signal: seed $4$.\n\nImplementation constraints:\n- Use sparse matrix operations for the scalable pipeline.\n- For the dense spectral reference, compute the full eigen-decomposition of $L$ as a dense matrix to obtain the exact $\\lambda_{\\max}$ and eigenvectors.\n- For each test case, both methods must use the same exact $\\lambda_{\\max}$ for the spectrum rescaling.\n- All random number generation must use a pseudo-random number generator with the specified seeds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean corresponding to the respective test case in the order listed above. No other text should be printed.", "solution": "The problem requires the derivation and implementation of a scalable pipeline for applying a graph filter to a signal, based on a Chebyshev polynomial approximation. The solution must be derived from fundamental principles and validated against a direct spectral method.\n\n**1. Fundamentals of Spectral Graph Filtering**\n\nA graph is denoted by $G=(V, E)$, with $N=|V|$ vertices. A signal on this graph is a function $x: V \\to \\mathbb{R}$, represented by a vector $x \\in \\mathbb{R}^N$. The graph structure is described by its adjacency matrix $A$ and degree matrix $D$. The unnormalized combinatorial Laplacian is defined as $L = D - A$. For an undirected graph, $L$ is a real, symmetric, positive semi-definite matrix.\n\nAs $L$ is real and symmetric, it admits a complete set of orthonormal eigenvectors, which form the columns of an orthogonal matrix $U$. The spectral decomposition of $L$ is $L = U \\Lambda U^T$, where $\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_N)$ is the diagonal matrix of corresponding non-negative, real eigenvalues, $0 \\le \\lambda_1 \\le \\dots \\le \\lambda_N$. The matrix $U$ defines the Graph Fourier Transform (GFT). The GFT of a signal $x$ is $\\hat{x} = U^T x$, and the inverse GFT is $x = U \\hat{x}$.\n\nA linear, shift-invariant graph filter is defined as a matrix function of the Laplacian, $H = h(L)$. Applying this filter to a signal $x$ yields the output $y = Hx = h(L)x$. In the spectral domain, this operation becomes a simple multiplication:\n$$y = h(L)x = U h(\\Lambda) U^T x = U \\hat{y}$$\nwhere $\\hat{y}_i = h(\\lambda_i) \\hat{x}_i$. The function $h(\\cdot)$ is the filter's frequency response, evaluated at the graph frequencies (eigenvalues) $\\lambda_i$. This approach, while exact, is computationally prohibitive for large graphs as it requires the full eigen-decomposition of $L$, an $O(N^3)$ operation. This method serves as our **dense spectral reference**.\n\n**2. Scalable Filtering with Chebyshev Polynomials**\n\nTo create a scalable method, we avoid eigen-decomposition by approximating the filter response $h(\\lambda)$ with a polynomial of finite order $K$. A numerically stable and efficient choice for this approximation is the basis of Chebyshev polynomials of the first kind, $T_k(z)$. These polynomials are defined on the interval $z \\in \\left[-1, 1\\right]$ and satisfy the three-term recurrence relation:\n$$T_0(z) = 1, \\quad T_1(z) = z, \\quad T_{k+1}(z) = 2zT_k(z) - T_{k-1}(z) \\text{ for } k \\ge 1$$\nThe filter response is thus approximated by $h(\\lambda) \\approx \\sum_{k=0}^{K} c_k T_k(\\tilde{\\lambda})$, where $\\{c_k\\}_{k=0}^K$ are the given filter coefficients and $\\tilde{\\lambda}$ is the eigenvalue $\\lambda$ rescaled to the domain $\\left[-1, 1\\right]$.\n\n**3. Spectrum Rescaling and the Three-Term Recurrence**\n\nThe eigenvalues of the unnormalized Laplacian $L$ lie in the interval $\\left[0, \\lambda_{\\max}\\right]$. To use the Chebyshev basis, we must apply a linear transformation to map this interval to $\\left[-1, 1\\right]$. The standard mapping is:\n$$\\tilde{\\lambda} = \\frac{2\\lambda}{\\lambda_{\\max}} - 1$$\nThis maps $\\lambda=0$ to $\\tilde{\\lambda}=-1$ and $\\lambda=\\lambda_{\\max}$ to $\\tilde{\\lambda}=1$. The corresponding rescaled Laplacian *operator* is $\\tilde{L} = \\frac{2}{\\lambda_{\\max}}L - I$, where $I$ is the identity matrix. The eigenvalues of $\\tilde{L}$ are precisely $\\tilde{\\lambda}_i$, which lie in $\\left[-1, 1\\right]$.\n\nThe filtered signal $y$ can now be computed as:\n$$y = h(L)x \\approx \\left( \\sum_{k=0}^K c_k T_k(\\tilde{L}) \\right) x = \\sum_{k=0}^K c_k \\left( T_k(\\tilde{L})x \\right)$$\nLet us define a sequence of vectors $\\bar{x}_k = T_k(\\tilde{L})x$. We can compute this sequence efficiently by leveraging the Chebyshev recurrence relation:\n\\begin{itemize}\n    \\item $\\bar{x}_0 = T_0(\\tilde{L})x = I x = x$\n    \\item $\\bar{x}_1 = T_1(\\tilde{L})x = \\tilde{L}x = \\left(\\frac{2}{\\lambda_{\\max}}L - I\\right)x$\n    \\item For $k \\ge 1$, $\\bar{x}_{k+1} = T_{k+1}(\\tilde{L})x = (2\\tilde{L}T_k(\\tilde{L}) - T_{k-1}(\\tilde{L}))x = 2\\tilde{L}\\bar{x}_k - \\bar{x}_{k-1}$\n\\end{itemize}\nThis results in a scalable algorithm. The final signal $y$ is constructed as the linear combination $y = \\sum_{k=0}^K c_k \\bar{x}_k$. The computation involves a sequence of sparse matrix-vector products with $L$ (within the application of $\\tilde{L}$), vector additions, and scalar multiplications. For a filter of order $K$, this requires $K$ such products. The auxiliary memory is $O(N)$ to store the vectors $\\bar{x}_{k-1}$, $\\bar{x}_k$, and the resulting signal $y$, independent of $K$. This algorithm constitutes our **scalable Chebyshev pipeline**.\n\nThe boundary case $K=0$ is handled naturally. The filter is $y=c_0 T_0(\\tilde{L})x=c_0x$.\n\n**4. Estimation of the Largest Eigenvalue $\\lambda_{\\max}$**\n\nFor the provided test cases on small graphs, $\\lambda_{\\max}$ is computed exactly from the full spectrum of the dense Laplacian matrix. However, on large graphs where this is infeasible, $\\lambda_{\\max}$ must be estimated. A standard and effective method is the **Power Iteration algorithm**. It iteratively computes $v_{i+1} = Lv_i / \\|Lv_i\\|_2$, starting from a random vector $v_0$. The vector $v_i$ converges to the eigenvector corresponding to the eigenvalue with the largest magnitude. Since $L$ is positive semi-definite, this is $\\lambda_{\\max}$. After a sufficient number of iterations, $\\lambda_{\\max}$ can be estimated using the Rayleigh quotient: $\\lambda_{\\max} \\approx v^T L v / (v^T v)$. This estimation relies only on sparse matrix-vector products, aligning with the scalability requirement. For numerical stability of the Chebyshev expansion, it is sufficient to use an upper bound, $\\lambda_{\\text{est}} \\ge \\lambda_{\\max}$, which ensures the rescaled spectrum lies within $\\left[-1, 1\\right]$.\n\n**Summary of Implemented Methods**\n\n1.  **Scalable Chebyshev Pipeline ($y_{\\text{cheb}}$)**:\n    - Input: Sparse Laplacian $L$, signal $x$, coefficients $\\{c_k\\}_{k=0}^K$, $\\lambda_{\\max}$.\n    - Algorithm: Uses the three-term recurrence.\n    - Complexity: $O(K \\cdot \\text{nnz}(L))$ time, $O(N)$ auxiliary space.\n\n2.  **Dense Spectral Reference ($y_{\\text{spec}}$)**:\n    - Input: Dense Laplacian $L$, signal $x$, coefficients $\\{c_k\\}_{k=0}^K$, $\\lambda_{\\max}$.\n    - Algorithm: Computes full eigen-decomposition of $L$, applies the filter in the spectral domain, and transforms back. The filter response $h(\\lambda_i)$ for each eigenvalue is evaluated by summing the Chebyshev polynomial series.\n    - Complexity: $O(N^3)$ time, $O(N^2)$ space.\n\nThe relative error $\\|y_{\\text{cheb}} - y_{\\text{spec}}\\|_2 / \\max\\{\\|y_{\\text{spec}}\\|_2, \\epsilon\\}$ is then computed to validate the pipeline's correctness against the spectral ground truth.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.linalg import eigh\nimport collections\n\n# Global constants for validation\n_TOLERANCE = 1e-8\n_EPSILON = 1e-14\n\ndef make_path_graph(n):\n    \"\"\"Creates the adjacency matrix for a path graph with n nodes.\"\"\"\n    A = diags([1, 1], [-1, 1], shape=(n, n)).toarray()\n    return A\n\ndef make_star_graph(n):\n    \"\"\"Creates the adjacency matrix for a star graph with n nodes.\"\"\"\n    A = np.zeros((n, n))\n    A[0, 1:] = 1\n    A[1:, 0] = 1\n    return A\n\ndef make_disjoint_cycles_graph(n1, n2):\n    \"\"\"Creates adjacency matrix for a graph of two disjoint cycles.\"\"\"\n    n = n1 + n2\n    A = np.zeros((n, n))\n    # Cycle 1\n    for i in range(n1):\n        A[i, (i + 1) % n1] = 1\n        A[(i + 1) % n1, i] = 1\n    # Cycle 2\n    for i in range(n2):\n        A[n1 + i, n1 + (i + 1) % n2] = 1\n        A[n1 + (i + 1) % n2, n1 + i] = 1\n    return A\n\ndef make_er_graph(n, p, seed):\n    \"\"\"Creates an Erdos-Renyi G(n,p) random graph.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.random((n, n)) < p\n    A = np.triu(A, 1)  # Take upper triangle to make it undirected\n    A = A + A.T\n    return A.astype(float)\n\ndef get_laplacian(A):\n    \"\"\"Computes the unnormalized graph Laplacian L = D - A.\"\"\"\n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n    return L\n\ndef chebyshev_pipeline(L_sparse, x, coeffs, K, lambda_max):\n    \"\"\"\n    Computes the filtered signal y = h(L)x using a Chebyshev polynomial expansion.\n    h(L) is approximated by sum_{k=0 to K} c_k T_k(L_tilde).\n    This implementation is scalable and uses only sparse matrix-vector products.\n    \"\"\"\n    n = x.shape[0]\n    K_from_coeffs = len(coeffs) - 1\n    if K != K_from_coeffs:\n        raise ValueError(\"Filter order K does not match number of coefficients.\")\n\n    if K == 0:\n        return coeffs[0] * x\n\n    # T_0(L_tilde)x\n    T_k_minus_2 = x\n    y = coeffs[0] * T_k_minus_2\n\n    # L_tilde @ v = (2/lambda_max) * (L @ v) - v\n    # T_1(L_tilde)x\n    T_k_minus_1 = (2.0 / lambda_max) * (L_sparse @ x) - x\n    y += coeffs[1] * T_k_minus_1\n\n    for k in range(2, K + 1):\n        # L_tilde @ T_{k-1}\n        L_tilde_T_k_minus_1 = (2.0 / lambda_max) * (L_sparse @ T_k_minus_1) - T_k_minus_1\n        # T_k = 2 * L_tilde * T_{k-1} - T_{k-2}\n        T_k = 2.0 * L_tilde_T_k_minus_1 - T_k_minus_2\n        \n        y += coeffs[k] * T_k\n        \n        T_k_minus_2, T_k_minus_1 = T_k_minus_1, T_k\n        \n    return y\n\ndef eval_cheb_poly_scalar(z, coeffs):\n    \"\"\"Evaluates a Chebyshev polynomial sum for a scalar z.\"\"\"\n    K = len(coeffs) - 1\n    if K == 0:\n        return np.array(coeffs[0], dtype=float)\n\n    T_k_minus_2 = 1.0\n    val = coeffs[0] * T_k_minus_2\n\n    T_k_minus_1 = z\n    val += coeffs[1] * T_k_minus_1\n\n    for k in range(2, K + 1):\n        T_k = 2.0 * z * T_k_minus_1 - T_k_minus_2\n        val += coeffs[k] * T_k\n        T_k_minus_2, T_k_minus_1 = T_k_minus_1, T_k\n        \n    return val\n\ndef spectral_reference(L_dense, x, coeffs, K, lambda_max):\n    \"\"\"\n    Computes the filtered signal using the GFT (dense eigen-decomposition).\n    This serves as the ground truth for validation.\n    \"\"\"\n    K_from_coeffs = len(coeffs) - 1\n    if K != K_from_coeffs:\n        raise ValueError(\"Filter order K does not match number of coefficients.\")\n        \n    eigvals, U = eigh(L_dense)\n    \n    # Rescale eigenvalues to [-1, 1]\n    eigvals_tilde = (2.0 / lambda_max) * eigvals - 1.0\n    \n    # Evaluate filter response on eigenvalues\n    h_lambda = eval_cheb_poly_scalar(eigvals_tilde, coeffs)\n        \n    # Apply filter in spectral domain\n    x_hat = U.T @ x\n    y_hat = h_lambda * x_hat\n    \n    # Inverse GFT to get the filtered signal\n    y = U @ y_hat\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and validate the Chebyshev pipeline.\n    \"\"\"\n    TestCase = collections.namedtuple('TestCase', ['graph_func', 'params', 'K', 'coeffs', 'seed'])\n    \n    test_cases = [\n        TestCase(make_path_graph, (12,), 5, [0.7, -0.4, 0.3, 0.0, -0.05, 0.01], 0),\n        TestCase(make_star_graph, (37,), 0, [2.3], 1),\n        TestCase(make_disjoint_cycles_graph, (15, 15), 4, [1.0, -0.2, 0.05, -0.01, 0.002], 2),\n        TestCase(make_star_graph, (60,), 7, [0.5, -0.3, 0.2, -0.1, 0.05, -0.02, 0.01, -0.005], 3),\n        TestCase(make_er_graph, (120, 0.05, 4), 6, [0.4, 0.1, -0.08, 0.06, -0.04, 0.02, -0.01], 4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate graph and Laplacian\n        A = case.graph_func(*case.params)\n        n = A.shape[0]\n        L_dense = get_laplacian(A)\n        L_sparse = csr_matrix(L_dense)\n\n        # 2. Generate signal\n        rng = np.random.default_rng(case.seed)\n        x = rng.standard_normal(n)\n\n        # 3. Get exact lambda_max from dense matrix for validation\n        eigvals = eigh(L_dense, eigvals_only=True)\n        lambda_max = np.max(eigvals)\n        if lambda_max < 1e-9: # Handle disconnected or single-node graphs\n            lambda_max = 1.0\n\n        # 4. Compute with a scalable pipeline\n        y_cheb = chebyshev_pipeline(L_sparse, x, case.coeffs, case.K, lambda_max)\n\n        # 5. Compute with spectral reference method\n        y_spec = spectral_reference(L_dense, x, case.coeffs, case.K, lambda_max)\n        \n        # 6. Validate\n        norm_y_spec = np.linalg.norm(y_spec)\n        norm_diff = np.linalg.norm(y_cheb - y_spec)\n        \n        relative_error = norm_diff / max(norm_y_spec, _EPSILON)\n        \n        results.append(relative_error < _TOLERANCE)\n\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "2913014"}]}