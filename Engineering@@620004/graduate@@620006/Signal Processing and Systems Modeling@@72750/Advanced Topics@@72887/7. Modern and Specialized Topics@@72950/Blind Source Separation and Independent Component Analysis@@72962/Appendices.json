{"hands_on_practices": [{"introduction": "To truly master Blind Source Separation, one must first grapple with its most fundamental question: under what conditions is separation even possible? This property, known as identifiability, is the bedrock upon which all BSS algorithms are built. This exercise guides you through a powerful thought experiment: first, by constructing a scenario with independent, identically distributed Gaussian sources where identifiability fails due to rotational symmetry, and second, by demonstrating how introducing distinct temporal structures restores identifiability, allowing separation through the joint diagonalization of time-lagged covariance matrices [@problem_id:2855524]. Tackling this problem provides deep insight into why non-Gaussianity is essential for standard ICA and how second-order statistics can be leveraged in methods like SOBI (Second-Order Blind Identification).", "problem": "Consider an instantaneous linear mixing model in Blind Source Separation (BSS), where the observed two-channel signal $x(t) \\in \\mathbb{R}^{2}$ is generated by $x(t) = A s(t)$ from latent sources $s(t) = \\begin{pmatrix} s_{1}(t) \\\\ s_{2}(t) \\end{pmatrix}$, with $A \\in \\mathbb{R}^{2 \\times 2}$ being full-rank. Assume the following fundamental base for modeling and inference:\n- The sources $s_{1}(t)$ and $s_{2}(t)$ are statistically independent.\n- The zero-lag covariance of a stationary process $u(t)$ is $R_{u}(0) = \\mathbb{E}[u(t) u(t)^{\\top}]$, and the lag-$\\tau$ covariance is $R_{u}(\\tau) = \\mathbb{E}[u(t) u(t-\\tau)^{\\top}]$.\n- A whitening transform $W$ is any invertible matrix such that the whitened observation $y(t) = W x(t)$ satisfies $R_{y}(0) = I$, the identity matrix.\n- A real, zero-mean, unit-variance Gaussian vector with identity covariance is spherically symmetric, and any orthogonal transformation preserves its distribution.\n\nPart I (constructive failure of identifiability). Suppose $s_{1}(t)$ and $s_{2}(t)$ are independent, identically distributed zero-mean, unit-variance, temporally independent (white) Gaussian processes. Explain, using only the base above, why after whitening the mixtures there is an infinite family of statistically indistinguishable demixing solutions. Your explanation must identify the family explicitly and justify why no choice within this family is preferred by any criterion that depends only on the instantaneous distribution of the whitened data.\n\nPart II (restoring identifiability with temporal structure). Now suppose $s_{1}(t)$ and $s_{2}(t)$ remain independent, zero-mean, unit-variance Gaussian, but are no longer temporally white. Instead, they are stationary and obey distinct first-order autoregressive dynamics in the sense that their normalized lag-$1$ autocorrelations are\n$$\n\\rho_{1} = \\mathbb{E}[s_{1}(t) s_{1}(t-1)] = \\frac{3}{5}, \n\\quad \n\\rho_{2} = \\mathbb{E}[s_{2}(t) s_{2}(t-1)] = -\\frac{1}{5}.\n$$\nLet $y(t) = W x(t)$ be any whitening of the observations at lag $0$, so $R_{y}(0) = I$. Show that there exists an orthogonal matrix $R \\in \\mathbb{R}^{2 \\times 2}$ such that $y(t) = R s(t)$, and derive from the base that the lag-$1$ covariance of $y(t)$ satisfies\n$$\nR_{y}(1) = R \\, \\mathrm{diag}(\\rho_{1}, \\rho_{2}) \\, R^{\\top}.\n$$\nGiven a single empirical estimate of $R_{y}(1)$ as\n$$\nR_{y}(1) = \n\\begin{pmatrix}\n\\frac{2}{5} & \\frac{\\sqrt{3}}{5} \\\\\n\\frac{\\sqrt{3}}{5} & 0\n\\end{pmatrix},\n$$\nand parameterizing $R$ as a rotation $R(\\theta) = \\begin{pmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix}$, compute the rotation angle $\\theta$ (in radians) that recovers the sources by jointly diagonalizing $R_{y}(1)$ with $R(\\theta)^{\\top} R_{y}(1) R(\\theta) = \\mathrm{diag}(\\rho_{1}, \\rho_{2})$. Express your final answer as a single exact value in radians. Do not round.", "solution": "The problem will be addressed in two sequential parts as presented.\n\nPart I: Constructive Failure of Identifiability.\nThe sources $s(t) = \\begin{pmatrix} s_1(t) \\\\ s_2(t) \\end{pmatrix}$ are stipulated to be independent, identically distributed, zero-mean, unit-variance, and temporally white Gaussian processes. The properties of independence, zero mean, and unit variance for the components $s_1(t)$ and $s_2(t)$ collectively imply that the zero-lag covariance matrix of the source vector $s(t)$ is the identity matrix: $R_s(0) = \\mathbb{E}[s(t)s(t)^\\top] = I$, where $I$ is the $2 \\times 2$ identity matrix. The additional property that the sources are Gaussian means the probability distribution of $s(t)$ is the standard multivariate normal distribution, denoted $s(t) \\sim \\mathcal{N}(0, I)$.\n\nA whitening transform $W$ is applied to the observed signal $x(t) = A s(t)$ to generate $y(t) = W x(t)$. The whitened signal $y(t)$ is defined by the property that its zero-lag covariance matrix is the identity, $R_y(0) = I$. We can express $R_y(0)$ using the givens:\n$$ R_y(0) = \\mathbb{E}[y(t)y(t)^\\top] = \\mathbb{E}[(W A s(t))(W A s(t))^\\top] = W A \\, \\mathbb{E}[s(t)s(t)^\\top] \\, (W A)^\\top = (W A) R_s(0) (W A)^\\top $$\nGiven that $R_s(0) = I$ and $R_y(0) = I$, the equation becomes $I = (W A) I (W A)^\\top = (W A)(W A)^\\top$. Let us define the composite matrix $R_{mix} = W A$. The condition $R_{mix} R_{mix}^\\top = I$ signifies that $R_{mix}$ is an orthogonal matrix. Consequently, the whitened signal $y(t)$ is related to the sources $s(t)$ by an orthogonal transformation: $y(t) = R_{mix} s(t)$.\n\nThe problem states as a fundamental base that a zero-mean Gaussian vector with identity covariance is spherically symmetric, and any orthogonal transformation preserves its distribution. Since $s(t) \\sim \\mathcal{N}(0, I)$ and $R_{mix}$ is an orthogonal matrix, the distribution of $y(t)$ is identical to that of $s(t)$. That is, $y(t) \\sim \\mathcal{N}(0, I)$.\n\nThe aim of demixing is to find an \"unmixing\" matrix $B$ such that the estimated sources $\\hat{s}(t) = B y(t)$ recover the statistical properties of the original sources. For a valid recovery in this context, the components of $\\hat{s}(t)$ must be independent and have unit variance. For Gaussian variables, this is equivalent to the condition that the covariance matrix of $\\hat{s}(t)$ is the identity, i.e., $R_{\\hat{s}}(0)=I$.\nLet us determine the condition on $B$ for this to hold. The covariance of the estimated sources is:\n$$ R_{\\hat{s}}(0) = \\mathbb{E}[\\hat{s}(t)\\hat{s}(t)^\\top] = \\mathbb{E}[(B y(t))(B y(t))^\\top] = B \\, \\mathbb{E}[y(t)y(t)^\\top] \\, B^\\top = B R_y(0) B^\\top $$\nSince $R_y(0) = I$, we have $R_{\\hat{s}}(0) = B I B^\\top = B B^\\top$. The requirement $R_{\\hat{s}}(0) = I$ thus implies $B B^\\top = I$, which means $B$ must be an orthogonal matrix.\n\nHerein lies the ambiguity. If we select *any* orthogonal matrix $B$ as our demixing matrix, the resulting signal $\\hat{s}(t) = B y(t)$ is an orthogonal transformation of the vector $y(t) \\sim \\mathcal{N}(0, I)$. Due to the spherical symmetry of the Gaussian distribution, $\\hat{s}(t)$ also follows the distribution $\\mathcal{N}(0, I)$. This means that any such choice of $B$ produces an output signal whose components are independent standard normal variables, making it a valid candidate for the recovered sources based on instantaneous statistics. The family of possible demixing matrices is the set of all $2 \\times 2$ orthogonal matrices, known as the orthogonal group $O(2)$. This is an infinite family, parameterized by a rotation angle and a choice of reflection. No specific choice of orthogonal matrix can be preferred over another if the only available criterion is the statistical independence of the output, as all such choices yield an output with the same distribution $\\mathcal{N}(0, I)$. The true inverse rotation $R_{mix}^\\top$ is just one member of this infinite family of statistically indistinguishable solutions.\n\nPart II: Restoring Identifiability with Temporal Structure.\nFirst, we demonstrate that $y(t) = R s(t)$ for some orthogonal matrix $R$. The sources $s_1(t)$ and $s_2(t)$ are given as independent, zero-mean, and unit-variance. Their instantaneous covariance matrix is therefore $R_s(0) = I$. The whitened signal $y(t) = W A s(t)$ has the property $R_y(0) = I$. Following the same derivation as in Part I, this leads to the conclusion that the matrix $R = W A$ must be orthogonal, yielding $y(t) = R s(t)$.\n\nNext, we derive the expression for the lag-$1$ covariance matrix $R_y(1)$.\n$$ R_y(1) = \\mathbb{E}[y(t) y(t-1)^\\top] = \\mathbb{E}[(R s(t))(R s(t-1))^\\top] = \\mathbb{E}[R s(t) s(t-1)^\\top R^\\top] $$\nAs $R$ is a constant matrix, we can factor it out of the expectation:\n$$ R_y(1) = R \\, \\mathbb{E}[s(t) s(t-1)^\\top] \\, R^\\top = R R_s(1) R^\\top $$\nThe lag-$1$ covariance of the sources, $R_s(1)$, is defined as:\n$$ R_s(1) = \\begin{pmatrix} \\mathbb{E}[s_1(t)s_1(t-1)] & \\mathbb{E}[s_1(t)s_2(t-1)] \\\\ \\mathbb{E}[s_2(t)s_1(t-1)] & \\mathbb{E}[s_2(t)s_2(t-1)] \\end{pmatrix} $$\nThe source processes $s_1(t)$ and $s_2(t)$ are independent. This implies that for any time indices, $\\mathbb{E}[s_1(t)s_2(t-1)] = \\mathbb{E}[s_1(t)]\\mathbb{E}[s_2(t-1)] = 0 \\cdot 0 = 0$. Similarly, the other off-diagonal term is zero. The diagonal elements are the given lag-$1$ autocorrelations, $\\rho_1 = \\mathbb{E}[s_1(t)s_1(t-1)] = \\frac{3}{5}$ and $\\rho_2 = \\mathbb{E}[s_2(t)s_2(t-1)] = -\\frac{1}{5}$. Thus, the source lag-$1$ covariance is a diagonal matrix:\n$$ R_s(1) = \\begin{pmatrix} \\rho_1 & 0 \\\\ 0 & \\rho_2 \\end{pmatrix} = \\mathrm{diag}(\\rho_1, \\rho_2) $$\nSubstituting this into the expression for $R_y(1)$ gives the identifiability relation:\n$$ R_y(1) = R \\, \\mathrm{diag}(\\rho_1, \\rho_2) \\, R^\\top $$\nThis equation shows that $R_y(1)$ is diagonalized by the matrix $R$.\n\nFinally, we compute the rotation angle $\\theta$. The problem provides the parameterization of the mixing rotation as $R = R(\\theta) = \\begin{pmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix}$. The equation $R_y(1) = R D R^\\top$, where $D = \\mathrm{diag}(\\rho_1, \\rho_2)$, is used to find $\\theta$. Let $c = \\cos\\theta$ and $s = \\sin\\theta$. The components of the matrix $R(\\theta) D R(\\theta)^\\top$ are computed as:\n$$ (R(\\theta) D R(\\theta)^\\top)_{11} = c^2 \\rho_1 + s^2 \\rho_2 $$\n$$ (R(\\theta) D R(\\theta)^\\top)_{22} = s^2 \\rho_1 + c^2 \\rho_2 $$\n$$ (R(\\theta) D R(\\theta)^\\top)_{12} = (R(\\theta) D R(\\theta)^\\top)_{21} = cs(\\rho_1 - \\rho_2) $$\nWe are given $\\rho_1 = \\frac{3}{5}$, $\\rho_2 = -\\frac{1}{5}$, and $R_y(1) = \\begin{pmatrix} \\frac{2}{5} & \\frac{\\sqrt{3}}{5} \\\\ \\frac{\\sqrt{3}}{5} & 0 \\end{pmatrix}$.\nThe difference in autocorrelations is $\\rho_1 - \\rho_2 = \\frac{3}{5} - (-\\frac{1}{5}) = \\frac{4}{5}$.\nBy equating the off-diagonal elements of $R(\\theta) D R(\\theta)^\\top$ and the given $R_y(1)$:\n$$ cs(\\frac{4}{5}) = \\frac{\\sqrt{3}}{5} $$\nUsing the identity $cs = \\frac{1}{2}\\sin(2\\theta)$, we obtain:\n$$ \\frac{1}{2}\\sin(2\\theta) \\cdot \\frac{4}{5} = \\frac{\\sqrt{3}}{5} \\implies \\sin(2\\theta) = \\frac{\\sqrt{3}}{2} $$\nNext, we equate the $(1,1)$ diagonal elements:\n$$ c^2\\rho_1 + s^2\\rho_2 = \\frac{2}{5} \\implies c^2(\\frac{3}{5}) + s^2(-\\frac{1}{5}) = \\frac{2}{5} \\implies 3c^2 - s^2 = 2 $$\nUsing the identity $c^2 + s^2 = 1$, we can write this as $3(1-s^2) - s^2 = 2$, which gives $3-4s^2 = 2$, so $s^2 = \\frac{1}{4}$. This means $\\sin\\theta = \\pm \\frac{1}{2}$. Consequently, $c^2 = 1 - s^2 = \\frac{3}{4}$, so $\\cos\\theta = \\pm \\frac{\\sqrt{3}}{2}$.\nThe condition $\\sin(2\\theta) = 2sc = \\frac{\\sqrt{3}}{2} > 0$ requires that $\\sin\\theta$ and $\\cos\\theta$ have the same sign. This leaves two possibilities:\n1. $\\sin\\theta = \\frac{1}{2}$ and $\\cos\\theta = \\frac{\\sqrt{3}}{2}$. This corresponds to $\\theta = \\frac{\\pi}{6} + 2k\\pi$ for any integer $k$.\n2. $\\sin\\theta = -\\frac{1}{2}$ and $\\cos\\theta = -\\frac{\\sqrt{3}}{2}$. This corresponds to $\\theta = \\frac{7\\pi}{6} + 2k\\pi$ for any integer $k$.\n\nWe must also check consistency with the $(2,2)$ element:\n$s^2\\rho_1 + c^2\\rho_2 = (\\frac{1}{4})(\\frac{3}{5}) + (\\frac{3}{4})(-\\frac{1}{5}) = \\frac{3}{20} - \\frac{3}{20} = 0$. This matches the given matrix.\nBoth families of solutions are mathematically valid. The problem asks for a single exact value, so we provide the principal value in $[0, 2\\pi)$, which is $\\theta = \\frac{\\pi}{6}$.", "answer": "$$ \\boxed{\\frac{\\pi}{6}} $$", "id": "2855524"}, {"introduction": "While non-Gaussianity opens the door to separating mixed signals, standard Independent Component Analysis (ICA) can still produce physically nonsensical results if its assumptions do not align with the properties of the true sources. This practice explores such a scenario, where the sources are non-negative (like image intensities or power spectra) but the standard ICA pipeline—specifically, mean-subtraction and whitening—yields a negative value for an estimated component [@problem_id:2855504]. This calculation serves as a crucial reality check, demonstrating that ICA's objective of statistical independence is blind to physical constraints and motivating the development of constrained methods like Nonnegative Matrix Factorization (NMF) for applications where non-negativity is a hard requirement.", "problem": "Consider the linear instantaneous mixing model of Independent Component Analysis (ICA), where the observation vector $\\boldsymbol{x} \\in \\mathbb{R}^{2}$ is given by $\\boldsymbol{x} = \\boldsymbol{A}\\boldsymbol{s}$ with invertible mixing matrix $\\boldsymbol{A} \\in \\mathbb{R}^{2 \\times 2}$ and statistically independent sources $\\boldsymbol{s} = [s_{1}, s_{2}]^{\\top}$. Assume the physical sources are nonnegative intensities so that $s_{1} \\ge 0$ and $s_{2} \\ge 0$ almost surely. Let the sources be independent and identically distributed with $s_{1} \\sim \\mathrm{Uniform}([0,1])$ and $s_{2} \\sim \\mathrm{Uniform}([0,1])$, and let the mixing be the identity $\\boldsymbol{A} = \\boldsymbol{I}_{2}$ so that $\\boldsymbol{x} = \\boldsymbol{s}$.\n\nAn ICA practitioner applies a standard preprocessing pipeline:\n- Centering: subtract the mean $\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{x}]$ to obtain $\\boldsymbol{y} = \\boldsymbol{x} - \\boldsymbol{\\mu}$.\n- Whitening: multiply by a whitening matrix $\\boldsymbol{P}$ constructed from the population covariance $\\boldsymbol{\\Sigma}_{x} = \\mathbb{E}[(\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\top}]$ to obtain $\\boldsymbol{z} = \\boldsymbol{P}\\boldsymbol{y}$ with $\\mathbb{E}[\\boldsymbol{z}\\boldsymbol{z}^{\\top}] = \\boldsymbol{I}_{2}$.\n\nAfter whitening, ICA reduces to finding an orthogonal rotation $\\boldsymbol{R}(\\theta) \\in \\mathrm{SO}(2)$ to optimize a contrast based on non-Gaussianity of components. For the given i.i.d. uniform sources mixed by $\\boldsymbol{I}_{2}$, the kurtosis-based contrast is flat due to symmetry, and the optimizer is not unique. Suppose the algorithm converges to the rotation by angle $\\theta = \\pi/4$:\n$$\n\\boldsymbol{R}\\!\\left(\\frac{\\pi}{4}\\right) \\;=\\; \\begin{bmatrix}\n\\cos\\!\\left(\\frac{\\pi}{4}\\right) & -\\sin\\!\\left(\\frac{\\pi}{4}\\right) \\\\\n\\sin\\!\\left(\\frac{\\pi}{4}\\right) & \\cos\\!\\left(\\frac{\\pi}{4}\\right)\n\\end{bmatrix} \\;=\\; \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1\\\\ 1 & 1\\end{bmatrix}.\n$$\nDefine the estimated independent components as $\\boldsymbol{u} = \\boldsymbol{R}(\\pi/4)\\,\\boldsymbol{P}\\,(\\boldsymbol{x} - \\boldsymbol{\\mu})$.\n\nUsing only fundamental definitions and population moments of the uniform distribution, and without invoking any prepackaged ICA formulas, compute the exact value of the second recovered component $u_{2}$ for the physically feasible observation $\\boldsymbol{x} = [0.1,\\, 0.1]^{\\top}$. Express your final answer as a single closed-form analytic expression. Do not approximate or round your answer. This explicit calculation demonstrates that ICA can yield a recovered component with a negative entry even though the true sources are nonnegative, which motivates the use of Nonnegative Matrix Factorization (NMF) that enforces nonnegativity constraints on the factors.", "solution": "The objective is to compute the second component, $u_{2}$, of the estimated source vector $\\boldsymbol{u} = [u_{1}, u_{2}]^{\\top}$, which is defined by the transformation:\n$$\n\\boldsymbol{u} = \\boldsymbol{R}\\! \\left(\\frac{\\pi}{4}\\right)\\,\\boldsymbol{P}\\,(\\boldsymbol{x} - \\boldsymbol{\\mu})\n$$\nfor a specific observation $\\boldsymbol{x} = [0.1, 0.1]^{\\top}$. We must first determine the components of this transformation: the mean vector $\\boldsymbol{\\mu}$ and the whitening matrix $\\boldsymbol{P}$.\n\nFirst, we determine the mean vector $\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{x}]$. The problem states that the mixing matrix is the identity, $\\boldsymbol{A} = \\boldsymbol{I}_{2}$, so the observations are identical to the sources, $\\boldsymbol{x} = \\boldsymbol{s}$. The sources $s_{1}$ and $s_{2}$ are independent and identically distributed, following a uniform distribution on the interval $[0, 1]$. The expectation of a random variable $X \\sim \\mathrm{Uniform}([a,b])$ is $\\mathbb{E}[X] = \\frac{a+b}{2}$.\nFor our sources, this gives:\n$$\n\\mathbb{E}[s_{1}] = \\frac{0+1}{2} = \\frac{1}{2}\n$$\n$$\n\\mathbb{E}[s_{2}] = \\frac{0+1}{2} = \\frac{1}{2}\n$$\nTherefore, the mean vector is:\n$$\n\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{x}] = \\mathbb{E}[\\boldsymbol{s}] = \\begin{bmatrix} \\mathbb{E}[s_{1}] \\\\ \\mathbb{E}[s_{2}] \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix}\n$$\n\nNext, we determine the whitening matrix $\\boldsymbol{P}$. This matrix is constructed from the population covariance matrix $\\boldsymbol{\\Sigma}_{x} = \\mathbb{E}[(\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\top}]$.\nSince $\\boldsymbol{x}=\\boldsymbol{s}$, this is the covariance matrix of the source vector $\\boldsymbol{s}$. The components of the covariance matrix are given by $\\Sigma_{ij} = \\mathrm{Cov}(s_i, s_j)$.\nThe variance of a random variable $X \\sim \\mathrm{Uniform}([a,b])$ is $\\mathrm{Var}(X) = \\frac{(b-a)^2}{12}$. For our sources:\n$$\n\\mathrm{Var}(s_{1}) = \\mathrm{Var}(s_{2}) = \\frac{(1-0)^2}{12} = \\frac{1}{12}\n$$\nSince the sources $s_{1}$ and $s_{2}$ are statistically independent, their covariance is zero: $\\mathrm{Cov}(s_{1}, s_{2}) = 0$.\nThe covariance matrix is therefore diagonal:\n$$\n\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{\\Sigma}_{s} = \\begin{bmatrix} \\mathrm{Var}(s_{1}) & \\mathrm{Cov}(s_{1}, s_{2}) \\\\ \\mathrm{Cov}(s_{2}, s_{1}) & \\mathrm{Var}(s_{2}) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{12} & 0 \\\\ 0 & \\frac{1}{12} \\end{bmatrix} = \\frac{1}{12} \\boldsymbol{I}_{2}\n$$\nThe whitening matrix $\\boldsymbol{P}$ must satisfy the condition $\\boldsymbol{P} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{P}^{\\top} = \\boldsymbol{I}_{2}$. A standard choice for the whitening matrix is the symmetric whitening transformation, given by the inverse square root of the covariance matrix, $\\boldsymbol{P} = \\boldsymbol{\\Sigma}_{x}^{-1/2}$.\n$$\n\\boldsymbol{P} = \\left(\\frac{1}{12} \\boldsymbol{I}_{2}\\right)^{-1/2} = \\sqrt{12} \\cdot \\boldsymbol{I}_{2}^{-1/2} = \\sqrt{4 \\cdot 3} \\cdot \\boldsymbol{I}_{2} = 2\\sqrt{3} \\, \\boldsymbol{I}_{2}\n$$\nSo, the whitening matrix is:\n$$\n\\boldsymbol{P} = \\begin{bmatrix} 2\\sqrt{3} & 0 \\\\ 0 & 2\\sqrt{3} \\end{bmatrix}\n$$\n\nNow we have all the elements to compute $\\boldsymbol{u}$. The specific observation is $\\boldsymbol{x} = [0.1, 0.1]^{\\top} = [\\frac{1}{10}, \\frac{1}{10}]^{\\top}$.\nThe centered vector $\\boldsymbol{y} = \\boldsymbol{x} - \\boldsymbol{\\mu}$ is:\n$$\n\\boldsymbol{y} = \\begin{bmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{bmatrix} - \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{10} - \\frac{5}{10} \\\\ \\frac{1}{10} - \\frac{5}{10} \\end{bmatrix} = \\begin{bmatrix} -\\frac{4}{10} \\\\ -\\frac{4}{10} \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{5} \\\\ -\\frac{2}{5} \\end{bmatrix}\n$$\nThe whitened vector $\\boldsymbol{z} = \\boldsymbol{P}\\boldsymbol{y}$ is:\n$$\n\\boldsymbol{z} = \\left(2\\sqrt{3} \\, \\boldsymbol{I}_{2}\\right) \\begin{bmatrix} -\\frac{2}{5} \\\\ -\\frac{2}{5} \\end{bmatrix} = 2\\sqrt{3} \\begin{bmatrix} -\\frac{2}{5} \\\\ -\\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{4\\sqrt{3}}{5} \\\\ -\\frac{4\\sqrt{3}}{5} \\end{bmatrix}\n$$\nFinally, we apply the rotation $\\boldsymbol{R}(\\frac{\\pi}{4})$ to obtain $\\boldsymbol{u} = \\boldsymbol{R}(\\frac{\\pi}{4})\\boldsymbol{z}$:\n$$\n\\boldsymbol{u} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\ 1 & 1\\end{bmatrix} \\begin{bmatrix} -\\frac{4\\sqrt{3}}{5} \\\\ -\\frac{4\\sqrt{3}}{5} \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} (1)(-\\frac{4\\sqrt{3}}{5}) + (-1)(-\\frac{4\\sqrt{3}}{5}) \\\\ (1)(-\\frac{4\\sqrt{3}}{5}) + (1)(-\\frac{4\\sqrt{3}}{5}) \\end{bmatrix}\n$$\n$$\n\\boldsymbol{u} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 0 \\\\ -\\frac{8\\sqrt{3}}{5} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -\\frac{8\\sqrt{3}}{5\\sqrt{2}} \\end{bmatrix}\n$$\nWe are asked for the second component, $u_{2}$:\n$$\nu_{2} = -\\frac{8\\sqrt{3}}{5\\sqrt{2}}\n$$\nTo simplify this expression, we rationalize the denominator:\n$$\nu_{2} = -\\frac{8\\sqrt{3}}{5\\sqrt{2}} \\cdot \\frac{\\sqrt{2}}{\\sqrt{2}} = -\\frac{8\\sqrt{6}}{5 \\cdot 2} = -\\frac{8\\sqrt{6}}{10} = -\\frac{4\\sqrt{6}}{5}\n$$\nThis result is negative, demonstrating that a standard ICA procedure does not inherently preserve the nonnegativity of the original sources, even in a simple case.", "answer": "$$\\boxed{-\\frac{4\\sqrt{6}}{5}}$$", "id": "2855504"}, {"introduction": "Once a BSS algorithm produces a set of estimated sources, the critical next step is to quantify the quality of the separation. This practice moves from theory to application by focusing on a key performance metric: the Interference-to-Signal Ratio (ISR). Starting from the overall system matrix $G = WA$, which links the true sources to the estimated ones, you will derive and compute the ISR matrix, whose off-diagonal entries provide a direct measure of the \"cross-talk\" or \"leakage\" from one source into the estimate of another [@problem_id:2855503]. This hands-on calculation is invaluable for developing a concrete understanding of how to evaluate and interpret the performance of any BSS algorithm in a real-world context.", "problem": "In a three-source instantaneous linear mixing model for blind source separation, let the observed vector be given by $x = A s$, where $s \\in \\mathbb{R}^{3}$ contains mutually independent, zero-mean, unit-variance sources, and $A \\in \\mathbb{R}^{3 \\times 3}$ is an invertible mixing matrix. An Independent Component Analysis (ICA) algorithm produces a demixing matrix $W \\in \\mathbb{R}^{3 \\times 3}$, yielding estimated sources $y = W x = W A s$. Define the overall gain matrix $G = W A$. Under perfect separation, $G$ would equal a product of a permutation and a diagonal scaling matrix. In practice, $G$ exhibits small off-diagonal entries that represent residual leakage (cross-talk) between components.\n\nSuppose that, after appropriate permutation and row-scaling alignment so that the desired component for each output corresponds to the diagonal element, the measured gain matrix is\n$$\nG \\;=\\; \\begin{pmatrix}\n0.98 & 0.07 & -0.02 \\\\\n-0.05 & 1.05 & 0.03 \\\\\n0.01 & -0.04 & 0.97\n\\end{pmatrix}.\n$$\nUsing only the foundational model $x = A s$, $y = W x$, the independence and unit-variance of the sources, and the definition that the interference-to-signal ratio (ISR) for entry $(i,j)$ compares the interference power contributed by source $j$ into output $i$ against the desired signal power of source $i$ at output $i$, derive an expression for the ISR matrix in terms of $G$ and compute it for the given $G$. Express your final ISR matrix in exact fractional form. Finally, briefly explain which off-diagonal entry indicates the largest leakage and what it means physically. The final numerical answer must be the ISR matrix only, with entries given as exact fractions and no units.", "solution": "The relationship between the estimated source vector $y$ and the original source vector $s$ is given by $y = G s$. The $i$-th component of the estimated source vector, $y_i$, is given by the linear combination:\n$$ y_i = \\sum_{j=1}^{3} G_{ij} s_j $$\nSince the problem states that the gain matrix $G$ is aligned such that the desired component for output $y_i$ is source $s_i$, we can separate the expression for $y_i$ into a signal component and an interference component:\n$$ y_i = \\underbrace{G_{ii} s_i}_{\\text{Signal}} + \\underbrace{\\sum_{j \\neq i} G_{ij} s_j}_{\\text{Interference}} $$\nThe power of a zero-mean random signal is defined as its variance. The sources $s_j$ are given to be zero-mean and have unit variance, meaning $E[s_j] = 0$ and $E[s_j^2] = 1$ for all $j \\in \\{1, 2, 3\\}$. Due to mutual independence, $E[s_i s_j] = 0$ for $i \\neq j$.\n\nThe \"desired signal power of source $i$ at output $i$\", which we denote as $P_{S,i}$, is the power of the signal term $G_{ii} s_i$:\n$$ P_{S,i} = E\\left[ (G_{ii} s_i)^2 \\right] = G_{ii}^2 E[s_i^2] = G_{ii}^2 (1) = G_{ii}^2 $$\nThe \"interference power contributed by source $j$ into output $i$\" (for $j \\neq i$), denoted $P_{I, i \\leftarrow j}$, is the power of the interference term $G_{ij} s_j$ originating from source $s_j$:\n$$ P_{I, i \\leftarrow j} = E\\left[ (G_{ij} s_j)^2 \\right] = G_{ij}^2 E[s_j^2] = G_{ij}^2 (1) = G_{ij}^2 $$\nAccording to the problem's definition, the interference-to-signal ratio for entry $(i, j)$, denoted $\\text{ISR}_{ij}$, is the ratio of these two powers:\n$$ \\text{ISR}_{ij} = \\frac{P_{I, i \\leftarrow j}}{P_{S,i}} = \\frac{G_{ij}^2}{G_{ii}^2} $$\nThis expression is valid for the off-diagonal elements, where $i \\neq j$. For the diagonal elements $(i,i)$, the concept of interference from source $s_i$ into the estimation of $s_i$ is not applicable. Therefore, there is no self-interference, and the diagonal elements of the ISR matrix are set to $0$. The ISR matrix, which we denote as $R$, thus has elements given by:\n$$ R_{ij} = \\begin{cases} \\frac{G_{ij}^2}{G_{ii}^2} & \\text{if } i \\neq j \\\\ 0 & \\text{if } i = j \\end{cases} $$\nWe now compute this matrix for the given $G$. The entries of $G$ are converted to exact fractions: $G_{11} = 0.98 = \\frac{49}{50}$, $G_{12} = 0.07 = \\frac{7}{100}$, $G_{13} = -0.02 = -\\frac{1}{50}$, $G_{21} = -0.05 = -\\frac{1}{20}$, $G_{22} = 1.05 = \\frac{21}{20}$, $G_{23} = 0.03 = \\frac{3}{100}$, $G_{31} = 0.01 = \\frac{1}{100}$, $G_{32} = -0.04 = -\\frac{1}{25}$, and $G_{33} = 0.97 = \\frac{97}{100}$.\n\nFirst, we compute the squares of the diagonal elements:\n$G_{11}^2 = \\left(\\frac{49}{50}\\right)^2 = \\frac{2401}{2500}$\n$G_{22}^2 = \\left(\\frac{21}{20}\\right)^2 = \\frac{441}{400}$\n$G_{33}^2 = \\left(\\frac{97}{100}\\right)^2 = \\frac{9409}{10000}$\n\nNext, we compute the required off-diagonal elements of the ISR matrix $R$:\n$R_{12} = \\frac{G_{12}^2}{G_{11}^2} = \\frac{(\\frac{7}{100})^2}{(\\frac{49}{50})^2} = \\frac{\\frac{49}{10000}}{\\frac{2401}{2500}} = \\frac{49}{10000} \\cdot \\frac{2500}{2401} = \\frac{49}{4 \\cdot 2401} = \\frac{49}{4 \\cdot 49^2} = \\frac{1}{4 \\cdot 49} = \\frac{1}{196}$\n$R_{13} = \\frac{G_{13}^2}{G_{11}^2} = \\frac{(-\\frac{1}{50})^2}{(\\frac{49}{50})^2} = \\frac{\\frac{1}{2500}}{\\frac{2401}{2500}} = \\frac{1}{2401}$\n$R_{21} = \\frac{G_{21}^2}{G_{22}^2} = \\frac{(-\\frac{1}{20})^2}{(\\frac{21}{20})^2} = \\frac{\\frac{1}{400}}{\\frac{441}{400}} = \\frac{1}{441}$\n$R_{23} = \\frac{G_{23}^2}{G_{22}^2} = \\frac{(\\frac{3}{100})^2}{(\\frac{21}{20})^2} = \\frac{\\frac{9}{10000}}{\\frac{441}{400}} = \\frac{9}{10000} \\cdot \\frac{400}{441} = \\frac{9}{25 \\cdot 441} = \\frac{9}{25 \\cdot 9 \\cdot 49} = \\frac{1}{25 \\cdot 49} = \\frac{1}{1225}$\n$R_{31} = \\frac{G_{31}^2}{G_{33}^2} = \\frac{(\\frac{1}{100})^2}{(\\frac{97}{100})^2} = \\frac{\\frac{1}{10000}}{\\frac{9409}{10000}} = \\frac{1}{9409}$\n$R_{32} = \\frac{G_{32}^2}{G_{33}^2} = \\frac{(-\\frac{1}{25})^2}{(\\frac{97}{100})^2} = \\frac{\\frac{1}{625}}{\\frac{9409}{10000}} = \\frac{1}{625} \\cdot \\frac{10000}{9409} = \\frac{16}{9409}$\n\nAssembling the ISR matrix $R$:\n$$ R \\;=\\; \\begin{pmatrix} 0 & \\frac{1}{196} & \\frac{1}{2401} \\\\ \\frac{1}{441} & 0 & \\frac{1}{1225} \\\\ \\frac{1}{9409} & \\frac{16}{9409} & 0 \\end{pmatrix} $$\nTo identify the largest leakage, we compare the magnitudes of the off-diagonal entries of $R$.\n$R_{12} = \\frac{1}{196} \\approx 0.0051$\n$R_{13} = \\frac{1}{2401} \\approx 0.0004$\n$R_{21} = \\frac{1}{441} \\approx 0.0023$\n$R_{23} = \\frac{1}{1225} \\approx 0.0008$\n$R_{31} = \\frac{1}{9409} \\approx 0.0001$\n$R_{32} = \\frac{16}{9409} \\approx 0.0017$\nThe largest value is $R_{12} = \\frac{1}{196}$. This term indicates the most significant residual interference, or cross-talk, that the separation algorithm failed to remove. Physically, it means that the leakage from source $s_2$ into the estimated output $y_1$ is the strongest among all possible cross-talk paths. The power of the interference component in $y_1$ due to source $s_2$ is $\\frac{1}{196}$ times the power of the desired signal component in $y_1$ from source $s_1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{1}{196} & \\frac{1}{2401} \\\\\n\\frac{1}{441} & 0 & \\frac{1}{1225} \\\\\n\\frac{1}{9409} & \\frac{16}{9409} & 0\n\\end{pmatrix}\n}\n$$", "id": "2855503"}]}