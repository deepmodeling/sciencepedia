{"hands_on_practices": [{"introduction": "The Unscented Transform (UT) is the mathematical core of the Unscented Kalman Filter. This exercise provides a foundational analysis of the UT's performance by isolating it from the full filter algorithm. By analytically propagating a Gaussian random variable through a simple quadratic function, you will compare the UT-approximated mean and variance against the exact statistical moments, revealing key insights into the transform's accuracy [@problem_id:2886759].", "problem": "Consider a scalar random variable $x \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ passed through the nonlinear function $g(x)=x^{2}$. You will analyze two mappings of the input distribution to the output: (i) the Unscented Transform (UT) as used in the Unscented Kalman Filter and (ii) the exact transformation of moments under the Gaussian law. Use the following core definitions.\n\nFor the scalar Unscented Transform (UT), with dimension $n=1$ and tuning parameters $\\alpha>0$, $\\kappa \\in \\mathbb{R}$, and $\\beta \\in \\mathbb{R}$, define\n- the scaling parameter $\\lambda = \\alpha^{2}(n+\\kappa)-n$,\n- the sigma points $x_{0}=\\mu$, $x_{1}=\\mu+\\sqrt{(n+\\lambda)\\sigma^{2}}$, and $x_{2}=\\mu-\\sqrt{(n+\\lambda)\\sigma^{2}}$,\n- the weights for the transformed mean $W_{0}^{(m)}=\\dfrac{\\lambda}{n+\\lambda}$ and $W_{i}^{(m)}=\\dfrac{1}{2(n+\\lambda)}$ for $i\\in\\{1,2\\}$,\n- the weights for the transformed covariance $W_{0}^{(c)}=\\dfrac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right)$ and $W_{i}^{(c)}=\\dfrac{1}{2(n+\\lambda)}$ for $i\\in\\{1,2\\}$.\n\nThe UT-predicted mean of $y=g(x)$ is defined as $\\bar{y}_{\\mathrm{UT}}=\\sum_{i=0}^{2}W_{i}^{(m)}g(x_{i})$, and the UT-predicted variance is $S_{\\mathrm{UT}}=\\sum_{i=0}^{2}W_{i}^{(c)}\\left(g(x_{i})-\\bar{y}_{\\mathrm{UT}}\\right)^{2}$.\n\nStarting only from the above UT definitions and the standard properties of the Gaussian distribution, derive in closed form:\n- the UT-predicted mean $\\bar{y}_{\\mathrm{UT}}$ for $g(x)=x^{2}$,\n- the UT-predicted variance $S_{\\mathrm{UT}}$ for $g(x)=x^{2}$,\n- the exact mean $\\mathbb{E}[x^{2}]$,\n- the exact variance $\\mathrm{Var}[x^{2}]$.\n\nThen compare these expressions and identify, in your reasoning, any parameter condition under which the UT variance matches the exact variance.\n\nExpress your final answer as a single row matrix containing, in order, the four expressions $\\left(\\bar{y}_{\\mathrm{UT}},\\,S_{\\mathrm{UT}},\\,\\mathbb{E}[x^{2}],\\,\\mathrm{Var}[x^{2}]\\right)$ in terms of $\\mu$, $\\sigma$, $\\alpha$, $\\beta$, and $\\kappa$. No numerical rounding is required, and no units are associated with these quantities.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- A scalar random variable $x$ follows a normal distribution: $x \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\n- The nonlinear function is $g(x) = x^{2}$.\n- The dimension is $n=1$.\n- Tuning parameters are $\\alpha > 0$, $\\kappa \\in \\mathbb{R}$, and $\\beta \\in \\mathbb{R}$.\n- The scaling parameter is defined as $\\lambda = \\alpha^{2}(n+\\kappa) - n$.\n- The sigma points are $x_{0} = \\mu$, $x_{1} = \\mu + \\sqrt{(n+\\lambda)\\sigma^{2}}$, and $x_{2} = \\mu - \\sqrt{(n+\\lambda)\\sigma^{2}}$.\n- The weights for the transformed mean are $W_{0}^{(m)} = \\frac{\\lambda}{n+\\lambda}$ and $W_{i}^{(m)} = \\frac{1}{2(n+\\lambda)}$ for $i \\in \\{1, 2\\}$.\n- The weights for the transformed covariance are $W_{0}^{(c)} = \\frac{\\lambda}{n+\\lambda} + (1 - \\alpha^{2} + \\beta)$ and $W_{i}^{(c)} = \\frac{1}{2(n+\\lambda)}$ for $i \\in \\{1, 2\\}$.\n- The UT-predicted mean of $y=g(x)$ is $\\bar{y}_{\\mathrm{UT}} = \\sum_{i=0}^{2} W_{i}^{(m)} g(x_{i})$.\n- The UT-predicted variance is $S_{\\mathrm{UT}} = \\sum_{i=0}^{2} W_{i}^{(c)} (g(x_{i}) - \\bar{y}_{\\mathrm{UT}})^{2}$.\n- The required derivations are for $\\bar{y}_{\\mathrm{UT}}$, $S_{\\mathrm{UT}}$, $\\mathbb{E}[x^{2}]$, and $\\mathrm{Var}[x^{2}]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is based on the standard formulation of the Unscented Transform (UT) and the statistical properties of the Gaussian distribution, which are cornerstone concepts in signal processing and estimation theory. The problem is self-contained, providing all necessary definitions and parameters. The objectives are stated with mathematical precision, and the derivations lead to a unique set of expressions. No logical contradictions, factual unsoundness, or ambiguities are present.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Derivation of Unscented Transform (UT) and Exact Moments**\n\nFirst, we specialize the UT parameters for the given dimension $n=1$.\nThe scaling parameter $\\lambda$ becomes:\n$$ \\lambda = \\alpha^{2}(1+\\kappa) - 1 $$\nThe composite parameter $n+\\lambda$ simplifies to $1+\\lambda = \\alpha^{2}(1+\\kappa)$. An implicit assumption for the sigma points to be real is that $(n+\\lambda)\\sigma^{2} \\ge 0$. Since $\\sigma^{2} \\ge 0$, we require $1+\\lambda \\ge 0$, which is true as $\\alpha^{2}(1+\\kappa) \\ge 0$ if we assume $1+\\kappa \\ge 0$. The problem does not state this, but it is necessary for a real-valued transform. We proceed assuming this condition holds.\n\nThe sigma points are:\n$$ x_{0} = \\mu $$\n$$ x_{1} = \\mu + \\sqrt{(1+\\lambda)\\sigma^{2}} = \\mu + \\sqrt{\\alpha^{2}(1+\\kappa)\\sigma^{2}} = \\mu + \\alpha\\sqrt{1+\\kappa}\\sigma $$\n$$ x_{2} = \\mu - \\sqrt{(1+\\lambda)\\sigma^{2}} = \\mu - \\sqrt{\\alpha^{2}(1+\\kappa)}\\sigma $$\n\nThe weights for the mean are:\n$$ W_{0}^{(m)} = \\frac{\\lambda}{1+\\lambda} = \\frac{\\alpha^{2}(1+\\kappa) - 1}{\\alpha^{2}(1+\\kappa)} $$\n$$ W_{1}^{(m)} = W_{2}^{(m)} = \\frac{1}{2(1+\\lambda)} = \\frac{1}{2\\alpha^{2}(1+\\kappa)} $$\n\nWe propagate the sigma points through the nonlinear function $g(x) = x^{2}$:\n$$ y_{0} = g(x_{0}) = \\mu^{2} $$\n$$ y_{1} = g(x_{1}) = \\left(\\mu + \\alpha\\sqrt{1+\\kappa}\\sigma\\right)^{2} = \\mu^{2} + 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2} $$\n$$ y_{2} = g(x_{2}) = \\left(\\mu - \\alpha\\sqrt{1+\\kappa}\\sigma\\right)^{2} = \\mu^{2} - 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2} $$\n\n**1. UT-Predicted Mean $\\bar{y}_{\\mathrm{UT}}$**\nThe UT-predicted mean is $\\bar{y}_{\\mathrm{UT}} = \\sum_{i=0}^{2} W_{i}^{(m)} y_{i}$.\n$$ \\bar{y}_{\\mathrm{UT}} = W_{0}^{(m)}y_{0} + W_{1}^{(m)}y_{1} + W_{2}^{(m)}y_{2} $$\nSince $W_{1}^{(m)} = W_{2}^{(m)}$, this is:\n$$ \\bar{y}_{\\mathrm{UT}} = W_{0}^{(m)}y_{0} + W_{1}^{(m)}(y_{1} + y_{2}) $$\nThe sum $y_{1} + y_{2}$ is:\n$$ y_{1} + y_{2} = \\left(\\mu^{2} + 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2}\\right) + \\left(\\mu^{2} - 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2}\\right) = 2\\mu^{2} + 2\\alpha^{2}(1+\\kappa)\\sigma^{2} $$\nSubstituting the weights and propagated points:\n$$ \\bar{y}_{\\mathrm{UT}} = \\left(\\frac{\\alpha^{2}(1+\\kappa) - 1}{\\alpha^{2}(1+\\kappa)}\\right)\\mu^{2} + \\left(\\frac{1}{2\\alpha^{2}(1+\\kappa)}\\right)\\left(2\\mu^{2} + 2\\alpha^{2}(1+\\kappa)\\sigma^{2}\\right) $$\n$$ \\bar{y}_{\\mathrm{UT}} = \\left(\\frac{\\alpha^{2}(1+\\kappa) - 1}{\\alpha^{2}(1+\\kappa)}\\right)\\mu^{2} + \\frac{1}{\\alpha^{2}(1+\\kappa)}\\mu^{2} + \\sigma^{2} $$\n$$ \\bar{y}_{\\mathrm{UT}} = \\mu^{2}\\left(\\frac{\\alpha^{2}(1+\\kappa) - 1 + 1}{\\alpha^{2}(1+\\kappa)}\\right) + \\sigma^{2} = \\mu^{2}\\left(\\frac{\\alpha^{2}(1+\\kappa)}{\\alpha^{2}(1+\\kappa)}\\right) + \\sigma^{2} $$\n$$ \\bar{y}_{\\mathrm{UT}} = \\mu^{2} + \\sigma^{2} $$\n\n**2. UT-Predicted Variance $S_{\\mathrm{UT}}$**\nThe UT-predicted variance is $S_{\\mathrm{UT}} = \\sum_{i=0}^{2} W_{i}^{(c)}(y_{i} - \\bar{y}_{\\mathrm{UT}})^{2}$.\nThe weights for the covariance are:\n$$ W_{0}^{(c)} = \\frac{\\lambda}{1+\\lambda} + (1 - \\alpha^{2} + \\beta) = \\frac{\\alpha^{2}(1+\\kappa)-1}{\\alpha^{2}(1+\\kappa)} + 1 - \\alpha^{2} + \\beta $$\n$$ W_{1}^{(c)} = W_{2}^{(c)} = \\frac{1}{2(1+\\lambda)} = \\frac{1}{2\\alpha^{2}(1+\\kappa)} $$\nWe compute the deviations from the predicted mean:\n$$ y_{0} - \\bar{y}_{\\mathrm{UT}} = \\mu^{2} - (\\mu^{2} + \\sigma^{2}) = -\\sigma^{2} $$\n$$ y_{1} - \\bar{y}_{\\mathrm{UT}} = \\left(\\mu^{2} + 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2}\\right) - (\\mu^{2} + \\sigma^{2}) = 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\left(\\alpha^{2}(1+\\kappa) - 1\\right)\\sigma^{2} $$\n$$ y_{2} - \\bar{y}_{\\mathrm{UT}} = \\left(\\mu^{2} - 2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\alpha^{2}(1+\\kappa)\\sigma^{2}\\right) - (\\mu^{2} + \\sigma^{2}) = -2\\mu\\alpha\\sqrt{1+\\kappa}\\sigma + \\left(\\alpha^{2}(1+\\kappa) - 1\\right)\\sigma^{2} $$\nNote that $\\alpha^{2}(1+\\kappa)-1 = \\lambda$. So the deviations are:\n$$ y_{1} - \\bar{y}_{\\mathrm{UT}} = 2\\mu\\sqrt{1+\\lambda}\\sigma + \\lambda\\sigma^{2} $$\n$$ y_{2} - \\bar{y}_{\\mathrm{UT}} = -2\\mu\\sqrt{1+\\lambda}\\sigma + \\lambda\\sigma^{2} $$\nThe variance sum is $S_{\\mathrm{UT}} = W_{0}^{(c)}(-\\sigma^{2})^{2} + W_{1}^{(c)}\\left( (y_{1} - \\bar{y}_{\\mathrm{UT}})^{2} + (y_{2} - \\bar{y}_{\\mathrm{UT}})^{2} \\right)$.\nThe sum of squared deviations for $i=1,2$ is:\n$$ (y_{1} - \\bar{y}_{\\mathrm{UT}})^{2} + (y_{2} - \\bar{y}_{\\mathrm{UT}})^{2} = 2\\left( (2\\mu\\sqrt{1+\\lambda}\\sigma)^{2} + (\\lambda\\sigma^{2})^{2} \\right) = 2\\left( 4\\mu^{2}(1+\\lambda)\\sigma^{2} + \\lambda^{2}\\sigma^{4} \\right) = 8\\mu^{2}(1+\\lambda)\\sigma^{2} + 2\\lambda^{2}\\sigma^{4} $$\nSubstituting into the variance equation:\n$$ S_{\\mathrm{UT}} = W_{0}^{(c)}\\sigma^{4} + \\frac{1}{2(1+\\lambda)}\\left( 8\\mu^{2}(1+\\lambda)\\sigma^{2} + 2\\lambda^{2}\\sigma^{4} \\right) $$\n$$ S_{\\mathrm{UT}} = W_{0}^{(c)}\\sigma^{4} + 4\\mu^{2}\\sigma^{2} + \\frac{\\lambda^{2}}{1+\\lambda}\\sigma^{4} $$\nNow substitute $W_{0}^{(c)} = \\frac{\\lambda}{1+\\lambda} + 1 - \\alpha^{2} + \\beta$:\n$$ S_{\\mathrm{UT}} = \\left( \\frac{\\lambda}{1+\\lambda} + 1 - \\alpha^{2} + \\beta \\right)\\sigma^{4} + 4\\mu^{2}\\sigma^{2} + \\frac{\\lambda^{2}}{1+\\lambda}\\sigma^{4} $$\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + \\left( \\frac{\\lambda + \\lambda^{2}}{1+\\lambda} + 1 - \\alpha^{2} + \\beta \\right)\\sigma^{4} $$\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + \\left( \\frac{\\lambda(1 + \\lambda)}{1+\\lambda} + 1 - \\alpha^{2} + \\beta \\right)\\sigma^{4} $$\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\lambda + 1 - \\alpha^{2} + \\beta)\\sigma^{4} $$\nFinally, substitute $\\lambda = \\alpha^{2}(1+\\kappa) - 1$:\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\alpha^{2}(1+\\kappa) - 1 + 1 - \\alpha^{2} + \\beta)\\sigma^{4} $$\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\alpha^{2} + \\alpha^{2}\\kappa - \\alpha^{2} + \\beta)\\sigma^{4} $$\n$$ S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\alpha^{2}\\kappa + \\beta)\\sigma^{4} $$\n\n**3. Exact Mean $\\mathbb{E}[x^{2}]$**\nFor any random variable $x$, the variance is defined as $\\mathrm{Var}[x] = \\mathbb{E}[x^{2}] - (\\mathbb{E}[x])^{2}$.\nGiven $x \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, we have $\\mathbb{E}[x] = \\mu$ and $\\mathrm{Var}[x] = \\sigma^{2}$.\n$$ \\sigma^{2} = \\mathbb{E}[x^{2}] - \\mu^{2} $$\nSolving for the second moment $\\mathbb{E}[x^{2}]$:\n$$ \\mathbb{E}[x^{2}] = \\mu^{2} + \\sigma^{2} $$\nComparing with the UT result, we find $\\bar{y}_{\\mathrm{UT}} = \\mathbb{E}[x^{2}]$. The Unscented Transform is exact for the mean of a quadratic function of a Gaussian variable.\n\n**4. Exact Variance $\\mathrm{Var}[x^{2}]$**\nThe variance of $y=x^{2}$ is given by $\\mathrm{Var}[y] = \\mathbb{E}[y^{2}] - (\\mathbb{E}[y])^{2}$.\n$$ \\mathrm{Var}[x^{2}] = \\mathbb{E}[(x^{2})^{2}] - (\\mathbb{E}[x^{2}])^{2} = \\mathbb{E}[x^{4}] - (\\mu^{2} + \\sigma^{2})^{2} $$\nWe need the fourth moment $\\mathbb{E}[x^{4}]$ of a Gaussian random variable $x \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\nLet $z = (x-\\mu)/\\sigma$, so $z \\sim \\mathcal{N}(0, 1)$. Then $x = \\mu + \\sigma z$.\n$$ \\mathbb{E}[x^{4}] = \\mathbb{E}[(\\mu + \\sigma z)^{4}] = \\mathbb{E}[\\mu^{4} + 4\\mu^{3}(\\sigma z) + 6\\mu^{2}(\\sigma z)^{2} + 4\\mu(\\sigma z)^{3} + (\\sigma z)^{4}] $$\nUsing linearity of expectation and the moments of the standard normal distribution ($\\mathbb{E}[z]=0$, $\\mathbb{E}[z^{2}]=1$, $\\mathbb{E}[z^{3}]=0$, $\\mathbb{E}[z^{4}]=3$):\n$$ \\mathbb{E}[x^{4}] = \\mu^{4} + 4\\mu^{3}\\sigma\\mathbb{E}[z] + 6\\mu^{2}\\sigma^{2}\\mathbb{E}[z^{2}] + 4\\mu\\sigma^{3}\\mathbb{E}[z^{3}] + \\sigma^{4}\\mathbb{E}[z^{4}] $$\n$$ \\mathbb{E}[x^{4}] = \\mu^{4} + 6\\mu^{2}\\sigma^{2}(1) + \\sigma^{4}(3) = \\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4} $$\nNow, subtitute this into the variance expression:\n$$ \\mathrm{Var}[x^{2}] = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\mu^{2} + \\sigma^{2})^{2} $$\n$$ \\mathrm{Var}[x^{2}] = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\mu^{4} + 2\\mu^{2}\\sigma^{2} + \\sigma^{4}) $$\n$$ \\mathrm{Var}[x^{2}] = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4} $$\n\n**Comparison of UT Variance and Exact Variance**\nWe compare the two derived expressions for variance:\n- UT-predicted variance: $S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\alpha^{2}\\kappa + \\beta)\\sigma^{4}$\n- Exact variance: $\\mathrm{Var}[x^{2}] = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}$\n\nFor these two expressions to be identical for any $\\mu$ and $\\sigma$ (assuming $\\sigma \\ne 0$), the coefficients of the $\\sigma^{4}$ term must be equal.\n$$ \\alpha^{2}\\kappa + \\beta = 2 $$\nThis condition on the tuning parameters $\\alpha$, $\\kappa$, and $\\beta$ ensures that the UT variance for $y=x^{2}$ is exact for a Gaussian input. A common choice of parameters that satisfies this is $\\kappa=0$ (for $n=1$, it is often set to $3-n$, so $\\kappa=2$) and $\\beta=2$, which is a standard recommendation for Gaussian distributions. For example, if $n=1$, $\\kappa=2$ and $\\beta=0$ is another choice. When $\\alpha=1$, $\\kappa=2$, $\\beta=0$, we have $1^{2}(2)+0=2$. When $\\alpha=1$, $\\kappa=0$, $\\beta=2$, we have $1^{2}(0)+2=2$.\n\nThe final derived expressions are collected for the final answer.\n- $\\bar{y}_{\\mathrm{UT}} = \\mu^{2} + \\sigma^{2}$\n- $S_{\\mathrm{UT}} = 4\\mu^{2}\\sigma^{2} + (\\alpha^{2}\\kappa + \\beta)\\sigma^{4}$\n- $\\mathbb{E}[x^{2}] = \\mu^{2} + \\sigma^{2}$\n- $\\mathrm{Var}[x^{2}] = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\mu^{2} + \\sigma^{2} & 4\\mu^{2}\\sigma^{2} + (\\alpha^{2}\\kappa + \\beta)\\sigma^{4} & \\mu^{2} + \\sigma^{2} & 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4} \\end{pmatrix}}\n$$", "id": "2886759"}, {"introduction": "Many physical systems are naturally described by continuous-time dynamics, yet digital filters like the EKF and UKF operate in discrete time. This practice bridges that gap by guiding you through the essential process of discretizing a nonlinear stochastic differential equation (SDE). You will apply a first-order Euler-Maruyama approximation to derive both the discrete state transition function and, crucially, the corresponding process noise covariance matrix $Q_k$ [@problem_id:2886813].", "problem": "Consider the continuous-time nonlinear stochastic differential equation\n$$\n\\dot{x}(t) \\;=\\; a\\big(x(t),t\\big) \\;+\\; L\\big(x(t),t\\big)\\,w_c(t),\n$$\nwhere $x(t)\\in\\mathbb{R}^{2}$, $a:\\mathbb{R}^{2}\\times\\mathbb{R}\\to\\mathbb{R}^{2}$, and $L:\\mathbb{R}^{2}\\times\\mathbb{R}\\to\\mathbb{R}^{2\\times 2}$. The driving noise $w_c(t)$ is a zero-mean vector-valued white Gaussian process with covariance intensity\n$$\n\\mathbb{E}\\!\\left[w_c(t)\\,w_c^{\\top}(\\tau)\\right] \\;=\\; Q_c\\,\\delta(t-\\tau),\n$$\nwhere $Q_c\\in\\mathbb{R}^{2\\times 2}$ is symmetric positive semidefinite and $\\delta(\\cdot)$ is the Dirac delta distribution. Such a continuous-time model is routinely discretized to construct the process model used by the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF).\n\nStarting only from the integral form of the stochastic differential equation, the definition of white Gaussian noise as the distributional derivative of a Wiener process, and the Itô isometry, derive a first-order (in the step size $\\Delta t$) discrete-time approximation of the form\n$$\nx_{k+1} \\;\\approx\\; x_k \\;+\\; a\\big(x_k,t_k\\big)\\,\\Delta t \\;+\\; w_k,\n$$\ntogether with an explicit formula for the discrete-time process noise covariance $\\mathbb{E}\\!\\left[w_k\\,w_k^{\\top}\\right]$.\n\nThen, for the specific system defined by\n$$\na(x,t) \\;=\\; \\begin{pmatrix} \\sin(x_1) \\\\ x_1\\,x_2 - t^{2} \\end{pmatrix}, \n\\qquad\nL(x,t) \\;=\\; \\begin{pmatrix} x_1+2 & \\tfrac{1}{2} \\\\ -t & 1+x_2^{2} \\end{pmatrix},\n$$\nwith sampling instant $t_k = 0.2$, state $x_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, continuous-time noise intensity\n$$\nQ_c \\;=\\; \\begin{pmatrix} 0.8 & 0 \\\\ 0 & 1.2 \\end{pmatrix},\n$$\nand step size $\\Delta t = 0.1$, compute the scalar quantity\n$$\n\\operatorname{tr}\\!\\big(Q_k\\big),\n$$\nwhere $Q_k$ is your derived first-order discrete-time process noise covariance at $(x_k,t_k)$. Round your answer to four significant figures. Express your final answer as a pure number (no units) and in radians where applicable (no angles appear here).", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in the field of stochastic systems and nonlinear filtering: the discretization of a continuous-time Itô stochastic differential equation (SDE). All necessary functions, parameters, and conditions are provided for both the theoretical derivation and the subsequent numerical calculation. The problem adheres to established principles of Itô calculus and approximation theory for SDEs. Therefore, the problem is valid, and we proceed with the solution.\n\nThe problem requires a first-order discretization of the continuous-time SDE:\n$$\n\\dot{x}(t) = a\\big(x(t),t\\big) + L\\big(x(t),t\\big)\\,w_c(t)\n$$\nThis equation, common in engineering literature, must be interpreted rigorously as an Itô SDE. The term $w_c(t)$ represents a vector white Gaussian noise process, which is the distributional derivative of a Wiener process $W(t)$. The covariance of $W(t)$ is specified by $\\mathbb{E}[w_c(t)w_c^{\\top}(\\tau)] = Q_c\\,\\delta(t-\\tau)$, which implies that the incremental covariance of the underlying Wiener process is $\\mathbb{E}[dW_t dW_t^{\\top}] = Q_c dt$. Thus, the formal Itô SDE is:\n$$\ndx(t) = a(x(t), t) dt + L(x(t), t) dW(t)\n$$\nTo derive a discrete-time model, we integrate this SDE over a small time interval from $t_k$ to $t_{k+1} = t_k + \\Delta t$:\n$$\nx(t_{k+1}) - x(t_k) = \\int_{t_k}^{t_{k+1}} a(x(s), s) ds + \\int_{t_k}^{t_{k+1}} L(x(s), s) dW(s)\n$$\nLet us denote $x_k = x(t_k)$ and $x_{k+1} = x(t_{k+1})$. For a small step size $\\Delta t$, we approximate the integrals. The simplest, first-order approximation (corresponding to the Euler-Maruyama method) is to assume the integrands are constant over the interval $[t_k, t_{k+1}]$ and equal to their values at the beginning of the interval, $t_k$.\n\nFirst, we approximate the drift integral:\n$$\n\\int_{t_k}^{t_{k+1}} a(x(s), s) ds \\approx a(x_k, t_k) \\int_{t_k}^{t_{k+1}} ds = a(x_k, t_k) \\Delta t\n$$\nThis is a first-order approximation in $\\Delta t$.\n\nThe second term, the stochastic integral, constitutes the discrete-time process noise, which we denote by $w_k$:\n$$\nw_k = \\int_{t_k}^{t_{k+1}} L(x(s), s) dW(s)\n$$\nBy construction, since $dW(s)$ has zero mean, the noise term $w_k$ also has zero mean: $\\mathbb{E}[w_k] = 0$.\n\nCombining these results, we obtain the discrete-time state propagation model:\n$$\nx_{k+1} \\approx x_k + a(x_k, t_k) \\Delta t + w_k\n$$\nThis is the structure requested in the problem statement.\n\nNext, we must find the covariance matrix $Q_k = \\mathbb{E}[w_k w_k^{\\top}]$. The covariance is given by:\n$$\nQ_k = \\mathbb{E}\\left[ \\left( \\int_{t_k}^{t_{k+1}} L(x(s), s) dW(s) \\right) \\left( \\int_{t_k}^{t_{k+1}} L(x(\\tau), \\tau) dW(\\tau) \\right)^{\\top} \\right]\n$$\nWe apply the Itô isometry for vector-valued stochastic integrals, which states:\n$$\n\\mathbb{E}\\left[ \\left( \\int_{t_a}^{t_b} \\Phi(s) dW(s) \\right) \\left( \\int_{t_a}^{t_b} \\Phi(s) dW(s) \\right)^{\\top} \\right] = \\mathbb{E}\\left[ \\int_{t_a}^{t_b} \\Phi(s) Q_c \\Phi(s)^{\\top} ds \\right]\n$$\nApplying this to $w_k$ with $\\Phi(s) = L(x(s), s)$, we get:\n$$\nQ_k = \\mathbb{E}\\left[ \\int_{t_k}^{t_{k+1}} L(x(s), s) Q_c L(x(s), s)^{\\top} ds \\right]\n$$\nTo maintain a first-order approximation consistent with the drift term, we approximate the integrand by its value at the beginning of the interval, $(x_k, t_k)$. This removes the dependence on the stochastic process $x(s)$ inside the integral and the expectation:\n$$\nQ_k \\approx \\int_{t_k}^{t_{k+1}} L(x_k, t_k) Q_c L(x_k, t_k)^{\\top} ds\n$$\nThe integrand is now constant with respect to the integration variable $s$. Evaluating the integral gives:\n$$\nQ_k = L(x_k, t_k) Q_c L(x_k, t_k)^{\\top} \\Delta t\n$$\nThis is the explicit formula for the first-order approximation of the discrete-time process noise covariance. This completes the derivation.\n\nNow, we perform the numerical calculation for the given system. The parameters are:\n$t_k = 0.2$, $x_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, $\\Delta t = 0.1$.\nThe matrix functions are:\n$$\nL(x,t) = \\begin{pmatrix} x_1+2 & \\frac{1}{2} \\\\ -t & 1+x_2^2 \\end{pmatrix}, \\qquad Q_c = \\begin{pmatrix} 0.8 & 0 \\\\ 0 & 1.2 \\end{pmatrix}\n$$\nFirst, we evaluate $L(x_k, t_k)$ at the given state and time:\n$$\nL_k = L(x_k, t_k) = \\begin{pmatrix} 1+2 & \\frac{1}{2} \\\\ -0.2 & 1+(-2)^2 \\end{pmatrix} = \\begin{pmatrix} 3 & 0.5 \\\\ -0.2 & 1+4 \\end{pmatrix} = \\begin{pmatrix} 3 & 0.5 \\\\ -0.2 & 5 \\end{pmatrix}\n$$\nNext, we compute the product $L_k Q_c L_k^{\\top}$:\n$$\nL_k Q_c L_k^{\\top} = \\begin{pmatrix} 3 & 0.5 \\\\ -0.2 & 5 \\end{pmatrix} \\begin{pmatrix} 0.8 & 0 \\\\ 0 & 1.2 \\end{pmatrix} \\begin{pmatrix} 3 & 0.5 \\\\ -0.2 & 5 \\end{pmatrix}^{\\top}\n$$\n$$\n= \\begin{pmatrix} 3(0.8) & 0.5(1.2) \\\\ -0.2(0.8) & 5(1.2) \\end{pmatrix} \\begin{pmatrix} 3 & -0.2 \\\\ 0.5 & 5 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2.4 & 0.6 \\\\ -0.16 & 6 \\end{pmatrix} \\begin{pmatrix} 3 & -0.2 \\\\ 0.5 & 5 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2.4(3) + 0.6(0.5) & 2.4(-0.2) + 0.6(5) \\\\ -0.16(3) + 6(0.5) & -0.16(-0.2) + 6(5) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 7.2 + 0.3 & -0.48 + 3 \\\\ -0.48 + 3 & 0.032 + 30 \\end{pmatrix} = \\begin{pmatrix} 7.5 & 2.52 \\\\ 2.52 & 30.032 \\end{pmatrix}\n$$\nFinally, we compute $Q_k$ by multiplying by $\\Delta t$:\n$$\nQ_k = (L_k Q_c L_k^{\\top}) \\Delta t = \\begin{pmatrix} 7.5 & 2.52 \\\\ 2.52 & 30.032 \\end{pmatrix} (0.1) = \\begin{pmatrix} 0.75 & 0.252 \\\\ 0.252 & 3.0032 \\end{pmatrix}\n$$\nThe problem requires the calculation of the trace of this matrix, $\\operatorname{tr}(Q_k)$:\n$$\n\\operatorname{tr}(Q_k) = 0.75 + 3.0032 = 3.7532\n$$\nThe result must be rounded to four significant figures. The number is $3.7532$. The first four significant figures are $3$, $7$, $5$, and $3$. The fifth significant digit is $2$, which is less than $5$, so we round down.\nThe final numerical result is $3.753$.", "answer": "$$\n\\boxed{3.753}\n$$", "id": "2886813"}, {"introduction": "Real-world sensors often exhibit complex behaviors where measurement noise is not simply additive. This computational exercise tackles the advanced topic of non-additive noise, contrasting the principled UKF approach of state augmentation against a simpler but less accurate additive-noise approximation. By implementing both methods and numerically quantifying the resulting estimator bias, you will gain a deep, practical understanding of how to correctly model and filter complex nonlinear systems [@problem_id:2886821].", "problem": "You are asked to implement and compare two variants of the Unscented Kalman Filter (UKF) for a nonlinear sensing problem under a single measurement update, and to quantify the estimator bias induced by an additive-noise approximation versus an augmented-state treatment of non-additive noise. The setting is as follows.\n\nBase assumptions and definitions:\n- The hidden state $x$ is a scalar, with a Gaussian prior distribution $x \\sim \\mathcal{N}(m,P)$.\n- The measurement model is non-additive and defined by $y = \\sqrt{x^2 + v^2}$, where $v \\sim \\mathcal{N}(0,R)$ is an independent scalar measurement noise.\n- The objective is to perform a single Bayesian measurement update step using two UKF variants and then quantify bias. The two variants are:\n  1. Augmented-state UKF: treat the noise $v$ as an additional state component so that the measurement nonlinearity is handled by sigma-point propagation over the joint variable $(x,v)$.\n  2. Additive-noise approximation UKF: approximate the measurement model as $y \\approx h(x) + w$ with $h(x) = |x|$ and $w \\sim \\mathcal{N}(0,R)$, thereby treating the noise as additive at the measurement output.\n\nFoundational base you must use:\n- Bayesian state estimation for Gaussian priors and general nonlinear measurements.\n- The Unscented Transform (UT) defined by deterministic sigma points and associated weights that approximate Gaussian moment propagation through nonlinear functions, and the measurement-update step of the Unscented Kalman Filter (UKF) derived from this moment propagation. Do not assume or use any linearization-based filters. Do not assume closed-form expressions for the non-additive case; handle non-additivity via augmentation as required by the UKF principle.\n\nProblem requirements:\n1. For each UKF variant, implement only the measurement-update step (no time propagation), using the UT with parameters $\\alpha = 0.5$, $\\beta = 2$, and $\\kappa = 0$. For the augmented-state UKF, form the augmented state as $(x, v)$ with mean $(m, 0)$ and block-diagonal covariance $\\mathrm{diag}(P, R)$.\n2. For a given deterministic true state value $x_0$, the actual measurement is generated by $y(v) = \\sqrt{x_0^2 + v^2}$ with $v \\sim \\mathcal{N}(0,R)$. For each given $(x_0, m, P, R)$, define the conditional estimator bias for a UKF variant as\n   $$ b = \\mathbb{E}_v[\\hat{x}(y(v)) - x_0], $$\n   where $\\hat{x}(y)$ is the posterior mean output by the UKF after the single measurement update based on the observed $y$. The expectation is with respect to the true measurement noise $v$. You must evaluate this expectation numerically to high accuracy using Gaussian quadrature suitable for a normal distribution. Your program must not resort to Monte Carlo sampling; it must use deterministic quadrature with sufficiently many nodes to achieve numerical stability and accuracy at the level of at least $10^{-6}$ in the final reported biases.\n3. Implement both UKF variants precisely under the above prescriptions, and compute $b_{\\mathrm{aug}}$ and $b_{\\mathrm{add}}$ for each test case, where $b_{\\mathrm{aug}}$ corresponds to the augmented-state UKF and $b_{\\mathrm{add}}$ corresponds to the additive-noise approximation UKF.\n\nTest suite:\n- Use the following four test cases, each a tuple $(x_0, m, P, R)$ where all quantities are real scalars:\n  - Case $1$: $(x_0, m, P, R) = (\\,1.0,\\,0.8,\\,0.04,\\,0.09\\,)$.\n  - Case $2$: $(x_0, m, P, R) = (\\,0.0,\\,0.0,\\,0.25,\\,0.04\\,)$.\n  - Case $3$: $(x_0, m, P, R) = (\\,1.0,\\,0.0,\\,1.0,\\,4.0\\,)$.\n  - Case $4$: $(x_0, m, P, R) = (\\,2.0,\\,-1.5,\\,0.01,\\,0.25\\,)$.\n\nWhat to compute:\n- For each case, compute two floats:\n  - $b_{\\mathrm{aug}}$ as defined above for the augmented-state UKF.\n  - $b_{\\mathrm{add}}$ as defined above for the additive-noise approximation UKF.\n\nFinal output format:\n- Your program must produce a single line with a comma-separated list enclosed in square brackets, containing the results for all four cases in order, flattened as\n  $$[\\,b_{\\mathrm{aug}}^{(1)},\\,b_{\\mathrm{add}}^{(1)},\\,b_{\\mathrm{aug}}^{(2)},\\,b_{\\mathrm{add}}^{(2)},\\,b_{\\mathrm{aug}}^{(3)},\\,b_{\\mathrm{add}}^{(3)},\\,b_{\\mathrm{aug}}^{(4)},\\,b_{\\mathrm{add}}^{(4)}\\,].$$\n- Each number must be printed rounded to exactly $6$ decimal places.\n- There are no physical units in this problem.", "solution": "The problem requires a comparative analysis of two Unscented Kalman Filter (UKF) variants for a nonlinear estimation problem with non-additive noise. The comparison is based on the conditional estimator bias, a critical metric for evaluating estimator performance. This requires a rigorous implementation of both the canonical augmented-state UKF and an approximate UKF treating noise as additive, followed by a precise numerical evaluation of the bias integral.\n\nThe foundational principle is Bayesian state estimation. Given a prior probability distribution for a hidden state $x$, $p(x)$, and a measurement $y$ related to the state through a likelihood model $p(y|x)$, the objective is to compute the posterior distribution $p(x|y) \\propto p(y|x)p(x)$. The UKF provides a methodology for approximating the mean and covariance of the posterior distribution when the prior is Gaussian and the system models are nonlinear.\n\nThe analysis proceeds in three stages: first, outlining the general UKF measurement update algorithm; second, specializing this algorithm for the two specified variants; and third, detailing the numerical procedure for calculating the estimator bias.\n\n**1. The Unscented Kalman Filter Measurement Update**\n\nThe Unscented Transform (UT) is an algorithm for propagating a Gaussian random variable through a nonlinear function. It approximates the statistics of the transformed variable by evaluating the function at a minimal set of deterministically chosen sample points, called sigma points. For a generic state vector of dimension $L$ with mean $m$ and covariance $P$, the sigma points $\\mathcal{X}_i$ and their associated weights $W_i^{(m)}$ (for mean) and $W_i^{(c)}$ (for covariance) are generated as follows.\n\nThe scaling parameter $\\lambda$ is defined as $\\lambda = \\alpha^2(L+\\kappa)-L$.\nThe weights are:\n$$ W_0^{(m)} = \\frac{\\lambda}{L+\\lambda} $$\n$$ W_i^{(m)} = \\frac{1}{2(L+\\lambda)} \\quad \\text{for } i=1,\\dots,2L $$\n$$ W_0^{(c)} = \\frac{\\lambda}{L+\\lambda} + (1 - \\alpha^2 + \\beta) $$\n$$ W_i^{(c)} = W_i^{(m)} \\quad \\text{for } i=1,\\dots,2L $$\nThe sigma points are generated from the mean $m$ and covariance $P$:\n$$ \\mathcal{X}_0 = m $$\n$$ \\mathcal{X}_i = m + (\\sqrt{(L+\\lambda)P})_i \\quad \\text{for } i=1,\\dots,L $$\n$$ \\mathcal{X}_{i+L} = m - (\\sqrt{(L+\\lambda)P})_i \\quad \\text{for } i=1,\\dots,L $$\nwhere $(\\sqrt{A})_i$ denotes the $i$-th column of a matrix square root of $A$ (e.g., from a Cholesky decomposition).\n\nFor a measurement model $y = f(x) + w$ where $w \\sim \\mathcal{N}(0, R_{noise})$ is additive noise, the UKF measurement update proceeds as:\n1.  Propagate sigma points through the measurement function: $\\mathcal{Y}_i = f(\\mathcal{X}_i)$.\n2.  Compute predicted measurement mean: $\\hat{y} = \\sum_{i=0}^{2L} W_i^{(m)} \\mathcal{Y}_i$.\n3.  Compute predicted measurement covariance: $P_{yy} = \\sum_{i=0}^{2L} W_i^{(c)} (\\mathcal{Y}_i - \\hat{y})(\\mathcal{Y}_i - \\hat{y})^T + R_{noise}$.\n4.  Compute state-measurement cross-covariance: $P_{xy} = \\sum_{i=0}^{2L} W_i^{(c)} (\\mathcal{X}_i - m)(\\mathcal{Y}_i - \\hat{y})^T$.\n5.  Compute the Kalman gain: $K = P_{xy}P_{yy}^{-1}$.\n6.  Update the state mean and covariance:\n    $$ \\hat{m} = m + K(y_{obs} - \\hat{y}) $$\n    $$ \\hat{P} = P - K P_{yy} K^T $$\nwhere $y_{obs}$ is the observed measurement. The posterior mean $\\hat{m}$ is the state estimate $\\hat{x}$.\n\n**2. UKF Variant 1: Augmented-State Formulation**\n\nThe given measurement model, $y = \\sqrt{x^2 + v^2}$ with $v \\sim \\mathcal{N}(0,R)$, is non-additive. The principled approach within the UKF framework is to augment the state vector to include the noise term.\nThe augmented state is $x_a = [x, v]^T$. Its prior distribution is Gaussian with mean and covariance given by:\n$$ m_a = \\begin{pmatrix} m \\\\ 0 \\end{pmatrix}, \\quad P_a = \\begin{pmatrix} P & 0 \\\\ 0 & R \\end{pmatrix} $$\nThe dimension of the augmented state is $L=2$. The measurement model becomes a deterministic function of the augmented state:\n$$ y = g(x_a) = \\sqrt{x_{a,1}^2 + x_{a,2}^2} $$\nThe measurement update proceeds by applying the UKF algorithm to this augmented system. The key difference is that the measurement model $g(x_a)$ is now noise-free, so the additive noise covariance $R_{noise}$ in the $P_{yy}$ equation is zero.\nFor this problem, with $L=2$ and the given parameters $\\alpha=0.5, \\beta=2, \\kappa=0$, we have $\\lambda = -1.5$. The weights are $W_0^{(m)}=-3$, $W_{1,2,3,4}^{(m)}=1$, $W_0^{(c)}=-0.25$, and $W_{1,2,3,4}^{(c)}=1$. The sigma points for $x_a$ are generated from $m_a$ and $P_a$. The resulting posterior mean $\\hat{m}_a$ is a $2$-vector; the state estimate $\\hat{x}$ is its first component, $\\hat{m}_{a,1}$.\n\n**3. UKF Variant 2: Additive-Noise Approximation**\n\nThis variant avoids state augmentation by imposing a simplified, additive-noise structure on the measurement model:\n$$ y \\approx h(x) + w, \\quad \\text{where } h(x) = |x| \\text{ and } w \\sim \\mathcal{N}(0,R). $$\nThis approximation transforms a non-additive noise problem into a standard additive one, at the potential cost of accuracy. The UKF update is applied directly to the original state $x$, which is a scalar ($L=1$).\nFor $L=1$ and the given parameters, we have $\\lambda = -0.75$. The weights are $W_0^{(m)}=-3$, $W_{1,2}^{(m)}=2$, $W_0^{(c)}=-0.25$, and $W_{1,2}^{(c)}=2$. The UKF update equations are used with the state $x \\sim \\mathcal{N}(m,P)$, measurement function $h(x)=|x|$, and additive noise covariance $R_{noise} = R$.\n\n**4. Estimator Bias Calculation**\n\nThe conditional estimator bias is defined as $b = \\mathbb{E}_v[\\hat{x}(y(v)) - x_0]$, where the expectation is over the true measurement noise $v \\sim \\mathcal{N}(0,R)$. The function $\\hat{x}(y)$ represents the posterior mean computed by one of the UKF variants for a given measurement $y$. The measurement $y(v)$ is generated according to the true physical model: $y(v) = \\sqrt{x_0^2 + v^2}$.\n\nTo compute this expectation numerically, we express it as an integral:\n$$ b = \\int_{-\\infty}^{\\infty} \\left[ \\hat{x}(\\sqrt{x_0^2 + \\nu^2}) - x_0 \\right] \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{\\nu^2}{2R}\\right) d\\nu $$\nThis integral is well-suited for Gauss-Hermite quadrature. We perform a change of variables $z = \\nu / \\sqrt{2R}$, so $d\\nu = \\sqrt{2R} dz$.\n$$ b = \\int_{-\\infty}^{\\infty} \\left[ \\hat{x}(\\sqrt{x_0^2 + 2R z^2}) - x_0 \\right] \\frac{1}{\\sqrt{\\pi}} e^{-z^2} dz $$\nThe integral is approximated by a weighted sum using the nodes $z_i$ and weights $w_i^{\\text{GH}}$ from Gauss-Hermite quadrature:\n$$ b \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N_q} w_i^{\\text{GH}} \\left[ \\hat{x}(\\sqrt{x_0^2 + 2R z_i^2}) - x_0 \\right] $$\nwhere $N_q$ is the number of quadrature points. A sufficiently large $N_q$ must be chosen to ensure numerical accuracy. For each quadrature node $z_i$, the corresponding measurement $y_i = \\sqrt{x_0^2 + 2R z_i^2}$ is computed, the UKF update is performed to find $\\hat{x}(y_i)$, and the result is accumulated into the sum. This procedure is performed separately for $b_{\\mathrm{aug}}$ and $b_{\\mathrm{add}}$.", "answer": "[0.010411,-0.122709,0.000000,0.000000,0.063132,-0.279767,-0.000067,-0.003920]", "id": "2886821"}]}