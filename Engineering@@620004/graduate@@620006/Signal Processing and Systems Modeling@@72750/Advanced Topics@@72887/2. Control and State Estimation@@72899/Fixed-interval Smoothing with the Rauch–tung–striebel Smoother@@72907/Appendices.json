{"hands_on_practices": [{"introduction": "This foundational exercise grounds your understanding of the Rauch-Tung-Striebel smoother by starting from the beginning. You will first derive the scalar RTS recursions directly from the properties of Gaussian distributions, reinforcing the theoretical underpinnings of the algorithm. You will then apply these derived formulas to a concrete numerical example, bridging the gap between abstract theory and practical computation. [@problem_id:2872818]", "problem": "Consider the scalar linear Gaussian state-space model defined by the random walk dynamics and direct observations: the state evolves as $x_{k+1} = x_k + w_k$ and the observation is $y_k = x_k + v_k$, where $\\{w_k\\}$ and $\\{v_k\\}$ are mutually independent, zero-mean Gaussian sequences with variances $\\operatorname{Var}(w_k) = q$ and $\\operatorname{Var}(v_k) = r$, respectively. Assume the initial state prior is $x_0 \\sim \\mathcal{N}(m_0, p_0)$. Throughout, all random variables are real-valued.\n\nTasks:\n1. Using only the properties of multivariate Gaussian distributions and conditional expectations, derive closed-form expressions for the scalar Rauch–Tung–Striebel (RTS) smoothing gain $J_k$, the fixed-interval smoothed mean $x_{k|N}$, and the fixed-interval smoothed covariance $P_{k|N}$ for horizon $N=2$. Your derivation must start from the Gaussian conditioning identities for linear transformations of jointly Gaussian variables and may use the fact that the one-step ahead prediction and filtering distributions of the Kalman filter are Gaussian for linear-Gaussian systems.\n2. For the specific numerical case with $q = 0.5$, $r = 2.0$, $m_0 = 0$, $p_0 = 1$, and observations $y_0 = 1.0$, $y_1 = 0.0$, and $y_2 = 2.0$, perform the forward (filtering) pass to obtain the filtering means $x_{k|k}$ and covariances $P_{k|k}$ for $k \\in \\{0,1,2\\}$, and the one-step predictions $x_{k+1|k}$ and $P_{k+1|k}$ for $k \\in \\{0,1\\}$. Then compute the RTS smoothing gains $J_0$ and $J_1$, and carry out the backward (smoothing) pass to obtain $x_{0|2}$, $x_{1|2}$, $x_{2|2}$ and $P_{0|2}$, $P_{1|2}$, $P_{2|2}$.\n3. Report the final numerical values in the order $(J_0, J_1, x_{0|2}, x_{1|2}, x_{2|2}, P_{0|2}, P_{1|2}, P_{2|2})$ as a single row matrix. Round your entries to four significant figures. No units are required.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It describes a standard application of the Rauch–Tung–Striebel (RTS) smoother to a linear Gaussian state-space model. All necessary parameters and observations are provided. Therefore, the problem is valid and a solution will be provided.\n\nThe problem is addressed in three parts: first, the derivation of the scalar RTS smoother equations; second, the numerical computation involving a forward Kalman filter pass and a backward RTS smoother pass; and third, the reporting of the final numerical values.\n\n**Part 1: Derivation of the Rauch–Tung–Striebel (RTS) Smoother Equations**\n\nThe RTS smoother is a fixed-interval smoothing algorithm that operates by a backward recursion, starting from the final state of a forward Kalman filter pass. The objective is to compute the smoothed state distribution $p(x_k | y_{0:N})$ for $k=0, 1, \\dots, N-1$. This distribution is Gaussian, characterized by its mean $x_{k|N}$ and covariance $P_{k|N}$.\n\nThe derivation relies on the properties of jointly Gaussian random variables. We consider the joint distribution of the state $x_k$ and the subsequent state $x_{k+1}$ conditioned on the observations up to time $k$, denoted $y_{0:k}$. In a linear Gaussian system, this distribution $p(x_k, x_{k+1} | y_{0:k})$ is Gaussian.\n\nThe mean vector of this joint distribution is:\n$$\n\\mathbb{E}\\left[\\begin{pmatrix} x_k \\\\ x_{k+1} \\end{pmatrix} \\middle| y_{0:k}\\right] = \\begin{pmatrix} \\mathbb{E}[x_k | y_{0:k}] \\\\ \\mathbb{E}[x_{k+1} | y_{0:k}] \\end{pmatrix} = \\begin{pmatrix} x_{k|k} \\\\ x_{k+1|k} \\end{pmatrix}\n$$\nThe covariance matrix is:\n$$\n\\operatorname{Cov}\\left(\\begin{pmatrix} x_k \\\\ x_{k+1} \\end{pmatrix} \\middle| y_{0:k}\\right) = \\begin{pmatrix} \\operatorname{Cov}(x_k, x_k | y_{0:k}) & \\operatorname{Cov}(x_k, x_{k+1} | y_{0:k}) \\\\ \\operatorname{Cov}(x_{k+1}, x_k | y_{0:k}) & \\operatorname{Cov}(x_{k+1}, x_{k+1} | y_{0:k}) \\end{pmatrix}\n$$\nThe elements are the filtering covariance $P_{k|k}$, prediction covariance $P_{k+1|k}$, and the cross-covariance. For the state model $x_{k+1} = F_k x_k + w_k$ (with $F_k=1$ in this problem), we have:\n$\\operatorname{Cov}(x_k, x_{k+1} | y_{0:k}) = \\operatorname{Cov}(x_k, x_k + w_k | y_{0:k}) = \\operatorname{Cov}(x_k, x_k | y_{0:k}) = P_{k|k}$, because the process noise $w_k$ is independent of $x_k$ and $y_{0:k}$.\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} x_k \\\\ x_{k+1} \\end{pmatrix} | y_{0:k} \\sim \\mathcal{N} \\left( \\begin{pmatrix} x_{k|k} \\\\ x_{k+1|k} \\end{pmatrix}, \\begin{pmatrix} P_{k|k} & P_{k|k} \\\\ P_{k|k} & P_{k+1|k} \\end{pmatrix} \\right)\n$$\nThe core of the RTS derivation is to combine the filtering information about $x_k$ contained in $p(x_k | y_{0:k})$ with the information that propagates backward from the future, represented by the smoothed distribution $p(x_{k+1} | y_{0:N})$. This is achieved using the law of total probability and the Markov property of the state-space model:\n$$\np(x_k | y_{0:N}) = \\int p(x_k, x_{k+1} | y_{0:N}) d x_{k+1} = \\int p(x_k | x_{k+1}, y_{0:N}) p(x_{k+1} | y_{0:N}) d x_{k+1}\n$$\nBy the Markov property, $p(x_k | x_{k+1}, y_{0:N}) = p(x_k | x_{k+1}, y_{0:k})$. We find this conditional distribution from the joint distribution of $(x_k, x_{k+1})$ given $y_{0:k}$ using the formula for Gaussian conditioning. For variables $z_1$, $z_2$ with joint mean $(\\mu_1, \\mu_2)$ and covariance $(\\Sigma_{11}, \\Sigma_{12}; \\Sigma_{21}, \\Sigma_{22})$, the conditional mean and covariance of $z_1$ given $z_2$ are:\n$$\n\\mathbb{E}[z_1 | z_2] = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (z_2 - \\mu_2)\n$$\n$$\n\\operatorname{Cov}(z_1 | z_2) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\nApplying this to $x_k$ conditioned on $x_{k+1}$ and $y_{0:k}$:\n$$\n\\mathbb{E}[x_k | x_{k+1}, y_{0:k}] = x_{k|k} + P_{k|k} (P_{k+1|k})^{-1} (x_{k+1} - x_{k+1|k})\n$$\n$$\n\\operatorname{Cov}(x_k | x_{k+1}, y_{0:k}) = P_{k|k} - P_{k|k} (P_{k+1|k})^{-1} P_{k|k}\n$$\nThe smoothed mean $x_{k|N}$ is found by taking the expectation of $\\mathbb{E}[x_k | x_{k+1}, y_{0:k}]$ with respect to $p(x_{k+1} | y_{0:N})$:\n$$\nx_{k|N} = \\mathbb{E}[\\mathbb{E}[x_k | x_{k+1}, y_{0:k}] | y_{0:N}] = x_{k|k} + P_{k|k} (P_{k+1|k})^{-1} (\\mathbb{E}[x_{k+1} | y_{0:N}] - x_{k+1|k})\n$$\nDefining the RTS gain $J_k = P_{k|k} F_k^T (P_{k+1|k})^{-1}$ (with $F_k=1$ here), we obtain the smoothed mean update:\n$$\nx_{k|N} = x_{k|k} + J_k (x_{k+1|N} - x_{k+1|k})\n$$\nThe smoothed covariance $P_{k|N}$ is found using the law of total covariance:\n$$\nP_{k|N} = \\mathbb{E}[\\operatorname{Cov}(x_k | x_{k+1}, y_{0:k}) | y_{0:N}] + \\operatorname{Cov}(\\mathbb{E}[x_k | x_{k+1}, y_{0:k}] | y_{0:N})\n$$\nThe first term is constant with respect to the outer expectation, yielding $P_{k|k} - J_k P_{k+1|k} J_k^T$. The second term is $\\operatorname{Cov}(J_k x_{k+1} | y_{0:N}) = J_k P_{k+1|N} J_k^T$. Combining them gives the smoothed covariance update:\n$$\nP_{k|N} = P_{k|k} + J_k (P_{k+1|N} - P_{k+1|k}) J_k^T\n$$\nThese equations define the backward recursion for the RTS smoother.\n\n**Part 2: Numerical Calculation**\n\nWe are given the parameters $q=0.5$, $r=2.0$, $m_0=0$, $p_0=1$, and observations $y_0=1.0$, $y_1=0.0$, $y_2=2.0$. The model matrices are $F=1$, $H=1$, $Q=q=0.5$, $R=r=2.0$.\n\n**Forward Pass: Kalman Filter**\nThe filter operates in prediction-update cycles. We start with the prior $x_{0|-1}=0$ and $P_{0|-1}=1$.\n\n**Step $k=0$:**\n-   **Update:**\n    -   Innovation covariance: $S_0 = H P_{0|-1} H^T + R = 1 \\cdot 1 \\cdot 1 + 2.0 = 3.0$\n    -   Kalman gain: $K_0 = P_{0|-1} H^T S_0^{-1} = 1 \\cdot 1 \\cdot (3.0)^{-1} = 1/3$\n    -   Filtered mean: $x_{0|0} = x_{0|-1} + K_0 (y_0 - H x_{0|-1}) = 0 + (1/3)(1.0 - 0) = 1/3$\n    -   Filtered covariance: $P_{0|0} = (1 - K_0 H) P_{0|-1} = (1 - 1/3) \\cdot 1 = 2/3$\n\n**Step $k=1$:**\n-   **Prediction:**\n    -   $x_{1|0} = F x_{0|0} = 1 \\cdot (1/3) = 1/3$\n    -   $P_{1|0} = F P_{0|0} F^T + Q = 1 \\cdot (2/3) \\cdot 1 + 0.5 = 2/3 + 1/2 = 7/6$\n-   **Update:**\n    -   $S_1 = H P_{1|0} H^T + R = 1 \\cdot (7/6) \\cdot 1 + 2.0 = 19/6$\n    -   $K_1 = P_{1|0} H^T S_1^{-1} = (7/6) \\cdot 1 \\cdot (19/6)^{-1} = 7/19$\n    -   $x_{1|1} = x_{1|0} + K_1 (y_1 - H x_{1|0}) = 1/3 + (7/19)(0.0 - 1/3) = 4/19$\n    -   $P_{1|1} = (1 - K_1 H) P_{1|0} = (1 - 7/19) \\cdot (7/6) = (12/19) \\cdot (7/6) = 14/19$\n\n**Step $k=2$:**\n-   **Prediction:**\n    -   $x_{2|1} = F x_{1|1} = 1 \\cdot (4/19) = 4/19$\n    -   $P_{2|1} = F P_{1|1} F^T + Q = 1 \\cdot (14/19) \\cdot 1 + 0.5 = 47/38$\n-   **Update:**\n    -   $S_2 = H P_{2|1} H^T + R = 1 \\cdot (47/38) \\cdot 1 + 2.0 = 123/38$\n    -   $K_2 = P_{2|1} H^T S_2^{-1} = (47/38) \\cdot 1 \\cdot (123/38)^{-1} = 47/123$\n    -   $x_{2|2} = x_{2|1} + K_2 (y_2 - H x_{2|1}) = 4/19 + (47/123)(2.0 - 4/19) = 110/123$\n    -   $P_{2|2} = (1 - K_2 H) P_{2|1} = (1 - 47/123) \\cdot (47/38) = (76/123) \\cdot (47/38) = 94/123$\n\n**Backward Pass: RTS Smoother**\nThe backward pass starts at $k=N-1=1$ with the final filtered estimates $x_{2|2} = 110/123$ and $P_{2|2} = 94/123$.\n\n**Step $k=1$:**\n-   RTS gain: $J_1 = P_{1|1} F^T (P_{2|1})^{-1} = (14/19) \\cdot 1 \\cdot (47/38)^{-1} = 28/47$\n-   Smoothed mean: $x_{1|2} = x_{1|1} + J_1 (x_{2|2} - x_{2|1}) = 4/19 + (28/47)(110/123 - 4/19) = 76/123$\n-   Smoothed covariance: $P_{1|2} = P_{1|1} + J_1 (P_{2|2} - P_{2|1}) J_1^T = 14/19 + (28/47)(94/123 - 47/38)(28/47) = 70/123$\n\n**Step $k=0$:**\n-   RTS gain: $J_0 = P_{0|0} F^T (P_{1|0})^{-1} = (2/3) \\cdot 1 \\cdot (7/6)^{-1} = 4/7$\n-   Smoothed mean: $x_{0|2} = x_{0|0} + J_0 (x_{1|2} - x_{1|0}) = 1/3 + (4/7)(76/123 - 1/3) = 61/123$\n-   Smoothed covariance: $P_{0|2} = P_{0|0} + J_0 (P_{1|2} - P_{1|0}) J_0^T = 2/3 + (4/7)(70/123 - 7/6)(4/7) = 58/123$\n\n**Part 3: Final Numerical Values**\nThe required values are $(J_0, J_1, x_{0|2}, x_{1|2}, x_{2|2}, P_{0|2}, P_{1|2}, P_{2|2})$, rounded to four significant figures.\n\n- $J_0 = 4/7 \\approx 0.571428... \\to 0.5714$\n- $J_1 = 28/47 \\approx 0.595744... \\to 0.5957$\n- $x_{0|2} = 61/123 \\approx 0.495934... \\to 0.4959$\n- $x_{1|2} = 76/123 \\approx 0.617886... \\to 0.6179$\n- $x_{2|2} = 110/123 \\approx 0.894308... \\to 0.8943$\n- $P_{0|2} = 58/123 \\approx 0.471544... \\to 0.4715$\n- $P_{1|2} = 70/123 \\approx 0.569105... \\to 0.5691$\n- $P_{2|2} = 94/123 \\approx 0.764227... \\to 0.7642$\n\nThese values are compiled into the final answer.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5714 & 0.5957 & 0.4959 & 0.6179 & 0.8943 & 0.4715 & 0.5691 & 0.7642 \\end{pmatrix}}\n$$", "id": "2872818"}, {"introduction": "Real-world estimation problems often include specific constraints or known information beyond typical sensor measurements. This practice challenges you to adapt the standard RTS smoother to handle an exact terminal state constraint, a scenario common in trajectory planning or post-mission analysis. By reasoning about the boundary conditions of the backward pass, you will gain deeper insight into how the smoother propagates information through time. [@problem_id:2872810]", "problem": "Consider a scalar, linear, time-invariant, discrete-time, Gaussian state-space model over a finite horizon $k=0,1,2,3$ given by\n- State dynamics: $x_{k+1} = a\\,x_{k} + w_{k}$ with $w_{k} \\sim \\mathcal{N}(0,q)$,\n- Measurement: $y_{k} = h\\,x_{k} + v_{k}$ with $v_{k} \\sim \\mathcal{N}(0,r)$,\nwhere $\\{w_{k}\\}$ and $\\{v_{k}\\}$ are mutually independent and independent of the initial state. The initial prior is $x_{0} \\sim \\mathcal{N}(m_{0},P_{0})$. Suppose that the measurement sequence is observed for $k=0,1,2$ and there is no measurement at $k=3$. In addition, a terminal state constraint is known exactly: $x_{3} = \\bar{x}_{3}$.\n\nYou are asked to incorporate the exact terminal constraint into the fixed-interval Rauch-Tung-Striebel (RTS) smoother by formulating the appropriate boundary condition at $k=3$ and deriving the backward recursion from first principles of Gaussian conditioning. Specifically, starting from the definitions of the one-step predictor and filter as the conditional mean-square optimal estimators for linear-Gaussian models, and using only the properties of jointly Gaussian vectors and Bayes’ rule, derive how the backward recursion for the smoothed conditional mean must be modified when $x_{3}$ is known exactly.\n\nThen, for the concrete instance with parameters\n- $a=1$, $h=1$, $q=\\frac{1}{2}$, $r=1$,\n- $m_{0}=0$, $P_{0}=1$,\n- measurements $y_{0}=1$, $y_{1}=0$, $y_{2}=2$,\n- terminal constraint $x_{3}=\\bar{x}_{3}=1$ exactly,\ncompute the smoothed conditional means $x_{k|3} \\triangleq \\mathbb{E}[x_{k} | y_{0},y_{1},y_{2},x_{3}=\\bar{x}_{3}]$ for $k=0,1,2,3$.\n\nExpress your final answer as a single row matrix containing $x_{0|3}$, $x_{1|3}$, $x_{2|3}$, $x_{3|3}$ in that order. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n-   Model: Scalar, linear, time-invariant, discrete-time, Gaussian state-space model.\n-   Horizon: $k=0,1,2,3$.\n-   State dynamics: $x_{k+1} = a\\,x_{k} + w_{k}$, where $w_{k} \\sim \\mathcal{N}(0,q)$.\n-   Measurement model: $y_{k} = h\\,x_{k} + v_{k}$, where $v_{k} \\sim \\mathcal{N}(0,r)$.\n-   Noise properties: The process noise $\\{w_{k}\\}$ and measurement noise $\\{v_{k}\\}$ are mutually independent white noise sequences, and are independent of the initial state.\n-   Initial prior: $x_{0} \\sim \\mathcal{N}(m_{0},P_{0})$.\n-   Observation sequence: Measurements $y_0, y_1, y_2$ are observed. No measurement is available at $k=3$.\n-   Terminal constraint: The state at time $k=3$ is known exactly: $x_{3} = \\bar{x}_{3}$.\n-   Task 1: Derive the modified backward recursion for the smoothed conditional mean, incorporating the exact terminal constraint, based on first principles of Gaussian conditioning.\n-   Task 2: For the specific instance with parameters $a=1$, $h=1$, $q=\\frac{1}{2}$, $r=1$, $m_{0}=0$, $P_{0}=1$, and measurements $y_{0}=1$, $y_{1}=0$, $y_{2}=2$, with terminal constraint $\\bar{x}_{3}=1$, compute the smoothed conditional means $x_{k|3} \\triangleq \\mathbb{E}[x_{k} | y_{0},y_{1},y_{2},x_{3}=\\bar{x}_{3}]$ for $k=0,1,2,3$.\n-   Final Answer Format: A single row matrix containing the computed values in the order $x_{0|3}$, $x_{1|3}$, $x_{2|3}$, $x_{3|3}$.\n\nStep 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is a standard exercise in state estimation theory, focusing on the Rauch–Tung–Striebel (RTS) smoother, a cornerstone algorithm in signal processing and control. The inclusion of a hard terminal constraint is a well-defined and practically relevant extension. The problem is grounded in the established theory of Bayesian inference for linear-Gaussian systems.\n-   **Well-Posed:** The problem is well-posed. It provides a complete description of the system model, all necessary parameters, and a specific set of observations and constraints. The objective is clearly stated, and a unique solution is expected.\n-   **Objective:** The problem is formulated in precise, objective mathematical language, free from ambiguity or subjective elements.\n\nStep 3: Verdict and Action\n-   The problem is deemed **valid**. It is scientifically sound, well-posed, objective, and contains no discernible flaws. A complete solution will be provided.\n\nThe solution proceeds in two parts. First, the derivation of the modified smoother recursion is presented. Second, the numerical computation for the given instance is performed.\n\n**Part 1: Derivation of the Modified Smoother**\n\nThe objective is to compute the smoothed state estimate $x_{k|3} = \\mathbb{E}[x_k | Y_3]$, where the information set is $Y_3 = \\{y_0, y_1, y_2, x_3=\\bar{x}_3\\}$. The Rauch-Tung-Striebel (RTS) smoother provides a backward recursion to compute such estimates. We derive this recursion from first principles for the given information set.\n\nLet $Y_{0:k} = \\{y_0, \\dots, y_k\\}$. We use the law of total expectation:\n$$x_{k|3} = \\mathbb{E}[x_k | Y_3] = \\mathbb{E}\\left[ \\mathbb{E}[x_k | x_{k+1}, Y_3] | Y_3 \\right]$$\nThe key step is to analyze the inner expectation, $\\mathbb{E}[x_k | x_{k+1}, Y_3]$. The state $x_k$ is related to $x_{k+1}$ via the state dynamics $x_{k+1} = a x_k + w_k$. The information set $Y_3$ can be partitioned into information available up to time $k$, $Y_{0:k}$, and information from after time $k$, which is $\\{y_{k+1}, y_{k+2}, x_3=\\bar{x}_3\\}$.\n\nDue to the Markovian structure of the state-space model, the state $x_k$ is conditionally independent of all future states and measurements given the next state $x_{k+1}$. That is, $x_k \\perp \\{y_{k+1}, y_2, x_3\\} | x_{k+1}$. This implies that conditioning on additional future information does not alter the distribution of $x_k$ once $x_{k+1}$ is known.\nTherefore,\n$$p(x_k | x_{k+1}, Y_3) = p(x_k | x_{k+1}, y_0, \\dots, y_k) = p(x_k | x_{k+1}, Y_{0:k})$$\nThis simplifies the inner expectation:\n$$\\mathbb{E}[x_k | x_{k+1}, Y_3] = \\mathbb{E}[x_k | x_{k+1}, Y_{0:k}]$$\nThis expectation is a standard result from the theory of linear-Gaussian systems. The joint distribution of $(x_k, x_{k+1})$ conditioned on the measurements $Y_{0:k}$ is Gaussian. From the forward pass of the Kalman filter, we have the filtered distribution $p(x_k | Y_{0:k}) = \\mathcal{N}(x_k; x_{k|k}, P_{k|k})$. The predicted distribution for $x_{k+1}$ is $p(x_{k+1} | Y_{0:k}) = \\mathcal{N}(x_{k+1}; x_{k+1|k}, P_{k+1|k})$, where $x_{k+1|k} = a x_{k|k}$ and $P_{k+1|k} = a^2 P_{k|k} + q$.\n\nThe covariance between $x_k$ and $x_{k+1}$ given $Y_{0:k}$ is:\n$$\\mathrm{Cov}(x_k, x_{k+1} | Y_{0:k}) = \\mathrm{Cov}(x_k, a x_k + w_k | Y_{0:k}) = a \\mathrm{Var}(x_k | Y_{0:k}) = a P_{k|k}$$\nUsing the formula for the conditional mean of a jointly Gaussian distribution, we get:\n$$\\mathbb{E}[x_k | x_{k+1}, Y_{0:k}] = \\mathbb{E}[x_k | Y_{0:k}] + \\mathrm{Cov}(x_k, x_{k+1} | Y_{0:k}) \\mathrm{Var}(x_{k+1} | Y_{0:k})^{-1} (x_{k+1} - \\mathbb{E}[x_{k+1} | Y_{0:k}])$$\nSubstituting the Kalman filter quantities:\n$$\\mathbb{E}[x_k | x_{k+1}, Y_{0:k}] = x_{k|k} + (a P_{k|k}) (P_{k+1|k})^{-1} (x_{k+1} - x_{k+1|k})$$\nLet the smoother gain be $J_k = P_{k|k} a P_{k+1|k}^{-1}$. The expression becomes:\n$$\\mathbb{E}[x_k | x_{k+1}, Y_3] = x_{k|k} + J_k (x_{k+1} - x_{k+1|k})$$\nSubstituting this back into the law of total expectation:\n$$x_{k|3} = \\mathbb{E}[ x_{k|k} + J_k (x_{k+1} - x_{k+1|k}) | Y_3 ]$$\nBy linearity of expectation, and noting that $x_{k|k}$ and $x_{k+1|k}$ are functions of $Y_{0:k}$ (a subset of $Y_3$) and thus are constants with respect to the outer expectation:\n$$x_{k|3} = x_{k|k} + J_k (\\mathbb{E}[x_{k+1} | Y_3] - x_{k+1|k})$$\nBy definition, $\\mathbb{E}[x_{k+1} | Y_3] = x_{k+1|3}$. This yields the backward recursion:\n$$x_{k|3} = x_{k|k} + J_k (x_{k+1|3} - x_{k+1|k})$$\nThis recursion is identical in form to the standard RTS smoother recursion for the mean. The modification enters through the boundary condition at the terminal time $k=3$. The information set $Y_3$ explicitly contains the value $x_3=\\bar{x}_3$. Thus, the expectation of $x_3$ given $Y_3$ is simply this known value:\n$$x_{3|3} = \\mathbb{E}[x_3 | y_0, y_1, y_2, x_3=\\bar{x}_3] = \\bar{x}_3$$\nThis contrasts with the standard RTS smoother, which would start with the final filtered estimate. Here, the recursion is initialized with the exact known terminal state.\n\n**Part 2: Numerical Computation**\n\nWe first execute the forward Kalman filter pass to compute the required filtered ($x_{k|k}$, $P_{k|k}$) and predicted ($x_{k+1|k}$, $P_{k+1|k}$) quantities. The parameters are $a=1$, $h=1$, $q=1/2$, $r=1$, $m_0=0$, $P_0=1$. The measurements are $y_0=1, y_1=0, y_2=2$.\n\n**Forward Kalman Filter Pass ($k=0,1,2$)**\n\nKalman filter equations:\n-   Prediction: $x_{k|k-1} = a x_{k-1|k-1}$, $P_{k|k-1} = a^2 P_{k-1|k-1} + q$.\n-   Update: $K_k = P_{k|k-1} h (h^2 P_{k|k-1} + r)^{-1}$, $x_{k|k} = x_{k|k-1} + K_k (y_k - h x_{k|k-1})$, $P_{k|k} = (1 - K_k h) P_{k|k-1}$.\n\n_For k = 0:_\n-   Prior: $x_{0|-1} = m_0 = 0$, $P_{0|-1} = P_0 = 1$.\n-   Kalman Gain: $K_0 = 1 \\cdot 1 \\cdot (1^2 \\cdot 1 + 1)^{-1} = 1/2$.\n-   Update: $x_{0|0} = 0 + \\frac{1}{2}(1 - 1 \\cdot 0) = \\frac{1}{2}$.\n-   Update: $P_{0|0} = (1 - \\frac{1}{2}\\cdot 1) \\cdot 1 = \\frac{1}{2}$.\n\n_For k = 1:_\n-   Prediction: $x_{1|0} = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}$.\n-   Prediction: $P_{1|0} = 1^2 \\cdot \\frac{1}{2} + \\frac{1}{2} = 1$.\n-   Kalman Gain: $K_1 = 1 \\cdot 1 \\cdot (1^2 \\cdot 1 + 1)^{-1} = 1/2$.\n-   Update: $x_{1|1} = \\frac{1}{2} + \\frac{1}{2}(0 - 1 \\cdot \\frac{1}{2}) = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$.\n-   Update: $P_{1|1} = (1 - \\frac{1}{2}\\cdot 1) \\cdot 1 = \\frac{1}{2}$.\n\n_For k = 2:_\n-   Prediction: $x_{2|1} = 1 \\cdot \\frac{1}{4} = \\frac{1}{4}$.\n-   Prediction: $P_{2|1} = 1^2 \\cdot \\frac{1}{2} + \\frac{1}{2} = 1$.\n-   Kalman Gain: $K_2 = 1 \\cdot 1 \\cdot (1^2 \\cdot 1 + 1)^{-1} = 1/2$.\n-   Update: $x_{2|2} = \\frac{1}{4} + \\frac{1}{2}(2 - 1 \\cdot \\frac{1}{4}) = \\frac{1}{4} + \\frac{1}{2} \\cdot \\frac{7}{4} = \\frac{2}{8} + \\frac{7}{8} = \\frac{9}{8}$.\n-   Update: $P_{2|2} = (1 - \\frac{1}{2}\\cdot 1) \\cdot 1 = \\frac{1}{2}$.\n\n_For k = 3 (Prediction only):_\n-   Prediction: $x_{3|2} = 1 \\cdot \\frac{9}{8} = \\frac{9}{8}$.\n-   Prediction: $P_{3|2} = 1^2 \\cdot \\frac{1}{2} + \\frac{1}{2} = 1$.\n\n**Backward Smoother Pass ($k=3,2,1,0$)**\n\nSmoother recursion: $x_{k|3} = x_{k|k} + J_k (x_{k+1|3} - x_{k+1|k})$, with $J_k = P_{k|k} a P_{k+1|k}^{-1}$.\n\n_For k = 3:_\n-   Boundary condition: $x_{3|3} = \\bar{x}_3 = 1$.\n\n_For k = 2:_\n-   Smoother Gain: $J_2 = P_{2|2} \\cdot 1 \\cdot (P_{3|2})^{-1} = \\frac{1}{2} \\cdot 1 \\cdot (1)^{-1} = \\frac{1}{2}$.\n-   Smoothed mean: $x_{2|3} = x_{2|2} + J_2 (x_{3|3} - x_{3|2}) = \\frac{9}{8} + \\frac{1}{2}(1 - \\frac{9}{8}) = \\frac{9}{8} + \\frac{1}{2}(-\\frac{1}{8}) = \\frac{9}{8} - \\frac{1}{16} = \\frac{18-1}{16} = \\frac{17}{16}$.\n\n_For k = 1:_\n-   Smoother Gain: $J_1 = P_{1|1} \\cdot 1 \\cdot (P_{2|1})^{-1} = \\frac{1}{2} \\cdot 1 \\cdot (1)^{-1} = \\frac{1}{2}$.\n-   Smoothed mean: $x_{1|3} = x_{1|1} + J_1 (x_{2|3} - x_{2|1}) = \\frac{1}{4} + \\frac{1}{2}(\\frac{17}{16} - \\frac{1}{4}) = \\frac{1}{4} + \\frac{1}{2}(\\frac{17-4}{16}) = \\frac{1}{4} + \\frac{13}{32} = \\frac{8+13}{32} = \\frac{21}{32}$.\n\n_For k = 0:_\n-   Smoother Gain: $J_0 = P_{0|0} \\cdot 1 \\cdot (P_{1|0})^{-1} = \\frac{1}{2} \\cdot 1 \\cdot (1)^{-1} = \\frac{1}{2}$.\n-   Smoothed mean: $x_{0|3} = x_{0|0} + J_0 (x_{1|3} - x_{1|0}) = \\frac{1}{2} + \\frac{1}{2}(\\frac{21}{32} - \\frac{1}{2}) = \\frac{1}{2} + \\frac{1}{2}(\\frac{21-16}{32}) = \\frac{1}{2} + \\frac{5}{64} = \\frac{32+5}{64} = \\frac{37}{64}$.\n\nThe computed smoothed means are:\n$x_{0|3} = \\frac{37}{64}$\n$x_{1|3} = \\frac{21}{32}$\n$x_{2|3} = \\frac{17}{16}$\n$x_{3|3} = 1$\n\nThese values are collected into the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{37}{64} & \\frac{21}{32} & \\frac{17}{16} & 1\n\\end{pmatrix}\n}\n$$", "id": "2872810"}, {"introduction": "A crucial step in any filtering or smoothing pipeline is initialization, especially when prior knowledge is vague or \"diffuse\". This final practice moves into implementation, tasking you with handling a non-informative prior for the initial state. You will compare an exact Bayesian treatment with a common but potentially problematic large-variance approximation, highlighting important nuances in the numerical application of smoothing algorithms. [@problem_id:2872799]", "problem": "You are given a linear Gaussian state-space model for a scalar local level process with additive process and measurement noise. The model is defined by the two fundamental equations\n$$\nx_{k+1} = x_k + w_k, \\quad w_k \\sim \\mathcal{N}(0,q),\n$$\n$$\ny_k = x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0,r),\n$$\nwhere $x_k$ is the unknown state at discrete time $k$, $y_k$ is the observation, $q \\ge 0$ is the process noise variance, and $r > 0$ is the measurement noise variance. The initial state $x_1$ is diffuse, meaning that, a priori, it has an improper uniform distribution on the real line. The primary objective is to apply fixed-interval smoothing using the Rauch–Tung–Striebel (RTS) smoother to compute the early smoothed state uncertainty, specifically the smoothed covariance $P_{1|N}$, where $N$ is the final time and $P_{k|N} = \\operatorname{Var}(x_k | y_{1:N})$.\n\nYour task is to derive, implement, and compare two treatments of the diffuse initialization and then quantify their effect on the early smoothed covariance $P_{1|N}$:\n- Exact diffuse initialization, using a mathematically exact treatment equivalent to partitioned updates or augmented states for the initial diffuse prior. In the scalar model above, this must be handled from first principles by conditioning on the first observation and then proceeding with standard recursive filtering and fixed-interval smoothing thereafter.\n- Naive large-variance initialization, which approximates the diffuse prior by taking a very large but finite initial variance $P_{1|0} = M$ and then applying standard Kalman filtering and RTS smoothing recursions.\n\nStarting from the fundamental definitions of linear Gaussian models, Gaussian conditioning, and Bayes' rule (without assuming or quoting specialized diffuse Kalman filter formulas), you must:\n1. Explain why the exact diffuse treatment leads to a well-defined initial update based solely on the likelihood at the first observed time, and how this logically extends to subsequent Kalman filter prediction-update steps and the Rauch–Tung–Striebel smoothing recursion.\n2. Implement both approaches described above and compute the smoothed covariance $P_{1|N}$ under each method.\n3. Quantify the effect of the diffuse-initialization method on the early smoothed covariance by reporting, for each test case below, the relative difference\n$$\n\\delta = \\frac{P_{1|N}^{\\text{naive}} - P_{1|N}^{\\text{exact}}}{P_{1|N}^{\\text{exact}}}.\n$$\n\nAll answers must be in pure numbers (unitless variances), provided as floating-point values.\n\nTest suite:\n- Case A (general, well-posed): $q = 0.1$, $r = 1.0$, $N = 5$, observations $y_{1:5} = [0.5,\\,-0.1,\\,0.2,\\,1.2,\\,0.7]$.\n- Case B (deterministic state evolution): $q = 0.0$, $r = 0.25$, $N = 5$, observations $y_{1:5} = [1.0,\\,1.1,\\,0.9,\\,1.05,\\,0.95]$.\n- Case C (highly informative measurements): $q = 0.1$, $r = 10^{-6}$, $N = 5$, observations $y_{1:5} = [0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0]$.\n\nImplementation requirements and conventions:\n- For the exact diffuse initialization, treat the initial prior $p(x_1)$ as improper uniform and derive the first update distribution from $p(y_1 | x_1)$ via Bayes' rule, then proceed with standard recursive filtering and fixed-interval smoothing for $k \\ge 2$.\n- For the naive method, use standard Kalman filtering and Rauch–Tung–Striebel smoothing recursions with finite initial mean and variance, taking $x_{1|0} = 0$ and $P_{1|0} = M$ with $M = 10^6$.\n- For both methods, you must compute $P_{1|N}$ exactly as implied by the specified model and assumptions for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the three relative differences $\\delta$ for Cases A, B, and C, as a comma-separated list enclosed in square brackets (e.g., \"[dA,dB,dC]\").", "solution": "The problem posed is to analyze and compare two distinct methods for handling a diffuse prior on the initial state of a scalar linear Gaussian state-space model, specifically a local level model. The objective is to compute the smoothed state covariance at the initial time, $P_{1|N}$, using the Rauch-Tung-Striebel (RTS) smoother and to quantify the difference between an exact diffuse treatment and a naive large-variance approximation.\n\nThe problem is scientifically grounded, well-posed, and all necessary parameters for its solution are provided. We proceed with the derivation and analysis.\n\nThe state-space model is defined by the process and measurement equations for discrete time steps $k=1, 2, \\ldots, N$:\n$$\nx_{k+1} = F x_k + w_k, \\quad w_k \\sim \\mathcal{N}(0, q)\n$$\n$$\ny_k = H x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, r)\n$$\nFor the given scalar local level model, the state transition matrix is $F=1$ and the observation matrix is $H=1$. The process noise variance is $q \\ge 0$ and the measurement noise variance is $r > 0$. The state $x_k$ and observation $y_k$ are scalars.\n\nThe standard algorithm for estimation in this model consists of a forward pass (Kalman filter) and a backward pass (RTS smoother).\n\nThe Kalman filter recursively computes the filtered mean $x_{k|k} = \\mathbb{E}[x_k | y_{1:k}]$ and variance $P_{k|k} = \\operatorname{Var}(x_k | y_{1:k})$. The steps are:\n1.  **Prediction:**\n    $$\n    x_{k|k-1} = F x_{k-1|k-1} = x_{k-1|k-1}\n    $$\n    $$\n    P_{k|k-1} = F P_{k-1|k-1} F^T + q = P_{k-1|k-1} + q\n    $$\n2.  **Update:**\n    $$\n    S_k = H P_{k|k-1} H^T + r = P_{k|k-1} + r\n    $$\n    $$\n    K_k = P_{k|k-1} H^T S_k^{-1} = \\frac{P_{k|k-1}}{P_{k|k-1} + r}\n    $$\n    $$\n    x_{k|k} = x_{k|k-1} + K_k (y_k - H x_{k|k-1}) = x_{k|k-1} + K_k (y_k - x_{k|k-1})\n    $$\n    $$\n    P_{k|k} = (1 - K_k H) P_{k|k-1} = (1 - K_k) P_{k|k-1}\n    $$\n\nThe RTS smoother then performs a backward pass from $k=N-1$ down to $1$ to compute the smoothed mean $x_{k|N} = \\mathbb{E}[x_k | y_{1:N}]$ and variance $P_{k|N} = \\operatorname{Var}(x_k | y_{1:N})$. The recursion starts with the final filtered estimates, $x_{N|N}$ and $P_{N|N}$.\n1.  **Smoother Gain:**\n    $$\n    G_k = P_{k|k} F^T P_{k+1|k}^{-1} = \\frac{P_{k|k}}{P_{k+1|k}} = \\frac{P_{k|k}}{P_{k|k} + q}\n    $$\n2.  **Smoothing Update:**\n    $$\n    x_{k|N} = x_{k|k} + G_k (x_{k+1|N} - x_{k+1|k})\n    $$\n    $$\n    P_{k|N} = P_{k|k} + G_k (P_{k+1|N} - P_{k+1|k}) G_k^T\n    $$\nThe core of the problem lies in the initialization of this recursive process at $k=1$.\n\n**1. Exact Diffuse Initialization**\n\nA diffuse prior for the initial state $x_1$ means its probability distribution is improper uniform, $p(x_1) \\propto C$, where $C$ is a constant. The standard Kalman filter update, which assumes a proper Gaussian prior, is not directly applicable. We must return to first principles, specifically Bayes' rule for the posterior distribution of $x_1$ given the first observation $y_1$:\n$$\np(x_1 | y_1) \\propto p(y_1 | x_1) p(x_1)\n$$\nThe likelihood $p(y_1 | x_1)$ is defined by the measurement model as a Gaussian distribution for the variable $y_1$, but viewed as a function of $x_1$, it is a Gaussian function centered at $y_1$:\n$$\np(y_1 | x_1) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{(y_1 - x_1)^2}{2r}\\right)\n$$\nSince $p(x_1)$ is constant, the posterior probability density of $x_1$ is proportional to the likelihood function:\n$$\np(x_1 | y_1) \\propto \\exp\\left(-\\frac{(x_1 - y_1)^2}{2r}\\right)\n$$\nThis is the kernel of a Gaussian distribution with mean $y_1$ and variance $r$. Thus, the posterior distribution $p(x_1 | y_1)$ is proper and is given by $\\mathcal{N}(x_1; y_1, r)$. This provides the exact initialization for the filtered estimates at $k=1$:\n$$\nx_{1|1}^{\\text{exact}} = y_1\n$$\n$$\nP_{1|1}^{\\text{exact}} = r\n$$\nThis result is exact and does not rely on any approximation. From this point forward, for $k=2, \\dots, N$, the standard Kalman filter recursions can be applied to obtain all filtered estimates up to time $N$. Subsequently, the RTS smoother backward pass is applied from $k=N-1$ down to $k=1$ to compute the smoothed covariances, including the target quantity $P_{1|N}^{\\text{exact}}$.\n\n**2. Naive Large-Variance Initialization**\n\nThis method approximates the diffuse prior with a proper Gaussian prior having a very large variance. We set the prior for $x_1$ as $p(x_1) = \\mathcal{N}(x_1; x_{1|0}, P_{1|0})$, with $x_{1|0} = 0$ and $P_{1|0} = M$, where $M$ is a large number (here, $M=10^6$).\n\nWith this prior, the standard Kalman filter update for $k=1$ can be used directly. The \"predicted\" state and covariance are simply the prior, $x_{1|0}$ and $P_{1|0}$.\nThe innovation covariance is $S_1 = P_{1|0} + r = M + r$.\nThe Kalman gain is $K_1 = P_{1|0} / S_1 = \\frac{M}{M+r}$.\nThe updated mean is $x_{1|1}^{\\text{naive}} = x_{1|0} + K_1(y_1 - x_{1|0}) = 0 + \\frac{M}{M+r}(y_1 - 0) = \\frac{M}{M+r}y_1$.\nThe updated covariance is $P_{1|1}^{\\text{naive}} = (1 - K_1)P_{1|0} = \\left(1 - \\frac{M}{M+r}\\right)M = \\frac{rM}{M+r}$.\n\nIn the limit as $M \\to \\infty$, we observe:\n$$\n\\lim_{M\\to\\infty} x_{1|1}^{\\text{naive}} = \\lim_{M\\to\\infty} \\frac{M}{M+r}y_1 = y_1 = x_{1|1}^{\\text{exact}}\n$$\n$$\n\\lim_{M\\to\\infty} P_{1|1}^{\\text{naive}} = \\lim_{M\\to\\infty} \\frac{rM}{M+r} = \\lim_{M\\to\\infty} \\frac{r}{1+r/M} = r = P_{1|1}^{\\text{exact}}\n$$\nThis demonstrates that the large-variance approximation converges to the exact diffuse initialization. However, for any finite $M$, there will be a discrepancy. Specifically, $P_{1|1}^{\\text{naive}} = \\frac{rM}{M+r} = r \\left(\\frac{M}{M+r}\\right) < r$. The initial filtered variance is always slightly underestimated. This small initial error propagates through the remaining filter steps and the subsequent smoother steps, leading to a difference in the final smoothed covariance $P_{1|N}$.\n\nThe procedure is to run the full Kalman filter from $k=1$ to $N$ using this approximate initialization, and then the full RTS smoother from $k=N-1$ down to $1$ to compute $P_{1|N}^{\\text{naive}}$.\n\n**3. Comparison and Quantification**\n\nThe effect of the initialization method is quantified by the relative difference $\\delta$:\n$$\n\\delta = \\frac{P_{1|N}^{\\text{naive}} - P_{1|N}^{\\text{exact}}}{P_{1|N}^{\\text{exact}}}\n$$\nThis value will be computed for three test cases by implementing both procedures as described.\n- Case A: $q = 0.1$, $r = 1.0$, $N = 5$, observations $y_{1:5}$ given.\n- Case B: $q = 0.0$, $r = 0.25$, $N = 5$, observations $y_{1:5}$ given.\n- Case C: $q = 0.1$, $r = 10^{-6}$, $N = 5$, observations $y_{1:5}$ given.\n\nThe implementation will strictly follow the derived equations to ensure a correct comparison.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_rts_smoother(q, r, y, M=None):\n    \"\"\"\n    Runs the Kalman filter and Rauch-Tung-Striebel smoother for a scalar local level model.\n\n    Args:\n        q (float): Process noise variance.\n        r (float): Measurement noise variance.\n        y (np.ndarray): Array of observations y_1, ..., y_N.\n        M (float, optional): Initial variance for naive large-variance method.\n                             If None, the exact diffuse method is used.\n\n    Returns:\n        float: The smoothed covariance P_{1|N}.\n    \"\"\"\n    N = len(y)\n\n    # Arrays to store filter results\n    x_filt = np.zeros(N)  # Filtered state means x_{k|k}\n    P_filt = np.zeros(N)  # Filtered state variances P_{k|k}\n    P_pred = np.zeros(N)  # Predicted state variances P_{k|k-1}\n\n    # --- Kalman Filter Forward Pass ---\n    if M is None:  # Exact diffuse initialization\n        # At k=1 (index 0), the posterior is N(y_1, r)\n        x_filt[0] = y[0]\n        P_filt[0] = r\n        \n        # We need P_pred[0] for the smoother, which is P_{1|0}.\n        # For the exact case, P_{1|0} is infinite. The smoother gain G_0 will be computed\n        # using the derived values at k=1. However, to unify the code, we can\n        # note that the first step of the exact smoother (k=1 from k=2) only depends\n        # on P_{1|1} and P_{2|1}. P_pred[0] is not used.\n        # So we can just run the loop from k=2.\n\n        # Loop for k = 2 to N (indices 1 to N-1)\n        for k in range(1, N):\n            # Prediction\n            x_pred_k = x_filt[k-1]\n            P_pred_k = P_filt[k-1] + q\n            P_pred[k] = P_pred_k\n\n            # Update\n            S_k = P_pred_k + r\n            K_k = P_pred_k / S_k\n            x_filt[k] = x_pred_k + K_k * (y[k] - x_pred_k)\n            P_filt[k] = (1 - K_k) * P_pred_k\n\n    else:  # Naive large-variance initialization\n        x_1_0 = 0.0\n        P_1_0 = M\n        \n        # Loop for k = 1 to N (indices 0 to N-1)\n        for k in range(N):\n            # Prediction\n            if k == 0:\n                x_pred_k = x_1_0\n                P_pred_k = P_1_0\n            else:\n                x_pred_k = x_filt[k-1]\n                P_pred_k = P_filt[k-1] + q\n            P_pred[k] = P_pred_k\n\n            # Update\n            S_k = P_pred_k + r\n            K_k = P_pred_k / S_k\n            x_filt[k] = x_pred_k + K_k * (y[k] - x_pred_k)\n            P_filt[k] = (1 - K_k) * P_pred_k\n\n    # --- RTS Smoother Backward Pass ---\n    x_smooth = np.zeros(N)\n    P_smooth = np.zeros(N)\n\n    # Initialization at k=N (index N-1)\n    x_smooth[N-1] = x_filt[N-1]\n    P_smooth[N-1] = P_filt[N-1]\n\n    # Loop for k = N-1 down to 1 (indices N-2 down to 0)\n    for k in range(N - 2, -1, -1):\n        # Smoother gain G_k\n        # G_k = P_{k|k} / P_{k+1|k}\n        # In our arrays, P_filt[k] is P_{k|k} and P_pred[k+1] is P_{k+1|k}.\n        G_k = P_filt[k] / P_pred[k+1]\n\n        # Smoothed state and covariance update\n        # x_{k+1|k} = x_pred_k+1 = x_{k|k} for this model\n        x_smooth[k] = x_filt[k] + G_k * (x_smooth[k+1] - x_filt[k])\n        P_smooth[k] = P_filt[k] + G_k * (P_smooth[k+1] - P_pred[k+1]) * G_k\n\n    return P_smooth[0]\n\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case A: q = 0.1, r = 1.0, N = 5, y = [0.5, -0.1, 0.2, 1.2, 0.7]\n        (0.1, 1.0, np.array([0.5, -0.1, 0.2, 1.2, 0.7])),\n        # Case B: q = 0.0, r = 0.25, N = 5, y = [1.0, 1.1, 0.9, 1.05, 0.95]\n        (0.0, 0.25, np.array([1.0, 1.1, 0.9, 1.05, 0.95])),\n        # Case C: q = 0.1, r = 1e-6, N = 5, y = [0.0, 0.0, 0.0, 0.0, 0.0]\n        (0.1, 1e-6, np.array([0.0, 0.0, 0.0, 0.0, 0.0])),\n    ]\n\n    M_naive = 1e6\n    results = []\n\n    for case in test_cases:\n        q, r, y = case\n\n        # Calculate P_{1|N} using the exact diffuse method\n        P_1_N_exact = run_rts_smoother(q=q, r=r, y=y, M=None)\n\n        # Calculate P_{1|N} using the naive large-variance method\n        P_1_N_naive = run_rts_smoother(q=q, r=r, y=y, M=M_naive)\n\n        # Calculate the relative difference delta\n        if P_1_N_exact != 0:\n            delta = (P_1_N_naive - P_1_N_exact) / P_1_N_exact\n        else:\n            # Handle case where exact value is zero (unlikely for P > 0)\n            delta = np.inf if P_1_N_naive != 0 else 0.0\n\n        results.append(delta)\n\n    # Format the final output string\n    output_str = f\"[{','.join(f'{res:.12f}' for res in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2872799"}]}