{"hands_on_practices": [{"introduction": "Many real-world systems, from navigating vehicles to tracking objects, are naturally described by continuous-time dynamics. To apply the discrete-time Kalman filter, we must first accurately convert this continuous model into a discrete-time equivalent. This exercise guides you through the rigorous conversion of process noise from its continuous-time spectral density to the discrete-time covariance matrix $Q_d$. Mastering this derivation is a crucial first step in designing high-fidelity estimators, ensuring your filter's model of uncertainty correctly reflects the underlying physics of the system [@problem_id:2912361].", "problem": "A one-dimensional kinematic target is modeled with position, velocity, and acceleration states, forming the continuous-time linear time-invariant state vector $x(t) = \\begin{pmatrix} p(t) \\\\ v(t) \\\\ a(t) \\end{pmatrix}$. The dynamics obey the linear stochastic differential equation\n$$\n\\dot{x}(t) = F x(t) + L w(t),\n$$\nwhere $F \\in \\mathbb{R}^{3 \\times 3}$ and $L \\in \\mathbb{R}^{3 \\times 1}$ are constant matrices, and $w(t)$ is zero-mean white Gaussian process noise with spectral density $q \\in \\mathbb{R}_{>0}$, that is $\\mathbb{E}[w(t) w(\\tau)] = q \\,\\delta(t-\\tau)$. The model is the constant-acceleration-with-random-walk-acceleration model, specified by\n$$\nF = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n\\qquad\nL = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\nLet the sampling interval be $T \\in \\mathbb{R}_{>0}$. Using only foundational facts about linear time-invariant systems and stochastic processes, and applying the Van Loan method for exact discrete-time conversion of continuous-time process noise, derive the exact discrete-time state transition matrix $A_{d}$ and the exact discrete-time process noise covariance $Q_{d}$ associated with the sampling interval $T$. Your derivation must begin from the definitions of the state transition for linear time-invariant systems and the characterization of white Gaussian noise, and must logically justify the integral form that the Van Loan method encodes, without assuming any ready-made discrete-time formulas.\n\nAfter deriving explicit closed-form expressions for $A_{d}$ and $Q_{d}$, compute the scalar $s$, defined as the $(1,1)$ entry of $Q_{d}$, simplified to a closed-form analytic expression in terms of $q$ and $T$. Report only $s$ as your final answer. No numerical substitution is required. Do not include units in the final answer. If you choose to approximate, you must state and justify the approximation; otherwise, provide the exact expression.", "solution": "The problem presented is a standard exercise in the discretization of continuous-time linear stochastic systems. It is well-posed, scientifically sound, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe system is described by the linear stochastic differential equation:\n$$\n\\dot{x}(t) = F x(t) + L w(t)\n$$\nwhere $x(t) \\in \\mathbb{R}^3$, $w(t)$ is a continuous-time white Gaussian noise process with $\\mathbb{E}[w(t)]=0$ and covariance $\\mathbb{E}[w(t) w(\\tau)] = q \\delta(t-\\tau)$, with $q > 0$. The matrices are given as:\n$$\nF = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad L = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n$$\nThe solution to this linear time-invariant (LTI) system over an interval $[t_k, t_{k+1}]$ is given by the variation of constants formula:\n$$\nx(t_{k+1}) = e^{F(t_{k+1}-t_k)} x(t_k) + \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau)} L w(\\tau) d\\tau\n$$\nWe consider a uniform sampling interval $T = t_{k+1}-t_k$. Let $x_k \\triangleq x(t_k) = x(kT)$. The equation becomes:\n$$\nx_{k+1} = e^{FT} x_k + \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau)} L w(\\tau) d\\tau\n$$\nThis is the discrete-time state-space model $x_{k+1} = A_d x_k + w_d_k$, where the state transition matrix $A_d$ and the discrete-time process noise vector $w_d_k$ are identified as:\n$$\nA_d = e^{FT}\n$$\n$$\nw_d_k = \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau)} L w(\\tau) d\\tau\n$$\nThe discrete-time process noise covariance, $Q_d$, is defined as the covariance of $w_d_k$. As $w(t)$ is zero-mean, $w_d_k$ is also zero-mean. Thus, $Q_d = \\mathbb{E}[w_d_k w_d_k^T]$.\n$$\nQ_d = \\mathbb{E} \\left[ \\left( \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau_1)} L w(\\tau_1) d\\tau_1 \\right) \\left( \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau_2)} L w(\\tau_2) d\\tau_2 \\right)^T \\right]\n$$\nBy moving the expectation inside the integrals and using the linearity of the transpose operator, we obtain:\n$$\nQ_d = \\int_{kT}^{(k+1)T} \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau_1)} L \\mathbb{E}[w(\\tau_1)w(\\tau_2)] L^T \\left(e^{F((k+1)T-\\tau_2)}\\right)^T d\\tau_1 d\\tau_2\n$$\nSubstituting $\\mathbb{E}[w(\\tau_1)w(\\tau_2)] = q \\delta(\\tau_1 - \\tau_2)$:\n$$\nQ_d = \\int_{kT}^{(k+1)T} \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau_1)} L (q \\delta(\\tau_1 - \\tau_2)) L^T e^{F^T((k+1)T-\\tau_2)} d\\tau_1 d\\tau_2\n$$\nUsing the sifting property of the Dirac delta function to evaluate the integral over $\\tau_1$:\n$$\nQ_d = q \\int_{kT}^{(k+1)T} e^{F((k+1)T-\\tau_2)} L L^T e^{F^T((k+1)T-\\tau_2)} d\\tau_2\n$$\nLet us perform a change of variables $s = (k+1)T - \\tau_2$. Then $ds = -d\\tau_2$. The limits of integration become $s=T$ for $\\tau_2=kT$ and $s=0$ for $\\tau_2=(k+1)T$.\n$$\nQ_d = q \\int_{T}^{0} e^{Fs} L L^T e^{F^T s} (-ds) = q \\int_{0}^{T} e^{Fs} L L^T e^{F^T s} ds\n$$\nThis is the fundamental integral form for the discrete-time process noise covariance. The Van Loan method provides a numerical technique to compute both $A_d$ and $Q_d$ simultaneously by calculating the exponential of an augmented matrix. We will justify this method from first principles.\n\nConsider the augmented matrix $\\mathcal{N} = \\begin{pmatrix} -F & qLL^T \\\\ 0 & F^T \\end{pmatrix}$. We examine the matrix exponential $\\Phi(T) = \\exp(\\mathcal{N}T)$. Let $\\Phi(t) = \\exp(\\mathcal{N}t) = \\begin{pmatrix} \\Phi_{11}(t) & \\Phi_{12}(t) \\\\ \\Phi_{21}(t) & \\Phi_{22}(t) \\end{pmatrix}$. This matrix is the state transition matrix for the system $\\dot{Z}(t) = \\mathcal{N}Z(t)$. From the block structure of $\\mathcal{N}$, we can write the differential equations for the blocks of $\\Phi(t)$ with initial conditions $\\Phi(0)=I$:\n$\\dot{\\Phi}_{11} = -F\\Phi_{11}$, with $\\Phi_{11}(0)=I \\implies \\Phi_{11}(t) = e^{-Ft}$.\n$\\dot{\\Phi}_{22} = F^T\\Phi_{22}$, with $\\Phi_{22}(0)=I \\implies \\Phi_{22}(t) = e^{F^T t}$.\n$\\dot{\\Phi}_{12} = -F\\Phi_{12} + qLL^T\\Phi_{22}$, with $\\Phi_{12}(0)=0$.\nSubstituting $\\Phi_{22}(t)$, we have $\\dot{\\Phi}_{12}(t) = -F\\Phi_{12}(t) + qLL^T e^{F^T t}$. The solution is:\n$$\n\\Phi_{12}(t) = \\int_0^t e^{-F(t-s)} qLL^T e^{F^T s} ds = e^{-Ft} \\int_0^t e^{Fs} qLL^T e^{F^T s} ds\n$$\nAt time $t=T$, we have $\\Phi_{12}(T) = e^{-FT} \\left( q \\int_0^T e^{Fs} L L^T e^{F^T s} ds \\right) = e^{-FT} Q_d$.\nThus, $Q_d = (e^{-FT})^{-1} \\Phi_{12}(T) = e^{FT} \\Phi_{12}(T) = A_d \\Phi_{12}(T)$. Also, $A_d = e^{FT} = (\\Phi_{22}(T)^T)^{-1}$. But $e^{F^T T} = (e^{FT})^T=A_d^T$, so $\\Phi_{22}(T) = A_d^T$ and $\\Phi_{11}(T) = A_d^{-1}$.\nThe Van Loan method, therefore, encodes the integral for $Q_d$ within the block $\\Phi_{12}$ of $\\exp(\\mathcal{N}T)$.\n\nFor this specific problem, the matrix $F$ is nilpotent. This property simplifies calculations significantly.\n$F^2 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$, and $F^3 = 0$.\nThe matrix exponential for $A_d$ can be computed via its Taylor series, which terminates:\n$$\nA_d = e^{FT} = I + FT + \\frac{(FT)^2}{2!} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + T\\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\frac{T^2}{2}\\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & T & \\frac{T^2}{2} \\\\ 0 & 1 & T \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe nilpotency also simplifies the integrand for $Q_d$. The term $e^{Fs}$ becomes a simple polynomial in $s$:\n$$\ne^{Fs} = I + Fs + \\frac{F^2s^2}{2} = \\begin{pmatrix} 1 & s & \\frac{s^2}{2} \\\\ 0 & 1 & s \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nWe now directly evaluate the integral for $Q_d$, which is the most efficient application of the justified formula.\n$$\nQ_d = q \\int_0^T (e^{Fs}L)(e^{Fs}L)^T ds\n$$\nFirst, we compute the vector $e^{Fs}L$:\n$$\ne^{Fs}L = \\begin{pmatrix} 1 & s & \\frac{s^2}{2} \\\\ 0 & 1 & s \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{s^2}{2} \\\\ s \\\\ 1 \\end{pmatrix}\n$$\nThe integrand is the outer product $(e^{Fs}L)(e^{Fs}L)^T$:\n$$\nq \\begin{pmatrix} \\frac{s^2}{2} \\\\ s \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\frac{s^2}{2} & s & 1 \\end{pmatrix} = q \\begin{pmatrix} \\frac{s^4}{4} & \\frac{s^3}{2} & \\frac{s^2}{2} \\\\ \\frac{s^3}{2} & s^2 & s \\\\ \\frac{s^2}{2} & s & 1 \\end{pmatrix}\n$$\nIntegrating this matrix element-wise from $0$ to $T$:\n$$\nQ_d = q \\int_0^T \\begin{pmatrix} \\frac{s^4}{4} & \\frac{s^3}{2} & \\frac{s^2}{2} \\\\ \\frac{s^3}{2} & s^2 & s \\\\ \\frac{s^2}{2} & s & 1 \\end{pmatrix} ds = q \\begin{pmatrix} \\frac{T^5}{20} & \\frac{T^4}{8} & \\frac{T^3}{6} \\\\ \\frac{T^4}{8} & \\frac{T^3}{3} & \\frac{T^2}{2} \\\\ \\frac{T^3}{6} & \\frac{T^2}{2} & T \\end{pmatrix}\n$$\nThe problem requires the scalar $s$, defined as the $(1,1)$ entry of $Q_d$.\n$$\ns = (Q_d)_{11} = q \\frac{T^5}{20}\n$$\nThis is the exact, closed-form expression for the requested quantity.", "answer": "$$\\boxed{q \\frac{T^{5}}{20}}$$", "id": "2912361"}, {"introduction": "A direct implementation of the Kalman filter's covariance update equations can be numerically unstable, leading to a filter that fails in practice despite being mathematically correct in theory. This problem explores this critical pitfall by contrasting the common, simplified covariance update with the more robust \"Joseph form\" [@problem_id:2912301]. Understanding how finite-precision arithmetic can cause the computed covariance matrix to lose its fundamental properties of symmetry and positive-semidefiniteness is essential for any practitioner building reliable estimation software.", "problem": "Consider the linear, time-invariant, discrete-time, Gaussian state-space model\n$$\nx_{k} = F x_{k-1} + G w_{k-1}, \\quad y_{k} = H x_{k} + v_{k},\n$$\nwith process noise $w_{k-1} \\sim \\mathcal{N}(0,Q)$ and measurement noise $v_{k} \\sim \\mathcal{N}(0,R)$, where $Q$ and $R$ are symmetric positive semidefinite (PSD) covariances. Let $P_{k|k-1}$ denote the a priori covariance and $P_{k|k}$ denote the a posteriori covariance. The Kalman gain is $K_{k}$, the identity matrix is $I$, and the innovation covariance is $S_{k} \\triangleq H P_{k|k-1} H^{\\top} + R$. Assume the usual Kalman filter setting in which $P_{k|k-1}$ and $R$ are symmetric PSD and $K_{k}$ is computed from $P_{k|k-1}$ and $R$.\n\nFrom the definition of covariance $P \\triangleq \\mathbb{E}\\{ e e^{\\top} \\}$ for an estimation error $e$, one can derive the covariance update that is a sum of symmetric PSD terms. In exact arithmetic and for the optimal Kalman gain $K_{k} = P_{k|k-1} H^{\\top} S_{k}^{-1}$, that covariance update is algebraically equivalent to the simplified form\n$$\nP_{k|k} = (I - K_{k} H)\\, P_{k|k-1}.\n$$\nHowever, in floating-point arithmetic, users often observe loss of exact symmetry and even loss of positive semidefiniteness in the simplified update. A widely used remedy is to compute the covariance using the so-called Joseph stabilized form, which is constructed as a sum of symmetric PSD terms.\n\nUsing only first principles about covariance propagation, properties of symmetric PSD matrices, and the effect of finite-precision arithmetic (roundoff and subtractive cancellation), answer the following. Which of the statements below correctly explain why the simplified covariance update can lose symmetry and positive semidefiniteness numerically, and how the Joseph form mitigates this?\n\nSelect all that apply.\n\nA. In floating-point arithmetic, the product $(I - K_{k} H) P_{k|k-1}$ does not enforce symmetry because the transpose is not applied on the right, and algebraic cancellations that guarantee nonnegativity in exact arithmetic are not realized numerically; small asymmetric roundoff and cancellation can produce slightly negative eigenvalues. The Joseph form explicitly constructs $P_{k|k}$ as a sum of symmetric PSD terms, which numerically preserves symmetry and positive semidefiniteness when $P_{k|k-1}$ and $R$ are PSD.\n\nB. The simplified update loses positive semidefiniteness only when the process noise covariance $Q$ is singular. Choosing $Q$ strictly positive definite guarantees that $(I - K_{k} H) P_{k|k-1}$ remains positive semidefinite, so the Joseph form is unnecessary if $Q \\succ 0$.\n\nC. In exact arithmetic with the optimal Kalman gain, the Joseph form and the simplified form are algebraically identical. In floating-point arithmetic, the identity can be numerically violated by roundoff and subtractive cancellation, so computing the Joseph form is more stable and tends to maintain the defining properties of a covariance matrix.\n\nD. The loss of symmetry in $(I - K_{k} H) P_{k|k-1}$ occurs because $H$ is typically rectangular. If $H$ were square and invertible, then $(I - K_{k} H) P_{k|k-1}$ would be symmetric by construction, and the issue would disappear.\n\nE. Post hoc symmetrization by replacing $P_{k|k}$ with $\\tfrac{1}{2}\\big[(I - K_{k} H) P_{k|k-1} + \\big((I - K_{k} H) P_{k|k-1}\\big)^{\\top}\\big]$ is mathematically equivalent to the Joseph form, so it fully restores positive semidefiniteness without needing the Joseph construction.\n\nF. When the innovation covariance $S_{k}$ is ill-conditioned, forming $(I - K_{k} H) P_{k|k-1}$ can suffer large relative error from subtractive cancellation between $P_{k|k-1}$ and $K_{k} H P_{k|k-1}$; the Joseph form avoids this specific cancellation by assembling $P_{k|k}$ as a sum of Gram-type terms that are individually symmetric PSD, thereby reducing the risk of spurious negative eigenvalues.\n\nChoose the correct option(s).", "solution": "The problem statement is subjected to validation and is found to be valid. It presents a standard, well-posed problem in the field of signal processing concerning the numerical stability of the Kalman filter covariance update equations. The premises are scientifically sound and the terminology is precise. We may therefore proceed with a rigorous analysis.\n\nThe foundation of the measurement update for the a posteriori error covariance, $P_{k|k}$, is derived from the definition of the a posteriori estimation error, $e_{k|k} \\triangleq x_k - \\hat{x}_{k|k}$. Using the standard Kalman filter equations, this error propagates as $e_{k|k} = (I - K_k H) e_{k|k-1} - K_k v_k$. The covariance is then derived by taking the expectation of the outer product, $P_{k|k} = \\mathbb{E}\\{e_{k|k} e_{k|k}^\\top\\}$. Assuming the a priori error $e_{k|k-1}$ and the measurement noise $v_k$ are uncorrelated, this yields:\n$$ P_{k|k} = (I - K_k H) \\mathbb{E}\\{e_{k|k-1} e_{k|k-1}^\\top\\} (I - K_k H)^\\top + K_k \\mathbb{E}\\{v_k v_k^\\top\\} K_k^\\top $$\nSubstituting $\\mathbb{E}\\{e_{k|k-1} e_{k|k-1}^\\top\\} = P_{k|k-1}$ and $\\mathbb{E}\\{v_k v_k^\\top\\} = R$, we obtain the numerically robust **Joseph stabilized form**:\n$$ P_{k|k} = (I - K_k H) P_{k|k-1} (I - K_k H)^\\top + K_k R K_k^\\top $$\nThis form is a sum of two matrices. Given that $P_{k|k-1}$ and $R$ are symmetric positive semidefinite (PSD), the terms $(I - K_k H) P_{k|k-1} (I - K_k H)^\\top$ and $K_k R K_k^\\top$ are also symmetric PSD. The sum of two symmetric PSD matrices is guaranteed to be symmetric and PSD. This structure inherently preserves the mathematical properties of a covariance matrix, even in finite-precision arithmetic.\n\nFor the optimal Kalman gain, $K_{k} = P_{k|k-1} H^{\\top} S_{k}^{-1}$, where $S_{k} = H P_{k|k-1} H^{\\top} + R$, the Joseph form can be algebraically simplified.\n$$ P_{k|k} = P_{k|k-1} - K_k H P_{k|k-1} - P_{k|k-1} H^\\top K_k^\\top + K_k (H P_{k|k-1} H^\\top + R) K_k^\\top $$\n$$ P_{k|k} = P_{k|k-1} - K_k H P_{k|k-1} - P_{k|k-1} H^\\top K_k^\\top + K_k S_k K_k^\\top $$\nSubstituting $K_k S_k = P_{k|k-1} H^\\top$ into the final term gives $K_k S_k K_k^\\top = P_{k|k-1} H^\\top K_k^\\top$. The expression becomes:\n$$ P_{k|k} = P_{k|k-1} - K_k H P_{k|k-1} - P_{k|k-1} H^\\top K_k^\\top + P_{k|k-1} H^\\top K_k^\\top = P_{k|k-1} - K_k H P_{k|k-1} $$\nThis leads to the **simplified form** stated in the problem:\n$$ P_{k|k} = (I - K_{k} H) P_{k|k-1} $$\nWhile algebraically equivalent to the Joseph form in exact arithmetic, its numerical properties are inferior. The reason is twofold:\n$1$. **Loss of Symmetry**: In floating-point arithmetic, the computed matrix product $(I - \\tilde{K}_{k} H) \\tilde{P}_{k|k-1}$ is not guaranteed to be symmetric, as the matrix $(I - \\tilde{K}_{k} H)$ is generally not symmetric and the algebraic identity $K_k H P_{k|k-1} = P_{k|k-1} H^\\top K_k^\\top$ that ensures symmetry in exact arithmetic is broken by roundoff errors.\n$2$. **Loss of Positive Semidefiniteness**: The form $P_{k|k} = P_{k|k-1} - K_k H P_{k|k-1}$ involves a subtraction. When the measurement is informative (e.g., small $R$), the correction term $K_k H P_{k|k-1}$ becomes very close to the a priori covariance $P_{k|k-1}$. The subtraction of two large, nearly-equal matrices leads to subtractive cancellation, a catastrophic loss of relative precision. This numerical error can result in a computed matrix $\\tilde{P}_{k|k}$ that has small negative eigenvalues, thus failing to be positive semidefinite.\n\nWe now evaluate each statement based on this analysis.\n\nA. In floating-point arithmetic, the product $(I - K_{k} H) P_{k|k-1}$ does not enforce symmetry because the transpose is not applied on the right, and algebraic cancellations that guarantee nonnegativity in exact arithmetic are not realized numerically; small asymmetric roundoff and cancellation can produce slightly negative eigenvalues. The Joseph form explicitly constructs $P_{k|k}$ as a sum of symmetric PSD terms, which numerically preserves symmetry and positive semidefiniteness when $P_{k|k-1}$ and $R$ are PSD.\nThis statement is correct. It precisely identifies that the structure of the simplified form does not enforce symmetry computationally. It correctly notes that the algebraic properties guaranteeing non-negativity in exact arithmetic (which are manifest in the Joseph form) are lost in the simplified form, leading to potential negative eigenvalues from numerical errors. It accurately describes the Joseph form's structural advantage.\nVerdict: **Correct**.\n\nB. The simplified update loses positive semidefiniteness only when the process noise covariance $Q$ is singular. Choosing $Q$ strictly positive definite guarantees that $(I - K_{k} H) P_{k|k-1}$ remains positive semidefinite, so the Joseph form is unnecessary if $Q \\succ 0$.\nThis statement is incorrect. While a larger, well-conditioned $P_{k|k-1}$ (which is promoted by a positive definite $Q$) can mitigate the numerical issues, it does not eliminate them. The problem of subtractive cancellation is most severe when measurements are very precise ($R$ is small), irrespective of whether $Q$ is singular or not. The numerical instability is inherent to the update formula's structure, not solely dependent on the singularity of $Q$. The Joseph form may still be necessary even with $Q \\succ 0$.\nVerdict: **Incorrect**.\n\nC. In exact arithmetic with the optimal Kalman gain, the Joseph form and the simplified form are algebraically identical. In floating-point arithmetic, the identity can be numerically violated by roundoff and subtractive cancellation, so computing the Joseph form is more stable and tends to maintain the defining properties of a covariance matrix.\nThis statement is a correct and concise summary of the entire issue. The algebraic equivalence in exact arithmetic is a standard result. The core of the problem is that this equivalence breaks down in floating-point arithmetic due to different susceptibility to numerical errors. The conclusion that the Joseph form is more stable and better preserves the properties of a covariance matrix is accurate.\nVerdict: **Correct**.\n\nD. The loss of symmetry in $(I - K_{k} H) P_{k|k-1}$ occurs because $H$ is typically rectangular. If $H$ were square and invertible, then $(I - K_{k} H) P_{k|k-1}$ would be symmetric by construction, and the issue would disappear.\nThis statement is incorrect. The matrix $(I - K_k H)$ is generally not symmetric, regardless of whether $H$ is square or rectangular. The product of a non-symmetric matrix $(I - K_k H)$ and a symmetric matrix $P_{k|k-1}$ is not generally symmetric. Symmetry in exact arithmetic is a consequence of the specific value of the optimal gain $K_k$, not the general structure of the expression. Loss of symmetry is a numerical artifact that would persist even if $H$ were square and invertible.\nVerdict: **Incorrect**.\n\nE. Post hoc symmetrization by replacing $P_{k|k}$ with $\\tfrac{1}{2}\\big[(I - K_{k} H) P_{k|k-1} + \\big((I - K_{k} H) P_{k|k-1}\\big)^{\\top}\\big]$ is mathematically equivalent to the Joseph form, so it fully restores positive semidefiniteness without needing the Joseph construction.\nThis statement is incorrect. While this procedure, $P_{new} = \\frac{1}{2} (P_{old} + P_{old}^\\top)$, does enforce symmetry, it does not guarantee positive semidefiniteness. A matrix that has become indefinite due to numerical errors will not necessarily become positive semidefinite after symmetrization; it may still possess negative eigenvalues. This method is a heuristic fix for symmetry but is not a robust solution for restoring the required PSD property. It is not mathematically equivalent to the Joseph form in its numerical behavior or guarantees.\nVerdict: **Incorrect**.\n\nF. When the innovation covariance $S_{k}$ is ill-conditioned, forming $(I - K_{k} H) P_{k|k-1}$ can suffer large relative error from subtractive cancellation between $P_{k|k-1}$ and $K_{k} H P_{k|k-1}$; the Joseph form avoids this specific cancellation by assembling $P_{k|k}$ as a sum of Gram-type terms that are individually symmetric PSD, thereby reducing the risk of spurious negative eigenvalues.\nThis statement is correct. It provides a deep and accurate explanation of the mechanism for losing positive semidefiniteness. An ill-conditioned $S_k$ often corresponds to highly accurate measurements, which is precisely the scenario where the update term $K_k H P_{k|k-1}$ is close to $P_{k|k-1}$, leading to subtractive cancellation. The statement correctly identifies that the Joseph form's additive structure of \"Gram-type\" terms (i.e., of the form $APA^\\top$) circumvents this explicit subtraction, thus preserving the positive semidefinite property far more reliably.\nVerdict: **Correct**.", "answer": "$$\\boxed{ACF}$$", "id": "2912301"}, {"introduction": "Beyond numerical precision, a Kalman filter's stability depends on the structural properties of the system it is estimating. This hands-on coding exercise demonstrates how a perfectly implemented filter can still diverge if the system has an unstable mode that is unobservable through the available measurements—a condition known as non-detectability [@problem_id:2912311]. By simulating this scenario, you will gain concrete insight into how this abstract system-theoretic concept has profound and direct consequences on real-world filter performance and boundedness of the estimation error.", "problem": "Consider the discrete-time linear Gaussian state-space model with state vector $x_k \\in \\mathbb{R}^n$ and measurement $y_k \\in \\mathbb{R}^m$ given by\n$$\nx_{k+1} = A x_k + w_k,\\quad y_k = H x_k + v_k,\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$, $H \\in \\mathbb{R}^{m \\times n}$, $w_k \\sim \\mathcal{N}(0,Q)$ with $Q \\succeq 0$, and $v_k \\sim \\mathcal{N}(0,R)$ with $R \\succ 0$. The discrete-time Kalman filter uses recursive prediction-update based on this model to estimate the state from measurements. The pair $(A,H)$ is said to be detectable if every eigenvalue $\\lambda$ of $A$ with $|\\lambda| \\ge 1$ corresponds to an observable mode.\n\nYour task is to implement a program that constructs a clear example where $(A,H)$ is not detectable and quantitatively demonstrates that the Kalman filter's estimation error diverges for some initial conditions. You must also include contrasting detectable cases and a boundary case to form a test suite that probes different aspects of detectability and covariance design. You must start from fundamental definitions of the linear Gaussian model and conditional expectations and implement the standard discrete-time Kalman filter prediction-update algorithm derived from these principles. Do not assume any external packages beyond the ones explicitly permitted in the final answer specification.\n\nYou will simulate the true system with zero process noise and zero measurement noise, i.e., $w_k \\equiv 0$ and $v_k \\equiv 0$, so that any divergence arises from structural properties, not random perturbations. However, you must still choose positive semidefinite $Q$ and positive definite $R$ for the filter design to be well-posed and numerically meaningful; the prediction step uses $Q$ and the update step uses $R$. For the non-detectable example, choose $Q$ and $R$ so that the divergence due to an unobservable unstable mode is apparent from initial conditions alone.\n\nThe program must implement the Kalman filter recursion and compute, for each test case, the following two quantitative metrics after $N$ time steps:\n1. The ratio of the estimation error norm at the final time to that at the initial time,\n$$\n\\rho_e = \\frac{\\|e_N\\|_2}{\\|e_0\\|_2}, \\quad e_k = x_k - \\hat{x}_{k|k}.\n$$\n2. The ratio of the error covariance trace at the final time to that at the initial time (with a small positive regularization in the denominator to avoid division by zero if applicable),\n$$\n\\rho_P = \\frac{\\mathrm{tr}(P_N)}{\\mathrm{tr}(P_0) + \\varepsilon},\n$$\nwhere $\\varepsilon$ is a small positive scalar that you must fix in your code.\n\nThe test suite consists of four cases with $n=2$ and $m=1$, horizon $N=50$, and the following system parameters, initial conditions, and filter covariances:\n- Case 1 (non-detectable, diverging error): $A = \\mathrm{diag}(1.2, 0.9)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$, $Q = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}$, $R = \\begin{bmatrix}10^{-4}\\end{bmatrix}$, $x_0 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, $\\hat{x}_{0|0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $P_0 = \\mathrm{diag}(1,1)$.\n- Case 2 (detectable, convergent error; unstable mode observed): $A = \\mathrm{diag}(1.2, 0.9)$, $H = \\begin{bmatrix}1 & 0\\end{bmatrix}$, $Q = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}$, $R = \\begin{bmatrix}10^{-4}\\end{bmatrix}$, $x_0 = \\begin{bmatrix}1 \\\\ 0.5\\end{bmatrix}$, $\\hat{x}_{0|0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $P_0 = \\mathrm{diag}(1,1)$.\n- Case 3 (detectable, unobservable mode stable): $A = \\mathrm{diag}(0.8, 1.1)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$, $Q = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}$, $R = \\begin{bmatrix}10^{-4}\\end{bmatrix}$, $x_0 = \\begin{bmatrix}1 \\\\ 0.2\\end{bmatrix}$, $\\hat{x}_{0|0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $P_0 = \\mathrm{diag}(1,1)$.\n- Case 4 (boundary, marginally unstable unobservable mode): $A = \\mathrm{diag}(1.0, 0.9)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$, $Q = \\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}$, $R = \\begin{bmatrix}10^{-4}\\end{bmatrix}$, $x_0 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, $\\hat{x}_{0|0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $P_0 = \\mathrm{diag}(1,1)$.\n\nAll angles, if any appear, must be expressed in radians. There are no physical units in this problem. The final program must compute $(\\rho_e, \\rho_P)$ for each case and aggregate the $8$ real-valued results into a single line printed output as a flat list of $8$ floating-point numbers in the following order:\n$$\n[\\rho_e^{(1)}, \\rho_P^{(1)}, \\rho_e^{(2)}, \\rho_P^{(2)}, \\rho_e^{(3)}, \\rho_P^{(3)}, \\rho_e^{(4)}, \\rho_P^{(4)}].\n$$\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5,r6,r7,r8]\").", "solution": "The problem requires an analysis and implementation related to the stability of the discrete-time Kalman filter, contingent on the system's detectability. We begin by validating the problem statement and then proceed to a principled solution. The problem is scientifically grounded in control and estimation theory, is well-posed with all necessary parameters defined, and is objective. The provided test cases are constructed correctly to demonstrate the theoretical concepts of detectability and filter divergence. The problem is therefore deemed valid.\n\nWe consider the discrete-time linear Gaussian state-space model:\n$$\nx_{k+1} = A x_k + w_k\n$$\n$$\ny_k = H x_k + v_k\n$$\nwhere $k \\in \\mathbb{N}$ is the time index, $x_k \\in \\mathbb{R}^n$ is the state vector, and $y_k \\in \\mathbb{R}^m$ is the measurement vector. The system matrices are $A \\in \\mathbb{R}^{n \\times n}$ and $H \\in \\mathbb{R}^{m \\times n}$. The process noise $w_k$ and measurement noise $v_k$ are assumed to be independent, zero-mean, white Gaussian processes with covariances $w_k \\sim \\mathcal{N}(0, Q)$ and $v_k \\sim \\mathcal{N}(0, R)$, respectively, where $Q \\succeq 0$ and $R \\succ 0$.\n\nThe goal of filtering is to find the best estimate of the state $x_k$ given the history of measurements $Y_k = \\{y_1, y_2, \\dots, y_k\\}$. For a linear Gaussian system, the optimal estimate in the minimum mean square error sense is the conditional expectation $\\hat{x}_{k|k} = E[x_k | Y_k]$. The uncertainty of this estimate is characterized by the error covariance matrix $P_{k|k} = E[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T | Y_k]$.\n\nThe Kalman filter provides a recursive algorithm to compute $\\hat{x}_{k|k}$ and $P_{k|k}$. It consists of two steps: prediction and update. Starting with an initial estimate $\\hat{x}_{0|0}$ and covariance $P_{0|0}$, the filter iteratively computes for $k=1, 2, \\dots, N$:\n\n1.  **Prediction (Time Update):** This step projects the state and covariance estimates from time $k-1$ to time $k$.\n    $$\n    \\hat{x}_{k|k-1} = A \\hat{x}_{k-1|k-1}\n    $$\n    $$\n    P_{k|k-1} = A P_{k-1|k-1} A^T + Q\n    $$\n    Here, $\\hat{x}_{k|k-1}$ is the predicted state estimate at time $k$ given measurements up to time $k-1$, and $P_{k|k-1}$ is the predicted error covariance.\n\n2.  **Update (Measurement Update):** This step incorporates the new measurement $y_k$ to refine the predicted estimate.\n    $$\n    \\tilde{y}_k = y_k - H \\hat{x}_{k|k-1} \\quad (\\text{Innovation})\n    $$\n    $$\n    S_k = H P_{k|k-1} H^T + R \\quad (\\text{Innovation Covariance})\n    $$\n    $$\n    K_k = P_{k|k-1} H^T S_k^{-1} \\quad (\\text{Kalman Gain})\n    $$\n    $$\n    \\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k \\tilde{y}_k \\quad (\\text{Updated State Estimate})\n    $$\n    $$\n    P_{k|k} = (I - K_k H) P_{k|k-1} \\quad (\\text{Updated Error Covariance})\n    $$\n\nA fundamental property of the Kalman filter is that the estimation error covariance $P_{k|k}$ will be bounded for any initial condition $P_{0|0} \\succeq 0$ if and only if the pair $(A, H)$ is detectable. A system is detectable if every unstable mode (associated with an eigenvalue $\\lambda$ of $A$ where $|\\lambda| \\ge 1$) is observable. An eigenvector $v$ corresponding to an eigenvalue $\\lambda$ represents an observable mode if $Hv \\neq 0$. If an unstable mode is unobservable ($Hv=0$), the filter cannot \"see\" this mode through the measurements. If the initial state has an error component along this unobservable, unstable direction, the filter cannot correct it, and the error will grow according to the unstable dynamics of $A$. The covariance associated with this direction will also diverge.\n\nWe will now analyze the four test cases specified in the problem statement within this theoretical framework. The simulation will use a deterministic true system where $w_k \\equiv 0$ and $v_k \\equiv 0$, so that the true state evolves as $x_{k+1} = A x_k$ and measurements are $y_k = H x_k$. This isolates the structural properties of the filter from stochastic effects. The filter itself is designed with $Q=0$ and a small $R > 0$.\n\nLet's examine the system properties for each case ($n=2, m=1, N=50$):\n\n**Case 1: Non-detectable, Diverging Error**\n-   $A = \\mathrm{diag}(1.2, 0.9)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The eigenvalues are $\\lambda_1 = 1.2$ (unstable) and $\\lambda_2 = 0.9$ (stable). The corresponding eigenvectors are $v_1 = [1, 0]^T$ and $v_2 = [0, 1]^T$.\n-   Observability of the unstable mode: $H v_1 = \\begin{bmatrix}0 & 1\\end{bmatrix} [1, 0]^T = 0$. The unstable mode is unobservable. The system is not detectable.\n-   The initial state is $x_0 = [1, 0]^T$, purely in the unobservable, unstable direction. The initial estimate is $\\hat{x}_{0|0} = [0, 0]^T$. The initial error $e_0 = x_0$ is along this mode. The filter receives measurements $y_k = H A^k x_0 = 0$ for all $k$, providing no information about the first state component. The estimation error $e_k$ is expected to grow as $1.2^k$. The error norm ratio $\\rho_e$ and covariance trace ratio $\\rho_P$ should be significantly greater than $1$.\n\n**Case 2: Detectable, Convergent Error**\n-   $A = \\mathrm{diag}(1.2, 0.9)$, $H = \\begin{bmatrix}1 & 0\\end{bmatrix}$. The eigenvalues and eigenvectors are the same as in Case $1$.\n-   Observability of the unstable mode: $H v_1 = \\begin{bmatrix}1 & 0\\end{bmatrix} [1, 0]^T = 1 \\neq 0$. The unstable mode is observable. The system is detectable.\n-   Although the stable mode is unobservable ($H v_2 = 0$), this does not violate detectability. The filter can correct any error in the unstable direction. Any error in the stable, unobservable direction will decay naturally. The estimation error and covariance are expected to converge. We anticipate $\\rho_e < 1$ and $\\rho_P < 1$.\n\n**Case 3: Detectable, Stable Unobservable Mode**\n-   $A = \\mathrm{diag}(0.8, 1.1)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The eigenvalues are $\\lambda_1 = 0.8$ (stable) and $\\lambda_2 = 1.1$ (unstable).\n-   The unstable mode corresponds to eigenvector $v_2 = [0, 1]^T$. Its observability is checked by $H v_2 = \\begin{bmatrix}0 & 1\\end{bmatrix} [0, 1]^T = 1 \\neq 0$. The unstable mode is observable. The system is detectable.\n-   Similar to Case $2$, the filter is stable. The unobservable mode corresponds to the stable eigenvalue $\\lambda_1 = 0.8$. Any initial error in this direction will decay. We expect $\\rho_e < 1$ and $\\rho_P < 1$.\n\n**Case 4: Boundary Case, Marginally Unstable Unobservable Mode**\n-   $A = \\mathrm{diag}(1.0, 0.9)$, $H = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The eigenvalues are $\\lambda_1 = 1.0$ (marginally unstable) and $\\lambda_2 = 0.9$ (stable).\n-   Observability of the marginally unstable mode: $H v_1 = \\begin{bmatrix}0 & 1\\end{bmatrix} [1, 0]^T = 0$. This mode is unobservable, so the system is not detectable.\n-   The initial error is $e_0 = [1, 0]^T$. The true state $x_k = A^k x_0 = x_0$ is constant. Measurements are $y_k = H x_k = 0$. The estimate $\\hat{x}_{k|k}$ remains at zero. Thus, the error $e_k = x_0$ is constant. The error norm will not change, so we expect $\\rho_e = 1$. The component of the covariance matrix corresponding to the unobservable mode does not grow since $\\lambda_1=1.0$ and $Q_{11}=0$. However, the covariance of the observable mode will decrease. The total trace of the covariance matrix will decrease, so we predict $\\rho_P < 1$.\n\nThe simulation will proceed by initializing the system and filter for each case, running the recursions for $N=50$ steps, and then computing the specified metrics:\n-   Estimation error norm ratio: $\\rho_e = \\|e_N\\|_2 / \\|e_0\\|_2$, with $e_k = x_k - \\hat{x}_{k|k}$.\n-   Error covariance trace ratio: $\\rho_P = \\mathrm{tr}(P_N) / (\\mathrm{tr}(P_0) + \\varepsilon)$ with a regularization term $\\varepsilon = 10^{-12}$.\n\nThe implementation will compute these values for all four cases and report the results as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_kalman_simulation(A, H, Q, R, x0, x0_hat, P0, N):\n    \"\"\"\n    Simulates a linear system and applies a Kalman filter.\n\n    The true system dynamics are deterministic (noise-free), while the filter\n    uses the provided Q and R matrices.\n    \"\"\"\n    # Initialize true state and filter state\n    x = x0.copy()\n    x_hat = x0_hat.copy()\n    P = P0.copy()\n    \n    # Store initial values for metric calculation\n    x_initial = x0.copy()\n    x_hat_initial = x0_hat.copy()\n    P_initial = P0.copy()\n\n    # Identity matrix for covariance update\n    n = A.shape[0]\n    I = np.identity(n)\n\n    # Main filter loop over N time steps\n    for _ in range(N):\n        # 1. Prediction (Time Update)\n        x_hat_pred = A @ x_hat\n        P_pred = A @ P @ A.T + Q\n\n        # 2. True system evolution (deterministic)\n        x = A @ x\n        y = H @ x\n\n        # 3. Update (Measurement Update)\n        innovation = y - H @ x_hat_pred\n        S = H @ P_pred @ H.T + R\n        K = P_pred @ H.T @ np.linalg.inv(S)\n        \n        x_hat = x_hat_pred + K @ innovation\n        P = (I - K @ H) @ P_pred\n\n    # Final values after N steps\n    x_final = x\n    x_hat_final = x_hat\n    P_final = P\n\n    # Calculate metrics\n    e_0 = x_initial - x_hat_initial\n    e_N = x_final - x_hat_final\n\n    norm_e0 = np.linalg.norm(e_0)\n    norm_eN = np.linalg.norm(e_N)\n\n    # Avoid division by zero if initial error is zero\n    rho_e = norm_eN / norm_e0 if norm_e0 > 1e-9 else float('inf')\n\n    trace_P0 = np.trace(P_initial)\n    trace_PN = np.trace(P_final)\n    \n    epsilon = 1e-12\n    rho_P = trace_PN / (trace_P0 + epsilon)\n\n    return rho_e, rho_P\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the specified metrics for each.\n    \"\"\"\n    N = 50\n    # Common parameters for most cases\n    Q_common = np.array([[0.0, 0.0], [0.0, 0.0]])\n    R_common = np.array([[1e-4]])\n    x0_hat_common = np.array([[0.0], [0.0]])\n    P0_common = np.identity(2)\n    \n    test_cases = [\n        # Case 1 (non-detectable, diverging error)\n        {\n            \"A\": np.diag([1.2, 0.9]),\n            \"H\": np.array([[0.0, 1.0]]),\n            \"Q\": Q_common,\n            \"R\": R_common,\n            \"x0\": np.array([[1.0], [0.0]]),\n            \"x0_hat\": x0_hat_common,\n            \"P0\": P0_common,\n            \"N\": N\n        },\n        # Case 2 (detectable, convergent error)\n        {\n            \"A\": np.diag([1.2, 0.9]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"Q\": Q_common,\n            \"R\": R_common,\n            \"x0\": np.array([[1.0], [0.5]]),\n            \"x0_hat\": x0_hat_common,\n            \"P0\": P0_common,\n            \"N\": N\n        },\n        # Case 3 (detectable, unobservable mode stable)\n        {\n            \"A\": np.diag([0.8, 1.1]),\n            \"H\": np.array([[0.0, 1.0]]),\n            \"Q\": Q_common,\n            \"R\": R_common,\n            \"x0\": np.array([[1.0], [0.2]]),\n            \"x0_hat\": x0_hat_common,\n            \"P0\": P0_common,\n            \"N\": N\n        },\n        # Case 4 (boundary, marginally unstable unobservable mode)\n        {\n            \"A\": np.diag([1.0, 0.9]),\n            \"H\": np.array([[0.0, 1.0]]),\n            \"Q\": Q_common,\n            \"R\": R_common,\n            \"x0\": np.array([[1.0], [0.0]]),\n            \"x0_hat\": x0_hat_common,\n            \"P0\": P0_common,\n            \"N\": N\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        rho_e, rho_P = run_kalman_simulation(\n            case[\"A\"], case[\"H\"], case[\"Q\"], case[\"R\"],\n            case[\"x0\"], case[\"x0_hat\"], case[\"P0\"], case[\"N\"]\n        )\n        results.append(rho_e)\n        results.append(rho_P)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2912311"}]}