{"hands_on_practices": [{"introduction": "Before we can balance a system, we must understand what we are balancing: the controllability and observability Gramians. This exercise takes you back to first principles, connecting the integral definitions of the Gramians, which represent system energies, to the computationally convenient Lyapunov equations. By working through both derivations for a simple system, you will gain a deep appreciation for the theory underlying all balanced truncation methods and verify the equivalence of these two fundamental characterizations [@problem_id:2854283].", "problem": "Consider the continuous-time linear time-invariant (LTI) state-space system defined by\n$$\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t),$$\nwith\n$$A = \\begin{pmatrix} -2 & 1 \\\\ 0 & -3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 2 \\end{pmatrix}.$$\nYou are to work from first principles of state-transition matrices and energy Gramians used in balanced truncation. Proceed as follows:\n\n1. Justify that the matrix $A$ is Hurwitz by identifying its eigenvalues using only the upper-triangular structure and the fundamental definition of eigenvalues.\n\n2. Starting from the definition of the controllability Gramian\n$$P \\triangleq \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top}\\, \\exp(A^{\\top} t)\\, dt,$$\ncompute $\\exp(A t)$ directly from the series definition of the matrix exponential and elementary properties of exponentials of triangular matrices, and evaluate the integral explicitly to obtain $P_{\\mathrm{int}}$ in closed form.\n\n3. Using only the identity $\\frac{d}{dt}\\exp(A t) = A \\exp(A t)$, the product rule, and the fact that $A$ is Hurwitz (so that boundary terms vanish at $t \\to \\infty$), derive the continuous-time Lyapunov equation satisfied by $P$ and solve it algebraically for $P_{\\mathrm{lyap}}$.\n\n4. Repeat steps $2$ and $3$ for the observability Gramian\n$$Q \\triangleq \\int_{0}^{\\infty} \\exp(A^{\\top} t)\\, C^{\\top} C\\, \\exp(A t)\\, dt,$$\nobtaining $Q_{\\mathrm{int}}$ by explicit integration and $Q_{\\mathrm{lyap}}$ by solving the associated Lyapunov equation.\n\n5. Let $\\|\\cdot\\|_{F}$ denote the Frobenius norm defined by $\\|M\\|_{F}^{2} \\triangleq \\operatorname{trace}(M^{\\top} M)$. Compute the scalar\n$$S \\triangleq \\|P_{\\mathrm{int}} - P_{\\mathrm{lyap}}\\|_{F}^{2} \\;+\\; \\|Q_{\\mathrm{int}} - Q_{\\mathrm{lyap}}\\|_{F}^{2}.$$\n\nProvide your final answer as a single real number. No rounding is required, and no physical units are involved.", "solution": "We begin with the continuous-time linear time-invariant system\n$$\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t),$$\nwhere\n$$A = \\begin{pmatrix} -2 & 1 \\\\ 0 & -3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 2 \\end{pmatrix}.$$\n\nStep 1: Hurwitz property of $A$.\nBecause $A$ is upper triangular, its eigenvalues are the diagonal entries by the fundamental definition of eigenvalues for triangular matrices. Thus the eigenvalues are $\\lambda_{1} = -2$ and $\\lambda_{2} = -3$. Both have strictly negative real parts, so $A$ is Hurwitz. This guarantees that the integrals defining the Gramians converge and that the associated Lyapunov equations have unique positive definite solutions.\n\nStep 2: Controllability Gramian $P$ by explicit integration.\nWe compute the state transition matrix $\\exp(A t)$ using the series definition and the property that for an upper-triangular $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ 0 & d \\end{pmatrix}$ with $a \\neq d$, one has\n$$\\exp(A t) = \\begin{pmatrix} \\exp(a t) & b \\frac{\\exp(a t) - \\exp(d t)}{a - d} \\\\ 0 & \\exp(d t) \\end{pmatrix}.$$\nHere $a = -2$, $d = -3$, $b = 1$, hence\n$$\\exp(A t) = \\begin{pmatrix} \\exp(-2 t) & \\exp(-2 t) - \\exp(-3 t) \\\\ 0 & \\exp(-3 t) \\end{pmatrix}.$$\nLet $E_{2}(t) \\triangleq \\exp(-2 t)$ and $E_{3}(t) \\triangleq \\exp(-3 t)$, and $f(t) \\triangleq E_{2}(t) - E_{3}(t)$. Then\n$$\\exp(A t) = \\begin{pmatrix} E_{2}(t) & f(t) \\\\ 0 & E_{3}(t) \\end{pmatrix}, \\quad \\exp(A^{\\top} t) = \\begin{pmatrix} E_{2}(t) & 0 \\\\ f(t) & E_{3}(t) \\end{pmatrix}.$$\nDefine $v(t) \\triangleq \\exp(A t) B$. Since $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n$$v(t) = \\begin{pmatrix} E_{2}(t) + f(t) \\\\ E_{3}(t) \\end{pmatrix} = \\begin{pmatrix} 2 E_{2}(t) - E_{3}(t) \\\\ E_{3}(t) \\end{pmatrix}.$$\nTherefore the integrand is\n$$\\exp(A t) B B^{\\top} \\exp(A^{\\top} t) = v(t) v(t)^{\\top} = \\begin{pmatrix} v_{1}(t)^{2} & v_{1}(t) v_{2}(t) \\\\ v_{1}(t) v_{2}(t) & v_{2}(t)^{2} \\end{pmatrix},$$\nwith $v_{1}(t) = 2 E_{2}(t) - E_{3}(t)$ and $v_{2}(t) = E_{3}(t)$. Expanding:\n\\begin{align*}\nv_{1}(t)^{2} &= \\left(2 E_{2}(t) - E_{3}(t)\\right)^{2} = 4 E_{2}(t)^{2} - 4 E_{2}(t) E_{3}(t) + E_{3}(t)^{2}, \\\\\nv_{1}(t) v_{2}(t) &= \\left(2 E_{2}(t) - E_{3}(t)\\right) E_{3}(t) = 2 E_{2}(t) E_{3}(t) - E_{3}(t)^{2}, \\\\\nv_{2}(t)^{2} &= E_{3}(t)^{2}.\n\\end{align*}\nIntegrating termwise over $t \\in [0,\\infty)$ and using $\\int_{0}^{\\infty} \\exp(-\\alpha t)\\, dt = \\frac{1}{\\alpha}$ for $\\alpha > 0$, we obtain:\n\\begin{align*}\n\\int_{0}^{\\infty} v_{1}(t)^{2}\\, dt &= 4 \\cdot \\frac{1}{4} - 4 \\cdot \\frac{1}{5} + 1 \\cdot \\frac{1}{6} = 1 - \\frac{4}{5} + \\frac{1}{6} = \\frac{11}{30}, \\\\\n\\int_{0}^{\\infty} v_{1}(t) v_{2}(t)\\, dt &= 2 \\cdot \\frac{1}{5} - 1 \\cdot \\frac{1}{6} = \\frac{2}{5} - \\frac{1}{6} = \\frac{7}{30}, \\\\\n\\int_{0}^{\\infty} v_{2}(t)^{2}\\, dt &= \\frac{1}{6}.\n\\end{align*}\nThus\n$$P_{\\mathrm{int}} = \\begin{pmatrix} \\frac{11}{30} & \\frac{7}{30} \\\\ \\frac{7}{30} & \\frac{1}{6} \\end{pmatrix}.$$\n\nStep 3: Controllability Gramian $P$ via Lyapunov equation.\nStarting from the definition\n$$P = \\int_{0}^{\\infty} \\exp(A t) B B^{\\top} \\exp(A^{\\top} t)\\, dt,$$\ndifferentiate the integrand with respect to $t$ using $\\frac{d}{dt}\\exp(A t) = A \\exp(A t)$ and the product rule:\n\\begin{align*}\n\\frac{d}{dt}\\left(\\exp(A t) B B^{\\top} \\exp(A^{\\top} t)\\right)\n&= A \\exp(A t) B B^{\\top} \\exp(A^{\\top} t) + \\exp(A t) B B^{\\top} \\exp(A^{\\top} t) A^{\\top}.\n\\end{align*}\nIntegrating from $0$ to $\\infty$ and using that $A$ is Hurwitz so that $\\lim_{t \\to \\infty} \\exp(A t) = 0$ and $\\exp(A 0) = I$, we obtain\n\\begin{align*}\n\\int_{0}^{\\infty} A \\exp(A t) B B^{\\top} \\exp(A^{\\top} t)\\, dt &+ \\int_{0}^{\\infty} \\exp(A t) B B^{\\top} \\exp(A^{\\top} t) A^{\\top}\\, dt \\\\\n&= - \\exp(A 0) B B^{\\top} \\exp(A^{\\top} 0) = - B B^{\\top}.\n\\end{align*}\nRecognizing $P$ inside the integrals yields the continuous-time Lyapunov equation\n$$A P + P A^{\\top} + B B^{\\top} = 0.$$\nLet $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$. Compute\n$$A P = \\begin{pmatrix} -2 & 1 \\\\ 0 & -3 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} -2 p_{11} + p_{12} & -2 p_{12} + p_{22} \\\\ -3 p_{12} & -3 p_{22} \\end{pmatrix},$$\n$$P A^{\\top} = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} -2 & 0 \\\\ 1 & -3 \\end{pmatrix} = \\begin{pmatrix} -2 p_{11} + p_{12} & -3 p_{12} \\\\ -2 p_{12} + p_{22} & -3 p_{22} \\end{pmatrix}.$$\nSumming and adding $B B^{\\top} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$ gives the linear system\n\\begin{align*}\n-4 p_{11} + 2 p_{12} + 1 &= 0, \\\\\n-5 p_{12} + p_{22} + 1 &= 0, \\\\\n-6 p_{22} + 1 &= 0.\n\\end{align*}\nSolve sequentially: from the third equation, $p_{22} = \\frac{1}{6}$. Then the second yields $-5 p_{12} + \\frac{1}{6} + 1 = 0 \\Rightarrow p_{12} = \\frac{7}{30}$. The first gives $-4 p_{11} + 2 \\cdot \\frac{7}{30} + 1 = 0 \\Rightarrow p_{11} = \\frac{11}{30}$. Hence\n$$P_{\\mathrm{lyap}} = \\begin{pmatrix} \\frac{11}{30} & \\frac{7}{30} \\\\ \\frac{7}{30} & \\frac{1}{6} \\end{pmatrix}.$$\n\nStep 4: Observability Gramian $Q$ by explicit integration and Lyapunov equation.\nBy definition,\n$$Q = \\int_{0}^{\\infty} \\exp(A^{\\top} t) C^{\\top} C \\exp(A t)\\, dt.$$\nLet $S \\triangleq C^{\\top} C = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix}$. Using the expressions for $\\exp(A t)$ and $\\exp(A^{\\top} t)$ from Step $2$,\n$$\\exp(A^{\\top} t) = \\begin{pmatrix} E_{2}(t) & 0 \\\\ f(t) & E_{3}(t) \\end{pmatrix}, \\quad \\exp(A t) = \\begin{pmatrix} E_{2}(t) & f(t) \\\\ 0 & E_{3}(t) \\end{pmatrix},$$\nwe compute the integrand\n\\begin{align*}\nM(t) &\\triangleq \\exp(A^{\\top} t) S \\exp(A t) \\\\\n&= \\begin{pmatrix} E_{2}(t) & 0 \\\\ f(t) & E_{3}(t) \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} E_{2}(t) & f(t) \\\\ 0 & E_{3}(t) \\end{pmatrix}.\n\\end{align*}\nFirst multiply on the left:\n$$\\exp(A^{\\top} t) S = \\begin{pmatrix} E_{2}(t) & 2 E_{2}(t) \\\\ f(t) + 2 E_{3}(t) & 2 f(t) + 4 E_{3}(t) \\end{pmatrix}.$$\nThen\n\\begin{align*}\nM_{11}(t) &= E_{2}(t) \\cdot E_{2}(t) + 2 E_{2}(t) \\cdot 0 = E_{2}(t)^{2}, \\\\\nM_{12}(t) &= E_{2}(t) \\cdot f(t) + 2 E_{2}(t) \\cdot E_{3}(t) = E_{2}(t)^{2} + E_{2}(t) E_{3}(t), \\\\\nM_{22}(t) &= \\left(f(t) + 2 E_{3}(t)\\right) f(t) + \\left(2 f(t) + 4 E_{3}(t)\\right) E_{3}(t) \\\\\n&= f(t)^{2} + 4 E_{3}(t) f(t) + 4 E_{3}(t)^{2}.\n\\end{align*}\nWith $f(t) = E_{2}(t) - E_{3}(t)$, this simplifies to\n\\begin{align*}\nM_{11}(t) &= E_{2}(t)^{2}, \\\\\nM_{12}(t) &= E_{2}(t)^{2} + E_{2}(t) E_{3}(t), \\\\\nM_{22}(t) &= E_{2}(t)^{2} + 2 E_{2}(t) E_{3}(t) + E_{3}(t)^{2}.\n\\end{align*}\nIntegrating termwise:\n\\begin{align*}\n\\int_{0}^{\\infty} E_{2}(t)^{2}\\, dt &= \\frac{1}{4}, \\\\\n\\int_{0}^{\\infty} E_{2}(t) E_{3}(t)\\, dt &= \\int_{0}^{\\infty} \\exp(-5 t)\\, dt = \\frac{1}{5}, \\\\\n\\int_{0}^{\\infty} \\left(E_{2}(t)^{2} + 2 E_{2}(t) E_{3}(t) + E_{3}(t)^{2}\\right)\\, dt &= \\frac{1}{4} + 2 \\cdot \\frac{1}{5} + \\frac{1}{6} = \\frac{49}{60}.\n\\end{align*}\nTherefore\n$$Q_{\\mathrm{int}} = \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} + \\frac{1}{5} \\\\ \\frac{1}{4} + \\frac{1}{5} & \\frac{49}{60} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & \\frac{9}{20} \\\\ \\frac{9}{20} & \\frac{49}{60} \\end{pmatrix}.$$\n\nFor the Lyapunov equation, differentiate the integrand using $\\frac{d}{dt}\\exp(A^{\\top} t) = A^{\\top} \\exp(A^{\\top} t)$ and the product rule, integrate over $[0,\\infty)$, and use the Hurwitz property to eliminate boundary terms. This yields\n$$A^{\\top} Q + Q A + C^{\\top} C = 0.$$\nLet $Q = \\begin{pmatrix} q_{11} & q_{12} \\\\ q_{12} & q_{22} \\end{pmatrix}$. Compute\n$$A^{\\top} Q = \\begin{pmatrix} -2 & 0 \\\\ 1 & -3 \\end{pmatrix} \\begin{pmatrix} q_{11} & q_{12} \\\\ q_{12} & q_{22} \\end{pmatrix} = \\begin{pmatrix} -2 q_{11} & -2 q_{12} \\\\ q_{11} - 3 q_{12} & q_{12} - 3 q_{22} \\end{pmatrix},$$\n$$Q A = \\begin{pmatrix} q_{11} & q_{12} \\\\ q_{12} & q_{22} \\end{pmatrix} \\begin{pmatrix} -2 & 1 \\\\ 0 & -3 \\end{pmatrix} = \\begin{pmatrix} -2 q_{11} & q_{11} - 3 q_{12} \\\\ -2 q_{12} & q_{12} - 3 q_{22} \\end{pmatrix}.$$\nSumming and adding $C^{\\top} C = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix}$ gives\n\\begin{align*}\n-4 q_{11} + 1 &= 0, \\\\\nq_{11} - 5 q_{12} + 2 &= 0, \\\\\n2 q_{12} - 6 q_{22} + 4 &= 0.\n\\end{align*}\nSolving, $q_{11} = \\frac{1}{4}$, then $-5 q_{12} + \\frac{1}{4} + 2 = 0 \\Rightarrow q_{12} = \\frac{9}{20}$, and $2 \\cdot \\frac{9}{20} - 6 q_{22} + 4 = 0 \\Rightarrow q_{22} = \\frac{49}{60}$. Hence\n$$Q_{\\mathrm{lyap}} = \\begin{pmatrix} \\frac{1}{4} & \\frac{9}{20} \\\\ \\frac{9}{20} & \\frac{49}{60} \\end{pmatrix}.$$\n\nStep 5: Scalar verification measure.\nDefine\n$$S \\triangleq \\|P_{\\mathrm{int}} - P_{\\mathrm{lyap}}\\|_{F}^{2} + \\|Q_{\\mathrm{int}} - Q_{\\mathrm{lyap}}\\|_{F}^{2}.$$\nFrom Steps $2$ and $3$, $P_{\\mathrm{int}} = P_{\\mathrm{lyap}}$. From Step $4$, $Q_{\\mathrm{int}} = Q_{\\mathrm{lyap}}$. Therefore each Frobenius norm is zero, and\n$$S = 0.$$\nThis completes the computation and the verification that the integral and Lyapunov characterizations agree for both Gramians, as required by the theory underlying balanced truncation for model reduction.", "answer": "$$\\boxed{0}$$", "id": "2854283"}, {"introduction": "With a solid grasp of Gramians, the next step is to compute the similarity transformation that balances them. This practice guides you through implementing the numerically preferred \"square-root method,\" which relies on Cholesky factorization and the Singular Value Decomposition (SVD) [@problem_id:2854301]. This is a vital computational skill for applying model reduction in practice and understanding how numerical stability is maintained in the balancing process.", "problem": "You are given symmetric positive-definite matrices that play the role of the controllability Gramian and the observability Gramian in a stable continuous-time Linear Time-Invariant (LTI) system. In model reduction by balanced truncation, a balancing similarity transformation is constructed so that, in the balanced coordinates, the transformed controllability and observability Gramians are equal and diagonal. Your task is to compute such a balancing transformation using the square-root method and to verify numerically that the transformed Gramians are nearly equal and diagonal.\n\nFundamental base:\n- For a stable continuous-time Linear Time-Invariant (LTI) system with state matrix $A$, input matrix $B$, and output matrix $C$, the controllability Gramian $P$ and the observability Gramian $Q$ are the unique symmetric positive-definite solutions to the Lyapunov equations $A P + P A^{\\top} + B B^{\\top} = 0$ and $A^{\\top} Q + Q A + C^{\\top} C = 0$, respectively, provided that the system is minimal (controllable and observable) and $A$ is Hurwitz (all eigenvalues have negative real parts).\n- A balancing transformation is a similarity transformation that renders $P$ and $Q$ equal and diagonal in the transformed coordinates.\n- The square-root method constructs a balancing transformation using Cholesky factorizations and the Singular Value Decomposition (SVD), relying only on the properties that $P$ and $Q$ are symmetric positive-definite.\n\nYour program must:\n- For each given pair $(P,Q)$, construct a balancing transformation using the square-root method, without using any formula that bypasses the derivation from Cholesky factors and the Singular Value Decomposition (SVD).\n- Form the transformed Gramians $P_{\\mathrm{bal}} = T P T^{\\top}$ and $Q_{\\mathrm{bal}} = T^{-\\top} Q T^{-1}$, where $T$ is your computed balancing transformation.\n- Compute two relative error metrics:\n  1. The diagonality error of $P_{\\mathrm{bal}}$, defined as the Frobenius-norm relative off-diagonal energy:\n  $$\\varepsilon_{\\mathrm{diag}} = \\dfrac{\\left\\|P_{\\mathrm{bal}} - \\operatorname{diag}\\left(\\operatorname{diag}\\left(P_{\\mathrm{bal}}\\right)\\right)\\right\\|_{F}}{\\left\\|P_{\\mathrm{bal}}\\right\\|_{F}}$$\n  2. The equality error between $P_{\\mathrm{bal}}$ and $Q_{\\mathrm{bal}}$, defined as:\n  $$\\varepsilon_{\\mathrm{eq}} = \\dfrac{\\left\\|P_{\\mathrm{bal}} - Q_{\\mathrm{bal}}\\right\\|_{F}}{\\left\\|P_{\\mathrm{bal}}\\right\\|_{F}}$$\n- For each test case, output a boolean that is `True` if both $\\varepsilon_{\\mathrm{diag}} \\leq 10^{-8}$ and $\\varepsilon_{\\mathrm{eq}} \\leq 10^{-8}$, and `False` otherwise.\n\nTest suite:\n- Case $1$ (general $2 \\times 2$):\n  - $P_{1} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 1 \\end{bmatrix}$,\n  - $Q_{1} = \\begin{bmatrix} 4 & 1.2 \\\\ 1.2 & 1 \\end{bmatrix}$.\n- Case $2$ (repeated Hankel singular values edge case, $3 \\times 3$):\n  - $P_{2} = I_{3}$,\n  - $Q_{2} = 4 I_{3}$.\n- Case $3$ (ill-conditioned but symmetric positive-definite, $4 \\times 4$): define lower-triangular Cholesky-like factors $L_{P}$ and $L_{Q}$,\n  $$L_{P} = \\begin{bmatrix}\n  10 & 0 & 0 & 0 \\\\\n  2 & 5 & 0 & 0 \\\\\n  0.5 & -1 & 2 & 0 \\\\\n  0.1 & 0.2 & 0.3 & 1\n  \\end{bmatrix}, \\quad\n  L_{Q} = \\begin{bmatrix}\n  10^{-3} & 0 & 0 & 0 \\\\\n  0.002 & 0.2 & 0 & 0 \\\\\n  0.001 & -0.01 & 0.05 & 0 \\\\\n  0 & 0.001 & 0.002 & 0.7\n  \\end{bmatrix},$$\n  and set $P_{3} = L_{P} L_{P}^{\\top}$, $Q_{3} = L_{Q} L_{Q}^{\\top}$.\n\nAngle units and physical units are not applicable.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, for example `[True,False,True]`.", "solution": "We start with a minimal stable continuous-time Linear Time-Invariant (LTI) system, for which the controllability Gramian $P$ and the observability Gramian $Q$ are symmetric positive-definite matrices solving the continuous-time Lyapunov equations. The goal of balanced truncation is to find a similarity transformation that renders $P$ and $Q$ equal and diagonal. The square-root method achieves this with the following reasoning.\n\nPrinciples and derivation:\n1. Because $P$ and $Q$ are symmetric positive-definite, they admit Cholesky factorizations. There exist lower-triangular matrices $L_{P}$ and $L_{Q}$ such that\n   $$P = L_{P} L_{P}^{\\top}, \\qquad Q = L_{Q} L_{Q}^{\\top}.$$\n   The existence and uniqueness (up to sign on the diagonal) of Cholesky factors follow from positive definiteness.\n\n2. Consider the product\n   $$M = L_{Q}^{\\top} L_{P}.$$\n   Compute the Singular Value Decomposition (SVD) of $M$:\n   $$M = U \\Sigma V^{\\top},$$\n   where $U$ and $V$ are orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ with singular values $\\sigma_{i} > 0$ because $M$ is nonsingular as a product of nonsingular factors $L_{Q}^{\\top}$ and $L_{P}$.\n\n3. Define the balancing similarity transformation $T$ and its inverse $T^{-1}$ by\n   $$T = \\Sigma^{-1/2} U^{\\top} L_{Q}^{\\top}, \\qquad T^{-1} = L_{P} V \\Sigma^{-1/2}.$$\n   These definitions are consistent because $\\Sigma$ is diagonal with strictly positive entries. The above expressions arise by demanding that the transformed Gramians be equal and diagonal and solving for a similarity that uses only the square roots of $P$ and $Q$ via their Cholesky factors combined with the SVD of $M$.\n\n4. Verification that the transformed Gramians are equal and diagonal:\n   Compute\n   $$P_{\\mathrm{bal}} = T P T^{\\top} = \\left(\\Sigma^{-1/2} U^{\\top} L_{Q}^{\\top}\\right)\\left(L_{P} L_{P}^{\\top}\\right)\\left(L_{Q} U \\Sigma^{-1/2}\\right).$$\n   Grouping factors,\n   $$P_{\\mathrm{bal}} = \\Sigma^{-1/2} U^{\\top} \\left(L_{Q}^{\\top} L_{P}\\right)\\left(L_{Q}^{\\top} L_{P}\\right)^{\\top} U \\Sigma^{-1/2}.$$\n   Because $L_{Q}^{\\top} L_{P} = U \\Sigma V^{\\top}$, we have\n   $$(L_{Q}^{\\top} L_{P})(L_{Q}^{\\top} L_{P})^{\\top} = (U \\Sigma V^{\\top})(V \\Sigma U^{\\top}) = U \\Sigma^{2} U^{\\top}.$$\n   Therefore,\n   $$P_{\\mathrm{bal}} = \\Sigma^{-1/2} U^{\\top} \\left(U \\Sigma^{2} U^{\\top}\\right) U \\Sigma^{-1/2} = \\Sigma^{-1/2} \\Sigma^{2} \\Sigma^{-1/2} = \\Sigma.$$\n   Similarly, with $T^{-T} = \\Sigma^{-1/2} V^{\\top} L_{P}^{\\top}$,\n   $$Q_{\\mathrm{bal}} = T^{-\\top} Q T^{-1} = \\left(\\Sigma^{-1/2} V^{\\top} L_{P}^{\\top}\\right)\\left(L_{Q} L_{Q}^{\\top}\\right)\\left(L_{P} V \\Sigma^{-1/2}\\right).$$\n   Grouping factors,\n   $$Q_{\\mathrm{bal}} = \\Sigma^{-1/2} V^{\\top} \\left(L_{P}^{\\top} L_{Q}\\right)\\left(L_{P}^{\\top} L_{Q}\\right)^{\\top} V \\Sigma^{-1/2}.$$\n   Since $L_{P}^{\\top} L_{Q} = (L_{Q}^{\\top} L_{P})^{\\top} = V \\Sigma U^{\\top}$, we obtain\n   $$(L_{P}^{\\top} L_{Q})(L_{P}^{\\top} L_{Q})^{\\top} = (V \\Sigma U^{\\top})(U \\Sigma V^{\\top}) = V \\Sigma^{2} V^{\\top},$$\n   whence\n   $$Q_{\\mathrm{bal}} = \\Sigma^{-1/2} V^{\\top} \\left(V \\Sigma^{2} V^{\\top}\\right) V \\Sigma^{-1/2} = \\Sigma.$$\n   Thus, in exact arithmetic, both transformed Gramians are identical and diagonal, with diagonal entries equal to the Hankel singular values (the singular values of $M$).\n\n5. Numerical verification metrics:\n   In floating-point arithmetic, roundoff errors produce small deviations. We evaluate\n   $$\\varepsilon_{\\mathrm{diag}} = \\frac{\\left\\|P_{\\mathrm{bal}} - \\operatorname{diag}\\left(\\operatorname{diag}\\left(P_{\\mathrm{bal}}\\right)\\right)\\right\\|_{F}}{\\left\\|P_{\\mathrm{bal}}\\right\\|_{F}}, \\quad \\varepsilon_{\\mathrm{eq}} = \\frac{\\left\\|P_{\\mathrm{bal}} - Q_{\\mathrm{bal}}\\right\\|_{F}}{\\left\\|P_{\\mathrm{bal}}\\right\\|_{F}},$$\n   and declare success if both are at most $10^{-8}$.\n\nAlgorithmic steps implemented by the program:\n- For each $(P,Q)$: compute Cholesky factors $L_{P}$ and $L_{Q}$ with $P = L_{P} L_{P}^{\\top}$ and $Q = L_{Q} L_{Q}^{\\top}$.\n- Form $M = L_{Q}^{\\top} L_{P}$ and compute its Singular Value Decomposition (SVD), $M = U \\Sigma V^{\\top}$.\n- Construct $T = \\Sigma^{-1/2} U^{\\top} L_{Q}^{\\top}$ and $T^{-1} = L_{P} V \\Sigma^{-1/2}$.\n- Form $P_{\\mathrm{bal}} = T P T^{\\top}$ and $Q_{\\mathrm{bal}} = T^{-\\top} Q T^{-1}$.\n- Compute $\\varepsilon_{\\mathrm{diag}}$ and $\\varepsilon_{\\mathrm{eq}}$ and compare to $10^{-8}$.\n- Output for each case a boolean indicating whether both criteria are satisfied.\n\nApplication to the test suite:\n- Case $1$: With $P_{1} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 1 \\end{bmatrix}$ and $Q_{1} = \\begin{bmatrix} 4 & 1.2 \\\\ 1.2 & 1 \\end{bmatrix}$, the procedure yields $P_{\\mathrm{bal}}$ and $Q_{\\mathrm{bal}}$ that coincide up to roundoff and are diagonal up to roundoff; we expect both errors below $10^{-8}$.\n- Case $2$: With $P_{2} = I_{3}$ and $Q_{2} = 4 I_{3}$, the singular values are repeated; the method still produces a diagonal $\\Sigma$ with equal entries, and both transformed Gramians equal $\\Sigma$ up to roundoff; errors should be below $10^{-8}$.\n- Case $3$: With ill-conditioned $P_{3}$ and $Q_{3}$ constructed from $L_{P}$ and $L_{Q}$ as specified, the same identities hold; due to conditioning, errors may be larger than in the previous cases, but are expected to remain below $10^{-8}$ using stable Cholesky and SVD.\n\nTherefore, the program outputs a list of three booleans, each expected to be `True` for the respective case if implemented correctly and executed in standard double-precision arithmetic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg as la\n\ndef balance_sqrt(P: np.ndarray, Q: np.ndarray):\n    \"\"\"\n    Compute the balancing transformation using the square-root method.\n    Returns T, T_inv, and the Hankel singular values (singular values of L_Q^T L_P).\n    \"\"\"\n    # Cholesky factors: P = Lp Lp^T, Q = Lq Lq^T (lower-triangular)\n    Lp = la.cholesky(P, lower=True, check_finite=True)\n    Lq = la.cholesky(Q, lower=True, check_finite=True)\n\n    # SVD of M = Lq^T Lp\n    M = Lq.T @ Lp\n    U, s, Vh = la.svd(M, full_matrices=False, check_finite=True)\n    # Construct diagonal inverse square root of Sigma\n    inv_sqrt_s = 1.0 / np.sqrt(s)\n    inv_sqrt_Sigma = np.diag(inv_sqrt_s)\n\n    # Balancing transformation and its inverse:\n    # T = Sigma^{-1/2} U^T Lq^T\n    # T^{-1} = Lp V Sigma^{-1/2}\n    T = inv_sqrt_Sigma @ U.T @ Lq.T\n    T_inv = Lp @ Vh.T @ inv_sqrt_Sigma\n    return T, T_inv, s\n\ndef transformed_gramians(P: np.ndarray, Q: np.ndarray, T: np.ndarray, T_inv: np.ndarray):\n    \"\"\"\n    Compute P_bal = T P T^T and Q_bal = T^{-T} Q T^{-1}.\n    \"\"\"\n    P_bal = T @ P @ T.T\n    Q_bal = T_inv.T @ Q @ T_inv\n    return P_bal, Q_bal\n\ndef diag_and_equality_errors(P_bal: np.ndarray, Q_bal: np.ndarray):\n    \"\"\"\n    Compute the relative off-diagonal Frobenius norm of P_bal and\n    the relative Frobenius norm of P_bal - Q_bal, both normalized by ||P_bal||_F.\n    \"\"\"\n    # Ensure symmetry numerically\n    P_bal = 0.5 * (P_bal + P_bal.T)\n    Q_bal = 0.5 * (Q_bal + Q_bal.T)\n    # Diagonality error\n    diag_P = np.diag(np.diag(P_bal))\n    denom = la.norm(P_bal, ord='fro')\n    # denom should be > 0 for SPD; guard anyway\n    if denom == 0:\n        denom = 1.0\n    diag_err = la.norm(P_bal - diag_P, ord='fro') / denom\n    # Equality error\n    eq_err = la.norm(P_bal - Q_bal, ord='fro') / denom\n    return diag_err, eq_err\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1: general 2x2 SPD\n    P1 = np.array([[3.0, 1.0],\n                   [1.0, 1.0]], dtype=float)\n    Q1 = np.array([[4.0, 1.2],\n                   [1.2, 1.0]], dtype=float)\n\n    # Case 2: repeated singular values, 3x3\n    P2 = np.eye(3, dtype=float)\n    Q2 = 4.0 * np.eye(3, dtype=float)\n\n    # Case 3: ill-conditioned but SPD via lower-triangular factors, 4x4\n    Lp3 = np.array([\n        [10.0, 0.0, 0.0, 0.0],\n        [ 2.0, 5.0, 0.0, 0.0],\n        [ 0.5,-1.0, 2.0, 0.0],\n        [ 0.1, 0.2, 0.3, 1.0]\n    ], dtype=float)\n    Lq3 = np.array([\n        [1e-3,   0.0,    0.0,   0.0],\n        [0.002,  0.2,    0.0,   0.0],\n        [0.001, -0.01,   0.05,  0.0],\n        [0.0,    0.001,  0.002, 0.7]\n    ], dtype=float)\n    P3 = Lp3 @ Lp3.T\n    Q3 = Lq3 @ Lq3.T\n\n    test_cases = [\n        (P1, Q1),\n        (P2, Q2),\n        (P3, Q3),\n    ]\n\n    tol = 1e-8\n    results = []\n    for P, Q in test_cases:\n        T, T_inv, _ = balance_sqrt(P, Q)\n        P_bal, Q_bal = transformed_gramians(P, Q, T, T_inv)\n        d_err, e_err = diag_and_equality_errors(P_bal, Q_bal)\n        results.append(d_err <= tol and e_err <= tol)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2854301"}, {"introduction": "Model reduction is a trade-off between simplicity and accuracy. This final exercise examines the consequences of balanced truncation by comparing a key system property—the direct-current (DC) gain—between an original and a reduced-order model [@problem_id:2854310]. You will see that balanced truncation does not generally preserve DC gain and learn how the observed error relates to the powerful $H_{\\infty}$ error bound, which is determined by the truncated Hankel singular values.", "problem": "Consider a stable, single-input single-output continuous-time linear time-invariant (LTI) system in a minimal realization with state-space matrices\n$$\nA=\\begin{pmatrix}\n-1 & 0 & 0 \\\\\n0 & -2 & 0 \\\\\n0 & 0 & -5\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n2 \\\\ 1 \\\\ -1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n1 & 2 & 5\n\\end{pmatrix},\\quad\nD=0.\n$$\nA standard balanced truncation of order $2$ is performed on a balanced realization of this system. The resulting reduced-order model has state-space matrices\n$$\nA_{r}=\\begin{pmatrix}\n-\\frac{6}{5} & 0 \\\\\n0 & -\\frac{5}{2}\n\\end{pmatrix},\\quad\nB_{r}=\\begin{pmatrix}\n\\frac{21}{10} \\\\ \\frac{4}{5}\n\\end{pmatrix},\\quad\nC_{r}=\\begin{pmatrix}\n\\frac{9}{10} & \\frac{23}{10}\n\\end{pmatrix},\\quad\nD_{r}=0.\n$$\nThe balanced realization used for truncation has Hankel singular values $\\sigma_{1}=3$, $\\sigma_{2}=\\frac{9}{10}$, and $\\sigma_{3}=\\frac{4}{25}$. For a continuous-time LTI system with transfer function $G(s)$, define the direct-current (DC) gain as the zero-frequency value $G(0)$. Using only fundamental definitions and well-tested facts from linear systems theory, derive an expression for the DC gain from the state-space data and compute the exact DC gains of the original and reduced models. Then compute the absolute discrepancy in DC gain between the original and reduced models.\n\nReport as your final answer the absolute discrepancy in DC gain as a single exact number with no units. No rounding is required. In your reasoning, also comment on whether this discrepancy is consistent with the standard balanced truncation Hardy space infinity norm (H-infinity) error bound based on the discarded Hankel singular value, but do not include that commentary in your final reported answer.", "solution": "The transfer function $G(s)$ of a continuous-time LTI system defined by the state-space quadruple $(A, B, C, D)$ is given by the expression $G(s) = C(sI - A)^{-1}B + D$.\nThe direct-current (DC) gain is defined as the value of the transfer function at zero frequency, $s=0$. Since the given system is stable, its state matrix $A$ is invertible, and the DC gain can be calculated as:\n$$\nG(0) = C(-A)^{-1}B + D = -CA^{-1}B + D\n$$\nWe apply this formula to both the original and the reduced-order systems.\n\n**DC Gain of the Original System**\nFor the original system, the matrices are:\n$$\nA = \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & -5 \\end{pmatrix},\\quad\nB = \\begin{pmatrix} 2 \\\\ 1 \\\\ -1 \\end{pmatrix},\\quad\nC = \\begin{pmatrix} 1 & 2 & 5 \\end{pmatrix},\\quad\nD = 0.\n$$\nSince $A$ is diagonal, its inverse is $A^{-1} = \\mathrm{diag}(-1, -1/2, -1/5)$. The DC gain is:\n$$\nG(0) = - \\begin{pmatrix} 1 & 2 & 5 \\end{pmatrix} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -1/2 & 0 \\\\ 0 & 0 & -1/5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\nFirst, computing $-CA^{-1}$:\n$$\n-CA^{-1} = - \\begin{pmatrix} 1(-1) & 2(-1/2) & 5(-1/5) \\end{pmatrix} = - \\begin{pmatrix} -1 & -1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}\n$$\nThen, completing the calculation:\n$$\nG(0) = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ -1 \\end{pmatrix} = 2 + 1 - 1 = 2\n$$\nThe DC gain of the original system is $G(0) = 2$.\n\n**DC Gain of the Reduced-Order System**\nFor the reduced system, the matrices are:\n$$\nA_{r}=\\begin{pmatrix} -\\frac{6}{5} & 0 \\\\ 0 & -\\frac{5}{2} \\end{pmatrix},\\quad\nB_{r}=\\begin{pmatrix} \\frac{21}{10} \\\\ \\frac{4}{5} \\end{pmatrix},\\quad\nC_{r}=\\begin{pmatrix} \\frac{9}{10} & \\frac{23}{10} \\end{pmatrix},\\quad\nD_{r}=0.\n$$\nThe inverse of $A_{r}$ is $A_{r}^{-1} = \\mathrm{diag}(-5/6, -2/5)$. The DC gain $G_r(0)$ is:\n$$\nG_{r}(0) = -C_{r}A_{r}^{-1}B_{r} = - \\begin{pmatrix} \\frac{9}{10} & \\frac{23}{10} \\end{pmatrix} \\begin{pmatrix} -5/6 & 0 \\\\ 0 & -2/5 \\end{pmatrix} \\begin{pmatrix} \\frac{21}{10} \\\\ \\frac{4}{5} \\end{pmatrix}\n$$\nComputing $-C_{r}A_{r}^{-1}$:\n$$\n-C_{r}A_{r}^{-1} = - \\begin{pmatrix} \\frac{9}{10}(-\\frac{5}{6}) & \\frac{23}{10}(-\\frac{2}{5}) \\end{pmatrix} = - \\begin{pmatrix} -3/4 & -23/25 \\end{pmatrix} = \\begin{pmatrix} 3/4 & 23/25 \\end{pmatrix}\n$$\nThen, the full product:\n$$\nG_{r}(0) = \\begin{pmatrix} 3/4 & 23/25 \\end{pmatrix} \\begin{pmatrix} \\frac{21}{10} \\\\ \\frac{4}{5} \\end{pmatrix} = \\frac{63}{40} + \\frac{92}{125}\n$$\nUsing a common denominator of 1000:\n$$\nG_{r}(0) = \\frac{63 \\cdot 25}{1000} + \\frac{92 \\cdot 8}{1000} = \\frac{1575 + 736}{1000} = \\frac{2311}{1000}\n$$\nThe DC gain of the reduced system is $G_{r}(0) = 2311/1000$.\n\n**Absolute Discrepancy**\nThe absolute discrepancy in the DC gain is:\n$$\n|G(0) - G_{r}(0)| = \\left|2 - \\frac{2311}{1000}\\right| = \\left|\\frac{2000 - 2311}{1000}\\right| = \\left|-\\frac{311}{1000}\\right| = \\frac{311}{1000}\n$$\n**Commentary on the Error Bound**\nThe standard balanced truncation error bound states that the infinity norm of the error, $\\|G - G_r\\|_{H_\\infty}$, is no more than twice the sum of the truncated Hankel singular values. In this case, a model of order 2 was created from a model of order 3, so only the smallest singular value, $\\sigma_3 = 4/25$, was truncated. The error bound is:\n$$\n\\|G - G_r\\|_{H_\\infty} \\le 2\\sigma_3 = 2 \\left(\\frac{4}{25}\\right) = \\frac{8}{25} = 0.32\n$$\nThe absolute error at any specific frequency, $|G(j\\omega) - G_r(j\\omega)|$, must be less than or equal to this worst-case (H-infinity) error. We found the DC gain discrepancy to be $|G(0) - G_r(0)| = \\frac{311}{1000} = 0.311$. Since $0.311 \\le 0.32$, the calculated discrepancy is consistent with the theoretical error bound. This demonstrates that standard balanced truncation does not generally preserve DC gain, though the error is bounded.", "answer": "$$\\boxed{\\frac{311}{1000}}$$", "id": "2854310"}]}