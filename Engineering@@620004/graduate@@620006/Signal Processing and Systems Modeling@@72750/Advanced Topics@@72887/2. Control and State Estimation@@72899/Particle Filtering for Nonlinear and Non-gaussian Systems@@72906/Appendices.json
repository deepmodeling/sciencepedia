{"hands_on_practices": [{"introduction": "A key challenge in particle filtering is weight degeneracy, where after a few iterations, a small number of particles hold most of the importance weight, rendering the approximation of the posterior distribution ineffective. To combat this, a resampling step is periodically triggered. This exercise [@problem_id:2890403] provides hands-on practice with the most common criterion for this decision: the Effective Sample Size ($N_{\\text{eff}}$). By calculating the ESS from a given set of particle weights and comparing it to a threshold, you will develop a concrete understanding of how a particle filter intelligently decides when to rejuvenate its particle set to maintain filtering accuracy.", "problem": "Consider a Sequential Monte Carlo particle filter for a nonlinear and Non-gaussian state-space model at time index $t$, where the filtering distribution is represented by $N$ weighted particles. You are given $N = 8$ particles with normalized importance weights\n$$\n\\big(w_{1},w_{2},\\ldots,w_{8}\\big) = \\big(0.35,\\;0.20,\\;0.15,\\;0.10,\\;0.08,\\;0.06,\\;0.04,\\;0.02\\big),\n$$\nwhich satisfy $\\sum_{i=1}^{8} w_{i} = 1$. The resampling criterion is defined by a threshold $\\tau N$ with $\\tau \\in (0,1)$, where $\\tau = 0.60$.\n\nStarting from the foundational definition of importance sampling and the notion that weight degeneracy increases the variance of weighted estimators, derive the Effective Sample Size (ESS) of the particle system in terms of the normalized weights, then compute its numerical value for the given weights. Based on your computed ESS, determine whether resampling should be triggered under the threshold rule $\\text{trigger if } \\text{ESS} \\le \\tau N$.\n\nRound your final numerical value of the Effective Sample Size to four significant figures. Report only the Effective Sample Size as your final numeric answer (dimensionless). Do not include any units in your final numeric answer.", "solution": "The problem posed is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of Sequential Monte Carlo methods, specifically particle filtering. All necessary data are provided, and the requested tasks—derivation, computation, and decision-making—are clear and logically follow from the premises. The problem is valid.\n\nThe central task is to evaluate the necessity of resampling in a particle filter. Resampling is a mechanism to combat the problem of *weight degeneracy*, a phenomenon where, after several iterations of a particle filter, the variance of the importance weights increases. This leads to a situation where a small number of particles have significant weights, while the majority have weights close to zero. Consequently, the particle set becomes a poor representation of the target distribution, and the variance of any estimators derived from it increases.\n\nThe Effective Sample Size, denoted $N_{\\text{eff}}$, is a heuristic measure used to quantify the severity of weight degeneracy. A low $N_{\\text{eff}}$ relative to the total number of particles, $N$, indicates significant degeneracy. The problem requests a derivation of the standard formula for $N_{\\text{eff}}$.\n\nA widely accepted approximation for the Effective Sample Size, given a set of normalized importance weights $\\{w_i\\}_{i=1}^N$ such that $\\sum_{i=1}^N w_i = 1$, is:\n$$\nN_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} w_i^2}\n$$\nThe justification for this formula arises from its behavior in limiting cases.\n1.  **Ideal Case (No Degeneracy)**: If all particles are equally important, their weights are uniform, i.e., $w_i = \\frac{1}{N}$ for all $i \\in \\{1, \\dots, N\\}$. The sum of squared weights is $\\sum_{i=1}^N w_i^2 = \\sum_{i=1}^N \\left(\\frac{1}{N}\\right)^2 = N \\cdot \\frac{1}{N^2} = \\frac{1}{N}$. Substituting this into the formula yields $N_{\\text{eff}} = \\frac{1}{1/N} = N$. In this optimal scenario, the effective sample size is equal to the total number of particles.\n2.  **Worst Case (Complete Degeneracy)**: If one particle, say particle $k$, has a weight of $w_k = 1$ and all other particles have weights $w_i = 0$ for $i \\ne k$, the particle set has completely collapsed. The sum of squared weights is $\\sum_{i=1}^N w_i^2 = 1^2 + \\sum_{i \\ne k} 0^2 = 1$. The formula gives $N_{\\text{eff}} = \\frac{1}{1} = 1$. This correctly reflects that the entire particle representation is dependent on a single particle.\n\nSince the formula for $N_{\\text{eff}}$ correctly interpolates between the best-case value of $N$ and the worst-case value of $1$, it serves as a robust and computationally simple metric for weight degeneracy.\n\nWe now proceed to the numerical computation for the given problem. We are given $N=8$ particles with normalized weights:\n$$\n\\big(w_{1},w_{2},\\ldots,w_{8}\\big) = \\big(0.35,\\;0.20,\\;0.15,\\;0.10,\\;0.08,\\;0.06,\\;0.04,\\;0.02\\big)\n$$\nFirst, we compute the sum of the squares of these weights:\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.35^2 + 0.20^2 + 0.15^2 + 0.10^2 + 0.08^2 + 0.06^2 + 0.04^2 + 0.02^2\n$$\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.1225 + 0.0400 + 0.0225 + 0.0100 + 0.0064 + 0.0036 + 0.0016 + 0.0004\n$$\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.2070\n$$\nUsing this sum, we calculate the Effective Sample Size:\n$$\nN_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{8} w_i^2} = \\frac{1}{0.2070} \\approx 4.83091787...\n$$\nThe problem requires the result to be rounded to four significant figures.\n$$\nN_{\\text{eff}} \\approx 4.831\n$$\nFinally, we must determine if resampling should be triggered. The resampling criterion is given by the rule $\\text{trigger if } N_{\\text{eff}} \\le \\tau N$. We are given the threshold parameter $\\tau = 0.60$ and the number of particles $N = 8$. The resampling threshold is:\n$$\n\\tau N = 0.60 \\times 8 = 4.8\n$$\nWe must now check if the computed $N_{\\text{eff}}$ satisfies the condition:\n$$\n4.831 \\le 4.8\n$$\nThis inequality is false. Since the Effective Sample Size ($4.831$) is greater than the threshold value ($4.8$), the level of weight degeneracy is not considered critical. Therefore, resampling should not be triggered. The final answer required is only the numerical value of the Effective Sample Size.", "answer": "$$\n\\boxed{4.831}\n$$", "id": "2890403"}, {"introduction": "The power of a particle filter lies in its ability to sequentially update the importance weights of particles as new data arrives, a process central to the Bayesian filtering recursion. This practice [@problem_id:2890404] focuses on implementing this core mathematical engine: the one-step weight update for a nonlinear, Non-gaussian model. You will not only translate the fundamental weight update formula into code but also focus on numerical stability by working in the logarithmic domain and exploring how different proposal distributions alter the calculation, a key concept for designing more advanced and efficient filters.", "problem": "Consider the scalar state-space model with nonlinear dynamics and nonlinear observations, defined for discrete time index $t \\in \\mathbb{N}$ by the following equations:\n$$\nx_t = x_{t-1}^2 + v_t, \\quad y_t = \\sin(x_t) + e_t,\n$$\nwhere $x_t \\in \\mathbb{R}$ is the latent state and $y_t \\in \\mathbb{R}$ is the observation. Assume that $\\sin(\\cdot)$ uses angles measured in radians. Let the process noise $v_t$ be independent and identically distributed according to a Laplace distribution with zero mean and scale $b_v  0$, and the observation noise $e_t$ be independent and identically distributed according to a Laplace distribution with zero mean and scale $b_e  0$. The Laplace probability density function with location $\\mu \\in \\mathbb{R}$ and scale $b  0$ is\n$$\n\\mathrm{Laplace}(z \\mid \\mu, b) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|z - \\mu|}{b}\\right).\n$$\n\nYou are asked to implement a one-step Sequential Importance Sampling (SIS) update for this model. Sequential Importance Sampling (SIS) proceeds by maintaining a set of particles and weights $\\{(x_{t-1}^{(i)}, w_{t-1}^{(i)})\\}_{i=1}^N$ that approximate the filtering distribution at time $t-1$. Given a proposal distribution $q(x_t \\mid x_{t-1}, y_t)$ and proposed samples $\\{x_t^{(i)}\\}_{i=1}^N$, the incremental importance weight update for each particle index $i \\in \\{1,\\dots,N\\}$ is defined, up to a common proportionality constant, by\n$$\n\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\cdot \\frac{p(y_t \\mid x_t^{(i)}) \\, p(x_t^{(i)} \\mid x_{t-1}^{(i)})}{q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)}.\n$$\nThe normalized weights are then\n$$\nw_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^N \\tilde{w}_t^{(j)}}.\n$$\nYour implementation must compute the exact incremental weight factor using the model-defined transition density\n$$\np(x_t \\mid x_{t-1}) = \\mathrm{Laplace}\\!\\left(x_t \\,\\middle|\\, x_{t-1}^2, \\, b_v\\right),\n$$\nand the model-defined likelihood\n$$\np(y_t \\mid x_t) = \\mathrm{Laplace}\\!\\left(y_t \\,\\middle|\\, \\sin(x_t), \\, b_e\\right),\n$$\ncombined with one of the following proposal distributions $q(x_t \\mid x_{t-1}, y_t)$:\n- Case A (prior proposal): $q_A(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1})$,\n- Case B (Gaussian proposal): $q_B(x_t \\mid x_{t-1}, y_t) = \\mathcal{N}\\!\\left(x_t \\,\\middle|\\, x_{t-1}^2, \\, \\sigma_q^2\\right)$, where $\\mathcal{N}$ denotes the Gaussian density with variance $\\sigma_q^2  0$. The Gaussian density is\n$$\n\\mathcal{N}(z \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\!\\left( -\\frac{(z - \\mu)^2}{2 \\sigma^2} \\right).\n$$\n\nYour program must:\n- Accept a fixed set of particle states $\\{x_{t-1}^{(i)}\\}_{i=1}^N$, previous weights $\\{w_{t-1}^{(i)}\\}_{i=1}^N$, proposed states $\\{x_t^{(i)}\\}_{i=1}^N$, an observation $y_t$, noise scales $b_v$ and $b_e$, and, if applicable, the Gaussian proposal standard deviation $\\sigma_q$.\n- Compute the normalized weights $\\{w_t^{(i)}\\}_{i=1}^N$ using the exact incremental weight expression above for the specified proposal.\n- Perform all weight computations in the logarithmic domain with a log-sum-exp normalization to ensure numerical stability.\n- If the provided $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ are not normalized, your program must internally normalize them before applying the SIS update.\n\nTest suite. Implement your solution for the following three independent cases. For each case, return the list of normalized weights in the particle index order as provided. In all cases, use angles in radians.\n\n- Case $1$ (prior proposal):\n  - $N = 4$,\n  - $b_v = 0.5$, $b_e = 0.2$,\n  - $y_t = 0.2$,\n  - $x_{t-1}^{(i)} = [\\,0.0, \\,0.5, \\,-0.5, \\,1.0\\,]$,\n  - $w_{t-1}^{(i)} = [\\,0.25, \\,0.25, \\,0.25, \\,0.25\\,]$,\n  - $x_t^{(i)} = [\\,0.1, \\,0.2, \\,0.25, \\,0.8\\,]$,\n  - Proposal: $q_A$.\n- Case $2$ (Gaussian proposal):\n  - $N = 3$,\n  - $b_v = 0.7$, $b_e = 0.5$, $\\sigma_q = 0.3$,\n  - $y_t = -1.0$,\n  - $x_{t-1}^{(i)} = [\\,0.2, \\,-0.7, \\,1.3\\,]$,\n  - $w_{t-1}^{(i)} = [\\,0.2, \\,0.5, \\,0.3\\,]$,\n  - $x_t^{(i)} = [\\,0.14, \\,0.29, \\,1.69\\,]$,\n  - Proposal: $q_B$.\n- Case $3$ (prior proposal with extreme likelihood; tests log-sum-exp robustness):\n  - $N = 2$,\n  - $b_v = 0.3$, $b_e = 0.01$,\n  - $y_t = 3.0$,\n  - $x_{t-1}^{(i)} = [\\,1.5, \\,-1.5\\,]$,\n  - $w_{t-1}^{(i)} = [\\,1000.0, \\,1.0\\,]$,\n  - $x_t^{(i)} = [\\,2.25, \\,2.0\\,]$,\n  - Proposal: $q_A$.\n\nFinal output format. Your program should produce a single line of output containing a single list of three elements, where each element is itself a list of the normalized weights for the corresponding case, in the same particle order as provided. The line must be formatted as a comma-separated list enclosed in square brackets with no spaces, for example:\n\"[ [w_case1], [w_case2], [w_case3] ]\" but without any spaces, i.e.,\n\"[[w11,w12,...],[w21,w22,...],[w31,w32,...]]\".", "solution": "The problem requires the implementation of a single-step weight update for a Sequential Importance Sampling (SIS) particle filter. The state-space model is defined by a nonlinear state transition and a nonlinear observation model, both corrupted by non-Gaussian (Laplace) noise.\n\nThe core of the SIS algorithm is the weight update rule. For a set of particles $\\{ (x_{t-1}^{(i)}, w_{t-1}^{(i)}) \\}_{i=1}^N$ approximating the filtering distribution at time $t-1$, and a new set of proposed particles $\\{ x_t^{(i)} \\}_{i=1}^N$ drawn from a proposal distribution $q(x_t \\mid x_{t-1}, y_t)$, the unnormalized weights $\\tilde{w}_t^{(i)}$ at time $t$ are given by:\n$$\n\\tilde{w}_t^{(i)} \\propto w_{t-1}^{(i)} \\cdot \\frac{p(y_t \\mid x_t^{(i)}) \\, p(x_t^{(i)} \\mid x_{t-1}^{(i)})}{q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)}\n$$\nThe terms in this expression are the prior weight ($w_{t-1}^{(i)}$), the likelihood ($p(y_t \\mid x_t^{(i)})$), the state transition probability ($p(x_t^{(i)} \\mid x_{t-1}^{(i)})$), and the proposal distribution density ($q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)$).\n\nThe multiplicative nature of this update can lead to numerical underflow, especially when dealing with low-probability events or a large number of particles. To ensure numerical stability, computations must be performed in the logarithmic domain. Taking the logarithm of the update rule gives the unnormalized log-weight:\n$$\n\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)}) + \\log p(x_t^{(i)} \\mid x_{t-1}^{(i)}) - \\log q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t) + C\n$$\nwhere $C$ is an arbitrary constant that is identical for all particles and cancels upon normalization.\n\nThe specific log-density functions for this problem are:\n1.  **Log-Likelihood**: Based on $y_t = \\sin(x_t) + e_t$ where $e_t \\sim \\mathrm{Laplace}(0, b_e)$. The log-likelihood $\\log p(y_t \\mid x_t)$ is derived from the Laplace PDF:\n    $$\n    \\log p(y_t \\mid x_t) = \\log\\left( \\mathrm{Laplace}(y_t \\mid \\sin(x_t), b_e) \\right) = -\\log(2) - \\log(b_e) - \\frac{|y_t - \\sin(x_t)|}{b_e}\n    $$\n2.  **Log-Transition**: Based on $x_t = x_{t-1}^2 + v_t$ where $v_t \\sim \\mathrm{Laplace}(0, b_v)$. The log-transition probability $\\log p(x_t \\mid x_{t-1})$ is:\n    $$\n    \\log p(x_t \\mid x_{t-1}) = \\log\\left( \\mathrm{Laplace}(x_t \\mid x_{t-1}^2, b_v) \\right) = -\\log(2) - \\log(b_v) - \\frac{|x_t - x_{t-1}^2|}{b_v}\n    $$\n\nThe proposal distribution $q(x_t \\mid x_{t-1}, y_t)$ determines the final term in the log-weight update:\n-   **Case A (Prior Proposal)**: Here, $q_A(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1})$. The log-transition and log-proposal terms cancel each other out, simplifying the update to:\n    $$\n    \\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)})\n    $$\n-   **Case B (Gaussian Proposal)**: Here, $q_B(x_t \\mid x_{t-1}, y_t) = \\mathcal{N}(x_t \\mid x_{t-1}^2, \\sigma_q^2)$. The log-proposal density is derived from the Gaussian PDF:\n    $$\n    \\log q_B(x_t \\mid x_{t-1}, y_t) = \\log\\left(\\mathcal{N}(x_t \\mid x_{t-1}^2, \\sigma_q^2)\\right) = -\\frac{1}{2}\\log(2\\pi\\sigma_q^2) - \\frac{(x_t - x_{t-1}^2)^2}{2\\sigma_q^2}\n    $$\n    The full update expression must be used in this case.\n\nThe problem states that the input weights $\\{w_{t-1}^{(i)}\\}$ might not be normalized. The first step is to normalize them. In the log domain, this means computing $\\log w_{t-1, \\text{norm}}^{(i)} = \\log w_{t-1}^{(i)} - \\log(\\sum_j w_{t-1}^{(j)})$.\n\nAfter computing the unnormalized log-weights $\\log \\tilde{w}_t^{(i)}$ for all particles, they must be normalized to sum to one. This is achieved robustly using the log-sum-exp trick. Let $S = \\sum_j \\tilde{w}_t^{(j)}$. Then the normalized weight is $w_t^{(i)} = \\tilde{w}_t^{(i)} / S$. In the log domain:\n$$\n\\log w_t^{(i)} = \\log \\tilde{w}_t^{(i)} - \\log S = \\log \\tilde{w}_t^{(i)} - \\log\\left(\\sum_j \\exp(\\log \\tilde{w}_t^{(j)})\\right)\n$$\nThe term $\\log(\\sum_j \\exp(\\log \\tilde{w}_t^{(j)}))$ is computed as `logsumexp`$(\\{\\log \\tilde{w}_t^{(j)}\\})$. Let $m = \\max_j(\\log \\tilde{w}_t^{(j)})$. Then $\\text{logsumexp} = m + \\log(\\sum_j \\exp(\\log \\tilde{w}_t^{(j)} - m))$. This prevents overflow and underflow in the intermediate exponentiation.\nFinally, the normalized weights are obtained by exponentiating the normalized log-weights: $w_t^{(i)} = \\exp(\\log w_t^{(i)})$.\n\nThe algorithm proceeds as follows:\n1.  Receive particle sets $\\{x_{t-1}^{(i)}\\}$, $\\{x_t^{(i)}\\}$, prior weights $\\{w_{t-1}^{(i)}\\}$, and model parameters.\n2.  Convert inputs to numerical arrays.\n3.  Compute the log of prior weights $\\log w_{t-1}^{(i)}$ and normalize them using the log-sum-exp trick to get $\\log w_{t-1, \\text{norm}}^{(i)}$.\n4.  Calculate the log-likelihood term $\\log p(y_t \\mid x_t^{(i)})$ for all particles.\n5.  Based on the specified proposal distribution:\n    -   If prior proposal, the unnormalized log-weight is $\\log \\tilde{w}_t^{(i)} = \\log w_{t-1, \\text{norm}}^{(i)} + \\log p(y_t \\mid x_t^{(i)})$.\n    -   If Gaussian proposal, additionally compute the log-transition $\\log p(x_t^{(i)} \\mid x_{t-1}^{(i)})$ and log-proposal $\\log q_B(\\dots)$ terms, and combine them to get $\\log \\tilde{w}_t^{(i)}$.\n6.  Normalize the resulting set of unnormalized log-weights $\\{\\log \\tilde{w}_t^{(i)}\\}$ using the log-sum-exp trick to get the normalized log-weights $\\log w_t^{(i)}$.\n7.  Exponentiate to obtain the final normalized weights $\\{w_t^{(i)}\\}$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import laplace, norm\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the SIS weight update problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"N\": 4,\n            \"b_v\": 0.5,\n            \"b_e\": 0.2,\n            \"y_t\": 0.2,\n            \"x_prev\": [0.0, 0.5, -0.5, 1.0],\n            \"w_prev\": [0.25, 0.25, 0.25, 0.25],\n            \"x_t\": [0.1, 0.2, 0.25, 0.8],\n            \"proposal\": \"A\",\n        },\n        {\n            \"case_id\": 2,\n            \"N\": 3,\n            \"b_v\": 0.7,\n            \"b_e\": 0.5,\n            \"sigma_q\": 0.3,\n            \"y_t\": -1.0,\n            \"x_prev\": [0.2, -0.7, 1.3],\n            \"w_prev\": [0.2, 0.5, 0.3],\n            \"x_t\": [0.14, 0.29, 1.69],\n            \"proposal\": \"B\",\n        },\n        {\n            \"case_id\": 3,\n            \"N\": 2,\n            \"b_v\": 0.3,\n            \"b_e\": 0.01,\n            \"y_t\": 3.0,\n            \"x_prev\": [1.5, -1.5],\n            \"w_prev\": [1000.0, 1.0],\n            \"x_t\": [2.25, 2.0],\n            \"proposal\": \"A\",\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Extract and convert parameters to numpy arrays for vectorized operations\n        y_t = case[\"y_t\"]\n        b_v = case[\"b_v\"]\n        b_e = case[\"b_e\"]\n        x_prev = np.array(case[\"x_prev\"])\n        w_prev = np.array(case[\"w_prev\"])\n        x_t = np.array(case[\"x_t\"])\n\n        # Step 1: Normalize previous weights w_{t-1} in the log domain\n        # The use of np.log on potentially zero weights is safe if they are not present\n        # in the test cases, which they are not. A more robust implementation would\n        # handle w_prev=0 by assigning -inf to the log-weight.\n        log_w_prev_unnorm = np.log(w_prev)\n        log_sum_w_prev = logsumexp(log_w_prev_unnorm)\n        log_w_prev_norm = log_w_prev_unnorm - log_sum_w_prev\n\n        # Step 2: Calculate log-likelihood component\n        # p(y_t | x_t) = Laplace(y_t | sin(x_t), b_e)\n        log_likelihood = laplace.logpdf(y_t, loc=np.sin(x_t), scale=b_e)\n        \n        # Step 3: Calculate unnormalized log weights based on proposal type\n        if case[\"proposal\"] == \"A\":\n            # Proposal q_A = p(x_t | x_{t-1}), so transition and proposal terms cancel.\n            log_w_t_unnorm = log_w_prev_norm + log_likelihood\n        elif case[\"proposal\"] == \"B\":\n            sigma_q = case[\"sigma_q\"]\n            # Proposal q_B = N(x_t | x_{t-1}^2, sigma_q^2)\n            # Log transition density: p(x_t | x_{t-1}) = Laplace(x_t | x_{t-1}^2, b_v)\n            log_transition = laplace.logpdf(x_t, loc=x_prev**2, scale=b_v)\n            \n            # Log proposal density: q(x_t | ...) = N(x_t | x_{t-1}^2, sigma_q^2)\n            # scipy.stats.norm uses standard deviation (scale), not variance.\n            log_proposal = norm.logpdf(x_t, loc=x_prev**2, scale=sigma_q)\n            \n            log_w_t_unnorm = log_w_prev_norm + log_likelihood + log_transition - log_proposal\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown proposal type: {case['proposal']}\")\n\n        # Step 4: Normalize the new log weights using log-sum-exp\n        log_sum_w_t = logsumexp(log_w_t_unnorm)\n        log_w_t_norm = log_w_t_unnorm - log_sum_w_t\n\n        # Step 5: Convert back from log domain to get final weights\n        w_t = np.exp(log_w_t_norm)\n        \n        results.append(w_t.tolist())\n    \n    # Format the final output string exactly as specified\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, res))}]' for res in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2890404"}, {"introduction": "Having explored the decision to resample and the mechanics of weight updating, this practice [@problem_id:2890374] integrates these concepts into a full, single iteration of the foundational bootstrap particle filter. You will implement the complete cycle: propagating particles according to the system dynamics, weighting them based on a new observation, calculating the ESS to assess particle health, and performing systematic resampling if degeneracy is detected. This comprehensive, hands-on implementation solidifies your understanding of how these distinct steps work in concert to track a hidden state through a complex, nonlinear system.", "problem": "Consider the following discrete-time state-space model designed to stress-test Sequential Monte Carlo (SMC) methods for nonlinear and Non-gaussian systems. The latent state $x_t \\in \\mathbb{R}$ evolves according to a nonlinear dynamics\n$$\nx_t = g(x_{t-1}, t) + v_t,\n$$\nwhere\n$$\ng(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t),\n$$\nand the process noise $v_t$ is independently and identically distributed as a Laplace distribution with location $0$ and scale $b_v$, denoted $v_t \\sim \\mathrm{Laplace}(0, b_v)$, with probability density function\n$$\np_{V}(v) = \\frac{1}{2 b_v} \\exp\\!\\left(-\\frac{|v|}{b_v}\\right).\n$$\nThe scalar observation $y_t \\in \\mathbb{R}$ is given by the nonlinear measurement model\n$$\ny_t = \\arctan(x_t) + e_t,\n$$\nwhere the measurement noise $e_t$ is independently and identically distributed as a Laplace distribution with location $0$ and scale $b_e$, denoted $e_t \\sim \\mathrm{Laplace}(0, b_e)$, with probability density function\n$$\np_{E}(e) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|e|}{b_e}\\right).\n$$\nAll angles are in radians. The function $\\arctan(\\cdot)$ denotes the principal value inverse tangent.\n\nYou are asked to implement a single iteration (from time $t-1$ to time $t$) of the bootstrap particle filter (also called the Sampling Importance Resampling filter) as follows, starting from a prior particle approximation $\\{(x_{t-1}^{(i)}, w_{t-1}^{(i)})\\}_{i=1}^N$ of the filtering distribution at time $t-1$ with $N$ particles, where $w_{t-1}^{(i)} \\ge 0$ and $\\sum_{i=1}^N w_{t-1}^{(i)} = 1$:\n\n1. Propagation (proposal equals the transition prior): For each particle $i \\in \\{1,\\dots,N\\}$, sample\n$$\nx_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathrm{Laplace}\\!\\left(g\\!\\left(x_{t-1}^{(i)}, t\\right),\\, b_v\\right).\n$$\n\n2. Weight update using the measurement likelihood and prior weights:\n$$\n\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p\\!\\left(y_t \\,\\middle|\\, x_t^{(i)}\\right), \\quad\np\\!\\left(y_t \\,\\middle|\\, x_t\\right) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right).\n$$\nThen normalize to get $w_t^{(i)} = \\tilde{w}_t^{(i)} \\Big/ \\sum_{j=1}^N \\tilde{w}_t^{(j)}$.\n\n3. Compute the Effective Sample Size (ESS) defined by\n$$\n\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N \\left(w_t^{(i)}\\right)^2}.\n$$\n\n4. Resampling decision and execution: Given a threshold ratio $\\tau \\in [0, 1]$, perform systematic resampling if $\\mathrm{ESS}_t \\le \\tau N$, producing a resampled particle set $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ with equal weights $\\bar{w}_t^{(i)} = 1/N$. If resampling is not triggered, retain $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$.\n\nFor this single iteration, define the posterior estimate after the resampling decision as follows:\n- If resampling occurred, use the resampled equally weighted particles $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ to compute the posterior mean and variance as the standard unweighted sample mean and variance.\n- If no resampling occurred, use the weighted set $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ to compute the posterior mean and variance as the weighted mean and weighted variance.\n\nThe program you implement must:\n- Initialize the prior particles by sampling $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ independently for $i \\in \\{1,\\dots,N\\}$, and use uniform prior weights $w_{t-1}^{(i)} = 1/N$.\n- Use a fixed pseudorandom seed for each test case to ensure deterministic behavior.\n- Use numerically stable computations for weights (for example, via log-weights stabilization).\n- Implement systematic resampling.\n\nYour program must process the following test suite, where each tuple specifies $(N, \\mathrm{seed}, t, y_t, m_0, s_0, b_v, b_e, \\tau)$:\n- Test $1$: $(200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5)$.\n- Test $2$: $(200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9)$.\n- Test $3$: $(150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9)$.\n- Test $4$: $(100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0)$.\n\nFor each test, after one complete bootstrap filtering iteration (including the resampling decision), report:\n- The posterior mean of $x_t$ rounded to $6$ decimals,\n- The posterior variance of $x_t$ rounded to $6$ decimals,\n- The Effective Sample Size $\\mathrm{ESS}_t$ rounded to $6$ decimals (computed before any resampling),\n- The resampling indicator, where resampling is indicated by the integer $1$ and no resampling by the integer $0$.\n\nFinal output format: Your program should produce a single line of output containing a JSON-like list of results, one per test case, with each result itself being a list of the four values described above in this exact order. For example, the output should look like\n$$\n\\big[\\,[m_1, v_1, \\mathrm{ESS}_1, r_1],\\,[m_2, v_2, \\mathrm{ESS}_2, r_2],\\,\\dots\\,\\big],\n$$\nwhere each $m_k$, $v_k$, and $\\mathrm{ESS}_k$ are decimal numbers rounded to $6$ decimals and each $r_k$ is either $0$ or $1$. There are no physical units required beyond specifying that angles are in radians. The program must not read any input and must be self-contained.", "solution": "The problem statement is analyzed and found to be valid. It constitutes a well-posed problem in the domain of nonlinear state estimation, providing a complete and consistent specification for the implementation of a single iteration of a bootstrap particle filter. All necessary models, parameters, and algorithms are defined unambiguously. We proceed with the solution.\n\nThe task is to implement one iteration of a sequential Monte Carlo method, specifically the bootstrap particle filter, for a given nonlinear, non-Gaussian state-space model. The process starts at time $t-1$ and ends after the resampling decision at time $t$.\n\n**1. System Model Specification**\nThe state-space model is defined by two equations:\nThe state transition equation:\n$$x_t = g(x_{t-1}, t) + v_t$$\nwhere the nonlinear function is $g(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t)$, and the process noise is $v_t \\sim \\mathrm{Laplace}(0, b_v)$.\n\nThe measurement equation:\n$$y_t = \\arctan(x_t) + e_t$$\nwhere the measurement noise is $e_t \\sim \\mathrm{Laplace}(0, b_e)$.\n\n**2. Bootstrap Particle Filter Iteration**\nThe algorithm proceeds through several steps, starting from a particle set $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^N$ approximating the posterior distribution $p(x_{t-1} | y_{1:t-1})$.\n\n**Step 0: Initialization at time $t-1$**\nAs specified, the prior particle set is initialized for this single iteration. The states $x_{t-1}^{(i)}$ are drawn independently from a Gaussian distribution, $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ for $i=1, \\dots, N$. The initial weights are uniform, $w_{t-1}^{(i)} = 1/N$ for all $i$.\n\n**Step 1: Propagation (Prediction)**\nEach particle is propagated forward in time according to the state transition model. For the bootstrap filter, the proposal distribution is the transition prior itself. This means, for each particle $i$, we sample a new state $x_t^{(i)}$ from the distribution $p(x_t | x_{t-1}^{(i)})$. This is accomplished by:\n1.  Calculating the deterministic component of the state evolution: $\\mu_t^{(i)} = g(x_{t-1}^{(i)}, t)$.\n2.  Sampling a noise term $v_t^{(i)}$ from the process noise distribution: $v_t^{(i)} \\sim \\mathrm{Laplace}(0, b_v)$.\n3.  Combining these to obtain the new particle state: $x_t^{(i)} = \\mu_t^{(i)} + v_t^{(i)}$.\n\n**Step 2: Weight Update (Correction)**\nUpon receiving the measurement $y_t$, the importance weights of the propagated particles are updated to reflect how well each particle explains the observation. The updated, unnormalized weight $\\tilde{w}_t^{(i)}$ is the product of the prior weight and the likelihood of the measurement given the particle state:\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p(y_t | x_t^{(i)})$$\nThe likelihood function $p(y_t | x_t)$ is derived from the measurement model and the distribution of the measurement noise $e_t$:\n$$p(y_t | x_t) = p_{E}(y_t - \\arctan(x_t)) = \\frac{1}{2 b_e} \\exp\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right)$$\nSince the prior weights $w_{t-1}^{(i)}$ are uniform ($1/N$), they are constant across all particles and can be ignored during normalization. The unnormalized weights are thus proportional to the likelihoods: $\\tilde{w}_t^{(i)} \\propto p(y_t | x_t^{(i)})$.\n\nFor numerical stability, computations are performed in the log domain. The log-likelihood for particle $i$ is:\n$$\\log p(y_t | x_t^{(i)}) = -\\log(2 b_e) - \\frac{|y_t - \\arctan(x_t^{(i)})|}{b_e}$$\nLet $l_i = \\log p(y_t | x_t^{(i)})$. We find the maximum log-likelihood, $l_{\\max} = \\max_i \\{l_i\\}$. The normalized weights $w_t^{(i)}$ are then computed using the log-sum-exp trick to prevent numerical underflow:\n$$w_t^{(i)} = \\frac{\\exp(l_i - l_{\\max})}{\\sum_{j=1}^N \\exp(l_j - l_{\\max})}$$\nThe set $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ now represents an approximation of the filtering distribution $p(x_t | y_{1:t})$.\n\n**Step 3: Effective Sample Size (ESS) Calculation**\nThe degeneracy of the particle set is quantified by the Effective Sample Size, $\\mathrm{ESS}_t$. A low ESS indicates that a few particles have very high weights, and the approximation is poor. It is calculated as:\n$$\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N (w_t^{(i)})^2}$$\nThe value of $\\mathrm{ESS}_t$ ranges from $1$ (complete degeneracy) to $N$ (uniform weights).\n\n**Step 4: Resampling Decision and Execution**\nResampling is a mechanism to mitigate particle degeneracy by replicating particles with high weights and discarding those with low weights. A decision to resample is made by comparing the ESS to a threshold, $\\tau N$, where $\\tau \\in [0, 1]$ is a user-defined ratio.\n- If $\\mathrm{ESS}_t \\le \\tau N$, resampling is performed. The problem specifies systematic resampling. This algorithm draws $N$ particles from the current discrete distribution $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ to form a new set $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ where each new particle has an equal weight $\\bar{w}_t^{(i)}=1/N$.\n- If $\\mathrm{ESS}_t  \\tau N$, no action is taken. The particle set remains $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$.\n\nA resampling indicator $r_t$ is set to $1$ if resampling occurred, and $0$ otherwise.\n\n**Step 5: Posterior Statistics Calculation**\nThe final step is to compute the mean and variance of the posterior distribution of $x_t$ from the resulting particle set. The formulae depend on whether resampling was performed.\n- **If resampling occurred ($r_t=1$)**: The posterior is approximated by the equally weighted particles $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$. The mean and variance are the standard unweighted sample mean and variance:\n  $$\\hat{x}_t = \\frac{1}{N} \\sum_{i=1}^N \\bar{x}_t^{(i)}$$\n  $$\\hat{V}_t = \\frac{1}{N} \\sum_{i=1}^N (\\bar{x}_t^{(i)} - \\hat{x}_t)^2$$\n- **If no resampling occurred ($r_t=0$)**: The posterior is approximated by the weighted particles $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$. The mean and variance are the weighted sample mean and variance:\n  $$\\hat{x}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n  $$\\hat{V}_t = \\sum_{i=1}^N w_t^{(i)} (x_t^{(i)} - \\hat{x}_t)^2$$\n\nThese computed values—posterior mean, posterior variance, ESS, and resampling indicator—constitute the required output for one iteration of the filter.", "answer": "```python\nimport numpy as np\n\ndef g(x, t):\n    \"\"\"The nonlinear state transition function.\"\"\"\n    return 0.5 * x + 25.0 * x / (1.0 + x**2) + 8.0 * np.cos(1.2 * t)\n\ndef particle_filter_step(N, seed, t, yt, m0, s0, bv, be, tau):\n    \"\"\"\n    Performs a single iteration of a bootstrap particle filter.\n    Returns posterior mean, variance, ESS, and resampling indicator.\n    \"\"\"\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Step 0: Initialize prior particles at time t-1.\n    # The weights w_{t-1} are uniform (1/N) for all particles.\n    xt_prev = rng.normal(loc=m0, scale=s0, size=N)\n\n    # Step 1: Propagation (Prediction) to time t.\n    # The proposal is the transition prior p(x_t|x_{t-1}).\n    mu_t = g(xt_prev, t)\n    # Sample from Laplace(mu_t, bv) which is mu_t + Laplace(0, bv)\n    xt = mu_t + rng.laplace(loc=0.0, scale=bv, size=N)\n\n    # Step 2: Weight update using the measurement y_t.\n    # We work with log-weights for numerical stability.\n    # The log-likelihood is log p(y_t|x_t^(i)).\n    log_likelihood = -np.log(2.0 * be) - np.abs(yt - np.arctan(xt)) / be\n\n    # Since w_{t-1} are uniform, log(w_{t-1}) is a constant offset\n    # that cancels during normalization. So, log_tilde_wt is proportional\n    # to the log_likelihood.\n    # We use the log-sum-exp trick for normalization.\n    log_wt_max = np.max(log_likelihood)\n    wt_unnorm = np.exp(log_likelihood - log_wt_max)\n    wt = wt_unnorm / np.sum(wt_unnorm)\n\n    # Step 3: Compute Effective Sample Size (ESS).\n    # This is calculated before any resampling.\n    ess = 1.0 / np.sum(wt**2)\n\n    # Step 4: Resampling decision and execution.\n    resampling_occurred = 0\n    # The threshold condition ESS = tau * N might be sensitive to floating-point\n    # issues, but for typical values, direct comparison is acceptable.\n    # Note: ESS is always = 1. If tau*N  1, resampling will not trigger.\n    if ess = tau * N:\n        resampling_occurred = 1\n        \n        # Perform systematic resampling.\n        csw = np.cumsum(wt)\n        csw[-1] = 1.0  # Ensure the sum is exactly 1\n        u0 = rng.random()\n        positions = (np.arange(N) + u0) / N\n        \n        indices = np.searchsorted(csw, positions)\n        \n        # The new particles are copies of the old ones based on indices.\n        xt = xt[indices]\n        # After resampling, all weights are reset to uniform 1/N.\n        wt = np.full(N, 1.0 / N)\n\n    # Step 5: Compute posterior mean and variance.\n    if resampling_occurred == 1:\n        # Use unweighted sample statistics for the resampled set.\n        post_mean = np.mean(xt)\n        post_var = np.var(xt) # np.var uses 1/N denominator\n    else:\n        # Use weighted sample statistics.\n        post_mean = np.sum(wt * xt)\n        post_var = np.sum(wt * (xt - post_mean)**2)\n\n    return [\n        round(post_mean, 6),\n        round(post_var, 6),\n        round(ess, 6),\n        resampling_occurred\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (N, seed, t, y_t, m_0, s_0, b_v, b_e, tau)\n        (200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5),\n        (200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9),\n        (150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9),\n        (100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = particle_filter_step(*case)\n        results.append(result)\n\n    # Format the final output string as a JSON-like list of lists.\n    # f'{val:.6f}' is used to ensure 6 decimal places and avoid scientific notation.\n    inner_strings = []\n    for res_list in results:\n        m, v, e, r = res_list\n        inner_strings.append(f\"[{m:.6f},{v:.6f},{e:.6f},{r}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2890374"}]}