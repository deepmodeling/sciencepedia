{"hands_on_practices": [{"introduction": "The Luenberger observer is a cornerstone of modern control theory, providing a systematic way to estimate the internal states of a system when only its inputs and outputs are measurable. This first exercise focuses on the core technique of pole placement, a deterministic approach where we mathematically engineer the observer's performance. By choosing an appropriate observer gain matrix $L$, you will place the eigenvalues of the error dynamics matrix $A-LC$ at specific locations in the complex plane, thereby dictating the speed and character of the estimation error's convergence to zero [@problem_id:2888317].", "problem": "Consider the continuous-time Linear Time-Invariant (LTI) state-space model\n$$\n\\dot{x}(t)=A\\,x(t),\\quad y(t)=C\\,x(t),\n$$\nwhere $x(t)\\in\\mathbb{R}^{2}$ is the state and $y(t)\\in\\mathbb{R}$ is the measured output. The system matrices are\n$$\nA=\\begin{bmatrix}0&1\\\\-2&-3\\end{bmatrix},\\qquad C=\\begin{bmatrix}1&0\\end{bmatrix}.\n$$\nYou are to design a continuous-time Luenberger observer (LO) gain $L\\in\\mathbb{R}^{2\\times 1}$ for the estimator\n$$\n\\dot{\\hat{x}}(t)=A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big),\n$$\nso that the estimator error dynamics achieves asymptotic convergence with eigenvalues at $\\{-4,-5\\}$. Begin from first principles: use the state and output equations and the definition of the observer to derive the error dynamics, use the definition of observability to justify assignability of the error dynamics eigenvalues, and then impose the desired eigenvalues by matching the characteristic polynomial of the error dynamics to the target one. Provide the exact $L$ that accomplishes this. Express your final answer as a $1\\times 2$ row matrix listing the entries of $L$ from top to bottom. No rounding is required and no units are involved.", "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed problem in linear systems theory, specifically concerning observer design. The provided information is complete, consistent, and scientifically sound. All terms are defined, and the objective is unambiguous. We may proceed with the solution.\n\nThe problem requires the design of a Luenberger observer for a given continuous-time LTI system. The system dynamics are described by:\n$$\n\\dot{x}(t)=A\\,x(t)\n$$\n$$\ny(t)=C\\,x(t)\n$$\nwith state $x(t)\\in\\mathbb{R}^{2}$, output $y(t)\\in\\mathbb{R}$, and system matrices:\n$$\nA=\\begin{bmatrix}0&1\\\\-2&-3\\end{bmatrix},\\qquad C=\\begin{bmatrix}1&0\\end{bmatrix}\n$$\nThe Luenberger observer is designed to estimate the state $x(t)$ using a model of the system and a correction term proportional to the output estimation error. The state estimate $\\hat{x}(t)$ evolves according to:\n$$\n\\dot{\\hat{x}}(t)=A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big)\n$$\nwhere $L \\in \\mathbb{R}^{2 \\times 1}$ is the observer gain matrix that we must determine. The objective is to place the eigenvalues of the estimator error dynamics at $\\{-4, -5\\}$ to ensure asymptotic convergence of the estimate $\\hat{x}(t)$ to the true state $x(t)$.\n\nFirst, we derive the dynamics of the estimation error, defined as $e(t) = x(t) - \\hat{x}(t)$. Differentiating the error with respect to time gives:\n$$\n\\dot{e}(t) = \\dot{x}(t) - \\dot{\\hat{x}}(t)\n$$\nSubstitute the expressions for $\\dot{x}(t)$ and $\\dot{\\hat{x}}(t)$:\n$$\n\\dot{e}(t) = A\\,x(t) - \\left[ A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big) \\right]\n$$\nNow, substitute $y(t) = C\\,x(t)$ into the equation:\n$$\n\\dot{e}(t) = A\\,x(t) - A\\,\\hat{x}(t) - L\\big(C\\,x(t)-C\\,\\hat{x}(t)\\big)\n$$\nFactoring the terms, we obtain:\n$$\n\\dot{e}(t) = A\\big(x(t) - \\hat{x}(t)\\big) - L\\,C\\big(x(t) - \\hat{x}(t)\\big)\n$$\nRecognizing that $e(t) = x(t) - \\hat{x}(t)$, the error dynamics are governed by the linear homogeneous differential equation:\n$$\n\\dot{e}(t) = (A - L\\,C)e(t)\n$$\nThe stability of the error dynamics, and thus the convergence of $\\hat{x}(t)$ to $x(t)$, is determined by the eigenvalues of the matrix $A_e = A - L\\,C$. For the error to converge to zero, all eigenvalues of $A_e$ must have negative real parts. The problem demands specific eigenvalues at $\\{-4, -5\\}$.\n\nThe ability to arbitrarily place the eigenvalues of $A - L\\,C$ by choosing $L$ is guaranteed if and only if the system pair $(A, C)$ is observable. We must verify this condition. The system is observable if the observability matrix $\\mathcal{O}$ has full rank, which for a state dimension of $n=2$ means $\\text{rank}(\\mathcal{O})=2$. The observability matrix is constructed as:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nFirst, we compute the product $CA$:\n$$\nCA = \\begin{bmatrix}1&0\\end{bmatrix} \\begin{bmatrix}0&1\\\\-2&-3\\end{bmatrix} = \\begin{bmatrix}(1)(0)+(0)(-2) & (1)(1)+(0)(-3)\\end{bmatrix} = \\begin{bmatrix}0&1\\end{bmatrix}\n$$\nNow, we construct the observability matrix $\\mathcal{O}$:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\nThis is the $2 \\times 2$ identity matrix. Its determinant is $\\det(\\mathcal{O}) = 1 \\neq 0$, so its rank is $2$. Since the rank of $\\mathcal{O}$ is equal to the dimension of the state space, the system is completely observable. Therefore, the eigenvalues of the error dynamics matrix $A - L\\,C$ can be arbitrarily placed by selecting an appropriate gain matrix $L$.\n\nWe now proceed to find $L$. Let the gain matrix be $L = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix}$. The matrix $A - L\\,C$ is:\n$$\nA - L\\,C = \\begin{bmatrix}0&1\\\\-2&-3\\end{bmatrix} - \\begin{bmatrix}l_1\\\\l_2\\end{bmatrix}\\begin{bmatrix}1&0\\end{bmatrix} = \\begin{bmatrix}0&1\\\\-2&-3\\end{bmatrix} - \\begin{bmatrix}l_1 & 0 \\\\ l_2 & 0\\end{bmatrix} = \\begin{bmatrix}-l_1 & 1 \\\\ -2-l_2 & -3\\end{bmatrix}\n$$\nThe eigenvalues of this matrix are the roots of its characteristic polynomial, $p(\\lambda) = \\det(\\lambda I - (A - L\\,C))$.\n$$\n\\lambda I - (A - L\\,C) = \\begin{bmatrix}\\lambda & 0 \\\\ 0 & \\lambda\\end{bmatrix} - \\begin{bmatrix}-l_1 & 1 \\\\ -2-l_2 & -3\\end{bmatrix} = \\begin{bmatrix}\\lambda+l_1 & -1 \\\\ 2+l_2 & \\lambda+3\\end{bmatrix}\n$$\nThe characteristic polynomial is:\n$$\np(\\lambda) = (\\lambda+l_1)(\\lambda+3) - (-1)(2+l_2) = \\lambda^2 + 3\\lambda + l_1\\lambda + 3l_1 + 2 + l_2\n$$\n$$\np(\\lambda) = \\lambda^2 + (3+l_1)\\lambda + (3l_1 + l_2 + 2)\n$$\nThe desired eigenvalues are given as $\\{-4, -5\\}$. The desired characteristic polynomial, $p_{des}(\\lambda)$, is formed from these eigenvalues:\n$$\np_{des}(\\lambda) = (\\lambda - (-4))(\\lambda - (-5)) = (\\lambda+4)(\\lambda+5) = \\lambda^2 + 9\\lambda + 20\n$$\nTo achieve the desired pole placement, we equate the coefficients of the actual characteristic polynomial $p(\\lambda)$ with those of the desired polynomial $p_{des}(\\lambda)$:\n$$\n\\lambda^2 + (3+l_1)\\lambda + (3l_1 + l_2 + 2) = \\lambda^2 + 9\\lambda + 20\n$$\nComparing the coefficients for each power of $\\lambda$:\n\\begin{enumerate}\n    \\item Coefficient of $\\lambda^1$: $3+l_1 = 9$\n    \\item Coefficient of $\\lambda^0$: $3l_1 + l_2 + 2 = 20$\n\\end{enumerate}\nFrom the first equation, we find $l_1$:\n$$\nl_1 = 9 - 3 = 6\n$$\nSubstitute $l_1=6$ into the second equation to find $l_2$:\n$$\n3(6) + l_2 + 2 = 20\n$$\n$$\n18 + l_2 + 2 = 20\n$$\n$$\n20 + l_2 = 20\n$$\n$$\nl_2 = 0\n$$\nThus, the required observer gain matrix is:\n$$\nL = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\n$$\nThis gain ensures that the error dynamics $\\dot{e}(t) = (A-LC)e(t)$ have eigenvalues at $\\{-4, -5\\}$, guaranteeing that the estimation error $e(t)$ converges asymptotically to zero.", "answer": "$$\n\\boxed{\\begin{pmatrix} 6 & 0 \\end{pmatrix}}\n$$", "id": "2888317"}, {"introduction": "Real-world systems are rarely free from noise. This practice moves beyond the deterministic world into stochastic estimation by introducing the celebrated Kalman filter. Unlike pole placement, which allows arbitrary assignment of error dynamics, the Kalman filter provides an optimal estimate in the presence of Gaussian process and measurement noise. You will perform one complete prediction-update cycle, calculating the propagation of the state's error covariance to understand how the filter balances confidence in its model prediction against the information from a new, noisy measurement [@problem_id:2888322].", "problem": "A discrete-time linear time-invariant (LTI) state-space model for a constant-velocity motion in one spatial dimension is given by the stochastic difference equations\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad y_{k} = C x_{k} + v_{k},\n$$\nwhere $x_{k} \\in \\mathbb{R}^{2}$ is the state with components position and velocity, $y_{k} \\in \\mathbb{R}$ is the position measurement, $w_{k}$ is zero-mean process noise with covariance $Q$, and $v_{k}$ is zero-mean measurement noise with covariance $R$. Assume the sampling interval is $1$ time unit so that\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad Q = \\operatorname{diag}(0.01,\\,0.01), \\quad R = \\begin{bmatrix} 0.04 \\end{bmatrix}.\n$$\nSuppose the initial a posteriori error covariance at time $k=0$ is\n$$\nP_{0} = I_{2}.\n$$\nStarting from this prior, perform one full Kalman filter (KF) prediction-update cycle to obtain the a posteriori error covariance at time $k=1$. Your derivation should begin from the definitions of the estimation error covariance and the minimum-variance linear estimator for linear Gaussian systems and proceed to the covariance recursion for prediction and update. Compute all intermediate quantities necessary for the covariance update, but report only the a posteriori error covariance matrix at time $k=1$ as your final answer. Round each reported matrix entry to $4$ significant figures. No physical units are required.", "solution": "The user-provided problem statement has been evaluated and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of state estimation, containing all necessary information for a unique solution via the standard Kalman filter algorithm. The parameters are self-consistent and realistic for a theoretical exercise.\n\nThe objective is to compute the a posteriori error covariance matrix at time $k=1$, denoted $P_{1|1}$, given the a posteriori error covariance at time $k=0$, denoted $P_{0|0}$. The Kalman filter provides the optimal linear estimator for a linear system with Gaussian noise by propagating the state estimate and its error covariance through a two-step recursive cycle: a prediction (time update) step and an update (measurement update) step.\n\nThe covariance propagation equations are as follows.\nPrediction:\n$$\nP_{k|k-1} = A P_{k-1|k-1} A^T + Q\n$$\nUpdate:\n$$\nK_k = P_{k|k-1} C^T (C P_{k|k-1} C^T + R)^{-1}\n$$\n$$\nP_{k|k} = (I - K_k C) P_{k|k-1}\n$$\nHere, $P_{k|k-1}$ is the a priori error covariance at time $k$, and $P_{k|k}$ is the a posteriori error covariance at time $k$. $K_k$ is the Kalman gain.\n\nThe given parameters are:\nState transition matrix: $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}$\nMeasurement matrix: $C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$\nProcess noise covariance: $Q = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}$\nMeasurement noise covariance: $R = [0.04]$\nInitial a posteriori error covariance at $k=0$: $P_{0|0} = I_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n\nWe shall now perform one full cycle for $k=1$.\n\n**Step 1: Prediction (Time Update)**\n\nWe compute the a priori error covariance at $k=1$, $P_{1|0}$, using the state of the covariance at $k=0$.\n$$\nP_{1|0} = A P_{0|0} A^T + Q\n$$\nSubstituting the given values:\n$$\nA P_{0|0} A^T = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 & 0 \\cdot 0 + 1 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix}\n$$\nNow, we add the process noise covariance $Q$:\n$$\nP_{1|0} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix} = \\begin{bmatrix} 2.01 & 1 \\\\ 1 & 1.01 \\end{bmatrix}\n$$\nThis is the predicted error covariance before incorporating the measurement at $k=1$.\n\n**Step 2: Update (Measurement Update)**\n\nFirst, we compute the innovation covariance, which we denote as $S_1$:\n$$\nS_1 = C P_{1|0} C^T + R\n$$\n$$\nC P_{1|0} C^T = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2.01 & 1 \\\\ 1 & 1.01 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.01 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = [2.01]\n$$\n$$\nS_1 = [2.01] + [0.04] = [2.05]\n$$\nNext, we compute the Kalman gain $K_1$:\n$$\nK_1 = P_{1|0} C^T S_1^{-1}\n$$\n$$\nP_{1|0} C^T = \\begin{bmatrix} 2.01 & 1 \\\\ 1 & 1.01 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.01 \\\\ 1 \\end{bmatrix}\n$$\n$$\nK_1 = \\begin{bmatrix} 2.01 \\\\ 1 \\end{bmatrix} [2.05]^{-1} = \\begin{bmatrix} \\frac{2.01}{2.05} \\\\ \\frac{1}{2.05} \\end{bmatrix}\n$$\nFinally, we compute the a posteriori error covariance $P_{1|1}$:\n$$\nP_{1|1} = (I - K_1 C) P_{1|0}\n$$\nLet us compute the term $(I - K_1 C)$:\n$$\nK_1 C = \\begin{bmatrix} \\frac{2.01}{2.05} \\\\ \\frac{1}{2.05} \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{2.01}{2.05} & 0 \\\\ \\frac{1}{2.05} & 0 \\end{bmatrix}\n$$\n$$\nI - K_1 C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\begin{bmatrix} \\frac{2.01}{2.05} & 0 \\\\ \\frac{1}{2.05} & 0 \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{2.01}{2.05} & 0 \\\\ -\\frac{1}{2.05} & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{0.04}{2.05} & 0 \\\\ -\\frac{1}{2.05} & 1 \\end{bmatrix}\n$$\nNow we perform the final matrix multiplication:\n$$\nP_{1|1} = \\begin{bmatrix} \\frac{0.04}{2.05} & 0 \\\\ -\\frac{1}{2.05} & 1 \\end{bmatrix} \\begin{bmatrix} 2.01 & 1 \\\\ 1 & 1.01 \\end{bmatrix}\n$$\n$$\nP_{1|1} = \\begin{bmatrix} (\\frac{0.04}{2.05})(2.01) + (0)(1) & (\\frac{0.04}{2.05})(1) + (0)(1.01) \\\\ (-\\frac{1}{2.05})(2.01) + (1)(1) & (-\\frac{1}{2.05})(1) + (1)(1.01) \\end{bmatrix}\n$$\n$$\nP_{1|1} = \\begin{bmatrix} \\frac{0.0804}{2.05} & \\frac{0.04}{2.05} \\\\ \\frac{-2.01 + 2.05}{2.05} & \\frac{-1 + 1.01 \\cdot 2.05}{2.05} \\end{bmatrix} = \\begin{bmatrix} \\frac{0.0804}{2.05} & \\frac{0.04}{2.05} \\\\ \\frac{0.04}{2.05} & \\frac{1.0705}{2.05} \\end{bmatrix}\n$$\nThe problem requires the matrix entries to be rounded to $4$ significant figures.\n$$\nP_{1|1}(1,1) = \\frac{0.0804}{2.05} \\approx 0.0392195... \\approx 0.03922\n$$\n$$\nP_{1|1}(1,2) = P_{1|1}(2,1) = \\frac{0.04}{2.05} \\approx 0.0195121... \\approx 0.01951\n$$\n$$\nP_{1|1}(2,2) = \\frac{1.0705}{2.05} \\approx 0.5221951... \\approx 0.5222\n$$\nThus, the a posteriori error covariance matrix at $k=1$ is:\n$$\nP_{1|1} \\approx \\begin{bmatrix} 0.03922 & 0.01951 \\\\ 0.01951 & 0.5222 \\end{bmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.03922 & 0.01951 \\\\ 0.01951 & 0.5222 \\end{pmatrix}}\n$$", "id": "2888322"}, {"introduction": "Theoretical design must ultimately be validated through practice. This final exercise bridges the gap by requiring you to simulate and verify the performance of different observer designs, including those based on pole placement and the Kalman filter. By implementing the autonomous error dynamics $\\dot{e}(t) = (A-LC)e(t)$ and numerically checking the results against a formal definition of exponential convergence, you will gain a practical and quantitative understanding of how different gain selection strategies affect observer stability and transient response [@problem_id:2888309].", "problem": "Consider the continuous-time, linear time-invariant state-space model with state vector $x(t) \\in \\mathbb{R}^{2}$, input $u(t) \\in \\mathbb{R}$, and output $y(t) \\in \\mathbb{R}$:\n$$\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t),$$\nwhere\n$$A = \\begin{bmatrix} 0 & 1 \\\\ -1 & -1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}.$$\nAssume the input is $u(t) = \\sin(t)$, where the angle is in radians and time is in seconds. Consider a full-order deterministic observer with estimated state $\\hat{x}(t) \\in \\mathbb{R}^{2}$ and observer gain $L \\in \\mathbb{R}^{2 \\times 1}$. The initial estimation error is $e(0) = x(0) - \\hat{x}(0) = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n\nYour task is to derive from first principles and then implement a simulator to verify exponential convergence of the estimation error. In this problem, exponential convergence is defined as follows: there exist constants $k \\ge 1$ and $\\alpha > 0$ such that, for all $t \\ge 0$,\n$$\\lVert e(t) \\rVert_{2} \\le k \\, e^{-\\alpha t} \\lVert e(0) \\rVert_{2}.$$\n\nRequirements:\n- Start from the fundamental definitions of state-space systems, observers that utilize the innovation $y(t) - C \\hat{x}(t)$, and the induced dynamics of the estimation error. Formulate the condition under which the estimation error evolves autonomously and the structural matrix property that guarantees exponential decay in the sense stated above.\n- Implement a program that, for several observer designs, simulates the estimation error $e(t)$ over a uniform time grid $t_{i} = i h$ for $i = 0, 1, \\dots, N$ with $Nh = T$, using an exact or numerically stable method for linear time-invariant dynamics. You may select the step size $h$ prescribed per test case below. All angles must be in radians, and time must be in seconds.\n- For each test case, numerically verify the exponential bound by checking whether\n$$\\max_{0 \\le i \\le N} \\frac{\\lVert e(t_{i}) \\rVert_{2}}{\\lVert e(0) \\rVert_{2} \\, e^{-\\alpha t_{i}}} \\le k.$$\n\nTest suite:\n- Case $1$ (pole placement): choose $L$ so that the eigenvalues of the error dynamics matrix are at $\\{-2, -3\\}$. Use horizon $T = 6$, step $h = 0.01$, parameters $\\alpha = 1.0$, $k = 2.0$.\n- Case $2$ (Kalman-Bucy gain): choose $L$ via the estimator Algebraic Riccati Equation using process noise covariance $Q = q I_{2}$ with $q = 0.01$ and measurement noise covariance $R = [1]$. Use horizon $T = 6$, step $h = 0.01$, parameters $\\alpha = 0.25$, $k = 5.0$.\n- Case $3$ (open-loop model injection): use $L = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. Use horizon $T = 8$, step $h = 0.01$, parameters $\\alpha = 0.30$, $k = 5.0$.\n- Case $4$ (unstable design for contrast): use $L = \\begin{bmatrix} -2 \\\\ -3 \\end{bmatrix}$. Use horizon $T = 1.2$, step $h = 0.01$, parameters $\\alpha = 0.10$, $k = 10.0$.\n\nImplementation clarifications:\n- To ensure that the observer uses the same input $u(t)$ as the plant, and to focus on estimation error dynamics, you may simulate the estimation error directly under the autonomous dynamics induced by the chosen $L$. The input $u(t)$ is provided for completeness of the system model and is expressed using radians for the argument of the sine.\n- The initial error is $e(0) = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ for all cases.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be the string \"True\" or \"False\" corresponding to whether the exponential bound holds over the grid for that test case.", "solution": "The problem requires the derivation of the estimation error dynamics for a continuous-time linear time-invariant (LTI) system and the numerical verification of its exponential convergence for several observer designs.\n\nFirst, we establish the fundamental equations. The plant is described by:\n$$ \\dot{x}(t) = A x(t) + B u(t) $$\n$$ y(t) = C x(t) $$\nA full-order Luenberger observer is constructed to generate an estimate $\\hat{x}(t)$ of the state $x(t)$. The observer's dynamics are given by:\n$$ \\dot{\\hat{x}}(t) = A \\hat{x}(t) + B u(t) + L(y(t) - \\hat{y}(t)) $$\nwhere $L$ is the observer gain matrix and $\\hat{y}(t) = C\\hat{x}(t)$ is the estimated output. The term $y(t) - \\hat{y}(t)$ is the innovation or output error, which serves as a correction signal for the observer.\n\nThe state estimation error is defined as $e(t) = x(t) - \\hat{x}(t)$. To derive the dynamics of this error, we differentiate $e(t)$ with respect to time:\n$$ \\dot{e}(t) = \\dot{x}(t) - \\dot{\\hat{x}}(t) $$\nSubstituting the expressions for the state and observer dynamics:\n$$ \\dot{e}(t) = (A x(t) + B u(t)) - (A \\hat{x}(t) + B u(t) + L(y(t) - C\\hat{x}(t))) $$\nSubstitute $y(t) = C x(t)$ into the equation:\n$$ \\dot{e}(t) = A x(t) + B u(t) - A \\hat{x}(t) - B u(t) - L(C x(t) - C\\hat{x}(t)) $$\nThe terms involving the input $u(t)$ cancel. We can then group terms related to $x(t)$ and $\\hat{x}(t)$:\n$$ \\dot{e}(t) = A (x(t) - \\hat{x}(t)) - L C (x(t) - \\hat{x}(t)) $$\nBy definition of the error $e(t)$, this simplifies to:\n$$ \\dot{e}(t) = (A - L C) e(t) $$\nThis is the fundamental equation for the estimation error dynamics. Observe that the dynamics are autonomous, meaning they are independent of the system input $u(t)$ and the system state $x(t)$. The behavior of the error depends only on the initial error $e(0)$ and the properties of the matrix $A_e = A - LC$.\n\nFor the estimation error to converge to zero, the origin must be a stable equilibrium for the system $\\dot{e}(t) = A_e e(t)$. Exponential convergence, as defined by $\\lVert e(t) \\rVert_{2} \\le k \\, e^{-\\alpha t} \\lVert e(0) \\rVert_{2}$ for some constants $k \\ge 1$ and $\\alpha > 0$, is achieved if and only if the matrix $A_e = A - LC$ is Hurwitz. A matrix is Hurwitz if all of its eigenvalues have strictly negative real parts. If the eigenvalues of $A_e$ are $\\lambda_j$, then the condition is $\\text{Re}(\\lambda_j) < 0$ for all $j$. The stability margin is defined as $\\sigma = \\min_j \\{-\\text{Re}(\\lambda_j)\\}$. The bound can be satisfied for any $\\alpha$ such that $0 < \\alpha < \\sigma$.\n\nThe ability to arbitrarily place the eigenvalues of $A_e$ by choosing $L$ is guaranteed if the pair $(A, C)$ is observable. The observability matrix for the system is:\n$$ \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} $$\nWith the given matrices $A = \\begin{bmatrix} 0 & 1 \\\\ -1 & -1 \\end{bmatrix}$ and $C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$, we compute $CA = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ -1 & -1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\end{bmatrix}$. Thus, the observability matrix is:\n$$ \\mathcal{O} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $$\nSince $\\det(\\mathcal{O}) = 1 \\ne 0$, the system is completely observable, and pole placement is possible.\n\nThe numerical verification is performed by simulating the error dynamics over a time grid $t_i = i h$. The exact solution to the error dynamics is $e(t) = e^{A_e t} e(0)$. The discrete-time evolution is $e(t_{i+1}) = e^{A_e h} e(t_i)$. We compute the state-transition matrix $\\Phi_d = e^{A_e h}$ once and iterate $e_{i+1} = \\Phi_d e_i$. At each step, we verify if the inequality $\\lVert e(t_{i}) \\rVert_{2} \\le k \\, e^{-\\alpha t_{i}} \\lVert e(0) \\rVert_{2}$ holds.\n\nWe now analyze each test case:\n\nCase 1 (Pole Placement): We must choose $L = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix}$ to place the eigenvalues of $A_e = A - LC$ at $\\{-2, -3\\}$.\n$$ A_e = A - LC = \\begin{bmatrix} 0 & 1 \\\\ -1 & -1 \\end{bmatrix} - \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} -l_1 & 1 \\\\ -1-l_2 & -1 \\end{bmatrix} $$\nThe characteristic polynomial is $\\det(\\lambda I - A_e) = \\lambda^2 + (l_1+1)\\lambda + (l_1+1+l_2)$. The desired polynomial is $(\\lambda+2)(\\lambda+3) = \\lambda^2+5\\lambda+6$. Equating coefficients gives $l_1+1=5 \\implies l_1=4$ and $l_1+1+l_2=6 \\implies 5+l_2=6 \\implies l_2=1$. So, $L = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}$. The stability margin is $\\sigma=2$. The check with $\\alpha=1.0 < 2$ is appropriate.\n\nCase 2 (Kalman-Bucy Gain): The gain is $L = PC^T R^{-1}$, where $P$ is the symmetric positive semi-definite solution to the estimator Algebraic Riccati Equation (ARE): $AP + PA^T - PC^T R^{-1} C P + Q = 0$. With $Q = 0.01 I_2$ and $R = [1]$, this equation is solved numerically. The resulting filter is guaranteed to be stable since the system is observable.\n\nCase 3 (Open-loop Model Injection): Here, $L = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. The error dynamics matrix is $A_e = A$. The eigenvalues of $A$ are the roots of $\\lambda^2+\\lambda+1=0$, which are $\\lambda = -0.5 \\pm i \\frac{\\sqrt{3}}{2}$. The real part is $-0.5$, so the open-loop error dynamics are stable. The stability margin is $\\sigma=0.5$. The check with $\\alpha=0.3 < 0.5$ is plausible.\n\nCase 4 (Unstable Design): Here, $L = \\begin{bmatrix} -2 \\\\ -3 \\end{bmatrix}$. The error dynamics matrix is:\n$$ A_e = \\begin{bmatrix} -(-2) & 1 \\\\ -1-(-3) & -1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 2 & -1 \\end{bmatrix} $$\nThe characteristic polynomial is $\\det(\\lambda I - A_e) = (\\lambda-2)(\\lambda+1)-2 = \\lambda^2-\\lambda-4=0$. The eigenvalues are $\\lambda = \\frac{1 \\pm \\sqrt{17}}{2}$. One eigenvalue, $\\frac{1+\\sqrt{17}}{2} \\approx 2.56$, is positive. The error dynamics are unstable, so the error norm $\\lVert e(t) \\rVert_2$ will grow exponentially. The inequality for exponential decay cannot hold for all $t > 0$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm, solve_continuous_are\n\ndef check_convergence(A, C, L, T, h, alpha, k, e0):\n    \"\"\"\n    Simulates the error dynamics and verifies the exponential convergence bound.\n\n    Args:\n        A (np.ndarray): System matrix.\n        C (np.ndarray): Output matrix.\n        L (np.ndarray): Observer gain.\n        T (float): Simulation horizon.\n        h (float): Time step.\n        alpha (float): Decay rate for the bound.\n        k (float): Scaling factor for the bound.\n        e0 (np.ndarray): Initial estimation error.\n\n    Returns:\n        bool: True if the bound holds for all time steps, False otherwise.\n    \"\"\"\n    Ae = A - L @ C\n    e0_norm = np.linalg.norm(e0)\n    \n    # Pre-calculate the discrete-time state transition matrix\n    Phi_d = expm(Ae * h)\n    \n    e = e0.copy()\n    N = int(round(T / h))\n\n    for i in range(N + 1):\n        t = i * h\n        e_norm = np.linalg.norm(e)\n        \n        # Calculate the stability bound\n        bound = k * e0_norm * np.exp(-alpha * t)\n        \n        # In floating point arithmetic, a small tolerance is practical\n        # e_norm > bound is the strict check\n        if e_norm > bound + 1e-12:\n            return False\n            \n        # Update the error for the next time step\n        e = Phi_d @ e\n        \n    return True\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # System definition\n    A = np.array([[0, 1], [-1, -1]])\n    C = np.array([[1, 0]])\n    e0 = np.array([[1], [-1]])\n\n    test_cases = [\n        {'id': 1, 'type': 'pole_placement', 'T': 6.0, 'h': 0.01, 'alpha': 1.0, 'k': 2.0},\n        {'id': 2, 'type': 'kalman_bucy', 'q': 0.01, 'R_val': 1.0, 'T': 6.0, 'h': 0.01, 'alpha': 0.25, 'k': 5.0},\n        {'id': 3, 'type': 'open_loop', 'T': 8.0, 'h': 0.01, 'alpha': 0.30, 'k': 5.0},\n        {'id': 4, 'type': 'unstable', 'T': 1.2, 'h': 0.01, 'alpha': 0.10, 'k': 10.0}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        L = None\n        if case['type'] == 'pole_placement':\n            # For eigenvalues {-2, -3}, characteristic polynomial is (s+2)(s+3) = s^2 + 5s + 6\n            # Char poly of (A-LC) is s^2 + (l1+1)s + (l1+l2+1)\n            # l1+1=5 -> l1=4\n            # l1+l2+1=6 -> 4+l2+1=6 -> l2=1\n            L = np.array([[4.0], [1.0]])\n        \n        elif case['type'] == 'kalman_bucy':\n            Q = case['q'] * np.eye(2)\n            R = np.array([[case['R_val']]])\n            # Scipy's solve_continuous_are solves the regulator ARE: A'X + XA - XBR^-1B'X + Q = 0\n            # The filter ARE is AP + PA' - PC'R^-1CP + Q = 0.\n            # By duality, we solve the regulator ARE with A.T, C.T, Q, R.\n            P = solve_continuous_are(A.T, C.T, Q, R)\n            L = P @ C.T @ np.linalg.inv(R)\n\n        elif case['type'] == 'open_loop':\n            L = np.array([[0.0], [0.0]])\n\n        elif case['type'] == 'unstable':\n            L = np.array([[-2.0], [-3.0]])\n\n        result = check_convergence(A, C, L, case['T'], case['h'], case['alpha'], case['k'], e0)\n        results.append(str(result))\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2888309"}]}