{"hands_on_practices": [{"introduction": "Understanding the input-output properties of a system is a cornerstone of control theory. This exercise provides fundamental practice in certifying performance by connecting a system's state-space model to its induced $L_2$ gain, which for LTI systems is the $\\mathcal{H}_\\infty$ norm. By applying the Kalman-Yakubovich-Popov (KYP) lemma, you will construct a storage function to find the tightest possible performance bound, offering a concrete look at the dissipativity concepts that underpin modern robust control analysis [@problem_id:2901523].", "problem": "Consider the Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) system with state-space realization\n$$\n\\dot{x}(t) = -2\\,x(t) + w(t), \\quad z(t) = x(t),\n$$\nwhere $x(t) \\in \\mathbb{R}$ is the state, $w(t) \\in \\mathbb{R}$ is the exogenous input, and $z(t) \\in \\mathbb{R}$ is the performance output. Assume the initial condition $x(0)=0$. The induced $L_{2}$ gain from $w$ to $z$ equals the $\\mathcal{H}_{\\infty}$ norm for this asymptotically stable LTI system.\n\nStarting only from the definition of the induced $L_{2}$ gain and the dissipation inequality with supply rate $s(w,z) = z^{\\top} z - \\gamma^{2} w^{\\top} w$, use the Kalman–Yakubovich–Popov (KYP) lemma to construct a quadratic storage function $V(x) = x^{\\top} P x$ with $P \\succ 0$ that certifies an upper bound $\\gamma$ on the induced $L_{2}$ gain. Explicitly derive the associated Linear Matrix Inequality (LMI) condition and reduce it to scalar inequalities appropriate for this system. Determine the minimal bound $\\gamma_{\\star}$ for which there exists such $P \\succ 0$, exhibit one feasible $P$ at $\\gamma = \\gamma_{\\star}$, and directly verify the dissipation inequality\n$$\n\\dot{V}(x) + z^{\\top}z - \\gamma^{2} w^{\\top} w \\le 0\n$$\nfor all $x \\in \\mathbb{R}$ and $w \\in \\mathbb{R}$.\n\nExpress the final answer as the exact real number equal to the minimal achievable induced $L_{2}$ gain bound $\\gamma_{\\star}$. No rounding is required. State your final answer without units.", "solution": "The problem requires the determination of the minimal induced $L_{2}$ gain bound $\\gamma_{\\star}$ for a given Linear Time-Invariant (LTI) system, certified by a quadratic storage function via the Kalman–Yakubovich–Popov (KYP) lemma framework.\n\nThe system is given by the state-space equations:\n$$\n\\dot{x}(t) = -2\\,x(t) + w(t), \\quad z(t) = x(t)\n$$\nThis corresponds to the general LTI form $\\dot{x} = Ax + Bw$ and $z = Cx + Dw$ with the following scalar coefficients, which we will treat as $1 \\times 1$ matrices for formal consistency:\n$A = -2$, $B = 1$, $C = 1$, and $D = 0$. The state $x$, input $w$, and output $z$ are all in $\\mathbb{R}$.\n\nThe problem is to find the smallest $\\gamma > 0$ for which there exists a quadratic storage function $V(x) = x^{\\top}Px = Px^2$ with $P > 0$ satisfying the dissipation inequality:\n$$\n\\frac{d}{dt}V(x(t)) + s(w(t), z(t)) \\le 0\n$$\nfor all trajectories of the system. The supply rate is given as $s(w,z) = z^{\\top}z - \\gamma^2 w^{\\top}w$. With zero initial conditions, the existence of such a $V(x)$ guarantees that the induced $L_2$ gain is bounded by $\\gamma$. The inequality is:\n$$\n\\dot{V}(x) + z^{\\top}z - \\gamma^2 w^{\\top}w \\le 0\n$$\nFirst, we compute the time derivative of the storage function $V(x) = Px^2$:\n$$\n\\dot{V}(x) = \\frac{d}{dt}(Px^2) = 2Px\\dot{x}\n$$\nSubstituting the state equation $\\dot{x} = -2x+w$:\n$$\n\\dot{V}(x) = 2Px(-2x+w) = -4Px^2 + 2Pxw\n$$\nNow, substituting $\\dot{V}(x)$ and $z=x$ into the dissipation inequality:\n$$\n(-4Px^2 + 2Pxw) + (x^2) - \\gamma^2 w^2 \\le 0\n$$\nRearranging the terms yields a quadratic form in $x$ and $w$:\n$$\n(1 - 4P)x^2 + 2Pxw - \\gamma^2 w^2 \\le 0\n$$\nThis inequality must hold for all $x \\in \\mathbb{R}$ and $w \\in \\mathbb{R}$. We can express the left-hand side in matrix form:\n$$\n\\begin{pmatrix} x & w \\end{pmatrix}\n\\begin{pmatrix}\n1 - 4P & P \\\\\nP & -\\gamma^2\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ w \\end{pmatrix}\n\\le 0\n$$\nFor this quadratic form to be negative semidefinite for all $x$ and $w$, the matrix must be negative semidefinite. This is the Linear Matrix Inequality (LMI) associated with the problem:\n$$\n\\begin{pmatrix}\n1 - 4P & P \\\\\nP & -\\gamma^2\n\\end{pmatrix}\n\\preceq 0\n$$\nA symmetric matrix is negative semidefinite if and only if its leading principal minors alternate in sign, starting with non-positive.\n$1$. The first principal minor (the $(1,1)$ element) must be non-positive:\n$$\n1 - 4P \\le 0 \\implies 4P \\ge 1 \\implies P \\ge \\frac{1}{4}\n$$\n$2$. The determinant (the second principal minor) must be non-negative:\n$$\n(1 - 4P)(-\\gamma^2) - P^2 \\ge 0\n$$\n$$\n- \\gamma^2 + 4P\\gamma^2 - P^2 \\ge 0\n$$\n$$\nP^2 - 4\\gamma^2 P + \\gamma^2 \\le 0\n$$\nWe need to find the minimum $\\gamma > 0$ for which there exists a $P$ satisfying both $P \\ge 1/4$ and $P^2 - 4\\gamma^2 P + \\gamma^2 \\le 0$. The condition $P \\succ 0$ from the problem statement becomes $P > 0$, which is guaranteed by $P \\ge 1/4$.\n\nConsider the quadratic inequality in $P$, $f(P) = P^2 - 4\\gamma^2 P + \\gamma^2 \\le 0$. This describes a parabola opening upwards. For this inequality to have a solution for $P$, the quadratic equation $f(P) = 0$ must have real roots. This requires its discriminant to be non-negative:\n$$\n\\Delta = (-4\\gamma^2)^2 - 4(1)(\\gamma^2) = 16\\gamma^4 - 4\\gamma^2 \\ge 0\n$$\nSince we are seeking $\\gamma > 0$, we can divide by $4\\gamma^2$:\n$$\n4\\gamma^2 - 1 \\ge 0 \\implies \\gamma^2 \\ge \\frac{1}{4}\n$$\nAs $\\gamma$ represents a gain, it must be non-negative, so $\\gamma \\ge \\frac{1}{2}$. The minimal value for $\\gamma$ for which a solution $P$ exists is therefore $\\gamma_{\\star} = \\frac{1}{2}$.\n\nNow, we must exhibit a feasible $P > 0$ for this minimal bound $\\gamma_{\\star}$. Let $\\gamma = \\gamma_{\\star} = \\frac{1}{2}$. The inequality for $P$ becomes:\n$$\nP^2 - 4\\left(\\frac{1}{2}\\right)^2 P + \\left(\\frac{1}{2}\\right)^2 \\le 0\n$$\n$$\nP^2 - P + \\frac{1}{4} \\le 0\n$$\nThis expression is a perfect square:\n$$\n\\left(P - \\frac{1}{2}\\right)^2 \\le 0\n$$\nSince the square of a real number cannot be negative, the only solution is $\\left(P - \\frac{1}{2}\\right)^2 = 0$, which implies $P = \\frac{1}{2}$. This solution satisfies the conditions $P > 0$ and $P \\ge 1/4$. Thus, for $\\gamma_{\\star} = 1/2$, there is a unique feasible storage function parameter $P=1/2$.\n\nFinally, we directly verify the dissipation inequality for $\\gamma = \\gamma_{\\star} = 1/2$ and $P=1/2$.\nThe storage function is $V(x) = \\frac{1}{2}x^2$. Its derivative is $\\dot{V}(x) = x\\dot{x} = x(-2x+w) = -2x^2+xw$.\nThe inequality $\\dot{V}(x) + z^2 - \\gamma^2 w^2 \\le 0$ becomes:\n$$\n(-2x^2 + xw) + x^2 - \\left(\\frac{1}{2}\\right)^2 w^2 \\le 0\n$$\n$$\n-x^2 + xw - \\frac{1}{4}w^2 \\le 0\n$$\nFactoring out a negative sign:\n$$\n-\\left(x^2 - xw + \\frac{1}{4}w^2\\right) \\le 0\n$$\nThe expression in the parenthesis is a perfect square:\n$$\n-\\left(x - \\frac{1}{2}w\\right)^2 \\le 0\n$$\nThis inequality is true for all $x \\in \\mathbb{R}$ and $w \\in \\mathbb{R}$, as the square of any real number is non-negative. This completes the verification. The minimal achievable induced $L_{2}$ gain bound is $\\gamma_{\\star}$.\nThe problem is well-posed and the analysis yields a conclusive result. The minimal bound is $\\gamma_{\\star} = 1/2$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2901523"}, {"introduction": "Moving from analysis to synthesis, this practice guides you through the design of an optimal $\\mathcal{H}_2$ controller, famously solved by the Linear-Quadratic-Gaussian (LQG) framework. You will apply the separation principle, a cornerstone of modern control, to independently design a state-feedback regulator and a state estimator by solving their respective algebraic Riccati equations. This exercise provides a complete, hands-on walkthrough of a canonical optimal control problem, from computing the controller gains to verifying the stability of the final closed-loop system [@problem_id:2901515].", "problem": "Consider a continuous-time linear time-invariant plant arising in two-norm-based optimal control (the $\\mathcal{H}_{2}$ method) with a Linear Quadratic Gaussian (LQG) structure. The state-space dynamics are\n$$\n\\dot{x}(t) = A\\,x(t) + B\\,u(t) + w(t), \\quad y(t) = C\\,x(t) + v(t),\n$$\nwith state $x(t) \\in \\mathbb{R}^{2}$, control $u(t) \\in \\mathbb{R}^{2}$, measurement $y(t) \\in \\mathbb{R}^{2}$, process disturbance $w(t)$, and measurement disturbance $v(t)$. The matrices are\n$$\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & -3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n$$\nAssume the standard infinite-horizon quadratic performance index with state-weighting matrix\n$$\nQ = \\begin{bmatrix} 2 & 0 \\\\ 0 & 12 \\end{bmatrix}\n$$\nand control-weighting matrix\n$$\nR = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix},\n$$\ntogether with white Gaussian process and measurement disturbances having covariances\n$$\nW = \\begin{bmatrix} 9 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix},\n$$\nand zero cross-covariance. Using the Riccati solutions $X \\succeq 0$ and $Y \\succeq 0$ associated with the continuous-time LQG problem, construct the optimal dynamic output-feedback controller in observer-based realization\n$$\n\\dot{\\hat{x}}(t) = A_{k}\\,\\hat{x}(t) + B_{k}\\,y(t), \\quad u(t) = C_{k}\\,\\hat{x}(t) + D_{k}\\,y(t),\n$$\nand form the deterministic closed-loop augmented state matrix (with $w(t) \\equiv 0$ and $v(t) \\equiv 0$) for the combined plant-controller interconnection. Then, by examining the eigenvalues of this closed-loop state matrix, determine the maximum real part among all its eigenvalues. Express your final answer as an exact analytic expression. No rounding is required.", "solution": "The problem presented is a standard continuous-time Linear Quadratic Gaussian (LQG) control problem. We begin by performing a rigorous validation of the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe system dynamics are given by $\\dot{x}(t) = A\\,x(t) + B\\,u(t) + w(t)$ and $y(t) = C\\,x(t) + v(t)$.\nThe matrices defining the plant are:\n$A = \\begin{bmatrix} 1 & 0 \\\\ 0 & -3 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\n\nThe performance index weighting matrices are:\n$Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 12 \\end{bmatrix}$, $R = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}$.\n\nThe noise covariance matrices are:\n$W = \\begin{bmatrix} 9 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 4 \\end{bmatrix}$.\n\nThe problem requires constructing the optimal dynamic output-feedback controller and finding the maximum real part of the eigenvalues of the resulting deterministic closed-loop system matrix.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is a textbook application of LQG control theory, which is a fundamental concept in optimal control and signal processing. It is scientifically sound.\n2.  **Well-Posed:** For a unique, stabilizing solution to the LQG problem to exist, two conditions must be met:\n    *   The pair $(A, B)$ must be stabilizable. The system matrix $A$ has eigenvalues at $1$ (unstable) and $-3$ (stable). Since $A, B$ are diagonal, we examine the modes separately. The unstable mode corresponding to eigenvalue $1$ is controllable because the corresponding element in $B$ is non-zero ($b_{11}=1$). The stable mode is inherently stabilizable. Thus, $(A, B)$ is stabilizable.\n    *   The pair $(A, C)$ must be detectable. Similarly, the unstable mode corresponding to eigenvalue $1$ is observable because the corresponding element in $C$ is non-zero ($c_{11}=1$). The stable mode is inherently detectable. Thus, $(A, C)$ is detectable.\n    The stabilizability and detectability conditions ensure the existence of unique positive semi-definite stabilizing solutions $X$ and $Y$ to the respective algebraic Riccati equations.\n3.  **Objective and Complete:** The problem is formulated with precise mathematical definitions and provides all necessary matrices and parameters. No information is missing, contradictory, or ambiguous.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. We proceed with the solution.\n\n### Solution Derivation\n\nThe LQG control problem is addressed using the separation principle, which allows for the separate design of an optimal state-feedback controller (LQR) and an optimal state estimator (Kalman filter).\n\n**1. Linear Quadratic Regulator (LQR) Design**\n\nThe optimal state-feedback gain $K$ is found by solving the continuous-time Algebraic Riccati Equation (CARE) for the control problem:\n$$A^T X + X A - X B R^{-1} B^T X + Q = 0$$\nwhere $X \\succeq 0$ is the solution matrix. Given that $A, B, Q, R$ are all diagonal, the solution $X$ will also be diagonal. Let $X = \\text{diag}(x_1, x_2)$. The matrix equation decouples into two scalar algebraic Riccati equations.\n\nThe inverse of the control-weighting matrix is $R^{-1} = \\text{diag}(1, 1/2)$.\nSince $B = I$, the term $B R^{-1} B^T$ is simply $R^{-1}$. The CARE simplifies to $A^T X + X A - X R^{-1} X + Q = 0$.\n\nFor the first mode (indexed by $1$):\n$1 \\cdot x_1 + x_1 \\cdot 1 - x_1 \\cdot 1 \\cdot x_1 + 2 = 0 \\implies x_1^2 - 2x_1 - 2 = 0$.\nThe solutions are $x_1 = \\frac{2 \\pm \\sqrt{4 - 4(1)(-2)}}{2} = 1 \\pm \\sqrt{3}$. For $X \\succeq 0$, we require $x_1 \\ge 0$, so we select the positive solution $x_1 = 1 + \\sqrt{3}$.\n\nFor the second mode (indexed by $2$):\n$(-3) \\cdot x_2 + x_2 \\cdot (-3) - x_2 \\cdot (1/2) \\cdot x_2 + 12 = 0 \\implies \\frac{1}{2}x_2^2 + 6x_2 - 12 = 0 \\implies x_2^2 + 12x_2 - 24 = 0$.\nThe solutions are $x_2 = \\frac{-12 \\pm \\sqrt{144 - 4(1)(-24)}}{2} = \\frac{-12 \\pm \\sqrt{240}}{2} = -6 \\pm 2\\sqrt{15}$. For $x_2 \\ge 0$, we must choose $x_2 = -6 + 2\\sqrt{15}$.\n\nThus, the solution to the CARE is $X = \\begin{bmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & -6+2\\sqrt{15} \\end{bmatrix}$.\nThe optimal LQR gain is $K = R^{-1} B^T X = R^{-1} X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1/2 \\end{bmatrix} \\begin{bmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & -6+2\\sqrt{15} \\end{bmatrix} = \\begin{bmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & -3+\\sqrt{15} \\end{bmatrix}$.\n\n**2. Kalman Filter Design**\n\nThe optimal estimator gain $L$ is found by solving the dual CARE for the filtering problem:\n$$A Y + Y A^T - Y C^T V^{-1} C Y + W = 0$$\nwhere $Y \\succeq 0$ is the state estimation error covariance. As before, we assume a diagonal solution $Y = \\text{diag}(y_1, y_2)$.\n\nThe inverse of the measurement noise covariance is $V^{-1} = \\text{diag}(1, 1/4)$.\nSince $C = I$, the term $C^T V^{-1} C$ is simply $V^{-1}$. The filter ARE is $A Y + Y A^T - Y V^{-1} Y + W = 0$.\n\nFor the first mode:\n$1 \\cdot y_1 + y_1 \\cdot 1 - y_1 \\cdot 1 \\cdot y_1 + 9 = 0 \\implies y_1^2 - 2y_1 - 9 = 0$.\nThe solutions are $y_1 = \\frac{2 \\pm \\sqrt{4 - 4(1)(-9)}}{2} = 1 \\pm \\sqrt{10}$. For $Y \\succeq 0$, we select $y_1 = 1 + \\sqrt{10}$.\n\nFor the second mode:\n$(-3) \\cdot y_2 + y_2 \\cdot (-3) - y_2 \\cdot (1/4) \\cdot y_2 + 1 = 0 \\implies \\frac{1}{4}y_2^2 + 6y_2 - 1 = 0 \\implies y_2^2 + 24y_2 - 4 = 0$.\nThe solutions are $y_2 = \\frac{-24 \\pm \\sqrt{576 - 4(1)(-4)}}{2} = \\frac{-24 \\pm \\sqrt{592}}{2} = -12 \\pm 2\\sqrt{37}$. For $y_2 \\ge 0$, we must choose $y_2 = -12 + 2\\sqrt{37}$.\n\nThus, the solution is $Y = \\begin{bmatrix} 1+\\sqrt{10} & 0 \\\\ 0 & -12+2\\sqrt{37} \\end{bmatrix}$.\nThe optimal filter gain is $L = Y C^T V^{-1} = Y V^{-1} = \\begin{bmatrix} 1+\\sqrt{10} & 0 \\\\ 0 & -12+2\\sqrt{37} \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{bmatrix} = \\begin{bmatrix} 1+\\sqrt{10} & 0 \\\\ 0 & -3+\\frac{1}{2}\\sqrt{37} \\end{bmatrix}$.\n\n**3. Closed-Loop System Eigenvalues**\n\nThe LQG controller is given by $\\dot{\\hat{x}} = (A - BK - LC)\\hat{x} + Ly$ and $u = -K\\hat{x}$. The augmented state vector for the plant and controller is $x_{cl} = [x^T \\ \\hat{x}^T]^T$. The deterministic closed-loop state matrix is:\n$$A_{cl} = \\begin{bmatrix} A & -BK \\\\ LC & A-BK-LC \\end{bmatrix}$$\nAccording to the separation principle, the eigenvalues of $A_{cl}$ are the union of the eigenvalues of the regulator matrix $(A-BK)$ and the estimator matrix $(A-LC)$.\n\nThe eigenvalues of the regulator matrix $(A-BK)$ are found from:\n$A - BK = A - K = \\begin{bmatrix} 1 & 0 \\\\ 0 & -3 \\end{bmatrix} - \\begin{bmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & -3+\\sqrt{15} \\end{bmatrix} = \\begin{bmatrix} -\\sqrt{3} & 0 \\\\ 0 & -\\sqrt{15} \\end{bmatrix}$.\nThe eigenvalues are $\\lambda_{1,reg} = -\\sqrt{3}$ and $\\lambda_{2,reg} = -\\sqrt{15}$.\n\nThe eigenvalues of the estimator matrix $(A-LC)$ are found from:\n$A - LC = A - L = \\begin{bmatrix} 1 & 0 \\\\ 0 & -3 \\end{bmatrix} - \\begin{bmatrix} 1+\\sqrt{10} & 0 \\\\ 0 & -3+\\frac{1}{2}\\sqrt{37} \\end{bmatrix} = \\begin{bmatrix} -\\sqrt{10} & 0 \\\\ 0 & -\\frac{1}{2}\\sqrt{37} \\end{bmatrix}$.\nThe eigenvalues are $\\lambda_{1,est} = -\\sqrt{10}$ and $\\lambda_{2,est} = -\\frac{1}{2}\\sqrt{37}$.\n\nThe set of all eigenvalues of the closed-loop system is $\\{-\\sqrt{3}, -\\sqrt{15}, -\\sqrt{10}, -\\frac{1}{2}\\sqrt{37}\\}$. All eigenvalues are real and negative, which confirms that the closed-loop system is stable.\n\n**4. Maximum Real Part**\n\nWe must find the maximum value in the set $\\{-\\sqrt{3}, -\\sqrt{15}, -\\sqrt{10}, -\\frac{\\sqrt{37}}{2}\\}$. This is equivalent to finding the minimum magnitude among $\\{\\sqrt{3}, \\sqrt{15}, \\sqrt{10}, \\frac{\\sqrt{37}}{2}\\}$.\nLet's compare the arguments of the square roots:\n$\\sqrt{3}$\n$\\sqrt{15}$\n$\\sqrt{10}$\n$\\frac{\\sqrt{37}}{2} = \\sqrt{\\frac{37}{4}} = \\sqrt{9.25}$\n\nWe have the ordering $3 < 9.25 < 10 < 15$.\nTherefore, $\\sqrt{3} < \\sqrt{9.25} < \\sqrt{10} < \\sqrt{15}$.\nThis implies $-\\sqrt{3} > -\\sqrt{9.25} > -\\sqrt{10} > -\\sqrt{15}$.\n\nThe maximum real part among all eigenvalues is $-\\sqrt{3}$.", "answer": "$$\n\\boxed{-\\sqrt{3}}\n$$", "id": "2901515"}, {"introduction": "While $\\mathcal{H}_\\infty$ synthesis provides robustness against unstructured uncertainty, practical problems often involve uncertainties with known structures, such as parametric variations. This advanced exercise introduces the structured singular value, $\\mu$, a powerful tool for analyzing robust performance and stability in the presence of such structured uncertainty [@problem_id:2901532]. Your task is to implement the D-scaling method, which provides a computable upper bound for $\\mu$, bridging the gap between theoretical concepts and the numerical algorithms essential for modern robust control design.", "problem": "Consider the structured singular value for a complex matrix at a fixed frequency, defined for a block-diagonal uncertainty set. Let the structured uncertainty be block-diagonal with given positive integer block sizes. The structured singular value of a complex square matrix $M \\in \\mathbb{C}^{n \\times n}$ with respect to the given block structure is defined as the reciprocal of the smallest norm of an uncertainty $\\Delta$ in that structure that drives $I - M \\Delta$ to singularity, if such a $\\Delta$ exists, and zero otherwise. Your objective is to start from the exact definition of the structured singular value and the fundamental properties of norms and similarity transforms, and derive an implementable upper bound that uses positive-definite diagonal scaling matrices $D$ that are block-diagonal and commute with the uncertainty structure. Then, implement a numerical algorithm that computes this upper bound by optimizing over such $D$.\n\nYou must:\n- Begin from the definition of the structured singular value and use only core definitions and the submultiplicativity of the spectral norm to derive a computable upper bound based on block-diagonal positive-definite scaling matrices that commute with the uncertainty structure.\n- Model the admissible scaling matrices $D$ as block-diagonal with one positive scalar per uncertainty block, each replicated along that block’s diagonal to match its size. That is, if the uncertainty has $b$ blocks with sizes $n_{1}, \\dots, n_{b}$, then $D = \\mathrm{diag}(d_{1} I_{n_{1}}, \\dots, d_{b} I_{n_{b}})$ with $d_{i} &gt; 0$.\n- Reduce the optimization to a finite-dimensional unconstrained problem by working with logarithms of the scale factors so that positivity is automatically enforced.\n- Implement a robust numerical procedure to compute the bound for each test case below. You may use iterative numerical optimization and should take care to ensure deterministic behavior.\n\nTest suite:\nEach test case provides a single-frequency complex matrix $M$ and a list of block sizes indicating the uncertainty structure. For each test case, compute the value of the upper bound obtained via the block-diagonal scaling approach described above.\n\n- Test case 1:\n  - $M \\in \\mathbb{C}^{2 \\times 2}$ given by\n    $$\n    M = \\begin{bmatrix}\n    0.2 + 0.1 \\mathrm{i} & 2.0 - 0.3 \\mathrm{i} \\\\\n    0.0 + 0.5 \\mathrm{i} & 0.1 - 0.2 \\mathrm{i}\n    \\end{bmatrix}.\n    $$\n  - Block sizes: $[1, 1]$.\n- Test case 2:\n  - $M \\in \\mathbb{C}^{3 \\times 3}$ given by\n    $$\n    M = \\begin{bmatrix}\n    0.05 + 0.0 \\mathrm{i} & 1.0 - 0.1 \\mathrm{i} & 0.2 - 0.1 \\mathrm{i} \\\\\n    0.0 + 0.1 \\mathrm{i} & 0.3 + 0.0 \\mathrm{i} & 2.0 + 0.2 \\mathrm{i} \\\\\n    0.0 + 0.0 \\mathrm{i} & 0.5 - 0.1 \\mathrm{i} & 0.0 + 0.1 \\mathrm{i}\n    \\end{bmatrix}.\n    $$\n  - Block sizes: $[1, 2]$.\n- Test case 3:\n  - $M \\in \\mathbb{C}^{4 \\times 4}$ is block-diagonal with two blocks of size $2$,\n    $$\n    M = \\mathrm{diag}(A, B), \\quad\n    A = \\begin{bmatrix}\n    0.8 + 0.05 \\mathrm{i} & 0.02 - 0.01 \\mathrm{i} \\\\\n    0.01 + 0.0 \\mathrm{i} & 0.7 + 0.02 \\mathrm{i}\n    \\end{bmatrix}, \\quad\n    B = \\begin{bmatrix}\n    0.6 + 0.04 \\mathrm{i} & 0.03 + 0.0 \\mathrm{i} \\\\\n    0.0 + 0.01 \\mathrm{i} & 0.65 + 0.03 \\mathrm{i}\n    \\end{bmatrix}.\n    $$\n  - Block sizes: $[2, 2]$.\n\nOutput specification:\n- For each test case, compute the upper bound value obtained by optimizing over all admissible block-diagonal positive-definite scaling matrices $D$ as described above.\n- Express each bound as a real floating-point number rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[x1,x2,x3]\"), where $x_{k}$ is the computed bound for test case $k$.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and complete. It describes a standard problem in robust control theory: the computation of an upper bound for the structured singular value, $\\mu$. The provided definitions, matrices, and objectives are consistent and sufficient for a rigorous solution. We proceed with the derivation and implementation.\n\nThe structured singular value, $\\mu_{\\Delta}(M)$, of a complex matrix $M \\in \\mathbb{C}^{n \\times n}$ with respect to a block-diagonal uncertainty structure $\\Delta$ is defined as:\n$$\n\\mu_{\\Delta}(M) \\triangleq \\frac{1}{\\min \\{ \\|\\Delta\\|_2 \\mid \\Delta \\in \\mathcal{B}_{\\Delta}, \\det(I - M\\Delta) = 0 \\}}\n$$\nif a matrix $\\Delta$ exists that makes the matrix $I - M\\Delta$ singular, and $\\mu_{\\Delta}(M) \\triangleq 0$ otherwise. Here, $\\|\\cdot\\|_2$ denotes the spectral norm (maximum singular value), and $\\mathcal{B}_{\\Delta}$ is the set of all block-diagonal uncertainty matrices conforming to the specified structure. That is, if the block sizes are $n_1, n_2, \\dots, n_b$ such that $\\sum_{i=1}^{b} n_i = n$, then $\\Delta = \\mathrm{diag}(\\Delta_1, \\dots, \\Delta_b)$ where each $\\Delta_i \\in \\mathbb{C}^{n_i \\times n_i}$.\n\nThe condition $\\det(I - M\\Delta) = 0$ is equivalent to stating that $1$ is an eigenvalue of the matrix $M\\Delta$. This implies the existence of a non-zero vector $x \\in \\mathbb{C}^n$ such that $M\\Delta x = x$.\n\nWe are to derive an upper bound using a set of scaling matrices $\\mathcal{D}$. This set consists of block-diagonal, positive-definite matrices that commute with the uncertainty structure. A matrix $D$ commutes with the uncertainty structure if $D\\Delta = \\Delta D$ for all $\\Delta \\in \\mathcal{B}_{\\Delta}$. The prescribed form for $D \\in \\mathcal{D}$ is:\n$$\nD = \\mathrm{diag}(d_1 I_{n_1}, d_2 I_{n_2}, \\dots, d_b I_{n_b})\n$$\nwhere $d_i \\in \\mathbb{R}$, $d_i > 0$, for $i = 1, \\dots, b$, and $I_{n_i}$ is the identity matrix of size $n_i \\times n_i$. Since $D$ and any $\\Delta$ from the set have the same block-diagonal structure, they commute.\n\nLet us analyze the relationship between the singularity condition for $M$ and a scaled version of $M$. For any invertible $D \\in \\mathcal{D}$, the condition $\\det(I - M\\Delta) = 0$ is equivalent to $\\det(I - DMD^{-1}\\Delta) = 0$. This can be shown as follows:\n$$\n\\det(I - M\\Delta) = \\det(D(I - M\\Delta)D^{-1}) = \\det(D I D^{-1} - D M \\Delta D^{-1}) = \\det(I - D M D^{-1} \\Delta)\n$$\nwhere the last step uses the commutative property $D\\Delta = \\Delta D$. This means that the set of singularizing uncertainties $\\Delta$ is identical for both $M$ and the similarity-transformed matrix $DMD^{-1}$. Consequently, their structured singular values are equal:\n$$\n\\mu_{\\Delta}(M) = \\mu_{\\Delta}(DMD^{-1}) \\quad \\forall D \\in \\mathcal{D}\n$$\n\nNext, we establish a general upper bound for $\\mu_{\\Delta}(A)$ for any matrix $A$. If $\\det(I - A\\Delta) = 0$, then there exists a vector $x \\ne 0$ such that $A\\Delta x = x$. Taking the spectral norm of both sides and applying the submultiplicative property of the norm, we have:\n$$ \\|x\\|_2 = \\|A\\Delta x\\|_2 \\le \\|A\\|_2 \\|\\Delta x\\|_2 \\le \\|A\\|_2 \\|\\Delta\\|_2 \\|x\\|_2 $$\nSince $\\|x\\|_2 > 0$, we can divide by it to obtain $1 \\le \\|A\\|_2 \\|\\Delta\\|_2$, which implies that any singularizing perturbation $\\Delta$ must satisfy $\\|\\Delta\\|_2 \\ge 1/\\|A\\|_2$. The minimum norm of such a $\\Delta$ must therefore also satisfy this inequality:\n$$\n\\min \\{ \\|\\Delta\\|_2 \\mid \\det(I - A\\Delta)=0 \\} \\ge \\frac{1}{\\|A\\|_2}\n$$\nTaking the reciprocal of both sides reverses the inequality:\n$$\n\\mu_{\\Delta}(A) = \\left( \\min \\{ \\|\\Delta\\|_2 \\} \\right)^{-1} \\le \\|A\\|_2 = \\sigma_{\\max}(A)\n$$\nwhere $\\sigma_{\\max}(A)$ is the maximum singular value of $A$.\n\nCombining these two results, we have $\\mu_{\\Delta}(M) = \\mu_{\\Delta}(DMD^{-1}) \\le \\sigma_{\\max}(DMD^{-1})$. This inequality holds for any choice of $D$ from the admissible set $\\mathcal{D}$. To find the tightest possible upper bound from this family of bounds, we should seek the infimum over all such $D$:\n$$\n\\mu_{\\Delta}(M) \\le \\inf_{D \\in \\mathcal{D}} \\sigma_{\\max}(DMD^{-1})\n$$\nThis is the computable upper bound we seek to implement. The task is now an optimization problem:\n$$\n\\min_{d_1 > 0, \\dots, d_b > 0} f(d_1, \\dots, d_b) = \\sigma_{\\max}\\left( \\mathrm{diag}(d_1 I_{n_1}, \\dots) M \\mathrm{diag}(d_1^{-1} I_{n_1}, \\dots) \\right)\n$$\nThe positivity constraints $d_i > 0$ make this a constrained optimization problem. As suggested, we can convert it into an unconstrained problem by a change of variables. Let $d_i = e^{\\alpha_i}$ for $\\alpha_i \\in \\mathbb{R}$. The optimization variables are now $\\alpha_1, \\dots, \\alpha_b$, and the positivity of $d_i$ is inherently satisfied. The objective function becomes:\n$$\ng(\\alpha_1, \\dots, \\alpha_b) = \\sigma_{\\max}(D(\\alpha) M D(\\alpha)^{-1})\n$$\nwhere $D(\\alpha) = \\mathrm{diag}(e^{\\alpha_1} I_{n_1}, \\dots, e^{\\alpha_b} I_{n_b})$. The problem is to find $\\min_{\\alpha \\in \\mathbb{R}^b} g(\\alpha)$. Note that scaling all $d_i$ by a common positive factor $c$ does not change the value of $DMD^{-1}$, as $c D M (c D)^{-1} = c D M c^{-1} D^{-1} = DMD^{-1}$. This means the objective function is constant along the direction $[1, 1, \\dots, 1]^T$ in the $\\alpha$-space. The optimization problem is convex, ensuring that any local minimum is also a global minimum.\n\nTo solve this numerically, we employ a quasi-Newton optimization algorithm, specifically L-BFGS-B, provided by the `scipy.optimize.minimize` function. This method is suitable for unconstrained or box-constrained problems and is efficient for smooth objective functions. While the maximum singular value function is not differentiable everywhere (specifically, when singular values are repeated), it is differentiable almost everywhere, and standard gradient-based optimization algorithms perform well in practice. We start the optimization from the initial guess $\\alpha = [0, 0, \\dots, 0]^T$, which corresponds to $d_i=1$ for all $i$, meaning $D=I$. The initial value of the objective function is thus $\\sigma_{\\max}(M)$.\n\nThe numerical procedure is as follows:\n1.  For a given matrix $M$ and a list of block sizes $\\{n_1, \\dots, n_b\\}$, define an objective function that takes a vector $\\alpha \\in \\mathbb{R}^b$ as input.\n2.  Inside this function, construct the diagonal matrix $D$ from $\\alpha$ by setting the diagonal entries corresponding to block $i$ to $e^{\\alpha_i}$. Construct its inverse $D^{-1}$ with diagonal entries $e^{-\\alpha_i}$.\n3.  Compute the scaled matrix $M' = D M D^{-1}$.\n4.  Calculate the spectral norm of $M'$, i.e., its largest singular value, using a standard singular value decomposition (SVD) algorithm. This value is the function output.\n5.  Invoke `scipy.optimize.minimize` with this objective function, an initial guess of $\\alpha_0 = 0 \\in \\mathbb{R}^b$, and the L-BFGS-B method.\n6.  The minimum value returned by the optimizer is the computed upper bound on $\\mu_{\\Delta}(M)$. This procedure is repeated for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes the D-scaling upper bound for the structured singular value (mu)\n    for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"M\": np.array([\n                [0.2 + 0.1j, 2.0 - 0.3j],\n                [0.0 + 0.5j, 0.1 - 0.2j]\n            ]),\n            \"block_sizes\": [1, 1]\n        },\n        {\n            \"M\": np.array([\n                [0.05 + 0.0j, 1.0 - 0.1j, 0.2 - 0.1j],\n                [0.0 + 0.1j, 0.3 + 0.0j, 2.0 + 0.2j],\n                [0.0 + 0.0j, 0.5 - 0.1j, 0.0 + 0.1j]\n            ]),\n            \"block_sizes\": [1, 2]\n        },\n        {\n            \"M\": np.block([\n                [np.array([[0.8 + 0.05j, 0.02 - 0.01j],\n                           [0.01 + 0.0j, 0.7 + 0.02j]]), np.zeros((2, 2), dtype=np.complex128)],\n                [np.zeros((2, 2), dtype=np.complex128), np.array([[0.6 + 0.04j, 0.03 + 0.0j],\n                                                                 [0.0 + 0.01j, 0.65 + 0.03j]])]\n            ]),\n            \"block_sizes\": [2, 2]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        M = case[\"M\"]\n        block_sizes = case[\"block_sizes\"]\n        num_blocks = len(block_sizes)\n\n        # Create a mapping from row/column index to block index\n        block_indices = []\n        for i, size in enumerate(block_sizes):\n            block_indices.extend([i] * size)\n\n        def objective_function(alpha):\n            \"\"\"\n            Objective function to minimize: sigma_max(D * M * D^-1).\n            alpha is a vector of logarithms of the scaling factors.\n            \"\"\"\n            # Construct the scaling matrix D and its inverse D_inv\n            # d_i = exp(alpha_i)\n            d_vals = np.exp(alpha)\n            \n            # The problem can be parameterized by b-1 variables, but using b variables\n            # is general and the optimizer handles the redundant degree of freedom.\n            d_vec = np.array([d_vals[block_indices[i]] for i in range(M.shape[0])])\n            \n            # Efficiently compute D * M * D^-1 for diagonal D\n            # (DMD^-1)_ij = d_i/d_j * M_ij\n            D_M_D_inv = M * (d_vec[:, np.newaxis] / d_vec)\n\n            # Compute the largest singular value (spectral norm)\n            # We don't need the singular vectors, so compute_uv=False for efficiency\n            s = np.linalg.svd(D_M_D_inv, compute_uv=False)\n            \n            return s[0]\n\n        # Initial guess for alpha is all zeros, which corresponds to D = I\n        alpha0 = np.zeros(num_blocks)\n\n        # Perform the optimization\n        # L-BFGS-B is a robust and efficient quasi-Newton method\n        # for unconstrained or box-constrained problems.\n        opt_result = minimize(\n            objective_function,\n            alpha0,\n            method='L-BFGS-B',\n            options={'disp': False}\n        )\n\n        # The minimum value found is the upper bound\n        mu_upper_bound = opt_result.fun\n        results.append(round(mu_upper_bound, 6))\n\n    # Format the final output according to the problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2901532"}]}