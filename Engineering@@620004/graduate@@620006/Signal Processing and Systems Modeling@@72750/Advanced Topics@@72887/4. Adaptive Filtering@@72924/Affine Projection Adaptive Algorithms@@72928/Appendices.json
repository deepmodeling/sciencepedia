{"hands_on_practices": [{"introduction": "A deep understanding of any algorithm begins with its mathematical foundations. This first exercise guides you through deriving the Affine Projection Algorithm (APA) update directly from its underlying principle: finding the smallest weight vector change that satisfies a set of recent data constraints. By working through the constrained optimization problem and applying the result to a concrete numerical example, you will bridge the gap between abstract theory and practical calculation [@problem_id:2850728].", "problem": "Consider a real-valued single-input single-output linear finite impulse response system identification problem with an adaptive filter of length $M$. Let the current weight vector be $\\mathbf{w}(n) \\in \\mathbb{R}^{M}$, and define the regressor vectors $\\mathbf{x}(n), \\mathbf{x}(n-1) \\in \\mathbb{R}^{M}$ with corresponding desired outputs $d(n), d(n-1) \\in \\mathbb{R}$. The Affine Projection Algorithm (APA) of order $P=2$ seeks an update increment $\\Delta \\mathbf{w}(n) \\in \\mathbb{R}^{M}$ that minimally disturbs the current estimate while reconciling the most recent two error equations. The principle of minimal disturbance enforces that the updated estimate $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\,\\Delta \\mathbf{w}(n)$ lies as close as possible to $\\mathbf{w}(n)$ in Euclidean norm, while driving the pair of a posteriori equations at times $n$ and $n-1$ to the corresponding desired values in a regularized sense governed by a nonnegative regularization parameter $\\delta \\ge 0$ and a positive stepsize $\\mu > 0$.\n\n1) Starting from the fundamental optimization principle that the update increment $\\Delta \\mathbf{w}(n)$ should have minimum Euclidean norm subject to the affine constraints implied by the two most recent data equations, formulate a regularized constrained optimization problem whose solution produces the $P=2$ Affine Projection Algorithm (APA) update. Introduce Lagrange multipliers to enforce the two affine constraints and include a Tikhonov regularization term with parameter $\\delta$ on the multipliers to guarantee numerical stability. Derive, from first principles, a closed-form expression for $\\Delta \\mathbf{w}(n)$ expressed in terms of the $M \\times 2$ data matrix $\\mathbf{X}(n) \\triangleq [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,]$, the two-sample a priori error vector $\\mathbf{e}(n) \\triangleq \\begin{bmatrix} d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) \\\\ d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) \\end{bmatrix}$, and the regularization parameter $\\delta$. Show explicitly how the update increment is a linear combination of the two regressors, thereby combining the information in the two error equations.\n\n2) Use your derived expression to compute the explicit numerical value of the updated weight vector $\\mathbf{w}(n+1)$ for the following data: let $M=3$, $\\mathbf{w}(n) = \\mathbf{0}_{3\\times 1}$, $\\mathbf{x}(n) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $\\mathbf{x}(n-1) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $d(n) = 2$, $d(n-1)=1$, stepsize $\\mu = 1$, and regularization parameter $\\delta = 1$. Express your final answer for $\\mathbf{w}(n+1)$ as a single row vector.\n\nYour final answer must be the single updated weight vector as a row matrix. No rounding is required, and no units are involved.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard problem in adaptive signal processing that requires the derivation and application of the Affine Projection Algorithm (APA). The provided parameters are consistent and sufficient for a unique solution. Therefore, the problem is valid, and I will proceed with its solution.\n\nThe problem is divided into two parts. The first part requires the derivation of the update increment $\\Delta \\mathbf{w}(n)$ for the order $P=2$ Affine Projection Algorithm. The second part requires the numerical computation of the updated weight vector $\\mathbf{w}(n+1)$ for a specific set of data.\n\nPart 1: Derivation of the APA Update Increment\n\nThe core principle of the APA is to find an updated weight vector $\\mathbf{w}(n+1)$ that is minimally different from the current vector $\\mathbf{w}(n)$, while satisfying a set of constraints. The update is given by $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)$, where $\\mu > 0$ is a stepsize and $\\Delta\\mathbf{w}(n)$ is the update increment. Minimizing the disturbance means minimizing the Euclidean norm of the change in the weight vector, $\\|\\mathbf{w}(n+1) - \\mathbf{w}(n)\\|^2 = \\|\\mu \\Delta\\mathbf{w}(n)\\|^2$. Since $\\mu$ is a positive constant, this is equivalent to minimizing $\\|\\Delta\\mathbf{w}(n)\\|^2$.\n\nThe constraints are that the updated filter should satisfy the two most recent desired output relations. That is, for times $n$ and $n-1$:\n$$ \\mathbf{x}^{\\top}(n)\\mathbf{w}(n+1) = d(n) $$\n$$ \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n+1) = d(n-1) $$\nSubstituting $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)$, we can express the constraints in terms of the update increment $\\Delta\\mathbf{w}(n)$:\n$$ \\mathbf{x}^{\\top}(n)(\\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)) = d(n) \\implies \\mu \\mathbf{x}^{\\top}(n) \\Delta\\mathbf{w}(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) $$\n$$ \\mathbf{x}^{\\top}(n-1)(\\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)) = d(n-1) \\implies \\mu \\mathbf{x}^{\\top}(n-1) \\Delta\\mathbf{w}(n) = d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) $$\nUsing the provided definitions for the data matrix $\\mathbf{X}(n) \\triangleq [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,]$ and the a priori error vector $\\mathbf{e}(n) \\triangleq \\begin{bmatrix} d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) \\\\ d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) \\end{bmatrix}$, these two scalar constraints can be written compactly as a single vector equation:\n$$ \\mu \\mathbf{X}(n)^{\\top} \\Delta\\mathbf{w}(n) = \\mathbf{e}(n) $$\nThe optimization problem is thus to find the $\\Delta\\mathbf{w}(n)$ that minimizes the cost function $J(\\Delta\\mathbf{w}(n)) = \\frac{1}{2}\\|\\Delta\\mathbf{w}(n)\\|^2$ subject to the linear constraint $\\mu \\mathbf{X}(n)^{\\top} \\Delta\\mathbf{w}(n) = \\mathbf{e}(n)$. The factor of $\\frac{1}{2}$ is included for algebraic convenience.\n\nWe solve this constrained optimization problem using the method of Lagrange multipliers. We introduce a vector of Lagrange multipliers $\\boldsymbol{\\lambda} \\in \\mathbb{R}^2$ and form the Lagrangian:\n$$ L(\\Delta\\mathbf{w}(n), \\boldsymbol{\\lambda}) = \\frac{1}{2} \\Delta\\mathbf{w}(n)^{\\top}\\Delta\\mathbf{w}(n) + \\boldsymbol{\\lambda}^{\\top}(\\mathbf{e}(n) - \\mu \\mathbf{X}(n)^{\\top}\\Delta\\mathbf{w}(n)) $$\nTo find the minimum, we first find the stationary point of $L$ with respect to $\\Delta\\mathbf{w}(n)$ by setting the gradient to zero:\n$$ \\nabla_{\\Delta\\mathbf{w}(n)} L = \\Delta\\mathbf{w}(n) - \\mu \\mathbf{X}(n)\\boldsymbol{\\lambda} = \\mathbf{0} $$\nThis gives the structure of the optimal update increment:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n)\\boldsymbol{\\lambda} $$\nThis equation shows that the update increment $\\Delta\\mathbf{w}(n)$ is a linear combination of the regressor vectors $\\mathbf{x}(n)$ and $\\mathbf{x}(n-1)$, the columns of $\\mathbf{X}(n)$.\n\nThe problem states that regularization should be introduced on the multipliers. This is accomplished by regularizing the dual problem. Substituting the expression for $\\Delta\\mathbf{w}(n)$ back into the Lagrangian gives the dual objective function $L_D(\\boldsymbol{\\lambda})$:\n$$ L_D(\\boldsymbol{\\lambda}) = \\frac{1}{2}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda})^{\\top}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda}) + \\boldsymbol{\\lambda}^{\\top}(\\mathbf{e}(n) - \\mu \\mathbf{X}(n)^{\\top}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda})) $$\n$$ L_D(\\boldsymbol{\\lambda}) = \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} + \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\mu^2 \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} $$\n$$ L_D(\\boldsymbol{\\lambda}) = \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} $$\nMaximizing this dual function yields the Lagrange multipliers. To incorporate Tikhonov regularization on the multipliers as specified, we subtract a quadratic penalty term $\\frac{\\delta}{2}\\|\\boldsymbol{\\lambda}\\|^2$ from the dual objective, where $\\delta \\ge 0$ is the regularization parameter. The regularized dual objective is:\n$$ L_{D,reg}(\\boldsymbol{\\lambda}) = \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} - \\frac{\\delta}{2} \\boldsymbol{\\lambda}^{\\top}\\boldsymbol{\\lambda} $$\nTo maximize $L_{D,reg}(\\boldsymbol{\\lambda})$, we set its gradient with respect to $\\boldsymbol{\\lambda}$ to zero:\n$$ \\nabla_{\\boldsymbol{\\lambda}} L_{D,reg} = \\mathbf{e}(n) - \\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} - \\delta \\boldsymbol{\\lambda} = \\mathbf{0} $$\nRearranging to solve for $\\boldsymbol{\\lambda}$:\n$$ (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I}) \\boldsymbol{\\lambda} = \\mathbf{e}(n) $$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix. For $\\delta > 0$, the matrix $(\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})$ is guaranteed to be positive definite and thus invertible. The Lagrange multipliers are:\n$$ \\boldsymbol{\\lambda} = (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\nFinally, we substitute this expression for $\\boldsymbol{\\lambda}$ back into the equation for $\\Delta\\mathbf{w}(n)$:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n) (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\nThis is the closed-form expression for the update increment, showing it is a linear combination of the columns of $\\mathbf{X}(n)$.\n\nPart 2: Numerical Calculation\n\nWe are given the following data:\n- Filter length $M=3$.\n- Initial weights $\\mathbf{w}(n) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Regressor vectors $\\mathbf{x}(n) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{x}(n-1) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Desired outputs $d(n) = 2$ and $d(n-1)=1$.\n- Stepsize $\\mu = 1$.\n- Regularization parameter $\\delta = 1$.\n\nFirst, we construct the data matrix $\\mathbf{X}(n)$ and the a priori error vector $\\mathbf{e}(n)$.\n$$ \\mathbf{X}(n) = [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,] = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\nThe components of the a priori error vector are:\n$$ e(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) = 2 - \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = 2 $$\n$$ e(n-1) = d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) = 1 - \\begin{bmatrix} 0 & 1 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = 1 $$\nSo, the error vector is $\\mathbf{e}(n) = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n\nNext, we compute the matrix product $\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)$:\n$$ \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(0)+(1)(1) & (1)(0)+(0)(1)+(1)(1) \\\\ (0)(1)+(1)(0)+(1)(1) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\nNow we form the matrix to be inverted, using $\\mu=1$ and $\\delta=1$:\n$$ \\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I} = (1)^2 \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} + (1) \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} $$\nThe inverse of this $2 \\times 2$ matrix is:\n$$ \\left(\\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\right)^{-1} = \\frac{1}{(3)(3)-(1)(1)} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} $$\nNow we can calculate the update increment $\\Delta\\mathbf{w}(n)$:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n) (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\n$$ \\Delta\\mathbf{w}(n) = (1) \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\left( \\frac{1}{8} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} \\right) \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$\n$$ \\Delta\\mathbf{w}(n) = \\frac{1}{8} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} (3)(2)+(-1)(1) \\\\ (-1)(2)+(3)(1) \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix} $$\n$$ \\Delta\\mathbf{w}(n) = \\frac{1}{8} \\begin{bmatrix} (1)(5)+(0)(1) \\\\ (0)(5)+(1)(1) \\\\ (1)(5)+(1)(1) \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5 \\\\ 1 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 6/8 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} $$\nFinally, we compute the updated weight vector $\\mathbf{w}(n+1)$:\n$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + (1) \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} $$\nThe final answer is requested as a single row vector.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{5}{8} & \\frac{1}{8} & \\frac{3}{4} \\end{pmatrix} } $$", "id": "2850728"}, {"introduction": "Once we understand how an algorithm works, we must analyze its performance. This practice delves into the steady-state behavior of the APA, specifically its Excess Mean-Square Error (EMSE), under the important special case of a white noise input. This analysis reveals a crucial, and perhaps counter-intuitive, aspect of the algorithm's performance, challenging the assumption that a larger projection order $P$ always leads to better results [@problem_id:2850820].", "problem": "Consider identifying a linear time-invariant system of length $M$ driven by a zero-mean white input. The desired response is modeled by\n$$\nd(n) \\;=\\; \\mathbf{x}^{\\top}(n)\\,\\mathbf{w}_{o} \\;+\\; v(n),\n$$\nwhere $\\mathbf{x}(n)\\in\\mathbb{R}^{M}$ is a zero-mean white input with covariance $\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}=\\sigma_{x}^{2}\\mathbf{I}_{M}$, $\\mathbf{w}_{o}\\in\\mathbb{R}^{M}$ is the unknown parameter vector, and $v(n)$ is a zero-mean white disturbance with variance $\\sigma_{v}^{2}$, independent of $\\mathbf{x}(n)$.\n\nAn Affine Projection Algorithm (APA) of order $P$ (with no diagonal loading) is used to adapt the estimate $\\mathbf{w}(n)$. Define the $M\\times P$ regression matrix and the $P\\times 1$ desired vector\n$$\n\\mathbf{X}(n) \\;=\\; \\big[\\mathbf{x}(n),\\,\\mathbf{x}(n-1),\\,\\ldots,\\,\\mathbf{x}(n-P+1)\\big],\\qquad\n\\mathbf{d}(n) \\;=\\; \\big[d(n),\\,d(n-1),\\,\\ldots,\\,d(n-P+1)\\big]^{\\top},\n$$\nand the $P\\times 1$ a priori error vector\n$$\n\\mathbf{e}(n) \\;=\\; \\mathbf{d}(n) \\;-\\; \\mathbf{X}^{\\top}(n)\\,\\mathbf{w}(n).\n$$\nThe APA update is\n$$\n\\mathbf{w}(n+1) \\;=\\; \\mathbf{w}(n) \\;+\\; \\mu\\,\\mathbf{X}(n)\\big(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\big)^{-1}\\mathbf{e}(n),\n$$\nwhere $\\mu\\in(0,2)$ is a constant step size, and assume $P\\leq M$ so that $\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)$ is invertible almost surely for white inputs. Define the steady-state Excess Mean-Square Error (EMSE) as\n$$\n\\mathrm{EMSE} \\;\\triangleq\\; \\mathbb{E}\\big\\{\\big(\\mathbf{x}^{\\top}(n)\\big(\\mathbf{w}_{o}-\\mathbf{w}(n)\\big)\\big)^{2}\\big\\}.\n$$\n\nStarting from the update definition, the whiteness of $\\mathbf{x}(n)$, and standard independence assumptions used in adaptive filter analysis at steady state, derive a closed-form expression for the steady-state EMSE as a function of $\\mu$ and $\\sigma_{v}^{2}$ for arbitrary projection order $P$. Then, specialize your expression to $P\\in\\{1,2,4\\}$ and report the three EMSE values in that order.\n\nYou must:\n- Start from the given definitions and the white-input model; do not invoke target results without derivation.\n- Justify any approximations you use, including the use of expectations for random matrices.\n- Clearly state whether the steady-state EMSE depends on $P$ under the stated assumptions, and explain why.\n\nExpress your final answer as three closed-form analytic expressions (corresponding to $P=1,2,4$) in terms of $\\mu$ and $\\sigma_{v}^{2}$ only. No numerical evaluation is required.", "solution": "The problem statement is scrutinized for scientific validity, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- System model: $d(n) = \\mathbf{x}^{\\top}(n)\\mathbf{w}_{o} + v(n)$.\n- Input vector: $\\mathbf{x}(n) \\in \\mathbb{R}^{M}$ is a zero-mean, white vector process.\n- Input covariance: $\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = \\sigma_{x}^{2}\\mathbf{I}_{M}$.\n- Unknown system vector: $\\mathbf{w}_{o} \\in \\mathbb{R}^{M}$.\n- Disturbance: $v(n)$ is a zero-mean, white noise process with variance $\\sigma_{v}^{2}$, independent of $\\mathbf{x}(n)$.\n- APA projection order: $P$.\n- Regression matrix: $\\mathbf{X}(n) = [\\mathbf{x}(n), \\mathbf{x}(n-1), \\ldots, \\mathbf{x}(n-P+1)]$.\n- Desired response vector: $\\mathbf{d}(n) = [d(n), d(n-1), \\ldots, d(n-P+1)]^{\\top}$.\n- A priori error vector: $\\mathbf{e}(n) = \\mathbf{d}(n) - \\mathbf{X}^{\\top}(n)\\mathbf{w}(n)$.\n- APA update equation: $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu\\,\\mathbf{X}(n)(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n))^{-1}\\mathbf{e}(n)$.\n- Step size: $\\mu \\in (0,2)$.\n- Invertibility condition: $P \\le M$, and $\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)$ is invertible.\n- EMSE definition: $\\mathrm{EMSE} \\triangleq \\mathbb{E}\\{(\\mathbf{x}^{\\top}(n)(\\mathbf{w}_{o}-\\mathbf{w}(n)))^{2}\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in the analysis of adaptive filters, specifically the Affine Projection Algorithm (APA). All definitions and models are from established theory in signal processing.\n- **Well-Posed:** The problem provides all necessary definitions and statistical properties to derive a specific quantity (EMSE). It asks for a unique analytical expression.\n- **Objective:** The problem is stated in precise mathematical language, free from subjective or ambiguous terms.\n- **Conclusion:** The problem statement is valid. It is self-contained, scientifically sound, and well-posed. A rigorous derivation can proceed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\nWe begin by defining the weight error vector as $\\tilde{\\mathbf{w}}(n) \\triangleq \\mathbf{w}(n) - \\mathbf{w}_{o}$. Subtracting $\\mathbf{w}_{o}$ from both sides of the APA update equation yields the update for the weight error vector:\n$$\n\\tilde{\\mathbf{w}}(n+1) = \\tilde{\\mathbf{w}}(n) + \\mu\\,\\mathbf{X}(n)\\big(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\big)^{-1}\\mathbf{e}(n).\n$$\nThe a priori error vector, $\\mathbf{e}(n)$, can be expressed in terms of the weight error vector. The $i$-th element of the desired vector $\\mathbf{d}(n)$ is $d(n-i) = \\mathbf{x}^{\\top}(n-i)\\mathbf{w}_{o} + v(n-i)$. In matrix form, this becomes:\n$$\n\\mathbf{d}(n) = \\mathbf{X}^{\\top}(n)\\mathbf{w}_{o} + \\mathbf{v}(n),\n$$\nwhere $\\mathbf{v}(n) = [v(n), v(n-1), \\ldots, v(n-P+1)]^{\\top}$. Substituting this into the definition of $\\mathbf{e}(n)$:\n$$\n\\mathbf{e}(n) = \\mathbf{X}^{\\top}(n)\\mathbf{w}_{o} + \\mathbf{v}(n) - \\mathbf{X}^{\\top}(n)\\mathbf{w}(n) = \\mathbf{v}(n) - \\mathbf{X}^{\\top}(n)\\tilde{\\mathbf{w}}(n).\n$$\nSubstituting this back into the error vector update gives:\n$$\n\\tilde{\\mathbf{w}}(n+1) = \\tilde{\\mathbf{w}}(n) + \\mu\\,\\mathbf{X}(n)\\big(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\big)^{-1}\\big(\\mathbf{v}(n) - \\mathbf{X}^{\\top}(n)\\tilde{\\mathbf{w}}(n)\\big).\n$$\nRearranging terms, we obtain:\n$$\n\\tilde{\\mathbf{w}}(n+1) = \\left[\\mathbf{I}_{M} - \\mu\\,\\mathbf{X}(n)\\big(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\big)^{-1}\\mathbf{X}^{\\top}(n)\\right]\\tilde{\\mathbf{w}}(n) + \\mu\\,\\mathbf{X}(n)\\big(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\big)^{-1}\\mathbf{v}(n).\n$$\nTo analyze the steady-state behavior, we employ an energy conservation argument. We take the squared Euclidean norm of both sides:\n$$\n\\|\\tilde{\\mathbf{w}}(n+1)\\|^{2} = \\tilde{\\mathbf{w}}^{\\top}(n+1)\\tilde{\\mathbf{w}}(n+1).\n$$\nTaking the expectation at steady state, we have $\\mathbb{E}\\{\\|\\tilde{\\mathbf{w}}(n+1)\\|^{2}\\} = \\mathbb{E}\\{\\|\\tilde{\\mathbf{w}}(n)\\|^{2}\\}$. This implies that the expectation of the change in energy is zero. Taking the expectation of the squared norm of the update equation and canceling these terms leads to a steady-state energy balance equation. A more direct route is to take the squared norm of $\\tilde{\\mathbf{w}}(n+1)$ and then apply the expectation.\nLet us analyze the terms after squaring. For simplicity, we define the matrix $\\mathbf{A}(n) = (\\mathbf{X}^{\\top}(n)\\mathbf{X}(n))^{-1}$ and the projection matrix $\\mathbf{P}_{\\mathbf{X}}(n) = \\mathbf{X}(n)\\mathbf{A}(n)\\mathbf{X}^{\\top}(n)$. The update is $\\tilde{\\mathbf{w}}(n+1) = (\\mathbf{I}_M - \\mu\\mathbf{P}_{\\mathbf{X}}(n))\\tilde{\\mathbf{w}}(n) + \\mu\\mathbf{X}(n)\\mathbf{A}(n)\\mathbf{v}(n)$.\nThe cross-term from squaring is $2\\mu\\tilde{\\mathbf{w}}^{\\top}(n)(\\mathbf{I}_M - \\mu\\mathbf{P}_{\\mathbf{X}}(n))^{\\top}\\mathbf{X}(n)\\mathbf{A}(n)\\mathbf{v}(n)$.\nAt steady state, we invoke the standard independence assumption: the weight error vector $\\tilde{\\mathbf{w}}(n)$ is statistically independent of the input regressor matrix $\\mathbf{X}(n)$ and the noise vector $\\mathbf{v}(n)$. Since $\\mathbf{v}(n)$ is zero-mean, the expectation of this cross-term is zero.\nThus, taking the expectation of the squared norm yields:\n$$\n\\mathbb{E}\\{\\|\\tilde{\\mathbf{w}}(n+1)\\|^{2}\\} = \\mathbb{E}\\{\\|(\\mathbf{I}_{M} - \\mu\\mathbf{P}_{\\mathbf{X}}(n))\\tilde{\\mathbf{w}}(n)\\|^{2}\\} + \\mu^{2}\\mathbb{E}\\{\\|\\mathbf{X}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\|^{2}\\}.\n$$\nAt steady state, $\\mathbb{E}\\{\\|\\tilde{\\mathbf{w}}(n+1)\\|^{2}\\} = \\mathbb{E}\\{\\|\\tilde{\\mathbf{w}}(n)\\|^{2}\\} \\triangleq \\xi$.\n$$\n\\xi = \\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)(\\mathbf{I}_{M} - \\mu\\mathbf{P}_{\\mathbf{X}}(n))^{2}\\tilde{\\mathbf{w}}(n)\\} + \\mu^{2}\\mathbb{E}\\{\\mathbf{v}^{\\top}(n)\\mathbf{A}(n)\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\}.\n$$\nAs $\\mathbf{P}_{\\mathbf{X}}(n)$ is a projection matrix, $\\mathbf{P}_{\\mathbf{X}}^{2}(n)=\\mathbf{P}_{\\mathbf{X}}(n)$. The first term becomes:\n$$\n\\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)(\\mathbf{I}_{M} - (2\\mu-\\mu^2)\\mathbf{P}_{\\mathbf{X}}(n))\\tilde{\\mathbf{w}}(n)\\} = \\xi - (2\\mu-\\mu^2)\\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)\\mathbf{P}_{\\mathbf{X}}(n)\\tilde{\\mathbf{w}}(n)\\}.\n$$\nThe second term simplifies to $\\mu^{2}\\mathbb{E}\\{\\mathbf{v}^{\\top}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\}$. The balance equation is:\n$$\n(2\\mu-\\mu^2)\\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)\\mathbf{P}_{\\mathbf{X}}(n)\\tilde{\\mathbf{w}}(n)\\} = \\mu^{2}\\mathbb{E}\\{\\mathbf{v}^{\\top}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\}.\n$$\nFor $\\mu \\neq 0$, we can divide by $\\mu$:\n$$\n(2-\\mu)\\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)\\mathbf{P}_{\\mathbf{X}}(n)\\tilde{\\mathbf{w}}(n)\\} = \\mu\\mathbb{E}\\{\\mathbf{v}^{\\top}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\}.\n$$\nNow we evaluate the expectations. We use the trace operator and the independence assumption. Let $\\mathbf{C} = \\mathbb{E}\\{\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}^{\\top}(n)\\}$ be the steady-state weight error covariance matrix.\nThe RHS expectation is:\n$$\n\\mathbb{E}\\{\\mathbf{v}^{\\top}(n)\\mathbf{A}(n)\\mathbf{v}(n)\\} = \\mathbb{E}\\{\\mathrm{Tr}(\\mathbf{A}(n)\\mathbf{v}(n)\\mathbf{v}^{\\top}(n))\\} = \\mathrm{Tr}(\\mathbb{E}\\{\\mathbf{A}(n)\\}\\mathbb{E}\\{\\mathbf{v}(n)\\mathbf{v}^{\\top}(n)\\}).\n$$\nSince $v(n)$ is a white noise process, $\\mathbb{E}\\{\\mathbf{v}(n)\\mathbf{v}^{\\top}(n)\\} = \\sigma_{v}^{2}\\mathbf{I}_{P}$.\nFor the white input vector process $\\mathbf{x}(n)$, the vectors $\\mathbf{x}(n-i)$ and $\\mathbf{x}(n-j)$ are uncorrelated for $i \\neq j$. The $(i,j)$-th entry of $\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)$ is $\\mathbf{x}^{\\top}(n-i+1)\\mathbf{x}(n-j+1)$. Its expectation is $\\mathbb{E}\\{\\mathbf{x}^{\\top}(n-i+1)\\mathbf{x}(n-j+1)\\} = \\mathrm{Tr}(\\mathbb{E}\\{\\mathbf{x}(n-j+1)\\mathbf{x}^{\\top}(n-i+1)\\}) = \\delta_{ij}\\mathrm{Tr}(\\sigma_{x}^{2}\\mathbf{I}_{M}) = M\\sigma_{x}^{2}\\delta_{ij}$.\nTherefore, $\\mathbb{E}\\{\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\} = M\\sigma_{x}^{2}\\mathbf{I}_{P}$. We use the common approximation $\\mathbb{E}\\{\\mathbf{A}(n)\\} = \\mathbb{E}\\{(\\mathbf{X}^{\\top}(n)\\mathbf{X}(n))^{-1}\\} \\approx (\\mathbb{E}\\{\\mathbf{X}^{\\top}(n)\\mathbf{X}(n)\\})^{-1} = (M\\sigma_{x}^{2})^{-1}\\mathbf{I}_{P}$. This approximation is justified for small step-sizes $\\mu$.\nThe RHS expectation becomes:\n$$\n\\mathrm{Tr}((M\\sigma_{x}^{2})^{-1}\\mathbf{I}_{P} \\cdot \\sigma_{v}^{2}\\mathbf{I}_{P}) = \\frac{\\sigma_{v}^{2}}{M\\sigma_{x}^{2}}\\mathrm{Tr}(\\mathbf{I}_{P}) = \\frac{P\\sigma_{v}^{2}}{M\\sigma_{x}^{2}}.\n$$\nThe LHS expectation is:\n$$\n\\mathbb{E}\\{\\tilde{\\mathbf{w}}^{\\top}(n)\\mathbf{P}_{\\mathbf{X}}(n)\\tilde{\\mathbf{w}}(n)\\} = \\mathbb{E}\\{\\mathrm{Tr}(\\mathbf{P}_{\\mathbf{X}}(n)\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}^{\\top}(n))\\} = \\mathrm{Tr}(\\mathbb{E}\\{\\mathbf{P}_{\\mathbf{X}}(n)\\}\\mathbf{C}).\n$$\nFor a white input, the statistics are isotropic. The expected projection matrix $\\mathbb{E}\\{\\mathbf{P}_{\\mathbf{X}}(n)\\}$ must be proportional to the identity matrix, i.e., $\\mathbb{E}\\{\\mathbf{P}_{\\mathbf{X}}(n)\\} = c\\mathbf{I}_{M}$. The constant $c$ is found from the trace: $\\mathrm{Tr}(\\mathbb{E}\\{\\mathbf{P}_{\\mathbf{X}}(n)\\}) = \\mathbb{E}\\{\\mathrm{Tr}(\\mathbf{P}_{\\mathbf{X}}(n))\\} = \\mathbb{E}\\{\\mathrm{Tr}(\\mathbf{I}_{P})\\} = P$. So, $cM = P$, which means $c = P/M$.\nTherefore, $\\mathbb{E}\\{\\mathbf{P}_{\\mathbf{X}}(n)\\} = \\frac{P}{M}\\mathbf{I}_{M}$.\nLHS expectation is $\\mathrm{Tr}(\\frac{P}{M}\\mathbf{I}_{M}\\mathbf{C}) = \\frac{P}{M}\\mathrm{Tr}(\\mathbf{C})$.\nSubstituting these results back into the energy balance equation:\n$$\n(2-\\mu)\\frac{P}{M}\\mathrm{Tr}(\\mathbf{C}) = \\mu\\frac{P\\sigma_{v}^{2}}{M\\sigma_{x}^{2}}.\n$$\nThe terms $P$ and $M$ cancel from both sides, which is a critical observation.\n$$\n(2-\\mu)\\mathrm{Tr}(\\mathbf{C}) = \\frac{\\mu\\sigma_{v}^{2}}{\\sigma_{x}^{2}}.\n$$\nSolving for $\\mathrm{Tr}(\\mathbf{C})$:\n$$\n\\mathrm{Tr}(\\mathbf{C}) = \\frac{\\mu}{(2-\\mu)}\\frac{\\sigma_{v}^{2}}{\\sigma_{x}^{2}}.\n$$\nThe EMSE is defined as $\\mathrm{EMSE} = \\mathbb{E}\\{(\\mathbf{x}^{\\top}(n)\\tilde{\\mathbf{w}}(n))^{2}\\} = \\mathbb{E}\\{\\mathbf{x}^{\\top}(n)\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}^{\\top}(n)\\mathbf{x}(n)\\}$. Using the independence of $\\mathbf{x}(n)$ and $\\tilde{\\mathbf{w}}(n)$ at steady state:\n$$\n\\mathrm{EMSE} = \\mathbb{E}\\{\\mathrm{Tr}(\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}^{\\top}(n))\\} = \\mathrm{Tr}(\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}\\mathbb{E}\\{\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}^{\\top}(n)\\}).\n$$\n$$\n\\mathrm{EMSE} = \\mathrm{Tr}(\\sigma_{x}^{2}\\mathbf{I}_{M}\\mathbf{C}) = \\sigma_{x}^{2}\\mathrm{Tr}(\\mathbf{C}).\n$$\nSubstituting the expression for $\\mathrm{Tr}(\\mathbf{C})$:\n$$\n\\mathrm{EMSE} = \\sigma_{x}^{2} \\left( \\frac{\\mu}{(2-\\mu)}\\frac{\\sigma_{v}^{2}}{\\sigma_{x}^{2}} \\right) = \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2}.\n$$\nThis final expression for the steady-state EMSE is independent of the projection order $P$. This is a specific result for white input signals, arising from the fact that the scaling effect of $P$ on the filter's learning dynamics and its susceptibility to noise cancel each other out under the stated statistical assumptions. For correlated inputs, this cancellation does not occur, and the EMSE does depend on $P$.\n\nThe problem asks for the EMSE values for $P \\in \\{1, 2, 4\\}$. Based on the derived result, the expression is identical for all three cases.\n\nFor $P=1$: $\\mathrm{EMSE} = \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2}$.\nFor $P=2$: $\\mathrm{EMSE} = \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2}$.\nFor $P=4$: $\\mathrm{EMSE} = \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu}{2-\\mu}\\sigma_{v}^{2} & \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2} & \\frac{\\mu}{2-\\mu}\\sigma_{v}^{2}\n\\end{pmatrix}\n}\n$$", "id": "2850820"}, {"introduction": "Theory must eventually meet the reality of implementation on finite-precision machines, where numerical stability is paramount. This final practice explores how to build a robust APA implementation using QR factorization, a cornerstone technique in numerical linear algebra that avoids the potential instability of forming the Gram matrix directly. You will analyze and select a method that ensures both accuracy and efficiency, a critical skill for any practicing engineer or researcher [@problem_id:2850737].", "problem": "Consider an Affine Projection Algorithm (APA) adaptive filter of length $M$ and projection order $P$ that updates a weight vector $w(n) \\in \\mathbb{R}^{M}$ using the most recent $P$ input regressors. At each iteration, the direct form of the update requires solving a symmetric positive definite linear system of dimension $P$ whose coefficient matrix is constructed from inner products of the most recent input regressors together with a small diagonal loading for regularization, and whose right-hand side is the vector of $P$ a priori errors. You are asked to choose an implementation that avoids computing any explicit matrix inverse and that leverages an orthogonal-triangular (QR) factorization to enhance numerical robustness.\n\nStarting only from the following fundamentals:\n- The least-squares normal equations for a tall matrix $A \\in \\mathbb{R}^{m \\times n}$, $m \\ge n$, are $A^{\\top} A x = A^{\\top} b$, and forming $A^{\\top} A$ squares the condition number of $A$.\n- A thin QR factorization of a tall matrix $B \\in \\mathbb{R}^{m \\times n}$, $m \\ge n$, writes $B = Q R$ with $Q \\in \\mathbb{R}^{m \\times n}$ having orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ upper-triangular, and satisfies $B^{\\top} B = R^{\\top} R$.\n- Solving a triangular system of size $P$ by back-substitution costs on the order of $\\mathcal{O}(P^{2})$ operations, while a Householder QR of an $m \\times n$ matrix with $m \\ge n$ costs approximately $2 m n^{2} - \\tfrac{2}{3} n^{3}$ floating-point operations.\n\nWhich option best describes a QR-based per-iteration implementation of the APA that avoids explicit matrix inversion, together with its main numerical advantages and computational costs?\n\nA. Build the $M \\times P$ regressor matrix $X(n)$ and the $P \\times 1$ a priori error vector $e(n)$. Form the $(M+P) \\times P$ augmented matrix $B(n) \\triangleq \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix}$ with a small diagonal loading $\\epsilon > 0$. Compute the thin QR factorization $B(n) = Q(n) R(n)$ using orthogonal transformations (e.g., Householder reflectors or Givens rotations), so that $R(n)^{\\top} R(n) = B(n)^{\\top} B(n) = X(n)^{\\top} X(n) + \\epsilon I_{P}$. Solve the two triangular systems $R(n)^{\\top} z(n) = e(n)$ and then $R(n) g(n) = z(n)$, and update $w(n+1) = w(n) + \\mu X(n) g(n)$ with stepsize $\\mu \\in (0,1]$. From-scratch per-iteration cost is approximately $2 (M+P) P^{2} - \\tfrac{2}{3} P^{3}$ for the QR, plus $\\mathcal{O}(P^{2})$ for the triangular solves and $\\mathcal{O}(M P)$ for the projection, i.e., $\\mathcal{O}(M P^{2} + P^{3})$, but with sliding-window QR updates/downdates via Givens rotations the amortized cost can be reduced toward $\\mathcal{O}(M P)$. Advantages: no explicit inverse, no explicit forming of $X(n)^{\\top} X(n)$, improved numerical stability under collinearity, and graceful handling of near-rank deficiency due to regularization; trade-offs: higher constants than naive methods and extra storage for $R(n)$ and update operators.\n\nB. At each iteration, form the $P \\times P$ Gram matrix $G(n) \\triangleq X(n)^{\\top} X(n) + \\epsilon I_{P}$, compute its eigenvalue decomposition $G(n) = U(n) \\Lambda(n) U(n)^{\\top}$, and then set $g(n) = U(n) \\Lambda(n)^{-1} U(n)^{\\top} e(n)$ to avoid an explicit inverse. This is as stable as QR while being cheaper per iteration, because diagonal inversion dominates and costs only $\\mathcal{O}(P)$; the total per-iteration cost is $\\mathcal{O}(P^{2})$ independent of $M$.\n\nC. Form the $P \\times P$ Gram matrix $G(n) \\triangleq X(n)^{\\top} X(n) + \\epsilon I_{P}$ and compute its Cholesky factorization $G(n) = L(n) L(n)^{\\top}$. Compute the explicit inverse $G(n)^{-1}$ by inverting $L(n)$ and then compute $g(n) = G(n)^{-1} e(n)$. This yields per-iteration complexity $\\mathcal{O}(P^{2})$ because triangular inversion is cheap, and it is numerically equivalent to QR without the overhead of orthogonal transformations.\n\nD. Maintain a thin QR factorization $X(n) = \\tilde{Q}(n) \\tilde{R}(n)$ and, at each iteration, update $\\tilde{R}(n)$ by appending the newest column and removing the oldest via Givens rotations. Then solve $\\tilde{R}(n) g(n) = e(n)$ by a single back-substitution and update $w(n+1) = w(n) + \\mu X(n) g(n)$. The regularization is unnecessary if the QR is maintained, and the cost is $\\mathcal{O}(P^{2})$ per iteration independent of $M$ due to the triangular solve.", "solution": "The problem asks for an analysis of a numerically robust, QR-based implementation of the Affine Projection Algorithm (APA) that avoids explicit matrix inversion. I will first validate the problem statement and then, assuming it is valid, derive the correct method from the provided first principles and evaluate each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Algorithm**: Affine Projection Algorithm (APA) adaptive filter.\n- **Filter Length**: $M$.\n- **Projection Order**: $P$.\n- **Weight Vector**: $w(n) \\in \\mathbb{R}^{M}$.\n- **Update Rule**: Based on solving a $P$-dimensional symmetric positive definite linear system.\n- **System Matrix**: Constructed from inner products of the $P$ most recent input regressors, plus a small diagonal loading for regularization.\n- **System Right-Hand Side**: The vector of $P$ a priori errors.\n- **Constraints**: Avoid explicit matrix inversion; use QR factorization for robustness.\n- **Given Principle 1**: Normal equations for a tall matrix $A \\in \\mathbb{R}^{m \\times n}$ ($m \\ge n$) are $A^{\\top} A x = A^{\\top} b$. Forming $A^{\\top} A$ squares the condition number of $A$.\n- **Given Principle 2**: Thin QR factorization of a tall matrix $B \\in \\mathbb{R}^{m \\times n}$ ($m \\ge n$) is $B = Q R$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is upper-triangular. This implies $B^{\\top} B = R^{\\top} R$.\n- **Given Principle 3**: Solving a triangular system of size $P$ costs $\\mathcal{O}(P^{2})$ operations.\n- **Given Principle 4**: The cost of a Householder QR factorization for an $m \\times n$ matrix is approximately $2 m n^{2} - \\tfrac{2}{3} n^{3}$ floating-point operations for $m \\ge n$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective. It describes a standard problem in the field of adaptive signal processing: the implementation of the regularized APA. The givens are standard definitions and results from numerical linear algebra. The task is to apply these principles to identify the correct implementation strategy among the choices. There are no contradictions, ambiguities, or factual errors in the problem setup. The context is purely scientific and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed with the solution derivation.\n\n### Derivation of the QR-based APA Implementation\n\nThe APA weight update is given by $w(n+1) = w(n) + \\mu X(n) g(n)$, where $X(n) \\in \\mathbb{R}^{M \\times P}$ is the matrix of the $P$ most recent input regressors. The vector $g(n) \\in \\mathbb{R}^{P}$ is the solution to the regularized normal equations:\n$$ (X(n)^{\\top} X(n) + \\epsilon I_{P}) g(n) = e(n) $$\nwhere $e(n) \\in \\mathbb{R}^{P}$ is the a priori error vector, $\\epsilon > 0$ is the regularization parameter, and $I_{P}$ is the $P \\times P$ identity matrix.\n\nThe primary objective is to solve this system for $g(n)$ without explicitly forming the matrix $X(n)^{\\top} X(n)$, as this operation squares the condition number, leading to potential numerical instability (as stated in Principle 1). We must use a QR-based method.\n\nLet us construct an augmented matrix $B(n)$ such that its Gram matrix $B(n)^{\\top} B(n)$ is precisely the coefficient matrix of our linear system. Consider the $(M+P) \\times P$ matrix:\n$$ B(n) = \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix} $$\nComputing its Gram matrix gives:\n$$ B(n)^{\\top} B(n) = \\begin{bmatrix} X(n)^{\\top} & \\sqrt{\\epsilon} I_{P} \\end{bmatrix} \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix} = X(n)^{\\top} X(n) + (\\sqrt{\\epsilon} I_{P})(\\sqrt{\\epsilon} I_{P}) = X(n)^{\\top} X(n) + \\epsilon I_{P} $$\nThis successfully maps our problem onto the structure of normal equations without requiring the explicit formation of the product. The original system can now be written as:\n$$ B(n)^{\\top} B(n) g(n) = e(n) $$\nTo solve this using QR factorization (Principle 2), we compute the thin QR factorization of $B(n)$:\n$$ B(n) = Q(n) R(n) $$\nwhere $Q(n)$ is an $(M+P) \\times P$ matrix with orthonormal columns ($Q(n)^{\\top} Q(n) = I_{P}$) and $R(n)$ is a $P \\times P$ upper-triangular matrix.\n\nSubstituting this into our system:\n$$ (Q(n) R(n))^{\\top} (Q(n) R(n)) g(n) = e(n) $$\n$$ R(n)^{\\top} Q(n)^{\\top} Q(n) R(n) g(n) = e(n) $$\n$$ R(n)^{\\top} R(n) g(n) = e(n) $$\nThis system can be solved efficiently without any matrix inversions by using a two-step procedure involving triangular solves:\n1. Define an intermediate vector $z(n) = R(n) g(n)$. The system becomes $R(n)^{\\top} z(n) = e(n)$. Since $R(n)^{\\top}$ is lower-triangular, we can solve for $z(n)$ using forward substitution.\n2. With $z(n)$ known, we solve $R(n) g(n) = z(n)$ for $g(n)$. Since $R(n)$ is upper-triangular, we can use back substitution.\n\nThe computational cost of this procedure is dominated by the QR factorization of the $(M+P) \\times P$ matrix $B(n)$. Using the provided formula (Principle 4) with $m = M+P$ and $n = P$, the cost is approximately $2(M+P)P^2 - \\frac{2}{3}P^3$, which is of order $\\mathcal{O}(MP^2 + P^3)$. The two triangular solves cost $\\mathcal{O}(P^2)$ each (Principle 3). The final weight update $w(n+1) = w(n) + \\mu X(n) g(n)$ involves a matrix-vector product costing $\\mathcal{O}(MP)$. Thus, the total per-iteration cost if computed from scratch is $\\mathcal{O}(MP^2 + P^3)$. For a sliding-window regressor matrix $X(n)$, efficient update/downdate procedures for the QR factorization (e.g., using Givens rotations) can reduce the amortized cost to $\\mathcal{O}(MP)$.\n\n### Evaluation of Options\n\n**Option A:**\nThis option describes the exact procedure derived above. It correctly forms the augmented matrix $B(n) = \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix}$, computes its thin QR factorization $B(n) = Q(n) R(n)$, and solves the system $R(n)^{\\top} R(n) g(n) = e(n)$ via the two-step triangular solve: first $R(n)^{\\top} z(n) = e(n)$ and then $R(n) g(n) = z(n)$. The analysis of numerical advantages (avoiding explicit inverse and Gram matrix formation, improved stability) is correct. The computational cost analysis, both for the from-scratch version ($ \\mathcal{O}(M P^{2} + P^{3})$) and the fast-update version ($\\mathcal{O}(M P)$), is also correct. The trade-offs are accurately stated.\n**Verdict: Correct**\n\n**Option B:**\nThis option proposes to form the Gram matrix $G(n) = X(n)^{\\top} X(n) + \\epsilon I_{P}$ explicitly. This contradicts the goal of avoiding the numerically problematic squaring of the condition number. It uses eigenvalue decomposition (EVD) instead of the required QR factorization. The cost analysis is incorrect: forming $G(n)$ costs $\\mathcal{O}(MP^2)$, so the total cost is not independent of $M$. EVD of a $P \\times P$ matrix costs $\\mathcal{O}(P^3)$, not $\\mathcal{O}(P^2)$.\n**Verdict: Incorrect**\n\n**Option C:**\nThis option also proposes forming the Gram matrix $G(n)$ explicitly, which is undesirable. It suggests using Cholesky factorization, not QR factorization. Most importantly, it proposes to \"compute the explicit inverse $G(n)^{-1}$\", which directly violates a core constraint of the problem. Further, its claim of being \"numerically equivalent to QR\" is false; QR on the data matrix $B(n)$ is more stable than Cholesky on the Gram matrix $B(n)^{\\top}B(n)$. The complexity is also misstated as $\\mathcal{O}(P^2)$, ignoring the $\\mathcal{O}(MP^2)$ cost of forming $G(n)$.\n**Verdict: Incorrect**\n\n**Option D:**\nThis option correctly suggests maintaining a QR factorization of $X(n)$ and using fast updates. However, it makes a critical error in formulating the system to be solved. It claims the system is $\\tilde{R}(n) g(n) = e(n)$, but the correct system, derived from $(X(n)^{\\top} X(n) + \\epsilon I_{P}) g(n) = e(n)$, is $(\\tilde{R}(n)^{\\top} \\tilde{R}(n) + \\epsilon I_{P}) g(n) = e(n)$. Option D completely ignores the regularization term $\\epsilon I_P$ in the solution step and makes the false assertion that \"regularization is unnecessary if the QR is maintained\". Regularization is crucial for handling collinearity in regressors, which QR factorization diagnoses but does not by itself resolve without modifying the problem. The complexity analysis is also flawed; updates cost $\\mathcal{O}(MP)$, not a cost independent of $M$.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "2850737"}]}