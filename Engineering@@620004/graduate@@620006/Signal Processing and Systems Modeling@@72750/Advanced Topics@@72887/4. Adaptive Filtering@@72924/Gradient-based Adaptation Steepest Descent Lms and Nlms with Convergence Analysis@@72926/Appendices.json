{"hands_on_practices": [{"introduction": "The steepest-descent algorithm forms the deterministic foundation upon which stochastic gradient methods like LMS are built. Before delving into stochastic effects, it is crucial to master the mechanics of moving down a quadratic error surface using the true gradient. This practice exercise [@problem_id:2874690] challenges you to derive and compute the optimal step-size using an exact line search, providing a concrete understanding of how the algorithm maximally reduces the cost function in a single, ideal step.", "problem": "Consider three canonical adaptive filtering applications—system identification, noise cancellation, and linear equalization—each modeled by the same quadratic mean-squared-error criterion. In all three cases, one observes a length-$3$ input vector $\\mathbf{x}(n) \\in \\mathbb{R}^{3}$ driving an unknown linear system with coefficient vector $\\mathbf{w}_{\\star} \\in \\mathbb{R}^{3}$ and a measurement noise $v(n)$ that is zero-mean, independent of $\\mathbf{x}(n)$, and of finite variance. The desired signal is $d(n) = \\mathbf{w}_{\\star}^{\\top} \\mathbf{x}(n) + v(n)$. The adaptive filter seeks $\\mathbf{w} \\in \\mathbb{R}^{3}$ that minimizes the mean-squared error cost $J(\\mathbf{w}) \\triangleq \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^2\\}$.\n\nAssume the input $\\mathbf{x}(n)$ is formed by stacking three consecutive samples of a zero-mean, wide-sense stationary, autoregressive process of order $1$ (AR($1$)) with unit variance and correlation coefficient $\\rho = \\tfrac{1}{2}$, so that the autocorrelation function is $r_{x}(k) = \\rho^{|k|}$ for $k \\in \\mathbb{Z}$. The corresponding input covariance matrix is the Toeplitz matrix $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$ with entries $[\\mathbf{R}]_{i,j} = r_{x}(i-j)$ for $i,j \\in \\{1,2,3\\}$. Let the true system be $\\mathbf{w}_{\\star} = [\\,1,\\,-\\tfrac{1}{2},\\,\\tfrac{1}{4}\\,]^{\\top}$. Define the cross-correlation vector $\\mathbf{p} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)\\,d(n)\\}$.\n\nA steepest-descent recursion with exact line search is employed to minimize $J(\\mathbf{w})$:\n$\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\mu_{k}\\,\\nabla J(\\mathbf{w}_{k})$,\nwhere the step-size $\\mu_{k}$ at iteration $k$ is chosen to minimize $J(\\mathbf{w}_{k} - \\mu\\,\\nabla J(\\mathbf{w}_{k}))$ with respect to $\\mu \\in \\mathbb{R}$. Suppose the current iterate is $\\mathbf{w}_{k} = \\mathbf{0}$.\n\nStarting only from the definitions of $J(\\mathbf{w})$, $\\mathbf{R}$, and $\\mathbf{p}$, and standard rules of matrix calculus for quadratic forms, derive the exact line-search step-size $\\mu_{k}$ and evaluate it numerically for the data above. Express your final answer as an exact fraction (no decimal approximation). No rounding is required and no units are involved.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in the field of adaptive signal processing. We shall proceed with a complete solution.\n\nThe mean-squared-error (MSE) cost function is defined as\n$$ J(\\mathbf{w}) \\triangleq \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^2\\} $$\nWe expand this expression to reveal its quadratic structure in terms of the weight vector $\\mathbf{w}$.\n$$ J(\\mathbf{w}) = \\mathbb{E}\\{d(n)^2 - 2d(n)\\mathbf{x}(n)^{\\top}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\mathbf{w}\\} $$\nUsing the linearity of the expectation operator, we can write\n$$ J(\\mathbf{w}) = \\mathbb{E}\\{d(n)^2\\} - 2\\mathbb{E}\\{d(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w} $$\nBy the definitions given in the problem statement, we identify the terms:\n- $\\sigma_d^2 \\triangleq \\mathbb{E}\\{d(n)^2\\}$ is the variance of the desired signal $d(n)$.\n- $\\mathbf{p} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$ is the cross-correlation vector between the input $\\mathbf{x}(n)$ and the desired signal $d(n)$. So, $\\mathbb{E}\\{d(n)\\mathbf{x}(n)^{\\top}\\} = \\mathbf{p}^{\\top}$.\n- $\\mathbf{R} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}$ is the autocorrelation matrix of the input vector $\\mathbf{x}(n)$.\n\nSubstituting these definitions, the cost function becomes a quadratic form:\n$$ J(\\mathbf{w}) = \\sigma_d^2 - 2\\mathbf{p}^{\\top}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w} $$\nThe gradient of this cost function with respect to $\\mathbf{w}$ is required for the steepest-descent algorithm. Using standard rules of vector calculus for a symmetric matrix $\\mathbf{R}$ (which it is, being an autocorrelation matrix), we have:\n$$ \\nabla J(\\mathbf{w}) = \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = -2\\mathbf{p} + 2\\mathbf{R}\\mathbf{w} $$\nThe steepest-descent recursion updates the weight vector $\\mathbf{w}_k$ at iteration $k$ as follows:\n$$ \\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\mu_{k}\\,\\nabla J(\\mathbf{w}_{k}) $$\nHere, the step-size $\\mu_k$ is chosen via exact line search, meaning it minimizes the cost function along the direction of the negative gradient. Let $\\mathbf{g}_k \\triangleq \\nabla J(\\mathbf{w}_k)$. We seek to find $\\mu_k$ that minimizes the function $f(\\mu) = J(\\mathbf{w}_k - \\mu \\mathbf{g}_k)$.\n$$ f(\\mu) = J(\\mathbf{w}_k - \\mu \\mathbf{g}_k) = \\sigma_d^2 - 2\\mathbf{p}^{\\top}(\\mathbf{w}_k - \\mu \\mathbf{g}_k) + (\\mathbf{w}_k - \\mu \\mathbf{g}_k)^{\\top}\\mathbf{R}(\\mathbf{w}_k - \\mu \\mathbf{g}_k) $$\nTo find the minimum, we set the derivative of $f(\\mu)$ with respect to $\\mu$ to zero.\n$$ \\frac{df(\\mu)}{d\\mu} = 2\\mathbf{p}^{\\top}\\mathbf{g}_k - 2\\mathbf{w}_k^{\\top}\\mathbf{R}\\mathbf{g}_k + 2\\mu \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k = 0 $$\nNote that $\\mathbf{w}_k^{\\top}\\mathbf{R}\\mathbf{g}_k = \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{w}_k$ since the result is a scalar.\nSolving for $\\mu$, we get:\n$$ \\mu (\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k) = \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{w}_k - \\mathbf{g}_k^{\\top}\\mathbf{p} = \\mathbf{g}_k^{\\top}(\\mathbf{R}\\mathbf{w}_k - \\mathbf{p}) $$\nWe know that $\\mathbf{g}_k = 2\\mathbf{R}\\mathbf{w}_k - 2\\mathbf{p}$, which implies $\\mathbf{R}\\mathbf{w}_k - \\mathbf{p} = \\frac{1}{2}\\mathbf{g}_k$.\nSubstituting this into the expression for $\\mu$:\n$$ \\mu (\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k) = \\mathbf{g}_k^{\\top}\\left(\\frac{1}{2}\\mathbf{g}_k\\right) = \\frac{1}{2}\\mathbf{g}_k^{\\top}\\mathbf{g}_k $$\nThus, the optimal step-size for exact line search is:\n$$ \\mu_k = \\frac{\\mathbf{g}_k^{\\top}\\mathbf{g}_k}{2\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k} $$\nThe problem states that the current iterate is $\\mathbf{w}_k = \\mathbf{0}$. We evaluate the gradient at this point:\n$$ \\mathbf{g}_k = \\nabla J(\\mathbf{0}) = -2\\mathbf{p} + 2\\mathbf{R}(\\mathbf{0}) = -2\\mathbf{p} $$\nSubstituting this specific gradient into the formula for $\\mu_k$:\n$$ \\mu_k = \\frac{(-2\\mathbf{p})^{\\top}(-2\\mathbf{p})}{2(-2\\mathbf{p})^{\\top}\\mathbf{R}(-2\\mathbf{p})} = \\frac{4\\mathbf{p}^{\\top}\\mathbf{p}}{2 \\cdot 4 \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} = \\frac{\\mathbf{p}^{\\top}\\mathbf{p}}{2\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} $$\nNow, we must compute the numerical values for $\\mathbf{R}$ and $\\mathbf{p}$.\nThe input autocorrelation matrix $\\mathbf{R}$ is a $3 \\times 3$ Toeplitz matrix with entries $[\\mathbf{R}]_{i,j} = r_x(i-j) = \\rho^{|i-j|}$, where $\\rho = \\frac{1}{2}$.\n$$ \\mathbf{R} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} $$\nThe cross-correlation vector $\\mathbf{p}$ is given by $\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$. Substituting $d(n) = \\mathbf{w}_{\\star}^{\\top}\\mathbf{x}(n) + v(n)$:\n$$ \\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)(\\mathbf{x}(n)^{\\top}\\mathbf{w}_{\\star} + v(n))\\} = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w}_{\\star} + \\mathbb{E}\\{\\mathbf{x}(n)v(n)\\} $$\nSince $\\mathbf{x}(n)$ and $v(n)$ are independent and $v(n)$ is zero-mean, $\\mathbb{E}\\{\\mathbf{x}(n)v(n)\\} = \\mathbb{E}\\{\\mathbf{x}(n)\\}\\mathbb{E}\\{v(n)\\} = \\mathbf{0}$. Therefore:\n$$ \\mathbf{p} = \\mathbf{R}\\mathbf{w}_{\\star} $$\nGiven $\\mathbf{w}_{\\star} = [\\,1,\\,-\\tfrac{1}{2},\\,\\tfrac{1}{4}\\,]^{\\top}$:\n$$ \\mathbf{p} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1(1) + \\frac{1}{2}(-\\frac{1}{2}) + \\frac{1}{4}(\\frac{1}{4}) \\\\ \\frac{1}{2}(1) + 1(-\\frac{1}{2}) + \\frac{1}{2}(\\frac{1}{4}) \\\\ \\frac{1}{4}(1) + \\frac{1}{2}(-\\frac{1}{2}) + 1(\\frac{1}{4}) \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{4} + \\frac{1}{16} \\\\ \\frac{1}{2} - \\frac{1}{2} + \\frac{1}{8} \\\\ \\frac{1}{4} - \\frac{1}{4} + \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{16} \\\\ \\frac{1}{8} \\\\ \\frac{1}{4} \\end{pmatrix} $$\nNext, we compute the quadratic forms $\\mathbf{p}^{\\top}\\mathbf{p}$ and $\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}$.\n$$ \\mathbf{p}^{\\top}\\mathbf{p} = \\left(\\frac{13}{16}\\right)^2 + \\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 = \\left(\\frac{13}{16}\\right)^2 + \\left(\\frac{2}{16}\\right)^2 + \\left(\\frac{4}{16}\\right)^2 = \\frac{13^2 + 2^2 + 4^2}{16^2} = \\frac{169 + 4 + 16}{256} = \\frac{189}{256} $$\nTo compute $\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}$, we first compute the vector $\\mathbf{z} = \\mathbf{R}\\mathbf{p}$:\n$$ \\mathbf{z} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{13}{16} \\\\ \\frac{2}{16} \\\\ \\frac{4}{16} \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 13 \\\\ 2 \\\\ 4 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 13 + 1 + 1 \\\\ \\frac{13}{2} + 2 + 2 \\\\ \\frac{13}{4} + 1 + 4 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 15 \\\\ \\frac{21}{2} \\\\ \\frac{33}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{15}{16} \\\\ \\frac{21}{32} \\\\ \\frac{33}{64} \\end{pmatrix} $$\nNow, we compute the dot product $\\mathbf{p}^{\\top}\\mathbf{z}$:\n$$ \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p} = \\mathbf{p}^{\\top}\\mathbf{z} = \\begin{pmatrix} \\frac{13}{16} & \\frac{2}{16} & \\frac{4}{16} \\end{pmatrix} \\begin{pmatrix} \\frac{15}{16} \\\\ \\frac{21}{32} \\\\ \\frac{33}{64} \\end{pmatrix} = \\frac{13}{16}\\frac{15}{16} + \\frac{2}{16}\\frac{21}{32} + \\frac{4}{16}\\frac{33}{64} $$\n$$ = \\frac{195}{256} + \\frac{42}{512} + \\frac{132}{1024} = \\frac{195 \\cdot 4}{1024} + \\frac{42 \\cdot 2}{1024} + \\frac{132}{1024} = \\frac{780 + 84 + 132}{1024} = \\frac{996}{1024} $$\nSimplifying this fraction by dividing the numerator and denominator by $4$:\n$$ \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p} = \\frac{249}{256} $$\nFinally, we substitute these values into the expression for $\\mu_k$:\n$$ \\mu_k = \\frac{\\mathbf{p}^{\\top}\\mathbf{p}}{2\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} = \\frac{\\frac{189}{256}}{2 \\cdot \\frac{249}{256}} = \\frac{189}{2 \\cdot 249} = \\frac{189}{498} $$\nWe simplify the final fraction. The numerator $189 = 3 \\cdot 63 = 3^3 \\cdot 7$. The denominator $498 = 2 \\cdot 249 = 2 \\cdot 3 \\cdot 83$.\n$$ \\mu_k = \\frac{3^3 \\cdot 7}{2 \\cdot 3 \\cdot 83} = \\frac{3^2 \\cdot 7}{2 \\cdot 83} = \\frac{63}{166} $$\nThis fraction is irreducible as $63$ and $166$ share no common prime factors.", "answer": "$$ \\boxed{\\frac{63}{166}} $$", "id": "2874690"}, {"introduction": "While the LMS algorithm's simplicity is one of its greatest strengths, this comes at the cost of using a noisy gradient estimate. A fundamental question in adaptive filter design is how this gradient noise impacts the filter's performance after it has converged. This practice [@problem_id:2874692] guides you through the mean-square analysis to derive and calculate the steady-state misadjustment, a critical performance metric quantifying the excess error attributable to the algorithm's stochastic nature.", "problem": "Consider an unknown linear time-invariant system with an optimal weight vector $w_{\\star} \\in \\mathbb{R}^{3}$. The input $x(n) \\in \\mathbb{R}^{3}$ is a zero-mean, wide-sense stationary Gaussian vector process with covariance matrix $R \\triangleq \\mathbb{E}\\{x(n) x(n)^{\\top}\\}$ given by\n$$\nR \\;=\\; \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}.\n$$\nThe desired response is $d(n) \\triangleq x(n)^{\\top} w_{\\star} + v(n)$, where $v(n)$ is zero-mean measurement noise with variance $\\sigma_{v}^{2}$ that is independent of $x(n)$ for all $n$. An adaptive filter of length $3$ is trained using the Least Mean Squares (LMS) algorithm, that is, $w(n+1) \\triangleq w(n) + \\mu\\, x(n)\\, e(n)$ with $e(n) \\triangleq d(n) - x(n)^{\\top} w(n)$ and a constant step size $\\mu \\triangleq 5.0 \\times 10^{-2}$.\n\nStarting from the weight-error recursion and using only core definitions and well-tested facts (e.g., properties of second-order moments of Gaussian vectors and the independence assumption used in mean-square analysis of LMS), perform a small-step-size mean-square analysis that neglects terms of order higher than first in $\\mu$ when they multiply the weight-error covariance. Under these conditions, derive the steady-state misadjustment, defined as\n$$\n\\mathcal{M} \\;\\triangleq\\; \\frac{\\text{excess mean-square error (EMSE)}}{\\text{minimum mean-square error (MMSE)}} \\;=\\; \\frac{J_{\\infty} - J_{\\min}}{J_{\\min}},\n$$\nwhere $J_{\\infty} \\triangleq \\lim_{n \\to \\infty} \\mathbb{E}\\{e(n)^{2}\\}$ and $J_{\\min} \\triangleq \\sigma_{v}^{2}$. Then, compute the numerical value of $\\mathcal{M}$ for the given data. Round your answer to four significant figures. Express the final result as a unitless decimal number.", "solution": "The problem requires the derivation and computation of the steady-state misadjustment, $\\mathcal{M}$, for a Least Mean Squares (LMS) adaptive filter. The analysis will be performed under the small step-size, $\\mu$, assumption.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Optimal weight vector $w_{\\star} \\in \\mathbb{R}^{3}$.\n- Input vector $x(n) \\in \\mathbb{R}^{3}$ is a zero-mean, wide-sense stationary (WSS) Gaussian process.\n- Input covariance matrix: $R \\triangleq \\mathbb{E}\\{x(n) x(n)^{\\top}\\} = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}$.\n- Desired response: $d(n) \\triangleq x(n)^{\\top} w_{\\star} + v(n)$.\n- Measurement noise $v(n)$ is zero-mean with variance $\\sigma_{v}^{2}$, independent of $x(n)$.\n- LMS algorithm: $w(n+1) \\triangleq w(n) + \\mu\\, x(n)\\, e(n)$.\n- Error signal: $e(n) \\triangleq d(n) - x(n)^{\\top} w(n)$.\n- Step size: $\\mu \\triangleq 5.0 \\times 10^{-2}$.\n- Misadjustment definition: $\\mathcal{M} \\triangleq \\frac{J_{\\infty} - J_{\\min}}{J_{\\min}}$, where $J_{\\infty} \\triangleq \\lim_{n \\to \\infty} \\mathbb{E}\\{e(n)^{2}\\}$ and $J_{\\min} \\triangleq \\sigma_{v}^{2}$.\n\nThe problem is scientifically grounded, well-posed, and objective. All components are standard in the theory of adaptive filtering. The covariance matrix $R$ is symmetric, and its eigenvalues can be calculated as $\\lambda_1 = 2$, $\\lambda_2 = 2+\\sqrt{2}$, and $\\lambda_3 = 2-\\sqrt{2}$. Since all eigenvalues are positive, $R$ is a valid positive-definite covariance matrix. The condition for mean-square convergence of the LMS algorithm is $0 < \\mu < 2/\\lambda_{\\max}$. Here, $\\lambda_{\\max} = 2+\\sqrt{2} \\approx 3.414$, so the upper bound for $\\mu$ is approximately $2/3.414 \\approx 0.586$. The given step size $\\mu=0.05$ satisfies this condition. The problem is valid.\n\nWe proceed with the derivation.\n\nThe weight-error vector is defined as $\\tilde{w}(n) \\triangleq w(n) - w_{\\star}$.\nThe error signal $e(n)$ can be expressed in terms of the weight-error vector:\n$$e(n) = d(n) - x(n)^{\\top} w(n) = (x(n)^{\\top} w_{\\star} + v(n)) - x(n)^{\\top} w(n) = v(n) - x(n)^{\\top} (w(n) - w_{\\star}) = v(n) - x(n)^{\\top} \\tilde{w}(n)$$\nSubstituting this into the LMS update equation, $w(n+1) = w(n) + \\mu x(n)e(n)$, and subtracting $w_{\\star}$ from both sides, we obtain the recursion for the weight-error vector:\n$$\\tilde{w}(n+1) = \\tilde{w}(n) + \\mu x(n) (v(n) - x(n)^{\\top} \\tilde{w}(n))$$\n$$\\tilde{w}(n+1) = [I - \\mu x(n) x(n)^{\\top}] \\tilde{w}(n) + \\mu v(n) x(n)$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\n\nThe mean-square error (MSE) is $J(n) = \\mathbb{E}\\{e(n)^2\\}$.\n$$J(n) = \\mathbb{E}\\{[v(n) - x(n)^{\\top} \\tilde{w}(n)]^2\\} = \\mathbb{E}\\{v(n)^2\\} - 2 \\mathbb{E}\\{v(n) x(n)^{\\top} \\tilde{w}(n)\\} + \\mathbb{E}\\{[x(n)^{\\top} \\tilde{w}(n)]^2\\}$$\nSince $v(n)$ is zero-mean and independent of $x(n)$ and $\\tilde{w}(n)$ (which depends on past inputs and noise), the cross-term $\\mathbb{E}\\{v(n) x(n)^{\\top} \\tilde{w}(n)\\}$ is zero.\nWe are given $\\mathbb{E}\\{v(n)^2\\} = \\sigma_v^2 = J_{\\min}$, the minimum mean-square error. The last term is the excess mean-square error (EMSE), $J_{ex}(n) = \\mathbb{E}\\{[x(n)^{\\top} \\tilde{w}(n)]^2\\}$.\nThus, $J(n) = J_{\\min} + J_{ex}(n)$. In steady state ($n \\to \\infty$), the MSE is $J_\\infty = J_{\\min} + J_{ex}(\\infty)$.\nThe misadjustment is therefore $\\mathcal{M} = \\frac{J_{ex}(\\infty)}{J_{\\min}}$.\n\nTo find $J_{ex}(\\infty)$, we analyze the weight-error covariance matrix $K(n) \\triangleq \\mathbb{E}\\{\\tilde{w}(n) \\tilde{w}(n)^{\\top}\\}$.\nThe EMSE can be expressed using $K(n)$. Using the independence assumption, where $x(n)$ is assumed to be independent of $\\tilde{w}(n)$, and the trace property $\\text{Tr}(A) = A$ for a scalar $A$:\n$$J_{ex}(n) = \\mathbb{E}\\{\\tilde{w}(n)^{\\top} x(n) x(n)^{\\top} \\tilde{w}(n)\\} = \\mathbb{E}\\{\\text{Tr}(\\tilde{w}(n)^{\\top} x(n) x(n)^{\\top} \\tilde{w}(n))\\}$$\n$$J_{ex}(n) = \\mathbb{E}\\{\\text{Tr}(x(n) x(n)^{\\top} \\tilde{w}(n) \\tilde{w}(n)^{\\top})\\} = \\text{Tr}(\\mathbb{E}\\{x(n) x(n)^{\\top}\\} \\mathbb{E}\\{\\tilde{w}(n) \\tilde{w}(n)^{\\top}\\}) = \\text{Tr}(R K(n))$$\nWe need to find the recursion for $K(n+1) = \\mathbb{E}\\{\\tilde{w}(n+1) \\tilde{w}(n+1)^{\\top}\\}$.\n$$K(n+1) = \\mathbb{E}\\{[ (I - \\mu x(n) x(n)^{\\top}) \\tilde{w}(n) + \\mu v(n) x(n) ] [ \\dots ]^{\\top}\\}$$\nExpanding and taking the expectation, the cross-terms involving $v(n)$ are zero. We obtain:\n$$K(n+1) = \\mathbb{E}\\{(I - \\mu x(n) x(n)^{\\top}) \\tilde{w}(n) \\tilde{w}(n)^{\\top} (I - \\mu x(n) x(n)^{\\top})\\} + \\mu^2 \\mathbb{E}\\{v(n)^2\\} \\mathbb{E}\\{x(n) x(n)^{\\top}\\}$$\nThe second term is $\\mu^2 \\sigma_v^2 R$. Applying the independence assumption to the first term:\n$$K(n+1) = \\mathbb{E}\\{K(n) - \\mu K(n) x(n)x(n)^{\\top} - \\mu x(n)x(n)^{\\top} K(n) + \\mu^2 x(n)x(n)^{\\top} K(n) x(n)x(n)^{\\top} \\} + \\mu^2 \\sigma_v^2 R$$\n$$K(n+1) = K(n) - \\mu(K(n)R + R K(n)) + \\mu^2 \\mathbb{E}\\{x(n)x(n)^{\\top} K(n) x(n)x(n)^{\\top}\\} + \\mu^2 \\sigma_v^2 R$$\nFor a small step-size $\\mu$, the weight-error covariance $K(n)$ is driven by the noise term $\\mu^2 \\sigma_v^2 R$, so we expect $K(n)$ to be of order $\\mu$. The term $\\mu^2 \\mathbb{E}\\{x(n)x(n)^{\\top} K(n) x(n)x(n)^{\\top}\\}$ is therefore of order $\\mu^3$. As stipulated by the problem, we neglect terms of order higher than first in $\\mu$ that multiply the weight-error covariance. This means we neglect the $\\mu^2 \\mathbb{E}\\{\\dots\\}$ term relative to the other terms. The simplified recursion is:\n$$K(n+1) \\approx K(n) - \\mu(K(n)R + R K(n)) + \\mu^2 \\sigma_v^2 R$$\nIn steady state, $K(n+1) = K(n) = K_{\\infty}$, so:\n$$0 \\approx -\\mu(K_{\\infty}R + R K_{\\infty}) + \\mu^2 \\sigma_v^2 R$$\nThis gives the Lyapunov equation for the steady-state covariance:\n$$K_{\\infty}R + R K_{\\infty} \\approx \\mu \\sigma_v^2 R$$\nTo find the steady-state EMSE, $J_{ex}(\\infty) = \\text{Tr}(R K_{\\infty})$, we take the trace of both sides of the Lyapunov equation:\n$$\\text{Tr}(K_{\\infty}R + R K_{\\infty}) \\approx \\text{Tr}(\\mu \\sigma_v^2 R)$$\nUsing the linearity of the trace and $\\text{Tr}(AB) = \\text{Tr}(BA)$:\n$$\\text{Tr}(K_{\\infty}R) + \\text{Tr}(R K_{\\infty}) \\approx \\mu \\sigma_v^2 \\text{Tr}(R)$$\n$$2 \\text{Tr}(R K_{\\infty}) \\approx \\mu \\sigma_v^2 \\text{Tr}(R)$$\nSubstituting $J_{ex}(\\infty) = \\text{Tr}(R K_{\\infty})$:\n$$2 J_{ex}(\\infty) \\approx \\mu \\sigma_v^2 \\text{Tr}(R) \\implies J_{ex}(\\infty) \\approx \\frac{\\mu}{2} \\sigma_v^2 \\text{Tr}(R)$$\nFinally, the misadjustment is:\n$$\\mathcal{M} = \\frac{J_{ex}(\\infty)}{J_{\\min}} = \\frac{J_{ex}(\\infty)}{\\sigma_v^2} \\approx \\frac{\\frac{\\mu}{2} \\sigma_v^2 \\text{Tr}(R)}{\\sigma_v^2} = \\frac{\\mu}{2} \\text{Tr}(R)$$\nNow we compute the numerical value. The step size is $\\mu = 5.0 \\times 10^{-2}$. The trace of the covariance matrix $R$ is:\n$$\\text{Tr}(R) = \\text{Tr}\\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} = 2 + 2 + 2 = 6$$\nSubstituting these values into the expression for $\\mathcal{M}$:\n$$\\mathcal{M} \\approx \\frac{5.0 \\times 10^{-2}}{2} \\times 6 = (2.5 \\times 10^{-2}) \\times 6 = 15.0 \\times 10^{-2} = 0.15$$\nThe problem asks for the result to be rounded to four significant figures. Thus, the value is $0.1500$.", "answer": "$$\n\\boxed{0.1500}\n$$", "id": "2874692"}, {"introduction": "Theoretical analysis provides us with crucial stability bounds for the step-size parameters of adaptive algorithms, but seeing these bounds in action provides an invaluable intuition. This hands-on computational practice [@problem_id:2874693] bridges the gap between theory and implementation by tasking you with designing a numerical experiment to test the convergence boundaries of steepest descent, LMS, and NLMS. By simulating the algorithms at step-sizes near the theoretical limits, you will empirically validate the conditions for convergence and witness the onset of divergence.", "problem": "Consider the adaptive estimation of an unknown coefficient vector $w^\\star \\in \\mathbb{R}^m$ for the linear model $d(n) = x^\\top(n) w^\\star + v(n)$, where $x(n) \\in \\mathbb{R}^m$ is a zero-mean, wide-sense stationary input with positive definite autocorrelation matrix $R = \\mathbb{E}\\{x(n) x^\\top(n)\\}$, and $v(n)$ is a zero-mean noise independent of $x(n)$. The mean-square error cost is $J(w) = \\mathbb{E}\\{(d(n) - x^\\top(n) w)^2\\}$. You will analyze step-size bounds for convergence of three algorithms: deterministic steepest descent, Least-Mean-Square (LMS), and Normalized Least-Mean-Square (NLMS), with an emphasis on necessary, sufficient, and tight bounds.\n\nBased on first principles:\n- The gradient of a quadratic cost of the form $J(w) = \\tfrac{1}{2} (w - w^\\star)^\\top R (w - w^\\star) + \\text{const}$ is $\\nabla J(w) = R (w - w^\\star)$.\n- The steepest descent recursion is $w_{n+1} = w_n - \\mu \\nabla J(w_n)$, which induces the linear error recursion $e_{n+1} = (I - \\mu R) e_n$ with $e_n = w_n - w^\\star$.\n- For a linear time-invariant recursion $e_{n+1} = A e_n$, the necessary and sufficient condition for $e_n \\to 0$ is that the spectral radius $\\rho(A)$ satisfies $\\rho(A) < 1$.\n- For LMS, using the independence assumption between $w_n$ and the current regressor $x(n)$, the mean error update is $\\mathbb{E}\\{e_{n+1}\\} = (I - \\mu R) \\mathbb{E}\\{e_n\\}$, while mean-square analysis involves higher-order moments and typically admits conservative sufficient bounds in terms of $R$.\n- For NLMS, the normalized update uses $\\mu_0$ as a dimensionless step-size parameter.\n\nYour tasks:\n1) Deterministic steepest descent convergence bound and tightness. Starting from the spectral condition on $I - \\mu R$, derive the necessary and sufficient interval for $\\mu$ in terms of the largest eigenvalue of $R$. Then design a numerical test to assess tightness by simulating the error recursion near the boundary.\n2) LMS mean convergence bound and a conservative sufficient mean-square stability bound. Starting from the independence assumption and the induced linear mean recursion, derive a necessary and sufficient mean-stability interval for $\\mu$ in terms of the extreme eigenvalue(s) of $R$. Additionally, derive a conservative, easily computable sufficient bound for mean-square stability in terms of $R$ alone. Design a numerical test for the tightness of the mean-stability bound at and around the boundary.\n3) NLMS stability bound and tightness. Starting from the normalized update, derive a stability interval for the dimensionless step-size parameter $\\mu_0$ independent of $R$ (assume a small positive regularization $ \\delta $ in the denominator is used to avoid division by zero). Design a numerical test to assess tightness by simulating NLMS near the boundary.\n\nTest suite specification:\n- Use the following numeric data for the three cases.\n- Case A (Steepest Descent, dimension $m=2$):\n  - $R_A = \\begin{bmatrix} 1.2 & 0.3 \\\\ 0.3 & 0.8 \\end{bmatrix}$.\n  - Choose $w_A^\\star = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$.\n  - Use initial error $e_0$ with components across all eigendirections by setting $w_{A,0} = w_A^\\star + \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n  - Consider step-sizes $\\mu_A \\in \\{0.3, 1.0, 1.6\\}$.\n  - For tightness testing, define $\\mu_{A,\\text{crit}} = \\dfrac{2}{\\lambda_{\\max}(R_A)}$, and test the triplet $\\{\\mu_{A,\\text{low}}, \\mu_{A,\\text{crit}}, \\mu_{A,\\text{high}}\\} = \\{0.999 \\mu_{A,\\text{crit}}, \\mu_{A,\\text{crit}}, 1.001 \\mu_{A,\\text{crit}}\\}$.\n  - Use $N_A = 2000$ iterations for the recursion $e_{n+1} = (I - \\mu R_A) e_n$ to empirically judge convergence by the ratio of final to initial error norms.\n- Case B (LMS, dimension $m=3$):\n  - $R_B = \\begin{bmatrix} 3.0 & 0.1 & 0.0 \\\\ 0.1 & 1.0 & 0.2 \\\\ 0.0 & 0.2 & 0.5 \\end{bmatrix}$.\n  - $w_B^\\star = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 1.5 \\end{bmatrix}$, noise variance $\\sigma_v^2 = 0.01$.\n  - For mean analysis, initialize $w_{B,0} = w_B^\\star + \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$ and use the deterministic mean recursion $e_{n+1} = (I - \\mu R_B) e_n$ for tightness testing.\n  - Consider step-sizes $\\mu_B \\in \\{0.3, 0.5, 0.7, 0.9\\}$.\n  - For the conservative sufficient mean-square bound, express it in terms of $R_B$ only; evaluate it numerically for each $\\mu_B$.\n  - For tightness testing of the mean bound, define $\\mu_{B,\\text{crit}} = \\dfrac{2}{\\lambda_{\\max}(R_B)}$ and the triplet $\\{\\mu_{B,\\text{low}}, \\mu_{B,\\text{crit}}, \\mu_{B,\\text{high}}\\} = \\{0.999 \\mu_{B,\\text{crit}}, \\mu_{B,\\text{crit}}, 1.001 \\mu_{B,\\text{crit}}\\}$ with $N_B = 2000$ iterations of the mean recursion.\n- Case C (NLMS, dimension $m=3$):\n  - Use the same $R_B$, $w_B^\\star$, and $\\sigma_v^2$ as in Case B.\n  - Generate data $x(n)$ as zero-mean Gaussian with covariance $R_B$ by Cholesky factorization and independent $v(n) \\sim \\mathcal{N}(0, \\sigma_v^2)$.\n  - NLMS update with regularization $\\delta = 10^{-6}$:\n    $$ w_{n+1} = w_n + \\mu_0 \\frac{e(n) x(n)}{\\delta + \\lVert x(n) \\rVert_2^2}, \\quad e(n) = d(n) - x^\\top(n) w_n. $$\n  - Consider $\\mu_{0,C} \\in \\{0.5, 1.5, 2.0, 2.2\\}$.\n  - For tightness testing, define $\\{\\mu_{0,\\text{low}}, \\mu_{0,\\text{crit}}, \\mu_{0,\\text{high}}\\} = \\{1.999, 2.0, 2.001\\}$.\n  - Use $N_C = 1500$ iterations and average the mean error vector across $R_C = 20$ independent runs (with fixed random seed) to empirically decide convergence in the mean for the triplet.\n  - For simulation reproducibility, set the random seed to $42$; generate $x(n)$ via $x(n) = L z(n)$ with $L$ from the Cholesky factorization of $R_B$ and $z(n) \\sim \\mathcal{N}(0, I)$.\n\nDecision rules for empirical convergence classification:\n- For deterministic mean recursions in Cases A and B, declare “convergent” if the average of the last $K = 50$ error-norms divided by the initial error-norm is strictly less than $10^{-2}$; declare “divergent” if the ratio exceeds $10^{2}$; otherwise treat it as non-convergent for the purpose of the required boolean outcomes.\n- For the NLMS Monte Carlo test in Case C, at each tested $\\mu_0$, compute the norm of the sample mean of the error vector across the $R_C$ runs at iteration $0$ and the mean of the norms over the last $K = 50$ iterations; declare “convergent” if the latter is less than $10^{-2}$ times the former.\n\nRequired outputs to compute and print:\n- Case A (Steepest Descent): For $\\mu_A = \\{0.3, 1.0, 1.6\\}$, output three booleans indicating whether each $\\mu$ lies inside the necessary-and-sufficient convergence interval derived from $R_A$. Then output three booleans for the tightness triplet $\\{\\mu_{A,\\text{low}}, \\mu_{A,\\text{crit}}, \\mu_{A,\\text{high}}\\}$ indicating convergence of the deterministic recursion.\n- Case B (LMS): For $\\mu_B = \\{0.3, 0.5, 0.7, 0.9\\}$, first output four booleans indicating whether each $\\mu$ satisfies the derived necessary-and-sufficient mean-stability condition. Then output four booleans indicating whether each $\\mu$ satisfies the derived conservative sufficient mean-square stability bound. Then output three booleans for the tightness triplet $\\{\\mu_{B,\\text{low}}, \\mu_{B,\\text{crit}}, \\mu_{B,\\text{high}}\\}$ indicating convergence of the deterministic mean recursion.\n- Case C (NLMS): For $\\mu_{0,C} = \\{0.5, 1.5, 2.0, 2.2\\}$, output four booleans indicating whether each $\\mu_0$ lies inside the derived stability interval. Then output three booleans for the tightness triplet $\\{\\mu_{0,\\text{low}}, \\mu_{0,\\text{crit}}, \\mu_{0,\\text{high}}\\}$ indicating empirical mean convergence across runs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, constructed by concatenating in order all the boolean results from Cases A, B, and C as specified above. Concretely, the output should be a list of length $24$ containing booleans in this exact order:\n  - Case A: $3$ booleans for interval membership of $\\mu_A$, then $3$ booleans for the tightness triplet.\n  - Case B: $4$ booleans for mean-stability, then $4$ booleans for the conservative mean-square bound, then $3$ booleans for the tightness triplet.\n  - Case C: $4$ booleans for NLMS stability interval, then $3$ booleans for the tightness triplet.\n\nNo physical units or angles are involved; all quantities are dimensionless real numbers. The numerical simulations should adhere to the parameters listed above without requiring any user input.", "solution": "The problem presented is a well-posed and scientifically sound exercise in the field of adaptive signal processing. It requests the derivation and numerical validation of convergence bounds for three fundamental gradient-based adaptation algorithms: Steepest Descent, Least-Mean-Square (LMS), and Normalized Least-Mean-Square (NLMS). The problem is self-contained, providing all necessary definitions, constants, and test conditions. All givens are consistent with established theory. We shall proceed with the analysis.\n\nThe analysis is based on the linear model $d(n) = x^\\top(n) w^\\star + v(n)$, where the goal is to estimate the unknown vector $w^\\star \\in \\mathbb{R}^m$. The error vector is defined as $e_n = w_n - w^\\star$, where $w_n$ is the estimate at iteration $n$.\n\n**1. Deterministic Steepest Descent Convergence**\n\nThe steepest descent algorithm updates the weight vector to minimize the mean-square error (MSE) cost function, $J(w)$. The problem states the gradient is $\\nabla J(w_n) = R(w_n - w^\\star)$, where $R = \\mathbb{E}\\{x(n)x^\\top(n)\\}$ is the positive definite autocorrelation matrix of the input signal. The update recursion is given as $w_{n+1} = w_n - \\mu \\nabla J(w_n)$.\n\nSubstituting the definitions, we obtain the recursion for the weight vector:\n$$w_{n+1} = w_n - \\mu R(w_n - w^\\star)$$\nSubtracting the optimal vector $w^\\star$ from both sides yields the error vector recursion:\n$$w_{n+1} - w^\\star = w_n - w^\\star - \\mu R(w_n - w^\\star)$$\n$$e_{n+1} = e_n - \\mu R e_n = (I - \\mu R) e_n$$\nThis is a linear time-invariant system. For the error vector $e_n$ to converge to zero as $n \\to \\infty$, it is necessary and sufficient that the spectral radius of the state transition matrix, $\\rho(I - \\mu R)$, be strictly less than $1$.\n\nLet $\\lambda_i$ be the eigenvalues of $R$ for $i=1, \\dots, m$. Since $R$ is a symmetric positive definite matrix, its eigenvalues are real and positive, i.e., $\\lambda_i > 0$. The eigenvalues of the matrix $I - \\mu R$ are given by $1 - \\mu \\lambda_i$. The spectral radius condition is therefore:\n$$\\max_i |1 - \\mu \\lambda_i| < 1$$\nThis must hold for all eigenvalues $\\lambda_i$. This single inequality is equivalent to the set of inequalities:\n$$-1 < 1 - \\mu \\lambda_i < 1 \\quad \\forall i$$\nWe solve this for $\\mu$:\n1.  The right side, $1 - \\mu \\lambda_i < 1$, implies $-\\mu \\lambda_i < 0$. Since $\\mu$ must be positive for the update to be in the descent direction and $\\lambda_i > 0$, this simplifies to $\\mu > 0$.\n2.  The left side, $-1 < 1 - \\mu \\lambda_i$, implies $\\mu \\lambda_i < 2$, or $\\mu < 2/\\lambda_i$.\n\nTo satisfy this condition for all $i$, $\\mu$ must be smaller than the minimum of all $2/\\lambda_i$, which is determined by the largest eigenvalue, $\\lambda_{\\max}(R)$. Combining these, the necessary and sufficient condition for the convergence of the steepest descent algorithm is:\n$$0 < \\mu < \\frac{2}{\\lambda_{\\max}(R)}$$\nThis bound is tight. If $\\mu = 2/\\lambda_{\\max}(R)$, the eigenvalue of $(I - \\mu R)$ corresponding to $\\lambda_{\\max}(R)$ is $1 - (2/\\lambda_{\\max}(R)) \\lambda_{\\max}(R) = -1$. An eigenvalue of magnitude $1$ means the error component in that eigendirection does not decay, thus violating the strict convergence condition. If $\\mu > 2/\\lambda_{\\max}(R)$, that eigenvalue's magnitude will exceed $1$, causing the error to diverge.\n\n**2. Least-Mean-Square (LMS) Convergence**\n\nThe LMS algorithm uses an instantaneous estimate of the gradient. The weight update rule is $w_{n+1} = w_n + \\mu e(n) x(n)$, where $e(n) = d(n) - x^\\top(n) w_n$ is the instantaneous error signal.\n\n**2.1. Convergence in the Mean**\n\nTo analyze convergence in the mean, we study the expectation of the error vector, $\\mathbb{E}\\{e_n\\}$. The error recursion is:\n$$e_{n+1} = w_{n+1} - w^\\star = e_n + \\mu (d(n) - x^\\top(n) w_n) x(n)$$\n$$e_{n+1} = e_n + \\mu (x^\\top(n)w^\\star + v(n) - x^\\top(n) w_n) x(n) = e_n - \\mu (x^\\top(n)e_n) x(n) + \\mu v(n) x(n)$$\nTaking the expectation and using the independence assumption (i.e., $w_n$ is independent of $x(n)$ and $v(n)$), we have:\n$$\\mathbb{E}\\{e_{n+1}\\} = \\mathbb{E}\\{e_n\\} - \\mu \\mathbb{E}\\{x(n)x^\\top(n)e_n\\} + \\mu \\mathbb{E}\\{v(n)x(n)\\}$$\nGiven that $v(n)$ is zero-mean and independent of $x(n)$, $\\mathbb{E}\\{v(n)x(n)\\} = \\mathbf{0}$. Using the independence of $e_n$ and $x(n)$, $\\mathbb{E}\\{x(n)x^\\top(n)e_n\\} = \\mathbb{E}\\{x(n)x^\\top(n)\\} \\mathbb{E}\\{e_n\\} = R \\mathbb{E}\\{e_n\\}$. This gives the mean error recursion:\n$$\\mathbb{E}\\{e_{n+1}\\} = (I - \\mu R) \\mathbb{E}\\{e_n\\}$$\nThis recursion is identical in form to the deterministic steepest descent error recursion. Therefore, the necessary and sufficient condition for the mean weight vector to converge to $w^\\star$ (i.e., $\\mathbb{E}\\{e_n\\} \\to \\mathbf{0}$) is also identical:\n$$0 < \\mu < \\frac{2}{\\lambda_{\\max}(R)}$$\n\n**2.2. Mean-Square Stability**\n\nMean-square stability or convergence analysis considers the behavior of $\\mathbb{E}\\{\\|e_n\\|^2\\}$. A full analysis is complex. The problem asks for a conservative, easily computable sufficient bound. A widely-used sufficient condition for mean-square convergence is:\n$$0 < \\mu < \\frac{2}{\\text{tr}(R)}$$\nwhere $\\text{tr}(R) = \\sum_{i=1}^m \\lambda_i$ is the trace of the autocorrelation matrix. This condition is more restrictive (conservative) than the mean-convergence condition because $\\text{tr}(R) \\ge \\lambda_{\\max}(R)$ for a positive definite matrix. This bound is derived under certain assumptions (like Gaussian data) or by applying inequalities during the derivation of the recursion for the weight error covariance matrix. Its value lies in its simplicity, as it does not require computing eigenvalues.\n\n**3. Normalized Least-Mean-Square (NLMS) Stability**\n\nThe NLMS algorithm normalizes the step size by the squared Euclidean norm of the input vector, effectively using a time-varying step size. The update is:\n$$w_{n+1} = w_n + \\mu_0 \\frac{e(n) x(n)}{\\delta + \\lVert x(n) \\rVert_2^2}$$\nwhere $\\mu_0$ is a dimensionless step-size parameter and $\\delta > 0$ is a small regularization constant. The error recursion, neglecting the noise term for stability analysis of the homogeneous system, is:\n$$e_{n+1} = e_n - \\mu_0 \\frac{x^\\top(n) e_n}{\\delta + \\lVert x(n) \\rVert_2^2} x(n) = \\left(I - \\mu_0 \\frac{x(n) x^\\top(n)}{\\delta + \\lVert x(n) \\rVert_2^2}\\right) e_n$$\nTo analyze stability, we can examine how the squared norm of the error vector changes. Let's decompose $e_n$ into components parallel and orthogonal to $x(n)$. Let $P_n = \\frac{x(n)x^\\top(n)}{\\|x(n)\\|_2^2}$ be the projection matrix onto the span of $x(n)$. Approximating $\\delta \\approx 0$ for nonzero $x(n)$, the update becomes $e_{n+1} = (I - \\mu_0 P_n) e_n$.\nThe vector $e_n$ can be written as $e_n = e_{n, \\parallel} + e_{n, \\perp}$, where $e_{n, \\parallel} = P_n e_n$ and $e_{n, \\perp} = (I-P_n) e_n$ are orthogonal.\nApplying the update operator:\n$$e_{n+1} = (I - \\mu_0 P_n)(e_{n, \\parallel} + e_{n, \\perp}) = (I - \\mu_0 P_n)e_{n, \\parallel} + (I - \\mu_0 P_n)e_{n, \\perp}$$\nSince $P_n e_{n, \\parallel} = e_{n, \\parallel}$ and $P_n e_{n, \\perp} = \\mathbf{0}$, we get:\n$$e_{n+1} = (1 - \\mu_0) e_{n, \\parallel} + e_{n, \\perp}$$\nThe squared norm is (by orthogonality):\n$$\\|e_{n+1}\\|_2^2 = (1-\\mu_0)^2 \\|e_{n, \\parallel}\\|_2^2 + \\|e_{n, \\perp}\\|_2^2$$\nFor the error to decrease, we require $\\|e_{n+1}\\|_2^2 < \\|e_n\\|_2^2 = \\|e_{n, \\parallel}\\|_2^2 + \\|e_{n, \\perp}\\|_2^2$. This holds if $(1 - \\mu_0)^2 < 1$, which is true if $-1 < 1 - \\mu_0 < 1$. This implies:\n$$0 < \\mu_0 < 2$$\nThis is the standard stability range for the NLMS algorithm. It is a sufficient condition. The presence of the regularization term $\\delta > 0$ ensures the update remains well-behaved if $\\|x(n)\\|_2^2$ is very small or zero, and does not alter this fundamental stability bound. This bound is remarkably independent of the input signal's statistics (i.e., independent of $R$). The tightness test will confirm that values of $\\mu_0$ at or beyond this range lead to non-convergence or divergence.", "answer": "[True,True,False,True,False,False,True,True,False,False,True,False,False,False,True,False,False,True,True,False,False,True,False,False]", "id": "2874693"}]}