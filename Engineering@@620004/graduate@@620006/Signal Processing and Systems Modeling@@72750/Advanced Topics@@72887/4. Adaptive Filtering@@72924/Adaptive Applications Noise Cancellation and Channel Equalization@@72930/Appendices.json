{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with the fundamental building block of many adaptive systems: a single update step of the Least Mean Squares (LMS) algorithm. This exercise provides a concrete scenario of a channel equalizer and asks you to manually compute the change in the filter's coefficients based on a single data point. By working through this calculation [@problem_id:2850033], you will gain a practical understanding of how an adaptive filter \"learns\" by making small adjustments to reduce the instantaneous error, forming the core of its adaptive capability.", "problem": "A baseband communication receiver uses a 2-tap finite impulse response adaptive equalizer to mitigate intersymbol interference. At discrete time $n=0$, the equalizer input regression vector is $x_0=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, the desired symbol is $d_0=1$, and the current coefficient vector is $w_0=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. The equalizer performs one step of stochastic gradient descent on the instantaneous squared error cost $J_0(w)=\\tfrac{1}{2}\\left(d_0 - x_0^{\\top}w\\right)^{2}$ with step size $\\mu=0.1$, starting from $w_0$. Define the a priori error at time $0$ as $e^{\\text{ap}}(0)=d_0 - x_0^{\\top}w_0$ and the a posteriori error as $e^{\\text{po}}(0)=d_0 - x_0^{\\top}w_1$, where $w_1$ is the coefficient vector after the single gradient step.\n\nUsing only the principle of gradient descent applied to $J_0(w)$ and the given data, compute the updated coefficient vector $w_1$ and the a posteriori error $e^{\\text{po}}(0)$. Provide your final answer as three entries in a single row matrix in the order $\\big(w_{1,1},\\,w_{1,2},\\,e^{\\text{po}}(0)\\big)$. No rounding is required.", "solution": "The problem statement provides a complete and well-posed scenario for the application of the stochastic gradient descent (SGD) algorithm, specifically the Least Mean Squares (LMS) variant, which is standard in adaptive signal processing. The problem is scientifically sound and all necessary data for a unique solution are provided. Therefore, the problem is valid.\n\nThe task is to perform one update step for the coefficient vector $w$ of an adaptive equalizer, starting from an initial vector $w_0$ at discrete time $n=0$. The update is based on minimizing the instantaneous squared error cost function $J_0(w)$:\n$$J_0(w) = \\frac{1}{2}(d_0 - x_0^{\\top}w)^{2}$$\nThe general formula for a gradient descent update is:\n$$w_1 = w_0 - \\mu \\nabla_w J_0(w) \\bigg|_{w=w_0}$$\nwhere $\\mu$ is the step size, and $\\nabla_w J_0(w)$ is the gradient of the cost function with respect to the vector $w$.\n\nFirst, we must derive the gradient of the cost function. Let the instantaneous error at time $n=0$ be a function of $w$, defined as $e_0(w) = d_0 - x_0^{\\top}w$. The cost function is then $J_0(w) = \\frac{1}{2}e_0(w)^2$. Using the chain rule for vector differentiation:\n$$\\nabla_w J_0(w) = \\frac{\\partial J_0}{\\partial e_0} \\nabla_w e_0(w)$$\nThe individual derivatives are:\n$$\\frac{\\partial J_0}{\\partial e_0} = e_0(w)$$\n$$\\nabla_w e_0(w) = \\nabla_w (d_0 - x_0^{\\top}w) = -x_0$$\nCombining these, we obtain the gradient:\n$$\\nabla_w J_0(w) = e_0(w)(-x_0) = -(d_0 - x_0^{\\top}w)x_0$$\nThe gradient must be evaluated at the current coefficient vector, $w_0$:\n$$\\nabla_w J_0(w_0) = -(d_0 - x_0^{\\top}w_0)x_0$$\nThe term $d_0 - x_0^{\\top}w_0$ is defined in the problem as the a priori error, $e^{\\text{ap}}(0)$. Thus, the gradient update term is:\n$$\\nabla_w J_0(w_0) = -e^{\\text{ap}}(0) x_0$$\nSubstituting this back into the gradient descent formula gives the specific update rule for this problem, which is the LMS update equation:\n$$w_1 = w_0 - \\mu (-e^{\\text{ap}}(0) x_0) = w_0 + \\mu e^{\\text{ap}}(0) x_0$$\n\nWe are given the following values:\n-   Desired symbol: $d_0 = 1$\n-   Input regression vector: $x_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$\n-   Initial coefficient vector: $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   Step size: $\\mu = 0.1$\n\nThe first step is to calculate the a priori error, $e^{\\text{ap}}(0)$:\n$$e^{\\text{ap}}(0) = d_0 - x_0^{\\top}w_0 = 1 - \\begin{bmatrix} 1  -1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 1 - (1 \\cdot 0 + (-1) \\cdot 0) = 1 - 0 = 1$$\nNext, we compute the updated coefficient vector, $w_1$:\n$$w_1 = w_0 + \\mu e^{\\text{ap}}(0)x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + (0.1)(1)\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$$\nThe components of the updated vector are therefore $w_{1,1} = 0.1$ and $w_{1,2} = -0.1$.\n\nFinally, we compute the a posteriori error, $e^{\\text{po}}(0)$, which is the error calculated using the updated coefficient vector $w_1$:\n$$e^{\\text{po}}(0) = d_0 - x_0^{\\top}w_1$$\nSubstituting the known and calculated values:\n$$e^{\\text{po}}(0) = 1 - \\begin{bmatrix} 1  -1 \\end{bmatrix}\\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = 1 - (1 \\cdot 0.1 + (-1) \\cdot (-0.1)) = 1 - (0.1 + 0.1) = 1 - 0.2 = 0.8$$\nThe required quantities are $w_{1,1}=0.1$, $w_{1,2}=-0.1$, and $e^{\\text{po}}(0)=0.8$. These are to be presented in a single row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} 0.1  -0.1  0.8 \\end{pmatrix}}$$", "id": "2850033"}, {"introduction": "While the standard LMS algorithm is powerful, its performance can be sensitive to the power of the input signal, potentially leading to slow convergence or instability. To address this, the Normalized Least Mean Squares (NLMS) algorithm was developed. This practice [@problem_id:2850035] introduces a normalization factor based on the input signal's energy, making the adaptation step size relative to the input power. This exercise will guide you through the calculation for an NLMS update, highlighting a crucial modification that enhances the robustness and convergence speed of adaptive filters in real-world applications.", "problem": "A two-tap adaptive filter is used as a simplified model for noise cancellation in a single-channel sensing scenario. The filter weight vector at time index $n$ is $w(n) \\in \\mathbb{R}^{2}$, the input regressor is $x(n) \\in \\mathbb{R}^{2}$, and the desired signal is $d(n) \\in \\mathbb{R}$. The objective at time $n$ is to minimize the instantaneous squared error, defined as $J(n) = \\tfrac{1}{2} e^{2}(n)$, where $e(n) = d(n) - w^{\\top}(n) x(n)$. Starting from this objective and the gradient $\\nabla_{w} J(n)$ with respect to $w$, the Least Mean Squares (LMS) method performs a gradient step. The Normalized Least Mean Squares (NLMS) method rescales this step by the instantaneous input power to make it invariant to input amplitude, with a small positive regularization to avoid division by zero.\n\nGiven the initial weight vector $w(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, step size $\\mu = 1$, regularization $\\delta = 10^{-3}$, input $x(0) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, and desired response $d(0) = 5$, perform one update of the Normalized Least Mean Squares (NLMS) algorithm starting from the definition of $J(n)$ and its gradient, and compute:\n- the updated coefficient vector $w(1)$, and\n- the resulting a posteriori instantaneous error $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$.\n\nExpress your final answer as a single row matrix containing, in order, the two components of $w(1)$ followed by $e^{+}(0)$. No rounding is required; provide the exact expression.", "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- **Model:** Two-tap adaptive filter\n- **Weight vector:** $w(n) \\in \\mathbb{R}^{2}$\n- **Input regressor:** $x(n) \\in \\mathbb{R}^{2}$\n- **Desired signal:** $d(n) \\in \\mathbb{R}$\n- **Objective function:** $J(n) = \\frac{1}{2} e^{2}(n)$\n- **Error signal:** $e(n) = d(n) - w^{\\top}(n) x(n)$\n- **Algorithm:** Normalized Least Mean Squares (NLMS)\n- **Initial weight vector:** $w(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **Step size:** $\\mu = 1$\n- **Regularization:** $\\delta = 10^{-3}$\n- **Input at $n=0$:** $x(0) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$\n- **Desired response at $n=0$:** $d(0) = 5$\n- **Required outputs:** $w(1)$ and $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it concerns the standard NLMS algorithm, a fundamental topic in adaptive signal processing. It is well-posed, providing all necessary parameters and initial conditions for a unique solution. The language is objective and precise. The data are consistent and complete. Therefore, the problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nThe objective is to perform one update of the Normalized Least Mean Squares (NLMS) algorithm to find the updated weight vector $w(1)$ and the a posteriori error $e^{+}(0)$.\n\nThe instantaneous cost function at time $n$ is the squared error:\n$$J(n) = \\frac{1}{2} e^{2}(n) = \\frac{1}{2} (d(n) - w^{\\top}(n) x(n))^{2}$$\nThe NLMS algorithm is a form of stochastic gradient descent. The first step is to compute the gradient of the cost function $J(n)$ with respect to the weight vector $w(n)$. Using the chain rule:\n$$\\nabla_{w} J(n) = \\frac{\\partial J(n)}{\\partial w(n)} = \\frac{\\partial J(n)}{\\partial e(n)} \\frac{\\partial e(n)}{\\partial w(n)}$$\nThe derivatives are:\n$$\\frac{\\partial J(n)}{\\partial e(n)} = e(n)$$\n$$\\frac{\\partial e(n)}{\\partial w(n)} = \\frac{\\partial}{\\partial w(n)} (d(n) - w^{\\top}(n) x(n)) = -x(n)$$\nThus, the gradient is:\n$$\\nabla_{w} J(n) = -e(n) x(n)$$\nThe general stochastic gradient descent update rule is $w(n+1) = w(n) - \\alpha \\nabla_{w} J(n)$, where $\\alpha$ is a step-size parameter. For the NLMS algorithm, the update rule is specifically defined as:\n$$w(n+1) = w(n) - \\left( \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} \\right) \\nabla_{w} J(n)$$\nSubstituting the expression for the gradient, we obtain the NLMS update equation:\n$$w(n+1) = w(n) + \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} e(n) x(n)$$\nwhere $\\|x(n)\\|^{2} = x^{\\top}(n) x(n)$ is the squared Euclidean norm (instantaneous power) of the input vector.\n\nWe are asked to compute the update for $n=0$. The provided values are:\n$w(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x(0) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $d(0) = 5$, $\\mu = 1$, and $\\delta = 10^{-3}$.\n\nFirst, we calculate the a priori error at time $n=0$:\n$$e(0) = d(0) - w^{\\top}(0) x(0) = 5 - \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 5 - 0 = 5$$\nNext, we calculate the squared norm of the input vector $x(0)$:\n$$\\|x(0)\\|^{2} = x^{\\top}(0) x(0) = 3^{2} + 4^{2} = 9 + 16 = 25$$\nNow we can compute the updated weight vector $w(1)$:\n$$w(1) = w(0) + \\frac{\\mu}{\\delta + \\|x(0)\\|^{2}} e(0) x(0)$$\n$$w(1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{10^{-3} + 25} (5) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\nThe term $\\delta + \\|x(0)\\|^2$ is $25.001$, which is $\\frac{25001}{1000}$. The coefficient for the update is:\n$$\\frac{5}{25.001} = \\frac{5}{25001/1000} = \\frac{5000}{25001}$$\nSo, the updated weight vector is:\n$$w(1) = \\frac{5000}{25001} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{15000}{25001} \\\\ \\frac{20000}{25001} \\end{pmatrix}$$\nThe components of the updated weight vector are $w_{1}(1) = \\frac{15000}{25001}$ and $w_{2}(1) = \\frac{20000}{25001}$.\n\nFinally, we compute the a posteriori error, $e^{+}(0)$:\n$$e^{+}(0) = d(0) - w^{\\top}(1) x(0)$$\n$$e^{+}(0) = 5 - \\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n$$e^{+}(0) = 5 - \\left( \\frac{15000 \\cdot 3 + 20000 \\cdot 4}{25001} \\right)$$\n$$e^{+}(0) = 5 - \\left( \\frac{45000 + 80000}{25001} \\right) = 5 - \\frac{125000}{25001}$$\n$$e^{+}(0) = \\frac{5 \\cdot 25001 - 125000}{25001} = \\frac{125005 - 125000}{25001} = \\frac{5}{25001}$$\n\nThe required outputs are the two components of $w(1)$ and the value of $e^{+}(0)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001}  \\frac{5}{25001} \\end{pmatrix}}$$", "id": "2850035"}, {"introduction": "After exploring iterative, adaptive algorithms, it is essential to understand the theoretical target they aim to reach. This practice shifts our focus from the step-by-step adaptation process to the calculation of the optimal filter itself, known as the Wiener filter. Assuming we have access to the statistical properties of our signals—specifically, the autocorrelation of the input and the cross-correlation between the input and desired signal—we can directly solve for the filter coefficients that produce the absolute minimum mean-square error ($J_{\\min}$) [@problem_id:2850046]. This exercise provides a benchmark for performance and deepens your understanding of the error surface that adaptive algorithms navigate.", "problem": "A two-tap linear equalizer is designed to minimize the mean-square error between a zero-mean desired scalar signal $d(n)$ and a linear estimate $\\hat{d}(n)=\\mathbf{w}^{\\top}\\mathbf{x}(n)$ formed from a zero-mean, jointly wide-sense stationary two-dimensional input $\\mathbf{x}(n)\\in\\mathbb{R}^{2}$. Let the input autocorrelation matrix be $R=\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$ and the cross-correlation vector be $\\mathbf{p}=\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$. Suppose that $R=\\begin{bmatrix}21\\\\12\\end{bmatrix}$ and $\\mathbf{p}=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ have been obtained from second-order statistics of the data, and the desired signal variance is $\\sigma_{d}^{2}=\\mathbb{E}\\{d^{2}(n)\\}=1$.\n\nStarting from the minimum mean-square error formulation based on second-order moments and the orthogonality principle, determine:\n\n1. The optimal Wiener solution $\\mathbf{w}_{o}\\in\\mathbb{R}^{2}$ that minimizes $\\mathbb{E}\\{(d(n)-\\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\}$.\n\n2. The minimum mean-square error $J_{\\min}$ achieved at $\\mathbf{w}_{o}$.\n\nExpress the final numerical values exactly. No rounding is required. The answer must be provided as exact rational numbers.", "solution": "The problem presented is a standard exercise in statistical signal processing concerning the derivation of the Wiener filter. It is scientifically grounded, well-posed, and contains all necessary information for a complete solution. Therefore, it is valid. We proceed with the derivation.\n\nThe objective is to find the weight vector $\\mathbf{w}$ that minimizes the mean-square error (MSE) $J(\\mathbf{w})$. The MSE is defined as the expected value of the squared error between the desired signal $d(n)$ and its estimate $\\hat{d}(n)$.\nThe estimate is a linear combination of the input vector $\\mathbf{x}(n)$, given by $\\hat{d}(n) = \\mathbf{w}^{\\top}\\mathbf{x}(n)$. The error signal is $e(n) = d(n) - \\hat{d}(n) = d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n)$.\n\nThe MSE cost function $J(\\mathbf{w})$ is:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{e^2(n)\\} = \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^2\\}$$\nExpanding the squared term, we obtain:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n) - 2d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n) + (\\mathbf{w}^{\\top}\\mathbf{x}(n))(\\mathbf{x}^{\\top}(n)\\mathbf{w})\\}$$\nUsing the linearity of the expectation operator, this becomes:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbb{E}\\{d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n)\\} + \\mathbb{E}\\{\\mathbf{w}^{\\top}\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{w}\\}$$\nSince $\\mathbf{w}$ is a deterministic vector of coefficients, it can be moved outside the expectation:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}\\mathbf{w}$$\nWe are given the following second-order statistics:\nThe variance of the desired signal: $\\sigma_d^2 = \\mathbb{E}\\{d^2(n)\\} = 1$.\nThe cross-correlation vector between the input and the desired signal: $\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe autocorrelation matrix of the input vector: $R = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$.\n\nSubstituting these quantities into the expression for $J(\\mathbf{w})$ yields the performance surface:\n$$J(\\mathbf{w}) = \\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}$$\nTo find the optimal weight vector $\\mathbf{w}_o$ that minimizes this quadratic function, we must compute the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ and set it to the zero vector.\n$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{\\mathrm{d}}{\\mathrm{d}\\mathbf{w}} (\\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}) = -2\\mathbf{p} + 2R\\mathbf{w}$$\nSetting the gradient to zero for the optimal vector $\\mathbf{w}_o$:\n$$-2\\mathbf{p} + 2R\\mathbf{w}_o = \\mathbf{0}$$\nThis leads to the Wiener-Hopf equation:\n$$R\\mathbf{w}_o = \\mathbf{p}$$\nThe optimal Wiener solution $\\mathbf{w}_o$ is found by solving this system of linear equations:\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p}$$\nFirst, we compute the inverse of the autocorrelation matrix $R$. The determinant of $R$ is:\n$$\\det(R) = (2)(2) - (1)(1) = 4 - 1 = 3$$\nSince $\\det(R) \\neq 0$, the inverse exists. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\n$$R^{-1} = \\frac{1}{3}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\nNow, we can calculate $\\mathbf{w}_o$:\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) \\\\ (-\\frac{1}{3})(1) + (\\frac{2}{3})(0) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nThis is the optimal Wiener solution for the filter taps.\n\nNext, we determine the minimum mean-square error, $J_{\\min}$, which is the value of the cost function evaluated at $\\mathbf{w} = \\mathbf{w}_o$:\n$$J_{\\min} = J(\\mathbf{w}_o) = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{w}_o^{\\top}R\\mathbf{w}_o$$\nUsing the Wiener-Hopf equation, $R\\mathbf{w}_o = \\mathbf{p}$, we can simplify this expression. Since $\\mathbf{w}_o^{\\top}R\\mathbf{w}_o = \\mathbf{w}_o^{\\top}(R\\mathbf{w}_o) = \\mathbf{w}_o^{\\top}\\mathbf{p}$, we can substitute this into the equation:\n$$J_{\\min} = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{w}_o^{\\top}\\mathbf{p} = \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p}$$\nThis is the most common and simplified expression for the minimum MSE. Using the values we have:\n$\\sigma_d^2 = 1$, $\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$, and $\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$$\\mathbf{w}_o^{\\top}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) = \\frac{2}{3}$$\nTherefore, the minimum mean-square error is:\n$$J_{\\min} = \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p} = 1 - \\frac{2}{3} = \\frac{1}{3}$$\nThe optimal Wiener filter is $\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$ and the minimum MSE is $J_{\\min} = \\frac{1}{3}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3}  -\\frac{1}{3}  \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "2850046"}]}