{"hands_on_practices": [{"introduction": "Before we can compare the performance of Least Mean Squares (LMS) and Recursive Least Squares (RLS) algorithms, we must understand how their average behavior differs fundamentally. This first exercise challenges you to analyze the mean-weight error dynamics for both algorithms under the standard Independence Assumption. By deriving and contrasting their update equations in expectation, you will uncover why the convergence speed of LMS is dictated by the statistics of the input signal's correlation matrix $R$, while RLS offers a path to rapid convergence largely irrespective of input coloration [@problem_id:2891049].", "problem": "Consider a linear data model with a zero-mean, wide-sense stationary input vector process $\\{x(n)\\} \\in \\mathbb{R}^{M}$ with correlation matrix $R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\} \\succ 0$, an unknown constant parameter vector $w_{o} \\in \\mathbb{R}^{M}$, and scalar desired signal $d(n) = x^{\\top}(n) w_{o} + v(n)$, where $\\{v(n)\\}$ is zero-mean measurement noise independent of $\\{x(n)\\}$ and with finite variance. An adaptive estimator produces an estimate $w(n)$ of $w_{o}$ from the data $\\{x(k), d(k)\\}_{k \\le n}$.\n\nDefine the weight-error vector $\\tilde{w}(n) \\triangleq w(n) - w_{o}$. Under the standard Independence Assumption (IA) used in mean analyses of adaptive filters, $\\tilde{w}(n)$ is treated as independent of the current regressor $x(n)$. The Least Mean Squares (LMS) algorithm (Least Mean Squares (LMS)) uses a stochastic gradient descent on the mean-square error, while the Recursive Least Squares (RLS) algorithm (Recursive Least Squares (RLS)) at each $n$ minimizes the exponentially weighted cost $J(n) \\triangleq \\sum_{k=0}^{n} \\lambda^{n-k} e^{2}(k)$ with $e(k) \\triangleq d(k)-x^{\\top}(k)w(k-1)$ and forgetting factor $\\lambda \\in (0,1]$.\n\nStarting from these definitions and IA, compare the mean weight-error dynamics of LMS and RLS when $\\lambda \\lesssim 1$ in the stationary case described above. Which statement(s) correctly characterize the contrast between their mean updates?\n\nA. Under IA, LMS exhibits mode-dependent mean contraction governed by the eigenvalues of $R$ (each mode $i$ scales roughly by a factor $1-\\mu \\lambda_{i}(R)$ for a stepsize $\\mu$), whereas for RLS with $\\lambda \\lesssim 1$ the mean weight error approximately contracts uniformly as $\\mathbb{E}\\{\\tilde{w}(n+1)\\} \\approx \\lambda\\, \\mathbb{E}\\{\\tilde{w}(n)\\}$, independently of $R$.\n\nB. Under IA, the RLS mean update behaves like $\\mathbb{E}\\{\\tilde{w}(n+1)\\} \\approx \\left(I - \\mu R^{-1}\\right) \\mathbb{E}\\{\\tilde{w}(n)\\}$ for some stepsize $\\mu$, making RLS faster along small-eigenvalue directions of $R$ and slower along large-eigenvalue directions.\n\nC. For $\\lambda \\lesssim 1$, RLS and LMS can be matched in mean by choosing the LMS stepsize $\\mu$ so that $I-\\mu R = \\lambda I$, hence they have indistinguishable mean dynamics irrespective of $R$.\n\nD. With $\\lambda < 1$ in stationary conditions and $\\mathbb{E}\\{v(n)\\}=0$, RLS becomes biased in the mean so that $\\lim_{n\\to\\infty} \\mathbb{E}\\{w(n)\\} \\neq w_{o}$, unlike LMS which remains unbiased in the mean if it is mean-stable.\n\nE. The mean convergence speed of LMS is essentially independent of the eigenvalue spread of $R$, while the mean convergence speed of RLS degrades severely when $R$ is ill-conditioned; thus RLS is more sensitive than LMS to $\\kappa(R)$, the condition number of $R$.", "solution": "The supplied problem statement is subjected to rigorous validation before any attempt at a solution.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information:\n- Data model: A linear model $d(n) = x^{\\top}(n) w_{o} + v(n)$.\n- Input process: $\\{x(n)\\} \\in \\mathbb{R}^{M}$ is a zero-mean, wide-sense stationary (WSS) vector process.\n- Input correlation matrix: $R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\} \\succ 0$ (positive definite).\n- True parameter vector: $w_{o} \\in \\mathbb{R}^{M}$ is an unknown constant.\n- Measurement noise: $\\{v(n)\\}$ is a zero-mean scalar process with finite variance, independent of the process $\\{x(n)\\}$.\n- Weight estimate: $w(n)$ is the estimate of $w_{o}$ at time $n$.\n- Weight-error vector: $\\tilde{w}(n) \\triangleq w(n) - w_{o}$.\n- Key assumption: The Independence Assumption (IA) is to be used, which treats $\\tilde{w}(n)$ as statistically independent of the current input vector $x(n)$.\n- LMS algorithm: A stochastic gradient descent method.\n- RLS algorithm: Minimizes the cost function $J(n) \\triangleq \\sum_{k=0}^{n} \\lambda^{n-k} e^{2}(k)$, where $e(k) \\triangleq d(k)-x^{\\top}(k)w(k-1)$ and the forgetting factor $\\lambda$ is in the interval $(0,1]$.\n- Context: The comparison is for the case where $\\lambda \\lesssim 1$ in a stationary environment.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is set firmly within the established domain of adaptive filter theory, a core topic in signal processing and systems engineering. All concepts, including the LMS and RLS algorithms, the data model, and the Independence Assumption, are standard and rigorously defined in the field. The setup does not violate any scientific or mathematical principles.\n- **Well-Posed**: The question asks for a comparative analysis of the mean weight-error dynamics of two standard algorithms under a specified set of standard assumptions. The problem is structured to have a definite, derivable answer based on these assumptions.\n- **Objective**: The problem statement is formulated using precise, unambiguous mathematical and technical language. It is free of subjective or opinion-based content.\n- **Completeness**: The problem provides all necessary information (data model, algorithm definitions, statistical properties of signals, and analytical assumptions) required to derive the mean-error dynamics for both algorithms.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is scientifically sound, well-posed, objective, and complete. It presents a standard, non-trivial question in adaptive filter theory. Therefore, the problem is **valid**. We proceed to derive the solution.\n\n**Derivation of Mean Weight-Error Dynamics**\n\n**Least Mean Squares (LMS) Algorithm**\n\nThe weight update for the LMS algorithm is given by:\n$$ w(n+1) = w(n) + \\mu x(n) e(n) $$\nwhere $\\mu$ is the step-size parameter and $e(n) = d(n) - x^{\\top}(n)w(n)$ is the estimation error. Substituting the definitions of $d(n)$ and the weight-error vector $\\tilde{w}(n) = w(n) - w_o$, we have:\n$$ \\tilde{w}(n+1) + w_o = \\tilde{w}(n) + w_o + \\mu x(n) [x^{\\top}(n)w_o + v(n) - x^{\\top}(n)(\\tilde{w}(n) + w_o)] $$\n$$ \\tilde{w}(n+1) = \\tilde{w}(n) + \\mu x(n) [-x^{\\top}(n)\\tilde{w}(n) + v(n)] $$\n$$ \\tilde{w}(n+1) = (I - \\mu x(n)x^{\\top}(n))\\tilde{w}(n) + \\mu x(n)v(n) $$\nTo analyze the mean behavior, we take the expectation of both sides:\n$$ \\mathbb{E}\\{\\tilde{w}(n+1)\\} = \\mathbb{E}\\{(I - \\mu x(n)x^{\\top}(n))\\tilde{w}(n)\\} + \\mu \\mathbb{E}\\{x(n)v(n)\\} $$\nSince $v(n)$ is independent of $x(n)$ and has zero mean, $\\mathbb{E}\\{x(n)v(n)\\} = \\mathbb{E}\\{x(n)\\}\\mathbb{E}\\{v(n)\\} = 0$. Applying the Independence Assumption (IA), which states that $\\tilde{w}(n)$ is independent of $x(n)$, we can separate the expectation:\n$$ \\mathbb{E}\\{\\tilde{w}(n+1)\\} = \\mathbb{E}\\{I - \\mu x(n)x^{\\top}(n)\\} \\mathbb{E}\\{\\tilde{w}(n)\\} $$\n$$ \\mathbb{E}\\{\\tilde{w}(n+1)\\} = (I - \\mu \\mathbb{E}\\{x(n)x^{\\top}(n)\\}) \\mathbb{E}\\{\\tilde{w}(n)\\} $$\nBy definition, $R = \\mathbb{E}\\{x(n)x^{\\top}(n)\\}$, so the mean weight-error dynamics for LMS are:\n$$ \\mathbb{E}\\{\\tilde{w}(n+1)\\} = (I - \\mu R) \\mathbb{E}\\{\\tilde{w}(n)\\} $$\nThe convergence of the mean error is governed by the eigenvalues of the matrix $(I - \\mu R)$. If $R = Q\\Lambda Q^{\\top}$ is the eigendecomposition of $R$ with eigenvalues $\\{\\lambda_i\\}$, then the dynamics in the principal-axis coordinate system are decoupled: each mode of the transformed error vector contracts by a factor of $(1 - \\mu \\lambda_i)$. The overall convergence speed is thus limited by the slowest mode, which corresponds to the smallest eigenvalue of $R$, and the stability is constrained by the largest eigenvalue. Consequently, the mean convergence of LMS is highly dependent on the eigenvalue spread of $R$.\n\n**Recursive Least Squares (RLS) Algorithm**\n\nThe RLS algorithm recursively computes the weight vector $w(n)$ that minimizes the exponentially weighted least-squares cost function. The weight update is:\n$$ w(n) = w(n-1) + k(n)[d(n) - x^{\\top}(n)w(n-1)] $$\nwhere $k(n) = P(n)x(n)$ is the gain vector and $P(n) = \\left(\\sum_{k=0}^n \\lambda^{n-k}x(k)x^\\top(k)\\right)^{-1}$.\nIn terms of the weight-error vector $\\tilde{w}(n) = w(n) - w_o$, the update becomes:\n$$ \\tilde{w}(n) = (I - k(n)x^{\\top}(n))\\tilde{w}(n-1) + k(n)v(n) $$\nTaking the expectation gives:\n$$ \\mathbb{E}\\{\\tilde{w}(n)\\} = \\mathbb{E}\\{(I - P(n)x(n)x^{\\top}(n))\\tilde{w}(n-1)\\} + \\mathbb{E}\\{P(n)x(n)v(n)\\} $$\nThe second term is zero because $v(n)$ is zero-mean and independent of past data, which determine $P(n)$ and $x(n)$. Applying IA, we approximate $\\mathbb{E}\\{\\tilde{w}(n-1)\\}$ as being independent of the current data $x(n)$ (and thus $P(n)$):\n$$ \\mathbb{E}\\{\\tilde{w}(n)\\} \\approx (I - \\mathbb{E}\\{P(n)x(n)x^{\\top}(n)\\}) \\mathbb{E}\\{\\tilde{w}(n-1)\\} $$\nFor large $n$ in a stationary environment, the matrix $P(n)$ tends to a steady-state value. Its expected value can be approximated by:\n$$ \\mathbb{E}\\{P(n)\\} \\approx \\left(\\mathbb{E}\\left\\{\\sum_{k=0}^n \\lambda^{n-k}x(k)x^\\top(k)\\right\\}\\right)^{-1} = \\left(R \\sum_{k=0}^n \\lambda^{n-k}\\right)^{-1} $$\nFor large $n$, the geometric series sum is $\\sum_{k=0}^n \\lambda^{n-k} \\approx \\frac{1}{1-\\lambda}$. Thus, $\\mathbb{E}\\{P(n)\\} \\approx (1-\\lambda)R^{-1}$.\nSubstituting this approximation into the expectation $\\mathbb{E}\\{P(n)x(n)x^{\\top}(n)\\}$:\n$$ \\mathbb{E}\\{P(n)x(n)x^{\\top}(n)\\} \\approx \\mathbb{E}\\{(1-\\lambda)R^{-1}x(n)x^{\\top}(n)\\} = (1-\\lambda)R^{-1}\\mathbb{E}\\{x(n)x^{\\top}(n)\\} = (1-\\lambda)R^{-1}R = (1-\\lambda)I $$\nTherefore, the mean weight-error dynamics for RLS are approximately:\n$$ \\mathbb{E}\\{\\tilde{w}(n)\\} \\approx (I - (1-\\lambda)I)\\mathbb{E}\\{\\tilde{w}(n-1)\\} = \\lambda \\mathbb{E}\\{\\tilde{w}(n-1)\\} $$\nThis result shows that all modes of the mean weight-error for RLS contract approximately by the same factor $\\lambda$ at each iteration. The convergence of the mean is therefore independent of the eigenvalue structure of the input correlation matrix $R$.\n\n**Evaluation of Options**\n\n**A. Under IA, LMS exhibits mode-dependent mean contraction governed by the eigenvalues of $R$ (each mode $i$ scales roughly by a factor $1-\\mu \\lambda_{i}(R)$ for a stepsize $\\mu$), whereas for RLS with $\\lambda \\lesssim 1$ the mean weight error approximately contracts uniformly as $\\mathbb{E}\\{\\tilde{w}(n+1)\\} \\approx \\lambda\\, \\mathbb{E}\\{\\tilde{w}(n)\\}$, independently of $R$.**\n- This statement accurately summarizes our derivations for both LMS and RLS. The LMS part correctly identifies the mode-dependent convergence factor $(1-\\mu \\lambda_i)$. The RLS part correctly states the approximate uniform contraction by a factor $\\lambda$, which is a key feature rendering its convergence speed insensitive to the input correlation.\n- **Verdict: Correct.**\n\n**B. Under IA, the RLS mean update behaves like $\\mathbb{E}\\{\\tilde{w}(n+1)\\} \\approx \\left(I - \\mu R^{-1}\\right) \\mathbb{E}\\{\\tilde{w}(n)\\}$ for some stepsize $\\mu$, making RLS faster along small-eigenvalue directions of $R$ and slower along large-eigenvalue directions.**\n- The proposed dynamics matrix $(I - \\mu R^{-1})$ is incorrect. The RLS algorithm approximates Newton's method, whose mean dynamics are governed by a matrix proportional to $I$, not $R^{-1}$. As derived, the correct form of the approximate propagator is $\\lambda I$. The statement incorrectly characterizes the a RLS update and its convergence properties.\n- **Verdict: Incorrect.**\n\n**C. For $\\lambda \\lesssim 1$, RLS and LMS can be matched in mean by choosing the LMS stepsize $\\mu$ so that $I-\\mu R = \\lambda I$, hence they have indistinguishable mean dynamics irrespective of $R$.**\n- The condition for matching the dynamics is $I - \\mu R = \\lambda I$, which simplifies to $\\mu R = (1-\\lambda)I$. This requires $R = \\frac{1-\\lambda}{\\mu}I$. This means the correlation matrix $R$ must be a scaled identity matrix (i.e., the input signal must be white). A single scalar $\\mu$ cannot satisfy this equality for an arbitrary matrix $R$. The claim that matching is possible \"irrespective of $R$\" is fundamentally false.\n- **Verdict: Incorrect.**\n\n**D. With $\\lambda < 1$ in stationary conditions and $\\mathbb{E}\\{v(n)\\}=0$, RLS becomes biased in the mean so that $\\lim_{n\\to\\infty} \\mathbb{E}\\{w(n)\\} \\neq w_{o}$, unlike LMS which remains unbiased in the mean if it is mean-stable.**\n- Standard analysis shows that the RLS algorithm is asymptotically unbiased in the mean. That is, $\\lim_{n\\to\\infty} \\mathbb{E}\\{w(n)\\} = w_{o}$. The noise term $v(n)$ has zero mean and is independent of the input signal, so its influence on the mean of the weight estimate averages to zero. The term \"bias\" in the context of RLS with $\\lambda<1$ typically refers to the fact that the algorithm minimizes a cost function that is a biased estimate of the true MSE, leading to a non-zero steady-state excess mean-square error (misadjustment), but not a bias in the mean of the weights themselves. The statement is factually wrong about RLS.\n- **Verdict: Incorrect.**\n\n**E. The mean convergence speed of LMS is essentially independent of the eigenvalue spread of $R$, while the mean convergence speed of RLS degrades severely when $R$ is ill-conditioned; thus RLS is more sensitive than LMS to $\\kappa(R)$, the condition number of $R$.**\n- This statement entirely reverses the known facts. The mean convergence of LMS is critically dependent on the eigenvalue spread (condition number) of $R$. In contrast, a primary advantage of RLS is its insensitivity to the eigenvalue spread of $R$, as its mean convergence rate is determined by $\\lambda$.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2891049"}, {"introduction": "Knowing that LMS performance degrades with input coloration, a natural question arises: by how much? This practice guides you to quantify this performance gap by deriving an elegant closed-form expression for the total transient error of an optimized LMS filter. By comparing this to the error of an idealized RLS algorithm, you will derive a powerful result that links the performance ratio directly to the input signal's condition number, $\\kappa$ [@problem_id:2891072].", "problem": "Consider an adaptive identification problem for an $M$-tap finite impulse response filter with unknown coefficient vector $\\,\\mathbf{w}_{\\star}\\in\\mathbb{R}^{M}\\,$. The regressor $\\,\\mathbf{x}(k)\\in\\mathbb{R}^{M}\\,$ is a wide-sense stationary, zero-mean process with a positive-definite covariance matrix $\\,\\mathbf{R}=\\mathbb{E}\\{\\mathbf{x}(k)\\mathbf{x}(k)^{\\top}\\}\\,$. Assume a noise-free desired response $\\,d(k)=\\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star}\\,$. Let the eigenvalues of $\\,\\mathbf{R}\\,$ be contained in the compact interval $[\\lambda_{\\min},\\lambda_{\\max}]$ with $\\,0<\\lambda_{\\min}\\le \\lambda_{\\max}<\\infty\\,$, and define the condition number $\\,\\kappa\\triangleq \\lambda_{\\max}/\\lambda_{\\min}\\,$.\n\nTwo algorithms are considered:\n\n1) Least Mean Squares (LMS; Least Mean Squares) with fixed step size $\\,\\mu>0\\,$:\n$$\n\\mathbf{w}_{k+1}=\\mathbf{w}_{k}+\\mu\\,\\mathbf{x}(k)\\big(d(k)-\\mathbf{x}(k)^{\\top}\\mathbf{w}_{k}\\big).\n$$\nDefine the weight error $\\,\\mathbf{v}_{k}\\triangleq \\mathbf{w}_{k}-\\mathbf{w}_{\\star}\\,$. Under the standard independence assumption, use the quadratic nature of the mean-square cost $\\,J(\\mathbf{w})=\\mathbb{E}\\{(d-\\mathbf{x}^{\\top}\\mathbf{w})^{2}\\}\\,$ and the spectral properties of $\\,\\mathbf{R}\\,$ to derive a worst-mode exponential bound of the form\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}\\le \\rho(\\mu)^{k}\\,\\|\\mathbf{v}_{0}\\|^{2},\n$$\nfor some contraction factor $\\,\\rho(\\mu)\\in(0,1)\\,$ when $\\,\\mu\\in(0,2/\\lambda_{\\max})\\,$. Then define the integrated worst-case transient mean-square bound\n$$\nS_{\\mathrm{LMS}}(\\mu)\\triangleq \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}},\n$$\nand minimize this bound over the admissible $\\,\\mu\\,$ to obtain a closed-form expression for $\\,S_{\\mathrm{LMS}}^{\\star}\\triangleq \\inf_{\\mu\\in(0,2/\\lambda_{\\max})}S_{\\mathrm{LMS}}(\\mu)\\,$ in terms of $\\,\\kappa\\,$ only.\n\n2) Recursive Least Squares (RLS; Recursive Least Squares) with an exponentially-weighted cost and forgetting factor $\\,\\alpha\\in(0,1]\\,$:\n$$\nJ_{\\alpha,k}(\\mathbf{w})=\\sum_{i=0}^{k}\\alpha^{k-i}\\big(d(i)-\\mathbf{x}(i)^{\\top}\\mathbf{w}\\big)^{2}.\n$$\nIn the idealized limit of exact second-order curvature information for the quadratic cost $\\,J(\\mathbf{w})\\,$ (that is, access to $\\,\\mathbf{R}^{-1}\\,$ so that a Newton step is realizable), determine the minimum achievable value of the integrated worst-case transient mean-square bound\n$$\nS_{\\mathrm{RLS}}^{\\star}\\triangleq \\inf_{\\alpha\\in(0,1]}\\ \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}},\n$$\nover $\\,\\alpha\\,$, again expressed without units.\n\nFinally, report the ratio\n$$\n\\Gamma(\\kappa)\\triangleq \\frac{S_{\\mathrm{LMS}}^{\\star}}{S_{\\mathrm{RLS}}^{\\star}}\n$$\nas a closed-form analytic expression in terms of $\\,\\kappa\\,$ only. Provide the exact symbolic expression for $\\,\\Gamma(\\kappa)\\,$ as your final answer. No numerical approximation is required, and no units are to be included.", "solution": "The problem as stated is a standard exercise in the analysis of adaptive filter algorithms, specifically comparing the transient performance of the Least Mean Squares (LMS) and idealized Recursive Least Squares (RLS) algorithms. All givens are standard in the field and the problem is self-contained, scientifically grounded, and well-posed. There are no logical contradictions, missing information, or pseudoscientific elements. Therefore, the problem is valid and can be solved.\n\nFirst, we analyze the LMS algorithm. The weight error vector is defined as $\\mathbf{v}_{k} \\triangleq \\mathbf{w}_{k}-\\mathbf{w}_{\\star}$. The update equation for the weight vector is $\\mathbf{w}_{k+1}=\\mathbf{w}_{k}+\\mu\\,\\mathbf{x}(k)(d(k)-\\mathbf{x}(k)^{\\top}\\mathbf{w}_{k})$. Substituting $\\mathbf{w}_k = \\mathbf{v}_k + \\mathbf{w}_\\star$ and the noise-free desired signal $d(k) = \\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star}$, we obtain the recursion for the error vector:\n$$\n\\mathbf{v}_{k+1} + \\mathbf{w}_{\\star} = \\mathbf{v}_{k} + \\mathbf{w}_{\\star} + \\mu \\mathbf{x}(k)(\\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star} - \\mathbf{x}(k)^{\\top}(\\mathbf{v}_{k} + \\mathbf{w}_{\\star}))\n$$\n$$\n\\mathbf{v}_{k+1} = \\mathbf{v}_{k} - \\mu \\mathbf{x}(k)\\mathbf{x}(k)^{\\top}\\mathbf{v}_{k} = (\\mathbf{I} - \\mu \\mathbf{x}(k)\\mathbf{x}(k)^{\\top}) \\mathbf{v}_{k}\n$$\nTo analyze the mean-square performance, we consider the evolution of $\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}$. A standard method for deriving a worst-mode bound is to work in the coordinate system defined by the eigenvectors of the covariance matrix $\\mathbf{R}$. Let $\\mathbf{R} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top}$ be the eigendecomposition of $\\mathbf{R}$, where $\\mathbf{Q}$ is an orthogonal matrix of eigenvectors and $\\mathbf{\\Lambda}$ is a diagonal matrix of eigenvalues $\\lambda_i$. Define the transformed error vector $\\tilde{\\mathbf{v}}_{k} \\triangleq \\mathbf{Q}^{\\top}\\mathbf{v}_{k}$. The squared norm is invariant under this transformation: $\\|\\mathbf{v}_{k}\\|^{2} = \\|\\tilde{\\mathbf{v}}_{k}\\|^{2} = \\sum_{i=1}^{M} |\\tilde{v}_{k,i}|^2$.\n\nUnder the standard independence assumption, the evolution of the mean-square of each mode of the transformed error vector can be approximated or bounded. A common bound used for noise-free transient analysis considers that the decay of the energy in each mode is governed by the square of the decay factor of the mean. The mean of the $i$-th transformed error component evolves as $\\mathbb{E}\\{\\tilde{v}_{k+1,i}\\} = (1-\\mu\\lambda_i)\\mathbb{E}\\{\\tilde{v}_{k,i}\\}$. This leads to a bound on the mean-square error evolution for each mode: $\\mathbb{E}\\{|\\tilde{v}_{k,i}|^2\\} \\approx (1-\\mu\\lambda_i)^{2k} |\\tilde{v}_{0,i}|^2$.\nTo obtain a worst-case bound on the total mean-square error, we bound the decay of all modes by the slowest-decaying mode:\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\} = \\sum_{i=1}^{M} \\mathbb{E}\\{|\\tilde{v}_{k,i}|^2\\} \\approx \\sum_{i=1}^{M} (1-\\mu\\lambda_i)^{2k} |\\tilde{v}_{0,i}|^2 \\le \\left(\\max_i (1-\\mu\\lambda_i)^2\\right)^k \\sum_{i=1}^{M} |\\tilde{v}_{0,i}|^2\n$$\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\} \\le \\left(\\max_i (1-\\mu\\lambda_i)^2\\right)^k \\|\\mathbf{v}_{0}\\|^{2}\n$$\nThis matches the requested form $\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}\\le \\rho(\\mu)^{k}\\,\\|\\mathbf{v}_{0}\\|^{2}$, with the contraction factor defined as $\\rho(\\mu) \\triangleq \\max_{i} (1-\\mu\\lambda_i)^2 = (\\max_{i} |1-\\mu\\lambda_i|)^2$. The eigenvalues $\\lambda_i$ are in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. The term $|1-\\mu\\lambda|$ is maximized at the boundaries of this interval, so $\\rho(\\mu) = (\\max(|1-\\mu\\lambda_{\\min}|, |1-\\mu\\lambda_{\\max}|))^2$. For convergence, we need $\\rho(\\mu)<1$, which requires $\\mu \\in (0, 2/\\lambda_{\\max})$.\n\nThe integrated transient bound is $S_{\\mathrm{LMS}}(\\mu) = \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}} \\le \\sum_{k=0}^{\\infty} \\rho(\\mu)^k = \\frac{1}{1-\\rho(\\mu)}$. To find $S_{\\mathrm{LMS}}^{\\star}$, we must minimize this upper bound by choosing $\\mu$ to minimize $\\rho(\\mu)$. This is equivalent to minimizing $\\max(|1-\\mu\\lambda_{\\min}|, |1-\\mu\\lambda_{\\max}|)$. The minimum is achieved when the two arguments are equal in magnitude. Since $\\mu < 2/\\lambda_{\\max} \\le 2/\\lambda_{\\min}$, both $1-\\mu\\lambda_{\\min}$ and $1-\\mu\\lambda_{\\max}$ are greater than $-1$. Thus we set $1-\\mu\\lambda_{\\min} = -(1-\\mu\\lambda_{\\max}) = \\mu\\lambda_{\\max}-1$.\n$$\n2 = \\mu(\\lambda_{\\min}+\\lambda_{\\max}) \\implies \\mu_{\\mathrm{opt}} = \\frac{2}{\\lambda_{\\min}+\\lambda_{\\max}}\n$$\nAt this optimal step size, the maximal value is $1-\\mu_{\\mathrm{opt}}\\lambda_{\\min} = 1 - \\frac{2\\lambda_{\\min}}{\\lambda_{\\min}+\\lambda_{\\max}} = \\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}}$.\nUsing the condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$, this becomes $\\frac{\\kappa\\lambda_{\\min}-\\lambda_{\\min}}{\\kappa\\lambda_{\\min}+\\lambda_{\\min}} = \\frac{\\kappa-1}{\\kappa+1}$.\nThe minimum value of $\\rho(\\mu)$ is thus $\\rho_{\\min} = \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2$.\nThe minimal integrated bound is then:\n$$\nS_{\\mathrm{LMS}}^{\\star} = \\frac{1}{1-\\rho_{\\min}} = \\frac{1}{1 - \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2} = \\frac{(\\kappa+1)^2}{(\\kappa+1)^2 - (\\kappa-1)^2} = \\frac{(\\kappa+1)^2}{(\\kappa^2+2\\kappa+1) - (\\kappa^2-2\\kappa+1)} = \\frac{(\\kappa+1)^2}{4\\kappa}\n$$\n\nNext, we analyze the idealized RLS algorithm. The problem states this corresponds to a Newton step, which implies using the inverse of the Hessian of the true mean-square error cost function $J(\\mathbf{w}) = \\mathbb{E}\\{(d-\\mathbf{x}^{\\top}\\mathbf{w})^2\\} = \\mathbf{v}^\\top \\mathbf{R} \\mathbf{v}$. The Hessian is $2\\mathbf{R}$. The Newton update rule is $\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\gamma H^{-1} \\nabla J(\\mathbf{w}_k)$. With $\\nabla J(\\mathbf{w}_k) = 2\\mathbf{R}\\mathbf{v}_k$ and $H=2\\mathbf{R}$, this becomes:\n$$\n\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\gamma (2\\mathbf{R})^{-1} (2\\mathbf{R}\\mathbf{v}_k) = \\mathbf{w}_k - \\gamma \\mathbf{v}_k\n$$\nSubtracting $\\mathbf{w}_{\\star}$ from both sides, we get the error recursion $\\mathbf{v}_{k+1} = \\mathbf{v}_k - \\gamma \\mathbf{v}_k = (1-\\gamma)\\mathbf{v}_k$.\nThe problem links this to an RLS formulation with forgetting factor $\\alpha$. In RLS, the step size is effectively controlled by $1-\\alpha$. Thus, we identify the Newton step size $\\gamma$ with $1-\\alpha$.\n$$\n\\mathbf{v}_{k+1} = (1-(1-\\alpha))\\mathbf{v}_{k} = \\alpha \\mathbf{v}_{k}\n$$\nThis is a deterministic recursion. The error vector at step $k$ is $\\mathbf{v}_{k} = \\alpha^k \\mathbf{v}_0$. The squared norm is $\\|\\mathbf{v}_{k}\\|^2 = \\alpha^{2k}\\|\\mathbf{v}_0\\|^2$. This idealized algorithm pre-whitens the problem, causing all modes to decay at the same rate $\\alpha$, independent of the covariance structure $\\mathbf{R}$.\nThe integrated mean-square bound is:\n$$\nS_{\\mathrm{RLS}}(\\alpha) = \\sum_{k=0}^{\\infty} \\frac{\\|\\mathbf{v}_{k}\\|^{2}}{\\|\\mathbf{v}_{0}\\|^{2}} = \\sum_{k=0}^{\\infty} \\alpha^{2k} = \\frac{1}{1-\\alpha^2}\n$$\nThis sum converges for $|\\alpha|<1$. We need to find the infimum for $\\alpha \\in (0,1]$. For $\\alpha=1$, the sum diverges. The function $\\frac{1}{1-\\alpha^2}$ is strictly increasing for $\\alpha \\in [0,1)$. The infimum is therefore at the lower boundary of the interval, as $\\alpha \\to 0^+$.\n$$\nS_{\\mathrm{RLS}}^{\\star} = \\inf_{\\alpha\\in(0,1]} \\frac{1}{1-\\alpha^2} = \\lim_{\\alpha \\to 0^+} \\frac{1}{1-\\alpha^2} = 1\n$$\n\nFinally, we compute the ratio $\\Gamma(\\kappa) = S_{\\mathrm{LMS}}^{\\star} / S_{\\mathrm{RLS}}^{\\star}$.\n$$\n\\Gamma(\\kappa) = \\frac{\\frac{(\\kappa+1)^2}{4\\kappa}}{1} = \\frac{(\\kappa+1)^2}{4\\kappa}\n$$\nThis ratio quantifies the performance degradation of LMS relative to an idealized RLS/Newton method due to eigenvalue spread in the input signal's covariance matrix. For $\\kappa=1$ (white input), $\\Gamma(1)=1$, indicating both algorithms have the same integrated transient error. As $\\kappa$ increases, the performance of LMS degrades.", "answer": "$$\n\\boxed{\\frac{(\\kappa+1)^2}{4\\kappa}}\n$$", "id": "2891072"}, {"introduction": "Theoretical analysis provides deep insights, but observing algorithms in action solidifies understanding. In this hands-on coding exercise, you will implement both LMS and RLS to identify an unknown system and visualize their convergence trajectories [@problem_id:2891050]. This simulation will not only confirm the rapid convergence of RLS predicted by theory but also reveal a critical practical aspect: the risk of early-time transient overshoot, a phenomenon tied to the algorithm's initialization.", "problem": "You will implement and compare two adaptive identification algorithms for a linear time-invariant system: Least Mean Squares (LMS) and Recursive Least Squares (RLS). The goal is to quantify how the initialization parameter $\\delta$ in the RLS information matrix $P(0)=\\delta^{-1} I$ affects early-time overshoot along covariance eigenmodes, and to contrast this with the mode-wise monotonic decay obtained by LMS under a conservative step size.\n\nFundamental base for the derivation:\n- Linear system identification under zero-mean, independent and identically distributed Gaussian regressors modeled as $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$ with a symmetric positive definite covariance $R$.\n- Desired response $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$ with a fixed, unknown parameter vector $\\mathbf{w}^\\star$ and no measurement noise.\n- Gradient descent on the instantaneous squared error for the Least Mean Squares (LMS) algorithm, initialized as $\\mathbf{w}(0)=\\mathbf{0}$.\n- Recursive Least Squares (RLS) algorithm with exponential forgetting factor $\\lambda_{\\mathrm{RLS}} \\in (0,1]$, initialized with $\\mathbf{w}(0)=\\mathbf{0}$ and $P(0)=\\delta^{-1} I$.\n- Eigen-decomposition of $R$ as $R = U \\Lambda U^\\top$ with $\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ and $U$ orthonormal, which induces statistically decoupled modal dynamics for LMS under the independence assumption.\n\nDefinitions you must implement:\n- LMS update driven by the gradient descent on the instantaneous squared error: $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\,\\mathbf{x}_t \\,\\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)\\big)$, with step size $\\mu>0$.\n- RLS update defined by the recursive least-squares with forgetting factor $\\lambda_{\\mathrm{RLS}}$ and initial information matrix $P(0)$:\n  - Gain $\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$,\n  - Weight update $\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t \\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1)\\big)$,\n  - Information matrix update $P(t) = \\lambda_{\\mathrm{RLS}}^{-1} \\big(P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1)\\big)$.\n\nPerformance metric to compute:\n- Let the modal misalignment be $\\mathbf{z}(t) = U^\\top (\\mathbf{w}(t) - \\mathbf{w}^\\star)$ and $m_k(t) = \\mathbb{E}[z_k(t)^2]$.\n- For RLS, define the early-time overshoot factor for mode $k$ over the time window $t \\in \\{0,1,\\dots,T_0\\}$ as\n  $$\\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1.$$\n- Summarize the RLS early overshoot at a given $\\delta$ by the maximum across modes,\n  $$\\rho_{\\max} = \\max_k \\rho_k.$$\n- For LMS, assess whether the averaged modal misalignments $m_k(t)$ are monotonically nonincreasing over $t \\in \\{0,1,\\dots,T_0\\}$ for every mode $k$.\n\nSimulation protocol to approximate expectations:\n- Use $n=4$ parameters, with $R$ diagonal so that $U=I$ and $\\Lambda=\\mathrm{diag}(\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4)$ with $\\lambda_1=3$, $\\lambda_2=1$, $\\lambda_3=0.3$, $\\lambda_4=0.1$.\n- Let the true parameter be $\\mathbf{w}^\\star = [1,-0.5,0.25,-0.125]^\\top$.\n- Generate independent and identically distributed regressors $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$ by drawing $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ and setting $\\mathbf{x}_t = \\Lambda^{1/2} \\mathbf{g}_t$.\n- Use no observation noise so that $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$.\n- Fix the LMS step size to $\\mu=0.05$ and initialize $\\mathbf{w}(0)=\\mathbf{0}$.\n- Fix the RLS forgetting factor to $\\lambda_{\\mathrm{RLS}}=0.995$ and initialize $\\mathbf{w}(0)=\\mathbf{0}$ and $P(0)=\\delta^{-1} I$.\n- Simulate for a total horizon of $T=200$ iterations and define the early window as $T_0=30$.\n- Approximate $\\mathbb{E}[\\,\\cdot\\,]$ by averaging over $M=200$ independent Monte Carlo trials. Ensure reproducibility by using a fixed pseudorandom number generator seed $s=2025$ to generate all Monte Carlo sequences, and reuse the exact same Monte Carlo regressor sequences across all algorithms and parameter settings.\n- Evaluate three test cases corresponding to RLS initialization scales $\\delta \\in \\{10^{-3}, 1, 10^2\\}$, with the same $\\lambda_{\\mathrm{RLS}}$ and data for all three $\\delta$ values. LMS settings remain fixed and are used once for the monotonicity assessment.\n\nWhat to compute for each test case:\n- For each $\\delta \\in \\{10^{-3}, 1, 10^2\\}$, compute $\\rho_{\\max}$ as defined above from the RLS simulations.\n- From the LMS simulations, compute a single boolean indicating whether $m_k(t)$ is nonincreasing in $t$ for all modes $k \\in \\{1,2,3,4\\}$ over the window $t \\in \\{0,1,\\dots,T_0\\}$, based on the Monte Carlo average.\n\nFinal output format:\n- Your program must produce a single line containing a list in the format $[\\rho_{\\max}(\\delta=10^{-3}), \\rho_{\\max}(\\delta=1), \\rho_{\\max}(\\delta=10^{2}), \\mathrm{LMS\\_all\\_modes\\_monotone}]$, where the first three entries are floating-point numbers and the last entry is a boolean. No additional text should be printed.\n\nTest suite summary to ensure coverage:\n- Happy path: $\\delta=1$ with moderate initial information magnitude.\n- Boundary-like aggressive adaptation: $\\delta=10^{-3}$ giving large $P(0)$ and likely larger early overshoot.\n- Edge-like conservative adaptation: $\\delta=10^{2}$ giving small $P(0)$ and likely negligible early overshoot.\n- The LMS monotonic boolean must be computed once under the fixed $\\mu$ and shared across all $\\delta$ test cases.\n\nNo physical units or angle units are involved in this problem. All results are dimensionless real numbers or a boolean as specified.", "solution": "The problem is valid. It is a well-posed, scientifically grounded exercise in the field of adaptive signal processing, requiring the comparison of the Least Mean Squares (LMS) and Recursive Least Squares (RLS) algorithms. All parameters and procedures are specified with sufficient detail and are consistent with established theory and practice. The investigation of the RLS initialization parameter $\\delta$ and its effect on convergence overshoot is a classic and instructive topic. I will now provide the full solution.\n\nThe objective is to implement and numerically compare the convergence behavior of the LMS and RLS algorithms for a system identification task. The primary focus is on the early-time performance, specifically quantifying the overshoot in RLS modal error dynamics as a function of its initialization and contrasting this with the monotonic convergence of LMS under a conservative step size.\n\nThe foundation of this analysis is a linear system model where the desired signal $d_t$ is generated by a true, unknown weight vector $\\mathbf{w}^\\star$ acting on an input regressor vector $\\mathbf{x}_t$, such that $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$. The regressors $\\mathbf{x}_t$ are drawn from a zero-mean Gaussian distribution with a known covariance matrix $R$, i.e., $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$. The adaptive algorithms, initialized with a zero weight vector $\\mathbf{w}(0) = \\mathbf{0}$, aim to estimate $\\mathbf{w}^\\star$ by sequentially processing pairs of $(\\mathbf{x}_t, d_t)$.\n\nPerformance is evaluated in the modal domain defined by the eigenvectors of the input covariance matrix $R$. Since $R$ is specified as a diagonal matrix $R = \\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$, its eigenvectors are the standard basis vectors, and the modal decomposition simplifies significantly. The modal misalignment for the $k$-th mode is $z_k(t) = (\\mathbf{w}(t) - \\mathbf{w}^\\star)_k$. The performance metric is the mean squared modal misalignment, $m_k(t) = \\mathbb{E}[z_k(t)^2]$, which we will approximate by averaging over many independent Monte Carlo trials.\n\nThe simulation will be conducted according to the following steps:\n\n1.  **Environment and Data Generation**: We first establish the simulation parameters: number of dimensions $n=4$, total iterations $T=200$, early-time window size $T_0=30$, and number of Monte Carlo trials $M=200$. The fixed parameters are the true weight vector $\\mathbf{w}^\\star = [1, -0.5, 0.25, -0.125]^\\top$ and the input covariance matrix $R = \\Lambda = \\mathrm{diag}(3, 1, 0.3, 0.1)$. To ensure reproducibility and fair comparison, we generate $M$ sequences of input vectors $\\{\\mathbf{x}_t\\}_{t=0}^{T-1}$ and corresponding desired signals $\\{d_t\\}_{t=0}^{T-1}$ once, using a fixed random number generator seed $s=2025$. These data sets will be reused for both algorithms and all parameter settings. An input vector $\\mathbf{x}_t$ is generated by sampling $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ and setting $\\mathbf{x}_t = \\Lambda^{1/2}\\mathbf{g}_t$.\n\n2.  **LMS Algorithm Simulation and Analysis**: The LMS algorithm updates the weight estimate using a simple gradient descent approach on the instantaneous squared error. The update rule is:\n    $$ \\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\mathbf{x}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)) $$\n    We simulate this process for each of the $M$ data sequences with the step size $\\mu=0.05$. For each trial, we record the trajectory of the squared modal misalignments, $(z_k^{(i)}(t))^2 = (\\mathbf{w}^{(i)}(t) - \\mathbf{w}^\\star)_k^2$, for $i \\in \\{1, \\dots, M\\}$, $t \\in \\{0, \\dots, T\\}$, and $k \\in \\{1, \\dots, n\\}$. Averaging these over the $M$ trials yields the estimated mean-squared modal misalignment curves, $m_k(t)$.\n    \n    We then assess the monotonicity of these curves. For each mode $k$, we check if $m_k(t) \\le m_k(t-1)$ holds for all $t \\in \\{1, \\dots, T_0\\}$. The final result is a single boolean value, `LMS_all_modes_monotone`, which is true if and only if all four modes exhibit nonincreasing mean-squared misalignment over the early-time window.\n\n3.  **RLS Algorithm Simulation and Analysis**: The RLS algorithm provides a recursive solution to the least squares problem. We implement the specified update equations for a forgetting factor $\\lambda_{\\mathrm{RLS}}=0.995$. For each time step $t$, starting from the initial states $\\mathbf{w}(0)=\\mathbf{0}$ and $P(0)=\\delta^{-1}I$, the algorithm computes:\n    -   Gain vector: $\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$\n    -   Weight vector: $\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1))$\n    -   Inverse covariance matrix: $P(t) = \\lambda_{\\mathrm{RLS}}^{-1} (P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1))$\n\n    We execute three separate sets of simulations for the RLS algorithm, corresponding to the initialization parameters $\\delta \\in \\{10^{-3}, 1, 10^2\\}$. For each $\\delta$, we run $M$ trials using the same data as the LMS simulation. As with LMS, we compute the mean-squared modal misalignment curves $m_k(t)$ by averaging over the trials.\n\n    The key performance metric for RLS is the early-time overshoot factor, $\\rho_{\\max}$. For each mode $k$, we first compute the initial error $m_k(0) = \\mathbb{E}[z_k(0)^2] = \\mathbb{E}[(-w^\\star_k)^2] = (w^\\star_k)^2$. Then, we find the peak error within the early-time window, $\\max_{0 \\le t \\le T_0} m_k(t)$. The overshoot factor for that mode is:\n    $$ \\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1 $$\n    The final metric for a given $\\delta$ is the maximum overshoot across all modes, $\\rho_{\\max} = \\max_k \\{\\rho_k\\}$. A value of $\\rho_{\\max} > 0$ indicates that at least one mode's mean-squared error overshot its initial value.\n\n4.  **Implementation and Final Output**: The described procedures are implemented in a Python script. The script first generates the shared dataset, then runs the LMS simulation to determine the monotonicity boolean. Subsequently, it iterates through the specified $\\delta$ values, running the RLS simulation for each and calculating the corresponding $\\rho_{\\max}$. The final results are collected and printed in the specified list format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares LMS and RLS algorithms for system identification\n    to analyze early-time convergence behavior.\n    \"\"\"\n    # --- Simulation Parameters ---\n    n = 4\n    T = 200\n    T0 = 30\n    M = 200\n    seed = 2025\n    \n    w_star = np.array([1.0, -0.5, 0.25, -0.125])\n    lambda_vals = np.array([3.0, 1.0, 0.3, 0.1])\n    Lambda = np.diag(lambda_vals)\n    Lambda_sqrt = np.diag(np.sqrt(lambda_vals))\n    \n    mu_lms = 0.05\n    lambda_rls = 0.995\n    delta_values = [1e-3, 1.0, 1e2]\n    \n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate M sets of data sequences, each of length T\n    # x_mc shape: (M, T, n), d_mc shape: (M, T)\n    g_mc = rng.standard_normal(size=(M, T, n))\n    x_mc = np.einsum('ij,ktj->kti', Lambda_sqrt, g_mc)\n    d_mc = np.einsum('kti,i->kt', x_mc, w_star)\n    \n    # --- LMS Simulation and Analysis ---\n    \n    # Store squared modal error for each trial\n    # mse_lms shape: (M, T+1, n)\n    mse_lms = np.zeros((M, T + 1, n))\n    \n    for i in range(M):\n        w = np.zeros(n)\n        initial_error_sq = (w - w_star)**2\n        mse_lms[i, 0, :] = initial_error_sq\n        \n        for t in range(T):\n            xt = x_mc[i, t, :]\n            dt = d_mc[i, t]\n            \n            error_signal = dt - xt.T @ w\n            w = w + mu_lms * xt * error_signal\n            \n            modal_error_sq = (w - w_star)**2\n            mse_lms[i, t + 1, :] = modal_error_sq\n            \n    # Average over Monte Carlo trials to get m_k(t)\n    m_k_lms = np.mean(mse_lms, axis=0) # Shape: (T+1, n)\n    \n    # Check for monotonicity in all modes over t in [0, T0]\n    lms_all_modes_monotone = True\n    for k in range(n):\n        for t in range(1, T0 + 1):\n            if m_k_lms[t, k] > m_k_lms[t-1, k]:\n                # Use a small tolerance for floating point comparisons\n                if not np.isclose(m_k_lms[t, k], m_k_lms[t-1, k]):\n                    lms_all_modes_monotone = False\n                    break\n        if not lms_all_modes_monotone:\n            break\n            \n    # --- RLS Simulation and Analysis ---\n    \n    rls_rho_max_results = []\n    \n    for delta in delta_values:\n        # mse_rls shape: (M, T+1, n)\n        # The first arugment of range for time is 1, so the first update is at index 1\n        # To align with LMS (0 to T), we use slightly different indexing logic from problem\n        # The logic here is: w(t+1) is updated from w(t) using x(t), d(t)\n        \n        mse_rls = np.zeros((M, T + 1, n))\n\n        for i in range(M):\n            w = np.zeros(n)\n            P = (1.0 / delta) * np.identity(n)\n            \n            initial_error_sq = (w - w_star)**2\n            mse_rls[i, 0, :] = initial_error_sq\n            \n            for t in range(T):\n                xt = x_mc[i, t, :]\n                dt = d_mc[i, t]\n                \n                # RLS update equations re-indexed for a causal loop t=0..T-1\n                # producing w(t+1) from w(t), P(t) and x(t),d(t)\n                den = lambda_rls + xt.T @ P @ xt\n                k = (P @ xt) / den\n                \n                error_signal = dt - xt.T @ w\n                w = w + k * error_signal\n                P = (1.0 / lambda_rls) * (P - np.outer(k, xt) @ P)\n\n                modal_error_sq = (w - w_star)**2\n                mse_rls[i, t + 1, :] = modal_error_sq\n\n        # Average over Monte Carlo trials\n        m_k_rls = np.mean(mse_rls, axis=0) # Shape: (T+1, n)\n        \n        # Calculate overshoot factor rho_k for each mode\n        rho_k_list = []\n        for k in range(n):\n            m_k_0 = (w_star[k])**2\n            if m_k_0 == 0:\n                # If initial error is zero, any deviation is infinite overshoot.\n                # Avoid division by zero. This shouldn't happen with the given w_star.\n                max_val = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = np.inf if max_val > 0 else 0.0\n            else:\n                max_m_k_t0 = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = (max_m_k_t0 / m_k_0) - 1.0\n            rho_k_list.append(rho_k)\n            \n        rho_max = np.max(rho_k_list)\n        rls_rho_max_results.append(rho_max)\n        \n    # --- Final Output ---\n    final_results = rls_rho_max_results + [lms_all_modes_monotone]\n    print(f\"[{final_results[0]},{final_results[1]},{final_results[2]},{final_results[3]}]\")\n\nsolve()\n```", "id": "2891050"}]}