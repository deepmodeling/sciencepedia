{"hands_on_practices": [{"introduction": "The foundation of optimal linear estimation is the Wiener filter. This exercise provides a concrete opportunity to apply the orthogonality principle, the theoretical bedrock of our topic, to derive the optimal filter coefficients and the resulting minimum mean-square error ($MSE$) for a specific statistical scenario. Solving this problem solidifies the connection between the abstract principle of orthogonality and the concrete calculation of the Wiener-Hopf equations [@problem_id:2850274].", "problem": "Consider a real-valued, zero-mean, jointly wide-sense stationary (WSS) pair of random vectors and processes where the goal is to estimate a scalar desired random variable $d$ from a two-dimensional regressor $\\mathbf{x} \\in \\mathbb{R}^{2}$ using a linear estimator $y = \\mathbf{w}^{\\top}\\mathbf{x}$. Let the mean-square error (MSE) cost be defined by\n$$\nJ(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right],\n$$\nand define the input correlation matrix, cross-correlation vector, and desired power as\n$$\n\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}], \\quad \\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d], \\quad \\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}],\n$$\nrespectively. Using only fundamental definitions and the orthogonality principle, derive the normal equations that the optimal Wiener solution $\\mathbf{w}^{\\star}$ must satisfy, solve explicitly for $\\mathbf{w}^{\\star}$ given\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n$$\nand compute the minimum achievable MSE,\n$$\nJ_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w}),\n$$\nin closed form as a function of $\\sigma_{d}^{2}$. Provide your final answer as a single row containing the two components of $\\mathbf{w}^{\\star}$ followed by $J_{\\min}$, in exact form with no rounding. The answer is unitless and no angle units are involved.", "solution": "The problem is subjected to rigorous validation before any attempt at a solution is made.\n\nThe givens are extracted verbatim from the problem statement:\n- A real-valued, zero-mean scalar random variable $d$.\n- A real-valued, zero-mean two-dimensional random vector $\\mathbf{x} \\in \\mathbb{R}^{2}$.\n- The pair $(d, \\mathbf{x})$ is jointly wide-sense stationary (WSS).\n- The linear estimator is $y = \\mathbf{w}^{\\top}\\mathbf{x}$.\n- The mean-square error (MSE) cost function is $J(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right]$.\n- The input correlation matrix is $\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$.\n- The cross-correlation vector is $\\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d]$.\n- The power of the desired signal is $\\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}]$.\n- The specific values for the correlation matrix and cross-correlation vector are:\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\n- The objective is to derive the normal equations, find the optimal Wiener solution $\\mathbf{w}^{\\star}$, and compute the minimum MSE, $J_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w})$.\n\nThe problem is evaluated for validity. It is a standard problem in linear estimation theory, specifically Wiener filtering. All terms are well-defined, and the context is firmly rooted in the field of signal processing. The given correlation matrix $\\mathbf{R}$ is symmetric and its determinant is $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 3 \\neq 0$, which ensures that $\\mathbf{R}$ is invertible and a unique solution for the optimal weight vector $\\mathbf{w}^{\\star}$ exists. The problem is self-contained, scientifically grounded, and well-posed. Therefore, the problem is deemed valid. We proceed with the solution.\n\nThe mean-square error is given by $J(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2]$. We seek the optimal weight vector $\\mathbf{w}^{\\star}$ that minimizes this cost function. The estimation error is defined as $e = d - y = d - \\mathbf{w}^{\\top}\\mathbf{x}$. For the optimal filter, $\\mathbf{w} = \\mathbf{w}^{\\star}$, the orthogonality principle states that the estimation error $e^{\\star} = d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ must be orthogonal to the input vector $\\mathbf{x}$. Mathematically, this is expressed as the expectation of the product of the error and the input vector being zero.\n$$\n\\mathbb{E}[\\mathbf{x} e^{\\star}] = \\mathbf{0}\n$$\nSubstituting the expression for the error $e^{\\star}$:\n$$\n\\mathbb{E}[\\mathbf{x} (d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x})] = \\mathbf{0}\n$$\nUsing the linearity of the expectation operator:\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}] = \\mathbf{0}\n$$\nThe term $(\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ is a scalar, so it can be reordered: $\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x} = \\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{w}^{\\star}$. The weight vector $\\mathbf{w}^{\\star}$ is deterministic with respect to the expectation.\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\nUsing the provided definitions $\\mathbf{p} = \\mathbb{E}[\\mathbf{x}d]$ and $\\mathbf{R} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$, we arrive at the normal equations, also known as the Wiener-Hopf equations for this discrete case:\n$$\n\\mathbf{p} - \\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\n$$\n\\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{p}\n$$\nThis is the set of linear equations that the optimal Wiener solution $\\mathbf{w}^{\\star}$ must satisfy.\n\nTo find the explicit solution for $\\mathbf{w}^{\\star}$, we must solve this system of equations. Since $\\mathbf{R}$ is invertible, the unique solution is given by:\n$$\n\\mathbf{w}^{\\star} = \\mathbf{R}^{-1} \\mathbf{p}\n$$\nWe are given the matrices:\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\nFirst, we compute the inverse of $\\mathbf{R}$. For a $2 \\times 2$ matrix $\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{bmatrix}d & -b \\\\ -c & a\\end{bmatrix}$. The determinant of $\\mathbf{R}$ is $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 4 - 1 = 3$.\nThe inverse is therefore:\n$$\n\\mathbf{R}^{-1} = \\frac{1}{3}\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}\n$$\nNow we can compute $\\mathbf{w}^{\\star}$:\n$$\n\\mathbf{w}^{\\star} = \\frac{1}{3}\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}(2)(1) + (-1)(0) \\\\ (-1)(1) + (2)(0)\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}2/3 \\\\ -1/3\\end{bmatrix}\n$$\nSo, the components of the optimal weight vector are $w_1^{\\star} = 2/3$ and $w_2^{\\star} = -1/3$.\n\nNext, we compute the minimum achievable MSE, $J_{\\min} = J(\\mathbf{w}^{\\star})$. We start with the definition of $J(\\mathbf{w})$ and expand it:\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[d^2 - 2d\\mathbf{w}^{\\top}\\mathbf{x} + (\\mathbf{w}^{\\top}\\mathbf{x})^2]\n$$\nBy linearity of expectation:\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[d^2] - 2\\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}d] + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\sigma_{d}^{2} - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w}\n$$\nTo find $J_{\\min}$, we substitute $\\mathbf{w} = \\mathbf{w}^{\\star}$:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{R}\\mathbf{w}^{\\star}\n$$\nFrom the normal equations, we know $\\mathbf{R}\\mathbf{w}^{\\star} = \\mathbf{p}$. Substituting this into the last term:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\nThis simplifies to a general expression for the minimum MSE:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\nNow, we substitute the numerical values we found for $\\mathbf{w}^{\\star}$ and the given $\\mathbf{p}$:\n$$\n(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} = \\begin{bmatrix}2/3 & -1/3\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = (2/3)(1) + (-1/3)(0) = 2/3\n$$\nTherefore, the minimum MSE is:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - \\frac{2}{3}\n$$\nThe final answer comprises the two components of $\\mathbf{w}^{\\star}$ and the expression for $J_{\\min}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & \\sigma_{d}^{2} - \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "2850274"}, {"introduction": "While the Wiener filter provides the optimal static solution, most real-world applications require filters that can adapt in real time. This practice delves into the performance of the celebrated Least Mean Squares (LMS) algorithm, requiring a derivation of its learning curve from first principles. By analyzing the transient and steady-state Excess Mean-Square Error (EMSE), you will gain a deep, quantitative understanding of how the algorithm learns and the fundamental trade-offs between convergence speed and final misadjustment [@problem_id:2850271].", "problem": "Consider an unknown finite impulse response system of length $M$ with coefficient vector $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{M}$. The input regression vector is $\\mathbf{x}(n) \\in \\mathbb{R}^{M}$, and the measured desired response is $d(n) = \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)$, where $v(n)$ is zero-mean measurement noise. Assume the following data model and statistical properties:\n- $\\mathbf{x}(n)$ is zero-mean, white, Gaussian with covariance $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$ and independent across time.\n- $v(n)$ is zero-mean, white, Gaussian with variance $\\sigma_{v}^{2}$, independent of $\\mathbf{x}(n)$ for all $n$.\n- The initial weight vector of the adaptive filter is $\\mathbf{w}(0)$, and the Least Mean Squares (LMS) update is given by $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\mathbf{x}(n)e(n)$ with $e(n) \\triangleq d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n)$ and step size $\\mu$ chosen to ensure mean-square stability.\n\nDefine the weight-error vector $\\widetilde{\\mathbf{w}}(n) \\triangleq \\mathbf{w}^{\\star} - \\mathbf{w}(n)$ and the Excess Mean-Square Error (EMSE) as $\\xi_{\\mathrm{ex}}(n) \\triangleq \\mathbb{E}\\!\\left[e^{2}(n)\\right] - \\xi_{\\min}$, where $\\xi_{\\min}$ is the minimum mean-square error characterized by the Orthogonality Principle of Wiener filtering. Work from first principles, using only the LMS update, the data model, the Orthogonality Principle, and standard properties of expectations and Gaussian moment factoring, to obtain a closed-form expression for the transient EMSE $\\xi_{\\mathrm{ex}}(n)$ as a function of the nonnegative integer $n$, and its steady-state limit as $n \\to \\infty$. Your final expression must be in terms of $\\mu$, $\\sigma_{x}^{2}$, $\\sigma_{v}^{2}$, $M$, and the initial EMSE $\\xi_{\\mathrm{ex}}(0)$, and it must be valid under a mean-square stability step-size condition that you should state and enforce in your derivation. Provide the exact analytic expressions; no approximations are allowed. The final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement as presented is valid. It is scientifically grounded in the established theory of adaptive signal processing, well-posed with a unique and meaningful solution derivable from the provided information, and is expressed with objective, formal language. It is a standard, non-trivial problem in the analysis of the Least Mean Squares (LMS) algorithm. We will proceed with a rigorous derivation from first principles.\n\nThe derivation is structured as follows: First, we establish the value of the minimum mean-square error, $\\xi_{\\min}$, by applying the Orthogonality Principle. Second, we express the Excess Mean-Square Error (EMSE), $\\xi_{\\mathrm{ex}}(n)$, in terms of the weight-error vector, $\\widetilde{\\mathbf{w}}(n)$. Third, we derive the recursion for the weight-error covariance matrix, $\\mathbf{K}(n) \\triangleq \\mathbb{E}[\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)]$. This step critically relies on the specified data model and properties of Gaussian random vectors. Fourth, from the matrix recursion, we derive a scalar recursion for $\\xi_{\\mathrm{ex}}(n)$. Finally, we solve this linear first-order difference equation to find the closed-form expressions for the transient EMSE, $\\xi_{\\mathrm{ex}}(n)$, and its steady-state limit, $\\xi_{\\mathrm{ex}}(\\infty)$.\n\nFirst, we determine the minimum mean-square error, $\\xi_{\\min}$. The optimal Wiener filter, $\\mathbf{w}_{\\mathrm{o}}$, minimizes the cost function $\\xi = \\mathbb{E}[e^{2}(n)]$. The Orthogonality Principle states that the error signal corresponding to the optimal filter, $e_{\\mathrm{o}}(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}_{\\mathrm{o}}$, must be orthogonal to the input regression vector, $\\mathbf{x}(n)$.\n$$\n\\mathbb{E}[\\mathbf{x}(n)e_{\\mathrm{o}}(n)] = \\mathbf{0}\n$$\nSubstituting the expression for $d(n) = \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)$:\n$$\n\\mathbb{E}[\\mathbf{x}(n) ( \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}_{\\mathrm{o}} )] = \\mathbf{0}\n$$\nBy linearity of expectation, and using the fact that $v(n)$ is zero-mean and independent of $\\mathbf{x}(n)$ so that $\\mathbb{E}[\\mathbf{x}(n)v(n)]=\\mathbf{0}$:\n$$\n\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)](\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}}) = \\mathbf{0}\n$$\n$$\n\\mathbf{R}(\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}}) = \\mathbf{0}\n$$\nGiven $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$ with $\\sigma_{x}^{2} > 0$, the matrix $\\mathbf{R}$ is invertible, which implies $\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}} = \\mathbf{0}$, or $\\mathbf{w}_{\\mathrm{o}} = \\mathbf{w}^{\\star}$. The optimal filter is the true system. The minimum mean-square error is then the error achieved by this optimal filter:\n$$\n\\xi_{\\min} = \\mathbb{E}[e_{\\mathrm{o}}^{2}(n)] = \\mathbb{E}[(d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star})^{2}] = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star})^{2}] = \\mathbb{E}[v^{2}(n)] = \\sigma_{v}^{2}\n$$\nNext, we express the EMSE in terms of the weight-error vector $\\widetilde{\\mathbf{w}}(n) = \\mathbf{w}^{\\star} - \\mathbf{w}(n)$. The error signal is $e(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n)$.\n$$\ne(n) = (\\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) = \\mathbf{x}^{\\top}(n)(\\mathbf{w}^{\\star} - \\mathbf{w}(n)) + v(n) = \\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n)\n$$\nThe mean-square error a time $n$ is $\\xi(n) = \\mathbb{E}[e^{2}(n)]$.\n$$\n\\xi(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n))^{2}] = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] + 2\\mathbb{E}[v(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] + \\mathbb{E}[v^{2}(n)]\n$$\nThe weight-error vector $\\widetilde{\\mathbf{w}}(n)$ is a function of data up to time $n-1$. Since $v(n)$ is white and independent of $\\mathbf{x}(k)$ for all $k$, $v(n)$ is independent of $\\widetilde{\\mathbf{w}}(n)$. As $v(n)$ is zero-mean, the cross-term vanishes: $\\mathbb{E}[v(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = \\mathbb{E}[v(n)]\\mathbb{E}[\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = 0$.\n$$\n\\xi(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] + \\sigma_{v}^{2}\n$$\nThe EMSE is defined as $\\xi_{\\mathrm{ex}}(n) \\triangleq \\xi(n) - \\xi_{\\min} = \\xi(n) - \\sigma_{v}^{2}$.\n$$\n\\xi_{\\mathrm{ex}}(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)]\n$$\nSince the input vector $\\mathbf{x}(n)$ is independent of past data, it is independent of $\\widetilde{\\mathbf{w}}(n)$. We can take the expectation with respect to $\\mathbf{x}(n)$ first.\n$$\n\\xi_{\\mathrm{ex}}(n) = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n) \\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] \\widetilde{\\mathbf{w}}(n)] = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\mathbf{R}\\widetilde{\\mathbf{w}}(n)] = \\sigma_{x}^{2}\\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = \\sigma_{x}^{2}\\mathbb{E}[\\|\\widetilde{\\mathbf{w}}(n)\\|^{2}]\n$$\nLet the weight-error covariance matrix be $\\mathbf{K}(n) \\triangleq \\mathbb{E}[\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)]$. Then $\\mathbb{E}[\\|\\widetilde{\\mathbf{w}}(n)\\|^{2}] = \\mathrm{Tr}(\\mathbf{K}(n))$. Thus,\n$$\n\\xi_{\\mathrm{ex}}(n) = \\sigma_{x}^{2}\\mathrm{Tr}(\\mathbf{K}(n))\n$$\nNow we find the recursion for $\\mathbf{K}(n)$. Starting from the LMS update rule, we derive the recursion for $\\widetilde{\\mathbf{w}}(n)$:\n$$\n\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu\\mathbf{x}(n)e(n) \\implies \\mathbf{w}^{\\star} - \\mathbf{w}(n+1) = \\mathbf{w}^{\\star} - \\mathbf{w}(n) - \\mu\\mathbf{x}(n)e(n)\n$$\n$$\n\\widetilde{\\mathbf{w}}(n+1) = \\widetilde{\\mathbf{w}}(n) - \\mu\\mathbf{x}(n)[\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n)] = [\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\widetilde{\\mathbf{w}}(n) - \\mu v(n)\\mathbf{x}(n)\n$$\nThen, we compute $\\mathbf{K}(n+1) = \\mathbb{E}[\\widetilde{\\mathbf{w}}(n+1)\\widetilde{\\mathbf{w}}^{\\top}(n+1)]$. The cross-terms involving $v(n)$ vanish upon taking expectation because $v(n)$ is zero-mean and independent of $\\mathbf{x}(n)$ and $\\widetilde{\\mathbf{w}}(n)$.\n$$\n\\mathbf{K}(n+1) = \\mathbb{E}\\left[[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]^{\\top}\\right] + \\mathbb{E}[\\mu^{2}v^{2}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\n$$\nThe second term is $\\mu^{2}\\mathbb{E}[v^{2}(n)]\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] = \\mu^{2}\\sigma_{v}^{2}\\mathbf{R} = \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}$. For the first term, we use the independence of $\\mathbf{x}(n)$ and $\\widetilde{\\mathbf{w}}(n)$ again:\n$$\n\\mathbb{E}[\\dots] = \\mathbb{E}_{\\mathbf{x}}\\left[[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\mathbf{K}(n)[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\right]\n$$\n$$\n= \\mathbf{K}(n) - \\mu\\mathbf{R}\\mathbf{K}(n) - \\mu\\mathbf{K}(n)\\mathbf{R} + \\mu^{2}\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{K}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\n$$\nFor a real, zero-mean Gaussian vector $\\mathbf{x}$ with covariance $\\mathbf{R}$ and a symmetric matrix $\\mathbf{A}$, the fourth-order moment is given by $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}\\mathbf{x}^{\\top}] = \\mathbf{R}\\mathrm{Tr}(\\mathbf{A}\\mathbf{R}) + 2\\mathbf{R}\\mathbf{A}\\mathbf{R}$.\nHere, $\\mathbf{A} = \\mathbf{K}(n)$ and $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$.\n$$\n\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{K}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] = (\\sigma_{x}^{2}\\mathbf{I}_{M})\\mathrm{Tr}(\\mathbf{K}(n)\\sigma_{x}^{2}\\mathbf{I}_{M}) + 2(\\sigma_{x}^{2}\\mathbf{I}_{M})\\mathbf{K}(n)(\\sigma_{x}^{2}\\mathbf{I}_{M}) = \\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + 2\\sigma_{x}^{4}\\mathbf{K}(n)\n$$\nSubstituting this back into the recursion for $\\mathbf{K}(n+1)$:\n$$\n\\mathbf{K}(n+1) = \\mathbf{K}(n) - 2\\mu\\sigma_{x}^{2}\\mathbf{K}(n) + \\mu^{2}[ \\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + 2\\sigma_{x}^{4}\\mathbf{K}(n) ] + \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}\n$$\n$$\n\\mathbf{K}(n+1) = (1 - 2\\mu\\sigma_{x}^{2} + 2\\mu^{2}\\sigma_{x}^{4})\\mathbf{K}(n) + \\mu^{2}\\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}\n$$\nTo obtain a scalar recursion, we take the trace of this equation using $\\mathrm{Tr}(\\mathbf{I}_{M})=M$:\n$$\n\\mathrm{Tr}(\\mathbf{K}(n+1)) = (1 - 2\\mu\\sigma_{x}^{2} + 2\\mu^{2}\\sigma_{x}^{4})\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\n$$\n\\mathrm{Tr}(\\mathbf{K}(n+1)) = (1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4})\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\nNow, substitute $\\mathrm{Tr}(\\mathbf{K}(n)) = \\xi_{\\mathrm{ex}}(n)/\\sigma_{x}^{2}$:\n$$\n\\frac{\\xi_{\\mathrm{ex}}(n+1)}{\\sigma_{x}^{2}} = (1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4})\\frac{\\xi_{\\mathrm{ex}}(n)}{\\sigma_{x}^{2}} + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\n$$\n\\xi_{\\mathrm{ex}}(n+1) = \\alpha \\cdot \\xi_{\\mathrm{ex}}(n) + \\beta\n$$\nwhere $\\alpha \\triangleq 1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}$ and $\\beta \\triangleq M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{4}$.\n\nFor mean-square stability, the magnitude of the coefficient $\\alpha$ must be less than $1$.\n$$\n-1 < 1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} < 1\n$$\nThe right inequality, $1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} < 1$, gives $\\mu\\sigma_{x}^{2}((M+2)\\mu\\sigma_{x}^{2}-2) < 0$. Since $\\mu\\sigma_{x}^{2} > 0$, this requires $(M+2)\\mu\\sigma_{x}^{2}-2 < 0$, or $\\mu < \\frac{2}{(M+2)\\sigma_{x}^{2}}$. The left inequality, $-1 < 1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}$, is always satisfied for any real $\\mu$ since the quadratic in $\\mu$ has a negative discriminant. The stability condition is therefore:\n$$\n0 < \\mu < \\frac{2}{(M+2)\\sigma_{x}^{2}}\n$$\nWe solve the difference equation $\\xi_{\\mathrm{ex}}(n+1) = \\alpha \\xi_{\\mathrm{ex}}(n) + \\beta$. The solution is of the form $\\xi_{\\mathrm{ex}}(n) = \\alpha^{n}(\\xi_{\\mathrm{ex}}(0) - \\xi_{\\mathrm{ex}}(\\infty)) + \\xi_{\\mathrm{ex}}(\\infty)$, where $\\xi_{\\mathrm{ex}}(\\infty)$ is the steady-state value.\n$$\n\\xi_{\\mathrm{ex}}(\\infty) = \\lim_{n\\to\\infty} \\xi_{\\mathrm{ex}}(n) = \\frac{\\beta}{1-\\alpha}\n$$\n$$\n1-\\alpha = 2\\mu\\sigma_{x}^{2} - (M+2)\\mu^{2}\\sigma_{x}^{4} = \\mu\\sigma_{x}^{2}(2 - (M+2)\\mu\\sigma_{x}^{2})\n$$\n$$\n\\xi_{\\mathrm{ex}}(\\infty) = \\frac{M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{4}}{\\mu\\sigma_{x}^{2}(2 - (M+2)\\mu\\sigma_{x}^{2})} = \\frac{M\\mu\\sigma_{v}^{2}\\sigma_{x}^{2}}{2 - (M+2)\\mu\\sigma_{x}^{2}}\n$$\nThis is the steady-state Excess Mean-Square Error. The transient EMSE is then given by the full solution to the difference equation, expressed in terms of the given parameters and the initial condition $\\xi_{\\mathrm{ex}}(0)$.\n\nThe final closed-form expression for the transient EMSE is:\n$$\n\\xi_{\\mathrm{ex}}(n) = \\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\xi_{\\mathrm{ex}}(\\infty) \\right) + \\xi_{\\mathrm{ex}}(\\infty)\n$$\nSubstituting the expression for $\\xi_{\\mathrm{ex}}(\\infty)$:\n$$\n\\xi_{\\mathrm{ex}}(n) = \\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}} \\right) + \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}}\n$$\nThis expression is valid under the mean-square stability condition $0 < \\mu < \\frac{2}{(M+2)\\sigma_{x}^{2}}$. As $n \\to \\infty$, since $|\\alpha| < 1$, the first term decays to zero, leaving $\\xi_{\\mathrm{ex}}(n) \\to \\xi_{\\mathrm{ex}}(\\infty)$.", "answer": "$$\n\\boxed{\\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}} \\right) + \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}}}\n$$", "id": "2850271"}, {"introduction": "The theoretical performance of an adaptive filter often depends on idealized assumptions, such as a white input signal. This exercise confronts a more realistic scenario by investigating how the convergence rate of the LMS algorithm is affected by input signal correlation, quantified by the condition number $\\kappa$ of the input covariance matrix. By working through this problem, you will uncover one of the most critical limitations of the standard LMS algorithm and appreciate why the structure of the input data is paramount in practical system design [@problem_id:2850237].", "problem": "Consider a linear system identification problem with an unknown vector $w^{\\star} \\in \\mathbb{R}^{m}$, driven by a zero-mean, wide-sense stationary input $x(n) \\in \\mathbb{R}^{m}$ with positive-definite correlation matrix $R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\}$. The desired response is $d(n) = x^{\\top}(n) w^{\\star} + v(n)$, where $v(n)$ is zero-mean, white, and independent of $x(n)$. The Least-Mean-Squares (LMS) algorithm updates $w(n)$ according to $w(n+1) = w(n) + \\mu\\, x(n)\\, e(n)$ with $e(n) \\triangleq d(n) - x^{\\top}(n) w(n)$. Let the step-size be chosen as $\\mu = \\beta / \\lambda_{\\max}$ with a fixed $\\beta \\in (0,2)$, where $\\lambda_{\\max}$ is the largest eigenvalue of $R$. The condition number of $R$ is $\\kappa \\triangleq \\lambda_{\\max} / \\lambda_{\\min}$, where $\\lambda_{\\min}$ is the smallest eigenvalue.\n\nUsing only fundamental definitions (including the orthogonality principle stating that the Wiener solution $w^{\\star}$ makes the optimal error orthogonal to the regressors) and standard small-step-size and independence assumptions, derive from first principles how the transient mean-square deviation decays along the eigen-directions of $R$. Define a target mean-square deviation reduction factor $\\rho \\in (0,1)$ to be achieved by the transient (prior to the steady-state floor dominated by gradient noise), and let $n(\\kappa,\\rho,\\beta)$ denote the number of iterations required to achieve this reduction when the input has condition number $\\kappa$ under the above step-size rule.\n\nBy “how many more iterations,” interpret the question as the multiplicative factor $F(\\kappa,\\beta) \\triangleq \\dfrac{n(\\kappa,\\rho,\\beta)}{n(1,\\rho,\\beta)}$, which is required to achieve the same fixed reduction factor $\\rho$ when moving from $\\kappa = 1$ (white input) to a colored input with general $\\kappa$. Under the small-$\\beta$ regime, estimate $F(\\kappa,\\beta)$ in closed form and then evaluate it at $\\kappa = 100$. Provide the final answer as a single number with no units. No rounding is required.", "solution": "The problem statement must first be validated for scientific soundness, consistency, and completeness.\n\nStep 1: Extract Givens.\n-   Unknown linear system: A vector $w^{\\star} \\in \\mathbb{R}^{m}$.\n-   Input signal: $x(n) \\in \\mathbb{R}^{m}$, a zero-mean, wide-sense stationary (WSS) process.\n-   Input correlation matrix: $R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\}$, which is positive-definite.\n-   Eigenvalues of $R$: $\\lambda_{\\max}$ (largest) and $\\lambda_{\\min}$ (smallest).\n-   Desired response: $d(n) = x^{\\top}(n) w^{\\star} + v(n)$.\n-   Noise signal: $v(n)$, a zero-mean, white process, independent of $x(n)$.\n-   LMS algorithm: $w(n+1) = w(n) + \\mu\\, x(n)\\, e(n)$.\n-   Estimation error: $e(n) = d(n) - x^{\\top}(n) w(n)$.\n-   Step-size: $\\mu = \\beta / \\lambda_{\\max}$ for a constant $\\beta \\in (0,2)$.\n-   Condition number: $\\kappa \\triangleq \\lambda_{\\max} / \\lambda_{\\min}$.\n-   Assumptions: Small-step-size and independence assumptions. The independence assumption refers to the standard simplification in LMS analysis where the input vector $x(n)$ is assumed to be statistically independent of the past weight vectors $\\{w(k) | k \\leq n\\}$.\n-   Objective: Derive the decay of transient mean-square deviation along eigen-directions, define a multiplicative factor for the number of iterations $F(\\kappa,\\beta) \\triangleq n(\\kappa,\\rho,\\beta)/n(1,\\rho,\\beta)$, estimate this factor in the small-$\\beta$ regime, and evaluate for $\\kappa=100$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is a canonical exercise in the analysis of the LMS adaptive algorithm, a cornerstone of signal processing. All definitions and models are standard. The parameters are well-defined and consistent. The assumptions (small step-size, independence) are explicitly stated and are standard in introductory analyses of the algorithm's dynamics. The objective is clear and requires a derivation based on fundamental principles of adaptive filter theory. The problem is scientifically grounded, well-posed, objective, and contains no discernible flaws.\n\nStep 3: Verdict and Action.\nThe problem is valid. A rigorous solution will be constructed.\n\nThe analysis begins by defining the weight error vector, $\\tilde{w}(n) \\triangleq w(n) - w^{\\star}$. We derive its dynamics.\nSubstituting the definitions for $e(n)$ and $d(n)$ into the LMS update rule:\n$$\nw(n+1) = w(n) + \\mu x(n) [ (x^{\\top}(n) w^{\\star} + v(n)) - x^{\\top}(n) w(n) ]\n$$\n$$\nw(n+1) = w(n) + \\mu x(n) [ v(n) - x^{\\top}(n) (w(n) - w^{\\star}) ]\n$$\nSubtracting the optimal weight vector $w^{\\star}$ from both sides yields the update equation for the weight error vector:\n$$\n\\tilde{w}(n+1) = \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) + \\mu x(n) v(n)\n$$\nTo analyze the transient behavior, we study the evolution of the weight error correlation matrix, $K(n) \\triangleq \\mathbb{E}\\{\\tilde{w}(n) \\tilde{w}^{\\top}(n)\\}$. We take the expectation of the outer product of the error vector update equation with itself. The cross-term involving $v(n)$ vanishes upon expectation because $v(n)$ is zero-mean and independent of all other signals at time $n$ and earlier.\n$$\nK(n+1) = \\mathbb{E}\\{ [ \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) ] [ \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) ]^{\\top} \\} + \\mu^2 \\mathbb{E}\\{ v^2(n) x(n) x^{\\top}(n) \\}\n$$\nThe second term is the gradient noise, which determines the steady-state mean-square error: $\\mu^2 \\sigma_v^2 R$, where $\\sigma_v^2 = \\mathbb{E}\\{v^2(n)\\}$. The transient behavior is governed by the first term. Expanding it gives:\n$$\nK(n+1)_{\\text{transient}} = \\mathbb{E}\\{ \\tilde{w}(n)\\tilde{w}^{\\top}(n) - \\mu \\tilde{w}(n)\\tilde{w}^{\\top}(n)x(n)x^{\\top}(n) - \\mu x(n)x^{\\top}(n)\\tilde{w}(n)\\tilde{w}^{\\top}(n) + \\mu^2 x(n)x^{\\top}(n)\\tilde{w}(n)\\tilde{w}^{\\top}(n)x(n)x^{\\top}(n) \\}\n$$\nUsing the independence assumption (that $x(n)$ is independent of $\\tilde{w}(n)$) and neglecting the term of order $\\mu^2$, which is valid for small step-sizes, we get a simplified recursion:\n$$\nK(n+1) \\approx K(n) - \\mu K(n) R - \\mu R K(n)\n$$\nThis first-order approximation describes the transient evolution of the weight error correlation matrix. To analyze the decay along the eigen-directions of $R$, we perform a change of coordinates. Let $R = Q \\Lambda Q^{\\top}$ be the eigendecomposition of $R$, where $Q$ is the orthogonal matrix of eigenvectors and $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_m)$ is the diagonal matrix of corresponding eigenvalues.\nLet us define the transformed correlation matrix $K'(n) \\triangleq Q^{\\top} K(n) Q$. Pre- and post-multiplying the recursion by $Q^{\\top}$ and $Q$ respectively:\n$$\nQ^{\\top}K(n+1)Q \\approx Q^{\\top}K(n)Q - \\mu (Q^{\\top}K(n)Q)(Q^{\\top}RQ) - \\mu (Q^{\\top}RQ)(Q^{\\top}K(n)Q)\n$$\n$$\nK'(n+1) \\approx K'(n) - \\mu K'(n) \\Lambda - \\mu \\Lambda K'(n)\n$$\nSince $\\Lambda$ is diagonal, this matrix equation decouples for the diagonal elements of $K'(n)$. Let $k'_i(n) \\triangleq [K'(n)]_{ii}$ be the $i$-th diagonal element, which represents the mean-square value of the weight error along the $i$-th eigenvector direction. The recursion for $k'_i(n)$ is:\n$$\nk'_i(n+1) \\approx k'_i(n) - \\mu k'_i(n) \\lambda_i - \\mu \\lambda_i k'_i(n) = (1 - 2\\mu\\lambda_i)k'_i(n)\n$$\nThis is a simple geometric progression. Its solution is:\n$$\nk'_i(n) \\approx (1 - 2\\mu\\lambda_i)^n k'_i(0)\n$$\nThe convergence of the overall algorithm is limited by the slowest mode of decay. The decay factor for mode $i$ is $(1 - 2\\mu\\lambda_i)$. Since $\\mu > 0$ and all $\\lambda_i > 0$, the slowest convergence occurs for the mode with the decay factor closest to $1$, which corresponds to the smallest eigenvalue, $\\lambda_{\\min}$.\nThe number of iterations $n$ required to achieve a reduction factor $\\rho$ in the transient component of this slowest mode is given by:\n$$\n(1 - 2\\mu\\lambda_{\\min})^n = \\rho\n$$\nSolving for $n$ gives:\n$$\nn = \\frac{\\ln(\\rho)}{\\ln(1 - 2\\mu\\lambda_{\\min})}\n$$\nThe problem specifies the step-size rule $\\mu = \\beta / \\lambda_{\\max}$ and the condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. Substituting these into the expression for $n$ yields the number of iterations as a function of these parameters:\n$$\nn(\\kappa, \\rho, \\beta) = \\frac{\\ln(\\rho)}{\\ln(1 - 2 (\\frac{\\beta}{\\lambda_{\\max}}) \\lambda_{\\min})} = \\frac{\\ln(\\rho)}{\\ln(1 - \\frac{2\\beta}{\\kappa})}\n$$\nFor the reference case of a white input, $\\kappa=1$. This implies $\\lambda_{\\min} = \\lambda_{\\max}$. The number of iterations becomes:\n$$\nn(1, \\rho, \\beta) = \\frac{\\ln(\\rho)}{\\ln(1 - 2\\beta)}\n$$\nThe multiplicative factor $F(\\kappa, \\beta)$ is the ratio of these two quantities:\n$$\nF(\\kappa, \\beta) = \\frac{n(\\kappa, \\rho, \\beta)}{n(1, \\rho, \\beta)} = \\frac{\\ln(\\rho) / \\ln(1 - 2\\beta/\\kappa)}{\\ln(\\rho) / \\ln(1 - 2\\beta)} = \\frac{\\ln(1 - 2\\beta)}{\\ln(1 - 2\\beta/\\kappa)}\n$$\nThe problem asks for an estimate of this factor in the \"small-$\\beta$ regime\". This condition implies that $\\beta \\ll 1$. If $\\beta$ is small, then the arguments of the logarithm functions, $2\\beta$ and $2\\beta/\\kappa$ (since $\\kappa \\ge 1$), are also small. We can therefore use the first-order Taylor approximation $\\ln(1-x) \\approx -x$ for small $x$.\nApplying this approximation to both the numerator and the denominator:\n$$\nF(\\kappa, \\beta) \\approx \\frac{-2\\beta}{-2\\beta/\\kappa} = \\kappa\n$$\nThis is the required estimate. It shows that the convergence time of the LMS algorithm, for a normalized step-size, scales linearly with the condition number of the input correlation matrix.\n\nFinally, we are asked to evaluate this factor at $\\kappa = 100$.\n$$\nF(100, \\beta) \\approx 100\n$$\nThe result is independent of $\\beta$ under this approximation.", "answer": "$$\n\\boxed{100}\n$$", "id": "2850237"}]}