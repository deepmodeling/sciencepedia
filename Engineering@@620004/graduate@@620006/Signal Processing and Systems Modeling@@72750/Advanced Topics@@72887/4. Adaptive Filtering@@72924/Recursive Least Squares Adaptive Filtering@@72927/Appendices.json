{"hands_on_practices": [{"introduction": "This exercise gets to the heart of the RLS algorithm by focusing on a single, concrete update step. By manually calculating the new parameter estimate $\\hat{\\theta}_k$ from the previous estimate $\\hat{\\theta}_{k-1}$ and a new piece of data, you will gain a tangible understanding of the core mechanics. This practice solidifies the role of the gain vector, the *a priori* error, and the inverse covariance matrix in refining the estimate [@problem_id:2899717].", "id": "2899717", "problem": "Consider a linear-in-parameters model where the measured scalar output at discrete time index $k$ is modeled as $y_k \\approx \\varphi_k^{\\top} \\theta$, with a time-varying regressor vector $\\varphi_k \\in \\mathbb{R}^n$ and an unknown parameter vector $\\theta \\in \\mathbb{R}^n$. Define the exponentially weighted least squares cost at time $k$ by\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi_i^{\\top}\\theta\\big)^2,\n$$\nwhere the forgetting factor satisfies $0<\\lambda\\leq 1$. Let the weighted normal-equation matrix and cross-correlation vector be\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}, \\qquad r_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i,\n$$\nso that the minimizer satisfies $R_k \\hat{\\theta}_k = r_k$. Let $P_k \\triangleq R_k^{-1}$, and assume $R_k$ is nonsingular.\n\nStarting from the cost definition $J_k(\\theta)$ and the normal equations, derive the one-step recursive least squares update for the parameter estimate $\\hat{\\theta}_k$ in terms of $P_{k-1}$, $\\hat{\\theta}_{k-1}$, $\\varphi_k$, $y_k$, and $\\lambda$. Then evaluate the update numerically for the following data (all quantities are exact):\n- Dimension $n=2$,\n- Forgetting factor $\\lambda = \\tfrac{1}{2}$,\n- Previous inverse normal-equation matrix\n$$\nP_{k-1} = \\begin{pmatrix} 2 & \\tfrac{1}{2} \\\n$$4pt] \\tfrac{1}{2} & 1 \\end{pmatrix},\n$$\n- Previous parameter estimate\n$$\n\\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\n$$4pt] 1 \\end{pmatrix},\n$$\n- Current regressor and output\n$$\n\\varphi_k = \\begin{pmatrix} 1 \\\n$$4pt] 2 \\end{pmatrix}, \\qquad y_k = 5.\n$$\n\nReport the updated parameter estimate vector $\\hat{\\theta}_k$ in its simplest exact form. Do not round. Your final answer must be written as a single row matrix.", "solution": "The problem presented is a standard exercise in adaptive filtering, specifically concerning the derivation and application of the recursive least squares (RLS) algorithm. It is scientifically sound, well-posed, and contains all necessary information. I shall proceed to solve it.\n\nThe problem requires two parts: first, a formal derivation of the one-step update for the parameter estimate $\\hat{\\theta}_k$, and second, a numerical evaluation using the provided data.\n\nFirst, we derive the recursive update for $\\hat{\\theta}_k$. The foundation of the derivation lies in the recursive nature of the normal-equation matrix $R_k$ and the cross-correlation vector $r_k$.\n\nFrom their definitions:\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}\n$$\n$$\nr_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i\n$$\nWe can separate the term for index $k$ from the sum:\n$$\nR_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i \\varphi_i^{\\top} \\right) + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i y_i \\right) + \\varphi_k y_k\n$$\nThis reveals the recursive relationships:\n$$\nR_k = \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\nThe least-squares estimate $\\hat{\\theta}_k$ is the solution to the normal equation $R_k \\hat{\\theta}_k = r_k$. Substituting the recursive expressions, we have:\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\nUsing the previous estimate's normal equation, $r_{k-1} = R_{k-1} \\hat{\\theta}_{k-1}$, we substitute for $r_{k-1}$:\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda R_{k-1} \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\nWe now rearrange this equation to express $\\hat{\\theta}_k$ as a correction to $\\hat{\\theta}_{k-1}$. We can substitute $R_k$ on the left side and rewrite $\\lambda R_{k-1}$ on the right side as $R_k - \\varphi_k \\varphi_k^{\\top}$:\n$$\nR_k \\hat{\\theta}_k = (R_k - \\varphi_k \\varphi_k^{\\top}) \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\nRearranging terms yields:\n$$\nR_k \\hat{\\theta}_k - R_k \\hat{\\theta}_{k-1} = \\varphi_k y_k - \\varphi_k \\varphi_k^{\\top} \\hat{\\theta}_{k-1}\n$$\n$$\nR_k (\\hat{\\theta}_k - \\hat{\\theta}_{k-1}) = \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nLeft-multiplying by $P_k = R_k^{-1}$ gives the update equation:\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + P_k \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nThis equation depends on $P_k$. To express the update in terms of $P_{k-1}$, we must find a recursive formula for $P_k$. Using the recursive formula for $R_k$ and the Sherman-Morrison-Woodbury matrix inversion lemma, we have:\n$$\nP_k = R_k^{-1} = (\\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top})^{-1} = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right)\n$$\nThe gain vector in the update for $\\hat{\\theta}_k$ is $K_k \\triangleq P_k \\varphi_k$. Let's derive its expression in terms of $P_{k-1}$:\n$$\nK_k = P_k \\varphi_k = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right) \\varphi_k\n$$\nLet the scalar term be $s_k = \\varphi_k^{\\top} P_{k-1} \\varphi_k$.\n$$\nK_k = \\frac{1}{\\lambda} \\left( P_{k-1} \\varphi_k - \\frac{(P_{k-1} \\varphi_k) (\\varphi_k^{\\top} P_{k-1} \\varphi_k)}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( 1 - \\frac{s_k}{\\lambda + s_k} \\right)\n$$\n$$\nK_k = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda + s_k - s_k}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda}{\\lambda + s_k} \\right) = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n$$\nSubstituting this result for the gain vector into the update for $\\hat{\\theta}_k$ yields the complete RLS update algorithm based on quantities from time $k-1$:\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nThis completes the derivation.\n\nSecond, we perform the numerical calculation. The given data are:\n$$\n\\lambda = \\frac{1}{2}, \\quad P_{k-1} = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix}, \\quad \\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad \\varphi_k = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad y_k = 5\n$$\nWe proceed step by step.\n\n1.  Calculate the *a priori* estimation error, $e_k = y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1}$:\n    $$\n    \\varphi_k^{\\top} \\hat{\\theta}_{k-1} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (2)(1) = 2\n    $$\n    $$\n    e_k = 5 - 2 = 3\n    $$\n\n2.  Calculate the numerator of the gain vector, $P_{k-1} \\varphi_k$:\n    $$\n    P_{k-1} \\varphi_k = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2(1) + \\frac{1}{2}(2) \\\\ \\frac{1}{2}(1) + 1(2) \\end{pmatrix} = \\begin{pmatrix} 2 + 1 \\\\ \\frac{1}{2} + 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}\n    $$\n\n3.  Calculate the denominator of the gain vector, $\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k$:\n    $$\n    \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\varphi_k^{\\top} (P_{k-1} \\varphi_k) = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = (1)(3) + (2)\\left(\\frac{5}{2}\\right) = 3 + 5 = 8\n    $$\n    $$\n    \\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\frac{1}{2} + 8 = \\frac{17}{2}\n    $$\n\n4.  Calculate the gain vector, $K_k$:\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} = \\frac{\\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}}{\\frac{17}{2}} = \\frac{2}{17} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix}\n    $$\n\n5.  Finally, compute the updated parameter estimate, $\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$:\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix} (3) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{15}{17} \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} \\frac{18}{17} \\\\ 1 + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{17}{17} + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{32}{17} \\end{pmatrix}\n    $$\nThe updated parameter estimate vector is $\\hat{\\theta}_k$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{18}{17} & \\frac{32}{17} \\end{pmatrix}\n}\n$$"}, {"introduction": "Moving from a single calculation to a full simulation, this practice challenges you to implement the RLS algorithm to identify the parameters of an unknown system. You will generate synthetic data from an Autoregressive with Exogenous input (ARX) model and then use your RLS filter to recover its coefficients. This hands-on coding exercise demonstrates the power of RLS in practical system identification and allows you to observe the convergence of the parameter estimates over time [@problem_id:2899673].", "id": "2899673", "problem": "You are given an Autoregressive with Exogenous input (ARX) model driven by a known input. The data-generating model is\n$$\ny_k \\;=\\; -\\sum_{i=1}^{p} a_i\\,y_{k-i} \\;+\\; \\sum_{j=1}^{q} b_j\\,u_{k-j} \\;+\\; e_k,\n$$\nwhere $y_k$ is the output, $u_k$ is the known input, $e_k$ is white noise, and the unknown parameter vector to estimate is\n$$\n\\theta \\;=\\; [\\,a_1,\\dots,a_p,\\,b_1,\\dots,b_q\\,]^{\\top}.\n$$\nAssume $y_k=0$ and $u_k=0$ for all negative indices. The white noise sequence is independent and identically distributed Gaussian with zero mean and variance $\\sigma^2$, that is $e_k \\sim \\mathcal{N}(0,\\sigma^2)$, and is independent of the input process.\n\nYour task is to implement a Recursive Least Squares (RLS) estimator with forgetting factor to estimate $\\theta$ from synthetic data generated by the model. Construct the regressor vector at each time $k$ as\n$$\n\\varphi_k \\;=\\; \\big[\\, -y_{k-1},\\,-y_{k-2},\\,\\dots,\\,-y_{k-p},\\; u_{k-1},\\,u_{k-2},\\,\\dots,\\,u_{k-q}\\,\\big]^{\\top},\n$$\nso that the one-step linear predictor is $\\widehat{y}_k = \\varphi_k^{\\top}\\,\\widehat{\\theta}_{k-1}$. Initialize the RLS recursion with $\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$ and $P_0 = \\delta I_{p+q}$. Use a forgetting factor $\\lambda \\in (0,1]$.\n\nYou must generate the synthetic datasets as follows in each test case:\n- Generate the input sequence $u_k$ as independent Gaussian samples with zero mean and unit variance, using the specified random seed.\n- Generate the noise sequence $e_k$ as independent Gaussian samples with zero mean and variance $\\sigma^2$, using the specified random seed (if $\\sigma=0$, set $e_k=0$ deterministically).\n- Form the output $y_k$ sequentially using the ARX recursion above and the stated initial conditions.\n- Run the RLS recursion for $N$ iterations, processing samples $k=0,1,\\dots,N-1$, and report the final estimate $\\widehat{\\theta}_N$.\n\nImportant requirements:\n- For each test case, output the final estimated parameter vector $\\widehat{\\theta}_N$ ordered as $[a_1,\\dots,a_p,b_1,\\dots,b_q]$.\n- Round each element of each estimated parameter vector to $6$ decimal places.\n- There are no physical units in this problem.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the rounded list of estimates for the corresponding test case. For example, a valid output format is\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$,\nwith no additional text printed.\n\nTest suite:\n- Test case $1$ (happy path, moderate noise, strong prior):\n  - $p=1$, $q=1$,\n  - true $a_1 = 0.7$, true $b_1 = 0.9$,\n  - forgetting factor $\\lambda = 0.99$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.05$,\n  - number of samples $N = 80$,\n  - input seed $= 12345$, noise seed $= 54321$.\n- Test case $2$ (no forgetting, noise-free):\n  - $p=2$, $q=1$,\n  - true $[a_1,a_2] = [0.4,-0.15]$, true $[b_1] = [1.2]$,\n  - forgetting factor $\\lambda = 1.0$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.0$,\n  - number of samples $N = 120$,\n  - input seed $= 111$, noise seed $= 222$.\n- Test case $3$ (pure FIR, $p=0$ edge case, mild noise, strong forgetting):\n  - $p=0$, $q=2$,\n  - true $[b_1,b_2] = [0.3,-0.1]$,\n  - forgetting factor $\\lambda = 0.98$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.1$,\n  - number of samples $N = 60$,\n  - input seed $= 7$, noise seed $= 8$.\n- Test case $4$ (higher noise, near-unity forgetting, smaller prior scale):\n  - $p=1$, $q=2$,\n  - true $[a_1] = [0.3]$, true $[b_1,b_2] = [0.5,0.2]$,\n  - forgetting factor $\\lambda = 0.995$,\n  - initial covariance scale $\\delta = 10$,\n  - noise standard deviation $\\sigma = 0.5$,\n  - number of samples $N = 150$,\n  - input seed $= 999$, noise seed $= 1001$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the rounded list of estimated parameters for one test case, in the same order as listed above, for example\n$[[\\text{case1\\_estimates}],[\\text{case2\\_estimates}],[\\text{case3\\_estimates}],[\\text{case4\\_estimates}]]$.", "solution": "The problem presented is a standard exercise in system identification, specifically, the estimation of parameters for an Autoregressive with Exogenous input (ARX) model using a Recursive Least Squares (RLS) adaptive filter. The problem is well-defined, scientifically sound, and internally consistent. It provides all necessary parameters, initial conditions, and procedural steps for both data generation and estimation. Therefore, the problem is valid, and a solution will be provided.\n\nThe core of the problem lies in applying the RLS algorithm to identify the parameters of a linear system from its input-output data. The system is described by the ARX model:\n$$\ny_k = -\\sum_{i=1}^{p} a_i y_{k-i} + \\sum_{j=1}^{q} b_j u_{k-j} + e_k\n$$\nwhere $k$ is the discrete time index, $y_k$ is the system output, $u_k$ is the exogenous input, and $e_k$ is a zero-mean white noise process. The goal is to estimate the unknown parameter vector $\\theta$, which contains the model coefficients:\n$$\n\\theta = [a_1, \\dots, a_p, b_1, \\dots, b_q]^{\\top} \\in \\mathbb{R}^{p+q}\n$$\nTo facilitate a linear regression framework, we define a regressor vector $\\varphi_k$ that contains the past output and input values:\n$$\n\\varphi_k = [-y_{k-1}, -y_{k-2}, \\dots, -y_{k-p}, u_{k-1}, u_{k-2}, \\dots, u_{k-q}]^{\\top}\n$$\nWith this definition, the ARX model can be expressed in a compact linear form:\n$$\ny_k = \\varphi_k^{\\top} \\theta + e_k\n$$\nThis formulation states that the current output $y_k$ is a linear function of the regressor vector $\\varphi_k$ and the true parameter vector $\\theta$, corrupted by noise $e_k$. Our task is to find an estimate $\\widehat{\\theta}_k$ of $\\theta$ at each time step $k$.\n\nThe Recursive Least Squares algorithm provides an efficient method to solve the exponentially weighted least-squares problem, which aims to find the parameter estimate $\\widehat{\\theta}_k$ that minimizes the cost function:\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\varphi_i^{\\top} \\theta)^2 + \\lambda^k (\\theta - \\widehat{\\theta}_0)^{\\top} P_0^{-1} (\\theta - \\widehat{\\theta}_0)\n$$\nHere, $\\lambda \\in (0, 1]$ is the forgetting factor, which gives exponentially less weight to older data, allowing the algorithm to track time-varying parameters or discard the influence of initial conditions. The second term incorporates the prior knowledge about the parameters, where $\\widehat{\\theta}_0$ is the initial estimate and $P_0$ is the initial inverse covariance matrix, typically chosen as $P_0 = \\delta I$ for a large scalar $\\delta$.\n\nInstead of solving this minimization problem by batch processing at each time step, RLS provides a recursive update rule. Given the estimate $\\widehat{\\theta}_{k-1}$ and the inverse covariance matrix $P_{k-1}$ at time $k-1$, and a new data pair $(y_k, u_k)$, the algorithm computes the new estimate $\\widehat{\\theta}_k$ and matrix $P_k$.\n\nThe standard RLS algorithm proceeds as follows:\n\n1.  **Initialization** (at time $k=0$):\n    -   Parameter estimate: $\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$\n    -   Inverse covariance matrix: $P_0 = \\delta I_{p+q}$, where $I$ is the identity matrix and $\\delta$ is a large positive scalar.\n\n2.  **Recursion** (for each time step $k=1, 2, \\dots, N$ or, following the problem's indexing, $k=0, 1, \\dots, N-1$ for $N$ samples):\n    a. Form the regressor vector $\\varphi_k$ using past data $\\{y_{k-i}\\}_{i=1}^p$ and $\\{u_{k-j}\\}_{j=1}^q$. Per the problem statement, values for negative time indices are zero.\n    b. Calculate the *a priori* prediction error, $\\epsilon_k$, which is the difference between the measured output and the output predicted using the previous estimate $\\widehat{\\theta}_{k-1}$:\n    $$\n    \\epsilon_k = y_k - \\varphi_k^{\\top} \\widehat{\\theta}_{k-1}\n    $$\n    c. Compute the Kalman gain vector, $K_k$:\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n    $$\n    d. Update the parameter estimate vector. The new estimate is the old estimate plus a correction term proportional to the gain and the prediction error:\n    $$\n    \\widehat{\\theta}_k = \\widehat{\\theta}_{k-1} + K_k \\epsilon_k\n    $$\n    e. Update the inverse covariance matrix $P_k$. This step is derived from the matrix inversion lemma and simplifies the update of the inverse of the correlation matrix:\n    $$\n    P_k = \\frac{1}{\\lambda} (I - K_k \\varphi_k^{\\top}) P_{k-1}\n    $$\n\nThe implementation will first generate the synthetic data $(u_k, y_k)$ for $k=0, \\dots, N-1$ according to the specified ARX model, true parameters, and noise characteristics. The output $y_k$ must be generated sequentially as it depends on its own past values. Then, the RLS recursion is executed for $N$ iterations, processing one sample at each step, to obtain the final parameter estimate $\\widehat{\\theta}_N$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RLS estimation problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 1, \"q\": 1, \"a\": [0.7], \"b\": [0.9],\n            \"lambda\": 0.99, \"delta\": 1000, \"sigma\": 0.05, \"N\": 80,\n            \"input_seed\": 12345, \"noise_seed\": 54321,\n        },\n        {\n            \"p\": 2, \"q\": 1, \"a\": [0.4, -0.15], \"b\": [1.2],\n            \"lambda\": 1.0, \"delta\": 1000, \"sigma\": 0.0, \"N\": 120,\n            \"input_seed\": 111, \"noise_seed\": 222,\n        },\n        {\n            \"p\": 0, \"q\": 2, \"a\": [], \"b\": [0.3, -0.1],\n            \"lambda\": 0.98, \"delta\": 1000, \"sigma\": 0.1, \"N\": 60,\n            \"input_seed\": 7, \"noise_seed\": 8,\n        },\n        {\n            \"p\": 1, \"q\": 2, \"a\": [0.3], \"b\": [0.5, 0.2],\n            \"lambda\": 0.995, \"delta\": 10, \"sigma\": 0.5, \"N\": 150,\n            \"input_seed\": 999, \"noise_seed\": 1001,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p, q = case[\"p\"], case[\"q\"]\n        true_a = np.array(case[\"a\"])\n        true_b = np.array(case[\"b\"])\n        lam, delta, sigma, N = case[\"lambda\"], case[\"delta\"], case[\"sigma\"], case[\"N\"]\n        input_seed, noise_seed = case[\"input_seed\"], case[\"noise_seed\"]\n\n        # 1. Generate synthetic data\n        rng_u = np.random.default_rng(input_seed)\n        rng_e = np.random.default_rng(noise_seed)\n\n        u = rng_u.normal(0, 1, size=N)\n        if sigma == 0.0:\n            e = np.zeros(N)\n        else:\n            e = rng_e.normal(0, sigma, size=N)\n\n        y = np.zeros(N)\n        for k in range(N):\n            ar_term = 0.0\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        ar_term += true_a[i - 1] * y[k - i]\n            \n            x_term = 0.0\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        x_term += true_b[j - 1] * u[k - j]\n            \n            y[k] = -ar_term + x_term + e[k]\n\n        # 2. Run RLS estimation\n        dim = p + q\n        if dim == 0:\n            results.append([])\n            continue\n\n        theta_hat = np.zeros(dim)\n        P = delta * np.eye(dim)\n\n        for k in range(N):\n            # Construct regressor vector phi_k\n            phi = np.zeros(dim)\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        phi[i - 1] = -y[k - i]\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        phi[p + j - 1] = u[k - j]\n\n            # RLS update equations\n            phi = phi.reshape(-1, 1) # Ensure phi is a column vector\n            \n            # Gain vector K_k\n            P_phi = P @ phi\n            denominator = lam + (phi.T @ P_phi)\n            K = P_phi / denominator\n\n            # A priori prediction error\n            prediction_error = y[k] - (phi.T @ theta_hat.reshape(-1, 1))\n\n            # Update parameter estimate\n            theta_hat = theta_hat + (K * prediction_error).flatten()\n\n            # Update inverse covariance matrix\n            P = (1 / lam) * (P - K @ phi.T @ P)\n        \n        # 3. Format result\n        final_estimate = np.round(theta_hat, 6).tolist()\n        results.append(final_estimate)\n        \n    # Final output formatting\n    # Using map(str, results) correctly stringifies each inner list.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "A key aspect of using an RLS filter is tuning the forgetting factor $\\lambda$, which manages the trade-off between tracking time-varying parameters and rejecting noise. This exercise provides a deep, analytical look into this trade-off by having you derive the steady-state tracking error for a parameter that is drifting linearly. Completing this derivation [@problem_id:2899704] will give you a quantitative understanding of how $\\lambda$ creates a \"lag\" in the estimate, a fundamental concept for applying adaptive filters in non-stationary environments.", "id": "2899704", "problem": "Consider a scalar parameter tracking problem in which the observation at discrete time $k \\in \\{1,2,\\dots\\}$ is given by $y_k = \\theta_k + v_k$, where $\\theta_k$ is a time-varying true parameter and $v_k$ is zero-mean observation noise with finite variance. The estimator at time $k$ is the exponentially weighted least squares solution, which is the minimizer of the exponentially weighted sum of squared residuals\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{\\,k-i} \\big(y_i - \\theta\\big)^{2},\n$$\nwhere the forgetting factor $\\lambda$ satisfies $0<\\lambda<1$. Assume that the true parameter drifts linearly as $\\theta_k = \\theta_0 + r k$ with a constant drift rate $r \\in \\mathbb{R}$. Define the tracking error as $e_{\\mathrm{tr}}(k) \\triangleq \\theta_k - \\hat{\\theta}(k)$, where $\\hat{\\theta}(k)$ is the minimizer of $J_k(\\theta)$ at time $k$. Using only the fundamental definition of the exponentially weighted least squares criterion above and standard properties of geometric series, derive the steady-state expected tracking error\n$$\ne_{\\mathrm{tr},\\infty} \\triangleq \\lim_{k \\to \\infty} \\mathbb{E}\\big[e_{\\mathrm{tr}}(k)\\big]\n$$\nas a closed-form analytic expression in terms of the drift rate $r$ and the forgetting factor $\\lambda$. Express your final answer as a single analytic expression. No numerical approximation or rounding is required.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe estimator $\\hat{\\theta}(k)$ is defined as the value of $\\theta$ that minimizes the exponentially weighted sum of squared residuals:\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\theta)^{2}\n$$\nTo find the minimizer, we compute the derivative of $J_k(\\theta)$ with respect to $\\theta$ and set it to zero.\n$$\n\\frac{dJ_k(\\theta)}{d\\theta} = \\sum_{i=1}^{k} \\lambda^{k-i} \\cdot 2(y_i - \\theta) \\cdot (-1) = -2 \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\theta)\n$$\nSetting the derivative to zero for $\\theta = \\hat{\\theta}(k)$ yields:\n$$\n\\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\hat{\\theta}(k)) = 0\n$$\n$$\n\\left( \\sum_{i=1}^{k} \\lambda^{k-i} \\right) \\hat{\\theta}(k) = \\sum_{i=1}^{k} \\lambda^{k-i} y_i\n$$\nFrom this, we obtain the expression for the estimator $\\hat{\\theta}(k)$, which is an exponentially weighted moving average of the observations $y_i$:\n$$\n\\hat{\\theta}(k) = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} y_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nThe problem asks for the steady-state expected tracking error. We first find the expected value of the estimator $\\hat{\\theta}(k)$. The observation model is $y_i = \\theta_i + v_i$, where the noise term $v_i$ has zero mean, $\\mathbb{E}[v_i] = 0$. The true parameter $\\theta_i$ is a deterministic sequence.\nUsing the linearity of the expectation operator:\n$$\n\\mathbb{E}[\\hat{\\theta}(k)] = \\mathbb{E}\\left[ \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} y_i}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right] = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\mathbb{E}[y_i]}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nSince $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta_i + v_i] = \\mathbb{E}[\\theta_i] + \\mathbb{E}[v_i] = \\theta_i + 0 = \\theta_i$, we have:\n$$\n\\mathbb{E}[\\hat{\\theta}(k)] = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nThe expected tracking error at time $k$ is defined as $\\mathbb{E}[e_{\\mathrm{tr}}(k)] = \\mathbb{E}[\\theta_k - \\hat{\\theta}(k)]$. Since $\\theta_k$ is deterministic, this becomes:\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = \\theta_k - \\mathbb{E}[\\hat{\\theta}(k)] = \\theta_k - \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nNow, we substitute the given linear drift model for the true parameter, $\\theta_i = \\theta_0 + r i$:\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = (\\theta_0 + rk) - \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} (\\theta_0 + r i)}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nWe can separate the terms in the numerator of the fraction:\n$$\n\\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_0 + \\sum_{i=1}^{k} \\lambda^{k-i} r i}{\\sum_{i=1}^{k} \\lambda^{k-i}} = \\frac{\\theta_0 \\sum_{i=1}^{k} \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} + \\frac{r \\sum_{i=1}^{k} \\lambda^{k-i} i}{\\sum_{i=1}^{k} \\lambda^{k-i}} = \\theta_0 + r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nSubstituting this back into the expression for the expected tracking error gives:\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = (\\theta_0 + rk) - \\left( \\theta_0 + r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right) = rk - r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nThis expression can be combined into a single fraction:\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\left( \\frac{k \\sum_{i=1}^{k} \\lambda^{k-i} - \\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right) = r \\frac{\\sum_{i=1}^{k} (k-i) \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\nThis is the tracking error bias, or lag error, at time $k$. To find the steady-state error $e_{\\mathrm{tr},\\infty}$, we must evaluate the limit of this expression as $k \\to \\infty$. Let us introduce a change of summation index $j = k-i$. As $i$ runs from $1$ to $k$, $j$ runs from $k-1$ down to $0$. The expression becomes:\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\frac{\\sum_{j=0}^{k-1} j \\lambda^{j}}{\\sum_{j=0}^{k-1} \\lambda^{j}}\n$$\nNow, we take the limit as $k \\to \\infty$. Given that $0 < \\lambda < 1$, the sums converge to their respective infinite series:\n$$\ne_{\\mathrm{tr},\\infty} = \\lim_{k \\to \\infty} \\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\frac{\\sum_{j=0}^{\\infty} j \\lambda^{j}}{\\sum_{j=0}^{\\infty} \\lambda^{j}}\n$$\nThe denominator is a standard geometric series:\n$$\n\\sum_{j=0}^{\\infty} \\lambda^{j} = \\frac{1}{1-\\lambda}\n$$\nThe numerator is an arithmetico-geometric series. Its sum can be found by differentiating the geometric series formula. For $|x|<1$, we have $\\sum_{j=0}^{\\infty} x^j = (1-x)^{-1}$. Differentiating with respect to $x$:\n$$\n\\frac{d}{dx} \\sum_{j=0}^{\\infty} x^j = \\sum_{j=1}^{\\infty} j x^{j-1} = \\frac{d}{dx}(1-x)^{-1} = (1-x)^{-2}\n$$\nNote that the sum starts at $j=1$ since the $j=0$ term is constant. Multiplying by $x$ gives:\n$$\n\\sum_{j=1}^{\\infty} j x^{j} = \\frac{x}{(1-x)^2}\n$$\nSince the $j=0$ term is $0 \\cdot x^0 = 0$, this is also the sum from $j=0$ to $\\infty$. Substituting $x=\\lambda$:\n$$\n\\sum_{j=0}^{\\infty} j \\lambda^{j} = \\frac{\\lambda}{(1-\\lambda)^2}\n$$\nFinally, we substitute the values of the two infinite series back into the expression for $e_{\\mathrm{tr},\\infty}$:\n$$\ne_{\\mathrm{tr},\\infty} = r \\frac{\\frac{\\lambda}{(1-\\lambda)^2}}{\\frac{1}{1-\\lambda}} = r \\left( \\frac{\\lambda}{(1-\\lambda)^2} \\cdot (1-\\lambda) \\right)\n$$\nThis simplifies to the final expression for the steady-state expected tracking error.", "answer": "$$ \\boxed{r \\frac{\\lambda}{1-\\lambda}} $$"}]}