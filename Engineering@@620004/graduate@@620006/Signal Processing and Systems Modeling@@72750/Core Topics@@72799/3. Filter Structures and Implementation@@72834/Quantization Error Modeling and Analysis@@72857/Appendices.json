{"hands_on_practices": [{"introduction": "The journey into quantization analysis begins with understanding what makes a quantizer \"optimal.\" This exercise provides a foundational, first-principles derivation of an optimal scalar quantizer for a simple, uniformly distributed source. By working through the minimization of mean-squared error, you will derive the two cornerstones of quantizer design: the centroid condition for reproduction levels and the nearest-neighbor condition for decision thresholds, gaining a deep intuition for their interplay [@problem_id:2898106].", "problem": "Consider a memoryless real-valued source $X$ with probability density function (PDF) $f_{X}(x)$ that is uniform on the bounded interval $\\left[-A, A\\right]$, where $A>0$ is known, that is $f_{X}(x)=\\frac{1}{2A}$ for $x\\in[-A,A]$ and $f_{X}(x)=0$ otherwise. Let $Q$ be a scalar quantizer with $3$ reproduction levels $y_{1}<y_{2}<y_{3}$ and $2$ finite decision thresholds $b_{1}<b_{2}$ that partition $\\left[-A, A\\right]$ into three contiguous cells. Assume $Q$ maps $x\\in[-A,b_{1})$ to $y_{1}$, $x\\in[b_{1},b_{2})$ to $y_{2}$, and $x\\in[b_{2},A]$ to $y_{3}$, with measure-zero conventions at the boundaries being irrelevant. Define the mean squared error (MSE) distortion\n$$\nD(y_{1},y_{2},y_{3},b_{1},b_{2}) \\triangleq \\mathbb{E}\\!\\left[(X-Q(X))^{2}\\right].\n$$\nStarting only from the definition of $D$ and first principles of optimization, and without invoking any pre-quoted quantizer optimality conditions, do the following:\n- Justify that, without loss of optimality, an optimal solution can be taken to be symmetric about $0$ and deduce the resulting structural constraints on $(y_{1},y_{2},y_{3})$ and $(b_{1},b_{2})$.\n- Derive the necessary conditions for optimality by minimizing $D$ jointly over $(y_{1},y_{2},y_{3})$ for fixed $(b_{1},b_{2})$ and by minimizing $D$ over $(b_{1},b_{2})$ for fixed $(y_{1},y_{2},y_{3})$, using only the stated PDF and the MSE criterion.\n- Solve these conditions in closed form to obtain the globally optimal decision thresholds and reproduction levels as functions of $A$.\n- Compute the resulting minimized distortion $D^{\\star}$ as a closed-form expression in $A$.\n\nProvide the final answer as a single row matrix in the following order:\n$$\n\\begin{pmatrix}\nb_{1} & b_{2} & y_{1} & y_{2} & y_{3} & D^{\\star}\n\\end{pmatrix}.\n$$\nNo numerical approximation is required; provide exact symbolic expressions. Do not include any units.", "solution": "We begin with $X\\sim\\mathrm{Unif}([-A,A])$ and a scalar quantizer $Q$ with three cells and reproduction levels $y_{1}<y_{2}<y_{3}$ separated by thresholds $b_{1}<b_{2}$. The mean squared error (MSE) distortion is\n$$\nD=\\mathbb{E}\\!\\left[(X-Q(X))^{2}\\right]\n=\\int_{-A}^{A} (x-Q(x))^{2} f_{X}(x)\\,dx\n=\\frac{1}{2A}\\left[\\int_{-A}^{b_{1}}(x-y_{1})^{2}\\,dx+\\int_{b_{1}}^{b_{2}}(x-y_{2})^{2}\\,dx+\\int_{b_{2}}^{A}(x-y_{3})^{2}\\,dx\\right].\n$$\n\nSymmetry reduction. The PDF $f_{X}$ is even, and the squared error loss $(x-q)^{2}$ is invariant under the joint sign flip $(x,q)\\mapsto(-x,-q)$. If $Q$ is any quantizer with distortion $D(Q)$, define $\\widetilde{Q}(x)\\triangleq -Q(-x)$. Then $D(\\widetilde{Q})=D(Q)$ because the distribution of $X$ is symmetric and the loss is even. Consider the symmetrized mapping $Q_{s}(x)\\triangleq \\frac{1}{2}\\left(Q(x)+\\widetilde{Q}(x)\\right)$. By convexity of the function $q\\mapsto (x-q)^{2}$ for each fixed $x$, Jensen’s inequality implies\n$$\n(x-Q_{s}(x))^{2}\\leq \\frac{1}{2}\\left[(x-Q(x))^{2}+(x-\\widetilde{Q}(x))^{2}\\right].\n$$\nIntegrating against the symmetric PDF yields $D(Q_{s})\\leq D(Q)$. Therefore an optimal quantizer can be taken to be symmetric about $0$. For a $3$-level quantizer, symmetry imposes\n$$\nb_{1}=-t,\\quad b_{2}=t,\\quad y_{1}=-r,\\quad y_{2}=0,\\quad y_{3}=r,\n$$\nfor some $t\\in(0,A)$ and $r>0$. The cells are $[-A,-t)$, $[-t,t)$, and $[t,A]$.\n\nOptimal reproduction levels for fixed thresholds. For fixed $t$, minimizing $D$ with respect to each reproduction level decouples cellwise. For a given cell $[a,b]$ with reproduction level $y$, the contribution to $D$ is\n$$\n\\frac{1}{2A}\\int_{a}^{b}(x-y)^{2}\\,dx.\n$$\nDifferentiating with respect to $y$ and setting the derivative to zero gives\n$$\n\\frac{\\partial}{\\partial y}\\left[\\int_{a}^{b}(x-y)^{2}\\,dx\\right]=\\int_{a}^{b}-2(x-y)\\,dx=0\n\\;\\;\\Longrightarrow\\;\\; y=\\frac{1}{b-a}\\int_{a}^{b}x\\,dx=\\frac{a+b}{2}.\n$$\nThus, in each cell, the optimal reproduction level is the conditional mean of $X$ restricted to that cell, which for a uniform density is simply the interval midpoint. Applying this to the three cells yields\n$$\ny_{1}=\\frac{-A+(-t)}{2}=\\frac{-A-t}{2},\\quad y_{2}=\\frac{-t+t}{2}=0,\\quad y_{3}=\\frac{t+A}{2}.\n$$\nWith symmetry $y_{1}=-y_{3}$, which is automatically satisfied by these expressions.\n\nOptimal thresholds for fixed reproduction levels. For the squared error criterion on the real line, the pointwise optimal decision is the nearest-neighbor rule: for any $x$, the quantizer should choose the $y_{k}$ minimizing $(x-y_{k})^{2}$, which is equivalent to minimizing $|x-y_{k}|$. The decision boundary between two adjacent reproduction levels $y_{i}$ and $y_{i+1}$ is the point $x$ where $(x-y_{i})^{2}=(x-y_{i+1})^{2}$, which simplifies to $x=\\frac{y_{i}+y_{i+1}}{2}$. Applying this to the symmetric structure, the positive threshold $t$ (boundary between $y_{2}=0$ and $y_{3}=\\frac{t+A}{2}$) must satisfy\n$$\nt=\\frac{y_{2}+y_{3}}{2}=\\frac{0+\\frac{t+A}{2}}{2}=\\frac{t+A}{4}.\n$$\nSolving gives\n$$\n4t=t+A\\;\\;\\Longrightarrow\\;\\; 3t=A\\;\\;\\Longrightarrow\\;\\; t=\\frac{A}{3}.\n$$\nThe reproduction levels are then\n$$\ny_{3}=\\frac{t+A}{2}=\\frac{\\frac{A}{3}+A}{2}=\\frac{2A}{3},\\quad y_{2}=0,\\quad y_{1}=-\\frac{2A}{3}.\n$$\nThus the optimal thresholds and reproduction levels are\n$$\nb_{1}=-\\frac{A}{3},\\quad b_{2}=\\frac{A}{3},\\quad y_{1}=-\\frac{2A}{3},\\quad y_{2}=0,\\quad y_{3}=\\frac{2A}{3}.\n$$\n\nResulting minimized distortion. Substitute these values into the distortion integral. Using $f_{X}(x)=\\frac{1}{2A}$ on $[-A,A]$, we have\n$$\nD^{\\star}=\\frac{1}{2A}\\left[\\int_{-A}^{-A/3}\\left(x+\\frac{2A}{3}\\right)^{2}\\,dx+\\int_{-A/3}^{A/3}x^{2}\\,dx+\\int_{A/3}^{A}\\left(x-\\frac{2A}{3}\\right)^{2}\\,dx\\right].\n$$\nA convenient general identity for a uniform subinterval $[a,b]$ with midpoint $m=\\frac{a+b}{2}$ is\n$$\n\\int_{a}^{b}(x-m)^{2}\\,dx=\\frac{(b-a)^{3}}{12}.\n$$\nEach of the three cells has length $\\ell=\\frac{2A}{3}$ and, with the optimal $y_{k}$ set to the midpoint of its cell, each cell’s contribution equals $\\frac{1}{2A}\\cdot \\frac{\\ell^{3}}{12}$. Therefore,\n$$\nD^{\\star}=3\\cdot \\frac{1}{2A}\\cdot \\frac{\\left(\\frac{2A}{3}\\right)^{3}}{12}\n=\\frac{3}{2A}\\cdot \\frac{8A^{3}}{27}\\cdot \\frac{1}{12}\n=\\frac{3}{2A}\\cdot \\frac{2A^{3}}{81}\n=\\frac{A^{2}}{27}.\n$$\nHence, the globally optimal $3$-level scalar quantizer for a bounded uniform source on $\\left[-A,A\\right]$ has thresholds at $\\pm \\frac{A}{3}$, reproduction levels at $\\left\\{-\\frac{2A}{3},0,\\frac{2A}{3}\\right\\}$, and minimized distortion $D^{\\star}=\\frac{A^{2}}{27}$.", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{A}{3} & \\frac{A}{3} & -\\frac{2A}{3} & 0 & \\frac{2A}{3} & \\frac{A^{2}}{27}\\end{pmatrix}}$$", "id": "2898106"}, {"introduction": "Moving from idealized, bounded sources to more realistic statistical models like the Gaussian distribution reveals a fundamental trade-off in practical quantizer design. This practice explores the balance between granular error, which arises from the finite step size within the quantizer's range, and overload error, which occurs when the input signal exceeds that range. Mastering this analysis allows you to quantify the total distortion and make principled engineering decisions about setting a quantizer's dynamic range to handle real-world signals [@problem_id:2898089].", "problem": "Consider a real-valued, midtread, symmetric, uniform quantizer with saturation (clipping) at thresholds $\\pm X_{\\max}$ and a total of $2^{N}$ quantization intervals across the granular region $\\left[-X_{\\max},X_{\\max}\\right]$. The quantizer step size is therefore $\\Delta=2X_{\\max}/2^{N}$. The input signal $x$ is a zero-mean Gaussian random variable with variance $\\sigma^{2}$, that is, $x \\sim \\mathcal{N}(0,\\sigma^{2})$. The output $y$ equals the quantized value when $|x|\\leq X_{\\max}$ and equals $\\operatorname{sgn}(x)\\,X_{\\max}$ when $|x|>X_{\\max}$ (hard clipping in overload). Assume the standard high-resolution granular error model: conditioned on $|x|\\leq X_{\\max}$, the granular error $e_{g}=y-x$ is uniformly distributed over $\\left(-\\Delta/2,\\Delta/2\\right)$ and is independent of $x$; in overload, $y=\\operatorname{sgn}(x)\\,X_{\\max}$ and the overload error equals $e_{o}=y-x=\\operatorname{sgn}(x)\\left(X_{\\max}-|x|\\right)$.\n\nDefine the normalized backoff $\\alpha=X_{\\max}/\\sigma$. Use the Gaussian Probability Density Function (PDF) $\\varphi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$ and the Gaussian tail (complementary cumulative distribution) $Q(\\alpha)=\\int_{\\alpha}^{\\infty}\\varphi(z)\\,dz$.\n\nStarting only from these definitions and modeling assumptions, and without introducing any additional approximations beyond the stated granular error model, do the following for a fixed $N$:\n- Compute the overload probability $P_{\\mathrm{ol}}=\\mathbb{P}(|x|>X_{\\max})$ in terms of $\\alpha$.\n- Determine the analytic expression that captures the trade-off between backoff $\\alpha$ (equivalently $X_{\\max}$) and granular distortion by deriving the exact total mean-squared error $D(\\alpha;N,\\sigma)=\\mathbb{E}\\!\\left[(y-x)^{2}\\right]$ as a closed-form function of $\\alpha$, $N$, and $\\sigma$ that combines granular and overload contributions.\n\nYour final answer must be a single closed-form analytic expression or a single row matrix containing the closed-form overload probability and the total mean-squared error as functions of $\\alpha$, $N$, and $\\sigma$. Do not introduce any numerical values. Do not provide an inequality or an equation to be solved; provide explicit expressions in terms of $\\alpha$, $N$, $\\sigma$, $\\varphi(\\cdot)$, and $Q(\\cdot)$ only. No rounding is required and no units are needed.", "solution": "The solution requires the calculation of two quantities: the overload probability $P_{\\mathrm{ol}}$ and the total mean-squared error (MSE) $D$.\n\n**1. Overload Probability, $P_{\\mathrm{ol}}$**\n\nThe overload probability is the probability that the input signal magnitude exceeds the quantizer's maximum threshold, $X_{\\max}$.\n$$P_{\\mathrm{ol}} = \\mathbb{P}(|x| > X_{\\max})$$\nGiven that the input $x$ is a zero-mean Gaussian random variable, its PDF $f_x(u)$ is symmetric about $u=0$. Therefore, $\\mathbb{P}(x > X_{\\max}) = \\mathbb{P}(x < -X_{\\max})$.\n$$P_{\\mathrm{ol}} = \\mathbb{P}(x > X_{\\max}) + \\mathbb{P}(x < -X_{\\max}) = 2\\mathbb{P}(x > X_{\\max})$$\nTo compute this probability, we standardize the random variable $x$. Let $z = x/\\sigma$. Then $z$ follows the standard normal distribution, $z \\sim \\mathcal{N}(0,1)$. The condition $x > X_{\\max}$ is equivalent to $z > X_{\\max}/\\sigma$. Using the provided definition $\\alpha = X_{\\max}/\\sigma$, this becomes $z > \\alpha$.\n$$\\mathbb{P}(x > X_{\\max}) = \\mathbb{P}(z > \\alpha) = \\int_{\\alpha}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{v^2}{2}\\right) dv$$\nThis integral is the definition of the Gaussian Q-function, $Q(\\alpha)$.\nTherefore, the overload probability is:\n$$P_{\\mathrm{ol}} = 2Q(\\alpha)$$\n\n**2. Total Mean-Squared Error, $D(\\alpha; N, \\sigma)$**\n\nThe total MSE, $D$, is the expected value of the squared error $e^2 = (y-x)^2$. We can compute this by partitioning the expectation over the granular region ($|x| \\leq X_{\\max}$) and the overload region ($|x| > X_{\\max}$).\n$$D = \\mathbb{E}[e^2] = \\mathbb{E}[e^2 \\cdot \\mathbb{I}(|x| \\leq X_{\\max})] + \\mathbb{E}[e^2 \\cdot \\mathbb{I}(|x| > X_{\\max})]$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Let's call these two terms the granular distortion contribution, $D_g$, and the overload distortion contribution, $D_o$.\n\n**Granular Distortion Contribution, $D_g$**:\n$$D_g = \\mathbb{E}[e_g^2 \\cdot \\mathbb{I}(|x| \\leq X_{\\max})]$$\nBy the law of total expectation, this is $\\mathbb{E}[e_g^2 | |x| \\leq X_{\\max}] \\cdot \\mathbb{P}(|x| \\leq X_{\\max})$.\nThe problem states that conditioned on $|x| \\leq X_{\\max}$, the granular error $e_g$ is uniform on $(-\\Delta/2, \\Delta/2)$. The second moment (and variance, since the mean is $0$) of a uniform distribution $U(a,b)$ is $\\frac{(b-a)^2}{12}$. For $e_g$, this is $\\frac{(\\Delta/2 - (-\\Delta/2))^2}{12} = \\frac{\\Delta^2}{12}$.\nSo, $\\mathbb{E}[e_g^2 | |x| \\leq X_{\\max}] = \\frac{\\Delta^2}{12}$.\nThe probability of being in the granular region is $\\mathbb{P}(|x| \\leq X_{\\max}) = 1 - P_{\\mathrm{ol}} = 1 - 2Q(\\alpha)$.\nSubstituting $\\Delta = \\frac{2X_{\\max}}{2^N} = \\frac{2\\alpha\\sigma}{2^N}$:\n$$D_g = \\frac{1}{12}\\left(\\frac{2\\alpha\\sigma}{2^N}\\right)^2 (1 - 2Q(\\alpha)) = \\frac{4\\alpha^2\\sigma^2}{12 \\cdot (2^N)^2} (1 - 2Q(\\alpha)) = \\frac{\\alpha^2\\sigma^2}{3 \\cdot 2^{2N}}(1 - 2Q(\\alpha))$$\n\n**Overload Distortion Contribution, $D_o$**:\n$$D_o = \\mathbb{E}[e_o^2 \\cdot \\mathbb{I}(|x| > X_{\\max})] = \\int_{|u| > X_{\\max}} (\\operatorname{sgn}(u)X_{\\max} - u)^2 f_x(u) du$$\nThe integrand is symmetric. For $u > X_{\\max}$, the squared error is $(X_{\\max}-u)^2$. For $u < -X_{\\max}$, it is $(-X_{\\max}-u)^2 = (X_{\\max}+u)^2$. Due to the symmetry of $f_x(u)$, the contributions from positive and negative overload regions are identical.\n$$D_o = 2 \\int_{X_{\\max}}^{\\infty} (u-X_{\\max})^2 f_x(u) du = 2 \\int_{X_{\\max}}^{\\infty} (u-X_{\\max})^2 \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) du$$\nWe perform a change of variable $z = u/\\sigma$, so $u = z\\sigma$ and $du = \\sigma dz$. The lower limit of integration becomes $\\alpha = X_{\\max}/\\sigma$.\n$$D_o = 2 \\int_{\\alpha}^{\\infty} (z\\sigma - \\alpha\\sigma)^2 \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\sigma dz = 2\\sigma^2 \\int_{\\alpha}^{\\infty} (z-\\alpha)^2 \\varphi(z) dz$$\nWe expand the square: $(z-\\alpha)^2 = z^2 - 2\\alpha z + \\alpha^2$. The integral becomes:\n$$ \\int_{\\alpha}^{\\infty} (z^2 - 2\\alpha z + \\alpha^2) \\varphi(z) dz = \\int_{\\alpha}^{\\infty} z^2\\varphi(z)dz - 2\\alpha\\int_{\\alpha}^{\\infty} z\\varphi(z)dz + \\alpha^2\\int_{\\alpha}^{\\infty} \\varphi(z)dz $$\nWe evaluate each integral term:\n- The last term is by definition $\\alpha^2 Q(\\alpha)$.\n- For the middle term, $\\int z\\varphi(z)dz = \\int \\frac{z}{\\sqrt{2\\pi}}\\exp(-z^2/2)dz = -\\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2) = -\\varphi(z)$. So, $\\int_{\\alpha}^{\\infty} z\\varphi(z)dz = [-\\varphi(z)]_{\\alpha}^{\\infty} = 0 - (-\\varphi(\\alpha)) = \\varphi(\\alpha)$.\n- For the first term, we use integration by parts $\\int z^2\\varphi(z)dz = \\int z(z\\varphi(z))dz$. Let $u=z, dv=z\\varphi(z)dz$, so $du=dz, v=-\\varphi(z)$.\n  $\\int_{\\alpha}^{\\infty} z^2\\varphi(z)dz = [-z\\varphi(z)]_{\\alpha}^{\\infty} - \\int_{\\alpha}^{\\infty} (-\\varphi(z))dz = (0 - (-\\alpha\\varphi(\\alpha))) + \\int_{\\alpha}^{\\infty}\\varphi(z)dz = \\alpha\\varphi(\\alpha) + Q(\\alpha)$.\nCombining these results:\n$$ \\int_{\\alpha}^{\\infty} (z-\\alpha)^2 \\varphi(z) dz = (\\alpha\\varphi(\\alpha) + Q(\\alpha)) - 2\\alpha(\\varphi(\\alpha)) + \\alpha^2 Q(\\alpha) = (\\alpha^2+1)Q(\\alpha) - \\alpha\\varphi(\\alpha) $$\nThus, the overload distortion contribution is:\n$$ D_o = 2\\sigma^2 \\left( (\\alpha^2+1)Q(\\alpha) - \\alpha\\varphi(\\alpha) \\right) $$\n\n**Total MSE**:\nThe total MSE is the sum $D = D_g + D_o$:\n$$ D(\\alpha; N, \\sigma) = \\frac{\\alpha^2\\sigma^2}{3 \\cdot 2^{2N}}(1 - 2Q(\\alpha)) + 2\\sigma^2 \\left( (\\alpha^2+1)Q(\\alpha) - \\alpha\\varphi(\\alpha) \\right) $$\nThis is the required closed-form expression.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2Q(\\alpha) & \\frac{\\alpha^2 \\sigma^2}{3 \\cdot 2^{2N}}(1 - 2Q(\\alpha)) + 2 \\sigma^2 \\left( (\\alpha^2+1)Q(\\alpha) - \\alpha \\varphi(\\alpha) \\right)\n\\end{pmatrix}\n}\n$$", "id": "2898089"}, {"introduction": "The effects of quantization extend beyond single samples, often accumulating in digital systems that perform recursive operations. This exercise contrasts two common quantization schemes, rounding and truncation, within a simple accumulator to reveal their long-term consequences. You will discover how a seemingly small implementation detail—the method of quantization—can introduce a systematic bias, leading to significant drift in computations over time, a critical consideration in fixed-point digital signal processing [@problem_id:2898061].", "problem": "A fixed-point digital system is used to compute the arithmetic mean of a constant input sequence under two different quantization strategies: rounding to the nearest and truncation. The accumulator is modeled as follows. Let the constant input be $x_{0} \\in \\mathbb{R}$ with $x_{0} > 0$. The accumulator state $\\{y_{k}\\}_{k \\geq 0}$ obeys\n$$\ny_{0} = 0, \\quad y_{k} = Q\\!\\left(y_{k-1} + x_{0}\\right), \\quad k = 1, 2, \\dots, N,\n$$\nwhere $Q(\\cdot)$ is a midtread uniform quantizer with step size $\\Delta > 0$ and quantization grid $\\{\\ldots, -\\Delta, 0, \\Delta, 2\\Delta, \\ldots\\}$. Consider two specific realizations of $Q(\\cdot)$:\n- Rounding to the nearest: $Q_{\\mathrm{rnd}}(z)$ maps $z$ to the nearest grid point (assume that ties at half-step occur with probability zero).\n- Truncation toward zero: $Q_{\\mathrm{trunc}}(z)$ maps $z$ to the nearest grid point not exceeding $|z|$ in magnitude and with the same sign (for $z \\geq 0$, this equals $Q_{\\mathrm{trunc}}(z) = \\Delta \\left\\lfloor \\frac{z}{\\Delta} \\right\\rfloor$).\n\nAfter $N$ accumulation steps, an estimate of the mean is formed by exact scaling (without further quantization),\n$$\nm_{N} \\triangleq \\frac{y_{N}}{N}.\n$$\nDefine the quantization error at step $k$ by $e_{k} \\triangleq Q\\!\\left(y_{k-1} + x_{0}\\right) - \\left(y_{k-1} + x_{0}\\right)$, so that $y_{k} = y_{k-1} + x_{0} + e_{k}$.\n\nAssume the following modeling conditions, which are standard in quantization error analysis under fixed-point accumulation:\n- The reduced-phase (also called sawtooth phase) $\\left(y_{k-1} + x_{0}\\right) \\bmod \\Delta$ is uniformly distributed over an interval of length $\\Delta$ and is independent across $k$, which can be justified either by a uniformly random initial accumulator state modulo $\\Delta$ or by the presence of ideal subtractive dither. Under this assumption, the error sequence $\\{e_{k}\\}$ is independent and identically distributed (i.i.d.) with a distribution determined by $Q(\\cdot)$.\n- Because $x_{0} > 0$ and $y_{0} = 0$, the truncation operation toward zero coincides with truncation toward $-\\infty$ over the trajectory, yielding $e_{k} \\in [-\\Delta, 0)$ for truncation.\n\nStarting from the definitions above and the properties of the uniform distribution on an interval, derive in closed form:\n- The bias of the mean estimate $m_{N}$, that is, $\\mathbb{E}[m_{N} - x_{0}]$, under rounding and under truncation.\n- The variance of $m_{N}$, that is, $\\mathrm{Var}(m_{N})$, under rounding and under truncation.\n- The expected squared deviation of the average value, defined as the mean-squared error $\\mathbb{E}\\!\\left[(m_{N} - x_{0})^{2}\\right]$, under rounding and under truncation.\n\nExpress your final results solely in terms of $\\Delta$ and $N$, and provide them in the following order as a single row matrix:\n$$\n\\big[\\ \\text{bias under rounding},\\ \\text{variance under rounding},\\ \\text{bias under truncation},\\ \\text{variance under truncation},\\ \\text{MSE under rounding},\\ \\text{MSE under truncation}\\ \\big].\n$$\nNo numerical substitution is required and no units are to be reported. Give a closed-form analytic expression as your final answer.", "solution": "The problem requires the derivation of the bias, variance, and mean-squared error (MSE) for an arithmetic mean estimate computed using a quantized accumulator under two different quantization schemes: rounding and truncation.\n\nFirst, we establish a general relationship between the mean estimate $m_N$ and the sequence of quantization errors $\\{e_k\\}$. The accumulator state is defined by the recursion $y_k = Q(y_{k-1} + x_0)$ with $y_0 = 0$. The quantization error at step $k$ is $e_k = Q(y_{k-1} + x_0) - (y_{k-1} + x_0)$. This allows us to rewrite the recursion as:\n$$y_k = y_{k-1} + x_0 + e_k$$\nfor $k = 1, 2, \\dots, N$. We can unroll this recursion starting from $y_0 = 0$:\n$$y_1 = y_0 + x_0 + e_1 = x_0 + e_1$$\n$$y_2 = y_1 + x_0 + e_2 = (x_0 + e_1) + x_0 + e_2 = 2x_0 + \\sum_{i=1}^2 e_i$$\nBy induction, the state at step $N$ is:\n$$y_N = N x_0 + \\sum_{k=1}^N e_k$$\nThe mean estimate $m_N$ is defined as $m_N = \\frac{y_N}{N}$. Substituting the expression for $y_N$, we get:\n$$m_N = \\frac{1}{N} \\left( N x_0 + \\sum_{k=1}^N e_k \\right) = x_0 + \\frac{1}{N} \\sum_{k=1}^N e_k$$\nThe deviation of the estimate from the true mean $x_0$ is therefore the average of the quantization errors:\n$$m_N - x_0 = \\frac{1}{N} \\sum_{k=1}^N e_k$$\nThe problem states that the error sequence $\\{e_k\\}$ is independent and identically distributed (i.i.d.). Let $\\mu_e = \\mathbb{E}[e_k]$ and $\\sigma_e^2 = \\mathrm{Var}(e_k)$ be the mean and variance of a single error term, respectively. We can now express the desired statistical quantities in terms of $\\mu_e$ and $\\sigma_e^2$.\n\nThe bias of the estimate is $\\mathbb{E}[m_N - x_0]$:\n$$\\text{Bias} = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{k=1}^N e_k\\right] = \\frac{1}{N} \\sum_{k=1}^N \\mathbb{E}[e_k] = \\frac{1}{N} (N \\mu_e) = \\mu_e$$\nThe variance of the estimate is $\\mathrm{Var}(m_N)$:\n$$\\mathrm{Var}(m_N) = \\mathrm{Var}\\left(x_0 + \\frac{1}{N} \\sum_{k=1}^N e_k\\right) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{k=1}^N e_k\\right) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{k=1}^N e_k\\right)$$\nSince the errors are independent, the variance of the sum is the sum of the variances:\n$$\\mathrm{Var}(m_N) = \\frac{1}{N^2} \\sum_{k=1}^N \\mathrm{Var}(e_k) = \\frac{1}{N^2} (N \\sigma_e^2) = \\frac{\\sigma_e^2}{N}$$\nThe mean-squared error (MSE) is $\\mathbb{E}[(m_N - x_0)^2]$, which is given by the sum of the variance and the squared bias:\n$$\\text{MSE} = \\mathrm{Var}(m_N) + (\\text{Bias})^2 = \\frac{\\sigma_e^2}{N} + \\mu_e^2$$\nNext, we determine $\\mu_e$ and $\\sigma_e^2$ for each quantization scheme. The modeling assumption is that the reduced phase $(y_{k-1} + x_0) \\bmod \\Delta$ is uniformly distributed. This implies the quantization error $e_k$ is uniformly distributed over its support.\n\nCase 1: Rounding to the nearest ($Q_{\\mathrm{rnd}}$)\nThe error $e_k$ for rounding lies in the interval $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$. The uniform distribution assumption implies $e_k \\sim U[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$.\nThe mean of this distribution is:\n$$\\mu_{e, \\mathrm{rnd}} = \\mathbb{E}[e_k] = \\frac{-\\frac{\\Delta}{2} + \\frac{\\Delta}{2}}{2} = 0$$\nThe variance of this distribution is:\n$$\\sigma_{e, \\mathrm{rnd}}^2 = \\mathrm{Var}(e_k) = \\frac{\\left(\\frac{\\Delta}{2} - \\left(-\\frac{\\Delta}{2}\\right)\\right)^2}{12} = \\frac{\\Delta^2}{12}$$\nUsing these values, we find the statistics for rounding:\n- Bias (rounding): $\\mu_{e, \\mathrm{rnd}} = 0$\n- Variance (rounding): $\\frac{\\sigma_{e, \\mathrm{rnd}}^2}{N} = \\frac{\\Delta^2}{12N}$\n- MSE (rounding): $\\frac{\\sigma_{e, \\mathrm{rnd}}^2}{N} + \\mu_{e, \\mathrm{rnd}}^2 = \\frac{\\Delta^2}{12N} + 0^2 = \\frac{\\Delta^2}{12N}$\n\nCase 2: Truncation toward zero ($Q_{\\mathrm{trunc}}$)\nFor positive inputs, truncation toward zero is $Q_{\\mathrm{trunc}}(z) = \\Delta \\lfloor \\frac{z}{\\Delta} \\rfloor$. The error $e_k$ is in $[-\\Delta, 0)$. Assuming a uniform distribution, $e_k \\sim U[-\\Delta, 0]$.\nThe mean of this distribution is:\n$$\\mu_{e, \\mathrm{trunc}} = \\mathbb{E}[e_k] = \\frac{-\\Delta + 0}{2} = -\\frac{\\Delta}{2}$$\nThe variance of this distribution is:\n$$\\sigma_{e, \\mathrm{trunc}}^2 = \\mathrm{Var}(e_k) = \\frac{(0 - (-\\Delta))^2}{12} = \\frac{\\Delta^2}{12}$$\nUsing these values, we find the statistics for truncation:\n- Bias (truncation): $\\mu_{e, \\mathrm{trunc}} = -\\frac{\\Delta}{2}$\n- Variance (truncation): $\\frac{\\sigma_{e, \\mathrm{trunc}}^2}{N} = \\frac{\\Delta^2}{12N}$\n- MSE (truncation): $\\frac{\\sigma_{e, \\mathrm{trunc}}^2}{N} + \\mu_{e, \\mathrm{trunc}}^2 = \\frac{\\Delta^2}{12N} + \\left(-\\frac{\\Delta}{2}\\right)^2 = \\frac{\\Delta^2}{12N} + \\frac{\\Delta^2}{4} = \\frac{\\Delta^2 + 3N\\Delta^2}{12N} = \\frac{\\Delta^2(1+3N)}{12N}$\n\nFinally, we assemble the results into a single row matrix as requested.\nThe quantities are, in order:\n1.  Bias (rounding): $0$\n2.  Variance (rounding): $\\frac{\\Delta^2}{12N}$\n3.  Bias (truncation): $-\\frac{\\Delta}{2}$\n4.  Variance (truncation): $\\frac{\\Delta^2}{12N}$\n5.  MSE (rounding): $\\frac{\\Delta^2}{12N}$\n6.  MSE (truncation): $\\frac{\\Delta^2(1+3N)}{12N}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{\\Delta^2}{12N} & -\\frac{\\Delta}{2} & \\frac{\\Delta^2}{12N} & \\frac{\\Delta^2}{12N} & \\frac{\\Delta^2(1+3N)}{12N}\n\\end{pmatrix}\n}\n$$", "id": "2898061"}]}