{"hands_on_practices": [{"introduction": "This first exercise returns to the fundamentals of fixed-point number systems. Before analyzing complex algorithms, one must have a firm grasp of how a chosen bit allocation defines the capabilities and limitations of the representation. This practice [@problem_id:2887760] challenges you to derive the essential properties of resolution and dynamic range directly from the bit-level definition of a signed number, leading to an understanding of the fundamental design trade-off between signal precision and overflow risk.", "problem": "A uniformly scaled, signed fixed-point representation is used in a signal processing chain. The representation is two’s complement with $m$ integer bits (including the sign bit) and $n$ fractional bits, for a total word length $W=m+n$. Each bit position has binary weight consistent with two’s complement: the sign bit has weight $-2^{m-1}$, the remaining integer bits have weights $2^{m-2},\\dots,2^{0}$, and the fractional bits have weights $2^{-1},2^{-2},\\dots,2^{-n}$. Quantization is by rounding to the nearest representable value (mid-tread), and, when not explicitly stated otherwise, arithmetic is assumed not to saturate nor wrap; rather, one is concerned with whether the real-valued signal falls outside the representable set (overflow event). \n\nStarting from these definitions:\n1) Derive the resolution $\\Delta$, defined as the smallest positive difference between two adjacent representable numbers.\n2) Derive the maximal absolute representable value $V_{\\text{abs,max}}=\\max\\{|x|:x\\text{ is representable}\\}$, and state the closed interval of representable values.\n\nNow suppose the input signal is bounded in magnitude by $|x(t)|\\leq X_{\\max}$ and the total word length $W$ is fixed. Using only first principles of binary weighting and the high-resolution model for rounding quantization, reason about the trade-off between overflow and quantization error as the integer–fractional bit split $(m,n)$ varies under the constraint $m+n=W$. In particular, derive the condition on $(m,n)$ under which overflow cannot occur for the bounded signal, and explain how the mean-squared quantization error scales with $(m,n)$ through $\\Delta$.\n\nExpress your final answer as the ordered pair $\\left(\\Delta,\\,V_{\\text{abs,max}}\\right)$ in exact symbolic form. Do not round.", "solution": "The problem statement must first undergo a rigorous validation process.\n\nStep 1: Extract Givens.\n- Representation: Uniformly scaled, signed fixed-point, two’s complement.\n- Word length: $W=m+n$.\n- Integer bits: $m$ (including a sign bit).\n- Fractional bits: $n$.\n- Bit weights: Sign bit has weight $-2^{m-1}$, remaining integer bits have weights $2^{m-2}, \\dots, 2^{0}$, and fractional bits have weights $2^{-1}, 2^{-2}, \\dots, 2^{-n}$.\n- Quantization: Rounding to the nearest representable value (mid-tread).\n- Overflow: Occurs when a real-valued signal falls outside the representable set. Saturation and wrap-around are not considered.\n- Input signal: Bounded by $|x(t)| \\le X_{\\max}$.\n- Constraint: $W$ is a fixed total word length.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded and well-posed. It presents a standard, fundamental scenario in digital signal processing concerning the properties of fixed-point number systems. The definitions provided for two's complement fixed-point representation are standard and internally consistent. The problem is objective, using precise technical language. It does not violate any scientific principles, is not based on false premises, and contains all necessary information for a rigorous derivation. The problem is directly relevant to the topic of number representations in signal processing. The questions posed are structured to have unique, derivable answers based on first principles. The problem does not contain any of the flaws listed in the validation criteria.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A complete solution will be provided.\n\nA number $x$ in this fixed-point format is represented by a set of $W=m+n$ bits, denoted $(b_{m-1}, b_{m-2}, \\dots, b_0, b_{-1}, \\dots, b_{-n})$, where $b_i \\in \\{0, 1\\}$. The value of the number is given by the weighted sum of its bits:\n$$ x = -b_{m-1}2^{m-1} + \\sum_{i=0}^{m-2} b_i 2^i + \\sum_{j=1}^{n} b_{-j} 2^{-j} $$\n\n1) Derivation of Resolution $\\Delta$.\nThe resolution, $\\Delta$, is defined as the smallest positive difference between two adjacent representable numbers. This difference corresponds to a change in the least significant bit (LSB). In this representation, the LSB is the bit with the smallest weight, which is $b_{-n}$ with a weight of $2^{-n}$. Changing this bit from $0$ to $1$ while all other bits remain constant increases the value of the number by $2^{-n}$. Any other change in bits would result in a change of at least this magnitude. Therefore, the resolution is the weight of the LSB.\n$$ \\Delta = 2^{-n} $$\n\n2) Derivation of $V_{\\text{abs,max}}$ and the Representable Range.\nTo determine the range of representable values, we must find the minimum and maximum values that can be formed.\n\nThe maximum value, $x_{\\max}$, occurs when the sign bit $b_{m-1}$ is $0$ and all other bits are $1$:\n$$ x_{\\max} = -0 \\cdot 2^{m-1} + \\sum_{i=0}^{m-2} 1 \\cdot 2^i + \\sum_{j=1}^{n} 1 \\cdot 2^{-j} $$\nThe first sum is a geometric series for the integer part: $\\sum_{i=0}^{m-2} 2^i = \\frac{2^{m-1}-1}{2-1} = 2^{m-1}-1$.\nThe second sum is a geometric series for the fractional part: $\\sum_{j=1}^{n} 2^{-j} = \\frac{2^{-1}(1-(2^{-1})^n)}{1-2^{-1}} = 1-2^{-n}$.\nCombining these results:\n$$ x_{\\max} = (2^{m-1}-1) + (1-2^{-n}) = 2^{m-1} - 2^{-n} $$\n\nThe minimum value, $x_{\\min}$, occurs in two's complement representation when the sign bit $b_{m-1}$ is $1$ and all other bits are $0$:\n$$ x_{\\min} = -1 \\cdot 2^{m-1} + \\sum_{i=0}^{m-2} 0 \\cdot 2^i + \\sum_{j=1}^{n} 0 \\cdot 2^{-j} = -2^{m-1} $$\n\nThe closed interval of representable values is therefore $[x_{\\min}, x_{\\max}] = [-2^{m-1}, 2^{m-1} - 2^{-n}]$.\n\nThe maximal absolute representable value, $V_{\\text{abs,max}}$, is the maximum of the absolute values of the numbers in this interval. We compare the magnitudes of $x_{\\min}$ and $x_{\\max}$:\n$$ |x_{\\min}| = |-2^{m-1}| = 2^{m-1} $$\n$$ |x_{\\max}| = |2^{m-1} - 2^{-n}| = 2^{m-1} - 2^{-n} $$\nSince $n \\ge 0$, the term $2^{-n}$ is positive (assuming $n$ is finite), which means $|x_{\\max}|  |x_{\\min}|$.\nTherefore, the maximal absolute representable value is the magnitude of the most negative number:\n$$ V_{\\text{abs,max}} = 2^{m-1} $$\n\n3) Trade-off between Overflow and Quantization Error.\nThe total word length $W = m+n$ is fixed.\n\nOverflow: An overflow event occurs if the input signal $x(t)$ falls outside the representable range $[-2^{m-1}, 2^{m-1}-2^{-n}]$. Given that the input signal is bounded by $|x(t)| \\le X_{\\max}$, this means its values lie in the interval $[-X_{\\max}, X_{\\max}]$. To prevent overflow, this entire signal interval must be contained within the representable range:\n$$ [-X_{\\max}, X_{\\max}] \\subseteq [-2^{m-1}, 2^{m-1}-2^{-n}] $$\nThis requires satisfaction of two conditions:\n$$ X_{\\max} \\le 2^{m-1}-2^{-n} \\quad \\text{and} \\quad -X_{\\max} \\ge -2^{m-1} $$\nThe second condition is equivalent to $X_{\\max} \\le 2^{m-1}$. The first condition, $X_{\\max} \\le 2^{m-1}-2^{-n}$, is more restrictive. Thus, the rigorous condition on $(m,n)$ to guarantee that overflow cannot occur is:\n$$ X_{\\max} \\le 2^{m-1} - 2^{-n} $$\nTo accommodate a signal with a large maximum amplitude $X_{\\max}$, one must choose a sufficiently large number of integer bits, $m$. A larger $m$ expands the dynamic range of the representation.\n\nQuantization Error: Quantization by rounding to the nearest representable value introduces an error $e_q$ bounded by $|e_q| \\le \\frac{\\Delta}{2}$. The high-resolution quantization model assumes $e_q$ is a uniformly distributed random variable on the interval $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$. The mean-squared error (MSE), or quantization noise power, is the variance of this distribution:\n$$ \\sigma_q^2 = E[e_q^2] = \\int_{-\\Delta/2}^{\\Delta/2} e^2 \\frac{1}{\\Delta} de = \\frac{1}{\\Delta} \\left[ \\frac{e^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{\\Delta^2}{12} $$\nSubstituting $\\Delta = 2^{-n}$, the MSE is:\n$$ \\sigma_q^2 = \\frac{(2^{-n})^2}{12} = \\frac{2^{-2n}}{12} $$\nTo minimize quantization error, $\\sigma_q^2$ must be small, which requires $\\Delta$ to be small. This is achieved by increasing the number of fractional bits, $n$.\n\nTrade-off: The constraint is $m+n=W$, where $W$ is constant.\n- To prevent overflow for a signal of a given amplitude $X_{\\max}$, one must allocate a sufficient number of bits to $m$. Increasing $m$ increases the representable dynamic range, reducing the risk of overflow.\n- To improve precision (i.e., reduce quantization noise), one must allocate more bits to $n$. Increasing $n$ reduces the step size $\\Delta$, thereby lowering the quantization error power $\\sigma_q^2$.\nBecause $m$ and $n$ are coupled by a fixed sum $W$, these two objectives are in direct conflict. Increasing $m$ to expand the dynamic range forces a decrease in $n$, which degrades precision by increasing $\\Delta$ and thus $\\sigma_q^2$. Conversely, increasing $n$ to improve precision forces a decrease in $m$, which shrinks the dynamic range and increases the likelihood of overflow. This represents the fundamental trade-off between dynamic range and precision in fixed-point system design. For a given total word length $W$, the engineer must select an $(m,n)$ split that provides sufficient dynamic range to avoid overflow for the expected signal levels, while providing acceptable precision for the application.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2^{-n}  2^{m-1} \\end{pmatrix}}\n$$", "id": "2887760"}, {"introduction": "While fixed-point formats are crucial for many embedded systems, most scientific computing relies on the vast dynamic range of floating-point numbers. This exercise [@problem_id:2887751] focuses on the ubiquitous IEEE 754 standard, guiding you to deconstruct its representation to find the smallest and largest representable normalized values. By calculating the dynamic range in decibels, you will gain a concrete appreciation for the enormous scale these formats can handle, a key reason for their widespread adoption in science and engineering.", "problem": "A digital signal processing system uses the Institute of Electrical and Electronics Engineers (IEEE) 754 binary floating-point formats as its sole numeric representation for modeling continuous-time signals. In these formats, a finite normalized floating-point number is represented by a signed significand and an exponent, with an implicit leading bit in the significand and a biased exponent. Specifically, for a format with $f$ fraction bits and $w$ exponent bits, the significand $m$ satisfies $1 \\leq m  2$, the exponent field is interpreted with a bias $B$, and the unbiased exponent $E$ for normalized numbers ranges over a contiguous interval with the all-zeros and all-ones exponent fields excluded. The largest normalized number uses the largest unbiased exponent and the largest significand $m$ obtainable by setting all $f$ fraction bits to one. The smallest positive normalized number uses the smallest unbiased exponent and the smallest significand $m = 1$.\n\nConsider the binary32 and binary64 formats:\n- For binary32, there are $1$ sign bit, $w = 8$ exponent bits with bias $B_{32} = 127$, and $f = 23$ fraction bits.\n- For binary64, there are $1$ sign bit, $w = 11$ exponent bits with bias $B_{64} = 1023$, and $f = 52$ fraction bits.\n\nUsing only these structural facts and first principles from the IEEE 754 standard (implicit leading $1$ for normalized significands, biased exponents, and the exclusion of subnormal numbers and special values), do the following:\n1. Derive the smallest positive finite normalized number and the largest finite normalized number in each of the binary32 and binary64 formats, expressed in closed form using powers of $2$.\n2. Define the amplitude dynamic range in decibels as $20 \\log_{10}(x_{\\max}/x_{\\min})$, where $x_{\\max}$ and $x_{\\min}$ denote, respectively, the largest and smallest positive finite normalized representable amplitudes in the format. Derive exact closed-form expressions for the amplitude dynamic ranges (in decibels) of binary32 and binary64.\n3. Assuming a power quantity proportional to the square of amplitude, define the power dynamic range in decibels as $10 \\log_{10}(P_{\\max}/P_{\\min})$, where $P \\propto x^{2}$. Derive exact closed-form expressions for the power dynamic ranges (in decibels) of binary32 and binary64, and compare them to the amplitude dynamic ranges.\n\nExpress all final expressions in exact closed form using powers of $2$ and $\\log_{10}(\\cdot)$ without numerical approximation. Do not include any units inside your final boxed answer. Present your final answer as a single row matrix, in the order\n$[x_{\\min}^{(32)},\\, x_{\\max}^{(32)},\\, x_{\\min}^{(64)},\\, x_{\\max}^{(64)},\\, \\mathrm{DR}_{\\mathrm{amp}}^{(32)}\\ \\mathrm{(dB)},\\, \\mathrm{DR}_{\\mathrm{amp}}^{(64)}\\ \\mathrm{(dB)},\\, \\mathrm{DR}_{\\mathrm{pow}}^{(32)}\\ \\mathrm{(dB)},\\, \\mathrm{DR}_{\\mathrm{pow}}^{(64)}\\ \\mathrm{(dB)}]$.", "solution": "The problem statement is subjected to validation before any solution is attempted.\n\nFirst, we extract the givens verbatim:\n- A digital signal processing system uses IEEE 754 binary floating-point formats.\n- A finite normalized floating-point number is represented by a signed significand and an exponent, with an implicit leading bit in the significand and a biased exponent.\n- For a format with $f$ fraction bits and $w$ exponent bits, the significand $m$ satisfies $1 \\leq m  2$.\n- The exponent field is interpreted with a bias $B$.\n- The unbiased exponent $E$ for normalized numbers ranges over a contiguous interval with the all-zeros and all-ones exponent fields excluded.\n- The largest normalized number uses the largest unbiased exponent and the largest significand $m$ (all $f$ fraction bits are $1$).\n- The smallest positive normalized number uses the smallest unbiased exponent and the smallest significand $m=1$.\n- For binary32: $1$ sign bit, $w = 8$ exponent bits, bias $B_{32} = 127$, and $f = 23$ fraction bits.\n- For binary64: $1$ sign bit, $w = 11$ exponent bits, bias $B_{64} = 1023$, and $f = 52$ fraction bits.\n- Task 1: Derive smallest positive normalized number ($x_{\\min}$) and largest finite normalized number ($x_{\\max}$) for binary32 and binary64.\n- Task 2: Define and derive amplitude dynamic range $\\mathrm{DR}_{\\mathrm{amp}} = 20 \\log_{10}(x_{\\max}/x_{\\min})$ for binary32 and binary64.\n- Task 3: Define and derive power dynamic range $\\mathrm{DR}_{\\mathrm{pow}} = 10 \\log_{10}(P_{\\max}/P_{\\min})$ where $P \\propto x^2$, and compare to amplitude dynamic range.\n\nThe problem is determined to be valid. It is a well-posed problem grounded in the established principles of the IEEE 754 standard for floating-point arithmetic. All necessary parameters ($w$, $f$, $B$) and definitions are provided, and the questions posed are unambiguous, leading to a unique, verifiable solution. The problem is scientifically sound, objective, and self-contained. We shall proceed with the derivation.\n\nA finite normalized floating-point number $x$ is represented by the formula:\n$$x = (-1)^s \\times m \\times 2^E$$\nwhere $s$ is the sign bit ($0$ for positive, $1$ for negative), $m$ is the significand, and $E$ is the unbiased exponent.\n\nThe significand $m$ for a normalized number has an implicit leading bit of $1$. With $f$ fraction bits $b_{f-1}b_{f-2}...b_0$, the significand is given by:\n$$m = 1 + \\sum_{i=1}^{f} b_{i} 2^{-i}$$\nThe range of the significand for normalized numbers is $1 \\le m  2$.\n\nThe unbiased exponent $E$ is calculated from the biased exponent field $e$, which is an unsigned integer of $w$ bits. The relation is $E = e - B$, where the bias $B = 2^{w-1}-1$. For normalized numbers, the exponent field $e$ cannot be all zeros ($e=0$) or all ones ($e=2^w-1$). Thus, the range for $e$ is $1 \\le e \\le 2^w-2$.\n\n**1. Derivation of Smallest and Largest Normalized Numbers**\n\nTo find the smallest positive normalized number, $x_{\\min}$, we set the sign bit $s=0$, and use the smallest possible significand and the smallest possible unbiased exponent.\n- Smallest significand ($m_{\\min}$): This occurs when all fraction bits are $0$.\n  $$m_{\\min} = 1 + \\sum_{i=1}^{f} 0 \\cdot 2^{-i} = 1$$\n- Smallest unbiased exponent ($E_{\\min}$): This corresponds to the smallest biased exponent value, $e_{\\min}=1$.\n  $$E_{\\min} = e_{\\min} - B = 1 - B$$\nTherefore, the general expression for $x_{\\min}$ is:\n$$x_{\\min} = 1 \\times 2^{1-B}$$\n\nFor **binary32**: $f=23$, $B_{32}=127$.\n$$x_{\\min}^{(32)} = 2^{1-127} = 2^{-126}$$\nFor **binary64**: $f=52$, $B_{64}=1023$.\n$$x_{\\min}^{(64)} = 2^{1-1023} = 2^{-1022}$$\n\nTo find the largest finite normalized number, $x_{\\max}$, we set $s=0$ and use the largest possible significand and the largest possible unbiased exponent.\n- Largest significand ($m_{\\max}$): This occurs when all $f$ fraction bits are $1$.\n  $$m_{\\max} = 1 + \\sum_{i=1}^{f} 1 \\cdot 2^{-i} = 1 + (1 - 2^{-f}) = 2 - 2^{-f}$$\n- Largest unbiased exponent ($E_{\\max}$): This corresponds to the largest biased exponent value, $e_{\\max}=2^w-2$.\n  $$E_{\\max} = e_{\\max} - B = (2^w-2) - (2^{w-1}-1) = 2^w - 2^{w-1} - 1 = 2^{w-1} - 1 = B$$\nTherefore, the general expression for $x_{\\max}$ is:\n$$x_{\\max} = (2 - 2^{-f}) \\times 2^B$$\n\nFor **binary32**: $f=23$, $B_{32}=127$.\n$$x_{\\max}^{(32)} = (2 - 2^{-23}) \\times 2^{127}$$\nFor **binary64**: $f=52$, $B_{64}=1023$.\n$$x_{\\max}^{(64)} = (2 - 2^{-52}) \\times 2^{1023}$$\n\n**2. Derivation of Amplitude Dynamic Range**\n\nThe amplitude dynamic range in decibels is given by $\\mathrm{DR}_{\\mathrm{amp}} = 20 \\log_{10}(x_{\\max}/x_{\\min})$. We first find the ratio $x_{\\max}/x_{\\min}$:\n$$\\frac{x_{\\max}}{x_{\\min}} = \\frac{(2 - 2^{-f}) \\times 2^B}{1 \\times 2^{1-B}} = (2 - 2^{-f}) \\times 2^{B - (1-B)} = (2 - 2^{-f}) \\times 2^{2B-1}$$\nNow, we substitute this into the formula for $\\mathrm{DR}_{\\mathrm{amp}}$:\n$$\\mathrm{DR}_{\\mathrm{amp}} = 20 \\log_{10}\\left( (2 - 2^{-f}) \\times 2^{2B-1} \\right) = 20 \\left( \\log_{10}(2 - 2^{-f}) + \\log_{10}(2^{2B-1}) \\right)$$\n$$\\mathrm{DR}_{\\mathrm{amp}} = 20 \\left( \\log_{10}(2 - 2^{-f}) + (2B-1) \\log_{10}(2) \\right)$$\n\nFor **binary32**: $f=23$, $B_{32}=127$. The exponent multiplier is $2B_{32}-1 = 2(127)-1 = 254-1 = 253$.\n$$\\mathrm{DR}_{\\mathrm{amp}}^{(32)} = 20 \\left( \\log_{10}(2 - 2^{-23}) + 253 \\log_{10}(2) \\right)$$\nFor **binary64**: $f=52$, $B_{64}=1023$. The exponent multiplier is $2B_{64}-1 = 2(1023)-1 = 2046-1 = 2045$.\n$$\\mathrm{DR}_{\\mathrm{amp}}^{(64)} = 20 \\left( \\log_{10}(2 - 2^{-52}) + 2045 \\log_{10}(2) \\right)$$\n\n**3. Derivation of Power Dynamic Range and Comparison**\n\nThe power dynamic range in decibels is given by $\\mathrm{DR}_{\\mathrm{pow}} = 10 \\log_{10}(P_{\\max}/P_{\\min})$, where power $P$ is proportional to the square of amplitude $x$, so $P=kx^2$.\nThe ratio $P_{\\max}/P_{\\min}$ is:\n$$\\frac{P_{\\max}}{P_{\\min}} = \\frac{k(x_{\\max})^2}{k(x_{\\min})^2} = \\left(\\frac{x_{\\max}}{x_{\\min}}\\right)^2$$\nSubstituting this into the formula for $\\mathrm{DR}_{\\mathrm{pow}}$:\n$$\\mathrm{DR}_{\\mathrm{pow}} = 10 \\log_{10}\\left( \\left(\\frac{x_{\\max}}{x_{\\min}}\\right)^2 \\right) = 10 \\times 2 \\log_{10}\\left(\\frac{x_{\\max}}{x_{\\min}}\\right) = 20 \\log_{10}\\left(\\frac{x_{\\max}}{x_{\\min}}\\right)$$\nUpon inspection, this is identical to the definition of the amplitude dynamic range.\n$$\\mathrm{DR}_{\\mathrm{pow}} = \\mathrm{DR}_{\\mathrm{amp}}$$\nTherefore, the power dynamic ranges for the two formats are equal to their respective amplitude dynamic ranges.\n\nFor **binary32**:\n$$\\mathrm{DR}_{\\mathrm{pow}}^{(32)} = \\mathrm{DR}_{\\mathrm{amp}}^{(32)} = 20 \\left( \\log_{10}(2 - 2^{-23}) + 253 \\log_{10}(2) \\right)$$\nFor **binary64**:\n$$\\mathrm{DR}_{\\mathrm{pow}}^{(64)} = \\mathrm{DR}_{\\mathrm{amp}}^{(64)} = 20 \\left( \\log_{10}(2 - 2^{-52}) + 2045 \\log_{10}(2) \\right)$$\n\nThe comparison is that for a quantity proportional to the square of the amplitude, the power dynamic range in decibels is exactly equal to the amplitude dynamic range in decibels. This is a direct consequence of the properties of logarithms.\n\nWe compile the eight required expressions for the final answer.\n- $x_{\\min}^{(32)} = 2^{-126}$\n- $x_{\\max}^{(32)} = (2-2^{-23}) \\times 2^{127}$\n- $x_{\\min}^{(64)} = 2^{-1022}$\n- $x_{\\max}^{(64)} = (2-2^{-52}) \\times 2^{1023}$\n- $\\mathrm{DR}_{\\mathrm{amp}}^{(32)} = 20 ( \\log_{10}(2 - 2^{-23}) + 253 \\log_{10}(2) )$\n- $\\mathrm{DR}_{\\mathrm{amp}}^{(64)} = 20 ( \\log_{10}(2 - 2^{-52}) + 2045 \\log_{10}(2) )$\n- $\\mathrm{DR}_{\\mathrm{pow}}^{(32)} = 20 ( \\log_{10}(2 - 2^{-23}) + 253 \\log_{10}(2) )$\n- $\\mathrm{DR}_{\\mathrm{pow}}^{(64)} = 20 ( \\log_{10}(2 - 2^{-52}) + 2045 \\log_{10}(2) )$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2^{-126}  (2 - 2^{-23}) \\times 2^{127}  2^{-1022}  (2 - 2^{-52}) \\times 2^{1023}  20(\\log_{10}(2 - 2^{-23}) + 253\\log_{10}(2))  20(\\log_{10}(2 - 2^{-52}) + 2045\\log_{10}(2))  20(\\log_{10}(2 - 2^{-23}) + 253\\log_{10}(2))  20(\\log_{10}(2 - 2^{-52}) + 2045\\log_{10}(2))\n\\end{pmatrix}\n}\n$$", "id": "2887751"}, {"introduction": "The wide dynamic range of floating-point arithmetic can mask subtle but severe numerical pitfalls. This practice [@problem_id:2887738] presents a classic case of \"catastrophic cancellation,\" a phenomenon where subtracting two nearly-equal numbers results in a dramatic loss of significant figures. By working through this example, you will learn to identify the conditions that lead to this instability, quantify the resulting loss of precision, and apply an algebraic reformulation to achieve a numerically robust result.", "problem": "A computation is performed in the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary64 format with round-to-nearest, ties-to-even, and correctly rounded square root. Recall that binary64 has a fixed precision of $p=53$ bits in the significand (including the implicit leading bit). The unit in the last place (ULP) at magnitude approximately $2^{k}$ is $2^{k-52}$. Let $\\operatorname{fl}(\\cdot)$ denote the correctly rounded binary64 result of a real operation.\n\nConsider the real quantities $r_{1}=\\sqrt{N+1}$ and $r_{2}=\\sqrt{N}$ with $N=2^{104}$. Define the binary64 numbers $a=\\operatorname{fl}(r_{1})$ and $b=\\operatorname{fl}(r_{2})$.\n\n(i) Using only the definitions above and general properties of ULP spacing and rounding to nearest, construct the explicit pair of binary64 numbers $a$ and $b$ and show that their subtraction exhibits catastrophic cancellation in the sense that $\\operatorname{fl}(a-b)$ does not resolve the true real difference $r_{1}-r_{2}$.\n\n(ii) To quantify the loss of significant bits in the subtraction, define the loss index\n$$\nK \\triangleq \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor,\n$$\nwhere $M=\\max\\{|r_{1}|,|r_{2}|\\}$ and $D=|r_{1}-r_{2}|$. Compute $K$ exactly for the $N$ specified above.\n\n(iii) Recommend and justify a mathematically equivalent reformulation to compute $r_{1}-r_{2}$ that avoids catastrophic cancellation in binary64, deriving it from first principles rather than quoting a known formula, and explain (qualitatively, in terms of ULPs and rounding to nearest) why the reformulated computation is numerically stable.\n\nProvide as your final answer only the value of $K$ from part (ii). No rounding instruction is needed because the result is an exact integer with no physical units.", "solution": "The problem will be validated against the specified criteria before a solution is attempted.\n\nStep 1: Extract Givens\n- Computation format: Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary64.\n- Rounding mode: round-to-nearest, ties-to-even.\n- Square root: correctly rounded, denoted $\\operatorname{fl}(\\sqrt{\\cdot})$.\n- Significand precision: $p=53$ bits (including the implicit leading bit).\n- Unit in the last place (ULP): For a magnitude of approximately $2^{k}$, the ULP is $2^{k-52}$.\n- Floating-point rounding function: $\\operatorname{fl}(\\cdot)$.\n- Real quantities: $r_{1}=\\sqrt{N+1}$ and $r_{2}=\\sqrt{N}$.\n- Constant: $N=2^{104}$.\n- Floating-point numbers: $a=\\operatorname{fl}(r_{1})$ and $b=\\operatorname{fl}(r_{2})$.\n- Loss index definition: $K \\triangleq \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor$, with $M=\\max\\{|r_{1}|,|r_{2}|\\}$ and $D=|r_{1}-r_{2}|$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the standard principles of numerical analysis and floating-point arithmetic as defined by IEEE 754. It is well-posed, providing all necessary parameters ($N$, $p$, rounding mode) to uniquely determine the quantities in question. The problem statement is objective and uses precise, formal language. It is a standard, non-trivial problem designed to illustrate the phenomenon of catastrophic cancellation and its mitigation, directly relevant to the topic of number representations. No flaws from the checklist are present.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\nThe solution proceeds by addressing each of the three parts of the problem.\n\n(i) Explicit construction of $a$ and $b$ and demonstration of catastrophic cancellation.\n\nFirst, we analyze the quantity $r_{2} = \\sqrt{N} = \\sqrt{2^{104}} = 2^{52}$. In the binary64 format, a number is represented as $\\pm (1.f)_{2} \\times 2^{e_{\\text{biased}}-1023}$. The value $2^{52}$ can be written as $1.0 \\times 2^{52}$. This representation has a significand of $1$ followed by all zeros and an exponent of $52$. It is an exact binary floating-point number. Therefore, its representation in binary64 is exact: $b = \\operatorname{fl}(r_{2}) = \\operatorname{fl}(2^{52}) = 2^{52}$.\n\nNext, we analyze the quantity $r_{1} = \\sqrt{N+1} = \\sqrt{2^{104}+1}$. We use the binomial expansion for $\\sqrt{1+x}$ where $x$ is small: $\\sqrt{1+x} = 1 + \\frac{1}{2}x - \\frac{1}{8}x^2 + O(x^3)$.\n$r_{1} = \\sqrt{2^{104}(1 + 2^{-104})} = 2^{52}\\sqrt{1 + 2^{-104}}$.\nLetting $x = 2^{-104}$, we have:\n$r_{1} = 2^{52} \\left(1 + \\frac{1}{2}(2^{-104}) - \\frac{1}{8}(2^{-104})^2 + \\dots \\right) = 2^{52} \\left(1 + 2^{-105} - 2^{-3} \\cdot 2^{-208} + \\dots \\right) = 2^{52} + 2^{-53} - 2^{-159} + \\dots$.\nThe value of $r_{1}$ is slightly greater than $2^{52}$. We must determine how it rounds to a binary64 number. The floating-point number $b$ is $2^{52}$. We need to find the next representable binary64 number greater than $b$.\nThe unit in the last place for numbers with magnitude around $2^{52}$ is $\\operatorname{ulp}(2^{52})$. The exponent is $E=52$. The precision is $p=53$. The ULP is given by $2^{E-(p-1)} = 2^{52-(53-1)} = 2^{0} = 1$.\nThe representable numbers around $2^{52}$ are therefore spaced by $1$. The two binary64 numbers that bracket $r_{1}$ are $y_{\\text{low}} = 2^{52}$ and $y_{\\text{high}} = 2^{52}+1$.\nThe rounding mode is round-to-nearest. The midpoint between $y_{\\text{low}}$ and $y_{\\text{high}}$ is $m = \\frac{y_{\\text{low}}+y_{\\text{high}}}{2} = \\frac{2^{52} + (2^{52}+1)}{2} = 2^{52} + \\frac{1}{2} = 2^{52} + 2^{-1}$.\nOur value is $r_{1} = 2^{52} + 2^{-53} - 2^{-159} + \\dots$. Since $2^{-53}  2^{-1}$, it is clear that $r_{1}  m$.\nTherefore, $r_{1}$ rounds down to the nearest representable number, which is $y_{\\text{low}}$.\nSo, $a = \\operatorname{fl}(r_{1}) = 2^{52}$.\nWe have thus constructed the explicit pair: $a = 2^{52}$ and $b = 2^{52}$.\nThe computed subtraction is $\\operatorname{fl}(a-b) = \\operatorname{fl}(2^{52}-2^{52}) = \\operatorname{fl}(0) = 0$.\nThe true real difference is $r_{1}-r_{2} = \\sqrt{2^{104}+1} - 2^{52}$. From our expansion, this is $r_{1}-r_{2} = 2^{-53} - 2^{-159} + \\dots$, which is a small positive number approximately equal to $2^{-53}$.\nThe computed result is $0$, while the true result is non-zero. The relative error is essentially infinite. This demonstrates catastrophic cancellation: the initial rounding of $r_1$ and $r_2$ into $a$ and $b$ loses all information about their small difference. The subtraction $\\operatorname{fl}(a-b)$ fails to resolve the true difference $r_1-r_2$.\n\n(ii) Computation of the loss index $K$.\n\nThe loss index is defined as $K = \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor$.\nWe have $M = \\max\\{|r_{1}|,|r_{2}|\\} = \\max\\{\\sqrt{N+1}, \\sqrt{N}\\} = \\sqrt{N+1}$.\nAnd $D = |r_{1}-r_{2}| = \\sqrt{N+1} - \\sqrt{N}$.\nThe ratio is $\\frac{M}{D} = \\frac{\\sqrt{N+1}}{\\sqrt{N+1} - \\sqrt{N}}$.\nTo simplify this expression, we multiply the numerator and denominator of the fraction $\\frac{1}{D}$ by the conjugate of its denominator:\n$D = \\frac{(\\sqrt{N+1} - \\sqrt{N})(\\sqrt{N+1} + \\sqrt{N})}{\\sqrt{N+1} + \\sqrt{N}} = \\frac{(N+1) - N}{\\sqrt{N+1} + \\sqrt{N}} = \\frac{1}{\\sqrt{N+1} + \\sqrt{N}}$.\nThus, $\\frac{M}{D} = M \\cdot (\\sqrt{N+1} + \\sqrt{N}) = \\sqrt{N+1}(\\sqrt{N+1} + \\sqrt{N}) = (N+1) + \\sqrt{N(N+1)}$.\nSubstituting $N=2^{104}$:\n$\\frac{M}{D} = (2^{104}+1) + \\sqrt{2^{104}(2^{104}+1)} = 2^{104}+1 + \\sqrt{2^{208}+2^{104}}$.\nWe need to find the integer part of the base-$2$ logarithm of this quantity. Let $Y = \\frac{M}{D}$.\nWe establish bounds for $Y$.\nFor a lower bound:\n$\\sqrt{2^{208}+2^{104}}  \\sqrt{2^{208}} = 2^{104}$.\nSo, $Y  (2^{104}+1) + 2^{104} = 2 \\cdot 2^{104} + 1 = 2^{105} + 1$.\nFor an upper bound, consider the identity $(x+y)^{2} = x^2+2xy+y^2$. Let $x=2^{104}$ and $y=1/2=2^{-1}$.\n$(2^{104} + 2^{-1})^2 = (2^{104})^2 + 2(2^{104})(2^{-1}) + (2^{-1})^2 = 2^{208} + 2^{104} + \\frac{1}{4}$.\nSince $2^{208}+2^{104}  2^{208}+2^{104}+1/4$, we have $\\sqrt{2^{208}+2^{104}}  \\sqrt{(2^{104}+2^{-1})^2} = 2^{104}+2^{-1}$.\nSo, $Y  (2^{104}+1) + (2^{104}+2^{-1}) = 2 \\cdot 2^{104} + 1.5 = 2^{105} + 1.5$.\nWe have established the strict bounds $2^{105} + 1  Y  2^{105} + 1.5$.\nTaking the base-$2$ logarithm:\n$\\log_{2}(2^{105}+1)  \\log_{2}(Y)  \\log_{2}(2^{105}+1.5)$.\nSince $2^{105}  2^{105}+1$, we have $\\log_{2}(2^{105})  \\log_{2}(2^{105}+1)$, which means $105  \\log_{2}(Y)$.\nAlso, since $2^{105}+1.5  2^{106}$, we have $\\log_{2}(Y)  \\log_{2}(2^{106}) = 106$.\nSo, $105  \\log_{2}(Y)  106$.\nThe floor of this value is therefore $\\lfloor \\log_{2}(Y) \\rfloor = 105$.\n$K=105$.\n\n(iii) Reformulation and justification.\n\nThe catastrophic cancellation in $r_{1}-r_{2} = \\sqrt{N+1} - \\sqrt{N}$ arises from the subtraction of two nearly identical large numbers. To avoid this, we must reformulate the expression to eliminate the subtraction. This can be derived by multiplying and dividing by the conjugate expression, $\\sqrt{N+1}+\\sqrt{N}$:\n$r_{1}-r_{2} = (\\sqrt{N+1} - \\sqrt{N}) \\times \\frac{\\sqrt{N+1}+\\sqrt{N}}{\\sqrt{N+1}+\\sqrt{N}} = \\frac{(N+1)-N}{\\sqrt{N+1}+\\sqrt{N}} = \\frac{1}{\\sqrt{N+1}+\\sqrt{N}}$.\nThis is the recommended reformulation.\n\nJustification: The reformulated expression is numerically stable because it replaces the problematic subtraction with a benign addition.\nLet us analyze the computation of $y = \\frac{1}{\\sqrt{N+1}+\\sqrt{N}}$ in binary64.\n1. Compute $a = \\operatorname{fl}(\\sqrt{N+1})$ and $b = \\operatorname{fl}(\\sqrt{N})$. As shown in part (i), for $N=2^{104}$, both of these round to the same value, $2^{52}$.\n2. Compute the sum $S = \\operatorname{fl}(a+b) = \\operatorname{fl}(2^{52}+2^{52}) = \\operatorname{fl}(2 \\cdot 2^{52}) = \\operatorname{fl}(2^{53})$. This operation is exact, so $S=2^{53}$.\n3. Compute the final result $R = \\operatorname{fl}(1/S) = \\operatorname{fl}(1/2^{53}) = \\operatorname{fl}(2^{-53})$. The value $2^{-53}$ is a power of two and is exactly representable as a normal binary64 number. So the final computed result is exactly $R=2^{-53}$.\n\nThe true value is $D = \\frac{1}{\\sqrt{2^{104}+1}+2^{52}}$. As shown in part (i), $\\sqrt{2^{104}+1} = 2^{52}+2^{-53}-\\dots$.\nSo, $D = \\frac{1}{(2^{52}+2^{-53}-\\dots)+2^{52}} = \\frac{1}{2^{53}+2^{-53}-\\dots}$.\nThis is extremely close to $\\frac{1}{2^{53}} = 2^{-53}$.\nThe relative error of the reformulated computation is approximately $\\left| \\frac{D-R}{D} \\right| \\approx \\left| \\frac{(2^{-53}(1-2^{-106})) - 2^{-53}}{2^{-53}(1-2^{-106})} \\right| \\approx 2^{-106}$, which is extremely small.\nQualitatively, the original form $\\operatorname{fl}(\\operatorname{fl}(\\sqrt{N+1}) - \\operatorname{fl}(\\sqrt{N}))$ fails because the true difference $|r_1-r_2| \\approx 2^{-53}$ is smaller than the rounding error in the computation of the individual roots, which is on the order of $\\operatorname{ulp}(\\sqrt{N}) = \\operatorname{ulp}(2^{52}) = 1$. All significant bits of the true difference are lost.\nIn contrast, the reformulated expression calculates $\\operatorname{fl}(1 / (\\operatorname{fl}(\\sqrt{N+1}) + \\operatorname{fl}(\\sqrt{N})))$. The addition $\\sqrt{N+1}+\\sqrt{N}$ is numerically stable as it adds two large, positive, nearly equal numbers. The relative error of the sum is small. The subsequent division is also a stable operation. The new algorithm preserves the information and yields a highly accurate result.", "answer": "$$\n\\boxed{105}\n$$", "id": "2887738"}]}