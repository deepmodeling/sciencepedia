{"hands_on_practices": [{"introduction": "When implementing a digital multiplier, a key architectural decision is where to place the quantizer. This practice explores the trade-off between quantizing the input operands before multiplication versus quantizing the full-precision product afterward. By deriving the mean-squared error ($J$) for both scenarios under the standard additive noise model, you will gain a foundational understanding of how different implementation choices propagate quantization error. [@problem_id:2893694]", "problem": "Consider two independent, zero-mean, dimensionless, wide-sense stationary real-valued random variables $a$ and $b$ with finite variances $\\sigma_{a}^{2} = \\mathbb{E}[a^{2}]$ and $\\sigma_{b}^{2} = \\mathbb{E}[b^{2}]$. You are to compare two architectures for producing an approximation to the exact product $z = a b$:\n\n1. Operand quantization before multiplication (product quantization): The operands are passed through mid-tread uniform rounding quantizers $Q_{a}$ and $Q_{b}$ with steps $\\Delta_{a}$ and $\\Delta_{b}$, respectively, yielding $a_{q} = Q_{a}(a)$ and $b_{q} = Q_{b}(b)$, and the output is $y_{\\mathrm{pre}} = a_{q} b_{q}$.\n\n2. Product quantization after exact multiplication: The exact product $z = a b$ is computed and then quantized by a mid-tread uniform rounding quantizer $Q_{z}$ with step $\\Delta_{z}$, giving $y_{\\mathrm{post}} = Q_{z}(z)$.\n\nAssume the additive noise model for each quantizer under the high-resolution regime: $Q_{x}(x) = x + e_{x}$, where $e_{x}$ is the quantization error for signal $x \\in \\{a,b,z\\}$. For each $x$, model $e_{x}$ as independent of $x$ and of the other error processes, zero mean, and uniformly distributed on $[-\\Delta_{x}/2, \\Delta_{x}/2]$. In particular, $\\mathbb{E}[e_{x}] = 0$ and $\\mathbb{E}[e_{x}^{2}] = \\Delta_{x}^{2}/12$. \n\nDefine the total output mean-squared error (MSE) for each architecture with respect to the exact product $z = a b$ as $J_{\\mathrm{pre}} = \\mathbb{E}\\big[(y_{\\mathrm{pre}} - z)^{2}\\big]$ and $J_{\\mathrm{post}} = \\mathbb{E}\\big[(y_{\\mathrm{post}} - z)^{2}\\big]$. Work to second order in the quantization step sizes, meaning you must retain terms up to products of two quantization variances (for example, terms proportional to $\\Delta_{a}^{2} \\Delta_{b}^{2}$), and may neglect any higher-order terms beyond that order.\n\nDerive, from first principles and the stated assumptions, a closed-form analytic expression for the difference in output MSE between the two architectures,\n$$\n\\Delta J \\triangleq J_{\\mathrm{pre}} - J_{\\mathrm{post}},\n$$\nas a function of $\\sigma_{a}^{2}$, $\\sigma_{b}^{2}$, $\\Delta_{a}$, $\\Delta_{b}$, and $\\Delta_{z}$. Express your final answer as a single simplified symbolic expression. No units are required.", "solution": "The problem requires a comparative analysis of the mean-squared error (MSE) for two distinct quantization architectures in a multiplication operation. We are given two independent, zero-mean, wide-sense stationary random variables, $a$ and $b$, with variances $\\sigma_{a}^{2}$ and $\\sigma_{b}^{2}$, respectively. The objective is to derive an expression for $\\Delta J = J_{\\mathrm{pre}} - J_{\\mathrm{post}}$, where $J_{\\mathrm{pre}}$ and $J_{\\mathrm{post}}$ are the MSEs for pre-multiplication and post-multiplication quantization, respectively. We shall proceed by deriving expressions for each MSE individually, adhering to the provided additive noise model for quantization and retaining terms up to the second order in quantization variances.\n\nFirst, let us analyze the post-multiplication quantization architecture. The exact product is $z = ab$. This product is quantized to yield the output $y_{\\mathrm{post}} = Q_{z}(z)$. According to the additive noise model, we can write $y_{\\mathrm{post}} = z + e_{z}$, where $e_{z}$ is the quantization error associated with the quantizer $Q_{z}$. The properties of this error are given as $\\mathbb{E}[e_{z}] = 0$ and $\\mathbb{E}[e_{z}^{2}] = \\frac{\\Delta_{z}^{2}}{12}$.\n\nThe MSE for this architecture, $J_{\\mathrm{post}}$, is defined with respect to the exact product $z$:\n$$\nJ_{\\mathrm{post}} = \\mathbb{E}\\big[(y_{\\mathrm{post}} - z)^{2}\\big]\n$$\nSubstituting the expression for $y_{\\mathrm{post}}$, we have:\n$$\nJ_{\\mathrm{post}} = \\mathbb{E}\\big[((z + e_{z}) - z)^{2}\\big] = \\mathbb{E}[e_{z}^{2}]\n$$\nUsing the given variance of the quantization error, we find the MSE for the post-quantization case:\n$$\nJ_{\\mathrm{post}} = \\frac{\\Delta_{z}^{2}}{12}\n$$\nThis result is exact under the stated model assumptions.\n\nNext, we analyze the pre-multiplication quantization architecture. The operands $a$ and $b$ are first quantized, yielding $a_{q} = Q_{a}(a)$ and $b_{q} = Q_{b}(b)$. Using the additive noise model, we have:\n$$\na_{q} = a + e_{a}\n$$\n$$\nb_{q} = b + e_{b}\n$$\nwhere $e_{a}$ and $e_{b}$ are the respective quantization errors. Their properties are given as $\\mathbb{E}[e_{a}] = 0$, $\\mathbb{E}[e_{a}^{2}] = \\frac{\\Delta_{a}^{2}}{12}$, $\\mathbb{E}[e_{b}] = 0$, and $\\mathbb{E}[e_{b}^{2}] = \\frac{\\Delta_{b}^{2}}{12}$. The problem states that the error processes are independent of each other and of the signals.\n\nThe output for this architecture is the product of the quantized operands:\n$$\ny_{\\mathrm{pre}} = a_{q} b_{q} = (a + e_{a})(b + e_{b}) = ab + a e_{b} + b e_{a} + e_{a} e_{b}\n$$\nThe MSE, $J_{\\mathrm{pre}}$, is the expectation of the squared difference between this output and the exact product $z = ab$:\n$$\ny_{\\mathrm{pre}} - z = (ab + a e_{b} + b e_{a} + e_{a} e_{b}) - ab = a e_{b} + b e_{a} + e_{a} e_{b}\n$$\n$$\nJ_{\\mathrm{pre}} = \\mathbb{E}\\big[(y_{\\mathrm{pre}} - z)^{2}\\big] = \\mathbb{E}\\big[(a e_{b} + b e_{a} + e_{a} e_{b})^{2}\\big]\n$$\nTo evaluate this expectation, we expand the squared term:\n$$\n(a e_{b} + b e_{a} + e_{a} e_{b})^{2} = a^{2}e_{b}^{2} + b^{2}e_{a}^{2} + e_{a}^{2}e_{b}^{2} + 2ab e_{a} e_{b} + 2a e_{a} e_{b}^{2} + 2b e_{a}^{2} e_{b}\n$$\nNow, we take the expectation of this expression term by term, leveraging the mutual independence of $a, b, e_{a}, e_{b}$, and the fact that they are all zero-mean signals (except for their variances).\n\nThe expectation of the diagonal terms:\n$$\n\\mathbb{E}[a^{2}e_{b}^{2}] = \\mathbb{E}[a^{2}]\\mathbb{E}[e_{b}^{2}] = \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12}\n$$\n$$\n\\mathbb{E}[b^{2}e_{a}^{2}] = \\mathbb{E}[b^{2}]\\mathbb{E}[e_{a}^{2}] = \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12}\n$$\n$$\n\\mathbb{E}[e_{a}^{2}e_{b}^{2}] = \\mathbb{E}[e_{a}^{2}]\\mathbb{E}[e_{b}^{2}] = \\left(\\frac{\\Delta_{a}^{2}}{12}\\right)\\left(\\frac{\\Delta_{b}^{2}}{12}\\right) = \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThe expectation of the cross-product terms, which all contain at least one first-order moment of a zero-mean random variable:\n$$\n\\mathbb{E}[2ab e_{a} e_{b}] = 2\\mathbb{E}[a]\\mathbb{E}[b]\\mathbb{E}[e_{a}]\\mathbb{E}[e_{b}] = 2(0)(0)(0)(0) = 0\n$$\n$$\n\\mathbb{E}[2a e_{a} e_{b}^{2}] = 2\\mathbb{E}[a]\\mathbb{E}[e_{a}]\\mathbb{E}[e_{b}^{2}] = 2(0)(0)\\left(\\frac{\\Delta_{b}^{2}}{12}\\right) = 0\n$$\n$$\n\\mathbb{E}[2b e_{a}^{2} e_{b}] = 2\\mathbb{E}[b]\\mathbb{E}[e_{a}^{2}]\\mathbb{E}[e_{b}] = 2(0)\\left(\\frac{\\Delta_{a}^{2}}{12}\\right)(0) = 0\n$$\nSumming the expectations of all terms gives the total MSE for the pre-quantization case:\n$$\nJ_{\\mathrm{pre}} = \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12} + \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThis expression is consistent with the requirement to retain terms up to the second order in quantization variances (i.e., products like $\\Delta_{a}^{2}\\Delta_{b}^{2}$).\n\nFinally, we compute the difference in MSE between the two architectures, $\\Delta J = J_{\\mathrm{pre}} - J_{\\mathrm{post}}$:\n$$\n\\Delta J = \\left( \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12} + \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144} \\right) - \\left( \\frac{\\Delta_{z}^{2}}{12} \\right)\n$$\nTo present this as a single simplified expression, we find a common denominator of $144$:\n$$\n\\Delta J = \\frac{12\\sigma_{a}^{2} \\Delta_{b}^{2}}{144} + \\frac{12\\sigma_{b}^{2} \\Delta_{a}^{2}}{144} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144} - \\frac{12\\Delta_{z}^{2}}{144}\n$$\n$$\n\\Delta J = \\frac{12(\\sigma_{a}^{2} \\Delta_{b}^{2} + \\sigma_{b}^{2} \\Delta_{a}^{2} - \\Delta_{z}^{2}) + \\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThis expression represents the difference in mean-squared error between the pre-quantization and post-quantization schemes under the specified assumptions.", "answer": "$$\n\\boxed{\\frac{12(\\sigma_{a}^{2} \\Delta_{b}^{2} + \\sigma_{b}^{2} \\Delta_{a}^{2} - \\Delta_{z}^{2}) + \\Delta_{a}^{2}\\Delta_{b}^{2}}{144}}\n$$", "id": "2893694"}, {"introduction": "Theoretical models of quantization noise are most powerful when they guide practical design decisions, like selecting the minimum bit depth required to meet a performance target. This exercise bridges theory and practice by having you determine the minimum word length for a Finite Impulse Response (FIR) filter to achieve a desired Signal-to-Noise Ratio (SNR). You will first derive this value analytically and then verify your result using Monte Carlo simulations, a standard workflow in modern digital signal processing. [@problem_id:2893697]", "problem": "You are given a finite impulse response implementation scenario for a discrete-time Linear Time-Invariant (LTI) system under product quantization, and you must determine the minimum word length that guarantees a target Signal-to-Noise Ratio (SNR) by analysis, then verify by Monte Carlo simulation the fraction of random trials that meet the target. Use the following fundamental base and definitions only.\n\nAssumptions and definitions:\n- The system is a real-valued finite impulse response filter with impulse response coefficients $\\{h[k]\\}_{k=0}^{N-1}$, producing the ideal output $s[n] = \\sum_{k=0}^{N-1} h[k]\\,x[n-k]$.\n- The system is implemented with product quantization: each product $h[k]\\,x[n-k]$ is individually quantized to $b$ bits by a uniform mid-tread quantizer with range $[-1,1)$ and step size $\\Delta = 2^{1-b}$. The quantized product is $q(h[k]\\,x[n-k])$ and the realized output is $y[n] = \\sum_{k=0}^{N-1} q(h[k]\\,x[n-k])$.\n- Quantization round-off noise model: for each product, the quantization error $e_k[n] = q(h[k]\\,x[n-k]) - h[k]\\,x[n-k]$ is modeled as a white, zero-mean random process independent of the signal and independent across $k$ and $n$, with variance $\\sigma_q^2 = \\Delta^2/12$.\n- Input signal is a real sinusoid $x[n] = A \\sin(\\omega_0 n + \\phi)$ with amplitude $A$ drawn independently and uniformly from $[A_{\\min},A_{\\max}]$ and phase $\\phi$ drawn independently and uniformly from $[0,2\\pi)$. Angles are in radians.\n- The ideal output $s[n]$ is a sinusoid at the same frequency $\\omega_0$ with amplitude $A\\,|H(e^{j\\omega_0})|$, where $H(e^{j\\omega}) = \\sum_{k=0}^{N-1} h[k]\\,e^{-j\\omega k}$ is the Discrete-Time Fourier Transform of the impulse response. The time-average power of a real sinusoid with amplitude $\\alpha$ is $\\alpha^2/2$.\n- The decibel (dB) SNR is defined as $\\mathrm{SNR_{dB}} = 10 \\log_{10}\\left(\\frac{P_{\\text{signal}}}{P_{\\text{noise}}}\\right)$, where $P_{\\text{signal}}$ and $P_{\\text{noise}}$ are the signal and noise powers, respectively.\n\nTasks:\n1) Analytical minimum word length. For each test case, derive from first principles the minimum integer word length $b_{\\min}$ that guarantees the target SNR for a reference amplitude equal to the $p_{\\mathrm{ref}}$-quantile of the amplitude distribution, that is $A_{\\mathrm{ref}} = A_{\\min} + p_{\\mathrm{ref}}(A_{\\max} - A_{\\min})$. Use only the above assumptions and definitions to reason from the base.\n2) Monte Carlo verification. For each test case, run a simulation with $T$ trials. In each trial, draw $A \\sim \\mathcal{U}[A_{\\min},A_{\\max}]$ and $\\phi \\sim \\mathcal{U}[0,2\\pi)$ independently, generate $x[n] = A \\sin(\\omega_0 n + \\phi)$ for $n=0,1,\\dots, M+N-2$, and form both $s[n]$ and $y[n]$ for the $M$ valid output samples according to the described implementation. Estimate the SNR for the trial by the sample-variance ratio $\\widehat{\\mathrm{SNR}} = \\frac{\\mathrm{var}(s)}{\\mathrm{var}(y-s)}$ over the $M$ valid samples. A trial is counted as meeting the target if $10 \\log_{10}(\\widehat{\\mathrm{SNR}}) \\ge \\mathrm{SNR_{dB}^{target}}$. If $\\mathrm{var}(s)=0$ or $\\mathrm{var}(y-s)=0$, count the trial as not meeting the target.\n3) For each test case, report the simulated fraction (a decimal in $[0,1]$) of trials meeting the target when the quantizer uses $b_{\\min}$ bits computed in Task $1$.\n\nUniform quantizer details:\n- Use a mid-tread uniform quantizer with $2^b$ levels over $[-1,1)$ and step size $\\Delta = 2^{1-b}$. Quantization is by rounding to the nearest grid point followed by saturation to the nearest representable level if necessary, i.e., $q(v) = \\mathrm{clip}\\!\\left(\\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{v}{\\Delta}\\right), -1+\\frac{\\Delta}{2}, 1-\\frac{\\Delta}{2}\\right)$.\n\nImportant constraints:\n- Assume $|h[k]| \\le 1$ and $A_{\\max} \\le 1$ so that each unquantized product $h[k]\\,x[n-k]$ lies in $[-1,1]$ and the quantizer does not saturate under ideal conditions.\n- Angles must be treated in radians.\n- All outputs that are percentages must be expressed as decimals in $[0,1]$.\n\nTest suite:\nProvide results for the following three test cases. Each case specifies the impulse response coefficients $\\{h[k]\\}$, the sinusoid frequency $\\omega_0$, the target SNR in decibels, the amplitude range $[A_{\\min},A_{\\max}]$, the amplitude reference quantile $p_{\\mathrm{ref}}$, the number of trials $T$, and the number of valid output samples $M$ to use in each trial.\n\n- Case 1 (happy path):\n  - $h = [0.25,\\,0.25,\\,0.25,\\,0.25]$\n  - $\\omega_0 = 0.2\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 40$\n  - $A_{\\min} = 0.5$, $A_{\\max} = 1.0$\n  - $p_{\\mathrm{ref}} = 0.5$\n  - $T = 600$\n  - $M = 4096$\n\n- Case 2 (frequency response dip coverage):\n  - $h = [1,\\,-1,\\,1,\\,-1,\\,1,\\,-1,\\,1,\\,-1]$\n  - $\\omega_0 = 0.2\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 30$\n  - $A_{\\min} = 0.2$, $A_{\\max} = 0.8$\n  - $p_{\\mathrm{ref}} = 0.5$\n  - $T = 400$\n  - $M = 4096$\n\n- Case 3 (amplitude edge case with zero lower bound):\n  - $h = [0.8,\\,0.6]$\n  - $\\omega_0 = 0.1\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 20$\n  - $A_{\\min} = 0.0$, $A_{\\max} = 1.0$\n  - $p_{\\mathrm{ref}} = 0.9$\n  - $T = 800$\n  - $M = 4096$\n\nDeterminism requirement:\n- Use a fixed pseudorandom generator seed equal to $12345$ to ensure reproducibility.\n\nRequired final output format:\n- Your program should produce a single line of output containing the three simulated fractions, in the order of the test cases, as a comma-separated list enclosed in square brackets, with each fraction rounded to three decimal places. For example, the format must be like $[0.812,0.945,0.903]$.", "solution": "The problem requires the determination of the minimum quantizer word length for a Finite Impulse Response (FIR) filter under product quantization to meet a specific Signal-to-Noise Ratio (SNR), and subsequent verification of this result via Monte Carlo simulation. The problem is valid as it is scientifically grounded in established principles of digital signal processing, is well-posed with all necessary information provided, and is stated objectively.\n\nThe solution is presented in two main parts: first, an analytical derivation of the minimum word length $b_{\\min}$, and second, a description of the Monte Carlo simulation methodology used for verification.\n\n**1. Analytical Derivation of Minimum Word Length ($b_{\\min}$)**\n\nThe goal is to find the minimum integer word length $b$ that satisfies a target SNR, $\\mathrm{SNR_{dB}^{target}}$, for a reference input amplitude $A_{\\mathrm{ref}}$. This involves deriving expressions for the output signal power and the quantization noise power.\n\n**1.1. Output Signal Power ($P_s$)**\nThe input signal is a sinusoid $x[n] = A \\sin(\\omega_0 n + \\phi)$. The LTI system is an FIR filter with impulse response $\\{h[k]\\}_{k=0}^{N-1}$. The ideal output $s[n]$ is the convolution of the input with the impulse response. Due to the properties of LTI systems, a sinusoidal input produces a sinusoidal output at the same frequency, but with modified amplitude and phase. The output amplitude is the input amplitude $A$ multiplied by the magnitude of the filter's frequency response $|H(e^{j\\omega_0})|$ evaluated at the input frequency $\\omega_0$. The frequency response $H(e^{j\\omega})$ is the Discrete-Time Fourier Transform (DTFT) of $h[k]$:\n$$H(e^{j\\omega}) = \\sum_{k=0}^{N-1} h[k] e^{-j\\omega k}$$\nThe output signal is thus $s[n] = A |H(e^{j\\omega_0})| \\sin(\\omega_0 n + \\phi')$, where $\\phi'$ is the new phase. The time-average power of a real sinusoid with amplitude $\\alpha$ is given as $P = \\alpha^2/2$. Therefore, the output signal power is:\n$$P_s = \\frac{\\left( A |H(e^{j\\omega_0})| \\right)^2}{2} = \\frac{A^2 |H(e^{j\\omega_0})|^2}{2}$$\n\n**1.2. Quantization Noise Power ($P_n$)**\nThe filter implementation uses product quantization, where each product term $h[k]x[n-k]$ is quantized before summation. The realized output is $y[n] = \\sum_{k=0}^{N-1} q(h[k]x[n-k])$. The total output error is the difference between the realized and ideal outputs:\n$$e[n] = y[n] - s[n] = \\sum_{k=0}^{N-1} \\left[ q(h[k]x[n-k]) - h[k]x[n-k] \\right] = \\sum_{k=0}^{N-1} e_k[n]$$\nHere, $e_k[n]$ is the quantization error for the $k$-th product at time $n$. According to the problem's statistical model, each $e_k[n]$ is a zero-mean, white random process with variance $\\sigma_q^2 = \\Delta^2/12$, where $\\Delta = 2^{1-b}$ is the quantizer step size. The errors $e_k[n]$ are assumed to be independent for different $k$. The variance of a sum of independent random variables is the sum of their variances. Therefore, the total noise power $P_n$ (which is the variance of $e[n]$) is:\n$$P_n = \\sigma_e^2 = \\mathrm{Var}\\left(\\sum_{k=0}^{N-1} e_k[n]\\right) = \\sum_{k=0}^{N-1} \\mathrm{Var}(e_k[n]) = N \\sigma_q^2$$\nSubstituting the expressions for $\\sigma_q^2$ and $\\Delta$:\n$$P_n = N \\frac{\\Delta^2}{12} = N \\frac{(2^{1-b})^2}{12} = N \\frac{2^{2-2b}}{12} = \\frac{N}{3} 2^{-2b}$$\n\n**1.3. Signal-to-Noise Ratio (SNR) and Minimum Word Length**\nThe SNR in linear scale is the ratio of signal power to noise power:\n$$\\mathrm{SNR} = \\frac{P_s}{P_n} = \\frac{A^2 |H(e^{j\\omega_0})|^2 / 2}{N 2^{-2b} / 3} = \\frac{3 A^2 |H(e^{j\\omega_0})|^2}{2N} 2^{2b}$$\nThe SNR in decibels is $\\mathrm{SNR_{dB}} = 10 \\log_{10}(\\mathrm{SNR})$. We require this to be greater than or equal to the target SNR, $\\mathrm{SNR_{dB}^{target}}$, for the reference amplitude $A = A_{\\mathrm{ref}}$.\n$$10 \\log_{10}(\\mathrm{SNR}) \\ge \\mathrm{SNR_{dB}^{target}} \\implies \\mathrm{SNR} \\ge 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\nSubstituting the expression for SNR and solving for $b$:\n$$\\frac{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2}{2N} 2^{2b} \\ge 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\n$$2^{2b} \\ge \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\nTaking the base-2 logarithm of both sides gives:\n$$2b \\ge \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right)$$\n$$b \\ge \\frac{1}{2} \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right)$$\nSince the word length $b$ must be an integer, the minimum required word length $b_{\\min}$ is the smallest integer that satisfies this inequality:\n$$b_{\\min} = \\left\\lceil \\frac{1}{2} \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right) \\right\\rceil$$\nFor each test case, we compute $A_{\\mathrm{ref}} = A_{\\min} + p_{\\mathrm{ref}}(A_{\\max} - A_{\\min})$, evaluate $|H(e^{j\\omega_0})|^2$, and substitute the given parameters to find $b_{\\min}$.\n\n**2. Monte Carlo Simulation Methodology**\n\nThe simulation aims to verify the analytically derived $b_{\\min}$ by estimating the fraction of random trials that meet the target SNR. The procedure is as follows, repeated for $T$ trials for each test case.\n\n**2.1. Trial Setup**\nIn each trial, an input signal is generated with random parameters. The amplitude $A$ is drawn from a uniform distribution $\\mathcal{U}[A_{\\min},A_{\\max}]$, and the phase $\\phi$ from $\\mathcal{U}[0, 2\\pi)$. A fixed pseudorandom number generator seed ($12345$) is used to ensure reproducibility.\n\n**2.2. Signal Generation**\nFor each trial, the following signals of length $M$ are generated:\n- **Input Signal**: An input sequence $x[n] = A \\sin(\\omega_0 n + \\phi)$ is generated for $n=0, 1, \\dots, M+N-2$. This provides sufficient data for $M$ valid output samples.\n- **Ideal Output**: The ideal output $s[n]$ is computed by convolving the input $x[n]$ with the filter's impulse response $h[k]$. We use the 'valid' part of the convolution, resulting in $M$ samples: $s[j] = \\sum_{k=0}^{N-1} h[k]x[j+k]$ for $j=0, \\dots, M-1$ (after reversing one sequence for standard convolution definition).\n- **Quantized Output**: The quantized output $y[n]$ is generated by simulating the product quantization process. For each output sample $j=0, \\dots, M-1$, the corresponding sum of quantized products is calculated:\n$$y[j+N-1] = \\sum_{k=0}^{N-1} q(h[k]x[j+N-1-k])$$\nThe quantizer function $q(v)$ for a value $v$ is implemented exactly as specified, using the word length $b=b_{\\min}$ derived analytically:\n$$q(v) = \\mathrm{clip}\\!\\left(\\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{v}{\\Delta}\\right), -1+\\frac{\\Delta}{2}, 1-\\frac{\\Delta}{2}\\right) \\quad \\text{with} \\quad \\Delta = 2^{1-b_{\\min}}$$\n\n**2.3. SNR Estimation and Verification**\nFor each trial, the output error sequence is computed as $e[n] = y[n] - s[n]$. The sample variances of the ideal output, $\\mathrm{var}(s)$, and the error, $\\mathrm{var}(y-s)$, are calculated over the $M$ samples. The estimated SNR for the trial is:\n$$\\widehat{\\mathrm{SNR}} = \\frac{\\mathrm{var}(s)}{\\mathrm{var}(y-s)}$$\nA trial is counted as successful if its SNR in decibels meets or exceeds the target, i.e., $10 \\log_{10}(\\widehat{\\mathrm{SNR}}) \\ge \\mathrm{SNR_{dB}^{target}}$. Trials where $\\mathrm{var}(s)=0$ or $\\mathrm{var}(y-s)=0$ are counted as failures.\n\n**2.4. Final Output**\nAfter all $T$ trials for a given test case are complete, the final reported value is the fraction of successful trials, calculated as (number of successes) / $T$. This process is repeated for all three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates the analytical minimum word length and then runs a Monte Carlo\n    simulation to find the fraction of trials meeting the SNR target.\n    \"\"\"\n    # Use a single random number generator for all test cases for reproducibility.\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"h\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"w0\": 0.2 * np.pi,\n            \"snr_db_target\": 40.0,\n            \"A_min\": 0.5,\n            \"A_max\": 1.0,\n            \"p_ref\": 0.5,\n            \"T\": 600,\n            \"M\": 4096,\n        },\n        # Case 2 (frequency response dip coverage)\n        {\n            \"h\": np.array([1, -1, 1, -1, 1, -1, 1, -1]),\n            \"w0\": 0.2 * np.pi,\n            \"snr_db_target\": 30.0,\n            \"A_min\": 0.2,\n            \"A_max\": 0.8,\n            \"p_ref\": 0.5,\n            \"T\": 400,\n            \"M\": 4096,\n        },\n        # Case 3 (amplitude edge case with zero lower bound)\n        {\n            \"h\": np.array([0.8, 0.6]),\n            \"w0\": 0.1 * np.pi,\n            \"snr_db_target\": 20.0,\n            \"A_min\": 0.0,\n            \"A_max\": 1.0,\n            \"p_ref\": 0.9,\n            \"T\": 800,\n            \"M\": 4096,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        h = case[\"h\"]\n        w0 = case[\"w0\"]\n        snr_db_target = case[\"snr_db_target\"]\n        A_min = case[\"A_min\"]\n        A_max = case[\"A_max\"]\n        p_ref = case[\"p_ref\"]\n        T = case[\"T\"]\n        M = case[\"M\"]\n        \n        # --- Task 1: Analytical minimum word length ---\n        A_ref = A_min + p_ref * (A_max - A_min)\n        N = len(h)\n        \n        k_indices = np.arange(N)\n        H_w0 = np.sum(h * np.exp(-1j * w0 * k_indices))\n        H_w0_mag_sq = np.abs(H_w0)**2\n\n        # Handle case where gain is zero. Set b_min to a high value as SNR is -inf.\n        if H_w0_mag_sq == 0 or A_ref == 0:\n            b_min = 32 # A practical large number, problem won't be solvable\n        else:\n            snr_linear_target = 10**(snr_db_target / 10.0)\n            # Argument for log2\n            log_arg = ( (2 * N * snr_linear_target) / \n                        (3 * A_ref**2 * H_w0_mag_sq) )\n            \n            b_min = math.ceil(0.5 * np.log2(log_arg)) if log_arg > 0 else 1\n\n        # --- Task 2: Monte Carlo verification ---\n        \n        # Define the quantizer function based on problem specification\n        delta = 2**(1 - b_min)\n        clip_min = -1 + delta / 2\n        clip_max = 1 - delta / 2\n\n        def quantizer(v):\n            quantized_v = delta * np.round(v / delta)\n            return np.clip(quantized_v, clip_min, clip_max)\n\n        success_count = 0\n        for _ in range(T):\n            A = rng.uniform(A_min, A_max)\n            phi = rng.uniform(0, 2 * np.pi)\n            \n            # Generate signals\n            x_len = M + N - 1\n            n_x = np.arange(x_len)\n            x = A * np.sin(w0 * n_x + phi)\n            \n            # Ideal output\n            s = np.convolve(x, h, mode='valid')\n            \n            # Quantized output (product quantization)\n            y = np.zeros(M)\n            n_out_start = N - 1\n            for k in range(N):\n                n_for_x = np.arange(n_out_start, n_out_start + M) - k\n                x_slice = x[n_for_x]\n                products = h[k] * x_slice\n                quantized_products = quantizer(products)\n                y += quantized_products\n\n            # Calculate estimated SNR\n            error_signal = y - s\n            var_s = np.var(s)\n            var_e = np.var(error_signal)\n            \n            if var_s > 0 and var_e > 0:\n                snr_est = var_s / var_e\n                snr_est_db = 10 * np.log10(snr_est)\n                if snr_est_db >= snr_db_target:\n                    success_count += 1\n            # else: trial fails if variance is zero (treated as not meeting target)\n\n        # --- Task 3: Report simulated fraction ---\n        fraction_success = success_count / T\n        results.append(fraction_success)\n        \n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2893697"}, {"introduction": "Our analytical models, including the one used in the preceding practice, often rely on the convenient assumption that quantization error is an additive, white-noise process. This final exercise challenges you to become a critical practitioner by designing statistical tests to validate this very assumption. By examining the error's autocorrelation and its periodogram, you will learn to identify conditions under which the white noise model is reliable and when it fails, a crucial skill for robust system analysis and debugging. [@problem_id:2893750]", "problem": "You are to design, implement, and validate a statistical detection procedure for non-white quantization error in a simulated product quantization pipeline. The overall goal is to start from core probabilistic and spectral definitions of whiteness and derive acceptance thresholds for two complementary tests: one based on the error autocorrelation and one based on the error periodogram. Your program must implement the tests, apply them to a specified test suite of signal and quantizer configurations, and output a single consolidated decision per configuration.\n\nBegin from the following base principles and definitions, and do not assume any result that is not derivable from them:\n- A discrete-time stochastic process is white if its autocorrelation function is zero for all nonzero lags, and the process is identically distributed across time. Quantization error that is white has vanishing autocorrelation at nonzero lags and a flat power spectral density.\n- The sample autocorrelation at lag $k$ of a zero-mean sequence $e[n]$ of length $N$ is defined from its quadratic form, and portmanteau tests combine multiple sample autocorrelations to test the composite null hypothesis of whiteness.\n- The periodogram of a finite-length realization is the squared magnitude of its Discrete Fourier Transform (DFT) scaled by the sequence length. At Fourier frequencies, for large $N$, the normalized periodogram ordinates of a white sequence behave like realizations of an exponential distribution with unit mean.\n\nYour tasks:\n1. From these bases, derive a statistically valid portmanteau decision rule that rejects the null hypothesis of white quantization error when the aggregate of the first $m$ nonzero sample autocorrelations is unexpectedly large under the null. Specify how to compute the test statistic from $e[n]$, how to approximate its sampling distribution under the null, and how to convert this into a two-sided significance decision at overall level $\\alpha$.\n2. From the same bases, derive a frequency-domain acceptance region for the periodogram of a white sequence by constructing a simultaneous two-sided confidence band at overall level $\\alpha$ across the nonzero positive Fourier frequencies. Use a multiple-comparisons correction that controls the family-wise error rate at $\\alpha$, and explicitly state the null distribution used to obtain the quantiles.\n3. Combine these two decisions into a single detection rule of non-white quantization error: declare “non-white detected” if either test rejects its null hypothesis at level $\\alpha$.\n\nQuantizer and signal models to simulate:\n- Use a midtread uniform quantizer with full-scale range $[-A, A]$ and $b$ bits. The quantization step is $\\Delta = \\dfrac{2A}{2^{b}}$. Implement three quantization modes:\n  - Rounding without dither: $q(x) = \\Delta \\cdot \\mathrm{clip}(\\mathrm{round}(x/\\Delta))$ with saturation to the output range consistent with $b$ and $A$.\n  - Truncation without dither: $q(x) = \\Delta \\cdot \\mathrm{clip}(\\lfloor x/\\Delta \\rfloor)$ with the same saturation rule.\n  - Subtractive dithering with rounding: generate $d[n]$ independently and identically distributed as uniform on $[-\\Delta/2, \\Delta/2]$, form $y[n] = x[n] + d[n]$, quantize $y[n]$ by rounding with saturation to get $\\hat{y}[n]$, and produce the final output $\\hat{x}[n] = \\hat{y}[n] - d[n]$. In all cases define the error as $e[n] = \\hat{x}[n] - x[n]$.\n- Product quantization context: the signal presented to the quantizer is the pointwise product $s[n] = x[n] \\cdot y[n]$ of two discrete-time signals constructed as specified for each test case below.\n\nDecision parameters:\n- Use overall significance level $\\alpha = 0.05$.\n- Use $m = 40$ autocorrelation lags for the portmanteau test.\n- For the periodogram test, evaluate the periodogram at the canonical DFT frequencies and construct a simultaneous two-sided acceptance band at overall level $\\alpha$ across all strictly positive Fourier frequencies below the Nyquist frequency. Exclude the zero-frequency bin and the Nyquist bin if $N$ is even.\n\nTest suite:\nImplement the simulation and detection pipeline for the following three configurations. For all cases, set the random number generator with the specified seed at the beginning of the case, generate a realization of length $N$, and use angles in radians per sample.\n\n- Case 1 (nominal white error under subtractive dithering):\n  - Seed: $12345$.\n  - Length: $N = 8192$.\n  - Full-scale: $A = 1.0$.\n  - Bits: $b = 10$.\n  - Quantization mode: subtractive dithering with rounding as defined above.\n  - Inputs: $x[n] = 0.9 \\sin(0.2\\, n)$ and $y[n]$ independent and identically distributed Gaussian with zero mean and standard deviation $0.1$. Signal to be quantized: $s[n] = x[n] \\cdot y[n]$.\n\n- Case 2 (coarse rounding without dither, periodic product):\n  - Seed: $2024$.\n  - Length: $N = 8192$.\n  - Full-scale: $A = 1.0$.\n  - Bits: $b = 4$.\n  - Quantization mode: rounding without dither.\n  - Inputs: $x[n] = 0.9 \\sin(0.01\\, n)$ and $y[n] = 0.9 \\sin(0.21\\, n)$. Signal to be quantized: $s[n] = x[n] \\cdot y[n]$.\n\n- Case 3 (truncation without dither, slowly varying correlated product):\n  - Seed: $777$.\n  - Length: $N = 8192$.\n  - Full-scale: $A = 1.0$.\n  - Bits: $b = 5$.\n  - Quantization mode: truncation without dither.\n  - Inputs: $x[n]$ and $y[n]$ are independent, stationary first-order autoregressive processes defined by $x[n] = \\phi\\, x[n-1] + u_x[n]$ and $y[n] = \\phi\\, y[n-1] + u_y[n]$ with $\\phi = 0.999$, where $u_x[n]$ and $u_y[n]$ are independent and identically distributed Gaussian with zero mean and variance chosen so that the stationary standard deviation of both $x[n]$ and $y[n]$ is $0.3$. Signal to be quantized: $s[n] = x[n] \\cdot y[n]$.\n\nProgramming requirements:\n- Implement the two tests and the combined decision using the derivations you establish from the bases stated above. For the periodogram-based band, estimate the variance of $e[n]$ from the realization and normalize the periodogram ordinates by this estimate before using the asymptotic null distribution to set the acceptance band with a family-wise error rate of $\\alpha$ via a suitable multiple-comparisons correction.\n- For each case, return a boolean value that is True if non-white quantization error is detected and False otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of three boolean values corresponding to the three cases in the order given (for example, \"[False,True,True]\").\n\nThere are no physical units to report beyond the angular specification in radians per sample. All angles must be interpreted in radians per sample. All probabilities and significance levels must be interpreted as decimals (for example, $0.05$ is a decimal).", "solution": "We outline the solution by deriving the decision rules from first principles and then specifying their algorithmic implementation.\n\n1. Portmanteau test based on autocorrelation.\nLet $e[n]$ denote the quantization error sequence of length $N$. Under the null hypothesis of white quantization error, $e[n]$ is a discrete-time, zero-mean, identically distributed process with autocorrelation function $R_e[k] = \\mathbb{E}\\{e[n] e[n-k]\\} = 0$ for all nonzero integer lags $k \\neq 0$. The sample autocorrelation at lag $k$ is\n$$\n\\hat{r}[k] = \\frac{\\sum_{n=k}^{N-1} \\tilde{e}[n] \\tilde{e}[n-k]}{\\sum_{n=0}^{N-1} \\tilde{e}[n]^2},\n$$\nwhere $\\tilde{e}[n] = e[n] - \\bar{e}$ and $\\bar{e}$ is the sample mean. For large $N$, under the null hypothesis that $e[n]$ is white and identically distributed with finite variance, the vector $(\\hat{r}[1], \\ldots, \\hat{r}[m])$ is approximately jointly Gaussian with zero mean. A classical portmanteau statistic that aggregates these correlations is the Ljung–Box (LB) statistic,\n$$\nQ = N(N+2)\\sum_{k=1}^{m} \\frac{\\hat{r}[k]^2}{N-k},\n$$\nwhich, under the null for large $N$, is approximately distributed as a chi-square random variable with $m$ degrees of freedom. At overall significance $\\alpha$, we reject the null of whiteness if the upper-tail probability $\\mathbb{P}(\\chi^2_m \\ge Q)$ is less than $\\alpha$. Equivalently, compute the $p$-value $p_{\\mathrm{LB}} = 1 - F_{\\chi^2_m}(Q)$ and reject if $p_{\\mathrm{LB}} < \\alpha$.\n\n2. Periodogram-based simultaneous confidence band.\nThe periodogram of $\\tilde{e}[n]$ is defined at the DFT frequencies $\\omega_k = 2\\pi k/N$ by\n$$\nI(\\omega_k) = \\frac{1}{N}\\left|\\sum_{n=0}^{N-1} \\tilde{e}[n] e^{-j\\omega_k n}\\right|^2.\n$$\nFor real-valued $\\tilde{e}[n]$, we consider the strictly positive frequencies below Nyquist: $k = 1, 2, \\ldots, \\lfloor (N-1)/2 \\rfloor$. If $e[n]$ is white with variance $\\sigma^2$, then for large $N$ and at the canonical Fourier frequencies, the periodogram ordinates are approximately independent and satisfy\n$$\n\\frac{I(\\omega_k)}{\\sigma^2} \\stackrel{\\mathrm{approx}}{\\sim} \\mathrm{Exp}(1),\n$$\nthat is, exponential with unit mean. In practice, $\\sigma^2$ is unknown and we estimate it by the sample variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{n=0}^{N-1} \\tilde{e}[n]^2$. Using the large-sample approximation, the normalized ordinates $Z_k = I(\\omega_k)/\\hat{\\sigma}^2$ are close in distribution to $\\mathrm{Exp}(1)$. To construct a simultaneous two-sided acceptance band at family-wise error rate $\\alpha$ across $M$ frequencies, we use a Bonferroni correction. For two-sided control, assign tail probability $\\alpha/(2M)$ to each tail at each frequency. If $X \\sim \\mathrm{Exp}(1)$, then $\\mathbb{P}(X \\le x) = 1 - e^{-x}$, so the lower and upper quantiles at tail probability $\\alpha/(2M)$ are\n$$\nq_{\\mathrm{low}} = -\\ln\\left(1 - \\frac{\\alpha}{2M}\\right), \\quad q_{\\mathrm{high}} = -\\ln\\left(\\frac{\\alpha}{2M}\\right).\n$$\nForm the acceptance band $[q_{\\mathrm{low}}, q_{\\mathrm{high}}]$ on $Z_k$. We accept the null if all $Z_k$ for $k$ in the strictly positive set lie within this band. Otherwise we reject.\n\n3. Combined decision.\nDefine the detection decision as\n$$\n\\mathrm{nonwhite} = \\left( p_{\\mathrm{LB}} < \\alpha \\right) \\ \\lor \\ \\left( \\exists k \\in \\{1,\\ldots,M\\} \\ \\text{s.t.} \\ Z_k \\notin [q_{\\mathrm{low}}, q_{\\mathrm{high}}] \\right).\n$$\n\n4. Quantizer implementation details.\nLet the full-scale range be $[-A, A]$ with $b$ bits and quantization step $\\Delta = 2A/2^{b}$. Let $\\mathrm{clip}(\\cdot)$ enforce saturation to the representable output levels. For rounding without dither, compute $\\hat{x}[n] = \\Delta \\cdot \\mathrm{clip}(\\mathrm{round}(x[n]/\\Delta))$, and for truncation without dither, compute $\\hat{x}[n] = \\Delta \\cdot \\mathrm{clip}(\\lfloor x[n]/\\Delta \\rfloor)$. For subtractive dithering, draw $d[n] \\sim \\mathrm{Uniform}(-\\Delta/2, \\Delta/2)$ independently, form $y[n] = x[n] + d[n]$, compute $\\hat{y}[n]$ by rounding with saturation, and then set $\\hat{x}[n] = \\hat{y}[n] - d[n]$. In all cases define the error $e[n] = \\hat{x}[n] - x[n]$ and remove its sample mean before testing.\n\n5. Signals and parameters.\n- Case $1$: $N = 8192$, $A = 1.0$, $b = 10$, subtractive dithering with rounding, $x[n] = 0.9 \\sin(0.2 n)$, $y[n] \\sim \\mathcal{N}(0, 0.1^2)$ independent across $n$, seed $12345$. The product $s[n] = x[n] y[n]$ is quantized to produce the error sequence. Under subtractive dithering, theory for high-resolution quantization predicts error close to white and independent of the input, so both tests should accept the null with high probability.\n- Case $2$: $N = 8192$, $A = 1.0$, $b = 4$, rounding without dither, $x[n] = 0.9 \\sin(0.01 n)$, $y[n] = 0.9 \\sin(0.21 n)$, seed $2024$. The product of two sinusoids is a sum of sinusoids at sum and difference frequencies; coarse quantization without dither introduces signal-dependent structure in the error, leading to nonzero autocorrelations and spectral lines, so at least one test should reject.\n- Case $3$: $N = 8192$, $A = 1.0$, $b = 5$, truncation without dither, $x[n]$ and $y[n]$ are independent autoregressive of order one (AR(1)) processes with parameter $\\phi = 0.999$ and stationary standard deviation $0.3$ (achieved by setting the innovation variance to $\\sigma_u^2 = (1-\\phi^2)\\cdot 0.3^2$), seed $777$. The slowly varying product yields error sequences under truncation that exhibit correlation due to threshold sticking and bias, so the portmanteau test should reject, and the combined rule should detect non-whiteness.\n\n6. Implementation summary and expected outcomes.\n- Compute $\\hat{r}[k]$ for $k=1,\\ldots,m$ on the zero-mean error, evaluate the Ljung–Box statistic $Q$, and compute the $p$-value against the chi-square distribution with $m$ degrees of freedom. Reject if $p < \\alpha$ with $\\alpha = 0.05$ and $m = 40$.\n- Compute the periodogram at the DFT frequencies and form $Z_k = I(\\omega_k)/\\hat{\\sigma}^2$ for the strictly positive frequencies below Nyquist. With $M$ bins, compute $q_{\\mathrm{low}}$ and $q_{\\mathrm{high}}$ as above with a Bonferroni correction and accept if all $Z_k$ lie within the band.\n- Declare non-white detected if either test rejects.\n\nGiven the structure of the cases, the expected logical outcomes are: Case $1$: False (do not detect non-white), Case $2$: True (detect non-white), Case $3$: True (detect non-white).\n\nThe program will generate the three cases with the specified seeds, apply the combined decision rule, and print a single line with a list of three boolean values in the specified order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef rng(seed):\n    rs = np.random.RandomState(seed)\n    return rs\n\ndef make_quantizer(A, b, mode, rs=None):\n    \"\"\"\n    mode: 'round', 'trunc', or 'sub_dither_round'\n    Returns a function q(x) that applies quantization and returns quantized output and error.\n    \"\"\"\n    L = 2**b\n    delta = 2 * A / L\n    # Determine representable indices for saturation with midtread quantizer\n    # Midtread with rounding: indices from -L/2 to L/2 inclusive? For symmetry and to avoid out-of-range,\n    # we saturate to indices in [-L/2, L/2 - 1] so that output remains within [-A, A - delta].\n    min_index = -L // 2\n    max_index = L // 2 - 1\n\n    def clip_indices(idx):\n        return np.clip(idx, min_index, max_index)\n\n    if mode == 'round':\n        def q(x):\n            idx = np.round(x / delta)\n            idx = clip_indices(idx)\n            y = delta * idx\n            e = y - x\n            return y, e\n        return q, delta\n    elif mode == 'trunc':\n        def q(x):\n            idx = np.floor(x / delta)\n            idx = clip_indices(idx)\n            y = delta * idx\n            e = y - x\n            return y, e\n        return q, delta\n    elif mode == 'sub_dither_round':\n        if rs is None:\n            raise ValueError(\"RandomState required for subtractive dithering.\")\n        def q(x):\n            d = rs.uniform(low=-delta/2.0, high=delta/2.0, size=x.shape)\n            y_in = x + d\n            idx = np.round(y_in / delta)\n            idx = clip_indices(idx)\n            yq = delta * idx\n            y = yq - d\n            e = y - x\n            return y, e\n        return q, delta\n    else:\n        raise ValueError(\"Unknown quantizer mode.\")\n\ndef ljung_box_test(e_zero_mean, m, alpha):\n    \"\"\"\n    Compute Ljung-Box Q statistic and p-value for whiteness up to lag m.\n    Returns (reject_boolean, p_value).\n    \"\"\"\n    N = len(e_zero_mean)\n    # Autocorrelations up to lag m\n    # Use unbiased estimator normalized by r0\n    r0 = np.dot(e_zero_mean, e_zero_mean)\n    if r0 == 0:\n        # Degenerate error (all zeros); treat as not rejecting\n        return False, 1.0\n    Q = 0.0\n    for k in range(1, m + 1):\n        # sum_{n=k}^{N-1} e[n]*e[n-k]\n        num = np.dot(e_zero_mean[k:], e_zero_mean[:-k])\n        rk = num / r0\n        Q += (rk * rk) / (N - k)\n    Q *= N * (N + 2)\n    # p-value upper tail\n    p_value = chi2.sf(Q, df=m)\n    reject = p_value  alpha\n    return reject, p_value\n\ndef periodogram_band_test(e_zero_mean, alpha):\n    \"\"\"\n    Build Bonferroni-corrected two-sided simultaneous band on normalized periodogram.\n    Returns (reject_boolean, (q_low,q_high), num_outliers).\n    \"\"\"\n    N = len(e_zero_mean)\n    # FFT and periodogram\n    E = np.fft.rfft(e_zero_mean)  # includes DC and Nyquist (if even N)\n    # Frequencies: bins 0..N//2\n    # Exclude k=0 (DC), and if N even, exclude k=N//2 (Nyquist)\n    if N % 2 == 0:\n        k_start = 1\n        k_end = (N // 2)  # exclusive\n        E_pos = E[k_start:k_end]\n    else:\n        k_start = 1\n        k_end = (N // 2) + 1\n        E_pos = E[k_start:k_end]\n    M = len(E_pos)\n    if M == 0:\n        # Too short; cannot test\n        return False, (0.0, np.inf), 0\n    I = (np.abs(E_pos) ** 2) / N\n    sigma2_hat = np.var(e_zero_mean, ddof=0)\n    if sigma2_hat = 0:\n        return False, (0.0, np.inf), 0\n    Z = I / sigma2_hat\n    # Bonferroni two-sided correction\n    tail = alpha / (2.0 * M)\n    # guard against numerical underflow/overflow in logs\n    tail = max(min(tail, 1 - 1e-15), 1e-15)\n    q_low = -np.log(1.0 - tail)\n    q_high = -np.log(tail)\n    outliers = np.logical_or(Z  q_low, Z > q_high)\n    reject = np.any(outliers)\n    return reject, (q_low, q_high), int(np.sum(outliers))\n\ndef generate_case(case_id):\n    \"\"\"\n    Returns tuple (error_sequence_zero_mean, alpha, m) for a given case.\n    \"\"\"\n    alpha = 0.05\n    m = 40\n\n    if case_id == 1:\n        seed = 12345\n        rs = rng(seed)\n        N = 8192\n        A = 1.0\n        b = 10\n        # Signals\n        n = np.arange(N, dtype=float)\n        x = 0.9 * np.sin(0.2 * n)  # radians per sample\n        y = rs.normal(loc=0.0, scale=0.1, size=N)\n        s = x * y\n        # Quantizer with subtractive dither and rounding\n        q, _ = make_quantizer(A=A, b=b, mode='sub_dither_round', rs=rs)\n        yq, e = q(s)\n        e = e - np.mean(e)\n        return e, alpha, m\n\n    elif case_id == 2:\n        seed = 2024\n        rs = rng(seed)\n        N = 8192\n        A = 1.0\n        b = 4\n        n = np.arange(N, dtype=float)\n        x = 0.9 * np.sin(0.01 * n)\n        y = 0.9 * np.sin(0.21 * n)\n        s = x * y\n        q, _ = make_quantizer(A=A, b=b, mode='round', rs=rs)\n        yq, e = q(s)\n        e = e - np.mean(e)\n        return e, alpha, m\n\n    elif case_id == 3:\n        seed = 777\n        rs = rng(seed)\n        N = 8192\n        A = 1.0\n        b = 5\n        phi = 0.999\n        target_std = 0.3\n        sigma_u = target_std * np.sqrt(1 - phi**2)\n        # Generate AR(1)\n        def ar1(N, phi, sigma_u, rs):\n            x = np.zeros(N, dtype=float)\n            u = rs.normal(loc=0.0, scale=sigma_u, size=N)\n            for n in range(1, N):\n                x[n] = phi * x[n-1] + u[n]\n            # For n=0, x[0] = 0 initial; transient negligible for large N\n            return x\n        x = ar1(N, phi, sigma_u, rs)\n        y = ar1(N, phi, sigma_u, rs)\n        s = x * y\n        q, _ = make_quantizer(A=A, b=b, mode='trunc', rs=rs)\n        yq, e = q(s)\n        e = e - np.mean(e)\n        return e, alpha, m\n\n    else:\n        raise ValueError(\"Unknown case id\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for cid in test_cases:\n        e, alpha, m = generate_case(cid)\n        # Perform Ljung-Box test\n        lb_reject, lb_p = ljung_box_test(e, m=m, alpha=alpha)\n        # Perform periodogram band test\n        per_reject, band, num_out = periodogram_band_test(e, alpha=alpha)\n        # Combined decision\n        nonwhite_detected = bool(lb_reject or per_reject)\n        results.append(nonwhite_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2893750"}]}