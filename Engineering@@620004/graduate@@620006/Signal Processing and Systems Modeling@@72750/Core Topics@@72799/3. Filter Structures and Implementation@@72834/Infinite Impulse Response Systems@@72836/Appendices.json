{"hands_on_practices": [{"introduction": "The impulse response $h[n]$ is the fundamental signature of a linear time-invariant system, and deriving it from a transfer function $H(z)$ is a cornerstone of system analysis. This exercise guides you through a first-principles derivation of the impulse response for a general IIR system, paying special attention to the case of repeated poles. By building the solution from the basic geometric series, you will gain a deeper intuition for how the algebraic structure of $H(z)$ in the Z-domain dictates the polynomial-exponential form of the system's behavior in the time domain [@problem_id:2878198].", "problem": "Consider a causal Linear Time-Invariant (LTI) discrete-time Infinite Impulse Response (IIR) system with transfer function given in partial-fraction form by\n$$\nH(z) \\;=\\; \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} \\frac{c_{k,m}}{\\left(1 - p_k z^{-1}\\right)^{m}},\n$$\nwhere the distinct poles are $\\{p_k\\}_{k=1}^{K}$ with $|p_k|<1$ for all $k$, each with multiplicity $M_k \\in \\mathbb{N}$, and complex coefficients $\\{c_{k,m}\\}$. Assume the region of convergence is $|z|>\\max_k |p_k|$, so the system is causal and stable. Starting from first principles, namely the definition of the bilateral $Z$-transform $X(z) = \\sum_{n=-\\infty}^{\\infty} x[n] z^{-n}$ and fundamental series identities derivable from the geometric series $\\sum_{n=0}^{\\infty} r^{n} = \\frac{1}{1-r}$ for $|r|<1$, derive the explicit time-domain impulse response $h[n]$ as a closed-form expression in terms of $\\{c_{k,m}\\}$, $\\{p_k\\}$, and $n$, making clear the polynomial-in-$n$ factors that arise from repeated poles (that is, $m>1$). Your derivation should justify the appearance and the exact form of these polynomial factors from first principles, without invoking pre-memorized transform pairs for repeated poles.\n\nExpress your final answer as a single analytic expression for $h[n]$ that is valid for all integer $n$, using the unit-step sequence $u[n]$. Do not provide intermediate steps in the final answer. No numerical evaluation is required.", "solution": "The problem presented is a standard theoretical exercise in the analysis of discrete-time linear time-invariant systems. A rigorous validation of the problem statement is a non-negotiable prerequisite to any analytical work.\n\nFirst, we extract the given information verbatim.\n- **System**: Causal, Linear Time-Invariant (LTI), discrete-time, Infinite Impulse Response (IIR).\n- **Transfer Function**: $H(z) \\;=\\; \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} \\frac{c_{k,m}}{\\left(1 - p_k z^{-1}\\right)^{m}}$.\n- **Poles**: $\\{p_k\\}_{k=1}^{K}$ are distinct complex numbers with $|p_k|<1$.\n- **Multiplicities**: $M_k \\in \\mathbb{N}$ is the multiplicity of pole $p_k$.\n- **Coefficients**: $\\{c_{k,m}\\}$ are complex numbers.\n- **Region of Convergence (ROC)**: $|z|>\\max_k |p_k|$.\n- **Fundamental Postulates**:\n    1.  Bilateral $Z$-transform: $X(z) = \\sum_{n=-\\infty}^{\\infty} x[n] z^{-n}$.\n    2.  Geometric series: $\\sum_{n=0}^{\\infty} r^{n} = \\frac{1}{1-r}$ for $|r|<1$.\n- **Objective**: Derive the impulse response $h[n]$ from first principles, explaining the origin of polynomial factors for repeated poles. The result must be a closed-form expression using the unit-step sequence $u[n]$, defined as $1$ for $n \\ge 0$ and $0$ for $n < 0$.\n\nNext, we validate the problem statement.\nThe problem is scientifically grounded in the established theory of digital signal processing. All terms and concepts, such as the $Z$-transform, LTI systems, causality, stability, and partial fraction expansion, are standard and well-defined. The structure of the problem is that of a direct mathematical derivation. The given conditions—$|p_k|<1$ and an ROC of $|z| > \\max_k|p_k|$—correctly correspond to a causal and stable system. The problem is self-contained, consistent, and well-posed, asking for a unique and meaningful solution derivable from the premises. It is neither trivial nor pseudo-profound, as the derivation for repeated poles from first principles requires non-obvious steps. The problem is therefore valid.\n\nWe proceed with the derivation.\n\nThe objective is to find the impulse response $h[n]$, which is the inverse $Z$-transform of the transfer function $H(z)$. The $Z$-transform is a linear operator. Consequently, the inverse transform of the sum is the sum of the inverse transforms:\n$$ h[n] = \\mathcal{Z}^{-1}\\{H(z)\\} = \\mathcal{Z}^{-1}\\left\\{ \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} \\frac{c_{k,m}}{\\left(1 - p_k z^{-1}\\right)^{m}} \\right\\} = \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} c_{k,m} \\mathcal{Z}^{-1}\\left\\{ \\frac{1}{\\left(1 - p_k z^{-1}\\right)^{m}} \\right\\} $$\nOur task reduces to finding the inverse $Z$-transform of the general term $G_{k,m}(z) = \\left(1 - p_k z^{-1}\\right)^{-m}$.\n\nLet us begin with the fundamental case where $m=1$. We need to find the inverse transform of $G_{k,1}(z) = \\frac{1}{1-p_k z^{-1}}$. We are given the geometric series identity:\n$$ \\sum_{n=0}^{\\infty} r^n = \\frac{1}{1-r}, \\quad \\text{for } |r|<1 $$\nLet us set $r = p_k z^{-1}$. The condition for convergence becomes $|p_k z^{-1}| < 1$, or $|z| > |p_k|$. This is consistent with the given ROC for a causal system, as $|z| > \\max_j |p_j| \\ge |p_k|$. Substituting $r = p_k z^{-1}$ into the series identity gives:\n$$ G_{k,1}(z) = \\frac{1}{1-p_k z^{-1}} = \\sum_{n=0}^{\\infty} (p_k z^{-1})^n = \\sum_{n=0}^{\\infty} (p_k^n) z^{-n} $$\nBy definition, the $Z$-transform of a sequence $x[n]$ is $X(z) = \\sum_{n=-\\infty}^{\\infty} x[n] z^{-n}$. Comparing this definition with our series expansion, we identify the corresponding time-domain sequence. The sequence is $p_k^n$ for $n \\ge 0$ and $0$ otherwise. This is precisely $p_k^n u[n]$. Thus, we have the transform pair:\n$$ p_k^n u[n] \\quad \\overset{\\mathcal{Z}}{\\longleftrightarrow} \\quad \\frac{1}{1-p_k z^{-1}} $$\n\nNow, we must address the case of repeated poles, where $m>1$. The problem forbids the use of pre-memorized transform pairs. We must derive the result from first principles. A rigorous method is to differentiate the geometric series identity with respect to the parameter $r$. Differentiation of a power series term-by-term is valid within its radius of convergence.\nStarting with the identity:\n$$ (1-r)^{-1} = \\sum_{n=0}^{\\infty} r^n $$\nDifferentiating both sides with respect to $r$ gives:\n$$ \\frac{d}{dr}(1-r)^{-1} = (-1)(1-r)^{-2}(-1) = (1-r)^{-2} $$\n$$ \\frac{d}{dr}\\sum_{n=0}^{\\infty} r^n = \\sum_{n=0}^{\\infty} n r^{n-1} = \\sum_{n=1}^{\\infty} n r^{n-1} $$\nLet's shift the index of summation by letting $j=n-1$, so $n=j+1$. The sum becomes $\\sum_{j=0}^{\\infty} (j+1) r^j$. Using $n$ as the index again, we have:\n$$ (1-r)^{-2} = \\sum_{n=0}^{\\infty} (n+1) r^n $$\nThis result can be generalized by repeated differentiation. The $(m-1)$-th derivative of $(1-r)^{-1}$ with respect to $r$ is:\n$$ \\frac{d^{m-1}}{dr^{m-1}}(1-r)^{-1} = (m-1)! (1-r)^{-m} $$\nApplying the same differentiation to the series side:\n$$ \\frac{d^{m-1}}{dr^{m-1}}\\sum_{n=0}^{\\infty} r^n = \\sum_{n=m-1}^{\\infty} n(n-1)\\cdots(n-m+2) r^{n-(m-1)} $$\nThe product of descending integers can be expressed using factorials and binomial coefficients:\n$$ n(n-1)\\cdots(n-m+2) = \\frac{n!}{(n-(m-1))!} = (m-1)! \\binom{n}{m-1} $$\nSo we have:\n$$ (m-1)! (1-r)^{-m} = \\sum_{n=m-1}^{\\infty} (m-1)! \\binom{n}{m-1} r^{n-m+1} $$\nDividing by $(m-1)!$ and changing the index of summation to $j = n-(m-1)$, so $n=j+m-1$:\n$$ (1-r)^{-m} = \\sum_{j=0}^{\\infty} \\binom{j+m-1}{m-1} r^j $$\nThe term $\\binom{j+m-1}{m-1} = \\frac{(j+m-1)!}{j!(m-1)!}$ is a polynomial in $j$ of degree $m-1$, which is the source of the polynomial factor requested in the problem statement.\nNow, we substitute $r = p_k z^{-1}$ back into this generalized identity. Using $n$ as the summation index:\n$$ \\frac{1}{(1-p_k z^{-1})^{m}} = \\sum_{n=0}^{\\infty} \\binom{n+m-1}{m-1} (p_k z^{-1})^n = \\sum_{n=0}^{\\infty} \\left[ \\binom{n+m-1}{m-1} p_k^n \\right] z^{-n} $$\nAgain, by comparison with the definition of the $Z$-transform, we deduce the inverse transform. The sequence is non-zero only for $n \\ge 0$.\n$$ \\binom{n+m-1}{m-1} p_k^n u[n] \\quad \\overset{\\mathcal{Z}}{\\longleftrightarrow} \\quad \\frac{1}{\\left(1-p_k z^{-1}\\right)^{m}} $$\nThe derivation from first principles is complete.\n\nFinally, we assemble the total impulse response $h[n]$ by summing the contributions from all terms in the partial fraction expansion:\n$$ h[n] = \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} c_{k,m} \\left( \\binom{n+m-1}{m-1} p_k^n u[n] \\right) $$\nSince the unit step $u[n]$ is a common factor for all terms (a consequence of causality), it can be factored out of the summation:\n$$ h[n] = \\left( \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} c_{k,m} \\binom{n+m-1}{m-1} p_k^n \\right) u[n] $$\nThis is the final closed-form expression for the impulse response, valid for all integers $n$.", "answer": "$$\\boxed{\nh[n] = \\left( \\sum_{k=1}^{K} \\sum_{m=1}^{M_k} c_{k,m} \\binom{n+m-1}{m-1} p_k^n \\right) u[n]\n}$$", "id": "2878198"}, {"introduction": "Moving from ideal theory to real-world implementation reveals the crucial effects of finite precision. This practice tackles one of the most distinctive phenomena in recursive digital filters: zero-input limit cycles, which arise from quantization nonlinearities in the feedback path. By analyzing the conditions that sustain these small-scale oscillations and deriving the \"dead-band\" within which the state correctly decays to zero, you will develop a critical understanding of the non-ideal behaviors that must be managed in practical hardware implementations [@problem_id:2878204].", "problem": "Consider a causal, stable, second-order infinite impulse response (IIR) section implemented in fixed-point arithmetic with rounding-to-nearest. Let the rounding quantizer be denoted by $Q_{\\Delta}(\\cdot)$ with quantization step size $\\Delta&gt;0$, defined by mapping any $x\\in\\mathbb{R}$ to the nearest integer multiple of $\\Delta$, with ties rounded away from zero. Assume zero input for all times. The implementation is described by the nonlinear recursion\n$$\nx[n] \\;=\\; Q_{\\Delta}\\!\\left(-a_{1}\\,x[n-1] \\;-\\; a_{2}\\,x[n-2]\\right), \\qquad y[n] \\;=\\; x[n],\n$$\nwhere $a_{1},a_{2}\\in\\mathbb{R}$ are such that the corresponding linear recursion without quantization is asymptotically stable, and where initial conditions $x[-1]$ and $x[-2]$ are representable fixed-point values (i.e., integer multiples of $\\Delta$).\n\n(1) Using only fundamental properties of fixed-point rounding quantizers and deterministic discrete-time dynamical systems on finite sets, argue from first principles why this IIR section can exhibit nontrivial zero-input limit cycles.\n\n(2) Define the symmetric dead-band as the largest radius $R(a_{1},a_{2},\\Delta)&gt;0$ with the following property: for any initial fixed-point conditions satisfying $\\lvert x[-1]\\rvert\\leq R$ and $\\lvert x[-2]\\rvert\\leq R$, the output sequence satisfies $y[n]=0$ for all $n\\geq 0$. Derive, from first principles, an exact analytic expression for the supremal symmetric dead-band radius $R(a_{1},a_{2},\\Delta)$ as a function of $a_{1}$, $a_{2}$, and $\\Delta$. Your final answer must be a single closed-form expression. Do not assume any particular realization other than the given recursion, and do not invoke any precomputed formulas; start from the definitions and properties stated above.\n\nNo numerical approximation is required in this problem, and no units are involved. Express your final result as a symbolic function of $a_{1}$, $a_{2}$, and $\\Delta$.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a rigorous solution. It is a standard problem in the analysis of quantization effects in digital filters. Thus, it is deemed valid. We proceed with the solution.\n\nThe system is described by the nonlinear difference equation:\n$$x[n] = Q_{\\Delta}(-a_1 x[n-1] - a_2 x[n-2])$$\nwhere $Q_{\\Delta}(\\cdot)$ is a quantizer with step size $\\Delta > 0$ that rounds to the nearest integer multiple of $\\Delta$. The tie-breaking rule specifies rounding away from zero. This implies that for any a real value $v$, $Q_{\\Delta}(v) = 0$ if and only if $|v| < \\frac{\\Delta}{2}$. If $|v| \\ge \\frac{\\Delta}{2}$, then $|Q_{\\Delta}(v)| \\ge \\Delta$. The initial conditions $x[-1]$ and $x[-2]$ are integer multiples of $\\Delta$.\n\n(1) Argument for the existence of nontrivial zero-input limit cycles.\n\nThe state of the system at time $n$ is defined by the pair of previous values, $\\mathbf{s}[n] = (x[n-1], x[n-2])$. The recursion provides a deterministic rule for transitioning from state $\\mathbf{s}[n]$ to $\\mathbf{s}[n+1] = (x[n], x[n-1])$.\n\nThe underlying linear system, $x_{\\text{lin}}[n] = -a_1 x_{\\text{lin}}[n-1] - a_2 x_{\\text{lin}}[n-2]$, is stipulated to be asymptotically stable. This means that for any initial conditions, the state of the linear system converges to zero, i.e., $\\lim_{n \\to \\infty} x_{\\text{lin}}[n] = 0$. Consequently, the state trajectory is bounded.\n\nIn the nonlinear system with quantization, the state values $x[n]$ are always integer multiples of $\\Delta$. The stability of the linear counterpart suggests that for any given initial state, the trajectory of the nonlinear system will not diverge. The state will eventually enter and remain within a bounded region of the state space. Let this region be defined by $|x[n]| \\le M$ for some constant $M > 0$.\n\nWithin this bounded region, the number of possible values for $x[n]$ is finite, as they must be integer multiples of $\\Delta$ in the interval $[-M, M]$. Specifically, the number of such values is $2\\lfloor \\frac{M}{\\Delta} \\rfloor + 1$. The number of possible states $\\mathbf{s}[n] = (x[n-1], x[n-2])$ is therefore also finite, bounded by $(2\\lfloor \\frac{M}{\\Delta} \\rfloor + 1)^2$.\n\nThe system's evolution is governed by a deterministic function on this finite state space. By the pigeonhole principle, any deterministic system evolving on a finite state space must eventually revisit a state. Once a state is repeated, the system's trajectory becomes periodic, as the sequence of subsequent states is uniquely determined. This periodic trajectory is, by definition, a limit cycle.\n\nSuch a limit cycle can be nontrivial (i.e., not the zero state where $x[n]=0$ for all $n$ in the cycle). The zero state $(0,0)$ is a fixed point, since $Q_{\\Delta}(-a_1 \\cdot 0 - a_2 \\cdot 0) = 0$. However, the system may not reach this state. Consider a small, non-zero state $(x[n-1], x[n-2])$. The argument of the quantizer is $v[n] = -a_1 x[n-1] - a_2 x[n-2]$. While the linear dynamics would drive $v[n]$ toward zero, the quantization introduces a \"floor\" effect. If at some point $|v[n]| \\ge \\frac{\\Delta}{2}$, the output $x[n]$ will be a non-zero multiple of $\\Delta$, with magnitude at least $\\Delta$. This can prevent the state from ever reaching zero, sustaining a small, persistent oscillation. The interplay between the contracting nature of the stable linear dynamics and the expanding/maintaining nature of the quantization for inputs of magnitude greater than or equal to $\\frac{\\Delta}{2}$ is what enables the existence of these nontrivial, self-sustaining oscillations.\n\n(2) Derivation of the supremal symmetric dead-band radius $R$.\n\nThe dead-band is the region of initial states for which the system converges to the zero state and remains there. We require that if the initial conditions $x[-1]$ and $x[-2]$ are fixed-point values satisfying $|x[-1]| \\le R$ and $|x[-2]| \\le R$, then $x[n] = 0$ for all $n \\ge 0$.\n\nFor the output to become and remain zero, the system must reach the state $(0,0)$. Let us analyze the first few steps.\nFor $n=0$, we require $x[0]=0$.\n$$x[0] = Q_{\\Delta}(-a_1 x[-1] - a_2 x[-2]) = 0$$\nBased on the quantizer property, this is equivalent to the condition:\n$$|a_1 x[-1] + a_2 x[-2]| < \\frac{\\Delta}{2} \\quad (*)$$\n\nFor $n=1$, if $x[0]=0$, the state is now $(x[0], x[-1]) = (0, x[-1])$. We require $x[1]=0$.\n$$x[1] = Q_{\\Delta}(-a_1 x[0] - a_2 x[-1]) = Q_{\\Delta}(-a_2 x[-1]) = 0$$\nThis is equivalent to the condition:\n$$|a_2 x[-1]| < \\frac{\\Delta}{2} \\quad (**)$$\n\nIf both conditions $(*)$ and $(**)$ are met, then $x[0]=0$ and $x[1]=0$. The state at time $n=2$ becomes $(x[1], x[0]) = (0, 0)$. Consequently, for all $n \\ge 2$:\n$$x[n] = Q_{\\Delta}(-a_1 x[n-1] - a_2 x[n-2]) = Q_{\\Delta}(0) = 0$$\nThus, the problem reduces to finding the supremal radius $R > 0$ such that for any fixed-point values $x[-1], x[-2]$ with $|x[-1]| \\le R$ and $|x[-2]| \\le R$, conditions $(*)$ and $(**)$ are satisfied.\n\nA sufficient condition is to require that $(*)$ and $(**)$ hold for all *real* values $u, v$ in the continuous square region defined by $|u| \\le R$ and $|v| \\le R$. If this holds, it must also hold for the subset of fixed-point values within that region.\n\nLet us analyze the required inequalities for all $u,v$ such that $|u| \\le R$ and $|v| \\le R$:\n1. From condition $(*)$: $|a_1 u + a_2 v| < \\frac{\\Delta}{2}$.\nTo guarantee this for the entire region, the maximum value of the left-hand side must be less than $\\frac{\\Delta}{2}$. Using the triangle inequality, we find the maximum value:\n$$\\max_{|u|\\le R, |v|\\le R} |a_1 u + a_2 v| \\le \\max_{|u|\\le R, |v|\\le R} \\{|a_1||u| + |a_2||v|\\}$$\nThis maximum is achieved when $|u|=R$ and $|v|=R$, which gives $(|a_1| + |a_2|)R$. The condition thus becomes:\n$$(|a_1| + |a_2|)R < \\frac{\\Delta}{2} \\implies R < \\frac{\\Delta}{2(|a_1| + |a_2|)}$$\n\n2. From condition $(**)$: $|a_2 u| < \\frac{\\Delta}{2}$.\nSimilarly, the maximum value of the left-hand side is:\n$$\\max_{|u|\\le R} |a_2 u| = |a_2|R$$\nThe condition becomes:\n$$|a_2|R < \\frac{\\Delta}{2} \\implies R < \\frac{\\Delta}{2|a_2|}$$\nThis second condition is only relevant if $a_2 \\ne 0$. If $a_2=0$, it is trivially satisfied.\n\nTo satisfy both conditions, $R$ must be smaller than the minimum of the two derived upper bounds:\n$$R < \\min \\left( \\frac{\\Delta}{2(|a_1| + |a_2|)}, \\frac{\\Delta}{2|a_2|} \\right)$$\nWe compare the denominators of the two terms in the minimum. Since $|a_1| \\ge 0$, it follows that $|a_1| + |a_2| \\ge |a_2|$. Assuming the filter is non-trivial ($|a_1|+|a_2| > 0$, which is implied by the second-order nature), taking the positive reciprocal reverses the inequality:\n$$\\frac{1}{2(|a_1| + |a_2|)} \\le \\frac{1}{2|a_2|}$$\nTherefore, the first term is always the more restrictive one. The comprehensive condition on $R$ simplifies to:\n$$R < \\frac{\\Delta}{2(|a_1| + |a_2|)}$$\nThe problem asks for the supremal radius $R$ with the specified property. The set of all radii $R$ that satisfy the property is the open interval $\\left(0, \\frac{\\Delta}{2(|a_1| + |a_2|)}\\right)$. The supremum (least upper bound) of this set is its right endpoint.\n\nTherefore, the supremal symmetric dead-band radius is given by the expression:\n$$R(a_1, a_2, \\Delta) = \\frac{\\Delta}{2(|a_1| + |a_2|)}$$\nAt this supremum value itself, a fixed-point value on the boundary of the region may exist which is mapped to a non-zero value due to the tie-breaking rule, but for any radius strictly smaller than this, all interior fixed-point values are guaranteed to be mapped to zero. The supremum is the limit of these valid radii.", "answer": "$$\\boxed{\\frac{\\Delta}{2(|a_1| + |a_2|)}}$$", "id": "2878204"}, {"introduction": "A robust filter design must not only achieve the desired frequency response but also operate reliably within the constraints of fixed-point hardware, where dynamic range is limited. This exercise introduces the state-space perspective to systematically address the problem of preventing arithmetic overflow. You will use matrix norm inequalities to derive a safe scaling factor $\\gamma$ for the input signal, providing a guaranteed bound on internal state variables and ensuring the filter's stability and predictability under all bounded-input conditions [@problem_id:2878188].", "problem": "Consider a discrete-time linear time-invariant (LTI) infinite impulse response (IIR) system in state-space form with state vector $x_{k} \\in \\mathbb{R}^{3}$, input $u_{k} \\in \\mathbb{R}$, and zero initial state $x_{0} = 0$. The realization is\n$$\nx_{k+1} = A x_{k} + B u_{k},\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n0.2 & 0.1 & 0 \\\\\n0.05 & 0.3 & 0.05 \\\\\n0 & 0.1 & 0.25\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n0.2 \\\\ 0.1 \\\\ 0.05\n\\end{pmatrix}.\n$$\nThe input prior to a scalar pre-scaling is a bounded sequence $v_{k}$ satisfying $\\lvert v_{k} \\rvert \\leq 1$ for all $k \\in \\mathbb{Z}_{\\ge 0}$. To avoid overflow in fixed-point hardware, a scalar pre-scaling factor $\\gamma > 0$ is inserted so that $u_{k} = \\gamma \\, v_{k}$. The hardware constraint is that the internal state vector must satisfy $\\lVert x_{k} \\rVert_{\\infty} \\leq X_{\\max}$ for all $k \\in \\mathbb{Z}_{\\ge 0}$, with $X_{\\max} = 0.2$.\n\nUsing only state-space inequalities grounded in induced norms and submultiplicativity (without appealing to any unproven shortcut formulas), derive a conservative uniform bound on $\\lVert x_{k} \\rVert_{\\infty}$ in terms of $\\gamma$, and determine the largest admissible $\\gamma$ that guarantees the no-overflow constraint for every bounded input sequence with $\\lvert v_{k} \\rvert \\leq 1$.\n\nProvide your final answer as a single exact number (no units and no approximation required).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary information is provided, and there are no internal contradictions or logical flaws. Therefore, the problem is valid, and we may proceed to a solution.\n\nThe system dynamics are described by the discrete-time linear time-invariant state-space equation:\n$$\nx_{k+1} = A x_{k} + B u_{k}\n$$\nwith the initial state $x_{0} = 0$. The state vector at time step $k$ can be found by unrolling the recursion:\n$$\nx_{1} = A x_{0} + B u_{0} = B u_{0}\n$$\n$$\nx_{2} = A x_{1} + B u_{1} = A(B u_{0}) + B u_{1} = A B u_{0} + B u_{1}\n$$\n$$\nx_{3} = A x_{2} + B u_{2} = A(A B u_{0} + B u_{1}) + B u_{2} = A^{2} B u_{0} + A B u_{1} + B u_{2}\n$$\nBy induction, the general solution for the state vector $x_{k}$ for $k \\ge 1$ is given by the convolution sum:\n$$\nx_{k} = \\sum_{j=0}^{k-1} A^{k-1-j} B u_{j}\n$$\nWe are asked to find a bound on the infinity norm of the state vector, $\\lVert x_{k} \\rVert_{\\infty}$. Applying the infinity norm to the expression for $x_{k}$ and using the triangle inequality for vector norms, we obtain:\n$$\n\\lVert x_{k} \\rVert_{\\infty} = \\left\\lVert \\sum_{j=0}^{k-1} A^{k-1-j} B u_{j} \\right\\rVert_{\\infty} \\le \\sum_{j=0}^{k-1} \\lVert A^{k-1-j} B u_{j} \\rVert_{\\infty}\n$$\nSince $u_{j}$ is a scalar, we can write $\\lVert A^{k-1-j} B u_{j} \\rVert_{\\infty} = \\lvert u_{j} \\rvert \\lVert A^{k-1-j} B \\rVert_{\\infty}$. Here, $A^{k-1-j} B$ is a $3 \\times 1$ column vector, and its norm is the vector infinity norm (maximum absolute element). The input $u_{k}$ is related to a bounded sequence $v_{k}$ by $u_{k} = \\gamma v_{k}$, where $\\lvert v_{k} \\rvert \\leq 1$. This implies that for all $j$, $\\lvert u_{j} \\rvert = \\lvert \\gamma v_{j} \\rvert = \\gamma \\lvert v_{j} \\rvert \\le \\gamma$.\nTo find a bound that holds for every admissible input sequence, we use the worst-case value for $\\lvert u_{j} \\rvert$:\n$$\n\\lVert x_{k} \\rVert_{\\infty} \\le \\sum_{j=0}^{k-1} \\gamma \\lVert A^{k-1-j} B \\rVert_{\\infty} = \\gamma \\sum_{j=0}^{k-1} \\lVert A^{k-1-j} B \\rVert_{\\infty}\n$$\nThe problem requires a conservative bound based on induced norms and submultiplicativity. We use the property of compatible matrix and vector norms, $\\lVert M \\mathbf{v} \\rVert_{\\infty} \\le \\lVert M \\rVert_{\\infty} \\lVert \\mathbf{v} \\rVert_{\\infty}$, where $\\lVert M \\rVert_{\\infty}$ is the induced matrix infinity norm (maximum absolute row sum). In our case, $M = A^{k-1-j}$ and $\\mathbf{v} = B$.\n$$\n\\lVert A^{k-1-j} B \\rVert_{\\infty} \\le \\lVert A^{k-1-j} \\rVert_{\\infty} \\lVert B \\rVert_{\\infty}\n$$\nFurthermore, due to the submultiplicative property of induced matrix norms, we have $\\lVert A^{m} \\rVert_{\\infty} \\le (\\lVert A \\rVert_{\\infty})^{m}$ for any non-negative integer $m$. Applying this, we get:\n$$\n\\lVert A^{k-1-j} \\rVert_{\\infty} \\le (\\lVert A \\rVert_{\\infty})^{k-1-j}\n$$\nCombining these inequalities provides the desired conservative bound:\n$$\n\\lVert x_{k} \\rVert_{\\infty} \\le \\gamma \\sum_{j=0}^{k-1} (\\lVert A \\rVert_{\\infty})^{k-1-j} \\lVert B \\rVert_{\\infty}\n$$\nLet us re-index the summation by setting $m = k-1-j$. As $j$ goes from $0$ to $k-1$, $m$ goes from $k-1$ to $0$.\n$$\n\\lVert x_{k} \\rVert_{\\infty} \\le \\gamma \\lVert B \\rVert_{\\infty} \\sum_{m=0}^{k-1} (\\lVert A \\rVert_{\\infty})^{m}\n$$\nThis bound depends on $k$. To guarantee the hardware constraint $\\lVert x_{k} \\rVert_{\\infty} \\leq X_{\\max}$ for all $k \\ge 0$, we need a uniform bound, which is obtained by considering the limit as $k \\rightarrow \\infty$. First, we must calculate the matrix infinity norm of $A$:\n$$\nA = \\begin{pmatrix}\n0.2 & 0.1 & 0 \\\\\n0.05 & 0.3 & 0.05 \\\\\n0 & 0.1 & 0.25\n\\end{pmatrix}\n$$\nThe absolute row sums are:\nRow $1$: $\\lvert 0.2 \\rvert + \\lvert 0.1 \\rvert + \\lvert 0 \\rvert = 0.3$\nRow $2$: $\\lvert 0.05 \\rvert + \\lvert 0.3 \\rvert + \\lvert 0.05 \\rvert = 0.4$\nRow $3$: $\\lvert 0 \\rvert + \\lvert 0.1 \\rvert + \\lvert 0.25 \\rvert = 0.35$\nThe maximum absolute row sum is $\\lVert A \\rVert_{\\infty} = \\max(0.3, 0.4, 0.35) = 0.4$.\nSince $\\lVert A \\rVert_{\\infty} = 0.4 < 1$, the system is stable, and the geometric series $\\sum_{m=0}^{\\infty} (\\lVert A \\rVert_{\\infty})^{m}$ converges. The sum for finite $k$ is bounded by the infinite sum:\n$$\n\\sum_{m=0}^{k-1} (\\lVert A \\rVert_{\\infty})^{m} \\le \\sum_{m=0}^{\\infty} (\\lVert A \\rVert_{\\infty})^{m} = \\frac{1}{1 - \\lVert A \\rVert_{\\infty}}\n$$\nThis gives a uniform bound on the state norm, valid for all $k \\ge 0$:\n$$\n\\sup_{k \\ge 0} \\lVert x_{k} \\rVert_{\\infty} \\le \\frac{\\gamma \\lVert B \\rVert_{\\infty}}{1 - \\lVert A \\rVert_{\\infty}}\n$$\nThis is the conservative uniform bound on $\\lVert x_{k} \\rVert_{\\infty}$ in terms of $\\gamma$.\nNext, we calculate the vector infinity norm of $B$:\n$$\nB = \\begin{pmatrix}\n0.2 \\\\ 0.1 \\\\ 0.05\n\\end{pmatrix}\n$$\n$\\lVert B \\rVert_{\\infty} = \\max(\\lvert 0.2 \\rvert, \\lvert 0.1 \\rvert, \\lvert 0.05 \\rvert) = 0.2$.\n\nThe no-overflow constraint requires $\\lVert x_{k} \\rVert_{\\infty} \\le X_{\\max}$ for all $k$. To guarantee this, our upper bound must satisfy the constraint:\n$$\n\\frac{\\gamma \\lVert B \\rVert_{\\infty}}{1 - \\lVert A \\rVert_{\\infty}} \\le X_{\\max}\n$$\nWe wish to find the largest admissible $\\gamma$. We achieve this by setting the bound equal to the maximum allowed value:\n$$\n\\frac{\\gamma_{\\max} \\lVert B \\rVert_{\\infty}}{1 - \\lVert A \\rVert_{\\infty}} = X_{\\max}\n$$\nSolving for $\\gamma_{\\max}$:\n$$\n\\gamma_{\\max} = \\frac{X_{\\max} (1 - \\lVert A \\rVert_{\\infty})}{\\lVert B \\rVert_{\\infty}}\n$$\nSubstituting the given and calculated values, $X_{\\max} = 0.2$, $\\lVert A \\rVert_{\\infty} = 0.4$, and $\\lVert B \\rVert_{\\infty} = 0.2$:\n$$\n\\gamma_{\\max} = \\frac{0.2 \\times (1 - 0.4)}{0.2} = 1 - 0.4 = 0.6\n$$\nTherefore, the largest admissible pre-scaling factor is $0.6$.", "answer": "$$\\boxed{0.6}$$", "id": "2878188"}]}