{"hands_on_practices": [{"introduction": "Before delving into complex non-uniform designs, it is crucial to understand the foundational case where a signal's statistics are simple. This practice guides you in deriving an optimal quantizer for a uniformly distributed signal from first principles, demonstrating that it aligns perfectly with a uniform quantizer structure [@problem_id:2915963]. Completing this exercise provides a vital benchmark and deepens your intuition for the necessary conditions of optimality in mean-squared error quantization.", "id": "2915963", "problem": "Consider a zero-mean random variable $X$ uniformly distributed on the bounded interval $[-1,1]$ with probability density function $f_{X}(x)=\\frac{1}{2}$ for $x \\in [-1,1]$ and $f_{X}(x)=0$ otherwise. A scalar $3$-level quantizer $q(\\cdot)$ is specified by two decision thresholds $t_{1}<t_{2}$ in $(-1,1)$ and three representation levels (also called codepoints or centroids) $m_{1},m_{2},m_{3}$, with decision regions $[-1,t_{1})$, $[t_{1},t_{2})$, and $[t_{2},1]$, respectively. The quantizer output is $q(x)=m_{i}$ if $x$ lies in the $i$-th region. The quality of the quantizer is measured by the mean squared error $J=\\mathbb{E}\\{(X-q(X))^{2}\\}$.\n\nStarting only from the definition of $J$ as an integral over the decision regions and the uniform density of $X$, and invoking first principles of optimality by minimizing $J$ with respect to both the thresholds and the representation levels, derive the exact optimal thresholds and centroids for the $3$-level quantizer that minimizes $J$. Exploit any symmetry that follows from the problem data, but do not assume any formulaic optimality conditions without derivation.\n\nThen, for comparison, construct the uniform-spacing $3$-level quantizer on $[-1,1]$ whose decision regions have equal width and whose representation levels are the midpoints of the regions, and compute its thresholds and centroids exactly.\n\nReport your final results as exact rational numbers. Provide your final answer as a single row matrix containing, in order, the optimal tuple $(m_{1},m_{2},m_{3},t_{1},t_{2})$ followed by the uniform-spacing tuple $(\\tilde{m}_{1},\\tilde{m}_{2},\\tilde{m}_{3},\\tilde{t}_{1},\\tilde{t}_{2})$. No rounding is required and no units are needed.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Random variable $X$ is zero-mean.\n- The probability density function (PDF) of $X$ is $f_{X}(x)=\\frac{1}{2}$ for $x \\in [-1,1]$ and $f_{X}(x)=0$ otherwise.\n- A scalar $3$-level quantizer $q(\\cdot)$ is defined by two decision thresholds $t_{1}<t_{2}$ in $(-1,1)$ and three representation levels $m_{1},m_{2},m_{3}$.\n- The decision regions are $R_1 = [-1,t_{1})$, $R_2 = [t_{1},t_{2})$, and $R_3 = [t_{2},1]$.\n- The quantizer output is $q(x)=m_{i}$ for $x \\in R_i$.\n- The performance metric is the mean squared error (MSE), $J=\\mathbb{E}\\{(X-q(X))^{2}\\}$.\n- The task is to derive the optimal set of parameters $(m_1, m_2, m_3, t_1, t_2)$ that minimize $J$ from first principles, and also to determine the parameters $(\\tilde{m}_1, \\tilde{m}_2, \\tilde{m}_3, \\tilde{t}_1, \\tilde{t}_2)$ for a uniform quantizer on the same interval.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard, fundamental problem in quantization theory, a subfield of signal processing. All necessary definitions and constraints are provided, and there are no contradictions. The problem is complete and solvable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A rigorous derivation will now be provided.\n\nThe mean squared error $J$ is defined as the expectation of the squared error:\n$$J = \\mathbb{E}\\{(X - q(X))^2\\} = \\int_{-\\infty}^{\\infty} (x-q(x))^2 f_{X}(x) dx$$\nSubstituting the given PDF and the structure of the quantizer, the expression for $J$ becomes a function of the parameters $\\{m_1, m_2, m_3, t_1, t_2\\}$:\n$$J(m_1, m_2, m_3, t_1, t_2) = \\int_{-1}^{t_1} (x - m_1)^2 \\frac{1}{2} dx + \\int_{t_1}^{t_2} (x - m_2)^2 \\frac{1}{2} dx + \\int_{t_2}^{1} (x - m_3)^2 \\frac{1}{2} dx$$\nTo minimize $J$, we must find the stationary points by setting the partial derivatives of $J$ with respect to each parameter to zero.\n\nFirst, we derive the necessary conditions for the optimal representation levels $\\{m_1, m_2, m_3\\}$. We differentiate $J$ with respect to each $m_i$ and set the result to zero:\n$$\\frac{\\partial J}{\\partial m_1} = \\frac{\\partial}{\\partial m_1} \\left[ \\frac{1}{2} \\int_{-1}^{t_1} (x - m_1)^2 dx \\right] = \\frac{1}{2} \\int_{-1}^{t_1} 2(x - m_1)(-1) dx = -\\int_{-1}^{t_1} (x - m_1) dx = 0$$\nThis implies $\\int_{-1}^{t_1} x dx = m_1 \\int_{-1}^{t_1} dx$, which gives the optimal $m_1$ as the conditional mean of $X$ given $X \\in R_1$:\n$$m_1 = \\frac{\\int_{-1}^{t_1} x dx}{\\int_{-1}^{t_1} dx} = \\frac{\\frac{1}{2}[x^2]_{-1}^{t_1}}{[x]_{-1}^{t_1}} = \\frac{\\frac{1}{2}(t_1^2 - 1)}{t_1 + 1} = \\frac{t_1 - 1}{2}$$\nBy identical reasoning for $m_2$ and $m_3$:\n$$\\frac{\\partial J}{\\partial m_2} = -\\int_{t_1}^{t_2} (x - m_2) dx = 0 \\implies m_2 = \\frac{\\int_{t_1}^{t_2} x dx}{\\int_{t_1}^{t_2} dx} = \\frac{\\frac{1}{2}(t_2^2 - t_1^2)}{t_2 - t_1} = \\frac{t_1 + t_2}{2}$$\n$$\\frac{\\partial J}{\\partial m_3} = -\\int_{t_2}^{1} (x - m_3) dx = 0 \\implies m_3 = \\frac{\\int_{t_2}^{1} x dx}{\\int_{t_2}^{1} dx} = \\frac{\\frac{1}{2}(1 - t_2^2)}{1 - t_2} = \\frac{1 + t_2}{2}$$\nThese are the centroid conditions for optimality.\n\nNext, we derive the necessary conditions for the optimal decision thresholds $\\{t_1, t_2\\}$. We differentiate $J$ with respect to each $t_j$. According to the Leibniz integral rule, and given that the conditions $\\frac{\\partial J}{\\partial m_i}=0$ are already satisfied, the derivatives simplify significantly:\n$$\\frac{\\partial J}{\\partial t_1} = \\frac{1}{2} (t_1 - m_1)^2 - \\frac{1}{2} (t_1 - m_2)^2 = 0$$\nThis implies $(t_1 - m_1)^2 = (t_1 - m_2)^2$. For an optimal quantizer, the representation levels must be ordered such that $m_1 < t_1 < m_2$, so we must have $t_1 - m_1 = -(t_1 - m_2) = m_2 - t_1$. This yields:\n$$t_1 = \\frac{m_1 + m_2}{2}$$\nSimilarly, for $t_2$:\n$$\\frac{\\partial J}{\\partial t_2} = \\frac{1}{2} (t_2 - m_2)^2 - \\frac{1}{2} (t_2 - m_3)^2 = 0$$\nWith $m_2 < t_2 < m_3$, we find $t_2 - m_2 = -(t_2 - m_3)$, which gives:\n$$t_2 = \\frac{m_2 + m_3}{2}$$\nThese are the nearest-neighbor conditions for optimality.\n\nThe problem possesses symmetry. The PDF $f_X(x)$ is an even function on a symmetric interval $[-1, 1]$. For such a distribution, the optimal quantizer must be symmetric about the origin. This means its structure must be anti-symmetric, $q(x) = -q(-x)$. This symmetry implies $t_2 = -t_1$, $m_3 = -m_1$, and $m_2 = -m_2$, which forces $m_2=0$. This greatly simplifies the system of equations.\n\nLet us apply the symmetry conditions to the optimality equations.\n$m_2=0$.\nThe condition $t_1 = \\frac{m_1+m_2}{2}$ becomes $t_1 = \\frac{m_1}{2}$.\nThe condition $m_1 = \\frac{t_1-1}{2}$ becomes $m_1 = \\frac{(m_1/2) - 1}{2}$.\nSolving for $m_1$:\n$$2m_1 = \\frac{m_1}{2} - 1 \\implies \\frac{3}{2}m_1 = -1 \\implies m_1 = -\\frac{2}{3}$$\nFrom this, we find the other parameters for the optimal quantizer:\n$$t_1 = \\frac{m_1}{2} = \\frac{-2/3}{2} = -\\frac{1}{3}$$\n$$m_2 = 0$$\n$$m_3 = -m_1 = \\frac{2}{3}$$\n$$t_2 = -t_1 = \\frac{1}{3}$$\nThe optimal parameters are $(m_{1},m_{2},m_{3},t_{1},t_{2}) = (-\\frac{2}{3}, 0, \\frac{2}{3}, -\\frac{1}{3}, \\frac{1}{3})$.\n\nNext, we construct the uniform-spacing $3$-level quantizer on $[-1,1]$.\nThe total length of the interval is $1 - (-1) = 2$.\nFor $3$ regions of equal width, each region has a width of $\\Delta = \\frac{2}{3}$.\nThe decision thresholds, denoted by $\\tilde{t}_i$, are the boundaries of these regions:\n$$\\tilde{t}_1 = -1 + \\Delta = -1 + \\frac{2}{3} = -\\frac{1}{3}$$\n$$\\tilde{t}_2 = -1 + 2\\Delta = -1 + \\frac{4}{3} = \\frac{1}{3}$$\nThe decision regions are therefore $R_1 = [-1, -1/3)$, $R_2 = [-1/3, 1/3)$, and $R_3 = [1/3, 1]$.\nThe representation levels, denoted by $\\tilde{m}_i$, are the midpoints of these regions:\n$$\\tilde{m}_1 = \\frac{-1 + (-\\frac{1}{3})}{2} = \\frac{-4/3}{2} = -\\frac{2}{3}$$\n$$\\tilde{m}_2 = \\frac{-\\frac{1}{3} + \\frac{1}{3}}{2} = 0$$\n$$\\tilde{m}_3 = \\frac{\\frac{1}{3} + 1}{2} = \\frac{4/3}{2} = \\frac{2}{3}$$\nThe parameters for the uniform-spacing quantizer are $(\\tilde{m}_{1},\\tilde{m}_{2},\\tilde{m}_{3},\\tilde{t}_{1},\\tilde{t}_{2}) = (-\\frac{2}{3}, 0, \\frac{2}{3}, -\\frac{1}{3}, \\frac{1}{3})$.\nIt is a known result that for a uniformly distributed source, the optimal scalar quantizer is a uniform quantizer. Our derivation confirms this, as both sets of parameters are identical.\n\nThe final result is reported as a single row matrix containing the ten values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{2}{3} & 0 & \\frac{2}{3} & -\\frac{1}{3} & \\frac{1}{3} & -\\frac{2}{3} & 0 & \\frac{2}{3} & -\\frac{1}{3} & \\frac{1}{3}\n\\end{pmatrix}\n}\n$$"}, {"introduction": "Most real-world signals, such as those found in speech or image processing, are not uniformly distributed, necessitating non-uniform quantizers for efficient representation. This exercise provides direct, hands-on experience with the celebrated Lloyd algorithm, the cornerstone for designing such quantizers [@problem_id:2915969]. You will perform one full iteration for a signal with a Laplacian distribution, analytically updating reconstruction levels and decision thresholds and verifying the algorithm's core promise: a guaranteed reduction in mean-squared error.", "id": "2915969", "problem": "Consider a memoryless scalar source with zero-mean Laplace distribution having probability density function $f_{X}(x) = \\frac{1}{2}\\exp(-|x|)$, which corresponds to a Laplace scale parameter $b = 1$. A symmetric $3$-level quantizer is initialized with decision thresholds at $-1$ and $+1$, and with initial reconstruction levels at $-1$, $0$, and $+1$. One iteration of the Lloyd algorithm (also known as the Lloydâ€“Max algorithm for squared-error distortion) consists of the following two substeps applied in sequence: first, for fixed thresholds, update each reconstruction level to the conditional mean of $X$ over its corresponding decision region; second, for fixed reconstruction levels, update each threshold to the midpoint between its two neighboring reconstruction levels.\n\nUsing only core definitions of mean-squared error, conditional expectation, and decision region partitions for quantizers, carry out precisely one Lloyd iteration starting from the specified initialization. Then, compute the mean-squared error before the iteration and after the iteration, and verify analytically that the latter is smaller than the former.\n\nProvide as your final answer the exact closed-form analytic expression for the mean-squared error after the single Lloyd iteration. Do not round. Do not include units.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information to proceed. It is a standard problem in scalar quantizer design. Thus, the problem is valid.\n\nThe memoryless scalar source $X$ has a zero-mean Laplace distribution with a probability density function (PDF) given by $f_{X}(x) = \\frac{1}{2}\\exp(-|x|)$. The quantizer is a symmetric $3$-level quantizer.\n\nThe initial state of the quantizer is defined by:\nDecision thresholds: $t_1 = -1$, $t_2 = 1$. These define three decision regions: $R_1 = (-\\infty, -1]$, $R_2 = (-1, 1]$, and $R_3 = (1, \\infty)$.\nReconstruction levels (codepoints): $c_1 = -1$, $c_2 = 0$, $c_3 = 1$.\n\nThe Mean-Squared Error (MSE) of a quantizer $Q$ is given by the formula $D = E[(X - Q(X))^2]$. For this specific quantizer, it is:\n$$D = \\sum_{i=1}^{3} \\int_{R_i} (x - c_i)^2 f_X(x) \\, dx$$\n\nFirst, we calculate the initial MSE, denoted as $D_{initial}$, using the initial quantizer parameters.\n$$D_{initial} = \\int_{-\\infty}^{-1} (x - (-1))^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{-1}^{1} (x - 0)^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{1}^{\\infty} (x - 1)^2 \\frac{1}{2}\\exp(-|x|) \\, dx$$\nFor $x < 0$, $|x| = -x$, so $\\exp(-|x|) = \\exp(x)$. For $x > 0$, $|x| = x$, so $\\exp(-|x|) = \\exp(-x)$.\n$$D_{initial} = \\frac{1}{2} \\int_{-\\infty}^{-1} (x+1)^2 \\exp(x) \\, dx + \\frac{1}{2} \\int_{-1}^{1} x^2 \\exp(-|x|) \\, dx + \\frac{1}{2} \\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx$$\nDue to the symmetry of the PDF and the quantizer, the first and third integrals are equal. The middle integral can be split due to the evenness of its integrand $x^2 \\exp(-|x|)$.\n$$D_{initial} = \\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx + \\int_{0}^{1} x^2 \\exp(-x) \\, dx$$\nWe require the following indefinite integrals, which are found by repeated integration by parts:\n$\\int x^2 \\exp(-x) \\, dx = -(x^2 + 2x + 2)\\exp(-x) + C$\n$\\int (x-1)^2 \\exp(-x) \\, dx = \\int (x^2 - 2x + 1)\\exp(-x) \\, dx = -(x^2+1)\\exp(-x) + C$\n\nUsing these, we evaluate the definite integrals:\n$\\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx = [-(x^2+1)\\exp(-x)]_{1}^{\\infty} = 0 - (-(1^2+1)\\exp(-1)) = 2\\exp(-1)$.\n$\\int_{0}^{1} x^2 \\exp(-x) \\, dx = [-(x^2+2x+2)\\exp(-x)]_{0}^{1} = -(1^2+2(1)+2)\\exp(-1) - (-(0+0+2)\\exp(0)) = -5\\exp(-1) + 2$.\nTherefore, the initial MSE is:\n$$D_{initial} = 2\\exp(-1) + (2 - 5\\exp(-1)) = 2 - 3\\exp(-1)$$\n\nNext, we perform one iteration of the Lloyd algorithm.\nStep 1: Update reconstruction levels to be the centroids of their corresponding decision regions. The new reconstruction level $c'_i$ is the conditional mean $E[X | X \\in R_i]$.\n$$c'_i = \\frac{\\int_{R_i} x f_X(x) \\, dx}{\\int_{R_i} f_X(x) \\, dx}$$\nFirst, we find the probabilities of the regions, $P(R_i) = \\int_{R_i} f_X(x) \\, dx$.\n$P(R_3) = \\int_{1}^{\\infty} \\frac{1}{2}\\exp(-x) \\, dx = \\frac{1}{2}[-\\exp(-x)]_{1}^{\\infty} = \\frac{1}{2}\\exp(-1)$.\nBy symmetry, $P(R_1) = P(R_3) = \\frac{1}{2}\\exp(-1)$.\n$P(R_2) = 1 - P(R_1) - P(R_3) = 1 - \\exp(-1)$.\n\nNow, we compute the numerators for the centroids. For $c'_3$:\n$\\int_{1}^{\\infty} x f_X(x) \\, dx = \\frac{1}{2}\\int_{1}^{\\infty} x \\exp(-x) \\, dx = \\frac{1}{2}[-(x+1)\\exp(-x)]_{1}^{\\infty} = \\frac{1}{2}(0 - (-(1+1)\\exp(-1))) = \\exp(-1)$.\nSo, $c'_3 = \\frac{\\exp(-1)}{P(R_3)} = \\frac{\\exp(-1)}{\\frac{1}{2}\\exp(-1)} = 2$.\nBy symmetry, the centroid for $R_1$ is $c'_1 = -2$.\nFor $c'_2$, the region is $R_2 = (-1, 1)$, which is symmetric about the origin. The integrand $x f_X(x) = \\frac{1}{2}x\\exp(-|x|)$ is an odd function. Therefore, the integral over this symmetric interval is zero: $\\int_{-1}^{1} x f_X(x) \\, dx = 0$.\nSo, $c'_2 = 0$.\nThe updated reconstruction levels are $c'_1 = -2$, $c'_2 = 0$, $c'_3 = 2$.\n\nStep 2: Update decision thresholds to be the midpoints of adjacent new reconstruction levels.\nThe new threshold $t'_1$ is the midpoint of $c'_1$ and $c'_2$:\n$t'_1 = \\frac{c'_1 + c'_2}{2} = \\frac{-2 + 0}{2} = -1$.\nThe new threshold $t'_2$ is the midpoint of $c'_2$ and $c'_3$:\n$t'_2 = \\frac{c'_2 + c'_3}{2} = \\frac{0 + 2}{2} = 1$.\nThe decision thresholds remain unchanged: $t'_1 = -1$ and $t'_2 = 1$. Consequently, the decision regions also remain unchanged.\n\nNow we compute the final MSE, denoted as $D_{final}$, after one full iteration. The quantizer is defined by regions $R_1, R_2, R_3$ and new reconstruction levels $c'_1 = -2, c'_2 = 0, c'_3 = 2$.\n$$D_{final} = \\int_{-\\infty}^{-1} (x - (-2))^2 \\frac{1}{2}\\exp(x) \\, dx + \\int_{-1}^{1} (x - 0)^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{1}^{\\infty} (x - 2)^2 \\frac{1}{2}\\exp(-x) \\, dx$$\nAgain, by symmetry:\n$$D_{final} = \\int_{1}^{\\infty} (x-2)^2 \\exp(-x) \\, dx + \\int_{0}^{1} x^2 \\exp(-x) \\, dx$$\nThe second term was already computed as $2 - 5\\exp(-1)$. We compute the first term.\nThe indefinite integral is $\\int (x-2)^2 \\exp(-x) \\, dx = \\int(x^2 - 4x + 4)\\exp(-x) \\, dx = (-x^2+2x-2)\\exp(-x) + C$.\n$\\int_{1}^{\\infty} (x-2)^2 \\exp(-x) \\, dx = [(-x^2+2x-2)\\exp(-x)]_{1}^{\\infty} = 0 - ((-1^2+2(1)-2)\\exp(-1)) = -(-1)\\exp(-1) = \\exp(-1)$.\nThus, the final MSE is:\n$$D_{final} = \\exp(-1) + (2 - 5\\exp(-1)) = 2 - 4\\exp(-1)$$\n\nFinally, we verify that the MSE has decreased, i.e., $D_{final} < D_{initial}$.\nWe must check if $2 - 4\\exp(-1) < 2 - 3\\exp(-1)$.\nSubtracting $2$ from both sides gives $-4\\exp(-1) < -3\\exp(-1)$.\nMultiplying by $-1$ and reversing the inequality sign yields $4\\exp(-1) > 3\\exp(-1)$.\nSince $\\exp(-1) > 0$, we can divide by it, resulting in $4 > 3$. This is a true statement, which confirms that the MSE decreased, as expected from the Lloyd algorithm.\n\nThe problem asks for the exact closed-form analytic expression for the mean-squared error after the single Lloyd iteration. This is $D_{final}$.", "answer": "$$\\boxed{2 - 4\\exp(-1)}$$"}, {"introduction": "Quantizer design often involves trade-offs, and minimizing mean-squared error is not the only objective. This practice expands our perspective by analyzing a different type of quantizer based on truncation (the floor function) rather than rounding [@problem_id:2916010]. By deriving the mean error, or bias, for this quantizer, you will discover that it introduces a systematic offset, even for symmetric signals where a rounding quantizer would be unbiased. This result highlights the importance of understanding how different quantizer structures impact signal statistics beyond just variance.", "id": "2916010", "problem": "Consider a real-valued random variable $X$ with an absolutely continuous probability density function symmetric about the origin, that is, $f_{X}(x)=f_{X}(-x)$ and $\\int_{-\\infty}^{\\infty} f_{X}(x)\\,dx=1$. Let $\\Delta>0$ be a fixed step size and define the truncation quantizer $Q(x)=\\Delta \\lfloor x/\\Delta \\rfloor$. Let the quantization error be $e(x)=Q(x)-x$. Assume that $\\mathbb{P}(X \\in \\Delta \\mathbb{Z})=0$ so that boundary events have zero probability.\n\nUsing only the definitions of mathematical expectation, symmetry of a probability density function, and the interval mapping induced by the floor function, proceed as follows:\n\n1. Derive $\\mathbb{E}[e]$ in terms of a sum of integrals over the quantization bins and show, by an argument that pairs positive and negative bins, that for a symmetric $X$ the mean quantization error of truncation is nonzero. Your derivation must start from $\\mathbb{E}[e]=\\mathbb{E}[Q(X)-X]$ and from the definition of the floor function as the unique integer $k$ such that $X \\in [k\\Delta,(k+1)\\Delta)$.\n\n2. Specialize your result to the case where $X$ is uniformly distributed over a single bin, i.e., $X \\sim \\mathrm{Uniform}([k\\Delta,(k+1)\\Delta))$ for a fixed $k \\in \\mathbb{Z}$. Compute $\\mathbb{E}[e]$ exactly in terms of $\\Delta$.\n\nProvide your final answer as a single closed-form expression for $\\mathbb{E}[e]$ in the uniform-over-one-bin case. No numerical rounding is required and no units are involved.", "solution": "The problem statement is scientifically sound, well-posed, objective, and contains all necessary information for a unique and meaningful solution. It is a valid problem in the domain of signal processing and systems modeling. We proceed with the solution.\n\nThe problem requires a two-part derivation. First, we will derive a general expression for the mean quantization error $\\mathbb{E}[e]$ for a symmetric probability density function $f_X(x)$, and then we will specialize this result to compute the exact mean error for a uniform distribution over a single quantization interval.\n\nPart 1: General Derivation for Symmetric PDF\n\nWe are given the quantization error $e(x) = Q(x) - x$, where the truncation quantizer is defined as $Q(x) = \\Delta \\lfloor x/\\Delta \\rfloor$ for a step size $\\Delta > 0$. The mean quantization error is the expected value of the error random variable $e(X) = Q(X) - X$. By the definition of mathematical expectation for a real-valued random variable $X$ with an absolutely continuous probability density function (PDF) $f_X(x)$, we have:\n$$ \\mathbb{E}[e] = \\mathbb{E}[Q(X) - X] = \\int_{-\\infty}^{\\infty} (Q(x) - x) f_X(x) \\,dx $$\nThe quantizer $Q(x)$ is a step function. The floor function $\\lfloor x/\\Delta \\rfloor$ evaluates to a unique integer $k$ for all $x$ such that $k \\le x/\\Delta < k+1$, which corresponds to the interval $x \\in [k\\Delta, (k+1)\\Delta)$. Over each such interval, $Q(x)$ is constant and equal to $k\\Delta$. We can thus partition the real line into disjoint quantization bins $[k\\Delta, (k+1)\\Delta)$ for all integers $k \\in \\mathbb{Z}$ and express the integral as an infinite sum of integrals over these bins:\n$$ \\mathbb{E}[e] = \\sum_{k=-\\infty}^{\\infty} \\int_{k\\Delta}^{(k+1)\\Delta} (k\\Delta - x) f_X(x) \\,dx $$\nThis expression gives the mean error in terms of a sum of integrals over the quantization bins, as required by the first part of the problem.\n\nNext, we must show that for a symmetric PDF, where $f_X(x) = f_X(-x)$, this mean error is nonzero. We will do this by pairing the terms in the sum. Let us pair the term for index $k$ with the term for index $j = -k-1$. This pairing covers all integers exhaustively as $k$ ranges from $0$ to $\\infty$. Let $I_k$ be the integral for a given index $k$:\n$$ I_k = \\int_{k\\Delta}^{(k+1)\\Delta} (k\\Delta - x) f_X(x) \\,dx $$\nThe integral for index $j = -k-1$ is:\n$$ I_{-k-1} = \\int_{(-k-1)\\Delta}^{-k\\Delta} ((-k-1)\\Delta - x) f_X(x) \\,dx $$\nIn this second integral, let us perform a substitution $u = -x$, which implies $x = -u$ and $dx = -du$. The limits of integration change from $x = (-k-1)\\Delta$ to $u = (k+1)\\Delta$, and from $x = -k\\Delta$ to $u = k\\Delta$. Using the symmetry of the PDF, $f_X(x) = f_X(-u) = f_X(u)$, the integral becomes:\n$$ I_{-k-1} = \\int_{(k+1)\\Delta}^{k\\Delta} (-(k+1)\\Delta + u) f_X(u) (-du) = \\int_{k\\Delta}^{(k+1)\\Delta} (u - (k+1)\\Delta) f_X(u) \\,du $$\nNow we sum the paired terms $I_k$ and $I_{-k-1}$. Replacing the dummy variable $u$ with $x$ in the transformed integral, we get:\n$$ I_k + I_{-k-1} = \\int_{k\\Delta}^{(k+1)\\Delta} (k\\Delta - x) f_X(x) \\,dx + \\int_{k\\Delta}^{(k+1)\\Delta} (x - (k+1)\\Delta) f_X(x) \\,dx $$\n$$ = \\int_{k\\Delta}^{(k+1)\\Delta} (k\\Delta - x + x - (k+1)\\Delta) f_X(x) \\,dx $$\n$$ = \\int_{k\\Delta}^{(k+1)\\Delta} (-\\Delta) f_X(x) \\,dx = -\\Delta \\int_{k\\Delta}^{(k+1)\\Delta} f_X(x) \\,dx $$\nThe total expected error is the sum of these paired terms over all non-negative integers $k$:\n$$ \\mathbb{E}[e] = \\sum_{k=0}^{\\infty} (I_k + I_{-k-1}) = \\sum_{k=0}^{\\infty} \\left(-\\Delta \\int_{k\\Delta}^{(k+1)\\Delta} f_X(x) \\,dx\\right) $$\n$$ = -\\Delta \\sum_{k=0}^{\\infty} \\int_{k\\Delta}^{(k+1)\\Delta} f_X(x) \\,dx = -\\Delta \\int_{0}^{\\infty} f_X(x) \\,dx $$\nFor any symmetric PDF that integrates to $1$ over $(-\\infty, \\infty)$, the integral over the non-negative real axis is exactly half of the total probability: $\\int_{0}^{\\infty} f_X(x) \\,dx = \\frac{1}{2}$.\nTherefore, for any random variable $X$ with a symmetric PDF, the mean error of a truncation quantizer is:\n$$ \\mathbb{E}[e] = -\\Delta \\left(\\frac{1}{2}\\right) = -\\frac{\\Delta}{2} $$\nSince $\\Delta > 0$, the mean error $\\mathbb{E}[e]$ is strictly negative and thus nonzero. This demonstrates that truncation quantization introduces a systematic bias (a non-zero mean error), even for symmetrically distributed signals.\n\nPart 2: Specialization to Uniform Distribution over One Bin\n\nWe are now asked to specialize this result to the case where $X$ is uniformly distributed over a single quantization bin, $X \\sim \\mathrm{Uniform}([m\\Delta, (m+1)\\Delta))$ for some fixed integer $m$.\nThe PDF for this distribution is:\n$$ f_X(x) = \\begin{cases} \\frac{1}{(m+1)\\Delta - m\\Delta} = \\frac{1}{\\Delta} & \\text{for } x \\in [m\\Delta, (m+1)\\Delta) \\\\ 0 & \\text{otherwise} \\end{cases} $$\nWe use our derived expression for the mean error:\n$$ \\mathbb{E}[e] = \\sum_{k=-\\infty}^{\\infty} \\int_{k\\Delta}^{(k+1)\\Delta} (k\\Delta - x) f_X(x) \\,dx $$\nSince $f_X(x)$ is zero everywhere except for the interval $[m\\Delta, (m+1)\\Delta)$, all terms in the summation are zero except for the term where $k=m$. The sum thus collapses to a single integral:\n$$ \\mathbb{E}[e] = \\int_{m\\Delta}^{(m+1)\\Delta} (m\\Delta - x) f_X(x) \\,dx $$\nSubstituting the value of the PDF, $f_X(x) = 1/\\Delta$, into this integral:\n$$ \\mathbb{E}[e] = \\int_{m\\Delta}^{(m+1)\\Delta} (m\\Delta - x) \\frac{1}{\\Delta} \\,dx $$\nWe now compute this definite integral:\n$$ \\mathbb{E}[e] = \\frac{1}{\\Delta} \\left[ m\\Delta x - \\frac{x^2}{2} \\right]_{m\\Delta}^{(m+1)\\Delta} $$\n$$ = \\frac{1}{\\Delta} \\left( \\left( m\\Delta(m+1)\\Delta - \\frac{((m+1)\\Delta)^2}{2} \\right) - \\left( m\\Delta(m\\Delta) - \\frac{(m\\Delta)^2}{2} \\right) \\right) $$\n$$ = \\frac{\\Delta^2}{\\Delta} \\left( \\left( m(m+1) - \\frac{(m+1)^2}{2} \\right) - \\left( m^2 - \\frac{m^2}{2} \\right) \\right) $$\n$$ = \\Delta \\left( \\frac{2m(m+1) - (m+1)^2}{2} - \\frac{m^2}{2} \\right) $$\n$$ = \\Delta \\left( \\frac{(m+1)(2m - (m+1))}{2} - \\frac{m^2}{2} \\right) = \\Delta \\left( \\frac{(m+1)(m-1)}{2} - \\frac{m^2}{2} \\right) $$\n$$ = \\Delta \\left( \\frac{m^2 - 1}{2} - \\frac{m^2}{2} \\right) = \\Delta \\left( \\frac{m^2 - 1 - m^2}{2} \\right) = -\\frac{\\Delta}{2} $$\nThe result is independent of the specific bin index $m$ and matches the general result for symmetric distributions, although the uniform distribution on a single bin $[m\\Delta, (m+1)\\Delta)$ is not symmetric about the origin (unless $m=-1/2$, which is not an integer). The calculation demonstrates that the average error within any single truncation bin is consistently $-\\Delta/2$.", "answer": "$$ \\boxed{-\\frac{\\Delta}{2}} $$"}]}