{"hands_on_practices": [{"introduction": "Understanding ergodicity begins with quantifying how time averages behave, and a key metric is the variance of the time-average estimator. This exercise provides fundamental practice by requiring a first-principles derivation of this variance for a discrete-time Moving Average (MA) process. By directly connecting the filter's structure to the convergence properties of the sample mean, you will build the analytical skills needed to assess the quality of statistical estimates derived from finite data records [@problem_id:2869728].", "problem": "Consider the discrete-time process defined by the finite-support moving-average filter of order $q = 3$ acting on a zero-mean, independent and identically distributed (i.i.d.) innovation sequence. Specifically, let $\\{w[n]\\}$ be zero-mean i.i.d. with variance $\\sigma_{w}^{2} = 2$, and define\n$$\nx[n] \\triangleq \\sum_{k=0}^{3} b_{k}\\, w[n-k], \\quad b_{0} = 1,\\; b_{1} = -2,\\; b_{2} = \\tfrac{1}{2},\\; b_{3} = 3.\n$$\nThe time average over $N$ samples is\n$$\n\\overline{x}_{N} \\triangleq \\frac{1}{N}\\sum_{n=1}^{N} x[n].\n$$\nAssume $N \\geq 4$ so that end effects do not truncate the nonzero lags in the autocovariance summations. Starting only from the core definitions of wide-sense stationarity (WSS) and second-order moments, compute the exact variance $\\operatorname{Var}(\\overline{x}_{N})$ and then derive its asymptotic rate in $N$, identifying the explicit leading constant in the $1/N$ term and the next-order constant. Your derivation must proceed from first principles: use linearity of expectation, the definition of autocovariance for a WSS process, and the independence and variance of the innovations to construct the necessary expressions; do not assume any pre-packaged formulas for the variance of time averages.\n\nExpress your final answer as a single closed-form analytic expression in terms of $N$ only. Do not round your answer.", "solution": "The problem statement has been evaluated and is deemed valid. It is a well-posed problem in the analysis of stochastic processes, grounded in established principles of signal processing and free of scientific or logical flaws. We will now proceed with a complete solution derived from first principles.\n\nThe objective is to compute the variance of the time average, $\\operatorname{Var}(\\overline{x}_{N})$, for the given discrete-time process. The process is defined as\n$$\nx[n] = \\sum_{k=0}^{3} b_{k} w[n-k]\n$$\nwhere $\\{w[n]\\}$ is an independent and identically distributed (i.i.d.) sequence with $E[w[n]] = 0$ and $E[w[n]^2] = \\sigma_{w}^{2} = 2$. The coefficients are $b_{0} = 1$, $b_{1} = -2$, $b_{2} = \\tfrac{1}{2}$, and $b_{3} = 3$.\n\nFirst, we establish that the process $\\{x[n]\\}$ is wide-sense stationary (WSS). We compute its mean and autocovariance.\nThe mean of the process is\n$$\nE[x[n]] = E\\left[\\sum_{k=0}^{3} b_{k} w[n-k]\\right] = \\sum_{k=0}^{3} b_{k} E[w[n-k]]\n$$\nSince $E[w[m]] = 0$ for all $m$, it follows that $E[x[n]] = 0$. The mean is constant for all $n$.\n\nThe autocovariance function is $C_x[m] = E[(x[n] - E[x[n]])(x[n-m] - E[x[n-m]])]$. Since the mean is zero, the autocovariance is identical to the autocorrelation function, $R_x[m] = E[x[n]x[n-m]]$.\n$$\nR_x[m] = E\\left[ \\left(\\sum_{k=0}^{3} b_{k} w[n-k]\\right) \\left(\\sum_{j=0}^{3} b_{j} w[n-m-j]\\right) \\right]\n$$\nBy linearity of expectation,\n$$\nR_x[m] = \\sum_{k=0}^{3} \\sum_{j=0}^{3} b_{k} b_{j} E[w[n-k]w[n-m-j]]\n$$\nThe sequence $\\{w[n]\\}$ is i.i.d. with zero mean, so $E[w[i]w[l]] = \\sigma_{w}^{2} \\delta[i-l]$, where $\\delta[\\cdot]$ is the Kronecker delta. Thus,\n$$\nE[w[n-k]w[n-m-j]] = \\sigma_{w}^{2} \\delta[(n-k) - (n-m-j)] = \\sigma_{w}^{2} \\delta[m - k + j]\n$$\nThe expectation is non-zero only if $j = k-m$. Substituting this into the double summation,\n$$\nR_x[m] = \\sigma_{w}^{2} \\sum_{k=0}^{3} b_{k} b_{k-m}\n$$\nThis expression depends only on the lag $m$, not on $n$. Since the mean is constant and the autocovariance depends only on the time lag, the process $\\{x[n]\\}$ is WSS. The filter length is $4$ (order $q=3$), so the autocovariance $R_x[m]$ is non-zero only for $|m| \\le 3$.\n\nWe now calculate the specific values of $R_x[m]$ for $m \\ge 0$, given $\\sigma_{w}^{2} = 2$ and the coefficients $b_k$.\nFor $m=0$:\n$$\nR_x[0] = \\sigma_{w}^{2} \\sum_{k=0}^{3} b_{k}^{2} = 2 \\left( 1^2 + (-2)^2 + \\left(\\frac{1}{2}\\right)^2 + 3^2 \\right) = 2 \\left( 1 + 4 + \\frac{1}{4} + 9 \\right) = 2 \\left( \\frac{57}{4} \\right) = \\frac{57}{2}\n$$\nFor $m=1$:\n$$\nR_x[1] = \\sigma_{w}^{2} (b_1 b_0 + b_2 b_1 + b_3 b_2) = 2 \\left( (-2)(1) + \\left(\\frac{1}{2}\\right)(-2) + (3)\\left(\\frac{1}{2}\\right) \\right) = 2 \\left( -2 - 1 + \\frac{3}{2} \\right) = 2 \\left( -\\frac{3}{2} \\right) = -3\n$$\nFor $m=2$:\n$$\nR_x[2] = \\sigma_{w}^{2} (b_2 b_0 + b_3 b_1) = 2 \\left( \\left(\\frac{1}{2}\\right)(1) + (3)(-2) \\right) = 2 \\left( \\frac{1}{2} - 6 \\right) = 2 \\left( -\\frac{11}{2} \\right) = -11\n$$\nFor $m=3$:\n$$\nR_x[3] = \\sigma_{w}^{2} (b_3 b_0) = 2 \\left( (3)(1) \\right) = 6\n$$\nFor $|m| > 3$, $R_x[m]=0$. The autocovariance function is even, so $R_x[-m] = R_x[m]$.\n\nNext, we compute the variance of the time average $\\overline{x}_{N} = \\frac{1}{N}\\sum_{n=1}^{N} x[n]$.\nThe mean of the time average is $E[\\overline{x}_{N}] = \\frac{1}{N}\\sum_{n=1}^{N} E[x[n]] = 0$.\nTherefore, its variance is\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = E[\\overline{x}_{N}^2] = E\\left[ \\left(\\frac{1}{N}\\sum_{n=1}^{N} x[n]\\right)^2 \\right] = \\frac{1}{N^2} E\\left[ \\left(\\sum_{n=1}^{N} x[n]\\right) \\left(\\sum_{l=1}^{N} x[l]\\right) \\right]\n$$\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\sum_{n=1}^{N} \\sum_{l=1}^{N} E[x[n]x[l]] = \\frac{1}{N^2} \\sum_{n=1}^{N} \\sum_{l=1}^{N} R_x[n-l]\n$$\nTo evaluate the double summation, let $m = n-l$. For a fixed $m$, the number of pairs $(n, l)$ in the summation range $1 \\le n,l \\le N$ such that $n-l = m$ is $N - |m|$. The lag $m$ ranges from $-(N-1)$ to $N-1$.\n$$\n\\sum_{n=1}^{N} \\sum_{l=1}^{N} R_x[n-l] = \\sum_{m=-(N-1)}^{N-1} (N - |m|) R_x[m]\n$$\nThe problem specifies $N \\ge 4$, so $N-1 \\ge 3$. Since $R_x[m]=0$ for $|m|>3$, the sum simplifies to\n$$\n\\sum_{m=-3}^{3} (N - |m|) R_x[m]\n$$\nUsing the even property $R_x[-m]=R_x[m]$, we can write this as\n$$\n(N-0)R_x[0] + \\sum_{m=1}^{3} (N-m)R_x[-m] + \\sum_{m=1}^{3} (N-m)R_x[m] = N R_x[0] + 2 \\sum_{m=1}^{3} (N-m)R_x[m]\n$$\nThe sum becomes\n$$\nN R_x[0] + 2(N-1)R_x[1] + 2(N-2)R_x[2] + 2(N-3)R_x[3]\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\left[ N R_x[0] + 2(N-1)R_x[1] + 2(N-2)R_x[2] + 2(N-3)R_x[3] \\right]\n$$\nNow, we substitute the calculated values of $R_x[m]$:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\left[ N \\left(\\frac{57}{2}\\right) + 2(N-1)(-3) + 2(N-2)(-11) + 2(N-3)(6) \\right]\n$$\nExpand the terms inside the brackets:\n$$\n\\frac{57}{2}N - 6(N-1) - 22(N-2) + 12(N-3) = \\frac{57}{2}N - 6N + 6 - 22N + 44 + 12N - 36\n$$\nGroup terms by powers of $N$:\n$$\n\\left(\\frac{57}{2} - 6 - 22 + 12\\right)N + (6 + 44 - 36) = \\left(\\frac{57}{2} - 16\\right)N + 14 = \\left(\\frac{57 - 32}{2}\\right)N + 14 = \\frac{25}{2}N + 14\n$$\nTherefore, the exact variance is\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2}\\left(\\frac{25}{2}N + 14\\right) = \\frac{25}{2N} + \\frac{14}{N^2}\n$$\nThis expression provides the exact variance for any $N \\ge 4$. It also explicitly shows the asymptotic behavior. The leading term, which dictates the rate of convergence, is $\\frac{25}{2N}$. The next-order term is $\\frac{14}{N^2}$.\n\nTo present the result as a single closed-form expression, we combine the terms over a common denominator:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{25N}{2N^2} + \\frac{28}{2N^2} = \\frac{25N + 28}{2N^2}\n$$\nThis is the final analytical expression for the variance of the time average.", "answer": "$$\n\\boxed{\\frac{25N + 28}{2N^2}}\n$$", "id": "2869728"}, {"introduction": "While many processes are ergodic in the mean, it is crucial to understand when and why this property fails. This problem illustrates a canonical example of a non-ergodic process by introducing a random, time-invariant component—a random DC offset—to an otherwise well-behaved signal [@problem_id:2869742]. By analyzing this construction, you will gain a concrete understanding of how a process can be wide-sense stationary (WSS) yet have time averages that converge to a random variable rather than a deterministic constant.", "problem": "Consider the following explicit construction. Let the stochastic process $v[n]$ be a finite linear combination of sinusoids with random phases:\n$$\nv[n] \\triangleq 2\\cos\\!\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg) + 3\\cos\\!\\big(\\sqrt{2}\\, n + \\Theta_2\\big) + \\cos\\!\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg),\n$$\nwhere $\\Theta_1$, $\\Theta_2$, and $\\Theta_3$ are independent and identically distributed, each uniform on $[0,2\\pi)$, and independent of all other sources of randomness. Define a random variable $U$ with distribution $U \\sim \\mathcal{N}(0,\\sigma_U^2)$ for some fixed $\\sigma_U^2>0$, independent of $(\\Theta_1,\\Theta_2,\\Theta_3)$. Define the process\n$$\nx[n] \\triangleq U + v[n], \\quad n \\in \\mathbb{Z}.\n$$\nStarting only from the definitions of wide-sense stationarity and ergodicity in the mean, and standard properties of trigonometric sums derived from first principles, do the following:\n\n- Justify from definitions that $x[n]$ is wide-sense stationary (that is, its mean is constant and its autocorrelation depends only on the lag).\n- Compute the almost sure limit of the time average\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n].\n$$\n\nYour final answer must be a single, closed-form analytic expression for this limit in terms of the primitive random variable(s) introduced above. No numerical rounding is required. Express the final answer without any units. Use radians for any angles that may appear in intermediate steps.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\nStep 1: Extract Givens.\n- The stochastic process $v[n]$ is defined as $v[n] \\triangleq 2\\cos(\\frac{\\pi}{4} n + \\Theta_1) + 3\\cos(\\sqrt{2}\\, n + \\Theta_2) + \\cos(\\frac{5\\pi}{7} n + \\Theta_3)$.\n- The random variables $\\Theta_1$, $\\Theta_2$, $\\Theta_3$ are independent and identically distributed (i.i.d.) with a uniform distribution on the interval $[0, 2\\pi)$.\n- A random variable $U$ is defined with a normal distribution $U \\sim \\mathcal{N}(0,\\sigma_U^2)$ for some constant $\\sigma_U^2 > 0$.\n- The random variable $U$ is independent of the random vector $(\\Theta_1, \\Theta_2, \\Theta_3)$.\n- The stochastic process $x[n]$ is defined as $x[n] \\triangleq U + v[n]$ for all integers $n \\in \\mathbb{Z}$.\n- The task is twofold: first, to justify from formal definitions that $x[n]$ is wide-sense stationary (WSS), and second, to compute the almost sure limit of the time average $\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n]$.\n- The solution must be derived from first principles.\n\nStep 2: Validate Using Extracted Givens.\n- The problem is scientifically grounded. It presents a standard construction of a stochastic process and asks for analysis of its fundamental properties, WSS and ergodicity, which are core concepts in signal processing and probability theory.\n- The problem is well-posed. The definitions of the processes and random variables are explicit and self-consistent. The questions posed have unique, derivable answers based on established mathematical theory.\n- The problem is objective and uses precise mathematical language. There are no subjective or ambiguous statements.\n- The problem is complete. All necessary information regarding the distributions and independence of random variables is provided. No essential data is missing.\n- The problem is not trivial. It requires a careful application of definitions and an understanding of how a random DC offset affects ergodicity, as well as the properties of time averages of sinusoids with both rational and irrational frequencies relative to $\\pi$.\n\nStep 3: Verdict and Action.\nThe problem is valid. A rigorous solution will be constructed.\n\nA discrete-time stochastic process $x[n]$ is defined as wide-sense stationary (WSS) if it satisfies two conditions:\n1.  The mean function $m_x[n] = E[x[n]]$ is a constant, independent of the time index $n$.\n2.  The autocorrelation function $R_{xx}[n_1, n_2] = E[x[n_1]x[n_2]]$ depends only on the time lag $\\tau = n_1 - n_2$.\n\nWe will now verify these two conditions for the process $x[n] = U + v[n]$.\n\nFirst, we compute the mean function $m_x[n]$. By the linearity of the expectation operator:\n$$\nm_x[n] = E[x[n]] = E[U + v[n]] = E[U] + E[v[n]].\n$$\nThe random variable $U$ is given to follow a normal distribution $\\mathcal{N}(0, \\sigma_U^2)$, so its expectation is $E[U] = 0$.\nNext, we compute the expectation of $v[n]$:\n$$\nE[v[n]] = E\\bigg[2\\cos\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg) + 3\\cos\\big(\\sqrt{2}\\, n + \\Theta_2\\big) + \\cos\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg)\\bigg].\n$$\nUsing the linearity of expectation again:\n$$\nE[v[n]] = 2E\\bigg[\\cos\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg)\\bigg] + 3E\\bigg[\\cos\\big(\\sqrt{2}\\, n + \\Theta_2\\big)\\bigg] + E\\bigg[\\cos\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg)\\bigg].\n$$\nEach term involves the expectation of a cosine with a random phase $\\Theta_i \\sim U[0, 2\\pi)$. For a generic term with frequency $\\omega$ and phase $\\Theta \\sim U[0, 2\\pi)$, the expectation is calculated by definition:\n$$\nE[\\cos(\\omega n + \\Theta)] = \\int_{0}^{2\\pi} \\cos(\\omega n + \\theta) \\frac{1}{2\\pi} d\\theta = \\frac{1}{2\\pi} \\bigg[\\sin(\\omega n + \\theta)\\bigg]_{\\theta=0}^{\\theta=2\\pi}.\n$$\n$$\n= \\frac{1}{2\\pi} \\big(\\sin(\\omega n + 2\\pi) - \\sin(\\omega n)\\big) = \\frac{1}{2\\pi} \\big(\\sin(\\omega n) - \\sin(\\omega n)\\big) = 0.\n$$\nSince this result is $0$ for any $\\omega$ and $n$, each of the three terms in the expression for $E[v[n]]$ is zero. Thus, $E[v[n]] = 0$ for all $n \\in \\mathbb{Z}$.\nCombining these results, the mean of $x[n]$ is:\n$$\nm_x[n] = E[U] + E[v[n]] = 0 + 0 = 0.\n$$\nThe mean is $0$, a constant, so the first condition for WSS is satisfied.\n\nSecond, we compute the autocorrelation function $R_{xx}[n_1, n_2]$.\n$$\nR_{xx}[n_1, n_2] = E[x[n_1]x[n_2]] = E\\big[(U + v[n_1])(U + v[n_2])\\big].\n$$\nExpanding the product:\n$$\nR_{xx}[n_1, n_2] = E[U^2 + U v[n_2] + v[n_1] U + v[n_1]v[n_2]].\n$$\nBy linearity of expectation:\n$$\nR_{xx}[n_1, n_2] = E[U^2] + E[U v[n_2]] + E[v[n_1]U] + E[v[n_1]v[n_2]].\n$$\nThe random variable $U$ is independent of $(\\Theta_1, \\Theta_2, \\Theta_3)$, and thus independent of $v[n_1]$ and $v[n_2]$. Therefore, the expectation of their products separates:\n$E[U v[n_2]] = E[U]E[v[n_2]] = 0 \\cdot 0 = 0$.\n$E[v[n_1]U] = E[v[n_1]]E[U] = 0 \\cdot 0 = 0$.\nThe term $E[U^2]$ is the second moment of $U$. Since $U \\sim \\mathcal{N}(0, \\sigma_U^2)$, its variance is $\\text{Var}(U) = E[U^2] - (E[U])^2$. Thus, $E[U^2] = \\text{Var}(U) + (E[U])^2 = \\sigma_U^2 + 0^2 = \\sigma_U^2$.\nThe autocorrelation function simplifies to:\n$$\nR_{xx}[n_1, n_2] = \\sigma_U^2 + E[v[n_1]v[n_2]].\n$$\nWe now compute the autocorrelation of $v[n]$, denoted $R_{vv}[n_1, n_2]$. Let $v_k[n] = A_k \\cos(\\omega_k n + \\Theta_k)$ for $k \\in \\{1, 2, 3\\}$, where $(A_1, \\omega_1) = (2, \\frac{\\pi}{4})$, $(A_2, \\omega_2) = (3, \\sqrt{2})$, and $(A_3, \\omega_3) = (1, \\frac{5\\pi}{7})$.\n$$\nR_{vv}[n_1, n_2] = E\\bigg[\\bigg(\\sum_{k=1}^3 v_k[n_1]\\bigg)\\bigg(\\sum_{j=1}^3 v_j[n_2]\\bigg)\\bigg] = \\sum_{k=1}^3 \\sum_{j=1}^3 E[v_k[n_1]v_j[n_2]].\n$$\nThe random phases $\\Theta_k$ are independent for different $k$. For any cross-term where $k \\neq j$:\n$$\nE[v_k[n_1]v_j[n_2]] = E[A_k \\cos(\\omega_k n_1 + \\Theta_k) A_j \\cos(\\omega_j n_2 + \\Theta_j)].\n$$\nDue to independence of $\\Theta_k$ and $\\Theta_j$:\n$$\nE[v_k[n_1]v_j[n_2]] = A_k A_j E[\\cos(\\omega_k n_1 + \\Theta_k)] E[\\cos(\\omega_j n_2 + \\Theta_j)] = A_k A_j \\cdot 0 \\cdot 0 = 0.\n$$\nThus, only the terms with $k=j$ survive:\n$$\nR_{vv}[n_1, n_2] = \\sum_{k=1}^3 E[v_k[n_1]v_k[n_2]] = \\sum_{k=1}^3 E[A_k^2 \\cos(\\omega_k n_1 + \\Theta_k) \\cos(\\omega_k n_2 + \\Theta_k)].\n$$\nWe use the trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$.\n$$\nE[A_k^2 \\cos(\\omega_k n_1 + \\Theta_k) \\cos(\\omega_k n_2 + \\Theta_k)] = \\frac{A_k^2}{2} E[\\cos(\\omega_k(n_1 - n_2)) + \\cos(\\omega_k(n_1+n_2) + 2\\Theta_k)].\n$$\nThe term $\\cos(\\omega_k(n_1 - n_2))$ is deterministic, so its expectation is itself. The expectation of the second term is:\n$$\nE[\\cos(\\omega_k(n_1+n_2) + 2\\Theta_k)] = \\int_0^{2\\pi} \\cos(\\omega_k(n_1+n_2) + 2\\theta) \\frac{1}{2\\pi} d\\theta.\n$$\nThis integral evaluates to $0$ because it is an integral of a sinusoid over two full periods of its argument's phase component ($2\\theta$ spans $[0, 4\\pi)$). Formally, $\\frac{1}{2\\pi}[\\frac{1}{2}\\sin(\\omega_k(n_1+n_2) + 2\\theta)]_0^{2\\pi} = 0$.\nTherefore, for each $k$:\n$$\nE[v_k[n_1]v_k[n_2]] = \\frac{A_k^2}{2} \\cos(\\omega_k(n_1 - n_2)).\n$$\nThis depends only on the lag $\\tau = n_1 - n_2$. Summing over $k$:\n$$\nR_{vv}[n_1, n_2] = \\frac{2^2}{2}\\cos\\bigg(\\frac{\\pi}{4}(n_1-n_2)\\bigg) + \\frac{3^2}{2}\\cos\\big(\\sqrt{2}(n_1-n_2)\\big) + \\frac{1^2}{2}\\cos\\bigg(\\frac{5\\pi}{7}(n_1-n_2)\\bigg).\n$$\nLet $\\tau=n_1-n_2$. Then $R_{vv}(\\tau) = 2\\cos(\\frac{\\pi}{4}\\tau) + \\frac{9}{2}\\cos(\\sqrt{2}\\tau) + \\frac{1}{2}\\cos(\\frac{5\\pi}{7}\\tau)$.\nFinally, the full autocorrelation function for $x[n]$ is:\n$$\nR_{xx}[n_1, n_2] = \\sigma_U^2 + 2\\cos\\bigg(\\frac{\\pi}{4}(n_1-n_2)\\bigg) + \\frac{9}{2}\\cos\\big(\\sqrt{2}(n_1-n_2)\\big) + \\frac{1}{2}\\cos\\bigg(\\frac{5\\pi}{7}(n_1-n_2)\\bigg).\n$$\nThis function depends only on the time lag $\\tau = n_1 - n_2$. Since both conditions are met, the process $x[n]$ is wide-sense stationary.\n\nNow, we proceed to the second part of the problem: computing the almost sure limit of the time average of $x[n]$.\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n] = \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} (U + v[n]).\n$$\nBy the linearity of summation and limits:\n$$\n= \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} U + \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} v[n].\n$$\nFor the first term, $U$ is a random variable that does not depend on the time index $n$. For any particular realization of the process, $U$ is a constant.\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} U = \\lim_{N \\to \\infty} \\frac{N \\cdot U}{N} = \\lim_{N \\to \\infty} U = U.\n$$\nFor the second term, we analyze the time average of $v[n]$ by analyzing each of its sinusoidal components. We need to evaluate the time average of a generic sinusoid $\\cos(\\omega n + \\Theta)$. This is derived from the time average of a complex exponential $e^{j\\omega_0 n}$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega_0 n}.\n$$\nIf $\\omega_0 = 2\\pi k$ for some integer $k$, then $e^{j\\omega_0 n} = e^{j2\\pi k n} = (e^{j2\\pi k})^n = 1^n=1$. The sum is $N$, and the limit of the average is $1$.\nIf $\\omega_0 \\neq 2\\pi k$ for any integer $k$, the sum is a finite geometric series:\n$$\n\\sum_{n=0}^{N-1} (e^{j\\omega_0})^n = \\frac{1 - (e^{j\\omega_0})^N}{1 - e^{j\\omega_0}}.\n$$\nThe magnitude of the numerator is bounded: $|1 - e^{j\\omega_0 N}| \\leq |1| + |e^{j\\omega_0 N}| = 1+1=2$. The denominator $1 - e^{j\\omega_0}$ is a non-zero constant. Thus, the sum is bounded in magnitude. The time average is this bounded quantity divided by $N$, which converges to $0$ as $N \\to \\infty$.\nIn summary, $\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega_0 n} = \\delta(\\omega_0 \\text{ mod } 2\\pi)$, where $\\delta$ is $1$ if the argument is $0$ and $0$ otherwise.\n\nThe time average of $\\cos(\\omega n + \\Theta)$ is:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} \\cos(\\omega n + \\Theta) = \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} \\frac{e^{j(\\omega n + \\Theta)} + e^{-j(\\omega n + \\Theta)}}{2}\n$$\n$$\n= \\frac{e^{j\\Theta}}{2} \\bigg(\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega n}\\bigg) + \\frac{e^{-j\\Theta}}{2} \\bigg(\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{-j\\omega n}\\bigg).\n$$\nThis limit is non-zero only if $\\omega = 2\\pi k$ or $-\\omega = 2\\pi k$ for some integer $k$. In other words, $\\omega$ must be an integer multiple of $2\\pi$.\nLet us examine the frequencies in $v[n]$:\n1.  $\\omega_1 = \\frac{\\pi}{4}$. This is not an integer multiple of $2\\pi$. Thus, the time average of $2\\cos(\\frac{\\pi}{4} n + \\Theta_1)$ is $0$.\n2.  $\\omega_2 = \\sqrt{2}$. As $\\sqrt{2}$ is irrational, $\\sqrt{2}/(2\\pi)$ is also irrational, so $\\sqrt{2}$ cannot be an integer multiple of $2\\pi$. Thus, the time average of $3\\cos(\\sqrt{2} n + \\Theta_2)$ is $0$.\n3.  $\\omega_3 = \\frac{5\\pi}{7}$. This is not an integer multiple of $2\\pi$. Thus, the time average of $\\cos(\\frac{5\\pi}{7} n + \\Theta_3)$ is $0$.\n\nSince the time average of each sinusoidal component of $v[n]$ is $0$, the time average of their sum is also $0$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} v[n] = 0.\n$$\nCombining the results for the two parts of $x[n]$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n] = U + 0 = U.\n$$\nThe almost sure limit of the time average is the random variable $U$. This implies that the process $x[n]$ is not ergodic in the mean, because its time average converges to a random variable $U$, which is almost surely not equal to the constant ensemble mean $m_x=0$.", "answer": "$$\n\\boxed{U}\n$$", "id": "2869742"}, {"introduction": "Ergodicity is not a monolithic property; a process can be ergodic in its mean but fail to be so in higher-order moments. This advanced exercise explores this subtlety by constructing a non-Gaussian process with second-order statistics identical to an ergodic Gaussian process [@problem_id:2869703]. By analyzing its fourth moment, you will discover how a random scaling factor breaks ergodicity for higher-order statistics, reinforcing the crucial lesson that a complete statistical characterization requires looking beyond the autocorrelation function.", "problem": "Let $\\{Y(t): t \\in \\mathbb{R}\\}$ be a zero-mean, unit-variance, real-valued, Gaussian, wide-sense stationary (WSS; wide-sense stationary) process with autocorrelation $R_{Y}(\\tau)$ and with an absolutely continuous power spectral density that is finite and nonzero almost everywhere. Assume $Y(t)$ is ergodic for polynomial time averages up to order $4$ in the sense that for any polynomial $p$ of degree at most $4$, the time average $\\frac{1}{T}\\int_{0}^{T} p(Y(t))\\,dt$ converges almost surely (a.s.; almost surely) to the ensemble expectation $\\mathbb{E}\\{p(Y(0))\\}$ as $T \\to \\infty$. Let $A$ be a nonnegative random variable independent of $Y(t)$, constant in time within each realization, with $\\mathbb{E}\\{A^{2}\\} = 1$ and $\\mathbb{E}\\{A^{4}\\} = \\kappa$ for some fixed $\\kappa \\in (1,\\infty)$. Define the process $X(t) = A \\, Y(t)$.\n\nYou will analyze how $X(t)$ inherits second-order statistics from $Y(t)$ but fails to be ergodic in the fourth moment due to the deterministic-in-time component $A$.\n\nUsing only the core definitions of wide-sense stationarity, time averages, and ergodicity, along with well-tested facts about Gaussian moments, do the following:\n\n1. Show that $X(t)$ is WSS and that $R_{X}(\\tau) = R_{Y}(\\tau)$, so that $X(t)$ has identical second-order statistics to $Y(t)$.\n2. Argue that the one-dimensional marginal distribution of $X(t)$ is non-Gaussian whenever $\\operatorname{Var}(A) > 0$.\n3. Define the finite-time fourth-moment time average\n$$\n\\widehat{m}_{4}^{(T)} \\triangleq \\frac{1}{T}\\int_{0}^{T} X(t)^{4}\\,dt.\n$$\nDerive the a.s. limit of $\\widehat{m}_{4}^{(T)}$ as $T \\to \\infty$ and express it in closed form.\n\nYour final answer must be the single analytic expression for the almost sure limit of $\\widehat{m}_{4}^{(T)}$. No rounding is required, and no units are involved.", "solution": "The problem statement must first be rigorously validated.\n\nStep 1: Extracted Givens.\n- Process $\\{Y(t): t \\in \\mathbb{R}\\}$ is a zero-mean, unit-variance, real-valued, Gaussian, wide-sense stationary (WSS) process.\n- Autocorrelation of $Y(t)$ is $R_{Y}(\\tau)$.\n- Its power spectral density is absolutely continuous, finite, and nonzero almost everywhere.\n- $Y(t)$ is ergodic for polynomial time averages up to order $4$, meaning for any polynomial $p$ of degree at most $4$, $\\frac{1}{T}\\int_{0}^{T} p(Y(t))\\,dt \\to \\mathbb{E}\\{p(Y(0))\\}$ almost surely as $T \\to \\infty$.\n- $A$ is a nonnegative random variable, independent of $Y(t)$.\n- $A$ is constant in time for each realization.\n- $\\mathbb{E}\\{A^{2}\\} = 1$.\n- $\\mathbb{E}\\{A^{4}\\} = \\kappa$, with $\\kappa \\in (1,\\infty)$.\n- Process $X(t) = A \\, Y(t)$.\n- Time average definition: $\\widehat{m}_{4}^{(T)} \\triangleq \\frac{1}{T}\\int_{0}^{T} X(t)^{4}\\,dt$.\n\nStep 2: Validation.\nThe problem is scientifically grounded, situated within the established mathematical theory of stochastic processes. It is a well-posed problem with sufficient and consistent data. For instance, the conditions $\\mathbb{E}\\{A^2\\} = 1$ and $\\mathbb{E}\\{A^4\\} = \\kappa > 1$ imply that $\\operatorname{Var}(A^2) = \\mathbb{E}\\{(A^2)^2\\} - (\\mathbb{E}\\{A^2\\})^2 = \\kappa - 1 > 0$, which confirms that $A$ is indeed a non-constant random variable. The language is objective and formal. The problem avoids all specified flaws such as scientific unsoundness, incompleteness, or ambiguity. It presents a non-trivial conceptual challenge regarding the nature of ergodicity.\n\nStep 3: Verdict.\nThe problem is valid. A solution will be provided.\n\nThe analysis proceeds in three parts as requested.\n\n1.  Wide-Sense Stationarity of $X(t)$\nA process is WSS if its mean function is constant and its autocorrelation function depends only on the time difference.\n\nFirst, we compute the mean of $X(t)$:\n$$\n\\mathbb{E}\\{X(t)\\} = \\mathbb{E}\\{A Y(t)\\}\n$$\nDue to the independence of $A$ and the process $Y(t)$, we can separate the expectations:\n$$\n\\mathbb{E}\\{X(t)\\} = \\mathbb{E}\\{A\\} \\mathbb{E}\\{Y(t)\\}\n$$\nThe problem states that $Y(t)$ is a zero-mean process, so $\\mathbb{E}\\{Y(t)\\} = 0$ for all $t \\in \\mathbb{R}$. Consequently,\n$$\n\\mathbb{E}\\{X(t)\\} = \\mathbb{E}\\{A\\} \\cdot 0 = 0\n$$\nThe mean of $X(t)$ is constant and equal to $0$.\n\nNext, we compute the autocorrelation function of $X(t)$:\n$$\nR_{X}(t, t+\\tau) = \\mathbb{E}\\{X(t) X(t+\\tau)\\} = \\mathbb{E}\\{ (A Y(t)) (A Y(t+\\tau)) \\} = \\mathbb{E}\\{A^2 Y(t) Y(t+\\tau)\\}\n$$\nSince the random variable $A$ is independent of the entire process $Y(t)$, it is independent of the product $Y(t) Y(t+\\tau)$. We can again separate the expectations:\n$$\nR_{X}(t, t+\\tau) = \\mathbb{E}\\{A^2\\} \\mathbb{E}\\{Y(t) Y(t+\\tau)\\}\n$$\nWe are given that $\\mathbb{E}\\{A^2\\} = 1$. We are also given that $Y(t)$ is WSS with autocorrelation function $R_{Y}(\\tau) = \\mathbb{E}\\{Y(t) Y(t+\\tau)\\}$. Substituting these into the expression for $R_{X}(t, t+\\tau)$ yields:\n$$\nR_{X}(t, t+\\tau) = 1 \\cdot R_{Y}(\\tau) = R_{Y}(\\tau)\n$$\nThe autocorrelation function depends only on the time lag $\\tau$, not on $t$. Since the mean is constant and the autocorrelation function depends only on $\\tau$, the process $X(t)$ is WSS. Furthermore, we have shown that $R_{X}(\\tau) = R_{Y}(\\tau)$. Thus, $X(t)$ and $Y(t)$ share the same second-order statistics.\n\n2.  Non-Gaussian Nature of $X(t)$\nWe will demonstrate that the one-dimensional marginal distribution of $X(t)$ is not Gaussian when $\\operatorname{Var}(A) > 0$. First, note that if $\\operatorname{Var}(A) = 0$, then $A$ is a constant. Since $A$ is non-negative and $\\mathbb{E}\\{A^2\\} = 1$, we must have $A=1$ almost surely. In this case, $\\mathbb{E}\\{A^4\\} = 1$, which contradicts the given condition $\\kappa > 1$. Therefore, we must have $\\operatorname{Var}(A) > 0$.\n\nConsider the characteristic function of $X(t)$ for a fixed $t$:\n$$\n\\Phi_{X(t)}(\\omega) = \\mathbb{E}\\{\\exp(i\\omega X(t))\\} = \\mathbb{E}\\{\\exp(i\\omega A Y(t))\\}\n$$\nWe evaluate this by conditioning on the value of $A$:\n$$\n\\Phi_{X(t)}(\\omega) = \\mathbb{E}_{A}\\left[ \\mathbb{E}_{Y(t)}\\{\\exp(i\\omega A Y(t)) | A\\} \\right]\n$$\nGiven $A=a$, the random variable $a Y(t)$ is Gaussian, since $Y(t)$ is Gaussian. $Y(t)$ is zero-mean and unit-variance, so $Y(t) \\sim \\mathcal{N}(0,1)$. Thus, $a Y(t) \\sim \\mathcal{N}(0, a^2)$. The characteristic function of a $\\mathcal{N}(0, a^2)$ random variable is $\\exp(-\\frac{1}{2}a^2\\omega^2)$. Therefore:\n$$\n\\mathbb{E}_{Y(t)}\\{\\exp(i\\omega A Y(t)) | A=a\\} = \\exp\\left(-\\frac{1}{2}a^2\\omega^2\\right)\n$$\nTaking the expectation with respect to $A$, we get:\n$$\n\\Phi_{X(t)}(\\omega) = \\mathbb{E}_{A}\\left[ \\exp\\left(-\\frac{1}{2}A^2\\omega^2\\right) \\right]\n$$\nIf $X(t)$ were Gaussian, it would have to be zero-mean and have variance $R_{X}(0) = R_{Y}(0) = 1$. A standard Gaussian random variable has characteristic function $\\exp(-\\frac{1}{2}\\omega^2)$. We must check if $\\mathbb{E}_{A}[ \\exp(-\\frac{1}{2}A^2\\omega^2) ]$ is equal to $\\exp(-\\frac{1}{2}\\omega^2)$.\n\nLet $f(u) = \\exp(-u)$ for $u \\ge 0$. This function is strictly convex. For $\\omega \\neq 0$, let the random variable be $U = \\frac{1}{2}A^2\\omega^2$. By Jensen's inequality, for a non-constant positive random variable $U$, $\\mathbb{E}\\{f(U)\\} > f(\\mathbb{E}\\{U\\})$. The random variable $A^2$ is not constant, as $\\operatorname{Var}(A^2) = \\mathbb{E}\\{A^4\\} - (\\mathbb{E}\\{A^2\\})^2 = \\kappa - 1 > 0$. Thus, $U$ is not constant for $\\omega \\neq 0$.\nApplying Jensen's inequality:\n$$\n\\mathbb{E}_{A}\\left[ \\exp\\left(-\\frac{1}{2}A^2\\omega^2\\right) \\right] > \\exp\\left(-\\mathbb{E}_{A}\\left[\\frac{1}{2}A^2\\omega^2\\right]\\right)\n$$\nThe argument of the exponent on the right side is:\n$$\n-\\mathbb{E}_{A}\\left[\\frac{1}{2}A^2\\omega^2\\right] = -\\frac{1}{2}\\omega^2\\mathbb{E}\\{A^2\\} = -\\frac{1}{2}\\omega^2\n$$\nSo, for any $\\omega \\neq 0$:\n$$\n\\Phi_{X(t)}(\\omega) > \\exp\\left(-\\frac{1}{2}\\omega^2\\right)\n$$\nSince the characteristic function of $X(t)$ is not that of a standard normal random variable, its distribution is non-Gaussian.\n\n3.  Almost Sure Limit of the Fourth-Moment Time Average\nWe are asked to find the almost sure limit of $\\widehat{m}_{4}^{(T)}$ as $T \\to \\infty$.\n$$\n\\widehat{m}_{4}^{(T)} = \\frac{1}{T}\\int_{0}^{T} X(t)^{4}\\,dt = \\frac{1}{T}\\int_{0}^{T} (A Y(t))^{4}\\,dt\n$$\nThe random variable $A$ is constant in time for any given realization of the process. Therefore, we can factor $A^4$ out of the integral:\n$$\n\\widehat{m}_{4}^{(T)} = A^4 \\left( \\frac{1}{T}\\int_{0}^{T} Y(t)^{4}\\,dt \\right)\n$$\nThe problem states that $Y(t)$ is ergodic for polynomial time averages up to order $4$. This implies that the time average of $Y(t)^4$ converges almost surely to its ensemble average:\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} Y(t)^{4}\\,dt = \\mathbb{E}\\{Y(0)^4\\} \\quad \\text{a.s.}\n$$\nThe limit is a deterministic constant. To find its value, we use the properties of the Gaussian process $Y(t)$. $Y(0)$ is a Gaussian random variable with zero mean and unit variance, i.e., $Y(0) \\sim \\mathcal{N}(0,1)$. The fourth moment of a zero-mean Gaussian random variable with variance $\\sigma^2$ is $3\\sigma^4$. For $\\sigma^2=1$, we have:\n$$\n\\mathbb{E}\\{Y(0)^4\\} = 3\n$$\nNow we can determine the limit of $\\widehat{m}_{4}^{(T)}$. Since $A^4$ is a time-invariant random variable for each realization and the integral part converges a.s. to a constant, the limit of their product is the product of the random variable and the limit:\n$$\n\\lim_{T \\to \\infty} \\widehat{m}_{4}^{(T)} = A^4 \\left( \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} Y(t)^{4}\\,dt \\right) \\quad \\text{a.s.}\n$$\n$$\n\\lim_{T \\to \\infty} \\widehat{m}_{4}^{(T)} = A^4 \\cdot \\mathbb{E}\\{Y(0)^4\\} = A^4 \\cdot 3 \\quad \\text{a.s.}\n$$\nThe almost sure limit of the finite-time fourth-moment average is the random variable $3A^4$. This result demonstrates that the process $X(t)$ is not ergodic in the fourth moment, because its time average converges to a random variable, not to the constant ensemble average $\\mathbb{E}\\{X(0)^4\\} = \\mathbb{E}\\{A^4 Y(0)^4\\} = \\mathbb{E}\\{A^4\\}\\mathbb{E}\\{Y(0)^4\\} = 3\\kappa$.\n\nThe final answer is the expression for this limit.", "answer": "$$\n\\boxed{3A^{4}}\n$$", "id": "2869703"}]}