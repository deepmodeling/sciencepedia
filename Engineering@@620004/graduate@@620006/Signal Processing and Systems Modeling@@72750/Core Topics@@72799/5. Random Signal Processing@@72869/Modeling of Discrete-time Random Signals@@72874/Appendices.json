{"hands_on_practices": [{"introduction": "Understanding how a linear system shapes a random signal is a cornerstone of signal processing. This exercise asks you to derive the fundamental statistical properties—the autocorrelation function and the power spectral density (PSD)—of a signal generated by passing white noise through a simple first-order autoregressive (AR) filter. By working from first principles, you will solidify your understanding of the deep connection between a system's time-domain characteristics (its impulse response) and the frequency-domain properties of the signals it produces.", "problem": "A discrete-time, linear time-invariant (LTI) system has transfer function $H(z)=\\frac{1}{1-0.8 z^{-1}}$. The input is a zero-mean, unit-variance, white Gaussian noise process $w[n]$ with $w[n]\\sim\\mathcal{N}(0,1)$ and $\\mathbb{E}\\{w[n]w[m]\\}=\\delta[n-m]$. Let $x[n]$ denote the output in the steady-state wide-sense stationary (WSS) regime, assuming the system is initially at rest and has been driven for a sufficiently long time for transients to decay.\n\nStarting only from the fundamental definitions of autocorrelation $r_{x}[k]=\\mathbb{E}\\{x[n]\\,x[n-k]\\}$, whiteness of $w[n]$, linear time-invariance, and the definition of power spectral density (PSD) $S_{x}(\\mathrm{e}^{j\\omega})$ as the discrete-time Fourier transform of $r_{x}[k]$, derive a closed-form expression for the autocorrelation sequence $r_{x}[k]$ for all integer lags $k$, and the PSD $S_{x}(\\mathrm{e}^{j\\omega})$ for $\\omega\\in[-\\pi,\\pi]$.\n\nExpress your final answer as a single closed-form analytic expression containing both results as a row matrix in the order $[r_{x}[k],\\,S_{x}(\\mathrm{e}^{j\\omega})]$. No rounding is required. No units are involved.", "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim from the problem statement:\n- System type: discrete-time, linear time-invariant (LTI).\n- Transfer function: $H(z)=\\frac{1}{1-0.8 z^{-1}}$.\n- Input signal: $w[n]$, a zero-mean, unit-variance, white Gaussian noise process.\n- Input properties: $w[n]\\sim\\mathcal{N}(0,1)$, $\\mathbb{E}\\{w[n]w[m]\\}=\\delta[n-m]$.\n- Output signal: $x[n]$ in the steady-state wide-sense stationary (WSS) regime.\n- Initial conditions: system is initially at rest.\n- Task: Derive closed-form expressions for the autocorrelation sequence $r_{x}[k]=\\mathbb{E}\\{x[n]\\,x[n-k]\\}$ and the power spectral density (PSD) $S_{x}(\\mathrm{e}^{j\\omega})$ as the discrete-time Fourier transform (DTFT) of $r_{x}[k]$.\n- Constraint: Derivations must start from fundamental definitions.\n\nValidation against criteria:\n- **Scientific Groundedness**: The problem is a standard exercise in stochastic signal processing, involving the analysis of a WSS random process filtered by a stable LTI system. All concepts—LTI systems, white noise, autocorrelation, PSD—are fundamental and well-established in the field of signal processing. The problem is scientifically sound.\n- **Well-Posedness**: The transfer function $H(z)$ has a single pole at $z=0.8$. Since the pole is inside the unit circle ($|0.8|<1$), the system is stable. A stable LTI system driven by a WSS input process will produce a WSS output in the steady state. The input process is fully characterized. The problem is well-posed and a unique solution exists.\n- **Objectivity**: The problem is stated in precise, technical language, free from ambiguity or subjective content.\n\nConclusion of validation: The problem is valid. It is scientifically sound, self-contained, and well-posed. We proceed to the solution.\n\nThe output $x[n]$ of an LTI system is given by the convolution of the input $w[n]$ with the system's impulse response $h[n]$:\n$$x[n] = \\sum_{m=-\\infty}^{\\infty} h[m] w[n-m]$$\nThe impulse response $h[n]$ is the inverse Z-transform of the transfer function $H(z)$. For the given causal system, we have:\n$$H(z) = \\frac{1}{1-0.8 z^{-1}} = \\sum_{n=0}^{\\infty} (0.8)^n z^{-n}$$\nBy inspection, the impulse response is $h[n] = (0.8)^n u[n]$, where $u[n]$ is the discrete-time unit step function.\n\nThe autocorrelation of the output process $x[n]$ is defined as $r_{x}[k] = \\mathbb{E}\\{x[n] x[n-k]\\}$. Substituting the convolution expression for $x[n]$:\n$$r_{x}[k] = \\mathbb{E}\\left\\{ \\left(\\sum_{m=-\\infty}^{\\infty} h[m] w[n-m]\\right) \\left(\\sum_{l=-\\infty}^{\\infty} h[l] w[n-k-l]\\right) \\right\\}$$\nUsing the linearity of the expectation operator, we can move it inside the sums:\n$$r_{x}[k] = \\sum_{m=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} h[m] h[l] \\mathbb{E}\\{w[n-m] w[n-k-l]\\}$$\nThe input $w[n]$ is a white noise process with unit variance, meaning its autocorrelation is given by $\\mathbb{E}\\{w[i]w[j]\\} = \\delta[i-j]$. In our case, the argument of the expectation is $\\mathbb{E}\\{w[n-m] w[n-k-l]\\}$, which evaluates to $\\delta[(n-m) - (n-k-l)] = \\delta[l-m+k]$.\nSubstituting this back into the expression for $r_{x}[k]$:\n$$r_{x}[k] = \\sum_{m=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} h[m] h[l] \\delta[l-m+k]$$\nThe Kronecker delta $\\delta[l-m+k]$ is non-zero only when $l-m+k=0$, which implies $l=m-k$. We use this property to eliminate the summation over $l$:\n$$r_{x}[k] = \\sum_{m=-\\infty}^{\\infty} h[m] h[m-k]$$\nNow, we substitute the specific impulse response $h[n]=(0.8)^n u[n]$:\n$$r_{x}[k] = \\sum_{m=-\\infty}^{\\infty} (0.8)^m u[m] \\, (0.8)^{m-k} u[m-k]$$\nThe product of the unit step functions $u[m]u[m-k]$ is non-zero only for $m \\ge 0$ and $m \\ge k$. This constrains the lower limit of the summation to $m_{min} = \\max(0, k)$. The summation becomes:\n$$r_{x}[k] = \\sum_{m=\\max(0,k)}^{\\infty} (0.8)^{2m-k} = (0.8)^{-k} \\sum_{m=\\max(0,k)}^{\\infty} (0.8^2)^m = (0.8)^{-k} \\sum_{m=\\max(0,k)}^{\\infty} (0.64)^m$$\nWe evaluate this for two cases based on the sign of $k$.\n\nCase 1: $k \\ge 0$. The lower limit is $m=k$.\n$$r_{x}[k] = (0.8)^{-k} \\sum_{m=k}^{\\infty} (0.64)^m$$\nLet $j=m-k$. The sum becomes $\\sum_{j=0}^{\\infty} (0.64)^{j+k} = (0.64)^k \\sum_{j=0}^{\\infty} (0.64)^j$. This is a standard geometric series which converges to $\\frac{1}{1-0.64}$.\n$$r_{x}[k] = (0.8)^{-k} (0.64)^k \\frac{1}{1-0.64} = (0.8)^{-k} (0.8^{2})^k \\frac{1}{0.36} = (0.8)^k \\frac{1}{0.36}$$\nCase 2: $k < 0$. The lower limit is $m=0$.\n$$r_{x}[k] = (0.8)^{-k} \\sum_{m=0}^{\\infty} (0.64)^m = (0.8)^{-k} \\frac{1}{1-0.64} = (0.8)^{-k} \\frac{1}{0.36}$$\nCombining both cases, we can use the absolute value notation $|k|$. For $k \\ge 0$, $|k|=k$, and for $k<0$, $|k|=-k$. The expression is therefore:\n$$r_{x}[k] = \\frac{1}{0.36} (0.8)^{|k|}$$\nUsing exact fractions, $0.8 = \\frac{4}{5}$ and $0.36 = \\frac{9}{25}$.\n$$r_{x}[k] = \\frac{1}{9/25} \\left(\\frac{4}{5}\\right)^{|k|} = \\frac{25}{9} \\left(\\frac{4}{5}\\right)^{|k|}$$\nThis is the closed-form expression for the autocorrelation sequence.\n\nNext, we derive the power spectral density (PSD), $S_{x}(\\mathrm{e}^{j\\omega})$, which is defined as the DTFT of the autocorrelation sequence $r_{x}[k]$:\n$$S_{x}(\\mathrm{e}^{j\\omega}) = \\sum_{k=-\\infty}^{\\infty} r_{x}[k] \\mathrm{e}^{-j\\omega k} = \\sum_{k=-\\infty}^{\\infty} \\frac{25}{9} \\left(\\frac{4}{5}\\right)^{|k|} \\mathrm{e}^{-j\\omega k}$$\nWe split the sum into two parts: non-negative $k$ and negative $k$.\n$$S_{x}(\\mathrm{e}^{j\\omega}) = \\frac{25}{9} \\left( \\sum_{k=0}^{\\infty} \\left(\\frac{4}{5}\\right)^{k} \\mathrm{e}^{-j\\omega k} + \\sum_{k=-\\infty}^{-1} \\left(\\frac{4}{5}\\right)^{-k} \\mathrm{e}^{-j\\omega k} \\right)$$\nThe first sum is a geometric series: $\\sum_{k=0}^{\\infty} \\left(\\frac{4}{5} \\mathrm{e}^{-j\\omega}\\right)^k = \\frac{1}{1 - \\frac{4}{5}\\mathrm{e}^{-j\\omega}}$.\nFor the second sum, let $m=-k$. When $k=-1$, $m=1$. When $k=-\\infty$, $m=\\infty$.\n$$\\sum_{m=1}^{\\infty} \\left(\\frac{4}{5}\\right)^{m} \\mathrm{e}^{j\\omega m} = \\sum_{m=1}^{\\infty} \\left(\\frac{4}{5} \\mathrm{e}^{j\\omega}\\right)^m = \\frac{\\frac{4}{5}\\mathrm{e}^{j\\omega}}{1 - \\frac{4}{5}\\mathrm{e}^{j\\omega}}$$\nCombining the terms:\n$$S_{x}(\\mathrm{e}^{j\\omega}) = \\frac{25}{9} \\left( \\frac{1}{1 - \\frac{4}{5}\\mathrm{e}^{-j\\omega}} + \\frac{\\frac{4}{5}\\mathrm{e}^{j\\omega}}{1 - \\frac{4}{5}\\mathrm{e}^{j\\omega}} \\right)$$\nWe find a common denominator: $\\left(1 - \\frac{4}{5}\\mathrm{e}^{-j\\omega}\\right)\\left(1 - \\frac{4}{5}\\mathrm{e}^{j\\omega}\\right)$.\nThe numerator becomes: $\\left(1 - \\frac{4}{5}\\mathrm{e}^{j\\omega}\\right) + \\frac{4}{5}\\mathrm{e}^{j\\omega}\\left(1 - \\frac{4}{5}\\mathrm{e}^{-j\\omega}\\right) = 1 - \\frac{4}{5}\\mathrm{e}^{j\\omega} + \\frac{4}{5}\\mathrm{e}^{j\\omega} - \\left(\\frac{4}{5}\\right)^2 = 1 - \\frac{16}{25} = \\frac{9}{25}$.\nThe denominator is: $1 - \\frac{4}{5}(\\mathrm{e}^{j\\omega} + \\mathrm{e}^{-j\\omega}) + \\left(\\frac{4}{5}\\right)^2 = 1 - \\frac{8}{5}\\cos(\\omega) + \\frac{16}{25} = \\frac{25 - 40\\cos(\\omega) + 16}{25} = \\frac{41 - 40\\cos(\\omega)}{25}$.\nSubstituting these back:\n$$S_{x}(\\mathrm{e}^{j\\omega}) = \\frac{25}{9} \\left( \\frac{9/25}{(41 - 40\\cos(\\omega))/25} \\right) = \\frac{25}{9} \\frac{9}{41 - 40\\cos(\\omega)} = \\frac{1}{1.64 - 1.6\\cos(\\omega)}$$\nThis is the closed-form expression for the power spectral density.\n\nThe final results for $r_{x}[k]$ and $S_{x}(\\mathrm{e}^{j\\omega})$ are collected.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{25}{9} \\left(\\frac{4}{5}\\right)^{|k|} & \\frac{1}{1.64 - 1.6\\cos(\\omega)}\n\\end{pmatrix}\n}\n$$", "id": "2885717"}, {"introduction": "Beyond just calculating statistics, a key skill is interpreting what a model's parameters tell us about a signal's behavior. This practice guides you to explore the rich structure of a second-order autoregressive (AR(2)) model, which is capable of producing oscillatory behavior. You will discover how the model's coefficients determine the location of its poles in the complex plane, and how complex-conjugate poles lead directly to a damped sinusoidal autocorrelation function, providing crucial intuition for model analysis and design.", "problem": "Consider a zero-mean, wide-sense stationary discrete-time Autoregressive of order two (AR($2$)) process $x[n]$ driven by a zero-mean, finite-variance, discrete-time white noise $w[n]$ with variance $\\sigma_{w}^{2} \\in (0,\\infty)$:\n$$\nx[n] \\;=\\; a_{1}\\,x[n-1] \\;+\\; a_{2}\\,x[n-2] \\;+\\; w[n],\n$$\nwhere $a_{1},a_{2} \\in \\mathbb{R}$. Let $\\gamma_{x}[k] \\triangleq \\mathbb{E}\\{x[n]\\,x[n-k]\\}$ denote the autocovariance sequence. Starting from first principles and well-tested facts in linear systems and stochastic processes, do the following:\n\n- Derive the linear constant-coefficient difference equation obeyed by $\\gamma_{x}[k]$ for integer lags $k \\geq 1$ by multiplying the autoregressive recursion by $x[n-k]$ and taking expectations, using only the whiteness and independence properties of $w[n]$.\n- Using the theory of linear difference equations, characterize when $\\gamma_{x}[k]$ is a damped sinusoid in $k$, and explain why this is equivalent to the characteristic roots being a stable complex-conjugate pair. Relate these roots to the poles of the AR($2$) system.\n- From that characterization, express the coefficient pair $(a_{1},a_{2})$ in terms of a polar parametrization of the poles $r\\,\\exp(\\pm \\mathrm{j}\\,\\theta)$ with modulus $r \\in (0,1)$ and angle $\\theta \\in (0,\\pi)$ measured in radians, which ensures a damped sinusoidal autocovariance and stability.\n\nProvide your final answer as the row vector $\\begin{pmatrix} a_{1} & a_{2} \\end{pmatrix}$ expressed analytically in terms of $r$ and $\\theta$ (no numerical evaluation required). Do not include any inequalities in your final answer. No units are required, and no rounding is needed.", "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique, meaningful solution. We will proceed with the derivation as requested.\n\nThe Autoregressive model of order $2$ (AR($2$)) is given by the difference equation:\n$$\nx[n] \\;=\\; a_{1}\\,x[n-1] \\;+\\; a_{2}\\,x[n-2] \\;+\\; w[n]\n$$\nwhere $x[n]$ is the process output, $w[n]$ is a zero-mean white noise process with variance $\\sigma_{w}^{2}$, and $a_{1}, a_{2}$ are real coefficients. The process $x[n]$ is assumed to be zero-mean and wide-sense stationary (WSS). The autocovariance sequence is defined as $\\gamma_{x}[k] \\triangleq \\mathbb{E}\\{x[n]\\,x[n-k]\\}$.\n\nFirst, we derive the difference equation governing $\\gamma_{x}[k]$ for integer lags $k \\geq 1$. We multiply the AR($2$) recursion by $x[n-k]$:\n$$\nx[n]\\,x[n-k] \\;=\\; a_{1}\\,x[n-1]\\,x[n-k] \\;+\\; a_{2}\\,x[n-2]\\,x[n-k] \\;+\\; w[n]\\,x[n-k]\n$$\nNext, we take the expectation of both sides of this equation. By the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}\\{x[n]\\,x[n-k]\\} \\;=\\; a_{1}\\,\\mathbb{E}\\{x[n-1]\\,x[n-k]\\} \\;+\\; a_{2}\\,\\mathbb{E}\\{x[n-2]\\,x[n-k]\\} \\;+\\; \\mathbb{E}\\{w[n]\\,x[n-k]\\}\n$$\nBy definition of the autocovariance, $\\mathbb{E}\\{x[n]\\,x[n-k]\\} = \\gamma_{x}[k]$. Since the process is WSS, the autocovariance depends only on the time lag. Thus, we have:\n$$\n\\mathbb{E}\\{x[n-1]\\,x[n-k]\\} \\;=\\; \\gamma_{x}[(n-1) - (n-k)] \\;=\\; \\gamma_{x}[k-1]\n$$\n$$\n\\mathbb{E}\\{x[n-2]\\,x[n-k]\\} \\;=\\; \\gamma_{x}[(n-2) - (n-k)] \\;=\\; \\gamma_{x}[k-2]\n$$\nSubstituting these into the equation yields:\n$$\n\\gamma_{x}[k] \\;=\\; a_{1}\\,\\gamma_{x}[k-1] \\;+\\; a_{2}\\,\\gamma_{x}[k-2] \\;+\\; \\mathbb{E}\\{w[n]\\,x[n-k]\\}\n$$\nNow, we must evaluate the cross-correlation term $\\mathbb{E}\\{w[n]\\,x[n-k]\\}$. The AR($2$) process is causal, meaning the output $x[m]$ at any time $m$ depends only on the noise inputs $w[j]$ for $j \\leq m$. Therefore, $x[n-k]$ is a function of the noise samples $\\{w[n-k], w[n-k-1], \\dots\\}$. For $k \\geq 1$, the time indices of all these noise samples are strictly less than $n$. The driving noise $w[n]$ is a white noise process, which means its samples are uncorrelated over time: $\\mathbb{E}\\{w[n]\\,w[m]\\} = \\sigma_{w}^{2}\\,\\delta[n-m]$. This implies that $w[n]$ is uncorrelated with all past noise samples $w[m]$ where $m < n$. Since $x[n-k]$ is a linear combination of such past noise samples (for a stable system), $w[n]$ is also uncorrelated with $x[n-k]$ for $k \\geq 1$. As both $w[n]$ and $x[n]$ are zero-mean processes, their uncorrelation implies that the expectation of their product is zero:\n$$\n\\mathbb{E}\\{w[n]\\,x[n-k]\\} \\;=\\; 0 \\quad \\text{for } k \\geq 1\n$$\nThus, for $k \\geq 1$, the autocovariance sequence $\\gamma_{x}[k]$ obeys the homogeneous linear constant-coefficient difference equation:\n$$\n\\gamma_{x}[k] \\;-\\; a_{1}\\,\\gamma_{x}[k-1] \\;-\\; a_{2}\\,\\gamma_{x}[k-2] \\;=\\; 0\n$$\nThis is a portion of the set of relations known as the Yule-Walker equations.\n\nSecond, we characterize the conditions under which $\\gamma_{x}[k]$ is a damped sinusoid. The behavior of the solution to this homogeneous difference equation is determined by the roots of its characteristic polynomial, $p(z) = z^2 - a_1 z - a_2$. Let the roots be $p_1$ and $p_2$. The general solution for $\\gamma_{x}[k]$ (for $k \\geq 0$) is of the form $C_1 p_1^k + C_2 p_2^k$ (assuming distinct roots). For the solution to be a damped sinusoid, two conditions must be met:\n$1$. The sequence must decay, which requires the magnitude of the roots to be less than $1$, i.e., $|p_1| < 1$ and $|p_2| < 1$. This ensures $\\lim_{k \\to \\infty} \\gamma_x[k] = 0$.\n$2$. The sequence must oscillate, which requires the roots to be a complex-conjugate pair, as real roots would only produce decaying exponentials.\nTherefore, the roots must take the form $p_{1,2} = r\\,\\exp(\\pm \\mathrm{j}\\,\\theta)$, where the modulus $r$ satisfies $0 < r < 1$ for damping, and the angle $\\theta$ satisfies $\\theta \\in (0, \\pi)$ for the roots to be complex and not purely real. With such roots, the autocovariance sequence has the form $\\gamma_x[k] = r^k (A \\cos(k\\theta) + B \\sin(k\\theta))$, which is a damped sinusoid.\n\nThe characteristic polynomial of the autocovariance difference equation is identical to the denominator polynomial of the system's transfer function $H(z)$ written in terms of positive powers of $z$:\n$$\nH(z) = \\frac{X(z)}{W(z)} = \\frac{1}{1 - a_1 z^{-1} - a_2 z^{-2}} = \\frac{z^2}{z^2 - a_1 z - a_2}\n$$\nThe roots of $z^2 - a_1 z - a_2 = 0$ are the poles of the AR($2$) system. Thus, the condition for a damped sinusoidal autocovariance is that the poles of the AR($2$) system must be a complex-conjugate pair located strictly inside the unit circle of the complex $z$-plane. This is also the condition for the stability and WSS nature of the AR($2$) process.\n\nThird, we express the coefficient pair $(a_1, a_2)$ in terms of the polar parameterization of the poles, $p_{1,2} = r\\,\\exp(\\pm \\mathrm{j}\\,\\theta)$, with $r \\in (0,1)$ and $\\theta \\in (0,\\pi)$. From the characteristic polynomial $z^2 - a_1 z - a_2 = (z-p_1)(z-p_2)$, we use Vieta's formulas, which relate the polynomial coefficients to its roots:\n$$\na_1 = p_1 + p_2\n$$\n$$\n-a_2 = p_1 p_2\n$$\nSubstituting the polar forms of the poles $p_1 = r\\,\\exp(\\mathrm{j}\\,\\theta)$ and $p_2 = r\\,\\exp(-\\mathrm{j}\\,\\theta)$:\nFor $a_1$:\n$$\na_1 = r\\,\\exp(\\mathrm{j}\\,\\theta) + r\\,\\exp(-\\mathrm{j}\\,\\theta) = r(\\cos\\theta + \\mathrm{j}\\sin\\theta + \\cos\\theta - \\mathrm{j}\\sin\\theta) = 2r\\cos\\theta\n$$\nFor $a_2$:\n$$\n-a_2 = (r\\,\\exp(\\mathrm{j}\\,\\theta)) (r\\,\\exp(-\\mathrm{j}\\,\\theta)) = r^2 \\exp(\\mathrm{j}\\theta - \\mathrm{j}\\theta) = r^2 \\exp(0) = r^2\n$$\nTherefore, $a_2 = -r^2$.\nThe coefficient pair $(a_1, a_2)$ is given by $(2r\\cos\\theta, -r^2)$. The constraints $r \\in (0,1)$ and $\\theta \\in (0,\\pi)$ ensure that the poles form a complex-conjugate pair inside the unit circle, guaranteeing stability and a damped sinusoidal autocovariance function, as required. The coefficients $a_1$ and $a_2$ are real, as stipulated in the problem statement.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2r\\cos\\theta & -r^2 \\end{pmatrix}}\n$$", "id": "2885730"}, {"introduction": "In practical applications, model parameters estimated from data are not guaranteed to satisfy theoretical constraints like stability and invertibility. This exercise provides a hands-on procedure for diagnosing and correcting an estimated ARMA model whose parameters fall outside the valid region. You will learn how to check for stationarity and invertibility by finding the roots of the autoregressive and moving-average polynomials, and then apply a standard technique—reflecting violating roots across the unit circle—to enforce these crucial properties, a common step in real-world time series analysis.", "problem": "Consider a causal autoregressive moving-average model of orders three and two, denoted ARMA$(3,2)$, for a discrete-time random signal $x[n]$ driven by a zero-mean, finite-variance white-noise sequence $e[n]$:\n$$\nx[n] - \\sum_{k=1}^{3} \\phi_k\\, x[n-k] \\;=\\; e[n] + \\sum_{k=1}^{2} \\theta_k\\, e[n-k].\n$$\nAn engineer estimates the parameters as $\\phi_1 = 1.7$, $\\phi_2 = -0.06$, $\\phi_3 = -0.36$, and $\\theta_1 = -1.9$, $\\theta_2 = 0.84$.\n\nStarting only from fundamental definitions for stability of linear constant-coefficient difference equations and invertibility of moving-average systems, do the following:\n\n1) State the stationarity (stability) condition for the autoregressive part in terms of the roots $\\lambda$ of the characteristic equation associated with the homogeneous difference equation, and the invertibility (minimum-phase) condition for the moving-average part in terms of the zeros $z_0$ of the moving-average polynomial. Justify each condition from first principles.\n\n2) For the given parameter estimates, explicitly check whether the stationarity and invertibility conditions hold by computing the roots of the relevant polynomials.\n\n3) If any condition is violated, enforce the constraints by applying the following remedy: reflect each violating root across the unit circle by mapping any root with magnitude greater than or equal to $1$ to its reciprocal, and for complex roots, reflect conjugate pairs together to preserve real coefficients. Reconstruct the corresponding normalized polynomials and hence the corrected parameter vector $(\\phi_1, \\phi_2, \\phi_3, \\theta_1, \\theta_2)$.\n\nExpress the final corrected parameter vector as exact rational numbers in a single row matrix. No numerical rounding is required and no units are involved. Your final answer must be a single row matrix written in LaTeX.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Model: A causal autoregressive moving-average model of orders $3$ and $2$, ARMA($3,2$).\n- Model Equation: $x[n] - \\sum_{k=1}^{3} \\phi_k\\, x[n-k] \\;=\\; e[n] + \\sum_{k=1}^{2} \\theta_k\\, e[n-k]$.\n- Input: $e[n]$ is a zero-mean, finite-variance white-noise sequence.\n- AR Parameters: $\\phi_1 = 1.7$, $\\phi_2 = -0.06$, $\\phi_3 = -0.36$.\n- MA Parameters: $\\theta_1 = -1.9$, $\\theta_2 = 0.84$.\n- Task 1: State and justify stationarity and invertibility conditions from first principles.\n- Task 2: Check these conditions for the given parameters.\n- Task 3: If a condition is violated, reflect the violating roots/zeros (those with magnitude $\\ge 1$) to their reciprocals and reconstruct the normalized polynomials to find the corrected parameter vector.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem addresses standard concepts of stability (stationarity) and invertibility (minimum-phase) for ARMA models, which are fundamental topics in signal processing and time series analysis. The model and analysis techniques are scientifically sound.\n- **Well-Posed**: The problem is clearly specified. It provides a model, parameters, and a precise procedure for analysis and correction. A unique solution is expected.\n- **Objective**: The problem is stated using precise, objective, and standard terminology from the field of signal processing. There is no subjective or ambiguous language.\n- **Completeness**: All necessary parameters and model definitions are provided to perform the required tasks.\n- **Consistency and Feasibility**: The problem is internally consistent and the required calculations (finding roots of low-order polynomials) are feasible.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe ARMA($3,2$) model is described by the linear constant-coefficient difference equation:\n$$\nx[n] - \\phi_1 x[n-1] - \\phi_2 x[n-2] - \\phi_3 x[n-3] = e[n] + \\theta_1 e[n-1] + \\theta_2 e[n-2]\n$$\nTaking the Z-transform, we find the system transfer function $H(z) = X(z)/E(z)$:\n$$\nH(z) = \\frac{1 + \\theta_1 z^{-1} + \\theta_2 z^{-2}}{1 - \\phi_1 z^{-1} - \\phi_2 z^{-2} - \\phi_3 z^{-3}} = \\frac{B(z)}{A(z)}\n$$\n\nPart 1: Stationarity and Invertibility Conditions\n\nThe **stationarity** of the signal $x[n]$ is determined by the stability of the autoregressive (AR) part of the model. A linear time-invariant (LTI) system is stable if and only if its impulse response is absolutely summable. The AR part is an all-pole system with transfer function $H_{AR}(z) = 1/A(z)$. Its behavior is governed by the homogeneous difference equation:\n$$\ny[n] - \\phi_1 y[n-1] - \\phi_2 y[n-2] - \\phi_3 y[n-3] = 0\n$$\nThe general solution to this equation is a linear combination of modes of the form $c_i \\lambda_i^n$, where the $\\lambda_i$ are the roots of the characteristic polynomial associated with $A(z)$. The roots of $A(z) = 1 - \\sum_{k=1}^3 \\phi_k z^{-k} = 0$ are the poles of the system $H(z)$. Multiplying by $z^3$ gives the polynomial $P(z) = z^3 - \\phi_1 z^2 - \\phi_2 z - \\phi_3 = 0$, whose roots $\\lambda_i$ are identical to the poles of $H(z)$. The impulse response of the AR part consists of terms proportional to $\\lambda_i^n$. For the process to be wide-sense stationary, the system must be stable, which for a causal system requires the impulse response to decay to zero. This occurs if and only if all roots $\\lambda_i$ of the characteristic polynomial have a magnitude less than $1$.\nCondition for Stationarity: All roots $\\lambda$ of the polynomial $P(z) = z^3 - \\phi_1 z^2 - \\phi_2 z - \\phi_3$ must satisfy $|\\lambda| < 1$.\n\nThe **invertibility** of the model refers to the ability to uniquely determine the input noise sequence $e[n]$ from the output sequence $x[n]$ using a stable and causal filter. The relationship is $E(z) = H(z)^{-1} X(z) = \\frac{A(z)}{B(z)} X(z)$. The inverse system has poles at the zeros of the original system. The zeros of $H(z)$ are the roots of the numerator polynomial $B(z) = 1 + \\theta_1 z^{-1} + \\theta_2 z^{-2} = 0$. Multiplying by $z^2$ gives $Q(z) = z^2 + \\theta_1 z + \\theta_2 = 0$. Let the roots of $Q(z)$ be $z_{0,k}$. For the inverse system to be stable and causal, its poles must lie inside the unit circle. Therefore, the zeros $z_{0,k}$ of the moving-average (MA) part must lie inside the unit circle. Such a system is also called minimum-phase.\nCondition for Invertibility: All roots $z_0$ of the polynomial $Q(z) = z^2 + \\theta_1 z + \\theta_2$ must satisfy $|z_0| < 1$.\n\nPart 2: Checking the Conditions\n\nFor the AR part (stationarity), we examine the roots of $P(z) = z^3 - \\phi_1 z^2 - \\phi_2 z - \\phi_3 = 0$ with the given parameters $\\phi_1 = 1.7$, $\\phi_2 = -0.06$, $\\phi_3 = -0.36$:\n$$\nP(z) = z^3 - 1.7 z^2 + 0.06 z + 0.36 = 0\n$$\nBy inspection, one can test for rational roots. For $z=1.5$:\n$P(1.5) = (1.5)^3 - 1.7(1.5)^2 + 0.06(1.5) + 0.36 = 3.375 - 1.7(2.25) + 0.09 + 0.36 = 3.375 - 3.825 + 0.45 = 0$.\nSo, $\\lambda_1 = 1.5$ is a root. We perform polynomial division of $P(z)$ by $(z-1.5)$ to find the remaining roots:\n$$\n\\frac{z^3 - 1.7 z^2 + 0.06 z + 0.36}{z - 1.5} = z^2 - 0.2 z - 0.24\n$$\nThe roots of $z^2 - 0.2 z - 0.24 = 0$ are given by the quadratic formula:\n$$\n\\lambda = \\frac{0.2 \\pm \\sqrt{(-0.2)^2 - 4(1)(-0.24)}}{2} = \\frac{0.2 \\pm \\sqrt{0.04 + 0.96}}{2} = \\frac{0.2 \\pm 1}{2}\n$$\nThis gives $\\lambda_2 = 1.2/2 = 0.6$ and $\\lambda_3 = -0.8/2 = -0.4$. The roots of the AR characteristic polynomial are $\\lambda_1 = 1.5$, $\\lambda_2 = 0.6$, and $\\lambda_3 = -0.4$. Since $|\\lambda_1| = 1.5 > 1$, the stationarity condition is violated.\n\nFor the MA part (invertibility), we examine the roots of $Q(z) = z^2 + \\theta_1 z + \\theta_2 = 0$ with $\\theta_1 = -1.9$, $\\theta_2 = 0.84$:\n$$\nQ(z) = z^2 - 1.9 z + 0.84 = 0\n$$\nUsing the quadratic formula:\n$$\nz_0 = \\frac{1.9 \\pm \\sqrt{(-1.9)^2 - 4(1)(0.84)}}{2} = \\frac{1.9 \\pm \\sqrt{3.61 - 3.36}}{2} = \\frac{1.9 \\pm \\sqrt{0.25}}{2} = \\frac{1.9 \\pm 0.5}{2}\n$$\nThis gives the zeros $z_{0,1} = 2.4/2 = 1.2$ and $z_{0,2} = 1.4/2 = 0.7$. Since $|z_{0,1}| = 1.2 > 1$, the invertibility condition is violated.\n\nPart 3: Correcting the Parameters\n\nWe must reflect the violating roots/zeros across the unit circle.\nFor the AR part, the violating root is $\\lambda_1 = 1.5$. The reflected root is $\\lambda'_1 = 1/1.5 = 2/3$. The other roots, $\\lambda_2 = 0.6 = 3/5$ and $\\lambda_3 = -0.4 = -2/5$, are already inside the unit circle. The new set of stable roots is $\\{\\lambda'_1, \\lambda_2, \\lambda_3\\} = \\{2/3, 3/5, -2/5\\}$.\nWe reconstruct the normalized (monic) polynomial $P'(z) = z^3 - \\phi'_1 z^2 - \\phi'_2 z - \\phi'_3$ from these roots. Using Vieta's formulas:\nSum of roots: $\\phi'_1 = \\lambda'_1 + \\lambda_2 + \\lambda_3 = \\frac{2}{3} + \\frac{3}{5} - \\frac{2}{5} = \\frac{2}{3} + \\frac{1}{5} = \\frac{10+3}{15} = \\frac{13}{15}$.\nSum of products of roots taken two at a time: $-\\phi'_2 = (\\frac{2}{3})(\\frac{3}{5}) + (\\frac{2}{3})(-\\frac{2}{5}) + (\\frac{3}{5})(-\\frac{2}{5}) = \\frac{6}{15} - \\frac{4}{15} - \\frac{6}{25} = \\frac{2}{15} - \\frac{6}{25} = \\frac{10-18}{75} = -\\frac{8}{75}$. Thus, $\\phi'_2 = \\frac{8}{75}$.\nProduct of roots: $\\phi'_3 = (\\frac{2}{3})(\\frac{3}{5})(-\\frac{2}{5}) = -\\frac{12}{75} = -\\frac{4}{25}$.\nThe corrected AR parameters are $\\phi'_1 = 13/15$, $\\phi'_2 = 8/75$, $\\phi'_3 = -4/25$.\n\nFor the MA part, the violating zero is $z_{0,1} = 1.2$. The reflected zero is $z'_{0,1} = 1/1.2 = 1/(6/5) = 5/6$. The other zero, $z_{0,2} = 0.7 = 7/10$, is already inside the unit circle. The new set of minimum-phase zeros is $\\{z'_{0,1}, z_{0,2}\\} = \\{5/6, 7/10\\}$.\nWe reconstruct the normalized (monic) polynomial $Q'(z) = z^2 + \\theta'_1 z + \\theta'_2$ from these zeros.\nUsing Vieta's formulas for $Q'(z)=0$:\nSum of roots: $-\\theta'_1 = \\frac{5}{6} + \\frac{7}{10} = \\frac{25+21}{30} = \\frac{46}{30} = \\frac{23}{15}$. Thus, $\\theta'_1 = -\\frac{23}{15}$.\nProduct of roots: $\\theta'_2 = (\\frac{5}{6})(\\frac{7}{10}) = \\frac{35}{60} = \\frac{7}{12}$.\nThe corrected MA parameters are $\\theta'_1 = -23/15$, $\\theta'_2 = 7/12$.\n\nThe final corrected parameter vector $(\\phi'_1, \\phi'_2, \\phi'_3, \\theta'_1, \\theta'_2)$ is $(\\frac{13}{15}, \\frac{8}{75}, -\\frac{4}{25}, -\\frac{23}{15}, \\frac{7}{12})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{13}{15} & \\frac{8}{75} & -\\frac{4}{25} & -\\frac{23}{15} & \\frac{7}{12} \\end{pmatrix}}\n$$", "id": "2885736"}]}