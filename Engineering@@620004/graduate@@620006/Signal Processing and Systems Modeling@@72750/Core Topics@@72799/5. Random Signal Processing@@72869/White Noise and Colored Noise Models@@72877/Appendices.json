{"hands_on_practices": [{"introduction": "Our first practice exercise explores one of the most fundamental methods for generating colored noise: filtering white noise with a simple first-order autoregressive (AR) filter. This problem challenges you to derive the output autocorrelation function from first principles, revealing a direct and elegant link between the filter's pole and the exponential decay of correlation in the output signal. This is a cornerstone concept for modeling processes with persistent memory, such as physical systems with inertia or economic time series [@problem_id:2916615].", "problem": "Consider a discrete-time, causal, linear time-invariant (LTI) filter with transfer function $H(z)=\\frac{1}{1-a z^{-1}}$, where $|a|<1$. The input $w[n]$ is zero-mean white noise that is wide-sense stationary (WSS), with autocorrelation $R_{w}[k]=\\sigma_{w}^{2}\\,\\delta[k]$, where $\\sigma_{w}^{2}>0$ and $\\delta[k]$ is the Kronecker delta. Let the output be $y[n]$. Assume the unique WSS solution exists.\n\nStarting only from the core definitions of autocorrelation for WSS processes, the whiteness property of $w[n]$, and linearity and causality of the system, derive an explicit closed-form expression for the output autocorrelation $R_{y}[k]=\\mathbb{E}\\{y[n]\\,y[n-k]\\}$ for all integer lags $k$. Then, using your expression, verify that $|R_{y}[k]|$ decays exponentially as $|k|\\to\\infty$ and identify the base of this exponential. Express your final result for $R_{y}[k]$ in terms of $a$, $\\sigma_{w}^{2}$, and $k$. No numerical rounding is required.", "solution": "The task is to find the autocorrelation function $R_{y}[k]$ of the output process $y[n]$ of a discrete-time LTI system. The system is defined by its transfer function $H(z) = \\frac{1}{1 - a z^{-1}}$, with the stability condition $|a| < 1$. The input $w[n]$ is a zero-mean wide-sense stationary (WSS) white noise process with autocorrelation $R_{w}[k] = \\sigma_{w}^{2} \\delta[k]$.\n\nWe begin from first principles. The output $y[n]$ is given by the convolution of the input $w[n]$ with the system's impulse response $h[n]$:\n$$y[n] = \\sum_{m=-\\infty}^{\\infty} h[m] w[n-m]$$\nFirst, we must determine the impulse response $h[n]$ from the transfer function $H(z)$. Given that the system is causal and $|a|<1$, the transfer function $H(z) = \\frac{1}{1 - a z^{-1}}$ corresponds to the one-sided Z-transform of the sequence $h[n] = a^{n} u[n]$, where $u[n]$ is the Heaviside unit step function. The region of convergence is $|z| > |a|$, which includes the unit circle, confirming stability.\n\nSince $h[m] = 0$ for $m < 0$, the convolution sum becomes:\n$$y[n] = \\sum_{m=0}^{\\infty} h[m] w[n-m] = \\sum_{m=0}^{\\infty} a^{m} w[n-m]$$\nThe output autocorrelation $R_{y}[k]$ is defined for a WSS process as $R_{y}[k] = \\mathbb{E}\\{y[n] y[n-k]\\}$. Substituting the expression for $y[n]$ and $y[n-k]$:\n$$y[n-k] = \\sum_{l=0}^{\\infty} a^{l} w[n-k-l]$$\n$$R_{y}[k] = \\mathbb{E}\\left\\{ \\left( \\sum_{m=0}^{\\infty} a^{m} w[n-m] \\right) \\left( \\sum_{l=0}^{\\infty} a^{l} w[n-k-l] \\right) \\right\\}$$\nBy linearity of the expectation operator, we can interchange the expectation and the summations:\n$$R_{y}[k] = \\sum_{m=0}^{\\infty} \\sum_{l=0}^{\\infty} a^{m} a^{l} \\mathbb{E}\\{w[n-m] w[n-k-l]\\}$$\nThe expectation term is the autocorrelation of the input process $w[n]$. Since $w[n]$ is WSS, its autocorrelation depends only on the time difference:\n$$\\mathbb{E}\\{w[n-m] w[n-k-l]\\} = R_{w}[(n-m) - (n-k-l)] = R_{w}[k+l-m]$$\nWe are given that the input is white noise, so $R_{w}[\\tau] = \\sigma_{w}^{2} \\delta[\\tau]$, where $\\delta[\\tau]$ is the Kronecker delta function. Therefore:\n$$\\mathbb{E}\\{w[n-m] w[n-k-l]\\} = \\sigma_{w}^{2} \\delta[k+l-m]$$\nSubstituting this back into the expression for $R_{y}[k]$:\n$$R_{y}[k] = \\sum_{m=0}^{\\infty} \\sum_{l=0}^{\\infty} a^{m+l} \\sigma_{w}^{2} \\delta[m - (k+l)]$$\nWe can evaluate the inner sum over $m$ using the sifting property of the Kronecker delta. The term $\\delta[m - (k+l)]$ is non-zero only when $m = k+l$. This substitution collapses the sum over $m$:\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=0}^{\\infty} a^{(k+l)+l} \\quad\\text{subject to } m = k+l \\ge 0$$\nThe condition $k+l \\ge 0$ implies $l \\ge -k$. The summation index $l$ must satisfy both $l \\ge 0$ and $l \\ge -k$. This leads to two cases depending on the sign of $k$.\n\nCase 1: $k \\ge 0$.\nIn this case, $-k \\le 0$. The condition $l \\ge -k$ is automatically satisfied for all $l \\ge 0$. Thus, the summation is from $l=0$ to $\\infty$.\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=0}^{\\infty} a^{k+2l} = \\sigma_{w}^{2} a^{k} \\sum_{l=0}^{\\infty} (a^{2})^{l}$$\nThis is a geometric series with ratio $a^{2}$. Since $|a|<1$, we have $|a^{2}|<1$, and the series converges to $\\frac{1}{1-a^{2}}$.\nFor $k \\ge 0$:\n$$R_{y}[k] = \\sigma_{w}^{2} a^{k} \\frac{1}{1-a^{2}}$$\n\nCase 2: $k < 0$.\nIn this case, $-k > 0$. The condition $l \\ge -k$ is the more restrictive one. Thus, the summation starts from $l=-k$.\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=-k}^{\\infty} a^{k+2l}$$\nLet us perform a change of index. Let $j = l+k$. When $l = -k$, $j=0$. The sum becomes:\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{j=0}^{\\infty} a^{k+2(j-k)} = \\sigma_{w}^{2} \\sum_{j=0}^{\\infty} a^{2j-k} = \\sigma_{w}^{2} a^{-k} \\sum_{j=0}^{\\infty} (a^{2})^{j}$$\nAgain, the geometric series sums to $\\frac{1}{1-a^{2}}$.\nFor $k < 0$:\n$$R_{y}[k] = \\sigma_{w}^{2} a^{-k} \\frac{1}{1-a^{2}}$$\n\nWe can combine these two cases into a single closed-form expression using the absolute value function. For $k \\ge 0$, $|k|=k$. For $k < 0$, $|k|=-k$. Both results are captured by:\n$$R_{y}[k] = \\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|}$$\nThis is the explicit closed-form expression for the output autocorrelation for all integer lags $k$.\n\nFinally, we must verify that $|R_{y}[k]|$ decays exponentially as $|k|\\to\\infty$ and identify the base of this exponential. Taking the absolute value of our result:\n$$|R_{y}[k]| = \\left| \\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|} \\right|$$\nSince $\\sigma_{w}^{2} > 0$ and $|a|<1$ (which implies $1-a^{2} > 0$), the term $\\frac{\\sigma_{w}^{2}}{1-a^{2}}$ is a positive constant.\n$$|R_{y}[k]| = \\frac{\\sigma_{w}^{2}}{1-a^{2}} |a^{|k|}| = \\frac{\\sigma_{w}^{2}}{1-a^{2}} |a|^{|k|}$$\nThis expression has the form $C \\cdot b^{|k|}$, where $C = \\frac{\\sigma_{w}^{2}}{1-a^{2}}$ is the constant amplitude and $b = |a|$ is the base. As it is given that $|a|<1$, the base $b$ is strictly between $0$ and $1$ (assuming $a \\neq 0$). Consequently, as $|k| \\to \\infty$, the term $|a|^{|k|}$ decays to zero exponentially.\nThe base of this exponential decay is indeed $|a|$. This completes the verification. The problem is solved.", "answer": "$$\n\\boxed{\\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|}}\n$$", "id": "2916615"}, {"introduction": "Next, we shift our focus from infinite to finite memory systems by examining a simple first-difference filter. This exercise demonstrates how even a basic finite impulse response (FIR) filter can induce correlation, transforming an unpredictable white noise sequence into a more structured 'colored' noise process [@problem_id:2916640]. By deriving both the autocorrelation and the power spectral density, you will gain hands-on insight into how filtering shapes the noise's characteristics in both the time and frequency domains.", "problem": "Consider a discrete-time stochastic process defined by $w[n]$, where $\\{w[n]\\}$ are independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and variance $\\sigma^{2}$. Define a new process by differencing as $v[n] = w[n] - w[n-1]$.\n\nUsing only core definitions from stochastic process theory and linear time-invariant systems, do the following:\n\n1. Determine the marginal distribution of $v[n]$ at any fixed time index $n$.\n2. Determine whether $v[n]$ is wide-sense stationary (WSS). If it is WSS, derive its autocorrelation function $r_{v}[k] = \\mathbb{E}\\{v[n]\\,v[n+k]\\}$ for all integer lags $k$.\n3. From first principles, derive the power spectral density (PSD) $S_{v}(e^{j\\omega})$ of $v[n]$ using its definition as the discrete-time Fourier transform of the autocorrelation function.\n4. Determine whether $v[n]$ is a white process under the standard definition of a white process in discrete time.\n\nProvide clear reasoning grounded in the definitions of independence, Gaussianity, wide-sense stationarity, autocorrelation, and power spectral density. Do not invoke any result without justification from these definitions.\n\nProvide, as your final reported answer, the closed-form analytic expression for the power spectral density $S_{v}(e^{j\\omega})$. No numerical approximation is required and no units are involved. The final answer must be a single closed-form expression.", "solution": "The process $w[n]$ is a discrete-time Gaussian white noise process, as it is i.i.d. with zero mean. Its autocorrelation function $r_{w}[k]$ is given by definition as $r_{w}[k] = \\mathbb{E}\\{w[n] w[n+k]\\}$. Due to the i.i.d. property and zero mean, $w[n]$ and $w[n+k]$ are independent for $k \\neq 0$, making $\\mathbb{E}\\{w[n] w[n+k]\\} = \\mathbb{E}\\{w[n]\\}\\mathbb{E}\\{w[n+k]\\} = 0 \\cdot 0 = 0$. For $k=0$, $\\mathbb{E}\\{w[n]^2\\}$ is the variance, $\\sigma^2$. Thus, the autocorrelation of $w[n]$ is $r_{w}[k] = \\sigma^2 \\delta[k]$, where $\\delta[k]$ is the Kronecker delta function.\n\n**1. Marginal Distribution of $v[n]$**\nThe process $v[n]$ is defined as $v[n] = w[n] - w[n-1]$. This is a linear combination of two Gaussian random variables, $w[n]$ and $w[n-1]$. A fundamental property of Gaussian distributions is that any linear combination of Gaussian random variables results in another Gaussian random variable. Therefore, $v[n]$ is a Gaussian random variable for any fixed $n$.\nTo specify its distribution, we must find its mean and variance.\n\nThe mean of $v[n]$ is:\n$$\n\\mathbb{E}\\{v[n]\\} = \\mathbb{E}\\{w[n] - w[n-1]\\}\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}\\{v[n]\\} = \\mathbb{E}\\{w[n]\\} - \\mathbb{E}\\{w[n-1]\\} = 0 - 0 = 0\n$$\nThe mean of $v[n]$ is zero.\n\nThe variance of $v[n]$ is $\\text{Var}(v[n]) = \\mathbb{E}\\{(v[n] - \\mathbb{E}\\{v[n]\\})^2\\} = \\mathbb{E}\\{v[n]^2\\}$, as the mean is zero.\n$$\n\\text{Var}(v[n]) = \\text{Var}(w[n] - w[n-1])\n$$\nBecause $w[n]$ and $w[n-1]$ are independent (since the process $w[n]$ is i.i.d.), the variance of their difference is the sum of their variances:\n$$\n\\text{Var}(v[n]) = \\text{Var}(w[n]) + \\text{Var}(w[n-1]) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nThus, for any time index $n$, $v[n]$ follows a Gaussian distribution with mean $0$ and variance $2\\sigma^2$. This is denoted as $v[n] \\sim \\mathcal{N}(0, 2\\sigma^2)$.\n\n**2. Wide-Sense Stationarity and Autocorrelation of $v[n]$**\nA process is wide-sense stationary (WSS) if its mean is constant and its autocorrelation function depends only on the time lag $k$.\nThe mean of $v[n]$ is $\\mathbb{E}\\{v[n]\\} = 0$, which is a constant for all $n$. The first condition for WSS is satisfied.\n\nNext, we derive the autocorrelation function $r_{v}[k] = \\mathbb{E}\\{v[n] v[n+k]\\}$.\n$$\nr_{v}[k] = \\mathbb{E}\\{(w[n] - w[n-1])(w[n+k] - w[n+k-1])\\}\n$$\nExpanding the product:\n$$\nr_{v}[k] = \\mathbb{E}\\{w[n]w[n+k] - w[n]w[n+k-1] - w[n-1]w[n+k] + w[n-1]w[n+k-1]\\}\n$$\nBy linearity of expectation:\n$$\nr_{v}[k] = \\mathbb{E}\\{w[n]w[n+k]\\} - \\mathbb{E}\\{w[n]w[n+k-1]\\} - \\mathbb{E}\\{w[n-1]w[n+k]\\} + \\mathbb{E}\\{w[n-1]w[n+k-1]\\}\n$$\nEach term is an autocorrelation of the process $w[n]$ at a specific lag. Using the notation $r_{w}[m] = \\mathbb{E}\\{w[j]w[j+m]\\}$:\n$$\nr_{v}[k] = r_{w}[k] - r_{w}[k-1] - r_{w}[k+1] + r_{w}[k] = 2r_{w}[k] - r_{w}[k-1] - r_{w}[k+1]\n$$\nNow, substitute $r_{w}[m] = \\sigma^2 \\delta[m]$:\n$$\nr_{v}[k] = 2\\sigma^2\\delta[k] - \\sigma^2\\delta[k-1] - \\sigma^2\\delta[k+1]\n$$\nWe evaluate this for different integer values of $k$:\n- For $k=0$: $r_{v}[0] = 2\\sigma^2\\delta[0] - \\sigma^2\\delta[-1] - \\sigma^2\\delta[1] = 2\\sigma^2(1) - \\sigma^2(0) - \\sigma^2(0) = 2\\sigma^2$. This correctly matches the variance of $v[n]$ computed earlier.\n- For $k=1$: $r_{v}[1] = 2\\sigma^2\\delta[1] - \\sigma^2\\delta[0] - \\sigma^2\\delta[2] = 2\\sigma^2(0) - \\sigma^2(1) - \\sigma^2(0) = -\\sigma^2$.\n- For $k=-1$: $r_{v}[-1] = 2\\sigma^2\\delta[-1] - \\sigma^2\\delta[-2] - \\sigma^2\\delta[0] = 2\\sigma^2(0) - \\sigma^2(0) - \\sigma^2(1) = -\\sigma^2$.\n- For $|k| \\geq 2$: All delta functions $\\delta[k]$, $\\delta[k-1]$, and $\\delta[k+1]$ are zero. Thus, $r_{v}[k] = 0$.\n\nThe autocorrelation function is:\n$$\nr_{v}[k] = \\begin{cases} 2\\sigma^2 & \\text{if } k=0 \\\\ -\\sigma^2 & \\text{if } |k|=1 \\\\ 0 & \\text{if } |k|\\geq 2 \\end{cases}\n$$\nSince $r_{v}[k]$ depends only on the lag $k$ and not on the time index $n$, the second condition for WSS is satisfied. Therefore, the process $v[n]$ is wide-sense stationary.\n\n**3. Power Spectral Density of $v[n]$**\nThe power spectral density (PSD) $S_{v}(e^{j\\omega})$ is defined as the discrete-time Fourier transform (DTFT) of the autocorrelation function $r_{v}[k]$.\n$$\nS_{v}(e^{j\\omega}) = \\sum_{k=-\\infty}^{\\infty} r_{v}[k] e^{-j\\omega k}\n$$\nSince $r_{v}[k]$ is non-zero only for $k \\in \\{-1, 0, 1\\}$, the infinite sum reduces to three terms:\n$$\nS_{v}(e^{j\\omega}) = r_{v}[-1]e^{-j\\omega(-1)} + r_{v}[0]e^{-j\\omega(0)} + r_{v}[1]e^{-j\\omega(1)}\n$$\nSubstituting the values of $r_{v}[k]$:\n$$\nS_{v}(e^{j\\omega}) = (-\\sigma^2)e^{j\\omega} + (2\\sigma^2)(1) + (-\\sigma^2)e^{-j\\omega}\n$$\nFactor out $2\\sigma^2$:\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 - \\sigma^2(e^{j\\omega} + e^{-j\\omega})\n$$\nUsing Euler's formula, $\\cos(\\omega) = \\frac{e^{j\\omega} + e^{-j\\omega}}{2}$, which implies $e^{j\\omega} + e^{-j\\omega} = 2\\cos(\\omega)$.\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 - \\sigma^2(2\\cos(\\omega)) = 2\\sigma^2(1 - \\cos(\\omega))\n$$\nThis expression can be further simplified using the half-angle trigonometric identity $1 - \\cos(\\omega) = 2\\sin^2(\\frac{\\omega}{2})$.\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 \\left( 2\\sin^2\\left(\\frac{\\omega}{2}\\right) \\right) = 4\\sigma^2\\sin^2\\left(\\frac{\\omega}{2}\\right)\n$$\nThis is the closed-form expression for the power spectral density of $v[n]$.\n\n**4. White Process Determination**\nA discrete-time stochastic process is defined as white if its samples are uncorrelated. This is equivalent to its autocorrelation function being non-zero only at lag $k=0$. That is, $r_v[k]$ must be of the form $C\\delta[k]$ for some constant $C$.\n\nFrom our derivation in part 2, the autocorrelation function of $v[n]$ is $r_{v}[k] = 2\\sigma^2\\delta[k] - \\sigma^2\\delta[k-1] - \\sigma^2\\delta[k+1]$.\nThis function has non-zero values at lags $k=1$ and $k=-1$, specifically $r_{v}[1] = r_{v}[-1] = -\\sigma^2$. Because the autocorrelation is non-zero for non-zero lags, the samples of $v[n]$ are not uncorrelated. For example, $v[n]$ and $v[n+1]$ are correlated.\n\nAlternatively, a process is white if its power spectral density is a constant for all frequencies, i.e., $S_v(e^{j\\omega}) = \\text{constant}$. From part 3, we found $S_v(e^{j\\omega}) = 4\\sigma^2\\sin^2(\\frac{\\omega}{2})$. This function is dependent on the frequency $\\omega$ and is not constant. It varies from $0$ at $\\omega=0$ to $4\\sigma^2$ at $\\omega=\\pi$.\n\nTherefore, the process $v[n]$ is not a white process. It is a colored noise process.", "answer": "$$\n\\boxed{4\\sigma^{2}\\sin^{2}\\left(\\frac{\\omega}{2}\\right)}\n$$", "id": "2916640"}, {"introduction": "This final practice problem investigates the properties of a discrete-time random walk, a process generated by accumulating white noise samples. You will demonstrate from basic principles why this process is non-stationary, a crucial distinction in time-series modeling where properties like variance change over time [@problem_id:2916613]. Furthermore, by analyzing its first-order difference, you will discover a profound connection back to white noise, illustrating a fundamental technique for stabilizing non-stationary data.", "problem": "A discrete-time process is defined by the recursion $x[n] = x[n-1] + w[n]$ for all integers $n \\geq 1$, with the initial condition $x[0] = 0$. The input $w[n]$ is a discrete-time white noise process with zero mean and variance $\\sigma^{2}$, that is, $\\mathbb{E}\\{w[n]\\} = 0$ and $\\mathbb{E}\\{w[n] w[m]\\} = \\sigma^{2} \\delta[n-m]$, where $\\delta[\\cdot]$ denotes the Kronecker delta. Throughout, assume all second-order moments exist and that $\\{w[n]\\}$ is independent across time.\n\nUsing only these definitions and the linearity of expectation, carry out the following tasks:\n\n1. Derive the second-order moment $\\mathbb{E}\\{x[n] x[m]\\}$ explicitly in terms of $n$ and $m$, and use it to assess whether there exists an autocorrelation function $R_{x}[k]$ that depends only on the lag $k = n-m$. Explain the stationarity or nonstationarity of $\\{x[n]\\}$ at the level of wide-sense stationarity (Wide-Sense Stationary (WSS)) based on your derivation.\n\n2. Define the first-order difference process $v[n] = x[n] - x[n-1]$. Determine whether $\\{v[n]\\}$ is a white noise process in the second-order sense, and compute its autocorrelation function $R_{v}[k] = \\mathbb{E}\\{v[n] v[n-k]\\}$ in closed form.\n\nExpress your final answer as the closed-form expression for $R_{v}[k]$. No units are involved. The final answer must be a single expression and does not require rounding.", "solution": "The analysis is divided into two parts, corresponding to the two tasks.\n\nFirst, we analyze the properties of the process $\\{x[n]\\}$. The recursive definition $x[n] = x[n-1] + w[n]$ with the initial condition $x[0]=0$ can be unrolled to express $x[n]$ as a direct sum of the noise samples:\n$$x[n] = \\sum_{i=1}^{n} w[i]$$\nThis expression holds for $n \\geq 1$. For $n=0$, we have $x[0]=0$.\n\nWe compute the mean of $\\{x[n]\\}$. By the linearity of the expectation operator:\n$$\\mathbb{E}\\{x[n]\\} = \\mathbb{E}\\left\\{\\sum_{i=1}^{n} w[i]\\right\\} = \\sum_{i=1}^{n} \\mathbb{E}\\{w[i]\\}$$\nGiven that $\\mathbb{E}\\{w[i]\\} = 0$ for all $i$, the mean is:\n$$\\mathbb{E}\\{x[n]\\} = \\sum_{i=1}^{n} 0 = 0$$\nThe mean of the process $\\{x[n]\\}$ is constant (zero) for all $n$. This satisfies the first condition for wide-sense stationarity.\n\nNext, we derive the second-order moment, $\\mathbb{E}\\{x[n] x[m]\\}$. Since the process is zero-mean, this is equivalent to its autocorrelation function.\n$$\\mathbb{E}\\{x[n] x[m]\\} = \\mathbb{E}\\left\\{ \\left(\\sum_{i=1}^{n} w[i]\\right) \\left(\\sum_{j=1}^{m} w[j]\\right) \\right\\} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\mathbb{E}\\{w[i] w[j]\\}$$\nWe use the given property of the white noise process, $\\mathbb{E}\\{w[i] w[j]\\} = \\sigma^{2} \\delta[i-j]$. The Kronecker delta $\\delta[i-j]$ is $1$ only when $i=j$ and $0$ otherwise. The double summation thus collapses to a single sum over the non-zero terms where $i=j$:\n$$\\mathbb{E}\\{x[n] x[m]\\} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\sigma^{2} \\delta[i-j] = \\sum_{i=1}^{\\min(n,m)} \\sigma^{2}$$\nThis simplifies to:\n$$\\mathbb{E}\\{x[n] x[m]\\} = \\sigma^{2} \\min(n, m)$$\nThis is the explicit expression for the second-order moment.\n\nTo assess for wide-sense stationarity, we examine if this autocorrelation function, $R_{x}(n, m) = \\sigma^{2} \\min(n, m)$, depends only on the lag $k = n-m$. It clearly does not. For instance, let $n=3$ and $m=2$, so the lag is $k=1$. In this case, $R_{x}(3, 2) = \\sigma^{2} \\min(3, 2) = 2\\sigma^{2}$. Now consider $n=4$ and $m=3$, for which the lag is also $k=1$. Here, $R_{x}(4, 3) = \\sigma^{2} \\min(4, 3) = 3\\sigma^{2}$. Since $R_{x}(3, 2) \\neq R_{x}(4, 3)$, the autocorrelation depends on the specific time indices $n$ and $m$, not just their difference. Therefore, the process $\\{x[n]\\}$ is not wide-sense stationary. This is further confirmed by observing that the variance, $\\mathbb{E}\\{x[n]^2\\} = \\sigma^{2} \\min(n, n) = n\\sigma^{2}$, is time-dependent.\n\nSecond, we analyze the first-order difference process, $v[n] = x[n] - x[n-1]$. From the original definition of $x[n]$, we have $x[n] = x[n-1] + w[n]$. Rearranging this equation gives:\n$$v[n] = x[n] - x[n-1] = w[n]$$\nThus, the process $\\{v[n]\\}$ is identical to the input white noise process $\\{w[n]\\}$.\n\nTo confirm $\\{v[n]\\}$ is a white noise process, we check its properties. The mean is:\n$$\\mathbb{E}\\{v[n]\\} = \\mathbb{E}\\{w[n]\\} = 0$$\nThe mean is constant. The autocorrelation function is $R_{v}[k] = \\mathbb{E}\\{v[n] v[n-k]\\}$. Substituting $v[n]=w[n]$:\n$$R_{v}[k] = \\mathbb{E}\\{w[n] w[n-k]\\}$$\nUsing the given autocorrelation of $\\{w[n]\\}$, $\\mathbb{E}\\{w[n] w[m]\\} = \\sigma^{2} \\delta[n-m]$, and setting the index $m = n-k$, we find:\n$$R_{v}[k] = \\sigma^{2} \\delta[k]$$\nThe process $\\{v[n]\\}$ has a constant zero mean and an autocorrelation function that is an impulse at zero lag. This is, by definition, a discrete-time white noise process. The requested closed-form expression for its autocorrelation function is $R_{v}[k] = \\sigma^{2} \\delta[k]$.", "answer": "$$\\boxed{\\sigma^{2} \\delta[k]}$$", "id": "2916613"}]}