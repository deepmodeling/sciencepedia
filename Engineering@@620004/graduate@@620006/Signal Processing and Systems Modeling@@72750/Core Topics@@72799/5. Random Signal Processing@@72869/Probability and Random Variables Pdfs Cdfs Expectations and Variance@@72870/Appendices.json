{"hands_on_practices": [{"introduction": "Mastering the mechanics of joint probability density functions (PDFs) is the first step toward modeling complex systems with multiple random components. This exercise provides foundational practice in working with a two-dimensional random vector, a common scenario in signal processing for representing quantities like in-phase and quadrature components. By first ensuring the PDF is properly normalized and then calculating the probability that the vector falls within a specific sub-region, you will solidify your understanding of how PDFs map to probabilities through integration and the practical use of coordinate transformations.[@problem_id:2893239]", "problem": "Consider a complex baseband coefficient modeled by a two-dimensional real random vector $(X,Y)$ representing in-phase and quadrature components in a narrowband signal processing system. Suppose $(X,Y)$ has joint probability density function (pdf) $f_{X,Y}(x,y)$ supported on the sector-shaped region\n$$\nS \\triangleq \\left\\{(x,y)\\in\\mathbb{R}^{2} : \\exists\\,(r,\\theta)\\ \\text{with}\\ 1 \\le r \\le 3,\\ 0 \\le \\theta \\le \\frac{\\pi}{3},\\ x = r\\cos\\theta,\\ y = r\\sin\\theta \\right\\},\n$$\nand zero outside $S$. On $S$ the pdf is given by\n$$\nf_{X,Y}(x,y) = \\kappa\\left(x^{2}+y^{2}\\right),\n$$\nwhere $\\kappa>0$ is a constant chosen so that $f_{X,Y}$ integrates to $1$ over $\\mathbb{R}^{2}$. Angles are measured in radians.\n\n1. Using only the defining properties of a probability density function (pdf) and the standard polar-coordinate change of variables with its Jacobian, verify that $f_{X,Y}$ integrates to $1$ and determine the value of $\\kappa$.\n\n2. Let\n$$\nA \\triangleq \\left\\{(x,y)\\in S : 2 \\le \\sqrt{x^{2}+y^{2}} \\le 3,\\ \\frac{\\pi}{12} \\le \\theta \\le \\frac{\\pi}{6}\\right\\},\n$$\nwhere $\\theta$ is the polar angle associated with $(x,y)$, i.e., $x = r\\cos\\theta$, $y = r\\sin\\theta$ with $r = \\sqrt{x^{2}+y^{2}}$. Compute the probability $\\mathbb{P}\\big((X,Y)\\in A\\big)$.\n\nProvide your final answer as a single exact value. Do not round or approximate. No units are required for probability.", "solution": "The problem as stated is scientifically grounded and mathematically well-posed. It is a standard exercise in probability theory involving a joint probability density function, normalization, and calculation of probability over a specified region. We proceed with the solution.\n\nThe problem requires two calculations. First, we must determine the normalization constant $\\kappa$. Second, we must compute the probability of the random vector $(X,Y)$ belonging to a subregion $A$.\n\nPart 1: Determination of the constant $\\kappa$.\nThe defining property of any probability density function $f_{X,Y}(x,y)$ is that its integral over the entire sample space must equal $1$.\n$$ \\iint_{\\mathbb{R}^{2}} f_{X,Y}(x,y) \\,dx\\,dy = 1 $$\nThe function $f_{X,Y}(x,y)$ is non-zero only on the support region $S$. Therefore, the integral simplifies to:\n$$ \\iint_{S} \\kappa\\left(x^{2}+y^{2}\\right) \\,dx\\,dy = 1 $$\nThe region $S$ is a sector of an annulus, which is most naturally described using polar coordinates. We apply the transformation $x = r\\cos\\theta$ and $y = r\\sin\\theta$. The expression $x^2+y^2$ becomes $r^2$. The differential area element in polar coordinates is $dx\\,dy = r\\,dr\\,d\\theta$, where $r$ is the Jacobian of the transformation. The region $S$ is defined by the constraints $1 \\le r \\le 3$ and $0 \\le \\theta \\le \\frac{\\pi}{3}$.\n\nSubstituting these into the integral, we get:\n$$ \\int_{0}^{\\pi/3} \\int_{1}^{3} \\kappa(r^2) \\cdot r \\,dr\\,d\\theta = 1 $$\nThe constant $\\kappa$ can be moved outside the integral:\n$$ \\kappa \\int_{0}^{\\pi/3} \\int_{1}^{3} r^3 \\,dr\\,d\\theta = 1 $$\nWe first evaluate the inner integral with respect to $r$:\n$$ \\int_{1}^{3} r^3 \\,dr = \\left[ \\frac{r^4}{4} \\right]_{1}^{3} = \\frac{3^4}{4} - \\frac{1^4}{4} = \\frac{81-1}{4} = \\frac{80}{4} = 20 $$\nNow, we substitute this result into the outer integral with respect to $\\theta$:\n$$ \\kappa \\int_{0}^{\\pi/3} 20 \\,d\\theta = 1 $$\n$$ 20\\kappa \\left[ \\theta \\right]_{0}^{\\pi/3} = 1 $$\n$$ 20\\kappa \\left(\\frac{\\pi}{3} - 0\\right) = 1 $$\n$$ \\frac{20\\pi\\kappa}{3} = 1 $$\nSolving for $\\kappa$, we find:\n$$ \\kappa = \\frac{3}{20\\pi} $$\nSince $\\pi > 0$, we have $\\kappa > 0$, which is consistent with the problem statement.\n\nPart 2: Computation of the probability $\\mathbb{P}\\big((X,Y)\\in A\\big)$.\nThe probability of the event $(X,Y) \\in A$ is found by integrating the probability density function over the region $A$:\n$$ \\mathbb{P}\\big((X,Y)\\in A\\big) = \\iint_A f_{X,Y}(x,y) \\,dx\\,dy $$\nUsing the value of $\\kappa$ we found, the pdf on $S$ is $f_{X,Y}(x,y) = \\frac{3}{20\\pi}(x^2+y^2)$. The region $A$ is a subset of $S$, defined in polar coordinates by $2 \\le r \\le 3$ and $\\frac{\\pi}{12} \\le \\theta \\le \\frac{\\pi}{6}$. We again convert the integral to polar coordinates:\n$$ \\mathbb{P}\\big((X,Y)\\in A\\big) = \\int_{\\pi/12}^{\\pi/6} \\int_{2}^{3} \\frac{3}{20\\pi}(r^2) \\cdot r \\,dr\\,d\\theta $$\n$$ = \\frac{3}{20\\pi} \\int_{\\pi/12}^{\\pi/6} \\int_{2}^{3} r^3 \\,dr\\,d\\theta $$\nWe evaluate the inner integral with respect to $r$:\n$$ \\int_{2}^{3} r^3 \\,dr = \\left[ \\frac{r^4}{4} \\right]_{2}^{3} = \\frac{3^4}{4} - \\frac{2^4}{4} = \\frac{81-16}{4} = \\frac{65}{4} $$\nNow we substitute this into the expression for the probability:\n$$ \\mathbb{P}\\big((X,Y)\\in A\\big) = \\frac{3}{20\\pi} \\int_{\\pi/12}^{\\pi/6} \\frac{65}{4} \\,d\\theta = \\frac{3 \\cdot 65}{20\\pi \\cdot 4} \\int_{\\pi/12}^{\\pi/6} d\\theta $$\nWe simplify the constant factor:\n$$ \\frac{3 \\cdot 65}{80\\pi} = \\frac{195}{80\\pi} = \\frac{39 \\cdot 5}{16 \\cdot 5 \\pi} = \\frac{39}{16\\pi} $$\nNext, we evaluate the outer integral with respect to $\\theta$:\n$$ \\int_{\\pi/12}^{\\pi/6} d\\theta = \\left[ \\theta \\right]_{\\pi/12}^{\\pi/6} = \\frac{\\pi}{6} - \\frac{\\pi}{12} = \\frac{2\\pi}{12} - \\frac{\\pi}{12} = \\frac{\\pi}{12} $$\nFinally, we multiply the two parts to find the probability:\n$$ \\mathbb{P}\\big((X,Y)\\in A\\big) = \\frac{39}{16\\pi} \\cdot \\frac{\\pi}{12} = \\frac{39}{16 \\cdot 12} $$\nSimplifying the fraction:\n$$ \\frac{39}{192} = \\frac{3 \\cdot 13}{3 \\cdot 64} = \\frac{13}{64} $$\nThis is the required probability.", "answer": "$$\\boxed{\\frac{13}{64}}$$", "id": "2893239"}, {"introduction": "Beyond calculating simple probabilities, the real power of joint distributions lies in understanding the relationship between random variables. This practice moves from the joint description of a system to a conditional one, a crucial step in inference and signal estimation. By deriving the conditional density and computing the conditional expectation, you will learn how to formally update your knowledge about one random variable (e.g., signal amplitude) given an observation of another (e.g., a gating threshold).[@problem_id:2893245]", "problem": "In a simplified model for a gated amplitude measurement in signal processing, let $(X,Y)$ be jointly absolutely continuous random variables representing, respectively, a latent nonnegative signal amplitude and a random gating threshold. Due to the gating hardware and operating regime, assume the support constraint $0 < X < Y < 1$ holds almost surely, and that the joint probability density function is\n$$\nf_{X,Y}(x,y) \\;=\\; 2\\,(x+y)\\,\\mathbf{1}_{\\{0<x<y<1\\}},\n$$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ denotes the indicator function. Starting only from the core definitions of marginal density, conditional density for absolutely continuous random variables, and conditional expectation as an integral with respect to the conditional density, carry out the following for a fixed $y$ with $0<y<1$:\n1) Derive the conditional density $f_{X\\mid Y}(x\\mid y)$ for $x$ in the interior of its support.\n2) Compute the conditional expectation $\\mathbb{E}[X\\mid Y=y]$ as a function of $y$.\n\nExpress your final answer as closed-form analytic expressions. No rounding is required. No physical units are involved. If multiple expressions are reported, present them together as the final answer.", "solution": "The problem statement is subjected to validation before any attempt at a solution.\n\nGivens are extracted verbatim:\n1.  $(X,Y)$ are jointly absolutely continuous random variables.\n2.  The support is defined by the constraint $0 < X < Y < 1$.\n3.  The joint probability density function (PDF) is $f_{X,Y}(x,y) = 2(x+y)\\mathbf{1}_{\\{0<x<y<1\\}}$.\n4.  The task is to derive the conditional density $f_{X\\mid Y}(x\\mid y)$ and the conditional expectation $\\mathbb{E}[X\\mid Y=y]$ for a fixed $y$ such that $0<y<1$.\n5.  Derivations must start from core definitions.\n\nValidation is performed:\n1.  **Scientific and Factual Soundness**: The problem is a standard exercise in multivariate probability theory. The provided function $f_{X,Y}(x,y)$ must be a valid joint PDF. It is non-negative on its support since for $0 < x < y < 1$, the term $(x+y)$ is strictly positive. We must verify that its integral over $\\mathbb{R}^2$ is equal to $1$.\n$$ \\int_{-\\infty}^{\\infty} \\!\\! \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx \\, dy = \\int_0^1 \\! \\int_0^y 2(x+y) \\, dx \\, dy $$\nThe inner integral with respect to $x$ is:\n$$ \\int_0^y 2(x+y) \\, dx = 2 \\left[ \\frac{x^2}{2} + yx \\right]_0^y = 2 \\left( \\frac{y^2}{2} + y^2 \\right) = 2 \\left( \\frac{3y^2}{2} \\right) = 3y^2 $$\nThe outer integral with respect to $y$ is:\n$$ \\int_0^1 3y^2 \\, dy = \\left[ y^3 \\right]_0^1 = 1^3 - 0^3 = 1 $$\nThe function is a valid joint PDF. The problem is sound.\n\n2.  **Well-Posed and Objective**: The problem is specified with mathematical precision and contains no ambiguity. The objectives are clearly defined and a unique solution exists.\n\n3.  **Other Flaws**: The problem is formalizable, self-contained, and scientifically relevant to the stated field. It is not trivial.\n\nThe verdict is that the problem is **valid**. A solution will be provided.\n\nThe derivation proceeds from fundamental definitions as required.\n\n**Part 1: Derivation of the conditional density $f_{X\\mid Y}(x\\mid y)$**\n\nThe definition of the conditional PDF of $X$ given $Y=y$ is:\n$$ f_{X\\mid Y}(x\\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} $$\nThis is defined for all $y$ such that the marginal PDF $f_Y(y)$ is positive.\n\nFirst, we must find the marginal PDF $f_Y(y)$. By definition, this is obtained by integrating the joint PDF with respect to $x$:\n$$ f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx $$\nThe support of the joint PDF is $0<x<y<1$. For a fixed $y \\in (0,1)$, the variable $x$ is integrated over the interval $(0, y)$. For $y$ outside of $(0,1)$, $f_Y(y)=0$.\nFor $y \\in (0,1)$:\n$$ f_Y(y) = \\int_0^y 2(x+y) \\, dx $$\nThis integral was computed during the validation step:\n$$ f_Y(y) = 2 \\left[ \\frac{x^2}{2} + yx \\right]_0^y = 2 \\left( \\frac{y^2}{2} + y^2 \\right) = 3y^2 $$\nSo, the marginal PDF for $Y$ is $f_Y(y) = 3y^2$ for $0 < y < 1$, and $0$ otherwise. Since the problem specifies $0 < y < 1$, $f_Y(y) > 0$.\n\nNow, we construct the conditional density $f_{X\\mid Y}(x\\mid y)$. For a fixed $y \\in (0,1)$, the conditional support for $x$ is $0 < x < y$. Within this support, we have:\n$$ f_{X\\mid Y}(x\\mid y) = \\frac{2(x+y)}{3y^2} $$\nThe full expression for the conditional PDF is:\n$$ f_{X\\mid Y}(x\\mid y) = \\frac{2(x+y)}{3y^2} \\mathbf{1}_{\\{0<x<y\\}} $$\nAs a check, we ensure it integrates to $1$ over its support:\n$$ \\int_0^y \\frac{2(x+y)}{3y^2} \\, dx = \\frac{2}{3y^2} \\int_0^y (x+y) \\, dx = \\frac{2}{3y^2} \\left[ \\frac{x^2}{2} + yx \\right]_0^y = \\frac{2}{3y^2} \\left( \\frac{y^2}{2} + y^2 \\right) = \\frac{2}{3y^2} \\left( \\frac{3y^2}{2} \\right) = 1 $$\nThe result is correct.\n\n**Part 2: Computation of the conditional expectation $\\mathbb{E}[X\\mid Y=y]$**\n\nThe definition of the conditional expectation of $X$ given $Y=y$ is:\n$$ \\mathbb{E}[X\\mid Y=y] = \\int_{-\\infty}^{\\infty} x \\cdot f_{X\\mid Y}(x\\mid y) \\, dx $$\nThe integration domain is the conditional support of $X$, which is $(0, y)$.\n$$ \\mathbb{E}[X\\mid Y=y] = \\int_0^y x \\left( \\frac{2(x+y)}{3y^2} \\right) dx $$\nWe proceed with the integration:\n$$ \\mathbb{E}[X\\mid Y=y] = \\frac{2}{3y^2} \\int_0^y (x^2 + xy) \\, dx $$\n$$ = \\frac{2}{3y^2} \\left[ \\frac{x^3}{3} + \\frac{x^2y}{2} \\right]_0^y $$\n$$ = \\frac{2}{3y^2} \\left( \\left(\\frac{y^3}{3} + \\frac{y^2 \\cdot y}{2}\\right) - (0) \\right) $$\n$$ = \\frac{2}{3y^2} \\left( \\frac{y^3}{3} + \\frac{y^3}{2} \\right) $$\nCombine the terms inside the parenthesis:\n$$ \\frac{y^3}{3} + \\frac{y^3}{2} = \\frac{2y^3 + 3y^3}{6} = \\frac{5y^3}{6} $$\nSubstitute this back into the expression for the expectation:\n$$ \\mathbb{E}[X\\mid Y=y] = \\frac{2}{3y^2} \\left( \\frac{5y^3}{6} \\right) = \\frac{10y^3}{18y^2} $$\nSimplifying the expression by canceling common factors gives the final result for the conditional expectation:\n$$ \\mathbb{E}[X\\mid Y=y] = \\frac{5y}{9} $$\nThis expression is valid for $y \\in (0,1)$ as specified.\n\nThe two requested expressions are the conditional density function on its support, $\\frac{2(x+y)}{3y^2}$, and the conditional expectation, $\\frac{5y}{9}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2(x+y)}{3y^2} & \\frac{5y}{9}\n\\end{pmatrix}\n}\n$$", "id": "2893245"}, {"introduction": "A critical task in signal processing is to estimate the statistical properties of a process from a finite number of samples. This exercise bridges the gap between theoretical population parameters like variance ($\\sigma^2$) and their practical sample-based estimators. By analyzing the expectation of two common variance estimators, you will uncover the concept of statistical bias and understand the fundamental reason for Bessel's correction—the loss of a degree of freedom when the true mean is unknown and must also be estimated from the data.[@problem_id:2893255]", "problem": "A discrete-time sensor acquires $n \\ge 2$ successive samples $X_{1},\\dots,X_{n}$ from a real-valued, wide-sense stationary random process over a time window short enough that the samples can be modeled as independent and identically distributed (i.i.d.) draws from a common distribution with finite mean $\\mu$ and finite variance $\\sigma^{2}$. Let the sample mean be $\\bar{X} \\triangleq \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. Consider two estimators of the process variance:\n- The $n$-denominator estimator $\\hat{\\sigma}^{2}_{n} \\triangleq \\frac{1}{n}\\sum_{i=1}^{n}\\bigl(X_{i}-\\bar{X}\\bigr)^{2}$.\n- The $n-1$-denominator estimator $\\hat{\\sigma}^{2}_{n-1} \\triangleq \\frac{1}{n-1}\\sum_{i=1}^{n}\\bigl(X_{i}-\\bar{X}\\bigr)^{2}$.\n\nStarting only from the fundamental definitions of expectation, variance, and linearity of expectation, and using the independence of the samples, derive $\\mathbb{E}\\!\\left[\\hat{\\sigma}^{2}_{n}\\right]$ and $\\mathbb{E}\\!\\left[\\hat{\\sigma}^{2}_{n-1}\\right]$ in terms of $n$ and $\\sigma^{2}$, and compute the bias of each estimator with respect to $\\sigma^{2}$, defined as $\\mathrm{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}[\\hat{\\theta}] - \\theta$. Then, explain why replacing the denominator $n$ by $n-1$ removes the bias, commonly referred to as Bessel’s correction, in terms of the degrees of freedom consumed by estimating $\\mu$ with $\\bar{X}$.\n\nReport your final answer as a row matrix in the order $\\bigl(\\mathbb{E}[\\hat{\\sigma}^{2}_{n}],\\ \\mathrm{bias}(\\hat{\\sigma}^{2}_{n}),\\ \\mathbb{E}[\\hat{\\sigma}^{2}_{n-1}],\\ \\mathrm{bias}(\\hat{\\sigma}^{2}_{n-1}]\\bigr)$. No numerical rounding is required. No physical units are needed.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard derivation in statistical theory and is free of any invalidating flaws. We may proceed with the solution.\n\nThe problem asks for the expected values and biases of two estimators for the variance $\\sigma^2$ of a random process from which $n$ independent and identically distributed (i.i.d.) samples $X_1, \\dots, X_n$ are drawn. The underlying distribution has mean $\\mathbb{E}[X_i] = \\mu$ and variance $\\mathrm{Var}(X_i) = \\mathbb{E}[(X_i - \\mu)^2] = \\sigma^2$. Both estimators are based on the sum of squared deviations from the sample mean, $\\bar{X} \\triangleq \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nThe core of the analysis is to find the expectation of the sum of squares term, $\\sum_{i=1}^{n} (X_i - \\bar{X})^2$. We manipulate this expression by introducing the true mean $\\mu$:\n$$\n\\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} \\left( (X_i - \\mu) - (\\bar{X} - \\mu) \\right)^2\n$$\nExpanding the square within the summation gives:\n$$\n\\sum_{i=1}^{n} \\left[ (X_i - \\mu)^2 - 2(X_i - \\mu)(\\bar{X} - \\mu) + (\\bar{X} - \\mu)^2 \\right]\n$$\nWe can distribute the summation across the three terms:\n$$\n\\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} (X_i - \\mu)^2 - 2 \\sum_{i=1}^{n} (X_i - \\mu)(\\bar{X} - \\mu) + \\sum_{i=1}^{n} (\\bar{X} - \\mu)^2\n$$\nLet us analyze each term separately.\nThe first term is $\\sum_{i=1}^{n} (X_i - \\mu)^2$.\nFor the second term, the factor $(\\bar{X} - \\mu)$ is constant with respect to the summation index $i$:\n$$\n-2 (\\bar{X} - \\mu) \\sum_{i=1}^{n} (X_i - \\mu) = -2 (\\bar{X} - \\mu) \\left( \\left(\\sum_{i=1}^{n} X_i\\right) - n\\mu \\right)\n$$\nBy definition of the sample mean, $\\sum_{i=1}^{n} X_i = n\\bar{X}$. Substituting this in, we get:\n$$\n-2 (\\bar{X} - \\mu) (n\\bar{X} - n\\mu) = -2n (\\bar{X} - \\mu)^2\n$$\nFor the third term, $(\\bar{X} - \\mu)^2$ is also constant with respect to $i$, so the sum is:\n$$\n\\sum_{i=1}^{n} (\\bar{X} - \\mu)^2 = n(\\bar{X} - \\mu)^2\n$$\nCombining these results, the sum of squares expression simplifies to:\n$$\n\\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} (X_i - \\mu)^2 - 2n(\\bar{X} - \\mu)^2 + n(\\bar{X} - \\mu)^2 = \\sum_{i=1}^{n} (X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2\n$$\nNow, we take the expectation of this expression. By the linearity of expectation:\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_i - \\mu)^2 \\right] - n\\mathbb{E}\\left[ (\\bar{X} - \\mu)^2 \\right]\n$$\nLet us evaluate the expectation of each term on the right-hand side.\nFor the first term:\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_i - \\mu)^2 \\right] = \\sum_{i=1}^{n} \\mathbb{E}\\left[ (X_i - \\mu)^2 \\right]\n$$\nBy definition, $\\mathbb{E}\\left[ (X_i - \\mu)^2 \\right]$ is the variance of $X_i$, which is $\\sigma^2$. Since the samples are i.i.d., this is true for all $i=1, \\dots, n$. Thus:\n$$\n\\sum_{i=1}^{n} \\mathbb{E}\\left[ (X_i - \\mu)^2 \\right] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2\n$$\nFor the second term, $\\mathbb{E}\\left[ (\\bar{X} - \\mu)^2 \\right]$ is the variance of the sample mean, $\\mathrm{Var}(\\bar{X})$. We compute this as follows:\n$$\n\\mathrm{Var}(\\bar{X}) = \\mathrm{Var}\\left( \\frac{1}{n}\\sum_{i=1}^{n} X_i \\right) = \\frac{1}{n^2} \\mathrm{Var}\\left( \\sum_{i=1}^{n} X_i \\right)\n$$\nSince the samples $X_i$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}\\left( \\sum_{i=1}^{n} X_i \\right) = \\sum_{i=1}^{n} \\mathrm{Var}(X_i) = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2\n$$\nTherefore, the variance of the sample mean is:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}\n$$\nSubstituting these results back into the main expectation equation, we find the expected value of the sum of squares:\n$$\n\\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\right] = n\\sigma^2 - n\\left( \\frac{\\sigma^2}{n} \\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2\n$$\nWith this key result, we can now find the expectation of each variance estimator.\n\nFor the $n$-denominator estimator, $\\hat{\\sigma}^{2}_{n} = \\frac{1}{n}\\sum_{i=1}^{n}\\bigl(X_{i}-\\bar{X}\\bigr)^{2}$:\n$$\n\\mathbb{E}[\\hat{\\sigma}^{2}_{n}] = \\mathbb{E}\\left[ \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\right] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\right] = \\frac{1}{n} (n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2\n$$\nThe bias is defined as $\\mathrm{bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. For $\\hat{\\sigma}^{2}_{n}$, with $\\theta = \\sigma^2$:\n$$\n\\mathrm{bias}(\\hat{\\sigma}^{2}_{n}) = \\mathbb{E}[\\hat{\\sigma}^{2}_{n}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left( \\frac{n-1 - n}{n} \\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2\n$$\nThis estimator is biased, as its expectation is not equal to $\\sigma^2$. It systematically underestimates the true variance.\n\nFor the $n-1$-denominator estimator, $\\hat{\\sigma}^{2}_{n-1} = \\frac{1}{n-1}\\sum_{i=1}^{n}\\bigl(X_{i}-\\bar{X}\\bigr)^{2}$:\n$$\n\\mathbb{E}[\\hat{\\sigma}^{2}_{n-1}] = \\mathbb{E}\\left[ \\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\right] = \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\right] = \\frac{1}{n-1} (n-1)\\sigma^2 = \\sigma^2\n$$\nThe bias for this estimator is:\n$$\n\\mathrm{bias}(\\hat{\\sigma}^{2}_{n-1}) = \\mathbb{E}[\\hat{\\sigma}^{2}_{n-1}] - \\sigma^2 = \\sigma^2 - \\sigma^2 = 0\n$$\nThis estimator is unbiased.\n\nThe reason replacing the denominator $n$ with $n-1$ removes the bias relates to degrees of freedom. The variance $\\sigma^2$ is defined by the expected squared deviation from the true mean $\\mu$. If $\\mu$ were known, the estimator $\\frac{1}{n}\\sum(X_i-\\mu)^2$ would be unbiased for $\\sigma^2$, as it averages $n$ independent squared deviations, each with expectation $\\sigma^2$.\nHowever, $\\mu$ is unknown and is estimated by $\\bar{X}$. The sample mean has the property that it minimizes the sum of squared deviations, i.e., $\\sum(X_i - c)^2$ is minimized when $c = \\bar{X}$. This implies $\\sum(X_i - \\bar{X})^2 \\le \\sum(X_i - \\mu)^2$ for any collection of samples. Consequently, using $\\bar{X}$ in place of $\\mu$ results in a sum of squares that is, on average, smaller than the sum of squares around the true mean. Our calculation shows that $\\mathbb{E}[\\sum(X_i - \\bar{X})^2]$ is not $n\\sigma^2$ but $(n-1)\\sigma^2$.\nThe set of $n$ deviations from the sample mean, $\\{X_i - \\bar{X}\\}_{i=1}^n$, is not fully independent. They are subject to the constraint $\\sum_{i=1}^n(X_i - \\bar{X}) = (\\sum X_i) - n\\bar{X} = n\\bar{X} - n\\bar{X} = 0$. This constraint means that if $n-1$ of the deviations are known, the $n$-th is completely determined. The collection of deviations possesses only $n-1$ degrees of freedom. By dividing the sum of squares by its degrees of freedom, $n-1$, we obtain an unbiased estimate of the variance. This correction factor, which scales the biased estimate by $\\frac{n}{n-1}$, is known as Bessel's correction. It accounts for the loss of one degree of freedom used to estimate the population mean with the sample mean.\n\nThe final results are compiled into the requested row matrix.\n$\\mathbb{E}[\\hat{\\sigma}^{2}_{n}] = \\frac{n-1}{n}\\sigma^2$\n$\\mathrm{bias}(\\hat{\\sigma}^{2}_{n}) = -\\frac{1}{n}\\sigma^2$\n$\\mathbb{E}[\\hat{\\sigma}^{2}_{n-1}] = \\sigma^2$\n$\\mathrm{bias}(\\hat{\\sigma}^{2}_{n-1}) = 0$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n-1}{n}\\sigma^{2} & -\\frac{1}{n}\\sigma^{2} & \\sigma^{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "2893255"}]}