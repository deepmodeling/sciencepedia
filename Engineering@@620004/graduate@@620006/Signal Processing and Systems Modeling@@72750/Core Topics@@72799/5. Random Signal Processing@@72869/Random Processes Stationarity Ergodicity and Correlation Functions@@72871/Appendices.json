{"hands_on_practices": [{"introduction": "Understanding the power spectral density (PSD) is key to analyzing random signals in the frequency domain. This first exercise bridges the time and frequency domains by exploring how a non-zero mean, a constant DC offset in the time domain, manifests in the PSD. By working from first principles, you will verify the foundational result that a non-zero mean $m_X$ contributes a spectral impulse $|m_X|^2 \\delta(f)$ at zero frequency and confirm the consistency of the Wiener-Khinchin relations through direct calculation [@problem_id:2899117].", "problem": "Consider a continuous-time wide-sense stationary (WSS) random process $X(t)$ with possibly complex values, having constant (time-invariant) mean $m_{X} \\triangleq \\mathbb{E}[X(t)] \\neq 0$. Let $Y(t) \\triangleq X(t) - m_{X}$ denote the zero-mean fluctuation about the mean. Denote by $R_{X}(\\tau) \\triangleq \\mathbb{E}[X(t) X^{\\ast}(t+\\tau)]$ the autocorrelation function and by $S_{X}(f)$ the power spectral density (PSD), related for WSS processes by the Fourier transform pair\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{X}(\\tau)\\, \\exp(-j 2\\pi f \\tau)\\, d\\tau, \n\\quad\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} S_{X}(f)\\, \\exp(j 2\\pi f \\tau)\\, df,\n$$\nwhere $\\delta(\\cdot)$ denotes the Dirac delta distribution.\n\n1) Starting strictly from the definitions above, and using only linearity of expectation and stationarity, derive how a nonzero mean $m_{X}$ contributes a discrete spectral line to $S_{X}(f)$. State precisely the frequency at which this line occurs and its amplitude.\n\n2) Suppose the zero-mean component $Y(t)$ has a continuous PSD given by\n$$\nS_{Y}(f) = \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}},\n$$\nwith constants $a > 0$ and $\\sigma^{2} > 0$. Using only the inverse Fourier transform definition above, integrate the total PSD\n$$\nS_{X}(f) = |m_{X}|^{2}\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}}\n$$\nto recover the autocorrelation $R_{X}(\\tau)$ as a closed-form analytic expression in terms of $m_{X}$, $\\sigma^{2}$, $a$, and $\\tau$. Provide your final answer as a single expression for $R_{X}(\\tau)$. No numerical approximation is required, and no units are needed.", "solution": "The problem statement has been analyzed and is determined to be valid. It is a well-posed, self-contained, and scientifically sound problem in the field of signal processing and random processes. The problem is formalizable and free from contradiction, ambiguity, or factual error. We may therefore proceed with the solution.\n\nThe problem is divided into two parts.\n\nPart 1: Derivation of the spectral line from a non-zero mean.\n\nWe are given a wide-sense stationary (WSS) random process $X(t)$ with a constant, non-zero mean $m_{X} = \\mathbb{E}[X(t)] \\neq 0$. The zero-mean fluctuation is defined as $Y(t) = X(t) - m_{X}$, from which it follows that $\\mathbb{E}[Y(t)] = \\mathbb{E}[X(t) - m_{X}] = \\mathbb{E}[X(t)] - m_{X} = m_{X} - m_{X} = 0$.\n\nThe autocorrelation function of $X(t)$ is defined as $R_{X}(\\tau) \\triangleq \\mathbb{E}[X(t) X^{\\ast}(t+\\tau)]$. We substitute $X(t) = Y(t) + m_{X}$ into this definition:\n$$\nR_{X}(\\tau) = \\mathbb{E}\\left[ \\left( Y(t) + m_{X} \\right) \\left( Y(t+\\tau) + m_{X} \\right)^{\\ast} \\right]\n$$\nExpanding the product, we get:\n$$\nR_{X}(\\tau) = \\mathbb{E}\\left[ \\left( Y(t) + m_{X} \\right) \\left( Y^{\\ast}(t+\\tau) + m_{X}^{\\ast} \\right) \\right] = \\mathbb{E}\\left[ Y(t)Y^{\\ast}(t+\\tau) + Y(t)m_{X}^{\\ast} + m_{X}Y^{\\ast}(t+\\tau) + m_{X}m_{X}^{\\ast} \\right]\n$$\nBy linearity of the expectation operator, we can write this as four separate terms:\n$$\nR_{X}(\\tau) = \\mathbb{E}[Y(t)Y^{\\ast}(t+\\tau)] + \\mathbb{E}[Y(t)m_{X}^{\\ast}] + \\mathbb{E}[m_{X}Y^{\\ast}(t+\\tau)] + \\mathbb{E}[m_{X}m_{X}^{\\ast}]\n$$\nSince $m_{X}$ is a constant, we can factor it out of the expectations:\n$$\nR_{X}(\\tau) = \\mathbb{E}[Y(t)Y^{\\ast}(t+\\tau)] + m_{X}^{\\ast}\\mathbb{E}[Y(t)] + m_{X}\\mathbb{E}[Y^{\\ast}(t+\\tau)] + |m_{X}|^{2}\n$$\nWe have already established that $Y(t)$ is a zero-mean process, so $\\mathbb{E}[Y(t)] = 0$ for any time $t$. By WSS properties, this holds for all $t$. This also implies $\\mathbb{E}[Y^{\\ast}(t+\\tau)] = (\\mathbb{E}[Y(t+\\tau)])^{\\ast} = 0^{\\ast} = 0$. The middle two terms are therefore zero. The first term is, by definition, the autocorrelation function of the zero-mean process $Y(t)$, which we denote $R_{Y}(\\tau)$.\nThe expression for $R_{X}(\\tau)$ simplifies to:\n$$\nR_{X}(\\tau) = R_{Y}(\\tau) + |m_{X}|^{2}\n$$\nNow, we find the power spectral density (PSD) $S_{X}(f)$ by taking the Fourier transform of $R_{X}(\\tau)$, as per the given Wiener-Khinchin relation:\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{X}(\\tau) \\exp(-j 2\\pi f \\tau) d\\tau = \\int_{-\\infty}^{\\infty} (R_{Y}(\\tau) + |m_{X}|^{2}) \\exp(-j 2\\pi f \\tau) d\\tau\n$$\nUsing the linearity of the Fourier transform, we separate the integral:\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{Y}(\\tau) \\exp(-j 2\\pi f \\tau) d\\tau + \\int_{-\\infty}^{\\infty} |m_{X}|^{2} \\exp(-j 2\\pi f \\tau) d\\tau\n$$\nThe first integral is the definition of the PSD of $Y(t)$, which is $S_{Y}(f)$. For the second integral, we recognize the integral representation of the Dirac delta distribution, $\\delta(f) = \\int_{-\\infty}^{\\infty} \\exp(-j 2\\pi f \\tau) d\\tau$.\n$$\n\\int_{-\\infty}^{\\infty} |m_{X}|^{2} \\exp(-j 2\\pi f \\tau) d\\tau = |m_{X}|^{2} \\int_{-\\infty}^{\\infty} \\exp(-j 2\\pi f \\tau) d\\tau = |m_{X}|^{2} \\delta(f)\n$$\nCombining these results, we obtain the total PSD of $X(t)$:\n$$\nS_{X}(f) = S_{Y}(f) + |m_{X}|^{2} \\delta(f)\n$$\nThis result demonstrates that the non-zero mean $m_{X}$ contributes a discrete spectral line to the power spectral density. This line is precisely described as follows:\n-   **Frequency:** It occurs at frequency $f=0$, which is the DC component.\n-   **Amplitude:** The term \"amplitude\" for a Dirac delta function refers to its weight or integrated strength. The line is given by the term $|m_{X}|^{2} \\delta(f)$, so its amplitude (strength) is $|m_{X}|^{2}$.\n\nPart 2: Recovery of the autocorrelation function $R_{X}(\\tau)$.\n\nWe are given the total power spectral density of the process $X(t)$:\n$$\nS_{X}(f) = |m_{X}|^{2}\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}}\n$$\nwhere the second term is the PSD of the zero-mean component, $S_{Y}(f)$. We are to find the autocorrelation function $R_{X}(\\tau)$ by applying the inverse Fourier transform as defined in the problem:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} S_{X}(f) \\exp(j 2\\pi f \\tau) df\n$$\nSubstituting the expression for $S_{X}(f)$:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} \\left( |m_{X}|^{2}\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\right) \\exp(j 2\\pi f \\tau) df\n$$\nBy linearity, we can split this into two integrals:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} |m_{X}|^{2}\\, \\delta(f) \\exp(j 2\\pi f \\tau) df + \\int_{-\\infty}^{\\infty} \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\exp(j 2\\pi f \\tau) df\n$$\nLet us evaluate each integral separately. The first integral is evaluated using the sifting property of the Dirac delta function, $\\int_{-\\infty}^{\\infty} g(f) \\delta(f-f_{0}) df = g(f_{0})$. Here, $g(f) = |m_{X}|^{2} \\exp(j 2\\pi f \\tau)$ and $f_{0}=0$.\n$$\n\\int_{-\\infty}^{\\infty} |m_{X}|^{2}\\, \\delta(f) \\exp(j 2\\pi f \\tau) df = |m_{X}|^{2} \\exp(j 2\\pi (0) \\tau) = |m_{X}|^{2}\n$$\nThis is the constant component of the autocorrelation function, corresponding to the mean.\n\nThe second integral is the inverse Fourier transform of $S_{Y}(f)$, which yields $R_{Y}(\\tau)$:\n$$\nR_{Y}(\\tau) = \\int_{-\\infty}^{\\infty} \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\exp(j 2\\pi f \\tau) df\n$$\nThis is a standard Fourier transform pair. We recognize that the function $\\sigma^{2} \\exp(-a|\\tau|)$ for $a>0$ has the Fourier transform:\n$$\n\\mathcal{F}\\{\\sigma^2 \\exp(-a|\\tau|)\\} = \\sigma^{2} \\int_{-\\infty}^{\\infty} \\exp(-a|\\tau|) \\exp(-j 2\\pi f \\tau) d\\tau = \\sigma^{2} \\frac{2a}{a^{2} + (2\\pi f)^{2}} = S_Y(f)\n$$\nTherefore, by the definition of the inverse Fourier transform, we have:\n$$\nR_{Y}(\\tau) = \\mathcal{F}^{-1}\\{S_{Y}(f)\\} = \\sigma^{2} \\exp(-a|\\tau|)\n$$\nCombining the results for the two parts of the integral, we obtain the complete autocorrelation function for $X(t)$:\n$$\nR_{X}(\\tau) = |m_{X}|^{2} + R_{Y}(\\tau) = |m_{X}|^{2} + \\sigma^{2} \\exp(-a|\\tau|)\n$$\nThis is the final closed-form analytic expression for $R_{X}(\\tau)$.", "answer": "$$\n\\boxed{|m_{X}|^{2} + \\sigma^{2} \\exp(-a |\\tau|)}\n$$", "id": "2899117"}, {"introduction": "Many signal processing applications, such as energy detection, involve non-linear operations on random signals. This exercise investigates how the statistical properties of a signal are altered by a squaring operation. Starting with a wide-sense stationary (WSS) Gaussian process, you will derive the autocorrelation function of the squared process $Y(t) = X^2(t)$ and determine the conditions required for this new process to be ergodic in the mean [@problem_id:2899144]. This provides invaluable insight into how statistical characteristics, including stationarity and ergodicity, propagate through non-linear systems.", "problem": "Let $X(t)$ be a real-valued, zero-mean, wide-sense stationary (WSS) Gaussian random process with autocorrelation function $R_{X}(\\tau) = \\mathbb{E}\\{X(t)X(t+\\tau)\\}$ and variance $\\sigma_{X}^{2} = R_{X}(0)$. Define the squared process $Y(t) = X^{2}(t)$. Using only fundamental definitions of wide-sense stationarity, properties of expectations, and well-tested facts about jointly Gaussian random variables, perform the following:\n\n1. Derive a closed-form expression for $\\mathbb{E}\\{X^{2}(t)X^{2}(t+\\tau)\\}$ in terms of $R_{X}(\\tau)$ and $R_{X}(0)$.\n2. Using the result of Part $1$, determine the mean and autocorrelation function of $Y(t)$, and state whether $Y(t)$ is wide-sense stationary. Justify your conclusion from first principles starting with the definitions.\n3. Discuss the ergodicity in the mean of $Y(t)$ by relating the variance of its time average to the autocovariance of $Y(t)$. Provide a sufficient condition, expressed either in the time domain via $R_{X}(\\tau)$ or in the frequency domain via the power spectral density, that ensures mean ergodicity of $Y(t)$.\n\nYour final answer should be the analytic expression obtained in Part $1$. No numerical evaluation is required, and no units are involved. Do not round.", "solution": "We begin with the given assumptions. The process $X(t)$ is zero-mean, wide-sense stationary (WSS), and Gaussian. Thus, for any finite collection of times, the corresponding vector of samples is a jointly Gaussian random vector with mean $0$, and the covariance between $X(t)$ and $X(t+\\tau)$ is $R_{X}(\\tau)$.\n\nPart 1. Compute $\\mathbb{E}\\{X^{2}(t)X^{2}(t+\\tau)\\}$.\n\nLet $X_{1} \\triangleq X(t)$ and $X_{2} \\triangleq X(t+\\tau)$. The pair $(X_{1},X_{2})$ is a zero-mean jointly Gaussian vector with $\\mathbb{E}\\{X_{1}^{2}\\} = \\mathbb{E}\\{X_{2}^{2}\\} = \\sigma_{X}^{2} = R_{X}(0)$ and $\\mathbb{E}\\{X_{1}X_{2}\\} = R_{X}(\\tau)$. For zero-mean jointly Gaussian random variables, fourth moments can be expressed in terms of second moments via Isserlis’ theorem (also known as Wick’s formula), which is a well-tested fact: for any zero-mean jointly Gaussian $(Z_{1},Z_{2},Z_{3},Z_{4})$,\n$$\n\\mathbb{E}\\{Z_{1}Z_{2}Z_{3}Z_{4}\\} = \\mathbb{E}\\{Z_{1}Z_{2}\\}\\mathbb{E}\\{Z_{3}Z_{4}\\} + \\mathbb{E}\\{Z_{1}Z_{3}\\}\\mathbb{E}\\{Z_{2}Z_{4}\\} + \\mathbb{E}\\{Z_{1}Z_{4}\\}\\mathbb{E}\\{Z_{2}Z_{3}\\}.\n$$\nApplying this with $(Z_{1},Z_{2},Z_{3},Z_{4}) = (X_{1},X_{1},X_{2},X_{2})$, we obtain\n$$\n\\mathbb{E}\\{X_{1}^{2}X_{2}^{2}\\} = \\mathbb{E}\\{X_{1}X_{1}\\}\\mathbb{E}\\{X_{2}X_{2}\\} + \\mathbb{E}\\{X_{1}X_{2}\\}\\mathbb{E}\\{X_{1}X_{2}\\} + \\mathbb{E}\\{X_{1}X_{2}\\}\\mathbb{E}\\{X_{1}X_{2}\\}.\n$$\nUsing the covariances,\n$$\n\\mathbb{E}\\{X_{1}^{2}X_{2}^{2}\\} = \\sigma_{X}^{2}\\sigma_{X}^{2} + R_{X}^{2}(\\tau) + R_{X}^{2}(\\tau) = \\sigma_{X}^{4} + 2R_{X}^{2}(\\tau).\n$$\nEquivalently, in terms of $R_{X}(0)$,\n$$\n\\mathbb{E}\\{X^{2}(t)X^{2}(t+\\tau)\\} = R_{X}^{2}(0) + 2R_{X}^{2}(\\tau).\n$$\n\nPart 2. Mean and autocorrelation of $Y(t) = X^{2}(t)$ and stationarity.\n\nThe mean of $Y(t)$ is\n$$\n\\mu_{Y} \\triangleq \\mathbb{E}\\{Y(t)\\} = \\mathbb{E}\\{X^{2}(t)\\} = \\sigma_{X}^{2} = R_{X}(0),\n$$\nwhich is constant in $t$ due to wide-sense stationarity of $X(t)$.\n\nThe autocorrelation of $Y(t)$ is\n$$\nR_{Y}(\\tau) \\triangleq \\mathbb{E}\\{Y(t)Y(t+\\tau)\\} = \\mathbb{E}\\{X^{2}(t)X^{2}(t+\\tau)\\} = \\sigma_{X}^{4} + 2R_{X}^{2}(\\tau).\n$$\nThis depends only on the lag $\\tau$, because $R_{X}(\\tau)$ does. Hence $Y(t)$ has constant mean and an autocorrelation function that depends only on $\\tau$, so $Y(t)$ is wide-sense stationary.\n\nIt is often useful to compute the autocovariance of $Y(t)$:\n$$\nC_{Y}(\\tau) \\triangleq R_{Y}(\\tau) - \\mu_{Y}^{2} = \\left(\\sigma_{X}^{4} + 2R_{X}^{2}(\\tau)\\right) - \\sigma_{X}^{4} = 2R_{X}^{2}(\\tau).\n$$\n\nPart 3. Ergodicity in the mean of $Y(t)$.\n\nA standard route to mean ergodicity for a wide-sense stationary process $Y(t)$ is to study the variance of the time average over a long interval. Define the time average\n$$\n\\overline{Y}_{T} \\triangleq \\frac{1}{T}\\int_{0}^{T} Y(t)\\,dt.\n$$\nFor a wide-sense stationary process with autocovariance $C_{Y}(\\tau)$, a well-tested result states\n$$\n\\operatorname{var}(\\overline{Y}_{T}) = \\frac{1}{T^{2}}\\int_{0}^{T}\\int_{0}^{T} C_{Y}(t_{1}-t_{2})\\,dt_{1}\\,dt_{2} = \\frac{1}{T}\\int_{-T}^{T}\\left(1 - \\frac{|\\tau|}{T}\\right) C_{Y}(\\tau)\\,d\\tau.\n$$\nIf $C_{Y}(\\tau)$ is absolutely integrable, i.e., $\\int_{-\\infty}^{\\infty} |C_{Y}(\\tau)|\\,d\\tau  \\infty$, then $\\operatorname{var}(\\overline{Y}_{T}) \\to 0$ as $T \\to \\infty$, which implies mean ergodicity. Substituting $C_{Y}(\\tau) = 2R_{X}^{2}(\\tau)$ gives the sufficient condition\n$$\n\\int_{-\\infty}^{\\infty} R_{X}^{2}(\\tau)\\,d\\tau  \\infty.\n$$\nIn the frequency domain, by the Wiener–Khintchine theorem and Plancherel’s identity, this is equivalent to the power spectral density $S_{X}(\\omega)$ of $X(t)$ being square integrable:\n$$\n\\int_{-\\infty}^{\\infty} R_{X}^{2}(\\tau)\\,d\\tau = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} S_{X}^{2}(\\omega)\\,d\\omega  \\infty.\n$$\nThus, if $S_{X}(\\omega)$ is square integrable (in particular, has no Dirac impulse at $\\omega = 0$ and finite spectral power spread), then $Y(t) = X^{2}(t)$ is ergodic in the mean. More generally, any sufficient condition ensuring $\\int_{-\\infty}^{\\infty} |R_{X}(\\tau)|^{2}\\,d\\tau  \\infty$ guarantees mean ergodicity of the squared process.\n\nIn summary, the key computation is\n$$\n\\mathbb{E}\\{X^{2}(t)X^{2}(t+\\tau)\\} = R_{X}^{2}(0) + 2R_{X}^{2}(\\tau),\n$$\nfrom which $Y(t)$ is seen to be wide-sense stationary with mean $R_{X}(0)$, autocovariance $2R_{X}^{2}(\\tau)$, and mean ergodicity under the sufficient integrability condition above.", "answer": "$$\\boxed{R_{X}^{2}(0) + 2\\,R_{X}^{2}(\\tau)}$$", "id": "2899144"}, {"introduction": "Beyond analyzing existing signals, a central task in systems modeling is to synthesize processes with desired statistical properties. This problem guides you through the design of a discrete-time random process using the powerful technique of spectral factorization. Starting with a target power spectral density $S_{X}(\\exp(i\\omega))$, you will construct a causal and stable linear filter that shapes simple white noise into a process with the specified characteristics, providing a hands-on application of the innovations representation and Wold's theorem [@problem_id:2899153].", "problem": "Consider a real, zero-mean, discrete-time, wide-sense stationary random process $X[n]$ with power spectral density $S_{X}(\\exp(i\\omega))$ given for all real $\\omega$ by\n$$\nS_{X}(\\exp(i\\omega))=\\frac{5+4\\cos\\omega}{\\frac{5}{4}-\\cos\\omega}.\n$$\nAssume that $X[n]$ admits an innovations representation of the Wold type, namely $X[n]=\\sum_{k=0}^{\\infty} h[k]\\,\\epsilon[n-k]$, where $\\{\\epsilon[n]\\}$ is a white, zero-mean, finite-variance process with variance $\\sigma_{\\epsilon}^{2}$, and $\\{h[k]\\}$ is the impulse response of a causal, stable, linear time-invariant filter with $z$-transform $H(z)=\\sum_{k=0}^{\\infty} h[k] z^{-k}$.\n\n- Starting from the definitions of autocorrelation, power spectral density, and whiteness (that is, $\\mathbb{E}\\{\\epsilon[n]\\epsilon[n+m]\\}=\\sigma_{\\epsilon}^{2}\\delta[m]$), derive from first principles that any such representation necessarily satisfies\n$$\nS_{X}(\\exp(i\\omega))=\\left|H(\\exp(i\\omega))\\right|^{2}\\,\\sigma_{\\epsilon}^{2}.\n$$\n\n- Using spectral factorization and the requirement that $H(z)$ be causal, stable, and minimum-phase (all zeros strictly inside the unit circle), construct such an $H(z)$ and $\\sigma_{\\epsilon}^{2}$ that realize the given $S_{X}(\\exp(i\\omega))$ in the innovations representation above. Impose the canonical normalization $h[0]=1$ to fix the scaling between $H(z)$ and $\\sigma_{\\epsilon}^{2}$. Provide $H(z)$ explicitly as a rational function of $z^{-1}$ and give the corresponding $\\sigma_{\\epsilon}^{2}$.\n\nYour final answer must consist of the pair $\\big(H(z),\\sigma_{\\epsilon}^{2}\\big)$ in closed form. No numerical rounding is required.", "solution": "The user has provided a problem from the field of signal processing and systems modeling, specifically concerning random processes. The problem is well-defined, scientifically grounded, and contains sufficient information for a unique solution. It is a standard exercise in spectral factorization. The problem is deemed valid.\n\nThe problem consists of two parts. First, a derivation of a fundamental relationship is required. Second, this relationship is to be applied to a specific case.\n\n**Part 1: Derivation of the Power Spectral Density Relation**\n\nWe are asked to derive the relation $S_{X}(\\exp(i\\omega)) = |H(\\exp(i\\omega))|^2 \\sigma_{\\epsilon}^{2}$ from first principles.\n\nLet $X[n]$ be a wide-sense stationary (WSS) random process given by the innovations representation:\n$$ X[n] = \\sum_{k=0}^{\\infty} h[k] \\epsilon[n-k] $$\nThis represents the convolution of the impulse response $\\{h[k]\\}$ with the white noise process $\\{\\epsilon[n]\\}$. The process $X[n]$ is zero-mean because $\\mathbb{E}\\{X[n]\\} = \\sum_{k=0}^{\\infty} h[k] \\mathbb{E}\\{\\epsilon[n-k]\\} = 0$, given that $\\epsilon[n]$ is a zero-mean process.\n\nThe autocorrelation function of a zero-mean WSS process $X[n]$ is defined as $R_{X}[m] = \\mathbb{E}\\{X[n]X[n+m]\\}$. We substitute the expression for $X[n]$:\n$$ R_{X}[m] = \\mathbb{E}\\left\\{ \\left(\\sum_{k=0}^{\\infty} h[k]\\epsilon[n-k]\\right) \\left(\\sum_{l=0}^{\\infty} h[l]\\epsilon[n+m-l]\\right) \\right\\} $$\nBy linearity of the expectation operator, we can move it inside the summations:\n$$ R_{X}[m] = \\sum_{k=0}^{\\infty} \\sum_{l=0}^{\\infty} h[k]h[l] \\mathbb{E}\\{\\epsilon[n-k]\\epsilon[n+m-l]\\} $$\nThe process $\\{\\epsilon[n]\\}$ is white noise, which is defined by its autocorrelation function $\\mathbb{E}\\{\\epsilon[i]\\epsilon[j]\\} = \\sigma_{\\epsilon}^{2}\\delta[j-i]$, where $\\sigma_{\\epsilon}^{2}$ is the variance and $\\delta[\\cdot]$ is the Kronecker delta function. In our expression, $i=n-k$ and $j=n+m-l$. Thus, the expectation becomes:\n$$ \\mathbb{E}\\{\\epsilon[n-k]\\epsilon[n+m-l]\\} = \\sigma_{\\epsilon}^{2}\\delta[(n+m-l)-(n-k)] = \\sigma_{\\epsilon}^{2}\\delta[m-l+k] $$\nSubstituting this back into the expression for $R_{X}[m]$:\n$$ R_{X}[m] = \\sum_{k=0}^{\\infty} \\sum_{l=0}^{\\infty} h[k]h[l] \\sigma_{\\epsilon}^{2}\\delta[m-l+k] $$\nThe summation over $l$ collapses due to the delta function, which is non-zero only when $l = k+m$.\n$$ R_{X}[m] = \\sigma_{\\epsilon}^{2} \\sum_{k=0}^{\\infty} h[k]h[k+m] $$\nThe summation $\\sum_{k=-\\infty}^{\\infty} h[k]h[k+m]$, since $h[k]$ is causal ($h[k]=0$ for $k0$), is the deterministic autocorrelation of the real sequence $\\{h[k]\\}$, which we denote as $R_h[m]$. This can be expressed as the convolution of $h[m]$ with its time-reversed version, $h[-m]$. Let $h_{rev}[m] = h[-m]$. Then $R_h[m] = \\sum_{k} h[k]h[-(m-k)] = (h * h_{rev})[m]$.\nSo, $R_X[m] = \\sigma_{\\epsilon}^{2}R_h[m]$.\n\nThe power spectral density (PSD) $S_{X}(\\exp(i\\omega))$ is the Discrete-Time Fourier Transform (DTFT) of the autocorrelation function $R_{X}[m]$.\n$$ S_{X}(\\exp(i\\omega)) = \\mathcal{F}\\{R_{X}[m]\\} = \\mathcal{F}\\{\\sigma_{\\epsilon}^{2}R_h[m]\\} = \\sigma_{\\epsilon}^{2} \\mathcal{F}\\{R_h[m]\\} $$\nUsing the convolution theorem, the DTFT of $R_h[m] = (h * h_{rev})[m]$ is the product of the DTFTs of $h[m]$ and $h_{rev}[m]$.\nThe DTFT of $h[m]$ is the transfer function $H(\\exp(i\\omega))$.\nThe DTFT of $h_{rev}[m] = h[-m]$ is $H(\\exp(-i\\omega))$.\nTherefore, the DTFT of $R_h[m]$ is $H(\\exp(i\\omega))H(\\exp(-i\\omega))$.\n$$ S_{X}(\\exp(i\\omega)) = \\sigma_{\\epsilon}^{2} H(\\exp(i\\omega))H(\\exp(-i\\omega)) $$\nSince the impulse response $\\{h[k]\\}$ is a real sequence, its DTFT exhibits conjugate symmetry: $H(\\exp(-i\\omega)) = H^{*}(\\exp(i\\omega))$, where $H^{*}$ denotes the complex conjugate.\nSubstituting this property yields the final relation:\n$$ S_{X}(\\exp(i\\omega)) = \\sigma_{\\epsilon}^{2} H(\\exp(i\\omega))H^{*}(\\exp(i\\omega)) = \\sigma_{\\epsilon}^{2} \\left|H(\\exp(i\\omega))\\right|^{2} $$\nThis completes the required derivation.\n\n**Part 2: Spectral Factorization**\n\nWe are given the PSD:\n$$ S_{X}(\\exp(i\\omega))=\\frac{5+4\\cos\\omega}{\\frac{5}{4}-\\cos\\omega} $$\nWe perform spectral factorization by converting the expression to the $z$-domain using the substitution $\\cos\\omega = \\frac{1}{2}(z+z^{-1})$ for $z = \\exp(i\\omega)$.\n$$ S_{X}(z) = \\frac{5+4\\left(\\frac{1}{2}(z+z^{-1})\\right)}{\\frac{5}{4}-\\left(\\frac{1}{2}(z+z^{-1})\\right)} = \\frac{5+2(z+z^{-1})}{\\frac{5}{4}-\\frac{1}{2}(z+z^{-1})} $$\nThe factorization is based on the relation $S_X(z) = \\sigma^2_{\\epsilon}H(z)H(z^{-1})$. We must find the poles and zeros of $S_X(z)$.\nThe zeros of $S_X(z)$ are the roots of the numerator: $5+2(z+z^{-1})=0$, which is equivalent to the quadratic equation $2z^{2}+5z+2=0$. The roots are:\n$$ z = \\frac{-5 \\pm \\sqrt{5^2 - 4(2)(2)}}{2(2)} = \\frac{-5 \\pm 3}{4} $$\nSo, the zeros are $z_1 = -1/2$ and $z_2 = -2$.\n\nThe poles of $S_X(z)$ are the roots of the denominator: $\\frac{5}{4}-\\frac{1}{2}(z+z^{-1})=0$, which is equivalent to $-\\frac{1}{2}z^2+\\frac{5}{4}z-\\frac{1}{2}=0$, or $z^2 - \\frac{5}{2}z + 1 = 0$. The roots are:\n$$ z = \\frac{\\frac{5}{2} \\pm \\sqrt{(\\frac{5}{2})^2 - 4}}{2} = \\frac{\\frac{5}{2} \\pm \\sqrt{\\frac{25}{4}-4}}{2} = \\frac{\\frac{5}{2} \\pm \\frac{3}{2}}{2} $$\nSo, the poles are $p_1 = 1/2$ and $p_2 = 2$.\n\nThe filter $H(z)$ must be causal and stable, meaning all its poles must be strictly inside the unit circle ($|p|1$). It must also be minimum-phase, meaning all its zeros must be strictly inside the unit circle ($|z|1$).\nFrom the poles of $S_X(z)$, we assign the pole $p_1=1/2$ to $H(z)$.\nFrom the zeros of $S_X(z)$, we assign the zero $z_1=-1/2$ to $H(z)$.\nThe pole $p_2=2$ and zero $z_2=-2$ (which are the reciprocals of $p_1$ and $z_1$, respectively) will be associated with $H(z^{-1})$.\n\nThe transfer function $H(z)$ must therefore have the form:\n$$ H(z) = K \\frac{1-z_1 z^{-1}}{1-p_1 z^{-1}} = K \\frac{1 - (-1/2)z^{-1}}{1-(1/2)z^{-1}} = K \\frac{1+\\frac{1}{2}z^{-1}}{1-\\frac{1}{2}z^{-1}} $$\nwhere $K$ is a constant. We are given the normalization condition $h[0]=1$. The value $h[0]$ is found by evaluating $H(z)$ at $z \\to \\infty$ (or $z^{-1} \\to 0$):\n$$ h[0] = \\lim_{z\\to\\infty} H(z) = K \\frac{1+0}{1-0} = K $$\nThus, the condition $h[0]=1$ implies $K=1$. The filter transfer function is:\n$$ H(z) = \\frac{1+\\frac{1}{2}z^{-1}}{1-\\frac{1}{2}z^{-1}} = \\frac{2+z^{-1}}{2-z^{-1}} $$\n\nNow we determine the variance $\\sigma_{\\epsilon}^2$. We have:\n$$ S_{X}(z) = \\sigma_{\\epsilon}^{2} H(z) H(z^{-1}) $$\nLet us compute $H(z)H(z^{-1})$:\n$$ H(z)H(z^{-1}) = \\left(\\frac{1+\\frac{1}{2}z^{-1}}{1-\\frac{1}{2}z^{-1}}\\right)\\left(\\frac{1+\\frac{1}{2}z}{1-\\frac{1}{2}z}\\right) = \\frac{1+\\frac{1}{2}(z+z^{-1})+\\frac{1}{4}}{1-\\frac{1}{2}(z+z^{-1})+\\frac{1}{4}} = \\frac{\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})}{\\frac{5}{4}-\\frac{1}{2}(z+z^{-1})} $$\nSubstituting this into the identity:\n$$ \\frac{5+2(z+z^{-1})}{\\frac{5}{4}-\\frac{1}{2}(z+z^{-1})} = \\sigma_{\\epsilon}^{2} \\frac{\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})}{\\frac{5}{4}-\\frac{1}{2}(z+z^{-1})} $$\nThe denominators are identical. Equating the numerators:\n$$ 5+2(z+z^{-1}) = \\sigma_{\\epsilon}^{2} \\left(\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})\\right) $$\nWe can rewrite the left-hand side as $4\\left(\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})\\right)$.\n$$ 4\\left(\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})\\right) = \\sigma_{\\epsilon}^{2} \\left(\\frac{5}{4}+\\frac{1}{2}(z+z^{-1})\\right) $$\nThis equality must hold for all $z$ on the unit circle. By comparing the coefficients, we find:\n$$ \\sigma_{\\epsilon}^{2} = 4 $$\nThe required pair is composed of the transfer function $H(z)$ and the white noise variance $\\sigma_{\\epsilon}^{2}$.", "answer": "$$\n\\boxed{\n\\left( H(z), \\sigma_{\\epsilon}^{2} \\right) = \\left( \\frac{1+\\frac{1}{2}z^{-1}}{1-\\frac{1}{2}z^{-1}}, 4 \\right)\n}\n$$", "id": "2899153"}]}