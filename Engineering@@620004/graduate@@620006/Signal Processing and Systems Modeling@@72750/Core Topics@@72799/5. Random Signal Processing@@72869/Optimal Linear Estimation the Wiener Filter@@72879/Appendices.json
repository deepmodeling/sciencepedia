{"hands_on_practices": [{"introduction": "We begin our practical exploration with the most fundamental application of Wiener filtering: one-step linear prediction. This exercise [@problem_id:2888998] considers a first-order autoregressive (AR) process, a ubiquitous model in time-series analysis. By deriving the optimal single-coefficient predictor from first principles, you will solidify your understanding of how the core concepts of autocorrelation, cross-correlation, and the orthogonality principle combine to yield the celebrated Wiener-Hopf equations in their simplest scalar form.", "problem": "Consider a wide-sense stationary, zero-mean, discrete-time stochastic process defined by the first-order autoregressive recursion $x[n] = a\\,x[n-1] + w[n]$, where $|a|  1$ and $w[n]$ is a zero-mean, white random process with variance $\\sigma_{w}^{2}$ and is uncorrelated with $\\{x[k]\\}$ for all $k \\leq n-1$. You are to design the causal, order-$M=1$ linear minimum mean-square error (LMMSE) one-step predictor of $x[n]$ based on the single regressor $x[n-1]$. That is, consider the estimator $\\hat{x}[n] = h\\,x[n-1]$, and minimize $\\mathbb{E}\\{(x[n]-\\hat{x}[n])^{2}\\}$ over the scalar coefficient $h$.\n\nStarting only from the definitions of autocorrelation and cross-correlation, stationarity, and the given autoregressive model, perform the following:\n\n- Derive the scalar input autocorrelation matrix $R$ (which is $1 \\times 1$ for $M=1$) in closed form in terms of $a$ and $\\sigma_{w}^{2}$.\n- Solve the Wiener–Hopf normal equation for $M=1$ to obtain the optimal coefficient $h$ explicitly in closed form in terms of $a$ and $\\sigma_{w}^{2}$.\n\nReport your final answer as a single row matrix whose two entries are, in order, the scalar $R$ and the optimal scalar $h$. No numerical approximation is required; provide exact closed-form expressions.", "solution": "The objective is to find the optimal scalar coefficient $h$ for the one-step linear predictor $\\hat{x}[n] = h\\,x[n-1]$ that minimizes the mean-square error (MSE), defined as $J(h) = \\mathbb{E}\\{(x[n]-\\hat{x}[n])^{2}\\}$. The process $x[n]$ is a first-order autoregressive, or AR($1$), process given by $x[n] = a\\,x[n-1] + w[n]$, where $|a|  1$, and $w[n]$ is zero-mean white noise with variance $\\sigma_{w}^{2}$, uncorrelated with past values of the process $x[k]$ for $k \\le n-1$.\n\nThe optimal filter coefficients are found by solving the Wiener-Hopf normal equations, which take the form $R\\mathbf{h} = \\mathbf{p}$. For the specified problem, we have a single-tap predictor, so the order is $M=1$. Consequently, the filter coefficient vector $\\mathbf{h}$ is a scalar $h$, the input autocorrelation matrix $R$ is a $1 \\times 1$ matrix (a scalar), and the cross-correlation vector $\\mathbf{p}$ is also a scalar $p$.\n\nThe scalar Wiener-Hopf equation is thus $Rh=p$.\n\nFirst, we must derive the expression for the scalar autocorrelation matrix $R$. For a predictor of order $M=1$ with input $x[n-1]$, $R$ is defined by:\n$$R = \\mathbb{E}\\{x[n-1]x[n-1]\\} = \\mathbb{E}\\{x[n-1]^2\\}$$\nSince the process $x[n]$ is given as wide-sense stationary (WSS), its autocorrelation function $r_{xx}[k] = \\mathbb{E}\\{x[n]x[n-k]\\}$ is independent of $n$ and depends only on the lag $k$. The variance of the process is constant, so $\\mathbb{E}\\{x[n-1]^2\\} = \\mathbb{E}\\{x[n]^2\\} = r_{xx}[0]$. Let us find $r_{xx}[0]$.\nStarting from the process definition:\n$$x[n] = a\\,x[n-1] + w[n]$$\nWe compute the variance:\n$$r_{xx}[0] = \\mathbb{E}\\{x[n]^2\\} = \\mathbb{E}\\{[a\\,x[n-1] + w[n]]^2\\}$$\n$$r_{xx}[0] = \\mathbb{E}\\{a^2 x[n-1]^2 + 2a\\,x[n-1]w[n] + w[n]^2\\}$$\nUsing the linearity of the expectation operator:\n$$r_{xx}[0] = a^2 \\mathbb{E}\\{x[n-1]^2\\} + 2a\\,\\mathbb{E}\\{x[n-1]w[n]\\} + \\mathbb{E}\\{w[n]^2\\}$$\nWe now use the given properties:\n$1$. Stationarity implies $\\mathbb{E}\\{x[n-1]^2\\} = r_{xx}[0]$.\n$2$. The noise variance is $\\mathbb{E}\\{w[n]^2\\} = \\sigma_w^2$.\n$3$. The noise $w[n]$ is uncorrelated with $x[k]$ for $k \\le n-1$, which means $\\mathbb{E}\\{x[n-1]w[n]\\} = 0$.\n\nSubstituting these into the equation for $r_{xx}[0]$:\n$$r_{xx}[0] = a^2 r_{xx}[0] + 2a(0) + \\sigma_w^2$$\n$$r_{xx}[0](1 - a^2) = \\sigma_w^2$$\nSince $|a|  1$, we know $1-a^2 \\neq 0$, so we can solve for $r_{xx}[0]$:\n$$r_{xx}[0] = \\frac{\\sigma_w^2}{1 - a^2}$$\nThis is the expression for the scalar autocorrelation matrix $R$.\n$$R = r_{xx}[0] = \\frac{\\sigma_w^2}{1 - a^2}$$\n\nNext, we derive the expression for the scalar cross-correlation $p$. This is defined as the correlation between the desired signal $x[n]$ and the input to the filter, which is $x[n-1]$.\n$$p = \\mathbb{E}\\{x[n]x[n-1]\\}$$\nBy definition of the autocorrelation function for a WSS process, this is $r_{xx}[1]$. We compute this value:\n$$p = r_{xx}[1] = \\mathbb{E}\\{x[n]x[n-1]\\} = \\mathbb{E}\\{[a\\,x[n-1] + w[n]]x[n-1]\\}$$\n$$p = \\mathbb{E}\\{a\\,x[n-1]^2 + w[n]x[n-1]\\}$$\nBy linearity of expectation:\n$$p = a\\,\\mathbb{E}\\{x[n-1]^2\\} + \\mathbb{E}\\{w[n]x[n-1]\\}$$\nUsing the same properties as before: $\\mathbb{E}\\{x[n-1]^2\\} = r_{xx}[0]$ and $\\mathbb{E}\\{w[n]x[n-1]\\} = 0$.\n$$p = a\\,r_{xx}[0] + 0 = a\\,r_{xx}[0]$$\nSubstituting the expression for $r_{xx}[0]$:\n$$p = a \\left(\\frac{\\sigma_w^2}{1 - a^2}\\right)$$\n\nFinally, we solve the scalar Wiener-Hopf equation $Rh = p$ for the optimal coefficient $h$:\n$$\\left(\\frac{\\sigma_w^2}{1 - a^2}\\right) h = a \\left(\\frac{\\sigma_w^2}{1 - a^2}\\right)$$\nAssuming a non-trivial process where $\\sigma_w^2 > 0$, we can divide both sides by the non-zero quantity $\\frac{\\sigma_w^2}{1 - a^2}$. This yields:\n$$h = a$$\nThis result is correct and intuitive. The best linear predictor for an AR($1$) process, based on the previous sample, uses the known autoregressive coefficient.\n\nThe problem requires reporting the scalar $R$ and the optimal scalar $h$ as a single row matrix.\nThe first entry is $R = \\frac{\\sigma_{w}^{2}}{1 - a^{2}}$.\nThe second entry is $h = a$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{w}^{2}}{1 - a^{2}}  a\n\\end{pmatrix}\n}\n$$", "id": "2888998"}, {"introduction": "Building on the scalar case, we now advance to a multi-tap filter, which requires a matrix formulation of the normal equations. This problem [@problem_id:2888953] asks you to design a two-tap optimal filter for a system with a specified, exponentially decaying correlation structure, a common model for physical processes. Solving this system will provide essential practice with constructing the Toeplitz autocorrelation matrix $R$ and the cross-correlation vector $\\mathbf{p}$, and the elegant result offers insight into how the filter structure adapts to the underlying signal statistics.", "problem": "Consider two jointly wide-sense stationary, zero-mean, real-valued discrete-time random processes $x[n]$ and $d[n]$. You are to form the optimal linear mean-square estimate of $d[n]$ from the two most recent samples of $x[n]$, namely\n$$\n\\hat{d}[n] = h_{0}\\,x[n] + h_{1}\\,x[n-1],\n$$\nwhere $h_{0}$ and $h_{1}$ are real filter coefficients to be determined. Let $M=2$ denote the number of taps in the finite impulse response filter. The input autocorrelation and the desired–input cross-correlation are specified by\n$$\nR_{xx}[\\tau] = \\alpha^{|\\tau|}, \\quad R_{dx}[\\tau] = \\beta\\,\\alpha^{|\\tau|},\n$$\nfor all integer lags $\\tau$, where $\\alpha \\in (-1,1)$ and $\\beta \\in \\mathbb{R}$. Assume that the normal equations arising from the orthogonality principle are well-posed under these conditions.\n\nStarting from the definitions of mean-square error and the orthogonality principle for optimal linear estimation, derive the system of equations that determines the optimal coefficients in terms of the autocorrelation and cross-correlation functions. Then, construct the $2 \\times 2$ autocorrelation matrix $R$ of the input vector $[x[n],\\,x[n-1]]^{\\top}$, the $2 \\times 1$ cross-correlation vector $p$ between $[x[n],\\,x[n-1]]^{\\top}$ and $d[n]$, using the given $R_{xx}[\\tau]$ and $R_{dx}[\\tau]$, and solve the resulting linear system to obtain the optimal coefficient vector $h = [h_{0},\\,h_{1}]^{\\top}$.\n\nExpress your final answer as a single row vector $\\begin{pmatrix} h_{0}  h_{1} \\end{pmatrix}$. No numerical approximation or rounding is required.", "solution": "The objective is to find the optimal filter coefficients $h_0$ and $h_1$ that minimize the mean-square error (MSE), defined as $J$.\n$$\nJ = E[(d[n] - \\hat{d}[n])^2]\n$$\nSubstituting the expression for the estimate $\\hat{d}[n]$:\n$$\nJ(h_0, h_1) = E\\left[\\left(d[n] - \\left(h_0 x[n] + h_1 x[n-1]\\right)\\right)^2\\right]\n$$\nAccording to the orthogonality principle, the MSE $J$ is minimized if and only if the error signal $e[n] = d[n] - \\hat{d}[n]$ is orthogonal to each of the input samples used to form the estimate. This gives a system of two linear equations:\n$$\nE[e[n]\\,x[n]] = 0\n$$\n$$\nE[e[n]\\,x[n-1]] = 0\n$$\nSubstituting the expression for $e[n]$ into these two equations yields:\n$$\nE[(d[n] - h_0 x[n] - h_1 x[n-1]) x[n]] = 0\n$$\n$$\nE[(d[n] - h_0 x[n] - h_1 x[n-1]) x[n-1]] = 0\n$$\nBy the linearity of the expectation operator, we expand these equations:\n$$\nE[d[n]x[n]] - h_0 E[x[n]x[n]] - h_1 E[x[n-1]x[n]] = 0\n$$\n$$\nE[d[n]x[n-1]] - h_0 E[x[n]x[n-1]] - h_1 E[x[n-1]x[n-1]] = 0\n$$\nThese equations can be expressed in terms of the given autocorrelation and cross-correlation functions. We use the definitions $R_{dx}[\\tau] = E[d[n]x[n-\\tau]]$ and $R_{xx}[\\tau] = E[x[n]x[n-\\tau]]$. Note that for real WSS processes, $R_{xx}[\\tau] = R_{xx}[-\\tau]$.\nThe system becomes:\n$$\nR_{dx}[0] - h_0 R_{xx}[0] - h_1 R_{xx}[-1] = 0\n$$\n$$\nR_{dx}[1] - h_0 R_{xx}[1] - h_1 R_{xx}[0] = 0\n$$\nRearranging to place the unknown coefficients $h_0$ and $h_1$ on one side, we obtain the normal equations, also known as the Wiener-Hopf equations for the discrete-time case:\n$$\nh_0 R_{xx}[0] + h_1 R_{xx}[1] = R_{dx}[0]\n$$\n$$\nh_0 R_{xx}[1] + h_1 R_{xx}[0] = R_{dx}[1]\n$$\nThis system of linear equations can be written in matrix form as $\\mathbf{R} \\mathbf{h} = \\mathbf{p}$, where $\\mathbf{h} = [h_0, h_1]^{\\top}$ is the vector of optimal coefficients, $\\mathbf{R}$ is the $2 \\times 2$ autocorrelation matrix of the input vector $[x[n], x[n-1]]^{\\top}$, and $\\mathbf{p}$ is the $2 \\times 1$ cross-correlation vector between the desired signal $d[n]$ and the input vector.\nThe elements of $\\mathbf{R}$ are given by $R_{ij} = E[x[n-i]x[n-j]] = R_{xx}[j-i]$ for $i,j \\in \\{0, 1\\}$.\n$$\n\\mathbf{R} = \\begin{pmatrix} R_{xx}[0]  R_{xx}[1] \\\\ R_{xx}[-1]  R_{xx}[0] \\end{pmatrix} = \\begin{pmatrix} R_{xx}[0]  R_{xx}[1] \\\\ R_{xx}[1]  R_{xx}[0] \\end{pmatrix}\n$$\nThe elements of $\\mathbf{p}$ are given by $p_i = E[d[n]x[n-i]] = R_{dx}[i]$ for $i \\in \\{0, 1\\}$.\n$$\n\\mathbf{p} = \\begin{pmatrix} R_{dx}[0] \\\\ R_{dx}[1] \\end{pmatrix}\n$$\nNow, we substitute the specified correlation functions $R_{xx}[\\tau] = \\alpha^{|\\tau|}$ and $R_{dx}[\\tau] = \\beta\\alpha^{|\\tau|}$:\n$$\n\\mathbf{R} = \\begin{pmatrix} \\alpha^{|0|}  \\alpha^{|1|} \\\\ \\alpha^{|1|}  \\alpha^{|0|} \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha \\\\ \\alpha  1 \\end{pmatrix}\n$$\n$$\n\\mathbf{p} = \\begin{pmatrix} \\beta\\alpha^{|0|} \\\\ \\beta\\alpha^{|1|} \\end{pmatrix} = \\begin{pmatrix} \\beta \\\\ \\beta\\alpha \\end{pmatrix}\n$$\nThe linear system to be solved is:\n$$\n\\begin{pmatrix} 1  \\alpha \\\\ \\alpha  1 \\end{pmatrix} \\begin{pmatrix} h_0 \\\\ h_1 \\end{pmatrix} = \\begin{pmatrix} \\beta \\\\ \\beta\\alpha \\end{pmatrix}\n$$\nThe solution for the coefficient vector $\\mathbf{h}$ is given by $\\mathbf{h} = \\mathbf{R}^{-1} \\mathbf{p}$. First, we compute the inverse of the matrix $\\mathbf{R}$. The determinant of $\\mathbf{R}$ is $\\det(\\mathbf{R}) = (1)(1) - (\\alpha)(\\alpha) = 1 - \\alpha^2$.\n$$\n\\mathbf{R}^{-1} = \\frac{1}{\\det(\\mathbf{R})} \\begin{pmatrix} 1  -\\alpha \\\\ -\\alpha  1 \\end{pmatrix} = \\frac{1}{1-\\alpha^2} \\begin{pmatrix} 1  -\\alpha \\\\ -\\alpha  1 \\end{pmatrix}\n$$\nNow, we multiply $\\mathbf{R}^{-1}$ by $\\mathbf{p}$ to find $\\mathbf{h}$:\n$$\n\\begin{pmatrix} h_0 \\\\ h_1 \\end{pmatrix} = \\frac{1}{1-\\alpha^2} \\begin{pmatrix} 1  -\\alpha \\\\ -\\alpha  1 \\end{pmatrix} \\begin{pmatrix} \\beta \\\\ \\beta\\alpha \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} h_0 \\\\ h_1 \\end{pmatrix} = \\frac{\\beta}{1-\\alpha^2} \\begin{pmatrix} 1  -\\alpha \\\\ -\\alpha  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\alpha \\end{pmatrix}\n$$\nWe perform the matrix-vector multiplication:\n$$\n\\begin{pmatrix} h_0 \\\\ h_1 \\end{pmatrix} = \\frac{\\beta}{1-\\alpha^2} \\begin{pmatrix} (1)(1) + (-\\alpha)(\\alpha) \\\\ (-\\alpha)(1) + (1)(\\alpha) \\end{pmatrix} = \\frac{\\beta}{1-\\alpha^2} \\begin{pmatrix} 1-\\alpha^2 \\\\ -\\alpha + \\alpha \\end{pmatrix} = \\frac{\\beta}{1-\\alpha^2} \\begin{pmatrix} 1-\\alpha^2 \\\\ 0 \\end{pmatrix}\n$$\nThis simplifies to:\n$$\n\\begin{pmatrix} h_0 \\\\ h_1 \\end{pmatrix} = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix}\n$$\nThus, the optimal filter coefficients are $h_0 = \\beta$ and $h_1 = 0$. The resulting optimal estimate is $\\hat{d}[n] = \\beta x[n]$. This result is a consequence of the specific relationship $R_{dx}[\\tau] = \\beta R_{xx}[\\tau]$, which implies that the component of $d[n]$ that is correlated with the process $x[n]$ is simply a scaled version, $\\beta x[n]$. Therefore, the optimal linear filter logically reduces to a simple scaling of the most recent input sample $x[n]$, with no contribution from past samples such as $x[n-1]$.\n\nThe final answer is required as a row vector $\\begin{pmatrix} h_0  h_1 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\beta  0 \\end{pmatrix}}\n$$", "id": "2888953"}, {"introduction": "Ideal theoretical models often assume perfect knowledge and numerically stable calculations, but real-world implementations must confront noise and ill-conditioning. This practice [@problem_id:2888981] introduces diagonal loading, a form of Tikhonov regularization, as a powerful technique to improve the numerical robustness of the Wiener filter solution. You will not only solve for the regularized filter coefficients but also derive an explicit expression for the resulting Mean-Squared Error (MSE), directly quantifying the bias introduced to achieve a more stable estimate and illuminating the fundamental bias-variance trade-off.", "problem": "Consider a zero-mean, wide-sense stationary (WSS) scalar process $x[n]$ generated by the first-order autoregressive model $x[n] = a\\,x[n-1] + u[n]$, with $|a|1$ and $u[n]$ a zero-mean, white process independent of $x[n-1]$. Assume $x[n]$ has been normalized so that $\\mathbb{E}\\{x[n]^2\\} = 1$. For this model, the autocorrelation is $r_{k} = \\mathbb{E}\\{x[n]x[n-k]\\} = a^{|k|}$.\n\nYou observe the two-dimensional regressor $\\mathbf{x}[n] = [x[n],\\,x[n-1]]^{\\top}$ and wish to linearly estimate the desired signal $d[n] = x[n-1]$ using a two-tap filter $\\mathbf{w} \\in \\mathbb{R}^{2}$ that minimizes the mean square error (MSE), where the MSE is defined as $J(\\mathbf{w}) = \\mathbb{E}\\{(d[n] - \\mathbf{w}^{\\top}\\mathbf{x}[n])^{2}\\}$. Starting from the orthogonality principle and the definitions of the autocorrelation matrix and cross-correlation vector, the unregularized normal equations can be formed using the Toeplitz matrix structure induced by $r_{k}$.\n\nTo improve numerical conditioning, you apply diagonal loading (a form of preconditioning) with a loading parameter $\\lambda>0$, yielding the modified system $(\\mathbf{R} + \\lambda \\mathbf{I}) \\mathbf{w}_{\\lambda} = \\mathbf{p}$, where $\\mathbf{R}$ is the $2\\times 2$ Toeplitz autocorrelation matrix of $\\mathbf{x}[n]$ and $\\mathbf{p}$ is the $2\\times 1$ cross-correlation vector between $\\mathbf{x}[n]$ and $d[n]$.\n\nTasks:\n1. Using only the properties of WSS processes and the orthogonality principle, form $\\mathbf{R}$ and $\\mathbf{p}$ symbolically in terms of $a$, and write down the diagonally loaded system $(\\mathbf{R} + \\lambda \\mathbf{I}) \\mathbf{w}_{\\lambda} = \\mathbf{p}$ for $\\lambda>0$.\n2. Solve explicitly for $\\mathbf{w}_{\\lambda}$ as a function of $a$ and $\\lambda$.\n3. Starting from the definition $J(\\mathbf{w}) = \\mathbb{E}\\{(d[n] - \\mathbf{w}^{\\top}\\mathbf{x}[n])^{2}\\}$, and without invoking any shortcut formulas, derive a closed-form expression for the diagonally loaded MSE $J(\\lambda) \\triangleq J(\\mathbf{w}_{\\lambda})$ in terms of $a$ and $\\lambda$. Simplify your result to a single rational expression in $a$ and $\\lambda$.\n\nExplain briefly, based on your expression, how diagonal loading modifies the Toeplitz system and affects the optimality and MSE. Your final answer must be the single simplified analytic expression for $J(\\lambda)$ in terms of $a$ and $\\lambda$. No numerical rounding is required and no units are needed.", "solution": "The problem asks for three tasks to be completed, followed by a brief explanation.\n\nTask 1: Formation of the diagonally loaded system.\n\nThe autocorrelation matrix $\\mathbf{R}$ is defined as $\\mathbf{R} = \\mathbb{E}\\{\\mathbf{x}[n]\\mathbf{x}[n]^{\\top}\\}$. The regressor vector is $\\mathbf{x}[n] = \\begin{pmatrix} x[n] \\\\ x[n-1] \\end{pmatrix}$.\n$$\n\\mathbf{R} = \\mathbb{E}\\left\\{ \\begin{pmatrix} x[n] \\\\ x[n-1] \\end{pmatrix} \\begin{pmatrix} x[n]  x[n-1] \\end{pmatrix} \\right\\} = \\mathbb{E}\\left\\{ \\begin{pmatrix} x[n]^2  x[n]x[n-1] \\\\ x[n-1]x[n]  x[n-1]^2 \\end{pmatrix} \\right\\}\n$$\nBy taking the expectation of each element and using the wide-sense stationary (WSS) property and the given autocorrelation function $r_{k} = \\mathbb{E}\\{x[m]x[m-k]\\} = a^{|k|}$, with the normalization $r_0 = \\mathbb{E}\\{x[n]^2\\}=1$:\n$$\n\\mathbf{R} = \\begin{pmatrix} \\mathbb{E}\\{x[n]^2\\}  \\mathbb{E}\\{x[n]x[n-1]\\} \\\\ \\mathbb{E}\\{x[n-1]x[n]\\}  \\mathbb{E}\\{x[n-1]^2\\} \\end{pmatrix} = \\begin{pmatrix} r_0  r_1 \\\\ r_1  r_0 \\end{pmatrix} = \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix}\n$$\nThe cross-correlation vector $\\mathbf{p}$ is defined as $\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}[n]d[n]\\}$, where the desired signal is $d[n] = x[n-1]$.\n$$\n\\mathbf{p} = \\mathbb{E}\\left\\{ \\begin{pmatrix} x[n] \\\\ x[n-1] \\end{pmatrix} x[n-1] \\right\\} = \\begin{pmatrix} \\mathbb{E}\\{x[n]x[n-1]\\} \\\\ \\mathbb{E}\\{x[n-1]^2\\} \\end{pmatrix} = \\begin{pmatrix} r_1 \\\\ r_0 \\end{pmatrix} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\nThe diagonally loaded system $(\\mathbf{R} + \\lambda \\mathbf{I}) \\mathbf{w}_{\\lambda} = \\mathbf{p}$ is therefore:\n$$\n\\left( \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) \\mathbf{w}_{\\lambda} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1+\\lambda  a \\\\ a  1+\\lambda \\end{pmatrix} \\mathbf{w}_{\\lambda} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\n\nTask 2: Solve for $\\mathbf{w}_{\\lambda}$.\n\nTo find $\\mathbf{w}_{\\lambda}$, we must invert the matrix $(\\mathbf{R}+\\lambda \\mathbf{I})$. Let this matrix be $\\mathbf{A} = \\begin{pmatrix} 1+\\lambda  a \\\\ a  1+\\lambda \\end{pmatrix}$.\nThe determinant is $\\det(\\mathbf{A}) = (1+\\lambda)^2 - a^2$. The inverse is $\\mathbf{A}^{-1} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} 1+\\lambda  -a \\\\ -a  1+\\lambda \\end{pmatrix}$.\nThe solution is $\\mathbf{w}_{\\lambda} = \\mathbf{A}^{-1}\\mathbf{p}$:\n$$\n\\mathbf{w}_{\\lambda} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} 1+\\lambda  -a \\\\ -a  1+\\lambda \\end{pmatrix} \\begin{pmatrix} a \\\\ 1 \\end{pmatrix} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} a(1+\\lambda) - a \\\\ -a^2 + (1+\\lambda) \\end{pmatrix}\n$$\n$$\n\\mathbf{w}_{\\lambda} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} a\\lambda \\\\ 1 - a^2 + \\lambda \\end{pmatrix}\n$$\n\nTask 3: Derive the closed-form expression for the MSE $J(\\lambda)$.\n\nThe MSE is given by $J(\\mathbf{w}) = \\mathbb{E}\\{(d[n] - \\mathbf{w}^{\\top}\\mathbf{x}[n])^{2}\\}$. Expanding this yields the general quadratic form:\n$$\nJ(\\mathbf{w}) = \\mathbb{E}\\{d[n]^2\\} - 2\\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}[n]d[n]\\} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}[n]\\mathbf{x}[n]^{\\top}\\}\\mathbf{w} = \\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w}\n$$\nHere, $\\sigma_d^2 = \\mathbb{E}\\{d[n]^2\\} = \\mathbb{E}\\{x[n-1]^2\\} = r_0 = 1$.\nThe optimal unregularized Wiener filter solution, $\\mathbf{w}_o$, minimizes $J(\\mathbf{w})$ and is given by $\\mathbf{R}\\mathbf{w}_o=\\mathbf{p}$. The minimum MSE is $J_{min} = J(\\mathbf{w}_o) = \\sigma_d^2 - \\mathbf{p}^{\\top}\\mathbf{w}_o$.\nIn this problem, the optimal solution is found by setting $\\lambda=0$ in the expression for $\\mathbf{w}_{\\lambda}$:\n$$\n\\mathbf{w}_o = \\mathbf{w}_{\\lambda=0} = \\frac{1}{1-a^2} \\begin{pmatrix} 0 \\\\ 1-a^2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis is an intuitive result, as estimating $d[n]=x[n-1]$ from $[x[n], x[n-1]]^{\\top}$ is perfectly achieved by selecting the second component. The minimum MSE is:\n$$\nJ_{min} = J(\\mathbf{w}_o) = 1 - \\mathbf{p}^{\\top}\\mathbf{w}_o = 1 - \\begin{pmatrix} a  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1 - 1 = 0\n$$\nThe MSE for the regularized solution $\\mathbf{w}_{\\lambda}$ is $J(\\lambda) = J(\\mathbf{w}_\\lambda)$. This can be expressed as the sum of the minimum MSE and an excess MSE term due to the filter mismatch:\n$J(\\lambda) = J_{min} + (\\mathbf{w}_{\\lambda}-\\mathbf{w}_o)^{\\top}\\mathbf{R}(\\mathbf{w}_{\\lambda}-\\mathbf{w}_o)$. Since $J_{min}=0$:\n$$\nJ(\\lambda) = (\\mathbf{w}_{\\lambda}-\\mathbf{w}_o)^{\\top}\\mathbf{R}(\\mathbf{w}_{\\lambda}-\\mathbf{w}_o)\n$$\nFirst, we find the filter mismatch vector $\\mathbf{w}_{\\lambda}-\\mathbf{w}_o$:\n$$\n\\mathbf{w}_{\\lambda}-\\mathbf{w}_o = \\frac{1}{(1+\\lambda)^2-a^2} \\begin{pmatrix} a\\lambda \\\\ 1 - a^2 + \\lambda \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{a\\lambda}{(1+\\lambda)^2-a^2} \\\\ \\frac{1-a^2+\\lambda - ((1+\\lambda)^2-a^2)}{(1+\\lambda)^2-a^2} \\end{pmatrix}\n$$\nThe second component simplifies to $\\frac{1-a^2+\\lambda - (1+2\\lambda+\\lambda^2-a^2)}{(1+\\lambda)^2-a^2} = \\frac{-\\lambda-\\lambda^2}{(1+\\lambda)^2-a^2} = \\frac{-\\lambda(1+\\lambda)}{(1+\\lambda)^2-a^2}$.\nThus, the mismatch vector is:\n$$\n\\mathbf{w}_{\\lambda}-\\mathbf{w}_o = \\frac{\\lambda}{(1+\\lambda)^2-a^2} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\nNow we compute the quadratic form. Let $D = (1+\\lambda)^2-a^2$.\n$$\nJ(\\lambda) = \\left(\\frac{\\lambda}{D}\\right)^2 \\begin{pmatrix} a  -(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\nThe matrix product is:\n$$\n\\begin{pmatrix} a  -(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix} = \\begin{pmatrix} a-a(1+\\lambda)  a^2-(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -a\\lambda  a^2-1-\\lambda \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix} = -a^2\\lambda - (a^2-1-\\lambda)(1+\\lambda)\n$$\n$$\n= -a^2\\lambda - (a^2(1+\\lambda) - (1+\\lambda)^2) = -a^2\\lambda - a^2 - a^2\\lambda + (1+\\lambda)^2\n$$\n$$\n= (1+\\lambda)^2 - a^2 - 2a^2\\lambda = D - 2a^2\\lambda\n$$\nSubstituting this back into the expression for $J(\\lambda)$:\n$$\nJ(\\lambda) = \\left(\\frac{\\lambda}{D}\\right)^2 (D - 2a^2\\lambda) = \\frac{\\lambda^2(D - 2a^2\\lambda)}{D^2}\n$$\nReplacing $D$ with its definition, we arrive at the final simplified rational expression:\n$$\nJ(\\lambda) = \\frac{\\lambda^2((1+\\lambda)^2 - a^2 - 2a^2\\lambda)}{((1+\\lambda)^2 - a^2)^2}\n$$\nThe numerator can be rewritten as $\\lambda^2((1+\\lambda)^2 - a^2(1+2\\lambda))$.\n\nExplanation of effects:\nDiagonal loading modifies the normal equations by adding a positive constant $\\lambda$ to the diagonal elements of the autocorrelation matrix $\\mathbf{R}$. This operation, $\\mathbf{R} \\to \\mathbf{R}+\\lambda \\mathbf{I}$, increases the eigenvalues of the matrix by $\\lambda$. If $\\mathbf{R}$ is ill-conditioned (i.e., has a large condition number, which occurs here as $|a| \\to 1$), this shift makes the matrix $\\mathbf{R}+\\lambda \\mathbf{I}$ better conditioned, improving the numerical stability of the solution for $\\mathbf{w}_{\\lambda}$.\n\nThis stability comes at a cost to optimality. The resulting filter $\\mathbf{w}_{\\lambda}$ is not the minimizer of the original mean square error $J(\\mathbf{w})$, but rather the minimizer of a regularized cost function $J(\\mathbf{w}) + \\lambda\\|\\mathbf{w}\\|^2$. This introduces a bias into the estimate. Consequently, the MSE achieved with the regularized filter, $J(\\lambda)$, is strictly greater than the minimum possible MSE, $J_{min}=0$, for any $\\lambda>0$. As seen from the derived expression, $J(\\lambda) > 0$ for $\\lambda > 0$ and $J(\\lambda) \\to 0$ as $\\lambda \\to 0$. This illustrates the fundamental bias-variance trade-off in regularization: one accepts a higher (biased) MSE in exchange for a more stable solution with a smaller norm, which can improve generalization performance in practice.", "answer": "$$\n\\boxed{\\frac{\\lambda^2 ( (1+\\lambda)^2 - a^2(1+2\\lambda) )}{((1+\\lambda)^2 - a^2)^2}}\n$$", "id": "2888981"}]}