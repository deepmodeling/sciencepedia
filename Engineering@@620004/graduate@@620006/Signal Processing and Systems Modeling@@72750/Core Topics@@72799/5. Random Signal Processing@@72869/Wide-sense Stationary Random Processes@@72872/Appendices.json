{"hands_on_practices": [{"introduction": "The definition of an autocovariance function (ACF) for a wide-sense stationary process imposes strict mathematical constraints that are not always intuitive. While properties like evenness, $R_X[k] = R_X[-k]$, are straightforward consequences of the definition, the condition of positive semidefiniteness is more abstract yet absolutely essential. This practice problem provides a crucial hands-on demonstration of why this condition is not implied by other simple properties, guiding you to show that a plausible-looking sequence fails this fundamental test and thus cannot be a valid ACF [@problem_id:2916976].", "problem": "A real-valued discrete-time sequence $R_X[k]$ is a valid autocovariance function of a real, zero-mean, wide-sense stationary (WSS) process if and only if it satisfies two properties derived from first principles: (i) evenness, $R_X[k]=R_X[-k]$, which follows from the definition $R_X[k]=\\mathbb{E}\\{X[n]\\overline{X[n-k]}\\}$ for a WSS process, and (ii) positive semidefiniteness, meaning that for every integer $N\\geq 1$ and every vector $\\mathbf{a}\\in\\mathbb{C}^{N}$, the quadratic form $\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1}a_{i}\\overline{a_{j}}\\,R_X[i-j]\\geq 0$, equivalently, every finite Toeplitz covariance matrix constructed from $R_X[k]$ is positive semidefinite. Although absolute summability, $\\sum_{k\\in\\mathbb{Z}}|R_X[k]|<\\infty$, is often convenient for analytic purposes, it is not a substitute for positive semidefiniteness.\n\nStarting from these foundations, construct explicitly an even, absolutely summable sequence $R_X[k]$ that nonetheless fails the positive semidefiniteness requirement. Then, using the $3\\times 3$ Toeplitz matrix\n$$\nR_{3}=\\begin{pmatrix}\nR_X[0] & R_X[1] & R_X[2]\\\\\nR_X[1] & R_X[0] & R_X[1]\\\\\nR_X[2] & R_X[1] & R_X[0]\n\\end{pmatrix},\n$$\ndemonstrate the violation by showing that $\\det(R_{3})<0$.\n\nTo make the task concrete and ensure an unambiguous numerical outcome, consider the specific construction\n$$\nR_X[0]=1,\\quad R_X[1]=R_X[-1]=2,\\quad R_X[k]=0\\ \\text{for all}\\ |k|\\geq 2.\n$$\nVerify the evenness and absolute summability of this $R_X[k]$, form $R_{3}$, and compute $\\det(R_{3})$ explicitly to demonstrate the failure of positive semidefiniteness.\n\nProvide as your final answer the single real number equal to $\\det(R_{3})$ as computed from this $R_X[k]$. No rounding is required, and no units are involved. The answer must be a single number.", "solution": "The problem presented is a valid and fundamental exercise in the study of wide-sense stationary (WSS) random processes. It aims to demonstrate that the property of positive semidefiniteness is a necessary condition for a sequence to be a valid autocovariance function, and that other intuitive properties such as evenness and absolute summability are not sufficient. The problem provides a specific candidate sequence and requires a rigorous verification of its failure to meet the positive semidefiniteness criterion. I will proceed with the analysis as specified.\n\nThe candidate sequence is defined as:\n$$\nR_X[0]=1,\\quad R_X[1]=R_X[-1]=2,\\quad R_X[k]=0\\ \\text{for all}\\ |k|\\geq 2.\n$$\nFirst, we verify its stated properties: evenness and absolute summability.\n\n$1$. Evenness: A sequence $R_X[k]$ is even if $R_X[k] = R_X[-k]$ for all integers $k$.\nFor $k=0$, the condition is trivially satisfied, as $R_X[0] = R_X[-0] = 1$.\nFor $k=1$, we are given $R_X[1] = 2$ and $R_X[-1] = 2$. Thus, $R_X[1] = R_X[-1]$.\nFor any integer $k$ such that $|k| \\geq 2$, we have $R_X[k] = 0$. Since $|-k| = |k|$, it follows that $R_X[-k] = 0$ as well. So, $R_X[k] = R_X[-k] = 0$ for $|k| \\geq 2$.\nThe condition $R_X[k] = R_X[-k]$ holds for all $k \\in \\mathbb{Z}$. The sequence is indeed even.\n\n$2$. Absolute Summability: A sequence $R_X[k]$ is absolutely summable if the sum $\\sum_{k=-\\infty}^{\\infty} |R_X[k]|$ converges to a finite value.\nFor the given sequence, the only non-zero terms are for $k \\in \\{-1, 0, 1\\}$. The sum is:\n$$\n\\sum_{k=-\\infty}^{\\infty} |R_X[k]| = |R_X[-1]| + |R_X[0]| + |R_X[1]| = |2| + |1| + |2| = 2 + 1 + 2 = 5.\n$$\nSince the sum is $5$, a finite number, the sequence is absolutely summable.\n\nNow, we investigate the core property of positive semidefiniteness. A sequence $R_X[k]$ is positive semidefinite if and only if for any positive integer $N$, the $N \\times N$ Toeplitz matrix $R_N$, whose entries are given by $(R_N)_{ij} = R_X[i-j]$, is a positive semidefinite matrix. A necessary condition for a matrix to be positive semidefinite is that all of its principal minors are non-negative. This includes the determinant of the matrix itself. If we can find any $N$ for which the determinant of $R_N$ is negative, we have proven that the sequence is not positive semidefinite and thus cannot be a valid autocovariance function.\n\nThe problem directs us to examine the case $N=3$. The corresponding $3 \\times 3$ Toeplitz matrix, for a real and even sequence, is:\n$$\nR_{3}=\\begin{pmatrix}\nR_X[0] & R_X[1] & R_X[2]\\\\\nR_X[1] & R_X[0] & R_X[1]\\\\\nR_X[2] & R_X[1] & R_X[0]\n\\end{pmatrix}.\n$$\nSubstituting the given values $R_X[0]=1$, $R_X[1]=2$, and $R_X[2]=0$ into this matrix structure, we obtain:\n$$\nR_{3}=\\begin{pmatrix}\n1 & 2 & 0\\\\\n2 & 1 & 2\\\\\n0 & 2 & 1\n\\end{pmatrix}.\n$$\nThis is a real, symmetric matrix. For such a matrix to be positive semidefinite, its determinant must be non-negative. We compute the determinant:\n$$\n\\det(R_{3}) = 1 \\cdot \\det\\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 2 & 2 \\\\ 0 & 1 \\end{pmatrix} + 0 \\cdot \\det\\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix}\n$$\n$$\n\\det(R_{3}) = 1 \\cdot ((1)(1) - (2)(2)) - 2 \\cdot ((2)(1) - (2)(0)) + 0\n$$\n$$\n\\det(R_{3}) = 1 \\cdot (1 - 4) - 2 \\cdot (2 - 0)\n$$\n$$\n\\det(R_{3}) = 1(-3) - 2(2)\n$$\n$$\n\\det(R_{3}) = -3 - 4 = -7.\n$$\nThe determinant of the matrix $R_3$ is $-7$. Since $\\det(R_3) < 0$, the matrix $R_3$ is not positive semidefinite. This violates a necessary condition for $R_X[k]$ to be a valid autocovariance function. The fact that the sequence is even and absolutely summable is irrelevant in the face of this failure. The sequence corresponds to a Discrete-Time Fourier Transform (the power spectral density) of $S_X(\\omega) = \\sum_{k} R_X[k] \\exp(-j\\omega k) = 1 + 2\\exp(-j\\omega) + 2\\exp(j\\omega) = 1 + 4\\cos(\\omega)$. This function takes on negative values, for instance at $\\omega = \\pi$, where $S_X(\\pi) = 1 + 4(-1) = -3$. A power spectral density must be non-negative for all frequencies, which is the frequency-domain equivalent of the time-domain positive semidefiniteness condition. The calculation confirms the violation.\n\nThe final answer is the computed value of the determinant.", "answer": "$$\\boxed{-7}$$", "id": "2916976"}, {"introduction": "Having established the critical nature of positive semidefiniteness, we now turn to a powerful method for its verification: Bochner's theorem. This fundamental result connects the time-domain ACF to its frequency-domain counterpart, the power spectral density (PSD), stating that a function is a valid ACF if and only if its corresponding PSD is non-negative at all frequencies. This exercise allows you to apply this theorem directly, calculating the PSD for a candidate continuous-time ACF to determine its validity based on this frequency-domain criterion [@problem_id:2916949].", "problem": "Consider a real-valued, zero-mean, continuous-time wide-sense stationary (WSS) random process with a candidate autocorrelation function (ACF) given by $R_{X}(\\tau)=\\exp(-|\\tau|)-\\tfrac{1}{2}\\exp(-2|\\tau|)$. An autocorrelation function for a WSS process is, by definition, an even function that is positive semidefinite in the sense that for any finite collection of times $\\{t_{k}\\}$ and complex scalars $\\{c_{k}\\}$, one has $\\sum_{i,j}c_{i}\\overline{c_{j}}\\,R_{X}(t_{i}-t_{j})\\geq 0$. A classical characterization asserts that $R_{X}(\\tau)$ is a valid ACF if and only if there exists a finite, nonnegative, even spectral measure $\\mu$ on the real line such that $R_{X}(\\tau)=\\int_{\\mathbb{R}}\\exp(j\\omega \\tau)\\,\\mathrm{d}\\mu(\\omega)$. When $R_{X}$ is integrable, the measure is absolutely continuous with respect to Lebesgue measure and admits a nonnegative density $S_{X}(\\omega)$, called the power spectral density (PSD), so that, under the angular-frequency convention without $2\\pi$ factors, $S_{X}(\\omega)=\\int_{-\\infty}^{\\infty}R_{X}(\\tau)\\exp(-j\\omega \\tau)\\,\\mathrm{d}\\tau$ and $R_{X}(\\tau)=(2\\pi)^{-1}\\int_{-\\infty}^{\\infty}S_{X}(\\omega)\\exp(j\\omega \\tau)\\,\\mathrm{d}\\omega$.\n\nStarting from these foundational facts, decide whether the given $R_{X}(\\tau)$ is a valid ACF by constructing a nonnegative spectral density $S_{X}(\\omega)$ corresponding to $R_{X}(\\tau)$, if it exists. Your derivation must start from the definitions above and proceed by first principles. Provide the PSD $S_{X}(\\omega)$ in a single, fully simplified, closed-form analytic expression. No numerical approximation is required, and no rounding instructions apply. Your final answer should consist solely of the analytic expression for $S_{X}(\\omega)$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   A real-valued, zero-mean, continuous-time wide-sense stationary (WSS) random process $X(t)$.\n-   A candidate autocorrelation function (ACF): $R_{X}(\\tau)=\\exp(-|\\tau|)-\\tfrac{1}{2}\\exp(-2|\\tau|)$.\n-   Condition for a valid ACF: The function must be positive semidefinite.\n-   Equivalency condition (Bochner's Theorem): The power spectral density (PSD), $S_{X}(\\omega)$, which is the Fourier transform of the ACF, must be a non-negative function, i.e., $S_{X}(\\omega) \\geq 0$ for all $\\omega \\in \\mathbb{R}$.\n-   Definition of the PSD (forward Fourier transform with angular frequency $\\omega$): $S_{X}(\\omega)=\\int_{-\\infty}^{\\infty}R_{X}(\\tau)\\exp(-j\\omega \\tau)\\,\\mathrm{d}\\tau$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in the theory of stochastic processes: to determine the validity of a candidate autocorrelation function by computing its corresponding power spectral density and checking for non-negativity, a direct application of Bochner's theorem. All definitions and data are provided, and no inconsistencies or ambiguities are present.\n\n**Verdict**\nThe problem is deemed valid. A solution will be constructed.\n\nThe fundamental criterion for a function to be a valid autocorrelation function of a wide-sense stationary process is that its Fourier transform, the power spectral density $S_{X}(\\omega)$, must be non-negative for all frequencies $\\omega$. We are given the candidate ACF:\n$$R_{X}(\\tau) = \\exp(-|\\tau|) - \\frac{1}{2}\\exp(-2|\\tau|)$$\nThe power spectral density $S_{X}(\\omega)$ is obtained by computing the Fourier transform of $R_{X}(\\tau)$:\n$$S_{X}(\\omega) = \\int_{-\\infty}^{\\infty} R_{X}(\\tau) \\exp(-j\\omega\\tau) \\, \\mathrm{d}\\tau$$\nBy the linearity of the Fourier transform, we can write:\n$$S_{X}(\\omega) = \\mathcal{F}\\{\\exp(-|\\tau|)\\}(\\omega) - \\frac{1}{2} \\mathcal{F}\\{\\exp(-2|\\tau|)\\}(\\omega)$$\nWe must first compute the Fourier transform of the general function $f(\\tau) = \\exp(-a|\\tau|)$ for a real constant $a > 0$.\n$$\\mathcal{F}\\{\\exp(-a|\\tau|)\\}(\\omega) = \\int_{-\\infty}^{\\infty} \\exp(-a|\\tau|) \\exp(-j\\omega\\tau) \\, \\mathrm{d}\\tau$$\nWe split the integral based on the definition of the absolute value function $|\\tau|$:\n$$= \\int_{-\\infty}^{0} \\exp(a\\tau) \\exp(-j\\omega\\tau) \\, \\mathrm{d}\\tau + \\int_{0}^{\\infty} \\exp(-a\\tau) \\exp(-j\\omega\\tau) \\, \\mathrm{d}\\tau$$\n$$= \\int_{-\\infty}^{0} \\exp((a - j\\omega)\\tau) \\, \\mathrm{d}\\tau + \\int_{0}^{\\infty} \\exp(-(a + j\\omega)\\tau) \\, \\mathrm{d}\\tau$$\nEvaluating these elementary integrals gives:\n$$= \\left[ \\frac{\\exp((a - j\\omega)\\tau)}{a - j\\omega} \\right]_{-\\infty}^{0} + \\left[ \\frac{\\exp(-(a + j\\omega)\\tau)}{-(a + j\\omega)} \\right]_{0}^{\\infty}$$\n$$= \\left( \\frac{\\exp(0)}{a - j\\omega} - \\lim_{\\tau \\to -\\infty} \\frac{\\exp((a - j\\omega)\\tau)}{a - j\\omega} \\right) + \\left( \\lim_{\\tau \\to \\infty} \\frac{\\exp(-(a + j\\omega)\\tau)}{-(a + j\\omega)} - \\frac{\\exp(0)}{-(a + j\\omega)} \\right)$$\nSince $a > 0$, the limits evaluate to zero.\n$$= \\left( \\frac{1}{a - j\\omega} - 0 \\right) + \\left( 0 - \\frac{1}{-(a + j\\omega)} \\right)$$\n$$= \\frac{1}{a - j\\omega} + \\frac{1}{a + j\\omega}$$\nCombining the terms by finding a common denominator:\n$$= \\frac{(a + j\\omega) + (a - j\\omega)}{(a - j\\omega)(a + j\\omega)} = \\frac{2a}{a^{2} - (j\\omega)^{2}} = \\frac{2a}{a^{2} + \\omega^{2}}$$\nThis is the standard Fourier transform pair for the two-sided exponential function.\n\nNow, we apply this result to the two components of $R_{X}(\\tau)$.\nFor the first term, $\\exp(-|\\tau|)$, we have $a=1$. Its Fourier transform is:\n$$\\mathcal{F}\\{\\exp(-|\\tau|)\\}(\\omega) = \\frac{2(1)}{1^{2} + \\omega^{2}} = \\frac{2}{1 + \\omega^{2}}$$\nFor the second term, $\\exp(-2|\\tau|)$, we have $a=2$. Its Fourier transform is:\n$$\\mathcal{F}\\{\\exp(-2|\\tau|)\\}(\\omega) = \\frac{2(2)}{2^{2} + \\omega^{2}} = \\frac{4}{4 + \\omega^{2}}$$\nSubstituting these transforms back into the expression for $S_{X}(\\omega)$:\n$$S_{X}(\\omega) = \\frac{2}{1 + \\omega^{2}} - \\frac{1}{2} \\left( \\frac{4}{4 + \\omega^{2}} \\right)$$\n$$S_{X}(\\omega) = \\frac{2}{1 + \\omega^{2}} - \\frac{2}{4 + \\omega^{2}}$$\nTo analyze the non-negativity of $S_{X}(\\omega)$, we combine the terms into a single fraction:\n$$S_{X}(\\omega) = 2 \\left( \\frac{1}{1 + \\omega^{2}} - \\frac{1}{4 + \\omega^{2}} \\right) = 2 \\left( \\frac{(4 + \\omega^{2}) - (1 + \\omega^{2})}{(1 + \\omega^{2})(4 + \\omega^{2})} \\right)$$\n$$S_{X}(\\omega) = 2 \\left( \\frac{4 + \\omega^{2} - 1 - \\omega^{2}}{(1 + \\omega^{2})(4 + \\omega^{2})} \\right)$$\n$$S_{X}(\\omega) = 2 \\left( \\frac{3}{(1 + \\omega^{2})(4 + \\omega^{2})} \\right)$$\n$$S_{X}(\\omega) = \\frac{6}{(1 + \\omega^{2})(4 + \\omega^{2})}$$\nThis is the power spectral density corresponding to the given $R_{X}(\\tau)$. We now check its sign. The numerator is the positive constant $6$. The denominator is a product of two terms, $(1 + \\omega^{2})$ and $(4 + \\omega^{2})$. Since $\\omega$ is a real variable, $\\omega^{2} \\geq 0$. Consequently, $(1 + \\omega^{2}) \\geq 1$ and $(4 + \\omega^{2}) \\geq 4$. Both terms in the denominator are strictly positive for all real $\\omega$. The product of two strictly positive numbers is strictly positive. Therefore, the denominator is strictly positive, and $S_{X}(\\omega)$ as a ratio of a positive constant and a strictly positive function is strictly positive for all $\\omega \\in \\mathbb{R}$.\n\nSince $S_{X}(\\omega) > 0$ for all $\\omega$, the non-negativity condition $S_{X}(\\omega) \\geq 0$ is satisfied. We conclude that the given function $R_{X}(\\tau)$ is a valid autocorrelation function. The problem requires the construction of the spectral density $S_{X}(\\omega)$, which has been derived.", "answer": "$$\\boxed{\\frac{6}{(1+\\omega^{2})(4+\\omega^{2})}}$$", "id": "2916949"}, {"introduction": "The theoretical properties of an autocorrelation function are foundational to its practical use in signal processing and forecasting. One of the most important applications is optimal linear prediction, where we use the statistical information encoded in the ACF to estimate future values of a random process. This exercise tasks you with deriving the optimal linear predictor for a given process by applying the orthogonality principle to set up and solve the famous Yule-Walker equations, thereby finding the filter coefficients that minimize the prediction error [@problem_id:2916954].", "problem": "Consider a real-valued, zero-mean, wide-sense stationary (WSS) discrete-time random process $X[n]$ with autocorrelation sequence $R_{X}[k] \\triangleq \\mathbb{E}\\{X[n]X[n-k]\\}$ specified by $R_{X}[0]=1$, $R_{X}[1]=0.7$, $R_{X}[2]=0.49$, and $R_{X}[k]=0$ for all integers $k \\ge 3$. Define the order-$2$ one-step-ahead linear predictor $\\hat{X}[n] \\triangleq a_{1}X[n-1]+a_{2}X[n-2]$, and the corresponding innovation $e[n]\\triangleq X[n]-\\hat{X}[n]$ with variance $\\sigma_{e}^{2}\\triangleq \\mathbb{E}\\{e[n]^{2}\\}$. Using only the orthogonality principle for linear minimum mean-square error estimation and the stationarity properties of $X[n]$, derive the normal equations from first principles and compute the unique predictor coefficients $a_{1}$ and $a_{2}$ and the innovation variance $\\sigma_{e}^{2}$. Express your final answer as a single row matrix in the order $\\left(a_{1},\\,a_{2},\\,\\sigma_{e}^{2}\\right)$. No rounding is required.", "solution": "The problem statement has been subjected to validation and is found to be scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in linear prediction theory for wide-sense stationary random processes. All necessary data are provided, and the given autocorrelation values form a valid positive-definite sequence. We shall proceed with the derivation as requested.\n\nThe objective is to find the coefficients $a_{1}$ and $a_{2}$ of the linear predictor $\\hat{X}[n] = a_{1}X[n-1]+a_{2}X[n-2]$ that minimizes the mean-square error (MSE), $\\mathbb{E}\\{e[n]^{2}\\}$, where $e[n] = X[n] - \\hat{X}[n]$ is the prediction error, also known as the innovation. The process $X[n]$ is a real-valued, zero-mean, wide-sense stationary (WSS) process.\n\nThe solution is derived from first principles using the orthogonality principle for linear minimum mean-square error (LMMSE) estimation. This principle states that for the MSE to be minimized, the error vector $e[n]$ must be orthogonal to every vector in the subspace spanned by the observations used for the prediction. In this case, the observations are $X[n-1]$ and $X[n-2]$. The orthogonality conditions are therefore:\n$$\n\\mathbb{E}\\{e[n]X[n-1]\\} = 0\n$$\n$$\n\\mathbb{E}\\{e[n]X[n-2]\\} = 0\n$$\n\nWe substitute the expression for the error, $e[n] = X[n] - a_{1}X[n-1] - a_{2}X[n-2]$, into these two equations.\n\nFor the first condition:\n$$\n\\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n-1]\\} = 0\n$$\nBy linearity of the expectation operator:\n$$\n\\mathbb{E}\\{X[n]X[n-1]\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n-1]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n-1]\\} = 0\n$$\nUsing the definition of the autocorrelation function, $R_{X}[k] \\triangleq \\mathbb{E}\\{X[m]X[m-k]\\}$, and the WSS property, this becomes:\n$$\nR_{X}[1] - a_{1}R_{X}[0] - a_{2}R_{X}[-1] = 0\n$$\n\nFor the second condition:\n$$\n\\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n-2]\\} = 0\n$$\n$$\n\\mathbb{E}\\{X[n]X[n-2]\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n-2]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n-2]\\} = 0\n$$\nThis translates to:\n$$\nR_{X}[2] - a_{1}R_{X}[1] - a_{2}R_{X}[0] = 0\n$$\n\nFor a real-valued process, the autocorrelation function is an even function, so $R_{X}[-k] = R_{X}[k]$. The two equations, known as the normal equations (or Yule-Walker equations in this context), can be rearranged as:\n$$\na_{1}R_{X}[0] + a_{2}R_{X}[1] = R_{X}[1]\n$$\n$$\na_{1}R_{X}[1] + a_{2}R_{X}[0] = R_{X}[2]\n$$\n\nWe are given the values $R_{X}[0]=1$, $R_{X}[1]=0.7$, and $R_{X}[2]=0.49$. Substituting these into the normal equations:\n$$\na_{1}(1) + a_{2}(0.7) = 0.7\n$$\n$$\na_{1}(0.7) + a_{2}(1) = 0.49\n$$\nThis is a system of two linear equations in two variables. In matrix form:\n$$\n\\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{pmatrix} \\begin{pmatrix} a_{1} \\\\ a_{2} \\end{pmatrix} = \\begin{pmatrix} 0.7 \\\\ 0.49 \\end{pmatrix}\n$$\nFrom the second equation, $a_{2} = 0.49 - 0.7a_{1}$. Substituting this into the first equation:\n$$\na_{1} + 0.7(0.49 - 0.7a_{1}) = 0.7\n$$\n$$\na_{1} + 0.343 - 0.49a_{1} = 0.7\n$$\n$$\n0.51a_{1} = 0.7 - 0.343 = 0.357\n$$\n$$\na_{1} = \\frac{0.357}{0.51} = \\frac{357}{510} = \\frac{7 \\times 51}{10 \\times 51} = \\frac{7}{10} = 0.7\n$$\nNow, we find $a_{2}$:\n$$\na_{2} = 0.49 - 0.7(0.7) = 0.49 - 0.49 = 0\n$$\nThe unique predictor coefficients are $a_{1}=0.7$ and $a_{2}=0$. The optimal order-$2$ predictor degenerates to an order-$1$ predictor due to the specific autocorrelation values provided.\n\nNext, we calculate the innovation variance, $\\sigma_{e}^{2}$, which is the minimum MSE:\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]^{2}\\} = \\mathbb{E}\\{e[n] \\cdot e[n]\\} = \\mathbb{E}\\{e[n](X[n] - \\hat{X}[n])\\}\n$$\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]X[n]\\} - \\mathbb{E}\\{e[n]\\hat{X}[n]\\}\n$$\nThe second term, $\\mathbb{E}\\{e[n]\\hat{X}[n]\\}$, is $\\mathbb{E}\\{e[n](a_{1}X[n-1] + a_{2}X[n-2])\\} = a_{1}\\mathbb{E}\\{e[n]X[n-1]\\} + a_{2}\\mathbb{E}\\{e[n]X[n-2]\\}$. By the orthogonality principle, both expectation terms are zero. Thus, $\\mathbb{E}\\{e[n]\\hat{X}[n]\\} = 0$.\nThe variance simplifies to:\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]X[n]\\} = \\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n]\\}\n$$\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{X[n]^{2}\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n]\\}\n$$\n$$\n\\sigma_{e}^{2} = R_{X}[0] - a_{1}R_{X}[-1] - a_{2}R_{X}[-2] = R_{X}[0] - a_{1}R_{X}[1] - a_{2}R_{X}[2]\n$$\nSubstituting the known values for the autocorrelation function and the computed coefficients:\n$$\n\\sigma_{e}^{2} = 1 - (0.7)(0.7) - (0)(0.49) = 1 - 0.49 = 0.51\n$$\n\nThe required quantities are therefore $a_{1}=0.7$, $a_{2}=0$, and $\\sigma_{e}^{2}=0.51$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.7 & 0 & 0.51 \\end{pmatrix}}$$", "id": "2916954"}]}