{"hands_on_practices": [{"introduction": "A deep understanding of sampling begins with its fundamental effect on a signal's spectrum. This practice guides you through the foundational derivation of how a continuous-time power spectral density, $S_{x}(\\omega)$, transforms into its discrete-time counterpart, $S_{x_{d}}(\\exp(j\\Omega))$. By starting from first principles with the autocorrelation function, you will uncover the mathematical origins of aliasing and prove the critical relationship between the continuous and discrete-time signal variances [@problem_id:2902615].", "problem": "Let $x(t)$ be a zero-mean wide-sense stationary (WSS) continuous-time random process with autocorrelation function $R_{x}(\\tau)$ and power spectral density (PSD) $S_{x}(\\omega)$, defined by the continuous-time Fourier transform (CTFT) relation $S_{x}(\\omega)=\\int_{-\\infty}^{\\infty}R_{x}(\\tau)\\exp(-j\\omega\\tau)\\,d\\tau$. The process is uniformly sampled with sampling period $T>0$ to form the discrete-time sequence $x_{d}[n]=x(nT)$. Let $R_{x_{d}}[m]$ denote the autocorrelation sequence of $x_{d}[n]$ and $S_{x_{d}}(\\exp(j\\Omega))$ denote its discrete-time power spectral density, defined as the discrete-time Fourier transform (DTFT) of $R_{x_{d}}[m]$.\n\nAssume $S_{x}(\\omega)$ is not bandlimited. Work from first principles and fundamental definitions only; do not assume any pre-derived sampling identities.\n\n1) Starting from the definitions of $R_{x_{d}}[m]$ and $S_{x_{d}}(\\exp(j\\Omega))$, express $S_{x_{d}}(\\exp(j\\Omega))$ in terms of $R_{x}(\\tau)$ and then, by relating sums over samples to integrals via the continuous-time Fourier transform, derive an explicit expression of $S_{x_{d}}(\\exp(j\\Omega))$ in terms of $S_{x}(\\omega)$ that makes the aliasing mechanism explicit across $2\\pi$-spaced discrete-time frequency intervals.\n\n2) Using your expression from part 1), derive a condition on $S_{x}(\\omega)$ under which the variance of the sampled sequence, $\\sigma_{x_{d}}^{2}=R_{x_{d}}[0]$, is finite, and show that when this condition holds,\n$$\n\\sigma_{x_{d}}^{2}=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}S_{x}(\\omega)\\,d\\omega.\n$$\nClearly state any integrability assumptions you invoke to justify interchanging sums and integrals.\n\n3) Consider the specific non-bandlimited PSD\n$$\nS_{x}(\\omega)=\\frac{2D}{\\omega^{2}+\\omega_{c}^{2}},\n$$\nwith fixed positive constants $D>0$ and $\\omega_{c}>0$. For sampling period $T>0$, compute the discrete-time variance $\\sigma_{x_{d}}^{2}$ in closed form as a function of $D$ and $\\omega_{c}$. Express your final result as a symbolic expression in $D$ and $\\omega_{c}$ only. Do not include units in your final boxed answer; interpret the variance in the same units as the square of $x(t)$.", "solution": "The problem shall be considered in three parts, as specified. The analysis will proceed from fundamental definitions of correlation functions and power spectral densities for stationary random processes.\n\nPart 1: Derivation of the discrete-time Power Spectral Density\n\nWe are given a continuous-time, zero-mean, wide-sense stationary (WSS) random process $x(t)$ with autocorrelation function $R_x(\\tau) = E[x(t+\\tau)x^*(t)]$. The sampled sequence is $x_d[n] = x(nT)$ for a sampling period $T>0$. The autocorrelation sequence of the discrete-time process $x_d[n]$ is defined as $R_{x_d}[m] = E[x_d[n+m]x_d^*[n]]$.\n\nSubstituting the definition of $x_d[n]$:\n$$R_{x_d}[m] = E[x((n+m)T)x^*(nT)]$$\nSince $x(t)$ is WSS, its autocorrelation depends only on the time difference, which is $(n+m)T - nT = mT$. Therefore,\n$$R_{x_d}[m] = R_x(mT)$$\nThis shows that the autocorrelation sequence of the sampled process is the sampled version of the continuous-time autocorrelation function.\n\nThe power spectral density (PSD) of the discrete-time process, $S_{x_d}(\\exp(j\\Omega))$, is the discrete-time Fourier transform (DTFT) of its autocorrelation sequence $R_{x_d}[m]$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} R_{x_d}[m] \\exp(-j\\Omega m)$$\nSubstituting $R_{x_d}[m] = R_x(mT)$, we get the desired expression in terms of $R_x(\\tau)$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} R_x(mT) \\exp(-j\\Omega m)$$\nTo relate this to the continuous-time PSD, $S_x(\\omega)$, we express $R_x(\\tau)$ using the inverse continuous-time Fourier transform (CTFT) of $S_x(\\omega)$:\n$$R_x(\\tau) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\exp(j\\omega\\tau) d\\omega$$\nSubstituting this expression for $\\tau = mT$ into the sum for $S_{x_d}(\\exp(j\\Omega))$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} \\left[ \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\exp(j\\omega mT) d\\omega \\right] \\exp(-j\\Omega m)$$\nAssuming that we can interchange the order of summation and integration (which can be justified, for instance, by Fubini's theorem if $\\int \\sum |...| < \\infty$), we have:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ \\sum_{m=-\\infty}^{\\infty} \\exp(j\\omega mT - j\\Omega m) \\right] d\\omega$$\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ \\sum_{m=-\\infty}^{\\infty} \\exp(jm(\\omega T - \\Omega)) \\right] d\\omega$$\nThe sum inside the bracket is a representation of the Dirac comb function, a periodic train of impulses. This is a key identity from the theory of distributions or generalized functions, related to the Poisson summation formula:\n$$\\sum_{m=-\\infty}^{\\infty} \\exp(jm\\phi) = 2\\pi \\sum_{k=-\\infty}^{\\infty} \\delta(\\phi - 2\\pi k)$$\nwhere $\\delta(\\cdot)$ is the Dirac delta function. In our expression, $\\phi = \\omega T - \\Omega$. Substituting this identity into the integral:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ 2\\pi \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega T - \\Omega - 2\\pi k) \\right] d\\omega$$\n$$S_{x_d}(\\exp(j\\Omega)) = \\int_{-\\infty}^{\\infty} S_x(\\omega) \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega T - (\\Omega + 2\\pi k)) d\\omega$$\nUsing the scaling property of the Dirac delta function, $\\delta(ax) = \\frac{1}{|a|}\\delta(x)$, with $a=T>0$:\n$$\\delta(\\omega T - (\\Omega + 2\\pi k)) = \\frac{1}{T} \\delta\\left(\\omega - \\frac{\\Omega + 2\\pi k}{T}\\right)$$\nSubstituting this back and interchanging the sum and integral:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\delta\\left(\\omega - \\frac{\\Omega + 2\\pi k}{T}\\right) d\\omega$$\nUsing the sifting property of the delta function, $\\int_{-\\infty}^{\\infty} f(x) \\delta(x-x_0) dx = f(x_0)$, the integral evaluates to $S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right)$. This yields the final expression:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right)$$\nThis formula explicitly shows the aliasing mechanism: the discrete-time PSD at normalized frequency $\\Omega$ is the sum of the continuous-time PSD values at all frequencies $\\omega_k = \\frac{\\Omega}{T} + k\\frac{2\\pi}{T}$ (where $\\frac{2\\pi}{T}$ is the sampling frequency in rad/s), scaled by $\\frac{1}{T}$.\n\nPart 2: Condition for Finite Variance\n\nThe variance of the discrete-time sequence, $\\sigma_{x_d}^2$, is given by its autocorrelation at lag $m=0$: $\\sigma_{x_d}^2 = R_{x_d}[0]$. According to the Wiener-Khinchin theorem for discrete-time processes, this is also given by the integral of the PSD over one period:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_{x_d}(\\exp(j\\Omega)) d\\Omega$$\nUsing the expression for $S_{x_d}(\\exp(j\\Omega))$ derived in Part 1:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\left[ \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) \\right] d\\Omega$$\nWe invoke the assumption that the sum and integral can be interchanged. This is justified by Tonelli's theorem because the PSD $S_x(\\omega)$ is a non-negative function, making the integrand non-negative.\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi T} \\sum_{k=-\\infty}^{\\infty} \\int_{-\\pi}^{\\pi} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) d\\Omega$$\nFor each integral in the sum, we perform a change of variable. Let $u = \\frac{\\Omega + 2\\pi k}{T}$. Then $d\\Omega = T du$. The limits of integration for $\\Omega$ from $-\\pi$ to $\\pi$ correspond to limits for $u$ from $\\frac{-\\pi + 2\\pi k}{T}$ to $\\frac{\\pi + 2\\pi k}{T}$.\n$$\\int_{-\\pi}^{\\pi} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) d\\Omega = \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) (T du) = T \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du$$\nSubstituting this result into the expression for $\\sigma_{x_d}^2$:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi T} \\sum_{k=-\\infty}^{\\infty} T \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du = \\frac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du$$\nThe summation is over a set of contiguous, non-overlapping intervals that tile the entire real line $(-\\infty, \\infty)$. Therefore, the sum of integrals is equivalent to a single integral over the entire real line:\n$$\\sum_{k=-\\infty}^{\\infty} \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du = \\int_{-\\infty}^{\\infty} S_x(u) du$$\nThis leads to the result:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega$$\nFor the variance $\\sigma_{x_d}^2$ to be finite, the integral on the right-hand side must converge. Since a PSD is always non-negative ($S_x(\\omega) \\ge 0$), the condition for finite variance is that $S_x(\\omega)$ must be integrable over $\\mathbb{R}$:\n$$\\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega < \\infty$$\nWe also observe that the variance of the continuous-time process is $\\sigma_x^2 = R_x(0) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega$. Thus, $\\sigma_{x_d}^2 = \\sigma_x^2$, which is an expected result, as sampling does not alter the instantaneous power of the signal.\n\nPart 3: Variance for a Specific PSD\n\nWe are given the PSD $S_x(\\omega) = \\frac{2D}{\\omega^2 + \\omega_c^2}$, with constants $D>0$ and $\\omega_c>0$. We must compute the variance of the sampled sequence, $\\sigma_{x_d}^2 = R_{x_d}[0]$.\nFrom Part 1, we know $R_{x_d}[0] = R_x(0)$. Thus, the variance of the sampled sequence is identical to the variance of the continuous-time process. We can find this by first determining the autocorrelation function $R_x(\\tau)$ via the inverse CTFT of $S_x(\\omega)$.\n\nWe use the standard Fourier transform pair:\n$$\\mathcal{F}\\{\\exp(-a|\\tau|)\\} = \\int_{-\\infty}^{\\infty} \\exp(-a|\\tau|) \\exp(-j\\omega\\tau) d\\tau = \\frac{2a}{\\omega^2 + a^2}$$\nComparing this with the given $S_x(\\omega)$, we can write:\n$$S_x(\\omega) = \\frac{2D}{\\omega^2 + \\omega_c^2} = \\frac{D}{\\omega_c} \\left( \\frac{2\\omega_c}{\\omega^2 + \\omega_c^2} \\right)$$\nBy inspection, we identify $a = \\omega_c$. The autocorrelation function is therefore:\n$$R_x(\\tau) = \\frac{D}{\\omega_c} \\exp(-\\omega_c|\\tau|)$$\nThe variance is found by evaluating $R_x(\\tau)$ at $\\tau=0$:\n$$\\sigma_{x_d}^2 = R_x(0) = \\frac{D}{\\omega_c} \\exp(-\\omega_c \\cdot |0|) = \\frac{D}{\\omega_c}$$\nThis result is independent of the sampling period $T$, as expected.\n\nAlternatively, using the integral expression derived in Part 2:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{2D}{\\omega^2 + \\omega_c^2} d\\omega$$\n$$\\sigma_{x_d}^2 = \\frac{D}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{\\omega^2 + \\omega_c^2} d\\omega$$\nThe integral is a standard form:\n$$\\int \\frac{1}{\\omega^2 + a^2} d\\omega = \\frac{1}{a} \\arctan\\left(\\frac{\\omega}{a}\\right)$$\nEvaluating the definite integral:\n$$\\int_{-\\infty}^{\\infty} \\frac{1}{\\omega^2 + \\omega_c^2} d\\omega = \\left[ \\frac{1}{\\omega_c} \\arctan\\left(\\frac{\\omega}{\\omega_c}\\right) \\right]_{-\\infty}^{\\infty} = \\frac{1}{\\omega_c} \\left(\\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right)\\right) = \\frac{\\pi}{\\omega_c}$$\nSubstituting this back into the expression for the variance:\n$$\\sigma_{x_d}^2 = \\frac{D}{\\pi} \\left(\\frac{\\pi}{\\omega_c}\\right) = \\frac{D}{\\omega_c}$$\nBoth methods yield the same result, confirming the consistency of the derivations.", "answer": "$$\\boxed{\\frac{D}{\\omega_{c}}}$$", "id": "2902615"}, {"introduction": "Ideal, perfectly bandlimited signals are a useful theoretical abstraction, but real-world signals often possess energy at all frequencies. This exercise bridges theory and practice by challenging you to quantify the impact of such non-ideal conditions on reconstruction fidelity [@problem_id:2902649]. You will calculate the mean-square error that arises from sampling a process with a known spectral tail using a realistic, non-ideal anti-aliasing filter, providing insight into how aliasing contributes to overall system error.", "problem": "Consider a real-valued, zero-mean, wide-sense stationary (WSS) random process with power spectral density (PSD) $S_{x}(f)$ given by\n$$\nS_{x}(f)=\n\\begin{cases}\nS_{0}, & |f|\\leq B,\\\\\nS_{0}\\left(\\dfrac{B}{|f|}\\right)^{2}, & |f|>B\n\\end{cases}\n$$\nwhere $S_{0}>0$ and $B>0$ are constants. The process is passed through an analog anti-aliasing filter with frequency response $H_{a}(f)$ satisfying\n$$\nH_{a}(f)=\n\\begin{cases}\n1, & |f|\\leq B,\\\\\n\\alpha, & |f|>B\n\\end{cases}\n$$\nwith $0<\\alpha<1$. The filtered signal is then sampled at the rate $f_{s}=2B$, and reconstructed by an ideal continuous-time brick-wall lowpass filter of unit gain and cutoff frequency $B$.\n\nUsing only fundamental definitions from sampling theory and stochastic processes (namely, the frequency-domain replication induced by ideal impulse sampling, the effect of linear time-invariant filtering on PSDs, and the relationship between the PSD of a WSS process and its mean-square value), derive the mean-square reconstruction error\n$$\n\\mathbb{E}\\big[(x(t)-y(t))^{2}\\big],\n$$\nwhere $x(t)$ is the original process and $y(t)$ is the reconstructed signal. Express your final answer as a single closed-form symbolic expression in terms of $S_{0}$, $B$, and $\\alpha$. No numerical evaluation or rounding is required. State your answer in power units consistent with $S_{0}$ (do not include the unit symbol in the final boxed expression).", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard problem in the analysis of stochastic processes in sampling systems. Therefore, the problem is deemed valid.\n\nThe objective is to compute the mean-square reconstruction error, $\\mathcal{E}$, defined as $\\mathbb{E}[(x(t)-y(t))^{2}]$, where $x(t)$ is the original wide-sense stationary (WSS) random process and $y(t)$ is the signal reconstructed after anti-aliasing filtering, sampling, and low-pass filtering. For a WSS process, the mean-square value is equal to the total power, which can be found by integrating the power spectral density (PSD) over all frequencies. Let $e(t) = x(t) - y(t)$ be the error signal. The mean-square error is then $\\mathcal{E} = R_e(0) = \\int_{-\\infty}^{\\infty} S_e(f) df$, where $S_e(f)$ is the PSD of the error signal.\n\nWe can decompose the total error into two components. Let $x_B(t)$ be the ideal bandlimited version of the original signal $x(t)$, obtained by passing $x(t)$ through an ideal low-pass filter with cutoff frequency $B$. The error can then be written as:\n$$\ne(t) = x(t) - y(t) = (x(t) - x_B(t)) + (x_B(t) - y(t))\n$$\nThe first term, $e_{out}(t) = x(t) - x_B(t)$, represents the components of the original signal $x(t)$ that lie outside the reconstruction band $[-B, B]$. Its spectrum is non-zero only for $|f| > B$.\nThe second term, $e_{in}(t) = x_B(t) - y(t)$, represents the difference between the ideal in-band signal and the reconstructed signal. Both $x_B(t)$ and $y(t)$ are, by definition, bandlimited to the interval $[-B, B]$. Therefore, the spectrum of $e_{in}(t)$ is non-zero only for $|f| \\leq B$.\n\nSince the two error components $e_{out}(t)$ and $e_{in}(t)$ occupy disjoint frequency bands, they are orthogonal. The total mean-square error is the sum of their individual powers:\n$$\n\\mathcal{E} = \\mathbb{E}[e(t)^2] = \\mathbb{E}[e_{out}(t)^2] + \\mathbb{E}[e_{in}(t)^2]\n$$\n\nFirst, we calculate the power of the out-of-band error, $\\mathcal{E}_{out} = \\mathbb{E}[e_{out}(t)^2]$. This is the power of the original signal $x(t)$ outside the frequency band $[-B, B]$.\n$$\n\\mathcal{E}_{out} = \\int_{|f|>B} S_x(f) df = 2 \\int_{B}^{\\infty} S_x(f) df\n$$\nUsing the given PSD $S_x(f) = S_{0}(B/|f|)^{2}$ for $|f|>B$:\n$$\n\\mathcal{E}_{out} = 2 \\int_{B}^{\\infty} S_{0} \\frac{B^2}{f^2} df = 2 S_0 B^2 \\left[ -\\frac{1}{f} \\right]_{B}^{\\infty} = 2 S_0 B^2 \\left( 0 - \\left(-\\frac{1}{B}\\right) \\right) = 2 S_0 B\n$$\n\nSecond, we calculate the power of the in-band error, $\\mathcal{E}_{in} = \\mathbb{E}[e_{in}(t)^2]$. This error arises from the imperfections of the sampling and reconstruction process within the band $[-B, B]$. The ideal in-band signal is $x_B(t)$, with PSD $S_{x_B}(f) = S_x(f) = S_0$ for $|f| \\le B$. The reconstructed signal $y(t)$ is generated from the samples of the anti-aliased signal, $x_a(t)$. The PSD of $x_a(t)$ is $S_{x_a}(f) = |H_a(f)|^2 S_x(f)$.\nGiven $H_a(f)$ and $S_x(f)$, we have:\n$$\nS_{x_a}(f) =\n\\begin{cases}\n1^2 \\cdot S_0, & |f|\\leq B \\\\\n\\alpha^2 \\cdot S_{0}\\left(\\dfrac{B}{|f|}\\right)^{2}, & |f|>B\n\\end{cases}\n=\n\\begin{cases}\nS_0, & |f|\\leq B \\\\\n\\alpha^2 S_{0}\\dfrac{B^2}{f^2}, & |f|>B\n\\end{cases}\n$$\nWhen $x_a(t)$ is sampled at $f_s = 2B$ and reconstructed with an ideal low-pass filter with cutoff $B$, the PSD of the resulting signal $y(t)$ in the band $|f| \\le B$ is the sum of the baseband spectrum and the folded (aliased) spectra from higher frequencies.\n$$\nS_y(f) = \\sum_{k=-\\infty}^{\\infty} S_{x_a}(f - kf_s) = \\sum_{k=-\\infty}^{\\infty} S_{x_a}(f - 2kB) \\quad \\text{for } |f| \\le B\n$$\nWe can separate the $k=0$ term (baseband) from the $k \\neq 0$ terms (aliasing).\n$$\nS_y(f) = S_{x_a}(f) + \\sum_{k \\neq 0} S_{x_a}(f - 2kB)\n$$\nFor $|f| \\leq B$, $S_{x_a}(f) = S_0$. This corresponds to the correctly reconstructed part of the signal. The ideal signal $x_B(t)$ has this same PSD. The error $e_{in}(t) = x_B(t) - y(t)$ is thus due to the aliasing terms. The reconstructed signal can be viewed as $y(t) = y_{baseband}(t) + y_{alias}(t)$, where $y_{baseband}(t)$ corresponds to the ideal reconstruction of the in-band signal (which is $x_B(t)$ since $H_a(f)=1$ for $|f|\\le B$), and $y_{alias}(t)$ is the noise due to aliasing.\nSo, $e_{in}(t) = x_B(t) - (x_B(t) + y_{alias}(t)) = -y_{alias}(t)$. The power of the in-band error is therefore the power of the aliasing noise.\nThe PSD of the aliasing noise is $S_{alias}(f) = \\sum_{k \\neq 0} S_{x_a}(f - 2kB)$.\n$$\n\\mathcal{E}_{in} = \\int_{-B}^{B} S_{alias}(f) df = \\int_{-B}^{B} \\sum_{k \\neq 0} S_{x_a}(f - 2kB) df\n$$\nFor $|f| \\le B$ and $k \\neq 0$, the argument $f - 2kB$ is always outside $[-B, B]$. Thus, we must use $S_{x_a}(f) = \\alpha^2 S_0 B^2/f^2$.\n$$\n\\mathcal{E}_{in} = \\int_{-B}^{B} \\sum_{k=1}^{\\infty} [S_{x_a}(f - 2kB) + S_{x_a}(f + 2kB)] df\n$$\n$$\n\\mathcal{E}_{in} = \\alpha^2 S_0 B^2 \\sum_{k=1}^{\\infty} \\int_{-B}^{B} \\left( \\frac{1}{(f-2kB)^2} + \\frac{1}{(f+2kB)^2} \\right) df\n$$\nWe evaluate the integral for a fixed $k$:\n$$\n\\int_{-B}^{B} \\frac{1}{(f-2kB)^2} df = \\left[ -\\frac{1}{f-2kB} \\right]_{-B}^{B} = -\\frac{1}{B-2kB} + \\frac{1}{-B-2kB} = \\frac{1}{2kB-B} - \\frac{1}{2kB+B} = \\frac{1}{B}\\left( \\frac{1}{2k-1} - \\frac{1}{2k+1} \\right) = \\frac{2}{B(4k^2-1)}\n$$\nThe integral of the second term is identical due to symmetry.\n$$\n\\int_{-B}^{B} \\left( \\frac{1}{(f-2kB)^2} + \\frac{1}{(f+2kB)^2} \\right) df = \\frac{2}{B(4k^2-1)} + \\frac{2}{B(4k^2-1)} = \\frac{4}{B(4k^2-1)}\n$$\nNow, substitute this back into the expression for $\\mathcal{E}_{in}$:\n$$\n\\mathcal{E}_{in} = \\alpha^2 S_0 B^2 \\sum_{k=1}^{\\infty} \\frac{4}{B(4k^2-1)} = 4 \\alpha^2 S_0 B \\sum_{k=1}^{\\infty} \\frac{1}{4k^2-1}\n$$\nThe sum is a telescoping series:\n$$\n\\sum_{k=1}^{\\infty} \\frac{1}{4k^2-1} = \\sum_{k=1}^{\\infty} \\frac{1}{(2k-1)(2k+1)} = \\frac{1}{2} \\sum_{k=1}^{\\infty} \\left( \\frac{1}{2k-1} - \\frac{1}{2k+1} \\right)\n$$\n$$\n= \\frac{1}{2} \\left[ \\left(1 - \\frac{1}{3}\\right) + \\left(\\frac{1}{3} - \\frac{1}{5}\\right) + \\left(\\frac{1}{5} - \\frac{1}{7}\\right) + \\dots \\right] = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n$$\nThe in-band aliasing error power is therefore:\n$$\n\\mathcal{E}_{in} = 4 \\alpha^2 S_0 B \\left(\\frac{1}{2}\\right) = 2 \\alpha^2 S_0 B\n$$\nThe total mean-square error is the sum of the out-of-band and in-band error powers:\n$$\n\\mathcal{E} = \\mathcal{E}_{out} + \\mathcal{E}_{in} = 2 S_0 B + 2 \\alpha^2 S_0 B = 2 S_0 B (1 + \\alpha^2)\n$$\nThis expression represents the total average power of the difference between the original signal and its reconstruction.", "answer": "$$\\boxed{2S_{0}B(1 + \\alpha^{2})}$$", "id": "2902649"}, {"introduction": "Designing a sampling system is an act of balancing competing engineering requirements, chiefly performance and cost. This hands-on programming exercise elevates your understanding to a systems-level perspective by exploring the fundamental trade-off between oversampling and anti-aliasing filter complexity [@problem_id:2902598]. By implementing code to minimize a cost function subject to an aliasing error constraint, you will gain practical experience in the crucial design decisions that define modern signal processing systems.", "problem": "You are given a continuous-time, wide-sense stationary source with two-sided power spectral density modeled as $S_{x}(f) = \\dfrac{S_{0}}{1 + \\left(\\dfrac{|f|}{f_{b}}\\right)^{p}}$ for all real $f$, where $S_{0} > 0$, $f_{b} > 0$, and $p > 1$ are parameters. The source is first passed through an analog anti-aliasing low-pass filter $H(f)$ and then uniformly sampled at sampling rate $f_{s}$ (in Hz). Reconstruction is performed by ideal sinc interpolation followed by an ideal low-pass filter that retains only the baseband frequencies $|f| \\leq B$, where $B > 0$ is the desired information bandwidth.\n\nThe goal is to analyze the trade-off between oversampling and analog anti-aliasing filter complexity by minimizing a cost function that penalizes the sampling rate and the analog filter order, subject to a rigorous upper bound on the reconstruction error due to aliasing.\n\nFundamental base for modeling:\n- Uniform sampling replicates the continuous-time spectrum with period $f_{s}$.\n- The mean-squared reconstruction error caused by aliasing from frequencies $|f| \\geq \\dfrac{f_{s}}{2}$ can be upper-bounded by the stopband energy of the prefiltered source entering the sampler.\n- A classical analog Type I Chebyshev low-pass filter with passband ripple not exceeding $R_{p}$ decibels in $|f| \\leq B$ and stopband attenuation at and beyond $|f| \\geq \\dfrac{f_{s}}{2}$ characterized by a stopband gain bound $\\delta_{s}$ (where $\\delta_{s} = 10^{-A_{s}/20}$ and $A_{s}$ is the stopband attenuation in decibels) has a minimum integer order that depends on the ratio $\\dfrac{f_{s}/2}{B}$, the passband ripple $R_{p}$, and the required $\\delta_{s}$.\n\nConstraint:\n- Let $J(f_{s}) \\triangleq 2 \\int_{f_{s}/2}^{\\infty} S_{x}(f) \\, df$. If the anti-aliasing filter is designed so that $|H(f)| \\leq \\delta_{s}$ for all $|f| \\geq \\dfrac{f_{s}}{2}$, then the mean-squared aliasing error is upper-bounded by $\\delta_{s}^{2} \\, J(f_{s})$. Impose the constraint $\\delta_{s}^{2} \\, J(f_{s}) \\leq \\varepsilon^{2}$, where $\\varepsilon > 0$ is a specified tolerance. This yields the requirement $\\delta_{s} \\leq \\min\\!\\left(1, \\dfrac{\\varepsilon}{\\sqrt{J(f_{s})}} \\right)$ and therefore a required stopband attenuation $A_{s}(f_{s}) = \\max\\!\\left(0, -20 \\log_{10} \\left(\\dfrac{\\varepsilon}{\\sqrt{J(f_{s})}}\\right)\\right)$ decibels.\n\nFilter family and complexity model:\n- Use an analog Type I Chebyshev low-pass filter with passband edge at $B$ and stopband edge at $\\dfrac{f_{s}}{2}$. The minimum integer filter order required to satisfy the passband ripple specification $R_{p}$ decibels and the stopband attenuation $A_{s}(f_{s})$ decibels is determined by the standard Chebyshev response relations.\n- Define the cost function $C(f_{s}, N) = c_{s} \\, f_{s} + c_{N} \\, N$, where $N$ is the minimum integer filter order that meets the specifications at sampling rate $f_{s}$, $c_{s} > 0$ is the cost coefficient for sampling rate (in arbitrary cost-per-Hz units), and $c_{N} > 0$ is the cost coefficient per unit filter order (dimensionless). The objective is to minimize $C(f_{s}, N)$ subject to the constraint above and $f_{s} > 2B$.\n\nYour task is to write a complete program that, for each test case below, searches over a specified discrete set of candidate sampling rates and returns the triple consisting of:\n- the optimal sampling rate $f_{s}^{\\star}$ in Hz,\n- the corresponding minimum integer filter order $N^{\\star}$,\n- the minimum cost $C^{\\star}$.\n\nDetails and requirements:\n- For each candidate $f_{s}$, compute $J(f_{s}) = 2 \\int_{f_{s}/2}^{\\infty} \\dfrac{S_{0}}{1 + \\left(\\dfrac{f}{f_{b}}\\right)^{p}} \\, df$ numerically. Treat $S_{0}$ as $1$ (normalized units).\n- Enforce $f_{s} > 2B$. If a candidate violates this, discard it.\n- For each valid $f_{s}$, compute the required stopband gain $\\delta_{s}(f_{s}) = \\min\\!\\left(1, \\dfrac{\\varepsilon}{\\sqrt{J(f_{s})}}\\right)$ and the corresponding required stopband attenuation $A_{s}(f_{s})$ in decibels.\n- Determine the minimum integer Chebyshev Type I order $N$ that satisfies the passband ripple bound $R_{p}$ decibels over $|f| \\leq B$ and the stopband attenuation bound $A_{s}(f_{s})$ decibels at $|f| \\geq \\dfrac{f_{s}}{2}$.\n- Compute the cost $C(f_{s}, N)$ and select the triple $(f_{s}^{\\star}, N^{\\star}, C^{\\star})$ that minimizes the cost. If there are multiple minimizers, choose the one with the smallest $f_{s}$ among them.\n\nAngle units are not used. Sampling rates must be reported in Hz. Filter orders must be integers. Costs are dimensionless real numbers.\n\nTest suite:\n- Case $1$ (happy path): $B = 10000$ Hz, $f_{b} = 30000$ Hz, $p = 4$, $\\varepsilon = 1.0 \\times 10^{-3}$, $R_{p} = 0.1$ decibels, $c_{s} = 1.0 \\times 10^{-6}$, $c_{N} = 1.0$, candidate $f_{s}$ uniformly over $[22000, 120000]$ Hz with $81$ points.\n- Case $2$ (tight error tolerance): $B = 10000$ Hz, $f_{b} = 30000$ Hz, $p = 4$, $\\varepsilon = 1.0 \\times 10^{-5}$, $R_{p} = 0.1$ decibels, $c_{s} = 1.0 \\times 10^{-6}$, $c_{N} = 1.0$, candidate $f_{s}$ uniformly over $[22000, 240000]$ Hz with $91$ points.\n- Case $3$ (sampling rate expensive): $B = 10000$ Hz, $f_{b} = 25000$ Hz, $p = 3$, $\\varepsilon = 1.0 \\times 10^{-3}$, $R_{p} = 0.1$ decibels, $c_{s} = 5.0 \\times 10^{-6}$, $c_{N} = 0.2$, candidate $f_{s}$ uniformly over $[21000, 100000]$ Hz with $80$ points.\n- Case $4$ (slow spectral decay, loose tolerance): $B = 10000$ Hz, $f_{b} = 20000$ Hz, $p = 2$, $\\varepsilon = 5.0 \\times 10^{-3}$, $R_{p} = 0.1$ decibels, $c_{s} = 1.0 \\times 10^{-6}$, $c_{N} = 1.0$, candidate $f_{s}$ uniformly over $[22000, 160000]$ Hz with $70$ points.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list enclosed in square brackets. The list must contain the sequence $[f_{s,1}^{\\star}, N_{1}^{\\star}, C_{1}^{\\star}, f_{s,2}^{\\star}, N_{2}^{\\star}, C_{2}^{\\star}, f_{s,3}^{\\star}, N_{3}^{\\star}, C_{3}^{\\star}, f_{s,4}^{\\star}, N_{4}^{\\star}, C_{4}^{\\star}]$.\n- Report $f_{s,i}^{\\star}$ in Hz (floating-point), $N_{i}^{\\star}$ as an integer, and $C_{i}^{\\star}$ as a floating-point value.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is a scientifically grounded, well-posed problem in signal processing and systems engineering that is free of contradictions or ambiguities. It presents a standard optimization task involving a trade-off between system parameters. We may therefore proceed with a formal solution.\n\nThe objective is to minimize a cost function, $C(f_{s}, N) = c_{s} f_{s} + c_{N} N$, which represents a weighted sum of the costs associated with the sampling rate $f_{s}$ and the order $N$ of an analog anti-aliasing filter. The minimization is performed over a specified discrete set of candidate sampling frequencies $f_{s}$, subject to a constraint on the maximum permissible aliasing error. The procedure to find the optimal triplet $(f_{s}^{\\star}, N^{\\star}, C^{\\star})$ for each test case is executed as follows.\n\nFor each candidate sampling rate $f_s$ from the provided set, subject to the fundamental requirement $f_{s} > 2B$ where $B$ is the information bandwidth:\n\n1.  **Calculate Stopband Power Integral**: The first step is to quantify the signal power residing in the frequencies that will be aliased by the sampling process. This power, contained in the stopband $|f| \\geq f_{s}/2$ of the anti-aliasing filter, is given by the integral $J(f_{s})$. Since the power spectral density $S_{x}(f)$ is an even function of frequency $f$, this integral simplifies to:\n    $$ J(f_{s}) = 2 \\int_{f_{s}/2}^{\\infty} S_{x}(f) \\, df = 2 \\int_{f_{s}/2}^{\\infty} \\frac{S_{0}}{1 + \\left(\\frac{f}{f_{b}}\\right)^{p}} \\, df $$\n    As specified, we set the normalization constant $S_0 = 1$. The integral is computed using standard numerical quadrature methods due to the absence of a general closed-form solution for an arbitrary exponent $p > 1$.\n\n2.  **Determine Required Stopband Attenuation**: The aliasing error is bounded by the product of the filter's squared stopband gain, $\\delta_{s}^2$, and the stopband power, $J(f_{s})$. The posed constraint, $\\delta_{s}^{2} J(f_{s}) \\leq \\varepsilon^{2}$, dictates the required performance of the anti-aliasing filter. From this, we derive the necessary stopband attenuation $A_{s}$ in decibels. The required stopband gain must satisfy $\\delta_{s} \\leq \\varepsilon / \\sqrt{J(f_{s})}$, and since gain cannot exceed unity ($\\delta_{s} \\leq 1$), the required stopband attenuation is:\n    $$ A_{s}(f_{s}) = -20 \\log_{10}(\\delta_{s}) = -20 \\log_{10} \\left( \\min\\left(1, \\frac{\\varepsilon}{\\sqrt{J(f_{s})}}\\right) \\right) $$\n    This is equivalent to the provided expression $A_{s}(f_{s}) = \\max\\left(0, -20 \\log_{10}\\left(\\frac{\\varepsilon}{\\sqrt{J(f_{s})}}\\right)\\right)$. A larger sampling rate $f_{s}$ shifts the integration limit $f_{s}/2$ to higher frequencies, reducing $J(f_{s})$ and thus relaxing the attenuation requirement $A_{s}(f_{s})$.\n\n3.  **Calculate Minimum Filter Order**: We employ a Type I Chebyshev filter, which is characterized by its sharp roll-off. The minimum integer order $N$ required to satisfy a given set of specifications is determined by the passband edge frequency $f_{p} = B$, the stopband edge frequency $f_{st} = f_{s}/2$, the maximum passband ripple $R_{p}$ (in decibels), and the minimum stopband attenuation $A_{s}$ (in decibels). The standard formula for the order is:\n    $$ N = \\left\\lceil \\frac{\\text{arccosh}\\left(\\sqrt{\\frac{10^{A_{s}/10} - 1}{10^{R_{p}/10} - 1}}\\right)}{\\text{arccosh}\\left(\\frac{f_{st}}{f_{p}}\\right)} \\right\\rceil $$\n    where $\\lceil \\cdot \\rceil$ denotes the ceiling function, as the filter order must be an integer. This calculation confirms the intuitive principle that a higher-order filter is needed for smaller transition bands (i.e., when $f_{s}/2$ is close to $B$) or for higher attenuation requirements $A_{s}$. This calculation is reliably performed using established scientific library functions.\n\n4.  **Compute Total Cost**: With the sampling rate $f_{s}$ and the corresponding required filter order $N$ determined, the total cost is computed using the linear model $C(f_{s}, N) = c_{s} f_{s} + c_{N} N$.\n\nThis entire sequence is repeated for every candidate $f_{s}$ in the search space. The resulting set of costs is then analyzed to identify the minimum value, $C^{\\star}$. The triplet $(f_{s}^{\\star}, N^{\\star}, C^{\\star})$ corresponding to this minimum is the solution. In case of multiple solutions with the same minimum cost, the one with the smallest sampling rate $f_{s}^{\\star}$ is selected, as per the problem specification. This procedure is systematically applied to all four test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Main function to solve the optimization problem for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: B, fb, p, eps, Rp, cs, cN, (fs_min, fs_max), fs_points\n        (10000.0, 30000.0, 4.0, 1.0e-3, 0.1, 1.0e-6, 1.0, (22000.0, 120000.0), 81),\n        # Case 2\n        (10000.0, 30000.0, 4.0, 1.0e-5, 0.1, 1.0e-6, 1.0, (22000.0, 240000.0), 91),\n        # Case 3\n        (10000.0, 25000.0, 3.0, 1.0e-3, 0.1, 5.0e-6, 0.2, (21000.0, 100000.0), 80),\n        # Case 4\n        (10000.0, 20000.0, 2.0, 5.0e-3, 0.1, 1.0e-6, 1.0, (22000.0, 160000.0), 70),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = _solve_single_case(case)\n        all_results.extend(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef _solve_single_case(case_params):\n    \"\"\"\n    Solves the optimization for a single set of parameters.\n    \"\"\"\n    B, fb, p, eps, Rp, cs, cN, fs_range, fs_points = case_params\n\n    fs_candidates = np.linspace(fs_range[0], fs_range[1], fs_points)\n\n    min_cost = np.inf\n    optimal_fs = None\n    optimal_N = None\n\n    for fs in fs_candidates:\n        # Per problem statement, all candidate fs are > 2B.\n        # So we skip the explicit check 'if fs <= 2 * B:'.\n\n        # Step 1: Compute J(fs) = 2 * Integral from fs/2 to inf of S_x(f) df\n        # S0 is normalized to 1. |f|=f since we integrate over positive f.\n        integrand = lambda f: 1.0 / (1.0 + (f / fb)**p)\n        J_fs, _ = integrate.quad(integrand, fs / 2.0, np.inf)\n        J_fs *= 2.0\n\n        # Step 2: Compute required stopband attenuation As(fs) in dB.\n        if J_fs <= 1e-30:  # If stopband power is negligible, no attenuation is needed.\n            As = 0.0\n        else:\n            # As(fs) = max(0, -20 * log10(eps / sqrt(J(fs))))\n            # This avoids math domain error if a large J_fs makes the ratio > 1.\n            arg = eps / np.sqrt(J_fs)\n            if arg >= 1.0:\n                As = 0.0\n            else:\n                As = -20.0 * np.log10(arg)\n\n        # Step 3: Determine minimum Type I Chebyshev filter order N.\n        # Passband edge frequency (Hz)\n        wp = B\n        # Stopband edge frequency (Hz)\n        ws = fs / 2.0\n        # Passband ripple (dB)\n        gpass = Rp\n        # Stopband attenuation (dB)\n        gstop = As\n        \n        # 'cheb1ord' returns the minimum integer order required.\n        N, _ = signal.cheb1ord(wp, ws, gpass, gstop, analog=True)\n        N = int(N)\n\n        # Step 4: Compute the cost C(fs, N)\n        cost = cs * fs + cN * float(N)\n        \n        # Update minimum cost and corresponding parameters. The strict inequality\n        # and ordered search over fs handles the tie-breaking rule correctly.\n        if cost < min_cost:\n            min_cost = cost\n            optimal_fs = fs\n            optimal_N = N\n            \n    return [optimal_fs, optimal_N, min_cost]\n\nsolve()\n```", "id": "2902598"}]}