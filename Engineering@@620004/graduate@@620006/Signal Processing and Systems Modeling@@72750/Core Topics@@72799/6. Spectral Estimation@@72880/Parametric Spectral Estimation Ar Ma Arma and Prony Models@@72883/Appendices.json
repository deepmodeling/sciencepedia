{"hands_on_practices": [{"introduction": "A deep understanding of parametric models begins with the ability to connect the model's algebraic coefficients to its spectral characteristics. This exercise provides fundamental practice in this skill, guiding you through the analysis of a second-order autoregressive, or AR(2), process. By starting with the model's difference equation coefficients, you will determine the system's poles and use their location in the $z$-plane to predict key features of the power spectral density, namely its peak frequency and bandwidth [@problem_id:2889646].", "problem": "Consider a real, wide-sense stationary autoregressive of order two (AR(2)) process defined by the difference equation $x[n] + a_1 x[n-1] + a_2 x[n-2] = e[n]$, where $e[n]$ is zero-mean, white, Gaussian driving noise with variance $\\sigma_e^2 \\in (0,\\infty)$. The associated autoregressive polynomial is $A(z) = 1 + a_1 z^{-1} + a_2 z^{-2}$. The power spectral density (PSD) of an all-pole process is inversely proportional to $|A(\\exp(j\\omega))|^2$. Let the coefficients be $a_1 = -0.95$ and $a_2 = 0.9025$.\n\nUsing only first principles about the pole locations of $A(z)$, the structure of all-pole spectra, and the definition of full width at half maximum (FWHM) for spectral peaks, do the following:\n\n- Compute the two poles of $A(z)$ in the complex $z$-plane and express them in polar form.\n- Identify the frequency (in radians per sample) at which the dominant spectral peak of the PSD occurs.\n- Under the narrowband assumption appropriate for pole radius close to unity, derive from the factorized form of $A(\\exp(j\\omega))$ an analytic expression for the $3$ decibel (dB) FWHM (that is, the frequency separation between the two frequencies where the PSD falls to one-half of its peak value) around the identified peak, and then evaluate it numerically.\n\nExpress the final $3$ dB bandwidth as a single number in radians per sample and round your answer to four significant figures. Angles must be in radians. For the answer box, report only the $3$ dB bandwidth in radians per sample.", "solution": "The system transfer function is $H(z) = 1/A(z)$, where $A(z) = 1 + a_1 z^{-1} + a_2 z^{-2}$. The poles of the system are the roots of the denominator polynomial, which are the zeros of $A(z)$. These are found by solving the characteristic equation $z^2 + a_1 z + a_2 = 0$.\n\nFirst, we compute the poles. For a real AR(2) process with complex poles, the poles must form a complex conjugate pair, which can be expressed in polar form as $p_1 = r \\exp(j\\theta)$ and $p_2 = r \\exp(-j\\theta)$, where $r$ is the pole radius and $\\pm\\theta$ are the pole angles. The characteristic polynomial can be written as:\n$$ (z - p_1)(z - p_2) = z^2 - (p_1 + p_2)z + p_1 p_2 = z^2 - (2r\\cos\\theta)z + r^2 = 0 $$\nBy comparing the coefficients of this polynomial with $z^2 + a_1 z + a_2 = 0$, we find:\n$$ a_1 = -2r\\cos\\theta $$\n$$ a_2 = r^2 $$\nUsing the given values $a_1 = -0.95$ and $a_2 = 0.9025$:\nThe pole radius $r$ is found from $a_2$:\n$$ r^2 = 0.9025 \\implies r = \\sqrt{0.9025} = 0.95 $$\nThe pole angle $\\theta$ is found from $a_1$:\n$$ -0.95 = -2(0.95)\\cos\\theta \\implies -0.95 = -1.9\\cos\\theta $$\n$$ \\cos\\theta = \\frac{0.95}{1.9} = \\frac{1}{2} \\implies \\theta = \\arccos\\left(\\frac{1}{2}\\right) = \\frac{\\pi}{3} \\text{ radians} $$\nThe discriminant of the quadratic equation $z^2 - 0.95z + 0.9025 = 0$ is $(-0.95)^2 - 4(1)(0.9025) = 0.9025 - 3.61 = -2.7075$, which is negative, confirming the poles are a complex conjugate pair. The two poles are:\n$$ p_1 = 0.95 \\exp\\left(j\\frac{\\pi}{3}\\right) \\quad \\text{and} \\quad p_2 = 0.95 \\exp\\left(-j\\frac{\\pi}{3}\\right) $$\n\nSecond, we identify the spectral peak frequency. The power spectral density of the AR(2) process is $P_x(\\omega) = \\frac{\\sigma_e^2}{|A(\\exp(j\\omega))|^2}$. The peaks in the PSD occur where the denominator $|A(\\exp(j\\omega))|^2$ is at a minimum. The term $|A(\\exp(j\\omega))|^2$ represents the squared distance from the point $\\exp(j\\omega)$ on the unit circle to the zeros of $A(z)$. The minimum occurs when $\\exp(j\\omega)$ is closest to one of the poles, which are the zeros of the polynomial $z^2 A(z)$. For poles close to the unit circle (i.e., $r \\approx 1$), the PSD exhibits sharp peaks at frequencies corresponding to the pole angles. Therefore, the dominant spectral peak in the frequency range $[0, \\pi]$ occurs at:\n$$ \\omega_{peak} = \\theta = \\frac{\\pi}{3} \\text{ radians per sample} $$\n\nThird, we derive and compute the $3$ dB bandwidth (FWHM). The $3$ dB bandwidth is the frequency width between the two points where the PSD drops to half of its peak value. This is equivalent to finding the frequencies $\\omega$ where the denominator $|A(\\exp(j\\omega))|^2$ is twice its minimum value.\nThe factorized form of the AR polynomial is $A(z) = (1 - p_1 z^{-1})(1 - p_2 z^{-1})$. The squared magnitude of its frequency response is:\n$$ |A(\\exp(j\\omega))|^2 = |(1 - p_1 \\exp(-j\\omega))(1 - p_2 \\exp(-j\\omega))|^2 = |1 - p_1 \\exp(-j\\omega)|^2 |1 - p_2 \\exp(-j\\omega)|^2 $$\nUnder the narrowband assumption for a pole at $p_1 = r\\exp(j\\theta)$ with $r$ close to $1$, the spectrum near $\\omega = \\theta$ is dominated by the factor $|1 - p_1 \\exp(-j\\omega)|^2$. The other factor, $|1 - p_2 \\exp(-j\\omega)|^2$, is slowly varying in this region and can be approximated by its value at the peak, i.e., at $\\omega = \\theta$.\nLet $\\omega = \\theta \\pm \\Delta\\omega$ be the half-power frequencies. The condition for the 3 dB points is:\n$$ |A(\\exp(j(\\theta \\pm \\Delta\\omega)))|^2 = 2 |A(\\exp(j\\theta))|^2 $$\nUsing the narrowband approximation:\n$$ |1-p_1 \\exp(-j(\\theta \\pm \\Delta\\omega))|^2 |1-p_2 \\exp(-j\\theta)|^2 \\approx 2 |1-p_1 \\exp(-j\\theta)|^2 |1-p_2 \\exp(-j\\theta)|^2 $$\nThe slowly varying term cancels out, leaving:\n$$ |1 - p_1 \\exp(-j(\\theta \\pm \\Delta\\omega))|^2 \\approx 2 |1 - p_1 \\exp(-j\\theta)|^2 $$\nLet's evaluate the terms. The left side is:\n$$ |1 - r\\exp(j\\theta)\\exp(-j(\\theta \\pm \\Delta\\omega))|^2 = |1 - r\\exp(\\mp j\\Delta\\omega)|^2 = 1 - 2r\\cos(\\Delta\\omega) + r^2 $$\nThe right side is:\n$$ 2 |1 - r\\exp(j\\theta)\\exp(-j\\theta)|^2 = 2|1-r|^2 = 2(1-r)^2 $$\nEquating these gives the condition for the half-bandwidth $\\Delta\\omega$:\n$$ 1 - 2r\\cos(\\Delta\\omega) + r^2 \\approx 2(1-r)^2 $$\nFor small $\\Delta\\omega$, we use the Taylor approximation $\\cos(\\Delta\\omega) \\approx 1 - \\frac{(\\Delta\\omega)^2}{2}$:\n$$ 1 - 2r\\left(1 - \\frac{(\\Delta\\omega)^2}{2}\\right) + r^2 \\approx 2(1-r)^2 $$\n$$ (1 - 2r + r^2) + r(\\Delta\\omega)^2 \\approx 2(1-r)^2 $$\n$$ (1-r)^2 + r(\\Delta\\omega)^2 \\approx 2(1-r)^2 $$\n$$ r(\\Delta\\omega)^2 \\approx (1-r)^2 $$\n$$ (\\Delta\\omega)^2 \\approx \\frac{(1-r)^2}{r} \\implies |\\Delta\\omega| \\approx \\frac{1-r}{\\sqrt{r}} $$\nThe FWHM, or $3$ dB bandwidth, is twice this value:\n$$ BW_{3dB} = 2|\\Delta\\omega| \\approx \\frac{2(1-r)}{\\sqrt{r}} $$\nNow, we substitute the numerical value $r=0.95$:\n$$ BW_{3dB} \\approx \\frac{2(1 - 0.95)}{\\sqrt{0.95}} = \\frac{2(0.05)}{\\sqrt{0.95}} = \\frac{0.1}{\\sqrt{0.95}} $$\nCalculating the numerical value:\n$$ BW_{3dB} \\approx \\frac{0.1}{0.9746794} \\approx 0.1025978... $$\nRounding to four significant figures, the $3$ dB bandwidth is $0.1026$ radians per sample.", "answer": "$$\n\\boxed{0.1026}\n$$", "id": "2889646"}, {"introduction": "While Autoregressive Moving-Average (ARMA) models offer greater flexibility than pure AR models, they also introduce potential complexities such as non-identifiability. This practice explores a crucial case of model redundancy where a common pole and zero in the system transfer function effectively reduce the model's true order. By analyzing a seemingly ARMA(2,1) model that simplifies to an AR(1) process, you will gain insight into the practical implications of over-parameterization for model estimation and uniqueness [@problem_id:2889653].", "problem": "Consider a discrete-time Autoregressive Moving-Average (ARMA) model defined by polynomials $A(z)$ and $B(z)$ and driven by zero-mean white noise $e[n]$ with variance $\\sigma_{e}^{2}$. Let the model be specified in the $z$-domain by $A(z) X(z) = B(z) E(z)$, where $X(z)$ and $E(z)$ are the $z$-transforms of $x[n]$ and $e[n]$, respectively. Suppose\n$A(z) = \\bigl(1 - 0.7 z^{-1}\\bigr)\\bigl(1 - 0.2 z^{-1}\\bigr)$ and $B(z) = \\bigl(1 - 0.7 z^{-1}\\bigr)$, with $\\sigma_{e}^{2} = 2$.\n  \nUsing only foundational definitions (that a linear time-invariant system maps an input $e[n]$ to an output $x[n]$ via convolution with an impulse response, that the power spectral density is the Fourier transform of the autocorrelation function, and that the frequency response is the $z$-transform evaluated on the unit circle), do the following:\n\n1. Derive the transfer function $H(z)$ from $E(z)$ to $X(z)$ and show explicitly how common factors in $A(z)$ and $B(z)$ reduce the modelâ€™s effective order.\n\n2. Starting from first principles, derive the power spectral density $S_{x}(\\omega)$ of $x[n]$ as a function of the angular frequency $\\omega$ (in radians), expressed in closed form in terms of elementary functions after any factor cancellations.\n\n3. Briefly explain, in words, the implications of such pole-zero cancellations for parameter identifiability and the behavior of practical estimation algorithms when over-parameterizing this system.\n\nYour final answer must be the single closed-form analytic expression for $S_{x}(\\omega)$ requested in item $2$. No rounding is required.", "solution": "The problem asks for three distinct but related tasks. We will address them in order.\n\n**1. Transfer Function and Model Order Reduction**\n\nThe system is described by the linear constant-coefficient difference equation whose $z$-transform is given as:\n$$A(z) X(z) = B(z) E(z)$$\nThe transfer function $H(z)$ of a linear time-invariant (LTI) system is defined as the ratio of the $z$-transform of the output $X(z)$ to the $z$-transform of the input $E(z)$.\n$$H(z) = \\frac{X(z)}{E(z)}$$\nFrom the system equation, we can write:\n$$H(z) = \\frac{B(z)}{A(z)}$$\nSubstituting the given expressions for the polynomials $A(z)$ and $B(z)$:\n$$H(z) = \\frac{1 - 0.7 z^{-1}}{\\bigl(1 - 0.7 z^{-1}\\bigr)\\bigl(1 - 0.2 z^{-1}\\bigr)}$$\nThe initial model is an ARMA(p,q) model where $p$ is the order of $A(z)$ and $q$ is the order of $B(z)$. Here, $p=2$ and $q=1$, so it is an ARMA(2,1) model. The poles of the system are the roots of $A(z)=0$, which are $z=0.7$ and $z=0.2$. The zero is the root of $B(z)=0$, which is $z=0.7$.\n\nWe observe a common factor of $(1 - 0.7 z^{-1})$ in both the numerator and the denominator. This corresponds to a pole and a zero at the same location, $z=0.7$. For a stable system, the region of convergence (ROC) of $H(z)$ must include the unit circle. Since both poles ($0.2$ and $0.7$) are inside the unit circle, the ROC is $|z| > 0.7$. In this region, the common factor can be cancelled.\nThe reduced (or effective) transfer function is:\n$$H(z) = \\frac{1}{1 - 0.2 z^{-1}}$$\nThis simplified transfer function corresponds to an Autoregressive model of order $1$, or AR(1), with a single pole at $z=0.2$. The original ARMA(2,1) parameterization is redundant; its input-output behavior is identical to that of a simpler AR(1) system. The pole at $z=0.7$ is unobservable from the output because it is cancelled by the zero at the same location.\n\n**2. Derivation of the Power Spectral Density**\n\nWe proceed from first principles as requested.\nAn LTI system relates its output $x[n]$ to its input $e[n]$ via convolution with the impulse response $h[n]$:\n$$x[n] = h[n] * e[n] = \\sum_{k=-\\infty}^{\\infty} h[k] e[n-k]$$\nThe autocorrelation function of the output, $R_{xx}[m]$, is defined as:\n$$R_{xx}[m] = E\\{x[n] x^*[n+m]\\}$$\nSubstituting the convolution expression for $x[n]$ and $x[n+m]$:\n$$R_{xx}[m] = E\\left\\{ \\left(\\sum_{k=-\\infty}^{\\infty} h[k] e[n-k]\\right) \\left(\\sum_{l=-\\infty}^{\\infty} h^*[l] e^*[n+m-l]\\right) \\right\\}$$\nBy linearity of expectation, we can move the expectation operator inside the summations:\n$$R_{xx}[m] = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} h[k] h^*[l] E\\{e[n-k] e^*[n+m-l]\\}$$\nThe term $E\\{e[n-k] e^*[n+m-l]\\}$ is the autocorrelation of the input noise, $R_{ee}[(n+m-l) - (n-k)] = R_{ee}[m-l+k]$. Since $e[n]$ is zero-mean white noise with variance $\\sigma_{e}^{2}$, its autocorrelation is given by:\n$$R_{ee}[j] = \\sigma_{e}^{2} \\delta[j]$$\nwhere $\\delta[j]$ is the Kronecker delta function. Substituting this into the expression for $R_{xx}[m]$ requires $j=m-l+k=0$, which means $l=m+k$.\n$$R_{xx}[m] = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} h[k] h^*[l] \\sigma_{e}^{2} \\delta[m-l+k] = \\sigma_{e}^{2} \\sum_{k=-\\infty}^{\\infty} h[k] h^*[m+k]$$\nThe summation $\\sum_{k=-\\infty}^{\\infty} h[k] h^*[k-(-m)]$ is the deterministic autocorrelation of the impulse response, denoted $R_{hh}[m]$. Thus:\n$$R_{xx}[m] = \\sigma_{e}^{2} R_{hh}[m]$$\nThe Power Spectral Density (PSD) is the Discrete-Time Fourier Transform (DTFT) of the autocorrelation function (Wiener-Khinchin theorem):\n$$S_{x}(\\omega) = \\sum_{m=-\\infty}^{\\infty} R_{xx}[m] e^{-j\\omega m}$$\nApplying the DTFT to our expression for $R_{xx}[m]$:\n$$S_{x}(\\omega) = \\mathcal{F}\\{R_{xx}[m]\\} = \\mathcal{F}\\{\\sigma_{e}^{2} R_{hh}[m]\\} = \\sigma_{e}^{2} \\mathcal{F}\\{R_{hh}[m]\\\n\\}$$\nA fundamental property of the DTFT is that the transform of a signal's autocorrelation is the squared magnitude of the signal's transform: $\\mathcal{F}\\{R_{hh}[m]\\} = |H(e^{j\\omega})|^2$, where $H(e^{j\\omega})$ is the frequency response of the system.\nThis yields the central result for the output PSD of an LTI system driven by white noise:\n$$S_{x}(\\omega) = \\sigma_{e}^{2} |H(e^{j\\omega})|^2$$\nThe frequency response $H(e^{j\\omega})$ is obtained by evaluating the transfer function $H(z)$ on the unit circle, $z = e^{j\\omega}$. Using the reduced transfer function $H(z) = \\frac{1}{1 - 0.2 z^{-1}}$:\n$$H(e^{j\\omega}) = \\frac{1}{1 - 0.2 e^{-j\\omega}}$$\nNext, we compute the squared magnitude:\n$$|H(e^{j\\omega})|^2 = \\left| \\frac{1}{1 - 0.2 e^{-j\\omega}} \\right|^2 = \\frac{1}{|1 - 0.2 e^{-j\\omega}|^2}$$\nThe squared magnitude of the denominator is:\n$$|1 - 0.2 e^{-j\\omega}|^2 = (1 - 0.2 e^{-j\\omega}) (1 - 0.2 e^{j\\omega}) = 1 - 0.2 e^{j\\omega} - 0.2 e^{-j\\omega} + (0.2)^2$$\n$$= 1 - 0.2(e^{j\\omega} + e^{-j\\omega}) + 0.04 = 1 - 0.4 \\cos(\\omega) + 0.04 = 1.04 - 0.4 \\cos(\\omega)$$\nSubstituting this back into the PSD expression with $\\sigma_{e}^{2} = 2$:\n$$S_{x}(\\omega) = 2 \\cdot \\frac{1}{1.04 - 0.4 \\cos(\\omega)} = \\frac{2}{1.04 - 0.4 \\cos(\\omega)}$$\nTo express this with integer coefficients, we convert decimals to fractions: $1.04 = \\frac{104}{100} = \\frac{26}{25}$ and $0.4 = \\frac{4}{10} = \\frac{2}{5} = \\frac{10}{25}$.\n$$S_{x}(\\omega) = \\frac{2}{\\frac{26}{25} - \\frac{10}{25} \\cos(\\omega)} = \\frac{2 \\cdot 25}{26 - 10 \\cos(\\omega)} = \\frac{50}{26 - 10 \\cos(\\omega)}$$\nThis expression can be simplified by dividing the numerator and denominator by $2$:\n$$S_{x}(\\omega) = \\frac{25}{13 - 5 \\cos(\\omega)}$$\nThis is the final closed-form expression for the power spectral density of the output signal $x[n]$.\n\n**3. Implications of Pole-Zero Cancellation**\n\nThe existence of a pole-zero cancellation in an ARMA model has critical implications for system identification.\nThe system is specified as an ARMA(2,1) model, which has $p=2$ AR parameters and $q=1$ MA parameter, for a total of $3$ filter coefficients to be determined, plus the noise variance $\\sigma_{e}^2$. However, due to the cancellation, its true input-output behavior is indistinguishable from that of a simpler AR(1) model, which has only one AR parameter. This means the original ARMA(2,1) model is **over-parameterized** and **not identifiable**.\n\nFor practical estimation algorithms (e.g., maximum likelihood, prediction error minimization), this lack of identifiability manifests as a non-unique solution. The optimization cost function will have a \"valley\" or flat region where any combination of parameters corresponding to a cancelled pole-zero pair yields the same minimum error. An iterative algorithm attempting to find the parameters will exhibit poor convergence, numerical instability, and high variance in the estimated parameters associated with the cancelled pole and zero. The algorithm might converge to an arbitrary point in this valley, or it may fail to converge at all. Because there are infinitely many parameter sets that describe the same system dynamics, a unique set of \"true\" parameters for the over-parameterized model cannot be recovered from the output data. A model selection procedure, such as one based on the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), would correctly favor the more parsimonious AR(1) model.", "answer": "$$\\boxed{\\frac{25}{13 - 5 \\cos(\\omega)}}$$", "id": "2889653"}, {"introduction": "Having explored the analytical properties of parametric models, the next step is to estimate these models from observed data, which is the central task in practical spectral estimation. This hands-on exercise challenges you to implement the Burg algorithm, a powerful and widely used method for fitting AR models that guarantees a stable result. By writing the code to perform the order-recursive estimation yourself, you will gain a concrete understanding of how reflection coefficients and prediction errors are used to build a spectral model from a raw time series [@problem_id:2889606].", "problem": "You are asked to implement a complete, reproducible program that estimates an autoregressive (AR) model by the Burg recursion and then evaluates the corresponding parametric power spectral density (PSD) on specified frequency grids. The program must start from the fundamental definitions of linear prediction for stationary, zero-mean discrete-time signals and the autoregressive (AR) all-pole model driven by white noise.\n\nYour implementation must proceed from the following foundational base:\n- The AR model is defined by the linear difference equation $x[n] + \\sum_{k=1}^{p} a_k \\, x[n-k] = e[n]$, where $x[n]$ is the real, discrete-time signal, $p$ is the AR order, $\\{a_k\\}$ are the AR coefficients, and $e[n]$ is a zero-mean, independent and identically distributed white-noise process with variance $\\sigma_e^2$.\n- Linear prediction minimizes the mean-squared one-step prediction error subject to causality and the model order constraint.\n- The Burg recursion determines reflection coefficients order-by-order by minimizing, at each order $m$, the sum of the squared forward and backward prediction errors computed over the available data, while preserving the stability condition for the all-pole model. The recursion then maps the reflection coefficients to the AR polynomial coefficients in a way that preserves stability.\n- The parametric PSD of the AR model is the spectrum of the stable all-pole system driven by white noise of variance $\\sigma_e^2$ implied by the AR polynomial, evaluated over a prescribed grid of angular frequencies in radians.\n\nYour program must:\n1. Implement a numerically stable, real-valued Burg recursion that, given a real data sequence $x[n]$ and an integer order $p \\ge 0$, returns the AR coefficient vector $[1, a_1, \\dots, a_p]$ and the estimated prediction error variance $\\sigma_e^2$. Use the unbiased initial error variance equal to the sample mean-square, and ensure that the recursion respects the stability constraint at each order.\n2. Given the AR coefficients and the error variance, compute the parametric PSD implied by the stable all-pole model on specified angular frequency grids (in radians). Angles are in radians. You must evaluate the spectrum implied by the all-pole filter corresponding to the AR polynomial and the driving variance. Do not use any external spectral estimation functions.\n3. For each test case, output the natural logarithm of the PSD values, rounded to six decimal places, evaluated at the prescribed angular frequencies. This rounding is required for the final output format.\n\nTest Suite:\nProvide results for the following three deterministic test cases. In each case, all angles are in radians, and the natural logarithm is the base-$e$ logarithm.\n\n- Test case $1$ (happy path, two tones, moderate order):\n  - Data: $x^{(1)}[n] = \\sin(2 \\pi \\cdot 0.2 \\cdot n) + 0.5 \\, \\sin(2 \\pi \\cdot 0.35 \\cdot n)$ for $n = 0, 1, \\dots, 63$.\n  - Order: $p^{(1)} = 4$.\n  - Frequency grid: $\\Omega^{(1)} = \\{0.4 \\pi, 0.7 \\pi, 0.2 \\pi\\}$.\n\n- Test case $2$ (boundary condition, $p=0$ white model):\n  - Data: $x^{(2)}[n] = (-1)^n$ for $n = 0, 1, \\dots, 7$.\n  - Order: $p^{(2)} = 0$.\n  - Frequency grid: $\\Omega^{(2)} = \\{0, 0.5 \\pi, \\pi\\}$.\n\n- Test case $3$ (higher order relative to short data, smooth spectrum):\n  - Data: $x^{(3)}[n] = (0.9)^n$ for $n = 0, 1, \\dots, 15$.\n  - Order: $p^{(3)} = 8$.\n  - Frequency grid: $\\Omega^{(3)} = \\{0, 0.15 \\pi, 0.8 \\pi\\}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the lists of rounded natural-log PSD values for the three test cases in order, aggregated as a comma-separated list of lists enclosed in square brackets and using standard Python literal formatting. Concretely, the output must be of the form\n$[[r_{1,1}, r_{1,2}, r_{1,3}], [r_{2,1}, r_{2,2}, r_{2,3}], [r_{3,1}, r_{3,2}, r_{3,3}]]$\nwhere each $r_{i,j}$ is a float with six decimal places produced by rounding the natural logarithm of the PSD at the $j$-th frequency of the $i$-th test case.\n\nAngle unit: radians. No physical units apply. The output values must be floats. The program must not read any input and must not write any auxiliary text. It must only print the single required line in the exact format described above.", "solution": "The problem posed is the implementation of the Burg algorithm for autoregressive (AR) model parameter estimation and the subsequent evaluation of the parametric power spectral density (PSD). The problem is well-defined, scientifically sound, and internally consistent.\n\nThe fundamental principles underlying the solution are derived from the theory of linear prediction and stationary time series analysis.\n\nAn autoregressive process of order $p$, denoted AR($p$), models a discrete-time signal $x[n]$ as the output of an all-pole filter driven by white noise $e[n]$. The governing linear constant-coefficient difference equation is:\n$$\nx[n] + \\sum_{k=1}^{p} a_k x[n-k] = e[n]\n$$\nHere, $\\{a_k\\}_{k=1}^p$ are the AR coefficients, and $e[n]$ is a zero-mean white noise process with variance $\\sigma_e^2$. The transfer function of the synthesis filter from the input $e[n]$ to the output $x[n]$ is an all-pole filter $H(z)$:\n$$\nH(z) = \\frac{1}{A(z)} = \\frac{1}{1 + \\sum_{k=1}^{p} a_k z^{-k}}\n$$\nThe power spectral density (PSD) of the AR($p$) process is given by the squared magnitude of this transfer function evaluated on the unit circle ($z = e^{j\\omega}$), scaled by the noise variance $\\sigma_e^2$:\n$$\nP_{AR}(\\omega) = \\sigma_e^2 |H(e^{j\\omega})|^2 = \\frac{\\sigma_e^2}{\\left|1 + \\sum_{k=1}^{p} a_k e^{-jk\\omega}\\right|^2}\n$$\nwhere $\\omega$ is the angular frequency in radians.\n\nThe task is to estimate the parameters $\\{a_k\\}$ and $\\sigma_e^2$ from a finite data sequence $x[n]$ for $n=0, 1, \\dots, N-1$. The Burg method is specified for this estimation.\n\nThe Burg algorithm is an order-recursive method that estimates the reflection coefficients $k_m$ for $m=1, \\dots, p$. It is renowned for yielding a stable model, meaning the poles of $H(z)$ are guaranteed to be inside the unit circle, which requires $|k_m| < 1$. This stability is a direct consequence of its derivation. At each order $m$, Burg's method minimizes the sum of the average powers of the forward and backward prediction errors.\n\nLet $e_m^f[n]$ and $e_m^b[n]$ be the forward and backward prediction errors of order $m$, respectively. The algorithm proceeds as follows:\n\n1.  **Initialization (Order $m=0$)**:\n    The $0$-th order model assumes the signal is white noise. The forward and backward prediction errors are simply the signal itself: $e_0^f[n] = x[n]$ and $e_0^b[n] = x[n]$ for $n=0, \\dots, N-1$. The initial prediction error power, $P_0$, is the sample mean-square of the data, as specified:\n    $$\n    P_0 = \\frac{1}{N} \\sum_{n=0}^{N-1} x[n]^2\n    $$\n    For $p=0$, the coefficient vector is $[1]$ and the error variance is $P_0$.\n\n2.  **Recursion (Order $m = 1, \\dots, p$)**:\n    For each order $m$, the reflection coefficient $k_m$ is calculated to minimize the total error energy. This yields:\n    $$\n    k_m = \\frac{-2 \\sum_{n=m}^{N-1} e_{m-1}^f[n] \\, e_{m-1}^b[n-1]}{\\sum_{n=m}^{N-1} \\left( |e_{m-1}^f[n]|^2 + |e_{m-1}^b[n-1]|^2 \\right)}\n    $$\n    The AR coefficients are then updated using the Levinson-Durbin recursion. If $\\{a_k^{(m-1)}\\}_{k=1}^{m-1}$ are the coefficients for the order-($m-1$) model, the coefficients for the order-$m$ model are found as:\n    $$\n    a_m^{(m)} = k_m\n    $$\n    $$\n    a_k^{(m)} = a_k^{(m-1)} + k_m a_{m-k}^{(m-1)} \\quad \\text{for } k=1, \\dots, m-1\n    $$\n    The prediction error variance is updated by:\n    $$\n    P_m = P_{m-1}(1 - k_m^2)\n    $$\n    Finally, the forward and backward error sequences are updated for the next iteration:\n    $$\n    e_m^f[n] = e_{m-1}^f[n] + k_m e_{m-1}^b[n-1]\n    $$\n    $$\n    e_m^b[n] = e_{m-1}^b[n-1] + k_m e_{m-1}^f[n]\n    $$\n    These new error sequences are defined for $n=m, \\dots, N-1$.\n\nAfter completing the recursion up to order $p$, the algorithm provides the final AR coefficient vector $[a_1^{(p)}, \\dots, a_p^{(p)}]$ (to which $a_0 = 1$ is prepended) and the final error variance $\\sigma_e^2 = P_p$.\n\nWith these estimated parameters, the PSD is calculated using the formula for $P_{AR}(\\omega)$ at the specified frequencies $\\Omega^{(i)}$. The natural logarithm of the resulting PSD values is then computed and rounded to six decimal places, as required for the final output. The implementation will handle the case where the denominator of the PSD is very close to zero, which can occur for models of deterministic signals with poles on or near the unit circle.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the Burg recursion for AR model estimation\n    and calculating the parametric PSD for three specified test cases.\n    \"\"\"\n\n    def burg_ar(x: np.ndarray, p: int) -> tuple[np.ndarray, float]:\n        \"\"\"\n        Estimates AR model parameters using the Burg recursion.\n\n        Args:\n            x: Real-valued 1D data sequence.\n            p: The desired AR model order, p >= 0.\n\n        Returns:\n            A tuple containing:\n            - The AR coefficient vector [1, a_1, ..., a_p].\n            - The estimated prediction error variance sigma_e^2.\n        \"\"\"\n        N = len(x)\n        x = np.asarray(x, dtype=float)\n\n        if p == 0:\n            a = np.array([1.0])\n            var_e = np.mean(x**2)\n            return a, var_e\n\n        # Initialization (m=0)\n        P = np.mean(x**2)\n        a_coeffs = np.array([], dtype=float)  # Stores [a_1, a_2, ...]\n        ef = x.copy()  # Forward prediction error\n        eb = x.copy()  # Backward prediction error\n\n        # Recursion from m=1 to p\n        for m in range(1, p + 1):\n            # Calculate reflection coefficient k_m\n            # Slices correspond to valid time indices for error sequences at step m\n            ef_slice = ef[1:]\n            eb_slice = eb[:-1]\n\n            numerator = -2.0 * np.dot(ef_slice, eb_slice)\n            denominator = np.dot(ef_slice, ef_slice) + np.dot(eb_slice, eb_slice)\n\n            if denominator < 1e-20:  # Avoid division by zero\n                k = 0.0\n            else:\n                k = numerator / denominator\n\n            # Update AR coefficients using Levinson recursion\n            if m > 1:\n                a_coeffs = a_coeffs + k * a_coeffs[::-1]\n            a_coeffs = np.append(a_coeffs, k)\n\n            # Update error variance\n            P *= (1.0 - k**2)\n\n            # Update prediction errors for the next iteration\n            ef_new = ef_slice + k * eb_slice\n            eb_new = eb_slice + k * ef_slice\n            ef, eb = ef_new, eb_new\n\n        # Prepend a_0=1 to the coefficient vector\n        final_a = np.concatenate(([1.0], a_coeffs))\n        return final_a, P\n\n    def calculate_psd(a: np.ndarray, var_e: float, freqs: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Calculates the PSD from AR model parameters.\n\n        Args:\n            a: AR coefficient vector [1, a_1, ..., a_p].\n            var_e: Prediction error variance.\n            freqs: Array of angular frequencies (in radians) to evaluate the PSD.\n\n        Returns:\n            An array of PSD values corresponding to the frequencies in `freqs`.\n        \"\"\"\n        p = len(a) - 1\n        if p == 0:\n            return np.full_like(freqs, var_e, dtype=float)\n\n        psd_vals = []\n        k_vec = np.arange(p + 1)\n        for w in freqs:\n            A_exp_jw = np.sum(a * np.exp(-1j * k_vec * w))\n            den = np.abs(A_exp_jw)**2\n            \n            # Use a small epsilon to handle cases where poles are on the unit circle\n            if den < 1e-20:\n                psd = np.inf\n            else:\n                psd = var_e / den\n            psd_vals.append(psd)\n            \n        return np.array(psd_vals)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"data_func\": lambda n: np.sin(2 * np.pi * 0.2 * n) + 0.5 * np.sin(2 * np.pi * 0.35 * n),\n            \"n_range\": np.arange(64),\n            \"p\": 4,\n            \"freqs\": np.array([0.4 * np.pi, 0.7 * np.pi, 0.2 * np.pi])\n        },\n        # Test case 2\n        {\n            \"data_func\": lambda n: (-1)**n,\n            \"n_range\": np.arange(8),\n            \"p\": 0,\n            \"freqs\": np.array([0, 0.5 * np.pi, np.pi])\n        },\n        # Test case 3\n        {\n            \"data_func\": lambda n: 0.9**n,\n            \"n_range\": np.arange(16),\n            \"p\": 8,\n            \"freqs\": np.array([0, 0.15 * np.pi, 0.8 * np.pi])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Generate data\n        x = case[\"data_func\"](case[\"n_range\"])\n        \n        # Estimate AR parameters\n        a, var_e = burg_ar(x, case[\"p\"])\n        \n        # Calculate PSD\n        psd_values = calculate_psd(a, var_e, case[\"freqs\"])\n        \n        # Compute natural log and round\n        with np.errstate(divide='ignore'): # suppress log(0) warning\n           log_psd = np.log(psd_values)\n        rounded_log_psd = np.round(log_psd, 6).tolist()\n        \n        results.append(rounded_log_psd)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required output format.\n    print(str(results).replace(\"'inf'\", \"float('inf')\"))\n\n\nsolve()\n```", "id": "2889606"}]}