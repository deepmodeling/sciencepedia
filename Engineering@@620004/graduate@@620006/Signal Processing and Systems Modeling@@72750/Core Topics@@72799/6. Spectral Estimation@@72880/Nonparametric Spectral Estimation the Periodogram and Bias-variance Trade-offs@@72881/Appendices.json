{"hands_on_practices": [{"introduction": "This first practice tackles a common pitfall in digital signal processing where visual appeal can be misleading. Through a direct computational experiment [@problem_id:2887395], you will verify that zero-padding a signal before computing its Discrete Fourier Transform (DFT) merely interpolates the underlying Discrete-Time Fourier Transform (DTFT), increasing the density of frequency samples. This exercise is crucial for developing the intuition that such an operation does not reduce the estimator's variance or improve its fundamental statistical resolution, which is determined by the original data length $N$.", "problem": "You are given the task of empirically demonstrating, using reproducible computation, that zero-padding a finite-length discrete-time signal prior to computing the Discrete Fourier Transform (DFT) increases the density of the frequency grid (apparent resolution) but does not reduce the variance of the periodogram at frequencies that are common to both the unpadded and padded DFT grids. Base your reasoning and implementation on the following foundational definitions:\n- The discrete-time sequence is of length $N$ samples.\n- The Discrete Fourier Transform (DFT) of a length-$N$ sequence evaluated at index $k$ corresponds to normalized radian frequency $\\omega_k = 2\\pi k/N$.\n- Zero-padding to length $M = zN$ (with integer zero-padding factor $z \\ge 1$) yields the $M$-point DFT with grid $\\omega^{(M)}_k = 2\\pi k/M$.\n- The periodogram at frequency $\\omega$ is the squared magnitude of the finite-length Discrete-Time Fourier Transform (DTFT), normalized by the number of observed samples. Specifically, for a sequence $x[n]$, the periodogram at a grid frequency corresponding to DFT index $k$ is $I(\\omega_k) = \\frac{1}{N}\\lvert X[k]\\rvert^2$, where $X[k]$ is the DFT of $x[n]$ at index $k$.\n\nYour program must implement the following, strictly adhering to these definitions:\n- For each independent trial, generate a real-valued discrete-time sequence $x[n]$ of length $N$ using one of two processes:\n  1. Zero-mean independent and identically distributed Gaussian (normal) noise with variance $\\sigma^2$, denoted $\\mathcal{N}(0,\\sigma^2)$.\n  2. A real cosine tone of the form $A\\cos(2\\pi m_0 n/N + \\phi)$ with random phase $\\phi$ uniformly distributed in $[0,2\\pi)$, added to independent Gaussian noise $\\mathcal{N}(0,\\sigma^2)$, all in discrete time $n=0,1,\\dots,N-1$. Angles are in radians.\n- Use a rectangular window (no additional weighting) prior to the DFT.\n- Compute, for each trial, two periodogram values at the same normalized radian frequency $\\omega_m = 2\\pi m/N$ for a chosen integer $m$:\n  - Unpadded: compute the $N$-point DFT $X_N[k]$ of $x[n]$ and form $I_N(\\omega_m)=\\frac{1}{N}\\lvert X_N[m]\\rvert^2$.\n  - Zero-padded: compute the $M$-point DFT $X_M[k]$ of the sequence formed by appending $M-N$ zeros to $x[n]$ and form $I_M(\\omega_m)=\\frac{1}{N}\\lvert X_M[z\\,m]\\rvert^2$, where $k=z\\,m$ aligns the same frequency on the denser grid. Note the normalization by $N$ in both cases.\n- Over $T$ independent trials, compute the unbiased sample variance (with divisor $T-1$) of $\\{I_N(\\omega_m)\\}$ and of $\\{I_M(\\omega_m)\\}$ across trials. Then compute the variance ratio $r = \\mathrm{Var}[I_M(\\omega_m)] / \\mathrm{Var}[I_N(\\omega_m)]$.\n- Also compute the frequency grid spacing ratio $g = \\Delta\\omega_M / \\Delta\\omega_N = (2\\pi/M)/(2\\pi/N) = N/M = 1/z$.\n\nYour program must process the following test suite. For each case, use the specified parameters exactly and assume $\\sigma^2 = 1$ for the Gaussian noise variance unless otherwise specified:\n- Case $1$ (happy path, noise only): $N=256$, $z=8$, $T=400$, process is $\\mathcal{N}(0,1)$, frequency index $m=37$.\n- Case $2$ (tone plus noise, on-grid sinusoid): $N=256$, $z=8$, $T=300$, process is $x[n] = \\cos(2\\pi m_0 n/N + \\phi) + w[n]$, with $m_0=45$, $A=1$, $w[n]\\sim \\mathcal{N}(0,1)$, and $\\phi\\sim \\mathrm{Uniform}[0,2\\pi)$ independently per trial; evaluate at $m=m_0$.\n- Case $3$ (edge case, very short record, heavy padding): $N=16$, $z=16$, $T=1000$, process is $\\mathcal{N}(0,1)$, frequency index $m=3$.\n- Case $4$ (boundary condition, no padding): $N=128$, $z=1$, $T=300$, process is $\\mathcal{N}(0,1)$, frequency index $m=17$.\n\nFor each case, compute:\n- The variance ratio $r$.\n- The grid spacing ratio $g$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, in the order $[r_1, g_1, r_2, g_2, r_3, g_3, r_4, g_4]$, where the subscript denotes the case index. All values are dimensionless. No additional text should be printed.", "solution": "The problem statement presented is a standard, well-posed exercise in computational signal processing. It is scientifically sound, contains no contradictions, and provides all necessary parameters for a reproducible empirical study. The objective is to demonstrate a fundamental property of the periodogram spectral estimator concerning zero-padding. As such, the problem is valid and I will proceed with its solution.\n\nThe core of this problem lies in the mathematical relationship between the Discrete Fourier Transform (DFT), the Discrete-Time Fourier Transform (DTFT), and zero-padding. It is a common misconception among students that zero-padding a signal to increase the number of points in a DFT somehow improves the \"resolution\" of the resulting spectrum in a statistical sense. This exercise is designed to demonstrate, through direct computation, that this is false. Zero-padding increases the *density* of the frequency grid, which can be seen as a form of interpolation, but it does not decrease the variance of the spectral estimate, nor does it improve the fundamental ability to resolve closely-spaced sinusoids, which is dictated by the original signal length $N$.\n\nLet us first establish the theoretical foundation. A discrete-time signal $x[n]$ of finite length $N$, defined for $n \\in \\{0, 1, \\dots, N-1\\}$, has a DTFT given by:\n$$\nX(e^{j\\omega}) = \\sum_{n=0}^{N-1} x[n] e^{-j\\omega n}\n$$\nThe DTFT is a continuous function of the normalized radian frequency $\\omega$. The $N$-point DFT of this signal, denoted $X_N[k]$, is a set of samples of its DTFT at the frequencies $\\omega_k = \\frac{2\\pi k}{N}$ for $k \\in \\{0, 1, \\dots, N-1\\}$. That is:\n$$\nX_N[k] = X(e^{j\\omega}) \\Big|_{\\omega = \\frac{2\\pi k}{N}} = \\sum_{n=0}^{N-1} x[n] e^{-j\\frac{2\\pi kn}{N}}\n$$\nNow, consider a new sequence $x_p[n]$ of length $M = zN$ (where $z \\ge 1$ is an integer padding factor) which is formed by appending $M-N$ zeros to $x[n]$. Its $M$-point DFT, $X_M[k]$, is:\n$$\nX_M[k] = \\sum_{n=0}^{M-1} x_p[n] e^{-j\\frac{2\\pi kn}{M}} = \\sum_{n=0}^{N-1} x[n] e^{-j\\frac{2\\pi kn}{M}}\n$$\nNotice that the summation limit is $N-1$ because $x_p[n]$ is zero for $n \\ge N$. By comparing this to the definition of the DTFT, we see that $X_M[k]$ is simply the DTFT of the *original* sequence $x[n]$ sampled at the new, denser frequency grid $\\omega^{(M)}_k = \\frac{2\\pi k}{M}$:\n$$\nX_M[k] = X(e^{j\\omega}) \\Big|_{\\omega = \\frac{2\\pi k}{M}}\n$$\nThe problem requires us to compare periodogram values at a specific frequency common to both grids, $\\omega_m = \\frac{2\\pi m}{N}$. For the unpadded case, this corresponds to the DFT index $m$. For the padded case, we must find the index $k'$ on the $M$-point grid such that $\\frac{2\\pi k'}{M} = \\frac{2\\pi m}{N}$. Solving for $k'$ gives $k' = m \\frac{M}{N} = m \\cdot z$.\n\nTherefore, the DFT value for the padded sequence at index $k' = zm$ is:\n$$\nX_M[zm] = \\sum_{n=0}^{N-1} x[n] e^{-j\\frac{2\\pi (zm)n}{M}} = \\sum_{n=0}^{N-1} x[n] e^{-j\\frac{2\\pi (zm)n}{zN}} = \\sum_{n=0}^{N-1} x[n] e^{-j\\frac{2\\pi mn}{N}} = X_N[m]\n$$\nThis identity is the crux of the matter. For any given signal realization $x[n]$, the DFT value at frequency $\\omega_m$ is precisely the same whether it is computed using an $N$-point DFT or an $M$-point DFT of the zero-padded signal.\n\nThe periodogram, as defined in the problem, is $I(\\omega_k) = \\frac{1}{N} \\lvert X[k] \\rvert^2$. Consequently, for any single trial, the periodogram values must be identical:\n$$\nI_N(\\omega_m) = \\frac{1}{N} \\lvert X_N[m] \\rvert^2 = \\frac{1}{N} \\lvert X_M[zm] \\rvert^2 = I_M(\\omega_m)\n$$\nSince the sequence of values $\\{I_N(\\omega_m)\\}$ obtained over $T$ trials is identical to the sequence $\\{I_M(\\omega_m)\\}$, their sample variances must also be identical.\n$$\n\\mathrm{Var}[\\{I_N(\\omega_m)\\}] = \\frac{1}{T-1} \\sum_{i=1}^{T} (I_N(\\omega_m)^{(i)} - \\bar{I}_N)^2 = \\frac{1}{T-1} \\sum_{i=1}^{T} (I_M(\\omega_m)^{(i)} - \\bar{I}_M)^2 = \\mathrm{Var}[\\{I_M(\\omega_m)\\}]\n$$\nThus, the theoretical value for the variance ratio $r = \\mathrm{Var}[I_M(\\omega_m)] / \\mathrm{Var}[I_N(\\omega_m)]$ is exactly $1$. Any deviation in the computed result will be due to floating-point numerical imprecision, which should be negligible. The frequency grid spacing ratio $g = \\Delta\\omega_M / \\Delta\\omega_N = (2\\pi/M)/(2\\pi/N) = N/M = 1/z$ is a simple calculation.\n\nThe implementation will proceed as follows:\n$1$. For each of the four test cases, specified by parameters $N$, $z$, $T$, $m$, and the signal generation process, we will perform a Monte Carlo simulation.\n$2$. We will loop $T$ times. In each iteration, we generate a new realization of the random signal $x[n]$ of length $N$.\n$3$. For the noise-only process, $x[n]$ is drawn from $\\mathcal{N}(0, 1)$.\n$4$. For the tone-plus-noise process, we generate $x[n] = A\\cos(2\\pi m_0 n/N + \\phi) + w[n]$, where $w[n] \\sim \\mathcal{N}(0, 1)$ and the phase $\\phi$ is drawn anew for each trial from $\\mathrm{Uniform}[0, 2\\pi)$.\n$5$. For each generated $x[n]$, we compute two periodogram values at the frequency of interest $\\omega_m = 2\\pi m/N$:\n    a. The unpadded value $I_N(\\omega_m) = \\frac{1}{N}\\lvert X_N[m] \\rvert^2$, where $X_N$ is the $N$-point DFT of $x[n]$.\n    b. The padded value $I_M(\\omega_m) = \\frac{1}{N}\\lvert X_M[zm] \\rvert^2$, where $X_M$ is the $M$-point DFT of $x[n]$ appended with $M-N$ zeros.\n$6$. We store these two values, collecting $T$ pairs in total.\n$7$. After all trials, we compute the unbiased sample variance (with divisor $T-1$) of the collected $\\{I_N(\\omega_m)\\}$ values and $\\{I_M(\\omega_m)\\}$ values.\n$8$. We compute their ratio $r$ and the grid spacing ratio $g=1/z$.\n$9$. The results $[r_1, g_1, r_2, g_2, r_3, g_3, r_4, g_4]$ are then formatted into the required output string.\n\nThis procedure will computationally verify the theoretical conclusion that $r=1$, demonstrating that zero-padding does not reduce periodogram variance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the effect of zero-padding on the periodogram variance\n    and frequency grid density.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, noise only): N=256, z=8, T=400, process is N(0,1), frequency index m=37.\n        {'N': 256, 'z': 8, 'T': 400, 'process': 'noise', 'm': 37, 'A': None, 'm0': None},\n        # Case 2 (tone plus noise, on-grid sinusoid): N=256, z=8, T=300, process is cos(...) + N(0,1), m0=45, A=1, m=m0.\n        {'N': 256, 'z': 8, 'T': 300, 'process': 'tone', 'm': 45, 'A': 1.0, 'm0': 45},\n        # Case 3 (edge case, very short record, heavy padding): N=16, z=16, T=1000, process is N(0,1), m=3.\n        {'N': 16, 'z': 16, 'T': 1000, 'process': 'noise', 'm': 3, 'A': None, 'm0': None},\n        # Case 4 (boundary condition, no padding): N=128, z=1, T=300, process is N(0,1), m=17.\n        {'N': 128, 'z': 1, 'T': 300, 'process': 'noise', 'm': 17, 'A': None, 'm0': None}\n    ]\n\n    results = []\n    # Default noise variance sigma^2 = 1, so standard deviation sigma = 1.\n    sigma = 1.0\n\n    for case in test_cases:\n        N = case['N']\n        z = case['z']\n        T = case['T']\n        process = case['process']\n        m = case['m']\n        A = case['A']\n        m0 = case['m0']\n\n        M = z * N\n\n        # Arrays to store periodogram values from each trial\n        periodograms_N = np.zeros(T, dtype=np.float64)\n        periodograms_M = np.zeros(T, dtype=np.float64)\n\n        for i in range(T):\n            # Step 1: Generate the signal x[n] of length N.\n            noise = np.random.normal(loc=0.0, scale=sigma, size=N)\n            \n            if process == 'noise':\n                x = noise\n            elif process == 'tone':\n                n_vec = np.arange(N)\n                # Random phase phi is drawn independently for each trial.\n                phi = np.random.uniform(0, 2 * np.pi)\n                # Signal is a cosine tone plus Gaussian noise.\n                x = A * np.cos(2 * np.pi * m0 * n_vec / N + phi) + noise\n\n            # Step 2: Compute the unpadded periodogram value.\n            # The N-point DFT is computed.\n            X_N = np.fft.fft(x)\n            # Periodogram at frequency index m, normalized by N.\n            I_N = (1 / N) * np.abs(X_N[m])**2\n            periodograms_N[i] = I_N\n\n            # Step 3: Compute the zero-padded periodogram value.\n            # The M-point DFT is computed by specifying n=M, which zero-pads automatically.\n            X_M = np.fft.fft(x, n=M)\n            # The frequency index that corresponds to the unpadded grid is zm.\n            I_M = (1 / N) * np.abs(X_M[z * m])**2\n            periodograms_M[i] = I_M\n\n        # Step 4: Compute the ratio of unbiased sample variances.\n        # ddof=1 computes the sample variance (division by T-1).\n        var_N = np.var(periodograms_N, ddof=1)\n        var_M = np.var(periodograms_M, ddof=1)\n\n        # The theoretical analysis shows periodograms_N and periodograms_M are identical,\n        # so var_N and var_M should be identical, and their ratio 1.\n        # A check for var_N == 0 is robust but unlikely to be needed.\n        if var_N == 0:\n            # If var_N is zero, var_M must also be zero. Their ratio is 1.\n            r = 1.0\n        else:\n            r = var_M / var_N\n\n        # Step 5: Compute the frequency grid spacing ratio.\n        g = N / M  # This is equivalent to 1/z.\n\n        results.extend([r, g])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2887395"}, {"introduction": "Building on the need for robust methods, this exercise exposes a deeper pathology that can arise from naive approaches to spectral estimation. Using a handcrafted, simple data sequence [@problem_id:2887459], you will compute the sample autocovariance and discover that its direct Fourier transform can yield a physically impossible negative power spectral density estimate. This striking result demonstrates why the sequence of sample autocovariances is not guaranteed to be a valid (nonnegative-definite) function and motivates the use of tapering lag windows to ensure a valid spectral estimate.", "problem": "A central challenge in nonparametric spectral estimation is that the unbiased sample autocovariance can have large variance at large lags, and naive truncation can produce a spectral estimate that is not nonnegative. Consider the following explicit example.\n\nYou observe a zero-mean, real-valued, discrete-time record of length $N=4$,\n$x[0]=2$, $x[1]=-2$, $x[2]=2$, $x[3]=-2$. The unbiased sample autocovariance is defined for $k \\in \\{0,1,2,3\\}$ by\n$$\n\\widehat{\\gamma}(k) \\triangleq \\frac{1}{N-k}\\sum_{n=0}^{N-1-k} x[n]\\,x[n+k],\n$$\nand $\\widehat{\\gamma}(-k)\\triangleq \\widehat{\\gamma}(k)$ by even symmetry. The naive truncated Blackman–Tukey estimator uses the rectangular lag window $w_{\\mathrm{rect}}(k)=1$ for $|k|\\leq M$ and $0$ otherwise, where $M=N-1=3$, and forms\n$$\n\\widehat{S}_{\\mathrm{rect}}(f)\\triangleq \\sum_{k=-M}^{M} \\widehat{\\gamma}(k)\\,\\exp\\!\\big(-j\\,2\\pi f k\\big).\n$$\nEvaluate $\\widehat{S}_{\\mathrm{rect}}(f)$ at the frequency $f_{0}=\\frac{1}{4}$ cycles per sample.\n\nThen, to correct the pathology, choose a valid (nonnegative-definite) lag window, namely the Bartlett window\n$$\nw_{\\mathrm{Bart}}(k)\\triangleq \\begin{cases}\n1-\\dfrac{|k|}{M+1},  |k|\\leq M \\\\\n0,  \\text{otherwise}\n\\end{cases}\n$$\nwith $M=3$, and form the Bartlett-windowed Blackman–Tukey estimate\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f)\\triangleq \\sum_{k=-M}^{M} w_{\\mathrm{Bart}}(k)\\,\\widehat{\\gamma}(k)\\,\\exp\\!\\big(-j\\,2\\pi f k\\big).\n$$\nEvaluate $\\widehat{S}_{\\mathrm{Bart}}(f)$ at the same frequency $f_{0}=\\frac{1}{4}$.\n\nReport your final answer as the ordered pair $\\big(\\widehat{S}_{\\mathrm{rect}}(f_{0}),\\,\\widehat{S}_{\\mathrm{Bart}}(f_{0})\\big)$. No rounding is required. Provide no units.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded exercise in nonparametric spectral estimation, a core topic within signal processing. All definitions and data are standard, complete, and consistent. We will proceed with the calculation.\n\nThe first step is to compute the unbiased sample autocovariance, $\\widehat{\\gamma}(k)$, for the given data record. The record is $x[0]=2$, $x[1]=-2$, $x[2]=2$, $x[3]=-2$, with length $N=4$. The definition is:\n$$\n\\widehat{\\gamma}(k) = \\frac{1}{N-k}\\sum_{n=0}^{N-1-k} x[n]\\,x[n+k]\n$$\nWe compute this for $k \\in \\{0, 1, 2, 3\\}$.\n\nFor $k=0$:\n$$\n\\widehat{\\gamma}(0) = \\frac{1}{4-0}\\sum_{n=0}^{3} x[n]^{2} = \\frac{1}{4}\\left(x[0]^{2} + x[1]^{2} + x[2]^{2} + x[3]^{2}\\right) = \\frac{1}{4}\\left(2^{2} + (-2)^{2} + 2^{2} + (-2)^{2}\\right) = \\frac{1}{4}(4+4+4+4) = 4\n$$\n\nFor $k=1$:\n$$\n\\widehat{\\gamma}(1) = \\frac{1}{4-1}\\sum_{n=0}^{2} x[n]x[n+1] = \\frac{1}{3}\\left(x[0]x[1] + x[1]x[2] + x[2]x[3]\\right) = \\frac{1}{3}\\left((2)(-2) + (-2)(2) + (2)(-2)\\right) = \\frac{1}{3}(-4-4-4) = -4\n$$\n\nFor $k=2$:\n$$\n\\widehat{\\gamma}(2) = \\frac{1}{4-2}\\sum_{n=0}^{1} x[n]x[n+2] = \\frac{1}{2}\\left(x[0]x[2] + x[1]x[3]\\right) = \\frac{1}{2}\\left((2)(2) + (-2)(-2)\\right) = \\frac{1}{2}(4+4) = 4\n$$\n\nFor $k=3$:\n$$\n\\widehat{\\gamma}(3) = \\frac{1}{4-3}\\sum_{n=0}^{0} x[n]x[n+3] = \\frac{1}{1}\\left(x[0]x[3]\\right) = (2)(-2) = -4\n$$\n\nBy definition, the autocovariance is an even function, so $\\widehat{\\gamma}(-k) = \\widehat{\\gamma}(k)$.\n\nNext, we evaluate the naive truncated Blackman–Tukey estimate, $\\widehat{S}_{\\mathrm{rect}}(f)$, at frequency $f_{0}=\\frac{1}{4}$. The truncation parameter is $M=N-1=3$.\n$$\n\\widehat{S}_{\\mathrm{rect}}(f_{0}) = \\sum_{k=-3}^{3} \\widehat{\\gamma}(k)\\,\\exp\\!\\left(-j\\,2\\pi f_{0} k\\right) = \\sum_{k=-3}^{3} \\widehat{\\gamma}(k)\\,\\exp\\!\\left(-j\\,2\\pi \\frac{1}{4} k\\right) = \\sum_{k=-3}^{3} \\widehat{\\gamma}(k)\\,\\exp\\!\\left(-j\\,\\frac{\\pi}{2} k\\right)\n$$\nUsing the even symmetry of $\\widehat{\\gamma}(k)$ and the property $\\exp(j\\theta) + \\exp(-j\\theta) = 2\\cos(\\theta)$, we can rewrite the sum:\n$$\n\\widehat{S}_{\\mathrm{rect}}(f_{0}) = \\widehat{\\gamma}(0) + \\sum_{k=1}^{3} \\left(\\widehat{\\gamma}(k)\\exp\\!\\left(-j\\frac{\\pi k}{2}\\right) + \\widehat{\\gamma}(-k)\\exp\\!\\left(j\\frac{\\pi k}{2}\\right)\\right) = \\widehat{\\gamma}(0) + 2\\sum_{k=1}^{3} \\widehat{\\gamma}(k)\\cos\\!\\left(\\frac{\\pi k}{2}\\right)\n$$\nExpanding the sum:\n$$\n\\widehat{S}_{\\mathrm{rect}}(f_{0}) = \\widehat{\\gamma}(0) + 2\\widehat{\\gamma}(1)\\cos\\!\\left(\\frac{\\pi}{2}\\right) + 2\\widehat{\\gamma}(2)\\cos\\!\\left(\\pi\\right) + 2\\widehat{\\gamma}(3)\\cos\\!\\left(\\frac{3\\pi}{2}\\right)\n$$\nWe know $\\cos(\\frac{\\pi}{2})=0$, $\\cos(\\pi)=-1$, and $\\cos(\\frac{3\\pi}{2})=0$. Substituting these values:\n$$\n\\widehat{S}_{\\mathrm{rect}}(f_{0}) = \\widehat{\\gamma}(0) + 2\\widehat{\\gamma}(1)(0) + 2\\widehat{\\gamma}(2)(-1) + 2\\widehat{\\gamma}(3)(0) = \\widehat{\\gamma}(0) - 2\\widehat{\\gamma}(2)\n$$\nSubstituting the calculated values $\\widehat{\\gamma}(0)=4$ and $\\widehat{\\gamma}(2)=4$:\n$$\n\\widehat{S}_{\\mathrm{rect}}(f_{0}) = 4 - 2(4) = -4\n$$\nThis result demonstrates the pathology: a spectral density estimate cannot be negative.\n\nNow, we apply the Bartlett lag window to correct this. First, we compute the window coefficients $w_{\\mathrm{Bart}}(k)$ for $M=3$, using $w_{\\mathrm{Bart}}(k) = 1 - \\frac{|k|}{M+1} = 1 - \\frac{|k|}{4}$:\n$$\nw_{\\mathrm{Bart}}(0) = 1 - \\frac{0}{4} = 1\n$$\n$$\nw_{\\mathrm{Bart}}(1) = w_{\\mathrm{Bart}}(-1) = 1 - \\frac{1}{4} = \\frac{3}{4}\n$$\n$$\nw_{\\mathrm{Bart}}(2) = w_{\\mathrm{Bart}}(-2) = 1 - \\frac{2}{4} = \\frac{1}{2}\n$$\n$$\nw_{\\mathrm{Bart}}(3) = w_{\\mathrm{Bart}}(-3) = 1 - \\frac{3}{4} = \\frac{1}{4}\n$$\nWe evaluate the Bartlett-windowed estimate $\\widehat{S}_{\\mathrm{Bart}}(f)$ at $f_{0}=\\frac{1}{4}$:\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f_{0}) = \\sum_{k=-3}^{3} w_{\\mathrm{Bart}}(k)\\,\\widehat{\\gamma}(k)\\,\\exp\\!\\left(-j\\,\\frac{\\pi}{2} k\\right)\n$$\nSimilar to the previous case, this simplifies to:\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f_{0}) = w_{\\mathrm{Bart}}(0)\\widehat{\\gamma}(0) + 2\\sum_{k=1}^{3} w_{\\mathrm{Bart}}(k)\\widehat{\\gamma}(k)\\cos\\!\\left(\\frac{\\pi k}{2}\\right)\n$$\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f_{0}) = w_{\\mathrm{Bart}}(0)\\widehat{\\gamma}(0) + 2w_{\\mathrm{Bart}}(1)\\widehat{\\gamma}(1)\\cos\\!\\left(\\frac{\\pi}{2}\\right) + 2w_{\\mathrm{Bart}}(2)\\widehat{\\gamma}(2)\\cos\\!\\left(\\pi\\right) + 2w_{\\mathrm{Bart}}(3)\\widehat{\\gamma}(3)\\cos\\!\\left(\\frac{3\\pi}{2}\\right)\n$$\nSubstituting the cosine values:\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f_{0}) = w_{\\mathrm{Bart}}(0)\\widehat{\\gamma}(0) - 2w_{\\mathrm{Bart}}(2)\\widehat{\\gamma}(2)\n$$\nSubstituting the window coefficients and autocovariance values:\n$$\n\\widehat{S}_{\\mathrm{Bart}}(f_{0}) = (1)(4) - 2\\left(\\frac{1}{2}\\right)(4) = 4 - 4 = 0\n$$\nThe Bartlett-windowed estimate is nonnegative, as expected. The final result is the ordered pair $\\left(\\widehat{S}_{\\mathrm{rect}}(f_{0}), \\widehat{S}_{\\mathrm{Bart}}(f_{0})\\right)$.\n$$\n\\left(-4, 0\\right)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -4  0 \\end{pmatrix}}\n$$", "id": "2887459"}, {"introduction": "Having confronted the high variance of the periodogram and the potential for invalid estimates, this final practice empowers you to actively manage the bias-variance trade-off. Here, you will explore the Daniell smoother, a method that reduces variance by averaging adjacent periodogram ordinates [@problem_id:2887402]. By deriving the relationship between the smoother's width $W$ and the resulting variance reduction, you will learn to design an estimator that meets a predefined performance target, a core engineering skill in signal analysis.", "problem": "You are given the task of implementing a Daniell smoother operating over bins of a Discrete Fourier Transform (DFT) and using it to analyze the bias-variance trade-off of smoothed periodograms. You must base your derivation and algorithm on the following well-tested facts: the Discrete Fourier Transform (DFT) of a discrete-time, zero-mean, wide-sense stationary Gaussian process yields a periodogram whose normalized ordinates at distinct Fourier frequencies are asymptotically independent and approximately follow an exponential distribution; linear smoothing of independent exponential ordinates reduces variance in proportion to the squared weights. You must not assume any shortcut formulas beyond these fundamental bases.\n\nDefinitions required for the task:\n- The DFT of a length-$N$ sequence $x[n]$ is $X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j 2\\pi kn / N}$ for $k \\in \\{0,1,\\dots,N-1\\}$, and the periodogram is $I_N[\\omega_k] = \\frac{1}{N} \\lvert X[k] \\rvert^2$ at Fourier frequencies $\\omega_k = 2\\pi k / N$.\n- A Daniell smoother of width $m$ uses an equal-weight moving average over $W = 2m + 1$ adjacent DFT bins. Denote weights by $w_j = 1/W$ for $j \\in \\{-m,\\dots,+m\\}$ and $w_j = 0$ otherwise. The smoothed spectral estimate at bin $k$ is $\\widehat{S}[\\omega_k] = \\sum_{j=-m}^{m} w_j \\, I_N[\\omega_{k-j}]$, with circular wrap-around in the DFT index $k$.\n- The effective degrees of freedom (EDF) $\\nu_{\\mathrm{eff}}$ of a positive spectral estimator is defined so that $\\widehat{S}[\\omega_k]/S[\\omega_k]$ is approximately distributed as $\\frac{1}{\\nu_{\\mathrm{eff}}} \\chi^2_{\\nu_{\\mathrm{eff}}}$, where $\\chi^2_{\\nu}$ denotes a chi-square random variable with $\\nu$ degrees of freedom. Equivalently, the relative variance satisfies $\\mathrm{Var}\\{\\widehat{S}[\\omega_k]\\}/S[\\omega_k]^2 \\approx 2 / \\nu_{\\mathrm{eff}}$.\n\nYour tasks:\n- Implement a Daniell smoother that performs circular smoothing over DFT bins for any integer width parameter $m \\ge 0$, with $W = 2m + 1$. Use wrap-around in the bin index to preserve $W$ near the spectrum edges.\n- From the base facts provided, derive the relationship that connects the width $m$ (equivalently $W = 2m+1$) to $\\nu_{\\mathrm{eff}}$, as well as the resulting relative variance as a function of $m$.\n- Use this relationship to choose the smallest integer $m$ that meets a target relative variance $r_{\\mathrm{target}}$ given a length-$N$ periodogram, subject to the constraints that the Daniell window length $W = 2m + 1$ must be odd and must not exceed the available number of DFT bins. If $N$ is even, the maximum admissible odd $W$ is $N - 1$; if $N$ is odd, the maximum admissible $W$ is $N$. If the target cannot be met because it requires $W$ larger than the admissible maximum, you must choose the maximum admissible $W$.\n\nFor verification and to ensure a clear mapping between specification and implementation, your program must compute for each test case:\n- the chosen $m$,\n- the implied effective degrees of freedom $\\nu_{\\mathrm{eff}}$,\n- the implied achieved relative variance $r_{\\mathrm{achieved}}$.\n\nYou must implement circular smoothing but you do not need to output the smoothed spectrum itself.\n\nTest suite:\nProvide the outputs for the following parameter sets $(N, r_{\\mathrm{target}})$:\n- Case A: $N = 1024$, $r_{\\mathrm{target}} = 0.08$,\n- Case B: $N = 300$, $r_{\\mathrm{target}} = 0.01$,\n- Case C: $N = 65$, $r_{\\mathrm{target}} = 0.5$,\n- Case D (edge case, infeasible target): $N = 31$, $r_{\\mathrm{target}} = 0.001$.\n\nAnswer specification:\n- For each case, return a triple $[m, \\nu_{\\mathrm{eff}}, r_{\\mathrm{achieved}}]$ computed from your derivation-based algorithm; then aggregate these in order A, B, C, D into a single flat list.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concretely, the final output must be of the form\n$[m_A,\\nu_{\\mathrm{eff},A},r_{\\mathrm{achieved},A},m_B,\\nu_{\\mathrm{eff},B},r_{\\mathrm{achieved},B},m_C,\\nu_{\\mathrm{eff},C},r_{\\mathrm{achieved},C},m_D,\\nu_{\\mathrm{eff},D},r_{\\mathrm{achieved},D}]$.\nAll quantities must be dimensionless real numbers or integers. Do not print any additional text.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded in the principles of nonparametric spectral estimation, is well-posed with clear objectives and constraints, and is expressed in objective, formal language. There are no contradictions, ambiguities, or factual errors. We may therefore proceed with the derivation and solution.\n\nThe task is to determine the parameters of a Daniell smoother for a periodogram to meet a specified variance target. The derivation must originate from the fundamental properties provided in the problem statement.\n\nFirst, we establish the statistical properties of the spectral estimators. The problem states that for a zero-mean, wide-sense stationary Gaussian process, the normalized periodogram ordinates, which we denote as $Y_k = I_N[\\omega_k]/S[\\omega_k]$ at distinct Fourier frequencies $\\omega_k$, are approximately independent random variables. They follow a distribution such that $Y_k$ has an expected value $E[Y_k] \\approx 1$ and a variance $\\mathrm{Var}(Y_k) \\approx 1$. This corresponds to the behavior of a scaled chi-square distribution with two degrees of freedom, $\\chi^2_2$. The relative variance of the raw periodogram estimator, $I_N[\\omega_k]$, is thus:\n$$ \\frac{\\mathrm{Var}(I_N[\\omega_k])}{(E[I_N[\\omega_k]])^2} = \\frac{\\mathrm{Var}(S[\\omega_k] Y_k)}{(S[\\omega_k] E[Y_k])^2} = \\frac{S[\\omega_k]^2 \\mathrm{Var}(Y_k)}{S[\\omega_k]^2 (E[Y_k])^2} \\approx \\frac{1}{1^2} = 1 $$\nFrom the given definition of effective degrees of freedom, $\\nu_{\\mathrm{eff}}$, this relative variance is also equal to $2/\\nu_{\\mathrm{eff}}$. For the raw periodogram, this implies $1 = 2/\\nu_{\\mathrm{eff}}$, yielding $\\nu_{\\mathrm{eff}} = 2$, consistent with a $\\chi^2_2$ distribution.\n\nNext, we analyze the smoothed spectral estimator, $\\widehat{S}[\\omega_k]$, produced by a Daniell smoother of width $m$. The smoother applies a moving average with uniform weights over $W = 2m+1$ adjacent frequency bins:\n$$ \\widehat{S}[\\omega_k] = \\sum_{j=-m}^{m} w_j I_N[\\omega_{k-j}] $$\nwhere the weights are $w_j = 1/W$ for $j \\in \\{-m, \\dots, m\\}$ and sum to unity, $\\sum w_j = 1$.\n\nTo analyze the estimator's properties, we assume the true power spectral density, $S(\\omega)$, is approximately constant over the smoothing window, i.e., $S[\\omega_{k-j}] \\approx S[\\omega_k]$ for the small range of $j$. The expected value of the smoothed estimator is:\n$$ E[\\widehat{S}[\\omega_k]] = \\sum_{j=-m}^{m} w_j E[I_N[\\omega_{k-j}]] \\approx \\sum_{j=-m}^{m} w_j S[\\omega_k] = S[\\omega_k] \\sum_{j=-m}^{m} w_j = S[\\omega_k] $$\nThus, the smoothed estimator is approximately unbiased under the local-flatness assumption.\n\nThe variance of $\\widehat{S}[\\omega_k]$ is calculated based on the stated asymptotic independence of the periodogram ordinates:\n$$ \\mathrm{Var}(\\widehat{S}[\\omega_k]) = \\mathrm{Var}\\left(\\sum_{j=-m}^{m} w_j I_N[\\omega_{k-j}]\\right) = \\sum_{j=-m}^{m} \\mathrm{Var}(w_j I_N[\\omega_{k-j}]) = \\sum_{j=-m}^{m} w_j^2 \\mathrm{Var}(I_N[\\omega_{k-j}]) $$\nUsing $\\mathrm{Var}(I_N[\\omega_{k-j}]) \\approx S[\\omega_{k-j}]^2 \\approx S[\\omega_k]^2$ and $w_j=1/W$, we obtain:\n$$ \\mathrm{Var}(\\widehat{S}[\\omega_k]) \\approx \\sum_{j=-m}^{m} \\left(\\frac{1}{W}\\right)^2 S[\\omega_k]^2 = W \\cdot \\frac{1}{W^2} S[\\omega_k]^2 = \\frac{1}{W}S[\\omega_k]^2 $$\nThe achieved relative variance, $r_{\\mathrm{achieved}}$, is the ratio of the variance to the squared mean:\n$$ r_{\\mathrm{achieved}} = \\frac{\\mathrm{Var}(\\widehat{S}[\\omega_k])}{(E[\\widehat{S}[\\omega_k]])^2} \\approx \\frac{(1/W)S[\\omega_k]^2}{S[\\omega_k]^2} = \\frac{1}{W} $$\nSubstituting $W=2m+1$, we arrive at the first required relationship: $r_{\\mathrm{achieved}} = 1/(2m+1)$.\n\nThe second relationship connects this to the effective degrees of freedom, $\\nu_{\\mathrm{eff}}$. Using the provided formula $r_{\\mathrm{achieved}} \\approx 2/\\nu_{\\mathrm{eff}}$:\n$$ \\frac{1}{W} = \\frac{2}{\\nu_{\\mathrm{eff}}} \\implies \\nu_{\\mathrm{eff}} = 2W = 2(2m+1) $$\nThis completes the derivation.\n\nFinally, we construct the algorithm to select the parameter $m$ for a given data length $N$ and target relative variance $r_{\\mathrm{target}}$. The goal is to find the smallest integer $m \\ge 0$ such that $r_{\\mathrm{achieved}} \\le r_{\\mathrm{target}}$ and the window length $W=2m+1$ is valid.\n\n$1$. The condition $r_{\\mathrm{achieved}} \\le r_{\\mathrm{target}}$ translates to $1/W \\le r_{\\mathrm{target}}$, or $W \\ge 1/r_{\\mathrm{target}}$. Since $W$ must be an odd integer, we must find the smallest odd integer $W_{\\mathrm{req}}$ satisfying this inequality. Let $W^* = 1/r_{\\mathrm{target}}$. If $W^* \\le 1$, the smallest odd integer required is $W_{\\mathrm{req}}=1$. Otherwise, we compute $W_c = \\lceil W^* \\rceil$. If $W_c$ is odd, $W_{\\mathrm{req}} = W_c$. If $W_c$ is even, $W_{\\mathrm{req}} = W_c + 1$.\n\n$2$. The window length $W$ is constrained by the data length $N$. The maximum admissible odd window length, $W_{\\mathrm{max}}$, is given as:\n-   $W_{\\mathrm{max}} = N - 1$ if $N$ is even.\n-   $W_{\\mathrm{max}} = N$ if $N$ is odd.\n\n$3$. If the required window $W_{\\mathrm{req}}$ exceeds the maximum allowed $W_{\\mathrm{max}}$, the target variance is unachievable. In this case, we must use the largest possible window, $W_{\\mathrm{max}}$. Therefore, the final window length to be used is $W_{\\mathrm{final}} = \\min(W_{\\mathrm{req}}, W_{\\mathrm{max}})$.\n\n$4$. From $W_{\\mathrm{final}}$, we compute the required output quantities:\n-   The smoother width parameter: $m = (W_{\\mathrm{final}} - 1)/2$.\n-   The effective degrees of freedom: $\\nu_{\\mathrm{eff}} = 2W_{\\mathrm{final}}$.\n-   The achieved relative variance: $r_{\\mathrm{achieved}} = 1/W_{\\mathrm{final}}$.\n\nThis algorithm is now implemented to solve for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates the Daniell smoother parameters based on a target relative variance\n    and constraints on the data length.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, r_target)\n        (1024, 0.08),  # Case A\n        (300, 0.01),   # Case B\n        (65, 0.5),     # Case C\n        (31, 0.001),   # Case D\n    ]\n\n    results = []\n    for N, r_target in test_cases:\n        m, nu_eff, r_achieved = calculate_smoother_params(N, r_target)\n        results.extend([m, nu_eff, r_achieved])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_smoother_params(N, r_target):\n    \"\"\"\n    Calculates the Daniell smoother parameters (m, nu_eff, r_achieved).\n\n    Args:\n        N (int): The length of the periodogram sequence.\n        r_target (float): The target relative variance.\n\n    Returns:\n        tuple: A tuple containing the calculated (m, nu_eff, r_achieved).\n    \"\"\"\n    \n    # 1. Determine the required window width W_req to meet the target variance.\n    # We need r_achieved = r_target, which means 1/W = r_target, or W = 1/r_target.\n    # W must be an odd integer.\n    w_star = 1.0 / r_target\n    \n    if w_star = 1.0:\n        # If the target variance is 1 or more, no smoothing is needed.\n        # W=1 is the smallest odd integer.\n        w_req = 1\n    else:\n        # Find the smallest odd integer = w_star.\n        w_c = int(np.ceil(w_star))\n        if w_c % 2 != 0:\n            w_req = w_c\n        else:\n            w_req = w_c + 1\n\n    # 2. Determine the maximum admissible odd window length W_max.\n    if N % 2 == 0:\n        w_max = N - 1\n    else:\n        w_max = N\n\n    # 3. Choose the final window length, respecting the constraint.\n    # If the target is infeasible, use the maximum possible width.\n    w_final = min(w_req, w_max)\n\n    # 4. Calculate the output parameters from the final window length.\n    # m = (W - 1) / 2\n    m = (w_final - 1) // 2\n    \n    # nu_eff = 2 * W\n    nu_eff = 2 * w_final\n    \n    # r_achieved = 1 / W\n    r_achieved = 1.0 / w_final\n\n    return m, float(nu_eff), r_achieved\n\nsolve()\n```", "id": "2887402"}]}