{"hands_on_practices": [{"introduction": "The cornerstone of autoregressive (AR) model estimation is the set of Yule-Walker equations, which establish a direct mathematical link between a process's autocorrelation and its model coefficients. This first exercise solidifies this foundational concept by having you derive the equations from the orthogonality principle for a complex-valued AR(2) process. By solving for the parameters and innovation variance, you will gain hands-on experience with the theoretical underpinnings of linear prediction [@problem_id:2853134].", "problem": "Consider a zero-mean, complex-valued, wide-sense stationary (WSS) discrete-time process $x[n]$ that is modeled as a second-order autoregressive (AR) process: $x[n] + a_{1} x[n-1] + a_{2} x[n-2] = e[n]$, where $e[n]$ is a complex, white innovation with variance $\\sigma_{e}^{2}$, uncorrelated with $\\{x[n-k]\\}_{k \\geq 1}$. You are given sample autocorrelations $r_{x}[0] = 1$, $r_{x}[1] = 0.5 \\exp(j \\pi/4)$, and $r_{x}[2] = 0.2 \\exp(j \\pi/2)$, with angles in radians. Using only the orthogonality principle for linear minimum mean-square error prediction and the definition of autocorrelation $r_{x}[k] \\triangleq \\mathbb{E}\\{x[n] x^{*}[n-k]\\}$, do the following:\n\n- From first principles, derive the linear equations relating $a_{1}$ and $a_{2}$ to the autocorrelations for the complex-valued case, recognizing Hermitian symmetry $r_{x}[-k] = r_{x}^{*}[k]$, and write them in a Hermitian Toeplitz matrix form suitable for order $p=2$.\n- Solve this system for $a_{1}$ and $a_{2}$.\n- Using the zero-lag relation implied by the same principles, compute the innovation variance $\\sigma_{e}^{2}$.\n\nProvide the exact value of $\\sigma_{e}^{2}$ as your final answer. Do not round; express the final result as a reduced rational number.", "solution": "We are tasked with analyzing a second-order complex-valued autoregressive process, AR($2$), defined by the equation $x[n] + a_{1} x[n-1] + a_{2} x[n-2] = e[n]$. The process $x[n]$ is zero-mean and wide-sense stationary (WSS). The innovation $e[n]$ is a complex white noise process with variance $\\sigma_{e}^{2}$, and it is uncorrelated with past samples of the process, $\\{x[n-k]\\}_{k \\geq 1}$. This uncorrelation is the core of the orthogonality principle for linear prediction.\n\nThe linear minimum mean-square error (LMMSE) one-step prediction of $x[n]$ is given by $\\hat{x}[n] = -a_{1} x[n-1] - a_{2} x[n-2]$. The prediction error is $e[n] = x[n] - \\hat{x}[n]$. The orthogonality principle states that the error $e[n]$ must be orthogonal to the data space used for prediction, which in this case is spanned by $x[n-1]$ and $x[n-2]$. For complex WSS processes, this orthogonality is expressed in terms of expectations: $\\mathbb{E}\\{e[n] x^{*}[n-k]\\} = 0$ for $k \\geq 1$.\n\nWe shall derive the linear equations for the coefficients $a_1$ and $a_2$ by applying this principle. We start with the AR model equation, expressed in terms of the error:\n$$e[n] = x[n] + a_{1} x[n-1] + a_{2} x[n-2]$$\nWe multiply this equation by $x^{*}[n-k]$ for $k=1, 2$ and take the expectation.\n\nFor $k=1$:\n$$ \\mathbb{E}\\{e[n] x^{*}[n-1]\\} = \\mathbb{E}\\{x[n] x^{*}[n-1]\\} + a_{1} \\mathbb{E}\\{x[n-1] x^{*}[n-1]\\} + a_{2} \\mathbb{E}\\{x[n-2] x^{*}[n-1]\\} = 0 $$\nUsing the definition of the autocorrelation function $r_{x}[k] \\triangleq \\mathbb{E}\\{x[n] x^{*}[n-k]\\}$ and its Hermitian symmetry property $r_{x}[-k] = r_{x}^{*}[k]$, this becomes:\n$$ r_{x}[1] + a_{1} r_{x}[0] + a_{2} r_{x}[-1] = 0 \\implies r_{x}[1] + a_{1} r_{x}[0] + a_{2} r_{x}^{*}[1] = 0 $$\n\nFor $k=2$:\n$$ \\mathbb{E}\\{e[n] x^{*}[n-2]\\} = \\mathbb{E}\\{x[n] x^{*}[n-2]\\} + a_{1} \\mathbb{E}\\{x[n-1] x^{*}[n-2]\\} + a_{2} \\mathbb{E}\\{x[n-2] x^{*}[n-2]\\} = 0 $$\nIn terms of autocorrelations, this is:\n$$ r_{x}[2] + a_{1} r_{x}[1] + a_{2} r_{x}[0] = 0 $$\nThese two relations constitute the Yule-Walker equations for an AR($2$) process. We rearrange them to solve for $a_1$ and $a_2$:\n$$ a_{1} r_{x}[0] + a_{2} r_{x}^{*}[1] = -r_{x}[1] $$\n$$ a_{1} r_{x}[1] + a_{2} r_{x}[0] = -r_{x}[2] $$\nThis system of linear equations is correctly expressed in the requested Hermitian Toeplitz matrix form:\n$$ \\begin{pmatrix} r_{x}[0] & r_{x}^{*}[1] \\\\ r_{x}[1] & r_{x}[0] \\end{pmatrix} \\begin{pmatrix} a_{1} \\\\ a_{2} \\end{pmatrix} = -\\begin{pmatrix} r_{x}[1] \\\\ r_{x}[2] \\end{pmatrix} $$\n\nNext, we solve this system using the provided autocorrelation values:\n$r_{x}[0] = 1$\n$r_{x}[1] = 0.5 \\exp(j \\pi/4) = \\frac{1}{2}\\left(\\cos(\\frac{\\pi}{4}) + j\\sin(\\frac{\\pi}{4})\\right) = \\frac{1}{2}\\left(\\frac{\\sqrt{2}}{2} + j\\frac{\\sqrt{2}}{2}\\right) = \\frac{\\sqrt{2}}{4}(1+j)$\n$r_{x}[2] = 0.2 \\exp(j \\pi/2) = \\frac{1}{5}j$\n\nWe also need the conjugate of $r_{x}[1]$: $r_{x}^{*}[1] = \\frac{\\sqrt{2}}{4}(1-j)$.\nThe autocorrelation matrix is $\\mathbf{R}_{2} = \\begin{pmatrix} 1 & \\frac{\\sqrt{2}}{4}(1-j) \\\\ \\frac{\\sqrt{2}}{4}(1+j) & 1 \\end{pmatrix}$.\nThe determinant is $\\det(\\mathbf{R}_{2}) = (1)(1) - \\left(\\frac{\\sqrt{2}}{4}(1+j)\\right)\\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) = 1 - \\frac{2}{16}(1-j^{2}) = 1 - \\frac{1}{8}(2) = \\frac{3}{4}$.\n\nWe use Cramer's rule to find the coefficients.\nFor $a_{1}$:\n$$ a_{1} = \\frac{1}{\\det(\\mathbf{R}_{2})} \\det\\begin{pmatrix} -r_{x}[1] & r_{x}^{*}[1] \\\\ -r_{x}[2] & r_{x}[0] \\end{pmatrix} = \\frac{4}{3} \\left( -r_{x}[1] r_{x}[0] - (-r_{x}[2]) r_{x}^{*}[1] \\right) $$\n$$ a_{1} = \\frac{4}{3} \\left( -\\frac{\\sqrt{2}}{4}(1+j) + \\left(\\frac{1}{5}j\\right) \\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) \\right) = \\frac{4}{3} \\left( -\\frac{\\sqrt{2}}{4}(1+j) + \\frac{\\sqrt{2}}{20}(j-j^{2}) \\right) $$\n$$ a_{1} = \\frac{\\sqrt{2}}{3} \\left( -(1+j) + \\frac{1}{5}(1+j) \\right) = \\frac{\\sqrt{2}}{3} (1+j) \\left(-1 + \\frac{1}{5}\\right) = -\\frac{4\\sqrt{2}}{15}(1+j) $$\nFor $a_{2}$:\n$$ a_{2} = \\frac{1}{\\det(\\mathbf{R}_{2})} \\det\\begin{pmatrix} r_{x}[0] & -r_{x}[1] \\\\ r_{x}[1] & -r_{x}[2] \\end{pmatrix} = \\frac{4}{3} \\left( r_{x}[0](-r_{x}[2]) - (-r_{x}[1])r_{x}[1] \\right) $$\n$$ a_{2} = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\left(\\frac{\\sqrt{2}}{4}(1+j)\\right)^{2} \\right) = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{2}{16}(1+2j+j^{2}) \\right) $$\n$$ a_{2} = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{1}{8}(2j) \\right) = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{1}{4}j \\right) = \\frac{4}{3} \\left(\\frac{-4+5}{20}j\\right) = \\frac{4}{3}\\frac{j}{20} = \\frac{1}{15}j $$\n\nFinally, we compute the innovation variance $\\sigma_{e}^{2}$. We return to the AR equation, multiply by $x^{*}[n]$, and take the expectation:\n$$ \\mathbb{E}\\{x[n]x^{*}[n]\\} + a_{1}\\mathbb{E}\\{x[n-1]x^{*}[n]\\} + a_{2}\\mathbb{E}\\{x[n-2]x^{*}[n]\\} = \\mathbb{E}\\{e[n]x^{*}[n]\\} $$\nIn terms of autocorrelations: $r_{x}[0] + a_{1} r_{x}[-1] + a_{2} r_{x}[-2] = \\mathbb{E}\\{e[n]x^{*}[n]\\}$. Using Hermitian symmetry, this is $r_{x}[0] + a_{1} r_{x}^{*}[1] + a_{2} r_{x}^{*}[2] = \\mathbb{E}\\{e[n]x^{*}[n]\\}$.\nThe term $\\mathbb{E}\\{e[n]x^{*}[n]\\}$ simplifies. Since $x[n] = -\\sum_{k=1}^{2} a_{k}x[n-k] + e[n]$ and $e[n]$ is uncorrelated with past values of $x[n]$, we have $\\mathbb{E}\\{e[n]x^{*}[n]\\} = \\mathbb{E}\\{e[n](-\\sum_{k=1}^{2} a_{k}^{*}x^{*}[n-k] + e^{*}[n])\\} = \\mathbb{E}\\{e[n]e^{*}[n]\\} = \\sigma_{e}^{2}$.\nTherefore, the desired relationship for the innovation variance is:\n$$ \\sigma_{e}^{2} = r_{x}[0] + a_{1} r_{x}^{*}[1] + a_{2} r_{x}^{*}[2] $$\nWe substitute the known and computed values into this expression. We need $r_{x}^{*}[2] = (\\frac{1}{5}j)^{*} = -\\frac{1}{5}j$.\n$$ \\sigma_{e}^{2} = 1 + \\left(-\\frac{4\\sqrt{2}}{15}(1+j)\\right) \\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) + \\left(\\frac{1}{15}j\\right) \\left(-\\frac{1}{5}j\\right) $$\nThe first product is:\n$$ a_{1} r_{x}^{*}[1] = -\\frac{4 \\cdot 2}{15 \\cdot 4}(1+j)(1-j) = -\\frac{8}{60}(1-j^{2}) = -\\frac{2}{15}(2) = -\\frac{4}{15} $$\nThe second product is:\n$$ a_{2} r_{x}^{*}[2] = -\\frac{1}{75}j^{2} = -\\frac{1}{75}(-1) = \\frac{1}{75} $$\nSumming the terms gives the variance:\n$$ \\sigma_{e}^{2} = 1 - \\frac{4}{15} + \\frac{1}{75} = \\frac{75}{75} - \\frac{20}{75} + \\frac{1}{75} = \\frac{75-20+1}{75} = \\frac{56}{75} $$\nThe fraction $\\frac{56}{75}$ is in its simplest form, as $56 = 2^{3} \\cdot 7$ and $75 = 3 \\cdot 5^{2}$ share no common factors.", "answer": "$$\\boxed{\\frac{56}{75}}$$", "id": "2853134"}, {"introduction": "While theory assumes a perfectly known autocorrelation, practical applications require estimating it from finite data, a step where estimator choice is critical. An estimator that is statistically 'unbiased' can, paradoxically, yield an autocorrelation matrix that is not positive semidefinite, implying an impossible power spectrum. This hands-on problem demonstrates this critical pitfall by having you compute a negative eigenvalue from a sample-estimated matrix, proving its invalidity by direct calculation [@problem_id:2853132].", "problem": "A real-valued, length-$N$ data record $\\{x[n]\\}_{n=0}^{N-1}$ with $N=3$ is given by $x[0]=1$, $x[1]=0$, and $x[2]=-2$. Consider the unbiased estimator of the autocorrelation sequence defined, for integer lag $\\ell$ with $|\\ell| \\leq N-1$, by\n$$\n\\tilde{r}_{x}[\\ell] \\triangleq \\frac{1}{N-|\\ell|} \\sum_{n=|\\ell|}^{N-1} x[n]\\,x[n-\\ell].\n$$\nForm the $3 \\times 3$ Toeplitz autocorrelation matrix used in the AutoRegressive (AR) Yule–Walker normal equations by\n$$\n\\mathbf{R} \\triangleq \\begin{pmatrix}\n\\tilde{r}_{x}[0] & \\tilde{r}_{x}[1] & \\tilde{r}_{x}[2] \\\\\n\\tilde{r}_{x}[1] & \\tilde{r}_{x}[0] & \\tilde{r}_{x}[1] \\\\\n\\tilde{r}_{x}[2] & \\tilde{r}_{x}[1] & \\tilde{r}_{x}[0]\n\\end{pmatrix}.\n$$\nStarting only from the definitions above and standard linear algebra, compute the smallest eigenvalue of $\\mathbf{R}$ exactly. Provide your final answer as a single reduced fraction (no decimal approximation).", "solution": "The task is to find the smallest eigenvalue of the matrix $\\mathbf{R}$. The first step is to compute the necessary autocorrelation values $\\tilde{r}_{x}[\\ell]$ for $\\ell \\in \\{0, 1, 2\\}$ using the provided data sequence and the given estimator formula.\n\nThe data sequence is $\\{x[n]\\}_{n=0}^{2}$ with $x[0]=1$, $x[1]=0$, and $x[2]=-2$. The length is $N=3$.\n\nCalculation of $\\tilde{r}_{x}[0]$ (for $\\ell=0$):\n$$\n\\tilde{r}_{x}[0] = \\frac{1}{3-0} \\sum_{n=0}^{2} x[n]x[n-0] = \\frac{1}{3} \\sum_{n=0}^{2} (x[n])^{2}\n$$\n$$\n\\tilde{r}_{x}[0] = \\frac{1}{3} \\left( (x[0])^{2} + (x[1])^{2} + (x[2])^{2} \\right) = \\frac{1}{3} \\left( 1^{2} + 0^{2} + (-2)^{2} \\right) = \\frac{1}{3} (1+0+4) = \\frac{5}{3}\n$$\n\nCalculation of $\\tilde{r}_{x}[1]$ (for $\\ell=1$):\n$$\n\\tilde{r}_{x}[1] = \\frac{1}{3-1} \\sum_{n=1}^{2} x[n]x[n-1] = \\frac{1}{2} \\left( x[1]x[0] + x[2]x[1] \\right)\n$$\n$$\n\\tilde{r}_{x}[1] = \\frac{1}{2} \\left( (0)(1) + (-2)(0) \\right) = \\frac{1}{2}(0) = 0\n$$\n\nCalculation of $\\tilde{r}_{x}[2]$ (for $\\ell=2$):\n$$\n\\tilde{r}_{x}[2] = \\frac{1}{3-2} \\sum_{n=2}^{2} x[n]x[n-2] = 1 \\cdot \\left( x[2]x[0] \\right)\n$$\n$$\n\\tilde{r}_{x}[2] = (-2)(1) = -2\n$$\n\nNow, we construct the matrix $\\mathbf{R}$ using these values. The problem specifies a symmetric Toeplitz structure.\n$$\n\\mathbf{R} = \\begin{pmatrix}\n\\tilde{r}_{x}[0] & \\tilde{r}_{x}[1] & \\tilde{r}_{x}[2] \\\\\n\\tilde{r}_{x}[1] & \\tilde{r}_{x}[0] & \\tilde{r}_{x}[1] \\\\\n\\tilde{r}_{x}[2] & \\tilde{r}_{x}[1] & \\tilde{r}_{x}[0]\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{5}{3} & 0 & -2 \\\\\n0 & \\frac{5}{3} & 0 \\\\\n-2 & 0 & \\frac{5}{3}\n\\end{pmatrix}\n$$\n\nTo find the eigenvalues $\\lambda$ of $\\mathbf{R}$, we solve the characteristic equation $\\det(\\mathbf{R} - \\lambda\\mathbf{I}) = 0$, where $\\mathbf{I}$ is the $3 \\times 3$ identity matrix.\n$$\n\\det\\left( \\begin{pmatrix}\n\\frac{5}{3} & 0 & -2 \\\\\n0 & \\frac{5}{3} & 0 \\\\\n-2 & 0 & \\frac{5}{3}\n\\end{pmatrix} - \\lambda \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} \\right) = 0\n$$\n$$\n\\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda & 0 & -2 \\\\\n0 & \\frac{5}{3} - \\lambda & 0 \\\\\n-2 & 0 & \\frac{5}{3} - \\lambda\n\\end{pmatrix} = 0\n$$\nWe use cofactor expansion along the second row, which simplifies the calculation.\n$$\n\\left(\\frac{5}{3} - \\lambda\\right) \\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda & -2 \\\\\n-2 & \\frac{5}{3} - \\lambda\n\\end{pmatrix} - 0 + 0 = 0\n$$\nThis equation yields two possibilities. First:\n$$\n\\frac{5}{3} - \\lambda = 0 \\implies \\lambda_{1} = \\frac{5}{3}\n$$\nSecond:\n$$\n\\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda & -2 \\\\\n-2 & \\frac{5}{3} - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} - (-2)(-2) = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} - 4 = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} = 4\n$$\nTaking the square root of both sides gives:\n$$\n\\frac{5}{3} - \\lambda = \\pm 2\n$$\nThis leads to two more eigenvalues:\nCase 1:\n$$\n\\frac{5}{3} - \\lambda = 2 \\implies \\lambda = \\frac{5}{3} - 2 = \\frac{5}{3} - \\frac{6}{3} = -\\frac{1}{3}\n$$\nSo, $\\lambda_{2} = -\\frac{1}{3}$.\nCase 2:\n$$\n\\frac{5}{3} - \\lambda = -2 \\implies \\lambda = \\frac{5}{3} + 2 = \\frac{5}{3} + \\frac{6}{3} = \\frac{11}{3}\n$$\nSo, $\\lambda_{3} = \\frac{11}{3}$.\n\nThe three eigenvalues of the matrix $\\mathbf{R}$ are $\\left\\{ \\frac{11}{3}, \\frac{5}{3}, -\\frac{1}{3} \\right\\}$.\nWe are asked for the smallest eigenvalue. Comparing the three values:\n$$\n\\frac{11}{3} \\approx 3.67\n$$\n$$\n\\frac{5}{3} \\approx 1.67\n$$\n$$\n-\\frac{1}{3} \\approx -0.33\n$$\nThe smallest value is $-\\frac{1}{3}$. This is a reduced fraction as required. It is notable that an autocorrelation matrix, which for a wide-sense stationary process must be non-negative definite, here has a negative eigenvalue. This is possible because $\\mathbf{R}$ is constructed from a finite-sample *estimator*, which does not guarantee the non-negative definite property.", "answer": "$$\\boxed{-\\frac{1}{3}}$$", "id": "2853132"}, {"introduction": "To avoid the invalid models demonstrated previously, practitioners often use the 'biased' autocorrelation estimator, which guarantees a stable model and a non-negative power spectrum. This final exercise explores the inherent trade-off by having you analytically derive the finite-sample bias that this robust estimation choice introduces. Mastering the balance between statistical bias and model validity is a key skill in time-series analysis and spectral estimation [@problem_id:2853136].", "problem": "Consider a real, zero-mean, wide-sense stationary, causal autoregressive process of order one, denoted as an $\\mathrm{AR}(1)$ process, satisfying $x[n] = a\\,x[n-1] + w[n]$ for all integer $n$, where $|a| < 1$ and $\\{w[n]\\}$ is a white Gaussian sequence with variance $\\sigma_{w}^{2}$. You observe $N \\ge 2$ consecutive samples $\\{x[1],\\dots,x[N]\\}$ from the stationary regime. Define the biased sample autocorrelation estimator by\n$$\n\\hat{r}_{x}[\\ell] \\triangleq \\frac{1}{N}\\sum_{n=\\ell+1}^{N} x[n]\\,x[n-\\ell], \\quad \\ell = 0,1,\\dots,N-1.\n$$\nAn estimate of the autoregressive coefficient $a$ is obtained by solving the Yule–Walker equations using $\\hat{r}_{x}[\\ell]$, yielding an estimator $\\hat{a}_{N}$.\n\nStarting only from the definitions of autocorrelation, stationarity, and the Yule–Walker relations for an $\\mathrm{AR}(1)$ process, derive the finite-sample bias of $\\hat{a}_{N}$ attributable to the biased normalization used in $\\hat{r}_{x}[\\ell]$. In particular, under a large-sample approximation that replaces any random ratio by the ratio of expectations and keeps only the leading term in $1/N$, obtain a closed-form expression for\n$$\n\\mathrm{Bias}_{N}(a) \\triangleq \\mathbb{E}[\\hat{a}_{N}] - a\n$$\nas a function of $a$ and $N$. Your final answer must be a single closed-form analytic expression in terms of $a$ and $N$; do not include terms beyond first order in $1/N$.\n\nThen, explain in words the trade-off between finite-sample bias and guaranteed positive semidefiniteness (Power Spectral Density (PSD) nonnegativity) when using $\\hat{r}_{x}[\\ell]$ versus autocorrelation estimators that normalize by $N-\\ell$.\n\nNo numerical rounding is required. Express the final answer without units.", "solution": "**Derivation of Finite-Sample Bias**\n\nFor a true $\\mathrm{AR}(1)$ process, the Yule-Walker equations consist of a single relation connecting the autocorrelation function $r_x[\\ell] \\triangleq \\mathbb{E}[x[n]x[n-\\ell]]$ to the parameter $a$. This is found by multiplying the process equation $x[n] = a\\,x[n-1] + w[n]$ by $x[n-1]$ and taking the expectation:\n$$\n\\mathbb{E}[x[n]x[n-1]] = a\\,\\mathbb{E}[x[n-1]x[n-1]] + \\mathbb{E}[w[n]x[n-1]]\n$$\nFor a causal process, $x[n-1]$ depends only on past values of the noise, $w[k]$ for $k \\le n-1$. Since $w[n]$ is white noise, it is uncorrelated with its own past values, and therefore $\\mathbb{E}[w[n]x[n-1]] = 0$. This yields the Yule-Walker equation for an $\\mathrm{AR}(1)$ process:\n$$\nr_x[1] = a\\,r_x[0]\n$$\nThe true parameter is thus given by $a = r_x[1]/r_x[0]$.\n\nThe estimator $\\hat{a}_{N}$ is defined by solving the same equation but using the provided sample autocorrelation estimates, $\\hat{r}_{x}[0]$ and $\\hat{r}_{x}[1]$. Therefore, the estimator is given by the ratio:\n$$\n\\hat{a}_{N} = \\frac{\\hat{r}_{x}[1]}{\\hat{r}_{x}[0]}\n$$\nThe bias of this estimator is $\\mathrm{Bias}_{N}(a) = \\mathbb{E}[\\hat{a}_{N}] - a$. The primary difficulty is computing the expectation of a ratio of random variables. The problem statement mandates the use of a large-sample approximation:\n$$\n\\mathbb{E}[\\hat{a}_{N}] = \\mathbb{E}\\left[\\frac{\\hat{r}_{x}[1]}{\\hat{r}_{x}[0]}\\right] \\approx \\frac{\\mathbb{E}[\\hat{r}_{x}[1]]}{\\mathbb{E}[\\hat{r}_{x}[0]]}\n$$\nThis approximation is valid to first order in $1/N$. Our task reduces to finding the expectations of the sample autocorrelation estimators.\n\nThe definition of the sample autocorrelation estimator is $\\hat{r}_{x}[\\ell] = \\frac{1}{N}\\sum_{n=\\ell+1}^{N} x[n]\\,x[n-\\ell]$. Taking the expectation and using its linearity:\n$$\n\\mathbb{E}[\\hat{r}_{x}[\\ell]] = \\frac{1}{N}\\sum_{n=\\ell+1}^{N} \\mathbb{E}[x[n]\\,x[n-\\ell]]\n$$\nSince the process is wide-sense stationary, the expectation $\\mathbb{E}[x[n]\\,x[n-\\ell]]$ is equal to the true autocorrelation $r_x[\\ell]$, which does not depend on the time index $n$. The sum contains $N - (\\ell+1) + 1 = N - \\ell$ identical terms.\n$$\n\\mathbb{E}[\\hat{r}_{x}[\\ell]] = \\frac{1}{N} \\sum_{n=\\ell+1}^{N} r_x[\\ell] = \\frac{N-\\ell}{N} r_x[\\ell] = \\left(1 - \\frac{\\ell}{N}\\right) r_x[\\ell]\n$$\nThis result shows that the biased sample autocorrelation estimator is, in fact, biased in expectation for $\\ell > 0$. The factor $\\frac{N-\\ell}{N}$ is the source of the bias.\n\nWe now apply this result for the specific lags $\\ell=0$ and $\\ell=1$:\nFor $\\ell=0$:\n$$\n\\mathbb{E}[\\hat{r}_{x}[0]] = \\left(1 - \\frac{0}{N}\\right) r_x[0] = r_x[0]\n$$\nFor $\\ell=1$:\n$$\n\\mathbb{E}[\\hat{r}_{x}[1]] = \\left(1 - \\frac{1}{N}\\right) r_x[1] = \\frac{N-1}{N} r_x[1]\n$$\nNow we substitute these expectations back into the approximation for $\\mathbb{E}[\\hat{a}_{N}]$:\n$$\n\\mathbb{E}[\\hat{a}_{N}] \\approx \\frac{\\frac{N-1}{N} r_x[1]}{r_x[0]} = \\frac{N-1}{N} \\left(\\frac{r_x[1]}{r_x[0]}\\right)\n$$\nRecalling that $a = r_x[1]/r_x[0]$, we obtain the approximate expected value of the estimator:\n$$\n\\mathbb{E}[\\hat{a}_{N}] \\approx \\frac{N-1}{N} a = \\left(1 - \\frac{1}{N}\\right) a\n$$\nFinally, we compute the bias to first order in $1/N$:\n$$\n\\mathrm{Bias}_{N}(a) = \\mathbb{E}[\\hat{a}_{N}] - a \\approx \\left(1 - \\frac{1}{N}\\right) a - a = a - \\frac{a}{N} - a = -\\frac{a}{N}\n$$\nThis is the closed-form expression for the finite-sample bias attributable to the normalization of the sample autocorrelation estimator.\n\n**Explanation of Trade-off**\n\nThe use of the \"biased\" sample autocorrelation estimator, $\\hat{r}_{x}[\\ell] = \\frac{1}{N}\\sum x[n]x[n-\\ell]$, introduces a systematic bias into the estimate of the AR parameter, as we have just derived. The bias is approximately $-\\frac{a}{N}$, which means the magnitude of the coefficient is typically underestimated.\n\nAn alternative is the \"unbiased\" sample autocorrelation estimator, $\\tilde{r}_{x}[\\ell] = \\frac{1}{N-|\\ell|}\\sum x[n]x[n-\\ell]$. This estimator is unbiased in expectation for a zero-mean process, i.e., $\\mathbb{E}[\\tilde{r}_{x}[\\ell]] = r_x[\\ell]$. Using this estimator in the Yule-Walker equations would yield an AR parameter estimate that is approximately unbiased.\n\nHowever, there is a critical trade-off. The sequence of estimates $\\{\\hat{r}_{x}[\\ell]\\}$ produced by the $1/N$ normalization is guaranteed to be positive semidefinite. This property ensures that the Toeplitz autocorrelation matrix formed from these estimates is also positive semidefinite. This, in turn, guarantees that the estimated power spectral density (PSD) is non-negative for all frequencies, a fundamental physical requirement. Furthermore, it guarantees that the resulting AR model is stable (all poles lie inside the unit circle).\n\nIn contrast, the \"unbiased\" sequence $\\{\\tilde{r}_{x}[\\ell]\\}$ (normalized by $1/(N-|\\ell|)$) is not guaranteed to be positive semidefinite. The autocorrelation matrix formed from these estimates may have negative eigenvalues, which can lead to a nonsensical estimated PSD that takes on negative values. This can also result in an estimated AR model that is unstable.\n\nTherefore, the trade-off is between **bias and model validity**. The $1/N$ normalization accepts a small, asymptotically vanishing bias in exchange for the crucial guarantees of a non-negative PSD and a stable model. The $1/(N-|\\ell|)$ normalization removes the bias in the autocorrelation estimate at the cost of potentially producing physically invalid spectral estimates and unstable models. For most applications in signal processing, the guaranteed validity of the model is considered more important than the small finite-sample bias, making the $1/N$ normalization the standard choice.", "answer": "$$\n\\boxed{-\\frac{a}{N}}\n$$", "id": "2853136"}]}