{"hands_on_practices": [{"introduction": "The foundation of any supervised machine learning task is a robust, high-quality dataset. This first exercise guides you through the fundamental process of generating such a dataset from first principles. By deriving the analytical solution to a classic one-dimensional heat conduction problem and verifying it with a numerical finite-difference scheme, you will create ground-truth labels to train a simple surrogate model, bridging the gap between classical physics and data-driven methods [@problem_id:2503006].", "problem": "You are tasked with transforming a one-dimensional steady-state heat conduction model with uniform internal heat generation into a synthetic supervised learning dataset for temperature prediction. Start from first principles of heat conduction to derive the governing equation and its closed-form solution, then implement a numerical scheme to verify the correctness of the solution and to generate labels for supervised learning. You must implement a single program that carries out the following specification and produces a single-line list of floating-point results as the final output.\n\nFundamental base and modeling assumptions:\n- Consider a homogeneous slab occupying the interval $0 \\le x \\le L$, with constant thermal conductivity $k$ and uniform volumetric heat generation $q$. Assume one-dimensional, steady, purely conductive heat transfer with no heat loss aside from the prescribed boundaries. Start from energy conservation and Fourierâ€™s law of heat conduction to arrive at the standard one-dimensional steady conduction equation with uniform generation. The derivation must begin from the energy balance and the constitutive relation, not from a pre-remembered target formula. Dirichlet boundary conditions $T(0)=T_0$ and $T(L)=T_L$ are imposed.\n- The governing equation is a second-order linear ordinary differential equation obtained from the steady energy balance. Your derivation should show why the constant-coefficient operator and the uniform source lead to a quadratic temperature profile, and how boundary data determine the integration constants.\n\nImplementation tasks to be encoded in the program:\n1) Analytic label generation:\n   - Derive the closed-form solution $T(x)$ from the above assumptions and boundary conditions by integrating the governing equation and applying the boundary conditions. Use this $T(x)$ as the ground-truth label generator for supervised learning. The program must use this derived $T(x)$ explicitly.\n2) Numerical finite-difference verification:\n   - Implement a second-order central-difference finite-difference discretization of the governing equation on a uniform grid with $N_{\\text{fd}}=101$ nodes, including the boundaries. Enforce the Dirichlet boundary conditions strongly at $x=0$ and $x=L$. Assemble and solve the resulting tridiagonal linear system for the interior temperatures. Compare the numerical solution against the analytic solution on the same grid, and compute the maximum absolute error $e_{\\text{fd}}$. Report $e_{\\text{fd}}$ in Kelvin.\n3) Supervised learning dataset construction and surrogate training:\n   - For each case, generate a training set of $N_{\\text{train}}$ equally spaced points in $[0,L]$ with inputs (features) given by the vector $[1,\\, \\xi,\\, \\xi^2]$, where $\\xi = x/L$, and labels given by the analytic temperature $T(x)$. Train a polynomial regression model by Ordinary Least Squares (OLS) to map $[1,\\, \\xi,\\, \\xi^2] \\mapsto T$. Then, evaluate the trained model on a separate evaluation set of $N_{\\text{eval}}$ equally spaced points in $[0,L]$ using the same features, and compute the Root-Mean-Squared Error (RMSE) $e_{\\text{ml}}$ between the model predictions and the analytic solution. Report $e_{\\text{ml}}$ in Kelvin.\n4) Test suite and output:\n   - Use the following test suite, where all parameters are in the International System of Units (SI):\n     - Case $1$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,10.0,\\,1.0\\times 10^{5},\\,0.1,\\,300.0,\\,350.0,\\,15,\\,51\\,)$.\n     - Case $2$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,200.0,\\,0.0,\\,0.2,\\,400.0,\\,300.0,\\,7,\\,41\\,)$.\n     - Case $3$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,15.0,\\,5.0\\times 10^{4},\\,0.05,\\,350.0,\\,350.0,\\,11,\\,61\\,)$.\n   - For each case, compute and collect $e_{\\text{fd}}$ and $e_{\\text{ml}}$ as floating-point numbers. All error metrics must be reported in Kelvin $(\\mathrm{K})$ and should be computed in double precision.\n   - Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\,e_{\\text{fd}}^{(1)},\\,e_{\\text{ml}}^{(1)},\\,e_{\\text{fd}}^{(2)},\\,e_{\\text{ml}}^{(2)},\\,e_{\\text{fd}}^{(3)},\\,e_{\\text{ml}}^{(3)}\\,]$ with no spaces.\n\nScientific realism requirements:\n- All physics and numerics must be consistent with the described one-dimensional steady conduction with uniform heat generation. Use $k>0$, $L>0$, and real-valued $q$, $T_0$, $T_L$. The dataset labels must come from the analytic $T(x)$ implied by the physical model. The supervised learning task should be justified by the fact that the target mapping from $[1,\\, \\xi,\\, \\xi^2]$ to $T$ is a fixed quadratic function for each case and thus suitable for Ordinary Least Squares (OLS) training.\n\nAngle units are not applicable. Express all reported error metrics in Kelvin $(\\mathrm{K})$ as floating-point values. The final output must strictly match the single-line format specified above.", "solution": "The problem statement is valid. It is scientifically grounded in the principles of heat transfer, mathematically well-posed, and provides a complete and unambiguous specification for all required tasks. The problem will be solved by first deriving the governing physics and its analytic solution, then implementing the specified numerical and machine learning procedures.\n\n**Part $1$: Derivation of Governing Equation and Analytic Solution**\n\nThe analysis begins from the first principle of energy conservation for a one-dimensional, steady-state system with internal heat generation. Consider an infinitesimal control volume of length $dx$ and constant cross-sectional area $A$ within a slab. The energy balance equation is:\n$$ \\dot{E}_{\\text{in}} - \\dot{E}_{\\text{out}} + \\dot{E}_{\\text{gen}} = 0 $$\nwhere $\\dot{E}_{\\text{in}}$ is the rate of heat conduction into the control volume at position $x$, $\\dot{E}_{\\text{out}}$ is the rate of heat conduction out of the control volume at position $x+dx$, and $\\dot{E}_{\\text{gen}}$ is the rate of heat generation within the volume. These terms are expressed as:\n$$ \\dot{E}_{\\text{in}} = q_x(x) A $$\n$$ \\dot{E}_{\\text{out}} = q_x(x+dx) A $$\n$$ \\dot{E}_{\\text{gen}} = q A dx $$\nHere, $q_x$ is the heat flux (heat rate per unit area) in the $x$-direction, and $q$ is the uniform volumetric heat generation rate. Substituting these into the energy balance gives:\n$$ q_x(x) A - q_x(x+dx) A + q A dx = 0 $$\nDividing by the volume $A dx$ and rearranging yields:\n$$ -\\frac{q_x(x+dx) - q_x(x)}{dx} + q = 0 $$\nTaking the limit as $dx \\to 0$ results in the differential form of the energy conservation equation:\n$$ -\\frac{d q_x}{dx} + q = 0 $$\nThe constitutive relation for heat conduction is Fourier's Law, which states that heat flux is proportional to the negative temperature gradient:\n$$ q_x = -k \\frac{dT}{dx} $$\nwhere $k$ is the thermal conductivity of the material, assumed to be constant. Substituting Fourier's Law into the energy equation gives the governing ordinary differential equation for temperature $T(x)$:\n$$ -\\frac{d}{dx}\\left(-k \\frac{dT}{dx}\\right) + q = 0 $$\n$$ k \\frac{d^2 T}{dx^2} + q = 0 $$\n$$ \\frac{d^2 T}{dx^2} = -\\frac{q}{k} $$\nThis is a second-order linear ordinary differential equation. To find the temperature distribution $T(x)$, we integrate twice with respect to $x$:\n$$ \\frac{dT}{dx} = -\\frac{q}{k} x + C_1 $$\n$$ T(x) = -\\frac{q}{2k} x^2 + C_1 x + C_2 $$\nThe integration constants $C_1$ and $C_2$ are determined from the specified Dirichlet boundary conditions: $T(0) = T_0$ and $T(L) = T_L$.\nApplying the first boundary condition at $x=0$:\n$$ T(0) = T_0 = -\\frac{q}{2k}(0)^2 + C_1(0) + C_2 \\implies C_2 = T_0 $$\nApplying the second boundary condition at $x=L$:\n$$ T(L) = T_L = -\\frac{q}{2k} L^2 + C_1 L + T_0 $$\nSolving for $C_1$:\n$$ C_1 L = T_L - T_0 + \\frac{q L^2}{2k} $$\n$$ C_1 = \\frac{T_L - T_0}{L} + \\frac{qL}{2k} $$\nSubstituting the expressions for $C_1$ and $C_2$ back into the general solution yields the final analytic expression for the temperature profile:\n$$ T(x) = -\\frac{q}{2k} x^2 + \\left(\\frac{T_L - T_0}{L} + \\frac{qL}{2k}\\right)x + T_0 $$\nThis equation provides the ground-truth temperature at any position $x$ in the slab and will be used to generate labels for the supervised learning task.\n\n**Part $2$: Numerical Finite-Difference Verification**\n\nA finite-difference method (FDM) is used to numerically solve the governing equation and verify the analytic solution. The domain $[0, L]$ is discretized into a uniform grid of $N_{\\text{fd}} = 101$ nodes, $x_i = i \\cdot \\Delta x$ for $i \\in \\{0, 1, \\dots, 100\\}$, with grid spacing $\\Delta x = L / (N_{\\text{fd}} - 1)$. At each interior node $x_i$ for $i \\in \\{1, \\dots, 99\\}$, the second derivative $\\frac{d^2 T}{dx^2}$ is approximated using a second-order accurate central difference scheme:\n$$ \\frac{d^2 T}{dx^2}\\bigg|_{x_i} \\approx \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} $$\nwhere $T_i \\approx T(x_i)$. Substituting this into the governing equation gives a system of linear algebraic equations:\n$$ \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} = -\\frac{q}{k} $$\n$$ T_{i-1} - 2T_i + T_{i+1} = -\\frac{q(\\Delta x)^2}{k} $$\nThis forms a tridiagonal system of $N_{\\text{fd}}-2=99$ equations for the unknown interior temperatures $\\mathbf{T}_{\\text{int}} = [T_1, T_2, \\dots, T_{99}]^T$. The boundary temperatures $T_0$ and $T_{N_{\\text{fd}}-1}=T_L$ are known. The system is solved, and the maximum absolute error $e_{\\text{fd}} = \\max_i |T_i^{\\text{numerical}} - T(x_i)^{\\text{analytic}}|$ is computed to assess the accuracy of the FDM solution.\n\n**Part $3$: Supervised Learning Surrogate Model**\n\nA surrogate model for temperature prediction is constructed using supervised learning. The analytic solution shows that $T(x)$ is a quadratic function of $x$. This implies it is also a quadratic function of the non-dimensional coordinate $\\xi = x/L$. The machine learning model is therefore chosen to be a quadratic polynomial regression model:\n$$ \\hat{T}(\\xi) = w_0 \\cdot 1 + w_1 \\cdot \\xi + w_2 \\cdot \\xi^2 $$\nThe features are the components of the vector $[1, \\xi, \\xi^2]$. A training dataset is generated with $N_{\\text{train}}$ equally spaced points $x_j$ in the interval $[0, L]$. For each point, the feature vector $\\mathbf{x}_{\\text{train}, j} = [1, x_j/L, (x_j/L)^2]$ is computed, and the corresponding label $y_{\\text{train}, j} = T(x_j)$ is generated using the analytic solution. The model weights $\\mathbf{w} = [w_0, w_1, w_2]^T$ are determined by solving the Ordinary Least Squares (OLS) problem, which minimizes the sum of squared differences between the predicted and actual labels. The solution is given by:\n$$ \\mathbf{w} = (X_{\\text{train}}^T X_{\\text{train}})^{-1} X_{\\text{train}}^T \\mathbf{y}_{\\text{train}} $$\nwhere $X_{\\text{train}}$ is the matrix of feature vectors and $\\mathbf{y}_{\\text{train}}$ is the vector of labels. Since the model family perfectly matches the true function, the OLS procedure is expected to learn the function with an error close to floating-point precision. The trained model is then evaluated on a separate set of $N_{\\text{eval}}$ points. The predictive performance is measured by the Root-Mean-Squared Error ($e_{\\text{ml}}$) between the model's predictions and the analytic temperatures on this evaluation set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the heat conduction problem for three test cases, computing\n    finite difference error (e_fd) and machine learning error (e_ml) for each.\n    \"\"\"\n\n    # Test suite as per the problem statement.\n    # (k, q, L, T0, TL, N_train, N_eval)\n    test_cases = [\n        (10.0, 1.0e5, 0.1, 300.0, 350.0, 15, 51),\n        (200.0, 0.0, 0.2, 400.0, 300.0, 7, 41),\n        (15.0, 5.0e4, 0.05, 350.0, 350.0, 11, 61),\n    ]\n\n    results = []\n\n    def get_analytic_T(x, k, q, L, T0, TL):\n        \"\"\"\n        Computes the analytic solution for the temperature T at position(s) x.\n        T(x) = -q/(2k) * x^2 + ( (T_L-T_0)/L + qL/(2k) ) * x + T_0\n        \"\"\"\n        C1 = (TL - T0) / L + q * L / (2 * k)\n        C2 = T0\n        return -q / (2 * k) * x**2 + C1 * x + C2\n\n    for case in test_cases:\n        k, q, L, T0, TL, N_train, N_eval = case\n\n        # --- 2) Numerical Finite-Difference Verification ---\n        N_fd = 101\n        N_int = N_fd - 2  # Number of interior nodes\n        x_fd = np.linspace(0, L, N_fd)\n        dx = L / (N_fd - 1)\n\n        # Assemble the tridiagonal matrix A for the interior nodes\n        A = np.zeros((N_int, N_int))\n        main_diag = -2 * np.ones(N_int)\n        off_diag = np.ones(N_int - 1)\n        np.fill_diagonal(A, main_diag)\n        np.fill_diagonal(A[1:], off_diag)\n        np.fill_diagonal(A[:, 1:], off_diag)\n\n        # Assemble the right-hand side vector b\n        b = np.full(N_int, -q * dx**2 / k)\n        b[0] -= T0\n        b[-1] -= TL\n\n        # Solve the linear system A * T_int = b\n        T_int = linalg.solve(A, b)\n\n        # Combine with boundary conditions to get the full numerical solution\n        T_numerical = np.concatenate(([T0], T_int, [TL]))\n\n        # Compute analytic solution on the same grid for comparison\n        T_analytic_fd = get_analytic_T(x_fd, k, q, L, T0, TL)\n\n        # Compute the maximum absolute error e_fd\n        e_fd = np.max(np.abs(T_numerical - T_analytic_fd))\n        results.append(e_fd)\n\n        # --- 3) Supervised Learning Dataset Construction and Surrogate Training ---\n        # Generate training data\n        x_train = np.linspace(0, L, N_train)\n        xi_train = x_train / L\n        # Feature matrix X_train: [1, xi, xi^2]\n        X_train = np.vstack([np.ones_like(xi_train), xi_train, xi_train**2]).T\n        y_train = get_analytic_T(x_train, k, q, L, T0, TL)\n\n        # Train polynomial regression model using Ordinary Least Squares (OLS)\n        # np.linalg.lstsq solves the equation Xw = y for w\n        w, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n        # Evaluate the model\n        x_eval = np.linspace(0, L, N_eval)\n        xi_eval = x_eval / L\n        # Feature matrix for evaluation\n        X_eval = np.vstack([np.ones_like(xi_eval), xi_eval, xi_eval**2]).T\n        \n        # Get model predictions\n        y_pred = X_eval @ w\n        \n        # Get true labels for the evaluation set\n        y_true = get_analytic_T(x_eval, k, q, L, T0, TL)\n\n        # Compute the Root-Mean-Squared Error (RMSE) e_ml\n        e_ml = np.sqrt(np.mean((y_pred - y_true)**2))\n        results.append(e_ml)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "2503006"}, {"introduction": "While data-driven models can be powerful, their predictions may not always respect fundamental physical laws. This practice delves into the core of physics-informed machine learning by demonstrating how to embed thermodynamic constraints directly into the learning process. You will derive the conditions for a physically plausible enthalpy model and construct a custom loss function that ensures the model's predictions are consistent with thermodynamic principles, a crucial step toward building more generalizable and reliable scientific ML models [@problem_id:2502951].", "problem": "A data-driven heat transfer model is being developed to predict temperature-dependent enthalpy for a single-phase fluid at constant pressure using Machine Learning (ML). The model targets the mapping from temperature to enthalpy and is trained on enthalpy measurements while using thermodynamic consistency to regularize the fit.\n\nStart from the definitions that, at constant pressure, the differential enthalpy satisfies $dh = c_{p}(T)\\,dT$, and that the differential entropy satisfies $ds = \\frac{c_{p}(T)}{T}\\,dT$. A physically plausible enthalpy over a temperature interval $[T_{\\min},T_{\\max}]$ with $T_{\\min} > 0$ is one for which the heat capacity is nonnegative and integrable, and the entropy is finite and increases with temperature.\n\nYou are given $M$ enthalpy measurements $\\{(T_{i},\\hat{h}_{i})\\}_{i=1}^{M}$, each referenced to a baseline temperature $T_{r}\\in[T_{\\min},T_{\\max}]$, so that the true enthalpy satisfies $h(T_{r})=0$. The learner does not predict $h(T)$ directly; instead, it predicts a parametric heat capacity\n$$\nc_{p,\\theta}(T) = \\big(a + b\\,T + c\\,T^{2}\\big)^{2},\n$$\nwith parameters $\\theta = (a,b,c)$, and defines the predicted enthalpy by\n$$\nh_{\\theta}(T) = \\int_{T_{r}}^{T} c_{p,\\theta}(\\tau)\\,d\\tau.\n$$\nTo embed thermodynamic consistency beyond mere monotonicity, one known calorimetric reference is available: the enthalpy difference between two calibration temperatures $T_{a},T_{b}\\in[T_{\\min},T_{\\max}]$ satisfies $h(T_{b})-h(T_{a})=\\Delta h_{\\mathrm{ref}}$. The learning objective augments the empirical risk with a penalty that enforces this enthalpy difference:\n$$\n\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(h_{\\theta}(T_{i})-\\hat{h}_{i}\\Big)^{2} \\;+\\; \\mu\\,\\Big(h_{\\theta}(T_{b})-h_{\\theta}(T_{a})-\\Delta h_{\\mathrm{ref}}\\Big)^{2},\n$$\nwith $\\mu>0$ a given penalty weight.\n\nTasks:\n- Using the fundamental definitions above, state sufficient conditions on $c_{p}(T)$ and the temperature domain that ensure physically plausible $h(T)$ and $s(T)$ for constant pressure.\n- Show that the chosen parametrization $c_{p,\\theta}(T) = \\big(a + b\\,T + c\\,T^{2}\\big)^{2}$ satisfies these conditions on $[T_{\\min},T_{\\max}]$ with $T_{\\min}>0$.\n- Compute $h_{\\theta}(T)$ in closed form, and then derive the explicit analytic expression for $\\mathcal{J}(a,b,c)$ in terms of $(a,b,c)$, $\\{(T_{i},\\hat{h}_{i})\\}_{i=1}^{M}$, $T_{r}$, $T_{a}$, $T_{b}$, $\\Delta h_{\\mathrm{ref}}$, and $\\mu$.\n\nProvide your final answer as a single closed-form expression for $\\mathcal{J}(a,b,c)$. All temperatures must be treated in $\\text{K}$ and enthalpies in $\\text{kJ}\\,\\text{kg}^{-1}$. Express the final loss in $(\\text{kJ}\\,\\text{kg}^{-1})^{2}$. No numerical substitution is required, and no rounding is required. The final answer must be a single analytic expression.", "solution": "The problem posed is a well-defined exercise in applying fundamental thermodynamic principles to construct a regularized objective function for a machine learning model. The problem is scientifically sound, internally consistent, and contains all necessary information. We will proceed with its systematic resolution.\n\nThe task is to derive the explicit analytical form of a learning objective function $\\mathcal{J}(a,b,c)$ for a data-driven model of enthalpy. This will be done in three stages: first, by formalizing the physical constraints; second, by verifying the proposed model satisfies these constraints; and third, by direct calculation of the objective function.\n\nFirst, we address the conditions for a physically plausible thermodynamic model for enthalpy $h(T)$ and entropy $s(T)$ for a single-phase fluid at constant pressure over a temperature range $[T_{\\min}, T_{\\max}]$ where $T_{\\min} > 0$. The analysis starts from the fundamental Gibbs relations provided:\n$$\ndh = c_{p}(T)\\,dT\n$$\n$$\nds = \\frac{c_{p}(T)}{T}\\,dT\n$$\nFor these relations to describe a physically plausible system, certain conditions on the specific heat capacity $c_{p}(T)$ must be met.\n1.  **Thermodynamic Stability**: The heat capacity must be non-negative, $c_{p}(T) \\ge 0$. If it were negative, a spontaneous transfer of heat from a colder part of the system to a hotter part could occur, which violates the Second Law of Thermodynamics. This condition ensures local thermal stability.\n2.  **Entropy Increase**: The entropy of a closed system must be a non-decreasing function of its internal energy. At constant pressure, for a single-phase substance, this implies that entropy must be a non-decreasing function of temperature. From the relation for entropy change, the rate of change of entropy with temperature is $\\frac{ds}{dT} = \\frac{c_{p}(T)}{T}$. Since absolute temperature $T$ (in Kelvin) is strictly positive on the given domain ($T \\ge T_{\\min} > 0$), the condition $\\frac{ds}{dT} \\ge 0$ is equivalent to $c_{p}(T) \\ge 0$. The problem asks for entropy to increase, which corresponds to the strict inequality $c_p(T) > 0$. The model should at a minimum satisfy the non-negativity constraint.\n3.  **Integrability and Finiteness**: For the enthalpy $h(T)$ and entropy $s(T)$ to be well-defined through integration, both $c_{p}(T)$ and $\\frac{c_{p}(T)}{T}$ must be integrable over the domain. A sufficient condition for this is the continuity of $c_{p}(T)$ on the closed, bounded interval $[T_{\\min}, T_{\\max}]$. Continuity guarantees integrability. The condition $T_{\\min} > 0$ is critical to ensure that the integrand $\\frac{c_{p}(T)}{T}$ does not have a singularity at $T=0$, thus ensuring the entropy integral is finite.\n\nIn summary, sufficient conditions for a physically plausible model on $[T_{\\min}, T_{\\max}]$ with $T_{\\min} > 0$ are that $c_{p}(T)$ is a continuous, non-negative function.\n\nSecond, we verify that the proposed parametric model for heat capacity, $c_{p,\\theta}(T) = \\big(a + bT + cT^{2}\\big)^{2}$, satisfies these conditions. The parameters are $\\theta = (a, b, c)$.\n1.  The function $P(T) = a + bT + cT^{2}$ is a polynomial in temperature $T$. Polynomials are continuous for all real $T$. The function $c_{p,\\theta}(T)$ is the square of $P(T)$, and the composition of continuous functions is continuous. Therefore, $c_{p,\\theta}(T)$ is continuous on any interval, including $[T_{\\min}, T_{\\max}]$.\n2.  For real-valued parameters $a$, $b$, $c$ and real temperature $T$, the value of $P(T)$ is a real number. The square of any real number is non-negative. Thus, $c_{p,\\theta}(T) = (P(T))^{2} \\ge 0$ for all $T$. This structural form programmatically enforces the non-negativity constraint, which is a key requirement for physical plausibility.\n\nThe model form is therefore valid.\n\nThird, we derive the closed-form expression for the objective function $\\mathcal{J}(a,b,c)$. This requires computing the predicted enthalpy $h_{\\theta}(T)$. We begin by expanding the expression for $c_{p,\\theta}(T)$:\n$$\nc_{p,\\theta}(\\tau) = \\big(a + b\\tau + c\\tau^{2}\\big)^{2} = a^{2} + b^{2}\\tau^{2} + c^{2}\\tau^{4} + 2ab\\tau + 2ac\\tau^{2} + 2bc\\tau^{3}\n$$\nGrouping by powers of $\\tau$:\n$$\nc_{p,\\theta}(\\tau) = a^{2} + (2ab)\\tau + (b^{2}+2ac)\\tau^{2} + (2bc)\\tau^{3} + c^{2}\\tau^{4}\n$$\nThe predicted enthalpy $h_{\\theta}(T)$ is defined as the integral of $c_{p,\\theta}(\\tau)$ from the reference temperature $T_{r}$ to $T$:\n$$\nh_{\\theta}(T) = \\int_{T_{r}}^{T} c_{p,\\theta}(\\tau)\\,d\\tau = \\int_{T_{r}}^{T} \\left( a^{2} + 2ab\\tau + (b^{2}+2ac)\\tau^{2} + 2bc\\tau^{3} + c^{2}\\tau^{4} \\right) d\\tau\n$$\nTo evaluate this, we find the primitive function, let us call it $H(\\tau)$:\n$$\nH(\\tau) = a^{2}\\tau + ab\\tau^{2} + \\frac{b^{2}+2ac}{3}\\tau^{3} + \\frac{bc}{2}\\tau^{4} + \\frac{c^{2}}{5}\\tau^{5}\n$$\nThe definite integral is then $h_{\\theta}(T) = H(T) - H(T_{r})$. Expanding this gives:\n$$\nh_{\\theta}(T) = \\left( a^{2}T + abT^{2} + \\frac{b^{2}+2ac}{3}T^{3} + \\frac{bc}{2}T^{4} + \\frac{c^{2}}{5}T^{5} \\right) - \\left( a^{2}T_{r} + abT_{r}^{2} + \\frac{b^{2}+2ac}{3}T_{r}^{3} + \\frac{bc}{2}T_{r}^{4} + \\frac{c^{2}}{5}T_{r}^{5} \\right)\n$$\nThis can be regrouped by parameter products:\n$$\nh_{\\theta}(T) = a^{2}(T - T_{r}) + ab(T^{2} - T_{r}^{2}) + \\frac{2ac}{3}(T^{3} - T_{r}^{3}) + \\frac{b^{2}}{3}(T^{3} - T_{r}^{3}) + \\frac{bc}{2}(T^{4} - T_{r}^{4}) + \\frac{c^{2}}{5}(T^{5} - T_{r}^{5})\n$$\nThe objective function $\\mathcal{J}(a,b,c)$ has two parts. The first is the mean squared error over the data points $\\{(T_{i},\\hat{h}_{i})\\}$:\n$$\n\\text{Term 1} = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(h_{\\theta}(T_{i})-\\hat{h}_{i}\\Big)^{2}\n$$\nThe second part is the penalty term enforcing the calorimetric constraint:\n$$\n\\text{Term 2} = \\mu\\,\\Big(h_{\\theta}(T_{b})-h_{\\theta}(T_{a})-\\Delta h_{\\mathrm{ref}}\\Big)^{2}\n$$\nFor the penalty term, the enthalpy difference is:\n$$\nh_{\\theta}(T_{b}) - h_{\\theta}(T_{a}) = (H(T_{b}) - H(T_{r})) - (H(T_{a}) - H(T_{r})) = H(T_{b}) - H(T_{a})\n$$\nThis gives:\n$$\nh_{\\theta}(T_{b})-h_{\\theta}(T_{a}) = a^{2}(T_{b}- T_{a}) + ab(T_{b}^{2} - T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3} - T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3} - T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4} - T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5} - T_{a}^{5})\n$$\nSubstituting these expressions into the objective function yields the final explicit form.\n\nThe complete expression for $\\mathcal{J}(a,b,c)$ is:\n$$\n\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\left( a^{2}(T_{i}-T_{r}) + ab(T_{i}^{2}-T_{r}^{2}) + \\frac{2ac}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{b^{2}}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{bc}{2}(T_{i}^{4}-T_{r}^{4}) + \\frac{c^{2}}{5}(T_{i}^{5}-T_{r}^{5}) - \\hat{h}_{i} \\right)^{2} \\\\\n+ \\mu \\left( a^{2}(T_{b}-T_{a}) + ab(T_{b}^{2}-T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4}-T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5}-T_{a}^{5}) - \\Delta h_{\\mathrm{ref}} \\right)^{2}\n$$\nThis is the required result.", "answer": "$$\n\\boxed{\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\left( a^{2}(T_{i}-T_{r}) + ab(T_{i}^{2}-T_{r}^{2}) + \\frac{2ac}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{b^{2}}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{bc}{2}(T_{i}^{4}-T_{r}^{4}) + \\frac{c^{2}}{5}(T_{i}^{5}-T_{r}^{5}) - \\hat{h}_{i} \\right)^{2} + \\mu \\left( a^{2}(T_{b}-T_{a}) + ab(T_{b}^{2}-T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4}-T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5}-T_{a}^{5}) - \\Delta h_{\\mathrm{ref}} \\right)^{2}}\n$$", "id": "2502951"}, {"introduction": "Many critical heat transfer problems are dynamic, requiring models that can accurately predict how a system evolves over time. However, autoregressive rollouts of learned models can suffer from error accumulation and numerical instability. This final practice explores how to build more robust dynamic surrogates by borrowing concepts from classical numerical methods. By implementing and comparing explicit and implicit time-stepping schemes, you will gain hands-on experience with the practical challenge of ensuring the long-term stability of learned simulators [@problem_id:2502968].", "problem": "You will implement and analyze a machine-learned implicit-in-time surrogate for one-dimensional heat conduction with a nonlinear source, formulated as a discrete residual equation whose next-time temperature $T^{n+1}$ is obtained by fixed-point iterations. You will compare its stability against an explicit (forward Euler) rollout using the same learned residual. All computations are to be carried out in a non-dimensional form, hence no physical units are required. All angles in trigonometric functions must be in radians. The final program must produce a single-line output as specified below.\n\nConsider the one-dimensional non-dimensional heat equation derived from conservation of energy and Fourierâ€™s law of heat conduction,\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T),\n$$\non the spatial interval $[0,1]$ with homogeneous Dirichlet boundary conditions $T(0,t)=0$ and $T(1,t)=0$. Let $x_i = i h$ for $i = 1,2,\\dots,M$, where $h = \\frac{1}{M+1}$ is the uniform grid spacing. The centered finite-difference approximation of the Laplacian on the interior grid yields the semi-discrete operator $L \\in \\mathbb{R}^{M \\times M}$ acting on the interior vector $T \\in \\mathbb{R}^M$ as\n$$\n(LT)_i = \\frac{T_{i-1} - 2 T_i + T_{i+1}}{h^2}, \\quad i \\in \\{1,2,\\dots,M\\},\n$$\nwith the convention that $T_0 = 0$ and $T_{M+1} = 0$ due to the boundary conditions. We define the nonlinear source as $s(T) = \\sin(T)$ applied elementwise.\n\nLet $T^n \\in \\mathbb{R}^M$ be the temperature vector at time $t^n$, and consider a learned residual surrogate corresponding to backward Euler time stepping,\n$$\nR_\\theta\\left(T^{n+1}; T^n\\right) \\equiv T^{n+1} - T^n - \\Delta t \\left( (1+\\theta_L) \\, L T^{n+1} + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^{n+1}\\right) \\right),\n$$\nwhere $\\Delta t$ is the time step, $\\theta = (\\theta_L,\\theta_s)$ are learned scalars encoding model-form error, and $\\beta$ is a scalar controlling the source strength.\n\nYour task has three parts:\n- Implement an implicit-in-time surrogate that advances $T^n \\mapsto T^{n+1}$ by solving $R_\\theta\\left(T^{n+1}; T^n\\right) = 0$ using fixed-point iterations. Use the mapping\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right),\n$$\nand perform damped Picard iterations\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right),\n$$\ninitialized with $U^{(0)} = T^n$, where $\\lambda \\in (0,1]$ is a damping parameter. Terminate the iteration when $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon$ or when a maximum of $K$ iterations is reached. If convergence is not achieved (i.e., the maximum is reached without satisfying the tolerance), classify that time step as unstable for the implicit surrogate and stop further stepping for that test case.\n- Implement an explicit rollout surrogate using the same learned residual model, i.e., forward Euler with\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^n\\right) \\right).\n$$\n- Compare stability across multiple time steps up to a specified final time. Define the stability criterion as follows: a method is stable for a test case if at every time step the iterate contains no Not-a-Number and no infinity and also satisfies $\\|T^n\\|_2 \\le B_{\\max}$, where $B_{\\max}$ is a specified bound. If a time step violates any of these checks, classify the method as unstable and stop further stepping for that test case.\n\nInitialization and common settings:\n- Use the initial condition $T_i^0 = \\sin(\\pi x_i)$ for all $i \\in \\{1,2,\\dots,M\\}$.\n- Use homogeneous Dirichlet boundary conditions implemented implicitly via the interior-only operator $L$ with $T_0 = 0$ and $T_{M+1} = 0$.\n- Use the following fixed-point parameters: damping $\\lambda = 0.2$, tolerance $\\varepsilon = 10^{-8}$, and maximum iterations $K = 200$.\n- Use the stability bound $B_{\\max} = 10^6$.\n- All trigonometric function arguments are in radians.\n\nTest suite:\nUse $M = 20$ interior nodes for all tests. For each test case, run time stepping from $t^0 = 0$ to $t^{N} = t_{\\mathrm{final}}$ with $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$ steps using both the implicit surrogate and the explicit surrogate. The four test cases are:\n- Case $1$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0006, 0.05, 0.1, 0.5, 0.05)$.\n- Case $2$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.002, 0.05, 0.1, 0.5, 0.05)$.\n- Case $3$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.00115, 0.05, 0.1, 0.5, 0.05)$.\n- Case $4$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0018, -0.4, -0.2, 0.5, 0.05)$.\n\nOutput specification:\n- For each test case, compute a stability code $c$ defined by\n  - $c = 0$ if both implicit and explicit surrogates are stable,\n  - $c = 1$ if the implicit surrogate is stable and the explicit surrogate is unstable,\n  - $c = 2$ if both are unstable,\n  - $c = 3$ if the implicit surrogate is unstable and the explicit surrogate is stable.\n- Also compute the terminal norms $n_{\\mathrm{imp}} = \\|T^N_{\\mathrm{imp}}\\|_2$ and $n_{\\mathrm{exp}} = \\|T^N_{\\mathrm{exp}}\\|_2$ for each test case, where $T^N_{\\mathrm{imp}}$ and $T^N_{\\mathrm{exp}}$ denote the terminal states produced by the implicit and explicit surrogates, respectively.\n- Your program should produce a single line of output containing the results aggregated for all four cases as a comma-separated list enclosed in square brackets in the order\n$$\n[c_1, \\text{round}(n_{\\mathrm{imp},1}, 6), \\text{round}(n_{\\mathrm{exp},1}, 6), c_2, \\text{round}(n_{\\mathrm{imp},2}, 6), \\text{round}(n_{\\mathrm{exp},2}, 6), c_3, \\text{round}(n_{\\mathrm{imp},3}, 6), \\text{round}(n_{\\mathrm{exp},3}, 6), c_4, \\text{round}(n_{\\mathrm{imp},4}, 6), \\text{round}(n_{\\mathrm{exp},4}, 6)].\n$$\nEach floating-point number must be rounded to exactly $6$ decimals in the printed output.", "solution": "The problem requires the implementation and stability analysis of two numerical surrogates for a one-dimensional nonlinear heat equation. One surrogate is based on an implicit time-stepping scheme, solved with fixed-point iterations, while the other is an explicit forward-time integration. The stability of these two methods will be compared across a suite of test cases.\n\nThe governing partial differential equation is the non-dimensional reaction-diffusion equation:\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T)\n$$\nHere, $T(x,t)$ is the temperature, $x \\in [0,1]$ is the spatial coordinate, $t$ is time, $\\beta$ is a scalar parameter for the source term strength, and $s(T) = \\sin(T)$ is the nonlinear source function. Homogeneous Dirichlet boundary conditions, $T(0,t) = T(1,t) = 0$, are imposed.\n\nFirst, we discretize the spatial domain. We use a uniform grid with $M$ interior points $x_i = i h$ for $i \\in \\{1, 2, \\dots, M\\}$, where the grid spacing is $h = \\frac{1}{M+1}$. The spatial derivative term $\\frac{\\partial^2 T}{\\partial x^2}$ is approximated using a second-order central finite difference scheme. This results in a system of ordinary differential equations for the vector of temperatures at the interior grid points, $T(t) \\in \\mathbb{R}^M$. The spatial operator is represented by a matrix $L \\in \\mathbb{R}^{M \\times M}$, where $(LT)_i = \\frac{T_{i-1} - 2T_i + T_{i+1}}{h^2}$. The boundary conditions $T_0 = T_{M+1} = 0$ are incorporated into the structure of this matrix. Specifically, $L$ is a symmetric tridiagonal matrix with $-\\frac{2}{h^2}$ on the main diagonal and $\\frac{1}{h^2}$ on the first super- and sub-diagonals.\n\nThe initial condition is given by a sinusoidal profile: $T_i^0 = \\sin(\\pi x_i)$ for $i \\in \\{1,\\dots,M\\}$.\n\nThe problem then introduces a \"machine-learned\" surrogate model, which modifies the standard numerical schemes with scalar parameters $\\theta_L$ and $\\theta_s$. These parameters represent learned corrections to the physical model's diffusion and source terms.\n\nThe explicit surrogate advances the solution in time using a forward Euler-like method:\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s(T^n) \\right)\n$$\nwhere $T^n$ is the temperature vector at time $t^n = n \\Delta t$.\n\nThe implicit surrogate is based on a backward Euler-like scheme. The solution at the next time step, $T^{n+1}$, is the root of the residual equation $R_\\theta(T^{n+1}; T^n) = 0$, where:\n$$\nR_\\theta\\left(U; T^n\\right) \\equiv U - T^n - \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\nTo solve this nonlinear system for $U = T^{n+1}$, we rearrange the equation into a fixed-point problem $U = \\mathcal{G}(U; T^n)$, with the mapping function defined as:\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\nThis is solved using damped Picard iterations, initialized with $U^{(0)} = T^n$. The iterative update rule is:\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right)\n$$\nwhere $\\lambda = 0.2$ is the damping parameter. The iteration continues until the change between successive iterates is sufficiently small, $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon = 10^{-8}$, or a maximum of $K=200$ iterations is reached.\n\nStability for each method is assessed at each time step. A method is considered unstable for a given test case if at any time step $n$, the computed solution $T^n$ contains Not-a-Number (NaN) or infinity (inf) values, or if its Euclidean norm exceeds a bound, $\\|T^n\\|_2 > B_{\\max} = 10^6$. For the implicit method, failure of the Picard iteration to converge within $K$ steps also constitutes instability. If a method is deemed unstable, the time-stepping process for that method is terminated.\n\nThe overall procedure for each of the four test cases is as follows:\n1. Initialize the temperature vector $T^0$ using the given initial condition.\n2. For both the implicit and explicit surrogates, perform time stepping from $t=0$ to $t_{\\mathrm{final}}$, for a total of $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$ steps.\n3. During each simulation, monitor for the specified instability conditions.\n4. Record whether each method completed all $N$ steps stably.\n5. Based on the stability outcomes, determine the stability code $c$: $0$ if both are stable, $1$ if implicit is stable and explicit is not, $2$ if both are unstable, and $3$ if implicit is unstable and explicit is stable.\n6. Compute the Euclidean norm of the final temperature vector for each method. If a method becomes unstable at step $k < N$, the state $T^k$ is considered the terminal state for that method.\n7. Collate these results ($c$, and the two terminal norms) for all four test cases into a single formatted output string. The norms are reported formatted to $6$ decimal places.\n\nThis involves a careful implementation of the matrix operator, the time-stepping loops, the iterative solver, and the specified stability checks.", "answer": "```python\nimport numpy as np\nimport math\n\n# Define global constants from the problem statement\nM = 20\nLAMBDA = 0.2\nEPSILON = 1e-8\nK_MAX = 200\nB_MAX = 1e6\n\ndef get_laplacian(m_nodes):\n    \"\"\"Constructs the 1D finite difference Laplacian matrix L.\"\"\"\n    h = 1.0 / (m_nodes + 1)\n    h2 = h * h\n    main_diag = -2.0 / h2 * np.ones(m_nodes)\n    off_diag = 1.0 / h2 * np.ones(m_nodes - 1)\n    L = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    return L\n\ndef source_term(T):\n    \"\"\"Computes the nonlinear source term s(T) = sin(T).\"\"\"\n    return np.sin(T)\n\ndef solve_implicit_step(Tn, dt, theta_L, theta_s, beta, L):\n    \"\"\"\n    Solves for T^{n+1} using damped Picard iterations.\n    Returns the solution and a boolean indicating convergence.\n    \"\"\"\n    U_k = Tn.copy()\n    \n    for _ in range(K_MAX):\n        # Calculate G(U_k; Tn)\n        Lu = L @ U_k\n        s_U = source_term(U_k)\n        G_U = Tn + dt * ((1 + theta_L) * Lu + (1 + theta_s) * beta * s_U)\n        \n        # Damped Picard update\n        U_k_plus_1 = (1 - LAMBDA) * U_k + LAMBDA * G_U\n        \n        # Check for convergence\n        diff = np.linalg.norm(U_k_plus_1 - U_k)\n        \n        # Check for numerical issues in the iteration itself\n        if np.isnan(diff) or np.isinf(diff):\n            return U_k_plus_1, False\n        \n        if diff <= EPSILON:\n            return U_k_plus_1, True\n            \n        U_k = U_k_plus_1\n        \n    return U_k, False # Did not converge\n\ndef run_simulation(params, L, x_grid, method):\n    \"\"\"\n    Runs a simulation for a given method ('implicit' or 'explicit').\n    Returns a stability flag and the final terminal norm.\n    \"\"\"\n    dt, theta_L, theta_s, beta, t_final = params\n    \n    # Initial condition\n    T = np.sin(np.pi * x_grid)\n    \n    # Check initial condition stability (unlikely to be unstable)\n    norm_T = np.linalg.norm(T)\n    if not np.isfinite(norm_T) or norm_T > B_MAX:\n        return False, norm_T\n        \n    num_steps = math.floor(t_final / dt)\n    \n    for _ in range(num_steps):\n        if method == 'implicit':\n            T_next, converged = solve_implicit_step(T, dt, theta_L, theta_s, beta, L)\n            if not converged:\n                # Instability due to non-convergence\n                return False, np.linalg.norm(T) # Return norm of last good state\n        else: # explicit\n            LTn = L @ T\n            s_T = source_term(T)\n            T_next = T + dt * ((1 + theta_L) * LTn + (1 + theta_s) * beta * s_T)\n            \n        # General stability check for the new state T_next\n        norm_T_next = np.linalg.norm(T_next)\n        if not np.isfinite(norm_T_next) or norm_T_next > B_MAX:\n            # Instability due to blow-up or NaN\n            return False, norm_T_next\n            \n        T = T_next\n        \n    # If all steps completed successfully\n    return True, np.linalg.norm(T)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    \n    # Setup spatial discretization\n    L = get_laplacian(M)\n    h = 1.0 / (M + 1)\n    x_grid = np.array([i * h for i in range(1, M + 1)])\n\n    test_cases = [\n        (0.0006, 0.05, 0.1, 0.5, 0.05),\n        (0.002, 0.05, 0.1, 0.5, 0.05),\n        (0.00115, 0.05, 0.1, 0.5, 0.05),\n        (0.0018, -0.4, -0.2, 0.5, 0.05),\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        # Run implicit simulation\n        imp_stable, n_imp = run_simulation(case_params, L, x_grid, 'implicit')\n        \n        # Run explicit simulation\n        exp_stable, n_exp = run_simulation(case_params, L, x_grid, 'explicit')\n        \n        # Determine stability code c\n        if imp_stable and exp_stable:\n            c = 0\n        elif imp_stable and not exp_stable:\n            c = 1\n        elif not imp_stable and not exp_stable:\n            c = 2\n        else: # not imp_stable and exp_stable\n            c = 3\n        \n        # Append results for this case\n        results.append(c)\n        results.append(f\"{n_imp:.6f}\")\n        results.append(f\"{n_exp:.6f}\")\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2502968"}]}