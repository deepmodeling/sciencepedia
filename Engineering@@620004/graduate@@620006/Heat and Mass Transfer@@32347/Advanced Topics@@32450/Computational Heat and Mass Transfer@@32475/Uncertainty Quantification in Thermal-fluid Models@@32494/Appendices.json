{"hands_on_practices": [{"introduction": "The Monte Carlo (MC) method is a cornerstone of uncertainty quantification, valued for its robustness and conceptual simplicity. However, its practical application, especially with computationally expensive models like CFD, hinges on a crucial planning step: determining the necessary number of simulation runs. This exercise guides you through the fundamental process of sample size determination, bridging the gap between statistical theory and practical computational budgeting. By deriving the sample size formula from the Central Limit Theorem [@problem_id:2536828], you will gain a practical skill essential for designing efficient and statistically rigorous MC studies.", "problem": "A research team is performing Uncertainty Quantification (UQ) of the mean wall heat flux in a heated internal flow using Monte Carlo (MC) sampling of uncertain inlet and boundary conditions in a Computational Fluid Dynamics (CFD) solver. Let the wall heat flux from one independent simulation-realization be the random variable $Y$ with finite mean $\\mu$ and finite variance $\\sigma^{2}$. The team seeks to estimate $\\mu$ by the sample mean $\\bar{Y}_{M}$ of $M$ independent and identically distributed runs, and to report a two-sided $(1-\\alpha)$ confidence interval for $\\mu$ based on a large-sample Central Limit Theorem (CLT) approximation. A prior pilot study has provided an estimate $s$ of the output standard deviation $\\sigma$ that will be reused in the planning calculation.\n\nStarting from the Central Limit Theorem and the definition of a two-sided confidence interval for a mean, derive the minimal total sample size $M$ such that the half-width of the $(1-\\alpha)$ confidence interval for $\\mu$ is less than a prescribed tolerance $\\delta$. Assume independence of MC samples, finite second moment of $Y$, and use the large-sample normal quantile in place of the Student’s $t$ quantile for this planning calculation. Express your final answer as a single closed-form analytic expression for $M$ in terms of $s$, $\\delta$, and $\\alpha$ that uses the standard normal inverse cumulative distribution function and the ceiling operator to ensure an integer sample size. Do not include units in your final expression.", "solution": "We are asked to plan the number of Monte Carlo (MC) samples, $M$, needed so that the two-sided $(1-\\alpha)$ confidence interval for the population mean wall heat flux $\\mu$ has half-width less than a target $\\delta$, given an available estimate $s$ for the standard deviation of the output.\n\nLet $Y$ denote the wall heat flux from one independent simulation, with mean $\\mu$ and variance $\\sigma^{2}$. Define the sample mean over $M$ independent and identically distributed realizations as\n$$\n\\bar{Y}_{M} \\equiv \\frac{1}{M}\\sum_{i=1}^{M} Y_{i}.\n$$\nBy the Central Limit Theorem (CLT), for large $M$,\n$$\n\\frac{\\bar{Y}_{M}-\\mu}{\\sigma/\\sqrt{M}} \\;\\overset{\\text{approx}}{\\sim}\\; \\mathcal{N}(0,1),\n$$\nwhere $\\mathcal{N}(0,1)$ denotes the standard normal distribution. Because $\\sigma$ is unknown, we use the pilot estimate $s$ for the planning calculation. Under the large-sample approximation, the two-sided $(1-\\alpha)$ confidence interval for $\\mu$ based on $\\bar{Y}_{M}$ is\n$$\n\\bar{Y}_{M} \\pm z_{1-\\alpha/2}\\,\\frac{s}{\\sqrt{M}},\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution, equivalently $z_{1-\\alpha/2}=\\Phi^{-1}(1-\\alpha/2)$ with $\\Phi$ the standard normal cumulative distribution function. The half-width of this interval is therefore\n$$\nH(M) \\equiv z_{1-\\alpha/2}\\,\\frac{s}{\\sqrt{M}}.\n$$\nThe requirement that the half-width be less than the tolerance $\\delta$ is\n$$\nH(M) \\le \\delta \\quad \\Longleftrightarrow \\quad z_{1-\\alpha/2}\\,\\frac{s}{\\sqrt{M}} \\le \\delta.\n$$\nSolving this inequality for $M$ gives\n$$\n\\sqrt{M} \\ge z_{1-\\alpha/2}\\,\\frac{s}{\\delta}\n\\quad\\Longleftrightarrow\\quad\nM \\ge \\left(z_{1-\\alpha/2}\\,\\frac{s}{\\delta}\\right)^{2}.\n$$\nBecause $M$ must be an integer and we want the minimal $M$ satisfying the inequality, we take the ceiling:\n$$\nM \\;=\\; \\left\\lceil \\left(z_{1-\\alpha/2}\\,\\frac{s}{\\delta}\\right)^{2} \\right\\rceil \\;=\\; \\left\\lceil \\left(\\Phi^{-1}\\!\\left(1-\\frac{\\alpha}{2}\\right)\\,\\frac{s}{\\delta}\\right)^{2} \\right\\rceil.\n$$\nThis provides a closed-form planning formula in terms of $s$, $\\delta$, and $\\alpha$, using the standard normal inverse cumulative distribution function and enforcing integrality via the ceiling operator. Note that a Student’s $t$-based design would introduce an implicit dependence on $M$ through the degrees of freedom; the large-sample normal-quantile approximation used here is standard for pre-study sample size planning in Monte Carlo uncertainty quantification.", "answer": "$$\\boxed{\\left\\lceil \\left( \\frac{\\Phi^{-1}\\!\\left(1-\\frac{\\alpha}{2}\\right)\\, s}{\\delta} \\right)^{2} \\right\\rceil}$$", "id": "2536828"}, {"introduction": "While Monte Carlo methods are robust, they can be computationally prohibitive. For models where uncertainties in the input parameters are relatively small, analytical methods like the First-Order Second-Moment (FOSM) technique offer a highly efficient alternative for propagating uncertainty. This practice focuses on applying the FOSM method to a classic heat transfer problem: series conduction through a multilayer wall [@problem_id:2536848]. By deriving the variance of the overall heat transfer coefficient, you will see how the model's sensitivities, represented by partial derivatives, directly quantify how input uncertainties contribute to the output variance.", "problem": "A plane multilayer wall comprises $N \\geq 2$ homogeneous layers in perfect thermal contact, each of uniform thickness $t_i$ and thermal conductivity $k_i$, for $i=1,\\dots,N$. The overall heat transfer coefficient $U$ for one-dimensional steady conduction across the series layers satisfies the series resistance relation $1/U=\\sum_{i=1}^{N} t_i/k_i$. Suppose the thicknesses $t_i$ are known exactly (deterministic), while the thermal conductivities $k_i$ are random variables with small relative uncertainties. Let $\\mathbf{k}=(k_1,\\dots,k_N)^{\\top}$ have mean vector $\\boldsymbol{\\mu}_{\\mathbf{k}}=(\\mu_{k_1},\\dots,\\mu_{k_N})^{\\top}$ and covariance matrix $\\mathbf{C}_{\\mathbf{k}}$ with entries $\\operatorname{Cov}(k_i,k_j)$ that may be nonzero for $i\\neq j$. Using the First-Order Second-Moment (FOSM) method, starting from a first-order Taylor expansion of $U(\\mathbf{k})$ about $\\boldsymbol{\\mu}_{\\mathbf{k}}$, derive a closed-form analytic expression for the variance of $U$ in terms of $t_i$, $\\mu_{k_i}$, and $\\operatorname{Cov}(k_i,k_j)$. Express your final result as a single analytic expression. No numerical evaluation or rounding is required, and no units should be included in the final expression.", "solution": "The overall heat transfer coefficient, $U$, is a function of the random variables $k_1, k_2, \\dots, k_N$. The function is given by:\n$$\nU(\\mathbf{k}) = \\left( \\sum_{i=1}^{N} \\frac{t_i}{k_i} \\right)^{-1}\n$$\nThe First-Order Second-Moment (FOSM) method approximates the variance of a function $Y = f(X_1, \\dots, X_N)$ using a first-order Taylor series expansion of the function about the mean values of the input variables $\\boldsymbol{\\mu}_{\\mathbf{X}} = (\\mu_{X_1}, \\dots, \\mu_{X_N})^{\\top}$. The general formula for the variance is:\n$$\n\\operatorname{Var}(Y) \\approx \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\left( \\left. \\frac{\\partial f}{\\partial X_i} \\right|_{\\boldsymbol{\\mu}_{\\mathbf{X}}} \\right) \\left( \\left. \\frac{\\partial f}{\\partial X_j} \\right|_{\\boldsymbol{\\mu}_{\\mathbf{X}}} \\right) \\operatorname{Cov}(X_i, X_j)\n$$\nIn our case, the function is $U(\\mathbf{k})$ and the random variables are $k_1, \\dots, k_N$. We must first compute the partial derivatives of $U$ with respect to each $k_j$, where $j \\in \\{1, \\dots, N\\}$.\n\nLet us define the total thermal resistance $R_{\\text{th}}$ as:\n$$\nR_{\\text{th}}(\\mathbf{k}) = \\sum_{i=1}^{N} \\frac{t_i}{k_i}\n$$\nThen, $U = R_{\\text{th}}^{-1}$. Using the chain rule, the partial derivative of $U$ with respect to $k_j$ is:\n$$\n\\frac{\\partial U}{\\partial k_j} = \\frac{dU}{dR_{\\text{th}}} \\frac{\\partial R_{\\text{th}}}{\\partial k_j}\n$$\nFirst, we compute the derivative of $U$ with respect to $R_{\\text{th}}$:\n$$\n\\frac{dU}{dR_{\\text{th}}} = \\frac{d}{dR_{\\text{th}}} (R_{\\text{th}}^{-1}) = -1 \\cdot R_{\\text{th}}^{-2} = -U^2\n$$\nNext, we compute the partial derivative of $R_{\\text{th}}$ with respect to $k_j$. Since the summation terms are independent with respect to the variables of differentiation, only the term where $i=j$ will have a non-zero derivative:\n$$\n\\frac{\\partial R_{\\text{th}}}{\\partial k_j} = \\frac{\\partial}{\\partial k_j} \\left( \\sum_{i=1}^{N} \\frac{t_i}{k_i} \\right) = \\frac{\\partial}{\\partial k_j} \\left( \\frac{t_j}{k_j} \\right) = -\\frac{t_j}{k_j^2}\n$$\nCombining these results, we find the partial derivative of $U$ with respect to $k_j$:\n$$\n\\frac{\\partial U}{\\partial k_j} = (-U^2) \\left( -\\frac{t_j}{k_j^2} \\right) = U^2 \\frac{t_j}{k_j^2}\n$$\nThese partial derivatives, often called sensitivity coefficients, must be evaluated at the mean values of the random variables, $\\mathbf{k} = \\boldsymbol{\\mu}_{\\mathbf{k}} = (\\mu_{k_1}, \\dots, \\mu_{k_N})^{\\top}$. Let us denote the value of $U$ evaluated at the mean conductivities as $\\mu_U'$ (noting this is the function evaluated at the mean, not necessarily the mean of the function itself, though it is the first-order approximation):\n$$\n\\mu_U' = U(\\boldsymbol{\\mu}_{\\mathbf{k}}) = \\left( \\sum_{i=1}^{N} \\frac{t_i}{\\mu_{k_i}} \\right)^{-1}\n$$\nThe sensitivity coefficients evaluated at the mean are therefore:\n$$\n\\left. \\frac{\\partial U}{\\partial k_j} \\right|_{\\boldsymbol{\\mu}_{\\mathbf{k}}} = (U(\\boldsymbol{\\mu}_{\\mathbf{k}}))^2 \\frac{t_j}{\\mu_{k_j}^2} = (\\mu_U')^2 \\frac{t_j}{\\mu_{k_j}^2}\n$$\nNow we substitute these evaluated derivatives into the FOSM formula for the variance of $U$:\n$$\n\\operatorname{Var}(U) \\approx \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\left( (\\mu_U')^2 \\frac{t_i}{\\mu_{k_i}^2} \\right) \\left( (\\mu_U')^2 \\frac{t_j}{\\mu_{k_j}^2} \\right) \\operatorname{Cov}(k_i, k_j)\n$$\nWe can factor out the terms involving $\\mu_U'$ from the double summation:\n$$\n\\operatorname{Var}(U) \\approx (\\mu_U')^4 \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{t_i t_j}{\\mu_{k_i}^2 \\mu_{k_j}^2} \\operatorname{Cov}(k_i, k_j)\n$$\nFinally, we substitute the expression for $\\mu_U'$ to obtain the final result in terms of the given parameters $t_i$, $\\mu_{k_i}$, and $\\operatorname{Cov}(k_i, k_j)$. Let the dummy index in the sum for $\\mu_U'$ be $l$ to avoid confusion with $i$ and $j$.\n$$\n(\\mu_U')^4 = \\left[ \\left( \\sum_{l=1}^{N} \\frac{t_l}{\\mu_{k_l}} \\right)^{-1} \\right]^4 = \\left( \\sum_{l=1}^{N} \\frac{t_l}{\\mu_{k_l}} \\right)^{-4}\n$$\nThe final expression for the variance of $U$ is:\n$$\n\\operatorname{Var}(U) \\approx \\left( \\sum_{l=1}^{N} \\frac{t_l}{\\mu_{k_l}} \\right)^{-4} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{t_i t_j}{\\mu_{k_i}^2 \\mu_{k_j}^2} \\operatorname{Cov}(k_i, k_j)\n$$\nThis is the required closed-form analytic expression derived using the FOSM method.", "answer": "$$\n\\boxed{\\left( \\sum_{l=1}^{N} \\frac{t_l}{\\mu_{k_l}} \\right)^{-4} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{t_i t_j}{\\mu_{k_i}^2 \\mu_{k_j}^2} \\operatorname{Cov}(k_i, k_j)}\n$$", "id": "2536848"}, {"introduction": "Moving beyond local approximations like FOSM, spectral methods provide a powerful global framework for representing uncertainty. Among these, the Polynomial Chaos Expansion (PCE) is a prominent technique that constructs an analytical surrogate model of the uncertain output as a function of the random inputs. This exercise introduces the core concepts of PCE by applying it to a one-dimensional heat conduction problem with an uncertain boundary condition [@problem_id:2536803]. You will learn how to construct the expansion using orthonormal polynomials and see how statistical moments, such as the mean and variance, can be extracted directly and exactly from the expansion coefficients.", "problem": "A homogeneous slab of thickness $L$ and constant thermal conductivity $k$ undergoes steady one-dimensional heat conduction along the $x$-direction. The boundary at $x=0$ is maintained at a spatially uniform but uncertain temperature $T_0$, modeled as a Gaussian random variable with mean $\\mu_0$ and standard deviation $\\sigma_0$. The boundary at $x=L$ is maintained at a deterministic temperature $T_L$. Assume no internal heat generation and that material properties are deterministic and spatially uniform.\n\n1. Starting only from the steady one-dimensional energy balance and Fourier’s law, derive the temperature field $T(x)$ and obtain the mid-plane temperature $T_m \\equiv T(x=L/2)$ as a function of $T_0$ and $T_L$.\n2. Introduce the standardized Gaussian random variable $\\xi \\sim \\mathcal{N}(0,1)$ via $T_0 = \\mu_0 + \\sigma_0 \\,\\xi$. Using the orthonormal Hermite polynomial basis with respect to the standard normal measure,\n   $$\\psi_0(\\xi)=1, \\quad \\psi_1(\\xi)=\\xi, \\quad \\psi_2(\\xi)=\\frac{\\xi^2-1}{\\sqrt{2}},$$\n   construct the second-order Hermite Polynomial Chaos Expansion (PCE) for $T_m$,\n   $$T_m(\\xi) \\approx c_0 \\,\\psi_0(\\xi) + c_1 \\,\\psi_1(\\xi) + c_2 \\,\\psi_2(\\xi),$$\n   and determine the coefficients $c_0$, $c_1$, and $c_2$ in terms of $\\mu_0$, $\\sigma_0$, and $T_L$.\n3. Using only definitions and properties of orthonormal polynomial chaos, express the mean $\\mathbb{E}[T_m]$ and the variance $\\mathrm{Var}(T_m)$ in terms of the PCE coefficients.\n\nProvide your final answer as a row matrix $\\big[c_0 \\;\\; c_1 \\;\\; c_2 \\;\\; \\mathbb{E}[T_m] \\;\\; \\mathrm{Var}(T_m)\\big]$. Express $c_0$, $c_1$, $c_2$, and $\\mathbb{E}[T_m]$ in kelvin (K) and $\\mathrm{Var}(T_m)$ in kelvin squared ($\\mathrm{K}^2$). Do not include units inside your final boxed answer. No numerical values are required; leave your answer as closed-form symbolic expressions.", "solution": "Part 1: Derivation of the Temperature Field and Mid-Plane Temperature\nThe governing equation for steady-state, one-dimensional heat conduction with no internal heat generation is the energy balance, which, combined with Fourier's law ($q_x = -k \\frac{dT}{dx}$), yields:\n$$\n\\frac{d}{dx} \\left( k \\frac{dT}{dx} \\right) = 0\n$$\nGiven that the thermal conductivity $k$ is a constant, the equation simplifies to:\n$$\n\\frac{d^2T}{dx^2} = 0\n$$\nIntegrating this ordinary differential equation twice with respect to $x$ gives the general solution for the temperature field $T(x)$:\n$$\nT(x) = C_1 x + C_2\n$$\nwhere $C_1$ and $C_2$ are constants of integration. These constants are determined by applying the boundary conditions.\nAt $x=0$, the temperature is $T(0) = T_0$.\n$$\nT(0) = C_1(0) + C_2 = T_0 \\implies C_2 = T_0\n$$\nAt $x=L$, the temperature is $T(L) = T_L$.\n$$\nT(L) = C_1 L + C_2 = T_L\n$$\nSubstituting $C_2 = T_0$ into the second equation gives:\n$$\nC_1 L + T_0 = T_L \\implies C_1 = \\frac{T_L - T_0}{L}\n$$\nSubstituting the constants $C_1$ and $C_2$ back into the general solution gives the explicit temperature field $T(x)$:\n$$\nT(x) = \\left( \\frac{T_L - T_0}{L} \\right) x + T_0\n$$\nThe mid-plane temperature $T_m$ is the temperature evaluated at $x = L/2$.\n$$\nT_m \\equiv T(x=L/2) = \\left( \\frac{T_L - T_0}{L} \\right) \\frac{L}{2} + T_0 = \\frac{T_L - T_0}{2} + T_0 = \\frac{T_L + T_0}{2}\n$$\nThus, the mid-plane temperature is the arithmetic average of the two boundary temperatures.\n\nPart 2: Polynomial Chaos Expansion and Determination of Coefficients\nThe uncertain boundary temperature $T_0$ is modeled as $T_0 = \\mu_0 + \\sigma_0 \\xi$, where $\\xi \\sim \\mathcal{N}(0,1)$. We substitute this expression into the equation for $T_m$:\n$$\nT_m(\\xi) = \\frac{T_L + (\\mu_0 + \\sigma_0 \\xi)}{2} = \\frac{T_L + \\mu_0}{2} + \\frac{\\sigma_0}{2} \\xi\n$$\nThis shows that $T_m$ is a linear function of the random variable $\\xi$. We are asked to construct a second-order Polynomial Chaos Expansion (PCE) for $T_m$:\n$$\nT_m(\\xi) \\approx c_0 \\psi_0(\\xi) + c_1 \\psi_1(\\xi) + c_2 \\psi_2(\\xi)\n$$\nThe coefficients $c_j$ are determined via the Galerkin projection method. Due to the orthonormality of the Hermite polynomials $\\psi_j$ with respect to the standard normal probability measure, the coefficients are given by the expectation:\n$$\nc_j = \\mathbb{E}[T_m(\\xi) \\psi_j(\\xi)]\n$$\nWe compute each coefficient systematically.\nFor $c_0$:\n$$\nc_0 = \\mathbb{E}[T_m(\\xi) \\psi_0(\\xi)] = \\mathbb{E}[T_m(\\xi) \\cdot 1] = \\mathbb{E}\\left[ \\frac{T_L + \\mu_0}{2} + \\frac{\\sigma_0}{2} \\xi \\right]\n$$\nBy linearity of expectation, and using the fact that $\\mathbb{E}[\\xi] = 0$:\n$$\nc_0 = \\frac{T_L + \\mu_0}{2} + \\frac{\\sigma_0}{2} \\mathbb{E}[\\xi] = \\frac{T_L + \\mu_0}{2}\n$$\nFor $c_1$:\n$$\nc_1 = \\mathbb{E}[T_m(\\xi) \\psi_1(\\xi)] = \\mathbb{E}[T_m(\\xi) \\cdot \\xi] = \\mathbb{E}\\left[ \\left( \\frac{T_L + \\mu_0}{2} + \\frac{\\sigma_0}{2} \\xi \\right) \\xi \\right]\n$$\n$$\nc_1 = \\mathbb{E}\\left[ \\left(\\frac{T_L + \\mu_0}{2}\\right) \\xi + \\frac{\\sigma_0}{2} \\xi^2 \\right] = \\left(\\frac{T_L + \\mu_0}{2}\\right) \\mathbb{E}[\\xi] + \\frac{\\sigma_0}{2} \\mathbb{E}[\\xi^2]\n$$\nFor a standard normal variable $\\xi$, $\\mathbb{E}[\\xi] = 0$ and the variance $\\mathrm{Var}(\\xi) = \\mathbb{E}[\\xi^2] - (\\mathbb{E}[\\xi])^2 = 1$, which implies $\\mathbb{E}[\\xi^2] = 1$.\n$$\nc_1 = \\left(\\frac{T_L + \\mu_0}{2}\\right) (0) + \\frac{\\sigma_0}{2} (1) = \\frac{\\sigma_0}{2}\n$$\nFor $c_2$:\n$$\nc_2 = \\mathbb{E}[T_m(\\xi) \\psi_2(\\xi)] = \\mathbb{E}\\left[ T_m(\\xi) \\cdot \\frac{\\xi^2-1}{\\sqrt{2}} \\right] = \\frac{1}{\\sqrt{2}} \\mathbb{E}\\left[ \\left( \\frac{T_L + \\mu_0}{2} + \\frac{\\sigma_0}{2} \\xi \\right) (\\xi^2-1) \\right]\n$$\n$$\nc_2 = \\frac{1}{\\sqrt{2}} \\mathbb{E}\\left[ \\left(\\frac{T_L + \\mu_0}{2}\\right)(\\xi^2-1) + \\frac{\\sigma_0}{2}(\\xi^3-\\xi) \\right]\n$$\nBy linearity of expectation:\n$$\nc_2 = \\frac{1}{\\sqrt{2}} \\left[ \\left(\\frac{T_L + \\mu_0}{2}\\right)(\\mathbb{E}[\\xi^2]-\\mathbb{E}[1]) + \\frac{\\sigma_0}{2}(\\mathbb{E}[\\xi^3]-\\mathbb{E}[\\xi]) \\right]\n$$\nThe odd moments of a standard normal distribution are zero, so $\\mathbb{E}[\\xi]=0$ and $\\mathbb{E}[\\xi^3]=0$. We also have $\\mathbb{E}[\\xi^2]=1$.\n$$\nc_2 = \\frac{1}{\\sqrt{2}} \\left[ \\left(\\frac{T_L + \\mu_0}{2}\\right)(1-1) + \\frac{\\sigma_0}{2}(0-0) \\right] = 0\n$$\nThe coefficient $c_2$ is zero because the model output $T_m(\\xi)$ is a linear polynomial in $\\xi$. The PCE is exact and terminates at the first order. The second-order expansion is technically correct, but the second-order coefficient is simply zero.\n\nPart 3: Mean and Variance from PCE Coefficients\nA key property of an orthonormal PCE is that the statistical moments of the output can be computed directly from the coefficients.\nThe mean of the quantity of interest is the first coefficient, $c_0$:\n$$\n\\mathbb{E}[T_m] = \\mathbb{E}\\left[ \\sum_{j=0}^{\\infty} c_j \\psi_j(\\xi) \\right] = \\sum_{j=0}^{\\infty} c_j \\mathbb{E}[\\psi_j(\\xi)]\n$$\nDue to orthonormality, $\\mathbb{E}[\\psi_j(\\xi)] = \\mathbb{E}[\\psi_j(\\xi)\\psi_0(\\xi)] = \\delta_{j0}$ (the Kronecker delta). Thus, only the $j=0$ term survives:\n$$\n\\mathbb{E}[T_m] = c_0 = \\frac{\\mu_0 + T_L}{2}\n$$\nThe variance is the sum of the squares of the higher-order coefficients (for $j \\ge 1$):\n$$\n\\mathrm{Var}(T_m) = \\mathbb{E}[(T_m - \\mathbb{E}[T_m])^2] = \\mathbb{E}\\left[ \\left(\\sum_{j=1}^{\\infty} c_j \\psi_j(\\xi)\\right)^2 \\right] = \\sum_{j=1}^{\\infty} \\sum_{k=1}^{\\infty} c_j c_k \\mathbb{E}[\\psi_j(\\xi)\\psi_k(\\xi)]\n$$\nUsing the orthonormality condition $\\mathbb{E}[\\psi_j(\\xi)\\psi_k(\\xi)] = \\delta_{jk}$:\n$$\n\\mathrm{Var}(T_m) = \\sum_{j=1}^{\\infty} c_j^2\n$$\nFor our second-order expansion, since all coefficients $c_j$ for $j \\ge 2$ are zero, this sum simplifies to:\n$$\n\\mathrm{Var}(T_m) = c_1^2 + c_2^2 = \\left(\\frac{\\sigma_0}{2}\\right)^2 + 0^2 = \\frac{\\sigma_0^2}{4}\n$$\n\nSummary of results:\n- $c_0 = \\frac{\\mu_0 + T_L}{2}$\n- $c_1 = \\frac{\\sigma_0}{2}$\n- $c_2 = 0$\n- $\\mathbb{E}[T_m] = c_0 = \\frac{\\mu_0 + T_L}{2}$\n- $\\mathrm{Var}(T_m) = c_1^2 = \\frac{\\sigma_0^2}{4}$\n\nThese will be arranged in the specified row matrix for the final answer.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\mu_0 + T_L}{2} & \\frac{\\sigma_0}{2} & 0 & \\frac{\\mu_0 + T_L}{2} & \\frac{\\sigma_0^2}{4} \\end{pmatrix}}\n$$", "id": "2536803"}]}