{"hands_on_practices": [{"introduction": "The journey into solving inverse problems begins with a mastery of the forward problem. This practice focuses on the critical first step: ensuring your numerical simulation of the physical process is both stable and accurate [@problem_id:2497720]. By performing a von Neumann stability analysis on a discretized heat equation, you will not only derive the famous Courant–Friedrichs–Lewy (CFL) condition but also gain deep insight into why operating near the stability limit, while numerically stable, can introduce non-physical oscillations that catastrophically degrade the quality of an inverse solution.", "problem": "A one-dimensional ($1$D) semi-infinite solid occupies $x \\ge 0$ and obeys the heat equation $\\partial T/\\partial t = \\alpha\\,\\partial^{2}T/\\partial x^{2}$ with constant thermal diffusivity $\\alpha$. In an inverse heat conduction problem (IHCP), an unknown surface heat flux $q(t)$ at $x=0$ is to be estimated from internal temperature measurements at a fixed location $x=x_{m}$ by repeatedly solving the forward model and using gradient-based optimization. The forward model is discretized on a uniform spatial grid $x_{i}=i\\,\\Delta x$ and advanced in time using the forward-Euler method with a second-order central-difference approximation for the spatial derivative.\n\nStarting only from the heat equation, the stated discretization, and the definition of the Fourier amplification factor for linear constant-coefficient schemes, perform the following:\n\n- Derive the necessary and sufficient stability condition for the explicit scheme by von Neumann analysis, and express it in the form of an inequality on the non-dimensional time step parameter $r=\\alpha\\,\\Delta t/\\Delta x^{2}$. From this, determine the largest admissible value of the constant $C$ such that $\\Delta t_{\\max}=C\\,\\Delta x^{2}/\\alpha$.\n\n- Using your result, compute the maximum stable time step for the values $\\Delta x=8.0\\times 10^{-4}\\,\\text{m}$ and $\\alpha=8.5\\times 10^{-6}\\,\\text{m}^{2}\\,\\text{s}^{-1}$. Express your final numerical answer in seconds and round to $4$ significant figures.\n\n- Explain, based on the amplification factor you derived, how choosing $\\Delta t$ close to the stability limit affects (i) the accuracy of spatial temperature gradients in the forward solution and (ii) the stability and conditioning of the gradient used in the IHCP inversion when employing a standard quadratic data-misfit with Tikhonov regularization on $q(t)$. Your explanation must be grounded in the spectral properties of the scheme and must not assume any results not derivable from your amplification factor.\n\nProvide only the numerical value of the maximum stable time step as your final answer, in seconds, rounded to $4$ significant figures as requested. All intermediate derivations and discussions should be presented in your solution.", "solution": "The problem requires a three-part response: first, the derivation of the stability condition for a forward-time, central-space (FTCS) discretization of the $1$D heat equation; second, the calculation of the maximum stable time step for given parameters; and third, an explanation of the consequences of operating near this stability limit in the context of an inverse heat conduction problem (IHCP). The entire analysis must be self-contained, starting from the governing partial differential equation.\n\nThe one-dimensional heat equation is given as:\n$$\n\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}\n$$\nwhere $T$ is temperature, $t$ is time, $x$ is the spatial coordinate, and $\\alpha$ is the constant thermal diffusivity.\n\nWe discretize this equation on a uniform grid where $T(x_i, t_n) \\equiv T_i^n$, with $x_i = i \\Delta x$ and $t_n = n \\Delta t$. Using a forward-Euler scheme for the time derivative and a second-order central-difference scheme for the spatial derivative, we obtain the FTCS scheme:\n$$\n\\frac{T_i^{n+1} - T_i^n}{\\Delta t} = \\alpha \\frac{T_{i+1}^n - 2T_i^n + T_{i-1}^n}{(\\Delta x)^2}\n$$\nRearranging for the temperature at the next time step, $T_i^{n+1}$, gives:\n$$\nT_i^{n+1} = T_i^n + \\frac{\\alpha \\Delta t}{(\\Delta x)^2} \\left( T_{i+1}^n - 2T_i^n + T_{i-1}^n \\right)\n$$\nWe define the non-dimensional time step parameter, or Fourier number, as $r = \\frac{\\alpha \\Delta t}{(\\Delta x)^2}$. The equation then simplifies to:\n$$\nT_i^{n+1} = T_i^n + r \\left( T_{i+1}^n - 2T_i^n + T_{i-1}^n \\right)\n$$\n\nTo perform the von Neumann stability analysis, we consider the evolution of a single Fourier mode of the temperature field. We represent the temperature at time step $n$ as a superposition of such modes. A typical mode has the form:\n$$\nT_i^n = A^n(k) \\exp(I k x_i)\n$$\nwhere $A^n(k)$ is the amplitude of the mode with wavenumber $k$ at time step $n$, and $I$ is the imaginary unit ($I^2 = -1$). For the scheme to be stable, the amplitude of any mode must not grow in time, which means the magnitude of the amplification factor $G(k) = A^{n+1}(k) / A^n(k)$ must satisfy $|G(k)| \\le 1$ for all possible wavenumbers $k$.\n\nSubstituting the Fourier mode into the discretized equation:\n$$\nA^{n+1} \\exp(I k i \\Delta x) = A^n \\exp(I k i \\Delta x) + r \\left( A^n \\exp(I k (i+1) \\Delta x) - 2A^n \\exp(I k i \\Delta x) + A^n \\exp(I k (i-1) \\Delta x) \\right)\n$$\nDividing by $A^n \\exp(I k i \\Delta x)$, which is non-zero, we get the expression for the amplification factor $G(k)$:\n$$\nG(k) = \\frac{A^{n+1}}{A^n} = 1 + r \\left( \\exp(I k \\Delta x) - 2 + \\exp(-I k \\Delta x) \\right)\n$$\nUsing Euler's identity, $\\exp(I\\theta) + \\exp(-I\\theta) = 2\\cos(\\theta)$, we can simplify the term in the parenthesis:\n$$\nG(k) = 1 + r \\left( 2\\cos(k \\Delta x) - 2 \\right) = 1 - 2r \\left( 1 - \\cos(k \\Delta x) \\right)\n$$\nApplying the trigonometric half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$:\n$$\nG(k) = 1 - 4r \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)\n$$\nThe stability condition is $|G(k)| \\le 1$. Since $r > 0$ and $\\sin^2(\\cdot) \\ge 0$, the term $4r \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)$ is non-negative. This implies that $G(k)$ is always real and $G(k) \\le 1$. The stability condition therefore reduces to the single inequality:\n$$\nG(k) \\ge -1\n$$\n$$\n1 - 4r \\sin^2\\left(\\frac{k \\Delta x}{2}\\right) \\ge -1\n$$\n$$\n2 \\ge 4r \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)\n$$\n$$\nr \\le \\frac{2}{4 \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)} = \\frac{1}{2 \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)}\n$$\nThis inequality must hold for all possible wavenumbers $k$. The most restrictive, or \"worst-case\", condition occurs when the denominator is at its minimum value, which corresponds to the maximum value of $\\sin^2\\left(\\frac{k \\Delta x}{2}\\right)$. The maximum value of $\\sin^2(\\cdot)$ is $1$. This occurs for the highest frequency mode that the grid can resolve, where $k \\Delta x = \\pi$.\nSubstituting this maximum value into the inequality gives the necessary and sufficient stability condition:\n$$\nr \\le \\frac{1}{2}\n$$\nGiven the definition $r=\\alpha\\,\\Delta t/\\Delta x^{2}$, this implies:\n$$\n\\frac{\\alpha \\Delta t}{(\\Delta x)^2} \\le \\frac{1}{2} \\implies \\Delta t \\le \\frac{1}{2} \\frac{(\\Delta x)^2}{\\alpha}\n$$\nFrom this, the maximum allowed time step is $\\Delta t_{\\max} = \\frac{1}{2} \\frac{(\\Delta x)^2}{\\alpha}$. Comparing this with the form $\\Delta t_{\\max}=C\\,\\Delta x^{2}/\\alpha$, we find that the largest admissible value for the constant is $C = 1/2$.\n\nNext, we compute the maximum stable time step for the given values: $\\Delta x = 8.0 \\times 10^{-4}\\,\\text{m}$ and $\\alpha = 8.5 \\times 10^{-6}\\,\\text{m}^{2}\\,\\text{s}^{-1}$.\n$$\n\\Delta t_{\\max} = \\frac{1}{2} \\frac{(\\Delta x)^2}{\\alpha} = \\frac{1}{2} \\frac{(8.0 \\times 10^{-4}\\,\\text{m})^2}{8.5 \\times 10^{-6}\\,\\text{m}^{2}\\,\\text{s}^{-1}}\n$$\n$$\n\\Delta t_{\\max} = \\frac{1}{2} \\frac{64.0 \\times 10^{-8}}{8.5 \\times 10^{-6}} \\,\\text{s} = \\frac{32.0 \\times 10^{-8}}{8.5 \\times 10^{-6}} \\,\\text{s} = \\frac{32.0}{8.5} \\times 10^{-2} \\,\\text{s}\n$$\n$$\n\\Delta t_{\\max} \\approx 3.764705... \\times 10^{-2} \\,\\text{s}\n$$\nRounding to $4$ significant figures, we get $\\Delta t_{\\max} = 0.03765\\,\\text{s}$.\n\nFinally, we explain the consequences of choosing $\\Delta t$ close to this stability limit, based on the derived amplification factor $G(k) = 1 - 4r \\sin^2\\left(\\frac{k \\Delta x}{2}\\right)$.\n\n(i) Effect on the accuracy of spatial temperature gradients in the forward solution:\nAs $r$ approaches its limit of $1/2$, the amplification factor for the highest-frequency mode ($k \\Delta x = \\pi$) approaches $G(\\pi/\\Delta x) = 1 - 4(\\frac{1}{2}) \\sin^2(\\frac{\\pi}{2}) = 1 - 2 = -1$. An amplification factor of $-1$ means that the amplitude of this mode does not decay; instead, it is preserved while its sign flips at every time step. This corresponds to a non-physical, high-frequency \"checkerboard\" oscillation (e.g., a spatial profile alternating between positive and negative perturbations at adjacent grid points, with the entire pattern flipping sign at each time step). The true physical solution of the heat equation is diffusive, meaning high-frequency components should decay rapidly. The numerical scheme's failure to damp these modes when $r \\approx 1/2$ introduces significant error, which manifests as spatial oscillations. Spatial gradients, calculated using finite differences such as $\\frac{\\partial T}{\\partial x}|_i \\approx \\frac{T_{i+1} - T_{i-1}}{2\\Delta x}$, are extremely sensitive to such high-frequency content. The presence of these numerical oscillations will completely corrupt the computed gradient, making it dominated by artifactual noise rather than the true physical gradient. Thus, operating near the stability limit severely degrades the accuracy of spatial gradients.\n\n(ii) Effect on the stability and conditioning of the gradient used in the IHCP inversion:\nAn IHCP is fundamentally ill-posed because finding the boundary condition $q(t)$ from internal data requires amplifying high-frequency information that is naturally and strongly attenuated by the diffusion process. The process of inverting this attenuation is highly sensitive to noise.\nWhen the forward model is solved with $r \\approx 1/2$, it introduces its own high-frequency numerical noise into the computed temperature field $T(x_m, t)$, as explained above. The gradient-based optimization algorithm for the IHCP uses the mismatch between this computed temperature and the measured temperature to update the estimate of $q(t)$. The gradient calculation (often done via an adjoint method) will interpret the numerical noise in the forward solution as a real signal that must be explained by the boundary flux $q(t)$. This forces the optimization algorithm to introduce spurious, high-frequency oscillations into the estimate for $q(t)$ in an attempt to match the numerical noise. This makes the gradient of the misfit functional unstable and highly oscillatory. The conditioning of the inverse problem, which is already poor, is drastically worsened by the numerical instability of the forward solver. While Tikhonov regularization is designed to suppress such oscillatory solutions by penalizing the norm or semi-norm of $q(t)$, a forward solver operating on the edge of stability generates such strong numerical artifacts that the regularization may be insufficient or may require an impractically large regularization parameter, which in turn would oversmooth the estimated flux and destroy any useful detail. In summary, the spectral deficiency of the numerical scheme at $r \\approx 1/2$ directly pollutes the data-misfit gradient, leading to an unstable and ill-conditioned inversion process.", "answer": "$$\n\\boxed{0.03765}\n$$", "id": "2497720"}, {"introduction": "With a reliable forward model in hand, we confront the central challenge of inverse heat conduction: its inherent ill-posedness. This exercise uses the Singular Value Decomposition (SVD) as a powerful diagnostic tool to dissect the structure of the discretized inverse problem [@problem_id:2497780]. You will explore the profound connection between the physics of thermal diffusion, which naturally dampens high-frequency information, and the mathematical decay of singular values, which amplifies noise during inversion. This understanding is key to grasping how regularization techniques like Truncated Singular Value Decomposition (TSVD) work by acting as a low-pass filter to recover a stable and physically meaningful solution.", "problem": "An Inverse Heat Conduction Problem (IHCP) arises when an unknown time-dependent surface heat flux $q(t)$ on the boundary $x=0$ of a homogeneous semi-infinite solid $x \\ge 0$ must be inferred from interior temperature measurements. Consider the one-dimensional heat equation $\\partial T/\\partial t = \\alpha \\, \\partial^{2} T/\\partial x^{2}$ with thermal diffusivity $\\alpha>0$, zero initial condition, and governing boundary condition $-k \\, \\partial T/\\partial x(0,t) = q(t)$, where $k>0$ is the thermal conductivity. Temperature is measured at a fixed interior location $x=x_{m}>0$ for times $t \\in (0,T]$. After discretizing $q(t)$ on a suitable temporal basis and sampling the measured temperature, the forward map from coefficients of $q$ to the data $y \\in \\mathbb{R}^{m}$ is a linear system $A q = y + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ and $\\varepsilon$ represents measurement noise.\n\nStarting from the heat equation and its Laplace-transform solution in the semi-infinite domain, argue how temporal oscillations in $q(t)$ are transmitted to $T(x_{m},t)$ and how that transmission depends on frequency. Then interpret the Singular Value Decomposition (SVD) of $A$ in this context and explain which statements correctly characterize Truncated Singular Value Decomposition (TSVD) at truncation index $k$ in terms of filtering properties relative to the SVD expansion.\n\nChoose all statements that are correct:\n\nA. In the SVD $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$ and right singular vectors $v_{i}$, the TSVD solution at index $k$ is $\\widehat{q}_{k} = \\sum_{i=1}^{k} (u_{i}^{\\top} y/\\sigma_{i}) \\, v_{i}$. Because the heat equation attenuates high temporal frequencies, the right singular vectors $v_{i}$ tend to become more oscillatory as $i$ increases, so truncation at $k$ filters out higher-frequency components of $q$.\n\nB. TSVD keeps the $k$ smallest singular values to avoid over-smoothing and thereby emphasizes high-frequency content, which is desirable because measurement noise in IHCPs is predominantly low frequency.\n\nC. For diffusive forward operators arising from the heat equation, small singular values are associated with increasingly oscillatory right singular vectors. Truncating the SVD expansion at $k$ imposes an effective low-pass filter on $q$, with a cutoff determined by $k$.\n\nD. The TSVD filter factors in the spectral expansion are $f_{i} = \\sigma_{i}/(\\sigma_{i}^{2}+\\lambda)$ for some $\\lambda>0$, so truncating at $k$ is equivalent to choosing an appropriate $\\lambda$ in Tikhonov regularization.", "solution": "The analysis begins with the continuous problem to understand the physics of the forward operator. The one-dimensional heat equation is $\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}$. We apply the Laplace transform with respect to time $t$, with $s$ as the transform variable. Let $\\bar{T}(x,s) = \\mathcal{L}\\{T(x,t)\\}$. Given the zero initial condition $T(x,0)=0$, the transformed PDE becomes an ODE in $x$:\n$$ s\\bar{T}(x,s) = \\alpha \\frac{d^2 \\bar{T}}{dx^2} \\implies \\frac{d^2 \\bar{T}}{dx^2} - \\frac{s}{\\alpha}\\bar{T} = 0 $$\nThe general solution is $\\bar{T}(x,s) = C_1 e^{\\sqrt{s/\\alpha}x} + C_2 e^{-\\sqrt{s/\\alpha}x}$. For the temperature to remain bounded as $x \\to \\infty$, we must set $C_1 = 0$, assuming $\\text{Re}(\\sqrt{s}) > 0$. Thus, $\\bar{T}(x,s) = C_2 e^{-\\sqrt{s/\\alpha}x}$.\n\nThe constant $C_2$ is found from the boundary condition at $x=0$. The transformed boundary condition is $-k \\frac{d\\bar{T}}{dx}(0,s) = \\bar{q}(s)$, where $\\bar{q}(s) = \\mathcal{L}\\{q(t)\\}$. Differentiating $\\bar{T}(x,s)$ yields $\\frac{d\\bar{T}}{dx} = -C_2 \\sqrt{s/\\alpha} e^{-\\sqrt{s/\\alpha}x}$. At $x=0$, this gives $-k(-C_2\\sqrt{s/\\alpha}) = \\bar{q}(s)$, which determines $C_2 = \\frac{\\bar{q}(s)}{k\\sqrt{s/\\alpha}}$.\n\nThe complete solution in the Laplace domain is:\n$$ \\bar{T}(x,s) = \\bar{q}(s) \\frac{e^{-\\sqrt{s/\\alpha}x}}{k\\sqrt{s/\\alpha}} $$\nThis equation describes the transfer function from the input flux $\\bar{q}(s)$ to the temperature response $\\bar{T}(x,s)$. To understand the frequency dependence, we consider a sinusoidal input by setting $s = i\\omega$, where $\\omega$ is the angular frequency. The magnitude of the transfer function at a depth $x$ behaves as $|\\bar{T}(x,i\\omega)/\\bar{q}(i\\omega)| \\propto e^{-\\sqrt{\\omega/(2\\alpha)}x}$. This demonstrates that the amplitude of temperature oscillations is exponentially attenuated with both depth $x$ and the square root of the frequency $\\omega$. High-frequency components of the heat flux $q(t)$ are severely damped and barely register at the interior sensor location $x_m$.\n\nThis physical insight directly informs the structure of the discretized forward operator $A$. The matrix $A$ models this strong damping (low-pass filtering) behavior. We consider its Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\top} = \\sum_{i=1}^r \\sigma_i u_i v_i^\\top$, where $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ are the singular values. The right singular vectors $v_i$ represent input flux patterns, and the singular value $\\sigma_i$ is the amplification factor for that pattern. Because the physical system strongly attenuates high-frequency inputs, right singular vectors $v_i$ representing increasingly oscillatory (high-frequency) flux patterns must be associated with rapidly decreasing singular values $\\sigma_i$.\n\nTruncated SVD (TSVD) is a regularization method that addresses the ill-posed nature of the inverse problem. The TSVD solution at a truncation index $k$ is formed by including only the first $k$ terms of the SVD expansion of the inverse:\n$$ \\widehat{q}_{k} = \\sum_{i=1}^{k} \\frac{1}{\\sigma_i} (u_i^{\\top} y) v_i = \\sum_{i=1}^{k} \\frac{u_i^{\\top} y}{\\sigma_i} v_i $$\nBy truncating the sum, we explicitly discard contributions from terms with $i > k$. These are precisely the terms associated with small singular values and highly oscillatory vectors $v_i$, which are most corrupted by noise amplification. Therefore, TSVD acts as a low-pass filter.\n\n### Evaluation of Options\n\n**A. Correct.** The formula for the TSVD solution is correct. The reasoning that the heat equation's damping of high frequencies causes the right singular vectors $v_i$ associated with small singular values (large $i$) to be oscillatory is correct. The conclusion that truncating the summation at $k$ discards these high-frequency components is correct.\n\n**B. Incorrect.** TSVD keeps the $k$ *largest* singular values, as these carry the most significant, non-noisy information. It *suppresses*, not emphasizes, high-frequency content.\n\n**C. Correct.** This statement accurately summarizes the core principle. Diffusive operators are smoothing operators, meaning their singular values decay, and small singular values correspond to high-frequency singular vectors. Truncating the SVD expansion at index $k$ effectively acts as a low-pass filter, where $k$ determines the cutoff.\n\n**D. Incorrect.** The statement confuses the filter factors of TSVD with those of Tikhonov regularization. The filter factors for TSVD are a step function: $f_i = 1$ for $i \\le k$ and $f_i = 0$ for $i > k$. The factors given, $f_i = \\sigma_i^2 / (\\sigma_i^2+\\lambda)$, belong to Tikhonov regularization. The two methods are not equivalent; TSVD uses a sharp cutoff while Tikhonov uses a smooth one.", "answer": "$$\\boxed{AC}$$", "id": "2497780"}, {"introduction": "Many real-world scenarios involve complexities, such as temperature-dependent material properties, that render the inverse problem nonlinear. This practice transitions from the linear domain to the more general framework of nonlinear least-squares optimization [@problem_id:2497725]. You will derive the Gauss-Newton and Levenberg-Marquardt algorithms, two cornerstone iterative methods for solving such problems, and explore strategies to ensure they converge to a valid solution. This exercise provides the analytical and computational foundation for tackling a vast range of practical inverse heat conduction challenges.", "problem": "Consider a one-dimensional slab of thickness $L$ with spatial coordinate $x \\in [0,L]$. The thermal conductivity is temperature dependent, $k(T) = k_0 \\left(1 + \\beta \\left(T - T_{\\mathrm{ref}}\\right)\\right)$, with constant density $\\rho$ and heat capacity $c_p$. The forward heat transfer model is governed by the transient energy balance\n$$\n\\rho c_p \\frac{\\partial T}{\\partial t} = \\frac{\\partial}{\\partial x}\\!\\left(k(T)\\,\\frac{\\partial T}{\\partial x}\\right),\n$$\nsubject to a prescribed initial temperature field and boundary conditions. At the boundary $x=0$, the heat flux $q(t)$ is unknown and is parameterized by a vector of coefficients $\\mathbf{m} \\in \\mathbb{R}^M$ through a fixed basis in time, so that $q(t;\\mathbf{m}) = \\sum_{j=1}^M m_j \\,\\varphi_j(t)$. Temperature measurements are available at $x=x_m$ and discrete times $t_1,\\dots,t_N$, producing data $\\mathbf{d} \\in \\mathbb{R}^N$. Define the forward response map $\\mathbf{y}(\\mathbf{m}) \\in \\mathbb{R}^N$ as the vector of model-predicted temperatures at $(x_m,t_i)$, $i=1,\\dots,N$.\n\nBecause $k(T)$ depends on $T$, the forward map $\\mathbf{y}(\\mathbf{m})$ is nonlinear. The Inverse Heat Conduction Problem (IHCP) is posed as a Tikhonov-regularized, weighted nonlinear least-squares optimization:\n$$\n\\min_{\\mathbf{m} \\in \\mathbb{R}^M} \\; \\Phi(\\mathbf{m}) \\equiv \\frac{1}{2}\\left\\| \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d}\\right) \\right\\|_2^2 + \\frac{\\gamma}{2}\\left\\|\\mathbf{L}\\left(\\mathbf{m} - \\mathbf{m}_0\\right)\\right\\|_2^2,\n$$\nwhere $\\mathbf{W} \\in \\mathbb{R}^{N \\times N}$ is a symmetric positive-definite weighting, $\\gamma>0$ is a regularization parameter, and $\\mathbf{L} \\in \\mathbb{R}^{P \\times M}$ is a regularization operator with prior $\\mathbf{m}_0$.\n\nTasks:\n- Starting from the definition of $\\Phi(\\mathbf{m})$, a first-order Taylor expansion of the model residual, and the principle of least squares, derive the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ as the solution of a linear system in terms of the Jacobian matrix and residual vector.\n- By introducing a damping parameter $\\lambda \\ge 0$ and a diagonal scaling $\\mathbf{D} \\succ \\mathbf{0}$, derive the Levenberg–Marquardt (LM) step $\\mathbf{s}_{\\mathrm{LM}}$ as a modified normal equation. State clearly how $\\mathbf{D}$ enters the system.\n- Briefly discuss two globalization strategies to handle nonconvexity induced by $k(T)$: a backtracking line-search using a sufficient-decrease condition and a trust-region strategy based on the ratio of actual to predicted reduction. State a sufficient-decrease condition and the expression for the predicted reduction under a Gauss–Newton quadratic model.\n- Numerical micro-instance: at a current iterate $\\mathbf{m}^{(k)}$ assume the whitened residual and Jacobian (i.e., post-multiplication by $\\mathbf{W}$) are\n$$\n\\mathbf{r} \\equiv \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}^{(k)}) - \\mathbf{d}\\right) = \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix}, \\qquad\n\\mathbf{J} \\equiv \\mathbf{W}\\,\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)}) = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 0.8 \\end{bmatrix}.\n$$\nUse a Tikhonov term with $\\gamma = 0.1$, $\\mathbf{L} = \\mathbf{I}$, and assume $\\mathbf{m}^{(k)} = \\mathbf{m}_0$ so that the regularization gradient term vanishes at $\\mathbf{m}^{(k)}$. For the LM scaling take $\\mathbf{D} = \\mathrm{diag}\\!\\left(\\mathbf{J}^{\\top}\\mathbf{J}\\right)$ and damping $\\lambda = 0.5$. Compute the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ and the Levenberg–Marquardt step $\\mathbf{s}_{\\mathrm{LM}}$, and then compute the ratio\n$$\n\\eta \\equiv \\frac{\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2}{\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2}.\n$$\nExpress your final answer for $\\eta$ as a unitless real number rounded to four significant figures.", "solution": "First, we address the derivation of the Gauss–Newton and Levenberg–Marquardt update steps. The objective function to be minimized is\n$$\n\\Phi(\\mathbf{m}) = \\frac{1}{2}\\left\\| \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d}\\right) \\right\\|_2^2 + \\frac{\\gamma}{2}\\left\\|\\mathbf{L}\\left(\\mathbf{m} - \\mathbf{m}_0\\right)\\right\\|_2^2.\n$$\nLet us define the whitened model residual $\\mathbf{r}(\\mathbf{m}) = \\mathbf{W}(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d})$ and the regularization residual $\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}) = \\mathbf{L}(\\mathbf{m} - \\mathbf{m}_0)$. The objective function is now $\\Phi(\\mathbf{m}) = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{m})\\|_2^2 + \\frac{\\gamma}{2}\\|\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m})\\|_2^2$.\n\nTo derive an iterative update, we consider an iterate $\\mathbf{m}^{(k)}$ and seek a step $\\mathbf{s}$ such that $\\mathbf{m}^{(k+1)} = \\mathbf{m}^{(k)} + \\mathbf{s}$ minimizes $\\Phi$. The Gauss–Newton method is based on a linear approximation of the nonlinear residual $\\mathbf{r}(\\mathbf{m})$ around $\\mathbf{m}^{(k)}$. A first-order Taylor expansion gives\n$$\n\\mathbf{r}(\\mathbf{m}^{(k)} + \\mathbf{s}) \\approx \\mathbf{r}(\\mathbf{m}^{(k)}) + \\mathbf{J}^{(k)}\\mathbf{s},\n$$\nwhere $\\mathbf{J}^{(k)} \\equiv \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)}) = \\mathbf{W} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)})$ is the whitened Jacobian matrix at $\\mathbf{m}^{(k)}$. The regularization residual is already linear in $\\mathbf{m}$, so\n$$\n\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)} + \\mathbf{s}) = \\mathbf{L}(\\mathbf{m}^{(k)} + \\mathbf{s} - \\mathbf{m}_0) = \\mathbf{L}(\\mathbf{m}^{(k)} - \\mathbf{m}_0) + \\mathbf{L}\\mathbf{s} = \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) + \\mathbf{L}\\mathbf{s}.\n$$\nSubstituting these approximations into the objective function yields a quadratic model for $\\Phi$ in terms of the step $\\mathbf{s}$. To find the step $\\mathbf{s}_{\\mathrm{GN}}$ that minimizes this quadratic model, we set its gradient with respect to $\\mathbf{s}$ to zero. This yields the Gauss–Newton normal equations for the step $\\mathbf{s}_{\\mathrm{GN}}$:\n$$\n\\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L} \\right) \\mathbf{s}_{\\mathrm{GN}} = - \\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{r}(\\mathbf{m}^{(k)}) + \\gamma \\mathbf{L}^\\top \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) \\right).\n$$\nThe matrix on the left is the regularized Gauss–Newton approximation of the Hessian of $\\Phi$. The vector on the right is the negative of the gradient of $\\Phi$ at $\\mathbf{m}^{(k)}$.\n\nThe Levenberg–Marquardt (LM) method modifies the Gauss–Newton step to improve robustness. It introduces a damping parameter $\\lambda \\ge 0$ and a diagonal scaling matrix $\\mathbf{D} \\succ \\mathbf{0}$. The LM step $\\mathbf{s}_{\\mathrm{LM}}$ is the solution of the modified linear system:\n$$\n\\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L} + \\lambda \\mathbf{D} \\right) \\mathbf{s}_{\\mathrm{LM}} = - \\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{r}(\\mathbf{m}^{(k)}) + \\gamma \\mathbf{L}^\\top \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) \\right).\n$$\nThe term $\\lambda \\mathbf{D}$ acts as a trust-region-like regularization. When $\\lambda$ is large, the step is small and points towards the steepest descent direction. When $\\lambda$ is small, the step approaches the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$. The scaling matrix $\\mathbf{D}$ is often chosen as $\\mathbf{D} = \\mathrm{diag}((\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)})$.\n\nDue to the nonlinearity of $k(T)$, globalization strategies are required to ensure convergence.\n1.  **Backtracking Line Search**: A search direction $\\mathbf{s}$ is computed. Then, a step length $\\alpha \\in (0, 1]$ is sought to ensure sufficient decrease. A sufficient-decrease condition, such as the Armijo condition, must be satisfied:\n    $$\n    \\Phi(\\mathbf{m}^{(k)} + \\alpha\\mathbf{s}) \\le \\Phi(\\mathbf{m}^{(k)}) + c_1 \\alpha \\nabla\\Phi(\\mathbf{m}^{(k)})^\\top \\mathbf{s},\n    $$\n    for a small constant $c_1 \\in (0, 1)$. One starts with $\\alpha=1$ and reduces it until the condition is met.\n2.  **Trust-Region Strategy**: A quadratic model $q_k(\\mathbf{s})$ is built and trusted within a radius $\\Delta_k$. The quality of the step $\\mathbf{s}_k$ (which minimizes $q_k$ in the trust region) is assessed by the ratio of actual reduction to predicted reduction, $\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}$. Under the Gauss–Newton quadratic model, the predicted reduction is:\n    $$\n    \\text{pred}_k = -\\nabla\\Phi(\\mathbf{m}^{(k)})^\\top \\mathbf{s}_k - \\frac{1}{2}\\mathbf{s}_k^\\top \\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L} \\right) \\mathbf{s}_k.\n    $$\n    Based on $\\rho_k$, the step is accepted or rejected and $\\Delta_k$ is updated.\n\nFinally, we perform the numerical calculation. The given data are:\n$\\mathbf{r} = \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix}$, $\\mathbf{J} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 0.8 \\end{bmatrix}$, $\\gamma = 0.1$, $\\mathbf{L} = \\mathbf{I}$, $\\lambda = 0.5$.\nThe assumption $\\mathbf{m}^{(k)} = \\mathbf{m}_0$ implies that the regularization residual term $\\gamma \\mathbf{L}^\\top \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)})$ is zero. The right-hand side (RHS) for both steps simplifies to $-\\mathbf{J}^\\top \\mathbf{r}$.\n$$\n\\text{RHS} = -\\mathbf{J}^\\top \\mathbf{r} = -\\begin{bmatrix} 1.0 & 0.2 \\\\ 0.5 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix} = -\\begin{bmatrix} 0.36 \\\\ 0.04 \\end{bmatrix}.\n$$\nThe Gauss–Newton Hessian approximation is $\\mathbf{H}_{\\mathrm{GN}} = \\mathbf{J}^\\top\\mathbf{J} + \\gamma\\mathbf{I}$:\n$$\n\\mathbf{J}^\\top\\mathbf{J} = \\begin{bmatrix} 1.04 & 0.66 \\\\ 0.66 & 0.89 \\end{bmatrix}, \\quad \\mathbf{H}_{\\mathrm{GN}} = \\begin{bmatrix} 1.04 & 0.66 \\\\ 0.66 & 0.89 \\end{bmatrix} + 0.1 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.14 & 0.66 \\\\ 0.66 & 0.99 \\end{bmatrix}.\n$$\nThe Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ solves $\\mathbf{H}_{\\mathrm{GN}} \\mathbf{s}_{\\mathrm{GN}} = \\text{RHS}$.\n$\\det(\\mathbf{H}_{\\mathrm{GN}}) = (1.14)(0.99) - (0.66)^2 = 0.693$.\n$$\n\\mathbf{s}_{\\mathrm{GN}} = \\frac{1}{0.693} \\begin{bmatrix} 0.99 & -0.66 \\\\ -0.66 & 1.14 \\end{bmatrix} \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix} = \\frac{1}{0.693} \\begin{bmatrix} -0.33 \\\\ 0.192 \\end{bmatrix}.\n$$\n$\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2^2 = \\frac{(-0.33)^2 + (0.192)^2}{(0.693)^2} \\approx 0.303527 \\implies \\|\\mathbf{s}_{\\mathrm{GN}}\\|_2 \\approx 0.550933$.\n\nFor the Levenberg–Marquardt step, the scaling matrix is $\\mathbf{D} = \\mathrm{diag}(\\mathbf{J}^\\top\\mathbf{J}) = \\begin{bmatrix} 1.04 & 0 \\\\ 0 & 0.89 \\end{bmatrix}$.\nThe LM matrix is $\\mathbf{H}_{\\mathrm{LM}} = \\mathbf{H}_{\\mathrm{GN}} + \\lambda\\mathbf{D}$:\n$$\n\\mathbf{H}_{\\mathrm{LM}} = \\begin{bmatrix} 1.14 & 0.66 \\\\ 0.66 & 0.99 \\end{bmatrix} + 0.5 \\begin{bmatrix} 1.04 & 0 \\\\ 0 & 0.89 \\end{bmatrix} = \\begin{bmatrix} 1.66 & 0.66 \\\\ 0.66 & 1.435 \\end{bmatrix}.\n$$\nThe LM step $\\mathbf{s}_{\\mathrm{LM}}$ solves $\\mathbf{H}_{\\mathrm{LM}} \\mathbf{s}_{\\mathrm{LM}} = \\text{RHS}$.\n$\\det(\\mathbf{H}_{\\mathrm{LM}}) = (1.66)(1.435) - (0.66)^2 = 1.9465$.\n$$\n\\mathbf{s}_{\\mathrm{LM}} = \\frac{1}{1.9465} \\begin{bmatrix} 1.435 & -0.66 \\\\ -0.66 & 1.66 \\end{bmatrix} \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix} = \\frac{1}{1.9465} \\begin{bmatrix} -0.4902 \\\\ 0.1712 \\end{bmatrix}.\n$$\n$\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2^2 = \\frac{(-0.4902)^2 + (0.1712)^2}{(1.9465)^2} \\approx 0.0711576 \\implies \\|\\mathbf{s}_{\\mathrm{LM}}\\|_2 \\approx 0.266754$.\n\nFinally, the ratio $\\eta$:\n$$\n\\eta = \\frac{\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2}{\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2} \\approx \\frac{0.266754}{0.550933} \\approx 0.484186.\n$$\nRounding to four significant figures gives $\\eta = 0.4842$.", "answer": "$$\\boxed{0.4842}$$", "id": "2497725"}]}