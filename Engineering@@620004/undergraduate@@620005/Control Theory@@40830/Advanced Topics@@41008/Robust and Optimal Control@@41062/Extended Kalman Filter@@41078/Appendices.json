{"hands_on_practices": [{"introduction": "The heart of the Extended Kalman Filter's ability to handle nonlinear systems lies in its use of linearization. This first practice focuses on a core skill: calculating the measurement Jacobian, $H_k$. This exercise frames the task within a practical engineering scenario of monitoring an AC motor, demonstrating how we mathematically determine the sensitivity of a sensor measurement to changes in the underlying state variables. Mastering this step is essential for correctly implementing the EKF's measurement update stage.", "problem": "An engineer is developing a real-time health monitoring system for an industrial AC motor. The system uses an Extended Kalman Filter (EKF) to estimate the motor's key electrical parameters, which can change due to factors like operating temperature and material degradation. The two parameters of interest are the winding resistance $R$ and the winding inductance $L$.\n\nThe state vector of the system is defined as $x = \\begin{pmatrix} R \\\\ L \\end{pmatrix}$. A sensor provides a single measurement at each time step: the magnitude of the motor's electrical impedance, $Z$. This measurement is taken at a constant, known angular frequency $\\omega$. The physical relationship between the state variables and the measurement is described by the non-linear function $h(x)$:\n$$Z = h(x) = \\sqrt{R^2 + (\\omega L)^2}$$\nThe measurement update stage of the EKF requires linearizing this measurement function around the current best estimate of the state, denoted as $\\hat{x} = \\begin{pmatrix} \\hat{R} \\\\ \\hat{L} \\end{pmatrix}$. This linearization is captured by the measurement Jacobian matrix, $H$.\n\nDetermine the symbolic expression for the measurement Jacobian matrix $H$, which is defined as $H = \\frac{\\partial h}{\\partial x}\\bigg|_{x=\\hat{x}}$. Your answer should be expressed in terms of $\\hat{R}$, $\\hat{L}$, and $\\omega$.", "solution": "We are given the scalar measurement function $h(x)$ with state $x=\\begin{pmatrix}R \\\\ L\\end{pmatrix}$:\n$$\nh(x)=\\sqrt{R^{2}+(\\omega L)^{2}}=(R^{2}+\\omega^{2}L^{2})^{\\frac{1}{2}}.\n$$\nThe measurement Jacobian $H$ is the row vector of partial derivatives of $h$ with respect to $R$ and $L$, evaluated at $x=\\hat{x}=\\begin{pmatrix}\\hat{R} \\\\ \\hat{L}\\end{pmatrix}$.\n\nUsing the chain rule,\n$$\n\\frac{\\partial h}{\\partial R}=\\frac{1}{2}(R^{2}+\\omega^{2}L^{2})^{-\\frac{1}{2}}\\cdot 2R=\\frac{R}{\\sqrt{R^{2}+\\omega^{2}L^{2}}}.\n$$\nSimilarly,\n$$\n\\frac{\\partial h}{\\partial L}=\\frac{1}{2}(R^{2}+\\omega^{2}L^{2})^{-\\frac{1}{2}}\\cdot 2\\omega^{2}L=\\frac{\\omega^{2}L}{\\sqrt{R^{2}+\\omega^{2}L^{2}}}.\n$$\nEvaluating at $R=\\hat{R}$ and $L=\\hat{L}$, the Jacobian becomes\n$$\nH=\\begin{pmatrix}\n\\dfrac{\\hat{R}}{\\sqrt{\\hat{R}^{2}+\\omega^{2}\\hat{L}^{2}}} & \\dfrac{\\omega^{2}\\hat{L}}{\\sqrt{\\hat{R}^{2}+\\omega^{2}\\hat{L}^{2}}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{\\hat{R}}{\\sqrt{\\hat{R}^{2}+\\omega^{2}\\hat{L}^{2}}} & \\dfrac{\\omega^{2}\\hat{L}}{\\sqrt{\\hat{R}^{2}+\\omega^{2}\\hat{L}^{2}}}\\end{pmatrix}}$$", "id": "1574774"}, {"introduction": "Moving beyond pure calculation, it is crucial to develop an intuitive understanding of how the EKF update affects our belief about the state. This problem presents a hypothetical scenario involving a drone tracking a target to explore the geometric consequences of a measurement update. By analyzing how a bearing measurement reshapes the uncertainty ellipse, you will gain insight into how the EKF incorporates information and reduces uncertainty in a non-uniform way, a key characteristic of filtering with nonlinear sensors.", "problem": "An autonomous drone is hovering at a stationary position, which we define as the origin $(0,0)$ of a 2D Cartesian coordinate system. It is tasked with estimating the position of a stationary target on the ground. The state of the target is represented by the vector $x = [p_x, p_y]^T$.\n\nInitially, the drone's belief about the target's position is described by a Gaussian distribution with a mean $\\hat{x}_{k|k-1}$ and a covariance matrix $P_{k|k-1}$. The initial uncertainty is isotropic, meaning the covariance matrix is of the form $P_{k|k-1} = \\sigma^2 I$, where $\\sigma^2$ is a positive constant and $I$ is the 2x2 identity matrix. This implies the initial uncertainty contour is a circle.\n\nThe drone then uses its onboard camera to take a single, highly precise measurement of the bearing angle to the target. The measurement function is $h(x) = \\arctan(p_y/p_x)$, and the measurement noise is modeled as a zero-mean Gaussian with a very small variance, $R_k = \\sigma_\\theta^2$.\n\nAn Extended Kalman Filter (EKF) is used to update the state estimate and its covariance based on this new measurement, resulting in a posterior estimate $\\hat{x}_{k|k}$ and a posterior covariance $P_{k|k}$. The posterior covariance matrix $P_{k|k}$ describes the new uncertainty, which can be visualized as an ellipse in the 2D plane. The line of sight is defined as the line connecting the drone (at the origin) to the estimated position of the target.\n\nWhich of the following statements most accurately describes the orientation of this updated uncertainty ellipse?\n\nA. The major axis (longest axis) of the ellipse is oriented perpendicular to the line of sight.\n\nB. The ellipse remains a circle, but with a radius smaller than the initial uncertainty.\n\nC. The major axis (longest axis) of the ellipse is oriented along the line of sight.\n\nD. The orientation of the ellipse is at a 45-degree angle to the line of sight.\n\nE. The shape of the uncertainty region is no longer an ellipse due to the non-linear measurement.", "solution": "Let the state be $x = [p_{x}, p_{y}]^{T}$ and the prior covariance be $P_{k|k-1} = \\sigma^{2} I$. The measurement is the bearing $\\theta = h(x) = \\arctan(p_{y}/p_{x})$ with noise covariance $R_{k} = \\sigma_{\\theta}^{2}$. Assume the linearization point $\\hat{x}_{k|k-1} = [\\hat{p}_{x}, \\hat{p}_{y}]^{T}$ satisfies $\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2} > 0$ so that the Jacobian exists.\n\nThe EKF uses the measurement Jacobian $H = \\partial h/\\partial x$ evaluated at $\\hat{x}_{k|k-1}$. Using the derivative of $\\arctan(p_{y}/p_{x})$ with respect to $p_{x}$ and $p_{y}$,\n$$\nH = \\left[ \\frac{\\partial h}{\\partial p_{x}} \\;\\; \\frac{\\partial h}{\\partial p_{y}} \\right]\n= \\left[ -\\frac{\\hat{p}_{y}}{\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}} \\;\\; \\frac{\\hat{p}_{x}}{\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}} \\right].\n$$\nDefine $r^{2} = \\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}$. Then $H = \\frac{1}{r^{2}}[-\\hat{p}_{y}, \\hat{p}_{x}]$.\n\nThe innovation covariance is\n$$\nS = H P_{k|k-1} H^{T} + R_{k} = \\sigma^{2} H H^{T} + \\sigma_{\\theta}^{2}.\n$$\nSince $H H^{T} = \\frac{\\hat{p}_{y}^{2} + \\hat{p}_{x}^{2}}{r^{4}} = \\frac{1}{r^{2}}$, we obtain\n$$\nS = \\frac{\\sigma^{2}}{r^{2}} + \\sigma_{\\theta}^{2}.\n$$\nThe Kalman gain is\n$$\nK = P_{k|k-1} H^{T} S^{-1} = \\sigma^{2} I \\, H^{T} S^{-1} = \\frac{\\sigma^{2}}{S} H^{T}.\n$$\nTherefore $K$ is proportional to $H^{T} = \\frac{1}{r^{2}}[-\\hat{p}_{y}, \\hat{p}_{x}]^{T}$, which is orthogonal to the line-of-sight direction $[\\hat{p}_{x}, \\hat{p}_{y}]^{T}$ because $[-\\hat{p}_{y}, \\hat{p}_{x}] \\cdot [\\hat{p}_{x}, \\hat{p}_{y}] = 0$. Thus, the state update moves primarily perpendicular to the line of sight.\n\nThe posterior covariance for a scalar measurement is\n$$\nP_{k|k} = (I - K H) P_{k|k-1} = P_{k|k-1} - K H P_{k|k-1}.\n$$\nCompute the rank-one term:\n$$\nK H P_{k|k-1} = \\left(\\frac{\\sigma^{2}}{S} H^{T}\\right) \\left(H \\sigma^{2}\\right) = \\frac{\\sigma^{4}}{S} \\left(H^{T} H\\right).\n$$\nNote that\n$$\nH^{T} H = \\frac{1}{r^{4}}\n\\begin{bmatrix}\n\\hat{p}_{y}^{2} & -\\hat{p}_{x}\\hat{p}_{y} \\\\\n-\\hat{p}_{x}\\hat{p}_{y} & \\hat{p}_{x}^{2}\n\\end{bmatrix}.\n$$\nIntroduce the orthonormal basis aligned with radial (line-of-sight) and tangential (perpendicular) directions:\n$$\nu_{r} = \\frac{1}{r} \\begin{bmatrix} \\hat{p}_{x} \\\\ \\hat{p}_{y} \\end{bmatrix}, \\quad\nu_{t} = \\frac{1}{r} \\begin{bmatrix} -\\hat{p}_{y} \\\\ \\hat{p}_{x} \\end{bmatrix}.\n$$\nThen $H = \\frac{1}{r} u_{t}^{T}$ and $H^{T} H = \\frac{1}{r^{2}} u_{t} u_{t}^{T}$. Consequently,\n$$\nP_{k|k} = \\sigma^{2} I - \\frac{\\sigma^{4}}{S} \\cdot \\frac{1}{r^{2}} \\, u_{t} u_{t}^{T}.\n$$\nThis shows that the posterior covariance is reduced only along $u_{t}$ (perpendicular to the line of sight) and remains unchanged along $u_{r}$ (the line of sight). Specifically, the posterior variances along these principal directions are\n$$\n\\lambda_{r}^{+} = u_{r}^{T} P_{k|k} u_{r} = \\sigma^{2},\n\\qquad\n\\lambda_{t}^{+} = u_{t}^{T} P_{k|k} u_{t} = \\sigma^{2} - \\frac{\\sigma^{4}}{S} \\cdot \\frac{1}{r^{2}}.\n$$\nSince $S = \\frac{\\sigma^{2}}{r^{2}} + \\sigma_{\\theta}^{2} > 0$, we have $\\lambda_{t}^{+} < \\sigma^{2}$, with the limiting case $\\lambda_{t}^{+} \\to 0$ as $\\sigma_{\\theta}^{2} \\to 0$. Therefore, the largest posterior variance is along the line of sight, and the smallest is perpendicular to it. The updated uncertainty ellipse thus has its major axis oriented along the line of sight.\n\nHence, the most accurate statement is that the major axis is oriented along the line of sight.", "answer": "$$\\boxed{C}$$", "id": "1574759"}, {"introduction": "A robust filter design requires understanding not only how the filter works, but also how it can fail. This advanced problem investigates a critical failure mode known as \"estimator blindness,\" where the EKF stops incorporating new measurement data. By working backward from the failure condition, you will diagnose the specific circumstances under which the measurement Jacobian vanishes, causing the Kalman gain to become zero and effectively blinding the filter. This exercise is invaluable for learning to anticipate and mitigate potential issues in real-world EKF applications.", "problem": "An autonomous robot navigates a 2D plane. Its state at time step $k$ is represented by the vector $\\mathbf{x}_k = [p_{x,k}, p_{y,k}]^T$, which contains its Cartesian coordinates. The robot's state is estimated using an Extended Kalman Filter (EKF).\n\nThe filter employs the following model equations:\n1.  **Prediction Model**: The state is predicted forward in time using a simple random walk model, where the process transition matrix is the identity matrix $I$.\n    -   Predicted state: $\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k-1|k-1}$\n    -   Predicted covariance: $P_{k|k-1} = P_{k-1|k-1} + Q$, where $Q$ is a positive definite process noise covariance matrix.\n\n2.  **Measurement Model**: At each time step, a sensor provides a scalar measurement $z_k$ which is related to the robot's state by the non-linear function $h(\\mathbf{x}_k)$:\n    -   $z_k = h(\\mathbf{x}_k) + v_k$, where $h(\\mathbf{x}_k) = (p_{x,k}^2 + p_{y,k}^2 - L^2)^2$.\n    -   The constant parameter is given as $L = 5.0$ m.\n    -   $v_k$ is a zero-mean Gaussian measurement noise with variance $R > 0$.\n\n3.  **Update Step**: The predicted state and covariance are updated using the measurement $z_k$:\n    -   Measurement Jacobian: $H_k = \\frac{\\partial h}{\\partial \\mathbf{x}}|_{\\hat{\\mathbf{x}}_{k|k-1}}$\n    -   Innovation Covariance: $S_k = H_k P_{k|k-1} H_k^T + R$\n    -   Kalman Gain: $K_k = P_{k|k-1} H_k^T S_k^{-1}$\n    -   Updated State: $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))$\n    -   Updated Covariance: $P_{k|k} = (I - K_k H_k) P_{k|k-1}$\n\nA critical failure mode, known as state estimator blindness, is discovered. In this mode, the filter completely ignores new measurement data, resulting in the updated state being identical to the predicted state, i.e., $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$. This failure is observed to occur regardless of the value of the measurement $z_k$, provided the innovation term, $z_k - h(\\hat{\\mathbf{x}}_{k|k-1})$, is non-zero.\n\nThis specific failure occurs when the predicted state estimate $\\hat{\\mathbf{x}}_{k|k-1}$ lies on the positive $p_x$-axis, meaning its form is $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$ for some $p_x > 0$.\n\nAssuming that the predicted error covariance matrix $P_{k|k-1}$ is always positive definite, determine the specific value of $p_x$ that induces this filter blindness. Express your answer in meters, rounded to two significant figures.", "solution": "The problem asks for the value of $p_x$ in a predicted state estimate $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$ that causes the Extended Kalman Filter (EKF) to exhibit \"blindness\". This condition is defined by the updated state estimate being equal to the predicted state estimate, $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$, even for a non-zero measurement innovation.\n\nLet's start with the state update equation of the EKF:\n$$\n\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))\n$$\nThe condition for filter blindness is $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$. Substituting this into the update equation gives:\n$$\n\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))\n$$\nThis simplifies to:\n$$\nK_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1})) = \\mathbf{0}\n$$\nThe problem states that this blindness occurs for any non-zero measurement innovation, which means the scalar term $(z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))$ is non-zero. For the equation to hold true, the Kalman gain vector $K_k$ must be the zero vector, $K_k = \\mathbf{0}$.\n\nNow, let's analyze the equation for the Kalman gain:\n$$\nK_k = P_{k|k-1} H_k^T S_k^{-1}\n$$\nFor $K_k$ to be the zero vector, we must have $P_{k|k-1} H_k^T S_k^{-1} = \\mathbf{0}$.\nThe innovation covariance $S_k = H_k P_{k|k-1} H_k^T + R$. Since $P_{k|k-1}$ is positive definite and $R > 0$, $S_k$ is a positive scalar, so its inverse $S_k^{-1}$ exists and is non-zero.\nTherefore, the condition for a zero Kalman gain simplifies to:\n$$\nP_{k|k-1} H_k^T = \\mathbf{0}\n$$\nThe problem states that the predicted covariance matrix $P_{k|k-1}$ is positive definite. A positive definite matrix is, by definition, invertible. We can multiply both sides by $P_{k|k-1}^{-1}$ from the left:\n$$\nP_{k|k-1}^{-1} (P_{k|k-1} H_k^T) = P_{k|k-1}^{-1} \\mathbf{0}\n$$\n$$\n(P_{k|k-1}^{-1} P_{k|k-1}) H_k^T = \\mathbf{0}\n$$\n$$\nI H_k^T = \\mathbf{0} \\implies H_k^T = \\mathbf{0}\n$$\nThis means that the measurement Jacobian matrix $H_k$, evaluated at the predicted state $\\hat{\\mathbf{x}}_{k|k-1}$, must be a zero row vector.\n\nThe measurement function is given by $h(p_x, p_y) = (p_x^2 + p_y^2 - L^2)^2$. The state is $\\mathbf{x} = [p_x, p_y]^T$. The Jacobian $H$ is a $1 \\times 2$ matrix:\n$$\nH = \\begin{bmatrix} \\frac{\\partial h}{\\partial p_x} & \\frac{\\partial h}{\\partial p_y} \\end{bmatrix}\n$$\nLet's compute the partial derivatives using the chain rule:\n$$\n\\frac{\\partial h}{\\partial p_x} = 2(p_x^2 + p_y^2 - L^2)^{1} \\cdot \\frac{\\partial}{\\partial p_x}(p_x^2 + p_y^2 - L^2) = 2(p_x^2 + p_y^2 - L^2)(2p_x) = 4p_x(p_x^2 + p_y^2 - L^2)\n$$\n$$\n\\frac{\\partial h}{\\partial p_y} = 2(p_x^2 + p_y^2 - L^2)^{1} \\cdot \\frac{\\partial}{\\partial p_y}(p_x^2 + p_y^2 - L^2) = 2(p_x^2 + p_y^2 - L^2)(2p_y) = 4p_y(p_x^2 + p_y^2 - L^2)\n$$\nSo, the Jacobian matrix is:\n$$\nH = \\begin{bmatrix} 4p_x(p_x^2 + p_y^2 - L^2) & 4p_y(p_x^2 + p_y^2 - L^2) \\end{bmatrix}\n$$\nThe problem states that the filter blindness occurs when the predicted state is $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$. We must evaluate the Jacobian $H_k$ at this specific state. Let's substitute the coordinates of $\\hat{\\mathbf{x}}_{k|k-1}$ into the expressions for the partial derivatives:\n$$\nH_k = \\begin{bmatrix} 4p_x(p_x^2 + 0^2 - L^2) & 4(0)(p_x^2 + 0^2 - L^2) \\end{bmatrix}\n$$\n$$\nH_k = \\begin{bmatrix} 4p_x(p_x^2 - L^2) & 0 \\end{bmatrix}\n$$\nTo achieve filter blindness, we need $H_k = \\mathbf{0} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}$. The second component is already zero. The first component must also be zero:\n$$\n4p_x(p_x^2 - L^2) = 0\n$$\nThe problem specifies that we are looking for a solution where $p_x > 0$. This implies that the term $4p_x$ is non-zero. Therefore, for the product to be zero, the other term must be zero:\n$$\np_x^2 - L^2 = 0\n$$\n$$\np_x^2 = L^2\n$$\nTaking the square root of both sides gives $p_x = \\pm L$. Since the problem specifies $p_x > 0$, we must choose the positive root:\n$$\np_x = L\n$$\nThe problem provides the value $L=5.0$ m. Thus, the critical value of $p_x$ is $5.0$ m.\n\nThe question asks for the answer to be rounded to two significant figures. The value $5.0$ has two significant figures.\n\nTherefore, the filter experiences blindness when the predicted state is on the positive x-axis at a distance from the origin exactly equal to the parameter $L$.", "answer": "$$\\boxed{5.0}$$", "id": "1574809"}]}