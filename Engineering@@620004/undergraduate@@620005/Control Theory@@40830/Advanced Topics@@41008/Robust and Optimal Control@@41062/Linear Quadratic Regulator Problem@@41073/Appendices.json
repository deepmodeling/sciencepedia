{"hands_on_practices": [{"introduction": "The power of the Linear Quadratic Regulator (LQR) lies in its ability to translate high-level performance goals into a precise, optimal control law. This first exercise bridges the gap between the abstract cost function and the concrete behavior of the controlled system. By working through a simple scalar system, you will derive the explicit relationship between the system parameters ($a, b$), the cost weights ($q, r$), and the resulting closed-loop pole location, providing a foundational understanding of how LQR achieves stabilization [@problem_id:1589492].", "problem": "Consider a simplified one-dimensional model for a particle's deviation, $x(t)$, from an unstable equilibrium point in a magnetic confinement system. The dynamics of the deviation are described by the linear differential equation:\n$$\n\\dot{x}(t) = ax(t) + bu(t)\n$$\nwhere $u(t)$ is the control input voltage applied to an electromagnet. The parameters $a$ and $b$ are positive real constants representing the inherent instability of the system and the effectiveness of the control input, respectively.\n\nTo stabilize the particle at the equilibrium point ($x=0$), a feedback controller is designed. The controller's behavior is determined by optimizing a performance criterion using the Linear Quadratic Regulator (LQR) method. This involves minimizing a cost functional, $J$, over an infinite time horizon, defined as:\n$$\nJ = \\int_{0}^{\\infty} \\left(q x(t)^2 + r u(t)^2\\right) dt\n$$\nHere, $q$ and $r$ are positive real constants that assign a penalty to the state deviation and the control effort, respectively.\n\nThe optimal LQR control law is a state feedback controller of the form $u(t) = -Kx(t)$, where $K$ is a constant gain. The application of this controller modifies the system dynamics into a stable, first-order closed-loop system, $\\dot{x}(t) = s x(t)$. Your task is to determine the location of the closed-loop system's pole, $s$, in terms of the physical and cost parameters $a, b, q$, and $r$.", "solution": "We start from the linear system $\\dot{x}(t) = a x(t) + b u(t)$ with cost $J = \\int_{0}^{\\infty} \\left(q x(t)^{2} + r u(t)^{2}\\right) dt$, where $a>0$, $b>0$, $q>0$, and $r>0$. The continuous-time LQR optimal control law is $u(t) = -K x(t)$ with $K = R^{-1} B^{T} P$, where $P$ is the unique positive semidefinite stabilizing solution of the algebraic Riccati equation\n$$\nA^{T} P + P A - P B R^{-1} B^{T} P + Q = 0.\n$$\nIn the scalar case, $A=a$, $B=b$, $Q=q$, and $R=r$, so the Riccati equation becomes\n$$\n2 a P - \\frac{b^{2}}{r} P^{2} + q = 0.\n$$\nThe closed-loop system under $u=-Kx$ is $\\dot{x} = (a - bK) x$, so its single pole is\n$$\ns = a - b K = a - \\frac{b^{2}}{r} P.\n$$\nTo express $s$ directly in terms of $a$, $b$, $q$, and $r$, substitute $P = \\frac{r}{b^{2}}(a - s)$ into the Riccati equation:\n$$\n2 a \\left(\\frac{r}{b^{2}}(a - s)\\right) - \\frac{b^{2}}{r} \\left(\\frac{r}{b^{2}}(a - s)\\right)^{2} + q = 0,\n$$\nwhich simplifies to\n$$\n\\frac{2 a r}{b^{2}}(a - s) - \\frac{r}{b^{2}}(a - s)^{2} + q = 0.\n$$\nMultiplying by $\\frac{b^{2}}{r}$ yields\n$$\n2 a (a - s) - (a - s)^{2} + \\frac{q b^{2}}{r} = 0.\n$$\nLetting $y = a - s$, we obtain the quadratic equation\n$$\ny^{2} - 2 a y - \\frac{q b^{2}}{r} = 0.\n$$\nSolving for $y$ gives\n$$\ny = a \\pm \\sqrt{a^{2} + \\frac{q b^{2}}{r}}.\n$$\nSince $P = \\frac{r}{b^{2}} y$ must be nonnegative for the stabilizing solution and $a>0$, we select the positive branch $y = a + \\sqrt{a^{2} + \\frac{q b^{2}}{r}}$. Therefore,\n$$\ns = a - y = a - \\left(a + \\sqrt{a^{2} + \\frac{q b^{2}}{r}}\\right) = - \\sqrt{a^{2} + \\frac{q b^{2}}{r}}.\n$$\nThis is a real negative pole, ensuring closed-loop stability.", "answer": "$$\\boxed{-\\sqrt{a^{2}+\\frac{q b^{2}}{r}}}$$", "id": "1589492"}, {"introduction": "Understanding the LQR framework goes beyond mere calculation; it requires developing an intuition for the trade-offs involved in tuning. This practice explores a fundamental aspect of this trade-off by examining the system's behavior under an extreme condition: when the cost of control effort approaches infinity. By analyzing the asymptotic location of the closed-loop pole as the control weight $R$ becomes very large, you will verify the intuitive notion that an infinitely expensive control leads to an inactive controller, revealing the open-loop dynamics of the system [@problem_id:1589454].", "problem": "Consider a scalar, linear time-invariant (LTI) system described by the state-space equation:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\nwhere $x(t)$ is the state, $u(t)$ is the control input, and $A$ and $B$ are non-zero real scalar constants. The system is known to be open-loop stable.\n\nA state-feedback controller of the form $u(t) = -Kx(t)$ is designed using the Linear Quadratic Regulator (LQR) framework. The controller gain $K$ is chosen to minimize the infinite-horizon quadratic cost function:\n$$\nJ = \\int_{0}^{\\infty} \\left( Q x(t)^2 + R u(t)^2 \\right) dt\n$$\nwhere $Q$ and $R$ are positive scalar weighting factors ($Q > 0, R > 0$). This design results in a closed-loop system $\\dot{x}(t) = (A - BK)x(t)$.\n\nDetermine the asymptotic location of the single pole of this closed-loop system as the control weighting parameter $R$ approaches infinity. Express your answer in terms of the system parameter $A$.", "solution": "We have the scalar continuous-time LQR problem with dynamics $\\dot{x}(t) = A x(t) + B u(t)$ and state-feedback $u(t) = -K x(t)$ minimizing\n$$\nJ = \\int_{0}^{\\infty} \\left( Q x(t)^{2} + R u(t)^{2} \\right) dt,\n$$\nwith $Q>0$ and $R>0$. The optimal gain is $K = R^{-1} B P$, where $P$ solves the algebraic Riccati equation\n$$\nA^{T} P + P A - P B R^{-1} B^{T} P + Q = 0.\n$$\nFor scalars, $A^{T} = A$ and $B^{T} = B$, so the ARE becomes\n$$\n2 A P - \\frac{B^{2}}{R} P^{2} + Q = 0.\n$$\nThe closed-loop pole is\n$$\n\\lambda(R) = A - B K = A - \\frac{B^{2}}{R} P.\n$$\nAs $R \\to \\infty$, we analyze the ARE. Since $R^{-1} \\to 0$, the term with $R^{-1}$ vanishes in the limit, and any limit point $P_{\\infty}$ must satisfy the Lyapunov equation\n$$\n2 A P_{\\infty} + Q = 0.\n$$\nSolving gives\n$$\nP_{\\infty} = -\\frac{Q}{2 A}.\n$$\nBecause the system is open-loop stable, $A<0$, so $P_{\\infty}>0$ and is finite. Therefore,\n$$\n\\lim_{R \\to \\infty} K = \\lim_{R \\to \\infty} \\frac{B}{R} P = 0,\n$$\nand the closed-loop pole tends to\n$$\n\\lim_{R \\to \\infty} \\lambda(R) = A - \\lim_{R \\to \\infty} \\frac{B^{2}}{R} P = A.\n$$\nHence, the asymptotic location of the single closed-loop pole as $R \\to \\infty$ is $A$.", "answer": "$$\\boxed{A}$$", "id": "1589454"}, {"introduction": "We now pivot from analyzing a given LQR problem to a more profound and practical question: can a pre-existing, effective controller be justified as LQR-optimal? This exercise introduces the concept of inverse optimal control, where you will determine the family of cost functions for which a given stabilizing gain is the solution. By working through this problem, you will discover that optimality is not tied to a unique cost function and learn how to construct a state-weighting matrix $Q$ that validates a given controller, connecting practical controller design with the theoretical rigor of optimal control [@problem_id:1589483].", "problem": "Consider a controllable second-order linear time-invariant (LTI) system governed by the state-space equation $\\dot{x}(t) = Ax(t) + bu(t)$, where the state is $x(t) \\in \\mathbb{R}^2$ and the control input is a scalar $u(t) \\in \\mathbb{R}$. The system matrices are given in terms of symbolic parameters $\\alpha, \\beta, \\gamma$ as:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ \\alpha & \\beta \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0 \\\\ \\gamma \\end{pmatrix}\n$$\nA state-feedback controller of the form $u(t) = -kx(t)$ has been designed, with a known row vector gain $k = \\begin{pmatrix} k_1 & k_2 \\end{pmatrix}$. This gain is known to stabilize the system, meaning the closed-loop matrix $A_{cl} = A - bk$ is Hurwitz.\n\nWe wish to determine if this controller is optimal for a Linear Quadratic Regulator (LQR) problem with the cost function $J = \\int_{0}^{\\infty} (x^T Q x + u^2) dt$. Notice the control-weighting \"matrix\" is the scalar $R=1$. The state-weighting matrix $Q$ is symmetric and positive semi-definite but is unknown.\n\nFor a given stabilizing gain $k$ and a fixed $R$, there is generally not a unique matrix $Q$ for which $k$ is the optimal LQR gain. Instead, there exists a family of such matrices. For the given system, this family of solutions for $Q$ can be parameterized by a single scalar variable.\n\nYour task is to find a general expression for the state-weighting matrix $Q$. Express your answer in terms of the given symbolic constants $\\alpha, \\beta, \\gamma, k_1, k_2$, and the parameter $p_{11}$, which represents the entry in the first row and first column of the symmetric positive-definite matrix $P$ that is the unique solution to the corresponding Algebraic Riccati Equation.", "solution": "We consider the continuous-time LQR with cost $J=\\int_{0}^{\\infty}(x^{T}Qx+u^{2})\\,dt$ and $R=1$. The Algebraic Riccati Equation (ARE) is\n$$\nA^{T}P+PA-Pbb^{T}P+Q=0,\n$$\nand the optimal gain is\n$$\nk=R^{-1}b^{T}P=b^{T}P.\n$$\nLet $P=\\begin{pmatrix}p_{11}&p_{12}\\\\ p_{12}&p_{22}\\end{pmatrix}$ with $P=P^{T}\\succ 0$. Using $b=\\begin{pmatrix}0\\\\ \\gamma\\end{pmatrix}$, we compute\n$$\nk=b^{T}P=\\begin{pmatrix}0&\\gamma\\end{pmatrix}\\begin{pmatrix}p_{11}&p_{12}\\\\ p_{12}&p_{22}\\end{pmatrix}=\\begin{pmatrix}\\gamma p_{12}&\\gamma p_{22}\\end{pmatrix}.\n$$\nEquating with the given stabilizing gain $k=\\begin{pmatrix}k_{1}&k_{2}\\end{pmatrix}$ yields\n$$\np_{12}=\\frac{k_{1}}{\\gamma},\\qquad p_{22}=\\frac{k_{2}}{\\gamma},\n$$\nso $p_{11}$ remains a free parameter (denoted $p_{11}$). Next, from the ARE we obtain\n$$\nQ=-A^{T}P-PA+Pbb^{T}P.\n$$\nWith $A=\\begin{pmatrix}0&1\\\\ \\alpha&\\beta\\end{pmatrix}$ and $A^{T}=\\begin{pmatrix}0&\\alpha\\\\ 1&\\beta\\end{pmatrix}$, compute\n$$\nA^{T}P=\\begin{pmatrix}\\alpha p_{12}&\\alpha p_{22}\\\\ p_{11}+\\beta p_{12}&p_{12}+\\beta p_{22}\\end{pmatrix},\\qquad\nPA=\\begin{pmatrix}\\alpha p_{12}&p_{11}+\\beta p_{12}\\\\ \\alpha p_{22}&p_{12}+\\beta p_{22}\\end{pmatrix},\n$$\nso\n$$\nA^{T}P+PA=\\begin{pmatrix}\n2\\alpha p_{12} & p_{11}+\\beta p_{12}+\\alpha p_{22}\\\\\np_{11}+\\beta p_{12}+\\alpha p_{22} & 2(p_{12}+\\beta p_{22})\n\\end{pmatrix}.\n$$\nAlso,\n$$\nPbb^{T}P=(Pb)(b^{T}P)=(Pb)\\,k=\\begin{pmatrix}\\gamma p_{12}\\\\ \\gamma p_{22}\\end{pmatrix}\\begin{pmatrix}k_{1}&k_{2}\\end{pmatrix}\n=\\begin{pmatrix}k_{1}^{2}&k_{1}k_{2}\\\\ k_{1}k_{2}&k_{2}^{2}\\end{pmatrix},\n$$\nsince $\\gamma p_{12}=k_{1}$ and $\\gamma p_{22}=k_{2}$. Substituting $p_{12}=\\frac{k_{1}}{\\gamma}$ and $p_{22}=\\frac{k_{2}}{\\gamma}$ gives\n$$\nA^{T}P+PA=\\begin{pmatrix}\n\\frac{2\\alpha k_{1}}{\\gamma} & p_{11}+\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma}\\\\\np_{11}+\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma} & \\frac{2(k_{1}+\\beta k_{2})}{\\gamma}\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nQ=Pbb^{T}P-(A^{T}P+PA)=\\begin{pmatrix}\nk_{1}^{2}-\\frac{2\\alpha k_{1}}{\\gamma} & k_{1}k_{2}-p_{11}-\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma}\\\\\nk_{1}k_{2}-p_{11}-\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma} & k_{2}^{2}-\\frac{2(k_{1}+\\beta k_{2})}{\\gamma}\n\\end{pmatrix}.\n$$\nThis is the desired one-parameter family of state-weighting matrices $Q$, parameterized by $p_{11}$, for which the given stabilizing gain $k$ is the optimal LQR feedback with $R=1$.", "answer": "$$\\boxed{\\begin{pmatrix}\nk_{1}^{2}-\\frac{2\\alpha k_{1}}{\\gamma} & k_{1}k_{2}-p_{11}-\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma}\\\\\nk_{1}k_{2}-p_{11}-\\frac{\\alpha k_{2}+\\beta k_{1}}{\\gamma} & k_{2}^{2}-\\frac{2\\left(k_{1}+\\beta k_{2}\\right)}{\\gamma}\n\\end{pmatrix}}$$", "id": "1589483"}]}