{"hands_on_practices": [{"introduction": "The first step in any practical system identification task is to structure the collected data into a format suitable for estimation. This exercise focuses on this crucial translation, where you will take a given discrete-time model and a set of experimental measurements to construct the regressor matrix, the cornerstone of the least squares formulation $\\mathbf{y} = \\mathbf{\\Phi}\\boldsymbol{\\theta}$ [@problem_id:1588623]. Mastering this setup is essential before proceeding to calculate the parameter estimates themselves.", "problem": "An engineer is conducting a system identification experiment on a thermal process. The process is modeled by the following second-order linear discrete-time equation:\n$$y(k) = a_1 y(k-1) + a_2 y(k-2) + b_1 u(k-1)$$\nwhere $k$ is the discrete time index, $y(k)$ is the measured temperature output, $u(k)$ is the heater power input, and $a_1, a_2, b_1$ are the unknown model parameters.\n\nThe system is initially at rest, meaning $y(k) = 0$ for all $k \\leq 0$. At time $k=1$, a unit step input is applied to the system, such that the input signal is defined as $u(k) = 0$ for $k < 1$ and $u(k) = 1$ for $k \\geq 1$.\n\nThe following sequence of temperature measurements is recorded:\n$y(1) = 0.5$\n$y(2) = 1.5$\n$y(3) = 2.0$\n$y(4) = 2.2$\n$y(5) = 2.3$\n\nFor the purpose of estimating the parameters using least squares, the system equations are stacked into a linear regression model of the form $\\mathbf{y} = \\mathbf{\\Phi} \\boldsymbol{\\theta}$, where $\\boldsymbol{\\theta} = \\begin{pmatrix} a_1 & a_2 & b_1 \\end{pmatrix}^T$ is the vector of parameters. Using the available measurements from $k=2$ to $k=5$, construct the numerical regressor matrix $\\mathbf{\\Phi}$.", "solution": "The given model is the second-order linear discrete-time equation\n$$y(k)=a_{1}y(k-1)+a_{2}y(k-2)+b_{1}u(k-1).$$\nTo form a linear regression $\\mathbf{y}=\\mathbf{\\Phi}\\boldsymbol{\\theta}$ with $\\boldsymbol{\\theta}=\\begin{pmatrix}a_{1}&a_{2}&b_{1}\\end{pmatrix}^{T}$, each row of the regressor matrix $\\mathbf{\\Phi}$ for time index $k$ is\n$$\\boldsymbol{\\phi}(k)^T=\\begin{pmatrix}y(k-1) & y(k-2) & u(k-1)\\end{pmatrix}.$$\nThe system is at rest for $k\\leq 0$, so $y(0)=0$, and the input is a unit step at $k=1$, so $u(k)=0$ for $k<1$ and $u(k)=1$ for $k\\geq 1$. Therefore, for $k=2,3,4,5$ we have $u(k-1)=1$.\n\nUsing the measured outputs $y(1)=0.5$, $y(2)=1.5$, $y(3)=2.0$, $y(4)=2.2$, $y(5)=2.3$, the rows of $\\mathbf{\\Phi}$ are computed as follows:\n- For $k=2$: $\\boldsymbol{\\phi}(2)^T=\\begin{pmatrix}y(1) & y(0) & u(1)\\end{pmatrix}=\\begin{pmatrix}0.5 & 0 & 1\\end{pmatrix}$.\n- For $k=3$: $\\boldsymbol{\\phi}(3)^T=\\begin{pmatrix}y(2) & y(1) & u(2)\\end{pmatrix}=\\begin{pmatrix}1.5 & 0.5 & 1\\end{pmatrix}$.\n- For $k=4$: $\\boldsymbol{\\phi}(4)^T=\\begin{pmatrix}y(3) & y(2) & u(3)\\end{pmatrix}=\\begin{pmatrix}2.0 & 1.5 & 1\\end{pmatrix}$.\n- For $k=5$: $\\boldsymbol{\\phi}(5)^T=\\begin{pmatrix}y(4) & y(3) & u(4)\\end{pmatrix}=\\begin{pmatrix}2.2 & 2.0 & 1\\end{pmatrix}$.\n\nStacking these rows yields the numerical regressor matrix\n$$\\mathbf{\\Phi}=\\begin{pmatrix}\n0.5 & 0 & 1\\\\\n1.5 & 0.5 & 1\\\\\n2.0 & 1.5 & 1\\\\\n2.2 & 2.0 & 1\n\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}0.5 & 0 & 1 \\\\ 1.5 & 0.5 & 1 \\\\ 2.0 & 1.5 & 1 \\\\ 2.2 & 2.0 & 1\\end{pmatrix}}$$", "id": "1588623"}, {"introduction": "Real-world systems are rarely described by simple homogeneous equations; they are often subject to constant disturbances or sensor biases. This practice demonstrates how to adapt the least squares framework to account for such effects by incorporating a constant offset term into the model [@problem_id:1588627]. You will work through the full estimation process, from modifying the regressor matrix to solving for all the system parameters, including the disturbance.", "problem": "An engineer is working to identify the parameters of a discrete-time dynamical system observed in a manufacturing process. The system's output, $y(k)$, is believed to follow a first-order linear model driven by an input, $u(k)$, and affected by a constant environmental disturbance, $d$. The proposed model is:\n$$y(k) = a y(k-1) + b u(k-1) + d$$\nwhere $k$ is the time index, and $a$, $b$, and $d$ are the unknown constant parameters to be determined. A sequence of input and output data has been collected from the process as follows:\n- Input sequence: $u(0) = 1$, $u(1) = 0$, $u(2) = 1$, $u(3) = 0$.\n- Output sequence: $y(0) = 2$, $y(1) = 4.1$, $y(2) = 2.9$, $y(3) = 4.6$, $y(4) = 3.3$.\n\nUsing the entire dataset provided, apply the method of batch least squares to find the best estimates for the parameters. The parameter vector to be estimated is $\\boldsymbol{\\theta} = \\begin{pmatrix} a & b & d \\end{pmatrix}^T$.\n\nProvide your final answer as a row matrix containing the estimated values of $a$, $b$, and $d$, in that order. Round each parameter to three significant figures.", "solution": "We model the data with the first-order linear system\n$$y(k) = a\\,y(k-1) + b\\,u(k-1) + d,$$\nand form the batch least-squares regression $\\mathbf{y} = \\mathbf{\\Phi} \\boldsymbol{\\theta}$ with $\\boldsymbol{\\theta} = \\begin{pmatrix} a & b & d \\end{pmatrix}^{T}$. Using the samples for $k=1,2,3,4$, we have\n$$\\mathbf{\\Phi} = \\begin{pmatrix}\n2 & 1 & 1\\\\\n4.1 & 0 & 1\\\\\n2.9 & 1 & 1\\\\\n4.6 & 0 & 1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\n4.1\\\\\n2.9\\\\\n4.6\\\\\n3.3\n\\end{pmatrix}.$$\nThe batch least-squares solution is\n$$\\hat{\\boldsymbol{\\theta}} = \\left(\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^{T}\\mathbf{y},$$\nso we compute\n$$\\mathbf{\\Phi}^{T}\\mathbf{\\Phi} = \\begin{pmatrix}\n50.38 & 4.9 & 13.6\\\\\n4.9 & 2 & 2\\\\\n13.6 & 2 & 4\n\\end{pmatrix}, \\quad\n\\mathbf{\\Phi}^{T}\\mathbf{y} = \\begin{pmatrix}\n48.61\\\\\n8.7\\\\\n14.9\n\\end{pmatrix}.$$\nThus $\\hat{\\boldsymbol{\\theta}}$ solves the normal equations\n$$\\begin{cases}\n50.38\\,a + 4.9\\,b + 13.6\\,d = 48.61,\\\\\n4.9\\,a + 2\\,b + 2\\,d = 8.7,\\\\\n13.6\\,a + 2\\,b + 4\\,d = 14.9.\n\\end{cases}$$\nSubtract the second from the third to eliminate $b$:\n$$(13.6 - 4.9)a + (2-2)b + (4-2)d = 14.9 - 8.7,$$\nwhich gives\n$$8.7\\,a + 2\\,d = 6.2 \\;\\Rightarrow\\; d = 3.1 - 4.35\\,a.$$\nFrom the second equation,\n$$2\\,b = 8.7 - 4.9\\,a - 2\\,d = 8.7 - 4.9\\,a - (6.2 - 8.7\\,a) = 2.5 + 3.8\\,a,$$\nso\n$$b = 1.25 + 1.9\\,a.$$\nSubstitute $b$ and $d$ into the first equation:\n$$50.38\\,a + 4.9(1.25 + 1.9\\,a) + 13.6(3.1 - 4.35\\,a) = 48.61.$$\nCompute terms:\n$$50.38\\,a + 6.125 + 9.31\\,a + 42.16 - 59.16\\,a = 48.61,$$\n$$(50.38 + 9.31 - 59.16)a + (6.125 + 42.16) = 48.61,$$\n$$0.53\\,a + 48.285 = 48.61 \\;\\Rightarrow\\; 0.53\\,a = 0.325 \\;\\Rightarrow\\; a = \\frac{0.325}{0.53} \\approx 0.613207547.$$\nThen\n$$b = 1.25 + 1.9\\,a \\approx 1.25 + 1.9 \\times 0.613207547 \\approx 2.415094339,$$\n$$d = 3.1 - 4.35\\,a \\approx 3.1 - 4.35 \\times 0.613207547 \\approx 0.432547171.$$\nRounding each parameter to three significant figures yields\n$$a \\approx 0.613,\\quad b \\approx 2.42,\\quad d \\approx 0.433.$$", "answer": "$$\\boxed{\\begin{pmatrix}0.613 & 2.42 & 0.433\\end{pmatrix}}$$", "id": "1588627"}, {"introduction": "Standard least squares can falter when experimental data lacks sufficient excitation, leading to an ill-conditioned or singular information matrix and unreliable parameter estimates. This advanced practice introduces ridge regression, a powerful technique that ensures a stable and unique solution by adding a regularization term to the cost function [@problem_id:1588663]. You will explore the theoretical consequences of this method, specifically by analyzing the bias it introduces, to understand the fundamental trade-off between estimation accuracy and robustness.", "problem": "In the field of system identification and control, a common task is to estimate a parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^p$ from a set of measurements that can be described by the linear model $\\mathbf{y} = \\mathbf{\\Phi}\\boldsymbol{\\theta} + \\mathbf{e}$. Here, $\\mathbf{y} \\in \\mathbb{R}^N$ is the vector of measurements, $\\mathbf{\\Phi} \\in \\mathbb{R}^{N \\times p}$ is the known regressor matrix, and $\\mathbf{e} \\in \\mathbb{R}^N$ is a vector of unmeasured noise with a zero mean, i.e., $\\mathbb{E}[\\mathbf{e}]=\\mathbf{0}$.\n\nWhen experimental conditions lead to a regressor matrix $\\mathbf{\\Phi}$ for which the \"information matrix\" $\\mathbf{R} = \\mathbf{\\Phi}^T\\mathbf{\\Phi}$ is ill-conditioned or singular, the standard least-squares estimate becomes highly sensitive to noise. To mitigate this issue, a method known as ridge regression (or Tikhonov regularization) is employed. The ridge regression estimate, $\\hat{\\boldsymbol{\\theta}}_{ridge}$, is found by minimizing the following cost function:\n$$ J(\\boldsymbol{\\theta}) = \\|\\mathbf{y} - \\mathbf{\\Phi}\\boldsymbol{\\theta}\\|_2^2 + \\gamma\\|\\boldsymbol{\\theta}\\|_2^2 $$\nwhere $\\| \\cdot \\|_2$ denotes the Euclidean norm and $\\gamma > 0$ is a scalar regularization parameter.\n\nThe introduction of the regularization term, while stabilizing the solution, introduces a systematic bias into the estimate. Let this bias be defined as $\\mathbf{B} = \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_{ridge}] - \\boldsymbol{\\theta}$, where the expectation is taken over the noise $\\mathbf{e}$, and we assume the regressors in $\\mathbf{\\Phi}$ are deterministic or uncorrelated with the noise. We are interested in understanding how this bias is geometrically distributed across the eigenspace of the information matrix $\\mathbf{R}$.\n\nLet $\\mathbf{R} = \\mathbf{V}\\mathbf{\\Lambda} \\mathbf{V}^T$ be the eigendecomposition of the symmetric, positive semi-definite matrix $\\mathbf{R}$, where $\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_p]$ is an orthogonal matrix whose columns $\\mathbf{v}_i$ are the orthonormal eigenvectors of $\\mathbf{R}$, and $\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_p)$ is the diagonal matrix of corresponding non-negative eigenvalues $\\lambda_i$.\n\nYour task is to determine the component of the bias vector $\\mathbf{B}$ along the direction of an arbitrary eigenvector $\\mathbf{v}_i$. Find a closed-form analytic expression for this component, which is given by the scalar projection $c_i = \\mathbf{v}_i^T \\mathbf{B}$. Your final answer should be expressed in terms of the regularization parameter $\\gamma$, the eigenvalue $\\lambda_i$, and the component of the true parameter vector $\\boldsymbol{\\theta}$ along the direction of $\\mathbf{v}_i$, which is denoted by $\\alpha_i = \\mathbf{v}_i^T \\boldsymbol{\\theta}$.", "solution": "We start from the ridge regression cost\n$$\nJ(\\boldsymbol{\\theta})=\\|\\mathbf{y}-\\mathbf{\\Phi}\\boldsymbol{\\theta}\\|_{2}^{2}+\\gamma\\|\\boldsymbol{\\theta}\\|_{2}^{2}.\n$$\nSetting the gradient with respect to $\\boldsymbol{\\theta}$ to zero yields the normal equations for ridge regression. Using $\\nabla_{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\mathbf{\\Phi}\\boldsymbol{\\theta}\\|_{2}^{2}=-2\\mathbf{\\Phi}^{T}(\\mathbf{y}-\\mathbf{\\Phi}\\boldsymbol{\\theta})$ and $\\nabla_{\\boldsymbol{\\theta}}\\|\\boldsymbol{\\theta}\\|_{2}^{2}=2\\boldsymbol{\\theta}$, we obtain\n$$\n-2\\mathbf{\\Phi}^{T}(\\mathbf{y}-\\mathbf{\\Phi}\\boldsymbol{\\theta})+2\\gamma\\boldsymbol{\\theta}=\\mathbf{0} \\;\\;\\Rightarrow\\;\\; (\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}+\\gamma \\mathbf{I})\\boldsymbol{\\theta}=\\mathbf{\\Phi}^{T}\\mathbf{y}.\n$$\nThus the ridge estimator is\n$$\n\\hat{\\boldsymbol{\\theta}}_{ridge}=(\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}+\\gamma \\mathbf{I})^{-1}\\mathbf{\\Phi}^{T}\\mathbf{y}.\n$$\nWith the linear model $\\mathbf{y}=\\mathbf{\\Phi}\\boldsymbol{\\theta}+\\mathbf{e}$ and defining $\\mathbf{R}=\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}$, we substitute to get\n$$\n\\hat{\\boldsymbol{\\theta}}_{ridge}=(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\mathbf{\\Phi}^{T}(\\mathbf{\\Phi}\\boldsymbol{\\theta}+\\mathbf{e})=(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}(\\mathbf{R}\\boldsymbol{\\theta}+\\mathbf{\\Phi}^{T}\\mathbf{e}).\n$$\nTaking expectation over the noise and using $\\mathbb{E}[\\mathbf{e}]=\\mathbf{0}$ with $\\mathbf{\\Phi}$ deterministic (or uncorrelated with $\\mathbf{e}$) gives\n$$\n\\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_{ridge}]=(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}(\\mathbf{R}\\boldsymbol{\\theta}+\\mathbb{E}[\\mathbf{\\Phi}^{T}\\mathbf{e}])=(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\mathbf{R}\\boldsymbol{\\theta}.\n$$\nThe bias is\n$$\n\\mathbf{B}=\\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_{ridge}]-\\boldsymbol{\\theta}=\\big((\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\mathbf{R}-\\mathbf{I}\\big)\\boldsymbol{\\theta}.\n$$\nWe simplify the matrix factor:\n$$\n(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\mathbf{R}-\\mathbf{I}=(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\big(\\mathbf{R}-(\\mathbf{R}+\\gamma \\mathbf{I})\\big)=-\\gamma(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}.\n$$\nTherefore\n$$\n\\mathbf{B}=-\\gamma(\\mathbf{R}+\\gamma \\mathbf{I})^{-1}\\boldsymbol{\\theta}.\n$$\nLet $\\mathbf{R}=\\mathbf{V}\\mathbf{\\Lambda} \\mathbf{V}^{T}$ be the eigendecomposition with $\\mathbf{V}=[\\mathbf{v}_{1},\\dots,\\mathbf{v}_{p}]$ orthogonal and $\\mathbf{\\Lambda}=\\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{p})$. Then\n$$\n\\mathbf{R}+\\gamma \\mathbf{I}=\\mathbf{V}(\\mathbf{\\Lambda}+\\gamma \\mathbf{I})\\mathbf{V}^{T},\\qquad (\\mathbf{R}+\\gamma \\mathbf{I})^{-1}=\\mathbf{V}(\\mathbf{\\Lambda}+\\gamma \\mathbf{I})^{-1}\\mathbf{V}^{T}.\n$$\nHence\n$$\n\\mathbf{B}=-\\gamma \\mathbf{V}(\\mathbf{\\Lambda}+\\gamma \\mathbf{I})^{-1}\\mathbf{V}^{T}\\boldsymbol{\\theta}.\n$$\nThe component of $\\mathbf{B}$ along $\\mathbf{v}_{i}$ is $c_{i}=\\mathbf{v}_{i}^{T}\\mathbf{B}$, so\n$$\nc_{i}=-\\gamma\\,\\mathbf{v}_{i}^{T}\\mathbf{V}(\\mathbf{\\Lambda}+\\gamma \\mathbf{I})^{-1}\\mathbf{V}^{T}\\boldsymbol{\\theta}.\n$$\nUsing $\\mathbf{v}_{i}^{T}\\mathbf{V}=\\mathbf{e}_{i}^{T}$, where $\\mathbf{e}_{i}$ is the $i$-th canonical basis vector, and defining $\\alpha_{i}=\\mathbf{v}_{i}^{T}\\boldsymbol{\\theta}$, we obtain\n$$\nc_{i}=-\\gamma\\,\\mathbf{e}_{i}^{T}(\\mathbf{\\Lambda}+\\gamma \\mathbf{I})^{-1}\\mathbf{V}^{T}\\boldsymbol{\\theta}=-\\gamma\\,\\frac{1}{\\lambda_{i}+\\gamma}\\,(\\mathbf{v}_{i}^{T}\\boldsymbol{\\theta})=-\\frac{\\gamma}{\\lambda_{i}+\\gamma}\\,\\alpha_{i}.\n$$\nThis shows that the bias component along $\\mathbf{v}_{i}$ is a shrinkage of the true component $\\alpha_{i}$ by the factor $-\\gamma/(\\lambda_{i}+\\gamma)$.", "answer": "$$\\boxed{-\\frac{\\gamma}{\\lambda_{i}+\\gamma}\\,\\alpha_{i}}$$", "id": "1588663"}]}