{"hands_on_practices": [{"introduction": "At its heart, Model Predictive Control relies on a \"crystal ball\" â€“ a mathematical model of the system it's controlling. Before it can make any decision, the controller first uses this model to predict how the system will behave in the near future under a potential sequence of actions. This foundational exercise [@problem_id:1583625] puts you in the driver's seat, allowing you to manually perform this open-loop prediction for a simple thermal system. Mastering this step is key to understanding how MPC foresees the consequences of its actions.", "problem": "An engineer is developing a simple open-loop control strategy for a large, well-insulated chamber whose thermal dynamics can be modeled by a discrete-time linear system. The state of the system is the temperature deviation, $T_k$, from a desired setpoint at time step $k$. The system's evolution is described by the equation:\n$$T_{k+1} = T_k + \\beta u_k$$\nwhere $u_k$ is the control input from the heating element at step $k$, and $\\beta$ is a constant representing the thermal gain of the system.\n\nThe initial temperature deviation is measured to be $T_0 = 10.0$ degrees Celsius. The thermal gain is known to be $\\beta = 0.25$ degrees Celsius per heater unit. To test the system's response, the engineer decides to apply a constant control sequence where $u_k = 8.0$ heater units for all time steps.\n\nAs a preliminary step in designing a more advanced controller, such as a Model Predictive Controller, the engineer needs to predict the future states of the system under this simple control sequence. Compute the predicted state trajectory, which consists of the temperature deviations $\\{T_1, T_2, T_3\\}$, for a prediction horizon of three time steps.\n\nProvide the three numerical values of the trajectory in degrees Celsius.", "solution": "We are given the discrete-time linear system update\n$$T_{k+1} = T_{k} + \\beta u_{k}.$$\nBy repeated substitution, the state at step $k$ can be written as the sum of the initial condition and the accumulated inputs:\n$$T_{k} = T_{0} + \\beta \\sum_{i=0}^{k-1} u_{i}.$$\nFor a constant input $u_{i} = u$ for all $i$, this simplifies to\n$$T_{k} = T_{0} + k \\beta u.$$\n\nSubstituting the given values $T_{0} = 10.0$, $\\beta = 0.25$, and $u = 8.0$, first compute the per-step increment:\n$$\\beta u = 0.25 \\times 8.0 = 2.0.$$\nTherefore,\n$$T_{1} = 10.0 + 1 \\times 2.0 = 12.0,$$\n$$T_{2} = 10.0 + 2 \\times 2.0 = 14.0,$$\n$$T_{3} = 10.0 + 3 \\times 2.0 = 16.0.$$\nHence, the predicted trajectory $\\{T_{1}, T_{2}, T_{3}\\}$ is $\\{12.0, 14.0, 16.0\\}$.", "answer": "$$\\boxed{\\begin{pmatrix}12.0 & 14.0 & 16.0\\end{pmatrix}}$$", "id": "1583625"}, {"introduction": "Simply predicting the future is not enough; a controller must choose the best path forward. This is where the optimization engine of MPC comes into play, turning prediction into intelligent action. In this practice [@problem_id:1583584], you will calculate the optimal control move by minimizing a cost function. This exercise demonstrates the fundamental trade-off between driving a system to its target and conserving control effort, which lies at the core of nearly all advanced control strategies.", "problem": "A simplified model for the state of charge (SoC) deviation of a satellite's battery is described by the discrete-time linear system:\n$$x_{k+1} = a x_k + b u_k$$\nwhere $x_k$ is the deviation from the nominal SoC in percentage points at time step $k$, and $u_k$ is the control action (proportional to charging/discharging current) in control units applied during the interval $[k, k+1)$. The system is characterized by the parameters $a = 0.8$ and $b = 1.0$.\n\nTo regulate the SoC, a Model Predictive Control (MPC) strategy is employed. At the current time step $k=0$, the controller solves an optimization problem over a prediction horizon of $N=2$ time steps. The goal is to find a sequence of control actions, $\\{u_0, u_1\\}$, that minimizes a cost function $J$. This function penalizes the control effort and the terminal state deviation from the desired value of zero. The cost function is defined as:\n$$J = p_w x_2^2 + \\sum_{j=0}^{1} r_w u_j^2$$\nHere, $x_2$ is the predicted state deviation at the end of the horizon. The weighting factors are given as $r_w = 1.0$ for the control effort and $p_w = 5.0$ for the terminal state penalty.\n\nGiven an initial SoC deviation of $x_0 = 10.0$, determine the optimal first control action, $u_0$, that the MPC controller will apply at time step $k=0$.\n\nExpress your answer in control units, rounded to three significant figures.", "solution": "The system is $x_{k+1} = a x_{k} + b u_{k}$ with cost over horizon $N=2$ given by $J = p_{w} x_{2}^{2} + \\sum_{j=0}^{1} r_{w} u_{j}^{2}$. At $k=0$ with initial state $x_{0}$, the predicted states are\n$$\nx_{1} = a x_{0} + b u_{0}, \\quad x_{2} = a x_{1} + b u_{1} = a^{2} x_{0} + a b u_{0} + b u_{1}.\n$$\nThe cost becomes\n$$\nJ = p_{w}\\left(a^{2} x_{0} + a b u_{0} + b u_{1}\\right)^{2} + r_{w}\\left(u_{0}^{2} + u_{1}^{2}\\right).\n$$\nTo minimize $J$ with respect to $u_{0}$ and $u_{1}$, set the partial derivatives to zero. First,\n$$\n\\frac{\\partial J}{\\partial u_{1}} = 2 p_{w} x_{2} \\frac{\\partial x_{2}}{\\partial u_{1}} + 2 r_{w} u_{1} = 2 p_{w} x_{2} b + 2 r_{w} u_{1} = 0,\n$$\nwhich gives\n$$\nu_{1} = - \\frac{p_{w} b}{r_{w}} x_{2}.\n$$\nSince $x_{2} = a^{2} x_{0} + a b u_{0} + b u_{1}$, substitute this $u_{1}$ to solve for $u_{1}$ in terms of $x_{0}$ and $u_{0}$:\n$$\nu_{1} = - \\frac{p_{w} b}{r_{w}} \\left(a^{2} x_{0} + a b u_{0} + b u_{1}\\right)\n\\;\\Rightarrow\\;\nu_{1} \\left(1 + \\frac{p_{w} b^{2}}{r_{w}}\\right) = - \\frac{p_{w} b}{r_{w}} \\left(a^{2} x_{0} + a b u_{0}\\right),\n$$\nhence\n$$\nu_{1} = - \\frac{p_{w} b}{r_{w} + p_{w} b^{2}} \\left(a^{2} x_{0} + a b u_{0}\\right).\n$$\nThis yields $x_{2}$ in terms of $x_{0}$ and $u_{0}$:\n$$\nx_{2} = a^{2} x_{0} + a b u_{0} + b u_{1}\n= \\left(1 - \\frac{p_{w} b^{2}}{r_{w} + p_{w} b^{2}}\\right)\\left(a^{2} x_{0} + a b u_{0}\\right)\n= \\frac{r_{w}}{r_{w} + p_{w} b^{2}} \\left(a^{2} x_{0} + a b u_{0}\\right).\n$$\nNext,\n$$\n\\frac{\\partial J}{\\partial u_{0}} = 2 p_{w} x_{2} \\frac{\\partial x_{2}}{\\partial u_{0}} + 2 r_{w} u_{0}\n= 2 p_{w} x_{2} (a b) + 2 r_{w} u_{0} = 0,\n$$\nso\n$$\nu_{0} = - \\frac{p_{w} a b}{r_{w}} x_{2}.\n$$\nSubstitute the expression for $x_{2}$:\n$$\nu_{0} = - \\frac{p_{w} a b}{r_{w}} \\cdot \\frac{r_{w}}{r_{w} + p_{w} b^{2}} \\left(a^{2} x_{0} + a b u_{0}\\right)\n= - \\frac{p_{w} a b}{r_{w} + p_{w} b^{2}} \\left(a^{2} x_{0} + a b u_{0}\\right).\n$$\nSolve for $u_{0}$:\n$$\nu_{0} \\left(1 + \\frac{p_{w} a^{2} b^{2}}{r_{w} + p_{w} b^{2}}\\right)\n= - \\frac{p_{w} a^{3} b}{r_{w} + p_{w} b^{2}} x_{0}\n\\;\\Rightarrow\\;\nu_{0} = - \\frac{p_{w} a^{3} b}{r_{w} + p_{w} b^{2} + p_{w} a^{2} b^{2}} x_{0}.\n$$\nThus, the optimal first control action is\n$$\nu_{0}^{\\star} = - \\frac{p_{w} a^{3} b}{r_{w} + p_{w} b^{2} (1 + a^{2})} x_{0}.\n$$\nSubstitute $a = 0.8$, $b = 1.0$, $p_{w} = 5.0$, $r_{w} = 1.0$, and $x_{0} = 10.0$:\n$$\na^{3} = 0.512, \\quad 1 + a^{2} = 1.64, \\quad r_{w} + p_{w} b^{2} (1 + a^{2}) = 1.0 + 5.0 \\times 1.64 = 9.2,\n$$\nso\n$$\nu_{0}^{\\star} = - \\frac{5.0 \\times 0.512 \\times 1.0}{9.2} \\times 10.0 = - 2.782608695\\ldots\n$$\nRounded to three significant figures,\n$$\nu_{0}^{\\star} \\approx - 2.78.\n$$", "answer": "$$\\boxed{-2.78}$$", "id": "1583584"}, {"introduction": "What does \"optimal\" control truly mean? Is it a race car's aggressive precision or a luxury car's smooth comfort? In MPC, we answer this question by \"tuning\" the cost function with weighting factors. This conceptual problem [@problem_id:1583614] moves beyond pure calculation to explore the art of tuning. By analyzing two different control scenarios, you will develop an intuition for how adjusting these weights shapes the controller's personality, allowing engineers to balance performance against factors like energy use and user comfort.", "problem": "An automotive engineering team is developing a lane-keeping assist system for an autonomous vehicle. The system uses a Model Predictive Control (MPC) strategy to regulate the vehicle's lateral position relative to the center of the lane. The core of the MPC is a cost function, $J$, which is minimized at each time step to determine the optimal steering angle. For this system, the cost function is defined as:\n$$J = \\sum_{k=0}^{N-1} (x_k^T Q x_k + u_k^T R u_k)$$\nHere, $k$ is the discrete time index over a prediction horizon of length $N$. The term $x_k$ represents the state error, primarily the vehicle's lateral deviation from the lane center. The term $u_k$ represents the control input, which is the commanded change in steering angle. For simplicity in this initial tuning phase, $Q$ and $R$ are positive scalar weighting factors. A large $Q$ heavily penalizes deviation from the lane center, while a large $R$ heavily penalizes large or rapid changes in steering.\n\nThe team conducts two test runs, Alpha and Beta, on a straight road segment with different tunings of the weights $Q$ and $R$.\n\n**Scenario Alpha:** The controller makes frequent and aggressive steering corrections. Observers note a somewhat jerky ride, but data shows that the vehicle tracks the centerline with very high precision, rarely deviating more than a few centimeters.\n\n**Scenario Beta:** The controller makes smooth and gentle steering adjustments. The ride is reported as very comfortable. Data shows the vehicle is allowed to drift slightly further from the centerline during corrections, with deviations sometimes reaching tens of centimeters before being gently guided back.\n\nBased on these observations, which scenario was likely tuned with a significantly larger input weight $R$ relative to the state error weight $Q$, and why?\n\nA. Scenario Alpha, because a large relative value of $R$ is required to achieve high-precision tracking.\n\nB. Scenario Beta, because a large relative value of $R$ forces the controller to be more aggressive in its corrections.\n\nC. Scenario Alpha, because a large relative value of $R$ penalizes state error, leading to a focus on tracking performance.\n\nD. Scenario Beta, because a large relative value of $R$ penalizes the control input, leading to smoother, more conservative actions.\n\nE. The outcome is independent of the ratio of $R$ to $Q$ and depends primarily on the prediction horizon $N$.", "solution": "We are given an MPC stage cost\n$$J=\\sum_{k=0}^{N-1}\\left(x_{k}^{T}Qx_{k}+u_{k}^{T}Ru_{k}\\right),$$\nwith $Q>0$ and $R>0$ scalars in this tuning phase. The optimizer trades off reducing the state error $x_{k}$ by applying control $u_{k}$ versus penalizing the magnitude of $u_{k}$. A larger $Q$ places more emphasis on reducing $|x_{k}|$, while a larger $R$ places more emphasis on keeping $|u_{k}|$ small.\n\nTo make the dependence on $Q$ and $R$ explicit, consider the standard discrete-time linear-quadratic regulator structure for a linear prediction model $x_{k+1}=Ax_{k}+Bu_{k}$. The optimal MPC law (for the unconstrained case) is of the form\n$$u_{k}=-Kx_{k},$$\nwhere\n$$K=\\left(R+B^{T}PB\\right)^{-1}B^{T}PA,$$\nand $P$ is the unique positive semidefinite solution of the discrete Riccati equation\n$$P=Q+A^{T}PA-A^{T}PB\\left(R+B^{T}PB\\right)^{-1}B^{T}PA.$$\nFrom these expressions, the qualitative dependencies are:\n- If $R$ increases (with $Q$ fixed), then $\\left(R+B^{T}PB\\right)^{-1}$ decreases in magnitude, which decreases $K$. Smaller $K$ implies $|u_{k}|$ is smaller for a given $|x_{k}|$, leading to smoother, more conservative control actions and permitting larger transient $|x_{k}|$ before strong corrections occur.\n- If $Q$ increases (with $R$ fixed), then $P$ increases, which increases $K$. Larger $K$ implies more aggressive control $u_{k}$ for a given $x_{k}$, reducing $|x_{k}|$ more quickly but at the cost of larger and more frequent control effort.\n\nMapping these effects to the scenarios:\n- Scenario Alpha shows frequent, aggressive steering and very small deviations, which is consistent with relatively small $R$ compared to $Q$ (large $K$ and aggressive regulation of $x_{k}$).\n- Scenario Beta shows smooth, gentle steering and allows larger deviations before correction, which is consistent with relatively large $R$ compared to $Q$ (small $K$, conservative control effort).\n\nTherefore, the scenario tuned with a significantly larger input weight $R$ relative to $Q$ is Scenario Beta, because a large $R$ penalizes the control input and yields smoother, more conservative actions, even if that allows larger temporary state deviations.\n\nOptions A, B, and C contradict the role of $R$ and $Q$ in the cost: $R$ penalizes input, not state error, and larger $R$ does not force aggressive corrections. Option E is incorrect because, although the horizon $N$ influences performance, the ratio of $R$ to $Q$ directly shapes the aggressiveness and smoothness of the controller as shown by the dependence of $K$ on $Q$ and $R$.", "answer": "$$\\boxed{D}$$", "id": "1583614"}]}