{"hands_on_practices": [{"introduction": "Receding Horizon Control fundamentally works by repeatedly solving an optimization problem to find the best control action. This first exercise [@problem_id:1603975] strips the problem down to its most essential core: optimizing over a single time step, where the prediction horizon is $N=1$. By performing this simple calculation, you will gain direct experience with the trade-off between minimizing the future state error and the control effort required to achieve it, which is the foundational principle of RHC.", "problem": "An autonomous agent's position along a single linear track is described by the discrete-time state equation $x_{k+1} = x_k + u_k$, where $x_k$ is its position at time step $k$ and $u_k$ is the control input applied at that step. All quantities are dimensionless. The agent starts at an initial position $x_0 = 5$.\n\nThe goal is to choose a control input $u_0$ at the initial time step $k=0$ that minimizes a performance cost function over a one-step horizon. The cost function is defined as $J = x_1^2 + u_0^2$, where $x_1$ is the position at the next time step, and $u_0$ is the control input.\n\nCalculate the numerical value of the control input $u_0$ that minimizes this cost function $J$.", "solution": "The discrete-time dynamics are given by $x_{k+1} = x_{k} + u_{k}$. At $k=0$, this gives $x_{1} = x_{0} + u_{0}$. With $x_{0} = 5$, we have\n$$\nx_{1} = 5 + u_{0}.\n$$\nThe cost function over the one-step horizon is\n$$\nJ(u_{0}) = x_{1}^{2} + u_{0}^{2} = (5 + u_{0})^{2} + u_{0}^{2}.\n$$\nTo minimize $J$ with respect to $u_{0}$, differentiate and set the derivative to zero. Using the chain rule and linearity of differentiation,\n$$\n\\frac{dJ}{du_{0}} = 2(5 + u_{0}) \\cdot 1 + 2u_{0} = 10 + 2u_{0} + 2u_{0} = 10 + 4u_{0}.\n$$\nSet the first derivative to zero for optimality:\n$$\n10 + 4u_{0} = 0 \\quad \\Rightarrow \\quad 4u_{0} = -10 \\quad \\Rightarrow \\quad u_{0} = -\\frac{10}{4} = -\\frac{5}{2}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}J}{du_{0}^{2}} = 2 + 2 = 4 > 0,\n$$\nwhich confirms that this critical point is a minimizer. Therefore, the control input that minimizes $J$ is $u_{0} = -\\frac{5}{2}$.", "answer": "$$\\boxed{-\\frac{5}{2}}$$", "id": "1603975"}, {"introduction": "The performance of an RHC controller is critically dependent on how the cost function is defined. The weighting factors, such as $q$ on the state and $r$ on the control input in the quadratic cost, allow an engineer to tune the controller's behavior. This practice [@problem_id:1603988] explores a key conceptual question: what happens when we heavily penalize state deviations compared to control effort? Understanding this relationship is fundamental to designing controllers that are either aggressive and fast, or conservative and energy-efficient.", "problem": "A control engineer is tasked with designing a temperature regulation system for a new microprocessor. A simplified, discrete-time linear model for the temperature deviation from the desired setpoint is given by:\n$$x_{k+1} = a x_k + b u_k$$\nwhere $x_k$ is the temperature deviation from the setpoint in degrees Celsius at time step $k$, and $u_k$ is the dimensionless control input related to the cooling system's power level. The system parameters are known to be $a=1.05$ and $b=-0.8$. The initial temperature deviation is $x_0 > 0$.\n\nThe engineer implements a Receding Horizon Control (RHC) strategy, also known as Model Predictive Control (MPC). At each time step, an optimal sequence of control inputs is found by minimizing a quadratic cost function over a finite prediction horizon of $N$ steps. The cost function is:\n$$J = p x_N^2 + \\sum_{k=0}^{N-1} (q x_k^2 + r u_k^2)$$\nHere, $q, r,$ and $p$ are positive scalar weighting factors. After computing the optimal control sequence $\\{u_0^*, u_1^*, \\dots, u_{N-1}^*\\}$, only the first input, $u_0^*$, is applied to the system.\n\nWhich of the following statements best describes the behavior of the first optimal control input, $u_0^*$, in the limit where the state weighting factor $q$ becomes extremely large compared to the control weighting factor $r$?\n\nA. $u_0^*$ approaches zero, as the controller prioritizes minimizing control effort.\n\nB. $u_0^*$ approaches a specific positive value, independent of the initial state $x_0$.\n\nC. $u_0^*$ approaches a value that attempts to drive the next state, $x_1$, to exactly zero.\n\nD. $u_0^*$ becomes infinitely large in magnitude to correct the deviation instantly.\n\nE. The magnitude of $u_0^*$ decreases as the prediction horizon $N$ is increased.", "solution": "We are given the scalar linear system $x_{k+1} = a x_k + b u_k$ and the finite-horizon quadratic cost\n$$\nJ = p x_{N}^{2} + \\sum_{k=0}^{N-1} \\left( q x_{k}^{2} + r u_{k}^{2} \\right),\n$$\nwith $q>0$, $r>0$, $p>0$. This is the standard finite-horizon LQR/MPC problem without constraints. The optimal policy is linear state feedback obtained via dynamic programming.\n\nDefine the cost-to-go $V_{k}(x_k) = \\min_{\\{u_{k},\\dots,u_{N-1}\\}} \\left( \\sum_{i=k}^{N-1} (q x_{i}^{2} + r u_{i}^{2}) + p x_{N}^{2} \\right)$. For the scalar LQR, $V_{k}(x_k)$ is quadratic, so write $V_{k}(x_k) = P_{k} x_{k}^{2}$ with $P_{N} = p$. The Bellman recursion gives\n$$\nV_{k}(x_k) = \\min_{u_k} \\left( q x_{k}^{2} + r u_{k}^{2} + V_{k+1}(x_{k+1}) \\right) = \\min_{u_k} \\left( q x_{k}^{2} + r u_{k}^{2} + P_{k+1} (a x_{k} + b u_{k})^{2} \\right).\n$$\nMinimizing the quadratic in $u_k$ by taking the derivative and setting it to zero,\n$$\n\\frac{\\partial}{\\partial u_k} \\left( r u_{k}^{2} + P_{k+1} (a x_{k} + b u_{k})^{2} \\right) = 2 r u_{k} + 2 P_{k+1} b (a x_{k} + b u_{k}) = 0,\n$$\nwhich yields\n$$\nu_{k}^{*} = -\\frac{a b P_{k+1}}{r + b^{2} P_{k+1}} \\, x_{k}.\n$$\nSubstituting $u_{k}^{*}$ back gives the Riccati recursion\n$$\nP_{k} = q + a^{2} P_{k+1} - \\frac{a^{2} b^{2} P_{k+1}^{2}}{r + b^{2} P_{k+1}}, \\quad P_{N} = p.\n$$\n\nWe analyze the limit where $q$ becomes extremely large compared to $r$. For any fixed finite horizon $N$ and fixed $a$, $b$, $p$, as $q \\to \\infty$, the backward recursion shows $P_{k}$ grows without bound for all $k \\leq N-1$; in particular, $P_{1} \\to \\infty$. Therefore, the first-step feedback gain satisfies\n$$\nK_{0} := \\frac{a b P_{1}}{r + b^{2} P_{1}} \\quad \\Longrightarrow \\quad \\lim_{q \\to \\infty} K_{0} = \\lim_{P_{1} \\to \\infty} \\frac{a b P_{1}}{r + b^{2} P_{1}} = \\frac{a}{b}.\n$$\nHence the first optimal input tends to\n$$\n\\lim_{q \\to \\infty} u_{0}^{*} = - \\lim_{q \\to \\infty} K_{0} x_{0} = -\\frac{a}{b} x_{0}.\n$$\nThe resulting next state is\n$$\nx_{1} = a x_{0} + b u_{0}^{*} \\to a x_{0} + b \\left( -\\frac{a}{b} x_{0} \\right) = 0.\n$$\nThus, in the limit of very large state penalty relative to control effort, the controller chooses $u_{0}^{*}$ to drive $x_{1}$ exactly to zero, provided $b \\neq 0$ (which holds here). This choice is finite and depends linearly on $x_{0}$, so it is neither zero, nor independent of $x_{0}$, nor infinite, and it does not monotonically decrease with prediction horizon $N$ in this limit; indeed, the limiting feedback $-\\frac{a}{b}$ is independent of $N$.\n\nTherefore, the correct description is that $u_{0}^{*}$ approaches a value that attempts to drive $x_{1}$ to exactly zero.", "answer": "$$\\boxed{C}$$", "id": "1603988"}, {"introduction": "While optimizing a cost function is central to RHC, real-world applications demand that controllers operate within strict physical and safety limits. This problem [@problem_id:1583595] moves from pure optimization to constrained optimization, a hallmark of Model Predictive Control. Using a practical bioreactor example, you will learn to distinguish between inviolable 'hard' constraints and negotiable 'soft' constraints, a critical skill for designing robust and safe control systems.", "problem": "A biopharmaceutical company is designing a control system for a sensitive fermentation process in a bioreactor. The controller uses a Model Predictive Control (MPC) strategy to regulate the process. Two critical process variables must be managed: the internal temperature, $T$, and the acidity, measured by pH.\n\nThe product of the fermentation is a delicate protein that is irreversibly denatured and destroyed if the temperature exceeds a critical value, $T_{max} = 38.0^\\circ\\text{C}$. Any violation of this temperature limit, no matter how small or brief, results in the complete loss of the entire batch.\n\nThe optimal pH for the reaction is $pH_{opt} = 7.2$. Deviations from this value are undesirable as they reduce the reaction rate and final yield. However, these effects are not catastrophic; the process can recover, and small, temporary deviations are tolerable if they are necessary to satisfy other operational goals, such as maintaining the temperature constraint.\n\nYour task is to determine how the constraints on temperature ($T \\le T_{max}$) and the desired pH level (keeping pH near $pH_{opt}$) should be formulated within the optimization problem at the core of the MPC controller.\n\nWhich of the following describes the most appropriate formulation?\n\nA. The temperature limit should be a hard constraint, and the pH target should be a soft constraint.\n\nB. The temperature limit should be a soft constraint, and the pH target should be a hard constraint.\n\nC. Both the temperature limit and the pH target should be implemented as hard constraints.\n\nD. Both the temperature limit and the pH target should be implemented as soft constraints.\n\nE. The temperature limit should be implemented as a soft constraint, and the pH target should be an unconstrained objective.", "solution": "In model predictive control, constraints that cannot be violated under any circumstance must be modeled as hard constraints, meaning the feasible set excludes any violation. Constraints whose violation is tolerable but undesirable are modeled as soft constraints, typically by introducing slack variables and penalizing their values in the objective function, or equivalently by penalizing deviations directly in the stage cost.\n\nGiven the process description:\n- Any violation of the temperature limit results in the complete loss of the batch. This corresponds to an effectively infinite penalty for any violation, so the temperature constraint must be enforced as a hard inequality constraint for all steps over the prediction horizon:\n$$\nT(k+i) \\le T_{max} \\quad \\text{for all } i \\in \\{0,\\dots,N-1\\}.\n$$\n- Deviations of pH from $pH_{opt}$ are tolerable and part of performance trade-offs, so they should be treated as soft. This can be formulated either by directly penalizing the deviation in the cost or by introducing a soft constraint with a slack variable. A standard MPC cost may include a quadratic penalty on the deviation:\n$$\nJ = \\sum_{i=0}^{N-1} \\left( w_{pH} \\left|\\mathrm{pH}(k+i) - pH_{opt}\\right|^{2} + w_{u} \\|\\Delta u(k+i)\\|^{2} \\right),\n$$\nsubject to the system dynamics and the hard temperature constraint above. Alternatively, one could write a soft constraint with slack $\\sigma(k+i) \\ge 0$:\n$$\n\\left|\\mathrm{pH}(k+i) - pH_{opt}\\right| \\le \\sigma(k+i),\n$$\nand add to the cost a penalty such as\n$$\nJ_{\\text{soft}} = \\sum_{i=0}^{N-1} \\lambda \\,\\sigma(k+i)^{2},\n$$\nagain with $T(k+i) \\le T_{max}$ enforced as a hard constraint.\n\nEvaluating the options:\n- A matches the above: temperature as a hard constraint, pH as a soft constraint.\n- B makes temperature soft and pH hard, which contradicts the catastrophic nature of temperature violations.\n- C enforces both hard, which needlessly restricts feasibility by disallowing beneficial trade-offs in pH.\n- D makes both soft, which risks catastrophic temperature violations.\n- E makes temperature soft, which is unacceptable, and removes explicit constraint guidance on pH even as a soft constraint.\n\nTherefore, the appropriate formulation is A.", "answer": "$$\\boxed{A}$$", "id": "1583595"}]}