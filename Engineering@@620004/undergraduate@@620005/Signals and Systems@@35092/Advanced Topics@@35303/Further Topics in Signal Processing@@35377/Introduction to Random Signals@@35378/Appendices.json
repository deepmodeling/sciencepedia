{"hands_on_practices": [{"introduction": "Understanding random signals often begins with characterizing their basic statistical properties, such as the mean or expected value. This first exercise provides a practical entry point by examining quantization error, an inevitable byproduct of converting continuous analog signals into a digital format. By modeling this error as a uniformly distributed random variable, you will practice calculating the expected value, a fundamental skill for analyzing the average behavior of any random signal or noise source. [@problem_id:1730075]", "problem": "A digital measurement system employs an analog-to-digital converter (ADC) to process a continuous input signal. The ADC operates by rounding the incoming analog voltage to the nearest discrete quantization level. The voltage difference between any two adjacent quantization levels is a constant, denoted by $\\Delta$.\n\nThe quantization error, $e$, is defined as the difference between the quantized level and the true analog input voltage. It is a standard practice in signal processing to model this error as a continuous random variable that is uniformly distributed over the interval $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$.\n\nBased on this model, determine the mean value (or expected value) of the quantization error, $E[e]$. Express your answer as a symbolic expression which may involve $\\Delta$.", "solution": "The quantization error $e$ is modeled as a continuous random variable uniformly distributed over the interval $\\left[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right]$. The probability density function (pdf) for a continuous uniform distribution over $[a,b]$ is $f(x)=\\frac{1}{b-a}$ for $x \\in [a,b]$ and $0$ otherwise. Therefore, for $e$ we have\n$$\nf_{e}(x)=\\frac{1}{\\Delta} \\quad \\text{for } x \\in \\left[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right], \\quad \\text{and } 0 \\text{ otherwise}.\n$$\nThe expected value of a continuous random variable with pdf $f_{e}(x)$ is defined by\n$$\nE[e]=\\int_{-\\infty}^{\\infty} x f_{e}(x)\\,dx.\n$$\nSubstituting the uniform pdf and restricting the integral to the support,\n$$\nE[e]=\\int_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}} x \\left(\\frac{1}{\\Delta}\\right)\\,dx=\\frac{1}{\\Delta}\\int_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}} x\\,dx.\n$$\nEvaluating the integral,\n$$\n\\int x\\,dx=\\frac{x^{2}}{2} \\quad \\Rightarrow \\quad \\frac{1}{\\Delta}\\left[\\frac{x^{2}}{2}\\right]_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}}=\\frac{1}{\\Delta}\\left(\\frac{\\left(\\frac{\\Delta}{2}\\right)^{2}}{2}-\\frac{\\left(-\\frac{\\Delta}{2}\\right)^{2}}{2}\\right)=\\frac{1}{\\Delta}\\left(\\frac{\\Delta^{2}}{8}-\\frac{\\Delta^{2}}{8}\\right)=0.\n$$\nAlternatively, by symmetry of the uniform distribution about zero, the mean is zero. Hence,\n$$\nE[e]=0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1730075"}, {"introduction": "While the mean provides a measure of central tendency, the complete behavior of a random signal is captured by its probability density function (PDF). This practice problem explores how the PDF of a signal changes when it undergoes a non-linear transformation, a common scenario in signal analysis. You will derive the PDF for the power dissipated in a resistor given the PDF of the noise voltage, illustrating a powerful method for characterizing transformed random variables. [@problem_id:1730052]", "problem": "In the analysis of high-precision electronic amplifiers, it is crucial to characterize the effects of thermal noise. A simplified model for the noise voltage, $V$, at the input of an amplifier treats it as a random variable. Assume this noise voltage follows a Laplace distribution, which is often a good approximation for certain types of signal interference. The Probability Density Function (PDF) for this voltage is given by:\n$$f_V(v) = \\frac{1}{2\\beta} \\exp\\left(-\\frac{|v|}{\\beta}\\right)$$\nwhere $v$ can be any real number, and $\\beta$ is a positive constant that characterizes the average magnitude of the noise.\n\nThis noise voltage is applied across a precision resistor with resistance $R=1\\,\\Omega$. The instantaneous power, $P$, dissipated by the resistor is given by the relation $P = V^2$.\n\nDetermine the PDF of the random variable $P$, denoted as $f_P(p)$, for $p > 0$. Your answer should be an expression in terms of $p$ and $\\beta$.", "solution": "The problem asks for the Probability Density Function (PDF) of the power $P$, given that $P = V^2$ and the PDF of the voltage $V$ is known. We will use the method of transformation of random variables. The most direct approach is to first find the Cumulative Distribution Function (CDF) of $P$ and then differentiate it to obtain the PDF.\n\nLet $F_P(p)$ be the CDF of the random variable $P$. By definition, for $p \\ge 0$:\n$$F_P(p) = \\text{Prob}(P \\le p)$$\n\nSince $P = V^2$, we can write this in terms of the random variable $V$:\n$$F_P(p) = \\text{Prob}(V^2 \\le p)$$\n\nThe inequality $V^2 \\le p$ is equivalent to $-\\sqrt{p} \\le V \\le \\sqrt{p}$. Therefore, the CDF of $P$ can be expressed as the probability that $V$ falls within this interval:\n$$F_P(p) = \\text{Prob}(-\\sqrt{p} \\le V \\le \\sqrt{p})$$\n\nThis probability can be calculated by integrating the PDF of $V$, $f_V(v)$, over the interval $[-\\sqrt{p}, \\sqrt{p}]$:\n$$F_P(p) = \\int_{-\\sqrt{p}}^{\\sqrt{p}} f_V(v) \\, dv$$\n\nThe given PDF for $V$ is $f_V(v) = \\frac{1}{2\\beta} \\exp\\left(-\\frac{|v|}{\\beta}\\right)$. Substituting this into the integral:\n$$F_P(p) = \\int_{-\\sqrt{p}}^{\\sqrt{p}} \\frac{1}{2\\beta} \\exp\\left(-\\frac{|v|}{\\beta}\\right) \\, dv$$\n\nThe integrand is an even function because $|-v| = |v|$. We can simplify the integral by integrating from $0$ to $\\sqrt{p}$ and multiplying by 2:\n$$F_P(p) = 2 \\int_{0}^{\\sqrt{p}} \\frac{1}{2\\beta} \\exp\\left(-\\frac{|v|}{\\beta}\\right) \\, dv$$\n\nFor $v \\ge 0$, we have $|v| = v$. So the expression becomes:\n$$F_P(p) = \\frac{1}{\\beta} \\int_{0}^{\\sqrt{p}} \\exp\\left(-\\frac{v}{\\beta}\\right) \\, dv$$\n\nNow, we evaluate the definite integral:\n$$F_P(p) = \\frac{1}{\\beta} \\left[ -\\beta \\exp\\left(-\\frac{v}{\\beta}\\right) \\right]_{0}^{\\sqrt{p}}$$\n$$F_P(p) = - \\left[ \\exp\\left(-\\frac{v}{\\beta}\\right) \\right]_{0}^{\\sqrt{p}}$$\n$$F_P(p) = - \\left( \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) - \\exp(0) \\right)$$\n$$F_P(p) = - \\left( \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) - 1 \\right)$$\n$$F_P(p) = 1 - \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right)$$\n\nThis is the CDF of $P$ for $p \\ge 0$. Note that for $p < 0$, $F_P(p) = 0$ since power cannot be negative.\n\nTo find the PDF, $f_P(p)$, we differentiate the CDF, $F_P(p)$, with respect to $p$. We are interested in the case where $p > 0$.\n$$f_P(p) = \\frac{d}{dp} F_P(p) = \\frac{d}{dp} \\left( 1 - \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) \\right)$$\n\nUsing the chain rule, where the inner function is $u = -\\frac{\\sqrt{p}}{\\beta} = -\\frac{1}{\\beta}p^{1/2}$:\n$$\\frac{du}{dp} = -\\frac{1}{\\beta} \\left(\\frac{1}{2} p^{-1/2}\\right) = -\\frac{1}{2\\beta\\sqrt{p}}$$\n\nThe derivative is:\n$$f_P(p) = - \\frac{d}{dp} \\left(\\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right)\\right)$$\n$$f_P(p) = - \\left( \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) \\cdot \\frac{d}{dp}\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) \\right)$$\n$$f_P(p) = - \\left( \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right) \\cdot \\left(-\\frac{1}{2\\beta\\sqrt{p}}\\right) \\right)$$\n$$f_P(p) = \\frac{1}{2\\beta\\sqrt{p}} \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right)$$\n\nThis expression is valid for $p > 0$. For $p \\le 0$, the PDF is $f_P(p) = 0$.", "answer": "$$\\boxed{\\frac{1}{2\\beta\\sqrt{p}} \\exp\\left(-\\frac{\\sqrt{p}}{\\beta}\\right)}$$", "id": "1730052"}, {"introduction": "Real-world applications rarely involve a single random variable, but rather sequences of them, known as random processes. This final exercise introduces the crucial concept of autocorrelation, which describes the relationship between a signal's values at different points in time. By analyzing a simple digital smoothing filter, you will investigate how a linear system transforms the autocorrelation of an input noise process, a fundamental principle in filter design and system analysis. [@problem_id:1730025]", "problem": "A digital signal processing system is designed to smooth a noisy data stream. The input to the system, a discrete-time random process denoted by $X[n]$, represents the noise component of a sequence of measurements. This noise process is stationary and consists of uncorrelated random variables, where each variable has a mean of zero and a variance of $\\sigma_X^2$. The system implements a simple 2-tap Finite Impulse Response (FIR) filter, described by the difference equation $Y[n] = c_0 X[n] + c_1 X[n-1]$, where $Y[n]$ is the smoothed output signal and the filter coefficients are $c_0 = c_1 = \\frac{1}{\\sqrt{2}}$.\n\nDetermine the autocorrelation function, $R_Y[k] = E[Y[n]Y[n-k]]$, of the output process $Y[n]$ as a function of the integer time lag $k$ and the input variance $\\sigma_X^2$. Express your answer using the Kronecker delta function, $\\delta[k]$.", "solution": "The output process is defined by the 2-tap FIR filter $Y[n]=c_{0}X[n]+c_{1}X[n-1]$ with $c_{0}=c_{1}=1/\\sqrt{2}$. The autocorrelation function is\n$$\nR_{Y}[k]=E\\!\\left[Y[n]\\,Y[n-k]\\right]=E\\!\\left[\\left(c_{0}X[n]+c_{1}X[n-1]\\right)\\left(c_{0}X[n-k]+c_{1}X[n-k-1]\\right)\\right].\n$$\nUsing linearity of expectation and $c_{0}c_{0}=c_{0}c_{1}=c_{1}c_{0}=c_{1}c_{1}=1/2$, we expand:\n$$\nR_{Y}[k]=\\frac{1}{2}E\\!\\left[X[n]X[n-k]\\right]+\\frac{1}{2}E\\!\\left[X[n]X[n-k-1]\\right]+\\frac{1}{2}E\\!\\left[X[n-1]X[n-k]\\right]+\\frac{1}{2}E\\!\\left[X[n-1]X[n-k-1]\\right].\n$$\nSince $\\{X[n]\\}$ is white, zero-mean, and stationary with variance $\\sigma_{X}^{2}$, its autocorrelation is $R_{X}[k]=E[X[n]X[n-k]]=\\sigma_{X}^{2}\\delta[k]$. Therefore,\n$$\nE[X[n]X[n-k]]=\\sigma_{X}^{2}\\delta[k],\\quad E[X[n]X[n-k-1]]=\\sigma_{X}^{2}\\delta[k+1],\n$$\n$$\nE[X[n-1]X[n-k]]=\\sigma_{X}^{2}\\delta[k-1],\\quad E[X[n-1]X[n-k-1]]=\\sigma_{X}^{2}\\delta[k].\n$$\nSubstituting these into the expansion and combining terms yields\n$$\nR_{Y}[k]=\\frac{1}{2}\\sigma_{X}^{2}\\left(\\delta[k]+\\delta[k+1]+\\delta[k-1]+\\delta[k]\\right)\n=\\sigma_{X}^{2}\\delta[k]+\\frac{\\sigma_{X}^{2}}{2}\\left(\\delta[k-1]+\\delta[k+1]\\right).\n$$\nEquivalently, this can be seen from the general result for filtering white noise: with impulse response $h[0]=h[1]=1/\\sqrt{2}$ and $h[m]=0$ otherwise, $R_{Y}[k]=\\sigma_{X}^{2}\\sum_{m}h[m]h[m-k]$, which gives the same three-point sequence.", "answer": "$$\\boxed{\\sigma_{X}^{2}\\left(\\delta[k]+\\frac{1}{2}\\delta[k-1]+\\frac{1}{2}\\delta[k+1]\\right)}$$", "id": "1730025"}]}