{"hands_on_practices": [{"introduction": "The autocorrelation function is a cornerstone for characterizing random signals, describing how a signal correlates with a time-shifted version of itself. A particularly useful piece of information, the average power of the signal, is directly embedded within this function. This exercise provides direct practice in extracting this fundamental physical quantity from the mathematical description of a random process, reinforcing the link between abstract statistical functions and tangible signal properties [@problem_id:1699365]. For a wide-sense stationary (WSS) process $X(t)$, the average power is given by $P_X = \\mathbb{E}[X(t)^2] = R_X(0)$.", "problem": "An electrical engineer is characterizing the thermal noise in a sensitive preamplifier circuit. The noise voltage, denoted by $V(t)$, is modeled as a real-valued, zero-mean, Wide-Sense Stationary (WSS) random process. Through experimental measurements, the autocorrelation function of this noise signal is found to be well-described by the following equation:\n$$R_V(\\tau) = A \\exp(-B|\\tau|)$$\nwhere $\\tau$ represents the time lag. The experimentally determined constants are $A = 15.5 \\text{ V}^2$ and $B = 5.0 \\text{ s}^{-1}$.\n\nThe average power of a continuous-time random process $X(t)$ is defined as $P_X = \\mathbb{E}[X(t)^2]$, where $\\mathbb{E}[\\cdot]$ is the expectation operator.\n\nCalculate the average power of the noise signal $V(t)$. Express your answer in units of volts squared ($V^2$).", "solution": "The average power of a continuous-time random process is defined as $P_{X} = \\mathbb{E}[X(t)^{2}]$. For a Wide-Sense Stationary (WSS) process $V(t)$, the autocorrelation function is $R_{V}(\\tau) = \\mathbb{E}[V(t)V(t+\\tau)]$, and in particular,\n$$\nR_{V}(0) = \\mathbb{E}[V(t)V(t)] = \\mathbb{E}[V(t)^{2}] = P_{V}.\n$$\nGiven the measured autocorrelation $R_{V}(\\tau) = A \\exp(-B|\\tau|)$, we evaluate at $\\tau = 0$:\n$$\nP_{V} = R_{V}(0) = A \\exp(-B|0|) = A \\exp(0) = A.\n$$\nSubstituting the given value $A = 15.5$, the average power is\n$$\nP_{V} = 15.5.\n$$", "answer": "$$\\boxed{15.5}$$", "id": "1699365"}, {"introduction": "After learning to use the properties of a Wide-Sense Stationary (WSS) process, it is crucial to understand how to verify if a process even meets these criteria. A process is WSS if its mean value is constant for all time $t$, and its autocorrelation function depends only on the time difference $\\tau$ between two points, not on their absolute times. This exercise challenges you to rigorously apply these two conditions to a seemingly simple sinusoidal signal with a random phase component [@problem_id:1746544]. By doing so, you will develop a deeper intuition for the strict requirements of stationarity and appreciate how subtle changes in a random variable's distribution can fundamentally alter a process's behavior over time.", "problem": "In a simplified model of a digital communication system, a signal is affected by a random phase offset during transmission. The received signal, a random process, is described by $X(t) = A \\cos(2 \\pi f_0 t + \\Theta)$, where $A$ and $f_0$ are positive real constants representing the amplitude and frequency, respectively. The phase offset $\\Theta$ is a random variable that is uniformly distributed over the interval $[0, \\pi/2]$.\n\nYour task is to analyze the statistical properties of this process. A random process is defined as Wide-Sense Stationary (WSS) if its mean value is constant for all time $t$, and its autocorrelation function depends only on the time difference between two points, not on their absolute times.\n\nBased on the definition of a WSS process, which of the following statements correctly describes the random process $X(t)$?\n\nA. The mean of $X(t)$ is constant, and its autocorrelation function depends only on the time difference $\\tau = t_2 - t_1$. Therefore, the process is wide-sense stationary.\n\nB. The mean of $X(t)$ is time-varying, but its autocorrelation function depends only on the time difference $\\tau = t_2 - t_1$.\n\nC. The mean of $X(t)$ is constant, but its autocorrelation function depends on the absolute times $t_1$ and $t_2$.\n\nD. The mean of $X(t)$ is time-varying, and its autocorrelation function also depends on the absolute times $t_1$ and $t_2$.", "solution": "We are given the random process $X(t) = A \\cos(2 \\pi f_{0} t + \\Theta)$, where $A$ and $f_{0}$ are constants and $\\Theta$ is a random variable uniformly distributed on $[0, \\pi/2]$. A process is wide-sense stationary (WSS) if its mean is constant in $t$ and its autocorrelation depends only on the time difference $\\tau$.\n\nFirst, compute the mean. The probability density function of $\\Theta$ is $f_{\\Theta}(\\theta) = \\frac{2}{\\pi}$ for $\\theta \\in [0, \\pi/2]$. Therefore,\n$$\nm_{X}(t) = \\mathbb{E}[X(t)] = A \\,\\mathbb{E}\\big[\\cos(2 \\pi f_{0} t + \\Theta)\\big] = A \\int_{0}^{\\pi/2} \\cos(2 \\pi f_{0} t + \\theta)\\,\\frac{2}{\\pi}\\,d\\theta.\n$$\nEvaluating the integral,\n$$\n\\int_{0}^{\\pi/2} \\cos(2 \\pi f_{0} t + \\theta)\\,d\\theta = \\left.\\sin(2 \\pi f_{0} t + \\theta)\\right|_{0}^{\\pi/2} = \\sin\\!\\left(2 \\pi f_{0} t + \\frac{\\pi}{2}\\right) - \\sin(2 \\pi f_{0} t).\n$$\nUsing $\\sin(x + \\frac{\\pi}{2}) = \\cos x$, we obtain\n$$\nm_{X}(t) = A \\,\\frac{2}{\\pi}\\,\\big[\\cos(2 \\pi f_{0} t) - \\sin(2 \\pi f_{0} t)\\big],\n$$\nwhich is explicitly time-varying. Hence the mean is not constant.\n\nNext, compute the autocorrelation $R_{X}(t_{1}, t_{2}) = \\mathbb{E}[X(t_{1}) X(t_{2})]$. Using $\\cos u \\cos v = \\frac{1}{2}\\big[\\cos(u - v) + \\cos(u + v)\\big]$, with $u = 2 \\pi f_{0} t_{1} + \\Theta$ and $v = 2 \\pi f_{0} t_{2} + \\Theta$, we get\n$$\nX(t_{1}) X(t_{2}) = \\frac{A^{2}}{2}\\Big[\\cos\\big(2 \\pi f_{0}(t_{1} - t_{2})\\big) + \\cos\\big(2 \\pi f_{0}(t_{1} + t_{2}) + 2 \\Theta\\big)\\Big].\n$$\nTaking expectation over $\\Theta$,\n$$\nR_{X}(t_{1}, t_{2}) = \\frac{A^{2}}{2}\\cos\\big(2 \\pi f_{0}(t_{1} - t_{2})\\big) + \\frac{A^{2}}{2}\\,\\mathbb{E}\\big[\\cos\\big(2 \\pi f_{0}(t_{1} + t_{2}) + 2 \\Theta\\big)\\big].\n$$\nLet $C = 2 \\pi f_{0}(t_{1} + t_{2})$. Then\n$$\n\\mathbb{E}\\big[\\cos(C + 2 \\Theta)\\big] = \\int_{0}^{\\pi/2} \\cos(C + 2 \\theta)\\,\\frac{2}{\\pi}\\,d\\theta = \\frac{2}{\\pi}\\left[\\frac{1}{2}\\sin(C + 2 \\theta)\\right]_{0}^{\\pi/2} = \\frac{2}{\\pi}\\cdot\\frac{1}{2}\\big[\\sin(C + \\pi) - \\sin C\\big].\n$$\nUsing $\\sin(C + \\pi) = -\\sin C$, we obtain\n$$\n\\mathbb{E}\\big[\\cos(C + 2 \\Theta)\\big] = \\frac{2}{\\pi}\\cdot\\frac{1}{2}\\cdot(-2 \\sin C) = -\\frac{2}{\\pi}\\sin C = -\\frac{2}{\\pi}\\sin\\big(2 \\pi f_{0}(t_{1} + t_{2})\\big).\n$$\nTherefore,\n$$\nR_{X}(t_{1}, t_{2}) = \\frac{A^{2}}{2}\\cos\\big(2 \\pi f_{0}(t_{1} - t_{2})\\big) - \\frac{A^{2}}{\\pi}\\sin\\big(2 \\pi f_{0}(t_{1} + t_{2})\\big).\n$$\nThe first term depends only on $t_{1} - t_{2}$, but the second term depends on $t_{1} + t_{2}$, i.e., on the absolute times. Hence the autocorrelation is not a function of the time difference alone.\n\nConclusion: The mean is time-varying, and the autocorrelation depends on the absolute times. Thus, the correct statement is D.", "answer": "$$\\boxed{D}$$", "id": "1746544"}, {"introduction": "One of the most powerful applications of random process theory is predicting how a random signal's characteristics are altered when it passes through a linear time-invariant (LTI) system, such as an electronic filter. This practice explores the transformation of an idealized \"white noise\" signal—a process with uniform power at all frequencies—by a simple RC low-pass filter. You will use frequency-domain tools, specifically the power spectral density (PSD), to determine the statistical properties of the resulting output voltage [@problem_id:1746529]. The exercise is a classic demonstration of the fundamental relationship $S_Y(\\omega) = |H(j\\omega)|^2 S_X(\\omega)$, which is essential for analyzing and designing systems that operate in the presence of noise.", "problem": "A simple electronic circuit consists of a resistor with resistance $R$ connected in series with a capacitor of capacitance $C$. This circuit is driven by a thermal noise voltage source, $V_n(t)$, which can be modeled as a stationary, zero-mean, white random process. The autocorrelation function of this noise voltage is given by $R_{V_n}(\\tau) = \\mathbb{E}[V_n(t)V_n(t+\\tau)] = \\frac{N_0}{2} \\delta(\\tau)$, where $N_0$ is a positive constant representing the noise power level and $\\delta(\\tau)$ is the Dirac delta function.\n\nThe voltage across the capacitor, denoted by $V_C(t)$, constitutes the output of the system. At time $t=0$, the capacitor is fully discharged, i.e., $V_C(0) = 0$.\n\nWe are interested in the long-term statistical properties of the capacitor voltage. Determine the steady-state root-mean-square (RMS) voltage across the capacitor, which is defined as $\\sqrt{\\lim_{t\\to\\infty} \\mathbb{E}[V_C(t)^2]}$. Express your final answer as a symbolic expression in terms of $R$, $C$, and $N_0$.", "solution": "The problem asks for the steady-state root-mean-square (RMS) voltage across a capacitor in an RC circuit driven by white noise. We can solve this by analyzing the system in the frequency domain.\n\nFirst, let's establish the relationship between the input noise voltage $V_n(t)$ and the output capacitor voltage $V_C(t)$. Applying Kirchhoff's voltage law to the series RC circuit, we have:\n$$V_n(t) = V_R(t) + V_C(t)$$\nwhere $V_R(t)$ is the voltage across the resistor. The current through the circuit is $i(t) = C \\frac{dV_C(t)}{dt}$. The voltage across the resistor is $V_R(t) = i(t)R = RC \\frac{dV_C(t)}{dt}$. Substituting this into the voltage law equation gives the governing differential equation for the circuit:\n$$RC \\frac{dV_C(t)}{dt} + V_C(t) = V_n(t)$$\nThis is a linear time-invariant (LTI) system. To find its transfer function $H(s)$, we take the Laplace transform of the equation, assuming zero initial conditions as specified ($V_C(0)=0$):\n$$RC s V_C(s) + V_C(s) = V_n(s)$$\n$$(RCs + 1) V_C(s) = V_n(s)$$\nThe transfer function $H(s)$ is the ratio of the output's Laplace transform to the input's Laplace transform:\n$$H(s) = \\frac{V_C(s)}{V_n(s)} = \\frac{1}{RCs + 1} = \\frac{1/RC}{s + 1/RC}$$\nThe frequency response of the system is obtained by substituting $s = j\\omega$:\n$$H(j\\omega) = \\frac{1/RC}{j\\omega + 1/RC}$$\nNext, we consider the properties of the input noise. The input is a white noise process with an autocorrelation function $R_{V_n}(\\tau) = \\frac{N_0}{2} \\delta(\\tau)$. The power spectral density (PSD) of the input, $S_{V_n}(\\omega)$, is the Fourier transform of its autocorrelation function:\n$$S_{V_n}(\\omega) = \\mathcal{F}\\{R_{V_n}(\\tau)\\} = \\int_{-\\infty}^{\\infty} \\frac{N_0}{2} \\delta(\\tau) e^{-j\\omega\\tau} d\\tau = \\frac{N_0}{2}$$\nFor an LTI system, the PSD of the output, $S_{V_C}(\\omega)$, is related to the PSD of the input by the squared magnitude of the frequency response:\n$$S_{V_C}(\\omega) = |H(j\\omega)|^2 S_{V_n}(\\omega)$$\nThe squared magnitude of the frequency response is:\n$$|H(j\\omega)|^2 = \\left| \\frac{1/RC}{j\\omega + 1/RC} \\right|^2 = \\frac{(1/RC)^2}{\\omega^2 + (1/RC)^2}$$\nTherefore, the PSD of the capacitor voltage is:\n$$S_{V_C}(\\omega) = \\frac{(1/RC)^2}{\\omega^2 + (1/RC)^2} \\cdot \\frac{N_0}{2}$$\nThe mean square value of a stationary random process is equal to its total average power. According to the Wiener-Khinchin theorem, this can be calculated by integrating its PSD over all frequencies, divided by $2\\pi$. The problem asks for the steady-state mean square value, $\\lim_{t\\to\\infty} \\mathbb{E}[V_C(t)^2]$, which for a system with a stationary input is simply the mean square value of the stationary output process.\n$$\\mathbb{E}[V_C^2]_{\\text{ss}} = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_{V_C}(\\omega) d\\omega$$\nSubstituting the expression for $S_{V_C}(\\omega)$:\n$$\\mathbb{E}[V_C^2]_{\\text{ss}} = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{(1/RC)^2}{\\omega^2 + (1/RC)^2} \\frac{N_0}{2} d\\omega$$\nLet's simplify by factoring out the constants and defining $\\alpha = 1/RC$:\n$$\\mathbb{E}[V_C^2]_{\\text{ss}} = \\frac{N_0}{4\\pi} \\int_{-\\infty}^{\\infty} \\frac{\\alpha^2}{\\omega^2 + \\alpha^2} d\\omega$$\nThe integral can be evaluated as:\n$$\\int_{-\\infty}^{\\infty} \\frac{\\alpha^2}{\\omega^2 + \\alpha^2} d\\omega = \\alpha^2 \\left[ \\frac{1}{\\alpha} \\arctan\\left(\\frac{\\omega}{\\alpha}\\right) \\right]_{-\\infty}^{\\infty} = \\alpha \\left[ \\arctan\\left(\\frac{\\omega}{\\alpha}\\right) \\right]_{-\\infty}^{\\infty}$$\n$$\\alpha \\left( \\lim_{\\omega\\to\\infty} \\arctan\\left(\\frac{\\omega}{\\alpha}\\right) - \\lim_{\\omega\\to-\\infty} \\arctan\\left(\\frac{\\omega}{\\alpha}\\right) \\right) = \\alpha \\left( \\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right) \\right) = \\alpha \\pi$$\nNow, substitute this result back into the expression for the mean square voltage:\n$$\\mathbb{E}[V_C^2]_{\\text{ss}} = \\frac{N_0}{4\\pi} (\\alpha \\pi) = \\frac{N_0 \\alpha}{4}$$\nFinally, substitute back $\\alpha = 1/RC$:\n$$\\mathbb{E}[V_C^2]_{\\text{ss}} = \\frac{N_0}{4RC}$$\nThe problem asks for the root-mean-square (RMS) voltage, which is the square root of the mean square value:\n$$V_{C, \\text{rms}} = \\sqrt{\\mathbb{E}[V_C^2]_{\\text{ss}}} = \\sqrt{\\frac{N_0}{4RC}}$$\n$$V_{C, \\text{rms}} = \\frac{1}{2}\\sqrt{\\frac{N_0}{RC}}$$\nThis is the steady-state RMS voltage across the capacitor.", "answer": "$$\\boxed{\\frac{1}{2}\\sqrt{\\frac{N_0}{RC}}}$$", "id": "1746529"}]}