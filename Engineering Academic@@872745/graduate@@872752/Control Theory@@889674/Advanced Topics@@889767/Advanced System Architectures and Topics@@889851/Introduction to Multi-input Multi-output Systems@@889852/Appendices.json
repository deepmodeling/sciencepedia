{"hands_on_practices": [{"introduction": "Understanding a complex system often begins with understanding its structure. In MIMO control, systems can be represented in state-space form or as a transfer function matrix. This practice guides you through the fundamental process of deriving the transfer matrix $G(s)$ from a state-space model [@problem_id:2713773]. By considering partitioned inputs and outputs, you will see how the internal structure of the system naturally gives rise to a block-structured transfer matrix, where each block describes the coupling between different groups of inputs and outputs.", "problem": "Consider a continuous-time linear time-invariant (LTI) multi-input multi-output (MIMO) system with state, input, and output described by\n$$\\dot{x}(t) = A x(t) + B u(t), \\quad y(t) = C x(t) + D u(t),$$\nwhere $x(t) \\in \\mathbb{R}^{n}$, $u(t) \\in \\mathbb{R}^{m}$, and $y(t) \\in \\mathbb{R}^{p}$. The inputs and outputs are grouped into two subsystems each, so that\n$$u(t) = \\begin{pmatrix} u_{1}(t) \\\\ u_{2}(t) \\end{pmatrix}, \\quad y(t) = \\begin{pmatrix} y_{1}(t) \\\\ y_{2}(t) \\end{pmatrix},$$\nwith $u_{1}(t) \\in \\mathbb{R}^{m_{1}}$, $u_{2}(t) \\in \\mathbb{R}^{m_{2}}$, $y_{1}(t) \\in \\mathbb{R}^{p_{1}}$, $y_{2}(t) \\in \\mathbb{R}^{p_{2}}$, and $m_{1} + m_{2} = m$, $p_{1} + p_{2} = p$. The matrices $B$, $C$, and $D$ are conformably partitioned as\n$$B = \\begin{pmatrix} B_{1}  B_{2} \\end{pmatrix}, \\quad C = \\begin{pmatrix} C_{1} \\\\ C_{2} \\end{pmatrix}, \\quad D = \\begin{pmatrix} D_{11}  D_{12} \\\\ D_{21}  D_{22} \\end{pmatrix},$$\nwhere $B_{j} \\in \\mathbb{R}^{n \\times m_{j}}$, $C_{i} \\in \\mathbb{R}^{p_{i} \\times n}$, and $D_{ij} \\in \\mathbb{R}^{p_{i} \\times m_{j}}$ for $i \\in \\{1,2\\}$ and $j \\in \\{1,2\\}$.\n\nStarting from first principles using the Laplace transform of the state equations under zero initial conditions and the definition of the transfer matrix as the mapping between the Laplace transforms $U(s)$ and $Y(s)$, derive the input-output transfer matrix $G(s)$ and show how the grouping of inputs and outputs induces a $2 \\times 2$ block structure. Express $G(s)$ explicitly in terms of $A$, $B_{1}$, $B_{2}$, $C_{1}$, $C_{2}$, $D_{ij}$, and the complex Laplace variable $s$, as a single $2 \\times 2$ block matrix. Your derivation should make clear which sub-block maps each input group to each output group, and why.\n\nProvide your final result as a single closed-form analytic expression for the $2 \\times 2$ block transfer matrix $G(s)$. No numerical evaluation is required, and no rounding is necessary. Do not include units.", "solution": "The problem statement is examined for validity before any attempt at a solution is made.\n\nFirst, we extract the givens of the problem.\nThe system is a continuous-time linear time-invariant (LTI) multi-input multi-output (MIMO) system described by the state-space equations:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nThe state, input, and output vectors have dimensions $x(t) \\in \\mathbb{R}^{n}$, $u(t) \\in \\mathbb{R}^{m}$, and $y(t) \\in \\mathbb{R}^{p}$.\nThe initial conditions are specified as zero: $x(0) = 0$.\nThe input and output vectors are partitioned as:\n$$\nu(t) = \\begin{pmatrix} u_{1}(t) \\\\ u_{2}(t) \\end{pmatrix}, \\quad y(t) = \\begin{pmatrix} y_{1}(t) \\\\ y_{2}(t) \\end{pmatrix}\n$$\nThe dimensions of these partitions are $u_{1}(t) \\in \\mathbb{R}^{m_{1}}$, $u_{2}(t) \\in \\mathbb{R}^{m_{2}}$, $y_{1}(t) \\in \\mathbb{R}^{p_{1}}$, $y_{2}(t) \\in \\mathbb{R}^{p_{2}}$, with $m_{1} + m_{2} = m$ and $p_{1} + p_{2} = p$.\nThe system matrices $B$, $C$, and $D$ are partitioned conformably:\n$$\nB = \\begin{pmatrix} B_{1}  B_{2} \\end{pmatrix}, \\quad C = \\begin{pmatrix} C_{1} \\\\ C_{2} \\end{pmatrix}, \\quad D = \\begin{pmatrix} D_{11}  D_{12} \\\\ D_{21}  D_{22} \\end{pmatrix}\n$$\nThe dimensions of the matrix partitions are $B_{j} \\in \\mathbb{R}^{n \\times m_{j}}$, $C_{i} \\in \\mathbb{R}^{p_{i} \\times n}$, and $D_{ij} \\in \\mathbb{R}^{p_{i} \\times m_{j}}$ for $i \\in \\{1,2\\}$ and $j \\in \\{1,2\\}$.\nThe task is to derive the input-output transfer matrix $G(s)$ from first principles, expressing it as a $2 \\times 2$ block matrix in terms of the given partitioned matrices and the Laplace variable $s$.\n\nNext, we validate the problem statement.\nThe problem is scientifically grounded, rooted in the fundamental principles of linear systems and control theory. The state-space representation and the derivation of a transfer function via Laplace transform are standard, well-established procedures. The problem is well-posed, providing all necessary information (equations, partitions, zero initial conditions) to derive a unique mathematical expression for the transfer matrix. The language is objective and mathematically precise. The setup is complete and internally consistent, with all matrix and vector dimensions being conformable for the specified operations. The problem is a formalizable and relevant exercise in the analysis of MIMO systems. It does not violate any criteria for invalidity.\n\nThe problem is deemed valid. We proceed with the solution.\n\nThe derivation begins by applying the Laplace transform to the state-space equations. Let $X(s)$, $U(s)$, and $Y(s)$ be the Laplace transforms of $x(t)$, $u(t)$, and $y(t)$, respectively. Applying the transform to the state equation yields:\n$$\n\\mathcal{L}\\{\\dot{x}(t)\\} = \\mathcal{L}\\{A x(t) + B u(t)\\}\n$$\nUsing the differentiation property of the Laplace transform, $\\mathcal{L}\\{\\dot{x}(t)\\} = sX(s) - x(0)$. Given the condition of zero initial state, $x(0)=0$, this simplifies to $sX(s)$. The equation becomes:\n$$\nsX(s) = AX(s) + BU(s)\n$$\nThis is an algebraic equation in the Laplace domain. We must solve for the state vector transform, $X(s)$, in terms of the input transform, $U(s)$.\n$$\nsX(s) - AX(s) = BU(s)\n$$\nFactoring $X(s)$ requires the use of the identity matrix $I$ of dimension $n \\times n$:\n$$\n(sI - A)X(s) = BU(s)\n$$\nAssuming that $s$ is not an eigenvalue of $A$, the matrix $(sI - A)$ is invertible. We can pre-multiply by its inverse, $(sI - A)^{-1}$, which is the resolvent of matrix $A$.\n$$\nX(s) = (sI - A)^{-1} B U(s)\n$$\nNow, we apply the Laplace transform to the output equation:\n$$\n\\mathcal{L}\\{y(t)\\} = \\mathcal{L}\\{C x(t) + D u(t)\\}\n$$\n$$\nY(s) = CX(s) + DU(s)\n$$\nSubstitute the expression for $X(s)$ into the transformed output equation:\n$$\nY(s) = C(sI - A)^{-1} B U(s) + D U(s)\n$$\nBy factoring out $U(s)$ on the right, we obtain the relationship between the input and output transforms:\n$$\nY(s) = \\left[ C(sI - A)^{-1} B + D \\right] U(s)\n$$\nBy definition, the transfer matrix $G(s)$ is the matrix that relates the input transform $U(s)$ to the output transform $Y(s)$ as $Y(s) = G(s)U(s)$. Therefore, we identify the transfer matrix as:\n$$\nG(s) = C(sI - A)^{-1} B + D\n$$\nTo reveal the $2 \\times 2$ block structure induced by the partitioning of inputs and outputs, we substitute the partitioned forms of the matrices $B$, $C$, and $D$ into this expression.\n$$\nG(s) = \\begin{pmatrix} C_{1} \\\\ C_{2} \\end{pmatrix} (sI - A)^{-1} \\begin{pmatrix} B_{1}  B_{2} \\end{pmatrix} + \\begin{pmatrix} D_{11}  D_{12} \\\\ D_{21}  D_{22} \\end{pmatrix}\n$$\nThe product of the block matrices $C$ and $B_j$ with the central matrix $(sI-A)^{-1}$ is carried out according to the rules of block matrix multiplication:\n$$\n\\begin{pmatrix} C_{1} \\\\ C_{2} \\end{pmatrix} (sI - A)^{-1} \\begin{pmatrix} B_{1}  B_{2} \\end{pmatrix} = \\begin{pmatrix} C_{1}(sI - A)^{-1}B_{1}  C_{1}(sI - A)^{-1}B_{2} \\\\ C_{2}(sI - A)^{-1}B_{1}  C_{2}(sI - A)^{-1}B_{2} \\end{pmatrix}\n$$\nNow, we add the partitioned feedthrough matrix $D$:\n$$\nG(s) = \\begin{pmatrix} C_{1}(sI - A)^{-1}B_{1}  C_{1}(sI - A)^{-1}B_{2} \\\\ C_{2}(sI - A)^{-1}B_{1}  C_{2}(sI - A)^{-1}B_{2} \\end{pmatrix} + \\begin{pmatrix} D_{11}  D_{12} \\\\ D_{21}  D_{22} \\end{pmatrix}\n$$\nThe sum of two conformably partitioned block matrices is the block-wise sum of their corresponding entries. This results in the final $2 \\times 2$ block structure for the transfer matrix $G(s)$:\n$$\nG(s) = \\begin{pmatrix} C_{1}(sI - A)^{-1}B_{1} + D_{11}  C_{1}(sI - A)^{-1}B_{2} + D_{12} \\\\ C_{2}(sI - A)^{-1}B_{1} + D_{21}  C_{2}(sI - A)^{-1}B_{2} + D_{22} \\end{pmatrix}\n$$\nThis expression shows the explicit $2 \\times 2$ block form of the system transfer matrix. If we denote $G(s)$ as $G(s) = \\begin{pmatrix} G_{11}(s)  G_{12}(s) \\\\ G_{21}(s)  G_{22}(s) \\end{pmatrix}$, then the input-output relationship $Y(s) = G(s)U(s)$ can be written as:\n$$\n\\begin{pmatrix} Y_{1}(s) \\\\ Y_{2}(s) \\end{pmatrix} = \\begin{pmatrix} G_{11}(s)  G_{12}(s) \\\\ G_{21}(s)  G_{22}(s) \\end{pmatrix} \\begin{pmatrix} U_{1}(s) \\\\ U_{2}(s) \\end{pmatrix}\n$$\nFrom this, it is evident that each sub-block $G_{ij}(s)$ maps the input group $j$ to the output group $i$. Specifically:\n$G_{11}(s) = C_{1}(sI - A)^{-1}B_{1} + D_{11}$ maps $U_{1}(s)$ to $Y_{1}(s)$.\n$G_{12}(s) = C_{1}(sI - A)^{-1}B_{2} + D_{12}$ maps $U_{2}(s)$ to $Y_{1}(s)$.\n$G_{21}(s) = C_{2}(sI - A)^{-1}B_{1} + D_{21}$ maps $U_{1}(s)$ to $Y_{2}(s)$.\n$G_{22}(s) = C_{2}(sI - A)^{-1}B_{2} + D_{22}$ maps $U_{2}(s)$ to $Y_{2}(s)$.\nThis completes the derivation as requested. The final expression is a single closed-form analytic expression for the $2 \\times 2$ block transfer matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nC_{1}(sI - A)^{-1}B_{1} + D_{11}  C_{1}(sI - A)^{-1}B_{2} + D_{12} \\\\\nC_{2}(sI - A)^{-1}B_{1} + D_{21}  C_{2}(sI - A)^{-1}B_{2} + D_{22}\n\\end{pmatrix}\n}\n$$", "id": "2713773"}, {"introduction": "Once a plant model $G(s)$ is established, the next step is to analyze its behavior within a feedback loop. This exercise focuses on the core algebraic manipulations of multivariable feedback by introducing a controller $K$ [@problem_id:2713799]. You will compute the loop transfer matrix $L(s) = G(s)K$ and the return difference matrix $I+L(s)$, two cornerstones of feedback theory whose properties govern the stability and performance of the closed-loop system.", "problem": "Consider a two-input two-output linear time-invariant plant with transfer matrix\n$$\nG(s)=\\begin{pmatrix}\n\\dfrac{1}{s+1}  \\dfrac{1}{s+2} \\\\\n0  \\dfrac{2}{s+3}\n\\end{pmatrix},\n$$\nand a static multivariable controller\n$$\nK=\\begin{pmatrix}\nk_{1}  0\\\\\nk_{2}  k_{3}\n\\end{pmatrix},\n$$\nwhere $k_{1}$, $k_{2}$, and $k_{3}$ are real constants. The plant and controller are interconnected in standard negative feedback, where the controller acts on the tracking error $e=r-y$ to produce the control input $u=K e$, the plant output is $y=G u$, and the measurement is subtracted from the reference.\n\nStarting from the foundational block-diagram relations for negative feedback and the definition of loop transfer in multivariable feedback, do the following:\n\n- Derive the loop transfer matrix $L(s)$ and the return difference matrix $I+L(s)$.\n- Evaluate both at $s=0$.\n- Then, using only matrix algebra, compute the determinant of the return difference at $s=0$ and simplify it to a closed-form analytic expression in terms of $k_{1}$, $k_{2}$, and $k_{3}$.\n\nAdditionally, explain (qualitatively, without appealing to any unintroduced shortcut formulas) why the loop transfer and return difference play a central role in quantifying multivariable gain and phase margins, in particular via their connection to sensitivity and complementary sensitivity.\n\nYour final reported answer must be the simplified closed-form expression for $\\det(I+L(0))$ in terms of $k_{1}$, $k_{2}$, and $k_{3}$. No numerical substitution is required, and no rounding is needed. Do not include units in your final answer.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a complete solution. It is a standard exercise in multivariable control theory. We shall proceed with the derivation.\n\nThe fundamental relations for the negative feedback system are given as:\n$$\n\\mathbf{y}(s) = \\mathbf{G}(s) \\mathbf{u}(s)\n$$\n$$\n\\mathbf{u}(s) = \\mathbf{K} \\mathbf{e}(s)\n$$\n$$\n\\mathbf{e}(s) = \\mathbf{r}(s) - \\mathbf{y}(s)\n$$\nwhere $\\mathbf{y}(s)$ is the plant output, $\\mathbf{u}(s)$ is the control input, $\\mathbf{e}(s)$ is the tracking error, and $\\mathbf{r}(s)$ is the reference signal.\n\nFirst, we derive the loop transfer matrix, $\\mathbf{L}(s)$. The loop transfer matrix is the transfer function matrix from the error signal $\\mathbf{e}(s)$ to the plant output $\\mathbf{y}(s)$ which is then fed back to the summing junction. Combining the first two equations, we have:\n$$\n\\mathbf{y}(s) = \\mathbf{G}(s) (\\mathbf{K} \\mathbf{e}(s)) = (\\mathbf{G}(s)\\mathbf{K}) \\mathbf{e}(s)\n$$\nTherefore, the loop transfer matrix is defined as $\\mathbf{L}(s) = \\mathbf{G}(s)\\mathbf{K}$.\n\nWe are given the plant transfer matrix $\\mathbf{G}(s)$ and the controller matrix $\\mathbf{K}$:\n$$\n\\mathbf{G}(s)=\\begin{pmatrix}\n\\frac{1}{s+1}  \\frac{1}{s+2} \\\\\n0  \\frac{2}{s+3}\n\\end{pmatrix}, \\quad \\mathbf{K}=\\begin{pmatrix}\nk_{1}  0\\\\\nk_{2}  k_{3}\n\\end{pmatrix}\n$$\nWe compute the product $\\mathbf{L}(s) = \\mathbf{G}(s)\\mathbf{K}$:\n$$\n\\mathbf{L}(s) = \\begin{pmatrix}\n\\frac{1}{s+1}  \\frac{1}{s+2} \\\\\n0  \\frac{2}{s+3}\n\\end{pmatrix} \\begin{pmatrix}\nk_{1}  0\\\\\nk_{2}  k_{3}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{1}{s+1} \\cdot k_{1} + \\frac{1}{s+2} \\cdot k_{2}  \\frac{1}{s+1} \\cdot 0 + \\frac{1}{s+2} \\cdot k_{3} \\\\\n0 \\cdot k_{1} + \\frac{2}{s+3} \\cdot k_{2}  0 \\cdot 0 + \\frac{2}{s+3} \\cdot k_{3}\n\\end{pmatrix}\n$$\n$$\n\\mathbf{L}(s) = \\begin{pmatrix}\n\\frac{k_{1}}{s+1} + \\frac{k_{2}}{s+2}  \\frac{k_{3}}{s+2} \\\\\n\\frac{2k_{2}}{s+3}  \\frac{2k_{3}}{s+3}\n\\end{pmatrix}\n$$\nThe return difference matrix is defined as $\\mathbf{I}+\\mathbf{L}(s)$, where $\\mathbf{I}$ is the $2 \\times 2$ identity matrix.\n$$\n\\mathbf{I}+\\mathbf{L}(s) = \\begin{pmatrix}\n1  0 \\\\\n0  1\n\\end{pmatrix} + \\begin{pmatrix}\n\\frac{k_{1}}{s+1} + \\frac{k_{2}}{s+2}  \\frac{k_{3}}{s+2} \\\\\n\\frac{2k_{2}}{s+3}  \\frac{2k_{3}}{s+3}\n\\end{pmatrix} = \\begin{pmatrix}\n1 + \\frac{k_{1}}{s+1} + \\frac{k_{2}}{s+2}  \\frac{k_{3}}{s+2} \\\\\n\\frac{2k_{2}}{s+3}  1 + \\frac{2k_{3}}{s+3}\n\\end{pmatrix}\n$$\nThe next step is to evaluate these matrices at $s=0$. First, we evaluate $\\mathbf{L}(0)$:\n$$\n\\mathbf{L}(0) = \\begin{pmatrix}\n\\frac{k_{1}}{0+1} + \\frac{k_{2}}{0+2}  \\frac{k_{3}}{0+2} \\\\\n\\frac{2k_{2}}{0+3}  \\frac{2k_{3}}{0+3}\n\\end{pmatrix} = \\begin{pmatrix}\nk_{1} + \\frac{k_{2}}{2}  \\frac{k_{3}}{2} \\\\\n\\frac{2k_{2}}{3}  \\frac{2k_{3}}{3}\n\\end{pmatrix}\n$$\nAnd the return difference matrix at $s=0$:\n$$\n\\mathbf{I}+\\mathbf{L}(0) = \\begin{pmatrix}\n1 + k_{1} + \\frac{k_{2}}{2}  \\frac{k_{3}}{2} \\\\\n\\frac{2k_{2}}{3}  1 + \\frac{2k_{3}}{3}\n\\end{pmatrix}\n$$\nNow, we compute the determinant of the return difference matrix at $s=0$ as requested.\n$$\n\\det(\\mathbf{I}+\\mathbf{L}(0)) = \\left(1 + k_{1} + \\frac{k_{2}}{2}\\right) \\left(1 + \\frac{2k_{3}}{3}\\right) - \\left(\\frac{k_{3}}{2}\\right) \\left(\\frac{2k_{2}}{3}\\right)\n$$\nWe expand the product:\n$$\n\\det(\\mathbf{I}+\\mathbf{L}(0)) = 1\\left(1 + \\frac{2k_{3}}{3}\\right) + k_{1}\\left(1 + \\frac{2k_{3}}{3}\\right) + \\frac{k_{2}}{2}\\left(1 + \\frac{2k_{3}}{3}\\right) - \\frac{2k_{2}k_{3}}{6}\n$$\n$$\n\\det(\\mathbf{I}+\\mathbf{L}(0)) = 1 + \\frac{2k_{3}}{3} + k_{1} + \\frac{2k_{1}k_{3}}{3} + \\frac{k_{2}}{2} + \\frac{2k_{2}k_{3}}{6} - \\frac{k_{2}k_{3}}{3}\n$$\nNoticing that $\\frac{2k_{2}k_{3}}{6} = \\frac{k_{2}k_{3}}{3}$, the last two terms cancel each other. This leaves the simplified expression:\n$$\n\\det(\\mathbf{I}+\\mathbf{L}(0)) = 1 + k_{1} + \\frac{k_{2}}{2} + \\frac{2k_{3}}{3} + \\frac{2k_{1}k_{3}}{3}\n$$\nThis is the required closed-form analytic expression.\n\nAdditionally, a qualitative explanation is required. The stability of the closed-loop system is determined by the locations of its poles, which are the roots of the characteristic equation $\\det(\\mathbf{I}+\\mathbf{L}(s)) = 0$. For the SISO case, this reduces to $1+L(s)=0$. The Nyquist stability criterion examines the encirclements of the critical point $-1$ by the frequency response $L(j\\omega)$. The distance $|1+L(j\\omega)|$ is a measure of robustness; a small value indicates proximity to instability.\n\nIn the MIMO case, the return difference matrix $\\mathbf{I}+\\mathbf{L}(s)$ plays an analogous role. The generalized Nyquist criterion states that for a stable open-loop system, closed-loop stability is guaranteed if the locus of $\\det(\\mathbf{I}+\\mathbf{L}(j\\omega))$ for $\\omega \\in [-\\infty, \\infty]$ does not encircle the origin. The value of $\\det(\\mathbf{I}+\\mathbf{L}(j\\omega))$ is the multivariable equivalent of the scalar distance $1+L(j\\omega)$ from the critical point. A value close to zero indicates that the system is approaching a condition of instability. Thus, quantities like the minimum singular value of $\\mathbf{I}+\\mathbf{L}(j\\omega)$, $\\sigma_{\\min}(\\mathbf{I}+\\mathbf{L}(j\\omega))$, provide a robust measure of the multivariable gain and phase margins by quantifying how close the matrix $\\mathbf{I}+\\mathbf{L}(j\\omega)$ is to being singular.\n\nFurthermore, the return difference is fundamentally linked to system performance through the sensitivity function $\\mathbf{S}(s) = (\\mathbf{I}+\\mathbf{L}(s))^{-1}$ and the complementary sensitivity function $\\mathbf{T}(s) = \\mathbf{L}(s)(\\mathbf{I}+\\mathbf{L}(s))^{-1}$. The sensitivity function $\\mathbf{S}(s)$ maps reference commands and output disturbances to the tracking error. For good performance (tracking and disturbance rejection), $\\mathbf{S}(s)$ must be small at low frequencies, which requires $\\mathbf{L}(s)$ to be \"large\". The complementary sensitivity function $\\mathbf{T}(s)$ maps sensor noise to the plant output. For robustness against noise and model uncertainty, $\\mathbf{T}(s)$ must be small at high frequencies, which requires $\\mathbf{L}(s)$ to be \"small\". The behavior of $\\mathbf{I}+\\mathbf{L}(s)$ across all frequencies therefore governs the fundamental trade-offs between performance and robustness, with its determinant dictating the absolute stability of the closed-loop system.", "answer": "$$\n\\boxed{1 + k_{1} + \\frac{k_{2}}{2} + \\frac{2k_{3}}{3} + \\frac{2k_{1}k_{3}}{3}}\n$$", "id": "2713799"}, {"introduction": "While the determinant of the return difference matrix, $\\det(I+L(s))$, governs nominal closed-loop stability, it can provide a misleading sense of security regarding robustness. This advanced problem explores a scenario where the system is close to instability in one direction, even though the determinant remains large [@problem_id:2713830]. This will help you build intuition for why singular value analysis, which measures the gain of a matrix in its strongest and weakest directions, is an indispensable tool for assessing the true robustness of MIMO feedback systems.", "problem": "Consider a square linear time-invariant multi-input multi-output plant with standard negative unity feedback. Let the loop transfer matrix be $L(s) = G(s)K(s)$, where $G(s)$ is the plant and $K(s)$ is a stabilizing controller. The closed-loop characteristic function is determined by the return-difference matrix $I + L(s)$. Suppose the plant is nonminimum-phase, i.e., $G(s)$ has at least one right-half-plane zero, so that unavoidable phase limitations arise in $L(j\\omega)$.\n\nAt a particular frequency $\\omega_0$, assume $L(j\\omega_0)$ is diagonalizable with\n$$\nL(j\\omega_0) = Q \\,\\mathrm{diag}(-1 + \\varepsilon,\\; K)\\, Q^{-1},\n$$\nwhere $Q$ is invertible, $K > 0$, and $0  \\varepsilon \\ll 1$. Further suppose $Q$ is ill-conditioned in the sense that its spectral condition number $\\kappa_2(Q) = \\|Q\\|_2 \\,\\|Q^{-1}\\|_2$ is large. You are asked to evaluate the merits and limitations of using the determinant-based Nyquist test that inspects the scalar mapping $g(j\\omega) = \\det(I + L(j\\omega))$ for closed-loop stability assessment, and to explain how singular value tests based on the smallest singular value of $I + L(j\\omega)$ or the largest singular value of $T(j\\omega) = L(j\\omega)(I+L(j\\omega))^{-1}$ complement this perspective in the nonminimum-phase setting.\n\nWhich of the following statements is the best explanation of why the determinant-based Nyquist plot may be misleading in this setting and how singular value tests complement it?\n\nA. Even when the determinant-based Nyquist plot shows zero encirclements of the origin and a seemingly comfortable distance of $g(j\\omega)$ from the origin, a nonminimum-phase multi-input multi-output plant can have $I + L(j\\omega)$ nearly singular in certain input-output directions. This occurs because $\\det(I + L)$ equals the product of the eigenvalues of $I + L$, so large factors can mask a small one. Singular value tests, especially tracking $\\sigma_{\\min}(I + L)$ and $\\sigma_{\\max}(T)$, expose such directional near-instability and quantify multivariable gain and phase robustness.\n\nB. The determinant-based Nyquist test can falsely certify stability in exact arithmetic because $\\det(I + L)$ may be nonzero while $I + L$ is singular; only singular values can detect singularity.\n\nC. The only failure mode of the determinant-based Nyquist test for nonminimum-phase plants is that it double-counts right-half-plane zeros; singular value tests fix the encirclement count by dividing by $\\det(G)$.\n\nD. Singular value tests are redundant with the determinant-based Nyquist test because $\\sigma_{\\min}(I + L)$ always equals $\\left|\\det(I + L)\\right|^{1/n}$ for an $n \\times n$ loop, making both methods equivalent.\n\nE. For nonminimum-phase plants with right-half-plane zeros, the determinant-based Nyquist test must be fully replaced by the scalar condition that $\\sigma_{\\max}(L(j\\omega))$ crosses unity at phase $-\\pi$; this condition is necessary and sufficient for internal stability in multi-input multi-output loops.\n\nSelect all correct options.", "solution": "Let's analyze the properties of the return difference matrix $I+L(j\\omega_0)$. The eigenvalues of a matrix are invariant under a similarity transformation, so the eigenvalues of $L(j\\omega_0)$ are $-1+\\epsilon$ and $K$. The eigenvalues of $I+L(j\\omega_0)$ are therefore $1+(-1+\\epsilon) = \\epsilon$ and $1+K$.\n\nThe determinant of a matrix is the product of its eigenvalues. Thus,\n$$\n\\det(I+L(j\\omega_0)) = \\epsilon(1+K)\n$$\nSince $K > 0$ and $\\epsilon \\ll 1$, if $K$ is large (e.g., $K=99$), the determinant could be $\\epsilon(100)$, which might not be close to zero, suggesting stability. For example, if $\\epsilon = 0.01$, the determinant is $1$. In a determinant-based Nyquist plot, this point would appear to be a safe distance from the origin.\n\nHowever, the stability margin is better characterized by how close the matrix $I+L$ is to being singular. A matrix is singular if it has a zero eigenvalue. Here, $I+L(j\\omega_0)$ has an eigenvalue of $\\epsilon$, which is very close to zero. This indicates that the system is close to instability in the direction corresponding to that eigenvector.\n\nSingular values provide a more reliable measure of this proximity to singularity. The smallest singular value, $\\sigma_{\\min}(I+L)$, gives the distance to the nearest singular matrix. In this case, $\\sigma_{\\min}(I+L(j\\omega_0))$ will be on the order of $\\epsilon$, which is small, correctly indicating a small stability margin (poor robustness).\n\nFurthermore, the complementary sensitivity function $T = L(I+L)^{-1}$ has a maximum singular value $\\sigma_{\\max}(T)$ that is large when $\\sigma_{\\min}(I+L)$ is small. A large peak in $\\sigma_{\\max}(T)$ indicates poor robustness to uncertainty and sensitivity to noise.\n\nLet's evaluate the options based on this analysis:\n- **A**: Correctly states that the determinant (product of eigenvalues) can mask a small eigenvalue (near-singularity) if other eigenvalues are large. It correctly identifies $\\sigma_{\\min}(I+L)$ and $\\sigma_{\\max}(T)$ as the proper tools to detect this directional weakness.\n- **B**: Incorrect. A matrix is singular if and only if its determinant is zero.\n- **C**: Incorrect. The Nyquist test is fundamentally sound; its interpretation is the issue.\n- **D**: Incorrect. The relationship between the smallest singular value and the determinant is not this simple in general.\n- **E**: Incorrect. This describes a sufficient condition for robust stability under specific uncertainty models, not a replacement for the general Nyquist criterion.\n\nTherefore, option A is the only correct and complete explanation.", "answer": "$$\\boxed{A}$$", "id": "2713830"}]}