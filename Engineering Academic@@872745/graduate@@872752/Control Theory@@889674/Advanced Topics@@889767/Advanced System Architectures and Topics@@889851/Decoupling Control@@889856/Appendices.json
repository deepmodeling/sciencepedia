{"hands_on_practices": [{"introduction": "Before designing a controller, it is crucial to understand the inherent complexity of the control task. This exercise [@problem_id:2699010] challenges you to determine the minimal dynamic order—the McMillan degree—of a compensator required to decouple a given linear multivariable system. By doing so, you will connect the algebraic conditions for decoupling with the fundamental concepts of system realization and complexity.", "problem": "Consider the square, proper, stable, linear time-invariant transfer function matrix\n$$\nG(s) \\;=\\;\n\\begin{pmatrix}\n\\dfrac{1}{s+1}  \\dfrac{1}{(s+1)^{2}} \\\\\n\\dfrac{1}{(s+3)^{2}}  \\dfrac{1}{s+2}\n\\end{pmatrix}.\n$$\nA right decoupling compensator is any proper, stable, rational matrix $R(s)$ such that $G(s)R(s)$ is diagonal with both diagonal entries not identically zero. The McMillan degree of a rational matrix is the minimal state dimension among all realizations that implement it. Starting from the definitions of properness, rational transfer matrices, and the requirement that the off-diagonal entries of $G(s)R(s)$ vanish identically, determine the minimal possible McMillan degree of any proper, stable right decoupling compensator $R(s)$.\n\nProvide your answer as a single integer. No rounding is required, and no physical units are involved.", "solution": "The user wants to find the minimal McMillan degree of a proper, stable, right decoupling compensator $R(s)$ for the given transfer function matrix $G(s)$.\n\nFirst, we validate the problem statement.\nThe givens are:\n$G(s) = \\begin{pmatrix} \\frac{1}{s+1}  \\frac{1}{(s+1)^{2}} \\\\ \\frac{1}{(s+3)^{2}}  \\frac{1}{s+2} \\end{pmatrix}$.\n$R(s)$ is a proper, stable, rational matrix.\n$Q(s) = G(s)R(s)$ is a diagonal matrix, with diagonal entries not identically zero.\nThe McMillan degree is the minimal state dimension of a realization.\nWe must find the minimal McMillan degree of $R(s)$.\n\nThe problem is formulated within standard linear multivariable control theory. The given matrix $G(s)$ is proper as all its entries are strictly proper. Its poles are located at $s=-1$, $s=-2$, and $s=-3$, all in the stable left-half plane. The problem is well-posed, objective, and scientifically grounded. We may proceed with the solution.\n\nLet the compensator be $R(s) = \\begin{pmatrix} r_{11}(s)  r_{12}(s) \\\\ r_{21}(s)  r_{22}(s) \\end{pmatrix}$.\nThe condition that $G(s)R(s)$ is diagonal implies that the off-diagonal entries are zero.\n$(G(s)R(s))_{12} = g_{11}(s)r_{12}(s) + g_{12}(s)r_{22}(s) = 0$\n$(G(s)R(s))_{21} = g_{21}(s)r_{11}(s) + g_{22}(s)r_{21}(s) = 0$\n\nFrom a structural point of view, these two equations impose constraints on the columns of $R(s)$:\n1. The second column of $R(s)$, $\\begin{pmatrix} r_{12}(s) \\\\ r_{22}(s) \\end{pmatrix}$, must be in the null space of the row vector $\\begin{pmatrix} g_{11}(s)  g_{12}(s) \\end{pmatrix}$.\n2. The first column of $R(s)$, $\\begin{pmatrix} r_{11}(s) \\\\ r_{21}(s) \\end{pmatrix}$, must be in the null space of the row vector $\\begin{pmatrix} g_{21}(s)  g_{22}(s) \\end{pmatrix}$.\n\nLet us analyze these constraints separately.\n\nFor the second column of $R(s)$:\nThe condition implies the ratio of its elements must satisfy:\n$$\n\\frac{r_{12}(s)}{r_{22}(s)} = -\\frac{g_{12}(s)}{g_{11}(s)} = -\\frac{1/(s+1)^2}{1/(s+1)} = -\\frac{1}{s+1}\n$$\nSo, we must have $r_{22}(s) = -(s+1)r_{12}(s)$.\nAlternatively, we can express the column vector as a multiple of a basis vector for the null space:\n$$\n\\begin{pmatrix} r_{12}(s) \\\\ r_{22}(s) \\end{pmatrix} = k_2(s) \\begin{pmatrix} -g_{12}(s) \\\\ g_{11}(s) \\end{pmatrix} = k_2(s) \\begin{pmatrix} -1/(s+1)^2 \\\\ 1/(s+1) \\end{pmatrix}\n$$\nwhere $k_2(s)$ is an arbitrary rational function. To make $r_{12}(s)$ and $r_{22}(s)$ proper, we must select $k_2(s)$ accordingly.\nFor $r_{12}(s) = -k_2(s)/(s+1)^2$ to be proper, we need $\\text{degree}(k_2) \\le 2$.\nFor $r_{22}(s) = k_2(s)/(s+1)$ to be proper, we need $\\text{degree}(k_2) \\le 1$.\nBoth conditions must hold, so $k_2(s)$ must be a polynomial of degree at most $1$. To minimize the degree of the compensator, we should aim for pole-zero cancellations. A choice of $k_2(s) = c_2(s+1)$ (where $c_2$ is a non-zero constant) seems promising. This gives:\n$r_{12}(s) = -c_2(s+1)/(s+1)^2 = -c_2/(s+1)$.\n$r_{22}(s) = c_2(s+1)/(s+1) = c_2$.\nThis choice yields a proper and stable second column $\\begin{pmatrix} -c_2/(s+1) \\\\ c_2 \\end{pmatrix}$. The McMillan degree of this subsystem is the degree of the pole at $s=-1$, which is $1$.\n\nFor the first column of $R(s)$:\nThe condition implies the ratio:\n$$\n\\frac{r_{11}(s)}{r_{21}(s)} = -\\frac{g_{22}(s)}{g_{21}(s)} = -\\frac{1/(s+2)}{1/(s+3)^2} = -\\frac{(s+3)^2}{s+2}\n$$\nAs before, we can write the column as:\n$$\n\\begin{pmatrix} r_{11}(s) \\\\ r_{21}(s) \\end{pmatrix} = k_1(s) \\begin{pmatrix} g_{22}(s) \\\\ -g_{21}(s) \\end{pmatrix} = k_1(s) \\begin{pmatrix} 1/(s+2) \\\\ -1/(s+3)^2 \\end{pmatrix}\n$$\nwhere $k_1(s)$ is an arbitrary rational function. For properness:\nFor $r_{11}(s) = k_1(s)/(s+2)$ to be proper, we need $\\text{degree}(k_1) \\le 1$.\nFor $r_{21}(s) = -k_1(s)/(s+3)^2$ to be proper, we need $\\text{degree}(k_1) \\le 2$.\nBoth must hold, so $k_1(s)$ must be a polynomial of degree at most $1$. Again, to seek a minimal realization, we look for cancellations. A choice of $k_1(s) = c_1(s+2)$ (where $c_1$ is a non-zero constant) is appropiate. This gives:\n$r_{11}(s) = c_1(s+2)/(s+2) = c_1$.\n$r_{21}(s) = -c_1(s+2)/(s+3)^2$.\nThis choice yields a proper and stable first column $\\begin{pmatrix} c_1 \\\\ -c_1(s+2)/(s+3)^2 \\end{pmatrix}$. The McMillan degree of this subsystem is the degree of the pole at $s=-3$, which is $2$.\n\nCombining these minimal selections for each column, we construct a candidate compensator $R(s)$. Let $c_1=1$ and $c_2=1$ for simplicity.\n$$\nR(s) = \\begin{pmatrix} 1  -\\dfrac{1}{s+1} \\\\ -\\dfrac{s+2}{(s+3)^2}  1 \\end{pmatrix}\n$$\nThis matrix $R(s)$ is proper and stable (poles at $s=-1, s=-3$). We must determine its McMillan degree. The McMillan degree is the degree of the pole polynomial of $R(s)$, which is the least common multiple (LCM) of the denominators of all minors of $R(s)$, assuming all entries are written as ratios of coprime polynomials.\nThe minors of first order are the entries themselves. Their denominators are $1$, $s+1$, and $(s+3)^2$. The LCM of these is $(s+1)(s+3)^2$.\nThe minor of second order is the determinant:\n$$\n\\det(R(s)) = (1)(1) - \\left(-\\frac{1}{s+1}\\right)\\left(-\\frac{s+2}{(s+3)^2}\\right) = 1 - \\frac{s+2}{(s+1)(s+3)^2}\n$$\n$$\n\\det(R(s)) = \\frac{(s+1)(s+3)^2 - (s+2)}{(s+1)(s+3)^2} = \\frac{(s+1)(s^2+6s+9) - s - 2}{(s+1)(s+3)^2}\n$$\n$$\n\\det(R(s)) = \\frac{s^3+6s^2+9s+s^2+6s+9-s-2}{(s+1)(s+3)^2} = \\frac{s^3+7s^2+14s+7}{(s+1)(s+3)^2}\n$$\nThe denominator of the determinant is $(s+1)(s+3)^2$.\n\nThe pole polynomial of $R(s)$ is the LCM of the denominators of all minors. In this case, it is $(s+1)(s+3)^2$.\nThe McMillan degree of this compensator is the degree of this polynomial, which is $1+2=3$.\n\nThe minimal degree of the total compensator is generally greater than or equal to the sum of the minimal degrees required for each column's decoupling task, especially when the required dynamics are distinct. The first column requires dynamics of degree $2$ (related to poles at $s=-3$), while the second column requires dynamics of degree $1$ (related to a pole at $s=-1$). Since these dynamics are associated with different poles, it is expected that the minimal degree of the full compensator would be the sum of the individual minimal degrees, which is $2+1=3$.\nSince we have constructed a valid compensator with McMillan degree $3$, and we have argued that the degree must be at least $3$, we conclude that the minimal possible McMillan degree is $3$.", "answer": "$$\\boxed{3}$$", "id": "2699010"}, {"introduction": "While linear models are powerful, many physical systems exhibit fundamentally nonlinear behavior, requiring more advanced tools. This problem [@problem_id:2698987] extends the concept of decoupling to the nonlinear domain using the language of differential geometry. You will employ Lie derivatives to determine the system's relative degree, construct a partially decoupling feedback law, and analyze the stability of the resulting internal dynamics, which are often the limiting factor in performance.", "problem": "Consider the nonlinear multiple-input multiple-output system in control-affine form with state $x \\in \\mathbb{R}^{4}$, inputs $u = (u_{1},u_{2}) \\in \\mathbb{R}^{2}$, and outputs $y = (y_{1},y_{2}) \\in \\mathbb{R}^{2}$:\n- State dynamics:\n$$\n\\begin{aligned}\n\\dot{x}_{1} = x_{2} + x_{1} x_{4},\\\\\n\\dot{x}_{2} = u_{1} + x_{3},\\\\\n\\dot{x}_{3} = u_{2},\\\\\n\\dot{x}_{4} = -x_{1} + x_{2} x_{3} + x_{4}^{3}.\n\\end{aligned}\n$$\n- Outputs:\n$$\ny_{1} = x_{1}, \\qquad y_{2} = x_{3}.\n$$\nStarting from the foundational definitions of Lie derivatives, relative degree, and the concept of input-output decoupling for nonlinear systems, perform the following:\n- Determine the vector of relative degrees $(r_{1},r_{2})$ associated with $(y_{1},y_{2})$ and the corresponding decoupling matrix $A(x)$ whose $(i,j)$ entry is given by $L_{g_{j}} L_{f}^{r_{i}-1} h_{i}(x)$, where $h_{1}(x)=x_{1}$ and $h_{2}(x)=x_{3}$.\n- Show that the system admits partial input-output linearization into a triangular form with relative degree vector $(r_{1},r_{2})$ and internal dynamics of dimension $n - (r_{1}+r_{2})$, where $n$ is the state dimension.\n- Choose normal form coordinates $(z_{1},z_{2},z_{3},\\eta)$ defined by\n$$\nz_{1} = y_{1}, \\quad z_{2} = \\dot{y}_{1}, \\quad z_{3} = y_{2}, \\quad \\eta = x_{4},\n$$\nand express their dynamics explicitly using only the system definition and the concept of Lie derivatives.\n- Using the input transformation implied by partial decoupling, define the virtual inputs $v_{1} = \\ddot{y}_{1}$ and $v_{2} = \\dot{y}_{2}$ and identify the corresponding feedback of the form $u = \\alpha(x) + \\beta(x) v$ that renders the input-output map triangular.\n- Consider the zero dynamics manifold defined by $z_{1}=0$, $z_{2}=0$, and $z_{3}=0$, and enforce invariance of this manifold by choosing $v_{1}=0$ and $v_{2}=0$. Compute the instantaneous internal dynamics rate $\\dot{\\eta}$ at the state\n$$\nx = (x_{1},x_{2},x_{3},x_{4}) = (0,0,0,2).\n$$\nProvide your final answer as a single real number. No rounding is required.", "solution": "The problem statement is a standard exercise in nonlinear control theory, specifically concerning input-output linearization and decoupling. It is scientifically grounded, well-posed, objective, and self-contained, presenting a valid system of equations and a set of clear, formalizable tasks. There are no contradictions, ambiguities, or factual unsoundness. Therefore, the problem is valid, and we proceed with the solution.\n\nThe system is given by the state-space representation $\\dot{x} = f(x) + g(x)u$, where $x \\in \\mathbb{R}^{4}$, $u \\in \\mathbb{R}^{2}$, and the outputs are $y=h(x)$. The components are explicitly:\n$$\nf(x) = \\begin{pmatrix} x_{2} + x_{1} x_{4} \\\\ x_{3} \\\\ 0 \\\\ -x_{1} + x_{2} x_{3} + x_{4}^{3} \\end{pmatrix}, \\quad g(x) = \\begin{bmatrix} g_{1}(x)  g_{2}(x) \\end{bmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nand the output functions are $h_{1}(x) = y_{1} = x_{1}$ and $h_{2}(x) = y_{2} = x_{3}$.\n\nFirst, we determine the vector of relative degrees $(r_{1}, r_{2})$. The relative degree $r_{i}$ is the smallest integer for which the Lie derivative of the $i$-th output function $h_{i}$ with respect to at least one of the input vector fields $g_{j}$ is non-zero, after having been differentiated $r_{i}-1$ times along the drift vector field $f$. That is, $L_{g_{j}}L_{f}^{k}h_{i}(x) = 0$ for all $k  r_{i}-1$ and for all $j \\in \\{1, 2\\}$, and for some $j$, $L_{g_{j}}L_{f}^{r_{i}-1}h_{i}(x) \\neq 0$.\n\nFor the first output, $h_{1}(x) = x_{1}$:\nThe Lie derivative with respect to $f$ is $\\dot{y}_{1}$ in the absence of control.\n$L_{f}h_{1}(x) = \\nabla h_{1}(x) \\cdot f(x) = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} f(x) = x_{2} + x_{1} x_{4}$.\nNow, we compute the Lie derivatives of $h_{1}(x)$ with respect to the input vector fields:\n$L_{g_{1}}h_{1}(x) = \\nabla h_{1}(x) \\cdot g_{1}(x) = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} g_{1}(x) = 0$.\n$L_{g_{2}}h_{1}(x) = \\nabla h_{1}(x) \\cdot g_{2}(x) = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} g_{2}(x) = 0$.\nSince both are zero, the relative degree $r_{1}$ is greater than $1$. We must differentiate further. We compute the Lie derivatives of $L_{f}h_{1}(x)$ with respect to $g_{j}$:\n$L_{g_{1}}L_{f}h_{1}(x) = \\nabla(L_{f}h_{1}(x)) \\cdot g_{1}(x) = \\nabla(x_{2} + x_{1} x_{4}) \\cdot g_{1}(x) = \\begin{pmatrix} x_{4}  1  0  x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1$.\nSince $L_{g_{1}}L_{f}^{1}h_{1}(x) = 1 \\neq 0$, the relative degree for the first output is $r_{1} = 2$.\nFor completeness, $L_{g_{2}}L_{f}h_{1}(x) = \\begin{pmatrix} x_{4}  1  0  x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = 0$.\n\nFor the second output, $h_{2}(x) = x_{3}$:\nWe compute the Lie derivatives with respect to $g_{j}$:\n$L_{g_{1}}h_{2}(x) = \\nabla h_{2}(x) \\cdot g_{1}(x) = \\begin{pmatrix} 0  0  1  0 \\end{pmatrix} g_{1}(x) = 0$.\n$L_{g_{2}}h_{2}(x) = \\nabla h_{2}(x) \\cdot g_{2}(x) = \\begin{pmatrix} 0  0  1  0 \\end{pmatrix} g_{2}(x) = 1$.\nSince $L_{g_{2}}h_{2}(x) \\neq 0$, the relative degree for the second output is $r_{2} = 1$.\nThe vector of relative degrees is therefore $(r_{1}, r_{2}) = (2, 1)$.\n\nThe decoupling matrix $A(x)$ is defined by its entries $A_{ij}(x) = L_{g_{j}} L_{f}^{r_{i}-1} h_{i}(x)$.\n$A_{11}(x) = L_{g_{1}} L_{f}^{2-1} h_{1}(x) = L_{g_{1}} L_{f} h_{1}(x) = 1$.\n$A_{12}(x) = L_{g_{2}} L_{f}^{2-1} h_{1}(x) = L_{g_{2}} L_{f} h_{1}(x) = 0$.\n$A_{21}(x) = L_{g_{1}} L_{f}^{1-1} h_{2}(x) = L_{g_{1}} h_{2}(x) = 0$.\n$A_{22}(x) = L_{g_{2}} L_{f}^{1-1} h_{2}(x) = L_{g_{2}} h_{2}(x) = 1$.\nThus, the decoupling matrix is the constant identity matrix:\n$$\nA(x) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe system admits partial input-output linearization because the decoupling matrix $A(x)$ is non-singular for all $x$. A diagonal matrix is a special case of a triangular matrix, so the condition is met. The total relative degree is $r_{1} + r_{2} = 2 + 1 = 3$, which is less than the state dimension $n=4$. This implies the existence of internal dynamics of dimension $n - (r_{1}+r_{2}) = 4 - 3 = 1$.\n\nNext, we define the normal form coordinates:\n$z_{1} = y_{1} = h_{1}(x) = x_{1}$.\n$z_{2} = \\dot{y}_{1} = L_{f}h_{1}(x) = x_{2} + x_{1} x_{4}$.\n$z_{3} = y_{2} = h_{2}(x) = x_{3}$.\n$\\eta = x_{4}$.\nThe coordinate transformation $\\Phi(x) = (z_{1}, z_{2}, z_{3}, \\eta)^T$ has Jacobian determinant equal to $1$, making it a global diffeomorphism. The dynamics of these new coordinates are found by differentiation:\n$\\dot{z}_{1} = \\frac{d}{dt}(x_{1}) = \\dot{x}_{1} = x_{2} + x_{1} x_{4} = z_{2}$.\n$\\dot{z}_{3} = \\frac{d}{dt}(x_{3}) = \\dot{x}_{3} = u_{2}$.\nThe time derivative of $z_{2}$ corresponds to the second derivative of $y_1$:\n$\\dot{z}_{2} = \\ddot{y}_{1} = L_{f}^{2}h_{1}(x) + L_{g_{1}}L_{f}h_{1}(x) u_{1} + L_{g_{2}}L_{f}h_{1}(x) u_{2}$.\nWe require $L_{f}^{2}h_{1}(x) = \\nabla(L_{f}h_{1}(x)) \\cdot f(x) = \\begin{pmatrix} x_{4}  1  0  x_{1} \\end{pmatrix} f(x) = x_{4}(x_{2} + x_{1}x_{4}) + x_{3} + x_{1}(-x_{1} + x_{2}x_{3} + x_{4}^{3}) = x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}$.\nSo, $\\dot{z}_{2} = (x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}) + u_{1}$.\nThe dynamics of the internal state $\\eta$ are:\n$\\dot{\\eta} = \\dot{x}_{4} = -x_{1} + x_{2}x_{3} + x_{4}^{3}$.\n\nWe now define the virtual inputs $v_{1} = \\ddot{y}_{1}$ and $v_{2} = \\dot{y}_{2}$. The input-output dynamics are:\n$\\ddot{y}_{1} = L_{f}^{2}h_{1}(x) + u_{1}$.\n$\\dot{y}_{2} = L_{f}h_{2}(x) + u_{2} = 0 + u_{2} = u_{2}$.\nSetting $\\ddot{y}_{1} = v_{1}$ and $\\dot{y}_{2} = v_{2}$ yields the linearizing feedback law.\n$v_{1} = L_{f}^{2}h_{1}(x) + u_{1} \\implies u_{1} = -L_{f}^{2}h_{1}(x) + v_{1}$.\n$v_{2} = u_{2}$.\nIn the form $u = \\alpha(x) + \\beta(x)v$, we have:\n$\\alpha(x) = \\begin{pmatrix} -L_{f}^{2}h_{1}(x) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -(x_{2}x_{4} + x_{1}x_{4}^{2} + x_{3} - x_{1}^{2} + x_{1}x_{2}x_{3} + x_{1}x_{4}^{3}) \\\\ 0 \\end{pmatrix}$\n$\\beta(x) = A(x)^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\nThe triangular structure is evident as the input transformation is diagonal (a special case of triangular).\n\nFinally, we analyze the zero dynamics. The zero dynamics manifold is defined by keeping the outputs and their derivatives at zero: $y_{1}=0$, $\\dot{y}_{1}=0$, and $y_{2}=0$.\n$z_{1} = y_{1} = x_{1} = 0$.\n$z_{2} = \\dot{y}_{1} = x_{2} + x_{1}x_{4} = 0$. Since $x_{1}=0$, we have $x_{2}=0$.\n$z_{3} = y_{2} = x_{3} = 0$.\nThe zero dynamics manifold is the set of states where $x_{1}=x_{2}=x_{3}=0$.\nTo enforce invariance of this manifold, we must set the virtual inputs to zero: $v_{1}=0$ and $v_{2}=0$. This implies $\\ddot{y}_{1}=0$ and $\\dot{y}_{2}=0$, ensuring the outputs remain at zero.\nThe internal dynamics are described by the evolution of $\\eta = x_{4}$ on this manifold. The governing equation is $\\dot{\\eta} = \\dot{x}_{4} = -x_{1} + x_{2}x_{3} + x_{4}^{3}$.\nSubstituting the conditions of the zero dynamics manifold ($x_{1}=0, x_{2}=0, x_{3}=0$) into this equation gives the zero dynamics:\n$\\dot{\\eta} = -0 + (0)(0) + x_{4}^{3} = x_{4}^{3}$.\nIn terms of the internal coordinate $\\eta$, this is $\\dot{\\eta} = \\eta^{3}$.\n\nWe are asked to compute the instantaneous internal dynamics rate $\\dot{\\eta}$ at the state $x = (0, 0, 0, 2)$. This state lies on the zero dynamics manifold. At this point, the internal coordinate has the value $\\eta = x_{4} = 2$.\nSubstituting this value into the zero dynamics equation provides the rate:\n$\\dot{\\eta} = \\eta^{3} = (2)^{3} = 8$.", "answer": "$$\\boxed{8}$$", "id": "2698987"}, {"introduction": "In practical engineering, ideal performance in one area often comes at a cost in another; perfect decoupling, for instance, can lead to excessive control action or sensitivity to noise. This computational exercise [@problem_id:2699011] immerses you in this essential trade-off, tasking you with optimizing a controller to balance decoupling at low frequencies against the amplification of sensor noise at high frequencies. This practice illuminates the compromises inherent in real-world control system design and the role of optimization in finding a suitable balance.", "problem": "Design a program that evaluates approximate decoupling and the associated performance tradeoffs for a square, continuous-time, linear time-invariant multivariable plant under a static precompensator followed by identical diagonal first-order loop filters. The plant is two-input and two-output. The standard negative feedback architecture is used, where the control input is given by $u=K(r-y)$ and the output is $y=G(s)u$, with $r$ the reference input. The plant transfer matrix $G(s)$ is strictly proper and stable, and each entry is a first-order rational function of the form $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$ with $k_{ij}\\in\\mathbb{R}$ and $\\tau_{ij}0$. The decoupling controller is constructed as follows: a static precompensator $W=G(0)^{-1}$ is used to approximately invert the plant at steady state, and identical diagonal loop filters are applied per channel as $F(s)=\\mathrm{diag}\\!\\left(\\dfrac{\\omega_c}{s+\\omega_c},\\dfrac{\\omega_c}{s+\\omega_c}\\right)$, with $\\omega_c0$ the tunable bandwidth parameter. The resulting controller is $K(s)=F(s)W$. The closed-loop map from $r$ to $y$ under unity feedback is $T(s)$, defined by the standard negative feedback relation based on $G(s)$ and $K(s)$. The approximate-decoupling quality is measured at low frequency by the maximum off-diagonal magnitude of $T(\\mathrm{j}\\omega)$ over a prescribed low-frequency band, while the performance penalty is measured by the high-frequency gain of $K(\\mathrm{j}\\omega)$ over a prescribed high-frequency band. The scalarized tradeoff cost to be minimized over $\\omega_c$ is\n$$\nJ_{\\alpha}(\\omega_c)=\\alpha\\,E_{\\mathrm{dec}}(\\omega_c)+(1-\\alpha)\\,E_{\\mathrm{noise}}(\\omega_c),\n$$\nwith $0\\alpha1$.\n\nYour program must implement the following precise definitions and computations.\n\n- Low-frequency decoupling error:\n$$\nE_{\\mathrm{dec}}(\\omega_c)=\\max_{\\omega\\in\\Omega_L}\\left\\|\\mathrm{OffDiag}\\!\\left(T(\\mathrm{j}\\omega)\\right)\\right\\|_F,\n$$\nwhere $\\mathrm{OffDiag}(M)$ is the matrix with the same off-diagonal elements as $M$ and zeros on the diagonal, and $\\|\\cdot\\|_F$ denotes the Frobenius norm. The low-frequency grid is $\\Omega_L=\\{\\omega_{\\ell,m}\\}_{m=1}^{N_L}$, logarithmically spaced from $\\omega_{\\ell,\\min}$ to $\\omega_{\\ell,\\max}$.\n\n- High-frequency noise amplification proxy:\n$$\nE_{\\mathrm{noise}}(\\omega_c)=\\sup_{\\omega\\in\\Omega_H}\\,\\bar{\\sigma}\\!\\left(K(\\mathrm{j}\\omega)\\right),\n$$\nwhere $\\bar{\\sigma}(\\cdot)$ is the largest singular value and $\\Omega_H=\\{\\omega_{h,m}\\}_{m=1}^{N_H}$ is logarithmically spaced from $\\omega_{h,\\min}$ to $\\omega_{h,\\max}$.\n\n- Controller construction details:\n    - Compute $G(0)$ entrywise from $g_{ij}(s)$ and set $W=G(0)^{-1}$.\n    - For each frequency $\\omega$, evaluate $G(\\mathrm{j}\\omega)$ and $F(\\mathrm{j}\\omega)$, then $K(\\mathrm{j}\\omega)=F(\\mathrm{j}\\omega)W$.\n    - The closed-loop map is $T(\\mathrm{j}\\omega)=(I+G(\\mathrm{j}\\omega)K(\\mathrm{j}\\omega))^{-1}G(\\mathrm{j}\\omega)K(\\mathrm{j}\\omega)$.\n\n- Optimization over controller bandwidth:\n    - Search over $\\omega_c$ in the grid $\\Omega_c=\\{\\omega_{c,n}\\}_{n=1}^{N_c}$, logarithmically spaced between $\\omega_{c,\\min}$ and $\\omega_{c,\\max}$.\n    - Choose the minimizing $\\omega_c$ for $J_{\\alpha}(\\omega_c)$; in case of ties within numerical tolerance, select the smallest $\\omega_c$ achieving the minimum.\n\nUse the following globally fixed parameters for all test cases:\n- Tradeoff weight $\\alpha=0.6$.\n- Low-frequency band: $\\omega_{\\ell,\\min}=10^{-3}$ in rad/s and $\\omega_{\\ell,\\max}=1.0$ in rad/s, with $N_L=80$ logarithmically spaced points.\n- High-frequency band: $\\omega_{h,\\min}=20.0$ in rad/s and $\\omega_{h,\\max}=200.0$ in rad/s, with $N_H=80$ logarithmically spaced points.\n- Controller bandwidth search: $\\omega_{c,\\min}=0.1$ in rad/s and $\\omega_{c,\\max}=100.0$ in rad/s, with $N_c=160$ logarithmically spaced points.\n\nTest suite. For each test, the plant is specified by the matrices $K=[k_{ij}]$ and $\\Tau=[\\tau_{ij}]$ that define each entry $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$.\n\n- Test $1$ (well-conditioned steady-state gain with moderate coupling):\n    - $K=\\begin{bmatrix}2.00.3\\\\0.21.2\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}1.00.5\\\\0.70.8\\end{bmatrix}$.\n\n- Test $2$ (nearly singular steady-state gain):\n    - $K=\\begin{bmatrix}1.00.95\\\\0.950.91\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}0.31.2\\\\0.90.4\\end{bmatrix}$.\n\n- Test $3$ (mixed-sign coupling and disparate time constants):\n    - $K=\\begin{bmatrix}1.0-0.8\\\\0.61.1\\end{bmatrix}$,\n      $\\Tau=\\begin{bmatrix}0.50.6\\\\0.41.5\\end{bmatrix}$.\n\nAngle and frequency units must be in radians per second. Your program must compute, for each test, the $\\omega_c$ in rad/s that minimizes $J_{\\alpha}(\\omega_c)$ over the specified grid, and report the three optimal $\\omega_c$ values.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places in rad/s, ordered by tests $1$, $2$, $3$, for example, $[1.234000,5.678000,9.101000]$.", "solution": "The problem presented is valid. It is a well-posed optimization problem in the domain of multivariable control theory, grounded in established principles of decoupling and performance trade-offs. All constants, functions, and metrics are defined with sufficient rigor to permit a unique, verifiable solution. We shall proceed with the design of the required computational procedure.\n\nThe task is to find the optimal controller bandwidth $\\omega_c$ that minimizes a weighted cost function $J_{\\alpha}(\\omega_c)$, which represents a trade-off between low-frequency decoupling performance and high-frequency noise sensitivity for a given two-input, two-output (TITO) LTI plant.\n\nThe plant's transfer function matrix is given by $G(s)$, with entries $g_{ij}(s)=\\dfrac{k_{ij}}{\\tau_{ij}s+1}$. The matrices of gains $K=[k_{ij}]$ and time constants $\\Tau=[\\tau_{ij}]$ are provided for each test case.\n\nThe controller $K(s)$ is composed of a static precompensator $W$ and a diagonal filter matrix $F(s)$, such that $K(s) = F(s)W$.\nThe precompensator $W$ is designed to achieve perfect steady-state decoupling. This is accomplished by setting $W$ to be the inverse of the plant's steady-state gain matrix, $G(0)$. Since $g_{ij}(0) = k_{ij}$, we have $G(0) = K$. Therefore, the static precompensator is $W=K^{-1}$. This requires that $K$ is invertible, which is true for all test cases provided.\n\nThe filter matrix is $F(s)=\\mathrm{diag}\\!\\left(\\dfrac{\\omega_c}{s+\\omega_c},\\dfrac{\\omega_c}{s+\\omega_c}\\right) = \\dfrac{\\omega_c}{s+\\omega_c} I$, where $I$ is the $2 \\times 2$ identity matrix and $\\omega_c  0$ is the tunable controller bandwidth.\n\nThe total cost to be minimized is $J_{\\alpha}(\\omega_c)=\\alpha\\,E_{\\mathrm{dec}}(\\omega_c)+(1-\\alpha)\\,E_{\\mathrm{noise}}(\\omega_c)$ with $\\alpha=0.6$. The optimization is performed by searching over a discrete grid of $\\omega_c$ values, $\\Omega_c$. We now analyze the two components of the cost function.\n\nFirst, the low-frequency decoupling error, $E_{\\mathrm{dec}}(\\omega_c)$, is defined as:\n$$\nE_{\\mathrm{dec}}(\\omega_c)=\\max_{\\omega\\in\\Omega_L}\\left\\|\\mathrm{OffDiag}\\!\\left(T(\\mathrm{j}\\omega)\\right)\\right\\|_F\n$$\nHere, $T(\\mathrm{j}\\omega)$ is the closed-loop transfer function matrix evaluated at frequency $\\omega$. It is given by the standard formula for negative feedback systems:\n$$\nT(s) = (I+L(s))^{-1}L(s)\n$$\nwhere $L(s) = G(s)K(s)$ is the open-loop transfer function. Substituting the expressions for $G(s)$ and $K(s)$, we evaluate this at $s=\\mathrm{j}\\omega$ for each $\\omega$ in the low-frequency grid $\\Omega_L$. For a given matrix $M$, $\\mathrm{OffDiag}(M)$ sets the diagonal elements to zero, and $\\|\\cdot\\|_F$ is the Frobenius norm, calculated as the square root of the sum of the squared magnitudes of the matrix elements. The computation of $E_{\\mathrm{dec}}(\\omega_c)$ requires a numerical sweep across the frequency grid $\\Omega_L$. For a $2 \\times 2$ matrix $T = \\begin{bmatrix} T_{11}  T_{12} \\\\ T_{21}  T_{22} \\end{bmatrix}$, this norm is $\\sqrt{|T_{12}|^2 + |T_{21}|^2}$.\n\nSecond, the high-frequency noise amplification proxy, $E_{\\mathrm{noise}}(\\omega_c)$, is defined as:\n$$\nE_{\\mathrm{noise}}(\\omega_c)=\\sup_{\\omega\\in\\Omega_H}\\,\\bar{\\sigma}\\!\\left(K(\\mathrm{j}\\omega)\\right)\n$$\nwhere $\\bar{\\sigma}(\\cdot)$ denotes the largest singular value. The controller is $K(s) = \\left(\\dfrac{\\omega_c}{s+\\omega_c}\\right)W$. Evaluating at $s=\\mathrm{j}\\omega$, we have $K(\\mathrm{j}\\omega) = \\left(\\dfrac{\\omega_c}{\\mathrm{j}\\omega+\\omega_c}\\right)W$.\nUsing the property of singular values that $\\bar{\\sigma}(cA) = |c|\\bar{\\sigma}(A)$ for a complex scalar $c$ and matrix $A$, we can write:\n$$\n\\bar{\\sigma}(K(\\mathrm{j}\\omega)) = \\left|\\dfrac{\\omega_c}{\\mathrm{j}\\omega+\\omega_c}\\right|\\bar{\\sigma}(W) = \\dfrac{\\omega_c}{\\sqrt{\\omega^2+\\omega_c^2}}\\bar{\\sigma}(W)\n$$\nThe term $\\bar{\\sigma}(W)$ is a constant for a given plant. The function $f(\\omega) = \\dfrac{\\omega_c}{\\sqrt{\\omega^2+\\omega_c^2}}$ is a strictly decreasing function of $\\omega$ for $\\omega  0$. The high-frequency evaluation band is $\\Omega_H = [\\omega_{h,\\min}, \\omega_{h,\\max}]$, where $\\omega_{h,min}  0$. Therefore, the supremum of $f(\\omega)$ over this band occurs at its lowest frequency, $\\omega_{h,min}$. This provides a simplified analytical expression for $E_{\\mathrm{noise}}(\\omega_c)$:\n$$\nE_{\\mathrm{noise}}(\\omega_c) = \\dfrac{\\omega_c}{\\sqrt{\\omega_{h,\\min}^2+\\omega_c^2}}\\bar{\\sigma}(W)\n$$\nThis simplification avoids a numerical sweep over the high-frequency grid $\\Omega_H$ and will be used in the implementation.\n\nThe optimization algorithm proceeds as follows:\n1. For each test case defined by matrices $K$ and $\\Tau$:\n2. Pre-compute the static precompensator $W = K^{-1}$ and its largest singular value $\\bar{\\sigma}(W)$.\n3. Iterate through each candidate bandwidth $\\omega_c$ in the specified grid $\\Omega_c$.\n4. For each $\\omega_c$:\n    a. Calculate $E_{\\mathrm{noise}}(\\omega_c)$ using the simplified analytical formula.\n    b. Calculate $E_{\\mathrm{dec}}(\\omega_c)$ by iterating through each frequency $\\omega$ in the grid $\\Omega_L$, computing $T(\\mathrm{j}\\omega)$, its off-diagonal Frobenius norm, and finding the maximum value.\n    c. Compute the total cost $J_{\\alpha}(\\omega_c) = \\alpha E_{\\mathrm{dec}}(\\omega_c) + (1-\\alpha) E_{\\mathrm{noise}}(\\omega_c)$.\n5. After evaluating the cost for all $\\omega_c \\in \\Omega_c$, find the minimum cost value.\n6. Identify the $\\omega_c$ that produces this minimum cost. If there is a tie (within numerical tolerance), select the smallest $\\omega_c$. This value is the solution for the test case.\n7. Repeat for all test cases and format the results as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multivariable controller optimization problem for three test cases.\n    \"\"\"\n    # Globally fixed parameters\n    ALPHA = 0.6\n    OMEGA_L_MIN, OMEGA_L_MAX, N_L = 1e-3, 1.0, 80\n    OMEGA_H_MIN = 20.0  # OMEGA_H_MAX and N_H are not needed due to analytical simplification\n    OMEGA_C_MIN, OMEGA_C_MAX, N_C = 0.1, 100.0, 160\n\n    # Define frequency grids\n    omega_l_grid = np.logspace(np.log10(OMEGA_L_MIN), np.log10(OMEGA_L_MAX), N_L)\n    omega_c_grid = np.logspace(np.log10(OMEGA_C_MIN), np.log10(OMEGA_C_MAX), N_C)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"K\": np.array([[2.0, 0.3], [0.2, 1.2]]),\n            \"Tau\": np.array([[1.0, 0.5], [0.7, 0.8]]),\n        },\n        {\n            \"K\": np.array([[1.0, 0.95], [0.95, 0.91]]),\n            \"Tau\": np.array([[0.3, 1.2], [0.9, 0.4]]),\n        },\n        {\n            \"K\": np.array([[1.0, -0.8], [0.6, 1.1]]),\n            \"Tau\": np.array([[0.5, 0.6], [0.4, 1.5]]),\n        },\n    ]\n\n    optimal_omegas = []\n\n    for case in test_cases:\n        K_plant = case[\"K\"]\n        Tau_plant = case[\"Tau\"]\n\n        # Step 1: Compute static precompensator W and its largest singular value\n        try:\n            W = np.linalg.inv(K_plant)\n        except np.linalg.LinAlgError:\n            # This case should not be reached with the given valid problems\n            # but is good practice.\n            optimal_omegas.append(np.nan)\n            continue\n        \n        s_bar_W = np.linalg.svd(W, compute_uv=False)[0]\n\n        costs = []\n        # Step 2: Iterate through each candidate bandwidth omega_c\n        for omega_c in omega_c_grid:\n            # Step 2a: Calculate E_noise using the analytical simplification\n            e_noise = (omega_c / np.sqrt(omega_c**2 + OMEGA_H_MIN**2)) * s_bar_W\n\n            # Step 2b: Calculate E_dec by sweeping over the low-frequency grid\n            max_off_diag_norm = 0.0\n            for omega in omega_l_grid:\n                s = 1j * omega\n                \n                # Evaluate plant transfer matrix G(s)\n                G_s = K_plant / (Tau_plant * s + 1)\n                \n                # Evaluate controller K(s) and loop gain L(s)\n                f_scalar = omega_c / (s + omega_c)\n                K_s = f_scalar * W\n                L_s = G_s @ K_s\n                \n                # Evaluate closed-loop transfer function T(s)\n                I = np.eye(2)\n                T_s = np.linalg.inv(I + L_s) @ L_s\n                \n                # Calculate Frobenius norm of off-diagonal part\n                off_diag_T = T_s.copy()\n                off_diag_T[0, 0] = 0\n                off_diag_T[1, 1] = 0\n                current_norm = np.linalg.norm(off_diag_T, 'fro')\n                \n                if current_norm > max_off_diag_norm:\n                    max_off_diag_norm = current_norm\n            \n            e_dec = max_off_diag_norm\n            \n            # Step 2c: Compute total cost\n            j_cost = ALPHA * e_dec + (1 - ALPHA) * e_noise\n            costs.append(j_cost)\n\n        # Step 3: Find the optimal omega_c that minimizes the cost\n        costs_arr = np.array(costs)\n        min_cost = np.min(costs_arr)\n        \n        # Find all indices where cost is close to the minimum (handles numerical ties)\n        min_indices = np.where(np.isclose(costs_arr, min_cost))[0]\n        \n        # Per tie-breaking rule, choose the smallest omega_c, which corresponds to the first index\n        optimal_index = min_indices[0]\n        optimal_omega_c = omega_c_grid[optimal_index]\n        \n        optimal_omegas.append(optimal_omega_c)\n\n    # Final formatting and printing\n    formatted_results = [f\"{val:.6f}\" for val in optimal_omegas]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2699011"}]}