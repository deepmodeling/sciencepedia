{"hands_on_practices": [{"introduction": "At each time step, a Model Predictive Controller solves a constrained optimal control problem, which for linear systems typically takes the form of a Quadratic Program (QP). While numerical solvers handle this efficiently in practice, a deep understanding of MPC requires knowing what happens 'under the hood'. This practice will guide you through solving a simple MPC problem analytically using the fundamental Karush-Kuhn-Tucker (KKT) conditions, revealing precisely how the optimal control action is shaped by the system dynamics, cost function, and active constraints [@problem_id:2724762].", "problem": "Consider a one-dimensional discrete-time linear system used in Model Predictive Control (MPC) with dynamics given by $x_{k+1}=a x_k + b u_k$, where $x_k \\in \\mathbb{R}$ and $u_k \\in \\mathbb{R}$. Let the prediction horizon be $N=2$ with decision variables $u_0$ and $u_1$. The quadratic performance index to be minimized is\n$$J = q x_1^2 + r u_0^2 + p x_2^2 + r u_1^2,$$\nsubject to the dynamics\n$$x_1 = a x_0 + b u_0, \\quad x_2 = a x_1 + b u_1,$$\nand a single affine inequality constraint at the terminal step,\n$$x_2 \\ge \\underline{x},$$\nwith all other constraints absent. Assume that at the optimum this single inequality constraint is active and all other constraints are inactive.\n\nUse the Karush–Kuhn–Tucker (KKT) conditions for convex quadratic programming to derive and solve the resulting system analytically for the optimal first control move $u_0^{\\star}$. You must start from the definitions of the dynamics, the cost, and the active inequality, construct the Lagrangian, write down the stationarity, primal feasibility, dual feasibility, and complementarity conditions, and solve them to obtain $u_0^{\\star}$.\n\nUse the following numerically specified and scientifically consistent parameters: $a=1.2$, $b=1$, $q=1$, $r=0.5$, $p=2$, $x_0=1$, and $\\underline{x}=0.8$.\n\nExpress your final answer as a real number (unitless) rounded to four significant figures.", "solution": "The problem provided is a well-posed, scientifically grounded optimal control problem that can be solved using the Karush–Kuhn–Tucker (KKT) conditions for constrained optimization. The problem is valid and we proceed with the analytical solution.\n\nThe system dynamics are given by the linear difference equation $x_{k+1}=a x_k + b u_k$. With an initial state $x_0$ and a prediction horizon of $N=2$, the states are expressed in terms of the control inputs $u_0$ and $u_1$ as:\n$$x_1 = a x_0 + b u_0$$\n$$x_2 = a x_1 + b u_1 = a(a x_0 + b u_0) + b u_1 = a^2 x_0 + ab u_0 + b u_1$$\n\nThe objective is to minimize the quadratic performance index $J$:\n$$J(u_0, u_1) = q x_1^2 + r u_0^2 + p x_2^2 + r u_1^2$$\nSubstituting the expressions for $x_1$ and $x_2$ in terms of $u_0$ and $u_1$, the cost function becomes:\n$$J(u_0, u_1) = q(a x_0 + b u_0)^2 + r u_0^2 + p(a^2 x_0 + ab u_0 + b u_1)^2 + r u_1^2$$\n\nThe optimization is subject to the terminal state constraint $x_2 \\ge \\underline{x}$, which can be written in the standard form $g(u_0, u_1) \\le 0$ as:\n$$g(u_0, u_1) = \\underline{x} - x_2 = \\underline{x} - (a^2 x_0 + ab u_0 + b u_1) \\le 0$$\n\nThis is a convex quadratic program, as the cost function $J$ is a sum of squared terms with positive coefficients ($q, r, p > 0$), making it strictly convex, and the constraint is affine. We construct the Lagrangian $\\mathcal{L}$ by introducing a Lagrange multiplier $\\lambda$:\n$$\\mathcal{L}(u_0, u_1, \\lambda) = J(u_0, u_1) + \\lambda g(u_0, u_1)$$\n$$\\mathcal{L}(u_0, u_1, \\lambda) = q(a x_0 + b u_0)^2 + r u_0^2 + p(a^2 x_0 + ab u_0 + b u_1)^2 + r u_1^2 + \\lambda(\\underline{x} - (a^2 x_0 + ab u_0 + b u_1))$$\n\nThe KKT conditions for optimality are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the decision variables must be zero.\n    $$\\frac{\\partial \\mathcal{L}}{\\partial u_0} = 2q(a x_0 + b u_0)b + 2r u_0 + 2p(a^2 x_0 + ab u_0 + b u_1)ab - \\lambda ab = 0$$\n    $$\\frac{\\partial \\mathcal{L}}{\\partial u_1} = 2p(a^2 x_0 + ab u_0 + b u_1)b + 2r u_1 - \\lambda b = 0$$\n2.  **Primal Feasibility**: The constraint must be satisfied.\n    $$\\underline{x} - (a^2 x_0 + ab u_0 + b u_1) \\le 0$$\n3.  **Dual Feasibility**: The Lagrange multiplier for an inequality of the form $g \\le 0$ must be non-negative.\n    $$\\lambda \\ge 0$$\n4.  **Complementary Slackness**: The product of the multiplier and the constraint function must be zero.\n    $$\\lambda(\\underline{x} - (a^2 x_0 + ab u_0 + b u_1)) = 0$$\n\nThe problem states that the inequality constraint is active at the optimum. This implies:\n$$x_2 = a^2 x_0 + ab u_0 + b u_1 = \\underline{x}$$\nThis satisfies the primal feasibility condition. It also satisfies the complementary slackness condition, leaving $\\lambda$ to be determined. The stationarity equations can be simplified using $x_2 = \\underline{x}$ and dividing by non-zero factors ($b=1 \\ne 0$).\n\nThe first stationarity equation becomes, after dividing by $2b$:\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - \\frac{\\lambda a}{2} = 0 \\quad (1)$$\nThe second stationarity equation becomes, after dividing by $2b$:\n$$p\\underline{x} + \\frac{r}{b}u_1 - \\frac{\\lambda}{2} = 0 \\quad (2)$$\n\nFrom equation $(2)$, we express $\\frac{\\lambda}{2}$ in terms of $u_1$:\n$$\\frac{\\lambda}{2} = p\\underline{x} + \\frac{r}{b}u_1$$\nWe substitute this expression for $\\frac{\\lambda}{2}$ into equation $(1)$:\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - a\\left(p\\underline{x} + \\frac{r}{b}u_1\\right) = 0$$\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 + pa\\underline{x} - pa\\underline{x} - \\frac{ar}{b}u_1 = 0$$\n$$q(a x_0 + b u_0) + \\frac{r}{b}u_0 - \\frac{ar}{b}u_1 = 0$$\nMultiplying by $b$ and expanding yields:\n$$qabx_0 + qb^2 u_0 + ru_0 - aru_1 = 0$$\n$$u_0(qb^2+r) + qabx_0 - aru_1 = 0 \\quad (A)$$\n\nWe now have a system of two linear equations in the two unknowns $u_0$ and $u_1$:\n$$(A): \\quad u_0(qb^2+r) + qabx_0 - aru_1 = 0$$\n$$(B): \\quad a^2 x_0 + ab u_0 + b u_1 = \\underline{x}$$\nFrom $(B)$, we solve for $u_1$:\n$$b u_1 = \\underline{x} - a^2 x_0 - ab u_0 \\implies u_1 = \\frac{1}{b}(\\underline{x} - a^2 x_0 - ab u_0)$$\nSubstitute this into $(A)$:\n$$u_0(qb^2+r) + qabx_0 - ar \\left(\\frac{1}{b}(\\underline{x} - a^2 x_0 - ab u_0)\\right) = 0$$\n$$u_0(qb^2+r) + qabx_0 - \\frac{ar}{b}(\\underline{x} - a^2 x_0) + a^2 r u_0 = 0$$\nGroup terms containing $u_0$:\n$$u_0(qb^2+r+a^2r) = \\frac{ar}{b}(\\underline{x} - a^2 x_0) - qabx_0$$\nFinally, we solve for the optimal first control move, $u_0^{\\star}$:\n$$u_0^{\\star} = \\frac{\\frac{ar}{b}(\\underline{x} - a^2 x_0) - qabx_0}{qb^2 + r + a^2 r}$$\n\nNow we substitute the given numerical values: $a=1.2$, $b=1$, $q=1$, $r=0.5$, $x_0=1$, and $\\underline{x}=0.8$.\nThe numerator is:\n$$\\frac{(1.2)(0.5)}{1}(0.8 - (1.2)^2(1)) - (1)(1.2)(1)(1) = 0.6(0.8 - 1.44) - 1.2 = 0.6(-0.64) - 1.2 = -0.384 - 1.2 = -1.584$$\nThe denominator is:\n$$(1)(1)^2 + 0.5 + (1.2)^2(0.5) = 1 + 0.5 + (1.44)(0.5) = 1.5 + 0.72 = 2.22$$\nThus, the optimal control input is:\n$$u_0^{\\star} = \\frac{-1.584}{2.22} \\approx -0.7135135...$$\nRounding to four significant figures, we obtain $u_0^{\\star} = -0.7135$.\n\nAs a consistency check, we can compute the value of $\\lambda$.\n$u_0^{\\star} \\approx -0.7135135$\n$u_1^{\\star} = \\frac{1}{1}(0.8 - (1.2)^2(1) - (1.2)(1)u_0^{\\star}) = -0.64 - 1.2(-0.7135135) \\approx 0.2162162$\n$\\lambda^{\\star} = 2\\left(p\\underline{x} + \\frac{r}{b}u_1^{\\star}\\right) = 2\\left((2)(0.8) + \\frac{0.5}{1}(0.2162162)\\right) = 2(1.6 + 0.1081081) \\approx 3.416$\nSince $\\lambda^{\\star} > 0$, the dual feasibility condition is met, confirming the correctness of assuming the constraint is active.", "answer": "$$\\boxed{-0.7135}$$", "id": "2724762"}, {"introduction": "A key challenge in constrained control is ensuring recursive feasibility—the guarantee that the optimization problem remains solvable at all future time steps. A standard technique in MPC is to impose a terminal constraint, requiring the final predicted state to lie within a specially designed terminal set. This practice focuses on the computation of the maximal control invariant set, a fundamental object in constrained control that serves as an ideal candidate for such a terminal set, thereby providing guarantees of stability and feasibility [@problem_id:2724632].", "problem": "Consider the discrete-time linear time-invariant system given by the state-update equation $x^{+} = A x + B u$, where $x \\in \\mathbb{R}^{n}$ is the state and $u \\in \\mathbb{R}^{m}$ is the control input. A set $\\mathcal{C} \\subset \\mathbb{R}^{n}$ is called a controlled invariant set if for every $x \\in \\mathcal{C}$ there exists an input $u \\in \\mathcal{U}$ such that the successor state satisfies $A x + B u \\in \\mathcal{C}$ while also respecting state constraints $x \\in \\mathcal{X}$. The one-step preimage operator for a target set $\\mathcal{S} \\subset \\mathbb{R}^{n}$ under polytopic input constraints $\\mathcal{U} \\subset \\mathbb{R}^{m}$ is defined as $\\operatorname{Pre}(\\mathcal{S}) = \\{ x \\in \\mathbb{R}^{n} \\mid \\exists u \\in \\mathcal{U} \\ \\text{s.t.} \\ A x + B u \\in \\mathcal{S} \\}$. When $\\mathcal{S}$ is polyhedral in half-space (H-) representation, the set $\\operatorname{Pre}(\\mathcal{S})$ can be computed exactly by projecting, onto the $x$-space, the polyhedron in the joint variables $(x,u)$ that encodes $A x + B u \\in \\mathcal{S}$ and $u \\in \\mathcal{U}$. The maximal controlled invariant subset of $\\mathcal{X}$ relative to $\\mathcal{U}$ can be obtained by iterating the one-step preimage operator and intersecting with $\\mathcal{X}$, starting from $\\mathcal{K}^{0} = \\mathcal{X}$, via $\\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X}$, until convergence.\n\nYour task is to implement a program that, for each of the following test cases, computes the polyhedral set sequence $\\{ \\mathcal{K}^{k} \\}_{k \\ge 0}$ as above, by representing each set in H-representation and computing $\\operatorname{Pre}(\\cdot)$ exactly using Fourier–Motzkin elimination to eliminate the input variables $u$. At each iteration, represent $\\mathcal{K}^{k}$ in H-representation, compute $\\mathcal{K}^{k+1}$ exactly, simplify by removing redundant inequalities, and stop when the H-representation is unchanged up to an absolute tolerance $\\varepsilon = 10^{-8}$ (after normalizing inequality rows by a positive scalar to unit Euclidean norm), or after at most $N_{\\max}$ iterations, whichever occurs first. If the set becomes empty at any iteration, treat all subsequent iterates as empty. For the final iterate $\\mathcal{K}^{\\star}$ returned by this termination rule, compute the area of $\\mathcal{K}^{\\star}$ in $\\mathbb{R}^{2}$ by enumerating all vertices of its polygon (if fewer than three vertices exist or the set is empty, the area is zero). Express the area as a real number rounded to five decimal places.\n\nAll sets are polytopes defined by linear inequalities. The state constraint set is given by a hyper-rectangle $\\mathcal{X} = \\{ x \\in \\mathbb{R}^{2} \\mid \\|x\\|_{\\infty} \\le r \\}$ with potentially different bounds per coordinate, encoded by $G x \\le g$ with $G = \\begin{bmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\end{bmatrix}$ and $g = \\begin{bmatrix} r_{1} \\\\ r_{1} \\\\ r_{2} \\\\ r_{2} \\end{bmatrix}$. The input constraint set is a one-dimensional interval $\\mathcal{U} = \\{ u \\in \\mathbb{R} \\mid |u| \\le u_{\\max} \\}$ encoded by $H_{u} u \\le h_{u}$ with $H_{u} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and $h_{u} = \\begin{bmatrix} u_{\\max} \\\\ u_{\\max} \\end{bmatrix}$. The system data and bounds for the test suite are:\n\n- Test case $1$ (happy path):\n  - $A = \\begin{bmatrix} 0.9  0.1 \\\\ 0.0  0.95 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.0 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 2.0$, $r_{2} = 2.0$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.5$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\n- Test case $2$ (boundary where invariance may collapse):\n  - $A = \\begin{bmatrix} 1.2  0.0 \\\\ 0.0  1.1 \\end{bmatrix}$, $B = \\begin{bmatrix} 0.1 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 0.2$, $r_{2} = 0.2$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.05$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\n- Test case $3$ (coupled dynamics):\n  - $A = \\begin{bmatrix} 0.8  0.3 \\\\ -0.2  0.9 \\end{bmatrix}$, $B = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\mathcal{X}$ with bounds $r_{1} = 1.5$, $r_{2} = 1.5$,\n  - $\\mathcal{U}$ with $u_{\\max} = 0.7$,\n  - $N_{\\max} = 25$, $\\varepsilon = 10^{-8}$.\n\nImplementation requirements and definitions:\n\n- Represent any polyhedron $\\{ z \\mid H z \\le h \\}$ by the pair $(H,h)$, where $H \\in \\mathbb{R}^{p \\times d}$, $h \\in \\mathbb{R}^{p}$, $p \\in \\mathbb{N}$, $d \\in \\mathbb{N}$. Intersection corresponds to stacking inequalities.\n- The exact one-step preimage $\\operatorname{Pre}(\\mathcal{S})$ for $\\mathcal{S} = \\{ x \\mid H_{S} x \\le h_{S} \\}$ under $u \\in \\mathcal{U}$ is the projection onto $x$ of $\\{ (x,u) \\mid H_{S} (A x + B u) \\le h_{S}, \\ H_{u} u \\le h_{u} \\}$. Compute this projection by Fourier–Motzkin elimination eliminating $u$ exactly. For a single scalar $u$, if an inequality has coefficient $\\beta$ on $u$, classify it as positive if $\\beta  0$, negative if $\\beta  0$, or zero if $\\beta = 0$. The projected inequalities on $x$ are all zero-$\\beta$ inequalities (with $u$ removed) together with all pairwise combinations of one positive and one negative inequality, yielding $(\\alpha^{+}/\\beta^{+} - \\alpha^{-}/\\beta^{-})^{\\top} x \\le d^{+}/\\beta^{+} - d^{-}/\\beta^{-}$, where $\\alpha^{\\pm}$ are the $x$-coefficients and $d^{\\pm}$ the right-hand sides.\n- After each preimage and intersection, remove redundant inequalities: an inequality $h_{i}^{\\top} x \\le b_{i}$ is redundant if the optimal value of $\\max \\{ h_{i}^{\\top} x \\mid H x \\le h, \\ (h_{i}, b_{i}) \\ \\text{removed} \\}$ is less than or equal to $b_{i}$ up to $\\varepsilon$.\n- Detect emptiness by solving a linear program for feasibility. If the set is empty, its area is defined as $0$.\n- Compute the polygon area of $\\mathcal{K}^{\\star}$ by enumerating all vertices as pairwise intersections of active constraints (in $\\mathbb{R}^{2}$), discarding those that violate any inequality, ordering the unique feasible vertices by polar angle about their centroid, and applying the shoelace formula. If fewer than three vertices exist, the area is $0$.\n\nYour program should run the three test cases above in order, compute the area of the final iterate $\\mathcal{K}^{\\star}$ for each, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[a\\_1,a\\_2,a\\_3]$), where each $a\\_i$ is the area rounded to five decimal places.", "solution": "The problem posed is to compute the maximal controlled invariant subset of a given state constraint set $\\mathcal{X}$ for a discrete-time linear time-invariant (LTI) system. This is a fundamental problem in control theory, particularly in the synthesis of controllers for constrained systems and in the domain of Model Predictive Control (MPC), where such sets often serve as terminal sets to guarantee stability and recursive feasibility. The solution requires a sequence of polyhedral computations.\n\nThe system is described by the state-update equation:\n$$ x^{+} = A x + B u $$\nwhere $x \\in \\mathbb{R}^{n}$ is the state vector and $u \\in \\mathbb{R}^{m}$ is the control input vector. For this problem, $n=2$ and $m=1$. The state and control inputs are subject to polytopic constraints: $x \\in \\mathcal{X}$ and $u \\in \\mathcal{U}$.\n\nThe algorithm to compute the maximal controlled invariant set, denoted $\\mathcal{K}^{\\star}$, is an iterative procedure based on the one-step preimage operator, $\\operatorname{Pre}(\\cdot)$. Starting with an initial set $\\mathcal{K}^{0} = \\mathcal{X}$, we generate a sequence of sets $\\{ \\mathcal{K}^{k} \\}_{k \\ge 0}$ using the recursion:\n$$ \\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X} $$\nThis sequence is guaranteed to be monotonically non-increasing, $\\mathcal{K}^{k+1} \\subseteq \\mathcal{K}^{k}$, and converges to the maximal controlled invariant subset of $\\mathcal{X}$.\n\nAll sets are represented as polyhedra in half-space (H-) representation, $\\{ z \\mid H z \\le h \\}$. The core of each iteration involves three main steps: preimage calculation, intersection, and simplification.\n\nStep 1: Preimage Calculation\nGiven a polyhedral set $\\mathcal{K}^{k} = \\{ x \\in \\mathbb{R}^{n} \\mid H_{k} x \\le h_{k} \\}$, the preimage $\\operatorname{Pre}(\\mathcal{K}^{k})$ is the set of all states $x$ for which there exists an admissible control $u \\in \\mathcal{U}$ such that the successor state $x^{+} = A x + B u$ lies in $\\mathcal{K}^{k}$. Formally:\n$$ \\operatorname{Pre}(\\mathcal{K}^{k}) = \\{ x \\in \\mathbb{R}^{n} \\mid \\exists u \\in \\mathcal{U} \\text{ s.t. } A x + B u \\in \\mathcal{K}^{k} \\} $$\nWith polytopic constraints $\\mathcal{U} = \\{ u \\in \\mathbb{R}^{m} \\mid H_{u} u \\le h_{u} \\}$, this can be computed exactly. The conditions define a polyhedron in the joint space of $(x, u)$:\n$$ H_{k} (A x + B u) \\le h_{k} $$\n$$ H_{u} u \\le h_{u} $$\nThe first set of inequalities can be rewritten as $(H_{k}A)x + (H_{k}B)u \\le h_{k}$. To obtain $\\operatorname{Pre}(\\mathcal{K}^{k})$, we must project this joint polyhedron onto the $x$-space by eliminating the variables $u$. For this problem, with a single input $u \\in \\mathbb{R}$, this projection is performed using Fourier-Motzkin elimination.\n\nLet the set of all inequalities involving $u$ be written as $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i}$ for $i=1, \\dots, p$. These inequalities are partitioned based on the sign of the scalar coefficient $\\beta_{i}$:\n1.  Positive coefficient ($\\beta_{i} > 0$): $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i} \\implies u \\le (d_{i} - \\alpha_{i}^{\\top} x) / \\beta_{i}$. Let this be a \"positive\" inequality, denoted by superscripts $(+)$.\n2.  Negative coefficient ($\\beta_{i}  0$): $\\alpha_{i}^{\\top} x + \\beta_{i} u \\le d_{i} \\implies u \\ge (d_{i} - \\alpha_{i}^{\\top} x) / \\beta_{i}$. Let this be a \"negative\" inequality, denoted by superscripts $(-)$.\n3.  Zero coefficient ($\\beta_{i} = 0$): $\\alpha_{i}^{\\top} x \\le d_{i}$. These inequalities are already in the $x$-space and are part of the projection.\n\nFor a value of $u$ to exist, every lower bound on $u$ must be less than or equal to every upper bound. Combining each negative inequality with each positive one yields a new set of inequalities in $x$:\n$$ \\frac{d^{-} - (\\alpha^{-})^{\\top} x}{\\beta^{-}} \\le \\frac{d^{+} - (\\alpha^{+})^{\\top} x}{\\beta^{+}} $$\nRearranging this gives the formula specified in the problem statement:\n$$ \\left( \\frac{\\alpha^{+}}{\\beta^{+}} - \\frac{\\alpha^{-}}{\\beta^{-}} \\right)^{\\top} x \\le \\frac{d^{+}}{\\beta^{+}} - \\frac{d^{-}}{\\beta^{-}} $$\nThe resulting set $\\operatorname{Pre}(\\mathcal{K}^{k})$ is described by the collection of all such derived inequalities and the original zero-coefficient inequalities.\n\nStep 2: Intersection and Simplification\nThe intersection $\\mathcal{K}^{k+1} = \\operatorname{Pre}(\\mathcal{K}^{k}) \\cap \\mathcal{X}$ is straightforward in H-representation: the matrices and vectors defining the inequalities of $\\operatorname{Pre}(\\mathcal{K}^{k})$ and $\\mathcal{X}$ are vertically concatenated. This operation, however, introduces many redundant inequalities. To maintain a minimal representation and for the convergence check to be reliable, these redundancies must be removed.\n\nAn inequality $H_{i} x \\le h_{i}$ from a system $H x \\le h$ is redundant if it does not define a facet of the polyhedron. This is tested by solving a linear program (LP):\n$$ v^{\\star} = \\max_{x} \\left\\{ H_{i} x \\mid H_{j} x \\le h_{j} \\text{ for all } j \\neq i \\right\\} $$\nIf $v^{\\star} \\le h_{i} + \\varepsilon$ for a small tolerance $\\varepsilon$, the inequality is redundant and can be removed. Before this, a feasibility LP is solved to check if the set $\\mathcal{K}^{k+1}$ is empty. If it is, the process terminates for this branch, and all subsequent iterates are considered empty.\n\nStep 3: Termination\nThe iterative process is terminated under one of two conditions:\n1.  Convergence: The H-representation of $\\mathcal{K}^{k+1}$ is equivalent to that of $\\mathcal{K}^{k}$. To check this, the non-redundant H-representations $(H_{k}, h_{k})$ and $(H_{k+1}, h_{k+1})$ are converted to a canonical form. This involves normalizing each inequality row (e.g., making the normal vector $H_{i}$ a unit vector) and then sorting the rows lexicographically. The canonical forms are then compared element-wise up to a tolerance $\\varepsilon$.\n2.  Maximum Iterations: The number of iterations reaches a predefined limit $N_{\\max}$.\n\nFinal Step: Area Calculation\nUpon termination, the final iterate $\\mathcal{K}^{\\star}$ is represented by $(H^{\\star}, h^{\\star})$. To compute its area in $\\mathbb{R}^{2}$, we first enumerate its vertices. A vertex is the unique intersection of two boundary-defining hyperplanes, i.e., the solution to a system $H^{\\star}_{\\{i,j\\}, :} x = h^{\\star}_{\\{i,j\\}}$ for two rows $i,j$, provided this solution is feasible with respect to all other inequalities in the system. After finding all unique, feasible vertices, they are sorted by polar angle around their centroid. The area of the resulting polygon is then calculated using the shoelace formula. If the set is empty or has fewer than three vertices, its area is $0$.\n\nThe implementation proceeds by meticulously following these steps for each test case provided. The Python code below implements this logic.\n\n```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve_problem_2():\n    \"\"\"\n    Main function to run all test cases and print the results for problem 2.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[0.9, 0.1], [0.0, 0.95]]),\n            \"B\": np.array([[1.0], [0.2]]),\n            \"r1\": 2.0,\n            \"r2\": 2.0,\n            \"umax\": 0.5,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"A\": np.array([[1.2, 0.0], [0.0, 1.1]]),\n            \"B\": np.array([[0.1], [0.0]]),\n            \"r1\": 0.2,\n            \"r2\": 0.2,\n            \"umax\": 0.05,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"A\": np.array([[0.8, 0.3], [-0.2, 0.9]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"r1\": 1.5,\n            \"r2\": 1.5,\n            \"umax\": 0.7,\n            \"Nmax\": 25,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        area = run_iteration_scheme(params)\n        results.append(f\"{area:.5f}\")\n\n    return f\"[{','.join(results)}]\"\n\ndef run_iteration_scheme(params):\n    \"\"\"\n    Computes the maximal controlled invariant set and its area for a given test case.\n    \"\"\"\n    A, B, r1, r2, umax, Nmax, epsilon = (\n        params[\"A\"], params[\"B\"], params[\"r1\"], params[\"r2\"],\n        params[\"umax\"], params[\"Nmax\"], params[\"epsilon\"]\n    )\n    n, m = A.shape[1], B.shape[1]\n\n    # State constraints X = {x | Gx = g}\n    G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n    g = np.array([r1, r1, r2, r2])\n\n    # Input constraints U = {u | Hu*u = hu}\n    Hu = np.array([[1], [-1]])\n    hu = np.array([umax, umax])\n\n    H_k_prev, h_k_prev = None, None\n    H_k, h_k = G, g\n\n    H_k, h_k, is_empty = remove_redundant_inequalities(H_k, h_k, n, epsilon)\n    if is_empty:\n        return 0.0\n\n    for _ in range(Nmax):\n        H_k_prev, h_k_prev = H_k, h_k\n\n        # Step 1: Compute Pre(K^k)\n        H_pre, h_pre = fourier_motzkin_preimage(H_k, h_k, A, B, Hu, hu, n, m, epsilon)\n\n        # Step 2: Intersect with X\n        H_k1 = np.vstack([H_pre, G])\n        h_k1 = np.hstack([h_pre, g])\n\n        # Step 3: Simplify and check for emptiness\n        H_k, h_k, is_empty = remove_redundant_inequalities(H_k1, h_k1, n, epsilon)\n\n        if is_empty:\n            return 0.0\n\n        # Step 4: Check for convergence\n        if are_sets_equal(H_k, h_k, H_k_prev, h_k_prev, epsilon):\n            break\n\n    return calculate_area(H_k, h_k, n, epsilon)\n\ndef fourier_motzkin_preimage(H_k, h_k, A, B, Hu, hu, n, m, epsilon):\n    \"\"\"\n    Computes the preimage of a polytope using Fourier-Motzkin elimination.\n    \"\"\"\n    # System inequalities: H_k*(A*x + B*u) = h_k  = (H_k*A)*x + (H_k*B)*u = h_k\n    Hk_A = H_k @ A\n    Hk_B = (H_k @ B).flatten()\n\n    # Input inequalities: Hu*u = hu\n    Hu_x = np.zeros((Hu.shape[0], n))\n\n    # Combine all inequalities involving u\n    alpha_coeffs = np.vstack([Hk_A, Hu_x])\n    beta_coeffs = np.hstack([Hk_B, Hu.flatten()])\n    d_rhs = np.hstack([h_k, hu])\n\n    # Partition inequalities based on sign of beta\n    pos_indices = np.where(beta_coeffs > epsilon)[0]\n    neg_indices = np.where(beta_coeffs  -epsilon)[0]\n    zero_indices = np.where(np.abs(beta_coeffs) = epsilon)[0]\n\n    new_inequalities = []\n    # Add zero-coefficient inequalities\n    if len(zero_indices) > 0:\n        new_inequalities.append((alpha_coeffs[zero_indices], d_rhs[zero_indices]))\n\n    # Add cross-combinations of positive and negative inequalities\n    for p_idx in pos_indices:\n        for n_idx in neg_indices:\n            alpha_p, beta_p, d_p = alpha_coeffs[p_idx], beta_coeffs[p_idx], d_rhs[p_idx]\n            alpha_n, beta_n, d_n = alpha_coeffs[n_idx], beta_coeffs[n_idx], d_rhs[n_idx]\n            \n            # (alpha_p/beta_p - alpha_n/beta_n) * x = d_p/beta_p - d_n / beta_n\n            new_H_row = alpha_p / beta_p - alpha_n / beta_n\n            new_h_val = d_p / beta_p - d_n / beta_n\n            new_inequalities.append((new_H_row.reshape(1, -1), np.array([new_h_val])))\n\n    if not new_inequalities:\n        return np.zeros((0, n)), np.zeros(0)\n\n    H_pre = np.vstack([ineq[0] for ineq in new_inequalities])\n    h_pre = np.concatenate([ineq[1] for ineq in new_inequalities])\n    return H_pre, h_pre\n\ndef remove_redundant_inequalities(H, h, n, epsilon):\n    \"\"\"\n    Removes redundant inequalities from an H-representation {x | Hx = h}.\n    Also returns a flag indicating if the set is empty.\n    \"\"\"\n    if H.shape[0] == 0:\n        return H, h, False\n\n    res = linprog(c=np.zeros(n), A_ub=H, b_ub=h, bounds=(None, None), method='highs-ds')\n    if res.status == 2:\n        return np.zeros((0, n)), np.zeros(0), True\n    \n    non_redundant_indices = []\n    for i in range(H.shape[0]):\n        c = -H[i]\n        A_ub = np.delete(H, i, axis=0)\n        b_ub = np.delete(h, i, axis=0)\n        \n        if A_ub.shape[0] == 0:\n            non_redundant_indices.append(i)\n            continue\n            \n        res_feasibility = linprog(c=np.zeros(n), A_ub=A_ub, b_ub=b_ub, bounds=(None, None), method='highs-ds')\n        if res_feasibility.status == 2:\n            non_redundant_indices.append(i)\n            continue\n        \n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(None, None), method='highs-ds')\n        \n        is_redundant = False\n        if res.success:\n            max_val = -res.fun\n            if max_val = h[i] + epsilon:\n                is_redundant = True\n\n        if not is_redundant:\n            non_redundant_indices.append(i)\n\n    return H[non_redundant_indices], h[non_redundant_indices], False\n\n\ndef get_canonical_representation(H, h, epsilon):\n    \"\"\"\n    Normalizes and sorts an H-representation to get a canonical form.\n    \"\"\"\n    if H.shape[0] == 0:\n        return H, h\n\n    norms = np.linalg.norm(H, axis=1)\n    valid_rows = norms > epsilon\n    H_norm = H[valid_rows] / norms[valid_rows, np.newaxis]\n    h_norm = h[valid_rows] / norms[valid_rows]\n    \n    combined = np.hstack([H_norm, h_norm[:, np.newaxis]])\n    sorted_indices = np.lexsort(combined.T[::-1])\n    return H_norm[sorted_indices], h_norm[sorted_indices]\n\n\ndef are_sets_equal(H1, h1, H2, h2, epsilon):\n    \"\"\"\n    Checks if two H-representations define the same set.\n    \"\"\"\n    if H1 is None or H2 is None or H1.shape[0] == 0 or H2.shape[0] == 0:\n        return False\n    if H1.shape != H2.shape:\n        return False\n\n    H1_canon, h1_canon = get_canonical_representation(H1, h1, epsilon)\n    H2_canon, h2_canon = get_canonical_representation(H2, h2, epsilon)\n    \n    if H1_canon.shape != H2_canon.shape:\n        return False\n\n    return np.allclose(H1_canon, H2_canon, atol=epsilon) and \\\n           np.allclose(h1_canon, h2_canon, atol=epsilon)\n\n\ndef calculate_area(H, h, n, epsilon):\n    \"\"\"\n    Calculates the area of a 2D polygon defined by Hx = h.\n    \"\"\"\n    if H.shape[0]  n or n != 2:\n        return 0.0\n\n    vertices = []\n    num_constraints = H.shape[0]\n    for i in range(num_constraints):\n        for j in range(i + 1, num_constraints):\n            H_sub = H[[i, j], :]\n            if abs(np.linalg.det(H_sub))  epsilon:\n                continue\n\n            try:\n                vertex = np.linalg.solve(H_sub, h[[i, j]])\n            except np.linalg.LinAlgError:\n                continue\n            \n            if np.all(H @ vertex = h + epsilon):\n                is_duplicate = False\n                for v in vertices:\n                    if np.linalg.norm(vertex - v)  epsilon:\n                        is_duplicate = True\n                        break\n                if not is_duplicate:\n                    vertices.append(vertex)\n\n    if len(vertices)  3:\n        return 0.0\n    \n    vertices = np.array(vertices)\n    \n    centroid = np.mean(vertices, axis=0)\n    angles = np.arctan2(vertices[:, 1] - centroid[1], vertices[:, 0] - centroid[0])\n    sorted_indices = np.argsort(angles)\n    sorted_vertices = vertices[sorted_indices]\n\n    x = sorted_vertices[:, 0]\n    y = sorted_vertices[:, 1]\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n    \n    return area\n```", "answer": "[15.16104,0.00000,8.49079]", "id": "2724632"}, {"introduction": "While terminal constraints provide theoretical guarantees, practical MPC implementations must be robust to unexpected events like large disturbances or model-plant mismatch, which can render the optimization problem infeasible. This exercise introduces the concept of feasibility restoration through soft constraints, where slack variables allow for temporary, penalized violations of state limits. By implementing a soft-constrained MPC, you will investigate how this powerful technique ensures the controller always finds a solution and explore the dynamics of recovering from constraint violations [@problem_id:2724817].", "problem": "Consider the discrete-time, single-input, single-output linear time-invariant system defined by the state update equation $x_{k+1} = a x_k + b u_k$ with $a = 1$ and $b = 1$. The hard state constraint is $|x_k| \\le x_{\\max}$ with $x_{\\max} = 2$, and the hard input constraint is $u_k \\in \\mathcal{U}_d$, where the finite, discrete input set is $\\mathcal{U}_d = \\{-u_{\\max}, -\\tfrac{u_{\\max}}{2}, 0, \\tfrac{u_{\\max}}{2}, u_{\\max}\\}$ with $u_{\\max} = 1$. You will implement a finite-horizon Model Predictive Control (MPC) law by solving, at each time step, an open-loop optimal control problem over a prediction horizon of length $N = 4$ that uses feasibility restoration by adding slack variables to the state constraints.\n\nFormulate the soft-constrained MPC problem as follows. For a given current state $x_0$, decision variables are the input sequence $(u_0,\\dots,u_{N-1}) \\in \\mathcal{U}_d^N$ and the nonnegative slack sequence $(s_1,\\dots,s_N)$ with $s_i \\ge 0$. The predicted states $(x_1,\\dots,x_N)$ evolve according to $x_{i+1} = a x_i + b u_i$ for $i \\in \\{0,\\dots,N-1\\}$. The slack-augmented state constraints are $|x_i| \\le x_{\\max} + s_i$ for $i \\in \\{1,\\dots,N\\}$. The stage and terminal cost uses weights $q = 0$, $r = 0.1$, $p = 0$, and an $\\ell_1$-penalty on slack with weight $\\rho  0$, so the finite-horizon cost is\n$$\nJ(x_0; u_{0:N-1}, s_{1:N}) = \\sum_{i=0}^{N-1} \\left( q x_i^2 + r u_i^2 + \\rho s_{i+1} \\right) + p x_N^2.\n$$\nBecause $q = 0$ and $p = 0$, the cost simplifies to the input-effort and slack-penalty sum. At each time step, apply the first element $u_0^\\star$ of an optimal sequence to the plant, update the state via $x^+ = a x + b u_0^\\star$, and repeat with a shifted horizon (receding-horizon implementation).\n\nYour program must implement the following, starting strictly from the above fundamental definitions and without using any continuous optimization shortcuts:\n\n- Use exhaustive enumeration over the discrete set $\\mathcal{U}_d$ to solve the open-loop problem exactly at each time step. For each candidate input sequence $(u_0,\\dots,u_{N-1})$, compute the predicted trajectory $(x_1,\\dots,x_N)$ and the minimal slack values defined by $s_i^\\star = \\max\\{0, |x_i| - x_{\\max}\\}$ for $i \\in \\{1,\\dots,N\\}$, which are optimal for the $\\ell_1$-penalized feasibility restoration. Evaluate the total cost $J$ and select an optimal sequence; if multiple sequences have equal cost, select the one with minimal $\\sum_{i=1}^N s_i^\\star$ as a deterministic tie-breaker.\n\n- Execute this receding-horizon procedure for a finite closed-loop time $T$ (specified per test case below), starting from an initial state $x_{\\mathrm{init}}$. Record:\n  1. The realized slack sequence $\\sigma_k = \\max\\{0, |x_k| - x_{\\max}\\}$ for $k \\in \\{0,1,\\dots,T\\}$.\n  2. At each time step $k$, whether the optimal prediction satisfies zero slack on the prediction horizon, i.e., whether $s_i^\\star = 0$ for all $i \\in \\{1,\\dots,N\\}$.\n\n- Analyze recursive feasibility under feasibility restoration as follows and return three boolean indicators per test case:\n  1. always_feasible: Return true if, at every time step $k \\in \\{0,\\dots,T-1\\}$, the open-loop problem admits at least one feasible solution with finite cost under the slack-augmented constraints. Under the given construction, this should hold if the algorithm is implemented as specified.\n  2. zero_slack_invariant: Return true if, whenever at some time step $k$ the optimal predicted slack sequence satisfies $s_i^\\star = 0$ for all $i \\in \\{1,\\dots,N\\}$, the one-step shifted candidate input sequence $(u_1^\\star,\\dots,u_{N-1}^\\star,0)$ yields a predicted state sequence at time $k+1$ that also satisfies $|x_i| \\le x_{\\max}$ for all indices on the new horizon (including the appended terminal prediction with terminal input $0$). If no time step with zero predicted slack occurs, define this property to be vacuously true.\n  3. eventual_zero_slack: Return true if there exists an index $\\bar{k} \\in \\{0,\\dots,T\\}$ such that $\\sigma_k = 0$ for all $k \\in \\{\\bar{k}, \\bar{k}+1, \\dots, T\\}$.\n\nYour implementation must use $a = 1$, $b = 1$, $x_{\\max} = 2$, $u_{\\max} = 1$, $N = 4$, $q = 0$, $r = 0.1$, $p = 0$, and $\\mathcal{U}_d = \\{-1, -0.5, 0, 0.5, 1\\}$.\n\nTest Suite:\n- Case A (happy path with strictly feasible start and strong slack penalty): $x_{\\mathrm{init}} = 1.0$, $\\rho = 100.0$, $T = 8$.\n- Case B (boundary-violating start but moderate slack penalty that prompts correction): $x_{\\mathrm{init}} = 2.5$, $\\rho = 10.0$, $T = 6$.\n- Case C (large violation with very weak slack penalty that discourages actuation): $x_{\\mathrm{init}} = 5.0$, $\\rho = 0.01$, $T = 6$.\n- Case D (exactly on the boundary): $x_{\\mathrm{init}} = 2.0$, $\\rho = 1.0$, $T = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the three booleans for each test case in order A, B, C, D, with each triple enclosed in square brackets and the entire list enclosed in square brackets. For example, the output format must be like [[True,False,True],[...],...]. No additional text must be printed. No physical units are involved, and no angles are used.", "solution": "The problem presented is a valid and well-posed exercise in Model Predictive Control (MPC). It concerns the control of a discrete-time linear time-invariant (LTI) system, specifically a discrete integrator, subject to state and input constraints. The problem requires the implementation of a receding-horizon controller that utilizes soft constraints through slack variables and solves the underlying finite-horizon optimal control problem by exhaustive enumeration over a discrete input set. The analysis focuses on properties of feasibility and constraint satisfaction.\n\nThe system dynamics are given by the state update equation:\n$$\nx_{k+1} = a x_k + b u_k\n$$\nwith parameters $a=1$ and $b=1$. The state $x_k$ must satisfy the hard constraint $|x_k| \\le x_{\\max}$, where $x_{\\max}=2$. The control input $u_k$ is restricted to a finite, discrete set $\\mathcal{U}_d = \\{-1, -0.5, 0, 0.5, 1\\}$.\n\nAt each time step $k$, given the current state $x_k$, we solve an open-loop optimal control problem over a prediction horizon of length $N=4$. The decision variables are the input sequence $u_{0:N-1} = (u_0, u_1, u_2, u_3)$ where each $u_i \\in \\mathcal{U}_d$, and a sequence of non-negative slack variables $s_{1:N} = (s_1, s_2, s_3, s_4)$. The predicted state trajectory, denoted by $x_i^{pred}$, evolves from the initial condition $x_0^{pred} = x_k$ according to $x_{i+1}^{pred} = x_i^{pred} + u_i$.\n\nThe state constraints are softened using the slack variables:\n$$\n|x_i^{pred}| \\le x_{\\max} + s_i \\quad \\text{for } i \\in \\{1, \\dots, N\\}\n$$\nThe objective is to minimize a cost function that penalizes input usage and constraint violation. With the given parameters $q=0$, $p=0$, and $r=0.1$, the cost function is:\n$$\nJ = \\sum_{i=0}^{N-1} r u_i^2 + \\rho \\sum_{i=1}^{N} s_i\n$$\nwhere $\\rho$ is a weight on the $\\ell_1$-norm of the slack vector. For any given input sequence $u_{0:N-1}$, the cost is minimized by choosing the smallest possible non-negative slack variables that satisfy the softened constraints. This yields the optimal slack values:\n$$\ns_i^\\star = \\max\\{0, |x_i^{pred}| - x_{\\max}\\}\n$$\nSubstituting this into the cost function, the optimization problem to be solved at each time step $k$ becomes minimization over the input sequence only:\n$$\n\\min_{u_{0:N-1} \\in \\mathcal{U}_d^N} \\left( 0.1 \\sum_{i=0}^{3} u_i^2 + \\rho \\sum_{i=1}^{4} \\max\\{0, |x_i^{pred}| - 2\\} \\right)\n$$\nThis problem is solved by exhaustive search over all $|\\mathcal{U}_d|^N = 5^4 = 625$ possible input sequences. If multiple sequences yield the same minimum cost, a deterministic tie-breaker selects the sequence with the minimum total slack, $\\sum_{i=1}^N s_i^\\star$.\n\nThe MPC law is implemented in a receding-horizon fashion: after finding the optimal sequence $u^\\star = (u_0^\\star, \\dots, u_{N-1}^\\star)$, only the first control action $u_0^\\star$ is applied to the system, $x_{k+1} = x_k + u_0^\\star$. The process is then repeated at the next time step $k+1$ from the new state $x_{k+1}$. This simulation is run for a total of $T$ time steps.\n\nThe solution requires the evaluation of three properties based on the closed-loop simulation:\n1.  **always_feasible**: This boolean indicates if the open-loop problem is feasible at every time step $k \\in \\{0, \\dots, T-1\\}$. Due to the use of soft constraints (slack variables), a feasible solution with finite cost can always be found for any state $x_k$, as any constraint violation can be accommodated by a sufficiently large slack variable. Therefore, this property is `True` by construction.\n\n2.  **zero_slack_invariant**: This property assesses a form of recursive feasibility. It is `True` if, for any time step $k$ where the optimal predicted trajectory has zero slack (i.e., $|x_i^{pred}| \\le x_{\\max}$ for all $i \\in \\{1, \\dots, N\\}$), the shifted candidate control sequence $(u_1^\\star, \\dots, u_{N-1}^\\star, 0)$ also produces a zero-slack trajectory when applied from the subsequent state $x_{k+1}$. As shown by analysis of the system dynamics, the new predicted state sequence is a subsequence of the previous one, appended with its terminal state. Specifically, the new sequence of states is $(x_2^{pred}, \\dots, x_N^{pred}, x_N^{pred})$. Since the original predicted trajectory was assumed to have zero slack, all these states satisfy the constraint. Thus, this property also holds by construction, provided the implementation is correct. The property is vacuously `True` if no zero-slack optimal plan is ever found.\n\n3.  **eventual_zero_slack**: This boolean is `True` if the realized state trajectory $x_k$ eventually enters and remains within the hard constraint set. Formally, there must exist a time index $\\bar{k} \\in \\{0, \\dots, T\\}$ such that the realized slack $\\sigma_k = \\max\\{0, |x_k| - x_{\\max}\\}$ is zero for all $k \\in \\{\\bar{k}, \\dots, T\\}$. This is equivalent to the suffix of the realized slack sequence being all zeros. Its value depends on the initial state and the penalty weight $\\rho$.\n\nThe algorithm is implemented by iterating through the test cases, running the receding-horizon simulation for each, and evaluating these three properties.\n```python\nimport numpy as np\nfrom itertools import product\n\ndef solve_problem_3():\n    \"\"\"\n    Main function to solve the MPC problem for all test cases.\n    \"\"\"\n    # System and MPC parameters defined in the problem statement.\n    A = 1.0\n    B = 1.0\n    X_MAX = 2.0\n    U_D = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    N = 4\n    R = 0.1\n    TOL = 1e-9  # Tolerance for floating point comparisons\n\n    def solve_open_loop(x_k, rho):\n        \"\"\"\n        Solves the open-loop optimal control problem using exhaustive enumeration.\n        \"\"\"\n        candidate_sequences = list(product(U_D, repeat=N))\n        \n        results = []\n\n        for u_seq in candidate_sequences:\n            x_pred = np.zeros(N + 1)\n            x_pred[0] = x_k\n            s_seq = np.zeros(N)\n            \n            cost = 0.0\n            slack_sum = 0.0\n            \n            for i in range(N):\n                cost += R * u_seq[i]**2\n                x_pred[i+1] = A * x_pred[i] + B * u_seq[i]\n                slack = max(0, abs(x_pred[i+1]) - X_MAX)\n                s_seq[i] = slack\n                cost += rho * slack\n                slack_sum += slack\n\n            results.append((cost, slack_sum, u_seq, s_seq))\n\n        # Sort by cost, then by sum of slacks as the deterministic tie-breaker.\n        results.sort(key=lambda item: (item[0], item[1]))\n        \n        best_result = results[0]\n        optimal_u_seq = best_result[2]\n        optimal_s_seq = best_result[3]\n\n        return optimal_u_seq, optimal_s_seq\n\n    def run_simulation(x_init, rho, T):\n        \"\"\"\n        Runs the receding horizon simulation for one test case and evaluates the properties.\n        \"\"\"\n        x_history = np.zeros(T + 1)\n        x_history[0] = x_init\n        \n        realized_slacks = np.zeros(T + 1)\n        realized_slacks[0] = max(0, abs(x_history[0]) - X_MAX)\n\n        always_feasible = True\n        zero_slack_invariant = True\n        found_zero_slack_plan = False\n\n        for k in range(T):\n            x_k = x_history[k]\n            optimal_u_seq, optimal_s_seq = solve_open_loop(x_k, rho)\n            \n            is_zero_slack_plan = (np.sum(optimal_s_seq)  TOL)\n            if is_zero_slack_plan:\n                found_zero_slack_plan = True\n                u_cand = list(optimal_u_seq[1:]) + [0.0]\n                x_k_plus_1 = A * x_k + B * optimal_u_seq[0]\n                \n                x_pred_check = np.zeros(N + 1)\n                x_pred_check[0] = x_k_plus_1\n                \n                violates = False\n                for i in range(N):\n                    x_pred_check[i+1] = A * x_pred_check[i] + B * u_cand[i]\n                    if abs(x_pred_check[i+1]) > X_MAX + TOL:\n                        violates = True\n                        break\n                if violates:\n                    zero_slack_invariant = False\n            \n            u_k_optimal = optimal_u_seq[0]\n            x_history[k+1] = A * x_k + B * u_k_optimal\n            realized_slacks[k+1] = max(0, abs(x_history[k+1]) - X_MAX)\n\n        if not found_zero_slack_plan:\n            zero_slack_invariant = True # Vacuously true\n\n        eventual_zero_slack = False\n        for k_bar in range(T + 1):\n            if np.all(realized_slacks[k_bar:]  TOL):\n                eventual_zero_slack = True\n                break\n                \n        return [always_feasible, zero_slack_invariant, eventual_zero_slack]\n\n    test_cases = [\n        (1.0, 100.0, 8),  # Case A\n        (2.5, 10.0, 6),   # Case B\n        (5.0, 0.01, 6),   # Case C\n        (2.0, 1.0, 5),    # Case D\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result_booleans = run_simulation(x_init=case[0], rho=case[1], T=case[2])\n        formatted_result = f\"[{','.join(map(str, result_booleans))}]\"\n        all_results.append(formatted_result)\n\n    return f\"[{','.join(all_results)}]\"\n```", "answer": "[[True,True,True],[True,True,True],[True,True,False],[True,True,True]]", "id": "2724817"}]}