{"hands_on_practices": [{"introduction": "Many physical systems, from robotic arms to orbiting satellites, are described by continuous-time nonlinear differential equations. To apply a discrete-time filter like the EKF, we must first translate these continuous dynamics into a discrete-time model. This practice guides you through this crucial first step, demonstrating how to linearize the system dynamics and use the forward-Euler method to approximate the state transition and process noise covariance matrices for the filter's prediction step [@problem_id:2705962].", "problem": "Consider the continuous-time nonlinear system intended for use with the Extended Kalman Filter (EKF) for nonlinear systems (Extended Kalman Filter (EKF)) given by\n$$\n\\dot{x}(t) = f(x(t)) + L\\,w(t), \\quad f(x) \\triangleq \\begin{bmatrix} x_2 \\\\ -\\omega^{2}\\,\\sin(x_1) \\end{bmatrix},\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$ with $x_1$ an angle in radians, $\\omega \\in \\mathbb{R}_{>0}$, $L \\in \\mathbb{R}^{2\\times 1}$, and $w(t)$ is zero-mean white Gaussian process noise with continuous-time spectral density $Q_c \\in \\mathbb{R}$ (a scalar). Assume $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and $\\omega = 1$. The EKF uses the linearization of the drift $f(x)$ about the mean state $\\mu \\in \\mathbb{R}^{2}$ through the Jacobian $F(\\mu) \\triangleq \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=\\mu}$. For a small sampling interval $\\Delta t$, adopt a first-order forward-Euler discretization of the linearized dynamics and a first-order approximation of the discrete process noise covariance under a zero-order hold on the diffusion.\n\nLet the numerical values be $\\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$, $Q_c = q = 1/5$, and $\\Delta t = 0.05$. Compute $F(\\mu)$, then obtain the first-order discrete-time approximations of the state transition matrix and process noise covariance, denoted $F_d$ and $Q_d$, respectively, consistent with the above modeling assumptions. Finally, evaluate the scalar\n$$\ns \\triangleq \\operatorname{tr}(F_d) + [Q_d]_{22}.\n$$\nProvide your final result for $s$ as an exact value with no rounding and no units. Angles are in radians. Do not report any intermediate matrices; only report the final value of $s$.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for its resolution. It is a standard exercise in the discretization of a continuous-time nonlinear system for the application of an Extended Kalman Filter. We proceed with the solution.\n\nThe problem requires the computation of a scalar value $s \\triangleq \\operatorname{tr}(F_d) + [Q_d]_{22}$, where $F_d$ and $Q_d$ are the discrete-time state transition matrix and process noise covariance, respectively. These are derived from a continuous-time nonlinear system model.\n\nThe continuous-time dynamics are given by $\\dot{x}(t) = f(x(t)) + L\\,w(t)$, with the drift function defined as:\n$$\nf(x) = \\begin{bmatrix} x_2 \\\\ -\\omega^{2}\\,\\sin(x_1) \\end{bmatrix}\n$$\nThe problem specifies the values $\\omega=1$ and $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Thus, the drift function simplifies to:\n$$\nf(x) = \\begin{bmatrix} x_2 \\\\ -\\sin(x_1) \\end{bmatrix}\n$$\n\nThe first step is to linearize this drift function about the provided mean state $\\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$. This is done by computing the Jacobian matrix $F(\\mu) \\triangleq \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=\\mu}$. The partial derivatives of the components of $f(x)$ are:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(x_2) = 0\n$$\n$$\n\\frac{\\partial f_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(x_2) = 1\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(-\\sin(x_1)) = -\\cos(x_1)\n$$\n$$\n\\frac{\\partial f_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(-\\sin(x_1)) = 0\n$$\nAssembling these into the Jacobian matrix gives:\n$$\nF(x) = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} 0 & 1 \\\\ -\\cos(x_1) & 0 \\end{bmatrix}\n$$\nWe evaluate this matrix at the point $x = \\mu = \\begin{bmatrix} \\pi/6 \\\\ 1/2 \\end{bmatrix}$:\n$$\nF(\\mu) = \\begin{bmatrix} 0 & 1 \\\\ -\\cos(\\pi/6) & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -\\frac{\\sqrt{3}}{2} & 0 \\end{bmatrix}\n$$\n\nThe second step is to obtain the discrete-time state transition matrix, $F_d$. The problem specifies a first-order forward-Euler discretization of the linearized dynamics over a sampling interval $\\Delta t = 0.05$. The formula for this is $F_d \\approx I + \\Delta t F(\\mu)$, where $I$ is the $2 \\times 2$ identity matrix.\nSubstituting the values $\\Delta t = 0.05 = \\frac{1}{20}$ and the computed $F(\\mu)$:\n$$\nF_d = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\frac{1}{20} \\begin{bmatrix} 0 & 1 \\\\ -\\frac{\\sqrt{3}}{2} & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & \\frac{1}{20} \\\\ -\\frac{\\sqrt{3}}{40} & 1 \\end{bmatrix}\n$$\n\nThe third step is to compute the discrete-time process noise covariance matrix, $Q_d$. The problem asks for a first-order approximation consistent with a zero-order hold on the process noise diffusion. For a continuous-time system with process noise input matrix $L$ and spectral density $Q_c$, the simplest and most common first-order approximation for small $\\Delta t$ is:\n$$\nQ_d \\approx (L Q_c L^T) \\Delta t\n$$\nWe are given $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $Q_c = q = 1/5$, and $\\Delta t = 0.05 = \\frac{1}{20}$.\nFirst, we compute the continuous-time covariance matrix term $L Q_c L^T$:\n$$\nL Q_c L^T = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\left(\\frac{1}{5}\\right) \\begin{bmatrix} 0 & 1 \\end{bmatrix} = \\frac{1}{5} \\begin{bmatrix} 0 \\cdot 0 & 0 \\cdot 1 \\\\ 1 \\cdot 0 & 1 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{5} \\end{bmatrix}\n$$\nNow, we multiply by $\\Delta t$ to get $Q_d$:\n$$\nQ_d = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{5} \\end{bmatrix} \\cdot \\left(\\frac{1}{20}\\right) = \\begin{bmatrix} 0 & 0 \\\\ 0 & \\frac{1}{100} \\end{bmatrix}\n$$\n\nThe final step is to compute the scalar $s = \\operatorname{tr}(F_d) + [Q_d]_{22}$.\nThe trace of $F_d$ is the sum of its diagonal elements:\n$$\n\\operatorname{tr}(F_d) = 1 + 1 = 2\n$$\nThe element at the second row and second column of $Q_d$, denoted $[Q_d]_{22}$, is:\n$$\n[Q_d]_{22} = \\frac{1}{100}\n$$\nSumming these two values gives the final result for $s$:\n$$\ns = 2 + \\frac{1}{100} = \\frac{200}{100} + \\frac{1}{100} = \\frac{201}{100}\n$$\nAs a decimal, this is $2.01$. The value is exact.", "answer": "$$\\boxed{\\frac{201}{100}}$$", "id": "2705962"}, {"introduction": "The core of any Kalman filter is the measurement update, where a prediction is corrected using new information from a sensor. For the EKF, this involves linearizing the nonlinear measurement model around the predicted state. This exercise provides a complete, step-by-step walkthrough of the measurement update for a classic bearing-only tracking problem, reinforcing your understanding of how to compute the Jacobian, Kalman gain, and the final updated state estimate and covariance [@problem_id:2705967].", "problem": "Consider a planar bearing-only sensor that measures the bearing angle of a target at state $x_k = \\begin{bmatrix}x_{1,k} & x_{2,k}\\end{bmatrix}^{\\top}$ relative to the sensor colocated at the origin. The measurement model is the nonlinear function $h:\\mathbb{R}^2 \\to \\mathbb{R}$ given by $h(x) = \\arctan\\!\\big(x_2/x_1\\big)$, with angle measured in radians. Assume additive zero-mean Gaussian measurement noise with covariance $R = 0.01$. At time $k$, an Extended Kalman Filter (EKF) (Extended Kalman Filter (EKF)) performs a measurement update using a single scalar measurement $y_k = 0.3$ radians, a predicted state mean $\\mu_k^{-} = \\begin{bmatrix}1.0 & 0.5\\end{bmatrix}^{\\top}$, and a predicted covariance\n$$\nP_k^{-} \\;=\\; \\begin{bmatrix} 0.1 & 0.02 \\\\\n0.02 & 0.05 \\end{bmatrix}.\n$$\nStarting from the core definition that the EKF linearizes the measurement model about the predicted mean and applies the linear Kalman filter update to the linearized model, carry out the following tasks:\n1) Derive the measurement Jacobian $H_k = \\left.\\dfrac{\\partial h}{\\partial x}\\right|_{x=\\mu_k^{-}}$ from first principles and evaluate it at $\\mu_k^{-}$.\n2) Using first principles for the linearized Bayesian update, compute the innovation covariance $S_k$, the Kalman gain $K_k$, the updated mean $\\mu_k$, and the updated covariance $P_k$. You may use any algebraically equivalent form of the covariance update that preserves positive semidefiniteness.\nExpress all intermediate quantities exactly whenever possible (for example, using rational numbers and elementary constants), and keep angles in radians. As your final answer, report only the first component of the updated mean, i.e., the first entry of $\\mu_k$, as a single closed-form analytic expression without numerical rounding.", "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. It provides all necessary components for executing a single measurement update step of an Extended Kalman Filter. We shall proceed with the derivation and computation from first principles.\n\nThe state vector is $x_k = \\begin{bmatrix} x_{1,k} & x_{2,k} \\end{bmatrix}^{\\top}$. The nonlinear measurement function is given by $h(x) = \\arctan(x_2/x_1)$. To perform the EKF update, we must linearize this function about the predicted state mean, $\\mu_k^{-} = \\begin{bmatrix}1.0 & 0.5\\end{bmatrix}^{\\top}$.\n\n**1) Measurement Jacobian $H_k$**\n\nThe measurement Jacobian $H$ is the matrix of first-order partial derivatives of the measurement function $h(x)$. For a scalar function of a vector $x = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}^{\\top}$, the Jacobian is a row vector.\n\nThe partial derivative of $h(x)$ with respect to $x_1$ is:\n$$\n\\frac{\\partial h}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left( \\arctan\\left(\\frac{x_2}{x_1}\\right) \\right) = \\frac{1}{1 + \\left(\\frac{x_2}{x_1}\\right)^2} \\cdot \\left(-\\frac{x_2}{x_1^2}\\right) = \\frac{-x_2}{x_1^2 + x_2^2}\n$$\nThe partial derivative of $h(x)$ with respect to $x_2$ is:\n$$\n\\frac{\\partial h}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left( \\arctan\\left(\\frac{x_2}{x_1}\\right) \\right) = \\frac{1}{1 + \\left(\\frac{x_2}{x_1}\\right)^2} \\cdot \\left(\\frac{1}{x_1}\\right) = \\frac{x_1}{x_1^2 + x_2^2}\n$$\nThus, the Jacobian is the row vector:\n$$\nH(x) = \\frac{\\partial h}{\\partial x} = \\begin{bmatrix} \\frac{-x_2}{x_1^2 + x_2^2} & \\frac{x_1}{x_1^2 + x_2^2} \\end{bmatrix}\n$$\nWe evaluate this Jacobian at the predicted mean $\\mu_k^{-} = \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix}$. Let $\\mu_k^{-} = \\begin{bmatrix} \\mu_1^{-} & \\mu_2^{-} \\end{bmatrix}^{\\top}$.\nThe denominator is $(\\mu_1^{-})^2 + (\\mu_2^{-})^2 = 1^2 + (\\frac{1}{2})^2 = 1 + \\frac{1}{4} = \\frac{5}{4}$.\nThe components of the Jacobian $H_k = H(\\mu_k^{-})$ are:\n$$\n\\frac{\\partial h}{\\partial x_1}\\bigg|_{x=\\mu_k^{-}} = \\frac{-\\mu_2^{-}}{(\\mu_1^{-})^2 + (\\mu_2^{-})^2} = \\frac{-1/2}{5/4} = -\\frac{1}{2} \\cdot \\frac{4}{5} = -\\frac{2}{5}\n$$\n$$\n\\frac{\\partial h}{\\partial x_2}\\bigg|_{x=\\mu_k^{-}} = \\frac{\\mu_1^{-}}{(\\mu_1^{-})^2 + (\\mu_2^{-})^2} = \\frac{1}{5/4} = \\frac{4}{5}\n$$\nSo, the evaluated measurement Jacobian is:\n$$\nH_k = \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix}\n$$\n\n**2) EKF Measurement Update**\n\nThe EKF measurement update equations for the mean and covariance are:\n$$\n\\mu_k = \\mu_k^{-} + K_k (y_k - h(\\mu_k^{-}))\n$$\n$$\nP_k = (I - K_k H_k) P_k^{-}\n$$\nwhere $K_k$ is the Kalman gain, defined as:\n$$\nK_k = P_k^{-} H_k^{\\top} S_k^{-1}\n$$\nand $S_k$ is the innovation covariance:\n$$\nS_k = H_k P_k^{-} H_k^{\\top} + R\n$$\nWe are given the following quantities, which we express with rational numbers for precision:\n$$\n\\mu_k^{-} = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix}, \\quad P_k^{-} = \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix}, \\quad y_k = \\frac{3}{10}, \\quad R = \\frac{1}{100}\n$$\n\nFirst, we compute the **innovation covariance $S_k$**.\n$$\nH_k P_k^{-} = \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{50}+\\frac{4}{250} & -\\frac{2}{250}+\\frac{4}{100} \\end{bmatrix} = \\begin{bmatrix} -\\frac{10}{250}+\\frac{4}{250} & -\\frac{2}{250}+\\frac{10}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{250} & \\frac{8}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{125} & \\frac{4}{125} \\end{bmatrix}\n$$\n$$\nH_k P_k^{-} H_k^{\\top} = \\begin{bmatrix} -\\frac{3}{125} & \\frac{4}{125} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} \\\\ \\frac{4}{5} \\end{bmatrix} = \\frac{6}{625} + \\frac{16}{625} = \\frac{22}{625}\n$$\n$$\nS_k = \\frac{22}{625} + R = \\frac{22}{625} + \\frac{1}{100} = \\frac{88}{2500} + \\frac{25}{2500} = \\frac{113}{2500}\n$$\n\nNext, we compute the **Kalman gain $K_k$**.\n$$\nP_k^{-} H_k^{\\top} = \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} \\\\ \\frac{4}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{2}{50} + \\frac{4}{250} \\\\ -\\frac{2}{250} + \\frac{4}{100} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{250} \\\\ \\frac{8}{250} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{125} \\\\ \\frac{4}{125} \\end{bmatrix}\n$$\n$$\nK_k = P_k^{-} H_k^{\\top} S_k^{-1} = \\begin{bmatrix} -\\frac{3}{125} \\\\ \\frac{4}{125} \\end{bmatrix} \\left(\\frac{2500}{113}\\right) = \\begin{bmatrix} -\\frac{3 \\cdot 20}{113} \\\\ \\frac{4 \\cdot 20}{113} \\end{bmatrix} = \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix}\n$$\n\nNow, we compute the **updated mean $\\mu_k$**.\nThe innovation (or residual) is $\\tilde{y}_k = y_k - h(\\mu_k^{-})$.\nThe predicted measurement is $h(\\mu_k^{-}) = \\arctan(\\frac{1/2}{1}) = \\arctan(\\frac{1}{2})$.\nThe innovation is $\\tilde{y}_k = \\frac{3}{10} - \\arctan(\\frac{1}{2})$.\n$$\n\\mu_k = \\mu_k^{-} + K_k \\tilde{y}_k = \\begin{bmatrix} 1 \\\\ \\frac{1}{2} \\end{bmatrix} + \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix} \\left(\\frac{3}{10} - \\arctan\\left(\\frac{1}{2}\\right)\\right)\n$$\nThe first component of the updated mean, $\\mu_{1,k}$, is:\n$$\n\\mu_{1,k} = 1 - \\frac{60}{113} \\left(\\frac{3}{10} - \\arctan\\left(\\frac{1}{2}\\right)\\right) = 1 - \\frac{18}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right) = \\frac{113-18}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right)\n$$\n$$\n\\mu_{1,k} = \\frac{95}{113} + \\frac{60}{113}\\arctan\\left(\\frac{1}{2}\\right) = \\frac{95 + 60 \\arctan(\\frac{1}{2})}{113}\n$$\nThe second component is $\\mu_{2,k} = \\frac{1}{2} + \\frac{80}{113} (\\frac{3}{10} - \\arctan(\\frac{1}{2})) = \\frac{1}{2} + \\frac{24}{113} - \\frac{80}{113}\\arctan(\\frac{1}{2}) = \\frac{113+48}{226} - \\frac{80}{113}\\arctan(\\frac{1}{2}) = \\frac{161}{226} - \\frac{80}{113}\\arctan(\\frac{1}{2})$.\nThe updated state mean is $\\mu_k = \\begin{bmatrix} \\frac{95 + 60 \\arctan(\\frac{1}{2})}{113} \\\\ \\frac{161 - 160 \\arctan(\\frac{1}{2})}{226} \\end{bmatrix}$.\n\nFinally, we compute the **updated covariance $P_k$**.\n$$\nK_k H_k = \\begin{bmatrix} -\\frac{60}{113} \\\\ \\frac{80}{113} \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{5} & \\frac{4}{5} \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} (-60)(-\\frac{2}{5}) & (-60)(\\frac{4}{5}) \\\\ (80)(-\\frac{2}{5}) & (80)(\\frac{4}{5}) \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 24 & -48 \\\\ -32 & 64 \\end{bmatrix}\n$$\n$$\nI - K_k H_k = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\frac{1}{113}\\begin{bmatrix} 24 & -48 \\\\ -32 & 64 \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 113-24 & 48 \\\\ 32 & 113-64 \\end{bmatrix} = \\frac{1}{113} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix}\n$$\n$$\nP_k = (I - K_k H_k) P_k^{-} = \\frac{1}{113} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{10} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{20} \\end{bmatrix} = \\frac{1}{11300} \\begin{bmatrix} 89 & 48 \\\\ 32 & 49 \\end{bmatrix} \\begin{bmatrix} 10 & 2 \\\\ 2 & 5 \\end{bmatrix}\n$$\n$$\nP_k = \\frac{1}{11300} \\begin{bmatrix} 890+96 & 178+240 \\\\ 320+98 & 64+245 \\end{bmatrix} = \\frac{1}{11300} \\begin{bmatrix} 986 & 418 \\\\ 418 & 309 \\end{bmatrix}\n$$\nThis completes all required computations. The final answer requested is the first component of the updated mean, $\\mu_{1,k}$.", "answer": "$$\n\\boxed{\\frac{95 + 60 \\arctan\\left(\\frac{1}{2}\\right)}{113}}\n$$", "id": "2705967"}, {"introduction": "The performance of an EKF is critically sensitive to the accuracy of its noise covariance parameters, $Q$ and $R$. In practice, these values are rarely known perfectly and must be tuned. This exercise introduces a fundamental and powerful technique for filter tuning by analyzing the innovation sequenceâ€”the difference between the actual measurement and the predicted measurement. You will learn how the statistics of the innovation can reveal a mismatch and how to adjust the filter's parameters to improve its performance [@problem_id:2706005].", "problem": "Consider a scalar Extended Kalman Filter (EKF) for a discrete-time nonlinear measurement model of the form $y_{k} = h(x_{k}) + v_{k}$, where $v_{k}$ is zero-mean white measurement noise with variance $R_{k}$. The EKF employs a first-order linearization of the measurement function about the predicted state estimate $\\hat{x}_{k}^{-}$ via the Jacobian $H_{k} := \\left.\\frac{\\partial h}{\\partial x}\\right|_{\\hat{x}_{k}^{-}}$. The predicted state error covariance prior to the measurement update is $P_{k}^{-} := \\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})(x_{k} - \\hat{x}_{k}^{-})^{\\top}\\right]$, which in the scalar case is a nonnegative real number. Assume that the linearization error is negligible at the operating point and that the state prediction error $x_{k} - \\hat{x}_{k}^{-}$ is uncorrelated with the measurement noise $v_{k}$.\n\nDefine the innovation as $\\tilde{y}_{k} := y_{k} - h(\\hat{x}_{k}^{-})$. Starting from the measurement model, use first principles to express the scalar innovation variance $S_{k} := \\mathbb{E}\\left[\\tilde{y}_{k}^{2}\\right]$ in terms of $H_{k}$, $P_{k}^{-}$, and $R_{k}$.\n\nSuppose that over a sufficiently long window, the empirical variance of the innovations is measured to be $\\hat{S} = 0.5$. The EKF, using its current tuning $R_{\\text{old}}$, predicts an innovation variance of $S_{\\text{old}} = 0.2$. You are tasked with retuning only the measurement noise variance while keeping $H_{k}$ and $P_{k}^{-}$ unchanged so that the predicted innovation variance matches the empirical value $\\hat{S}$.\n\nCompute the additive adjustment $\\Delta R := R_{\\text{new}} - R_{\\text{old}}$ needed to achieve $S_{k} = \\hat{S}$ with $H_{k}$ and $P_{k}^{-}$ held fixed. Express your final answer as a real number. No rounding is required.", "solution": "The problem requires the derivation of the innovation variance for a scalar Extended Kalman Filter (EKF) and the subsequent calculation of an adjustment to the measurement noise variance. The problem statement is subjected to validation first.\n\n**Problem Validation**\n\nStep 1: Extract Givens\n- Measurement model: $y_{k} = h(x_{k}) + v_{k}$ (scalar)\n- Measurement noise $v_{k}$: zero-mean, white, with variance $R_{k} = \\mathbb{E}[v_{k}^2]$.\n- Jacobian of measurement function: $H_{k} := \\left.\\frac{\\partial h}{\\partial x}\\right|_{\\hat{x}_{k}^{-}}$.\n- Predicted state error covariance (prior): $P_{k}^{-} := \\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})^{2}\\right]$.\n- Innovation: $\\tilde{y}_{k} := y_{k} - h(\\hat{x}_{k}^{-})$.\n- Innovation variance: $S_{k} := \\mathbb{E}\\left[\\tilde{y}_{k}^{2}\\right]$.\n- Assumption 1: Linearization error is negligible.\n- Assumption 2: State prediction error $(x_{k} - \\hat{x}_{k}^{-})$ is uncorrelated with measurement noise $v_{k}$.\n- Empirical innovation variance: $\\hat{S} = 0.5$.\n- Old predicted innovation variance: $S_{\\text{old}} = 0.2$.\n- Old measurement noise variance: $R_{\\text{old}}$.\n- New measurement noise variance: $R_{\\text{new}}$.\n- Adjustment to be computed: $\\Delta R := R_{\\text{new}} - R_{\\text{old}}$.\n- Constraint: $H_{k}$ and $P_{k}^{-}$ are held constant during retuning.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in control theory and state estimation, specifically concerning the analysis and tuning of the Extended Kalman Filter. It is well-posed, providing a clear objective and sufficient data to arrive at a unique solution. The language is precise and objective. All assumptions are explicitly stated and are standard in the context of EKF analysis. The problem does not violate any fundamental principles, is not incomplete, and does not contain contradictory information. The scenario of tuning a filter parameter based on empirical innovation statistics is a standard engineering practice.\n\nStep 3: Verdict and Action\nThe problem is deemed valid. A solution will be furnished.\n\n**Derivation of Innovation Variance**\n\nThe first task is to derive an expression for the innovation variance $S_{k}$. We begin with the definition of the innovation, $\\tilde{y}_{k}$:\n$$\n\\tilde{y}_{k} = y_{k} - h(\\hat{x}_{k}^{-})\n$$\nSubstitute the measurement model $y_{k} = h(x_{k}) + v_{k}$:\n$$\n\\tilde{y}_{k} = (h(x_{k}) + v_{k}) - h(\\hat{x}_{k}^{-}) = h(x_{k}) - h(\\hat{x}_{k}^{-}) + v_{k}\n$$\nThe problem states that the EKF uses a first-order linearization of $h(x)$ around the predicted state estimate $\\hat{x}_{k}^{-}$. We perform a Taylor series expansion of $h(x_{k})$ about $\\hat{x}_{k}^{-}$:\n$$\nh(x_{k}) = h(\\hat{x}_{k}^{-}) + \\left.\\frac{\\partial h}{\\partial x}\\right|_{\\hat{x}_{k}^{-}}(x_{k} - \\hat{x}_{k}^{-}) + \\text{higher-order terms}\n$$\nBy definition, the Jacobian is $H_{k} = \\left.\\frac{\\partial h}{\\partial x}\\right|_{\\hat{x}_{k}^{-}}$. The problem permits us to neglect the linearization error, so the higher-order terms are ignored. The approximation is:\n$$\nh(x_{k}) \\approx h(\\hat{x}_{k}^{-}) + H_{k}(x_{k} - \\hat{x}_{k}^{-})\n$$\nSubstituting this approximation into the expression for the innovation gives:\n$$\n\\tilde{y}_{k} \\approx [h(\\hat{x}_{k}^{-}) + H_{k}(x_{k} - \\hat{x}_{k}^{-})] - h(\\hat{x}_{k}^{-}) + v_{k}\n$$\n$$\n\\tilde{y}_{k} \\approx H_{k}(x_{k} - \\hat{x}_{k}^{-}) + v_{k}\n$$\nThe innovation variance $S_{k}$ is the expectation of the square of the innovation, $S_{k} = \\mathbb{E}[\\tilde{y}_{k}^{2}]$. First, we confirm the innovation is zero-mean under this approximation:\n$$\n\\mathbb{E}[\\tilde{y}_{k}] \\approx \\mathbb{E}[H_{k}(x_{k} - \\hat{x}_{k}^{-}) + v_{k}] = H_{k}\\mathbb{E}[x_{k} - \\hat{x}_{k}^{-}] + \\mathbb{E}[v_{k}]\n$$\nBy definition of the predicted state estimate, $\\mathbb{E}[x_{k} - \\hat{x}_{k}^{-}] = 0$. The measurement noise is also zero-mean, $\\mathbb{E}[v_{k}] = 0$. Thus, $\\mathbb{E}[\\tilde{y}_{k}] = 0$.\n\nNow, we compute the variance:\n$$\nS_{k} = \\mathbb{E}[\\tilde{y}_{k}^{2}] \\approx \\mathbb{E}\\left[ (H_{k}(x_{k} - \\hat{x}_{k}^{-}) + v_{k})^{2} \\right]\n$$\nExpanding the square:\n$$\nS_{k} \\approx \\mathbb{E}\\left[ H_{k}^{2}(x_{k} - \\hat{x}_{k}^{-})^{2} + 2H_{k}(x_{k} - \\hat{x}_{k}^{-})v_{k} + v_{k}^{2} \\right]\n$$\nBy linearity of expectation:\n$$\nS_{k} \\approx H_{k}^{2}\\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})^{2}\\right] + 2H_{k}\\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})v_{k}\\right] + \\mathbb{E}\\left[v_{k}^{2}\\right]\n$$\nWe identify the terms based on the given definitions and assumptions:\n1.  $\\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})^{2}\\right] = P_{k}^{-}$, the scalar prior state error variance.\n2.  $\\mathbb{E}\\left[(x_{k} - \\hat{x}_{k}^{-})v_{k}\\right] = 0$, as the state prediction error is uncorrelated with the measurement noise.\n3.  $\\mathbb{E}\\left[v_{k}^{2}\\right] = R_{k}$, the measurement noise variance.\n\nSubstituting these into the expression for $S_{k}$ yields the fundamental relation for the innovation variance in a scalar EKF:\n$$\nS_{k} = H_{k}^{2}P_{k}^{-} + R_{k}\n$$\n\n**Computation of Noise Variance Adjustment**\n\nThe second task is to compute the additive adjustment $\\Delta R$ required to retune the filter. We are given the \"old\" state of the filter, where the predicted innovation variance is $S_{\\text{old}} = 0.2$. This corresponds to using the old measurement noise variance $R_{\\text{old}}$. Using our derived formula:\n$$\nS_{\\text{old}} = H_{k}^{2}P_{k}^{-} + R_{\\text{old}}\n$$\nSubstituting the given value for $S_{\\text{old}}$:\n$$\n0.2 = H_{k}^{2}P_{k}^{-} + R_{\\text{old}}\n$$\nWe wish to retune the filter such that the new predicted innovation variance, $S_{\\text{new}}$, matches the empirically measured variance, $\\hat{S} = 0.5$. This is to be achieved by adjusting $R_{k}$ to a new value, $R_{\\text{new}}$, while $H_{k}$ and $P_{k}^{-}$ are held constant.\n$$\nS_{\\text{new}} = H_{k}^{2}P_{k}^{-} + R_{\\text{new}}\n$$\nSetting $S_{\\text{new}} = \\hat{S}$:\n$$\n0.5 = H_{k}^{2}P_{k}^{-} + R_{\\text{new}}\n$$\nWe are required to find $\\Delta R = R_{\\text{new}} - R_{\\text{old}}$. To find this, we subtract the equation for the old state from the equation for the new state:\n$$\nS_{\\text{new}} - S_{\\text{old}} = (H_{k}^{2}P_{k}^{-} + R_{\\text{new}}) - (H_{k}^{2}P_{k}^{-} + R_{\\text{old}})\n$$\nThe term $H_{k}^{2}P_{k}^{-}$ cancels, as required by the problem's constraint that $H_{k}$ and $P_{k}^{-}$ do not change.\n$$\nS_{\\text{new}} - S_{\\text{old}} = R_{\\text{new}} - R_{\\text{old}} = \\Delta R\n$$\nSubstituting the numerical values for $S_{\\text{new}} = \\hat{S}$ and $S_{\\text{old}}$:\n$$\n\\Delta R = 0.5 - 0.2 = 0.3\n$$\nThe required additive adjustment to the measurement noise variance is $0.3$.", "answer": "$$\n\\boxed{0.3}\n$$", "id": "2706005"}]}