{"hands_on_practices": [{"introduction": "This first exercise provides foundational practice in synthesizing a controller from first principles. By defining a desired stable error dynamic, you will use the technique of feedback linearization to derive a control law that forces a nonlinear system to track a reference trajectory. This problem [@problem_id:2695616] illustrates how the existence of a Lyapunov function for the target linear dynamics implicitly validates the resulting controller within the Control Lyapunov Function (CLF) framework.", "problem": "Consider the control-affine, second-order nonlinear system\n$$\n\\dot{x}_{1} = x_{2}, \\qquad \\dot{x}_{2} = -x_{1}^{3} - x_{2} + u,\n$$\nand the tracking objective of following the sinusoidal reference\n$$\nr(t) = \\sin(2t),\n$$\nwhere angles are in radians. Define the tracking error as\n$$\ne_{1} := x_{1} - r(t), \\qquad e_{2} := x_{2} - \\dot{r}(t).\n$$\nUsing the definition of a Control Lyapunov Function (CLF), where a CLF is any continuously differentiable, positive definite function whose time derivative along the closed-loop trajectories is negative definite, synthesize a smooth, time-varying state-feedback controller that guarantees global exponential convergence of the tracking error for the above system. Your design must proceed from first principles: explicitly form the error dynamics, impose a Hurwitz linear error model, and justify the CLF decrease via a symmetric positive definite matrix solution to a Lyapunov equation.\n\nLet the desired linear error dynamics be given by\n$$\n\\dot{e}_{1} = e_{2}, \\qquad \\dot{e}_{2} = -3 e_{1} - 4 e_{2},\n$$\nso that the closed-loop error matrix is Hurwitz. Compute the corresponding CLF-based input $u(x,t)$ for the given nonlinear plant and reference $r(t)$, as an explicit expression in terms of $x_{1}$, $x_{2}$, and $t$. Provide the exact, simplified analytic expression for $u(x,t)$ as your final answer. No numerical rounding is required, and no physical units are involved.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **System Dynamics:**\n$$\n\\dot{x}_{1} = x_{2}, \\qquad \\dot{x}_{2} = -x_{1}^{3} - x_{2} + u\n$$\n- **Reference Signal:**\n$$\nr(t) = \\sin(2t)\n$$\n- **Tracking Error Definition:**\n$$\ne_{1} := x_{1} - r(t), \\qquad e_{2} := x_{2} - \\dot{r}(t)\n$$\n- **Desired Linear Error Dynamics:**\n$$\n\\dot{e}_{1} = e_{2}, \\qquad \\dot{e}_{2} = -3 e_{1} - 4 e_{2}\n$$\n- **Objective:** Compute the control input $u(x,t)$ that achieves these error dynamics. The synthesis must be justified using the concept of a Control Lyapunov Function (CLF).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in nonlinear control theory, specifically concerning tracking control for a control-affine system using the method of feedback linearization. All provided information is scientifically sound, self-contained, and mathematically consistent. The system and reference are well-defined. The objective is clear. The problem is well-posed and allows for a unique solution for the control law $u(x, t)$. There are no contradictions, ambiguities, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be furnished.\n\n**Solution Derivation**\nThe objective is to synthesize a state-feedback control law $u(x,t)$ that forces the tracking error vector $e(t) = \\begin{pmatrix} e_{1}(t)  e_{2}(t) \\end{pmatrix}^T$ to converge to zero. The method employed is feedback linearization, where the nonlinear dynamics are canceled and replaced by a desired linear error dynamic.\n\nFirst, we compute the time derivatives of the reference signal $r(t)$:\n$$\nr(t) = \\sin(2t)\n$$\n$$\n\\dot{r}(t) = \\frac{d}{dt} \\sin(2t) = 2 \\cos(2t)\n$$\n$$\n\\ddot{r}(t) = \\frac{d}{dt} \\left( 2 \\cos(2t) \\right) = -4 \\sin(2t)\n$$\n\nNext, we establish the error dynamics by differentiating the error variables $e_{1}$ and $e_{2}$ with respect to time:\n$$\n\\dot{e}_{1} = \\frac{d}{dt} (x_{1} - r(t)) = \\dot{x}_{1} - \\dot{r}(t)\n$$\nSubstituting the system dynamics $\\dot{x}_{1} = x_{2}$ gives:\n$$\n\\dot{e}_{1} = x_{2} - \\dot{r}(t)\n$$\nBy definition, $e_{2} = x_{2} - \\dot{r}(t)$, so we find:\n$$\n\\dot{e}_{1} = e_{2}\n$$\nThis result is consistent with the first equation of the desired linear error dynamics.\n\nNow, we differentiate the second error variable, $e_{2}$:\n$$\n\\dot{e}_{2} = \\frac{d}{dt} (x_{2} - \\dot{r}(t)) = \\dot{x}_{2} - \\ddot{r}(t)\n$$\nSubstituting the system dynamics for $\\dot{x}_{2}$ yields:\n$$\n\\dot{e}_{2} = (-x_{1}^{3} - x_{2} + u) - \\ddot{r}(t)\n$$\nThis is the actual dynamics for $\\dot{e}_{2}$, which contains the control input $u$. To achieve the tracking objective, we set this expression equal to the desired dynamics for $\\dot{e}_{2}$, which is given as $-3 e_{1} - 4 e_{2}$:\n$$\n-x_{1}^{3} - x_{2} + u - \\ddot{r}(t) = -3 e_{1} - 4 e_{2}\n$$\nWe can now solve for the control input $u$:\n$$\nu(x,t) = x_{1}^{3} + x_{2} + \\ddot{r}(t) - 3 e_{1} - 4 e_{2}\n$$\nThis is the general form of the control law. To obtain an explicit expression in terms of the state variables $x_1, x_2$ and time $t$, we substitute the definitions of $e_1$, $e_2$, and the expression for $\\ddot{r}(t)$:\n$$\nu(x_1, x_2, t) = x_{1}^{3} + x_{2} + (-4 \\sin(2t)) - 3(x_{1} - r(t)) - 4(x_{2} - \\dot{r}(t))\n$$\n$$\nu(x_1, x_2, t) = x_{1}^{3} + x_{2} - 4 \\sin(2t) - 3(x_{1} - \\sin(2t)) - 4(x_{2} - 2 \\cos(2t))\n$$\nDistributing the terms, we get:\n$$\nu(x_1, x_2, t) = x_{1}^{3} + x_{2} - 4 \\sin(2t) - 3 x_{1} + 3 \\sin(2t) - 4 x_{2} + 8 \\cos(2t)\n$$\nFinally, we combine like terms to obtain the simplified expression for the control law:\n$$\nu(x_{1}, x_{2}, t) = x_{1}^{3} - 3 x_{1} - 3 x_{2} - \\sin(2t) + 8 \\cos(2t)\n$$\nThis is a smooth, time-varying state-feedback controller.\n\n**Justification via Control Lyapunov Function (CLF)**\nThe problem requires justification of the controller design via CLF principles. By construction, the control law $u(x,t)$ enforces the following linear, time-invariant error dynamics:\n$$\n\\dot{e} = A e, \\quad \\text{where} \\quad e = \\begin{pmatrix} e_{1} \\\\ e_{2} \\end{pmatrix}, \\quad A = \\begin{pmatrix} 0  1 \\\\ -3  -4 \\end{pmatrix}\n$$\nThe stability of the origin $e=0$ is determined by the eigenvalues of the matrix $A$. The characteristic equation is $\\det(A - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -\\lambda  1 \\\\ -3  -4-\\lambda \\end{pmatrix} = (-\\lambda)(-4-\\lambda) - (1)(-3) = 4\\lambda + \\lambda^2 + 3 = 0\n$$\n$$\n\\lambda^2 + 4\\lambda + 3 = (\\lambda + 1)(\\lambda + 3) = 0\n$$\nThe eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = -3$. Since both eigenvalues have negative real parts, the matrix $A$ is Hurwitz, and the error dynamics are globally exponentially stable.\n\nTo formalize this using a Lyapunov function, we consider a quadratic candidate $V(e) = e^T P e$, where $P$ is a symmetric positive definite matrix. For $V(e)$ to be a strict Lyapunov function, its time derivative along the trajectories of the system must be negative definite.\n$$\n\\dot{V}(e) = \\frac{d}{dt}(e^T P e) = \\dot{e}^T P e + e^T P \\dot{e} = (A e)^T P e + e^T P (A e) = e^T (A^T P + P A) e\n$$\nAccording to Lyapunov's theorem for linear systems, since $A$ is Hurwitz, for any symmetric positive definite matrix $Q$, there exists a unique symmetric positive definite matrix $P$ that solves the Lyapunov equation:\n$$\nA^T P + P A = -Q\n$$\nChoosing $Q=I$ (the identity matrix), we have $\\dot{V}(e) = -e^T Q e = -e^T I e = -e_{1}^{2} - e_{2}^{2}$. Since $P$ is positive definite, $V(e)$ is positive definite, and since $Q$ is positive definite, $\\dot{V}(e)$ is negative definite for all $e \\neq 0$.\n\nTherefore, $V(e)$ is a valid Lyapunov function that proves global exponential stability of the error dynamics. This same function $V(e(x,t))$ serves as a Control Lyapunov Function (CLF) for the original tracking problem. The control law $u$ was specifically designed to render $\\dot{V}$ negative definite. This fulfills the problem requirements.\nThe derived control input $u(x,t)$ ensures that the tracking error converges to zero globally and exponentially.", "answer": "$$\n\\boxed{x_{1}^{3} - 3 x_{1} - 3 x_{2} - \\sin(2t) + 8 \\cos(2t)}\n$$", "id": "2695616"}, {"introduction": "While the existence of a Control Lyapunov Function (CLF) guarantees the existence of a stabilizing feedback law, it does not guarantee that this law is continuous. This exercise [@problem_id:2695614] explores this critical theoretical subtlety through a classic counterexample. By analyzing a simple scalar system, you will investigate the Small Control Property (SCP) and understand why its failure precludes stabilization by any continuous controller, providing deep insight into the limits of CLF-based synthesis.", "problem": "Consider the scalar control-affine nonlinear system given by\n$$\\dot{x} = f(x) + g(x) u,$$\nwith\n$f(x) = x$ and $g(x) = x^{2},$\nand candidate Control Lyapunov Function (CLF) $V(x) = \\tfrac{1}{2} x^{2}$. Start from the core definitions in control theory: a Control Lyapunov Function (CLF) is a positive definite and proper function $V$ such that for every $x \\neq 0$ there exists a control input $u \\in \\mathbb{R}$ making $\\dot{V}(x,u)  0$. The small control property (SCP) requires that for any $\\varepsilon > 0$ there exists $\\delta > 0$ such that for all $x$ with $0  |x|  \\delta$ there exists a control $u$ with $|u|  \\varepsilon$ rendering $\\dot{V}(x,u)  0$. Use only these fundamental definitions and standard calculus facts.\n\nTasks:\n- Using first principles, verify that $V(x)$ is a CLF for the given system by explicitly exhibiting, for each $x \\neq 0$, a control $u$ that yields $\\dot{V}(x,u)  0$.\n- Fix any $\\alpha \\in (0,1)$ and define the minimal control magnitude function $m(x)$ as the smallest $|u|$ such that\n$$\\dot{V}(x,u) \\leq -\\alpha x^{2} \\quad \\text{for a given} \\quad x \\neq 0.$$\nDerive an exact closed-form expression for $m(x)$ in terms of $x$ and $\\alpha$.\n- Compute the limit\n$$L(\\alpha) = \\lim_{x \\to 0} \\, |x| \\, m(x).$$\n- Briefly explain the implication of your result for the small control property (SCP) at the origin and the existence of a continuous stabilizing state feedback, using only standard definitions and well-known theorems connecting CLFs, the SCP, and continuous stabilizers.\n\nAnswer specification:\nProvide, as your final answer, the exact closed-form expression for $L(\\alpha)$ as a function of $\\alpha$. No rounding is required, and no physical units are involved. The final answer must be a single analytic expression.", "solution": "The problem will be analyzed and solved in a sequential, deductive manner, adhering strictly to the provided definitions and first principles.\n\nFirst, we must validate the problem statement. The problem provides a scalar nonlinear control system, a candidate Control Lyapunov Function (CLF), and a set of tasks involving analysis based on fundamental control theory definitions.\nThe givens are:\n- System dynamics: $\\dot{x} = f(x) + g(x) u$, where $x \\in \\mathbb{R}$ is the state and $u \\in \\mathbb{R}$ is the control input.\n- System functions: $f(x) = x$ and $g(x) = x^{2}$.\n- Candidate CLF: $V(x) = \\frac{1}{2} x^{2}$.\n- CLF definition: $V$ is a positive definite and proper function such that for every $x \\neq 0$, there exists a control $u$ making $\\dot{V}(x,u)  0$.\n- Small control property (SCP) definition: For any $\\varepsilon > 0$, there exists $\\delta > 0$ such that for all $x$ with $0  |x|  \\delta$, there is a control $u$ with $|u|  \\varepsilon$ for which $\\dot{V}(x,u)  0$.\n- Minimal control magnitude $m(x)$: The smallest $|u|$ such that $\\dot{V}(x,u) \\leq -\\alpha x^{2}$ for a given $x \\neq 0$ and a fixed constant $\\alpha \\in (0,1)$.\n- Limit to compute: $L(\\alpha) = \\lim_{x \\to 0} \\, |x| \\, m(x)$.\n\nThe problem is scientifically grounded in the field of nonlinear control theory, is well-posed with all necessary information provided, and is formulated using objective, unambiguous mathematical language. The statements do not violate any scientific principles, are not contradictory, and the tasks are logically structured. Therefore, the problem is deemed valid and a formal solution can be constructed.\n\nThe first task is to verify that $V(x) = \\frac{1}{2} x^{2}$ is a CLF for the system $\\dot{x} = x + x^{2} u$.\nThe function $V(x)$ is positive definite as $V(x)  0$ for all $x \\neq 0$ and $V(0) = 0$. It is also proper (radially unbounded) as $V(x) \\to \\infty$ as $|x| \\to \\infty$. We must now analyze its time derivative, $\\dot{V}$, along the system trajectories.\nUsing the chain rule, the time derivative of $V(x)$ is:\n$$ \\dot{V}(x,u) = \\frac{\\partial V}{\\partial x} \\dot{x} = \\frac{\\partial}{\\partial x} \\left(\\frac{1}{2} x^{2}\\right) (f(x) + g(x) u) $$\nSubstituting the given functions:\n$$ \\dot{V}(x,u) = x (x + x^{2} u) = x^{2} + x^{3} u $$\nIn the language of Lie derivatives, this is $\\dot{V} = L_f V + (L_g V) u$, where $L_f V = \\frac{\\partial V}{\\partial x} f(x) = x \\cdot x = x^{2}$ and $L_g V = \\frac{\\partial V}{\\partial x} g(x) = x \\cdot x^{2} = x^{3}$.\nFor $V(x)$ to be a CLF, for any state $x \\neq 0$, we must be able to find a control $u$ such that $\\dot{V}(x,u)  0$. The condition is:\n$$ x^{2} + x^{3} u  0 $$\nFor any $x \\neq 0$, the term $L_g V = x^{3}$ is non-zero. This is the crucial property that guarantees control authority. We can thus solve for $u$:\n$$ x^{3} u  -x^{2} $$\nIf $x > 0$, then $x^{3} > 0$, and the inequality becomes $u  -\\frac{x^{2}}{x^{3}} = -\\frac{1}{x}$. For example, one can choose $u = -\\frac{2}{x}$. This gives $\\dot{V} = x^2 + x^3(-\\frac{2}{x}) = x^2 - 2x^2 = -x^2  0$.\nIf $x  0$, then $x^{3}  0$, and dividing by a negative number reverses the inequality: $u > -\\frac{x^{2}}{x^{3}} = -\\frac{1}{x}$. For example, choosing $u = -\\frac{2}{x}$ still works. Since $x0$, $-\\frac{2}{x}>0$ and $-\\frac{1}{x}>0$. The condition $u > -\\frac{1}{x}$ is satisfied because $-\\frac{2}{x} > -\\frac{1}{x}$ for $x0$. The derivative is again $\\dot{V} = -x^2  0$.\nSince for any $x \\neq 0$ we can explicitly find a control $u$ that makes $\\dot{V}$ negative, the function $V(x) = \\frac{1}{2} x^{2}$ is a valid CLF for the given system.\n\nThe second task is to derive the minimal control magnitude $m(x)$ that satisfies the condition $\\dot{V}(x,u) \\leq -\\alpha x^{2}$ for a fixed $\\alpha \\in (0,1)$ and a given $x \\neq 0$.\nThe inequality is:\n$$ x^{2} + x^{3} u \\leq -\\alpha x^{2} $$\nRearranging the terms, we get:\n$$ x^{3} u \\leq -x^{2} - \\alpha x^{2} = -(1+\\alpha) x^{2} $$\nWe must find the control $u$ that satisfies this inequality and has the minimum possible absolute value, $|u|$.\nCase 1: $x > 0$. Then $x^{3} > 0$. The inequality is:\n$$ u \\leq -\\frac{(1+\\alpha)x^{2}}{x^{3}} = -\\frac{1+\\alpha}{x} $$\nThe set of admissible controls is $u \\in (-\\infty, -\\frac{1+\\alpha}{x}]$. Since $x > 0$ and $1+\\alpha > 0$, the upper bound $-\\frac{1+\\alpha}{x}$ is negative. The absolute value $|u|$ is minimized at the boundary of this set, where $u$ is closest to zero. Thus, the minimum $|u|$ is $|-\\frac{1+\\alpha}{x}| = \\frac{1+\\alpha}{x}$.\nCase 2: $x  0$. Then $x^{3}  0$. Dividing by $x^3$ reverses the inequality:\n$$ u \\geq -\\frac{(1+\\alpha)x^{2}}{x^{3}} = -\\frac{1+\\alpha}{x} $$\nThe set of admissible controls is $u \\in [-\\frac{1+\\alpha}{x}, \\infty)$. Since $x  0$ and $1+\\alpha > 0$, the lower bound $-\\frac{1+\\alpha}{x}$ is positive. The absolute value $|u|$ is minimized at this lower bound. The minimum $|u|$ is $|-\\frac{1+\\alpha}{x}| = -\\frac{1+\\alpha}{x}$.\nCombining both cases, we can express the minimal control magnitude as a single function of $x$:\n$$ m(x) = \\begin{cases} \\frac{1+\\alpha}{x}  \\text{if } x>0 \\\\ -\\frac{1+\\alpha}{x}  \\text{if } x0 \\end{cases} $$\nThis is equivalent to the compact expression:\n$$ m(x) = \\frac{1+\\alpha}{|x|} $$\n\nThe third task is to compute the limit $L(\\alpha) = \\lim_{x \\to 0} \\, |x| \\, m(x)$.\nSubstituting the derived expression for $m(x)$:\n$$ L(\\alpha) = \\lim_{x \\to 0} \\, |x| \\left(\\frac{1+\\alpha}{|x|}\\right) $$\nFor any $x \\neq 0$, the term $|x|$ in the numerator and denominator cancels.\n$$ L(\\alpha) = \\lim_{x \\to 0} (1+\\alpha) $$\nThe expression $(1+\\alpha)$ is a constant with respect to $x$. Therefore, the limit is simply:\n$$ L(\\alpha) = 1+\\alpha $$\n\nThe final task is to explain the implication of this result for the small control property (SCP) and the existence of a continuous stabilizing feedback.\nThe minimal control magnitude required to achieve a specified rate of decay, $m(x)$, behaves as $\\frac{1+\\alpha}{|x|}$ as $x$ approaches $0$. This means $m(x) \\to \\infty$ as $x \\to 0$. The control effort required to stabilize the system blows up near the origin.\nThe SCP requires that for any arbitrarily small control bound $\\varepsilon > 0$, there exists a neighborhood of the origin (defined by $\\delta > 0$) where a stabilizing control $u$ with $|u|  \\varepsilon$ can be found.\nOur analysis of the minimal control needed to just make $\\dot{V}  0$ showed that one must have $|u| > \\frac{1}{|x|}$. To find a control $|u|  \\varepsilon$, it is necessary that $\\frac{1}{|x|}  \\varepsilon$, which implies $|x| > \\frac{1}{\\varepsilon}$. This condition cannot hold for all $x$ in a neighborhood $0  |x|  \\delta$, because any such neighborhood contains points with $|x|$ arbitrarily close to $0$. Thus, for any given $\\varepsilon$, we cannot find a suitable $\\delta$. The small control property is violated at the origin.\nA cornerstone theorem by Sontag states that a system is asymptotically stabilizable by a continuous state feedback law $u=k(x)$ (with $k(0)=0$) if and only if there exists a smooth CLF that satisfies the SCP.\nSince our CLF—and indeed any CLF for this system—fails to satisfy the SCP at the origin, we conclude that no continuous feedback law can asymptotically stabilize the origin of this system. This is a manifestation of a more general principle, often related to Brockett's necessary condition for continuous stabilization, which is violated here because the control vector field $g(x)=x^2$ vanishes faster than the drift vector field $f(x)=x$ at the origin, thereby providing insufficient control authority. The non-zero value of $L(\\alpha)$ is a quantitative indicator of this failure.", "answer": "$$ \\boxed{1+\\alpha} $$", "id": "2695614"}, {"introduction": "Real-world control systems must operate under uncertainty and physical limitations, where the idealized conditions for CLF synthesis may not hold. This practice problem [@problem_id:2695588] confronts this challenge directly by analyzing a system with bounded disturbances where the standard CLF constraint can become impossible to satisfy. You will explore the mechanism behind this infeasibility and evaluate powerful regularization techniques used in optimization-based controllers to recover robust performance and guarantee practical stability.", "problem": "Consider a control-affine nonlinear system $\\dot{x} = f(x) + g(x)\\,u$ with state $x \\in \\mathbb{R}^{n}$ and control $u \\in \\mathbb{R}^{m}$. A continuously differentiable, positive definite, and radially unbounded function $V:\\mathbb{R}^{n}\\to\\mathbb{R}_{\\ge 0}$ is a Control Lyapunov Function (CLF) if there exists a class-$\\mathcal{K}$ function $\\alpha(\\cdot)$ such that for all $x \\neq 0$ there exists $u$ satisfying $L_{f}V(x) + L_{g}V(x)\\,u \\le -\\alpha(V(x))$, where $L_{f}V(x) := \\nabla V(x)^{\\top} f(x)$ and $L_{g}V(x) := \\nabla V(x)^{\\top} g(x)$. A standard online synthesis enforces a pointwise decrease via a Quadratic Program (QP) of the form: minimize a convex cost in $u$ subject to the affine-in-$u$ CLF inequality and actuator constraints.\n\nYou are given the concrete instance\n- System: $\\dot{x} = A x + B u + d(t)$ with $A = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix}$, $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, and an unknown disturbance $d(t)$ satisfying $\\|d(t)\\|_{2} \\le \\Delta$ for a known bound $\\Delta > 0$.\n- Control bounds: $\\|u\\|_{\\infty} \\le u_{\\max}$ with $u_{\\max} = 1$.\n- CLF: $V(x)=x^{\\top}x$ and target exponential decay rate $c>0$ so that, in the nominal case $d \\equiv 0$, one enforces $L_{f}V(x) + L_{g}V(x)\\,u \\le -c\\,V(x)$.\n- Robust pointwise CLF constraint: to account for the disturbance $d$, require that the worst-case directional derivative satisfies $\\sup_{\\|d\\|_{2}\\le \\Delta} \\nabla V(x)^{\\top}\\big(Ax + Bu + d\\big) \\le -c\\,V(x)$.\n\nPart I (analysis of emptiness). Using only the definitions above, determine whether, at the state $\\bar{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, the robust CLF constraint has any feasible $u$ within the input bounds. Explain the mechanism by which the feasible set in $u$ can become empty.\n\nPart II (regularization for feasibility recovery). In practice the robust CLF constraint set in $u$ can become empty due to modeling errors (e.g., unknown $d$) or due to nonconvex actuator sets (e.g., on-off or dead-zone actuators). Which of the following techniques are sound ways to regularize the synthesis so that a well-posed optimization-based control law is obtained and feasibility is recovered while retaining at least practical stability guarantees under standard assumptions?\n\nA. Augment the CLF inequality with a nonnegative slack $\\delta \\ge 0$ as $L_{f}V(x) + L_{g}V(x)\\,u \\le -c\\,V(x) + \\delta$ (or its robust counterpart), and penalize $\\delta$ strongly in the QP objective with a large weight $\\rho \\gg 0$, for example adding $\\rho\\,\\delta^{2}$. This softens the constraint while discouraging violation.\n\nB. Increase the decay rate $c$ adaptively whenever the constraint appears infeasible, so that the desired decrease is stronger far from the origin. This should force feasibility because a larger $c$ ostensibly promotes faster decay.\n\nC. Replace $\\alpha(V)$ by a state-dependent schedule that becomes very small near the origin, i.e., use a function $\\alpha(V)$ with $\\alpha(V) \\to 0^{+}$ as $V \\to 0^{+}$ while keeping $\\alpha(V)  0$ elsewhere. This reduces the stringency of the inequality for small $V$ and should remove infeasibility.\n\nD. Add a small Tikhonov regularization on the input in the objective, for example minimize $\\tfrac{1}{2}\\,u^{\\top}R\\,u + \\varepsilon \\|u\\|_{2}^{2}$ with $\\varepsilon  0$ and keep all constraints unchanged. This improves numerical conditioning and hence removes infeasibility.\n\nE. When the actuator set is nonconvex in $u$ (e.g., $u \\in \\{-u_{\\max},0, u_{\\max}\\}$), replace it in the optimizer by its convex hull (e.g., $[-u_{\\max}, u_{\\max}]$) so that the CLF constraint becomes convex in $u$, then realize the computed continuous $u$ via high-frequency pulse-width modulation (PWM) to match the time-average. Under a standard time-scale separation assumption, this recovers feasibility of the optimization and achieves practical CLF decrease.\n\nSelect all correct options.", "solution": "The problem statement has been subjected to rigorous validation and is found to be scientifically grounded, well-posed, and complete. It presents a standard, non-trivial problem in robust control using Control Lyapunov Functions (CLFs). We may therefore proceed with the derivation of a solution.\n\nThe problem is divided into two parts. Part I requires an analysis of the feasibility of a robust CLF constraint for a specific system at a given state. Part II asks for an evaluation of several common regularization techniques for optimization-based control laws.\n\n### Part I: Analysis of Feasibility\n\nThe system dynamics are given by $\\dot{x} = A x + B u + d(t)$, where $A = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix}$ and $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. The disturbance $d(t)$ is bounded such that $\\|d(t)\\|_{2} \\le \\Delta$ for some known $\\Delta > 0$. The control is scalar, $u \\in \\mathbb{R}$, and is bounded by $|u| \\le u_{\\max} = 1$.\n\nThe Control Lyapunov Function is $V(x) = x^{\\top}x = x_{1}^{2} + x_{2}^{2}$. Its gradient is $\\nabla V(x) = 2x$.\n\nThe robust pointwise CLF constraint is stated as:\n$$ \\sup_{\\|d\\|_{2}\\le \\Delta} \\nabla V(x)^{\\top}\\big(Ax + Bu + d\\big) \\le -c\\,V(x) $$\nwhere $c > 0$ is the desired exponential decay rate.\n\nLet us expand the term inside the supremum:\n$$ \\nabla V(x)^{\\top}\\big(Ax + Bu + d\\big) = \\nabla V(x)^{\\top}Ax + \\nabla V(x)^{\\top}Bu + \\nabla V(x)^{\\top}d $$\nThe first two terms are the Lie derivatives $L_{f}V(x)$ and $L_{g}V(x)u$ for the nominal system part $f(x)=Ax$ and $g(x)=B$.\n$L_{f}V(x) = (2x)^{\\top}(Ax) = 2\\begin{bmatrix} x_1  x_2 \\end{bmatrix} \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 2\\begin{bmatrix} x_1  x_2 \\end{bmatrix} \\begin{bmatrix} x_2 \\\\ 0 \\end{bmatrix} = 2x_1x_2$.\n$L_{g}V(x)u = (2x)^{\\top}(Bu) = 2\\begin{bmatrix} x_1  x_2 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u = 2x_2 u$.\n\nThe supremum acts on the disturbance term. By the Cauchy-Schwarz inequality, the worst-case effect of the disturbance occurs when $d$ is aligned with $\\nabla V(x)$, hence:\n$$ \\sup_{\\|d\\|_{2}\\le \\Delta} \\nabla V(x)^{\\top}d = \\sup_{\\|d\\|_{2}\\le \\Delta} (2x)^{\\top}d = \\|2x\\|_{2} \\Delta = 2\\|x\\|_{2} \\Delta $$\n\nCombining these terms, the robust CLF constraint becomes:\n$$ 2x_1x_2 + 2x_2 u + 2\\|x\\|_{2} \\Delta \\le -c(x_1^2 + x_2^2) $$\n\nWe are asked to evaluate this constraint at the state $\\bar{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. Substituting $x_1 = 1$ and $x_2 = 0$:\n$$ 2(1)(0) + 2(0) u + 2\\sqrt{1^2+0^2} \\Delta \\le -c(1^2+0^2) $$\n$$ 0 + 0 + 2\\Delta \\le -c $$\n$$ 2\\Delta \\le -c $$\n\nGiven that $\\Delta > 0$ and $c > 0$, the left-hand side ($2\\Delta$) is strictly positive, while the right-hand side ($-c$) is strictly negative. The inequality $2\\Delta \\le -c$ is therefore a contradiction. This demonstrates that at the state $\\bar{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, there is no value of $u \\in \\mathbb{R}$ that can satisfy the robust CLF constraint. The set of feasible controls $u$ is empty.\n\nThe mechanism by which the feasible set becomes empty is a loss of control authority over the CLF derivative. The term multiplying the control input $u$ is $L_gV(x) = 2x_2$. At any state on the $x_1$-axis (where $x_2=0$), including $\\bar{x}$, this term vanishes: $L_gV(\\bar{x}) = 0$. This means the control input has no instantaneous effect on $\\dot{V}$. The evolution of $V$ is then dictated solely by the system's drift and the disturbance: $\\dot{V} = L_f V(x) + \\nabla V(x)^\\top d$. At $\\bar{x}$, the drift term $L_fV(\\bar{x})=0$, but the worst-case disturbance causes a strict increase in $V$, as $\\sup_d \\dot{V} = 2\\Delta > 0$. Since the required condition is $\\dot{V} \\le -c  0$, and the control cannot influence the outcome, the constraint is fundamentally infeasible.\n\n### Part II: Evaluation of Regularization Techniques\n\nWe now evaluate each proposed technique for recovering feasibility in an optimization-based controller when the CLF constraint set becomes empty.\n\n**A. Augment the CLF inequality with a nonnegative slack $\\delta \\ge 0$ ... and penalize $\\delta$ strongly...**\n\nThis method, known as constraint softening, modifies the CLF inequality to $L_{f}V(x) + L_{g}V(x)\\,u \\le -c\\,V(x) + \\delta$ (or the robust equivalent). The optimization problem then seeks to minimize a cost that includes a large penalty on $\\delta$, such as $\\rho\\delta^2$ with $\\rho \\gg 0$. This formulation always has a feasible solution for the pair $(u, \\delta)$; if the original constraint is infeasible for all admissible $u$, one can simply choose a sufficiently large $\\delta > 0$ to satisfy the softened inequality. The optimizer will then find a control $u$ that \"best\" satisfies the original constraint by minimizing $\\delta$. If $\\delta$ must be positive, this indicates a region where the desired exponential decay cannot be met. However, analysis shows that if this region is bounded, this technique guarantees practical stability, meaning trajectories converge to a small neighborhood around the origin whose size depends on the magnitude of the infeasibility and the penalty weight $\\rho$. This is a standard, theoretically sound, and widely used method in CLF-based control.\n**Verdict: Correct.**\n\n**B. Increase the decay rate $c$ adaptively...**\n\nThe proposal is to increase $c$ to recover feasibility. Let us examine the robust CLF inequality rearranged for $u$: $L_{g}V(x)\\,u \\le -c\\,V(x) - L_{f}V(x) - 2\\|x\\|_2\\Delta$. Increasing $c$ makes the right-hand side of the inequality more negative. This makes the constraint *more stringent*, not less. For example, if $L_gV(x) > 0$, the upper bound on $u$ becomes smaller. If $L_gV(x)  0$, the lower bound on $u$ becomes larger. In either case, the set of feasible $u$ shrinks or remains empty. This action is counterproductive to recovering feasibility. This reasoning is fundamentally flawed.\n**Verdict: Incorrect.**\n\n**C. Replace $\\alpha(V)$ by a state-dependent schedule that becomes very small near the origin...**\n\nThe problem of infeasibility, as demonstrated in Part I, is not intrinsically linked to the origin. It occurred at state $\\bar{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, where $V(\\bar{x})=1$, which is not necessarily small. The infeasibility arose because $L_gV(x) = 0$. At such a point, the CLF constraint (in its robust form) becomes $L_fV(x) + 2\\|x\\|_2\\Delta \\le -\\alpha(V(x))$. If $L_fV(x) + 2\\|x\\|_2\\Delta > 0$, this inequality cannot be satisfied for any choice of class-$\\mathcal{K}$ function $\\alpha$, which must be positive for $V(x)>0$. Modifying the behavior of $\\alpha(V)$ *near the origin* has no bearing on infeasibility that occurs far from the origin. The proposed solution does not address the root cause of the problem.\n**Verdict: Incorrect.**\n\n**D. Add a small Tikhonov regularization on the input in the objective...**\n\nTikhonov regularization involves adding a quadratic penalty on the control, such as $\\varepsilon \\|u\\|_{2}^{2}$ with $\\varepsilon > 0$, to the objective function of the QP. This technique is used to ensure the objective function is strictly convex, which guarantees the existence and uniqueness of a solution *if the feasible set is non-empty*. It also improves the numerical conditioning of the problem. However, regularization of the objective function has no effect on the constraint set itself. If the set of controls $u$ satisfying the CLF and actuator constraints is empty, it will remain empty regardless of the choice of objective function. This method cannot recover feasibility.\n**Verdict: Incorrect.**\n\n**E. When the actuator set is nonconvex ... replace it in the optimizer by its convex hull ... then realize the computed continuous $u$ via high-frequency pulse-width modulation (PWM)...**\n\nThis technique addresses infeasibility arising from nonconvex actuator sets (e.g., discrete values). The procedure involves two steps. First, the optimization is performed over the convex hull of the actuator set. This convexifies the problem and enlarges the set of available controls, which can recover feasibility. A solution $u^*$ is found within this convex set. Second, since $u^*$ may not belong to the original nonconvex set, it is realized in an average sense using high-frequency switching (e.g., PWM) among the available discrete control values. Under a time-scale separation assumption (i.e., system dynamics are slow relative to the switching frequency), averaging theory guarantees that the system state will evolve as if driven by the average control $u^*$. This approach effectively achieves the desired CLF decay rate in an averaged sense, leading to practical stability. This is a theoretically sound and well-established method in control engineering, particularly related to sliding mode control and switched systems.\n**Verdict: Correct.**", "answer": "$$\\boxed{AE}$$", "id": "2695588"}]}