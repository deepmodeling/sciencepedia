{"hands_on_practices": [{"introduction": "Before applying the powerful tools of geometric control, one must first achieve fluency in their fundamental mechanics. The Lie derivative measures the rate of change of a scalar function along the flow of a vector field. This exercise provides essential practice in the recursive computation of higher-order Lie derivatives, a skill that is indispensable for analyzing the input-output behavior and observability of nonlinear systems [@problem_id:2710286].", "problem": "Consider a smooth vector field $f:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ and a smooth scalar field $h:\\mathbb{R}^{2}\\to\\mathbb{R}$. The Lie derivative of $h$ along $f$ at a point $x\\in\\mathbb{R}^{2}$ is defined as the directional derivative of $h$ in the direction of $f$, namely $L_{f}h(x)=\\nabla h(x)\\cdot f(x)$. Higher-order Lie derivatives are defined recursively by $L_{f}^{k+1}h=L_{f}\\!\\left(L_{f}^{k}h\\right)$ with $L_{f}^{0}h=h$.  \n\nLet $f(x)=\\begin{pmatrix}x_{2}\\\\0\\end{pmatrix}$ and $h(x)=x_{1}^{3}$, where $x=\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix}\\in\\mathbb{R}^{2}$. Compute the first, second, and third Lie derivatives $L_{f}h$, $L_{f}^{2}h$, and $L_{f}^{3}h$ symbolically and evaluate them at a general point $x$.  \n\nExpress your final answer as a single row matrix whose entries are $L_{f}h(x)$, $L_{f}^{2}h(x)$, and $L_{f}^{3}h(x)$, respectively. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Smooth vector field: $f(x)=\\begin{pmatrix}x_{2}\\\\0\\end{pmatrix}$\n- Smooth scalar field: $h(x)=x_{1}^{3}$\n- State vector: $x=\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix}\\in\\mathbb{R}^{2}$\n- Definition of the Lie derivative: $L_{f}h(x)=\\nabla h(x)\\cdot f(x)$\n- Definition of higher-order Lie derivatives: $L_{f}^{k+1}h=L_{f}\\!\\left(L_{f}^{k}h\\right)$ with $L_{f}^{0}h=h$\n- Task: Compute $L_{f}h(x)$, $L_{f}^{2}h(x)$, and $L_{f}^{3}h(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard exercise in geometric control theory. The functions $f(x)$ and $h(x)$ are smooth (polynomials, thus $C^{\\infty}$). The definitions provided are standard and correct. All necessary information is present, and there are no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\nThe solution requires the iterative computation of Lie derivatives. We begin with the first-order Lie derivative and proceed to the third.\n\n**First Lie Derivative: $L_{f}h(x)$**\nThe Lie derivative of a scalar field $h$ along a vector field $f$ is given by $L_{f}h(x) = \\nabla h(x) \\cdot f(x)$, where $\\nabla h(x)$ is the gradient of $h(x)$.\nFirst, we compute the gradient of $h(x)=x_{1}^{3}$:\n$$\n\\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_{1}} \\\\ \\frac{\\partial h}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_{1}}(x_{1}^{3}) \\\\ \\frac{\\partial}{\\partial x_{2}}(x_{1}^{3}) \\end{pmatrix} = \\begin{pmatrix} 3x_{1}^{2} \\\\ 0 \\end{pmatrix}\n$$\nNow, we compute the dot product with the vector field $f(x) = \\begin{pmatrix}x_{2}\\\\0\\end{pmatrix}$:\n$$\nL_{f}h(x) = \\nabla h(x) \\cdot f(x) = \\begin{pmatrix} 3x_{1}^{2}  0 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = (3x_{1}^{2})(x_{2}) + (0)(0)\n$$\nThus, the first Lie derivative is:\n$$\nL_{f}h(x) = 3x_{1}^{2}x_{2}\n$$\n\n**Second Lie Derivative: $L_{f}^{2}h(x)$**\nThe second Lie derivative is defined as $L_{f}^{2}h(x) = L_{f}(L_{f}h(x))$. Let us define a new scalar field $h_{1}(x) = L_{f}h(x) = 3x_{1}^{2}x_{2}$. We must compute $L_{f}h_{1}(x)$.\nFirst, we find the gradient of $h_{1}(x)$:\n$$\n\\nabla h_{1}(x) = \\begin{pmatrix} \\frac{\\partial h_{1}}{\\partial x_{1}} \\\\ \\frac{\\partial h_{1}}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_{1}}(3x_{1}^{2}x_{2}) \\\\ \\frac{\\partial}{\\partial x_{2}}(3x_{1}^{2}x_{2}) \\end{pmatrix} = \\begin{pmatrix} 6x_{1}x_{2} \\\\ 3x_{1}^{2} \\end{pmatrix}\n$$\nNow, we compute the dot product with $f(x)$:\n$$\nL_{f}^{2}h(x) = \\nabla h_{1}(x) \\cdot f(x) = \\begin{pmatrix} 6x_{1}x_{2}  3x_{1}^{2} \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = (6x_{1}x_{2})(x_{2}) + (3x_{1}^{2})(0)\n$$\nThus, the second Lie derivative is:\n$$\nL_{f}^{2}h(x) = 6x_{1}x_{2}^{2}\n$$\n\n**Third Lie Derivative: $L_{f}^{3}h(x)$**\nThe third Lie derivative is defined as $L_{f}^{3}h(x) = L_{f}(L_{f}^{2}h(x))$. Let us define a new scalar field $h_{2}(x) = L_{f}^{2}h(x) = 6x_{1}x_{2}^{2}$. We must compute $L_{f}h_{2}(x)$.\nFirst, we find the gradient of $h_{2}(x)$:\n$$\n\\nabla h_{2}(x) = \\begin{pmatrix} \\frac{\\partial h_{2}}{\\partial x_{1}} \\\\ \\frac{\\partial h_{2}}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x_{1}}(6x_{1}x_{2}^{2}) \\\\ \\frac{\\partial}{\\partial x_{2}}(6x_{1}x_{2}^{2}) \\end{pmatrix} = \\begin{pmatrix} 6x_{2}^{2} \\\\ 12x_{1}x_{2} \\end{pmatrix}\n$$\nNow, we compute the dot product with $f(x)$:\n$$\nL_{f}^{3}h(x) = \\nabla h_{2}(x) \\cdot f(x) = \\begin{pmatrix} 6x_{2}^{2}  12x_{1}x_{2} \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = (6x_{2}^{2})(x_{2}) + (12x_{1}x_{2})(0)\n$$\nThus, the third Lie derivative is:\n$$\nL_{f}^{3}h(x) = 6x_{2}^{3}\n$$\n\nThe problem requires the results to be expressed as a single row matrix. The entries are the computed Lie derivatives in order.\nThe first derivative is $L_{f}h(x) = 3x_{1}^{2}x_{2}$.\nThe second derivative is $L_{f}^{2}h(x) = 6x_{1}x_{2}^{2}$.\nThe third derivative is $L_{f}^{3}h(x) = 6x_{2}^{3}$.\nThe final answer is the row matrix containing these three expressions.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3x_{1}^{2}x_{2}  6x_{1}x_{2}^{2}  6x_{2}^{3} \\end{pmatrix}}\n$$", "id": "2710286"}, {"introduction": "Moving beyond pure computation, this practice applies Lie derivatives to a concrete control problem, revealing their true analytical power. Lie derivatives allow us to systematically differentiate a system's output along its dynamics to understand how the input affects it. This exercise demonstrates how to find the \"relative degree,\" which quantifies the delay between an input action and its effect on the output, a critical step in designing feedback linearization controllers and assessing system observability [@problem_id:2710283].", "problem": "Consider the control-affine system with state $x \\in \\mathbb{R}^{2}$, input $u \\in \\mathbb{R}$, and output $y \\in \\mathbb{R}$ given by\n$$\n\\dot{x} = f(x) + g(x)\\,u, \\quad y = h(x),\n$$\nwhere\n$$\nf(x) = \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix}, \\quad g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad h(x) = x_{1}^{2}.\n$$\nUsing only the core definitions of Lie derivatives for smooth vector fields and outputs, perform the following tasks:\n- Compute $L_{g} h(x)$ and $L_{f} h(x)$ directly from their definitions.\n- Compute $L_{g}L_{f} h(x)$ and verify that $L_{g} h(x) \\equiv 0$ but $L_{g} L_{f} h(x) \\not\\equiv 0$, thereby showing that the input $u$ affects the output $y$ at a higher derivative order.\n- By differentiating the output along trajectories, derive $\\dot{y}$ and $\\ddot{y}$ in terms of Lie derivatives and the input $u$, and identify where $u$ first appears.\n- Analyze local weak observability using the Hermann–Krener local observability rank condition by constructing the observability codistribution generated by $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$, and determine for which $x$ the rank is full.\n\nProvide the exact analytic expression for $L_{g} L_{f} h(x)$ as your final answer. No rounding is required. There are no physical units involved. Angles are not present in this problem.", "solution": "The posed problem is subject to rigorous validation.\n\n**Step 1: Extract Givens**\nThe control-affine system is defined by:\n- State dynamics: $\\dot{x} = f(x) + g(x)\\,u$, where $x \\in \\mathbb{R}^{2}$ and $u \\in \\mathbb{R}$.\n- Output equation: $y = h(x)$, where $y \\in \\mathbb{R}$.\n- Drift vector field: $f(x) = \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix}$.\n- Control vector field: $g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n- Output function: $h(x) = x_{1}^{2}$.\n\nThe tasks are:\n1.  Compute $L_{g} h(x)$ and $L_{f} h(x)$ using their definitions.\n2.  Compute $L_{g}L_{f} h(x)$ and verify that $L_{g} h(x) \\equiv 0$ but $L_{g} L_{f} h(x) \\not\\equiv 0$.\n3.  Derive the time derivatives $\\dot{y}$ and $\\ddot{y}$ and identify where the input $u$ first appears.\n4.  Analyze local weak observability using the Hermann–Krener rank condition on the codistribution generated by $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in nonlinear control theory. The system, functions, and tasks are well-defined and mathematically sound. The vector fields $f(x)$ and $g(x)$, and the output function $h(x)$, are smooth (infinitely differentiable) functions on $\\mathbb{R}^{2}$. The concepts of Lie derivatives and observability are central to the specified topic. The problem is self-contained, scientifically grounded, and objective. It does not violate any of the specified invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe state vector is $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$. The output function is $h(x) = x_{1}^{2}$. The Lie derivative of a scalar function $\\phi(x)$ with respect to a vector field $v(x)$ is defined as $L_{v}\\phi(x) = \\frac{\\partial \\phi}{\\partial x} v(x) = \\nabla \\phi(x) \\cdot v(x)$.\n\nFirst, we must compute the gradient of the output function $h(x)$:\n$$\n\\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_{1}}  \\frac{\\partial h}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2x_{1}  0 \\end{pmatrix}\n$$\n\n**Task 1: Computation of $L_{g} h(x)$ and $L_{f} h(x)$**\n\nUsing the definition of the Lie derivative, we compute $L_{f} h(x)$:\n$$\nL_{f} h(x) = \\nabla h(x) \\cdot f(x) = \\begin{pmatrix} 2x_{1}  0 \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = (2x_{1})(x_{2}) + (0)(0) = 2x_{1}x_{2}\n$$\nNext, we compute $L_{g} h(x)$:\n$$\nL_{g} h(x) = \\nabla h(x) \\cdot g(x) = \\begin{pmatrix} 2x_{1}  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (2x_{1})(0) + (0)(1) = 0\n$$\n\n**Task 2: Computation of $L_{g}L_{f} h(x)$ and Verification**\n\nTo compute $L_{g}L_{f} h(x)$, we treat $L_{f}h(x)$ as a new scalar function, which we will call $\\psi(x) = L_{f}h(x) = 2x_{1}x_{2}$. We must find the Lie derivative of $\\psi(x)$ with respect to $g(x)$. First, we compute the gradient of $\\psi(x)$:\n$$\n\\nabla \\psi(x) = \\nabla (L_{f}h(x)) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_{1}}(2x_{1}x_{2})  \\frac{\\partial}{\\partial x_{2}}(2x_{1}x_{2}) \\end{pmatrix} = \\begin{pmatrix} 2x_{2}  2x_{1} \\end{pmatrix}\n$$\nNow, we compute $L_{g}L_{f} h(x) = L_{g}\\psi(x)$:\n$$\nL_{g}L_{f} h(x) = \\nabla(L_{f}h(x)) \\cdot g(x) = \\begin{pmatrix} 2x_{2}  2x_{1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (2x_{2})(0) + (2x_{1})(1) = 2x_{1}\n$$\nWe have shown that $L_{g} h(x) = 0$ for all $x \\in \\mathbb{R}^{2}$, which is an identity. We have also shown that $L_{g}L_{f} h(x) = 2x_{1}$, which is not identically zero (it is only zero on the subspace where $x_{1}=0$). This demonstrates that the input $u$ does not have an instantaneous effect on the output $y$, but it does affect a higher-order derivative of $y$.\n\n**Task 3: Derivation of Output Derivatives**\n\nThe first time derivative of the output $y = h(x(t))$ is given by the chain rule:\n$$\n\\dot{y} = \\frac{d}{dt}h(x(t)) = \\frac{\\partial h}{\\partial x} \\frac{dx}{dt} = \\nabla h(x) \\cdot \\dot{x}\n$$\nSubstituting the system dynamics $\\dot{x} = f(x) + g(x)u$:\n$$\n\\dot{y} = \\nabla h(x) \\cdot (f(x) + g(x)u) = \\nabla h(x) \\cdot f(x) + (\\nabla h(x) \\cdot g(x))u\n$$\nUsing the Lie derivative notation:\n$$\n\\dot{y} = L_{f} h(x) + (L_{g} h(x))u\n$$\nFrom our previous calculations, $L_{f} h(x) = 2x_{1}x_{2}$ and $L_{g} h(x) = 0$. Thus:\n$$\n\\dot{y} = 2x_{1}x_{2}\n$$\nThe input $u$ does not appear in the expression for $\\dot{y}$. We proceed to the second derivative, $\\ddot{y}$:\n$$\n\\ddot{y} = \\frac{d}{dt} \\dot{y} = \\frac{d}{dt} (L_{f} h(x(t))) = \\frac{\\partial (L_{f} h(x))}{\\partial x} \\frac{dx}{dt} = \\nabla(L_{f}h(x)) \\cdot \\dot{x}\n$$\nAgain, substituting the dynamics $\\dot{x} = f(x) + g(x)u$:\n$$\n\\ddot{y} = \\nabla(L_{f}h(x)) \\cdot (f(x) + g(x)u) = \\nabla(L_{f}h(x)) \\cdot f(x) + (\\nabla(L_{f}h(x)) \\cdot g(x))u\n$$\nThis is precisely the definition of iterated Lie derivatives:\n$$\n\\ddot{y} = L_{f}^{2}h(x) + (L_{g}L_{f}h(x))u\n$$\nWe need to compute $L_{f}^{2}h(x) = L_{f}(L_{f}h(x))$. We already have $\\nabla(L_{f}h(x)) = \\begin{pmatrix} 2x_{2}  2x_{1} \\end{pmatrix}$.\n$$\nL_{f}^{2}h(x) = \\nabla(L_{f}h(x)) \\cdot f(x) = \\begin{pmatrix} 2x_{2}  2x_{1} \\end{pmatrix} \\begin{pmatrix} x_{2} \\\\ 0 \\end{pmatrix} = 2x_{2}^{2}\n$$\nSubstituting this and our previous result for $L_{g}L_{f}h(x)$ into the expression for $\\ddot{y}$:\n$$\n\\ddot{y} = 2x_{2}^{2} + (2x_{1})u\n$$\nThe input $u$ first appears in the second time derivative of the output, $\\ddot{y}$. This implies the system has a relative degree of $r=2$ at all points where $L_{g}L_{f}h(x) = 2x_{1} \\neq 0$.\n\n**Task 4: Local Weak Observability Analysis**\n\nThe Hermann–Krener observability rank condition states that the system is locally weakly observable at a point $x$ if the rank of the observability matrix, formed by the gradients of successive Lie derivatives of $h(x)$ along the system's vector fields, is equal to the dimension of the state space, which is $n=2$. The problem specifies constructing the codistribution from the covectors $\\mathrm{d} h(x)$ and $\\mathrm{d} L_{f} h(x)$. In this context, $\\mathrm{d} \\phi(x)$ is the gradient $\\nabla \\phi(x)$.\n\nThe covectors are:\n- $\\mathrm{d} h(x) = \\nabla h(x) = \\begin{pmatrix} 2x_{1}  0 \\end{pmatrix}$\n- $\\mathrm{d} L_{f} h(x) = \\nabla(L_{f}h(x)) = \\begin{pmatrix} 2x_{2}  2x_{1} \\end{pmatrix}$\n\nWe form the observability matrix $M_{O}(x)$ by stacking these covectors:\n$$\nM_{O}(x) = \\begin{pmatrix} \\mathrm{d} h(x) \\\\ \\mathrm{d} L_{f} h(x) \\end{pmatrix} = \\begin{pmatrix} 2x_{1}  0 \\\\ 2x_{2}  2x_{1} \\end{pmatrix}\n$$\nThe rank of this $2 \\times 2$ matrix is full (equal to $2$) if and only if its determinant is non-zero.\n$$\n\\det(M_{O}(x)) = (2x_{1})(2x_{1}) - (0)(2x_{2}) = 4x_{1}^{2}\n$$\nFor the rank to be full, we require $\\det(M_{O}(x)) \\neq 0$, which implies $4x_{1}^{2} \\neq 0$. This condition holds if and only if $x_{1} \\neq 0$.\n\nTherefore, the system is locally weakly observable for all $x \\in \\mathbb{R}^{2}$ such that $x_{1} \\neq 0$. The observability is lost on the set of points where $x_{1} = 0$, which is the $x_{2}$-axis. This is logical, as for any state on this axis, $x=(0, x_2)$, the output is $y=h(x)=0^2=0$ and its first derivative is $\\dot{y}=L_f h(x) = 2(0)x_2=0$, making it impossible to distinguish between different values of $x_2$ from the output and its derivatives.", "answer": "$$\n\\boxed{2x_{1}}\n$$", "id": "2710283"}, {"introduction": "While Lie derivatives are ideal for analyzing outputs, the Lie bracket is the primary tool for understanding a system's controllability. Intuitively, the Lie bracket $[f, g]$ quantifies the new direction of motion created by infinitesimally alternating flows along two vector fields, $f$ and $g$. This practice challenges you to use iterated Lie brackets to verify the Lie Algebra Rank Condition (LARC), a cornerstone result that determines if a system's control inputs can generate motion in all directions locally [@problem_id:2710214].", "problem": "Consider the control-affine system on $\\mathbb{R}^{4}$ with state $x=(x_{1},x_{2},x_{3},x_{4})$ and control vector fields $f_{1}$, $f_{2}$, $f_{3}$ given in standard coordinates by $f_{1}(x)=(1,0,0,0)$, $f_{2}(x)=(0,1,0,0)$, and $f_{3}(x)=(0,0,x_{2},1)$. Let the dynamics be $\\dot{x}=u_{1}f_{1}(x)+u_{2}f_{2}(x)+u_{3}f_{3}(x)$, where $u_{1},u_{2},u_{3}$ are measurable and locally bounded inputs. Use the following fundamental definitions as your starting point:\n\n- A smooth vector field on $\\mathbb{R}^{n}$ is a smooth map $f:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$.\n- The Lie bracket of smooth vector fields $X,Y:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ is defined by $[X,Y](x)=\\mathrm{D}Y(x)\\,X(x)-\\mathrm{D}X(x)\\,Y(x)$, where $\\mathrm{D}Y(x)$ denotes the Jacobian matrix of $Y$ at $x$.\n- The Lie Algebra Rank Condition (LARC) at a point $x_{0}$ holds if the linear span of the evaluations at $x_{0}$ of all iterated Lie brackets of the control vector fields equals the tangent space $T_{x_{0}}\\mathbb{R}^{n}$, which is naturally identified with $\\mathbb{R}^{n}$.\n\nCompute the vector subspace of $T_{0}\\mathbb{R}^{4}$ spanned by $\\{f_{i}(0)\\}$ together with all their iterated Lie brackets evaluated at $0$. From this, determine whether the Lie Algebra Rank Condition holds at $0$. For your final numeric answer, report only the dimension of this span at $0$. No rounding is needed. The answer is unitless and should be reported as a single integer.", "solution": "The problem as stated is a well-posed exercise in geometric nonlinear control theory, specifically concerning the verification of the Lie Algebra Rank Condition (LARC) for a control-affine system. The provided definitions and data are complete, consistent, and scientifically sound. I will proceed with the analysis.\n\nThe system dynamics are defined on the state space $\\mathbb{R}^{4}$ by $\\dot{x}=u_{1}f_{1}(x)+u_{2}f_{2}(x)+u_{3}f_{3}(x)$, where the state is $x=(x_{1},x_{2},x_{3},x_{4})$ and the control vector fields are given by:\n$$f_{1}(x) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{2}(x) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{3}(x) = \\begin{pmatrix} 0 \\\\ 0 \\\\ x_{2} \\\\ 1 \\end{pmatrix}$$\nThe task is to determine the dimension of the subspace of the tangent space $T_{0}\\mathbb{R}^{4}$ spanned by the set of vector fields $\\{f_1, f_2, f_3\\}$ and all their iterated Lie brackets, evaluated at the origin $x=0$. The tangent space $T_{0}\\mathbb{R}^{4}$ is identified with $\\mathbb{R}^{4}$.\n\nFirst, we evaluate the given vector fields at the point $x=0$:\n$$f_{1}(0) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{2}(0) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f_{3}(0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\nLet $\\mathcal{S}_{0} = \\mathrm{span}\\{f_{1}(0), f_{2}(0), f_{3}(0)\\}$. These three vectors are evidently linearly independent. They correspond to the standard basis vectors $e_{1}$, $e_{2}$, and $e_{4}$ in $\\mathbb{R}^{4}$. Thus, the dimension of this initial subspace is $\\dim(\\mathcal{S}_{0}) = 3$.\nSince the dimension of the state space is $4$, we must compute Lie brackets to determine if additional linearly independent directions can be generated.\n\nThe Lie bracket of two vector fields $X$ and $Y$ is defined as $[X,Y](x) = \\mathrm{D}Y(x)X(x) - \\mathrm{D}X(x)Y(x)$, where $\\mathrm{D}F(x)$ represents the Jacobian matrix of a vector field $F$ at $x$. We compute the Jacobian matrices for $f_1$, $f_2$, and $f_3$:\nSince $f_1$ and $f_2$ are constant vector fields, their Jacobian matrices are the $4 \\times 4$ zero matrix, $\\mathbf{0}$:\n$$\\mathrm{D}f_{1}(x) = \\mathbf{0}, \\quad \\mathrm{D}f_{2}(x) = \\mathbf{0}$$\nFor $f_{3}(x)$, we have components $f_{3,1}=0$, $f_{3,2}=0$, $f_{3,3}=x_2$, $f_{3,4}=1$. The only non-zero partial derivative is $\\frac{\\partial f_{3,3}}{\\partial x_{2}} = 1$. The Jacobian matrix is:\n$$\\mathrm{D}f_{3}(x) = \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$\nThis Jacobian is constant with respect to $x$.\n\nNow, we compute the first-order Lie brackets:\n1.  $[f_{1}, f_{2}](x) = \\mathrm{D}f_{2}(x)f_{1}(x) - \\mathrm{D}f_{1}(x)f_{2}(x) = \\mathbf{0} \\cdot f_{1}(x) - \\mathbf{0} \\cdot f_{2}(x) = 0$. This bracket yields the zero vector field and provides no new information.\n2.  $[f_{1}, f_{3}](x) = \\mathrm{D}f_{3}(x)f_{1}(x) - \\mathrm{D}f_{1}(x)f_{3}(x) = \\mathrm{D}f_{3}(x)f_{1}(x) - \\mathbf{0} \\cdot f_{3}(x) = \\mathrm{D}f_{3}(x)f_{1}(x)$.\n    $$\\mathrm{D}f_{3}(x)f_{1}(x) = \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n    So, $[f_{1}, f_{3}](x) = 0$. This bracket is also trivial.\n3.  $[f_{2}, f_{3}](x) = \\mathrm{D}f_{3}(x)f_{2}(x) - \\mathrm{D}f_{2}(x)f_{3}(x) = \\mathrm{D}f_{3}(x)f_{2}(x) - \\mathbf{0} \\cdot f_{3}(x) = \\mathrm{D}f_{3}(x)f_{2}(x)$.\n    $$\\mathrm{D}f_{3}(x)f_{2}(x) = \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n    This yields a non-zero, constant vector field: $[f_2, f_3](x) = (0, 0, 1, 0)^{T}$.\n\nLet us evaluate this new vector at $x=0$:\n$$[f_2, f_3](0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThis vector corresponds to the standard basis vector $e_{3}$ in $\\mathbb{R}^{4}$. We check if this vector is in the span $\\mathcal{S}_{0}$. A generic vector in $\\mathcal{S}_{0}$ has the form $c_{1}f_{1}(0) + c_{2}f_{2}(0) + c_{3}f_{3}(0) = (c_{1}, c_{2}, 0, c_{3})^{T}$. It is impossible for this to equal $(0, 0, 1, 0)^{T}$ for any choice of scalars $c_{1}, c_{2}, c_{3}$. Therefore, $[f_2, f_3](0)$ is linearly independent of $\\{f_{1}(0), f_{2}(0), f_{3}(0)\\}$.\n\nWe augment our set of vectors with this new vector and define the new subspace $\\mathcal{S}_{1} = \\mathrm{span}\\{f_{1}(0), f_{2}(0), f_{3}(0), [f_2, f_3](0)\\}$. The set of vectors is:\n$$\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\}$$\nThis is the set of standard basis vectors $\\{e_1, e_2, e_4, e_3\\}$ for $\\mathbb{R}^{4}$. These four vectors are linearly independent and form a basis for $\\mathbb{R}^{4}$.\nThe dimension of the subspace spanned by these vectors is $\\dim(\\mathcal{S}_{1}) = 4$.\n\nSince the dimension of this subspace is $4$, which is equal to the dimension of the state space $\\mathbb{R}^{4}$, the span of the evaluated vector fields and their brackets is the entire tangent space $T_{0}\\mathbb{R}^{4}$. Any further iterated Lie brackets will produce vectors that are already in this span. For instance, any bracket involving the constant vector $[f_2, f_3]$ with one of the constant vector fields $f_1$ or $f_2$ will be zero. A bracket with $f_3$ will also result in a zero vector. The process terminates.\n\nThe vector subspace of $T_{0}\\mathbb{R}^{4}$ spanned by the control vector fields and all their iterated Lie brackets is $\\mathbb{R}^{4}$ itself. The dimension of this subspace is $4$. Consequently, the Lie Algebra Rank Condition is satisfied at the origin $x=0$. The question asks for the dimension of this span.", "answer": "$$\\boxed{4}$$", "id": "2710214"}]}