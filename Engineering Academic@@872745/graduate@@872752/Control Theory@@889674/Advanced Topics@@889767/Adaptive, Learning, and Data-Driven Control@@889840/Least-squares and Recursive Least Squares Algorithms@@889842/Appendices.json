{"hands_on_practices": [{"introduction": "To truly master an algorithm, it is essential to understand its origins. This exercise guides you through the process of deriving the Recursive Least Squares (RLS) algorithm from its fundamental weighted least-squares cost function. By working through the scalar case from first principles, you will build a solid intuition for how the recursive updates for the parameter estimate and its covariance arise naturally from the optimization problem, and how a forgetting factor influences the estimator's steady-state properties [@problem_id:2718871].", "problem": "Consider the scalar linear regression model in discrete time given by the measurements $y_k \\in \\mathbb{R}$, generated according to $y_k = \\varphi \\,\\theta^{\\star} + v_k$, where the regressor $\\varphi \\in \\mathbb{R}$ is a known, nonzero constant (i.e., $\\varphi \\neq 0$), the unknown constant parameter is $\\theta^{\\star} \\in \\mathbb{R}$, and $\\{v_k\\}$ is a zero-mean, independent, identically distributed measurement noise sequence with finite variance. You are to estimate $\\theta^{\\star}$ online by minimizing, at each time $k \\geq 1$, the exponentially weighted least-squares cost\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1},\n$$\nwhere $\\lambda \\in (0,1)$ is a known forgetting factor, $\\theta_0 \\in \\mathbb{R}$ is the prior mean, and $P_00$ is the prior variance. Starting from the core definition of the least-squares estimator as the minimizer of $J_k(\\theta)$, derive the scalar Recursive Least Squares (RLS) recursion for the estimate $\\theta_k$ and its associated covariance $P_k$, without assuming any pre-stated recursion. Then, specializing to the constant regressor case, establish a closed-form expression for $P_k$ and use it to analyze the convergence properties of $\\theta_k$ and $P_k$ as $k \\to \\infty$ under the stated assumptions on the noise. As your final answer, report the closed-form limit $\\lim_{k\\to\\infty} P_k$ in terms of $\\lambda$ and $\\varphi$ only. No numerical rounding is required, and no physical units are involved.", "solution": "The problem presented is a standard exercise in recursive parameter estimation and is scientifically grounded, well-posed, and objective. It is therefore deemed valid. We proceed with the derivation and analysis as requested.\n\nThe objective is to find the parameter estimate $\\theta_k$ that minimizes the exponentially weighted least-squares cost function at each time step $k \\geq 1$:\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1}\n$$\nwhere $\\lambda \\in (0,1)$. Since $J_k(\\theta)$ is a quadratic and convex function of $\\theta$, its minimum can be found by setting its first derivative with respect to $\\theta$ to zero.\n\nFirst, we compute the derivative:\n$$\n\\frac{dJ_k(\\theta)}{d\\theta} = \\sum_{i=1}^{k} \\lambda^{k-i} \\cdot 2 \\big(y_i - \\varphi \\,\\theta\\big) (-\\varphi) + \\lambda^{k} \\cdot 2 (\\theta - \\theta_0) P_0^{-1}\n$$\nSetting $\\frac{dJ_k(\\theta)}{d\\theta} = 0$ at $\\theta = \\theta_k$ and dividing by $2$:\n$$\n-\\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\varphi^2 \\theta_k \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} (\\theta_k - \\theta_0) = 0\n$$\nRearranging the terms to solve for $\\theta_k$:\n$$\n\\theta_k \\left( \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} \\right) = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0\n$$\nThe Hessian of the cost function is $\\frac{d^2J_k(\\theta)}{d\\theta^2}$, and the covariance of the estimate, $P_k$, is defined as the inverse of half the Hessian. Thus,\n$$\nP_k^{-1} \\triangleq \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\nWith this definition, the batch solution for $\\theta_k$ is:\n$$\n\\theta_k = P_k \\left( \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 \\right)\n$$\nTo derive the recursive form, we first establish a recursion for $P_k^{-1}$. We relate $P_k^{-1}$ to $P_{k-1}^{-1}$:\n$$\nP_k^{-1} = \\varphi^2 \\lambda^{k-k} + \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\n$$\nP_k^{-1} = \\varphi^2 + \\lambda \\left( \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-1-i} + \\lambda^{k-1} P_0^{-1} \\right)\n$$\nThe term in the parenthesis is precisely the definition of $P_{k-1}^{-1}$. This yields the simple recursion for the inverse covariance:\n$$\nP_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2\n$$\nTo obtain the recursion for $P_k$, we apply the Sherman-Morrison-Woodbury formula (or, in this scalar case, simple inversion):\n$$\nP_k = (\\lambda P_{k-1}^{-1} + \\varphi^2)^{-1} = \\left(\\frac{\\lambda}{P_{k-1}} + \\varphi^2\\right)^{-1} = \\left(\\frac{\\lambda + \\varphi^2 P_{k-1}}{P_{k-1}}\\right)^{-1} = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\nNow, we derive the recursion for $\\theta_k$. We expand the expression for $P_k^{-1}\\theta_k$:\n$$\nP_k^{-1}\\theta_k = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 = \\varphi y_k + \\lambda \\left( \\varphi \\sum_{i=1}^{k-1} \\lambda^{k-1-i} y_i + \\lambda^{k-1} P_0^{-1} \\theta_0 \\right)\n$$\nRecognizing the term in the parenthesis as $P_{k-1}^{-1}\\theta_{k-1}$, we have:\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + \\lambda P_{k-1}^{-1} \\theta_{k-1}\n$$\nSubstituting $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$, which implies $\\lambda P_{k-1}^{-1} = P_k^{-1} - \\varphi^2$:\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + (P_k^{-1} - \\varphi^2) \\theta_{k-1}\n$$\nMultiplying by $P_k$ from the left:\n$$\n\\theta_k = P_k \\varphi y_k + (I - P_k \\varphi^2) \\theta_{k-1} = \\theta_{k-1} + P_k \\varphi (y_k - \\varphi \\theta_{k-1})\n$$\nThe complete scalar RLS recursion is thus:\n$$\n\\theta_k = \\theta_{k-1} + K_k (y_k - \\varphi \\theta_{k-1})\n$$\n$$\nK_k = P_k \\varphi = \\frac{P_{k-1}\\varphi}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n$$\nP_k = (1 - K_k \\varphi) P_{k-1} \\lambda^{-1} \\text{ or directly } P_k = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\nNext, we find a closed-form expression for $P_k$. The recursion $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$ is a linear first-order non-homogeneous difference equation for the variable $x_k \\triangleq P_k^{-1}$. The solution is obtained by unrolling the recursion:\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\sum_{j=0}^{k-1} \\lambda^j\n$$\nThe finite geometric series sums to $\\sum_{j=0}^{k-1} \\lambda^j = \\frac{1-\\lambda^k}{1-\\lambda}$. Substituting this gives the closed-form for $P_k^{-1}$:\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\left(\\frac{1-\\lambda^k}{1-\\lambda}\\right)\n$$\nThe closed-form expression for $P_k$ is the inverse of this:\n$$\nP_k = \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right)^{-1}\n$$\nWe now analyze the convergence as $k \\to \\infty$. Given that $\\lambda \\in (0,1)$, we have $\\lim_{k\\to\\infty} \\lambda^k = 0$.\nThe limit of the inverse covariance is:\n$$\n\\lim_{k\\to\\infty} P_k^{-1} = \\lim_{k\\to\\infty} \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right) = 0 \\cdot P_0^{-1} + \\frac{\\varphi^2(1-0)}{1-\\lambda} = \\frac{\\varphi^2}{1-\\lambda}\n$$\nSince $\\varphi \\neq 0$ and $\\lambda \\in (0,1)$, this limit is finite and positive. The limit of the covariance $P_k$ is the reciprocal of this value:\n$$\n\\lim_{k\\to\\infty} P_k = \\left( \\lim_{k\\to\\infty} P_k^{-1} \\right)^{-1} = \\left(\\frac{\\varphi^2}{1-\\lambda}\\right)^{-1} = \\frac{1-\\lambda}{\\varphi^2}\n$$\nFor the convergence of $\\theta_k$, consider the estimation error $\\tilde{\\theta}_k \\triangleq \\theta_k - \\theta^{\\star}$. The error dynamics are given by $\\tilde{\\theta}_k = (1 - P_k \\varphi^2)\\tilde{\\theta}_{k-1} + P_k \\varphi v_k$. As $k \\to \\infty$, $P_k \\to \\frac{1-\\lambda}{\\varphi^2}$, so the contraction factor $(1 - P_k \\varphi^2) \\to (1 - (\\frac{1-\\lambda}{\\varphi^2})\\varphi^2) = \\lambda$. Since $|\\lambda|  1$, the estimate is asymptotically unbiased, i.e., $\\lim_{k\\to\\infty} E[\\theta_k] = \\theta^{\\star}$. The variance of the estimation error converges to a non-zero steady-state value $E[\\tilde{\\theta}_{\\infty}^2] = \\frac{(1-\\lambda)\\sigma_v^2}{\\varphi^2(1+\\lambda)}$, where $\\sigma_v^2$ is the variance of the noise $v_k$. This implies that $\\theta_k$ converges in distribution to a random variable centered around the true value $\\theta^{\\star}$, not to the value itself. This is the expected behavior for RLS with a forgetting factor $\\lambda  1$.\n\nThe problem specifically asks for the closed-form limit of $P_k$.", "answer": "$$\\boxed{\\frac{1-\\lambda}{\\varphi^2}}$$", "id": "2718871"}, {"introduction": "While the normal equations provide a direct analytical path to the least-squares solution, they are notoriously sensitive to numerical errors in finite-precision arithmetic, especially with ill-conditioned data. This practice explores a more robust implementation based on QR factorization, demonstrating why it is the preferred approach in high-quality numerical software. You will derive the solution via QR decomposition and provide a formal justification for its superior numerical stability by analyzing the problem's condition number [@problem_id:2718842].", "problem": "Consider the overdetermined linear model in control-oriented system identification, where a measured output vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ is related to a regressor matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ and an unknown parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}$ by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, with $m \\geq n$ and $\\mathrm{rank}(\\mathbf{X}) = n$. The least-squares (LS) estimate $\\widehat{\\boldsymbol{\\theta}}$ minimizes the Euclidean norm of the residual, that is, $\\widehat{\\boldsymbol{\\theta}} \\in \\arg\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$. Assume a thin orthogonal-triangular factorization $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ with $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ satisfying $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$ and $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ upper triangular with positive diagonal entries.\n\nUsing only the definition of the least-squares criterion $\\min_{\\boldsymbol{\\theta}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$, the fact that multiplication by a matrix with orthonormal columns preserves the Euclidean norm of components in its column space, and standard properties of upper triangular systems, derive an explicit expression for the unique minimizer $\\widehat{\\boldsymbol{\\theta}}$ in terms of $\\mathbf{Q}$, $\\mathbf{R}$, and $\\mathbf{y}$.\n\nThen, justify from first principles of numerical linear algebra and finite-precision arithmetic why solving the least-squares problem via the $\\mathbf{Q}\\mathbf{R}$ factorization is numerically more stable than forming and solving the normal equations $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. Your justification must be grounded in the Euclidean norm, conditioning, and the effect of squaring the condition number.\n\nFinally, compute the least-squares estimate for the specific data\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n0  1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\n2 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\nUse your $\\mathbf{Q}\\mathbf{R}$-based derivation to obtain $\\widehat{\\boldsymbol{\\theta}}$ without forming $\\mathbf{X}^{\\top}\\mathbf{X}$. Report the final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete solution. We shall proceed in three parts as requested.\n\nFirst, we derive the least-squares estimator $\\widehat{\\boldsymbol{\\theta}}$ using the provided thin orthogonal-triangular ($\\mathbf{Q}\\mathbf{R}$) factorization of the regressor matrix $\\mathbf{X}$. The objective is to find $\\widehat{\\boldsymbol{\\theta}}$ that minimizes the squared Euclidean norm of the residual vector $\\mathbf{r} = \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}$, which is given by the cost function $J(\\boldsymbol{\\theta}) = \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}$.\n\nWe substitute the thin $\\mathbf{Q}\\mathbf{R}$ factorization $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ into the cost function:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}\n$$\nHere, $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns, meaning $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$. To utilize the norm-preserving property of orthogonal matrices, we consider a full orthogonal matrix $\\mathbf{Q}_{\\text{full}} \\in \\mathbb{R}^{m \\times m}$, which can be constructed by appending $m-n$ orthonormal columns $\\mathbf{Q}_{\\perp}$ to $\\mathbf{Q}$, such that $\\mathbf{Q}_{\\text{full}} = \\begin{pmatrix} \\mathbf{Q}  \\mathbf{Q}_{\\perp} \\end{pmatrix}$. Since $\\mathbf{Q}_{\\text{full}}$ is an orthogonal matrix, $\\mathbf{Q}_{\\text{full}}^{\\top}\\mathbf{Q}_{\\text{full}} = \\mathbf{Q}_{\\text{full}}\\mathbf{Q}_{\\text{full}}^{\\top} = \\mathbf{I}_{m}$.\n\nMultiplication of a vector by an orthogonal matrix preserves its Euclidean norm. Thus, we have:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y})\\|_{2}^{2}\n$$\nWe expand the expression inside the norm using the partitioned form of $\\mathbf{Q}_{\\text{full}}^{\\top} = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}$:\n$$\n\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nUsing the property $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$, this simplifies to:\n$$\n\\begin{pmatrix} \\mathbf{I}_{n}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nThe squared norm of this partitioned vector is the sum of the squared norms of its components:\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} + \\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}\n$$\nTo minimize $J(\\boldsymbol{\\theta})$, we must choose $\\boldsymbol{\\theta}$ to minimize this sum. The second term, $\\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}$, is the squared norm of the residual component orthogonal to the column space of $\\mathbf{X}$ and is independent of $\\boldsymbol{\\theta}$. The minimization is therefore achieved by minimizing the first term. The minimum possible value for a squared norm is $0$. This minimum is attainable, so we set the first term to zero:\n$$\n\\|\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} = 0 \\implies \\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\nThis yields the upper triangular system of linear equations:\n$$\n\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}\n$$\nThe problem states that $\\mathrm{rank}(\\mathbf{X}) = n$ and that $\\mathbf{R}$ has positive diagonal entries. This ensures that $\\mathbf{R}$ is invertible. The unique least-squares estimate $\\widehat{\\boldsymbol{\\theta}}$ is therefore given by:\n$$\n\\widehat{\\boldsymbol{\\theta}} = \\mathbf{R}^{-1}\\mathbf{Q}^{\\top}\\mathbf{y}\n$$\nThis system is efficiently solved by back substitution without explicitly computing $\\mathbf{R}^{-1}$.\n\nSecond, we justify the superior numerical stability of the $\\mathbf{Q}\\mathbf{R}$ method over solving the normal equations $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. The stability of solving a linear system is related to the condition number of the system matrix. The condition number of a matrix $\\mathbf{A}$ with respect to the Euclidean norm is $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$. For a full-rank rectangular matrix $\\mathbf{X}$, the condition number of the least-squares problem is $\\kappa_2(\\mathbf{X}) = \\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $\\mathbf{X}$.\n\nThe normal equations method involves forming the matrix $\\mathbf{A} = \\mathbf{X}^{\\top}\\mathbf{X}$ and then solving the $n \\times n$ system $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$. The condition number of this system is $\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$. The eigenvalues of the symmetric positive definite matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ are the squares of the singular values of $\\mathbf{X}$. Thus, $\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\max}(\\mathbf{X})^2$ and $\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\min}(\\mathbf{X})^2$. The condition number of $\\mathbf{X}^{\\top}\\mathbf{X}$ is:\n$$\n\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X}) = \\frac{\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X})}{\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X})} = \\frac{\\sigma_{\\max}(\\mathbf{X})^2}{\\sigma_{\\min}(\\mathbf{X})^2} = \\left(\\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}\\right)^2 = (\\kappa_2(\\mathbf{X}))^2\n$$\nThis shows that forming the normal equations squares the condition number of the original problem. If $\\mathbf{X}$ is ill-conditioned (i.e., $\\kappa_2(\\mathbf{X})$ is large), $\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$ will be much larger, severely amplifying the effects of finite-precision arithmetic and round-off errors. Information can be lost in the very formation of $\\mathbf{X}^{\\top}\\mathbf{X}$ before solving the system.\n\nIn contrast, the $\\mathbf{Q}\\mathbf{R}$ method solves the system $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}$. The condition number of this system is $\\kappa_2(\\mathbf{R})$. Since $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ and $\\mathbf{Q}$ has orthonormal columns, $\\mathbf{Q}$ acts as an isometry. It can be shown that $\\kappa_2(\\mathbf{X}) = \\kappa_2(\\mathbf{Q}\\mathbf{R}) = \\kappa_2(\\mathbf{R})$. Therefore, the $\\mathbf{Q}\\mathbf{R}$ method solves a system with a condition number of $\\kappa_2(\\mathbf{X})$, not its square. By avoiding the formation of $\\mathbf{X}^{\\top}\\mathbf{X}$, the $\\mathbf{Q}\\mathbf{R}$ method bypasses the numerical instability associated with squaring the condition number, making it a more robust and accurate method for solving least-squares problems, especially for ill-conditioned matrices $\\mathbf{X}$.\n\nFinally, we compute the least-squares estimate for the given data:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nWe first compute the thin $\\mathbf{Q}\\mathbf{R}$ factorization of $\\mathbf{X}$ using the Gram-Schmidt process on its columns, $\\mathbf{x}_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^\\top$ and $\\mathbf{x}_2 = \\begin{pmatrix} 1  -1  1 \\end{pmatrix}^\\top$.\nFor the first column:\n$r_{11} = \\|\\mathbf{x}_1\\|_2 = \\sqrt{1^2+1^2+0^2} = \\sqrt{2}$.\n$\\mathbf{q}_1 = \\frac{\\mathbf{x}_1}{r_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor the second column:\n$r_{12} = \\mathbf{q}_1^\\top \\mathbf{x}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1-1) = 0$.\n$\\mathbf{u}_2 = \\mathbf{x}_2 - r_{12}\\mathbf{q}_1 = \\mathbf{x}_2 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n$r_{22} = \\|\\mathbf{u}_2\\|_2 = \\sqrt{1^2+(-1)^2+1^2} = \\sqrt{3}$.\n$\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{r_{22}} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n\nThus, we have:\n$$\n\\mathbf{Q} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{3}} \\\\ 0  \\frac{1}{\\sqrt{3}} \\end{pmatrix}, \\quad\n\\mathbf{R} = \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix}\n$$\nNext, we compute the vector $\\mathbf{Q}^\\top\\mathbf{y}$:\n$$\n\\mathbf{Q}^\\top\\mathbf{y} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{3}}  -\\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{\\sqrt{2}} \\\\ \\frac{2}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\frac{3}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\nNow we solve the system $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^\\top\\mathbf{y}$:\n$$\n\\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\widehat{\\theta}_1 \\\\ \\widehat{\\theta}_2 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\nThis diagonal system yields the solution directly:\n$\\sqrt{2} \\cdot \\widehat{\\theta}_1 = \\sqrt{2} \\implies \\widehat{\\theta}_1 = 1$.\n$\\sqrt{3} \\cdot \\widehat{\\theta}_2 = \\sqrt{3} \\implies \\widehat{\\theta}_2 = 1$.\nThe least-squares estimate is $\\widehat{\\boldsymbol{\\theta}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1\n\\end{pmatrix}\n}\n$$", "id": "2718842"}, {"introduction": "Applying estimation techniques in closed-loop control systems presents unique challenges, as feedback can introduce correlation between the controller's input and the system's inherent noise. This violates a key assumption of Ordinary Least Squares (OLS), leading to biased and inconsistent parameter estimates. This applied problem demonstrates this critical issue and introduces the Instrumental Variables (IV) method as a powerful technique to obtain consistent estimates in the presence of feedback [@problem_id:2718799], a crucial skill for practical system identification.", "problem": "Consider a closed-loop control setting with a static plant and instantaneous feedback. The scalar plant is given by the equation $y_t = b\\,u_t + e_t$, where $y_t$ is the plant output, $u_t$ is the control input, $b$ is the true plant gain to be identified, and $e_t$ is a zero-mean disturbance. The controller uses the current plant output in a static feedback law $u_t = g\\,r_t - k\\,y_t$, where $r_t$ is an exogenous reference signal, $g$ is a known reference gain, and $k$ is a known feedback gain. Assume $\\{r_t\\}$ and $\\{e_t\\}$ are mutually independent sequences of independent and identically distributed Gaussian random variables, $r_t \\sim \\mathcal{N}(0,\\sigma_r^2)$ and $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$, with $\\sigma_r^2  0$ and $\\sigma_e^2  0$. All quantities are real-valued. The goal is to estimate $b$ from a batch of $N$ samples $\\{(u_t,y_t)\\}_{t=1}^N$ using both Ordinary Least Squares (OLS) and an Instrumental Variables (IV) estimator with instrument $z_t = r_t$, and to compare these with a Recursive Least Squares (RLS) estimate.\n\nYour tasks are as follows.\n\n1) Using only fundamental definitions (linearity of expectation, the Law of Large Numbers, and the definitions of Ordinary Least Squares (OLS), Instrumental Variables (IV), and Recursive Least Squares (RLS)), derive the probability limit of the OLS estimator that regresses $y_t$ on $u_t$ under the specified feedback, and determine whether it equals $b$ when $k \\neq 0$. Do not use any pre-quoted final results; start from $y_t = b\\,u_t + e_t$ and $u_t = g\\,r_t - k\\,y_t$ and reason about endogeneity induced by feedback. Conclude under what condition on $k$ OLS is consistent for $b$.\n\n2) Argue, again from first principles, that the IV estimator using $z_t = r_t$ as an instrument is consistent for $b$ under the stated assumptions. Explicitly identify the relevance and exogeneity conditions in terms of the joint statistics of $r_t$, $u_t$, and $e_t$.\n\n3) Implement the following estimators on simulated data:\n- Batch OLS estimate $\\hat{b}_{\\mathrm{OLS}}$ from $\\{(u_t,y_t)\\}$.\n- Batch IV estimate $\\hat{b}_{\\mathrm{IV}}$ using instrument $z_t = r_t$ from $\\{(z_t,u_t,y_t)\\}$.\n- Recursive Least Squares (RLS) estimate $\\hat{b}_{\\mathrm{RLS}}$ updated sequentially over the same data with forgetting factor $\\lambda = 1$, scalar covariance initialization $P_0 = 10^6$, and initial parameter $\\hat{b}_0 = 0$.\n\n4) For each test case below, compute the following truth-valued (boolean) assertions using explicit numerical tolerances:\n- Define a tolerance for truth as $\\varepsilon_{\\text{true}} = 10^{-2}$, for the OLS probability limit as $\\varepsilon_{\\text{lim}} = 10^{-2}$, and for RLS agreement as $\\varepsilon_{\\text{rls}} = 5 \\times 10^{-3}$.\n- Let $b_{\\infty,\\mathrm{OLS}}$ denote the theoretical OLS probability limit you derived in Task 1, expressed in terms of $(b,k,g,\\sigma_r,\\sigma_e)$.\n- For each test case, report three booleans in this order:\n    a) $\\text{OLS\\_inconsistent}$: this is true if $|\\hat{b}_{\\mathrm{OLS}} - b|  \\varepsilon_{\\text{true}}$ and $|\\hat{b}_{\\mathrm{OLS}} - b_{\\infty,\\mathrm{OLS}}| \\le \\varepsilon_{\\text{lim}}$.\n    b) $\\text{IV\\_eliminates\\_bias}$: this is true if $|\\hat{b}_{\\mathrm{IV}} - b| \\le \\varepsilon_{\\text{true}}$.\n    c) $\\text{RLS\\_matches\\_OLS}$: this is true if $|\\hat{b}_{\\mathrm{RLS}} - \\hat{b}_{\\mathrm{OLS}}| \\le \\varepsilon_{\\text{rls}}$.\n\nSimulation details:\n- Use a fixed pseudorandom seed equal to $12345$ for reproducibility.\n- For each test case, generate $N$ independent samples of $r_t$ and $e_t$ and then form $u_t$ and $y_t$ according to the equations above.\n- You may assume that $1 + k\\,b \\neq 0$ for the test cases below.\n\nTest suite:\n- Case A (feedback present): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.5,1.0,1.0,0.7,200000)$.\n- Case B (no feedback): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.0,1.0,1.0,0.7,200000)$.\n- Case C (stronger feedback): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,1.5,1.0,1.0,0.7,200000)$.\n\nFinal output format:\n- Your program should produce a single line of output containing all results for the three cases concatenated in order A, B, C, each case contributing the three booleans $(\\text{OLS\\_inconsistent}, \\text{IV\\_eliminates\\_bias}, \\text{RLS\\_matches\\_OLS})$, as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\ldots]$.\n- No other output should be produced.", "solution": "The problem presented requires a rigorous analysis of parameter estimation for a linear static plant operating within a closed-loop feedback system. We must first validate the problem's integrity. The problem is a canonical exercise in system identification, grounded in established principles of control theory and statistical estimation. It is well-posed, objective, and contains all necessary information. Its parameters are scientifically plausible, and it poses a non-trivial question regarding estimator consistency in the presence of feedback. The problem is therefore deemed valid and a formal solution will be provided.\n\nThe system is described by the following set of equations:\nPlant model: $y_t = b u_t + e_t$\nFeedback controller: $u_t = g r_t - k y_t$\n\nHere, $y_t$ is the output, $u_t$ is the control input, $b$ is the unknown parameter to estimate, $e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$ is a stochastic disturbance, $r_t \\sim \\mathcal{N}(0, \\sigma_r^2)$ is an exogenous reference signal, and $g$ and $k$ are known controller gains. The sequences $\\{r_t\\}$ and $\\{e_t\\}$ are independent. We assume $1+kb \\neq 0$ to ensure the closed-loop system is well-defined.\n\nFirst, we must express the endogenous variables $u_t$ and $y_t$ solely in terms of the exogenous inputs $r_t$ and $e_t$. Substituting the plant equation into the controller equation yields:\n$u_t = g r_t - k (b u_t + e_t) \\implies u_t(1+kb) = g r_t - k e_t \\implies u_t = \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t$\nSubstituting this back into the plant equation yields:\n$y_t = b \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) + e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\left(1 - \\frac{bk}{1+kb}\\right) e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\frac{1}{1+kb} e_t$\nThese are the reduced-form equations for the system.\n\n**1. Analysis of the Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator for the parameter $b$ in the regression of $y_t$ on $u_t$ is given by:\n$$ \\hat{b}_{\\mathrm{OLS}} = \\left( \\sum_{t=1}^N u_t^2 \\right)^{-1} \\left( \\sum_{t=1}^N u_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N u_t y_t}{\\frac{1}{N}\\sum_{t=1}^N u_t^2} $$\nBy the Law of Large Numbers, the probability limit (plim) of this estimator as $N \\to \\infty$ is the ratio of expectations:\n$$ b_{\\infty,\\mathrm{OLS}} = \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{OLS}} = \\frac{\\mathrm{E}[u_t y_t]}{\\mathrm{E}[u_t^2]} $$\nThe fundamental condition for OLS consistency ($b_{\\infty,\\mathrm{OLS}} = b$) is that the regressor, $u_t$, must be uncorrelated with the error term, $e_t$. This is the property of exogeneity. Let us examine this condition. We compute the covariance $\\mathrm{E}[u_t e_t]$:\n$$ \\mathrm{E}[u_t e_t] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) e_t \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t e_t] - \\frac{k}{1+kb}\\mathrm{E}[e_t^2] $$\nGiven that $r_t$ and $e_t$ are independent and zero-mean, $\\mathrm{E}[r_t e_t] = \\mathrm{E}[r_t]\\mathrm{E}[e_t] = 0$. With $\\mathrm{E}[e_t^2] = \\sigma_e^2$, we find:\n$$ \\mathrm{E}[u_t e_t] = -\\frac{k \\sigma_e^2}{1+kb} $$\nThis covariance is non-zero if and only if $k \\neq 0$ (as $\\sigma_e^2  0$ and $1+kb \\neq 0$). The feedback gain $k$ creates a direct algebraic link from the disturbance $e_t$ to the input $u_t$. This phenomenon, where the regressor is correlated with the error term, is known as endogeneity. It violates the core assumption of OLS and leads to a biased and inconsistent estimator.\n\nTo find the asymptotic value of the OLS estimator, we proceed to calculate the necessary moments.\n$$ \\mathrm{E}[u_t^2] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right)^2 \\right] = \\frac{1}{(1+kb)^2} (g^2 \\mathrm{E}[r_t^2] + k^2 \\mathrm{E}[e_t^2]) = \\frac{g^2 \\sigma_r^2 + k^2 \\sigma_e^2}{(1+kb)^2} $$\nThe numerator of the plim is $\\mathrm{E}[u_t y_t] = \\mathrm{E}[u_t(b u_t + e_t)] = b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]$.\nThus, the probability limit is:\n$$ b_{\\infty,\\mathrm{OLS}} = \\frac{b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} = b + \\frac{\\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} $$\nSubstituting our derived moments:\n$$ b_{\\infty,\\mathrm{OLS}} = b + \\frac{-k \\sigma_e^2 / (1+kb)}{(g^2 \\sigma_r^2 + k^2 \\sigma_e^2) / (1+kb)^2} = b - \\frac{k \\sigma_e^2 (1+kb)}{g^2 \\sigma_r^2 + k^2 \\sigma_e^2} $$\nThis expression confirms that the OLS estimator is inconsistent, converging to a value different from the true parameter $b$ whenever feedback is present ($k \\neq 0$). Consistency is achieved only in the open-loop case, where $k=0$.\n\n**2. Analysis of the Instrumental Variables (IV) Estimator**\n\nThe IV method is designed to overcome endogeneity. It requires an instrumental variable, $z_t$, that satisfies two conditions:\n1.  **Instrument Relevance**: The instrument must be correlated with the endogenous regressor $u_t$. Formally, $\\mathrm{E}[z_t u_t] \\neq 0$.\n2.  **Instrument Exogeneity**: The instrument must be uncorrelated with the model's error term $e_t$. Formally, $\\mathrm{E}[z_t e_t] = 0$.\n\nThe proposed instrument is the exogenous reference signal, $z_t=r_t$. Let us verify the two conditions.\nThe exogeneity condition is satisfied by definition: $r_t$ and $e_t$ are from mutually independent sequences, so $\\mathrm{E}[r_t e_t] = 0$.\nFor the relevance condition, we compute the covariance $\\mathrm{E}[r_t u_t]$:\n$$ \\mathrm{E}[r_t u_t] = \\mathrm{E}\\left[ r_t \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t^2] - \\frac{k}{1+kb}\\mathrm{E}[r_t e_t] = \\frac{g \\sigma_r^2}{1+kb} $$\nAssuming $g \\neq 0$ and $\\sigma_r^2  0$ (which are true for any meaningful control objective and are satisfied in the test cases), this covariance is non-zero. Thus, $r_t$ is a valid instrument.\n\nThe IV estimator is constructed as:\n$$ \\hat{b}_{\\mathrm{IV}} = \\left( \\sum_{t=1}^N z_t u_t \\right)^{-1} \\left( \\sum_{t=1}^N z_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N r_t y_t}{\\frac{1}{N}\\sum_{t=1}^N r_t u_t} $$\nIts probability limit is:\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{\\mathrm{E}[r_t y_t]}{\\mathrm{E}[r_t u_t]} $$\nThe numerator is $\\mathrm{E}[r_t y_t] = \\mathrm{E}[r_t(b u_t + e_t)] = b\\mathrm{E}[r_t u_t] + \\mathrm{E}[r_t e_t]$. Due to exogeneity, $\\mathrm{E}[r_t e_t]=0$. Therefore, $\\mathrm{E}[r_t y_t] = b\\mathrm{E}[r_t u_t]$.\nSubstituting this into the plim expression:\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{b\\mathrm{E}[r_t u_t]}{\\mathrm{E}[r_t u_t]} = b $$\nThe IV estimator is consistent for $b$, correctly accounting for the endogeneity induced by the feedback loop.\n\n**3. Recursive Least Squares (RLS) Estimator**\n\nThe RLS algorithm provides a sequential update for the least-squares estimate. When the forgetting factor $\\lambda$ is set to $1$, RLS gives exactly the same result as the batch OLS estimator over the same dataset, assuming a non-informative prior (i.e., initial covariance $P_0$ is large). Therefore, we expect $\\hat{b}_{\\mathrm{RLS}}$ to be numerically very close to $\\hat{b}_{\\mathrm{OLS}}$ and to suffer from the same asymptotic bias when $k \\neq 0$. The simulation will verify this equivalence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the system identification problem for OLS, IV, and RLS estimators.\n    \"\"\"\n    # Set the fixed pseudorandom seed for reproducibility.\n    np.random.seed(12345)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (b, k, g, sigma_r, sigma_e, N)\n        (0.8, 0.5, 1.0, 1.0, 0.7, 200000),  # Case A: feedback present\n        (0.8, 0.0, 1.0, 1.0, 0.7, 200000),  # Case B: no feedback\n        (0.8, 1.5, 1.0, 1.0, 0.7, 200000),  # Case C: stronger feedback\n    ]\n\n    # Define numerical tolerances for asserting truth values.\n    eps_true = 1e-2    # Tolerance for proximity to the true parameter b\n    eps_lim = 1e-2     # Tolerance for proximity to the OLS theoretical limit\n    eps_rls = 5e-3     # Tolerance for agreement between RLS and OLS estimates\n\n    all_results = []\n\n    for case in test_cases:\n        b, k, g, sigma_r, sigma_e, N = case\n\n        # Generate exogenous signals r_t and e_t.\n        r = np.random.normal(0, sigma_r, N)\n        e = np.random.normal(0, sigma_e, N)\n\n        # Compute endogenous signals u_t and y_t using the reduced-form equations.\n        # This is more efficient than a step-by-step simulation loop.\n        denom = 1.0 + k * b\n        if denom == 0:\n            # This case is excluded by the problem statement.\n            # Handle defensively to avoid division by zero.\n            u = np.full(N, np.nan)\n            y = np.full(N, np.nan)\n        else:\n            u = (g / denom) * r - (k / denom) * e\n            y = (b * g / denom) * r + (1.0 / denom) * e\n\n        # Task 3.1: Batch Ordinary Least Squares (OLS) estimate.\n        # This is the standard formula for scalar regression.\n        b_ols = np.dot(u, y) / np.dot(u, u)\n\n        # Task 3.2: Batch Instrumental Variables (IV) estimate using z_t = r_t.\n        # The instrument r_t replaces the regressor u_t in the \"X'X\" part of the projection.\n        b_iv = np.dot(r, y) / np.dot(r, u)\n\n        # Task 3.3: Recursive Least Squares (RLS) estimate.\n        # Initialize parameter and covariance matrix (scalar in this case).\n        b_rls = 0.0\n        P = 1e6\n        lambda_rls = 1.0  # Forgetting factor set to 1.\n        \n        # Sequentially update the estimate for each data point.\n        for i in range(N):\n            phi = u[i]  # The regressor at step i\n            y_i = y[i]  # The observation at step i\n            \n            # Kalman gain calculation\n            # denom_K = lambda_rls + phi * P * phi is dimensionally incorrect for vector case\n            # Correct scalar form is lambda_rls + P * phi^2\n            gain = (P * phi) / (lambda_rls + P * phi * phi)\n            \n            # Prediction error\n            prediction_error = y_i - phi * b_rls\n            \n            # Update parameter estimate\n            b_rls = b_rls + gain * prediction_error\n            \n            # Update covariance\n            P = (1.0 / lambda_rls) * (1.0 - gain * phi) * P\n            \n        # Task 4: Compute boolean assertions.\n        # Calculate the theoretical probability limit of the OLS estimator.\n        if k == 0:\n            b_inf_ols = b\n        else:\n            b_inf_ols = b - (k * sigma_e**2 * (1.0 + k * b)) / (g**2 * sigma_r**2 + k**2 * sigma_e**2)\n\n        # a) OLS_inconsistent: True if OLS is far from true 'b' but close to its theoretical limit.\n        ols_inconsistent = (abs(b_ols - b) > eps_true) and (abs(b_ols - b_inf_ols) = eps_lim)\n        \n        # b) IV_eliminates_bias: True if IV estimate is close to the true 'b'.\n        iv_eliminates_bias = (abs(b_iv - b) = eps_true)\n        \n        # c) RLS_matches_OLS: True if final RLS estimate is close to batch OLS estimate.\n        rls_matches_ols = (abs(b_rls - b_ols) = eps_rls)\n\n        all_results.extend([ols_inconsistent, iv_eliminates_bias, rls_matches_ols])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly converts Python booleans (True/False) to strings (\"True\"/\"False\").\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2718799"}]}