{"hands_on_practices": [{"introduction": "The foundation of many adaptive control schemes is the parameter update law. This first exercise guides you through deriving the most common update law from first principles using the gradient descent method on a simple quadratic cost function. You will then use a Lyapunov function to not only prove that the parameter error is stable but also to show how a sufficiently \"rich\" input signal, a property known as persistence of excitation, allows you to guarantee a specific exponential rate of convergence for your parameter estimates [@problem_id:2722772]. This practice is essential for building intuition on how adaptation gain and signal properties directly influence performance.", "problem": "Consider a scalar linear-in-parameters regression model used inside an adaptive estimator for a plant with unknown constant parameter. The measured scalar output is modeled as $y(t) = \\theta \\, \\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter and $\\phi(t) \\in \\mathbb{R}$ is a known, time-varying regressor that is continuous and bounded. An estimator produces the prediction $\\hat{y}(t) = \\hat{\\theta}(t) \\, \\phi(t)$ with scalar estimate $\\hat{\\theta}(t) \\in \\mathbb{R}$. Define the prediction error as $e(t) = \\hat{y}(t) - y(t)$. Starting from the steepest-descent principle applied to the instantaneous squared loss $J(t) = \\frac{1}{2} e^{2}(t)$, derive the scalar gradient update law for $\\hat{\\theta}(t)$ and the corresponding parameter error dynamics for $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$. Then, using the direct method of Lyapunov, construct a smooth positive definite Lyapunov candidate depending only on $\\tilde{\\theta}(t)$ and a constant adaptation gain $\\gamma > 0$, and prove that the resulting adaptive law is Lyapunov stable for any $\\gamma > 0$.\n\nNext, assume that the regressor satisfies a uniform pointwise lower bound on its instantaneous energy: there exists a known constant $m > 0$ such that $\\phi^{2}(t) \\ge m$ for all $t \\ge 0$. Using only this bound and your error dynamics, derive a bound of the form $\\lvert \\tilde{\\theta}(t) \\rvert \\le \\lvert \\tilde{\\theta}(0) \\rvert \\exp\\!\\big(-\\alpha t\\big)$ for some rate $\\alpha > 0$ expressed in terms of $\\gamma$ and $m$. You are tasked with meeting a prescribed adaptation rate $\\lambda_{d} > 0$, meaning that the exponential bound is no slower than $\\exp(-\\lambda_{d} t)$ for all $t \\ge 0$. Determine a constant adaptation gain $\\gamma$ that guarantees this requirement while preserving Lyapunov stability.\n\nProvide your final answer as a single, closed-form analytical expression in terms of $\\lambda_{d}$ and $m$. No numerical approximation is required. No units are needed.", "solution": "The problem posed is a standard exercise in the fundamentals of adaptive control theory. It is well-posed, scientifically sound, and contains all necessary information for its resolution. We will proceed with the derivation in a logical, step-by-step manner.\n\nThe system is described by the scalar model $y(t) = \\theta \\, \\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter and $\\phi(t) \\in \\mathbb{R}$ is a known, bounded, and continuous regressor signal. The parameter estimate is $\\hat{\\theta}(t)$, which forms the prediction $\\hat{y}(t) = \\hat{\\theta}(t) \\, \\phi(t)$. The prediction error is $e(t) = \\hat{y}(t) - y(t)$.\n\nFirst, we derive the update law for $\\hat{\\theta}(t)$ using the steepest-descent method to minimize the instantaneous squared error cost function, $J(t) = \\frac{1}{2} e^{2}(t)$. The gradient update law is given by\n$$\n\\dot{\\hat{\\theta}}(t) = - \\gamma \\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)}\n$$\nwhere $\\gamma > 0$ is the constant adaptation gain.\n\nWe compute the partial derivative of $J(t)$ with respect to $\\hat{\\theta}(t)$:\n$$\n\\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)} = \\frac{\\partial}{\\partial \\hat{\\theta}(t)} \\left( \\frac{1}{2} e^{2}(t) \\right) = e(t) \\frac{\\partial e(t)}{\\partial \\hat{\\theta}(t)}\n$$\nThe error $e(t)$ can be expressed as $e(t) = \\hat{\\theta}(t)\\phi(t) - y(t)$. Its partial derivative with respect to $\\hat{\\theta}(t)$ is:\n$$\n\\frac{\\partial e(t)}{\\partial \\hat{\\theta}(t)} = \\frac{\\partial}{\\partial \\hat{\\theta}(t)} (\\hat{\\theta}(t)\\phi(t) - y(t)) = \\phi(t)\n$$\nSubstituting this back, we find the gradient:\n$$\n\\frac{\\partial J(t)}{\\partial \\hat{\\theta}(t)} = e(t) \\phi(t)\n$$\nThus, the gradient update law for the parameter estimate is:\n$$\n\\dot{\\hat{\\theta}}(t) = - \\gamma e(t) \\phi(t)\n$$\nNext, we derive the dynamics of the parameter error, defined as $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$. Since $\\theta$ is a constant, its time derivative is zero, so $\\dot{\\tilde{\\theta}}(t) = \\dot{\\hat{\\theta}}(t)$. Substituting the update law and expressing the prediction error in terms of the parameter error, $e(t) = (\\hat{\\theta}(t) - \\theta)\\phi(t) = \\tilde{\\theta}(t)\\phi(t)$, we obtain the parameter error dynamics:\n$$\n\\dot{\\tilde{\\theta}}(t) = - \\gamma \\left( \\tilde{\\theta}(t)\\phi(t) \\right) \\phi(t) = - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t)\n$$\nThis is a linear time-varying ordinary differential equation governing the evolution of the parameter error.\n\nNow, we prove Lyapunov stability. As suggested, we choose a smooth, positive definite Lyapunov function candidate $V$ that depends on $\\tilde{\\theta}(t)$ and $\\gamma$. A suitable choice is:\n$$\nV(\\tilde{\\theta}) = \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t)\n$$\nSince $\\gamma > 0$, $V(\\tilde{\\theta}) > 0$ for all $\\tilde{\\theta} \\neq 0$ and $V(0) = 0$. Thus, $V$ is positive definite. We analyze its time derivative along the trajectories of the error dynamics:\n$$\n\\dot{V}(t) = \\frac{d}{dt} \\left( \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\right) = \\frac{1}{2\\gamma} \\left( 2 \\tilde{\\theta}(t) \\dot{\\tilde{\\theta}}(t) \\right) = \\frac{1}{\\gamma} \\tilde{\\theta}(t) \\dot{\\tilde{\\theta}}(t)\n$$\nSubstituting the error dynamics $\\dot{\\tilde{\\theta}}(t) = - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t)$:\n$$\n\\dot{V}(t) = \\frac{1}{\\gamma} \\tilde{\\theta}(t) \\left( - \\gamma \\phi^{2}(t) \\tilde{\\theta}(t) \\right) = - \\phi^{2}(t) \\tilde{\\theta}^{2}(t)\n$$\nSince $\\phi^{2}(t) \\ge 0$ for all $t$, the time derivative $\\dot{V}(t) \\le 0$ for all $t$. This shows that $V(t)$ is a non-increasing function of time, which implies that $\\tilde{\\theta}(t)$ is bounded. Specifically, $V(t) \\le V(0)$, which means $\\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\le \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(0)$, or $|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)|$. This implies that the equilibrium $\\tilde{\\theta} = 0$ is globally uniformly stable, which satisfies the requirement of Lyapunov stability for any $\\gamma > 0$.\n\nThe second part of the problem assumes a uniform pointwise lower bound on the regressor's energy, $\\phi^{2}(t) \\ge m$ for some known constant $m > 0$. This is a persistence of excitation (PE) condition. Using this condition, we can establish an exponential convergence rate. From our Lyapunov analysis, we have $\\dot{V}(t) = - \\phi^{2}(t) \\tilde{\\theta}^{2}(t)$. Applying the PE condition, we get:\n$$\n\\dot{V}(t) \\le -m \\tilde{\\theta}^{2}(t)\n$$\nWe relate $\\tilde{\\theta}^{2}(t)$ back to $V(t)$ using the definition $V(t) = \\frac{1}{2\\gamma}\\tilde{\\theta}^{2}(t)$, which gives $\\tilde{\\theta}^{2}(t) = 2\\gamma V(t)$. Substituting this into the inequality for $\\dot{V}(t)$:\n$$\n\\dot{V}(t) \\le -m (2\\gamma V(t)) = -2\\gamma m V(t)\n$$\nThis is a standard differential inequality. By the Comparison Lemma (a version of GrÃ¶nwall's inequality), we can conclude that:\n$$\nV(t) \\le V(0) \\exp(-2\\gamma m t)\n$$\nSubstituting the definition of $V$ back in terms of $\\tilde{\\theta}$:\n$$\n\\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(t) \\le \\left( \\frac{1}{2\\gamma} \\tilde{\\theta}^{2}(0) \\right) \\exp(-2\\gamma m t)\n$$\nMultiplying both sides by $2\\gamma$ and taking the square root, we obtain the desired exponential bound on the parameter error:\n$$\n|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)| \\exp(-\\gamma m t)\n$$\nThis bound has the form $|\\tilde{\\theta}(t)| \\le |\\tilde{\\theta}(0)| \\exp(-\\alpha t)$, with the exponential decay rate being $\\alpha = \\gamma m$.\n\nFinally, we are tasked with choosing the adaptation gain $\\gamma$ to meet a prescribed adaptation rate $\\lambda_{d} > 0$. This means the decay must be at least as fast as $\\exp(-\\lambda_{d} t)$, which requires the rate $\\alpha$ to satisfy $\\alpha \\ge \\lambda_{d}$.\n$$\n\\gamma m \\ge \\lambda_{d}\n$$\nTo guarantee this condition, we may simply select $\\gamma$ such that the rates are equal. This provides a specific choice for the gain:\n$$\n\\gamma m = \\lambda_{d}\n$$\nSolving for $\\gamma$ yields:\n$$\n\\gamma = \\frac{\\lambda_{d}}{m}\n$$\nSince it is given that $\\lambda_{d} > 0$ and $m > 0$, the resulting adaptation gain $\\gamma$ is guaranteed to be positive. Therefore, this choice of $\\gamma$ preserves the Lyapunov stability established earlier. This expression for $\\gamma$ is the solution.", "answer": "$$\\boxed{\\frac{\\lambda_{d}}{m}}$$", "id": "2722772"}, {"introduction": "In the previous exercise, we saw how a persistently exciting (PE) regressor leads to desirable exponential convergence of parameter estimates. This practice explores the critical counterpoint: what happens when the PE condition is not met? By analyzing a system with a regressor signal that decays to zero, you will demonstrate a classic result in adaptive control: the prediction error can converge to zero even if the parameter estimates do not converge to their true values [@problem_id:2722709]. This is not merely a theoretical curiosity; it highlights why parameter convergence is a stronger condition than output tracking and reinforces the central importance of the PE condition in system identification and adaptive control.", "problem": "Consider the scalar adaptive estimation problem arising in Lyapunov-based adaptive control design. A measurable regressor signal $\\,\\phi(t)\\,$ and an unknown constant parameter $\\,\\theta \\in \\mathbb{R}\\,$ generate the scalar output $\\,y(t) = \\theta\\,\\phi(t)\\,$. An estimator uses the parametric model $\\,\\hat{y}(t) = \\hat{\\theta}(t)\\,\\phi(t)\\,$ and the standard gradient update law derived from a quadratic Lyapunov function:\n$$\n\\dot{\\hat{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,\\big(\\hat{y}(t) - y(t)\\big), \\quad \\gamma > 0.\n$$\nDefine the prediction error $\\,e(t) := \\hat{y}(t) - y(t)\\,$ and the parameter error $\\,\\tilde{\\theta}(t) := \\hat{\\theta}(t) - \\theta\\,$. Suppose the regressor is $\\,\\phi(t) = \\exp(-t)\\,$ for all $\\,t \\ge 0\\,$, which is not persistently exciting in the sense of persistence of excitation (PE), namely, there do not exist $\\,\\alpha > 0\\,$ and $\\,T > 0\\,$ such that for all $\\,t \\ge 0\\,$ one has $\\,\\int_{t}^{t+T} \\phi(\\tau)^2\\,\\mathrm{d}\\tau \\ge \\alpha\\,$.\n\nStarting from the fundamental definitions of $\\,e(t)\\,$ and $\\,\\tilde{\\theta}(t)\\,$ and the given update law, analyze the closed-loop error dynamics implied by the Lyapunov approach. Show that $\\,e(t) \\to 0\\,$ as $\\,t \\to \\infty\\,$ even though the parameter error need not converge to $\\,0\\,$ due to the lack of persistence of excitation. Compute the closed-form limit\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t)\n$$\nas a function of the initial parameter error $\\,\\tilde{\\theta}(0)\\,$ and the adaptation gain $\\,\\gamma\\,$. Your final answer must be a single analytic expression without units. Do not round or approximate your result.", "solution": "The validity of the problem statement must first be established.\n\nStep 1: Extract Givens.\nThe problem provides the following definitions and equations:\n- The output signal is given by $y(t) = \\theta\\,\\phi(t)$, where $\\theta \\in \\mathbb{R}$ is an unknown constant parameter.\n- The measurable regressor signal is $\\phi(t) = \\exp(-t)$ for $t \\ge 0$.\n- The parametric model for the estimator is $\\hat{y}(t) = \\hat{\\theta}(t)\\,\\phi(t)$.\n- The prediction error is defined as $e(t) := \\hat{y}(t) - y(t)$.\n- The parameter error is defined as $\\tilde{\\theta}(t) := \\hat{\\theta}(t) - \\theta$.\n- The update law for the parameter estimate $\\hat{\\theta}(t)$ is given by the gradient rule $\\dot{\\hat{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,e(t)$, where the adaptation gain $\\gamma > 0$.\n- It is stated that the regressor $\\phi(t)$ is not persistently exciting.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. It presents a canonical scenario in adaptive control theory, specifically parameter estimation using a gradient descent algorithm. The system model, estimator structure, and update law are standard. The choice of regressor, $\\phi(t) = \\exp(-t)$, is a classic example used to illustrate the consequences of the lack of the persistence of excitation (PE) condition. The PE condition is a fundamental concept in system identification and adaptive control that guarantees parameter convergence. The problem is well-posed, providing all necessary information to derive the dynamics of the parameter error and analyze its asymptotic behavior. The language is objective and precise. The problem is a standard exercise in the analysis of adaptive systems and does not violate any criteria for invalidity.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A solution will be provided.\n\nThe objective is to analyze the error dynamics and compute the limit of the parameter error, $\\lim_{t \\to \\infty} \\tilde{\\theta}(t)$.\n\nFirst, we express the prediction error $e(t)$ in terms of the parameter error $\\tilde{\\theta}(t)$. From the definitions provided:\n$$\ne(t) = \\hat{y}(t) - y(t) = \\hat{\\theta}(t)\\phi(t) - \\theta\\phi(t) = \\left(\\hat{\\theta}(t) - \\theta\\right)\\phi(t)\n$$\nUsing the definition of the parameter error, $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$, we have:\n$$\ne(t) = \\tilde{\\theta}(t)\\phi(t)\n$$\nNext, we derive the differential equation governing the parameter error $\\tilde{\\theta}(t)$. We differentiate the definition of $\\tilde{\\theta}(t)$ with respect to time $t$:\n$$\n\\dot{\\tilde{\\theta}}(t) = \\frac{d}{dt}\\left(\\hat{\\theta}(t) - \\theta\\right) = \\dot{\\hat{\\theta}}(t) - \\dot{\\theta}(t)\n$$\nSince $\\theta$ is a constant parameter, its time derivative is zero, $\\dot{\\theta}(t) = 0$. Therefore, $\\dot{\\tilde{\\theta}}(t) = \\dot{\\hat{\\theta}}(t)$.\nSubstituting the given update law for $\\dot{\\hat{\\theta}}(t)$:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,e(t)\n$$\nNow, we substitute the expression for $e(t)$ into this equation to obtain the closed-loop error dynamics for $\\tilde{\\theta}(t)$:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\phi(t)\\,\\left(\\tilde{\\theta}(t)\\phi(t)\\right) = -\\gamma\\,\\phi(t)^2\\,\\tilde{\\theta}(t)\n$$\nThis is a first-order linear time-varying ordinary differential equation for the parameter error.\n\nThe problem specifies the regressor signal as $\\phi(t) = \\exp(-t)$. We substitute this into the differential equation:\n$$\n\\dot{\\tilde{\\theta}}(t) = -\\gamma\\,\\left(\\exp(-t)\\right)^2\\,\\tilde{\\theta}(t) = -\\gamma\\,\\exp(-2t)\\,\\tilde{\\theta}(t)\n$$\nThis ODE is separable and can be written as:\n$$\n\\frac{d\\tilde{\\theta}(t)}{\\tilde{\\theta}(t)} = -\\gamma\\,\\exp(-2t)\\,dt\n$$\nWe integrate both sides of the equation from the initial time $t=0$ to an arbitrary time $t > 0$:\n$$\n\\int_{\\tilde{\\theta}(0)}^{\\tilde{\\theta}(t)} \\frac{1}{\\tau} d\\tau = \\int_{0}^{t} -\\gamma\\,\\exp(-2\\xi)\\,d\\xi\n$$\nEvaluating the integrals:\n$$\n\\left[ \\ln|\\tau| \\right]_{\\tilde{\\theta}(0)}^{\\tilde{\\theta}(t)} = -\\gamma \\left[ -\\frac{1}{2}\\exp(-2\\xi) \\right]_{0}^{t}\n$$\n$$\n\\ln|\\tilde{\\theta}(t)| - \\ln|\\tilde{\\theta}(0)| = \\frac{\\gamma}{2} \\left[ \\exp(-2t) - \\exp(0) \\right]\n$$\n$$\n\\ln\\left(\\frac{|\\tilde{\\theta}(t)|}{|\\tilde{\\theta}(0)|}\\right) = \\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\n$$\nExponentiating both sides gives the solution for the magnitude of the parameter error:\n$$\n|\\tilde{\\theta}(t)| = |\\tilde{\\theta}(0)|\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right)\n$$\nSince the differential equation is linear, the solution $\\tilde{\\theta}(t)$ will not change sign if $\\tilde{\\theta}(0) \\ne 0$. If $\\tilde{\\theta}(0) = 0$, then $\\tilde{\\theta}(t) = 0$ for all $t$. Thus, we can remove the absolute value signs:\n$$\n\\tilde{\\theta}(t) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right)\n$$\nThis is the closed-form solution for the parameter error $\\tilde{\\theta}(t)$.\n\nThe problem also requires showing that $e(t) \\to 0$ as $t \\to \\infty$. We use the relationship $e(t) = \\tilde{\\theta}(t)\\phi(t)$:\n$$\ne(t) = \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right) \\exp(-t)\n$$\nWe take the limit as $t \\to \\infty$:\n$$\n\\lim_{t \\to \\infty} e(t) = \\lim_{t \\to \\infty} \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right) \\cdot \\lim_{t \\to \\infty} \\exp(-t)\n$$\nAs $t \\to \\infty$, we have $\\exp(-2t) \\to 0$ and $\\exp(-t) \\to 0$.\nThe first term converges to a constant:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}(0 - 1)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\n$$\nThe second term converges to zero:\n$$\n\\lim_{t \\to \\infty} \\exp(-t) = 0\n$$\nTherefore, the limit of the prediction error is:\n$$\n\\lim_{t \\to \\infty} e(t) = \\left(\\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\\right) \\cdot 0 = 0\n$$\nThis demonstrates that the prediction error converges to zero, even though, as we will see, the parameter error does not.\n\nFinally, we compute the limit of the parameter error $\\tilde{\\theta}(t)$ as $t \\to \\infty$:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t) = \\lim_{t \\to \\infty} \\left( \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}\\left(\\exp(-2t) - 1\\right)\\right) \\right)\n$$\nAs established, $\\lim_{t \\to \\infty} \\exp(-2t) = 0$. Substituting this into the expression:\n$$\n\\lim_{t \\to \\infty} \\tilde{\\theta}(t) = \\tilde{\\theta}(0)\\,\\exp\\left(\\frac{\\gamma}{2}(0 - 1)\\right) = \\tilde{\\theta}(0)\\,\\exp\\left(-\\frac{\\gamma}{2}\\right)\n$$\nThe final result shows that the parameter error does not converge to $0$ in general, but to a constant value that depends on its initial condition $\\tilde{\\theta}(0)$ and the adaptation gain $\\gamma$. This is a direct consequence of the regressor $\\phi(t)$ not being persistently exciting. The total \"energy\" of the regressor, $\\int_{0}^{\\infty} \\phi(\\tau)^2 d\\tau = \\int_{0}^{\\infty} \\exp(-2\\tau) d\\tau = \\frac{1}{2}$, is finite, which is insufficient to drive the parameter estimate to its true value.", "answer": "$$\n\\boxed{\\tilde{\\theta}(0)\\exp\\left(-\\frac{\\gamma}{2}\\right)}\n$$", "id": "2722709"}, {"introduction": "This final practice serves as a capstone exercise, integrating the concepts of parameter estimation and control law synthesis into a complete indirect Model Reference Adaptive Control (MRAC) system. You will first design a stable parameter estimator for an unknown first-order plant using a Lyapunov-based approach. Then, applying the certainty equivalence principle, you will synthesize a control law that uses these real-time parameter estimates to force the plant to behave like a desired reference model [@problem_id:2722775]. This problem demonstrates the full design cycle of a fundamental adaptive controller, bridging the gap between theoretical analysis and practical implementation.", "problem": "Consider a single-input single-output linear time-invariant plant with unknown constant parameters described by the differential equation $\\dot{y}(t)=-a\\,y(t)+b\\,u(t)$, where $a>0$ and $b>0$ are unknown constants, $u(t)$ is the control input, and $y(t)$ is the measured output. Assume that $\\dot{y}(t)$ is measurable and that the reference signal $r(t)$ is bounded and piecewise continuously differentiable. The desired behavior is specified by a first-order reference model $\\dot{y}_{m}(t)=-a_{m}\\,y_{m}(t)+b_{m}\\,r(t)$, where $a_{m}>0$ and $b_{m}>0$ are known design parameters.\n\nYour tasks are as follows.\n\n1. Starting from the fundamental definition of a Lyapunov function and the parameter estimation error, formulate a prediction model that is linear in the unknown parameters and derive a parameter estimator for $(a,b)$ using only measurable signals and a Lyapunov-based argument. You may use a constant, symmetric positive definite adaptation gain matrix, but you must explicitly construct a candidate Lyapunov function and its time derivative to justify stability of the estimator. Ensure that your estimator keeps all internal signals bounded for bounded regressors, and discuss how to guarantee that the estimate of $b$ remains bounded away from zero without breaking the Lyapunov nonincreasing property.\n\n2. Using the certainty equivalence principle, synthesize a control law that achieves exact model matching when the estimates equal the true parameters. Express your control input in terms of the current estimates $\\hat{a}(t)$ and $\\hat{b}(t)$, the measured output $y(t)$, and the reference $r(t)$, together with the known model parameters $a_{m}$ and $b_{m}$.\n\n3. Define the tracking error $e(t)=y(t)-y_{m}(t)$ and argue, using first principles and without invoking any prepackaged theorems beyond Lyapunov stability and properties of linear systems, that the certainty-equivalent controller from part 2 stabilizes the tracking error dynamics in the presence of bounded parameter estimation errors generated by your estimator. Clearly state any additional mild conditions you need for well-posedness and stability.\n\nProvide the closed-form analytical expression for the certainty-equivalent control law $u(t)$ from part 2 as your final answer. No numerical computation is required. Do not include any units. The final answer must be a single analytical expression.", "solution": "The problem as stated is a standard exercise in Lyapunov-based model reference adaptive control for a linear time-invariant system. All provided information is self-contained and scientifically sound. The problem is well-posed and objective, free of contradictions or ambiguities. It represents a valid theoretical problem in control engineering. We may therefore proceed with the solution.\n\nThe problem is addressed in three parts as requested.\n\n1. Parameter Estimator Formulation\n\nThe plant dynamics are given by the differential equation\n$$\n\\dot{y}(t) = -a y(t) + b u(t)\n$$\nwhere $a > 0$ and $b > 0$ are unknown constants. This equation is linear in the parameters $a$ and $b$. We define a parameter vector $\\theta$ and a regressor vector $\\phi(t)$ as\n$$\n\\theta = \\begin{pmatrix} a \\\\ b \\end{pmatrix}, \\quad \\phi(t) = \\begin{pmatrix} -y(t) \\\\ u(t) \\end{pmatrix}\n$$\nWith these definitions, the plant dynamics can be written in the linear parametric form\n$$\n\\dot{y}(t) = \\phi^T(t) \\theta\n$$\nThe problem states that $y(t)$ and $\\dot{y}(t)$ are measurable, and $u(t)$ is the known control input. Therefore, the regressor vector $\\phi(t)$ is directly measurable.\n\nWe formulate a prediction model for $\\dot{y}(t)$ using the parameter estimates $\\hat{a}(t)$ and $\\hat{b}(t)$, which are components of the estimated parameter vector $\\hat{\\theta}(t) = [\\hat{a}(t), \\hat{b}(t)]^T$. The predicted value $\\hat{\\dot{y}}(t)$ is\n$$\n\\hat{\\dot{y}}(t) = -\\hat{a}(t) y(t) + \\hat{b}(t) u(t) = \\phi^T(t) \\hat{\\theta}(t)\n$$\nThe prediction error, or equation error, $\\epsilon_1(t)$ is defined as\n$$\n\\epsilon_1(t) = \\hat{\\dot{y}}(t) - \\dot{y}(t) = \\phi^T(t) (\\hat{\\theta}(t) - \\theta) = \\phi^T(t) \\tilde{\\theta}(t)\n$$\nwhere $\\tilde{\\theta}(t) = \\hat{\\theta}(t) - \\theta$ is the parameter estimation error vector.\n\nTo derive the parameter update law, we consider a Lyapunov function candidate for the parameter error dynamics:\n$$\nV_e(\\tilde{\\theta}) = \\frac{1}{2} \\tilde{\\theta}^T \\Gamma^{-1} \\tilde{\\theta}\n$$\nwhere $\\Gamma$ is a constant, symmetric, positive definite adaptation gain matrix. Since $\\theta$ is constant, the time derivative of the parameter error is $\\dot{\\tilde{\\theta}} = \\dot{\\hat{\\theta}}$. The time derivative of $V_e$ is\n$$\n\\dot{V}_e = \\tilde{\\theta}^T \\Gamma^{-1} \\dot{\\tilde{\\theta}} = \\tilde{\\theta}^T \\Gamma^{-1} \\dot{\\hat{\\theta}}\n$$\nWe choose a gradient-based update law for $\\hat{\\theta}(t)$ that makes $\\dot{V}_e$ non-positive. The standard choice is\n$$\n\\dot{\\hat{\\theta}}(t) = - \\Gamma \\phi(t) \\epsilon_1(t)\n$$\nSubstituting this into the expression for $\\dot{V}_e$, we obtain\n$$\n\\dot{V}_e = - \\tilde{\\theta}^T \\Gamma^{-1} (\\Gamma \\phi \\epsilon_1) = - \\tilde{\\theta}^T \\phi \\epsilon_1\n$$\nSince $\\epsilon_1 = \\phi^T \\tilde{\\theta}$, this becomes\n$$\n\\dot{V}_e = -(\\phi^T \\tilde{\\theta})^2 = - \\epsilon_1^2 \\leq 0\n$$\nBecause $\\dot{V}_e$ is negative semi-definite, $V_e(t)$ is non-increasing. This implies that $V_e(t) \\leq V_e(0)$ for all $t > 0$, and therefore the parameter error $\\tilde{\\theta}(t)$ is bounded. Since $\\theta$ is a bounded constant, the estimate $\\hat{\\theta}(t)$ is also bounded. This confirms the stability of the estimator itself.\n\nThe control law, to be derived next, will involve division by $\\hat{b}(t)$. To prevent this estimate from becoming zero or negative (since we know $b > 0$), we must ensure it remains bounded away from zero. This is achieved by using a projection modification to the update law. We define a convex set for the parameters, for instance, $\\mathcal{D} = \\{ (\\hat{a}, \\hat{b}) \\mid \\hat{b} \\geq b_{\\min} \\}$, where $b_{\\min}$ is a small positive constant chosen based on a priori knowledge about the possible range of $b$. The projected update law, $\\dot{\\hat{\\theta}} = \\text{Proj}(\\hat{\\theta}, -\\Gamma \\phi \\epsilon_1)$, ensures that if $\\hat{\\theta}$ is on the boundary of $\\mathcal{D}$ (i.e., $\\hat{b} = b_{\\min}$), the update will not point outside the set. This modification preserves the non-increasing property of the Lyapunov function, i.e., $\\dot{V}_e \\leq 0$.\n\n2. Certainty-Equivalent Control Law Synthesis\n\nThe objective is to make the plant output $y(t)$ track the reference model output $y_m(t)$, whose dynamics are given by $\\dot{y}_m = -a_m y_m + b_m r$. The ideal scenario is \"exact model matching,\" where the plant dynamics for $y(t)$ become identical to the reference model dynamics:\n$$\n\\dot{y}(t) = -a_m y(t) + b_m r(t)\n$$\nEquating the plant equation with this desired behavior, we get\n$$\n-a y(t) + b u(t) = -a_m y(t) + b_m r(t)\n$$\nSolving for the ideal control input $u(t)$ gives:\n$$\nb u(t) = (a - a_m) y(t) + b_m r(t) \\implies u(t) = \\frac{1}{b} \\left( (a - a_m) y(t) + b_m r(t) \\right)\n$$\nThe certainty equivalence principle dictates that we replace the unknown true parameters $a$ and $b$ with their current estimates $\\hat{a}(t)$ and $\\hat{b}(t)$. This yields the adaptive control law:\n$$\nu(t) = \\frac{1}{\\hat{b}(t)} \\left( (\\hat{a}(t) - a_m) y(t) + b_m r(t) \\right)\n$$\nThe projection mechanism discussed previously ensures that $\\hat{b}(t) \\geq b_{\\min} > 0$, so the control law is always well-defined.\n\n3. Closed-Loop Stability Analysis\n\nWe define the tracking error as $e(t) = y(t) - y_m(t)$. In an indirect adaptive control scheme, the stability of the overall system is not guaranteed by the separation principle and must be analyzed by considering the coupled dynamics.\n\nThe time derivative of the tracking error is $\\dot{e}(t) = \\dot{y}(t) - \\dot{y}_m(t)$. Substituting the plant dynamics and the control law:\n$$\n\\dot{y} = -ay + bu = -ay + b \\left[ \\frac{1}{\\hat{b}} ((\\hat{a} - a_m)y + b_m r) \\right]\n$$\nNow, substitute this into the error dynamics expression:\n$$\n\\dot{e} = \\left[ -ay + \\frac{b}{\\hat{b}}((\\hat{a} - a_m)y + b_m r) \\right] - [-a_m y_m + b_m r]\n$$\nLet's add and subtract $a_m y$ to the right-hand side:\n$$\n\\dot{e} = -a_m(y - y_m) - (a - a_m)y + \\frac{b}{\\hat{b}}((\\hat{a} - a_m)y + b_m r) - b_m r\n$$\n$$\n\\dot{e} = -a_m e - (a - \\hat{a} + \\hat{a} - a_m)y + \\frac{\\hat{b}+\\tilde{b}}{\\hat{b}}((\\hat{a} - a_m)y + b_m r) - b_m r\n$$\nwhere $\\tilde{b} = b-\\hat{b}$. After expansion and simplification, the error dynamics become:\n$$\n\\dot{e}(t) = -a_m e(t) - \\tilde{a}(t) y(t) + \\tilde{b}(t) u(t)\n$$\nwhere $\\tilde{a} = a - \\hat{a}$ and $\\tilde{b} = b - \\hat{b}$. This can be expressed as $\\dot{e} = -a_m e - \\tilde{\\theta}^T \\phi$.\nThe overall system consists of the stable estimator dynamics from Part 1 ensuring $\\tilde{\\theta}(t)$ is bounded, coupled with the tracking error dynamics, which can be viewed as a stable linear system ($\\dot{e}=-a_m e$) driven by a perturbation term $-\\tilde{\\theta}^T \\phi$.\n\nTo argue for stability, we must show that all signals in the closed loop remain bounded.\n1. From Part 1, the parameter error $\\tilde{\\theta}(t)$ is bounded, and thus the estimate $\\hat{\\theta}(t)$ is bounded.\n2. The reference signal $r(t)$ is bounded, and the reference model is stable ($a_m > 0$), so the reference output $y_m(t)$ is bounded.\n3. The tracking error dynamics is $\\dot{e} = -a_m e + w(t)$, where $w(t) = -\\tilde{\\theta}^T(t)\\phi(t)$. The regressor $\\phi(t)=[-y(t), u(t)]^T$ depends on the plant output $y(t)$ and the control input $u(t)$, which themselves depend on $e(t)$.\n4. The key insight is that since $\\tilde{\\theta}(t)$ is bounded and $\\hat{\\theta}(t)$ is well-behaved (due to projection), the perturbation $w(t)$ does not grow faster than $e(t)$. Because $a_m > 0$, the term $-a_m e$ provides stable, dissipative dynamics. A rigorous proof (e.g., using small-gain theory or a full system Lyapunov function) would show that this stabilizing term is sufficient to prevent the tracking error $e(t)$ from growing without bound.\n5. With $e(t)$ bounded, the plant output $y(t)=e(t)+y_m(t)$ is also bounded.\n6. Since $y(t), \\hat{a}(t), \\hat{b}(t),$ and $r(t)$ are all bounded (with $\\hat{b}(t)$ bounded away from zero), the control law $u(t)$ produces a bounded output.\n7. With $y(t)$ and $u(t)$ bounded, the regressor $\\phi(t)$ is bounded.\n\nTherefore, all signals in the closed-loop system are bounded. The controller \"stabilizes\" the system in the sense that it ensures uniform ultimate boundedness (UUB) of the tracking error. Proving asymptotic convergence ($e(t) \\to 0$) is more complex and typically requires a persistence of excitation condition on the regressor $\\phi(t)$. The only mild condition required is knowledge of the sign of $b$ and a lower bound for its magnitude to properly implement the projection in the estimator.", "answer": "$$\\boxed{\\frac{1}{\\hat{b}(t)} \\left( \\left(\\hat{a}(t) - a_{m}\\right) y(t) + b_{m} r(t) \\right)}$$", "id": "2722775"}]}