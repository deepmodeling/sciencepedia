{"hands_on_practices": [{"introduction": "This first exercise addresses the most fundamental question in any iterative process: how fast does it converge? By deriving the relationship between the abstract convergence factor $\\gamma$ and the number of trials required for a specified error reduction, you will gain a concrete understanding of what linear convergence implies in practice. This essential calculation forms the basis for predicting the performance and learning time of ILC and RC systems. [@problem_id:2714829]", "problem": "A discrete-time control system is operated repetitively on the same finite-time task. An Iterative Learning Control (ILC) scheme updates the control input from trial to trial. Let the trial-to-trial tracking error at the end of each trial be denoted by $e_{k}$, where $k \\in \\mathbb{N}_{0}$ is the trial index. Assume the learning update yields a linear error propagation of the form $e_{k+1} = \\mathcal{L}\\, e_{k}$, where $\\mathcal{L}$ is a bounded linear operator on a normed vector space with an induced norm satisfying $\\|\\mathcal{L}\\| \\leq \\gamma$ for some constant $0  \\gamma  1$. This same framework is used for Repetitive Control (RC) when interpreting $k$ as the period index.\n\nStarting from the submultiplicativity of induced norms and without invoking any specialized convergence result beyond basic properties of logarithms and norms, derive an inequality that relates the number of trials $k$ required to reduce the error norm by a factor $\\epsilon$ (with $0  \\epsilon  1$) to the iteration convergence factor $\\gamma$. Then, using your derived result, determine the smallest integer number of trials $k$ such that the error norm is reduced by at least a factor $\\epsilon$ when $\\gamma = 0.83$ and $\\epsilon = 0.01$.\n\nExpress your final answer as the minimal integer $k$ only. No rounding instructions are needed; choose the smallest integer that satisfies the requirement.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\nThe givens extracted from the statement are as follows:\n- A discrete-time control system is described.\n- The system operates repetitively on a finite-time task.\n- An Iterative Learning Control (ILC) or Repetitive Control (RC) scheme is used.\n- The trial/period index is $k \\in \\mathbb{N}_{0}$.\n- The error at the end of trial $k$ is $e_k$, an element of a normed vector space.\n- The error propagation is governed by the linear relationship $e_{k+1} = \\mathcal{L}\\, e_{k}$.\n- $\\mathcal{L}$ is a bounded linear operator.\n- The induced norm of the operator satisfies $\\|\\mathcal{L}\\| \\leq \\gamma$.\n- The convergence factor $\\gamma$ is a constant such that $0  \\gamma  1$.\n- The required reduction in error norm is by a factor $\\epsilon$, where $0  \\epsilon  1$.\n- The derivation must start from the submultiplicativity of induced norms and use basic properties of logarithms and norms.\n- The final calculation requires finding the smallest integer $k$ for $\\gamma = 0.83$ and $\\epsilon = 0.01$.\n\nThe problem is assessed for validity. It is a standard problem in the analysis of linear iterative systems, common in control theory, specifically in the study of ILC and RC. The model $e_{k+1} = \\mathcal{L}\\, e_{k}$ with the condition $\\|\\mathcal{L}\\|  1$ is the canonical form for demonstrating linear convergence. The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. There are no contradictions, no reliance on pseudoscience, and no ambiguities. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to derive an inequality for the number of trials $k$ required to achieve an error norm reduction of at least a factor of $\\epsilon$, and then to compute this minimal integer $k$ for the given parameters.\n\nWe begin with the error propagation dynamics:\n$$\ne_{k+1} = \\mathcal{L}\\, e_{k}\n$$\nwhere $k$ is the trial index. We take the vector norm of both sides of this equation:\n$$\n\\|e_{k+1}\\| = \\|\\mathcal{L}\\, e_{k}\\|\n$$\nThe problem specifies that the norm is an induced norm. A fundamental property of an induced norm is its submultiplicativity, which states that for a linear operator $\\mathcal{L}$ and a vector $e_k$, the inequality $\\|\\mathcal{L}\\, e_k\\| \\leq \\|\\mathcal{L}\\| \\|e_k\\|$ holds. Applying this property, we get:\n$$\n\\|e_{k+1}\\| \\leq \\|\\mathcal{L}\\| \\|e_k\\|\n$$\nWe are given that $\\|\\mathcal{L}\\| \\leq \\gamma$. Substituting this into the inequality yields the fundamental recursive relationship for the error norm:\n$$\n\\|e_{k+1}\\| \\leq \\gamma \\|e_k\\|\n$$\nThis inequality relates the error norm at trial $k+1$ to the error norm at trial $k$. We can unroll this recursion to establish a relationship between the error at trial $k$ and the initial error at trial $0$, namely $\\|e_0\\|$.\n\nFor $k=1$:\n$$\n\\|e_1\\| \\leq \\gamma \\|e_0\\|\n$$\nFor $k=2$:\n$$\n\\|e_2\\| \\leq \\gamma \\|e_1\\| \\leq \\gamma (\\gamma \\|e_0\\|) = \\gamma^2 \\|e_0\\|\n$$\nBy mathematical induction, we can prove the general relationship $\\|e_k\\| \\leq \\gamma^k \\|e_0\\|$ for any integer $k \\geq 1$.\nThe base case for $k=1$ is shown above.\nAssume the hypothesis is true for some integer $j \\geq 1$, i.e., $\\|e_j\\| \\leq \\gamma^j \\|e_0\\|$.\nThen for $j+1$, we have:\n$$\n\\|e_{j+1}\\| \\leq \\gamma \\|e_j\\| \\leq \\gamma (\\gamma^j \\|e_0\\|) = \\gamma^{j+1} \\|e_0\\|\n$$\nThe hypothesis holds for $j+1$. Thus, the inequality $\\|e_k\\| \\leq \\gamma^k \\|e_0\\|$ is proven for all $k \\geq 1$.\n\nThe problem asks for the number of trials $k$ required to reduce the error norm by a factor $\\epsilon$. This is equivalent to finding $k$ such that the ratio of the error norm at trial $k$ to the initial error norm is less than or equal to $\\epsilon$. Assuming a non-trivial initial error, i.e., $\\|e_0\\|  0$, this condition is:\n$$\n\\frac{\\|e_k\\|}{\\|e_0\\|} \\leq \\epsilon\n$$\nFrom our derived inequality, we have $\\frac{\\|e_k\\|}{\\|e_0\\|} \\leq \\gamma^k$. Therefore, a sufficient condition to satisfy the error reduction requirement is:\n$$\n\\gamma^k \\leq \\epsilon\n$$\nThis is a key inequality relating $k$, $\\gamma$, and $\\epsilon$. To solve for $k$, we use the properties of logarithms. We take the natural logarithm of both sides:\n$$\n\\ln(\\gamma^k) \\leq \\ln(\\epsilon)\n$$\nUsing the logarithm power rule, $\\ln(a^b) = b \\ln(a)$, we obtain:\n$$\nk \\ln(\\gamma) \\leq \\ln(\\epsilon)\n$$\nTo isolate $k$, we must divide by $\\ln(\\gamma)$. It is given that $0  \\gamma  1$, which implies that $\\ln(\\gamma)$ is a negative number. Dividing an inequality by a negative number reverses the direction of the inequality sign.\n$$\nk \\geq \\frac{\\ln(\\epsilon)}{\\ln(\\gamma)}\n$$\nThis is the general inequality relating the number of trials $k$ to the convergence factor $\\gamma$ and the desired error reduction factor $\\epsilon$.\n\nWe now proceed to the second part of the problem: to calculate the smallest integer number of trials $k$ for the specific values $\\gamma = 0.83$ and $\\epsilon = 0.01$.\nSubstituting these values into the derived inequality:\n$$\nk \\geq \\frac{\\ln(0.01)}{\\ln(0.83)}\n$$\nThe values of the logarithms are:\n$$\n\\ln(0.01) = \\ln(10^{-2}) = -2 \\ln(10) \\approx -4.605170185988092\n$$\n$$\n\\ln(0.83) \\approx -0.186320953188556\n$$\nThe ratio is:\n$$\n\\frac{\\ln(0.01)}{\\ln(0.83)} \\approx \\frac{-4.605170185988092}{-0.186320953188556} \\approx 24.71618...\n$$\nSo, the condition on $k$ is:\n$$\nk \\geq 24.71618...\n$$\nThe problem demands the smallest integer $k$ that satisfies this condition. Since $k$ must be an integer, we must choose the smallest integer greater than or equal to $24.71618...$, which is $25$. This operation is mathematically equivalent to the ceiling function, $\\lceil 24.71618... \\rceil = 25$.\n\nTherefore, a minimum of $25$ trials is required to guarantee that the error norm is reduced by at least a factor of $0.01$.", "answer": "$$\n\\boxed{25}\n$$", "id": "2714829"}, {"introduction": "Building on the general principle of convergence, this practice delves into the analysis of a specific ILC system using the powerful time-domain lifted representation. You will derive the condition for monotonic convergence in the $\\ell_\\infty$ norm, which guarantees that the peak tracking error over the trial decreases with each iteration. This exercise is crucial for learning how to formally prove stability and for appreciating how the choice of norm corresponds to different performance objectives. [@problem_id:2714828]", "problem": "Consider a Single-Input Single-Output (SISO) Iterative Learning Control (ILC) setup over a finite horizon of length $N=2$, with the lifted input-output map represented by a lower-triangular Toeplitz matrix $G \\in \\mathbb{R}^{2 \\times 2}$ formed from the plant’s first two Markov parameters. Let\n$$\nG = \\begin{pmatrix} g_0  0 \\\\ g_1  g_0 \\end{pmatrix},\n$$\nwith $g_0 = 0.8$ and $g_1 = 0.3$. The ILC learning update is $u_{k+1} = u_k + L e_k$, where $L \\in \\mathbb{R}^{2 \\times 2}$ is the learning filter, $u_k \\in \\mathbb{R}^2$ is the $k$-th trial input, and $e_k \\in \\mathbb{R}^2$ is the $k$-th trial tracking error. Assume a repeatable reference so that the trial-to-trial error propagation is given by the lifted linear map\n$$\ne_{k+1} = \\left( I - G L \\right) e_k.\n$$\nYou are to analyze monotonic convergence in the $\\ell_\\infty$ sense and contrast it with the $\\ell_2$ sense, starting only from the definitions of induced norms and fundamental norm properties. In particular:\n\n1. Using only the definition of the induced vector norm and submultiplicativity, derive a sufficient condition on $L$ that guarantees the monotone bound\n$$\n\\| e_{k+1} \\|_\\infty \\le \\gamma \\, \\| e_k \\|_\\infty \\quad \\text{with} \\quad \\gamma  1,\n$$\nin terms of the induced $\\ell_\\infty$ matrix norm of $I - G L$.\n\n2. Specialize to the scalar-gain learning filter $L = \\alpha I$, with $\\alpha \\in \\mathbb{R}$. Derive the condition on $\\alpha$ that enforces $\\| I - G L \\|_\\infty  1$ by explicitly computing the induced $\\ell_\\infty$ norm of $I - \\alpha G$ from first principles.\n\n3. For the specific numerical values $g_0 = 0.8$ and $g_1 = 0.3$, determine the supremum scalar gain\n$$\n\\alpha_\\star = \\sup \\left\\{ \\alpha  0 : \\| I - \\alpha G \\|_\\infty  1 \\right\\}.\n$$\nReport your final answer for $\\alpha_\\star$ as an exact value (no rounding).\n\n4. Briefly explain, without computing a numerical value, how the analogous $\\ell_2$-monotonicity condition would be posed in terms of the induced $\\ell_2$ norm, and why it can be less conservative or more conservative than the $\\ell_\\infty$-based condition depending on $G$.\n\nYour final answer must be the single exact value of $\\alpha_\\star$ with no units and no rounding instructions required.", "solution": "We begin by validating the problem. The problem statement is situated within the standard theoretical framework of Iterative Learning Control (ILC) for discrete-time, linear time-invariant systems. All components—the lifted system representation using a Toeplitz matrix of Markov parameters, the P-type ILC update law, the resulting error dynamics, and the analysis of convergence using induced matrix norms—are canonical concepts in this field. The provided numerical values are physically plausible. The problem is self-contained, unambiguous, and mathematically well-posed. The problem is therefore valid, and we may proceed to the solution.\n\nThe problem is addressed in four parts as requested.\n\n1. The trial-to-trial error propagation dynamics are given by the linear map\n$$\ne_{k+1} = (I - G L) e_k.\n$$\nTo establish a condition for monotonic convergence in the $\\ell_\\infty$ norm, we take the $\\ell_\\infty$ norm of both sides of this equation:\n$$\n\\| e_{k+1} \\|_\\infty = \\| (I - G L) e_k \\|_\\infty.\n$$\nBy the definition of an induced matrix norm, for any vector $x$ and matrix $A$, we have the inequality $\\|Ax\\| \\le \\|A\\| \\|x\\|$. Applying this property to our expression yields\n$$\n\\| (I - G L) e_k \\|_\\infty \\le \\| I - G L \\|_\\infty \\| e_k \\|_\\infty.\n$$\nCombining the previous steps, we obtain the bound\n$$\n\\| e_{k+1} \\|_\\infty \\le \\| I - G L \\|_\\infty \\| e_k \\|_\\infty.\n$$\nThe problem requires finding a condition that guarantees $\\| e_{k+1} \\|_\\infty \\le \\gamma \\| e_k \\|_\\infty$ for some constant $\\gamma  1$. By inspection of the derived bound, this is satisfied if we choose $\\gamma = \\| I - G L \\|_\\infty$. Therefore, the sufficient condition for monotonic convergence in the $\\ell_\\infty$ sense is\n$$\n\\| I - G L \\|_\\infty  1.\n$$\n\n2. We now specialize to the case of a scalar learning gain, $L = \\alpha I$, where $\\alpha \\in \\mathbb{R}$ is a scalar and $I$ is the $2 \\times 2$ identity matrix. The convergence condition becomes $\\| I - \\alpha G \\|_\\infty  1$. Let us compute the matrix $I - \\alpha G$:\n$$\nI - \\alpha G = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\alpha \\begin{pmatrix} g_0  0 \\\\ g_1  g_0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\alpha g_0  0 \\\\ -\\alpha g_1  1 - \\alpha g_0 \\end{pmatrix}.\n$$\nThe induced $\\ell_\\infty$ matrix norm, also known as the maximum absolute row sum norm, is defined for a matrix $A$ as $\\|A\\|_\\infty = \\max_{i} \\sum_{j} |A_{ij}|$. We apply this definition to the matrix $I - \\alpha G$.\nThe absolute sum of the elements in the first row is:\n$$\n|1 - \\alpha g_0| + |0| = |1 - \\alpha g_0|.\n$$\nThe absolute sum of the elements in the second row is:\n$$\n|-\\alpha g_1| + |1 - \\alpha g_0| = |\\alpha| |g_1| + |1 - \\alpha g_0|.\n$$\nThe $\\ell_\\infty$ norm is the maximum of these two row sums:\n$$\n\\|I - \\alpha G\\|_\\infty = \\max \\left( |1 - \\alpha g_0|, \\;\\; |\\alpha| |g_1| + |1 - \\alpha g_0| \\right).\n$$\nSince $|\\alpha| |g_1| \\ge 0$, the second term is always greater than or equal to the first term. Thus, the maximum is always given by the second row sum:\n$$\n\\|I - \\alpha G\\|_\\infty = |1 - \\alpha g_0| + |\\alpha| |g_1|.\n$$\nThe condition for monotonic convergence is therefore $|1 - \\alpha g_0| + |\\alpha| |g_1|  1$.\n\n3. We are asked to find $\\alpha_\\star = \\sup \\{ \\alpha  0 : \\| I - \\alpha G \\|_\\infty  1 \\}$ for the specific values $g_0 = 0.8$ and $g_1 = 0.3$. Since we seek $\\alpha  0$, the expression $|\\alpha|$ simplifies to $\\alpha$. The convergence condition becomes:\n$$\n|1 - 0.8 \\alpha| + 0.3 \\alpha  1.\n$$\nTo solve this inequality, we must resolve the absolute value by considering the sign of its argument, $1 - 0.8 \\alpha$. The term is zero when $0.8 \\alpha = 1$, which corresponds to $\\alpha = 1 / 0.8 = 1.25$. We analyze two cases for $\\alpha  0$.\nCase 1: $0  \\alpha \\le 1.25$.\nIn this interval, $1 - 0.8 \\alpha \\ge 0$, so $|1 - 0.8 \\alpha| = 1 - 0.8 \\alpha$. The inequality is:\n$$\n(1 - 0.8 \\alpha) + 0.3 \\alpha  1\n$$\n$$\n1 - 0.5 \\alpha  1\n$$\n$$\n-0.5 \\alpha  0\n$$\nThis implies $\\alpha  0$. The solution for this case is the intersection of the domain $0  \\alpha \\le 1.25$ and the result $\\alpha  0$, which is the interval $(0, 1.25]$.\n\nCase 2: $\\alpha  1.25$.\nIn this interval, $1 - 0.8 \\alpha  0$, so $|1 - 0.8 \\alpha| = -(1 - 0.8 \\alpha) = 0.8 \\alpha - 1$. The inequality is:\n$$\n(0.8 \\alpha - 1) + 0.3 \\alpha  1\n$$\n$$\n1.1 \\alpha - 1  1\n$$\n$$\n1.1 \\alpha  2\n$$\nThis implies $\\alpha  \\frac{2}{1.1} = \\frac{20}{11}$. The solution for this case is the intersection of the domain $\\alpha  1.25$ and the result $\\alpha  20/11$. Note that $1.25 = 5/4$ and $20/11 \\approx 1.818$, so $1.25  20/11$. The interval is $(1.25, 20/11)$.\n\nCombining the results from both cases, the set of all positive $\\alpha$ satisfying the convergence condition is the union $(0, 1.25] \\cup (1.25, 20/11)$, which is the open interval $(0, 20/11)$.\nThe supremum of this set is the upper bound of the interval. Therefore,\n$$\n\\alpha_\\star = \\sup \\left( 0, \\frac{20}{11} \\right) = \\frac{20}{11}.\n$$\n\n4. The analogous condition for monotonic convergence in the $\\ell_2$ sense is $\\| I - G L \\|_2  1$, where $\\| \\cdot \\|_2$ denotes the induced matrix $2$-norm (the spectral norm). For $L = \\alpha I$, this becomes $\\| I - \\alpha G \\|_2  1$. The induced $2$-norm of a matrix $A$ is its largest singular value, $\\sigma_{\\max}(A)$, which is the square root of the largest eigenvalue of $A^T A$. Thus, the condition is $\\sigma_{\\max}(I - \\alpha G)  1$.\n\nThe relative conservativeness of the $\\ell_\\infty$ and $\\ell_2$ conditions depends on the specific matrix $G$. For any matrix $A \\in \\mathbb{R}^{n \\times n}$, the induced norms are related by inequalities such as $\\|A\\|_2 \\le \\sqrt{n} \\|A\\|_\\infty$ and $\\|A\\|_\\infty \\le \\sqrt{n} \\|A\\|_2$, but there is no universal ordering between $\\|A\\|_2$ and $\\|A\\|_\\infty$. One can be larger, smaller, or equal to the other depending on the structure of $A$. Consequently, for a given $G$ and $\\alpha$, it is possible to have $\\|I - \\alpha G\\|_\\infty  1$ while $\\|I - \\alpha G\\|_2 \\ge 1$, which would mean the $\\ell_2$ condition is more conservative for that $\\alpha$. Conversely, it is also possible that $\\|I - \\alpha G\\|_2  1$ while $\\|I - \\alpha G\\|_\\infty \\ge 1$, meaning the $\\ell_\\infty$ condition is more conservative. The choice of norm dictates the shape and size of the set of gains $\\alpha$ that guarantee monotonic convergence, and neither the $\\ell_2$ nor the $\\ell_\\infty$ condition is universally less conservative than the other. The comparison depends entirely on the numerical properties of the matrix $I - \\alpha G$.", "answer": "$$\n\\boxed{\\frac{20}{11}}\n$$", "id": "2714828"}, {"introduction": "Moving from analysis to synthesis, this final exercise introduces a modern method for designing an optimal ILC controller. You will formulate the problem of finding the best learning gain matrix $L$ as a convex optimization problem, which can be solved efficiently to minimize the convergence rate in the $\\ell_2$ sense. This practice demonstrates how to systematically engineer a learning controller for the fastest possible convergence, representing a key technique in advanced control design. [@problem_id:2714813]", "problem": "Consider a finite-horizon lifted representation of a single-input single-output discrete-time plant in Iterative Learning Control (ILC), with trial length $N = 2$. The lifted plant matrix is assumed lower-triangular Toeplitz of the form\n$$\nG = \\begin{bmatrix} g_{0}  0 \\\\ g_{1}  g_{0} \\end{bmatrix},\n$$\nand the learning operator is constrained to be a lower-triangular banded Toeplitz matrix\n$$\nL = \\begin{bmatrix} \\ell_{0}  0 \\\\ \\ell_{1}  \\ell_{0} \\end{bmatrix}.\n$$\nDefine the iteration-to-iteration error propagation as $e^{k+1} = (I - G L)\\, e^{k}$, where $I$ is the identity matrix. Stability and monotonic convergence in norm are ensured if the induced two-norm satisfies $\\| I - G L \\|_{2}  1$. To synthesize $L$, we will minimize the induced two-norm of the error propagation operator subject to structural and magnitude constraints on $L$.\n\nTasks:\n1. Starting from the definition of the induced two-norm $\\|A\\|_{2} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{2}}{\\|x\\|_{2}}$ and the Schur complement, pose a convex optimization problem that minimizes $\\| I - G L \\|_{2}$ over the decision variables $(\\ell_{0}, \\ell_{1})$, subject to the linear equality constraints implied by the lower-triangular banded Toeplitz structure and the elementwise magnitude bounds $|\\ell_{0}| \\leq \\tfrac{1}{2}$ and $|\\ell_{1}| \\leq \\tfrac{1}{2}$. Your formulation must use a Linear Matrix Inequality (LMI) to represent the spectral-norm objective via an epigraph variable $\\gamma$, and it must make clear why the problem is a Semidefinite Program (SDP).\n2. Specialize to the numerically specified plant with $g_{0} = 1$ and $g_{1} = 1$. Using first principles and exact algebra (no numerical solvers), compute the optimal value of the objective $\\min \\| I - G L \\|_{2}$ subject to $|\\ell_{0}| \\leq \\tfrac{1}{2}$ and $|\\ell_{1}| \\leq \\tfrac{1}{2}$. You may exploit the Toeplitz structure to simplify the singular-value calculation, but you must derive any intermediate expressions you use. Express your final answer as a single real number. No rounding is required. No units are required.\n\nYour final answer must be the optimal objective value $\\min \\| I - G L \\|_{2}$ for the given numerical $G$ and bounds on $(\\ell_{0}, \\ell_{1})$.", "solution": "We begin from the definition of the induced two-norm for a real matrix $A$, namely $\\|A\\|_{2} = \\sigma_{\\max}(A)$, the largest singular value of $A$. The epigraph form of minimizing a spectral norm is\n$$\n\\min_{A,\\gamma} \\;\\gamma \\quad \\text{subject to} \\quad \\|A\\|_{2} \\leq \\gamma.\n$$\nA standard Linear Matrix Inequality (LMI) representation of the spectral-norm bound uses the Schur complement. Specifically, for any real matrix $A$, the bound $\\|A\\|_{2} \\leq \\gamma$ is equivalent to the LMI\n$$\n\\begin{bmatrix}\n\\gamma I  A \\\\\nA^{\\mathsf{T}}  \\gamma I\n\\end{bmatrix} \\succeq 0,\n$$\nwith $\\gamma \\geq 0$. This equivalence follows because the positive semidefiniteness of the block matrix enforces $\\gamma^{2} I - A^{\\mathsf{T}} A \\succeq 0$ via the Schur complement, which is in turn equivalent to all singular values of $A$ being at most $\\gamma$.\n\nWe now apply this to the lifted ILC error propagation operator $I - G L$. The decision variables are $(\\ell_{0},\\ell_{1})$, and the structural constraints that $L$ be lower-triangular banded Toeplitz are captured by the parameterization\n$$\nL = \\begin{bmatrix} \\ell_{0}  0 \\\\ \\ell_{1}  \\ell_{0} \\end{bmatrix}.\n$$\nThe magnitude constraints are $|\\ell_{0}| \\leq \\tfrac{1}{2}$ and $|\\ell_{1}| \\leq \\tfrac{1}{2}$. Because $I - G L$ depends affinely on $(\\ell_{0}, \\ell_{1})$, and the spectral norm constraint is convex and admits an LMI representation, the overall problem is a Semidefinite Program (SDP), hence tractable in polynomial time using standard interior-point methods.\n\nFormally, the convex optimization problem is:\n$$\n\\begin{aligned}\n\\min_{\\ell_{0},\\ell_{1},\\;\\gamma}  \\gamma \\\\\n\\text{subject to}  \\begin{bmatrix}\n\\gamma I  I - G L \\\\\n(I - G L)^{\\mathsf{T}}  \\gamma I\n\\end{bmatrix} \\succeq 0, \\\\\n |\\ell_{0}| \\leq \\tfrac{1}{2},\\;\\; |\\ell_{1}| \\leq \\tfrac{1}{2}.\n\\end{aligned}\n$$\nThis is an SDP because the feasible set is defined by Linear Matrix Inequalities in the decision variables and the objective is linear in $\\gamma$.\n\nWe now specialize to the numerical plant with $g_{0} = 1$ and $g_{1} = 1$, namely\n$$\nG = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}, \\qquad\nL = \\begin{bmatrix} \\ell_{0}  0 \\\\ \\ell_{1}  \\ell_{0} \\end{bmatrix}.\n$$\nCompute the product $G L$:\n$$\nG L = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}\n\\begin{bmatrix} \\ell_{0}  0 \\\\ \\ell_{1}  \\ell_{0} \\end{bmatrix}\n= \\begin{bmatrix} \\ell_{0}  0 \\\\ \\ell_{0} + \\ell_{1}  \\ell_{0} \\end{bmatrix}.\n$$\nTherefore\n$$\nI - G L = \\begin{bmatrix} 1 - \\ell_{0}  0 \\\\ -(\\ell_{0} + \\ell_{1})  1 - \\ell_{0} \\end{bmatrix}.\n$$\nIt is convenient to define\n$$\na = 1 - \\ell_{0}, \\qquad b = -(\\ell_{0} + \\ell_{1}),\n$$\nso that\n$$\nA \\triangleq I - G L = \\begin{bmatrix} a  0 \\\\ b  a \\end{bmatrix}.\n$$\nWe now compute the singular values of $A$ exactly. Since $A^{\\mathsf{T}} A$ is\n$$\nA^{\\mathsf{T}} A = \\begin{bmatrix} a  b \\\\ 0  a \\end{bmatrix} \\begin{bmatrix} a  0 \\\\ b  a \\end{bmatrix}\n= \\begin{bmatrix} a^{2} + b^{2}  a b \\\\ a b  a^{2} \\end{bmatrix},\n$$\nthe squared singular values of $A$ are the eigenvalues $\\lambda$ of $A^{\\mathsf{T}} A$. The characteristic polynomial gives $\\lambda^2 - (2a^2+b^2)\\lambda + a^4 = 0$. The largest eigenvalue is\n$$\n\\lambda_{\\max}(A^{\\mathsf{T}} A) = \\frac{ 2 a^{2} + b^{2} + \\sqrt{ (2 a^{2} + b^{2})^2 - 4a^4 } }{2}\n= \\frac{ 2 a^{2} + b^{2} + \\sqrt{ b^{2} \\left(4 a^{2} + b^{2}\\right) } }{2}\n= \\frac{ 2 a^{2} + b^{2} + |b| \\sqrt{ 4 a^{2} + b^{2} } }{2}.\n$$\nHence the induced two-norm is\n$$\n\\|A\\|_{2} = \\sqrt{ \\lambda_{\\max}(A^{\\mathsf{T}} A) } = \\left[ \\frac{ 2 a^{2} + b^{2} + |b| \\sqrt{ 4 a^{2} + b^{2} } }{2} \\right]^{\\!1/2}.\n$$\nWe must minimize $\\|A\\|_{2}$ over $(\\ell_{0},\\ell_{1})$ subject to $|\\ell_{0}| \\leq \\tfrac{1}{2}$ and $|\\ell_{1}| \\leq \\tfrac{1}{2}$. In the variables $(a,b)$, these constraints become\n$$\na = 1 - \\ell_{0} \\in \\left[ 1 - \\tfrac{1}{2},\\, 1 + \\tfrac{1}{2} \\right] = [\\tfrac{1}{2},\\, \\tfrac{3}{2}],\n\\qquad\nb = -(\\ell_{0} + \\ell_{1}) \\in \\left[ -1,\\, 1 \\right],\n$$\nbecause $\\ell_{0} + \\ell_{1}$ ranges over $[-1,1]$ when each lies in $[-\\tfrac{1}{2},\\tfrac{1}{2}]$.\n\nObserve from the expression for $\\lambda_{\\max}$, that for any fixed $a$ on its feasible domain $[\\tfrac{1}{2}, \\tfrac{3}{2}]$, all terms are non-negative. The terms $b^2$ and $|b|\\sqrt{4a^2+b^2}$ are both non-decreasing functions of $|b|$ for $|b| \\ge 0$. Therefore, $\\lambda_{\\max}$, and consequently its square root $\\|A\\|_2$, is minimized by choosing the smallest feasible value for $|b|$, which is $|b| = 0$.\n\nThe condition $b = 0$ is achieved by choosing\n$$\nb = -(\\ell_{0} + \\ell_{1}) = 0 \\quad \\Longleftrightarrow \\quad \\ell_{1} = -\\ell_{0}.\n$$\nThis choice is feasible because whenever $|\\ell_{0}| \\leq \\tfrac{1}{2}$, setting $\\ell_{1} = -\\ell_{0}$ yields $|\\ell_{1}| = |\\ell_{0}| \\leq \\tfrac{1}{2}$. With $b = 0$, the matrix reduces to $A = a I$, and the induced two-norm simplifies to\n$$\n\\|A\\|_{2} = |a| = |1 - \\ell_{0}|.\n$$\nWe now minimize $|1 - \\ell_{0}|$ over $|\\ell_{0}| \\leq \\tfrac{1}{2}$. Because $\\ell_{0} \\in [-\\tfrac{1}{2}, \\tfrac{1}{2}]$, the value $1 - \\ell_{0}$ is minimized in absolute value by taking $\\ell_{0}$ as large as possible, namely $\\ell_{0} = \\tfrac{1}{2}$, which yields\n$$\na = 1 - \\ell_{0} = 1 - \\tfrac{1}{2} = \\tfrac{1}{2}, \\qquad b = 0.\n$$\nTherefore the optimal induced two-norm is\n$$\n\\| I - G L \\|_{2}^{\\star} = \\left\\| \\begin{bmatrix} \\tfrac{1}{2}  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix} \\right\\|_{2} = \\tfrac{1}{2}.\n$$\nThis value can be achieved by, for example,\n$$\n\\ell_{0}^{\\star} = \\tfrac{1}{2}, \\qquad \\ell_{1}^{\\star} = -\\tfrac{1}{2},\n$$\nwhich satisfy the magnitude bounds and yield $I - G L = \\tfrac{1}{2} I$.\n\nTractability discussion: In general, for lifted horizon $N$, with $G$ lower-triangular Toeplitz (capturing convolution) and $L$ constrained to a banded Toeplitz structure, the map $(\\text{parameters of }L) \\mapsto I - G L$ is affine. The objective $\\| I - G L \\|_{2}$ is a convex function (composition of a norm with an affine mapping), and the Toeplitz/banded constraints are linear equalities, with optional magnitude constraints being convex. The epigraph reformulation with the block LMI\n$$\n\\begin{bmatrix} \\gamma I  I - G L \\\\ (I - G L)^{\\mathsf{T}}  \\gamma I \\end{bmatrix} \\succeq 0\n$$\nrenders the problem as an SDP in the coefficients of $L$ and the scalar $\\gamma$, solvable in polynomial time for finite $N$ using interior-point methods. Thus the synthesis is convex and tractable in the lifted finite-horizon setting; in the infinite-horizon periodic or repetitive-control setting, an analogous frequency-domain formulation leads to a norm minimization over frequencies that can be approximated or handled using standard $\\mathcal{H}_{\\infty}$ techniques.", "answer": "$$\\boxed{0.5}$$", "id": "2714813"}]}