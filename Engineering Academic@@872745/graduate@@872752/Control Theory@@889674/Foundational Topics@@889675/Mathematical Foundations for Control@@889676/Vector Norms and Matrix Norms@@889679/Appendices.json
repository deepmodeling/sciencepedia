{"hands_on_practices": [{"introduction": "In analyzing dynamical systems, the spectral radius $\\rho(A)$ governs long-term stability, but it doesn't tell the whole story. This exercise explores the crucial distinction between the spectral radius and the matrix norm, which can characterize short-term transient behavior. You will construct a matrix that is stable in the long run ($\\rho(A)=0$) but whose norm is non-zero, a key to understanding the performance of non-normal systems [@problem_id:2449584].", "problem": "In a linear fixed-point iteration used in computational engineering, the iteration matrix controls stability. Construct a $2 \\times 2$ real matrix $A$ that satisfies the two properties $\\rho(A) = 0$ and $\\|A\\|_{F} = 1$, where $\\rho(A)$ denotes the spectral radius and $\\|A\\|_{F}$ denotes the Frobenius norm. Then determine the exact value of the limit\n$$\nL = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F}.\n$$\nProvide your answer as a single real number. No rounding is necessary.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It presents a solvable problem in linear algebra with direct relevance to computational engineering. We shall proceed with a rigorous solution.\n\nThe problem requires us to first understand the implications of the given properties for a $2 \\times 2$ real matrix $A$, and then to compute a limit involving its powers. The properties are:\n$1$. The spectral radius $\\rho(A) = 0$.\n$2$. The Frobenius norm $\\|A\\|_{F} = 1$.\n\nThe spectral radius $\\rho(A)$ of a matrix $A$ is defined as the maximum of the absolute values of its eigenvalues.\n$$\n\\rho(A) = \\max_{i} |\\lambda_i|\n$$\nwhere $\\lambda_i$ are the eigenvalues of $A$. For the given $2 \\times 2$ matrix $A$, let its eigenvalues be $\\lambda_1$ and $\\lambda_2$. The condition $\\rho(A) = 0$ implies that $\\max(|\\lambda_1|, |\\lambda_2|) = 0$. This is only possible if $|\\lambda_1| = 0$ and $|\\lambda_2| = 0$, which means both eigenvalues must be zero: $\\lambda_1 = \\lambda_2 = 0$.\n\nA matrix whose eigenvalues are all zero is a nilpotent matrix. The characteristic polynomial of $A$, denoted $p(\\lambda)$, is given by $p(\\lambda) = \\det(A - \\lambda I)$. Since the roots of the characteristic polynomial are the eigenvalues of the matrix, for $A$ we have:\n$$\np(\\lambda) = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = (\\lambda - 0)(\\lambda - 0) = \\lambda^2\n$$\nAccording to the Cayley-Hamilton theorem, every square matrix satisfies its own characteristic equation. Therefore, substituting the matrix $A$ into its characteristic polynomial yields the zero matrix:\n$$\np(A) = A^2 = O\n$$\nwhere $O$ is the $2 \\times 2$ zero matrix. Thus, any $2 \\times 2$ matrix $A$ with $\\rho(A)=0$ must satisfy $A^2 = O$. The second condition, $\\|A\\|_{F}=1$, ensures that $A$ is not itself the zero matrix, since $\\|O\\|_F = 0$. Therefore, $A$ is a non-zero nilpotent matrix of index $2$.\n\nAn explicit construction confirms the existence of such a matrix. Consider the matrix\n$$\nA = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nThis is an upper triangular matrix, so its eigenvalues are its diagonal entries, which are $\\lambda_1 = 0$ and $\\lambda_2 = 0$. Thus, $\\rho(A) = 0$. The Frobenius norm is calculated as\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{2} \\sum_{j=1}^{2} |a_{ij}|^2} = \\sqrt{0^2 + 1^2 + 0^2 + 0^2} = \\sqrt{1} = 1\n$$\nBoth conditions are satisfied. Computing its square gives:\n$$\nA^2 = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = O\n$$\nThis confirms our deduction from the Cayley-Hamilton theorem.\n\nThe problem requires the evaluation of the limit $L = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F}$. We must analyze the sequence of matrix powers $A^k$ for integers $k \\ge 1$.\nFor $k=1$: $A^1 = A$.\nFor $k=2$: $A^2 = O$.\nFor $k=3$: $A^3 = A^2 \\cdot A = O \\cdot A = O$.\nBy induction, for any integer $k \\ge 2$, the matrix power $A^k$ is the zero matrix:\n$$\nA^k = O \\quad \\forall k \\ge 2\n$$\nNow, we consider the sequence of the Frobenius norms of these matrix powers, $\\{ \\|A^k\\|_F \\}_{k=1}^\\infty$.\nFor $k=1$, we have $\\|A^1\\|_F = \\|A\\|_F = 1$.\nFor any integer $k \\ge 2$, we have $\\|A^k\\|_F = \\|O\\|_F$. The Frobenius norm of the zero matrix is:\n$$\n\\|O\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0\n$$\nTherefore, the sequence of real numbers is $\\{1, 0, 0, 0, \\dots\\}$. The limit of this sequence as $k \\to \\infty$ is the value the terms eventually take and remain at. For any integer $k  1$, the term in the sequence is $0$. By the formal definition of a limit, for any chosen $\\epsilon  0$, we can select an integer $N=1$. Then, for all $k  N$, we have $\\|A^k\\|_F = 0$, and thus $|\\|A^k\\|_F - 0| = 0  \\epsilon$.\n\nThe limit is unequivocally $0$.\n$$\nL = \\lim_{k \\to \\infty} \\|A^{k}\\|_{F} = 0\n$$\nThe result is independent of the specific choice of matrix $A$, provided it satisfies the stated properties. The structure imposed by $\\rho(A)=0$ on a $2 \\times 2$ matrix is what dictates the result.", "answer": "$$\n\\boxed{0}\n$$", "id": "2449584"}, {"introduction": "The choice of coordinate system can significantly influence the measured properties of a linear system. This practice investigates how different induced matrix norms behave under an orthogonal change of basis, which corresponds to a rotation of the state space [@problem_id:2757383]. By comparing the basis-invariant induced $2$-norm with the basis-dependent induced $1$-norm, you will gain a deeper appreciation for their distinct roles in system analysis.", "problem": "Consider the continuous-time linear time-invariant (LTI) state equation $\\dot{x}(t) = A x(t)$ with $A \\in \\mathbb{R}^{2 \\times 2}$. For a change of state coordinates given by an orthogonal matrix $Q \\in \\mathbb{R}^{2 \\times 2}$, let $\\tilde{x}(t) = Q^{\\top} x(t)$ so that the transformed state matrix is $\\tilde{A} = Q^{\\top} A Q$. Adopt the induced operator norms defined for any $p \\in [1,\\infty]$ by\n$$\n\\|A\\|_{p \\to p} \\;=\\; \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}.\n$$\nUse only this definition and the fact that orthogonal matrices preserve the Euclidean norm to reason about $\\|\\cdot\\|_{2 \\to 2}$ under orthogonal transformations.\n\nNow specialize to the family\n$$\nA(\\theta,L) \\;=\\; Q(\\theta)\\,\\mathrm{diag}(L,1)\\,Q(\\theta)^{\\top},\n$$\nwhere $Q(\\theta) = \\begin{pmatrix}\\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{pmatrix}$ is a planar rotation, with $\\theta = \\pi/3$ (radians) and $L = 10$. This choice produces a matrix with large off-diagonal entries in the standard basis.\n\nWithout invoking any results not derivable from the above definition and basic linear algebra, do the following:\n- Reason from first principles why $\\|A(\\theta,L)\\|_{2 \\to 2}$ is invariant under the orthogonal similarity induced by $Q(\\theta)$ and determine its value for the given $A(\\theta,L)$.\n- Compute $\\|A(\\theta,L)\\|_{1 \\to 1}$ explicitly by evaluating the maximum absolute column sum in the standard basis.\n- Compare the sensitivity of $\\|\\cdot\\|_{1 \\to 1}$ to the choice of basis by forming the ratio\n$$\nr \\;=\\; \\frac{\\|A(\\theta,L)\\|_{1 \\to 1}}{\\|\\mathrm{diag}(L,1)\\|_{1 \\to 1}}.\n$$\n\nExpress the final ratio $r$ as a pure number, rounded to four significant figures. No units are required.", "solution": "The problem is well-posed and scientifically sound, resting on fundamental principles of linear algebra and matrix theory as applied in control systems. We proceed with the solution.\n\nThe problem consists of three parts. First, we must establish the invariance of the induced $2$-norm, $\\|\\cdot\\|_{2 \\to 2}$, under an orthogonal similarity transformation and use this property to determine $\\|A(\\theta,L)\\|_{2 \\to 2}$. Second, we must compute $\\|A(\\theta,L)\\|_{1 \\to 1}$. Third, we must compute the ratio, $r$, of this norm to the $1$-norm of the diagonal matrix $\\mathrm{diag}(L,1)$.\n\nPart 1: Invariance of the $\\|\\cdot\\|_{2 \\to 2}$ norm and its value.\nLet $M \\in \\mathbb{R}^{n \\times n}$ be a matrix and let $Q \\in \\mathbb{R}^{n \\times n}$ be an orthogonal matrix, which satisfies $Q^{\\top}Q = QQ^{\\top} = I$, where $I$ is the identity matrix. The problem states that orthogonal matrices preserve the Euclidean norm ($p=2$ norm), a property we write as $\\|Qy\\|_2 = \\|y\\|_2$ for any vector $y \\in \\mathbb{R}^n$. This also holds for $Q^{\\top}$, as it is also an orthogonal matrix.\n\nThe similarity transformation of $M$ by $Q$ is given by $\\tilde{M} = Q^{\\top}MQ$. We are to show that $\\|\\tilde{M}\\|_{2 \\to 2} = \\|M\\|_{2 \\to 2}$ using the definition:\n$$\n\\|M\\|_{2 \\to 2} = \\sup_{x \\neq 0} \\frac{\\|Mx\\|_2}{\\|x\\|_2}\n$$\nWe begin by writing the definition for $\\|\\tilde{M}\\|_{2 \\to 2}$:\n$$\n\\|\\tilde{M}\\|_{2 \\to 2} = \\sup_{y \\neq 0} \\frac{\\|\\tilde{M}y\\|_2}{\\|y\\|_2} = \\sup_{y \\neq 0} \\frac{\\|Q^{\\top}MQy\\|_2}{\\|y\\|_2}\n$$\nApplying the norm-preserving property of $Q^{\\top}$ to the numerator, we have $\\|Q^{\\top}(MQy)\\|_2 = \\|MQy\\|_2$. The expression becomes:\n$$\n\\|\\tilde{M}\\|_{2 \\to 2} = \\sup_{y \\neq 0} \\frac{\\|MQy\\|_2}{\\|y\\|_2}\n$$\nNext, we apply the norm-preserving property of $Q$ to the denominator: $\\|y\\|_2 = \\|Qy\\|_2$. This yields:\n$$\n\\|\\tilde{M}\\|_{2 \\to 2} = \\sup_{y \\neq 0} \\frac{\\|M(Qy)\\|_2}{\\|Qy\\|_2}\n$$\nLet us define a new vector variable $x = Qy$. Since $Q$ is an invertible matrix, the set of all non-zero vectors $y \\in \\mathbb{R}^n$ corresponds to the set of all non-zero vectors $x \\in \\mathbb{R}^n$. Therefore, taking the supremum over all non-zero $y$ is equivalent to taking the supremum over all non-zero $x$:\n$$\n\\|\\tilde{M}\\|_{2 \\to 2} = \\sup_{x \\neq 0} \\frac{\\|Mx\\|_2}{\\|x\\|_2} = \\|M\\|_{2 \\to 2}\n$$\nThis completes the proof of invariance.\n\nThe given matrix is $A(\\theta,L) = Q(\\theta)\\,\\mathrm{diag}(L,1)\\,Q(\\theta)^{\\top}$. Let $D = \\mathrm{diag}(L,1)$. Then $A(\\theta,L) = Q(\\theta) D Q(\\theta)^{\\top}$. This is a similarity transformation of $D$, but with $Q$ and $Q^\\top$ swapped from our proof. However, the proof can be trivially adapted. If we let $\\tilde{A} = Q A Q^\\top$, then $\\|\\tilde{A}\\|_{2 \\to 2} = \\|A\\|_{2 \\to 2}$. So, $\\|A(\\theta,L)\\|_{2 \\to 2} = \\|D\\|_{2 \\to 2}$.\n\nWe compute $\\|D\\|_{2 \\to 2}$ for $D = \\mathrm{diag}(L,1)$ with $L=10$: $D = \\begin{pmatrix} 10  0 \\\\ 0  1 \\end{pmatrix}$.\n$$\n\\|D\\|_{2 \\to 2}^2 = \\left( \\sup_{x \\neq 0} \\frac{\\|Dx\\|_2}{\\|x\\|_2} \\right)^2 = \\sup_{x \\neq 0} \\frac{\\|Dx\\|_2^2}{\\|x\\|_2^2}\n$$\nFor $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, we have $Dx = \\begin{pmatrix} 10x_1 \\\\ x_2 \\end{pmatrix}$.\n$$\n\\frac{\\|Dx\\|_2^2}{\\|x\\|_2^2} = \\frac{(10x_1)^2 + x_2^2}{x_1^2 + x_2^2} = \\frac{100x_1^2 + x_2^2}{x_1^2 + x_2^2}\n$$\nThis expression is maximized when $x_1$ is as large as possible relative to $x_2$. This occurs for vectors along the $x_1$-axis, i.e., $x_2=0$. For such a vector, the ratio is $\\frac{100x_1^2}{x_1^2} = 100$. The minimum occurs for $x_1=0$, where the ratio is $1$. The supremum is $100$.\nTherefore, $\\|A(\\theta,L)\\|_{2 \\to 2} = \\|D\\|_{2 \\to 2} = \\sqrt{100} = 10$.\n\nPart 2: Computation of $\\|A(\\theta,L)\\|_{1 \\to 1}$.\nThe induced $1$-norm, $\\|A\\|_{1 \\to 1}$, is the maximum absolute column sum of the matrix $A$. We first must compute the matrix $A(\\theta,L)$ for the given parameters $\\theta=\\pi/3$ and $L=10$.\nThe rotation matrix is $Q(\\theta) = \\begin{pmatrix}\\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{pmatrix}$. With $\\theta = \\pi/3$, $\\cos(\\pi/3) = 1/2$ and $\\sin(\\pi/3) = \\sqrt{3}/2$.\n$$\nQ(\\pi/3) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\end{pmatrix}, \\quad D = \\begin{pmatrix} 10  0 \\\\ 0  1 \\end{pmatrix}, \\quad Q(\\pi/3)^{\\top} = \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\end{pmatrix}\n$$\nWe compute $A(\\theta,L) = Q(\\theta)D Q(\\theta)^{\\top}$:\n$$\nA = \\begin{pmatrix} \\frac{1}{2}  -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 10  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 5  -\\frac{\\sqrt{3}}{2} \\\\ 5\\sqrt{3}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\end{pmatrix}\n$$\nThe entries of $A$ are calculated as:\n$A_{11} = (5)(\\frac{1}{2}) + (-\\frac{\\sqrt{3}}{2})(-\\frac{\\sqrt{3}}{2}) = \\frac{5}{2} + \\frac{3}{4} = \\frac{13}{4}$.\n$A_{12} = (5)(\\frac{\\sqrt{3}}{2}) + (-\\frac{\\sqrt{3}}{2})(\\frac{1}{2}) = \\frac{5\\sqrt{3}}{2} - \\frac{\\sqrt{3}}{4} = \\frac{9\\sqrt{3}}{4}$.\n$A_{21} = (5\\sqrt{3})(\\frac{1}{2}) + (\\frac{1}{2})(-\\frac{\\sqrt{3}}{2}) = \\frac{5\\sqrt{3}}{2} - \\frac{\\sqrt{3}}{4} = \\frac{9\\sqrt{3}}{4}$.\n$A_{22} = (5\\sqrt{3})(\\frac{\\sqrt{3}}{2}) + (\\frac{1}{2})(\\frac{1}{2}) = \\frac{15}{2} + \\frac{1}{4} = \\frac{31}{4}$.\nThus, the matrix is $A(\\pi/3, 10) = \\frac{1}{4}\\begin{pmatrix} 13  9\\sqrt{3} \\\\ 9\\sqrt{3}  31 \\end{pmatrix}$.\nThe absolute column sums are:\n$C_1 = |A_{11}| + |A_{21}| = \\frac{13}{4} + \\frac{9\\sqrt{3}}{4} = \\frac{13 + 9\\sqrt{3}}{4}$.\n$C_2 = |A_{12}| + |A_{22}| = \\frac{9\\sqrt{3}}{4} + \\frac{31}{4} = \\frac{31 + 9\\sqrt{3}}{4}$.\nThe norm $\\|A\\|_{1 \\to 1} = \\max(C_1, C_2)$. Since $31  13$, $C_2  C_1$.\nSo, $\\|A(\\pi/3, 10)\\|_{1 \\to 1} = \\frac{31 + 9\\sqrt{3}}{4}$.\n\nPart 3: Computation of the ratio $r$.\nFirst, we find $\\|\\mathrm{diag}(L,1)\\|_{1 \\to 1} = \\|D\\|_{1 \\to 1}$.\nFor $D = \\begin{pmatrix} 10  0 \\\\ 0  1 \\end{pmatrix}$, the column sums are:\n$C'_1 = |10| + |0| = 10$.\n$C'_2 = |0| + |1| = 1$.\nThe maximum is $10$, so $\\|D\\|_{1 \\to 1} = 10$.\n\nThe ratio $r$ is:\n$$\nr = \\frac{\\|A(\\theta,L)\\|_{1 \\to 1}}{\\|\\mathrm{diag}(L,1)\\|_{1 \\to 1}} = \\frac{\\frac{31 + 9\\sqrt{3}}{4}}{10} = \\frac{31 + 9\\sqrt{3}}{40}\n$$\nTo obtain a numerical value, we substitute $\\sqrt{3} \\approx 1.73205$:\n$$\nr \\approx \\frac{31 + 9(1.73205)}{40} = \\frac{31 + 15.58845}{40} = \\frac{46.58845}{40} \\approx 1.16471125\n$$\nRounding to four significant figures, we get $r \\approx 1.165$. This ratio shows that the $\\|\\cdot\\|_{1 \\to 1}$ norm, unlike the $\\|\\cdot\\|_{2 \\to 2}$ norm, is sensitive to the choice of basis, with a value over $16\\%$ larger for the rotated coordinates compared to the diagonalizing coordinates.", "answer": "$$\\boxed{1.165}$$", "id": "2757383"}, {"introduction": "The sensitivity of a matrix's properties to its basis is not just an analytical curiosity; it is a feature that can be harnessed for practical advantage. This exercise demonstrates how a simple change of state coordinates, known as diagonal scaling, can be used to improve the numerical conditioning of a system matrix [@problem_id:2757387]. By explicitly designing a scaling that balances the matrix's column norms, you will practice a fundamental technique for enhancing the robustness of control-related computations.", "problem": "In linear time-invariant state-space control, a change of state coordinates $x = D z$ with a positive diagonal matrix $D \\in \\mathbb{R}^{n \\times n}$ induces a similarity transformation $A \\mapsto B = D A D^{-1}$. The sensitivity of computations involving the state matrix is often reflected by the condition number in the induced $1$-norm. The induced $1$-norm of a matrix $M$ is defined by $\\|M\\|_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{n} |m_{ij}|$, and the $1$-norm condition number is $\\kappa_{1}(M) = \\|M\\|_{1} \\|M^{-1}\\|_{1}$ for invertible $M$.\n\nConsider the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with strictly positive entries\n$$\nA = \\begin{pmatrix}\n1  3 \\\\\n\\frac{1}{5}  2\n\\end{pmatrix}.\n$$\nYou will investigate whether a diagonal similarity scaling can reduce the $1$-norm condition number and design a scaling that equilibrates the $1$-norms of the columns of the scaled matrix.\n\nTasks:\n1. Using only the definitions above, compute $\\|A\\|_{1}$, $\\|A^{-1}\\|_{1}$, and $\\kappa_{1}(A)$.\n2. Let $D = \\operatorname{diag}(d_{1}, d_{2})$ with $d_{1}  0$ and $d_{2}  0$, and define $B = D A D^{-1}$. Using the definition of the induced $1$-norm, derive the condition on the ratio $r = d_{2}/d_{1}$ that makes the two column $1$-norms of $B$ equal. Solve explicitly for $r  0$.\n3. With this choice of $D$, compute $\\|B\\|_{1}$ and $\\|B^{-1}\\|_{1}$ exactly, and hence $\\kappa_{1}(B)$. Conclude whether $\\kappa_{1}(B)  \\kappa_{1}(A)$ holds for this design.\n\nProvide your final answer as the exact closed-form expression for $\\kappa_{1}(B)$ obtained in Task $3$. Do not round. No units are required.", "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. All definitions are standard in linear algebra and control theory. The given matrix $A$ is invertible, allowing for the computation of its condition number. We proceed with the solution.\n\nThe problem is divided into three consecutive tasks. We shall address them in order.\n\nTask 1: Compute $\\|A\\|_{1}$, $\\|A^{-1}\\|_{1}$, and $\\kappa_{1}(A)$.\n\nThe given matrix $A$ is\n$$\nA = \\begin{pmatrix}\n1  3 \\\\\n\\frac{1}{5}  2\n\\end{pmatrix}\n$$\nThe induced $1$-norm of a matrix is the maximum absolute column sum. For matrix $A$, the entries are all positive, so we do not need to take absolute values.\nThe sum of the first column is $1 + \\frac{1}{5} = \\frac{6}{5}$.\nThe sum of the second column is $3 + 2 = 5$.\nThe $1$-norm of $A$ is the maximum of these two values:\n$$\n\\|A\\|_{1} = \\max\\left(\\frac{6}{5}, 5\\right) = 5\n$$\nNext, we compute the inverse of $A$. The determinant of $A$ is\n$$\n\\det(A) = (1)(2) - (3)\\left(\\frac{1}{5}\\right) = 2 - \\frac{3}{5} = \\frac{10-3}{5} = \\frac{7}{5}\n$$\nSince $\\det(A) \\neq 0$, the matrix is invertible. The inverse is given by\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix}\n2  -3 \\\\\n-\\frac{1}{5}  1\n\\end{pmatrix} = \\frac{5}{7} \\begin{pmatrix}\n2  -3 \\\\\n-\\frac{1}{5}  1\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{10}{7}  -\\frac{15}{7} \\\\\n-\\frac{1}{7}  \\frac{5}{7}\n\\end{pmatrix}\n$$\nNow, we compute the $1$-norm of $A^{-1}$. We must sum the absolute values of the entries in each column.\nThe sum for the first column is $\\left|\\frac{10}{7}\\right| + \\left|-\\frac{1}{7}\\right| = \\frac{10}{7} + \\frac{1}{7} = \\frac{11}{7}$.\nThe sum for the second column is $\\left|-\\frac{15}{7}\\right| + \\left|\\frac{5}{7}\\right| = \\frac{15}{7} + \\frac{5}{7} = \\frac{20}{7}$.\nThe $1$-norm of $A^{-1}$ is the maximum of these two values:\n$$\n\\|A^{-1}\\|_{1} = \\max\\left(\\frac{11}{7}, \\frac{20}{7}\\right) = \\frac{20}{7}\n$$\nThe $1$-norm condition number of $A$ is the product of these norms:\n$$\n\\kappa_{1}(A) = \\|A\\|_{1} \\|A^{-1}\\|_{1} = 5 \\cdot \\frac{20}{7} = \\frac{100}{7}\n$$\n\nTask 2: Find the condition on the ratio $r = d_{2}/d_{1}$ that makes the two column $1$-norms of $B=DAD^{-1}$ equal.\n\nLet $D = \\operatorname{diag}(d_{1}, d_{2}) = \\begin{pmatrix} d_{1}  0 \\\\ 0  d_{2} \\end{pmatrix}$. Its inverse is $D^{-1} = \\begin{pmatrix} 1/d_{1}  0 \\\\ 0  1/d_{2} \\end{pmatrix}$.\nThe similarity transformation gives the matrix $B$:\n$$\nB = DAD^{-1} = \\begin{pmatrix} d_{1}  0 \\\\ 0  d_{2} \\end{pmatrix} \\begin{pmatrix} 1  3 \\\\ \\frac{1}{5}  2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{d_{1}}  0 \\\\ 0  \\frac{1}{d_{2}} \\end{pmatrix}\n$$\n$$\nB = \\begin{pmatrix} d_{1}  3d_{1} \\\\ \\frac{1}{5}d_{2}  2d_{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{d_{1}}  0 \\\\ 0  \\frac{1}{d_{2}} \\end{pmatrix} = \\begin{pmatrix} (d_{1})(\\frac{1}{d_{1}})  (3d_{1})(\\frac{1}{d_{2}}) \\\\ (\\frac{1}{5}d_{2})(\\frac{1}{d_{1}})  (2d_{2})(\\frac{1}{d_{2}}) \\end{pmatrix} = \\begin{pmatrix} 1  3\\frac{d_{1}}{d_{2}} \\\\ \\frac{1}{5}\\frac{d_{2}}{d_{1}}  2 \\end{pmatrix}\n$$\nUsing the definition $r = d_{2}/d_{1}$, we have $1/r = d_{1}/d_{2}$. The matrix $B$ can be written as:\n$$\nB = \\begin{pmatrix} 1  \\frac{3}{r} \\\\ \\frac{r}{5}  2 \\end{pmatrix}\n$$\nSince $d_{1}  0$ and $d_{2}  0$, we have $r  0$. The entries of $B$ are all positive.\nThe absolute column sums (column norms) of $B$ are:\nColumn 1: $C_{1} = 1 + \\frac{r}{5}$\nColumn 2: $C_{2} = \\frac{3}{r} + 2$\nTo equilibrate the column norms, we set $C_{1} = C_{2}$:\n$$\n1 + \\frac{r}{5} = \\frac{3}{r} + 2\n$$\nRearranging the terms to solve for $r$:\n$$\n\\frac{r}{5} - 1 = \\frac{3}{r}\n$$\nMultiplying by $5r$ (which is non-zero as $r0$):\n$$\nr^{2} - 5r = 15 \\implies r^{2} - 5r - 15 = 0\n$$\nThis is a quadratic equation for $r$. We use the quadratic formula $r = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\nr = \\frac{-(-5) \\pm \\sqrt{(-5)^{2} - 4(1)(-15)}}{2(1)} = \\frac{5 \\pm \\sqrt{25 + 60}}{2} = \\frac{5 \\pm \\sqrt{85}}{2}\n$$\nSince we must have $r  0$, and $\\sqrt{85}  \\sqrt{25} = 5$, the solution $r = \\frac{5 - \\sqrt{85}}{2}$ is negative. We must choose the positive root:\n$$\nr = \\frac{5 + \\sqrt{85}}{2}\n$$\n\nTask 3: Compute $\\|B\\|_{1}$, $\\|B^{-1}\\|_{1}$, $\\kappa_{1}(B)$ and compare with $\\kappa_{1}(A)$.\n\nWith the value of $r$ from Task $2$, the column norms of $B$ are equal. We can compute this common value to find $\\|B\\|_{1}$. Using the expression for the first column norm:\n$$\n\\|B\\|_{1} = 1 + \\frac{r}{5} = 1 + \\frac{1}{5}\\left(\\frac{5 + \\sqrt{85}}{2}\\right) = 1 + \\frac{5 + \\sqrt{85}}{10} = \\frac{10 + 5 + \\sqrt{85}}{10} = \\frac{15 + \\sqrt{85}}{10}\n$$\nNext, we compute $B^{-1}$. The determinant of $B$ is $\\det(B) = (1)(2) - (\\frac{3}{r})(\\frac{r}{5}) = 2 - \\frac{3}{5} = \\frac{7}{5}$. This is equal to $\\det(A)$, as expected for a similarity transformation.\n$$\nB^{-1} = \\frac{1}{\\det(B)} \\begin{pmatrix} 2  -\\frac{3}{r} \\\\ -\\frac{r}{5}  1 \\end{pmatrix} = \\frac{5}{7} \\begin{pmatrix} 2  -\\frac{3}{r} \\\\ -\\frac{r}{5}  1 \\end{pmatrix} = \\begin{pmatrix}\n\\frac{10}{7}  -\\frac{15}{7r} \\\\ -\\frac{r}{7}  \\frac{5}{7}\n\\end{pmatrix}\n$$\nThe absolute column sums of $B^{-1}$ are:\nColumn 1: $C'_{1} = \\left|\\frac{10}{7}\\right| + \\left|-\\frac{r}{7}\\right| = \\frac{10+r}{7}$\nColumn 2: $C'_{2} = \\left|-\\frac{15}{7r}\\right| + \\left|\\frac{5}{7}\\right| = \\frac{15}{7r} + \\frac{5}{7} = \\frac{15+5r}{7r}$\nWe substitute $r = \\frac{5 + \\sqrt{85}}{2}$:\n$C'_{1} = \\frac{1}{7} \\left(10 + \\frac{5 + \\sqrt{85}}{2}\\right) = \\frac{20 + 5 + \\sqrt{85}}{14} = \\frac{25 + \\sqrt{85}}{14}$.\nFor $C'_{2}$, we can use the relation from the quadratic equation for $r$: $r-5 = \\frac{15}{r}$.\n$C'_{2} = \\frac{1}{7}\\left(\\frac{15}{r} + 5 \\right) = \\frac{1}{7}\\left((r-5) + 5\\right) = \\frac{r}{7} = \\frac{1}{7}\\left(\\frac{5 + \\sqrt{85}}{2}\\right) = \\frac{5 + \\sqrt{85}}{14}$.\nWe take the maximum of these two sums for $\\|B^{-1}\\|_{1}$:\n$$\n\\|B^{-1}\\|_{1} = \\max\\left(\\frac{25 + \\sqrt{85}}{14}, \\frac{5 + \\sqrt{85}}{14}\\right) = \\frac{25 + \\sqrt{85}}{14}\n$$\nThe condition number of $B$ is $\\kappa_{1}(B) = \\|B\\|_{1}\\|B^{-1}\\|_{1}$:\n$$\n\\kappa_{1}(B) = \\left(\\frac{15 + \\sqrt{85}}{10}\\right) \\left(\\frac{25 + \\sqrt{85}}{14}\\right) = \\frac{(15 + \\sqrt{85})(25 + \\sqrt{85})}{140}\n$$\nExpanding the numerator:\n$(15 + \\sqrt{85})(25 + \\sqrt{85}) = 15 \\cdot 25 + 15\\sqrt{85} + 25\\sqrt{85} + (\\sqrt{85})^{2} = 375 + 40\\sqrt{85} + 85 = 460 + 40\\sqrt{85}$.\nSo,\n$$\n\\kappa_{1}(B) = \\frac{460 + 40\\sqrt{85}}{140} = \\frac{46 + 4\\sqrt{85}}{14} = \\frac{23 + 2\\sqrt{85}}{7}\n$$\nFinally, we compare $\\kappa_{1}(B)$ with $\\kappa_{1}(A) = \\frac{100}{7}$. We must compare $23 + 2\\sqrt{85}$ with $100$. This is equivalent to comparing $2\\sqrt{85}$ with $77$, or $\\sqrt{85}$ with $38.5$. Squaring both sides: $(\\sqrt{85})^{2} = 85$ and $(38.5)^{2} = 1482.25$. Since $85  1482.25$, it follows that $\\sqrt{85}  38.5$. Therefore, $23 + 2\\sqrt{85}  100$.\nWe conclude that $\\kappa_{1}(B)  \\kappa_{1}(A)$, so the diagonal scaling has indeed reduced the condition number.\nThe question asks for the exact expression for $\\kappa_{1}(B)$.", "answer": "$$\n\\boxed{\\frac{23 + 2\\sqrt{85}}{7}}\n$$", "id": "2757387"}]}