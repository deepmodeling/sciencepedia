{"hands_on_practices": [{"introduction": "The foundation of non-uniqueness in state-space models lies in the concept of coordinate freedom. This first practice exercise provides a direct demonstration of this principle by starting with a standard state-space realization and applying a similarity transformation to generate a new, algebraically distinct model. By working through this process ([@problem_id:2727852]), you will verify that despite the internal state matrices being different, the external input-output behavior, described by the transfer function, remains invariant.", "problem": "Consider the Single-Input Single-Output (SISO) transfer function $G(s) = \\dfrac{s+1}{s^{2}+3s+2}$. Starting only from the definition of a state-space realization and input-output equivalence, do the following:\n\n- Construct an explicit second-order state-space realization $(A,B,C,D)$ that realizes $G(s)$ without canceling common factors in the ratio; ensure that $D=0$.\n- Use the invertible similarity transformation matrix $T = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$ to construct a distinct realization $(\\tilde A,\\tilde B,\\tilde C,\\tilde D)$ defined by $\\tilde A = TAT^{-1}$, $\\tilde B = TB$, $\\tilde C = CT^{-1}$, and $\\tilde D = D$. Explain why the transfer function defined by $(\\tilde A,\\tilde B,\\tilde C,\\tilde D)$ equals $G(s)$ even though $\\tilde A \\neq A$.\n- Compute the scalar $\\operatorname{trace}(A^{2})$. Provide your final answer as a single real number with no units.\n\nNo rounding is required; report the exact value.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Transfer function: $G(s) = \\dfrac{s+1}{s^{2}+3s+2}$.\n- Task: Construct a second-order state-space realization $(A,B,C,D)$ for $G(s)$.\n- Constraint: Do not cancel common factors.\n- Constraint: The direct feedthrough term must be zero, i.e., $D=0$.\n- Transformation: Use an invertible matrix $T = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$ to find a new realization $(\\tilde A,\\tilde B,\\tilde C,\\tilde D)$.\n- Transformation equations: $\\tilde A = TAT^{-1}$, $\\tilde B = TB$, $\\tilde C = CT^{-1}$, and $\\tilde D = D$.\n- Task: Explain why the transfer function corresponding to $(\\tilde A,\\tilde B,\\tilde C,\\tilde D)$ is identical to $G(s)$.\n- Task: Compute the scalar value of $\\operatorname{trace}(A^{2})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within the established framework of linear systems and control theory. The concepts of state-space realization, transfer functions, and similarity transformations are fundamental. The problem statement explicitly forbids the cancellation of a common factor, $s+1$, in the numerator and denominator. This instruction forces the construction of a non-minimal, second-order realization for a system whose minimal order is one. This is a standard pedagogical exercise to illustrate the concepts of controllability and observability. The quantities to be computed are well-defined. The value $\\operatorname{trace}(A^{k})$ for any integer $k$ is an invariant under similarity transformations, ensuring that the final numerical answer is unique regardless of the initial choice of a valid state-space realization. The problem is self-contained, objective, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A rigorous solution will be developed.\n\nThe transfer function for a linear time-invariant (LTI) state-space system is given by $G(s) = C(sI - A)^{-1}B + D$. We are tasked to find a set of matrices $(A,B,C,D)$ that satisfies this relationship for the given $G(s)$.\n\nFirst, we construct *an* explicit second-order state-space realization. A direct and systematic method is to use a canonical form. We will use the controllable canonical form. The given transfer function is $G(s) = \\dfrac{s+1}{s^{2}+3s+2}$. We compare this to the general form for a second-order system:\n$$ G(s) = \\frac{b_1 s + b_0}{s^2 + a_1 s + a_0} $$\nBy inspection, we identify the coefficients: $a_1 = 3$, $a_0 = 2$, $b_1 = 1$, and $b_0 = 1$. The problem specifies $D=0$.\n\nFor a system in controllable canonical form, the state-space matrices are defined as:\n$$ A = \\begin{pmatrix} 0 & 1 \\\\ -a_0 & -a_1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} b_0 & b_1 \\end{pmatrix}, \\quad D = [0] $$\nSubstituting the identified coefficients, we obtain the realization $(A,B,C,D)$:\n$$ A = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad D = [0] $$\nThis is a valid second-order realization for the given transfer function, constructed without cancellation of factors. As a consequence of the pole-zero cancellation at $s=-1$, this realization is not minimal (it is, in fact, uncontrollable or unobservable).\n\nNext, we construct the distinct realization $(\\tilde A,\\tilde B,\\tilde C,\\tilde D)$ using the similarity transformation matrix $T = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$. First, we must find the inverse of $T$. The determinant is $\\det(T) = (1)(1) - (0)(1) = 1$.\n$$ T^{-1} = \\frac{1}{\\det(T)} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} $$\nNow we compute the new state-space matrices:\n$$ \\tilde A = TAT^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} $$\n$$ \\tilde A = \\begin{pmatrix} 0 & 1 \\\\ -2 & -2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ 0 & -2 \\end{pmatrix} $$\n$$ \\tilde B = TB = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\n$$ \\tilde C = CT^{-1} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} $$\n$$ \\tilde D = D = [0] $$\nThe new, distinct realization is $(\\tilde A, \\tilde B, \\tilde C, \\tilde D) = \\left( \\begin{pmatrix} -1 & 1 \\\\ 0 & -2 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 & 1 \\end{pmatrix}, [0] \\right)$. Note that $\\tilde A \\neq A$.\n\nWe must now explain why the transfer function $\\tilde G(s)$ of this new realization remains equal to $G(s)$. The transfer function for the new system is defined as $\\tilde G(s) = \\tilde C(sI - \\tilde A)^{-1}\\tilde B + \\tilde D$. Substituting the transformation rules:\n$$ \\tilde G(s) = (CT^{-1})(sI - TAT^{-1})^{-1}(TB) + D $$\nThe key is to simplify the inverse term. We can rewrite $sI$ as $sTIT^{-1}$:\n$$ (sI - TAT^{-1})^{-1} = (sTIT^{-1} - TAT^{-1})^{-1} = (T(sI - A)T^{-1})^{-1} $$\nUsing the matrix inverse property $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$, we get:\n$$ (T(sI - A)T^{-1})^{-1} = (T^{-1})^{-1}(sI - A)^{-1}T^{-1} = T(sI - A)^{-1}T^{-1} $$\nSubstituting this back into the expression for $\\tilde G(s)$:\n$$ \\tilde G(s) = (CT^{-1}) [T(sI - A)^{-1}T^{-1}] (TB) + D $$\nThe matrix products $T^{-1}T$ are identity matrices $I$. The expression simplifies due to associativity of matrix multiplication:\n$$ \\tilde G(s) = C(T^{-1}T)(sI - A)^{-1}(T^{-1}T)B + D = C(I)(sI-A)^{-1}(I)B + D $$\n$$ \\tilde G(s) = C(sI - A)^{-1}B + D $$\nThis is, by definition, the original transfer function $G(s)$. Therefore, $\\tilde G(s) = G(s)$. A similarity transformation changes the basis of the state vector ($z=Tx$), but it does not alter the external input-output behavior of the system. This property is fundamental to the theory of state-space representations.\n\nFinally, we compute the scalar $\\operatorname{trace}(A^{2})$. Using the matrix $A$ from our initial realization:\n$$ A = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} $$\nWe first compute $A^{2}$:\n$$ A^{2} = A \\cdot A = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} $$\n$$ A^{2} = \\begin{pmatrix} (0)(0)+(1)(-2) & (0)(1)+(1)(-3) \\\\ (-2)(0)+(-3)(-2) & (-2)(1)+(-3)(-3) \\end{pmatrix} = \\begin{pmatrix} -2 & -3 \\\\ 6 & 7 \\end{pmatrix} $$\nThe trace of a square matrix is the sum of the elements on the main diagonal.\n$$ \\operatorname{trace}(A^{2}) = -2 + 7 = 5 $$\nIt is a known property that the trace of any power of a matrix is invariant under similarity transformations, i.e., $\\operatorname{trace}(A^k) = \\operatorname{trace}(\\tilde{A}^k)$. We can verify this for our case:\n$$ \\tilde A^2 = \\begin{pmatrix} -1 & 1 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} -1 & 1 \\\\ 0 & -2 \\end{pmatrix} = \\begin{pmatrix} (-1)(-1)+(1)(0) & (-1)(1)+(1)(-2) \\\\ (0)(-1)+(-2)(0) & (0)(1)+(-2)(-2) \\end{pmatrix} = \\begin{pmatrix} 1 & -3 \\\\ 0 & 4 \\end{pmatrix} $$\n$$ \\operatorname{trace}(\\tilde A^2) = 1 + 4 = 5 $$\nThis confirms the invariance property. The final numerical result requested is derived from the first realization $(A,B,C,D)$.\n\nThe computation is:\n$$ A^{2} = \\begin{pmatrix} -2 & -3 \\\\ 6 & 7 \\end{pmatrix} $$\n$$ \\operatorname{trace}(A^{2}) = -2 + 7 = 5 $$", "answer": "$$\n\\boxed{5}\n$$", "id": "2727852"}, {"introduction": "Beyond similarity transformations, a crucial aspect of non-uniqueness arises from the distinction between minimal and non-minimal realizations. This exercise challenges you to analyze a given state-space model using the Popov–Belevitch–Hautus (PBH) tests to identify its internal structure ([@problem_id:2727800]). You will diagnose which dynamic modes are uncontrollable or unobservable, thereby understanding how a system's transfer function only reflects the subsystem that is both controllable and observable.", "problem": "Consider a single-input single-output linear time-invariant state-space realization with\n$$\nA=\\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & -2 & 0\\\\\n0 & 0 & 3\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n1\\\\\n0\\\\\n1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n0 & 1 & 0\n\\end{pmatrix},\\quad\nD=0.\n$$\nUsing only core definitions of controllability, observability, minimality, and the Popov–Belevitch–Hautus (PBH) tests, do the following:\n\n1) Determine for which eigenvalue(s) of $A$ the PBH controllability test fails, and explain why this implies a failure of controllability.\n\n2) Determine for which eigenvalue(s) of $A$ the PBH observability test fails, and explain why this implies a failure of observability.\n\n3) Interpret your findings in terms of the input-output map and the presence of uncontrollable and unobservable modes. Conclude whether the given realization is minimal.\n\n4) Let $n_{\\min}$ denote the order of a minimal realization that is input-output equivalent to the given $(A,B,C,D)$. Compute $n_{\\min}$.\n\nThe final answer must be the single number $n_{\\min}$. No units are required. Do not provide intermediate steps in the final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. It is a standard exercise in linear systems theory and is therefore valid.\n\nThe provided state-space realization is given by the matrices:\n$$\nA=\\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & -2 & 0\\\\\n0 & 0 & 3\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n1\\\\\n0\\\\\n1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n0 & 1 & 0\n\\end{pmatrix},\\quad\nD=0\n$$\nThe state matrix $A$ is diagonal, so its eigenvalues are the diagonal elements: $\\lambda_1 = 1$, $\\lambda_2 = -2$, and $\\lambda_3 = 3$. The order of the system is $n=3$.\n\n1) To determine controllability, we apply the Popov–Belevitch–Hautus (PBH) test. A linear time-invariant system $(A, B)$ is controllable if and only if the matrix $[sI-A \\quad B]$ has full row rank for all complex numbers $s$. Specifically, for each eigenvalue $\\lambda_i$ of $A$, the condition $\\text{rank}([\\lambda_i I - A \\quad B]) = n$ must hold. If this condition fails for any eigenvalue, the system is uncontrollable.\n\nWe test this condition for each eigenvalue:\nFor $\\lambda_1 = 1$:\n$$\n[\\lambda_1 I - A \\quad B] = [1 \\cdot I - A \\quad B] = \\begin{pmatrix}\n1-1 & 0 & 0 & 1 \\\\\n0 & 1-(-2) & 0 & 0 \\\\\n0 & 0 & 1-3 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & -2 & 1\n\\end{pmatrix}\n$$\nThe rank of this matrix is $3$, as its second, third, and fourth columns are linearly independent. Thus, the mode associated with $\\lambda_1 = 1$ is controllable.\n\nFor $\\lambda_2 = -2$:\n$$\n[\\lambda_2 I - A \\quad B] = [-2 \\cdot I - A \\quad B] = \\begin{pmatrix}\n-2-1 & 0 & 0 & 1 \\\\\n0 & -2-(-2) & 0 & 0 \\\\\n0 & 0 & -2-3 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n-3 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & -5 & 1\n\\end{pmatrix}\n$$\nThis matrix contains a row of zeros, so its rank is less than $3$. The rank is $2$. The PBH test fails for the eigenvalue $\\lambda_2 = -2$. This failure implies that the system is uncontrollable. The dynamic mode corresponding to $e^{-2t}$ cannot be influenced by the input $u(t)$. This is because the second component of the state vector, $x_2(t)$, evolves according to $\\dot{x}_2(t) = -2x_2(t)$, which is decoupled from the input.\n\nFor $\\lambda_3 = 3$:\n$$\n[\\lambda_3 I - A \\quad B] = [3 \\cdot I - A \\quad B] = \\begin{pmatrix}\n3-1 & 0 & 0 & 1 \\\\\n0 & 3-(-2) & 0 & 0 \\\\\n0 & 0 & 3-3 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n2 & 0 & 0 & 1 \\\\\n0 & 5 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe rank of this matrix is $3$. The mode associated with $\\lambda_3 = 3$ is controllable.\n\nIn summary, the PBH controllability test fails only for the eigenvalue $\\lambda = -2$.\n\n2) To determine observability, we use the dual form of the PBH test. The system $(A, C)$ is observable if and only if the matrix $\\begin{pmatrix} sI - A \\\\ C \\end{pmatrix}$ has full column rank for all complex numbers $s$. This requires $\\text{rank}\\left(\\begin{pmatrix} \\lambda_i I - A \\\\ C \\end{pmatrix}\\right) = n$ for each eigenvalue $\\lambda_i$ of $A$.\n\nWe test this condition for each eigenvalue:\nFor $\\lambda_1 = 1$:\n$$\n\\begin{pmatrix} \\lambda_1 I - A \\\\ C \\end{pmatrix} = \\begin{pmatrix} 1-1 & 0 & 0 \\\\ 0 & 1-(-2) & 0 \\\\ 0 & 0 & 1-3 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & -2 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe first column of this matrix is a zero vector. Therefore, its rank is less than $3$. The rank is $2$. The PBH test fails for $\\lambda_1 = 1$. This implies the system is unobservable. The mode $e^{t}$ is not visible at the output.\n\nFor $\\lambda_2 = -2$:\n$$\n\\begin{pmatrix} \\lambda_2 I - A \\\\ C \\end{pmatrix} = \\begin{pmatrix} -2-1 & 0 & 0 \\\\ 0 & -2-(-2) & 0 \\\\ 0 & 0 & -2-3 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} -3 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -5 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe three columns of this matrix are linearly independent. Its rank is $3$. The mode associated with $\\lambda_2 = -2$ is observable.\n\nFor $\\lambda_3 = 3$:\n$$\n\\begin{pmatrix} \\lambda_3 I - A \\\\ C \\end{pmatrix} = \\begin{pmatrix} 3-1 & 0 & 0 \\\\ 0 & 3-(-2) & 0 \\\\ 0 & 0 & 3-3 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe third column of this matrix is a zero vector. Its rank is $2$, which is less than $3$. The PBH test fails for $\\lambda_3 = 3$. The mode $e^{3t}$ is unobservable.\n\nIn summary, the PBH observability test fails for eigenvalues $\\lambda = 1$ and $\\lambda = 3$.\n\n3) The analysis reveals that the system dynamics can be decomposed. The state space can be partitioned based on controllability and observability properties of the modes (Kalman decomposition).\n- The mode associated with $\\lambda_1 = 1$ is controllable but unobservable.\n- The mode associated with $\\lambda_2 = -2$ is uncontrollable but observable.\n- The mode associated with $\\lambda_3 = 3$ is controllable but unobservable.\n\nThe input-output map, represented by the transfer function $G(s) = C(sI-A)^{-1}B + D$, only reflects the part of the system that is both controllable and observable.\n$$\nG(s) = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} s-1 & 0 & 0 \\\\ 0 & s+2 & 0 \\\\ 0 & 0 & s-3 \\end{pmatrix}^{-1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + 0\n$$\n$$\nG(s) = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{s-1} & 0 & 0 \\\\ 0 & \\frac{1}{s+2} & 0 \\\\ 0 & 0 & \\frac{1}{s-3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\nG(s) = \\begin{pmatrix} 0 & \\frac{1}{s+2} & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = 0 \\cdot \\frac{1}{s-1} + \\frac{1}{s+2} \\cdot 0 + 0 \\cdot \\frac{1}{s-3} = 0\n$$\nThe transfer function is $G(s) = 0$. This is because there are no modes that are simultaneously controllable and observable. The uncontrollable mode at $\\lambda = -2$ is \"cancelled\" because the input cannot excite it. The unobservable modes at $\\lambda=1$ and $\\lambda=3$ are \"cancelled\" because they do not appear in the output.\nA realization is defined as minimal if and only if it is both completely controllable and completely observable. Since the given realization is neither, it is not minimal.\n\n4) The order of a minimal realization, denoted $n_{\\min}$, is equal to the dimension of the controllable and observable subspace of the state space. As established, this dimension is zero. Equivalently, $n_{\\min}$ is the McMillan degree of the transfer function $G(s)$. For a scalar transfer function $G(s) = \\frac{N(s)}{P(s)}$ where $N(s)$ and $P(s)$ are coprime polynomials, the degree is the degree of the denominator $P(s)$. The transfer function $G(s) = 0$ can be written as $G(s) = \\frac{0}{1}$. Here, $N(s) = 0$ and $P(s) = 1$. The degree of the denominator polynomial is $0$. Therefore, the order of a minimal realization is $n_{\\min} = 0$. A minimal realization would consist of no states, with the input-output relationship given by $y(t) = Du(t) = 0 \\cdot u(t) = 0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2727800"}, {"introduction": "This final practice synthesizes our understanding by addressing a central question in system identification: how do we construct a state-space model from experimental data? Starting from a sequence of Markov parameters, you will use the Ho-Kalman algorithm framework to construct two distinct minimal realizations by choosing different factorizations of the Hankel matrix ([@problem_id:2727804]). This process reveals that the non-uniqueness of state-space models is not merely a theoretical curiosity but an inherent feature of the modeling process itself, with the similarity transformation being the bridge between different valid representations.", "problem": "Consider a discrete-time, Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) system with zero direct feedthrough term, whose Markov parameters are defined by $h_k = C A^{k-1} B$ for $k \\geq 1$, where $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times 1}$, and $C \\in \\mathbb{R}^{1 \\times n}$. You are given the finite sequence of Markov parameters\n$$h_1 = 0,\\quad h_2 = 1,\\quad h_3 = -3,\\quad h_4 = 7,\\quad h_5 = -15.$$\n1) Using these data, form the $2 \\times 2$ block Hankel matrices\n$$H_0 = \\begin{pmatrix} h_1 & h_2 \\\\ h_2 & h_3 \\end{pmatrix}, \\qquad H_1 = \\begin{pmatrix} h_2 & h_3 \\\\ h_3 & h_4 \\end{pmatrix}.$$\n2) Construct two realizations of order $2$ by performing two distinct full-rank factorizations of $H_0$ as $H_0 = O Q$:\n- Factorization $1$: take $O_1 = I_2$ and $Q_1 = H_0$, where $I_2$ denotes the $2 \\times 2$ identity matrix.\n- Factorization $2$: take $O_2 = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$ and $Q_2 = O_2^{-1} H_0$.\nFor each factorization $H_0 = O Q$, use only the foundational definitions of the Hankel factorization and Markov parameters, namely $H_0 = O Q$ and $H_1 = O A Q$, together with the convention that for a SISO realization of order $2$, the first row of $O$ equals $C$ and the first column of $Q$ equals $B$, to construct corresponding minimal realizations $(A_1,B_1,C_1)$ and $(A_2,B_2,C_2)$.\n\n3) Verify, starting from first principles, that the two realizations are similar, meaning there exists an invertible matrix $T \\in \\mathbb{R}^{2 \\times 2}$ such that $A_2 = T^{-1} A_1 T$, $B_2 = T^{-1} B_1$, and $C_2 = C_1 T$. Determine the explicit similarity transform $T$.\n\nReport your final answer as the explicit $2 \\times 2$ matrix $T$. No rounding is required, and no physical units are involved. The final answer must be a single closed-form analytic expression.", "solution": "The problem is valid. It is a standard exercise in linear systems theory, specifically on the non-uniqueness of state-space realizations and their relationship via similarity transformations. The provided data is self-contained, consistent, and sufficient for a unique solution.\n\nThe problem requires constructing two state-space realizations from a given sequence of Markov parameters and then finding the similarity transformation matrix $T$ that relates them.\n\nThe given sequence of Markov parameters is:\n$h_1 = 0$, $h_2 = 1$, $h_3 = -3$, $h_4 = 7$, $h_5 = -15$.\n\nStep 1: Construct the Hankel matrices $H_0$ and $H_1$.\nUsing the given values, we form the $2 \\times 2$ Hankel matrices as specified.\n$$H_0 = \\begin{pmatrix} h_1 & h_2 \\\\ h_2 & h_3 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 1 & -3 \\end{pmatrix}$$\n$$H_1 = \\begin{pmatrix} h_2 & h_3 \\\\ h_3 & h_4 \\end{pmatrix} = \\begin{pmatrix} 1 & -3 \\\\ -3 & 7 \\end{pmatrix}$$\nThe determinant of $H_0$ is $\\det(H_0) = (0)(-3) - (1)(1) = -1 \\neq 0$, which confirms that a minimal realization of order $2$ exists.\n\nStep 2: Construct two minimal realizations $(A_1, B_1, C_1)$ and $(A_2, B_2, C_2)$.\nA realization is determined from a factorization of the Hankel matrix $H_0 = OQ$, where $O$ is an observability-related matrix and $Q$ is a controllability-related matrix. The system matrix $A$ is then found using the shifted Hankel matrix $H_1$ via the relation $H_1 = OAQ$. For a single-input, single-output (SISO) system, the vector $C$ is the first row of $O$, and the vector $B$ is the first column of $Q$.\n\nRealization 1: $(A_1, B_1, C_1)$\nThis realization is based on the factorization $H_0 = O_1 Q_1$, where:\n$$O_1 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n$$Q_1 = H_0 = \\begin{pmatrix} 0 & 1 \\\\ 1 & -3 \\end{pmatrix}$$\nFrom the definitions:\n$C_1$ is the first row of $O_1$: $C_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\n$B_1$ is the first column of $Q_1$: $B_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nTo find $A_1$, we use the relation $H_1 = O_1 A_1 Q_1$. Since $O_1 = I_2$, this simplifies to $H_1 = A_1 Q_1$, which gives $A_1 = H_1 Q_1^{-1}$.\nFirst, we compute the inverse of $Q_1 = H_0$:\n$$Q_1^{-1} = H_0^{-1} = \\frac{1}{\\det(H_0)} \\begin{pmatrix} -3 & -1 \\\\ -1 & 0 \\end{pmatrix} = \\frac{1}{-1} \\begin{pmatrix} -3 & -1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\nNow, we compute $A_1$:\n$$A_1 = H_1 Q_1^{-1} = \\begin{pmatrix} 1 & -3 \\\\ -3 & 7 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} (1)(3)+(-3)(1) & (1)(1)+(-3)(0) \\\\ (-3)(3)+(7)(1) & (-3)(1)+(7)(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}$$\nThus, the first realization is $(A_1, B_1, C_1)$ with $A_1 = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}$, $B_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $C_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. This is the observer canonical form.\n\nRealization 2: $(A_2, B_2, C_2)$\nThis realization is based on the factorization $H_0 = O_2 Q_2$, where:\n$$O_2 = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$$\nAnd $Q_2 = O_2^{-1} H_0$. First, we compute $O_2^{-1}$:\n$$O_2^{-1} = \\frac{1}{(1)(1)-(1)(0)} \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix}$$\nNow, we find $Q_2$:\n$$Q_2 = O_2^{-1} H_0 = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & -3 \\end{pmatrix} = \\begin{pmatrix} -1 & 4 \\\\ 1 & -3 \\end{pmatrix}$$\nFrom the definitions:\n$C_2$ is the first row of $O_2$: $C_2 = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n$B_2$ is the first column of $Q_2$: $B_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nTo find $A_2$, we use $H_1 = O_2 A_2 Q_2$, which gives $A_2 = O_2^{-1} H_1 Q_2^{-1}$.\nFirst, we compute $Q_2^{-1}$:\n$$Q_2^{-1} = \\frac{1}{(-1)(-3)-(4)(1)} \\begin{pmatrix} -3 & -4 \\\\ -1 & -1 \\end{pmatrix} = \\frac{1}{-1} \\begin{pmatrix} -3 & -4 \\\\ -1 & -1 \\end{pmatrix} = \\begin{pmatrix} 3 & 4 \\\\ 1 & 1 \\end{pmatrix}$$\nNow, we compute $A_2$:\n$$A_2 = O_2^{-1} H_1 Q_2^{-1} = \\left( \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -3 \\\\ -3 & 7 \\end{pmatrix} \\right) \\begin{pmatrix} 3 & 4 \\\\ 1 & 1 \\end{pmatrix}$$\n$$A_2 = \\begin{pmatrix} 4 & -10 \\\\ -3 & 7 \\end{pmatrix} \\begin{pmatrix} 3 & 4 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} (4)(3)+(-10)(1) & (4)(4)+(-10)(1) \\\\ (-3)(3)+(7)(1) & (-3)(4)+(7)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 6 \\\\ -2 & -5 \\end{pmatrix}$$\nThus, the second realization is $(A_2, B_2, C_2)$ with $A_2 = \\begin{pmatrix} 2 & 6 \\\\ -2 & -5 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, $C_2 = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n\nStep 3: Determine the similarity transformation $T$.\nAny two minimal realizations $(A_1, B_1, C_1)$ and $(A_2, B_2, C_2)$ of the same system are related by a similarity transformation matrix $T$ such that $A_2 = T^{-1}A_1T$, $B_2 = T^{-1}B_1$, and $C_2 = C_1T$.\nThe matrix $T$ relates the factorization matrices as well. If $H_0 = O_1 Q_1 = O_2 Q_2$, then the relationship is $O_2 = O_1 T$ and $Q_2 = T^{-1} Q_1$.\nFrom $O_2 = O_1 T$, we can find $T = O_1^{-1} O_2$.\nIn our case, $O_1 = I_2$, so $O_1^{-1} = I_2$. Therefore, $T$ is simply $O_2$:\n$$T = I_2^{-1} O_2 = O_2 = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$$\nTo be rigorous, we verify this result using the transformation equations. We have $T = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$ and $T^{-1} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix}$.\n1. Verify $C_2 = C_1 T$:\n$$C_1 T = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} = C_2$$\nThis is correct.\n2. Verify $B_2 = T^{-1} B_1$:\n$$T^{-1} B_1 = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = B_2$$\nThis is also correct.\n3. Verify $A_2 = T^{-1} A_1 T$:\n$$T^{-1} A_1 T = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$$\nFirst, compute $A_1 T$:\n$$A_1 T = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -2 & -5 \\end{pmatrix}$$\nNow, pre-multiply by $T^{-1}$:\n$$T^{-1}(A_1 T) = \\begin{pmatrix} 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -2 & -5 \\end{pmatrix} = \\begin{pmatrix} (1)(0)+(-1)(-2) & (1)(1)+(-1)(-5) \\\\ (0)(0)+(1)(-2) & (0)(1)+(1)(-5) \\end{pmatrix} = \\begin{pmatrix} 2 & 6 \\\\ -2 & -5 \\end{pmatrix} = A_2$$\nThis is also correct. All three conditions are satisfied, confirming that the similarity transformation matrix is indeed $T = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$.\nThe final answer is the explicit form of this matrix $T$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n}\n$$", "id": "2727804"}]}