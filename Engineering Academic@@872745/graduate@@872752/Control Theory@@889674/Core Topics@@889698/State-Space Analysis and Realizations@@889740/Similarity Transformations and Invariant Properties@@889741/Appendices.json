{"hands_on_practices": [{"introduction": "Real-world linear systems often exhibit oscillatory or spiral dynamics, which are mathematically described by complex conjugate eigenvalues. A crucial skill is to understand how these complex numbers translate into the behavior of a real state vector. This exercise [@problem_id:2744704] provides hands-on practice in constructing the real similarity transformation that converts a state matrix with complex eigenvalues into its real Jordan canonical form, a block that explicitly represents rotation and scaling. Mastering this fundamental construction from first principles will solidify your understanding of the direct link between a system's invariant eigenvalues and its dynamic, real-world behavior.", "problem": "Consider the real linear time-invariant (LTI) state-space model $\\dot{x} = A x$ with a real $2 \\times 2$ state matrix $A \\in \\mathbb{R}^{2 \\times 2}$. Recall the foundational definitions: two matrices $A$ and $J$ are similar if there exists an invertible matrix $S$ such that $S^{-1} A S = J$, and an eigenpair $(\\lambda, v)$ of $A$ satisfies $A v = \\lambda v$ with $\\lambda \\in \\mathbb{C}$ and $v \\in \\mathbb{C}^{2} \\setminus \\{0\\}$. For real $A$, nonreal eigenvalues occur in complex conjugate pairs. The real Jordan canonical form for such a pair is a real $2 \\times 2$ block representing a rotation-scaling.\n\nUsing only these definitions and properties of similarity transformations and eigenpairs, do the following:\n\n1. Starting from $A v = \\lambda v$ where $\\lambda = \\alpha + \\mathrm{i} \\beta$ with $\\alpha, \\beta \\in \\mathbb{R}$ and $v = p + \\mathrm{i} q$ with $p, q \\in \\mathbb{R}^{2}$, derive a real invertible similarity transformation $S \\in \\mathbb{R}^{2 \\times 2}$ such that $S^{-1} A S$ is the real Jordan block corresponding to the eigenvalues $\\alpha \\pm \\mathrm{i} \\beta$. Express the resulting real Jordan block explicitly in terms of $\\alpha$ and $\\beta$, and identify $\\alpha$ and $\\beta$ in terms of real invariants of $A$.\n\n2. Apply your construction to the specific matrix\n$$\nA = \\begin{pmatrix}\n3  -5 \\\\\n2  1\n\\end{pmatrix},\n$$\nand produce one explicit real invertible matrix $S$ satisfying $S^{-1} A S$ equal to the real Jordan block associated with the eigenvalues of $A$. Your $S$ must be given with exact integer entries.\n\n3. Justify, using only invariants preserved by similarity (such as the trace and determinant), the necessary and sufficient condition on $A$ under which such a real $S$ exists that converts $A$ to a single real $2 \\times 2$ rotation-scaling block. Formulate this condition in terms of $\\operatorname{tr}(A)$ and $\\det(A)$.\n\nAnswer specification: Provide as your final answer only the specific matrix $S$ you obtain in part $2$, written as a $2 \\times 2$ matrix. No rounding is required and no units are involved.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a complete solution. We shall proceed with the derivation and calculation as requested. The problem is divided into three parts which we will address in sequence.\n\nPart 1: Derivation of the real similarity transformation and real Jordan form.\n\nWe begin with the defining equation for an eigenpair $(\\lambda, v)$ of the real matrix $A \\in \\mathbb{R}^{2 \\times 2}$, which is $A v = \\lambda v$.\nThe problem specifies a nonreal eigenvalue $\\lambda = \\alpha + \\mathrm{i} \\beta$, where $\\alpha, \\beta \\in \\mathbb{R}$ and $\\beta \\neq 0$, with a corresponding eigenvector $v = p + \\mathrm{i} q$, where $p, q \\in \\mathbb{R}^{2}$. Since $v$ is an eigenvector, $v \\neq 0$, which implies that $p$ and $q$ cannot both be the zero vector.\n\nSubstituting the complex forms of $\\lambda$ and $v$ into the eigenvalue equation:\n$$A (p + \\mathrm{i} q) = (\\alpha + \\mathrm{i} \\beta) (p + \\mathrm{i} q)$$\nUsing the linearity of matrix multiplication and expanding the right side, we obtain:\n$$A p + \\mathrm{i} A q = (\\alpha p - \\beta q) + \\mathrm{i} (\\beta p + \\alpha q)$$\nSince $A$, $p$, and $q$ are real, we can equate the real and imaginary parts of this equation. This yields two separate real vector equations:\n1. $A p = \\alpha p - \\beta q$\n2. $A q = \\beta p + \\alpha q$\n\nThese two equations describe the action of $A$ on the real vectors $p$ and $q$. We can express this relationship in matrix form. Let us define a real matrix $S$ whose columns are the vectors $p$ and $q$: $S = \\begin{pmatrix} p  q \\end{pmatrix}$. Then the action of $A$ on the columns of $S$ is given by:\n$$A S = A \\begin{pmatrix} p  q \\end{pmatrix} = \\begin{pmatrix} A p  A q \\end{pmatrix}$$\nSubstituting our expressions for $A p$ and $A q$:\n$$A S = \\begin{pmatrix} \\alpha p - \\beta q  \\beta p + \\alpha q \\end{pmatrix}$$\nWe now seek to write the right-hand side as a product of $S$ and a real $2 \\times 2$ matrix, which we will call $J$.\n$$\\begin{pmatrix} p  q \\end{pmatrix} \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix} = \\begin{pmatrix} p(\\alpha) + q(-\\beta)  p(\\beta) + q(\\alpha) \\end{pmatrix} = \\begin{pmatrix} \\alpha p - \\beta q  \\beta p + \\alpha q \\end{pmatrix}$$\nBy comparison, we see that:\n$$A S = S \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$$\nFor this to be a similarity transformation, the matrix $S$ must be invertible. This requires its column vectors $p$ and $q$ to be linearly independent. Let us prove this is the case.\nAssume, for contradiction, that $p$ and $q$ are linearly dependent. Since $v = p + \\mathrm{i} q \\neq 0$, both $p$ and $q$ cannot be zero. Furthermore, if either $p=0$ or $q=0$, the eigenvalue $\\lambda$ must be real, contradicting $\\beta \\neq 0$. For instance, if $q=0$, then $v=p$, and $Ap = \\lambda p$. Since $A$ and $p$ are real, $\\lambda$ must be real. Thus, $p$ and $q$ must be non-zero.\nIf they are linearly dependent, then there exists a non-zero scalar $k \\in \\mathbb{R}$ such that $q = k p$.\nThe eigenvector becomes $v = p + \\mathrm{i}(k p) = (1 + \\mathrm{i} k) p$.\nSubstituting this into the eigenvalue equation:\n$$A ((1 + \\mathrm{i} k) p) = \\lambda ((1 + \\mathrm{i} k) p)$$\nSince $k$ is real, $(1 + \\mathrm{i} k)$ is a non-zero complex scalar, which we can cancel from both sides:\n$$A p = \\lambda p$$\nThis equation, where $A$ and $p \\in \\mathbb{R}^{2} \\setminus \\{0\\}$ are real, implies that the eigenvalue $\\lambda$ must be real. This contradicts our initial premise that $\\lambda$ is nonreal (i.e., $\\beta \\neq 0$). Therefore, the assumption of linear dependence is false. The vectors $p$ and $q$ must be linearly independent.\n\nSince $p, q \\in \\mathbb{R}^2$ are linearly independent, the $2 \\times 2$ matrix $S = \\begin{pmatrix} p  q \\end{pmatrix}$ is invertible. We can thus pre-multiply the equation $A S = S J$ by $S^{-1}$:\n$$S^{-1} A S = J = \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$$\nThis matrix $J$ is the real Jordan block corresponding to the complex conjugate eigenvalues $\\alpha \\pm \\mathrm{i} \\beta$.\n\nThe parameters $\\alpha$ and $\\beta$ are determined by the invariants of $A$. Since the eigenvalues are $\\lambda_1 = \\alpha + \\mathrm{i}\\beta$ and $\\lambda_2 = \\alpha - \\mathrm{i}\\beta$:\nThe trace is $\\operatorname{tr}(A) = \\lambda_1 + \\lambda_2 = (\\alpha + \\mathrm{i}\\beta) + (\\alpha - \\mathrm{i}\\beta) = 2\\alpha$.\nThus, $\\alpha = \\frac{1}{2} \\operatorname{tr}(A)$.\nThe determinant is $\\det(A) = \\lambda_1 \\lambda_2 = (\\alpha + \\mathrm{i}\\beta)(\\alpha - \\mathrm{i}\\beta) = \\alpha^2 + \\beta^2$.\nThus, $\\beta^2 = \\det(A) - \\alpha^2 = \\det(A) - \\frac{1}{4} (\\operatorname{tr}(A))^2$. We can choose $\\beta = \\sqrt{\\det(A) - \\frac{1}{4} (\\operatorname{tr}(A))^2}$, where the positive root corresponds to the choice of eigenvalue $\\lambda = \\alpha + \\mathrm{i}\\beta$ with $\\beta  0$.\n\nPart 2: Application to the specific matrix $A = \\begin{pmatrix} 3  -5 \\\\ 2  1 \\end{pmatrix}$.\n\nFirst, we find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 3 - \\lambda  -5 \\\\ 2  1 - \\lambda \\end{pmatrix} = (3 - \\lambda)(1 - \\lambda) - (2)(-5) = 0$$\n$$\\lambda^2 - 4\\lambda + 3 + 10 = 0$$\n$$\\lambda^2 - 4\\lambda + 13 = 0$$\nUsing the quadratic formula, the eigenvalues are:\n$$\\lambda = \\frac{4 \\pm \\sqrt{16 - 4(13)}}{2} = \\frac{4 \\pm \\sqrt{16 - 52}}{2} = \\frac{4 \\pm \\sqrt{-36}}{2} = \\frac{4 \\pm 6\\mathrm{i}}{2} = 2 \\pm 3\\mathrm{i}$$\nWe identify $\\alpha = 2$ and choose $\\beta = 3$. Let us find the eigenvector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ corresponding to the eigenvalue $\\lambda = 2 + 3\\mathrm{i}$.\n$$(A - \\lambda I) v = \\begin{pmatrix} 3 - (2 + 3\\mathrm{i})  -5 \\\\ 2  1 - (2 + 3\\mathrm{i}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 1 - 3\\mathrm{i}  -5 \\\\ 2  -1 - 3\\mathrm{i} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nFrom the first row, we have $(1 - 3\\mathrm{i}) v_1 - 5 v_2 = 0$, which gives $v_1 = \\frac{5}{1 - 3\\mathrm{i}} v_2$.\n$$v_1 = \\frac{5(1 + 3\\mathrm{i})}{(1 - 3\\mathrm{i})(1 + 3\\mathrm{i})} v_2 = \\frac{5(1 + 3\\mathrm{i})}{1^2 + 3^2} v_2 = \\frac{5(1 + 3\\mathrm{i})}{10} v_2 = \\frac{1 + 3\\mathrm{i}}{2} v_2$$\nTo obtain an eigenvector with integer-valued real and imaginary parts, we can choose $v_2 = 2$.\nThis gives $v_1 = 1 + 3\\mathrm{i}$.\nThe eigenvector is $v = \\begin{pmatrix} 1 + 3\\mathrm{i} \\\\ 2 \\end{pmatrix}$.\nNow we decompose $v$ into its real and imaginary parts, $v = p + \\mathrm{i} q$:\n$$v = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\mathrm{i} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$$\nSo, $p = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $q = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\nFollowing the construction from Part 1, the similarity transformation matrix $S$ is formed by these vectors:\n$$S = \\begin{pmatrix} p  q \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 2  0 \\end{pmatrix}$$\nThis is a real invertible matrix with exact integer entries, as required. Its determinant is $\\det(S) = (1)(0) - (3)(2) = -6 \\neq 0$.\n\nPart 3: Condition for transformation to a rotation-scaling block.\n\nA real $2 \\times 2$ matrix $A$ can be transformed by a real similarity transformation $S$ into a rotation-scaling block $J = \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$ if and only if $A$ and $J$ are similar. A necessary condition for similarity is that the matrices have the same eigenvalues.\nThe eigenvalues of $J$ are the roots of its characteristic polynomial:\n$$\\det(J - \\lambda I) = (\\alpha - \\lambda)^2 - (\\beta)(-\\beta) = (\\alpha - \\lambda)^2 + \\beta^2 = 0$$\nThis gives $\\lambda - \\alpha = \\pm \\mathrm{i}\\beta$, so $\\lambda = \\alpha \\pm \\mathrm{i}\\beta$.\nFor $J$ to be a genuine rotation-scaling block, a rotational component must exist, which requires $\\beta \\neq 0$. This means the eigenvalues must be a pair of non-real complex conjugates.\nSince eigenvalues are invariant under similarity transformations, the matrix $A$ must also have a pair of non-real complex conjugate eigenvalues.\n\nThe eigenvalues of $A$ are the roots of its characteristic polynomial, $p(\\lambda) = \\lambda^2 - \\operatorname{tr}(A)\\lambda + \\det(A) = 0$. The nature of these roots is determined by the discriminant of this quadratic equation, $\\Delta = (\\operatorname{tr}(A))^2 - 4\\det(A)$.\nFor the eigenvalues to be a non-real complex conjugate pair, the discriminant must be negative:\n$$\\Delta = (\\operatorname{tr}(A))^2 - 4\\det(A)  0$$\nThis is the necessary condition.\n\nTo show sufficiency, if $(\\operatorname{tr}(A))^2 - 4\\det(A)  0$, then the eigenvalues of $A$ are $\\lambda = \\frac{\\operatorname{tr}(A) \\pm \\mathrm{i}\\sqrt{4\\det(A) - (\\operatorname{tr}(A))^2}}{2}$. These are of the form $\\alpha \\pm \\mathrm{i}\\beta$ where $\\alpha = \\frac{1}{2}\\operatorname{tr}(A)$ and $\\beta = \\frac{1}{2}\\sqrt{4\\det(A) - (\\operatorname{tr}(A))^2} \\neq 0$.\nSince the eigenvalues are distinct, the matrix $A$ is diagonalizable over $\\mathbb{C}$. As shown in Part 1, the existence of such a complex eigenpair guarantees the existence of linearly independent real vectors $p$ and $q$ from which an invertible real matrix $S = \\begin{pmatrix}p  q\\end{pmatrix}$ can be constructed, such that $S^{-1}AS$ is a real Jordan block.\nTherefore, the necessary and sufficient condition for a real $2 \\times 2$ matrix $A$ to be similar to a single real $2 \\times 2$ rotation-scaling block is $(\\operatorname{tr}(A))^2  4\\det(A)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  3 \\\\ 2  0 \\end{pmatrix}}\n$$", "id": "2744704"}, {"introduction": "A key principle in modern control theory is that a system's input-output behavior, captured by its transfer function $G(s)$, has infinite state-space realizations. However, all minimal realizations are linked by a similarity transformation, making them equivalent. This practice [@problem_id:2744720] challenges you to derive the transformation matrix that connects two of the most fundamental representations: the controllable and observable canonical forms. By using the system's controllability matrices as your building blocks, you will gain a profound insight into how these objects define a coordinate system for the state space and prove the invariant nature of the underlying system dynamics.", "problem": "Consider a single-input single-output (SISO) strictly proper rational transfer function\n$$\nG(s) \\;=\\; \\frac{b_{n-1}s^{n-1} + b_{n-2}s^{n-2} + \\cdots + b_{1}s + b_{0}}{s^{n} + a_{n-1}s^{n-1} + \\cdots + a_{1}s + a_{0}},\n$$\nwith real coefficients and monic denominator of order $n \\ge 1$. Let $\\left(A_{c}, B_{c}, C_{c}, 0\\right)$ be the controllable canonical form realization of $G(s)$ and let $\\left(A_{o}, B_{o}, C_{o}, 0\\right)$ be the observable canonical form realization of the same $G(s)$. Assume both realizations are minimal. By definition of canonical forms, $\\left(A_{c}, B_{c}\\right)$ is controllable and $\\left(C_{o}, A_{o}\\right)$ is observable.\n\nUsing only foundational definitions in linear systems and control, namely the definitions of similarity transformation, controllability matrix, and observability matrix, and without appealing to any pre-stated transformation formulas, derive constructively a closed-form expression for the unique similarity transformation matrix $T \\in \\mathbb{R}^{n \\times n}$ that maps the controllable canonical realization to the observable canonical realization of the same transfer function, in the sense that\n$$\nA_{o} \\;=\\; T A_{c} T^{-1}, \\qquad B_{o} \\;=\\; T B_{c}, \\qquad C_{o} \\;=\\; C_{c} T^{-1}.\n$$\nExpress your final result for $T$ explicitly in terms of $A_{c}$, $B_{c}$, $A_{o}$, and $B_{o}$, using only operations justified from first principles. Your answer must be a single closed-form analytic expression. No numerical evaluation is required, and no units are needed.", "solution": "The problem requires the derivation of the unique similarity transformation matrix $T$ that connects the controllable canonical realization $\\left(A_{c}, B_{c}, C_{c}\\right)$ and the observable canonical realization $\\left(A_{o}, B_{o}, C_{o}\\right)$ of a given transfer function $G(s)$. The derivation must be constructive and based solely on the foundational definitions of similarity transformation, controllability matrix, and observability matrix. The final expression must be in terms of $A_{c}$, $B_{c}$, $A_{o}$, and $B_{o}$.\n\nFirst, we validate the problem statement.\nThe givens are:\n1. A single-input single-output (SISO) strictly proper rational transfer function $G(s) = \\frac{b_{n-1}s^{n-1} + \\cdots + b_{0}}{s^{n} + a_{n-1}s^{n-1} + \\cdots + a_{0}}$.\n2. The controllable canonical realization $\\left(A_{c}, B_{c}, C_{c}, 0\\right)$.\n3. The observable canonical realization $\\left(A_{o}, B_{o}, C_{o}, 0\\right)$.\n4. Both realizations are minimal for a state-space of dimension $n$.\n5. The similarity transformation relations: $A_{o} = T A_{c} T^{-1}$, $B_{o} = T B_{c}$, and $C_{o} = C_{c} T^{-1}$.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in linear control theory. Minimal realizations of the same transfer function are known to be related by a unique similarity transformation. The problem statement is self-contained and consistent, providing all necessary information. It does not violate any scientific principles or contain ambiguities. Therefore, the problem is valid and we may proceed with the solution.\n\nThe core of the derivation relies on the behavior of the controllability matrix under a similarity transformation. Let a general linear time-invariant system be described by the pair $(A, B)$, where $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times 1}$. The controllability matrix for this system is defined as:\n$$\n\\mathcal{C}(A, B) = \\begin{pmatrix} B  AB  A^{2}B  \\cdots  A^{n-1}B \\end{pmatrix}\n$$\nThis is an $n \\times n$ matrix.\n\nWe are given the similarity transformation equations relating the two realizations:\n1. $A_{o} = T A_{c} T^{-1}$\n2. $B_{o} = T B_{c}$\n\nOur objective is to find an expression for $T$. We will analyze how the controllability matrix of the observable realization, $\\mathcal{C}_{o} = \\mathcal{C}(A_{o}, B_{o})$, relates to the controllability matrix of the controllable realization, $\\mathcal{C}_{c} = \\mathcal{C}(A_{c}, B_{c})$.\n\nLet us examine the columns of $\\mathcal{C}_{o}$.\nThe first column is $B_{o}$. Using relation (2), we have:\n$$\nB_{o} = T B_{c}\n$$\nThe second column is $A_{o} B_{o}$. Substituting relations (1) and (2):\n$$\nA_{o} B_{o} = \\left( T A_{c} T^{-1} \\right) \\left( T B_{c} \\right) = T A_{c} \\left( T^{-1} T \\right) B_{c} = T A_{c} I B_{c} = T (A_{c} B_{c})\n$$\nThe third column is $A_{o}^{2} B_{o}$. We can write this as $A_{o} (A_{o} B_{o})$ and use the previous result:\n$$\nA_{o}^{2} B_{o} = A_{o} (T A_{c} B_{c}) = \\left( T A_{c} T^{-1} \\right) \\left( T A_{c} B_{c} \\right) = T A_{c}^{2} B_{c}\n$$\nWe can establish a general formula by induction. Assume for some integer $k \\ge 0$ that $A_{o}^{k} B_{o} = T A_{c}^{k} B_{c}$. We then show this holds for $k+1$:\n$$\nA_{o}^{k+1} B_{o} = A_{o} (A_{o}^{k} B_{o}) = \\left( T A_{c} T^{-1} \\right) \\left( T A_{c}^{k} B_{c} \\right) = T A_{c} \\left( T^{-1} T \\right) A_{c}^{k} B_{c} = T A_{c}^{k+1} B_{c}\n$$\nThe base case for $k=0$ is $A_{o}^{0} B_{o} = I B_{o} = B_o$ and $T A_{c}^{0} B_{c} = T I B_c = T B_c$, which is true by relation (2). Thus, the formula $A_{o}^{k} B_{o} = T A_{c}^{k} B_{c}$ holds for all integers $k \\ge 0$.\n\nNow, we can write the full controllability matrix $\\mathcal{C}_{o}$ using this property for each of its columns:\n$$\n\\mathcal{C}_{o} = \\mathcal{C}(A_{o}, B_{o}) = \\begin{pmatrix} B_{o}  A_{o}B_{o}  \\cdots  A_{o}^{n-1}B_{o} \\end{pmatrix}\n$$\n$$\n\\mathcal{C}_{o} = \\begin{pmatrix} T B_{c}  T(A_{c}B_{c})  \\cdots  T(A_{c}^{n-1}B_{c}) \\end{pmatrix}\n$$\nBy the properties of matrix multiplication, we can factor out the matrix $T$ from the left:\n$$\n\\mathcal{C}_{o} = T \\begin{pmatrix} B_{c}  A_{c}B_{c}  \\cdots  A_{c}^{n-1}B_{c} \\end{pmatrix}\n$$\nThe matrix on the right is, by definition, the controllability matrix for the controllable realization, $\\mathcal{C}_{c} = \\mathcal{C}(A_{c}, B_{c})$. Therefore, we have derived the fundamental relationship:\n$$\n\\mathcal{C}(A_{o}, B_{o}) = T \\mathcal{C}(A_{c}, B_{c})\n$$\n\nTo solve for $T$, we must be able to invert the matrix $\\mathcal{C}(A_{c}, B_{c})$. The problem states that the realization $(A_{c}, B_{c}, C_{c})$ is minimal. For a state-space system of dimension $n$, minimality implies both controllability and observability. The controllability of the pair $(A_c, B_c)$ is equivalent to the condition that its controllability matrix $\\mathcal{C}(A_{c}, B_{c})$ has full rank, i.e., $\\text{rank}(\\mathcal{C}(A_{c}, B_{c})) = n$. Since $\\mathcal{C}(A_{c}, B_{c})$ is an $n \\times n$ matrix, having full rank means it is nonsingular and thus invertible.\n\nGiven that $\\mathcal{C}(A_{c}, B_{c})^{-1}$ exists, we can right-multiply both sides of our derived equation by this inverse to isolate $T$:\n$$\n\\mathcal{C}(A_{o}, B_{o}) \\left( \\mathcal{C}(A_{c}, B_{c}) \\right)^{-1} = T \\mathcal{C}(A_{c}, B_{c}) \\left( \\mathcal{C}(A_{c}, B_{c}) \\right)^{-1}\n$$\n$$\n\\mathcal{C}(A_{o}, B_{o}) \\left( \\mathcal{C}(A_{c}, B_{c}) \\right)^{-1} = T I\n$$\nThis gives the final closed-form expression for the similarity transformation matrix $T$:\n$$\nT = \\mathcal{C}(A_{o}, B_{o}) \\left( \\mathcal{C}(A_{c}, B_{c}) \\right)^{-1}\n$$\nThis expression is derived constructively from first principles as required, and is expressed solely in terms of the given matrices $A_{c}$, $B_{c}$, $A_{o}$, and $B_{o}$.", "answer": "$$\n\\boxed{\\mathcal{C}(A_{o}, B_{o}) \\left( \\mathcal{C}(A_{c}, B_{c}) \\right)^{-1}}\n$$", "id": "2744720"}, {"introduction": "Realizations of complex systems are often non-minimal, containing states that are either uncontrollable, unobservable, or both. To analyze such systems, it is essential to isolate the core dynamics that govern the input-output relationship. This is precisely the purpose of the Kalman decomposition, which uses a similarity transformation to partition the state space into four fundamental subspaces. This capstone exercise [@problem_id:2744735] guides you through the complete structural decomposition of a non-minimal system, a powerful technique that reveals the true internal structure of any linear system and forms the basis for model reduction and controller design.", "problem": "Consider the linear time-invariant state-space system with state vector $x \\in \\mathbb{R}^{5}$, input $u \\in \\mathbb{R}$, and output $y \\in \\mathbb{R}$:\n$$\n\\dot{x} = A x + B u,\\quad y = C x,\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n-4  0  0  0  0 \\\\\n0  -1  0  -2  0 \\\\\n0  0  7  0  0 \\\\\n0  0  0  -2  0 \\\\\n0  0  0  0  5\n\\end{pmatrix},\\quad\nB = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix},\\quad\nC = \\begin{pmatrix} 1  -1  0  1  0 \\end{pmatrix}.\n$$\nStarting from the fundamental definitions of controllability and observability, construct a similarity transformation $T$ that puts the realization into the Kalman decomposition, separating the four canonical subspaces: controllable-observable, controllable-unobservable, uncontrollable-observable, and uncontrollable-unobservable. Your construction must explicitly identify the controllable subspace, the unobservable subspace, their intersections, and a basis selection that yields a block upper-triangular realization in Kalman form under the transformation $\\tilde{x} = T^{-1} x$.\n\nAfter obtaining the Kalman decomposition, report the characteristic polynomial (in the indeterminate $s$) of the controllable-and-observable block of $T^{-1} A T$. Give your final answer as a single analytic expression. No rounding is required. No physical units are involved.", "solution": "The problem requires the construction of a similarity transformation for a given linear time-invariant (LTI) system to bring it into Kalman canonical form, and then to determine the characteristic polynomial of the controllable and observable subsystem.\n\nThe system is defined by the state-space equations:\n$$ \\dot{x} = Ax + Bu, \\quad y = Cx $$\nwith matrices given as:\n$$ A = \\begin{pmatrix} -4  0  0  0  0 \\\\ 0  -1  0  -2  0 \\\\ 0  0  7  0  0 \\\\ 0  0  0  -2  0 \\\\ 0  0  0  0  5 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1  -1  0  1  0 \\end{pmatrix} $$\n\nFirst, we validate the problem. The givens are the matrices $A$, $B$, and $C$, with state dimension $n=5$, input dimension $m=1$, and output dimension $p=1$. The problem statement is self-contained, scientifically grounded in established control theory, and objective. It presents a standard, well-posed problem. The problem is valid.\n\nThe solution proceeds by identifying the four fundamental subspaces of the state space $\\mathbb{R}^5$: the controllable subspace $\\mathcal{C}$, the unobservable subspace $\\mathcal{O}^\\perp$, and their intersections which define the Kalman decomposition.\n\nStep 1: Determine the Controllable Subspace $\\mathcal{C}$.\nThe controllable subspace is the range of the controllability matrix $\\mathcal{C}_M = \\begin{pmatrix} B  AB  A^2B  A^3B  A^4B \\end{pmatrix}$.\nWe compute the columns:\n$B = \\begin{pmatrix} 0  1  0  1  1 \\end{pmatrix}^T$\n$AB = \\begin{pmatrix} 0  -3  0  -2  5 \\end{pmatrix}^T$\n$A^2B = \\begin{pmatrix} 0  7  0  4  25 \\end{pmatrix}^T$\n$A^3B = \\begin{pmatrix} 0  -15  0  -8  125 \\end{pmatrix}^T$\n$A^4B = \\begin{pmatrix} 0  31  0  16  625 \\end{pmatrix}^T$\nThe controllability matrix is:\n$$ \\mathcal{C}_M = \\begin{pmatrix} 0  0  0  0  0 \\\\ 1  -3  7  -15  31 \\\\ 0  0  0  0  0 \\\\ 1  -2  4  -8  16 \\\\ 1  5  25  125  625 \\end{pmatrix} $$\nThe first and third rows are zero. The rank is determined by the linear independence of the remaining rows. Let us examine the first three columns. The submatrix formed by rows $2$, $4$, $5$ and columns $1$, $2$, $3$ is $\\begin{pmatrix} 1  -3  7 \\\\ 1  -2  4 \\\\ 1  5  25 \\end{pmatrix}$, which has a determinant of $42 \\neq 0$. Thus, the rank of $\\mathcal{C}_M$ is $3$, and $\\dim(\\mathcal{C}) = 3$. The columns of $\\mathcal{C}_M$ all lie in the subspace where the first and third components are zero, which is $\\text{span}\\{e_2, e_4, e_5\\}$, where $e_i$ are the standard basis vectors. Since $\\dim(\\mathcal{C})=3$, we must have $\\mathcal{C} = \\text{span}\\{e_2, e_4, e_5\\}$.\n\nStep 2: Determine the Unobservable Subspace $\\mathcal{O}^\\perp$.\nThe unobservable subspace is the null space of the observability matrix $\\mathcal{O}_M$:\n$$ \\mathcal{O}_M = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\\\ CA^4 \\end{pmatrix} $$\nWe compute the rows:\n$C = \\begin{pmatrix} 1  -1  0  1  0 \\end{pmatrix}$\n$CA = \\begin{pmatrix} -4  -1  0  2  0 \\end{pmatrix}$\n$CA^2 = \\begin{pmatrix} 16  -3  0  -4  0 \\end{pmatrix}$\n$CA^3 = \\begin{pmatrix} -64  -5  0  8  0 \\end{pmatrix}$\n$CA^4 = \\begin{pmatrix} 256  -11  0  -16  0 \\end{pmatrix}$\nA vector $x = (x_1, x_2, x_3, x_4, x_5)^T$ is in $\\mathcal{O}^\\perp$ if $\\mathcal{O}_M x = 0$. The columns corresponding to $x_3$ and $x_5$ are zero, so any vector in $\\text{span}\\{e_3, e_5\\}$ is a candidate. The submatrix formed by columns $1$, $2$, and $4$ has rank $3$, as its top $3 \\times 3$ determinant is $86 \\neq 0$. Thus, for any $x \\in \\mathcal{O}^\\perp$, we must have $x_1=x_2=x_4=0$. The null space is therefore spanned by vectors where only $x_3$ and $x_5$ can be non-zero.\nHence, the unobservable subspace is $\\mathcal{O}^\\perp = \\text{span}\\{e_3, e_5\\}$, and $\\dim(\\mathcal{O}^\\perp) = 2$.\nThe observable subspace is $\\mathcal{O} = (\\mathcal{O}^\\perp)^\\perp = \\text{span}\\{e_1, e_2, e_4\\}$.\n\nStep 3: Construct Bases for the Kalman Decomposition Subspaces.\nThe state space $\\mathbb{R}^5$ is decomposed into a direct sum of four subspaces.\n1. Controllable and observable: $S_1 = \\mathcal{C} \\cap \\mathcal{O} = \\text{span}\\{e_2, e_4, e_5\\} \\cap \\text{span}\\{e_1, e_2, e_4\\} = \\text{span}\\{e_2, e_4\\}$. A basis is $\\{e_2, e_4\\}$. $\\dim(S_1) = 2$.\n2. Controllable and unobservable: $S_2 = \\mathcal{C} \\cap \\mathcal{O}^\\perp = \\text{span}\\{e_2, e_4, e_5\\} \\cap \\text{span}\\{e_3, e_5\\} = \\text{span}\\{e_5\\}$. A basis is $\\{e_5\\}$. $\\dim(S_2) = 1$.\n3. Uncontrollable and observable: $S_3 = \\mathcal{C}^\\perp \\cap \\mathcal{O}$. First, $\\mathcal{C}^\\perp = (\\text{span}\\{e_2, e_4, e_5\\})^\\perp = \\text{span}\\{e_1, e_3\\}$. So, $S_3 = \\text{span}\\{e_1, e_3\\} \\cap \\text{span}\\{e_1, e_2, e_4\\} = \\text{span}\\{e_1\\}$. A basis is $\\{e_1\\}$. $\\dim(S_3) = 1$.\n4. Uncontrollable and unobservable: $S_4 = \\mathcal{C}^\\perp \\cap \\mathcal{O}^\\perp = \\text{span}\\{e_1, e_3\\} \\cap \\text{span}\\{e_3, e_5\\} = \\text{span}\\{e_3\\}$. A basis is $\\{e_3\\}$. $\\dim(S_4) = 1$.\n\nStep 4: Construct the Similarity Transformation.\nThe transformation matrix $T$ is formed by concatenating the basis vectors in the order $S_1, S_2, S_3, S_4$.\n$$ T = \\begin{pmatrix} e_2  e_4  e_5  e_1  e_3 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  1  0 \\\\ 1  0  0  0  0 \\\\ 0  0  0  0  1 \\\\ 0  1  0  0  0 \\\\ 0  0  1  0  0 \\end{pmatrix} $$\nThis is a permutation matrix, so its inverse is its transpose: $T^{-1} = T^T$. The new state is $\\tilde{x} = T^{-1}x = (x_2, x_4, x_5, x_1, x_3)^T$.\n\nStep 5: Compute the Transformed System.\nThe transformed state matrix is $\\tilde{A} = T^{-1}AT$. The columns of $AT$ are $Ae_2, Ae_4, Ae_5, Ae_1, Ae_3$.\n$AT = A\\begin{pmatrix} e_2  e_4  e_5  e_1  e_3 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  -4  0 \\\\ -1  -2  0  0  0 \\\\ 0  0  0  0  7 \\\\ 0  -2  0  0  0 \\\\ 0  0  5  0  0 \\end{pmatrix}$.\nThen $\\tilde{A} = T^{-1}(AT)$ reorders the rows of $AT$ to $(2, 4, 5, 1, 3)$:\n$$ \\tilde{A} = \\begin{pmatrix} -1  -2  0  0  0 \\\\ 0  -2  0  0  0 \\\\ 0  0  5  0  0 \\\\ 0  0  0  -4  0 \\\\ 0  0  0  0  7 \\end{pmatrix} $$\nThis matrix is block diagonal, which is a special case of the required block upper-triangular form. The blocks correspond to the four subspaces:\n$$\n\\tilde{A} =\n\\left(\n\\begin{array}{cc|c|c|c}\n-1  -2  0  0  0 \\\\\n0  -2  0  0  0 \\\\\n\\hline\n0  0  5  0  0 \\\\\n\\hline\n0  0  0  -4  0 \\\\\n\\hline\n0  0  0  0  7\n\\end{array}\n\\right)\n= \\begin{pmatrix} A_{co}  0  0  0 \\\\ 0  A_{c\\bar{o}}  0  0 \\\\ 0  0  A_{\\bar{c}o}  0 \\\\ 0  0  0  A_{\\bar{c}\\bar{o}} \\end{pmatrix}\n$$\nThe controllable-and-observable block is the top-left $2 \\times 2$ matrix:\n$$ A_{co} = \\begin{pmatrix} -1  -2 \\\\ 0  -2 \\end{pmatrix} $$\n\nStep 6: Determine the Characteristic Polynomial.\nThe characteristic polynomial of the controllable-and-observable block is $p(s) = \\det(sI - A_{co})$.\n$$ sI - A_{co} = s\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} -1  -2 \\\\ 0  -2 \\end{pmatrix} = \\begin{pmatrix} s+1  2 \\\\ 0  s+2 \\end{pmatrix} $$\nThe determinant is:\n$$ p(s) = (s+1)(s+2) - (2)(0) = s^2 + 3s + 2 $$\nThis is the required characteristic polynomial.", "answer": "$$\\boxed{s^2 + 3s + 2}$$", "id": "2744735"}]}