{"hands_on_practices": [{"introduction": "A cornerstone of scientific machine learning is the creation of surrogate models, which are fast-running approximations of slower, high-fidelity physical simulations. This practice [@problem_id:2503006] provides a complete walkthrough of this fundamental process for a steady-state heat conduction problem. By deriving the analytical solution from first principles, verifying it with a numerical solver, and then training a simple regression model, you will build a foundational understanding of the entire pipeline for developing and validating a data-driven surrogate.", "problem": "You are tasked with transforming a one-dimensional steady-state heat conduction model with uniform internal heat generation into a synthetic supervised learning dataset for temperature prediction. Start from first principles of heat conduction to derive the governing equation and its closed-form solution, then implement a numerical scheme to verify the correctness of the solution and to generate labels for supervised learning. You must implement a single program that carries out the following specification and produces a single-line list of floating-point results as the final output.\n\nFundamental base and modeling assumptions:\n- Consider a homogeneous slab occupying the interval $0 \\le x \\le L$, with constant thermal conductivity $k$ and uniform volumetric heat generation $q$. Assume one-dimensional, steady, purely conductive heat transfer with no heat loss aside from the prescribed boundaries. Start from energy conservation and Fourier’s law of heat conduction to arrive at the standard one-dimensional steady conduction equation with uniform generation. The derivation must begin from the energy balance and the constitutive relation, not from a pre-remembered target formula. Dirichlet boundary conditions $T(0)=T_0$ and $T(L)=T_L$ are imposed.\n- The governing equation is a second-order linear ordinary differential equation obtained from the steady energy balance. Your derivation should show why the constant-coefficient operator and the uniform source lead to a quadratic temperature profile, and how boundary data determine the integration constants.\n\nImplementation tasks to be encoded in the program:\n1) Analytic label generation:\n   - Derive the closed-form solution $T(x)$ from the above assumptions and boundary conditions by integrating the governing equation and applying the boundary conditions. Use this $T(x)$ as the ground-truth label generator for supervised learning. The program must use this derived $T(x)$ explicitly.\n2) Numerical finite-difference verification:\n   - Implement a second-order central-difference finite-difference discretization of the governing equation on a uniform grid with $N_{\\text{fd}}=101$ nodes, including the boundaries. Enforce the Dirichlet boundary conditions strongly at $x=0$ and $x=L$. Assemble and solve the resulting tridiagonal linear system for the interior temperatures. Compare the numerical solution against the analytic solution on the same grid, and compute the maximum absolute error $e_{\\text{fd}}$. Report $e_{\\text{fd}}$ in Kelvin.\n3) Supervised learning dataset construction and surrogate training:\n   - For each case, generate a training set of $N_{\\text{train}}$ equally spaced points in $[0,L]$ with inputs (features) given by the vector $[1,\\, \\xi,\\, \\xi^2]$, where $\\xi = x/L$, and labels given by the analytic temperature $T(x)$. Train a polynomial regression model by Ordinary Least Squares (OLS) to map $[1,\\, \\xi,\\, \\xi^2] \\mapsto T$. Then, evaluate the trained model on a separate evaluation set of $N_{\\text{eval}}$ equally spaced points in $[0,L]$ using the same features, and compute the Root-Mean-Squared Error (RMSE) $e_{\\text{ml}}$ between the model predictions and the analytic solution. Report $e_{\\text{ml}}$ in Kelvin.\n4) Test suite and output:\n   - Use the following test suite, where all parameters are in the International System of Units (SI):\n     - Case $1$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,10.0,\\,1.0\\times 10^{5},\\,0.1,\\,300.0,\\,350.0,\\,15,\\,51\\,)$.\n     - Case $2$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,200.0,\\,0.0,\\,0.2,\\,400.0,\\,300.0,\\,7,\\,41\\,)$.\n     - Case $3$: $(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,15.0,\\,5.0\\times 10^{4},\\,0.05,\\,350.0,\\,350.0,\\,11,\\,61\\,)$.\n   - For each case, compute and collect $e_{\\text{fd}}$ and $e_{\\text{ml}}$ as floating-point numbers. All error metrics must be reported in Kelvin $(\\mathrm{K})$ and should be computed in double precision.\n   - Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\,e_{\\text{fd}}^{(1)},\\,e_{\\text{ml}}^{(1)},\\,e_{\\text{fd}}^{(2)},\\,e_{\\text{ml}}^{(2)},\\,e_{\\text{fd}}^{(3)},\\,e_{\\text{ml}}^{(3)}\\,]$ with no spaces.\n\nScientific realism requirements:\n- All physics and numerics must be consistent with the described one-dimensional steady conduction with uniform heat generation. Use $k>0$, $L>0$, and real-valued $q$, $T_0$, $T_L$. The dataset labels must come from the analytic $T(x)$ implied by the physical model. The supervised learning task should be justified by the fact that the target mapping from $[1,\\, \\xi,\\, \\xi^2]$ to $T$ is a fixed quadratic function for each case and thus suitable for Ordinary Least Squares (OLS) training.\n\nAngle units are not applicable. Express all reported error metrics in Kelvin $(\\mathrm{K})$ as floating-point values. The final output must strictly match the single-line format specified above.", "solution": "The problem statement is valid. It is scientifically grounded in the principles of heat transfer, mathematically well-posed, and provides a complete and unambiguous specification for all required tasks. The problem will be solved by first deriving the governing physics and its analytic solution, then implementing the specified numerical and machine learning procedures.\n\n**Part $1$: Derivation of Governing Equation and Analytic Solution**\n\nThe analysis begins from the first principle of energy conservation for a one-dimensional, steady-state system with internal heat generation. Consider an infinitesimal control volume of length $dx$ and constant cross-sectional area $A$ within a slab. The energy balance equation is:\n$$ \\dot{E}_{\\text{in}} - \\dot{E}_{\\text{out}} + \\dot{E}_{\\text{gen}} = 0 $$\nwhere $\\dot{E}_{\\text{in}}$ is the rate of heat conduction into the control volume at position $x$, $\\dot{E}_{\\text{out}}$ is the rate of heat conduction out of the control volume at position $x+dx$, and $\\dot{E}_{\\text{gen}}$ is the rate of heat generation within the volume. These terms are expressed as:\n$$ \\dot{E}_{\\text{in}} = q_x(x) A $$\n$$ \\dot{E}_{\\text{out}} = q_x(x+dx) A $$\n$$ \\dot{E}_{\\text{gen}} = q A dx $$\nHere, $q_x$ is the heat flux (heat rate per unit area) in the $x$-direction, and $q$ is the uniform volumetric heat generation rate. Substituting these into the energy balance gives:\n$$ q_x(x) A - q_x(x+dx) A + q A dx = 0 $$\nDividing by the volume $A dx$ and rearranging yields:\n$$ -\\frac{q_x(x+dx) - q_x(x)}{dx} + q = 0 $$\nTaking the limit as $dx \\to 0$ results in the differential form of the energy conservation equation:\n$$ -\\frac{d q_x}{dx} + q = 0 $$\nThe constitutive relation for heat conduction is Fourier's Law, which states that heat flux is proportional to the negative temperature gradient:\n$$ q_x = -k \\frac{dT}{dx} $$\nwhere $k$ is the thermal conductivity of the material, assumed to be constant. Substituting Fourier's Law into the energy equation gives the governing ordinary differential equation for temperature $T(x)$:\n$$ -\\frac{d}{dx}\\left(-k \\frac{dT}{dx}\\right) + q = 0 $$\n$$ k \\frac{d^2 T}{dx^2} + q = 0 $$\n$$ \\frac{d^2 T}{dx^2} = -\\frac{q}{k} $$\nThis is a second-order linear ordinary differential equation. To find the temperature distribution $T(x)$, we integrate twice with respect to $x$:\n$$ \\frac{dT}{dx} = -\\frac{q}{k} x + C_1 $$\n$$ T(x) = -\\frac{q}{2k} x^2 + C_1 x + C_2 $$\nThe integration constants $C_1$ and $C_2$ are determined from the specified Dirichlet boundary conditions: $T(0) = T_0$ and $T(L) = T_L$.\nApplying the first boundary condition at $x=0$:\n$$ T(0) = T_0 = -\\frac{q}{2k}(0)^2 + C_1(0) + C_2 \\implies C_2 = T_0 $$\nApplying the second boundary condition at $x=L$:\n$$ T(L) = T_L = -\\frac{q}{2k} L^2 + C_1 L + T_0 $$\nSolving for $C_1$:\n$$ C_1 L = T_L - T_0 + \\frac{q L^2}{2k} $$\n$$ C_1 = \\frac{T_L - T_0}{L} + \\frac{qL}{2k} $$\nSubstituting the expressions for $C_1$ and $C_2$ back into the general solution yields the final analytic expression for the temperature profile:\n$$ T(x) = -\\frac{q}{2k} x^2 + \\left(\\frac{T_L - T_0}{L} + \\frac{qL}{2k}\\right)x + T_0 $$\nThis equation provides the ground-truth temperature at any position $x$ in the slab and will be used to generate labels for the supervised learning task.\n\n**Part $2$: Numerical Finite-Difference Verification**\n\nA finite-difference method (FDM) is used to numerically solve the governing equation and verify the analytic solution. The domain $[0, L]$ is discretized into a uniform grid of $N_{\\text{fd}} = 101$ nodes, $x_i = i \\cdot \\Delta x$ for $i \\in \\{0, 1, \\dots, 100\\}$, with grid spacing $\\Delta x = L / (N_{\\text{fd}} - 1)$. At each interior node $x_i$ for $i \\in \\{1, \\dots, 99\\}$, the second derivative $\\frac{d^2 T}{dx^2}$ is approximated using a second-order accurate central difference scheme:\n$$ \\frac{d^2 T}{dx^2}\\bigg|_{x_i} \\approx \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} $$\nwhere $T_i \\approx T(x_i)$. Substituting this into the governing equation gives a system of linear algebraic equations:\n$$ \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} = -\\frac{q}{k} $$\n$$ T_{i-1} - 2T_i + T_{i+1} = -\\frac{q(\\Delta x)^2}{k} $$\nThis forms a tridiagonal system of $N_{\\text{fd}}-2=99$ equations for the unknown interior temperatures $\\mathbf{T}_{\\text{int}} = [T_1, T_2, \\dots, T_{99}]^T$. The boundary temperatures $T_0$ and $T_{N_{\\text{fd}}-1}=T_L$ are known. The system is solved, and the maximum absolute error $e_{\\text{fd}} = \\max_i |T_i^{\\text{numerical}} - T(x_i)^{\\text{analytic}}|$ is computed to assess the accuracy of the FDM solution.\n\n**Part $3$: Supervised Learning Surrogate Model**\n\nA surrogate model for temperature prediction is constructed using supervised learning. The analytic solution shows that $T(x)$ is a quadratic function of $x$. This implies it is also a quadratic function of the non-dimensional coordinate $\\xi = x/L$. The machine learning model is therefore chosen to be a quadratic polynomial regression model:\n$$ \\hat{T}(\\xi) = w_0 \\cdot 1 + w_1 \\cdot \\xi + w_2 \\cdot \\xi^2 $$\nThe features are the components of the vector $[1, \\xi, \\xi^2]$. A training dataset is generated with $N_{\\text{train}}$ equally spaced points $x_j$ in the interval $[0, L]$. For each point, the feature vector $\\mathbf{x}_{\\text{train}, j} = [1, x_j/L, (x_j/L)^2]$ is computed, and the corresponding label $y_{\\text{train}, j} = T(x_j)$ is generated using the analytic solution. The model weights $\\mathbf{w} = [w_0, w_1, w_2]^T$ are determined by solving the Ordinary Least Squares (OLS) problem, which minimizes the sum of squared differences between the predicted and actual labels. The solution is given by:\n$$ \\mathbf{w} = (X_{\\text{train}}^T X_{\\text{train}})^{-1} X_{\\text{train}}^T \\mathbf{y}_{\\text{train}} $$\nwhere $X_{\\text{train}}$ is the matrix of feature vectors and $\\mathbf{y}_{\\text{train}}$ is the vector of labels. Since the model family perfectly matches the true function, the OLS procedure is expected to learn the function with an error close to floating-point precision. The trained model is then evaluated on a separate set of $N_{\\text{eval}}$ points. The predictive performance is measured by the Root-Mean-Squared Error ($e_{\\text{ml}}$) between the model's predictions and the analytic temperatures on this evaluation set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the heat conduction problem for three test cases, computing\n    finite difference error (e_fd) and machine learning error (e_ml) for each.\n    \"\"\"\n\n    # Test suite as per the problem statement.\n    # (k, q, L, T0, TL, N_train, N_eval)\n    test_cases = [\n        (10.0, 1.0e5, 0.1, 300.0, 350.0, 15, 51),\n        (200.0, 0.0, 0.2, 400.0, 300.0, 7, 41),\n        (15.0, 5.0e4, 0.05, 350.0, 350.0, 11, 61),\n    ]\n\n    results = []\n\n    def get_analytic_T(x, k, q, L, T0, TL):\n        \"\"\"\n        Computes the analytic solution for the temperature T at position(s) x.\n        T(x) = -q/(2k) * x^2 + ( (T_L-T_0)/L + qL/(2k) ) * x + T_0\n        \"\"\"\n        C1 = (TL - T0) / L + q * L / (2 * k)\n        C2 = T0\n        return -q / (2 * k) * x**2 + C1 * x + C2\n\n    for case in test_cases:\n        k, q, L, T0, TL, N_train, N_eval = case\n\n        # --- 2) Numerical Finite-Difference Verification ---\n        N_fd = 101\n        N_int = N_fd - 2  # Number of interior nodes\n        x_fd = np.linspace(0, L, N_fd)\n        dx = L / (N_fd - 1)\n\n        # Assemble the tridiagonal matrix A for the interior nodes\n        A = np.zeros((N_int, N_int))\n        main_diag = -2 * np.ones(N_int)\n        off_diag = np.ones(N_int - 1)\n        np.fill_diagonal(A, main_diag)\n        np.fill_diagonal(A[1:], off_diag)\n        np.fill_diagonal(A[:, 1:], off_diag)\n\n        # Assemble the right-hand side vector b\n        b = np.full(N_int, -q * dx**2 / k)\n        b[0] -= T0\n        b[-1] -= TL\n\n        # Solve the linear system A * T_int = b\n        T_int = linalg.solve(A, b)\n\n        # Combine with boundary conditions to get the full numerical solution\n        T_numerical = np.concatenate(([T0], T_int, [TL]))\n\n        # Compute analytic solution on the same grid for comparison\n        T_analytic_fd = get_analytic_T(x_fd, k, q, L, T0, TL)\n\n        # Compute the maximum absolute error e_fd\n        e_fd = np.max(np.abs(T_numerical - T_analytic_fd))\n        results.append(e_fd)\n\n        # --- 3) Supervised Learning Dataset Construction and Surrogate Training ---\n        # Generate training data\n        x_train = np.linspace(0, L, N_train)\n        xi_train = x_train / L\n        # Feature matrix X_train: [1, xi, xi^2]\n        X_train = np.vstack([np.ones_like(xi_train), xi_train, xi_train**2]).T\n        y_train = get_analytic_T(x_train, k, q, L, T0, TL)\n\n        # Train polynomial regression model using Ordinary Least Squares (OLS)\n        # np.linalg.lstsq solves the equation Xw = y for w\n        w, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n        # Evaluate the model\n        x_eval = np.linspace(0, L, N_eval)\n        xi_eval = x_eval / L\n        # Feature matrix for evaluation\n        X_eval = np.vstack([np.ones_like(xi_eval), xi_eval, xi_eval**2]).T\n        \n        # Get model predictions\n        y_pred = X_eval @ w\n        \n        # Get true labels for the evaluation set\n        y_true = get_analytic_T(x_eval, k, q, L, T0, TL)\n\n        # Compute the Root-Mean-Squared Error (RMSE) e_ml\n        e_ml = np.sqrt(np.mean((y_pred - y_true)**2))\n        results.append(e_ml)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "2503006"}, {"introduction": "While building a surrogate for a single scenario is useful, the true power of physics-informed ML lies in creating generalizable models that are valid across a range of physical parameters. This thought exercise [@problem_id:2502955] delves into non-dimensionalization, a powerful technique from classical fluid dynamics and heat transfer that is essential for modern scientific ML. By transforming the governing equations, you will discover how this principle allows you to collapse a complex, multi-parameter problem into a universal, parameter-free form, dramatically improving data efficiency and model generalization.", "problem": "A laboratory is building a machine learning (ML) surrogate to predict one-dimensional transient heat conduction in homogeneous rods whose lengths, materials, and initial thermal offsets differ across experiments. For each rod with length $L$ and constant thermal diffusivity $\\alpha$, temperature $T(x,t)$ evolves according to energy conservation and Fourier’s law, and the ends are held at the ambient temperature $T_{\\infty}$ for all time. The initial temperature is uniform at $T_{\\infty}+\\Delta T$, where $\\Delta T&gt;0$ may vary across rods. Two training pipelines are considered to build a neural network (NN) that maps space-time coordinates to temperature.\n\nPipeline $\\mathsf{Dim}$: Train a single NN to approximate the dimensional solution $T(x,t)$ by feeding $(x,t,L,\\alpha,T_{\\infty},\\Delta T)$ as inputs and predicting $T$.\n\nPipeline $\\mathsf{ND}$: First transform to dimensionless variables $T^{*}=(T-T_{\\infty})/\\Delta T$, $x^{*}=x/L$, $t^{*}=\\alpha t/L^{2}$, then train a single NN to approximate $T^{*}(x^{*},t^{*})$ using pooled data from rods with different $L$, $\\alpha$, $T_{\\infty}$, and $\\Delta T$. At inference, map back via $T=T_{\\infty}+\\Delta T\\,T^{*}$.\n\nFrom first principles, explain how non-dimensionalization changes the governing dependence of the solution on the physical parameters and what that implies for generalization across scales in ML. Which statement best captures the effect of the $(T^{*},x^{*},t^{*})$ transformation on parameter count and cross-scale generalization for this conduction problem?\n\nA. By substituting $T^{*}$, $x^{*}$, and $t^{*}$ into the energy equation with Fourier’s law under Dirichlet boundary conditions, the parameter dependence on $L$, $\\alpha$, $T_{\\infty}$, and $\\Delta T$ is removed from the partial differential equation (PDE) and its data-consistent initial and boundary conditions, yielding a single universal dimensionless solution $T^{*}(x^{*},t^{*})$; thus one NN trained on pooled $(x^{*},t^{*})\\mapsto T^{*}$ data generalizes across different $L$ and $\\alpha$, and dimensional predictions follow by inverse scaling.\n\nB. Non-dimensionalization merely normalizes inputs and outputs to lie between $0$ and $1$ and does not change the number of independent governing parameters; therefore, a separate NN is still required for each distinct $L$ and $\\alpha$, and cross-scale generalization does not improve.\n\nC. Because non-dimensionalization removes $L$ and $\\alpha$ from the governing equations, it becomes impossible to recover dimensional temperatures $T$; therefore, interpretability and transfer back to physical units are compromised.\n\nD. With a sufficiently large NN, explicit non-dimensionalization is unnecessary because the network will automatically learn the correct scaling; hence there is no reduction in effective parameter count or improvement in generalization from using $(T^{*},x^{*},t^{*})$.\n\nE. Even if the ends are not held at $T_{\\infty}$ and instead experience convection with heat transfer coefficient $h$, the same $(T^{*},x^{*},t^{*})$ transformation always eliminates all parameter dependence so that the dimensionless problem remains parameter-free; therefore, one NN trained on $T^{*}(x^{*},t^{*})$ will generalize across varying $h$ without any additional features.", "solution": "The problem statement must first be validated for scientific and logical consistency.\n\nThe problem describes one-dimensional transient heat conduction in a homogeneous rod. The governing partial differential equation (PDE) is derived from the principle of energy conservation combined with Fourier's law of heat conduction. For a material with constant thermal diffusivity $\\alpha$, this is the heat equation:\n$$\n\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}\n$$\nThis equation is defined on the spatial domain $x \\in [0, L]$ for time $t \\ge 0$.\n\nThe boundary conditions (BCs) are given as the ends of the rod being held at the ambient temperature $T_{\\infty}$ for all time. This specifies Dirichlet boundary conditions:\n$$\nT(0, t) = T_{\\infty} \\quad \\text{for } t \\ge 0\n$$\n$$\nT(L, t) = T_{\\infty} \\quad \\text{for } t \\ge 0\n$$\n\nThe initial condition (IC) is a uniform temperature offset $\\Delta T$ above ambient:\n$$\nT(x, 0) = T_{\\infty} + \\Delta T \\quad \\text{for } x \\in (0, L)\n$$\n\nThe parameters of the problem are the rod length $L$, the thermal diffusivity $\\alpha$, the ambient temperature $T_{\\infty}$, and the initial temperature offset $\\Delta T > 0$. The solution is the temperature field $T(x, t)$, which depends on these four parameters.\n\nThe problem proposes a non-dimensionalization using the variables:\n$$\nT^{*} = \\frac{T - T_{\\infty}}{\\Delta T}, \\quad x^{*} = \\frac{x}{L}, \\quad t^{*} = \\frac{\\alpha t}{L^2}\n$$\nThe quantity $t^{*}$ is also known as the Fourier number, $\\text{Fo}$.\n\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a canonical problem in heat transfer theory with clearly defined physics, geometry, and conditions. The non-dimensionalization scheme is standard for this class of problems. The problem is valid. We may proceed to the solution.\n\nThe core of the task is to transform the dimensional governing equation and its associated conditions into a dimensionless form to analyze the effect on parameter dependence. We use the chain rule to transform the partial derivatives.\n\nFirst, we express $T$ in terms of $T^{*}$: $T(x,t) = T_{\\infty} + \\Delta T \\, T^{*}(x^{*}, t^{*})$.\n\nThe time derivative is:\n$$\n\\frac{\\partial T}{\\partial t} = \\Delta T \\frac{\\partial T^{*}}{\\partial t} = \\Delta T \\left(\\frac{\\partial T^{*}}{\\partial t^{*}} \\frac{\\partial t^{*}}{\\partial t}\\right) = \\Delta T \\left(\\frac{\\partial T^{*}}{\\partial t^{*}} \\frac{\\alpha}{L^2}\\right) = \\frac{\\alpha \\Delta T}{L^2} \\frac{\\partial T^{*}}{\\partial t^{*}}\n$$\n\nThe spatial derivatives are:\n$$\n\\frac{\\partial T}{\\partial x} = \\Delta T \\frac{\\partial T^{*}}{\\partial x} = \\Delta T \\left(\\frac{\\partial T^{*}}{\\partial x^{*}} \\frac{\\partial x^{*}}{\\partial x}\\right) = \\Delta T \\left(\\frac{\\partial T^{*}}{\\partial x^{*}} \\frac{1}{L}\\right) = \\frac{\\Delta T}{L} \\frac{\\partial T^{*}}{\\partial x^{*}}\n$$\n$$\n\\frac{\\partial^2 T}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(\\frac{\\Delta T}{L} \\frac{\\partial T^{*}}{\\partial x^{*}}\\right) = \\frac{\\Delta T}{L} \\left(\\frac{\\partial^2 T^{*}}{\\partial (x^{*})^2} \\frac{\\partial x^{*}}{\\partial x}\\right) = \\frac{\\Delta T}{L^2} \\frac{\\partial^2 T^{*}}{\\partial (x^{*})^2}\n$$\n\nSubstituting these into the original heat equation:\n$$\n\\frac{\\alpha \\Delta T}{L^2} \\frac{\\partial T^{*}}{\\partial t^{*}} = \\alpha \\left(\\frac{\\Delta T}{L^2} \\frac{\\partial^2 T^{*}}{\\partial (x^{*})^2}\\right)\n$$\nSince $\\alpha > 0$, $\\Delta T > 0$, and $L > 0$, we can divide both sides by the group $\\frac{\\alpha \\Delta T}{L^2}$ to obtain the dimensionless heat equation:\n$$\n\\frac{\\partial T^{*}}{\\partial t^{*}} = \\frac{\\partial^2 T^{*}}{\\partial (x^{*})^2}\n$$\n\nNext, we transform the boundary and initial conditions.\nThe spatial domain $x \\in [0, L]$ becomes $x^{*} \\in [0, 1]$.\nThe time domain $t \\in [0, \\infty)$ becomes $t^{*} \\in [0, \\infty)$.\n\nFor the BC at $x=0$, we have $x^{*}=0$.\n$$\nT^{*}(0, t^{*}) = \\frac{T(0, t) - T_{\\infty}}{\\Delta T} = \\frac{T_{\\infty} - T_{\\infty}}{\\Delta T} = 0\n$$\n\nFor the BC at $x=L$, we have $x^{*}=1$.\n$$\nT^{*}(1, t^{*}) = \\frac{T(L, t) - T_{\\infty}}{\\Delta T} = \\frac{T_{\\infty} - T_{\\infty}}{\\Delta T} = 0\n$$\n\nFor the IC at $t=0$, we have $t^{*}=0$.\n$$\nT^{*}(x^{*}, 0) = \\frac{T(x, 0) - T_{\\infty}}{\\Delta T} = \\frac{(T_{\\infty} + \\Delta T) - T_{\\infty}}{\\Delta T} = 1\n$$\n\nThe complete dimensionless problem is:\nFind $T^{*}(x^{*}, t^{*})$ for $x^{*} \\in [0, 1]$ and $t^{*} \\ge 0$ such that:\n$$\n\\frac{\\partial T^{*}}{\\partial t^{*}} = \\frac{\\partial^2 T^{*}}{\\partial (x^{*})^2}\n$$\nwith BCs: $T^{*}(0, t^{*}) = 0$, $T^{*}(1, t^{*}) = 0$, and IC: $T^{*}(x^{*}, 0) = 1$.\n\nThis dimensionless formulation is entirely free of the original physical parameters $L$, $\\alpha$, $T_{\\infty}$, and $\\Delta T$. This means that the solution $T^{*}(x^{*}, t^{*})$ is a single, universal function. Any physical realization of this problem, regardless of the specific values of $L, \\alpha, T_{\\infty}, \\Delta T$, maps to this exact same dimensionless problem.\n\nFrom a machine learning perspective, this is a profound simplification.\nPipeline $\\mathsf{Dim}$ requires an NN to learn a function of $6$ variables: $T(x, t, L, \\alpha, T_{\\infty}, \\Delta T)$. The NN must internally discover the complex scaling relationships dictated by the physics.\nPipeline $\\mathsf{ND}$ requires an NN to learn a function of only $2$ variables: $T^{*}(x^{*}, t^{*})$. Data from experiments with vastly different physical parameters can all be transformed into the dimensionless space and pooled to train a single model on this universal function. This drastically reduces the complexity of the learning task, improves data efficiency, and ensures perfect generalization across all scales and parameters for which the physical model is valid. The dimensional answer is recovered trivially by the inverse transformation: $T(x,t) = T_{\\infty} + \\Delta T \\, T^{*}(\\frac{x}{L}, \\frac{\\alpha t}{L^2})$.\n\nNow we evaluate the given options.\n\nA. By substituting $T^{*}$, $x^{*}$, and $t^{*}$ into the energy equation with Fourier’s law under Dirichlet boundary conditions, the parameter dependence on $L$, $\\alpha$, $T_{\\infty}$, and $\\Delta T$ is removed from the partial differential equation (PDE) and its data-consistent initial and boundary conditions, yielding a single universal dimensionless solution $T^{*}(x^{*},t^{*})$; thus one NN trained on pooled $(x^{*},t^{*})\\mapsto T^{*}$ data generalizes across different $L$ and $\\alpha$, and dimensional predictions follow by inverse scaling.\nThis statement is a precise and complete summary of our derivation. The non-dimensionalization eliminates all parameters from the PDE system, resulting in a universal solution. This allows data pooling and guarantees generalization for an ML model trained in the dimensionless space. The recovery via inverse scaling is also correctly stated. **Correct**.\n\nB. Non-dimensionalization merely normalizes inputs and outputs to lie between $0$ and $1$ and does not change the number of independent governing parameters; therefore, a separate NN is still required for each distinct $L$ and $\\alpha$, and cross-scale generalization does not improve.\nThis statement is fundamentally flawed. While normalization is a side-effect (e.g., $T^{*}$ is initially $1$ and decays towards $0$), the primary achievement is the reduction of the parameter space. Our derivation explicitly shows that the number of governing parameters in the dimensionless PDE system is reduced from four to zero. The conclusion that a separate NN is needed is therefore false. **Incorrect**.\n\nC. Because non-dimensionalization removes $L$ and $\\alpha$ from the governing equations, it becomes impossible to recover dimensional temperatures $T$; therefore, interpretability and transfer back to physical units are compromised.\nThis statement is patently false. The transformation is algebraic and fully invertible. As shown above, $T(x,t) = T_{\\infty} + \\Delta T \\, T^{*}(x/L, \\alpha t/L^2)$. Given the universal solution $T^{*}$ and the parameters for a specific case, the physical temperature $T$ is uniquely and easily determined. Far from compromising interpretability, this process enhances it by separating universal behavior from case-specific scaling. **Incorrect**.\n\nD. With a sufficiently large NN, explicit non-dimensionalization is unnecessary because the network will automatically learn the correct scaling; hence there is no reduction in effective parameter count or improvement in generalization from using $(T^{*},x^{*},t^{*})$.\nThis reflects a naive belief in the power of neural networks as universal approximators, ignoring practical realities of training and data requirements. While a large enough network could theoretically approximate the multi-variable dimensional function, learning the complex physical scaling laws embedded within it from sparse data is an exceptionally difficult task that often leads to poor generalization. Embedding known physics, such as scaling laws via non-dimensionalization, is a cornerstone of efficient and robust scientific machine learning. It provides a strong inductive bias that dramatically improves sample efficiency and generalization. The claim of no reduction in effective parameter count is false; the learning problem is simplified from a $6 \\rightarrow 1$ mapping to a $2 \\rightarrow 1$ mapping. **Incorrect**.\n\nE. Even if the ends are not held at $T_{\\infty}$ and instead experience convection with heat transfer coefficient $h$, the same $(T^{*},x^{*},t^{*})$ transformation always eliminates all parameter dependence so that the dimensionless problem remains parameter-free; therefore, one NN trained on $T^{*}(x^{*},t^{*})$ will generalize across varying $h$ without any additional features.\nThis statement makes a strong claim about a different physical scenario that must be verified. A convective boundary condition at $x=L$ is of the form $-k \\frac{\\partial T}{\\partial x}\\big|_{x=L} = h [T(L,t) - T_{\\infty}]$, where $k$ is thermal conductivity. Substituting the dimensionless variables and using $k = \\alpha \\rho c_p$:\n$$\n-k \\left(\\frac{\\Delta T}{L} \\frac{\\partial T^{*}}{\\partial x^{*}}\\bigg|_{x^{*}=1}\\right) = h [\\Delta T \\cdot T^{*}(1, t^{*})]\n$$\n$$\n- \\frac{\\partial T^{*}}{\\partial x^{*}}\\bigg|_{x^{*}=1} = \\frac{hL}{k} T^{*}(1, t^{*})\n$$\nThe dimensionless group $\\frac{hL}{k}$ is the Biot number, $\\text{Bi}$. The resulting dimensionless boundary condition is $- \\frac{\\partial T^{*}}{\\partial x^{*}}\\big|_{x^{*}=1} = \\text{Bi} \\cdot T^{*}(1, t^{*})$. The dimensionless problem is now dependent on the Biot number. Therefore, the claim that the problem remains parameter-free is false. To generalize across different values of $h$, the NN would need to take $\\text{Bi}$ as an input. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2502955"}, {"introduction": "For dynamic systems, predicting the future state is only useful if the prediction process itself is stable over long rollouts. This practical exercise [@problem_id:2502968] investigates the critical issue of numerical stability in learned surrogate models for transient phenomena. By implementing both an explicit and an implicit time-stepping scheme for a learned residual, you will gain hands-on experience with how the choice of numerical solver—a concept from classical numerical analysis—dramatically affects the long-term reliability and robustness of a dynamic surrogate.", "problem": "You will implement and analyze a machine-learned implicit-in-time surrogate for one-dimensional heat conduction with a nonlinear source, formulated as a discrete residual equation whose next-time temperature $T^{n+1}$ is obtained by fixed-point iterations. You will compare its stability against an explicit (forward Euler) rollout using the same learned residual. All computations are to be carried out in a non-dimensional form, hence no physical units are required. All angles in trigonometric functions must be in radians. The final program must produce a single-line output as specified below.\n\nConsider the one-dimensional non-dimensional heat equation derived from conservation of energy and Fourier’s law of heat conduction,\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T),\n$$\non the spatial interval $[0,1]$ with homogeneous Dirichlet boundary conditions $T(0,t)=0$ and $T(1,t)=0$. Let $x_i = i h$ for $i = 1,2,\\dots,M$, where $h = \\frac{1}{M+1}$ is the uniform grid spacing. The centered finite-difference approximation of the Laplacian on the interior grid yields the semi-discrete operator $L \\in \\mathbb{R}^{M \\times M}$ acting on the interior vector $T \\in \\mathbb{R}^M$ as\n$$\n(LT)_i = \\frac{T_{i-1} - 2 T_i + T_{i+1}}{h^2}, \\quad i \\in \\{1,2,\\dots,M\\},\n$$\nwith the convention that $T_0 = 0$ and $T_{M+1} = 0$ due to the boundary conditions. We define the nonlinear source as $s(T) = \\sin(T)$ applied elementwise.\n\nLet $T^n \\in \\mathbb{R}^M$ be the temperature vector at time $t^n$, and consider a learned residual surrogate corresponding to backward Euler time stepping,\n$$\nR_\\theta\\left(T^{n+1}; T^n\\right) \\equiv T^{n+1} - T^n - \\Delta t \\left( (1+\\theta_L) \\, L T^{n+1} + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^{n+1}\\right) \\right),\n$$\nwhere $\\Delta t$ is the time step, $\\theta = (\\theta_L,\\theta_s)$ are learned scalars encoding model-form error, and $\\beta$ is a scalar controlling the source strength.\n\nYour task has three parts:\n- Implement an implicit-in-time surrogate that advances $T^n \\mapsto T^{n+1}$ by solving $R_\\theta\\left(T^{n+1}; T^n\\right) = 0$ using fixed-point iterations. Use the mapping\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right),\n$$\nand perform damped Picard iterations\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right),\n$$\ninitialized with $U^{(0)} = T^n$, where $\\lambda \\in (0,1]$ is a damping parameter. Terminate the iteration when $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon$ or when a maximum of $K$ iterations is reached. If convergence is not achieved (i.e., the maximum is reached without satisfying the tolerance), classify that time step as unstable for the implicit surrogate and stop further stepping for that test case.\n- Implement an explicit rollout surrogate using the same learned residual model, i.e., forward Euler with\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^n\\right) \\right).\n$$\n- Compare stability across multiple time steps up to a specified final time. Define the stability criterion as follows: a method is stable for a test case if at every time step the iterate contains no Not-a-Number and no infinity and also satisfies $\\|T^n\\|_2 \\le B_{\\max}$, where $B_{\\max}$ is a specified bound. If a time step violates any of these checks, classify the method as unstable and stop further stepping for that test case.\n\nInitialization and common settings:\n- Use the initial condition $T_i^0 = \\sin(\\pi x_i)$ for all $i \\in \\{1,2,\\dots,M\\}$.\n- Use homogeneous Dirichlet boundary conditions implemented implicitly via the interior-only operator $L$ with $T_0 = 0$ and $T_{M+1} = 0$.\n- Use the following fixed-point parameters: damping $\\lambda = 0.2$, tolerance $\\varepsilon = 10^{-8}$, and maximum iterations $K = 200$.\n- Use the stability bound $B_{\\max} = 10^6$.\n- All trigonometric function arguments are in radians.\n\nTest suite:\nUse $M = 20$ interior nodes for all tests. For each test case, run time stepping from $t^0 = 0$ to $t^{N} = t_{\\mathrm{final}}$ with $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$ steps using both the implicit surrogate and the explicit surrogate. The four test cases are:\n- Case $1$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0006, 0.05, 0.1, 0.5, 0.05)$.\n- Case $2$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.002, 0.05, 0.1, 0.5, 0.05)$.\n- Case $3$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.00115, 0.05, 0.1, 0.5, 0.05)$.\n- Case $4$: $(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0018, -0.4, -0.2, 0.5, 0.05)$.\n\nOutput specification:\n- For each test case, compute a stability code $c$ defined by\n  - $c = 0$ if both implicit and explicit surrogates are stable,\n  - $c = 1$ if the implicit surrogate is stable and the explicit surrogate is unstable,\n  - $c = 2$ if both are unstable,\n  - $c = 3$ if the implicit surrogate is unstable and the explicit surrogate is stable.\n- Also compute the terminal norms $n_{\\mathrm{imp}} = \\|T^N_{\\mathrm{imp}}\\|_2$ and $n_{\\mathrm{exp}} = \\|T^N_{\\mathrm{exp}}\\|_2$ for each test case, where $T^N_{\\mathrm{imp}}$ and $T^N_{\\mathrm{exp}}$ denote the terminal states produced by the implicit and explicit surrogates, respectively.\n- Your program should produce a single line of output containing the results aggregated for all four cases as a comma-separated list enclosed in square brackets in the order\n$$\n[c_1, \\text{round}(n_{\\mathrm{imp},1}, 6), \\text{round}(n_{\\mathrm{exp},1}, 6), c_2, \\text{round}(n_{\\mathrm{imp},2}, 6), \\text{round}(n_{\\mathrm{exp},2}, 6), c_3, \\text{round}(n_{\\mathrm{imp},3}, 6), \\text{round}(n_{\\mathrm{exp},3}, 6), c_4, \\text{round}(n_{\\mathrm{imp},4}, 6), \\text{round}(n_{\\mathrm{exp},4}, 6)].\n$$\nEach floating-point number must be rounded to exactly $6$ decimals in the printed output.", "solution": "The problem requires the implementation and stability analysis of two numerical surrogates for a one-dimensional nonlinear heat equation. One surrogate is based on an implicit time-stepping scheme, solved with fixed-point iterations, while the other is an explicit forward-time integration. The stability of these two methods will be compared across a suite of test cases.\n\nThe governing partial differential equation is the non-dimensional reaction-diffusion equation:\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T)\n$$\nHere, $T(x,t)$ is the temperature, $x \\in [0,1]$ is the spatial coordinate, $t$ is time, $\\beta$ is a scalar parameter for the source term strength, and $s(T) = \\sin(T)$ is the nonlinear source function. Homogeneous Dirichlet boundary conditions, $T(0,t) = T(1,t) = 0$, are imposed.\n\nFirst, we discretize the spatial domain. We use a uniform grid with $M$ interior points $x_i = i h$ for $i \\in \\{1, 2, \\dots, M\\}$, where the grid spacing is $h = \\frac{1}{M+1}$. The spatial derivative term $\\frac{\\partial^2 T}{\\partial x^2}$ is approximated using a second-order central finite difference scheme. This results in a system of ordinary differential equations for the vector of temperatures at the interior grid points, $T(t) \\in \\mathbb{R}^M$. The spatial operator is represented by a matrix $L \\in \\mathbb{R}^{M \\times M}$, where $(LT)_i = \\frac{T_{i-1} - 2T_i + T_{i+1}}{h^2}$. The boundary conditions $T_0 = T_{M+1} = 0$ are incorporated into the structure of this matrix. Specifically, $L$ is a symmetric tridiagonal matrix with $-\\frac{2}{h^2}$ on the main diagonal and $\\frac{1}{h^2}$ on the first super- and sub-diagonals.\n\nThe initial condition is given by a sinusoidal profile: $T_i^0 = \\sin(\\pi x_i)$ for $i \\in \\{1,\\dots,M\\}$.\n\nThe problem then introduces a \"machine-learned\" surrogate model, which modifies the standard numerical schemes with scalar parameters $\\theta_L$ and $\\theta_s$. These parameters represent learned corrections to the physical model's diffusion and source terms.\n\nThe explicit surrogate advances the solution in time using a forward Euler-like method:\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s(T^n) \\right)\n$$\nwhere $T^n$ is the temperature vector at time $t^n = n \\Delta t$.\n\nThe implicit surrogate is based on a backward Euler-like scheme. The solution at the next time step, $T^{n+1}$, is the root of the residual equation $R_\\theta(T^{n+1}; T^n) = 0$, where:\n$$\nR_\\theta\\left(U; T^n\\right) \\equiv U - T^n - \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\nTo solve this nonlinear system for $U = T^{n+1}$, we rearrange the equation into a fixed-point problem $U = \\mathcal{G}(U; T^n)$, with the mapping function defined as:\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\nThis is solved using damped Picard iterations, initialized with $U^{(0)} = T^n$. The iterative update rule is:\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right)\n$$\nwhere $\\lambda = 0.2$ is the damping parameter. The iteration continues until the change between successive iterates is sufficiently small, $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon = 10^{-8}$, or a maximum of $K=200$ iterations is reached.\n\nStability for each method is assessed at each time step. A method is considered unstable for a given test case if at any time step $n$, the computed solution $T^n$ contains Not-a-Number (NaN) or infinity (inf) values, or if its Euclidean norm exceeds a bound, $\\|T^n\\|_2 > B_{\\max} = 10^6$. For the implicit method, failure of the Picard iteration to converge within $K$ steps also constitutes instability. If a method is deemed unstable, the time-stepping process for that method is terminated.\n\nThe overall procedure for each of the four test cases is as follows:\n1. Initialize the temperature vector $T^0$ using the given initial condition.\n2. For both the implicit and explicit surrogates, perform time stepping from $t=0$ to $t_{\\mathrm{final}}$, for a total of $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$ steps.\n3. During each simulation, monitor for the specified instability conditions.\n4. Record whether each method completed all $N$ steps stably.\n5. Based on the stability outcomes, determine the stability code $c$: $0$ if both are stable, $1$ if implicit is stable and explicit is not, $2$ if both are unstable, and $3$ if implicit is unstable and explicit is stable.\n6. Compute the Euclidean norm of the final temperature vector for each method. If a method becomes unstable at step $k < N$, the state $T^k$ is considered the terminal state for that method.\n7. Collate these results ($c$, and the two terminal norms) for all four test cases into a single formatted output string. The norms are reported formatted to $6$ decimal places.\n\nThis involves a careful implementation of the matrix operator, the time-stepping loops, the iterative solver, and the specified stability checks.", "answer": "```python\nimport numpy as np\nimport math\n\n# Define global constants from the problem statement\nM = 20\nLAMBDA = 0.2\nEPSILON = 1e-8\nK_MAX = 200\nB_MAX = 1e6\n\ndef get_laplacian(m_nodes):\n    \"\"\"Constructs the 1D finite difference Laplacian matrix L.\"\"\"\n    h = 1.0 / (m_nodes + 1)\n    h2 = h * h\n    main_diag = -2.0 / h2 * np.ones(m_nodes)\n    off_diag = 1.0 / h2 * np.ones(m_nodes - 1)\n    L = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    return L\n\ndef source_term(T):\n    \"\"\"Computes the nonlinear source term s(T) = sin(T).\"\"\"\n    return np.sin(T)\n\ndef solve_implicit_step(Tn, dt, theta_L, theta_s, beta, L):\n    \"\"\"\n    Solves for T^{n+1} using damped Picard iterations.\n    Returns the solution and a boolean indicating convergence.\n    \"\"\"\n    U_k = Tn.copy()\n    \n    for _ in range(K_MAX):\n        # Calculate G(U_k; Tn)\n        Lu = L @ U_k\n        s_U = source_term(U_k)\n        G_U = Tn + dt * ((1 + theta_L) * Lu + (1 + theta_s) * beta * s_U)\n        \n        # Damped Picard update\n        U_k_plus_1 = (1 - LAMBDA) * U_k + LAMBDA * G_U\n        \n        # Check for convergence\n        diff = np.linalg.norm(U_k_plus_1 - U_k)\n        \n        # Check for numerical issues in the iteration itself\n        if np.isnan(diff) or np.isinf(diff):\n            return U_k_plus_1, False\n        \n        if diff <= EPSILON:\n            return U_k_plus_1, True\n            \n        U_k = U_k_plus_1\n        \n    return U_k, False # Did not converge\n\ndef run_simulation(params, L, x_grid, method):\n    \"\"\"\n    Runs a simulation for a given method ('implicit' or 'explicit').\n    Returns a stability flag and the final terminal norm.\n    \"\"\"\n    dt, theta_L, theta_s, beta, t_final = params\n    \n    # Initial condition\n    T = np.sin(np.pi * x_grid)\n    \n    # Check initial condition stability (unlikely to be unstable)\n    norm_T = np.linalg.norm(T)\n    if not np.isfinite(norm_T) or norm_T > B_MAX:\n        return False, norm_T\n        \n    num_steps = math.floor(t_final / dt)\n    \n    for _ in range(num_steps):\n        if method == 'implicit':\n            T_next, converged = solve_implicit_step(T, dt, theta_L, theta_s, beta, L)\n            if not converged:\n                # Instability due to non-convergence\n                return False, np.linalg.norm(T) # Return norm of last good state\n        else: # explicit\n            LTn = L @ T\n            s_T = source_term(T)\n            T_next = T + dt * ((1 + theta_L) * LTn + (1 + theta_s) * beta * s_T)\n            \n        # General stability check for the new state T_next\n        norm_T_next = np.linalg.norm(T_next)\n        if not np.isfinite(norm_T_next) or norm_T_next > B_MAX:\n            # Instability due to blow-up or NaN\n            return False, norm_T_next\n            \n        T = T_next\n        \n    # If all steps completed successfully\n    return True, np.linalg.norm(T)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    \n    # Setup spatial discretization\n    L = get_laplacian(M)\n    h = 1.0 / (M + 1)\n    x_grid = np.array([i * h for i in range(1, M + 1)])\n\n    test_cases = [\n        (0.0006, 0.05, 0.1, 0.5, 0.05),\n        (0.002, 0.05, 0.1, 0.5, 0.05),\n        (0.00115, 0.05, 0.1, 0.5, 0.05),\n        (0.0018, -0.4, -0.2, 0.5, 0.05),\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        # Run implicit simulation\n        imp_stable, n_imp = run_simulation(case_params, L, x_grid, 'implicit')\n        \n        # Run explicit simulation\n        exp_stable, n_exp = run_simulation(case_params, L, x_grid, 'explicit')\n        \n        # Determine stability code c\n        if imp_stable and exp_stable:\n            c = 0\n        elif imp_stable and not exp_stable:\n            c = 1\n        elif not imp_stable and not exp_stable:\n            c = 2\n        else: # not imp_stable and exp_stable\n            c = 3\n        \n        # Append results for this case\n        results.append(c)\n        results.append(f\"{n_imp:.6f}\")\n        results.append(f\"{n_exp:.6f}\")\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2502968"}]}