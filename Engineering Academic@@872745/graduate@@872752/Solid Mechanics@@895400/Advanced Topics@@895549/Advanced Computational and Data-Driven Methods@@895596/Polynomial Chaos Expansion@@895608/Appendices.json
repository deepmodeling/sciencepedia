{"hands_on_practices": [{"introduction": "The power of Polynomial Chaos Expansion stems from the properties of the underlying orthogonal polynomial bases. This first practice takes a foundational step, focusing on the Hermite polynomials that are central to modeling Gaussian uncertainties. By deriving the three-term recurrence relation from first principles, you will uncover a fundamental algebraic structure that is not just a mathematical elegance, but a cornerstone for efficiently assembling the large-scale linear systems encountered in intrusive PCE methods [@problem_id:2671707].", "problem": "A one-dimensional linear elastic bar of length $L$ is subjected to a prescribed axial displacement at $x=L$, while the Young's modulus $E$ is modeled as a lognormal random field with a single dominant standard normal mode $x \\sim \\mathcal{N}(0,1)$. In a non-intrusive Polynomial Chaos Expansion (PCE) analysis of the bar’s axial displacement $u(L;x)$, one employs a basis of orthonormal polynomials $\\{\\psi_{n}(x)\\}_{n \\geq 0}$ with respect to the standard normal probability measure, defined by the inner product\n$$\n\\langle f,g \\rangle = \\int_{-\\infty}^{\\infty} f(x)\\,g(x)\\,\\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)\\,dx = \\mathbb{E}[f(x)\\,g(x)].\n$$\nThese orthonormal polynomials are the orthonormal Hermite polynomials associated with the standard normal measure. A fundamental tool for assembling the PCE numerically is the three-term recurrence relation satisfied by the orthonormal basis. Starting only from the definition of orthonormality under the standard normal measure, the Rodrigues representation of the (probabilists’) Hermite polynomials,\n$$\n\\mathrm{He}_{n}(x) = (-1)^{n}\\exp\\!\\left(\\frac{x^{2}}{2}\\right)\\frac{d^{n}}{dx^{n}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right),\n$$\nand standard properties of integration by parts, derive the three-term recurrence\n$$\nx\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x), \\quad n \\geq 0,\n$$\nfor the orthonormal Hermite basis $\\{\\psi_{n}\\}$ under the standard normal weight, and specify the recurrence coefficients $a_{n}$ and $b_{n}$ as explicit functions of $n$. You may assume $\\psi_{-1} \\equiv 0$ by convention. Express your final answer as the row matrix $[\\,a_{n}\\;\\;b_{n}\\,]$. No numerical rounding is required, and no physical units apply.", "solution": "The problem as stated is valid. It is a well-posed mathematical exercise in the theory of orthogonal polynomials, which is a cornerstone of Polynomial Chaos Expansions in uncertainty quantification. All necessary definitions and properties are provided to derive a unique and verifiable result. We shall proceed with the derivation.\n\nThe objective is to derive the coefficients $a_n$ and $b_n$ for the three-term recurrence relation satisfied by the orthonormal Hermite polynomials $\\{\\psi_n(x)\\}_{n \\ge 0}$:\n$$\nx\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x)\n$$\nThe polynomials are orthonormal with respect to the standard normal weight function $w(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$. The inner product is $\\langle f,g \\rangle = \\int_{-\\infty}^{\\infty} f(x)g(x)w(x)dx$.\n\nThe polynomials $\\{\\psi_n(x)\\}$ are related to the orthogonal (but not orthonormal) probabilists' Hermite polynomials $\\{\\mathrm{He}_n(x)\\}$ by a normalization constant. Let $\\psi_n(x) = C_n \\mathrm{He}_n(x)$, where $C_n = 1/\\|\\mathrm{He}_n\\|$. The norm is defined as $\\|\\mathrm{He}_n\\| = \\sqrt{\\langle \\mathrm{He}_n, \\mathrm{He}_n \\rangle}$.\n\nFirst, we must calculate the squared norm, $\\|\\mathrm{He}_n\\|^2$. We use the provided Rodrigues' formula for $\\mathrm{He}_n(x)$:\n$$\n\\mathrm{He}_{n}(x) = (-1)^{n}\\exp\\left(\\frac{x^{2}}{2}\\right)\\frac{d^{n}}{dx^{n}}\\exp\\left(-\\frac{x^{2}}{2}\\right)\n$$\nThe squared norm is:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\langle \\mathrm{He}_n, \\mathrm{He}_n \\rangle = \\int_{-\\infty}^{\\infty} \\mathrm{He}_n(x) \\left[ (-1)^{n} \\exp\\left(\\frac{x^2}{2}\\right) \\frac{d^n}{dx^n}\\exp\\left(-\\frac{x^2}{2}\\right) \\right] \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThis simplifies to:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{(-1)^n}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\mathrm{He}_n(x) \\left( \\frac{d^n}{dx^n}\\exp\\left(-\\frac{x^2}{2}\\right) \\right) dx\n$$\nWe apply integration by parts $n$ times. For each step, the boundary terms of the form $[\\dots \\exp(-x^2/2)]_{-\\infty}^\\infty$ vanish because the exponential term decays to zero faster than any polynomial can grow. After $n$ applications, the derivatives are transferred from the exponential term to $\\mathrm{He}_n(x)$:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{(-1)^n(-1)^n}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\left( \\frac{d^n}{dx^n} \\mathrm{He}_n(x) \\right) \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThe polynomial $\\mathrm{He}_n(x)$ is of degree $n$ with a leading coefficient of $1$. This can be shown by induction from the Rodrigues formula or by using the known property $\\mathrm{He}_n'(x) = n \\mathrm{He}_{n-1}(x)$. It follows that the $n$-th derivative is a constant: $\\frac{d^n}{dx^n} \\mathrm{He}_n(x) = n!$.\nSubstituting this into the integral:\n$$\n\\|\\mathrm{He}_n\\|^2 = \\frac{n!}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{2}\\right) dx = n! \\times 1 = n!\n$$\nThe normalization constant is therefore $C_n = 1/\\|\\mathrm{He}_n\\| = 1/\\sqrt{n!}$. The orthonormal polynomial is $\\psi_n(x) = \\frac{1}{\\sqrt{n!}} \\mathrm{He}_n(x)$.\n\nNext, we establish the three-term recurrence for the orthogonal polynomials $\\mathrm{He}_n(x)$. The polynomial $x \\mathrm{He}_n(x)$ has degree $n+1$ and can be expressed in the basis of Hermite polynomials:\n$$\nx \\mathrm{He}_n(x) = \\sum_{k=0}^{n+1} c_k \\mathrm{He}_k(x)\n$$\nThe coefficient $c_k$ is found using orthogonality: $c_k = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_k \\rangle}{\\|\\mathrm{He}_k\\|^2} = \\frac{\\langle \\mathrm{He}_n, x \\mathrm{He}_k \\rangle}{\\|\\mathrm{He}_k\\|^2}$. Since $x\\mathrm{He}_k(x)$ is a polynomial of degree $k+1$, the inner product $\\langle \\mathrm{He}_n, x \\mathrm{He}_k \\rangle$ is zero if $n  k+1$, i.e., $k  n-1$. Thus, only the coefficients for $k=n+1, n, n-1$ can be non-zero. The recurrence has the form:\n$$\nx \\mathrm{He}_n(x) = c_{n+1} \\mathrm{He}_{n+1}(x) + c_n \\mathrm{He}_n(x) + c_{n-1} \\mathrm{He}_{n-1}(x)\n$$\nThe coefficients are:\n1. $c_{n+1}$: The leading term of $\\mathrm{He}_n(x)$ is $x^n$. Thus, the leading term of $x \\mathrm{He}_n(x)$ is $x^{n+1}$. The leading term of the right-hand side is $c_{n+1}x^{n+1}$. Equating the leading coefficients gives $c_{n+1}=1$.\n2. $c_n$: $c_n = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_n \\rangle}{\\|\\mathrm{He}_n\\|^2}$. The integrand for the inner product is $x (\\mathrm{He}_n(x))^2 w(x)$. The polynomial $\\mathrm{He}_n(x)$ has parity $(-1)^n$. Thus $(\\mathrm{He}_n(x))^2$ is an even function. The weight function $w(x)$ is also even. The term $x$ is odd. The total integrand is an odd function integrated over a symmetric domain $(-\\infty, \\infty)$, so the integral is zero. Hence, $c_n=0$.\n3. $c_{n-1}$: $c_{n-1} = \\frac{\\langle x \\mathrm{He}_n, \\mathrm{He}_{n-1} \\rangle}{\\|\\mathrm{He}_{n-1}\\|^2} = \\frac{\\langle \\mathrm{He}_n, x \\mathrm{He}_{n-1} \\rangle}{\\|\\mathrm{He}_{n-1}\\|^2}$. We know $x \\mathrm{He}_{n-1}(x) = \\mathrm{He}_n(x) + c_{n-2} \\mathrm{He}_{n-2}(x) + \\dots$. Using orthogonality: $\\langle \\mathrm{He}_n, x \\mathrm{He}_{n-1} \\rangle = \\langle \\mathrm{He}_n, \\mathrm{He}_n(x) \\rangle = \\|\\mathrm{He}_n\\|^2 = n!$.\nSo, $c_{n-1} = \\frac{n!}{\\|\\mathrm{He}_{n-1}\\|^2} = \\frac{n!}{(n-1)!} = n$.\n\nThe recurrence relation for the orthogonal polynomials $\\mathrm{He}_n(x)$ is:\n$$\nx \\mathrm{He}_n(x) = \\mathrm{He}_{n+1}(x) + n \\mathrm{He}_{n-1}(x)\n$$\nNow, substitute $\\mathrm{He}_k(x) = \\sqrt{k!} \\psi_k(x)$ into this relation:\n$$\nx \\sqrt{n!} \\psi_n(x) = \\sqrt{(n+1)!} \\psi_{n+1}(x) + n \\sqrt{(n-1)!} \\psi_{n-1}(x)\n$$\nDivide the entire equation by $\\sqrt{n!}$:\n$$\nx \\psi_n(x) = \\frac{\\sqrt{(n+1)!}}{\\sqrt{n!}} \\psi_{n+1}(x) + \\frac{n\\sqrt{(n-1)!}}{\\sqrt{n!}} \\psi_{n-1}(x)\n$$\n$$\nx \\psi_n(x) = \\sqrt{n+1} \\psi_{n+1}(x) + \\frac{n}{\\sqrt{n}} \\psi_{n-1}(x)\n$$\n$$\nx \\psi_n(x) = \\sqrt{n+1} \\psi_{n+1}(x) + \\sqrt{n} \\psi_{n-1}(x)\n$$\nWe compare this result with the target form $x\\,\\psi_{n}(x) = a_{n+1}\\,\\psi_{n+1}(x) + b_{n}\\,\\psi_{n}(x) + a_{n}\\,\\psi_{n-1}(x)$.\n- The coefficient of $\\psi_{n+1}(x)$ gives $a_{n+1} = \\sqrt{n+1}$. This implies $a_k = \\sqrt{k}$ for $k \\ge 1$.\n- The coefficient of $\\psi_n(x)$ is zero, so $b_n = 0$ for all $n \\ge 0$.\n- The coefficient of $\\psi_{n-1}(x)$ gives $a_n = \\sqrt{n}$ for $n \\ge 1$.\nFor $n=0$, the relation is $x\\psi_0(x) = a_1 \\psi_1(x) + b_0 \\psi_0(x)$ given $\\psi_{-1}=0$. Our derived relation for $n=0$ is $x\\psi_0(x) = \\sqrt{1}\\psi_1(x) + \\sqrt{0}\\psi_{-1}(x) = \\psi_1(x)$, which implies $a_1=1$ and $b_0=0$. This is consistent with our general formulas. The formula $a_n=\\sqrt{n}$ can be extended to $n=0$, where $a_0=0$. With the convention $\\psi_{-1}=0$, the term $a_0 \\psi_{-1}(x)$ vanishes, making the value of $a_0$ arbitrary, but $a_0=0$ is the consistent choice.\n\nThus, the recurrence coefficients are $a_n = \\sqrt{n}$ and $b_n = 0$ for $n \\ge 0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{n}  0\n\\end{pmatrix}\n}\n$$", "id": "2671707"}, {"introduction": "Having established the key properties of the polynomial basis, this exercise applies them to a practical scenario in solid mechanics. You will construct the global system of equations for an intrusive Stochastic Galerkin Method applied to a linear elastic problem with uncertain material properties. The goal is to demonstrate how the combination of an affine parameterization and the properties of the PCE basis, particularly the recurrence relation you explored previously, leads to a highly structured global stiffness matrix that can be expressed as a sum of Kronecker products [@problem_id:2671679]. This structure is critical for developing efficient numerical solvers.", "problem": "A small-strain, static, linear elastic body occupies a bounded domain $\\mathcal{D} \\subset \\mathbb{R}^{d}$ with boundary $\\partial \\mathcal{D} = \\Gamma_{D} \\cup \\Gamma_{N}$, where $\\Gamma_{D}$ and $\\Gamma_{N}$ are disjoint Dirichlet and Neumann boundaries. The displacement field is $u: \\mathcal{D} \\times \\Omega \\to \\mathbb{R}^{d}$, where $(\\Omega,\\mathcal{F},\\mathbb{P})$ is a complete probability space supporting a random vector of independent inputs $\\boldsymbol{\\xi} = (\\xi_{1},\\dots,\\xi_{r})$. The equilibrium equations and boundary conditions are\n$$\n-\\nabla \\cdot \\sigma(x,\\boldsymbol{\\xi}) = f(x) \\quad \\text{in } \\mathcal{D}, \n\\qquad \nu(x,\\boldsymbol{\\xi}) = 0 \\quad \\text{on } \\Gamma_{D},\n\\qquad \n\\sigma(x,\\boldsymbol{\\xi}) \\cdot n = t(x) \\quad \\text{on } \\Gamma_{N},\n$$\nwith small-strain tensor $\\varepsilon(u) = \\tfrac{1}{2}(\\nabla u + \\nabla u^{\\top})$ and stress $\\sigma = \\mathbb{C}(x,\\boldsymbol{\\xi}):\\varepsilon(u)$. Assume an affine-parametric constitutive law\n$$\n\\mathbb{C}(x,\\boldsymbol{\\xi}) = \\sum_{q=0}^{Q} \\theta_{q}(\\boldsymbol{\\xi})\\,\\mathbb{C}_{q}(x),\n$$\nwhere $\\theta_{0}(\\boldsymbol{\\xi}) \\equiv 1$ and $\\{\\theta_{q}\\}_{q=1}^{Q}$ are real-valued functions of $\\boldsymbol{\\xi}$. A conforming finite element discretization with $n_{u}$ displacement degrees of freedom yields a parametric stiffness operator of the form\n$$\nK(\\boldsymbol{\\xi}) = \\sum_{q=0}^{Q} \\theta_{q}(\\boldsymbol{\\xi})\\,K_{q}, \\qquad K_{q} \\in \\mathbb{R}^{n_{u}\\times n_{u}},\n$$\nand a deterministic load vector $f \\in \\mathbb{R}^{n_{u}}$ assembling body forces and Neumann tractions.\n\nLet $\\{\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\}_{\\alpha \\in \\mathcal{A}}$ be an orthonormal multivariate polynomial basis in $L^{2}(\\Omega)$, with multi-index set $\\mathcal{A}$ of cardinality $n_{\\psi}$. Consider the Polynomial Chaos Expansion (PCE) of the discrete displacement,\n$$\nu_{h}(\\boldsymbol{\\xi}) = \\sum_{\\alpha \\in \\mathcal{A}} \\hat{u}_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}), \n\\qquad \\hat{u}_{\\alpha} \\in \\mathbb{R}^{n_{u}}.\n$$\nApplying a stochastic Galerkin projection with test functions $\\Psi_{\\beta}$ for all $\\beta \\in \\mathcal{A}$ produces a coupled linear system for the coefficient vector $\\hat{u} \\in \\mathbb{R}^{n_{u} n_{\\psi}}$ obtained by stacking $\\{\\hat{u}_{\\alpha}\\}$.\n\nStarting from the weak form of equilibrium, the constitutive law, and the orthonormality of $\\{\\Psi_{\\alpha}\\}$, derive the algebraic global Galerkin system and show that it admits a Kronecker product representation involving the deterministic stiffness matrices $\\{K_{q}\\}$ and stochastic coupling matrices defined by triple-product expectations. Then, specialize to the case where $\\boldsymbol{\\xi}$ has independent standard Gaussian components, $\\theta_{0}(\\boldsymbol{\\xi}) \\equiv 1$, and $\\theta_{k}(\\boldsymbol{\\xi})=\\xi_{k}$ for $k=1,\\dots,r$, and the basis $\\{\\Psi_{\\alpha}\\}$ is the orthonormal multivariate Hermite basis up to a fixed total degree. Use only fundamental laws of solid mechanics, finite element Galerkin principles, and well-tested facts about orthonormal Hermite polynomials.\n\nFinally, provide the closed-form analytic expression, in Kronecker product form, for the global Galerkin matrix in this Gaussian-affine specialization, with an explicit entry-wise formula for each stochastic coupling matrix appearing in the Kronecker representation. Your final answer must be a single closed-form analytic expression. Do not include units. No rounding is required.", "solution": "The problem statement presented is a well-posed and scientifically grounded problem in the field of computational solid mechanics, specifically concerning the application of Polynomial Chaos Expansion (PCE) within a stochastic finite element method (SFEM) framework. The problem is self-contained, its premises are factually sound, and its objectives are clearly defined. It requires the derivation of a standard but non-trivial result, proceeding from fundamental principles. Therefore, the problem is valid, and a solution will be provided.\n\nThe starting point is the discretized linear elastic system, which, accounting for the stochastic nature of the material properties, takes the form of a parametric system of linear equations:\n$$\nK(\\boldsymbol{\\xi}) u_{h}(\\boldsymbol{\\xi}) = f\n$$\nHere, $u_{h}(\\boldsymbol{\\xi}) \\in \\mathbb{R}^{n_{u}}$ is the vector of nodal displacement degrees of freedom, which is a random vector due to its dependence on $\\boldsymbol{\\xi}$. The vector $f \\in \\mathbb{R}^{n_{u}}$ represents the deterministic external and body forces. The stiffness matrix $K(\\boldsymbol{\\xi}) \\in \\mathbb{R}^{n_{u} \\times n_{u}}$ is random and is given by the affine expansion:\n$$\nK(\\boldsymbol{\\xi}) = \\sum_{q=0}^{Q} \\theta_{q}(\\boldsymbol{\\xi})\\,K_{q}\n$$\nThe displacement vector $u_{h}(\\boldsymbol{\\xi})$ is approximated using a Polynomial Chaos Expansion:\n$$\nu_{h}(\\boldsymbol{\\xi}) = \\sum_{\\alpha \\in \\mathcal{A}} \\hat{u}_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nwhere $\\{\\Psi_{\\alpha}\\}_{\\alpha \\in \\mathcal{A}}$ is a basis of orthonormal polynomials in $L^{2}(\\Omega)$, and $\\{\\hat{u}_{\\alpha}\\}_{\\alpha \\in \\mathcal{A}}$ are the deterministic coefficient vectors to be determined.\n\nSubstituting these expansions into the static equilibrium equation yields:\n$$\n\\left( \\sum_{q=0}^{Q} \\theta_{q}(\\boldsymbol{\\xi})\\,K_{q} \\right) \\left( \\sum_{\\alpha \\in \\mathcal{A}} \\hat{u}_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\right) = f\n$$\nTo solve for the unknown coefficients $\\hat{u}_{\\alpha}$, we apply a stochastic Galerkin projection. This involves projecting the residual of the equation onto the space spanned by the polynomial basis $\\{\\Psi_{\\beta}\\}_{\\beta \\in \\mathcal{A}}$. Operationally, this is achieved by taking the inner product of the equation with each basis function $\\Psi_{\\beta}$ for all $\\beta \\in \\mathcal{A}$. The inner product for real-valued random variables on the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is the expectation operator $\\mathbb{E}[\\cdot]$.\nFor each $\\beta \\in \\mathcal{A}$, the projection is:\n$$\n\\mathbb{E}\\left[ \\left( \\sum_{q=0}^{Q} \\theta_{q}(\\boldsymbol{\\xi})\\,K_{q} \\right) \\left( \\sum_{\\alpha \\in \\mathcal{A}} \\hat{u}_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\right) \\Psi_{\\beta}(\\boldsymbol{\\xi}) \\right] = \\mathbb{E}\\left[ f \\Psi_{\\beta}(\\boldsymbol{\\xi}) \\right]\n$$\nThe deterministic quantities $K_{q}$, $\\hat{u}_{\\alpha}$, and $f$ can be moved outside the expectation operator due to its linearity:\n$$\n\\sum_{q=0}^{Q} \\sum_{\\alpha \\in \\mathcal{A}} K_{q} \\hat{u}_{\\alpha} \\mathbb{E}\\left[ \\theta_{q}(\\boldsymbol{\\xi}) \\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\Psi_{\\beta}(\\boldsymbol{\\xi}) \\right] = f \\mathbb{E}\\left[ \\Psi_{\\beta}(\\boldsymbol{\\xi}) \\right]\n$$\nLet us analyze the terms. The right-hand side involves $\\mathbb{E}[\\Psi_{\\beta}(\\boldsymbol{\\xi})]$. The polynomial basis includes the constant function $\\Psi_{\\mathbf{0}}(\\boldsymbol{\\xi})=1$ corresponding to the zero multi-index $\\mathbf{0}$. By orthonormality, $\\mathbb{E}[\\Psi_{\\beta}\\Psi_{\\mathbf{0}}] = \\delta_{\\beta\\mathbf{0}}$. Thus, $\\mathbb{E}[\\Psi_{\\beta}] = \\delta_{\\beta\\mathbf{0}}$. The right-hand side simplifies to $f \\delta_{\\beta\\mathbf{0}}$.\nThe left-hand side contains the triple-product expectation coefficients, which form the entries of the stochastic coupling matrices. Let us define:\n$$\nC_{q, \\beta\\alpha} = \\mathbb{E}\\left[ \\theta_{q}(\\boldsymbol{\\xi}) \\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\Psi_{\\beta}(\\boldsymbol{\\xi}) \\right]\n$$\nThe system of equations for the coefficients $\\{\\hat{u}_{\\alpha}\\}$ becomes:\n$$\n\\sum_{q=0}^{Q} \\sum_{\\alpha \\in \\mathcal{A}} C_{q, \\beta\\alpha} K_{q} \\hat{u}_{\\alpha} = f \\delta_{\\beta\\mathbf{0}} \\quad \\text{for each } \\beta \\in \\mathcal{A}\n$$\nThis is a large, coupled system of linear equations for the stacked vector of coefficients $\\hat{\\mathbf{u}} = (\\dots, \\hat{u}_{\\alpha}^{\\top}, \\dots)^{\\top}$. The global Galerkin system can be written as $\\mathbf{K} \\hat{\\mathbf{u}} = \\hat{\\mathbf{f}}$. The block of the matrix $\\mathbf{K}$ corresponding to indices $(\\beta, \\alpha)$ is $\\mathbf{K}_{\\beta\\alpha} = \\sum_{q=0}^{Q} C_{q, \\beta\\alpha} K_{q}$. The global right-hand side vector $\\hat{\\mathbf{f}}$ has only one non-zero block, corresponding to $\\beta=\\mathbf{0}$, which is $f$.\nThis block structure is captured compactly using the Kronecker product ($\\otimes$). Let $\\mathbf{C}_q$ be the $n_{\\psi} \\times n_{\\psi}$ matrix with entries $(\\mathbf{C}_q)_{\\beta\\alpha} = C_{q, \\beta\\alpha}$. The global Galerkin matrix $\\mathbf{K}$ is then:\n$$\n\\mathbf{K} = \\sum_{q=0}^{Q} \\mathbf{C}_{q} \\otimes K_{q}\n$$\nThis is the general Kronecker product representation of the global Galerkin system.\n\nNext, we specialize to the case where $\\boldsymbol{\\xi} = (\\xi_{1}, \\dots, \\xi_{r})$ consists of independent standard Gaussian random variables, $\\xi_{k} \\sim \\mathcal{N}(0,1)$. The constitutive law is specified by $\\theta_{0}(\\boldsymbol{\\xi}) = 1$ and $\\theta_{k}(\\boldsymbol{\\xi})=\\xi_{k}$ for $k=1,\\dots,r$. This implies $Q=r$. The polynomial basis $\\{\\Psi_{\\alpha}\\}$ is the orthonormal multivariate Hermite basis, where $\\Psi_{\\alpha}(\\boldsymbol{\\xi}) = \\prod_{i=1}^{r} \\psi_{\\alpha_i}(\\xi_i)$, and $\\{\\psi_j\\}_{j \\ge 0}$ are the univariate orthonormal Hermite polynomials.\n\nWe must compute the matrices $\\mathbf{C}_q$ for this specific case.\nFor $q=0$, we have $\\theta_{0}(\\boldsymbol{\\xi}) = 1$. The entries are:\n$$\n(\\mathbf{C}_0)_{\\beta\\alpha} = \\mathbb{E}[1 \\cdot \\Psi_{\\alpha}(\\boldsymbol{\\xi})\\Psi_{\\beta}(\\boldsymbol{\\xi})] = \\delta_{\\alpha\\beta}\n$$\nby the orthonormality of the basis. Thus, $\\mathbf{C}_0 = \\mathbf{I}$, the identity matrix of size $n_{\\psi} \\times n_{\\psi}$.\n\nFor $q=k \\in \\{1,\\dots,r\\}$, we have $\\theta_k(\\boldsymbol{\\xi}) = \\xi_k$. The entries are:\n$$\n(\\mathbf{C}_k)_{\\beta\\alpha} = \\mathbb{E}[\\xi_k \\Psi_{\\alpha}(\\boldsymbol{\\xi})\\Psi_{\\beta}(\\boldsymbol{\\xi})] = \\mathbb{E}\\left[\\xi_k \\prod_{i=1}^{r} \\psi_{\\alpha_i}(\\xi_i) \\psi_{\\beta_i}(\\xi_i)\\right]\n$$\nBecause the variables $\\xi_i$ are independent, the expectation of the product separates into a product of expectations:\n$$\n(\\mathbf{C}_k)_{\\beta\\alpha} = \\mathbb{E}[\\xi_k \\psi_{\\alpha_k}(\\xi_k)\\psi_{\\beta_k}(\\xi_k)] \\prod_{\\substack{i=1 \\\\ i \\neq k}}^{r} \\mathbb{E}[\\psi_{\\alpha_i}(\\xi_i)\\psi_{\\beta_i}(\\xi_i)]\n$$\nThe product term simplifies due to orthonormality: $\\prod_{i \\neq k} \\delta_{\\alpha_i \\beta_i}$. This implies that the entry is non-zero only if $\\alpha_i = \\beta_i$ for all $i \\neq k$.\nThe remaining expectation is evaluated using the three-term recurrence relation for orthonormal Hermite polynomials:\n$$\nz \\psi_p(z) = \\sqrt{p+1} \\psi_{p+1}(z) + \\sqrt{p} \\psi_{p-1}(z)\n$$\nApplying this to the term with $\\xi_k$ and $\\alpha_k$:\n$$\n\\mathbb{E}[\\xi_k \\psi_{\\alpha_k}(\\xi_k)\\psi_{\\beta_k}(\\xi_k)] = \\mathbb{E}\\left[ \\left(\\sqrt{\\alpha_k+1}\\psi_{\\alpha_k+1}(\\xi_k) + \\sqrt{\\alpha_k}\\psi_{\\alpha_k-1}(\\xi_k)\\right)\\psi_{\\beta_k}(\\xi_k) \\right]\n$$\nUsing the linearity of expectation and the orthonormality of the $\\psi_j$ polynomials:\n$$\n= \\sqrt{\\alpha_k+1}\\mathbb{E}[\\psi_{\\alpha_k+1}(\\xi_k)\\psi_{\\beta_k}(\\xi_k)] + \\sqrt{\\alpha_k}\\mathbb{E}[\\psi_{\\alpha_k-1}(\\xi_k)\\psi_{\\beta_k}(\\xi_k)] = \\sqrt{\\alpha_k+1}\\delta_{\\beta_k, \\alpha_k+1} + \\sqrt{\\alpha_k}\\delta_{\\beta_k, \\alpha_k-1}\n$$\nCombining these results, the entry-wise formula for the matrix $\\mathbf{C}_k$ is:\n$$\n(\\mathbf{C}_k)_{\\beta\\alpha} = \\left(\\sqrt{\\alpha_k+1}\\delta_{\\beta_k, \\alpha_k+1} + \\sqrt{\\alpha_k}\\delta_{\\beta_k, \\alpha_k-1}\\right) \\prod_{\\substack{i=1 \\\\ i \\neq k}}^{r} \\delta_{\\beta_i, \\alpha_i}\n$$\nThis formula shows that $\\mathbf{C}_k$ is a sparse matrix. For a given row $\\beta$, it has at most two non-zero entries corresponding to columns $\\alpha$ where the multi-index differs from $\\beta$ only in the $k$-th component, by $\\pm 1$.\n\nFinally, the global Galerkin matrix for the specified Gaussian-affine case is assembled:\n$$\n\\mathbf{K} = \\mathbf{C}_0 \\otimes K_0 + \\sum_{k=1}^{r} \\mathbf{C}_k \\otimes K_k = \\mathbf{I} \\otimes K_0 + \\sum_{k=1}^{r} \\mathbf{C}_k \\otimes K_k\n$$\nThis expression, together with the explicit formula for the entries of $\\mathbf{C}_k$, constitutes the complete solution.", "answer": "$$\n\\boxed{\\mathbf{K} = \\mathbf{I} \\otimes K_0 + \\sum_{k=1}^{r} \\mathbf{C}_k \\otimes K_k, \\text{ with } (\\mathbf{C}_k)_{\\beta\\alpha} = \\left(\\sqrt{\\alpha_k+1}\\delta_{\\beta_k, \\alpha_k+1} + \\sqrt{\\alpha_k}\\delta_{\\beta_k, \\alpha_k-1}\\right) \\prod_{\\substack{i=1 \\\\ i \\neq k}}^{r} \\delta_{\\beta_i, \\alpha_i}}\n$$", "id": "2671679"}, {"introduction": "Once a Polynomial Chaos Expansion for a quantity of interest has been constructed, its true value lies in the rich information it provides about the model's behavior under uncertainty. This final practice demonstrates one of the most powerful post-processing capabilities of PCE: variance-based sensitivity analysis. You will derive the direct link between the PCE coefficients and the Sobol' sensitivity indices, revealing how the expansion naturally decomposes the output variance and allows us to quantify the influence of each input parameter with remarkable ease [@problem_id:2671662].", "problem": "A straight, prismatic axial bar in linear elasticity is subjected to a deterministic axial load, producing a tip displacement denoted by $Y$. The uncertainty in the bar’s material and geometric parameters is modeled by two independent standardized inputs $\\boldsymbol{\\xi}=(\\xi_{1},\\xi_{2})$, and the response $Y$ is approximated by a Polynomial Chaos Expansion (PCE) constructed on an orthonormal tensor-product polynomial basis $\\{\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\}$ indexed by multi-indices $\\alpha=(\\alpha_{1},\\alpha_{2})\\in\\mathbb{N}_{0}^{2}$, such that $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}]=\\delta_{\\alpha\\beta}$ and $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for all $\\alpha\\neq (0,0)$. The resulting truncated PCE has the form\n$$\nY \\approx \\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\quad\\text{with}\\quad c_{\\alpha}\\in\\mathbb{R}.\n$$\nYou may assume the inputs are independent and that the orthonormal basis is consistent with their probability laws.\n\nTask 1. Starting from the definition of variance and the Analysis of Variance (ANOVA) decomposition underlying variance-based Sobol sensitivity indices, and using only the orthonormality of the polynomial chaos basis and independence of inputs, derive an expression for the first-order Sobol index $S_{i}$ of input $\\xi_{i}$ solely in terms of the PCE coefficients $\\{c_{\\alpha}\\}$.\n\nTask 2. For the specific two-dimensional expansion\n$$\nY \\approx c_{(0,0)}\\Psi_{(0,0)} + c_{(1,0)}\\Psi_{(1,0)} + c_{(0,1)}\\Psi_{(0,1)} + c_{(2,0)}\\Psi_{(2,0)} + c_{(0,2)}\\Psi_{(0,2)} + c_{(1,1)}\\Psi_{(1,1)},\n$$\nwith coefficients\n- $c_{(0,0)} = 1.0$,\n- $c_{(1,0)} = 0.8$,\n- $c_{(0,1)} = 0.6$,\n- $c_{(2,0)} = 0.3$,\n- $c_{(0,2)} = 0.1$,\n- $c_{(1,1)} = 0.4$,\ncompute the first-order Sobol index $S_{1}$ for $\\xi_{1}$ using your derived expression.\n\nRound your final numeric answer to four significant figures. Express the result as a pure decimal (unitless).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard task in the field of uncertainty quantification, specifically connecting Polynomial Chaos Expansion (PCE) with variance-based sensitivity analysis (Sobol indices). All necessary definitions, such as the orthonormality of the basis and independence of the inputs, are provided. The numerical values are consistent. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is divided into a derivation and a subsequent calculation.\n\nTask 1: Derivation of the First-Order Sobol Index $S_i$\n\nThe analysis begins from the fundamental definitions of variance and the Sobol indices. The total variance of the model output $Y$ is denoted by $D = \\text{Var}(Y)$. The first-order Sobol index for an input parameter $\\xi_i$ is defined as the fraction of the total variance contributed by $\\xi_i$ alone:\n$$\nS_i = \\frac{D_i}{D}\n$$\nwhere $D_i = \\text{Var}(\\mathbb{E}[Y | \\xi_i])$ is the first-order partial variance associated with $\\xi_i$.\n\nWe must express both $D$ and $D_i$ in terms of the PCE coefficients $\\{c_{\\alpha}\\}$. The PCE of the response $Y$ is given as:\n$$\nY(\\boldsymbol{\\xi}) \\approx \\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nThe basis functions $\\{\\Psi_{\\alpha}\\}$ are orthonormal with respect to the probability measure of the inputs $\\boldsymbol{\\xi}$, meaning $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$. Also, $\\mathbb{E}[\\Psi_{\\alpha}] = 0$ for $\\alpha \\neq \\mathbf{0}$, and $\\Psi_{\\mathbf{0}}$ is a constant, which we normalize to $\\Psi_{\\mathbf{0}} = 1$.\n\nFirst, we compute the mean and total variance of $Y$. The mean of $Y$ is:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{\\alpha} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\right] = \\sum_{\\alpha} c_{\\alpha}\\,\\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})]\n$$\nDue to the property $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for $\\alpha\\neq\\mathbf{0}$ and $\\mathbb{E}[\\Psi_{\\mathbf{0}}]=1$, the sum reduces to a single term:\n$$\n\\mathbb{E}[Y] = c_{\\mathbf{0}}\\,\\mathbb{E}[\\Psi_{\\mathbf{0}}] = c_{\\mathbf{0}}\n$$\nThe total variance $D$ is then:\n$$\nD = \\text{Var}(Y) = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2] = \\mathbb{E}\\left[\\left(\\sum_{\\alpha} c_{\\alpha}\\Psi_{\\alpha} - c_{\\mathbf{0}}\\right)^2\\right]\n$$\nSubstituting $c_{\\mathbf{0}} = c_{\\mathbf{0}}\\Psi_{\\mathbf{0}}$:\n$$\nD = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}\\Psi_{\\alpha}\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}\\Psi_{\\alpha}\\right) \\left(\\sum_{\\beta \\neq \\mathbf{0}} c_{\\beta}\\Psi_{\\beta}\\right)\\right]\n$$\nExpanding the product and applying the expectation operator:\n$$\nD = \\sum_{\\alpha \\neq \\mathbf{0}} \\sum_{\\beta \\neq \\mathbf{0}} c_{\\alpha} c_{\\beta} \\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}]\n$$\nBy orthonormality, $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$, so the expression simplifies to:\n$$\nD = \\sum_{\\alpha \\neq \\mathbf{0}} c_{\\alpha}^2\n$$\nThis is a fundamental result: the total variance of the PCE model is the sum of the squares of all non-constant coefficients.\n\nNext, we compute the partial variance $D_i$. We begin by evaluating the conditional expectation $\\mathbb{E}[Y | \\xi_i]$. The basis functions are of a tensor-product form, $\\Psi_{\\alpha}(\\boldsymbol{\\xi}) = \\prod_{k=1}^{d} \\psi_{\\alpha_k}(\\xi_k)$, where $d$ is the dimension of $\\boldsymbol{\\xi}$.\n$$\n\\mathbb{E}[Y | \\xi_i] = \\mathbb{E}\\left[\\sum_{\\alpha} c_{\\alpha} \\prod_{k=1}^{d} \\psi_{\\alpha_k}(\\xi_k) \\Bigg| \\xi_i \\right]\n$$\nBecause the inputs $\\{\\xi_k\\}$ are independent, the expectation with respect to $\\boldsymbol{\\xi}_{\\sim i}$ (all variables except $\\xi_i$) can be taken inside the sum and product:\n$$\n\\mathbb{E}[Y | \\xi_i] = \\sum_{\\alpha} c_{\\alpha} \\psi_{\\alpha_i}(\\xi_i) \\prod_{k \\neq i} \\mathbb{E}[\\psi_{\\alpha_k}(\\xi_k)]\n$$\nThe univariate polynomials are also orthonormal, implying $\\mathbb{E}[\\psi_{j}(\\xi_k)] = \\delta_{j0}$. The product term $\\prod_{k \\neq i} \\mathbb{E}[\\psi_{\\alpha_k}(\\xi_k)]$ is therefore equal to $1$ if and only if $\\alpha_k = 0$ for all $k \\neq i$, and it is $0$ otherwise. This restricts the sum to multi-indices $\\alpha$ that have non-zero components only at the $i$-th position (plus the zero vector $\\mathbf{0}$).\nLet's define the set of such indices as $\\mathcal{I}_i^* = \\{\\alpha \\in \\mathbb{N}_0^d \\mid \\alpha_j = 0 \\text{ for all } j \\neq i\\}$. The conditional expectation becomes:\n$$\n\\mathbb{E}[Y | \\xi_i] = \\sum_{\\alpha \\in \\mathcal{I}_i^*} c_{\\alpha} \\Psi_{\\alpha}(\\boldsymbol{\\xi}) = c_{\\mathbf{0}}\\Psi_{\\mathbf{0}} + \\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nwhere $\\mathcal{I}_i = \\{\\alpha \\in \\mathbb{N}_0^d \\mid \\alpha_i  0 \\text{ and } \\alpha_j = 0 \\text{ for all } j \\neq i\\}$. The functions $\\Psi_{\\alpha}$ for $\\alpha \\in \\mathcal{I}_i$ depend only on $\\xi_i$.\n\nNow we compute the variance of this quantity, $D_i = \\text{Var}(\\mathbb{E}[Y | \\xi_i])$. The constant term $c_{\\mathbf{0}}\\Psi_{\\mathbf{0}}$ does not contribute to the variance.\n$$\nD_i = \\text{Var}\\left(\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right) = \\mathbb{E}\\left[\\left(\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right)^2\\right] - \\left(\\mathbb{E}\\left[\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha} \\Psi_{\\alpha}\\right]\\right)^2\n$$\nThe second term is zero because $\\mathbb{E}[\\Psi_{\\alpha}]=0$ for all $\\alpha \\in \\mathcal{I}_i$. Using the same logic as for the total variance $D$, the orthonormality of the basis functions leads to:\n$$\nD_i = \\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha}^2\n$$\nThis demonstrates that the partial variance due to $\\xi_i$ is the sum of the squares of the coefficients corresponding to basis functions that depend only on $\\xi_i$.\n\nFinally, the first-order Sobol index $S_i$ is the ratio of the partial variance $D_i$ to the total variance $D$:\n$$\nS_i = \\frac{D_i}{D} = \\frac{\\sum_{\\alpha \\in \\mathcal{I}_i} c_{\\alpha}^2}{\\sum_{\\beta \\neq \\mathbf{0}} c_{\\beta}^2}\n$$\nThis is the required expression.\n\nTask 2: Calculation of $S_1$\n\nFor the specific two-dimensional case, $\\boldsymbol{\\xi}=(\\xi_1, \\xi_2)$, the PCE is given as:\n$$\nY \\approx c_{(0,0)}\\Psi_{(0,0)} + c_{(1,0)}\\Psi_{(1,0)} + c_{(0,1)}\\Psi_{(0,1)} + c_{(2,0)}\\Psi_{(2,0)} + c_{(0,2)}\\Psi_{(0,2)} + c_{(1,1)}\\Psi_{(1,1)}\n$$\nThe provided coefficients are:\n$c_{(0,0)} = 1.0$, $c_{(1,0)} = 0.8$, $c_{(0,1)} = 0.6$, $c_{(2,0)} = 0.3$, $c_{(0,2)} = 0.1$, $c_{(1,1)} = 0.4$.\n\nWe must calculate $S_1$. The denominator is the total variance $D$, which is the sum of squares of all coefficients except $c_{(0,0)}$:\n$$\nD = c_{(1,0)}^2 + c_{(0,1)}^2 + c_{(2,0)}^2 + c_{(0,2)}^2 + c_{(1,1)}^2\n$$\nSubstituting the values:\n$$\nD = (0.8)^2 + (0.6)^2 + (0.3)^2 + (0.1)^2 + (0.4)^2\n$$\n$$\nD = 0.64 + 0.36 + 0.09 + 0.01 + 0.16 = 1.26\n$$\nThe numerator is the partial variance $D_1$. According to our derived formula, this is the sum of squares of coefficients whose multi-indices are in $\\mathcal{I}_1$. For this problem, $\\mathcal{I}_1$ consists of indices of the form $(\\alpha_1, 0)$ with $\\alpha_1  0$. The relevant indices from the given expansion are $(1,0)$ and $(2,0)$.\n$$\nD_1 = c_{(1,0)}^2 + c_{(2,0)}^2\n$$\nSubstituting the values:\n$$\nD_1 = (0.8)^2 + (0.3)^2 = 0.64 + 0.09 = 0.73\n$$\nThe first-order Sobol index for $\\xi_1$ is therefore:\n$$\nS_1 = \\frac{D_1}{D} = \\frac{0.73}{1.26} \\approx 0.579365079...\n$$\nRounding to four significant figures, we obtain $S_1 = 0.5794$.", "answer": "$$\n\\boxed{0.5794}\n$$", "id": "2671662"}]}