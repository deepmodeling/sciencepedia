{"hands_on_practices": [{"introduction": "The power of the finite element method is built upon the geometric structure of Hilbert spaces, which are vector spaces equipped with an inner product. This exercise will have you explore the fundamental property that separates these spaces from more general normed spaces: the parallelogram law. By testing a custom-defined norm, you will gain a hands-on appreciation for the strict conditions a norm must satisfy to be induced by an inner product, the very structure that enables concepts like orthogonality and projection.", "problem": "Consider a conforming finite element subspace $V_h \\subset H_0^1(0,1)$ of dimension $2$ with basis functions $\\{\\varphi_1,\\varphi_2\\}$ chosen to be orthonormal with respect to the energy inner product associated with the Poisson operator, that is, $\\langle u,v \\rangle_E := \\int_0^1 u'(x)\\,v'(x)\\,\\mathrm{d}x$. For a coefficient vector $c = (c_1,c_2)^{\\top} \\in \\mathbb{R}^2$ associated with $u_h = c_1 \\varphi_1 + c_2 \\varphi_2$, the discrete energy norm equals the Euclidean norm of $c$, i.e., $\\|u_h\\|_E = \\|c\\|_2$. For a fixed parameter $\\gamma \\ge 0$, define on $\\mathbb{R}^2$ the norm\n$$\n\\|c\\|_{\\gamma} := \\|c\\|_2 + \\gamma \\|c\\|_1,\n$$\nwhere $\\|c\\|_2 := \\sqrt{c_1^2 + c_2^2}$ and $\\|c\\|_1 := |c_1| + |c_2|$. Using only the fundamental definitions of norms and inner products and the characterization that a norm is induced by an inner product if and only if it satisfies the parallelogram law, determine the unique value of $\\gamma \\ge 0$ (if any) for which $\\|\\cdot\\|_{\\gamma}$ is induced by an inner product on $\\mathbb{R}^2$. Additionally, construct an explicit counterexample demonstrating that the $1$-norm $\\|\\cdot\\|_1$ on $\\mathbb{R}^2$ does not satisfy the parallelogram identity. Express your final answer as the required value of $\\gamma$ (no units).", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It is rooted in the established mathematical theory of normed vector spaces. All definitions are provided, and the problem is self-contained and consistent. Therefore, a solution will be provided.\n\nThe initial context relating to the finite element method establishes that the Euclidean norm $\\|c\\|_2$ of the coefficient vector $c \\in \\mathbb{R}^2$ is physically meaningful, as it represents the energy norm $\\|u_h\\|_E$ of the corresponding function $u_h \\in V_h$. The core of the problem, however, is to determine for which value of the parameter $\\gamma \\ge 0$ the modified norm $\\|c\\|_{\\gamma} := \\|c\\|_2 + \\gamma \\|c\\|_1$ is induced by an inner product on $\\mathbb{R}^2$.\n\nAccording to the Jordan-von Neumann theorem, a norm on a vector space is induced by an inner product if and only if it satisfies the parallelogram law. For the norm $\\|\\cdot\\|_{\\gamma}$ on the vector space $\\mathbb{R}^2$, this identity is:\n$$\n\\|c+d\\|_{\\gamma}^2 + \\|c-d\\|_{\\gamma}^2 = 2\\left(\\|c\\|_{\\gamma}^2 + \\|d\\|_{\\gamma}^2\\right)\n$$\nThis must hold for all vectors $c, d \\in \\mathbb{R}^2$. To find a necessary condition on $\\gamma$, we can test this identity with a specific choice of vectors. Let us select the standard basis vectors $c = e_1 = (1, 0)^{\\top}$ and $d = e_2 = (0, 1)^{\\top}$.\n\nFirst, we compute the component norms for these vectors and their sum and difference.\nFor $c = e_1$:\n$\\|c\\|_2 = \\|e_1\\|_2 = \\sqrt{1^2 + 0^2} = 1$\n$\\|c\\|_1 = \\|e_1\\|_1 = |1| + |0| = 1$\n\nFor $d = e_2$:\n$\\|d\\|_2 = \\|e_2\\|_2 = \\sqrt{0^2 + 1^2} = 1$\n$\\|d\\|_1 = \\|e_2\\|_1 = |0| + |1| = 1$\n\nFor $c+d = (1, 1)^{\\top}$:\n$\\|c+d\\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}$\n$\\|c+d\\|_1 = |1| + |1| = 2$\n\nFor $c-d = (1, -1)^{\\top}$:\n$\\|c-d\\|_2 = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}$\n$\\|c-d\\|_1 = |1| + |-1| = 2$\n\nNow, we express the $\\|\\cdot\\|_{\\gamma}$ norms:\n$\\|c\\|_{\\gamma} = \\|c\\|_2 + \\gamma \\|c\\|_1 = 1 + \\gamma$\n$\\|d\\|_{\\gamma} = \\|d\\|_2 + \\gamma \\|d\\|_1 = 1 + \\gamma$\n$\\|c+d\\|_{\\gamma} = \\|c+d\\|_2 + \\gamma \\|c+d\\|_1 = \\sqrt{2} + 2\\gamma$\n$\\|c-d\\|_{\\gamma} = \\|c-d\\|_2 + \\gamma \\|c-d\\|_1 = \\sqrt{2} + 2\\gamma$\n\nSubstitute these into the parallelogram law:\nLeft-hand side (LHS):\n$$\n\\|c+d\\|_{\\gamma}^2 + \\|c-d\\|_{\\gamma}^2 = (\\sqrt{2} + 2\\gamma)^2 + (\\sqrt{2} + 2\\gamma)^2 = 2(\\sqrt{2} + 2\\gamma)^2\n$$\nRight-hand side (RHS):\n$$\n2\\left(\\|c\\|_{\\gamma}^2 + \\|d\\|_{\\gamma}^2\\right) = 2\\left((1+\\gamma)^2 + (1+\\gamma)^2\\right) = 4(1+\\gamma)^2\n$$\nEquating LHS and RHS gives:\n$$\n2(\\sqrt{2} + 2\\gamma)^2 = 4(1+\\gamma)^2\n$$\n$$\n(\\sqrt{2} + 2\\gamma)^2 = 2(1+\\gamma)^2\n$$\nExpanding both sides:\n$$\n2 + 4\\sqrt{2}\\gamma + 4\\gamma^2 = 2(1 + 2\\gamma + \\gamma^2) = 2 + 4\\gamma + 2\\gamma^2\n$$\nRearranging the terms to form an equation for $\\gamma$:\n$$\n(4-2)\\gamma^2 + (4\\sqrt{2}-4)\\gamma + (2-2) = 0\n$$\n$$\n2\\gamma^2 + 4(\\sqrt{2}-1)\\gamma = 0\n$$\n$$\n2\\gamma \\left(\\gamma + 2(\\sqrt{2}-1)\\right) = 0\n$$\nThis equation yields two possible solutions for $\\gamma$: $\\gamma = 0$ or $\\gamma = -2(\\sqrt{2}-1) = 2 - 2\\sqrt{2}$.\nSince $\\sqrt{2} > 1$, the second solution $2 - 2\\sqrt{2}$ is negative. The problem specifies the constraint $\\gamma \\ge 0$. Therefore, the only possible value for $\\gamma$ is $0$.\n\nIf $\\gamma = 0$, the norm becomes $\\|c\\|_0 = \\|c\\|_2 + 0 \\cdot \\|c\\|_1 = \\|c\\|_2$. The norm is the standard Euclidean norm. It is a fundamental result that the Euclidean norm is induced by the standard dot product $\\langle c, d \\rangle = c_1 d_1 + c_2 d_2$. Indeed, for any $c, d \\in \\mathbb{R}^2$:\n$$\n\\|c+d\\|_2^2 + \\|c-d\\|_2^2 = \\langle c+d, c+d \\rangle + \\langle c-d, c-d \\rangle\n$$\n$$\n= (\\langle c,c \\rangle + 2\\langle c,d \\rangle + \\langle d,d \\rangle) + (\\langle c,c \\rangle - 2\\langle c,d \\rangle + \\langle d,d \\rangle)\n$$\n$$\n= 2\\langle c,c \\rangle + 2\\langle d,d \\rangle = 2\\|c\\|_2^2 + 2\\|d\\|_2^2 = 2\\left(\\|c\\|_2^2 + \\|d\\|_2^2\\right)\n$$\nThe parallelogram law holds for all $c, d \\in \\mathbb{R}^2$ when $\\gamma = 0$. Since our initial analysis showed that $\\gamma = 0$ is the only non-negative possibility, it must be the unique solution.\n\nThe second task is to demonstrate that the $1$-norm, $\\|\\cdot\\|_1$, does not satisfy a parallelogram identity. We use the same specific vectors, $c = e_1$ and $d = e_2$. The parallelogram law for $\\|\\cdot\\|_1$ is $\\|c+d\\|_1^2 + \\|c-d\\|_1^2 = 2(\\|c\\|_1^2 + \\|d\\|_1^2)$.\nWe have previously calculated the required $1$-norms:\n$\\|c\\|_1 = 1$\n$\\|d\\|_1 = 1$\n$\\|c+d\\|_1 = 2$\n$\\|c-d\\|_1 = 2$\n\nSubstituting these values into the parallelogram law for the $1$-norm:\nLHS: $\\|c+d\\|_1^2 + \\|c-d\\|_1^2 = 2^2 + 2^2 = 4 + 4 = 8$.\nRHS: $2(\\|c\\|_1^2 + \\|d\\|_1^2) = 2(1^2 + 1^2) = 2(1 + 1) = 4$.\nSince LHS $= 8 \\neq 4 =$ RHS, the parallelogram law is not satisfied for this choice of vectors. This provides the required explicit counterexample and proves that the $1$-norm on $\\mathbb{R}^2$ is not induced by an inner product.", "answer": "$$\n\\boxed{0}\n$$", "id": "2575281"}, {"introduction": "In functional analysis for FEM, we frequently work with different spaces, like $L^2(\\Omega)$ and $H^1(\\Omega)$, each defined by its own inner product. This practice illustrates a critical and sometimes counter-intuitive consequence: convergence in one space does not guarantee convergence in another. By analyzing a specific sequence of functions, you will see firsthand why the $H^1$ inner product, which accounts for derivatives, defines a stricter sense of convergence and why this distinction is vital for error analysis in solving differential equations.", "problem": "Let $\\Omega = (0,\\pi)$ and consider the Hilbert spaces $L^{2}(\\Omega)$ and $H^{1}_{0}(\\Omega)$ that arise in the finite element method (FEM). Define the sequence $\\{u_{n}\\}_{n\\in\\mathbb{N}} \\subset H^{1}_{0}(\\Omega)$ by $u_{n}(x) = \\dfrac{\\sin(n x)}{n}$ for $x \\in \\Omega$. Using only the canonical inner products and norms of $L^{2}(\\Omega)$ and $H^{1}(\\Omega)$ as the foundational starting point, do the following:\n- Compute $\\|u_{n}\\|_{L^{2}(\\Omega)}$, $\\|u_{n}'\\|_{L^{2}(\\Omega)}$, and $\\|u_{n}\\|_{H^{1}(\\Omega)}$ in closed form for each $n \\in \\mathbb{N}$.\n- Determine $\\lim_{n\\to\\infty}\\|u_{n}\\|_{L^{2}(\\Omega)}$ and $\\lim_{n\\to\\infty}\\|u_{n}\\|_{H^{1}(\\Omega)}$.\n- Explain, from the viewpoint of inner products, why convergence in $L^{2}(\\Omega)$ does not imply convergence in $H^{1}(\\Omega)$ for this sequence, making explicit the role of the derivative term in the $H^{1}(\\Omega)$ inner product in inducing a strictly stronger topology than that of $L^{2}(\\Omega)$.\n\nYour final answer must be the exact value (no rounding) of $\\lim_{n\\to\\infty}\\|u_{n}\\|_{H^{1}(\\Omega)}$ as a single closed-form expression without units.", "solution": "The problem presented is a standard exercise in functional analysis, specifically concerning the properties of Sobolev spaces. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a full solution will be provided.\n\nLet $\\Omega = (0, \\pi)$. The Hilbert spaces in question are $L^{2}(\\Omega)$ and $H^{1}_{0}(\\Omega)$. The canonical inner product on $L^{2}(\\Omega)$ for real-valued functions $u, v$ is given by\n$$ \\langle u, v \\rangle_{L^{2}} = \\int_{\\Omega} u(x)v(x) \\, dx $$\nThe induced norm is $\\|u\\|_{L^{2}(\\Omega)} = \\sqrt{\\langle u, u \\rangle_{L^{2}}}$.\n\nThe canonical inner product on the Sobolev space $H^{1}(\\Omega)$ is\n$$ \\langle u, v \\rangle_{H^{1}} = \\int_{\\Omega} (u(x)v(x) + u'(x)v'(x)) \\, dx = \\langle u, v \\rangle_{L^{2}} + \\langle u', v' \\rangle_{L^{2}} $$\nwhere $u'$ and $v'$ are the weak derivatives. The induced norm is $\\|u\\|_{H^{1}(\\Omega)} = \\sqrt{\\langle u, u \\rangle_{H^{1}}}$. The space $H^{1}_{0}(\\Omega)$ is the subspace of $H^{1}(\\Omega)$ consisting of functions with zero trace on the boundary $\\partial\\Omega$. The sequence is given by $u_{n}(x) = \\frac{\\sin(n x)}{n}$ for $n \\in \\mathbb{N}$. Since $u_{n}(0) = 0$ and $u_{n}(\\pi) = \\sin(n\\pi)/n = 0$, the condition for being in $H^{1}_{0}(\\Omega)$ is satisfied. The derivative is $u_{n}'(x) = \\cos(nx)$.\n\nFirst, we compute the norms as requested.\n\nThe squared $L^{2}$-norm of $u_{n}$ is:\n$$ \\|u_{n}\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} \\left( \\frac{\\sin(nx)}{n} \\right)^{2} dx = \\frac{1}{n^{2}} \\int_{0}^{\\pi} \\sin^{2}(nx) \\, dx $$\nUsing the trigonometric identity $\\sin^{2}(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$, the integral becomes:\n$$ \\int_{0}^{\\pi} \\sin^{2}(nx) \\, dx = \\int_{0}^{\\pi} \\frac{1 - \\cos(2nx)}{2} \\, dx = \\frac{1}{2} \\left[ x - \\frac{\\sin(2nx)}{2n} \\right]_{0}^{\\pi} = \\frac{1}{2} \\left( (\\pi - \\frac{\\sin(2n\\pi)}{2n}) - (0 - 0) \\right) = \\frac{\\pi}{2} $$\nThus, the squared norm is $\\|u_{n}\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2n^{2}}$. Taking the square root gives the norm:\n$$ \\|u_{n}\\|_{L^{2}(\\Omega)} = \\sqrt{\\frac{\\pi}{2n^{2}}} = \\frac{\\sqrt{\\pi}}{\\sqrt{2} n} $$\n\nNext, we compute the $L^{2}$-norm of the derivative, $u_{n}'(x) = \\cos(nx)$:\n$$ \\|u_{n}'\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} (\\cos(nx))^{2} dx = \\int_{0}^{\\pi} \\cos^{2}(nx) \\, dx $$\nUsing the identity $\\cos^{2}(\\theta) = \\frac{1 + \\cos(2\\theta)}{2}$, the integral becomes:\n$$ \\int_{0}^{\\pi} \\cos^{2}(nx) \\, dx = \\int_{0}^{\\pi} \\frac{1 + \\cos(2nx)}{2} \\, dx = \\frac{1}{2} \\left[ x + \\frac{\\sin(2nx)}{2n} \\right]_{0}^{\\pi} = \\frac{1}{2} \\left( (\\pi + \\frac{\\sin(2n\\pi)}{2n}) - (0 + 0) \\right) = \\frac{\\pi}{2} $$\nThe norm of the derivative is therefore constant for all $n \\in \\mathbb{N}$:\n$$ \\|u_{n}'\\|_{L^{2}(\\Omega)} = \\sqrt{\\frac{\\pi}{2}} $$\n\nNow, we compute the $H^{1}$-norm of $u_{n}$. By definition, the squared $H^{1}$-norm is the sum of the squared $L^{2}$-norms of the function and its derivative:\n$$ \\|u_{n}\\|_{H^{1}(\\Omega)}^{2} = \\|u_{n}\\|_{L^{2}(\\Omega)}^{2} + \\|u_{n}'\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2n^{2}} + \\frac{\\pi}{2} = \\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right) $$\nTaking the square root gives the norm:\n$$ \\|u_{n}\\|_{H^{1}(\\Omega)} = \\sqrt{\\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right)} $$\n\nSecond, we determine the limits as $n \\to \\infty$.\nFor the $L^{2}$-norm:\n$$ \\lim_{n\\to\\infty} \\|u_{n}\\|_{L^{2}(\\Omega)} = \\lim_{n\\to\\infty} \\frac{\\sqrt{\\pi}}{\\sqrt{2} n} = 0 $$\nFor the $H^{1}$-norm:\n$$ \\lim_{n\\to\\infty} \\|u_{n}\\|_{H^{1}(\\Omega)} = \\lim_{n\\to\\infty} \\sqrt{\\frac{\\pi}{2} \\left( 1 + \\frac{1}{n^{2}} \\right)} = \\sqrt{\\frac{\\pi}{2} \\left( 1 + 0 \\right)} = \\sqrt{\\frac{\\pi}{2}} $$\n\nThird, we explain the differing convergence behaviors.\nConvergence of a sequence $\\{v_{n}\\}$ to a limit $v$ in a normed space $(V, \\|\\cdot\\|_{V})$ is defined as $\\|v_{n} - v\\|_{V} \\to 0$ as $n \\to \\infty$.\nOur calculations show that $\\|u_{n} - 0\\|_{L^{2}(\\Omega)} \\to 0$. Therefore, the sequence $\\{u_{n}\\}$ converges to the zero function in the space $L^{2}(\\Omega)$.\nHowever, $\\|u_{n} - 0\\|_{H^{1}(\\Omega)} \\to \\sqrt{\\frac{\\pi}{2}} \\neq 0$. The sequence does not converge to the zero function in $H^{1}(\\Omega)$. In fact, the sequence does not converge at all in $H^{1}(\\Omega)$ because it is not a Cauchy sequence. For $n \\neq m$:\n$$ \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\int_{0}^{\\pi} (\\cos(nx) - \\cos(mx))^{2} dx = \\int_{0}^{\\pi} \\cos^{2}(nx) dx - 2\\int_{0}^{\\pi} \\cos(nx)\\cos(mx) dx + \\int_{0}^{\\pi} \\cos^{2}(mx) dx $$\nThe set of functions $\\{\\cos(kx)\\}_{k \\in \\mathbb{N}}$ is an orthogonal system on $(0, \\pi)$, so $\\int_{0}^{\\pi} \\cos(nx)\\cos(mx) dx = 0$ for $n \\neq m$. Thus,\n$$ \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\frac{\\pi}{2} - 0 + \\frac{\\pi}{2} = \\pi $$\nThis implies $\\|u_{n} - u_{m}\\|_{H^{1}(\\Omega)}^{2} \\geq \\|u_{n}' - u_{m}'\\|_{L^{2}(\\Omega)}^{2} = \\pi$, which means $\\|u_{n} - u_{m}\\|_{H^{1}(\\Omega)} \\geq \\sqrt{\\pi}$ for all $n \\neq m$. The sequence is not Cauchy and therefore cannot converge in the complete space $H^{1}_{0}(\\Omega)$.\n\nThe fundamental reason for this discrepancy lies in the definitions of the inner products and the topologies they induce. The $H^{1}$ inner product, $\\langle u, v \\rangle_{H^{1}} = \\langle u, v \\rangle_{L^{2}} + \\langle u', v' \\rangle_{L^{2}}$, contains an additional term that measures the \"energy\" of the derivatives. For convergence in $H^{1}(\\Omega)$, both the functions and their derivatives must converge in the $L^{2}$ sense.\nFor the sequence $u_{n}(x) = \\frac{\\sin(nx)}{n}$:\n- The function values are scaled by $\\frac{1}{n}$, causing $\\|u_{n}\\|_{L^{2}}$ to vanish as $n \\to \\infty$. The functions are \"squashed\" to zero.\n- The derivatives $u_{n}'(x) = \\cos(nx)$ are not scaled down. As $n$ increases, the functions become more oscillatory, and their gradients do not diminish in magnitude. The term $\\|u_{n}'\\|_{L^{2}}$ remains constant at $\\sqrt{\\frac{\\pi}{2}}$.\n\nThe topology of a normed space is determined by its open sets, which are unions of open balls $B_{r}(f) = \\{ g : \\|f-g\\| < r \\}$. The inequality $\\|v\\|_{L^{2}} \\leq \\|v\\|_{H^{1}}$ for any $v \\in H^{1}(\\Omega)$ implies that any open ball in the $H^{1}$-norm is a subset of the corresponding open ball in the $L^{2}$-norm: $B_{r}^{H^{1}}(f) \\subseteq B_{r}^{L^{2}}(f)$. This means that any open set in the $L^{2}$ topology is also an open set in the $H^{1}$ topology. The converse is not true; there are open sets in the $H^{1}$ topology (e.g., $B_{r}^{H^{1}}(0)$ for $r < \\sqrt{\\pi/2}$) that are not open in the $L^{2}$ topology. This makes the $H^{1}$ topology *strictly stronger* (or finer) than the $L^{2}$ topology. Convergence in the stronger topology ($H^{1}$) implies convergence in the weaker topology ($L^{2}$), but the converse is false. This sequence $\\{u_{n}\\}$ serves as the canonical counterexample. It converges in $L^{2}(\\Omega)$ but fails to converge in $H^{1}(\\Omega)$ because the derivative term in the $H^{1}$ inner product imposes a stricter requirement that the sequence does not satisfy.", "answer": "$$\\boxed{\\sqrt{\\frac{\\pi}{2}}}$$", "id": "2575288"}, {"introduction": "Once an inner product is chosen, the properties of the resulting FEM linear system depend heavily on the chosen basis functions. This exercise makes that connection concrete by having you transform the standard \"hat\" basis into an orthonormal one using the Gram-Schmidt process. You will calculate the mass matrix and its condition number, revealing how this change of basis can, in theory, produce a perfectly conditioned system and why this ideal is not always practical.", "problem": "Let $V_h$ be the space of continuous, piecewise linear functions on the interval $[0,1]$ with respect to the uniform mesh with nodes at $x=0$, $x=\\tfrac{1}{2}$, and $x=1$. Consider the canonical nodal basis $\\{\\varphi_1,\\varphi_2,\\varphi_3\\}$ defined by\n- $\\varphi_1(x) = \\max\\{1-2x,0\\}$ for $x \\in [0,1]$,\n- $\\varphi_2(x) = \\begin{cases}2x, & x \\in [0,\\tfrac{1}{2}],\\\\ 2(1-x), & x \\in [\\tfrac{1}{2},1],\\end{cases}$\n- $\\varphi_3(x) = \\max\\{2x-1,0\\}$ for $x \\in [0,1]$.\nEquip $V_h$ with the $L^2$ inner product $(u,v)_{L^2} = \\int_0^1 u(x)v(x)\\,dx$.\n\nTasks:\n1. Using only the definitions of inner product and norm, perform the Gram–Schmidt orthonormalization process on the ordered set $\\{\\varphi_1,\\varphi_2,\\varphi_3\\}$ with respect to $(\\cdot,\\cdot)_{L^2}$ to produce an $L^2$-orthonormal set $\\{\\psi_1,\\psi_2,\\psi_3\\}$. Express each $\\psi_j$ as a linear combination of $\\{\\varphi_1,\\varphi_2,\\varphi_3\\}$ with exact coefficients.\n2. Let $M \\in \\mathbb{R}^{3 \\times 3}$ be the mass matrix with entries $M_{ij}=(\\varphi_i,\\varphi_j)_{L^2}$. Compute the spectral condition number $\\kappa_2(M)$, defined as the ratio of the largest to the smallest eigenvalue of $M$.\n3. Briefly discuss, based on first principles, how the Gram–Schmidt process changes the mass matrix and what that implies for conditioning of linear systems involving the mass matrix in the finite element method.\n\nProvide as your final boxed answer the exact value of $\\kappa_2(M)$. No rounding is required.", "solution": "The problem statement provided is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n-   The vector space $V_h$ is the space of continuous, piecewise linear functions on the interval $[0,1]$.\n-   The mesh nodes are at $x=0$, $x=\\tfrac{1}{2}$, and $x=1$.\n-   The canonical nodal basis is $\\{\\varphi_1, \\varphi_2, \\varphi_3\\}$, where:\n    -   $\\varphi_1(x) = \\max\\{1-2x,0\\}$ for $x \\in [0,1]$.\n    -   $\\varphi_2(x) = \\begin{cases}2x, & x \\in [0,\\tfrac{1}{2}],\\\\ 2(1-x), & x \\in [\\tfrac{1}{2},1].\\end{cases}$\n    -   $\\varphi_3(x) = \\max\\{2x-1,0\\}$ for $x \\in [0,1]$.\n-   The inner product on $V_h$ is the $L^2$ inner product: $(u,v)_{L^2} = \\int_0^1 u(x)v(x)\\,dx$.\n-   Task 1: Perform Gram–Schmidt orthonormalization on the ordered set $\\{\\varphi_1, \\varphi_2, \\varphi_3\\}$ to produce an orthonormal set $\\{\\psi_1, \\psi_2, \\psi_3\\}$, expressing each $\\psi_j$ as a linear combination of the original basis functions.\n-   Task 2: Compute the mass matrix $M \\in \\mathbb{R}^{3 \\times 3}$ with entries $M_{ij}=(\\varphi_i,\\varphi_j)_{L^2}$ and find its spectral condition number $\\kappa_2(M)$.\n-   Task 3: Discuss the effect of the Gram-Schmidt process on the mass matrix and its conditioning.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in the a priori analysis of the finite element method, dealing with fundamental concepts such as function spaces, basis functions, inner products, Gram-Schmidt orthogonalization, and matrix properties. All definitions and tasks are mathematically sound and central to numerical analysis.\n-   **Well-Posed:** The problem provides a clearly defined set of basis functions and an inner product. The Gram-Schmidt process is a deterministic algorithm that yields a unique orthonormal basis (up to sign choices, which are fixed by the standard procedure). The mass matrix is uniquely defined, and as it will be shown to be symmetric and positive definite, its spectral condition number is well-defined and unique.\n-   **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is mathematically sound, self-contained, and well-posed. A complete solution will now be furnished.\n\nThe solution is organized into the three tasks requested.\n\n**Task 1: Gram-Schmidt Orthonormalization**\n\nThe goal is to construct an orthonormal basis $\\{\\psi_1, \\psi_2, \\psi_3\\}$ from the given basis $\\{\\varphi_1, \\varphi_2, \\varphi_3\\}$ using the Gram-Schmidt process. We define an intermediate orthogonal basis $\\{u_1, u_2, u_3\\}$ and then normalize each vector. The process is:\n1.  $u_1 = \\varphi_1$ and $\\psi_1 = \\frac{u_1}{\\|u_1\\|_{L^2}}$.\n2.  $u_2 = \\varphi_2 - (\\varphi_2, \\psi_1)_{L^2} \\psi_1$ and $\\psi_2 = \\frac{u_2}{\\|u_2\\|_{L^2}}$.\n3.  $u_3 = \\varphi_3 - (\\varphi_3, \\psi_1)_{L^2} \\psi_1 - (\\varphi_3, \\psi_2)_{L^2} \\psi_2$ and $\\psi_3 = \\frac{u_3}{\\|u_3\\|_{L^2}}$.\n\nTo proceed, we must compute the inner products $(\\varphi_i, \\varphi_j)_{L^2}$, which form the mass matrix $M$.\n$M_{11} = (\\varphi_1, \\varphi_1)_{L^2} = \\int_0^{1/2} (1-2x)^2 \\,dx = \\left[ -\\frac{(1-2x)^3}{6} \\right]_0^{1/2} = 0 - \\left(-\\frac{1}{6}\\right) = \\frac{1}{6}$.\n$M_{22} = (\\varphi_2, \\varphi_2)_{L^2} = \\int_0^{1/2} (2x)^2 \\,dx + \\int_{1/2}^1 (2(1-x))^2 \\,dx = 2 \\int_0^{1/2} 4x^2 \\,dx = 8 \\left[ \\frac{x^3}{3} \\right]_0^{1/2} = 8 \\left(\\frac{1}{24}\\right) = \\frac{1}{3}$.\n$M_{33} = (\\varphi_3, \\varphi_3)_{L^2} = \\int_{1/2}^1 (2x-1)^2 \\,dx = \\left[ \\frac{(2x-1)^3}{6} \\right]_{1/2}^1 = \\frac{1}{6} - 0 = \\frac{1}{6}$.\n$M_{12} = (\\varphi_1, \\varphi_2)_{L^2} = \\int_0^{1/2} (1-2x)(2x) \\,dx = \\int_0^{1/2} (2x - 4x^2) \\,dx = \\left[ x^2 - \\frac{4x^3}{3} \\right]_0^{1/2} = \\frac{1}{4} - \\frac{4}{3}\\left(\\frac{1}{8}\\right) = \\frac{1}{4} - \\frac{1}{6} = \\frac{1}{12}$. By symmetry, $M_{21} = \\frac{1}{12}$.\n$M_{23} = (\\varphi_2, \\varphi_3)_{L^2} = \\int_{1/2}^1 2(1-x)(2x-1) \\,dx$. By a change of variable $y=1-x$, this integral transforms into the integral for $M_{12}$, so $M_{23} = \\frac{1}{12}$. By symmetry, $M_{32} = \\frac{1}{12}$.\n$M_{13} = (\\varphi_1, \\varphi_3)_{L^2} = \\int_0^1 \\varphi_1(x)\\varphi_3(x) \\,dx = 0$, since the supports of $\\varphi_1(x)$ and $\\varphi_3(x)$ are $[0, 1/2]$ and $[1/2, 1]$ respectively, overlapping only at a single point. Thus $M_{31}=0$.\n\nThe mass matrix is $M = \\begin{pmatrix} 1/6 & 1/12 & 0 \\\\ 1/12 & 1/3 & 1/12 \\\\ 0 & 1/12 & 1/6 \\end{pmatrix}$.\n\nNow, we perform the orthonormalization.\nStep 1:\n$u_1 = \\varphi_1$.\n$\\|u_1\\|_{L^2}^2 = (\\varphi_1, \\varphi_1)_{L^2} = M_{11} = \\frac{1}{6}$.\n$\\psi_1 = \\frac{\\varphi_1}{\\|\\varphi_1\\|_{L^2}} = \\sqrt{6}\\varphi_1$.\n$\\psi_1 = \\sqrt{6}\\varphi_1 + 0\\varphi_2 + 0\\varphi_3$.\n\nStep 2:\n$u_2 = \\varphi_2 - \\frac{(\\varphi_2, \\varphi_1)_{L^2}}{(\\varphi_1, \\varphi_1)_{L^2}} \\varphi_1 = \\varphi_2 - \\frac{M_{12}}{M_{11}} \\varphi_1 = \\varphi_2 - \\frac{1/12}{1/6} \\varphi_1 = \\varphi_2 - \\frac{1}{2}\\varphi_1$.\n$\\|u_2\\|_{L^2}^2 = (u_2, u_2)_{L^2} = (\\varphi_2 - \\frac{1}{2}\\varphi_1, \\varphi_2 - \\frac{1}{2}\\varphi_1)_{L^2} = M_{22} - M_{12} + \\frac{1}{4}M_{11} = \\frac{1}{3} - \\frac{1}{12} + \\frac{1}{4}\\left(\\frac{1}{6}\\right) = \\frac{4-1}{12} + \\frac{1}{24} = \\frac{3}{12} + \\frac{1}{24} = \\frac{7}{24}$.\n$\\psi_2 = \\frac{u_2}{\\|u_2\\|_{L^2}} = \\frac{\\varphi_2 - \\frac{1}{2}\\varphi_1}{\\sqrt{7/24}} = \\sqrt{\\frac{24}{7}} (\\varphi_2 - \\frac{1}{2}\\varphi_1) = \\frac{2\\sqrt{6}}{\\sqrt{7}}\\varphi_2 - \\frac{\\sqrt{6}}{\\sqrt{7}}\\varphi_1$.\n$\\psi_2 = -\\frac{\\sqrt{42}}{7}\\varphi_1 + \\frac{2\\sqrt{42}}{7}\\varphi_2 + 0\\varphi_3$.\n\nStep 3:\nIt is computationally more direct to use the orthogonal vectors $u_j$: $u_3 = \\varphi_3 - \\frac{(\\varphi_3, u_1)_{L^2}}{\\|u_1\\|_{L^2}^2} u_1 - \\frac{(\\varphi_3, u_2)_{L^2}}{\\|u_2\\|_{L^2}^2} u_2$.\n$(\\varphi_3, u_1)_{L^2} = (\\varphi_3, \\varphi_1)_{L^2} = M_{13} = 0$.\n$(\\varphi_3, u_2)_{L^2} = (\\varphi_3, \\varphi_2 - \\frac{1}{2}\\varphi_1)_{L^2} = M_{32} - \\frac{1}{2}M_{31} = \\frac{1}{12} - 0 = \\frac{1}{12}$.\n$u_3 = \\varphi_3 - \\frac{1/12}{7/24} u_2 = \\varphi_3 - \\frac{2}{7}u_2 = \\varphi_3 - \\frac{2}{7}(\\varphi_2 - \\frac{1}{2}\\varphi_1) = \\frac{1}{7}\\varphi_1 - \\frac{2}{7}\\varphi_2 + \\varphi_3$.\n$\\|u_3\\|_{L^2}^2 = (u_3, \\varphi_3)_{L^2}$ because $\\varphi_3$ is orthogonal to $u_1$ and $u_2$ projections.\n$\\|u_3\\|_{L^2}^2 = (\\frac{1}{7}\\varphi_1 - \\frac{2}{7}\\varphi_2 + \\varphi_3, \\varphi_3)_{L^2} = \\frac{1}{7}M_{13} - \\frac{2}{7}M_{23} + M_{33} = 0 - \\frac{2}{7}(\\frac{1}{12}) + \\frac{1}{6} = -\\frac{1}{42} + \\frac{7}{42} = \\frac{6}{42} = \\frac{1}{7}$.\n$\\psi_3 = \\frac{u_3}{\\|u_3\\|_{L^2}} = \\sqrt{7}u_3 = \\sqrt{7}(\\frac{1}{7}\\varphi_1 - \\frac{2}{7}\\varphi_2 + \\varphi_3)$.\n$\\psi_3 = \\frac{\\sqrt{7}}{7}\\varphi_1 - \\frac{2\\sqrt{7}}{7}\\varphi_2 + \\sqrt{7}\\varphi_3$.\n\n**Task 2: Spectral Condition Number of the Mass Matrix**\n\nThe mass matrix is $M = \\frac{1}{12} \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 4 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}$.\nThe spectral condition number is $\\kappa_2(M) = \\frac{\\lambda_{max}(M)}{\\lambda_{min}(M)}$.\nThe eigenvalues $\\lambda$ of $M$ are found by solving the characteristic equation $\\det(M - \\lambda I) = 0$. Let $A = 12M$ and find its eigenvalues $\\mu$. Then $\\lambda = \\mu/12$.\n$\\det(A - \\mu I) = \\det \\begin{pmatrix} 2-\\mu & 1 & 0 \\\\ 1 & 4-\\mu & 1 \\\\ 0 & 1 & 2-\\mu \\end{pmatrix} = 0$.\n$(2-\\mu) [(4-\\mu)(2-\\mu)-1] - 1(2-\\mu) = 0$.\nFactoring out $(2-\\mu)$:\n$(2-\\mu) [(4-\\mu)(2-\\mu)-1-1] = 0$.\n$(2-\\mu) [\\mu^2 - 6\\mu + 8 - 2] = 0$.\n$(2-\\mu)(\\mu^2 - 6\\mu + 6) = 0$.\nOne eigenvalue of $A$ is $\\mu_1 = 2$. The others are roots of $\\mu^2 - 6\\mu + 6 = 0$.\nUsing the quadratic formula: $\\mu = \\frac{-(-6) \\pm \\sqrt{(-6)^2 - 4(1)(6)}}{2(1)} = \\frac{6 \\pm \\sqrt{36-24}}{2} = \\frac{6 \\pm \\sqrt{12}}{2} = 3 \\pm \\sqrt{3}$.\nThe eigenvalues of $A$ are $\\mu_{min} = 3-\\sqrt{3}$, $\\mu_{mid} = 2$, and $\\mu_{max} = 3+\\sqrt{3}$.\nThe eigenvalues of $M$ are $\\lambda_{min} = \\frac{3-\\sqrt{3}}{12}$ and $\\lambda_{max} = \\frac{3+\\sqrt{3}}{12}$.\nThe condition number $\\kappa_2(M)$ is the ratio of the largest to the smallest eigenvalue:\n$\\kappa_2(M) = \\frac{\\lambda_{max}}{\\lambda_{min}} = \\frac{(3+\\sqrt{3})/12}{(3-\\sqrt{3})/12} = \\frac{3+\\sqrt{3}}{3-\\sqrt{3}}$.\nTo rationalize this expression, we multiply the numerator and denominator by the conjugate of the denominator:\n$\\kappa_2(M) = \\frac{3+\\sqrt{3}}{3-\\sqrt{3}} \\cdot \\frac{3+\\sqrt{3}}{3+\\sqrt{3}} = \\frac{(3+\\sqrt{3})^2}{3^2 - (\\sqrt{3})^2} = \\frac{9 + 6\\sqrt{3} + 3}{9-3} = \\frac{12 + 6\\sqrt{3}}{6} = 2 + \\sqrt{3}$.\n\n**Task 3: Discussion on Conditioning**\n\nThe Gram-Schmidt process transforms the original basis $\\{\\varphi_j\\}$ into an $L^2$-orthonormal basis $\\{\\psi_j\\}$. By definition, an orthonormal basis satisfies $(\\psi_i, \\psi_j)_{L^2} = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe mass matrix associated with the new basis $\\{\\psi_j\\}$, let's call it $M'$, has entries $M'_{ij} = (\\psi_i, \\psi_j)_{L^2}$. Due to orthonormality, $M'_{ij} = \\delta_{ij}$. Therefore, the mass matrix in the new basis is the identity matrix, $M' = I$.\n\nThe eigenvalues of the identity matrix are all equal to $1$. Consequently, its spectral condition number is $\\kappa_2(M') = \\kappa_2(I) = \\frac{\\lambda_{max}(I)}{\\lambda_{min}(I)} = \\frac{1}{1} = 1$.\n\nIn contrast, the original mass matrix $M$ has a condition number of $\\kappa_2(M) = 2+\\sqrt{3} \\approx 3.732$. The Gram-Schmidt process thus transforms the basis in such a way that the corresponding mass matrix becomes perfectly conditioned.\n\nIn finite element analysis, one frequently encounters linear systems of the form $M\\mathbf{c} = \\mathbf{b}$, where $M$ is the mass matrix. The condition number of $M$ is a critical factor determining the numerical stability of the solution and the efficiency of iterative solvers. A system with $\\kappa_2(M) = 1$ is optimally conditioned; its solution is insensitive to small perturbations in the data and can be found trivially. By changing the basis from $\\{\\varphi_j\\}$ to $\\{\\psi_j\\}$, the problem $M\\mathbf{c}=\\mathbf{b}$ transforms into $I\\mathbf{c'} = \\mathbf{b'}$, which has the immediate solution $\\mathbf{c'} = \\mathbf{b'}$.\n\nHowever, this theoretical advantage has a severe practical drawback. The original basis functions $\\{\\varphi_j\\}$ are *local*, meaning each has support over a small portion of the domain. This locality results in a sparse (in this case, tridiagonal) mass matrix, which is computationally efficient to store and operate on. The orthonormal basis functions $\\{\\psi_j\\}$, as shown in Task 1, are global linear combinations of the local functions. They have non-zero values across the entire domain. A basis of global functions leads to a dense matrix. For more complex differential equations, the stiffness matrix would become dense, and the computational cost of assembling and solving a dense linear system typically outweighs the benefit of perfect conditioning. For this reason, orthonormalization of the standard \"hat\" basis is not a common practice in finite element implementations.", "answer": "$$\\boxed{2+\\sqrt{3}}$$", "id": "2575259"}]}