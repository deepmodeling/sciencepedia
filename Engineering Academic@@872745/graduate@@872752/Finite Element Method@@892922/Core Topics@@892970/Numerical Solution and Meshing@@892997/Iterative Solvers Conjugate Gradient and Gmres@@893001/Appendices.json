{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is best understood by doing. This exercise provides a hands-on walk-through of the first two iterations of the algorithm for a simple symmetric positive-definite system [@problem_id:2570866]. By manually calculating the residuals $r_k$, search directions $p_k$, and update scalars $\\alpha_k$ and $\\beta_k$, you will gain an intuitive feel for the mechanics of CG and see firsthand how the orthogonality and conjugacy properties lead to convergence in a finite number of steps.", "problem": "Consider the symmetric positive definite (SPD) linear system arising from a two-degree-of-freedom finite element model, with stiffness matrix $A=\\begin{bmatrix}20\\\\01\\end{bmatrix}$, right-hand side $b=\\begin{bmatrix}2\\\\1\\end{bmatrix}$, and initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Apply the Conjugate Gradient (CG) method starting from $x_{0}$ for $2$ iterations. Begin from first principles appropriate for SPD systems: the quadratic energy $J(x)=\\tfrac{1}{2}x^T A x-b^T x$, the residual $r_{k}=b-Ax_{k}$, the search subspace $x_{k+1}\\in x_{k}+\\operatorname{span}\\{p_{k}\\}$, the $A$-conjugacy of the search directions $p_i^T A p_j=0$ for $i\\neq j$, and the orthogonality of residuals to the current search direction induced by minimization of $J$ along $p_{k}$. Using only these foundational facts, first derive the expressions for the step length $\\alpha_{k}$ and the recurrence coefficient $\\beta_{k}$ in terms of $r_{k}$, $p_{k}$, and $A$, and then carry out the computations to obtain, explicitly and in exact rational form, the quantities $\\alpha_{0}$, $\\beta_{0}$, $\\alpha_{1}$, $\\beta_{1}$, the iterates $x_{1}$ and $x_{2}$, the residuals $r_{0}$, $r_{1}$, $r_{2}$, and the search directions $p_{0}$, $p_{1}$, $p_{2}$. \n\nReport your final result as a single row matrix containing, in this exact order,\n$$\\bigl[\\alpha_{0},\\,\\beta_{0},\\,\\alpha_{1},\\,\\beta_{1},\\,x_{0,1},\\,x_{0,2},\\,x_{1,1},\\,x_{1,2},\\,x_{2,1},\\,x_{2,2},\\,r_{0,1},\\,r_{0,2},\\,r_{1,1},\\,r_{1,2},\\,r_{2,1},\\,r_{2,2},\\,p_{0,1},\\,p_{0,2},\\,p_{1,1},\\,p_{1,2},\\,p_{2,1},\\,p_{2,2}\\bigr].$$\n\nExpress all quantities exactly; no rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a canonical exercise in the application of the Conjugate Gradient (CG) method. Therefore, it is deemed valid and a full solution will be provided.\n\nThe problem requires the derivation of the Conjugate Gradient algorithm's parameters from first principles, followed by the execution of two iterations for the given linear system.\n\n**1. Derivation of CG Algorithm Parameters**\n\nLet the linear system be $Ax = b$, where $A$ is an $n \\times n$ symmetric positive definite (SPD) matrix. The solution $x$ uniquely minimizes the quadratic energy functional $J(x) = \\frac{1}{2}x^T A x - b^T x$. The CG method is an iterative process that generates a sequence of approximations $x_k$ that converge to the true solution.\n\nThe method is defined by the following update rules:\n$x_{k+1} = x_k + \\alpha_k p_k$\n$r_{k+1} = r_k - \\alpha_k A p_k$\n$p_{k+1} = r_{k+1} + \\beta_k p_k$\nwhere $r_k = b - Ax_k$ is the residual at step $k$, $p_k$ is the search direction, $\\alpha_k$ is the step length, and $\\beta_k$ is a coefficient for combining the new residual with the previous search direction.\n\n**Derivation of the step length $\\alpha_k$:**\nThe step length $\\alpha_k$ is chosen to minimize the energy functional $J(x_{k+1})$ along the search direction $p_k$. That is, we minimize $J(x_k + \\alpha_k p_k)$ with respect to $\\alpha_k$.\n$$J(x_k + \\alpha_k p_k) = \\frac{1}{2}(x_k + \\alpha_k p_k)^T A(x_k + \\alpha_k p_k) - b^T(x_k + \\alpha_k p_k)$$\nTo find the minimum, we set the derivative with respect to $\\alpha_k$ to zero:\n$$\\frac{d}{d\\alpha_k} J(x_k + \\alpha_k p_k) = 0$$\nUsing the product rule for matrix calculus and the symmetry of $A$, we get:\n$$p_k^T A(x_k + \\alpha_k p_k) - p_k^T b = 0$$\n$$p_k^T Ax_k + \\alpha_k p_k^T Ap_k - p_k^T b = 0$$\n$$\\alpha_k (p_k^T Ap_k) = p_k^T(b - Ax_k)$$\nUsing the definition of the residual, $r_k = b - Ax_k$, we obtain:\n$$\\alpha_k = \\frac{p_k^T r_k}{p_k^T Ap_k}$$\nThis is a general expression. For the CG algorithm, it can be shown that $p_k^T r_k = r_k^T r_k$. Starting with $p_0 = r_0$, this is trivial for $k=0$. For $k0$, $p_k = r_k + \\beta_{k-1}p_{k-1}$. Thus, $p_k^T r_k = (r_k + \\beta_{k-1}p_{k-1})^T r_k = r_k^T r_k + \\beta_{k-1}p_{k-1}^T r_k$. The optimality condition which yields $\\alpha_{k-1}$ is equivalent to $r_k^T p_{k-1}=0$. Therefore, $p_{k-1}^T r_k=0$, which simplifies the expression to $p_k^T r_k = r_k^T r_k$.\nThe standard formula for the step length is therefore:\n$$\\alpha_k = \\frac{r_k^T r_k}{p_k^T Ap_k}$$\n\n**Derivation of the recurrence coefficient $\\beta_k$:**\nThe search directions must be $A$-conjugate, i.e., $p_{k+1}^T A p_j = 0$ for all $j \\le k$. We enforce this for $j=k$:\n$$p_{k+1}^T A p_k = 0$$\nSubstitute the expression for the new search direction, $p_{k+1} = r_{k+1} + \\beta_k p_k$:\n$$(r_{k+1} + \\beta_k p_k)^T A p_k = 0$$\n$$r_{k+1}^T A p_k + \\beta_k p_k^T A p_k = 0$$\nSolving for $\\beta_k$:\n$$\\beta_k = - \\frac{r_{k+1}^T A p_k}{p_k^T A p_k}$$\nThis formula can be simplified. From the residual update formula $r_{k+1} = r_k - \\alpha_k A p_k$, we can write $A p_k = \\frac{1}{\\alpha_k}(r_k - r_{k+1})$. Substituting this into the numerator:\n$$\\beta_k = - \\frac{r_{k+1}^T\\left(\\frac{1}{\\alpha_k}(r_k - r_{k+1})\\right)}{p_k^T A p_k} = -\\frac{1}{\\alpha_k} \\frac{r_{k+1}^T r_k - r_{k+1}^T r_{k+1}}{p_k^T A p_k}$$\nA fundamental property of CG is the orthogonality of the residuals, $r_{k+1}^T r_k = 0$. This can be shown by induction using the $A$-conjugacy of search directions. Thus, the expression becomes:\n$$\\beta_k = \\frac{1}{\\alpha_k} \\frac{r_{k+1}^T r_{k+1}}{p_k^T A p_k}$$\nNow, substitute the expression for $\\alpha_k = \\frac{r_k^T r_k}{p_k^T Ap_k}$, which implies $\\alpha_k p_k^T Ap_k = r_k^T r_k$:\n$$\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$$\n\n**2. Application of CG Algorithm**\n\nThe problem provides:\n$A = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, and initial guess $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\n**Iteration $k=0$:**\nInitial residual:\n$r_0 = b - A x_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\nInitial search direction:\n$p_0 = r_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\nScalar product of the residual:\n$r_0^T r_0 = 2^2 + 1^2 = 5$.\nCompute $A p_0$:\n$A p_0 = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}$.\nCompute $p_0^T A p_0$:\n$p_0^T A p_0 = \\begin{bmatrix} 2  1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = 8 + 1 = 9$.\nStep length $\\alpha_0$:\n$\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0} = \\frac{5}{9}$.\nUpdate iterate $x_1$:\n$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{5}{9}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix}$.\nUpdate residual $r_1$:\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\frac{5}{9}\\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 18/9 - 20/9 \\\\ 9/9 - 5/9 \\end{bmatrix} = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$.\nScalar product of new residual:\n$r_1^T r_1 = (-\\frac{2}{9})^2 + (\\frac{4}{9})^2 = \\frac{4}{81} + \\frac{16}{81} = \\frac{20}{81}$.\nRecurrence coefficient $\\beta_0$:\n$\\beta_0 = \\frac{r_1^T r_1}{r_0^T r_0} = \\frac{20/81}{5} = \\frac{4}{81}$.\nUpdate search direction $p_1$:\n$p_1 = r_1 + \\beta_0 p_0 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} + \\frac{4}{81}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -18/81 \\\\ 36/81 \\end{bmatrix} + \\begin{bmatrix} 8/81 \\\\ 4/81 \\end{bmatrix} = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$.\n\n**Iteration $k=1$:**\nWe have $r_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$, $p_1 = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$, and $r_1^T r_1 = \\frac{20}{81}$.\nCompute $A p_1$:\n$Ap_1 = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix}$.\nCompute $p_1^T A p_1$:\n$p_1^T A p_1 = \\begin{bmatrix} -10/81  40/81 \\end{bmatrix} \\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix} = \\frac{200}{81^2} + \\frac{1600}{81^2} = \\frac{1800}{6561}$.\nStep length $\\alpha_1$:\n$\\alpha_1 = \\frac{r_1^T r_1}{p_1^T A p_1} = \\frac{20/81}{1800/6561} = \\frac{20}{81} \\cdot \\frac{6561}{1800} = \\frac{20 \\cdot 81}{1800} = \\frac{1620}{1800} = \\frac{9}{10}$.\nUpdate iterate $x_2$:\n$x_2 = x_1 + \\alpha_1 p_1 = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix} + \\frac{9}{10}\\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix} + \\begin{bmatrix} -1/9 \\\\ 4/9 \\end{bmatrix} = \\begin{bmatrix} 9/9 \\\\ 9/9 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nUpdate residual $r_2$:\n$r_2 = r_1 - \\alpha_1 A p_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} - \\frac{9}{10}\\begin{bmatrix} -20/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} - \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nScalar product of new residual:\n$r_2^T r_2 = 0^2 + 0^2 = 0$.\nRecurrence coefficient $\\beta_1$:\n$\\beta_1 = \\frac{r_2^T r_2}{r_1^T r_1} = \\frac{0}{20/81} = 0$.\nUpdate search direction $p_2$:\n$p_2 = r_2 + \\beta_1 p_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + 0 \\cdot \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThe algorithm has converged to the exact solution $x=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ in $2$ steps, as expected for a $2 \\times 2$ system.\n\n**Summary of Quantities:**\n$\\alpha_0 = 5/9$\n$\\beta_0 = 4/81$\n$\\alpha_1 = 9/10$\n$\\beta_1 = 0$\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n$x_1 = \\begin{bmatrix} 10/9 \\\\ 5/9 \\end{bmatrix}$\n$x_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n$r_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n$r_1 = \\begin{bmatrix} -2/9 \\\\ 4/9 \\end{bmatrix}$\n$r_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n$p_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n$p_1 = \\begin{bmatrix} -10/81 \\\\ 40/81 \\end{bmatrix}$\n$p_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\nThese will be assembled into the final answer matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{9}  \\frac{4}{81}  \\frac{9}{10}  0  0  0  \\frac{10}{9}  \\frac{5}{9}  1  1  2  1  -\\frac{2}{9}  \\frac{4}{9}  0  0  2  1  -\\frac{10}{81}  \\frac{40}{81}  0  0\n\\end{pmatrix}\n}\n$$", "id": "2570866"}, {"introduction": "While the standard CG method is powerful, its convergence rate depends heavily on the condition number of the system matrix. This is where preconditioning becomes essential. This practice [@problem_id:2571007] demonstrates the application of the Preconditioned Conjugate Gradient (PCG) method, using a simple diagonal (Jacobi) preconditioner. You will not only execute a step of the algorithm but also compute the error reduction factor in the $A$-norm, providing a quantitative measure of how preconditioning can accelerate convergence even in the first iteration.", "problem": "A symmetric positive definite linear system arising from a finite element discretization is given by the matrix $A=\\begin{bmatrix}4  1 \\\\ 1  3\\end{bmatrix}$ and the right-hand side $b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$. Consider left preconditioning by the symmetric positive definite preconditioner $M=\\mathrm{diag}(4,3)$. Starting from the initial guess $x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, perform one iteration of the left-preconditioned Conjugate Gradient (CG) method, understood from first principles as the method that minimizes the quadratic energy $J(x)=\\tfrac{1}{2}x^T A x - b^T x$ over affine subspaces generated by preconditioned residual directions, where the first search direction is $p_{0}=M^{-1}r_{0}$ with $r_{0}=b-Ax_{0}$, and the step length is determined by minimizing $J(x_{0}+\\alpha p_{0})$ with respect to $\\alpha$.\n\nLet $x^{\\star}$ denote the exact solution of $Ax=b$, and define the error $e_{k}=x^{\\star}-x_{k}$ and the $A$-norm $\\|e\\|_{A}=\\sqrt{e^T A e}$. After one iteration, compute the exact reduction factor\n$$\\rho \\;=\\; \\frac{\\|e_{1}\\|_{A}}{\\|e_{0}\\|_{A}}.$$\nReport only the value of $\\rho$ as a fully simplified exact expression. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- System matrix: $A=\\begin{bmatrix}4  1 \\\\ 1  3\\end{bmatrix}$\n- Right-hand side vector: $b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$\n- The matrix $A$ is described as symmetric positive definite.\n- Preconditioner matrix: $M=\\mathrm{diag}(4,3) = \\begin{bmatrix}4  0 \\\\ 0  3\\end{bmatrix}$\n- Preconditioning type: Left preconditioning.\n- Initial guess: $x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n- Method: One iteration of left-preconditioned Conjugate Gradient (CG).\n- First search direction: $p_{0}=M^{-1}r_{0}$, with $r_{0}=b-Ax_{0}$.\n- Step length $\\alpha$: Determined by minimizing $J(x_{0}+\\alpha p_{0})$, where $J(x)=\\tfrac{1}{2}x^T A x - b^T x$.\n- Exact solution: $x^{\\star}$ where $Ax^{\\star}=b$.\n- Error: $e_{k}=x^{\\star}-x_{k}$.\n- $A$-norm: $\\|e\\|_{A}=\\sqrt{e^T A e}$.\n- Quantity to compute: The exact reduction factor $\\rho = \\frac{\\|e_{1}\\|_{A}}{\\|e_{0}\\|_{A}}$.\n\nValidation verdict:\nThe problem is scientifically grounded, concerning standard procedures in numerical linear algebra for solving linear systems, specifically the preconditioned conjugate gradient method. The matrix $A$ is symmetric; its determinant is $\\det(A) = (4)(3) - (1)(1) = 11  0$ and its top-left entry is $40$, confirming it is positive definite. The preconditioner $M$ is also symmetric and positive definite. All definitions and conditions are standard, complete, and mathematically consistent. The problem is well-posed and objective. Therefore, the problem is valid.\n\nWe proceed with the solution. The task is to compute the reduction factor $\\rho = \\|e_{1}\\|_{A} / \\|e_{0}\\|_{A}$ after one iteration. A direct, but tedious, approach would be to compute $x_{1}$ and then the norms of the errors $e_{0}$ and $e_{1}$. A more insightful method is to derive a general expression for the reduction factor.\n\nThe first iteration of the CG method updates the solution as $x_{1} = x_{0} + \\alpha_{0} p_{0}$. The error after this step is $e_{1} = x^{\\star} - x_{1} = x^{\\star} - (x_{0} + \\alpha_{0} p_{0}) = (x^{\\star} - x_{0}) - \\alpha_{0} p_{0} = e_{0} - \\alpha_{0} p_{0}$.\n\nThe $A$-norm of the error $e_{1}$ is minimized with respect to the step length $\\alpha_{0}$. The objective function to minimize is $\\|e_{1}\\|_{A}^{2} = (e_{0} - \\alpha_{0} p_{0})^T A(e_{0} - \\alpha_{0} p_{0})$.\nExpanding this expression gives:\n$$ \\|e_{1}\\|_{A}^{2} = e_{0}^T A e_{0} - 2\\alpha_{0} e_{0}^T A p_{0} + \\alpha_{0}^{2} p_{0}^T A p_{0} $$\nTo find the minimum, we differentiate with respect to $\\alpha_{0}$ and set the derivative to zero:\n$$ \\frac{d}{d\\alpha_{0}} \\|e_{1}\\|_{A}^{2} = -2 e_{0}^T A p_{0} + 2\\alpha_{0} p_{0}^T A p_{0} = 0 $$\nSolving for the optimal step length $\\alpha_{0}$ yields:\n$$ \\alpha_{0} = \\frac{e_{0}^T A p_{0}}{p_{0}^T A p_{0}} $$\nThe problem states that $\\alpha$ is found by minimizing $J(x_{0}+\\alpha p_{0})$. The gradient of $J(x)$ is $\\nabla J(x) = Ax-b$. The minimum of $J(x_0+\\alpha_0 p_0)$ occurs when the gradient at the new point is orthogonal to the search direction: $\\nabla J(x_1)^T p_0 = 0$. This gives $(A(x_0+\\alpha_0 p_0) - b)^T p_0 = 0$, leading to $\\alpha_0 = \\frac{(b-Ax_0)^T p_0}{p_0^T Ap_0} = \\frac{r_0^T p_0}{p_0^T Ap_0}$. Since $r_{0} = b-Ax_{0} = Ax^{\\star}-Ax_{0}=A(x^{\\star}-x_{0})=Ae_{0}$, we have $r_{0}^T p_{0} = (Ae_{0})^T p_{0} = e_{0}^T A p_{0}$, which confirms the formula for $\\alpha_{0}$.\n\nSubstituting the optimal $\\alpha_{0}$ back into the expression for $\\|e_{1}\\|_{A}^{2}$:\n$$ \\|e_{1}\\|_{A}^{2} = \\|e_{0}\\|_{A}^{2} - 2 \\left( \\frac{e_{0}^T A p_{0}}{p_0^T A p_{0}} \\right) e_{0}^T A p_{0} + \\left( \\frac{e_{0}^T A p_{0}}{p_0^T A p_{0}} \\right)^{2} p_{0}^T A p_{0} = \\|e_{0}\\|_{A}^{2} - \\frac{(e_{0}^T A p_{0})^{2}}{p_{0}^T A p_{0}} $$\nThe square of the reduction factor is therefore:\n$$ \\rho^{2} = \\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}} = 1 - \\frac{(e_{0}^T A p_{0})^{2}}{ (\\|e_{0}\\|_{A}^{2}) (p_{0}^T A p_{0}) } $$\nWe now compute the components of this expression.\nThe initial guess is $x_{0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$.\nThe initial residual is $r_{0} = b - Ax_{0} = b = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$.\nThe preconditioner is $M = \\begin{bmatrix}4  0 \\\\ 0  3\\end{bmatrix}$, so its inverse is $M^{-1} = \\begin{bmatrix}1/4  0 \\\\ 0  1/3\\end{bmatrix}$.\nThe first search direction is $p_{0} = M^{-1}r_{0} = \\begin{bmatrix}1/4  0 \\\\ 0  1/3\\end{bmatrix} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}1/4 \\\\ 1/3\\end{bmatrix}$.\n\nThe initial error is $e_{0} = x^{\\star} - x_{0} = x^{\\star}$. The exact solution $x^{\\star}$ is found from $Ax=b$:\n$$ x^{\\star} = A^{-1}b = \\frac{1}{(4)(3)-(1)(1)} \\begin{bmatrix}3  -1 \\\\ -1  4\\end{bmatrix} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = \\frac{1}{11} \\begin{bmatrix}2 \\\\ 3\\end{bmatrix} $$\nSo, $e_{0} = \\frac{1}{11} \\begin{bmatrix}2 \\\\ 3\\end{bmatrix}$.\n\nNow we calculate the terms in the formula for $\\rho^2$:\n1. $\\|e_{0}\\|_{A}^{2} = e_{0}^T A e_{0}$. Since $Ae_{0} = A(x^{\\star}-x_0) = b-Ax_0 = r_{0}$, we have $\\|e_{0}\\|_{A}^{2} = e_{0}^T r_{0}$.\n$$ \\|e_{0}\\|_{A}^{2} = \\left(\\frac{1}{11} \\begin{bmatrix}2 \\\\ 3\\end{bmatrix}\\right)^T \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = \\frac{1}{11}(2 \\cdot 1 + 3 \\cdot 1) = \\frac{5}{11} $$\n2. $p_{0}^T A p_{0}$:\n$$ p_{0}^T A p_{0} = \\begin{bmatrix}1/4  1/3\\end{bmatrix} \\begin{bmatrix}4  1 \\\\ 1  3\\end{bmatrix} \\begin{bmatrix}1/4 \\\\ 1/3\\end{bmatrix} = \\begin{bmatrix}1/4  1/3\\end{bmatrix} \\begin{bmatrix}4(1/4)+1(1/3) \\\\ 1(1/4)+3(1/3)\\end{bmatrix} = \\begin{bmatrix}1/4  1/3\\end{bmatrix} \\begin{bmatrix}1+1/3 \\\\ 1/4+1\\end{bmatrix} $$\n$$ p_{0}^T A p_{0} = \\begin{bmatrix}1/4  1/3\\end{bmatrix} \\begin{bmatrix}4/3 \\\\ 5/4\\end{bmatrix} = (1/4)(4/3) + (1/3)(5/4) = 1/3 + 5/12 = 4/12 + 5/12 = \\frac{9}{12} = \\frac{3}{4} $$\n3. $e_{0}^T A p_{0}$. Again using $Ae_{0}=r_{0}$, this is $r_{0}^T p_{0}$:\n$$ e_{0}^T A p_{0} = r_{0}^T p_{0} = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}1/4 \\\\ 1/3\\end{bmatrix} = 1/4 + 1/3 = 3/12 + 4/12 = \\frac{7}{12} $$\nSubstituting these values into the expression for $\\rho^{2}$:\n$$ \\rho^{2} = 1 - \\frac{(7/12)^{2}}{(5/11)(3/4)} = 1 - \\frac{49/144}{15/44} = 1 - \\left(\\frac{49}{144} \\cdot \\frac{44}{15}\\right) $$\nWe simplify the fraction:\n$$ \\frac{49}{144} \\cdot \\frac{44}{15} = \\frac{49}{36 \\cdot 4} \\cdot \\frac{4 \\cdot 11}{15} = \\frac{49 \\cdot 11}{36 \\cdot 15} = \\frac{539}{540} $$\nThus,\n$$ \\rho^{2} = 1 - \\frac{539}{540} = \\frac{1}{540} $$\nThe reduction factor $\\rho$ is the positive square root:\n$$ \\rho = \\sqrt{\\frac{1}{540}} = \\frac{1}{\\sqrt{540}} $$\nTo simplify, we factor the term in the square root: $540 = 54 \\times 10 = 9 \\times 6 \\times 10 = 9 \\times 2 \\times 3 \\times 2 \\times 5 = 36 \\times 15$.\n$$ \\rho = \\frac{1}{\\sqrt{36 \\times 15}} = \\frac{1}{6\\sqrt{15}} $$\nRationalizing the denominator gives the fully simplified form:\n$$ \\rho = \\frac{\\sqrt{15}}{6 \\cdot 15} = \\frac{\\sqrt{15}}{90} $$\nThis is the final exact expression.", "answer": "$$ \\boxed{\\frac{\\sqrt{15}}{90}} $$", "id": "2571007"}, {"introduction": "When the system matrix is not symmetric, the theoretical foundation of the Conjugate Gradient method no longer holds, and we must turn to more general methods like the Generalized Minimal Residual (GMRES). This exercise [@problem_id:2570955] isolates the core components of the GMRES algorithm: the Arnoldi process for building an orthonormal basis for the Krylov subspace and the subsequent least-squares problem used to minimize the residual norm. Completing this single-step calculation provides a foundational understanding of the principles that govern GMRES for general linear systems.", "problem": "Consider the linear system $A x = b$ arising from a finite element discretization, with\n$$\nA=\\begin{bmatrix}2  1 \\\\ 0  3\\end{bmatrix}, \\quad b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}, \\quad x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}.\n$$\nLet $\\|\\cdot\\|$ denote the Euclidean norm. You will perform one step of the Arnoldi process and compute the residual norm produced by one step of the Generalized Minimal Residual (GMRES) method.\n\nStarting from the initial residual $r_{0}=b-A x_{0}$, define the first Arnoldi vector $v_{1}=r_{0}/\\|r_{0}\\|$. Perform one Arnoldi step to obtain scalars $h_{11}$ and $h_{21}$ such that\n$$\nA v_{1}=v_{1} h_{11}+v_{2} h_{21},\n$$\nwhere $v_{2}$ is the next Arnoldi vector if $h_{21}\\neq 0$. Use this to form the $(1+1)\\times 1$ upper Hessenberg matrix $\\hat{H}_{1}=\\begin{bmatrix}h_{11} \\\\ h_{21}\\end{bmatrix}$ and the scalar $\\beta=\\|r_{0}\\|$.\n\nUsing only the defining properties of Krylov subspaces, the Arnoldi relation, and the orthonormality of Arnoldi vectors, derive the $1$-step GMRES least-squares problem and compute the resulting GMRES residual norm after this single step. Provide the exact value of the residual norm as your final answer (no rounding required).", "solution": "The problem requires the computation of the residual norm after one step of the Generalized Minimal Residual (GMRES) method for a given linear system $A x = b$. We must first validate the problem statement, which is found to be well-posed and scientifically sound, before proceeding with the solution.\n\nThe problem is defined by the matrix $A$, the vector $b$, and the initial guess $x_{0}$:\n$$\nA=\\begin{bmatrix}2  1 \\\\ 0  3\\end{bmatrix}, \\quad b=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}, \\quad x_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n$$\nThe GMRES algorithm iteratively finds an approximate solution in a Krylov subspace. The process begins with the computation of the initial residual $r_{0}$.\n\nFirst, we compute the initial residual, $r_{0}$:\n$$\nr_{0} = b - A x_{0} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} - \\begin{bmatrix}2  1 \\\\ 0  3\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\n$$\nThe norm of the initial residual, denoted by $\\beta$, is computed using the Euclidean norm:\n$$\n\\beta = \\|r_{0}\\| = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}\n$$\nThe first step of the Arnoldi process generates an orthonormal basis for the Krylov subspace $\\mathcal{K}_{1}(A, r_{0})$. The first basis vector, $v_{1}$, is the normalized initial residual:\n$$\nv_{1} = \\frac{r_{0}}{\\|r_{0}\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\n$$\nNext, we perform one step of the Arnoldi iteration to find the entries of the Hessenberg matrix $\\hat{H}_{1}$. We compute the product of $A$ and $v_{1}$:\n$$\nw_{1} = A v_{1} = \\begin{bmatrix}2  1 \\\\ 0  3\\end{bmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}2(1)+1(1) \\\\ 0(1)+3(1)\\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix}\n$$\nThe coefficient $h_{11}$ is the projection of $w_{1}$ onto $v_{1}$. Since we are working with real vectors, $h_{11} = v_{1}^{T} w_{1}$:\n$$\nh_{11} = \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1  1\\end{bmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} \\right) = \\frac{1}{2} (1 \\cdot 3 + 1 \\cdot 3) = \\frac{6}{2} = 3\n$$\nThe next Arnoldi vector $v_{2}$ is obtained by orthogonalizing $w_{1}$ against $v_{1}$. Let $\\tilde{w}_{1} = w_{1} - h_{11} v_{1}$:\n$$\n\\tilde{w}_{1} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} - 3 \\left( \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\left( \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} - \\begin{bmatrix}3 \\\\ 3\\end{bmatrix} \\right) = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n$$\nThe coefficient $h_{21}$ is the norm of this vector:\n$$\nh_{21} = \\|\\tilde{w}_{1}\\| = \\sqrt{0^2 + 0^2} = 0\n$$\nThis situation, where $h_{m+1, m} = 0$, is known as a \"lucky breakdown.\" It implies that the Krylov subspace $\\mathcal{K}_{m}(A, r_{0})$ is an invariant subspace of $A$. In this case, for $m=1$, we have $A v_{1} = h_{11} v_{1}$, meaning $v_{1}$ (and thus $r_{0}$) is an eigenvector of $A$ with eigenvalue $h_{11}=3$.\n\nThe GMRES method finds an approximate solution $x_{m}$ of the form $x_{m} = x_{0} + z_{m}$, where $z_{m}$ is in the Krylov subspace $\\mathcal{K}_{m}(A, r_{0}) = \\text{span}\\{v_{1}, v_{2}, \\ldots, v_{m}\\}$. The vector $z_{m}$ is chosen to minimize the norm of the residual $r_{m} = \\|b - A x_{m}\\|$.\nFor the first step ($m=1$), we seek $x_{1} = x_{0} + y_{1}v_{1}$ that minimizes $\\|r_{1}\\|$. The residual is:\n$$\nr_{1} = b - A x_{1} = b - A(x_{0} + y_{1}v_{1}) = (b - A x_{0}) - y_{1}A v_{1} = r_{0} - y_{1}A v_{1}\n$$\nUsing the Arnoldi relations $r_{0} = \\beta v_{1}$ and $A v_{1} = h_{11}v_{1} + h_{21}v_{2}$, we get:\n$$\nr_{1} = \\beta v_{1} - y_{1}(h_{11}v_{1} + h_{21}v_{2}) = (\\beta - y_{1}h_{11})v_{1} - y_{1}h_{21}v_{2}\n$$\nSince the Arnoldi vectors are orthonormal, the norm of the residual is:\n$$\n\\|r_{1}\\|^{2} = \\|(\\beta - y_{1}h_{11})v_{1} - y_{1}h_{21}v_{2}\\|^{2} = (\\beta - y_{1}h_{11})^{2} + (-y_{1}h_{21})^{2}\n$$\nMinimizing $\\|r_{1}\\|$ is equivalent to solving the following $(m+1) \\times m$ least-squares problem for $y$:\n$$\n\\min_{y \\in \\mathbb{R}^{m}} \\|\\hat{H}_{m} y - \\beta e_{1}\\|\n$$\nwhere $\\hat{H}_{m}$ is the $(m+1) \\times m$ upper Hessenberg matrix from the Arnoldi process, and $e_{1}$ is the first standard basis vector in $\\mathbb{R}^{m+1}$. The norm of the GMRES residual, $\\|r_{m}\\|$, is a direct result of this minimization.\n\nFor $m=1$, we have $y \\in \\mathbb{R}^{1}$ (a scalar $y_1$) and $e_{1} \\in \\mathbb{R}^{2}$. The problem is:\n$$\n\\min_{y_{1}} \\left\\| \\begin{bmatrix} h_{11} \\\\ h_{21} \\end{bmatrix} [y_{1}] - \\beta \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\|\n$$\nSubstituting the computed values $h_{11}=3$, $h_{21}=0$, and $\\beta=\\sqrt{2}$:\n$$\n\\min_{y_{1}} \\left\\| \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} [y_{1}] - \\sqrt{2} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\| = \\min_{y_{1}} \\left\\| \\begin{bmatrix} 3y_{1} - \\sqrt{2} \\\\ 0 \\end{bmatrix} \\right\\|\n$$\nThe norm of this vector is $\\sqrt{(3y_{1} - \\sqrt{2})^{2} + 0^{2}} = |3y_{1} - \\sqrt{2}|$.\nThe minimum value of this expression is achieved when $3y_{1} - \\sqrt{2} = 0$, which means choosing $y_{1} = \\frac{\\sqrt{2}}{3}$. The minimum value itself, which corresponds to the norm of the GMRES residual after one step, is $0$.\n\nThus, GMRES converges to the exact solution in a single step because the initial residual $r_{0}$ is an eigenvector of the matrix $A$. The resulting residual norm, $\\|r_{1}\\|$, is zero.", "answer": "$$\\boxed{0}$$", "id": "2570955"}]}