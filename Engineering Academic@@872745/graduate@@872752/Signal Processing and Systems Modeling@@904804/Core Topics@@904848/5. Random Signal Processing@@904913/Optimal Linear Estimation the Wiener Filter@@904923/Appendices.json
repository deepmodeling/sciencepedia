{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a fundamental application of the Wiener filter: one-step linear prediction. This exercise [@problem_id:2888998] asks you to design the simplest possible predictor, with filter order $M=1$, for a first-order autoregressive, or AR(1), process. By deriving the necessary correlation functions from first principles and solving the scalar Wiener-Hopf equation, you will build a solid intuition for the core mechanics of optimal linear estimation and uncover an elegant, foundational result.", "problem": "Consider a wide-sense stationary, zero-mean, discrete-time stochastic process defined by the first-order autoregressive recursion $x[n] = a\\,x[n-1] + w[n]$, where $|a|  1$ and $w[n]$ is a zero-mean, white random process with variance $\\sigma_{w}^{2}$ and is uncorrelated with $\\{x[k]\\}$ for all $k \\leq n-1$. You are to design the causal, order-$M=1$ linear minimum mean-square error (Linear Minimum Mean-Square Error (LMMSE)) one-step predictor of $x[n]$ based on the single regressor $x[n-1]$. That is, consider the estimator $\\hat{x}[n] = h\\,x[n-1]$, and minimize $\\mathbb{E}\\{(x[n]-\\hat{x}[n])^{2}\\}$ over the scalar coefficient $h$.\n\nStarting only from the definitions of autocorrelation and cross-correlation, stationarity, and the given autoregressive model, perform the following:\n\n- Derive the scalar input autocorrelation matrix $R$ (which is $1 \\times 1$ for $M=1$) in closed form in terms of $a$ and $\\sigma_{w}^{2}$.\n- Solve the Wiener–Hopf normal equation for $M=1$ to obtain the optimal coefficient $h$ explicitly in closed form in terms of $a$ and $\\sigma_{w}^{2}$.\n\nReport your final answer as a single row matrix whose two entries are, in order, the scalar $R$ and the optimal scalar $h$. No numerical approximation is required; provide exact closed-form expressions.", "solution": "The problem presented is a standard exercise in optimal linear estimation theory and is well-posed, scientifically grounded, and internally consistent. It is a valid problem. We will proceed with its solution.\n\nThe objective is to find the optimal scalar coefficient $h$ for the one-step linear predictor $\\hat{x}[n] = h\\,x[n-1]$ that minimizes the mean-square error (MSE), defined as $J(h) = \\mathbb{E}\\{(x[n]-\\hat{x}[n])^{2}\\}$. The process $x[n]$ is a first-order autoregressive, or AR($1$), process given by $x[n] = a\\,x[n-1] + w[n]$, where $|a|  1$, and $w[n]$ is zero-mean white noise with variance $\\sigma_{w}^{2}$, uncorrelated with past values of the process $x[k]$ for $k \\le n-1$.\n\nThe optimal filter coefficients are found by solving the Wiener-Hopf normal equations, which take the form $R\\mathbf{h} = \\mathbf{p}$. For the specified problem, we have a single-tap predictor, so the order is $M=1$. Consequently, the filter coefficient vector $\\mathbf{h}$ is a scalar $h$, the input autocorrelation matrix $R$ is a $1 \\times 1$ matrix (a scalar), and the cross-correlation vector $\\mathbf{p}$ is also a scalar $p$.\n\nThe scalar Wiener-Hopf equation is thus $Rh=p$.\n\nFirst, we must derive the expression for the scalar autocorrelation matrix $R$. For a predictor of order $M=1$ with input $x[n-1]$, $R$ is defined by:\n$$R = \\mathbb{E}\\{x[n-1]x[n-1]\\} = \\mathbb{E}\\{x[n-1]^2\\}$$\nSince the process $x[n]$ is given as wide-sense stationary (WSS), its autocorrelation function $r_{xx}[k] = \\mathbb{E}\\{x[n]x[n-k]\\}$ is independent of $n$ and depends only on the lag $k$. The variance of the process is constant, so $\\mathbb{E}\\{x[n-1]^2\\} = \\mathbb{E}\\{x[n]^2\\} = r_{xx}[0]$. Let us find $r_{xx}[0]$.\nStarting from the process definition:\n$$x[n] = a\\,x[n-1] + w[n]$$\nWe compute the variance:\n$$r_{xx}[0] = \\mathbb{E}\\{x[n]^2\\} = \\mathbb{E}\\{[a\\,x[n-1] + w[n]]^2\\}$$\n$$r_{xx}[0] = \\mathbb{E}\\{a^2 x[n-1]^2 + 2a\\,x[n-1]w[n] + w[n]^2\\}$$\nUsing the linearity of the expectation operator:\n$$r_{xx}[0] = a^2 \\mathbb{E}\\{x[n-1]^2\\} + 2a\\,\\mathbb{E}\\{x[n-1]w[n]\\} + \\mathbb{E}\\{w[n]^2\\}$$\nWe now use the given properties:\n$1$. Stationarity implies $\\mathbb{E}\\{x[n-1]^2\\} = r_{xx}[0]$.\n$2$. The noise variance is $\\mathbb{E}\\{w[n]^2\\} = \\sigma_w^2$.\n$3$. The noise $w[n]$ is uncorrelated with $x[k]$ for $k \\le n-1$, which means $\\mathbb{E}\\{x[n-1]w[n]\\} = 0$.\n\nSubstituting these into the equation for $r_{xx}[0]$:\n$$r_{xx}[0] = a^2 r_{xx}[0] + 2a(0) + \\sigma_w^2$$\n$$r_{xx}[0](1 - a^2) = \\sigma_w^2$$\nSince $|a|  1$, we know $1-a^2 \\neq 0$, so we can solve for $r_{xx}[0]$:\n$$r_{xx}[0] = \\frac{\\sigma_w^2}{1 - a^2}$$\nThis is the expression for the scalar autocorrelation matrix $R$.\n$$R = r_{xx}[0] = \\frac{\\sigma_w^2}{1 - a^2}$$\n\nNext, we derive the expression for the scalar cross-correlation $p$. This is defined as the correlation between the desired signal $x[n]$ and the input to the filter, which is $x[n-1]$.\n$$p = \\mathbb{E}\\{x[n]x[n-1]\\}$$\nBy definition of the autocorrelation function for a WSS process, this is $r_{xx}[1]$. We compute this value:\n$$p = r_{xx}[1] = \\mathbb{E}\\{x[n]x[n-1]\\} = \\mathbb{E}\\{[a\\,x[n-1] + w[n]]x[n-1]\\}$$\n$$p = \\mathbb{E}\\{a\\,x[n-1]^2 + w[n]x[n-1]\\}$$\nBy linearity of expectation:\n$$p = a\\,\\mathbb{E}\\{x[n-1]^2\\} + \\mathbb{E}\\{w[n]x[n-1]\\}$$\nUsing the same properties as before: $\\mathbb{E}\\{x[n-1]^2\\} = r_{xx}[0]$ and $\\mathbb{E}\\{w[n]x[n-1]\\} = 0$.\n$$p = a\\,r_{xx}[0] + 0 = a\\,r_{xx}[0]$$\nSubstituting the expression for $r_{xx}[0]$:\n$$p = a \\left(\\frac{\\sigma_w^2}{1 - a^2}\\right)$$\n\nFinally, we solve the scalar Wiener-Hopf equation $Rh = p$ for the optimal coefficient $h$:\n$$\\left(\\frac{\\sigma_w^2}{1 - a^2}\\right) h = a \\left(\\frac{\\sigma_w^2}{1 - a^2}\\right)$$\nAssuming a non-trivial process where $\\sigma_w^2  0$, we can divide both sides by the non-zero quantity $\\frac{\\sigma_w^2}{1 - a^2}$. This yields:\n$$h = a$$\nThis result is correct and intuitive. The best linear predictor for an AR($1$) process, based on the previous sample, uses the known autoregressive coefficient.\n\nThe problem requires reporting the scalar $R$ and the optimal scalar $h$ as a single row matrix.\nThe first entry is $R = \\frac{\\sigma_{w}^{2}}{1 - a^{2}}$.\nThe second entry is $h = a$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{w}^{2}}{1 - a^{2}}  a\n\\end{pmatrix}\n}\n$$", "id": "2888998"}, {"introduction": "While theoretically elegant, the direct solution to the Wiener-Hopf equations can be numerically unstable if the autocorrelation matrix $R$ is ill-conditioned, a common issue when signals are highly correlated. This practice [@problem_id:2888981] introduces a powerful technique to combat this issue: diagonal loading, a form of Tikhonov regularization. By solving for the regularized filter coefficients and explicitly deriving the resulting mean-squared error, you will gain a concrete understanding of the fundamental bias-variance trade-off, where a small bias is intentionally introduced to achieve a more stable and robust solution.", "problem": "Consider a zero-mean, wide-sense stationary (WSS) scalar process $x(n)$ generated by the first-order autoregressive model $x(n) = a\\,x(n-1) + u(n)$, with $|a|1$ and $u(n)$ a zero-mean, white process independent of $x(n-1)$. Assume $x(n)$ has been normalized so that $\\mathbb{E}\\{x(n)^2\\} = 1$. For this model, the autocorrelation is $r_{k} = \\mathbb{E}\\{x(n)x(n-k)\\} = a^{|k|}$.\n\nYou observe the two-dimensional regressor $\\mathbf{x}(n) = [x(n),\\,x(n-1)]^{\\top}$ and wish to linearly estimate the desired signal $d(n) = x(n-1)$ using a two-tap filter $w \\in \\mathbb{R}^{2}$ that minimizes the mean square error (MSE), where the MSE is defined as $J(w) = \\mathbb{E}\\{(d(n) - w^{\\top}x(n))^{2}\\}$. Starting from the orthogonality principle and the definitions of the autocorrelation matrix and cross-correlation vector, the unregularized normal equations can be formed using the Toeplitz matrix structure induced by $r_{k}$.\n\nTo improve numerical conditioning, you apply diagonal loading (a form of preconditioning) with a loading parameter $\\lambda0$, yielding the modified system $(R + \\lambda I) w_{\\lambda} = p$, where $R$ is the $2\\times 2$ Toeplitz autocorrelation matrix of $x(n)$ and $p$ is the $2\\times 1$ cross-correlation vector between $x(n)$ and $d(n)$.\n\nTasks:\n1. Using only the properties of WSS processes and the orthogonality principle, form $R$ and $p$ symbolically in terms of $a$, and write down the diagonally loaded system $(R + \\lambda I) w_{\\lambda} = p$ for $\\lambda0$.\n2. Solve explicitly for $w_{\\lambda}$ as a function of $a$ and $\\lambda$.\n3. Starting from the definition $J(w) = \\mathbb{E}\\{(d(n) - w^{\\top}x(n))^{2}\\}$, and without invoking any shortcut formulas, derive a closed-form expression for the diagonally loaded MSE $J(\\lambda) \\triangleq J(w_{\\lambda})$ in terms of $a$ and $\\lambda$. Simplify your result to a single rational expression in $a$ and $\\lambda$.\n\nExplain briefly, based on your expression, how diagonal loading modifies the Toeplitz system and affects the optimality and MSE. Your final answer must be the single simplified analytic expression for $J(\\lambda)$ in terms of $a$ and $\\lambda$. No numerical rounding is required and no units are needed.", "solution": "The problem presented is scientifically grounded, well-posed, and contains all necessary information for a unique solution. It is a standard exercise in optimal linear estimation theory. The minor notational ambiguity where $x(n)$ is used to denote both a scalar process and a vector regressor can be resolved by context. For clarity, the regressor vector will be denoted as $\\mathbf{x}(n) = [x(n), x(n-1)]^{\\top}$. We proceed with the solution.\n\nThe problem asks for three tasks to be completed, followed by a brief explanation.\n\nTask 1: Formation of the diagonally loaded system.\n\nThe autocorrelation matrix $R$ is defined as $R = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}$. The regressor vector is $\\mathbf{x}(n) = \\begin{pmatrix} x(n) \\\\ x(n-1) \\end{pmatrix}$.\n$$\nR = \\mathbb{E}\\left\\{ \\begin{pmatrix} x(n) \\\\ x(n-1) \\end{pmatrix} \\begin{pmatrix} x(n)  x(n-1) \\end{pmatrix} \\right\\} = \\mathbb{E}\\left\\{ \\begin{pmatrix} x(n)^2  x(n)x(n-1) \\\\ x(n-1)x(n)  x(n-1)^2 \\end{pmatrix} \\right\\}\n$$\nBy taking the expectation of each element and using the wide-sense stationary (WSS) property and the given autocorrelation function $r_{k} = \\mathbb{E}\\{x(m)x(m-k)\\} = a^{|k|}$, with the normalization $r_0 = \\mathbb{E}\\{x(n)^2\\}=1$:\n$$\nR = \\begin{pmatrix} \\mathbb{E}\\{x(n)^2\\}  \\mathbb{E}\\{x(n)x(n-1)\\} \\\\ \\mathbb{E}\\{x(n-1)x(n)\\}  \\mathbb{E}\\{x(n-1)^2\\} \\end{pmatrix} = \\begin{pmatrix} r_0  r_1 \\\\ r_1  r_0 \\end{pmatrix} = \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix}\n$$\nThe cross-correlation vector $p$ is defined as $p = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$, where the desired signal is $d(n) = x(n-1)$.\n$$\np = \\mathbb{E}\\left\\{ \\begin{pmatrix} x(n) \\\\ x(n-1) \\end{pmatrix} x(n-1) \\right\\} = \\begin{pmatrix} \\mathbb{E}\\{x(n)x(n-1)\\} \\\\ \\mathbb{E}\\{x(n-1)^2\\} \\end{pmatrix} = \\begin{pmatrix} r_1 \\\\ r_0 \\end{pmatrix} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\nThe diagonally loaded system $(R + \\lambda I) w_{\\lambda} = p$ is therefore:\n$$\n\\left( \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) w_{\\lambda} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1+\\lambda  a \\\\ a  1+\\lambda \\end{pmatrix} w_{\\lambda} = \\begin{pmatrix} a \\\\ 1 \\end{pmatrix}\n$$\n\nTask 2: Solve for $w_{\\lambda}$.\n\nTo find $w_{\\lambda}$, we must invert the matrix $(R+\\lambda I)$. Let this matrix be $A = \\begin{pmatrix} 1+\\lambda  a \\\\ a  1+\\lambda \\end{pmatrix}$.\nThe determinant is $\\det(A) = (1+\\lambda)^2 - a^2$. The inverse is $A^{-1} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} 1+\\lambda  -a \\\\ -a  1+\\lambda \\end{pmatrix}$.\nThe solution is $w_{\\lambda} = A^{-1}p$:\n$$\nw_{\\lambda} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} 1+\\lambda  -a \\\\ -a  1+\\lambda \\end{pmatrix} \\begin{pmatrix} a \\\\ 1 \\end{pmatrix} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} a(1+\\lambda) - a \\\\ -a^2 + (1+\\lambda) \\end{pmatrix}\n$$\n$$\nw_{\\lambda} = \\frac{1}{(1+\\lambda)^2 - a^2} \\begin{pmatrix} a\\lambda \\\\ 1 - a^2 + \\lambda \\end{pmatrix}\n$$\n\nTask 3: Derive the closed-form expression for the MSE $J(\\lambda)$.\n\nThe MSE is given by $J(w) = \\mathbb{E}\\{(d(n) - w^{\\top}\\mathbf{x}(n))^{2}\\}$. Expanding this yields the general quadratic form:\n$$\nJ(w) = \\mathbb{E}\\{d(n)^2\\} - 2w^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} + w^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}w = \\sigma_d^2 - 2w^{\\top}p + w^{\\top}Rw\n$$\nHere, $\\sigma_d^2 = \\mathbb{E}\\{d(n)^2\\} = \\mathbb{E}\\{x(n-1)^2\\} = r_0 = 1$.\nThe optimal unregularized Wiener filter solution, $w_o$, minimizes $J(w)$ and is given by $Rw_o=p$. The minimum MSE is $J_{min} = J(w_o) = \\sigma_d^2 - p^{\\top}w_o$.\nIn this problem, the optimal solution is found by setting $\\lambda=0$ in the expression for $w_{\\lambda}$:\n$$\nw_o = w_{\\lambda=0} = \\frac{1}{1-a^2} \\begin{pmatrix} 0 \\\\ 1-a^2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis is an intuitive result, as estimating $d(n)=x(n-1)$ from $[x(n), x(n-1)]^{\\top}$ is perfectly achieved by selecting the second component. The minimum MSE is:\n$$\nJ_{min} = J(w_o) = 1 - p^{\\top}w_o = 1 - \\begin{pmatrix} a  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1 - 1 = 0\n$$\nThe MSE for the regularized solution $w_{\\lambda}$ is $J(\\lambda) = J(w_\\lambda)$. This can be expressed as the sum of the minimum MSE and an excess MSE term due to the filter mismatch:\n$J(\\lambda) = J_{min} + (w_{\\lambda}-w_o)^{\\top}R(w_{\\lambda}-w_o)$. Since $J_{min}=0$:\n$$\nJ(\\lambda) = (w_{\\lambda}-w_o)^{\\top}R(w_{\\lambda}-w_o)\n$$\nFirst, we find the filter mismatch vector $w_{\\lambda}-w_o$:\n$$\nw_{\\lambda}-w_o = \\frac{1}{(1+\\lambda)^2-a^2} \\begin{pmatrix} a\\lambda \\\\ 1 - a^2 + \\lambda \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{a\\lambda}{(1+\\lambda)^2-a^2} \\\\ \\frac{1-a^2+\\lambda - ((1+\\lambda)^2-a^2)}{(1+\\lambda)^2-a^2} \\end{pmatrix}\n$$\nThe second component simplifies to $\\frac{1-a^2+\\lambda - (1+2\\lambda+\\lambda^2-a^2)}{(1+\\lambda)^2-a^2} = \\frac{-\\lambda-\\lambda^2}{(1+\\lambda)^2-a^2} = \\frac{-\\lambda(1+\\lambda)}{(1+\\lambda)^2-a^2}$.\nThus, the mismatch vector is:\n$$\nw_{\\lambda}-w_o = \\frac{\\lambda}{(1+\\lambda)^2-a^2} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\nNow we compute the quadratic form. Let $D = (1+\\lambda)^2-a^2$.\n$$\nJ(\\lambda) = \\left(\\frac{\\lambda}{D}\\right)^2 \\begin{pmatrix} a  -(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\nThe matrix product is:\n$$\n\\begin{pmatrix} a  -(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} 1  a \\\\ a  1 \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix} = \\begin{pmatrix} a-a(1+\\lambda)  a^2-(1+\\lambda) \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -a\\lambda  a^2-1-\\lambda \\end{pmatrix} \\begin{pmatrix} a \\\\ -(1+\\lambda) \\end{pmatrix} = -a^2\\lambda - (a^2-1-\\lambda)(1+\\lambda)\n$$\n$$\n= -a^2\\lambda - (a^2(1+\\lambda) - (1+\\lambda)^2) = -a^2\\lambda - a^2 - a^2\\lambda + (1+\\lambda)^2\n$$\n$$\n= (1+\\lambda)^2 - a^2 - 2a^2\\lambda = D - 2a^2\\lambda\n$$\nSubstituting this back into the expression for $J(\\lambda)$:\n$$\nJ(\\lambda) = \\left(\\frac{\\lambda}{D}\\right)^2 (D - 2a^2\\lambda) = \\frac{\\lambda^2(D - 2a^2\\lambda)}{D^2}\n$$\nReplacing $D$ with its definition, we arrive at the final simplified rational expression:\n$$\nJ(\\lambda) = \\frac{\\lambda^2((1+\\lambda)^2 - a^2 - 2a^2\\lambda)}{((1+\\lambda)^2 - a^2)^2}\n$$\nThe numerator can be rewritten as $\\lambda^2((1+\\lambda)^2 - a^2(1+2\\lambda))$.\n\nExplanation of effects:\nDiagonal loading modifies the normal equations by adding a positive constant $\\lambda$ to the diagonal elements of the autocorrelation matrix $R$. This operation, $R \\to R+\\lambda I$, increases the eigenvalues of the matrix by $\\lambda$. If $R$ is ill-conditioned (i.e., has a large condition number, which occurs here as $|a| \\to 1$), this shift makes the matrix $R+\\lambda I$ better conditioned, improving the numerical stability of the solution for $w_{\\lambda}$.\n\nThis stability comes at a cost to optimality. The resulting filter $w_{\\lambda}$ is not the minimizer of the original mean square error $J(w)$, but rather the minimizer of a regularized cost function $J(w) + \\lambda\\|w\\|^2$. This introduces a bias into the estimate. Consequently, the MSE achieved with the regularized filter, $J(\\lambda)$, is strictly greater than the minimum possible MSE, $J_{min}=0$, for any $\\lambda0$. As seen from the derived expression, $J(\\lambda)  0$ for $\\lambda  0$ and $J(\\lambda) \\to 0$ as $\\lambda \\to 0$. This illustrates the fundamental bias-variance trade-off in regularization: one accepts a higher (biased) MSE in exchange for a more stable solution with a smaller norm, which can improve generalization performance in practice.", "answer": "$$\n\\boxed{\\frac{\\lambda^2 ( (1+\\lambda)^2 - a^2(1+2\\lambda) )}{((1+\\lambda)^2 - a^2)^2}}\n$$", "id": "2888981"}, {"introduction": "Having explored a specific method for improving numerical stability, we now generalize our analysis to the broader principles governing the sensitivity of the Wiener filter. This exercise [@problem_id:2888987] challenges you to think critically about how practical estimation errors in the autocorrelation matrix $R$ and cross-correlation vector $p$ impact the final filter solution. Through this analysis, you will appreciate the central role of the matrix condition number, $\\kappa_{2}(R)$, and its deep connection to the dynamic range of the input signal's power spectrum, providing a rigorous foundation for diagnosing and mitigating potential instabilities in filter design.", "problem": "Consider a finite-impulse-response (FIR) Wiener filter of length $M$ for linearly estimating a desired scalar process $d[n]$ from a vector input $x[n] \\in \\mathbb{R}^{M}$. Let $R \\in \\mathbb{R}^{M \\times M}$ denote the input autocorrelation matrix and $p \\in \\mathbb{R}^{M}$ denote the cross-correlation vector between $x[n]$ and $d[n]$. Assume $R$ is symmetric positive definite. The optimal filter $w \\in \\mathbb{R}^{M}$ minimizes the mean squared error and satisfies the normal equations $R w = p$. In practice, $R$ and $p$ are estimated from data, yielding perturbations $\\Delta R$ and $\\Delta p$, so that the computed quantities are $\\widehat{R} = R + \\Delta R$ and $\\widehat{p} = p + \\Delta p$, and the computed solution $\\widehat{w}$ solves $\\widehat{R}\\,\\widehat{w} = \\widehat{p}$. Use the spectral norm $\\|\\cdot\\|_{2}$ and the spectral condition number $\\kappa_{2}(R) \\triangleq \\|R\\|_{2}\\,\\|R^{-1}\\|_{2}$. Assume $\\|R^{-1}\\|_{2}\\,\\|\\Delta R\\|_{2}  1$ so that $\\widehat{R}$ is nonsingular. Also, suppose $R$ is Toeplitz when $x[n]$ is wide-sense stationary with power spectral density (PSD) $S_{x}(\\omega)$ that is bounded and strictly positive on $[-\\pi,\\pi]$.\n\nChoose all statements that are correct about the sensitivity of the FIR Wiener filter solution to perturbations and its relation to condition number and numerical stability:\n\nA. To first order in the perturbations, the solution perturbation satisfies $\\Delta w \\approx R^{-1}(\\Delta p - \\Delta R\\,w)$, which implies the relative error bound $\\dfrac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\kappa_{2}(R)\\left(\\dfrac{\\|\\Delta p\\|_{2}}{\\|p\\|_{2}} + \\dfrac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}}\\right)$ up to higher-order terms, provided $\\|R^{-1}\\|_{2}\\,\\|\\Delta R\\|_{2}  1$.\n\nB. If one uses a backward-stable linear solver for $R w = p$, the forward error $\\|\\widehat{w} - w\\|_{2}/\\|w\\|_{2}$ is bounded by a constant multiple of machine precision independently of $\\kappa_{2}(R)$.\n\nC. Diagonal loading (also called Tikhonov regularization), which replaces $R$ by $R_{\\lambda} \\triangleq R + \\lambda I$ with $\\lambda  0$, reduces $\\kappa_{2}(R)$ and thus the sensitivity of $w$ to perturbations, at the cost of introducing bias in the solution.\n\nD. If $\\Delta p = 0$, then to first order the relative error $\\|\\Delta w\\|_{2}/\\|w\\|_{2}$ is bounded by $\\|\\Delta R\\|_{2}/\\|R\\|_{2}$, independently of $\\kappa_{2}(R)$.\n\nE. For Toeplitz $R$ generated by a bounded, strictly positive PSD $S_{x}(\\omega)$, the eigenvalues of $R$ for large $M$ cluster within the range of $S_{x}(\\omega)$, so that $\\kappa_{2}(R)$ is controlled by $\\sup_{\\omega} S_{x}(\\omega) / \\inf_{\\omega} S_{x}(\\omega)$; thus, highly colored inputs with deep spectral valleys make $R$ ill-conditioned and the Wiener solution highly sensitive to perturbations.\n\nSelect all that apply.", "solution": "The validity of the problem statement is hereby confirmed. The problem describes the standard formulation for an FIR Wiener filter and poses questions regarding the sensitivity of its solution to perturbations. The concepts used—autocorrelation matrix, normal equations, Tikhonov regularization, condition number, and the relation to the power spectral density of the input process—are all standard and scientifically sound within the fields of signal processing and numerical linear algebra. The assumptions provided are appropriate and render the problem well-posed and self-contained.\n\nWe shall proceed with a detailed analysis of each statement. The core system of equations is $R w = p$, and the perturbed system is $\\widehat{R} \\widehat{w} = \\widehat{p}$, where $\\widehat{R} = R + \\Delta R$ and $\\widehat{p} = p + \\Delta p$. The solution perturbation is $\\Delta w = \\widehat{w} - w$.\n\n### Analysis of Option A\nThe statement claims that to first order, $\\Delta w \\approx R^{-1}(\\Delta p - \\Delta R\\,w)$, and from this, the relative error is bounded by $\\dfrac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\kappa_{2}(R)\\left(\\dfrac{\\|\\Delta p\\|_{2}}{\\|p\\|_{2}} + \\dfrac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}}\\right)$.\n\nLet us derive the first-order approximation for $\\Delta w$.\nWe start with the perturbed equation:\n$$ (R + \\Delta R)(w + \\Delta w) = p + \\Delta p $$\nExpanding the left side gives:\n$$ Rw + R\\Delta w + \\Delta R w + \\Delta R \\Delta w = p + \\Delta p $$\nSince $Rw=p$, we can cancel these terms from both sides:\n$$ R\\Delta w + \\Delta R w + \\Delta R \\Delta w = \\Delta p $$\nFor a first-order approximation, we assume the perturbations $\\Delta R$ and $\\Delta p$ are small, which implies $\\Delta w$ is also small. We can therefore neglect the second-order term $\\Delta R \\Delta w$:\n$$ R\\Delta w + \\Delta R w \\approx \\Delta p $$\nRearranging for $R\\Delta w$:\n$$ R\\Delta w \\approx \\Delta p - \\Delta R w $$\nSince $R$ is stated to be positive definite, it is invertible. We can multiply by $R^{-1}$ on the left:\n$$ \\Delta w \\approx R^{-1}(\\Delta p - \\Delta R w) $$\nThis confirms the first part of the statement.\n\nNow, let's derive the error bound from this approximation. Taking the spectral norm ($\\|\\cdot\\|_2$) of both sides:\n$$ \\|\\Delta w\\|_{2} \\approx \\|R^{-1}(\\Delta p - \\Delta R w)\\|_{2} $$\nUsing the triangle inequality and the sub-multiplicative property of matrix norms:\n$$ \\|\\Delta w\\|_{2} \\le \\|R^{-1}\\|_{2} (\\|\\Delta p\\|_{2} + \\|\\Delta R w\\|_{2}) \\le \\|R^{-1}\\|_{2}(\\|\\Delta p\\|_{2} + \\|\\Delta R\\|_{2}\\|w\\|_{2}) $$\nTo find the relative error, we divide by $\\|w\\|_{2}$ (assuming $w \\neq 0$, which implies $p \\neq 0$):\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\|R^{-1}\\|_{2}\\left(\\frac{\\|\\Delta p\\|_{2}}{\\|w\\|_{2}} + \\|\\Delta R\\|_{2}\\right) $$\nWe have the relationships $p=Rw$ and $\\|p\\|_{2} \\le \\|R\\|_{2}\\|w\\|_{2}$. This implies $\\frac{1}{\\|w\\|_{2}} \\le \\frac{\\|R\\|_{2}}{\\|p\\|_{2}}$. Substituting this into the first term inside the parenthesis:\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\|R^{-1}\\|_{2}\\left(\\frac{\\|\\Delta p\\|_{2}}{\\|p\\|_{2}}\\|R\\|_{2} + \\|\\Delta R\\|_{2}\\right) $$\nNow, we factor out $\\|R\\|_{2}$ and introduce the relative perturbation of R, $\\|\\Delta R\\|_{2} / \\|R\\|_{2}$:\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\|R^{-1}\\|_{2}\\|R\\|_{2}\\left(\\frac{\\|\\Delta p\\|_{2}}{\\|p\\|_{2}} + \\frac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}}\\right) $$\nRecognizing the spectral condition number $\\kappa_{2}(R) = \\|R\\|_{2}\\|R^{-1}\\|_{2}$, we arrive at the final bound:\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\kappa_{2}(R)\\left(\\frac{\\|\\Delta p\\|_{2}}{\\|p\\|_{2}} + \\frac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}}\\right) $$\nThis expression is a standard result in the perturbation theory of linear systems. The entire statement is correct, including the necessary condition $\\|R^{-1}\\|_{2}\\,\\|\\Delta R\\|_{2}  1$ which guarantees the existence and provides bounds for $(\\widehat{R})^{-1}$.\n\nVerdict: **Correct**.\n\n### Analysis of Option B\nThis statement asserts that a backward-stable linear solver guarantees a forward error bounded by a multiple of machine precision, independent of the condition number $\\kappa_{2}(R)$.\n\nA backward-stable algorithm for solving $Rw=p$ computes a solution $\\widehat{w}$ which is the exact solution to a nearby problem $(R+\\delta R)\\widehat{w} = p+\\delta p$, where the relative sizes of the perturbations, $\\|\\delta R\\|_{2}/\\|R\\|_{2}$ and $\\|\\delta p\\|_{2}/\\|p\\|_{2}$, are bounded by a small multiple of the machine precision, $\\epsilon_{mach}$.\n\nThe forward error is the relative error in the solution, $\\|\\widehat{w} - w\\|_{2}/\\|w\\|_{2}$. From the analysis in Option A, the relationship between the forward error and the perturbations to the system is given by:\n$$ \\frac{\\|\\widehat{w} - w\\|_{2}}{\\|w\\|_{2}} \\lesssim \\kappa_{2}(R)\\left(\\frac{\\|\\delta R\\|_{2}}{\\|R\\|_{2}} + \\frac{\\|\\delta p\\|_{2}}{\\|p\\|_{2}}\\right) $$\nSubstituting the bounds from backward stability:\n$$ \\frac{\\|\\widehat{w} - w\\|_{2}}{\\|w\\|_{2}} \\lesssim \\kappa_{2}(R)\\left(C_{1}\\epsilon_{mach} + C_{2}\\epsilon_{mach}\\right) = C \\cdot \\kappa_{2}(R) \\cdot \\epsilon_{mach} $$\nwhere $C$, $C_1$, and $C_2$ are modest constants. This shows that the forward error is approximately proportional to the condition number $\\kappa_{2}(R)$. If $R$ is ill-conditioned (i.e., $\\kappa_{2}(R)$ is large), a small backward error (of size $\\epsilon_{mach}$) can be amplified into a large forward error. The statement's claim that the bound is independent of $\\kappa_{2}(R)$ is fundamentally incorrect.\n\nVerdict: **Incorrect**.\n\n### Analysis of Option C\nThis statement discusses the effects of diagonal loading, or Tikhonov regularization. The regularized matrix is $R_{\\lambda} = R + \\lambda I$ for $\\lambda  0$.\n\n1.  **Effect on Condition Number**: Let the eigenvalues of the symmetric positive definite matrix $R$ be $0  \\sigma_{1} \\le \\sigma_{2} \\le \\dots \\le \\sigma_{M}$. The spectral norm is the largest eigenvalue, so $\\|R\\|_{2} = \\sigma_{M}$. The norm of the inverse is $1$ over the smallest eigenvalue, so $\\|R^{-1}\\|_{2} = 1/\\sigma_{1}$. The condition number is $\\kappa_{2}(R) = \\sigma_{M}/\\sigma_{1}$.\n    The matrix $R_{\\lambda} = R + \\lambda I$ has eigenvalues $\\sigma_{i} + \\lambda$. Its condition number is:\n    $$ \\kappa_{2}(R_{\\lambda}) = \\frac{\\max_{i}(\\sigma_{i} + \\lambda)}{\\min_{i}(\\sigma_{i} + \\lambda)} = \\frac{\\sigma_{M} + \\lambda}{\\sigma_{1} + \\lambda} $$\n    We wish to determine if $\\kappa_{2}(R_{\\lambda})  \\kappa_{2}(R)$.\n    $$ \\frac{\\sigma_{M} + \\lambda}{\\sigma_{1} + \\lambda}  \\frac{\\sigma_{M}}{\\sigma_{1}} $$\n    Since $\\sigma_{1}  0$ and $\\sigma_{1} + \\lambda  0$, we can cross-multiply:\n    $$ \\sigma_{1}(\\sigma_{M} + \\lambda)  \\sigma_{M}(\\sigma_{1} + \\lambda) $$\n    $$ \\sigma_{1}\\sigma_{M} + \\sigma_{1}\\lambda  \\sigma_{M}\\sigma_{1} + \\sigma_{M}\\lambda $$\n    $$ \\sigma_{1}\\lambda  \\sigma_{M}\\lambda $$\n    Since $\\lambda  0$, this is equivalent to $\\sigma_{1}  \\sigma_{M}$. This inequality holds unless all eigenvalues are equal, in which case $R$ is a multiple of the identity and $\\kappa_{2}(R) = 1$. In that special case, $\\kappa_{2}(R_{\\lambda})=1$ as well. In general, for any non-scalar matrix, diagonal loading strictly reduces the condition number. This in turn reduces the sensitivity of the solution to perturbations.\n\n2.  **Introduction of Bias**: The optimal Wiener filter $w = R^{-1}p$ minimizes the mean squared error (MSE), $J(w) = E[(d[n] - w^{T}x[n])^{2}]$. The regularized solution is $w_{\\lambda} = (R+\\lambda I)^{-1}p$. This solution does not minimize $J(w)$, but rather a penalized cost function $J_{\\lambda}(w) = J(w) + \\lambda\\|w\\|_{2}^{2}$. Since $w_{\\lambda} \\neq w$ (for $\\lambda0$ and $p \\neq 0$), $w_{\\lambda}$ is a biased estimate of the true optimal filter $w$. This is the principle of the bias-variance trade-off: regularization reduces variance (sensitivity) at the cost of increasing bias.\n\nThe statement correctly identifies both of these effects.\n\nVerdict: **Correct**.\n\n### Analysis of Option D\nThis statement claims that if the perturbation in the cross-correlation vector is zero ($\\Delta p = 0$), the relative error in the solution is bounded independently of the condition number.\n\nLet us return to the first-order approximation from Option A's analysis. With $\\Delta p = 0$, we have:\n$$ \\Delta w \\approx -R^{-1}\\Delta R w $$\nTaking the spectral norm:\n$$ \\|\\Delta w\\|_{2} \\le \\|R^{-1}\\|_{2} \\|\\Delta R\\|_{2} \\|w\\|_{2} $$\nDividing by $\\|w\\|_{2}$ to get the relative error:\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le \\|R^{-1}\\|_{2} \\|\\Delta R\\|_{2} $$\nTo compare with the statement, we can rewrite this as:\n$$ \\frac{\\|\\Delta w\\|_{2}}{\\|w\\|_{2}} \\le (\\|R^{-1}\\|_{2}\\|R\\|_{2}) \\frac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}} = \\kappa_{2}(R) \\frac{\\|\\Delta R\\|_{2}}{\\|R\\|_{2}} $$\nThe bound on the relative error is proportional to the condition number $\\kappa_{2}(R)$. It is not bounded by $\\|\\Delta R\\|_{2}/\\|R\\|_{2}$ alone. The statement that the bound is independent of $\\kappa_{2}(R)$ is false.\n\nVerdict: **Incorrect**.\n\n### Analysis of Option E\nThis statement connects the conditioning of a Toeplitz autocorrelation matrix $R$ to the power spectral density (PSD) $S_{x}(\\omega)$ of the WSS input process $x[n]$.\n\n1.  **Eigenvalue Distribution**: By the Grenander-Szegő theorem on the eigenvalue distribution of large Toeplitz matrices, for an $M \\times M$ Toeplitz matrix $R$ generated by a continuous, strictly positive PSD $S_{x}(\\omega)$, all eigenvalues $\\sigma_{i}$ of $R$ lie in the interval $[\\inf_{\\omega} S_{x}(\\omega), \\sup_{\\omega} S_{x}(\\omega)]$. Furthermore, as the matrix dimension $M \\rightarrow \\infty$, the eigenvalues become dense in this interval. Thus, for large $M$, the extreme eigenvalues of $R$ approach the infimum and supremum of the PSD:\n    $$ \\lambda_{\\min}(R) \\approx \\inf_{\\omega} S_{x}(\\omega) \\triangleq S_{\\min} $$\n    $$ \\lambda_{\\max}(R) \\approx \\sup_{\\omega} S_{x}(\\omega) \\triangleq S_{\\max} $$\n\n2.  **Condition Number**: The condition number is the ratio of the largest to smallest eigenvalue. Therefore, for large $M$:\n    $$ \\kappa_{2}(R) = \\frac{\\lambda_{\\max}(R)}{\\lambda_{\\min}(R)} \\approx \\frac{S_{\\max}}{S_{\\min}} $$\n    This confirms that the condition number is controlled by the ratio of the maximum to the minimum of the PSD, which is the dynamic range of the input signal's spectrum.\n\n3.  **Implication**: A \"highly colored\" input signal has a PSD that is far from constant. If it has \"deep spectral valleys\", this means that $\\inf_{\\omega} S_{x}(\\omega) = S_{\\min}$ is very small compared to $\\sup_{\\omega} S_{x}(\\omega) = S_{\\max}$. This causes the ratio $S_{\\max}/S_{\\min}$ to be very large, which in turn means $\\kappa_{2}(R)$ is very large. A large condition number signifies that the matrix $R$ is ill-conditioned. From our analysis of Option A, an ill-conditioned matrix $R$ leads to a Wiener filter solution $w$ that is highly sensitive to perturbations in $R$ and $p$.\n\nThe entire chain of reasoning presented in the statement is correct and represents a fundamental principle in signal processing.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ACE}$$", "id": "2888987"}]}