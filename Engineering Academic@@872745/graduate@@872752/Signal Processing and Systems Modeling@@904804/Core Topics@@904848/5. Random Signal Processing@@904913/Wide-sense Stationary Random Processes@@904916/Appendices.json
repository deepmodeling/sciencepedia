{"hands_on_practices": [{"introduction": "The definition of a wide-sense stationary process imposes strong constraints on its autocorrelation function (ACF). Beyond being an even function, the ACF must be positive semidefinite, a condition ensuring that the variance of any linear combination of the process's random variables is non-negative. This exercise provides a crucial hands-on demonstration of this principle by presenting a simple, plausible-looking sequence and asking you to prove it cannot be a valid ACF [@problem_id:2916976]. By computing the determinant of a small Toeplitz matrix, you will see directly how this fundamental property can be violated.", "problem": "A real-valued discrete-time sequence $r[k]$ is a valid autocovariance function of a real, zero-mean, wide-sense stationary (WSS) process if and only if it satisfies two properties derived from first principles: (i) evenness, $r[k]=r[-k]$, which follows from the definition $r[k]=\\mathbb{E}\\{x[n]\\overline{x[n-k]}\\}$ for a WSS process, and (ii) positive semidefiniteness, meaning that for every integer $N\\geq 1$ and every vector $a\\in\\mathbb{C}^{N}$, the quadratic form $\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1}a_{i}\\overline{a_{j}}\\,r[i-j]\\geq 0$, equivalently, every finite Toeplitz covariance matrix constructed from $r[k]$ is positive semidefinite. Although absolute summability, $\\sum_{k\\in\\mathbb{Z}}|r[k]|\\infty$, is often convenient for analytic purposes, it is not a substitute for positive semidefiniteness.\n\nStarting from these foundations, construct explicitly an even, absolutely summable sequence $r[k]$ that nonetheless fails the positive semidefiniteness requirement. Then, using the $3\\times 3$ Toeplitz matrix\n$$\nR_{3}=\\begin{pmatrix}\nr[0]  r[1]  r[2]\\\\\nr[1]  r[0]  r[1]\\\\\nr[2]  r[1]  r[0]\n\\end{pmatrix},\n$$\ndemonstrate the violation by showing that $\\det(R_{3})0$.\n\nTo make the task concrete and ensure an unambiguous numerical outcome, consider the specific construction\n$$\nr[0]=1,\\quad r[1]=r[-1]=2,\\quad r[k]=0\\ \\text{for all}\\ |k|\\geq 2.\n$$\nVerify the evenness and absolute summability of this $r[k]$, form $R_{3}$, and compute $\\det(R_{3})$ explicitly to demonstrate the failure of positive semidefiniteness.\n\nProvide as your final answer the single real number equal to $\\det(R_{3})$ as computed from this $r[k]$. No rounding is required, and no units are involved. The answer must be a single number.", "solution": "The problem presented is a valid and fundamental exercise in the study of wide-sense stationary (WSS) random processes. It aims to demonstrate that the property of positive semidefiniteness is a necessary condition for a sequence to be a valid autocovariance function, and that other intuitive properties such as evenness and absolute summability are not sufficient. The problem provides a specific candidate sequence and requires a rigorous verification of its failure to meet the positive semidefiniteness criterion. I will proceed with the analysis as specified.\n\nThe candidate sequence is defined as:\n$$\nr[0]=1,\\quad r[1]=r[-1]=2,\\quad r[k]=0\\ \\text{for all}\\ |k|\\geq 2.\n$$\nFirst, we verify its stated properties: evenness and absolute summability.\n\n$1$. Evenness: A sequence $r[k]$ is even if $r[k] = r[-k]$ for all integers $k$.\nFor $k=0$, the condition is trivially satisfied, as $r[0] = r[-0] = 1$.\nFor $k=1$, we are given $r[1] = 2$ and $r[-1] = 2$. Thus, $r[1] = r[-1]$.\nFor any integer $k$ such that $|k| \\geq 2$, we have $r[k] = 0$. Since $|-k| = |k|$, it follows that $r[-k] = 0$ as well. So, $r[k] = r[-k] = 0$ for $|k| \\geq 2$.\nThe condition $r[k] = r[-k]$ holds for all $k \\in \\mathbb{Z}$. The sequence is indeed even.\n\n$2$. Absolute Summability: A sequence $r[k]$ is absolutely summable if the sum $\\sum_{k=-\\infty}^{\\infty} |r[k]|$ converges to a finite value.\nFor the given sequence, the only non-zero terms are for $k \\in \\{-1, 0, 1\\}$. The sum is:\n$$\n\\sum_{k=-\\infty}^{\\infty} |r[k]| = |r[-1]| + |r[0]| + |r[1]| = |2| + |1| + |2| = 2 + 1 + 2 = 5.\n$$\nSince the sum is $5$, a finite number, the sequence is absolutely summable.\n\nNow, we investigate the core property of positive semidefiniteness. A sequence $r[k]$ is positive semidefinite if and only if for any positive integer $N$, the $N \\times N$ Toeplitz matrix $R_N$, whose entries are given by $(R_N)_{ij} = r[i-j]$, is a positive semidefinite matrix. A necessary condition for a matrix to be positive semidefinite is that all of its principal minors are non-negative. This includes the determinant of the matrix itself. If we can find any $N$ for which the determinant of $R_N$ is negative, we have proven that the sequence is not positive semidefinite and thus cannot be a valid autocovariance function.\n\nThe problem directs us to examine the case $N=3$. The corresponding $3 \\times 3$ Toeplitz matrix, for a real and even sequence, is:\n$$\nR_{3}=\\begin{pmatrix}\nr[0]  r[1]  r[2]\\\\\nr[1]  r[0]  r[1]\\\\\nr[2]  r[1]  r[0]\n\\end{pmatrix}.\n$$\nSubstituting the given values $r[0]=1$, $r[1]=2$, and $r[2]=0$ into this matrix structure, we obtain:\n$$\nR_{3}=\\begin{pmatrix}\n1  2  0\\\\\n2  1  2\\\\\n0  2  1\n\\end{pmatrix}.\n$$\nThis is a real, symmetric matrix. For such a matrix to be positive semidefinite, its determinant must be non-negative. We compute the determinant:\n$$\n\\det(R_{3}) = 1 \\cdot \\det\\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} - 2 \\cdot \\det\\begin{pmatrix} 2  2 \\\\ 0  1 \\end{pmatrix} + 0 \\cdot \\det\\begin{pmatrix} 2  1 \\\\ 0  2 \\end{pmatrix}\n$$\n$$\n\\det(R_{3}) = 1 \\cdot ((1)(1) - (2)(2)) - 2 \\cdot ((2)(1) - (2)(0)) + 0\n$$\n$$\n\\det(R_{3}) = 1 \\cdot (1 - 4) - 2 \\cdot (2 - 0)\n$$\n$$\n\\det(R_{3}) = 1(-3) - 2(2)\n$$\n$$\n\\det(R_{3}) = -3 - 4 = -7.\n$$\nThe determinant of the matrix $R_3$ is $-7$. Since $\\det(R_3)  0$, the matrix $R_3$ is not positive semidefinite. This violates a necessary condition for $r[k]$ to be a valid autocovariance function. The fact that the sequence is even and absolutely summable is irrelevant in the face of this failure. The sequence corresponds to a Discrete-Time Fourier Transform (the power spectral density) of $S(\\omega) = \\sum_{k} r[k] \\exp(-i\\omega k) = 1 + 2\\exp(-i\\omega) + 2\\exp(i\\omega) = 1 + 4\\cos(\\omega)$. This function takes on negative values, for instance at $\\omega = \\pi$, where $S(\\pi) = 1 + 4(-1) = -3$. A power spectral density must be non-negative for all frequencies, which is the frequency-domain equivalent of the time-domain positive semidefiniteness condition. The calculation confirms the violation.\n\nThe final answer is the computed value of the determinant.", "answer": "$$\\boxed{-7}$$", "id": "2916976"}, {"introduction": "A cornerstone of stationary signal analysis is the relationship between the time-domain autocorrelation function (ACF) and the frequency-domain power spectral density (PSD), as described by the Wiener-Khinchin theorem. This practice problem tasks you with deriving the PSD for one of the most common models in signal processing, a process with an exponentially decaying ACF [@problem_id:2916931]. This exercise will solidify your skills in applying the discrete-time Fourier transform to an ACF and will introduce the practical concept of equivalent rectangular bandwidth to characterize the resulting spectrum.", "problem": "A discrete-time, real-valued, zero-mean, wide-sense stationary (WSS) random process has autocorrelation function (ACF) given by $R_{X}[k] = \\sigma^{2} a^{|k|}$ for all integer lags $k$, where $|a| lt; 1$ and $\\sigma^{2} gt; 0$. Using only approved foundational facts for WSS processes and discrete-time Fourier analysis, do the following:\n\n1) Starting from first principles, derive a closed-form expression for the power spectral density (PSD) $S_{X}(\\exp(j\\omega))$ for $\\omega \\in [-\\pi,\\pi)$.\n\n2) Define the equivalent rectangular bandwidth $W$ (in radians per sample) of the PSD as the unique width such that a rectangular spectrum of height equal to the peak value $S_{\\max} \\triangleq \\max_{\\omega} S_{X}(\\exp(j\\omega))$ and width $W$ has the same area over one $2\\pi$-period as $S_{X}(\\exp(j\\omega))$. That is, $W$ is the unique nonnegative real number satisfying\n$$\nS_{\\max} \\, W \\;=\\; \\int_{-\\pi}^{\\pi} S_{X}(\\exp(j\\omega)) \\, d\\omega.\n$$\nDetermine $W$ as an explicit function of $a$.\n\nExpress the final bandwidth $W$ in radians per sample, as a closed-form analytic expression of $a$, with no numerical rounding. Your submitted final answer must be only the expression for $W(a)$.", "solution": "The problem is well-posed and scientifically grounded. It is a standard exercise in the analysis of wide-sense stationary (WSS) random processes. We proceed with the solution.\n\nThe problem requires us to determine the equivalent rectangular bandwidth $W$ for a discrete-time WSS random process with a given autocorrelation function (ACF), $R_{X}[k] = \\sigma^{2} a^{|k|}$, where $|a|  1$ and $\\sigma^{2}  0$. The determination of $W$ necessitates two primary steps: first, the derivation of the power spectral density (PSD), $S_{X}(\\exp(j\\omega))$; and second, the calculation of its peak value and total area (average power).\n\nFirst, we derive the PSD from first principles. The PSD of a discrete-time WSS random process is defined by the discrete-time Fourier transform (DTFT) of its ACF. This is a formulation of the Wiener-Khinchin theorem.\n$$\nS_{X}(\\exp(j\\omega)) = \\sum_{k=-\\infty}^{\\infty} R_{X}[k] \\exp(-j\\omega k)\n$$\nSubstituting the given ACF, $R_{X}[k] = \\sigma^{2} a^{|k|}$, we have:\n$$\nS_{X}(\\exp(j\\omega)) = \\sum_{k=-\\infty}^{\\infty} \\sigma^{2} a^{|k|} \\exp(-j\\omega k) = \\sigma^{2} \\sum_{k=-\\infty}^{\\infty} a^{|k|} \\exp(-j\\omega k)\n$$\nTo evaluate this summation, we decompose it into three parts corresponding to negative, zero, and positive values of the index $k$:\n$$\nS_{X}(\\exp(j\\omega)) = \\sigma^{2} \\left[ \\sum_{k=-\\infty}^{-1} a^{-k} \\exp(-j\\omega k) + a^{|0|} \\exp(-j\\omega \\cdot 0) + \\sum_{k=1}^{\\infty} a^{k} \\exp(-j\\omega k) \\right]\n$$\nThe term for $k=0$ is simply $a^{0} \\cdot 1 = 1$. For the first summation over negative integers, let us introduce a new index $m = -k$. As $k$ ranges from $-\\infty$ to $-1$, $m$ ranges from $\\infty$ to $1$.\n$$\n\\sum_{k=-\\infty}^{-1} a^{-k} \\exp(-j\\omega k) = \\sum_{m=1}^{\\infty} a^{m} \\exp(j\\omega m) = \\sum_{m=1}^{\\infty} (a \\exp(j\\omega))^{m}\n$$\nThe original expression for the PSD can now be written as:\n$$\nS_{X}(\\exp(j\\omega)) = \\sigma^{2} \\left[ \\sum_{m=1}^{\\infty} (a \\exp(j\\omega))^{m} + 1 + \\sum_{k=1}^{\\infty} (a \\exp(-j\\omega))^{k} \\right]\n$$\nThe two summations are geometric series. We use the fundamental formula for the sum of a geometric series, $\\sum_{n=1}^{\\infty} z^{n} = \\frac{z}{1-z}$, which is valid for $|z|1$.\nFor the first series, the common ratio is $z_{1} = a \\exp(j\\omega)$. Its magnitude is $|z_{1}| = |a||\\exp(j\\omega)| = |a| \\cdot 1 = |a|$. Since we are given that $|a|  1$, the series converges.\nFor the second series, the common ratio is $z_{2} = a \\exp(-j\\omega)$. Its magnitude is $|z_{2}| = |a||\\exp(-j\\omega)| = |a|$. This series also converges.\nApplying the formula, we get:\n$$\n\\sum_{m=1}^{\\infty} (a \\exp(j\\omega))^{m} = \\frac{a \\exp(j\\omega)}{1 - a \\exp(j\\omega)}\n$$\n$$\n\\sum_{k=1}^{\\infty} (a \\exp(-j\\omega))^{k} = \\frac{a \\exp(-j\\omega)}{1 - a \\exp(-j\\omega)}\n$$\nSubstituting these back into the expression for $S_{X}(\\exp(j\\omega))$:\n$$\nS_{X}(\\exp(j\\omega)) = \\sigma^{2} \\left[ \\frac{a \\exp(j\\omega)}{1 - a \\exp(j\\omega)} + 1 + \\frac{a \\exp(-j\\omega)}{1 - a \\exp(-j\\omega)} \\right]\n$$\nWe combine the terms by finding a common denominator:\n$$\nS_{X}(\\exp(j\\omega)) = \\sigma^{2} \\left[ \\frac{a \\exp(j\\omega)(1 - a \\exp(-j\\omega)) + (1 - a \\exp(j\\omega))(1 - a \\exp(-j\\omega)) + a \\exp(-j\\omega)(1 - a \\exp(j\\omega))}{(1 - a \\exp(j\\omega))(1 - a \\exp(-j\\omega))} \\right]\n$$\nExpanding the numerator:\n$$\n\\text{Numerator} = [a \\exp(j\\omega) - a^{2}] + [1 - a \\exp(-j\\omega) - a \\exp(j\\omega) + a^{2}] + [a \\exp(-j\\omega) - a^{2}] = 1 - a^{2}\n$$\nExpanding the denominator:\n$$\n\\text{Denominator} = 1 - a \\exp(-j\\omega) - a \\exp(j\\omega) + a^{2} = 1 - a(\\exp(j\\omega) + \\exp(-j\\omega)) + a^{2}\n$$\nUsing Euler's identity, $\\cos(\\omega) = \\frac{\\exp(j\\omega) + \\exp(-j\\omega)}{2}$, the denominator becomes $1 - 2a \\cos(\\omega) + a^{2}$.\nThus, the closed-form expression for the PSD is:\n$$\nS_{X}(\\exp(j\\omega)) = \\frac{\\sigma^{2} (1 - a^{2})}{1 - 2a \\cos(\\omega) + a^{2}}\n$$\nThis completes the first part of the task.\n\nNext, we determine the equivalent rectangular bandwidth $W$. The defining equation is:\n$$\nS_{\\max} W = \\int_{-\\pi}^{\\pi} S_{X}(\\exp(j\\omega)) d\\omega\n$$\nwhere $S_{\\max} = \\max_{\\omega} S_{X}(\\exp(j\\omega))$.\nFirst, we evaluate the integral, which represents the total average power of the process. From the inverse DTFT relationship, the ACF at lag $k=0$ is given by:\n$$\nR_{X}[0] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_{X}(\\exp(j\\omega)) d\\omega\n$$\nTherefore, the integral is directly related to $R_{X}[0]$:\n$$\n\\int_{-\\pi}^{\\pi} S_{X}(\\exp(j\\omega)) d\\omega = 2\\pi R_{X}[0]\n$$\nFrom the given ACF, $R_{X}[k] = \\sigma^{2} a^{|k|}$, we find $R_{X}[0] = \\sigma^{2} a^{|0|} = \\sigma^{2}$.\nThe total area under the PSD is $2\\pi \\sigma^{2}$.\n\nNow, we find the peak value of the PSD, $S_{\\max}$. The PSD expression is:\n$$\nS_{X}(\\exp(j\\omega)) = \\frac{\\sigma^{2} (1 - a^{2})}{1 - 2a \\cos(\\omega) + a^{2}}\n$$\nSince $\\sigma^{2}  0$ and $|a|  1$, the numerator $\\sigma^{2}(1 - a^{2})$ is a positive constant. To find the maximum of $S_{X}(\\exp(j\\omega))$, we must find the minimum of its denominator, $D(\\omega) = 1 - 2a \\cos(\\omega) + a^{2}$. The minimum value of $D(\\omega)$ depends on the sign of the parameter $a$.\nThe behavior is dictated by the term $-2a \\cos(\\omega)$.\nCase 1: $a \\in [0, 1)$. Here, $-2a \\le 0$. To minimize $D(\\omega)$, we must minimize $-2a\\cos(\\omega)$, which requires maximizing $\\cos(\\omega)$. The maximum value of $\\cos(\\omega)$ is $1$, occurring at $\\omega=0$. The minimum denominator is $D(0) = 1 - 2a(1) + a^{2} = (1-a)^{2}$.\nCase 2: $a \\in (-1, 0)$. Here, $-2a  0$. To minimize $D(\\omega)$, we must minimize $-2a\\cos(\\omega)$, which requires minimizing $\\cos(\\omega)$. The minimum value of $\\cos(\\omega)$ is $-1$, occurring at $\\omega = \\pm \\pi$. The minimum denominator is $D(\\pi) = 1 - 2a(-1) + a^{2} = 1 + 2a + a^{2} = (1+a)^{2}$.\nWe can unify these results using the absolute value $|a|$. The minimum value of the denominator is $(1-|a|)^{2}$, which occurs at $\\omega=0$ if $a \\ge 0$ and $\\omega=\\pi$ if $a  0$.\nThe peak value of the PSD is therefore:\n$$\nS_{\\max} = \\frac{\\sigma^{2} (1 - a^{2})}{(1 - |a|)^{2}}\n$$\nWe can simplify this expression by factoring the numerator: $1 - a^{2} = (1-a)(1+a)$. Since $|a|^{2}=a^{2}$, we have $1-a^{2} = 1-|a|^{2} = (1-|a|)(1+|a|)$.\n$$\nS_{\\max} = \\frac{\\sigma^{2} (1-|a|)(1+|a|)}{(1-|a|)^{2}} = \\sigma^{2}\\frac{1+|a|}{1-|a|}\n$$\nNow we assemble the equation for $W$:\n$$\n\\left( \\sigma^{2}\\frac{1+|a|}{1-|a|} \\right) W = 2\\pi \\sigma^{2}\n$$\nThe factor $\\sigma^{2}$ is positive and cancels from both sides:\n$$\n\\frac{1+|a|}{1-|a|} W = 2\\pi\n$$\nSolving for $W$, we obtain the final expression for the equivalent rectangular bandwidth:\n$$\nW = 2\\pi \\frac{1-|a|}{1+|a|}\n$$\nThis expression is a closed-form function of the parameter $a$ and is given in radians per sample.", "answer": "$$\n\\boxed{2 \\pi \\frac{1 - |a|}{1 + |a|}}\n$$", "id": "2916931"}, {"introduction": "The statistical structure of a wide-sense stationary process, captured by its autocorrelation function, is the key to predicting its future values from past observations. This hands-on practice delves into the design of an optimal linear predictor, a fundamental task in fields ranging from control theory to financial modeling [@problem_id:2916954]. By applying the orthogonality principle from first principles, you will derive the celebrated Yule-Walker equations and solve for the optimal filter coefficients and the resulting minimum prediction error variance.", "problem": "Consider a real-valued, zero-mean, wide-sense stationary (WSS) discrete-time random process $X[n]$ with autocorrelation sequence $R_{X}[k] \\triangleq \\mathbb{E}\\{X[n]X[n-k]\\}$ specified by $R_{X}[0]=1$, $R_{X}[1]=0.7$, $R_{X}[2]=0.49$, and $R_{X}[k]=0$ for all integers $k \\ge 3$. Define the order-$2$ one-step-ahead linear predictor $\\hat{X}[n] \\triangleq a_{1}X[n-1]+a_{2}X[n-2]$, and the corresponding innovation $e[n]\\triangleq X[n]-\\hat{X}[n]$ with variance $\\sigma_{e}^{2}\\triangleq \\mathbb{E}\\{e[n]^{2}\\}$. Using only the orthogonality principle for linear minimum mean-square error estimation and the stationarity properties of $X[n]$, derive the normal equations from first principles and compute the unique predictor coefficients $a_{1}$ and $a_{2}$ and the innovation variance $\\sigma_{e}^{2}$. Express your final answer as a single row matrix in the order $\\left(a_{1},\\,a_{2},\\,\\sigma_{e}^{2}\\right)$. No rounding is required.", "solution": "The problem statement has been subjected to validation and is found to be scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in linear prediction theory for wide-sense stationary random processes. All necessary data are provided, and the given autocorrelation values form a valid positive-definite sequence. We shall proceed with the derivation as requested.\n\nThe objective is to find the coefficients $a_{1}$ and $a_{2}$ of the linear predictor $\\hat{X}[n] = a_{1}X[n-1]+a_{2}X[n-2]$ that minimizes the mean-square error (MSE), $\\mathbb{E}\\{e[n]^{2}\\}$, where $e[n] = X[n] - \\hat{X}[n]$ is the prediction error, also known as the innovation. The process $X[n]$ is a real-valued, zero-mean, wide-sense stationary (WSS) process.\n\nThe solution is derived from first principles using the orthogonality principle for linear minimum mean-square error (LMMSE) estimation. This principle states that for the MSE to be minimized, the error vector $e[n]$ must be orthogonal to every vector in the subspace spanned by the observations used for the prediction. In this case, the observations are $X[n-1]$ and $X[n-2]$. The orthogonality conditions are therefore:\n$$\n\\mathbb{E}\\{e[n]X[n-1]\\} = 0\n$$\n$$\n\\mathbb{E}\\{e[n]X[n-2]\\} = 0\n$$\n\nWe substitute the expression for the error, $e[n] = X[n] - a_{1}X[n-1] - a_{2}X[n-2]$, into these two equations.\n\nFor the first condition:\n$$\n\\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n-1]\\} = 0\n$$\nBy linearity of the expectation operator:\n$$\n\\mathbb{E}\\{X[n]X[n-1]\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n-1]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n-1]\\} = 0\n$$\nUsing the definition of the autocorrelation function, $R_{X}[k] \\triangleq \\mathbb{E}\\{X[m]X[m-k]\\}$, and the WSS property, this becomes:\n$$\nR_{X}[1] - a_{1}R_{X}[0] - a_{2}R_{X}[-1] = 0\n$$\n\nFor the second condition:\n$$\n\\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n-2]\\} = 0\n$$\n$$\n\\mathbb{E}\\{X[n]X[n-2]\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n-2]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n-2]\\} = 0\n$$\nThis translates to:\n$$\nR_{X}[2] - a_{1}R_{X}[1] - a_{2}R_{X}[0] = 0\n$$\n\nFor a real-valued process, the autocorrelation function is an even function, so $R_{X}[-k] = R_{X}[k]$. The two equations, known as the normal equations (or Yule-Walker equations in this context), can be rearranged as:\n$$\na_{1}R_{X}[0] + a_{2}R_{X}[1] = R_{X}[1]\n$$\n$$\na_{1}R_{X}[1] + a_{2}R_{X}[0] = R_{X}[2]\n$$\n\nWe are given the values $R_{X}[0]=1$, $R_{X}[1]=0.7$, and $R_{X}[2]=0.49$. Substituting these into the normal equations:\n$$\na_{1}(1) + a_{2}(0.7) = 0.7\n$$\n$$\na_{1}(0.7) + a_{2}(1) = 0.49\n$$\nThis is a system of two linear equations in two variables. In matrix form:\n$$\n\\begin{pmatrix} 1  0.7 \\\\ 0.7  1 \\end{pmatrix} \\begin{pmatrix} a_{1} \\\\ a_{2} \\end{pmatrix} = \\begin{pmatrix} 0.7 \\\\ 0.49 \\end{pmatrix}\n$$\nFrom the second equation, $a_{2} = 0.49 - 0.7a_{1}$. Substituting this into the first equation:\n$$\na_{1} + 0.7(0.49 - 0.7a_{1}) = 0.7\n$$\n$$\na_{1} + 0.343 - 0.49a_{1} = 0.7\n$$\n$$\n0.51a_{1} = 0.7 - 0.343 = 0.357\n$$\n$$\na_{1} = \\frac{0.357}{0.51} = \\frac{357}{510} = \\frac{7 \\times 51}{10 \\times 51} = \\frac{7}{10} = 0.7\n$$\nNow, we find $a_{2}$:\n$$\na_{2} = 0.49 - 0.7(0.7) = 0.49 - 0.49 = 0\n$$\nThe unique predictor coefficients are $a_{1}=0.7$ and $a_{2}=0$. The optimal order-$2$ predictor degenerates to an order-$1$ predictor due to the specific autocorrelation values provided.\n\nNext, we calculate the innovation variance, $\\sigma_{e}^{2}$, which is the minimum MSE:\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]^{2}\\} = \\mathbb{E}\\{e[n] \\cdot e[n]\\} = \\mathbb{E}\\{e[n](X[n] - \\hat{X}[n])\\}\n$$\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]X[n]\\} - \\mathbb{E}\\{e[n]\\hat{X}[n]\\}\n$$\nThe second term, $\\mathbb{E}\\{e[n]\\hat{X}[n]\\}$, is $\\mathbb{E}\\{e[n](a_{1}X[n-1] + a_{2}X[n-2])\\} = a_{1}\\mathbb{E}\\{e[n]X[n-1]\\} + a_{2}\\mathbb{E}\\{e[n]X[n-2]\\}$. By the orthogonality principle, both expectation terms are zero. Thus, $\\mathbb{E}\\{e[n]\\hat{X}[n]\\} = 0$.\nThe variance simplifies to:\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{e[n]X[n]\\} = \\mathbb{E}\\{(X[n] - a_{1}X[n-1] - a_{2}X[n-2])X[n]\\}\n$$\n$$\n\\sigma_{e}^{2} = \\mathbb{E}\\{X[n]^{2}\\} - a_{1}\\mathbb{E}\\{X[n-1]X[n]\\} - a_{2}\\mathbb{E}\\{X[n-2]X[n]\\}\n$$\n$$\n\\sigma_{e}^{2} = R_{X}[0] - a_{1}R_{X}[-1] - a_{2}R_{X}[-2] = R_{X}[0] - a_{1}R_{X}[1] - a_{2}R_{X}[2]\n$$\nSubstituting the known values for the autocorrelation function and the computed coefficients:\n$$\n\\sigma_{e}^{2} = 1 - (0.7)(0.7) - (0)(0.49) = 1 - 0.49 = 0.51\n$$\n\nThe required quantities are therefore $a_{1}=0.7$, $a_{2}=0$, and $\\sigma_{e}^{2}=0.51$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7  0  0.51\n\\end{pmatrix}\n}\n$$", "id": "2916954"}]}