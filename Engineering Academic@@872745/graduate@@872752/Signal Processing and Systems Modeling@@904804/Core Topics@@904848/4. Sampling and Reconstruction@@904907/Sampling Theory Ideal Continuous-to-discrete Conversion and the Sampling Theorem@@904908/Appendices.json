{"hands_on_practices": [{"introduction": "Understanding the impact of sampling on a signal's power spectral density (PSD) is a cornerstone of advanced signal processing. This exercise guides you from first principles to derive the fundamental formula that relates the PSD of a continuous-time random process to that of its sampled version [@problem_id:2902615]. By completing this derivation, you will gain a deep, analytical insight into the mechanism of aliasing and establish the theoretical groundwork for analyzing the performance of any system involving sampled random signals.", "problem": "Let $x(t)$ be a zero-mean wide-sense stationary (WSS) continuous-time random process with autocorrelation function $R_{x}(\\tau)$ and power spectral density (PSD) $S_{x}(\\omega)$, defined by the continuous-time Fourier transform (CTFT) relation $S_{x}(\\omega)=\\int_{-\\infty}^{\\infty}R_{x}(\\tau)\\exp(-j\\omega\\tau)\\,d\\tau$. The process is uniformly sampled with sampling period $T0$ to form the discrete-time sequence $x_{d}[n]=x(nT)$. Let $R_{x_{d}}[m]$ denote the autocorrelation sequence of $x_{d}[n]$ and $S_{x_{d}}(\\exp(j\\Omega))$ denote its discrete-time power spectral density, defined as the discrete-time Fourier transform (DTFT) of $R_{x_{d}}[m]$.\n\nAssume $S_{x}(\\omega)$ is not bandlimited. Work from first principles and fundamental definitions only; do not assume any pre-derived sampling identities.\n\n1) Starting from the definitions of $R_{x_{d}}[m]$ and $S_{x_{d}}(\\exp(j\\Omega))$, express $S_{x_{d}}(\\exp(j\\Omega))$ in terms of $R_{x}(\\tau)$ and then, by relating sums over samples to integrals via the continuous-time Fourier transform, derive an explicit expression of $S_{x_{d}}(\\exp(j\\Omega))$ in terms of $S_{x}(\\omega)$ that makes the aliasing mechanism explicit across $2\\pi$-spaced discrete-time frequency intervals.\n\n2) Using your expression from part 1), derive a condition on $S_{x}(\\omega)$ under which the variance of the sampled sequence, $\\sigma_{x_{d}}^{2}=R_{x_{d}}[0]$, is finite, and show that when this condition holds,\n$$\n\\sigma_{x_{d}}^{2}=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}S_{x}(\\omega)\\,d\\omega.\n$$\nClearly state any integrability assumptions you invoke to justify interchanging sums and integrals.\n\n3) Consider the specific non-bandlimited PSD\n$$\nS_{x}(\\omega)=\\frac{2D}{\\omega^{2}+\\omega_{c}^{2}},\n$$\nwith fixed positive constants $D0$ and $\\omega_{c}0$. For sampling period $T0$, compute the discrete-time variance $\\sigma_{x_{d}}^{2}$ in closed form as a function of $D$ and $\\omega_{c}$. Express your final result as a symbolic expression in $D$ and $\\omega_{c}$ only. Do not include units in your final boxed answer; interpret the variance in the same units as the square of $x(t)$.", "solution": "The problem shall be considered in three parts, as specified. The analysis will proceed from fundamental definitions of correlation functions and power spectral densities for stationary random processes.\n\nPart 1: Derivation of the discrete-time Power Spectral Density\n\nWe are given a continuous-time, zero-mean, wide-sense stationary (WSS) random process $x(t)$ with autocorrelation function $R_x(\\tau) = E[x(t+\\tau)x^*(t)]$. The sampled sequence is $x_d[n] = x(nT)$ for a sampling period $T0$. The autocorrelation sequence of the discrete-time process $x_d[n]$ is defined as $R_{x_d}[m] = E[x_d[n+m]x_d^*[n]]$.\n\nSubstituting the definition of $x_d[n]$:\n$$R_{x_d}[m] = E[x((n+m)T)x^*(nT)]$$\nSince $x(t)$ is WSS, its autocorrelation depends only on the time difference, which is $(n+m)T - nT = mT$. Therefore,\n$$R_{x_d}[m] = R_x(mT)$$\nThis shows that the autocorrelation sequence of the sampled process is the sampled version of the continuous-time autocorrelation function.\n\nThe power spectral density (PSD) of the discrete-time process, $S_{x_d}(\\exp(j\\Omega))$, is the discrete-time Fourier transform (DTFT) of its autocorrelation sequence $R_{x_d}[m]$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} R_{x_d}[m] \\exp(-j\\Omega m)$$\nSubstituting $R_{x_d}[m] = R_x(mT)$, we get the desired expression in terms of $R_x(\\tau)$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} R_x(mT) \\exp(-j\\Omega m)$$\nTo relate this to the continuous-time PSD, $S_x(\\omega)$, we express $R_x(\\tau)$ using the inverse continuous-time Fourier transform (CTFT) of $S_x(\\omega)$:\n$$R_x(\\tau) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\exp(j\\omega\\tau) d\\omega$$\nSubstituting this expression for $\\tau = mT$ into the sum for $S_{x_d}(\\exp(j\\Omega))$:\n$$S_{x_d}(\\exp(j\\Omega)) = \\sum_{m=-\\infty}^{\\infty} \\left[ \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\exp(j\\omega mT) d\\omega \\right] \\exp(-j\\Omega m)$$\nAssuming that we can interchange the order of summation and integration (which can be justified, for instance, by Fubini's theorem if $\\int \\sum |...|  \\infty$), we have:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ \\sum_{m=-\\infty}^{\\infty} \\exp(j\\omega mT - j\\Omega m) \\right] d\\omega$$\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ \\sum_{m=-\\infty}^{\\infty} \\exp(jm(\\omega T - \\Omega)) \\right] d\\omega$$\nThe sum inside the bracket is a representation of the Dirac comb function, a periodic train of impulses. This is a key identity from the theory of distributions or generalized functions, related to the Poisson summation formula:\n$$\\sum_{m=-\\infty}^{\\infty} \\exp(jm\\phi) = 2\\pi \\sum_{k=-\\infty}^{\\infty} \\delta(\\phi - 2\\pi k)$$\nwhere $\\delta(\\cdot)$ is the Dirac delta function. In our expression, $\\phi = \\omega T - \\Omega$. Substituting this identity into the integral:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\left[ 2\\pi \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega T - \\Omega - 2\\pi k) \\right] d\\omega$$\n$$S_{x_d}(\\exp(j\\Omega)) = \\int_{-\\infty}^{\\infty} S_x(\\omega) \\sum_{k=-\\infty}^{\\infty} \\delta(\\omega T - (\\Omega + 2\\pi k)) d\\omega$$\nUsing the scaling property of the Dirac delta function, $\\delta(ax) = \\frac{1}{|a|}\\delta(x)$, with $a=T0$:\n$$\\delta(\\omega T - (\\Omega + 2\\pi k)) = \\frac{1}{T} \\delta\\left(\\omega - \\frac{\\Omega + 2\\pi k}{T}\\right)$$\nSubstituting this back and interchanging the sum and integral:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} S_x(\\omega) \\delta\\left(\\omega - \\frac{\\Omega + 2\\pi k}{T}\\right) d\\omega$$\nUsing the sifting property of the delta function, $\\int_{-\\infty}^{\\infty} f(x) \\delta(x-x_0) dx = f(x_0)$, the integral evaluates to $S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right)$. This yields the final expression:\n$$S_{x_d}(\\exp(j\\Omega)) = \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right)$$\nThis formula explicitly shows the aliasing mechanism: the discrete-time PSD at normalized frequency $\\Omega$ is the sum of the continuous-time PSD values at all frequencies $\\omega_k = \\frac{\\Omega}{T} + k\\frac{2\\pi}{T}$ (where $\\frac{2\\pi}{T}$ is the sampling frequency in rad/s), scaled by $\\frac{1}{T}$.\n\nPart 2: Condition for Finite Variance\n\nThe variance of the discrete-time sequence, $\\sigma_{x_d}^2$, is given by its autocorrelation at lag $m=0$: $\\sigma_{x_d}^2 = R_{x_d}[0]$. According to the Wiener-Khinchin theorem for discrete-time processes, this is also given by the integral of the PSD over one period:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_{x_d}(\\exp(j\\Omega)) d\\Omega$$\nUsing the expression for $S_{x_d}(\\exp(j\\Omega))$ derived in Part 1:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\left[ \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) \\right] d\\Omega$$\nWe invoke the assumption that the sum and integral can be interchanged. This is justified by Tonelli's theorem because the PSD $S_x(\\omega)$ is a non-negative function, making the integrand non-negative.\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi T} \\sum_{k=-\\infty}^{\\infty} \\int_{-\\pi}^{\\pi} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) d\\Omega$$\nFor each integral in the sum, we perform a change of variable. Let $u = \\frac{\\Omega + 2\\pi k}{T}$. Then $d\\Omega = T du$. The limits of integration for $\\Omega$ from $-\\pi$ to $\\pi$ correspond to limits for $u$ from $\\frac{-\\pi + 2\\pi k}{T}$ to $\\frac{\\pi + 2\\pi k}{T}$.\n$$\\int_{-\\pi}^{\\pi} S_x\\left(\\frac{\\Omega + 2\\pi k}{T}\\right) d\\Omega = \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) (T du) = T \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du$$\nSubstituting this result into the expression for $\\sigma_{x_d}^2$:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi T} \\sum_{k=-\\infty}^{\\infty} T \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du = \\frac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du$$\nThe summation is over a set of contiguous, non-overlapping intervals that tile the entire real line $(-\\infty, \\infty)$. Therefore, the sum of integrals is equivalent to a single integral over the entire real line:\n$$\\sum_{k=-\\infty}^{\\infty} \\int_{\\frac{2\\pi k - \\pi}{T}}^{\\frac{2\\pi k + \\pi}{T}} S_x(u) du = \\int_{-\\infty}^{\\infty} S_x(u) du$$\nThis leads to the result:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega$$\nFor the variance $\\sigma_{x_d}^2$ to be finite, the integral on the right-hand side must converge. Since a PSD is always non-negative ($S_x(\\omega) \\ge 0$), the condition for finite variance is that $S_x(\\omega)$ must be integrable over $\\mathbb{R}$:\n$$\\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega  \\infty$$\nWe also observe that the variance of the continuous-time process is $\\sigma_x^2 = R_x(0) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega$. Thus, $\\sigma_{x_d}^2 = \\sigma_x^2$, which is an expected result, as sampling does not alter the instantaneous power of the signal.\n\nPart 3: Variance for a Specific PSD\n\nWe are given the PSD $S_x(\\omega) = \\frac{2D}{\\omega^2 + \\omega_c^2}$, with constants $D0$ and $\\omega_c0$. We must compute the variance of the sampled sequence, $\\sigma_{x_d}^2 = R_{x_d}[0]$.\nFrom Part 1, we know $R_{x_d}[0] = R_x(0)$. Thus, the variance of the sampled sequence is identical to the variance of the continuous-time process. We can find this by first determining the autocorrelation function $R_x(\\tau)$ via the inverse CTFT of $S_x(\\omega)$.\n\nWe use the standard Fourier transform pair:\n$$\\mathcal{F}\\{\\exp(-a|\\tau|)\\} = \\int_{-\\infty}^{\\infty} \\exp(-a|\\tau|) \\exp(-j\\omega\\tau) d\\tau = \\frac{2a}{\\omega^2 + a^2}$$\nComparing this with the given $S_x(\\omega)$, we can write:\n$$S_x(\\omega) = \\frac{2D}{\\omega^2 + \\omega_c^2} = \\frac{D}{\\omega_c} \\left( \\frac{2\\omega_c}{\\omega^2 + \\omega_c^2} \\right)$$\nBy inspection, we identify $a = \\omega_c$. The autocorrelation function is therefore:\n$$R_x(\\tau) = \\frac{D}{\\omega_c} \\exp(-\\omega_c|\\tau|)$$\nThe variance is found by evaluating $R_x(\\tau)$ at $\\tau=0$:\n$$\\sigma_{x_d}^2 = R_x(0) = \\frac{D}{\\omega_c} \\exp(-\\omega_c \\cdot |0|) = \\frac{D}{\\omega_c}$$\nThis result is independent of the sampling period $T$, as expected.\n\nAlternatively, using the integral expression derived in Part 2:\n$$\\sigma_{x_d}^2 = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_x(\\omega) d\\omega = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{2D}{\\omega^2 + \\omega_c^2} d\\omega$$\n$$\\sigma_{x_d}^2 = \\frac{D}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{\\omega^2 + \\omega_c^2} d\\omega$$\nThe integral is a standard form:\n$$\\int \\frac{1}{\\omega^2 + a^2} d\\omega = \\frac{1}{a} \\arctan\\left(\\frac{\\omega}{a}\\right)$$\nEvaluating the definite integral:\n$$\\int_{-\\infty}^{\\infty} \\frac{1}{\\omega^2 + \\omega_c^2} d\\omega = \\left[ \\frac{1}{\\omega_c} \\arctan\\left(\\frac{\\omega}{\\omega_c}\\right) \\right]_{-\\infty}^{\\infty} = \\frac{1}{\\omega_c} \\left(\\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right)\\right) = \\frac{\\pi}{\\omega_c}$$\nSubstituting this back into the expression for the variance:\n$$\\sigma_{x_d}^2 = \\frac{D}{\\pi} \\left(\\frac{\\pi}{\\omega_c}\\right) = \\frac{D}{\\omega_c}$$\nBoth methods yield the same result, confirming the consistency of the derivations.", "answer": "$$\\boxed{\\frac{D}{\\omega_{c}}}$$", "id": "2902615"}, {"introduction": "Ideal models are instructive, but real-world systems are constrained by non-ideal components. Building upon the principles of spectral aliasing, this practice addresses a crucial engineering question: what is the total reconstruction error when a non-bandlimited signal is processed by a practical anti-aliasing filter and then sampled? [@problem_id:2902649]. By calculating the mean-square error, you will develop a quantitative framework for analyzing the fidelity of practical data acquisition systems.", "problem": "Consider a real-valued, zero-mean, wide-sense stationary (WSS) random process with power spectral density (PSD) $S_{x}(f)$ given by\n$$\nS_{x}(f)=\n\\begin{cases}\nS_{0},  |f|\\leq B,\\\\\nS_{0}\\left(\\dfrac{B}{|f|}\\right)^{2},  |f|B,\n\\end{cases}\n$$\nwhere $S_{0}0$ and $B0$ are constants. The process is passed through an analog anti-aliasing filter with frequency response $H_{a}(f)$ satisfying\n$$\nH_{a}(f)=\n\\begin{cases}\n1,  |f|\\leq B,\\\\\n\\alpha,  |f|B,\n\\end{cases}\n$$\nwith $0\\alpha1$. The filtered signal is then sampled at the rate $f_{s}=2B$, and reconstructed by an ideal continuous-time brick-wall lowpass filter of unit gain and cutoff frequency $B$.\n\nUsing only fundamental definitions from sampling theory and stochastic processes (namely, the frequency-domain replication induced by ideal impulse sampling, the effect of linear time-invariant filtering on PSDs, and the relationship between the PSD of a WSS process and its mean-square value), derive the mean-square reconstruction error\n$$\n\\mathbb{E}\\big[(x(t)-y(t))^{2}\\big],\n$$\nwhere $x(t)$ is the original process and $y(t)$ is the reconstructed signal. Express your final answer as a single closed-form symbolic expression in terms of $S_{0}$, $B$, and $\\alpha$. No numerical evaluation or rounding is required. State your answer in power units consistent with $S_{0}$ (do not include the unit symbol in the final boxed expression).", "solution": "The objective is to compute the mean-square reconstruction error, $\\mathcal{E}$, defined as $\\mathbb{E}[(x(t)-y(t))^{2}]$, where $x(t)$ is the original wide-sense stationary (WSS) random process and $y(t)$ is the signal reconstructed after anti-aliasing filtering, sampling, and low-pass filtering. For a WSS process, the mean-square value is equal to the total power, which can be found by integrating the power spectral density (PSD) over all frequencies. Let $e(t) = x(t) - y(t)$ be the error signal. The mean-square error is then $\\mathcal{E} = R_e(0) = \\int_{-\\infty}^{\\infty} S_e(f) df$, where $S_e(f)$ is the PSD of the error signal.\n\nWe can decompose the total error into two components. Let $x_B(t)$ be the ideal bandlimited version of the original signal $x(t)$, obtained by passing $x(t)$ through an ideal low-pass filter with cutoff frequency $B$. The error can then be written as:\n$$\ne(t) = x(t) - y(t) = (x(t) - x_B(t)) + (x_B(t) - y(t))\n$$\nThe first term, $e_{out}(t) = x(t) - x_B(t)$, represents the components of the original signal $x(t)$ that lie outside the reconstruction band $[-B, B]$. Its spectrum is non-zero only for $|f|  B$.\nThe second term, $e_{in}(t) = x_B(t) - y(t)$, represents the difference between the ideal in-band signal and the reconstructed signal. Both $x_B(t)$ and $y(t)$ are, by definition, bandlimited to the interval $[-B, B]$. Therefore, the spectrum of $e_{in}(t)$ is non-zero only for $|f| \\leq B$.\n\nSince the two error components $e_{out}(t)$ and $e_{in}(t)$ occupy disjoint frequency bands, they are orthogonal. The total mean-square error is the sum of their individual powers:\n$$\n\\mathcal{E} = \\mathbb{E}[e(t)^2] = \\mathbb{E}[e_{out}(t)^2] + \\mathbb{E}[e_{in}(t)^2]\n$$\n\nFirst, we calculate the power of the out-of-band error, $\\mathcal{E}_{out} = \\mathbb{E}[e_{out}(t)^2]$. This is the power of the original signal $x(t)$ outside the frequency band $[-B, B]$.\n$$\n\\mathcal{E}_{out} = \\int_{|f|B} S_x(f) df = 2 \\int_{B}^{\\infty} S_x(f) df\n$$\nUsing the given PSD $S_x(f) = S_{0}(B/|f|)^{2}$ for $|f|B$:\n$$\n\\mathcal{E}_{out} = 2 \\int_{B}^{\\infty} S_{0} \\frac{B^2}{f^2} df = 2 S_0 B^2 \\left[ -\\frac{1}{f} \\right]_{B}^{\\infty} = 2 S_0 B^2 \\left( 0 - \\left(-\\frac{1}{B}\\right) \\right) = 2 S_0 B\n$$\n\nSecond, we calculate the power of the in-band error, $\\mathcal{E}_{in} = \\mathbb{E}[e_{in}(t)^2]$. This error arises from the imperfections of the sampling and reconstruction process within the band $[-B, B]$. The ideal in-band signal is $x_B(t)$, with PSD $S_{x_B}(f) = S_x(f) = S_0$ for $|f| \\le B$. The reconstructed signal $y(t)$ is generated from the samples of the anti-aliased signal, $x_a(t)$. The PSD of $x_a(t)$ is $S_{x_a}(f) = |H_a(f)|^2 S_x(f)$.\nGiven $H_a(f)$ and $S_x(f)$, we have:\n$$\nS_{x_a}(f) =\n\\begin{cases}\n1^2 \\cdot S_0,  |f|\\leq B \\\\\n\\alpha^2 \\cdot S_{0}\\left(\\dfrac{B}{|f|}\\right)^{2},  |f|B\n\\end{cases}\n=\n\\begin{cases}\nS_0,  |f|\\leq B \\\\\n\\alpha^2 S_{0}\\dfrac{B^2}{f^2},  |f|B\n\\end{cases}\n$$\nWhen $x_a(t)$ is sampled at $f_s = 2B$ and reconstructed with an ideal low-pass filter with cutoff $B$, the PSD of the resulting signal $y(t)$ in the band $|f| \\le B$ is the sum of the baseband spectrum and the folded (aliased) spectra from higher frequencies.\n$$\nS_y(f) = \\sum_{k=-\\infty}^{\\infty} S_{x_a}(f - kf_s) = \\sum_{k=-\\infty}^{\\infty} S_{x_a}(f - 2kB) \\quad \\text{for } |f| \\le B\n$$\nWe can separate the $k=0$ term (baseband) from the $k \\neq 0$ terms (aliasing).\n$$\nS_y(f) = S_{x_a}(f) + \\sum_{k \\neq 0} S_{x_a}(f - 2kB)\n$$\nFor $|f| \\leq B$, $S_{x_a}(f) = S_0$. This corresponds to the correctly reconstructed part of the signal. The ideal signal $x_B(t)$ has this same PSD. The error $e_{in}(t) = x_B(t) - y(t)$ is thus due to the aliasing terms. The reconstructed signal can be viewed as $y(t) = y_{baseband}(t) + y_{alias}(t)$, where $y_{baseband}(t)$ corresponds to the ideal reconstruction of the in-band signal (which is $x_B(t)$ since $H_a(f)=1$ for $|f|\\le B$), and $y_{alias}(t)$ is the noise due to aliasing.\nSo, $e_{in}(t) = x_B(t) - (x_B(t) + y_{alias}(t)) = -y_{alias}(t)$. The power of the in-band error is therefore the power of the aliasing noise.\nThe PSD of the aliasing noise is $S_{alias}(f) = \\sum_{k \\neq 0} S_{x_a}(f - 2kB)$.\n$$\n\\mathcal{E}_{in} = \\int_{-B}^{B} S_{alias}(f) df = \\int_{-B}^{B} \\sum_{k \\neq 0} S_{x_a}(f - 2kB) df\n$$\nFor $|f| \\le B$ and $k \\neq 0$, the argument $f - 2kB$ is always outside $[-B, B]$. Thus, we must use $S_{x_a}(f) = \\alpha^2 S_0 B^2/f^2$.\n$$\n\\mathcal{E}_{in} = \\int_{-B}^{B} \\sum_{k=1}^{\\infty} [S_{x_a}(f - 2kB) + S_{x_a}(f + 2kB)] df\n$$\n$$\n\\mathcal{E}_{in} = \\alpha^2 S_0 B^2 \\sum_{k=1}^{\\infty} \\int_{-B}^{B} \\left( \\frac{1}{(f-2kB)^2} + \\frac{1}{(f+2kB)^2} \\right) df\n$$\nWe evaluate the integral for a fixed $k$:\n$$\n\\int_{-B}^{B} \\frac{1}{(f-2kB)^2} df = \\left[ -\\frac{1}{f-2kB} \\right]_{-B}^{B} = -\\frac{1}{B-2kB} + \\frac{1}{-B-2kB} = \\frac{1}{2kB-B} - \\frac{1}{2kB+B} = \\frac{1}{B}\\left( \\frac{1}{2k-1} - \\frac{1}{2k+1} \\right) = \\frac{2}{B(4k^2-1)}\n$$\nThe integral of the second term is identical due to symmetry.\n$$\n\\int_{-B}^{B} \\left( \\frac{1}{(f-2kB)^2} + \\frac{1}{(f+2kB)^2} \\right) df = \\frac{2}{B(4k^2-1)} + \\frac{2}{B(4k^2-1)} = \\frac{4}{B(4k^2-1)}\n$$\nNow, substitute this back into the expression for $\\mathcal{E}_{in}$:\n$$\n\\mathcal{E}_{in} = \\alpha^2 S_0 B^2 \\sum_{k=1}^{\\infty} \\frac{4}{B(4k^2-1)} = 4 \\alpha^2 S_0 B \\sum_{k=1}^{\\infty} \\frac{1}{4k^2-1}\n$$\nThe sum is a telescoping series:\n$$\n\\sum_{k=1}^{\\infty} \\frac{1}{4k^2-1} = \\sum_{k=1}^{\\infty} \\frac{1}{(2k-1)(2k+1)} = \\frac{1}{2} \\sum_{k=1}^{\\infty} \\left( \\frac{1}{2k-1} - \\frac{1}{2k+1} \\right)\n$$\n$$\n= \\frac{1}{2} \\left[ \\left(1 - \\frac{1}{3}\\right) + \\left(\\frac{1}{3} - \\frac{1}{5}\\right) + \\left(\\frac{1}{5} - \\frac{1}{7}\\right) + \\dots \\right] = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n$$\nThe in-band aliasing error power is therefore:\n$$\n\\mathcal{E}_{in} = 4 \\alpha^2 S_0 B \\left(\\frac{1}{2}\\right) = 2 \\alpha^2 S_0 B\n$$\nThe total mean-square error is the sum of the out-of-band and in-band error powers:\n$$\n\\mathcal{E} = \\mathcal{E}_{out} + \\mathcal{E}_{in} = 2 S_0 B + 2 \\alpha^2 S_0 B = 2 S_0 B (1 + \\alpha^2)\n$$\nThis expression represents the total average power of the difference between the original signal and its reconstruction.", "answer": "$$\\boxed{2S_{0}B(1 + \\alpha^{2})}$$", "id": "2902649"}, {"introduction": "The theoretical ideal of sinc reconstruction requires an infinitely long, non-causal low-pass filter, which is physically unrealizable. This exercise transitions from theory to practical implementation by tasking you with the design of a finite impulse response (FIR) reconstruction filter using the windowed-sinc method [@problem_id:2902671]. You will apply standard design formulas for the Kaiser window to translate performance specifications—passband ripple and stopband attenuation—into the minimum required filter length, a critical parameter in hardware and software design.", "problem": "A continuous-time real signal $x_{c}(t)$ is strictly bandlimited to $B = 20\\,\\text{kHz}$ and is sampled uniformly at a rate $f_{s} = 48\\,\\text{kHz}$ to obtain the discrete-time sequence $x[n] = x_{c}(n T_{s})$ with $T_{s} = 1/f_{s}$. To reconstruct $x_{c}(t)$ from $x[n]$ using ideal continuous-to-discrete conversion principles, an ideal lowpass filter of cutoff frequency $B$ would be used. In practice, you are tasked to implement a time-discrete reconstruction stage by convolving $x[n]$ with a symmetric, odd-length, windowed-sinc finite impulse response (FIR) kernel $h[n]$ that approximates the ideal lowpass of bandwidth $B$.\n\nYour design must satisfy the following amplitude specifications on the discrete-time frequency response $H(\\exp(j\\omega))$ of the FIR kernel when interpreted with frequency normalized by the sampling rate:\n- Passband: for all continuous-time frequencies $f$ with $0 \\le f \\le 20\\,\\text{kHz}$ (equivalently $0 \\le \\omega \\le 2\\pi \\cdot 20/48$), the passband ripple must not exceed a linear magnitude of $\\delta_{p} = 0.001$.\n- Stopband: for all continuous-time frequencies $f$ with $f \\ge 22\\,\\text{kHz}$ (equivalently $\\omega \\ge 2\\pi \\cdot 22/48$), the attenuation must be at least $A_{s} = 80\\,\\text{dB}$.\n\nYou are to use a Kaiser-windowed sinc design for the lowpass approximation, and you must dimension the kernel to meet the stated passband ripple and stopband attenuation. Determine the minimum odd integer kernel length $L$ (number of taps) required to satisfy these specifications. Report the smallest such $L$ as an integer with no units. No rounding instructions are needed because $L$ is an integer by definition.", "solution": "The problem requires the determination of the minimum odd integer length $L$ for a symmetric Finite Impulse Response (FIR) filter. The design is to be performed using the windowed-sinc method with a Kaiser window, and it must satisfy given amplitude specifications in the frequency domain.\n\nFirst, we must formalize the given specifications into parameters suitable for the Kaiser window design formulas.\n\nThe specifications are:\n- Passband edge frequency: $f_{p} = 20\\,\\text{kHz}$\n- Stopband edge frequency: $f_{sp} = 22\\,\\text{kHz}$\n- Sampling frequency: $f_{s} = 48\\,\\text{kHz}$\n- Maximum passband ripple (linear): $\\delta_{p} = 0.001$\n- Minimum stopband attenuation: $A_{s} = 80\\,\\text{dB}$\n\nThe design process begins by converting the continuous-time frequencies into normalized discrete-time frequencies, $\\omega$, which are measured in radians per sample. The conversion formula is $\\omega = 2\\pi \\frac{f}{f_{s}}$. The range for unique positive frequencies is $[0, \\pi]$.\n\nThe normalized passband edge frequency $\\omega_{p}$ is:\n$$ \\omega_{p} = 2\\pi \\frac{f_{p}}{f_{s}} = 2\\pi \\frac{20000}{48000} = 2\\pi \\frac{5}{12} = \\frac{5\\pi}{6} $$\nThe normalized stopband edge frequency $\\omega_{s}$ is:\n$$ \\omega_{s} = 2\\pi \\frac{f_{sp}}{f_{s}} = 2\\pi \\frac{22000}{48000} = 2\\pi \\frac{11}{24} = \\frac{11\\pi}{12} $$\nThe transition bandwidth $\\Delta\\omega$ is the difference between the stopband and passband edge frequencies:\n$$ \\Delta\\omega = \\omega_{s} - \\omega_{p} = \\frac{11\\pi}{12} - \\frac{10\\pi}{12} = \\frac{\\pi}{12} \\text{ radians/sample} $$\n\nNext, we address the ripple and attenuation specifications. The Kaiser window design formulas use a single parameter $\\delta$ to characterize the ripple in both the passband and stopband. To ensure both given specifications are met, we must use the more stringent of the two.\n\nThe passband ripple is given as $\\delta_{p} = 0.001$.\nThe stopband attenuation $A_{s}$ is related to the stopband ripple $\\delta_{s}$ by the formula:\n$$ A_{s} = -20 \\log_{10}(\\delta_{s}) $$\nWe can solve for $\\delta_{s}$:\n$$ \\delta_{s} = 10^{-A_{s}/20} = 10^{-80/20} = 10^{-4} = 0.0001 $$\nTo satisfy both constraints, we must select the smaller of the two ripple values for our design parameter $\\delta$:\n$$ \\delta = \\min(\\delta_{p}, \\delta_{s}) = \\min(0.001, 0.0001) = 0.0001 $$\n\nFrom this ripple parameter $\\delta$, we determine the attenuation parameter $A$ required for the Kaiser window formulas:\n$$ A = -20 \\log_{10}(\\delta) = -20 \\log_{10}(0.0001) = -20 \\log_{10}(10^{-4}) = (-20)(-4) = 80 $$\nThis parameter $A$, conventionally expressed in decibels, governs the trade-off between the main lobe width and the side lobe attenuation of the window function.\n\nWith the parameters $A$ and $\\Delta\\omega$ determined, we can estimate the required filter order $M$ (where the filter length is $L = M+1$) using the established empirical formula for the Kaiser window:\n$$ M \\geq \\frac{A - 8}{2.285 \\Delta\\omega} $$\nSubstituting our calculated values:\n$$ M \\geq \\frac{80 - 8}{2.285 \\left(\\frac{\\pi}{12}\\right)} = \\frac{72 \\times 12}{2.285 \\pi} = \\frac{864}{2.285 \\pi} $$\nEvaluating this expression numerically:\n$$ M \\geq \\frac{864}{7.1785...} \\approx 120.36 $$\nThe problem requires a symmetric FIR kernel with an odd length $L$. An odd-length symmetric filter is a Type I FIR filter, for which the order $M$ must be an even integer. Therefore, we must find the smallest even integer $M$ that satisfies the inequality $M \\geq 120.36$.\n\nThe smallest even integer greater than or equal to $120.36$ is $M = 122$.\n\nFinally, the length of the FIR filter kernel, $L$, is related to its order $M$ by $L = M+1$. With $M = 122$, the minimum required length is:\n$$ L = 122 + 1 = 123 $$\nThis value is an odd integer, as stipulated by the problem statement. Thus, a kernel length of $123$ taps is the minimum required to meet the given design specifications.", "answer": "$$\n\\boxed{123}\n$$", "id": "2902671"}]}