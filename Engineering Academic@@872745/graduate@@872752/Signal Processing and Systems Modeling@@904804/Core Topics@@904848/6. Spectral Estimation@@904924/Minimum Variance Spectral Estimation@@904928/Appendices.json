{"hands_on_practices": [{"introduction": "The heart of the MVDR spectral estimator is the computation of the quadratic form $\\mathbf{a}^H \\mathbf{R}^{-1} \\mathbf{a}$. A direct calculation involving matrix inversion is computationally expensive and numerically unstable, especially for large matrices. This first exercise [@problem_id:2883219] explores a much more efficient and robust method using Cholesky factorization, which is fundamental to any practical implementation of the MVDR algorithm.", "problem": "In minimum variance (Capon) spectral estimation (also known as Minimum Variance Distortionless Response (MVDR)), the denominator involves the quadratic form $\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)$, where $\\mathbf{R} \\in \\mathbb{C}^{M \\times M}$ is a Hermitian positive definite covariance matrix and $\\mathbf{a}(\\omega) \\in \\mathbb{C}^{M}$ is the frequency-dependent steering vector. Suppose $\\mathbf{R}$ has already been factored by a Cholesky factorization $\\mathbf{R} = \\mathbf{L} \\mathbf{L}^{H}$ with $\\mathbf{L}$ lower triangular with strictly positive diagonal. For computational efficiency and numerical stability, one should avoid forming $\\mathbf{R}^{-1}$ explicitly.\n\nWhich of the following procedures correctly computes the scalar $s = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$ using only triangular solves and vector inner products or norms, without explicitly forming any matrix inverse?\n\nA. Solve $\\mathbf{L} \\mathbf{y} = \\mathbf{a}$ for $\\mathbf{y}$, then solve $\\mathbf{L}^{H} \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$, and finally compute $s = \\mathbf{a}^{H} \\mathbf{x}$.\n\nB. Solve $\\mathbf{L}^{H} \\mathbf{y} = \\mathbf{a}$ for $\\mathbf{y}$, then solve $\\mathbf{L} \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$, and finally compute $s = \\mathbf{a}^{H} \\mathbf{y}$.\n\nC. Solve $\\mathbf{L} \\mathbf{z} = \\mathbf{a}$ for $\\mathbf{z}$, and then compute $s = \\mathbf{z}^{H} \\mathbf{z}$.\n\nD. Solve $\\mathbf{L}^{H} \\mathbf{z} = \\mathbf{a}$ for $\\mathbf{z}$, and then compute $s = \\mathbf{z}^{H} \\mathbf{z}$.\n\nE. Form $\\mathbf{R}^{-1}$ explicitly via $\\mathbf{R}^{-1} = \\mathbf{L}^{-H} \\mathbf{L}^{-1}$, and then compute $s = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$.\n\nSelect all that apply.", "solution": "The problem statement is subjected to validation before any solution is attempted.\n\n### Step 1: Extract Givens\n- The quantity to compute is the scalar $s = \\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)$. For brevity, we will use $s = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$.\n- $\\mathbf{R} \\in \\mathbb{C}^{M \\times M}$ is a Hermitian positive definite covariance matrix.\n- $\\mathbf{a} \\in \\mathbb{C}^{M}$ is a frequency-dependent steering vector.\n- $\\mathbf{R}$ is factored using a Cholesky factorization: $\\mathbf{R} = \\mathbf{L} \\mathbf{L}^{H}$.\n- $\\mathbf{L}$ is a lower triangular matrix with strictly positive diagonal entries.\n- The objective is to find a procedure that computes $s$ using only triangular solves and vector inner products or norms, without explicitly forming any matrix inverse.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It poses a standard question in numerical linear algebra as applied to signal processing. The existence of a unique Cholesky factorization for a Hermitian positive definite matrix is a fundamental theorem. The motivation to avoid explicit matrix inversion for reasons of computational cost and numerical stability is a core principle of modern numerical analysis. The problem statement is self-contained, consistent, and free of any scientific or logical flaws.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We proceed to the solution.\n\nThe objective is to compute the quadratic form $s = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$. We are given the Cholesky factorization $\\mathbf{R} = \\mathbf{L} \\mathbf{L}^{H}$. Substituting this into the expression for $s$ yields:\n$$s = \\mathbf{a}^{H} (\\mathbf{L} \\mathbf{L}^{H})^{-1} \\mathbf{a}$$\nUsing the property of matrix inverses $(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}$, we can write:\n$$(\\mathbf{L} \\mathbf{L}^{H})^{-1} = (\\mathbf{L}^{H})^{-1} \\mathbf{L}^{-1}$$\nIt is common to denote $(\\mathbf{A}^{H})^{-1}$ as $\\mathbf{A}^{-H}$. Thus, the expression for $s$ becomes:\n$$s = \\mathbf{a}^{H} \\mathbf{L}^{-H} \\mathbf{L}^{-1} \\mathbf{a}$$\nThis expression must be computed without forming the inverse matrices $\\mathbf{L}^{-1}$ or $\\mathbf{L}^{-H}$ or $\\mathbf{R}^{-1}$. We can achieve this by defining intermediate vectors and solving systems of linear equations. This can be done in two mathematically equivalent ways.\n\n**Method 1:**\nLet us define an intermediate vector $\\mathbf{z} \\in \\mathbb{C}^{M}$ such that $\\mathbf{z} = \\mathbf{L}^{-1} \\mathbf{a}$. This is equivalent to solving the lower triangular system of equations:\n$$\\mathbf{L} \\mathbf{z} = \\mathbf{a}$$\nThis system can be efficiently solved for $\\mathbf{z}$ using forward substitution, which has a computational complexity of $O(M^2)$.\nSubstituting $\\mathbf{z} = \\mathbf{L}^{-1} \\mathbf{a}$ back into the expression for $s$:\n$$s = \\mathbf{a}^{H} \\mathbf{L}^{-H} (\\mathbf{L}^{-1} \\mathbf{a}) = (\\mathbf{a}^{H} \\mathbf{L}^{-H}) \\mathbf{z}$$\nUsing the property of the conjugate transpose $(\\mathbf{A}\\mathbf{B})^H = \\mathbf{B}^H \\mathbf{A}^H$, we have $(\\mathbf{L}^{-1} \\mathbf{a})^H = \\mathbf{a}^H (\\mathbf{L}^{-1})^H = \\mathbf{a}^H \\mathbf{L}^{-H}$.\nTherefore, $\\mathbf{a}^{H} \\mathbf{L}^{-H} = \\mathbf{z}^{H}$.\nThe expression for $s$ simplifies to:\n$$s = \\mathbf{z}^{H} \\mathbf{z} = \\|\\mathbf{z}\\|_{2}^{2}$$\nThis procedure involves one triangular solve followed by the computation of a vector inner product (or squared L2-norm), which is an $O(M)$ operation.\n\n**Method 2:**\nLet us group the terms in $s = \\mathbf{a}^{H} (\\mathbf{L}^{-H} (\\mathbf{L}^{-1} \\mathbf{a}))$ differently.\nFirst, define an intermediate vector $\\mathbf{y} = \\mathbf{L}^{-1} \\mathbf{a}$. This is equivalent to solving the lower triangular system $\\mathbf{L} \\mathbf{y} = \\mathbf{a}$ using forward substitution.\nThen, the expression becomes $s = \\mathbf{a}^{H} (\\mathbf{L}^{-H} \\mathbf{y})$.\nLet us define a second intermediate vector $\\mathbf{x} = \\mathbf{L}^{-H} \\mathbf{y} = (\\mathbf{L}^{H})^{-1} \\mathbf{y}$. This is equivalent to solving the upper triangular system:\n$$\\mathbf{L}^{H} \\mathbf{x} = \\mathbf{y}$$\nThis system can be efficiently solved for $\\mathbf{x}$ using backward substitution, also an $O(M^2)$ operation.\nBy construction, $\\mathbf{x} = (\\mathbf{L}^{H})^{-1} \\mathbf{y} = (\\mathbf{L}^{H})^{-1} (\\mathbf{L}^{-1} \\mathbf{a}) = (\\mathbf{L} \\mathbf{L}^{H})^{-1} \\mathbf{a} = \\mathbf{R}^{-1} \\mathbf{a}$.\nThe final computation for $s$ is then:\n$$s = \\mathbf{a}^{H} \\mathbf{x}$$\nThis procedure involves two triangular solves (one forward, one backward) and one vector inner product.\n\nNow we evaluate each option based on these derivations.\n\n**Option A. Solve $\\mathbf{L} \\mathbf{y} = \\mathbf{a}$ for $\\mathbf{y}$, then solve $\\mathbf{L}^{H} \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$, and finally compute $s = \\mathbf{a}^{H} \\mathbf{x}$.**\nThis procedure exactly matches Method 2 derived above. The steps are:\n1.  $\\mathbf{y} = \\mathbf{L}^{-1} \\mathbf{a}$ (solved from $\\mathbf{L} \\mathbf{y} = \\mathbf{a}$).\n2.  $\\mathbf{x} = (\\mathbf{L}^{H})^{-1} \\mathbf{y}$ (solved from $\\mathbf{L}^H \\mathbf{x} = \\mathbf{y}$).\n3.  $s = \\mathbf{a}^{H} \\mathbf{x}$.\nSubstituting the intermediate steps, we get $\\mathbf{x} = (\\mathbf{L}^H)^{-1} (\\mathbf{L}^{-1} \\mathbf{a}) = \\mathbf{R}^{-1} \\mathbf{a}$, so the final computation is $s = \\mathbf{a}^{H} (\\mathbf{R}^{-1} \\mathbf{a}) = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$. This is correct. The procedure uses two triangular solves and one inner product, avoiding explicit inversion.\nVerdict: **Correct**.\n\n**Option B. Solve $\\mathbf{L}^{H} \\mathbf{y} = \\mathbf{a}$ for $\\mathbf{y}$, then solve $\\mathbf{L} \\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$, and finally compute $s = \\mathbf{a}^{H} \\mathbf{y}$.**\nLet us analyze the steps:\n1.  $\\mathbf{y} = (\\mathbf{L}^{H})^{-1} \\mathbf{a} = \\mathbf{L}^{-H} \\mathbf{a}$.\n2.  $\\mathbf{x} = \\mathbf{L}^{-1} \\mathbf{y} = \\mathbf{L}^{-1} \\mathbf{L}^{-H} \\mathbf{a}$. This vector $\\mathbf{x}$ is not $\\mathbf{R}^{-1}\\mathbf{a}$.\n3.  $s = \\mathbf{a}^{H} \\mathbf{y} = \\mathbf{a}^{H} (\\mathbf{L}^{-H} \\mathbf{a})$.\nThe correct expression is $s = \\mathbf{a}^{H} \\mathbf{L}^{-H} \\mathbf{L}^{-1} \\mathbf{a}$. The calculated quantity $\\mathbf{a}^{H} \\mathbf{L}^{-H} \\mathbf{a}$ is missing the $\\mathbf{L}^{-1}$ factor and is therefore incorrect.\nVerdict: **Incorrect**.\n\n**Option C. Solve $\\mathbf{L} \\mathbf{z} = \\mathbf{a}$ for $\\mathbf{z}$, and then compute $s = \\mathbf{z}^{H} \\mathbf{z}$.**\nThis procedure exactly matches Method 1 derived above. The steps are:\n1.  $\\mathbf{z} = \\mathbf{L}^{-1} \\mathbf{a}$ (solved from $\\mathbf{L} \\mathbf{z} = \\mathbf{a}$).\n2.  $s = \\mathbf{z}^{H} \\mathbf{z}$.\nSubstituting the expression for $\\mathbf{z}$, we get $s = (\\mathbf{L}^{-1} \\mathbf{a})^{H} (\\mathbf{L}^{-1} \\mathbf{a}) = (\\mathbf{a}^{H} \\mathbf{L}^{-H}) (\\mathbf{L}^{-1} \\mathbf{a}) = \\mathbf{a}^{H} \\mathbf{L}^{-H} \\mathbf{L}^{-1} \\mathbf{a} = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$. This is correct. The procedure uses one triangular solve and one inner product, avoiding explicit inversion. It is, in fact, computationally more efficient than option A.\nVerdict: **Correct**.\n\n**Option D. Solve $\\mathbf{L}^{H} \\mathbf{z} = \\mathbf{a}$ for $\\mathbf{z}$, and then compute $s = \\mathbf{z}^{H} \\mathbf{z}$.**\nLet us analyze the steps:\n1.  $\\mathbf{z} = (\\mathbf{L}^{H})^{-1} \\mathbf{a} = \\mathbf{L}^{-H} \\mathbf{a}$.\n2.  $s = \\mathbf{z}^{H} \\mathbf{z} = (\\mathbf{L}^{-H} \\mathbf{a})^{H} (\\mathbf{L}^{-H} \\mathbf{a})$.\nUsing the conjugate transpose property, $(\\mathbf{L}^{-H} \\mathbf{a})^{H} = \\mathbf{a}^{H} (\\mathbf{L}^{-H})^{H} = \\mathbf{a}^{H} \\mathbf{L}^{-1}$.\nSo, $s = (\\mathbf{a}^{H} \\mathbf{L}^{-1}) (\\mathbf{L}^{-H} \\mathbf{a}) = \\mathbf{a}^{H} \\mathbf{L}^{-1} \\mathbf{L}^{-H} \\mathbf{a}$.\nIn general, $\\mathbf{L}^{-1}$ and $\\mathbf{L}^{-H}$ do not commute, so $\\mathbf{L}^{-1} \\mathbf{L}^{-H} \\neq \\mathbf{L}^{-H} \\mathbf{L}^{-1} = \\mathbf{R}^{-1}$. Therefore, this procedure is incorrect.\nVerdict: **Incorrect**.\n\n**Option E. Form $\\mathbf{R}^{-1}$ explicitly via $\\mathbf{R}^{-1} = \\mathbf{L}^{-H} \\mathbf{L}^{-1}$, and then compute $s = \\mathbf{a}^{H} \\mathbf{R}^{-1} \\mathbf{a}$.**\nThe problem statement explicitly requires that the procedure must \"avoid forming $\\mathbf{R}^{-1}$ explicitly\" for reasons of \"computational efficiency and numerical stability\". While the mathematical identity $\\mathbf{R}^{-1} = \\mathbf{L}^{-H} \\mathbf{L}^{-1}$ is correct, forming this inverse would typically involve first inverting $\\mathbf{L}$ (an $O(M^3)$ operation) and then performing a matrix multiplication of $\\mathbf{L}^{-H}$ and $\\mathbf{L}^{-1}$ (another $O(M^3)$ operation). This directly violates the constraints and motivations given in the problem. Therefore, this is not a valid procedure in the context of the question.\nVerdict: **Incorrect**.\n\nIn summary, options A and C are both mathematically correct procedures that adhere to the computational constraints of the problem.", "answer": "$$\\boxed{AC}$$", "id": "2883219"}, {"introduction": "The high resolution of the MVDR estimator is predicated on perfect knowledge of the signal's steering vector. In practice, however, a mismatch between the assumed and true signal parameters is inevitable, leading to significant performance degradation. This practice problem [@problem_id:2883242] guides you through an analytical derivation to quantify how such a mismatch impacts the filter's output variance, revealing the estimator's inherent sensitivity.", "problem": "Consider a length-$M$ snapshot model for complex baseband sinusoidal spectral estimation. Let the $M \\times 1$ steering vector be $\\mathbf{a}(\\omega) \\triangleq [1, e^{-j \\omega}, e^{-j 2 \\omega}, \\dots, e^{-j (M-1)\\omega}]^{\\top}$, where $\\omega$ is an angular frequency in radians per sample. The received $M \\times 1$ vector is modeled as $\\mathbf{x} = s\\,\\mathbf{a}(\\omega_{0}) + \\mathbf{v}$, where $s$ is a zero-mean circular complex random amplitude with variance $\\mathbb{E}[|s|^{2}] = P$, independent of the additive noise $\\mathbf{v} \\sim \\mathcal{CN}(0, \\sigma^{2} \\mathbf{I}_{M})$. Assume the large-sample regime so that the covariance matrix is the true second-order moment, $\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{H}] = P\\,\\mathbf{a}(\\omega_{0}) \\mathbf{a}(\\omega_{0})^{H} + \\sigma^{2} \\mathbf{I}_{M}$.\n\nA minimum variance distortionless response (MVDR, also known as Capon) filter is designed at an assumed frequency $\\omega = \\omega_{0} + \\Delta \\omega$ using the constraint $\\mathbf{w}^{H} \\mathbf{a}(\\omega) = 1$. The true sinusoid arrives at $\\omega_{0}$, so there is a mismatch $\\Delta \\omega$ between the true sinusoid and the assumed steering vector.\n\nStarting only from the optimization statement “minimize $\\mathbf{w}^{H} \\mathbf{R} \\mathbf{w}$ subject to $\\mathbf{w}^{H} \\mathbf{a}(\\omega) = 1$” and fundamental linear algebra identities, derive analytically:\n- the distortionless response experienced by the true sinusoid, $g(\\Delta \\omega) \\triangleq \\mathbf{w}^{H} \\mathbf{a}(\\omega_{0})$, and\n- the MVDR output variance $v_{\\text{out}}(\\Delta \\omega) \\triangleq \\mathbf{w}^{H} \\mathbf{R} \\mathbf{w}$,\n\nboth in closed form as functions of $M$, $P$, $\\sigma^{2}$, and $\\Delta \\omega$ (in radians). Then simplify to obtain the ratio\n$$\n\\rho(\\Delta \\omega) \\triangleq \\frac{v_{\\text{out}}(\\Delta \\omega)}{v_{\\text{out}}(0)}.\n$$\nThis ratio quantifies the degradation in the MVDR output variance due to the assumed frequency mismatch.\n\nProvide, as your final reported result, the single simplified closed-form analytical expression for $\\rho(\\Delta \\omega)$ in terms of $M$, $P$, $\\sigma^{2}$, and $\\Delta \\omega$ (in radians). No numerical evaluation is required. Express angles in radians. The final answer must be a single analytical expression; do not include inequalities or equations in the final answer.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Snapshot length: $M$.\n- Steering vector: $\\mathbf{a}(\\omega) \\triangleq [1, e^{-j \\omega}, e^{-j 2 \\omega}, \\dots, e^{-j (M-1)\\omega}]^{\\top}$.\n- Received vector model: $\\mathbf{x} = s\\,\\mathbf{a}(\\omega_{0}) + \\mathbf{v}$.\n- Signal amplitude $s$: a zero-mean circular complex random variable with variance $\\mathbb{E}[|s|^{2}] = P$.\n- Noise vector $\\mathbf{v}$: additive, $\\mathbf{v} \\sim \\mathcal{CN}(0, \\sigma^{2} \\mathbf{I}_{M})$, independent of $s$.\n- Covariance matrix: $\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{H}] = P\\,\\mathbf{a}(\\omega_{0}) \\mathbf{a}(\\omega_{0})^{H} + \\sigma^{2} \\mathbf{I}_{M}$.\n- MVDR optimization problem: minimize $\\mathbf{w}^{H} \\mathbf{R} \\mathbf{w}$ subject to $\\mathbf{w}^{H} \\mathbf{a}(\\omega) = 1$.\n- Assumed frequency for filter design: $\\omega = \\omega_{0} + \\Delta \\omega$.\n- True signal frequency: $\\omega_{0}$.\n- Mismatch: $\\Delta \\omega$.\n- Quantities to derive: $g(\\Delta \\omega) \\triangleq \\mathbf{w}^{H} \\mathbf{a}(\\omega_{0})$ and $v_{\\text{out}}(\\Delta \\omega) \\triangleq \\mathbf{w}^{H} \\mathbf{R} \\mathbf{w}$.\n- Final result: the ratio $\\rho(\\Delta \\omega) \\triangleq \\frac{v_{\\text{out}}(\\Delta \\omega)}{v_{\\text{out}}(0)}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a canonical problem in adaptive array processing and spectral estimation—the performance analysis of the Minimum Variance Distortionless Response (MVDR) or Capon beamformer in the presence of a steering vector mismatch. The model, definitions, and objective are standard in the signal processing literature. The covariance matrix is structured correctly for a single signal in white noise. The optimization problem for the filter weights is the standard definition of the MVDR filter. All necessary parameters ($M$, $P$, $\\sigma^2$, $\\Delta\\omega$) are provided. The problem is free of contradictions, ambiguities, or unsound premises.\n\n**Verdict**\nThe problem is valid and can be solved.\n\n**Solution Derivation**\nThe MVDR filter weights $\\mathbf{w}$ are found by solving the constrained optimization problem:\n$$ \\text{minimize} \\quad \\mathbf{w}^{H} \\mathbf{R} \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^{H} \\mathbf{a}(\\omega) = 1 $$\nwhere $\\omega = \\omega_{0} + \\Delta\\omega$. We form the Lagrangian using a complex Lagrange multiplier $\\lambda$:\n$$ \\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{H} \\mathbf{R} \\mathbf{w} - \\lambda(\\mathbf{w}^{H} \\mathbf{a}(\\omega) - 1) - \\lambda^{*}(\\mathbf{a}(\\omega)^{H} \\mathbf{w} - 1) $$\nTo find the minimum, we compute the complex gradient with respect to $\\mathbf{w}^{H}$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{H}} = \\mathbf{R} \\mathbf{w} - \\lambda \\mathbf{a}(\\omega) = 0 $$\nThis gives $\\mathbf{w} = \\lambda \\mathbf{R}^{-1} \\mathbf{a}(\\omega)$. The covariance matrix $\\mathbf{R} = P \\mathbf{a}(\\omega_{0}) \\mathbf{a}(\\omega_{0})^{H} + \\sigma^{2} \\mathbf{I}_{M}$ is positive definite for $\\sigma^2 > 0$, so its inverse exists. We find $\\lambda$ by substituting $\\mathbf{w}$ into the constraint:\n$$ (\\lambda \\mathbf{R}^{-1} \\mathbf{a}(\\omega))^{H} \\mathbf{a}(\\omega) = 1 \\implies \\lambda^{*} \\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega) = 1 $$\nSince $\\mathbf{R}$ is Hermitian, $\\mathbf{R}^{-1}$ is also Hermitian, and the scalar quantity $\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)$ is real. Thus, $\\lambda = \\lambda^{*} = \\frac{1}{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)}$.\nThe optimal weight vector is:\n$$ \\mathbf{w} = \\frac{\\mathbf{R}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)} $$\nThe MVDR output variance, $v_{\\text{out}}(\\Delta \\omega)$, is the value of the objective function at the optimal $\\mathbf{w}$:\n$$ v_{\\text{out}}(\\Delta \\omega) = \\mathbf{w}^{H} \\mathbf{R} \\mathbf{w} = \\left(\\frac{\\mathbf{R}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)}\\right)^{H} \\mathbf{R} \\left(\\frac{\\mathbf{R}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)}\\right) = \\frac{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{R} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)}{|\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)|^{2}} = \\frac{1}{\\mathbf{a}(\\omega)^{H} \\mathbf{R}^{-1} \\mathbf{a}(\\omega)} $$\nTo proceed, we must find $\\mathbf{R}^{-1}$. The matrix $\\mathbf{R}$ is a rank-$1$ update to a scaled identity matrix. We use the Sherman-Morrison identity: $(\\mathbf{A} + \\mathbf{u}\\mathbf{v}^{H})^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^{H}\\mathbf{A}^{-1}}{1+\\mathbf{v}^{H}\\mathbf{A}^{-1}\\mathbf{u}}$.\nLet $\\mathbf{A} = \\sigma^2 \\mathbf{I}_M$, $\\mathbf{u}=P \\mathbf{a}(\\omega_0)$, and $\\mathbf{v}=\\mathbf{a}(\\omega_0)$. Then $\\mathbf{A}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_M$.\n$$ \\mathbf{R}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{(\\frac{1}{\\sigma^2}\\mathbf{I}_M)(P \\mathbf{a}(\\omega_0)) (\\mathbf{a}(\\omega_0)^H)(\\frac{1}{\\sigma^2}\\mathbf{I}_M)}{1 + \\mathbf{a}(\\omega_0)^H (\\frac{1}{\\sigma^2}\\mathbf{I}_M) (P \\mathbf{a}(\\omega_0))} = \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{\\frac{P}{\\sigma^4} \\mathbf{a}(\\omega_0)\\mathbf{a}(\\omega_0)^H}{1 + \\frac{P}{\\sigma^2}\\mathbf{a}(\\omega_0)^H \\mathbf{a}(\\omega_0)} $$\nSince $\\mathbf{a}(\\omega_0)^H \\mathbf{a}(\\omega_0) = M$, we have:\n$$ \\mathbf{R}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{P \\mathbf{a}(\\omega_0)\\mathbf{a}(\\omega_0)^H}{\\sigma^4(1 + PM/\\sigma^2)} = \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{P \\mathbf{a}(\\omega_0)\\mathbf{a}(\\omega_0)^H}{\\sigma^2(\\sigma^2 + PM)} $$\nNow we evaluate the denominator of $v_{\\text{out}}(\\Delta \\omega)$, which is $D(\\Delta \\omega) \\triangleq \\mathbf{a}(\\omega)^H \\mathbf{R}^{-1} \\mathbf{a}(\\omega)$:\n$$ D(\\Delta \\omega) = \\mathbf{a}(\\omega)^H \\left( \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{P \\mathbf{a}(\\omega_0)\\mathbf{a}(\\omega_0)^H}{\\sigma^2(\\sigma^2 + PM)} \\right) \\mathbf{a}(\\omega) $$\n$$ D(\\Delta \\omega) = \\frac{1}{\\sigma^2} \\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega) - \\frac{P}{\\sigma^2(\\sigma^2 + PM)} |\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)|^2 $$\nWe have $\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega) = M$. The inner product $\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)$ is:\n$$ \\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0) = \\mathbf{a}(\\omega_0+\\Delta\\omega)^H \\mathbf{a}(\\omega_0) = \\sum_{k=0}^{M-1} e^{j(\\omega_0+\\Delta\\omega)k} e^{-j\\omega_0 k} = \\sum_{k=0}^{M-1} (e^{j\\Delta\\omega})^k $$\nThis is a geometric sum, which for $\\Delta\\omega \\neq 0$ is $\\frac{1-e^{jM\\Delta\\omega}}{1-e^{j\\Delta\\omega}}$. The magnitude squared is:\n$$ |\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)|^2 = \\left|\\frac{\\sin(M\\Delta\\omega/2)}{\\sin(\\Delta\\omega/2)}\\right|^2 = \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)} $$\nThis expression is the squared magnitude of the Dirichlet kernel. Substituting into $D(\\Delta \\omega)$:\n$$ D(\\Delta \\omega) = \\frac{M}{\\sigma^2} - \\frac{P}{\\sigma^2(\\sigma^2 + PM)} \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)} $$\nSo, the output variance is:\n$$ v_{\\text{out}}(\\Delta \\omega) = \\frac{1}{D(\\Delta \\omega)} = \\left(\\frac{M}{\\sigma^2} - \\frac{P}{\\sigma^2(\\sigma^2 + PM)} \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}\\right)^{-1} $$\nNext, we derive the response to the true sinusoid, $g(\\Delta\\omega) = \\mathbf{w}^H \\mathbf{a}(\\omega_0)$:\n$$ g(\\Delta\\omega) = \\frac{\\mathbf{a}(\\omega)^H \\mathbf{R}^{-1} \\mathbf{a}(\\omega_0)}{\\mathbf{a}(\\omega)^H \\mathbf{R}^{-1} \\mathbf{a}(\\omega)} $$\nThe numerator is $N(\\Delta\\omega) = \\mathbf{a}(\\omega)^H \\left( \\frac{1}{\\sigma^2}\\mathbf{I}_M - \\frac{P \\mathbf{a}(\\omega_0)\\mathbf{a}(\\omega_0)^H}{\\sigma^2(\\sigma^2 + PM)} \\right) \\mathbf{a}(\\omega_0)$:\n$$ N(\\Delta\\omega) = \\frac{1}{\\sigma^2} \\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0) - \\frac{P(\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0))(\\mathbf{a}(\\omega_0)^H \\mathbf{a}(\\omega_0))}{\\sigma^2(\\sigma^2 + PM)} = \\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0) \\left( \\frac{1}{\\sigma^2} - \\frac{PM}{\\sigma^2(\\sigma^2 + PM)} \\right) = \\frac{\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)}{\\sigma^2 + PM} $$\nThus,\n$$ g(\\Delta \\omega) = \\frac{\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)/(\\sigma^2 + PM)}{D(\\Delta \\omega)} = \\frac{\\frac{1}{\\sigma^2+PM} \\frac{1-e^{jM\\Delta\\omega}}{1-e^{j\\Delta\\omega}}}{\\frac{M}{\\sigma^2} - \\frac{P}{\\sigma^2(\\sigma^2 + PM)} \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}} $$\nTo find the ratio $\\rho(\\Delta\\omega)$, we need $v_{\\text{out}}(0)$, which corresponds to $\\Delta\\omega=0$. In this case, $\\mathbf{a}(\\omega) = \\mathbf{a}(\\omega_0)$, so $|\\mathbf{a}(\\omega)^H \\mathbf{a}(\\omega_0)|^2 = |\\mathbf{a}(\\omega_0)^H \\mathbf{a}(\\omega_0)|^2 = M^2$.\n$$ D(0) = \\frac{M}{\\sigma^2} - \\frac{P M^2}{\\sigma^2(\\sigma^2 + PM)} = \\frac{M(\\sigma^2+PM) - PM^2}{\\sigma^2(\\sigma^2+PM)} = \\frac{M\\sigma^2}{\\sigma^2(\\sigma^2+PM)} = \\frac{M}{\\sigma^2+PM} $$\nSo, $v_{\\text{out}}(0) = \\frac{1}{D(0)} = \\frac{\\sigma^2+PM}{M} = P + \\frac{\\sigma^2}{M}$.\nThe ratio is $\\rho(\\Delta\\omega) = \\frac{v_{\\text{out}}(\\Delta \\omega)}{v_{\\text{out}}(0)} = \\frac{D(0)}{D(\\Delta \\omega)}$:\n$$ \\rho(\\Delta\\omega) = \\frac{\\frac{M}{\\sigma^2+PM}}{\\frac{M}{\\sigma^2} - \\frac{P}{\\sigma^2(\\sigma^2 + PM)} \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}} $$\nMultiplying the numerator and denominator by $\\sigma^2(\\sigma^2+PM)$ yields:\n$$ \\rho(\\Delta\\omega) = \\frac{M\\sigma^2}{M(\\sigma^2+PM) - P \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}} = \\frac{M\\sigma^2}{M\\sigma^2 + M^2 P - P \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}} $$\n$$ \\rho(\\Delta\\omega) = \\frac{M\\sigma^2}{M\\sigma^2 + P\\left(M^2 - \\frac{\\sin^2(M\\Delta\\omega/2)}{\\sin^2(\\Delta\\omega/2)}\\right)} $$\nThis is the final simplified closed-form expression.", "answer": "$$\n\\boxed{\\frac{M\\sigma^{2}}{M\\sigma^{2} + P\\left(M^{2} - \\frac{\\sin^{2}(M\\Delta\\omega/2)}{\\sin^{2}(\\Delta\\omega/2)}\\right)}}\n$$", "id": "2883242"}, {"introduction": "Having seen the performance degradation caused by model mismatch, we now turn to a widely-used technique for improving robustness: diagonal loading. By adding a small positive value to the diagonal of the covariance matrix, we can stabilize the matrix inverse at the cost of introducing a small bias. This exercise [@problem_id:2883258] delves into the mechanics of diagonal loading, asking you to derive its effect on the MVDR weights and precisely quantify the resulting bias in the spectral estimate.", "problem": "Consider a narrowband array processing scenario with $M$ sensors. Let $\\hat{\\mathbf{R}} \\in \\mathbb{C}^{M \\times M}$ denote a positive definite estimate of the data covariance matrix, and let $\\mathbf{a}(\\omega) \\in \\mathbb{C}^{M}$ denote the steering vector at angular spatial frequency $\\omega$. The Minimum Variance Distortionless Response (MVDR, also known as the Capon) spectral estimator at frequency $\\omega$ is obtained by solving the constrained quadratic program that minimizes the array output power subject to a distortionless response toward $\\mathbf{a}(\\omega)$. To improve robustness, diagonal loading is applied to form the loaded covariance $\\hat{\\mathbf{R}}_{\\delta} \\triangleq \\hat{\\mathbf{R}} + \\delta \\mathbf{I}$, with $\\delta  0$ and $\\mathbf{I}$ the identity matrix.\n\nStarting from the definition of the constrained quadratic minimization and standard Karush–Kuhn–Tucker optimality conditions for complex vectors, perform the following:\n\n1) Derive the optimal weight vector $\\mathbf{w}_{\\delta}(\\omega)$ that minimizes $\\mathbf{w}^{H} \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w}$ subject to $\\mathbf{a}^{H}(\\omega) \\mathbf{w} = 1$.\n\n2) Using your result, derive the corresponding Capon spectral estimate $P_{\\text{MVDR},\\delta}(\\omega)$ at frequency $\\omega$.\n\n3) Now adopt the single-signal plus white-noise model for the true covariance at the true signal frequency $\\omega_{0}$:\n$$\n\\mathbf{R} \\;=\\; \\sigma_{s}^{2}\\, \\mathbf{a}(\\omega_{0}) \\mathbf{a}^{H}(\\omega_{0}) \\;+\\; \\sigma_{n}^{2}\\, \\mathbf{I},\n$$\nwith $\\sigma_{s}^{2}  0$ the signal power, $\\sigma_{n}^{2}  0$ the noise power, and assume the scan steering vector equals the true one at $\\omega_{0}$, i.e., evaluate the spectrum at $\\omega = \\omega_{0}$ with the same $\\mathbf{a}(\\omega_{0})$. Under this model, explicitly quantify the deviation of the loaded spectrum from the unregularized one by computing\n$$\n\\Delta P(\\omega_{0}) \\;\\triangleq\\; P_{\\text{MVDR},\\delta}(\\omega_{0}) \\;-\\; P_{\\text{MVDR}}(\\omega_{0}),\n$$\nas a closed-form analytic expression in terms of $\\delta$, $\\sigma_{s}^{2}$, $\\sigma_{n}^{2}$, and $\\mathbf{a}(\\omega_{0})$. Express your final answer for $\\Delta P(\\omega_{0})$ as a single simplified symbolic expression. Do not include units.\n\nYour final answer must be a single closed-form expression. No rounding is required.", "solution": "The problem posed is a standard exercise in array signal processing, specifically in adaptive beamforming and spectral estimation. It is scientifically grounded, well-posed, and objective. There are no inconsistencies or ambiguities. The problem is valid and can be solved through direct application of optimization theory and matrix algebra.\n\nThe task is to derive the effect of diagonal loading on the Minimum Variance Distortionless Response (MVDR) spectral estimator. We are given the loaded covariance matrix $\\hat{\\mathbf{R}}_{\\delta} = \\hat{\\mathbf{R}} + \\delta \\mathbf{I}$. The analysis proceeds in three distinct parts.\n\nPart 1: Derivation of the optimal weight vector $\\mathbf{w}_{\\delta}(\\omega)$\n\nThe problem is a constrained quadratic minimization:\n$$\n\\min_{\\mathbf{w}} \\mathbf{w}^{H} \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{a}^{H}(\\omega) \\mathbf{w} = 1\n$$\nwhere $\\mathbf{w} \\in \\mathbb{C}^{M}$ is the weight vector, $\\hat{\\mathbf{R}}_{\\delta} \\in \\mathbb{C}^{M \\times M}$ is the positive definite loaded covariance matrix, and $\\mathbf{a}(\\omega) \\in \\mathbb{C}^{M}$ is the steering vector.\n\nWe formulate the Lagrangian for this optimization problem. The objective function $J(\\mathbf{w}) = \\mathbf{w}^{H} \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w}$ is real-valued. The constraint $c(\\mathbf{w}) = \\mathbf{a}^{H}(\\omega) \\mathbf{w} - 1 = 0$ is complex-valued. The Lagrangian is given by:\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{H} \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w} + \\lambda (\\mathbf{a}^{H}(\\omega) \\mathbf{w} - 1) + \\lambda^{*} (\\mathbf{w}^{H} \\mathbf{a}(\\omega) - 1)\n$$\nHere, $\\lambda$ is a complex Lagrange multiplier. The Karush-Kuhn-Tucker (KKT) conditions for optimality require that the gradient of the Lagrangian with respect to $\\mathbf{w}^{*}$ (the conjugate of $\\mathbf{w}$) be zero. Using Wirtinger calculus, we take the complex gradient:\n$$\n\\nabla_{\\mathbf{w}^{*}} \\mathcal{L}(\\mathbf{w}, \\lambda) = \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w} + \\lambda \\mathbf{a}(\\omega)\n$$\nSetting this gradient to zero provides the first optimality condition:\n$$\n\\hat{\\mathbf{R}}_{\\delta} \\mathbf{w} + \\lambda \\mathbf{a}(\\omega) = 0\n$$\nSince $\\hat{\\mathbf{R}}$ is positive definite and $\\delta  0$, the loaded matrix $\\hat{\\mathbf{R}}_{\\delta} = \\hat{\\mathbf{R}} + \\delta \\mathbf{I}$ is also positive definite and thus invertible. We can solve for $\\mathbf{w}$:\n$$\n\\mathbf{w} = -\\lambda \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)\n$$\nTo determine the Lagrange multiplier $\\lambda$, we enforce the distortionless response constraint $\\mathbf{a}^{H}(\\omega) \\mathbf{w} = 1$:\n$$\n\\mathbf{a}^{H}(\\omega) \\left( -\\lambda \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega) \\right) = 1\n$$\n$$\n-\\lambda \\left( \\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega) \\right) = 1\n$$\nThe term $\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)$ is a positive real scalar because $\\hat{\\mathbf{R}}_{\\delta}^{-1}$ is positive definite. Solving for $\\lambda$:\n$$\n\\lambda = -\\frac{1}{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}\n$$\nSubstituting this expression for $\\lambda$ back into the equation for $\\mathbf{w}$, we obtain the optimal weight vector:\n$$\n\\mathbf{w}_{\\delta}(\\omega) = \\frac{\\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}\n$$\n\nPart 2: Derivation of the Capon spectral estimate $P_{\\text{MVDR},\\delta}(\\omega)$\n\nThe MVDR spectral estimate is defined as the minimum output power of the array, which is achieved by the optimal weight vector $\\mathbf{w}_{\\delta}(\\omega)$. The power is given by $P = \\mathbf{w}^{H} \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w}$. Thus:\n$$\nP_{\\text{MVDR},\\delta}(\\omega) = \\mathbf{w}_{\\delta}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta} \\mathbf{w}_{\\delta}(\\omega)\n$$\nWe substitute the expression for $\\mathbf{w}_{\\delta}(\\omega)$:\n$$\nP_{\\text{MVDR},\\delta}(\\omega) = \\left( \\frac{\\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)} \\right)^{H} \\hat{\\mathbf{R}}_{\\delta} \\left( \\frac{\\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)} \\right)\n$$\nThe denominator is a scalar, so it can be factored out. Since it is real, its conjugate is itself.\n$$\nP_{\\text{MVDR},\\delta}(\\omega) = \\frac{ (\\mathbf{a}^{H}(\\omega) (\\hat{\\mathbf{R}}_{\\delta}^{-1})^{H}) \\hat{\\mathbf{R}}_{\\delta} (\\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)) }{ (\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega))^{2} }\n$$\nThe loaded covariance matrix $\\hat{\\mathbf{R}}_{\\delta}$ is Hermitian, so its inverse $(\\hat{\\mathbf{R}}_{\\delta}^{-1})$ is also Hermitian, i.e., $(\\hat{\\mathbf{R}}_{\\delta}^{-1})^{H} = \\hat{\\mathbf{R}}_{\\delta}^{-1}$. The numerator simplifies as:\n$$\n\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\hat{\\mathbf{R}}_{\\delta} \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega) = \\mathbf{a}^{H}(\\omega) \\mathbf{I} \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega) = \\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)\n$$\nSubstituting this back into the expression for the power:\n$$\nP_{\\text{MVDR},\\delta}(\\omega) = \\frac{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}{(\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega))^{2}} = \\frac{1}{\\mathbf{a}^{H}(\\omega) \\hat{\\mathbf{R}}_{\\delta}^{-1} \\mathbf{a}(\\omega)}\n$$\nThis is the well-known expression for the Capon spectral estimate.\n\nPart 3: Quantifying the deviation $\\Delta P(\\omega_0)$\n\nWe now shift from the estimated covariance $\\hat{\\mathbf{R}}$ to a theoretical true covariance matrix $\\mathbf{R}$ for a single signal at frequency $\\omega_0$ in the presence of white noise:\n$$\n\\mathbf{R} = \\sigma_{s}^{2}\\, \\mathbf{a}(\\omega_{0}) \\mathbf{a}^{H}(\\omega_{0}) + \\sigma_{n}^{2}\\, \\mathbf{I}\n$$\nThe unregularized spectrum $P_{\\text{MVDR}}(\\omega_0)$ is obtained using $\\mathbf{R}$, and the loaded spectrum $P_{\\text{MVDR},\\delta}(\\omega_0)$ is obtained using $\\mathbf{R}_{\\delta} = \\mathbf{R} + \\delta \\mathbf{I}$.\nWe need to compute $P_{\\text{MVDR}}(\\omega_0) = (\\mathbf{a}_{0}^{H} \\mathbf{R}^{-1} \\mathbf{a}_{0})^{-1}$ and $P_{\\text{MVDR},\\delta}(\\omega_0) = (\\mathbf{a}_{0}^{H} \\mathbf{R}_{\\delta}^{-1} \\mathbf{a}_{0})^{-1}$, where $\\mathbf{a}_0 \\triangleq \\mathbf{a}(\\omega_0)$.\n\nFirst, let us find $\\mathbf{R}^{-1}$. We use the Sherman-Morrison matrix inversion lemma: $(\\mathbf{A} + \\mathbf{u}\\mathbf{v}^H)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^H \\mathbf{A}^{-1}}{1 + \\mathbf{v}^H \\mathbf{A}^{-1} \\mathbf{u}}$.\nFor $\\mathbf{R}$, we identify $\\mathbf{A} = \\sigma_{n}^{2} \\mathbf{I}$, $\\mathbf{u} = \\sigma_{s}^{2} \\mathbf{a}_0$, and $\\mathbf{v} = \\mathbf{a}_0$. Then $\\mathbf{A}^{-1} = \\frac{1}{\\sigma_{n}^{2}}\\mathbf{I}$.\nThe inverse is:\n$$\n\\mathbf{R}^{-1} = \\frac{1}{\\sigma_{n}^{2}}\\mathbf{I} - \\frac{(\\frac{1}{\\sigma_{n}^{2}}\\mathbf{I})(\\sigma_{s}^{2} \\mathbf{a}_0) \\mathbf{a}_0^H (\\frac{1}{\\sigma_{n}^{2}}\\mathbf{I})}{1 + \\mathbf{a}_0^H (\\frac{1}{\\sigma_{n}^{2}}\\mathbf{I}) (\\sigma_{s}^{2} \\mathbf{a}_0)} = \\frac{1}{\\sigma_{n}^{2}}\\mathbf{I} - \\frac{\\frac{\\sigma_{s}^{2}}{(\\sigma_{n}^{2})^2} \\mathbf{a}_0 \\mathbf{a}_0^H}{1 + \\frac{\\sigma_{s}^{2}}{\\sigma_{n}^{2}} \\mathbf{a}_0^H \\mathbf{a}_0} = \\frac{1}{\\sigma_{n}^{2}}\\mathbf{I} - \\frac{\\sigma_{s}^{2} \\mathbf{a}_0 \\mathbf{a}_0^H}{\\sigma_{n}^{2}(\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0)}\n$$\nNow we compute the quadratic form $\\mathbf{a}_0^H \\mathbf{R}^{-1} \\mathbf{a}_0$:\n$$\n\\mathbf{a}_0^H \\mathbf{R}^{-1} \\mathbf{a}_0 = \\mathbf{a}_0^H \\left( \\frac{1}{\\sigma_{n}^{2}}\\mathbf{I} - \\frac{\\sigma_{s}^{2} \\mathbf{a}_0 \\mathbf{a}_0^H}{\\sigma_{n}^{2}(\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0)} \\right) \\mathbf{a}_0\n$$\n$$\n= \\frac{1}{\\sigma_{n}^{2}} \\mathbf{a}_0^H \\mathbf{a}_0 - \\frac{\\sigma_{s}^{2} (\\mathbf{a}_0^H \\mathbf{a}_0) (\\mathbf{a}_0^H \\mathbf{a}_0)}{\\sigma_{n}^{2}(\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0)} = \\frac{\\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2}} \\left( 1 - \\frac{\\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0} \\right)\n$$\n$$\n= \\frac{\\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2}} \\left( \\frac{(\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0) - \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0} \\right) = \\frac{\\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2}} \\left( \\frac{\\sigma_{n}^{2}}{\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0} \\right) = \\frac{\\mathbf{a}_0^H \\mathbf{a}_0}{\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0}\n$$\nThe unregularized spectral estimate is the inverse of this expression:\n$$\nP_{\\text{MVDR}}(\\omega_0) = \\frac{\\sigma_{n}^{2} + \\sigma_{s}^{2} \\mathbf{a}_0^H \\mathbf{a}_0}{\\mathbf{a}_0^H \\mathbf{a}_0} = \\sigma_{s}^{2} + \\frac{\\sigma_{n}^{2}}{\\mathbf{a}_0^H \\mathbf{a}_0}\n$$\nFor the loaded case, the covariance is $\\mathbf{R}_{\\delta} = \\mathbf{R} + \\delta \\mathbf{I} = \\sigma_{s}^{2} \\mathbf{a}_0 \\mathbf{a}_0^H + (\\sigma_{n}^{2} + \\delta)\\mathbf{I}$. This is identical in form to $\\mathbf{R}$, with $\\sigma_{n}^{2}$ replaced by $(\\sigma_{n}^{2} + \\delta)$. We can therefore obtain the loaded spectral estimate by direct substitution:\n$$\nP_{\\text{MVDR},\\delta}(\\omega_0) = \\sigma_{s}^{2} + \\frac{\\sigma_{n}^{2} + \\delta}{\\mathbf{a}_0^H \\mathbf{a}_0}\n$$\nFinally, we compute the deviation $\\Delta P(\\omega_0)$:\n$$\n\\Delta P(\\omega_0) = P_{\\text{MVDR},\\delta}(\\omega_0) - P_{\\text{MVDR}}(\\omega_0)\n$$\n$$\n= \\left(\\sigma_{s}^{2} + \\frac{\\sigma_{n}^{2} + \\delta}{\\mathbf{a}_0^H \\mathbf{a}_0}\\right) - \\left(\\sigma_{s}^{2} + \\frac{\\sigma_{n}^{2}}{\\mathbf{a}_0^H \\mathbf{a}_0}\\right)\n$$\n$$\n= \\frac{\\sigma_{n}^{2} + \\delta}{\\mathbf{a}_0^H \\mathbf{a}_0} - \\frac{\\sigma_{n}^{2}}{\\mathbf{a}_0^H \\mathbf{a}_0} = \\frac{\\sigma_{n}^{2} + \\delta - \\sigma_{n}^{2}}{\\mathbf{a}_0^H \\mathbf{a}_0}\n$$\nThe result is remarkably simple, as the terms involving signal and noise powers cancel.\n$$\n\\Delta P(\\omega_0) = \\frac{\\delta}{\\mathbf{a}_0^H \\mathbf{a}_0} = \\frac{\\delta}{\\mathbf{a}^{H}(\\omega_{0})\\mathbf{a}(\\omega_{0})}\n$$\nThis demonstrates that for the single-signal model evaluated at the true signal direction, the bias introduced by diagonal loading is independent of both signal and noise powers. It depends only on the loading factor $\\delta$ and the squared Euclidean norm of the steering vector.", "answer": "$$\n\\boxed{\\frac{\\delta}{\\mathbf{a}^{H}(\\omega_{0}) \\mathbf{a}(\\omega_{0})}}\n$$", "id": "2883258"}]}