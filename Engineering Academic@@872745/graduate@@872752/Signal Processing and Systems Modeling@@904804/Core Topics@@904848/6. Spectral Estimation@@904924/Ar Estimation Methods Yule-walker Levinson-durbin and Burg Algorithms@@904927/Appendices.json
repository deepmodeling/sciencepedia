{"hands_on_practices": [{"introduction": "The Yule-Walker equations form the theoretical foundation for estimating Autoregressive (AR) model parameters. This practice challenges you to derive these fundamental equations from the orthogonality principle, which is the core concept behind linear minimum mean-square error prediction. By working through this complex-valued AR(2) scenario [@problem_id:2853134], you will gain a deeper appreciation for how the statistical properties of a signal directly translate into a solvable system of linear equations, a crucial step in spectral analysis and modeling.", "problem": "Consider a zero-mean, complex-valued, wide-sense stationary (WSS) discrete-time process $x[n]$ that is modeled as a second-order autoregressive (AR) process: $x[n] + a_{1} x[n-1] + a_{2} x[n-2] = e[n]$, where $e[n]$ is a complex, white innovation with variance $\\sigma_{e}^{2}$, uncorrelated with $\\{x[n-k]\\}_{k \\geq 1}$. You are given sample autocorrelations $r_{x}[0] = 1$, $r_{x}[1] = 0.5 \\exp(j \\pi/4)$, and $r_{x}[2] = 0.2 \\exp(j \\pi/2)$, with angles in radians. Using only the orthogonality principle for linear minimum mean-square error prediction and the definition of autocorrelation $r_{x}[k] \\triangleq \\mathbb{E}\\{x[n] x^{*}[n-k]\\}$, do the following:\n\n- From first principles, derive the linear equations relating $a_{1}$ and $a_{2}$ to the autocorrelations for the complex-valued case, recognizing Hermitian symmetry $r_{x}[-k] = r_{x}^{*}[k]$, and write them in a Hermitian Toeplitz matrix form suitable for order $p=2$.\n- Solve this system for $a_{1}$ and $a_{2}$.\n- Using the zero-lag relation implied by the same principles, compute the innovation variance $\\sigma_{e}^{2}$.\n\nProvide the exact value of $\\sigma_{e}^{2}$ as your final answer. Do not round; express the final result as a reduced rational number.", "solution": "We are tasked with analyzing a second-order complex-valued autoregressive process, AR($2$), defined by the equation $x[n] + a_{1} x[n-1] + a_{2} x[n-2] = e[n]$. The process $x[n]$ is zero-mean and wide-sense stationary (WSS). The innovation $e[n]$ is a complex white noise process with variance $\\sigma_{e}^{2}$, and it is uncorrelated with past samples of the process, $\\{x[n-k]\\}_{k \\geq 1}$. This uncorrelation is the core of the orthogonality principle for linear prediction.\n\nThe linear minimum mean-square error (LMMSE) one-step prediction of $x[n]$ is given by $\\hat{x}[n] = -a_{1} x[n-1] - a_{2} x[n-2]$. The prediction error is $e[n] = x[n] - \\hat{x}[n]$. The orthogonality principle states that the error $e[n]$ must be orthogonal to the data space used for prediction, which in this case is spanned by $x[n-1]$ and $x[n-2]$. For complex WSS processes, this orthogonality is expressed in terms of expectations: $\\mathbb{E}\\{e[n] x^{*}[n-k]\\} = 0$ for $k \\geq 1$.\n\nWe shall derive the linear equations for the coefficients $a_1$ and $a_2$ by applying this principle. We start with the AR model equation, expressed in terms of the error:\n$$e[n] = x[n] + a_{1} x[n-1] + a_{2} x[n-2]$$\nWe multiply this equation by $x^{*}[n-k]$ for $k=1, 2$ and take the expectation.\n\nFor $k=1$:\n$$ \\mathbb{E}\\{e[n] x^{*}[n-1]\\} = \\mathbb{E}\\{x[n] x^{*}[n-1]\\} + a_{1} \\mathbb{E}\\{x[n-1] x^{*}[n-1]\\} + a_{2} \\mathbb{E}\\{x[n-2] x^{*}[n-1]\\} = 0 $$\nUsing the definition of the autocorrelation function $r_{x}[k] \\triangleq \\mathbb{E}\\{x[n] x^{*}[n-k]\\}$ and its Hermitian symmetry property $r_{x}[-k] = r_{x}^{*}[k]$, this becomes:\n$$ r_{x}[1] + a_{1} r_{x}[0] + a_{2} r_{x}[-1] = 0 \\implies r_{x}[1] + a_{1} r_{x}[0] + a_{2} r_{x}^{*}[1] = 0 $$\n\nFor $k=2$:\n$$ \\mathbb{E}\\{e[n] x^{*}[n-2]\\} = \\mathbb{E}\\{x[n] x^{*}[n-2]\\} + a_{1} \\mathbb{E}\\{x[n-1] x^{*}[n-2]\\} + a_{2} \\mathbb{E}\\{x[n-2] x^{*}[n-2]\\} = 0 $$\nIn terms of autocorrelations, this is:\n$$ r_{x}[2] + a_{1} r_{x}[1] + a_{2} r_{x}[0] = 0 $$\nThese two relations constitute the Yule-Walker equations for an AR($2$) process. We rearrange them to solve for $a_1$ and $a_2$:\n$$ a_{1} r_{x}[0] + a_{2} r_{x}^{*}[1] = -r_{x}[1] $$\n$$ a_{1} r_{x}[1] + a_{2} r_{x}[0] = -r_{x}[2] $$\nThis system of linear equations is correctly expressed in the requested Hermitian Toeplitz matrix form:\n$$ \\begin{pmatrix} r_{x}[0]  r_{x}^{*}[1] \\\\ r_{x}[1]  r_{x}[0] \\end{pmatrix} \\begin{pmatrix} a_{1} \\\\ a_{2} \\end{pmatrix} = -\\begin{pmatrix} r_{x}[1] \\\\ r_{x}[2] \\end{pmatrix} $$\n\nNext, we solve this system using the provided autocorrelation values:\n$r_{x}[0] = 1$\n$r_{x}[1] = 0.5 \\exp(j \\pi/4) = \\frac{1}{2}\\left(\\cos(\\frac{\\pi}{4}) + j\\sin(\\frac{\\pi}{4})\\right) = \\frac{1}{2}\\left(\\frac{\\sqrt{2}}{2} + j\\frac{\\sqrt{2}}{2}\\right) = \\frac{\\sqrt{2}}{4}(1+j)$\n$r_{x}[2] = 0.2 \\exp(j \\pi/2) = \\frac{1}{5}j$\n\nWe also need the conjugate of $r_{x}[1]$: $r_{x}^{*}[1] = \\frac{\\sqrt{2}}{4}(1-j)$.\nThe autocorrelation matrix is $\\mathbf{R}_{2} = \\begin{pmatrix} 1  \\frac{\\sqrt{2}}{4}(1-j) \\\\ \\frac{\\sqrt{2}}{4}(1+j)  1 \\end{pmatrix}$.\nThe determinant is $\\det(\\mathbf{R}_{2}) = (1)(1) - \\left(\\frac{\\sqrt{2}}{4}(1+j)\\right)\\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) = 1 - \\frac{2}{16}(1-j^{2}) = 1 - \\frac{1}{8}(2) = \\frac{3}{4}$.\n\nWe use Cramer's rule to find the coefficients.\nFor $a_{1}$:\n$$ a_{1} = \\frac{1}{\\det(\\mathbf{R}_{2})} \\det\\begin{pmatrix} -r_{x}[1]  r_{x}^{*}[1] \\\\ -r_{x}[2]  r_{x}[0] \\end{pmatrix} = \\frac{4}{3} \\left( -r_{x}[1] r_{x}[0] - (-r_{x}[2]) r_{x}^{*}[1] \\right) $$\n$$ a_{1} = \\frac{4}{3} \\left( -\\frac{\\sqrt{2}}{4}(1+j) + \\left(\\frac{1}{5}j\\right) \\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) \\right) = \\frac{4}{3} \\left( -\\frac{\\sqrt{2}}{4}(1+j) + \\frac{\\sqrt{2}}{20}(j-j^{2}) \\right) $$\n$$ a_{1} = \\frac{\\sqrt{2}}{3} \\left( -(1+j) + \\frac{1}{5}(1+j) \\right) = \\frac{\\sqrt{2}}{3} (1+j) \\left(-1 + \\frac{1}{5}\\right) = -\\frac{4\\sqrt{2}}{15}(1+j) $$\nFor $a_{2}$:\n$$ a_{2} = \\frac{1}{\\det(\\mathbf{R}_{2})} \\det\\begin{pmatrix} r_{x}[0]  -r_{x}[1] \\\\ r_{x}[1]  -r_{x}[2] \\end{pmatrix} = \\frac{4}{3} \\left( r_{x}[0](-r_{x}[2]) - (-r_{x}[1])r_{x}[1] \\right) $$\n$$ a_{2} = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\left(\\frac{\\sqrt{2}}{4}(1+j)\\right)^{2} \\right) = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{2}{16}(1+2j+j^{2}) \\right) $$\n$$ a_{2} = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{1}{8}(2j) \\right) = \\frac{4}{3} \\left( -\\frac{1}{5}j + \\frac{1}{4}j \\right) = \\frac{4}{3} \\left(\\frac{-4+5}{20}j\\right) = \\frac{4}{3}\\frac{j}{20} = \\frac{1}{15}j $$\n\nFinally, we compute the innovation variance $\\sigma_{e}^{2}$. We return to the AR equation, multiply by $x^{*}[n]$, and take the expectation:\n$$ \\mathbb{E}\\{x[n]x^{*}[n]\\} + a_{1}\\mathbb{E}\\{x[n-1]x^{*}[n]\\} + a_{2}\\mathbb{E}\\{x[n-2]x^{*}[n]\\} = \\mathbb{E}\\{e[n]x^{*}[n]\\} $$\nIn terms of autocorrelations: $r_{x}[0] + a_{1} r_{x}[-1] + a_{2} r_{x}[-2] = \\mathbb{E}\\{e[n]x^{*}[n]\\}$. Using Hermitian symmetry, this is $r_{x}[0] + a_{1} r_{x}^{*}[1] + a_{2} r_{x}^{*}[2] = \\mathbb{E}\\{e[n]x^{*}[n]\\}$.\nThe term $\\mathbb{E}\\{e[n]x^{*}[n]\\}$ simplifies. Since $x[n] = -\\sum_{k=1}^{2} a_{k}x[n-k] + e[n]$ and $e[n]$ is uncorrelated with past values of $x[n]$, we have $\\mathbb{E}\\{e[n]x^{*}[n]\\} = \\mathbb{E}\\{e[n](-\\sum_{k=1}^{2} a_{k}^{*}x^{*}[n-k] + e^{*}[n])\\} = \\mathbb{E}\\{e[n]e^{*}[n]\\} = \\sigma_{e}^{2}$.\nTherefore, the desired relationship for the innovation variance is:\n$$ \\sigma_{e}^{2} = r_{x}[0] + a_{1} r_{x}^{*}[1] + a_{2} r_{x}^{*}[2] $$\nWe substitute the known and computed values into this expression. We need $r_{x}^{*}[2] = (\\frac{1}{5}j)^{*} = -\\frac{1}{5}j$.\n$$ \\sigma_{e}^{2} = 1 + \\left(-\\frac{4\\sqrt{2}}{15}(1+j)\\right) \\left(\\frac{\\sqrt{2}}{4}(1-j)\\right) + \\left(\\frac{1}{15}j\\right) \\left(-\\frac{1}{5}j\\right) $$\nThe first product is:\n$$ a_{1} r_{x}^{*}[1] = -\\frac{4 \\cdot 2}{15 \\cdot 4}(1+j)(1-j) = -\\frac{8}{60}(1-j^{2}) = -\\frac{2}{15}(2) = -\\frac{4}{15} $$\nThe second product is:\n$$ a_{2} r_{x}^{*}[2] = -\\frac{1}{75}j^{2} = -\\frac{1}{75}(-1) = \\frac{1}{75} $$\nSumming the terms gives the variance:\n$$ \\sigma_{e}^{2} = 1 - \\frac{4}{15} + \\frac{1}{75} = \\frac{75}{75} - \\frac{20}{75} + \\frac{1}{75} = \\frac{75-20+1}{75} = \\frac{56}{75} $$\nThe fraction $\\frac{56}{75}$ is in its simplest form, as $56 = 2^{3} \\cdot 7$ and $75 = 3 \\cdot 5^{2}$ share no common factors.", "answer": "$$\\boxed{\\frac{56}{75}}$$", "id": "2853134"}, {"introduction": "A key motivation for AR modeling is to obtain a high-resolution estimate of a signal's power spectral density (PSD). This practice illuminates the direct relationship between an AR model's poles and the shape of its corresponding spectrum. By starting with a given pair of complex-conjugate poles [@problem_id:2853189], you will not only derive the exact PSD but also learn a valuable approximation technique to connect a pole's proximity to the unit circle with the bandwidth of the resulting spectral peak.", "problem": "A zero-mean, wide-sense stationary Autoregressive (AR) process of order $2$ is estimated from data using any consistent method such as the Yule–Walker equations, the Levinson–Durbin recursion, or the Burg algorithm. The identified pole locations in the complex $z$-plane are a complex-conjugate pair at radius $r \\in (0,1)$ and angle $\\theta \\in (0,\\pi)$, that is, at $z = r \\exp(j \\theta)$ and $z = r \\exp(-j \\theta)$. The process is driven by discrete-time white noise of variance $\\sigma_{w}^{2}$.\n\nStarting only from first principles of linear time-invariant systems and the definition of an Autoregressive model (namely, that an AR($p$) process is the output of an all-pole filter driven by white noise), carry out the following:\n\n1. Express the AR polynomial in terms of $r$ and $\\theta$ and write the corresponding transfer function $H(z)$ from the white noise input to the AR output.\n2. Derive the exact closed-form Power Spectral Density (PSD) $S_{x}(\\omega)$ of the AR($2$) process as a function of $\\omega$, $r$, $\\theta$, and $\\sigma_{w}^{2}$.\n3. Focusing on the spectral peak near $\\omega_{0} = \\theta$, perform a narrowband approximation by expanding the PSD denominator to second order in the small frequency offset $\\Delta = \\omega - \\theta$ and by treating any factor that varies slowly with $\\omega$ near $\\omega_{0}$ as locally constant. From this, define the $3$-dB bandwidth as the full width at half maximum (FWHM) of the peak and derive it in closed form as a function of $r$ only.\n\nProvide as your final answer the resulting analytic expression for the $3$-dB FWHM in radians (no units required in the final boxed expression). No numerical rounding is required.", "solution": "The solution is derived from first principles in three parts as requested.\n\n**Part 1: AR Polynomial and Transfer Function**\n\nAn Autoregressive process $x[n]$ of order $p$ is generated by filtering white noise $w[n]$ through an all-pole filter. The governing difference equation is:\n$$x[n] + \\sum_{k=1}^{p} a_k x[n-k] = w[n]$$\nTaking the Z-transform of this equation, we obtain:\n$$X(z) \\left( 1 + \\sum_{k=1}^{p} a_k z^{-k} \\right) = W(z)$$\nThe transfer function $H(z) = \\frac{X(z)}{W(z)}$ from the input noise to the output process is therefore:\n$$H(z) = \\frac{1}{1 + \\sum_{k=1}^{p} a_k z^{-k}} = \\frac{1}{A(z)}$$\nwhere $A(z)$ is the AR polynomial. The poles of the system are the roots of the denominator polynomial, i.e., the values of $z$ for which $A(z)=0$. For a stable and causal system, the polynomial $A(z)$ is expressed in terms of its roots (the poles $p_k$) as:\n$$A(z) = \\prod_{k=1}^{p} (1 - p_k z^{-1})$$\nFor the given AR($2$) process, the poles are $p_1 = r \\exp(j \\theta)$ and $p_2 = r \\exp(-j \\theta)$. Thus, the AR polynomial is:\n$$A(z) = (1 - r \\exp(j \\theta) z^{-1}) (1 - r \\exp(-j \\theta) z^{-1})$$\nExpanding this expression:\n$$A(z) = 1 - (r \\exp(j \\theta) + r \\exp(-j \\theta)) z^{-1} + (r^2 \\exp(j \\theta)\\exp(-j \\theta)) z^{-2}$$\nUsing Euler's identity, $\\exp(j\\phi) + \\exp(-j\\phi) = 2\\cos(\\phi)$, we simplify the polynomial to:\n$$A(z) = 1 - (2r \\cos(\\theta)) z^{-1} + r^2 z^{-2}$$\nThe corresponding transfer function is the inverse of this polynomial:\n$$H(z) = \\frac{1}{1 - 2r \\cos(\\theta) z^{-1} + r^2 z^{-2}}$$\n\n**Part 2: Power Spectral Density (PSD)**\n\nThe Power Spectral Density $S_x(\\omega)$ of the output process $x[n]$ is given by the formula:\n$$S_x(\\omega) = |H(\\exp(j \\omega))|^2 S_w(\\omega)$$\nwhere $S_w(\\omega)$ is the PSD of the input noise and $H(\\exp(j \\omega))$ is the frequency response of the filter. The input is discrete-time white noise with variance $\\sigma_w^2$, which has a constant PSD equal to its variance, $S_w(\\omega) = \\sigma_w^2$.\n\nThe frequency response is obtained by evaluating $H(z)$ on the unit circle, $z = \\exp(j \\omega)$:\n$$H(\\exp(j \\omega)) = \\frac{1}{A(\\exp(j \\omega))}$$\nThe squared magnitude of the frequency response is $|H(\\exp(j \\omega))|^2 = \\frac{1}{|A(\\exp(j \\omega))|^2}$. We evaluate the denominator:\n$$|A(\\exp(j \\omega))|^2 = |(1 - r \\exp(j \\theta) \\exp(-j \\omega)) (1 - r \\exp(-j \\theta) \\exp(-j \\omega))|^2$$\n$$|A(\\exp(j \\omega))|^2 = |1 - r \\exp(j(\\theta - \\omega))|^2 \\cdot |1 - r \\exp(-j(\\theta + \\omega))|^2$$\nUsing the identity $|1 - a\\exp(j\\phi)|^2 = (1-a\\cos\\phi)^2 + (a\\sin\\phi)^2 = 1 - 2a\\cos\\phi + a^2$, we find the two factors:\n$$|1 - r \\exp(j(\\theta - \\omega))|^2 = 1 - 2r \\cos(\\omega - \\theta) + r^2$$\n$$|1 - r \\exp(-j(\\theta + \\omega))|^2 = 1 - 2r \\cos(\\omega + \\theta) + r^2$$\nCombining these results, the exact PSD of the AR($2$) process is:\n$$S_x(\\omega) = \\frac{\\sigma_w^2}{(1 + r^2 - 2r \\cos(\\omega - \\theta))(1 + r^2 - 2r \\cos(\\omega + \\theta))}$$\n\n**Part 3: Narrowband Approximation and 3-dB Bandwidth**\n\nThe pole location at angle $\\theta$ creates a spectral peak around the frequency $\\omega_{0} = \\theta$. This occurs because the first term in the denominator, $D_1(\\omega) = 1 + r^2 - 2r \\cos(\\omega - \\theta)$, becomes minimal when $\\omega = \\theta$. For $r$ close to $1$, this term becomes very small, creating a sharp peak. The second term, $D_2(\\omega) = 1 + r^2 - 2r \\cos(\\omega + \\theta)$, varies slowly in the vicinity of $\\omega = \\theta$ (since $\\theta \\in (0,\\pi)$).\n\nWe perform a narrowband analysis by letting $\\omega = \\theta + \\Delta$, where $\\Delta$ is a small frequency offset.\nFor the rapidly varying term $D_1(\\omega)$, we use a second-order Taylor expansion for $\\cos(\\Delta)$ around $\\Delta=0$: $\\cos(\\Delta) \\approx 1 - \\frac{\\Delta^2}{2}$.\n$$D_1(\\theta+\\Delta) = 1 + r^2 - 2r \\cos(\\Delta) \\approx 1 + r^2 - 2r \\left(1 - \\frac{\\Delta^2}{2}\\right) = (1 - 2r + r^2) + r\\Delta^2 = (1-r)^2 + r\\Delta^2$$\nFor the slowly varying term $D_2(\\omega)$, we approximate it as constant by evaluating it at the peak frequency $\\omega = \\theta$:\n$$D_2(\\theta+\\Delta) \\approx D_2(\\theta) = 1 + r^2 - 2r \\cos(2\\theta)$$\nThe PSD near the peak is thus approximated as:\n$$S_x(\\theta+\\Delta) \\approx \\frac{\\sigma_w^2}{((1-r)^2 + r\\Delta^2)(1 + r^2 - 2r\\cos(2\\theta))}$$\nThe peak value of the PSD occurs at $\\Delta = 0$ (i.e., $\\omega = \\theta$):\n$$S_{peak} = S_x(\\theta) \\approx \\frac{\\sigma_w^2}{(1-r)^2(1 + r^2 - 2r\\cos(2\\theta))}$$\nThe $3$-dB bandwidth is the Full Width at Half Maximum (FWHM). We find the frequency offsets $\\pm\\Delta_{3dB}$ where the PSD is half of its peak value:\n$$S_x(\\theta \\pm \\Delta_{3dB}) = \\frac{1}{2} S_{peak}$$\n$$\\frac{\\sigma_w^2}{((1-r)^2 + r\\Delta_{3dB}^2)(1 + r^2 - 2r\\cos(2\\theta))} = \\frac{1}{2} \\frac{\\sigma_w^2}{(1-r)^2(1 + r^2 - 2r\\cos(2\\theta))}$$\nCanceling the common terms from both sides yields:\n$$\\frac{1}{(1-r)^2 + r\\Delta_{3dB}^2} = \\frac{1}{2(1-r)^2}$$\n$$(1-r)^2 + r\\Delta_{3dB}^2 = 2(1-r)^2$$\n$$r\\Delta_{3dB}^2 = (1-r)^2$$\n$$\\Delta_{3dB}^2 = \\frac{(1-r)^2}{r}$$\nThe half-bandwidth is therefore $|\\Delta_{3dB}| = \\frac{1-r}{\\sqrt{r}}$, since $r \\in (0,1)$. The FWHM, denoted $B_{3dB}$, is twice this value:\n$$B_{3dB} = 2 \\frac{1-r}{\\sqrt{r}}$$\nThis is the analytical expression for the $3$-dB FWHM as a function of $r$ only, derived from the specified narrowband approximation.", "answer": "$$\\boxed{2 \\frac{1-r}{\\sqrt{r}}}$$", "id": "2853189"}, {"introduction": "While the Yule-Walker equations provide a powerful theoretical framework, their practical application depends on estimating the autocorrelation sequence from a finite data record. This hands-on problem serves as a crucial cautionary tale, revealing the potential issues with seemingly intuitive estimators. By computing the eigenvalues of a Toeplitz matrix formed from the \"unbiased\" autocorrelation estimate [@problem_id:2853132], you will discover why this approach, despite its lack of bias, can yield physically meaningless results and why alternative estimators are often preferred in practice.", "problem": "A real-valued, length-$N$ data record $\\{x[n]\\}_{n=0}^{N-1}$ with $N=3$ is given by $x[0]=1$, $x[1]=0$, and $x[2]=-2$. Consider the unbiased estimator of the autocorrelation sequence defined, for integer lag $\\ell$ with $|\\ell| \\leq N-1$, by\n$$\n\\tilde{r}_{x}[\\ell] \\triangleq \\frac{1}{N-|\\ell|} \\sum_{n=|\\ell|}^{N-1} x[n]\\,x[n-\\ell].\n$$\nForm the $3 \\times 3$ Toeplitz autocorrelation matrix used in the AutoRegressive (AR) Yule–Walker normal equations by\n$$\n\\mathbf{R} \\triangleq \\begin{pmatrix}\n\\tilde{r}_{x}[0]  \\tilde{r}_{x}[1]  \\tilde{r}_{x}[2] \\\\\n\\tilde{r}_{x}[1]  \\tilde{r}_{x}[0]  \\tilde{r}_{x}[1] \\\\\n\\tilde{r}_{x}[2]  \\tilde{r}_{x}[1]  \\tilde{r}_{x}[0]\n\\end{pmatrix}.\n$$\nStarting only from the definitions above and standard linear algebra, compute the smallest eigenvalue of $\\mathbf{R}$ exactly. Provide your final answer as a single reduced fraction (no decimal approximation).", "solution": "The task is to find the smallest eigenvalue of the matrix $\\mathbf{R}$. The first step is to compute the necessary autocorrelation values $\\tilde{r}_{x}[\\ell]$ for $\\ell \\in \\{0, 1, 2\\}$ using the provided data sequence and the given estimator formula.\n\nThe data sequence is $\\{x[n]\\}_{n=0}^{2}$ with $x[0]=1$, $x[1]=0$, and $x[2]=-2$. The length is $N=3$.\n\nCalculation of $\\tilde{r}_{x}[0]$ (for $\\ell=0$):\n$$\n\\tilde{r}_{x}[0] = \\frac{1}{3-0} \\sum_{n=0}^{2} x[n]x[n-0] = \\frac{1}{3} \\sum_{n=0}^{2} (x[n])^{2}\n$$\n$$\n\\tilde{r}_{x}[0] = \\frac{1}{3} \\left( (x[0])^{2} + (x[1])^{2} + (x[2])^{2} \\right) = \\frac{1}{3} \\left( 1^{2} + 0^{2} + (-2)^{2} \\right) = \\frac{1}{3} (1+0+4) = \\frac{5}{3}\n$$\n\nCalculation of $\\tilde{r}_{x}[1]$ (for $\\ell=1$):\n$$\n\\tilde{r}_{x}[1] = \\frac{1}{3-1} \\sum_{n=1}^{2} x[n]x[n-1] = \\frac{1}{2} \\left( x[1]x[0] + x[2]x[1] \\right)\n$$\n$$\n\\tilde{r}_{x}[1] = \\frac{1}{2} \\left( (0)(1) + (-2)(0) \\right) = \\frac{1}{2}(0) = 0\n$$\n\nCalculation of $\\tilde{r}_{x}[2]$ (for $\\ell=2$):\n$$\n\\tilde{r}_{x}[2] = \\frac{1}{3-2} \\sum_{n=2}^{2} x[n]x[n-2] = 1 \\cdot \\left( x[2]x[0] \\right)\n$$\n$$\n\\tilde{r}_{x}[2] = (-2)(1) = -2\n$$\n\nNow, we construct the matrix $\\mathbf{R}$ using these values. The problem specifies a symmetric Toeplitz structure.\n$$\n\\mathbf{R} = \\begin{pmatrix}\n\\tilde{r}_{x}[0]  \\tilde{r}_{x}[1]  \\tilde{r}_{x}[2] \\\\\n\\tilde{r}_{x}[1]  \\tilde{r}_{x}[0]  \\tilde{r}_{x}[1] \\\\\n\\tilde{r}_{x}[2]  \\tilde{r}_{x}[1]  \\tilde{r}_{x}[0]\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{5}{3}  0  -2 \\\\\n0  \\frac{5}{3}  0 \\\\\n-2  0  \\frac{5}{3}\n\\end{pmatrix}\n$$\n\nTo find the eigenvalues $\\lambda$ of $\\mathbf{R}$, we solve the characteristic equation $\\det(\\mathbf{R} - \\lambda\\mathbf{I}) = 0$, where $\\mathbf{I}$ is the $3 \\times 3$ identity matrix.\n$$\n\\det\\left( \\begin{pmatrix}\n\\frac{5}{3}  0  -2 \\\\\n0  \\frac{5}{3}  0 \\\\\n-2  0  \\frac{5}{3}\n\\end{pmatrix} - \\lambda \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} \\right) = 0\n$$\n$$\n\\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda  0  -2 \\\\\n0  \\frac{5}{3} - \\lambda  0 \\\\\n-2  0  \\frac{5}{3} - \\lambda\n\\end{pmatrix} = 0\n$$\nWe use cofactor expansion along the second row, which simplifies the calculation.\n$$\n\\left(\\frac{5}{3} - \\lambda\\right) \\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda  -2 \\\\\n-2  \\frac{5}{3} - \\lambda\n\\end{pmatrix} - 0 + 0 = 0\n$$\nThis equation yields two possibilities. First:\n$$\n\\frac{5}{3} - \\lambda = 0 \\implies \\lambda_{1} = \\frac{5}{3}\n$$\nSecond:\n$$\n\\det\\begin{pmatrix}\n\\frac{5}{3} - \\lambda  -2 \\\\\n-2  \\frac{5}{3} - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} - (-2)(-2) = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} - 4 = 0\n$$\n$$\n\\left(\\frac{5}{3} - \\lambda\\right)^{2} = 4\n$$\nTaking the square root of both sides gives:\n$$\n\\frac{5}{3} - \\lambda = \\pm 2\n$$\nThis leads to two more eigenvalues:\nCase 1:\n$$\n\\frac{5}{3} - \\lambda = 2 \\implies \\lambda = \\frac{5}{3} - 2 = \\frac{5}{3} - \\frac{6}{3} = -\\frac{1}{3}\n$$\nSo, $\\lambda_{2} = -\\frac{1}{3}$.\nCase 2:\n$$\n\\frac{5}{3} - \\lambda = -2 \\implies \\lambda = \\frac{5}{3} + 2 = \\frac{5}{3} + \\frac{6}{3} = \\frac{11}{3}\n$$\nSo, $\\lambda_{3} = \\frac{11}{3}$.\n\nThe three eigenvalues of the matrix $\\mathbf{R}$ are $\\left\\{ \\frac{11}{3}, \\frac{5}{3}, -\\frac{1}{3} \\right\\}$.\nWe are asked for the smallest eigenvalue. Comparing the three values:\n$$\n\\frac{11}{3} \\approx 3.67\n$$\n$$\n\\frac{5}{3} \\approx 1.67\n$$\n$$\n-\\frac{1}{3} \\approx -0.33\n$$\nThe smallest value is $-\\frac{1}{3}$. This is a reduced fraction as required. It is notable that an autocorrelation matrix, which for a wide-sense stationary process must be non-negative definite, here has a negative eigenvalue. This is possible because $\\mathbf{R}$ is constructed from a finite-sample *estimator*, which does not guarantee the non-negative definite property.", "answer": "$$\\boxed{-\\frac{1}{3}}$$", "id": "2853132"}]}