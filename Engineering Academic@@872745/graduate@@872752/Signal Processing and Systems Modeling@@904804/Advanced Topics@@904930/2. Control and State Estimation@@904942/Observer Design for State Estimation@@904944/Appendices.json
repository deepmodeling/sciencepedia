{"hands_on_practices": [{"introduction": "This first practice lays the groundwork for deterministic observer design. You will calculate the observer gain for a continuous-time system to place the eigenvalues of the error dynamics at specific locations, thereby dictating the convergence rate of your state estimate. This exercise solidifies the core concepts of error dynamics and pole placement, which are fundamental to ensuring that an observer performs as intended [@problem_id:2888317].", "problem": "Consider the continuous-time Linear Time-Invariant (LTI) state-space model\n$$\n\\dot{x}(t)=A\\,x(t),\\quad y(t)=C\\,x(t),\n$$\nwhere $x(t)\\in\\mathbb{R}^{2}$ is the state and $y(t)\\in\\mathbb{R}$ is the measured output. The system matrices are\n$$\nA=\\begin{bmatrix}01\\\\-2-3\\end{bmatrix},\\qquad C=\\begin{bmatrix}10\\end{bmatrix}.\n$$\nYou are to design a continuous-time Luenberger observer (LO) gain $L\\in\\mathbb{R}^{2\\times 1}$ for the estimator\n$$\n\\dot{\\hat{x}}(t)=A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big),\n$$\nso that the estimator error dynamics achieves asymptotic convergence with eigenvalues at $\\{-4,-5\\}$. Begin from first principles: use the state and output equations and the definition of the observer to derive the error dynamics, use the definition of observability to justify assignability of the error dynamics eigenvalues, and then impose the desired eigenvalues by matching the characteristic polynomial of the error dynamics to the target one. Provide the exact $L$ that accomplishes this. Express your final answer as a $1\\times 2$ row matrix listing the entries of $L$ from top to bottom. No rounding is required and no units are involved.", "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed problem in linear systems theory, specifically concerning observer design. The provided information is complete, consistent, and scientifically sound. All terms are defined, and the objective is unambiguous. We may proceed with the solution.\n\nThe problem requires the design of a Luenberger observer for a given continuous-time LTI system. The system dynamics are described by:\n$$\n\\dot{x}(t)=A\\,x(t)\n$$\n$$\ny(t)=C\\,x(t)\n$$\nwith state $x(t)\\in\\mathbb{R}^{2}$, output $y(t)\\in\\mathbb{R}$, and system matrices:\n$$\nA=\\begin{bmatrix}01\\\\-2-3\\end{bmatrix},\\qquad C=\\begin{bmatrix}10\\end{bmatrix}\n$$\nThe Luenberger observer is designed to estimate the state $x(t)$ using a model of the system and a correction term proportional to the output estimation error. The state estimate $\\hat{x}(t)$ evolves according to:\n$$\n\\dot{\\hat{x}}(t)=A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big)\n$$\nwhere $L \\in \\mathbb{R}^{2 \\times 1}$ is the observer gain matrix that we must determine. The objective is to place the eigenvalues of the estimator error dynamics at $\\{-4, -5\\}$ to ensure asymptotic convergence of the estimate $\\hat{x}(t)$ to the true state $x(t)$.\n\nFirst, we derive the dynamics of the estimation error, defined as $e(t) = x(t) - \\hat{x}(t)$. Differentiating the error with respect to time gives:\n$$\n\\dot{e}(t) = \\dot{x}(t) - \\dot{\\hat{x}}(t)\n$$\nSubstitute the expressions for $\\dot{x}(t)$ and $\\dot{\\hat{x}}(t)$:\n$$\n\\dot{e}(t) = A\\,x(t) - \\left[ A\\,\\hat{x}(t)+L\\big(y(t)-C\\,\\hat{x}(t)\\big) \\right]\n$$\nNow, substitute $y(t) = C\\,x(t)$ into the equation:\n$$\n\\dot{e}(t) = A\\,x(t) - A\\,\\hat{x}(t) - L\\big(C\\,x(t)-C\\,\\hat{x}(t)\\big)\n$$\nFactoring the terms, we obtain:\n$$\n\\dot{e}(t) = A\\big(x(t) - \\hat{x}(t)\\big) - L\\,C\\big(x(t) - \\hat{x}(t)\\big)\n$$\nRecognizing that $e(t) = x(t) - \\hat{x}(t)$, the error dynamics are governed by the linear homogeneous differential equation:\n$$\n\\dot{e}(t) = (A - L\\,C)e(t)\n$$\nThe stability of the error dynamics, and thus the convergence of $\\hat{x}(t)$ to $x(t)$, is determined by the eigenvalues of the matrix $A_e = A - L\\,C$. For the error to converge to zero, all eigenvalues of $A_e$ must have negative real parts. The problem demands specific eigenvalues at $\\{-4, -5\\}$.\n\nThe ability to arbitrarily place the eigenvalues of $A - L\\,C$ by choosing $L$ is guaranteed if and only if the system pair $(A, C)$ is observable. We must verify this condition. The system is observable if the observability matrix $\\mathcal{O}$ has full rank, which for a state dimension of $n=2$ means $\\text{rank}(\\mathcal{O})=2$. The observability matrix is constructed as:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nFirst, we compute the product $CA$:\n$$\nCA = \\begin{bmatrix}10\\end{bmatrix} \\begin{bmatrix}01\\\\-2-3\\end{bmatrix} = \\begin{bmatrix}(1)(0)+(0)(-2)  (1)(1)+(0)(-3)\\end{bmatrix} = \\begin{bmatrix}01\\end{bmatrix}\n$$\nNow, we construct the observability matrix $\\mathcal{O}$:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}\n$$\nThis is the $2 \\times 2$ identity matrix. Its determinant is $\\det(\\mathcal{O}) = 1 \\neq 0$, so its rank is $2$. Since the rank of $\\mathcal{O}$ is equal to the dimension of the state space, the system is completely observable. Therefore, the eigenvalues of the error dynamics matrix $A - L\\,C$ can be arbitrarily placed by selecting an appropriate gain matrix $L$.\n\nWe now proceed to find $L$. Let the gain matrix be $L = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix}$. The matrix $A - L\\,C$ is:\n$$\nA - L\\,C = \\begin{bmatrix}01\\\\-2-3\\end{bmatrix} - \\begin{bmatrix}l_1\\\\l_2\\end{bmatrix}\\begin{bmatrix}10\\end{bmatrix} = \\begin{bmatrix}01\\\\-2-3\\end{bmatrix} - \\begin{bmatrix}l_1  0 \\\\ l_2  0\\end{bmatrix} = \\begin{bmatrix}-l_1  1 \\\\ -2-l_2  -3\\end{bmatrix}\n$$\nThe eigenvalues of this matrix are the roots of its characteristic polynomial, $p(\\lambda) = \\det(\\lambda I - (A - L\\,C))$.\n$$\n\\lambda I - (A - L\\,C) = \\begin{bmatrix}\\lambda  0 \\\\ 0  \\lambda\\end{bmatrix} - \\begin{bmatrix}-l_1  1 \\\\ -2-l_2  -3\\end{bmatrix} = \\begin{bmatrix}\\lambda+l_1  -1 \\\\ 2+l_2  \\lambda+3\\end{bmatrix}\n$$\nThe characteristic polynomial is:\n$$\np(\\lambda) = (\\lambda+l_1)(\\lambda+3) - (-1)(2+l_2) = \\lambda^2 + 3\\lambda + l_1\\lambda + 3l_1 + 2 + l_2\n$$\n$$\np(\\lambda) = \\lambda^2 + (3+l_1)\\lambda + (3l_1 + l_2 + 2)\n$$\nThe desired eigenvalues are given as $\\{-4, -5\\}$. The desired characteristic polynomial, $p_{des}(\\lambda)$, is formed from these eigenvalues:\n$$\np_{des}(\\lambda) = (\\lambda - (-4))(\\lambda - (-5)) = (\\lambda+4)(\\lambda+5) = \\lambda^2 + 9\\lambda + 20\n$$\nTo achieve the desired pole placement, we equate the coefficients of the actual characteristic polynomial $p(\\lambda)$ with those of the desired polynomial $p_{des}(\\lambda)$:\n$$\n\\lambda^2 + (3+l_1)\\lambda + (3l_1 + l_2 + 2) = \\lambda^2 + 9\\lambda + 20\n$$\nComparing the coefficients for each power of $\\lambda$:\n\\begin{enumerate}\n    \\item Coefficient of $\\lambda^1$: $3+l_1 = 9$\n    \\item Coefficient of $\\lambda^0$: $3l_1 + l_2 + 2 = 20$\n\\end{enumerate}\nFrom the first equation, we find $l_1$:\n$$\nl_1 = 9 - 3 = 6\n$$\nSubstitute $l_1=6$ into the second equation to find $l_2$:\n$$\n3(6) + l_2 + 2 = 20\n$$\n$$\n18 + l_2 + 2 = 20\n$$\n$$\n20 + l_2 = 20\n$$\n$$\nl_2 = 0\n$$\nThus, the required observer gain matrix is:\n$$\nL = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\n$$\nThis gain ensures that the error dynamics $\\dot{e}(t) = (A-LC)e(t)$ have eigenvalues at $\\{-4, -5\\}$, guaranteeing that the estimation error $e(t)$ converges asymptotically to zero.", "answer": "$$\n\\boxed{\\begin{pmatrix} 6  0 \\end{pmatrix}}\n$$", "id": "2888317"}, {"introduction": "Building on the principles of continuous-time design, this exercise transitions to the discrete-time domain, which is essential for digital implementation. You will again use pole placement to design an observer gain, but this time for a discrete-time system, paying close attention to the differences in stability criteria and algebraic formulation. This practice demonstrates the versatility of the pole placement technique and reinforces your understanding of observability and its role in state estimation [@problem_id:2888341].", "problem": "Consider a discrete-time Linear Time-Invariant (LTI) system with state matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and output matrix $C \\in \\mathbb{R}^{1 \\times 2}$ given by\n$$\nA=\\begin{bmatrix}1  0.1 \\\\ 0  0.9\\end{bmatrix}, \\quad C=\\begin{bmatrix}1  0\\end{bmatrix}.\n$$\nA full-order state observer is to be designed so that the state-estimation error dynamics are asymptotically stable with prescribed eigenvalues. Determine the observer gain $L \\in \\mathbb{R}^{2 \\times 1}$ that places the eigenvalues of the error dynamics at $\\{0.5,\\,0.6\\}$. Provide the exact values of the entries of $L$. Your final answer should be a single analytical expression listing the entries of $L$ in a single row using standard matrix notation. No rounding is required.", "solution": "The problem will first be validated for scientific and logical consistency.\n\nStep 1: Extract Givens.\nThe problem provides the following:\n- A discrete-time Linear Time-Invariant (LTI) system.\n- State matrix $A = \\begin{bmatrix} 1  0.1 \\\\ 0  0.9 \\end{bmatrix}$, with $A \\in \\mathbb{R}^{2 \\times 2}$.\n- Output matrix $C = \\begin{bmatrix} 1  0 \\end{bmatrix}$, with $C \\in \\mathbb{R}^{1 \\times 2}$.\n- A full-order state observer is to be designed.\n- The desired eigenvalues for the state-estimation error dynamics are $\\{0.5, 0.6\\}$.\n- The objective is to find the observer gain matrix $L \\in \\mathbb{R}^{2 \\times 1}$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is a standard exercise in control theory, specifically in state estimation and observer design (pole placement). The possibility of arbitrarily placing the eigenvalues of the observer error dynamics is contingent upon the observability of the system pair $(A, C)$. A system is completely observable if and only if its observability matrix $\\mathcal{O}$ has full rank. For this system, the state dimension is $n=2$. The observability matrix is given by\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nWe compute the product $CA$:\n$$\nCA = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 1  0.1 \\\\ 0  0.9 \\end{bmatrix} = \\begin{bmatrix} 1  0.1 \\end{bmatrix}\n$$\nThus, the observability matrix is:\n$$\n\\mathcal{O} = \\begin{bmatrix} 1  0 \\\\ 1  0.1 \\end{bmatrix}\n$$\nThe determinant of this matrix is $\\det(\\mathcal{O}) = (1)(0.1) - (0)(1) = 0.1$. Since $\\det(\\mathcal{O}) \\neq 0$, the matrix has full rank, which is rank $2$. The system is therefore completely observable. This confirms that the eigenvalues of the error dynamics can be arbitrarily placed, and a unique observer gain $L$ exists. The problem is well-posed, scientifically sound, and contains all necessary information.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be provided.\n\nThe structure of a full-order Luenberger observer for a discrete-time LTI system is given by:\n$$\n\\hat{x}(k+1) = A\\hat{x}(k) + L(y(k) - \\hat{y}(k))\n$$\nwhere $\\hat{x}(k)$ is the state estimate at time $k$, $y(k) = Cx(k)$ is the system output, and $\\hat{y}(k) = C\\hat{x}(k)$ is the estimated output. The observer gain is $L$. Substituting $\\hat{y}(k)$, the observer dynamics become:\n$$\n\\hat{x}(k+1) = A\\hat{x}(k) + L(Cx(k) - C\\hat{x}(k)) = (A - LC)\\hat{x}(k) + LCx(k)\n$$\nThe state estimation error is defined as $e(k) = x(k) - \\hat{x}(k)$. The dynamics of this error are derived as follows, using the system state equation $x(k+1) = Ax(k)$:\n$$\n\\begin{aligned}\ne(k+1) = x(k+1) - \\hat{x}(k+1) \\\\\n= Ax(k) - \\left( (A - LC)\\hat{x}(k) + LCx(k) \\right) \\\\\n= Ax(k) - A\\hat{x}(k) + LC\\hat{x}(k) - LCx(k) \\\\\n= A(x(k) - \\hat{x}(k)) - LC(x(k) - \\hat{x}(k)) \\\\\n= (A - LC)e(k)\n\\end{aligned}\n$$\nThe stability and response of the error are determined by the eigenvalues of the matrix $A-LC$. The objective is to select the observer gain $L$ such that these eigenvalues are located at the prescribed positions $\\{0.5, 0.6\\}$.\n\nThe gain matrix $L$ is of dimension $2 \\times 1$. Let its entries be $l_1$ and $l_2$:\n$$\nL = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix}\n$$\nFirst, we compute the product $LC$:\n$$\nLC = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix} \\begin{bmatrix} 1  0 \\end{bmatrix} = \\begin{bmatrix} l_1  0 \\\\ l_2  0 \\end{bmatrix}\n$$\nNext, we form the matrix $A - LC$:\n$$\nA - LC = \\begin{bmatrix} 1  0.1 \\\\ 0  0.9 \\end{bmatrix} - \\begin{bmatrix} l_1  0 \\\\ l_2  0 \\end{bmatrix} = \\begin{bmatrix} 1 - l_1  0.1 \\\\ -l_2  0.9 \\end{bmatrix}\n$$\nThe eigenvalues $\\lambda$ of this matrix are the roots of its characteristic polynomial, given by $\\det(\\lambda I - (A - LC)) = 0$.\n$$\n\\det \\begin{pmatrix} \\lambda - (1 - l_1)  -0.1 \\\\ -(-l_2)  \\lambda - 0.9 \\end{pmatrix} = 0\n$$\n$$\n(\\lambda - 1 + l_1)(\\lambda - 0.9) - (-0.1)(l_2) = 0\n$$\nExpanding this expression gives the characteristic polynomial:\n$$\n\\lambda^2 - 0.9\\lambda - \\lambda + 0.9 + l_1\\lambda - 0.9l_1 + 0.1l_2 = 0\n$$\n$$\n\\lambda^2 + (l_1 - 1.9)\\lambda + (0.9 - 0.9l_1 + 0.1l_2) = 0\n$$\nThe desired eigenvalues are $\\{0.5, 0.6\\}$. The desired characteristic polynomial is therefore:\n$$\n(\\lambda - 0.5)(\\lambda - 0.6) = 0\n$$\n$$\n\\lambda^2 - 0.6\\lambda - 0.5\\lambda + (0.5)(0.6) = 0\n$$\n$$\n\\lambda^2 - 1.1\\lambda + 0.3 = 0\n$$\nTo achieve the desired eigenvalue placement, we equate the coefficients of the derived characteristic polynomial with those of the desired polynomial.\nComparing the coefficients of the $\\lambda^1$ term:\n$$\nl_1 - 1.9 = -1.1\n$$\n$$\nl_1 = 1.9 - 1.1 = 0.8\n$$\nComparing the constant terms (coefficients of $\\lambda^0$):\n$$\n0.9 - 0.9l_1 + 0.1l_2 = 0.3\n$$\nSubstitute the value $l_1 = 0.8$ into this equation:\n$$\n0.9 - 0.9(0.8) + 0.1l_2 = 0.3\n$$\n$$\n0.9 - 0.72 + 0.1l_2 = 0.3\n$$\n$$\n0.18 + 0.1l_2 = 0.3\n$$\n$$\n0.1l_2 = 0.3 - 0.18 = 0.12\n$$\n$$\nl_2 = \\frac{0.12}{0.1} = 1.2\n$$\nThus, the entries of the observer gain matrix $L$ are $l_1 = 0.8$ and $l_2 = 1.2$. The observer gain matrix is:\n$$\nL = \\begin{bmatrix} 0.8 \\\\ 1.2 \\end{bmatrix}\n$$\nThe problem requires listing the entries of $L$ in a single row. This corresponds to the components $l_1$ and $l_2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8  1.2 \\end{pmatrix}}\n$$", "id": "2888341"}, {"introduction": "This final practice moves beyond deterministic design into the realm of stochastic estimation with the Kalman filter. Instead of arbitrarily placing poles, you will compute the optimal observer gain by executing one predict-update cycle of the filter, which minimizes the estimation error covariance in the presence of process and measurement noise. This exercise provides a hands-on introduction to the recursive logic of the Kalman filter, a cornerstone of modern estimation theory used in countless applications from navigation to finance [@problem_id:2888322].", "problem": "A discrete-time linear time-invariant (LTI) state-space model for a constant-velocity motion in one spatial dimension is given by the stochastic difference equations\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad y_{k} = C x_{k} + v_{k},\n$$\nwhere $x_{k} \\in \\mathbb{R}^{2}$ is the state with components position and velocity, $y_{k} \\in \\mathbb{R}$ is the position measurement, $w_{k}$ is zero-mean process noise with covariance $Q$, and $v_{k}$ is zero-mean measurement noise with covariance $R$. Assume the sampling interval is $1$ time unit so that\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1  0 \\end{bmatrix}, \\quad Q = \\operatorname{diag}(0.01,\\,0.01), \\quad R = \\begin{bmatrix} 0.04 \\end{bmatrix}.\n$$\nSuppose the initial a posteriori error covariance at time $k=0$ is\n$$\nP_{0} = I_{2}.\n$$\nStarting from this prior, perform one full Kalman filter (KF) prediction-update cycle to obtain the a posteriori error covariance at time $k=1$. Your derivation should begin from the definitions of the estimation error covariance and the minimum-variance linear estimator for linear Gaussian systems and proceed to the covariance recursion for prediction and update. Compute all intermediate quantities necessary for the covariance update, but report only the a posteriori error covariance matrix at time $k=1$ as your final answer. Round each reported matrix entry to $4$ significant figures. No physical units are required.", "solution": "The user-provided problem statement has been evaluated and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of state estimation, containing all necessary information for a unique solution via the standard Kalman filter algorithm. The parameters are self-consistent and realistic for a theoretical exercise.\n\nThe objective is to compute the a posteriori error covariance matrix at time $k=1$, denoted $P_{1|1}$, given the a posteriori error covariance at time $k=0$, denoted $P_{0|0}$. The Kalman filter provides the optimal linear estimator for a linear system with Gaussian noise by propagating the state estimate and its error covariance through a two-step recursive cycle: a prediction (time update) step and an update (measurement update) step.\n\nThe covariance propagation equations are as follows.\nPrediction:\n$$\nP_{k|k-1} = A P_{k-1|k-1} A^T + Q\n$$\nUpdate:\n$$\nK_k = P_{k|k-1} C^T (C P_{k|k-1} C^T + R)^{-1}\n$$\n$$\nP_{k|k} = (I - K_k C) P_{k|k-1}\n$$\nHere, $P_{k|k-1}$ is the a priori error covariance at time $k$, and $P_{k|k}$ is the a posteriori error covariance at time $k$. $K_k$ is the Kalman gain.\n\nThe given parameters are:\nState transition matrix: $A = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}$\nMeasurement matrix: $C = \\begin{bmatrix} 1  0 \\end{bmatrix}$\nProcess noise covariance: $Q = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$\nMeasurement noise covariance: $R = [0.04]$\nInitial a posteriori error covariance at $k=0$: $P_{0|0} = I_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$\n\nWe shall now perform one full cycle for $k=1$.\n\n**Step 1: Prediction (Time Update)**\n\nWe compute the a priori error covariance at $k=1$, $P_{1|0}$, using the state of the covariance at $k=0$.\n$$\nP_{1|0} = A P_{0|0} A^T + Q\n$$\nSubstituting the given values:\n$$\nA P_{0|0} A^T = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 1 \\cdot 1  1 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1  0 \\cdot 0 + 1 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 \\end{bmatrix}\n$$\nNow, we add the process noise covariance $Q$:\n$$\nP_{1|0} = \\begin{bmatrix} 2  1 \\\\ 1  1 \\end{bmatrix} + \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix} = \\begin{bmatrix} 2.01  1 \\\\ 1  1.01 \\end{bmatrix}\n$$\nThis is the predicted error covariance before incorporating the measurement at $k=1$.\n\n**Step 2: Update (Measurement Update)**\n\nFirst, we compute the innovation covariance, which we denote as $S_1$:\n$$\nS_1 = C P_{1|0} C^T + R\n$$\n$$\nC P_{1|0} C^T = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 2.01  1 \\\\ 1  1.01 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.01  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = [2.01]\n$$\n$$\nS_1 = [2.01] + [0.04] = [2.05]\n$$\nNext, we compute the Kalman gain $K_1$:\n$$\nK_1 = P_{1|0} C^T S_1^{-1}\n$$\n$$\nP_{1|0} C^T = \\begin{bmatrix} 2.01  1 \\\\ 1  1.01 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.01 \\\\ 1 \\end{bmatrix}\n$$\n$$\nK_1 = \\begin{bmatrix} 2.01 \\\\ 1 \\end{bmatrix} [2.05]^{-1} = \\begin{bmatrix} \\frac{2.01}{2.05} \\\\ \\frac{1}{2.05} \\end{bmatrix}\n$$\nFinally, we compute the a posteriori error covariance $P_{1|1}$:\n$$\nP_{1|1} = (I - K_1 C) P_{1|0}\n$$\nLet us compute the term $(I - K_1 C)$:\n$$\nK_1 C = \\begin{bmatrix} \\frac{2.01}{2.05} \\\\ \\frac{1}{2.05} \\end{bmatrix} \\begin{bmatrix} 1  0 \\end{bmatrix} = \\begin{bmatrix} \\frac{2.01}{2.05}  0 \\\\ \\frac{1}{2.05}  0 \\end{bmatrix}\n$$\n$$\nI - K_1 C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} \\frac{2.01}{2.05}  0 \\\\ \\frac{1}{2.05}  0 \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{2.01}{2.05}  0 \\\\ -\\frac{1}{2.05}  1 \\end{bmatrix} = \\begin{bmatrix} \\frac{0.04}{2.05}  0 \\\\ -\\frac{1}{2.05}  1 \\end{bmatrix}\n$$\nNow we perform the final matrix multiplication:\n$$\nP_{1|1} = \\begin{bmatrix} \\frac{0.04}{2.05}  0 \\\\ -\\frac{1}{2.05}  1 \\end{bmatrix} \\begin{bmatrix} 2.01  1 \\\\ 1  1.01 \\end{bmatrix}\n$$\n$$\nP_{1|1} = \\begin{bmatrix} (\\frac{0.04}{2.05})(2.01) + (0)(1)  (\\frac{0.04}{2.05})(1) + (0)(1.01) \\\\ (-\\frac{1}{2.05})(2.01) + (1)(1)  (-\\frac{1}{2.05})(1) + (1)(1.01) \\end{bmatrix}\n$$\n$$\nP_{1|1} = \\begin{bmatrix} \\frac{0.0804}{2.05}  \\frac{0.04}{2.05} \\\\ \\frac{-2.01 + 2.05}{2.05}  \\frac{-1 + 1.01 \\cdot 2.05}{2.05} \\end{bmatrix} = \\begin{bmatrix} \\frac{0.0804}{2.05}  \\frac{0.04}{2.05} \\\\ \\frac{0.04}{2.05}  \\frac{1.0705}{2.05} \\end{bmatrix}\n$$\nThe problem requires the matrix entries to be rounded to $4$ significant figures.\n$$\nP_{1|1}(1,1) = \\frac{0.0804}{2.05} \\approx 0.0392195... \\approx 0.03922\n$$\n$$\nP_{1|1}(1,2) = P_{1|1}(2,1) = \\frac{0.04}{2.05} \\approx 0.0195121... \\approx 0.01951\n$$\n$$\nP_{1|1}(2,2) = \\frac{1.0705}{2.05} \\approx 0.5221951... \\approx 0.5222\n$$\nThus, the a posteriori error covariance matrix at $k=1$ is:\n$$\nP_{1|1} \\approx \\begin{bmatrix} 0.03922  0.01951 \\\\ 0.01951  0.5222 \\end{bmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.03922  0.01951 \\\\ 0.01951  0.5222 \\end{pmatrix}}\n$$", "id": "2888322"}]}