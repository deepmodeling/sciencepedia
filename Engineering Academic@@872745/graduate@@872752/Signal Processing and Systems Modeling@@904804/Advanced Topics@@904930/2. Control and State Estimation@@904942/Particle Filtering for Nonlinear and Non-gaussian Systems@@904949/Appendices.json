{"hands_on_practices": [{"introduction": "A key challenge in particle filtering is weight degeneracy, where after a few iterations, a small fraction of particles holds most of the weight. This impoverishes the particle representation and increases the variance of state estimates. To monitor and combat this, we use a metric called the Effective Sample Size ($N_{\\text{eff}}$). This first exercise [@problem_id:2890403] provides a foundational, hands-on calculation of the ESS from a set of particle weights and demonstrates its use in a practical resampling decision.", "problem": "Consider a Sequential Monte Carlo particle filter for a nonlinear and non-Gaussian state-space model at time index $t$, where the filtering distribution is represented by $N$ weighted particles. You are given $N = 8$ particles with normalized importance weights\n$$\n\\big(w_{1},w_{2},\\ldots,w_{8}\\big) = \\big(0.35,\\;0.20,\\;0.15,\\;0.10,\\;0.08,\\;0.06,\\;0.04,\\;0.02\\big),\n$$\nwhich satisfy $\\sum_{i=1}^{8} w_{i} = 1$. The resampling criterion is defined by a threshold $\\tau N$ with $\\tau \\in (0,1)$, where $\\tau = 0.60$.\n\nStarting from the foundational definition of importance sampling and the notion that weight degeneracy increases the variance of weighted estimators, derive the Effective Sample Size (ESS) of the particle system in terms of the normalized weights, then compute its numerical value for the given weights. Based on your computed ESS, determine whether resampling should be triggered under the threshold rule $\\text{trigger if } \\text{ESS} \\le \\tau N$.\n\nRound your final numerical value of the Effective Sample Size to four significant figures. Report only the Effective Sample Size as your final numeric answer (dimensionless). Do not include any units in your final numeric answer.", "solution": "The problem posed is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of Sequential Monte Carlo methods, specifically particle filtering. All necessary data are provided, and the requested tasks—derivation, computation, and decision-making—are clear and logically follow from the premises. The problem is valid.\n\nThe central task is to evaluate the necessity of resampling in a particle filter. Resampling is a mechanism to combat the problem of *weight degeneracy*, a phenomenon where, after several iterations of a particle filter, the variance of the importance weights increases. This leads to a situation where a small number of particles have significant weights, while the majority have weights close to zero. Consequently, the particle set becomes a poor representation of the target distribution, and the variance of any estimators derived from it increases.\n\nThe Effective Sample Size, denoted $N_{\\text{eff}}$, is a heuristic measure used to quantify the severity of weight degeneracy. A low $N_{\\text{eff}}$ relative to the total number of particles, $N$, indicates significant degeneracy. The problem requests a derivation of the standard formula for $N_{\\text{eff}}$.\n\nA widely accepted approximation for the Effective Sample Size, given a set of normalized importance weights $\\{w_i\\}_{i=1}^N$ such that $\\sum_{i=1}^N w_i = 1$, is:\n$$\nN_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} w_i^2}\n$$\nThe justification for this formula arises from its behavior in limiting cases.\n1.  **Ideal Case (No Degeneracy)**: If all particles are equally important, their weights are uniform, i.e., $w_i = \\frac{1}{N}$ for all $i \\in \\{1, \\dots, N\\}$. The sum of squared weights is $\\sum_{i=1}^N w_i^2 = \\sum_{i=1}^N \\left(\\frac{1}{N}\\right)^2 = N \\cdot \\frac{1}{N^2} = \\frac{1}{N}$. Substituting this into the formula yields $N_{\\text{eff}} = \\frac{1}{1/N} = N$. In this optimal scenario, the effective sample size is equal to the total number of particles.\n2.  **Worst Case (Complete Degeneracy)**: If one particle, say particle $k$, has a weight of $w_k = 1$ and all other particles have weights $w_i = 0$ for $i \\ne k$, the particle set has completely collapsed. The sum of squared weights is $\\sum_{i=1}^N w_i^2 = 1^2 + \\sum_{i \\ne k} 0^2 = 1$. The formula gives $N_{\\text{eff}} = \\frac{1}{1} = 1$. This correctly reflects that the entire particle representation is dependent on a single particle.\n\nSince the formula for $N_{\\text{eff}}$ correctly interpolates between the best-case value of $N$ and the worst-case value of $1$, it serves as a robust and computationally simple metric for weight degeneracy.\n\nWe now proceed to the numerical computation for the given problem. We are given $N=8$ particles with normalized weights:\n$$\n\\big(w_{1},w_{2},\\ldots,w_{8}\\big) = \\big(0.35,\\;0.20,\\;0.15,\\;0.10,\\;0.08,\\;0.06,\\;0.04,\\;0.02\\big)\n$$\nFirst, we compute the sum of the squares of these weights:\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.35^2 + 0.20^2 + 0.15^2 + 0.10^2 + 0.08^2 + 0.06^2 + 0.04^2 + 0.02^2\n$$\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.1225 + 0.0400 + 0.0225 + 0.0100 + 0.0064 + 0.0036 + 0.0016 + 0.0004\n$$\n$$\n\\sum_{i=1}^{8} w_i^2 = 0.2070\n$$\nUsing this sum, we calculate the Effective Sample Size:\n$$\nN_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{8} w_i^2} = \\frac{1}{0.2070} \\approx 4.83091787...\n$$\nThe problem requires the result to be rounded to four significant figures.\n$$\nN_{\\text{eff}} \\approx 4.831\n$$\nFinally, we must determine if resampling should be triggered. The resampling criterion is given by the rule $\\text{trigger if } N_{\\text{eff}} \\le \\tau N$. We are given the threshold parameter $\\tau = 0.60$ and the number of particles $N = 8$. The resampling threshold is:\n$$\n\\tau N = 0.60 \\times 8 = 4.8\n$$\nWe must now check if the computed $N_{\\text{eff}}$ satisfies the condition:\n$$\n4.831 \\le 4.8\n$$\nThis inequality is false. Since the Effective Sample Size ($4.831$) is greater than the threshold value ($4.8$), the level of weight degeneracy is not considered critical. Therefore, resampling should not be triggered. The final answer required is only the numerical value of the Effective Sample Size.", "answer": "$$\n\\boxed{4.831}\n$$", "id": "2890403"}, {"introduction": "Having explored how to diagnose particle degeneracy, we now assemble the complete machinery of a particle filter. This practice [@problem_id:2890374] guides you through implementing a full iteration of the workhorse algorithm: the bootstrap filter, also known as the Sampling Importance Resampling (SIR) filter. You will put theory into practice by propagating particles through a highly nonlinear system, updating their weights based on a non-Gaussian measurement model, and applying the ESS-based resampling criterion you learned about previously.", "problem": "Consider the following discrete-time state-space model designed to stress-test Sequential Monte Carlo (SMC) methods for nonlinear and non-Gaussian systems. The latent state $x_t \\in \\mathbb{R}$ evolves according to a nonlinear dynamics\n$$\nx_t = g(x_{t-1}, t) + v_t,\n$$\nwhere\n$$\ng(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t),\n$$\nand the process noise $v_t$ is independently and identically distributed as a Laplace distribution with location $0$ and scale $b_v$, denoted $v_t \\sim \\mathrm{Laplace}(0, b_v)$, with probability density function\n$$\np_{V}(v) = \\frac{1}{2 b_v} \\exp\\!\\left(-\\frac{|v|}{b_v}\\right).\n$$\nThe scalar observation $y_t \\in \\mathbb{R}$ is given by the nonlinear measurement model\n$$\ny_t = \\arctan(x_t) + e_t,\n$$\nwhere the measurement noise $e_t$ is independently and identically distributed as a Laplace distribution with location $0$ and scale $b_e$, denoted $e_t \\sim \\mathrm{Laplace}(0, b_e)$, with probability density function\n$$\np_{E}(e) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|e|}{b_e}\\right).\n$$\nAll angles are in radians. The function $\\arctan(\\cdot)$ denotes the principal value inverse tangent.\n\nYou are asked to implement a single iteration (from time $t-1$ to time $t$) of the bootstrap particle filter (also called the Sampling Importance Resampling filter) as follows, starting from a prior particle approximation $\\{(x_{t-1}^{(i)}, w_{t-1}^{(i)})\\}_{i=1}^N$ of the filtering distribution at time $t-1$ with $N$ particles, where $w_{t-1}^{(i)} \\ge 0$ and $\\sum_{i=1}^N w_{t-1}^{(i)} = 1$:\n\n1. Propagation (proposal equals the transition prior): For each particle $i \\in \\{1,\\dots,N\\}$, sample\n$$\nx_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathrm{Laplace}\\!\\left(g\\!\\left(x_{t-1}^{(i)}, t\\right),\\, b_v\\right).\n$$\n\n2. Weight update using the measurement likelihood and prior weights:\n$$\n\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p\\!\\left(y_t \\,\\middle|\\, x_t^{(i)}\\right), \\quad\np\\!\\left(y_t \\,\\middle|\\, x_t\\right) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right).\n$$\nThen normalize to get $w_t^{(i)} = \\tilde{w}_t^{(i)} \\Big/ \\sum_{j=1}^N \\tilde{w}_t^{(j)}$.\n\n3. Compute the Effective Sample Size (ESS) defined by\n$$\n\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N \\left(w_t^{(i)}\\right)^2}.\n$$\n\n4. Resampling decision and execution: Given a threshold ratio $\\tau \\in [0, 1]$, perform systematic resampling if $\\mathrm{ESS}_t \\le \\tau N$, producing a resampled particle set $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ with equal weights $\\bar{w}_t^{(i)} = 1/N$. If resampling is not triggered, retain $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$.\n\nFor this single iteration, define the posterior estimate after the resampling decision as follows:\n- If resampling occurred, use the resampled equally weighted particles $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ to compute the posterior mean and variance as the standard unweighted sample mean and variance.\n- If no resampling occurred, use the weighted set $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ to compute the posterior mean and variance as the weighted mean and weighted variance.\n\nThe program you implement must:\n- Initialize the prior particles by sampling $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ independently for $i \\in \\{1,\\dots,N\\}$, and use uniform prior weights $w_{t-1}^{(i)} = 1/N$.\n- Use a fixed pseudorandom seed for each test case to ensure deterministic behavior.\n- Use numerically stable computations for weights (for example, via log-weights stabilization).\n- Implement systematic resampling.\n\nYour program must process the following test suite, where each tuple specifies $(N, \\mathrm{seed}, t, y_t, m_0, s_0, b_v, b_e, \\tau)$:\n- Test $1$: $(200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5)$.\n- Test $2$: $(200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9)$.\n- Test $3$: $(150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9)$.\n- Test $4$: $(100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0)$.\n\nFor each test, after one complete bootstrap filtering iteration (including the resampling decision), report:\n- The posterior mean of $x_t$ rounded to $6$ decimals,\n- The posterior variance of $x_t$ rounded to $6$ decimals,\n- The Effective Sample Size $\\mathrm{ESS}_t$ rounded to $6$ decimals (computed before any resampling),\n- The resampling indicator, where resampling is indicated by the integer $1$ and no resampling by the integer $0$.\n\nFinal output format: Your program should produce a single line of output containing a JSON-like list of results, one per test case, with each result itself being a list of the four values described above in this exact order. For example, the output should look like\n$$\n\\big[\\,[m_1, v_1, \\mathrm{ESS}_1, r_1],\\,[m_2, v_2, \\mathrm{ESS}_2, r_2],\\,\\dots\\,\\big],\n$$\nwhere each $m_k$, $v_k$, and $\\mathrm{ESS}_k$ are decimal numbers rounded to $6$ decimals and each $r_k$ is either $0$ or $1$. There are no physical units required beyond specifying that angles are in radians. The program must not read any input and must be self-contained.", "solution": "The problem statement is analyzed and found to be valid. It constitutes a well-posed problem in the domain of nonlinear state estimation, providing a complete and consistent specification for the implementation of a single iteration of a bootstrap particle filter. All necessary models, parameters, and algorithms are defined unambiguously. We proceed with the solution.\n\nThe task is to implement one iteration of a sequential Monte Carlo method, specifically the bootstrap particle filter, for a given nonlinear, non-Gaussian state-space model. The process starts at time $t-1$ and ends after the resampling decision at time $t$.\n\n**1. System Model Specification**\nThe state-space model is defined by two equations:\nThe state transition equation:\n$$x_t = g(x_{t-1}, t) + v_t$$\nwhere the nonlinear function is $g(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t)$, and the process noise is $v_t \\sim \\mathrm{Laplace}(0, b_v)$.\n\nThe measurement equation:\n$$y_t = \\arctan(x_t) + e_t$$\nwhere the measurement noise is $e_t \\sim \\mathrm{Laplace}(0, b_e)$.\n\n**2. Bootstrap Particle Filter Iteration**\nThe algorithm proceeds through several steps, starting from a particle set $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^N$ approximating the posterior distribution $p(x_{t-1} | y_{1:t-1})$.\n\n**Step 0: Initialization at time $t-1$**\nAs specified, the prior particle set is initialized for this single iteration. The states $x_{t-1}^{(i)}$ are drawn independently from a Gaussian distribution, $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ for $i=1, \\dots, N$. The initial weights are uniform, $w_{t-1}^{(i)} = 1/N$ for all $i$.\n\n**Step 1: Propagation (Prediction)**\nEach particle is propagated forward in time according to the state transition model. For the bootstrap filter, the proposal distribution is the transition prior itself. This means, for each particle $i$, we sample a new state $x_t^{(i)}$ from the distribution $p(x_t | x_{t-1}^{(i)})$. This is accomplished by:\n1.  Calculating the deterministic component of the state evolution: $\\mu_t^{(i)} = g(x_{t-1}^{(i)}, t)$.\n2.  Sampling a noise term $v_t^{(i)}$ from the process noise distribution: $v_t^{(i)} \\sim \\mathrm{Laplace}(0, b_v)$.\n3.  Combining these to obtain the new particle state: $x_t^{(i)} = \\mu_t^{(i)} + v_t^{(i)}$.\n\n**Step 2: Weight Update (Correction)**\nUpon receiving the measurement $y_t$, the importance weights of the propagated particles are updated to reflect how well each particle explains the observation. The updated, unnormalized weight $\\tilde{w}_t^{(i)}$ is the product of the prior weight and the likelihood of the measurement given the particle state:\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p(y_t | x_t^{(i)})$$\nThe likelihood function $p(y_t | x_t)$ is derived from the measurement model and the distribution of the measurement noise $e_t$:\n$$p(y_t | x_t) = p_{E}(y_t - \\arctan(x_t)) = \\frac{1}{2 b_e} \\exp\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right)$$\nSince the prior weights $w_{t-1}^{(i)}$ are uniform ($1/N$), they are constant across all particles and can be ignored during normalization. The unnormalized weights are thus proportional to the likelihoods: $\\tilde{w}_t^{(i)} \\propto p(y_t | x_t^{(i)})$.\n\nFor numerical stability, computations are performed in the log domain. The log-likelihood for particle $i$ is:\n$$\\log p(y_t | x_t^{(i)}) = -\\log(2 b_e) - \\frac{|y_t - \\arctan(x_t^{(i)})|}{b_e}$$\nLet $l_i = \\log p(y_t | x_t^{(i)})$. We find the maximum log-likelihood, $l_{\\max} = \\max_i \\{l_i\\}$. The normalized weights $w_t^{(i)}$ are then computed using the log-sum-exp trick to prevent numerical underflow:\n$$w_t^{(i)} = \\frac{\\exp(l_i - l_{\\max})}{\\sum_{j=1}^N \\exp(l_j - l_{\\max})}$$\nThe set $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ now represents an approximation of the filtering distribution $p(x_t | y_{1:t})$.\n\n**Step 3: Effective Sample Size (ESS) Calculation**\nThe degeneracy of the particle set is quantified by the Effective Sample Size, $\\mathrm{ESS}_t$. A low ESS indicates that a few particles have very high weights, and the approximation is poor. It is calculated as:\n$$\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N (w_t^{(i)})^2}$$\nThe value of $\\mathrm{ESS}_t$ ranges from $1$ (complete degeneracy) to $N$ (uniform weights).\n\n**Step 4: Resampling Decision and Execution**\nResampling is a mechanism to mitigate particle degeneracy by replicating particles with high weights and discarding those with low weights. A decision to resample is made by comparing the ESS to a threshold, $\\tau N$, where $\\tau \\in [0, 1]$ is a user-defined ratio.\n- If $\\mathrm{ESS}_t \\le \\tau N$, resampling is performed. The problem specifies systematic resampling. This algorithm draws $N$ particles from the current discrete distribution $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ to form a new set $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ where each new particle has an equal weight $\\bar{w}_t^{(i)}=1/N$.\n- If $\\mathrm{ESS}_t > \\tau N$, no action is taken. The particle set remains $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$.\n\nA resampling indicator $r_t$ is set to $1$ if resampling occurred, and $0$ otherwise.\n\n**Step 5: Posterior Statistics Calculation**\nThe final step is to compute the mean and variance of the posterior distribution of $x_t$ from the resulting particle set. The formulae depend on whether resampling was performed.\n- **If resampling occurred ($r_t=1$)**: The posterior is approximated by the equally weighted particles $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$. The mean and variance are the standard unweighted sample mean and variance:\n  $$\\hat{x}_t = \\frac{1}{N} \\sum_{i=1}^N \\bar{x}_t^{(i)}$$\n  $$\\hat{V}_t = \\frac{1}{N} \\sum_{i=1}^N (\\bar{x}_t^{(i)} - \\hat{x}_t)^2$$\n- **If no resampling occurred ($r_t=0$)**: The posterior is approximated by the weighted particles $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$. The mean and variance are the weighted sample mean and variance:\n  $$\\hat{x}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n  $$\\hat{V}_t = \\sum_{i=1}^N w_t^{(i)} (x_t^{(i)} - \\hat{x}_t)^2$$\n\nThese computed values—posterior mean, posterior variance, ESS, and resampling indicator—constitute the required output for one iteration of the filter.", "answer": "```python\nimport numpy as np\n\ndef g(x, t):\n    \"\"\"The nonlinear state transition function.\"\"\"\n    return 0.5 * x + 25.0 * x / (1.0 + x**2) + 8.0 * np.cos(1.2 * t)\n\ndef particle_filter_step(N, seed, t, yt, m0, s0, bv, be, tau):\n    \"\"\"\n    Performs a single iteration of a bootstrap particle filter.\n    Returns posterior mean, variance, ESS, and resampling indicator.\n    \"\"\"\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Step 0: Initialize prior particles at time t-1.\n    # The weights w_{t-1} are uniform (1/N) for all particles.\n    xt_prev = rng.normal(loc=m0, scale=s0, size=N)\n\n    # Step 1: Propagation (Prediction) to time t.\n    # The proposal is the transition prior p(x_t|x_{t-1}).\n    mu_t = g(xt_prev, t)\n    # Sample from Laplace(mu_t, bv) which is mu_t + Laplace(0, bv)\n    xt = mu_t + rng.laplace(loc=0.0, scale=bv, size=N)\n\n    # Step 2: Weight update using the measurement y_t.\n    # We work with log-weights for numerical stability.\n    # The log-likelihood is log p(y_t|x_t^(i)).\n    log_likelihood = -np.log(2.0 * be) - np.abs(yt - np.arctan(xt)) / be\n\n    # Since w_{t-1} are uniform, log(w_{t-1}) is a constant offset\n    # that cancels during normalization. So, log_tilde_wt is proportional\n    # to the log_likelihood.\n    # We use the log-sum-exp trick for normalization.\n    log_wt_max = np.max(log_likelihood)\n    wt_unnorm = np.exp(log_likelihood - log_wt_max)\n    wt = wt_unnorm / np.sum(wt_unnorm)\n\n    # Step 3: Compute Effective Sample Size (ESS).\n    # This is calculated before any resampling.\n    ess = 1.0 / np.sum(wt**2)\n\n    # Step 4: Resampling decision and execution.\n    resampling_occurred = 0\n    # The threshold condition ESS <= tau * N might be sensitive to floating-point\n    # issues, but for typical values, direct comparison is acceptable.\n    # Note: ESS is always >= 1. If tau*N < 1, resampling will not trigger.\n    if ess <= tau * N:\n        resampling_occurred = 1\n        \n        # Perform systematic resampling.\n        csw = np.cumsum(wt)\n        csw[-1] = 1.0  # Ensure the sum is exactly 1\n        u0 = rng.random()\n        positions = (np.arange(N) + u0) / N\n        \n        indices = np.searchsorted(csw, positions)\n        \n        # The new particles are copies of the old ones based on indices.\n        xt = xt[indices]\n        # After resampling, all weights are reset to uniform 1/N.\n        wt = np.full(N, 1.0 / N)\n\n    # Step 5: Compute posterior mean and variance.\n    if resampling_occurred == 1:\n        # Use unweighted sample statistics for the resampled set.\n        post_mean = np.mean(xt)\n        post_var = np.var(xt) # np.var uses 1/N denominator\n    else:\n        # Use weighted sample statistics.\n        post_mean = np.sum(wt * xt)\n        post_var = np.sum(wt * (xt - post_mean)**2)\n\n    return [\n        round(post_mean, 6),\n        round(post_var, 6),\n        round(ess, 6),\n        resampling_occurred\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (N, seed, t, y_t, m_0, s_0, b_v, b_e, tau)\n        (200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5),\n        (200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9),\n        (150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9),\n        (100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = particle_filter_step(*case)\n        results.append(result)\n\n    # Format the final output string as a JSON-like list of lists.\n    # f'{val:.6f}' is used to ensure 6 decimal places and avoid scientific notation.\n    inner_strings = []\n    for res_list in results:\n        m, v, e, r = res_list\n        inner_strings.append(f\"[{m:.6f},{v:.6f},{e:.6f},{r}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2890374"}, {"introduction": "The bootstrap filter you implemented in the previous exercise is powerful due to its simplicity, but its performance can degrade when observations are highly informative. This happens because its proposal distribution—the state prior—ignores the latest measurement, potentially proposing particles in low-likelihood regions. This final practice [@problem_id:2890437] delves into this crucial issue by comparing the bootstrap proposal to the 'optimal' proposal, which minimizes the variance of the particle weights. Through a direct empirical comparison, you will quantify the significant performance gain achieved by a more intelligent proposal choice, highlighting a key avenue for designing advanced particle filters.", "problem": "Consider a one-dimensional, discrete-time state-space model used in Sequential Monte Carlo (SMC) for nonlinear systems. Let the latent state at time $t$ be $x_t \\in \\mathbb{R}$. The system obeys the following dynamics and observation model:\n- State transition: $x_t \\mid x_{t-1} \\sim \\mathcal{N}(f(x_{t-1}), Q)$ with $f(x) = 0.5\\,x + 0.05\\,x^3$.\n- Observation: $y_t \\mid x_t \\sim \\mathcal{N}(c\\,x_t, R)$, with known scalar $c$.\n\nYou will compare two importance proposals for a single filtering step at a fixed time $t$:\n- The bootstrap proposal $q(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1})$.\n- The optimal proposal $q^\\star(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1}, y_t)$.\n\nYour task is to empirically estimate and compare the variance of the unnormalized incremental log-weights produced by these two proposals, under the same set of previous particles $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ and the same fixed observation $y_t$, for a set of test cases.\n\nFundamental base you must use:\n- Bayes' rule: $p(x_t \\mid x_{t-1}, y_t) \\propto p(y_t \\mid x_t)\\,p(x_t \\mid x_{t-1})$.\n- Importance sampling identity: for any proposal $q(x_t \\mid x_{t-1}, y_t)$, the incremental weight satisfies $w_t^{(i)} \\propto \\frac{p(y_t \\mid x_t^{(i)})\\,p(x_t^{(i)} \\mid x_{t-1}^{(i)})}{q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)}$, with unnormalized incremental log-weight $\\ell_t^{(i)} = \\log w_t^{(i)}$ defined up to an additive constant independent of the particle index $i$.\n- Gaussian algebra for linear-Gaussian models.\n\nWhat to compute for each test case:\n1. Generate the previous particle set $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ according to the specified rule (deterministic or Gaussian) with the given random seed to ensure reproducibility.\n2. Using the bootstrap proposal:\n   - For each particle $i$, sample $x_t^{(i)} \\sim \\mathcal{N}(f(x_{t-1}^{(i)}), Q)$.\n   - Compute the unnormalized incremental log-weight using the Gaussian observation likelihood, omitting any additive constant that is the same for all particles:\n     $$\\ell_{\\text{boot}}^{(i)} = -\\tfrac{1}{2}\\,\\frac{(y_t - c\\,x_t^{(i)})^2}{R}.$$\n   - Compute the unbiased sample variance of $\\{\\ell_{\\text{boot}}^{(i)}\\}_{i=1}^N$ with denominator $N-1$.\n3. Using the optimal proposal $q^\\star(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1}, y_t)$:\n   - Derive the closed-form of $p(x_t \\mid x_{t-1}, y_t)$ for this model and sample $x_t^{(i)}$ accordingly for each $i$.\n   - Compute the unnormalized incremental log-weight for each particle using the predictive density $p(y_t \\mid x_{t-1}^{(i)})$ obtained by marginalizing $x_t$ out of $p(y_t \\mid x_t)\\,p(x_t \\mid x_{t-1}^{(i)})$, omitting additive constants that are the same for all particles:\n     $$\\ell_{\\star}^{(i)} = -\\tfrac{1}{2}\\,\\frac{(y_t - c\\,f(x_{t-1}^{(i)}))^2}{c^2 Q + R}.$$\n   - Compute the unbiased sample variance of $\\{\\ell_{\\star}^{(i)}\\}_{i=1}^N$ with denominator $N-1$.\n4. Report, for each test case, a triple of floating-point numbers $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$, where each value is rounded to $6$ decimal places.\n\nTest suite:\n- Use the nonlinear transition $f(x) = 0.5\\,x + 0.05\\,x^3$ for all cases. Angles are not involved. No physical units are involved.\n- For all random draws, use the provided seed per test case to ensure deterministic outputs.\n\nCases:\n- Case $1$ (degenerate previous state; demonstrates zero variance under $q^\\star$):\n  - $N = 20000$, $Q = 0.25$, $R = 0.09$, $c = 1.0$, $y_t = 0.2$, previous particles: all $x_{t-1}^{(i)} = 0.5$, seed $= 42$.\n- Case $2$ (moderate noise, spread in previous particles):\n  - $N = 20000$, $Q = 1.0$, $R = 0.25$, $c = 1.2$, $y_t = -0.3$, previous particles: $x_{t-1}^{(i)} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0.0, 1.0)$, seed $= 7$.\n- Case $3$ (very informative observation, large process noise; stresses bootstrap):\n  - $N = 20000$, $Q = 2.0$, $R = 0.0025$, $c = 1.0$, $y_t = 1.0$, previous particles: $x_{t-1}^{(i)} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0.0, 2.0)$, seed $= 13$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of triples enclosed in square brackets (for example, $[[v_{b1}, v_{\\star 1}, \\Delta_1],[v_{b2}, v_{\\star 2}, \\Delta_2],[v_{b3}, v_{\\star 3}, \\Delta_3]]$)), where each numeric entry is rounded to $6$ decimal places.\n- Each triple corresponds to $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$ for Cases $1$ through $3$ in order.", "solution": "The problem presented is a well-defined exercise in the field of statistical signal processing, specifically concerning Sequential Monte Carlo methods, also known as particle filters. It requires a comparison of the variance of unnormalized incremental importance weights for two different proposal distributions: the standard bootstrap proposal and the optimal proposal. The model is a discrete-time, one-dimensional state-space system with a nonlinear state transition and a linear-Gaussian observation model. The problem is scientifically sound, mathematically consistent, and all parameters are specified. It is therefore valid for analysis.\n\nWe begin by establishing the mathematical framework. The system is defined by:\n- State transition model: $p(x_t|x_{t-1}) = \\mathcal{N}(x_t; f(x_{t-1}), Q)$, where the transition function is $f(x) = 0.5x + 0.05x^3$.\n- Observation model: $p(y_t|x_t) = \\mathcal{N}(y_t; c x_t, R)$.\n\nThe objective is to compute the sample variance of the unnormalized incremental log-weights, $\\ell_t^{(i)}$, for a set of particles $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ at a single time step $t$. The general form of the incremental weight $w_t^{(i)}$ for a proposal distribution $q(x_t|x_{t-1}^{(i)}, y_t)$ is given by the importance sampling identity:\n$$w_t^{(i)} \\propto \\frac{p(y_t|x_t^{(i)}) p(x_t^{(i)}|x_{t-1}^{(i)})}{q(x_t^{(i)}|x_{t-1}^{(i)}, y_t)}$$\nwhere $x_t^{(i)} \\sim q(x_t|x_{t-1}^{(i)}, y_t)$. The unnormalized log-weight is $\\ell_t^{(i)} = \\log w_t^{(i)}$.\n\n**1. Bootstrap Proposal**\n\nThe bootstrap proposal is the simplest choice, using the state transition prior as the proposal distribution:\n$$q_{\\text{boot}}(x_t | x_{t-1}, y_t) = p(x_t|x_{t-1}) = \\mathcal{N}(x_t; f(x_{t-1}), Q)$$\nSubstituting this into the weight equation, the prior term $p(x_t^{(i)}|x_{t-1}^{(i)})$ cancels with the proposal density $q_{\\text{boot}}$, leaving the weight proportional to the likelihood:\n$$w_{\\text{boot}}^{(i)} \\propto p(y_t|x_t^{(i)})$$\nParticles are first sampled from the prior, $x_t^{(i)} \\sim \\mathcal{N}(x_t; f(x_{t-1}^{(i)}), Q)$, and then weighted by how well they explain the observation $y_t$. The unnormalized incremental log-weight is the log-likelihood of the observation, omitting additive constants independent of the particle index $i$:\n$$\\ell_{\\text{boot}}^{(i)} = -\\frac{1}{2R}(y_t - c x_t^{(i)})^2$$\nThe variance of these log-weights, $\\mathrm{var}(\\{\\ell_{\\text{boot}}^{(i)}\\}_{i=1}^N)$, arises from the random sampling of $x_t^{(i)}$ and any pre-existing variation in the parent particles $\\{x_{t-1}^{(i)}\\}$. This variance can be large if the likelihood $p(y_t|x_t)$ is concentrated in a region where the prior $p(x_t|x_{t-1})$ has low density, a common inefficiency of the bootstrap filter.\n\n**2. Optimal Proposal**\n\nThe optimal proposal, which minimizes the variance of the importance weights, is the true posterior distribution of the state:\n$$q^{\\star}(x_t | x_{t-1}, y_t) = p(x_t|x_{t-1}, y_t)$$\nThe weight for this choice is derived from the normalization constant of the posterior:\n$$p(x_t|x_{t-1}, y_t) = \\frac{p(y_t|x_t) p(x_t|x_{t-1})}{\\int p(y_t|x_t) p(x_t|x_{t-1}) dx_t} = \\frac{p(y_t|x_t) p(x_t|x_{t-1})}{p(y_t|x_{t-1})}$$\nSubstituting $q^{\\star}$ into the weight equation yields:\n$$w_{\\star}^{(i)} \\propto \\frac{p(y_t|x_t^{(i)}) p(x_t^{(i)}|x_{t-1}^{(i)})}{p(x_t^{(i)}|x_{t-1}^{(i)}, y_t)} \\propto p(y_t|x_{t-1}^{(i)})$$\nThe weight is the marginal likelihood (or evidence) of the observation given the previous state. For our specific model, for a given $x_{t-1}$, the quantities $x_t$ and $y_t$ are jointly Gaussian. The conditional distribution of $y_t$ given $x_{t-1}$ is found by marginalizing out $x_t$. The state $x_t$ has mean $f(x_{t-1})$ and variance $Q$. The observation $y_t$ is a linear transformation of $x_t$ plus noise. Thus, the predictive distribution for $y_t$ is also Gaussian:\n$$p(y_t | x_{t-1}) = \\mathcal{N}(y_t; c f(x_{t-1}), c^2 Q + R)$$\nThe unnormalized incremental log-weight for the optimal proposal is therefore:\n$$\\ell_{\\star}^{(i)} = -\\frac{1}{2(c^2 Q + R)}(y_t - c f(x_{t-1}^{(i)}))^2$$\nCrucially, this log-weight depends only on the previous state $x_{t-1}^{(i)}$, not on the newly sampled state $x_t^{(i)}$. Any variance in these weights, $\\mathrm{var}(\\{\\ell_{\\star}^{(i)}\\}_{i=1}^N)$, is due solely to the variation in the parent particles $\\{x_{t-1}^{(i)}\\}$. If the parent particles are all identical, as in Case $1$, the variance of the optimal weights is exactly zero. This demonstrates the theoretical superiority of the optimal proposal.\n\nThe computational procedure for each test case is as follows:\n1.  Generate the set of $N$ previous-state particles $\\{x_{t-1}^{(i)}\\}$ according to the case specification.\n2.  For the bootstrap proposal, sample new particles $\\{x_t^{(i)}\\}$ and compute the variance of $\\{\\ell_{\\text{boot}}^{(i)}\\}$.\n3.  For the optimal proposal, compute the variance of $\\{\\ell_{\\star}^{(i)}\\}$ directly from $\\{x_{t-1}^{(i)}\\}$.\n4.  The final result for each case is the triple $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the variance of unnormalized incremental log-weights\n    for the bootstrap and optimal proposals in a particle filter step.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": 20000, \"Q\": 0.25, \"R\": 0.09, \"c\": 1.0, \"y_t\": 0.2,\n            \"x_tm1_spec\": (\"const\", 0.5), \"seed\": 42\n        },\n        {\n            \"N\": 20000, \"Q\": 1.0, \"R\": 0.25, \"c\": 1.2, \"y_t\": -0.3,\n            \"x_tm1_spec\": (\"normal\", 0.0, 1.0), \"seed\": 7\n        },\n        {\n            \"N\": 20000, \"Q\": 2.0, \"R\": 0.0025, \"c\": 1.0, \"y_t\": 1.0,\n            \"x_tm1_spec\": (\"normal\", 0.0, 2.0), \"seed\": 13\n        }\n    ]\n\n    # State transition function\n    def f(x):\n        return 0.5 * x + 0.05 * x**3\n\n    results_data = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        Q = case[\"Q\"]\n        R = case[\"R\"]\n        c = case[\"c\"]\n        y_t = case[\"y_t\"]\n        x_tm1_spec = case[\"x_tm1_spec\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate the previous particle set {x_{t-1}^{(i)}}\n        if x_tm1_spec[0] == \"const\":\n            x_tm1 = np.full(N, x_tm1_spec[1], dtype=np.float64)\n        elif x_tm1_spec[0] == \"normal\":\n            mean, var = x_tm1_spec[1], x_tm1_spec[2]\n            x_tm1 = rng.normal(loc=mean, scale=np.sqrt(var), size=N)\n        \n        # Calculate the mean of the state transition for all particles\n        mu_t = f(x_tm1)\n\n        # Step 2: Bootstrap Proposal\n        # Sample x_t from the prior p(x_t | x_{t-1})\n        x_t_boot = rng.normal(loc=mu_t, scale=np.sqrt(Q))\n        \n        # Compute unnormalized incremental log-weights\n        log_w_boot = -0.5 * ((y_t - c * x_t_boot)**2) / R\n        \n        # Compute unbiased sample variance\n        var_boot = np.var(log_w_boot, ddof=1)\n\n        # Step 3: Optimal Proposal\n        # The log-weight is a deterministic function of x_{t-1}\n        var_pred = c*c * Q + R\n        log_w_opt = -0.5 * ((y_t - c * mu_t)**2) / var_pred\n        \n        # Compute unbiased sample variance\n        var_opt = np.var(log_w_opt, ddof=1)\n        \n        # Step 4: Report results\n        diff = var_boot - var_opt\n        results_data.append([var_boot, var_opt, diff])\n\n    # Format the final output string exactly as required\n    formatted_triples = []\n    for triple in results_data:\n        # Format each triple as [val1,val2,val3] with numbers to 6 decimal places\n        s_triple = f\"[{triple[0]:.6f},{triple[1]:.6f},{triple[2]:.6f}]\"\n        formatted_triples.append(s_triple)\n    \n    final_output = f\"[{','.join(formatted_triples)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2890437"}]}