{"hands_on_practices": [{"introduction": "Before attempting to estimate a model's parameters, we must be certain that a unique set of parameters corresponds to a unique system behavior. This fundamental property, known as structural identifiability, ensures that our estimation problem is well-posed. This practice explores this concept by revealing a common scaling ambiguity in rational transfer function models and demonstrating how a simple normalization constraint can restore identifiability [@problem_id:2878968].", "problem": "Consider a Single-Input Single-Output (SISO) Output-Error (OE) model, defined by the input-output relation $y(t) = G(q^{-1}) u(t) + e(t)$ with $G(q^{-1}) = \\dfrac{B(q^{-1})}{F(q^{-1})}$, where $B(q^{-1})$ and $F(q^{-1})$ are real-coefficient polynomials in the backward shift operator $q^{-1}$, and $e(t)$ is an additive disturbance. A parameterization is said to be structurally identifiable if distinct parameter vectors induce distinct input-output mappings $G(q^{-1})$ for all admissible inputs $u(t)$. Assume $e(t) = 0$ to focus on the deterministic mapping and that $u(t)$ is arbitrary but well-defined.\n\nUsing only these definitions, show that without additional constraints, the OE parameterization can be non-identifiable due to a scaling ambiguity between $B(q^{-1})$ and $F(q^{-1})$. Concretely:\n\n1. Construct a counterexample by taking $B(q^{-1}) = 2 + 0.5\\, q^{-1}$ and $F(q^{-1}) = 4 - q^{-1}$. Demonstrate that the scaled pair $\\tilde{B}(q^{-1}) = \\alpha B(q^{-1})$ and $\\tilde{F}(q^{-1}) = \\alpha F(q^{-1})$ with $\\alpha = 3$ induces the same input-output mapping $G(q^{-1})$ for all $u(t)$, thereby showing non-identifiability.\n\n2. Propose a normalization constraint that eliminates this scaling ambiguity and restores identifiability. Your constraint must be stated as a condition on a coefficient of $F(q^{-1})$ that is well-defined whenever that coefficient is nonzero.\n\n3. Apply your proposed normalization to the scaled pair $\\tilde{B}(q^{-1})$ and $\\tilde{F}(q^{-1})$ from part $1$ and compute the unique normalization factor $\\alpha_{\\star}$ that must be applied to $(\\tilde{B}(q^{-1}), \\tilde{F}(q^{-1}))$ so that the normalized $F(q^{-1})$ has a unit coefficient on $q^{0}$. Report $\\alpha_{\\star}$ as a single real number. No rounding is required.", "solution": "The problem statement is evaluated and deemed valid. It is scientifically grounded in the principles of system identification, well-posed, and objective. It contains no logical contradictions, missing information, or pseudoscientific claims. We may proceed with the solution.\n\nThe problem investigates the structural identifiability of a Single-Input Single-Output (SISO) Output-Error (OE) model. The deterministic part of the model is given by the relation $y(t) = G(q^{-1}) u(t)$, where the transfer function is a rational function of the backward shift operator $q^{-1}$:\n$$\nG(q^{-1}) = \\frac{B(q^{-1})}{F(q^{-1})}\n$$\nThe parameters of the model are the coefficients of the polynomials $B(q^{-1})$ and $F(q^{-1})$. Structural identifiability requires that distinct vectors of these parameters must result in a distinct transfer function $G(q^{-1})$. We will show that this is not true without additional constraints.\n\nPart 1: Demonstrating non-identifiability with a counterexample.\nLet the first parameterization be defined by the polynomials:\n$$\nB(q^{-1}) = 2 + 0.5\\, q^{-1}\n$$\n$$\nF(q^{-1}) = 4 - q^{-1}\n$$\nThe corresponding transfer function, which we denote $G_1(q^{-1})$, is:\n$$\nG_1(q^{-1}) = \\frac{B(q^{-1})}{F(q^{-1})} = \\frac{2 + 0.5\\, q^{-1}}{4 - q^{-1}}\n$$\nNow, consider a second parameterization derived by scaling both polynomials by a non-zero constant $\\alpha$, given as $\\alpha = 3$. The new polynomials, $\\tilde{B}(q^{-1})$ and $\\tilde{F}(q^{-1})$, are:\n$$\n\\tilde{B}(q^{-1}) = \\alpha B(q^{-1}) = 3(2 + 0.5\\, q^{-1}) = 6 + 1.5\\, q^{-1}\n$$\n$$\n\\tilde{F}(q^{-1}) = \\alpha F(q^{-1}) = 3(4 - q^{-1}) = 12 - 3\\, q^{-1}\n$$\nThe parameter vector for this second model, containing the coefficients $\\{6, 1.5, 12, -3\\}$, is clearly distinct from the parameter vector for the first model, which contains $\\{2, 0.5, 4, -1\\}$.\n\nThe transfer function for this second parameterization, denoted $G_2(q^{-1})$, is:\n$$\nG_2(q^{-1}) = \\frac{\\tilde{B}(q^{-1})}{\\tilde{F}(q^{-1})} = \\frac{6 + 1.5\\, q^{-1}}{12 - 3\\, q^{-1}}\n$$\nWe can factor out the scaling constant $\\alpha = 3$ from the numerator and denominator:\n$$\nG_2(q^{-1}) = \\frac{3(2 + 0.5\\, q^{-1})}{3(4 - q^{-1})}\n$$\nSince $\\alpha=3$ is a non-zero constant, it cancels out, yielding:\n$$\nG_2(q^{-1}) = \\frac{2 + 0.5\\, q^{-1}}{4 - q^{-1}} = G_1(q^{-1})\n$$\nWe have shown that two distinct parameter vectors produce an identical transfer function, $G_1(q^{-1}) = G_2(q^{-1})$. Therefore, the input-output mapping $y(t) = G(q^{-1})u(t)$ is the same for both parameterizations. This violates the condition for structural identifiability. The observed ambiguity is a direct result of the freedom to scale both the numerator and denominator polynomials by an arbitrary non-zero constant without changing the rational function they define.\n\nPart 2: Proposing a normalization constraint.\nTo eliminate the scaling ambiguity demonstrated in Part 1, we must impose a constraint that fixes the value of the arbitrary scaling factor $\\alpha$. A common and effective method is to enforce a specific value for one of the polynomial coefficients. Let the denominator polynomial be written in its general form:\n$$\nF(q^{-1}) = f_0 + f_1 q^{-1} + f_2 q^{-2} + \\dots + f_{n_f} q^{-n_f}\n$$\nThe scaling ambiguity means that the model defined by $(\\alpha B(q^{-1}), \\alpha F(q^{-1}))$ is equivalent to $(B(q^{-1}), F(q^{-1}))$ for any $\\alpha \\neq 0$. To obtain a unique representation, we can select one coefficient and fix its value. The standard choice for discrete-time systems is to normalize the denominator polynomial $F(q^{-1})$ to be monic with respect to the $q^0$ term. This corresponds to the following constraint on the coefficients of $F(q^{-1})$:\n$$\nf_0 = 1\n$$\nThis constraint is well-defined as long as the original leading coefficient $f_0$ is not zero. If $f_0$ were zero, one could divide both $B(q^{-1})$ and $F(q^{-1})$ by a power of $q^{-1}$ until the leading coefficient of the new denominator is non-zero, or re-evaluate the model structure. For a stable, causal system, it is standard to assume $f_0 \\neq 0$. This normalization forces a unique scaling for the pair $(B(q^{-1}), F(q^{-1}))$, thereby restoring structural identifiability with respect to this scaling ambiguity.\n\nPart 3: Applying the normalization and computing the factor $\\alpha_{\\star}$.\nWe are given the scaled polynomials from Part 1:\n$$\n\\tilde{B}(q^{-1}) = 6 + 1.5\\, q^{-1}\n$$\n$$\n\\tilde{F}(q^{-1}) = 12 - 3\\, q^{-1}\n$$\nWe must find a unique normalization factor $\\alpha_{\\star}$ that, when applied to this pair, results in a new denominator polynomial, $F_{\\text{norm}}(q^{-1})$, whose coefficient for the $q^0$ term is equal to $1$.\nLet the normalized polynomials be defined as:\n$$\nB_{\\text{norm}}(q^{-1}) = \\alpha_{\\star} \\tilde{B}(q^{-1})\n$$\n$$\nF_{\\text{norm}}(q^{-1}) = \\alpha_{\\star} \\tilde{F}(q^{-1})\n$$\nThe coefficient of the $q^0$ term in $\\tilde{F}(q^{-1})$ is $\\tilde{f}_0 = 12$. Applying the scaling, the new denominator is:\n$$\nF_{\\text{norm}}(q^{-1}) = \\alpha_{\\star} (12 - 3\\, q^{-1}) = (12 \\alpha_{\\star}) + (-3 \\alpha_{\\star}) q^{-1}\n$$\nAccording to the normalization constraint proposed in Part 2, the coefficient of the $q^0$ term in $F_{\\text{norm}}(q^{-1})$ must be $1$. Thus, we set up the equation:\n$$\n12 \\alpha_{\\star} = 1\n$$\nSolving for $\\alpha_{\\star}$ gives the unique normalization factor:\n$$\n\\alpha_{\\star} = \\frac{1}{12}\n$$\nThis is the single real number that must be applied to the pair $(\\tilde{B}(q^{-1}), \\tilde{F}(q^{-1}))$ to satisfy the specified normalization condition.", "answer": "$$\n\\boxed{\\frac{1}{12}}\n$$", "id": "2878968"}, {"introduction": "Once we have an identifiable model structure, the next task is parameter estimation. However, when the measurement noise is correlated over time—a common occurrence in real systems—standard methods like Ordinary Least Squares (OLS) yield biased and inconsistent estimates. This exercise provides concrete, hands-on practice with the Instrumental Variables (IV) method, a powerful technique for obtaining correct estimates by introducing auxiliary signals (instruments) that are correlated with the system's dynamics but uncorrelated with the disturbance [@problem_id:2878920].", "problem": "Consider a single-input single-output discrete-time system excited by a known scalar input signal $u(t)$ and producing a measured scalar output $y(t)$. You adopt the following predictor form with an autoregressive with exogenous input (ARX) structure and a colored disturbance:\n$$\ny(t) \\;=\\; a\\,y(t-1)\\;+\\;b\\,u(t-1)\\;+\\;v(t),\\qquad v(t)\\;=\\;e(t)\\;+\\;c\\,e(t-1),\n$$\nwhere $\\{e(t)\\}$ is an independently and identically distributed (i.i.d.) zero-mean sequence with finite variance, and $\\{e(t)\\}$ is statistically independent of the input $\\{u(t)\\}$. Because the disturbance $v(t)$ is colored, ordinary least squares is inconsistent for estimating $\\theta \\triangleq \\begin{pmatrix} a  b \\end{pmatrix}^{\\top}$. To address this, you will use Instrumental Variables (IV), where a valid instrument matrix $Z$ must satisfy two properties: (i) instrument relevance (nonzero correlation with the regressors), and (ii) instrument exogeneity (zero correlation with $v(t)$).\n\nYou have access to the following measured data generated from a stable system with initial conditions $y(0)=0$ and $u(0)=0$:\n- Input: $u(1)=1$, $u(2)=0$, $u(3)=1$, $u(4)=0$, $u(5)=1$, $u(6)=0$.\n- Output: $y(1)=1$, $y(2)=1$, $y(3)=0$, $y(4)=2$, $y(5)=1.5$, $y(6)=0.75$.\n\nForm the regressor matrix $\\Phi$ and the data vector $y$ by stacking rows for $t\\in\\{3,4,5,6\\}$, with each row of $\\Phi$ given by $\\begin{pmatrix} y(t-1)  u(t-1) \\end{pmatrix}$ and the corresponding entry of $y$ given by $y(t)$. Construct a valid instrument matrix $Z$ using only lagged inputs such that each instrument is correlated with the columns of $\\Phi$ but uncorrelated with $v(t)$ for the above data. Then, using the instrumental variables method and the sample moment conditions implied by instrument exogeneity and relevance, compute the instrumental variables estimate $\\hat{\\theta}_{\\mathrm{IV}}$.\n\nExpress your final estimate as a single row vector $\\begin{pmatrix} \\hat{a}  \\hat{b} \\end{pmatrix}$. No rounding is required.", "solution": "The problem presented is a classic exercise in system identification, specifically estimating the parameters of an Autoregressive with eXogenous input (ARX) model when the disturbance term is not white noise but a colored moving-average process. This structure violates the assumptions of the ordinary least squares (OLS) method, necessitating a more sophisticated technique such as Instrumental Variables (IV).\n\nFirst, we must validate the problem statement.\n\nStep 1: Extract Givens.\nThe system model is given by a set of two equations:\n$$y(t) \\;=\\; a\\,y(t-1)\\;+\\;b\\,u(t-1)\\;+\\;v(t)$$\n$$v(t) \\;=\\; e(t)\\;+\\;c\\,e(t-1)$$\nwhere $\\{e(t)\\}$ is an independently and identically distributed (i.i.d.) zero-mean sequence with finite variance, and is statistically independent of the input sequence $\\{u(t)\\}$. The parameter vector to be estimated is $\\theta = \\begin{pmatrix} a  b \\end{pmatrix}^{\\top}$.\nThe available data are:\n- Initial conditions: $y(0)=0$, $u(0)=0$.\n- Input sequence: $u(1)=1$, $u(2)=0$, $u(3)=1$, $u(4)=0$, $u(5)=1$, $u(6)=0$.\n- Output sequence: $y(1)=1$, $y(2)=1$, $y(3)=0$, $y(4)=2$, $y(5)=1.5$, $y(6)=0.75$.\nThe task is to form the regressor matrix $\\Phi$ and the output vector $\\mathbf{y}$ for the time instances $t \\in \\{3, 4, 5, 6\\}$. The rows of $\\Phi$ are $\\phi_t^{\\top} = \\begin{pmatrix} y(t-1)  u(t-1) \\end{pmatrix}$. Subsequently, a valid instrument matrix $Z$ must be constructed using only lagged inputs to compute the IV estimate $\\hat{\\theta}_{\\mathrm{IV}}$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. The model is an ARMAX(1,1,1) model, a standard representation in time-series analysis and control theory. The issue of OLS inconsistency due to correlation between regressors (specifically $y(t-1)$) and the colored noise $v(t)$ is a fundamental concept in econometrics and system identification. The Instrumental Variables method is the standard and correct approach to resolve this issue. The problem is well-posed, providing all necessary data and a clear objective. The language is objective and precise. The problem is a standard, solvable problem in its field and contains no logical, scientific, or factual flaws.\n\nVerdict: The problem is valid. We proceed with the solution.\n\nThe system equations can be written in a linear regression form for a set of time indices. We are instructed to use $t \\in \\{3, 4, 5, 6\\}$. This gives us a system of $4$ equations:\n$$\n\\begin{pmatrix} y(3) \\\\ y(4) \\\\ y(5) \\\\ y(6) \\end{pmatrix} = \\begin{pmatrix} y(2)  u(2) \\\\ y(3)  u(3) \\\\ y(4)  u(4) \\\\ y(5)  u(5) \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} + \\begin{pmatrix} v(3) \\\\ v(4) \\\\ v(5) \\\\ v(6) \\end{pmatrix}\n$$\nThis is expressed in matrix form as $\\mathbf{y} = \\Phi\\theta + \\mathbf{v}$. Using the provided data, we construct the output vector $\\mathbf{y}$ and the regressor matrix $\\Phi$:\n$$\n\\mathbf{y} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1.5 \\\\ 0.75 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ \\frac{3}{2} \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n$$\n\\Phi = \\begin{pmatrix} y(2)  u(2) \\\\ y(3)  u(3) \\\\ y(4)  u(4) \\\\ y(5)  u(5) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 2  0 \\\\ 1.5  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 2  0 \\\\ \\frac{3}{2}  1 \\end{pmatrix}\n$$\nThe OLS estimator would be biased because the regressor $y(t-1)$ is correlated with the noise $v(t)$ via the term $e(t-1)$. We must choose instruments that are correlated with the regressors in $\\Phi$ but uncorrelated with the noise vector $\\mathbf{v}$. The problem directs us to use lagged inputs. A standard and valid choice for the instrument vector $z_t$ corresponding to the regressor vector $\\phi_t^{\\top} = \\begin{pmatrix} y(t-1)  u(t-1) \\end{pmatrix}$ is to replace the endogenous regressor $y(t-1)$ with a lagged input, such as $u(t-2)$, while keeping the exogenous regressor $u(t-1)$ as its own instrument. This yields the instrument vector $z_t^{\\top} = \\begin{pmatrix} u(t-2)  u(t-1) \\end{pmatrix}$. This choice satisfies the exogeneity condition, as inputs $\\{u(\\tau)\\}$ are independent of the noise process $\\{e(t)\\}$, and hence uncorrelated with $v(t) = e(t) + c e(t-1)$ for $\\tau  t$.\n\nWe construct the instrument matrix $Z$ for $t \\in \\{3, 4, 5, 6\\}$:\nFor $t=3$, $z_3^{\\top} = \\begin{pmatrix} u(1)  u(2) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\end{pmatrix}$.\nFor $t=4$, $z_4^{\\top} = \\begin{pmatrix} u(2)  u(3) \\end{pmatrix} = \\begin{pmatrix} 0  1 \\end{pmatrix}$.\nFor $t=5$, $z_5^{\\top} = \\begin{pmatrix} u(3)  u(4) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\end{pmatrix}$.\nFor $t=6$, $z_6^{\\top} = \\begin{pmatrix} u(4)  u(5) \\end{pmatrix} = \\begin{pmatrix} 0  1 \\end{pmatrix}$.\nStacking these row vectors gives the instrument matrix $Z$:\n$$\nZ = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe Instrumental Variables estimator $\\hat{\\theta}_{\\mathrm{IV}}$ is given by the solution to the sample moment conditions $Z^{\\top}(\\mathbf{y} - \\Phi\\hat{\\theta}) = 0$, which is:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (Z^{\\top}\\Phi)^{-1}Z^{\\top}\\mathbf{y}\n$$\nWe compute the required matrix products. First, $Z^{\\top}\\Phi$:\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 2  0 \\\\ \\frac{3}{2}  1 \\end{pmatrix} = \\begin{pmatrix} (1 \\cdot 1 + 1 \\cdot 2)  (1 \\cdot 0 + 1 \\cdot 0) \\\\ (1 \\cdot 0 + 1 \\cdot \\frac{3}{2})  (1 \\cdot 1 + 1 \\cdot 1) \\end{pmatrix} = \\begin{pmatrix} 3  0 \\\\ \\frac{3}{2}  2 \\end{pmatrix}\n$$\nThe determinant of this matrix is $(3)(2) - (0)(\\frac{3}{2}) = 6$, which is non-zero. Thus, the matrix is invertible, and our choice of instruments is valid for this data set. The inverse is:\n$$\n(Z^{\\top}\\Phi)^{-1} = \\frac{1}{6} \\begin{pmatrix} 2  0 \\\\ -\\frac{3}{2}  3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{6}  0 \\\\ -\\frac{3}{12}  \\frac{3}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ -\\frac{1}{4}  \\frac{1}{2} \\end{pmatrix}\n$$\nNext, we compute $Z^{\\top}\\mathbf{y}$:\n$$\nZ^{\\top}\\mathbf{y} = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ \\frac{3}{2} \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + 1 \\cdot \\frac{3}{2} \\\\ 1 \\cdot 2 + 1 \\cdot \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ 2 + \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{11}{4} \\end{pmatrix}\n$$\nFinally, we compute the IV estimate $\\hat{\\theta}_{\\mathrm{IV}}$:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\begin{pmatrix} \\hat{a} \\\\ \\hat{b} \\end{pmatrix} = (Z^{\\top}\\Phi)^{-1} (Z^{\\top}\\mathbf{y}) = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ -\\frac{1}{4}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{11}{4} \\end{pmatrix}\n$$\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\begin{pmatrix} (\\frac{1}{3})(\\frac{3}{2}) + (0)(\\frac{11}{4}) \\\\ (-\\frac{1}{4})(\\frac{3}{2}) + (\\frac{1}{2})(\\frac{11}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{3}{8} + \\frac{11}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{8}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}\n$$\nThe estimated parameters are $\\hat{a} = \\frac{1}{2} = 0.5$ and $\\hat{b} = 1$. The problem asks for the result as a single row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5  1 \\end{pmatrix}}\n$$", "id": "2878920"}, {"introduction": "A central challenge in system identification is selecting an appropriate model complexity, a decision that embodies the classic bias-variance trade-off. An overly simple model will underfit the true dynamics (high bias), while an overly complex one will overfit the measurement noise (high variance). This final practice demonstrates how to use an information criterion, such as the Bayesian Information Criterion (BIC), to quantitatively and objectively select the most suitable model order from a family of candidates [@problem_id:2878934].", "problem": "You are given a family of discrete-time single-input single-output AutoRegressive with eXogenous input (ARX) models indexed by an order parameter $p \\in \\{1,2,3,4,5\\}$. For each candidate order $p$, an identification experiment on a data record of length $N=1000$ produced a one-step-ahead residual sequence that is well-modeled as independent and identically distributed Gaussian noise with zero mean and unknown variance. The corresponding estimated residual variances $\\hat{\\sigma}_{p}^{2}$ (computed as the empirical mean of squared residuals) and the number of free model parameters $k_{p}$ are listed below:\n- $p=1$: $\\hat{\\sigma}_{1}^{2} = 1.05$, $k_{1} = 3$.\n- $p=2$: $\\hat{\\sigma}_{2}^{2} = 0.98$, $k_{2} = 5$.\n- $p=3$: $\\hat{\\sigma}_{3}^{2} = 0.94$, $k_{3} = 7$.\n- $p=4$: $\\hat{\\sigma}_{4}^{2} = 0.93$, $k_{4} = 9$.\n- $p=5$: $\\hat{\\sigma}_{5}^{2} = 0.929$, $k_{5} = 11$.\n\nStarting from the Gaussian likelihood for the residuals and the definition of the Bayesian Information Criterion (BIC) as a large-sample approximation to the negative twice log model evidence with a complexity penalty proportional to the number of free parameters and the logarithm of the sample size, derive a selection score that depends only on $N$, $\\hat{\\sigma}_{p}^{2}$, and $k_{p}$ and is sufficient to compare these models. Then, using that score, compute the value for each $p \\in \\{1,2,3,4,5\\}$ and identify the order $p^{\\star}$ that should be selected.\n\nReport only the minimizing order index $p^{\\star}$ as your final answer. No rounding instruction is necessary because $p^{\\star}$ is an integer.", "solution": "The problem requires the selection of an optimal model order $p$ from a set of candidate ARX models using the Bayesian Information Criterion (BIC). The first step is to derive the BIC score from first principles, as specified.\n\nLet the one-step-ahead prediction errors, or residuals, for a model of order $p$ be denoted by the sequence $\\{\\epsilon_p(t)\\}_{t=1}^{N}$. The problem states that these residuals are well-modeled as independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and variance $\\sigma_p^2$. The probability density function (PDF) of a single residual $\\epsilon_p(t)$ is given by:\n$$ f(\\epsilon_p(t) | \\sigma_p^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) $$\nGiven a data record of length $N$, the likelihood function $L_p$ for the model of order $p$ is the joint probability of observing the entire sequence of residuals. Due to the i.i.d. assumption, this is the product of the individual PDFs:\n$$ L_p(\\{\\epsilon_p(t)\\} | \\sigma_p^2) = \\prod_{t=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) $$\nIt is more convenient to work with the log-likelihood function, $\\ln L_p$:\n$$ \\ln L_p = \\sum_{t=1}^{N} \\ln\\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{\\epsilon_p(t)^2}{2\\sigma_p^2}\\right) \\right] = \\sum_{t=1}^{N} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_p^2) - \\frac{\\epsilon_p(t)^2}{2\\sigma_p^2} \\right] $$\n$$ \\ln L_p = -\\frac{N}{2}\\ln(2\\pi\\sigma_p^2) - \\frac{1}{2\\sigma_p^2}\\sum_{t=1}^{N} \\epsilon_p(t)^2 $$\nThe problem provides the estimated residual variance $\\hat{\\sigma}_p^2$, which is computed as the empirical mean of the squared residuals. This corresponds to the maximum likelihood estimate (MLE) of the variance, found by setting the derivative of the log-likelihood with respect to $\\sigma_p^2$ to zero. The sum of squared residuals is thus $\\sum_{t=1}^{N} \\epsilon_p(t)^2 = N\\hat{\\sigma}_p^2$.\n\nTo find the BIC, we need the maximized value of the log-likelihood, $\\ln L_{max, p}$. This is obtained by substituting the MLE $\\hat{\\sigma}_p^2$ into the log-likelihood expression:\n$$ \\ln L_{max, p} = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}_p^2) - \\frac{1}{2\\hat{\\sigma}_p^2}(N\\hat{\\sigma}_p^2) = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}_p^2) - \\frac{N}{2} $$\n$$ \\ln L_{max, p} = -\\frac{N}{2} \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] $$\nThe Bayesian Information Criterion (BIC) is generally defined as:\n$$ \\text{BIC}_p = -2 \\ln L_{max, p} + k_p \\ln(N) $$\nwhere $k_p$ is the number of free parameters in the model of order $p$ and $N$ is the sample size. Substituting our expression for $\\ln L_{max, p}$:\n$$ \\text{BIC}_p = -2 \\left( -\\frac{N}{2} \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] \\right) + k_p \\ln(N) $$\n$$ \\text{BIC}_p = N \\left[ \\ln(\\hat{\\sigma}_p^2) + \\ln(2\\pi) + 1 \\right] + k_p \\ln(N) $$\n$$ \\text{BIC}_p = N \\ln(\\hat{\\sigma}_p^2) + k_p \\ln(N) + N(\\ln(2\\pi) + 1) $$\nThe goal is to select the model order $p$ that minimizes $\\text{BIC}_p$. When comparing the BIC values for different orders $p$, any term that is constant across all models can be discarded. The term $N(\\ln(2\\pi) + 1)$ is such a constant. Therefore, a sufficient selection score, $S_p$, for comparing the models is:\n$$ S_p = N \\ln(\\hat{\\sigma}_p^2) + k_p \\ln(N) $$\nThis expression depends only on the given quantities $N$, $\\hat{\\sigma}_p^2$, and $k_p$, as required. We now compute this score for each candidate order $p \\in \\{1, 2, 3, 4, 5\\}$, using the provided data: $N=1000$. The natural logarithm of the sample size is $\\ln(1000) \\approx 6.907755$.\n\nFor $p=1$: $k_1=3$, $\\hat{\\sigma}_1^2=1.05$.\n$S_1 = 1000 \\ln(1.05) + 3 \\ln(1000) \\approx 1000(0.048790) + 3(6.907755) \\approx 48.790 + 20.723 = 69.513$\n\nFor $p=2$: $k_2=5$, $\\hat{\\sigma}_2^2=0.98$.\n$S_2 = 1000 \\ln(0.98) + 5 \\ln(1000) \\approx 1000(-0.020203) + 5(6.907755) \\approx -20.203 + 34.539 = 14.336$\n\nFor $p=3$: $k_3=7$, $\\hat{\\sigma}_3^2=0.94$.\n$S_3 = 1000 \\ln(0.94) + 7 \\ln(1000) \\approx 1000(-0.061875) + 7(6.907755) \\approx -61.875 + 48.354 = -13.521$\n\nFor $p=4$: $k_4=9$, $\\hat{\\sigma}_4^2=0.93$.\n$S_4 = 1000 \\ln(0.93) + 9 \\ln(1000) \\approx 1000(-0.072571) + 9(6.907755) \\approx -72.571 + 62.170 = -10.401$\n\nFor $p=5$: $k_5=11$, $\\hat{\\sigma}_5^2=0.929$.\n$S_5 = 1000 \\ln(0.929) + 11 \\ln(1000) \\approx 1000(-0.073651) + 11(6.907755) \\approx -73.651 + 75.985 = 2.334$\n\nComparing the computed scores:\n$S_1 \\approx 69.513$\n$S_2 \\approx 14.336$\n$S_3 \\approx -13.521$\n$S_4 \\approx -10.401$\n$S_5 \\approx 2.334$\n\nThe minimum score is achieved for $p=3$. This indicates that the model of order $3$ provides the best trade-off between goodness of fit (low residual variance) and model complexity (low number of parameters), according to the Bayesian Information Criterion. Therefore, the selected model order is $p^{\\star}=3$.", "answer": "$$\\boxed{3}$$", "id": "2878934"}]}