{"hands_on_practices": [{"introduction": "A common intuition in experiment design is that a \"strong\" input signal, one with high energy, should be good for system identification. This exercise serves as a fundamental counterexample to that notion, demonstrating that spectral richness is the critical factor, not merely signal power. By analyzing a pure sinusoidal input [@problem_id:2876722], we will rigorously show that it lacks the necessary frequency content to persistently excite a system of order three or higher, leading to a rank-deficient information matrix regardless of the input's amplitude.", "problem": "Consider a single-input single-output discrete-time finite impulse response system of known order $n \\ge 3$ but unknown coefficients. Let the regressor at time $k$ be the vector $\\varphi_{k} \\in \\mathbb{R}^{n}$ defined by $\\varphi_{k} = \\big[u(k), u(k-1), \\dots, u(k-n+1)\\big]^{\\top}$. A sequence $\\{u(k)\\}$ is said to be persistently exciting (PE) of order $n$ if there exist constants $\\alpha > 0$ and an integer $N_{0} \\ge n$ such that for all integers $t$, the finite-horizon regressor Gramian $G_{t} = \\sum_{k=t}^{t+N_{0}-1} \\varphi_{k} \\varphi_{k}^{\\top}$ satisfies $G_{t} \\succeq \\alpha I_{n}$, where $I_{n}$ is the $n \\times n$ identity matrix. One equivalent way to assess this property is via the asymptotic time-average correlation matrix $R = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\varphi_{k} \\varphi_{k}^{\\top}$, where positive definiteness of $R$ is necessary for persistency of excitation of order $n$.\n\nYou are given an input sequence of the form $u(k) = A \\cos(\\omega k)$ with $A > 0$ and frequency $\\omega \\in (0, \\pi)$ fixed. This class of inputs allows the average input energy per sample, $\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} u(k)^{2}$, to be made arbitrarily large by increasing $A$, thereby meeting any prescribed energy maximization objective that only depends on total or average energy.\n\nUsing only the above definitions and fundamental trigonometric identities, compute the exact value of the smallest eigenvalue $\\lambda_{\\min}$ of the matrix $R$ associated with this input, as a function of $A$, $n$, and $\\omega$. Your final answer must be a single closed-form expression. No rounding is required, and no units are involved. This computation will establish a counterexample demonstrating that maximizing input energy alone does not guarantee persistency of excitation of order $n \\ge 3$ and highlights the necessity of spectral richness in the input.", "solution": "The problem is subjected to validation and is found to be valid. It is scientifically grounded, well-posed, and all definitions required for its resolution are provided. I will now proceed with the solution.\n\nThe asymptotic time-average correlation matrix $R$ is defined as\n$$R = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\varphi_{k} \\varphi_{k}^{\\top}$$\nwhere $\\varphi_{k} = \\big[u(k), u(k-1), \\dots, u(k-n+1)\\big]^{\\top}$ is the regressor vector of dimension $n$. The matrix $R$ is an $n \\times n$ matrix whose elements, denoted $R_{ij}$ for $i, j \\in \\{1, 2, \\dots, n\\}$, are given by\n$$R_{ij} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} u(k-i+1) u(k-j+1)$$\nThe input sequence is given by $u(k) = A \\cos(\\omega k)$ with $A > 0$ and $\\omega \\in (0, \\pi)$. Substituting this into the expression for $R_{ij}$:\n$$R_{ij} = A^2 \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\omega k - \\omega(i-1)) \\cos(\\omega k - \\omega(j-1))$$\nWe employ the product-to-sum trigonometric identity $\\cos(X)\\cos(Y) = \\frac{1}{2}[\\cos(X-Y) + \\cos(X+Y)]$.\nLet $X = \\omega k - \\omega(i-1)$ and $Y = \\omega k - \\omega(j-1)$. Then,\n$$X-Y = \\omega(j-1) - \\omega(i-1) = \\omega(j-i)$$\n$$X+Y = 2\\omega k - \\omega(i-1) - \\omega(j-1) = 2\\omega k - \\omega(i+j-2)$$\nThe expression for $R_{ij}$ becomes\n$$R_{ij} = A^2 \\lim_{N \\to \\infty} \\frac{1}{2N} \\sum_{k=0}^{N-1} \\left[ \\cos(\\omega(j-i)) + \\cos(2\\omega k - \\omega(i+j-2)) \\right]$$\nWe can separate the limit of the sum into the sum of the limits:\n$$R_{ij} = \\frac{A^2}{2} \\left[ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\omega(j-i)) + \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(2\\omega k - \\omega(i+j-2)) \\right]$$\nThe first term, $\\cos(\\omega(j-i))$, is a constant with respect to the summation index $k$. Its time average is simply the constant itself.\nFor the second term, we must evaluate the time average of a sinusoid. For any non-zero frequency $\\nu$ that is not a multiple of $2\\pi$, the time average is zero: $\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(\\nu k + \\phi) = 0$. In our case, the frequency is $2\\omega$. Since $\\omega \\in (0, \\pi)$, we have $2\\omega \\in (0, 2\\pi)$. Thus, the frequency is not a multiple of $2\\pi$, and the limit is zero.\n$$ \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=0}^{N-1} \\cos(2\\omega k - \\omega(i+j-2)) = 0$$\nTherefore, the elements of the matrix $R$ are\n$$R_{ij} = \\frac{A^2}{2} \\cos(\\omega(j-i))$$\nNow, we must analyze the structure of the matrix $R$. We can express the entry $R_{ij}$ using the cosine angle subtraction formula, $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$:\n$$R_{ij} = \\frac{A^2}{2} \\left[ \\cos(\\omega(i-1)) \\cos(\\omega(j-1)) + \\sin(\\omega(i-1)) \\sin(\\omega(j-1)) \\right]$$\nThis reveals that the matrix $R$ can be written as the sum of two outer products. Let us define two vectors $c, s \\in \\mathbb{R}^n$:\n$$c = \\begin{pmatrix} \\cos(0\\omega) \\\\ \\cos(1\\omega) \\\\ \\vdots \\\\ \\cos((n-1)\\omega) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\cos(\\omega) \\\\ \\vdots \\\\ \\cos((n-1)\\omega) \\end{pmatrix}$$\n$$s = \\begin{pmatrix} \\sin(0\\omega) \\\\ \\sin(1\\omega) \\\\ \\vdots \\\\ \\sin((n-1)\\omega) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\sin(\\omega) \\\\ \\vdots \\\\ \\sin((n-1)\\omega) \\end{pmatrix}$$\nThen, the matrix $R$ can be expressed as\n$$R = \\frac{A^2}{2} \\left( c c^{\\top} + s s^{\\top} \\right)$$\nThe matrix $R$ is a linear combination of two rank-one matrices, $c c^{\\top}$ and $s s^{\\top}$. The rank of a sum of matrices is less than or equal to the sum of their ranks. Thus, $\\mathrm{rank}(R) \\le \\mathrm{rank}(c c^{\\top}) + \\mathrm{rank}(s s^{\\top})$. Since $A>0$, both $c$ and $s$ are non-zero vectors, so $\\mathrm{rank}(c c^{\\top})=1$ and $\\mathrm{rank}(s s^{\\top})=1$. This implies $\\mathrm{rank}(R) \\le 2$.\n\nTo determine the exact rank, we check if the vectors $c$ and $s$ are linearly independent. Consider a linear combination $\\alpha c + \\beta s = 0$. This vector equation implies a set of $n$ scalar equations:\n$$\\alpha \\cos(k\\omega) + \\beta \\sin(k\\omega) = 0 \\quad \\text{for } k = 0, 1, \\dots, n-1$$\nFor $k=0$, the equation becomes $\\alpha \\cos(0) + \\beta \\sin(0) = \\alpha \\cdot 1 + \\beta \\cdot 0 = \\alpha = 0$.\nSubstituting $\\alpha=0$ into the equation for $k=1$, we get $\\beta \\sin(\\omega) = 0$. Since $\\omega \\in (0, \\pi)$, $\\sin(\\omega) \\ne 0$, which implies $\\beta=0$.\nThus, the only solution is $\\alpha=\\beta=0$, which proves that the vectors $c$ and $s$ are linearly independent. The column space of $R$ is spanned by $c$ and $s$, so $\\mathrm{rank}(R) = 2$.\n\nWe are given that the order of the system is $n \\ge 3$. According to the rank-nullity theorem, for an $n \\times n$ matrix $R$, we have $\\mathrm{rank}(R) + \\mathrm{nullity}(R) = n$.\nWith $\\mathrm{rank}(R)=2$, the nullity of $R$ is $n-2$.\nSince $n \\ge 3$, the nullity is $n-2 \\ge 1$. A nullity of at least $1$ means that there exists at least one non-zero vector $v$ such that $Rv=0v$. By definition, this means that $\\lambda=0$ is an eigenvalue of $R$.\n\nFurthermore, the matrix $R$ is a Gramian-type matrix, constructed as a limit of sums of positive semi-definite matrices $\\varphi_k \\varphi_k^{\\top}$. Therefore, $R$ itself must be positive semi-definite. All eigenvalues of a positive semi-definite matrix must be non-negative.\nWe have established that $0$ is an eigenvalue of $R$, and that all eigenvalues of $R$ are greater than or equal to $0$. Consequently, the smallest eigenvalue of $R$, $\\lambda_{\\min}$, must be $0$. This result holds for any $A > 0$ and any $\\omega \\in (0, \\pi)$, given $n \\ge 3$.", "answer": "$$\\boxed{0}$$", "id": "2876722"}, {"introduction": "Building upon the need for spectral richness in single-input systems, we now turn to the complexities of multi-input systems. This practice explores a subtle but critical failure mode where identifiability is lost due to correlations between input channels. We will analyze a scenario [@problem_id:2876761] where each input signal, viewed in isolation, is perfectly persistently exciting. However, a simple linear dependency—one input being a delayed version of the other—causes the joint regressor to become rank-deficient, making it impossible to distinguish the effects of the individual inputs on the output.", "problem": "Consider a two-input single-output discrete-time linear time-invariant finite impulse response system of order $n=2$ per input channel. Let the unknown parameter vector be $\\theta \\in \\mathbb{R}^{4}$, stacking the two-tap coefficients from each input channel. The standard least-squares regressor at time $k$ is\n$$\n\\varphi(k) \\triangleq \\begin{pmatrix} u_{1}(k) \\\\ u_{1}(k-1) \\\\ u_{2}(k) \\\\ u_{2}(k-1) \\end{pmatrix} \\in \\mathbb{R}^{4}.\n$$\nFor a finite horizon $N \\geq 3$, define the data Gramian (information matrix)\n$$\nR_{N} \\triangleq \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top} \\in \\mathbb{R}^{4 \\times 4}.\n$$\nA scalar input sequence $u(k)$ is called persistently exciting (PE) of order $2$ if the $2 \\times 2$ Gramian formed from the regressor $\\psi(k) \\triangleq \\begin{pmatrix} u(k) \\\\ u(k-1) \\end{pmatrix}$ is positive definite over sufficiently long windows, equivalently if its asymptotic time-average Gramian is positive definite. Identifiability of $\\theta$ from least squares over a window requires $R_{N}$ to be positive definite.\n\nNow consider the following constructed input pair. Fix two incommensurate frequencies $\\omega_{1}, \\omega_{2} \\in (0,\\pi)$, for example $\\omega_{1}=\\pi/\\sqrt{2}$ and $\\omega_{2}=\\pi/\\sqrt{3}$. Let\n$$\nu_{1}(k) \\triangleq \\sin(\\omega_{1} k) + \\sin(\\omega_{2} k), \\qquad u_{2}(k) \\triangleq u_{1}(k-1), \\quad \\text{for all integers } k.\n$$\nTasks:\n- Using only fundamental definitions and properties of linear time-invariant systems and least-squares regression, argue why each scalar channel $u_{1}$ and $u_{2}$, considered individually with its own order-$2$ regressor $\\psi_i(k) \\triangleq \\begin{pmatrix} u_{i}(k) \\\\ u_{i}(k-1) \\end{pmatrix}$, is persistently exciting of order $2$.\n- Explain why, despite the individual persistency of excitation of each channel, the joint regressor $\\varphi(k)$ fails to be persistently exciting of order $4$, and characterize the resulting loss of identifiability for $\\theta$ in terms of linear dependencies in the data.\n- Compute the determinant $\\det(R_{N})$ as a function of $N$ for the above input pair. Express your final answer as an exact value with no units. No rounding is required.", "solution": "The problem is well-posed and scientifically sound. We proceed with the solution, addressing each task in order.\n\nFirst, we must establish that the individual input channels $u_1(k)$ and $u_2(k)$ are persistently exciting (PE) of order $2$. According to the definition, a signal $u(k)$ is PE of order $n$ if its asymptotic time-average Gramian matrix is positive definite. For a scalar signal $u_i(k)$ and order $n=2$, the regressor is $\\psi_{i}(k) = \\begin{pmatrix} u_{i}(k) & u_{i}(k-1) \\end{pmatrix}^{\\top}$, and we must analyze the definiteness of the matrix $M_{i} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} \\psi_{i}(k) \\psi_{i}(k)^{\\top}$.\n\nFor the first channel, $u_{1}(k) = \\sin(\\omega_{1} k) + \\sin(\\omega_{2} k)$. The matrix $M_1$ is given by:\n$$\nM_{1} = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} \\begin{pmatrix} u_{1}(k)^{2} & u_{1}(k) u_{1}(k-1) \\\\ u_{1}(k) u_{1}(k-1) & u_{1}(k-1)^{2} \\end{pmatrix}\n$$\nThe entries are the signal's autocorrelation values at lags $0$ and $1$. For a quasi-periodic signal composed of sinusoids with incommensurate frequencies, the time-average of cross-products of terms with different frequencies is zero. The time-average of $\\sin^{2}(\\omega k + \\phi)$ is $\\frac{1}{2}$. Therefore, the autocorrelation function $r_1(\\tau) = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{k=1}^{N} u_{1}(k) u_{1}(k+\\tau)$ is:\n$$\nr_{1}(\\tau) = \\frac{1}{2} \\cos(\\omega_{1} \\tau) + \\frac{1}{2} \\cos(\\omega_{2} \\tau)\n$$\nThe matrix $M_1$ is thus:\n$$\nM_{1} = \\begin{pmatrix} r_{1}(0) & r_{1}(1) \\\\ r_{1}(1) & r_{1}(0) \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2}(\\cos(\\omega_{1}) + \\cos(\\omega_{2})) \\\\ \\frac{1}{2}(\\cos(\\omega_{1}) + \\cos(\\omega_{2})) & 1 \\end{pmatrix}\n$$\nFor $M_1$ to be positive definite, its principal minors must be positive. The first minor is $1 > 0$. The second minor is its determinant, $\\det(M_1) = 1 - \\frac{1}{4}(\\cos(\\omega_{1}) + \\cos(\\omega_{2}))^{2}$. Since $\\omega_{1}, \\omega_{2} \\in (0, \\pi)$, we have $|\\cos(\\omega_{i})| < 1$. By the triangle inequality, $|\\cos(\\omega_{1}) + \\cos(\\omega_{2})| < 2$, which implies $(\\cos(\\omega_{1}) + \\cos(\\omega_{2}))^{2} < 4$. Consequently, $\\det(M_1) > 1 - \\frac{4}{4} = 0$. Since $M_1$ is symmetric with positive principal minors, it is positive definite. Thus, $u_1(k)$ is PE of order $2$. For the second channel, $u_2(k) = u_1(k-1)$. This is merely a time-shift of $u_1(k)$. The autocorrelation function of a stationary or cyclo-stationary process is invariant to a time shift of the signal. Therefore, the asymptotic Gramian matrix $M_2$ for $u_2(k)$ is identical to $M_1$, i.e., $M_2 = M_1$. As $M_1$ is positive definite, so is $M_2$, and $u_2(k)$ is also PE of order $2$.\n\nSecond, we analyze why the joint regressor $\\varphi(k)$ fails to be persistently exciting of order $4$. The regressor is defined as $\\varphi(k) = \\begin{pmatrix} u_{1}(k) & u_{1}(k-1) & u_{2}(k) & u_{2}(k-1) \\end{pmatrix}^{\\top}$. The crucial condition is the linear relationship between the two input signals: $u_{2}(k) = u_{1}(k-1)$ for all $k$. We substitute this into the regressor definition:\n$$\n\\varphi(k) = \\begin{pmatrix} u_{1}(k) \\\\ u_{1}(k-1) \\\\ u_{1}(k-1) \\\\ u_{1}(k-2) \\end{pmatrix}\n$$\nIt is immediately apparent that the second and third components of the vector $\\varphi(k)$ are identical for all $k$. This introduces a linear dependency among the columns of the data matrix over any time horizon. Specifically, let $c = \\begin{pmatrix} 0 & 1 & -1 & 0 \\end{pmatrix}^{\\top}$. Then for any time instant $k$, the inner product $c^{\\top} \\varphi(k)$ is:\n$$\nc^{\\top} \\varphi(k) = 0 \\cdot u_{1}(k) + 1 \\cdot u_{1}(k-1) - 1 \\cdot u_{1}(k-1) + 0 \\cdot u_{1}(k-2) = 0\n$$\nThe information matrix $R_N = \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top}$ is therefore singular. To prove this, we multiply $R_N$ by the non-zero vector $c$:\n$$\nR_N c = \\left( \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top} \\right) c = \\sum_{k=2}^{N} \\varphi(k) (\\varphi(k)^{\\top} c) = \\sum_{k=2}^{N} \\varphi(k) (c^{\\top} \\varphi(k))^{\\top} = \\sum_{k=2}^{N} \\varphi(k) \\cdot 0 = 0\n$$\nSince there exists a non-zero vector $c$ in the null space of $R_N$, the matrix $R_N$ is singular for any $N \\geq 2$. A singular matrix cannot be positive definite, so the joint regressor is not PE of order $4$. This singularity leads to a loss of identifiability. The system model is $y(k) = \\varphi(k)^{\\top} \\theta$. Consider an alternative parameter vector $\\theta' = \\theta + \\alpha c$ for any non-zero scalar $\\alpha$. The system output produced by $\\theta'$ is $\\varphi(k)^{\\top} \\theta' = \\varphi(k)^{\\top} (\\theta + \\alpha c) = \\varphi(k)^{\\top} \\theta + \\alpha \\varphi(k)^{\\top} c = \\varphi(k)^{\\top} \\theta$. The output is identical, meaning that the data cannot be used to distinguish between $\\theta$ and $\\theta'$. Writing $\\theta = \\begin{pmatrix} \\theta_{11} & \\theta_{12} & \\theta_{21} & \\theta_{22} \\end{pmatrix}^{\\top}$, the indistinguishable parameters are of the form $\\theta' = \\begin{pmatrix} \\theta_{11} & \\theta_{12} + \\alpha & \\theta_{21} - \\alpha & \\theta_{22} \\end{pmatrix}^{\\top}$. This shows that only the sum $\\theta_{12} + \\theta_{21}$ is identifiable, but not the individual parameters $\\theta_{12}$ and $\\theta_{21}$.\n\nThird, we are to compute the determinant of $R_N$. The information matrix is given by $R_N = \\sum_{k=2}^{N} \\varphi(k) \\varphi(k)^{\\top}$. Its $(i,j)$-th element is $(R_{N})_{ij} = \\sum_{k=2}^{N} \\varphi_{i}(k) \\varphi_{j}(k)$, where $\\varphi_{i}(k)$ is the $i$-th component of $\\varphi(k)$. As established, $\\varphi_{2}(k) = u_{1}(k-1)$ and $\\varphi_{3}(k) = u_{2}(k) = u_{1}(k-1)$. Therefore, $\\varphi_{2}(k) = \\varphi_{3}(k)$ for all $k$.\nLet us examine the second and third rows of the matrix $R_N$.\nThe $j$-th element of the second row is $(R_N)_{2j} = \\sum_{k=2}^{N} \\varphi_{2}(k) \\varphi_{j}(k)$.\nThe $j$-th element of the third row is $(R_N)_{3j} = \\sum_{k=2}^{N} \\varphi_{3}(k) \\varphi_{j}(k)$.\nSince $\\varphi_{2}(k) = \\varphi_{3}(k)$, it follows trivially that $(R_N)_{2j} = (R_N)_{3j}$ for all $j \\in \\{1, 2, 3, 4\\}$.\nThis means that the second row of the matrix $R_N$ is identical to its third row. A fundamental property of determinants is that if a square matrix has two identical rows (or columns), its determinant is zero. This holds for any value of $N \\geq 2$. Therefore, the determinant of $R_N$ is exactly zero, independent of $N$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2876761"}, {"introduction": "Theoretical persistency of excitation is a necessary condition for identifiability, but it is not always sufficient in practice. This final hands-on exercise bridges the gap between theory and implementation, confronting the challenge of numerical ill-conditioning. Using a coding-based approach [@problem_id:2876779], we will demonstrate how a theoretically valid set of high-order polynomial basis functions can become nearly linearly dependent in finite-precision arithmetic, causing a practical loss of PE. You will then implement and contrast standard mitigation techniques, including input scaling and basis orthogonalization, to restore numerical stability and ensure reliable parameter identification.", "problem": "You are given a linear regression setting with a regressor vector $\\phi(t) \\in \\mathbb{R}^{p+1}$ formed from deterministic basis functions of a scalar input $x(t) \\in \\mathbb{R}$. Identifiability of the parameter vector $\\theta \\in \\mathbb{R}^{p+1}$ in the model $y(t) = \\phi(t)^{\\top}\\theta + w(t)$ relies on properties of the regressor matrix $\\Phi \\in \\mathbb{R}^{N \\times (p+1)}$ with rows $\\phi(t)^{\\top}$, where $N$ is the number of samples and $w(t)$ is an additive error. The commonly used notion of Persistency of Excitation (PE) of order $p+1$ over a data window of length $N$ is expressed in terms of the Gram matrix $G = \\sum_{t=1}^{N} \\phi(t)\\phi(t)^{\\top} = \\Phi^{\\top}\\Phi \\in \\mathbb{R}^{(p+1)\\times (p+1)}$: there exists a constant $\\alpha > 0$ such that $G \\succeq \\alpha I_{p+1}$. When this holds, the model is identifiable on the window. However, numerical roundoff and ill-conditioning may render $G$ effectively singular in floating-point arithmetic even if it is theoretically full rank, thereby destroying practical PE.\n\nYour task is to construct a case where numerical roundoff effectively destroys Persistency of Excitation (PE) for high-order polynomial regressors, and to implement scaling and orthogonalization strategies that mitigate the issue. Base your reasoning and implementation on the following fundamentals:\n- The regressor matrix $\\Phi$ has columns given by deterministic basis functions evaluated on inputs $\\{x(t)\\}_{t=1}^{N}$.\n- The Gram matrix is $G = \\Phi^{\\top}\\Phi$.\n- Persistency of Excitation (PE) over the window is characterized by the smallest eigenvalue $\\lambda_{\\min}(G)$ being bounded away from zero relative to a positive benchmark.\n- Finite-precision arithmetic can cause severe loss of numerical rank for ill-conditioned bases such as high-order monomials on $[0,1]$.\n\nProgram requirements:\n1. Construct monomial regressors up to degree $p$ using inputs $x(t)$, that is, $\\phi(t) = [1, x(t), x(t)^{2}, \\dots, x(t)^{p}]^{\\top}$, and form $\\Phi$ by stacking these row vectors.\n2. Implement the following basis-construction strategies:\n   - Raw monomials on $[0,1]$ (no scaling).\n   - Scaled monomials: map inputs by an affine transform $z(t) = 2x(t) - 1$ to $[-1,1]$ and then normalize each column of $\\Phi$ to unit Euclidean norm.\n   - Orthonormalization by a thin (reduced) $QR$ factorization: replace $\\Phi$ by its $Q$ factor, which has orthonormal columns, when $N \\geq p+1$.\n3. For any constructed $\\Phi$, compute:\n   - The Gram matrix $G = \\Phi^{\\top}\\Phi$.\n   - The smallest and largest eigenvalues $\\lambda_{\\min}(G)$ and $\\lambda_{\\max}(G)$.\n   - The spectral condition number $\\kappa(G) = \\lambda_{\\max}(G) / \\lambda_{\\min}(G)$, with a large finite placeholder such as $10^{308}$ if $\\lambda_{\\min}(G) = 0$ numerically.\n   - The numerical rank of $\\Phi$ by singular value decomposition using the threshold $\\tau_{\\mathrm{rank}} = \\max(N,p+1)\\,\\varepsilon\\,\\sigma_{\\max}$, where $\\varepsilon$ is machine precision and $\\sigma_{\\max}$ is the largest singular value of $\\Phi$.\n   - Two PE checks:\n     - A relative PE check: declare $\\mathrm{PE}_{\\mathrm{rel}}$ true if $\\lambda_{\\min}(G)/\\lambda_{\\max}(G) \\geq \\tau_{\\mathrm{rel}}$ with $\\tau_{\\mathrm{rel}} = 10^{-8}$.\n     - A scale-invariant absolute PE proxy: declare $\\mathrm{PE}_{\\mathrm{abs}}$ true if $\\lambda_{\\min}(G) \\geq \\tau_{\\mathrm{abs}} \\cdot \\mathrm{tr}(G)/(p+1)$ with $\\tau_{\\mathrm{abs}} = 10^{-8}$.\n4. Use double-precision arithmetic throughout.\n\nTest suite:\nUse equally spaced inputs $x(t)$ on $[0,1]$ with $N$ samples. For each case, report a list of five items in this order: $[\\lambda_{\\min}(G), \\kappa(G), \\text{rank}(\\Phi), \\mathrm{PE}_{\\mathrm{rel}}, \\mathrm{PE}_{\\mathrm{abs}}]$.\n- Case A (high-order, raw monomials; roundoff destroys PE): $N=60, p=25$, method = raw monomials on $[0,1]$.\n- Case B (high-order, scaled monomials; mitigates ill-conditioning): $N=60, p=25$, method = scaled monomials via $z = 2x-1$ and column normalization.\n- Case C (high-order, orthonormalized by $QR$; mitigates ill-conditioning): $N=60, p=25$, method = $Q$ from the reduced $QR$ factorization of the raw monomial matrix.\n- Case D (happy path, low order, raw monomials): $N=60, p=5$, method = raw monomials on $[0,1]$.\n- Case E (edge case, insufficient excitation by dimension; structural non-PE): $N=10, p=25$, method = raw monomials on $[0,1]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the five-item list for a test case in the order A, B, C, D, E. For example, the output must look like:\n[[a11,a12,a13,a14,a15],[a21,a22,a23,a24,a25],[a31,a32,a33,a34,a35],[a41,a42,a43,a44,a45],[a51,a52,a53,a54,a55]]\nwhere $a_{ij}$ are floats, integers, or booleans as defined above. No units are involved in this problem. Angles are not used in this problem. Percentages, if any, must be expressed as decimals and not with the percentage sign.", "solution": "The problem statement presented is a valid and well-posed exercise in numerical analysis as applied to system identification. It is scientifically grounded, objective, and contains all necessary information to proceed. The task is to demonstrate the phenomenon of numerical loss of Persistency of Excitation (PE) for high-order polynomial regressors and to implement and evaluate standard mitigation techniques.\n\nThe core of the problem lies in the properties of the regressor matrix $\\Phi \\in \\mathbb{R}^{N \\times (p+1)}$ and its associated Gram matrix $G = \\Phi^{\\top}\\Phi \\in \\mathbb{R}^{(p+1)\\times (p+1)}$. For a linear regression model described by $y(t) = \\phi(t)^{\\top}\\theta + w(t)$, the parameter vector $\\theta \\in \\mathbb{R}^{p+1}$ is identifiable from a set of $N$ measurements if and only if the Gram matrix $G$ is invertible, or equivalently, positive definite ($G \\succ 0$). This condition is known as Persistency of Excitation (PE). Numerically, it is assessed by verifying that the smallest eigenvalue of $G$, denoted $\\lambda_{\\min}(G)$, is sufficiently bounded away from zero.\n\nThe condition number of the Gram matrix, $\\kappa(G) = \\lambda_{\\max}(G)/\\lambda_{\\min}(G)$, provides a measure of its proximity to singularity. A very large condition number indicates that the matrix is ill-conditioned and numerically close to being singular. For any matrix $\\Phi$ with full column rank, the condition number of its Gram matrix is related to its own condition number by $\\kappa(G) = (\\kappa(\\Phi))^2$. Thus, any ill-conditioning in $\\Phi$ is amplified in $G$.\n\nBelow is a step-by-step analysis corresponding to the test cases.\n\n**Case A: High-Order Raw Monomials ($N=60, p=25$)**\n\nThe regressor vector is constructed using a raw monomial basis on the interval $[0,1]$:\n$$ \\phi(t) = [1, x(t), x(t)^2, \\dots, x(t)^{25}]^{\\top} $$\nwhere the inputs $\\{x(t)\\}_{t=1}^{60}$ are equally spaced on $[0,1]$. For large powers $p$, the functions $x^p$ and $x^{p-1}$ become almost indistinguishable on this interval. For example, for $x=0.95$, $x^{25} \\approx 0.277$ and $x^{24} \\approx 0.292$. As $x$ approaches $1$, the functions become even more alike. This leads to the columns of the regressor matrix $\\Phi$ being nearly linearly dependent. Such a matrix is related to the notoriously ill-conditioned Hilbert matrix.\n\nConsequently, $\\Phi$ is severely ill-conditioned. In finite-precision arithmetic (double precision, in this case), the computed Gram matrix $G = \\Phi^{\\top}\\Phi$ becomes numerically singular. This means its smallest eigenvalue $\\lambda_{\\min}(G)$ will be computed as zero or a number on the order of machine epsilon, which is functionally equivalent to zero. The numerical rank of $\\Phi$ will be found to be less than the number of parameters, $p+1=26$. The PE checks, which rely on $\\lambda_{\\min}(G)$ being significantly positive, will fail. This demonstrates a practical loss of identifiability due to numerical error, even though the basis is theoretically full rank.\n\n**Case B: Scaled Monomials ($N=60, p=25$)**\n\nTo mitigate the ill-conditioning observed in Case A, two strategies are employed in this case. First, the input interval $[0,1]$ is affinely mapped to $[-1,1]$ via the transformation $z(t) = 2x(t)-1$. The monomial basis functions are now powers of $z(t)$, i.e., $\\{z(t)^k\\}_{k=0}^{p}$. These are more distinct on $[-1,1]$ than the powers of $x(t)$ are on $[0,1]$. Second, and more importantly, each column of the resulting regressor matrix $\\Phi$ is normalized to have a unit Euclidean norm.\n\nThis column normalization balances the scale of the basis functions. In the raw monomial case, the first column (for $x^0=1$) has a norm of $\\sqrt{N}$, while the last column (for $x^{25}$) has a much smaller norm. This extreme disparity in column magnitudes is a primary source of ill-conditioning. By enforcing $\\|\\Phi_{:,j}\\|_2 = 1$ for all columns $j$, we create a better-conditioned matrix $\\Phi$. As a result, the new Gram matrix $G = \\Phi^{\\top}\\Phi$ will have a drastically reduced condition number. Its smallest eigenvalue, $\\lambda_{\\min}(G)$, will be robustly positive and well-separated from zero, thus satisfying the PE checks.\n\n**Case C: Orthonormalized Basis ($N=60, p=25$)**\n\nThe most effective way to ensure good conditioning is to use an orthonormal basis. If the columns of the regressor matrix $\\Phi_{\\text{new}}$ are orthonormal, then the Gram matrix is the identity matrix, $G_{\\text{new}} = \\Phi_{\\text{new}}^{\\top}\\Phi_{\\text{new}} = I_{p+1}$. For an identity matrix, all eigenvalues are $1$, so $\\lambda_{\\min}(G) = \\lambda_{\\max}(G) = 1$, and the condition number is $\\kappa(G) = 1$, which is optimal.\n\nThis ideal state is achieved by performing a thin (or reduced) $QR$ factorization of the original, ill-conditioned raw monomial matrix from Case A, $\\Phi_{\\text{raw}} = QR$. Here, $Q \\in \\mathbb{R}^{N \\times (p+1)}$ is a matrix with orthonormal columns, and $R \\in \\mathbb{R}^{(p+1) \\times (p+1)}$ is an upper triangular matrix. By replacing the regressor matrix with $Q$, we work with a perfectly conditioned system. The resulting $\\lambda_{\\min}(G)$ will be exactly $1.0$ (up to floating-point precision), trivially passing both PE checks. This method completely resolves the numerical ill-conditioning issue.\n\n**Case D: Low-Order Raw Monomials ($N=60, p=5$)**\n\nThis case serves as a \"happy path\" baseline. With a low polynomial degree of $p=5$, the monomial basis functions $\\{1, x, x^2, x^3, x^4, x^5\\}$ are sufficiently distinct on $[0,1]$. The columns of the regressor matrix $\\Phi$ are not nearly linearly dependent. Consequently, both $\\Phi$ and the Gram matrix $G$ are well-conditioned. The smallest eigenvalue $\\lambda_{\\min}(G)$ will be small but comfortably above zero, and the PE checks will pass without any need for scaling or orthogonalization. This highlights that the problem is specific to high-order polynomials.\n\n**Case E: Insufficient Data ($N=10, p=25$)**\n\nThis case demonstrates structural, rather than numerical, lack of PE. We are attempting to identify $p+1=26$ parameters from only $N=10$ data points. The regressor matrix $\\Phi$ has dimensions $10 \\times 26$. The rank of any matrix cannot exceed the minimum of its dimensions, so $\\text{rank}(\\Phi) \\leq \\min(10, 26) = 10$. Since the rank is less than the number of columns ($10 < 26$), the columns of $\\Phi$ are linearly dependent by definition.\n\nThe Gram matrix $G = \\Phi^{\\top}\\Phi$ is a $26 \\times 26$ matrix, and its rank is equal to the rank of $\\Phi$. Since $\\text{rank}(G) = 10 < 26$, $G$ is singular. A singular matrix must have at least one zero eigenvalue; in this case, it will have at least $26 - 10 = 16$ zero eigenvalues. Therefore, $\\lambda_{\\min}(G) = 0$. This is not a numerical artifact but a fundamental consequence of an underspecified problem. The PE condition fails structurally, irrespective of the basis used.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef analyze_pe(N, p, method):\n    \"\"\"\n    Analyzes Persistency of Excitation for a given regressor construction.\n\n    Args:\n        N (int): Number of samples.\n        p (int): Polynomial degree.\n        method (str): Basis construction method ('raw', 'scaled', 'qr').\n\n    Returns:\n        list: A list containing [lambda_min(G), kappa(G), rank(Phi), PE_rel, PE_abs].\n    \"\"\"\n    p_plus_1 = p + 1\n    x = np.linspace(0.0, 1.0, N, dtype=np.float64)\n\n    # 1. Construct the regressor matrix Phi\n    if method in ['raw', 'qr']:\n        # The problem defines phi(t) = [1, x(t), ..., x(t)^p]^T.\n        # np.vander with increasing=True produces rows [x^0, x^1, ...], which is correct.\n        Phi = np.vander(x, N=p_plus_1, increasing=True)\n    elif method == 'scaled':\n        z = 2.0 * x - 1.0\n        Phi = np.vander(z, N=p_plus_1, increasing=True)\n        # Normalize columns to unit Euclidean norm\n        col_norms = np.linalg.norm(Phi, axis=0)\n        # Prevent division by zero if a column is all zeros\n        non_zero_norms = np.where(col_norms == 0, 1.0, col_norms)\n        Phi = Phi / non_zero_norms\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # For the QR method, replace Phi with its Q factor from a thin QR decomposition\n    if method == 'qr':\n        if N >= p_plus_1:\n            Q, _ = qr(Phi, mode='economic')\n            Phi = Q\n        # If N < p+1, Phi is structurally rank-deficient. We analyze it as is,\n        # since QR is meant as a mitigation for ill-conditioning, not structural deficiency.\n        # This branch is not hit by the specified test cases for 'qr'.\n\n    # 2. Compute the Gram matrix G and its eigenvalues\n    G = Phi.T @ Phi\n\n    try:\n        # eigvalsh is preferred for symmetric matrices for speed and stability\n        eigvals = np.linalg.eigvalsh(G)\n    except np.linalg.LinAlgError:\n        # Fallback if G isn't perfectly symmetric due to numerical artifacts\n        eigvals = np.linalg.eigvals(G)\n        eigvals = np.real(eigvals)\n\n    lambda_min = np.min(eigvals)\n    lambda_max = np.max(eigvals)\n\n    # Correct for small negative eigenvalues from floating-point errors\n    lambda_min = max(0.0, lambda_min)\n\n    # 3. Compute condition number of G\n    # Use a large placeholder if lambda_min is effectively zero.\n    if lambda_min < 1e-16:\n        kappa_G = 1.0e308\n    else:\n        kappa_G = lambda_max / lambda_min\n    \n    # For structural rank deficiency, lambda_min is theoretically 0.\n    if N < p_plus_1:\n         lambda_min = 0.0\n         kappa_G = 1.0e308\n\n    # 4. Compute numerical rank of Phi\n    _U, s, _Vh = np.linalg.svd(Phi)\n    sigma_max = np.max(s) if s.size > 0 else 0.0\n    eps = np.finfo(np.float64).eps\n    rank_threshold = max(N, p_plus_1) * eps * sigma_max\n    num_rank = np.sum(s > rank_threshold)\n\n    # 5. Perform PE checks\n    tau_rel = 1e-8\n    tau_abs = 1e-8\n    \n    # Relative PE check\n    pe_rel = (lambda_min / lambda_max) >= tau_rel if lambda_max > 0 else (lambda_min >= 0.0)\n    \n    # Scale-invariant absolute PE proxy\n    trace_G = np.trace(G)\n    pe_abs = (lambda_min >= tau_abs * trace_G / p_plus_1) if p_plus_1 > 0 else False\n\n    return [lambda_min, kappa_G, int(num_rank), pe_rel, pe_abs]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: High-order, raw monomials\n        {'N': 60, 'p': 25, 'method': 'raw'},\n        # Case B: High-order, scaled monomials\n        {'N': 60, 'p': 25, 'method': 'scaled'},\n        # Case C: High-order, orthonormalized by QR\n        {'N': 60, 'p': 25, 'method': 'qr'},\n        # Case D: Low-order, raw monomials\n        {'N': 60, 'p': 5, 'method': 'raw'},\n        # Case E: Insufficient data samples\n        {'N': 10, 'p': 25, 'method': 'raw'},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_pe(N=case['N'], p=case['p'], method=case['method'])\n        results.append(result)\n\n    # Format the final output string exactly as required, without extra spaces.\n    # This involves manually building the string representation of the list of lists.\n    outer_list_str = []\n    for res_list in results:\n        inner_list_str = [str(item) for item in res_list]\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n    \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2876779"}]}