{"hands_on_practices": [{"introduction": "Before applying any estimation technique, we must first ensure our experiment is capable of identifying the parameters of interest. This practice explores the concept of persistent excitation, a key condition for parameter identifiability in system identification. By analyzing a scenario where identification fails due to an uninformative input signal [@problem_id:2878418], you will gain a concrete understanding of why the quality of experimental data is paramount and learn how to diagnose non-identifiability by characterizing the information matrix.", "problem": "Consider a single-input single-output linear time-invariant system modeled for identification by a finite impulse response structure of second order. The measurement equation is\n$$\ny(t) \\;=\\; b_{1}\\,u(t-1) \\;+\\; b_{2}\\,u(t-2) \\;+\\; v(t),\n$$\nwhere $u(t)$ is a deterministic input, $v(t)$ is a zero-mean disturbance uncorrelated with all past inputs, and $\\theta \\triangleq \\begin{pmatrix} b_{1} & b_{2} \\end{pmatrix}^{\\top}$ is the unknown parameter vector to be identified. Define the regressor\n$$\n\\varphi(t) \\;\\triangleq\\; \\begin{pmatrix} u(t-1) \\\\ u(t-2) \\end{pmatrix},\n$$\nso that $y(t) = \\varphi(t)^\\top\\theta + v(t)$. To avoid endogeneity from $v(t)$, an instrumental variable method is used with instrument vector\n$$\nz(t) \\;\\triangleq\\; \\begin{pmatrix} u(t-2) \\\\ u(t-3) \\end{pmatrix},\n$$\nand stacked data matrices\n$$\n\\Phi \\;\\triangleq\\; \\begin{pmatrix} \\varphi(3)^{\\top} \\\\ \\varphi(4)^{\\top} \\\\ \\varphi(5)^{\\top} \\\\ \\varphi(6)^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 2}, \n\\qquad\nZ \\;\\triangleq\\; \\begin{pmatrix} z(3)^{\\top} \\\\ z(4)^{\\top} \\\\ z(5)^{\\top} \\\\ z(6)^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 2}.\n$$\nData are collected for times $t \\in \\{3,4,5,6\\}$ under the experiment $u(t) \\equiv c$ for all integers $t \\leq 6$, with $c$ a known nonzero constant $c = 3$. The instrumental variable normal equations are\n$$\n\\big(Z^{\\top}\\Phi\\big)\\,\\theta \\;=\\; Z^{\\top} y,\n$$\nand identifiability of $\\theta$ requires that $Z^{\\top}\\Phi$ have full column rank.\n\nStarting only from the regression form and the instrumental variable construction above, and using the given excitation, explicitly form $Z^{\\top}\\Phi$, determine its rank, and characterize the set of parameter combinations that are not identifiable from this experiment in terms of the null space of $Z^{\\top}\\Phi$. Then, report a single vector that quantitatively identifies the unidentifiable direction by the following convention: provide the unique unit-norm vector $v \\in \\mathbb{R}^{2}$ with first component positive that spans $\\operatorname{Null}(Z^{\\top}\\Phi)$.\n\nYour final answer must be the row vector $v^{\\top}$ written as a single $1 \\times 2$ matrix. No rounding is required and no units are involved.", "solution": "The problem as stated is valid. It is scientifically sound, well-posed, and objective, presenting a standard scenario in system identification. All necessary information is provided, and the task is to perform a concrete analysis of parameter identifiability under a specific experimental condition.\n\nThe objective is to characterize the unidentifiable directions for the parameter vector $\\theta = \\begin{pmatrix} b_{1} & b_{2} \\end{pmatrix}^{\\top}$. In the context of instrumental variable estimation, identifiability requires the matrix $Z^{\\top}\\Phi$ to be of full column rank. If this condition is not met, the matrix is rank-deficient, and its null space, $\\operatorname{Null}(Z^{\\top}\\Phi)$, defines the subspace of parameter variations that cannot be detected by the experiment. Any vector in this null space represents a combination of parameters that yields no change in the observed instrumental variable normal equations. The task is to find a specific basis vector for this null space.\n\nFirst, we must construct the data matrices $\\Phi$ and $Z$ based on the given input signal. The problem specifies that the input $u(t)$ is a constant, $u(t) = c = 3$, for all relevant times $t \\leq 6$.\n\nThe regressor vector is $\\varphi(t) = \\begin{pmatrix} u(t-1) \\\\ u(t-2) \\end{pmatrix}$. Given the constant input, for any $t$ used in the data matrices, we have $\\varphi(t) = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$. The stacked regressor matrix $\\Phi$ is defined over $t \\in \\{3,4,5,6\\}$:\n$$\n\\Phi = \\begin{pmatrix} \\varphi(3)^{\\top} \\\\ \\varphi(4)^{\\top} \\\\ \\varphi(5)^{\\top} \\\\ \\varphi(6)^{\\top} \\end{pmatrix} = \\begin{pmatrix} u(2) & u(1) \\\\ u(3) & u(2) \\\\ u(4) & u(3) \\\\ u(5) & u(4) \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\n\nThe instrumental variable vector is $z(t) = \\begin{pmatrix} u(t-2) \\\\ u(t-3) \\end{pmatrix}$. Similarly, with a constant input, we have $z(t) = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$ for any $t$. The stacked instrument matrix $Z$ is:\n$$\nZ = \\begin{pmatrix} z(3)^{\\top} \\\\ z(4)^{\\top} \\\\ z(5)^{\\top} \\\\ z(6)^{\\top} \\end{pmatrix} = \\begin{pmatrix} u(1) & u(0) \\\\ u(2) & u(1) \\\\ u(3) & u(2) \\\\ u(4) & u(3) \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\n\nNext, we compute the matrix product $Z^{\\top}\\Phi$. The transpose of $Z$ is:\n$$\nZ^{\\top} = \\begin{pmatrix} 3 & 3 & 3 & 3 \\\\ 3 & 3 & 3 & 3 \\end{pmatrix}.\n$$\nThe product is then:\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 3 & 3 & 3 & 3 \\\\ 3 & 3 & 3 & 3 \\end{pmatrix} \\begin{pmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\\\ 3 & 3 \\end{pmatrix}.\n$$\nPerforming the matrix multiplication:\nThe element at position $(1,1)$ is $(3)(3)+(3)(3)+(3)(3)+(3)(3) = 4 \\times 9 = 36$.\nDue to the structure of the matrices, all elements of the resulting product are identical.\n$$\nZ^{\\top}\\Phi = \\begin{pmatrix} 36 & 36 \\\\ 36 & 36 \\end{pmatrix}.\n$$\n\nNow we determine the rank of this matrix. A $2 \\times 2$ matrix has full column rank if its rank is $2$. The rank of $Z^{\\top}\\Phi$ can be determined by observing that its columns are linearly dependent; specifically, the second column is identical to the first. The determinant is $\\det(Z^{\\top}\\Phi) = (36)(36) - (36)(36) = 0$. Since the matrix is non-zero and singular, its rank is $1$. This is not full column rank, which confirms that the parameter vector $\\theta$ is not identifiable with this experimental setup.\n\nThe unidentifiable directions are given by the null space of $Z^{\\top}\\Phi$. We seek a vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ such that $(Z^{\\top}\\Phi)v=0$.\n$$\n\\begin{pmatrix} 36 & 36 \\\\ 36 & 36 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis matrix equation reduces to the single linear equation $36v_1 + 36v_2 = 0$, which simplifies to $v_1 + v_2 = 0$, or $v_1 = -v_2$. Any vector in the null space is therefore of the form $\\begin{pmatrix} -v_2 \\\\ v_2 \\end{pmatrix} = v_2 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ for some scalar $v_2$. The null space is a one-dimensional subspace of $\\mathbb{R}^2$ spanned by the vector $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n\nThe problem requires a specific vector: the unique unit-norm vector $v$ with a positive first component that spans this null space. Our current spanning vector, $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, has a negative first component. To obtain a spanning vector with a positive first component, we can multiply by any negative scalar. Multiplying by $-1$ gives the vector $w = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nFinally, we normalize $w$ to have unit norm. The norm is $\\|w\\| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}$. The required unit-norm vector $v$ is:\n$$\nv = \\frac{w}{\\|w\\|} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nRationalizing the denominator gives:\n$$\nv = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\end{pmatrix}.\n$$\nThis vector satisfies all conditions: it spans $\\operatorname{Null}(Z^{\\top}\\Phi)$, its first component is positive, and it has unit norm. The question asks for the row vector $v^{\\top}$.\n$$\nv^{\\top} = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\end{pmatrix}.\n$$\nThis vector quantitatively identifies the unidentifiable direction in the parameter space $\\{b_1, b_2\\}$. Any change to the parameter vector $\\theta$ along this direction is undetectable.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\end{pmatrix}}\n$$", "id": "2878418"}, {"introduction": "The Instrumental Variable (IV) method can be conceptualized from both a geometric perspective, involving projection, and an algorithmic one, known as Two-Stage Least Squares (2SLS). This exercise demonstrates the fundamental algebraic equivalence between these two essential viewpoints. By manually computing an estimate for a small dataset using both approaches [@problem_id:2878432], you will solidify your grasp of the underlying linear algebra and see precisely how the 2SLS procedure serves as a practical implementation of the theoretical IV principle.", "problem": "Consider the linear regression form of a discrete-time, finite-sample system identification problem with potentially endogenous regressors: for sample index $k \\in \\{1,\\dots,n\\}$, the measured output $y_k$ and regressor row vector $\\varphi_k^{\\top}$ satisfy $y_k = \\varphi_k^{\\top}\\theta + v_k$, where the disturbance $v_k$ may be correlated with components of $\\varphi_k$, but there exists an instrument vector $z_k$ such that $\\mathbb{E}[z_k v_k] = 0$ and $\\mathbb{E}[z_k \\varphi_k^{\\top}]$ has full column rank. Collecting $n$ samples into the stacked vectors and matrices yields $y \\in \\mathbb{R}^{n}$, $\\Phi \\in \\mathbb{R}^{n \\times p}$, $Z \\in \\mathbb{R}^{n \\times m}$, and an unknown parameter $\\theta \\in \\mathbb{R}^{p}$. The instrumental variable (IV) method and the Two-Stage Least Squares (2SLS) method are both based on projecting the regressors into the instrument space to remove endogeneity.\n\nYou are given a concrete dataset with $n=4$, $p=2$, $m=2$:\n$$\n\\Phi=\\begin{bmatrix}\n1 & 2\\\\\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\nZ=\\begin{bmatrix}\n1 & 0\\\\\n1 & 0\\\\\n0 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny=\\begin{bmatrix}\n5\\\\\n4\\\\\n3\\\\\n2\n\\end{bmatrix}.\n$$\nUsing only definitions and linear-algebraic facts about orthogonal projectors and the orthogonal-triangular (QR) decomposition, proceed as follows:\n\n1) Compute a QR decomposition of $Z$, write the associated orthogonal projector onto the column space of $Z$ as $P_Z$, and construct $P_Z$ explicitly from the $Q$ factor via $P_Z = QQ^{\\top}$.\n\n2) Using the instrumental variable principle that instruments enforce orthogonality of the residual to the instrument space, form the normal equations in the instrument space and use the projector $P_Z$ to obtain an explicit estimator $\\hat{\\theta}_{\\mathrm{IV}}$ for this dataset.\n\n3) Separately, perform two-stage least squares: first project the regressors into the instrument space to obtain fitted regressors, then regress $y$ onto these fitted regressors by ordinary least squares. Derive the resulting estimator $\\hat{\\theta}_{\\mathrm{2SLS}}$.\n\n4) Prove, using only properties of orthogonal projectors and the orthonormality of $Q$, that for full-column-rank $Z$ one has $\\hat{\\theta}_{\\mathrm{IV}}=\\hat{\\theta}_{\\mathrm{2SLS}}$, and verify numerically that they coincide for the given data.\n\nReport the final estimator vector $\\hat{\\theta}_{\\mathrm{IV}}$ as a single $1\\times 2$ row vector. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary data for its resolution. It constitutes a standard exercise in linear system identification theory. We may therefore proceed with the solution, which is executed in four parts as requested.\n\nThe given data are the regressor matrix $\\Phi$, the instrument matrix $Z$, and the output vector $y$:\n$$\n\\Phi=\\begin{bmatrix}\n1 & 2\\\\\n2 & 1\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\nZ=\\begin{bmatrix}\n1 & 0\\\\\n1 & 0\\\\\n0 & 1\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny=\\begin{bmatrix}\n5\\\\\n4\\\\\n3\\\\\n2\n\\end{bmatrix}\n$$\nThe dimensions are $n=4$, $p=2$, and $m=2$.\n\nFirst, we compute an orthogonal-triangular (QR) decomposition of the instrument matrix $Z$. The columns of $Z$, denoted $z_1$ and $z_2$, are:\n$$\nz_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad z_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\nWe observe that these columns are already orthogonal, as their inner product is $z_1^{\\top}z_2 = 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 1 = 0$. The Gram-Schmidt procedure thus simplifies to normalization. The squared norms are $\\|z_1\\|^2 = 1^2+1^2 = 2$ and $\\|z_2\\|^2 = 1^2+1^2 = 2$.\nThe orthonormal basis for the column space of $Z$ is given by the columns of the matrix $Q$:\n$$\nq_1 = \\frac{z_1}{\\|z_1\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad q_2 = \\frac{z_2}{\\|z_2\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\n$$\nQ = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix}\n$$\nThe upper triangular matrix $R$ is found via $R = Q^{\\top}Z$:\n$$\nR = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{2} \\end{bmatrix}\n$$\nThe orthogonal projector $P_Z$ onto the column space of $Z$ is constructed from $Q$ as $P_Z = QQ^{\\top}$:\n$$\nP_Z = \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix}\n$$\n\nSecond, we derive the instrumental variable (IV) estimator $\\hat{\\theta}_{\\mathrm{IV}}$. The IV principle imposes orthogonality of the residual, $y-\\Phi\\hat{\\theta}$, to the space spanned by the instruments. This is expressed as $Z^{\\top}(y-\\Phi\\hat{\\theta}_{\\mathrm{IV}}) = 0$. This condition is equivalent to requiring the projection of the residual onto the column space of $Z$ to be zero: $P_Z(y-\\Phi\\hat{\\theta}_{\\mathrm{IV}}) = 0$. This leads to the system of equations $P_Z \\Phi \\hat{\\theta}_{\\mathrm{IV}} = P_Z y$. The least-squares solution for $\\hat{\\theta}_{\\mathrm{IV}}$ is $\\hat{\\theta}_{\\mathrm{IV}} = ((P_Z\\Phi)^{\\top} P_Z\\Phi)^{-1} (P_Z\\Phi)^{\\top} P_Z y$. Since $P_Z$ is an orthogonal projector, it is symmetric ($P_Z^{\\top}=P_Z$) and idempotent ($P_Z^2=P_Z$). The expression simplifies to:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (\\Phi^{\\top}P_Z\\Phi)^{-1} \\Phi^{\\top}P_Z y\n$$\nWe compute the required matrices:\n$$\nP_Z \\Phi = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2\\\\ 2 & 1\\\\ 1 & 1\\\\ 0 & 1 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\n$$\n$$\nP_Z y = \\frac{1}{2}\\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 2 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 9 \\\\ 9 \\\\ 5 \\\\ 5 \\end{bmatrix}\n$$\nNext, we form the products with $\\Phi^{\\top}$:\n$$\n\\Phi^{\\top}P_Z \\Phi = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 1 & 1 & 1 \\end{bmatrix} \\left(\\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 3+6+1 & 3+6+2 \\\\ 6+3+1+1 & 6+3+2+2 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\n$$\n$$\n\\Phi^{\\top}P_Z y = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 1 & 1 & 1 \\end{bmatrix} \\left(\\frac{1}{2}\\begin{bmatrix} 9 \\\\ 9 \\\\ 5 \\\\ 5 \\end{bmatrix}\\right) = \\frac{1}{2}\\begin{bmatrix} 9+18+5 \\\\ 18+9+5+5 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nNow, we solve for $\\hat{\\theta}_{\\mathrm{IV}}$:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\left(\\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\\right)^{-1} \\left(\\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\\right) = \\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}^{-1} \\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nThe determinant of the matrix is $10 \\cdot 13 - 11 \\cdot 11 = 130 - 121 = 9$. Its inverse is $\\frac{1}{9}\\begin{bmatrix} 13 & -11 \\\\ -11 & 10 \\end{bmatrix}$.\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = \\frac{1}{9}\\begin{bmatrix} 13 & -11 \\\\ -11 & 10 \\end{bmatrix} \\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 13 \\cdot 32 - 11 \\cdot 37 \\\\ -11 \\cdot 32 + 10 \\cdot 37 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 416 - 407 \\\\ -352 + 370 \\end{bmatrix} = \\frac{1}{9}\\begin{bmatrix} 9 \\\\ 18 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n$$\n\nThird, we derive the Two-Stage Least Squares (2SLS) estimator $\\hat{\\theta}_{\\mathrm{2SLS}}$.\nIn Stage 1, the possibly endogenous regressors $\\Phi$ are projected onto the space of instruments. The fitted regressors are $\\hat{\\Phi} = P_Z \\Phi$. We have already computed this matrix:\n$$\n\\hat{\\Phi} = \\frac{1}{2}\\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\\\ 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\n$$\nIn Stage 2, the output $y$ is regressed onto the fitted regressors $\\hat{\\Phi}$ using ordinary least squares (OLS):\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = (\\hat{\\Phi}^{\\top}\\hat{\\Phi})^{-1}\\hat{\\Phi}^{\\top}y\n$$\nWe compute the components:\n$$\n\\hat{\\Phi}^{\\top}\\hat{\\Phi} = (P_Z\\Phi)^{\\top}(P_Z\\Phi) = \\Phi^{\\top}P_Z^{\\top}P_Z\\Phi = \\Phi^{\\top}P_Z\\Phi = \\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\n$$\n$$\n\\hat{\\Phi}^{\\top}y = (P_Z\\Phi)^{\\top}y = \\Phi^{\\top}P_Z^{\\top}y = \\Phi^{\\top}P_Z y = \\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\n$$\nThe expressions for $\\hat{\\Phi}^{\\top}\\hat{\\Phi}$ and $\\hat{\\Phi}^{\\top}y$ are identical to those computed for the IV estimator. Consequently, the resulting estimator must be the same:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = \\left(\\frac{1}{2}\\begin{bmatrix} 10 & 11 \\\\ 11 & 13 \\end{bmatrix}\\right)^{-1} \\left(\\frac{1}{2}\\begin{bmatrix} 32 \\\\ 37 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n$$\n\nFourth, we prove the general equivalence $\\hat{\\theta}_{\\mathrm{IV}}=\\hat{\\theta}_{\\mathrm{2SLS}}$ and verify it numerically.\nThe 2SLS estimator is defined as $\\hat{\\theta}_{\\mathrm{2SLS}} = (\\hat{\\Phi}^{\\top}\\hat{\\Phi})^{-1}\\hat{\\Phi}^{\\top}y$, with $\\hat{\\Phi}=P_Z\\Phi$. Using $P_Z=QQ^{\\top}$ and the property $Q^{\\top}Q=I_m$:\n$$\n\\hat{\\Phi}^{\\top}\\hat{\\Phi} = (QQ^{\\top}\\Phi)^{\\top}(QQ^{\\top}\\Phi) = \\Phi^{\\top}QQ^{\\top}QQ^{\\top}\\Phi = \\Phi^{\\top}Q(Q^{\\top}Q)Q^{\\top}\\Phi = \\Phi^{\\top}Q I_m Q^{\\top}\\Phi = (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)\n$$\nAnd\n$$\n\\hat{\\Phi}^{\\top}y = (QQ^{\\top}\\Phi)^{\\top}y = \\Phi^{\\top}QQ^{\\top}y = (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y)\n$$\nSubstituting these into the 2SLS formula gives:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = [(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)]^{-1} (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y)\n$$\nThis is the OLS solution for $\\theta$ in the transformed model $Q^{\\top}y = (Q^{\\top}\\Phi)\\theta + Q^{\\top}v$.\n\nThe IV estimator $\\hat{\\theta}_{\\mathrm{IV}}$ is rooted in the orthogonality condition $Z^{\\top}(y-\\Phi\\hat{\\theta})=0$. Since the columns of $Q$ form an orthonormal basis for the column space of $Z$, this is equivalent to $Q^{\\top}(y-\\Phi\\hat{\\theta})=0$, which yields the set of equations $Q^{\\top}\\Phi\\hat{\\theta}_{\\mathrm{IV}}=Q^{\\top}y$.\n\nIn the just-identified case, where the number of instruments $m$ equals the number of parameters $p$, the matrix $Q^{\\top}\\Phi$ is square ($p \\times p$) and, assuming instrument relevance, invertible. The unique solution is:\n$$\n\\hat{\\theta}_{\\mathrm{IV}} = (Q^{\\top}\\Phi)^{-1}Q^{\\top}y\n$$\nIn this same case ($m=p$), the 2SLS expression simplifies because $(Q^{\\top}\\Phi)$ is square and invertible:\n$$\n\\hat{\\theta}_{\\mathrm{2SLS}} = [(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}\\Phi)]^{-1} (Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y) = (Q^{\\top}\\Phi)^{-1}((Q^{\\top}\\Phi)^{\\top})^{-1}(Q^{\\top}\\Phi)^{\\top}(Q^{\\top}y) = (Q^{\\top}\\Phi)^{-1}Q^{\\top}y\n$$\nThus, we have proven that $\\hat{\\theta}_{\\mathrm{IV}} = \\hat{\\theta}_{\\mathrm{2SLS}}$ when $m=p$.\nThe provided dataset has $m=p=2$, so this equivalence holds. Our numerical calculations confirm this, as we found $\\hat{\\theta}_{\\mathrm{IV}} = \\hat{\\theta}_{\\mathrm{2SLS}} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nThe final estimator vector is reported as a $1 \\times 2$ row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 \\end{pmatrix}}\n$$", "id": "2878432"}, {"introduction": "Many real-world systems are best described by Output-Error (OE) models, which are nonlinear in the parameters and thus cannot be estimated with a simple, one-step IV procedure. This challenge introduces an iterative approach, the Simplified Refined Instrumental Variable (SRIV) algorithm, which cleverly extends the IV concept to handle these more complex models. Implementing a single iteration of the SRIV algorithm [@problem_id:2878451] provides direct experience with how instruments can be adaptively constructed from model predictions, a powerful technique essential for identifying a broad and important class of dynamic systems.", "problem": "You are asked to implement a single iteration of the Simplified Refined Instrumental Variable (SRIV) algorithm for a discrete-time Output Error (OE) model using the instrumental variable (IV) principle. The goal is to compute, for provided datasets and an initial model, the instrumental vector based on the model-predicted output and to update the parameter vector by solving the IV normal equations derived from the moment condition.\n\nThe discrete-time OE model considered is\n$$\ny(t) \\;=\\; \\frac{B\\!\\left(q^{-1}\\right)}{F\\!\\left(q^{-1}\\right)}\\,u(t) \\;+\\; v(t),\n$$\nwhere $q^{-1}$ is the unit delay operator, $u(t)$ is the input, $y(t)$ is the measured output, and $v(t)$ is a disturbance added at the output. The rational model has a monic denominator\n$$\nF\\!\\left(q^{-1}\\right) \\;=\\; 1 + a_1 q^{-1} + a_2 q^{-2},\n$$\nand a numerator with an input delay $n_k = 1$ and order $n_b = 2$,\n$$\nB\\!\\left(q^{-1}\\right) \\;=\\; b_0 + b_1 q^{-1}.\n$$\nUnder these conventions, the corresponding noise-free dynamics in the time domain are expressed by the recursion\n$$\ny_0(t) \\;=\\; -a_1\\,y_0(t-1) \\;-\\; a_2\\,y_0(t-2) \\;+\\; b_0\\,u(t-1) \\;+\\; b_1\\,u(t-2),\n$$\nwith initial conditions $y_0(0) = 0$ and $y_0(-1) = 0$. The measured output is $y(t) = y_0(t) + v(t)$.\n\nGiven an initial parameter vector\n$$\n\\hat{\\theta}^{(0)} \\;=\\; \\begin{bmatrix} a_1^{(0)} & a_2^{(0)} & b_0^{(0)} & b_1^{(0)} \\end{bmatrix}^\\top,\n$$\nthe SRIV procedure constructs a model-predicted output $\\hat{y}^{(0)}(t)$ by simulating the noise-free recursion using $\\hat{\\theta}^{(0)}$ and the measured input $u(t)$:\n$$\n\\hat{y}^{(0)}(t) \\;=\\; -a_1^{(0)}\\,\\hat{y}^{(0)}(t-1) \\;-\\; a_2^{(0)}\\,\\hat{y}^{(0)}(t-2) \\;+\\; b_0^{(0)}\\,u(t-1) \\;+\\; b_1^{(0)}\\,u(t-2),\n$$\nwith $\\hat{y}^{(0)}(0) = 0$ and $\\hat{y}^{(0)}(-1) = 0$.\n\nFor time indices $t$ with sufficient past data (here $t \\ge 3$ in one-based indexing), define the regression vector\n$$\n\\varphi(t) \\;=\\; \\begin{bmatrix} -y(t-1) & -y(t-2) & u(t-1) & u(t-2) \\end{bmatrix}^\\top,\n$$\nand construct the SRIV instrument by replacing the lagged outputs in the regression vector with their model-predicted counterparts,\n$$\nz^{(0)}(t) \\;=\\; \\begin{bmatrix} -\\hat{y}^{(0)}(t-1) & -\\hat{y}^{(0)}(t-2) & u(t-1) & u(t-2) \\end{bmatrix}^\\top.\n$$\nThe updated parameter vector is defined as the unique vector $\\hat{\\theta}^{(1)}$ that satisfies the instrumental variable orthogonality (moment) condition\n$$\n\\sum_{t=3}^{N} z^{(0)}(t)\\,\\Big( y(t) \\;-\\; \\varphi(t)^\\top \\hat{\\theta}^{(1)} \\Big) \\;=\\; 0,\n$$\nwhich yields a linear system in $\\hat{\\theta}^{(1)}$.\n\nYour program must implement exactly one SRIV iteration as follows:\n- Generate input $u(t)$ and disturbance $v(t)$ deterministically via a specified linear congruential generator (LCG).\n- Simulate the noise-free output $y_0(t)$ using the true parameters and initial conditions.\n- Form the measured output $y(t) = y_0(t) + v(t)$.\n- Using the provided initial parameter vector $\\hat{\\theta}^{(0)}$, simulate $\\hat{y}^{(0)}(t)$.\n- Build the regression vectors $\\varphi(t)$ and instruments $z^{(0)}(t)$ for all $t \\in \\{3,4,\\dots,N\\}$.\n- Compute the unique update $\\hat{\\theta}^{(1)}$ that satisfies the above moment condition.\n- For each test case, return the vector $\\hat{\\theta}^{(1)}$ rounded to six decimal places.\n\nFoundational definitions and modeling assumptions to use:\n- Linear regression form for the OE model obtained by multiplying the monic denominator and rearranging the recursion,\n- The instrumental variable condition that the instrument is uncorrelated with the disturbance but correlated with the regression vector,\n- The SRIV construction of instruments through the noise-free model-predicted output generated by the previous parameter estimate.\n\nData generation is deterministic and identical across implementations using the following linear congruential generator (LCG):\n- Modulus $m = 2147483647$,\n- Multiplier $a = 16807$,\n- Increment $c = 0$,\n- Recurrence $X_{k+1} = (a X_k + c) \\bmod m$,\n- Normalized output $r_k = X_k / m \\in (0,1)$.\nTo obtain a zero-mean sequence in $[-1,1]$, map via $w_k = 2 r_k - 1$. For an input length $N$, use the first $N$ values of $w_k$ to form $u(t) = w_t$ for $t = 1,\\dots,N$. For the output disturbance, use an independent LCG with its own seed to generate $w'_t$, and then set $v(t) = \\sigma\\,w'_t$, where $\\sigma$ is provided. All initial conditions for simulated outputs are zero:\n$$\ny_0(0) = y_0(-1) = 0, \\qquad \\hat{y}^{(0)}(0) = 0, \\qquad \\hat{y}^{(0)}(-1) = 0.\n$$\n\nTest suite (use the same true system and delay in all cases):\n- True system and structure:\n  - Denominator order $n_f = 2$, numerator order $n_b = 2$, input delay $n_k = 1$,\n  - True parameters: $a_1^\\star = -1.2$, $a_2^\\star = 0.35$, $b_0^\\star = 0.5$, $b_1^\\star = 0.1$.\n- Case $1$ (happy path): $N = 200$, $\\sigma = 0.05$, input seed $= 137$, noise seed $= 2468$, initial $\\hat{\\theta}^{(0)} = \\begin{bmatrix} -0.9 & 0.2 & 0.3 & 0.0 \\end{bmatrix}^\\top$.\n- Case $2$ (noiseless boundary): $N = 200$, $\\sigma = 0$, input seed $= 137$, noise seed $= 2468$ (unused), initial $\\hat{\\theta}^{(0)} = \\begin{bmatrix} -0.5 & 0.1 & 0.05 & 0.05 \\end{bmatrix}^\\top$.\n- Case $3$ (short/noisy data): $N = 50$, $\\sigma = 0.2$, input seed $= 999$, noise seed $= 2021$, initial $\\hat{\\theta}^{(0)} = \\begin{bmatrix} -1.0 & 0.25 & 0.4 & 0.05 \\end{bmatrix}^\\top$.\n\nImplementation requirements:\n- Use one-based logical indexing in the description but implement in any suitable programming indices; ensure that the vectors $\\varphi(t)$ and $z^{(0)}(t)$ are defined only for $t \\ge 3$.\n- For each case, compute the unique solution to the linear system implied by the moment condition. If you form\n$$\nS \\;=\\; \\sum_{t=3}^{N} z^{(0)}(t)\\,\\varphi(t)^\\top \\quad\\text{and}\\quad s \\;=\\; \\sum_{t=3}^{N} z^{(0)}(t)\\,y(t),\n$$\nthen solve $S\\,\\hat{\\theta}^{(1)} = s$ for $\\hat{\\theta}^{(1)}$.\n- Round each component of $\\hat{\\theta}^{(1)}$ to six decimal places before output aggregation.\n\nFinal output format:\n- Your program should produce a single line of output containing all components of the three updated parameter vectors concatenated into a single list, in the order\n$$\n\\left[ a_1^{(1)},\\,a_2^{(1)},\\,b_0^{(1)},\\,b_1^{(1)} \\right]_{\\text{case }1} \\;\\;\\Vert\\;\\; \\left[ a_1^{(1)},\\,a_2^{(1)},\\,b_0^{(1)},\\,b_1^{(1)} \\right]_{\\text{case }2} \\;\\;\\Vert\\;\\; \\left[ a_1^{(1)},\\,a_2^{(1)},\\,b_0^{(1)},\\,b_1^{(1)} \\right]_{\\text{case }3},\n$$\nas a comma-separated list enclosed in square brackets with no spaces and each number rounded to six decimal places, e.g.,\n$$\n\\left[ x_1,x_2,\\dots,x_{12} \\right].\n$$", "solution": "The problem requires the implementation of a single iteration of the Simplified Refined Instrumental Variable (SRIV) algorithm to estimate the parameters of a discrete-time Output Error (OE) model. The solution is constructed by following the sequence of steps defined by the SRIV procedure: data generation, system simulation, instrument construction, and solving the instrumental variable normal equations.\n\nThe OE model is given by\n$$\ny(t) \\;=\\; \\frac{B(q^{-1})}{F(q^{-1})}u(t) + v(t)\n$$\nwhere the system dynamics are described by the rational transfer function with polynomials $F(q^{-1}) = 1 + a_1 q^{-1} + a_2 q^{-2}$ and $B(q^{-1}) = (b_0 + b_1 q^{-1})q^{-1}$, incorporating a unit delay $n_k=1$. The parameters to be estimated are collected in the vector $\\theta = \\begin{bmatrix} a_1 & a_2 & b_0 & b_1 \\end{bmatrix}^\\top$.\n\nThe implementation proceeds as follows:\n\n1.  **Data Generation**: The input signal $u(t)$ and the noise precursor signal $w'(t)$ for $t=1, \\dots, N$ are deterministically generated using the specified Linear Congruential Generator (LCG). The LCG is defined by the recurrence relation $X_{k+1} = (a X_k) \\bmod m$ with multiplier $a=16807$ and modulus $m=2147483647$. The sequence of states $\\{X_k\\}$ is normalized to $(0,1)$ by division with $m$, and then mapped to $[-1,1]$ via the transformation $w_k = 2(X_k/m) - 1$. The input $u(t)$ is set to a sequence generated with a given `input_seed`, while the output disturbance is $v(t) = \\sigma w'(t)$, where $w'(t)$ is generated with an independent `noise_seed` and $\\sigma$ is the noise standard deviation. This deterministic generation ensures reproducibility.\n\n2.  **System Simulation**: The true noise-free output $y_0(t)$ is simulated using the true parameters $\\theta^\\star = \\begin{bmatrix} a_1^\\star & a_2^\\star & b_0^\\star & b_1^\\star \\end{bmatrix}^\\top = \\begin{bmatrix} -1.2 & 0.35 & 0.5 & 0.1 \\end{bmatrix}^\\top$. The simulation follows the time-domain recursion:\n    $$\n    y_0(t) = -a_1^\\star y_0(t-1) - a_2^\\star y_0(t-2) + b_0^\\star u(t-1) + b_1^\\star u(t-2)\n    $$\n    with initial conditions $y_0(t)=0$ for $t \\le 0$, and the assumption that $u(t)=0$ for $t \\le 0$. The measured output $y(t)$ is then formed by adding the generated disturbance: $y(t) = y_0(t) + v(t)$.\n\n3.  **Instrumental Variable Construction**: The core of the SRIV method is the construction of the instrumental variable vector $z^{(0)}(t)$. This is achieved by first simulating a model-predicted output, $\\hat{y}^{(0)}(t)$, using the initial parameter estimate $\\hat{\\theta}^{(0)} = \\begin{bmatrix} a_1^{(0)} & a_2^{(0)} & b_0^{(0)} & b_1^{(0)} \\end{bmatrix}^\\top$. This simulation uses the same recursion but with the initial parameters:\n    $$\n    \\hat{y}^{(0)}(t) = -a_1^{(0)} \\hat{y}^{(0)}(t-1) - a_2^{(0)} \\hat{y}^{(0)}(t-2) + b_0^{(0)} u(t-1) + b_1^{(0)} u(t-2)\n    $$\n    with initial conditions $\\hat{y}^{(0)}(t) = 0$ for $t \\le 0$. The instrument vector $z^{(0)}(t)$ is then constructed by replacing the noisy output measurements in the regression vector $\\varphi(t)$ with their noise-free, model-predicted counterparts:\n    $$\n    \\varphi(t) = \\begin{bmatrix} -y(t-1) & -y(t-2) & u(t-1) & u(t-2) \\end{bmatrix}^\\top\n    $$\n    $$\n    z^{(0)}(t) = \\begin{bmatrix} -\\hat{y}^{(0)}(t-1) & -\\hat{y}^{(0)}(t-2) & u(t-1) & u(t-2) \\end{bmatrix}^\\top\n    $$\n    This instrument $z^{(0)}(t)$ is, by construction, strongly correlated with the noise-free regressor but uncorrelated with the output noise $v(t)$, which is the defining property of a good instrument.\n\n4.  **Parameter Update**: The updated parameter vector $\\hat{\\theta}^{(1)}$ is found by solving the normal equations derived from the IV moment condition, which states that the prediction error must be orthogonal to the instrument.\n    $$\n    \\sum_{t=3}^{N} z^{(0)}(t) \\left( y(t) - \\varphi(t)^\\top \\hat{\\theta}^{(1)} \\right) = 0\n    $$\n    Rearranging this gives the linear system of equations $S\\hat{\\theta}^{(1)} = s$, where the matrix $S$ and vector $s$ are formed by summing over the available data points, $t=3, \\dots, N$:\n    $$\n    S = \\sum_{t=3}^{N} z^{(0)}(t) \\varphi(t)^\\top \\quad \\text{and} \\quad s = \\sum_{t=3}^{N} z^{(0)}(t) y(t)\n    $$\n    This $4 \\times 4$ system is solved for $\\hat{\\theta}^{(1)}$. The existence of a unique solution relies on the matrix $S$ being invertible, which is generally true if the input signal $u(t)$ is persistently exciting.\n\nThis entire procedure is applied to each of the three test cases specified, and the resulting parameter estimates $\\hat{\\theta}^{(1)}$ are collected. The components of each vector are rounded to six decimal places for the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a single iteration of the Simplified Refined Instrumental Variable \n    (SRIV) algorithm for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            'N': 200, 'sigma': 0.05, 'input_seed': 137, 'noise_seed': 2468,\n            'theta0': np.array([-0.9, 0.2, 0.3, 0.0])\n        },\n        {\n            'N': 200, 'sigma': 0.0, 'input_seed': 137, 'noise_seed': 2468,\n            'theta0': np.array([-0.5, 0.1, 0.05, 0.05])\n        },\n        {\n            'N': 50, 'sigma': 0.2, 'input_seed': 999, 'noise_seed': 2021,\n            'theta0': np.array([-1.0, 0.25, 0.4, 0.05])\n        }\n    ]\n\n    theta_true = np.array([-1.2, 0.35, 0.5, 0.1])\n    \n    # LCG parameters\n    m = 2147483647\n    a = 16807\n    c = 0\n    \n    def lcg_sequence(seed, N):\n        \"\"\"Generates a sequence of N pseudo-random numbers using LCG.\"\"\"\n        x = seed\n        numbers = np.zeros(N)\n        for i in range(N):\n            x = (a * x + c) % m\n            numbers[i] = x\n        return numbers\n\n    def generate_signal(seed, N):\n        \"\"\"Generates a zero-mean signal in [-1, 1] from an LCG sequence.\"\"\"\n        raw_seq = lcg_sequence(seed, N)\n        normalized_seq = raw_seq / m\n        return 2 * normalized_seq - 1\n\n    def simulate_oe_output(params, u_signal, N):\n        \"\"\"Simulates the noise-free output of the OE model.\"\"\"\n        a1, a2, b0, b1 = params\n        y_sim = np.zeros(N)\n\n        # Helper functions to get past values, assuming 0 for non-positive time indices\n        def get_y(arr, t_idx):\n            if t_idx <= 0:\n                return 0.0\n            return arr[t_idx - 1]\n\n        def get_u(arr, t_idx):\n            if t_idx <= 0:\n                return 0.0\n            return arr[t_idx - 1]\n\n        # Simulation loop for time t = 1, ..., N\n        for t in range(1, N + 1):\n            y_sim[t - 1] = -a1 * get_y(y_sim, t - 1) - a2 * get_y(y_sim, t - 2) + \\\n                            b0 * get_u(u_signal, t - 1) + b1 * get_u(u_signal, t - 2)\n        return y_sim\n\n    all_results = []\n    for case in test_cases:\n        N = case['N']\n        sigma = case['sigma']\n        input_seed = case['input_seed']\n        noise_seed = case['noise_seed']\n        theta0 = case['theta0']\n\n        # 1. Data Generation\n        u = generate_signal(input_seed, N)\n        w_prime = generate_signal(noise_seed, N)\n        v = sigma * w_prime\n\n        # 2. System Simulation (True Output)\n        y0_true = simulate_oe_output(theta_true, u, N)\n        y = y0_true + v\n\n        # 3. Instrument Generation (from initial parameter guess)\n        y_hat0 = simulate_oe_output(theta0, u, N)\n\n        # 4. Form matrices S and s for the normal equations\n        S = np.zeros((4, 4))\n        s = np.zeros(4)\n\n        # Summation from t=3 to N\n        for t in range(3, N + 1):\n            k = t - 1  # 0-based index for time t\n            \n            # y[k-1] corresponds to y(t-1), u[k-1] to u(t-1) etc.\n            phi_t = np.array([-y[k - 1], -y[k - 2], u[k - 1], u[k - 2]])\n            z_t = np.array([-y_hat0[k - 1], -y_hat0[k - 2], u[k - 1], u[k - 2]])\n            \n            S += np.outer(z_t, phi_t)\n            s += z_t * y[k]\n            \n        # 5. Solve for the updated parameter vector theta1\n        theta1 = np.linalg.solve(S, s)\n        \n        # Round and collect results\n        all_results.extend(np.round(theta1, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2878451"}]}