{"hands_on_practices": [{"introduction": "Many advanced neural state-space models are inspired by continuous-time systems that evolve smoothly over time. This practice delves into the fundamental process of translating a continuous-time linear model into its discrete-time equivalent, a crucial step for implementation on digital hardware. By applying the Zero-Order Hold (ZOH) assumption, you will derive and implement the discretization process from first principles, gaining insight into the origin of the state-space matrices $A_d$ and $B_d$ that govern the recurrent dynamics of sequence models. [@problem_id:2886125]", "problem": "You are given continuous-time linear time-invariant state-space models as the linear backbone of a neural state-space modeling pipeline. The backbone is specified by matrices $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{p \\times n}$, and $D \\in \\mathbb{R}^{p \\times m}$, together with a sampling period $\\Delta \\in \\mathbb{R}_{0}$. Assuming a Zero-Order Hold (ZOH) on the input, derive from first principles a correct discrete-time state update and output equation that map sampling instants $k \\in \\mathbb{Z}_{\\ge 0}$ to $k+1$. Use only the fundamental solution of linear time-invariant dynamics and the ZOH assumption as your starting point. Then, implement a program that computes the discrete-time matrices $(A_d,B_d,C_d,D_d)$ and simulates the discrete-time output sequence for a given piecewise constant input.\n\nThe continuous-time model is\n$x'(t) = A x(t) + B u(t)$ and $y(t) = C x(t) + D u(t)$.\nUnder the Zero-Order Hold (ZOH) assumption, the input $u(t)$ is held constant over each sampling interval $[k\\Delta, (k+1)\\Delta)$; denote that constant value by $u_k \\in \\mathbb{R}^m$. The discrete-time model is of the form\n$x_{k+1} = A_d x_k + B_d u_k$ and $y_k = C_d x_k + D_d u_k$,\nfor $k \\in \\mathbb{Z}_{\\ge 0}$.\n\nYour program must:\n- Compute $(A_d,B_d,C_d,D_d)$ implied by the ZOH assumption, without using any shortcut formulas stated a priori in this problem statement, but following from the fundamental solution of linear time-invariant systems and the ZOH input model.\n- Simulate the output sequence $\\{y_k\\}_{k=0}^{N-1}$ for each test case, given an initial state $x_0$ and a piecewise constant input schedule specified over $N$ steps.\n- For each test case, return a single real number equal to the sum of squares of the output sequence, that is $\\sum_{k=0}^{N-1} \\lVert y_k \\rVert_2^2$, expressed as a real scalar (no physical units are involved).\n- Round each returned scalar to exactly $6$ decimal places.\n\nInput schedules are given as a list of segments $(k_{\\mathrm{start}}, k_{\\mathrm{end}}, v)$, where $k_{\\mathrm{start}}$ and $k_{\\mathrm{end}}$ are integers with $0 \\le k_{\\mathrm{start}} \\le k_{\\mathrm{end}} \\le N-1$, and $v \\in \\mathbb{R}^m$ is the constant input applied for all integer $k$ in the inclusive range $\\{k_{\\mathrm{start}}, \\dots, k_{\\mathrm{end}}\\}$.\n\nTest Suite:\n- Test Case $1$ (happy path, second-order, strictly proper):\n  - $A = \\begin{bmatrix} 0  1 \\\\ -2  -0.5 \\end{bmatrix}$, $B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $C = \\begin{bmatrix} 1  0 \\end{bmatrix}$, $D = \\begin{bmatrix} 0 \\end{bmatrix}$.\n  - $\\Delta = 0.1$.\n  - $x_0 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n  - $N = 50$.\n  - Input segments: $(0, 19, [1.0])$, $(20, 49, [0.0])$.\n- Test Case $2$ (boundary case with $A$ equal to zero and nonzero direct feedthrough):\n  - $A = \\begin{bmatrix} 0 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 \\end{bmatrix}$, $D = \\begin{bmatrix} 0.5 \\end{bmatrix}$.\n  - $\\Delta = 0.25$.\n  - $x_0 = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n  - $N = 8$.\n  - Input segments: $(0, 3, [2.0])$, $(4, 7, [-2.0])$.\n- Test Case $3$ (oscillatory marginal dynamics):\n  - $A = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$, $B = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$, $C = \\begin{bmatrix} 0  1 \\end{bmatrix}$, $D = \\begin{bmatrix} 0 \\end{bmatrix}$.\n  - $\\Delta = 0.2$.\n  - $x_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$.\n  - $N = 20$.\n  - Input segments: $(0, 9, [0.3])$, $(10, 19, [0.0])$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with each number rounded to exactly six decimal places and without any additional text. For example: \"[1.234000,5.678900,0.000123]\".", "solution": "The problem requires the derivation and implementation of a method to discretize a continuous-time linear time-invariant (LTI) state-space model under the Zero-Order Hold (ZOH) assumption.\n\nThe continuous-time model is given by the state and output equations:\n$$\nx'(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nwhere $x(t) \\in \\mathbb{R}^n$ is the state vector, $u(t) \\in \\mathbb{R}^m$ is the input vector, and $y(t) \\in \\mathbb{R}^p$ is the output vector. The matrices $A$, $B$, $C$, and $D$ have compatible dimensions.\n\nThe objective is to find the equivalent discrete-time model sampled with a period $\\Delta  0$, of the form:\n$$\nx_{k+1} = A_d x_k + B_d u_k\n$$\n$$\ny_k = C_d x_k + D_d u_k\n$$\nwhere $x_k = x(k\\Delta)$, $y_k = y(k\\Delta)$, and $u_k$ is the constant input value over the interval $[k\\Delta, (k+1)\\Delta)$.\n\nThe derivation proceeds from the fundamental solution of the continuous-time LTI state equation, which is given by the variation of constants formula. For an initial time $t_0$ and a final time $t$, the state is:\n$$\nx(t) = e^{A(t-t_0)} x(t_0) + \\int_{t_0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau\n$$\n\nWe apply this formula over a single sampling interval, from $t_0 = k\\Delta$ to $t = (k+1)\\Delta$. The state at the beginning of the interval is $x(k\\Delta) = x_k$. The state at the end is $x((k+1)\\Delta) = x_{k+1}$. The ZOH assumption states that the input $u(\\tau)$ is constant for $\\tau \\in [k\\Delta, (k+1)\\Delta)$, with the value $u_k$.\n\nSubstituting these into the solution formula:\n$$\nx_{k+1} = e^{A((k+1)\\Delta - k\\Delta)} x_k + \\int_{k\\Delta}^{(k+1)\\Delta} e^{A((k+1)\\Delta - \\tau)} B u_k d\\tau\n$$\n\nWe can simplify this expression. The first term gives the state transition matrix for the discrete system:\n$$\ne^{A\\Delta} x_k\n$$\nBy inspection, we identify the discrete-time state matrix $A_d$:\n$$\nA_d = e^{A\\Delta}\n$$\nThe second term involves the integral. Since $u_k$ is a constant vector over the integration interval, it can be factored out of the integral:\n$$\n\\left( \\int_{k\\Delta}^{(k+1)\\Delta} e^{A((k+1)\\Delta - \\tau)} d\\tau \\right) B u_k\n$$\nTo evaluate the integral, we perform a change of variables. Let $\\sigma = (k+1)\\Delta - \\tau$. Then $d\\sigma = -d\\tau$. The integration limits change from $\\tau=k\\Delta \\rightarrow \\sigma=\\Delta$ and $\\tau=(k+1)\\Delta \\rightarrow \\sigma=0$.\n$$\n\\int_{\\Delta}^{0} e^{A\\sigma} (-d\\sigma) = \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma\n$$\nThus, the second term is $\\left(\\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B u_k$.\nBy comparing with the discrete-time model form, we identify the discrete-time input matrix $B_d$:\n$$\nB_d = \\left( \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B\n$$\nThe complete discrete-time state update equation is therefore:\n$$\nx_{k+1} = (e^{A\\Delta}) x_k + \\left( \\int_{0}^{\\Delta} e^{A\\sigma} d\\sigma \\right) B u_k\n$$\nWhile the integral can be computed as $A^{-1}(e^{A\\Delta}-I)$ if $A$ is invertible, a more general and numerically robust method is required for the implementation. This method, known as the matrix exponential method or van Loan's method, computes $A_d$ and $B_d$ simultaneously. Consider the augmented matrix $M$ of size $(n+m) \\times (n+m)$:\n$$\nM = \\begin{bmatrix} A  B \\\\ 0  0 \\end{bmatrix}\n$$\nThe matrix exponential of $M\\Delta$ can be expressed using a block matrix formulation. The series expansion of the exponential shows that:\n$$\ne^{M\\Delta} = \\begin{bmatrix} e^{A\\Delta}  \\left(\\int_0^\\Delta e^{A\\sigma} d\\sigma\\right) B \\\\ 0  I_m \\end{bmatrix} = \\begin{bmatrix} A_d  B_d \\\\ 0  I_m \\end{bmatrix}\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix. This allows for the computation of both $A_d$ and $B_d$ using a single matrix exponential calculation, which is robust even for singular $A$.\n\nNext, we derive the discrete-time output matrices, $C_d$ and $D_d$. The discrete output $y_k$ is the sampled value of the continuous output at time $t=k\\Delta$:\n$$\ny_k = y(k\\Delta) = C x(k\\Delta) + D u(k\\Delta)\n$$\nUsing the definitions $x_k = x(k\\Delta)$ and the ZOH property $u(k\\Delta) = u_k$, we have:\n$$\ny_k = C x_k + D u_k\n$$\nComparing this to the generic form $y_k = C_d x_k + D_d u_k$, we directly identify:\n$$\nC_d = C\n$$\n$$\nD_d = D\n$$\nThis completes the derivation. The implementation will construct the augmented matrix $M$, compute its exponential to find $A_d$ and $B_d$, and then use all four discrete matrices $(A_d, B_d, C_d, D_d)$ to simulate the system response over $N$ steps. For each step $k$ from $0$ to $N-1$, the output $y_k$ is computed, and the sum of its squared Euclidean norms, $\\sum_{k=0}^{N-1} \\lVert y_k \\rVert_2^2$, is accumulated.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the problem by discretizing continuous-time state-space models\n    and simulating their response to compute a metric.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[0.0, 1.0], [-2.0, -0.5]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"C\": np.array([[1.0, 0.0]]),\n            \"D\": np.array([[0.0]]),\n            \"Delta\": 0.1,\n            \"x0\": np.array([1.0, 0.0]),\n            \"N\": 50,\n            \"input_segments\": [(0, 19, [1.0]), (20, 49, [0.0])],\n        },\n        {\n            \"A\": np.array([[0.0]]),\n            \"B\": np.array([[1.0]]),\n            \"C\": np.array([[1.0]]),\n            \"D\": np.array([[0.5]]),\n            \"Delta\": 0.25,\n            \"x0\": np.array([0.0]),\n            \"N\": 8,\n            \"input_segments\": [(0, 3, [2.0]), (4, 7, [-2.0])],\n        },\n        {\n            \"A\": np.array([[0.0, -1.0], [1.0, 0.0]]),\n            \"B\": np.array([[0.0], [0.5]]),\n            \"C\": np.array([[0.0, 1.0]]),\n            \"D\": np.array([[0.0]]),\n            \"Delta\": 0.2,\n            \"x0\": np.array([0.0, 1.0]),\n            \"N\": 20,\n            \"input_segments\": [(0, 9, [0.3]), (10, 19, [0.0])],\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, B, C, D = case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"]\n        Delta, x0, N = case[\"Delta\"], case[\"x0\"], case[\"N\"]\n        input_segments = case[\"input_segments\"]\n\n        n, m = A.shape[1], B.shape[1]\n\n        # Construct the augmented matrix M for ZOH discretization.\n        M = np.zeros((n + m, n + m))\n        M[:n, :n] = A\n        M[:n, n:] = B\n        \n        # Compute the matrix exponential.\n        E = expm(M * Delta)\n        \n        # Extract discrete-time matrices Ad and Bd.\n        Ad = E[:n, :n]\n        Bd = E[:n, n:]\n        \n        # Cd and Dd are the same as C and D for ZOH sampled at t_k.\n        Cd = C\n        Dd = D\n\n        # Construct the full input sequence u_k for k=0...N-1.\n        u_sequence = np.zeros((N, m))\n        for k_start, k_end, v in input_segments:\n            for k in range(k_start, k_end + 1):\n                u_sequence[k, :] = v\n\n        # Simulate the discrete-time system.\n        x_current = x0.copy().reshape(-1, 1)\n        sum_of_squares = 0.0\n\n        for k in range(N):\n            u_k = u_sequence[k].reshape(-1, 1)\n\n            # Calculate output y_k = C_d * x_k + D_d * u_k\n            y_k = Cd @ x_current + Dd @ u_k\n\n            # Accumulate the sum of squares of the L2 norm of the output.\n            sum_of_squares += np.sum(y_k**2)\n\n            # Update state for next step: x_{k+1} = A_d * x_k + B_d * u_k\n            x_current = Ad @ x_current + Bd @ u_k\n            \n        results.append(sum_of_squares)\n\n    # Format the final output string as required.\n    output_str = \"[\" + \",\".join([f\"{r:.6f}\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2886125"}, {"introduction": "The power of many state-space models lies in their ability to model complex dependencies, a property best understood in the frequency domain. This practice bridges the time and frequency domains by tasking you with computing a system's frequency response directly from its state-space parameters $(A, B, C, D)$. By implementing a numerically robust algorithm, you will gain a practical tool for analyzing how a model responds to different frequencies, which is key to interpreting its behavior on sequential data. [@problem_id:2886136]", "problem": "You are given discrete-time Single-Input Single-Output (SISO) linear state-space models that arise as local linearizations of neural state-space models around an equilibrium. A neural state-space model consists of parametric maps $f_{\\theta}$ and $g_{\\theta}$ such that the state and output evolve as $x_{k+1} = f_{\\theta}(x_k,u_k)$ and $y_k = g_{\\theta}(x_k,u_k)$. Under a small-signal approximation near an equilibrium, the model linearizes to the discrete-time linear time-invariant state-space form $x_{k+1} = A x_k + B u_k$ and $y_k = C x_k + D u_k$, where $A$, $B$, $C$, and $D$ are Jacobians evaluated at the equilibrium.\n\nStarting from the definitions of the $z$-transform for discrete-time signals and the Discrete-Time Fourier Transform (DTFT), and using only the linear time-invariant state-space equations $x_{k+1} = A x_k + B u_k$ and $y_k = C x_k + D u_k$ with zero initial conditions, derive a frequency-domain expression for the complex frequency response from input $u_k$ to output $y_k$ on the unit circle $z = e^{j\\omega}$, then obtain the magnitude response $\\lvert H(e^{j\\omega}) \\rvert$. Your algorithm must evaluate the response by solving the linear system associated with the resolvent of $A$ for each frequency in a grid, without resorting to any pre-derived shortcut formulas. If the associated matrix is numerically singular or ill-conditioned, treat the magnitude as $+\\infty$.\n\nAngle unit specification: all frequencies $\\omega$ are in radians per sample.\n\nNumerical singularity and ill-conditioning rule: for each frequency $\\omega$, form the matrix $M(\\omega) = e^{j\\omega} I - A$. If the condition number of $M(\\omega)$ in the $2$-norm exceeds $10^{12}$ or $M(\\omega)$ is singular in numerical linear algebra (solve failure), define $\\lvert H(e^{j\\omega}) \\rvert = +\\infty$ at that $\\omega$. Represent $+\\infty$ as the string $\\,\\text{inf}\\,$ in the final output.\n\nFor each test case below, compute the maximum of $\\lvert H(e^{j\\omega}) \\rvert$ over the specified frequency grid. Return one scalar per test case. When the maximum is finite, round it to six decimal places. When the maximum is infinite, print $\\,\\text{inf}\\,$.\n\nThe frequency grid common to all test cases is $\\Omega = [\\,0,\\ \\pi/4,\\ \\pi/2,\\ 3\\pi/4,\\ \\pi\\,]$.\n\nTest Suite:\n- Test Case $1$ (stable second-order oscillator-like):\n  - $A_1 = \\begin{bmatrix} 0.7  -0.4 \\\\ 0.4  0.7 \\end{bmatrix}$, $B_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $C_1 = \\begin{bmatrix} 1.0  0.0 \\end{bmatrix}$, $D_1 = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n- Test Case $2$ (nonzero direct term):\n  - $A_2 = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$, $B_2 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$, $C_2 = \\begin{bmatrix} 0.5  -0.5 \\end{bmatrix}$, $D_2 = \\begin{bmatrix} 0.3 \\end{bmatrix}$.\n- Test Case $3$ (marginal pole on the unit circle leading to a resonance at $\\omega = 0$):\n  - $A_3 = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.2 \\end{bmatrix}$, $B_3 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $C_3 = \\begin{bmatrix} 1.0  0.0 \\end{bmatrix}$, $D_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n- Test Case $4$ (zero input-to-state channel; identically zero response):\n  - $A_4 = \\begin{bmatrix} 0.9 \\end{bmatrix}$, $B_4 = \\begin{bmatrix} 0.0 \\end{bmatrix}$, $C_4 = \\begin{bmatrix} 1.0 \\end{bmatrix}$, $D_4 = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n\nImplementation requirements:\n- For each $\\omega \\in \\Omega$, set $z = e^{j\\omega}$, form $M(\\omega) = z I - A$, solve $M(\\omega) X(\\omega) = B$ for $X(\\omega)$ if $M(\\omega)$ is well-conditioned, and then compute the complex scalar $H(e^{j\\omega})$ from $X(\\omega)$ and the given matrices. Take the magnitude and aggregate across $\\Omega$ by the maximum.\n- Ill-conditioning threshold: use the condition number cutoff $10^{12}$ in the $2$-norm to decide if $\\lvert H(e^{j\\omega}) \\rvert$ is set to $+\\infty$ at that frequency.\n- Express the final outputs as one scalar per test case: the maximum magnitude over $\\Omega$. For finite maxima, round to six decimal places. For infinite maxima, print $\\,\\text{inf}\\,$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four results corresponding to the four test cases, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ follows the rounding and infinity conventions above.", "solution": "The problem requires the derivation and computation of the frequency response magnitude for several discrete-time linear time-invariant (LTI) state-space models. The derivation must originate from first principles, namely the definition of the $z$-transform, and the computation must adhere to a specific numerical procedure involving the solution of a linear system.\n\nLet us begin with the provided state-space representation of a discrete-time LTI system:\n$$x_{k+1} = A x_k + B u_k$$\n$$y_k = C x_k + D u_k$$\nHere, $k \\in \\mathbb{Z}$ is the time index, $x_k \\in \\mathbb{R}^n$ is the state vector, $u_k \\in \\mathbb{R}$ is the scalar input, and $y_k \\in \\mathbb{R}$ is the scalar output. The matrices $A, B, C, D$ are of compatible dimensions: $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times 1}$, $C \\in \\mathbb{R}^{1 \\times n}$, and $D \\in \\mathbb{R}^{1 \\times 1}$.\n\nThe $z$-transform of a discrete-time sequence $f_k$ is defined as $F(z) = \\mathcal{Z}\\{f_k\\} = \\sum_{k=0}^{\\infty} f_k z^{-k}$, where $z$ is a complex variable. A key property of the $z$-transform is the time-shifting property, which states that for a sequence $f_{k+1}$, its transform is $\\mathcal{Z}\\{f_{k+1}\\} = z F(z) - z f_0$.\n\nWe are given the condition of zero initial state, i.e., $x_0 = 0$. Applying the $z$-transform to the state and output equations, we obtain:\n1.  Transform of the state equation:\n    $\\mathcal{Z}\\{x_{k+1}\\} = \\mathcal{Z}\\{A x_k + B u_k\\}$\n    Using linearity and the time-shifting property with $x_0=0$:\n    $$z X(z) = A X(z) + B U(z)$$\n    where $X(z)$, $U(z)$, and $Y(z)$ are the $z$-transforms of the sequences $x_k$, $u_k$, and $y_k$, respectively.\n\n2.  Transform of the output equation:\n    $\\mathcal{Z}\\{y_k\\} = \\mathcal{Z}\\{C x_k + D u_k\\}$\n    Using linearity:\n    $$Y(z) = C X(z) + D U(z)$$\n\nOur goal is to find the transfer function $H(z) = Y(z)/U(z)$, which represents the input-output relationship in the $z$-domain. To do this, we first solve the transformed state equation for $X(z)$:\n$$z X(z) - A X(z) = B U(z)$$\n$$(zI - A) X(z) = B U(z)$$\nwhere $I$ is the $n \\times n$ identity matrix. Provided that the matrix $(zI - A)$ is invertible, we can write:\n$$X(z) = (zI - A)^{-1} B U(z)$$\nThe matrix $(zI - A)^{-1}$ is known as the resolvent of the matrix $A$.\n\nNow, we substitute this expression for $X(z)$ into the transformed output equation:\n$$Y(z) = C \\left( (zI - A)^{-1} B U(z) \\right) + D U(z)$$\nFactoring out $U(z)$:\n$$Y(z) = \\left[ C (zI - A)^{-1} B + D \\right] U(z)$$\nThe transfer function $H(z)$ is therefore:\n$$H(z) = \\frac{Y(z)}{U(z)} = C (zI - A)^{-1} B + D$$\n\nThe frequency response of the system is obtained by evaluating the transfer function $H(z)$ on the unit circle of the complex plane, where $z = e^{j\\omega}$. Here, $\\omega$ is the normalized angular frequency in radians per sample, with $\\omega \\in [-\\pi, \\pi]$.\n$$H(e^{j\\omega}) = C (e^{j\\omega}I - A)^{-1} B + D$$\nThe magnitude of the frequency response is $\\lvert H(e^{j\\omega}) \\rvert$.\n\nThe problem demands that we avoid computing the matrix inverse directly. Instead, we must solve a system of linear equations. Let us define a complex vector $X(\\omega)$ for each frequency $\\omega$. This vector is not the state transform $X(z)$, but a frequency-dependent quantity derived from it. Let $M(\\omega) = e^{j\\omega}I - A$. The term $(e^{j\\omega}I - A)^{-1} B$ in the expression for $H(e^{j\\omega})$ can be computed by solving the linear system:\n$$M(\\omega) X(\\omega) = B$$\nfor the vector $X(\\omega)$. Once $X(\\omega)$ is found, the frequency response is computed as:\n$$H(e^{j\\omega}) = C X(\\omega) + D$$\nThis approach is numerically preferable to direct inversion.\n\nThe stability and response characteristics of the system are determined by the eigenvalues of $A$, which are the poles of the system. If an eigenvalue $\\lambda_p$ of $A$ lies on the unit circle, i.e., $|\\lambda_p|=1$, then for some frequency $\\omega_p$, we have $\\lambda_p = e^{j\\omega_p}$. At this frequency, $z = e^{j\\omega_p} = \\lambda_p$, and the matrix $M(\\omega_p) = e^{j\\omega_p}I - A = \\lambda_p I - A$ becomes singular. This corresponds to a pole on the unit circle, leading to an infinite response magnitude (resonance).\n\nIn numerical computation, true singularity is rare. Instead, we check if the matrix $M(\\omega)$ is ill-conditioned, which indicates proximity to a singularity. The condition number, $\\mathrm{cond}(M)$, quantifies this. A large condition number implies that small perturbations in the input can lead to large changes in the solution, making the linear system solution unreliable. The problem specifies a threshold of $10^{12}$: if $\\mathrm{cond}_2(M(\\omega)) > 10^{12}$, or if a numerical solver fails due to singularity, we must treat the magnitude as infinite, i.e., $\\lvert H(e^{j\\omega}) \\rvert = +\\infty$.\n\nThe overall algorithm is as follows:\nFor each given test case $(A, B, C, D)$:\n1.  Initialize an empty list to store magnitude values.\n2.  For each frequency $\\omega$ in the specified grid $\\Omega = [0, \\pi/4, \\pi/2, 3\\pi/4, \\pi]$:\n    a.  Set $z = e^{j\\omega}$.\n    b.  Construct the complex matrix $M(\\omega) = zI - A$.\n    c.  Compute the $2$-norm condition number of $M(\\omega)$.\n    d.  If $\\mathrm{cond}_2(M(\\omega)) > 10^{12}$ or if solving with $M(\\omega)$ fails, the magnitude for this frequency is $+\\infty$.\n    e.  Otherwise, solve the linear system $M(\\omega) X(\\omega) = B$ for the complex vector $X(\\omega)$.\n    f.  Compute the complex scalar response $H(e^{j\\omega}) = C X(\\omega) + D$.\n    g.  Calculate the magnitude $\\lvert H(e^{j\\omega}) \\rvert$.\n    h.  Add the computed magnitude (finite or infinite) to the list.\n3.  Find the maximum value in the list of magnitudes.\n4.  If the maximum is finite, format it to six decimal places. If it is infinite, represent it as 'inf'.\nThis procedure will be applied to all test cases.", "answer": "```python\nimport numpy as np\n\ndef calculate_max_magnitude(A_in, B_in, C_in, D_in, omega_grid, cond_thresh):\n    \"\"\"\n    Calculates the maximum magnitude of the frequency response for a state-space model.\n\n    The function computes the frequency response H(e^jω) = C(e^jω*I - A)^-1 * B + D\n    by solving a linear system for each ω in the grid. It handles ill-conditioning\n    by checking the matrix condition number against a threshold.\n    \"\"\"\n    # Convert input lists to numpy arrays with appropriate types and shapes\n    A = np.array(A_in, dtype=np.float64)\n    B = np.array(B_in, dtype=np.float64).reshape(-1, 1)\n    C = np.array(C_in, dtype=np.float64).reshape(1, -1)\n    D = np.array(D_in, dtype=np.float64).reshape(1, 1)\n    \n    n = A.shape[0]\n    I = np.eye(n, dtype=np.complex128)\n    \n    magnitudes = []\n\n    for omega in omega_grid:\n        z = np.exp(1j * omega)\n        M = z * I - A\n        is_infinite = False\n\n        # Check for singularity or ill-conditioning\n        try:\n            # np.linalg.cond can be expensive, but is required by the problem spec.\n            # A LinAlgError might be raised for exactly singular matrices.\n            cond_num = np.linalg.cond(M)\n            if cond_num > cond_thresh:\n                is_infinite = True\n        except np.linalg.LinAlgError:\n            is_infinite = True\n\n        if is_infinite:\n            magnitudes.append(np.inf)\n            continue\n            \n        # Solve the linear system (zI - A)X = B\n        try:\n            # X(ω) is the frequency-dependent state response vector\n            X_omega = np.linalg.solve(M, B)\n            \n            # H(e^jω) = C*X(ω) + D\n            H_omega = C @ X_omega + D\n            magnitudes.append(np.abs(H_omega[0, 0]))\n        except np.linalg.LinAlgError:\n            # This is a fallback; the condition number check should pre-empt this.\n            magnitudes.append(np.inf)\n            \n    max_mag = np.max(magnitudes)\n    \n    if np.isinf(max_mag):\n        return \"inf\"\n    else:\n        return f\"{max_mag:.6f}\"\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the result for each, printing the final output.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: stable second-order oscillator-like\n        {\n            \"A\": [[0.7, -0.4], [0.4, 0.7]],\n            \"B\": [[1.0], [0.0]],\n            \"C\": [[1.0, 0.0]],\n            \"D\": [[0.0]]\n        },\n        # Test Case 2: nonzero direct term\n        {\n            \"A\": [[0.2, 0.0], [0.0, 0.5]],\n            \"B\": [[1.0], [1.0]],\n            \"C\": [[0.5, -0.5]],\n            \"D\": [[0.3]]\n        },\n        # Test Case 3: marginal pole on the unit circle\n        {\n            \"A\": [[1.0, 0.0], [0.0, 0.2]],\n            \"B\": [[1.0], [0.0]],\n            \"C\": [[1.0, 0.0]],\n            \"D\": [[0.0]]\n        },\n        # Test Case 4: zero input-to-state channel\n        {\n            \"A\": [[0.9]],\n            \"B\": [[0.0]],\n            \"C\": [[1.0]],\n            \"D\": [[0.0]]\n        }\n    ]\n    \n    omega_grid = [0.0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi]\n    cond_thresh = 1e12\n    \n    results = []\n    for case in test_cases:\n        result = calculate_max_magnitude(\n            case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"], omega_grid, cond_thresh\n        )\n        results.append(result)\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2886136"}, {"introduction": "While theory provides us with model structures, real-world applications require learning the model parameters directly from observed data. This comprehensive practice guides you through a full system identification pipeline, from generating data to estimating a state-space realization using the classic Ho-Kalman algorithm. You will also confront the non-uniqueness of state-space representations by computing the similarity transformation that aligns your estimated model with a ground truth, a fundamental concept for understanding the internal representations of both classical and neural models. [@problem_id:2886056]", "problem": "You are given a sequence of linear time-invariant state-space models that instantiate a special case of a neural state-space model with affine state and output mappings, namely $f(\\mathbf{x},\\mathbf{u}) = A\\mathbf{x} + B\\mathbf{u}$ and $g(\\mathbf{x},\\mathbf{u}) = C\\mathbf{x} + D\\mathbf{u}$. For each model, you must generate input-output data, fit a realization from data only, and then compute a similarity transformation that aligns the estimated realization to the ground-truth.\n\nFundamental base. Use the standard linear time-invariant state-space equations and definitions: for discrete time index $k \\in \\mathbb{Z}_{\\ge 0}$, the state update and output equations are\n$$\n\\mathbf{x}_{k+1} = A \\mathbf{x}_k + B \\mathbf{u}_k, \\quad \\mathbf{y}_k = C \\mathbf{x}_k + D \\mathbf{u}_k,\n$$\nwith state $\\mathbf{x}_k \\in \\mathbb{R}^n$, input $\\mathbf{u}_k \\in \\mathbb{R}^m$, and output $\\mathbf{y}_k \\in \\mathbb{R}^p$. The impulse response (also called Markov parameters) $\\{H_i\\}_{i\\ge 0}$ satisfies $H_0 = D$ and $H_i = C A^{i-1} B$ for $i \\ge 1$. Any two minimal realizations that produce the same input-output behavior are related by a similarity transformation: if $(\\hat{A},\\hat{B},\\hat{C},\\hat{D})$ is another realization of the same transfer behavior, then there exists an invertible matrix $T \\in \\mathbb{R}^{n \\times n}$ such that $\\hat{A} = T^{-1} A T$, $\\hat{B} = T^{-1} B$, and $\\hat{C} = C T$, with $\\hat{D} = D$.\n\nTask. For each test case below, perform the following steps purely from input-output data generated from the given ground-truth $(A,B,C,D)$:\n1. Data generation. Using a fixed pseudorandom seed as specified in each test case, generate a zero-mean independent and identically distributed Gaussian input sequence $\\{\\mathbf{u}_k\\}_{k=0}^{N-1}$ with identity covariance in $\\mathbb{R}^m$. Simulate the system with initial state $\\mathbf{x}_0 = \\mathbf{0}$ to produce outputs $\\{\\mathbf{y}_k\\}_{k=0}^{N-1}$ using the exact state-space dynamics with no added noise.\n2. Markov parameter estimation. From the input-output data alone, estimate a finite sequence of Markov parameters $\\{\\hat{H}_i\\}_{i=0}^{L}$ by solving a linear least-squares problem that enforces the convolutional relation between outputs and past inputs over the horizon $L$, using $L = s + r + 10$, where $s$ and $r$ are specified per test case.\n3. Realization recovery via block Hankel factorization. Using the estimated $\\{\\hat{H}_i\\}$, build block Hankel matrices with $s$ block rows and $r$ block columns, then factor them using singular value decomposition (SVD) to obtain a realization $(\\hat{A},\\hat{B},\\hat{C},\\hat{D})$ of the specified state dimension $n$.\n4. Similarity alignment. Compute a similarity matrix $T \\in \\mathbb{R}^{n \\times n}$ that best aligns the estimated realization to the ground truth by enforcing the linear alignment constraints $T \\hat{B} \\approx B$, $C T \\approx \\hat{C}$, and $A T \\approx T \\hat{A}$ in the least-squares sense. Solve for $T$ as a single linear least-squares problem in the vectorized unknown entries of $T$.\n5. Quantitative error. Report the maximum of the three normalized Frobenius-norm residuals\n$$\n\\varepsilon_A = \\frac{\\lVert A T - T \\hat{A} \\rVert_F}{\\lVert A \\rVert_F + 10^{-12}}, \\quad\n\\varepsilon_B = \\frac{\\lVert T \\hat{B} - B \\rVert_F}{\\lVert B \\rVert_F + 10^{-12}}, \\quad\n\\varepsilon_C = \\frac{\\lVert C T - \\hat{C} \\rVert_F}{\\lVert C \\rVert_F + 10^{-12}},\n$$\nnamely output $e = \\max\\{\\varepsilon_A,\\varepsilon_B,\\varepsilon_C\\}$ for each test case.\n\nTest suite. Use the following three test cases. Each case specifies $(A,B,C,D)$, model dimensions $(n,m,p)$, simulation length $N$, Hankel design parameters $(s,r)$, and the pseudorandom seed for input generation.\n\n- Case 1 (single-input single-output):\n$$\nA = \\begin{bmatrix} 0.7  0.2 \\\\ -0.1  0.9 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 1.0  -0.3 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.1 \\end{bmatrix}.\n$$\nDimensions: $n = 2$, $m = 1$, $p = 1$. Use $N = 800$, $s = 5$, $r = 5$, seed $= 123$.\n\n- Case 2 (multi-input multi-output):\n$$\nA = \\begin{bmatrix} 0.6  0.2  0.0 \\\\ 0.0  0.7  0.1 \\\\ 0.0  -0.2  0.8 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\\\ 0.5  -0.2 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 1.0  0.0  0.3 \\\\ 0.2  0.8  -0.1 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}.\n$$\nDimensions: $n = 3$, $m = 2$, $p = 2$. Use $N = 1500$, $s = 6$, $r = 6$, seed $= 456$.\n\n- Case 3 (direct feedthrough emphasized):\n$$\nA = \\begin{bmatrix} 0.95 \\end{bmatrix}, \\;\nB = \\begin{bmatrix} 1.2 \\end{bmatrix}, \\;\nC = \\begin{bmatrix} 0.8 \\end{bmatrix}, \\;\nD = \\begin{bmatrix} 0.4 \\end{bmatrix}.\n$$\nDimensions: $n = 1$, $m = 1$, $p = 1$. Use $N = 400$, $s = 4$, $r = 4$, seed $= 789$.\n\nFinal output format. Your program should produce a single line of output containing the three scalar errors $e$ for the three cases, in order, rounded to exactly six digits after the decimal point, as a comma-separated list enclosed in square brackets, for example $[0.000123,0.001234,0.012345]$. No other text should be printed.", "solution": "The problem requires the identification of linear time-invariant (LTI) state-space models from input-output data and the subsequent alignment of the identified model with a ground-truth realization. The procedure is a standard multi-step process in system identification, which I will execute with methodical precision. Each step is grounded in fundamental principles of linear systems theory.\n\nThe system dynamics are described by the discrete-time state-space equations:\n$$\n\\mathbf{x}_{k+1} = A \\mathbf{x}_k + B \\mathbf{u}_k, \\quad \\mathbf{y}_k = C \\mathbf{x}_k + D \\mathbf{u}_k\n$$\nwhere $\\mathbf{x}_k \\in \\mathbb{R}^n$ is the state, $\\mathbf{u}_k \\in \\mathbb{R}^m$ is the input, and $\\mathbf{y}_k \\in \\mathbb{R}^p$ is the output at time step $k$. The matrices $(A, B, C, D)$ define the system.\n\nThe procedure is executed for each test case as follows:\n\n**Step 1: Data Generation**\nFirst, we simulate the system to generate a finite history of input-output pairs. For a given test case with parameters $(A, B, C, D, n, m, p, N, \\text{seed})$, we proceed as follows:\nA pseudorandom number generator is initialized with the specified seed to ensure reproducibility. An input sequence $\\{\\mathbf{u}_k\\}_{k=0}^{N-1}$ is generated, where each $\\mathbf{u}_k$ is drawn independently from a multivariate standard normal distribution, $\\mathcal{N}(\\mathbf{0}, I_m)$.\nWith the initial state set to the zero vector, $\\mathbf{x}_0 = \\mathbf{0}$, the system is simulated for $k = 0, 1, \\dots, N-1$ using the state-space equations to produce the output sequence $\\{\\mathbf{y}_k\\}_{k=0}^{N-1}$. No process or measurement noise is added, as per the problem statement.\n\n**Step 2: Markov Parameter Estimation**\nThe input-output relationship of an LTI system can be expressed via the convolution sum involving the system's impulse response, also known as Markov parameters $\\{H_i\\}_{i \\ge 0}$. The relationship is given by:\n$$\n\\mathbf{y}_k = \\sum_{i=0}^{\\infty} H_i \\mathbf{u}_{k-i}\n$$\nwhere $H_0 = D$ and $H_i = C A^{i-1} B$ for $i \\ge 1$. For practical estimation from finite data, we truncate this sum at a horizon $L$:\n$$\n\\mathbf{y}_k \\approx \\sum_{i=0}^{L} \\hat{H}_i \\mathbf{u}_{k-i}\n$$\nWe can estimate the sequence of Markov parameters $\\{\\hat{H}_i\\}_{i=0}^{L}$ by solving a linear least-squares problem. We form a set of linear equations for time steps $k = L, L+1, \\dots, N-1$. Let $\\mathcal{M} = [\\hat{H}_0, \\hat{H}_1, \\dots, \\hat{H}_L] \\in \\mathbb{R}^{p \\times (L+1)m}$ be the matrix of unknown parameters. Let $\\mathbf{\\Phi}_k = [\\mathbf{u}_k^T, \\mathbf{u}_{k-1}^T, \\dots, \\mathbf{u}_{k-L}^T]^T \\in \\mathbb{R}^{(L+1)m}$. The equations are $\\mathbf{y}_k^T = \\mathbf{\\Phi}_k^T \\mathcal{M}^T$. We stack these equations for $k=L, \\dots, N-1$ to form a large linear system $\\mathbf{Y} = \\mathbf{U}_{\\text{reg}} \\mathcal{M}^T$, where $\\mathbf{U}_{\\text{reg}} \\in \\mathbb{R}^{(N-L) \\times (L+1)m}$ is the regressor matrix whose rows are $\\mathbf{\\Phi}_k^T$, and $\\mathbf{Y} \\in \\mathbb{R}^{(N-L) \\times p}$ is the matrix of corresponding outputs.\nThe least-squares estimate for $\\mathcal{M}^T$ is obtained by solving this overdetermined system. From the resulting $\\hat{\\mathcal{M}}$, we extract the individual estimated Markov parameters $\\hat{H}_0, \\hat{H}_1, \\dots, \\hat{H}_L$. The horizon is set to $L = s + r + 10$, which is sufficiently long for the subsequent realization step.\n\n**Step 3: Realization Recovery via Block Hankel Factorization**\nThe Ho-Kalman algorithm provides a method to obtain a state-space realization $(\\hat{A}, \\hat{B}, \\hat{C})$ from the strictly proper part of the impulse response, i.e., from $\\{\\hat{H}_i\\}_{i \\ge 1}$. The direct feedthrough term is simply $\\hat{D} = \\hat{H}_0$.\nWe construct a block Hankel matrix of size $(s \\cdot p) \\times (r \\cdot m)$ using the estimated Markov parameters:\n$$\n\\mathcal{H}_{s,r} = \\begin{bmatrix}\n\\hat{H}_1  \\hat{H}_2  \\dots  \\hat{H}_r \\\\\n\\hat{H}_2  \\hat{H}_3  \\dots  \\hat{H}_{r+1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n\\hat{H}_s  \\hat{H}_{s+1}  \\dots  \\hat{H}_{s+r-1}\n\\end{bmatrix}\n$$\nThis matrix can be factored as the product of the extended observability matrix $\\mathcal{O}_s$ and the extended controllability matrix $\\mathcal{C}_r$: $\\mathcal{H}_{s,r} = \\mathcal{O}_s \\mathcal{C}_r$.\nWe perform a singular value decomposition (SVD) of $\\mathcal{H}_{s,r} = U \\Sigma V^T$. For a minimal realization of rank $n$, we truncate the SVD to the $n$ largest singular values: $U_n \\in \\mathbb{R}^{sp \\times n}$, $\\Sigma_n \\in \\mathbb{R}^{n \\times n}$, $V_n \\in \\mathbb{R}^{rm \\times n}$. A balanced realization is then given by choosing $\\mathcal{O}_s = U_n \\Sigma_n^{1/2}$ and $\\mathcal{C}_r = \\Sigma_n^{1/2} V_n^T$.\nFrom these factors, the system matrices are extracted:\n- $\\hat{C}$ is the first $p$ rows of $\\mathcal{O}_s$.\n- $\\hat{B}$ is the first $m$ columns of $\\mathcal{C}_r$.\n- $\\hat{A}$ is found using the shifted block Hankel matrix $\\mathcal{H}'_{s,r}$ (where all indices are incremented by 1), which satisfies $\\mathcal{H}'_{s,r} = \\mathcal{O}_s \\hat{A} \\mathcal{C}_r$. This gives $\\hat{A} = (\\mathcal{O}_s^\\dagger) \\mathcal{H}'_{s,r} (\\mathcal{C}_r^\\dagger) = \\Sigma_n^{-1/2} U_n^T \\mathcal{H}'_{s,r} V_n \\Sigma_n^{-1/2}$.\n\n**Step 4: Similarity Alignment**\nThe realization $(\\hat{A}, \\hat{B}, \\hat{C})$ is correct up to a similarity transformation. To compare it to the ground-truth $(A, B, C)$, we must find an invertible matrix $T \\in \\mathbb{R}^{n \\times n}$ that aligns them. The ideal transformation satisfies $A = T \\hat{A} T^{-1}$, $B = T \\hat{B}$, and $C = \\hat{C} T^{-1}$. Rearranging gives the set of linear equations for $T$:\n1. $A T - T \\hat{A} = 0$\n2. $T \\hat{B} - B = 0$\n3. $C T - \\hat{C} = 0$\nWe find the matrix $T$ that best satisfies these three equations simultaneously in a least-squares sense. To formulate this as a standard linear least-squares problem, we vectorize the matrix equations using the Kronecker product ($\\otimes$):\n1. $(I_n \\otimes A - \\hat{A}^T \\otimes I_n) \\text{vec}(T) = \\text{vec}(0)$\n2. $(\\hat{B}^T \\otimes I_n) \\text{vec}(T) = \\text{vec}(B)$\n3. $(I_n \\otimes C) \\text{vec}(T) = \\text{vec}(\\hat{C})$\nThese three systems are stacked into a single large system $\\mathbf{M} \\mathbf{t} \\approx \\mathbf{v}$, where $\\mathbf{t} = \\text{vec}(T)$. This overdetermined system is solved for $\\mathbf{t}$ using least-squares, and the solution is reshaped into the $n \\times n$ matrix $T$.\n\n**Step 5: Quantitative Error**\nWith the computed alignment matrix $T$, we quantify the alignment error using the normalized Frobenius norm for each of the three matrix equations:\n$$\n\\varepsilon_A = \\frac{\\lVert A T - T \\hat{A} \\rVert_F}{\\lVert A \\rVert_F + \\epsilon}, \\quad\n\\varepsilon_B = \\frac{\\lVert T \\hat{B} - B \\rVert_F}{\\lVert B \\rVert_F + \\epsilon}, \\quad\n\\varepsilon_C = \\frac{\\lVert C T - \\hat{C} \\rVert_F}{\\lVert C \\rVert_F + \\epsilon}\n$$\nwhere $\\epsilon = 10^{-12}$ is added for numerical stability. The final error for the test case is the maximum of these three values, $e = \\max\\{\\varepsilon_A, \\varepsilon_B, \\varepsilon_C\\}$. This entire procedure is repeated for all specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve_case(A, B, C, D, n, m, p, N, s, r, seed):\n    \"\"\"\n    Solves a single test case for LTI system identification and alignment.\n    \"\"\"\n    # Step 1: Data Generation\n    rng = np.random.default_rng(seed)\n    u = rng.standard_normal(size=(N, m))\n    x = np.zeros((N + 1, n))\n    y = np.zeros((N, p))\n\n    for k in range(N):\n        y[k, :] = (C @ x[k, :]) + (D @ u[k, :])\n        x[k + 1, :] = (A @ x[k, :]) + (B @ u[k, :])\n\n    # Step 2: Markov Parameter Estimation\n    L = s + r + 10\n    num_eqs = N - L\n    \n    # Form regressor matrix U_reg and target Y_target\n    # U_reg has rows [u_k^T, u_{k-1}^T, ..., u_{k-L}^T] for k = L..N-1\n    U_reg = np.zeros((num_eqs, (L + 1) * m))\n    for i in range(L + 1):\n        U_reg[:, i * m:(i + 1) * m] = u[L - i:N - i, :]\n    \n    Y_target = y[L:N, :]\n    \n    # Solve U_reg @ M_hat_T = Y_target for M_hat_T\n    M_hat_T, _, _, _ = np.linalg.lstsq(U_reg, Y_target, rcond=None)\n    \n    M_hat = M_hat_T.T  # Shape: (p, (L + 1) * m)\n    \n    H_hat = [M_hat[:, i * m:(i + 1) * m] for i in range(L + 1)]\n    D_hat = H_hat[0]\n\n    # Step 3: Realization Recovery (Ho-Kalman)\n    H_sys = H_hat[1:]  # Strictly proper part\n    \n    # Build block Hankel matrix\n    H_sr = np.zeros((s * p, r * m))\n    for i in range(s):\n        for j in range(r):\n            if i + j  len(H_sys):\n                H_sr[i * p:(i + 1) * p, j * m:(j + 1) * m] = H_sys[i + j]\n\n    # SVD and truncation\n    U, S_vec, Vt = np.linalg.svd(H_sr, full_matrices=False)\n    U_n = U[:, :n]\n    S_n_vec = S_vec[:n]\n    Vt_n = Vt[:n, :]\n    \n    # Balanced realization factors\n    sqrt_Sigma_n = np.diag(np.sqrt(S_n_vec))\n    Obs = U_n @ sqrt_Sigma_n\n    Ctrl = sqrt_Sigma_n @ Vt_n\n    \n    C_hat = Obs[:p, :]\n    B_hat = Ctrl[:, :m]\n\n    # Shifted Hankel matrix for A_hat\n    H_prime_sr = np.zeros((s * p, r * m))\n    for i in range(s):\n        for j in range(r):\n            if i + j + 1  len(H_sys):\n                H_prime_sr[i * p:(i + 1) * p, j * m:(j + 1) * m] = H_sys[i + j + 1]\n\n    inv_sqrt_Sigma_n = np.diag(1.0 / np.sqrt(S_n_vec))\n    V_n = Vt_n.T\n    A_hat = inv_sqrt_Sigma_n @ U_n.T @ H_prime_sr @ V_n @ inv_sqrt_Sigma_n\n\n    # Step 4: Similarity Alignment\n    # Form the stacked least-squares problem to solve for vec(T)\n    # A T - T A_hat = 0\n    M_A = np.kron(np.eye(n), A) - np.kron(A_hat.T, np.eye(n))\n    # T B_hat - B = 0  -> (B_hat.T kron I) vec(T) = vec(B)\n    M_B = np.kron(B_hat.T, np.eye(n))\n    # C T - C_hat = 0 -> (I kron C) vec(T) = vec(C_hat)\n    M_C = np.kron(np.eye(n), C)\n    \n    M_stack = np.vstack([M_A, M_B, M_C])\n    \n    v_A = np.zeros(n * n)\n    v_B = B.flatten('F')\n    v_C = C_hat.flatten('F')\n    v_stack = np.concatenate([v_A, v_B, v_C])\n    \n    t_vec, _, _, _ = np.linalg.lstsq(M_stack, v_stack, rcond=None)\n    T = t_vec.reshape((n, n), order='F')\n\n    # Step 5: Quantitative Error\n    eps = 1e-12\n    norm_A = np.linalg.norm(A, 'fro') + eps\n    norm_B = np.linalg.norm(B, 'fro') + eps\n    norm_C = np.linalg.norm(C, 'fro') + eps\n    \n    err_A = np.linalg.norm(A @ T - T @ A_hat, 'fro') / norm_A\n    err_B = np.linalg.norm(T @ B_hat - B, 'fro') / norm_B\n    err_C = np.linalg.norm(C @ T - C_hat, 'fro') / norm_C\n    \n    return max(err_A, err_B, err_C)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"A\": np.array([[0.7, 0.2], [-0.1, 0.9]]),\n            \"B\": np.array([[1.0], [0.5]]),\n            \"C\": np.array([[1.0, -0.3]]),\n            \"D\": np.array([[0.1]]),\n            \"dims\": {\"n\": 2, \"m\": 1, \"p\": 1},\n            \"params\": {\"N\": 800, \"s\": 5, \"r\": 5, \"seed\": 123},\n        },\n        # Case 2\n        {\n            \"A\": np.array([[0.6, 0.2, 0.0], [0.0, 0.7, 0.1], [0.0, -0.2, 0.8]]),\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0], [0.5, -0.2]]),\n            \"C\": np.array([[1.0, 0.0, 0.3], [0.2, 0.8, -0.1]]),\n            \"D\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"dims\": {\"n\": 3, \"m\": 2, \"p\": 2},\n            \"params\": {\"N\": 1500, \"s\": 6, \"r\": 6, \"seed\": 456},\n        },\n        # Case 3\n        {\n            \"A\": np.array([[0.95]]),\n            \"B\": np.array([[1.2]]),\n            \"C\": np.array([[0.8]]),\n            \"D\": np.array([[0.4]]),\n            \"dims\": {\"n\": 1, \"m\": 1, \"p\": 1},\n            \"params\": {\"N\": 400, \"s\": 4, \"r\": 4, \"seed\": 789},\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        error = solve_case(\n            case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"],\n            case[\"dims\"][\"n\"], case[\"dims\"][\"m\"], case[\"dims\"][\"p\"],\n            case[\"params\"][\"N\"], case[\"params\"][\"s\"], case[\"params\"][\"r\"], case[\"params\"][\"seed\"]\n        )\n        results.append(error)\n    \n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2886056"}]}