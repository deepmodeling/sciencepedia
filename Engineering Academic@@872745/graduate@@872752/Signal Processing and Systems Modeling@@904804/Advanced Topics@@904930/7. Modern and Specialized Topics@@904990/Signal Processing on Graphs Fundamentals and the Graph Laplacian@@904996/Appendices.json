{"hands_on_practices": [{"introduction": "Understanding the graph Laplacian begins with its fundamental construction. This exercise demystifies the Laplacian by guiding you to build it from a basic component, the node-edge incidence matrix, for the simple yet ubiquitous path graph [@problem_id:2903901]. By then deriving its eigenvalues from first principles, you will gain a concrete understanding of how a graph's connectivity is encoded in its spectral properties.", "problem": "Consider the undirected, unweighted path graph on $n \\geq 2$ labeled nodes $\\{1,2,\\dots,n\\}$, with edges $\\{(1,2),(2,3),\\dots,(n-1,n)\\}$. Impose an arbitrary orientation by directing each edge from the lower-indexed node to the higher-indexed node. Let $B \\in \\mathbb{R}^{(n-1)\\times n}$ denote the node-edge incidence matrix determined by this orientation, where each row corresponds to an edge and each column to a node, and entries $-1$, $+1$, and $0$ indicate the tail, head, and non-incidence, respectively.\n\nUsing only the fundamental definitions of the incidence matrix and the unnormalized graph Laplacian $L$ as $L = B^{\\top} B$, do the following:\n\n1. Construct $B$ explicitly for the given orientation of the path graph and derive the explicit tridiagonal structure of $L$ from $B^{\\top} B$. Your derivation must start from the definition of $B$ and $L$ and use only linear-algebraic operations, without invoking any pre-memorized formulas for $L$.\n\n2. From the obtained tridiagonal form of $L$, derive the closed-form expression of the algebraic connectivity, defined as the second-smallest eigenvalue of $L$. Your derivation must start from the eigenvalue equation for the tridiagonal matrix and proceed by first principles (e.g., solving the associated linear recurrence with appropriate boundary conditions).\n\nProvide your final answer as a single exact analytic expression in terms of $n$. Do not approximate, and do not include any units. If you introduce any angles, they must be in radians. The final answer must be a single expression, not an inequality or an equation.", "solution": "The problem as stated is mathematically well-defined, self-contained, and scientifically sound. It presents a standard problem in spectral graph theory that admits a unique, verifiable solution. Thus, we proceed with the derivation.\n\nThe problem requires a two-part derivation. First, the construction of the graph Laplacian matrix $L$ from the incidence matrix $B$. Second, the derivation of the algebraic connectivity of $L$ by solving its eigenvalue problem.\n\nPart 1: Derivation of the Laplacian Matrix $L$\n\nThe graph is an unweighted path graph on $n$ nodes, which we label $\\{1, 2, \\dots, n\\}$. The edges are $\\{(k, k+1) \\mid k = 1, 2, \\dots, n-1\\}$. There are $m = n-1$ edges. An orientation is imposed by directing each edge from node $k$ to node $k+1$.\n\nThe incidence matrix $B \\in \\mathbb{R}^{(n-1) \\times n}$ has its rows indexed by edges and columns by nodes. Let the $k$-th edge be $e_k = (k, k+1)$, where $k$ is the tail and $k+1$ is the head. The entries of $B$ are defined as:\n$B_{k,j} = -1$ if node $j$ is the tail of edge $e_k$,\n$B_{k,j} = +1$ if node $j$ is the head of edge $e_k$,\n$B_{k,j} = 0$ otherwise.\n\nFor the $k$-th row, corresponding to edge $e_k = (k, k+1)$, the non-zero entries are at columns $j=k$ and $j=k+1$. Specifically, for $k \\in \\{1, \\dots, n-1\\}$:\n$$\nB_{k,j} = \\begin{cases}\n-1  \\text{if } j=k \\\\\n+1  \\text{if } j=k+1 \\\\\n0   \\text{otherwise}\n\\end{cases}\n$$\nThe unnormalized graph Laplacian is defined as $L = B^{\\top}B$. $L$ is an $n \\times n$ matrix whose element $L_{ij}$ is the dot product of the $i$-th and $j$-th columns of $B$. Let $\\mathbf{b}_j \\in \\mathbb{R}^{n-1}$ be the $j$-th column of $B$. Then $L_{ij} = \\mathbf{b}_i^{\\top}\\mathbf{b}_j$.\n\nWe now determine the structure of the columns of $B$:\n- The first column, $\\mathbf{b}_1$, corresponds to node $1$. Node $1$ is the tail only of edge $e_1=(1,2)$. So, $\\mathbf{b}_1$ has a $-1$ in the first row and zeros elsewhere.\n- A generic column $\\mathbf{b}_j$ for $j \\in \\{2, \\dots, n-1\\}$ corresponds to an internal node $j$. Node $j$ is the head of edge $e_{j-1}=(j-1, j)$ and the tail of edge $e_j=(j, j+1)$. So, $\\mathbf{b}_j$ has a $+1$ in row $j-1$, a $-1$ in row $j$, and zeros elsewhere.\n- The last column, $\\mathbf{b}_n$, corresponds to node $n$. Node $n$ is the head only of edge $e_{n-1}=(n-1, n)$. So, $\\mathbf{b}_n$ has a $+1$ in the last row ($n-1$) and zeros elsewhere.\n\nNow we compute the entries of $L = (L_{ij})$:\n\n1.  Diagonal elements ($i=j$): $L_{ii} = \\mathbf{b}_i^{\\top}\\mathbf{b}_i$.\n    - For $i=1$: $L_{11} = \\mathbf{b}_1^{\\top}\\mathbf{b}_1 = (-1)^2 = 1$.\n    - For $i \\in \\{2, \\dots, n-1\\}$: $L_{ii} = \\mathbf{b}_i^{\\top}\\mathbf{b}_i = (+1)^2 + (-1)^2 = 2$.\n    - For $i=n$: $L_{nn} = \\mathbf{b}_n^{\\top}\\mathbf{b}_n = (+1)^2 = 1$.\n    The diagonal of $L$ is $(1, 2, 2, \\dots, 2, 1)$.\n\n2.  Off-diagonal elements ($i \\neq j$): $L_{ij} = \\mathbf{b}_i^{\\top}\\mathbf{b}_j$.\n    - For adjacent nodes, $j=i+1$: We compute $L_{i, i+1} = \\mathbf{b}_i^{\\top}\\mathbf{b}_{i+1}$.\n        - For $i=1$: $\\mathbf{b}_1$ has a $-1$ at row $1$. $\\mathbf{b}_2$ has a $+1$ at row $1$. Their supports overlap only at row $1$. $L_{12} = (-1)(+1) = -1$.\n        - For $i \\in \\{2, \\dots, n-2\\}$: $\\mathbf{b}_i$ has non-zero entries at rows $i-1$ and $i$. $\\mathbf{b}_{i+1}$ has non-zero entries at rows $i$ and $i+1$. Their supports overlap only at row $i$. The entry of $\\mathbf{b}_i$ at row $i$ is $-1$ and the entry of $\\mathbf{b}_{i+1}$ at row $i$ is $+1$. So, $L_{i, i+1} = (-1)(+1) = -1$.\n        - For $i=n-1$: $\\mathbf{b}_{n-1}$ has non-zero entries at rows $n-2$ and $n-1$. $\\mathbf{b}_n$ has a non-zero entry at row $n-1$. Their supports overlap only at row $n-1$. The entry of $\\mathbf{b}_{n-1}$ at row $n-1$ is $-1$ and the entry of $\\mathbf{b}_n$ is $+1$. So, $L_{n-1,n} = (-1)(+1) = -1$.\n    - Since $L$ is symmetric, $L_{i+1, i} = L_{i, i+1} = -1$.\n    - For non-adjacent nodes, $|i-j|  1$: The columns $\\mathbf{b}_i$ and $\\mathbf{b}_j$ have non-zero entries at disjoint sets of rows. Thus, their dot product is zero. $L_{ij} = 0$.\n\nCombining these results, the Laplacian matrix $L$ is the $n \\times n$ symmetric tridiagonal matrix:\n$$\nL = \\begin{pmatrix}\n1  -1  0  \\cdots  0  0 \\\\\n-1  2  -1  \\cdots  0  0 \\\\\n0  -1  2  \\cdots  0  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n0  0  0  \\cdots  2  -1 \\\\\n0  0  0  \\cdots  -1  1\n\\end{pmatrix}\n$$\n\nPart 2: Derivation of the Algebraic Connectivity\n\nThe algebraic connectivity is the second-smallest eigenvalue of $L$. We must solve the eigenvalue equation $L\\mathbf{v} = \\lambda\\mathbf{v}$, where $\\mathbf{v} = [v_1, v_2, \\dots, v_n]^{\\top}$ is an eigenvector and $\\lambda$ is its corresponding eigenvalue. This matrix equation represents a system of $n$ linear equations.\n\n- Row $1$: $v_1 - v_2 = \\lambda v_1 \\implies (1-\\lambda)v_1 - v_2 = 0$\n- Row $k$ ($1  k  n$): $-v_{k-1} + 2v_k - v_{k+1} = \\lambda v_k \\implies -v_{k-1} + (2-\\lambda)v_k - v_{k+1} = 0$\n- Row $n$: $-v_{n-1} + v_n = \\lambda v_n \\implies -v_{n-1} + (1-\\lambda)v_n = 0$\n\nThe equation for internal nodes defines a second-order linear homogeneous recurrence relation: $v_{k+1} - (2-\\lambda)v_k + v_{k-1} = 0$. The first and last equations act as boundary conditions.\n\nWe can unify these equations by defining \"ghost points\" $v_0$ and $v_{n+1}$.\nThe boundary equation at $k=1$ is $v_2 = (1-\\lambda)v_1$. The general recurrence for $k=1$ would be $v_2 - (2-\\lambda)v_1 + v_0 = 0$. Substituting $v_2$, we get $(1-\\lambda)v_1 - (2-\\lambda)v_1 + v_0 = 0$, which simplifies to $-v_1 + v_0 = 0$, or $v_0 = v_1$.\nSimilarly, the boundary equation at $k=n$ is $v_{n-1} = (1-\\lambda)v_n$. The general recurrence for $k=n$ would be $v_{n+1} - (2-\\lambda)v_n + v_{n-1} = 0$. Substituting $v_{n-1}$, we get $v_{n+1} - (2-\\lambda)v_n + (1-\\lambda)v_n = 0$, which simplifies to $v_{n+1} - v_n = 0$, or $v_{n+1} = v_n$.\n\nThus, the problem is equivalent to solving the recurrence relation $v_{k+1} - (2-\\lambda)v_k + v_{k-1} = 0$ for $k=1, \\dots, n$ subject to the Neumann-type boundary conditions $v_0 = v_1$ and $v_{n+1} = v_n$.\n\nLet $2-\\lambda = 2\\cos(\\theta)$ for some $\\theta$. The recurrence becomes $v_{k+1} - 2\\cos(\\theta)v_k + v_{k-1} = 0$. The characteristic equation is $r^2 - 2\\cos(\\theta)r + 1 = 0$, with roots $r = e^{\\pm i\\theta}$. The general solution is of the form $v_k = A \\cos(k\\theta) + B \\sin(k\\theta)$.\nFor boundary conditions of the form $v_0=v_1$ and $v_n=v_{n+1}$, a more convenient form of the solution is $v_k = C\\cos((k-1/2)\\theta)$.\nLet's verify the first condition: $v_1 = C\\cos(\\theta/2)$ and $v_0 = C\\cos(-\\theta/2) = C\\cos(\\theta/2)$. So $v_0=v_1$ is satisfied for any $\\theta$.\n\nNow we apply the second boundary condition, $v_{n+1} = v_n$:\n$$ C\\cos\\left(\\left(n+1-\\frac{1}{2}\\right)\\theta\\right) = C\\cos\\left(\\left(n-\\frac{1}{2}\\right)\\theta\\right) $$\n$$ \\cos\\left(\\left(n+\\frac{1}{2}\\right)\\theta\\right) = \\cos\\left(\\left(n-\\frac{1}{2}\\right)\\theta\\right) $$\nThis equality holds if $\\left(n+\\frac{1}{2}\\right)\\theta = \\pm \\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j$ for some integer $j$.\n- Case 1: $\\left(n+\\frac{1}{2}\\right)\\theta = \\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j \\implies \\theta = 2\\pi j$. This yields $\\lambda = 2 - 2\\cos(2\\pi j) = 0$. This corresponds to the smallest eigenvalue $\\lambda_1 = 0$, whose eigenvector is the constant vector $[1, 1, \\dots, 1]^{\\top}$.\n- Case 2: $\\left(n+\\frac{1}{2}\\right)\\theta = -\\left(n-\\frac{1}{2}\\right)\\theta + 2\\pi j \\implies 2n\\theta = 2\\pi j \\implies \\theta = \\frac{\\pi j}{n}$.\n\nThis gives a set of $n$ distinct eigenvalues by taking $j = 0, 1, \\dots, n-1$. The eigenvalues $\\lambda_j$ are given by:\n$$ \\lambda_j = 2 - 2\\cos(\\theta_j) = 2 - 2\\cos\\left(\\frac{\\pi j}{n}\\right) \\quad \\text{for } j=0, 1, \\dots, n-1. $$\nThe function $\\cos(x)$ is strictly decreasing for $x \\in [0, \\pi]$. Our arguments $\\frac{\\pi j}{n}$ lie in the range $[0, \\pi - \\pi/n]$. Therefore, as $j$ increases, $\\cos(\\frac{\\pi j}{n})$ decreases, and $\\lambda_j$ increases. The eigenvalues are naturally ordered by the index $j$.\n\nThe smallest eigenvalue corresponds to $j=0$: $\\lambda_0 = 2 - 2\\cos(0) = 0$.\nThe second-smallest eigenvalue, which is the algebraic connectivity, corresponds to $j=1$:\n$$ \\lambda_1 = 2 - 2\\cos\\left(\\frac{\\pi}{n}\\right) $$\nThis is the required expression.", "answer": "$$\n\\boxed{2 - 2\\cos\\left(\\frac{\\pi}{n}\\right)}\n$$", "id": "2903901"}, {"introduction": "A core question in analyzing any system is understanding its sensitivity to perturbations. For graph signals, this translates to asking how the graph's 'frequencies'—the eigenvalues of the Laplacian—shift when the underlying graph structure is altered [@problem_id:2903899]. This practice employs Weyl’s inequalities, a powerful tool from matrix analysis, to derive precise bounds on these spectral shifts, providing fundamental insight into the stability of graph-based models.", "problem": "Consider a connected, undirected, weighted graph on $n \\geq 2$ vertices with symmetric adjacency weight matrix $W \\in \\mathbb{R}^{n \\times n}$ and combinatorial graph Laplacian $L = D - W \\in \\mathbb{R}^{n \\times n}$, where $D$ is the diagonal degree matrix defined by $D_{ii} = \\sum_{j=1}^{n} W_{ij}$. Let $(i,j)$ be a fixed edge with current weight $w_{ij}  0$. The weight of this edge is increased by a positive increment $\\delta  0$, yielding a new Laplacian $L'$. Starting from the definition of the graph Laplacian and the effect of a single edge-weight change on $L$, and using Weyl’s inequalities for Hermitian matrices, derive bounds on the eigenvalue shifts $\\lambda_{k}(L') - \\lambda_{k}(L)$ for all $k \\in \\{1,\\dots,n\\}$ in terms of $\\delta$ only. Then determine the smallest constant $c$ such that for every connected graph, every choice of edge $(i,j)$, and every $\\delta  0$, the bound\n$$\n0 \\leq \\lambda_{k}(L') - \\lambda_{k}(L) \\leq c\\,\\delta\n$$\nholds for all $k \\in \\{1,\\dots,n\\}$. Provide $c$ as your final answer. Express the final answer as an exact real number (no rounding).", "solution": "We begin with the definition of the combinatorial graph Laplacian $L = D - W$, where $W$ is symmetric and $D$ is diagonal with $D_{ii} = \\sum_{j=1}^{n} W_{ij}$. Increasing a single edge weight $w_{ij}$ by $\\delta  0$ modifies both the off-diagonal entries at positions $(i,j)$ and $(j,i)$ and the diagonal entries at $(i,i)$ and $(j,j)$ through the degree update. Specifically, since $L_{ij} = -W_{ij}$ for $i \\neq j$ and $L_{ii} = \\sum_{l \\neq i} W_{il}$, the change $\\Delta := L' - L$ has entries\n$$\n\\Delta_{ii} = \\delta,\\quad \\Delta_{jj} = \\delta,\\quad \\Delta_{ij} = -\\delta,\\quad \\Delta_{ji} = -\\delta,\n$$\nand $\\Delta_{ab} = 0$ for all other $(a,b)$. If we define the standard basis vectors $e_{1},\\dots,e_{n} \\in \\mathbb{R}^{n}$ and the vector $b := e_{i} - e_{j}$, then the matrix $\\Delta$ can be written compactly as\n$$\n\\Delta \\;=\\; \\delta\\,(e_{i} - e_{j})(e_{i} - e_{j})^{\\top} \\;=\\; \\delta\\, b b^{\\top}.\n$$\nThis shows that $\\Delta$ is positive semidefinite, symmetric, and of rank at most $1$.\n\nNext, we characterize the eigenvalues of $\\Delta$. Since $\\Delta = \\delta\\, b b^{\\top}$ is a rank-$1$ outer product, its nonzero eigenvalue equals $\\delta$ times the squared Euclidean norm of $b$. We compute\n$$\n\\|b\\|_{2}^{2} \\;=\\; \\|e_{i} - e_{j}\\|_{2}^{2} \\;=\\; \\|e_{i}\\|_{2}^{2} + \\|e_{j}\\|_{2}^{2} - 2\\, e_{i}^{\\top} e_{j} \\;=\\; 1 + 1 - 2\\cdot 0 \\;=\\; 2.\n$$\nTherefore, $\\Delta$ has spectrum consisting of one eigenvalue equal to $2\\delta$ and the remaining $n-1$ eigenvalues equal to $0$. Concretely,\n$$\n\\lambda_{1}(\\Delta) = 0,\\;\\dots,\\;\\lambda_{n-1}(\\Delta) = 0,\\;\\lambda_{n}(\\Delta) = 2\\delta,\n$$\nwhen the eigenvalues are ordered in nondecreasing order.\n\nWe now use Weyl’s inequalities for Hermitian matrices. For any real symmetric matrices $A$ and $B$ of size $n \\times n$ with eigenvalues ordered nondecreasingly, Weyl’s inequalities state that for each $k \\in \\{1,\\dots,n\\}$,\n$$\n\\lambda_{k}(A) + \\lambda_{1}(B) \\;\\leq\\; \\lambda_{k}(A+B) \\;\\leq\\; \\lambda_{k}(A) + \\lambda_{n}(B).\n$$\nWe apply this with $A = L$ and $B = \\Delta$. Using the eigenvalues of $\\Delta$ computed above, we obtain for every $k \\in \\{1,\\dots,n\\}$,\n$$\n\\lambda_{k}(L) + 0 \\;\\leq\\; \\lambda_{k}(L + \\Delta) \\;\\leq\\; \\lambda_{k}(L) + 2\\delta,\n$$\nwhich simplifies to the shift bounds\n$$\n0 \\;\\leq\\; \\lambda_{k}(L') - \\lambda_{k}(L) \\;\\leq\\; 2\\delta.\n$$\nThis shows that one can take $c = 2$. It remains to argue that this constant is the smallest possible universal constant (independent of the particular connected graph, edge $(i,j)$, and $\\delta$). To establish minimality, it suffices to provide a connected graph for which the upper bound $2\\delta$ is attained for some eigenvalue. Consider the graph with $n = 2$ vertices and a single edge of weight $w  0$. Its Laplacian is\n$$\nL = \\begin{pmatrix} w  -w \\\\ -w  w \\end{pmatrix},\n$$\nwhose eigenvalues are $0$ and $2w$. Increasing the unique edge weight by $\\delta$ yields\n$$\nL' = \\begin{pmatrix} w+\\delta  -(w+\\delta) \\\\ -(w+\\delta)  w+\\delta \\end{pmatrix},\n$$\nwhose eigenvalues are $0$ and $2(w+\\delta)$. Thus the nonzero eigenvalue shifts by exactly $2\\delta$, achieving the upper bound. Therefore, no universal constant $c  2$ can satisfy the bound for all connected graphs, all edges, and all $\\delta  0$.\n\nWe conclude that the smallest constant $c$ satisfying\n$$\n0 \\leq \\lambda_{k}(L') - \\lambda_{k}(L) \\leq c\\,\\delta \\quad \\text{for all } k \\in \\{1,\\dots,n\\}\n$$\nis $c = 2$.", "answer": "$$\\boxed{2}$$", "id": "2903899"}, {"introduction": "The power of graph signal processing is realized through the application of graph filters, defined as functions of the Laplacian, $g(L)$. Direct computation via eigendecomposition is often intractable for large graphs, demanding efficient alternatives. This exercise addresses this critical issue by guiding you through the development of a Chebyshev polynomial approximation, a cornerstone technique for designing numerically stable and scalable graph filters for real-world applications [@problem_id:2903956].", "problem": "Consider a weighted, undirected graph with $N$ nodes and combinatorial graph Laplacian $L \\in \\mathbb{R}^{N \\times N}$ defined by $L = D - W$, where $W$ is the symmetric weight matrix and $D$ is the degree matrix. The matrix $L$ is symmetric positive semidefinite with real eigenvalues in $[0,\\lambda_{\\max}]$. Let $g : [0,\\lambda_{\\max}] \\to \\mathbb{R}$ be a continuous function. The graph filter $g(L)$ is defined by the spectral functional calculus: if $L = U \\Lambda U^{\\top}$ with $\\Lambda = \\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{N})$ and $U$ orthogonal, then $g(L) = U \\, g(\\Lambda) \\, U^{\\top}$ with $g(\\Lambda) = \\mathrm{diag}(g(\\lambda_{1}),\\dots,g(\\lambda_{N}))$.\n\nYour goal is to obtain a numerically stable, degree-$K$ polynomial approximation of $g(L)$ using Chebyshev polynomials of the first kind. Starting only from the properties that (i) $L$ is diagonalizable with spectrum in $[0,\\lambda_{\\max}]$, and (ii) Chebyshev polynomials of the first kind $\\{T_{k}\\}_{k \\ge 0}$ form an orthogonal basis on $[-1,1]$ with respect to the weight $(1-x^{2})^{-1/2}$, derive:\n\n1. An affine rescaling of the spectrum that maps $[0,\\lambda_{\\max}]$ to $[-1,1]$ and the corresponding rescaled operator $\\widetilde{L}$ expressed in terms of $L$ and $\\lambda_{\\max}$.\n2. The degree-$K$ Chebyshev approximation $p_{K}(L)$ of $g(L)$ as a sum of Chebyshev polynomials evaluated at $\\widetilde{L}$, with coefficients written explicitly as integrals involving $g$ on $[0,\\lambda_{\\max}]$ via the inverse of your affine rescaling. Your derivation should make clear how the orthogonality on $[-1,1]$ determines these coefficients.\n3. A brief justification of why this rescaling promotes numerical stability when evaluating the approximation via the three-term Chebyshev recurrence.\n\nProvide your final answer as a single, closed-form analytic expression for $p_{K}(L)$ in terms of $g$, $\\lambda_{\\max}$, $K$, and $L$. No numerical evaluation is required, and no units are needed. The final answer must be a single expression, not an equation or inequality.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of graph signal processing, based on established principles of linear algebra and approximation theory. All necessary data and definitions are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nThe objective is to construct a degree-$K$ polynomial approximation of a graph filter $g(L)$ using Chebyshev polynomials. We will address the three required parts of the derivation in sequence.\n\n1. Affine Rescaling of the Spectrum\n\nThe graph Laplacian $L$ is a symmetric positive semidefinite matrix, and its spectrum, denoted $\\sigma(L)$, lies within the interval $[0, \\lambda_{\\max}]$, where $\\lambda_{\\max}$ is the largest eigenvalue of $L$. Chebyshev polynomials $T_k(x)$ are defined and possess their desirable orthogonality and stability properties on the interval $[-1, 1]$. Therefore, a mapping from the spectral domain of $L$ to this interval is required.\n\nLet $\\lambda \\in [0, \\lambda_{\\max}]$ be an eigenvalue of $L$. We seek an affine transformation $\\tilde{\\lambda} = f(\\lambda) = a\\lambda + b$ such that the interval $[0, \\lambda_{\\max}]$ is mapped to $[-1, 1]$. This requires that the endpoints of the interval are mapped accordingly:\n- $f(0) = -1$\n- $f(\\lambda_{\\max}) = 1$\n\nSubstituting these conditions into the affine form yields a system of two linear equations for the coefficients $a$ and $b$:\n$$a(0) + b = -1 \\implies b = -1$$\n$$a(\\lambda_{\\max}) + b = 1 \\implies a\\lambda_{\\max} - 1 = 1 \\implies a\\lambda_{\\max} = 2 \\implies a = \\frac{2}{\\lambda_{\\max}}$$\nThe required affine transformation for the eigenvalues is thus $\\tilde{\\lambda} = \\frac{2}{\\lambda_{\\max}}\\lambda - 1$.\n\nBy the principles of functional calculus for matrices, applying this same transformation to the operator $L$ yields the rescaled operator $\\widetilde{L}$. If $L\\mathbf{u} = \\lambda\\mathbf{u}$ for an eigenvector $\\mathbf{u}$, then:\n$$\\left(\\frac{2}{\\lambda_{\\max}}L - I\\right)\\mathbf{u} = \\frac{2}{\\lambda_{\\max}}(L\\mathbf{u}) - I\\mathbf{u} = \\frac{2}{\\lambda_{\\max}}(\\lambda\\mathbf{u}) - \\mathbf{u} = \\left(\\frac{2\\lambda}{\\lambda_{\\max}} - 1\\right)\\mathbf{u} = \\tilde{\\lambda}\\mathbf{u}$$\nwhere $I$ is the $N \\times N$ identity matrix. This confirms that the eigenvalues of the new operator $\\widetilde{L}$ are the rescaled eigenvalues $\\tilde{\\lambda}$.\n\nThe rescaled operator is therefore:\n$$\\widetilde{L} = \\frac{2}{\\lambda_{\\max}}L - I$$\nThe spectrum of $\\widetilde{L}$, $\\sigma(\\widetilde{L})$, lies entirely within the interval $[-1, 1]$, as required.\n\n2. Derivation of the Chebyshev Approximation\n\nWe wish to approximate the function $g(\\lambda)$ for $\\lambda \\in [0, \\lambda_{\\max}]$. Using the inverse of the affine rescaling, $\\lambda(x) = \\frac{\\lambda_{\\max}}{2}(x+1)$, we define a corresponding function $\\tilde{g}(x)$ on the interval $[-1, 1]$:\n$$\\tilde{g}(x) = g(\\lambda(x)) = g\\left(\\frac{\\lambda_{\\max}}{2}(x+1)\\right)$$\nWe approximate $\\tilde{g}(x)$ using a truncated Chebyshev series of degree $K$. The Chebyshev series expansion is given by $\\tilde{g}(x) = \\sum_{k=0}^{\\infty} c_k T_k(x)$, where the coefficients $c_k$ are determined by exploiting the orthogonality of the Chebyshev polynomials $\\{T_k(x)\\}_{k \\ge 0}$ with respect to the weight function $w(x) = (1-x^2)^{-1/2}$ on $[-1, 1]$. The orthogonality relation is:\n$$\\int_{-1}^{1} T_k(x) T_j(x) \\frac{dx}{\\sqrt{1-x^2}} = \\begin{cases} 0  \\text{if } k \\neq j \\\\ \\pi  \\text{if } k=j=0 \\\\ \\frac{\\pi}{2}  \\text{if } k=j  0 \\end{cases}$$\nThe coefficients $c_k$ are the projections of $\\tilde{g}(x)$ onto the basis functions $T_k(x)$:\n$$c_k = \\frac{\\langle \\tilde{g}, T_k \\rangle_w}{\\langle T_k, T_k \\rangle_w} = \\frac{\\int_{-1}^{1} \\tilde{g}(x) T_k(x) (1-x^2)^{-1/2} dx}{\\int_{-1}^{1} T_k(x)^2 (1-x^2)^{-1/2} dx}$$\nThis gives separate expressions for $k=0$ and $k0$:\n$$c_0 = \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{\\tilde{g}(x)}{\\sqrt{1-x^2}} dx$$\n$$c_k = \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{\\tilde{g}(x) T_k(x)}{\\sqrt{1-x^2}} dx \\quad \\text{for } k \\ge 1$$\nTo express these coefficients in terms of the original function $g$ on the interval $[0, \\lambda_{\\max}]$, we perform a change of variables in the integrals. Let $\\lambda' = \\frac{\\lambda_{\\max}}{2}(x+1)$, which implies $x = \\frac{2\\lambda'}{\\lambda_{\\max}}-1$. The differential is $d\\lambda' = \\frac{\\lambda_{\\max}}{2}dx$, so $dx = \\frac{2}{\\lambda_{\\max}}d\\lambda'$. The weight function term becomes:\n$$\\sqrt{1-x^2} = \\sqrt{1 - \\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)^2} = \\sqrt{\\frac{4\\lambda'(\\lambda_{\\max}-\\lambda')}{\\lambda_{\\max}^2}} = \\frac{2}{\\lambda_{\\max}}\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}$$\nSubstituting these into the integral for $c_k$ (for $k \\ge 1$):\n$$c_k = \\frac{2}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\frac{2}{\\lambda_{\\max}}\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} \\left(\\frac{2}{\\lambda_{\\max}} d\\lambda'\\right) = \\frac{2}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda'$$\nA similar substitution for $c_0$ yields:\n$$c_0 = \\frac{1}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda')}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda'$$\nThe degree-$K$ polynomial approximation of $g(\\lambda)$ is then the truncated series evaluated at the rescaled eigenvalue:\n$$p_K(\\lambda) = \\sum_{k=0}^{K} c_k T_k\\left(\\frac{2\\lambda}{\\lambda_{\\max}}-1\\right)$$\nBy functional calculus, the matrix polynomial approximation $p_K(L)$ of the graph filter $g(L)$ is obtained by substituting the operator $L$ for the scalar $\\lambda$:\n$$p_K(L) = \\sum_{k=0}^{K} c_k T_k\\left(\\frac{2}{\\lambda_{\\max}}L - I\\right) = \\sum_{k=0}^{K} c_k T_k(\\widetilde{L})$$\nSubstituting the integral expressions for the coefficients $c_k$ yields the final analytical form for the approximation.\n\n3. Justification for Numerical Stability\n\nThe use of the rescaled operator $\\widetilde{L}$ is critical for the numerical stability of the computation. The polynomial approximation $p_K(L)$ is evaluated in practice not by forming powers of $\\widetilde{L}$ but by using the three-term recurrence relation for Chebyshev polynomials:\n$$T_{k+1}(x) = 2x T_k(x) - T_{k-1}(x), \\quad \\text{with } T_0(x)=1, T_1(x)=x$$\nThis recurrence can be applied to the operator $\\widetilde{L}$ to generate the matrices $T_k(\\widetilde{L})$ or, more efficiently, used in algorithms like the Clenshaw algorithm to compute the action of $p_K(L)$ on a vector.\n\nThe stability of this recurrence hinges on the magnitude of its argument. For an argument $x$ such that $|x| \\le 1$, the Chebyshev polynomials are bounded: $|T_k(x)| \\le 1$ for all $k \\ge 0$. By rescaling the operator $L$ to $\\widetilde{L}$, we ensure all its eigenvalues $\\tilde{\\lambda}_i$ are in $[-1, 1]$. Consequently, the eigenvalues of each matrix $T_k(\\widetilde{L})$, which are $T_k(\\tilde{\\lambda}_i)$, are also bounded in magnitude by $1$. Since $T_k(\\widetilde{L})$ is a symmetric matrix, its spectral norm is equal to its spectral radius, so $\\|T_k(\\widetilde{L})\\|_2 \\le 1$. This prevents the magnitudes of the matrices and vectors involved in the recurrence from growing exponentially, thus avoiding numerical overflow and instability.\n\nWithout rescaling, we would evaluate $T_k(\\lambda)$ for $\\lambda \\in [0, \\lambda_{\\max}]$. If $\\lambda_{\\max}  1$, for any eigenvalue $\\lambda_i  1$, $T_k(\\lambda_i)$ grows exponentially with $k$, leading to catastrophic numerical errors. The affine rescaling is therefore not merely a convenience but a fundamental requirement for a stable computation.", "answer": "$$\\boxed{\\sum_{k=0}^{K} \\left( \\frac{2-\\delta_{k0}}{\\pi} \\int_{0}^{\\lambda_{\\max}} \\frac{g(\\lambda') T_k\\left(\\frac{2\\lambda'}{\\lambda_{\\max}}-1\\right)}{\\sqrt{\\lambda'(\\lambda_{\\max}-\\lambda')}} d\\lambda' \\right) T_k\\left(\\frac{2}{\\lambda_{\\max}}L-I\\right)}$$", "id": "2903956"}]}