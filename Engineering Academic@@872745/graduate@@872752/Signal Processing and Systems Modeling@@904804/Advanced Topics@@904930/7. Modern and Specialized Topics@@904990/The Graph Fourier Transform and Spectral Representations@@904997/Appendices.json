{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of the entire Graph Fourier Transform (GFT) process. By starting with a simple 4-node graph, you will manually compute the graph Laplacian, find its complete spectral decomposition (eigenvalues and eigenvectors), and then use this basis to transform a given graph signal into the spectral domain. This hands-on calculation is essential for building a concrete understanding of how abstract graph structures are translated into a frequency-based representation [@problem_id:2912992].", "problem": "Consider a weighted, undirected graph on $4$ nodes with adjacency matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 0 & 1 & 1\\\\\n0 & 0 & 1 & 1\\\\\n1 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0\n\\end{pmatrix}.\n$$\nLet $D$ be the degree matrix and let $L$ be the combinatorial Laplacian defined by $L \\equiv D - A$. Using only standard definitions from spectral graph theory and linear algebra for real symmetric matrices, do the following:\n- Derive $L$ from $A$.\n- Compute the eigen-decomposition of $L$ as $L = U \\Lambda U^{\\top}$, where $U$ is an orthonormal matrix whose columns are eigenvectors of $L$ and $\\Lambda$ is diagonal with the eigenvalues of $L$ in nondecreasing order.\n- For the graph signal $x \\in \\mathbb{R}^{4}$ given by\n$$\nx \\;=\\; \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\ncompute its Graph Fourier Transform (GFT), defined as $\\widehat{x} \\equiv U^{\\top} x$.\n\nAs your final answer, provide the single real number equal to the total spectral energy contained in the eigenspace of $L$ associated with eigenvalue $2$, that is, the sum of the squared magnitudes of the GFT coefficients corresponding to eigenvalue $2$. No rounding is required. Report only this number as your final answer.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n-   A weighted, undirected graph on $4$ nodes.\n-   Adjacency matrix: $A = \\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\end{pmatrix}$.\n-   Degree matrix: $D$.\n-   Combinatorial Laplacian: $L \\equiv D - A$.\n-   Eigen-decomposition: $L = U \\Lambda U^{\\top}$, with $U$ being an orthonormal matrix of eigenvectors and $\\Lambda$ a diagonal matrix of eigenvalues in nondecreasing order.\n-   Graph signal: $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n-   Graph Fourier Transform (GFT): $\\widehat{x} \\equiv U^{\\top} x$.\n-   Objective: Compute the total spectral energy in the eigenspace of $L$ associated with eigenvalue $2$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard definitions from spectral graph theory. It is well-posed, providing all necessary information for a unique solution to the specified quantity. The adjacency matrix is symmetric, consistent with an undirected graph. All terms are defined with mathematical precision. The problem is a standard exercise in applying linear algebra to graph theory.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be derived.\n\nThe solution proceeds systematically.\n\nFirst, the degree matrix $D$ is derived from the adjacency matrix $A$. The degree of a node is the sum of its row (or column) entries in $A$.\nThe degrees of the nodes are:\n$d_1 = A_{11} + A_{12} + A_{13} + A_{14} = 0 + 0 + 1 + 1 = 2$.\n$d_2 = A_{21} + A_{22} + A_{23} + A_{24} = 0 + 0 + 1 + 1 = 2$.\n$d_3 = A_{31} + A_{32} + A_{33} + A_{34} = 1 + 1 + 0 + 0 = 2$.\n$d_4 = A_{41} + A_{42} + A_{43} + A_{44} = 1 + 1 + 0 + 0 = 2$.\nThe degree matrix $D$ is the diagonal matrix of these degrees:\n$$\nD = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} = 2I.\n$$\nThe combinatorial Laplacian $L$ is defined as $L = D - A$.\n$$\nL = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & -1 & -1 \\\\ 0 & 2 & -1 & -1 \\\\ -1 & -1 & 2 & 0 \\\\ -1 & -1 & 0 & 2 \\end{pmatrix}.\n$$\nNext, we compute the eigen-decomposition of $L$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(L - \\lambda I) = 0$.\n$$\n\\det(L - \\lambda I) = \\det \\begin{pmatrix} 2-\\lambda & 0 & -1 & -1 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nBy subtracting the second row from the first, we get:\n$$\n\\det \\begin{pmatrix} 2-\\lambda & -(2-\\lambda) & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix} = (2-\\lambda) \\det \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ -1 & -1 & 2-\\lambda & 0 \\\\ -1 & -1 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nAdding the first row to the third and fourth rows gives:\n$$\n(2-\\lambda) \\det \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ 0 & 2-\\lambda & -1 & -1 \\\\ 0 & -2 & 2-\\lambda & 0 \\\\ 0 & -2 & 0 & 2-\\lambda \\end{pmatrix} = (2-\\lambda) \\det \\begin{pmatrix} 2-\\lambda & -1 & -1 \\\\ -2 & 2-\\lambda & 0 \\\\ -2 & 0 & 2-\\lambda \\end{pmatrix}.\n$$\nThe determinant of the $3 \\times 3$ matrix is $(2-\\lambda)(2-\\lambda)^{2} - (-1)(-2(2-\\lambda)) + (-1)(0 - (-2)(2-\\lambda)) = (2-\\lambda)^{3} - 2(2-\\lambda) - 2(2-\\lambda) = (2-\\lambda)^{3} - 4(2-\\lambda) = (2-\\lambda)((2-\\lambda)^{2}-4)$.\nSo, the characteristic polynomial is $(2-\\lambda)(2-\\lambda-2)(2-\\lambda+2) = (2-\\lambda)(-\\lambda)(4-\\lambda) = \\lambda(\\lambda-2)^{2}(4-\\lambda)$.\nThe eigenvalues are $\\lambda=0$, $\\lambda=2$ (with multiplicity $2$), and $\\lambda=4$. In non-decreasing order: $\\lambda_1=0$, $\\lambda_2=2$, $\\lambda_3=2$, $\\lambda_4=4$.\nThe diagonal matrix of eigenvalues is $\\Lambda = \\text{diag}(0, 2, 2, 4)$.\n\nNow we find the corresponding orthonormal eigenvectors.\nFor $\\lambda_1 = 0$: $Lv=0$. This yields $v_1=v_2=v_3=v_4$. A normalized eigenvector is $u_1 = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^{\\top}$.\nFor $\\lambda_2=\\lambda_3 = 2$: $(L-2I)v=0$. This system reduces to $v_1+v_2=0$ and $v_3+v_4=0$. The eigenspace is two-dimensional. We can choose an orthonormal basis for this space. One such basis is:\n$u_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 & 0 & 0 \\end{pmatrix}^{\\top}$ and $u_3 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 & 0 & 1 & -1 \\end{pmatrix}^{\\top}$. These are orthogonal and have unit norm.\nFor $\\lambda_4 = 4$: $(L-4I)v=0$. This yields $v_1=v_2$ and $v_3=v_4=-v_1$. A normalized eigenvector is $u_4 = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & -1 & -1 \\end{pmatrix}^{\\top}$.\n\nThe matrix of eigenvectors $U$ is formed by these column vectors:\n$$\nU = \\begin{pmatrix} u_1 & u_2 & u_3 & u_4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{2} \\\\ \\frac{1}{2} & 0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{2} \\\\ \\frac{1}{2} & 0 & -\\frac{1}{\\sqrt{2}} & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe Graph Fourier Transform of the signal $x = \\begin{pmatrix} 1 & 2 & -1 & 0 \\end{pmatrix}^{\\top}$ is $\\widehat{x} = U^{\\top}x$. The components of $\\widehat{x}$ are $\\widehat{x}_i = u_i^{\\top}x$.\n$\\widehat{x}_1 = u_1^{\\top}x = \\frac{1}{2}(1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot (-1) + 1 \\cdot 0) = \\frac{1}{2}(2) = 1$.\n$\\widehat{x}_2 = u_2^{\\top}x = \\frac{1}{\\sqrt{2}}(1 \\cdot 1 - 1 \\cdot 2 + 0 \\cdot (-1) + 0 \\cdot 0) = \\frac{1}{\\sqrt{2}}(-1) = -\\frac{1}{\\sqrt{2}}$.\n$\\widehat{x}_3 = u_3^{\\top}x = \\frac{1}{\\sqrt{2}}(0 \\cdot 1 + 0 \\cdot 2 + 1 \\cdot (-1) - 1 \\cdot 0) = \\frac{1}{\\sqrt{2}}(-1) = -\\frac{1}{\\sqrt{2}}$.\n$\\widehat{x}_4 = u_4^{\\top}x = \\frac{1}{2}(1 \\cdot 1 + 1 \\cdot 2 - 1 \\cdot (-1) - 1 \\cdot 0) = \\frac{1}{2}(1+2+1) = \\frac{4}{2} = 2$.\nThe GFT is $\\widehat{x} = \\begin{pmatrix} 1 & -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 2 \\end{pmatrix}^{\\top}$.\n\nThe problem requires the total spectral energy in the eigenspace associated with eigenvalue $2$. This corresponds to the GFT coefficients $\\widehat{x}_2$ and $\\widehat{x}_3$. The energy is the sum of their squared magnitudes.\nLet $E_2$ be the energy for $\\lambda=2$.\n$$\nE_2 = |\\widehat{x}_2|^2 + |\\widehat{x}_3|^2.\n$$\nSubstituting the computed values:\n$$\nE_2 = \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 = \\frac{1}{2} + \\frac{1}{2} = 1.\n$$\nThis quantity is unique regardless of the specific choice of orthonormal basis for the eigenspace of the repeated eigenvalue.", "answer": "$$\\boxed{1}$$", "id": "2912992"}, {"introduction": "Having established the mechanics of the GFT, we now explore a more nuanced question: what does it mean for a signal to be \"smooth\" on a graph? This practice confronts the fact that different graph operators, like the Laplacian ($L$) and the adjacency matrix ($A$), induce different notions of frequency and smoothness. By analyzing two signals on a path graph, you will find that their smoothness ranking can be inverted depending on whether the Laplacian quadratic form ($x^{\\top}Lx$) or an adjacency-based variation ($TV_A(x)$) is used, highlighting that the concept of graph frequency is relative to the chosen spectral basis [@problem_id:2913015].", "problem": "Consider an undirected weighted graph with symmetric adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ and combinatorial graph Laplacian $L = D - A$, where $D$ is the diagonal degree matrix. Two standard smoothness measures for a graph signal $z \\in \\mathbb{R}^{n}$ are:\n- The adjacency-based total variation relative to $A$, defined as $TV_{A}(z) \\triangleq \\left\\| z - \\lambda_{\\max}(A)^{-1} A z \\right\\|_{1}$, where $\\lambda_{\\max}(A)$ is the spectral radius of $A$.\n- The Laplacian Dirichlet energy, defined as $z^{\\top} L z$.\n\nYour goal is to analyze how these two measures can disagree in smoothness ordering and explain why this occurs from a spectral perspective consistent with the Graph Fourier Transform (GFT), where eigenvectors of the chosen graph operator (either $A$ or $L$) form a frequency basis.\n\nWork with the path graph on $n=3$ vertices with unit edge weights. Let the vertices be $\\{1,2,3\\}$, with edges between $1$ and $2$, and between $2$ and $3$. Consider the two graph signals $x = [1,\\sqrt{2},1]^{\\top}$ and $y = [1,1,1]^{\\top}$.\n\nTasks:\n1. Starting from the definitions of $A$ and $L$, compute $\\lambda_{\\max}(A)$ and verify whether $x$ is an eigenvector of $A$. Then compute $TV_{A}(x)$ and $TV_{A}(y)$ explicitly.\n2. Starting from $L = D - A$ and fundamental properties of quadratic forms, compute $x^{\\top} L x$ and $y^{\\top} L y$ explicitly, justifying each step from first principles.\n3. Determine which signal is smoother under $TV_{A}(\\cdot)$ and which is smoother under $z^{\\top} L z$, and explain the disagreement by appealing to operator-dependent spectral interpretations of smoothness in the Graph Fourier Transform (GFT).\n4. Finally, define the scalar\n$$\nR \\triangleq \\frac{TV_{A}(y) - TV_{A}(x)}{x^{\\top} L x - y^{\\top} L y}.\n$$\nCompute $R$ exactly and simplify your result.\n\nProvide your final answer as the exact, fully simplified value of $R$ (no rounding or decimal approximations). No units are required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard exercise in graph signal processing that is verifiable through direct computation and established theory. Therefore, validation is successful and a solution will be provided.\n\nThe analysis proceeds by addressing the four specified tasks in sequence.\n\nFirst, we construct the necessary graph matrices for the path graph on $n=3$ vertices with unit edge weights. The vertices are $\\{1, 2, 3\\}$ and edges are $(1,2)$ and $(2,3)$. The adjacency matrix $A$ is given by its elements $A_{ij} = 1$ if an edge exists between vertices $i$ and $j$, and $A_{ij}=0$ otherwise. This yields:\n$$\nA = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe degree matrix $D$ is a diagonal matrix where $D_{ii}$ is the degree of vertex $i$, which is the sum of weights of edges connected to it. We have $d_1 = 1$, $d_2=2$, and $d_3=1$.\n$$\nD = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe combinatorial graph Laplacian is $L = D - A$.\n$$\nL = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}\n$$\n\nTask 1: Compute $\\lambda_{\\max}(A)$, verify if $x$ is an eigenvector of $A$, and compute total variations.\nTo find the spectral radius $\\lambda_{\\max}(A)$, we first find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} -\\lambda & 1 & 0 \\\\ 1 & -\\lambda & 1 \\\\ 0 & 1 & -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2 - 1) - 1(-\\lambda) = -\\lambda^3 + \\lambda + \\lambda = 2\\lambda - \\lambda^3 = \\lambda(2 - \\lambda^2) = 0\n$$\nThe eigenvalues are $\\lambda_1 = \\sqrt{2}$, $\\lambda_2 = 0$, and $\\lambda_3 = -\\sqrt{2}$. The spectral radius of $A$, which is the maximum absolute value of its eigenvalues, is $\\lambda_{\\max}(A) = \\max(|\\sqrt{2}|, |0|, |-\\sqrt{2}|) = \\sqrt{2}$.\n\nNext, we test if the signal $x = [1, \\sqrt{2}, 1]^{\\top}$ is an eigenvector of $A$. This requires showing $Ax = \\lambda x$ for some scalar $\\lambda$.\n$$\nAx = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\cdot 1 + 1 \\cdot \\sqrt{2} + 0 \\cdot 1 \\\\ 1 \\cdot 1 + 0 \\cdot \\sqrt{2} + 1 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot \\sqrt{2} + 0 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix}\n$$\nWe observe that $Ax = \\begin{pmatrix} \\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix} = \\sqrt{2} \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix} = \\sqrt{2}x$. Thus, $x$ is an eigenvector of $A$ corresponding to the eigenvalue $\\sqrt{2}$, which is $\\lambda_{\\max}(A)$.\n\nNow we compute the adjacency-based total variation $TV_{A}(z) = \\| z - \\lambda_{\\max}(A)^{-1} A z \\|_{1}$ for signals $x$ and $y$.\nFor $z=x$:\nSince $Ax = \\lambda_{\\max}(A) x$, we have:\n$$\nTV_{A}(x) = \\left\\| x - \\lambda_{\\max}(A)^{-1} (\\lambda_{\\max}(A) x) \\right\\|_{1} = \\|x - x\\|_{1} = \\|\\mathbf{0}\\|_{1} = 0\n$$\nFor $z=y = [1, 1, 1]^{\\top}$:\nFirst, compute $Ay$:\n$$\nAy = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\nThen compute the vector inside the $L_1$-norm:\n$$\ny - \\lambda_{\\max}(A)^{-1} A y = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{\\sqrt{2}} \\\\ 1 - \\frac{2}{\\sqrt{2}} \\\\ 1 - \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\sqrt{2}}{2} \\\\ 1 - \\sqrt{2} \\\\ 1 - \\frac{\\sqrt{2}}{2} \\end{pmatrix}\n$$\nThe $L_1$-norm is the sum of the absolute values of the components. Since $\\sqrt{2} \\approx 1.414$, we have $1 - \\frac{\\sqrt{2}}{2} > 0$ and $1 - \\sqrt{2} < 0$.\n$$\nTV_{A}(y) = \\left|1 - \\frac{\\sqrt{2}}{2}\\right| + |1 - \\sqrt{2}| + \\left|1 - \\frac{\\sqrt{2}}{2}\\right| = \\left(1 - \\frac{\\sqrt{2}}{2}\\right) + (\\sqrt{2} - 1) + \\left(1 - \\frac{\\sqrt{2}}{2}\\right)\n$$\n$$\nTV_{A}(y) = (1 - 1 + 1) + \\left(-\\frac{\\sqrt{2}}{2} + \\sqrt{2} - \\frac{\\sqrt{2}}{2}\\right) = 1 + (\\sqrt{2} - \\sqrt{2}) = 1\n$$\n\nTask 2: Compute the Laplacian Dirichlet energies $x^{\\top} L x$ and $y^{\\top} L y$.\nThe Dirichlet energy is the quadratic form $z^{\\top}Lz$, which for a graph with unit edge weights is equivalent to the sum of squared differences across edges: $z^{\\top}Lz = \\sum_{(i,j) \\in E} (z_i - z_j)^2$. The edges are $(1,2)$ and $(2,3)$.\nFor $z=x = [1, \\sqrt{2}, 1]^{\\top}$:\n$$\nx^{\\top} L x = (x_1 - x_2)^2 + (x_2 - x_3)^2 = (1 - \\sqrt{2})^2 + (\\sqrt{2} - 1)^2 = 2(1 - \\sqrt{2})^2\n$$\n$$\nx^{\\top} L x = 2(1^2 - 2\\sqrt{2} + (\\sqrt{2})^2) = 2(1 - 2\\sqrt{2} + 2) = 2(3 - 2\\sqrt{2}) = 6 - 4\\sqrt{2}\n$$\nFor $z=y = [1, 1, 1]^{\\top}$:\n$$\ny^{\\top} L y = (y_1 - y_2)^2 + (y_2 - y_3)^2 = (1 - 1)^2 + (1 - 1)^2 = 0^2 + 0^2 = 0\n$$\nThis result is expected, as the constant vector $y$ is always in the null space of the Laplacian $L$ for a connected graph, corresponding to the eigenvalue $0$. Thus, $Ly = \\mathbf{0}$ and $y^{\\top}Ly=0$.\n\nTask 3: Compare smoothness orderings and explain the disagreement.\nAccording to the adjacency-based total variation, we have $TV_{A}(x) = 0$ and $TV_{A}(y) = 1$. Since $TV_{A}(x) < TV_{A}(y)$, signal $x$ is considered smoother than signal $y$.\nAccording to the Laplacian Dirichlet energy, we have $x^{\\top}Lx = 6 - 4\\sqrt{2} \\approx 0.343$ and $y^{\\top}Ly = 0$. Since $y^{\\top}Ly < x^{\\top}Lx$, signal $y$ is considered smoother than signal $x$.\nThe smoothness orderings are inverted.\n\nThe disagreement arises from the different spectral properties of the operators $A$ and $L$, which form the basis for the two different Graph Fourier Transforms (GFTs).\nThe GFT defines graph frequencies based on the eigenvalues of a chosen graph operator (e.g., $L$ or $A$), and the corresponding eigenvectors serve as the basis signals (Fourier modes).\n1.  **Laplacian-based GFT**: The eigenvalues of $L$ are non-negative and interpreted as frequencies squared. A small eigenvalue corresponds to a low-frequency (smooth) mode. The smallest eigenvalue is $\\lambda_1(L) = 0$, and its corresponding eigenvector is the constant vector, here $y = [1, 1, 1]^{\\top}$. A signal with low Dirichlet energy $z^{\\top}Lz = \\sum_i \\lambda_i(L) |\\hat{z}_i|^2$ is smooth because its energy is concentrated in low-frequency modes. Signal $y$ is the \"DC component,\" the smoothest possible signal in this framework, hence $y^{\\top}Ly=0$. Signal $x$ is not the DC component and has non-zero energy, thus it is less smooth than $y$.\n2.  **Adjacency-based GFT**: The smoothness measure $TV_A(z)$ is constructed such that signals aligned with the eigenvector corresponding to $\\lambda_{\\max}(A)$ are considered smoothest. A signal $z$ is perfectly smooth if $z$ is an eigenvector of $A$ with eigenvalue $\\lambda_{\\max}(A)$, which results in $TV_A(z) = 0$. We have verified that $x$ is precisely this eigenvector for our graph, hence $TV_A(x)=0$. In this framework, $x$ represents the smoothest mode. Signal $y$ is not this eigenvector, and thus has a non-zero total variation, making it less smooth than $x$.\n\nIn summary, the contradiction is not a paradox. It is a direct consequence of defining \"smoothness\" with respect to different operators whose spectral decompositions (eigenvectors and eigenvalues) are different. The concept of \"frequency\" on a graph is not absolute; it is relative to the chosen operator.\n\nTask 4: Compute the scalar $R$.\nThe scalar is defined as:\n$$\nR = \\frac{TV_{A}(y) - TV_{A}(x)}{x^{\\top} L x - y^{\\top} L y}\n$$\nSubstituting the values computed in the previous tasks:\n$$\nR = \\frac{1 - 0}{(6 - 4\\sqrt{2}) - 0} = \\frac{1}{6 - 4\\sqrt{2}}\n$$\nTo simplify this expression, we rationalize the denominator:\n$$\nR = \\frac{1}{6 - 4\\sqrt{2}} \\cdot \\frac{6 + 4\\sqrt{2}}{6 + 4\\sqrt{2}} = \\frac{6 + 4\\sqrt{2}}{6^2 - (4\\sqrt{2})^2} = \\frac{6 + 4\\sqrt{2}}{36 - (16 \\cdot 2)} = \\frac{6 + 4\\sqrt{2}}{36 - 32} = \\frac{6 + 4\\sqrt{2}}{4}\n$$\nFinally, simplifying the fraction:\n$$\nR = \\frac{6}{4} + \\frac{4\\sqrt{2}}{4} = \\frac{3}{2} + \\sqrt{2}\n$$\nThis is the exact, simplified value for $R$.", "answer": "$$\\boxed{\\frac{3}{2} + \\sqrt{2}}$$", "id": "2913015"}, {"introduction": "The true power of spectral graph theory lies in its applications, such as filtering, but direct computation of the GFT is intractable for large-scale networks. This final practice bridges theory and practice by guiding you through the design of a scalable graph filtering pipeline that avoids costly eigen-decompositions. You will leverage the properties of Chebyshev polynomials to approximate a spectral filter, implementing a method that relies only on efficient sparse matrix-vector multiplications and is suitable for graphs with millions of nodes [@problem_id:2913014].", "problem": "Design and implement a complete, runnable program that constructs a scalable pipeline to apply a Chebyshev filter of order $K$ to a graph signal $x$ on a large, sparse, undirected graph, using only sparse matrix-vector multiplications and constant working memory per iteration. The derivation must start from the following fundamental base: the unnormalized combinatorial graph Laplacian $L = D - A$ for an undirected graph with adjacency matrix $A$ and degree matrix $D$, the spectral representation of linear graph filters via the Graph Fourier Transform (GFT), and the defining properties of Chebyshev polynomials as an orthogonal basis on the interval $\\left[-1, 1\\right]$. From these base definitions, derive a numerically stable three-term recurrence that enables evaluating a polynomial graph filter of degree $K$ on $x$ without computing any eigen-decomposition, and justify how to rescale the spectrum of $L$ to the interval $\\left[-1, 1\\right]$ using an estimate of the largest eigenvalue $\\lambda_{\\max}$. Your pipeline must:\n- Use only sparse matrix-vector products with $L$ and $O(N)$ auxiliary memory for vectors in $\\mathbb{R}^N$, where $N$ is the number of nodes.\n- Accept a filter specified by coefficients $\\{c_k\\}_{k=0}^K$ and output the filtered signal $y \\in \\mathbb{R}^N$.\n- Be robust to boundary cases such as $K = 0$.\n- Justify how $\\lambda_{\\max}$ is to be obtained without dense factorizations on large graphs, while still permitting exactly computed references for small test graphs.\n\nYour program must implement the pipeline and validate it against a spectral reference computed by dense eigen-decomposition on small graphs. For each test case, compute the filtered signal $y$ in two ways:\n- The scalable Chebyshev pipeline using the three-term recurrence on the rescaled Laplacian, with the largest eigenvalue $\\lambda_{\\max}$ provided exactly for the test graphs.\n- A dense spectral reference using the GFT, by applying the same polynomial in the eigen-domain and transforming back.\n\nFor each test case, report a boolean indicating whether the relative error $\\|y_{\\text{cheb}} - y_{\\text{spec}}\\|_2 / \\max\\{\\|y_{\\text{spec}}\\|_2, \\epsilon\\}$ is less than a tolerance $\\tau$, where $\\epsilon = 10^{-14}$ and $\\tau = 10^{-8}$. All Euclidean norms are the standard $\\ell_2$ norm.\n\nTest suite specification:\n- In all tests, the Laplacian is the unnormalized combinatorial Laplacian $L = D - A$ of an undirected, unweighted graph with no self-loops, and the adjacency matrix $A$ is symmetric with binary entries. The graph signal $x$ must be generated as independent and identically distributed (IID) standard normal entries, with the specified pseudo-random seed per test.\n- Test case $1$ (happy path):\n  - Graph: path graph with $n = 12$ nodes.\n  - Order: $K = 5$.\n  - Coefficients: $c = [0.7, -0.4, 0.3, 0.0, -0.05, 0.01]$.\n  - Signal: seed $0$.\n- Test case $2$ (boundary case $K = 0$):\n  - Graph: star graph with $n = 37$ nodes (one hub connected to all others).\n  - Order: $K = 0$.\n  - Coefficients: $c = [2.3]$.\n  - Signal: seed $1$.\n- Test case $3$ (spectrum with multiplicities):\n  - Graph: disjoint union of two cycles (rings) with sizes $n_1 = 15$ and $n_2 = 15$ (total $n = 30$).\n  - Order: $K = 4$.\n  - Coefficients: $c = [1.0, -0.2, 0.05, -0.01, 0.002]$.\n  - Signal: seed $2$.\n- Test case $4$ (high-degree hub, larger $K$):\n  - Graph: star graph with $n = 60$ nodes.\n  - Order: $K = 7$.\n  - Coefficients: $c = [0.5, -0.3, 0.2, -0.1, 0.05, -0.02, 0.01, -0.005]$.\n  - Signal: seed $3$.\n- Test case $5$ (random sparse graph):\n  - Graph: ErdÅ‘sâ€“RÃ©nyi random graph $G(n, p)$ with $n = 120$ nodes and edge probability $p = 0.05$, undirected, no self-loops, generated deterministically with the specified seed.\n  - Order: $K = 6$.\n  - Coefficients: $c = [0.4, 0.1, -0.08, 0.06, -0.04, 0.02, -0.01]$.\n  - Signal: seed $4$.\n\nImplementation constraints:\n- Use sparse matrix operations for the scalable pipeline.\n- For the dense spectral reference, compute the full eigen-decomposition of $L$ as a dense matrix to obtain the exact $\\lambda_{\\max}$ and eigenvectors.\n- For each test case, both methods must use the same exact $\\lambda_{\\max}$ for the spectrum rescaling.\n- All random number generation must use a pseudo-random number generator with the specified seeds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean corresponding to the respective test case in the order listed above. No other text should be printed.", "solution": "The problem requires the derivation and implementation of a scalable pipeline for applying a graph filter to a signal, based on a Chebyshev polynomial approximation. The solution must be derived from fundamental principles and validated against a direct spectral method.\n\n**1. Fundamentals of Spectral Graph Filtering**\n\nA graph is denoted by $G=(V, E)$, with $N=|V|$ vertices. A signal on this graph is a function $x: V \\to \\mathbb{R}$, represented by a vector $x \\in \\mathbb{R}^N$. The graph structure is described by its adjacency matrix $A$ and degree matrix $D$. The unnormalized combinatorial Laplacian is defined as $L = D - A$. For an undirected graph, $L$ is a real, symmetric, positive semi-definite matrix.\n\nAs $L$ is real and symmetric, it admits a complete set of orthonormal eigenvectors, which form the columns of an orthogonal matrix $U$. The spectral decomposition of $L$ is $L = U \\Lambda U^T$, where $\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_N)$ is the diagonal matrix of corresponding non-negative, real eigenvalues, $0 \\le \\lambda_1 \\le \\dots \\le \\lambda_N$. The matrix $U$ defines the Graph Fourier Transform (GFT). The GFT of a signal $x$ is $\\hat{x} = U^T x$, and the inverse GFT is $x = U \\hat{x}$.\n\nA linear, shift-invariant graph filter is defined as a matrix function of the Laplacian, $H = h(L)$. Applying this filter to a signal $x$ yields the output $y = Hx = h(L)x$. In the spectral domain, this operation becomes a simple multiplication:\n$$y = h(L)x = U h(\\Lambda) U^T x = U \\hat{y}$$\nwhere $\\hat{y}_i = h(\\lambda_i) \\hat{x}_i$. The function $h(\\cdot)$ is the filter's frequency response, evaluated at the graph frequencies (eigenvalues) $\\lambda_i$. This approach, while exact, is computationally prohibitive for large graphs as it requires the full eigen-decomposition of $L$, an $O(N^3)$ operation. This method serves as our **dense spectral reference**.\n\n**2. Scalable Filtering with Chebyshev Polynomials**\n\nTo create a scalable method, we avoid eigen-decomposition by approximating the filter response $h(\\lambda)$ with a polynomial of finite order $K$. A numerically stable and efficient choice for this approximation is the basis of Chebyshev polynomials of the first kind, $T_k(z)$. These polynomials are defined on the interval $z \\in [-1, 1]$ and satisfy the three-term recurrence relation:\n$$T_0(z) = 1, \\quad T_1(z) = z, \\quad T_{k+1}(z) = 2zT_k(z) - T_{k-1}(z) \\text{ for } k \\ge 1$$\nThe filter response is thus approximated by $h(\\lambda) \\approx \\sum_{k=0}^{K} c_k T_k(\\tilde{\\lambda})$, where $\\{c_k\\}_{k=0}^K$ are the given filter coefficients and $\\tilde{\\lambda}$ is the eigenvalue $\\lambda$ rescaled to the domain $[-1, 1]$.\n\n**3. Spectrum Rescaling and the Three-Term Recurrence**\n\nThe eigenvalues of the unnormalized Laplacian $L$ lie in the interval $[0, \\lambda_{\\max}]$. To use the Chebyshev basis, we must apply a linear transformation to map this interval to $[-1, 1]$. The standard mapping is:\n$$\\tilde{\\lambda} = \\frac{2\\lambda}{\\lambda_{\\max}} - 1$$\nThis maps $\\lambda=0$ to $\\tilde{\\lambda}=-1$ and $\\lambda=\\lambda_{\\max}$ to $\\tilde{\\lambda}=1$. The corresponding rescaled Laplacian *operator* is $\\tilde{L} = \\frac{2}{\\lambda_{\\max}}L - I$, where $I$ is the identity matrix. The eigenvalues of $\\tilde{L}$ are precisely $\\tilde{\\lambda}_i$, which lie in $[-1, 1]$.\n\nThe filtered signal $y$ can now be computed as:\n$$y = h(L)x \\approx \\left( \\sum_{k=0}^K c_k T_k(\\tilde{L}) \\right) x = \\sum_{k=0}^K c_k \\left( T_k(\\tilde{L})x \\right)$$\nLet us define a sequence of vectors $\\bar{x}_k = T_k(\\tilde{L})x$. We can compute this sequence efficiently by leveraging the Chebyshev recurrence relation:\n\\begin{itemize}\n    \\item $\\bar{x}_0 = T_0(\\tilde{L})x = I x = x$\n    \\item $\\bar{x}_1 = T_1(\\tilde{L})x = \\tilde{L}x = \\left(\\frac{2}{\\lambda_{\\max}}L - I\\right)x$\n    \\item For $k \\ge 1$, $\\bar{x}_{k+1} = T_{k+1}(\\tilde{L})x = (2\\tilde{L}T_k(\\tilde{L}) - T_{k-1}(\\tilde{L}))x = 2\\tilde{L}\\bar{x}_k - \\bar{x}_{k-1}$\n\\end{itemize}\nThis results in a scalable algorithm. The final signal $y$ is constructed as the linear combination $y = \\sum_{k=0}^K c_k \\bar{x}_k$. The computation involves a sequence of sparse matrix-vector products with $L$ (within the application of $\\tilde{L}$), vector additions, and scalar multiplications. For a filter of order $K$, this requires $K$ such products. The auxiliary memory is $O(N)$ to store the vectors $\\bar{x}_{k-1}$, $\\bar{x}_k$, and the resulting signal $y$, independent of $K$. This algorithm constitutes our **scalable Chebyshev pipeline**.\n\nThe boundary case $K=0$ is handled naturally. The filter is $y=c_0 T_0(\\tilde{L})x=c_0x$.\n\n**4. Estimation of the Largest Eigenvalue $\\lambda_{\\max}$**\n\nFor the provided test cases on small graphs, $\\lambda_{\\max}$ is computed exactly from the full spectrum of the dense Laplacian matrix. However, on large graphs where this is infeasible, $\\lambda_{\\max}$ must be estimated. A standard and effective method is the **Power Iteration algorithm**. It iteratively computes $v_{i+1} = Lv_i / \\|Lv_i\\|_2$, starting from a random vector $v_0$. The vector $v_i$ converges to the eigenvector corresponding to the eigenvalue with the largest magnitude. Since $L$ is positive semi-definite, this is $\\lambda_{\\max}$. After a sufficient number of iterations, $\\lambda_{\\max}$ can be estimated using the Rayleigh quotient: $\\lambda_{\\max} \\approx v^T L v / (v^T v)$. This estimation relies only on sparse matrix-vector products, aligning with the scalability requirement. For numerical stability of the Chebyshev expansion, it is sufficient to use an upper bound, $\\lambda_{\\text{est}} \\ge \\lambda_{\\max}$, which ensures the rescaled spectrum lies within $[-1, 1]$.\n\n**Summary of Implemented Methods**\n\n1.  **Scalable Chebyshev Pipeline ($y_{\\text{cheb}}$)**:\n    - Input: Sparse Laplacian $L$, signal $x$, coefficients $\\{c_k\\}_{k=0}^K$, $\\lambda_{\\max}$.\n    - Algorithm: Uses the three-term recurrence.\n    - Complexity: $O(K \\cdot \\text{nnz}(L))$ time, $O(N)$ auxiliary space.\n\n2.  **Dense Spectral Reference ($y_{\\text{spec}}$)**:\n    - Input: Dense Laplacian $L$, signal $x$, coefficients $\\{c_k\\}_{k=0}^K$, $\\lambda_{\\max}$.\n    - Algorithm: Computes full eigen-decomposition of $L$, applies the filter in the spectral domain, and transforms back. The filter response $h(\\lambda_i)$ for each eigenvalue is evaluated by summing the Chebyshev polynomial series.\n    - Complexity: $O(N^3)$ time, $O(N^2)$ space.\n\nThe relative error $\\|y_{\\text{cheb}} - y_{\\text{spec}}\\|_2 / \\max\\{\\|y_{\\text{spec}}\\|_2, \\epsilon\\}$ is then computed to validate the pipeline's correctness against the spectral ground truth.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.linalg import eigh\nimport collections\n\n# Global constants for validation\n_TOLERANCE = 1e-8\n_EPSILON = 1e-14\n\ndef make_path_graph(n):\n    \"\"\"Creates the adjacency matrix for a path graph with n nodes.\"\"\"\n    A = diags([1, 1], [-1, 1], shape=(n, n)).toarray()\n    return A\n\ndef make_star_graph(n):\n    \"\"\"Creates the adjacency matrix for a star graph with n nodes.\"\"\"\n    A = np.zeros((n, n))\n    A[0, 1:] = 1\n    A[1:, 0] = 1\n    return A\n\ndef make_disjoint_cycles_graph(n1, n2):\n    \"\"\"Creates adjacency matrix for a graph of two disjoint cycles.\"\"\"\n    n = n1 + n2\n    A = np.zeros((n, n))\n    # Cycle 1\n    for i in range(n1):\n        A[i, (i + 1) % n1] = 1\n        A[(i + 1) % n1, i] = 1\n    # Cycle 2\n    for i in range(n2):\n        A[n1 + i, n1 + (i + 1) % n2] = 1\n        A[n1 + (i + 1) % n2, n1 + i] = 1\n    return A\n\ndef make_er_graph(n, p, seed):\n    \"\"\"Creates an Erdos-Renyi G(n,p) random graph.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.random((n, n))  p\n    A = np.triu(A, 1)  # Take upper triangle to make it undirected\n    A = A + A.T\n    return A.astype(float)\n\ndef get_laplacian(A):\n    \"\"\"Computes the unnormalized graph Laplacian L = D - A.\"\"\"\n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n    return L\n\ndef chebyshev_pipeline(L_sparse, x, coeffs, K, lambda_max):\n    \"\"\"\n    Computes the filtered signal y = h(L)x using a Chebyshev polynomial expansion.\n    h(L) is approximated by sum_{k=0 to K} c_k T_k(L_tilde).\n    This implementation is scalable and uses only sparse matrix-vector products.\n    \"\"\"\n    n = x.shape[0]\n    K_from_coeffs = len(coeffs) - 1\n    if K != K_from_coeffs:\n        raise ValueError(\"Filter order K does not match number of coefficients.\")\n\n    if K == 0:\n        return coeffs[0] * x\n\n    # T_0(L_tilde)x\n    T_k_minus_2 = x\n    y = coeffs[0] * T_k_minus_2\n\n    # L_tilde @ v = (2/lambda_max) * (L @ v) - v\n    # T_1(L_tilde)x\n    T_k_minus_1 = (2.0 / lambda_max) * (L_sparse @ x) - x\n    y += coeffs[1] * T_k_minus_1\n\n    for k in range(2, K + 1):\n        # L_tilde @ T_{k-1}\n        L_tilde_T_k_minus_1 = (2.0 / lambda_max) * (L_sparse @ T_k_minus_1) - T_k_minus_1\n        # T_k = 2 * L_tilde * T_{k-1} - T_{k-2}\n        T_k = 2.0 * L_tilde_T_k_minus_1 - T_k_minus_2\n        \n        y += coeffs[k] * T_k\n        \n        T_k_minus_2, T_k_minus_1 = T_k_minus_1, T_k\n        \n    return y\n\ndef eval_cheb_poly_scalar(z, coeffs):\n    \"\"\"Evaluates a Chebyshev polynomial sum for a scalar z.\"\"\"\n    K = len(coeffs) - 1\n    if K == 0:\n        return np.array(coeffs[0], dtype=float)\n\n    T_k_minus_2 = 1.0\n    val = coeffs[0] * T_k_minus_2\n\n    T_k_minus_1 = z\n    val += coeffs[1] * T_k_minus_1\n\n    for k in range(2, K + 1):\n        T_k = 2.0 * z * T_k_minus_1 - T_k_minus_2\n        val += coeffs[k] * T_k\n        T_k_minus_2, T_k_minus_1 = T_k_minus_1, T_k\n        \n    return val\n\ndef spectral_reference(L_dense, x, coeffs, K, lambda_max):\n    \"\"\"\n    Computes the filtered signal using the GFT (dense eigen-decomposition).\n    This serves as the ground truth for validation.\n    \"\"\"\n    K_from_coeffs = len(coeffs) - 1\n    if K != K_from_coeffs:\n        raise ValueError(\"Filter order K does not match number of coefficients.\")\n        \n    eigvals, U = eigh(L_dense)\n    \n    # Rescale eigenvalues to [-1, 1]\n    eigvals_tilde = (2.0 / lambda_max) * eigvals - 1.0\n    \n    # Evaluate filter response on eigenvalues\n    h_lambda = eval_cheb_poly_scalar(eigvals_tilde, coeffs)\n        \n    # Apply filter in spectral domain\n    x_hat = U.T @ x\n    y_hat = h_lambda * x_hat\n    \n    # Inverse GFT to get the filtered signal\n    y = U @ y_hat\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and validate the Chebyshev pipeline.\n    \"\"\"\n    TestCase = collections.namedtuple('TestCase', ['graph_func', 'params', 'K', 'coeffs', 'seed'])\n    \n    test_cases = [\n        TestCase(make_path_graph, (12,), 5, [0.7, -0.4, 0.3, 0.0, -0.05, 0.01], 0),\n        TestCase(make_star_graph, (37,), 0, [2.3], 1),\n        TestCase(make_disjoint_cycles_graph, (15, 15), 4, [1.0, -0.2, 0.05, -0.01, 0.002], 2),\n        TestCase(make_star_graph, (60,), 7, [0.5, -0.3, 0.2, -0.1, 0.05, -0.02, 0.01, -0.005], 3),\n        TestCase(make_er_graph, (120, 0.05, 4), 6, [0.4, 0.1, -0.08, 0.06, -0.04, 0.02, -0.01], 4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate graph and Laplacian\n        A = case.graph_func(*case.params)\n        n = A.shape[0]\n        L_dense = get_laplacian(A)\n        L_sparse = csr_matrix(L_dense)\n\n        # 2. Generate signal\n        rng = np.random.default_rng(case.seed)\n        x = rng.standard_normal(n)\n\n        # 3. Get exact lambda_max from dense matrix for validation\n        eigvals = eigh(L_dense, eigvals_only=True)\n        lambda_max = np.max(eigvals)\n        if lambda_max  1e-9: # Handle disconnected or single-node graphs\n            lambda_max = 1.0\n\n        # 4. Compute with a scalable pipeline\n        y_cheb = chebyshev_pipeline(L_sparse, x, case.coeffs, case.K, lambda_max)\n\n        # 5. Compute with spectral reference method\n        y_spec = spectral_reference(L_dense, x, case.coeffs, case.K, lambda_max)\n        \n        # 6. Validate\n        norm_y_spec = np.linalg.norm(y_spec)\n        norm_diff = np.linalg.norm(y_cheb - y_spec)\n        \n        relative_error = norm_diff / max(norm_y_spec, _EPSILON)\n        \n        results.append(relative_error  _TOLERANCE)\n\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "2913014"}]}