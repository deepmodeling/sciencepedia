{"hands_on_practices": [{"introduction": "The concept of orthogonality is the geometric heart of optimal linear estimation. This foundational exercise grounds the theory in a concrete calculation, connecting the abstract principle to the algebraic Wiener-Hopf equations. By deriving these equations and solving for the optimal Wiener filter in a simple two-dimensional case, you will gain a tangible understanding of how the minimum mean-square error ($J_{\\min}$) is achieved when the estimation error vector is perpendicular to the input data space [@problem_id:2850274].", "problem": "Consider a real-valued, zero-mean, jointly wide-sense stationary (WSS) pair of random vectors and processes where the goal is to estimate a scalar desired random variable $d$ from a two-dimensional regressor $\\mathbf{x} \\in \\mathbb{R}^{2}$ using a linear estimator $y = \\mathbf{w}^{\\top}\\mathbf{x}$. Let the mean-square error (MSE) cost be defined by\n$$\nJ(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right],\n$$\nand define the input correlation matrix, cross-correlation vector, and desired power as\n$$\n\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}], \\quad \\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d], \\quad \\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}],\n$$\nrespectively. Using only fundamental definitions and the orthogonality principle, derive the normal equations that the optimal Wiener solution $\\mathbf{w}^{\\star}$ must satisfy, solve explicitly for $\\mathbf{w}^{\\star}$ given\n$$\n\\mathbf{R}=\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n$$\nand compute the minimum achievable MSE,\n$$\nJ_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w}),\n$$\nin closed form as a function of $\\sigma_{d}^{2}$. Provide your final answer as a single row containing the two components of $\\mathbf{w}^{\\star}$ followed by $J_{\\min}$, in exact form with no rounding. The answer is unitless and no angle units are involved.", "solution": "The problem is subjected to rigorous validation before any attempt at a solution is made.\n\nThe givens are extracted verbatim from the problem statement:\n- A real-valued, zero-mean scalar random variable $d$.\n- A real-valued, zero-mean two-dimensional random vector $\\mathbf{x} \\in \\mathbb{R}^{2}$.\n- The pair $(d, \\mathbf{x})$ is jointly wide-sense stationary (WSS).\n- The linear estimator is $y = \\mathbf{w}^{\\top}\\mathbf{x}$.\n- The mean-square error (MSE) cost function is $J(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right]$.\n- The input correlation matrix is $\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$.\n- The cross-correlation vector is $\\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d]$.\n- The power of the desired signal is $\\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}]$.\n- The specific values for the correlation matrix and cross-correlation vector are:\n$$\n\\mathbf{R}=\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\n- The objective is to derive the normal equations, find the optimal Wiener solution $\\mathbf{w}^{\\star}$, and compute the minimum MSE, $J_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w})$.\n\nThe problem is evaluated for validity. It is a standard problem in linear estimation theory, specifically Wiener filtering. All terms are well-defined, and the context is firmly rooted in the field of signal processing. The given correlation matrix $\\mathbf{R}$ is symmetric and its determinant is $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 3 \\neq 0$, which ensures that $\\mathbf{R}$ is invertible and a unique solution for the optimal weight vector $\\mathbf{w}^{\\star}$ exists. The problem is self-contained, scientifically grounded, and well-posed. Therefore, the problem is deemed valid. We proceed with the solution.\n\nThe mean-square error is given by $J(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2]$. We seek the optimal weight vector $\\mathbf{w}^{\\star}$ that minimizes this cost function. The estimation error is defined as $e = d - y = d - \\mathbf{w}^{\\top}\\mathbf{x}$. For the optimal filter, $\\mathbf{w} = \\mathbf{w}^{\\star}$, the orthogonality principle states that the estimation error $e^{\\star} = d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ must be orthogonal to the input vector $\\mathbf{x}$. Mathematically, this is expressed as the expectation of the product of the error and the input vector being zero.\n$$\n\\mathbb{E}[\\mathbf{x} e^{\\star}] = \\mathbf{0}\n$$\nSubstituting the expression for the error $e^{\\star}$:\n$$\n\\mathbb{E}[\\mathbf{x} (d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x})] = \\mathbf{0}\n$$\nUsing the linearity of the expectation operator:\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}] = \\mathbf{0}\n$$\nThe term $(\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ is a scalar, so it can be reordered: $\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x} = \\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{w}^{\\star}$. The weight vector $\\mathbf{w}^{\\star}$ is deterministic with respect to the expectation.\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\nUsing the provided definitions $\\mathbf{p} = \\mathbb{E}[\\mathbf{x}d]$ and $\\mathbf{R} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$, we arrive at the normal equations, also known as the Wiener-Hopf equations for this discrete case:\n$$\n\\mathbf{p} - \\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\n$$\n\\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{p}\n$$\nThis is the set of linear equations that the optimal Wiener solution $\\mathbf{w}^{\\star}$ must satisfy.\n\nTo find the explicit solution for $\\mathbf{w}^{\\star}$, we must solve this system of equations. Since $\\mathbf{R}$ is invertible, the unique solution is given by:\n$$\n\\mathbf{w}^{\\star} = \\mathbf{R}^{-1} \\mathbf{p}\n$$\nWe are given the matrices:\n$$\n\\mathbf{R}=\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\nFirst, we compute the inverse of $\\mathbf{R}$. For a $2 \\times 2$ matrix $\\begin{bmatrix}a  b \\\\ c  d\\end{bmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{bmatrix}d  -b \\\\ -c  a\\end{bmatrix}$. The determinant of $\\mathbf{R}$ is $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 4 - 1 = 3$.\nThe inverse is therefore:\n$$\n\\mathbf{R}^{-1} = \\frac{1}{3}\\begin{bmatrix}2  -1 \\\\ -1  2\\end{bmatrix}\n$$\nNow we can compute $\\mathbf{w}^{\\star}$:\n$$\n\\mathbf{w}^{\\star} = \\frac{1}{3}\\begin{bmatrix}2  -1 \\\\ -1  2\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}(2)(1) + (-1)(0) \\\\ (-1)(1) + (2)(0)\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}2/3 \\\\ -1/3\\end{bmatrix}\n$$\nSo, the components of the optimal weight vector are $w_1^{\\star} = 2/3$ and $w_2^{\\star} = -1/3$.\n\nNext, we compute the minimum achievable MSE, $J_{\\min} = J(\\mathbf{w}^{\\star})$. We start with the definition of $J(\\mathbf{w})$ and expand it:\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[d^2 - 2d\\mathbf{w}^{\\top}\\mathbf{x} + (\\mathbf{w}^{\\top}\\mathbf{x})^2]\n$$\nBy linearity of expectation:\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[d^2] - 2\\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}d] + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\sigma_{d}^{2} - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w}\n$$\nTo find $J_{\\min}$, we substitute $\\mathbf{w} = \\mathbf{w}^{\\star}$:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{R}\\mathbf{w}^{\\star}\n$$\nFrom the normal equations, we know $\\mathbf{R}\\mathbf{w}^{\\star} = \\mathbf{p}$. Substituting this into the last term:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\nThis simplifies to a general expression for the minimum MSE:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\nNow, we substitute the numerical values we found for $\\mathbf{w}^{\\star}$ and the given $\\mathbf{p}$:\n$$\n(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} = \\begin{bmatrix}2/3  -1/3\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = (2/3)(1) + (-1/3)(0) = 2/3\n$$\nTherefore, the minimum MSE is:\n$$\nJ_{\\min} = \\sigma_{d}^{2} - \\frac{2}{3}\n$$\nThe final answer comprises the two components of $\\mathbf{w}^{\\star}$ and the expression for $J_{\\min}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3}  \\sigma_{d}^{2} - \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "2850274"}, {"introduction": "While the Wiener filter provides the optimal static solution, most real-world applications require filters that adapt in real time. This practice problem puts you in the driver's seat of the Recursive Least Squares (RLS) algorithm, a powerful method for tracking time-varying systems. You will perform a single, complete update step [@problem_id:2850229], calculating the new filter weights and inverse correlation matrix, to see precisely how the algorithm recursively minimizes the sum of squared errors.", "problem": "Consider a real, time-varying, linear finite impulse response adaptive filter of order $M=2$ that is updated by the Recursive Least Squares (RLS) method with forgetting factor $\\lambda$. The RLS method at time $n$ seeks the weight vector $\\mathbf{w}(n) \\in \\mathbb{R}^{2}$ that minimizes the exponentially weighted least-squares cost\n$$\nJ(n) \\triangleq \\sum_{i=1}^{n} \\lambda^{n-i} \\left(d(i) - \\mathbf{u}^{\\top}(i)\\,\\mathbf{w}\\right)^{2},\n$$\nwhere $\\mathbf{u}(i) \\in \\mathbb{R}^{2}$ is the regressor and $d(i) \\in \\mathbb{R}$ is the desired response. The minimizer satisfies the orthogonality principle with respect to the weighted inner product induced by $\\lambda$, and the updates use the inverse weighted input correlation matrix $\\mathbf{P}(n) \\in \\mathbb{R}^{2 \\times 2}$.\n\nAt time $n$, you are given:\n- $\\lambda = 1$,\n- $\\mathbf{u}(n) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- $d(n) = 2$,\n- $\\mathbf{P}(n-1) = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$,\n- $\\mathbf{w}(n-1) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nUsing the standard RLS recursion consistent with the exponentially weighted least-squares criterion and the orthogonality principle, perform one update step at time $n$ to obtain the updated weight vector $\\mathbf{w}(n)$, the updated inverse correlation matrix $\\mathbf{P}(n)$, and the a posteriori error $e_{\\mathrm{po}}(n) \\triangleq d(n) - \\mathbf{u}^{\\top}(n)\\,\\mathbf{w}(n)$.\n\nExpress your final answer as a single row matrix containing, in order, the two components of $\\mathbf{w}(n)$, the four entries of $\\mathbf{P}(n)$ in row-major order, and $e_{\\mathrm{po}}(n)$. Use exact rational values; no rounding.", "solution": "The problem will first be validated for scientific and formal correctness.\n\n### Step 1: Extract Givens\nThe problem provides the following explicit data and definitions for a Recursive Least Squares (RLS) adaptive filter update at time step $n$:\n- Filter order: $M=2$\n- Forgetting factor: $\\lambda = 1$\n- Input regressor vector at time $n$: $\\mathbf{u}(n) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- Desired response at time $n$: $d(n) = 2$\n- Inverse correlation matrix at time $n-1$: $\\mathbf{P}(n-1) = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$\n- Weight vector at time $n-1$: $\\mathbf{w}(n-1) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n- Cost function to be minimized: $J(n) = \\sum_{i=1}^{n} \\lambda^{n-i} (d(i) - \\mathbf{u}^{\\top}(i)\\,\\mathbf{w})^{2}$\n- A posteriori error definition: $e_{\\mathrm{po}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\,\\mathbf{w}(n)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against established criteria.\n- **Scientifically Grounded**: The problem describes a standard application of the Recursive Least Squares algorithm, a cornerstone of adaptive filtering theory in signal processing. The RLS equations are derived from the principle of minimizing a weighted least-squares error criterion, which is a mathematically rigorous and scientifically valid procedure. The problem is free of any pseudoscience or factual inaccuracies.\n- **Well-Posed**: The problem provides a complete set of initial conditions ($\\mathbf{w}(n-1)$, $\\mathbf{P}(n-1)$) and new data ($\\mathbf{u}(n)$, $d(n)$) required to perform a single, unique update of the RLS algorithm. The specified operations are computationally well-defined.\n- **Objective**: The problem is stated in precise mathematical language, using standard notation from the field of signal processing. It is devoid of subjective, ambiguous, or opinion-based statements.\n- **Other Flaws**: The problem setup is not incomplete, contradictory, unrealistic, ill-posed, trivial, or unverifiable. It is a canonical exercise in applying a known algorithm.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined and self-contained engineering mathematics problem. A solution will be provided.\n\nThe RLS algorithm provides a recursive method for finding the weight vector $\\mathbf{w}(n)$ that minimizes the exponentially weighted least-squares cost function $J(n)$. The standard update procedure for time step $n$ consists of the following sequence of calculations.\n\n1.  **Calculate the gain vector $\\mathbf{k}(n)$**:\n    The gain vector determines the direction and magnitude of the correction to the weight vector. It is computed as:\n    $$\n    \\mathbf{k}(n) = \\frac{\\mathbf{P}(n-1)\\mathbf{u}(n)}{\\lambda + \\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)\\mathbf{u}(n)}\n    $$\n    First, we compute the numerator, $\\mathbf{P}(n-1)\\mathbf{u}(n)$:\n    $$\n    \\mathbf{P}(n-1)\\mathbf{u}(n) = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (0)(1) \\\\ (0)(1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n    $$\n    Next, we compute the term $\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)\\mathbf{u}(n)$ for the denominator:\n    $$\n    \\mathbf{u}^{\\top}(n)[\\mathbf{P}(n-1)\\mathbf{u}(n)] = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = (1)(2) + (1)(1) = 3\n    $$\n    The denominator is $\\lambda + 3 = 1 + 3 = 4$.\n    Therefore, the gain vector is:\n    $$\n    \\mathbf{k}(n) = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}\n    $$\n\n2.  **Calculate the a priori estimation error $e_{\\mathrm{pr}}(n)$**:\n    This error is the difference between the desired response $d(n)$ and the filter's output before the weight update, using the previous weight vector $\\mathbf{w}(n-1)$.\n    $$\n    e_{\\mathrm{pr}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\mathbf{w}(n-1)\n    $$\n    $$\n    e_{\\mathrm{pr}}(n) = 2 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 - ((1)(1) + (1)(-1)) = 2 - 0 = 2\n    $$\n\n3.  **Update the filter weight vector to obtain $\\mathbf{w}(n)$**:\n    The new weight vector is computed by adding a correction term, proportional to the a priori error and the gain vector, to the old weight vector.\n    $$\n    \\mathbf{w}(n) = \\mathbf{w}(n-1) + \\mathbf{k}(n) e_{\\mathrm{pr}}(n)\n    $$\n    $$\n    \\mathbf{w}(n) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} (2) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ -1+\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix}\n    $$\n\n4.  **Update the inverse correlation matrix to obtain $\\mathbf{P}(n)$**:\n    The update rule for $\\mathbf{P}(n)$, derived from the matrix inversion lemma, is:\n    $$\n    \\mathbf{P}(n) = \\frac{1}{\\lambda} \\left[ \\mathbf{P}(n-1) - \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) \\right]\n    $$\n    Since $\\lambda = 1$, the equation simplifies to $\\mathbf{P}(n) = \\mathbf{P}(n-1) - \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)$. First, we compute the term $\\mathbf{u}^{\\top}(n)\\mathbf{P(n-1)}$:\n    $$\n    \\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (1)(2)+(1)(0)  (1)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix}\n    $$\n    Next, we compute the outer product $\\mathbf{k}(n)[\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)]$:\n    $$\n    \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 2  1 \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{2})(2)  (\\frac{1}{2})(1) \\\\ (\\frac{1}{4})(2)  (\\frac{1}{4})(1) \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{4} \\end{pmatrix}\n    $$\n    Finally, we update $\\mathbf{P}(n)$:\n    $$\n    \\mathbf{P}(n) = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 2-1  0-\\frac{1}{2} \\\\ 0-\\frac{1}{2}  1-\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{3}{4} \\end{pmatrix}\n    $$\n\n5.  **Calculate the a posteriori estimation error $e_{\\mathrm{po}}(n)$**:\n    This is the final error using the updated weight vector $\\mathbf{w}(n)$.\n    $$\n    e_{\\mathrm{po}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\mathbf{w}(n)\n    $$\n    $$\n    e_{\\mathrm{po}}(n) = 2 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix} = 2 - \\left((1)(2) + (1)\\left(-\\frac{1}{2}\\right)\\right) = 2 - \\left(2 - \\frac{1}{2}\\right) = 2 - \\frac{3}{2} = \\frac{1}{2}\n    $$\n    Alternatively, the a posteriori error can be computed from the a priori error: $e_{\\mathrm{po}}(n) = (1 - \\mathbf{u}^{\\top}(n)\\mathbf{k}(n))e_{\\mathrm{pr}}(n)$. Let us verify:\n    $$\n    \\mathbf{u}^{\\top}(n)\\mathbf{k}(n) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n    $$\n    $$\n    e_{\\mathrm{po}}(n) = \\left(1 - \\frac{3}{4}\\right)(2) = \\left(\\frac{1}{4}\\right)(2) = \\frac{1}{2}\n    $$\n    The results are consistent.\n\nThe required outputs are the components of $\\mathbf{w}(n)$, the entries of $\\mathbf{P}(n)$ in row-major order, and the value of $e_{\\mathrm{po}}(n)$.\n- $\\mathbf{w}(n) = \\begin{pmatrix} w_1(n) \\\\ w_2(n) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix}$\n- $\\mathbf{P}(n) = \\begin{pmatrix} p_{11}(n)  p_{12}(n) \\\\ p_{21}(n)  p_{22}(n) \\end{pmatrix} = \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{3}{4} \\end{pmatrix}$\n- $e_{\\mathrm{po}}(n) = \\frac{1}{2}$\n\nThese are combined into a single row matrix as requested.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2  -\\frac{1}{2}  1  -\\frac{1}{2}  -\\frac{1}{2}  \\frac{3}{4}  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "2850229"}, {"introduction": "In contrast to the computational complexity of RLS, the Least Mean Squares (LMS) algorithm offers a simpler and more robust approach to adaptation, but its performance analysis is more nuanced. This exercise challenges you to derive the complete learning curve for the LMS filter's excess mean-square error (EMSE), from its initial transient decay to its final steady-state value. Successfully navigating this derivation [@problem_id:2850271] will provide deep insight into the algorithm's behavior and the fundamental trade-offs that govern its performance in the presence of noise.", "problem": "Consider an unknown finite impulse response system of length $M$ with coefficient vector $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{M}$. The input regression vector is $\\mathbf{x}(n) \\in \\mathbb{R}^{M}$, and the measured desired response is $d(n) = \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)$, where $v(n)$ is zero-mean measurement noise. Assume the following data model and statistical properties:\n- $\\mathbf{x}(n)$ is zero-mean, white, Gaussian with covariance $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$ and independent across time.\n- $v(n)$ is zero-mean, white, Gaussian with variance $\\sigma_{v}^{2}$, independent of $\\mathbf{x}(n)$ for all $n$.\n- The initial weight vector of the adaptive filter is $\\mathbf{w}(0)$, and the Least Mean Squares (LMS) update is given by $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\mathbf{x}(n)e(n)$ with $e(n) \\triangleq d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n)$ and step size $\\mu$ chosen to ensure mean-square stability.\n\nDefine the weight-error vector $\\widetilde{\\mathbf{w}}(n) \\triangleq \\mathbf{w}^{\\star} - \\mathbf{w}(n)$ and the Excess Mean-Square Error (EMSE) as $\\xi_{\\mathrm{ex}}(n) \\triangleq \\mathbb{E}\\!\\left[e^{2}(n)\\right] - \\xi_{\\min}$, where $\\xi_{\\min}$ is the minimum mean-square error characterized by the Orthogonality Principle of Wiener filtering. Work from first principles, using only the LMS update, the data model, the Orthogonality Principle, and standard properties of expectations and Gaussian moment factoring, to obtain a closed-form expression for the transient EMSE $\\xi_{\\mathrm{ex}}(n)$ as a function of the nonnegative integer $n$, and its steady-state limit as $n \\to \\infty$. Your final expression must be in terms of $\\mu$, $\\sigma_{x}^{2}$, $\\sigma_{v}^{2}$, $M$, and the initial EMSE $\\xi_{\\mathrm{ex}}(0)$, and it must be valid under a mean-square stability step-size condition that you should state and enforce in your derivation. Provide the exact analytic expressions; no approximations are allowed. The final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement as presented is valid. It is scientifically grounded in the established theory of adaptive signal processing, well-posed with a unique and meaningful solution derivable from the provided information, and is expressed with objective, formal language. It is a standard, non-trivial problem in the analysis of the Least Mean Squares (LMS) algorithm. We will proceed with a rigorous derivation from first principles.\n\nThe derivation is structured as follows: First, we establish the value of the minimum mean-square error, $\\xi_{\\min}$, by applying the Orthogonality Principle. Second, we express the Excess Mean-Square Error (EMSE), $\\xi_{\\mathrm{ex}}(n)$, in terms of the weight-error vector, $\\widetilde{\\mathbf{w}}(n)$. Third, we derive the recursion for the weight-error covariance matrix, $\\mathbf{K}(n) \\triangleq \\mathbb{E}[\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)]$. This step critically relies on the specified data model and properties of Gaussian random vectors. Fourth, from the matrix recursion, we derive a scalar recursion for $\\xi_{\\mathrm{ex}}(n)$. Finally, we solve this linear first-order difference equation to find the closed-form expressions for the transient EMSE, $\\xi_{\\mathrm{ex}}(n)$, and its steady-state limit, $\\xi_{\\mathrm{ex}}(\\infty)$.\n\nFirst, we determine the minimum mean-square error, $\\xi_{\\min}$. The optimal Wiener filter, $\\mathbf{w}_{\\mathrm{o}}$, minimizes the cost function $\\xi = \\mathbb{E}[e^{2}(n)]$. The Orthogonality Principle states that the error signal corresponding to the optimal filter, $e_{\\mathrm{o}}(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}_{\\mathrm{o}}$, must be orthogonal to the input regression vector, $\\mathbf{x}(n)$.\n$$\n\\mathbb{E}[\\mathbf{x}(n)e_{\\mathrm{o}}(n)] = \\mathbf{0}\n$$\nSubstituting the expression for $d(n) = \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)$:\n$$\n\\mathbb{E}[\\mathbf{x}(n) ( \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}_{\\mathrm{o}} )] = \\mathbf{0}\n$$\nBy linearity of expectation, and using the fact that $v(n)$ is zero-mean and independent of $\\mathbf{x}(n)$ so that $\\mathbb{E}[\\mathbf{x}(n)v(n)]=\\mathbf{0}$:\n$$\n\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)](\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}}) = \\mathbf{0}\n$$\n$$\n\\mathbf{R}(\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}}) = \\mathbf{0}\n$$\nGiven $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$ with $\\sigma_{x}^{2}  0$, the matrix $\\mathbf{R}$ is invertible, which implies $\\mathbf{w}^{\\star} - \\mathbf{w}_{\\mathrm{o}} = \\mathbf{0}$, or $\\mathbf{w}_{\\mathrm{o}} = \\mathbf{w}^{\\star}$. The optimal filter is the true system. The minimum mean-square error is then the error achieved by this optimal filter:\n$$\n\\xi_{\\min} = \\mathbb{E}[e_{\\mathrm{o}}^{2}(n)] = \\mathbb{E}[(d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star})^{2}] = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star})^{2}] = \\mathbb{E}[v^{2}(n)] = \\sigma_{v}^{2}\n$$\nNext, we express the EMSE in terms of the weight-error vector $\\widetilde{\\mathbf{w}}(n) = \\mathbf{w}^{\\star} - \\mathbf{w}(n)$. The error signal is $e(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n)$.\n$$\ne(n) = (\\mathbf{x}^{\\top}(n)\\mathbf{w}^{\\star} + v(n)) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) = \\mathbf{x}^{\\top}(n)(\\mathbf{w}^{\\star} - \\mathbf{w}(n)) + v(n) = \\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n)\n$$\nThe mean-square error a time $n$ is $\\xi(n) = \\mathbb{E}[e^{2}(n)]$.\n$$\n\\xi(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n))^{2}] = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] + 2\\mathbb{E}[v(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] + \\mathbb{E}[v^{2}(n)]\n$$\nThe weight-error vector $\\widetilde{\\mathbf{w}}(n)$ is a function of data up to time $n-1$. Since $v(n)$ is white and independent of $\\mathbf{x}(k)$ for all $k$, $v(n)$ is independent of $\\widetilde{\\mathbf{w}}(n)$. As $v(n)$ is zero-mean, the cross-term vanishes: $\\mathbb{E}[v(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = \\mathbb{E}[v(n)]\\mathbb{E}[\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = 0$.\n$$\n\\xi(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] + \\sigma_{v}^{2}\n$$\nThe EMSE is defined as $\\xi_{\\mathrm{ex}}(n) \\triangleq \\xi(n) - \\xi_{\\min} = \\xi(n) - \\sigma_{v}^{2}$.\n$$\n\\xi_{\\mathrm{ex}}(n) = \\mathbb{E}[(\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n))^{2}] = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)]\n$$\nSince the input vector $\\mathbf{x}(n)$ is independent of past data, it is independent of $\\widetilde{\\mathbf{w}}(n)$. We can take the expectation with respect to $\\mathbf{x}(n)$ first.\n$$\n\\xi_{\\mathrm{ex}}(n) = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n) \\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] \\widetilde{\\mathbf{w}}(n)] = \\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\mathbf{R}\\widetilde{\\mathbf{w}}(n)] = \\sigma_{x}^{2}\\mathbb{E}[\\widetilde{\\mathbf{w}}^{\\top}(n)\\widetilde{\\mathbf{w}}(n)] = \\sigma_{x}^{2}\\mathbb{E}[\\|\\widetilde{\\mathbf{w}}(n)\\|^{2}]\n$$\nLet the weight-error covariance matrix be $\\mathbf{K}(n) \\triangleq \\mathbb{E}[\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)]$. Then $\\mathbb{E}[\\|\\widetilde{\\mathbf{w}}(n)\\|^{2}] = \\mathrm{Tr}(\\mathbf{K}(n))$. Thus,\n$$\n\\xi_{\\mathrm{ex}}(n) = \\sigma_{x}^{2}\\mathrm{Tr}(\\mathbf{K}(n))\n$$\nNow we find the recursion for $\\mathbf{K}(n)$. Starting from the LMS update rule, we derive the recursion for $\\widetilde{\\mathbf{w}}(n)$:\n$$\n\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu\\mathbf{x}(n)e(n) \\implies \\mathbf{w}^{\\star} - \\mathbf{w}(n+1) = \\mathbf{w}^{\\star} - \\mathbf{w}(n) - \\mu\\mathbf{x}(n)e(n)\n$$\n$$\n\\widetilde{\\mathbf{w}}(n+1) = \\widetilde{\\mathbf{w}}(n) - \\mu\\mathbf{x}(n)[\\mathbf{x}^{\\top}(n)\\widetilde{\\mathbf{w}}(n) + v(n)] = [\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\widetilde{\\mathbf{w}}(n) - \\mu v(n)\\mathbf{x}(n)\n$$\nThen, we compute $\\mathbf{K}(n+1) = \\mathbb{E}[\\widetilde{\\mathbf{w}}(n+1)\\widetilde{\\mathbf{w}}^{\\top}(n+1)]$. The cross-terms involving $v(n)$ vanish upon taking expectation because $v(n)$ is zero-mean and independent of $\\mathbf{x}(n)$ and $\\widetilde{\\mathbf{w}}(n)$.\n$$\n\\mathbf{K}(n+1) = \\mathbb{E}\\left[[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\widetilde{\\mathbf{w}}(n)\\widetilde{\\mathbf{w}}^{\\top}(n)[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]^{\\top}\\right] + \\mathbb{E}[\\mu^{2}v^{2}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\n$$\nThe second term is $\\mu^{2}\\mathbb{E}[v^{2}(n)]\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] = \\mu^{2}\\sigma_{v}^{2}\\mathbf{R} = \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}$. For the first term, we use the independence of $\\mathbf{x}(n)$ and $\\widetilde{\\mathbf{w}}(n)$ again:\n$$\n\\mathbb{E}[\\dots] = \\mathbb{E}_{\\mathbf{x}}\\left[[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\mathbf{K}(n)[\\mathbf{I}_{M} - \\mu\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\\right]\n$$\n$$\n= \\mathbf{K}(n) - \\mu\\mathbf{R}\\mathbf{K}(n) - \\mu\\mathbf{K}(n)\\mathbf{R} + \\mu^{2}\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{K}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)]\n$$\nFor a real, zero-mean Gaussian vector $\\mathbf{x}$ with covariance $\\mathbf{R}$ and a symmetric matrix $\\mathbf{A}$, the fourth-order moment is given by $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}\\mathbf{x}^{\\top}] = \\mathbf{R}\\mathrm{Tr}(\\mathbf{A}\\mathbf{R}) + 2\\mathbf{R}\\mathbf{A}\\mathbf{R}$.\nHere, $\\mathbf{A} = \\mathbf{K}(n)$ and $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}_{M}$.\n$$\n\\mathbb{E}[\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{K}(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)] = (\\sigma_{x}^{2}\\mathbf{I}_{M})\\mathrm{Tr}(\\mathbf{K}(n)\\sigma_{x}^{2}\\mathbf{I}_{M}) + 2(\\sigma_{x}^{2}\\mathbf{I}_{M})\\mathbf{K}(n)(\\sigma_{x}^{2}\\mathbf{I}_{M}) = \\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + 2\\sigma_{x}^{4}\\mathbf{K}(n)\n$$\nSubstituting this back into the recursion for $\\mathbf{K}(n+1)$:\n$$\n\\mathbf{K}(n+1) = \\mathbf{K}(n) - 2\\mu\\sigma_{x}^{2}\\mathbf{K}(n) + \\mu^{2}[ \\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + 2\\sigma_{x}^{4}\\mathbf{K}(n) ] + \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}\n$$\n$$\n\\mathbf{K}(n+1) = (1 - 2\\mu\\sigma_{x}^{2} + 2\\mu^{2}\\sigma_{x}^{4})\\mathbf{K}(n) + \\mu^{2}\\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n))\\mathbf{I}_{M} + \\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\\mathbf{I}_{M}\n$$\nTo obtain a scalar recursion, we take the trace of this equation using $\\mathrm{Tr}(\\mathbf{I}_{M})=M$:\n$$\n\\mathrm{Tr}(\\mathbf{K}(n+1)) = (1 - 2\\mu\\sigma_{x}^{2} + 2\\mu^{2}\\sigma_{x}^{4})\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{x}^{4}\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\n$$\n\\mathrm{Tr}(\\mathbf{K}(n+1)) = (1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4})\\mathrm{Tr}(\\mathbf{K}(n)) + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\nNow, substitute $\\mathrm{Tr}(\\mathbf{K}(n)) = \\xi_{\\mathrm{ex}}(n)/\\sigma_{x}^{2}$:\n$$\n\\frac{\\xi_{\\mathrm{ex}}(n+1)}{\\sigma_{x}^{2}} = (1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4})\\frac{\\xi_{\\mathrm{ex}}(n)}{\\sigma_{x}^{2}} + M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{2}\n$$\n$$\n\\xi_{\\mathrm{ex}}(n+1) = \\alpha \\cdot \\xi_{\\mathrm{ex}}(n) + \\beta\n$$\nwhere $\\alpha \\triangleq 1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}$ and $\\beta \\triangleq M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{4}$.\n\nFor mean-square stability, the magnitude of the coefficient $\\alpha$ must be less than $1$.\n$$\n-1  1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}  1\n$$\nThe right inequality, $1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}  1$, gives $\\mu\\sigma_{x}^{2}((M+2)\\mu\\sigma_{x}^{2}-2)  0$. Since $\\mu\\sigma_{x}^{2}  0$, this requires $(M+2)\\mu\\sigma_{x}^{2}-2  0$, or $\\mu  \\frac{2}{(M+2)\\sigma_{x}^{2}}$. The left inequality, $-1  1 - 2\\mu\\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4}$, is always satisfied for any real $\\mu$ since the quadratic in $\\mu$ has a negative discriminant. The stability condition is therefore:\n$$\n0  \\mu  \\frac{2}{(M+2)\\sigma_{x}^{2}}\n$$\nWe solve the difference equation $\\xi_{\\mathrm{ex}}(n+1) = \\alpha \\xi_{\\mathrm{ex}}(n) + \\beta$. The solution is of the form $\\xi_{\\mathrm{ex}}(n) = \\alpha^{n}(\\xi_{\\mathrm{ex}}(0) - \\xi_{\\mathrm{ex}}(\\infty)) + \\xi_{\\mathrm{ex}}(\\infty)$, where $\\xi_{\\mathrm{ex}}(\\infty)$ is the steady-state value.\n$$\n\\xi_{\\mathrm{ex}}(\\infty) = \\lim_{n\\to\\infty} \\xi_{\\mathrm{ex}}(n) = \\frac{\\beta}{1-\\alpha}\n$$\n$$\n1-\\alpha = 2\\mu\\sigma_{x}^{2} - (M+2)\\mu^{2}\\sigma_{x}^{4} = \\mu\\sigma_{x}^{2}(2 - (M+2)\\mu\\sigma_{x}^{2})\n$$\n$$\n\\xi_{\\mathrm{ex}}(\\infty) = \\frac{M\\mu^{2}\\sigma_{v}^{2}\\sigma_{x}^{4}}{\\mu\\sigma_{x}^{2}(2 - (M+2)\\mu\\sigma_{x}^{2})} = \\frac{M\\mu\\sigma_{v}^{2}\\sigma_{x}^{2}}{2 - (M+2)\\mu\\sigma_{x}^{2}}\n$$\nThis is the steady-state Excess Mean-Square Error. The transient EMSE is then given by the full solution to the difference equation, expressed in terms of the given parameters and the initial condition $\\xi_{\\mathrm{ex}}(0)$.\n\nThe final closed-form expression for the transient EMSE is:\n$$\n\\xi_{\\mathrm{ex}}(n) = \\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\xi_{\\mathrm{ex}}(\\infty) \\right) + \\xi_{\\mathrm{ex}}(\\infty)\n$$\nSubstituting the expression for $\\xi_{\\mathrm{ex}}(\\infty)$:\n$$\n\\xi_{\\mathrm{ex}}(n) = \\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}} \\right) + \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}}\n$$\nThis expression is valid under the mean-square stability condition $0  \\mu  \\frac{2}{(M+2)\\sigma_{x}^{2}}$. As $n \\to \\infty$, since $|\\alpha|  1$, the first term decays to zero, leaving $\\xi_{\\mathrm{ex}}(n) \\to \\xi_{\\mathrm{ex}}(\\infty)$.", "answer": "$$\n\\boxed{\\left( 1 - 2\\mu \\sigma_{x}^{2} + (M+2)\\mu^{2}\\sigma_{x}^{4} \\right)^{n} \\left( \\xi_{\\mathrm{ex}}(0) - \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}} \\right) + \\frac{M\\mu \\sigma_{v}^{2} \\sigma_{x}^{2}}{2 - (M+2)\\mu \\sigma_{x}^{2}}}\n$$", "id": "2850271"}]}