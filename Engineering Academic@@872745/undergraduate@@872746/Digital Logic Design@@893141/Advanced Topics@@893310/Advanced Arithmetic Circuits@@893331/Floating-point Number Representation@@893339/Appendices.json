{"hands_on_practices": [{"introduction": "Understanding floating-point representation begins with acknowledging its inherent trade-offs, particularly the concept of representation error. This exercise provides a concrete example of how finite precision leads to discrepancies between a real number and its stored value. By converting a decimal number into a custom, space-constrained floating-point format and calculating the resulting error, you will gain a practical appreciation for the inaccuracies that can arise from truncation [@problem_id:1937508].", "problem": "A specialized embedded system uses a custom 8-bit floating-point number representation to store sensor data. This format is defined as follows:\n- 1 bit for the sign (S): 0 for positive, 1 for negative.\n- 4 bits for the exponent (E): The exponent is stored with a bias of 7.\n- 3 bits for the fraction (F): The number is normalized in the form $1.F...$ and the leading 1 is implicit.\n\nThe bits are arranged in the order S, E, F. If a number's binary representation has more fractional bits than can be stored, the system truncates the excess bits (it does not round).\n\nCalculate the absolute representation error that occurs when the system attempts to store the decimal value $9.2$. Provide your answer as a decimal number rounded to three significant figures.", "solution": "A normalized floating-point value in this format is\n$$V=(-1)^{S}\\left(1+\\sum_{i=1}^{3}f_{i}2^{-i}\\right)2^{E-\\mathrm{bias}},$$\nwith $\\mathrm{bias}=7$, $S$ the sign bit, $E$ the 4-bit exponent, and $f_{i}$ the three fraction bits. Truncation means we keep only the first three fractional bits after normalization and discard the rest.\n\nFor $9.2>0$, $S=0$. Convert $9.2$ to binary and normalize. Since $9.2=1001.0011001100\\ldots_{2}$, normalization gives\n$$9.2=1.0010011001100\\ldots_{2}\\times 2^{3},$$\nso the unbiased exponent is $e=3$ and the stored exponent is $E=e+\\mathrm{bias}=3+7=10$.\n\nThe normalized mantissa has fractional bits $0010011\\ldots$; truncating to three fraction bits yields $F=001$, so the stored mantissa is\n$$M_{\\text{stored}}=1.001_{2}=1+2^{-3}.$$\nTherefore, the stored value is\n$$V_{\\text{stored}}=(1+2^{-3})2^{3}=2^{3}+1=9.$$\n\nThe absolute representation error is\n$$\\left|9.2-9\\right|=0.2.$$\nRounded to three significant figures, this is $0.200$.", "answer": "$$\\boxed{0.200}$$", "id": "1937508"}, {"introduction": "While floating-point formats can represent an enormous range of values, they cannot represent every number with perfect accuracy, a limitation that even extends to integers. This problem challenges you to explore the boundaries of the widely-used IEEE 754 single-precision format. By finding the first integer that cannot be exactly represented, you will gain a profound insight into how precision scales with magnitude and the significance of the spacing between representable numbers [@problem_id:2173582].", "problem": "In modern computing, real numbers are often approximated using a floating-point representation. A widely adopted standard for this is the Institute of Electrical and Electronics Engineers (IEEE) 754 standard. The single-precision format under this standard uses 32 bits to represent a number.\n\nThe 32 bits are allocated as follows:\n- 1 bit for the sign ($s$).\n- 8 bits for the biased exponent ($E$).\n- 23 bits for the fraction ($f$).\n\nFor a positive, normalized number (where the sign bit $s=0$), its value $V$ is determined by the formula:\n$$V = 2^{E-127} \\times (1.f)_2$$\nHere, $127$ is the exponent bias. The term $(1.f)_2$ represents the significand in binary, where the '1.' is an implicit leading bit that is not stored, and $f$ is the 23-bit binary fraction stored in the fraction bits. For normalized numbers, the biased exponent $E$ can range from 1 to 254.\n\nWhile this format can represent a vast range of numbers, its finite precision means that not all numbers can be represented exactly. This limitation is particularly interesting when considering integers. For example, the integer 5 can be written as $1.25 \\times 4 = (1.01)_2 \\times 2^2$. This corresponds to a fraction $f=0100...0$ and an exponent part that yields a power of 2, so 5 is exactly representable.\n\nYour task is to find the smallest positive integer that cannot be represented exactly in the IEEE 754 single-precision floating-point format.", "solution": "A normalized single-precision value has the form\n$$V=2^{e}\\,(1.f)_{2},\\quad e=E-127,$$\nwhere the significand has one implicit leading bit plus 23 stored fraction bits, giving an effective precision of $p=24$ bits. Increasing the fraction $f$ by one unit changes $(1.f)_{2}$ by $2^{-23}$, so for a fixed exponent $e$ the spacing (unit in the last place) between consecutive representable numbers is\n$$\\mathrm{ulp}(e)=2^{e}\\cdot 2^{-23}=2^{e-23}.$$\n\nTo represent every integer in some interval exactly, the spacing there must be at most $1$. Thus we require\n$$\\mathrm{ulp}(e)\\leq 1 \\quad \\Longleftrightarrow \\quad 2^{e-23}\\leq 1 \\quad \\Longleftrightarrow \\quad e\\leq 23.$$\nNumbers in the range $[2^{23},\\,2^{24})$ have $e=23$ and hence $\\mathrm{ulp}=1$, so every integer in that range is representable. The value $2^{24}$ itself is exactly representable as a power of two with significand $(1.0)_{2}$ and $e=24$.\n\nFor the next range $[2^{24},\\,2^{25})$, we have $e=24$ and therefore\n$$\\mathrm{ulp}(24)=2^{24-23}=2,$$\nso only multiples of $2$ are representable there. Consequently, the smallest integer in this range that is not exactly representable is the smallest odd integer,\n$2^{24}+1.$\n\nSince all positive integers less than or equal to $2^{24}$ are exactly representable and $2^{24}+1$ is not, the smallest positive integer that cannot be represented exactly in IEEE 754 single precision is $2^{24}+1$.", "answer": "$$\\boxed{16777217}$$", "id": "2173582"}, {"introduction": "A deep understanding of data representation is key to designing efficient digital hardware, and this principle is powerfully demonstrated in floating-point arithmetic. This practice moves from theory to application by exploring a crucial optimization in Arithmetic Logic Unit (ALU) design for comparing floating-point numbers. You will analyze how, for positive numbers, the structure of the floating-point format allows for a clever shortcut, enabling fast comparisons using simple integer logic instead of complex subtraction circuits [@problem_id:1937471].", "problem": "A specialized Arithmetic Logic Unit (ALU) is being designed to perform fast comparisons on 16-bit custom floating-point numbers. The format for these numbers is defined as follows:\n- 1 bit for the sign (S), where S=0 is positive.\n- 5 bits for the biased exponent (E).\n- 10 bits for the mantissa (M).\n\nThe bit pattern is organized as `S | EEEEE | MMMMMMMMMM`. The value of a number is given by\n$$V = (-1)^S \\times 2^{E - \\text{bias}} \\times (1.M)_2,$$\nwhere the bias is $15$. This format only represents normalized numbers, meaning the special exponent values for zero, infinity, and Not-a-Number (NaN) are not used in this ALU's scope.\n\nYour task is to analyze the logic for a comparator that takes two **positive, normalized** floating-point numbers, $N_A$ and $N_B$, and determines their relative magnitude. The goal is to achieve this comparison with minimal hardware complexity, ideally avoiding a full floating-point subtraction.\n\nConsider the specific case where the 16-bit patterns for two numbers are $P_A = 42\\text{C}0_{16}$ and $P_B = 4280_{16}$. Based on the principles of floating-point comparison, which of the following statements is/are correct for comparing any two positive, normalized numbers in this format, including the specific pair provided?\n\nA. For any two positive, normalized numbers in this format, the number whose bit pattern represents a larger unsigned integer is the larger of the two numbers.\n\nB. For the specific inputs $P_A = 42\\text{C}0_{16}$ and $P_B = 4280_{16}$, the outcome of the comparison is that the number represented by $P_A$ is less than the number represented by $P_B$.\n\nC. A hardware comparator for this task can be implemented using a single 16-bit integer magnitude comparator, without needing to isolate the exponent and mantissa fields.\n\nD. Since the most significant hexadecimal digits of $P_A$ and $P_B$ are identical (both are '4'), the next step must be to compare the least significant hexadecimal digits ('0' for $P_A$ and '0' for $P_B$); since these are also identical, the numbers must be numerically equal.\n\nE. The most efficient comparison logic must first convert both numbers to a 32-bit fixed-point format before a comparison can be made to ensure no precision is lost.", "solution": "We are given a 16-bit positive, normalized floating-point format with fields $S|EEEEE|MMMMMMMMMM$, value\n$$V = (-1)^{S} \\times 2^{E-\\text{bias}} \\times (1.M)_{2}, \\quad \\text{bias}=15,$$\nand only normalized numbers are considered (no zero, infinity, NaN, or subnormals). For the comparator, the inputs are restricted to positive numbers, so $S=0$ for both operands.\n\nFor any two positive, normalized values $N_{A}$ and $N_{B}$ with fields $(E_{A},M_{A})$ and $(E_{B},M_{B})$, write the significand explicitly as\n$$(1.M)_{2} = 1 + \\frac{M}{2^{10}},$$\nso that\n$$N = 2^{E-\\text{bias}} \\left(1 + \\frac{M}{2^{10}}\\right).$$\nThe ordering follows directly:\n- If $E_{A} > E_{B}$, then $E_{A} \\geq E_{B}+1$, hence\n$$N_{A} \\geq 2^{E_{A}-\\text{bias}} > 2^{E_{B}-\\text{bias}} \\left(2 - 2^{-10}\\right) \\geq N_{B},$$\nso $N_{A} > N_{B}$.\n- If $E_{A} = E_{B}$, then\n$$N_{A} > N_{B} \\iff \\left(1 + \\frac{M_{A}}{2^{10}}\\right) > \\left(1 + \\frac{M_{B}}{2^{10}}\\right) \\iff M_{A} > M_{B}.$$\nTherefore, over this restricted domain, numerical order is exactly the lexicographic order of $(E,M)$. Because the bit pattern places $E$ in the more significant bits than $M$, for $S=0$ this is the same as the unsigned integer order of the entire 16-bit word. Hence:\n- For any two positive, normalized numbers, the larger unsigned 16-bit pattern corresponds to the larger value. This proves statement A is correct.\n- Consequently, a single 16-bit unsigned magnitude comparator applied to the full 16-bit patterns suffices; there is no need to isolate fields, provided both inputs are known to be positive and normalized. This proves statement C is correct.\n\nNow evaluate the specific inputs $P_{A} = 42\\text{C}0_{16}$ and $P_{B} = 4280_{16}$.\n\nDecode $P_{A} = 0100\\,0010\\,1100\\,0000_{2}$:\n- $S=0$.\n- $E$ is the next $5$ bits after $S$: bits $b_{14}\\ldots b_{10}$ are $10000_{2}$, so $E_{A}=16$.\n- $M_{A}$ is the remaining $10$ bits $b_{9}\\ldots b_{0}$: $1011000000_{2}$, i.e., $M_{A}=\\text{0x2C0}$.\n\nDecode $P_{B} = 0100\\,0010\\,1000\\,0000_{2}$:\n- $S=0$.\n- $E_{B}=10000_{2}=16$.\n- $M_{B}=1010000000_{2}$, i.e., $M_{B}=\\text{0x280}$.\n\nSince $E_{A}=E_{B}$ and $M_{A} > M_{B}$, it follows that $N_{A} > N_{B}$. Therefore, for the specific pair, the correct ordering is that $P_{A}$ represents a larger number than $P_{B}$, so statement B (claiming $P_{A} < P_{B}$) is false.\n\nAssess the remaining statements:\n- Statement D is false: equal most significant hexadecimal digits do not imply jumping to least significant digits; comparison must proceed from most significant to next most significant positions. Here, at the third hexadecimal digit, $\\text{C} > \\text{8}$, so $42\\text{C}0_{16} > 4280_{16}$.\n- Statement E is false: no conversion to a 32-bit fixed-point format is needed. The lexicographic property shown above ensures that a simple 16-bit unsigned comparator suffices without any precision transformation.\n\nThus, the correct statements are A and C.", "answer": "$$\\boxed{AC}$$", "id": "1937471"}]}