{"hands_on_practices": [{"introduction": "The journey into computational fluid dynamics begins with mastering the art of translation: converting the continuous language of differential equations into the discrete language of algebra that computers can understand. This first practice focuses on a cornerstone of transport phenomena, the one-dimensional convection-diffusion equation. By discretizing this equation, you will learn how to approximate its fundamental components—the advective (convection) and diffusive terms—using distinct numerical schemes, namely the upwind scheme for convection and the central difference scheme for diffusion. This exercise [@problem_id:1749174] provides a foundational understanding of how different physical processes are represented in a finite difference framework.", "problem": "Consider the one-dimensional, steady-state transport of a chemical species in a narrow channel with a uniform, unidirectional fluid flow. The concentration of the species, denoted by $\\phi(x)$, is governed by the convection-diffusion equation:\n$$u \\frac{d\\phi}{dx} - D \\frac{d^2\\phi}{dx^2} = 0$$\nHere, $u$ is the constant positive fluid velocity along the x-axis, and $D$ is the constant diffusion coefficient.\n\nTo solve this equation numerically, the spatial domain is discretized into a uniform grid with a constant grid spacing of $\\Delta x$. The value of the concentration at a grid node $i$ is denoted by $\\phi_i$.\n\nA numerical model is set up using a first-order upwind differencing scheme for the convection term ($u \\frac{d\\phi}{dx}$) and a second-order central differencing scheme for the diffusion term ($-D \\frac{d^2\\phi}{dx^2}$).\n\nBy applying these discretization schemes at an internal grid node $i$, the resulting algebraic equation can be rearranged into the form $A \\phi_i = B \\phi_{i-1} + C \\phi_{i+1}$. Which of the following options correctly represents this finite difference equation?\n\nA. $(2D - u \\Delta x) \\phi_i = D \\phi_{i-1} + (D - u \\Delta x) \\phi_{i+1}$\n\nB. $4D \\phi_i = (2D + u \\Delta x) \\phi_{i-1} + (2D - u \\Delta x) \\phi_{i+1}$\n\nC. $(u \\Delta x + 2D) \\phi_i = (u \\Delta x + D) \\phi_{i-1} + D \\phi_{i+1}$\n\nD. $(u \\Delta x + D) \\phi_i = (u \\Delta x + D) \\phi_{i-1}$\n\nE. $(u \\Delta x - 2D) \\phi_i = (u \\Delta x - D) \\phi_{i-1} - D \\phi_{i+1}$", "solution": "The goal is to discretize the given one-dimensional, steady-state convection-diffusion equation and rearrange it into the specified algebraic form.\n\nThe governing differential equation is:\n$$u \\frac{d\\phi}{dx} - D \\frac{d^2\\phi}{dx^2} = 0$$\n\nThe spatial domain is discretized with uniform spacing $\\Delta x$, such that the concentration at node $i$ is $\\phi_i$, at the west neighbor is $\\phi_{i-1}$, and at the east neighbor is $\\phi_{i+1}$.\n\nFirst, we discretize the convection (or advection) term, $u \\frac{d\\phi}{dx}$. The problem specifies a first-order upwind differencing scheme. Since the velocity $u$ is given as positive, the flow is in the positive x-direction. Therefore, the \"upwind\" direction for node $i$ is the node from which the flow is coming, which is node $i-1$. This corresponds to a first-order backward difference scheme.\n$$u \\frac{d\\phi}{dx} \\bigg|_i \\approx u \\frac{\\phi_i - \\phi_{i-1}}{\\Delta x}$$\n\nNext, we discretize the diffusion term, $-D \\frac{d^2\\phi}{dx^2}$. The problem specifies a second-order central differencing scheme for the second derivative.\n$$\\frac{d^2\\phi}{dx^2} \\bigg|_i \\approx \\frac{\\phi_{i+1} - 2\\phi_i + \\phi_{i-1}}{(\\Delta x)^2}$$\nTherefore, the discretized diffusion term is:\n$$-D \\frac{d^2\\phi}{dx^2} \\bigg|_i \\approx -D \\frac{\\phi_{i+1} - 2\\phi_i + \\phi_{i-1}}{(\\Delta x)^2}$$\n\nNow, we substitute these discretized forms back into the original governing equation:\n$$u \\frac{\\phi_i - \\phi_{i-1}}{\\Delta x} - D \\frac{\\phi_{i+1} - 2\\phi_i + \\phi_{i-1}}{(\\Delta x)^2} = 0$$\n\nTo simplify and remove the denominators, we multiply the entire equation by $(\\Delta x)^2$:\n$$(\\Delta x)^2 \\left( u \\frac{\\phi_i - \\phi_{i-1}}{\\Delta x} \\right) - (\\Delta x)^2 \\left( D \\frac{\\phi_{i+1} - 2\\phi_i + \\phi_{i-1}}{(\\Delta x)^2} \\right) = 0$$\n$$u \\Delta x (\\phi_i - \\phi_{i-1}) - D (\\phi_{i+1} - 2\\phi_i + \\phi_{i-1}) = 0$$\n\nNow, we expand the terms:\n$$u \\Delta x \\phi_i - u \\Delta x \\phi_{i-1} - D \\phi_{i+1} + 2D \\phi_i - D \\phi_{i-1} = 0$$\n\nThe final step is to rearrange this equation into the form $A \\phi_i = B \\phi_{i-1} + C \\phi_{i+1}$. We group all terms containing $\\phi_i$ on the left-hand side and all other terms on the right-hand side.\n\nGrouping terms with $\\phi_i$:\n$$(u \\Delta x + 2D) \\phi_i$$\n\nGrouping terms with $\\phi_{i-1}$ and $\\phi_{i+1}$ and moving them to the right side:\n$$u \\Delta x \\phi_{i-1} + D \\phi_{i-1} + D \\phi_{i+1}$$\n$$(u \\Delta x + D) \\phi_{i-1} + D \\phi_{i+1}$$\n\nEquating the left and right sides gives the final rearranged algebraic equation:\n$$(u \\Delta x + 2D) \\phi_i = (u \\Delta x + D) \\phi_{i-1} + D \\phi_{i+1}$$\n\nComparing this result with the given choices:\nA. $(2D - u \\Delta x) \\phi_i = D \\phi_{i-1} + (D - u \\Delta x) \\phi_{i+1}$ (Incorrect, results from using forward difference for advection)\nB. $4D \\phi_i = (2D + u \\Delta x) \\phi_{i-1} + (2D - u \\Delta x) \\phi_{i+1}$ (Incorrect, results from using central difference for advection)\nC. $(u \\Delta x + 2D) \\phi_i = (u \\Delta x + D) \\phi_{i-1} + D \\phi_{i+1}$ (Correct)\nD. $(u \\Delta x + D) \\phi_i = (u \\Delta x + D) \\phi_{i-1}$ (Incorrect, missing diffusion term and $\\phi_{i+1}$)\nE. $(u \\Delta x - 2D) \\phi_i = (u \\Delta x - D) \\phi_{i-1} - D \\phi_{i+1}$ (Incorrect, sign errors from mishandling diffusion term)\n\nThus, the correct option is C.", "answer": "$$\\boxed{C}$$", "id": "1749174"}, {"introduction": "After building a foundation in one dimension, the next step is to extend our methods to handle the complexities of multi-dimensional flows. In two or three dimensions, the governing equations, such as the Navier-Stokes equations, often include mixed partial derivatives which describe how a rate of change in one direction varies in another. This practice [@problem_id:1749175] challenges you to derive a finite difference approximation for the mixed derivative term $\\frac{\\partial^2 u}{\\partial x \\partial y}$ by systematically applying the familiar central difference operator. Successfully completing this task is crucial for developing solvers for more realistic and complex fluid dynamics problems.", "problem": "In the numerical simulation of viscous fluid flows, the discretization of governing equations is a fundamental step. Consider a scenario where you are developing a Finite Difference Method (FDM) code to solve the Navier-Stokes equations. On a uniform Cartesian grid, a key term that appears in the viscous stress tensor components is the mixed partial derivative.\n\nThe computational domain consists of a uniform Cartesian grid where grid points are denoted by coordinates $(x_i, y_j) = (i \\Delta x, j \\Delta y)$, for integer indices $i, j$ and constant grid spacings $\\Delta x$ and $\\Delta y$. The value of a scalar field, such as the x-velocity component $u$, at a grid point $(x_i, y_j)$ is denoted as $u_{i,j}$.\n\nYour task is to derive a second-order accurate finite difference approximation for the mixed partial derivative term $\\frac{\\partial^2 u}{\\partial x \\partial y}$ at the grid node $(i,j)$. The derivation must proceed by sequentially applying the second-order central difference operator for the first derivative. Provide the final approximation as a single algebraic expression in terms of the grid point values of $u$ (e.g., $u_{i,j}, u_{i+1,j}$, etc.) and the grid spacings $\\Delta x$ and $\\Delta y$.", "solution": "We seek a second-order accurate approximation to the mixed derivative $\\frac{\\partial^{2} u}{\\partial x \\partial y}$ at $(i,j)$ by sequentially applying the second-order central difference operator for the first derivative.\n\nFor a smooth function $u(x,y)$ on a uniform Cartesian grid with spacings $\\Delta x$ and $\\Delta y$, the second-order central difference approximation to the first derivative in $y$ at $(i,j)$ is\n$$\n\\left(\\frac{\\partial u}{\\partial y}\\right)_{i,j} = \\frac{u_{i,j+1} - u_{i,j-1}}{2 \\Delta y} + \\mathcal{O}(\\Delta y^{2}).\n$$\nTo obtain $\\frac{\\partial^{2} u}{\\partial x \\partial y}$, differentiate this quantity with respect to $x$ using the second-order central difference in $x$:\n$$\n\\left(\\frac{\\partial^{2} u}{\\partial x \\partial y}\\right)_{i,j} = \\frac{\\left(\\frac{\\partial u}{\\partial y}\\right)_{i+1,j} - \\left(\\frac{\\partial u}{\\partial y}\\right)_{i-1,j}}{2 \\Delta x} + \\mathcal{O}(\\Delta x^{2}).\n$$\nNow approximate the $y$-derivatives at $(i+1,j)$ and $(i-1,j)$ with second-order central differences in $y$:\n$$\n\\left(\\frac{\\partial u}{\\partial y}\\right)_{i+1,j} = \\frac{u_{i+1,j+1} - u_{i+1,j-1}}{2 \\Delta y} + \\mathcal{O}(\\Delta y^{2}), \\quad\n\\left(\\frac{\\partial u}{\\partial y}\\right)_{i-1,j} = \\frac{u_{i-1,j+1} - u_{i-1,j-1}}{2 \\Delta y} + \\mathcal{O}(\\Delta y^{2}).\n$$\nSubstituting these into the $x$-difference gives\n$$\n\\left(\\frac{\\partial^{2} u}{\\partial x \\partial y}\\right)_{i,j} = \\frac{1}{2 \\Delta x} \\left[ \\frac{u_{i+1,j+1} - u_{i+1,j-1}}{2 \\Delta y} - \\frac{u_{i-1,j+1} - u_{i-1,j-1}}{2 \\Delta y} \\right] + \\mathcal{O}(\\Delta x^{2} + \\Delta y^{2}).\n$$\nCollecting terms yields the standard compact stencil\n$$\n\\left(\\frac{\\partial^{2} u}{\\partial x \\partial y}\\right)_{i,j} = \\frac{u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}}{4 \\Delta x \\Delta y} + \\mathcal{O}(\\Delta x^{2} + \\Delta y^{2}).\n$$\nThus the second-order accurate finite difference approximation, obtained by sequentially applying central differences for the first derivatives, is the above four-point formula.", "answer": "$$\\boxed{\\frac{u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}}{4\\,\\Delta x\\,\\Delta y}}$$", "id": "1749175"}, {"introduction": "Deriving discretization formulas is only half the battle; the true test of a numerical method lies in its implementation and verification. This final practice moves from theory to application, tasking you with solving a complete boundary value problem computationally. You will explore and compare two distinct, second-order accurate methods for implementing a Neumann (derivative) boundary condition—a critical and often challenging aspect of numerical simulation. By writing code to solve the problem and performing an error analysis against an exact solution, this exercise [@problem_id:2392721] demonstrates the essential engineering practice of convergence testing to verify that your implementation achieves its theoretical accuracy.", "problem": "Consider the one-dimensional, linear, second-order boundary value problem on the closed interval $[0,1]$ given by\n$$-u''(x) = f(x), \\quad x \\in (0,1),$$\nsubject to the mixed boundary conditions\n$$u'(0) = g, \\qquad u(1) = \\beta.$$\nAssume that an exact, sufficiently smooth solution $u(x)$ is known a priori, and that $f(x)$, $g$, and $\\beta$ are defined consistently by this exact solution.\n\nYour task is to write a complete, runnable program that, for each of the test cases listed below, constructs a uniform grid $x_j = j h$ with $h = 1/N$ for $j = 0,1,\\dots,N$ and $N \\in \\{20,40,80,160\\}$, approximates the solution using the standard second-order centered finite difference discretization of the interior operator $-u''(x)$, and implements the Neumann boundary condition at $x=0$ in two distinct ways that are each consistent with second-order accuracy:\n- Method A: Enforce the Neumann condition using an auxiliary unknown located outside the domain to achieve a second-order accurate boundary treatment.\n- Method B: Enforce the Neumann condition using a boundary approximation supported only on nodes within the domain that attains second-order accuracy.\n\nIn both methods, use the Dirichlet boundary condition at $x=1$ as $u(1)=\\beta$ directly. For each method and each $N$, compute the maximum-norm error\n$$E_\\infty(h) = \\max_{0 \\le j \\le N} \\left| u_h(x_j) - u(x_j) \\right|,$$\nwhere $u_h(x_j)$ denotes the numerical approximation at the node $x_j$. Then, for each method, estimate the observed order of accuracy $p$ by fitting the model $E_\\infty(h) \\approx C h^p$ to the data pairs $(h, E_\\infty(h))$ over the set of $N$ values using ordinary least squares (OLS) on $\\log E_\\infty$ versus $\\log h$. Use the natural logarithm or the base-$10$ logarithm consistently; the estimated exponent $p$ must be the fitted slope. For trigonometric functions below, interpret angles in radians.\n\nUse the following three test cases, each defined by an exact solution $u(x)$, the corresponding forcing $f(x) = -u''(x)$, the Neumann data $g = u'(0)$, and the Dirichlet data $\\beta = u(1)$:\n- Test 1 (smooth exponential): $u(x) = \\mathrm{e}^{x}$, so $f(x) = -\\mathrm{e}^{x}$, $g = 1$, and $\\beta = \\mathrm{e}$.\n- Test 2 (zero-flux hyperbolic cosine): $u(x) = \\cosh(x)$, so $f(x) = -\\cosh(x)$, $g = 0$, and $\\beta = \\cosh(1)$.\n- Test 3 (oscillatory affine): $u(x) = \\sin(2\\pi x) + x$, so $f(x) = 4\\pi^2 \\sin(2\\pi x)$, $g = 2\\pi + 1$, and $\\beta = 1$.\n\nFor each test case, report three real numbers:\n- the estimated order $p_A$ for Method A,\n- the estimated order $p_B$ for Method B,\n- the error ratio $R = E_{\\infty,A}(h_{\\min}) / E_{\\infty,B}(h_{\\min})$, where $h_{\\min}$ corresponds to $N=160$ for both methods.\n\nFinal output format: Your program should produce a single line of output containing all results for the three tests as a comma-separated list of decimal numbers enclosed in square brackets. The numbers must appear in the order $[p_A^{(1)}, p_B^{(1)}, R^{(1)}, p_A^{(2)}, p_B^{(2)}, R^{(2)}, p_A^{(3)}, p_B^{(3)}, R^{(3)}]$, where the superscript denotes the test index. Each numeric value must be printed rounded to exactly six digits after the decimal point. No additional text should be printed.\n\nThere are no physical units in this problem. All angles must be interpreted in radians.", "solution": "The user requests a solution to a one-dimensional, linear, second-order boundary value problem (BVP) using finite difference methods. The problem is to be solved on the interval $[0,1]$:\n$$ -u''(x) = f(x), \\quad x \\in (0,1) $$\nwith mixed boundary conditions:\n$$ u'(0) = g, \\quad u(1) = \\beta $$\nThe solution involves discretizing the domain and the differential operator, implementing the boundary conditions with second-order accuracy using two different methods, computing the error against a known exact solution for several grid resolutions, and estimating the order of accuracy.\n\nThe domain $[0,1]$ is discretized using a uniform grid with $N+1$ points $x_j = j h$ for $j = 0, 1, \\dots, N$, where $h = 1/N$ is the step size. The numerical solution is a vector of values $u_j \\approx u(x_j)$ at these grid points. The value $u_N = u(x_N) = u(1) = \\beta$ is known from the Dirichlet boundary condition. The unknowns are the $N$ values $u_0, u_1, \\dots, u_{N-1}$.\n\nAt the interior grid points $x_j$ for $j=1, \\dots, N-1$, the second derivative $-u''(x)$ is approximated using a second-order accurate centered finite difference stencil:\n$$ -u''(x_j) \\approx -\\frac{u(x_{j-1}) - 2u(x_j) + u(x_{j+1})}{h^2} $$\nApplying this discretization to the differential equation at each interior point $x_j$ gives a set of linear equations:\n$$ -\\frac{u_{j-1} - 2u_j - u_{j+1}}{h^2} = f(x_j) \\equiv f_j $$\nwhich can be rearranged to:\n$$ -u_{j-1} + 2u_j - u_{j+1} = h^2 f_j $$\nThese are $N-1$ equations for $j=1, \\dots, N-1$. The equation for $j=N-1$ involves the known value $u_N=\\beta$:\n$$ -u_{N-2} + 2u_{N-1} - u_N = h^2 f_{N-1} \\implies -u_{N-2} + 2u_{N-1} = h^2 f_{N-1} + \\beta $$\nThis equation's right-hand side is modified by moving the known term $\\beta$ over. To form a complete system of $N$ equations for the $N$ unknowns, an additional equation is required at the left boundary, $x_0=0$, derived from the Neumann condition $u'(0)=g$. Two methods are specified for this.\n\n**Method A: Ghost Point Discretization**\nThis method introduces a \"ghost\" point $x_{-1} = -h$ outside the domain. The Neumann condition $u'(0) = g$ is discretized using a second-order centered difference about $x_0$:\n$$ u'(0) \\approx \\frac{u_1 - u_{-1}}{2h} = g \\implies u_{-1} = u_1 - 2hg $$\nThe differential equation is assumed to hold at the boundary point $x_0=0$ as well. Applying the centered difference stencil for $-u''(0)$ gives:\n$$ -\\frac{u_{-1} - 2u_0 + u_1}{h^2} = f_0 $$\nSubstituting the expression for the ghost point value $u_{-1}$ into this equation eliminates it:\n$$ -\\frac{(u_1 - 2hg) - 2u_0 + u_1}{h^2} = f_0 \\implies 2u_0 - 2u_1 = h^2 f_0 - 2hg $$\nThis constitutes the first equation of the linear system.\n\n**Method B: One-Sided Discretization**\nThis method avoids points outside the domain by using a one-sided difference formula for the Neumann condition that is second-order accurate. Using a forward difference stencil involving $u_0, u_1, u_2$:\n$$ u'(0) \\approx \\frac{-3u_0 + 4u_1 - u_2}{2h} = g $$\nThis approximation is derived from a Taylor series expansion and has a truncation error of order $O(h^2)$. Rearranging gives the first equation of the linear system directly:\n$$ -3u_0 + 4u_1 - u_2 = 2hg $$\nNote that this equation replaces the discretized PDE at $x_0=0$.\n\n**Linear System and Solution**\nFor both methods, we construct an $N \\times N$ linear system of the form $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} = [u_0, u_1, \\dots, u_{N-1}]^T$.\n\nFor Method A, the system is:\n$$\n\\begin{pmatrix}\n2 & -2 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-2} \\\\ u_{N-1} \\end{pmatrix} =\n\\begin{pmatrix} h^2 f_0 - 2hg \\\\ h^2 f_1 \\\\ \\vdots \\\\ h^2 f_{N-2} \\\\ h^2 f_{N-1} + \\beta \\end{pmatrix}\n$$\n\nFor Method B, the system is:\n$$\n\\begin{pmatrix}\n-3 & 4 & -1 & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-2} \\\\ u_{N-1} \\end{pmatrix} =\n\\begin{pmatrix} 2hg \\\\ h^2 f_1 \\\\ \\vdots \\\\ h^2 f_{N-2} \\\\ h^2 f_{N-1} + \\beta \\end{pmatrix}\n$$\nThe system is solved for $\\mathbf{u}$ using a standard linear solver. The full numerical solution is then $\\mathbf{u}_h = [u_0, \\dots, u_{N-1}, \\beta]^T$.\n\n**Error Analysis and Order of Accuracy**\nThe maximum-norm error is computed for each grid size $h$:\n$$ E_\\infty(h) = \\max_{0 \\le j \\le N} \\left| u_j - u(x_j) \\right| $$\nThe theoretical error relationship is $E_\\infty(h) \\approx C h^p$, where $p$ is the order of accuracy. To estimate $p$, we perform a linear regression on the logarithmic transformation of this model:\n$$ \\log(E_\\infty(h)) \\approx \\log(C) + p \\log(h) $$\nThe slope of the regression line fitted to the data points $(\\log(h), \\log(E_\\infty(h)))$ for $N \\in \\{20, 40, 80, 160\\}$ provides the estimated order of accuracy $p$. This is achieved using ordinary least squares (OLS). The program uses `numpy.polyfit` with degree $1$ to compute this slope.\n\nThe program implements this procedure for each of the three test cases, calculates the estimated orders $p_A$ and $p_B$ for Methods A and B respectively, and computes the ratio $R$ of the errors from the two methods at the finest grid resolution ($N=160$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_bvp(N, u_exact_func, f_func, g, beta, method):\n    \"\"\"\n    Solves the BVP for a given N and method.\n    Returns the maximum-norm error.\n    \"\"\"\n    h = 1.0 / N\n    # Grid for unknowns u_0 to u_{N-1}\n    x_unknowns = np.linspace(0, 1.0 - h, N)\n    f_vals = f_func(x_unknowns)\n\n    # Initialize matrix A and vector b for the system A*u = b\n    A = np.zeros((N, N))\n    b = np.zeros(N)\n\n    # --- Construct the linear system ---\n\n    # Fill interior rows (j=1 to N-2) based on the centered difference stencil\n    # -u_{j-1} + 2u_j - u_{j+1} = h^2 * f_j\n    for j in range(1, N - 1):\n        A[j, j-1] = -1.0\n        A[j, j]   = 2.0\n        A[j, j+1] = -1.0\n        b[j]      = h**2 * f_vals[j]\n\n    # Equation for j=N-1 (near Dirichlet boundary)\n    # -u_{N-2} + 2u_{N-1} = h^2 * f_{N-1} + beta\n    if N > 1:\n        A[N-1, N-2] = -1.0\n    A[N-1, N-1] = 2.0\n    b[N-1] = h**2 * f_vals[N-1] + beta\n\n    # Equation for j=0 (Neumann boundary)\n    if method == 'A':\n        # Ghost point method: 2u_0 - 2u_1 = h^2*f_0 - 2*h*g\n        A[0, 0] = 2.0\n        if N > 1:\n            A[0, 1] = -2.0\n        b[0] = h**2 * f_vals[0] - 2.0 * h * g\n        # For N > 1, the j=1 equation is standard\n        if N > 1:\n            A[1, 0] = -1.0\n            A[1, 1] = 2.0\n            if N > 2:\n                A[1, 2] = -1.0\n            b[1] = h**2 * f_vals[1]\n\n    elif method == 'B':\n        # One-sided 2nd order method: -3u_0 + 4u_1 - u_2 = 2*h*g\n        A[0, 0] = -3.0\n        if N > 1:\n            A[0, 1] = 4.0\n        if N > 2:\n            A[0, 2] = -1.0\n        b[0] = 2.0 * h * g\n        # For N > 1, the j=1 equation is standard\n        if N > 1:\n            A[1, 0] = -1.0\n            A[1, 1] = 2.0\n            if N > 2:\n                A[1, 2] = -1.0\n            b[1] = h**2 * f_vals[1]\n\n    # --- Solve and compute error ---\n    \n    u_numeric_part = np.linalg.solve(A, b)\n    u_numeric = np.append(u_numeric_part, beta)\n\n    # Full grid from x_0 = 0 to x_N = 1\n    x_full = np.linspace(0, 1, N + 1)\n    u_exact_vals = u_exact_func(x_full)\n\n    error = np.max(np.abs(u_numeric - u_exact_vals))\n    return error\n\ndef solve_one_case(test_case_def):\n    \"\"\"\n    Processes one test case for all N values and both methods.\n    Returns p_A, p_B, R.\n    \"\"\"\n    u_exact_func, f_func, g, beta = test_case_def\n    \n    N_vals = np.array([20, 40, 80, 160])\n    h_vals = 1.0 / N_vals\n    \n    errors_A = []\n    errors_B = []\n    \n    for N in N_vals:\n        errors_A.append(solve_bvp(N, u_exact_func, f_func, g, beta, 'A'))\n        errors_B.append(solve_bvp(N, u_exact_func, f_func, g, beta, 'B'))\n\n    # Use OLS on log-log data to find order of accuracy p\n    log_h_vals = np.log(h_vals)\n    log_errors_A = np.log(errors_A)\n    log_errors_B = np.log(errors_B)\n    \n    p_A, _ = np.polyfit(log_h_vals, log_errors_A, 1)\n    p_B, _ = np.polyfit(log_h_vals, log_errors_B, 1)\n    \n    # Error ratio at the finest grid\n    R = errors_A[-1] / errors_B[-1]\n    \n    return p_A, p_B, R\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: u(x) = exp(x)\n        (lambda x: np.exp(x), lambda x: -np.exp(x), 1.0, np.exp(1.0)),\n        # Test 2: u(x) = cosh(x)\n        (lambda x: np.cosh(x), lambda x: -np.cosh(x), 0.0, np.cosh(1.0)),\n        # Test 3: u(x) = sin(2*pi*x) + x\n        (lambda x: np.sin(2*np.pi*x) + x, lambda x: 4*np.pi**2*np.sin(2*np.pi*x), 2*np.pi + 1, 1.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        p_A, p_B, R = solve_one_case(case)\n        results.extend([p_A, p_B, R])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2392721"}]}