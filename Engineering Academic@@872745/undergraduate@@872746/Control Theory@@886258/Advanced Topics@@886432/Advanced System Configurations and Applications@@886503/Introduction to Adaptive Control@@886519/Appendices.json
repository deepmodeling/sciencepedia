{"hands_on_practices": [{"introduction": "At the heart of many adaptive systems is a simple yet powerful idea: using performance errors to progressively refine a model of the world. This exercise introduces the foundational mechanism for this process, a gradient-based update law, often related to the \"MIT rule.\" By applying this method to a practical scenario involving a smart thermostat learning its environment's thermal properties, you will perform a concrete, step-by-step calculation of how an adaptive system learns from data. This practice transforms an abstract algorithm into a tangible procedure, forming the bedrock of your understanding of adaptive control. [@problem_id:1582140]", "problem": "A smart thermostat uses a simplified model to relate the steady-state temperature difference, $\\Delta T$ (in Kelvin, K), between the inside of a room and the outside environment, to the power, $P$ (in Watts, W), supplied by a heater. The model is given by the linear relationship $\\Delta T = R_{th} P$, where $R_{th}$ is the effective thermal resistance of the room, a parameter that the thermostat needs to learn.\n\nTo estimate this parameter, the thermostat's control algorithm updates an estimate, $\\hat{R}_{th,k}$, at discrete time steps $k=0, 1, 2, \\dots$. The update is based on a gradient descent method designed to minimize the squared prediction error from the current time step, which is formulated as a cost function $J_k = \\frac{1}{2} e_k^2$, where $e_k = \\Delta T_k - \\hat{R}_{th,k} P_k$. The update rule for the estimate is given by:\n$$ \\hat{R}_{th, k+1} = \\hat{R}_{th, k} - \\mu \\left. \\frac{\\partial J_k}{\\partial \\hat{R}_{th}} \\right|_{\\hat{R}_{th} = \\hat{R}_{th,k}} $$\nwhere $\\mu$ is a constant positive learning rate.\n\nYou are given the following experimental data and parameters:\n- The initial estimate for the thermal resistance is $\\hat{R}_{th,0} = 0.0500 \\text{ K/W}$.\n- The learning rate is $\\mu = 5.00 \\times 10^{-5} \\text{ W}^{-2}$.\n- At time step $k=0$, the heater power was $P_0 = 100.0 \\text{ W}$ and the measured temperature difference was $\\Delta T_0 = 6.00 \\text{ K}$.\n- At time step $k=1$, the heater power was $P_1 = 150.0 \\text{ W}$ and the measured temperature difference was $\\Delta T_1 = 9.00 \\text{ K}$.\n\nCalculate the value of the estimate $\\hat{R}_{th,2}$. Express your answer in K/W, rounded to three significant figures.", "solution": "We are given the linear thermal model $\\Delta T = R_{th} P$ and the instantaneous squared-error cost $J_{k} = \\frac{1}{2} e_{k}^{2}$ with $e_{k} = \\Delta T_{k} - \\hat{R}_{th,k} P_{k}$. The gradient of $J_{k}$ with respect to $\\hat{R}_{th}$ is computed using the chain rule:\n$$\n\\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} = \\frac{\\partial}{\\partial \\hat{R}_{th}} \\left( \\frac{1}{2} e_{k}^{2} \\right) = e_{k} \\frac{\\partial e_{k}}{\\partial \\hat{R}_{th}}, \\quad \\text{and} \\quad \\frac{\\partial e_{k}}{\\partial \\hat{R}_{th}} = -P_{k},\n$$\nhence\n$$\n\\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} = - e_{k} P_{k}.\n$$\nThe gradient descent update is\n$$\n\\hat{R}_{th,k+1} = \\hat{R}_{th,k} - \\mu \\left. \\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} \\right|_{\\hat{R}_{th} = \\hat{R}_{th,k}} = \\hat{R}_{th,k} + \\mu\\, e_{k} P_{k}.\n$$\nEquivalently,\n$$\n\\hat{R}_{th,k+1} = \\hat{R}_{th,k} + \\mu\\, P_{k} \\left( \\Delta T_{k} - \\hat{R}_{th,k} P_{k} \\right).\n$$\n\nStep k = 0:\nGiven $\\hat{R}_{th,0} = 0.0500$, $P_{0} = 100.0$, $\\Delta T_{0} = 6.00$, compute\n$$\ne_{0} = \\Delta T_{0} - \\hat{R}_{th,0} P_{0} = 6.00 - (0.0500)(100.0) = 6.00 - 5.00 = 1.00.\n$$\nUpdate:\n$$\n\\hat{R}_{th,1} = \\hat{R}_{th,0} + \\mu e_{0} P_{0} = 0.0500 + (5.00 \\times 10^{-5})(1.00)(100.0) = 0.0500 + 0.00500 = 0.0550.\n$$\n\nStep k = 1:\nGiven $P_{1} = 150.0$, $\\Delta T_{1} = 9.00$, compute\n$$\ne_{1} = \\Delta T_{1} - \\hat{R}_{th,1} P_{1} = 9.00 - (0.0550)(150.0) = 9.00 - 8.25 = 0.75.\n$$\nUpdate:\n$$\n\\hat{R}_{th,2} = \\hat{R}_{th,1} + \\mu e_{1} P_{1} = 0.0550 + (5.00 \\times 10^{-5})(0.75)(150.0).\n$$\nCompute the increment:\n$$\n(5.00 \\times 10^{-5})(0.75)(150.0) = 5.00 \\times 10^{-5} \\times 112.5 = 0.005625.\n$$\nTherefore,\n$$\n\\hat{R}_{th,2} = 0.0550 + 0.005625 = 0.060625.\n$$\nRounded to three significant figures, this is $0.0606$.", "answer": "$$\\boxed{0.0606}$$", "id": "1582140"}, {"introduction": "After seeing how to update a parameter estimate, a critical question follows: will the estimate converge to the true value? The answer depends not just on the update rule, but on the quality of the data the system observes. This problem explores the crucial concept of \"persistent excitation,\" which dictates that input signals must be sufficiently \"rich\" to ensure all unknown parameters can be uniquely identified. By analyzing the frequency content needed to characterize a system, you will gain insight into how to design experiments or select inputs that make learning possible. [@problem_id:1582162]", "problem": "A control engineer is tasked with creating a high-fidelity digital model of an unknown mechanical system. The system's input-output behavior, relating an applied force $u(t)$ to the resulting displacement $y(t)$, is known to be accurately described by a second-order linear time-invariant (LTI) differential equation:\n$$\n\\frac{d^2 y(t)}{dt^2} + a_1 \\frac{dy(t)}{dt} + a_0 y(t) = b_0 u(t)\n$$\nThe parameters $a_1$, $a_0$, and $b_0$ are real, positive, and unknown constants that characterize the system's physical properties (damping, stiffness, and input scaling, respectively).\n\nTo perform system identification, the engineer decides to apply a specific test input signal $u(t)$ and measure the system's response $y(t)$. The identification algorithm to be used guarantees a unique solution for the parameter set $\\{a_0, a_1, b_0\\}$ if and only if the input signal is \"sufficiently rich\" in its frequency content. The engineer chooses to construct the input signal as a sum of a finite number of sinusoids:\n$$\nu(t) = \\sum_{i=1}^{N} A_i \\cos(\\omega_i t)\n$$\nHere, $N$ is the number of sinusoidal components, the amplitudes $A_i$ are all non-zero, and the angular frequencies $\\omega_i$ are distinct and strictly positive constants ($\\omega_i > 0$).\n\nWhat is the minimum integer value of $N$ that is required to guarantee that the three unknown parameters $\\{a_0, a_1, b_0\\}$ can be uniquely determined from the measurements of $u(t)$ and $y(t)$?\n\nA. 1\n\nB. 2\n\nC. 3\n\nD. 4\n\nE. 5", "solution": "Take the Laplace transform of the given LTI differential equation under zero initial conditions to obtain the transfer function from input to output:\n$$\ns^{2}Y(s)+a_{1}sY(s)+a_{0}Y(s)=b_{0}U(s)\\quad\\Rightarrow\\quad G(s)\\triangleq\\frac{Y(s)}{U(s)}=\\frac{b_{0}}{s^{2}+a_{1}s+a_{0}}.\n$$\nFor a sinusoidal input at angular frequency $\\omega>0$, the steady-state (phasor) response satisfies\n$$\nG(j\\omega)=\\frac{b_{0}}{a_{0}-\\omega^{2}+j a_{1}\\omega},\n$$\nwhere $j^{2}=-1$. By linearity and superposition, for the multisine input $u(t)=\\sum_{i=1}^{N}A_{i}\\cos(\\omega_{i}t)$ with distinct $\\omega_{i}>0$ and nonzero $A_{i}$, the steady-state output is the sum of sinusoidal components at the same frequencies, and at each $\\omega_{i}$ the complex frequency response $H_{i}\\triangleq G(j\\omega_{i})$ can be estimated from the measured input-output data (amplitude ratio and phase). Write $H_{i}=\\alpha_{i}+j\\beta_{i}$, where $\\alpha_{i},\\beta_{i}$ are real and known from measurements.\n\nFor each $i$, the model implies\n$$\nH_{i}\\bigl(a_{0}-\\omega_{i}^{2}+j a_{1}\\omega_{i}\\bigr)=b_{0}.\n$$\nEquating real and imaginary parts yields two real equations:\n$$\n\\alpha_{i}a_{1}\\omega_{i}+\\beta_{i}\\bigl(a_{0}-\\omega_{i}^{2}\\bigr)=0,\n$$\n$$\n\\alpha_{i}\\bigl(a_{0}-\\omega_{i}^{2}\\bigr)-\\beta_{i}a_{1}\\omega_{i}=b_{0}.\n$$\nAcross $N$ distinct frequencies, this provides $2N$ real equations in the three unknowns $a_{0},a_{1},b_{0}$.\n\nFirst, show that $N=1$ is insufficient. With a single frequency, the two equations above leave one degree of freedom among three unknowns, so the parameters cannot be uniquely determined.\n\nNext, show that $N=2$ is sufficient. Use the imaginary-part equations at $i=1,2$ to solve linearly for $a_{0}$ and $a_{1}$:\n$$\n\\beta_{i}a_{0}+\\alpha_{i}a_{1}\\omega_{i}=\\beta_{i}\\omega_{i}^{2},\\quad i\\in\\{1,2\\}.\n$$\nIn matrix form,\n$$\n\\begin{pmatrix}\n\\beta_{1} & \\alpha_{1}\\omega_{1}\\\\\n\\beta_{2} & \\alpha_{2}\\omega_{2}\n\\end{pmatrix}\n\\begin{pmatrix}\na_{0}\\\\\na_{1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\beta_{1}\\omega_{1}^{2}\\\\\n\\beta_{2}\\omega_{2}^{2}\n\\end{pmatrix}.\n$$\nTo guarantee a unique solution, the determinant\n$$\n\\Delta=\\beta_{1}\\alpha_{2}\\omega_{2}-\\beta_{2}\\alpha_{1}\\omega_{1}\n$$\nmust be nonzero. For data generated by the given model with $a_{0}>0$, $a_{1}>0$, $b_{0}>0$, and distinct positive $\\omega_{1}\\neq\\omega_{2}$, we can express $\\alpha_{i},\\beta_{i}$ explicitly in terms of the true parameters:\n$$\n\\alpha_{i}=\\frac{b_{0}(a_{0}-\\omega_{i}^{2})}{(a_{0}-\\omega_{i}^{2})^{2}+(a_{1}\\omega_{i})^{2}},\\quad\n\\beta_{i}=-\\frac{b_{0}(a_{1}\\omega_{i})}{(a_{0}-\\omega_{i}^{2})^{2}+(a_{1}\\omega_{i})^{2}}.\n$$\nSubstituting these into $\\Delta$ gives\n$$\n\\Delta=\\frac{b_{0}^{2}}{\\bigl((a_{0}-\\omega_{1}^{2})^{2}+(a_{1}\\omega_{1})^{2}\\bigr)\\bigl((a_{0}-\\omega_{2}^{2})^{2}+(a_{1}\\omega_{2})^{2}\\bigr)}\n\\cdot a_{1}\\omega_{1}\\omega_{2}\\bigl(\\omega_{2}^{2}-\\omega_{1}^{2}\\bigr).\n$$\nSince $a_{1}>0$, $b_{0}>0$, $\\omega_{1},\\omega_{2}>0$, and $\\omega_{1}\\neq\\omega_{2}$, all factors are nonzero, hence $\\Delta\\neq 0$. Therefore the two imaginary-part equations uniquely determine $a_{0}$ and $a_{1}$. With $a_{0}$ and $a_{1}$ known, solve for $b_{0}$ from the real-part equation at either frequency:\n$$\nb_{0}=\\alpha_{i}\\bigl(a_{0}-\\omega_{i}^{2}\\bigr)-\\beta_{i}a_{1}\\omega_{i},\\quad i\\in\\{1,2\\},\n$$\nwhich yields the same $b_{0}$ for both $i$.\n\nThus, two distinct sinusoidal components are sufficient to uniquely identify $\\{a_{0},a_{1},b_{0}\\}$, while one is insufficient. Therefore, the minimum required $N$ is $2$.", "answer": "$$\\boxed{B}$$", "id": "1582162"}, {"introduction": "A common and subtle trap in adaptive control is to assume that achieving the control objective, such as forcing a tracking error to zero, automatically means the system's parameters have been correctly identified. This is not always true, and this exercise demonstrates a classic case where this intuition fails. You will analyze a system where a lack of persistent excitation allows the controller to perform its task perfectly, even while the parameter estimates converge to incorrect values. This practice is essential for appreciating the distinct challenges of control versus identification within an adaptive framework. [@problem_id:1582114]", "problem": "Consider a first-order dynamical system whose behavior is influenced by two unknown, constant parameters, $\\theta_1$ and $\\theta_2$. The system is governed by the differential equation:\n$$ \\dot{y}(t) = u(t) + \\theta_1 w_1(t) + \\theta_2 w_2(t) $$\nwhere $y(t)$ is the system output, $u(t)$ is the control input, and $w_1(t)$ and $w_2(t)$ are known periodic signals, often called regressor signals.\n\nTo regulate the output $y(t)$ to zero, an adaptive controller is implemented. The control law is defined as:\n$$ u(t) = -k y(t) - \\hat{\\theta}_1(t) w_1(t) - \\hat{\\theta}_2(t) w_2(t) $$\nwhere $\\hat{\\theta}_1(t)$ and $\\hat{\\theta}_2(t)$ are the online estimates of the unknown parameters $\\theta_1$ and $\\theta_2$. The parameter estimates are updated according to the following adaptation laws:\n$$ \\dot{\\hat{\\theta}}_1(t) = \\gamma_1 y(t) w_1(t) $$\n$$ \\dot{\\hat{\\theta}}_2(t) = \\gamma_2 y(t) w_2(t) $$\nwhere $\\gamma_1$ and $\\gamma_2$ are positive constants known as adaptation gains.\n\nYou are given the following specific values:\n- True (but unknown to the controller) parameters: $\\theta_1 = 3.0$, $\\theta_2 = 1.5$\n- Controller and adaptation gains: $k = 10.0$, $\\gamma_1 = 5.0$, $\\gamma_2 = 10.0$\n- Regressor signals: $w_1(t) = \\cos(3t)$, $w_2(t) = 2\\cos(3t)$\n- Initial conditions: $y(0) = 0$, $\\hat{\\theta}_1(0) = 1.0$, $\\hat{\\theta}_2(0) = 8.0$\n\nThis adaptive scheme guarantees that the output converges to zero, i.e., $\\lim_{t\\to\\infty} y(t) = 0$. Your task is to determine the final, steady-state values to which the parameter estimates converge. Let these be $\\hat{\\theta}_{1,\\infty} = \\lim_{t\\to\\infty} \\hat{\\theta}_1(t)$ and $\\hat{\\theta}_{2,\\infty} = \\lim_{t\\to\\infty} \\hat{\\theta}_2(t)$.\n\nExpress your answer as a pair of exact fractions $(\\hat{\\theta}_{1,\\infty}, \\hat{\\theta}_{2,\\infty})$.", "solution": "Define the parameter-estimation errors $\\tilde{\\theta}_{1}(t) \\triangleq \\theta_{1} - \\hat{\\theta}_{1}(t)$ and $\\tilde{\\theta}_{2}(t) \\triangleq \\theta_{2} - \\hat{\\theta}_{2}(t)$. Substituting the control law into the plant gives\n$$\n\\dot{y}(t) = -k\\,y(t) + \\tilde{\\theta}_{1}(t)\\,w_{1}(t) + \\tilde{\\theta}_{2}(t)\\,w_{2}(t).\n$$\nThe adaptation laws yield\n$$\n\\dot{\\tilde{\\theta}}_{1}(t) = -\\gamma_{1}\\,y(t)\\,w_{1}(t), \\qquad \\dot{\\tilde{\\theta}}_{2}(t) = -\\gamma_{2}\\,y(t)\\,w_{2}(t).\n$$\nWith the given regressors $w_{1}(t)=\\cos(3t)$ and $w_{2}(t)=2\\cos(3t)$, define $w(t)\\triangleq \\cos(3t)$ and the combination\n$$\ns(t) \\triangleq \\tilde{\\theta}_{1}(t) + 2\\,\\tilde{\\theta}_{2}(t).\n$$\nThen the closed-loop output dynamics and the $s$-dynamics become\n$$\n\\dot{y}(t) = -k\\,y(t) + w(t)\\,s(t), \\qquad \\dot{s}(t) = -\\left(\\gamma_{1} + 4\\gamma_{2}\\right)\\,y(t)\\,w(t).\n$$\nBy assumption, $\\lim_{t\\to\\infty} y(t)=0$. For $y(t)\\equiv 0$ to be invariant, we must have $\\dot{y}(t)\\equiv 0$ as $t\\to\\infty$. Since $w(t)=\\cos(3t)$ is not identically zero, invariance requires\n$$\ns_{\\infty} \\triangleq \\lim_{t\\to\\infty} s(t) = 0,\n$$\ni.e.,\n$$\n\\tilde{\\theta}_{1,\\infty} + 2\\,\\tilde{\\theta}_{2,\\infty} = 0.\n$$\nNext, construct a conserved quantity. Consider\n$$\nr(t) \\triangleq 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(t) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(t).\n$$\nIts time derivative, using the adaptation laws and $w_{2}(t)=2w(t)$, is\n$$\n\\dot{r}(t) = 2\\gamma_{2}\\left(-\\gamma_{1}y(t)w(t)\\right) - \\gamma_{1}\\left(-\\gamma_{2}y(t)\\,2w(t)\\right) = -2\\gamma_{1}\\gamma_{2}y(t)w(t) + 2\\gamma_{1}\\gamma_{2}y(t)w(t) = 0.\n$$\nHence $r(t)$ is constant:\n$$\nr(t) \\equiv r(0) = 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(0) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(0).\n$$\nWith the given values $\\theta_{1}=3$, $\\theta_{2}=\\frac{3}{2}$, $\\hat{\\theta}_{1}(0)=1$, $\\hat{\\theta}_{2}(0)=8$, $\\gamma_{1}=5$, $\\gamma_{2}=10$, the initial errors are\n$$\n\\tilde{\\theta}_{1}(0) = \\theta_{1} - \\hat{\\theta}_{1}(0) = 3 - 1 = 2, \\qquad \\tilde{\\theta}_{2}(0) = \\theta_{2} - \\hat{\\theta}_{2}(0) = \\frac{3}{2} - 8 = -\\frac{13}{2},\n$$\nso\n$$\nr(0) = 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(0) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(0) = 2\\cdot 10 \\cdot 2 - 5\\left(-\\frac{13}{2}\\right) = 40 + \\frac{65}{2} = \\frac{145}{2}.\n$$\nAt steady state, the pair $(\\tilde{\\theta}_{1,\\infty},\\tilde{\\theta}_{2,\\infty})$ must satisfy the linear system\n$$\n\\tilde{\\theta}_{1,\\infty} + 2\\,\\tilde{\\theta}_{2,\\infty} = 0, \\qquad 2\\gamma_{2}\\,\\tilde{\\theta}_{1,\\infty} - \\gamma_{1}\\,\\tilde{\\theta}_{2,\\infty} = \\frac{145}{2}.\n$$\nFrom the first equation, $\\tilde{\\theta}_{1,\\infty} = -2\\,\\tilde{\\theta}_{2,\\infty}$. Substituting into the second yields\n$$\n-4\\gamma_{2}\\,\\tilde{\\theta}_{2,\\infty} - \\gamma_{1}\\,\\tilde{\\theta}_{2,\\infty} = \\frac{145}{2} \\;\\;\\Rightarrow\\;\\; \\tilde{\\theta}_{2,\\infty} = -\\frac{145/2}{4\\gamma_{2}+\\gamma_{1}}.\n$$\nWith $\\gamma_{2}=10$, $\\gamma_{1}=5$, we have $4\\gamma_{2}+\\gamma_{1}=45$, hence\n$$\n\\tilde{\\theta}_{2,\\infty} = -\\frac{145}{90} = -\\frac{29}{18}, \\qquad \\tilde{\\theta}_{1,\\infty} = -2\\,\\tilde{\\theta}_{2,\\infty} = \\frac{29}{9}.\n$$\nFinally, the steady-state parameter estimates are\n$$\n\\hat{\\theta}_{1,\\infty} = \\theta_{1} - \\tilde{\\theta}_{1,\\infty} = 3 - \\frac{29}{9} = \\frac{27}{9} - \\frac{29}{9} = -\\frac{2}{9}, \\qquad \\hat{\\theta}_{2,\\infty} = \\theta_{2} - \\tilde{\\theta}_{2,\\infty} = \\frac{3}{2} + \\frac{29}{18} = \\frac{27}{18} + \\frac{29}{18} = \\frac{56}{18} = \\frac{28}{9}.\n$$\nAs a check, the identifiable combination satisfies $\\hat{\\theta}_{1,\\infty} + 2\\hat{\\theta}_{2,\\infty} = -\\frac{2}{9} + 2\\left(\\frac{28}{9}\\right) = \\frac{54}{9} = 6$, which equals $\\theta_{1} + 2\\theta_{2} = 3 + 2(1.5) = 6$. This is consistent with $s_{\\infty}=0$.", "answer": "$$\\boxed{\\left(-\\frac{2}{9}, \\frac{28}{9}\\right)}$$", "id": "1582114"}]}