{"hands_on_practices": [{"introduction": "The Extended Kalman Filter adapts the linear Kalman filter to nonlinear systems by performing a local linearization at each time step. Mastering this linearization is the foundational skill for implementing an EKF. This first exercise [@problem_id:1574777] focuses on this core mathematical step, asking you to compute the Jacobian matrix for a nonlinear state transition function. This matrix, denoted $F_k$, is crucial as it approximates how the system's state evolves, forming the basis for the EKF's prediction step.", "problem": "In the analysis of nonlinear dynamical systems for applications such as state estimation, a common technique is to perform linearization around a nominal state trajectory. This is a foundational step in deriving the equations for the Extended Kalman Filter (EKF).\n\nConsider a discrete-time nonlinear system where the state at time step $k$ is represented by the vector $x_k = \\begin{pmatrix} p_k \\\\ q_k \\end{pmatrix}$. The system's state evolves according to the nonlinear function $f(x_k)$ such that $x_{k+1} = f(x_k)$, defined as:\n$$\nx_{k+1} = \\begin{pmatrix} p_{k+1} \\\\ q_{k+1} \\end{pmatrix} = f(p_k, q_k) = \\begin{pmatrix} p_k^{2} - q_k \\\\ p_k + \\sin(q_k) \\end{pmatrix}\n$$\nThe local linear approximation of this system is described by the Jacobian matrix of the state transition function, traditionally denoted as $F_k$. This matrix is defined by the partial derivatives of the function $f$ with respect to the components of the state vector $x_k$.\n\nDetermine the Jacobian matrix $F_k = \\frac{\\partial f}{\\partial x} \\bigg|_{x=x_k}$. Express your answer as a matrix whose elements are functions of $p_k$ and $q_k$. Assume that the argument of any trigonometric function is in radians.", "solution": "We are given the discrete-time nonlinear system with state $x_{k} = \\begin{pmatrix} p_{k} \\\\ q_{k} \\end{pmatrix}$ and state transition function\n$$\nf(p_{k}, q_{k}) = \\begin{pmatrix} f_{1}(p_{k}, q_{k}) \\\\ f_{2}(p_{k}, q_{k}) \\end{pmatrix} = \\begin{pmatrix} p_{k}^{2} - q_{k} \\\\ p_{k} + \\sin(q_{k}) \\end{pmatrix}.\n$$\nThe Jacobian matrix $F_{k} = \\frac{\\partial f}{\\partial x}\\bigg|_{x=x_{k}}$ collects the first-order partial derivatives of the components of $f$ with respect to the state variables. By definition, its entries are\n$$\n[F_{k}]_{ij} = \\frac{\\partial f_{i}}{\\partial x_{j}} \\bigg|_{(p_{k}, q_{k})},\n$$\nwhere $x_{1} = p_{k}$ and $x_{2} = q_{k}$.\n\nCompute the partial derivatives:\n- For $f_{1}(p_{k}, q_{k}) = p_{k}^{2} - q_{k}$, using the power rule and linearity,\n$$\n\\frac{\\partial f_{1}}{\\partial p_{k}} = 2 p_{k}, \\qquad \\frac{\\partial f_{1}}{\\partial q_{k}} = -1.\n$$\n- For $f_{2}(p_{k}, q_{k}) = p_{k} + \\sin(q_{k})$, using linearity and $\\frac{d}{dq}\\sin(q) = \\cos(q)$ (with $q$ in radians),\n$$\n\\frac{\\partial f_{2}}{\\partial p_{k}} = 1, \\qquad \\frac{\\partial f_{2}}{\\partial q_{k}} = \\cos(q_{k}).\n$$\n\nAssembling these into the Jacobian gives\n$$\nF_{k} = \\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial p_{k}} & \\frac{\\partial f_{1}}{\\partial q_{k}} \\\\\n\\frac{\\partial f_{2}}{\\partial p_{k}} & \\frac{\\partial f_{2}}{\\partial q_{k}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 p_{k} & -1 \\\\\n1 & \\cos(q_{k})\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 2 p_{k} & -1 \\\\ 1 & \\cos(q_{k}) \\end{pmatrix}}$$", "id": "1574777"}, {"introduction": "Moving beyond pure calculation, it is essential to build an intuitive understanding of how the EKF works. The real power of the filter lies in how it intelligently fuses predictions with measurements to reduce uncertainty. This practice [@problem_id:1574759] provides a geometric perspective on the EKF's update step, using an intuitive scenario of a drone locating a target. By analyzing how a bearing measurement reshapes the uncertainty from a simple circle into a specific ellipse, you will gain a deeper appreciation for how the measurement Jacobian dictates the flow of information within the filter.", "problem": "An autonomous drone is hovering at a stationary position, which we define as the origin $(0,0)$ of a 2D Cartesian coordinate system. It is tasked with estimating the position of a stationary target on the ground. The state of the target is represented by the vector $x = [p_x, p_y]^T$.\n\nInitially, the drone's belief about the target's position is described by a Gaussian distribution with a mean $\\hat{x}_{k|k-1}$ and a covariance matrix $P_{k|k-1}$. The initial uncertainty is isotropic, meaning the covariance matrix is of the form $P_{k|k-1} = \\sigma^2 I$, where $\\sigma^2$ is a positive constant and $I$ is the 2x2 identity matrix. This implies the initial uncertainty contour is a circle.\n\nThe drone then uses its onboard camera to take a single, highly precise measurement of the bearing angle to the target. The measurement function is $h(x) = \\arctan(p_y/p_x)$, and the measurement noise is modeled as a zero-mean Gaussian with a very small variance, $R_k = \\sigma_\\theta^2$.\n\nAn Extended Kalman Filter (EKF) is used to update the state estimate and its covariance based on this new measurement, resulting in a posterior estimate $\\hat{x}_{k|k}$ and a posterior covariance $P_{k|k}$. The posterior covariance matrix $P_{k|k}$ describes the new uncertainty, which can be visualized as an ellipse in the 2D plane. The line of sight is defined as the line connecting the drone (at the origin) to the estimated position of the target.\n\nWhich of the following statements most accurately describes the orientation of this updated uncertainty ellipse?\n\nA. The major axis (longest axis) of the ellipse is oriented perpendicular to the line of sight.\n\nB. The ellipse remains a circle, but with a radius smaller than the initial uncertainty.\n\nC. The major axis (longest axis) of the ellipse is oriented along the line of sight.\n\nD. The orientation of the ellipse is at a 45-degree angle to the line of sight.\n\nE. The shape of the uncertainty region is no longer an ellipse due to the non-linear measurement.", "solution": "Let the state be $x = [p_{x}, p_{y}]^{T}$ and the prior covariance be $P_{k|k-1} = \\sigma^{2} I$. The measurement is the bearing $\\theta = h(x) = \\arctan(p_{y}/p_{x})$ with noise covariance $R_{k} = \\sigma_{\\theta}^{2}$. Assume the linearization point $\\hat{x}_{k|k-1} = [\\hat{p}_{x}, \\hat{p}_{y}]^{T}$ satisfies $\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2} > 0$ so that the Jacobian exists.\n\nThe EKF uses the measurement Jacobian $H = \\partial h/\\partial x$ evaluated at $\\hat{x}_{k|k-1}$. Using the derivative of $\\arctan(p_{y}/p_{x})$ with respect to $p_{x}$ and $p_{y}$,\n$$\nH = \\left[ \\frac{\\partial h}{\\partial p_{x}} \\;\\; \\frac{\\partial h}{\\partial p_{y}} \\right]\n= \\left[ -\\frac{\\hat{p}_{y}}{\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}} \\;\\; \\frac{\\hat{p}_{x}}{\\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}} \\right].\n$$\nDefine $r^{2} = \\hat{p}_{x}^{2} + \\hat{p}_{y}^{2}$. Then $H = \\frac{1}{r^{2}}[-\\hat{p}_{y}, \\hat{p}_{x}]$.\n\nThe innovation covariance is\n$$\nS = H P_{k|k-1} H^{T} + R_{k} = \\sigma^{2} H H^{T} + \\sigma_{\\theta}^{2}.\n$$\nSince $H H^{T} = \\frac{\\hat{p}_{y}^{2} + \\hat{p}_{x}^{2}}{r^{4}} = \\frac{1}{r^{2}}$, we obtain\n$$\nS = \\frac{\\sigma^{2}}{r^{2}} + \\sigma_{\\theta}^{2}.\n$$\nThe Kalman gain is\n$$\nK = P_{k|k-1} H^{T} S^{-1} = \\sigma^{2} I \\, H^{T} S^{-1} = \\frac{\\sigma^{2}}{S} H^{T}.\n$$\nTherefore $K$ is proportional to $H^{T} = \\frac{1}{r^{2}}[-\\hat{p}_{y}, \\hat{p}_{x}]^{T}$, which is orthogonal to the line-of-sight direction $[\\hat{p}_{x}, \\hat{p}_{y}]^{T}$ because $[-\\hat{p}_{y}, \\hat{p}_{x}] \\cdot [\\hat{p}_{x}, \\hat{p}_{y}] = 0$. Thus, the state update moves primarily perpendicular to the line of sight.\n\nThe posterior covariance for a scalar measurement is\n$$\nP_{k|k} = (I - K H) P_{k|k-1} = P_{k|k-1} - K H P_{k|k-1}.\n$$\nCompute the rank-one term:\n$$\nK H P_{k|k-1} = \\left(\\frac{\\sigma^{2}}{S} H^{T}\\right) \\left(H \\sigma^{2}\\right) = \\frac{\\sigma^{4}}{S} \\left(H^{T} H\\right).\n$$\nNote that\n$$\nH^{T} H = \\frac{1}{r^{4}}\n\\begin{bmatrix}\n\\hat{p}_{y}^{2} & -\\hat{p}_{x}\\hat{p}_{y} \\\\\n-\\hat{p}_{x}\\hat{p}_{y} & \\hat{p}_{x}^{2}\n\\end{bmatrix}.\n$$\nIntroduce the orthonormal basis aligned with radial (line-of-sight) and tangential (perpendicular) directions:\n$$\nu_{r} = \\frac{1}{r} \\begin{bmatrix} \\hat{p}_{x} \\\\ \\hat{p}_{y} \\end{bmatrix}, \\quad\nu_{t} = \\frac{1}{r} \\begin{bmatrix} -\\hat{p}_{y} \\\\ \\hat{p}_{x} \\end{bmatrix}.\n$$\nThen $H = \\frac{1}{r} u_{t}^{T}$ and $H^{T} H = \\frac{1}{r^{2}} u_{t} u_{t}^{T}$. Consequently,\n$$\nP_{k|k} = \\sigma^{2} I - \\frac{\\sigma^{4}}{S} \\cdot \\frac{1}{r^{2}} \\, u_{t} u_{t}^{T}.\n$$\nThis shows that the posterior covariance is reduced only along $u_{t}$ (perpendicular to the line of sight) and remains unchanged along $u_{r}$ (the line of sight). Specifically, the posterior variances along these principal directions are\n$$\n\\lambda_{r}^{+} = u_{r}^{T} P_{k|k} u_{r} = \\sigma^{2},\n\\qquad\n\\lambda_{t}^{+} = u_{t}^{T} P_{k|k} u_{t} = \\sigma^{2} - \\frac{\\sigma^{4}}{S} \\cdot \\frac{1}{r^{2}}.\n$$\nSince $S = \\frac{\\sigma^{2}}{r^{2}} + \\sigma_{\\theta}^{2} > 0$, we have $\\lambda_{t}^{+} < \\sigma^{2}$, with the limiting case $\\lambda_{t}^{+} \\to 0$ as $\\sigma_{\\theta}^{2} \\to 0$. Therefore, the largest posterior variance is along the line of sight, and the smallest is perpendicular to it. The updated uncertainty ellipse thus has its major axis oriented along the line of sight.\n\nHence, the most accurate statement is that the major axis is oriented along the line of sight.", "answer": "$$\\boxed{C}$$", "id": "1574759"}, {"introduction": "While powerful, the EKF is not infallible, and its reliance on linearization can introduce significant vulnerabilities. A skilled engineer must be able to recognize and diagnose potential failure modes to build robust estimation systems. This final challenge [@problem_id:1574809] presents a critical scenario where the filter becomes \"blind,\" completely ignoring new measurement data. By working backward from the observed failure, you will identify the precise conditions under which the Kalman gain vanishes, providing a crucial lesson on the practical limitations of the EKF.", "problem": "An autonomous robot navigates a 2D plane. Its state at time step $k$ is represented by the vector $\\mathbf{x}_k = [p_{x,k}, p_{y,k}]^T$, which contains its Cartesian coordinates. The robot's state is estimated using an Extended Kalman Filter (EKF).\n\nThe filter employs the following model equations:\n1.  **Prediction Model**: The state is predicted forward in time using a simple random walk model, where the process transition matrix is the identity matrix $I$.\n    -   Predicted state: $\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k-1|k-1}$\n    -   Predicted covariance: $P_{k|k-1} = P_{k-1|k-1} + Q$, where $Q$ is a positive definite process noise covariance matrix.\n\n2.  **Measurement Model**: At each time step, a sensor provides a scalar measurement $z_k$ which is related to the robot's state by the non-linear function $h(\\mathbf{x}_k)$:\n    -   $z_k = h(\\mathbf{x}_k) + v_k$, where $h(\\mathbf{x}_k) = (p_{x,k}^2 + p_{y,k}^2 - L^2)^2$.\n    -   The constant parameter is given as $L = 5.0$ m.\n    -   $v_k$ is a zero-mean Gaussian measurement noise with variance $R > 0$.\n\n3.  **Update Step**: The predicted state and covariance are updated using the measurement $z_k$:\n    -   Measurement Jacobian: $H_k = \\frac{\\partial h}{\\partial \\mathbf{x}}|_{\\hat{\\mathbf{x}}_{k|k-1}}$\n    -   Innovation Covariance: $S_k = H_k P_{k|k-1} H_k^T + R$\n    -   Kalman Gain: $K_k = P_{k|k-1} H_k^T S_k^{-1}$\n    -   Updated State: $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))$\n    -   Updated Covariance: $P_{k|k} = (I - K_k H_k) P_{k|k-1}$\n\nA critical failure mode, known as state estimator blindness, is discovered. In this mode, the filter completely ignores new measurement data, resulting in the updated state being identical to the predicted state, i.e., $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$. This failure is observed to occur regardless of the value of the measurement $z_k$, provided the innovation term, $z_k - h(\\hat{\\mathbf{x}}_{k|k-1})$, is non-zero.\n\nThis specific failure occurs when the predicted state estimate $\\hat{\\mathbf{x}}_{k|k-1}$ lies on the positive $p_x$-axis, meaning its form is $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$ for some $p_x > 0$.\n\nAssuming that the predicted error covariance matrix $P_{k|k-1}$ is always positive definite, determine the specific value of $p_x$ that induces this filter blindness. Express your answer in meters, rounded to two significant figures.", "solution": "The problem asks for the value of $p_x$ in a predicted state estimate $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$ that causes the Extended Kalman Filter (EKF) to exhibit \"blindness\". This condition is defined by the updated state estimate being equal to the predicted state estimate, $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$, even for a non-zero measurement innovation.\n\nLet's start with the state update equation of the EKF:\n$$\n\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))\n$$\nThe condition for filter blindness is $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1}$. Substituting this into the update equation gives:\n$$\n\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k|k-1} + K_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))\n$$\nThis simplifies to:\n$$\nK_k (z_k - h(\\hat{\\mathbf{x}}_{k|k-1})) = \\mathbf{0}\n$$\nThe problem states that this blindness occurs for any non-zero measurement innovation, which means the scalar term $(z_k - h(\\hat{\\mathbf{x}}_{k|k-1}))$ is non-zero. For the equation to hold true, the Kalman gain vector $K_k$ must be the zero vector, $K_k = \\mathbf{0}$.\n\nNow, let's analyze the equation for the Kalman gain:\n$$\nK_k = P_{k|k-1} H_k^T S_k^{-1}\n$$\nFor $K_k$ to be the zero vector, we must have $P_{k|k-1} H_k^T S_k^{-1} = \\mathbf{0}$.\nThe innovation covariance $S_k = H_k P_{k|k-1} H_k^T + R$. Since $P_{k|k-1}$ is positive definite and $R > 0$, $S_k$ is a positive scalar, so its inverse $S_k^{-1}$ exists and is non-zero.\nTherefore, the condition for a zero Kalman gain simplifies to:\n$$\nP_{k|k-1} H_k^T = \\mathbf{0}\n$$\nThe problem states that the predicted covariance matrix $P_{k|k-1}$ is positive definite. A positive definite matrix is, by definition, invertible. We can multiply both sides by $P_{k|k-1}^{-1}$ from the left:\n$$\nP_{k|k-1}^{-1} (P_{k|k-1} H_k^T) = P_{k|k-1}^{-1} \\mathbf{0}\n$$\n$$\n(P_{k|k-1}^{-1} P_{k|k-1}) H_k^T = \\mathbf{0}\n$$\n$$\nI H_k^T = \\mathbf{0} \\implies H_k^T = \\mathbf{0}\n$$\nThis means that the measurement Jacobian matrix $H_k$, evaluated at the predicted state $\\hat{\\mathbf{x}}_{k|k-1}$, must be a zero row vector.\n\nThe measurement function is given by $h(p_x, p_y) = (p_x^2 + p_y^2 - L^2)^2$. The state is $\\mathbf{x} = [p_x, p_y]^T$. The Jacobian $H$ is a $1 \\times 2$ matrix:\n$$\nH = \\begin{bmatrix} \\frac{\\partial h}{\\partial p_x} & \\frac{\\partial h}{\\partial p_y} \\end{bmatrix}\n$$\nLet's compute the partial derivatives using the chain rule:\n$$\n\\frac{\\partial h}{\\partial p_x} = 2(p_x^2 + p_y^2 - L^2)^{1} \\cdot \\frac{\\partial}{\\partial p_x}(p_x^2 + p_y^2 - L^2) = 2(p_x^2 + p_y^2 - L^2)(2p_x) = 4p_x(p_x^2 + p_y^2 - L^2)\n$$\n$$\n\\frac{\\partial h}{\\partial p_y} = 2(p_x^2 + p_y^2 - L^2)^{1} \\cdot \\frac{\\partial}{\\partial p_y}(p_x^2 + p_y^2 - L^2) = 2(p_x^2 + p_y^2 - L^2)(2p_y) = 4p_y(p_x^2 + p_y^2 - L^2)\n$$\nSo, the Jacobian matrix is:\n$$\nH = \\begin{bmatrix} 4p_x(p_x^2 + p_y^2 - L^2) & 4p_y(p_x^2 + p_y^2 - L^2) \\end{bmatrix}\n$$\nThe problem states that the filter blindness occurs when the predicted state is $\\hat{\\mathbf{x}}_{k|k-1} = [p_x, 0]^T$. We must evaluate the Jacobian $H_k$ at this specific state. Let's substitute the coordinates of $\\hat{\\mathbf{x}}_{k|k-1}$ into the expressions for the partial derivatives:\n$$\nH_k = \\begin{bmatrix} 4p_x(p_x^2 + 0^2 - L^2) & 4(0)(p_x^2 + 0^2 - L^2) \\end{bmatrix}\n$$\n$$\nH_k = \\begin{bmatrix} 4p_x(p_x^2 - L^2) & 0 \\end{bmatrix}\n$$\nTo achieve filter blindness, we need $H_k = \\mathbf{0} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}$. The second component is already zero. The first component must also be zero:\n$$\n4p_x(p_x^2 - L^2) = 0\n$$\nThe problem specifies that we are looking for a solution where $p_x > 0$. This implies that the term $4p_x$ is non-zero. Therefore, for the product to be zero, the other term must be zero:\n$$\np_x^2 - L^2 = 0\n$$\n$$\np_x^2 = L^2\n$$\nTaking the square root of both sides gives $p_x = \\pm L$. Since the problem specifies $p_x > 0$, we must choose the positive root:\n$$\np_x = L\n$$\nThe problem provides the value $L=5.0$ m. Thus, the critical value of $p_x$ is $5.0$ m.\n\nThe question asks for the answer to be rounded to two significant figures. The value $5.0$ has two significant figures.\n\nTherefore, the filter experiences blindness when the predicted state is on the positive x-axis at a distance from the origin exactly equal to the parameter $L$.", "answer": "$$\\boxed{5.0}$$", "id": "1574809"}]}