{"hands_on_practices": [{"introduction": "A cornerstone of chronobiology is the ability to detect and quantify rhythms in biological data, such as the daily oscillations of cytokines. This exercise provides a foundational understanding of cosinor analysis, a widely used method for this purpose, by guiding you through the manual derivation of its key parameters—mesor, amplitude, and acrophase—from summary statistics. Mastering this calculation demystifies the \"black box\" of statistical software and solidifies your grasp of the underlying linear regression principles used to model rhythmic phenomena [@problem_id:2841166].", "problem": "Circadian oscillations of inflammatory cytokines are a hallmark of the circadian regulation of immunity. Interleukin-6 (IL-6) concentrations in human plasma measured at regular intervals often display a near-sinusoidal pattern with a period close to $24$ h. Suppose plasma Interleukin-6 (IL-6) (pg/mL) is sampled hourly over $48$ h in one individual at times $t_k = k$ h for $k \\in \\{0,1,2,\\dots,47\\}$, yielding observations $\\{y_k\\}$. Consider the $24$ h single-component cosinor model\n$$\ny(t) \\;=\\; M \\;+\\; A \\cos\\!\\left(\\frac{2\\pi}{24}\\big(t - \\phi\\big)\\right),\n$$\nwhere $M$ is the mesor (mean level), $A$ is the amplitude, and $\\phi$ (in hours) is the acrophase (time-of-day of peak).\n\nYou are given the following sufficient summaries computed from the hourly data with angular frequency $\\omega = \\frac{2\\pi}{24}$:\n- $N = 48$,\n- $\\sum_{k=0}^{47} y_k \\;=\\; 105.6$,\n- $\\sum_{k=0}^{47} y_k^2 \\;=\\; 248.13$,\n- $\\sum_{k=0}^{47} y_k \\cos(\\omega t_k) \\;=\\; 4.34784$,\n- $\\sum_{k=0}^{47} y_k \\sin(\\omega t_k) \\;=\\; 16.22712$.\n\nBecause the sampling spans exactly two full $24$ h cycles with equal spacing, you may use the orthogonality identities\n$$\n\\sum_{k=0}^{47} \\cos(\\omega t_k) \\;=\\; 0,\\quad \\sum_{k=0}^{47} \\sin(\\omega t_k) \\;=\\; 0,\\quad \\sum_{k=0}^{47} \\cos(\\omega t_k)\\sin(\\omega t_k)\\;=\\; 0,\n$$\n$$\n\\sum_{k=0}^{47} \\cos^2(\\omega t_k) \\;=\\; 24,\\quad \\sum_{k=0}^{47} \\sin^2(\\omega t_k) \\;=\\; 24.\n$$\n\nFit the cosinor model by ordinary least squares, derive from first principles the point estimates $\\hat M$, $\\hat A$, and $\\hat\\phi$ and their $95\\%$ confidence intervals, treating the residuals as independent, homoscedastic, mean-zero Gaussian errors. Use the Student’s $t$ critical value $t_{0.975,\\,45} = 2.014$ for $95\\%$ intervals and the delta method where needed. Report $M$ and $A$ in pg/mL and $\\phi$ in hours. Round the three point estimates $\\hat M$, $\\hat A$, and $\\hat\\phi$ to four significant figures for the final answer, and round each reported confidence-interval endpoint to three significant figures. Express the final answer as a single row vector containing only the three point estimates, with no units included in that vector.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a standard problem in chronobiology and statistics, namely, the fitting of a cosinor model to time-series data using the method of ordinary least squares (OLS). All necessary data, including summary statistics and orthogonality conditions, are provided and internally consistent. The problem is therefore deemed valid and a solution will be furnished.\n\nThe non-linear cosinor model is given by\n$$\ny(t) = M + A \\cos\\!\\left(\\frac{2\\pi}{24}(t - \\phi)\\right)\n$$\nwhere the parameters are the mesor $M$, amplitude $A$, and acrophase $\\phi$. The angular frequency is $\\omega = \\frac{2\\pi}{24} \\, \\text{rad/h}$. To apply linear least squares, the model must be linearized with respect to its parameters. Using the trigonometric identity $\\cos(x-y) = \\cos(x)\\cos(y) + \\sin(x)\\sin(y)$, we can rewrite the model as:\n$$\ny(t) = M + A \\cos(\\omega\\phi)\\cos(\\omega t) + A \\sin(\\omega\\phi)\\sin(\\omega t)\n$$\nWe define two new parameters, $\\beta = A \\cos(\\omega\\phi)$ and $\\gamma = A \\sin(\\omega\\phi)$. The model becomes a multiple linear regression model, linear in the parameters $M$, $\\beta$, and $\\gamma$:\n$$\ny_k = M + \\beta \\cos(\\omega t_k) + \\gamma \\sin(\\omega t_k) + \\epsilon_k\n$$\nwhere $t_k = k$ for $k \\in \\{0, 1, \\dots, 47\\}$ and $\\epsilon_k$ are the error terms. The goal of OLS is to find the parameter estimates $(\\hat{M}, \\hat{\\beta}, \\hat{\\gamma})$ that minimize the sum of squared residuals, $S(M, \\beta, \\gamma) = \\sum_{k=0}^{47} (y_k - (M + \\beta\\cos(\\omega t_k) + \\gamma\\sin(\\omega t_k)))^2$.\n\nThe normal equations for this minimization problem are found by setting the partial derivatives of $S$ with respect to each parameter to zero:\n$$\n\\frac{\\partial S}{\\partial M} = -2 \\sum_{k=0}^{47} (y_k - \\hat{M} - \\hat{\\beta}\\cos(\\omega t_k) - \\hat{\\gamma}\\sin(\\omega t_k)) = 0\n$$\n$$\n\\frac{\\partial S}{\\partial \\beta} = -2 \\sum_{k=0}^{47} (y_k - \\hat{M} - \\hat{\\beta}\\cos(\\omega t_k) - \\hat{\\gamma}\\sin(\\omega t_k))\\cos(\\omega t_k) = 0\n$$\n$$\n\\frac{\\partial S}{\\partial \\gamma} = -2 \\sum_{k=0}^{47} (y_k - \\hat{M} - \\hat{\\beta}\\cos(\\omega t_k) - \\hat{\\gamma}\\sin(\\omega t_k))\\sin(\\omega t_k) = 0\n$$\nRearranging and applying the provided orthogonality conditions simplifies this system dramatically. The conditions are $\\sum \\cos(\\omega t_k) = 0$, $\\sum \\sin(\\omega t_k) = 0$, $\\sum \\cos(\\omega t_k)\\sin(\\omega t_k) = 0$. The sums of squares are $\\sum \\cos^2(\\omega t_k) = N/2 = 24$ and $\\sum \\sin^2(\\omega t_k) = N/2 = 24$, where $N=48$.\n\nFrom the first normal equation:\n$$\n\\sum y_k - N\\hat{M} - \\hat{\\beta}\\sum\\cos(\\omega t_k) - \\hat{\\gamma}\\sum\\sin(\\omega t_k) = 0 \\implies \\sum y_k - N\\hat{M} = 0\n$$\n$$\n\\hat{M} = \\frac{\\sum y_k}{N} = \\frac{105.6}{48} = 2.2\n$$\nFrom the second normal equation:\n$$\n\\sum y_k \\cos(\\omega t_k) - \\hat{M}\\sum\\cos(\\omega t_k) - \\hat{\\beta}\\sum\\cos^2(\\omega t_k) - \\hat{\\gamma}\\sum\\cos(\\omega t_k)\\sin(\\omega t_k) = 0 \\implies \\sum y_k \\cos(\\omega t_k) - \\hat{\\beta}(N/2) = 0\n$$\n$$\n\\hat{\\beta} = \\frac{\\sum y_k \\cos(\\omega t_k)}{N/2} = \\frac{4.34784}{24} = 0.18116\n$$\nFrom the third normal equation:\n$$\n\\sum y_k \\sin(\\omega t_k) - \\hat{M}\\sum\\sin(\\omega t_k) - \\hat{\\beta}\\sum\\cos(\\omega t_k)\\sin(\\omega t_k) - \\hat{\\gamma}\\sum\\sin^2(\\omega t_k) = 0 \\implies \\sum y_k \\sin(\\omega t_k) - \\hat{\\gamma}(N/2) = 0\n$$\n$$\n\\hat{\\gamma} = \\frac{\\sum y_k \\sin(\\omega t_k)}{N/2} = \\frac{16.22712}{24} = 0.67613\n$$\nNow we transform these estimates back to the original parameters $A$ and $\\phi$ using the definitions $\\beta = A \\cos(\\omega\\phi)$ and $\\gamma = A \\sin(\\omega\\phi)$:\n$$\n\\hat{A} = \\sqrt{\\hat{\\beta}^2 + \\hat{\\gamma}^2} = \\sqrt{(0.18116)^2 + (0.67613)^2} = \\sqrt{0.032819 + 0.457151} = \\sqrt{0.48997} \\approx 0.699979 \\, \\text{pg/mL}\n$$\n$$\n\\omega\\hat{\\phi} = \\operatorname{atan2}(\\hat{\\gamma}, \\hat{\\beta}) = \\operatorname{atan2}(0.67613, 0.18116) \\approx 1.3090 \\, \\text{rad}\n$$\nThe function $\\operatorname{atan2}(y, x)$ is used to determine the correct quadrant. Since both $\\hat{\\beta}$ and $\\hat{\\gamma}$ are positive, the angle is in the first quadrant.\n$$\n\\hat{\\phi} = \\frac{\\omega\\hat{\\phi}}{\\omega} = \\frac{1.3090}{2\\pi/24} = \\frac{1.3090 \\times 12}{\\pi} \\approx 5.000 \\, \\text{h}\n$$\nThe point estimates are $\\hat{M} = 2.2 \\, \\text{pg/mL}$, $\\hat{A} \\approx 0.7000 \\, \\text{pg/mL}$, and $\\hat{\\phi} \\approx 5.000 \\, \\text{h}$.\n\nFor confidence intervals, we must first estimate the variance of the residuals, $\\sigma^2$. The residual sum of squares (RSS) is given by $RSS = \\sum_{k=0}^{47} (y_k - \\hat{y}_k)^2$. For an orthogonal design, this simplifies:\n$$\nRSS = \\sum y_k^2 - N\\hat{M}^2 - (N/2)\\hat{\\beta}^2 - (N/2)\\hat{\\gamma}^2 = \\sum y_k^2 - N\\hat{M}^2 - (N/2)(\\hat{\\beta}^2 + \\hat{\\gamma}^2)\n$$\nUsing the given values:\n$$\nRSS = 248.13 - 48(2.2)^2 - 24((0.18116)^2 + (0.67613)^2) = 248.13 - 232.32 - 24(0.48997)\n$$\n$$\nRSS = 15.81 - 11.75928 = 4.05072\n$$\nThe unbiased estimator for the error variance $\\sigma^2$ is $\\hat{\\sigma}^2 = \\frac{RSS}{N-p}$, where $p=3$ is the number of parameters in the linear model. The degrees of freedom are $df = N-p = 48 - 3 = 45$.\n$$\n\\hat{\\sigma}^2 = \\frac{4.05072}{45} \\approx 0.090016\n$$\nThe variances of the linear parameter estimates are $\\text{Var}(\\hat{M}) = \\hat{\\sigma}^2/N$, $\\text{Var}(\\hat{\\beta}) = \\hat{\\sigma}^2/(N/2)$, and $\\text{Var}(\\hat{\\gamma}) = \\hat{\\sigma}^2/(N/2)$.\n$$\n\\text{Var}(\\hat{M}) = \\frac{0.090016}{48} \\approx 0.0018753 \\implies SE(\\hat{M}) = \\sqrt{0.0018753} \\approx 0.043305\n$$\n$$\n\\text{Var}(\\hat{\\beta}) = \\text{Var}(\\hat{\\gamma}) = \\frac{0.090016}{24} \\approx 0.0037507 \\implies SE(\\hat{\\beta}) = SE(\\hat{\\gamma}) = \\sqrt{0.0037507} \\approx 0.061243\n$$\nThe $95\\%$ confidence interval for a parameter $\\theta$ is $\\hat{\\theta} \\pm t_{1-\\alpha/2, df} \\times SE(\\hat{\\theta})$. Here, $t_{0.975, 45} = 2.014$.\n\nFor the mesor $M$:\n$$\nCI_M = \\hat{M} \\pm t_{0.975, 45} \\times SE(\\hat{M}) = 2.2 \\pm 2.014 \\times 0.043305 = 2.2 \\pm 0.08722\n$$\n$CI_M = [2.11278, 2.28722]$. Rounded to three significant figures, this is $[2.11, 2.29]$ pg/mL.\n\nFor the amplitude $A$ and acrophase $\\phi$, we use the delta method. The estimates $\\hat{\\beta}$ and $\\hat{\\gamma}$ are uncorrelated due to orthogonality.\nFor $A = \\sqrt{\\beta^2+\\gamma^2}$:\n$$\n\\text{Var}(\\hat{A}) \\approx \\left(\\frac{\\partial A}{\\partial \\beta}\\right)^2 \\text{Var}(\\hat{\\beta}) + \\left(\\frac{\\partial A}{\\partial \\gamma}\\right)^2 \\text{Var}(\\hat{\\gamma}) = \\left(\\frac{\\hat{\\beta}}{\\hat{A}}\\right)^2 \\text{Var}(\\hat{\\beta}) + \\left(\\frac{\\hat{\\gamma}}{\\hat{A}}\\right)^2 \\text{Var}(\\hat{\\gamma})\n$$\nSince $\\text{Var}(\\hat{\\beta}) = \\text{Var}(\\hat{\\gamma}) = 2\\hat{\\sigma}^2/N$,\n$$\n\\text{Var}(\\hat{A}) \\approx \\frac{\\hat{\\beta}^2+\\hat{\\gamma}^2}{\\hat{A}^2} \\left(\\frac{2\\hat{\\sigma}^2}{N}\\right) = \\frac{\\hat{A}^2}{\\hat{A}^2} \\left(\\frac{2\\hat{\\sigma}^2}{N}\\right) = \\frac{2\\hat{\\sigma}^2}{N}\n$$\nSo, $SE(\\hat{A}) = \\sqrt{2\\hat{\\sigma}^2/N} = SE(\\hat{\\beta}) \\approx 0.061243$.\n$$\nCI_A = \\hat{A} \\pm t_{0.975, 45} \\times SE(\\hat{A}) = 0.7000 \\pm 2.014 \\times 0.061243 = 0.7000 \\pm 0.12332\n$$\n$CI_A = [0.57668, 0.82332]$. Rounded to three significant figures, this is $[0.577, 0.823]$ pg/mL.\n\nFor $\\phi = \\frac{1}{\\omega}\\arctan(\\gamma/\\beta)$:\n$$\n\\text{Var}(\\omega\\hat{\\phi}) \\approx \\left(\\frac{\\partial (\\omega\\phi)}{\\partial \\beta}\\right)^2 \\text{Var}(\\hat{\\beta}) + \\left(\\frac{\\partial (\\omega\\phi)}{\\partial \\gamma}\\right)^2 \\text{Var}(\\hat{\\gamma}) = \\left(\\frac{-\\hat{\\gamma}}{\\hat{A}^2}\\right)^2 \\text{Var}(\\hat{\\beta}) + \\left(\\frac{\\hat{\\beta}}{\\hat{A}^2}\\right)^2 \\text{Var}(\\hat{\\gamma})\n$$\n$$\n\\text{Var}(\\omega\\hat{\\phi}) \\approx \\frac{\\hat{\\gamma}^2 + \\hat{\\beta}^2}{\\hat{A}^4} \\left(\\frac{2\\hat{\\sigma}^2}{N}\\right) = \\frac{\\hat{A}^2}{\\hat{A}^4} \\left(\\frac{2\\hat{\\sigma}^2}{N}\\right) = \\frac{1}{\\hat{A}^2}\\frac{2\\hat{\\sigma}^2}{N}\n$$\n$SE(\\omega\\hat{\\phi}) = \\frac{1}{\\hat{A}}SE(\\hat{\\beta}) = \\frac{0.061243}{0.699979} \\approx 0.087492$ radians.\nThen, $SE(\\hat{\\phi}) = \\frac{1}{\\omega} SE(\\omega\\hat{\\phi}) = \\frac{24}{2\\pi} \\times 0.087492 \\approx 0.33423$ hours.\n$$\nCI_\\phi = \\hat{\\phi} \\pm t_{0.975, 45} \\times SE(\\hat{\\phi}) = 5.000 \\pm 2.014 \\times 0.33423 = 5.000 \\pm 0.67315\n$$\n$CI_\\phi = [4.32685, 5.67315]$. Rounded to three significant figures, this is $[4.33, 5.67]$ hours.\n\nThe final point estimates, rounded to four significant figures, are $\\hat{M}=2.200$, $\\hat{A}=0.7000$, and $\\hat{\\phi}=5.000$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.200 & 0.7000 & 5.000\n\\end{pmatrix}\n}\n$$", "id": "2841166"}, {"introduction": "While manual calculation is instructive, modern research relies on computational tools to analyze time-series data efficiently and robustly. This practice moves from theory to application, challenging you to implement a complete cosinor analysis pipeline programmatically [@problem_id:2841150]. By working with raw data on circulating T cell counts, you will not only estimate the rhythm's parameters but also perform a formal statistical test for significance, a critical step in distinguishing a true biological rhythm from random fluctuation.", "problem": "You are given time-stamped counts of circulating T cells measured by flow cytometry across a single $24$-hour day. Assume that circadian variation in circulating T cells can be modeled, if present, as a single-component cosine oscillation with a fixed $24$-hour period, superimposed on a constant mean (mesor). Your task is to implement a program that, for each provided dataset, estimates the acrophase (the time-of-peak within $[0,24)$ hours) under a cosinor model and performs a formal statistical test for significant rhythmicity by comparing the cosinor model to a constant (no-rhythm) model.\n\nFundamental base and modeling assumptions:\n- The biological hypothesis is that certain immune variables exhibit circadian rhythms with approximately $24$-hour periodicity, a well-tested and widely observed phenomenon in mammalian physiology.\n- Adopt the cosinor regression model with fixed period $T = 24$ hours. Use angular frequency $\\omega = 2\\pi/T$ in radians per hour.\n- The cosinor model is\n$$\ny_i = M + A \\cos(\\omega t_i) + B \\sin(\\omega t_i) + \\varepsilon_i,\n$$\nwhere $y_i$ are measured T-cell counts at times $t_i$ (in hours), $M$ is the mesor (mean level), $A$ and $B$ are regression coefficients, and $\\varepsilon_i$ are independent, mean-zero noise terms.\n- The null model (no rhythmicity) is\n$$\ny_i = M_0 + \\varepsilon_i.\n$$\n- Estimate parameters by ordinary least squares (OLS).\n- Define the amplitude as $C = \\sqrt{A^2 + B^2}$ and define the phase parameter $\\phi$ by identifying $M + A\\cos(\\omega t) + B\\sin(\\omega t) \\equiv M + C \\cos(\\omega t + \\phi)$, which yields $\\phi = \\operatorname{atan2}(-B, A)$ (in radians). The acrophase (time-of-peak) is\n$$\nt_{\\text{peak}} = \\frac{(-\\phi) \\bmod 2\\pi}{\\omega},\n$$\nexpressed in hours within $[0,24)$.\n\nSignificance test:\n- Use an Analysis of Variance (ANOVA) $F$-test to compare the full cosinor model to the null model. Let $\\text{RSS}_{\\text{full}}$ be the residual sum of squares of the cosinor model and $\\text{RSS}_{\\text{null}}$ be that of the null model. With $N$ observations, the full model uses $p_{\\text{full}} = 3$ parameters and the null model uses $p_{\\text{null}} = 1$ parameter. The test statistic is\n$$\nF = \\frac{\\left(\\text{RSS}_{\\text{null}} - \\text{RSS}_{\\text{full}}\\right)/(p_{\\text{full}} - p_{\\text{null}})}{\\text{RSS}_{\\text{full}}/(N - p_{\\text{full}})} = \\frac{\\left(\\text{RSS}_{\\text{null}} - \\text{RSS}_{\\text{full}}\\right)/2}{\\text{RSS}_{\\text{full}}/(N - 3)}.\n$$\n- Compute the $p$-value from the $F$ distribution with degrees of freedom $\\text{df}_1 = 2$ and $\\text{df}_2 = N - 3$ using a right-tail probability (survival function).\n- Declare significant rhythmicity if $p < 0.05$.\n\nAngle unit requirement:\n- All trigonometric computations must use angles in radians. The acrophase must be expressed as a time in hours.\n\nRequired outputs:\n- For each dataset, output a two-element list $[t_{\\text{peak}}, \\text{is\\_rhythmic}]$ where $t_{\\text{peak}}$ is the acrophase in hours rounded to $2$ decimals, and $\\text{is\\_rhythmic}$ is a boolean computed using the criterion $p < 0.05$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each $resultk$ must be the two-element list described above.\n\nTest suite (times in hours, counts in cells per microliter). Use these exact arrays:\n\n- Case $1$ (strong circadian signal, approximately peak near $4$ hours):\n  - Times: $[0,3,6,9,12,15,18,21]$\n  - Counts (cells$/\\mu$L): $[560,616,604,531,440,384,396,469]$\n\n- Case $2$ (approximately arrhythmic, near-constant counts):\n  - Times: $[0,4,8,12,16,20]$\n  - Counts (cells$/\\mu$L): $[498,505,492,501,499,504]$\n\n- Case $3$ (irregular sampling, peak near boundary at approximately $23$ hours):\n  - Times: $[1,5,7,10,13,17,19,22]$\n  - Counts (cells$/\\mu$L): $[369,300,260,222,231,300,340,377]$\n\nNumerical and unit requirements:\n- Express $t_{\\text{peak}}$ in hours, rounded to $2$ decimals.\n- Use radians internally for trigonometric functions.\n- The boolean rhythmicity flag must be computed using $p < 0.05$.\n\nYour program must implement the above logic and produce the final single-line output in the exact format specified.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- **Model**: Circadian variation is modeled as a single-component cosine oscillation with a fixed period $T=24$ hours, superimposed on a mean level $M$.\n- **Full Cosinor Model**: $y_i = M + A \\cos(\\omega t_i) + B \\sin(\\omega t_i) + \\varepsilon_i$, where $\\omega = 2\\pi/T$.\n- **Null Model (No Rhythm)**: $y_i = M_0 + \\varepsilon_i$.\n- **Estimation Method**: Ordinary least squares (OLS).\n- **Parameter Definitions**:\n    - Amplitude: $C = \\sqrt{A^2 + B^2}$.\n    - Phase parameter: $\\phi = \\operatorname{atan2}(-B, A)$.\n    - Acrophase (time-of-peak): $t_{\\text{peak}} = \\frac{(-\\phi) \\bmod 2\\pi}{\\omega}$, in hours within $[0,24)$.\n- **Significance Test**: Analysis of Variance ($F$-test) comparing the full and null models.\n    - Test Statistic: $F = \\frac{\\left(\\text{RSS}_{\\text{null}} - \\text{RSS}_{\\text{full}}\\right)/2}{\\text{RSS}_{\\text{full}}/(N - 3)}$.\n    - Number of parameters: $p_{\\text{full}} = 3$, $p_{\\text{null}} = 1$. Number of observations: $N$.\n    - Degrees of freedom: $\\text{df}_1 = 2$, $\\text{df}_2 = N - 3$.\n    - Significance Criterion: $p < 0.05$, where the $p$-value is from the right-tail of the $F$-distribution.\n- **Data (Test Suite)**:\n    - Case $1$: Times = $[0,3,6,9,12,15,18,21]$, Counts = $[560,616,604,531,440,384,396,469]$.\n    - Case $2$: Times = $[0,4,8,12,16,20]$, Counts = $[498,505,492,501,499,504]$.\n    - Case $3$: Times = $[1,5,7,10,13,17,19,22]$, Counts = $[369,300,260,222,231,300,340,377]$.\n- **Output Format**: For each dataset, a two-element list $[t_{\\text{peak}}, \\text{is\\_rhythmic}]$, with $t_{\\text{peak}}$ rounded to $2$ decimals and $\\text{is\\_rhythmic}$ as a boolean. The final output is a single line: a comma-separated list of these results, enclosed in brackets.\n\nStep 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is scientifically grounded. Cosinor analysis is a standard, widely used technique in chronobiology for analyzing rhythmic data. The hypothesis of circadian regulation of immune cell counts is a well-established area of research in immunology.\n- **Well-Posedness**: The problem is well-posed. It provides clear mathematical models, a defined statistical procedure for parameter estimation (OLS) and hypothesis testing ($F$-test), and complete datasets. The objective is unambiguous.\n- **Objectivity**: The problem is stated in precise, objective, and quantitative terms.\n- **Completeness and Consistency**: The problem is self-contained. All necessary equations, parameters, data, and criteria are provided without contradiction.\n- **Other Criteria**: The problem is formalizable, relevant to its stated field, realistic, and presents a non-trivial but solvable challenge. It is not ill-posed, pseudo-profound, or unverifiable.\n\nStep 3: Verdict and Action\nThe problem is valid. A rigorous solution must be developed.\n\nThe solution will be constructed based on the principles of linear regression and statistical model comparison.\n\n**Model Formulation in Matrix Form**\n\nThe cosinor model is a linear model. For a set of $N$ observations $(t_i, y_i)$, we can express the full model in matrix form:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y}$ is the $N \\times 1$ vector of T-cell counts, $\\mathbf{X}$ is the $N \\times 3$ design matrix, $\\boldsymbol{\\beta}$ is the $3 \\times 1$ parameter vector, and $\\boldsymbol{\\varepsilon}$ is the $N \\times 1$ vector of errors.\n\nThe components are:\n$$\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix}\n1 & \\cos(\\omega t_1) & \\sin(\\omega t_1) \\\\\n1 & \\cos(\\omega t_2) & \\sin(\\omega t_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & \\cos(\\omega t_N) & \\sin(\\omega t_N)\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} M \\\\ A \\\\ B \\end{pmatrix}\n$$\nThe angular frequency is $\\omega = 2\\pi/T$ with the period $T=24$ hours.\n\nThe null model, representing the absence of a rhythm, is $y_i = M_0 + \\varepsilon_i$. Its OLS estimate for the mesor $M_0$ is the sample mean, $\\hat{M}_0 = \\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$.\n\n**Parameter Estimation via Ordinary Least Squares (OLS)**\n\nThe OLS estimate $\\hat{\\boldsymbol{\\beta}}$ of the parameter vector $\\boldsymbol{\\beta}$ minimizes the residual sum of squares, $\\text{RSS} = ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$. The solution is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThis provides the estimates $[\\hat{M}, \\hat{A}, \\hat{B}]^T$. Computationally, this is solved using robust linear algebra routines, such as those implementing QR decomposition.\n\n**Acrophase Calculation**\n\nThe acrophase, $t_{\\text{peak}}$, is the time of the peak of the fitted cosine curve. The rhythmic component is $f(t) = \\hat{A} \\cos(\\omega t) + \\hat{B} \\sin(\\omega t)$. We rewrite this in the form $C \\cos(\\omega t + \\phi)$.\nUsing the cosine addition formula, $C \\cos(\\omega t + \\phi) = C(\\cos(\\omega t)\\cos(\\phi) - \\sin(\\omega t)\\sin(\\phi))$.\nBy comparing coefficients with $f(t)$, we obtain:\n$$\n\\hat{A} = C \\cos(\\phi) \\quad \\text{and} \\quad \\hat{B} = -C \\sin(\\phi)\n$$\nFrom these, the phase parameter $\\phi$ is determined by $\\tan(\\phi) = \\sin(\\phi)/\\cos(\\phi) = (-\\hat{B}/C)/(\\hat{A}/C) = -\\hat{B}/\\hat{A}$. The `atan2` function correctly resolves the quadrant ambiguity:\n$$\n\\phi = \\operatorname{atan2}(-\\hat{B}, \\hat{A})\n$$\nThe maximum of the cosine function occurs when its argument is an integer multiple of $2\\pi$. We seek the first peak time $t_{\\text{peak}} \\in [0, T)$. We solve for $\\omega t_{\\text{peak}} + \\phi = 0 \\pmod{2\\pi}$, which means $\\omega t_{\\text{peak}} = (-\\phi) \\pmod{2\\pi}$. This gives the formula for the acrophase:\n$$\nt_{\\text{peak}} = \\frac{(-\\phi) \\pmod{2\\pi}}{\\omega} = \\frac{T}{2\\pi} \\left( (-\\phi) \\pmod{2\\pi} \\right)\n$$\nThe result is expressed in hours, rounded to $2$ decimal places.\n\n**Hypothesis Testing for Rhythmicity**\n\nTo test for significant rhythmicity, we compare the full cosinor model ($H_1$) against the null model of a constant mean ($H_0$). The null hypothesis states that the rhythm amplitude is zero ($A=B=0$).\n\nThis comparison is performed using an $F$-test. We calculate the residual sum of squares (RSS) for both models:\n- $\\text{RSS}_{\\text{null}} = \\sum_{i=1}^N (y_i - \\bar{y})^2$\n- $\\text{RSS}_{\\text{full}} = \\sum_{i=1}^N (y_i - (\\hat{M} + \\hat{A}\\cos(\\omega t_i) + \\hat{B}\\sin(\\omega t_i)))^2$\n\nThe $F$-statistic is the ratio of the explained variance per additional parameter to the unexplained variance of the full model:\n$$\nF = \\frac{(\\text{RSS}_{\\text{null}} - \\text{RSS}_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{null}})}{\\text{RSS}_{\\text{full}} / (N - p_{\\text{full}})}\n$$\nHere, $p_{\\text{full}}=3$ (for $M, A, B$), $p_{\\text{null}}=1$ (for $M_0$), and $N$ is the number of data points. The degrees of freedom for the $F$-distribution are $\\text{df}_1 = p_{\\text{full}} - p_{\\text{null}} = 2$ and $\\text{df}_2 = N - p_{\\text{full}} = N-3$.\n\nA large $F$ value indicates that the cosinor model provides a significantly better fit than a simple constant mean, suggesting the presence of a rhythm. We compute the $p$-value, which is the probability of observing an $F$-statistic as large or larger than the one calculated, assuming the null hypothesis is true. This is given by the survival function (right-tail probability) of the $F_{2, N-3}$ distribution.\n\nThe rhythm is declared statistically significant if the $p$-value is less than the conventional threshold of $\\alpha=0.05$. The result $\\text{is\\_rhythmic}$ is a boolean value based on this test ($p < 0.05$).\n\nImplementation will now proceed by applying this validated methodology to the provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef cosinor_analysis(times, counts):\n    \"\"\"\n    Performs cosinor analysis to estimate acrophase and test for rhythmicity.\n\n    Args:\n        times (np.ndarray): Array of measurement times in hours.\n        counts (np.ndarray): Array of corresponding cell counts.\n\n    Returns:\n        list: A two-element list containing the acrophase [hours] rounded\n              to 2 decimals and a boolean indicating significant rhythmicity.\n    \"\"\"\n    # 1. Define constants and variables\n    N = len(times)\n    T = 24.0  # Period in hours\n    omega = 2 * np.pi / T  # Angular frequency in radians per hour\n\n    # 2. Set up and solve the full cosinor model using Ordinary Least Squares (OLS)\n    # The model is y = M + A*cos(omega*t) + B*sin(omega*t)\n    # This is a linear regression problem: y = X * beta\n    X = np.c_[np.ones(N), np.cos(omega * times), np.sin(omega * times)]\n    y = counts\n\n    # np.linalg.lstsq solves for beta in y = X*beta and returns RSS\n    # solution is beta = [M, A, B]\n    # residuals[0] is the residual sum of squares (RSS_full)\n    beta, residuals, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    M_hat, A_hat, B_hat = beta\n    \n    # Check if a valid solution was found. residuals is empty if N <= 3.\n    if N <= 3:\n        rss_full = np.sum((y - X @ beta)**2)\n    else:\n        rss_full = residuals[0]\n\n    # 3. Calculate parameters for the null model (no rhythm)\n    # The model is y = M0. The OLS estimate for M0 is the mean of y.\n    y_mean = np.mean(y)\n    rss_null = np.sum((y - y_mean)**2)\n\n    # 4. Perform the F-test for rhythmicity\n    p_full = 3  # Number of parameters in the full model (M, A, B)\n    p_null = 1  # Number of parameters in the null model (M0)\n    \n    # Degrees of freedom for the F-test\n    df1 = p_full - p_null\n    df2 = N - p_full\n    \n    is_rhythmic = False\n    p_value = 1.0 # Default to non-rhythmic\n    \n    # The F-test is only valid if RSS_full > 0 and the denominator df > 0\n    if df2 > 0 and rss_full > 1e-9: # Avoid division by zero\n        F_statistic = ((rss_null - rss_full) / df1) / (rss_full / df2)\n        \n        # Calculate p-value from the survival function (right-tail prob) of the F-distribution\n        p_value = f_dist.sf(F_statistic, df1, df2)\n        \n        # Check for significance\n        if p_value < 0.05:\n            is_rhythmic = True\n\n    # 5. Calculate the acrophase (time of peak)\n    # phi = atan2(-B, A)\n    # t_peak = ((-phi) mod 2pi) / omega\n    phi = np.arctan2(-B_hat, A_hat)\n    \n    # The modulo operator % in Python correctly handles negative numbers for this purpose.\n    # It ensures the result is in [0, 2*pi) for a positive divisor.\n    t_peak_rad = (-phi) % (2 * np.pi)\n    t_peak = t_peak_rad / omega\n    \n    t_peak_rounded = round(t_peak, 2)\n\n    return [t_peak_rounded, is_rhythmic]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the provided test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (strong circadian signal)\n        {'times': np.array([0, 3, 6, 9, 12, 15, 18, 21]),\n         'counts': np.array([560, 616, 604, 531, 440, 384, 396, 469])},\n        \n        # Case 2 (approximately arrhythmic)\n        {'times': np.array([0, 4, 8, 12, 16, 20]),\n         'counts': np.array([498, 505, 492, 501, 499, 504])},\n        \n        # Case 3 (irregular sampling, peak near boundary)\n        {'times': np.array([1, 5, 7, 10, 13, 17, 19, 22]),\n         'counts': np.array([369, 300, 260, 222, 231, 300, 340, 377])}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = cosinor_analysis(case['times'], case['counts'])\n        results.append(result)\n\n    # Format the final output string as specified\n    # e.g., [[4.0, True],[12.23, False]]\n    results_str =','.join(map(str, results))\n    print(f\"[{results_str}]\")\n\nsolve()\n\n```", "id": "2841150"}, {"introduction": "Beyond identifying individual rhythms, a central goal in systems immunology is to map the regulatory networks that connect the circadian clock and immune pathways. For instance, does the clock gene `Per2` drive the expression of the inflammatory cytokine `Il1b`, or does inflammation feed back to alter the clock? This advanced practice introduces Granger causality, a powerful statistical method for inferring directional influence between two time series [@problem_id:2841109]. By simulating and analyzing a vector autoregressive model, you will gain hands-on experience with a sophisticated technique used to probe the causal architecture of complex biological systems.", "problem": "You are given a conceptual model of how circadian regulation may shape inflammatory responses through interactions between Period Circadian Regulator $2$ ($\\mathrm{Per2}$) and Interleukin $1$ beta ($\\mathrm{Il1b}$). After an inflammatory insult, both $\\mathrm{Per2}$ and $\\mathrm{Il1b}$ can exhibit time-varying expression dynamics. Assume that, over the time window analyzed, these dynamics can be locally approximated by a jointly stationary, linear, time-invariant, Gaussian, bivariate autoregressive process of fixed order $k$ (Vector Autoregression of order $k$). According to the definition of Granger causality (GC), a variable $X$ Granger-causes a variable $Y$ if the predictive accuracy for $Y$ using past values of $Y$ is improved by also including past values of $X$, under the linear Gaussian modeling assumptions. This aligns with a mechanistic interpretation in immunology: directional influence can reflect regulatory gating by the circadian clock over inflammatory pathways or feedback from inflammation onto clock gene expression. Use this base (stationary linear Gaussian autoregression and the definition of Granger causality as improved predictability from past values) to implement an algorithm that infers directionality between $\\mathrm{Per2}$ and $\\mathrm{Il1b}$.\n\nYour task is to:\n- Programmatically generate synthetic bivariate time series $(\\mathrm{Per2}_t, \\mathrm{Il1b}_t)$ under the stationary linear Gaussian autoregressive assumptions for the parameter sets given below.\n- For each synthetic data set, perform bidirectional Granger causality testing between $\\mathrm{Per2}$ and $\\mathrm{Il1b}$ at a fixed lag order $k$, using a nested linear modeling approach under Gaussian noise assumptions and standard linear regression theory to quantify whether including past values of the putative cause significantly improves prediction of the effect.\n- Use a two-sided decision rule with significance level $\\alpha = 0.01$ to determine whether $\\mathrm{Per2}$ Granger-causes $\\mathrm{Il1b}$, whether $\\mathrm{Il1b}$ Granger-causes $\\mathrm{Per2}$, both, or neither.\n- Report one integer per test case encoding the inferred directionality: $1$ if only $\\mathrm{Per2} \\to \\mathrm{Il1b}$ is supported, $-1$ if only $\\mathrm{Il1b} \\to \\mathrm{Per2}$ is supported, $2$ if both directions are supported, and $0$ if neither direction is supported.\n\nThe autoregressive generator you must implement is as follows. Let $k \\in \\{2\\}$ for all cases below. For each time step $t$, generate\n- $\\mathrm{Per2}_t$ from an intercept $c_1$, the past $k$ lags of $\\mathrm{Per2}$ with coefficients $\\{a_{11,\\ell}\\}_{\\ell=1}^k$, the past $k$ lags of $\\mathrm{Il1b}$ with coefficients $\\{a_{12,\\ell}\\}_{\\ell=1}^k$, and an independent Gaussian innovation with standard deviation $\\sigma_1$,\n- $\\mathrm{Il1b}_t$ from an intercept $c_2$, the past $k$ lags of $\\mathrm{Il1b}$ with coefficients $\\{a_{22,\\ell}\\}_{\\ell=1}^k$, the past $k$ lags of $\\mathrm{Per2}$ with coefficients $\\{a_{21,\\ell}\\}_{\\ell=1}^k$, and an independent Gaussian innovation with standard deviation $\\sigma_2$,\nensuring that simulations begin from initial values of $0$ and include a sufficiently large burn-in to reach stationarity before collecting the requested sample length. Time is in hours and expression is in normalized arbitrary units; you do not need to output any units because the final outputs are integers.\n\nTest suite (each tuple specifies $(T, k, c_1, c_2, [a_{11,1}, a_{11,2}], [a_{22,1}, a_{22,2}], [a_{21,1}, a_{21,2}], [a_{12,1}, a_{12,2}], \\sigma_1, \\sigma_2)$, where $[a_{21,\\ell}]$ are coefficients for $\\mathrm{Per2}$ affecting $\\mathrm{Il1b}$ and $[a_{12,\\ell}]$ are coefficients for $\\mathrm{Il1b}$ affecting $\\mathrm{Per2}$):\n- Case A (happy path, $\\mathrm{Per2} \\to \\mathrm{Il1b}$): ($T=240, k=2, c_1=0, c_2=0, [$a_{11,1}, a_{11,2}$]=[1.5, -0.75], [$a_{22,1}, a_{22,2}$]=[1.2, -0.6], [$a_{21,1}, a_{21,2}$]=[0.5, 0.0], [$a_{12,1}, a_{12,2}$]=[0.0, 0.0], $\\sigma_1=0.2, \\sigma_2=0.2$).\n- Case B (null, no directionality): ($T=240, k=2, c_1=0, c_2=0, [$a_{11,1}, a_{11,2}$]=[1.5, -0.75], [$a_{22,1}, a_{22,2}$]=[1.2, -0.6], [$a_{21,1}, a_{21,2}$]=[0.0, 0.0], [$a_{12,1}, a_{12,2}$]=[0.0, 0.0], $\\sigma_1=0.3, \\sigma_2=0.3$).\n- Case C (reverse, $\\mathrm{Il1b} \\to \\mathrm{Per2}$): ($T=240, k=2, c_1=0, c_2=0, [$a_{11,1}, a_{11,2}$]=[1.5, -0.75], [$a_{22,1}, a_{22,2}$]=[1.2, -0.6], [$a_{21,1}, a_{21,2}$]=[0.0, 0.0], [$a_{12,1}, a_{12,2}$]=[0.35, 0.0], $\\sigma_1=0.2, \\sigma_2=0.2$).\n- Case D (boundary, short time series and weak coupling): ($T=40, k=2, c_1=0, c_2=0, [$a_{11,1}, a_{11,2}$]=[1.5, -0.75], [$a_{22,1}, a_{22,2}$]=[1.2, -0.6], [$a_{21,1}, a_{21,2}$]=[0.0, 0.0], [$a_{12,1}, a_{12,2}$]=[0.15, 0.0], $\\sigma_1=0.6, \\sigma_2=0.6$).\n\nAlgorithmic requirements:\n- Fit linear models by ordinary least squares, using only past $k$ lags, and treat each direction with a restricted model (without the other variable’s lags) and an unrestricted model (with both variables’ lags). Use the Gaussian linear model assumption to form a valid nested-model test to decide GC.\n- Use a fixed significance level $\\alpha = 0.01$ for both directions independently in each case.\n- For reproducibility, simulations must use a fixed random seed and include a burn-in longer than $200$ time steps before collecting the requested sample of length $T$.\n\nAngle units are not applicable. No physical unit conversion is required; all final outputs are integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[$ $1$,$0$,$-1$,$2$ $]$), corresponding to Cases A–D in order. Each element must be one of $-1$, $0$, $1$, or $2$ as defined above.", "solution": "The problem requires the development and implementation of an algorithm to infer directional Granger causality between two time series, representing the expression of Period Circadian Regulator $2$ ($\\mathrm{Per2}$) and Interleukin $1$ beta ($\\mathrm{Il1b}$). The analysis is based on a bivariate, stationary, linear, time-invariant, Gaussian vector autoregressive model of order $k$, denoted VAR($k$).\n\n**Problem Validation**\n\nFirst, a validation of the problem statement is conducted.\n\n**Step 1: Extracted Givens**\n- **Model**: Bivariate VAR($k$) process for $\\mathbf{X}_t = [\\mathrm{Per2}_t, \\mathrm{Il1b}_t]^T$.\n- **Equation for $\\mathrm{Per2}_t$**: $\\mathrm{Per2}_t = c_1 + \\sum_{\\ell=1}^k a_{11,\\ell} \\mathrm{Per2}_{t-\\ell} + \\sum_{\\ell=1}^k a_{12,\\ell} \\mathrm{Il1b}_{t-\\ell} + \\epsilon_{1,t}$, with $\\epsilon_{1,t} \\sim \\mathcal{N}(0, \\sigma_1^2)$.\n- **Equation for $\\mathrm{Il1b}_t$**: $\\mathrm{Il1b}_t = c_2 + \\sum_{\\ell=1}^k a_{21,\\ell} \\mathrm{Per2}_{t-\\ell} + \\sum_{\\ell=1}^k a_{22,\\ell} \\mathrm{Il1b}_{t-\\ell} + \\epsilon_{2,t}$, with $\\epsilon_{2,t} \\sim \\mathcal{N}(0, \\sigma_2^2)$.\n- **Granger Causality (GC) Definition**: Based on improved predictability from past values within a linear regression framework.\n- **Task**: For four given parameter sets, generate synthetic data, perform bidirectional GC testing, and report a coded integer result ($1$ for $\\mathrm{Per2} \\to \\mathrm{Il1b}$, $-1$ for $\\mathrm{Il1b} \\to \\mathrm{Per2}$, $2$ for bidirectional, $0$ for none).\n- **Parameters**: $k=2$ for all cases. Specific values for sample length $T$, intercepts $c_1, c_2$, autoregressive coefficients $\\{a_{ij,\\ell}\\}$, and innovation standard deviations $\\sigma_1, \\sigma_2$ are provided for four test cases.\n- **Statistical Test**: Nested linear model comparison using an $F$-test derived from Ordinary Least Squares (OLS) fits. Significance level $\\alpha = 0.01$.\n- **Simulation Requirements**: Use a fixed random seed, initial values of $0$, and a burn-in period greater than $200$ steps.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically grounded. Vector autoregression and Granger causality are standard methods for time series analysis, and their application to model interactions between circadian clock genes and inflammatory mediators is a conceptually valid simplification of a real biological research question.\n- **Well-Posedness**: The problem is well-posed. It specifies the data generation process, the analytical method (OLS-based $F$-test), a significance level, and the exact output format. A critical check is the stationarity of the specified VAR processes. A VAR($k$) process is stationary if all eigenvalues of its companion matrix have a modulus less than $1$. For the given parameters, the coefficient matrices $\\mathbf{A}_\\ell$ are either diagonal or triangular. This structure implies that the eigenvalues of the system's companion matrix are the union of the eigenvalues of the companion matrices for the individual autoregressive processes. For the $\\mathrm{Per2}$ process, the characteristic equation related to the companion matrix eigenvalues is $\\lambda^2 - 1.5\\lambda + 0.75 = 0$, whose roots have modulus $\\sqrt{0.75} < 1$. For the $\\mathrm{Il1b}$ process, the equation is $\\lambda^2 - 1.2\\lambda + 0.6 = 0$, whose roots have modulus $\\sqrt{0.6} < 1$. Since all eigenvalues for the uncoupled processes are within the unit circle, the specified coupled VAR systems are stationary, consistent with the problem's assumption.\n- **Objectivity**: The problem is stated in precise, objective mathematical and statistical terms.\n- **Completeness and Consistency**: The problem is self-contained, providing all necessary parameters and methodological constraints. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\n**Theoretical Framework and Algorithmic Design**\n\nThe solution is implemented by following a principled sequence of data generation and statistical analysis.\n\n**1. Vector Autoregressive Model**\nThe dynamics of the two series, $X_{1,t} = \\mathrm{Per2}_t$ and $X_{2,t} = \\mathrm{Il1b}_t$, are described by a bivariate VAR($k$) model:\n$$\n\\begin{pmatrix} X_{1,t} \\\\ X_{2,t} \\end{pmatrix}\n=\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n+\n\\sum_{\\ell=1}^{k}\n\\begin{pmatrix} a_{11,\\ell} & a_{12,\\ell} \\\\ a_{21,\\ell} & a_{22,\\ell} \\end{pmatrix}\n\\begin{pmatrix} X_{1,t-\\ell} \\\\ X_{2,t-\\ell} \\end{pmatrix}\n+\n\\begin{pmatrix} \\epsilon_{1,t} \\\\ \\epsilon_{2,t} \\end{pmatrix}\n$$\nwhere $\\mathbf{X}_t = [X_{1,t}, X_{2,t}]^T$ is the state vector at time $t$, $\\mathbf{c}$ is the vector of intercepts, $\\mathbf{A}_\\ell$ are the $2 \\times 2$ coefficient matrices for lag $\\ell$, and $\\mathbf{\\epsilon}_t$ is a vector of i.i.d. Gaussian white noise with a diagonal covariance matrix $\\mathbf{\\Sigma} = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$.\n\n**2. Data Generation**\nFor each test case, a synthetic dataset of length $T$ is generated. To ensure the process has reached its stationary distribution, the simulation is initialized with zero values and run for a burn-in period of $500$ steps (which is greater than the required $200$) before the $T$ data points are collected. A fixed random seed is employed to guarantee reproducibility.\n\n**3. Granger Causality Testing via Nested Models**\nGranger causality is assessed by testing for the statistical significance of lagged values of one variable in predicting the other. We test the hypothesis $X_2 \\to X_1$ (i.e., $\\mathrm{Il1b} \\to \\mathrm{Per2}$) as an example.\n\nThe **unrestricted model** for $X_{1,t}$ includes past values of both $X_1$ and $X_2$:\n$$\nX_{1,t} = \\beta_0 + \\sum_{\\ell=1}^{k} \\beta_{1,\\ell} X_{1,t-\\ell} + \\sum_{\\ell=1}^{k} \\gamma_{2,\\ell} X_{2,t-\\ell} + u_t\n$$\nThe null hypothesis for no Granger causality from $X_2$ to $X_1$ is $H_0: \\gamma_{2,1} = \\gamma_{2,2} = \\dots = \\gamma_{2,k} = 0$.\n\nThe **restricted model** is estimated under this null hypothesis:\n$$\nX_{1,t} = \\beta'_0 + \\sum_{\\ell=1}^{k} \\beta'_{1,\\ell} X_{1,t-\\ell} + v_t\n$$\nBoth models are fitted using Ordinary Least Squares (OLS) on the $n = T-k$ available observations. From these fits, we obtain the Sum of Squared Residuals for the restricted ($SSR_{res}$) and unrestricted ($SSR_{unres}$) models.\n\nAn $F$-statistic is calculated to compare the models:\n$$\nF = \\frac{(SSR_{res} - SSR_{unres}) / q}{SSR_{unres} / (n - p)}\n$$\n- $q = k$ is the number of restrictions (the number of coefficients for lagged $X_2$ terms).\n- $p = 1 + 2k$ is the number of parameters in the unrestricted model (one intercept, $k$ lags of $X_1$, and $k$ lags of $X_2$).\n- The denominator degrees of freedom is $n-p = (T-k) - (1+2k) = T-3k-1$.\n\nThe calculated $F$-statistic is compared to a critical value $F_{crit}$ from the F-distribution with $q$ and $n-p$ degrees of freedom at a significance level of $\\alpha = 0.01$. If $F > F_{crit}$, we reject $H_0$ and conclude that $X_2$ Granger-causes $X_1$.\n\n**4. Bidirectional Analysis and Output Encoding**\nThis procedure is performed for both potential causal directions:\n- Test 1: $H_0: \\mathrm{Il1b}$ does not Granger-cause $\\mathrm{Per2}$.\n- Test 2: $H_0: \\mathrm{Per2}$ does not Granger-cause $\\mathrm{Il1b}$.\n\nThe outcomes of these two independent tests are combined to produce a single integer code for each case, as specified in the problem statement:\n- $1$: Only $\\mathrm{Per2} \\to \\mathrm{Il1b}$ is significant.\n- $-1$: Only $\\mathrm{Il1b} \\to \\mathrm{Per2}$ is significant.\n- $2$: Both directions are significant.\n- $0$: Neither direction is significant.\n\nThe final Python implementation encapsulates this entire logic. It iterates through the provided test cases, generates data for each, performs the bidirectional Granger causality analysis, determines the integer code, and prints the collected results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to solve the Granger causality problem.\n    It iterates through predefined test cases, simulates VAR(k) data,\n    performs bidirectional Granger causality tests, and reports the results.\n    \"\"\"\n\n    # Set a fixed random seed for reproducibility of simulations.\n    np.random.seed(42)\n\n    # Test suite from the problem statement.\n    # Format: (T, k, c1, c2, a11, a22, a21, a12, sigma1, sigma2)\n    test_cases = [\n        # Case A: Per2 -> Il1b\n        (240, 2, 0.0, 0.0, [1.5, -0.75], [1.2, -0.6], [0.5, 0.0], [0.0, 0.0], 0.2, 0.2),\n        # Case B: Null\n        (240, 2, 0.0, 0.0, [1.5, -0.75], [1.2, -0.6], [0.0, 0.0], [0.0, 0.0], 0.3, 0.3),\n        # Case C: Il1b -> Per2\n        (240, 2, 0.0, 0.0, [1.5, -0.75], [1.2, -0.6], [0.0, 0.0], [0.35, 0.0], 0.2, 0.2),\n        # Case D: Boundary (short T, weak coupling)\n        (40, 2, 0.0, 0.0, [1.5, -0.75], [1.2, -0.6], [0.0, 0.0], [0.15, 0.0], 0.6, 0.6),\n    ]\n\n    alpha = 0.01\n    burn_in = 500\n    results = []\n\n    for case in test_cases:\n        T, k, c1, c2, a11, a22, a21, a12, sigma1, sigma2 = case\n        \n        # 1. Generate synthetic data from the VAR(k) model\n        total_len = T + burn_in\n        per2 = np.zeros(total_len)\n        il1b = np.zeros(total_len)\n        \n        # Generate Gaussian innovations\n        innovations1 = np.random.normal(0, sigma1, total_len)\n        innovations2 = np.random.normal(0, sigma2, total_len)\n        \n        # VAR simulation loop\n        for t in range(k, total_len):\n            lagged_per2 = per2[t-k:t][::-1]\n            lagged_il1b = il1b[t-k:t][::-1]\n            \n            per2[t] = c1 + np.dot(a11, lagged_per2) + np.dot(a12, lagged_il1b) + innovations1[t]\n            il1b[t] = c2 + np.dot(a21, lagged_per2) + np.dot(a22, lagged_il1b) + innovations2[t]\n\n        # Discard burn-in period\n        per2_final = per2[burn_in:]\n        il1b_final = il1b[burn_in:]\n\n        # 2. Perform bidirectional Granger causality testing\n        # Test if Per2 Granger-causes Il1b\n        gc_per2_to_il1b = is_granger_cause(\n            y_ts=il1b_final, x_ts=per2_final, k=k, alpha=alpha\n        )\n\n        # Test if Il1b Granger-causes Per2\n        gc_il1b_to_per2 = is_granger_cause(\n            y_ts=per2_final, x_ts=il1b_final, k=k, alpha=alpha\n        )\n\n        # 3. Encode the result\n        if gc_per2_to_il1b and not gc_il1b_to_per2:\n            results.append(1)\n        elif not gc_per2_to_il1b and gc_il1b_to_per2:\n            results.append(-1)\n        elif gc_per2_to_il1b and gc_il1b_to_per2:\n            results.append(2)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef is_granger_cause(y_ts, x_ts, k, alpha):\n    \"\"\"\n    Performs a Granger causality test to see if x_ts Granger-causes y_ts.\n    \n    Args:\n        y_ts (np.ndarray): The target time series (the \"effect\").\n        x_ts (np.ndarray): The source time series (the \"cause\").\n        k (int): The lag order.\n        alpha (float): The significance level.\n        \n    Returns:\n        bool: True if x_ts Granger-causes y_ts, False otherwise.\n    \"\"\"\n    T = len(y_ts)\n    n_obs = T - k\n    \n    # Target variable for regression\n    y_target = y_ts[k:]\n\n    # --- Restricted Model (lags of y only) ---\n    X_res = np.ones((n_obs, 1 + k))\n    for i in range(k):\n        X_res[:, i + 1] = y_ts[k-(i+1) : T-(i+1)]\n    \n    _, ssr_res, _, _ = np.linalg.lstsq(X_res, y_target, rcond=None)\n    ssr_res = ssr_res[0] if isinstance(ssr_res, np.ndarray) and ssr_res.size > 0 else 0\n\n    # --- Unrestricted Model (lags of y and x) ---\n    X_unres = np.ones((n_obs, 1 + 2 * k))\n    X_unres[:, :1+k] = X_res\n    for i in range(k):\n        X_unres[:, 1 + k + i] = x_ts[k-(i+1) : T-(i+1)]\n        \n    _, ssr_unres, _, _ = np.linalg.lstsq(X_unres, y_target, rcond=None)\n    ssr_unres = ssr_unres[0] if isinstance(ssr_unres, np.ndarray) and ssr_unres.size > 0 else 0\n    \n    # --- F-test ---\n    # Number of restrictions\n    q = k \n    # Number of parameters in unrestricted model\n    p_unres = 1 + 2 * k\n    \n    # Degrees of freedom for the F-statistic\n    df_num = q\n    df_den = n_obs - p_unres\n    \n    if df_den <= 0:\n        # Not enough data to perform the test\n        return False\n\n    # Avoid division by zero if SSR_unres is zero (perfect fit)\n    if ssr_unres < 1e-9:\n        # If unrestricted model is a perfect fit and restricted is not,\n        # it implies a very strong causal link.\n        return ssr_res > 1e-9\n    \n    f_stat = ((ssr_res - ssr_unres) / df_num) / (ssr_unres / df_den)\n    \n    # Critical value from F-distribution\n    f_crit = f_dist.ppf(1 - alpha, dfn=df_num, dfd=df_den)\n    \n    return f_stat > f_crit\n\nsolve()\n\n```", "id": "2841109"}]}