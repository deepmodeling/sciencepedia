{"hands_on_practices": [{"introduction": "A crucial first step in planning any single-cell experiment is determining the required sample size to achieve the scientific objective. This exercise guides you through a first-principles derivation to answer a common question: how many cells must be profiled to ensure a rare population is detected with high probability? By grounding experimental design in the fundamentals of binomial sampling, this practice [@problem_id:2892322] provides a critical tool for planning robust and cost-effective studies.", "problem": "A systems immunology group designs a single-cell RNA sequencing (scRNA-seq) study to detect a rare immune cell population present at true frequency $f \\in (0,1)$ in a large, well-mixed tissue. The experiment will yield exactly $n$ successfully profiled single cells, sampled independently from the tissue such that the binomial model is appropriate. The group will declare successful detection of the rare population if at least one of the $n$ profiled cells belongs to that population. Let $\\alpha \\in (0,1)$ denote a pre-specified tolerance for failing to detect the population when it is present at frequency $f$.\n\nStarting only from the fundamental probability model that independent draws from a large population with success probability $f$ yield a binomial count of successes, derive a closed-form expression for the minimal integer sample size $n^{\\ast}(f,\\alpha)$ that guarantees the probability of detecting the rare population (i.e., observing at least one such cell among the $n$ profiled cells) is at least $1-\\alpha$.\n\nYour final answer should be a single analytic expression for $n^{\\ast}(f,\\alpha)$ in terms of $f$ and $\\alpha$. No numerical evaluation or rounding is required.", "solution": "The appropriate starting point is the binomial model for independent sampling from a large population. Define indicator random variables $X_{i}$ for $i \\in \\{1,\\dots,n\\}$ such that $X_{i}=1$ if the $i$-th profiled cell belongs to the rare population and $X_{i}=0$ otherwise. Under the stated assumptions, the $X_{i}$ are independent and identically distributed with $\\mathbb{P}(X_{i}=1)=f$ and $\\mathbb{P}(X_{i}=0)=1-f$. The total number of rare cells among the $n$ profiled cells is\n$$\nS_{n} \\equiv \\sum_{i=1}^{n} X_{i} \\sim \\mathrm{Binomial}(n,f).\n$$\nThe event of detecting the population corresponds to observing at least one rare cell, that is, the event $\\{S_{n} \\ge 1\\}$. Its probability is\n$$\n\\mathbb{P}(S_{n} \\ge 1) \\;=\\; 1 - \\mathbb{P}(S_{n} = 0) \\;=\\; 1 - (1-f)^{n},\n$$\nwhere the last equality follows because $\\mathbb{P}(S_{n}=0)=\\mathbb{P}(X_{1}=0,\\dots,X_{n}=0)=(1-f)^{n}$ by independence.\n\nWe require this detection probability to be at least $1-\\alpha$, namely\n$$\n1 - (1-f)^{n} \\;\\ge\\; 1 - \\alpha.\n$$\nRearranging yields\n$$\n(1-f)^{n} \\;\\le\\; \\alpha,\n$$\nwith $\\alpha \\in (0,1)$ and $f \\in (0,1)$. Taking natural logarithms of both sides and using that $\\ln(1-f) < 0$ on $(0,1)$ yields\n$$\nn \\,\\ln(1-f) \\;\\le\\; \\ln(\\alpha).\n$$\nSince $\\ln(1-f) < 0$, dividing both sides by $\\ln(1-f)$ reverses the inequality:\n$$\nn \\;\\ge\\; \\frac{\\ln(\\alpha)}{\\ln(1-f)}.\n$$\nThe minimal integer $n^{\\ast}(f,\\alpha)$ satisfying the inequality is thus obtained by taking the ceiling:\n$$\nn^{\\ast}(f,\\alpha) \\;=\\; \\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(1-f)} \\right\\rceil.\n$$\n\nThis is the exact binomial result. For intuition in the rare-event regime where $f$ is small, one may note the approximation $\\ln(1-f) \\approx -f$, which gives $n^{\\ast}(f,\\alpha) \\approx \\left\\lceil \\ln(\\alpha^{-1})/f \\right\\rceil$, but the exact required expression is the one above.", "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(1 - f)} \\right\\rceil}$$", "id": "2892322"}, {"introduction": "Technical artifacts can confound the interpretation of high-dimensional data, and a key challenge in droplet-based single-cell RNA sequencing is contamination from ambient RNA. This practice [@problem_id:2892311] demonstrates how to construct a principled statistical model to represent this common artifact. You will derive a maximum likelihood estimator to quantify the level of contamination in each cell, a vital step for ensuring data integrity before downstream analysis.", "problem": "A droplet-based single-cell RNA sequencing (scRNA-seq; single-cell ribonucleic acid sequencing) experiment on peripheral blood mononuclear cells (PBMC; peripheral blood mononuclear cells) is suspected of ambient RNA contamination due to cell lysis in the suspension. Ambient RNA refers to transcripts that are not endogenous to a captured cell but enter droplets stochastically from the extracellular solution. In droplet methods using unique molecular identifiers (UMIs; unique molecular identifiers), barcodes with no encapsulated cell (empty droplets) and low-content barcodes (cell-free barcodes) capture only ambient RNA. The aim is to build a principled detection workflow and, within it, to derive and evaluate a simple estimator of the ambient contribution for an individual T cell.\n\nConstruct a conceptually minimal, scientifically defensible workflow that uses empty droplets and low-content barcodes to estimate the ambient composition and then quantifies the ambient fraction in each cell. Then, focusing on the estimation step for a single T cell, proceed as follows.\n\nModeling assumptions grounded in standard droplet scRNA-seq practice:\n- Let $G$ index genes and let $x_{g}$ denote the observed UMI count for gene $g$ in a particular T cell, with total $m=\\sum_{g\\in G} x_{g}$.\n- Let $a=(a_{g})_{g\\in G}$ be the ambient composition, estimated by aggregating empty droplets and cell-free barcodes and normalizing so that $\\sum_{g\\in G} a_{g}=1$.\n- In this T cell, consider a gene set $D\\subset G$ consisting of immunoglobulin heavy and light chain genes that are not endogenously expressed by T cells. Assume endogenous expression for $g\\in D$ is $0$ for this cell.\n- For the T cell, assume the observed counts are generated by a mixture of the cell’s true expression and ambient RNA such that the probability of sampling a UMI from gene $g$ is $(1-\\alpha) s_{g} + \\alpha a_{g}$, where $s=(s_{g})_{g\\in G}$ is the unknown cell-intrinsic composition ($\\sum_{g\\in G} s_{g}=1$) and $\\alpha\\in[0,1]$ is the cell-specific ambient fraction to be estimated.\n- Assume that, conditional on $m$, the vector of counts follows a multinomial sampling model with probabilities given above.\n\nTasks:\n1. From the modeling assumptions and using only the genes in $D$, derive a maximum likelihood estimator for the ambient fraction $\\alpha$ for this T cell that depends on $x_{g}$, $m$, and $a_{g}$ but not on the unknown $s_{g}$.\n2. For a particular T cell, suppose that the total UMI count is $m=5200$, the total count assigned to genes in $D$ is $\\sum_{g\\in D} x_{g} = 8$, and the estimated ambient mass in $D$ is $\\sum_{g\\in D} a_{g} = 0.015$. Evaluate your estimator numerically for this cell. Express your final answer as a pure number (dimensionless) and round to three significant figures.", "solution": "The problem presented is scientifically grounded, well-posed, and contains sufficient information for a rigorous solution. We shall proceed directly to the construction of the workflow and derivation of the estimator.\n\nA principled workflow to estimate and account for ambient RNA contamination consists of the following steps:\n$1$. Estimation of the Ambient RNA Profile: The gene expression profile of the ambient RNA, denoted by the vector $a=(a_{g})_{g\\in G}$ where $\\sum_{g \\in G} a_g = 1$, must be estimated. This is accomplished by identifying droplets that did not encapsulate a cell. These \"empty\" droplets, along with barcodes associated with very low UMI counts (indicative of cell-free RNA or debris), are assumed to sample exclusively from the ambient pool. The RNA counts from all such barcodes are aggregated and normalized to sum to $1$. This normalized vector serves as the empirical estimate of $a$.\n$2$. Identification of Contamination Marker Genes: For each cell type of interest, a set of genes must be identified that are known a priori to have zero endogenous expression in that cell type. For a T cell, as specified in the problem, the set $D$ of immunoglobulin genes is an appropriate choice, as these are expressed by B cells but not T cells. Any reads from these genes observed in a T cell are therefore attributed to ambient contamination.\n$3. $Estimation of the Cell-Specific Ambient Fraction $\\alpha$: For each individual cell, a statistical model is used to estimate the fraction $\\alpha$ of its total UMI counts that are derived from the ambient pool. The model utilizes the observed counts of the marker genes from step $2$ and the ambient profile from step $1$. The derivation of this estimator is the primary task.\n$4$. Correction of Expression Profile: Once $\\alpha$ is estimated for a cell, the expected number of ambient counts for each gene $g$ ($m \\alpha a_g$) can be calculated and used to correct the observed count $x_g$. This \"decontamination\" step provides a more accurate representation of the cell's endogenous transcriptome, although this final step is beyond the scope of the present task.\n\nWe now focus on Task $1$: the derivation of the maximum likelihood estimator (MLE) for the ambient fraction $\\alpha$.\nThe problem states that the vector of observed UMI counts $x = (x_g)_{g \\in G}$ for a single cell with total UMI count $m = \\sum_{g \\in G} x_g$ is drawn from a multinomial distribution:\n$$ x \\sim \\text{Multinomial}(m, p) $$\nwhere $p=(p_g)_{g \\in G}$ is the vector of sampling probabilities, with $p_g = (1-\\alpha)s_g + \\alpha a_g$. Here, $s_g$ represents the cell's true, unknown expression proportion for gene $g$, and $a_g$ is the known ambient proportion for gene $g$.\n\nThe critical assumption is the existence of a set of genes $D$ for which endogenous expression is zero, i.e., $s_g=0$ for all $g \\in D$. For these genes, the sampling probability simplifies to:\n$$ p_g = (1-\\alpha) \\cdot 0 + \\alpha a_g = \\alpha a_g, \\quad \\text{for } g \\in D $$\nTo derive an estimator for $\\alpha$ that is independent of the unknown profile $s$, we can re-frame the problem by collapsing all genes into two categories: those belonging to the marker set $D$ and those not belonging to $D$ (denoted by the complement set $D^c = G \\setminus D$).\n\nLet $X_D = \\sum_{g \\in D} x_g$ be the total number of UMI counts observed for genes in the marker set $D$.\nLet $P_D$ be the probability of observing a UMI from any gene in $D$. This is the sum of the individual probabilities:\n$$ P_D = \\sum_{g \\in D} p_g = \\sum_{g \\in D} \\alpha a_g = \\alpha \\sum_{g \\in D} a_g $$\nWe define $A_D = \\sum_{g \\in D} a_g$ as the total fraction of ambient RNA corresponding to the marker gene set $D$. Thus, $P_D = \\alpha A_D$.\n\nThe total number of trials is the total UMI count $m$. The number of \"successes\" (counts in $D$) is $X_D$. This situation is described by a binomial distribution:\n$$ X_D \\sim \\text{Binomial}(m, P_D) = \\text{Binomial}(m, \\alpha A_D) $$\nThe likelihood function $L$ for $\\alpha$, given the observation $X_D$, is the probability mass function of the binomial distribution:\n$$ L(\\alpha | X_D, m, A_D) = \\binom{m}{X_D} (P_D)^{X_D} (1-P_D)^{m-X_D} $$\nSubstituting $P_D = \\alpha A_D$:\n$$ L(\\alpha) = \\binom{m}{X_D} (\\alpha A_D)^{X_D} (1 - \\alpha A_D)^{m-X_D} $$\nTo find the maximum likelihood estimator $\\hat{\\alpha}$, we maximize the log-likelihood function, $\\ln L(\\alpha)$:\n$$ \\ln L(\\alpha) = \\ln \\binom{m}{X_D} + X_D \\ln(\\alpha A_D) + (m-X_D) \\ln(1 - \\alpha A_D) $$\n$$ \\ln L(\\alpha) = \\text{const} + X_D (\\ln \\alpha + \\ln A_D) + (m-X_D) \\ln(1 - \\alpha A_D) $$\nWe take the derivative with respect to $\\alpha$ and set it to zero:\n$$ \\frac{d \\ln L}{d\\alpha} = \\frac{X_D}{\\alpha} + (m-X_D) \\frac{-A_D}{1 - \\alpha A_D} = 0 $$\n$$ \\frac{X_D}{\\hat{\\alpha}} = \\frac{(m-X_D)A_D}{1 - \\hat{\\alpha} A_D} $$\nSolving for $\\hat{\\alpha}$:\n$$ X_D(1 - \\hat{\\alpha} A_D) = \\hat{\\alpha}(m-X_D)A_D $$\n$$ X_D - \\hat{\\alpha} X_D A_D = \\hat{\\alpha} m A_D - \\hat{\\alpha} X_D A_D $$\n$$ X_D = \\hat{\\alpha} m A_D $$\nThis yields the maximum likelihood estimator for $\\alpha$:\n$$ \\hat{\\alpha} = \\frac{X_D}{m A_D} = \\frac{\\sum_{g \\in D} x_g}{m \\sum_{g \\in D} a_g} $$\nThis expression depends only on the observed counts in the marker set, the total UMI count, and the aggregated ambient fraction for the marker set, as required.\n\nFor Task $2$, we evaluate this estimator numerically. The given values are:\n- Total UMI count, $m = 5200$.\n- Total count for genes in $D$, $\\sum_{g \\in D} x_g = 8$.\n- Total ambient mass in $D$, $\\sum_{g \\in D} a_g = 0.015$.\n\nSubstituting these values into our derived estimator:\n$$ \\hat{\\alpha} = \\frac{8}{5200 \\times 0.015} $$\nFirst, we calculate the denominator:\n$$ 5200 \\times 0.015 = 5200 \\times \\frac{15}{1000} = 5.2 \\times 15 = 78 $$\nNow, we compute the value of $\\hat{\\alpha}$:\n$$ \\hat{\\alpha} = \\frac{8}{78} = \\frac{4}{39} $$\nAs a decimal, this is approximately $0.1025641...$.\nThe problem requires the answer to be rounded to three significant figures. The first three significant figures are $1$, $0$, and $2$. The fourth digit is $5$, so we round up the third digit.\n$$ \\hat{\\alpha} \\approx 0.103 $$\nThis value represents an estimated ambient contamination fraction of $10.3\\%$.", "answer": "$$\\boxed{0.103}$$", "id": "2892311"}, {"introduction": "After data acquisition and quality control, a central task in systems immunology is to quantitatively compare cellular landscapes between different biological conditions. This advanced practice [@problem_id:2892437] introduces entropic-regularized optimal transport, a powerful framework for aligning and contrasting cell state distributions. You will derive the celebrated Sinkhorn algorithm from its optimization objective and apply it to a practical example, gaining hands-on experience with a state-of-the-art computational method for interpreting complex datasets.", "problem": "Consider two cell populations measured with single-cell RNA sequencing (scRNA-seq), each summarized as a discrete probability distribution over coarse-grained cell states obtained from a common latent embedding learned by a variational autoencoder. You wish to couple these distributions using entropic-regularized optimal transport to infer a stochastic mapping between conditions while discouraging overly sharp couplings in light of biological stochasticity. Let the source distribution be $a \\in \\mathbb{R}_{+}^{n}$ and the target distribution be $b \\in \\mathbb{R}_{+}^{m}$, with $\\sum_{i=1}^{n} a_{i} = 1$ and $\\sum_{j=1}^{m} b_{j} = 1$. Let $C \\in \\mathbb{R}^{n \\times m}$ be a nonnegative cost matrix derived from transcriptional dissimilarities between states, and let $\\varepsilon > 0$ be the entropic regularization strength.\n\nTask A (formulation): Using only the core definitions of probability distributions and the Kullback–Leibler Divergence (KLD), define the entropic-regularized optimal transport problem that seeks a coupling matrix $\\Pi \\in \\mathbb{R}_{+}^{n \\times m}$ minimizing transport cost plus entropy penalty subject to marginals $a$ and $b$. Your definition must be written as a constrained optimization problem in $\\Pi$ that depends on $C$, $a$, $b$, and $\\varepsilon$.\n\nTask B (derivation): Starting from your constrained optimization in Task A, perform a Lagrangian analysis and derive the fixed-point scaling equations that characterize the unique optimizer in the strictly positive case. Show how the optimal coupling admits the factorization $\\Pi^{\\star} = \\operatorname{diag}(u)\\,K\\,\\operatorname{diag}(v)$ with a Gibbs kernel $K$ determined by $C$ and $\\varepsilon$, and derive iterative update equations for the scaling vectors $u \\in \\mathbb{R}_{+}^{n}$ and $v \\in \\mathbb{R}_{+}^{m}$ that enforce the marginal constraints. Do not assume specific forms for $a$, $b$, or $C$ beyond positivity and feasibility.\n\nTask C (calculation): Now specialize to a minimal immunological scenario with $n = m = 2$ coarse-grained states representing resting and activated T cell phenotypes under two conditions. Suppose\n$$\na = \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix},\n\\quad\nC = \\begin{pmatrix} 0 & \\ln(2) \\\\ \\ln(2) & 0 \\end{pmatrix},\n\\quad\n\\varepsilon = 1.\n$$\nCompute the $(1,2)$ entry of the optimal coupling $\\Pi^{\\star}$ obtained by the entropic-regularized optimal transport in Tasks A and B, using your derived scaling updates to convergence. Round your final numeric answer to four significant figures. State only the numeric value without units.", "solution": "### Solution\n\n**Task A: Formulation**\n\nThe classical optimal transport problem seeks to minimize the total transport cost $\\langle C, \\Pi \\rangle = \\sum_{i,j} C_{ij} \\Pi_{ij}$ subject to marginal constraints. Entropic regularization introduces a penalty term to this objective, which encourages smoother, less deterministic couplings. The regularized problem can be formulated elegantly using the Kullback-Leibler (KL) divergence.\n\nLet $U(a,b)$ denote the transport polytope, which is the set of all matrices $\\Pi \\in \\mathbb{R}_{+}^{n \\times m}$ satisfying the marginal constraints:\n$$\nU(a,b) = \\left\\{ \\Pi \\in \\mathbb{R}_{+}^{n \\times m} \\mid \\sum_{j=1}^{m} \\Pi_{ij} = a_i, \\forall i; \\quad \\sum_{i=1}^{n} \\Pi_{ij} = b_j, \\forall j \\right\\}\n$$\nThe entropic-regularized optimal transport problem is equivalent to finding a coupling $\\Pi \\in U(a,b)$ that is closest, in the sense of KL divergence, to a Gibbs kernel matrix $K \\in \\mathbb{R}_{+}^{n \\times m}$ defined by the cost matrix $C$ and regularization strength $\\varepsilon$. The Gibbs kernel is defined as:\n$$\nK_{ij} = \\exp\\left(-\\frac{C_{ij}}{\\varepsilon}\\right)\n$$\nThe KL divergence between two discrete distributions $P$ and $Q$ is $\\text{KL}(P || Q) = \\sum_{k} P_k \\ln(P_k/Q_k)$. Adapting this to matrices, the problem is to minimize $\\text{KL}(\\Pi || K)$.\n\nThe constrained optimization problem is therefore:\n$$\n\\min_{\\Pi \\in U(a,b)} \\text{KL}(\\Pi || K) = \\min_{\\Pi \\in U(a,b)} \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right)\n$$\nExpanding this objective function reveals the connection to the transport cost and entropy:\n$$\n\\sum_{i,j} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) = \\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij}) - \\sum_{i,j} \\Pi_{ij} \\ln(K_{ij}) = \\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij}) + \\frac{1}{\\varepsilon} \\sum_{i,j} C_{ij} \\Pi_{ij}\n$$\nMinimizing this is equivalent to minimizing $\\frac{1}{\\varepsilon}\\langle C, \\Pi \\rangle + \\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij})$, which is the sum of the scaled transport cost and a negative entropy term, consistent with the problem's description of a \"transport cost plus entropy penalty\".\n\n**Task B: Derivation**\n\nWe derive the structure of the optimal coupling $\\Pi^{\\star}$ by analyzing the optimization problem from Task A using Lagrange multipliers. The objective function is $J(\\Pi) = \\text{KL}(\\Pi || K)$. The constraints are the marginals $\\sum_{j} \\Pi_{ij} = a_i$ and $\\sum_{i} \\Pi_{ij} = b_j$. Let $f_i \\in \\mathbb{R}$ and $g_j \\in \\mathbb{R}$ be the Lagrange multipliers associated with the source and target marginals, respectively. The Lagrangian $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(\\Pi, f, g) = \\sum_{i,j} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) - \\sum_{i=1}^{n} f_i \\left(\\sum_{j=1}^{m} \\Pi_{ij} - a_i\\right) - \\sum_{j=1}^{m} g_j \\left(\\sum_{i=1}^{n} \\Pi_{ij} - b_j\\right)\n$$\nThe optimality condition is found by setting the partial derivative of $\\mathcal{L}$ with respect to $\\Pi_{ij}$ to zero. The non-negativity constraint $\\Pi_{ij} \\ge 0$ is implicitly satisfied by the domain of the logarithm.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Pi_{ij}} = \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) + 1 - f_i - g_j = 0\n$$\nSolving for $\\Pi_{ij}$:\n$$\n\\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) = f_i + g_j - 1 \\implies \\frac{\\Pi_{ij}}{K_{ij}} = \\exp(f_i + g_j - 1)\n$$\n$$\n\\Pi_{ij} = K_{ij} \\exp(f_i-1) \\exp(g_j)\n$$\nThe form is arbitrary with respect to how the constant $\\exp(-1)$ is distributed. For symmetry, we can write $\\Pi_{ij} = K_{ij} \\exp(f_i-1/2) \\exp(g_j-1/2)$. Let us define two scaling vectors $u \\in \\mathbb{R}_{+}^{n}$ and $v \\in \\mathbb{R}_{+}^{m}$ such that:\n$$\nu_i = \\exp(f_i - 1/2) \\quad \\text{and} \\quad v_j = \\exp(g_j - 1/2)\n$$\nThe optimal coupling $\\Pi^{\\star}$ must therefore have the structure:\n$$\n\\Pi^{\\star}_{ij} = u_i K_{ij} v_j\n$$\nThis is equivalent to the matrix form $\\Pi^{\\star} = \\operatorname{diag}(u)\\,K\\,\\operatorname{diag}(v)$, where $\\operatorname{diag}(u)$ and $\\operatorname{diag}(v)$ are diagonal matrices with the elements of $u$ and $v$ on their diagonals.\n\nThe scaling vectors $u$ and $v$ are determined by enforcing the marginal constraints on this form of $\\Pi^{\\star}$:\n1.  Source marginal constraint: $\\sum_{j=1}^{m} \\Pi^{\\star}_{ij} = a_i$\n$$\n\\sum_{j=1}^{m} u_i K_{ij} v_j = a_i \\implies u_i \\left( \\sum_{j=1}^{m} K_{ij} v_j \\right) = a_i \\implies u_i = \\frac{a_i}{\\sum_{j=1}^{m} K_{ij} v_j}\n$$\nIn vector notation, this is $u = a \\oslash (Kv)$, where $\\oslash$ denotes element-wise division.\n2.  Target marginal constraint: $\\sum_{i=1}^{n} \\Pi^{\\star}_{ij} = b_j$\n$$\n\\sum_{i=1}^{n} u_i K_{ij} v_j = b_j \\implies v_j \\left( \\sum_{i=1}^{n} u_i K_{ij} \\right) = b_j \\implies v_j = \\frac{b_j}{\\sum_{i=1}^{n} K_{ij} u_i}\n$$\nIn vector notation, this is $v = b \\oslash (K^T u)$.\n\nThese two coupled equations define the scaling vectors. They can be solved iteratively using Sinkhorn's algorithm. Starting with an initial guess, typically $v^{(0)} = \\mathbf{1}_m$ (a vector of ones), one iterates the following updates for $k \\ge 1$ until convergence:\n$$\nu^{(k)} = a \\oslash (K v^{(k-1)})\n$$\n$$\nv^{(k)} = b \\oslash (K^T u^{(k)})\n$$\n\n**Task C: Calculation**\n\nWe are given $n=m=2$, $\\varepsilon=1$, and:\n$$\na = \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 0 & \\ln(2) \\\\ \\ln(2) & 0 \\end{pmatrix}\n$$\nFirst, we compute the Gibbs kernel $K = \\exp(-C/\\varepsilon) = \\exp(-C)$:\n$$\nK_{11} = \\exp(-0) = 1\n$$\n$$\nK_{12} = \\exp(-\\ln(2)) = \\frac{1}{2} = 0.5\n$$\n$$\nK_{21} = \\exp(-\\ln(2)) = \\frac{1}{2} = 0.5\n$$\n$$\nK_{22} = \\exp(-0) = 1\n$$\nSo, the kernel matrix is $K = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}$.\n\nNext, we apply the Sinkhorn iterative updates. We initialize $v^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Iteration $k=1$**:\n- Update $u$:\n$u_1^{(1)} = \\frac{a_1}{K_{11} v_1^{(0)} + K_{12} v_2^{(0)}} = \\frac{0.6}{1(1) + 0.5(1)} = \\frac{0.6}{1.5} = 0.4$\n$u_2^{(1)} = \\frac{a_2}{K_{21} v_1^{(0)} + K_{22} v_2^{(0)}} = \\frac{0.4}{0.5(1) + 1(1)} = \\frac{0.4}{1.5} = \\frac{4}{15} \\approx 0.266667$\n- Update $v$:\n$v_1^{(1)} = \\frac{b_1}{K_{11} u_1^{(1)} + K_{21} u_2^{(1)}} = \\frac{0.5}{1(0.4) + 0.5(4/15)} = \\frac{0.5}{0.4 + 2/15} = \\frac{0.5}{8/15} = \\frac{15}{16} = 0.9375$\n$v_2^{(1)} = \\frac{b_2}{K_{12} u_1^{(1)} + K_{22} u_2^{(1)}} = \\frac{0.5}{0.5(0.4) + 1(4/15)} = \\frac{0.5}{0.2 + 4/15} = \\frac{0.5}{7/15} = \\frac{15}{14} \\approx 1.071429$\n\n**Iteration $k=2$**:\n- Update $u$:\n$u_1^{(2)} = \\frac{0.6}{1(15/16) + 0.5(15/14)} = \\frac{0.6}{15/16 + 15/28} \\approx 0.407273$\n$u_2^{(2)} = \\frac{0.4}{0.5(15/16) + 1(15/14)} = \\frac{0.4}{15/32 + 15/14} \\approx 0.259710$\n- Update $v$:\n$v_1^{(2)} = \\frac{0.5}{1(0.407273) + 0.5(0.259710)} \\approx 0.930870$\n$v_2^{(2)} = \\frac{0.5}{0.5(0.407273) + 1(0.259710)} \\approx 1.079137$\n\nContinuing this process until convergence (stability in the first $5-6$ decimal places):\n$u^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, v^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n$k=1: u^{(1)} \\approx \\begin{pmatrix} 0.4 \\\\ 0.266667 \\end{pmatrix}, v^{(1)} \\approx \\begin{pmatrix} 0.9375 \\\\ 1.071429 \\end{pmatrix}$\n$k=2: u^{(2)} \\approx \\begin{pmatrix} 0.407273 \\\\ 0.259710 \\end{pmatrix}, v^{(2)} \\approx \\begin{pmatrix} 0.930870 \\\\ 1.079137 \\end{pmatrix}$\n$k=3: u^{(3)} \\approx \\begin{pmatrix} 0.408284 \\\\ 0.258828 \\end{pmatrix}, v^{(3)} \\approx \\begin{pmatrix} 0.930066 \\\\ 1.079822 \\end{pmatrix}$\n$k=4: u^{(4)} \\approx \\begin{pmatrix} 0.408401 \\\\ 0.258728 \\end{pmatrix}, v^{(4)} \\approx \\begin{pmatrix} 0.929986 \\\\ 1.079901 \\end{pmatrix}$\n$k=5: u^{(5)} \\approx \\begin{pmatrix} 0.408415 \\\\ 0.258716 \\end{pmatrix}, v^{(5)} \\approx \\begin{pmatrix} 0.929978 \\\\ 1.079910 \\end{pmatrix}$\n$k=6: u^{(6)} \\approx \\begin{pmatrix} 0.408417 \\\\ 0.258714 \\end{pmatrix}, v^{(6)} \\approx \\begin{pmatrix} 0.929977 \\\\ 1.079911 \\end{pmatrix}$\nThe values have converged sufficiently. Let the converged scaling vectors be $u^{\\star} \\approx \\begin{pmatrix} 0.408417 \\\\ 0.258714 \\end{pmatrix}$ and $v^{\\star} \\approx \\begin{pmatrix} 0.929977 \\\\ 1.079911 \\end{pmatrix}$.\n\nThe task is to compute the $(1,2)$ entry of the optimal coupling matrix, $\\Pi^{\\star}_{12}$.\n$$\n\\Pi^{\\star}_{12} = u^{\\star}_1 K_{12} v^{\\star}_2\n$$\nSubstituting the converged values and $K_{12}=0.5$:\n$$\n\\Pi^{\\star}_{12} \\approx (0.408417) \\times (0.5) \\times (1.079911)\n$$\n$$\n\\Pi^{\\star}_{12} \\approx 0.22052825\n$$\nRounding to four significant figures, we get $0.2205$.", "answer": "$$\\boxed{0.2205}$$", "id": "2892437"}]}