{"hands_on_practices": [{"introduction": "The quality of a mass cytometry experiment is fundamentally determined by the ability to distinguish true biological signal from background noise. This practice delves into the origins of the signal itself, connecting the biochemical properties of antibody-metal conjugates to the final signal-to-noise ratio ($SNR$) observed on the instrument. By modeling ion generation as a Poisson process, you will calculate how factors like polymer capacity and metal loading efficiency directly influence data quality, providing a quantitative basis for evaluating and selecting reagents [@problem_id:2866290].", "problem": "A lanthanide-loaded polymer is conjugated to each antibody to enable Cytometry by Time Of Flight (CyTOF) mass cytometry. Each polymer attached to an antibody has a fixed chelation capacity of metal-binding sites, and each site is independently occupied by a metal ion with a given probability due to incomplete loading. In a single-cell measurement, ions generated from metal atoms are counted; counts from the antigen-specific signal add linearly with the number of metal atoms, and an additive instrument background is present. Ion counting obeys Poisson statistics and independent sources of counts have additive means and variances. Define the Signal-to-Noise Ratio (SNR) as the ratio of the mean signal counts to the standard deviation of the total counts under this noise model.\n\nTwo antibodyâ€“polymer configurations are considered on the same instrument and cell type:\n- Baseline configuration: the polymer has capacity $C_{0} = 100$ chelation sites per antibody, and each site is loaded with probability $\\eta_{0} = 0.60$.\n- New configuration: the polymer has capacity $C_{1} = 150$ chelation sites per antibody, and each site is loaded with probability $\\eta_{1} = 0.82$.\n\nAssume the following for a homogeneous cell population:\n- Each cell presents $E = 2.0 \\times 10^{4}$ target epitopes, and a fraction $\\rho = 0.75$ of epitopes are bound by labeled antibodies at measurement.\n- The instrument produces, on average, $\\gamma = 2.0 \\times 10^{-2}$ counts per metal atom delivered to the plasma, and the additive background has a mean of $b = 400$ counts per event.\n- Counting statistics are Poisson for both signal and background, and independent.\n\nUsing only these assumptions and definitions:\n1) Compute the expected number of metal atoms per antibody for each configuration.\n2) From first principles, express the SNR for a single-cell event in terms of the expected signal counts and background under Poisson statistics.\n3) Compute the SNR for each configuration and then the fold-change in SNR, $\\mathcal{F} = \\dfrac{\\mathrm{SNR}_{1}}{\\mathrm{SNR}_{0}}$, where the subscripts $0$ and $1$ denote the baseline and new configurations, respectively.\n\nProvide only the numerical value of $\\mathcal{F}$ as your final answer. Express the fold-change as a dimensionless number and round your answer to four significant figures.", "solution": "The problem as stated is subjected to validation.\n\nStep 1: Extracted Givens.\n- A polymer attached to an antibody has a chelation capacity of metal-binding sites.\n- Each site is independently occupied by a metal ion with a given probability.\n- Signal counts add linearly with the number of metal atoms.\n- An additive instrument background is present.\n- Ion counting obeys Poisson statistics.\n- Independent sources of counts have additive means and variances.\n- Signal-to-Noise Ratio (SNR) is defined as the ratio of the mean signal counts to the standard deviation of the total counts.\n- Baseline configuration: polymer capacity $C_{0} = 100$ sites/antibody, loading probability $\\eta_{0} = 0.60$.\n- New configuration: polymer capacity $C_{1} = 150$ sites/antibody, loading probability $\\eta_{1} = 0.82$.\n- Number of target epitopes per cell: $E = 2.0 \\times 10^{4}$.\n- Fraction of epitopes bound by antibodies: $\\rho = 0.75$.\n- Instrument counts per metal atom: $\\gamma = 2.0 \\times 10^{-2}$.\n- Mean background counts per event: $b = 400$.\n- Counting statistics for signal and background are Poisson and independent.\n- Task 1: Compute the expected number of metal atoms per antibody for each configuration.\n- Task 2: Express SNR for a single-cell event in terms of expected signal counts and background.\n- Task 3: Compute SNR for each configuration and the fold-change $\\mathcal{F} = \\dfrac{\\mathrm{SNR}_{1}}{\\mathrm{SNR}_{0}}$.\n- Final answer: Numerical value of $\\mathcal{F}$, rounded to four significant figures.\n\nStep 2: Validation.\nThe problem is scientifically grounded, presenting a simplified but physically plausible model of mass cytometry based on standard principles of bioconjugation, protein-ligand binding, and counting statistics (Poisson processes). It is well-posed, providing a complete and consistent set of parameters and definitions necessary to arrive at a unique solution. The language is objective and quantitative. The problem does not violate any of the invalidity criteria.\n\nStep 3: Verdict.\nThe problem is deemed valid. A solution will be constructed.\n\nThe problem requires a step-by-step derivation and calculation. We will address each of the three tasks sequentially.\n\nTask 1: Expected number of metal atoms per antibody.\nFor a polymer with a capacity of $C$ chelation sites, where each site is independently occupied with a probability $\\eta$, the number of occupied sites (metal atoms) per polymer follows a binomial distribution, $L \\sim \\mathrm{Binomial}(C, \\eta)$. The expected number of metal atoms per polymer, and thus per antibody, is the mean of this distribution.\nLet $\\mathbb{E}[L]$ be the expected number of metal atoms per antibody.\n$$ \\mathbb{E}[L] = C \\cdot \\eta $$\nFor the baseline configuration (subscript $0$):\n$$ \\mathbb{E}[L_{0}] = C_{0} \\eta_{0} = 100 \\times 0.60 = 60 $$\nFor the new configuration (subscript $1$):\n$$ \\mathbb{E}[L_{1}] = C_{1} \\eta_{1} = 150 \\times 0.82 = 123 $$\nSo, the expected number of metal atoms per antibody is $60$ for the baseline and $123$ for the new configuration.\n\nTask 2: Expression for Signal-to-Noise Ratio (SNR).\nThe SNR is defined as the ratio of the mean signal counts to the standard deviation of the total counts. Let $S$ be the random variable for signal counts and $B$ be the random variable for background counts. The total counts are $T = S + B$.\nLet $\\mu_S$ be the mean signal counts and $\\mu_B$ be the mean background counts. The problem states that $S$ and $B$ are independent and follow Poisson statistics.\n$$ S \\sim \\mathrm{Poisson}(\\mu_S) \\quad , \\quad B \\sim \\mathrm{Poisson}(\\mu_B) $$\nFor a Poisson distribution, the variance is equal to the mean. Thus, $\\mathrm{Var}(S) = \\mu_S$ and $\\mathrm{Var}(B) = \\mu_B$.\nThe mean of the total counts $T$ is the sum of the means:\n$$ \\mathbb{E}[T] = \\mathbb{E}[S] + \\mathbb{E}[B] = \\mu_S + \\mu_B $$\nSince $S$ and $B$ are independent, the variance of the total counts is the sum of the variances:\n$$ \\mathrm{Var}(T) = \\mathrm{Var}(S) + \\mathrm{Var}(B) = \\mu_S + \\mu_B $$\nThe standard deviation of the total counts, $\\sigma_T$, is the square root of the variance:\n$$ \\sigma_T = \\sqrt{\\mathrm{Var}(T)} = \\sqrt{\\mu_S + \\mu_B} $$\nThe SNR is defined as:\n$$ \\mathrm{SNR} = \\frac{\\mathbb{E}[S]}{\\sigma_T} $$\nSubstituting the expressions for the mean signal and total standard deviation, and using the given mean background $b$ for $\\mu_B$, we get:\n$$ \\mathrm{SNR} = \\frac{\\mu_S}{\\sqrt{\\mu_S + b}} $$\nThis is the expression for SNR from first principles under the given noise model.\n\nTask 3: Compute SNRs and the fold-change $\\mathcal{F}$.\nFirst, we must determine the mean signal counts, $\\mu_S$, for each configuration. The mean signal is the product of the number of epitopes per cell $E$, the fraction of bound epitopes $\\rho$, the expected number of metal atoms per antibody $\\mathbb{E}[L]$, and the instrument's count yield per atom $\\gamma$.\n$$ \\mu_S = E \\cdot \\rho \\cdot \\mathbb{E}[L] \\cdot \\gamma = E \\rho (C \\eta) \\gamma $$\nFor the baseline configuration (subscript $0$):\n$$ \\mu_{S,0} = E \\rho C_{0} \\eta_{0} \\gamma = (2.0 \\times 10^{4}) \\times 0.75 \\times (100 \\times 0.60) \\times (2.0 \\times 10^{-2}) $$\n$$ \\mu_{S,0} = (1.5 \\times 10^{4}) \\times 60 \\times (2.0 \\times 10^{-2}) = 18000 $$\nThe SNR for the baseline configuration is:\n$$ \\mathrm{SNR}_{0} = \\frac{\\mu_{S,0}}{\\sqrt{\\mu_{S,0} + b}} = \\frac{18000}{\\sqrt{18000 + 400}} = \\frac{18000}{\\sqrt{18400}} $$\nFor the new configuration (subscript $1$):\n$$ \\mu_{S,1} = E \\rho C_{1} \\eta_{1} \\gamma = (2.0 \\times 10^{4}) \\times 0.75 \\times (150 \\times 0.82) \\times (2.0 \\times 10^{-2}) $$\n$$ \\mu_{S,1} = (1.5 \\times 10^{4}) \\times 123 \\times (2.0 \\times 10^{-2}) = 36900 $$\nThe SNR for the new configuration is:\n$$ \\mathrm{SNR}_{1} = \\frac{\\mu_{S,1}}{\\sqrt{\\mu_{S,1} + b}} = \\frac{36900}{\\sqrt{36900 + 400}} = \\frac{36900}{\\sqrt{37300}} $$\nThe fold-change $\\mathcal{F}$ is the ratio $\\mathrm{SNR}_{1} / \\mathrm{SNR}_{0}$:\n$$ \\mathcal{F} = \\frac{\\mathrm{SNR}_{1}}{\\mathrm{SNR}_{0}} = \\frac{\\frac{36900}{\\sqrt{37300}}}{\\frac{18000}{\\sqrt{18400}}} = \\frac{36900}{18000} \\cdot \\sqrt{\\frac{18400}{37300}} $$\n$$ \\mathcal{F} = \\frac{369}{180} \\cdot \\sqrt{\\frac{184}{373}} = 2.05 \\cdot \\sqrt{\\frac{184}{373}} $$\nNow, we compute the numerical value:\n$$ \\mathcal{F} \\approx 2.05 \\cdot \\sqrt{0.49329758...} \\approx 2.05 \\cdot 0.7023514... \\approx 1.4398204... $$\nRounding to four significant figures, the result is $1.440$.", "answer": "$$\n\\boxed{1.440}\n$$", "id": "2866290"}, {"introduction": "Raw mass cytometry data is an imperfect representation of biology due to an artifact known as spillover, where signal from one mass channel bleeds into adjacent channels. This exercise challenges you to implement a robust computational workflow to correct for this spectral mixing, a critical pre-processing step known as compensation. You will use the standard linear mixing model and solve for the true, underlying marker intensities using Non-Negative Least Squares (NNLS), a method that correctly enforces the physical reality that marker expression cannot be negative [@problem_id:2866251].", "problem": "You are given a linear mixing model for mass cytometry (Cytometry by Time of Flight (CyTOF)) signals in which the measured intensity vector $\\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^{C}$ for a single event across $C$ channels is related to the underlying true marker intensities $\\mathbf{t} \\in \\mathbb{R}_{\\ge 0}^{C}$ by the relation $\\mathbf{s} = \\mathbf{A}\\mathbf{t} + \\boldsymbol{\\eta}$, where $\\mathbf{A} \\in \\mathbb{R}^{C \\times C}$ is the spillover matrix estimated from single-stained controls (one control per channel) and $\\boldsymbol{\\eta}$ denotes measurement noise and model mismatch. The spillover matrix $\\mathbf{A}$ encodes on-diagonal primary signal and off-diagonal cross-channel contamination. For a test sample with $E$ events, the measured data are arranged as a matrix $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{E \\times C}$ whose $i$-th row is $\\mathbf{s}_i^\\top$.\n\nYour task is to compute compensated signals using Non-Negative Least Squares (NNLS) and quantify residual contamination per channel. Use the following principled definitions:\n\n1. For each event $i \\in \\{1,\\dots,E\\}$, estimate the compensated true signal $\\widehat{\\mathbf{t}}_i$ by solving the NNLS problem\n$$\n\\widehat{\\mathbf{t}}_i \\;=\\; \\arg\\min_{\\mathbf{t} \\in \\mathbb{R}_{\\ge 0}^{C}} \\left\\| \\mathbf{A}\\mathbf{t} - \\mathbf{s}_i \\right\\|_2^2.\n$$\n\n2. For each $i$ and channel $j \\in \\{1,\\dots,C\\}$, define the model-predicted measured signal $\\widehat{\\mathbf{s}}_i = \\mathbf{A}\\widehat{\\mathbf{t}}_i$ and the off-diagonal predicted contamination in channel $j$ as\n$$\n\\operatorname{offdiag}_{i,j} \\;=\\; \\left(\\widehat{\\mathbf{s}}_i\\right)_j \\;-\\; A_{j,j}\\, \\left(\\widehat{\\mathbf{t}}_i\\right)_j \\;=\\; \\sum_{\\substack{k=1\\\\k \\ne j}}^{C} A_{j,k}\\,\\left(\\widehat{\\mathbf{t}}_i\\right)_k.\n$$\n\n3. To quantify contamination and residuals in a numerically stable way for channels that may have zero measured intensity, define the per-event, per-channel denominators\n$$\nd_{i,j} \\;=\\; \\max\\!\\left( \\left(\\mathbf{s}_i\\right)_j, \\delta \\right),\n$$\nwith a fixed small constant $\\delta = 10^{-9}$.\n\n4. The predicted fractional contamination for event $i$ in channel $j$ is\n$$\nf^{\\mathrm{pred}}_{i,j} \\;=\\; \\frac{\\operatorname{offdiag}_{i,j}}{d_{i,j}},\n$$\nand the residual fractional error is\n$$\nf^{\\mathrm{resid}}_{i,j} \\;=\\; \\frac{\\left| \\left(\\mathbf{s}_i\\right)_j - \\left(\\widehat{\\mathbf{s}}_i\\right)_j \\right|}{d_{i,j}}.\n$$\n\n5. Aggregate across events to obtain two per-channel summaries:\n   - Mean predicted contamination per channel:\n   $$\n   \\overline{f}^{\\mathrm{pred}}_{j} \\;=\\; \\frac{1}{E}\\sum_{i=1}^{E} f^{\\mathrm{pred}}_{i,j}.\n   $$\n   - Maximum residual fractional error per channel:\n   $$\n   f^{\\mathrm{resid,max}}_{j} \\;=\\; \\max_{1 \\le i \\le E} f^{\\mathrm{resid}}_{i,j}.\n   $$\n\nImplement a program that, for each test case provided below, computes the vectors $\\left(\\overline{f}^{\\mathrm{pred}}_{j}\\right)_{j=1}^{C}$ and $\\left(f^{\\mathrm{resid,max}}_{j}\\right)_{j=1}^{C}$ using the definitions above. Use $C = 3$ channels for all cases. No physical units are required for intensities. All outputs must be rounded to $6$ decimal places.\n\nTest suite (three cases):\n\n- Case $1$ (well-conditioned spillover, noiseless sample):\n  - $C = 3$, $E = 4$.\n  - Spillover matrix\n    $$\n    \\mathbf{A}_1 = \\begin{bmatrix}\n    1.0  0.02  0.01\\\\\n    0.015  1.0  0.025\\\\\n    0.0  0.03  1.0\n    \\end{bmatrix}.\n    $$\n  - Measured sample matrix\n    $$\n    \\mathbf{S}_1 = \\begin{bmatrix}\n    1001.1  65.25  11.5\\\\\n    10.0  500.0  15.0\\\\\n    206.0  208.0  206.0\\\\\n    1.0  2.5  100.0\n    \\end{bmatrix}.\n    $$\n\n- Case $2$ (identity spillover, arbitrary signals including zeros):\n  - $C = 3$, $E = 3$.\n  - Spillover matrix\n    $$\n    \\mathbf{A}_2 = \\begin{bmatrix}\n    1.0  0.0  0.0\\\\\n    0.0  1.0  0.0\\\\\n    0.0  0.0  1.0\n    \\end{bmatrix}.\n    $$\n  - Measured sample matrix\n    $$\n    \\mathbf{S}_2 = \\begin{bmatrix}\n    10.0  0.0  0.0\\\\\n    0.0  20.0  30.0\\\\\n    5.0  5.0  5.0\n    \\end{bmatrix}.\n    $$\n\n- Case $3$ (strong cross-talk, mild additive noise):\n  - $C = 3$, $E = 4$.\n  - Spillover matrix\n    $$\n    \\mathbf{A}_3 = \\begin{bmatrix}\n    1.0  0.4  0.3\\\\\n    0.35  1.0  0.45\\\\\n    0.25  0.5  1.0\n    \\end{bmatrix}.\n    $$\n  - Noisy measured sample matrix\n    $$\n    \\mathbf{S}_3 = \\begin{bmatrix}\n    150.3  141.5  125.2\\\\\n    47.9  120.0  60.1\\\\\n    65.0  40.2  62.2\\\\\n    6.05  4.27  3.23\n    \\end{bmatrix}.\n    $$\n\nFinal output specification:\n\n- For each test case $m \\in \\{1,2,3\\}$, compute two lists:\n  - $\\left[\\overline{f}^{\\mathrm{pred}}_{1}, \\overline{f}^{\\mathrm{pred}}_{2}, \\overline{f}^{\\mathrm{pred}}_{3}\\right]$,\n  - $\\left[f^{\\mathrm{resid,max}}_{1}, f^{\\mathrm{resid,max}}_{2}, f^{\\mathrm{resid,max}}_{3}\\right]$,\n  with all entries rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of three elements, each element itself being a two-element list as described above, and with no spaces. For example, the top-level structure must look like\n  $$\n  \\big[\\,[\\,[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]\\,],\\;[\\,[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]\\,],\\;[\\,[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]\\,]\\,\\big]\n  $$\n  where each $\\cdot$ is a decimal rounded to $6$ places.\nUse $\\delta = 10^{-9}$ in all denominators $d_{i,j}$ as defined above. Angles are not involved. All outputs must be real-valued decimals; do not use percentages or symbols.", "solution": "The problem as stated is subjected to rigorous validation and is found to be valid. It is scientifically grounded in the principles of mass cytometry, mathematically well-posed, and provides a complete and unambiguous specification for all required computations. We shall therefore proceed to formulate the solution.\n\nThe objective is to analyze simulated mass cytometry data by first compensating for signal spillover and then quantifying both the extent of this spillover and the residual error of the model. The procedure is a sequence of well-defined analytical steps applied to each cellular event, followed by an aggregation of metrics across all events.\n\nThe foundational model is the linear mixing equation $\\mathbf{s} = \\mathbf{A}\\mathbf{t} + \\boldsymbol{\\eta}$. Here, $\\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^{C}$ is the vector of measured signal intensities across $C$ channels for a single cell. The vector $\\mathbf{t} \\in \\mathbb{R}_{\\ge 0}^{C}$ represents the true, unobserved intensities of the biological markers, which are physically constrained to be non-negative. The spillover matrix $\\mathbf{A} \\in \\mathbb{R}^{C \\times C}$ is a system matrix that quantifies how signal from one channel leaks into others. Its diagonal elements $A_{j,j}$ represent the primary signal sensitivity, while its off-diagonal elements $A_{j,k}$ ($j \\ne k$) represent the cross-channel contamination. The term $\\boldsymbol{\\eta}$ accounts for measurement noise and any deviations from the ideal linear model.\n\nThe algorithmic process for analyzing a dataset of $E$ events, represented by the matrix $\\mathbf{S} \\in \\mathbb{R}_{\\ge 0}^{E \\times C}$, is comprised of the following steps:\n\n1.  **Event-wise Signal Deconvolution:** For each event $i$ (each row $\\mathbf{s}_i$ of $\\mathbf{S}$), we must estimate the true underlying signal vector $\\mathbf{t}_i$. Since true marker expression cannot be negative, we use Non-Negative Least Squares (NNLS) to solve the optimization problem:\n    $$\n    \\widehat{\\mathbf{t}}_i \\;=\\; \\arg\\min_{\\mathbf{t} \\in \\mathbb{R}_{\\ge 0}^{C}} \\left\\| \\mathbf{A}\\mathbf{t} - \\mathbf{s}_i \\right\\|_2^2.\n    $$\n    This formulation finds the best non-negative true signal vector $\\widehat{\\mathbf{t}}_i$ that, when transformed by the spillover matrix $\\mathbf{A}$, most closely reconstructs the measured signal vector $\\mathbf{s}_i$ in the Euclidean norm sense. This is the core signal compensation step.\n\n2.  **Calculation of Model-Predicted Signals and Contamination:** Once the compensated signal $\\widehat{\\mathbf{t}}_i$ is estimated, we compute the idealized measured signal that our model predicts:\n    $$\n    \\widehat{\\mathbf{s}}_i = \\mathbf{A}\\widehat{\\mathbf{t}}_i.\n    $$\n    The off-diagonal contamination in a specific channel $j$ is the portion of the predicted signal $\\left(\\widehat{\\mathbf{s}}_i\\right)_j$ that originates from true signals in other channels $k \\ne j$. It is calculated as:\n    $$\n    \\operatorname{offdiag}_{i,j} \\;=\\; \\sum_{\\substack{k=1\\\\k \\ne j}}^{C} A_{j,k}\\,\\left(\\widehat{\\mathbf{t}}_i\\right)_k.\n    $$\n\n3.  **Quantification of Fractional Contamination and Residual Error:** To make these quantities comparable across different signal magnitudes and to ensure numerical stability, they are normalized. The denominator for each channel $j$ of event $i$ is defined as $d_{i,j} = \\max\\!\\left( \\left(\\mathbf{s}_i\\right)_j, \\delta \\right)$, where $\\delta = 10^{-9}$ is a small constant to prevent division by zero.\n    -   The predicted fractional contamination is the ratio of the off-diagonal contribution to the measured signal:\n        $$\n        f^{\\mathrm{pred}}_{i,j} \\;=\\; \\frac{\\operatorname{offdiag}_{i,j}}{d_{i,j}}.\n        $$\n    -   The residual fractional error measures the discrepancy between the measured signal and the model-reconstructed signal, indicating the goodness-of-fit:\n        $$\n        f^{\\mathrm{resid}}_{i,j} \\;=\\; \\frac{\\left| \\left(\\mathbf{s}_i\\right)_j - \\left(\\widehat{\\mathbf{s}}_i\\right)_j \\right|}{d_{i,j}}.\n        $$\n    A non-zero residual can be attributed to the noise term $\\boldsymbol{\\eta}$ or fundamental inaccuracies of the linear model.\n\n4.  **Aggregation of Per-Channel Statistics:** Finally, the event-level metrics are aggregated across all $E$ events to produce two summary vectors, each of dimension $C$:\n    -   The mean predicted contamination per channel, $\\overline{f}^{\\mathrm{pred}}_{j}$, provides an average measure of the spillover impact on channel $j$ across the cell population:\n        $$\n        \\overline{f}^{\\mathrm{pred}}_{j} \\;=\\; \\frac{1}{E}\\sum_{i=1}^{E} f^{\\mathrm{pred}}_{i,j}.\n        $$\n    -   The maximum residual fractional error per channel, $f^{\\mathrm{resid,max}}_{j}$, identifies the worst-case model failure for channel $j$, which is a critical diagnostic for model performance:\n        $$\n        f^{\\mathrm{resid,max}}_{j} \\;=\\; \\max_{1 \\le i \\le E} f^{\\mathrm{resid}}_{i,j}.\n        $$\n\nThe implementation will systematically apply this entire procedure to each provided test case. It will utilize the `nnls` function from the `scipy.optimize` library for the core optimization step and `numpy` for efficient matrix and vector operations. The final results for each case, consisting of the two vectors $(\\overline{f}^{\\mathrm{pred}}_{j})_{j=1}^{C}$ and $(f^{\\mathrm{resid,max}}_{j})_{j=1}^{C}$, will be formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Solves the mass cytometry compensation problem for the given test cases.\n    \"\"\"\n    \n    # Test case 1 data\n    A1 = np.array([\n        [1.0, 0.02, 0.01],\n        [0.015, 1.0, 0.025],\n        [0.0, 0.03, 1.0]\n    ])\n    S1 = np.array([\n        [1001.1, 65.25, 11.5],\n        [10.0, 500.0, 15.0],\n        [206.0, 208.0, 206.0],\n        [1.0, 2.5, 100.0]\n    ])\n\n    # Test case 2 data\n    A2 = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    S2 = np.array([\n        [10.0, 0.0, 0.0],\n        [0.0, 20.0, 30.0],\n        [5.0, 5.0, 5.0]\n    ])\n\n    # Test case 3 data\n    A3 = np.array([\n        [1.0, 0.4, 0.3],\n        [0.35, 1.0, 0.45],\n        [0.25, 0.5, 1.0]\n    ])\n    S3 = np.array([\n        [150.3, 141.5, 125.2],\n        [47.9, 120.0, 60.1],\n        [65.0, 40.2, 62.2],\n        [6.05, 4.27, 3.23]\n    ])\n\n    test_cases = [\n        (A1, S1),\n        (A2, S2),\n        (A3, S3)\n    ]\n\n    delta = 1e-9\n    all_results_str = []\n\n    for A, S in test_cases:\n        E, C = S.shape\n        \n        f_pred_matrix = np.zeros((E, C))\n        f_resid_matrix = np.zeros((E, C))\n\n        # Pre-calculate the off-diagonal part of A\n        A_offdiag = A - np.diag(np.diag(A))\n\n        for i in range(E):\n            s_i = S[i, :]\n            \n            # Step 1: Solve NNLS problem for the current event\n            t_hat_i, _ = nnls(A, s_i)\n            \n            # Step 2: Calculate predicted signal and off-diagonal contamination\n            s_hat_i = A @ t_hat_i\n            offdiag_i_vector = A_offdiag @ t_hat_i\n            \n            # Step 3: Calculate denominators for normalization\n            d_i = np.maximum(s_i, delta)\n            \n            # Step 4: Calculate fractional contamination and residual error\n            f_pred_i = offdiag_i_vector / d_i\n            f_resid_i = np.abs(s_i - s_hat_i) / d_i\n            \n            f_pred_matrix[i, :] = f_pred_i\n            f_resid_matrix[i, :] = f_resid_i\n            \n        # Step 5: Aggregate across events\n        mean_f_pred = np.mean(f_pred_matrix, axis=0)\n        max_f_resid = np.max(f_resid_matrix, axis=0)\n        \n        # Format results for the current test case\n        pred_list_str = [f\"{x:.6f}\" for x in mean_f_pred]\n        resid_list_str = [f\"{x:.6f}\" for x in max_f_resid]\n        \n        case_result_str = f\"[[{','.join(pred_list_str)}],[{','.join(resid_list_str)}]]\"\n        all_results_str.append(case_result_str)\n\n    # Final print statement must produce a single line with no extra characters\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "2866251"}, {"introduction": "After data processing and cell population identification, the final step is often to test a scientific hypothesis, such as whether a treatment alters the frequency of a particular cell type. This practice focuses on the statistical inference required to draw valid conclusions, particularly in the context of a paired experimental design (e.g., samples from the same donor before and after treatment). You will implement an exact permutation test, a powerful non-parametric method that leverages the paired structure of the data to provide a robust assessment of treatment effects on cluster abundances [@problem_id:2866321].", "problem": "You are given paired mass cytometry by time-of-flight (CyTOF) cluster counts for donors measured under a control condition and the same donors measured under a treatment. For donor-level inference that preserves pairing, design and implement an exact permutation test based on sign-flips of within-donor differences to assess whether treatment changes cluster abundance, while controlling type I error under the null hypothesis that treatment has no effect on the mean of transformed cluster proportions across donors.\n\nFundamental base and assumptions:\n- Cluster event counts measured by mass cytometry are modeled as binomial draws conditional on the total events per sample. Specifically, for donor index $i \\in \\{1,\\dots,n\\}$ and condition $c \\in \\{\\text{control}, \\text{treatment}\\}$, let $Y_{i,c}$ be the cluster count and $N_{i,c}$ the total event count, with $Y_{i,c} \\mid N_{i,c} \\sim \\text{Binomial}(N_{i,c}, p_{i,c})$. The cluster proportion is $P_{i,c} = Y_{i,c}/N_{i,c}$.\n- Use the arcsine square root variance-stabilizing transform $g(p) = \\arcsin(\\sqrt{p})$, with angles in radians. Define within-donor transformed differences $D_i = g(P_{i,\\text{treatment}}) - g(P_{i,\\text{control}})$.\n- Under the sharp null hypothesis that treatment does not change the within-donor cluster proportion distribution, the signs of $D_i$ are exchangeable across donors. A sign-flip permutation test that enumerates all $2^n$ sign patterns on $\\{D_i\\}_{i=1}^n$ yields an exact two-sided test at level $\\alpha$ for the null hypothesis that the mean of $D_i$ is zero.\n- Define the studentized test statistic $T = \\dfrac{\\bar{D}}{S_D/\\sqrt{n}}$, where $\\bar{D}$ is the sample mean of $\\{D_i\\}$ and $S_D$ is the unbiased sample standard deviation computed with degrees-of-freedom correction $n-1$.\n\nAlgorithm to implement:\n- Given paired counts $\\{(N_{i,\\text{control}}, Y_{i,\\text{control}}, N_{i,\\text{treatment}}, Y_{i,\\text{treatment}})\\}_{i=1}^n$, compute $P_{i,c} = Y_{i,c}/N_{i,c}$, $D_i = \\arcsin(\\sqrt{P_{i,\\text{treatment}}}) - \\arcsin(\\sqrt{P_{i,\\text{control}}})$ in radians, $\\bar{D}$, $S_D$, and $T_{\\text{obs}} = \\bar{D}/(S_D/\\sqrt{n})$.\n- Enumerate all $2^n$ sign patterns $\\mathbf{s} \\in \\{-1, +1\\}^n$ to generate permuted differences $D_i^{(\\mathbf{s})} = s_i D_i$, compute $T^{(\\mathbf{s})}$ analogously for each permutation, and compute the exact two-sided permutation $p$-value $p_{\\text{perm}} = \\dfrac{1}{2^n} \\sum_{\\mathbf{s}} \\mathbf{1}\\{\\,|T^{(\\mathbf{s})}| \\ge |T_{\\text{obs}}|\\,\\}$.\n- Use a two-sided significance level $\\alpha = 0.05$ and reject when $p_{\\text{perm}} \\le \\alpha$.\n- Angle unit requirement: the arcsine function must be evaluated in radians.\n\nTest suite:\nImplement your program to run the following four independent test cases. Each case is supplied as an explicit list of donors, where each donor is a tuple $(N_{\\text{control}}, Y_{\\text{control}}, N_{\\text{treatment}}, Y_{\\text{treatment}})$.\n\n- Case A (null-like, mixed small changes, $n = 8$):\n$$\n\\text{case}_A =\n[(12000,180,13000,195),\n(9000,90,9500,105),\n(15000,300,14500,280),\n(11000,132,11500,130),\n(8000,80,8500,88),\n(10000,140,12000,168),\n(13000,169,12500,162),\n(14000,210,15000,240)].\n$$\n\n- Case B (treatment increases abundance, $n = 10$):\n$$\n\\text{case}_B =\n[(10000,100,10000,200),\n(12000,96,11000,220),\n(9000,90,9000,180),\n(15000,225,15000,450),\n(16000,128,16000,256),\n(11000,110,12000,240),\n(8000,72,8000,160),\n(20000,220,20000,440),\n(14000,140,13000,299),\n(10000,130,9000,216)].\n$$\n\n- Case C (edge with zeros and small counts, $n = 6$):\n$$\n\\text{case}_C =\n[(5000,0,5000,10),\n(6000,0,6000,0),\n(7000,7,7000,0),\n(8000,0,8000,16),\n(9000,9,9000,18),\n(10000,10,10000,8)].\n$$\n\n- Case D (small sample, consistent increases, $n = 3$):\n$$\n\\text{case}_D =\n[(10000,100,10000,180),\n(12000,60,12000,120),\n(15000,150,15000,300)].\n$$\n\nTask:\n- For each case, compute the exact two-sided permutation $p$-value as specified and return a boolean indicating whether to reject the null hypothesis at level $\\alpha = 0.05$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{Case A}, \\text{Case B}, \\text{Case C}, \\text{Case D}]$, where each element is either True or False, for example, $[\\text{True},\\text{False},\\text{True},\\text{False}]$.\n\nNotes:\n- All computations must be performed in radians for trigonometric functions.\n- If $S_D = 0$, define $T = 0$ when $\\bar{D} = 0$ and define $T = \\text{sign}(\\bar{D}) \\cdot \\infty$ otherwise for the purpose of permutation comparisons.", "solution": "The problem as stated is valid. It presents a clear, self-contained, and scientifically sound task based on established principles of biostatistics. The experimental context, mass cytometry, is appropriate for the proposed data structure. The model assumptions, including the binomial distribution for cell counts and the use of an arcsine square root transformation for variance stabilization, are standard practice for analyzing proportional data. The core of the problem is the implementation of an exact permutation test, a non-parametric method that provides robust control of type I error by generating the null distribution directly from the data. The problem is well-posed, with all necessary data, parameters, and a deterministic algorithm provided, ensuring a unique and verifiable solution.\n\nWe will proceed with the solution by systematically implementing the specified algorithm. The foundation of this procedure rests on the statistical analysis of paired data, where each donor provides a pair of measurements: one under a control condition and one under a treatment condition.\n\nFirst, we formalize the initial data transformation. For each donor $i \\in \\{1, \\dots, n\\}$, we are given the total number of cells analyzed, $N_{i,c}$, and the number of cells belonging to a specific cluster, $Y_{i,c}$, for each condition $c \\in \\{\\text{control}, \\text{treatment}\\}$. The proportion of cells in the cluster is estimated as $P_{i,c} = Y_{i,c} / N_{i,c}$. These proportions are realizations of random variables, and their variance depends on the underlying true probability, which is undesirable for many statistical tests. To mitigate this, we apply the arcsine square root transformation, a standard variance-stabilizing transformation for binomial proportions. The transformed value is $g(p) = \\arcsin(\\sqrt{p})$, with the angle measured in radians. The variance of $g(P_{i,c})$ is approximately $\\frac{1}{4N_{i,c}}$, which is independent of the proportion $p_{i,c}$.\n\nThe paired nature of the experimental design is exploited by computing the within-donor difference of the transformed proportions:\n$$\nD_i = g(P_{i,\\text{treatment}}) - g(P_{i,\\text{control}}) = \\arcsin(\\sqrt{P_{i,\\text{treatment}}}) - \\arcsin(\\sqrt{P_{i,\\text{control}}})\n$$\nThe set of differences $\\{D_i\\}_{i=1}^n$ becomes the fundamental data for our test. The null hypothesis to be tested is that the treatment has no effect on cluster abundance, which implies that the mean of the distribution from which the $D_i$ are drawn is zero.\n\nThe test procedure is an exact permutation test based on sign-flipping. This method is justified by the sharp null hypothesis, which states that for any given donor, the treatment has absolutely no effect. Under this hypothesis, the labels \"control\" and \"treatment\" are interchangeable. Swapping these labels for donor $i$ would simply change the sign of the computed difference $D_i$. Therefore, under the sharp null, the sign of each $D_i$ can be considered a random outcome from a coin flip ($\\{-1, +1\\}$ with equal probability), independent across donors. This principle allows us to generate the complete null distribution of any test statistic by considering all $2^n$ possible sign combinations for the observed differences $\\{D_i\\}_{i=1}^n$.\n\nThe specified test statistic is the studentized mean of the differences, which is identical in form to a one-sample t-statistic:\n$$\nT = \\frac{\\bar{D}}{S_D / \\sqrt{n}}\n$$\nwhere $\\bar{D} = \\frac{1}{n} \\sum_{i=1}^n D_i$ is the sample mean of the differences, and $S_D = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (D_i - \\bar{D})^2}$ is the unbiased sample standard deviation. We first compute this statistic for the observed data, yielding $T_{\\text{obs}}$.\n\nThe core of the algorithm is the enumeration of the null distribution. We generate all $2^n$ sign vectors $\\mathbf{s} = (s_1, s_2, \\dots, s_n)$, where each $s_i \\in \\{-1, +1\\}$. For each vector $\\mathbf{s}$, we create a permuted set of differences $\\{D_i^{(\\mathbf{s})} = s_i D_i\\}_{i=1}^n$. For each such set, we compute the corresponding test statistic $T^{(\\mathbf{s})}$ using the same formula:\n$$\nT^{(\\mathbf{s})} = \\frac{\\bar{D}^{(\\mathbf{s})}}{S_D^{(\\mathbf{s})} / \\sqrt{n}}\n$$\nwhere $\\bar{D}^{(\\mathbf{s})}$ and $S_D^{(\\mathbf{s})}$ are the mean and standard deviation of the permuted differences $\\{D_i^{(\\mathbf{s})}\\}_{i=1}^n$. A special case arises if the standard deviation of a permuted set, $S_D^{(\\mathbf{s})}$, is zero. If the mean $\\bar{D}^{(\\mathbf{s})}$ is also zero, $T^{(\\mathbf{s})}$ is defined as $0$. If $\\bar{D}^{(\\mathbf{s})}$ is non-zero, $T^{(\\mathbf{s})}$ is defined as $\\text{sign}(\\bar{D}^{(\\mathbf{s})}) \\cdot \\infty$ to correctly reflect an infinitely significant deviation. The collection of the $2^n$ values of $T^{(\\mathbf{s})}$ forms the exact null distribution of the statistic.\n\nThe two-sided p-value is then calculated as the proportion of permutations for which the test statistic is at least as extreme as the observed statistic. Formally,\n$$\np_{\\text{perm}} = \\frac{1}{2^n} \\sum_{\\mathbf{s} \\in \\{-1, +1\\}^n} \\mathbf{1}\\{|T^{(\\mathbf{s})}| \\ge |T_{\\text{obs}}|\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which equals $1$ if the condition is true and $0$ otherwise.\n\nFinally, a decision is made based on a pre-defined significance level $\\alpha$. The problem specifies $\\alpha = 0.05$. If $p_{\\text{perm}} \\le 0.05$, we reject the null hypothesis and conclude there is statistically significant evidence that the treatment affects the cluster abundance. Otherwise, if $p_{\\text{perm}}  0.05$, we fail to reject the null hypothesis. This procedure will now be implemented for each of the four test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases specified in the problem.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = {\n        \"Case A\": [\n            (12000, 180, 13000, 195), (9000, 90, 9500, 105), (15000, 300, 14500, 280),\n            (11000, 132, 11500, 130), (8000, 80, 8500, 88), (10000, 140, 12000, 168),\n            (13000, 169, 12500, 162), (14000, 210, 15000, 240)\n        ],\n        \"Case B\": [\n            (10000, 100, 10000, 200), (12000, 96, 11000, 220), (9000, 90, 9000, 180),\n            (15000, 225, 15000, 450), (16000, 128, 16000, 256), (11000, 110, 12000, 240),\n            (8000, 72, 8000, 160), (20000, 220, 20000, 440), (14000, 140, 13000, 299),\n            (10000, 130, 9000, 216)\n        ],\n        \"Case C\": [\n            (5000, 0, 5000, 10), (6000, 0, 6000, 0), (7000, 7, 7000, 0),\n            (8000, 0, 8000, 16), (9000, 9, 9000, 18), (10000, 10, 10000, 8)\n        ],\n        \"Case D\": [\n            (10000, 100, 10000, 180), (12000, 60, 12000, 120), (15000, 150, 15000, 300)\n        ]\n    }\n\n    alpha = 0.05\n    results = []\n\n    # Order of cases for final output\n    case_order = [\"Case A\", \"Case B\", \"Case C\", \"Case D\"]\n\n    for case_name in case_order:\n        data = test_cases[case_name]\n        \n        n_donors = len(data)\n        \n        # Unpack data and calculate proportions\n        counts_ctrl = np.array([d[1] for d in data], dtype=float)\n        totals_ctrl = np.array([d[0] for d in data], dtype=float)\n        counts_treat = np.array([d[3] for d in data], dtype=float)\n        totals_treat = np.array([d[2] for d in data], dtype=float)\n\n        # Avoid division by zero, although not present in test data\n        prop_ctrl = np.divide(counts_ctrl, totals_ctrl, out=np.zeros_like(counts_ctrl), where=totals_ctrl!=0)\n        prop_treat = np.divide(counts_treat, totals_treat, out=np.zeros_like(counts_treat), where=totals_treat!=0)\n\n        # Arcsine square root transformation (in radians) and calculate differences\n        transformed_ctrl = np.arcsin(np.sqrt(prop_ctrl))\n        transformed_treat = np.arcsin(np.sqrt(prop_treat))\n        \n        D = transformed_treat - transformed_ctrl\n\n        def calculate_statistic(diffs):\n            \"\"\"Calculates the studentized test statistic for a given set of differences.\"\"\"\n            n = len(diffs)\n            if n  2: return 0.0 # Standard deviation is not defined for n  2\n            \n            mean_d = np.mean(diffs)\n            std_d = np.std(diffs, ddof=1) # ddof=1 for unbiased sample std dev\n\n            if std_d == 0:\n                if mean_d == 0:\n                    return 0.0\n                else:\n                    return np.sign(mean_d) * np.inf\n            \n            return mean_d / (std_d / np.sqrt(n))\n\n        # Calculate observed statistic\n        t_obs = calculate_statistic(D)\n        \n        num_permutations = 2**n_donors\n        extreme_count = 0\n        \n        # Enumerate all 2^n sign patterns\n        for s in product([-1, 1], repeat=n_donors):\n            sign_vector = np.array(s)\n            d_perm = D * sign_vector\n            t_perm = calculate_statistic(d_perm)\n            \n            if np.abs(t_perm) >= np.abs(t_obs):\n                extreme_count += 1\n                \n        # Calculate permutation p-value\n        p_value = extreme_count / num_permutations\n        \n        # Decision rule\n        reject_null = p_value = alpha\n        results.append(reject_null)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2866321"}]}