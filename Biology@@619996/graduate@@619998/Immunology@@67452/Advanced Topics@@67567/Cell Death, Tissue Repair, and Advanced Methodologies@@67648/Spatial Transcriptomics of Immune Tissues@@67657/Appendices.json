{"hands_on_practices": [{"introduction": "A crucial first step in analyzing spatial transcriptomics data is to determine the extent to which gene expression is spatially organized versus being driven by random noise. This exercise guides you through fitting a Gaussian Process, a powerful statistical model, to decompose gene expression variance into distinct spatial and non-spatial components. By estimating the Proportion of Variance Explained (PVE) by spatial effects, you will gain a quantitative understanding of how to measure and interpret the spatial structure inherent in gene expression patterns across different hypothetical tissue samples. [@problem_id:2890143]", "problem": "You are given spatially resolved messenger ribonucleic acid (mRNA) measurements for the chemokine gene CXCL13 in human lymph node tissue sections. Assume the following fundamental base: the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein) and the well-tested observation that log-normalized spatial transcript counts display spatial autocorrelation due to microanatomical niches in immune tissues, superimposed with independent measurement noise. Model the log-normalized CXCL13 expression across spots as a spatial random effects model: for a sample with $n$ spots at coordinates $\\{\\mathbf{x}_i \\in \\mathbb{R}^2\\}_{i=1}^n$ (in micrometers), the observed expression vector $\\mathbf{y} \\in \\mathbb{R}^n$ satisfies\n$$\n\\mathbf{y} = \\mu \\mathbf{1} + \\mathbf{s} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mu \\in \\mathbb{R}$ is an unknown global mean, $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_s^2 \\mathbf{K})$ is a zero-mean Gaussian Process spatial effect with variance $\\sigma_s^2 \\ge 0$ and isotropic squared-exponential kernel $\\mathbf{K}$ defined by\n$$\nK_{ij} = \\exp\\left(-\\frac{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2^2}{2 \\ell^2}\\right),\n$$\nand $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$ is independent residual noise with variance $\\sigma_e^2 \\ge 0$. The length-scale $\\ell > 0$ controls the spatial decay and is to be set deterministically for each sample as the median of all pairwise Euclidean distances between distinct spots, expressed in micrometers. The proportion of variance explained (PVE) by spatial effects is defined as\n$$\n\\mathrm{PVE} = \\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_e^2}.\n$$\n\nTask: For each sample listed below, estimate $\\sigma_s^2$ and $\\sigma_e^2$ by maximizing the Gaussian marginal log-likelihood of $\\mathbf{y}$ under the above model with $\\ell$ fixed as specified, treating $\\mu$ as an unknown nuisance parameter to be estimated jointly via its maximum likelihood value. Use a numerically stable method based on Cholesky factorization of the covariance matrix. If the total sample variance of $\\mathbf{y}$ is numerically negligible (defined as less than $10^{-12}$), define $\\mathrm{PVE} = 0.0$ for that sample. Report the estimated $\\mathrm{PVE}$ for each sample as a decimal in $[0,1]$, rounded to $3$ decimal places.\n\nPhysical units: All coordinates are given in micrometers ($\\mu$m). The output is dimensionless and must be expressed as decimals.\n\nAngle unit: Not applicable.\n\nPercentages: Not applicable; report as decimals.\n\nTest suite (four samples). All samples share the same $n = 9$ spatial locations given by the Cartesian product $\\{0,50,100\\} \\times \\{0,50,100\\}$ (in micrometers), ordered as follows:\n- Spot $1$: $(0,0)$\n- Spot $2$: $(0,50)$\n- Spot $3$: $(0,100)$\n- Spot $4$: $(50,0)$\n- Spot $5$: $(50,50)$\n- Spot $6$: $(50,100)$\n- Spot $7$: $(100,0)$\n- Spot $8$: $(100,50)$\n- Spot $9$: $(100,100)$\n\nFor each sample, the observed log-normalized CXCL13 expression vector $\\mathbf{y}$ (in the above spot order) is:\n\n- Sample A (pronounced spatial niche around the center): $\\mathbf{y}^{(A)} = [4.2, 5.8, 4.1, 6.0, 10.5, 6.2, 3.9, 5.7, 4.0]$.\n- Sample B (weak spatial structure): $\\mathbf{y}^{(B)} = [5.1, 5.0, 5.2, 4.9, 5.1, 5.0, 5.2, 5.0, 5.3]$.\n- Sample C (no evident spatial structure; heterogeneous noise): $\\mathbf{y}^{(C)} = [7.4, 3.1, 5.6, 8.0, 2.7, 4.9, 6.2, 7.9, 3.5]$.\n- Sample D (boundary case; essentially no variance): $\\mathbf{y}^{(D)} = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]$.\n\nAlgorithmic requirements:\n- For each sample, compute $\\ell$ as the median of $\\{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2 : 1 \\le i < j \\le 9\\}$.\n- For fixed $\\ell$, maximize the Gaussian marginal log-likelihood with respect to $(\\sigma_s^2, \\sigma_e^2)$, with $\\sigma_s^2 \\ge 0$ and $\\sigma_e^2 \\ge 0$, and with $\\mu$ profiled out at its maximum likelihood estimate.\n- Use a numerically stable evaluation of the log-likelihood via Cholesky factorization of the covariance matrix $\\boldsymbol{\\Sigma} = \\sigma_s^2 \\mathbf{K} + \\sigma_e^2 \\mathbf{I}$.\n- Define $\\mathrm{PVE}$ as above, and round to $3$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the four $\\mathrm{PVE}$ values corresponding to Samples A, B, C, and D, in that order, rounded to $3$ decimal places, as a comma-separated list enclosed in square brackets (e.g., $[0.732,0.058,0.113,0.000]$).", "solution": "The problem is valid. It is scientifically grounded in the principles of spatial statistics and molecular biology, specifically using a Gaussian Process model, which is a standard method for analyzing spatially resolved data like that from spatial transcriptomics. The problem is well-posed, with all necessary data and a clear objective: to estimate variance components by maximizing a profiled log-likelihood. The formulation is objective and mathematically precise.\n\nThe solution proceeds by first deriving the profiled log-likelihood function for the parameter of interest, the Proportion of Variance Explained (PVE), and then maximizing it numerically for each data sample.\n\n**1. Model Specification and Likelihood**\n\nThe model for the observed log-normalized expression vector $\\mathbf{y} \\in \\mathbb{R}^n$ is given by:\n$$\n\\mathbf{y} = \\mu \\mathbf{1} + \\mathbf{s} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mu$ is a global mean, $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_s^2 \\mathbf{K})$ is a spatial effect, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$ is independent noise. The total data vector $\\mathbf{y}$ therefore follows a multivariate normal distribution:\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mu\\mathbf{1}, \\boldsymbol{\\Sigma})\n$$\nwhere the covariance matrix $\\boldsymbol{\\Sigma}$ is $\\boldsymbol{\\Sigma} = \\sigma_s^2 \\mathbf{K} + \\sigma_e^2 \\mathbf{I}$. The parameters to be estimated are the variances $\\sigma_s^2 \\ge 0$ and $\\sigma_e^2 \\ge 0$, while the mean $\\mu$ is a nuisance parameter.\n\nThe log-likelihood function for $\\mathbf{y}$, ignoring constant terms, is:\n$$\n\\mathcal{L}(\\mu, \\sigma_s^2, \\sigma_e^2) = -\\frac{1}{2} (\\mathbf{y} - \\mu \\mathbf{1})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mu \\mathbf{1}) - \\frac{1}{2} \\log \\det(\\boldsymbol{\\Sigma})\n$$\n\n**2. Parameterization and Profiling**\n\nTo simplify the optimization, we re-parameterize the covariance matrix in terms of the Proportion of Variance Explained (PVE), denoted by $\\alpha \\in [0, 1]$, and a total variance term. Let $\\alpha = \\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_e^2}$. We can define a total variance parameter, but for profiling it is more convenient to use $\\sigma_e^2$ and the ratio $v = \\sigma_s^2 / \\sigma_e^2$. An even more stable and direct approach is to parameterize directly by $\\alpha$ and a total scale factor. Let the total variance be $\\sigma_{tot}^2 = \\sigma_s^2 + \\sigma_e^2$. Then $\\sigma_s^2 = \\alpha \\sigma_{tot}^2$ and $\\sigma_e^2 = (1-\\alpha) \\sigma_{tot}^2$. The covariance matrix becomes:\n$$\n\\boldsymbol{\\Sigma}(\\alpha, \\sigma_{tot}^2) = \\sigma_{tot}^2 \\left( \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I} \\right) = \\sigma_{tot}^2 \\mathbf{V}(\\alpha)\n$$\nwhere $\\mathbf{V}(\\alpha) = \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I}$. The log-likelihood is then:\n$$\n\\mathcal{L}(\\mu, \\alpha, \\sigma_{tot}^2) = -\\frac{1}{2\\sigma_{tot}^2} (\\mathbf{y} - \\mu\\mathbf{1})^\\top \\mathbf{V}(\\alpha)^{-1} (\\mathbf{y} - \\mu\\mathbf{1}) - \\frac{n}{2} \\log(\\sigma_{tot}^2) - \\frac{1}{2} \\log\\det(\\mathbf{V}(\\alpha))\n$$\nWe maximize this likelihood with respect to the parameters. The problem asks to treat $\\mu$ as a nuisance parameter. We can \"profile it out\" by replacing it with its maximum likelihood estimate (MLE), which, for fixed $\\alpha$ and $\\sigma_{tot}^2$, is the Generalized Least Squares (GLS) estimate:\n$$\n\\hat{\\mu}(\\alpha) = \\frac{\\mathbf{1}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{y}}{\\mathbf{1}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{1}}\n$$\nWe can also profile out the total variance $\\sigma_{tot}^2$. Taking the derivative of $\\mathcal{L}$ with respect to $\\sigma_{tot}^2$ and setting it to zero yields its MLE:\n$$\n\\hat{\\sigma}_{tot}^2(\\alpha) = \\frac{1}{n} (\\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1})^\\top \\mathbf{V}(\\alpha)^{-1} (\\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1}) = \\frac{Q(\\alpha)}{n}\n$$\nwhere we define $Q(\\alpha)$ as the GLS residual sum of squares. Substituting $\\hat{\\mu}(\\alpha)$ and $\\hat{\\sigma}_{tot}^2(\\alpha)$ back into the likelihood function gives the profiled log-likelihood, which is a function of only $\\alpha$:\n$$\n\\mathcal{L}_{prof}(\\alpha) \\propto -\\frac{n}{2} \\log(Q(\\alpha)) - \\frac{1}{2} \\log\\det(\\mathbf{V}(\\alpha))\n$$\nMaximizing $\\mathcal{L}_{prof}(\\alpha)$ is equivalent to minimizing the negative of it (scaled by $2$):\n$$\nf(\\alpha) = n \\log(Q(\\alpha)) + \\log\\det(\\mathbf{V}(\\alpha))\n$$\nThe estimate for PVE is the value of $\\alpha \\in [0, 1]$ that minimizes this objective function.\n\n**3. Algorithmic and Numerical Strategy**\n\nThe overall algorithm for each sample is as follows:\n\n1.  **Special Case Handling**: As per the problem, if the sample variance of $\\mathbf{y}$ is less than $10^{-12}$, we set $\\mathrm{PVE} = 0.0$ and do not proceed with optimization. This handles cases with no variance to explain, such as Sample D.\n\n2.  **Kernel Pre-computation**: The spot coordinates $\\{\\mathbf{x}_i\\}_{i=1}^n$ are fixed for all samples.\n    a. We compute all pairwise Euclidean distances $\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2$ for $i < j$. For $n=9$ spots, this gives $9 \\times 8 / 2 = 36$ unique distances.\n    b. The length-scale $\\ell$ is determined as the median of these $36$ distances.\n    c. The kernel matrix $\\mathbf{K}$ with entries $K_{ij} = \\exp\\left(-\\frac{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2^2}{2 \\ell^2}\\right)$ is then computed.\n\n3.  **Numerical Optimization**: We find the optimal $\\alpha$ by minimizing the objective function $f(\\alpha)$ over the interval $[0, 1]$ using a numerical optimization routine.\n\n4.  **Stable Evaluation of the Objective Function**: For each evaluation of $f(\\alpha)$, we must compute $Q(\\alpha)$ and $\\log\\det(\\mathbf{V}(\\alpha))$ in a numerically stable way.\n    a. First, form the matrix $\\mathbf{V}(\\alpha) = \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I}$.\n    b. Perform a Cholesky decomposition $\\mathbf{V}(\\alpha) = \\mathbf{L}\\mathbf{L}^\\top$, where $\\mathbf{L}$ is a lower triangular matrix. This is a robust way to check for positive-definiteness and is numerically stable.\n    c. The log-determinant is computed as $\\log\\det(\\mathbf{V}(\\alpha)) = 2 \\sum_{i=1}^n \\log(L_{ii})$.\n    d. Terms involving $\\mathbf{V}(\\alpha)^{-1}$, such as $\\mathbf{V}(\\alpha)^{-1}\\mathbf{y}$, are computed by solving linear systems using $\\mathbf{L}$. For a vector $\\mathbf{v}$, we find $\\mathbf{z} = \\mathbf{V}(\\alpha)^{-1}\\mathbf{v}$ by first solving $\\mathbf{L}\\mathbf{w} = \\mathbf{v}$ for $\\mathbf{w}$ (forward substitution) and then $\\mathbf{L}^\\top\\mathbf{z} = \\mathbf{w}$ for $\\mathbf{z}$ (backward substitution). This avoids explicit matrix inversion.\n    e. With these stable components, we compute $\\hat{\\mu}(\\alpha)$, the residual vector $\\mathbf{r} = \\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1}$, and finally the quadratic form $Q(\\alpha) = \\mathbf{r}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{r}$. A stable way to compute $Q(\\alpha)$ is to solve $\\mathbf{L}\\mathbf{w}_r = \\mathbf{r}$ for $\\mathbf{w}_r$, and then $Q(\\alpha) = \\mathbf{w}_r^\\top \\mathbf{w}_r$.\n\n5.  **Final Result**: The minimizer of $f(\\alpha)$ is the estimated PVE, which is then rounded to $3$ decimal places as required. This procedure is applied to each of the four samples.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.optimize import minimize_scalar\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the spatial transcriptomics variance component estimation problem.\n    \"\"\"\n    # Define the fixed spatial locations for all samples\n    coords_list = [\n        (0.0, 0.0), (0.0, 50.0), (0.0, 100.0),\n        (50.0, 0.0), (50.0, 50.0), (50.0, 100.0),\n        (100.0, 0.0), (100.0, 50.0), (100.0, 100.0)\n    ]\n    coords = np.array(coords_list, dtype=np.float64)\n    n = coords.shape[0]\n\n    # Define the observed expression vectors for each sample\n    test_cases = [\n        np.array([4.2, 5.8, 4.1, 6.0, 10.5, 6.2, 3.9, 5.7, 4.0], dtype=np.float64), # Sample A\n        np.array([5.1, 5.0, 5.2, 4.9, 5.1, 5.0, 5.2, 5.0, 5.3], dtype=np.float64), # Sample B\n        np.array([7.4, 3.1, 5.6, 8.0, 2.7, 4.9, 6.2, 7.9, 3.5], dtype=np.float64), # Sample C\n        np.array([2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], dtype=np.float64)  # Sample D\n    ]\n\n    # --- Pre-computation of the spatial kernel ---\n    # 1. Compute length-scale 'l' as the median of pairwise distances\n    pairwise_distances = pdist(coords)\n    l_scale = np.median(pairwise_distances)\n\n    # 2. Compute the squared-exponential kernel matrix 'K'\n    sq_dist_matrix = squareform(pairwise_distances**2)\n    K = np.exp(-sq_dist_matrix / (2 * l_scale**2))\n    \n    # --- Negative Profiled Log-Likelihood Function Factory ---\n    def get_objective_function(y, K_matrix):\n        \"\"\"\n        Returns the objective function to be minimized.\n        The objective function is the negative profiled log-likelihood for the PVE parameter 'alpha'.\n        \"\"\"\n        n_spots = len(y)\n        ones_vec = np.ones(n_spots)\n\n        def objective(alpha):\n            \"\"\"\n            Computes the value of the objective function for a given alpha (PVE).\n            alpha is in [0, 1].\n            \"\"\"\n            # Define V(alpha) = alpha*K + (1-alpha)*I\n            V_matrix = alpha * K_matrix + (1 - alpha) * np.identity(n_spots)\n\n            try:\n                # Cholesky factorization for stability\n                L = cholesky(V_matrix, lower=True)\n            except np.linalg.LinAlgError:\n                # If matrix is not positive definite, return a large value\n                # to penalize this region of the parameter space.\n                return np.inf\n\n            # log determinant of V\n            log_det_V = 2 * np.sum(np.log(np.diag(L)))\n\n            # Solve for V_inv @ y and V_inv @ 1 using forward/backward substitution\n            y_tilde = solve_triangular(L, y, lower=True)\n            z_y = solve_triangular(L.T, y_tilde, lower=False)\n            \n            o_tilde = solve_triangular(L, ones_vec, lower=True)\n            z_1 = solve_triangular(L.T, o_tilde, lower=False)\n\n            # GLS estimate of the mean mu\n            mu_hat = np.sum(z_y) / np.sum(z_1)\n\n            # Quadratic form Q(alpha)\n            residual = y - mu_hat\n            w_r = solve_triangular(L, residual, lower=True)\n            Q = np.dot(w_r, w_r)\n\n            # Guard against log(0) for perfect fits\n            if Q < 1e-20:\n                return -np.inf\n\n            # Final objective value (from profiled log-likelihood)\n            return n_spots * np.log(Q) + log_det_V\n\n        return objective\n\n    # --- Process each sample ---\n    results = []\n    for y_vec in test_cases:\n        # Special case: if sample has negligible variance, PVE is defined as 0\n        if np.var(y_vec, ddof=0) < 1e-12:\n            pve = 0.0\n        else:\n            # Create the objective function for the current sample\n            objective_func = get_objective_function(y_vec, K)\n            \n            # Find the PVE (alpha) that minimizes the objective function\n            res = minimize_scalar(\n                objective_func,\n                bounds=(0, 1),\n                method='bounded'\n            )\n            pve = res.x\n            \n        results.append(pve)\n    \n    # Format results as specified\n    rounded_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(rounded_results)}]\")\n\nsolve()\n```", "id": "2890143"}, {"introduction": "Once we establish that gene expression exhibits significant spatial patterns, we can leverage this information to algorithmically identify and delineate functional biological structures within the tissue. This practice simulates a complete, real-world bioinformatics pipeline for identifying Tertiary Lymphoid Structures (TLS), which are defined by the co-expression of specific chemokine signatures. You will implement a workflow involving data normalization, $k$-means clustering, and spatial contiguity filtering to transform raw gene expression data into a map of predicted biological structures, and then validate its accuracy. [@problem_id:2890083]", "problem": "You are given synthetic spatial transcriptomics data representing inflamed lung tissue with potential Tertiary Lymphoid Structures (TLS). The goal is to algorithmically identify TLS by clustering three ligand/signature channels—CXCL13, CCL19, and Peripheral Node Addressin (PNAd)—and to validate the predicted TLS against a provided binary histology-derived TLS annotation. Tertiary Lymphoid Structures (TLS) are ectopic lymphoid aggregates typically enriched for the chemokines CXCL13 and CCL19 and characterized by specialized vasculature expressing Peripheral Node Addressin (PNAd).\n\nYour program must implement the following pipeline using only first principles and core definitions:\n\n- Start from the Central Dogma of Molecular Biology, where transcription produces messenger ribonucleic acid (mRNA) that is quantifiable as counts in spatial spots. Represent each spot by a three-dimensional feature vector of observed counts for the three channels.\n- Apply a monotonic variance-stabilizing transform to counts using the natural logarithm: for each observed count $x \\in \\mathbb{R}_{\\ge 0}$, transform via $x_{\\mathrm{log}} = \\log(1 + x)$.\n- Standardize each feature across spots using the $z$-score: for a feature $f$ with sample mean $\\mu_f$ and sample standard deviation $\\sigma_f$, each transformed value is $z = \\frac{x_{\\mathrm{log}} - \\mu_f}{\\sigma_f}$, with the convention that if $\\sigma_f = 0$, then all standardized values for that feature are set to $0$.\n- Cluster spots in the standardized three-dimensional space into $k = 2$ clusters using the $k$-means objective. The objective minimizes the within-cluster sum of squared Euclidean distances. Use Euclidean distance $d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2$, deterministic initialization by choosing as initial centroids the feature vectors of the spots with the minimum and maximum sum over features, and iterate until assignments stabilize. If an empty cluster occurs, reinitialize its centroid to the point farthest from the other cluster’s centroid.\n- Designate the “TLS cluster” as the cluster whose centroid has the larger average across the three standardized features. This encodes the biological expectation that TLS co-localize with jointly elevated CXCL13, CCL19, and PNAd signatures.\n- Enforce spatial contiguity by retaining only those predicted TLS spots that belong to connected components (using $4$-neighborhood adjacency: up, down, left, right) of size at least a specified threshold $\\tau$. Remove all smaller connected components of predicted TLS.\n- Validate the predicted TLS versus the histology TLS annotation using the $F_1$ score. Let true positives be $TP$, false positives be $FP$, and false negatives be $FN$. Compute precision $P = \\frac{TP}{TP + FP}$ (defined as $0$ if $TP + FP = 0$), recall $R = \\frac{TP}{TP + FN}$ (defined as $0$ if $TP + FN = 0$), and\n$$\nF_1 =\n\\begin{cases}\n1, & \\text{if the number of positives in both prediction and ground truth is } 0 \\\\\n\\frac{2 P R}{P + R}, & \\text{if } P + R > 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThis convention avoids undefined values when both prediction and ground truth have no positive spots.\n\nImplement the above pipeline and run it on the following three test cases. In each test case, you are provided with a grid of spots of size $5 \\times 5$. Each grid is given as three matrices of counts for the channels CXCL13, CCL19, and PNAd, and a binary histology mask where $1$ denotes a TLS label and $0$ denotes background. For adjacency, use $4$-neighborhood connectivity on this grid. You must not take any input; the program must embed these datasets.\n\nTest case $1$ (happy path: compact high-expression TLS):\n- Grid size: $5 \\times 5$.\n- CXCL13 counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 60 & 60 & 60 & 1 \\\\\n  1 & 60 & 60 & 60 & 1 \\\\\n  1 & 60 & 60 & 60 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- CCL19 counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 50 & 50 & 50 & 1 \\\\\n  1 & 50 & 50 & 50 & 1 \\\\\n  1 & 50 & 50 & 50 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- PNAd signature counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 40 & 40 & 40 & 1 \\\\\n  1 & 40 & 40 & 40 & 1 \\\\\n  1 & 40 & 40 & 40 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- Histology TLS mask:\n  $$\n  \\begin{bmatrix}\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 1 & 1 & 1 & 0 \\\\\n  0 & 1 & 1 & 1 & 0 \\\\\n  0 & 1 & 1 & 1 & 0 \\\\\n  0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}\n  $$\n- Contiguity threshold: $\\tau = 5$.\n\nTest case $2$ (edge case: scattered high expression without histologic TLS):\n- Grid size: $5 \\times 5$.\n- CXCL13 counts:\n  $$\n  \\begin{bmatrix}\n  60 & 1 & 1 & 1 & 60 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  60 & 1 & 1 & 1 & 60\n  \\end{bmatrix}\n  $$\n- CCL19 counts:\n  $$\n  \\begin{bmatrix}\n  50 & 1 & 1 & 1 & 50 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  50 & 1 & 1 & 1 & 50\n  \\end{bmatrix}\n  $$\n- PNAd signature counts:\n  $$\n  \\begin{bmatrix}\n  40 & 1 & 1 & 1 & 40 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  40 & 1 & 1 & 1 & 40\n  \\end{bmatrix}\n  $$\n- Histology TLS mask:\n  $$\n  \\begin{bmatrix}\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}\n  $$\n- Contiguity threshold: $\\tau = 3$.\n\nTest case $3$ (boundary condition: histologic TLS smaller than contiguity threshold):\n- Grid size: $5 \\times 5$.\n- CXCL13 counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 20 & 20 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- CCL19 counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 18 & 18 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- PNAd signature counts:\n  $$\n  \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 15 & 15 & 1 \\\\\n  1 & 1 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  $$\n- Histology TLS mask:\n  $$\n  \\begin{bmatrix}\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 1 & 1 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}\n  $$\n- Contiguity threshold: $\\tau = 3$.\n\nAngle units are not applicable. There are no physical units to report. Your program should produce a single line of output containing the three $F_1$ scores for the test cases as a comma-separated list enclosed in square brackets, with each value rounded to three decimal places (for example, $[0.997,0.000,1.000]$). The output must be exactly this single line, with no extra spaces.\n\nYour implementation must be self-contained, require no user input, and follow the rules above. The final output format is strictly one line: $[f_1,f_2,f_3]$ with each $f_i$ a decimal rounded to three places.", "solution": "The problem statement submitted for evaluation is deemed valid. It is scientifically grounded, well-posed, and objective. It presents a clearly defined algorithmic task based on established principles in molecular biology, statistics, and computer science. The data, constraints, and evaluation metric are specified with sufficient precision to permit a unique and verifiable solution. We shall now proceed with the formal derivation and implementation of the required pipeline.\n\nThe task is to identify Tertiary Lymphoid Structures (TLS) from synthetic spatial transcriptomics data. The solution is structured as a multi-step computational pipeline, which we will construct from first principles.\n\n**1. Data Representation and Pre-processing**\n\nThe Central Dogma of Molecular Biology posits that genetic information flows from DNA to RNA to protein. Spatial transcriptomics quantifies the messenger RNA (mRNA) products of transcription at spatially resolved locations, or \"spots.\" For each spot, located by grid coordinates $(i, j)$, the data provides counts of mRNA for three specific gene signatures: CXCL13, CCL19, and PNAd. Thus, each of the $N$ spots in the grid is represented by a 3-dimensional feature vector of non-negative integer counts, $\\mathbf{x} = [x_{CXCL13}, x_{CCL19}, x_{PNAd}]^T \\in \\mathbb{N}_0^3$.\n\nTranscriptomics count data are known to exhibit a large dynamic range and heteroscedasticity (variance dependent on the mean). A common and mandatory step is to apply a variance-stabilizing transformation. As per the problem specification, we use a logarithmic transform. For each count $x_k$, the transformed value $x_{k, \\mathrm{log}}$ is computed as:\n$$\nx_{k, \\mathrm{log}} = \\log(1 + x_k)\n$$\nThe addition of $1$ ensures that zero counts are mapped to $0$ and avoids the singularity of the logarithm at zero.\n\nFollowing transformation, each feature (gene signature) must be standardized to have a common scale for distance-based clustering. We employ the $z$-score transformation. For each feature $f \\in \\{CXCL13, CCL19, PNAd\\}$, we compute its sample mean $\\mu_f$ and sample standard deviation $\\sigma_f$ across all $N$ spots. Each log-transformed value $x_{f, \\mathrm{log}}$ is then standardized to $z_f$:\n$$\nz_f = \\frac{x_{f, \\mathrm{log}} - \\mu_f}{\\sigma_f}\n$$\nIf a feature has zero variance across all spots ($\\sigma_f = 0$), all its standardized values are defined to be $0$. The result is a set of $N$ standardized feature vectors $\\{\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_N\\}$, where each $\\mathbf{z}_i \\in \\mathbb{R}^3$.\n\n**2. K-means Clustering**\n\nThe core of the identification task is to partition the spots into two groups—potential TLS and background tissue—based on their gene signature profiles. We use $k$-means clustering with $k=2$. The objective is to find a partition $S = \\{S_1, S_2\\}$ of the $N$ spots that minimizes the within-cluster sum of squared Euclidean distances. Let $\\mathbf{c}_j$ be the centroid of cluster $S_j$. The objective function to minimize is:\n$$\n\\sum_{j=1}^{2} \\sum_{\\mathbf{z} \\in S_j} \\|\\mathbf{z} - \\mathbf{c}_j\\|_2^2\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm. The algorithm proceeds as follows:\n\n- **Initialization**: Centroids must be initialized deterministically. We compute the sum of components for each standardized feature vector $\\mathbf{z}_i$. The two spots whose feature vectors yield the minimum and maximum sum are chosen, and their feature vectors become the initial centroids, $\\mathbf{c}_1$ and $\\mathbf{c}_2$.\n- **Iteration**: The algorithm iterates between two steps until cluster assignments do not change.\n    1.  **Assignment Step**: Each spot's feature vector $\\mathbf{z}_i$ is assigned to the cluster $S_j$ corresponding to the closest centroid $\\mathbf{c}_j$. The assignment is based on minimizing the Euclidean distance:\n        $$\n        j^* = \\arg\\min_{j \\in \\{1, 2\\}} d(\\mathbf{z}_i, \\mathbf{c}_j) = \\arg\\min_{j \\in \\{1, 2\\}} \\|\\mathbf{z}_i - \\mathbf{c}_j\\|_2\n        $$\n    2.  **Update Step**: The centroid of each cluster is re-computed as the mean of all feature vectors assigned to it:\n        $$\n        \\mathbf{c}_j = \\frac{1}{|S_j|} \\sum_{\\mathbf{z} \\in S_j} \\mathbf{z}\n        $$\n- **Empty Cluster Handling**: If an update step results in an empty cluster $S_j$ (i.e., $|S_j| = 0$), its centroid $\\mathbf{c}_j$ is re-initialized. It is set to the feature vector $\\mathbf{z}_i$ that is farthest from the centroid of the other, non-empty cluster. This prevents the algorithm from collapsing to a single cluster.\n\n**3. TLS Cluster Designation**\n\nUpon convergence, we have two clusters. Biologically, TLS are characterized by a joint up-regulation of CXCL13, CCL19, and PNAd. In the standardized feature space, this corresponds to a cluster whose centroid has a higher average value across its three components. Let the final centroids be $\\mathbf{c}_1 = [c_{11}, c_{12}, c_{13}]^T$ and $\\mathbf{c}_2 = [c_{21}, c_{22}, c_{23}]^T$. We designate the \"TLS cluster\" as the cluster $j^*$ for which the mean of its centroid's components is maximal:\n$$\nj^* = \\arg\\max_{j \\in \\{1, 2\\}} \\left( \\frac{1}{3} \\sum_{l=1}^{3} c_{jl} \\right)\n$$\nAll spots assigned to cluster $S_{j^*}$ are given a preliminary prediction label of $1$ (TLS), and all others are labeled $0$ (non-TLS). This yields a binary prediction mask.\n\n**4. Spatial Contiguity Filtering**\n\nTLS are organized structures, not scattered individual cells. Therefore, a valid prediction must enforce spatial contiguity. We apply a post-processing filter to the binary prediction mask. We identify connected components of spots labeled as $1$. Connectivity is defined by a $4$-neighborhood on the grid: two spots are adjacent if they share a side (up, down, left, or right). This can be implemented with a graph traversal algorithm such as Breadth-First Search (BFS) or Depth-First Search (DFS).\nFor each connected component of predicted TLS spots, we count its size (number of spots). Any component whose size is less than a given threshold $\\tau$ is considered biologically insignificant noise or artifact. All spots within such small components have their labels reverted from $1$ to $0$. This filtering step refines the prediction by retaining only spatially coherent structures of a minimum size.\n\n**5. Performance Validation**\n\nThe final step is to quantitatively validate the filtered prediction mask against the ground truth histology annotation. We use the $F_1$ score, which is the harmonic mean of precision and recall. First, we compute the number of True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$):\n- $TP$: Number of spots correctly predicted as TLS (prediction=$1$, truth=$1$).\n- $FP$: Number of spots incorrectly predicted as TLS (prediction=$1$, truth=$0$).\n- $FN$: Number of spots incorrectly predicted as non-TLS (prediction=$0$, truth=$1$).\n\nPrecision ($P$) and Recall ($R$) are then calculated:\n$$\nP = \\frac{TP}{TP + FP} \\quad \\text{(defined as 0 if } TP + FP = 0)\n$$\n$$\nR = \\frac{TP}{TP + FN} \\quad \\text{(defined as 0 if } TP + FN = 0)\n$$\nThe $F_1$ score is computed according to the specific rules provided:\n$$\nF_1 =\n\\begin{cases}\n1, & \\text{if } TP+FP=0 \\text{ and } TP+FN=0 \\\\\n\\frac{2 P R}{P + R}, & \\text{if } P + R > 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThe first case correctly handles the scenario where both the prediction and the ground truth contain no positive instances, which should be considered a perfect match ($F_1 = 1$). This entire pipeline is then applied to each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full TLS identification and validation pipeline.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"cxcl13\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 60, 60, 60, 1],\n                [1, 60, 60, 60, 1],\n                [1, 60, 60, 60, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"ccl19\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 50, 50, 50, 1],\n                [1, 50, 50, 50, 1],\n                [1, 50, 50, 50, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"pnad\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 40, 40, 40, 1],\n                [1, 40, 40, 40, 1],\n                [1, 40, 40, 40, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 1, 1, 1, 0],\n                [0, 1, 1, 1, 0],\n                [0, 1, 1, 1, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 5\n        },\n        # Test case 2\n        {\n            \"cxcl13\": np.array([\n                [60, 1, 1, 1, 60],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [60, 1, 1, 1, 60]\n            ]),\n            \"ccl19\": np.array([\n                [50, 1, 1, 1, 50],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [50, 1, 1, 1, 50]\n            ]),\n            \"pnad\": np.array([\n                [40, 1, 1, 1, 40],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [40, 1, 1, 1, 40]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 3\n        },\n        # Test case 3\n        {\n            \"cxcl13\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 20, 20, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"ccl19\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 18, 18, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"pnad\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 15, 15, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 3\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        cxcl13, ccl19, pnad, histology, tau = case[\"cxcl13\"], case[\"ccl19\"], case[\"pnad\"], case[\"histology\"], case[\"tau\"]\n        \n        # Combine channels into a feature matrix\n        shape = cxcl13.shape\n        n_spots = shape[0] * shape[1]\n        features = np.stack([cxcl13.flatten(), ccl19.flatten(), pnad.flatten()], axis=1)\n\n        # 1. Log transform\n        log_features = np.log1p(features)\n        \n        # 2. Z-score standardization\n        mean = np.mean(log_features, axis=0)\n        std = np.std(log_features, axis=0)\n        \n        # Handle std == 0 case\n        z_scores = np.zeros_like(log_features)\n        non_zero_std_mask = std != 0\n        if np.any(non_zero_std_mask):\n            z_scores[:, non_zero_std_mask] = (log_features[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n\n        # 3. K-means clustering (k=2)\n        # Initialization\n        feature_sums = np.sum(z_scores, axis=1)\n        min_idx, max_idx = np.argmin(feature_sums), np.argmax(feature_sums)\n        centroids = np.array([z_scores[min_idx], z_scores[max_idx]])\n        \n        assignments = np.zeros(n_spots, dtype=int)\n        \n        while True:\n            old_assignments = assignments.copy()\n            \n            # Assignment step\n            distances = np.sqrt(((z_scores[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2).sum(axis=2))\n            assignments = np.argmin(distances, axis=1)\n            \n            # Update step\n            new_centroids = np.zeros_like(centroids)\n            for i in range(2):\n                points_in_cluster = z_scores[assignments == i]\n                if len(points_in_cluster) == 0:\n                    # Empty cluster handling\n                    other_centroid = centroids[1 - i]\n                    dists_to_other = np.linalg.norm(z_scores - other_centroid, axis=1)\n                    farthest_point_idx = np.argmax(dists_to_other)\n                    new_centroids[i] = z_scores[farthest_point_idx]\n                else:\n                    new_centroids[i] = np.mean(points_in_cluster, axis=0)\n            \n            centroids = new_centroids\n            \n            if np.array_equal(assignments, old_assignments):\n                break\n\n        # 4. Designate TLS cluster\n        centroid_means = np.mean(centroids, axis=1)\n        tls_cluster_label = np.argmax(centroid_means)\n        \n        prediction_mask_flat = (assignments == tls_cluster_label).astype(int)\n        prediction_mask = prediction_mask_flat.reshape(shape)\n\n        # 5. Spatial contiguity filtering\n        visited = np.zeros(shape, dtype=bool)\n        filtered_prediction_mask = np.zeros(shape, dtype=int)\n        \n        for r in range(shape[0]):\n            for c in range(shape[1]):\n                if prediction_mask[r, c] == 1 and not visited[r, c]:\n                    component = []\n                    q = [(r, c)]\n                    visited[r, c] = True\n                    \n                    while q:\n                        curr_r, curr_c = q.pop(0)\n                        component.append((curr_r, curr_c))\n                        \n                        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                            nr, nc = curr_r + dr, curr_c + dc\n                            if 0 <= nr < shape[0] and 0 <= nc < shape[1] and \\\n                               prediction_mask[nr, nc] == 1 and not visited[nr, nc]:\n                                visited[nr, nc] = True\n                                q.append((nr, nc))\n                    \n                    if len(component) >= tau:\n                        for comp_r, comp_c in component:\n                            filtered_prediction_mask[comp_r, comp_c] = 1\n\n        # 6. Validation with F1 score\n        pred_flat = filtered_prediction_mask.flatten()\n        truth_flat = histology.flatten()\n        \n        tp = np.sum((pred_flat == 1) & (truth_flat == 1))\n        fp = np.sum((pred_flat == 1) & (truth_flat == 0))\n        fn = np.sum((pred_flat == 0) & (truth_flat == 1))\n        \n        num_pos_pred = tp + fp\n        num_pos_true = tp + fn\n        \n        if num_pos_pred == 0 and num_pos_true == 0:\n            f1 = 1.0\n        else:\n            precision = tp / num_pos_pred if num_pos_pred > 0 else 0.0\n            recall = tp / num_pos_true if num_pos_true > 0 else 0.0\n            \n            if precision + recall > 0:\n                f1 = 2 * (precision * recall) / (precision + recall)\n            else:\n                f1 = 0.0\n        \n        results.append(f\"{f1:.3f}\")\n\n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2890083"}, {"introduction": "Biological tissues are complex systems where multiple spatial processes often overlap, creating statistical challenges. For example, is a gene signature elevated in a specific microenvironment because of the environment itself, or because of its proximity to another feature like the vasculature? This advanced exercise tackles this challenge of confounding by guiding you to build a multiple regression model that tests a biological hypothesis—the localization of a hypoxia signature to germinal centers—while mathematically adjusting for the influence of a known confounding spatial process, the distance to blood vessels. [@problem_id:2890183]", "problem": "You are given a conceptual pipeline to assess whether a hypoxia-related gene signature remains localized to germinal centers (GCs) in a lymphoid tissue after adjusting for vasculature, where vasculature is marked by Immunofluorescence (IF) for CD31 and the gene signature is measured by spatial transcriptomics (ST). You must formalize this into a reproducible algorithm and implement it as a program.\n\nAssumptions and fundamental base:\n- In tissues, oxygen delivery is constrained by distance to blood vessels, so hypoxia increases with increasing distance to vessels. This is a well-accepted observation in physiology and tumor immunology.\n- Spatial transcriptomics measures gene expression at spatially indexed positions (spots). A hypoxia signature score is a continuous variable per spot.\n- We use a generative model to simulate a tissue section and then apply a statistical test derived from ordinary least squares (OLS), grounded in the definition that OLS minimizes the sum of squared residuals.\n- We treat the vasculature map as a binary mask derived from Immunofluorescence (IF) for CD31 and use the Euclidean distance from each spot to the nearest vessel as a covariate to adjust for oxygen supply.\n\nMathematical formulation of the generative process:\n- Consider a square grid of size $N \\times N$ representing ST spots at coordinates $(x_i,y_i)$ normalized to $[0,1] \\times [0,1]$. Let $i \\in \\{1,\\dots,n\\}$ index spots with $n = N^2$.\n- Define a binary indicator $G_i \\in \\{0,1\\}$ for spot $i$ lying inside a germinal center (GC), where GCs are modeled as circles with fixed centers and radius proportional to $N$.\n- Let $V$ be a binary mask where $V_{pq} = 1$ if grid location $(p,q)$ is on a vessel (positive IF for CD31) and $V_{pq}=0$ otherwise. Define $D_i \\ge 0$ as the Euclidean distance in pixel units from spot $i$ to the nearest vessel.\n- The hypoxia score at spot $i$ is simulated by\n$$\nH_i \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{GC}} \\, G_i \\;+\\; \\gamma \\, \\bigl(1 - e^{-D_i/\\lambda}\\bigr) \\;+\\; \\theta_1 x_i \\;+\\; \\theta_2 y_i \\;+\\; \\theta_3 x_i y_i \\;+\\; \\varepsilon_i,\n$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent errors, $\\gamma \\ge 0$ controls vessel-distance induced hypoxia, and $\\lambda > 0$ is a length scale in pixel units.\n\nAnalysis model to adjust for vasculature and spatial trend:\n- Fit an OLS regression that adjusts for distance to vessels and a low-order spatial polynomial:\n$$\nH_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_{\\text{GC}} \\, G_i \\;+\\; \\alpha_1 D_i \\;+\\; \\alpha_2 D_i^2 \\;+\\; \\alpha_3 x_i \\;+\\; \\alpha_4 y_i \\;+\\; \\alpha_5 x_i^2 \\;+\\; \\alpha_6 y_i^2 \\;+\\; \\alpha_7 x_i y_i \\;+\\; \\eta_i,\n$$\nwith $\\eta_i$ mean-zero errors. This model expresses adjustment for vasculature through $D_i$ and $D_i^2$ and removes broad spatial trends through polynomial terms in $x_i$ and $y_i$.\n- Let $\\widehat{\\alpha}_{\\text{GC}}$ be the OLS estimate of $\\alpha_{\\text{GC}}$. Under standard OLS assumptions, the $t$-statistic for testing the one-sided hypothesis $H_0: \\alpha_{\\text{GC}} \\le 0$ versus $H_1: \\alpha_{\\text{GC}} > 0$ is\n$$\nt \\;=\\; \\frac{\\widehat{\\alpha}_{\\text{GC}}}{\\widehat{\\mathrm{SE}}(\\widehat{\\alpha}_{\\text{GC}})},\n$$\nwhere $\\widehat{\\mathrm{SE}}(\\widehat{\\alpha}_{\\text{GC}})$ is obtained from the estimated residual variance and the inverse of the Gram matrix. The one-sided $p$-value is $p = 1 - F_{t,\\nu}(t)$ for $t \\ge 0$, and $p = 1$ for $t < 0$, where $F_{t,\\nu}$ is the cumulative distribution function of the Student distribution with $\\nu = n - p$ degrees of freedom and $p$ the number of fitted parameters.\n\nYour task:\n- Implement a simulator and analysis as above. For each test case, construct the grid, create the GC mask and the vessel mask, compute $D_i$, simulate $H_i$ using the generative model with specified parameters, fit the analysis model, compute the one-sided $p$-value for $H_0: \\alpha_{\\text{GC}} \\le 0$, and finally return a boolean indicating whether the GC effect remains localized (reject $H_0$ at significance level $\\alpha = 0.01$).\n- The program must use the following fixed GC geometry for all test cases: two circular GCs with centers at $(0.35,0.35)$ and $(0.68,0.62)$ in normalized coordinates, and radius $r = \\lceil r_{\\text{frac}} \\cdot N \\rceil$ for a specified fraction $r_{\\text{frac}}$. A spot is inside a GC if its center falls within one or both circles.\n- The vessel mask is constructed as vertical vessel lines: for integer column index $q \\in \\{0,\\dots,N-1\\}$, set $V_{pq}=1$ for all $p$ if $q \\bmod s = \\lfloor s/2 \\rfloor$, where $s$ is the specified vessel spacing in pixels. All other entries are $0$.\n\nFixed constants for the generative model across all test cases:\n- Use $\\beta_0 = 0$.\n- Use $(\\theta_1,\\theta_2,\\theta_3) = (0.5,-0.3,0.2)$.\n\nStatistical decision:\n- Use significance level $\\alpha = 0.01$ for all test cases.\n- Report a boolean for each test case: $true$ if $p < \\alpha$, otherwise $false$.\n\nTest suite:\nProvide results for the following parameter sets. Each test case is a tuple $(N, r_{\\text{frac}}, s, \\beta_{\\text{GC}}, \\gamma, \\lambda, \\sigma, \\text{seed})$:\n- Case $1$: $(35, \\; 0.14, \\; 6, \\; 2.0, \\; 1.5, \\; 4.0, \\; 0.4, \\; 12345)$.\n- Case $2$: $(35, \\; 0.14, \\; 6, \\; 0.0, \\; 1.5, \\; 4.0, \\; 0.4, \\; 23456)$.\n- Case $3$: $(35, \\; 0.14, \\; 4, \\; 1.5, \\; 3.0, \\; 2.0, \\; 0.5, \\; 34567)$.\n- Case $4$: $(25, \\; 0.16, \\; 5, \\; 0.05, \\; 1.5, \\; 3.0, \\; 2.0, \\; 45678)$.\n- Case $5$: $(35, \\; 0.14, \\; 6, \\; -0.8, \\; 1.5, \\; 4.0, \\; 0.4, \\; 56789)$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false,true]\"). The list must contain $5$ boolean values corresponding to the five cases in the order listed above.\n- No physical units or angle units are involved. Do not print any additional text.", "solution": "We formalize the biological question into a statistical hypothesis test grounded in first principles. From physiology, oxygen tension decreases with distance from vessels; thus, hypoxia gene expression increases with increasing distance to vasculature. Spatial transcriptomics (ST) provides a hypoxia signature score per spot, and Immunofluorescence (IF) for CD31 yields a binary vessel mask. Germinal centers (GCs) are hypothesized to be hypoxic microenvironments because of dense cellularity and high metabolic demand; we want to know whether hypoxia signatures are localized to GCs independent of vasculature.\n\nThe simulation is defined on a square grid of size $N \\times N$. Each spot $i$ has normalized coordinates $(x_i,y_i) \\in [0,1]^2$. Two circular germinal centers (GCs) are defined by centers at $(0.35,0.35)$ and $(0.68,0.62)$ and a radius $r = \\lceil r_{\\text{frac}} N \\rceil$ in pixel units. The GC indicator $G_i$ equals $1$ if a spot lies within at least one GC, and $0$ otherwise.\n\nThe vasculature mask is constructed as vertical lines: take a spacing parameter $s$ in pixels and set all positions in columns with index $q$ satisfying $q \\bmod s = \\lfloor s/2 \\rfloor$ to be vessels ($V_{pq}=1$), and others to be non-vessels ($V_{pq}=0$). The distance $D_i$ is the Euclidean distance from spot $i$ to the nearest vessel, computed via the Euclidean distance transform. This is well-defined on a discrete grid and approximates the physical distance in the section.\n\nThe generative model for the hypoxia score is:\n$$\nH_i \\;=\\; \\beta_0 \\;+\\; \\beta_{\\text{GC}} \\, G_i \\;+\\; \\gamma \\, \\bigl(1 - e^{-D_i/\\lambda}\\bigr) \\;+\\; \\theta_1 x_i \\;+\\; \\theta_2 y_i \\;+\\; \\theta_3 x_i y_i \\;+\\; \\varepsilon_i,\n$$\nwith $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent noise, $\\beta_0 = 0$, $(\\theta_1,\\theta_2,\\theta_3) = (0.5, -0.3, 0.2)$, and $(\\beta_{\\text{GC}},\\gamma,\\lambda,\\sigma)$ as specified per test case. The term $\\gamma(1 - e^{-D_i/\\lambda})$ captures the monotonic increase of hypoxia with distance to vessels and is a simple, biologically plausible diffusion-reaction inspired transform. The spatial polynomial terms remove coarse spatial trends often observed in tissue sections due to batch or anatomical gradients.\n\nWe analyze by adjusting for vasculature and spatial trends using an ordinary least squares (OLS) regression:\n$$\nH_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_{\\text{GC}} \\, G_i \\;+\\; \\alpha_1 D_i \\;+\\; \\alpha_2 D_i^2 \\;+\\; \\alpha_3 x_i \\;+\\; \\alpha_4 y_i \\;+\\; \\alpha_5 x_i^2 \\;+\\; \\alpha_6 y_i^2 \\;+\\; \\alpha_7 x_i y_i \\;+\\; \\eta_i.\n$$\nOLS is obtained from the core definition of least squares: choose parameters to minimize $\\sum_{i=1}^{n} (H_i - \\widehat{H}_i)^2$, which yields the normal equations $X^\\top X \\widehat{\\alpha} = X^\\top H$ with solution $\\widehat{\\alpha} = (X^\\top X)^{-1} X^\\top H$ when $X^\\top X$ is invertible. Here $X$ is the design matrix with columns corresponding to intercept, $G_i$, $D_i$, $D_i^2$, $x_i$, $y_i$, $x_i^2$, $y_i^2$, $x_i y_i$.\n\nWe quantify localization after adjustment by testing the one-sided hypothesis $H_0: \\alpha_{\\text{GC}} \\le 0$ versus $H_1: \\alpha_{\\text{GC}} > 0$. Under standard OLS assumptions, the sampling distribution of the $t$-statistic\n$$\nt \\;=\\; \\frac{\\widehat{\\alpha}_{\\text{GC}}}{\\widehat{\\mathrm{SE}}(\\widehat{\\alpha}_{\\text{GC}})}\n$$\nfollows a Student distribution with $\\nu = n - p$ degrees of freedom, where $p$ is the number of fitted parameters. The standard error is derived from the estimated residual variance $\\widehat{\\sigma}^2 = \\frac{1}{\\nu}\\sum_{i=1}^{n} \\widehat{\\eta}_i^2$ and the $(2,2)$ entry of $(X^\\top X)^{-1}$ corresponding to the $G_i$ coefficient:\n$$\n\\widehat{\\mathrm{SE}}(\\widehat{\\alpha}_{\\text{GC}}) \\;=\\; \\sqrt{\\widehat{\\sigma}^2 \\,\\left[(X^\\top X)^{-1}\\right]_{(G,G)}}.\n$$\nThe one-sided $p$-value is computed as $p = 1 - F_{t,\\nu}(t)$ if $t \\ge 0$, and $p=1$ otherwise.\n\nDecision rule: with significance $\\alpha = 0.01$, we reject $H_0$ (and declare that hypoxia remains localized to GCs after adjustment) if $p < \\alpha$. This encodes the immunological hypothesis that GCs exhibit elevated hypoxia signatures beyond what is explained by proximity to vasculature and broad spatial trends.\n\nAlgorithmic steps for each test case:\n1. Build the $N \\times N$ grid and normalized coordinate arrays $(x_i,y_i)$.\n2. Construct the GC mask using the two fixed centers and radius $r = \\lceil r_{\\text{frac}} N \\rceil$; assign $G_i$ accordingly.\n3. Construct the vessel mask using spacing $s$; compute the Euclidean distance transform to obtain $D_i$.\n4. Generate $H_i$ from the generative model using the specified parameters and a fixed random seed for reproducibility.\n5. Build design matrix $X$ and response vector $H$; fit OLS by solving the normal equations, compute residual variance, standard error, the $t$-statistic, and the one-sided $p$-value.\n6. Return $true$ if $p < 0.01$, else $false$.\n\nTest coverage rationale:\n- Case $1$ (happy path): strong positive GC effect with moderate vessel effect and low noise should yield rejection ($true$).\n- Case $2$ (null): zero GC effect should not be significant ($false$).\n- Case $3$ (potential confounding): strong vessel effect and denser vessels increase confounding, but a strong positive GC effect should remain significant after adjustment ($true$).\n- Case $4$ (low signal, high noise, smaller grid): very small positive GC effect and high noise should not be significant ($false$).\n- Case $5$ (negative effect): one-sided test for positivity should not reject ($false$).\n\nThe program must output a single line: a list of five booleans in the order of the cases.", "answer": "```python\nimport numpy as np\nfrom scipy import ndimage\nfrom scipy.stats import t as student_t\n\ndef make_grid(N):\n    # Normalized coordinates in [0,1]\n    y_idx, x_idx = np.indices((N, N))\n    x = (x_idx + 0.5) / N\n    y = (y_idx + 0.5) / N\n    return x, y\n\ndef make_gc_mask(N, r_frac):\n    # Two circular GCs with fixed centers in normalized coordinates\n    centers = [(0.35, 0.35), (0.68, 0.62)]\n    x, y = make_grid(N)\n    gc_mask = np.zeros((N, N), dtype=bool)\n    r = int(np.ceil(r_frac * N))\n    r2 = (r / N) ** 2  # radius in normalized coordinate squared\n    for cx, cy in centers:\n        dist2 = (x - cx) ** 2 + (y - cy) ** 2\n        gc_mask |= dist2 <= r2\n    return gc_mask\n\ndef make_vessel_mask(N, spacing):\n    # Vertical vessel lines: columns where q % spacing == spacing//2\n    mask = np.zeros((N, N), dtype=bool)\n    col_indices = np.arange(N)\n    vessel_cols = (col_indices % spacing) == (spacing // 2)\n    mask[:, vessel_cols] = True\n    return mask\n\ndef distance_to_vessels(vessel_mask):\n    # Euclidean distance transform: distance from False to nearest True\n    # For spots on a vessel, distance is zero.\n    inv_mask = ~vessel_mask\n    dist = ndimage.distance_transform_edt(inv_mask)\n    return dist\n\ndef simulate_hypoxia(N, r_frac, spacing, beta_gc, gamma, lam, sigma, seed):\n    # Build masks and covariates\n    x, y = make_grid(N)\n    gc_mask = make_gc_mask(N, r_frac)\n    vessel_mask = make_vessel_mask(N, spacing)\n    D = distance_to_vessels(vessel_mask)\n\n    # Flatten all arrays\n    x_f = x.ravel()\n    y_f = y.ravel()\n    G_f = gc_mask.ravel().astype(float)\n    D_f = D.ravel().astype(float)\n\n    # Generative parameters\n    beta0 = 0.0\n    theta1, theta2, theta3 = 0.5, -0.3, 0.2\n\n    # Random noise\n    rng = np.random.default_rng(seed)\n    eps = rng.normal(loc=0.0, scale=sigma, size=N*N)\n\n    # Hypoxia signal\n    # Use transform (1 - exp(-D/lam)), with lam in pixel units (same as D)\n    H = (beta0\n         + beta_gc * G_f\n         + gamma * (1.0 - np.exp(-D_f / lam))\n         + theta1 * x_f\n         + theta2 * y_f\n         + theta3 * x_f * y_f\n         + eps)\n    return H, G_f, D_f, x_f, y_f\n\ndef ols_t_test_gc(H, G, D, x, y):\n    n = H.shape[0]\n    # Design matrix columns:\n    # 0: intercept\n    # 1: GC indicator\n    # 2: D\n    # 3: D^2\n    # 4: x\n    # 5: y\n    # 6: x^2\n    # 7: y^2\n    # 8: x*y\n    X = np.column_stack([\n        np.ones(n),\n        G,\n        D,\n        D**2,\n        x,\n        y,\n        x**2,\n        y**2,\n        x * y\n    ])\n    p = X.shape[1]\n    # OLS via normal equations\n    XTX = X.T @ X\n    # Use pseudo-inverse for numerical stability\n    XTX_inv = np.linalg.pinv(XTX, rcond=1e-12)\n    beta_hat = XTX_inv @ (X.T @ H)\n    resid = H - X @ beta_hat\n    df = n - p\n    rss = float(resid.T @ resid)\n    s2 = rss / df\n    cov_hat = s2 * XTX_inv\n    # Index of GC coefficient is 1\n    se_gc = np.sqrt(max(cov_hat[1, 1], 0.0))\n    beta_gc_hat = beta_hat[1]\n    # Guard against zero standard error (degenerate)\n    if se_gc == 0.0:\n        # If no variability, set t to 0 (no evidence)\n        t_stat = 0.0\n    else:\n        t_stat = beta_gc_hat / se_gc\n    # One-sided p-value for H1: alpha_GC > 0\n    if t_stat <= 0:\n        pval = 1.0\n    else:\n        pval = 1.0 - student_t.cdf(t_stat, df)\n    return pval\n\ndef analyze_case(params):\n    N, r_frac, spacing, beta_gc, gamma, lam, sigma, seed = params\n    H, G, D, x, y = simulate_hypoxia(N, r_frac, spacing, beta_gc, gamma, lam, sigma, seed)\n    pval = ols_t_test_gc(H, G, D, x, y)\n    # Decision at alpha = 0.01\n    return pval < 0.01\n\ndef solve():\n    # Define the test cases as specified\n    test_cases = [\n        (35, 0.14, 6, 2.0, 1.5, 4.0, 0.4, 12345),\n        (35, 0.14, 6, 0.0, 1.5, 4.0, 0.4, 23456),\n        (35, 0.14, 4, 1.5, 3.0, 2.0, 0.5, 34567),\n        (25, 0.16, 5, 0.05, 1.5, 3.0, 2.0, 45678),\n        (35, 0.14, 6, -0.8, 1.5, 4.0, 0.4, 56789),\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = analyze_case(case)\n        results.append(decision)\n\n    # Print in the exact required format: lowercase booleans\n    # Convert Python True/False to 'true'/'false'\n    out = \"[\" + \",\".join(\"true\" if r else \"false\" for r in results) + \"]\"\n    print(out)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2890183"}]}