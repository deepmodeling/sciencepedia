{"hands_on_practices": [{"introduction": "A foundational tool for understanding how temperature drives phenology is the thermal time, or Growing Degree Day (GDD), model. This simple mechanistic framework posits that a biological event occurs only after a certain amount of thermal energy has accumulated above a base temperature. This exercise will not only familiarize you with calculating onset dates using this model but, more importantly, will guide you through a first-principles derivation of the event's sensitivity to warming, a critical skill for any climate change impact assessment [@problem_id:2595740].", "problem": "A threshold-based thermal time model is widely used to represent the timing of phenological events in plants and animals that respond to temperature. In this model, the event (e.g., first flowering or first migration arrival) occurs on the earliest day index $D^{\\ast}$ such that the cumulative growing degree days equals a species-specific requirement $H$. Daily growing degree days are computed as the positive part above a base temperature $T_{b}$:\n$$\nG(T) = \\max\\{T - T_{b}, 0\\},\n$$\nand the cumulative thermal time up to day $d$ is\n$$\nC(d) = \\sum_{i=1}^{d} G(T_i).\n$$\nThe onset day $D^{\\ast}$ is the smallest $d$ such that $C(d) \\ge H$. Consider a single site with base temperature $T_{b} = 5\\,^{\\circ}\\mathrm{C}$ and requirement $H = 100$ degree-days above base. Starting at day-of-year $\\mathrm{DOY} = 80$, the historical daily mean temperatures (in degrees Celsius) are given for $i = 1,\\dots,22$ by\n$$\n[T_1,\\dots,T_{22}] = [4.2,\\, 6.1,\\, 7.3,\\, 5.2,\\, 8.0,\\, 9.4,\\, 10.1,\\, 12.3,\\, 8.7,\\, 6.4,\\, 7.2,\\, 11.5,\\, 13.6,\\, 14.2,\\, 10.8,\\, 9.1,\\, 12.7,\\, 13.4,\\, 15.0,\\, 16.3,\\, 17.1,\\, 16.8].\n$$\nAssume that a uniform additive warming of magnitude $\\Delta T$ (in $^{\\circ}\\mathrm{C}$) shifts all daily means up by the same amount on every day prior to the event, leaving $H$ and $T_b$ unchanged.\n\nUsing only the definitions above and first principles of differentiation, perform the following:\n\n- Derive the first-order change in the onset index with respect to uniform warming, $\\frac{\\partial D^{\\ast}}{\\partial \\Delta T}$, evaluated at $\\Delta T = 0^{+}$, by differentiating the onset condition implicitly with respect to $\\Delta T$ and treating $d$ as continuously relaxable near $D^{\\ast}$. Clearly state any regularity assumptions needed for the derivative.\n\n- Compute the numerical value of the change in onset date per $+1\\,^{\\circ}\\mathrm{C}$ uniform warming using the data provided. Express your final answer in days per degree Celsius and round to four significant figures.\n\n- Briefly interpret the corresponding elasticity of timing with respect to uniform warming defined by\n$$\n\\mathcal{E} \\equiv \\left.\\frac{\\partial D^{\\ast}}{\\partial \\Delta T}\\right|_{\\Delta T=0^{+}} \\cdot \\frac{\\Delta T}{D^{\\ast}},\n$$\nevaluated at $\\Delta T = 1\\,^{\\circ}\\mathrm{C}$, in terms of the fraction of days contributing to thermal accumulation and the marginal degree-day accumulation on the onset day.\n\nYour final reported value must be the single numerical value of the change in onset date per $+1\\,^{\\circ}\\mathrm{C}$ in units of days per degree Celsius, rounded to four significant figures.", "solution": "The problem requires the derivation and calculation of the first-order change in a phenological onset day index, $D^{\\ast}$, with respect to a uniform warming, $\\Delta T$. This is the quantity $\\frac{\\partial D^{\\ast}}{\\partial \\Delta T}$.\n\nFirst, we must determine the baseline onset day, denoted $D^{\\ast}_0$, under the historical temperature regime where $\\Delta T = 0$. The model for daily growing degree days (GDD) is $G(T) = \\max\\{T - T_{b}, 0\\}$, with a base temperature of $T_{b} = 5\\,^{\\circ}\\mathrm{C}$. The onset event occurs on the first day index $D^{\\ast}$ for which the cumulative GDD, $C(d) = \\sum_{i=1}^{d} G(T_i)$, reaches or exceeds the required thermal sum $H = 100$ degree-days.\n\nWe compute the daily GDD, $G(T_i)$, for each day $i$ using the provided temperature data, and then compute the cumulative sum $C(d)$.\nFor $i=1$, $T_1=4.2$, so $G(T_1) = \\max\\{4.2 - 5, 0\\} = 0$. $C(1)=0$.\nFor $i=2$, $T_2=6.1$, so $G(T_2) = \\max\\{6.1 - 5, 0\\} = 1.1$. $C(2)=1.1$.\nThis process is repeated for all subsequent days:\n$C(3) = C(2) + \\max\\{7.3 - 5, 0\\} = 1.1 + 2.3 = 3.4$.\n$C(4) = C(3) + \\max\\{5.2 - 5, 0\\} = 3.4 + 0.2 = 3.6$.\n$C(5) = 3.6 + 3.0 = 6.6$.\n$C(6) = 6.6 + 4.4 = 11.0$.\n$C(7) = 11.0 + 5.1 = 16.1$.\n$C(8) = 16.1 + 7.3 = 23.4$.\n$C(9) = 23.4 + 3.7 = 27.1$.\n$C(10) = 27.1 + 1.4 = 28.5$.\n$C(11) = 28.5 + 2.2 = 30.7$.\n$C(12) = 30.7 + 6.5 = 37.2$.\n$C(13) = 37.2 + 8.6 = 45.8$.\n$C(14) = 45.8 + 9.2 = 55.0$.\n$C(15) = 55.0 + 5.8 = 60.8$.\n$C(16) = 60.8 + 4.1 = 64.9$.\n$C(17) = 64.9 + 7.7 = 72.6$.\n$C(18) = 72.6 + 8.4 = 81.0$.\n$C(19) = 81.0 + \\max\\{15.0 - 5, 0\\} = 81.0 + 10.0 = 91.0$.\n$C(20) = 91.0 + \\max\\{16.3 - 5, 0\\} = 91.0 + 11.3 = 102.3$.\n\nThe cumulative sum $C(d)$ first exceeds $H=100$ on day $d=20$, since $C(19)=91.0 < 100$ and $C(20)=102.3 \\ge 100$. Thus, the baseline onset day is $D^{\\ast}_0 = 20$.\n\nNext, we derive the sensitivity $\\frac{\\partial D^{\\ast}}{\\partial \\Delta T}$. The problem states that the day index $d$ can be treated as a continuous variable $D$ near the onset day $D^{\\ast}$. The onset condition is then an equation, $C(D^{\\ast}, \\Delta T) = H$, where $D^{\\ast}$ is treated as a function of the warming $\\Delta T$. The cumulative sum function is:\n$$C(D^{\\ast}, \\Delta T) = \\sum_{i=1}^{\\lfloor D^{\\ast} \\rfloor} \\max\\{T_i + \\Delta T - T_b, 0\\}$$\nTo apply calculus, we assume that $D^{\\ast}(\\Delta T)$ is a differentiable function and that the cumulative sum function $C$ is also differentiable with respect to its arguments $D$ and $\\Delta T$. A necessary regularity assumption is that for the given temperatures, $T_i + \\Delta T \\neq T_b$ for any day $i$, ensuring the differentiability of the $\\max$ function.\n\nWe differentiate the onset condition $C(D^{\\ast}(\\Delta T), \\Delta T) = H$ implicitly with respect to $\\Delta T$. Since $H$ is a constant, its derivative is $0$. Using the multivariable chain rule:\n$$ \\frac{dC}{d\\Delta T} = \\frac{\\partial C}{\\partial D^{\\ast}} \\frac{\\partial D^{\\ast}}{\\partial \\Delta T} + \\frac{\\partial C}{\\partial \\Delta T} = 0 $$\nSolving for the desired sensitivity yields:\n$$ \\frac{\\partial D^{\\ast}}{\\partial \\Delta T} = - \\frac{\\partial C / \\partial \\Delta T}{\\partial C / \\partial D^{\\ast}} $$\nThe partial derivative $\\frac{\\partial C}{\\partial D^{\\ast}}$ represents the rate of GDD accumulation per day. In the continuous approximation, this is the GDD value on the onset day $D^{\\ast}$:\n$$ \\frac{\\partial C}{\\partial D^{\\ast}} \\approx G(T_{D^{\\ast}} + \\Delta T) = \\max\\{T_{D^{\\ast}} + \\Delta T - T_b, 0\\} $$\nThe partial derivative $\\frac{\\partial C}{\\partial \\Delta T}$ is the change in the total accumulated GDD up to day $D^{\\ast}$ due to the warming $\\Delta T$:\n$$ \\frac{\\partial C}{\\partial \\Delta T} = \\frac{\\partial}{\\partial \\Delta T} \\sum_{i=1}^{D^{\\ast}} \\max\\{T_i + \\Delta T - T_b, 0\\} = \\sum_{i=1}^{D^{\\ast}} \\frac{\\partial}{\\partial \\Delta T} \\max\\{T_i + \\Delta T - T_b, 0\\} $$\nUnder our regularity assumption, the derivative of the $\\max$ term is the indicator function $\\mathbb{I}(T_i + \\Delta T > T_b)$, which is $1$ if the condition holds and $0$ otherwise.\n$$ \\frac{\\partial C}{\\partial \\Delta T} = \\sum_{i=1}^{D^{\\ast}} \\mathbb{I}(T_i + \\Delta T > T_b) = N(D^{\\ast}, \\Delta T) $$\nThis sum, $N(D^{\\ast}, \\Delta T)$, is the number of days up to $D^{\\ast}$ that contribute to thermal accumulation.\n\nCombining the components, the sensitivity is:\n$$ \\frac{\\partial D^{\\ast}}{\\partial \\Delta T} = - \\frac{N(D^{\\ast}, \\Delta T)}{\\max\\{T_{D^{\\ast}} + \\Delta T - T_b, 0\\}} $$\nWe evaluate this expression at $\\Delta T = 0^{+}$, where $D^{\\ast} = D^{\\ast}_0 = 20$.\nThe denominator is the GDD on the baseline onset day:\n$$ \\max\\{T_{20} + 0 - T_b, 0\\} = \\max\\{16.3 - 5, 0\\} = 11.3 $$\nThe numerator, $N(20, 0)$, is the number of days from $i=1$ to $i=20$ for which $T_i > T_b=5$. Examining the data reveals that only $T_1 = 4.2$ is not above $5$. Thus, the count of contributing days is $N(20, 0) = 20 - 1 = 19$.\n\nThe sensitivity is therefore:\n$$ \\left. \\frac{\\partial D^{\\ast}}{\\partial \\Delta T} \\right|_{\\Delta T = 0^{+}} = - \\frac{19}{11.3} \\approx -1.6814159... $$\nThe problem asks for this numerical value, which represents the change in onset date per $+1\\,^{\\circ}\\mathrm{C}$ warming, rounded to four significant figures. This value is $-1.681$. The negative sign correctly indicates that warming leads to an earlier onset (a smaller day index $D^{\\ast}$).\n\nFor the interpretation of the elasticity, $\\mathcal{E} \\equiv \\left.\\frac{\\partial D^{\\ast}}{\\partial \\Delta T}\\right|_{\\Delta T=0^{+}} \\cdot \\frac{\\Delta T}{D^{\\ast}}$, when evaluated at $\\Delta T = 1\\,^{\\circ}\\mathrm{C}$:\n$$ \\mathcal{E} = \\left(-\\frac{N(D_0^{\\ast}, 0)}{G(T_{D_0^{\\ast}})}\\right) \\cdot \\frac{\\Delta T}{D_0^{\\ast}} = -\\frac{\\Delta T}{G(T_{D_0^{\\ast}})} \\left(\\frac{N(D_0^{\\ast}, 0)}{D_0^{\\ast}}\\right) $$\nThis expression shows that the elasticity (for a given $\\Delta T = 1$) is negatively proportional to a ratio. The term $\\frac{N(D_0^{\\ast}, 0)}{D_0^{\\ast}}$ is the fraction of days up to onset that are actively contributing to thermal accumulation ($19/20$ in this case). The term $G(T_{D_0^{\\ast}})$ is the marginal GDD accumulation on the specific day the threshold is crossed ($11.3$ in this case). Thus, the phenological response is stronger when a higher fraction of days in the pre-onset period contribute to warming, and weaker when the GDD accumulation on the final day is large.\n\nThe final answer is the numerical value of the sensitivity derivative.", "answer": "$$\\boxed{-1.681}$$", "id": "2595740"}, {"introduction": "Phenological shifts rarely occur in isolation; they can disrupt the timing of critical species interactions, such as those between plants and their specialist pollinators, leading to a \"phenological mismatch.\" To move from conceptual concern to quantitative assessment, ecologists need robust metrics for temporal overlap. This practice challenges you to derive a closed-form expression for the overlap integral—a widely used synchrony metric—and apply it to a case where resource and consumer phenologies are modeled as Gaussian distributions, thereby connecting probability theory to a core problem in community ecology [@problem_id:2595735].", "problem": "In comparative zoology and botany, phenological synchrony between a resource and a consumer can be quantified as the area of temporal overlap between their normalized activity curves. Let $R(t)$ denote the normalized resource availability curve (for example, a plant’s flowering intensity over time) and $D(t)$ denote the normalized consumer demand curve (for example, a pollinator’s foraging demand over time). Assume both are nonnegative functions that integrate to $1$ over time, so that each can be treated as a probability density function (PDF). A widely used, dimensionless metric of temporal synchrony is the overlap integral defined by\n$$\nS \\equiv \\int_{-\\infty}^{\\infty} \\min\\big(R(t),D(t)\\big)\\,dt,\n$$\nwhich takes values in $[0,1]$ and equals $1$ if and only if $R(t)\\equiv D(t)$ almost everywhere.\n\nStarting from the fundamental definitions of a probability density function and basic properties of integrals, derive a closed-form expression for $S$ in terms of the intersection time(s) at which $R(t)=D(t)$ and the cumulative distribution functions (CDFs) corresponding to $R(t)$ and $D(t)$. Then, model a plant resource curve and an animal demand curve as Gaussian (normal) PDFs with different means and variances:\n$$\nR(t)=\\frac{1}{\\sigma_{R}\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(t-\\mu_{R})^{2}}{2\\sigma_{R}^{2}}\\right),\\quad\nD(t)=\\frac{1}{\\sigma_{D}\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(t-\\mu_{D})^{2}}{2\\sigma_{D}^{2}}\\right),\n$$\nwith parameters $\\mu_{R}=120$ (day-of-year), $\\sigma_{R}=10$ (days), $\\mu_{D}=135$ (day-of-year), and $\\sigma_{D}=15$ (days). Compute the synchrony $S$ for these parameter values. Round your final numerical answer to $4$ significant figures. Express the final answer as a pure number without units.", "solution": "The problem requires a two-part solution: first, to derive a general expression for the synchrony index $S$, and second, to calculate its value for two specific Gaussian distributions.\n\nWe begin with the definition of the synchrony index $S$:\n$$\nS = \\int_{-\\infty}^{\\infty} \\min\\big(R(t), D(t)\\big)\\,dt\n$$\nwhere $R(t)$ and $D(t)$ are probability density functions (PDFs). This means they are non-negative and integrate to $1$:\n$$\n\\int_{-\\infty}^{\\infty} R(t)\\,dt = 1, \\quad \\int_{-\\infty}^{\\infty} D(t)\\,dt = 1\n$$\nThe value of $\\min(R(t), D(t))$ depends on which function is smaller at a given time $t$. Let us define the set of intersection points $T_{int} = \\{t | R(t) = D(t)\\}$. These points partition the real line into regions where either $R(t) \\le D(t)$ or $R(t) > D(t)$. We can then split the integral for $S$:\n$$\nS = \\int_{\\{t | R(t) \\le D(t)\\}} R(t)\\,dt + \\int_{\\{t | R(t) > D(t)\\}} D(t)\\,dt\n$$\nFor two distinct Gaussian distributions with different variances, there are generally two intersection points, which we denote as $t_1$ and $t_2$ with $t_1 < t_2$. The relative magnitude of the functions in the intervals $(-\\infty, t_1)$, $(t_1, t_2)$, and $(t_2, \\infty)$ depends on their parameters. A Gaussian function $N(\\mu, \\sigma^2)$ has tails that decay to zero as $\\exp(-t^2/(2\\sigma^2))$. A larger variance $\\sigma^2$ results in a slower decay. The peak height of the PDF is inversely proportional to $\\sigma$, specifically $1/(\\sigma\\sqrt{2\\pi})$.\n\nIn the specific case given, $\\sigma_R = 10$ and $\\sigma_D = 15$. Since $\\sigma_D > \\sigma_R$, the function $D(t)$ has heavier tails than $R(t)$, but $R(t)$ has a higher peak. Consequently, for $t \\to \\pm\\infty$, $D(t)$ will be greater than $R(t)$. Between the intersection points, $R(t)$ will dominate. Therefore, we have:\n- $R(t) \\le D(t)$ for $t \\in (-\\infty, t_1] \\cup [t_2, \\infty)$\n- $R(t) > D(t)$ for $t \\in (t_1, t_2)$\n\nSubstituting these regions into the integral for $S$:\n$$\nS = \\int_{-\\infty}^{t_1} R(t)\\,dt + \\int_{t_1}^{t_2} D(t)\\,dt + \\int_{t_2}^{\\infty} R(t)\\,dt\n$$\nNow, we express these integrals using the cumulative distribution functions (CDFs) $F_R(t) = \\int_{-\\infty}^{t} R(\\tau)\\,d\\tau$ and $F_D(t) = \\int_{-\\infty}^{t} D(\\tau)\\,d\\tau$.\nThe first term is $\\int_{-\\infty}^{t_1} R(t)\\,dt = F_R(t_1)$.\nThe second term is $\\int_{t_1}^{t_2} D(t)\\,dt = F_D(t_2) - F_D(t_1)$.\nThe third term is $\\int_{t_2}^{\\infty} R(t)\\,dt = \\int_{-\\infty}^{\\infty} R(t)\\,dt - \\int_{-\\infty}^{t_2} R(t)\\,dt = 1 - F_R(t_2)$.\n\nCombining these terms gives the desired closed-form expression for $S$:\n$$\nS = F_R(t_1) + \\big(F_D(t_2) - F_D(t_1)\\big) + \\big(1 - F_R(t_2)\\big)\n$$\n$$\nS = 1 + F_R(t_1) - F_R(t_2) + F_D(t_2) - F_D(t_1)\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must calculate $S$ for the given Gaussian PDFs:\n$R(t) \\sim N(\\mu_R, \\sigma_R^2)$ with $\\mu_R = 120, \\sigma_R = 10$.\n$D(t) \\sim N(\\mu_D, \\sigma_D^2)$ with $\\mu_D = 135, \\sigma_D = 15$.\n\nFirst, we find the intersection points $t_1, t_2$ by solving $R(t) = D(t)$:\n$$\n\\frac{1}{\\sigma_{R}\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(t-\\mu_{R})^{2}}{2\\sigma_{R}^{2}}\\right) = \\frac{1}{\\sigma_{D}\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(t-\\mu_{D})^{2}}{2\\sigma_{D}^{2}}\\right)\n$$\nTaking the natural logarithm of both sides and rearranging terms yields a quadratic equation $At^2+Bt+C=0$:\n$$\n\\ln\\left(\\frac{1}{\\sigma_R}\\right) - \\frac{(t-\\mu_R)^2}{2\\sigma_R^2} = \\ln\\left(\\frac{1}{\\sigma_D}\\right) - \\frac{(t-\\mu_D)^2}{2\\sigma_D^2}\n$$\n$$\n\\left(\\frac{1}{2\\sigma_D^2} - \\frac{1}{2\\sigma_R^2}\\right)t^2 + \\left(\\frac{\\mu_R}{\\sigma_R^2} - \\frac{\\mu_D}{\\sigma_D^2}\\right)t + \\left(\\frac{\\mu_D^2}{2\\sigma_D^2} - \\frac{\\mu_R^2}{2\\sigma_R^2} + \\ln\\left(\\frac{\\sigma_R}{\\sigma_D}\\right)\\right) = 0\n$$\nThe coefficients are:\n$A = \\frac{\\sigma_R^2 - \\sigma_D^2}{2\\sigma_R^2\\sigma_D^2} = \\frac{10^2 - 15^2}{2 \\cdot 10^2 \\cdot 15^2} = \\frac{100 - 225}{45000} = \\frac{-125}{45000} = -\\frac{1}{360}$\n$B = \\frac{\\mu_R}{\\sigma_R^2} - \\frac{\\mu_D}{\\sigma_D^2} = \\frac{120}{100} - \\frac{135}{225} = 1.2 - 0.6 = 0.6$\n$C = \\frac{135^2}{2 \\cdot 15^2} - \\frac{120^2}{2 \\cdot 10^2} + \\ln\\left(\\frac{10}{15}\\right) = \\frac{18225}{450} - \\frac{14400}{200} - \\ln(1.5) = 40.5 - 72 - \\ln(1.5) = -31.5 - \\ln(1.5)$\n\nThe quadratic equation is $-\\frac{1}{360}t^2 + 0.6t - (31.5+\\ln(1.5)) = 0$. Multiplying by $-360$:\n$t^2 - 216t + 360(31.5+\\ln(1.5)) = 0$\n$t^2 - 216t + 11340 + 360\\ln(1.5) = 0$\nSolving for $t$ using the quadratic formula:\n$$\nt = \\frac{216 \\pm \\sqrt{216^2 - 4(11340 + 360\\ln(1.5))}}{2} = 108 \\pm \\sqrt{108^2 - (11340 + 360\\ln(1.5))}\n$$\n$$\nt = 108 \\pm \\sqrt{11664 - 11340 - 360\\ln(1.5)} = 108 \\pm \\sqrt{324 - 360\\ln(1.5)}\n$$\nUsing the value $\\ln(1.5) \\approx 0.405465$:\n$t = 108 \\pm \\sqrt{324 - 360(0.405465)} = 108 \\pm \\sqrt{324 - 145.9674} = 108 \\pm \\sqrt{178.0326}$\n$t \\approx 108 \\pm 13.3429$\nThis gives the intersection points:\n$t_1 \\approx 94.6571$\n$t_2 \\approx 108 + 13.3429 = 121.3429$\n\nNext, we evaluate the CDFs at these points. The CDF for a general normal distribution $N(\\mu, \\sigma^2)$ is $F(t) = \\Phi\\left(\\frac{t-\\mu}{\\sigma}\\right)$, where $\\Phi(z)$ is the standard normal CDF.\nWe compute the standardized arguments:\n$z_{R1} = \\frac{t_1 - \\mu_R}{\\sigma_R} = \\frac{94.6571 - 120}{10} = -2.53429$\n$z_{R2} = \\frac{t_2 - \\mu_R}{\\sigma_R} = \\frac{121.3429 - 120}{10} = 0.13429$\n$z_{D1} = \\frac{t_1 - \\mu_D}{\\sigma_D} = \\frac{94.6571 - 135}{15} = -2.68953$\n$z_{D2} = \\frac{t_2 - \\mu_D}{\\sigma_D} = \\frac{121.3429 - 135}{15} = -0.91047$\n\nWe find the corresponding $\\Phi(z)$ values:\n$F_R(t_1) = \\Phi(-2.53429) \\approx 0.005634$\n$F_R(t_2) = \\Phi(0.13429) \\approx 0.55343$\n$F_D(t_1) = \\Phi(-2.68953) \\approx 0.003577$\n$F_D(t_2) = \\Phi(-0.91047) \\approx 0.18129$\n\nFinally, substitute these values into the expression for $S$:\n$S = 1 + F_R(t_1) - F_R(t_2) + F_D(t_2) - F_D(t_1)$\n$S \\approx 1 + 0.005634 - 0.55343 + 0.18129 - 0.003577$\n$S \\approx 1 - (0.55343 - 0.005634) + (0.18129 - 0.003577)$\n$S \\approx 1 - 0.547796 + 0.177713 = 1 - 0.370083 = 0.629917$\n\nRounding the result to $4$ significant figures gives $0.6299$.", "answer": "$$\n\\boxed{0.6299}\n$$", "id": "2595735"}, {"introduction": "While many phenological trends may be gradual, some ecosystems or species exhibit abrupt shifts in response to environmental forcing. Differentiating a true step-change from random noise in a time-series is a non-trivial statistical problem. This advanced exercise introduces you to Bayesian change-point analysis, a powerful framework for inferring the probability and timing of abrupt shifts. You will perform the full workflow: deriving the marginal likelihood from first principles, designing a numerically stable algorithm, and applying it to test data, giving you a complete hands-on introduction to this modern computational method [@problem_id:2595654].", "problem": "You are given annual observations of the onset Day of Year (DOY) of a spring phenological event for a single species over a sequence of calendar years. Day of Year (DOY) values are measured in days. You will model potential phenological shifts as a single change-point in the mean DOY and compute the posterior probability that a shift occurred at a specified candidate calendar year.\n\nAssume the following generative model based on well-tested facts about Gaussian-distributed measurement error and Bayesian inference with conjugate priors:\n- The observed DOY series is denoted by $x_{1},\\dots,x_{N}$, corresponding to strictly increasing calendar years $y_{1},\\dots,y_{N}$.\n- Conditional on a change-point year $\\tau$ (or no change), the series is split into at most two contiguous segments. Within each segment $s \\in \\{1,2\\}$, the observations are independent and identically distributed as $x_{i} \\mid \\mu_{s},\\sigma^{2} \\sim \\mathcal{N}(\\mu_{s},\\sigma^{2})$, where $\\sigma^{2}$ is known and $\\mu_{s}$ is an unknown segment-specific mean.\n- The segment means have independent Gaussian priors $\\mu_{s} \\sim \\mathcal{N}(\\mu_{0},v_{0})$, with known hyperparameters $\\mu_{0}$ and $v_{0}$.\n- The prior over the change-point location is uniform over $N$ mutually exclusive states: either “no change” (one segment spanning all $N$ years) or a change beginning at one of the interior calendar years $\\{y_{2},\\dots,y_{N}\\}$ (two segments split between $y_{j-1}$ and $y_{j}$ for some $j \\in \\{2,\\dots,N\\}$). Thus, the prior assigns equal mass $1/N$ to each of these $N$ states.\n\nTasks:\n1) Starting from the definitions of the Gaussian likelihood, Gaussian prior, independence of observations, and Bayes’ theorem, derive a closed-form expression for the posterior distribution over the discrete change-point states by analytically integrating out the unknown mean parameter(s) $\\mu_{s}$. Your derivation must produce an expression for the log posterior (up to an additive normalization constant) for each candidate change-point state as a function of segment-wise sufficient statistics. Do not assume any “shortcut” formulas beyond the fundamental Gaussian identities and conjugacy.\n\n2) Design an algorithm that:\n- Enumerates all candidate change-point states (including the “no change” state).\n- Computes the unnormalized log posterior for each state using your derived expression.\n- Normalizes these values using a numerically stable log-sum-exp transformation to obtain exact posterior probabilities.\n- Extracts the posterior probability that a shift begins at a specified query calendar year $q \\in \\{y_{2},\\dots,y_{N}\\}$; if $q \\notin \\{y_{2},\\dots,y_{N}\\}$, this probability should be defined as $0$ by model design.\n\n3) Implement this algorithm as a complete, runnable program that uses the following test suite. For each test case, compute the posterior probability that a shift begins at the specified query calendar year. All DOY values are in days. The final answers are probabilities and must be expressed as decimals.\n\nTest suite (each case provides $(\\text{years}, \\text{DOY}, \\sigma, \\mu_{0}, v_{0}, q)$):\n- Case A:\n  - Years: (2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015)\n  - DOY: (121, 118, 119, 122, 120, 123, 117, 119, 106, 104, 107, 103, 108, 105, 104)\n  - Known standard deviation: $\\sigma=3.0$ days\n  - Prior mean: $\\mu_{0}=110.0$ days\n  - Prior variance: $v_{0}=100.0 \\text{ days}^2$\n  - Query year: $q=2009$\n- Case B:\n  - Years: (2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015)\n  - DOY: (111, 112, 113, 110, 114, 112, 111, 113, 112, 110, 113, 112, 111, 114, 112)\n  - Known standard deviation: $\\sigma=2.5$ days\n  - Prior mean: $\\mu_{0}=112.0$ days\n  - Prior variance: $v_{0}=25.0 \\text{ days}^2$\n  - Query year: $q=2010$\n- Case C:\n  - Years: (2012, 2013, 2014, 2015, 2016)\n  - DOY: (131, 129, 123, 125, 124)\n  - Known standard deviation: $\\sigma=2.0$ days\n  - Prior mean: $\\mu_{0}=127.0$ days\n  - Prior variance: $v_{0}=25.0 \\text{ days}^2$\n  - Query year: $q=2014$\n- Case D:\n  - Years: (2010, 2011, 2012, 2013, 2014)\n  - DOY: (100, 101, 102, 99, 103)\n  - Known standard deviation: $\\sigma=2.0$ days\n  - Prior mean: $\\mu_{0}=101.0$ days\n  - Prior variance: $v_{0}=25.0 \\text{ days}^2$\n  - Query year: $q=2010$ (boundary case; should yield zero probability)\n- Case E:\n  - Years: (2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014)\n  - DOY: (117, 121, 116, 120, 118, 114, 109, 115, 111, 113)\n  - Known standard deviation: $\\sigma=6.0$ days\n  - Prior mean: $\\mu_{0}=115.0$ days\n  - Prior variance: $v_{0}=100.0 \\text{ days}^2$\n  - Query year: $q=2010$\n\nImplementation and output requirements:\n- Your program must implement the derivation and algorithm above to compute the posterior probability that a shift begins at the specified query year $q$ for each test case.\n- Units: Input DOY values and hyperparameters $\\mu_{0}$ and $\\sigma$ are in days, $v_{0}$ is in days squared, and the output probabilities are unitless decimals.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each probability rounded to exactly six decimal places (for example, $\\left[\\text{0.123456},\\text{0.000000}\\right]$).", "solution": "We model DOY values as Gaussian with a possible single change in the mean. Let $x_{1:N}=(x_{1},\\dots,x_{N})$ denote the observed Day of Year (DOY) measurements in days aligned with increasing calendar years $y_{1:N}=(y_{1},\\dots,y_{N})$. The known observation variance is $\\sigma^{2}$ (in days squared). The prior over segment means is $\\mu_{s}\\sim\\mathcal{N}(\\mu_{0},v_{0})$ with $\\mu_{0}$ in days and $v_{0}$ in days squared. The change-point state $\\tau$ takes one of $N$ values: either “no change” (one segment) or “change begins at year $y_{j}$” for $j\\in\\{2,\\dots,N\\}$ (two segments: indices $1:(j-1)$ and $j:N$). The prior over these $N$ states is uniform with mass $1/N$ each.\n\nPrinciples and derivation:\n1) Likelihood for a segment. For any segment $s$ containing $m$ observations, conditional on its mean $\\mu_{s}$ and known variance $\\sigma^{2}$, the likelihood factorizes by independence:\n$$\np(x_{1:m}\\mid \\mu_{s},\\sigma^{2})=(2\\pi\\sigma^{2})^{-m/2}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{m}(x_{i}-\\mu_{s})^{2}\\right).\n$$\nThe conjugate prior is $\\mu_{s}\\sim\\mathcal{N}(\\mu_{0},v_{0})$ with density\n$$\np(\\mu_{s})=(2\\pi v_{0})^{-1/2}\\exp\\left(-\\frac{(\\mu_{s}-\\mu_{0})^{2}}{2v_{0}}\\right).\n$$\nThe marginal likelihood for a single segment is obtained by integrating out $\\mu_{s}$:\n$$\np(x_{1:m}\\mid \\mu_{0},v_{0},\\sigma^{2})=\\int p(x_{1:m}\\mid \\mu_{s},\\sigma^{2})\\,p(\\mu_{s})\\,d\\mu_{s}.\n$$\nCompleting the square in the exponent yields a Gaussian integral. Define the sufficient statistics\n$$\nS_{1}=\\sum_{i=1}^{m}x_{i},\\quad S_{2}=\\sum_{i=1}^{m}x_{i}^{2}.\n$$\nLet\n$$\nA=\\frac{m}{\\sigma^{2}}+\\frac{1}{v_{0}},\\quad B=\\frac{S_{1}}{\\sigma^{2}}+\\frac{\\mu_{0}}{v_{0}},\\quad C=\\frac{S_{2}}{\\sigma^{2}}+\\frac{\\mu_{0}^{2}}{v_{0}}.\n$$\nUsing the standard Gaussian integral identity, one obtains the closed-form marginal likelihood\n$$\np(x_{1:m}\\mid \\mu_{0},v_{0},\\sigma^{2})=(2\\pi)^{-m/2}\\,\\sigma^{-m}\\,v_{0}^{-1/2}\\,A^{-1/2}\\,\\exp\\left(-\\frac{1}{2}\\left(C-\\frac{B^{2}}{A}\\right)\\right).\n$$\nTherefore, the log marginal likelihood is\n$$\n\\log p(x_{1:m})=-\\frac{m}{2}\\log(2\\pi)-m\\log\\sigma-\\frac{1}{2}\\log v_{0}-\\frac{1}{2}\\log A-\\frac{1}{2}\\left(C-\\frac{B^{2}}{A}\\right).\n$$\n\n2) Change-point states. For a candidate change beginning at year $y_{j}$ with $j\\in\\{2,\\dots,N\\}$, we have two independent segments, so the marginal likelihood factorizes:\n$$\np(x_{1:N}\\mid \\tau=y_{j})=p(x_{1:(j-1)}\\mid \\mu_{0},v_{0},\\sigma^{2})\\cdot p(x_{j:N}\\mid \\mu_{0},v_{0},\\sigma^{2}).\n$$\nFor the “no change” state, there is a single segment:\n$$\np(x_{1:N}\\mid \\tau=\\text{no change})=p(x_{1:N}\\mid \\mu_{0},v_{0},\\sigma^{2}).\n$$\n\n3) Posterior over states. By Bayes’ theorem with the uniform prior over the $N$ states,\n$$\np(\\tau\\mid x_{1:N})=\\frac{p(x_{1:N}\\mid \\tau)\\,p(\\tau)}{\\sum_{\\tau'}p(x_{1:N}\\mid \\tau')\\,p(\\tau')},\\quad p(\\tau)=\\frac{1}{N}.\n$$\nFor numerical stability, compute unnormalized log posteriors\n$$\n\\ell(\\tau)=\\log p(x_{1:N}\\mid \\tau)+\\log p(\\tau),\n$$\nthen normalize via the log-sum-exp transformation. Let $\\{\\ell_{k}\\}_{k=1}^{N}$ be the log unnormalized values over all $N$ states and let $M=\\max_{k}\\ell_{k}$. Then the normalized posterior probabilities are\n$$\nw_{k}=\\frac{\\exp(\\ell_{k}-M)}{\\sum_{j=1}^{N}\\exp(\\ell_{j}-M)}.\n$$\n\n4) Query probability. The posterior probability that a shift begins at a specified calendar year $q$ equals $p(\\tau=q\\mid x_{1:N})$ if $q\\in\\{y_{2},\\dots,y_{N}\\}$. If $q\\notin\\{y_{2},\\dots,y_{N}\\}$ (including the boundary $q=y_{1}$), we define the probability as zero consistent with the model’s state space.\n\nAlgorithmic design:\n- Precompute cumulative sufficient statistics is optional; since each candidate involves at most two segments, direct segment-wise sums suffice.\n- For each candidate state (including “no change”), compute the corresponding log marginal likelihood using the segment formula above and add the constant $\\log(1/N)$ prior.\n- Normalize via log-sum-exp to obtain posterior probabilities over states.\n- Map the query year $q$ to its state index if valid and report the corresponding posterior mass; otherwise return zero.\n\nUnits and outputs:\n- Inputs $\\{x_{i}\\}$, $\\mu_{0}$, and $\\sigma$ are in days; $v_{0}$ is in days squared. The resulting posterior probabilities are unitless decimals.\n- For the test suite, compute the query-year posterior probabilities and round each to exactly six decimal places. Aggregate the results into a single line formatted as a comma-separated list enclosed in square brackets.\n\nThis approach is grounded in the independence of observations, the Gaussian likelihood with known variance, the Gaussian prior (conjugacy), and Bayes’ theorem, yielding an exact closed-form marginalization over the unknown segment means and a numerically stable normalization across discrete change-point states.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef log_marginal_segment(x: np.ndarray, mu0: float, v0: float, sigma: float) -> float:\n    \"\"\"\n    Compute log p(x | mu0, v0, sigma^2) for a single segment by integrating out the mean.\n    x: 1D numpy array of observations (in days)\n    mu0: prior mean (in days)\n    v0: prior variance (in days^2)\n    sigma: known standard deviation (in days)\n    \"\"\"\n    m = x.size\n    if m == 0:\n        # Empty segment should not occur in our construction; return -inf to avoid using it.\n        return -np.inf\n    sigma2 = sigma * sigma\n    S1 = float(np.sum(x))\n    S2 = float(np.sum(x * x))\n    A = m / sigma2 + 1.0 / v0\n    B = S1 / sigma2 + mu0 / v0\n    C = S2 / sigma2 + (mu0 * mu0) / v0\n    log_term = - (m / 2.0) * np.log(2.0 * np.pi) - m * np.log(sigma) - 0.5 * np.log(v0) - 0.5 * np.log(A)\n    quad_term = -0.5 * (C - (B * B) / A)\n    return log_term + quad_term\n\ndef posterior_probs_over_states(years, x, sigma, mu0, v0):\n    \"\"\"\n    Compute posterior probabilities over all change-point states:\n    - 'no_change' state\n    - change begins at each interior year years[j], j=1..N-1\n    Returns:\n      state_names: list of state identifiers (strings for 'no_change' and ints for years)\n      probs: numpy array of posterior probabilities aligned with state_names\n    \"\"\"\n    years = np.array(years, dtype=int)\n    x = np.array(x, dtype=float)\n    N = len(years)\n    assert len(x) == N, \"years and x must have the same length\"\n    # Build states: 0 -> 'no_change', 1..N-1 -> change begins at years[j]\n    state_names = ['no_change'] + [int(y) for y in years[1:]]\n    log_priors = np.full(N, -np.log(N))  # uniform over N states\n    log_liks = np.empty(N, dtype=float)\n\n    # No change: single segment\n    log_liks[0] = log_marginal_segment(x, mu0, v0, sigma)\n\n    # Each change candidate: split into two segments\n    for j in range(1, N):\n        x1 = x[:j]\n        x2 = x[j:]\n        log_lik = log_marginal_segment(x1, mu0, v0, sigma) + log_marginal_segment(x2, mu0, v0, sigma)\n        log_liks[j] = log_lik\n\n    # Unnormalized log posteriors\n    log_post = log_liks + log_priors\n    # Normalize with log-sum-exp\n    m = np.max(log_post)\n    weights = np.exp(log_post - m)\n    probs = weights / np.sum(weights)\n    return state_names, probs\n\ndef query_year_posterior(years, x, sigma, mu0, v0, q_year):\n    \"\"\"\n    Compute posterior probability that change begins at calendar year q_year.\n    Returns 0.0 if q_year is not a valid interior change candidate.\n    \"\"\"\n    state_names, probs = posterior_probs_over_states(years, x, sigma, mu0, v0)\n    # Valid change years are interior years: years[1:]\n    valid_years = set(state_names[1:])  # all ints\n    if q_year not in valid_years:\n        return 0.0\n    # Find index of q_year in state_names\n    idx = state_names.index(q_year)\n    return float(probs[idx])\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (years, DOY, sigma, mu0, v0, query_year)\n    test_cases = [\n        # Case A\n        (\n            [2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015],\n            [121,118,119,122,120,123,117,119,106,104,107,103,108,105,104],\n            3.0,\n            110.0,\n            100.0,\n            2009\n        ),\n        # Case B\n        (\n            [2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015],\n            [111,112,113,110,114,112,111,113,112,110,113,112,111,114,112],\n            2.5,\n            112.0,\n            25.0,\n            2010\n        ),\n        # Case C\n        (\n            [2012,2013,2014,2015,2016],\n            [131,129,123,125,124],\n            2.0,\n            127.0,\n            25.0,\n            2014\n        ),\n        # Case D (boundary query; should yield zero)\n        (\n            [2010,2011,2012,2013,2014],\n            [100,101,102,99,103],\n            2.0,\n            101.0,\n            25.0,\n            2010\n        ),\n        # Case E (larger variance, ambiguous)\n        (\n            [2005,2006,2007,2008,2009,2010,2011,2012,2013,2014],\n            [117,121,116,120,118,114,109,115,111,113],\n            6.0,\n            115.0,\n            100.0,\n            2010\n        ),\n    ]\n\n    results = []\n    for years, doy, sigma, mu0, v0, q in test_cases:\n        p = query_year_posterior(years, doy, sigma, mu0, v0, q)\n        # Round to 6 decimals in string formatting later\n        results.append(p)\n\n    # Final print statement in the exact required format with six decimal places.\n    formatted = \"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2595654"}]}