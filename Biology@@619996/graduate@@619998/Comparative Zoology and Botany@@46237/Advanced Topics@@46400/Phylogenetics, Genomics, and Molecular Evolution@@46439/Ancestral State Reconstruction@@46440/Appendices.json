{"hands_on_practices": [{"introduction": "We begin our hands-on practice with one of the most foundational methods in ancestral state reconstruction: maximum parsimony. This approach seeks the ancestral states that require the minimum number of evolutionary changes to explain the character states observed at the tips of a phylogeny. This exercise will guide you through the classic Fitch algorithm, a two-pass procedure for determining the set of possible ancestral states and the total parsimony score for an unordered, discrete character [@problem_id:2545589].", "problem": "In comparative zoology and botany, ancestral state reconstruction under maximum parsimony seeks to infer ancestral character states that minimize the total number of changes on a phylogeny. Consider an unrooted binary character that is unordered, meaning transitions $0 \\leftrightarrow 1$ each have equal cost. You are given a rooted three-taxon tree with topology $((A,B),C)$, where the observed tip states are $A=0$, $B=1$, and $C=1$. Using the parsimony criterion and the Fitch algorithm, determine the set of all possible ancestral state sets at the two internal nodes: the internal node uniting $A$ and $B$ (call this node $v$), and the root uniting $(A,B)$ with $C$ (call this node $r$). Then compute the minimal total number of state changes implied on the tree under this reconstruction. Report as your final answer the minimal total number of changes as a single integer. No rounding is required.", "solution": "The foundational principle is the maximum parsimony criterion: among all assignments of ancestral character states consistent with the observed tip states, choose those that minimize the total number of state changes. For unordered binary characters, transitions $0 \\to 1$ and $1 \\to 0$ each have the same cost, taken as $1$, and there is no directionality bias.\n\nThe Fitch algorithm provides a constructive procedure to compute, for each internal node, a set of possible states that can be assigned without increasing the minimal number of steps, and simultaneously yields the minimal number of changes. The algorithm proceeds in two passes.\n\nUpward pass (postorder) to compute ancestral state sets and count changes:\n- For leaves, assign the observed singleton sets: $S_A=\\{0\\}$, $S_B=\\{1\\}$, $S_C=\\{1\\}$.\n- Consider the internal node $v$ uniting $A$ and $B$. Compute $S_v$ as the intersection of child sets if nonempty; otherwise the union. Here, $S_A \\cap S_B = \\{0\\} \\cap \\{1\\} = \\varnothing$, so the intersection is empty. Therefore, assign $S_v = S_A \\cup S_B = \\{0,1\\}$ and increment the parsimony step count by $1$ due to the empty intersection at this node.\n- Consider the root node $r$ uniting $v$ and $C$. We have $S_v=\\{0,1\\}$ and $S_C=\\{1\\}$. The intersection is $S_v \\cap S_C = \\{0,1\\} \\cap \\{1\\} = \\{1\\}$, which is nonempty. Therefore, assign $S_r=\\{1\\}$ and do not increment the step count at this node.\n\nThe upward pass yields the ancestral state sets $S_v=\\{0,1\\}$ and $S_r=\\{1\\}$. The total minimal number of changes accumulated from empty intersections is $1$.\n\nDownward pass (preorder) to realize one minimal assignment and verify the step count:\n- Choose any state from $S_r$ for the root; here $S_r=\\{1\\}$, so set the root to $1$.\n- For the child $v$ with $S_v=\\{0,1\\}$, choose a state that minimizes change along edge $(r,v)$. Choosing state $1$ at $v$ matches the parent and incurs $0$ changes on $(r,v)$.\n- Propagate to leaves: from $v=1$ to $B=1$ no change on $(v,B)$; from $v=1$ to $A=0$ one change on $(v,A)$.\n\nThis explicit assignment realizes exactly $1$ change, matching the count from the upward pass. By the parsimony principle and the Fitch algorithm, this is the minimal possible number of changes.\n\nTherefore, the ancestral state sets at the internal nodes are $S_v=\\{0,1\\}$ and $S_r=\\{1\\}$, and the minimal total number of changes is $1$.", "answer": "$$\\boxed{1}$$", "id": "2545589"}, {"introduction": "Moving from parsimony to model-based inference, we now tackle the reconstruction of continuous traits. This practice uses the Brownian motion model, which treats trait evolution as a random walk along the branches of the phylogeny. By applying principles of generalized least squares to the phylogenetic variance-covariance structure, you will derive not only the most likely ancestral state but also a measure of its statistical uncertainty, a key advantage of probabilistic approaches [@problem_id:2545572].", "problem": "Consider a continuous trait evolving under a Brownian motion model on a rooted phylogeny with topology $\\big((A:1, B:1):1, C:2\\big)$. Under Brownian motion, conditional on the parent state, the change along a branch of length $t$ is Gaussian with mean $0$ and variance $\\sigma^{2} t$, and changes on distinct branches are independent. Assume a flat (improper uniform) prior for the root state, so that the joint distribution of all node states is obtained by compounding these Gaussian increments without introducing additional information at the root. The observed tip trait values are $A=2.0$, $B=3.0$, and $C=5.0$, and the diffusion rate is $\\sigma^{2}=1$.\n\nUsing only the stated Brownian motion assumptions and the properties of multivariate Gaussian distributions and conditional independence on trees, derive from first principles the maximum likelihood estimate (equivalently, the posterior mean under the flat root prior) and the associated conditional variance for the ancestral trait at the internal node that is the most recent common ancestor of $A$ and $B$. Express your final answer as exact rational numbers with no rounding and provide both the estimate and the variance together, in that order, as a single row vector. No units are required.", "solution": "Let the nodes be denoted as follows: $A$, $B$, and $C$ for the tips; $D$ for the most recent common ancestor (MRCA) of $A$ and $B$; and $R$ for the root of the tree. The trait values at these nodes are $x_A$, $x_B$, $x_C$, $x_D$, and $x_R$. The observed tip data are $x_A=2.0$, $x_B=3.0$, and $x_C=5.0$. The diffusion rate is $\\sigma^2=1$.\n\nThe problem can be solved using the generalized least squares (GLS) framework for phylogenetic data. The vector of observed tip states, $X = \\begin{pmatrix} x_A & x_B & x_C \\end{pmatrix}^T = \\begin{pmatrix} 2 & 3 & 5 \\end{pmatrix}^T$, follows a multivariate normal distribution. The assumption of a flat prior on the root state is equivalent to estimating the root state $\\hat{x}_R$ from the data using GLS.\n\nFirst, we construct the phylogenetic variance-covariance matrix, $V$. The element $V_{ij}$ is $\\sigma^2$ times the shared path length from the root to the MRCA of tips $i$ and $j$. Given the tree `((A:1, B:1):1, C:2)`, the root $R$ is at time $0$. Node $D$ is at time $1$. Tips $A$ and $B$ are at time $2$, and tip $C$ is at time $2$.\nThe elements of $V$ are:\n$V_{AA} = \\sigma^2 \\times (\\text{time from root to A}) = 1 \\times (1+1) = 2$.\n$V_{BB} = \\sigma^2 \\times (\\text{time from root to B}) = 1 \\times (1+1) = 2$.\n$V_{CC} = \\sigma^2 \\times (\\text{time from root to C}) = 1 \\times 2 = 2$.\n$V_{AB} = \\sigma^2 \\times (\\text{time of node D}) = 1 \\times 1 = 1$.\n$V_{AC} = \\sigma^2 \\times (\\text{time of node R}) = 1 \\times 0 = 0$.\n$V_{BC} = \\sigma^2 \\times (\\text{time of node R}) = 1 \\times 0 = 0$.\n\nThus, the matrix $V$ and its inverse $V^{-1}$ are:\n$$V = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}, \\quad V^{-1} = \\begin{pmatrix} 2/3 & -1/3 & 0 \\\\ -1/3 & 2/3 & 0 \\\\ 0 & 0 & 1/2 \\end{pmatrix}$$\nThe GLS estimate of the root state is $\\hat{x}_R = (\\mathbf{1}^T V^{-1} X) / (\\mathbf{1}^T V^{-1} \\mathbf{1})$.\n$\\mathbf{1}^T V^{-1} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} V^{-1} = \\begin{pmatrix} 1/3 & 1/3 & 1/2 \\end{pmatrix}$.\n$\\mathbf{1}^T V^{-1} \\mathbf{1} = 1/3 + 1/3 + 1/2 = 7/6$.\n$\\mathbf{1}^T V^{-1} X = \\begin{pmatrix} 1/3 & 1/3 & 1/2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = 2/3 + 1 + 5/2 = 25/6$.\nSo, $\\hat{x}_R = \\frac{25/6}{7/6} = 25/7$.\n\nThe ML estimate for an ancestral node $D$ is given by $\\hat{x}_D = \\hat{x}_R + \\mathbf{c}_D^T V^{-1}(X - \\hat{x}_R \\mathbf{1})$, where $\\mathbf{c}_D$ is the vector of covariances between node $D$ and the tips.\n$\\mathbf{c}_D = \\begin{pmatrix} \\text{cov}(D,A) & \\text{cov}(D,B) & \\text{cov}(D,C) \\end{pmatrix}^T = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}^T$.\nThe correction term is calculated:\n$X - \\hat{x}_R \\mathbf{1} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} - \\frac{25}{7}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -11/7 \\\\ -4/7 \\\\ 10/7 \\end{pmatrix}$.\n$\\mathbf{c}_D^T V^{-1} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} V^{-1} = \\begin{pmatrix} 1/3 & 1/3 & 0 \\end{pmatrix}$.\n$\\mathbf{c}_D^T V^{-1}(X - \\hat{x}_R \\mathbf{1}) = \\begin{pmatrix} 1/3 & 1/3 & 0 \\end{pmatrix} \\begin{pmatrix} -11/7 \\\\ -4/7 \\\\ 10/7 \\end{pmatrix} = -\\frac{11}{21} - \\frac{4}{21} = -\\frac{15}{21} = -\\frac{5}{7}$.\nSo, $\\hat{x}_D = \\hat{x}_R + (-\\frac{5}{7}) = \\frac{25}{7} - \\frac{5}{7} = \\frac{20}{7}$.\n\nNext, we calculate the conditional variance of the estimate for $x_D$. The formula is:\n$$\\text{Var}(x_D | X) = \\sigma^2 T_D - \\mathbf{c}_D^T V^{-1} \\mathbf{c}_D + \\frac{(1 - \\mathbf{1}^T V^{-1} \\mathbf{c}_D)^2}{\\mathbf{1}^T V^{-1} \\mathbf{1}}$$\nHere, $T_D$ is the time from the root to node $D$, which is $1$.\n$\\mathbf{c}_D^T V^{-1} \\mathbf{c}_D = \\begin{pmatrix} 1/3 & 1/3 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 2/3$.\n$\\mathbf{1}^T V^{-1} \\mathbf{c}_D = \\begin{pmatrix} 1/3 & 1/3 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 2/3$.\nPlugging these values into the variance formula:\n$\\text{Var}(x_D | X) = (1 \\times 1) - 2/3 + \\frac{(1 - 2/3)^2}{7/6} = 1/3 + \\frac{(1/3)^2}{7/6} = 1/3 + \\frac{1/9}{7/6} = 1/3 + \\frac{1}{9} \\cdot \\frac{6}{7} = 1/3 + \\frac{2}{21} = \\frac{7}{21} + \\frac{2}{21} = \\frac{9}{21} = \\frac{3}{7}$.\n\nThe maximum likelihood estimate for the state at node $D$ is $20/7$, and its conditional variance is $3/7$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{20}{7} & \\frac{3}{7} \\end{pmatrix}}$$", "id": "2545572"}, {"introduction": "Our final practice is a capstone exercise that challenges you to implement the engine of modern probabilistic ancestral reconstruction: Felsenstein’s pruning algorithm. You will build this algorithm from the ground up to analyze the joint evolution of two discrete traits, exploring the crucial difference between models of independent and correlated character evolution. This exercise bridges the gap between theoretical understanding and practical application, a vital skill for conducting your own comparative analyses [@problem_id:2545585].", "problem": "You are given a rooted phylogeny with five terminal taxa and two binary traits per taxon drawn from comparative zoology and botany: floral symmetry and specialized pollination. Floral symmetry is coded as actinomorphic ($A=0$) versus zygomorphic ($A=1$). Specialized pollination is coded as absent ($P=0$) versus present ($P=1$). The goal is to compute the posterior distribution over the joint ancestral states $(A,P)$ at the root under alternative continuous-time Markov chain models, starting from core definitions and principles of discrete-state Markov evolution on phylogenies and likelihood-based ancestral state reconstruction.\n\nThe fundamental base from which you must derive includes:\n- The definition of a continuous-time Markov chain (CTMC) on a finite state space, with a rate matrix $Q$ whose off-diagonal entries are nonnegative and whose rows sum to zero.\n- The Markov property and time-homogeneity, implying that for branch length $t$, the transition probability matrix is $P(t) = \\exp(Qt)$, where $\\exp$ denotes the matrix exponential.\n- The pruning recursion (often attributed to the work of Joseph Felsenstein) for computing the likelihood of observed tip states on a phylogeny by combining conditional likelihoods from children to parent via the transition probabilities along branches.\n\nThe phylogeny is rooted at node $R$ with the following structure and branch lengths (units are arbitrary time units and must be treated as dimensionless in all calculations):\n- $R$ has two children: internal nodes $I_1$ and $I_2$, with branch lengths $0.6$ and $0.7$ from $R$ to $I_1$ and $I_2$, respectively.\n- $I_1$ has two children: tips $S_1$ and $S_2$, with branch lengths $0.5$ and $0.5$ from $I_1$ to $S_1$ and $S_2$, respectively.\n- $I_2$ has two children: tip $S_3$ and internal node $I_3$, with branch lengths $0.4$ and $0.2$ from $I_2$ to $S_3$ and $I_3$, respectively.\n- $I_3$ has two children: tips $S_4$ and $S_5$, with branch lengths $0.3$ and $0.3$ from $I_3$ to $S_4$ and $S_5$, respectively.\n\nThe observed tip states are given as ordered pairs $(A,P)$, where $A \\in \\{0,1\\}$, $P \\in \\{0,1\\}$:\n- $S_1$: $(0,0)$,\n- $S_2$: $(0,1)$,\n- $S_3$: $(1,1)$,\n- $S_4$: $(1,0)$,\n- $S_5$: $(1,1)$.\n\nDefine the joint state index mapping as follows: the joint state $(A,P)$ is mapped to an index $s \\in \\{0,1,2,3\\}$ via $s = 2A + P$. Therefore, the canonical order of joint states is:\n- $s=0 \\equiv (A=0,P=0)$,\n- $s=1 \\equiv (A=0,P=1)$,\n- $s=2 \\equiv (A=1,P=0)$,\n- $s=3 \\equiv (A=1,P=1)$.\n\nYou must implement likelihood-based ancestral state reconstruction at the root under two model classes:\n- Independence model: traits $A$ and $P$ evolve independently on the tree. Each trait is a binary-state CTMC with rate matrix $Q_X$ for $X \\in \\{A,P\\}$ of the form\n$$\nQ_X = \\begin{pmatrix}\n-q_{01}^{(X)} & q_{01}^{(X)}\\\\\nq_{10}^{(X)} & -q_{10}^{(X)}\n\\end{pmatrix}.\n$$\nUnder independence, the joint process on the four states has transition matrix over time $t$ equal to $P_{\\text{joint}}(t) = \\exp\\!\\big((Q_A \\oplus Q_P)t\\big) = \\exp(Q_A t) \\otimes \\exp(Q_P t)$, where $\\oplus$ denotes the Kronecker sum and $\\otimes$ denotes the Kronecker product. The stationary distribution for the joint process is the Kronecker product of the stationary distributions of the marginals. For a binary $Q_X$ as above, the stationary distribution is $\\pi^{(X)} = \\left(\\frac{q_{10}^{(X)}}{q_{01}^{(X)} + q_{10}^{(X)}},\\ \\frac{q_{01}^{(X)}}{q_{01}^{(X)} + q_{10}^{(X)}}\\right)$.\n- Dependence model (correlated evolution in the sense of two-trait discrete evolution disallowing simultaneous changes): the joint process is a four-state CTMC with a $4 \\times 4$ rate matrix $Q$ that permits only single-trait changes at a time. Let the current state be $(A,P)$. Then the allowed transitions and their rates are:\n    - $(A,P) \\to (1-A,P)$ at rate $a_{01}^{(P)}$ if $A=0$ and $a_{10}^{(P)}$ if $A=1$ (that is, the rate of change in $A$ depends on the current value of $P$),\n    - $(A,P) \\to (A,1-P)$ at rate $p_{01}^{(A)}$ if $P=0$ and $p_{10}^{(A)}$ if $P=1$ (that is, the rate of change in $P$ depends on the current value of $A$).\nAll other off-diagonal entries are zero, and diagonal entries are set so rows sum to zero. The stationary distribution $\\pi$ is obtained by solving $\\pi^\\top Q = \\mathbf{0}^\\top$ with $\\sum_s \\pi_s = 1$.\n\nUse the pruning recursion as follows. For each tip with observed state $s$, assign a conditional likelihood vector $\\ell$ of length $4$ with $\\ell_s = 1$ and zeros elsewhere. For an internal node with children $c_1,\\dots,c_k$, the conditional likelihood vector at the node is computed as the element-wise product over children of the vectors $v^{(i)} = P_{\\text{joint}}(t_i)\\,\\ell^{(i)}$, where $\\ell^{(i)}$ is the conditional likelihood vector from child $c_i$. At the root, combine the root prior $\\pi$ (taken as the stationary distribution of the model) by element-wise multiplication with the root conditional likelihood vector and normalize to sum to $1$ to obtain the posterior distribution over the four joint states.\n\nTest suite. Implement and evaluate the posterior at the root for the following three parameter sets:\n- Case 1 (independence): use $q_{01}^{(A)} = 0.4$, $q_{10}^{(A)} = 0.1$, $q_{01}^{(P)} = 0.3$, $q_{10}^{(P)} = 0.2$.\n- Case 2 (dependence): use the eight rates\n    - $a_{01}^{(0)} = 0.05$, $a_{10}^{(0)} = 0.5$, $a_{01}^{(1)} = 0.6$, $a_{10}^{(1)} = 0.05$,\n    - $p_{01}^{(0)} = 0.05$, $p_{10}^{(0)} = 0.5$, $p_{01}^{(1)} = 0.6$, $p_{10}^{(1)} = 0.05$.\n- Case 3 (independence, symmetric boundary-like case): use $q_{01}^{(A)} = 0.2$, $q_{10}^{(A)} = 0.2$, $q_{01}^{(P)} = 0.2$, $q_{10}^{(P)} = 0.2$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list of three sublists, each sublist being the posterior distribution over the four joint states at the root in the canonical order $[ (0,0), (0,1), (1,0), (1,1) ]$. Each probability must be rounded to six decimal places. For example, the output format must look like\n\"[[p00_case1,p01_case1,p10_case1,p11_case1],[p00_case2,p01_case2,p10_case2,p11_case2],[p00_case3,p01_case3,p10_case3,p11_case3]]\"\nwith no spaces inserted anywhere in the string.", "solution": "The problem requires the computation of the posterior probability distribution of joint ancestral states at the root of a given phylogeny. This task is a standard application of likelihood-based ancestral state reconstruction using continuous-time Markov chain (CTMC) models of character evolution. The solution will be derived from first principles, beginning with the general framework of the pruning algorithm for phylogenetic likelihood calculation and then specializing it for the two specified models of evolution: an independence model and a dependence model.\n\nFirst, we formalize the problem components. The state space for the joint evolution of the two binary traits, floral symmetry ($A$) and specialized pollination ($P$), consists of four states. We map the state pair $(A, P)$ where $A, P \\in \\{0, 1\\}$ to an integer index $s \\in \\{0, 1, 2, 3\\}$ via the mapping $s = 2A + P$. This establishes a canonical ordering:\n- $s=0: (A=0, P=0)$\n- $s=1: (A=0, P=1)$\n- $s=2: (A=1, P=0)$\n- $s=3: (A=1, P=1)$\n\nThe evolution of these states along a branch of length $t$ is governed by a CTMC, characterized by a $4 \\times 4$ rate matrix $Q$. The matrix of transition probabilities over time $t$ is given by the matrix exponential $P(t) = \\exp(Qt)$. The entry $P_{ij}(t)$ is the probability that the state is $j$ at the end of the branch, given it was $i$ at the beginning.\n\nThe core of the calculation is the pruning algorithm, a dynamic programming method that computes the likelihood of observing the tip data given the model. For any node $u$ in the phylogeny, we compute a conditional likelihood vector $\\vec{L}_u$. The $i$-th component of this vector, $L_{u,i}$, is the probability of observing all the data in the clade descended from node $u$, given that the state at node $u$ is $i$.\n\nThe algorithm proceeds via a post-order traversal of the tree (from tips to root):\n1.  For a terminal (tip) node $u$ with observed state $s_{obs}$, the conditional likelihood vector $\\vec{L}_u$ is initialized as a vector of length $4$ with a $1$ at the position corresponding to $s_{obs}$ and $0$s elsewhere. That is, $L_{u,i} = \\delta_{i, s_{obs}}$, where $\\delta$ is the Kronecker delta.\n2.  For an internal node $u$ with two children $c_1$ and $c_2$, connected by branches of length $t_1$ and $t_2$ respectively, the conditional likelihood vector $\\vec{L}_u$ is computed from the vectors of its children, $\\vec{L}_{c_1}$ and $\\vec{L}_{c_2}$. For each possible state $i$ at node $u$, the conditional likelihood is the product of the likelihoods of the two sub-clades:\n    $$L_{u,i} = \\left( \\sum_{j=0}^{3} P_{ij}(t_1) L_{c_1, j} \\right) \\times \\left( \\sum_{k=0}^{3} P_{ik}(t_2) L_{c_2, k} \\right)$$\n    In vector notation, this is an element-wise product (Hadamard product, $\\odot$):\n    $$\\vec{L}_u = \\left( P(t_1) \\vec{L}_{c_1} \\right) \\odot \\left( P(t_2) \\vec{L}_{c_2} \\right)$$\n3.  This recursion continues until the root node $R$ is reached, yielding the final conditional likelihood vector $\\vec{L}_R$. The value $L_{R,i}$ is the likelihood of the entire dataset given the root is in state $i$.\n\nTo obtain the posterior probability of the root being in state $i$, we apply Bayes' theorem. The prior probability of the root state is assumed to be its stationary probability under the CTMC model, denoted by the vector $\\vec{\\pi}$. The marginal likelihood of the data is $P(\\text{data}) = \\sum_{j=0}^{3} \\pi_j L_{R,j} = \\vec{\\pi} \\cdot \\vec{L}_R$. The posterior probability distribution at the root, $\\vec{p}$, is then:\n$$p_i = P(\\text{state at root is } i | \\text{data}) = \\frac{\\pi_i L_{R,i}}{\\sum_{j=0}^{3} \\pi_j L_{R,j}}$$\nThis is equivalent to computing the element-wise product $\\vec{\\pi} \\odot \\vec{L}_R$ and normalizing the resulting vector to sum to $1$.\n\nThe specific phylogeny is: Root $R$ splits into $I_1$ (branch length $t_{R \\to I_1}=0.6$) and $I_2$ (branch length $t_{R \\to I_2}=0.7$). Node $I_1$ splits into tips $S_1$ and $S_2$ (both branch lengths $0.5$). Node $I_2$ splits into tip $S_3$ (branch length $0.4$) and internal node $I_3$ (branch length $0.2$). Node $I_3$ splits into tips $S_4$ and $S_5$ (both branch lengths $0.3$).\nThe tip states are $S_1:(0,0)$, $S_2:(0,1)$, $S_3:(1,1)$, $S_4:(1,0)$, $S_5:(1,1)$. The corresponding initial likelihood vectors are:\n$\\vec{L}_{S_1} = [1,0,0,0]^\\top$, $\\vec{L}_{S_2} = [0,1,0,0]^\\top$, $\\vec{L}_{S_3} = [0,0,0,1]^\\top$, $\\vec{L}_{S_4} = [0,0,1,0]^\\top$, and $\\vec{L}_{S_5} = [0,0,0,1]^\\top$.\n\nThe pruning computation proceeds as:\n1.  $\\vec{L}_{I_3} = (P(0.3) \\vec{L}_{S_4}) \\odot (P(0.3) \\vec{L}_{S_5})$\n2.  $\\vec{L}_{I_1} = (P(0.5) \\vec{L}_{S_1}) \\odot (P(0.5) \\vec{L}_{S_2})$\n3.  $\\vec{L}_{I_2} = (P(0.4) \\vec{L}_{S_3}) \\odot (P(0.2) \\vec{L}_{I_3})$\n4.  $\\vec{L}_{R} = (P(0.6) \\vec{L}_{I_1}) \\odot (P(0.7) \\vec{L}_{I_2})$\n\nWe now define the models.\n\n**Case 1 and 3: Independence Model**\nTraits $A$ and $P$ evolve independently. Their respective dynamics are governed by $2 \\times 2$ rate matrices:\n$$Q_A = \\begin{pmatrix} -q_{01}^{(A)} & q_{01}^{(A)} \\\\ q_{10}^{(A)} & -q_{10}^{(A)} \\end{pmatrix}, \\quad Q_P = \\begin{pmatrix} -q_{01}^{(P)} & q_{01}^{(P)} \\\\ q_{10}^{(P)} & -q_{10}^{(P)} \\end{pmatrix}$$\nThe stationary distributions are $\\pi^{(A)} = \\frac{1}{q_{01}^{(A)}+q_{10}^{(A)}} (q_{10}^{(A)}, q_{01}^{(A)})$ and $\\pi^{(P)} = \\frac{1}{q_{01}^{(P)}+q_{10}^{(P)}} (q_{10}^{(P)}, q_{01}^{(P)})$.\nThe joint stationary distribution is the Kronecker product $\\vec{\\pi} = \\pi^{(A)} \\otimes \\pi^{(P)}$.\nThe joint transition matrix is $P(t) = P_A(t) \\otimes P_P(t)$, where $P_X(t) = \\exp(Q_X t)$. For a $2 \\times 2$ matrix $Q_X$, the exponential can be calculated analytically. Let $\\lambda_X = q_{01}^{(X)} + q_{10}^{(X)}$. Then:\n$$P_X(t) = \\frac{1}{\\lambda_X} \\begin{pmatrix} q_{10}^{(X)} + q_{01}^{(X)}e^{-\\lambda_X t} & q_{01}^{(X)} - q_{01}^{(X)}e^{-\\lambda_X t} \\\\ q_{10}^{(X)} - q_{10}^{(X)}e^{-\\lambda_X t} & q_{01}^{(X)} + q_{10}^{(X)}e^{-\\lambda_X t} \\end{pmatrix}$$\nFor Case 1: $q_{01}^{(A)}=0.4, q_{10}^{(A)}=0.1, q_{01}^{(P)}=0.3, q_{10}^{(P)}=0.2$.\nFor Case 3: $q_{01}^{(A)}=0.2, q_{10}^{(A)}=0.2, q_{01}^{(P)}=0.2, q_{10}^{(P)}=0.2$.\n\n**Case 2: Dependence Model**\nThe joint process is a $4 \\times 4$ CTMC where only single-trait changes are allowed. The rates are context-dependent. From the problem description, the off-diagonal entries of the rate matrix $Q$ (in the order $0,1,2,3$) are:\n$Q_{0,1} = p_{01}^{(0)}$, $Q_{0,2} = a_{01}^{(0)}$\n$Q_{1,0} = p_{10}^{(0)}$, $Q_{1,3} = a_{01}^{(1)}$\n$Q_{2,0} = a_{10}^{(0)}$, $Q_{2,3} = p_{01}^{(1)}$\n$Q_{3,1} = a_{10}^{(1)}$, $Q_{3,2} = p_{10}^{(1)}$\nAll other off-diagonal entries are $0$. Diagonal entries are $Q_{i,i} = -\\sum_{j \\neq i} Q_{i,j}$.\nThe rates are: $a_{01}^{(0)}=0.05, a_{10}^{(0)}=0.5, a_{01}^{(1)}=0.6, a_{10}^{(1)}=0.05, p_{01}^{(0)}=0.05, p_{10}^{(0)}=0.5, p_{01}^{(1)}=0.6, p_{10}^{(1)}=0.05$.\nThe stationary distribution $\\vec{\\pi}$ is the normalized left eigenvector of $Q$ corresponding to the eigenvalue $0$, which can be found by solving $\\vec{\\pi}^\\top Q = \\vec{0}^\\top$ under the constraint $\\sum_i \\pi_i = 1$. The transition matrix $P(t) = \\exp(Qt)$ is computed numerically.\n\nThe procedure for each case is to:\n1.  Construct the appropriate rate matrix/matrices ($Q_A, Q_P$ or $Q$).\n2.  Compute the stationary distribution $\\vec{\\pi}$.\n3.  Define a function to compute $P(t)$ for any $t$.\n4.  Execute the pruning algorithm as detailed above to find $\\vec{L}_R$.\n5.  Compute the posterior $\\vec{p} = \\text{normalize}(\\vec{\\pi} \\odot \\vec{L}_R)$.\nThis procedure is implemented for each of the three test cases to obtain the final numerical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm, null_space\n\ndef solve():\n    \"\"\"\n    Solves the ancestral state reconstruction problem for the three specified cases.\n    \"\"\"\n\n    # --- Tree and Tip Data Definition ---\n    # The phylogeny can be represented as a post-order traversal list of nodes.\n    # Each node is a dict with name, children, and branch length to its parent.\n    # Root has no parent branch length.\n    tree_post_order = [\n        {'name': 'S1', 'children': [], 'blen': None},\n        {'name': 'S2', 'children': [], 'blen': None},\n        {'name': 'I1', 'children': ['S1', 'S2'], 'blen': 0.6},\n        {'name': 'S3', 'children': [], 'blen': None},\n        {'name': 'S4', 'children': [], 'blen': None},\n        {'name': 'S5', 'children': [], 'blen': None},\n        {'name': 'I3', 'children': ['S4', 'S5'], 'blen': 0.2},\n        {'name': 'I2', 'children': ['S3', 'I3'], 'blen': 0.7},\n        {'name': 'R',  'children': ['I1', 'I2'], 'blen': None},\n    ]\n    # Mapping node names to branch lengths leading to them.\n    # Special handling for children of the same parent having different branch lengths.\n    # Here, we can define branch lengths associated with children of a node.\n    child_blens = {\n        'I1': {'S1': 0.5, 'S2': 0.5},\n        'I2': {'S3': 0.4, 'I3': 0.2},\n        'I3': {'S4': 0.3, 'S5': 0.3},\n        'R':  {'I1': 0.6, 'I2': 0.7},\n    }\n\n    # Tip states: S1:(0,0), S2:(0,1), S3:(1,1), S4:(1,0), S5:(1,1)\n    # State mapping: s = 2*A + P => (0,0)->0, (0,1)->1, (1,0)->2, (1,1)->3\n    tip_states = {\n        'S1': 0, 'S2': 1, 'S3': 3, 'S4': 2, 'S5': 3\n    }\n    \n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            'type': 'independent',\n            'params': {'q01_A': 0.4, 'q10_A': 0.1, 'q01_P': 0.3, 'q10_P': 0.2}\n        },\n        {\n            'type': 'dependent',\n            'params': {\n                'a01_0': 0.05, 'a10_0': 0.5, 'a01_1': 0.6, 'a10_1': 0.05,\n                'p01_0': 0.05, 'p10_0': 0.5, 'p01_1': 0.6, 'p10_1': 0.05\n            }\n        },\n        {\n            'type': 'independent',\n            'params': {'q01_A': 0.2, 'q10_A': 0.2, 'q01_P': 0.2, 'q10_P': 0.2}\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        pi, p_matrix_func = get_model_components(case)\n        \n        # --- Pruning Algorithm ---\n        likelihoods = {}\n\n        # Initialize tip likelihoods\n        for tip_name, state_idx in tip_states.items():\n            l_vec = np.zeros(4)\n            l_vec[state_idx] = 1.0\n            likelihoods[tip_name] = l_vec\n\n        # Post-order traversal to compute internal node likelihoods\n        for node in tree_post_order:\n            if not node['children']: # It's a tip\n                continue\n            \n            # This is an internal node\n            child_name1, child_name2 = node['children']\n            blen1 = child_blens[node['name']][child_name1]\n            blen2 = child_blens[node['name']][child_name2]\n            \n            L_c1 = likelihoods[child_name1]\n            L_c2 = likelihoods[child_name2]\n            \n            P1 = p_matrix_func(blen1)\n            P2 = p_matrix_func(blen2)\n            \n            v1 = P1 @ L_c1\n            v2 = P2 @ L_c2\n            \n            likelihoods[node['name']] = v1 * v2 # Element-wise product\n\n        root_likelihood = likelihoods['R']\n        \n        # --- Posterior Calculation ---\n        raw_posterior = pi * root_likelihood\n        posterior = raw_posterior / np.sum(raw_posterior)\n        \n        all_results.append(np.round(posterior, 6).tolist())\n    \n    # Format output\n    output_str = f\"[[{','.join(map(str, all_results[0]))}],[{','.join(map(str, all_results[1]))}],[{','.join(map(str, all_results[2]))}]]\"\n    print(output_str.replace(\" \", \"\"))\n\n\ndef get_model_components(case_data):\n    \"\"\"\n    Constructs rate matrices, stationary distribution, and transition matrix function.\n    \"\"\"\n    model_type = case_data['type']\n    params = case_data['params']\n\n    if model_type == 'independent':\n        q01_A, q10_A = params['q01_A'], params['q10_A']\n        q01_P, q10_P = params['q01_P'], params['q10_P']\n\n        # Stationary distributions for A and P\n        pi_A_sum = q01_A + q10_A\n        pi_A = np.array([q10_A / pi_A_sum, q01_A / pi_A_sum]) if pi_A_sum > 0 else np.array([0.5, 0.5])\n        \n        pi_P_sum = q01_P + q10_P\n        pi_P = np.array([q10_P / pi_P_sum, q01_P / pi_P_sum]) if pi_P_sum > 0 else np.array([0.5, 0.5])\n        \n        # Joint stationary distribution (prior)\n        pi = np.kron(pi_A, pi_P)\n\n        def p_matrix_func_ind(t):\n            # Analytical exponential for 2x2 matrix\n            def expm_2x2(q01, q10, time):\n                lmbda = q01 + q10\n                if lmbda == 0:\n                    return np.identity(2)\n                p = np.zeros((2, 2))\n                exp_lt = np.exp(-lmbda * time)\n                p[0, 0] = (q10 + q01 * exp_lt) / lmbda\n                p[0, 1] = (q01 - q01 * exp_lt) / lmbda\n                p[1, 0] = (q10 - q10 * exp_lt) / lmbda\n                p[1, 1] = (q01 + q10 * exp_lt) / lmbda\n                return p\n\n            P_A_t = expm_2x2(q01_A, q10_A, t)\n            P_P_t = expm_2x2(q01_P, q10_P, t)\n            return np.kron(P_A_t, P_P_t)\n        \n        return pi, p_matrix_func_ind\n\n    elif model_type == 'dependent':\n        Q = np.zeros((4, 4))\n        # Off-diagonal elements\n        Q[0, 1] = params['p01_0']  # (0,0)->(0,1)\n        Q[0, 2] = params['a01_0']  # (0,0)->(1,0)\n        Q[1, 0] = params['p10_0']  # (0,1)->(0,0)\n        Q[1, 3] = params['a01_1']  # (0,1)->(1,1)\n        Q[2, 0] = params['a10_0']  # (1,0)->(0,0)\n        Q[2, 3] = params['p01_1']  # (1,0)->(1,1)\n        Q[3, 1] = params['a10_1']  # (1,1)->(0,1)\n        Q[3, 2] = params['p10_1']  # (1,1)->(1,0)\n        \n        # Diagonal elements\n        np.fill_diagonal(Q, -Q.sum(axis=1))\n        \n        # Stationary distribution (prior)\n        # Solve pi.T @ Q = 0; find null space of Q.T\n        ns = null_space(Q.T)\n        pi = ns[:, 0] / ns[:, 0].sum()\n        \n        memo_p_matrices = {}\n        def p_matrix_func_dep(t):\n            if t not in memo_p_matrices:\n                memo_p_matrices[t] = expm(Q * t)\n            return memo_p_matrices[t]\n\n        return pi, p_matrix_func_dep\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2545585"}]}