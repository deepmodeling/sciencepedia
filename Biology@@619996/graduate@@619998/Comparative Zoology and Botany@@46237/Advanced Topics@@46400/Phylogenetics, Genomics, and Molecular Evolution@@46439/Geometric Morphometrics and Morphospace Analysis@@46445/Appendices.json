{"hands_on_practices": [{"introduction": "After aligning landmark data via Generalized Procrustes Analysis, the first exploratory step is to characterize the primary patterns of shape variation in the sample. This practice guides you through the implementation of Principal Component Analysis (PCA), the core technique for reducing high-dimensional shape data into an interpretable low-dimensional \"morphospace\". By deriving the transformation from first principles and implementing the back-transformation from PC space to the original landmark coordinates ([@problem_id:2577650]), you will build the foundational skill of not just computing but also visualizing and interpreting the biological meaning of morphospaces.", "problem": "You are given a matrix of Procrustes-aligned landmark coordinates in two dimensions, where each specimen has been aligned by Generalized Procrustes Analysis (GPA) so that column-wise means are zero, and a separate vector giving the mean shape. The landmarks are stacked as coordinates in the order $\\left[x_1,y_1,x_2,y_2,\\dots,x_p,y_p\\right]$, so there are $2p$ columns. Your goal is to reconstruct the core steps of Principal Component Analysis (PCA) in geometric morphometrics and to implement a reversible transformation between the original landmark space and the Principal Component (PC) morphospace.\n\nFundamental base to use:\n- Orthogonal diagonalization of the empirical covariance matrix. For a centered data matrix $\\mathbf{Y}\\in\\mathbb{R}^{n\\times d}$ with $d=2p$, the empirical covariance is $\\mathbf{S}=\\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}$. Since $\\mathbf{S}$ is symmetric positive semidefinite, it admits an eigendecomposition $\\mathbf{S}=\\mathbf{V}\\,\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)\\,\\mathbf{V}^\\top$ with orthonormal columns $\\mathbf{v}_k$ and nonnegative eigenvalues $\\lambda_k$.\n- Coordinates in an orthonormal basis. For any centered vector $\\mathbf{y}\\in\\mathbb{R}^d$, its coordinates in the eigenbasis are the inner products with the basis vectors.\n\nTasks to implement (derive from these principles, without using any shortcut formulas presented to you):\n1. Compute the eigendecomposition of $\\mathbf{S}$ and order the eigenpairs so that $\\lambda_1\\ge \\lambda_2\\ge \\dots\\ge \\lambda_d\\ge 0$. Enforce a deterministic sign convention on each eigenvector $\\mathbf{v}_k$: find the index $j^\\star$ of the coordinate of largest absolute magnitude, and if $\\left(\\mathbf{v}_k\\right)_{j^\\star}<0$, multiply $\\mathbf{v}_k$ by $-1$.\n2. Form the PC scores for all specimens as the coordinates of each row of $\\mathbf{Y}$ expressed in the orthonormal eigenbasis $\\mathbf{V}$.\n3. Implement a back-transformation along a single PC axis $k$ to the original landmark space: given a mean shape vector $\\mathbf{m}\\in\\mathbb{R}^{d}$, an axis index $k$, and a scalar $t$ measured in units of standard deviations (that is, $t$ multiples of $\\sqrt{\\lambda_k}$), construct the deformed shape\n$$\n\\mathbf{x}(t)\\;=\\;\\mathbf{m}\\;+\\;t\\,\\sqrt{\\lambda_k}\\,\\mathbf{v}_k.\n$$\nThis deformation should be interpretable as moving $t$ standard deviations along axis $k$ in PC morphospace and back-transforming to the original landmark coordinates.\n\nAngle units do not apply in this problem. No physical units are involved. All outputs are dimensionless real numbers. When a rounding instruction is specified, round to $6$ decimal places.\n\nTest suite and required outputs:\nAdopt the column stacking order $\\left[x_1,y_1,x_2,y_2,\\dots\\right]$.\n\n- Test case A (happy path with two nonzero modes of variation):\n  - Let $n=4$, $p=3$, so $d=6$.\n  - Mean shape $\\mathbf{m}_A=\\left[0,0,1,0,0,1\\right]$.\n  - Data matrix $\\mathbf{Y}_A\\in\\mathbb{R}^{4\\times 6}$ with rows\n    - $\\left[0,0,-2,0,0,0\\right]$,\n    - $\\left[0,0,2,0,0,0\\right]$,\n    - $\\left[0,0,0,0,0,-1\\right]$,\n    - $\\left[0,0,0,0,0,1\\right]$.\n  - Use the first PC axis ($k=0$ after sorting by descending eigenvalue).\n  - For $t$ values $[0.0,\\,1.0,\\,-1.0,\\,2.5]$, back-transform to $\\mathbf{x}(t)$ and report the $x$-coordinate of landmark $2$ (this is the coordinate at index $2$ in the vector). Output a list of four floats, each rounded to $6$ decimal places.\n\n- Test case B (boundary-like check for per-landmark Euclidean effect):\n  - Let $n=2$, $p=2$, so $d=4$.\n  - Mean shape $\\mathbf{m}_B=\\left[2,2,0,0\\right]$.\n  - Data matrix $\\mathbf{Y}_B\\in\\mathbb{R}^{2\\times 4}$ with rows\n    - $\\left[\\frac{3}{\\sqrt{2}},\\frac{3}{\\sqrt{2}},0,0\\right]$,\n    - $\\left[-\\frac{3}{\\sqrt{2}},-\\frac{3}{\\sqrt{2}},0,0\\right]$.\n  - Use the first PC axis ($k=0$).\n  - Compute $\\mathbf{x}(+1)$ and $\\mathbf{x}(-1)$ and return the Euclidean distance between the two positions of landmark $1$ only, that is, the norm in $\\mathbb{R}^2$ using the $x_1,y_1$ entries. Return a single float rounded to $6$ decimal places.\n\n- Test case C (consistency of forward and backward transforms on a secondary axis):\n  - Reuse $\\left(\\mathbf{Y}_A,\\mathbf{m}_A\\right)$ from Test case A.\n  - Use the second PC axis ($k=1$ after sorting).\n  - Set $t=-1.5$. Construct $\\mathbf{x}(t)$ and compute the centered vector $\\mathbf{y}(t)=\\mathbf{x}(t)-\\mathbf{m}_A$. Project $\\mathbf{y}(t)$ onto axis $k$ (inner product with $\\mathbf{v}_k$) to obtain the scalar PC score on that axis. Return this scalar rounded to $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $\\left[\\text{result of A},\\text{result of B},\\text{result of C}\\right]$. The result of A is itself a list of four floats. For example, your output should look like $\\left[[a_1,a_2,a_3,a_4],b,c\\right]$, where each symbol stands for a numeric value as specified above.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The procedures described for Principal Component Analysis (PCA) in geometric morphometrics are standard and mathematically correct. We will proceed to solve the problem by following the specified tasks.\n\nThe core of the analysis is the eigendecomposition of the empirical covariance matrix $\\mathbf{S}$. Given a centered data matrix $\\mathbf{Y} \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of specimens and $d=2p$ is the number of landmark coordinates, the covariance matrix is defined as:\n$$\n\\mathbf{S} = \\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}\n$$\nThe eigenvalues $\\lambda_k$ of $\\mathbf{S}$ represent the variance of the data along the corresponding Principal Component (PC) axes, and the eigenvectors $\\mathbf{v}_k$ define the directions of these axes in the original $d$-dimensional landmark space. The eigenvectors form an orthonormal basis. The PC scores of a specimen with centered coordinates $\\mathbf{y}_i$ are its coordinates in this new basis, computed as inner products $\\mathbf{y}_i \\cdot \\mathbf{v}_k$.\n\nA shape can be reconstructed by moving along a specific PC axis $k$ from the mean shape $\\mathbf{m}$. A displacement of $t$ standard deviations along axis $k$ corresponds to adding the vector $t\\sqrt{\\lambda_k}\\mathbf{v}_k$ to the mean shape vector. The standard deviation of scores along axis $k$ is $\\sqrt{\\lambda_k}$. Thus, the reconstructed shape $\\mathbf{x}(t)$ is given by:\n$$\n\\mathbf{x}(t) = \\mathbf{m} + t\\sqrt{\\lambda_k}\\mathbf{v}_k\n$$\nWe will apply these principles to each test case.\n\n**Test Case A**\n\nGiven: a sample of $n=4$ specimens with $p=3$ landmarks, so the dimension is $d=2 \\times 3=6$.\nThe mean shape vector is $\\mathbf{m}_A = [0, 0, 1, 0, 0, 1]^\\top$.\nThe centered data matrix is:\n$$\n\\mathbf{Y}_A = \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix}\n$$\nFirst, we compute the matrix $\\mathbf{Y}_A^\\top\\mathbf{Y}_A$:\n$$\n\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix}^\\top \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2 \\end{pmatrix}\n$$\nThe covariance matrix $\\mathbf{S}_A$ is:\n$$\n\\mathbf{S}_A = \\frac{1}{4-1}\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\frac{1}{3}\\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8/3 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2/3 \\end{pmatrix}\n$$\nSince $\\mathbf{S}_A$ is a diagonal matrix, its eigenvalues are the diagonal entries and its eigenvectors are the standard basis vectors. Sorted in descending order, the non-zero eigenvalues and their corresponding eigenvectors are:\n$\\lambda_0 = 8/3$ with eigenvector $\\mathbf{v}_0 = [0, 0, 1, 0, 0, 0]^\\top$.\n$\\lambda_1 = 2/3$ with eigenvector $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$.\nThe remaining eigenvalues are $0$.\nThe sign convention requires checking the element of largest absolute magnitude. For both $\\mathbf{v}_0$ and $\\mathbf{v}_1$, this element is $1$ (a positive value), so their signs are not changed.\n\nWe use the first PC axis ($k=0$). The back-transformation is:\n$$\n\\mathbf{x}(t) = \\mathbf{m}_A + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [0, 0, 1, 0, 0, 1]^\\top + t\\sqrt{8/3}\\,[0, 0, 1, 0, 0, 0]^\\top = [0, 0, 1 + t\\sqrt{8/3}, 0, 0, 1]^\\top\n$$\nWe need to find the $x$-coordinate of landmark $2$, which is the element at index $2$ of $\\mathbf{x}(t)$. This coordinate is $1 + t\\sqrt{8/3}$.\nFor $t \\in [0.0, 1.0, -1.0, 2.5]$:\n- $t=0.0$: $1 + 0.0\\sqrt{8/3} = 1.0$\n- $t=1.0$: $1 + 1.0\\sqrt{8/3} \\approx 1 + 1.632993 = 2.632993$\n- $t=-1.0$: $1 - 1.0\\sqrt{8/3} \\approx 1 - 1.632993 = -0.632993$\n- $t=2.5$: $1 + 2.5\\sqrt{8/3} \\approx 1 + 4.082483 = 5.082483$\nThe results, rounded to $6$ decimal places, are $[1.000000, 2.632993, -0.632993, 5.082483]$.\n\n**Test Case B**\n\nGiven: $n=2$ specimens, $p=2$ landmarks, $d=4$.\nMean shape: $\\mathbf{m}_B = [2, 2, 0, 0]^\\top$.\nData matrix:\n$$\n\\mathbf{Y}_B = \\begin{pmatrix} 3/\\sqrt{2} & 3/\\sqrt{2} & 0 & 0 \\\\ -3/\\sqrt{2} & -3/\\sqrt{2} & 0 & 0 \\end{pmatrix}\n$$\nThe covariance matrix is $\\mathbf{S}_B = \\frac{1}{2-1}\\mathbf{Y}_B^\\top\\mathbf{Y}_B = \\mathbf{Y}_B^\\top\\mathbf{Y}_B$:\n$$\n\\mathbf{S}_B = \\begin{pmatrix} 3/\\sqrt{2} & -3/\\sqrt{2} \\\\ 3/\\sqrt{2} & -3/\\sqrt{2} \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3/\\sqrt{2} & 3/\\sqrt{2} & 0 & 0 \\\\ -3/\\sqrt{2} & -3/\\sqrt{2} & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 9 & 9 & 0 & 0 \\\\ 9 & 9 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of the upper-left $2\\times2$ block $\\begin{pmatrix} 9 & 9 \\\\ 9 & 9 \\end{pmatrix}$ are found from the characteristic equation $(9-\\lambda)^2 - 81 = 0$, which yields $\\lambda = 18$ and $\\lambda = 0$.\nThe largest eigenvalue is $\\lambda_0 = 18$. The corresponding eigenvector $\\mathbf{v}_0 = [c_1, c_2, 0, 0]^\\top$ satisfies $(9-18)c_1 + 9c_2 = 0$, or $-9c_1 + 9c_2 = 0$, implying $c_1=c_2$. For normalization, $c_1^2 + c_2^2 = 1 \\implies 2c_1^2 = 1 \\implies c_1 = 1/\\sqrt{2}$. Thus, $\\mathbf{v}_0 = [1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top$.\nThe element with maximum absolute value is positive, so no sign change is needed.\n\nThe first PC axis ($k=0$) is used. The back-transformation is:\n$$\n\\mathbf{x}(t) = \\mathbf{m}_B + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [2, 2, 0, 0]^\\top + t\\sqrt{18}\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top\n$$\nSince $\\sqrt{18} = 3\\sqrt{2}$, this simplifies to:\n$$\n\\mathbf{x}(t) = [2, 2, 0, 0]^\\top + t(3\\sqrt{2})\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top = [2, 2, 0, 0]^\\top + t[3, 3, 0, 0]^\\top = [2+3t, 2+3t, 0, 0]^\\top\n$$\nWe compute $\\mathbf{x}(+1)$ and $\\mathbf{x}(-1)$:\n$\\mathbf{x}(+1) = [2+3, 2+3, 0, 0]^\\top = [5, 5, 0, 0]^\\top$.\n$\\mathbf{x}(-1) = [2-3, 2-3, 0, 0]^\\top = [-1, -1, 0, 0]^\\top$.\nThe coordinates of landmark $1$ are the first two elements. For $t=+1$, the position is $(5, 5)$. For $t=-1$, it is $(-1, -1)$.\nThe Euclidean distance between these two positions is:\n$$\nd = \\sqrt{(5 - (-1))^2 + (5 - (-1))^2} = \\sqrt{6^2 + 6^2} = \\sqrt{72} \\approx 8.48528137\n$$\nRounding to $6$ decimal places gives $8.485281$.\n\n**Test Case C**\n\nWe reuse the data and eigendecomposition from Case A. We use the second PC axis ($k=1$), with eigenvalue $\\lambda_1 = 2/3$ and eigenvector $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$. We are given $t=-1.5$.\nWe first construct the shape $\\mathbf{x}(t)$:\n$$\n\\mathbf{x}(-1.5) = \\mathbf{m}_A + (-1.5)\\sqrt{\\lambda_1}\\,\\mathbf{v}_1\n$$\nNext, we find the centered vector $\\mathbf{y}(t) = \\mathbf{x}(t) - \\mathbf{m}_A$:\n$$\n\\mathbf{y}(-1.5) = (\\mathbf{m}_A - 1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) - \\mathbf{m}_A = -1.5\\sqrt{\\lambda_1}\\mathbf{v}_1\n$$\nThe problem asks for the PC score, which is the projection of this vector onto the axis $\\mathbf{v}_1$. This is calculated as the inner product $\\mathbf{y}(-1.5) \\cdot \\mathbf{v}_1$:\n$$\n\\text{Score} = (-1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) \\cdot \\mathbf{v}_1 = -1.5\\sqrt{\\lambda_1}(\\mathbf{v}_1 \\cdot \\mathbf{v}_1)\n$$\nSince $\\mathbf{v}_1$ is a unit vector, $\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 1$. The score simplifies to:\n$$\n\\text{Score} = -1.5\\sqrt{\\lambda_1} = -1.5\\sqrt{2/3} \\approx -1.5 \\times 0.81649658 \\approx -1.22474487\n$$\nThis demonstrates consistency: a shape constructed by moving $t$ standard deviations along axis $k$ has a PC score of exactly $t\\sqrt{\\lambda_k}$ on that axis.\nRounding to $6$ decimal places gives $-1.224745$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the geometric morphometrics problem by implementing PCA steps\n    and back-transformation for three test cases.\n    \"\"\"\n\n    def perform_pca_and_get_components(Y, n):\n        \"\"\"\n        Computes covariance matrix, its eigendecomposition, and sorts\n        and normalizes the components according to the problem statement.\n        \"\"\"\n        d = Y.shape[1]\n        \n        # Handle the case n=1 where variance is undefined, though not in test cases\n        if n <= 1:\n            return np.zeros(d), np.eye(d)\n\n        # 1. Compute the covariance matrix S\n        S = (1 / (n - 1)) * (Y.T @ Y)\n\n        # 2. Eigendecomposition of S. eigh is for symmetric matrices.\n        # It returns eigenvalues in ascending order.\n        eigvals, eigvecs = np.linalg.eigh(S)\n\n        # 3. Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigvals)[::-1]\n        sorted_eigvals = eigvals[sorted_indices]\n        sorted_eigvecs = eigvecs[:, sorted_indices]\n        \n        # 4. Enforce deterministic sign convention on eigenvectors\n        for k in range(d):\n            vk = sorted_eigvecs[:, k]\n            max_abs_idx = np.argmax(np.abs(vk))\n            if vk[max_abs_idx] < 0:\n                sorted_eigvecs[:, k] = -vk\n        \n        return sorted_eigvals, sorted_eigvecs\n\n    # --- Test Case A ---\n    n_A = 4\n    m_A = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0])\n    Y_A = np.array([\n        [0.0, 0.0, -2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, -1.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n    ])\n    \n    eigvals_A, eigvecs_A = perform_pca_and_get_components(Y_A, n_A)\n    \n    # First PC axis (k=0)\n    lambda0_A = eigvals_A[0]\n    v0_A = eigvecs_A[:, 0]\n    \n    t_values_A = [0.0, 1.0, -1.0, 2.5]\n    results_A = []\n    \n    for t in t_values_A:\n        # Back-transform to shape coordinates\n        x_t = m_A + t * np.sqrt(lambda0_A) * v0_A\n        # Get x-coordinate of landmark 2 (index 2)\n        coord = x_t[2]\n        results_A.append(round(coord, 6))\n\n    # --- Test Case B ---\n    n_B = 2\n    m_B = np.array([2.0, 2.0, 0.0, 0.0])\n    Y_B = np.array([\n        [3/np.sqrt(2), 3/np.sqrt(2), 0.0, 0.0],\n        [-3/np.sqrt(2), -3/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    eigvals_B, eigvecs_B = perform_pca_and_get_components(Y_B, n_B)\n\n    # First PC axis (k=0)\n    lambda0_B = eigvals_B[0]\n    v0_B = eigvecs_B[:, 0]\n\n    # Compute shapes at t=+1 and t=-1\n    x_plus_1 = m_B + 1.0 * np.sqrt(lambda0_B) * v0_B\n    x_minus_1 = m_B + (-1.0) * np.sqrt(lambda0_B) * v0_B\n    \n    # Landmark 1 coordinates are at indices 0 and 1\n    lm1_plus_1 = x_plus_1[:2]\n    lm1_minus_1 = x_minus_1[:2]\n    \n    # Euclidean distance between the two landmark 1 positions\n    dist_B = np.linalg.norm(lm1_plus_1 - lm1_minus_1)\n    result_B = round(dist_B, 6)\n\n    # --- Test Case C ---\n    \n    # Reuse PCA results from Case A\n    # Second PC axis (k=1)\n    lambda1_A = eigvals_A[1]\n    v1_A = eigvecs_A[:, 1]\n    \n    t_C = -1.5\n    \n    # The centered vector y(t) represents the deviation from the mean\n    y_t = t_C * np.sqrt(lambda1_A) * v1_A\n    \n    # Project y(t) onto the PC axis v1 to get the score.\n    # Mathematically, this simplifies to t * sqrt(lambda_k).\n    score_C = np.dot(y_t, v1_A)\n    result_C = round(score_C, 6)\n\n    # --- Final Output Formatting ---\n    # The required format is [[a1, a2, a3, a4], b, c]\n    # To match the no-space format in the list for A:\n    str_A = f\"[{','.join(f'{x:.6f}' for x in results_A)}]\"\n    str_B = f'{result_B:.6f}'\n    str_C = f'{result_C:.6f}'\n\n    print(f\"[{str_A},{str_B},{str_C}]\")\n\nsolve()\n\n```", "id": "2577650"}, {"introduction": "Once major axes of variation are identified, a central goal in biology is to test hypotheses about the factors driving this variation, such as the relationship between size and shape (allometry). This practice moves from description to modeling, challenging you to implement a robust procedure for detecting and characterizing potentially nonlinear allometric trajectories. You will fit and compare a series of regression models—from simple linear to more complex polynomial and spline functions—using the logarithm of Centroid Size, $\\ln(CS)$, as a predictor of shape ([@problem_id:2577673]). Mastering model selection via information criteria and validating your findings with rigorous permutation tests are key skills for testing sophisticated evolutionary and developmental hypotheses.", "problem": "You will implement a complete and testable procedure for detecting and modeling nonlinear allometry in a morphometric context using projections of shape variation and a size proxy. In geometric morphometrics, Centroid Size (CS) is a common size measure, and allometry is typically modeled as shape variation as a function of the logarithm of Centroid Size. For the purposes of this task, consider a simplified but scientifically sound setting in which shape variation is summarized by a small number of orthogonal shape scores (for example, the first few Principal Components (PC) of Procrustes-aligned landmark configurations), and the predictor is the logarithm of Centroid Size. The acronym CS stands for Centroid Size, PC stands for Principal Component, and AIC stands for Akaike Information Criterion.\n\nStarting from the following fundamental basis:\n- Shape scores after Generalized Procrustes Analysis can be modeled as a multivariate response $Y \\in \\mathbb{R}^{n \\times p}$ for $n$ specimens and $p$ shape axes.\n- The predictor is $L = \\log(CS) \\in \\mathbb{R}^{n}$.\n- Multivariate multiple regression relates $Y$ to a design matrix $X \\in \\mathbb{R}^{n \\times k}$ by $Y = X B + E$, where $B \\in \\mathbb{R}^{k \\times p}$ is the coefficient matrix and $E$ are residuals.\n- Under the well-tested assumption of multivariate normal residuals with constant covariance across specimens, model comparison can be conducted using likelihood-based information criteria and nonparametric permutation tests.\n- A regression spline basis can be constructed from $L$ using truncated linear (hinge) functions at interior knots $k_{j}$, where $(x)_{+} = \\max(0, x)$.\n\nYour task is to write a program that:\n1. Simulates multivariate shape data $Y$ under specified generative models of allometry with $L = \\log(CS)$ as input, where $L$ is sampled uniformly on a specified interval and $Y$ is produced by adding Gaussian noise to a deterministic function of $L$. The deterministic mean function can include linear, polynomial, and spline terms of $L$ projected along orthonormal directions in shape space.\n2. Fits four candidate multivariate models to $Y$ as functions of $L$:\n   - Model $1$ (Linear): columns $[1, L]$,\n   - Model $2$ (Quadratic): columns $[1, L, L^{2}]$,\n   - Model $3$ (Cubic): columns $[1, L, L^{2}, L^{3}]$,\n   - Model $4$ (Spline): columns $[1, L, (L - k_{1})_{+}, (L - k_{2})_{+}]$, where $k_{1}$ and $k_{2}$ are the empirical $33.3$rd and $66.7$th percentiles of $L$.\n3. Computes a likelihood-based information criterion derived from the multivariate normal model for each fitted model and selects a model by the following rule: identify the minimum criterion value across the four models, then choose the simplest model (fewest columns in $X$) among those within $\\Delta \\leq 2$ units of the minimum.\n4. Performs a permutation test for added nonlinearity beyond the linear model using the Freedman–Lane residual permutation framework: for the selected best nonlinear model (if the selected model is Model $2$, Model $3$, or Model $4$), compute a test statistic as the multivariate sum of squares explained by the added nonlinear terms after residualizing both the response and the nonlinear design with respect to the reduced linear model; then approximate the null distribution by permuting the reduced-model residuals across specimens $B$ times and recomputing the statistic each time. Compute the $P$-value as a Monte Carlo tail probability. If the selected model is linear, report a $P$-value of $1.0$.\n5. Produces final outputs for a predefined test suite of parameter sets. Each test case output must be a two-element list $[m, p]$, where $m$ is the selected model identifier as an integer in $\\{1,2,3,4\\}$ and $p$ is the permutation $P$-value as a float rounded to six decimals. The final program output must be a single line containing a list of these per-case results, i.e., a JSON-like list of two-element lists enclosed in square brackets.\n\nMathematical and algorithmic requirements:\n- The derivation of the information criterion must start from the multivariate normal likelihood for $E$ with constant column covariance and maximum-likelihood estimators under that model.\n- The permutation test must specifically assess the extra fit provided by the nonlinear terms beyond the linear model, using residualization that conditions on the reduced linear design (Freedman–Lane) and a statistic that is the Frobenius-norm squared of the projection of residualized $Y$ onto the residualized nonlinear design.\n- Angles are not involved. No physical units are involved. No percentages are to be reported; the $P$-value must be a decimal number.\n\nTest suite:\nFor reproducibility, each case specifies a seed for the random number generator. For all cases, let $(x)_{+} = \\max(0, x)$ and compute spline knots $k_{1}$ and $k_{2}$ as the empirical quantiles at $q = 1/3$ and $q = 2/3$ respectively from the realized $L$ values.\n\n- Case $1$ (happy path, linear allometry):\n  - Seed: $1729$,\n  - $n = 120$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.5, 3.0)$,\n  - Deterministic mean: linear effect amplitude $0.6$ along one orthonormal direction, no higher-order terms,\n  - Noise: independent, zero-mean Gaussian with standard deviation $\\sigma = 0.05$ in each of the $p$ dimensions.\n\n- Case $2$ (nonlinear allometry with curvature):\n  - Seed: $20211$,\n  - $n = 140$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.3, 3.1)$,\n  - Deterministic mean: quadratic effect amplitude $0.5$ along an orthonormal direction, negligible linear effect,\n  - Noise: $\\sigma = 0.05$.\n\n- Case $3$ (higher-order nonlinearity):\n  - Seed: $314159$,\n  - $n = 160$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.3, 3.1)$,\n  - Deterministic mean: cubic effect amplitude $0.35$ along an orthonormal direction,\n  - Noise: $\\sigma = 0.05$.\n\n- Case $4$ (piecewise allometry captured by spline):\n  - Seed: $271828$,\n  - $n = 150$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.4, 3.0)$,\n  - Deterministic mean: spline-like effect with two hinge terms $(L - k_{1})_{+}$ and $(L - k_{2})_{+}$ with amplitudes $0.8$ and $-0.6$ along orthonormal directions, and a small baseline linear effect of amplitude $0.1$,\n  - Noise: $\\sigma = 0.06$.\n\nImplementation details:\n- Construct the orthonormal effect directions by applying a QR decomposition to a random Gaussian matrix in $\\mathbb{R}^{p \\times p}$ and using the $p$ orthonormal columns.\n- For the spline model, use a truncated linear basis with two hinges at $k_{1}$ and $k_{2}$ as defined above.\n- Use $B = 499$ permutations for the permutation test in all cases.\n- For numerical stability when evaluating the information criterion, if the estimated residual covariance matrix is nearly singular, it is acceptable to add a small ridge term $\\epsilon I$ with $\\epsilon = 10^{-9}$ before computing its log-determinant.\n\nYour program should produce a single line of output containing the results as a JSON-like list of per-case results, where each per-case result is a two-element list $[m, p]$ as specified above (for example, $[[1,0.842000],[2,0.010000],\\dots]$). Ensure the $P$-value is rounded to six decimals in the output.", "solution": "We begin from the standard multivariate multiple regression model that arises in geometric morphometrics when modeling shape as a function of a covariate such as the logarithm of Centroid Size (CS). Let $Y \\in \\mathbb{R}^{n \\times p}$ be the matrix of $p$ shape scores for $n$ specimens, which can be conceived as the leading Principal Components (PC) of Procrustes-aligned shape coordinates. Let $L \\in \\mathbb{R}^{n}$ be the predictor vector with entries $L_{i} = \\log(CS_{i})$. A generic linear model is\n$$\nY = X B + E,\n$$\nwhere $X \\in \\mathbb{R}^{n \\times k}$ is a design matrix, $B \\in \\mathbb{R}^{k \\times p}$ is the coefficient matrix, and $E \\in \\mathbb{R}^{n \\times p}$ are residuals. We assume that the rows of $E$ are independent and identically distributed as multivariate normal with zero mean and constant column covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n\nWe consider four candidate models corresponding to different design matrices:\n- Model $1$: $X = [\\mathbf{1}, L]$, which has $k = 2$ columns.\n- Model $2$: $X = [\\mathbf{1}, L, L^{2}]$, which has $k = 3$ columns.\n- Model $3$: $X = [\\mathbf{1}, L, L^{2}, L^{3}]$, which has $k = 4$ columns.\n- Model $4$: $X = [\\mathbf{1}, L, (L - k_{1})_{+}, (L - k_{2})_{+}]$, a truncated linear spline basis with $k = 4$ columns, where $k_{1}$ and $k_{2}$ are interior knots at the empirical quantiles $q = 1/3$ and $q = 2/3$, and $(x)_{+} = \\max(0, x)$.\n\nFor each model, the maximum likelihood estimator of $B$ under the multivariate normal residual assumption is\n$$\n\\hat{B} = (X^{\\top} X)^{-1} X^{\\top} Y,\n$$\nand the residual matrix is\n$$\n\\hat{E} = Y - X \\hat{B}.\n$$\nThe maximum likelihood estimator of the column covariance is\n$$\n\\hat{\\Sigma} = \\frac{1}{n}\\, \\hat{E}^{\\top} \\hat{E}.\n$$\nThe maximized log-likelihood for the model with design $X$ under the matrix-normal model with identity row covariance and column covariance $\\Sigma$ evaluates to\n$$\n\\hat{\\ell} = -\\frac{n}{2}\\left[ p \\log(2\\pi) + \\log\\left|\\hat{\\Sigma}\\right| + p \\right],\n$$\nafter substituting $\\hat{\\Sigma}$ into the likelihood. The Akaike Information Criterion (AIC) is defined by\n$$\n\\mathrm{AIC} = 2 \\, \\#\\text{parameters} - 2 \\hat{\\ell}.\n$$\nIn this multivariate setting, the number of parameters is the sum of the regression parameters and the free parameters in the symmetric covariance matrix, namely $k p + \\frac{p(p+1)}{2}$. Because $p$ is the same across candidate models, the additive term for the covariance cancels in AIC differences, but for completeness and correctness we include it in the calculation. Numerically, to evaluate $\\log|\\hat{\\Sigma}|$ stably we compute a sign–log-determinant pair by $\\mathrm{slogdet}(\\hat{\\Sigma})$ and, in the event of near-singularity, add a small ridge term $\\epsilon I$ with $\\epsilon = 10^{-9}$ to $\\hat{\\Sigma}$.\n\nModel selection rule: compute $\\mathrm{AIC}$ for each of the four models, find $\\mathrm{AIC}_{\\min}$, and then select the simplest model (fewest columns $k$ in $X$) among those with $\\mathrm{AIC} \\leq \\mathrm{AIC}_{\\min} + 2$.\n\nPermutation test for nonlinearity beyond linear: To test the additional contribution of the nonlinear terms beyond Model $1$ (linear), we use the Freedman–Lane residual permutation framework. Let $X_{r} = [\\mathbf{1}, L]$ be the reduced linear design with projection matrix\n$$\nP_{r} = X_{r} (X_{r}^{\\top} X_{r})^{-1} X_{r}^{\\top}, \\quad M_{r} = I - P_{r}.\n$$\nLet the selected best nonlinear model have design $X_{f} = [X_{r}, Z]$, where $Z$ contains the added nonlinear columns, such as $L^{2}$ and $L^{3}$ for the polynomial models, or the hinge functions for the spline. Residualize both the response and the added nonlinear design:\n$$\nY_{r} = M_{r} Y, \\quad Z_{r} = M_{r} Z.\n$$\nDefine the projection onto the column space of $Z_{r}$ by\n$$\nP_{Z} = Z_{r} (Z_{r}^{\\top} Z_{r})^{-1} Z_{r}^{\\top}.\n$$\nA natural multivariate test statistic for the extra fit conferred by the added terms is the Frobenius-norm squared of the projection of the residualized response:\n$$\nT_{\\mathrm{obs}} = \\| P_{Z} Y_{r} \\|_{F}^{2} = \\mathrm{trace}\\left( Y_{r}^{\\top} P_{Z}^{\\vphantom{\\top}} Y_{r} \\right).\n$$\nTo generate a null distribution that preserves the structure conditioned on the reduced model, permute the rows of $Y_{r}$ across specimens. For each permutation $\\pi$ of $\\{1,\\dots,n\\}$, form $Y_{r}^{(\\pi)}$ by permuting the rows of $Y_{r}$ accordingly, compute\n$$\nT^{(\\pi)} = \\| P_{Z} Y_{r}^{(\\pi)} \\|_{F}^{2},\n$$\nand repeat for $B$ permutations to construct the empirical null distribution. The Monte Carlo one-sided $P$-value is\n$$\np = \\frac{1 + \\#\\{ T^{(\\pi)} \\geq T_{\\mathrm{obs}} \\}}{1 + B}.\n$$\nIf the selected model is linear, there are no added nonlinear terms to test, and we report $p = 1.0$ by convention.\n\nData generation for the test suite: For each case, we generate $L$ uniformly on a specified interval, build an orthonormal basis $V = [v_{1}, v_{2}, v_{3}] \\in \\mathbb{R}^{p \\times p}$ by applying a QR decomposition to a random Gaussian matrix, and define a deterministic mean function as a sum of terms along specific basis vectors, for example $v_{1} L$, $v_{2} L^{2}$, $v_{3} L^{3}$, or hinge functions $(L - k_{j})_{+}$ multiplied by fixed amplitudes. The observed $Y$ is the deterministic mean plus Gaussian noise with variance $\\sigma^{2}$ per dimension. The spline knots $k_{1}$ and $k_{2}$ are computed empirically from the realized $L$ values for each case, at quantiles $q = 1/3$ and $q = 2/3$. Seeds specified in the test suite initialize the pseudorandom number generator for reproducibility.\n\nAlgorithmic implementation:\n- For each case, simulate $(Y, L)$ as above with the provided seed, $n$, $p$, $L$ range, amplitudes, and $\\sigma$.\n- For each of the four candidate models, form $X$, compute $\\hat{B}$, $\\hat{E}$, $\\hat{\\Sigma}$, and $\\mathrm{AIC}$ with the multivariate normal log-likelihood at the maximum.\n- Select a model by the $\\Delta \\leq 2$ parsimony rule relative to the minimal $\\mathrm{AIC}$.\n- If the selected model is nonlinear, construct $Z$ and compute the Freedman–Lane $P$-value with $B = 499$ permutations; otherwise set $p = 1.0$.\n- Report for each case the pair $[m, p]$, where $m \\in \\{1,2,3,4\\}$ is the selected model identifier and $p$ is rounded to six decimals.\n- Print a single line containing the list of these per-case results, enclosed in square brackets.\n\nThis procedure operationalizes a principled test for nonlinear allometry by augmenting linear models with polynomial or spline terms of $L = \\log(CS)$ and justifying model selection using a likelihood-based information criterion and a permutation test that directly evaluates the incremental contribution of nonlinearity beyond the linear effect. The simulation settings in the four cases provide coverage of a linear “happy path,” quadratic curvature, higher-order cubic behavior, and piecewise spline-like allometry, while also enforcing reproducibility via fixed seeds and clearly defined parameters.", "answer": "```python\nimport numpy as np\n\n# No other external libraries are used; SciPy is not required for this implementation.\n\ndef orthonormal_basis(p, rng):\n    \"\"\"Generate an orthonormal basis in R^p via QR decomposition.\"\"\"\n    A = rng.normal(size=(p, p))\n    Q, _ = np.linalg.qr(A)\n    # Ensure right-handed orientation for determinism if desired\n    return Q\n\ndef design_matrices(L, model_id, knots=None):\n    \"\"\"Construct design matrix X for given model_id.\n    model_id: 1 linear, 2 quadratic, 3 cubic, 4 spline (linear truncated at knots).\"\"\"\n    n = L.shape[0]\n    ones = np.ones((n, 1))\n    Lc = L.reshape(-1, 1)\n    if model_id == 1:\n        X = np.hstack([ones, Lc])\n    elif model_id == 2:\n        X = np.hstack([ones, Lc, Lc**2])\n    elif model_id == 3:\n        X = np.hstack([ones, Lc, Lc**2, Lc**3])\n    elif model_id == 4:\n        if knots is None or len(knots) != 2:\n            raise ValueError(\"Knots required for spline model.\")\n        k1, k2 = knots\n        h1 = np.maximum(0.0, Lc - k1)\n        h2 = np.maximum(0.0, Lc - k2)\n        X = np.hstack([ones, Lc, h1, h2])\n    else:\n        raise ValueError(\"Unknown model_id\")\n    return X\n\ndef multivariate_aic(Y, X, ridge=1e-9):\n    \"\"\"Compute AIC for multivariate multiple regression under MVN residuals.\"\"\"\n    n, p = Y.shape\n    # OLS coefficients\n    XtX = X.T @ X\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        XtX_inv = np.linalg.pinv(XtX)\n    B_hat = XtX_inv @ X.T @ Y\n    E = Y - X @ B_hat\n    # Residual covariance MLE\n    Sigma_hat = (E.T @ E) / n\n    # Stabilize if necessary\n    try:\n        sign, logdet = np.linalg.slogdet(Sigma_hat)\n        if sign <= 0:\n            raise np.linalg.LinAlgError\n    except np.linalg.LinAlgError:\n        Sigma_hat = Sigma_hat + ridge * np.eye(p)\n        sign, logdet = np.linalg.slogdet(Sigma_hat)\n        if sign <= 0:\n            # As a last resort, use small positive diagonal\n            Sigma_hat = ridge * np.eye(p)\n            sign, logdet = 1.0, p * np.log(ridge)\n    # Maximized log-likelihood\n    ll = - (n / 2.0) * (p * np.log(2.0 * np.pi) + logdet + p)\n    # Parameter count: regression + covariance (symmetric)\n    k = X.shape[1]\n    param_count = k * p + (p * (p + 1)) // 2\n    aic = 2.0 * param_count - 2.0 * ll\n    return aic, B_hat, E\n\ndef freedman_lane_pvalue(Y, L, best_model_id, B=499, knots=None, seed=0):\n    \"\"\"Permutation p-value for added nonlinearity beyond linear using Freedman-Lane.\"\"\"\n    n, p = Y.shape\n    # Reduced model (linear)\n    Xr = design_matrices(L, 1)\n    # Full model components\n    if best_model_id == 1:\n        return 1.0  # No nonlinearity to test\n    Xf = design_matrices(L, best_model_id, knots=knots)\n    # Identify added columns Z beyond reduced Xr\n    # For model 2: added L^2; model 3: added L^2, L^3; model 4: added hinges h1,h2\n    Z = Xf[:, Xr.shape[1]:]\n    # Residualize both Y and Z w.r.t. Xr\n    XtX = Xr.T @ Xr\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        XtX_inv = np.linalg.pinv(XtX)\n    Pr = Xr @ XtX_inv @ Xr.T\n    Mr = np.eye(n) - Pr\n    Yr = Mr @ Y\n    Zr = Mr @ Z\n    # If Zr is rank-deficient or zero, return p=1.0\n    ZtZ = Zr.T @ Zr\n    # Handle degeneracy\n    try:\n        ZtZ_inv = np.linalg.inv(ZtZ)\n    except np.linalg.LinAlgError:\n        # If singular, use pseudo-inverse; if near-zero, treat as no added effect\n        if np.allclose(ZtZ, 0.0):\n            return 1.0\n        ZtZ_inv = np.linalg.pinv(ZtZ)\n    Pz = Zr @ ZtZ_inv @ Zr.T\n    # Observed statistic\n    T_obs = np.linalg.norm(Pz @ Yr, ord='fro')**2\n    # Permutations\n    rng = np.random.default_rng(seed)\n    count_ge = 0\n    for _ in range(B):\n        perm = rng.permutation(n)\n        Yperm = Yr[perm, :]\n        T_perm = np.linalg.norm(Pz @ Yperm, ord='fro')**2\n        if T_perm >= T_obs - 1e-12:\n            count_ge += 1\n    pval = (1 + count_ge) / (1 + B)\n    return pval\n\ndef simulate_case(case_params):\n    \"\"\"Simulate Y and L for one case per provided parameters and amplitudes.\"\"\"\n    seed = case_params['seed']\n    n = case_params['n']\n    p = case_params['p']\n    Lmin, Lmax = case_params['L_range']\n    sigma = case_params['sigma']\n    truth = case_params['truth']  # dict with amplitudes and type info\n    rng = np.random.default_rng(seed)\n\n    # Generate L uniformly\n    L = rng.uniform(Lmin, Lmax, size=n)\n    # Orthonormal basis for effects\n    V = orthonormal_basis(p, rng)\n    v1, v2, v3 = V[:, 0], V[:, 1], V[:, 2]\n\n    # Compute spline knots for truth if needed\n    k1 = np.quantile(L, 1/3)\n    k2 = np.quantile(L, 2/3)\n\n    # Deterministic mean function\n    mean_Y = np.zeros((n, p))\n    # Optional intercept along v1 with small amplitude for realism\n    intercept_amp = truth.get('intercept_amp', 0.0)\n    if intercept_amp != 0.0:\n        mean_Y += intercept_amp * np.outer(np.ones(n), v1)\n\n    # Linear component\n    lin_amp = truth.get('lin_amp', 0.0)\n    if lin_amp != 0.0:\n        mean_Y += lin_amp * np.outer(L, v1)\n\n    # Quadratic component\n    quad_amp = truth.get('quad_amp', 0.0)\n    if quad_amp != 0.0:\n        mean_Y += quad_amp * np.outer(L**2, v2)\n\n    # Cubic component\n    cubic_amp = truth.get('cubic_amp', 0.0)\n    if cubic_amp != 0.0:\n        mean_Y += cubic_amp * np.outer(L**3, v3)\n\n    # Spline (hinge) components\n    h1 = np.maximum(0.0, L - k1)\n    h2 = np.maximum(0.0, L - k2)\n    h1_amp = truth.get('h1_amp', 0.0)\n    h2_amp = truth.get('h2_amp', 0.0)\n    if h1_amp != 0.0:\n        mean_Y += h1_amp * np.outer(h1, v3)  # project on v3\n    if h2_amp != 0.0:\n        mean_Y += h2_amp * np.outer(h2, v2)  # project on v2\n\n    # Add Gaussian noise\n    Y = mean_Y + rng.normal(scale=sigma, size=(n, p))\n    return Y, L, (k1, k2)\n\ndef fit_and_select(Y, L, knots):\n    \"\"\"Fit four candidate models, compute AIC, and select model by delta <= 2 and parsimony.\"\"\"\n    aics = []\n    ks = []\n    for mid in [1, 2, 3, 4]:\n        X = design_matrices(L, mid, knots=knots)\n        aic, B_hat, E = multivariate_aic(Y, X)\n        aics.append(aic)\n        ks.append(X.shape[1])\n    aics = np.array(aics)\n    ks = np.array(ks)\n    min_aic = np.min(aics)\n    # Candidates within delta <= 2\n    within = np.where(aics <= min_aic + 2.0)[0]\n    # Select simplest (fewest columns)\n    min_k = np.min(ks[within])\n    simplest = within[ks[within] == min_k]\n    # If multiple, choose the one with the smallest AIC among them\n    if simplest.size > 1:\n        idx = simplest[np.argmin(aics[simplest])]\n    else:\n        idx = simplest[0]\n    selected_model_id = int(idx + 1)  # model IDs are 1-based\n    return selected_model_id, aics\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'seed': 1729,\n            'n': 120,\n            'p': 3,\n            'L_range': (1.5, 3.0),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.6,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 20211,\n            'n': 140,\n            'p': 3,\n            'L_range': (1.3, 3.1),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.0,\n                'quad_amp': 0.5,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 314159,\n            'n': 160,\n            'p': 3,\n            'L_range': (1.3, 3.1),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.0,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.35,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 271828,\n            'n': 150,\n            'p': 3,\n            'L_range': (1.4, 3.0),\n            'sigma': 0.06,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.1,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.8,\n                'h2_amp': -0.6,\n            },\n        },\n    ]\n\n    results = []\n    # Permutation settings\n    B_perms = 499\n    for i, case in enumerate(test_cases):\n        # Simulate data\n        Y, L, knots = simulate_case(case)\n        # Fit and select model by AIC with parsimony\n        selected_model_id, aics = fit_and_select(Y, L, knots)\n        # Permutation test for added nonlinearity beyond linear if applicable\n        # Use a distinct seed per case for permutations for reproducibility\n        perm_seed = case['seed'] + 12345\n        pval = freedman_lane_pvalue(Y, L, selected_model_id, B=B_perms, knots=knots, seed=perm_seed)\n        # Round p-value to six decimals\n        pval_rounded = round(float(pval), 6)\n        results.append([selected_model_id, pval_rounded])\n\n    # Final print statement in the exact required format.\n    # Print as a single-line JSON-like list of per-case results.\n    print(str(results).replace(\" \", \"\"))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2577673"}, {"introduction": "A powerful application of geometric morphometrics lies in quantifying and testing for shape differences among predefined groups (e.g., species, populations, or experimental treatments). This exercise focuses on a critical aspect of comparative analysis: choosing the appropriate discriminant analysis method by testing its underlying statistical assumptions. You will implement Box’s $M$ test to assess the homogeneity of within-group covariance matrices, a key assumption that separates Linear Discriminant Analysis (LDA) from Quadratic Discriminant Analysis (QDA). This practice ([@problem_id:2577658]) will equip you to make statistically sound, data-driven decisions, ensuring the robustness and validity of your comparative conclusions.", "problem": "A comparative morphometrician studies Procrustes-aligned landmark configurations for multiple biological groups in the Procrustes tangent space, yielding a multivariate shape vector of dimension $p$ per specimen. The analyst considers between-group discrimination based on Mahalanobis distance but is aware that the standard pooled-covariance Mahalanobis distance assumes homogeneous within-group covariance. Your task is to determine, for several concrete cases, whether the homogeneity assumption is tenable and then algorithmically recommend an alternative among Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), or a Regularized Discriminant Analysis (RDA) when appropriate.\n\nFoundational basis:\n- Assume that within each group $i \\in \\{1,\\dots,g\\}$, the shape vectors are independent draws from a multivariate normal distribution with mean $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^p$ and covariance $\\mathbf{S}_i \\in \\mathbb{R}^{p \\times p}$, where $\\mathbf{S}_i$ is symmetric positive definite.\n- The pooled covariance estimator is defined as\n$$\n\\mathbf{S}_p \\;=\\; \\frac{\\sum_{i=1}^g (n_i - 1)\\,\\mathbf{S}_i}{\\sum_{i=1}^g (n_i - 1)} \\,,\n$$\nwhere $n_i \\ge 2$ is the sample size of group $i$ and $N = \\sum_{i=1}^g n_i$ is the total sample size.\n- Box’s $M$ test (log-likelihood ratio under multivariate normality) assesses covariance homogeneity across groups. Define\n$$\nM \\;=\\; (N-g)\\,\\ln|\\mathbf{S}_p| \\;-\\; \\sum_{i=1}^g (n_i - 1)\\,\\ln|\\mathbf{S}_i| \\,,\n$$\nwith the small-sample correction\n$$\nc \\;=\\; \\frac{2p^2 + 3p - 1}{6(p+1)(g-1)}\\left(\\sum_{i=1}^g \\frac{1}{n_i - 1} - \\frac{1}{N - g}\\right) \\,,\n$$\nand the chi-square approximation\n$$\nX^2 \\;=\\; (1-c)\\,M \\quad\\text{with degrees of freedom}\\quad \\nu \\;=\\; \\frac{(g-1)\\,p\\,(p+1)}{2} \\,.\n$$\nLet the $p$-value be computed from the upper tail of the chi-square distribution with $\\nu$ degrees of freedom. Use significance level $\\alpha = 0.05$.\n- The condition number $\\kappa(\\mathbf{S}_i)$ quantifies numerical stability for inversion, where\n$$\n\\kappa(\\mathbf{S}_i) \\;=\\; \\frac{\\lambda_{\\max}(\\mathbf{S}_i)}{\\lambda_{\\min}(\\mathbf{S}_i)} \\,,\n$$\nwith $\\lambda_{\\max}$ and $\\lambda_{\\min}$ the largest and smallest eigenvalues of $\\mathbf{S}_i$. Use the threshold $\\kappa_{\\max} = 10^8$ to flag ill-conditioning.\n\nDecision logic to recommend a classifier:\n- If the Box’s $M$ test is not significant (i.e., $p$-value $\\ge \\alpha$), recommend LDA (code $0$).\n- If significant ($p$-value $< \\alpha$) and all groups have $\\kappa(\\mathbf{S}_i) \\le \\kappa_{\\max}$, recommend QDA (code $1$).\n- If significant and at least one group has $\\kappa(\\mathbf{S}_i) > \\kappa_{\\max}$, recommend RDA (code $2$), understood here as a ridge-regularized covariance approach that stabilizes inversion.\n\nImplement a program that, for each test case below, computes:\n- The Box’s $M$ test $p$-value (rounded to $6$ decimal places).\n- A boolean indicating heterogeneity of covariance (true if $p$-value $< \\alpha$).\n- The integer recommendation code ($0$ for LDA, $1$ for QDA, $2$ for RDA).\n\nAngles and physical units are not applicable in this setting.\n\nTest suite:\n- Case A: $g=2$, $p=3$, $n_1=30$, $n_2=28$, with\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n1.0 & 0.2 & 0.1\\\\\n0.2 & 1.0 & 0.05\\\\\n0.1 & 0.05 & 0.5\n\\end{bmatrix},\n\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n1.0 & 0.2 & 0.1\\\\\n0.2 & 1.0 & 0.05\\\\\n0.1 & 0.05 & 0.5\n\\end{bmatrix}.\n$$\n- Case B: $g=3$, $p=3$, $n_1=25$, $n_2=35$, $n_3=40$, with\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n0.8 & 0.1 & 0.0\\\\\n0.1 & 0.4 & 0.05\\\\\n0.0 & 0.05 & 0.6\n\\end{bmatrix},\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n1.5 & 0.4 & 0.2\\\\\n0.4 & 1.2 & 0.3\\\\\n0.2 & 0.3 & 1.0\n\\end{bmatrix},\\quad\n\\mathbf{S}_3 \\;=\\; \\begin{bmatrix}\n0.7 & -0.2 & 0.1\\\\\n-0.2 & 0.9 & -0.15\\\\\n0.1 & -0.15 & 0.5\n\\end{bmatrix}.\n$$\n- Case C: $g=2$, $p=3$, $n_1=12$, $n_2=12$, with\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n10^{-8} & 0.0 & 0.0\\\\\n0.0 & 1.0 & 0.2\\\\\n0.0 & 0.2 & 1.2\n\\end{bmatrix},\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n0.9 & 0.1 & 0.0\\\\\n0.1 & 0.9 & 0.05\\\\\n0.0 & 0.05 & 0.9\n\\end{bmatrix}.\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a three-element list of the form $[\\text{pvalue}, \\text{heterogeneous}, \\text{recommendation}]$. For example, an output for three cases should look like\n$$\n[\\,[0.123456,\\text{True},1],[0.987654,\\text{False},0],[0.000001,\\text{True},2]\\,].\n$$\n- The $p$-value must be rounded to $6$ decimal places. The boolean must be either $\\text{True}$ or $\\text{False}$, and the recommendation must be the integer code $0$, $1$, or $2$.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It presents a well-posed question within the domain of multivariate statistics, specifically as applied to morphometric analysis. All parameters, definitions, and conditions are explicitly stated, scientifically grounded, and free from internal contradiction or ambiguity. No flaws that would invalidate the problem have been identified. We shall therefore proceed with a complete solution.\n\nThe task is to algorithmically recommend a classification method—Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), or Regularized Discriminant Analysis (RDA)—for discriminating between several biological groups described by shape vectors. The decision is based on the statistical properties of the within-group sample covariance matrices. This decision process rests on two fundamental pillars: the homogeneity of covariances across groups and the numerical stability of these matrices.\n\nThe primary distinction between LDA and QDA lies in the assumption of covariance homogeneity. LDA operates under the simplifying assumption that all groups $i \\in \\{1, \\dots, g\\}$ share a single, common covariance matrix, denoted $\\mathbf{S}$. In contrast, QDA relaxes this constraint, allowing each group to possess its own unique covariance matrix, $\\mathbf{S}_i$. The choice between these models is therefore governed by a statistical test of the null hypothesis $H_0: \\mathbf{S}_1 = \\mathbf{S}_2 = \\dots = \\mathbf{S}_g$. If this hypothesis is not rejected, LDA is generally the more parsimonious and thus preferred model, as it requires estimating fewer parameters and is consequently more robust, especially when sample sizes are limited. If the hypothesis is rejected, indicating significant heterogeneity, employing the restrictive LDA model can lead to suboptimal classification accuracy, making QDA the theoretically superior choice.\n\nTo formally test the homogeneity hypothesis, Box's $M$ test is employed. This is a multivariate extension of Bartlett's test for the equality of variances. Assuming the shape vectors in each group are sampled from a multivariate normal distribution, $\\mathcal{N}(\\boldsymbol{\\mu}_i, \\mathbf{S}_i)$, the test statistic $M$ is formulated based on a likelihood ratio criterion:\n$$\nM = (N-g)\\ln|\\mathbf{S}_p| - \\sum_{i=1}^g (n_i - 1)\\ln|\\mathbf{S}_i|\n$$\nIn this expression, $g$ represents the number of groups, $p$ is the dimensionality of the shape vectors, $n_i$ is the sample size for group $i$, and $N = \\sum_{i=1}^g n_i$ is the total sample size. The matrix $\\mathbf{S}_p$ is the pooled covariance matrix, computed as a weighted average of the individual group covariance matrices $\\mathbf{S}_i$:\n$$\n\\mathbf{S}_p = \\frac{\\sum_{i=1}^g (n_i - 1)\\mathbf{S}_i}{N - g}\n$$\nThe distribution of the $M$ statistic is approximated by a chi-square distribution. For improved accuracy, a correction factor is applied, resulting in the statistic $X^2 = (1-c)M$, which is tested against a $\\chi^2$ distribution with $\\nu$ degrees of freedom. The correction factor $c$ and the degrees of freedom $\\nu$ are defined as:\n$$\nc = \\frac{2p^2 + 3p - 1}{6(p+1)(g-1)}\\left(\\sum_{i=1}^g \\frac{1}{n_i - 1} - \\frac{1}{N - g}\\right)\n$$\n$$\n\\nu = \\frac{(g-1)p(p+1)}{2}\n$$\nThe $p$-value is calculated from the upper tail of this distribution, as $P(\\chi^2_\\nu \\ge X^2)$. If this $p$-value is not less than the specified significance level $\\alpha = 0.05$, the null hypothesis of homogeneity is retained, and LDA is recommended (coded as $0$).\n\nConversely, if the $p$-value is less than $\\alpha$, the null hypothesis is rejected. This indicates that the covariance matrices are significantly different across groups. While QDA is the indicated model under such heterogeneity, its implementation requires computing the inverse of each group's covariance matrix, $\\mathbf{S}_i^{-1}$. This inversion is numerically unstable if any $\\mathbf{S}_i$ is ill-conditioned, meaning it is close to being singular. The numerical stability for inversion is assessed using the matrix condition number, $\\kappa(\\mathbf{S}_i)$, which is the ratio of the matrix's largest eigenvalue to its smallest:\n$$\n\\kappa(\\mathbf{S}_i) = \\frac{\\lambda_{\\max}(\\mathbf{S}_i)}{\\lambda_{\\min}(\\mathbf{S}_i)}\n$$\nA large condition number warns that the inverse is highly sensitive to small errors in the data, potentially yielding unreliable results. We employ a pre-defined threshold of $\\kappa_{\\max} = 10^8$ to flag such instability.\n\nThe complete algorithmic decision logic is therefore structured as follows:\n1.  Calculate the $p$-value associated with the Box's $M$ test for homogeneity of covariances.\n2.  If the $p$-value is greater than or equal to $0.05$, the assumption of homogeneity is considered tenable. The recommendation is LDA (code $0$).\n3.  If the $p$-value is less than $0.05$, the assumption of homogeneity is rejected. The condition number, $\\kappa(\\mathbf{S}_i)$, must be evaluated for all group covariance matrices.\n    a. If $\\kappa(\\mathbf{S}_i) \\le 10^8$ for all groups $i$: All matrices are considered well-conditioned and numerically stable for inversion. The recommendation is QDA (code $1$).\n    b. If $\\kappa(\\mathbf{S}_i) > 10^8$ for at least one group $i$: At least one matrix is ill-conditioned. Standard QDA would be unreliable. A regularized approach is required to ensure a stable solution. The recommendation is RDA (code $2$).\n\nThe following program implements this logic to systematically analyze the provided test cases and generate the specified outputs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"id\": \"A\", \"g\": 2, \"p\": 3, \"n\": [30, 28],\n            \"S\": [\n                np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.05], [0.1, 0.05, 0.5]]),\n                np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.05], [0.1, 0.05, 0.5]])\n            ]\n        },\n        # Case B\n        {\n            \"id\": \"B\", \"g\": 3, \"p\": 3, \"n\": [25, 35, 40],\n            \"S\": [\n                np.array([[0.8, 0.1, 0.0], [0.1, 0.4, 0.05], [0.0, 0.05, 0.6]]),\n                np.array([[1.5, 0.4, 0.2], [0.4, 1.2, 0.3], [0.2, 0.3, 1.0]]),\n                np.array([[0.7, -0.2, 0.1], [-0.2, 0.9, -0.15], [0.1, -0.15, 0.5]])\n            ]\n        },\n        # Case C\n        {\n            \"id\": \"C\", \"g\": 2, \"p\": 3, \"n\": [12, 12],\n            \"S\": [\n                np.array([[1e-8, 0.0, 0.0], [0.0, 1.0, 0.2], [0.0, 0.2, 1.2]]),\n                np.array([[0.9, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 0.9]])\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_value, heterogeneous, recommendation = _solve_case(case)\n        results.append([p_value, heterogeneous, recommendation])\n\n    # Final print statement in the exact required format.\n    result_strings = [f\"[{p:.6f},{h},{r}]\" for p, h, r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(case_data):\n    \"\"\"\n    Processes a single test case to determine covariance homogeneity and\n    recommend a classifier.\n\n    Args:\n        case_data (dict): A dictionary containing the parameters for a single case.\n\n    Returns:\n        tuple: A tuple containing (p-value, heterogeneous_flag, recommendation_code).\n    \"\"\"\n    g = case_data[\"g\"]\n    p = case_data[\"p\"]\n    n_i_list = case_data[\"n\"]\n    S_i_list = case_data[\"S\"]\n    \n    alpha = 0.05\n    kappa_max = 1e8\n\n    # Calculate total sample size N and degrees of freedom\n    N = sum(n_i_list)\n    df_i_list = [n - 1 for n in n_i_list]\n    df_total = N - g # This is sum(df_i_list)\n\n    # Calculate pooled covariance matrix S_p\n    S_p_numerator = np.zeros((p, p))\n    for i in range(g):\n        S_p_numerator += df_i_list[i] * S_i_list[i]\n    S_p = S_p_numerator / df_total\n    \n    # Calculate determinants and their logarithms.\n    # np.linalg.slogdet is used for stability, but problem guarantees PD matrices.\n    # Using np.linalg.det for simplicity as per problem setup.\n    det_S_p = np.linalg.det(S_p)\n    log_det_S_p = np.log(det_S_p)\n\n    sum_log_det_S_i = 0\n    for i in range(g):\n        det_S_i = np.linalg.det(S_i_list[i])\n        log_det_S_i = np.log(det_S_i)\n        sum_log_det_S_i += df_i_list[i] * log_det_S_i\n    \n    # Calculate Box's M statistic\n    M = df_total * log_det_S_p - sum_log_det_S_i\n\n    # Handle the case where matrices are identical, M can be exactly 0\n    if np.isclose(M, 0):\n        p_value = 1.0\n    else:\n        # Calculate Box's small-sample correction factor c\n        c_factor1 = (2 * p**2 + 3 * p - 1) / (6 * (p + 1) * (g - 1))\n        c_factor2 = sum(1.0/df for df in df_i_list) - (1.0 / df_total)\n        c = c_factor1 * c_factor2\n        \n        # Calculate chi-square statistic and degrees of freedom\n        X2 = (1 - c) * M\n        nu = (g - 1) * p * (p + 1) / 2\n        \n        # Calculate p-value from the chi-square distribution's survival function\n        p_value = chi2.sf(X2, nu)\n\n    # Apply decision logic based on p-value and condition numbers\n    heterogeneous = p_value < alpha\n    recommendation = -1  # Placeholder for an undefined recommendation\n\n    if not heterogeneous:\n        recommendation = 0  # LDA\n    else:\n        # Covariances are heterogeneous, check matrix conditioning for QDA\n        is_ill_conditioned = False\n        for S_i in S_i_list:\n            # eigvalsh is for symmetric matrices and returns sorted eigenvalues\n            eigenvalues = np.linalg.eigvalsh(S_i)\n            # Smallest eigenvalue must be positive for positive-definiteness\n            if eigenvalues[0] <= 0:\n                is_ill_conditioned = True\n                break\n            \n            kappa = eigenvalues[-1] / eigenvalues[0]\n            if kappa > kappa_max:\n                is_ill_conditioned = True\n                break\n        \n        if is_ill_conditioned:\n            recommendation = 2  # RDA\n        else:\n            recommendation = 1  # QDA\n            \n    return p_value, heterogeneous, recommendation\n\nsolve()\n```", "id": "2577658"}]}