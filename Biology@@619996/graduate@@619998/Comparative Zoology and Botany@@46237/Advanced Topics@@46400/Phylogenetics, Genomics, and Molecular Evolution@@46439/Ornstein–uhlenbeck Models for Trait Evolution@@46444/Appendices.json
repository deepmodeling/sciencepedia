{"hands_on_practices": [{"introduction": "This first exercise grounds our understanding in the fundamental mathematics of the Ornstein-Uhlenbeck (OU) process. By deriving the expression for trait variance over time from first principles, you will see how the model's parameters link directly to observable evolutionary patterns. This practice is essential for building intuition about how selection strength, $\\alpha$, and the diffusion rate, $\\sigma$, jointly determine the dynamics of trait evolution. [@problem_id:2592891]", "problem": "A comparative botanist is calibrating an Ornstein–Uhlenbeck (OU) model for a continuous plant trait (e.g., leaf mass per area) on an ultrametric phylogeny of height $T$. The trait evolves according to the stochastic differential equation\n$$\n\\mathrm{d}X_t \\;=\\; \\alpha\\,(\\theta - X_t)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}B_t,\n$$\nwhere $B_t$ is standard Brownian motion, $\\alpha \\gt 0$ is the strength of stabilizing selection toward the long-term optimum $\\theta$, and $\\sigma \\gt 0$ is the diffusion (microevolutionary) variance parameter. Fossil constraints anchor the root state at the optimum, so $X_0 = \\theta$, and there is no ancestral variance and no measurement error. For an ultrametric tree, the marginal variance at the tips equals the variance of $X_T$ along a lineage of length $T$.\n\nYou are given:\n- Tree height $T = 12$ million years (Myr), written as $T = 12$.\n- Selection strength $\\alpha = 0.8$ per Myr, written as $\\alpha = 0.8$.\n- Target among-tip variance (after removing within-species sampling variance) $V_T = 3.5$ in trait-squared units, written as $V_T = 3.5$.\n\nStarting from the stochastic differential equation above and only fundamental properties of linear stochastic differential equations and Itô isometry, derive the relationship between the tip variance at time $T$ and the parameters $\\alpha$, $\\sigma$, and $T$, and use it to calibrate $\\sigma^2$ so that the marginal variance at time $T$ equals $V_T$. Then evaluate your expression numerically for the given $T$, $\\alpha$, and $V_T$.\n\nRound your final numerical answer to four significant figures. Express the calibrated diffusion variance parameter in trait-square per million years ($trait^2$ per Myr). Provide only the final numerical value as your answer.", "solution": "The problem is determined to be valid as it is a well-posed, scientifically grounded problem in quantitative evolutionary biology. It provides all necessary information and parameters to derive a unique, meaningful solution. We will proceed with the formal derivation.\n\nThe evolution of the trait $X_t$ is described by the Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = \\alpha(\\theta - X_t)\\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\nwith the initial condition given as $X_0 = \\theta$. The parameters $\\alpha > 0$, $\\sigma > 0$, and $\\theta$ are constants, and $B_t$ is a standard Brownian motion process.\n\nTo derive the variance of the process, it is analytically convenient to center the process on its long-term mean $\\theta$. We define a new process $Y_t = X_t - \\theta$. The initial condition for this process is $Y_0 = X_0 - \\theta = \\theta - \\theta = 0$. The differential of $Y_t$ is $\\mathrm{d}Y_t = \\mathrm{d}X_t$. Substituting $X_t = Y_t + \\theta$ into the OU SDE yields the SDE for $Y_t$:\n$$\n\\mathrm{d}Y_t = \\alpha(\\theta - (Y_t + \\theta))\\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\n$$\n\\mathrm{d}Y_t = -\\alpha Y_t \\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\nThis is a homogeneous linear SDE. We can solve it using an integrating factor. Let us define a new process $Z_t = e^{\\alpha t} Y_t$. Using Itô's lemma for a function $f(t, y) = e^{\\alpha t}y$, the differential $\\mathrm{d}Z_t$ is:\n$$\n\\mathrm{d}Z_t = \\frac{\\partial f}{\\partial t}\\mathrm{d}t + \\frac{\\partial f}{\\partial y}\\mathrm{d}Y_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial y^2}(\\mathrm{d}Y_t)^2\n$$\nThe partial derivatives are $\\frac{\\partial f}{\\partial t} = \\alpha e^{\\alpha t}Y_t$, $\\frac{\\partial f}{\\partial y} = e^{\\alpha t}$, and $\\frac{\\partial^2 f}{\\partial y^2} = 0$. Substituting these into the formula for $\\mathrm{d}Z_t$:\n$$\n\\mathrm{d}Z_t = (\\alpha e^{\\alpha t}Y_t)\\mathrm{d}t + e^{\\alpha t}\\mathrm{d}Y_t\n$$\nNow, we substitute the expression for $\\mathrm{d}Y_t$:\n$$\n\\mathrm{d}Z_t = \\alpha e^{\\alpha t}Y_t\\mathrm{d}t + e^{\\alpha t}(-\\alpha Y_t \\mathrm{d}t + \\sigma\\mathrm{d}B_t)\n$$\n$$\n\\mathrm{d}Z_t = \\alpha e^{\\alpha t}Y_t\\mathrm{d}t - \\alpha e^{\\alpha t}Y_t\\mathrm{d}t + \\sigma e^{\\alpha t}\\mathrm{d}B_t\n$$\n$$\n\\mathrm{d}(e^{\\alpha t}Y_t) = \\sigma e^{\\alpha t}\\mathrm{d}B_t\n$$\nIntegrating this from $s=0$ to $s=t$:\n$$\n\\int_0^t \\mathrm{d}(e^{\\alpha s}Y_s) = \\int_0^t \\sigma e^{\\alpha s}\\mathrm{d}B_s\n$$\n$$\ne^{\\alpha t}Y_t - e^{\\alpha \\cdot 0}Y_0 = \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s\n$$\nWith the initial condition $Y_0=0$, the equation simplifies to $e^{\\alpha t}Y_t = \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s$. The explicit solution for $Y_t$ is then:\n$$\nY_t = e^{-\\alpha t} \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s = \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s\n$$\nThe variance of the original process $X_t$ is equal to the variance of the centered process $Y_t$, since $\\theta$ is a constant:\n$$\nV(t) = \\mathrm{Var}(X_t) = \\mathrm{Var}(\\theta + Y_t) = \\mathrm{Var}(Y_t)\n$$\nThe variance is given by $\\mathrm{Var}(Y_t) = \\mathrm{E}[Y_t^2] - (\\mathrm{E}[Y_t])^2$. The expectation of an Itô integral with a deterministic integrand is zero:\n$$\n\\mathrm{E}[Y_t] = \\mathrm{E}\\left[ \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right] = \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{E}[\\mathrm{d}B_s] = 0\n$$\nTherefore, the variance is equal to the second moment, $\\mathrm{Var}(Y_t) = \\mathrm{E}[Y_t^2]$. We compute this using Itô isometry, which states that for a deterministic integrand $g(s)$, $\\mathrm{E}\\left[\\left(\\int_0^t g(s)\\mathrm{d}B_s\\right)^2\\right] = \\int_0^t g(s)^2 \\mathrm{d}s$.\n$$\n\\mathrm{E}[Y_t^2] = \\mathrm{E}\\left[ \\left( \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right)^2 \\right] = \\sigma^2 \\mathrm{E}\\left[ \\left( \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right)^2 \\right]\n$$\n$$\n\\mathrm{E}[Y_t^2] = \\sigma^2 \\int_0^t \\left(e^{-\\alpha(t-s)}\\right)^2 \\mathrm{d}s = \\sigma^2 \\int_0^t e^{-2\\alpha(t-s)} \\mathrm{d}s\n$$\nEvaluating the integral:\n$$\n\\int_0^t e^{-2\\alpha(t-s)} \\mathrm{d}s = \\left[ \\frac{e^{-2\\alpha(t-s)}}{2\\alpha} \\right]_{s=0}^{s=t} = \\frac{e^{-2\\alpha(t-t)}}{2\\alpha} - \\frac{e^{-2\\alpha(t-0)}}{2\\alpha} = \\frac{1 - e^{-2\\alpha t}}{2\\alpha}\n$$\nThus, the variance of the trait at time $t$ is:\n$$\nV(t) = \\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\alpha} (1 - e^{-2\\alpha t})\n$$\nThis is the derived relationship between tip variance and the model parameters. The problem requires calibrating $\\sigma^2$ such that the marginal variance at tree height $T$, denoted $V_T$, is equal to the target value. We set $t=T$ and solve for $\\sigma^2$:\n$$\nV_T = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T))\n$$\n$$\n\\sigma^2 = \\frac{2\\alpha V_T}{1 - \\exp(-2\\alpha T)}\n$$\nThis is the analytical expression for the calibrated diffusion variance parameter. We substitute the provided numerical values: $T = 12$, $\\alpha = 0.8$, and $V_T = 3.5$.\n$$\n\\sigma^2 = \\frac{2(0.8)(3.5)}{1 - \\exp(-2(0.8)(12))} = \\frac{1.6 \\times 3.5}{1 - \\exp(-19.2)} = \\frac{5.6}{1 - \\exp(-19.2)}\n$$\nThe exponential term $\\exp(-19.2)$ is a very small positive number, approximately $4.605 \\times 10^{-9}$. The numerical value of $\\sigma^2$ is:\n$$\n\\sigma^2 \\approx \\frac{5.6}{1 - 4.605 \\times 10^{-9}} \\approx \\frac{5.6}{0.999999995395} \\approx 5.6000000257\n$$\nRounding this result to four significant figures gives $5.600$.", "answer": "$$\n\\boxed{5.600}\n$$", "id": "2592891"}, {"introduction": "After exploring the model's core properties, we now tackle a critical task in comparative biology: model selection. This exercise simulates a common research scenario where you must choose between the OU model and a simpler Brownian Motion (BM) model. By applying the Akaike Information Criterion with a small-sample correction ($AICc$), you will practice quantitatively balancing model fit against complexity, a crucial skill for rigorous hypothesis testing in phylogenetics. [@problem_id:2592906]", "problem": "A comparative dataset records a continuous functional trait (e.g., leaf specific area) for $n$ closely related plant species spanning a single clade, measured at the tips of a known ultrametric phylogeny. Two stochastic process models for trait evolution are fit by maximum likelihood on this phylogeny: a Brownian Motion (BM) model and an Ornstein–Uhlenbeck (OU) model. For the BM fit with a free root mean and diffusion rate, the maximum log-likelihood is $\\ell_{\\mathrm{BM}}=-42.3$ with parameter count $k_{\\mathrm{BM}}=2$. For the OU fit with free selection strength, stationary mean, diffusion rate, and root state, the maximum log-likelihood is $\\ell_{\\mathrm{OU}}=-39.5$ with parameter count $k_{\\mathrm{OU}}=4$. The number of tips is $n=18$. Using the corrected Akaike Information Criterion (AICc), compute and compare the model fits on the same data under the principle of expected information loss, starting from the standard definitions of model complexity and likelihood-based fit. Specifically, determine the AICc for each model from their parameter counts and maximized log-likelihoods for the given $n$, and then compute the AICc difference $\\Delta = \\mathrm{AICc}_{\\mathrm{OU}} - \\mathrm{AICc}_{\\mathrm{BM}}$. Based on the sign and magnitude of $\\Delta$, decide which model is favored and explain in your reasoning how the finite-sample penalty influences the comparison when $n$ is modest and $k$ differs across models. Report only the numerical value of $\\Delta$ as your final answer, rounded to four significant figures. Express your final answer without units.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in statistical model selection within the field of evolutionary biology. All necessary data are provided, and there are no contradictions or ambiguities. I will therefore proceed with the solution.\n\nThe problem requires the comparison of two evolutionary models, Brownian Motion (BM) and Ornstein–Uhlenbeck (OU), using the corrected Akaike Information Criterion ($\\mathrm{AICc}$). The goal is to determine which model is better supported by the data, considering both goodness of fit and model complexity, particularly under the condition of a small sample size.\n\nThe formula for the corrected Akaike Information Criterion is given by:\n$$ \\mathrm{AICc} = -2\\ell + 2k + \\frac{2k(k+1)}{n-k-1} $$\nwhere $\\ell$ is the maximum log-likelihood of the model, $k$ is the number of estimated parameters, and $n$ is the sample size. For this phylogenetic problem, the sample size $n$ corresponds to the number of tips, which is given as $n=18$.\n\nFirst, we compute the $\\mathrm{AICc}$ for the Brownian Motion (BM) model. The given parameters are $\\ell_{\\mathrm{BM}} = -42.3$ and $k_{\\mathrm{BM}} = 2$.\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = -2(\\ell_{\\mathrm{BM}}) + 2k_{\\mathrm{BM}} + \\frac{2k_{\\mathrm{BM}}(k_{\\mathrm{BM}}+1)}{n-k_{\\mathrm{BM}}-1} $$\nSubstituting the values:\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = -2(-42.3) + 2(2) + \\frac{2(2)(2+1)}{18-2-1} $$\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = 84.6 + 4 + \\frac{12}{15} $$\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = 88.6 + 0.8 = 89.4 $$\n\nNext, we compute the $\\mathrm{AICc}$ for the Ornstein–Uhlenbeck (OU) model. The given parameters are $\\ell_{\\mathrm{OU}} = -39.5$ and $k_{\\mathrm{OU}} = 4$.\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = -2(\\ell_{\\mathrm{OU}}) + 2k_{\\mathrm{OU}} + \\frac{2k_{\\mathrm{OU}}(k_{\\mathrm{OU}}+1)}{n-k_{\\mathrm{OU}}-1} $$\nSubstituting the values:\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = -2(-39.5) + 2(4) + \\frac{2(4)(4+1)}{18-4-1} $$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = 79.0 + 8 + \\frac{40}{13} $$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = 87.0 + \\frac{40}{13} $$\nTo combine these terms, we can express the fraction as a decimal: $\\frac{40}{13} \\approx 3.076923...$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} \\approx 87.0 + 3.076923 = 90.076923... $$\n\nThe problem requires the calculation of the difference $\\Delta = \\mathrm{AICc}_{\\mathrm{OU}} - \\mathrm{AICc}_{\\mathrm{BM}}$.\n$$ \\Delta \\approx 90.076923 - 89.4 $$\n$$ \\Delta \\approx 0.676923... $$\n\nThe problem asks for an explanation of the result, particularly the influence of the finite-sample penalty. The OU model shows a better fit to the data, as evidenced by its higher maximum log-likelihood ($\\ell_{\\mathrm{OU}} = -39.5 > \\ell_{\\mathrm{BM}} = -42.3$). However, this improved fit comes at the cost of increased complexity, as the OU model has two additional parameters ($k_{\\mathrm{OU}} = 4$ versus $k_{\\mathrm{BM}} = 2$). The $\\mathrm{AICc}$ penalizes this complexity. The penalty term, $\\frac{2k(k+1)}{n-k-1}$, is sensitive to both the number of parameters $k$ and the small sample size $n=18$. For the BM model, the penalty is $\\frac{12}{15} = 0.8$. For the OU model, the penalty is much larger: $\\frac{40}{13} \\approx 3.077$. The substantial penalty for the OU model, a direct consequence of its higher parameter count and the small sample size, outweighs its superior fit. This results in the $\\mathrm{AICc}$ for the OU model being higher than that for the BM model.\n\nThe difference, $\\Delta$, is positive, which indicates that the BM model has a lower $\\mathrm{AICc}$ score and is therefore the preferred model. A model with a lower $\\mathrm{AICc}$ is estimated to lose less information when approximating reality. However, the magnitude of the difference, $\\Delta \\approx 0.6769$, is small (conventionally, $\\Delta < 2$). This suggests that while the BM model is formally favored, there is still substantial support for the OU model, and it cannot be dismissed as a plausible alternative. The penalty for small sample sizes was critical in reversing the conclusion one might draw from the raw likelihoods or the standard AIC, where the OU model would have appeared superior.\n\nRounding the final numerical value of $\\Delta$ to four significant figures gives $0.6769$.", "answer": "$$\\boxed{0.6769}$$", "id": "2592906"}, {"introduction": "Our final practice moves from statistical comparison to the mechanics of Bayesian inference, a cornerstone of modern phylogenetics. You will implement a single Metropolis-Hastings update for the selection strength parameter, $\\alpha$, learning how to combine the OU model's likelihood with a prior distribution. This computational exercise provides a direct, hands-on look at the algorithmic machinery that powers parameter estimation in sophisticated phylogenetic software. [@problem_id:2592918]", "problem": "You will implement a single Metropolis–Hastings update for the Ornstein–Uhlenbeck (OU) selection strength parameter $\\alpha$ using a random-walk proposal on the log scale, and compute the corresponding acceptance probabilities for several parameter settings. The context is a univariate continuous trait evolving along a fixed phylogeny.\n\nFundamental base and assumptions:\n- The trait $X_t$ follows an Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE): $dX_t = -\\alpha (X_t - \\theta)\\,dt + \\sigma\\,dB_t$, where $\\alpha > 0$ is the selection strength, $\\theta \\in \\mathbb{R}$ is the optimum, $\\sigma > 0$ is the diffusion scale, and $B_t$ is standard Brownian motion.\n- Assume a stationary root for the OU process so that the trait mean at the tips is $\\theta$ and the tip trait vector is multivariate normal with mean vector $\\mu = \\theta \\mathbf{1}_n$ and covariance matrix $V(\\alpha)$ with entries\n$$\nV_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha}\\,\\exp\\!\\left(-\\alpha\\,d_{ij}\\right),\n$$\nwhere $d_{ij}$ is the patristic distance between tip $i$ and tip $j$ on the tree and $n$ is the number of tips.\n- The prior for $\\alpha$ is Gamma with shape $k$ and rate $r$, with density $p(\\alpha) \\propto \\alpha^{k-1} \\exp(-r \\alpha)$ for $\\alpha > 0$.\n- The Metropolis–Hastings (MH) proposal is performed on the log scale: if the current value is $\\alpha$, then propose $\\ell' \\sim \\mathcal{N}(\\log \\alpha, s^2)$ and set $\\alpha' = \\exp(\\ell')$. The acceptance probability is defined by the Metropolis–Hastings rule applied to the posterior of $\\alpha$ and the forward and reverse proposal densities.\n\nData and constants for this problem:\n- Number of tips $n = 4$.\n- Trait optimum $\\theta = 0.5$.\n- Diffusion scale squared $\\sigma^2 = 0.8$.\n- Gamma prior parameters: shape $k = 2.5$, rate $r = 1.0$.\n- Patristic distance matrix $D = (d_{ij})$ induced by the ultrametric tree $((A:1,B:1):1,(C:1,D:1):1)$, i.e.,\n$$\nD = \\begin{bmatrix}\n0 & 2 & 4 & 4 \\\\\n2 & 0 & 4 & 4 \\\\\n4 & 4 & 0 & 2 \\\\\n4 & 4 & 2 & 0 \\\\\n\\end{bmatrix}.\n$$\n- Observed tip trait vector $x = [0.30,\\, 0.60,\\, 0.45,\\, 0.70]$.\n\nYour task:\n- For each test case, compute the Metropolis–Hastings acceptance probability for the proposed $\\alpha'$ given the current $\\alpha$, using the OU multivariate normal likelihood with parameters $\\theta$ and $\\sigma^2$ and the Gamma prior for $\\alpha$, together with the proposal on the log scale described above. Use the exact multivariate normal likelihood with mean $\\mu = \\theta \\mathbf{1}_n$ and covariance $V(\\alpha)$ as defined above, and implement it via a numerically stable method.\n- The acceptance probability should be returned as a float in $[0,1]$.\n\nTest suite:\nFor each tuple $(\\alpha, \\Delta, s)$, interpret $\\Delta$ as the additive perturbation on the log scale, i.e., $\\ell' = \\log \\alpha + \\Delta$ and $\\alpha' = \\exp(\\ell')$. The proposal standard deviation on the log scale is $s$.\n\n- Test $1$: $(\\alpha, \\Delta, s) = (0.8,\\, 0.2,\\, 0.3)$.\n- Test $2$: $(\\alpha, \\Delta, s) = (0.8,\\, -0.2,\\, 0.3)$.\n- Test $3$: $(\\alpha, \\Delta, s) = (0.2,\\, -1.0,\\, 0.5)$.\n- Test $4$: $(\\alpha, \\Delta, s) = (2.0,\\, 0.0,\\, 0.7)$.\n\nImplementation requirements:\n- Treat $\\theta$, $\\sigma^2$, $k$, $r$, $D$, and $x$ as fixed constants given above.\n- For the likelihood, use the exact multivariate normal density with $V(\\alpha)$ and $\\mu = \\theta \\mathbf{1}_n$, computed via a numerically stable factorization (for example, Cholesky decomposition) to obtain the log determinant and the quadratic form.\n- For the prior, use the Gamma shape–rate parameterization given above.\n- For the proposal densities $q(\\alpha' \\mid \\alpha)$ and $q(\\alpha \\mid \\alpha')$, use the corresponding lognormal densities implied by a Gaussian random walk on $\\log \\alpha$ with standard deviation $s$.\n\nFinal output format:\n- Your program should produce a single line of output containing the acceptance probabilities for the four tests as a comma-separated list of floats rounded to $6$ decimal places and enclosed in square brackets. For example, $[0.123456,0.654321,0.500000,1.000000]$.\n\nNo physical units are involved. Angles are not involved. Percentages are not involved. All answers must be floats as specified.", "solution": "We begin from the fundamental description of the Ornstein–Uhlenbeck (OU) process for a continuous trait $X_t$ along a lineage, governed by the stochastic differential equation $dX_t = -\\alpha (X_t - \\theta)\\,dt + \\sigma\\,dB_t$, where $\\alpha > 0$, $\\theta \\in \\mathbb{R}$, and $\\sigma > 0$. Under a stationary root assumption for the OU process on a phylogeny, the vector of trait values $x \\in \\mathbb{R}^n$ at the tips is multivariate normal with mean vector $\\mu = \\theta \\mathbf{1}_n$ and covariance matrix $V(\\alpha)$ whose entries obey the well-tested formula\n$$\nV_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha}\\,\\exp\\!\\left(-\\alpha d_{ij}\\right),\n$$\nwhere $d_{ij}$ is the patristic distance between tips $i$ and $j$. This result is a standard property of stationary OU processes on trees: the stationary variance is $\\sigma^2/(2\\alpha)$ and cross-covariances decay exponentially in the shared evolutionary distance due to mean reversion at rate $\\alpha$.\n\nLikelihood. The likelihood of $x$ under parameters $\\alpha$, $\\theta$, and $\\sigma^2$ is\n$$\nL(x \\mid \\alpha) = \\mathcal{N}\\big(x; \\mu(\\theta), V(\\alpha)\\big),\n$$\nwith log-likelihood\n$$\n\\log L(x \\mid \\alpha) = -\\frac{1}{2} \\left( n \\log(2\\pi) + \\log \\det V(\\alpha) + (x - \\mu)^\\top V(\\alpha)^{-1} (x - \\mu) \\right).\n$$\nFor numerical stability, we compute $\\log \\det V(\\alpha)$ and the quadratic form using the Cholesky factorization $V(\\alpha) = L L^\\top$ with lower-triangular $L$. Then $\\log \\det V(\\alpha) = 2 \\sum_{i=1}^n \\log L_{ii}$, and $(x - \\mu)^\\top V^{-1} (x - \\mu)$ is computed by solving $L y = (x - \\mu)$ and then $L^\\top z = y$, with the quadratic form equal to $(x - \\mu)^\\top z$.\n\nPrior. We adopt a Gamma prior on $\\alpha$ with shape–rate parameters $k$ and $r$:\n$$\np(\\alpha) = \\frac{r^k}{\\Gamma(k)} \\alpha^{k-1} e^{-r\\alpha}, \\quad \\alpha > 0.\n$$\nThe log-prior is $\\log p(\\alpha) = k \\log r - \\log \\Gamma(k) + (k-1) \\log \\alpha - r \\alpha$. In the Metropolis–Hastings log-acceptance difference, the additive constant $k \\log r - \\log \\Gamma(k)$ cancels between current and proposed states, so it can be omitted in computation.\n\nProposal. We implement a random walk in $\\ell = \\log \\alpha$. Given current $\\alpha$, we draw a proposal $\\ell' \\sim \\mathcal{N}(\\log \\alpha, s^2)$ and set $\\alpha' = \\exp(\\ell')$. The corresponding proposal densities on the $\\alpha$ scale are lognormal:\n$$\nq(\\alpha' \\mid \\alpha) = \\operatorname{LogNormal}\\!\\left(\\alpha'; \\mu = \\log \\alpha, \\sigma = s\\right), \\quad\nq(\\alpha \\mid \\alpha') = \\operatorname{LogNormal}\\!\\left(\\alpha; \\mu = \\log \\alpha', \\sigma = s\\right).\n$$\nThe Metropolis–Hastings acceptance probability is\n$$\n\\mathrm{acc} = \\min\\left( 1, \\frac{L(x \\mid \\alpha') \\, p(\\alpha') \\, q(\\alpha \\mid \\alpha')}{L(x \\mid \\alpha) \\, p(\\alpha) \\, q(\\alpha' \\mid \\alpha)} \\right).\n$$\nIt is numerically stable to compute on the log scale:\n$$\n\\log R = \\left[ \\log L(x \\mid \\alpha') - \\log L(x \\mid \\alpha) \\right] + \\left[ \\log p(\\alpha') - \\log p(\\alpha) \\right] + \\left[ \\log q(\\alpha \\mid \\alpha') - \\log q(\\alpha' \\mid \\alpha) \\right],\n$$\nfollowed by $\\mathrm{acc} = \\min\\left(1, \\exp(\\log R)\\right)$.\n\nHastings correction simplification. For the lognormal proposals with identical $s$, the Gaussian symmetry in $\\ell$ implies that the difference in log proposal densities simplifies:\n$$\n\\log q(\\alpha \\mid \\alpha') - \\log q(\\alpha' \\mid \\alpha) = \\log \\alpha' - \\log \\alpha.\n$$\nThis can be verified by writing the lognormal density\n$$\n\\log q(x \\mid m, s) = -\\log x - \\frac{1}{2}\\log(2\\pi) - \\log s - \\frac{(\\log x - m)^2}{2 s^2}\n$$\nwith $m = \\log \\alpha$ or $m = \\log \\alpha'$, and observing that the quadratic terms cancel. Thus, the Hastings correction contributes exactly $\\log(\\alpha'/\\alpha)$ to $\\log R$. Equivalently, the acceptance ratio using a symmetric proposal in $\\ell$ coincides with evaluating the posterior density for $\\ell$ that includes the Jacobian factor $\\alpha$.\n\nAlgorithmic steps for each test case $(\\alpha, \\Delta, s)$:\n1. Form $\\ell = \\log \\alpha$ and $\\ell' = \\ell + \\Delta$, then $\\alpha' = \\exp(\\ell')$.\n2. Compute $\\log L(x \\mid \\alpha)$ and $\\log L(x \\mid \\alpha')$ using the OU multivariate normal with $\\mu = \\theta \\mathbf{1}_n$ and $V_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha} e^{-\\alpha d_{ij}}$, via a Cholesky factorization.\n3. Compute $\\log p(\\alpha)$ and $\\log p(\\alpha')$ up to an additive constant using the Gamma shape–rate prior: $\\log p(\\alpha) = (k-1)\\log \\alpha - r \\alpha$.\n4. Compute the Hastings correction either directly as $\\log \\alpha' - \\log \\alpha$ or by evaluating the lognormal proposal densities and subtracting them.\n5. Obtain $\\log R$ by summing the three differences, then compute $\\mathrm{acc} = \\min(1, \\exp(\\log R))$.\n6. Round $\\mathrm{acc}$ to $6$ decimal places.\n\nNumerical details:\n- We use the given constants $n = 4$, $\\theta = 0.5$, $\\sigma^2 = 0.8$, $k = 2.5$, $r = 1.0$, the distance matrix $D$ as specified, and the observed $x = [0.30, 0.60, 0.45, 0.70]$.\n- The Cholesky factorization provides stable evaluation of the multivariate normal log-likelihood, avoiding explicit matrix inversion.\n- For the four tests, the case with $\\Delta = 0.0$ yields $\\alpha' = \\alpha$ and hence $\\log R = 0$ and acceptance probability equal to $1.0$ by construction; the other cases reflect the balance between the likelihood fit and the prior regularization via the MH rule.\n\nThe final program implements these steps and prints a single line with the four acceptance probabilities as required.", "answer": "$$\n\\boxed{[0.724771,0.923869,0.060773,1.000000]}\n$$", "id": "2592918"}]}