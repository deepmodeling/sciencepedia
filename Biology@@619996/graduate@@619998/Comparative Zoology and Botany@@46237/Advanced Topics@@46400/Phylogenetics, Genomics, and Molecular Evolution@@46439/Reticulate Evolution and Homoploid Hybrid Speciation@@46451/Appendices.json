{"hands_on_practices": [{"introduction": "A primary challenge in evolutionary biology is to distinguish between shared ancestry due to common descent and that resulting from gene flow (introgression). The ABBA-BABA test, which yields Patterson's $D$-statistic, provides a powerful framework for this by detecting an asymmetry in allele sharing patterns that is indicative of introgression. This exercise gives you hands-on experience in applying this fundamental test, calculating the $D$-statistic from genomic data, and assessing its statistical significance using the robust block jackknife method to account for linkage between sites [@problem_id:2607877].", "problem": "You are investigating potential introgression associated with homoploid hybrid speciation using a four-taxon test of asymmetry in derived allele sharing among three ingroup taxa and an outgroup, arranged as $((P_1,P_2),P_3)$ with outgroup $O$. At a biallelic single nucleotide polymorphism (SNP; single nucleotide polymorphism), an $ABBA$ site is one where, relative to the outgroup’s ancestral allele $A$, the derived allele $B$ is present in $P_2$ and $P_3$ but not $P_1$, and a $BABA$ site is one where the derived allele $B$ is present in $P_1$ and $P_3$ but not $P_2$. Under the null hypothesis of no introgression, $ABBA$ and $BABA$ counts are expected to be symmetric across sites due to incomplete lineage sorting alone.\n\nYou have partitioned the genome into $50$ non-overlapping blocks to accommodate linkage. In each block you counted the number of informative $ABBA$ and $BABA$ sites. The block-wise totals of informative sites are equal across blocks. The data are:\n\n- Blocks $1$ through $10$: $ABBA=55$, $BABA=45$.\n- Blocks $11$ through $20$: $ABBA=53$, $BABA=47$.\n- Blocks $21$ through $30$: $ABBA=52$, $BABA=48$.\n- Blocks $31$ through $40$: $ABBA=51$, $BABA=49$.\n- Blocks $41$ through $50$: $ABBA=49$, $BABA=51$.\n\nUsing the standard definition of Patterson’s $D$-statistic as the normalized difference in $ABBA$ and $BABA$ counts across sites, and using a delete-one-block jackknife over the $50$ blocks to estimate the standard error of $D$, compute:\n- The point estimate of $D$ using all blocks,\n- The jackknife standard error of $D$,\n- The corresponding $Z$-score under a standard normal approximation.\n\nAssume each block contributes equally many informative sites, and use the usual delete-one-block jackknife variance estimator for a ratio statistic. Test for introgression at significance level $\\alpha=0.05$ based on the magnitude of the $Z$-score, but report only the numerical values.\n\nExpress your final numerical answers as a single row vector in the order $D$, standard error, $Z$-score, rounded to three significant figures. Do not include units.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It is a standard application of the ABBA-BABA test and jackknife resampling in population genetics. We may proceed with the solution.\n\nThe task is to compute Patterson's $D$-statistic, its standard error via a delete-one-block jackknife, and the corresponding $Z$-score. The system consists of three ingroup taxa $(P_1, P_2, P_3)$ and an outgroup $O$, with the specified phylogeny $((P_1,P_2),P_3)$.\n\nFirst, let us denote the total count of $ABBA$ sites across all blocks as $N_{ABBA}$ and the total count of $BABA$ sites as $N_{BABA}$. The data are provided for $n=50$ blocks, partitioned into five groups of $10$ blocks each.\n\nThe counts for each group of $10$ blocks are:\n- Group 1 (blocks $1-10$): $C_{ABBA}=55$, $C_{BABA}=45$.\n- Group 2 (blocks $11-20$): $C_{ABBA}=53$, $C_{BABA}=47$.\n- Group 3 (blocks $21-30$): $C_{ABBA}=52$, $C_{BABA}=48$.\n- Group 4 (blocks $31-40$): $C_{ABBA}=51$, $C_{BABA}=49$.\n- Group 5 (blocks $41-50$): $C_{ABBA}=49$, $C_{BABA}=51$.\n\nThe total counts are calculated by summing the contributions from all $50$ blocks:\n$$ N_{ABBA} = 10 \\times 55 + 10 \\times 53 + 10 \\times 52 + 10 \\times 51 + 10 \\times 49 $$\n$$ N_{ABBA} = 10 \\times (55 + 53 + 52 + 51 + 49) = 10 \\times 260 = 2600 $$\n$$ N_{BABA} = 10 \\times 45 + 10 \\times 47 + 10 \\times 48 + 10 \\times 49 + 10 \\times 51 $$\n$$ N_{BABA} = 10 \\times (45 + 47 + 48 + 49 + 51) = 10 \\times 240 = 2400 $$\n\nPatterson's $D$-statistic is defined as the normalized difference between the counts of $ABBA$ and $BABA$ sites:\n$$ D = \\frac{N_{ABBA} - N_{BABA}}{N_{ABBA} + N_{BABA}} $$\nUsing the total counts, the point estimate of $D$ is:\n$$ D = \\frac{2600 - 2400}{2600 + 2400} = \\frac{200}{5000} = \\frac{1}{25} = 0.04 $$\n\nNext, we compute the standard error of $D$ using a delete-one-block jackknife. Let $n=50$ be the number of blocks. Let $D_i$ be the estimate of the $D$-statistic computed on the dataset with the $i$-th block removed. The jackknife variance of $D$ is given by:\n$$ \\text{Var}_{\\text{jack}}(D) = \\frac{n-1}{n} \\sum_{i=1}^{n} (D_i - \\bar{D}_{\\text{jack}})^2 $$\nwhere $\\bar{D}_{\\text{jack}} = \\frac{1}{n} \\sum_{i=1}^{n} D_i$.\n\nLet $C_{ABBA,i}$ and $C_{BABA,i}$ be the counts in the $i$-th block. The problem states that the total number of informative sites per block is constant. For any block $i$, this total is $C_{ABBA,i} + C_{BABA,i} = 100$. Let this constant be $s = 100$.\nThe total number of informative sites across all $n$ blocks is $S = N_{ABBA} + N_{BABA} = 5000$.\n\nWhen block $j$ is removed, the recalculated $D$-statistic, $D_j$, is:\n$$ D_j = \\frac{(N_{ABBA} - C_{ABBA,j}) - (N_{BABA} - C_{BABA,j})}{(N_{ABBA} - C_{ABBA,j}) + (N_{BABA} - C_{BABA,j})} = \\frac{(N_{ABBA} - N_{BABA}) - (C_{ABBA,j} - C_{BABA,j})}{(N_{ABBA} + N_{BABA}) - (C_{ABBA,j} + C_{BABA,j})} $$\n$$ D_j = \\frac{200 - (C_{ABBA,j} - C_{BABA,j})}{5000 - 100} = \\frac{200 - (C_{ABBA,j} - C_{BABA,j})}{4900} $$\nBecause the denominator is constant for all $j$, $\\bar{D}_{\\text{jack}}$ is exactly equal to the full estimate $D$.\n$\\bar{D}_{\\text{jack}} = \\frac{1}{n}\\sum D_j = \\frac{1}{n(S-s)} \\sum( (N_{ABBA}-N_{BABA}) - (C_{ABBA,j} - C_{BABA,j}) ) = \\frac{1}{n(S-s)} (n(N_{ABBA}-N_{BABA}) - \\sum(C_{ABBA,j}-C_{BABA,j}))$.\nSince $\\sum(C_{ABBA,j}-C_{BABA,j}) = N_{ABBA}-N_{BABA}$, we have $\\bar{D}_{\\text{jack}} = \\frac{(n-1)(N_{ABBA}-N_{BABA})}{n(S-s)}$.\nLet's check the derivation. It is a known property for ratio statistics where the denominator contributions are constant.\nThe original derivation from the thought process was correct: $\\sum D_i = Y/c$ where $Y = N_{ABBA}-N_{BABA}$ and $c$ is the denominator contribution. This leads to $\\bar{D}_{jack} = Y/(nc) \\neq D$. Let's re-evaluate.\nLet $y_i = C_{ABBA,i}-C_{BABA,i}$ and $x_i = C_{ABBA,i}+C_{BABA,i}=s=100$. Let $Y=\\sum y_i$ and $X=\\sum x_i = ns$.\n$D = Y/X = Y/(ns)$.\n$D_i = (\\sum_{j \\neq i} y_j) / (\\sum_{j \\neq i} x_j) = (Y-y_i)/(X-x_i) = (Y-y_i)/((n-1)s)$.\n$\\bar{D}_{\\text{jack}} = \\frac{1}{n} \\sum D_i = \\frac{1}{n(n-1)s} \\sum(Y-y_i) = \\frac{1}{n(n-1)s} (nY-Y) = \\frac{Y(n-1)}{n(n-1)s} = \\frac{Y}{ns} = D$.\nThe simplification holds. The variance estimator is $\\text{Var}_{\\text{jack}}(D) = \\frac{n-1}{n} \\sum_{i=1}^{n} (D_i - D)^2$.\n\nWe have five distinct groups of $D_i$ values. Let $d_j = C_{ABBA,j} - C_{BABA,j}$. Also, $D = 0.04 = \\frac{196}{4900}$.\n- For blocks $1-10$: $d_j = 55-45=10$. $D_j = \\frac{200-10}{4900} = \\frac{190}{4900}$. $D_j-D = \\frac{190-196}{4900} = -\\frac{6}{4900}$.\n- For blocks $11-20$: $d_j = 53-47=6$. $D_j = \\frac{200-6}{4900} = \\frac{194}{4900}$. $D_j-D = \\frac{194-196}{4900} = -\\frac{2}{4900}$.\n- For blocks $21-30$: $d_j = 52-48=4$. $D_j = \\frac{200-4}{4900} = \\frac{196}{4900}$. $D_j-D = 0$.\n- For blocks $31-40$: $d_j = 51-49=2$. $D_j = \\frac{200-2}{4900} = \\frac{198}{4900}$. $D_j-D = \\frac{198-196}{4900} = \\frac{2}{4900}$.\n- For blocks $41-50$: $d_j = 49-51=-2$. $D_j = \\frac{200-(-2)}{4900} = \\frac{202}{4900}$. $D_j-D = \\frac{202-196}{4900} = \\frac{6}{4900}$.\n\nThe sum of squared differences is:\n$$ \\sum_{i=1}^{50} (D_i - D)^2 = 10 \\left[ \\left(-\\frac{6}{4900}\\right)^2 + \\left(-\\frac{2}{4900}\\right)^2 + (0)^2 + \\left(\\frac{2}{4900}\\right)^2 + \\left(\\frac{6}{4900}\\right)^2 \\right] $$\n$$ \\sum_{i=1}^{50} (D_i - D)^2 = \\frac{10}{4900^2} [36 + 4 + 0 + 4 + 36] = \\frac{10 \\times 80}{4900^2} = \\frac{800}{4900^2} $$\nThe jackknife variance is:\n$$ \\text{Var}_{\\text{jack}}(D) = \\frac{50-1}{50} \\times \\frac{800}{4900^2} = \\frac{49}{50} \\times \\frac{800}{(49 \\times 100)^2} = \\frac{49}{50} \\times \\frac{800}{49^2 \\times 100^2} $$\n$$ \\text{Var}_{\\text{jack}}(D) = \\frac{1}{50} \\times \\frac{800}{49 \\times 10000} = \\frac{16}{49 \\times 10000} = \\frac{16}{490000} $$\nThe jackknife standard error, $SE_{\\text{jack}}(D)$, is the square root of the variance:\n$$ SE_{\\text{jack}}(D) = \\sqrt{\\frac{16}{490000}} = \\frac{4}{700} = \\frac{1}{175} $$\nNumerically, $SE_{\\text{jack}}(D) \\approx 0.00571428...$\n\nFinally, the $Z$-score is calculated under the null hypothesis $H_0: D=0$.\n$$ Z = \\frac{D - 0}{SE_{\\text{jack}}(D)} = \\frac{D}{SE_{\\text{jack}}(D)} $$\n$$ Z = \\frac{1/25}{1/175} = \\frac{175}{25} = 7 $$\nThe problem requires reporting the numerical values for $D$, the standard error, and the $Z$-score, all rounded to three significant figures.\n- $D = 0.04$. To three significant figures, this is $0.0400$.\n- $SE_{\\text{jack}}(D) = 1/175 \\approx 0.00571428...$. To three significant figures, this is $0.00571$.\n- $Z = 7$. To three significant figures, this is $7.00$.\n\nThe computed $Z$-score of $7.00$ is far greater in magnitude than the standard critical value of $1.96$ for a significance level of $\\alpha = 0.05$, indicating a strong rejection of the null hypothesis of no introgression. However, as instructed, only the numerical results are to be reported.\nThe final results are:\n- Point estimate of $D$: $0.0400$\n- Jackknife standard error of $D$: $0.00571$\n- $Z$-score: $7.00$", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.0400 & 0.00571 & 7.00 \\end{pmatrix} } $$", "id": "2607877"}, {"introduction": "Once evidence for hybridization is established, a key question is how the introgressed genetic material is distributed across the genome over time. Recombination acts to break down continuous blocks of parental ancestry into a mosaic of shorter \"tracts.\" This exercise challenges you to model this process from first principles, deriving fundamental expressions for the expected number and length of ancestry tracts following a hybridization pulse [@problem_id:2607831]. Mastering this model provides deep insight into the temporal dynamics of admixed genomes and forms the basis for dating admixture events.", "problem": "A diploid animal lineage experienced a single hybridization pulse $t$ generations ago between parental species $\\mathrm{A}$ and $\\mathrm{B}$, producing an admixed population that has since mated at random in a very large population without selection. At the pulse, the contribution of species $\\mathrm{A}$ (the donor) to the admixed gene pool was $m$, so that the marginal ancestry frequency from $\\mathrm{A}$ is $m$ at every locus thereafter. Assume the following fundamental bases:\n- Along a chromosome of genetic length $L$ Morgans, the expected number of crossovers per meiosis equals $L$, and crossover locations follow a Poisson process without interference.\n- In a large randomly mating population, the ancestry heterozygosity at a site remains $2 m (1-m)$ across generations in the absence of selection or drift.\n- An ancestry junction is created when a crossover occurs in a meiosis at a position that is heterozygous for ancestry.\n\nYou are given a recombination map for three autosomes as genetic marker positions in centiMorgans (cM), where $1$ centiMorgan (cM) $= 0.01$ Morgans (M). For each chromosome, markers span the entire chromosome from the first to the last listed marker (i.e., there is no additional genetic length outside the listed range), and the interval between adjacent markers is fully covered.\n\nChromosome 1 markers (cM): $0, 20, 55, 95, 135$.\n\nChromosome 2 markers (cM): $0, 30, 60, 95$.\n\nChromosome 3 markers (cM): $0, 25, 48, 70$.\n\nLet the donor ancestry proportion be $m = 0.30$ and time since the admixture pulse be $t = 20$ generations. Work in the continuous-genome limit and ignore end effects due to chromosome termini.\n\nUsing only the fundamental bases stated above, derive from first principles expressions for:\n1) The expected number of species $\\mathrm{A}$ ancestry tracts across the three autosomes after $t$ generations.\n2) The mean length of species $\\mathrm{A}$ ancestry tracts after $t$ generations.\n\nThen evaluate these expressions numerically for the given data. Express the expected number as a pure number and the mean length in Morgans. Round both numerical results to four significant figures. Provide your final answer as two values in the order (expected number, mean length).", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The solution proceeds by deriving the required expressions from the fundamental principles provided.\n\nLet $m$ be the donor ancestry proportion from species A, and $t$ be the number of generations since the admixture pulse. The total genetic length of the genome is $L_{total}$, composed of $N_{chr} = 3$ chromosomes.\n\n**1. Derivation of the Expected Number of Ancestry Tracts**\n\nWe begin by deriving the expected density of ancestry junctions. An ancestry junction is a point on a chromosome where the ancestry switches (e.g., from species A to B, or B to A).\n\nAccording to the problem's stated bases:\n- A junction is created when a crossover occurs in a meiosis at a position that is heterozygous for ancestry.\n- The expected number of crossovers per meiosis along a chromosome of length $L$ Morgans is $L$. For a Poisson process, this implies a rate of $1$ crossover per Morgan per meiosis.\n- The ancestry heterozygosity at any site, which is the probability that a diploid individual is heterozygous for ancestry at that site, is given as a constant value $H = 2m(1-m)$.\n\nThe rate of new junction formation per Morgan, per generation, is the product of the crossover rate and the probability of ancestry heterozygosity. Let $\\rho(t)$ be the density of junctions per Morgan at generation $t$. The rate of change of this density is:\n$$\n\\frac{d\\rho(t)}{dt} = (\\text{crossover rate}) \\times (\\text{ancestry heterozygosity}) = 1 \\times [2m(1-m)] = 2m(1-m)\n$$\nWe assume that at the time of the pulse, $t=0$, the chromosomes are of pure A or B ancestry, so there are no junctions within any chromosome. Thus, the initial condition is $\\rho(0) = 0$.\n\nIntegrating this differential equation from $t=0$ to the present generation $t$ gives the total accumulated junction density:\n$$\n\\rho(t) = \\int_{0}^{t} 2m(1-m) \\,d\\tau = 2m(1-m)t\n$$\nThe total expected number of junctions across the entire genome, $E[N_J]$, is the product of this density and the total genetic length of the genome, $L_{total}$:\n$$\nE[N_J] = \\rho(t) \\times L_{total} = 2m(1-m)tL_{total}\n$$\nThe problem asks for the expected number of A-ancestry tracts, $E[N_A]$. We are instructed to ignore end effects due to chromosome termini. This implies a model where the genome is long enough that the tracts at the ends of chromosomes do not significantly alter the overall statistics. In this limit, the number of A-tracts is expected to be equal to the number of B-tracts, as any switch from A to B must be followed by a switch from B to A to create the next A-tract. The total number of tracts is approximately the number of junctions. Therefore, the number of A-tracts is half the total number of junctions.\n$$\nE[N_A] = \\frac{1}{2} E[N_J]\n$$\nSubstituting the expression for $E[N_J]$, we obtain the first required expression:\n$$\nE[N_A] = \\frac{1}{2} [2m(1-m)tL_{total}] = m(1-m)tL_{total}\n$$\n\n**2. Derivation of the Mean Length of Ancestry Tracts**\n\nThe mean length of A-ancestry tracts is the total expected length of all A-ancestry segments divided by the expected number of A-ancestry tracts.\n\nThe total expected length of species A ancestry, $E[L_{A,total}]$, across a genome of total length $L_{total}$ is given by the proportion of A-ancestry, $m$, multiplied by the total length:\n$$\nE[L_{A,total}] = m \\times L_{total}\n$$\nThe mean length of an A-tract, $E[\\bar{L}_A]$, is then:\n$$\nE[\\bar{L}_A] = \\frac{E[L_{A,total}]}{E[N_A]}\n$$\nSubstituting the expressions we derived for $E[L_{A,total}]$ and $E[N_A]$:\n$$\nE[\\bar{L}_A] = \\frac{m L_{total}}{m(1-m)tL_{total}}\n$$\nThe terms $m$ and $L_{total}$ cancel, yielding the second required expression:\n$$\nE[\\bar{L}_A] = \\frac{1}{(1-m)t}\n$$\nThis result is independent of the total genome length and depends only on the time since admixture and the proportion of the alternative ancestry.\n\n**3. Numerical Evaluation**\n\nFirst, we calculate the total genetic length, $L_{total}$, from the provided marker data. The unit conversion is $1 \\text{ cM} = 0.01 \\text{ M}$.\n- Chromosome $1$ length: $L_1 = (135 - 0) \\text{ cM} = 135 \\text{ cM} = 1.35 \\text{ M}$.\n- Chromosome $2$ length: $L_2 = (95 - 0) \\text{ cM} = 95 \\text{ cM} = 0.95 \\text{ M}$.\n- Chromosome $3$ length: $L_3 = (70 - 0) \\text{ cM} = 70 \\text{ cM} = 0.70 \\text{ M}$.\n\nThe total length of the three autosomes is:\n$$\nL_{total} = L_1 + L_2 + L_3 = 1.35 \\text{ M} + 0.95 \\text{ M} + 0.70 \\text{ M} = 3.00 \\text{ M}\n$$\nThe given parameters are $m = 0.30$ and $t = 20$ generations.\n\nNow we evaluate the derived expressions, rounding to four significant figures as requested.\n\nExpected number of species A ancestry tracts:\n$$\nE[N_A] = m(1-m)tL_{total} = (0.30)(1 - 0.30)(20)(3.00) = (0.30)(0.70)(60) = 0.21 \\times 60 = 12.6\n$$\nRounded to four significant figures, the result is $12.60$.\n\nMean length of species A ancestry tracts (in Morgans):\n$$\nE[\\bar{L}_A] = \\frac{1}{(1-m)t} = \\frac{1}{(1 - 0.30)(20)} = \\frac{1}{(0.70)(20)} = \\frac{1}{14} \\text{ M}\n$$\nAs a decimal, this is approximately $0.07142857...$ M. Rounded to four significant figures, the result is $0.07143$ M.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n12.60 & 0.07143\n\\end{pmatrix}\n}\n$$", "id": "2607831"}, {"introduction": "Homoploid hybrid speciation can be facilitated by genomic rearrangements, like chromosomal inversions, that reduce recombination between parental haplotypes and protect co-adapted sets of alleles. Identifying these inversions is crucial, but requires integrating subtle signals from sequencing data. This advanced exercise guides you through the process of building a sophisticated genome scan, implementing a Bayesian classifier to synthesize evidence from linkage disequilibrium, read-pair anomalies, and sequencing coverage [@problem_id:2607851]. This practice exemplifies a powerful, model-based approach to data integration that is central to modern computational genomics.", "problem": "You are studying a homoploid hybrid lineage formed by reticulate evolution, in which chromosomal inversions may maintain blocks of co-adapted alleles. You are given genome-wide windowed summaries that integrate three independent signal classes expected from inversions: long-range Linkage Disequilibrium (LD), paired-end read-pair orientation anomalies, and normalized coverage shifts. Your task is to implement a mathematically principled genome scan that detects inverted intervals from these features by deriving, implementing, and applying a probabilistic classifier based on fundamental definitions and Bayes’ rule.\n\nFundamental base and assumptions:\n- Linkage Disequilibrium (LD) between two biallelic loci is defined by $D = p_{AB} - p_A p_B$, and the standardized measure $r^2 = \\dfrac{D^2}{p_A (1-p_A) p_B (1-p_B)}$. Inversions tend to elevate long-range $r^2$ within or spanning the inversion due to suppressed recombination.\n- Paired-end sequencing produces read-pair orientation classes. In a reference genome with correct orientation, most pairs follow the expected inward-facing orientation; inversions yield an excess fraction of discordant orientations. We summarize this by a discordant orientation fraction per window. Fractions are constrained to $[0,1]$ and can be modeled as outcomes of a Binomial process; across windows we approximate their distribution by a Beta family.\n- Coverage is the count of aligned reads per window. After normalization by genome-wide expectation, multiplicative noise motivates a LogNormal approximation for the ratio. Inversions can produce systematic undercoverage (for example, due to mapping ambiguity at breakpoints) relative to the non-inverted state.\n\nClassifier design requirement:\n- Use Bayes’ rule to compute the posterior probability that a window is inverted given the three observed features. Under a class-conditional independence assumption, the joint likelihood factorizes across features. The features per window $i$ are:\n  - $L_i \\in [0,1]$: the mean long-range $r^2$ within the window, aggregated over locus pairs separated by at least a fixed distance.\n  - $O_i \\in [0,1]$: the fraction of discordant read-pair orientations in the window.\n  - $C_i > 0$: the normalized coverage ratio in the window (unitless).\n- Assume the following class-conditional distributions for the features, with all parameters known and fixed:\n  - Under the non-inversion (null) state $\\mathcal{N}$:\n    - $L \\sim \\mathrm{Beta}(a_{L0}, b_{L0})$ with $(a_{L0}, b_{L0}) = (2, 18)$.\n    - $O \\sim \\mathrm{Beta}(a_{O0}, b_{O0})$ with $(a_{O0}, b_{O0}) = (1, 49)$.\n    - $C \\sim \\mathrm{LogNormal}(\\mu_0, \\sigma_0)$ with $(\\mu_0, \\sigma_0) = (0, 0.1)$, meaning $\\ln C \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n  - Under the inversion (alternative) state $\\mathcal{I}$:\n    - $L \\sim \\mathrm{Beta}(a_{L1}, b_{L1})$ with $(a_{L1}, b_{L1}) = (12, 8)$.\n    - $O \\sim \\mathrm{Beta}(a_{O1}, b_{O1})$ with $(a_{O1}, b_{O1}) = (3, 17)$.\n    - $C \\sim \\mathrm{LogNormal}(\\mu_1, \\sigma_1)$ with $(\\mu_1, \\sigma_1) = (\\ln(0.9), 0.1)$.\n- Use a constant prior inversion probability per window $\\pi = 0.05$. Apply Bayes’ rule to compute, for each window $i$, the posterior $P(\\mathcal{I} \\mid L_i, O_i, C_i)$.\n- Call a window inverted if its posterior is at least the decision threshold $\\tau = 0.9$. Merge adjacent called windows into maximal contiguous intervals and retain only those intervals whose length in windows is at least $m = 3$. Windows are indexed from $0$ to $N-1$, and an interval is reported as inclusive integer indices $[s,e]$ with $0 \\le s \\le e \\le N-1$.\n\nNumerical stability requirement:\n- Implement all likelihood computations in log-space and transform to the posterior using a logit formulation to avoid numerical underflow or overflow.\n\nTest suite:\nImplement your program to run the following four test cases. Each test case consists of three aligned arrays $(L, O, C)$ of equal length $N$, whose entries are real numbers. The arrays are provided explicitly below. All values are unitless decimals.\n\n- Test case A (happy path; clear internal inversion):\n  - $L = [0.08, 0.12, 0.09, 0.11, 0.07, 0.10, 0.13, 0.09, 0.58, 0.62, 0.67, 0.66, 0.64, 0.61, 0.12, 0.10, 0.09, 0.11, 0.08, 0.10]$\n  - $O = [0.01, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02, 0.02, 0.12, 0.16, 0.18, 0.15, 0.14, 0.13, 0.02, 0.01, 0.03, 0.02, 0.02, 0.03]$\n  - $C = [1.02, 0.98, 1.01, 0.97, 1.05, 0.99, 1.00, 0.96, 0.92, 0.90, 0.88, 0.89, 0.91, 0.93, 1.02, 0.99, 1.01, 1.03, 0.97, 1.00]$\n\n- Test case B (boundary inversion at the start):\n  - $L = [0.60, 0.63, 0.66, 0.65, 0.62, 0.10, 0.11, 0.12, 0.09, 0.08, 0.13, 0.10]$\n  - $O = [0.14, 0.17, 0.16, 0.15, 0.13, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02]$\n  - $C = [0.91, 0.89, 0.90, 0.92, 0.93, 1.00, 1.02, 0.99, 1.01, 0.98, 1.03, 1.00]$\n\n- Test case C (no inversion anywhere):\n  - $L = [0.09, 0.12, 0.08, 0.10, 0.11, 0.07, 0.13, 0.09, 0.10, 0.12, 0.08, 0.11, 0.09, 0.10, 0.12]$\n  - $O = [0.02, 0.01, 0.03, 0.02, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02, 0.01, 0.03, 0.02, 0.02]$\n  - $C = [1.00, 0.99, 1.01, 1.02, 0.98, 1.03, 0.97, 1.00, 0.99, 1.01, 1.02, 0.98, 1.00, 1.03, 0.97]$\n\n- Test case D (short alt-like blip to be filtered by minimum length):\n  - $L = [0.10, 0.09, 0.11, 0.12, 0.13, 0.08, 0.10, 0.09, 0.11, 0.62, 0.64, 0.10, 0.12, 0.09, 0.11, 0.08, 0.10, 0.09]$\n  - $O = [0.02, 0.03, 0.01, 0.02, 0.02, 0.03, 0.02, 0.01, 0.02, 0.15, 0.14, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02]$\n  - $C = [1.01, 0.99, 1.00, 0.98, 1.02, 0.97, 1.03, 1.01, 0.99, 0.90, 0.89, 1.00, 1.02, 0.98, 1.01, 1.00, 1.03, 0.97]$\n\nDecision and post-processing parameters are shared across all tests: $\\pi = 0.05, \\tau = 0.9, m = 3$.\n\nProgram requirements:\n- Implement the Bayes classifier with the given distributions and parameters using log-likelihoods. Compute $P(\\mathcal{I} \\mid L_i, O_i, C_i)$ for every window $i$ in each test.\n- Classify and merge as specified above to obtain one or more intervals per test case.\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. Each test’s result is itself a list of $[s,e]$ integer pairs (inclusive window indices). For example, a valid output for two tests might look like $[[[3,7]],[]]$.\n\nAll answers are unitless and must be computed numerically by your code from the provided arrays and parameters. Do not read any external input. The only accepted output types are booleans, integers, floats, or lists of these types. The final output must be a single line in the exact format described.", "solution": "The problem posed is to develop and implement a probabilistic classifier to identify genomic intervals corresponding to chromosomal inversions. The classifier must utilize three distinct features derived from genome sequencing data: long-range Linkage Disequilibrium ($L$), discordant read-pair orientation fraction ($O$), and normalized coverage ($C$). The classification is to be performed on a per-window basis using Bayes' rule, followed by a procedure to merge and filter classified windows into significant intervals.\n\nFirst, a validation of the problem statement is mandatory.\n\n**Step 1: Extract Givens**\n\n- **Features per window $i$**:\n  - $L_i \\in [0,1]$: Mean long-range $r^2$.\n  - $O_i \\in [0,1]$: Fraction of discordant read-pair orientations.\n  - $C_i > 0$: Normalized coverage ratio.\n\n- **States**:\n  - $\\mathcal{N}$: Non-inversion (null) state.\n  - $\\mathcal{I}$: Inversion (alternative) state.\n\n- **Class-Conditional Distributions (Non-inversion state $\\mathcal{N}$)**:\n  - $L \\sim \\mathrm{Beta}(a_{L0}=2, b_{L0}=18)$.\n  - $O \\sim \\mathrm{Beta}(a_{O0}=1, b_{O0}=49)$.\n  - $C \\sim \\mathrm{LogNormal}(\\mu_0=0, \\sigma_0=0.1)$, which implies $\\ln C \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n\n- **Class-Conditional Distributions (Inversion state $\\mathcal{I}$)**:\n  - $L \\sim \\mathrm{Beta}(a_{L1}=12, b_{L1}=8)$.\n  - $O \\sim \\mathrm{Beta}(a_{O1}=3, b_{O1}=17)$.\n  - $C \\sim \\mathrm{LogNormal}(\\mu_1=\\ln(0.9), \\sigma_1=0.1)$, which implies $\\ln C \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$.\n\n- **Priors and Decision Parameters**:\n  - Prior probability of inversion: $\\pi = P(\\mathcal{I}) = 0.05$.\n  - Decision threshold for posterior probability: $\\tau = 0.9$.\n  - Minimum length of a called interval (in windows): $m = 3$.\n\n- **Assumptions**:\n  - Class-conditional independence of features $L$, $O$, and $C$.\n\n- **Post-processing**:\n  1. Classify window $i$ as inverted if $P(\\mathcal{I} \\mid L_i, O_i, C_i) \\ge \\tau$.\n  2. Merge adjacent called windows into maximal contiguous intervals.\n  3. Retain only intervals of length at least $m$.\n\n- **Numerical Requirement**: All computations must be performed in log-space for stability.\n\n- **Test Data**: Four test cases provided, each with arrays for $L$, $O$, and $C$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is subjected to validation against established criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the principles of population genomics and bioinformatics. The use of Linkage Disequilibrium, discordant read pairs, and coverage variation as signals for structural variants like inversions is standard practice. The choice of Beta distributions for modeling proportions ($L$, $O$) and a LogNormal distribution for a multiplicative ratio ($C$) is statistically appropriate and common in the field.\n- **Well-Posed**: The problem is mathematically well-defined. It provides all necessary data, parameters, and a deterministic algorithm (Bayesian classification, thresholding, merging, filtering). This ensures that a unique, stable, and meaningful solution not only exists but is computable.\n- **Objective**: The problem is stated with objective and precise language. All quantities are defined, and the requirements are unambiguous.\n\nBased on this analysis, the problem statement exhibits no flaws. It is scientifically sound, formally complete, and algorithmically specified. It is a valid problem.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be constructed.\n\n**Principle-Based Solution**\n\nOur goal is to compute the posterior probability of a window being in the inverted state $\\mathcal{I}$, given the observed data vector $\\mathbf{d_i} = (L_i, O_i, C_i)$ for that window. According to Bayes' rule, this posterior is:\n$$ P(\\mathcal{I} \\mid \\mathbf{d_i}) = \\frac{ P(\\mathbf{d_i} \\mid \\mathcal{I}) P(\\mathcal{I}) }{ P(\\mathbf{d_i}) } $$\nThe term in the denominator, the evidence, is given by the law of total probability:\n$$ P(\\mathbf{d_i}) = P(\\mathbf{d_i} \\mid \\mathcal{I}) P(\\mathcal{I}) + P(\\mathbf{d_i} \\mid \\mathcal{N}) P(\\mathcal{N}) $$\nwhere $P(\\mathcal{I}) = \\pi$ is the prior probability of inversion, and $P(\\mathcal{N}) = 1 - \\pi$ is the prior probability of non-inversion.\n\nThe problem states an assumption of class-conditional independence for the features. This allows us to factorize the likelihood terms:\n$$ P(\\mathbf{d_i} \\mid \\mathcal{S}) = P(L_i \\mid \\mathcal{S}) \\cdot P(O_i \\mid \\mathcal{S}) \\cdot P(C_i \\mid \\mathcal{S}) $$\nwhere $\\mathcal{S}$ can be either state $\\mathcal{I}$ or $\\mathcal{N}$.\n\nDirect computation of these products is prone to numerical underflow. We are required to work in logarithmic space. The log-likelihood for each state $\\mathcal{S}$ is:\n$$ \\ln \\mathcal{L}_{\\mathcal{S},i} = \\ln P(L_i \\mid \\mathcal{S}) + \\ln P(O_i \\mid \\mathcal{S}) + \\ln P(C_i \\mid \\mathcal{S}) $$\nThe individual log-probability density functions (log-PDFs) are:\n- For $L_i \\sim \\mathrm{Beta}(a_L, b_L)$: $\\ln P(L_i) = \\ln f_{\\text{Beta}}(L_i; a_L, b_L)$.\n- For $O_i \\sim \\mathrm{Beta}(a_O, b_O)$: $\\ln P(O_i) = \\ln f_{\\text{Beta}}(O_i; a_O, b_O)$.\n- For $C_i \\sim \\mathrm{LogNormal}(\\mu, \\sigma)$, which is equivalent to $\\ln C_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$: $\\ln P(C_i) = \\ln f_{\\text{LogNormal}}(C_i; \\mu, \\sigma)$. It is numerically more stable to compute this as the log-PDF of a Normal distribution for the logarithm of the data: $\\ln P(C_i) = \\ln f_{\\text{Normal}}(\\ln C_i; \\mu, \\sigma)$.\n\nTo compute the posterior probability stably, we use the logit transformation. The log-odds, or logit, of the posterior probability is:\n$$ \\lambda_i = \\mathrm{logit}(P(\\mathcal{I} \\mid \\mathbf{d_i})) = \\ln\\left( \\frac{P(\\mathcal{I} \\mid \\mathbf{d_i})}{1 - P(\\mathcal{I} \\mid \\mathbf{d_i})} \\right) $$\nSubstituting the expressions from Bayes' rule, this simplifies to the sum of the log-likelihood ratio and the log-prior-odds:\n$$ \\lambda_i = \\ln\\left( \\frac{P(\\mathbf{d_i} \\mid \\mathcal{I})}{P(\\mathbf{d_i} \\mid \\mathcal{N})} \\right) + \\ln\\left( \\frac{P(\\mathcal{I})}{P(\\mathcal{N})} \\right) = (\\ln \\mathcal{L}_{\\mathcal{I},i} - \\ln \\mathcal{L}_{\\mathcal{N},i}) + (\\ln \\pi - \\ln(1-\\pi)) $$\nThe posterior probability $P(\\mathcal{I} \\mid \\mathbf{d_i})$ is then recovered by applying the standard logistic function (the inverse of the logit function):\n$$ P(\\mathcal{I} \\mid \\mathbf{d_i}) = \\frac{1}{1 + e^{-\\lambda_i}} $$\nThis formulation robustly avoids numerical instability.\n\nThe full algorithm proceeds as follows:\n1.  For each window $i$, calculate the log-likelihoods $\\ln \\mathcal{L}_{\\mathcal{I},i}$ and $\\ln \\mathcal{L}_{\\mathcal{N},i}$ by summing the log-PDF values from the specified Beta and Normal distributions with their respective parameters for states $\\mathcal{I}$ and $\\mathcal{N}$.\n2.  Calculate the log-posterior-odds $\\lambda_i$ using the formula above.\n3.  Calculate the posterior probability $P(\\mathcal{I} \\mid \\mathbf{d_i})$ using the logistic function.\n4.  Create a boolean array indicating which windows are \"called\" as inverted, i.e., where $P(\\mathcal{I} \\mid \\mathbf{d_i}) \\ge \\tau = 0.9$.\n5.  Iterate through the boolean array to identify and merge consecutive called windows into contiguous intervals, represented as $[s, e]$ inclusive index pairs.\n6.  Filter this list of intervals, retaining only those for which the length $(e - s + 1)$ is at least $m = 3$.\n7.  This procedure is applied to each test case, and the final results are aggregated.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta, norm\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the genomic inversion detection problem for a set of test cases.\n    \"\"\"\n\n    # Define model parameters\n    params = {\n        'L0': (2, 18), 'O0': (1, 49), 'C0': (0, 0.1),\n        'L1': (12, 8), 'O1': (3, 17), 'C1': (math.log(0.9), 0.1),\n    }\n\n    # Define decision parameters\n    pi = 0.05\n    tau = 0.9\n    m = 3\n\n    # Log-prior-odds\n    log_prior_odds = math.log(pi) - math.log(1 - pi)\n\n    # Define Test Cases\n    test_cases = [\n        {\n            \"L\": np.array([0.08, 0.12, 0.09, 0.11, 0.07, 0.10, 0.13, 0.09, 0.58, 0.62, 0.67, 0.66, 0.64, 0.61, 0.12, 0.10, 0.09, 0.11, 0.08, 0.10]),\n            \"O\": np.array([0.01, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02, 0.02, 0.12, 0.16, 0.18, 0.15, 0.14, 0.13, 0.02, 0.01, 0.03, 0.02, 0.02, 0.03]),\n            \"C\": np.array([1.02, 0.98, 1.01, 0.97, 1.05, 0.99, 1.00, 0.96, 0.92, 0.90, 0.88, 0.89, 0.91, 0.93, 1.02, 0.99, 1.01, 1.03, 0.97, 1.00]),\n        },\n        {\n            \"L\": np.array([0.60, 0.63, 0.66, 0.65, 0.62, 0.10, 0.11, 0.12, 0.09, 0.08, 0.13, 0.10]),\n            \"O\": np.array([0.14, 0.17, 0.16, 0.15, 0.13, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02]),\n            \"C\": np.array([0.91, 0.89, 0.90, 0.92, 0.93, 1.00, 1.02, 0.99, 1.01, 0.98, 1.03, 1.00]),\n        },\n        {\n            \"L\": np.array([0.09, 0.12, 0.08, 0.10, 0.11, 0.07, 0.13, 0.09, 0.10, 0.12, 0.08, 0.11, 0.09, 0.10, 0.12]),\n            \"O\": np.array([0.02, 0.01, 0.03, 0.02, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02, 0.01, 0.03, 0.02, 0.02]),\n            \"C\": np.array([1.00, 0.99, 1.01, 1.02, 0.98, 1.03, 0.97, 1.00, 0.99, 1.01, 1.02, 0.98, 1.00, 1.03, 0.97]),\n        },\n        {\n            \"L\": np.array([0.10, 0.09, 0.11, 0.12, 0.13, 0.08, 0.10, 0.09, 0.11, 0.62, 0.64, 0.10, 0.12, 0.09, 0.11, 0.08, 0.10, 0.09]),\n            \"O\": np.array([0.02, 0.03, 0.01, 0.02, 0.02, 0.03, 0.02, 0.01, 0.02, 0.15, 0.14, 0.02, 0.03, 0.02, 0.01, 0.02, 0.03, 0.02]),\n            \"C\": np.array([1.01, 0.99, 1.00, 0.98, 1.02, 0.97, 1.03, 1.01, 0.99, 0.90, 0.89, 1.00, 1.02, 0.98, 1.01, 1.00, 1.03, 0.97]),\n        }\n    ]\n\n    all_results = []\n\n    for case_data in test_cases:\n        L, O, C = case_data['L'], case_data['O'], case_data['C']\n        \n        # Calculate log-likelihoods for non-inversion state (N)\n        log_lik_L0 = beta.logpdf(L, a=params['L0'][0], b=params['L0'][1])\n        log_lik_O0 = beta.logpdf(O, a=params['O0'][0], b=params['O0'][1])\n        log_lik_C0 = norm.logpdf(np.log(C), loc=params['C0'][0], scale=params['C0'][1])\n        total_log_lik_0 = log_lik_L0 + log_lik_O0 + log_lik_C0\n\n        # Calculate log-likelihoods for inversion state (I)\n        log_lik_L1 = beta.logpdf(L, a=params['L1'][0], b=params['L1'][1])\n        log_lik_O1 = beta.logpdf(O, a=params['O1'][0], b=params['O1'][1])\n        log_lik_C1 = norm.logpdf(np.log(C), loc=params['C1'][0], scale=params['C1'][1])\n        total_log_lik_1 = log_lik_L1 + log_lik_O1 + log_lik_C1\n\n        # Calculate log-posterior-odds\n        log_posterior_odds = (total_log_lik_1 - total_log_lik_0) + log_prior_odds\n\n        # Convert to posterior probabilities using the logistic function\n        posteriors = 1 / (1 + np.exp(-log_posterior_odds))\n\n        # Apply threshold to make calls\n        calls = posteriors >= tau\n\n        # Merge adjacent calls and filter by length\n        intervals = []\n        i = 0\n        n_windows = len(calls)\n        while i < n_windows:\n            if calls[i]:\n                start = i\n                while i + 1 < n_windows and calls[i+1]:\n                    i += 1\n                end = i\n                # Filter by minimum length\n                if (end - start + 1) >= m:\n                    intervals.append([start, end])\n            i += 1\n        \n        all_results.append(intervals)\n\n    # Format the final output string\n    # E.g., [[[8,13]],[[0,4]],[],[]]\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        res_str = str(res).replace(\" \", \"\")\n        output_str += res_str\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "2607851"}]}