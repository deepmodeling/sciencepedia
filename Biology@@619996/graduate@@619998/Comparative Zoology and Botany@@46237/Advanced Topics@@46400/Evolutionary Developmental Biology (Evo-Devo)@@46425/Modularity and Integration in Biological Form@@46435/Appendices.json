{"hands_on_practices": [{"introduction": "The fundamental concept of modularity posits that biological structures are organized into modules with strong internal connections and weaker external ones. A direct way to quantify this is by comparing the average covariation among traits within hypothesized modules to the covariation between them. This exercise provides a foundational, hands-on calculation of the Covariance Ratio (CR), one of the most intuitive metrics for this purpose, allowing you to solidify your understanding of how modularity is quantitatively defined from a covariance matrix [@problem_id:2590340].", "problem": "In comparative zoology and botany, hypotheses of modularity propose that traits within the same morphological or functional module covary more strongly with one another than they do with traits in other modules. Consider a four-trait dataset measured across a clade of related species, summarized by the empirical trait covariance matrix\n$$\nS \\;=\\; \\begin{pmatrix}\n1 & 0.6 & 0.1 & 0.0\\\\\n0.6 & 1 & 0.0 & 0.1\\\\\n0.1 & 0.0 & 1 & 0.7\\\\\n0.0 & 0.1 & 0.7 & 1\n\\end{pmatrix}.\n$$\nSuppose the hypothesized modular partition is $\\{1,2\\}$ versus $\\{3,4\\}$. Using core definitions of covariance and matrix partitioning, compute the Covariance Ratio (CR), defined as the ratio of the mean between-module covariance to the mean within-module covariance, where means are computed over unique trait pairs only (use the upper-triangular, off-diagonal entries of the appropriate blocks; exclude variances).\n\nThen, based on the general expectation that a modular partition should yield a value of $CR$ less than $1$ (lower average between-module covariance than within-module covariance), interpret whether the specified partition exhibits modularity for the given data. Report only the numerical value of $CR$ as your final answer; express it in exact form without rounding or units.", "solution": "The problem requires the calculation of the Covariance Ratio ($CR$), defined as the ratio of the mean between-module covariance to the mean within-module covariance. The provided data includes a $4 \\times 4$ trait covariance matrix, $S$, and a hypothesized modular partition of the four traits into two modules, $M_1 = \\{1, 2\\}$ and $M_2 = \\{3, 4\\}$.\n\nThe covariance matrix $S$ is given by:\n$$\nS \\;=\\; \\begin{pmatrix}\n1 & 0.6 & 0.1 & 0.0\\\\\n0.6 & 1 & 0.0 & 0.1\\\\\n0.1 & 0.0 & 1 & 0.7\\\\\n0.0 & 0.1 & 0.7 & 1\n\\end{pmatrix}.\n$$\nThe diagonal entries of $S$ are variances (normalized to $1$ in this case), and the off-diagonal entries $S_{ij}$ represent the covariance between trait $i$ and trait $j$.\n\nThe modular partition $\\{1, 2\\}$ versus $\\{3, 4\\}$ allows us to structure the matrix $S$ into blocks corresponding to within-module and between-module covariances:\n$$\nS = \\begin{pmatrix} S_{11} & S_{12} \\\\ S_{21} & S_{22} \\end{pmatrix}\n$$\nHere, $S_{11}$ is the $2 \\times 2$ submatrix of covariances among traits in module $M_1$, $S_{22}$ is the $2 \\times 2$ submatrix for module $M_2$, and $S_{12}$ is the $2 \\times 2$ submatrix of covariances between traits in $M_1$ and $M_2$. The submatrix $S_{21}$ is the transpose of $S_{12}$.\n\nExtracting these blocks from $S$:\nThe within-module covariance block for $M_1 = \\{1, 2\\}$ is:\n$$\nS_{11} = \\begin{pmatrix} 1 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\n$$\nThe within-module covariance block for $M_2 = \\{3, 4\\}$ is:\n$$\nS_{22} = \\begin{pmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{pmatrix}\n$$\nThe between-module covariance block is:\n$$\nS_{12} = \\begin{pmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{pmatrix}\n$$\n\nThe Covariance Ratio ($CR$) is defined as:\n$$\nCR = \\frac{\\bar{c}_{_B}}{\\bar{c}_{_W}}\n$$\nwhere $\\bar{c}_{_W}$ is the mean within-module covariance and $\\bar{c}_{_B}$ is the mean between-module covariance. The means are computed over unique, off-diagonal trait pairs, as specified.\n\nFirst, we calculate the mean within-module covariance, $\\bar{c}_{_W}$. This is the average of the unique off-diagonal covariances within the blocks $S_{11}$ and $S_{22}$. The problem specifies using the upper-triangular, off-diagonal entries to count unique pairs.\nFrom block $S_{11}$, the unique off-diagonal covariance is $S_{12} = 0.6$.\nFrom block $S_{22}$, the unique off-diagonal covariance is $S_{34} = 0.7$.\nThere are two such within-module covariances in total. The mean is:\n$$\n\\bar{c}_{_W} = \\frac{0.6 + 0.7}{2} = \\frac{1.3}{2} = 0.65\n$$\n\nSecond, we calculate the mean between-module covariance, $\\bar{c}_{_B}$. This is the average of all covariances between traits from different modules. These are all the elements of the block $S_{12}$, which represent the covariances between a trait from $M_1$ and a trait from $M_2$.\nThe elements of $S_{12}$ are $\\{0.1, 0.0, 0.0, 0.1\\}$.\nThere are $2 \\times 2 = 4$ between-module covariances. The mean is:\n$$\n\\bar{c}_{_B} = \\frac{0.1 + 0.0 + 0.0 + 0.1}{4} = \\frac{0.2}{4} = 0.05\n$$\n\nFinally, we compute the $CR$ using the calculated mean values:\n$$\nCR = \\frac{\\bar{c}_{_B}}{\\bar{c}_{_W}} = \\frac{0.05}{0.65}\n$$\nTo express this value in its exact fractional form, we perform the division:\n$$\nCR = \\frac{\\frac{5}{100}}{\\frac{65}{100}} = \\frac{5}{65} = \\frac{1}{13}\n$$\nThe condition that $CR < 1$ is expected for a valid modular partition. Our result, $CR = \\frac{1}{13}$, is significantly less than $1$, indicating that the average covariance within modules is $13$ times greater than the average covariance between modules. This provides strong support for the hypothesized modular structure. The problem, however, only requests the numerical value of $CR$.", "answer": "$$\n\\boxed{\\frac{1}{13}}\n$$", "id": "2590340"}, {"introduction": "Beyond identifying discrete modules, it is often critical to quantify the overall degree of covariation, or integration, within a set of traits. A major challenge in morphometrics is to disentangle the various biological sources contributing to this integration, especially the pervasive influence of organismal size (allometry). This practice explores the use of eigenvalue variance as a measure of integration and demonstrates how to partition this metric to distinguish between integration driven by size and that arising from other factors, a key skill in comparative shape analysis [@problem_id:2590377].", "problem": "In comparative zoology and botany, morphological integration reflects the tendency for multiple traits within an organism to covary due to shared developmental, functional, or size-related influences. Consider a dataset of homologous linear distances measured on the same module of cranial morphology across multiple species. Assume species can be treated as independent for the purposes of this exercise. To quantify integration, use the variance of the eigenvalues of the trait correlation matrix, which captures how unevenly total standardized variance is distributed across principal components from Principal Component Analysis (PCA).\n\nYou are given that the dataset contains $p=5$ traits. Before any size correction, the empirical correlation matrix across the $5$ traits is equicorrelated with off-diagonal entries $r_{\\text{before}}=0.4$ and diagonal entries $1$. After statistically removing log-centroid size by ordinary least squares regression (regressing each trait on log-centroid size and using residuals), the residual correlation matrix across the $5$ traits is also equicorrelated, now with off-diagonal entries $r_{\\text{after}}=0.1$ and diagonal entries $1$.\n\nStarting from the definitions of covariance, correlation, and eigen-decomposition of a symmetric matrix, and without assuming any shortcut formulas, derive the eigenvalues of an equicorrelated correlation matrix for general $p$ and correlation parameter $r$. Use this to compute the eigenvalue variance for the $5 \\times 5$ correlation matrices before and after size correction.\n\nDefine the proportion of eigenvalue variance attributable to size as\n$$\nP \\;=\\; \\frac{\\text{Var}_{\\lambda,\\text{before}} - \\text{Var}_{\\lambda,\\text{after}}}{\\text{Var}_{\\lambda,\\text{before}}}.\n$$\n\nCompute $P$ for $p=5$, $r_{\\text{before}}=0.4$, and $r_{\\text{after}}=0.1$. Express your answer as a unitless decimal rounded to four significant figures.", "solution": "The primary task is to find the eigenvalues of a general $p \\times p$ equicorrelated matrix, which we denote as $R$. This matrix has diagonal elements equal to $1$ and all off-diagonal elements equal to a constant $r$. The matrix $R$ can be written as:\n$$\nR = \\begin{pmatrix}\n1 & r & \\cdots & r \\\\\nr & 1 & \\cdots & r \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr & r & \\cdots & 1\n\\end{pmatrix}\n$$\nWe can express $R$ as a sum of two simpler matrices. Let $I$ be the $p \\times p$ identity matrix and $J$ be the $p \\times p$ matrix of all ones. Then,\n$$\nR = (1-r)I + rJ\n$$\nThe eigenvalue equation is $Rv = \\lambda v$, where $\\lambda$ is an eigenvalue and $v$ is the corresponding eigenvector. Substituting the expression for $R$:\n$$\n((1-r)I + rJ)v = \\lambda v\n$$\nDistributing $v$ on the left side gives:\n$$\n(1-r)Iv + rJv = \\lambda v\n$$\n$$\n(1-r)v + rJv = \\lambda v\n$$\nRearranging the terms, we get:\n$$\nrJv = (\\lambda - (1-r))v\n$$\nThis equation reveals that the eigenvectors of $R$ are the same as the eigenvectors of $J$. We must now find the eigenvalues and eigenvectors of the matrix $J$.\n\nThe matrix $J$ has rank $1$, as all its columns are identical. Let $\\mathbf{1}$ be a column vector of $p$ ones.\nAny column of $J$ is $\\mathbf{1}$. Thus, the column space is spanned by $\\mathbf{1}$. For this vector, we have:\n$$\nJ\\mathbf{1} = p\\mathbf{1}\n$$\nThis shows that $\\mathbf{1}$ is an eigenvector of $J$ with the corresponding eigenvalue $p$.\n\nThe null space of $J$, which contains all vectors $v$ such that $Jv=0$, has dimension $p-1$. For any such vector $v$, it is an eigenvector of $J$ with eigenvalue $0$. These vectors are orthogonal to the vector $\\mathbf{1}$, which means the sum of their components is zero, i.e., $\\mathbf{1}^T v = 0$.\n\nNow we use these two cases for the eigenvectors of $J$ to find the eigenvalues of $R$.\n\nCase 1: The eigenvector is $v = \\mathbf{1}$.\nThe corresponding eigenvalue of $J$ is $p$. Substituting into the eigenvalue equation for $R$:\n$R\\mathbf{1} = ((1-r)I+rJ)\\mathbf{1} = (1-r)\\mathbf{1} + r(J\\mathbf{1}) = (1-r)\\mathbf{1} + r(p\\mathbf{1}) = (1-r+pr)\\mathbf{1}$.\nThus, one eigenvalue of $R$ is:\n$$\n\\lambda_1 = 1 - r + pr = 1 + (p-1)r\n$$\n\nCase 2: The eigenvector $v$ is one of the $p-1$ linearly independent vectors in the null space of $J$.\nFor such an eigenvector, the eigenvalue of $J$ is $0$. Substituting into the eigenvalue equation for $R$:\n$Rv = ((1-r)I+rJ)v = (1-r)v + r(Jv) = (1-r)v + r(\\mathbf{0}) = (1-r)v$.\nThus, there are $p-1$ eigenvalues of $R$ that are equal to:\n$$\n\\lambda_{2, \\dots, p} = 1-r\n$$\n\nWith these general formulas, we can compute the eigenvalues for the two specified matrices. The total number of traits is $p=5$.\n\nFor the matrix before size correction, $r_{\\text{before}}=0.4$.\nThe eigenvalues are:\n$\\lambda_{1, \\text{before}} = 1 + (5-1)(0.4) = 1 + 4(0.4) = 1 + 1.6 = 2.6$.\nThe remaining $p-1 = 4$ eigenvalues are:\n$\\lambda_{2,\\dots,5,\\text{before}} = 1 - 0.4 = 0.6$.\nThe set of eigenvalues is $\\{2.6, 0.6, 0.6, 0.6, 0.6\\}$.\nThe sum of eigenvalues is $\\sum \\lambda_i = 2.6 + 4(0.6) = 2.6 + 2.4 = 5$, which correctly equals the trace of the $5 \\times 5$ correlation matrix, $p$.\nThe mean of the eigenvalues is $\\bar{\\lambda} = \\frac{\\sum \\lambda_i}{p} = \\frac{p}{p} = 1$.\nThe variance of the eigenvalues is $\\text{Var}_{\\lambda} = \\frac{1}{p}\\sum_{i=1}^{p} (\\lambda_i - \\bar{\\lambda})^2$.\n$$\n\\text{Var}_{\\lambda,\\text{before}} = \\frac{1}{5} \\left( (2.6-1)^2 + 4 \\times (0.6-1)^2 \\right)\n$$\n$$\n\\text{Var}_{\\lambda,\\text{before}} = \\frac{1}{5} \\left( (1.6)^2 + 4 \\times (-0.4)^2 \\right)\n$$\n$$\n\\text{Var}_{\\lambda,\\text{before}} = \\frac{1}{5} \\left( 2.56 + 4 \\times 0.16 \\right) = \\frac{1}{5} \\left( 2.56 + 0.64 \\right) = \\frac{3.2}{5} = 0.64\n$$\n\nFor the matrix after size correction, $r_{\\text{after}}=0.1$.\nThe eigenvalues are:\n$\\lambda_{1, \\text{after}} = 1 + (5-1)(0.1) = 1 + 4(0.1) = 1 + 0.4 = 1.4$.\nThe remaining $p-1 = 4$ eigenvalues are:\n$\\lambda_{2,\\dots,5,\\text{after}} = 1 - 0.1 = 0.9$.\nThe set of eigenvalues is $\\{1.4, 0.9, 0.9, 0.9, 0.9\\}$.\nThe sum is $1.4 + 4(0.9) = 1.4 + 3.6 = 5 = p$. The mean is $\\bar{\\lambda}=1$.\nThe variance of these eigenvalues is:\n$$\n\\text{Var}_{\\lambda,\\text{after}} = \\frac{1}{5} \\left( (1.4-1)^2 + 4 \\times (0.9-1)^2 \\right)\n$$\n$$\n\\text{Var}_{\\lambda,\\text{after}} = \\frac{1}{5} \\left( (0.4)^2 + 4 \\times (-0.1)^2 \\right)\n$$\n$$\n\\text{Var}_{\\lambda,\\text{after}} = \\frac{1}{5} \\left( 0.16 + 4 \\times 0.01 \\right) = \\frac{1}{5} \\left( 0.16 + 0.04 \\right) = \\frac{0.2}{5} = 0.04\n$$\n\nFinally, we compute the proportion of eigenvalue variance attributable to size, $P$.\n$$\nP \\;=\\; \\frac{\\text{Var}_{\\lambda,\\text{before}} - \\text{Var}_{\\lambda,\\text{after}}}{\\text{Var}_{\\lambda,\\text{before}}} = \\frac{0.64 - 0.04}{0.64}\n$$\n$$\nP = \\frac{0.60}{0.64} = \\frac{60}{64} = \\frac{15}{16}\n$$\nConverting this fraction to a decimal gives $15 \\div 16 = 0.9375$.\nThe problem requires the answer to be rounded to four significant figures. The value $0.9375$ has exactly four significant figures.", "answer": "$$\\boxed{0.9375}$$", "id": "2590377"}, {"introduction": "Observing a pattern of covariation that appears modular is only the first step; a rigorous analysis must determine if this pattern is stronger than what might occur by chance. This requires moving from descriptive metrics to formal hypothesis testing. This capstone exercise guides you through the design and implementation of a permutation test, a powerful and flexible method for assessing the statistical significance of a modularity hypothesis, providing a complete computational workflow to test for modularity in a dataset [@problem_id:2590370].", "problem": "You are given a matrix of quantitative traits measured on multiple specimens, together with a prespecified modular partition of traits. The scientific objective is to test whether the observed partition reflects modular organization in the sense that covariation among traits is stronger within modules than between modules. Base your reasoning and implementation on core definitions of covariance and exchangeability under the null hypothesis.\n\nLet $X \\in \\mathbb{R}^{n \\times p}$ be a data matrix with $n$ independent specimens (rows) and $p$ traits (columns). Let $S \\in \\mathbb{R}^{p \\times p}$ denote the sample covariance matrix of the columns of $X$, with entries $s_{ij}$ defined by the usual unbiased estimator of covariance. Let the prespecified modular partition be $\\mathcal{M} = \\{M_1, M_2, \\ldots, M_K\\}$, where $M_k \\subset \\{1,2,\\ldots,p\\}$ are disjoint nonempty sets whose union is $\\{1,2,\\ldots,p\\}$.\n\nDefine a scalar modularity index as the ratio of the total between-module covariance to the total within-module covariance, computed from $S$ by summing the off-diagonal covariances as follows. Let\n- the within-module sum be the sum of $s_{ij}$ over all unordered pairs of trait indices $(i,j)$ with $i<j$ such that both $i$ and $j$ belong to the same module in $\\mathcal{M}$, and\n- the between-module sum be the sum of $s_{ij}$ over all unordered pairs $(i,j)$ with $i<j$ such that $i$ and $j$ belong to different modules in $\\mathcal{M}$.\n\nLet the modularity index be the ratio $\\mathrm{R}$ equal to the between-module sum divided by the within-module sum. Smaller values of $\\mathrm{R}$ indicate stronger modularity when covariances are predominantly nonnegative.\n\nNull hypothesis and permutation test. Under the null hypothesis, the observed partition is not specially aligned with the covariance structure of $X$; in particular, given the observed covariance among traits, the assignment of trait labels to module labels is exchangeable among all assignments that preserve the module sizes. A valid permutation procedure therefore permutes trait-to-module labels uniformly at random across all assignments that keep the multiset of module sizes fixed, leaving $X$ (and hence $S$) unchanged. This preserves the within-specimen covariance structure while breaking any specific association between trait identity and module identity implied by $\\mathcal{M}$.\n\nDesign and implement a permutation test with $B$ permutations as follows:\n- Compute the observed $\\mathrm{R}_{\\mathrm{obs}}$ from $S$ and $\\mathcal{M}$.\n- For each permutation $b \\in \\{1,2,\\ldots,B\\}$, generate a random reassignment of the $p$ trait indices to $K$ module labels such that the sizes $\\{|M_1|,\\ldots,|M_K|\\}$ are preserved; compute $\\mathrm{R}_b$ from $S$ and the permuted assignment.\n- Compute the one-tailed permutation $p$-value appropriate for detecting modularity as $p_{\\mathrm{perm}} = \\frac{1 + \\#\\{b: \\mathrm{R}_b \\le \\mathrm{R}_{\\mathrm{obs}}\\}}{1 + B}$.\n\nAll computations must be expressed in purely mathematical terms. Angles and physical units are not involved. Your code must be reproducible by setting the specified pseudorandom number generator seeds.\n\nTest suite. Implement your program to run the following three test cases. In all cases, the data matrix $X$ is sampled as independent draws from a $p$-variate normal distribution with mean vector $0$ and a specified covariance matrix $\\Sigma$, using the given random seed. The observed partition is given as a list of modules specified by zero-based trait indices.\n\n- Test case A (clear modular structure):\n  - Dimensions: $n = 200$, $p = 6$.\n  - Partition: $M_1 = \\{0,1,2\\}$, $M_2 = \\{3,4,5\\}$.\n  - Covariance matrix construction: set $\\Sigma_{ii} = 1$ for all $i$; set $\\Sigma_{ij} = 0.8$ if $i \\neq j$ and $i,j \\in M_1$ or $i,j \\in M_2$; set $\\Sigma_{ij} = 0.05$ if $i \\in M_1$ and $j \\in M_2$ or vice versa.\n  - Data-generation seed: $s_{\\mathrm{data}} = 42$.\n  - Permutation seed: $s_{\\mathrm{perm}} = 4242$.\n  - Number of permutations: $B = 999$.\n\n- Test case B (no modular structure):\n  - Dimensions: $n = 300$, $p = 8$.\n  - Partition: $M_1 = \\{0,1,2,3\\}$, $M_2 = \\{4,5,6,7\\}$.\n  - Covariance matrix construction: set $\\Sigma_{ii} = 1$ for all $i$; set $\\Sigma_{ij} = 0.5$ for all $i \\neq j$.\n  - Data-generation seed: $s_{\\mathrm{data}} = 7$.\n  - Permutation seed: $s_{\\mathrm{perm}} = 7007$.\n  - Number of permutations: $B = 999$.\n\n- Test case C (near-perfect modular structure with three modules):\n  - Dimensions: $n = 180$, $p = 6$.\n  - Partition: $M_1 = \\{0,1\\}$, $M_2 = \\{2,3\\}$, $M_3 = \\{4,5\\}$.\n  - Covariance matrix construction: set $\\Sigma_{ii} = 1$ for all $i$; set $\\Sigma_{ij} = 0.7$ if $i \\neq j$ and $i,j$ are in the same module; set $\\Sigma_{ij} = 0.01$ if $i$ and $j$ are in different modules.\n  - Data-generation seed: $s_{\\mathrm{data}} = 99$.\n  - Permutation seed: $s_{\\mathrm{perm}} = 9090$.\n  - Number of permutations: $B = 999$.\n\nImplementation details:\n- Compute $S$ as the unbiased sample covariance matrix of the columns of $X$.\n- For each test case, report two numbers: the observed modularity index $\\mathrm{R}_{\\mathrm{obs}}$ and the permutation $p$-value $p_{\\mathrm{perm}}$.\n- Round each reported number to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the two results for Test case A, then for Test case B, then for Test case C: $[\\mathrm{R}_{\\mathrm{obs}}^{(A)}, p_{\\mathrm{perm}}^{(A)}, \\mathrm{R}_{\\mathrm{obs}}^{(B)}, p_{\\mathrm{perm}}^{(B)}, \\mathrm{R}_{\\mathrm{obs}}^{(C)}, p_{\\mathrm{perm}}^{(C)}]$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in statistical hypothesis testing, well-posed with all necessary information provided, and expressed in objective, formal language. There are no contradictions, ambiguities, or factual inaccuracies. The problem is a standard, albeit non-trivial, exercise in computational statistics applied to a biological question. We may therefore proceed with the solution.\n\nThe objective is to perform a permutation test to assess the statistical significance of a modular partition of quantitative traits. The core idea is to compare an observed modularity index, $\\mathrm{R}_{\\mathrm{obs}}$, against a null distribution of this index generated by randomly permuting the assignment of traits to modules. Strong modularity is indicated by a small value of the index, signifying that covariances within modules are substantially larger than covariances between modules.\n\nLet the data matrix be $X \\in \\mathbb{R}^{n \\times p}$, comprising $n$ specimens and $p$ traits. The sample covariance matrix $S \\in \\mathbb{R}^{p \\times p}$ is computed, where its element $s_{ij}$ is the unbiased sample covariance between trait $i$ and trait $j$:\n$$ s_{ij} = \\frac{1}{n-1} \\sum_{k=1}^{n} (x_{ki} - \\bar{x}_i)(x_{kj} - \\bar{x}_j) $$\nwhere $\\bar{x}_i$ is the sample mean of the $i$-th trait.\n\nA partition of the $p$ traits into $K$ modules is given as $\\mathcal{M} = \\{M_1, M_2, \\ldots, M_K\\}$, where each $M_k$ is a set of trait indices, $\\bigcup_{k=1}^K M_k = \\{1, 2, \\ldots, p\\}$, and $M_k \\cap M_{k'} = \\emptyset$ for $k \\ne k'$. To facilitate computation, we can define a mapping $\\mathrm{mod}(i)$ that returns the index $k$ of the module $M_k$ containing trait $i$.\n\nThe total within-module off-diagonal covariance, $C_{within}$, is the sum of covariances for all pairs of distinct traits belonging to the same module:\n$$ C_{within} = \\sum_{1 \\le i < j \\le p, \\atop \\mathrm{mod}(i) = \\mathrm{mod}(j)} s_{ij} $$\nThe total between-module covariance, $C_{between}$, is the sum of covariances for all pairs of traits belonging to different modules:\n$$ C_{between} = \\sum_{1 \\le i < j \\le p, \\atop \\mathrm{mod}(i) \\ne \\mathrm{mod}(j)} s_{ij} $$\nThe modularity index, $\\mathrm{R}$, is the ratio of these two sums:\n$$ \\mathrm{R} = \\frac{C_{between}}{C_{within}} $$\nA smaller value of $\\mathrm{R}$ suggests stronger modularity, assuming predominantly positive covariances. We compute this value for the observed partition $\\mathcal{M}$, yielding $\\mathrm{R}_{\\mathrm{obs}}$.\n\nThe null hypothesis, $H_0$, posits that there is no special correspondence between the trait identities and the modular structure. That is, the labels $\\{1, \\ldots, p\\}$ are exchangeable with respect to the partition structure. We test this hypothesis using a permutation test. The procedure entails generating a large number, $B$, of permuted partitions. Each permuted partition is created by randomly permuting the set of all trait indices $\\{1, \\ldots, p\\}$ and then re-assigning them to modules while preserving the original module sizes $\\{|M_1|, \\ldots, |M_K|\\}$.\n\nLet $\\pi$ be a permutation of $\\{1, \\ldots, p\\}$. A permuted assignment can be constructed by creating new modules $M'_k = \\{\\pi(i) | i \\in M_k\\}$. For each of $B$ such random permutations, we compute the modularity index $\\mathrm{R}_b$ using the original covariance matrix $S$ but with the permuted partition. This generates a null distribution of $\\mathrm{R}$ values.\n\nThe one-tailed $p$-value for detecting modularity (i.e., testing if $\\mathrm{R}_{\\mathrm{obs}}$ is significantly small) is then calculated by comparing $\\mathrm{R}_{\\mathrm{obs}}$ to the set of permuted values $\\{\\mathrm{R}_b\\}_{b=1}^B$. The formula is:\n$$ p_{\\mathrm{perm}} = \\frac{1 + |\\{b \\in \\{1, \\ldots, B\\} : \\mathrm{R}_b \\le \\mathrm{R}_{\\mathrm{obs}}\\}|}{1 + B} $$\nThe inclusion of $1$ in both the numerator and the denominator accounts for the observed value itself as an element of the null distribution.\n\nFor each test case specified, the following steps are implemented:\n1.  Construct the true covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$ according to the problem rules.\n2.  Generate a data matrix $X \\in \\mathbb{R}^{n \\times p}$ by drawing $n$ independent samples from the multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, using the specified data-generation seed $s_{\\mathrm{data}}$.\n3.  Compute the unbiased sample covariance matrix $S$ from $X$.\n4.  Compute the observed modularity index $\\mathrm{R}_{\\mathrm{obs}}$ using $S$ and the given partition $\\mathcal{M}$.\n5.  Initialize a pseudorandom number generator with the permutation seed $s_{\\mathrm{perm}}$.\n6.  Perform $B$ permutations. In each iteration, generate a permuted partition, compute the corresponding index $\\mathrm{R}_b$, and compare it to $\\mathrm{R}_{\\mathrm{obs}}$.\n7.  Calculate the final $p$-value, $p_{\\mathrm{perm}}$, based on the count of permuted indices less than or equal to $\\mathrm{R}_{\\mathrm{obs}}$.\n8.  The resulting values, $\\mathrm{R}_{\\mathrm{obs}}$ and $p_{\\mathrm{perm}}$, are rounded to $6$ decimal places.\n\nThis procedure is applied to each of the three test cases, and the results are aggregated into the required final format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def construct_sigma(p, partition, within_cov, between_cov):\n        \"\"\"Constructs the true covariance matrix Sigma.\"\"\"\n        sigma = np.full((p, p), between_cov)\n        for module in partition:\n            # Create index arrays for advanced indexing\n            indices = np.ix_(module, module)\n            sigma[indices] = within_cov\n        np.fill_diagonal(sigma, 1.0)\n        return sigma\n\n    def calculate_modularity_index(S, p, module_map):\n        \"\"\"Calculates the modularity index R.\"\"\"\n        within_sum = 0.0\n        between_sum = 0.0\n        for i in range(p):\n            for j in range(i + 1, p):\n                if module_map[i] == module_map[j]:\n                    within_sum += S[i, j]\n                else:\n                    between_sum += S[i, j]\n        \n        if within_sum == 0:\n            # Handle the case of zero within-module covariance to avoid division by zero.\n            # This is unlikely given the problem setup but is good practice.\n            return np.inf if between_sum > 0 else 0.0\n\n        return between_sum / within_sum\n\n    def run_permutation_test(n, p, partition, sigma_params, s_data, s_perm, B):\n        \"\"\"\n        Runs the full analysis for a single test case.\n        \"\"\"\n        # 1. Construct Sigma and generate data\n        rng_data = np.random.default_rng(s_data)\n        sigma = construct_sigma(p, partition, sigma_params['within'], sigma_params['between'])\n        mean_vec = np.zeros(p)\n        X = rng_data.multivariate_normal(mean=mean_vec, cov=sigma, size=n)\n\n        # 2. Compute the sample covariance matrix S\n        # np.cov with rowvar=False and default ddof=1 computes the unbiased estimator\n        S = np.cov(X, rowvar=False)\n\n        # 3. Compute the observed modularity index\n        # Create a map from trait index to module index for efficient lookup\n        module_map_obs = np.zeros(p, dtype=int)\n        for module_idx, module in enumerate(partition):\n            for trait_idx in module:\n                module_map_obs[trait_idx] = module_idx\n        \n        r_obs = calculate_modularity_index(S, p, module_map_obs)\n        \n        # 4. Perform the permutation test\n        rng_perm = np.random.default_rng(s_perm)\n        trait_indices = np.arange(p)\n        module_sizes = [len(m) for m in partition]\n        module_starts = np.concatenate(([0], np.cumsum(module_sizes[:-1])))\n\n        count_le = 0\n        for _ in range(B):\n            permuted_indices = rng_perm.permutation(trait_indices)\n            \n            # Create the permuted module map\n            module_map_perm = np.zeros(p, dtype=int)\n            for module_idx, start_pos in enumerate(module_starts):\n                 size = module_sizes[module_idx]\n                 permuted_module_traits = permuted_indices[start_pos : start_pos + size]\n                 for trait_idx in permuted_module_traits:\n                     module_map_perm[trait_idx] = module_idx\n            \n            r_perm = calculate_modularity_index(S, p, module_map_perm)\n            \n            if r_perm <= r_obs:\n                count_le += 1\n        \n        # 5. Calculate the p-value\n        p_perm = (1 + count_le) / (1 + B)\n\n        return round(r_obs, 6), round(p_perm, 6)\n    \n    # Define test cases from the problem statement\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 200, \"p\": 6, \"B\": 999,\n            \"partition\": [[0, 1, 2], [3, 4, 5]],\n            \"sigma_params\": {\"within\": 0.8, \"between\": 0.05},\n            \"s_data\": 42, \"s_perm\": 4242\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 300, \"p\": 8, \"B\": 999,\n            \"partition\": [[0, 1, 2, 3], [4, 5, 6, 7]],\n            \"sigma_params\": {\"within\": 0.5, \"between\": 0.5},\n            \"s_data\": 7, \"s_perm\": 7007\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 180, \"p\": 6, \"B\": 999,\n            \"partition\": [[0, 1], [2, 3], [4, 5]],\n            \"sigma_params\": {\"within\": 0.7, \"between\": 0.01},\n            \"s_data\": 99, \"s_perm\": 9090\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        r_obs, p_perm = run_permutation_test(\n            n=case[\"n\"],\n            p=case[\"p\"],\n            partition=case[\"partition\"],\n            sigma_params=case[\"sigma_params\"],\n            s_data=case[\"s_data\"],\n            s_perm=case[\"s_perm\"],\n            B=case[\"B\"]\n        )\n        all_results.extend([r_obs, p_perm])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "2590370"}]}