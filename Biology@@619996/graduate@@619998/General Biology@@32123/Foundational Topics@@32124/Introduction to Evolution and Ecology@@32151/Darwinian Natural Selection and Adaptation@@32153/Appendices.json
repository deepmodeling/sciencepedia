{"hands_on_practices": [{"introduction": "Simple mathematical models of selection are powerful tools for building intuition about fundamental evolutionary dynamics. This exercise explores overdominance, or heterozygote advantage, a classic form of balancing selection that explains how genetic variation can be actively maintained in a population. By deriving the equilibrium allele frequency from first principles, you will gain hands-on experience with the core mechanics of how viability selection shapes genetic polymorphisms. [@problem_id:2791273]", "problem": "Consider a large, randomly mating diploid population with non-overlapping generations, negligible mutation and migration, and density-independent viability selection. Let the two alleles at a single locus be denoted by $A$ and $a$, with allele $A$ at frequency $p$ before selection and $a$ at frequency $q = 1 - p$. Assume Hardy–Weinberg proportions before selection. The relative viabilities (fitnesses) of genotypes are given by $W_{AA} = 1 - s$, $W_{Aa} = 1$, and $W_{aa} = 1 - t$, where $s > 0$ and $t > 0$ are selection coefficients describing a heterozygote advantage (overdominance) regime in which the heterozygote has the highest viability.\n\nUsing only the standard definitions of viability selection in a randomly mating population and Hardy–Weinberg proportions, derive the exact closed-form expression for the internal stable equilibrium allele frequency $p^{*}$ of allele $A$ (with $0 < p^{*} < 1$) at which the allele frequency is stationary under selection. Express your final answer as a symbolic expression in terms of $s$ and $t$. No numerical substitution is required. The final answer must be a single analytic expression with no units.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It represents a fundamental case of balancing selection in population genetics. All necessary information is provided, and the premises are consistent. Therefore, we may proceed with a formal derivation.\n\nThe problem describes a diploid population with two alleles, $A$ and $a$, at a single locus. Before viability selection, the population is assumed to be in Hardy–Weinberg equilibrium. The frequencies of the three genotypes $AA$, $Aa$, and $aa$ are given by $p^2$, $2pq$, and $q^2$ respectively, where $p$ is the frequency of allele $A$ and $q = 1-p$ is the frequency of allele $a$.\n\nThe relative viabilities (fitnesses) are given as $W_{AA} = 1 - s$, $W_{Aa} = 1$, and $W_{aa} = 1 - t$, with $s > 0$ and $t > 0$. These fitness values are multiplied by the genotype frequencies before selection to determine the proportional contribution of each genotype to the next generation.\n\nThe frequency of each genotype after selection is proportional to:\n- For $AA$: $p^2 W_{AA} = p^2 (1 - s)$\n- For $Aa$: $2pq W_{Aa} = 2pq$\n- For $aa$: $q^2 W_{aa} = q^2 (1 - t)$\n\nThe sum of these proportional frequencies gives the mean fitness of the population, denoted by $\\bar{W}$. This serves as the normalization factor to ensure the frequencies in the next generation sum to $1$.\n$$ \\bar{W} = p^2 W_{AA} + 2pq W_{Aa} + q^2 W_{aa} $$\n$$ \\bar{W} = p^2(1-s) + 2pq + q^2(1-t) $$\n\nThe frequency of allele $A$ in the next generation, denoted $p'$, is the frequency of the $AA$ genotype plus half the frequency of the $Aa$ genotype after selection and normalization.\n$$ p' = \\frac{p^2 W_{AA} + \\frac{1}{2}(2pq W_{Aa})}{\\bar{W}} = \\frac{p^2 W_{AA} + pq W_{Aa}}{\\bar{W}} $$\nSubstituting the given fitnesses:\n$$ p' = \\frac{p^2(1-s) + pq(1)}{\\bar{W}} = \\frac{p(p(1-s) + q)}{\\bar{W}} $$\n\nAn equilibrium exists when the allele frequency does not change from one generation to the next, which means $\\Delta p = p' - p = 0$. This condition is met at the trivial equilibria $p = 0$ and $p = 1$, or at an internal equilibrium $p^{*}$ (where $0 < p^{*} < 1$) where the non-trivial part of the change equation is zero.\n$$ \\Delta p = p' - p = \\frac{p(p(1-s) + q)}{\\bar{W}} - p = \\frac{p(p(1-s) + q) - p\\bar{W}}{\\bar{W}} $$\nFor an internal equilibrium $p^*$, we require the numerator to be zero (while $p^* \\neq 0$):\n$$ p^*(p^*(1-s) + q^*) - p^*\\bar{W} = 0 $$\n$$ p^*(1-s) + q^* - \\bar{W} = 0 $$\nSubstituting the expression for $\\bar{W}$:\n$$ p(1-s) + q - [p^2(1-s) + 2pq + q^2(1-t)] = 0 $$\nUsing the identity $q = 1-p$:\n$$ p(1-s) + (1-p) - [p^2(1-s) + 2p(1-p) + (1-p)^2(1-t)] = 0 $$\n$$ p - ps + 1 - p - [p^2 - p^2s + 2p - 2p^2 + (1-2p+p^2)(1-t)] = 0 $$\n$$ 1 - ps - [ -p^2 - p^2s + 2p + 1 - 2p + p^2 - t(1-p)^2 ] = 0 $$\n$$ 1 - ps - [ 1 - p^2s - t(1-p)^2 ] = 0 $$\n$$ 1 - ps - 1 + p^2s + t(1-p)^2 = 0 $$\n$$ -ps + p^2s + t(1-p)^2 = 0 $$\nSince we are seeking $p^* \\in (0, 1)$, we can divide by $p$ and $1-p=q$.\n$$ p(1-p)s(-1) + tp(1-p)^2 / p = 0 $$\nThe equation is $s p(p-1) + t (1-p)^2 = 0$. This simplifies to $-spq + tq^2 = 0$. Since $q \\neq 0$, we can divide by $q$.\n$$ -sp + tq = 0 $$\nThis is a much more direct route. At the equilibrium frequency $p^*$, we have:\n$$ -sp^* + tq^* = 0 $$\nSubstitute $q^* = 1 - p^*$:\n$$ -sp^* + t(1-p^*) = 0 $$\n$$ -sp^* + t - tp^* = 0 $$\n$$ t = sp^* + tp^* $$\n$$ t = p^*(s+t) $$\nSolving for the equilibrium allele frequency $p^*$:\n$$ p^{*} = \\frac{t}{s+t} $$\nThe problem specifies heterozygote advantage (overdominance), where $s > 0$ and $t > 0$. This ensures that $s+t > t$ and $s+t > s$. Consequently, $0 < p^* < 1$, confirming that this is an internal equilibrium. This equilibrium is also globally stable, as selection will always drive the allele frequency towards this value from any starting frequency other than $0$ or $1$.", "answer": "$$\\boxed{\\frac{t}{s+t}}$$", "id": "2791273"}, {"introduction": "While single-locus models are insightful, most adaptively important traits, such as body size or metabolic rate, are quantitative and influenced by many genes. This practice applies the breeder's equation, a cornerstone of quantitative genetics that provides a remarkably simple yet powerful prediction for the short-term evolutionary response to selection on a continuous trait. You will learn to connect key evolutionary parameters—heritability ($h^2$), the selection differential ($S$), and the response to selection ($R$)—to forecast how a population's mean phenotype changes across generations. [@problem_id:2791233]", "problem": "A large, randomly mating, panmictic population of an annual plant exhibits a quantitative trait $z$ (seed mass) measured in milligrams (mg). Assume the following biological conditions hold: additive gene action with no dominance or epistasis, no genotype-by-environment interaction, no transmission bias, and no environmentally induced similarity between parents and offspring beyond what is mediated by genes. Selection acts on adult phenotypes prior to reproduction, and reproductive success is proportional to absolute fitness. The additive genetic variance is $V_{A} = 0.5$ mg$^{2}$, the phenotypic variance is $V_{P} = 1.0$ mg$^{2}$, and the phenotypic selection differential in the parental generation is $S = 1.2$ mg. The current population mean seed mass is $\\bar{z}_{0} = 23.17$ mg.\n\nStarting from foundational principles of quantitative genetics and evolutionary theory, including the definition of narrow-sense heritability $h^{2}$ as the ratio of additive genetic variance to phenotypic variance, and the definition of the phenotypic selection differential $S$ as the difference between the mean phenotype of selected parents and the mean phenotype of the whole population, derive the expected response of the offspring generation mean to selection under the assumptions stated. Then use the provided values to compute the predicted next-generation mean seed mass.\n\nExpress the final answer as a single number in milligrams. Round your answer to four significant figures.", "solution": "The problem presents a standard scenario in quantitative genetics and is deemed valid. It is self-contained, scientifically grounded in the principles of evolutionary theory, and well-posed. All necessary data and assumptions are provided to derive a unique, meaningful solution.\n\nThe objective is to predict the mean seed mass of the next generation, denoted as $\\bar{z}_{1}$, following one episode of selection. The starting point is the foundational principles of evolutionary quantitative genetics.\n\nThe change in the mean phenotype of a population from one generation to the next is defined as the response to selection, $R$. It is the difference between the mean phenotype of the offspring generation, $\\bar{z}_{1}$, and the mean phenotype of the parental generation before selection, $\\bar{z}_{0}$.\n$$R = \\bar{z}_{1} - \\bar{z}_{0}$$\n\nThe breeder's equation provides the predictive relationship between the response to selection, $R$, the narrow-sense heritability, $h^{2}$, and the selection differential, $S$.\n$$R = h^{2}S$$\nThe assumptions stated in the problem—additive gene action, no dominance or epistasis, no genotype-by-environment interaction—are precisely the conditions under which this linear relationship is expected to hold.\n\nThe problem requires us to proceed from the definitions of the components of this equation.\nNarrow-sense heritability, $h^{2}$, is defined as the proportion of the total phenotypic variance, $V_{P}$, that is attributable to the additive effects of genes, represented by the additive genetic variance, $V_{A}$.\n$$h^{2} = \\frac{V_{A}}{V_{P}}$$\nThis quantity measures the extent to which phenotypic differences among individuals are reliably transmitted to their offspring, and it is the key determinant of the potential for a population to respond to selection.\n\nThe selection differential, $S$, is defined as the difference between the mean phenotype of the individuals selected to be parents, $\\bar{z}_{\\text{sel}}$, and the mean phenotype of the entire parental population before selection, $\\bar{z}_{0}$.\n$$S = \\bar{z}_{\\text{sel}} - \\bar{z}_{0}$$\nIt quantifies the strength of directional selection acting on the phenotype within a generation.\n\nBy substituting the definition of $R$ and $h^{2}$ into the breeder's equation, we obtain a comprehensive expression for the expected mean of the next generation.\nFirst, we express the response $R$ in terms of the fundamental variances and the selection differential:\n$$R = \\left(\\frac{V_{A}}{V_{P}}\\right)S$$\nThen, we use the definition of $R$ to solve for the next-generation mean, $\\bar{z}_{1}$:\n$$\\bar{z}_{1} - \\bar{z}_{0} = \\left(\\frac{V_{A}}{V_{P}}\\right)S$$\n$$\\bar{z}_{1} = \\bar{z}_{0} + \\left(\\frac{V_{A}}{V_{P}}\\right)S$$\nThis is the final analytical expression derived from the specified foundational principles.\n\nNow, we substitute the numerical values provided in the problem statement into this equation:\n- Current population mean: $\\bar{z}_{0} = 23.17$ mg\n- Additive genetic variance: $V_{A} = 0.5$ mg$^{2}$\n- Phenotypic variance: $V_{P} = 1.0$ mg$^{2}$\n- Selection differential: $S = 1.2$ mg\n\nFirst, we calculate the narrow-sense heritability, $h^{2}$:\n$$h^{2} = \\frac{0.5 \\text{ mg}^{2}}{1.0 \\text{ mg}^{2}} = 0.5$$\nThe heritability is a dimensionless quantity.\n\nNext, we calculate the response to selection, $R$:\n$$R = h^{2}S = (0.5) \\times (1.2 \\text{ mg}) = 0.6 \\text{ mg}$$\n\nFinally, we calculate the predicted mean seed mass for the next generation, $\\bar{z}_{1}$:\n$$\\bar{z}_{1} = \\bar{z}_{0} + R = 23.17 \\text{ mg} + 0.6 \\text{ mg} = 23.77 \\text{ mg}$$\n\nThe problem requires the final answer to be rounded to four significant figures. The calculated value of $23.77$ already contains exactly four significant figures. Therefore, no further rounding is necessary.", "answer": "$$\\boxed{23.77}$$", "id": "2791233"}, {"introduction": "The ultimate test of evolutionary theory lies in its ability to explain and quantify patterns in empirical data. This advanced practice bridges the gap between deterministic models and statistical inference, demonstrating how to estimate a selection coefficient ($s$) from noisy, time-series allele frequency data. By using the framework of a Generalized Linear Model (GLM), you will engage with a powerful and widely used technique in modern evolutionary biology for quantifying the strength of natural selection from experimental or observational datasets. [@problem_id:2791293]", "problem": "You are given independent time series of sampled allele counts from a single biallelic locus evolving in discrete non-overlapping generations under haploid viability selection with constant relative fitnesses. Let the derived allele have relative fitness $1+s$ and the ancestral allele have relative fitness $1$, with $s &gt; -1$. Assume no mutation, no migration, and a constant environment. At sampling generation $t_i$, you observe $x_i$ derived alleles in a sample of $n_i$ chromosomes. Under the Wright-Fisher framework, conditional on the population allele frequency $p_{t_i}$, the observation model is binomial: $x_i \\sim \\mathrm{Binomial}(n_i, p_{t_i})$. The deterministic expectation of $p_{t}$ under selection follows from the definition of relative fitness and standard discrete-generation frequency updating. Your task is to:\n\n- Derive an estimable, linear-in-parameters representation (after an appropriate transformation) of the deterministic trajectory implied by constant relative fitness $1+s$ per generation, starting from an unknown initial condition.\n- Construct the binomial log-likelihood for the observations $\\{(t_i, x_i, n_i)\\}_{i=1}^m$ by treating $\\{p_{t_i}\\}$ as the mean function prescribed by your derived representation.\n- Maximize this likelihood with respect to the parameters, and then map the fitted parameters back to obtain the maximum likelihood estimate of the selection coefficient $s$.\n\nScientific realism constraints: You must account for the binomial sampling variance at each time point through the likelihood (equivalently, via the canonical-link Generalized Linear Model formulation). You must ensure the predicted mean satisfies $0 &lt; p_{t_i} &lt; 1$ for all $i$.\n\nNumerical constraints and output specification:\n- Implement an algorithm that is guaranteed to produce finite estimates for the provided datasets (for example, by using the canonical logistic link for the binomial likelihood and an Iteratively Reweighted Least Squares procedure with safeguards to keep $p_{t_i}$ strictly within $(0,1)$).\n- For each dataset below, compute the maximum likelihood estimate $\\hat{s}$ as a real number. Report each $\\hat{s}$ rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[0.123456,-0.000001,0.500000]$.\n\nTest suite (times are in generations; there are no physical units to report):\n- Case $1$ (general happy path, multiple time points): $t = [0,1,2,3]$, $n = [100,110,125,118]$, $x = [20,30,45,54]$.\n- Case $2$ (boundary condition of neutrality): $t = [0,1,2,3]$, $n = [70,140,91,77]$, $x = [20,40,26,22]$.\n- Case $3$ (negative selection): $t = [0,1,2,3]$, $n = [80,140,190,106]$, $x = [30,40,40,16]$.\n- Case $4$ (edge case with minimal time points and unequal spacing): $t = [0,5]$, $n = [20,371]$, $x = [4,243]$.\n\nFinal output format requirement:\n- Produce a single line that is a valid Python list literal of four floats, each rounded to exactly $6$ decimal places, in the order of Cases $1$ through $4$, for example $[\\hat{s}_1,\\hat{s}_2,\\hat{s}_3,\\hat{s}_4]$.", "solution": "The problem is subjected to rigorous validation and is found to be valid. It is a well-posed problem in statistical population genetics, grounded in established scientific principles. We may proceed with a solution.\n\nThe objective is to derive the maximum likelihood estimate of the selection coefficient, $s$, from time-series data of allele counts. This task comprises three main components: first, deriving the deterministic equation for allele frequency change; second, formulating this within a statistical framework suitable for count data; and third, implementing a numerical procedure for parameter estimation.\n\nLet $p_t$ be the frequency of the derived allele and $q_t = 1 - p_t$ be the frequency of the ancestral allele at generation $t$. The relative fitness of the derived allele is $w_1 = 1+s$, and for the ancestral allele, it is $w_0 = 1$. Under a haploid selection model with discrete generations, the mean fitness of the population at generation $t$ is $\\bar{w}_t = p_t w_1 + q_t w_0 = p_t(1+s) + (1-p_t)(1) = 1 + s p_t$.\n\nThe frequency of the derived allele in the subsequent generation, $p_{t+1}$, is given by its proportional contribution to the gene pool after selection:\n$$p_{t+1} = \\frac{p_t w_1}{\\bar{w}_t} = \\frac{p_t(1+s)}{1+sp_t}$$\nThe frequency of the ancestral allele is similarly updated:\n$$q_{t+1} = \\frac{q_t w_0}{\\bar{w}_t} = \\frac{q_t}{1+sp_t}$$\nTo find a tractable, linearizable expression for the frequency trajectory, we examine the ratio of the allele frequencies, $R_t = p_t / q_t$. The recurrence for this ratio is:\n$$\\frac{p_{t+1}}{q_{t+1}} = \\frac{p_t(1+s)}{q_t} = (1+s)\\left(\\frac{p_t}{q_t}\\right)$$\nThis reveals that the ratio of allele frequencies grows geometrically each generation by a factor of $1+s$. Iterating this relation from an initial generation $t=0$ gives:\n$$R_t = R_0 (1+s)^t$$\nwhere $R_0 = p_0 / (1-p_0)$. To linearize this relationship with respect to time $t$, we take the natural logarithm:\n$$\\ln(R_t) = \\ln(R_0) + t \\ln(1+s)$$\nThe term $\\ln(R_t) = \\ln(p_t / (1-p_t))$ is the logit transformation of the allele frequency $p_t$, denoted as $\\text{logit}(p_t)$. Thus, we have derived the linear model for the transformed allele frequency:\n$$\\text{logit}(p_t) = \\beta_0 + \\beta_1 t$$\nwhere the parameters to be estimated are $\\beta_0 = \\text{logit}(p_0)$ and $\\beta_1 = \\ln(1+s)$. This is the required linear-in-parameters representation of the deterministic trajectory.\n\nThe statistical model for the observed data must account for the sampling process. At each time point $t_i$, we observe $x_i$ derived alleles in a sample of $n_i$ chromosomes. This process is modeled by a Binomial distribution:\n$$x_i \\sim \\mathrm{Binomial}(n_i, p_{t_i})$$\nwhere $p_{t_i}$ is the true population allele frequency at generation $t_i$. The complete dataset is $\\{(t_i, x_i, n_i)\\}_{i=1}^m$.\n\nThis structure is precisely that of a Generalized Linear Model (GLM). The components are:\n1.  **Random Component**: The observations $x_i$ follow a Binomial distribution. The response variable can be viewed as the proportions $y_i = x_i/n_i$.\n2.  **Systematic Component**: A linear predictor, $\\eta_i$, is a linear function of the covariates. In this case, the only covariate is time, $t_i$. The linear predictor is $\\eta_i = \\beta_0 + \\beta_1 t_i$.\n3.  **Link Function**: A function $g(\\cdot)$ relates the expected value of the response, $\\mu_i = \\mathbb{E}[y_i] = p_{t_i}$, to the linear predictor: $g(\\mu_i) = \\eta_i$. Our derivation shows this to be the logit function, $g(p_{t_i}) = \\text{logit}(p_{t_i})$.\n\nThe logit link is the canonical link function for the Binomial family, which offers theoretical and computational advantages. The log-likelihood function for the entire dataset, expressed in terms of the parameters $\\beta_0$ and $\\beta_1$, is:\n$$L(\\beta_0, \\beta_1) = \\sum_{i=1}^{m} \\left( \\ln\\binom{n_i}{x_i} + x_i \\ln(p_{t_i}) + (n_i - x_i) \\ln(1 - p_{t_i}) \\right)$$\nSubstituting $p_{t_i} = \\text{logit}^{-1}(\\eta_i) = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}$ with $\\eta_i = \\beta_0 + \\beta_1 t_i$, we obtain the likelihood to be maximized with respect to $\\beta = (\\beta_0, \\beta_1)^T$.\n\nMaximization is performed numerically using the Iteratively Reweighted Least Squares (IRLS) algorithm, which is equivalent to Fisher scoring for canonical GLMs. The algorithm is as follows:\n1.  Initialize parameter vector $\\boldsymbol{\\beta}^{(0)}$, for example, as $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$.\n2.  For iteration $k=0, 1, 2, \\dots$ until convergence:\n    a.  Compute the linear predictor for each observation: $\\boldsymbol{\\eta}^{(k)} = \\mathbf{X}\\boldsymbol{\\beta}^{(k)}$, where $\\mathbf{X}$ is the $m \\times 2$ design matrix with rows $[1, t_i]$.\n    b.  Compute the current fitted probabilities (means): $\\boldsymbol{\\mu}^{(k)} = g^{-1}(\\boldsymbol{\\eta}^{(k)})$, where $g^{-1}$ is the logistic function.\n    c.  Construct the working response vector $\\mathbf{z}^{(k)}$: $z_i^{(k)} = \\eta_i^{(k)} + \\frac{y_i - \\mu_i^{(k)}}{\\mu_i^{(k)}(1-\\mu_i^{(k)})}$, where $y_i = x_i/n_i$.\n    d.  Construct the diagonal weight matrix $\\mathbf{W}^{(k)}$ with elements $W_{ii}^{(k)} = n_i \\mu_i^{(k)}(1-\\mu_i^{(k)})$.\n    e.  Update the parameters by solving the weighted least squares equation: $\\boldsymbol{\\beta}^{(k+1)} = (\\mathbf{X}^T \\mathbf{W}^{(k)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(k)} \\mathbf{z}^{(k)}$.\n3.  The process is terminated when the change in $\\boldsymbol{\\beta}$ between iterations is below a specified tolerance. To ensure numerical stability, fitted probabilities $\\mu_i^{(k)}$ are constrained to remain strictly within $(0, 1)$ by clipping them away from the boundaries, for instance, to $[\\epsilon, 1-\\epsilon]$ for a small $\\epsilon>0$.\n\nUpon convergence, the algorithm yields the maximum likelihood estimate $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T$. From our initial derivation, we have the relationship $\\hat{\\beta}_1 = \\ln(1+\\hat{s})$. Therefore, the maximum likelihood estimate for the selection coefficient $s$ is:\n$$\\hat{s} = e^{\\hat{\\beta}_1} - 1$$\nThis procedure provides a complete and robust method for estimating the selection coefficient from the given time-series data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_selection_glm(t, x, n, tol=1e-8, max_iter=25, epsilon=1e-10):\n    \"\"\"\n    Computes the maximum likelihood estimate of the selection coefficient 's'\n    using an Iteratively Reweighted Least Squares (IRLS) algorithm for a\n    Binomial GLM with a logit link.\n\n    Args:\n        t (np.ndarray): Array of time points (generations).\n        x (np.ndarray): Array of derived allele counts.\n        n (np.ndarray): Array of sample sizes.\n        tol (float): Convergence tolerance for the beta parameters.\n        max_iter (int): Maximum number of iterations.\n        epsilon (float): Small value to prevent probabilities of 0 or 1.\n\n    Returns:\n        float: The maximum likelihood estimate of the selection coefficient, s.\n    \"\"\"\n    # Design matrix X, with an intercept column and a time column.\n    X = np.vstack([np.ones(len(t)), t]).T\n    \n    # Observed proportions\n    y = x / n\n    \n    # Initial guess for beta (b0, b1)\n    beta = np.zeros(X.shape[1])\n    \n    for i in range(max_iter):\n        # Linear predictor eta = b0 + b1*t\n        eta = X @ beta\n        \n        # Mean response (probabilities) from the inverse link function (logistic)\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Safeguard: clip probabilities to avoid division by zero or log(0)\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        # Derivative of the link function, g'(mu) = 1/(mu*(1-mu))\n        g_prime_mu = 1.0 / (mu * (1.0 - mu))\n        \n        # Working response (adjusted dependent variable) z\n        z = eta + (y - mu) * g_prime_mu\n        \n        # Weights for IRLS\n        # For Binomial GLM, W_ii = n_i * V(mu_i) * (g'(mu_i))^2, where V is variance function\n        # For canonical logit link, g'(mu) = 1/V(mu), so W_ii = n_i * V(mu_i)\n        # V(mu) for Binomial/Bernoulli is mu*(1-mu)\n        w = n * mu * (1.0 - mu)\n        W = np.diag(w)\n        \n        # Update beta by solving the weighted least squares problem:\n        # beta_new = (X^T * W * X)^-1 * X^T * W * z\n        try:\n            # Equivalent to np.linalg.inv(X.T @ W @ X) @ X.T @ W @ z but more stable\n            beta_new = np.linalg.solve(X.T @ W @ X, X.T @ W @ z)\n        except np.linalg.LinAlgError:\n            print(\"Error: Singular matrix encountered in IRLS. The data may be uninformative.\")\n            return np.nan\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n            \n        beta = beta_new\n\n    # The estimated coefficient for time is beta[1]\n    # We have beta_1 = log(1+s)\n    beta_1_hat = beta[1]\n    s_hat = np.exp(beta_1_hat) - 1.0\n    \n    return s_hat\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general happy path, multiple time points)\n        {'t': [0, 1, 2, 3], 'n': [100, 110, 125, 118], 'x': [20, 30, 45, 54]},\n        # Case 2 (boundary condition of neutrality)\n        {'t': [0, 1, 2, 3], 'n': [70, 140, 91, 77], 'x': [20, 40, 26, 22]},\n        # Case 3 (negative selection)\n        {'t': [0, 1, 2, 3], 'n': [80, 140, 190, 106], 'x': [30, 40, 40, 16]},\n        # Case 4 (edge case with minimal time points and unequal spacing)\n        {'t': [0, 5], 'n': [20, 371], 'x': [4, 243]}\n    ]\n\n    results = []\n    for case in test_cases:\n        t_arr = np.array(case['t'], dtype=float)\n        x_arr = np.array(case['x'], dtype=float)\n        n_arr = np.array(case['n'], dtype=float)\n        \n        s_hat = fit_selection_glm(t_arr, x_arr, n_arr)\n        results.append(s_hat)\n    \n    # Format the output as a list of floats rounded to 6 decimal places\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "2791293"}]}