{"hands_on_practices": [{"introduction": "A fundamental principle of brain organization is that the amount of neural tissue devoted to a sensory or motor function is proportional to its importance and required precision, not its physical size. This exercise explores this concept of \"cortical magnification\" in the somatosensory system. By formalizing the relationship between tactile acuity (two-point discrimination) and the size of its cortical representation, you will see how a simple, powerful model can connect psychophysical data to neuroanatomical organization, offering a quantitative glimpse into the famous sensory homunculus [@problem_id:2779950].", "problem": "In the primary somatosensory cortex (S1), body surface is represented topographically with nonuniform areal allocation, often summarized by the concept of cortical magnification. Consider a simplified mechanistic model for two-point discrimination in which two punctate tactile stimuli separated by a skin distance $d$ evoke in S1 two peaks of activity whose cortical separation is $M \\, d$, where $M$ is the local areal cortical magnification factor expressed as cortical distance per unit skin distance. Assume that a just-noticeable difference (JND) for two-point discrimination is achieved when the cortical separation between the two activity peaks reaches a fixed criterion distance $\\delta_{c}$ that is invariant across body sites under matched stimulus conditions and comparable receptor submodalities.\n\nGiven the following empirically observed two-point discrimination thresholds at the skin:\n- Fingertip threshold: $d_{\\mathrm{finger}} = 2\\,\\mathrm{mm}$.\n- Forearm threshold: $d_{\\mathrm{arm}} = 40\\,\\mathrm{mm}$.\n\nUnder the assumptions above and taking as fundamental bases that (i) S1 contains a topographic map of the body surface and (ii) JND arises when the neural representation surpasses a fixed separability criterion in cortex, derive from first principles the relationship connecting $d$ and $M$ implied by the model, and then compute the ratio of cortical magnification factors $M_{\\mathrm{finger}}/M_{\\mathrm{arm}}$.\n\nExpress your final answer as a single dimensionless number (no units). No rounding is required, but if you choose to approximate any intermediate quantity, round the final answer to four significant figures.", "solution": "The problem presents a valid, well-posed scenario based on established principles in sensory neuroscience. It provides a simplified mechanistic model of two-point discrimination, which links perceptual thresholds to cortical representations. The model is based on the concept of cortical magnification and the hypothesis of a constant neural separability criterion for discrimination. All givens are explicit, consistent, and scientifically plausible. I will therefore proceed with the derivation and solution.\n\nThe problem statement defines a relationship between the distance between two stimuli on the skin, denoted by $d$, and the corresponding distance between the peaks of neural activity in the primary somatosensory cortex, $S1$. This cortical distance is given by the product $M \\cdot d$, where $M$ is the local cortical magnification factor. The factor $M$ is defined as cortical distance per unit skin distance, so it functions as a linear magnification factor.\n\nThe central assumption for two-point discrimination is that a just-noticeable difference (JND) is perceived when the cortical separation between the two activity peaks reaches a fixed, criterion distance, $\\delta_{c}$. This criterion $\\delta_{c}$ is postulated to be invariant across different body regions.\n\nLet $d_{threshold}$ be the two-point discrimination threshold at a specific skin location. According to the model, this is the skin distance $d$ for which the cortical separation equals $\\delta_{c}$. Therefore, we can write the fundamental equation of the model:\n$$M \\cdot d_{\\mathrm{threshold}} = \\delta_c$$\nThis equation represents the relationship connecting the perceptual threshold $d$ (specifically, $d_{threshold}$) and the cortical magnification factor $M$, derived from the first principles given in the problem. Since $\\delta_c$ is a constant, this relationship shows that the two-point discrimination threshold, $d_{\\mathrm{threshold}}$, is inversely proportional to the cortical magnification factor, $M$.\n$$d_{\\mathrm{threshold}} = \\frac{\\delta_c}{M}$$\nA higher magnification factor $M$ leads to a smaller, or finer, discrimination threshold $d_{\\mathrm{threshold}}$.\n\nWe are given empirical data for two body sites: the fingertip and the forearm. Let the cortical magnification factor for the fingertip be $M_{\\mathrm{finger}}$ and its discrimination threshold be $d_{\\mathrm{finger}}$. Similarly, for the forearm, we have $M_{\\mathrm{arm}}$ and $d_{\\mathrm{arm}}$. Applying the derived relationship to each site yields two equations:\n$1$. For the fingertip: $M_{\\mathrm{finger}} \\cdot d_{\\mathrm{finger}} = \\delta_c$\n$2$. For the forearm: $M_{\\mathrm{arm}} \\cdot d_{\\mathrm{arm}} = \\delta_c$\n\nBecause the criterion distance $\\delta_c$ is invariant, the left-hand sides of these two equations must be equal:\n$$M_{\\mathrm{finger}} \\cdot d_{\\mathrm{finger}} = M_{\\mathrm{arm}} \\cdot d_{\\mathrm{arm}}$$\nThe problem requires the computation of the ratio of the cortical magnification factors, $\\frac{M_{\\mathrm{finger}}}{M_{\\mathrm{arm}}}$. We can obtain this ratio through a simple algebraic rearrangement of the above equality:\n$$\\frac{M_{\\mathrm{finger}}}{M_{\\mathrm{arm}}} = \\frac{d_{\\mathrm{arm}}}{d_{\\mathrm{finger}}}$$\nNow, we substitute the given numerical values for the discrimination thresholds:\n$$d_{\\mathrm{finger}} = 2\\,\\mathrm{mm}$$\n$$d_{\\mathrm{arm}} = 40\\,\\mathrm{mm}$$\nSubstituting these values into the expression for the ratio gives:\n$$\\frac{M_{\\mathrm{finger}}}{M_{\\mathrm{arm}}} = \\frac{40\\,\\mathrm{mm}}{2\\,\\mathrm{mm}}$$\nThe units of millimeters ($\\mathrm{mm}$) in the numerator and denominator cancel, yielding a dimensionless ratio, as required.\n$$\\frac{M_{\\mathrm{finger}}}{M_{\\mathrm{arm}}} = 20$$\nThis result indicates that the cortical magnification factor for the fingertip is $20$ times greater than that for the forearm, which is consistent with the known overrepresentation of high-acuity surfaces like the fingertips in the somatosensory cortex.", "answer": "$$\\boxed{20}$$", "id": "2779950"}, {"introduction": "The brain constantly makes inferences about the world based on incomplete and noisy sensory information. The \"Bayesian brain\" hypothesis proposes that the brain performs these inferences optimally by combining prior knowledge with incoming sensory evidence. This practice provides a hands-on derivation of the core mathematical principle of Bayesian integration, showing how a new estimate is formed as a precision-weighted average of the prior belief and the sensory measurement [@problem_id:2779925]. Mastering this concept is fundamental to understanding modern computational models of perception, learning, and decision-making.", "problem": "A simplified predictive coding model in systems neuroscience often assumes that a cortical microcircuit estimates a latent stimulus intensity $x$ by combining a prior expectation with a noisy sensory measurement. Consider the following linear-Gaussian generative model, consistent with Bayesian brain hypotheses and the definition of precision as the inverse of variance. The prior over $x$ is Gaussian, $p(x)=\\mathcal{N}(x;\\mu_p,\\Pi_p^{-1})$, where $\\mu_p$ is the prior mean and $\\Pi_p$ is the prior precision. The sensory observation $y$ is generated from $x$ by a Gaussian likelihood, $p(y \\mid x)=\\mathcal{N}(y;x,\\Pi_s^{-1})$, where $\\Pi_s$ is the sensory precision. Starting only from Bayesâ€™ rule and the definitions of these Gaussian densities, first derive the posterior density $p(x \\mid y)$ up to normalization and obtain its mean in terms of $\\mu_p$, $\\Pi_p$, $y$, and $\\Pi_s$ by completing the square in the exponent. Then, for the parameter values $\\Pi_p=2$, $\\Pi_s=5$, $\\mu_p=0$, and $y=1$, compute the posterior mean. Express your final numerical answer as an exact fraction. Finally, provide a brief qualitative explanation (no calculation required) of how decreasing $\\Pi_s$ while holding $\\Pi_p$, $\\mu_p$, and $y$ fixed alters the posterior mean relative to the prior mean and the observation. No rounding is necessary, and no physical units apply.", "solution": "The objective is to determine the posterior distribution $p(x \\mid y)$ of a latent variable $x$ given an observation $y$. We begin with Bayes' rule, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\nThe prior is a Gaussian distribution with mean $\\mu_p$ and precision $\\Pi_p$:\n$$p(x) = \\mathcal{N}(x; \\mu_p, \\Pi_p^{-1}) = \\frac{1}{\\sqrt{2\\pi\\Pi_p^{-1}}} \\exp\\left(-\\frac{1}{2}(x - \\mu_p)^2 \\Pi_p\\right)$$\nThe likelihood is also Gaussian, with mean $x$ and precision $\\Pi_s$:\n$$p(y \\mid x) = \\mathcal{N}(y; x, \\Pi_s^{-1}) = \\frac{1}{\\sqrt{2\\pi\\Pi_s^{-1}}} \\exp\\left(-\\frac{1}{2}(y - x)^2 \\Pi_s\\right)$$\nSubstituting these into Bayes' rule and disregarding the normalization constants (terms not dependent on $x$), we obtain:\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2}(y - x)^2 \\Pi_s\\right) \\exp\\left(-\\frac{1}{2}(x - \\mu_p)^2 \\Pi_p\\right)$$\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (x - y)^2 \\Pi_s + (x - \\mu_p)^2 \\Pi_p \\right]\\right)$$\nThe product of two Gaussian distributions (or their exponents) results in another Gaussian distribution. To find its parameters, we analyze the term in the exponent by expanding the squares and collecting terms with respect to $x$:\n$$L(x) = (x - y)^2 \\Pi_s + (x - \\mu_p)^2 \\Pi_p$$\n$$L(x) = (x^2 - 2xy + y^2)\\Pi_s + (x^2 - 2x\\mu_p + \\mu_p^2)\\Pi_p$$\n$$L(x) = (\\Pi_s + \\Pi_p)x^2 - 2(y\\Pi_s + \\mu_p\\Pi_p)x + (y^2\\Pi_s + \\mu_p^2\\Pi_p)$$\nThe posterior distribution $p(x \\mid y)$ will be a Gaussian, let us say $p(x \\mid y) = \\mathcal{N}(x; \\mu_{post}, \\Pi_{post}^{-1})$. The exponent of this posterior will be proportional to $(x - \\mu_{post})^2\\Pi_{post}$. We can write:\n$$(x - \\mu_{post})^2\\Pi_{post} = \\Pi_{post}x^2 - 2\\Pi_{post}\\mu_{post}x + \\Pi_{post}\\mu_{post}^2$$\nBy comparing the coefficients of the powers of $x$ in the expression for $L(x)$ with the general form of the Gaussian exponent, we can identify the posterior precision $\\Pi_{post}$ and the posterior mean $\\mu_{post}$.\nThe coefficient of the $x^2$ term gives the posterior precision:\n$$\\Pi_{post} = \\Pi_p + \\Pi_s$$\nThe coefficient of the $x$ term gives the product of the posterior mean and precision:\n$$-2(y\\Pi_s + \\mu_p\\Pi_p) = -2\\Pi_{post}\\mu_{post}$$\n$$(y\\Pi_s + \\mu_p\\Pi_p) = (\\Pi_p + \\Pi_s)\\mu_{post}$$\nSolving for the posterior mean, $\\mu_{post}$:\n$$\\mu_{post} = \\frac{\\mu_p\\Pi_p + y\\Pi_s}{\\Pi_p + \\Pi_s}$$\nThis expression shows that the posterior mean is a precision-weighted average of the prior mean $\\mu_p$ and the sensory observation $y$.\n\nFor the second part of the problem, we substitute the provided numerical values: $\\Pi_p=2$, $\\Pi_s=5$, $\\mu_p=0$, and $y=1$.\n$$\\mu_{post} = \\frac{(0)(2) + (1)(5)}{2 + 5}$$\n$$\\mu_{post} = \\frac{0 + 5}{7}$$\n$$\\mu_{post} = \\frac{5}{7}$$\n\nFor the final part, we provide a qualitative explanation. The posterior mean is given by the formula:\n$$\\mu_{post} = \\frac{\\Pi_p}{\\Pi_p + \\Pi_s}\\mu_p + \\frac{\\Pi_s}{\\Pi_p + \\Pi_s}y$$\nThis is a weighted average of the prior mean $\\mu_p$ and the sensory observation $y$. The weights, $\\frac{\\Pi_p}{\\Pi_p + \\Pi_s}$ and $\\frac{\\Pi_s}{\\Pi_p + \\Pi_s}$, sum to $1$. The precision $\\Pi_s$ quantifies the reliability of the sensory observation $y$; a lower $\\Pi_s$ implies a less reliable (noisier) observation.\nIf we decrease $\\Pi_s$ while holding all other parameters constant, the weight $\\frac{\\Pi_s}{\\Pi_p + \\Pi_s}$ on the observation $y$ decreases, while the weight $\\frac{\\Pi_p}{\\Pi_p + \\Pi_s}$ on the prior mean $\\mu_p$ increases. Consequently, the posterior mean $\\mu_{post}$ will shift away from the sensory observation $y$ and move closer to the prior mean $\\mu_p$. This reflects the principle of Bayesian inference: as sensory evidence becomes less reliable, the optimal estimate relies more heavily on prior beliefs.", "answer": "$$\\boxed{\\frac{5}{7}}$$", "id": "2779925"}, {"introduction": "Generating smooth, purposeful movements is a complex engineering task that the brain solves effortlessly. Optimal feedback control theory provides a powerful framework for understanding this process, suggesting that the motor system minimizes a cost function that balances accuracy and effort. In this practical exercise, you will apply the Linear Quadratic Regulator (LQR), a cornerstone of optimal control, to a simplified model of arm movement [@problem_id:2779932]. By exploring how the penalty on motor commands affects movement smoothness, you will gain insight into the fundamental trade-offs that shape our actions.", "problem": "Consider a simplified optimal feedback control model of a two-joint arm in the framework of brain functional systems and motor control. The system is linearized around a posture target, and neuromuscular stiffness and damping in the operating regime permit an overdamped first-order approximation for the joint-angle error dynamics. Let the state be the pair of joint-angle errors $x(t) \\in \\mathbb{R}^2$, and the control be the joint torques $u(t) \\in \\mathbb{R}^2$. The continuous-time dynamics are\n$$\n\\frac{dx}{dt} = A x + B u,\n$$\nwith\n$$\nA = \\begin{bmatrix} -3 & 0 \\\\ 0 & -2 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n$$\nIn an infinite-horizon Linear Quadratic Regulator (LQR) formulation motivated by optimal feedback control of the sensorimotor system, the controller minimizes\n$$\nJ = \\int_0^\\infty \\left( x(t)^\\top Q\\, x(t) + u(t)^\\top R\\, u(t) \\right) dt,\n$$\nwith the state-cost matrix\n$$\nQ = \\mathrm{diag}(1,1),\n$$\nand a control-cost matrix scaled from a base matrix\n$$\nR_0 = \\mathrm{diag}(0.1, 0.1),\n$$\nso that for a given positive scalar $s$, the control-cost matrix is $R = s\\, R_0$.\n\nYour tasks are:\n- For each specified scale $s$, compute the optimal continuous-time state feedback gain $K(s)$ associated with $u(t) = -K(s)\\,x(t)$ that minimizes $J$ under the system dynamics.\n- For each $K(s)$, quantify movement smoothness by the integrated squared jerk of the joint-angle error trajectory under the closed-loop dynamics starting from the initial condition $x(0) = x_0$, with $x_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. Define the jerk as the third time derivative of $x(t)$, and define the smoothness metric\n$$\nS(s) = \\int_0^\\infty \\left\\| \\frac{d^3 x}{dt^3}(t) \\right\\|_2^2 \\, dt.\n$$\nAssume $x(t)$ and $u(t)$ are dimensionless for the purpose of this mathematical exercise (no physical units). Angles and derivatives are not to be expressed in degrees or radians because units are abstracted away; report all requested numerical quantities as dimensionless real numbers.\n\nProvide results for the following test suite of control-cost scales:\n- $s = 0.1$ (low control effort penalty),\n- $s = 1$ (baseline control effort penalty),\n- $s = 10$ (high control effort penalty),\n- $s = 50$ (very high control effort penalty).\n\nDesign constraints and guidance:\n- Use only well-tested foundational facts: the Linear Quadratic Regulator framework for linear systems and the definition of smoothness as integrated squared jerk. Do not assume or use any target results; derive what you need from these bases.\n- You should not introduce or require any physical units; treat all quantities as dimensionless.\n- Ensure numerical stability and correctness for all $s$ values in the test suite.\n\nProgram output requirements:\n- For each test case scale $s$, compute:\n  1. The Frobenius norm of the optimal gain matrix, $\\|K(s)\\|_F$.\n  2. The smoothness metric $S(s)$ as defined above.\n- Round both $\\|K(s)\\|_F$ and $S(s)$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order given and is itself a two-element list in the form $[\\|K(s)\\|_F, S(s)]$. For example: \"[[knorm_1,S_1],[knorm_2,S_2],[knorm_3,S_3],[knorm_4,S_4]]\".", "solution": "The solution proceeds in two parts: first, we derive the optimal feedback gain $K(s)$ by solving the ARE; second, we derive an analytical expression for the smoothness metric $S(s)$ using the closed-loop dynamics.\n\nPart 1: Derivation of the Optimal Gain $K(s)$\nThe optimal feedback gain is given by $K = R^{-1} B^\\top P$, where $P$ is the symmetric positive-definite solution to the continuous-time Algebraic Riccati Equation (ARE):\n$$\nA^\\top P + PA - PBR^{-1}B^\\top P + Q = 0\n$$\nThe provided matrices are $A = \\mathrm{diag}(-3, -2)$, $B = \\mathrm{diag}(3, 2)$, $Q = \\mathrm{diag}(1, 1)$, and $R = s \\cdot \\mathrm{diag}(0.1, 0.1) = \\mathrm{diag}(0.1s, 0.1s)$. Consequently, $R^{-1} = \\mathrm{diag}(10/s, 10/s)$.\nGiven that all system and cost matrices are diagonal, the solution $P$ to the ARE must also be diagonal. Let $P = \\mathrm{diag}(p_1, p_2)$. Substituting these matrices into the ARE yields:\n$$\n\\mathrm{diag}(-3, -2) \\mathrm{diag}(p_1, p_2) + \\mathrm{diag}(p_1, p_2) \\mathrm{diag}(-3, -2) - \\mathrm{diag}(p_1, p_2) \\mathrm{diag}(3, 2) \\mathrm{diag}(\\frac{10}{s}, \\frac{10}{s}) \\mathrm{diag}(3, 2) \\mathrm{diag}(p_1, p_2) + \\mathrm{diag}(1, 1) = 0\n$$\nThis simplifies to a diagonal matrix equation:\n$$\n\\mathrm{diag}(-6p_1, -4p_2) - \\mathrm{diag}(\\frac{90}{s} p_1^2, \\frac{40}{s} p_2^2) + \\mathrm{diag}(1, 1) = \\mathrm{diag}(0, 0)\n$$\nThis decouples into two independent scalar quadratic equations for $p_1$ and $p_2$:\n1. $\\frac{90}{s}p_1^2 + 6p_1 - 1 = 0$\n2. $\\frac{40}{s}p_2^2 + 4p_2 - 1 = 0$\n\nSolving for the positive root of each quadratic equation (since $P$ must be positive-definite):\nFor $p_1$: $p_1 = \\frac{-6 + \\sqrt{6^2 - 4(\\frac{90}{s})(-1)}}{2(\\frac{90}{s})} = \\frac{-6 + \\sqrt{36 + \\frac{360}{s}}}{180/s} = \\frac{s}{180}(-6 + 6\\sqrt{1 + \\frac{10}{s}}) = \\frac{s}{30}(\\sqrt{1 + \\frac{10}{s}} - 1)$.\nFor $p_2$: $p_2 = \\frac{-4 + \\sqrt{4^2 - 4(\\frac{40}{s})(-1)}}{2(\\frac{40}{s})} = \\frac{-4 + \\sqrt{16 + \\frac{160}{s}}}{80/s} = \\frac{s}{80}(-4 + 4\\sqrt{1 + \\frac{10}{s}}) = \\frac{s}{20}(\\sqrt{1 + \\frac{10}{s}} - 1)$.\n\nThe optimal gain matrix $K(s) = R^{-1} B^\\top P(s)$ is:\n$$\nK(s) = \\mathrm{diag}(\\frac{10}{s}, \\frac{10}{s}) \\mathrm{diag}(3, 2) \\mathrm{diag}(p_1, p_2) = \\mathrm{diag}(\\frac{30p_1}{s}, \\frac{20p_2}{s})\n$$\nSubstituting the expressions for $p_1$ and $p_2$:\n$k_{11}(s) = \\frac{30}{s} \\left[ \\frac{s}{30}(\\sqrt{1 + \\frac{10}{s}} - 1) \\right] = \\sqrt{1 + \\frac{10}{s}} - 1$.\n$k_{22}(s) = \\frac{20}{s} \\left[ \\frac{s}{20}(\\sqrt{1 + \\frac{10}{s}} - 1) \\right] = \\sqrt{1 + \\frac{10}{s}} - 1$.\nThus, the gain matrix is a scaled identity matrix:\n$$\nK(s) = \\left(\\sqrt{1 + \\frac{10}{s}} - 1\\right) I\n$$\nThe Frobenius norm is $\\|K(s)\\|_F = \\sqrt{\\sum_{i,j} |k_{ij}|^2} = \\sqrt{k_{11}^2 + k_{22}^2}$.\n$$\n\\|K(s)\\|_F = \\sqrt{2 \\left(\\sqrt{1 + \\frac{10}{s}} - 1\\right)^2} = \\sqrt{2} \\left(\\sqrt{1 + \\frac{10}{s}} - 1\\right)\n$$\n\nPart 2: Derivation of the Smoothness Metric $S(s)$\nThe closed-loop system dynamics are $\\frac{dx}{dt} = (A - BK)x$. Let $A_{cl}(s) = A - B K(s)$.\n$$\nA_{cl}(s) = \\mathrm{diag}(-3, -2) - \\mathrm{diag}(3, 2) \\left(\\sqrt{1 + \\frac{10}{s}} - 1\\right) I\n$$\n$$\nA_{cl}(s) = \\mathrm{diag}\\left(-3 - 3(\\sqrt{1 + \\frac{10}{s}} - 1), -2 - 2(\\sqrt{1 + \\frac{10}{s}} - 1)\\right)\n$$\n$$\nA_{cl}(s) = \\mathrm{diag}\\left(-3\\sqrt{1 + \\frac{10}{s}}, -2\\sqrt{1 + \\frac{10}{s}}\\right)\n$$\nLet the eigenvalues of $A_{cl}$ be $\\lambda_1(s) = -3\\sqrt{1 + 10/s}$ and $\\lambda_2(s) = -2\\sqrt{1 + 10/s}$.\nThe solution to the state equation with initial condition $x(0) = x_0 = [1, -1]^\\top$ is $x(t) = e^{A_{cl} t} x_0$:\n$$\nx(t) = \\begin{bmatrix} e^{\\lambda_1 t} & 0 \\\\ 0 & e^{\\lambda_2 t} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} e^{\\lambda_1 t} \\\\ -e^{\\lambda_2 t} \\end{bmatrix}\n$$\nThe jerk is the third derivative, $\\frac{d^3 x}{dt^3}$:\n$$\nj(t) = \\frac{d^3 x}{dt^3}(t) = \\begin{bmatrix} \\lambda_1^3 e^{\\lambda_1 t} \\\\ -\\lambda_2^3 e^{\\lambda_2 t} \\end{bmatrix}\n$$\nThe squared Euclidean norm of the jerk is $\\|j(t)\\|_2^2 = (\\lambda_1^3 e^{\\lambda_1 t})^2 + (-\\lambda_2^3 e^{\\lambda_2 t})^2 = \\lambda_1^6 e^{2\\lambda_1 t} + \\lambda_2^6 e^{2\\lambda_2 t}$.\nThe smoothness metric $S(s)$ is the integral of this quantity from $0$ to $\\infty$:\n$$\nS(s) = \\int_0^\\infty (\\lambda_1^6 e^{2\\lambda_1 t} + \\lambda_2^6 e^{2\\lambda_2 t}) dt = \\lambda_1^6 \\left[ \\frac{e^{2\\lambda_1 t}}{2\\lambda_1} \\right]_0^\\infty + \\lambda_2^6 \\left[ \\frac{e^{2\\lambda_2 t}}{2\\lambda_2} \\right]_0^\\infty\n$$\nSince $\\lambda_1, \\lambda_2 < 0$, $e^{2\\lambda_i t} \\to 0$ as $t \\to \\infty$. The integral evaluates to:\n$$\nS(s) = \\lambda_1^6 \\left(0 - \\frac{1}{2\\lambda_1}\\right) + \\lambda_2^6 \\left(0 - \\frac{1}{2\\lambda_2}\\right) = -\\frac{1}{2}(\\lambda_1^5 + \\lambda_2^5)\n$$\nSubstituting the expressions for $\\lambda_1$ and $\\lambda_2$:\n$$\nS(s) = -\\frac{1}{2} \\left[ \\left(-3\\sqrt{1 + \\frac{10}{s}}\\right)^5 + \\left(-2\\sqrt{1 + \\frac{10}{s}}\\right)^5 \\right]\n$$\n$$\nS(s) = -\\frac{1}{2} \\left[ (-3)^5 \\left(\\left(1 + \\frac{10}{s}\\right)^{1/2}\\right)^5 + (-2)^5 \\left(\\left(1 + \\frac{10}{s}\\right)^{1/2}\\right)^5 \\right]\n$$\n$$\nS(s) = \\frac{1}{2} (3^5 + 2^5) \\left(1 + \\frac{10}{s}\\right)^{5/2} = \\frac{243 + 32}{2} \\left(1 + \\frac{10}{s}\\right)^{5/2}\n$$\n$$\nS(s) = 137.5 \\left(1 + \\frac{10}{s}\\right)^{2.5}\n$$\nThese derived formulas are implemented in the program below to compute the numerical results for the specified values of $s$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the Frobenius norm of the optimal LQR gain and a smoothness metric\n    for a simplified two-joint arm model under varying control cost.\n    \"\"\"\n\n    # Define the test cases for the control cost scalar s.\n    test_cases = [0.1, 1.0, 10.0, 50.0]\n\n    results = []\n    \n    # Process each test case.\n    for s in test_cases:\n        # The analytical formulas derived in the solution are:\n        #\n        # 1. Frobenius norm of the gain matrix K(s):\n        #    ||K(s)||_F = sqrt(2) * (sqrt(1 + 10/s) - 1)\n        #\n        # 2. Smoothness metric S(s):\n        #    S(s) = 137.5 * (1 + 10/s)^2.5\n\n        # Calculate the intermediate term that appears in both formulas.\n        # This term is sqrt(1 + 10/s).\n        term_sqrt = np.sqrt(1 + 10 / s)\n\n        # Calculate the Frobenius norm of the gain matrix K.\n        k_norm = np.sqrt(2) * (term_sqrt - 1)\n\n        # Calculate the smoothness metric S.\n        # S can be calculated from term_sqrt as 137.5 * term_sqrt^5.\n        smoothness = 137.5 * (term_sqrt**5)\n\n        # Format the results as a two-element list string with 6 decimal places.\n        result_str = f\"[{k_norm:.6f},{smoothness:.6f}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single line string: \"[[knorm_1,S_1],[knorm_2,S_2],...]\"\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver function.\nsolve()\n```", "id": "2779932"}]}