{"hands_on_practices": [{"introduction": "A foundational task in population ecology is determining the size of a population, a parameter that is often impossible to measure directly. This exercise introduces the capture-mark-recapture method, a cornerstone technique for estimating the abundance of mobile or elusive animals. You will apply the bias-corrected Chapman estimator, which improves upon the simpler Lincoln-Petersen method, and construct a confidence interval to quantify the uncertainty inherent in the estimation process, a critical skill for any field ecologist [@problem_id:2826835].", "problem": "A small mammal population is sampled using a two-occasion capture–mark–recapture design under the standard closed-population assumptions: no births, deaths, immigration, or emigration between sampling occasions; equal capture probability for all individuals within each occasion; independence of captures; and no loss of marks. On the first occasion, a field team captures and uniquely marks $n_1$ individuals. On the second occasion, the team captures $n_2$ individuals of which $m_2$ are found to be marked from the first occasion.\n\nWorking from first principles of sampling without replacement from a finite population, treat the number of marked animals in the second sample as a hypergeometric random variable. Use this to motivate an approximately unbiased estimator of the total population size based on a bias-correction to the naive ratio estimator derived from the expectation of the hypergeometric sample. Then, using a large-sample normal approximation with the Chapman variance as the sampling variance of the estimator, construct an approximate $95\\%$ confidence interval for the population size. For clarity, define all acronyms at first use; for example, confidence interval (CI) and Lincoln–Petersen (LP).\n\nGiven the observed values $n_1 = 80$, $n_2 = 100$, and $m_2 = 30$:\n- Compute the point estimate of the population size using the bias-corrected Lincoln–Petersen (Chapman) estimator.\n- Compute the approximate variance of this estimator using the Chapman variance.\n- Using the standard normal critical value, construct the approximate $95\\%$ CI.\n\nRound the point estimate and both CI endpoints to four significant figures. Express the point estimate and the CI endpoints in units of individuals. Provide your final answer as three numbers in the order $[\\hat{N}, \\text{CI lower}, \\text{CI upper}]$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extracted Givens**\n-   **Design**: A two-occasion capture–mark–recapture design.\n-   **Assumptions**: A closed population (no births, deaths, immigration, or emigration between sampling occasions); equal capture probability for all individuals within each occasion; independence of captures; no loss of marks.\n-   **Variables**:\n    -   $n_1$: number of individuals captured and marked on the first occasion.\n    -   $n_2$: number of individuals captured on the second occasion.\n    -   $m_2$: number of marked individuals recaptured on the second occasion.\n-   **Statistical Model**: The number of marked animals in the second sample, $m_2$, is to be treated as a hypergeometric random variable.\n-   **Tasks**:\n    1.  Motivate an approximately unbiased estimator of the total population size based on a bias-correction to the naive ratio estimator derived from the expectation of the hypergeometric sample.\n    2.  Construct an approximate $95\\%$ confidence interval (CI) for the population size using a large-sample normal approximation with the Chapman variance.\n    3.  Define all acronyms at first use, such as confidence interval (CI) and Lincoln–Petersen (LP).\n-   **Observed Data**:\n    -   $n_1 = 80$\n    -   $n_2 = 100$\n    -   $m_2 = 30$\n-   **Required Computations**:\n    1.  The point estimate of the population size using the bias-corrected Lincoln–Petersen (Chapman) estimator.\n    2.  The approximate variance of this estimator using the Chapman variance.\n    3.  The approximate $95\\%$ CI using the standard normal critical value.\n-   **Formatting Requirements**: Round the point estimate and CI endpoints to four significant figures. Express the final answer as three numbers: $[\\hat{N}, \\text{CI lower}, \\text{CI upper}]$.\n\n**Step 2: Validation Using Extracted Givens**\n-   **Scientific Grounding**: The problem is fundamentally sound. Capture-mark-recapture is a standard ecological technique. The use of the hypergeometric distribution is the correct theoretical model for sampling without replacement from a finite population. The Lincoln–Petersen (LP) estimator and its bias-corrected form, the Chapman estimator, are canonical topics in population estimation theory. The use of a normal approximation for constructing a confidence interval (CI) is a standard statistical procedure for large samples.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary data ($n_1$, $n_2$, $m_2$) and parameters (confidence level of $95\\%$) to compute the required quantities. The instructions are clear and sequential.\n-   **Objectivity**: The problem is stated in objective, quantitative language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. There are no contradictions, missing data, or logical flaws. The solution process will now proceed.\n\n**Derivation and Solution**\nLet $N$ be the unknown total population size. The experiment consists of two sampling occasions. On the first occasion, $n_1$ individuals are captured, marked, and released. On the second occasion, a sample of $n_2$ individuals is captured. The number of marked individuals in this second sample, $m_2$, is the random variable of interest.\n\nUnder the stated assumptions, the population of size $N$ contains a subpopulation of $n_1$ marked individuals. The second sample of size $n_2$ is drawn without replacement from this finite population. The probability of observing exactly $k$ marked individuals in the second sample follows the hypergeometric distribution:\n$$ P(m_2 = k | N, n_1, n_2) = \\frac{\\binom{n_1}{k} \\binom{N-n_1}{n_2-k}}{\\binom{N}{n_2}} $$\nThe expectation of this distribution is $E[m_2] = n_2 \\frac{n_1}{N}$.\n\nBy substituting the observed value $m_2$ for its expectation, we can derive a simple estimator for $N$:\n$$ m_2 \\approx E[m_2] \\implies m_2 \\approx \\frac{n_1 n_2}{N} $$\nSolving for $N$ yields the classic Lincoln–Petersen (LP) estimator, which is also the maximum likelihood estimator:\n$$ \\hat{N}_{LP} = \\frac{n_1 n_2}{m_2} $$\nHowever, this estimator is known to be biased, especially for small sample sizes. The bias arises from the reciprocal of the random variable $m_2$. By Jensen's inequality, for the convex function $f(x)=1/x$, we have $E[1/m_2] > 1/E[m_2]$, which leads to $E[\\hat{N}_{LP}] > N$. The estimator tends to be an overestimate, and it is undefined if $m_2=0$.\n\nTo correct for this bias, Chapman (1951) proposed a modified estimator:\n$$ \\hat{N}_C = \\frac{(n_1+1)(n_2+1)}{m_2+1} - 1 $$\nThis is the Chapman estimator. It is nearly unbiased if $n_1 + n_2 \\ge N$ and has the practical advantage of being finite even if $m_2=0$.\n\nThe task requires the computation of this point estimate for the given data: $n_1=80$, $n_2=100$, and $m_2=30$.\n$$ \\hat{N}_C = \\frac{(80+1)(100+1)}{30+1} - 1 = \\frac{81 \\times 101}{31} - 1 = \\frac{8181}{31} - 1 $$\n$$ \\hat{N}_C \\approx 263.9032 - 1 = 262.9032 $$\nRounding to four significant figures, the estimated population size is $262.9$ individuals.\n\nNext, we must construct an approximate $95\\%$ CI. This requires an estimate of the sampling variance of $\\hat{N}_C$. An approximately unbiased estimator for the variance of $\\hat{N}_C$ was also provided by Chapman and is given by:\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(n_1+1)(n_2+1)(n_1-m_2)(n_2-m_2)}{(m_2+1)^2 (m_2+2)} $$\nSubstituting the observed values:\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(80+1)(100+1)(80-30)(100-30)}{(30+1)^2 (30+2)} $$\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(81)(101)(50)(70)}{(31)^2 (32)} = \\frac{28633500}{961 \\times 32} = \\frac{28633500}{30752} \\approx 931.1118 $$\nThe standard error (SE) is the square root of the variance:\n$$ \\text{SE}(\\hat{N}_C) = \\sqrt{\\text{Var}(\\hat{N}_C)} \\approx \\sqrt{931.1118} \\approx 30.5141 $$\nFor a large sample, the distribution of $\\hat{N}_C$ can be approximated by a normal distribution. An approximate $95\\%$ CI is constructed as:\n$$ \\text{CI}_{95\\%} = \\hat{N}_C \\pm z_{1-\\alpha/2} \\times \\text{SE}(\\hat{N}_C) $$\nFor a $95\\%$ CI, $\\alpha=0.05$, and the critical value from the standard normal distribution is $z_{0.975} \\approx 1.96$.\nThe margin of error (ME) is:\n$$ \\text{ME} = 1.96 \\times 30.5141 \\approx 59.8076 $$\nThe CI bounds are:\n-   Lower bound: $\\hat{N}_C - \\text{ME} = 262.9032 - 59.8076 = 203.0956$\n-   Upper bound: $\\hat{N}_C + \\text{ME} = 262.9032 + 59.8076 = 322.7108$\n\nRounding the point estimate and the CI endpoints to four significant figures:\n-   Point Estimate $\\hat{N}_C$: $262.9$\n-   Lower CI Bound: $203.1$\n-   Upper CI Bound: $322.7$\n\nThe final result consists of the point estimate and the lower and upper bounds of the $95\\%$ CI.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n262.9  203.1  322.7\n\\end{pmatrix}\n}\n$$", "id": "2826835"}, {"introduction": "Beyond just counting individuals, understanding their spatial arrangement provides deep insights into a population's ecology, revealing potential social interactions, resource distributions, or dispersal patterns. This problem transitions from density to dispersion, asking you to derive and calculate Morisita's index, a robust measure for quantifying spatial patterns. By comparing an observed distribution to a random expectation, this practice will allow you to quantitatively distinguish between aggregated, random, and regular (uniform) dispersion patterns from quadrat count data [@problem_id:2826834].", "problem": "An ecologist samples a sessile invertebrate population using $n$ equal-area quadrats placed within a single homogeneous habitat patch. The quadrat counts are $\\{x_{1}, x_{2}, \\dots, x_{n}\\}$ with total abundance $N=\\sum_{i=1}^{n} x_{i}$. A classical dispersion metric, Morisita’s index, is defined conceptually as the observed probability that $2$ individuals sampled without replacement from the entire set of $N$ individuals come from the same quadrat, scaled by the probability expected under spatial randomness across equal-area quadrats. Assume a closed population, perfect detection, equal quadrat areas, and that “spatial randomness” refers to independent placement of each of the $N$ individuals into $n$ quadrats with equal probability.\n\nYou are given data from $n=30$ quadrats with sample mean count $\\bar{x}=5.2$ and the sum $\\sum_{i=1}^{n} x_{i}(x_{i}-1)=420$. Starting from the conceptual definition above and first principles of sampling without replacement and independence assumptions under spatial randomness, derive an expression for Morisita’s index in terms of $n$, $N$, and $\\sum_{i=1}^{n} x_{i}(x_{i}-1)$. Then compute its exact value for the given data. Finally, state whether the pattern indicates aggregation, randomness, or regularity relative to a Poisson baseline, and justify your conclusion.\n\nGive the final numerical value of Morisita’s index as an exact fraction. Do not round.", "solution": "The problem as stated is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution. It is a standard exercise in quantitative ecology. We proceed to the solution.\n\nThe task is to derive an expression for Morisita’s index, $I_\\delta$, from its conceptual definition, compute its value from the given data, and interpret the result. Morisita's index is defined as the ratio of an observed probability to an expected probability:\n$$ I_\\delta = \\frac{P_{\\text{obs}}}{P_{\\text{exp}}} $$\n\nFirst, we derive the expression for $P_{\\text{obs}}$, the observed probability that two individuals sampled without replacement from the total population of $N$ individuals are from the same quadrat. The total number of ways to choose a pair of individuals from the $N$ available is given by the binomial coefficient $\\binom{N}{2}$:\n$$ \\text{Total pairs} = \\binom{N}{2} = \\frac{N(N-1)}{2} $$\nThe number of pairs that can be formed from individuals within a single quadrat $i$, which contains $x_i$ individuals, is $\\binom{x_i}{2}$. Summing over all $n$ quadrats gives the total number of pairs of individuals from the same quadrat:\n$$ \\text{Same-quadrat pairs} = \\sum_{i=1}^{n} \\binom{x_i}{2} = \\sum_{i=1}^{n} \\frac{x_i(x_i-1)}{2} = \\frac{1}{2} \\sum_{i=1}^{n} x_i(x_i-1) $$\nThe observed probability is the ratio of these quantities:\n$$ P_{\\text{obs}} = \\frac{\\frac{1}{2} \\sum_{i=1}^{n} x_i(x_i-1)}{\\frac{1}{2} N(N-1)} = \\frac{\\sum_{i=1}^{n} x_i(x_i-1)}{N(N-1)} $$\n\nNext, we derive the expression for $P_{\\text{exp}}$, the expected probability of two individuals being in the same quadrat under the condition of spatial randomness. Spatial randomness is defined as the independent placement of each of the $N$ individuals into one of the $n$ quadrats with equal probability $1/n$. Consider any two individuals from the population. The first individual will land in some quadrat. The second individual, placed independently, has a probability of $1/n$ of landing in that same quadrat. Thus, the probability that any two individuals are found in the same quadrat is $1/n$.\n$$ P_{\\text{exp}} = \\frac{1}{n} $$\n\nCombining the expressions for $P_{\\text{obs}}$ and $P_{\\text{exp}}$, we obtain the formula for Morisita's index:\n$$ I_\\delta = \\frac{P_{\\text{obs}}}{P_{\\text{exp}}} = \\frac{\\frac{\\sum_{i=1}^{n} x_i(x_i-1)}{N(N-1)}}{\\frac{1}{n}} = \\frac{n \\sum_{i=1}^{n} x_i(x_i-1)}{N(N-1)} $$\nThis is the required expression.\n\nNow, we compute the value of $I_\\delta$ for the given data:\n- Number of quadrats, $n = 30$.\n- Sample mean count, $\\bar{x} = 5.2$.\n- The sum $\\sum_{i=1}^{n} x_i(x_i-1) = 420$.\n\nFirst, we determine the total population size, $N$, from the sample mean and the number of quadrats:\n$$ N = n \\bar{x} = 30 \\times 5.2 = 156 $$\nWe can now substitute the numerical values into the derived formula for $I_\\delta$:\n$$ I_\\delta = \\frac{30 \\times 420}{156 \\times (156 - 1)} = \\frac{12600}{156 \\times 155} = \\frac{12600}{24180} $$\nTo express this as an exact fraction, we simplify the expression. We can cancel a factor of $10$:\n$$ I_\\delta = \\frac{1260}{2418} $$\nBoth numerator and denominator are divisible by $6$:\n$$ I_\\delta = \\frac{1260 \\div 6}{2418 \\div 6} = \\frac{210}{403} $$\nTo confirm if this fraction is in simplest form, we find the prime factors. The numerator is $210 = 2 \\times 3 \\times 5 \\times 7$. For the denominator, we test prime divisors: $403$ is not divisible by $2, 3, 5, 7, 11$. Testing $13$, we find $403 = 13 \\times 31$. As there are no common prime factors, the fraction $\\frac{210}{403}$ is in its simplest form.\n\nFinally, we must interpret this result. The baseline value for Morisita's index corresponding to a random (Poisson) spatial pattern is $I_\\delta = 1$.\n- If $I_\\delta > 1$, the pattern is aggregated (clumped).\n- If $I_\\delta  1$, the pattern is regular (uniform).\nOur calculated value is $I_\\delta = \\frac{210}{403}$. Since $210  403$, the value is less than $1$. This indicates that the observed probability of finding two individuals in the same quadrat is lower than what would be expected under random placement. Such a finding implies that the individuals are more evenly spaced than by chance. Therefore, the population exhibits a regular dispersion pattern. This may be caused by negative interactions among individuals, such as territoriality or competition.", "answer": "$$ \\boxed{\\frac{210}{403}} $$", "id": "2826834"}, {"introduction": "To predict a population's future, we must model its dynamics, a task that requires understanding the contributions of different life stages to overall growth. This practice delves into the powerful framework of matrix population models, specifically using a Leslie matrix to project the growth of a stage-structured population. You will perform an elasticity analysis to determine how sensitive the population growth rate, $\\lambda$, is to proportional changes in survival and fecundity, a crucial analysis for identifying effective conservation and management strategies [@problem_id:2826804].", "problem": "A stage-structured population of a temperate perennial plant is modeled over annual time steps by a Leslie matrix, where individuals progress deterministically through three age classes: yearlings (stage 1), subadults (stage 2), and reproductive adults (stage 3). Only subadults and adults reproduce. The projection from year $t$ to $t+1$ is given by a $3 \\times 3$ matrix $\\mathbf{A}$ acting on the population vector $\\mathbf{n}_{t}$ such that $\\mathbf{n}_{t+1} = \\mathbf{A}\\mathbf{n}_{t}$. The matrix $\\mathbf{A}$ is\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0  0.6  1.2 \\\\\n0.45  0  0 \\\\\n0  0.7  0\n\\end{pmatrix}.\n$$\nAssume density independence and that the long-run growth rate is governed by the dominant eigenvalue $\\lambda$ of $\\mathbf{A}$. Starting from the core definitions of stage-structured projection and the interpretation of sensitivity and elasticity as, respectively, the absolute and proportional responses of $\\lambda$ to infinitesimal perturbations of entries of $\\mathbf{A}$, derive the working expressions needed to compute the elasticity of $\\lambda$ with respect to each nonzero entry of $\\mathbf{A}$ in terms of the dominant left and right eigenvectors of $\\mathbf{A}$. Then compute numerically the full elasticity matrix for $\\mathbf{A}$, and determine which life-history entry (interpret each nonzero matrix element biologically) has the greatest influence on $\\lambda$.\n\nFor grading, report only the largest elasticity value. Express your final answer as a unitless decimal rounded to four significant figures.", "solution": "The problem requires the derivation of the formula for the elasticity of the dominant eigenvalue $\\lambda$ of a projection matrix $\\mathbf{A}$ with respect to its elements $a_{ij}$, and then the application of this formula to a specific Leslie matrix.\n\nFirst, we validate the problem statement.\nThe givens are:\nThe population projection is governed by $\\mathbf{n}_{t+1} = \\mathbf{A}\\mathbf{n}_{t}$, with the Leslie matrix\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0  0.6  1.2 \\\\\n0.45  0  0 \\\\\n0  0.7  0\n\\end{pmatrix}.\n$$\nThe long-run growth rate is the dominant eigenvalue $\\lambda$ of $\\mathbf{A}$.\nThe task is to derive the expression for elasticity $e_{ij}$ of $\\lambda$ with respect to $a_{ij}$, compute the elasticity matrix for $\\mathbf{A}$, and identify the largest elasticity value.\n\nThe problem is scientifically grounded, as Leslie matrix models are a cornerstone of population ecology. The matrix entries are biologically plausible: fecundities ($a_{12}=0.6$, $a_{13}=1.2$) are positive, and survival/transition probabilities ($a_{21}=0.45$, $a_{32}=0.7$) are between $0$ and $1$. The matrix $\\mathbf{A}$ is non-negative and irreducible, which, by the Perron-Frobenius theorem, guarantees the existence of a unique, simple, positive dominant eigenvalue $\\lambda$ and corresponding positive left and right eigenvectors. The problem is well-posed, objective, and contains all necessary information. Thus, the problem is valid.\n\nWe begin the derivation. The dominant eigenvalue $\\lambda$ and its corresponding right eigenvector $\\mathbf{w}$ (representing the stable stage distribution) and left eigenvector $\\mathbf{v}$ (representing stage-specific reproductive values) are defined by the equations:\n$$ \\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w} $$\n$$ \\mathbf{v}^T\\mathbf{A} = \\lambda\\mathbf{v}^T $$\nwhere $\\mathbf{v}^T$ is a row vector.\n\nTo find the sensitivity of $\\lambda$ to a change in a matrix element $a_{ij}$, we consider an infinitesimally perturbed matrix $\\mathbf{A}' = \\mathbf{A} + \\mathrm{d}\\mathbf{A}$, where $\\mathrm{d}\\mathbf{A}$ is a matrix with a single non-zero element $\\mathrm{d}a_{ij}$ at position $(i,j)$. This perturbation causes changes in the eigenvalue and eigenvector: $\\lambda' = \\lambda + \\mathrm{d}\\lambda$ and $\\mathbf{w}' = \\mathbf{w} + \\mathrm{d}\\mathbf{w}$. The new eigenvalue equation is:\n$$ (\\mathbf{A} + \\mathrm{d}\\mathbf{A})(\\mathbf{w} + \\mathrm{d}\\mathbf{w}) = (\\lambda + \\mathrm{d}\\lambda)(\\mathbf{w} + \\mathrm{d}\\mathbf{w}) $$\nExpanding and retaining only first-order terms gives:\n$$ \\mathbf{A}\\mathbf{w} + \\mathbf{A}\\mathrm{d}\\mathbf{w} + (\\mathrm{d}\\mathbf{A})\\mathbf{w} \\approx \\lambda\\mathbf{w} + \\lambda\\mathrm{d}\\mathbf{w} + (\\mathrm{d}\\lambda)\\mathbf{w} $$\nSince $\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}$, these terms cancel, leaving:\n$$ \\mathbf{A}\\mathrm{d}\\mathbf{w} + (\\mathrm{d}\\mathbf{A})\\mathbf{w} \\approx \\lambda\\mathrm{d}\\mathbf{w} + (\\mathrm{d}\\lambda)\\mathbf{w} $$\nLeft-multiplying by the left eigenvector $\\mathbf{v}^T$:\n$$ \\mathbf{v}^T\\mathbf{A}\\mathrm{d}\\mathbf{w} + \\mathbf{v}^T(\\mathrm{d}\\mathbf{A})\\mathbf{w} \\approx \\lambda\\mathbf{v}^T\\mathrm{d}\\mathbf{w} + (\\mathrm{d}\\lambda)\\mathbf{v}^T\\mathbf{w} $$\nUsing the property $\\mathbf{v}^T\\mathbf{A} = \\lambda\\mathbf{v}^T$, the first term on the left, $\\mathbf{v}^T\\mathbf{A}\\mathrm{d}\\mathbf{w}$, becomes $\\lambda\\mathbf{v}^T\\mathrm{d}\\mathbf{w}$. This cancels with the first term on the right, yielding:\n$$ \\mathbf{v}^T(\\mathrm{d}\\mathbf{A})\\mathbf{w} \\approx (\\mathrm{d}\\lambda)\\mathbf{v}^T\\mathbf{w} $$\nThe term $\\mathbf{v}^T(\\mathrm{d}\\mathbf{A})\\mathbf{w}$ evaluates to $v_i (\\mathrm{d}a_{ij}) w_j$, where $v_i$ is the $i$-th element of $\\mathbf{v}$ and $w_j$ is the $j$-th element of $\\mathbf{w}$. The term $\\mathbf{v}^T\\mathbf{w}$ is the scalar product $\\langle \\mathbf{v}, \\mathbf{w} \\rangle = \\sum_k v_k w_k$. Thus:\n$$ v_i w_j \\mathrm{d}a_{ij} \\approx \\mathrm{d}\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle $$\nThe sensitivity, $s_{ij}$, is the partial derivative $\\frac{\\partial \\lambda}{\\partial a_{ij}}$:\n$$ s_{ij} = \\frac{\\partial \\lambda}{\\partial a_{ij}} = \\frac{v_i w_j}{\\langle \\mathbf{v}, \\mathbf{w} \\rangle} $$\nElasticity, $e_{ij}$, is the proportional sensitivity:\n$$ e_{ij} = \\frac{a_{ij}}{\\lambda} \\frac{\\partial \\lambda}{\\partial a_{ij}} = \\frac{a_{ij} v_i w_j}{\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle} $$\nThis completes the derivation of the required expression.\n\nNow we apply this to the given matrix $\\mathbf{A}$. First, we find the dominant eigenvalue $\\lambda$ by solving the characteristic equation $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$:\n$$ \\det \\begin{pmatrix} -\\lambda  0.6  1.2 \\\\ 0.45  -\\lambda  0 \\\\ 0  0.7  -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2) - 0.6(-0.45\\lambda) + 1.2(0.45 \\cdot 0.7) = 0 $$\n$$ -\\lambda^3 + 0.27\\lambda + 0.378 = 0 \\quad \\implies \\quad \\lambda^3 - 0.27\\lambda - 0.378 = 0 $$\nSolving this cubic equation numerically gives the dominant (positive real) eigenvalue $\\lambda \\approx 0.84673315$.\n\nNext, we find the right eigenvector $\\mathbf{w} = (w_1, w_2, w_3)^T$ from $(\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{w} = \\mathbf{0}$.\nFrom the second row: $0.45w_1 - \\lambda w_2 = 0 \\implies w_2 = (0.45/\\lambda)w_1$.\nFrom the third row: $0.7w_2 - \\lambda w_3 = 0 \\implies w_3 = (0.7/\\lambda)w_2 = (0.315/\\lambda^2)w_1$.\nSetting $w_1=1$ for scaling, the right eigenvector is proportional to $\\mathbf{w} \\propto (1, 0.45/\\lambda, 0.315/\\lambda^2)^T$.\n\nThen, we find the left eigenvector $\\mathbf{v} = (v_1, v_2, v_3)^T$ from $\\mathbf{A}^T\\mathbf{v} = \\lambda\\mathbf{v}$.\n$$ \\mathbf{A}^T = \\begin{pmatrix} 0  0.45  0 \\\\ 0.6  0  0.7 \\\\ 1.2  0  0 \\end{pmatrix} $$\nFrom the first row: $-\\lambda v_1 + 0.45v_2 = 0 \\implies v_2 = (\\lambda/0.45)v_1$.\nFrom the third row: $1.2v_1 - \\lambda v_3 = 0 \\implies v_3 = (1.2/\\lambda)v_1$.\nSetting $v_1=1$, the left eigenvector is proportional to $\\mathbf{v} \\propto (1, \\lambda/0.45, 1.2/\\lambda)^T$.\n\nNow, we compute the scalar product $\\langle \\mathbf{v}, \\mathbf{w} \\rangle$ using these scaled vectors:\n$$ \\langle \\mathbf{v}, \\mathbf{w} \\rangle = v_1w_1+v_2w_2+v_3w_3 = 1 \\cdot 1 + \\left(\\frac{\\lambda}{0.45}\\right)\\left(\\frac{0.45}{\\lambda}\\right) + \\left(\\frac{1.2}{\\lambda}\\right)\\left(\\frac{0.315}{\\lambda^2}\\right) = 2 + \\frac{0.378}{\\lambda^3} $$\nThe denominator of the elasticity formula is $\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle = 2\\lambda + 0.378/\\lambda^2$.\n\nWe proceed to calculate the elasticities for the non-zero elements $a_{ij}$:\n$a_{12}=0.6$ (fecundity of subadults):\n$$ e_{12} = \\frac{a_{12}v_1w_2}{\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle} = \\frac{0.6 \\cdot 1 \\cdot (0.45/\\lambda)}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.27/\\lambda}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.27\\lambda}{2\\lambda^3 + 0.378} $$\n$a_{13}=1.2$ (fecundity of adults):\n$$ e_{13} = \\frac{a_{13}v_1w_3}{\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle} = \\frac{1.2 \\cdot 1 \\cdot (0.315/\\lambda^2)}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.378/\\lambda^2}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.378}{2\\lambda^3 + 0.378} $$\n$a_{21}=0.45$ (survival of yearlings):\n$$ e_{21} = \\frac{a_{21}v_2w_1}{\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle} = \\frac{0.45 \\cdot (\\lambda/0.45) \\cdot 1}{2\\lambda + 0.378/\\lambda^2} = \\frac{\\lambda}{2\\lambda + 0.378/\\lambda^2} = \\frac{\\lambda^3}{2\\lambda^3 + 0.378} $$\n$a_{32}=0.7$ (survival of subadults):\n$$ e_{32} = \\frac{a_{32}v_3w_2}{\\lambda \\langle \\mathbf{v}, \\mathbf{w} \\rangle} = \\frac{0.7 \\cdot (1.2/\\lambda) \\cdot (0.45/\\lambda)}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.378/\\lambda^2}{2\\lambda + 0.378/\\lambda^2} = \\frac{0.378}{2\\lambda^3 + 0.378} $$\nNote that $e_{13} = e_{32}$. Using the characteristic equation $\\lambda^3 = 0.27\\lambda + 0.378$, the sum of elasticities simplifies to $1$, confirming the calculations.\n$\\sum e_{ij} = \\frac{0.27\\lambda + 0.378 + \\lambda^3 + 0.378}{2\\lambda^3 + 0.378} = \\frac{\\lambda^3 - 0.378 + 0.378 + \\lambda^3 + 0.378}{2\\lambda^3 + 0.378} = \\frac{2\\lambda^3 + 0.378}{2\\lambda^3 + 0.378} = 1$.\n\nNow we compute the numerical values using $\\lambda \\approx 0.84673315$:\nThe common denominator in the simplified forms is $2\\lambda^3 + 0.378$. Using $\\lambda^3 = 0.27\\lambda+0.378$:\n$2\\lambda^3 + 0.378 = 2(0.27\\lambda+0.378) + 0.378 = 0.54\\lambda + 1.134$.\n$0.54(0.84673315) + 1.134 \\approx 0.4572359 + 1.134 \\approx 1.5912359$.\n\n$e_{21} = \\frac{\\lambda^3}{1.5912359} = \\frac{0.27\\lambda+0.378}{1.5912359} = \\frac{0.27(0.84673315)+0.378}{1.5912359} = \\frac{0.606618}{1.5912359} \\approx 0.38118$\n$e_{13} = e_{32} = \\frac{0.378}{1.5912359} \\approx 0.23753$\n$e_{12} = \\frac{0.27\\lambda}{1.5912359} = \\frac{0.228618}{1.5912359} \\approx 0.14366$\n\nThe elasticity values are:\n$e_{21}$ (yearling survival) $\\approx 0.3812$\n$e_{13}$ (adult fecundity) $\\approx 0.2375$\n$e_{32}$ (subadult survival) $\\approx 0.2375$\n$e_{12}$ (subadult fecundity) $\\approx 0.1437$\n\nThe largest elasticity value is $e_{21}$, corresponding to the matrix element $a_{21}$ (yearling survival). This indicates that the long-term population growth rate $\\lambda$ is most sensitive, in a proportional sense, to changes in the survival of yearlings.\n\nThe question asks for the largest elasticity value, rounded to four significant figures.\nThe largest value is $0.38118...$, which rounds to $0.3812$.", "answer": "$$\\boxed{0.3812}$$", "id": "2826804"}]}