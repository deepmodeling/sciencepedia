{"hands_on_practices": [{"introduction": "This first exercise provides a foundational experience in demographic analysis. Starting from a simple, hypothetical dataset of a cohort's survival record, we will construct a complete cohort life table from first principles. This practice is designed to solidify your understanding of the core columns of a life table—such as survivorship ($l_x$) and the probability of dying ($q_x$)—and see how they culminate in a key summary statistic, life expectancy [@problem_id:2811963].", "problem": "A closed cohort of size $100$ individuals is observed at integer ages $x \\in \\{0,1,2,3,4,5\\}$ with the cohort counts $\\{n_x\\} = (100,80,61,50,43,40)$ denoting the number alive at the start of each age $x$. Consider one-year age intervals $[x,x+1)$ for $x \\in \\{0,1,2,3,4\\}$. Using only fundamental definitions from survival analysis and life table theory in discrete time, do the following:\n\n- Define the survivorship schedule $l_x$ as the proportion of the initial cohort alive at the start of age $x$, the number dying in the interval $d_x$ as the decrement between adjacent $l_x$ values, the conditional probability of death $q_x$ in $[x,x+1)$ given survival to age $x$, and the conditional probability of survival $p_x$ in $[x,x+1)$ given survival to age $x$.\n- Adopt the uniform distribution of deaths within each interval (a standard approximation consistent with constant force of mortality within an interval), so that the person-years lived in the interval satisfies $L_x=(l_x+l_{x+1})/2$ for each unit-length interval $[x,x+1)$.\n- From first principles, compute the life expectancy at birth over the observation window $[0,5)$, interpreted as the expected number of years lived from birth until age $5$ by a newborn in this cohort, based on the constructed life table quantities.\n\nReport the final value of life expectancy at birth in years, and round your answer to four significant figures. Do not include units in your final boxed answer.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- A closed cohort of size $100$ individuals is observed.\n- Integer ages are $x \\in \\{0,1,2,3,4,5\\}$.\n- Cohort counts are $\\{n_x\\} = (100,80,61,50,43,40)$, where $n_x$ is the number alive at the start of age $x$.\n- Age intervals are one-year intervals $[x,x+1)$ for $x \\in \\{0,1,2,3,4\\}$.\n- Survivorship schedule $l_x$ is defined as the proportion of the initial cohort alive at the start of age $x$.\n- Number dying in an interval, $d_x$, is the decrement between adjacent $l_x$ values.\n- Conditional probability of death, $q_x$, is the probability of death in $[x,x+1)$ given survival to age $x$.\n- Conditional probability of survival, $p_x$, is the probability of survival in $[x,x+1)$ given survival to age $x$.\n- The uniform distribution of deaths (UDD) within each interval is adopted.\n- The person-years lived in an interval is $L_x=(l_x+l_{x+1})/2$.\n- The objective is to compute the life expectancy at birth over the observation window $[0,5)$, interpreted as the expected number of years lived from birth until age $5$.\n- The final answer must be rounded to four significant figures.\n\nThe problem is determined to be valid. It is scientifically grounded in established principles of demography and survival analysis. It is well-posed, providing all necessary data and definitions ($n_x$, UDD assumption) to calculate the required quantity. The data are consistent, as the number of survivors $\\{n_x\\}$ is a monotonically non-increasing sequence. The language is objective and precise. The problem is a standard exercise in life table construction and analysis.\n\nWe proceed with the solution.\n\nFirst, we construct the basic life table functions from the given cohort data. The initial cohort size, or radix, is $n_0=100$. The survivorship schedule, $l_x$, is defined as the proportion of the initial cohort alive at the start of age $x$. Therefore, $l_x = n_x / n_0$. The radix of this life table is $l_0 = n_0/n_0 = 100/100 = 1$.\n\nThe values of $l_x$ for $x \\in \\{0,1,2,3,4,5\\}$ are:\n$$l_0 = \\frac{100}{100} = 1.0$$\n$$l_1 = \\frac{80}{100} = 0.8$$\n$$l_2 = \\frac{61}{100} = 0.61$$\n$$l_3 = \\frac{50}{100} = 0.50$$\n$$l_4 = \\frac{43}{100} = 0.43$$\n$$l_5 = \\frac{40}{100} = 0.40$$\n\nAs per the problem request, we also define the other fundamental quantities, although they are not all required for the final calculation. The number of deaths in the interval $[x, x+1)$ for a cohort starting with $l_x$ individuals is $d_x = l_x - l_{x+1}$. The conditional probability of death in $[x, x+1)$ given survival to age $x$ is $q_x = d_x/l_x$. The conditional probability of survival is $p_x = 1 - q_x = l_{x+1}/l_x$.\n\nThe primary objective is to compute the life expectancy at birth over the observation window $[0,5)$. This is interpreted as the expected number of years lived by a newborn from age $0$ to age $5$. This quantity is known as the temporary life expectancy from age $0$ for $5$ years, denoted $e_{0:5\\rceil}$. It is calculated as the total person-years lived by the cohort between ages $0$ and $5$, divided by the initial number of individuals in the cohort at age $0$.\nThe total person-years lived between age $x$ and $x+k$ is $\\sum_{i=x}^{x+k-1} L_i$. Thus, the temporary life expectancy is given by:\n$$e_{0:5\\rceil} = \\frac{\\sum_{x=0}^{4} L_x}{l_0}$$\nHere, $L_x$ represents the person-years lived in the age interval $[x,x+1)$. The problem specifies the formula $L_x = (l_x+l_{x+1})/2$, which is the standard calculation under the assumption of a uniform distribution of deaths (UDD) within each one-year interval.\n\nWe now calculate the values of $L_x$ for each interval from $x=0$ to $x=4$:\n$$L_0 = \\frac{l_0 + l_1}{2} = \\frac{1.0 + 0.8}{2} = \\frac{1.8}{2} = 0.9$$\n$$L_1 = \\frac{l_1 + l_2}{2} = \\frac{0.8 + 0.61}{2} = \\frac{1.41}{2} = 0.705$$\n$$L_2 = \\frac{l_2 + l_3}{2} = \\frac{0.61 + 0.50}{2} = \\frac{1.11}{2} = 0.555$$\n$$L_3 = \\frac{l_3 + l_4}{2} = \\frac{0.50 + 0.43}{2} = \\frac{0.93}{2} = 0.465$$\n$$L_4 = \\frac{l_4 + l_5}{2} = \\frac{0.43 + 0.40}{2} = \\frac{0.83}{2} = 0.415$$\n\nNext, we sum these values to find the total person-years lived from age $0$ to age $5$:\n$$\\sum_{x=0}^{4} L_x = L_0 + L_1 + L_2 + L_3 + L_4$$\n$$\\sum_{x=0}^{4} L_x = 0.9 + 0.705 + 0.555 + 0.465 + 0.415 = 3.04$$\n\nThe calculated sum is an exact value, derived from exact rational numbers. Finally, we compute the life expectancy over the specified window:\n$$e_{0:5\\rceil} = \\frac{\\sum_{x=0}^{4} L_x}{l_0} = \\frac{3.04}{1.0} = 3.04$$\n\nThe problem requires the answer to be rounded to four significant figures. The exact value is $3.04$, which has three significant figures. To express this value with four significant figures, we must add a trailing zero, which gives $3.040$.\nThe life expectancy at birth over the observation window $[0,5)$ is $3.040$ years.", "answer": "$$\n\\boxed{3.040}\n$$", "id": "2811963"}, {"introduction": "Real-world studies rarely track every individual until the event of interest occurs; some participants are often lost to follow-up, creating what is known as right-censored data. This exercise introduces the powerful Kaplan-Meier product-limit estimator, a cornerstone of modern survival analysis, for estimating the survival function in the presence of such incomplete information. You will practice calculating survival probabilities step-by-step, learning how the risk set changes at each event time, a crucial skill for interpreting data from clinical trials or ecological field studies [@problem_id:2811971].", "problem": "A cohort study follows $n=4$ individuals with distinct follow-up completion times (in the same time unit): $2, 3, 5, 7$. Two individuals experience the event (death) at times $t=3$ and $t=7$, and two are right-censored at times $t=2$ and $t=5$. Assume non-informative right-censoring, no tied event times, and that the risk set at each event time includes all individuals still under observation strictly prior to that time. Using only first principles of survival analysis and standard large-sample theory, do the following in order:\n- Construct the Kaplan–Meier (KM) product-limit estimate $\\hat{S}(t)$ of the survival function evaluated at the observed times $t \\in \\{2, 3, 5, 7\\}$.\n- Derive a two-sided $95\\%$ confidence interval (CI) for $S(7)$ that is scientifically defensible under asymptotic theory, carefully justifying your choice of transformation or scale if needed near parameter boundaries.\n\nExpress all probabilities as decimals. Round all reported probabilities to four significant figures. Report as your final numeric answer the value of $\\hat{S}(5)$, rounded to four significant figures.", "solution": "The problem requires the construction of a Kaplan-Meier (KM) survival function estimate and a confidence interval (CI) for survival at a specific time point, based on a small cohort study. I will first validate the problem statement and, finding it valid, proceed with a rigorous, step-by-step derivation based on first principles.\n\nFirst, we organize the provided data. The total cohort size is $n=4$. The observations are as follows: one individual is right-censored at time $t=2$; one experiences an event (death) at time $t=3$; one is right-censored at time $t=5$; and one experiences an event at time $t=7$. The distinct event times are $t_1=3$ and $t_2=7$.\n\nLet $t_j$ denote the $j$-th distinct event time. Let $n_j$ be the number of individuals at risk (alive and under observation) just prior to time $t_j$. Let $d_j$ be the number of events occurring at time $t_j$. The risk sets and events are tabulated as follows:\n- At time $t=0$, the number of individuals at risk is $4$.\n- At time $t=2$, one individual is censored. The number at risk becomes $4-1=3$.\n- At the first event time, $t_1=3$: The number at risk is $n_1=3$. The number of events is $d_1=1$.\n- After the event at $t=3$, the number at risk becomes $3-1=2$.\n- At time $t=5$, one individual is censored. The number at risk becomes $2-1=1$.\n- At the second event time, $t_2=7$: The number at risk is $n_2=1$. The number of events is $d_2=1$.\n\nThe Kaplan-Meier product-limit estimator for the survival function $S(t) = P(T > t)$ is given by the formula:\n$$ \\hat{S}(t) = \\prod_{j: t_j \\le t} \\left(1 - \\frac{d_j}{n_j}\\right) $$\nWe evaluate this estimator at the specified time points $t \\in \\{2, 3, 5, 7\\}$.\n\n- For $t=2$: No events have occurred up to this time. The survival function estimate is therefore $\\hat{S}(2) = 1$. The censoring at $t=2$ does not change the estimate of $S(2)$, but it does reduce the size of the risk set for subsequent times. To four significant figures, $\\hat{S}(2) = 1.000$.\n\n- For $t=3$: An event occurs at this time. The estimate is calculated as:\n$$ \\hat{S}(3) = \\left(1 - \\frac{d_1}{n_1}\\right) = \\left(1 - \\frac{1}{3}\\right) = \\frac{2}{3} $$\nAs a decimal rounded to four significant figures, $\\hat{S}(3) \\approx 0.6667$.\n\n- For $t=5$: No event occurs between $t=3$ and $t=5$. The survival estimate remains constant over this interval. The censoring at $t=5$ does not change the survival estimate at that point.\n$$ \\hat{S}(5) = \\hat{S}(3) = \\frac{2}{3} $$\nAs a decimal rounded to four significant figures, $\\hat{S}(5) \\approx 0.6667$.\n\n- For $t=7$: A second event occurs. The survival estimate is updated by multiplying the previous survival probability by the conditional probability of surviving past $t=7$:\n$$ \\hat{S}(7) = \\hat{S}(t<7) \\times \\left(1 - \\frac{d_2}{n_2}\\right) = \\frac{2}{3} \\times \\left(1 - \\frac{1}{1}\\right) = \\frac{2}{3} \\times 0 = 0 $$\nTo four significant figures, $\\hat{S}(7) = 0.0000$.\n\nSummary of KM estimates: $\\hat{S}(2)=1.000$, $\\hat{S}(3) \\approx 0.6667$, $\\hat{S}(5) \\approx 0.6667$, $\\hat{S}(7)=0.0000$.\n\nNext, we must derive a two-sided $95\\%$ confidence interval for $S(7)$. The point estimate is $\\hat{S}(7)=0$. This is a boundary case which requires careful consideration. A standard approach for constructing a CI for $\\hat{S}(t)$ involves Greenwood's formula for the variance:\n$$ \\widehat{\\text{Var}}(\\hat{S}(t)) = [\\hat{S}(t)]^2 \\sum_{j: t_j \\le t} \\frac{d_j}{n_j(n_j - d_j)} $$\nA linear confidence interval is then $\\hat{S}(t) \\pm z_{\\alpha/2} \\sqrt{\\widehat{\\text{Var}}(\\hat{S}(t))}$, where $z_{0.025} \\approx 1.96$ for a $95\\%$ CI. However, this method has known deficiencies, particularly that it can produce limits outside the valid range of $[0, 1]$.\n\nTo rectify this, transformations of $S(t)$ are used. A common and theoretically justified choice is the complementary log-log transformation, $g(p) = \\ln(-\\ln(p))$, which maps the interval $(0, 1)$ to $(-\\infty, \\infty)$. A CI is constructed on this transformed scale and then back-transformed.\n\nHowever, in this specific problem, $\\hat{S}(7)=0$. Any transformation involving $\\ln(\\hat{S}(t))$ or $\\ln(-\\ln(\\hat{S}(t)))$ is undefined at this point. Furthermore, Greenwood's formula for variance itself becomes problematic at $t=7$. The second term in the summation corresponds to the event at $t_2=7$, where $n_2=1$ and $d_2=1$. The denominator term $n_2(n_2-d_2)$ becomes $1(1-1)=0$, leading to division by zero.\n\nThe scientifically defensible path, consistent with standard practice in survival analysis (e.g., as described in Klein and Moeschberger), is to address this boundary case directly. When the largest observed time is an uncensored event, the Kaplan-Meier estimate drops to $0$ and remains there. The variance of the estimator is conventionally taken to be $0$ for all subsequent times. This reflects the fact that, based on the observed data, there is no evidence of survival beyond this point; the estimate of survival is precisely $0$ with no uncertainty according to the asymptotic approximation. While this is a known limitation of the asymptotic theory, especially for small sample sizes, it is the standard convention.\n\nTherefore, for $t=7$, we have $\\hat{S}(7)=0$ and $\\widehat{\\text{Var}}(\\hat{S}(7))=0$. The standard error is $\\sqrt{0}=0$. The $95\\%$ confidence interval is then:\n$$ \\hat{S}(7) \\pm 1.96 \\times 0 = [0, 0] $$\nThis degenerate interval is the direct and logical consequence of applying standard large-sample theory to a dataset where the last observation is an event, rendering survival beyond this point an event with an estimated probability of $0$. No transformation-based method is applicable.\n\nThe final task is to report the value of $\\hat{S}(5)$ rounded to four significant figures. As calculated previously, $\\hat{S}(5) = \\frac{2}{3} \\approx 0.666666...$, which rounds to $0.6667$.", "answer": "$$\\boxed{0.6667}$$", "id": "2811971"}, {"introduction": "While non-parametric methods provide a step-wise estimate of survival, parametric models allow us to describe the underlying survival pattern with a smooth, continuous function defined by a few interpretable parameters. This final practice bridges the gap between data description and modeling by asking you to fit the classic Gompertz model to survivorship data. You will derive the survival function from its corresponding hazard rate, $\\mu(x) = \\alpha\\exp(\\beta x)$, and use numerical methods to estimate the parameters that best describe the aging process [@problem_id:2811928].", "problem": "You are given discretely observed survivorship values at integer ages for a cohort, and you must fit a parametric hazard model using Nonlinear Least Squares (NLS) to the survivorship scale. The biological context is a Gompertz hazard, which is commonly used in survivorship analysis. The hazard function is specified by two positive parameters: an initial hazard level and an aging rate. The core definitions to use as a starting point are the following. The hazard function $\\,\\mu(x)\\,$ is defined by the relationship $\\,\\mu(x) = -\\dfrac{d}{dx}\\log S(x)\\,$, where $\\,S(x)\\,$ is the survivorship function for age $\\,x\\,$, and $\\,S(x)\\in(0,1]\\,$ is a nonincreasing function with $\\,S(0)=1\\,$. For the Gompertz model, the hazard function is $\\,\\mu(x)=\\alpha e^{\\beta x}\\,$ with $\\,\\alpha>0\\,$ and $\\,\\beta>0\\,$. You must derive the corresponding survivorship $\\,S(x)\\,$ implied by this hazard, and then fit the parameters $\\,(\\alpha,\\beta)\\,$ by minimizing the sum of squared residuals on the survivorship scale.\n\nPrecisely, for a given dataset of ages $\\,\\{x_i\\}_{i=0}^{n}\\,$ and observed survivorship values $\\,\\{S_i\\}_{i=0}^{n}\\,$ with $\\,S_i=S(x_i)\\,$, you must solve the following unconstrained minimization problem over $\\,\\alpha>0\\,$ and $\\,\\beta>0\\,$: minimize\n$$\n\\sum_{i=0}^{n} \\left(S_i - \\widehat{S}(x_i;\\alpha,\\beta)\\right)^2,\n$$\nwhere $\\,\\widehat{S}(x;\\alpha,\\beta)\\,$ is the survivorship function implied by the hazard $\\,\\mu(x)=\\alpha e^{\\beta x}\\,$, derived from the fundamental relationship $\\,\\mu(x) = -\\dfrac{d}{dx}\\log S(x)\\,$. You must enforce $\\,\\alpha>0\\,$ and $\\,\\beta>0\\,$.\n\nYour program must solve this problem for the following three test cases, each with integer ages $\\,x\\in\\{0,1,2,3,4,5\\}\\,$ and survivorship values as specified. All survivorship values must be treated as decimal fractions on $[0,1]$ (never as percentages).\n\n- Test case A (empirical): ages $\\,x\\in\\{0,1,2,3,4,5\\}\\,$ with observed survivorship values\n$$\n\\{S_0,S_1,S_2,S_3,S_4,S_5\\}=\\{1.0,\\,0.9,\\,0.78,\\,0.61,\\,0.45,\\,0.30\\}.\n$$\n\n- Test case B (model-generated, exact): ages $\\,x\\in\\{0,1,2,3,4,5\\}\\,$, and data generated exactly from a Gompertz hazard with parameters $\\,\\alpha=0.2\\,$ and $\\,\\beta=0.3\\,$ at those ages. Specifically, construct the survivorship values using the derived $\\,\\widehat{S}(x;\\alpha,\\beta)\\,$ evaluated at $\\,x\\in\\{0,1,2,3,4,5\\}\\,$ with $\\,(\\alpha,\\beta)=(0.2,0.3)\\,$ and treat those as the observed $\\,\\{S_i\\}\\,$ to be fitted back to the model using NLS.\n\n- Test case C (model-generated, near-exponential): ages $\\,x\\in\\{0,1,2,3,4,5\\}\\,$, and data generated exactly from a Gompertz hazard with parameters $\\,\\alpha=0.1\\,$ and $\\,\\beta=0.01\\,$ at those ages. As in test case B, construct the survivorship values using the derived $\\,\\widehat{S}(x;\\alpha,\\beta)\\,$ at $\\,x\\in\\{0,1,2,3,4,5\\}\\,$ with $\\,(\\alpha,\\beta)=(0.1,0.01)\\,$ and then fit the model by NLS.\n\nImplementation and algorithmic requirements:\n- Begin from the definitions $\\,\\mu(x) = -\\dfrac{d}{dx}\\log S(x)\\,$ and $\\,\\mu(x)=\\alpha e^{\\beta x}\\,$, derive $\\,\\widehat{S}(x;\\alpha,\\beta)\\,$, and then formulate the NLS objective on the survivorship scale as stated.\n- Enforce $\\,\\alpha>0\\,$ and $\\,\\beta>0\\,$ explicitly, for example by reparameterizing with $\\,\\alpha=\\exp(\\theta_1)\\,$ and $\\,\\beta=\\exp(\\theta_2)\\,$, and optimizing over $\\,\\theta_1,\\theta_2\\in\\mathbb{R}\\,$.\n- Use a deterministic optimization routine suitable for NLS. Since the objective is smooth, a standard trust-region or Levenberg–Marquardt method is appropriate.\n- Numerical outputs must be rounded to $\\,6\\,$ decimal places.\n- Units: survivorship values are unitless proportions. No physical units and no percentages are to be used.\n\nFinal output specification:\n- For each test case, report the fitted parameter pair as a two-element list $[\\hat{\\alpha},\\hat{\\beta}]$ with each value rounded to $\\,6\\,$ decimal places.\n- Your program should produce a single line of output containing the results as a JSON-like list of the three fitted pairs in the order A, B, C. For example, the output format must be\n$$\n[[\\hat{\\alpha}_A,\\hat{\\beta}_A],[\\hat{\\alpha}_B,\\hat{\\beta}_B],[\\hat{\\alpha}_C,\\hat{\\beta}_C]]\n$$\nwith no additional text. Ensure exactly this single-line format, with each float rendered to $\\,6\\,$ decimal places.\n\nBoundary and edge-case coverage rationale:\n- Test case A provides an empirical, noisy scenario to assess general fitting on the survivorship scale.\n- Test case B provides an exact model-generated scenario to assess identifiability and numerical recovery of parameters in the absence of noise.\n- Test case C provides a near-exponential hazard case with very small $\\,\\beta\\,$ to test numerical stability as $\\,\\beta\\to 0^+\\,$.\n\nYour algorithm must succeed on all three test cases and adhere strictly to the output format.", "solution": "The problem statement is valid. It specifies a well-defined task in survivorship analysis: fitting a Gompertz hazard model to discrete survivorship data using Nonlinear Least Squares (NLS). The model, definitions, and objective function are standard in the field and are specified with mathematical precision. The problem is scientifically grounded, logically consistent, and all necessary data and constraints are provided. The three test cases are well-designed to assess the robustness of the fitting procedure. I will now proceed with the solution.\n\nThe core of the problem is to estimate the parameters $\\alpha$ and $\\beta$ of the Gompertz hazard model, $\\mu(x) = \\alpha e^{\\beta x}$, by fitting the corresponding survivorship function, $\\widehat{S}(x;\\alpha,\\beta)$, to a set of observed survivorship probabilities $\\{S_i\\}$ at ages $\\{x_i\\}$.\n\nFirst, we must derive the survivorship function $\\widehat{S}(x;\\alpha,\\beta)$ from the fundamental relationship given:\n$$\n\\mu(x) = -\\frac{d}{dx}\\log S(x)\n$$\nBy integrating this relationship from age $0$ to age $x$ and enforcing the initial condition $S(0)=1$, which implies $\\log S(0)=0$, we obtain an expression for $\\log S(x)$. Let $t$ be the integration variable.\n$$\n\\int_0^x \\frac{d}{dt}\\log S(t) \\,dt = \\log S(x) - \\log S(0) = \\log S(x)\n$$\nAnd,\n$$\n\\log S(x) = \\int_0^x -\\mu(t) \\,dt\n$$\nSubstituting the Gompertz hazard function, $\\mu(t) = \\alpha e^{\\beta t}$, we have:\n$$\n\\log S(x) = - \\int_0^x \\alpha e^{\\beta t} \\,dt = -\\alpha \\left[ \\frac{e^{\\beta t}}{\\beta} \\right]_0^x\n$$\nEvaluating the definite integral yields:\n$$\n\\log S(x) = -\\frac{\\alpha}{\\beta} \\left( e^{\\beta x} - e^{\\beta \\cdot 0} \\right) = -\\frac{\\alpha}{\\beta} \\left( e^{\\beta x} - 1 \\right)\n$$\nExponentiating both sides provides the Gompertz survivorship function:\n$$\n\\widehat{S}(x; \\alpha, \\beta) = \\exp\\left( -\\frac{\\alpha}{\\beta} (e^{\\beta x} - 1) \\right)\n$$\nThis is the theoretical model function $\\widehat{S}(x;\\alpha,\\beta)$ that will be fitted to the data.\n\nThe fitting procedure is Nonlinear Least Squares (NLS). We must find the parameter values $(\\hat{\\alpha}, \\hat{\\beta})$ that minimize the sum of squared residuals (SSR) between the observed survivorship values $S_i$ and the values predicted by our model $\\widehat{S}(x_i;\\alpha,\\beta)$:\n$$\n\\min_{\\alpha > 0, \\beta > 0} \\text{SSR}(\\alpha, \\beta) = \\min_{\\alpha > 0, \\beta > 0} \\sum_{i=0}^{n} \\left(S_i - \\widehat{S}(x_i;\\alpha,\\beta)\\right)^2\n$$\nThe constraints $\\alpha > 0$ and $\\beta > 0$ are critical for the model to be biologically meaningful (initial hazard and aging rate must be positive). A standard and robust method to enforce these positivity constraints is to reparameterize the model. Let:\n$$\n\\alpha = e^{\\theta_1} \\quad \\text{and} \\quad \\beta = e^{\\theta_2}\n$$\nThe optimization is now performed over the unconstrained real-valued parameters $\\theta_1$ and $\\theta_2$. The objective function becomes minimizing the sum of squares of the residual vector $\\mathbf{r}$, where each component is $r_i(\\theta_1, \\theta_2) = S_i - \\widehat{S}(x_i; e^{\\theta_1}, e^{\\theta_2})$.\n\nFor numerical implementation, a Levenberg-Marquardt or trust-region algorithm is appropriate. We will use `scipy.optimize.least_squares` for this purpose. A consideration for numerical stability arises in the term $(e^{\\beta x} - 1)/\\beta$ when $\\beta$ is small. To avoid potential loss of precision, the argument of the exponential in the survivorship function, $-\\frac{\\alpha}{\\beta}(e^{\\beta x} - 1)$, is calculated as $-\\frac{\\alpha}{\\beta} \\cdot \\text{expm1}(\\beta x)$, using the `numpy.expm1` function which computes $e^y-1$ accurately even for small $y$.\n\nThe procedure is applied to each of the three test cases. For Test Cases B and C, the \"observed\" survivorship data $\\{S_i\\}$ are first generated using the specified true parameters $(\\alpha, \\beta)$ and the derived survivorship function $\\widehat{S}(x;\\alpha,\\beta)$. The NLS procedure is then applied to this generated data to verify that the optimization can recover the known true parameters, confirming the correctness of the implementation and its numerical stability. For all cases, once the optimal parameters $(\\hat{\\theta}_1, \\hat{\\theta}_2)$ are found, they are transformed back to the original parameter space to obtain the final estimates $(\\hat{\\alpha}, \\hat{\\beta}) = (e^{\\hat{\\theta}_1}, e^{\\hat{\\theta}_2})$. The final reported values are rounded to $6$ decimal places as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Solves for the Gompertz parameters alpha and beta for three test cases\n    using Nonlinear Least Squares on the survivorship scale.\n    \"\"\"\n\n    def gompertz_survivorship(thetas, x):\n        \"\"\"\n        Calculates the Gompertz survivorship function S(x).\n        Parameters are log-transformed: alpha = exp(theta1), beta = exp(theta2).\n        Uses a numerically stable calculation for the exponent.\n        \"\"\"\n        # Note: thetas = [theta1, theta2]\n        alpha = np.exp(thetas[0])\n        beta = np.exp(thetas[1])\n        \n        # When beta is very small, direct computation of (exp(beta*x)-1)/beta\n        # can be unstable. Using np.expm1(y) for exp(y)-1 offers better precision for small y.\n        # The exponent's argument is -(alpha/beta) * (exp(beta*x) - 1).\n        # We compute this as -(alpha/beta) * np.expm1(beta * x).\n        # This handles the beta -> 0 limit correctly, approaching -alpha*x.\n        if np.abs(beta) < 1e-9: # Handle the limit case for numerical stability\n            exponent_arg = -alpha * x\n        else:\n            exponent_arg = - (alpha / beta) * np.expm1(beta * x)\n        \n        return np.exp(exponent_arg)\n\n    def residuals(thetas, x_data, s_data):\n        \"\"\"\n        Calculates the residuals between observed and predicted survivorship.\n        This is the objective function for scipy.optimize.least_squares.\n        \"\"\"\n        s_predicted = gompertz_survivorship(thetas, x_data)\n        return s_data - s_predicted\n\n    def solve_case(x_data, s_data, initial_guess_thetas):\n        \"\"\"\n        Performs the NLS optimization for a single data case.\n        \"\"\"\n        result = least_squares(\n            fun=residuals,\n            x0=initial_guess_thetas,\n            args=(x_data, s_data),\n            method='trf', # Trust Region Reflective algorithm, robust for this task.\n            bounds=([-np.inf, -np.inf], [np.inf, np.inf])\n        )\n        \n        # Extract optimal log-parameters and convert back\n        theta1_hat, theta2_hat = result.x\n        alpha_hat = np.exp(theta1_hat)\n        beta_hat = np.exp(theta2_hat)\n        \n        return [alpha_hat, beta_hat]\n\n    # --- Test Case Definitions ---\n\n    # Test Case A: Empirical Data\n    x_a = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    s_a = np.array([1.0, 0.9, 0.78, 0.61, 0.45, 0.30], dtype=float)\n    # Initial guess based on simple approximation from data\n    # alpha ~ -log(S(1)) = -log(0.9) ~ 0.1\n    # beta is an aging rate, 0.3 is a reasonable guess.\n    initial_guess_a = [np.log(0.1), np.log(0.3)]\n\n    # Test Case B: Model-generated, exact\n    x_b = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    alpha_b_true, beta_b_true = 0.2, 0.3\n    # S(x) = exp(-(alpha/beta)*(exp(beta*x)-1))\n    s_b = np.exp(-(alpha_b_true / beta_b_true) * np.expm1(beta_b_true * x_b))\n    # Perturbed initial guess to test convergence\n    initial_guess_b = [np.log(0.25), np.log(0.25)]\n\n    # Test Case C: Model-generated, near-exponential (small beta)\n    x_c = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    alpha_c_true, beta_c_true = 0.1, 0.01\n    s_c = np.exp(-(alpha_c_true / beta_c_true) * np.expm1(beta_c_true * x_c))\n    # Perturbed initial guess\n    initial_guess_c = [np.log(0.11), np.log(0.011)]\n\n    test_cases = [\n        (x_a, s_a, initial_guess_a),\n        (x_b, s_b, initial_guess_b),\n        (x_c, s_c, initial_guess_c),\n    ]\n\n    results = []\n    for x_data, s_data, initial_guess in test_cases:\n        result_pair = solve_case(x_data, s_data, initial_guess)\n        results.append(result_pair)\n\n    # Format the final output string as specified\n    formatted_strings = [f\"[{item[0]:.6f},{item[1]:.6f}]\" for item in results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```", "id": "2811928"}]}