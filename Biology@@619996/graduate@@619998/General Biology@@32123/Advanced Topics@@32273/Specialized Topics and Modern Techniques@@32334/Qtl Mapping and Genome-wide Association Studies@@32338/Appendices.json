{"hands_on_practices": [{"introduction": "The foundation of any successful Genome-Wide Association Study (GWAS) is rigorous data quality control (QC). Before searching for genetic associations, it is imperative to identify and remove low-quality samples and genetic markers that can introduce spurious signals and obscure true findings. This first exercise places you in the role of a data analyst tasked with designing a complete QC pipeline, using only the empirical summary statistics from a large, hypothetical GWAS dataset. By working through this scenario [@problem_id:2830645], you will develop the crucial skill of making principled, data-driven decisions to define QC thresholds, striking a delicate balance between retaining valuable data and eliminating potential artifacts.", "problem": "You are given a genome-wide association study (GWAS) dataset with $n = 12{,}000$ genotyped individuals (cases $= 5{,}000$, controls $= 7{,}000$) and $m = 700{,}000$ autosomal single nucleotide polymorphisms (SNPs) after standard array clustering and genotype calling. You plan to perform pre-association quality control (QC) using common GWAS metrics: per-individual call rate and missingness, per-SNP call rate and missingness, autosomal heterozygosity rate, sex inference on the X chromosome, relatedness, and Hardy–Weinberg Equilibrium (HWE). For concreteness, assume the following empirical summaries derived from the raw data:\n\n- Per-individual missingness (fraction of genotypes not called) has a primary mode near $0.003$, a $95$th percentile at $0.012$, and a long right tail up to $0.12$. There is a secondary cluster of approximately $150$ individuals with missingness between $0.06$ and $0.10$.\n- Per-SNP missingness has a mode near $0.002$, a $95$th percentile at $0.015$, and a long tail up to $0.25$. There is a minor mode around $0.06$ involving about $12{,}000$ SNPs.\n- Autosomal heterozygosity rate, computed on a linkage disequilibrium (LD)-pruned set of $100{,}000$ approximately independent SNPs, is approximately normally distributed with mean $0.315$ and standard deviation $0.012$. There are outlier clusters at $0.36$ (about $30$ individuals) and at $0.28$ (about $20$ individuals).\n- X chromosome sex inference using the inbreeding coefficient on the non-pseudoautosomal X ($F_X$) shows that reported males ($n \\approx 5{,}800$) have $F_X$ distributed around mean $0.95$ with standard deviation $0.05$, and reported females ($n \\approx 6{,}200$) around mean $0.02$ with standard deviation $0.03$. There are about $40$ reported females with $F_X > 0.85$ and $25$ reported males with $F_X < 0.15$.\n- Pairwise relatedness estimated by identity-by-descent (IBD) proportion $\\hat{\\pi}$ reveals approximately $60$ duplicate or monozygotic (MZ) twin pairs with $\\hat{\\pi} > 0.98$, $220$ putative first-degree relative pairs with $\\hat{\\pi} \\in [0.40, 0.60]$, and $300$ putative second-degree relative pairs with $\\hat{\\pi} \\in [0.18, 0.25]$. The remaining pairs have $\\hat{\\pi} < 0.05$.\n- HWE exact test $p$-values computed in controls show a near-uniform distribution except for an enrichment of small $p$-values: about $25{,}000$ SNPs have $p < 10^{-4}$ and $1{,}200$ SNPs have $p < 10^{-6}$. SNPs with $p < 10^{-6}$ are enriched for higher missingness (median missingness around $0.06$).\n\nDefinitions: per-individual call rate is the proportion of non-missing genotype calls across all SNPs for an individual; per-SNP call rate is the proportion of non-missing genotype calls across all individuals for a SNP. Autosomal heterozygosity rate is the fraction of heterozygous genotypes across a pruned set of autosomal SNPs for an individual. The X chromosome inbreeding coefficient $F_X$ is near $1$ for males (hemizygous) and near $0$ for females under typical diploid female X genotypes. The IBD proportion $\\hat{\\pi}$ is the estimated proportion of the genome shared IBD between a pair of individuals, with typical expectations near $1.0$ for duplicates/MZ twins, $0.5$ for first-degree relatives, and $0.25$ for second-degree relatives. Under Hardy–Weinberg equilibrium, genotype frequencies satisfy $p^2$, $2pq$, $q^2$ given allele frequencies $p$ and $q = 1 - p$; in large outbred populations with random mating, HWE test $p$-values are approximately uniform under the null.\n\nYou must choose a QC protocol that both correctly defines these metrics and sets thresholds that balance retention of high-quality data with removal of likely artifacts, using only the empirical summaries above and first principles of GWAS QC. Which option best meets this goal?\n\nA. Individuals: exclude if missingness $> 0.02$ (equivalently call rate $< 0.98$); SNPs: exclude if missingness $> 0.02$ (equivalently call rate $< 0.98$); heterozygosity: flag and exclude autosomal outliers outside mean $\\pm 3$ standard deviations computed on LD-pruned SNPs; sex check: declare genetic sex male if $F_X \\ge 0.80$ and female if $F_X \\le 0.20$, and exclude discordant or ambiguous individuals; relatedness: remove one sample from each pair with $\\hat{\\pi} > 0.185$ to limit to at most third-degree relatedness; HWE: compute in controls and exclude SNPs with $p < 10^{-6}$.\n\nB. Individuals: exclude if missingness $> 0.005$; SNPs: exclude if missingness $> 0.005$; heterozygosity: exclude outside mean $\\pm 2$ standard deviations; sex check: declare genetic sex by $F_X \\ge 0.50$ for males and $F_X < 0.50$ for females; relatedness: remove only duplicates and first-degree relatives using $\\hat{\\pi} > 0.35$; HWE: compute in all samples and exclude SNPs with $p < 10^{-4}$.\n\nC. Individuals: exclude if missingness $> 0.08$; SNPs: exclude if missingness $> 0.08$; heterozygosity: exclude outside mean $\\pm 5$ standard deviations; sex check: skip X-based sex inference to maximize retention; relatedness: remove one from each pair with $\\hat{\\pi} > 0.10$; HWE: compute in controls and exclude SNPs with $p < 10^{-2}$.\n\nD. Individuals: exclude if missingness $> 0.02$; SNPs: exclude if missingness $> 0.02$ when minor allele frequency (MAF) $\\ge 0.01$ but allow up to $0.05$ when MAF $< 0.01$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: use $F_X \\ge 0.80$ for males and $F_X \\le 0.20$ for females and remove discordant or ambiguous individuals; relatedness: remove one from each pair with $\\hat{\\pi} > 0.125$; HWE: compute in controls and exclude SNPs with $p < 10^{-6}$.\n\nE. Individuals: exclude if missingness $> 0.02$; SNPs: exclude if missingness $> 0.02$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: as in A; relatedness: remove one from each pair with $\\hat{\\pi} > 0.185$; HWE: compute in controls and exclude SNPs with $p < 7.1 \\times 10^{-8}$ by Bonferroni correction using $m = 700{,}000$.\n\nSelect the single best option.", "solution": "The task is to evaluate several proposed quality control (QC) protocols for a genome-wide association study (GWAS) and select the best one. The evaluation must be based on established principles of GWAS QC and the specific empirical data summaries provided in the problem statement.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- Sample: $n = 12,000$ individuals (cases $= 5,000$, controls $= 7,000$).\n- Genetic data: $m = 700,000$ autosomal single nucleotide polymorphisms (SNPs).\n- Per-individual missingness: mode $\\approx 0.003$, $95$th percentile $= 0.012$, tail up to $0.12$. A secondary cluster of $\\approx 150$ individuals has missingness $\\in [0.06, 0.10]$.\n- Per-SNP missingness: mode $\\approx 0.002$, $95$th percentile $= 0.015$, tail up to $0.25$. A minor mode at $\\approx 0.06$ involves $\\approx 12,000$ SNPs.\n- Autosomal heterozygosity rate: on $100,000$ LD-pruned SNPs, approximately normal with mean $\\mu = 0.315$ and standard deviation $\\sigma = 0.012$. Outlier clusters are present at $0.36$ ($\\approx 30$ individuals) and $0.28$ ($\\approx 20$ individuals).\n- X chromosome sex inference ($F_X$): for reported males, $F_X$ is distributed around mean $0.95$, $\\sigma = 0.05$. For reported females, $F_X$ is distributed around mean $0.02$, $\\sigma = 0.03$. About $40$ reported females have $F_X > 0.85$, and about $25$ reported males have $F_X < 0.15$.\n- Pairwise relatedness ($\\hat{\\pi}$): $\\approx 60$ duplicate/MZ twin pairs ($\\hat{\\pi} > 0.98$), $\\approx 220$ first-degree pairs ($\\hat{\\pi} \\in [0.40, 0.60]$), $\\approx 300$ second-degree pairs ($\\hat{\\pi} \\in [0.18, 0.25]$).\n- Hardy–Weinberg Equilibrium (HWE): $p$-values in controls are enriched at the low end. $\\approx 25,000$ SNPs have $p < 10^{-4}$ and $\\approx 1,200$ SNPs have $p < 10^{-6}$. These low-$p$ SNPs are enriched for high missingness.\n- Definitions of all metrics are provided and are standard.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. All metrics and their described empirical distributions are realistic for a large human GWAS dataset. The problem is well-posed, asking for the selection of the best QC protocol based on provided data, a standard task in bioinformatics. The language is objective and precise. The problem is self-contained and free of contradictions. The data provided are sufficient to make a principled choice.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the analysis.\n\n### Derivation of a Sound QC Protocol from First Principles and Data\n\nA sound QC protocol must remove likely artifacts while retaining as much high-quality data as possible. Each threshold should be justified by the provided empirical distributions.\n\n1.  **Per-individual missingness**: Most individuals have very low missingness (mode at $0.003$, $95$th percentile at $0.012$). There is a distinct cluster of poor-quality samples around $0.06-0.10$. A standard threshold is $0.02$ or $0.05$. A threshold of $0.02$ would remove the long tail of low-quality samples and the entire outlier cluster, while retaining over $95\\%$ of the individuals. This is a sound choice.\n\n2.  **Per-SNP missingness**: The situation is analogous to individual missingness. The vast majority of SNPs have low missingness (mode $0.002$, $95$th percentile $0.015$). A minor mode of problematic SNPs exists around $0.06$. A threshold of $0.02$ is reasonable, as it removes the long tail while retaining over $95\\%$ of SNPs and targets the problematic mode.\n\n3.  **Autosomal heterozygosity**: The data follows a normal distribution with $\\mu=0.315$ and $\\sigma=0.012$, with outlier clusters. We check the deviation of these clusters in units of standard deviation.\n    -   High-heterozygosity cluster at $0.36$: $z = (0.36 - 0.315) / 0.012 = 0.045 / 0.012 = 3.75$.\n    -   Low-heterozygosity cluster at $0.28$: $z = (0.28 - 0.315) / 0.012 = -0.035 / 0.012 \\approx -2.92$.\n    A threshold of mean $\\pm 3\\sigma$ corresponds to the interval $[0.315 - 3(0.012), 0.315 + 3(0.012)] = [0.279, 0.351]$. This is an excellent choice as it correctly identifies the high-heterozygosity individuals as outliers ($0.36 > 0.351$). It also correctly flags individuals at the fringe of the low-heterozygosity cluster ($0.28$ is close to the boundary of $0.279$). A $\\pm 3\\sigma$ rule is standard for identifying outliers in normally distributed data.\n\n4.  **Sex check**: The distribution of the inbreeding coefficient on the X chromosome, $F_X$, is clearly bimodal. For males, the mean is $0.95$ ($\\sigma=0.05$). For females, the mean is $0.02$ ($\\sigma=0.03$). Well-defined thresholds are needed to classify individuals and identify discordances. Let's consider intervals of $\\mu \\pm 3\\sigma$:\n    -   Male range: $[0.95 - 3(0.05), 0.95 + 3(0.05)] = [0.80, 1.10]$.\n    -   Female range: $[0.02 - 3(0.03), 0.02 + 3(0.03)] = [-0.07, 0.11]$.\n    Based on this, setting thresholds such that genetic males have $F_X \\ge 0.80$ and genetic females have $F_X \\le 0.20$ is a robust choice. The space between $0.20$ and $0.80$ serves as a buffer zone for ambiguous cases (e.g., sex chromosome aneuploidies). The problem notes there are reported females with $F_X > 0.85$ and reported males with $F_X < 0.15$. These thresholds would correctly identify them as sex-discordant, and they must be excluded.\n\n5.  **Relatedness**: GWAS requires a sample of (mostly) unrelated individuals. Duplicates, MZ twins, and first-degree relatives must be removed. It is also standard practice to remove second-degree relatives to ensure independence. The data shows a cluster of second-degree relatives with $\\hat{\\pi}$ in the range $[0.18, 0.25]$. A threshold of $\\hat{\\pi} > 0.185$ is well-chosen to remove pairs from this group and closer relationships (first-degree, duplicates). It is a data-driven threshold that targets the observed distribution.\n\n6.  **Hardy–Weinberg Equilibrium (HWE)**: HWE testing is a tool to detect genotyping errors. a) It must be performed in **controls only** in a case-control study, as a true association can cause HWE deviation in cases or in the combined sample. b) The $p$-value threshold should be stringent enough to remove likely errors but not so stringent that it is ineffective. The data shows an enrichment of SNPs with very low $p$-values ($1,200$ SNPs with $p < 10^{-6}$), which are also correlated with high missingness. This strongly suggests that these SNPs are artifacts. Therefore, a threshold of $p < 10^{-6}$ is well-justified by the data to specifically target this cluster of poor-quality SNPs. Applying a Bonferroni correction (e.g., $p < 0.05/700,000 \\approx 7 \\times 10^{-8}$) is inappropriate for a QC filter; it is too stringent and would fail to remove many problematic SNPs.\n\nBased on this analysis, the optimal protocol combines these principled and data-driven choices. I will now evaluate each option against this ideal protocol.\n\n### Option-by-Option Analysis\n\n**A. Individuals: exclude if missingness $> 0.02$; SNPs: exclude if missingness $> 0.02$; heterozygosity: flag and exclude autosomal outliers outside mean $\\pm 3$ standard deviations; sex check: declare genetic sex male if $F_X \\ge 0.80$ and female if $F_X \\le 0.20$, and exclude discordant or ambiguous individuals; relatedness: remove one sample from each pair with $\\hat{\\pi} > 0.185$; HWE: compute in controls and exclude SNPs with $p < 10^{-6}$.**\n\n- **Assessment:** Every single step in this option aligns perfectly with the optimal protocol derived from the empirical data and first principles. The thresholds for missingness, heterozygosity, sex check, relatedness, and HWE are all correct and well-justified by the provided numbers.\n- **Verdict:** **Correct**.\n\n**B. Individuals: exclude if missingness $> 0.005$; SNPs: exclude if missingness $> 0.005$; heterozygosity: exclude outside mean $\\pm 2$ standard deviations; sex check: declare genetic sex by $F_X \\ge 0.50$ for males and $F_X < 0.50$ for females; relatedness: remove only duplicates and first-degree relatives using $\\hat{\\pi} > 0.35$; HWE: compute in all samples and exclude SNPs with $p < 10^{-4}$.**\n\n- **Assessment:** This protocol is deeply flawed. The missingness ($>0.005$) and heterozygosity ($\\pm 2\\sigma$) thresholds are too stringent and would discard excessive amounts of valid data. The sex check threshold ($F_X=0.5$) is crude and less robust. The relatedness filter ($\\hat{\\pi} > 0.35$) is too lenient, retaining second-degree relatives. Critically, computing HWE in all samples is incorrect for a case-control study.\n- **Verdict:** **Incorrect**.\n\n**C. Individuals: exclude if missingness $> 0.08$; SNPs: exclude if missingness $> 0.08$; heterozygosity: exclude outside mean $\\pm 5$ standard deviations; sex check: skip X-based sex inference to maximize retention; relatedness: remove one from each pair with $\\hat{\\pi} > 0.10$; HWE: compute in controls and exclude SNPs with $p < 10^{-2}$.**\n\n- **Assessment:** This protocol is far too lenient, retaining low-quality data. Missingness thresholds of $>0.08$ would keep known outlier samples and SNPs. A $\\pm 5\\sigma$ heterozygosity rule is ineffective. Skipping the sex check is a major procedural error. The HWE threshold of $p < 10^{-2}$ is too accommodating. The relatedness threshold ($\\hat{\\pi} > 0.10$) is paradoxically too strict, removing more distant relatives than is standard.\n- **Verdict:** **Incorrect**.\n\n**D. Individuals: exclude if missingness $> 0.02$; SNPs: exclude if missingness $> 0.02$ when minor allele frequency (MAF) $\\ge 0.01$ but allow up to $0.05$ when MAF $< 0.01$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: use $F_X \\ge 0.80$ for males and $F_X \\le 0.20$ for females and remove discordant or ambiguous individuals; relatedness: remove one from each pair with $\\hat{\\pi} > 0.125$; HWE: compute in controls and exclude SNPs with $p < 10^{-6}$.**\n\n- **Assessment:** This is a strong protocol, very similar to A. However, it has two subtle weaknesses relative to A *in the context of the problem statement*. First, the MAF-dependent SNP missingness rule, while a good practice, introduces information (MAF) not provided in the empirical summaries, violating the instruction to use \"only the empirical summaries above\". Second, the relatedness threshold $\\hat{\\pi} > 0.125$ is a theoretical value for third-degree relatives, whereas the threshold $\\hat{\\pi} > 0.185$ in option A is better tailored to the observed data, cutting at the boundary of the second-degree relative cluster. Therefore, A is more directly supported by the problem's data.\n- **Verdict:** **Incorrect**.\n\n**E. Individuals: exclude if missingness $> 0.02$; SNPs: exclude if missingness $> 0.02$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: as in A; relatedness: remove one from each pair with $\\hat{\\pi} > 0.185$; HWE: compute in controls and exclude SNPs with $p < 7.1 \\times 10^{-8}$ by Bonferroni correction using $m = 700{,}000$.**\n\n- **Assessment:** This option is identical to A except for the HWE threshold. The use of a strict Bonferroni-corrected threshold ($p < 7.1 \\times 10^{-8}$) for HWE as a QC filter is conceptually incorrect. It is far too stringent and would fail to remove the cluster of $\\approx 1,200$ problematic SNPs observed at $p < 10^{-6}$, defeating the purpose of this QC step.\n- **Verdict:** **Incorrect**.\n\n### Conclusion\nOption A presents a complete and coherent QC protocol where every step is logically sound and every threshold is directly justified by the empirical data distributions provided in the problem statement. It represents the best balance between removing artifacts and retaining high-quality data.", "answer": "$$\\boxed{A}$$", "id": "2830645"}, {"introduction": "Once a high-quality dataset is prepared, the next step is to perform association testing using a statistical model that appropriately accounts for complex data structures like population stratification and cryptic relatedness. Linear mixed models (LMMs) have become the standard for this purpose, but their application is not without pitfalls. This practice delves into a subtle but critical artifact known as \"proximal contamination,\" which can deflate test statistics and reduce power when the genetic relatedness matrix includes the variant being tested. You will explore the statistical mechanism behind this issue and derive the now-standard Leave-One-Chromosome-Out (LOCO) strategy [@problem_id:2830658] that corrects this bias, ensuring the proper calibration of your association tests.", "problem": "In a genome-wide association study of a quantitative trait using a linear mixed model, you analyze $n$ individuals, with phenotype vector $y \\in \\mathbb{R}^{n}$. The standard linear mixed model writes the phenotype as the sum of fixed covariate effects, a single-variant effect under test, a polygenic background, and residual noise. The polygenic background is modeled as a mean-zero Gaussian with covariance proportional to a genetic relatedness (kinship) matrix computed from genome-wide single-nucleotide polymorphisms. Let $s \\in \\mathbb{R}^{n}$ denote the standardized genotype vector of a variant under test. The model is used both for null-hypothesis testing and for estimating the variant effect. A central requirement is that the association $p$-values are calibrated under the null hypothesis, that is, they have the correct distribution when the variant has no effect.\n\nStarting from well-tested facts about linear mixed models and properties of multivariate Gaussian vectors (for example, that conditioning and linear projections of Gaussian vectors remain Gaussian and that covariance defines the directions along which random effects can absorb signal), explain the mechanism of proximal contamination: when the kinship matrix is constructed from genome-wide markers that include the test variant (or variants in strong linkage disequilibrium on the same chromosome), the random polygenic effect absorbs part of the test-variant signal, deflating the test statistic and biasing effect estimates. Then, derive a principled modification to the kinship construction that breaks this absorption while preserving control of relatedness and population structure, and argue why this modification restores calibration of the test statistic under the null across the genome.\n\nWhich strategy below follows from your derivation and will preserve calibration while controlling relatedness and population structure in the presence of linkage disequilibrium?\n\nA. Use a single kinship matrix constructed from all markers once, including every variant, and use it for all tests.\n\nB. For each tested variant, recompute the kinship matrix after removing only that exact variant, keeping all other markers.\n\nC. For each tested variant on a given chromosome, recompute the kinship matrix after excluding all markers on that chromosome, and use that chromosome-specific exclusion for tests on that chromosome.\n\nD. Replace the mixed-model random effect by a small number of principal components from genome-wide markers and use ordinary least squares with those principal components as covariates for all tests.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   The study is a genome-wide association study (GWAS) of a quantitative trait.\n-   The number of individuals is $n$.\n-   The phenotype is represented by a vector $y \\in \\mathbb{R}^{n}$.\n-   The statistical model is a linear mixed model (LMM).\n-   The LMM for phenotype $y$ includes: fixed covariate effects, a single-variant effect, a polygenic background, and residual noise.\n-   The polygenic background is a random effect modeled as a mean-zero Gaussian with covariance proportional to a genetic relatedness (kinship) matrix, $K$.\n-   The kinship matrix $K$ is computed from genome-wide single-nucleotide polymorphisms (SNPs).\n-   The standardized genotype vector of a variant under test is denoted by $s \\in \\mathbb{R}^{n}$.\n-   A key requirement is the calibration of association $p$-values under the null hypothesis (i.e., they follow a uniform distribution).\n-   The problem posits the existence of \"proximal contamination\": when $K$ is constructed using markers that include the test variant or variants in strong linkage disequilibrium (LD), the random polygenic effect absorbs part of the test-variant signal.\n-   This absorption is stated to deflate the test statistic and bias effect estimates.\n-   The task is to explain this mechanism, derive a principled modification to the kinship construction to resolve it, and identify the correct strategy among the given options.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n\n-   **Scientifically Grounded:** The description of the linear mixed model is standard and accurate for applications in quantitative genetics and GWAS. The model structure, the use of a kinship matrix to model polygenic effects and control for population structure/relatedness, and the phenomenon of proximal contamination are all well-established, fundamental concepts in the field of statistical genetics. The problem is based on sound statistical and genetic principles.\n-   **Well-Posed:** The problem is clearly defined. It requests an explanation of a known statistical artifact and the derivation of a standard, accepted solution. The question is structured to lead to a unique conceptual conclusion that is widely implemented in modern GWAS software.\n-   **Objective:** The language is technical, precise, and devoid of subjectivity or ambiguity. Terms such as \"linear mixed model,\" \"kinship matrix,\" \"linkage disequilibrium,\" and \"p-value calibration\" are standard and have unambiguous meanings in this context.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-formulated question concerning a critical methodological issue in statistical genetics. A solution will be derived.\n\n### Derivation of Solution\n\nThe standard linear mixed model for a quantitative trait $y$ in a sample of $n$ individuals is given by:\n$$\ny = X\\beta + s\\gamma + u + \\epsilon\n$$\nwhere:\n-   $y$ is the $n \\times 1$ vector of phenotype values.\n-   $X$ is an $n \\times c$ matrix of $c$ fixed-effect covariates (e.g., age, sex, and principal components for ancestry).\n-   $\\beta$ is the $c \\times 1$ vector of corresponding effect sizes for the covariates.\n-   $s$ is the $n \\times 1$ standardized genotype vector for the SNP being tested.\n-   $\\gamma$ is the scalar fixed effect of the test SNP, which is the parameter of interest.\n-   $u$ is the $n \\times 1$ random vector representing the aggregated polygenic effect from all other variants across the genome. It is modeled as a draw from a multivariate normal distribution, $u \\sim \\mathcal{N}(0, \\sigma_g^2 K)$, where $\\sigma_g^2$ is the polygenic variance component and $K$ is the $n \\times n$ genetic relatedness matrix (kinship).\n-   $\\epsilon$ is the $n \\times 1$ random vector of non-genetic and environmental residuals, modeled as $\\epsilon \\sim \\mathcal{N}(0, \\sigma_e^2 I)$, where $\\sigma_e^2$ is the residual variance and $I$ is the $n \\times n$ identity matrix.\n\nThe null hypothesis for association testing is $H_0: \\gamma = 0$. The test statistic, typically from a likelihood ratio test or score test, evaluates the significance of including the term $s\\gamma$ in the model. For this test to be valid, the test statistic must follow its theoretical null distribution (e.g., a $\\chi^2_1$ distribution) when $H_0$ is true.\n\n**Mechanism of Proximal Contamination**\n\nThe kinship matrix $K$ is usually estimated from a large set of $M$ genome-wide markers. A standard estimator is $K = \\frac{1}{M} \\sum_{j=1}^M s_j s_j^T$, where $s_j$ is the standardized genotype vector for the $j$-th marker.\n\nThe problem arises when the set of markers $\\{s_j\\}_{j=1}^M$ used to construct $K$ includes the test SNP $s$ or SNPs in high linkage disequilibrium (LD) with $s$. Let the test SNP be $s_k$ for some index $k$.\n\nBy including $s_k$ in the sum to create $K$, we are explicitly including the term $\\frac{1}{M} s_k s_k^T$ in the covariance structure of the random effect $u$. This implies that the model assumes, a priori, that any phenotypic variance that projects onto the direction of the vector $s_k$ can be part of the random polygenic background.\n\nWhen we then test for a *fixed* effect $\\gamma$ for this same SNP $s_k$, we create a conflict. The model must partition the phenotypic signal associated with $s_k$ between the fixed effect term $s_k\\gamma$ and the random effect term $u$. Because the covariance of $u$ is explicitly designed to include variation along $s_k$, the random effect $u$ will \"absorb\" or \"soak up\" a portion of the true fixed-effect signal. This is a classic case of confounding between a fixed effect and a random effect when their defining vectors are not orthogonal.\n\nThis absorption has two main consequences:\n1.  **Biased Effect Estimate:** The estimate $\\hat{\\gamma}$ will be biased towards zero because part of the effect is incorrectly attributed to the random term $u$.\n2.  **Deflated Test Statistic:** The test statistic for $\\gamma$ will be smaller than it should be, leading to a loss of statistical power. Under the null hypothesis, this can also cause a subtle but systematic deflation of the test statistic across the genome, leading to miscalibration (i.e., the $p$-values are systematically larger than expected under the null). This is because even under the null for the tested SNP, LD with true causal variants nearby creates a local signal that gets absorbed by the contaminated random effect, again deflating the test statistic.\n\nThe same problem occurs if we test SNP $s_k$ but the kinship matrix $K$ includes a different SNP $s_j$ that is in high LD with $s_k$. In this case, their genotype vectors are highly correlated ($s_k \\approx \\rho s_j$), so including $s_j$ in the construction of $K$ provides a vector in the covariance of $u$ that is nearly collinear with the fixed effect vector $s_k$ being tested, leading to the same absorption phenomenon. This is \"proximal contamination\" because the contamination is sourced from markers genetically close (and thus in LD with) the test marker.\n\n**Principled Modification to Kinship Construction**\n\nTo eliminate this confounding, we must ensure that the random effect $u$ and the fixed effect $s\\gamma$ are not defined using overlapping information. The vector $s$ being tested for a fixed effect must be orthogonal to the space of variation defined for the random polygenic background.\n\nA direct way to achieve this is to construct the kinship matrix $K$ using only markers that are not in LD with the test marker $s$. Due to the nature of meiosis and recombination, markers on different chromosomes are, in general, not in LD (except for long-range correlations induced by population structure, which is what the LMM is designed to capture globally). Within a chromosome, LD is strong for nearby markers and decays with distance.\n\nTherefore, a principled and robust strategy is to partition the genome by chromosome. When testing any SNP on a specific chromosome, say chromosome $i$, we should model the polygenic background using a kinship matrix constructed *exclusively* from markers on all other chromosomes.\n\nLet $K_{(-i)}$ be the kinship matrix computed from all SNPs *not* on chromosome $i$. When testing a SNP $s$ located on chromosome $i$, the model becomes:\n$$\ny = X\\beta + s\\gamma + u_{(-i)} + \\epsilon\n$$\nwhere $u_{(-i)} \\sim \\mathcal{N}(0, \\sigma_g^2 K_{(-i)})$.\n\nThis approach, known as Leave-One-Chromosome-Out (LOCO), ensures that the fixed effect being tested, $s\\gamma$, is not confounded with the random effect $u_{(-i)}$. The random effect models the background from chromosomes $1, 2, ..., i-1, i+1, ...$, while the fixed effect models the local signal on chromosome $i$. This correctly separates the test signal from the background model, breaking the mechanism of proximal contamination. This restores the calibration of test statistics and provides unbiased effect estimates for variants on chromosome $i$, while still controlling for population structure and polygenic background arising from the rest of the genome. This procedure is repeated for each chromosome.\n\n### Evaluation of Options\n\n**A. Use a single kinship matrix constructed from all markers once, including every variant, and use it for all tests.**\nThis is the standard approach that *causes* proximal contamination. As derived above, including the test variant (or variants in LD) in the construction of the kinship matrix leads to deflated test statistics and biased estimates. This strategy fails to preserve calibration.\n**Verdict: Incorrect**\n\n**B. For each tested variant, recompute the kinship matrix after removing only that exact variant, keeping all other markers.**\nThis modification is insufficient. While it removes the direct inclusion of the test variant $s$, it fails to account for LD. Markers immediately adjacent to $s$ and in high LD with it will remain in the kinship calculation. Their high correlation with $s$ means the random effect will still absorb the signal from $s$, and proximal contamination will persist. This approach is also computationally prohibitive, requiring a new kinship matrix calculation for every SNP test.\n**Verdict: Incorrect**\n\n**C. For each tested variant on a given chromosome, recompute the kinship matrix after excluding all markers on that chromosome, and use that chromosome-specific exclusion for tests on that chromosome.**\nThis is the Leave-One-Chromosome-Out (LOCO) strategy derived above. It correctly separates the signal being tested (on one chromosome) from the polygenic background model (built from all other chromosomes), thereby eliminating proximal contamination. It preserves control of relatedness and population structure because these are genome-wide properties captured by the remaining chromosomes. This strategy restores test statistic calibration.\n**Verdict: Correct**\n\n**D. Replace the mixed-model random effect by a small number of principal components from genome-wide markers and use ordinary least squares with those principal components as covariates for all tests.**\nThis describes an alternative modeling strategy (PCA + OLS), not a modification to the LMM's kinship construction. While using principal components as covariates controls for population structure, it is well-known to be less powerful than an LMM when a substantial polygenic component to the trait exists, as it does not explicitly model the covariance due to cryptic relatedness and the thousands of small-effect variants. The problem asks for a principled modification *to the kinship construction within the LMM framework*, not for a replacement of the model. Furthermore, OLS with PCs can suffer from its own forms of test statistic inflation when relatedness is not fully captured. This option does not follow from the derivation.\n**Verdict: Incorrect**", "answer": "$$\\boxed{C}$$", "id": "2830658"}, {"introduction": "Modern genetic studies increasingly rely on next-generation sequencing to probe the role of rare variants, which present unique analytical challenges due to their low frequencies and the inherent uncertainty in genotype calls from sequencing data. A common but suboptimal practice is to use \"hard-called\" genotypes, which discards valuable probabilistic information. This final hands-on practice guides you through implementing a simulation to quantify the benefits of a more sophisticated approach: integrating genotype likelihoods to produce \"soft\" dosage estimates. By comparing the accuracy of hard calls versus integrated dosages [@problem_id:2830588], you will gain a practical understanding of how retaining uncertainty improves estimation and is crucial for the power and calibration of rare variant association studies.", "problem": "You are asked to implement a self-contained simulation and analysis to evaluate how integrating genotype likelihoods improves calibration relative to hard genotype calls in the context of rare variant detection for Quantitative Trait Loci (QTL) mapping and Genome-Wide Association Studies (GWAS). Your program must simulate next-generation sequencing data with realistic coverage variation and base-calling errors, then compute and compare calibration under two estimators of genotype dosage.\n\nFundamental base and assumptions to use:\n- Hardy–Weinberg Equilibrium (HWE): Given a minor allele frequency $p \\in (0,1)$ in a diploid organism, the genotype $G \\in \\{0,1,2\\}$ has prior probabilities $P(G=0) = (1-p)^2$, $P(G=1) = 2p(1-p)$, and $P(G=2) = p^2$ under random mating.\n- Binomial sampling of reads: Given total coverage $N \\in \\{0,1,2,\\dots\\}$ and per-read alternate-allele probability $q \\in [0,1]$ implied by the true genotype and sequencing error, the alternate read count $K$ is distributed as $K \\sim \\mathrm{Binomial}(N,q)$.\n- Overdispersed coverage: To model coverage variation across samples and loci, assume $N$ follows a Negative Binomial distribution with mean $\\mu > 0$ and integer dispersion parameter $r \\in \\{1,2,3,\\dots\\}$, parameterized so that $\\mathbb{E}[N] = \\mu$ and $\\mathrm{Var}[N] = \\mu + \\mu^2/r$.\n- Symmetric base-calling error: Let the per-base error rate be $\\varepsilon \\in (0,1/2)$, and assume independence across reads. Under a binary distinction of the alternate allele versus any non-alternate call, the probability that a read reports an alternate allele is\n  - $q = \\varepsilon$ when $G=0$,\n  - $q = 1/2$ when $G=1$,\n  - $q = 1-\\varepsilon$ when $G=2$.\n- Genotype likelihood and Bayes’ rule: For an observed pair $(N,K)$ at a locus and sample, the genotype likelihood satisfies $P(D \\mid G=g) \\propto q_g^K (1-q_g)^{N-K}$ (up to a combinatorial factor that cancels in normalization), and the posterior obeys $P(G=g \\mid D) \\propto P(D \\mid G=g) P(G=g)$.\n\nTargets to compute:\n- Two estimators of dosage (minor-allele count):\n  - Hard call dosage $\\hat{d}_{\\mathrm{hard}}$: the mode of the posterior, mapped to $\\{0,1,2\\}$.\n  - Integrated dosage $\\hat{d}_{\\mathrm{soft}}$: the posterior mean $\\sum_{g \\in \\{0,1,2\\}} g \\, P(G=g \\mid D)$.\n- Let the true dosage be $d_{\\mathrm{true}} \\in \\{0,1,2\\}$. Define the mean squared error under each estimator as the average of $(\\hat{d} - d_{\\mathrm{true}})^2$ over all samples and loci. Define the improvement as\n$$\n\\Delta = \\mathrm{MSE}(\\hat{d}_{\\mathrm{hard}}) - \\mathrm{MSE}(\\hat{d}_{\\mathrm{soft}}).\n$$\nYour program must output $\\Delta$ for each test case.\n\nSimulation design to implement:\n- For each test case, simulate $L$ independent loci with the same minor allele frequency $p$, and $N_{\\mathrm{samples}}$ independent diploid individuals per locus.\n- For each individual and locus, draw the true genotype $G \\sim \\mathrm{Binomial}(2,p)$ (which satisfies HWE), then draw coverage $N \\sim \\mathrm{NegBin}(r, \\mu)$ with mean $\\mu$ and dispersion $r$, then draw alternate reads $K \\sim \\mathrm{Binomial}(N, q_G)$ using the symmetric error model above with error rate $\\varepsilon$.\n- Compute posterior probabilities $P(G=g \\mid D)$ for $g \\in \\{0,1,2\\}$ using Bayes’ rule with HWE priors and the binomial likelihood; then compute $\\hat{d}_{\\mathrm{soft}}$ and $\\hat{d}_{\\mathrm{hard}}$, and finally compute $\\Delta$ as defined above.\n\nPrinciple to be demonstrated:\n- Show empirically, across the test suite, that using $\\hat{d}_{\\mathrm{soft}}$ reduces squared error relative to $\\hat{d}_{\\mathrm{hard}}$ (that is, $\\Delta \\ge 0$), thereby improving calibration.\n\nImplementation requirements:\n- Use a fixed pseudorandom seed $1337$ for reproducibility.\n- Do not assume any external files; everything must be simulated in-memory.\n- Avoid undefined behavior at zero coverage; when $N=0$, the posterior reduces to the prior.\n\nTest suite:\nImplement exactly the following four test cases, each as a tuple $(N_{\\mathrm{samples}}, L, p, \\mu, r, \\varepsilon)$:\n- Case $1$: $(2000, 50, 0.01, 12.0, 5, 0.005)$\n- Case $2$: $(2000, 50, 0.005, 3.0, 3, 0.01)$\n- Case $3$: $(4000, 80, 0.001, 8.0, 4, 0.02)$\n- Case $4$: $(3000, 40, 0.0005, 1.0, 2, 0.02)$\n\nWhat to output:\n- For each case, compute $\\Delta$ as a decimal number. Report the results rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example: \"[0.012345,0.000678,0.001234,0.000056]\".", "solution": "The problem statement has been rigorously validated and is determined to be well-posed, scientifically grounded, and internally consistent. It presents a standard, albeit simplified, model from statistical genetics and asks for a computational analysis of estimator performance. We will now proceed with a formal solution.\n\nThe objective is to quantify the improvement in genotype dosage estimation by using a posterior mean estimator (soft call) over a posterior mode estimator (hard call). This improvement is measured by the reduction in Mean Squared Error (MSE), denoted as $\\Delta = \\mathrm{MSE}(\\hat{d}_{\\mathrm{hard}}) - \\mathrm{MSE}(\\hat{d}_{\\mathrm{soft}})$. The context is the analysis of rare variants from next-generation sequencing data, where low allele frequencies and variable read coverage introduce significant uncertainty.\n\nThe analysis is conducted through a Monte Carlo simulation. For each test case defined by parameters $(N_{\\mathrm{samples}}, L, p, \\mu, r, \\varepsilon)$, we simulate a total of $N_{\\mathrm{total}} = N_{\\mathrm{samples}} \\times L$ independent loci for a diploid population.\n\n**1. Data Simulation**\n\nThe simulation for each of the $N_{\\mathrm{total}}$ data points proceeds in three steps, emulating the biological and technical processes:\n\n- **True Genotype Generation:** The true genotype $G$, representing the count of the minor allele, is drawn for each individual at each locus. Under the assumption of Hardy-Weinberg Equilibrium (HWE), the genotype follows a binomial distribution with $2$ trials (for a diploid organism) and a success probability equal to the Minor Allele Frequency (MAF), $p$.\n$$\nd_{\\mathrm{true}} = G \\sim \\mathrm{Binomial}(2, p)\n$$\n\n- **Sequencing Coverage Simulation:** Read coverage $N$ is variable across individuals and loci. This overdispersion is modeled using a Negative Binomial distribution, parameterized by its mean $\\mu$ and an integer dispersion parameter $r$. The probability mass function is that for which $\\mathbb{E}[N] = \\mu$ and $\\mathrm{Var}[N] = \\mu + \\mu^2/r$. This corresponds to a standard parameterization with number of successes $n=r$ and success probability $p_{\\mathrm{NB}} = r / (r+\\mu)$.\n$$\nN \\sim \\mathrm{NegativeBinomial}(n=r, p=r/(r+\\mu))\n$$\n\n- **Alternate Read Count Simulation:** Given the true genotype $G$ and total coverage $N$, the number of reads $K$ supporting the alternate allele is sampled from a Binomial distribution. The probability $q_G$ of observing an alternate allele on any given read depends on the true genotype $G$ and the symmetric base-calling error rate $\\varepsilon$.\n$$\nK \\sim \\mathrm{Binomial}(N, q_G)\n$$\nwhere the per-read alternate-allele probability $q_G$ is given by:\n$$\nq_G = \\begin{cases} \\varepsilon & \\text{if } G=0 \\\\ 1/2 & \\text{if } G=1 \\\\ 1-\\varepsilon & \\text{if } G=2 \\end{cases}\n$$\n\n**2. Bayesian Inference of Genotype**\n\nFor each simulated data point $(N, K)$, we apply Bayes' theorem to compute the posterior probability of each possible genotype $g \\in \\{0, 1, 2\\}$. The posterior probability is proportional to the product of the likelihood and the prior probability.\n\n- **Prior Probabilities:** The prior belief about the genotype is given by the HWE probabilities derived from the population's MAF, $p$:\n$$\nP(G=g) = \\begin{cases} (1-p)^2 & \\text{if } g=0 \\\\ 2p(1-p) & \\text{if } g=1 \\\\ p^2 & \\text{if } g=2 \\end{cases}\n$$\n\n- **Likelihood Function:** The likelihood of observing $K$ alternate reads out of $N$ total reads, given a true genotype $g$, is given by the Binomial probability mass function:\n$$\nP(D \\mid G=g) = \\binom{N}{K} q_g^K (1-q_g)^{N-K}\n$$\nwhere $D$ denotes the data $(N,K)$.\n\n- **Posterior Probabilities:** The posterior probability for genotype $g$ is then:\n$$\nP(G=g \\mid D) = \\frac{P(D \\mid G=g) P(G=g)}{\\sum_{i=0}^{2} P(D \\mid G=i) P(G=i)}\n$$\nTo ensure numerical stability, computations are performed in log-space. The log-posterior is calculated as:\n$$\n\\log P(G=g \\mid D) = \\left( \\log P(D \\mid G=g) + \\log P(G=g) \\right) - \\mathrm{logsumexp}_{i=0}^{2} \\left( \\log P(D \\mid G=i) + \\log P(G=i) \\right)\n$$\nThe special case of zero coverage ($N=0$) is handled naturally by this formulation. If $N=0$, then $K=0$, and the likelihood term $P(D \\mid G=g)$ becomes $1$ for all $g$, causing the posterior probabilities to revert to the prior probabilities, as is correct.\n\n**3. Dosage Estimation and Error Calculation**\n\nFrom the posterior distribution $P(G=g \\mid D)$, we compute two distinct estimators for the dosage:\n\n- **Hard Call Dosage ($\\hat{d}_{\\mathrm{hard}}$):** This is the Maximum A Posteriori (MAP) estimate, which corresponds to the genotype with the highest posterior probability.\n$$\n\\hat{d}_{\\mathrm{hard}} = \\underset{g \\in \\{0,1,2\\}}{\\mathrm{argmax}} \\, P(G=g \\mid D)\n$$\nThis estimator discards all information about uncertainty.\n\n- **Integrated Dosage ($\\hat{d}_{\\mathrm{soft}}$):** This is the posterior mean of the genotype distribution, which integrates over the uncertainty.\n$$\n\\hat{d}_{\\mathrm{soft}} = \\mathbb{E}[G \\mid D] = \\sum_{g=0}^{2} g \\cdot P(G=g \\mid D)\n$$\nFrom Bayesian decision theory, the posterior mean is the optimal estimator that minimizes the Mean Squared Error (MSE).\n\nFinally, the MSE for each estimator is calculated by averaging the squared difference between the estimate and the true simulated dosage, $d_{\\mathrm{true}}$, across all $N_{\\mathrm{total}}$ simulations.\n$$\n\\mathrm{MSE}(\\hat{d}) = \\frac{1}{N_{\\mathrm{total}}} \\sum_{j=1}^{N_{\\mathrm{total}}} (\\hat{d}_j - d_{\\mathrm{true}, j})^2\n$$\nThe final metric, $\\Delta = \\mathrm{MSE}(\\hat{d}_{\\mathrm{hard}}) - \\mathrm{MSE}(\\hat{d}_{\\mathrm{soft}})$, is computed. We expect $\\Delta \\ge 0$, demonstrating that the integrated dosage estimator is better calibrated in terms of squared error.\n\nThe entire process is implemented using a fixed pseudorandom seed of $1337$ to ensure reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import nbinom, binom\nfrom scipy.special import logsumexp\n\ndef compute_delta_for_case(n_samples, l_loci, p_maf, mu, r, epsilon, rng):\n    \"\"\"\n    Simulates sequencing data and computes the improvement in MSE from using\n    soft vs. hard genotype calls.\n\n    Args:\n        n_samples (int): Number of diploid individuals.\n        l_loci (int): Number of independent loci.\n        p_maf (float): Minor allele frequency.\n        mu (float): Mean coverage.\n        r (int): Dispersion parameter for coverage.\n        epsilon (float): Symmetric base-calling error rate.\n        rng (np.random.Generator): A numpy random number generator instance.\n\n    Returns:\n        float: The calculated improvement, Delta = MSE(hard) - MSE(soft).\n    \"\"\"\n\n    n_total = n_samples * l_loci\n    \n    # 1. Priors and Genotype-dependent probabilities\n    # HWE prior probabilities P(G=g) for g in {0, 1, 2}\n    prior_probs = np.array([(1 - p_maf)**2, 2 * p_maf * (1 - p_maf), p_maf**2])\n    log_priors = np.log(prior_probs)\n\n    # Per-read alternate allele probability q_g, given genotype g\n    q_vec = np.array([epsilon, 0.5, 1 - epsilon])\n\n    # 2. Simulation steps\n    # Step 2.1: Simulate true genotypes (dosages) from Binomial(2, p)\n    # This correctly models diploid genotypes under HWE.\n    d_true = rng.binomial(2, p_maf, size=n_total)\n\n    # Step 2.2: Simulate overdispersed read coverage N from Negative Binomial\n    # Parameterize for scipy.stats.nbinom using mean (mu) and dispersion (r)\n    p_nb = r / (r + mu)\n    # n_nb = r\n    N = nbinom.rvs(n=r, p=p_nb, size=n_total, random_state=rng)\n    \n    # Step 2.3: Simulate alternate read counts K from Binomial(N, q_G)\n    # The probability of success q_G depends on the true genotype G.\n    q_true = q_vec[d_true]\n    K = rng.binomial(N, q_true)\n\n    # 3. Bayesian Inference\n    # Calculate log-likelihoods P(K|N, G=g) for each g in {0, 1, 2}\n    # This creates a (3, N_total) array where rows correspond to G=0, 1, 2.\n    # The calculation is vectorized over all N_total data points.\n    log_likelihoods = np.zeros((3, n_total))\n    for g in range(3):\n        # binom.logpmf is numerically stable and handles N=0 cases correctly.\n        log_likelihoods[g, :] = binom.logpmf(K, N, q_vec[g])\n\n    # Combine with log-priors to get unnormalized log-posteriors\n    # Broadcasting log_priors of shape (3,1) over log_likelihoods of shape (3, n_total)\n    log_unnorm_posterior = log_likelihoods + log_priors[:, np.newaxis]\n\n    # Normalize using the log-sum-exp trick to get log-posteriors\n    log_marginal_likelihood = logsumexp(log_unnorm_posterior, axis=0)\n    log_posterior = log_unnorm_posterior - log_marginal_likelihood\n    \n    # Convert back to linear scale for posterior probabilities\n    posterior_probs = np.exp(log_posterior)\n\n    # 4. Compute Dosage Estimators\n    # Hard call dosage (MAP estimate)\n    d_hat_hard = np.argmax(posterior_probs, axis=0)\n\n    # Soft call dosage (posterior mean)\n    genotypes = np.array([0, 1, 2])\n    d_hat_soft = genotypes @ posterior_probs\n\n    # 5. Calculate Mean Squared Error and Delta\n    mse_hard = np.mean((d_hat_hard - d_true)**2)\n    mse_soft = np.mean((d_hat_soft - d_true)**2)\n    \n    delta = mse_hard - mse_soft\n    return delta\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    # Initialize a single random number generator for reproducibility.\n    rng = np.random.default_rng(1337)\n\n    # Define the test cases from the problem statement:\n    # (N_samples, L, p, mu, r, epsilon)\n    test_cases = [\n        (2000, 50, 0.01, 12.0, 5, 0.005),\n        (2000, 50, 0.005, 3.0, 3, 0.01),\n        (4000, 80, 0.001, 8.0, 4, 0.02),\n        (3000, 40, 0.0005, 1.0, 2, 0.02),\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = compute_delta_for_case(*case, rng=rng)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```", "id": "2830588"}]}