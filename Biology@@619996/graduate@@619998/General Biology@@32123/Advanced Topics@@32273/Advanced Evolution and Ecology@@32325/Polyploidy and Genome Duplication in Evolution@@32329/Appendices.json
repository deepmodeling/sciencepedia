{"hands_on_practices": [{"introduction": "The evolutionary journey of a polyploid lineage begins with a singular event: the formation of the first polyploid individual within a diploid population. This often occurs through the fusion of \"unreduced\" gametes, which have bypassed meiotic reduction. This exercise [@problem_id:2825719] uses first principles of probability, analogous to the Hardy-Weinberg framework, to model the frequency of such an event. Mastering this calculation provides a quantitative foundation for understanding how the rate of meiotic error, $r$, governs the initial input of polyploids into a population, transforming a key biological process into a simple, predictable mathematical model.", "problem": "In a large, randomly mating diploid population undergoing no selection, assume that each meiosis produces an unreduced gamete of ploidy $2n$ with probability $r$ and a reduced gamete of ploidy $n$ with probability $1 - r$. Assume that this probability $r$ is the same for gametes contributed by all individuals and, if there are separate sexes, is equal in the two sexes. Mating occurs by random union of gametes (as in Hardy–Weinberg Equilibrium (HWE)), and all formed zygotes are equally viable at the moment of formation.\n\nUsing only the principles that (i) meiosis in diploids normally produces haploid gametes and may produce unreduced gametes at a per-gamete probability $r$, and (ii) random mating corresponds to random union of gametes so that joint events factor as products of marginal probabilities, derive from first principles the expected frequency (as a function of $r$) of newly formed tetraploid ($4n$) zygotes (neo-tetraploids) among all zygotes formed in a single generation.\n\nExpress your final answer as a closed-form analytic expression in terms of $r$. Do not round. No units are required.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\nThe givens are as follows:\n1.  The population is diploid ($2n$), large, and undergoes random mating.\n2.  There is no selection.\n3.  During meiosis, an individual produces unreduced, diploid ($2n$) gametes with probability $r$.\n4.  During meiosis, an individual produces reduced, haploid ($n$) gametes with probability $1-r$.\n5.  The probability $r$ is constant for all individuals and sexes.\n6.  Mating is by random union of gametes.\n7.  All zygotes formed are equally viable.\n8.  The task is to derive the expected frequency of newly formed tetraploid ($4n$) zygotes as a function of $r$.\n\nThe problem is evaluated against the validation criteria:\n-   **Scientific Grounding**: The problem is well-grounded in the principles of population genetics and evolutionary biology. The formation of polyploids via unreduced gametes is a known and fundamental mechanism, particularly in plants. The use of a Hardy-Weinberg-like framework (random union of gametes) is a standard and appropriate modeling approach. The problem does not violate any scientific laws or contain factual inaccuracies.\n-   **Well-Posedness**: The problem is well-posed. It clearly defines the initial state (a diploid population), the process (gamete formation with a specified probability of non-reduction), and the outcome to be calculated (frequency of a specific zygote type). All necessary parameters ($r$) are provided, and the question leads to a unique, derivable solution.\n-   **Objectivity**: The problem is stated using precise, objective, and unambiguous mathematical language. There are no subjective or opinion-based elements.\n\nThe verdict is that the problem is **valid**. It is a standard, solvable problem in theoretical population genetics. We may now proceed to the solution.\n\nThe solution is derived from first principles as requested. The population of diploid ($2n$) individuals produces a common gamete pool. The composition of this pool is determined by the probability of producing either reduced or unreduced gametes.\n\nLet $P(n)$ be the frequency of reduced, haploid ($n$) gametes in the pool.\nLet $P(2n)$ be the frequency of unreduced, diploid ($2n$) gametes in the pool.\nAccording to the problem statement:\n$$P(n) = 1 - r$$\n$$P(2n) = r$$\nThe sum of these frequencies is $(1-r) + r = 1$, as required.\n\nThe principle of random mating is stated to be equivalent to the random union of gametes. This implies that the formation of a zygote can be modeled as two independent random draws from the gamete pool. The probability of any specific combination of two gametes is the product of their individual frequencies in the pool.\n\nA zygote's ploidy is the sum of the ploidies of the two gametes that fuse to form it. We seek the frequency of newly formed tetraploid ($4n$) zygotes. A tetraploid zygote can only be formed by the fusion of two diploid ($2n$) gametes.\n\nLet $Z_{4n}$ denote the event that a newly formed zygote is tetraploid. This event occurs if and only if the first randomly selected gamete has ploidy $2n$ AND the second randomly selected gamete has ploidy $2n$.\n\nThe probability of this event, which corresponds to the expected frequency in a large population, is calculated as follows:\n$$P(Z_{4n}) = P(\\text{gamete 1 is } 2n) \\times P(\\text{gamete 2 is } 2n)$$\nSince the selection of the two gametes are independent events, we can substitute the frequency of $2n$ gametes for each probability:\n$$P(Z_{4n}) = P(2n) \\times P(2n)$$\n$$P(Z_{4n}) = r \\times r = r^2$$\n\nThis is the frequency of neo-tetraploids among all zygotes formed in a single generation.\n\nTo confirm the logical consistency of this framework, we can calculate the frequencies of all possible zygote types and verify that they sum to $1$.\n1.  **Diploid ($2n$) Zygotes:** Formed by the fusion of two haploid ($n$) gametes.\n    Frequency = $P(n) \\times P(n) = (1-r)(1-r) = (1-r)^2$.\n2.  **Triploid ($3n$) Zygotes:** Formed by the fusion of one haploid ($n$) gamete and one diploid ($2n$) gamete. This can occur in two ways (first gamete is $n$ and second is $2n$, or vice versa).\n    Frequency = $[P(n) \\times P(2n)] + [P(2n) \\times P(n)] = (1-r)r + r(1-r) = 2r(1-r)$.\n3.  **Tetraploid ($4n$) Zygotes:** Formed by the fusion of two diploid ($2n$) gametes.\n    Frequency = $P(2n) \\times P(2n) = r \\times r = r^2$.\n\nThe sum of all zygote frequencies is:\n$$ \\text{Total Frequency} = (1-r)^2 + 2r(1-r) + r^2 $$\nThis is the binomial expansion of $((1-r) + r)^2$.\n$$ \\text{Total Frequency} = (1 - 2r + r^2) + (2r - 2r^2) + r^2 $$\n$$ \\text{Total Frequency} = 1 - 2r + 2r + r^2 - 2r^2 + r^2 = 1 $$\nThe frequencies sum to $1$, which confirms the derivation is self-consistent and correct. The expected frequency of newly formed tetraploid ($4n$) zygotes is therefore $r^2$.", "answer": "$$\\boxed{r^{2}}$$", "id": "2825719"}, {"introduction": "Once a polyploid population becomes established, its evolutionary dynamics diverge from its diploid progenitors, particularly concerning the impact of random genetic drift. The strength of drift is inversely proportional to the effective population size, $N_e$, which scales with the total number of gene copies in the breeding pool. Because autotetraploids carry four gene copies per individual at each locus, their gene pool is significantly larger than a diploid's for the same census size. This practice [@problem_id:2825710] guides you through the derivation of $N_e$ for both ploidy levels, revealing the fundamental consequence that polyploidy dampens the effects of genetic drift, a crucial insight for understanding the unique evolutionary trajectories and adaptive landscapes of polyploid species.", "problem": "A neutral biallelic locus with alleles $A$ and $a$ segregates at frequency $p \\in (0,1)$ in an idealized monoecious population of constant census size $N$ individuals and discrete, nonoverlapping generations. Assume a Wright–Fisher reproduction scheme with random mating, equal sex function, and no selection or migration. Consider two cases that share the same census size $N$ and the same mating system:\n\n- Case $1$: A diploid population with disomic inheritance.\n- Case $2$: An autotetraploid population with tetrasomic inheritance and no double reduction.\n\nDefine the variance effective population size $N_e$ by the condition that the one-generation variance in allele frequency due to genetic drift satisfies\n$$\n\\operatorname{Var}(p_{t+1}\\mid p_t=p) \\;=\\; \\frac{p\\,(1-p)}{2\\,N_e}.\n$$\nStarting only from the Wright–Fisher sampling interpretation at the level of gene copies and the above definition of $N_e$, derive $N_e$ for each case by expressing $\\operatorname{Var}(p_{t+1}\\mid p_t=p)$ in terms of $N$ and the per-copy sampling process appropriate to each ploidy level and inheritance mode. Then, compare the two cases by giving a closed-form expression for the ratio\n$$\nR \\;=\\; \\frac{N_e^{(4x)}}{N_e^{(2x)}},\n$$\nwhere $N_e^{(2x)}$ and $N_e^{(4x)}$ denote the effective population sizes in the diploid and autotetraploid cases, respectively. Provide the exact value of $R$ as your final answer. No rounding is required, and no units should be reported.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard problem in theoretical population genetics that requires the application of first principles of the Wright–Fisher model to populations of differing ploidy. We shall proceed with its solution.\n\nThe problem defines the variance effective population size, $N_e$, through the relationship for the one-generation variance in allele frequency, $p$:\n$$\n\\operatorname{Var}(p_{t+1}\\mid p_t=p) = \\frac{p(1-p)}{2N_e}\n$$\nOur task is to derive an expression for this variance starting from the Wright–Fisher model for each case (diploid and autotetraploid) and then equate it to the given definition to find the respective $N_e$.\n\nThe Wright–Fisher model posits that the gene copies at a locus in generation $t+1$ are obtained by sampling with replacement from the gene pool of generation $t$. Let the total number of gene copies at the locus in the population be $K$. In a population of census size $N$ and ploidy $k$, the total number of gene copies is $K = kN$.\n\nIf the frequency of allele $A$ in generation $t$ is $p_t = p$, then the number of $A$ alleles in the next generation, $X_{t+1}$, is a random variable that follows a binomial distribution. Specifically, we are drawing $K$ gene copies, and the probability of drawing an $A$ allele in any single draw is $p$. Thus, $X_{t+1} \\sim \\operatorname{Binomial}(K, p)$.\n\nThe allele frequency in generation $t+1$ is $p_{t+1} = \\frac{X_{t+1}}{K}$. The variance of a binomially distributed random variable $X \\sim \\operatorname{Binomial}(n, \\pi)$ is $\\operatorname{Var}(X) = n\\pi(1-\\pi)$. Therefore, the variance of the allele frequency in the next generation is:\n$$\n\\operatorname{Var}(p_{t+1}\\mid p_t=p) = \\operatorname{Var}\\left(\\frac{X_{t+1}}{K}\\right) = \\frac{1}{K^2}\\operatorname{Var}(X_{t+1}) = \\frac{Kp(1-p)}{K^2} = \\frac{p(1-p)}{K}\n$$\nThis general formula, $\\operatorname{Var}(p_{t+1}\\mid p_t=p) = \\frac{p(1-p)}{K}$, is the foundation of our derivation. We now apply it to the two specified cases.\n\nCase 1: Diploid population ($2x$)\nIn this case, the ploidy level is $k=2$. The population consists of $N$ diploid individuals. The total number of gene copies at the locus is $K^{(2x)} = 2N$.\nSubstituting this into our general variance formula:\n$$\n\\operatorname{Var}(p_{t+1}\\mid p_t=p) = \\frac{p(1-p)}{K^{(2x)}} = \\frac{p(1-p)}{2N}\n$$\nNow, we equate this result with the definition of the variance effective population size, $N_e^{(2x)}$:\n$$\n\\frac{p(1-p)}{2N_e^{(2x)}} = \\frac{p(1-p)}{2N}\n$$\nFor $p \\in (0,1)$, we can cancel the term $p(1-p)$. This yields:\n$$\n\\frac{1}{2N_e^{(2x)}} = \\frac{1}{2N} \\implies N_e^{(2x)} = N\n$$\nThis is the well-known result that the variance effective size of an idealized diploid Wright–Fisher population is equal to its census size.\n\nCase 2: Autotetraploid population ($4x$)\nIn this case, the ploidy level is $k=4$. The population consists of $N$ autotetraploid individuals. The total number of gene copies at the locus is $K^{(4x)} = 4N$. The problem specifies tetrasomic inheritance with no double reduction. In the context of Wright–Fisher sampling of alleles from a gene pool, this means we can model the formation of the next generation's gene pool as a simple random draw of $4N$ alleles from the parent generation's gene pool. The complexities of meiosis (like double reduction) would introduce another level of sampling (alleles into gametes) which would alter the variance, but the problem's assumption simplifies the model to direct binomial sampling from the gene pool.\nSubstituting $K^{(4x)} = 4N$ into our general variance formula:\n$$\n\\operatorname{Var}(p_{t+1}\\mid p_t=p) = \\frac{p(1-p)}{K^{(4x)}} = \\frac{p(1-p)}{4N}\n$$\nEquating this result with the definition of the variance effective population size, $N_e^{(4x)}$:\n$$\n\\frac{p(1-p)}{2N_e^{(4x)}} = \\frac{p(1-p)}{4N}\n$$\nAgain, for $p \\in (0,1)$, we cancel the term $p(1-p)$ to get:\n$$\n\\frac{1}{2N_e^{(4x)}} = \\frac{1}{4N} \\implies 4N = 2N_e^{(4x)} \\implies N_e^{(4x)} = 2N\n$$\nSo, the variance effective size of an idealized autotetraploid Wright–Fisher population is twice its census size.\n\nComparison of cases\nThe final task is to compute the ratio $R$:\n$$\nR = \\frac{N_e^{(4x)}}{N_e^{(2x)}}\n$$\nUsing the expressions we have derived:\n$$\nR = \\frac{2N}{N}\n$$\nThe census size $N$ cancels, provided $N \\neq 0$, which is implicit for a population.\n$$\nR = 2\n$$\nThe result indicates that for a given census size $N$, an autotetraploid population has twice the effective population size of a diploid population, under the idealized conditions specified. This is because the magnitude of genetic drift, as measured by the variance in allele frequency, is inversely proportional to the total number of gene copies in the population, which is $4N$ for autotetraploids versus $2N$ for diploids.", "answer": "$$\n\\boxed{2}\n$$", "id": "2825710"}, {"introduction": "The theoretical principles of polyploidy are not just abstract concepts; they are essential tools for interpreting the story of evolution written in genomes. Ancient whole-genome duplication (WGD) events leave distinct signatures in the age distribution of a species' duplicate genes, which can be visualized as peaks in a histogram of synonymous substitution rates ($K_s$). This advanced practice [@problem_id:2825757] bridges theory with data analysis by tasking you with building a statistical mixture model to formalize the detection of a WGD peak against the background noise of smaller, continuous gene duplications. By translating a biological hypothesis into a rigorous, quantitative test, this exercise provides invaluable hands-on experience in computational biology and the inference of major macroevolutionary events from modern genomic data.", "problem": "You are given the following scientific context and asked to translate it into a rigorous statistical decision procedure that can be implemented programmatically. In comparative genomics, duplicate gene pairs accumulate synonymous substitutions over time. Under the Molecular Clock approximation, the expected number of synonymous substitutions per synonymous site, denoted by $K_s$, increases approximately linearly with time for neutral sites. Small-scale gene duplications are expected to occur continuously over evolutionary time, whereas a Whole-Genome Duplication (WGD) event produces a pulse of duplicate births concentrated in a short time window. Consequently, the empirical distribution of $K_s$ among duplicate pairs can often be modeled as a mixture of latent processes. The question is whether a prominent peak near $K_s \\approx 0.8$ is better explained by a distinct WGD component or arises from background processes alone.\n\nFrom first principles, assume the following foundational base:\n- Duplicate birth times arise from latent processes with distinct age distributions. Under multiplicative noise in rates and times, a log-normal mixture is a reasonable approximation for the marginal distribution of $K_s$ values. If $Y = \\log K_s$, a log-normal mixture for $K_s$ corresponds to a Gaussian mixture for $Y$.\n- Let $Y_i = \\log K_{s,i}$ for $i = 1,\\dots,n$. Under a $K$-component Gaussian mixture, $Y_i$ are independently and identically distributed from $p(y) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(y \\mid \\mu_k, \\sigma_k^2)$ with mixture weights $\\pi_k > 0$, $\\sum_{k=1}^{K} \\pi_k = 1$, means $\\mu_k \\in \\mathbb{R}$, and variances $\\sigma_k^2 > 0$.\n- Maximum likelihood estimation for the Gaussian mixture can be performed via the Expectation–Maximization algorithm, and model comparison between $K=1$ and $K=2$ components can be based on the Bayesian Information Criterion (BIC), defined as $\\mathrm{BIC} = -2 \\log \\hat{L} + p \\log n$, where $\\hat{L}$ is the maximized likelihood and $p$ is the number of free parameters. For a univariate Gaussian mixture with $K$ components, $p = (K-1) + K + K = 3K - 1$.\n- The log-normal component corresponding to $(\\mu, \\sigma^2)$ in log-space has mode at $\\exp(\\mu - \\sigma^2)$. A WGD component associated with a peak near $K_s \\approx 0.8$ would therefore correspond to a Gaussian component in $Y$ whose induced log-normal mode lies near $0.8$ and whose mixture weight is non-trivial.\n\nDesign a program that implements the following mixture modeling decision rule to test whether a peak at $K_s \\approx 0.8$ represents a distinct WGD component:\n- Fit the $K=1$ and $K=2$ Gaussian mixture models to $Y_i = \\log K_{s,i}$ via maximum likelihood using the Expectation–Maximization algorithm with multiple random initializations to avoid poor local optima.\n- Compute $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{K=1} - \\mathrm{BIC}_{K=2}$.\n- Declare that the peak at $K_s \\approx 0.8$ is explained by a distinct WGD component if and only if both of the following criteria are met:\n  1. $\\Delta \\mathrm{BIC} \\ge \\tau_{\\mathrm{BIC}}$, where $\\tau_{\\mathrm{BIC}} = 10.0$.\n  2. Under the fitted $K=2$ model, there exists at least one component whose induced log-normal mode $m_k = \\exp(\\mu_k - \\sigma_k^2)$ satisfies $\\lvert m_k - 0.8 \\rvert \\le \\tau_{\\mathrm{loc}}$ with $\\tau_{\\mathrm{loc}} = 0.1$, and whose weight satisfies $\\pi_k \\ge \\tau_{\\mathrm{w}}$ with $\\tau_{\\mathrm{w}} = 0.05$.\n\nThere are no physical units because $K_s$ is dimensionless. All numerical tolerances in this problem are pure numbers. Angles are not involved.\n\nTest suite. To ensure the solution is testable and covers distinct regimes, generate synthetic $K_s$ datasets from log-normal mixtures with known parameters. For a log-normal component having target mode $m$ and log-scale standard deviation $\\sigma$, set $\\mu = \\log m + \\sigma^2$ so that the mode in $K_s$ space equals $m$. For each case below, draw $n$ independent samples and mix components according to the specified weights.\n- Case $1$ (clear WGD near $0.8$): $n = 4000$, two components with modes $m = [0.2, 0.8]$, log-scale standard deviations $\\sigma = [0.15, 0.15]$, and weights $\\pi = [0.6, 0.4]$.\n- Case $2$ (subtle WGD, small weight): $n = 4000$, two components with modes $m = [0.2, 0.8]$, log-scale standard deviations $\\sigma = [0.15, 0.08]$, and weights $\\pi = [0.9, 0.1]$.\n- Case $3$ (no WGD; single component centered near $0.8$): $n = 4000$, one component with mode $m = [0.8]$, log-scale standard deviation $\\sigma = [0.12]$, and weight $\\pi = [1.0]$.\n- Case $4$ (two overlapping components flanking $0.8$ but none centered at $0.8$): $n = 4000$, two components with modes $m = [0.7, 0.9]$, log-scale standard deviations $\\sigma = [0.12, 0.12]$, and weights $\\pi = [0.5, 0.5]$.\n\nYour program must:\n- Deterministically simulate the four datasets above using fixed random seeds.\n- Fit the $K=1$ and $K=2$ Gaussian mixtures to $Y = \\log K_s$ for each dataset using the Expectation–Maximization algorithm with at least $5$ random initializations, maximum $200$ iterations per run, and a convergence tolerance of $10^{-6}$ in absolute change in log-likelihood. Impose a minimum variance floor of $10^{-6}$ in log-space during estimation to avoid degeneracy.\n- Apply the decision rule described above to produce, for each case, a boolean indicating whether a distinct WGD component at $K_s \\approx 0.8$ is supported.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean decisions, in order for Cases $1$ through $4$, as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\").", "solution": "The problem requires the formulation and implementation of a statistical decision procedure to test for evidence of a Whole-Genome Duplication (WGD) event from the distribution of synonymous substitutions ($K_s$) between duplicate gene pairs. The procedure must be based on mixture modeling and information criteria. The problem is scientifically grounded, well-posed, and all necessary parameters and criteria are provided.\n\nThe scientific premise is that a WGD event creates a large number of gene duplicates at a single point in time, which manifest as a distinct peak in the $K_s$ distribution. Continuous, small-scale duplications form a background distribution. We model the overall $K_s$ distribution as a log-normal mixture, which implies that the log-transformed data, $Y_i = \\log K_{s,i}$, follow a Gaussian Mixture Model (GMM). The task is to distinguish between a single background process ($K=1$ model) and a background plus a WGD process ($K=2$ model).\n\nThe GMM density for $Y$ is given by:\n$$ p(y | \\theta) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(y \\mid \\mu_k, \\sigma_k^2) $$\nwhere $\\theta = \\{\\pi_k, \\mu_k, \\sigma_k^2\\}_{k=1}^K$ are the parameters: $\\pi_k$ are the mixture weights ($\\sum_k \\pi_k = 1, \\pi_k > 0$), $\\mu_k$ are the means, and $\\sigma_k^2$ are the variances of the Gaussian components.\n\nThe core of the solution involves three steps: parameter estimation, model comparison, and application of a decision rule.\n\n1.  **Parameter Estimation via Expectation-Maximization (EM)**\n    The parameters $\\theta$ of the GMM are estimated by maximizing the log-likelihood of the observed data, $\\mathcal{L}(\\theta) = \\sum_{i=1}^n \\log p(Y_i | \\theta)$. The Expectation-Maximization (EM) algorithm is an iterative procedure for this purpose. It consists of two steps:\n    -   **E-step (Expectation):** We compute the posterior probability, or \"responsibility,\" $\\gamma_{ik}$ that data point $Y_i$ was generated by component $k$, given the current parameter estimates $\\theta^{(t)}$.\n        $$ \\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(Y_i \\mid \\mu_k^{(t)}, (\\sigma_k^2)^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(Y_i \\mid \\mu_j^{(t)}, (\\sigma_j^2)^{(t)})} $$\n    -   **M-step (Maximization):** We update the parameters to maximize the expected complete-data log-likelihood, using the responsibilities calculated in the E-step. The updates are:\n        Let $N_k = \\sum_{i=1}^n \\gamma_{ik}$.\n        $$ \\pi_k^{(t+1)} = \\frac{N_k}{n} $$\n        $$ \\mu_k^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^n \\gamma_{ik} Y_i $$\n        $$ (\\sigma_k^2)^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^n \\gamma_{ik} (Y_i - \\mu_k^{(t+1)})^2 $$\n    To prevent numerical instability from variances approaching zero (a degenerate solution), a minimum variance floor of $10^{-6}$ is enforced. The algorithm is iterated until the change in log-likelihood falls below a tolerance of $10^{-6}$ or a maximum of $200$ iterations is reached. Since the likelihood surface for GMMs has multiple local optima, the EM algorithm will be run from $5$ different random initializations, and the solution with the highest log-likelihood will be selected.\n\n2.  **Model Comparison via Bayesian Information Criterion (BIC)**\n    To compare the $K=1$ model (null hypothesis: background only) with the $K=2$ model (alternative hypothesis: background + WGD), we use the Bayesian Information Criterion (BIC). BIC penalizes models for complexity, balancing goodness-of-fit with the number of parameters.\n    $$ \\mathrm{BIC} = -2 \\log \\hat{L} + p \\log n $$\n    Here, $\\hat{L}$ is the maximized likelihood, $n$ is the number of samples, and $p$ is the number of free parameters. For a univariate GMM with $K$ components, $p = (K-1) \\text{ weights} + K \\text{ means} + K \\text{ variances} = 3K-1$. We will compute $\\mathrm{BIC}_{K=1}$ (with $p_1=2$) and $\\mathrm{BIC}_{K=2}$ (with $p_2=5$). Strong evidence in favor of the more complex model is indicated by a lower BIC value.\n\n3.  **Decision Rule**\n    A WGD component associated with a peak at $K_s \\approx 0.8$ is declared to be present if and only if two conditions are met:\n    -   **Statistical Significance:** The $K=2$ model must provide a substantially better fit than the $K=1$ model, as measured by $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{K=1} - \\mathrm{BIC}_{K=2} \\ge \\tau_{\\mathrm{BIC}}$, with $\\tau_{\\mathrm{BIC}} = 10.0$. A difference of $10$ is generally considered very strong evidence.\n    -   **Biological Relevance:** The added component in the $K=2$ model must correspond to the hypothesized WGD event. This means there must exist at least one component $k$ whose induced log-normal mode, $m_k = \\exp(\\mu_k - \\sigma_k^2)$, is located near $0.8$ (specifically, $|m_k - 0.8| \\le \\tau_{\\mathrm{loc}} = 0.1$) and whose contribution to the mixture is non-trivial (weight $\\pi_k \\ge \\tau_{\\mathrm{w}} = 0.05$).\n\nThe implementation will proceed by first simulating four deterministic datasets according to the problem specification. For each dataset, we first estimate parameters for a single Gaussian ($K=1$)—for which the MLEs are the sample mean and variance—and compute $\\mathrm{BIC}_{K=1}$. Then, we fit a two-component GMM ($K=2$) using the described EM procedure and compute $\\mathrm{BIC}_{K=2}$. Finally, the two-part decision rule is applied to determine the outcome for each case. The entire process will be encapsulated in a Python program using `numpy` and `scipy` libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef generate_data(n, modes, log_stds, weights, seed):\n    \"\"\"\n    Generates synthetic log-transformed data (Y = log(K_s)) from a Gaussian mixture.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_components = len(modes)\n    \n    # Calculate log-space means (mu) from K_s-space modes (m) and log-stds (sigma)\n    # m = exp(mu - sigma^2) => mu = log(m) + sigma^2\n    log_means = np.log(modes) + np.array(log_stds)**2\n    \n    # Assign each sample to a component\n    component_choices = rng.choice(n_components, size=n, p=weights)\n    \n    # Generate data from the chosen components\n    y_samples = np.array([\n        rng.normal(loc=log_means[comp], scale=log_stds[comp])\n        for comp in component_choices\n    ])\n    \n    return y_samples\n\ndef _em_single_run(Y, K, max_iter, tol, var_floor, init_seed):\n    \"\"\"\n    A single run of the EM algorithm for a GMM.\n    \"\"\"\n    n_samples = len(Y)\n    rng = np.random.default_rng(init_seed)\n    \n    # Initialization\n    # Randomly choose K data points as initial means\n    initial_means = rng.choice(Y, K, replace=False)\n    # Set initial variances to the global variance\n    initial_vars = np.full(K, np.var(Y))\n    # Set initial weights to be uniform\n    weights = np.full(K, 1.0 / K)\n    \n    means = initial_means\n    variances = initial_vars\n\n    log_likelihood_old = -np.inf\n    \n    for i in range(max_iter):\n        # E-step: Calculate responsibilities\n        log_resp_num = np.zeros((n_samples, K))\n        for k in range(K):\n            log_resp_num[:, k] = np.log(weights[k]) + norm.logpdf(Y, means[k], np.sqrt(variances[k]))\n        \n        log_norm_const = logsumexp(log_resp_num, axis=1)\n        log_resp = log_resp_num - log_norm_const[:, np.newaxis]\n        responsibilities = np.exp(log_resp)\n\n        # M-step: Update parameters\n        Nk = np.sum(responsibilities, axis=0)\n        \n        weights = Nk / n_samples\n        \n        # Add a small constant to avoid division by zero if a component dies\n        means = np.sum(responsibilities * Y[:, np.newaxis], axis=0) / (Nk + 1e-9)\n        \n        variances_num = np.sum(responsibilities * (Y[:, np.newaxis] - means)**2, axis=0)\n        variances = variances_num / (Nk + 1e-9)\n        \n        # Apply variance floor\n        variances = np.maximum(variances, var_floor)\n\n        #_check for convergence\n        log_likelihood_new = np.sum(log_norm_const)\n        if abs(log_likelihood_new - log_likelihood_old) < tol:\n            break\n        log_likelihood_old = log_likelihood_new\n            \n    return weights, means, variances, log_likelihood_old\n\ndef em_gmm(Y, K, num_inits, max_iter, tol, var_floor, seed_seq):\n    \"\"\"\n    Fits a GMM using EM with multiple random initializations.\n    \"\"\"\n    best_log_likelihood = -np.inf\n    best_params = None\n\n    for i in range(num_inits):\n        init_seed = seed_seq.spawn(1)[0]\n        weights, means, variances, log_likelihood = _em_single_run(\n            Y, K, max_iter, tol, var_floor, init_seed\n        )\n        if log_likelihood > best_log_likelihood:\n            best_log_likelihood = log_likelihood\n            best_params = (weights, means, variances)\n            \n    return best_params, best_log_likelihood\n\ndef calculate_bic(log_likelihood, n_params, n_samples):\n    \"\"\"\n    Calculates the Bayesian Information Criterion (BIC).\n    \"\"\"\n    return -2 * log_likelihood + n_params * np.log(n_samples)\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the simulation, model fitting, and decision process.\n    \"\"\"\n    test_cases_params = [\n        # Case 1 (clear WGD)\n        {'n': 4000, 'modes': [0.2, 0.8], 'log_stds': [0.15, 0.15], 'weights': [0.6, 0.4]},\n        # Case 2 (subtle WGD)\n        {'n': 4000, 'modes': [0.2, 0.8], 'log_stds': [0.15, 0.08], 'weights': [0.9, 0.1]},\n        # Case 3 (no WGD, single component)\n        {'n': 4000, 'modes': [0.8], 'log_stds': [0.12], 'weights': [1.0]},\n        # Case 4 (overlapping components)\n        {'n': 4000, 'modes': [0.7, 0.9], 'log_stds': [0.12, 0.12], 'weights': [0.5, 0.5]},\n    ]\n\n    # EM Parameters\n    em_config = {\n        'num_inits': 5,\n        'max_iter': 200,\n        'tol': 1e-6,\n        'var_floor': 1e-6\n    }\n    \n    # Decision Rule Thresholds\n    tau_bic = 10.0\n    tau_loc = 0.1\n    tau_w = 0.05\n    target_mode = 0.8\n    \n    results = []\n    \n    # Master seed for deterministic execution\n    master_seed = 12345\n    rng_master = np.random.SeedSequence(master_seed)\n    case_seeds = rng_master.spawn(len(test_cases_params))\n    \n    for i, params in enumerate(test_cases_params):\n        data_seed = case_seeds[i]\n        Y = generate_data(\n            n=params['n'],\n            modes=params['modes'],\n            log_stds=params['log_stds'],\n            weights=params['weights'],\n            seed=data_seed\n        )\n        n = len(Y)\n\n        # --- Fit K=1 model ---\n        mu1_k1 = np.mean(Y)\n        var1_k1 = np.var(Y) # MLE for variance uses 1/n\n        \n        logL_1 = np.sum(norm.logpdf(Y, mu1_k1, np.sqrt(var1_k1)))\n        p_1 = 3 * 1 - 1\n        bic_1 = calculate_bic(logL_1, p_1, n)\n\n        # --- Fit K=2 model ---\n        em_seed_seq = case_seeds[i].spawn(1)[0] # Derive seed for EM initializations\n        params_2, logL_2 = em_gmm(\n            Y, K=2, \n            seed_seq=em_seed_seq,\n            **em_config\n        )\n        p_2 = 3 * 2 - 1\n        bic_2 = calculate_bic(logL_2, p_2, n)\n        \n        # --- Apply Decision Rule ---\n        delta_bic = bic_1 - bic_2\n        \n        # Condition 1: BIC evidence\n        cond1_met = delta_bic >= tau_bic\n        \n        # Condition 2: Biologically relevant component\n        cond2_met = False\n        if params_2 is not None:\n            weights_2, means_2, variances_2 = params_2\n            for k in range(2):\n                # Calculate induced log-normal mode: m = exp(mu - sigma^2)\n                mode_k = np.exp(means_2[k] - variances_2[k])\n                weight_k = weights_2[k]\n                \n                loc_check = abs(mode_k - target_mode) <= tau_loc\n                weight_check = weight_k >= tau_w\n                \n                if loc_check and weight_check:\n                    cond2_met = True\n                    break\n\n        decision = cond1_met and cond2_met\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2825757"}]}