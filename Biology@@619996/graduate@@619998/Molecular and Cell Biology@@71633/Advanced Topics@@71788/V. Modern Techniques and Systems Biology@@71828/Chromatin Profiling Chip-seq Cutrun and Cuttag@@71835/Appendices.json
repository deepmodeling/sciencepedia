{"hands_on_practices": [{"introduction": "Accurately comparing chromatin profiles between samples requires correcting for technical variability introduced during sample processing and sequencing. Spike-in normalization provides a robust solution by introducing a constant amount of exogenous reference chromatin into each sample. This exercise challenges you to derive the spike-in normalization formula from first principles, solidifying your understanding of how this internal standard allows for the accurate quantification of biological fold changes by controlling for technical noise. [@problem_id:2938912]", "problem": "A laboratory is comparing chromatin occupancy at a specific genomic peak between two samples profiled by Cleavage Under Targets and Tagmentation (CUT&Tag). The laboratory used a constant amount of exogenous spike-in chromatin as an internal reference in both libraries, a practice also common in Chromatin Immunoprecipitation Sequencing (ChIP-Seq) and Cleavage Under Targets and Release Using Nuclease (CUT&RUN). The following quantities were measured for sample $1$ and sample $2$:\n- $S_1$ and $S_2$: the numbers of mapped spike-in reads.\n- $N_1$ and $N_2$: the total numbers of mapped reads (spike-in plus endogenous).\n- $y_1$ and $y_2$: the raw read counts at the peak of interest from the endogenous genome.\n\nAssume the following foundational facts:\n- The same absolute number of spike-in molecules was added to each library.\n- Sequencing is a linear sampling process, so read counts are proportional to the underlying molecule counts after library preparation and mapping.\n- There is no mapping bias that differentially affects spike-in and endogenous reads across the two samples.\n\nStarting only from these assumptions and definitions, derive a spike-in–normalized fold change of the peak between sample $2$ and sample $1$ that removes differences in sequencing depth and library efficiency. Express your final answer as a single simplified analytic expression in terms of $S_1$, $S_2$, $N_1$, $N_2$, $y_1$, and $y_2$. The fold change is a dimensionless quantity. No rounding is required; provide a closed-form expression.", "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of quantitative molecular biology, specifically next-generation sequencing data normalization. It is well-posed, objective, and contains all necessary information to derive a unique solution.\n\nThe objective is to derive the fold change ($FC$) of chromatin occupancy at a specific peak between sample $2$ and sample $1$. This is the ratio of the true absolute molecular quantities of the target, which we denote as $C_{y_2}$ and $C_{y_1}$.\n$$FC = \\frac{C_{y_2}}{C_{y_1}}$$\nThe problem provides a set of measured read counts: $y_1$ and $y_2$ for the endogenous peak of interest, $S_1$ and $S_2$ for the exogenous spike-in reference, and $N_1$ and $N_2$ for the total mapped reads. It also posits a set of foundational assumptions from which we must construct our argument.\n\nLet $C_s$ be the absolute number of spike-in molecules added to each library. According to the problem, this quantity is identical for both samples. The fundamental assumption is that sequencing is a linear sampling process. This implies that for any given sample library, the number of observed reads is directly proportional to the initial number of molecules. We can introduce sample-specific proportionality constants, $k_1$ for sample $1$ and $k_2$ for sample $2$, which encapsulate all sources of technical variability, such as antibody efficiency, library preparation yield, and sequencing depth.\n\nThis allows us to write a system of equations relating the unknown molecular quantities to the observed read counts:\nFor sample $1$:\nThe number of endogenous reads at the peak, $y_1$, is proportional to the number of endogenous molecules at that peak, $C_{y_1}$.\n$$y_1 = k_1 C_{y_1}$$\nThe number of spike-in reads, $S_1$, is proportional to the number of spike-in molecules, $C_s$.\n$$S_1 = k_1 C_s$$\n\nFor sample $2$:\nThe number of endogenous reads at the peak, $y_2$, is proportional to the number of endogenous molecules at that peak, $C_{y_2}$.\n$$y_2 = k_2 C_{y_2}$$\nThe number of spike-in reads, $S_2$, is proportional to the number of spike-in molecules, $C_s$.\n$$S_2 = k_2 C_s$$\n\nOur goal is to find the value of $\\frac{C_{y_2}}{C_{y_1}}$. We begin by expressing the unknown molecular quantities in terms of the measured quantities and the unknown technical factors. From the equations for the endogenous reads:\n$$C_{y_1} = \\frac{y_1}{k_1}$$\n$$C_{y_2} = \\frac{y_2}{k_2}$$\n\nSubstituting these into the expression for the fold change, we get:\n$$FC = \\frac{C_{y_2}}{C_{y_1}} = \\frac{y_2 / k_2}{y_1 / k_1} = \\frac{y_2}{y_1} \\cdot \\frac{k_1}{k_2}$$\nThis expression contains the ratio of the unknown technical factors, $\\frac{k_1}{k_2}$. The purpose of the spike-in reference is to determine this ratio. From the equations for the spike-in reads, we can express $k_1$ and $k_2$:\n$$k_1 = \\frac{S_1}{C_s}$$\n$$k_2 = \\frac{S_2}{C_s}$$\n\nThe ratio of these factors is therefore:\n$$\\frac{k_1}{k_2} = \\frac{S_1 / C_s}{S_2 / C_s} = \\frac{S_1}{S_2}$$\nThe unknown absolute quantity of spike-in molecules, $C_s$, cancels out. This demonstrates that the ratio of technical efficiencies between the two samples can be determined directly from the ratio of their respective spike-in read counts.\n\nBy substituting this result back into the equation for the fold change, we eliminate the unknown technical factors:\n$$FC = \\frac{y_2}{y_1} \\cdot \\left(\\frac{S_1}{S_2}\\right) = \\frac{y_2 S_1}{y_1 S_2}$$\nThis is the final expression for the spike-in–normalized fold change. This calculation can also be viewed as the ratio of normalized peak signals, where the signal in each sample is normalized to its own spike-in signal: $FC = \\frac{(y_2/S_2)}{(y_1/S_1)}$. The term $y/S$ represents the ratio of endogenous target reads to spike-in reads, which is proportional to the true biological ratio $C_y/C_s$ since the technical factor $k$ cancels within the same sample: $\\frac{y}{S} = \\frac{k C_y}{k C_s} = \\frac{C_y}{C_s}$.\n\nThe total read counts, $N_1$ and $N_2$, are not required for this derivation. They are extraneous variables in the context of spike-in normalization, which is specifically designed to be independent of total library size, thereby correctly handling cases where global chromatin amounts differ between samples. A normalization scheme including $N_1$ and $N_2$ would be inappropriate and less robust under the given assumptions. The problem tests the ability to identify the correct variables and principles for the task.", "answer": "$$\\boxed{\\frac{y_2 S_1}{y_1 S_2}}$$", "id": "2938912"}, {"introduction": "Interpreting chromatin profiling data requires a keen awareness of potential bioinformatic artifacts, especially in complex genomic regions. Standard analysis pipelines can produce highly misleading results when applied to heterochromatin, whose repetitive nature challenges short-read alignment and can lead to artifactual signal depletion. This practice is a diagnostic exercise where you must deduce why a heterochromatic mark like $\\text{H3K9me3}$ might appear depleted and propose an analysis strategy that correctly handles multi-mapping reads to reveal the true biological signal. [@problem_id:2938927]", "problem": "A laboratory compares profiling of histone H3 lysine 9 trimethylation (H3K9me3) in mouse embryonic stem cells using Chromatin Immunoprecipitation sequencing (ChIP-Seq) and Cleavage Under Targets and Tagmentation (CUT&Tag). Both assays were sequenced as single-end reads of length $50$ base pairs and analyzed with the same pipeline: mapping to the current reference genome with a short-read aligner, discarding all reads that align to more than one genomic locus, and removing polymerase chain reaction (PCR) duplicates. The H3K9me3 signal appears depleted over pericentromeric regions and other known heterochromatic domains rich in satellite and transposable element repeats, while narrow peaks are observed at a small number of unique loci. The laboratory concludes that CUT&Tag fails in heterochromatin.\n\nUsing only fundamental definitions and well-tested facts, reason about the following. In a genome, the mappability of a read of length $\\ell$ at a genomic coordinate can be defined by the number of times the corresponding $\\ell$-mer occurs in the reference; if the count equals $1$, the read is uniquely mappable, and if it exceeds $1$, it is multi-mappable. Heterochromatin marked by H3K9me3 is enriched in repetitive DNA, leading to many reads whose $\\ell$-mers occur multiple times. Short-read assays like ChIP-Seq and CUT&Tag produce DNA fragments whose ends reflect protein-directed cleavage or tagmentation; in repetitive regions, many fragments share identical start and end positions, even when they arise from distinct molecules. Reference assemblies may incompletely represent simple repeat arrays (for example, some pericentromeric satellites), causing reads originating from those arrays to fail to align or to align ambiguously.\n\nWhich option best explains the observed depletion and proposes an analysis change that directly addresses the specific limitation while minimizing false discovery?\n\nA. The depletion is a mapping artifact: strict unique-mapper filtering and aggressive duplicate removal discard the majority of true H3K9me3 reads from repetitive heterochromatin, and incomplete reference assemblies further prevent alignment. A remedy is to adopt multi-mapper-aware quantification (for example, expectation–maximization to allocate reads proportionally across annotated repeat families), augment the reference with decoy or consensus repeat sequences, and report enrichment at the level of repeat families and assembled blocks rather than requiring per-locus uniqueness.\n\nB. The depletion indicates biochemical failure of CUT&Tag in compact chromatin because tethered nucleases cannot function in heterochromatin; the solution is to increase detergent concentrations to open chromatin globally so that cleavage occurs uniformly across the genome.\n\nC. The depletion reflects epitope occlusion by repetitive DNA that specifically interferes with the H3K9me3 antibody used only in CUT&Tag; the correct remedy is to switch to an antibody that recognizes a different region of histone H3, while keeping the same mapping and filtering strategy.\n\nD. The depletion is expected because H3K9me3 is primarily a promoter-associated mark; the correct interpretation is that both assays have faithfully detected H3K9me3 at transcription start sites, and no changes are needed.\n\nE. The depletion arises from insufficient library complexity in repetitive regions; the correct fix is to increase the number of PCR amplification cycles to enrich repetitive-derived fragments and simultaneously apply stricter duplicate removal thresholds so that only fragments with identical start sites are discarded more aggressively.\n\nSelect the single best option.", "solution": "The problem statement must first be validated for its scientific and logical integrity.\n\n**Step 1: Extract Givens**\n- **Experiment:** Comparison of histone H3 lysine 9 trimethylation ($\\text{H3K9me3}$) profiling in mouse embryonic stem cells.\n- **Methods:** Chromatin Immunoprecipitation sequencing (ChIP-Seq) and Cleavage Under Targets and Tagmentation (CUT&Tag).\n- **Sequencing Parameters:** Single-end reads of length $\\ell = 50$ base pairs.\n- **Bioinformatic Pipeline:**\n    1.  Map reads to the reference genome.\n    2.  Discard all reads aligning to more than one genomic locus (unique-mapper filtering).\n    3.  Remove polymerase chain reaction (PCR) duplicates.\n- **Observation:** Signal for $\\text{H3K9me3}$ is depleted over pericentromeric regions and other heterochromatic domains, which are known to be rich in repetitive DNA. Narrow peaks are seen at a few unique loci.\n- **Hypothetical Conclusion:** The laboratory concludes that CUT&Tag fails in heterochromatin.\n- **Provided Scientific Context:**\n    1.  Mappability definition: A read of length $\\ell$ is uniquely mappable if its $\\ell$-mer sequence occurs once in the reference genome; it is multi-mappable if it occurs more than once.\n    2.  $\\text{H3K9me3}$-marked heterochromatin is enriched in repetitive DNA, yielding many multi-mappable reads.\n    3.  In repetitive regions, fragments originating from distinct biological molecules can frequently share identical start and end positions.\n    4.  Reference genome assemblies may be incomplete for simple repeat arrays, causing reads from these regions to fail alignment or align ambiguously.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically valid.\n- **Scientific Grounding:** The problem is firmly based on established principles of molecular biology and computational genomics. $\\text{H3K9me3}$ is a canonical mark of constitutive heterochromatin. The challenges described—mappability of short reads in repetitive regions, artifacts from PCR duplicate removal, and incomplete reference genomes—are fundamental and well-documented issues in the analysis of next-generation sequencing data.\n- **Well-Posedness:** The problem presents a specific, realistic scenario: a common bioinformatic artifact is misinterpreted as a biochemical failure. It asks for the correct explanation and an appropriate methodological correction. The information provided is sufficient to deduce the cause and evaluate the proposed solutions.\n- **Objectivity:** The problem is stated in precise, technical language. It describes an experimental setup, an analysis pipeline, and an observation, which form the basis for a logical deduction.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\n**Derivation of the Correct Answer**\nThe central issue is the apparent contradiction between the known biological role of $\\text{H3K9me3}$ and the experimental observation. $\\text{H3K9me3}$ is the defining mark of constitutive heterochromatin, which is predominantly composed of repetitive DNA elements (e.g., satellite repeats in pericentromeric regions). Therefore, a correct experiment and analysis should show high enrichment of $\\text{H3K9me3}$ signal over these regions. The observation of signal *depletion* points to a methodological artifact.\n\nLet us analyze the pipeline in the context of the provided scientific facts:\n1.  **Repetitive DNA and Multi-mappability:** Heterochromatic regions are repetitive. Short reads of length $\\ell = 50$ originating from these regions will have sequences that are not unique in the genome. Such reads are multi-mappable.\n2.  **Unique-Mapper Filtering:** The pipeline explicitly \"discard[s] all reads that align to more than one genomic locus.\" This step systematically removes the very reads that originate from the repetitive heterochromatic regions where the $\\text{H3K9me3}$ mark is expected to be most abundant. This is a primary cause of the observed signal depletion.\n3.  **PCR Duplicate Removal:** In highly repetitive regions, it is statistically probable that transposition or fragmentation events at different physical locations (i.e., on different DNA molecules from different cells) will produce fragments with identical start and end coordinates. Standard duplicate removal algorithms mistake these unique biological events for PCR amplification artifacts and discard them. This further reduces the true signal from heterochromatin.\n4.  **Incomplete Reference Genome:** Pericentromeric satellite repeats are notoriously difficult to assemble. The reference genome is often a simplified or collapsed representation of these regions, or may omit them entirely. Reads originating from these true genomic regions will either fail to align or align to a small number of representative sequences, exacerbating the multi-mapping problem and potential for false duplicate identification.\n\nThe laboratory's conclusion that \"CUT&Tag fails\" is a premature and likely incorrect interpretation. The described analysis pipeline is flawed for the study of repetitive genomic elements. Since both ChIP-Seq and CUT&Tag data were processed with the same pipeline, both datasets would suffer from the same artifact. The problem is computational, not biochemical.\n\nThe observed \"narrow peaks ... at a small number of unique loci\" are also consistent with this analysis. These likely correspond to the few instances of $\\text{H3K9me3}$ that occur in non-repetitive, uniquely mappable parts of the genome (e.g., at the promoters of certain silent genes like ZFP clusters), which are the only signals to survive the stringent filtering.\n\nA correct solution must therefore address the flawed bioinformatic processing of reads from repetitive regions.\n\n**Option-by-Option Analysis**\n\n**A. The depletion is a mapping artifact: strict unique-mapper filtering and aggressive duplicate removal discard the majority of true H3K9me3 reads from repetitive heterochromatin, and incomplete reference assemblies further prevent alignment. A remedy is to adopt multi-mapper-aware quantification (for example, expectation–maximization to allocate reads proportionally across annotated repeat families), augment the reference with decoy or consensus repeat sequences, and report enrichment at the level of repeat families and assembled blocks rather than requiring per-locus uniqueness.**\n- **Analysis:** This option correctly identifies all three sources of the artifact: unique-mapper filtering, overly aggressive duplicate removal in repetitive regions, and incomplete reference assemblies. The proposed remedy is a comprehensive and state-of-the-art bioinformatic strategy. Using algorithms to handle multi-mappers, improving the reference with decoy/consensus sequences for repeats, and changing the reporting unit from unique loci to repeat families are all standard and effective solutions to this exact problem. This approach directly addresses the identified limitations and aims to recover the true signal while controlling for ambiguity, thus minimizing false discovery.\n- **Verdict:** **Correct**.\n\n**B. The depletion indicates biochemical failure of CUT&Tag in compact chromatin because tethered nucleases cannot function in heterochromatin; the solution is to increase detergent concentrations to open chromatin globally so that cleavage occurs uniformly across the genome.**\n- **Analysis:** This option proposes a biochemical failure, which is less likely than the glaring computational artifact described. CUT&Tag is generally very efficient. The proposed solution, to globally open chromatin with high detergent concentrations, is a crude and destructive approach. It would disrupt the native chromatin structure that the experiment is intended to measure, leading to widespread artifacts and compromising the integrity of the data. It does not address the fundamental problem of mapping reads in repetitive regions.\n- **Verdict:** **Incorrect**.\n\n**C. The depletion reflects epitope occlusion by repetitive DNA that specifically interferes with the H3K9me3 antibody used only in CUT&Tag; the correct remedy is to switch to an antibody that recognizes a different region of histone H3, while keeping the same mapping and filtering strategy.**\n- **Analysis:** This explanation is speculative and unlikely to be the primary cause, as the flawed analysis pipeline would affect data from any technique targeting repetitive regions. The most critical flaw is in the remedy: keeping the same mapping and filtering strategy would ensure that even with a perfect antibody, the signal from heterochromatin would still be discarded. The problem is not the antibody; it is the data analysis.\n- **Verdict:** **Incorrect**.\n\n**D. The depletion is expected because H3K9me3 is primarily a promoter-associated mark; the correct interpretation is that both assays have faithfully detected H3K9me3 at transcription start sites, and no changes are needed.**\n- **Analysis:** This statement is fundamentally incorrect from a biological standpoint. $\\text{H3K9me3}$ is a hallmark of constitutive heterochromatin and transcriptional repression, not active promoters. While some specific genes might have promoter $\\text{H3K9me3}$, its primary abundance is in repetitive, silent domains. Claiming the depletion is expected and that no changes are needed ignores both the known biology of $\\text{H3K9me3}$ and the severe artifacts introduced by the analysis pipeline.\n- **Verdict:** **Incorrect**.\n\n**E. The depletion arises from insufficient library complexity in repetitive regions; the correct fix is to increase the number of PCR amplification cycles to enrich repetitive-derived fragments and simultaneously apply stricter duplicate removal thresholds so that only fragments with identical start sites are discarded more aggressively.**\n- **Analysis:** The proposed remedy is counterproductive. Increasing PCR cycles amplifies biases and creates more PCR duplicates, which does not solve the underlying problem of mapping ambiguity. More importantly, applying *stricter* duplicate removal would *worsen* the problem by discarding even more of the true biological signal from repetitive regions, where multiple unique fragments can naturally share coordinates. This approach would amplify, not correct, the error.\n- **Verdict:** **Incorrect**.\n\nBased on the analysis, Option A provides the most accurate diagnosis of the problem as a computational artifact and proposes the most appropriate and sophisticated set of solutions that are standard practice in the field for this type of analysis.", "answer": "$$\\boxed{A}$$", "id": "2938927"}, {"introduction": "A key challenge in genome-wide studies is the problem of multiple hypothesis testing, where performing thousands of statistical tests inflates the number of false positives. Simply using a conventional p-value threshold is statistically unsound. This practice introduces the concept of the False Discovery Rate (FDR) and walks you through the Benjamini–Hochberg procedure, a standard method for adjusting p-values to ensure that the list of significant findings is robust and reliable. [@problem_id:2938847]", "problem": "In a genome-wide profiling experiment using Cleavage Under Targets and Tagmentation (CUT&Tag) to map the histone modification H3K27ac, you evaluate enrichment for a set of candidate peaks against a matched control. For each of $m$ peaks, a two-sided hypothesis test is performed with null hypothesis that the local read density is not enriched over control. Assume that under the null hypothesis, $p$-values are independent and identically distributed as uniform on $[0,1]$, and that rejections are made by controlling the false discovery rate (FDR) using the Benjamini–Hochberg procedure.\n\nYou obtain the following $p$-values for $m=12$ peaks (unordered): $0.039$, $0.0018$, $0.12$, $0.21$, $0.026$, $0.0009$, $0.57$, $0.071$, $0.055$, $0.006$, $0.017$, $0.010$.\n\nUsing the Benjamini–Hochberg procedure at target FDR level $\\alpha = 0.05$, compute the Benjamini–Hochberg $q$-values for these $p$-values and determine how many peaks would be retained as significant at FDR $=0.05$. Report only the number of peaks retained as your final answer; express it as an integer with no units. No rounding beyond exact arithmetic is required.", "solution": "The problem statement has been validated and is found to be well-posed, internally consistent, and scientifically grounded. It presents a standard statistical task in the analysis of high-throughput biological data. A solution can therefore be derived through rigorous application of the specified procedure.\n\nThe task is to determine the number of statistically significant peaks from a set of $p$-values by controlling the False Discovery Rate (FDR) using the Benjamini–Hochberg (BH) procedure.\n\nThe given parameters are:\nThe total number of hypothesis tests (peaks), $m = 12$.\nThe target FDR level, $\\alpha = 0.05$.\nThe set of $m=12$ unordered $p$-values: $\\{0.039, 0.0018, 0.12, 0.21, 0.026, 0.0009, 0.57, 0.071, 0.055, 0.006, 0.017, 0.010\\}$.\n\nThe Benjamini–Hochberg procedure involves the following steps:\n1.  Order the raw $p$-values from smallest to largest. Let these ordered $p$-values be denoted by $p_{(1)}, p_{(2)}, \\dots, p_{(m)}$.\n2.  For each ordered $p$-value $p_{(i)}$, where $i$ is its rank from $1$ to $m$, calculate the Benjamini-Hochberg critical value $(i/m)\\alpha$.\n3.  Find the largest rank $k$ for which the $p$-value $p_{(k)}$ is less than or equal to its corresponding critical value, i.e., $p_{(k)} \\le \\frac{k}{m}\\alpha$.\n4.  All null hypotheses for tests with ranks from $1$ to $k$ are rejected. The number of discoveries is thus $k$.\n\nFirst, we order the $12$ $p$-values in ascending order:\n$p_{(1)} = 0.0009$\n$p_{(2)} = 0.0018$\n$p_{(3)} = 0.006$\n$p_{(4)} = 0.010$\n$p_{(5)} = 0.017$\n$p_{(6)} = 0.026$\n$p_{(7)} = 0.039$\n$p_{(8)} = 0.055$\n$p_{(9)} = 0.071$\n$p_{(10)} = 0.12$\n$p_{(11)} = 0.21$\n$p_{(12)} = 0.57$\n\nNext, we compare each $p_{(i)}$ to the BH critical value $\\frac{i}{m}\\alpha = \\frac{i}{12} \\times 0.05$.\n$\n\\begin{array}{c|c|c|c}\n\\text{Rank } (i) & p_{(i)} & \\text{BH Critical Value } (\\frac{i}{12} \\times 0.05) & p_{(i)} \\le \\text{Critical Value?} \\\\\n\\hline\n1 & 0.0009 & \\frac{1}{12} \\times 0.05 \\approx 0.00417 & \\text{Yes} \\\\\n2 & 0.0018 & \\frac{2}{12} \\times 0.05 \\approx 0.00833 & \\text{Yes} \\\\\n3 & 0.006 & \\frac{3}{12} \\times 0.05 = 0.0125 & \\text{Yes} \\\\\n4 & 0.010 & \\frac{4}{12} \\times 0.05 \\approx 0.01667 & \\text{Yes} \\\\\n5 & 0.017 & \\frac{5}{12} \\times 0.05 \\approx 0.02083 & \\text{Yes} \\\\\n6 & 0.026 & \\frac{6}{12} \\times 0.05 = 0.025 & \\text{No} \\\\\n7 & 0.039 & \\frac{7}{12} \\times 0.05 \\approx 0.02917 & \\text{No} \\\\\n8 & 0.055 & \\frac{8}{12} \\times 0.05 \\approx 0.03333 & \\text{No} \\\\\n9 & 0.071 & \\frac{9}{12} \\times 0.05 = 0.0375 & \\text{No} \\\\\n10 & 0.12 & \\frac{10}{12} \\times 0.05 \\approx 0.04167 & \\text{No} \\\\\n11 & 0.21 & \\frac{11}{12} \\times 0.05 \\approx 0.04583 & \\text{No} \\\\\n12 & 0.57 & \\frac{12}{12} \\times 0.05 = 0.05 & \\text{No}\n\\end{array}\n$\nThe largest rank $k$ for which $p_{(k)} \\le \\frac{k}{m}\\alpha$ is $k=5$. Therefore, the null hypotheses for the $5$ peaks with the smallest p-values are rejected.\n\nThe problem also requires computation of the Benjamini–Hochberg $q$-values. The $q$-value for the $i$-th ordered $p$-value, $q_{(i)}$, is defined to be the minimum FDR at which the test would be called significant. It is calculated as:\n$$\nq_{(i)} = \\min_{j=i, \\dots, m} \\left( \\frac{m \\cdot p_{(j)}}{j} \\right)\n$$\nThis ensures that the sequence of $q$-values is monotonically non-decreasing. A practical way to compute this is to first calculate an intermediate value $q'_{(i)} = \\frac{m \\cdot p_{(i)}}{i}$ for all ranks $i$, and then enforce monotonicity by starting from $q_{(m)} = q'_{(m)}$ and iterating backwards: $q_{(i)} = \\min(q'_{(i)}, q_{(i+1)})$.\n\nWe calculate the $q$-values for our data:\n$\n\\begin{array}{c|c|c|l}\n\\text{Rank } (i) & p_{(i)} & q'_{(i)} = \\frac{12 \\cdot p_{(i)}}{i} & q_{(i)} \\\\\n\\hline\n1 & 0.0009 & 0.0108 & 0.0108 \\\\\n2 & 0.0018 & 0.0108 & 0.0108 \\\\\n3 & 0.006 & 0.0240 & 0.0240 \\\\\n4 & 0.010 & 0.0300 & 0.0300 \\\\\n5 & 0.017 & 0.0408 & 0.0408 \\\\\n6 & 0.026 & 0.0520 & 0.0520 \\\\\n7 & 0.039 & \\approx 0.06686 & \\approx 0.06686 \\\\\n8 & 0.055 & 0.0825 & 0.0825 \\\\\n9 & 0.071 & \\approx 0.09467 & \\approx 0.09467 \\\\\n10 & 0.12 & 0.1440 & 0.1440 \\\\\n11 & 0.21 & \\approx 0.22909 & \\approx 0.22909 \\\\\n12 & 0.57 & 0.5700 & 0.5700\n\\end{array}\n$\n\nTo be precise about the monotonicity enforcement:\n$q_{(12)} = 0.57$\n$q_{(11)} = \\min(q'_{(11)}, q_{(12)}) = \\min(0.22909..., 0.57) \\approx 0.22909$\n$q_{(10)} = \\min(q'_{(10)}, q_{(11)}) = \\min(0.144, 0.22909...) = 0.144$\n...\n$q_{(6)} = \\min(q'_{(6)}, q_{(7)}) = \\min(0.052, 0.06686...) = 0.052$\n$q_{(5)} = \\min(q'_{(5)}, q_{(6)}) = \\min(0.0408, 0.052) = 0.0408$\n$q_{(4)} = \\min(q'_{(4)}, q_{(5)}) = \\min(0.030, 0.0408) = 0.030$\n$q_{(3)} = \\min(q'_{(3)}, q_{(4)}) = \\min(0.024, 0.030) = 0.024$\n$q_{(2)} = \\min(q'_{(2)}, q_{(3)}) = \\min(0.0108, 0.024) = 0.0108$\n$q_{(1)} = \\min(q'_{(1)}, q_{(2)}) = \\min(0.0108, 0.0108) = 0.0108$\n\nThe number of significant peaks is the number of tests for which $q_{(i)} \\le \\alpha = 0.05$.\n- $q_{(1)} = 0.0108 \\le 0.05$\n- $q_{(2)} = 0.0108 \\le 0.05$\n- $q_{(3)} = 0.0240 \\le 0.05$\n- $q_{(4)} = 0.0300 \\le 0.05$\n- $q_{(5)} = 0.0408 \\le 0.05$\n- $q_{(6)} = 0.0520 > 0.05$\n\nThe condition holds for the first $5$ peaks. The number of peaks retained as significant at an FDR of $0.05$ is $5$.", "answer": "$$\\boxed{5}$$", "id": "2938847"}]}