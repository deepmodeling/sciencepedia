{"hands_on_practices": [{"introduction": "Ecologists often construct static life tables from cross-sectional data, which provides a snapshot of a population's age structure at a single point in time. While convenient, this approach can be misleading if the population is not stationary. This exercise [@problem_id:2503649] guides you through a crucial derivation to understand why and how a static life table produces biased estimates of survivorship $l_x$ in a growing population (where the finite rate of increase $\\lambda \\gt 1$), a fundamental concept for anyone interpreting demographic data.", "problem": "A demographer studies a species with discrete, non-overlapping annual age classes $x \\in \\{0,1,2,\\dots,\\omega\\}$ in a large, density-independent, environmentally constant population that has converged to a stable age distribution with constant finite rate of increase $\\lambda>0$ (so that the per-time-step population size multiplies by $\\lambda$). Let the cohort survivorship schedule $l_x$ be the probability that a newborn survives to exact age $x$, with $l_0=1$. The demographer constructs a static (cross-sectional) life table at census time $t$ by counting the number $n_x(t)$ of individuals of each age $x$, and then estimates survivorship by normalizing counts so that $\\hat{l}_0^{\\mathrm{static}}=1$ and, for $x>0$, $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$.\n\nStarting only from the definitions of cohort survivorship $l_x$, stable exponential growth with rate $\\lambda$, and the fact that the number of age-$x$ individuals at time $t$ equals the number of births that occurred at time $t-x$ times the probability of surviving from birth to age $x$, derive how $\\hat{l}_x^{\\mathrm{static}}$ relates to $l_x$ and $\\lambda$. Use your derivation to determine the direction of bias in $\\hat{l}_x^{\\mathrm{static}}$ when $\\lambda>1$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In a stable population with constant $\\lambda>1$, a static life table that estimates $\\hat{l}_x^{\\mathrm{static}}$ by normalizing $n_x(t)$ so that $\\hat{l}_0^{\\mathrm{static}}=1$ yields $\\hat{l}_x^{\\mathrm{static}}=l_x\\,\\lambda^{-x}$; consequently, it underestimates $l_x$ for all $x>0$.\n\nB. When $\\lambda>1$, a static life table overestimates $l_x$ at larger ages $x$ because older individuals are overrepresented in the cross-section relative to a single cohort.\n\nC. If $\\lambda=1$, the static estimate is unbiased for all ages $x$, that is, $\\hat{l}_x^{\\mathrm{static}}=l_x$.\n\nD. For any $\\lambda\\neq 1$, the ratio $\\hat{l}_x^{\\mathrm{static}}/l_x$ is a constant independent of age $x$.\n\nE. The sign (direction) of the bias for $\\lambda>1$ depends on the age-specific fertility schedule $b_x$; with sufficiently increasing $b_x$, the static life table can be unbiased even if $\\lambda>1$.", "solution": "The problem statement will first be subjected to rigorous validation.\n\nStep 1: Extract Givens\n- Age classes are discrete, non-overlapping, and annual, denoted by $x \\in \\{0, 1, 2, \\dots, \\omega\\}$.\n- The population is large, density-independent, and in a constant environment.\n- The population has converged to a stable age distribution.\n- The population experiences a constant finite rate of increase $\\lambda > 0$.\n- Cohort survivorship, $l_x$, is the probability that a newborn survives to exact age $x$, with the normalization $l_0 = 1$.\n- A static (cross-sectional) life table is constructed at census time $t$ by counting the number of individuals of each age, $n_x(t)$.\n- The static survivorship estimate, $\\hat{l}_x^{\\mathrm{static}}$, is defined by normalizing counts such that $\\hat{l}_0^{\\mathrm{static}} = 1$ and for $x > 0$, $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$.\n- A key premise is given: \"the number of age-$x$ individuals at time $t$ equals the number of births that occurred at time $t-x$ times the probability of surviving from birth to age $x$.\"\n\nStep 2: Validate Using Extracted Givens\nThe problem is a standard exercise in mathematical demography, concerned with the relationship between cohort and static life tables.\n- **Scientifically Grounded:** The concepts of stable age distribution, survivorship schedules ($l_x$), and the finite rate of increase ($\\lambda$) are fundamental pillars of population ecology and demography, established by Lotka, Volterra, and others. The premises are sound.\n- **Well-Posed:** The problem provides sufficient definitions and a clear directive to derive a specific relationship and analyze its implications. The question is unambiguous and a unique solution can be derived from the premises.\n- **Objective:** The language is formal, precise, and devoid of subjectivity.\nThe problem statement is internally consistent, complete for the task at hand, and grounded in established scientific theory. There are no identifiable flaws.\n\nStep 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and evaluation.\n\nDerivation of the Relationship\nLet $B(t)$ represent the number of births (individuals of age $0$ before mortality) at the beginning of time interval $t$. Since the population is growing at a constant finite rate $\\lambda$, the number of births must also increase exponentially at this rate. We can write the number of births at time $t$ in terms of the number of births at some reference time, say $t=0$, as:\n$$B(t) = B(0)\\lambda^t$$\nThe number of individuals of exact age $x$ at a census time $t$, denoted $n_x(t)$, consists of those individuals who were born at time $t-x$ and have survived to age $x$. The probability of surviving from birth to age $x$ is given by the cohort survivorship, $l_x$. Therefore, as stated in the problem:\n$$n_x(t) = B(t-x) \\cdot l_x$$\nWe can express $B(t-x)$ in terms of the number of births at the current time, $B(t)$:\n$$B(t-x) = B(0)\\lambda^{t-x} = B(0)\\lambda^t \\lambda^{-x} = B(t) \\lambda^{-x}$$\nSubstituting this into the expression for $n_x(t)$, we get:\n$$n_x(t) = B(t) \\lambda^{-x} l_x$$\nThe problem states that the static life table survivorship, $\\hat{l}_x^{\\mathrm{static}}$, is estimated by normalizing the age counts $n_x(t)$ such that $\\hat{l}_0^{\\mathrm{static}} = 1$. The general form of this normalization is $\\hat{l}_x^{\\mathrm{static}} = \\frac{n_x(t)}{n_0(t)}$. Let us verify this satisfies the conditions.\nFor $x=0$, we have:\n$$\\hat{l}_0^{\\mathrm{static}} = \\frac{n_0(t)}{n_0(t)} = 1$$\nThis matches the condition $\\hat{l}_0^{\\mathrm{static}} = 1$. For $x>0$, this formulation means $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$ with a constant of proportionality $1/n_0(t)$, which is constant with respect to $x$ for a given census time $t$. So, this is the correct formulation.\n\nNow we substitute our derived expression for $n_x(t)$:\n$$\\hat{l}_x^{\\mathrm{static}} = \\frac{n_x(t)}{n_0(t)} = \\frac{B(t) \\lambda^{-x} l_x}{B(t) \\lambda^{-0} l_0}$$\nGiven that $l_0 = 1$ and $\\lambda^0 = 1$, the denominator is simply $B(t)$.\n$$\\hat{l}_x^{\\mathrm{static}} = \\frac{B(t) \\lambda^{-x} l_x}{B(t)} = l_x \\lambda^{-x}$$\nThis is the fundamental relationship between the static life table estimate and the true cohort survivorship in a population with a stable age distribution growing at rate $\\lambda$.\n\nAnalysis of Bias for $\\lambda > 1$\nThe bias is the difference between the estimate and the true value, or can be analyzed via their ratio. The ratio is:\n$$\\frac{\\hat{l}_x^{\\mathrm{static}}}{l_x} = \\lambda^{-x}$$\nIf $\\lambda > 1$, then for any age $x > 0$, we have $\\lambda^x > 1$. Consequently, $\\lambda^{-x} = \\frac{1}{\\lambda^x} < 1$.\nThis implies that $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x} < l_x$ for all $x > 0$.\nTherefore, for a growing population ($\\lambda > 1$), the static life table systematically **underestimates** the true cohort survivorship for all positive ages. The intuition is that a cross-section of a growing population over-represents the younger age classes (which come from larger, more recent birth cohorts) relative to the older age classes (which come from smaller, past birth cohorts). When normalizing by the number of individuals at age $0$ (the largest group), the relative numbers in all older age classes appear smaller, leading to an apparent survivorship that is lower than the true cohort survivorship.\n\nEvaluation of Options\n\nA. In a stable population with constant $\\lambda>1$, a static life table that estimates $\\hat{l}_x^{\\mathrm{static}}$ by normalizing $n_x(t)$ so that $\\hat{l}_0^{\\mathrm{static}}=1$ yields $\\hat{l}_x^{\\mathrm{static}}=l_x\\,\\lambda^{-x}$; consequently, it underestimates $l_x$ for all $x>0$.\nThis statement is a direct consequence of our derivation. The formula $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$ is what we derived. For $\\lambda > 1$ and $x > 0$, $\\lambda^{-x} < 1$, which means $\\hat{l}_x^{\\mathrm{static}} < l_x$, an underestimation. This statement is entirely correct.\n**Verdict: Correct**\n\nB. When $\\lambda>1$, a static life table overestimates $l_x$ at larger ages $x$ because older individuals are overrepresented in the cross-section relative to a single cohort.\nThis is incorrect on two counts. First, as derived, for $\\lambda > 1$ the static method *underestimates* $l_x$, it does not overestimate it. Second, the reasoning is false. In a growing population, it is the younger individuals from larger, recent cohorts who are overrepresented, not the older individuals from smaller, past cohorts.\n**Verdict: Incorrect**\n\nC. If $\\lambda=1$, the static estimate is unbiased for all ages $x$, that is, $\\hat{l}_x^{\\mathrm{static}}=l_x$.\nWe use our derived general formula $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$ and substitute $\\lambda=1$:\n$$\\hat{l}_x^{\\mathrm{static}} = l_x (1)^{-x} = l_x \\cdot 1 = l_x$$\nIn a stationary population ($\\lambda=1$), the static estimate is identical to the cohort survivorship. It is unbiased. This is because the size of birth cohorts is constant over time, so the age structure is directly proportional to the survivorship curve.\n**Verdict: Correct**\n\nD. For any $\\lambda\\neq 1$, the ratio $\\hat{l}_x^{\\mathrm{static}}/l_x$ is a constant independent of age $x$.\nThe ratio is $\\frac{\\hat{l}_x^{\\mathrm{static}}}{l_x} = \\lambda^{-x}$. As $x$ increases, this ratio changes (it decreases for $\\lambda > 1$ and increases for $0 < \\lambda < 1$). It is a function of age $x$ and is not a constant. The bias is age-dependent.\n**Verdict: Incorrect**\n\nE. The sign (direction) of the bias for $\\lambda>1$ depends on the age-specific fertility schedule $b_x$; with sufficiently increasing $b_x$, the static life table can be unbiased even if $\\lambda>1$.\nThe age-specific fertility schedule $b_x$ and survivorship $l_x$ together determine the value of $\\lambda$ through the Euler-Lotka characteristic equation ($1 = \\sum_x \\lambda^{-x} l_x b_x$). However, once the value of $\\lambda > 1$ is established and the population has reached a stable age distribution, the relationship we derived, $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$, holds irrespective of the specific $b_x$ schedule that gave rise to that $\\lambda$. The bias is determined by $\\lambda$, not directly by $b_x$. For any $\\lambda > 1$, there will be an underestimation. It is not possible for the estimate to be unbiased.\n**Verdict: Incorrect**", "answer": "$$\\boxed{AC}$$", "id": "2503649"}, {"introduction": "Capture-mark-recapture (CMR) studies are essential for estimating survival rates in wild, mobile populations, but a frequent complication is the presence of 'transient' individuals who emigrate permanently after their first capture. This practice [@problem_id:2503610] explores how this phenomenon, if unmodeled, leads to negatively biased estimates of the true survival probability $p_x$. You will reason through the logic of a powerful statistical solution that uses time-since-marking models to separate resident survival from the effects of transience, yielding more accurate life table parameters.", "problem": "A field ecologist aims to derive a stage-structured life table for a long-lived bird using capture-mark-recapture (CMR) data collected annually. The life table will include the age-specific survival probability from age $x$ to age $x+1$, denoted $p_x$, and survivorship $l_x$, with $p_x = l_{x+1}/l_x$ by definition. The CMR analysis is set up under the Cormack-Jolly-Seber (CJS) framework, in which the key estimands are the apparent survival probability and the recapture probability. Apparent survival conflates true survival and permanent emigration. Assume there is a nonzero fraction of transient individuals, defined as individuals that are available for capture at their first encounter but permanently emigrate from the study area immediately thereafter. Let the fraction of newly marked individuals at biological age $x$ that are transient be $\\pi_x \\in [0,1)$, and let the true biological survival from age $x$ to $x+1$ be $p_x \\in (0,1)$. Let the per-occasion recapture probability at time $t+1$ be $c_{t+1} \\in (0,1)$.\n\nFundamental definitions you may use:\n- By the life table identity, $p_x = l_{x+1}/l_x$.\n- Under the Cormack-Jolly-Seber (CJS) model with homogeneous parameters and no permanent emigration, the limiting estimator of apparent survival at a given interval equates the expected number of re-encounters to the observed number of re-encounters among the relevant marked-at-risk set.\n- Apparent survival is the probability of remaining alive and available for recapture in the study area between sampling occasions.\n\nAssume for this question that detection is homogeneous across individuals and that transience pertains only to availability (permanent emigration), not to detection given availability. Consider a cohort of $n_x$ individuals first captured and marked at biological age $x$ during occasion $t$. Among these, a fraction $\\pi_x$ are transients that immediately and permanently emigrate before occasion $t+1$. The remainder, a fraction $(1-\\pi_x)$, are residents.\n\nUsing only the core definitions above and basic probability, reason about how failing to model transience affects estimation of $p_x$ from CMR data that are analyzed under a standard homogeneous Cormack-Jolly-Seber (CJS) model that ignores transience. Then consider a $2$-age-class, time-since-marking state structure to correct the bias, where individuals in their first interval after initial marking are in class $1$ and all subsequent intervals are class $2+$.\n\nSelect all statements that are correct.\n\nA. If transients are present but unmodeled, the Cormack-Jolly-Seber (CJS) estimator of $p_x$ will be biased upward because transients cannot be recaptured, artificially inflating the proportion of survivors among those ever seen again.\n\nB. If a fraction $\\pi_x$ of newly marked individuals at age $x$ permanently emigrate immediately after first capture, then, under homogeneous detection and large $n_x$, the limiting Cormack-Jolly-Seber (CJS) estimator obtained from newly marked histories that ignores transience converges to $(1-\\pi_x)\\,p_x$, thereby biasing $p_x$ downward by the factor $(1-\\pi_x)$.\n\nC. A $2$-age-class time-since-marking model that allows different recapture probabilities $c_1$ (newly marked) and $c_2$ (previously marked) but constrains survival to be equal across classes removes the transience bias in the survival component because lower recapture of newly marked individuals can be fully explained by $c_1<c_2$.\n\nD. A valid $2$-age-class correction for transience specifies apparent survival $\\phi_1$ for the first post-marking interval and $\\phi_2$ for all subsequent intervals, with $\\phi_1$ permitted to differ from $\\phi_2$; if true biological survival is constant across classes, then $\\phi_1 = (1-\\pi)\\,\\phi_2$, and the life table survival $p_x$ should be identified with $\\phi_2$ rather than $\\phi_1$.\n\nE. To construct a corrected life table from CMR with transients, one should propagate survivorship using $l_{x+1} = l_x\\,\\hat{\\phi}_1$ because the estimate $\\hat{\\phi}_1$ is based on the largest number of individuals (all newly marked) and is therefore the most precise and least biased estimate of $p_x$.", "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Topic**: Stage-structured life table for a long-lived bird using capture-mark-recapture (CMR) data.\n-   **Life Table Parameters**:\n    -   $p_x$: age-specific survival probability from age $x$ to age $x+1$.\n    -   $l_x$: survivorship at age $x$.\n    -   Definition: $p_x = l_{x+1}/l_x$.\n-   **CMR Framework**: Cormack-Jolly-Seber (CJS) model.\n-   **CJS Estimands**: Apparent survival probability ($\\phi$) and recapture probability ($c$).\n-   **Apparent Survival Definition**: The probability of remaining alive and available for recapture in the study area. It convolutes true survival and permanent emigration.\n-   **Transience**: A nonzero fraction of individuals are transients.\n-   **Transient Definition**: Individuals available for capture at first encounter but permanently emigrate immediately thereafter.\n-   **Variables**:\n    -   $\\pi_x \\in [0,1)$: fraction of newly marked individuals at biological age $x$ that are transient.\n    -   $p_x \\in (0,1)$: true biological survival from age $x$ to $x+1$.\n    -   $c_{t+1} \\in (0,1)$: per-occasion recapture probability at time $t+1$.\n-   **Assumptions**:\n    -   Detection is homogeneous across individuals.\n    -   Transience pertains only to availability (permanent emigration).\n-   **Scenario**: A cohort of $n_x$ individuals is first captured at age $x$ during occasion $t$. A fraction $\\pi_x$ are transients, and $(1-\\pi_x)$ are residents.\n-   **Task**: Analyze how unmodeled transience affects the estimation of $p_x$ in a standard CJS model and how a $2$-age-class, time-since-marking model corrects the bias.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the established theoretical framework of capture-mark-recapture population modeling, a core sub-discipline of quantitative ecology.\n-   **Scientifically Grounded**: The concepts presented—Cormack-Jolly-Seber models, apparent survival, true survival, transience, time-since-marking effects, and life tables—are standard and fundamental in wildlife population dynamics. The formulation is entirely consistent with the established literature (e.g., works by Lebreton, Pradel, Spendelow). The problem does not violate any scientific or mathematical principles.\n-   **Well-Posed**: The problem is structured to have a unique logical conclusion based on the provided definitions. It asks for a deductive analysis of model bias and structure, which has a determinate answer.\n-   **Objective**: The language is precise and quantitative. All terms are either standard in the field or explicitly defined (e.g., \"transient\"). There is no subjectivity or ambiguity.\n-   **Completeness**: All necessary information and definitions required to reason about the effects of transience are provided. The distinction between apparent and true survival is clearly articulated, as is the behavior of transient individuals.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a standard, albeit conceptually demanding, problem in ecological statistics. I will now proceed to the solution.\n\n### Derivation and Analysis\n\nThe core of the problem lies in understanding the composition of the group of animals being modeled and how a statistical model that assumes homogeneity interprets data from a heterogeneous group.\n\nLet us consider a cohort of $n_x$ individuals newly marked at age $x$ during sampling occasion $t$. According to the problem, this cohort is a mixture of two distinct groups:\n1.  **Residents**: A fraction $(1-\\pi_x)$ of the cohort. These individuals remain within the study area (barring mortality). Their probability of surviving from occasion $t$ to $t+1$ is the true biological survival, $p_x$. Since they remain in the area, their apparent survival for the interval $(t, t+1)$ is equal to their true survival, $p_x$.\n2.  **Transients**: A fraction $\\pi_x$ of the cohort. These individuals permanently emigrate immediately after being marked at occasion $t$. By definition, they are not present in the study area at occasion $t+1$ or any subsequent occasion. Therefore, their probability of surviving *and* being available for recapture at $t+1$ is zero. Their apparent survival for the interval $(t, t+1)$ is $0$.\n\nA standard, homogeneous CJS model assumes all individuals in the cohort share the same apparent survival probability, let us call it $\\phi$. When this model is fitted to data from the mixed cohort, the maximum likelihood estimator for $\\phi$ will converge to the average apparent survival of the individuals in the cohort. This average is the weighted mean of the apparent survival probabilities of the two subgroups (residents and transients):\n\n$$ \\phi_{\\text{estimated}} = (1-\\pi_x) \\cdot (\\text{Apparent Survival of Residents}) + \\pi_x \\cdot (\\text{Apparent Survival of Transients}) $$\n$$ \\phi_{\\text{estimated}} = (1-\\pi_x) \\cdot p_x + \\pi_x \\cdot 0 $$\n$$ \\phi_{\\text{estimated}} = (1-\\pi_x)p_x $$\n\nThis estimated parameter, $(1-\\pi_x)p_x$, is what the naive CJS model would provide as the \"survival\" for this group. Since the true biological survival we wish to estimate is $p_x$, and we are given $\\pi_x \\in [0, 1)$, it follows that for any non-zero transience ($\\pi_x > 0$), we have $(1-\\pi_x) < 1$. Consequently, the estimate $(1-\\pi_x)p_x$ is strictly less than $p_x$. This represents a negative bias, or a downward bias.\n\nNow, let us consider the proposed correction: a $2$-age-class, time-since-marking model. This model structure allows parameters to differ based on the time since an individual was first marked.\n-   **Class 1**: The first interval immediately following an individual's initial marking. The apparent survival parameter for this class is denoted $\\phi_1$.\n-   **Class 2+**: All subsequent intervals (for individuals that have survived at least one interval post-marking). The apparent survival parameter for this class is denoted $\\phi_2$.\n\nThe cohort in **Class 1** is the mixed group of residents and transients. Based on our derivation above, the apparent survival for this group is $(1-\\pi_x)p_x$. Thus, the estimator $\\hat{\\phi}_1$ will converge to $(1-\\pi_x)p_x$.\n\nThe cohort in **Class 2+** consists only of individuals that were marked, survived at least one interval, and remained in the study area. By definition, transients leave immediately and permanently, so they can never enter Class 2+. Therefore, the group of animals analyzed for $\\phi_2$ consists exclusively of residents. For residents, the apparent survival is simply their true biological survival, $p_x$. Thus, the estimator $\\hat{\\phi}_2$ will converge to $p_x$.\n\nThis analysis provides the foundation for evaluating the given options.\n\n### Option-by-Option Analysis\n\n**A. If transients are present but unmodeled, the Cormack-Jolly-Seber (CJS) estimator of $p_x$ will be biased upward because transients cannot be recaptured, artificially inflating the proportion of survivors among those ever seen again.**\n\nThe CJS model estimates apparent survival. The presence of transients, which have an apparent survival of $0$, lowers the average apparent survival of the newly marked cohort to $(1-\\pi_x)p_x$. This value is less than the true survival $p_x$. Therefore, the estimator is biased downward, not upward. The reasoning provided is also fallacious: the fact that transients are never seen again means that out of a given number of marked individuals, fewer are ever re-encountered. A model assuming homogeneity interprets this as lower survival, not higher.\n\n**Verdict: Incorrect.**\n\n**B. If a fraction $\\pi_x$ of newly marked individuals at age $x$ permanently emigrate immediately after first capture, then, under homogeneous detection and large $n_x$, the limiting Cormack-Jolly-Seber (CJS) estimator obtained from newly marked histories that ignores transience converges to $(1-\\pi_x)\\,p_x$, thereby biasing $p_x$ downward by the factor $(1-\\pi_x)$.**\n\nThis statement is a direct consequence of our derivation. The naive CJS model averages the apparent survival of the heterogeneous initial cohort. The average is the weighted sum of resident survival ($p_x$ with weight $1-\\pi_x$) and transient survival ($0$ with weight $\\pi_x$), which yields $(1-\\pi_x)p_x$. As $(1-\\pi_x) \\le 1$, this constitutes a downward bias if $\\pi_x > 0$. The statement is logically and mathematically sound.\n\n**Verdict: Correct.**\n\n**C. A $2$-age-class time-since-marking model that allows different recapture probabilities $c_1$ (newly marked) and $c_2$ (previously marked) but constrains survival to be equal across classes removes the transience bias in the survival component because lower recapture of newly marked individuals can be fully explained by $c_1<c_2$.**\n\nThis describes a model with parameters $(\\phi, c_1, c_2)$. The transience effect is fundamentally a heterogeneity in *survival* for the first interval: some individuals have apparent survival $p_x$, others have $0$. This results in a lower average apparent survival, $(1-\\pi_x)p_x$, for the first interval compared to subsequent intervals ($p_x$). By constraining survival to be equal ($\\phi_1=\\phi_2=\\phi$), the model is misspecified. It is forced to estimate a single survival parameter $\\phi$ that represents a compromise between the lower first-interval survival and the higher subsequent-interval survival. This estimated $\\phi$ will still be biased low relative to the true resident survival $p_x$. While allowing $c_1 < c_2$ might improve model fit by attributing some of the missing individuals to lower trap response, it cannot eliminate the bias in the survival parameter because the primary signal is a deficit of surviving individuals, not just un-captured ones.\n\n**Verdict: Incorrect.**\n\n**D. A valid $2$-age-class correction for transience specifies apparent survival $\\phi_1$ for the first post-marking interval and $\\phi_2$ for all subsequent intervals, with $\\phi_1$ permitted to differ from $\\phi_2$; if true biological survival is constant across classes, then $\\phi_1 = (1-\\pi)\\,\\phi_2$, and the life table survival $p_x$ should be identified with $\\phi_2$ rather than $\\phi_1$.**\n\nThis describes the correct model structure to account for transience, a model with parameters $(\\phi_1, \\phi_2, ...)$. As derived, $\\hat{\\phi}_1$ estimates the apparent survival of the mixed initial cohort, so $\\phi_1 = (1-\\pi)p_x$ (using $\\pi$ for $\\pi_x$). $\\hat{\\phi}_2$ estimates the apparent survival of the resident-only group in subsequent intervals, so $\\phi_2 = p_x$. Substituting $\\phi_2 = p_x$ into the expression for $\\phi_1$ gives $\\phi_1 = (1-\\pi)\\phi_2$. This relationship is correct. The parameter of interest for the life table is the true biological survival of residents, which is $p_x$. This is correctly estimated by $\\phi_2$. Therefore, the life table should be constructed using $\\hat{\\phi}_2$. The statement is entirely correct in its description of the model, the parameter relationships, and their interpretation.\n\n**Verdict: Correct.**\n\n**E. To construct a corrected life table from CMR with transients, one should propagate survivorship using $l_{x+1} = l_x\\,\\hat{\\phi}_1$ because the estimate $\\hat{\\phi}_1$ is based on the largest number of individuals (all newly marked) and is therefore the most precise and least biased estimate of $p_x$.**\n\nThis statement recommends using $\\hat{\\phi}_1$ for the life table. As established in the analysis for options B and D, $\\hat{\\phi}_1$ is an estimator for $(1-\\pi_x)p_x$, which is a negatively biased estimate of the true survival $p_x$. The reasoning provided confuses statistical precision (low variance) with accuracy (low bias). While $\\hat{\\phi}_1$ might be more precise because it is based on the initial, often largest, cohort, it is fundamentally biased. The \"least biased\" estimate for $p_x$ is $\\hat{\\phi}_2$. Using a biased parameter, no matter how precise, will lead to an incorrect life table.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "2503610"}, {"introduction": "In many ecological studies, we cannot track every individual until its death, resulting in 'right-censored' data where we only know an individual was alive up to a certain time. This hands-on coding challenge [@problem_id:2503639] equips you to tackle this common issue by fitting parametric survival models such as the Weibull and Gompertz. You will learn to construct the appropriate log-likelihood function that incorporates both observed events and censored observations, and then apply model selection using the Akaike Information Criterion ($AIC$) to identify the best-fitting survivorship pattern.", "problem": "You are given three right-censored cohort datasets of survival times measured in years, each representing a hypothetical species with different survivorship patterns. Your task is to implement maximum likelihood estimation for three parametric hazard models and perform model selection via the Akaike Information Criterion (AIC).\n\nFundamental base and definitions to be used:\n- The hazard function $h(t)$ is defined as $h(t) = \\lim_{\\Delta t \\to 0^{+}} \\dfrac{\\mathbb{P}(t \\le T < t + \\Delta t \\mid T \\ge t)}{\\Delta t}$, where $T$ is the non-negative random time-to-event.\n- The survival function $S(t)$ is $S(t) = \\mathbb{P}(T \\ge t)$ and satisfies $S(t) = \\exp\\!\\left(-\\int_{0}^{t} h(u)\\,du\\right)$.\n- The density function is $f(t) = -\\dfrac{d}{dt} S(t) = h(t)\\,S(t)$ for $t > 0$.\n- For right-censored observations at time $c$, only the information that $T \\ge c$ is observed. You must incorporate this censoring correctly in the likelihood using the definitions above.\n\nModels to be fitted to each dataset:\n- Exponential hazard model with constant hazard $h(t)$.\n- Weibull hazard model with a monotone hazard depending on time $t$.\n- Gompertz hazard model with an exponentially changing hazard in $t$.\n\nFor each dataset, you must:\n1. Derive and implement the log-likelihood under right-censoring using only the definitions above.\n2. Maximize the log-likelihood for each model to obtain the maximum likelihood estimates of the model parameters. Use numerically stable parameterizations suitable for constrained parameters (for example, parameters restricted to be positive) and handle limiting cases correctly (for example, the limiting case that reconciles an exponentially changing hazard with a constant hazard).\n3. Compute the Akaike Information Criterion $AIC = 2p - 2\\widehat{\\ell}$, where $p$ is the number of free parameters in the model and $\\widehat{\\ell}$ is the maximized log-likelihood.\n4. Select the model with the minimum $AIC$ for each dataset.\n\nRight-censoring is present as indicated by an event indicator array. For each observation $i$, $d_i = 1$ indicates an observed event (death), and $d_i = 0$ indicates a right-censored time.\n\nUse the following test suite (all times are in years):\n- Test case $1$ (moderate sample, mixed events and censoring, approximately constant hazard):\n  - Times $t$: $[\\,0.2,\\,0.6,\\,1.0,\\,1.4,\\,2.1,\\,2.5,\\,3.0,\\,3.7,\\,4.2,\\,4.9,\\,5.5,\\,6.1,\\,6.8,\\,7.4,\\,8.0,\\,9.0\\,]$\n  - Events $d$: $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n- Test case $2$ (increasing hazard consistent with aging, more events at larger $t$):\n  - Times $t$: $[\\,0.5,\\,0.8,\\,1.0,\\,1.2,\\,1.6,\\,2.0,\\,2.4,\\,3.0,\\,3.6,\\,4.2,\\,5.0,\\,5.8\\,]$\n  - Events $d$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- Test case $3$ (edge case with heavy right-censoring at early ages and concentration of events at large $t$):\n  - Times $t$: $[\\,0.5,\\,1.0,\\,2.0,\\,3.5,\\,5.0,\\,7.0,\\,9.5,\\,12.0,\\,15.0,\\,18.0\\,]$\n  - Events $d$: $[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n\nImplementation and output requirements:\n- Your program must estimate parameters via maximum likelihood for each model and dataset, compute $AIC$ for each model, and select the model with minimum $AIC$.\n- Use the following model indexing convention in your final output: $0$ for Exponential, $1$ for Weibull, $2$ for Gompertz.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each entry is the selected model index for the corresponding test case.\n- Units: All times are in years. No other unit conversions are required. The final outputs are integers and therefore unitless.\n\nYour program must be a complete, runnable implementation that does not require any user input. It must use numerically stable methods, including appropriate parameter transformations for positivity constraints and careful handling of small-parameter limits. The final output must be a single line as specified. No other text should be printed.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Hazard function**: $h(t) = \\lim_{\\Delta t \\to 0^{+}} \\dfrac{\\mathbb{P}(t \\le T < t + \\Delta t \\mid T \\ge t)}{\\Delta t}$, where $T$ is a non-negative random time-to-event variable.\n- **Survival function**: $S(t) = \\mathbb{P}(T \\ge t)$ and $S(t) = \\exp\\!\\left(-\\int_{0}^{t} h(u)\\,du\\right)$.\n- **Probability density function**: $f(t) = h(t)\\,S(t)$ for $t > 0$.\n- **Right-censoring**: An observation at time $c$ indicates that the event time $T$ satisfies $T \\ge c$. The data for each observation $i$ consists of a time $t_i$ and an event indicator $d_i$, where $d_i=1$ for an observed event and $d_i=0$ for right-censoring.\n- **Models for fitting**:\n    1.  Exponential hazard model (constant hazard).\n    2.  Weibull hazard model (monotone hazard).\n    3.  Gompertz hazard model (exponentially changing hazard).\n- **Tasks**:\n    1.  Derive and implement the log-likelihood function for right-censored data.\n    2.  Find Maximum Likelihood Estimates (MLE) for model parameters.\n    3.  Compute the Akaike Information Criterion (AIC): $AIC = 2p - 2\\widehat{\\ell}$, where $p$ is the number of free parameters and $\\widehat{\\ell}$ is the maximized log-likelihood.\n    4.  Select the model with the minimum AIC for each dataset.\n- **Data sets**:\n    - **Test case 1**:\n        - Times $t$: $[\\,0.2,\\,0.6,\\,1.0,\\,1.4,\\,2.1,\\,2.5,\\,3.0,\\,3.7,\\,4.2,\\,4.9,\\,5.5,\\,6.1,\\,6.8,\\,7.4,\\,8.0,\\,9.0\\,]$\n        - Events $d$: $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n    - **Test case 2**:\n        - Times $t$: $[\\,0.5,\\,0.8,\\,1.0,\\,1.2,\\,1.6,\\,2.0,\\,2.4,\\,3.0,\\,3.6,\\,4.2,\\,5.0,\\,5.8\\,]$\n        - Events $d$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n    - **Test case 3**:\n        - Times $t$: $[\\,0.5,\\,1.0,\\,2.0,\\,3.5,\\,5.0,\\,7.0,\\,9.5,\\,12.0,\\,15.0,\\,18.0\\,]$\n        - Events $d$: $[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- **Output Requirements**: A single-line list of integer model indices ($0$: Exponential, $1$: Weibull, $2$: Gompertz) corresponding to the best model for each test case.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of survival analysis, a core topic in biostatistics and reliability engineering. The definitions of hazard, survival, and likelihood functions are standard. The models—Exponential, Weibull, and Gompertz—are canonical parametric survival models. The task of deriving the likelihood, performing MLE, and using AIC for model selection constitutes a complete and standard statistical procedure. The provided datasets are self-contained and numerically reasonable. There are no contradictions, ambiguities, or violations of scientific or mathematical principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation and Methodology\nThe core of the problem is the formulation and maximization of the likelihood function under right-censoring. For a sample of $N$ individuals, let the data for individual $i$ be $(t_i, d_i)$, where $t_i$ is the time of event or censoring, and $d_i$ is the event indicator ($d_i=1$ if the event is observed, $d_i=0$ if censored).\n\nThe likelihood contribution of an individual $i$ is the probability density function $f(t_i)$ if the event is observed, and the survival function $S(t_i)$ if the observation is censored. The total likelihood over the sample is the product of these contributions:\n$$ L(\\boldsymbol{\\theta}) = \\prod_{i=1}^{N} [f(t_i; \\boldsymbol{\\theta})]^{d_i} [S(t_i; \\boldsymbol{\\theta})]^{1-d_i} $$\nwhere $\\boldsymbol{\\theta}$ is the vector of model parameters. Using the relations $f(t) = h(t)S(t)$ and $S(t) = \\exp(-H(t))$, where $H(t) = \\int_0^t h(u)du$ is the cumulative hazard function, we can rewrite the likelihood. It is more convenient to work with the log-likelihood, $\\ell(\\boldsymbol{\\theta}) = \\log L(\\boldsymbol{\\theta})$:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log f(t_i; \\boldsymbol{\\theta}) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log(h(t_i; \\boldsymbol{\\theta})S(t_i; \\boldsymbol{\\theta})) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) + d_i \\log S(t_i; \\boldsymbol{\\theta}) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) + \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\nSubstituting $\\log S(t) = -H(t)$, we arrive at the final form used for computation:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) - H(t_i; \\boldsymbol{\\theta}) \\right] $$\nWe now define the specific forms of $h(t)$ and $H(t)$ for each model and discuss their numerical implementation. Maximum likelihood estimates $\\widehat{\\boldsymbol{\\theta}}$ are found by numerically minimizing the negative log-likelihood, $-\\ell(\\boldsymbol{\\theta})$.\n\n**1. Exponential Model ($p=1$)**\n- Hazard function: $h(t; \\lambda) = \\lambda$, with $\\lambda > 0$.\n- Cumulative hazard: $H(t; \\lambda) = \\int_0^t \\lambda \\,du = \\lambda t$.\n- Log-likelihood: $\\ell(\\lambda) = \\sum_{i=1}^{N} [d_i \\log(\\lambda) - \\lambda t_i]$.\n- Parameterization: To enforce the constraint $\\lambda > 0$, we re-parameterize using $\\lambda = e^{\\alpha}$. The objective function is then minimized with respect to the unconstrained parameter $\\alpha \\in \\mathbb{R}$.\n\n**2. Weibull Model ($p=2$)**\n- Hazard function: $h(t; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{t}{\\lambda}\\right)^{k-1}$, with shape $k > 0$ and scale $\\lambda > 0$.\n- Cumulative hazard: $H(t; k, \\lambda) = \\left(\\frac{t}{\\lambda}\\right)^k$.\n- Log-likelihood: $\\ell(k, \\lambda) = \\sum_{i=1}^{N} \\left[ d_i \\left(\\log(k) - k\\log(\\lambda) + (k-1)\\log(t_i)\\right) - \\left(\\frac{t_i}{\\lambda}\\right)^k \\right]$. All $t_i > 0$ in the data, so $\\log(t_i)$ is well-defined.\n- Parameterization: We use $k = e^{\\alpha_1}$ and $\\lambda = e^{\\alpha_2}$ to enforce positivity, optimizing over unconstrained $\\alpha_1, \\alpha_2 \\in \\mathbb{R}$.\n\n**3. Gompertz Model ($p=2$)**\n- Hazard function: $h(t; a, b) = a e^{bt}$, with initial hazard $a > 0$ and shape parameter $b \\in \\mathbb{R}$.\n- Cumulative hazard: For $b \\neq 0$, $H(t; a, b) = \\int_0^t a e^{bu} \\,du = \\frac{a}{b}(e^{bt} - 1)$.\n- Limiting case: As $b \\to 0$, $h(t) \\to a$ and $H(t) \\to at$. The Gompertz model smoothly converges to the Exponential model with rate $\\lambda = a$.\n- Log-likelihood: $\\ell(a, b) = \\sum_{i=1}^{N} \\left[ d_i (\\log(a) + b t_i) - \\frac{a}{b}(e^{bt_i} - 1) \\right]$.\n- Parameterization: We use $a = e^{\\alpha_1}$ and $b = \\alpha_2$, optimizing over $\\alpha_1, \\alpha_2 \\in \\mathbb{R}$. To ensure numerical stability for $b \\approx 0$, the term $\\frac{e^{bt_i}-1}{b}$ is computed using a function equivalent to `expm1(x)/x`, which correctly evaluates to its limit $t_i$ as $b t_i \\to 0$.\n\n**Model Selection**\nFor each model, after finding the MLEs $\\widehat{\\boldsymbol{\\theta}}$, we compute the maximized log-likelihood $\\widehat{\\ell} = \\ell(\\widehat{\\boldsymbol{\\theta}})$. The Akaike Information Criterion is then calculated:\n$$ AIC = 2p - 2\\widehat{\\ell} $$\nwhere $p$ is the number of parameters ($1$ for Exponential, $2$ for Weibull, $2$ for Gompertz). The model with the lowest AIC value is selected as the best-fitting model for the given dataset, balancing goodness-of-fit with model complexity.\n\nThe implementation will proceed by defining a negative log-likelihood function for each model, using the specified parameterizations. An optimization routine (`scipy.optimize.minimize`) will be used to find the parameters that minimize these functions for each dataset. Finally, the AIC values will be compared to select the optimal model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Fits Exponential, Weibull, and Gompertz survival models to three\n    right-censored datasets, selecting the best model for each dataset\n    using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0.2, 0.6, 1.0, 1.4, 2.1, 2.5, 3.0, 3.7, 4.2, 4.9, 5.5, 6.1, 6.8, 7.4, 8.0, 9.0]),\n            np.array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n        ),\n        (\n            np.array([0.5, 0.8, 1.0, 1.2, 1.6, 2.0, 2.4, 3.0, 3.6, 4.2, 5.0, 5.8]),\n            np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n        ),\n        (\n            np.array([0.5, 1.0, 2.0, 3.5, 5.0, 7.0, 9.5, 12.0, 15.0, 18.0]),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n        )\n    ]\n\n    results = []\n    \n    # Model indexing: 0: Exponential, 1: Weibull, 2: Gompertz\n    model_indices = {'exp': 0, 'weibull': 1, 'gompertz': 2}\n    \n    # --- Negative Log-Likelihood Functions ---\n\n    def neg_log_lik_exp(params, t, d):\n        \"\"\"Negative log-likelihood for the Exponential model.\"\"\"\n        log_lambda = params[0]\n        lambda_ = np.exp(log_lambda)\n        \n        log_h = log_lambda\n        H = lambda_ * t\n        \n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    def neg_log_lik_weibull(params, t, d):\n        \"\"\"Negative log-likelihood for the Weibull model.\"\"\"\n        log_k, log_lambda = params\n        k = np.exp(log_k)\n        lambda_ = np.exp(log_lambda)\n        \n        # log(t) is safe as all t > 0 in the provided data.\n        log_t = np.log(t)\n        \n        log_h = log_k + (k - 1) * log_t - k * log_lambda\n        H = np.exp(k * (log_t - log_lambda))\n\n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    def neg_log_lik_gompertz(params, t, d):\n        \"\"\"Negative log-likelihood for the Gompertz model.\"\"\"\n        log_a, b = params\n        a = np.exp(log_a)\n        \n        log_h = log_a + b * t\n        \n        # Numerically stable computation for H(t)\n        if np.abs(b)  1e-9:\n            H = a * t\n        else:\n            H = a * np.expm1(b * t) / b\n            \n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    for t_data, d_data in test_cases:\n        \n        # --- Initial Guesses for Optimization ---\n        # A good initial guess is the MLE for the exponential model,\n        # which is a special case of both Weibull (k=1) and Gompertz (b=0).\n        sum_d = np.sum(d_data)\n        sum_t = np.sum(t_data)\n        \n        # Handle case where there are no events to avoid division by zero.\n        if sum_d == 0:\n            lambda_init = 1.0 / np.mean(t_data) if len(t_data) > 0 else 1.0\n        else:\n             lambda_init = sum_d / sum_t\n        \n        # [log(lambda)]\n        params0_exp = [np.log(lambda_init)]\n        # [log(k), log(lambda_weibull)], init k=1, lambda_weibull=1/lambda_exp\n        params0_weibull = [0.0, -np.log(lambda_init)]\n        # [log(a), b], init a=lambda_exp, b=0\n        params0_gompertz = [np.log(lambda_init), 0.0]\n\n        # --- Optimization ---\n        # Exponential\n        res_exp = minimize(neg_log_lik_exp, params0_exp, args=(t_data, d_data), method='Nelder-Mead')\n        # Weibull\n        res_weibull = minimize(neg_log_lik_weibull, params0_weibull, args=(t_data, d_data), method='Nelder-Mead')\n        # Gompertz\n        res_gompertz = minimize(neg_log_lik_gompertz, params0_gompertz, args=(t_data, d_data), method='Nelder-Mead')\n\n        # --- AIC Calculation ---\n        # AIC = 2*p - 2*logL, where logL = -min_val_of_neg_log_lik\n        p_exp = 1\n        aic_exp = 2 * p_exp + 2 * res_exp.fun\n\n        p_weibull = 2\n        aic_weibull = 2 * p_weibull + 2 * res_weibull.fun\n        \n        p_gompertz = 2\n        aic_gompertz = 2 * p_gompertz + 2 * res_gompertz.fun\n        \n        # --- Model Selection ---\n        aics = [aic_exp, aic_weibull, aic_gompertz]\n        best_model_index = np.argmin(aics)\n        results.append(best_model_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2503639"}]}