{"hands_on_practices": [{"introduction": "In ecological fieldwork, it is a rare luxury to detect every marked individual after a dispersal event. More often, our ability to find an individual declines the farther it has moved. This hands-on exercise [@problem_id:2480606] delves into this critical issue of imperfect detection, guiding you to understand why it systematically biases dispersal estimates and how to correct for it by deriving the proper conditional likelihood. Mastering this concept is a foundational step toward robustly analyzing real-world movement data.", "problem": "A field experiment releases a cohort of marked individuals at a single origin and later recovers a subset using a spatial detection system. Let the true dispersal distance of an individual from the release point be a random variable with radial probability density function (pdf) $k_{r}(r \\mid \\theta)$ on $r \\in [0,\\infty)$, where $\\theta$ are parameters of biological interest describing the dispersal kernel. Assume that, conditional on having dispersed a distance $r$, the individual is detected with probability $g(r)$, where $g(r)$ is a known detection function that is nonincreasing in $r$. Suppose $N$ individuals are released, $n$ are detected, and the observed distances for the detected individuals are $r_{1},\\dots,r_{n}$. You will estimate $\\theta$ using only the distances for the detected individuals, conditioning on the event of detection and treating $g(r)$ as known.\n\nFrom first principles of conditional probability and independent and identically distributed (i.i.d.) sampling, explain why fitting $k_{r}(r \\mid \\theta)$ directly to the empirical distribution of $r_{1},\\dots,r_{n}$ without accounting for $g(r)$ will generally bias estimates toward shorter dispersal distances when $g(r)$ decreases with $r$. Then, select the option that provides a correctly normalized conditional likelihood for $\\theta$ given $r_{1},\\dots,r_{n}$ that incorporates $g(r)$.\n\nWhich option is correct?\n\nA. Ignoring $g(r)$ fits the empirical distribution of $r$ to the thinned sample in which long distances are underrepresented because they are less likely to be detected when $g(r)$ decreases, shifting mass toward small $r$. The correct conditional likelihood is\n$$\nL(\\theta \\mid r_{1:n},\\mathrm{detected}) \\;=\\; \\prod_{i=1}^{n} \\frac{k_{r}(r_{i}\\mid \\theta)\\, g(r_{i})}{\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, \\mathrm{d}u}.\n$$\n\nB. Ignoring $g(r)$ biases estimates toward longer distances because undetected short moves are missed when $g(r)$ decreases. The correct conditional likelihood upweights long distances using inverse detection as\n$$\nL(\\theta \\mid r_{1:n},\\mathrm{detected}) \\;=\\; \\prod_{i=1}^{n} \\frac{k_{r}(r_{i}\\mid \\theta)/ g(r_{i})}{\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)/ g(u)\\, \\mathrm{d}u}.\n$$\n\nC. Ignoring $g(r)$ has no systematic effect on estimates because detection is independent of the kernel parameters $\\theta$. The correct conditional likelihood simply multiplies by detection at the observations as\n$$\nL(\\theta \\mid r_{1:n},\\mathrm{detected}) \\;=\\; \\prod_{i=1}^{n} k_{r}(r_{i}\\mid \\theta)\\, g(r_{i}).\n$$\n\nD. Ignoring $g(r)$ underestimates long-distance dispersal, but the correct conditional likelihood must account for the circumference of annuli, giving\n$$\nL(\\theta \\mid r_{1:n},\\mathrm{detected}) \\;=\\; \\prod_{i=1}^{n} \\frac{k_{r}(r_{i}\\mid \\theta)\\, g(r_{i})\\, 2\\pi r_{i}}{\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, 2\\pi u\\, \\mathrm{d}u}.\n$$", "solution": "We begin from first principles: conditional probability and the law of total probability. For an individual with true dispersal distance $R$ having radial pdf $k_{r}(r\\mid \\theta)$ and detection probability $g(r)$ given $R=r$, the joint probability for the event $\\{R \\in [r,r+\\mathrm{d}r), \\mathrm{detected}\\}$ is\n$$\n\\Pr\\{R \\in [r,r+\\mathrm{d}r), \\mathrm{detected} \\mid \\theta\\} \\;=\\; k_{r}(r\\mid \\theta)\\, \\mathrm{d}r \\cdot g(r).\n$$\nThe marginal probability of detection under $\\theta$ is obtained by integrating over all possible distances,\n$$\n\\Pr\\{\\mathrm{detected} \\mid \\theta\\} \\;=\\; \\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, \\mathrm{d}u.\n$$\nTherefore, by the definition of conditional probability, the conditional pdf of $R$ among detected individuals is\n$$\nf_{\\mathrm{obs}}(r \\mid \\mathrm{detected}, \\theta) \\;=\\; \\frac{\\Pr\\{R \\in [r,r+\\mathrm{d}r), \\mathrm{detected} \\mid \\theta\\}}{\\Pr\\{\\mathrm{detected} \\mid \\theta\\}\\, \\mathrm{d}r} \\;=\\; \\frac{k_{r}(r\\mid \\theta)\\, g(r)}{\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, \\mathrm{d}u}.\n$$\nGiven $n$ detected individuals whose distances $r_{1},\\dots,r_{n}$ are conditionally independent and identically distributed (i.i.d.) draws from $f_{\\mathrm{obs}}(r \\mid \\mathrm{detected}, \\theta)$, the conditional likelihood for $\\theta$ is\n$$\nL(\\theta \\mid r_{1:n}, \\mathrm{detected}) \\;=\\; \\prod_{i=1}^{n} f_{\\mathrm{obs}}(r_{i} \\mid \\mathrm{detected}, \\theta) \\;=\\; \\prod_{i=1}^{n} \\frac{k_{r}(r_{i}\\mid \\theta)\\, g(r_{i})}{\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, \\mathrm{d}u}.\n$$\n\nThis derivation clarifies the source of bias if one ignores $g(r)$. If $g(r)$ is nonincreasing in $r$ (for example, detection becomes harder with distance from the release point or detector array), then the observed sample of distances is distributed according to $k_{r}(r\\mid \\theta)\\, g(r)$ up to normalization. Because $g(r)$ downweights larger $r$, the observed distribution is shifted toward smaller $r$ relative to the true $k_{r}(r\\mid \\theta)$. Fitting $k_{r}(r\\mid \\theta)$ directly to the empirical distribution of $r_{1},\\dots,r_{n}$ without accounting for $g(r)$ is therefore equivalent to fitting to a systematically downweighted version of the truth, which will generally bias estimates toward shorter dispersal distances.\n\nWe now evaluate each option:\n\nA. This option gives the correct qualitative explanation: when $g(r)$ decreases with $r$, longer moves are less likely to be detected, so the observed distances are biased toward short $r$. The likelihood presented matches the conditional likelihood derived above: each observed $r_{i}$ contributes a factor $k_{r}(r_{i}\\mid \\theta) g(r_{i})$ divided by the normalizing constant $\\int_{0}^{\\infty} k_{r}(u\\mid \\theta) g(u)\\, \\mathrm{d}u$, ensuring the conditional density integrates to $1$. Verdict — Correct.\n\nB. This option incorrectly claims bias toward longer distances under decreasing $g(r)$, which contradicts the thinning logic. The proposed likelihood also inappropriately uses inverse detection weighting $1/g(r)$ within a likelihood for conditionally detected data; the detected-sample density is proportional to $k_{r}(r\\mid \\theta)\\, g(r)$, not $k_{r}(r\\mid \\theta)/g(r)$. Moreover, inverse weighting can lead to an improper normalizing integral if $g(r)$ becomes small. Verdict — Incorrect.\n\nC. This option claims there is no systematic effect, which is false: the observed distribution differs from the true distribution by the factor $g(r)$ when conditioning on detection. The likelihood omits the crucial normalizing denominator $\\int_{0}^{\\infty} k_{r}(u\\mid \\theta)\\, g(u)\\, \\mathrm{d}u$, which depends on $\\theta$ and thus cannot be dropped when comparing parameter values. Verdict — Incorrect.\n\nD. This option correctly notes underestimation of long-distance dispersal, but introduces a factor $2\\pi r$ that is inappropriate given the definition of $k_{r}(r\\mid \\theta)$ as a radial pdf for distance. The factor $2\\pi r$ arises when transforming from an isotropic displacement vector density $K(\\mathbf{x}\\mid \\theta)$ on the plane to the marginal distance density, because $\\mathrm{d}\\mathbf{x}$ integrates over circles of circumference $2\\pi r$. However, once $k_{r}(r\\mid \\theta)$ is defined as the pdf of $R$ itself, the $2\\pi r$ factor is already absorbed into $k_{r}(r\\mid \\theta)$ and must not be reintroduced. Verdict — Incorrect.\n\nTherefore, only option A is correct.", "answer": "$$\\boxed{A}$$", "id": "2480606"}, {"introduction": "Movement is rarely random; landscape features like rivers, ridges, or resource gradients often channel it, creating non-uniform, or anisotropic, dispersal patterns. This computational practice [@problem_id:2480590] bridges statistical theory and practical application by tasking you with implementing a Likelihood Ratio Test. By analyzing simulated two-dimensional displacement data, you will learn how to statistically test the fundamental hypothesis of isotropy versus anisotropy, a key step in linking movement patterns to environmental structure.", "problem": "You are studying two-dimensional animal displacement data in movement ecology, where one-step displacements are modeled by a Gaussian dispersal kernel. Assume that each observed displacement vector is an independent draw from a multivariate normal distribution with unknown mean and unknown covariance. You are to fit an anisotropic Gaussian kernel by maximum likelihood and then test the hypothesis of isotropy.\n\nFundamental base and definitions to be used:\n- The multivariate normal density in dimension $d$ with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ is a well-tested model for dispersal steps. Its likelihood for independent observations follows from the product of densities and its log-likelihood is additive.\n- For independent samples from a multivariate normal distribution, the maximum likelihood estimators for the mean and covariance exist and are obtained by optimizing the log-likelihood under positive definiteness constraints. The Likelihood Ratio Test (LRT) uses the ratio of maximized likelihoods under a null hypothesis and an alternative hypothesis, and Wilks's theorem provides the asymptotic null distribution of the test statistic.\n\nMathematical setup:\n- You observe $n$ two-dimensional displacement vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$ with $\\mathbf{x}_i \\in \\mathbb{R}^2$, assumed independent and identically distributed as $\\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n- The alternative hypothesis allows an unconstrained symmetric positive definite covariance $\\boldsymbol{\\Sigma}$, representing an anisotropic Gaussian dispersal kernel.\n- The null hypothesis of isotropy constrains the covariance to be a scalar multiple of the identity, $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}_2$.\n\nTasks:\n1. Given simulated data sets (specified below), estimate the mean $\\boldsymbol{\\mu}$ and the covariance matrix $\\boldsymbol{\\Sigma}$ by maximum likelihood under the alternative. Separately, estimate $\\boldsymbol{\\mu}$ and $\\sigma^2$ by maximum likelihood under the null.\n2. Compute the Likelihood Ratio Test statistic $T = 2\\big(\\ell_{\\text{alt}} - \\ell_{\\text{null}}\\big)$, where $\\ell_{\\text{alt}}$ and $\\ell_{\\text{null}}$ denote the maximized log-likelihoods under the alternative and null models, respectively. Under the null hypothesis, and for large $n$, $T$ is approximately $\\chi^2_k$ distributed with $k$ equal to the difference in the number of free covariance parameters between the two models. In two dimensions, $k = 2$.\n3. Compute the $p$-value for each data set as the upper tail probability of the $\\chi^2_k$ distribution evaluated at the observed $T$.\n\nAngle unit requirement:\n- Any rotation angle introduced below is in radians.\n\nUnits:\n- There are no physical units to report. All outputs are unitless probabilities.\n\nTest suite (data-generating mechanisms to be used by your program; each data set is generated by a fixed random seed for reproducibility):\nAll data are two-dimensional ($d = 2$), with zero-mean truth $\\boldsymbol{\\mu} = \\mathbf{0}$, and sample sizes and covariance structures as follows. In each case, draw $n$ samples from $\\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$ using the specified seed.\n\n- Case A (isotropic, large sample): $n = 500$, seed $= 17$, $\\boldsymbol{\\Sigma}_{\\text{true}} = 4 \\mathbf{I}_2$.\n- Case B (axis-aligned anisotropy, large sample): $n = 500$, seed $= 29$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathrm{diag}(9, 1)$.\n- Case C (rotated anisotropy, large sample): $n = 500$, seed $= 97$. Let $\\theta = \\pi/6$ (in radians). Define the rotation matrix $\\mathbf{R}(\\theta) = \\begin{pmatrix}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta\\end{pmatrix}$ and the axis-aligned covariance $\\mathbf{D} = \\mathrm{diag}(4, 1)$. Set $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathbf{R}(\\theta)\\,\\mathbf{D}\\,\\mathbf{R}(\\theta)^\\top$.\n- Case D (mild anisotropy, moderate sample): $n = 200$, seed $= 1234$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathrm{diag}(1.2, 0.8)$.\n- Case E (isotropic, smaller sample): $n = 50$, seed $= 2021$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathbf{I}_2$.\n\nImplementation and output requirements:\n- Your program must internally generate the data sets exactly as specified using the seeds above.\n- Use only the principle-based maximum likelihood estimators derived from the multivariate normal model, not ad hoc heuristics.\n- For each case, compute the $p$-value for the isotropy test based on the $\\chi^2_2$ reference distribution applied to the Likelihood Ratio Test statistic $T$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be the five $p$-values corresponding to Cases A through E, each rounded to exactly six digits after the decimal point, in order A, B, C, D, E. For example, an output with placeholder values would look like \"[0.123456,0.000321,0.543210,0.010000,0.999999]\".", "solution": "The problem requires performing a statistical test for isotropy of a two-dimensional Gaussian dispersal kernel. We are given $n$ independent and identically distributed displacement vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$, where each $\\mathbf{x}_i \\in \\mathbb{R}^2$ is assumed to be a sample from a multivariate normal distribution $\\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The analysis will proceed by deriving the Maximum Likelihood Estimators (MLEs) for the model parameters under two competing hypotheses and then constructing a Likelihood Ratio Test (LRT).\n\nThe two hypotheses are as follows:\nThe null hypothesis, $H_0$, posits an isotropic dispersal kernel. This constrains the covariance matrix to be a scalar multiple of the identity matrix:\n$$ H_0: \\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}_2 $$\nwhere $\\sigma^2 > 0$ is a single variance parameter and $\\mathbf{I}_2$ is the $2 \\times 2$ identity matrix.\n\nThe alternative hypothesis, $H_A$, allows for anisotropy. This means the covariance matrix $\\boldsymbol{\\Sigma}$ is an unconstrained symmetric positive definite matrix:\n$$ H_A: \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{pmatrix}, \\text{ with } \\sigma_{12} = \\sigma_{21} \\text{ and } \\boldsymbol{\\Sigma} \\succ 0 $$\n\nThe log-likelihood function for a sample of $n$ observations from $\\mathcal{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, with dimension $d=2$, is:\n$$ \\ell(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{n}{2}\\log|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n\nFirst, we find the MLEs under the alternative hypothesis, $H_A$. The parameters to estimate are $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. Standard results from multivariate statistics show that the MLEs are the sample mean and the sample covariance matrix (with divisor $n$):\n$$ \\hat{\\boldsymbol{\\mu}}_{\\text{alt}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i $$\n$$ \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}} = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top $$\nSubstituting these estimators into the log-likelihood function, the summation term simplifies due to a property of the trace operator:\n$$ \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}}) = \\mathrm{tr}\\left(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top\\right) = \\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} n \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) = n \\, \\mathrm{tr}(\\mathbf{I}_d) = nd $$\nThe maximized log-likelihood under $H_A$ is therefore:\n$$ \\ell_{\\text{alt}} = \\ell(\\hat{\\boldsymbol{\\mu}}_{\\text{alt}}, \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{n}{2}\\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}| - \\frac{nd}{2} $$\n\nNext, we find the MLEs under the null hypothesis, $H_0$. The parameters are $\\boldsymbol{\\mu}$ and $\\sigma^2$. The log-likelihood function under this constraint is:\n$$ \\ell(\\boldsymbol{\\mu}, \\sigma^2 | H_0) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{nd}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\|\\mathbf{x}_i - \\boldsymbol{\\mu}\\|^2_2 $$\nThe MLE for the mean is again the sample mean, $\\hat{\\boldsymbol{\\mu}}_{\\text{null}} = \\hat{\\boldsymbol{\\mu}}_{\\text{alt}}$. Let us denote it by $\\hat{\\boldsymbol{\\mu}}$. To find the MLE for $\\sigma^2$, we differentiate the log-likelihood with respect to $\\sigma^2$ and set the result to zero, yielding:\n$$ \\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{nd} \\sum_{i=1}^n \\|\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}\\|^2_2 $$\nThe sum of squares can be related to the trace of the unconstrained sample covariance matrix:\n$$ \\sum_{i=1}^n \\|\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}\\|^2_2 = \\sum_{i=1}^n \\mathrm{tr}\\left((\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top\\right) = \\mathrm{tr}\\left( \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top \\right) = \\mathrm{tr}(n \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) $$\nThus, the MLE for $\\sigma^2$ under $H_0$ is the average of the total variance across dimensions:\n$$ \\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{d} \\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) $$\nSubstituting these estimators into the null log-likelihood, the summation term becomes $\\frac{1}{2\\hat{\\sigma}^2_{\\text{null}}} (nd \\hat{\\sigma}^2_{\\text{null}}) = \\frac{nd}{2}$. The maximized log-likelihood under $H_0$ is:\n$$ \\ell_{\\text{null}} = \\ell(\\hat{\\boldsymbol{\\mu}}, \\hat{\\sigma}^2_{\\text{null}}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{nd}{2}\\log(\\hat{\\sigma}^2_{\\text{null}}) - \\frac{nd}{2} $$\n\nThe Likelihood Ratio Test statistic, $T$, is defined as twice the difference in the maximized log-likelihoods:\n$$ T = 2(\\ell_{\\text{alt}} - \\ell_{\\text{null}}) = 2 \\left( \\left(-\\frac{n}{2}\\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|\\right) - \\left(-\\frac{nd}{2}\\log(\\hat{\\sigma}^2_{\\text{null}})\\right) \\right) $$\n$$ T = n \\left( d\\log(\\hat{\\sigma}^2_{\\text{null}}) - \\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}| \\right) = n \\log\\left( \\frac{(\\hat{\\sigma}^2_{\\text{null}})^d}{|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|} \\right) $$\nSubstituting $\\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{d}\\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}})$ and setting $d=2$:\n$$ T = n \\log\\left( \\frac{\\left(\\frac{1}{2}\\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}})\\right)^2}{|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|} \\right) $$\nThis statistic compares the squared arithmetic mean of the eigenvalues of $\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}$ with their geometric mean (the determinant).\n\nAccording to Wilks's theorem, for large $n$, $T$ is asymptotically distributed as a chi-squared random variable under the null hypothesis. The number of degrees of freedom, $k$, is the difference in the number of free parameters in the covariance matrix between the alternative and null models. For $d=2$, $H_A$ has $3$ free covariance parameters ($\\sigma_{11}, \\sigma_{22}, \\sigma_{12}$), while $H_0$ has $1$ ($\\sigma^2$). Therefore, $k = 3 - 1 = 2$.\nSo, $T \\sim \\chi^2_2$ under $H_0$.\n\nThe procedure for each test case is:\n$1$. Generate $n$ data points from the specified true distribution $\\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$.\n$2$. Compute the sample mean $\\hat{\\boldsymbol{\\mu}}$ and the MLE of the covariance matrix $\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}$.\n$3$. Calculate the LRT statistic $T$ using the derived formula.\n$4$. Compute the $p$-value as the upper-tail probability of the $\\chi^2_2$ distribution, i.e., $P(X \\ge T)$ where $X \\sim \\chi^2_2$. This is calculated using the survival function of the chi-squared distribution.\nA small $p$-value (e.g., $< 0.05$) provides evidence to reject the null hypothesis of isotropy in favor of the anisotropic alternative.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs a Likelihood Ratio Test for isotropy on simulated 2D displacement data.\n\n    For each test case, the function:\n    1. Generates a dataset from a bivariate normal distribution.\n    2. Computes the maximum likelihood estimates for parameters under the\n       anisotropic (alternative) and isotropic (null) hypotheses.\n    3. Calculates the Likelihood Ratio Test statistic.\n    4. Computes the p-value using the asymptotic chi-squared distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Case C: Define the rotation and covariance matrix for rotated anisotropy.\n    theta = np.pi / 6\n    R_theta = np.array([\n        [np.cos(theta), -np.sin(theta)],\n        [np.sin(theta), np.cos(theta)]\n    ])\n    D_C = np.diag([4, 1])\n    Sigma_C = R_theta @ D_C @ R_theta.T\n\n    test_cases = [\n        {'n': 500, 'seed': 17, 'Sigma_true': np.array([[4, 0], [0, 4]]), 'case': 'A'},\n        {'n': 500, 'seed': 29, 'Sigma_true': np.array([[9, 0], [0, 1]]), 'case': 'B'},\n        {'n': 500, 'seed': 97, 'Sigma_true': Sigma_C, 'case': 'C'},\n        {'n': 200, 'seed': 1234, 'Sigma_true': np.array([[1.2, 0], [0, 0.8]]), 'case': 'D'},\n        {'n': 50, 'seed': 2021, 'Sigma_true': np.eye(2), 'case': 'E'}\n    ]\n\n    p_values = []\n    \n    # Dimension of the data\n    d = 2\n\n    for case in test_cases:\n        n = case['n']\n        seed = case['seed']\n        Sigma_true = case['Sigma_true']\n\n        # 1. Generate n samples from N_2(0, Sigma_true)\n        rng = np.random.default_rng(seed)\n        mean_true = np.zeros(d)\n        data = rng.multivariate_normal(mean_true, Sigma_true, size=n)\n\n        # 2. Estimate parameters by maximum likelihood.\n        # Although the true mean is 0, the model assumes an unknown mean.\n        # MLE for mu is the sample mean for both hypotheses.\n        mu_hat = np.mean(data, axis=0)\n\n        # MLE for covariance under the alternative hypothesis (anisotropic)\n        # np.cov with bias=True divides by n, which is the MLE.\n        Sigma_hat_alt = np.cov(data, rowvar=False, bias=True)\n\n        # 3. Calculate the Likelihood Ratio Test statistic T.\n        # T = n * log( (tr(Sigma_hat)/d)^d / det(Sigma_hat) )\n        det_Sigma_hat = np.linalg.det(Sigma_hat_alt)\n        tr_Sigma_hat = np.trace(Sigma_hat_alt)\n\n        # Handle potential numerical issues if determinant is\n        # non-positive, although unlikely with generated data.\n        if det_Sigma_hat <= 0:\n            # If Sigma_hat is not positive definite, the likelihood is not\n            # well-defined. This can happen with degenerate data.\n            # In such a case, the test is not applicable. For this problem,\n            # we can assume it will be positive.\n            # We can use a very large T to yield a p-value of 0.\n            T_statistic = np.inf\n        else:\n            # MLE for variance under the null hypothesis (isotropic)\n            sigma_sq_hat_null = tr_Sigma_hat / d\n            \n            # The argument to the logarithm is the ratio of the arithmetic mean\n            # to the geometric mean of the eigenvalues of Sigma_hat_alt, raised to power d.\n            # By AM-GM inequality, this is >= 1.\n            log_argument = (sigma_sq_hat_null**d) / det_Sigma_hat\n            T_statistic = n * np.log(log_argument)\n        \n        # 4. Compute the p-value.\n        # The test statistic T is asymptotically chi-squared distributed with\n        # k = (d*(d+1)/2 - 1) degrees of freedom. For d=2, k=2.\n        k = d * (d + 1) // 2 - 1\n        p_value = chi2.sf(T_statistic, df=k)\n        \n        p_values.append(f\"{p_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(p_values)}]\")\n\nsolve()\n\n```", "id": "2480590"}, {"introduction": "A statistical model provides not only point estimates but also a measure of uncertainty, often in the form of credible or confidence intervals. But how can we be sure that a $90\\%$ credible interval truly contains the true parameter value with the declared frequency? This advanced practice [@problem_id:2480520] introduces Simulation-Based Calibration (SBC), a powerful, modern technique to rigorously validate the uncertainty quantification of a Bayesian model. By implementing an SBC protocol from the ground up, you will gain a critical skill for verifying that your entire inferential pipeline is statistically sound and trustworthy.", "problem": "Design and implement a simulation-based calibration protocol for a Bayesian estimator of a dispersal kernel to validate uncertainty quantification and evaluate the coverage of credible intervals for the median dispersal distance. Consider the following scientifically grounded setting from movement ecology:\n\n- A dispersal kernel is a probability density for radial displacement distance. Model the radial distance $r$ as independently and identically distributed from an Exponential dispersal kernel with rate parameter $\\beta$, that is, with density $f(r \\mid \\beta) = \\beta \\exp(-\\beta r)$ for $r \\ge 0$.\n- Adopt a Bayesian framework with a Gamma prior on $\\beta$ using the shape-rate parameterization: $\\beta \\sim \\mathrm{Gamma}(a_0, b_0)$, where $a_0 &gt; 0$ and $b_0 &gt; 0$.\n- The target scalar functional is the median dispersal distance $m(\\beta)$, defined by the unique $m &gt; 0$ satisfying $\\mathbb{P}(R \\le m \\mid \\beta) = 1/2$. For the Exponential kernel, this median is a deterministic monotone function of $\\beta$.\n\nYour task is to implement a simulation-based calibration (SBC) protocol grounded in fundamental probability and Bayesian inference principles to assess calibration of the posterior uncertainty for the median $m(\\beta)$. The protocol must use the prior-predictive data-generating process. For each independent SBC replication $s \\in \\{1,\\dots,S\\}$:\n\n1. Draw a true rate $\\beta^{(s)}$ from the prior $\\mathrm{Gamma}(a_0,b_0)$.\n2. Generate a dataset of $n$ independent dispersal distances $\\{r^{(s)}_i\\}_{i=1}^n$ from the Exponential kernel with rate $\\beta^{(s)}$.\n3. Using Bayes’ rule and the independence of the observations, derive and obtain the posterior distribution for $\\beta \\mid \\{r^{(s)}_i\\}_{i=1}^n$ under the specified prior and likelihood. Then induce the posterior for the median $m(\\beta)$ via the transformation from $\\beta$ to $m(\\beta)$.\n4. From the posterior of $m(\\beta)$, produce $L$ Monte Carlo draws and compute an equal-tailed credible interval with nominal mass $c \\in (0,1)$. Record an indicator variable that the true median $m(\\beta^{(s)})$ lies within the interval.\n5. Compute the rank statistic of the true median among the $L$ posterior draws. Let $\\mathrm{rank}^{(s)}$ be the number of posterior draws strictly less than $m(\\beta^{(s)})$. Map $\\mathrm{rank}^{(s)}$ to a unit interval variable $u^{(s)}$ via $u^{(s)} = \\dfrac{\\mathrm{rank}^{(s)} + 0.5}{L + 1}$.\n\nAcross $S$ replications, report:\n- The empirical coverage $\\hat{\\kappa}$, defined as the average of the coverage indicators.\n- A Kolmogorov–Smirnov distance $D$ between the empirical distribution of $\\{u^{(s)}\\}_{s=1}^S$ and the continuous Uniform distribution on $[0,1]$.\n- The average equal-tailed credible interval width for the posterior median across replications.\n\nScientific realism and derivation constraints:\n- Begin from core definitions of the Exponential dispersal kernel, independence of observations, and Bayes’ theorem for posterior construction. Do not assume or quote any target posterior formulas in the problem statement.\n- The simulation must be prior-predictive: parameters are drawn from the prior and data from the likelihood.\n- All random number generation must be reproducible using a fixed base pseudorandom seed $123456789$.\n\nProgram specification:\n- Implement the above SBC protocol in a single program. Use the following test suite of parameter sets to exercise different regimes:\n    - Test case $1$: $(S, n, a_0, b_0, L, c, \\text{seed\\_offset}) = (300, 20, 2.0, 1.0, 400, 0.90, 0)$.\n    - Test case $2$: $(S, n, a_0, b_0, L, c, \\text{seed\\_offset}) = (300, 2, 2.0, 1.0, 400, 0.90, 1)$.\n    - Test case $3$: $(S, n, a_0, b_0, L, c, \\text{seed\\_offset}) = (300, 50, 5.0, 2.0, 400, 0.95, 2)$.\n- For each test case, run the SBC procedure using the base seed $123456789$ offset by the integer seed offset (for example, by adding the offset to the base seed). All outputs are dimensionless.\n- For each test case, produce a list of three floating-point numbers: $[\\hat{\\kappa}, D, \\overline{W}]$, where $\\overline{W}$ is the average equal-tailed credible interval width for the posterior median across the $S$ replications.\n- Round each reported float to exactly six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists enclosed in square brackets, for example, \"[[x1,x2,x3],[y1,y2,y3],[z1,z2,z3]]\", where each element is rounded to six decimal places as specified.", "solution": "The problem statement has been rigorously validated and is determined to be scientifically sound, well-posed, objective, and self-contained. It presents a clear task in computational statistics, grounded in the principles of Bayesian inference and movement ecology. We shall proceed with a complete solution.\n\nThe task is to design and implement a simulation-based calibration (SBC) protocol to validate a Bayesian estimator for a dispersal kernel. This involves three primary stages: first, the analytical derivation of the posterior distribution for the model parameter; second, the derivation of the posterior distribution for the target functional, the median dispersal distance; and third, the algorithmic implementation of the SBC protocol based on these derivations.\n\n**1. Bayesian Model and Posterior Distribution Derivation**\n\nThe model is constructed from a likelihood function and a prior distribution.\n\n**Likelihood:** The data consist of $n$ independent and identically distributed (i.i.d.) radial dispersal distances $\\{r_i\\}_{i=1}^n$, each drawn from an Exponential distribution with rate parameter $\\beta > 0$. The probability density function (PDF) for a single observation $r$ is:\n$$f(r \\mid \\beta) = \\beta e^{-\\beta r}, \\quad r \\ge 0$$\nGiven the independence of observations, the likelihood function for the entire dataset $\\mathbf{r} = \\{r_1, \\dots, r_n\\}$ is the product of the individual densities:\n$$L(\\beta \\mid \\mathbf{r}) = \\prod_{i=1}^n f(r_i \\mid \\beta) = \\prod_{i=1}^n \\left(\\beta e^{-\\beta r_i}\\right) = \\beta^n \\exp\\left(-\\beta \\sum_{i=1}^n r_i\\right)$$\n\n**Prior:** The prior distribution for the rate parameter $\\beta$ is specified as a Gamma distribution with shape parameter $a_0 > 0$ and rate parameter $b_0 > 0$. The PDF is given by:\n$$p(\\beta) = \\mathrm{Gamma}(\\beta \\mid a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\beta^{a_0 - 1} e^{-b_0 \\beta}$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\n**Posterior:** According to Bayes' theorem, the posterior distribution of $\\beta$ given the data $\\mathbf{r}$ is proportional to the product of the likelihood and the prior:\n$$p(\\beta \\mid \\mathbf{r}) \\propto L(\\beta \\mid \\mathbf{r}) \\times p(\\beta)$$\nSubstituting the expressions for the likelihood and prior:\n$$p(\\beta \\mid \\mathbf{r}) \\propto \\left(\\beta^n e^{-\\beta \\sum_{i=1}^n r_i}\\right) \\times \\left(\\beta^{a_0 - 1} e^{-b_0 \\beta}\\right)$$\nCombining the terms involving $\\beta$:\n$$p(\\beta \\mid \\mathbf{r}) \\propto \\beta^{n + a_0 - 1} \\exp\\left(-\\left(\\sum_{i=1}^n r_i + b_0\\right)\\beta\\right)$$\nThis expression is the kernel of a Gamma distribution. This demonstrates that the Gamma distribution is a conjugate prior for the rate parameter of an Exponential likelihood. The posterior distribution for $\\beta$ is therefore a Gamma distribution with updated parameters. Let the posterior shape be $a_n$ and the posterior rate be $b_n$, where:\n$$a_n = a_0 + n$$\n$$b_n = b_0 + \\sum_{i=1}^n r_i$$\nThus, the posterior distribution is:\n$$\\beta \\mid \\mathbf{r} \\sim \\mathrm{Gamma}(a_n, b_n)$$\n\n**2. Posterior Distribution for the Median Dispersal Distance**\n\nThe target functional is the median dispersal distance, denoted by $m$. It is defined as the value $m > 0$ such that the probability of a dispersal distance being less than or equal to $m$ is $1/2$. The cumulative distribution function (CDF) of the Exponential($\\beta$) distribution is $F(r \\mid \\beta) = 1 - e^{-\\beta r}$.\nSetting $F(m \\mid \\beta) = 1/2$:\n$$1 - e^{-\\beta m} = \\frac{1}{2}$$\n$$e^{-\\beta m} = \\frac{1}{2}$$\n$$-\\beta m = \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)$$\nThis yields the deterministic, monotone relationship between the median $m$ and the rate parameter $\\beta$:\n$$m = m(\\beta) = \\frac{\\ln(2)}{\\beta}$$\nSince we have the posterior distribution for $\\beta$, we can induce the posterior distribution for $m$. A direct method for obtaining samples from the posterior of $m$ is to first draw samples from the posterior of $\\beta$ and then apply the transformation. Specifically, if $\\{\\beta_j\\}_{j=1}^L$ are $L$ Monte Carlo draws from the posterior $\\mathrm{Gamma}(a_n, b_n)$, then the corresponding draws from the posterior of the median are given by $\\{m_j\\}_{j=1}^L$, where $m_j = \\ln(2) / \\beta_j$. This approach avoids the need to work with the PDF of the resulting Inverse-Gamma distribution for $m$ and is computationally straightforward.\n\n**3. The Simulation-Based Calibration (SBC) Protocol**\n\nThe SBC protocol verifies that the Bayesian procedure is correctly calibrated. For a well-calibrated model and a correct implementation, quantities calculated from the posterior should, on average over many simulations, align with their nominal values. The protocol proceeds through $S$ independent replications.\n\nFor each replication $s \\in \\{1, \\dots, S\\}$:\n1.  **Generate True Parameter:** A \"true\" rate parameter $\\beta^{(s)}$ is drawn from the prior distribution: $\\beta^{(s)} \\sim \\mathrm{Gamma}(a_0, b_0)$. The corresponding \"true\" median is $m^{(s)} = \\ln(2) / \\beta^{(s)}$.\n2.  **Generate Synthetic Data:** A dataset of $n$ dispersal distances $\\{r_i^{(s)}\\}_{i=1}^n$ is drawn from the likelihood model using the true parameter: $r_i^{(s)} \\sim \\mathrm{Exponential}(\\beta^{(s)})$.\n3.  **Perform Bayesian Inference:** Using the synthetic data, the posterior parameters are computed: $a_n^{(s)} = a_0 + n$ and $b_n^{(s)} = b_0 + \\sum_{i=1}^n r_i^{(s)}$.\n4.  **Sample from Posterior of Median:** $L$ draws for the median, $\\{m^{(s)}_j\\}_{j=1}^L$, are generated by first drawing $\\{\\beta^{(s)}_j\\}_{j=1}^L$ from the posterior $\\mathrm{Gamma}(a_n^{(s)}, b_n^{(s)})$ and then transforming each draw via $m^{(s)}_j = \\ln(2) / \\beta^{(s)}_j$.\n5.  **Compute Replication Statistics:**\n    *   An equal-tailed credible interval with nominal coverage $c$ is computed from the quantiles of the posterior samples $\\{m^{(s)}_j\\}_{j=1}^L$. The lower and upper bounds are $q_{(1-c)/2}$ and $q_{(1+c)/2}$, respectively. A binary indicator is recorded: $1$ if the true median $m^{(s)}$ falls within this interval, $0$ otherwise. The width of this interval is also recorded.\n    *   The rank statistic is calculated as the number of posterior draws strictly smaller than the true median: $\\mathrm{rank}^{(s)} = \\sum_{j=1}^L \\mathbb{I}(m^{(s)}_j < m^{(s)})$.\n    *   This rank is mapped to a unit interval variable $u^{(s)}$ using the specified formula: $u^{(s)} = (\\mathrm{rank}^{(s)} + 0.5) / (L + 1)$. For a correctly calibrated procedure, the distribution of these $u^{(s)}$ values across many replications should be uniform on $[0, 1]$.\n\n**Aggregation of Results:**\nAfter completing all $S$ replications, the following summary statistics are computed:\n-   **Empirical Coverage ($\\hat{\\kappa}$):** The mean of the binary coverage indicators. For a well-calibrated model, $\\hat{\\kappa}$ should be close to the nominal coverage $c$.\n-   **Kolmogorov–Smirnov (KS) Distance ($D$):** The distance between the empirical distribution of the $\\{u^{(s)}\\}_{s=1}^S$ values and the theoretical Uniform($0,1$) distribution. A small value of $D$ indicates good calibration.\n-   **Average Credible Interval Width ($\\overline{W}$):** The mean of the interval widths across all replications, which quantifies the typical posterior uncertainty for the median under the specified conditions.\n\nThe following Python program implements this protocol precisely. All random number generation is seeded for reproducibility as specified. The parameterization of `numpy` functions (shape/scale) is handled correctly in relation to the theoretical definitions (shape/rate).", "answer": "```python\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef run_sbc_protocol(S, n, a0, b0, L, c, seed):\n    \"\"\"\n    Runs the Simulation-Based Calibration (SBC) protocol for a Bayesian\n    estimator of an Exponential dispersal kernel's median distance.\n\n    Args:\n        S (int): Number of SBC replications.\n        n (int): Sample size for each synthetic dataset.\n        a0 (float): Shape parameter of the Gamma prior for beta.\n        b0 (float): Rate parameter of the Gamma prior for beta.\n        L (int): Number of Monte Carlo draws from the posterior.\n        c (float): Nominal coverage for the credible interval.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        list: A list containing [empirical_coverage, ks_distance, avg_width].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    coverage_indicators = []\n    u_values = []\n    interval_widths = []\n\n    for _ in range(S):\n        # Step 1: Draw a \"true\" parameter beta from the prior.\n        # numpy.random.gamma uses a scale parameter, where scale = 1 / rate.\n        beta_true = rng.gamma(shape=a0, scale=1.0/b0)\n        # Calculate the corresponding \"true\" median dispersal distance.\n        m_true = np.log(2) / beta_true\n\n        # Step 2: Generate a synthetic dataset from the likelihood.\n        # numpy.random.exponential uses a scale parameter, where scale = 1 / rate.\n        r_data = rng.exponential(scale=1.0/beta_true, size=n)\n\n        # Step 3: Compute the posterior parameters for beta.\n        # The posterior for beta is Gamma(an, bn).\n        an = a0 + n\n        bn = b0 + np.sum(r_data)\n\n        # Step 4: Generate L draws from the posterior of the median m.\n        # This is done by drawing from the beta posterior and transforming.\n        beta_posterior_draws = rng.gamma(shape=an, scale=1.0/bn, size=L)\n        m_posterior_draws = np.log(2) / beta_posterior_draws\n\n        # Step 5: Compute statistics for the current replication.\n        # 5a. Credible interval, coverage, and width.\n        alpha = 1.0 - c\n        q_lower = alpha / 2.0\n        q_upper = 1.0 - q_lower\n        lower_bound = np.quantile(m_posterior_draws, q_lower)\n        upper_bound = np.quantile(m_posterior_draws, q_upper)\n        \n        is_covered = (lower_bound <= m_true) and (m_true <= upper_bound)\n        coverage_indicators.append(1 if is_covered else 0)\n        \n        interval_width = upper_bound - lower_bound\n        interval_widths.append(interval_width)\n\n        # 5b. Rank statistic and the u-value.\n        # Rank is the number of posterior draws strictly less than the true value.\n        rank = np.sum(m_posterior_draws < m_true)\n        # Map rank to a unit interval variable as per the problem statement.\n        u = (rank + 0.5) / (L + 1.0)\n        u_values.append(u)\n\n    # Aggregate results over all S replications.\n    # Empirical coverage kappa_hat.\n    kappa_hat = np.mean(coverage_indicators)\n    \n    # Average credible interval width.\n    avg_width = np.mean(interval_widths)\n\n    # Kolmogorov-Smirnov distance D.\n    # Compare the empirical distribution of u-values to the standard uniform.\n    ks_result = kstest(u_values, 'uniform')\n    D = ks_result.statistic\n\n    return [kappa_hat, D, avg_width]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    test_cases = [\n        # (S, n, a0, b0, L, c, seed_offset)\n        (300, 20, 2.0, 1.0, 400, 0.90, 0),\n        (300, 2, 2.0, 1.0, 400, 0.90, 1),\n        (300, 50, 5.0, 2.0, 400, 0.95, 2),\n    ]\n\n    base_seed = 123456789\n    all_results = []\n\n    for S, n, a0, b0, L, c, seed_offset in test_cases:\n        seed = base_seed + seed_offset\n        result = run_sbc_protocol(S, n, a0, b0, L, c, seed)\n        # Round each float in the result list to 6 decimal places.\n        rounded_result = [f\"{x:.6f}\" for x in result]\n        all_results.append(f\"[{','.join(rounded_result)}]\")\n\n    # Print the final output in the exact specified format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2480520"}]}