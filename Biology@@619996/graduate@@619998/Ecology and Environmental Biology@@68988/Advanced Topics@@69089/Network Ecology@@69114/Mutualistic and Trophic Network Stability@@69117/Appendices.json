{"hands_on_practices": [{"introduction": "Before we can analyze the stability of an ecological community, we must first ask a more fundamental question: can all species coexist with positive populations? This practice introduces the concept of the \"feasibility cone,\" a powerful geometric tool derived from the Generalized Lotka-Volterra model. By applying basic linear algebra, you will determine if a given set of intrinsic growth rates, represented by the vector $r$, permits a biologically meaningful equilibrium where all species persist, grounding an abstract concept in a concrete calculation. [@problem_id:2510805]", "problem": "Consider a mutualistic community described by the Generalized Lotka–Volterra (GLV) equations for $n$ species,\n$$\\frac{dx_{i}}{dt} = x_{i}\\left(r_{i} + \\sum_{j=1}^{n} A_{ij} x_{j}\\right), \\quad i=1,\\dots,n,$$\nwhere $x_{i}$ is the abundance of species $i$, $r_{i}$ is its intrinsic per-capita growth rate, and $A_{ij}$ is the per-capita effect of species $j$ on species $i$. A strictly feasible equilibrium is a steady state with $x_{i}^{\\ast}  0$ for all $i$.\n\nFeasibility is equivalent to the existence of a vector $x^{\\ast} \\in \\mathbb{R}^{n}_{0}$ such that\n$$r + A x^{\\ast} = 0,$$\nor, equivalently,\n$$r = (-A)\\,x^{\\ast} \\quad \\text{with} \\quad x^{\\ast} \\in \\mathbb{R}^{n}_{\\ge 0}.$$\nDefine the feasibility cone generated by the columns of $-A$ as\n$$\\mathcal{C}(-A) = \\left\\{ \\sum_{j=1}^{n} \\alpha_{j} c_{j} \\,:\\, \\alpha_{j} \\ge 0 \\ \\text{for all } j \\right\\},$$\nwhere $c_{j}$ denotes the $j$-th column of $-A$. Then $r \\in \\mathcal{C}(-A)$ if and only if a strictly feasible equilibrium exists.\n\nA bipartite mutualistic network with one plant species (index $1$) and two animal mutualists (indices $2$ and $3$) has interaction matrix\n$$A \\;=\\; \\begin{pmatrix}\n-1.0  0.3  0.2 \\\\\n0.4  -1.1  -0.1 \\\\\n0.5  -0.2  -1.2\n\\end{pmatrix},$$\nwhere positive off-diagonal entries across the plant–animal guilds encode mutualistic benefits and negative off-diagonal entries within the animal guild encode competition. Consider the intrinsic growth-rate vector\n$$r \\;=\\; \\begin{pmatrix} 0.05 \\\\ 0.02 \\\\ 0.03 \\end{pmatrix}.$$\n\nUsing only first principles of the GLV model and the cone definition above, determine whether $r$ lies in the feasibility cone $\\mathcal{C}(-A)$, and, if so, compute the unique strictly feasible equilibrium $x^{\\ast}$ solving $r = (-A)\\,x^{\\ast}$. Express your final answer as the vector $x^{\\ast}$ in the same arbitrary abundance units as $x$, rounded to four significant figures. Your final answer must be a single row vector. Do not include any units in the final answer.", "solution": "The problem statement is a well-posed exercise in linear algebra applied to community ecology. It is scientifically grounded in the established theory of Lotka-Volterra systems, contains all necessary data, and is free of contradictions or ambiguities. It is therefore deemed valid.\n\nThe core task is to determine if a given intrinsic growth rate vector $r$ permits a strictly feasible equilibrium $x^{\\ast}$, where all species abundances are positive. According to the problem definition, this is equivalent to establishing whether the vector $r$ lies in the feasibility cone $\\mathcal{C}(-A)$, which is the set of all non-negative linear combinations of the columns of the matrix $-A$. Mathematically, this requires solving the linear system $r = (-A)x^{\\ast}$ and verifying that the solution vector $x^{\\ast}$ has strictly positive components, i.e., $x_{i}^{\\ast} > 0$ for all $i$.\n\nThe given interaction matrix is:\n$$A = \\begin{pmatrix} -1.0  0.3  0.2 \\\\ 0.4  -1.1  -0.1 \\\\ 0.5  -0.2  -1.2 \\end{pmatrix}$$\nAnd the intrinsic growth rate vector is:\n$$r = \\begin{pmatrix} 0.05 \\\\ 0.02 \\\\ 0.03 \\end{pmatrix}$$\nThe system of equations to be solved is $r = (-A)x^{\\ast}$, which can be written as $(-A)x^{\\ast} = r$. First, we construct the matrix $-A$:\n$$-A = -\\begin{pmatrix} -1.0  0.3  0.2 \\\\ 0.4  -1.1  -0.1 \\\\ 0.5  -0.2  -1.2 \\end{pmatrix} = \\begin{pmatrix} 1.0  -0.3  -0.2 \\\\ -0.4  1.1  0.1 \\\\ -0.5  0.2  1.2 \\end{pmatrix}$$\nThe linear system is:\n$$\\begin{pmatrix} 1.0  -0.3  -0.2 \\\\ -0.4  1.1  0.1 \\\\ -0.5  0.2  1.2 \\end{pmatrix} \\begin{pmatrix} x_{1}^{\\ast} \\\\ x_{2}^{\\ast} \\\\ x_{3}^{\\ast} \\end{pmatrix} = \\begin{pmatrix} 0.05 \\\\ 0.02 \\\\ 0.03 \\end{pmatrix}$$\nTo determine if a unique solution exists, we compute the determinant of the matrix $-A$.\n$$\\det(-A) = 1.0 \\left( (1.1)(1.2) - (0.1)(0.2) \\right) - (-0.3) \\left( (-0.4)(1.2) - (0.1)(-0.5) \\right) + (-0.2) \\left( (-0.4)(0.2) - (1.1)(-0.5) \\right)$$\n$$\\det(-A) = 1.0(1.32 - 0.02) + 0.3(-0.48 + 0.05) - 0.2(-0.08 + 0.55)$$\n$$\\det(-A) = 1.0(1.30) + 0.3(-0.43) - 0.2(0.47)$$\n$$\\det(-A) = 1.30 - 0.129 - 0.094 = 1.077$$\nSince $\\det(-A) = 1.077 \\neq 0$, the matrix $-A$ is invertible, and a unique solution $x^{\\ast}$ exists. The solution is given by $x^{\\ast} = (-A)^{-1}r$.\n\nWe compute the inverse matrix $(-A)^{-1}$ using the adjugate matrix method, $(-A)^{-1} = \\frac{1}{\\det(-A)}\\text{adj}(-A)$. The adjugate matrix is the transpose of the cofactor matrix.\nThe cofactor matrix $C$ of $-A$ is:\n$$C_{11} = 1.30, \\quad C_{12} = 0.43, \\quad C_{13} = 0.47$$\n$$C_{21} = 0.32, \\quad C_{22} = 1.10, \\quad C_{23} = -0.05$$\n$$C_{31} = 0.19, \\quad C_{32} = -0.02, \\quad C_{33} = 0.98$$\nSo, the adjugate matrix is:\n$$\\text{adj}(-A) = C^{T} = \\begin{pmatrix} 1.30  0.32  0.19 \\\\ 0.43  1.10  -0.02 \\\\ 0.47  -0.05  0.98 \\end{pmatrix}$$\nThe inverse matrix is:\n$$(-A)^{-1} = \\frac{1}{1.077} \\begin{pmatrix} 1.30  0.32  0.19 \\\\ 0.43  1.10  -0.02 \\\\ 0.47  -0.05  0.98 \\end{pmatrix}$$\nNow, we compute the equilibrium vector $x^{\\ast}$:\n$$x^{\\ast} = (-A)^{-1}r = \\frac{1}{1.077} \\begin{pmatrix} 1.30  0.32  0.19 \\\\ 0.43  1.10  -0.02 \\\\ 0.47  -0.05  0.98 \\end{pmatrix} \\begin{pmatrix} 0.05 \\\\ 0.02 \\\\ 0.03 \\end{pmatrix}$$\n$$x^{\\ast} = \\frac{1}{1.077} \\begin{pmatrix} (1.30)(0.05) + (0.32)(0.02) + (0.19)(0.03) \\\\ (0.43)(0.05) + (1.10)(0.02) - (0.02)(0.03) \\\\ (0.47)(0.05) - (0.05)(0.02) + (0.98)(0.03) \\end{pmatrix}$$\n$$x^{\\ast} = \\frac{1}{1.077} \\begin{pmatrix} 0.065 + 0.0064 + 0.0057 \\\\ 0.0215 + 0.0220 - 0.0006 \\\\ 0.0235 - 0.0010 + 0.0294 \\end{pmatrix} = \\frac{1}{1.077} \\begin{pmatrix} 0.0771 \\\\ 0.0429 \\\\ 0.0519 \\end{pmatrix}$$\nPerforming the final division for each component:\n$$x_{1}^{\\ast} = \\frac{0.0771}{1.077} \\approx 0.0715877...$$\n$$x_{2}^{\\ast} = \\frac{0.0429}{1.077} \\approx 0.0398328...$$\n$$x_{3}^{\\ast} = \\frac{0.0519}{1.077} \\approx 0.0481894...$$\nAll components $x_{1}^{\\ast}$, $x_{2}^{\\ast}$, and $x_{3}^{\\ast}$ are strictly positive. This confirms that the vector $r$ lies in the feasibility cone $\\mathcal{C}(-A)$ and a strictly feasible equilibrium exists.\n\nRounding the results to four significant figures as required:\n$$x_{1}^{\\ast} \\approx 0.07159$$\n$$x_{2}^{\\ast} \\approx 0.03983$$\n$$x_{3}^{\\ast} \\approx 0.04819$$\nThe equilibrium abundance vector is $x^{\\ast} = (0.07159, 0.03983, 0.04819)^{T}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.07159  0.03983  0.04819\n\\end{pmatrix}\n}\n$$", "id": "2510805"}, {"introduction": "The existence of a positive equilibrium does not guarantee a persistent community; the equilibrium must also be stable to perturbations. This exercise challenges you to derive the conditions for local stability from first principles for a classic three-species mutualistic system. You will analyze the critical interplay between stabilizing self-regulation ($d$) and destabilizing mutualism ($m$), uncovering the threshold where an otherwise stable community can abruptly collapse, demonstrating a key principle in the study of mutualistic networks. [@problem_id:2510887]", "problem": "Consider a three-species Generalized Lotka–Volterra (GLV) system with species abundances $x_{1}(t)$, $x_{2}(t)$, and $x_{3}(t)$ governed by\n$$\n\\frac{dx_{i}}{dt} \\;=\\; x_{i}\\!\\left(r_{i} \\;+\\; \\sum_{j=1}^{3} a_{ij}\\, x_{j}\\right), \\quad i \\in \\{1,2,3\\},\n$$\nwhere $r = (r_{1},r_{2},r_{3})^{\\top}$ is the intrinsic growth-rate vector and $A=(a_{ij})$ is the interaction matrix. Assume that all intrinsic growth rates are equal and positive, $r_{1}=r_{2}=r_{3}=r$ with $r0$. The interaction matrix is symmetric and tridiagonal,\n$$\nA \\;=\\; \\begin{pmatrix}\n-d  m  0 \\\\\nm  -d  m \\\\\n0  m  -d\n\\end{pmatrix},\n$$\nwith self-regulation strength $d0$ on the diagonal and mutualistic coupling strength $m\\ge 0$ between adjacent species.\n\nStarting from the defining properties of the GLV system, the definition of equilibrium, and the linearization principle for local stability, derive from first principles the conditions on $d$ and $m$ under which there exists a strictly positive equilibrium $x^{\\ast} \\in \\mathbb{R}^{3}_{0}$ and that this equilibrium is locally asymptotically stable. Then, determine the largest value $m_{\\max}$ as a function of $d$ such that for every $0 \\le m  m_{\\max}$, the system admits a strictly positive equilibrium that is locally asymptotically stable. Express your final answer as a single closed-form expression in terms of $d$. No rounding is required, and no units are needed. Your final answer must be only the expression for $m_{\\max}$.", "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The analysis proceeds by first establishing the conditions for the existence of a strictly positive equilibrium, and second, determining the conditions for the local asymptotic stability of that equilibrium.\n\nLet the species abundances be denoted by the vector $\\mathbf{x} = (x_1, x_2, x_3)^{\\top}$. The Generalized Lotka-Volterra (GLV) system is given by\n$$\n\\frac{dx_{i}}{dt} = x_{i} \\left(r_{i} + \\sum_{j=1}^{3} a_{ij} x_{j}\\right), \\quad i \\in \\{1, 2, 3\\}\n$$\nwith parameters $r_{1}=r_{2}=r_{3}=r0$ and the interaction matrix\n$$\nA = \\begin{pmatrix}\n-d  m  0 \\\\\nm  -d  m \\\\\n0  m  -d\n\\end{pmatrix}\n$$\nwhere $d0$ and $m \\ge 0$.\n\nAn equilibrium solution, $\\mathbf{x}^* = (x_1^*, x_2^*, x_3^*)^{\\top}$, is found by setting $\\frac{d\\mathbf{x}}{dt} = \\mathbf{0}$. We seek a strictly positive equilibrium, where $x_i^*  0$ for all $i \\in \\{1, 2, 3\\}$. For such an equilibrium, the condition simplifies to the linear system:\n$$\nr_{i} + \\sum_{j=1}^{3} a_{ij} x_{j}^* = 0, \\quad \\text{for } i \\in \\{1, 2, 3\\}\n$$\nIn matrix form, this is $A \\mathbf{x}^* = -\\mathbf{r}$, where $\\mathbf{r} = (r, r, r)^{\\top}$. The system of equations is:\n\\begin{align*}\n-d x_1^* + m x_2^* \\quad = -r \\quad (1) \\\\\nm x_1^* - d x_2^* + m x_3^* = -r \\quad (2) \\\\\nm x_2^* - d x_3^* = -r \\quad (3)\n\\end{align*}\nFrom equations $(1)$ and $(3)$, we have $d x_1^* - r = m x_2^* = d x_3^* - r$, which implies $d x_1^* = d x_3^*$. Since $d0$, we must have $x_1^* = x_3^*$. This symmetry is expected from the structure of the interaction matrix $A$.\n\nSubstituting $x_3^* = x_1^*$ into equation $(2)$ yields $2m x_1^* - d x_2^* = -r$. We now have a system of two linear equations for $x_1^*$ and $x_2^*$:\n\\begin{align*}\n-d x_1^* + m x_2^* = -r \\\\\n2m x_1^* - d x_2^* = -r\n\\end{align*}\nSolving this system, for instance by multiplying the first equation by $d$ and the second by $m$ and adding them, gives $(-d^2 + 2m^2)x_1^* = -dr - mr = -r(d+m)$. This yields:\n$$\nx_1^* = \\frac{-r(d+m)}{2m^2 - d^2} = \\frac{r(d+m)}{d^2 - 2m^2}\n$$\nFrom $x_1^* = x_3^*$, we have $x_3^* = \\frac{r(d+m)}{d^2 - 2m^2}$. Now, we solve for $x_2^*$. From equation $(1)$, $m x_2^* = d x_1^* - r$.\n$$\nm x_2^* = d \\left( \\frac{r(d+m)}{d^2 - 2m^2} \\right) - r = r \\left( \\frac{d(d+m)}{d^2 - 2m^2} - 1 \\right) = r \\left( \\frac{d^2+md - (d^2 - 2m^2)}{d^2 - 2m^2} \\right) = r \\frac{md+2m^2}{d^2 - 2m^2} = r \\frac{m(d+2m)}{d^2 - 2m^2}\n$$\nFor $m0$, we can divide by $m$ to get $x_2^* = \\frac{r(d+2m)}{d^2 - 2m^2}$. If $m=0$, the system decouples and one finds $x_i^*=r/d  0$. The derived formulae are consistent with this limit.\n\nFor the equilibrium to be strictly positive ($x_i^*  0$), given that $r0$, $d0$, and $m \\ge 0$, the numerators $r(d+m)$ and $r(d+2m)$ are strictly positive. Thus, the condition for positivity rests entirely on the sign of the common denominator:\n$$\nd^2 - 2m^2  0 \\implies d^2  2m^2\n$$\nSince $d$ and $m$ are non-negative, this is equivalent to $d  \\sqrt{2} m$, or $m  d/\\sqrt{2}$.\n\nNext, we analyze the local asymptotic stability of this equilibrium. The stability is determined by the eigenvalues of the Jacobian matrix $J$ evaluated at the equilibrium $\\mathbf{x}^*$. The elements of the Jacobian are:\n$$\nJ_{ik}(\\mathbf{x}) = \\frac{\\partial \\dot{x}_i}{\\partial x_k} = \\delta_{ik}\\left(r_{i} + \\sum_{j=1}^{3} a_{ij} x_{j}\\right) + x_{i} a_{ik}\n$$\nAt equilibrium $\\mathbf{x}^*$, the term in parentheses is zero, so the Jacobian is $J^* = J(\\mathbf{x}^*)$ with elements $J_{ik}^* = x_i^* a_{ik}$. This can be written in matrix form as $J^* = D_x A$, where $D_x = \\text{diag}(x_1^*, x_2^*, x_3^*)$ is a diagonal matrix with positive entries.\n\nThe equilibrium is locally asymptotically stable if all eigenvalues of $J^*$ have negative real parts. To analyze the eigenvalues of $J^* = D_x A$, we perform a similarity transformation, which preserves eigenvalues. Let $D_x^{1/2} = \\text{diag}(\\sqrt{x_1^*}, \\sqrt{x_2^*}, \\sqrt{x_3^*})$. Consider the matrix $S$:\n$$\nS = D_x^{1/2} A D_x^{1/2}\n$$\nThe Jacobian $J^*$ is similar to $S$ because $S = D_x^{-1/2} J^* D_x^{1/2}$. Since $A$ is a real symmetric matrix and $D_x^{1/2}$ is real and diagonal (hence symmetric), the matrix $S$ is also real and symmetric:\n$$\nS^{\\top} = (D_x^{1/2} A D_x^{1/2})^{\\top} = (D_x^{1/2})^{\\top} A^{\\top} (D_x^{1/2})^{\\top} = D_x^{1/2} A D_x^{1/2} = S\n$$\nA real symmetric matrix has only real eigenvalues. Therefore, the eigenvalues of $S$, and thus of $J^*$, are all real. For asymptotic stability, all eigenvalues must be strictly negative. This is equivalent to the condition that $S$ is a negative definite matrix.\n\nA matrix $S$ is negative definite if and only if $\\mathbf{z}^{\\top} S \\mathbf{z}  0$ for all non-zero vectors $\\mathbf{z}$. Substituting the definition of $S$:\n$$\n\\mathbf{z}^{\\top} (D_x^{1/2} A D_x^{1/2}) \\mathbf{z}  0\n$$\nLet $\\mathbf{y} = D_x^{1/2} \\mathbf{z}$. Since $D_x^{1/2}$ is invertible, $\\mathbf{y}$ is a non-zero vector if and only if $\\mathbf{z}$ is a non-zero vector. The condition becomes:\n$$\n\\mathbf{y}^{\\top} A \\mathbf{y}  0\n$$\nfor all non-zero vectors $\\mathbf{y}$. This is precisely the definition of the matrix $A$ being negative definite.\n\nTo determine when the symmetric matrix $A$ is negative definite, we apply Sylvester's criterion to the matrix $-A$. The matrix $-A$ must be positive definite, which means all of its leading principal minors must be strictly positive.\n$$\n-A = \\begin{pmatrix}\nd  -m  0 \\\\\n-m  d  -m \\\\\n0  -m  d\n\\end{pmatrix}\n$$\nThe first leading principal minor is $\\Delta_1 = d$. The condition is $\\Delta_1  0$, which is given as $d0$.\nThe second leading principal minor is:\n$$\n\\Delta_2 = \\det \\begin{pmatrix} d  -m \\\\ -m  d \\end{pmatrix} = d^2 - m^2\n$$\nThe condition is $\\Delta_2  0$, which implies $d^2  m^2$, or $d  m$ since $d, m \\ge 0$.\nThe third leading principal minor is $\\Delta_3 = \\det(-A)$:\n$$\n\\Delta_3 = d(d^2 - m^2) - (-m)(-md) = d^3 - dm^2 - dm^2 = d^3 - 2dm^2 = d(d^2 - 2m^2)\n$$\nThe condition is $\\Delta_3  0$. Since $d0$, this simplifies to $d^2 - 2m^2  0$, which is $d  \\sqrt{2} m$.\n\nThe set of conditions for stability is $\\{d0, dm, d\\sqrt{2} m\\}$. Since $\\sqrt{2} \\approx 1.414  1$, the condition $d  \\sqrt{2} m$ is the most restrictive and implies $dm$. Thus, the condition for local asymptotic stability is $m  d/\\sqrt{2}$.\n\nRemarkably, the condition for the existence of a strictly positive equilibrium ($m  d/\\sqrt{2}$) is identical to the condition for its local asymptotic stability. Therefore, the system admits a strictly positive, locally asymptotically stable equilibrium if and only if $0 \\le m  d/\\sqrt{2}$.\n\nThe problem asks for the largest value $m_{\\max}$ such that for every $m$ in the interval $0 \\le m  m_{\\max}$, the desired equilibrium exists and is stable. From our derived condition $0 \\le m  d/\\sqrt{2}$, we can directly identify $m_{\\max}$.\n$$\nm_{\\max} = \\frac{d}{\\sqrt{2}}\n$$", "answer": "$$\\boxed{\\frac{d}{\\sqrt{2}}}$$", "id": "2510887"}, {"introduction": "Moving from diagnosis to intervention, this final practice places you in the role of an ecosystem manager tasked with enhancing stability. Using the principles of matrix perturbation theory and numerical methods, you will identify which interaction strengths in a community matrix are the most sensitive levers for stabilization. This computational exercise demonstrates how theoretical eigenvalue sensitivity analysis can inform practical, targeted management strategies to make ecosystems more resilient. [@problem_id:2510908]", "problem": "You are given square real matrices that represent community Jacobians (community matrices) of ecological networks where diagonal entries encode self-regulation and off-diagonal entries encode net effects of interspecific interactions. Local stability is determined by the dominant eigenvalue, defined as the eigenvalue with the largest real part. Consider a small first-order (infinitesimal) change to a subset of interaction coefficients chosen by a binary mask. Your task is to compute, for each test case, the admissible perturbation direction of unit Frobenius norm that most efficiently decreases the real part of the dominant eigenvalue to first order and to report the corresponding maximal instantaneous decrease value.\n\nFundamental bases to use include: the definition of eigenvalues and left/right eigenvectors of a matrix, and first-order matrix perturbation theory for a simple eigenvalue. Assume the dominant eigenvalue is simple in all provided cases. Let the real part of the dominant eigenvalue be denoted by $\\Re\\{\\lambda_{\\max}(A)\\}$. Let $A \\in \\mathbb{R}^{n \\times n}$ denote the community matrix, and let $M \\in \\{0,1\\}^{n \\times n}$ be the binary mask specifying which entries can be altered. You must restrict perturbations $\\Delta \\in \\mathbb{R}^{n \\times n}$ to satisfy two constraints simultaneously: (i) elementwise support constraint $M \\odot \\Delta = \\Delta$ (only entries with mask value $1$ can be nonzero), and (ii) unit Frobenius norm constraint $\\|\\Delta\\|_{\\mathrm{F}} = 1$. Among all such $\\Delta$, compute the direction that yields the largest instantaneous decrease (most negative first-order directional derivative) of $\\Re\\{\\lambda_{\\max}(A)\\}$ at $A$, and report the value of this maximal instantaneous decrease. If no entries are allowed to change (the mask is all zeros), the admissible direction is the zero matrix and the reported value must be $0$.\n\nAngle units are not applicable. There are no physical units. All outputs must be real numbers.\n\nTest suite. For each test case below, $A$ is the community matrix and $M$ is the binary mask of admissible entries:\n- Test case $1$ (mutualistic-like network; only off-diagonal entries are alterable):\n  - $A_1 = \\begin{bmatrix} -0.5  0.3  0.2 \\\\ 0.4  -0.6  0.1 \\\\ 0.2  0.15  -0.4 \\end{bmatrix}$\n  - $M_1 = \\begin{bmatrix} 0  1  1 \\\\ 1  0  1 \\\\ 1  1  0 \\end{bmatrix}$\n- Test case $2$ (trophic-like network; only a specified subset of cross-guild links are alterable):\n  - $A_2 = \\begin{bmatrix} -0.7  0.0  -0.9  0.0 \\\\ 0.3  -0.5  0.0  -0.6 \\\\ 0.8  0.0  -0.9  0.0 \\\\ 0.0  0.7  0.2  -0.4 \\end{bmatrix}$\n  - $M_2$ has ones at positions $(0,2)$, $(1,3)$, $(3,2)$ and zeros elsewhere, i.e.,\n    $M_2 = \\begin{bmatrix} 0  0  1  0 \\\\ 0  0  0  1 \\\\ 0  0  0  0 \\\\ 0  0  1  0 \\end{bmatrix}$\n- Test case $3$ (no admissible changes; mask is all zeros):\n  - $A_3 = \\begin{bmatrix} -0.3  0.2  0.0 \\\\ 0.1  -0.4  0.1 \\\\ 0.0  0.2  -0.5 \\end{bmatrix}$\n  - $M_3 = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$\n- Test case $4$ (predator–prey oscillatory subsystem; only that subsystem is alterable):\n  - $A_4 = \\begin{bmatrix} -0.1  -1.2  0.0 \\\\ 1.5  -0.1  0.0 \\\\ 0.0  0.0  -0.8 \\end{bmatrix}$\n  - $M_4 = \\begin{bmatrix} 1  1  0 \\\\ 1  1  0 \\\\ 0  0  0 \\end{bmatrix}$\n\nPrecise computational requirements:\n- Let $\\lambda_{\\max}(A)$ denote the eigenvalue of $A$ with the largest real part. If there is a tie in real parts, any one of the tied eigenvalues may be used; the provided test suite avoids exact ties.\n- Compute the first-order directional derivative of $\\Re\\{\\lambda_{\\max}(A)\\}$ at $A$ along admissible directions and choose the admissible direction with unit Frobenius norm that minimizes this derivative (i.e., gives the largest instantaneous decrease).\n- Report, for each test case, the value of this minimized directional derivative (a real number $\\le 0$). If $M$ is the zero mask, report $0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the test cases in order as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$.\n- Each $\\text{result}_k$ must be a floating-point number representing the maximal instantaneous decrease of $\\Re\\{\\lambda_{\\max}(A_k)\\}$ per unit Frobenius-norm admissible perturbation.", "solution": "The problem is evaluated to be valid as it is scientifically grounded in the established principles of ecological network stability analysis and matrix perturbation theory, is well-posed with a clear objective and constraints, and is expressed in precise, objective mathematical language.\n\nThe objective is to find the maximal instantaneous rate of decrease of the real part of the dominant eigenvalue, $\\Re\\{\\lambda_{\\max}(A)\\}$, of a community matrix $A \\in \\mathbb{R}^{n \\times n}$ under an infinitesimal perturbation $\\Delta \\in \\mathbb{R}^{n \\times n}$. This is equivalent to finding the minimum value of the directional derivative of $\\Re\\{\\lambda_{\\max}(A)\\}$ in the direction of admissible perturbations $\\Delta$. The admissible perturbations are constrained by two conditions:\n1.  Support constraint: The perturbation may only affect entries of $A$ where a binary mask $M \\in \\{0,1\\}^{n \\times n}$ is non-zero. This is expressed as $M \\odot \\Delta = \\Delta$, where $\\odot$ denotes the element-wise Hadamard product.\n2.  Normalization constraint: The perturbation must have a unit Frobenius norm, $\\|\\Delta\\|_{\\mathrm{F}} = 1$.\n\nThe solution proceeds by deriving an explicit expression for this minimum value using first-order matrix perturbation theory.\n\nLet $\\lambda$ be a simple eigenvalue of a matrix $A$ with corresponding right eigenvector $v$ and left eigenvector $w$. These are defined by the relations:\n$$Av = \\lambda v$$\n$$w^T A = \\lambda w^T$$\nThe second equation is equivalent to $A^T w = \\lambda w$. The problem statement guarantees that the dominant eigenvalue, $\\lambda_{\\max}(A)$, is simple for all given test cases.\n\nAccording to first-order perturbation theory, the change in the eigenvalue $\\lambda$ due to an infinitesimal perturbation $dA$ of the matrix $A$ is given by:\n$$d\\lambda = \\frac{w^T (dA) v}{w^T v}$$\nThis normalization factor $w^T v$ is non-zero for simple eigenvalues.\n\nTo find the maximal rate of decrease, we formulate this as a directional derivative. The directional derivative of $\\lambda$ at $A$ in the direction of a matrix $\\Delta$ is:\n$$D_{\\Delta}\\lambda(A) = \\frac{w^T \\Delta v}{w^T v}$$\nWe can express this derivative as a Frobenius inner product, $\\langle X, Y \\rangle_{\\mathrm{F}} = \\text{tr}(X^T Y)$. The term $w^T \\Delta v$ can be written as:\n$$w^T \\Delta v = \\sum_{i,j} w_i \\Delta_{ij} v_j = \\sum_{i,j} (w_i v_j) \\Delta_{ij} = \\text{tr}((w v^T)^T \\Delta) = \\langle w v^T, \\Delta \\rangle_{\\mathrm{F}}$$\nThus, the directional derivative is:\n$$D_{\\Delta}\\lambda(A) = \\frac{\\langle w v^T, \\Delta \\rangle_{\\mathrm{F}}}{w^T v} = \\left\\langle \\frac{(w v^T)^T}{w^T v}, \\Delta \\right\\rangle_{\\mathrm{F}}$$\nThis reveals that the gradient of the eigenvalue $\\lambda$ with respect to the matrix $A$ is $\\nabla_A \\lambda = \\frac{(w v^T)^T}{w^T v} = \\frac{v w^T}{w^T v}$.\n\nWe are tasked with minimizing the directional derivative of the real part of the dominant eigenvalue, $\\lambda_{\\max}$. Since the perturbation $\\Delta$ is a real matrix, we have:\n$$D_{\\Delta}\\Re\\{\\lambda_{\\max}\\} = \\Re\\{D_{\\Delta}\\lambda_{\\max}\\} = \\Re\\{\\langle \\nabla_A \\lambda_{\\max}, \\Delta \\rangle_{\\mathrm{F}}\\} = \\langle \\Re\\{\\nabla_A \\lambda_{\\max}\\}, \\Delta \\rangle_{\\mathrm{F}}$$\nLet $G = \\Re\\{\\nabla_A \\lambda_{\\max}\\} = \\Re\\left\\{\\frac{v w^T}{w^T v}\\right\\}$, where $v$ and $w$ correspond to $\\lambda_{\\max}$. The objective is to minimize $\\langle G, \\Delta \\rangle_{\\mathrm{F}}$ subject to the given constraints.\n\nThe support constraint $M \\odot \\Delta = \\Delta$ restricts the search space. We can incorporate this into the objective function:\n$$\\langle G, \\Delta \\rangle_{\\mathrm{F}} = \\langle G, M \\odot \\Delta \\rangle_{\\mathrm{F}} = \\langle M \\odot G, \\Delta \\rangle_{\\mathrm{F}}$$\nLet $G' = M \\odot G$. The problem reduces to minimizing $\\langle G', \\Delta \\rangle_{\\mathrm{F}}$ subject to $\\|\\Delta\\|_{\\mathrm{F}} = 1$.\n\nBy the Cauchy-Schwarz inequality, for any two matrices $X$ and $Y$, we have $|\\langle X, Y \\rangle_{\\mathrm{F}}| \\le \\|X\\|_{\\mathrm{F}} \\|Y\\|_{\\mathrm{F}}$. The minimum value of the inner product is achieved when $\\Delta$ is in the exact opposite direction of $G'$:\n$$\\Delta_{\\text{opt}} = -\\frac{G'}{\\|G'\\|_{\\mathrm{F}}}$$\nThe minimum value of the directional derivative is:\n$$\\min_{\\Delta} \\langle G', \\Delta \\rangle_{\\mathrm{F}} = \\left\\langle G', -\\frac{G'}{\\|G'\\|_{\\mathrm{F}}} \\right\\rangle_{\\mathrm{F}} = -\\frac{\\|G'\\|_{\\mathrm{F}}^2}{\\|G'\\|_{\\mathrm{F}}} = -\\|G'\\|_{\\mathrm{F}}$$\nIf $G'$ is the zero matrix (which occurs if $M$ is the zero matrix), the norm is $0$ and the minimum value is $0$.\n\nThe final expression for the maximal instantaneous decrease is therefore:\n$$-\\|G'\\|_{\\mathrm{F}} = -\\left\\| M \\odot \\Re\\left\\{\\frac{v w^T}{w^T v}\\right\\} \\right\\|_{\\mathrm{F}}$$\nwhere $v$ and $w$ are, respectively, the right and left eigenvectors corresponding to the dominant eigenvalue $\\lambda_{\\max}$ of the matrix $A$.\n\nThe computational procedure for each test case $(A, M)$ is as follows:\n1.  If the mask $M$ is the zero matrix, the result is $0.0$.\n2.  Compute the eigenvalues and right eigenvectors of $A$. Identify the dominant eigenvalue $\\lambda_{\\max}$ (the one with the largest real part) and its corresponding right eigenvector $v$.\n3.  Compute the eigenvalues and right eigenvectors of the transpose matrix, $A^T$. The eigenvalues will be the same as for $A$. Identify the eigenvector $w$ of $A^T$ that corresponds to $\\lambda_{\\max}$. This $w$ is the left eigenvector of $A$.\n4.  Calculate the gradient matrix $G = \\Re\\left\\{\\frac{v w^T}{w^T v}\\right\\}$. Note that the scalar product $w^T v$ and outer product $v w^T$ must handle complex-valued vectors if $\\lambda_{\\max}$ is complex. The eigenvectors returned by numerical libraries are typically normalized, but the denominator $w^T v$ is essential for correct scaling of the gradient and is not generally equal to $1$.\n5.  Apply the mask to the gradient matrix: $G' = M \\odot G$.\n6.  Compute the Frobenius norm of the masked gradient, $\\|G'\\|_{\\mathrm{F}}$.\n7.  The final result is the negative of this norm, $-\\|G'\\|_{\\mathrm{F}}$. This value is guaranteed to be less than or equal to zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all defined test cases and prints the results.\n    \"\"\"\n\n    def calculate_max_decrease(A, M):\n        \"\"\"\n        Computes the maximal instantaneous decrease of the real part of the \n        dominant eigenvalue of matrix A, for admissible perturbations defined by mask M.\n\n        Args:\n            A (np.ndarray): The square community matrix.\n            M (np.ndarray): The binary mask of admissible perturbations.\n\n        Returns:\n            float: The minimal value of the directional derivative, which represents\n                   the maximal instantaneous decrease (a non-positive number).\n        \"\"\"\n        # If the mask M is all zeros, no changes are allowed. The derivative is 0.\n        if not np.any(M):\n            return 0.0\n\n        # Step 1  2: Compute eigenvalues, right eigenvectors of A, and find dominant.\n        eigvals, right_eigvecs = np.linalg.eig(A)\n        dominant_idx = np.argmax(eigvals.real)\n        lambda_max = eigvals[dominant_idx]\n        v = right_eigvecs[:, dominant_idx]\n\n        # Step 3: Compute the corresponding left eigenvector. This is the right\n        # eigenvector of A.T corresponding to the same eigenvalue lambda_max.\n        eigvals_T, left_eigvecs = np.linalg.eig(A.T)\n        left_dominant_idx = np.argmin(np.abs(eigvals_T - lambda_max))\n        w = left_eigvecs[:, left_dominant_idx]\n        # In numerical computation, the left eigenvector must be conjugated if complex\n        # before being used in standard inner products (w^H v), but for the formula\n        # d(lambda) = w^T dA v / w^T v, we use w^T directly.\n        # Python's np.dot for complex vectors computes w^T v, not w^H v.\n\n        # Step 4: Calculate the gradient of the eigenvalue. grad_lambda_ij = (w_i v_j) / (w^T v).\n        # The gradient matrix is G = (w v^T) / (w^T v).\n        w_dot_v = np.dot(w, v)\n        \n        # The derivative is with respect to A_ji, so grad_{ij} = d(lambda)/d(A_{ij}) = v_j w_i / (w^T v).\n        # The gradient matrix is (v w^T) / (w^T v).\n        grad_lambda = np.outer(v, w) / w_dot_v\n\n        # Step 5: We need the real part of the gradient.\n        real_grad = grad_lambda.real\n\n        # Step 6: Apply the binary mask M.\n        masked_grad = M * real_grad\n\n        # Step 7: The maximal instantaneous decrease is the negative of the Frobenius norm\n        # of the masked gradient.\n        norm_masked_grad = np.linalg.norm(masked_grad, 'fro')\n\n        return -norm_masked_grad\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (np.array([[-0.5, 0.3, 0.2], \n                   [0.4, -0.6, 0.1], \n                   [0.2, 0.15, -0.4]]),\n         np.array([[0, 1, 1], \n                   [1, 0, 1], \n                   [1, 1, 0]])),\n        # Test case 2\n        (np.array([[-0.7, 0.0, -0.9, 0.0], \n                   [0.3, -0.5, 0.0, -0.6], \n                   [0.8, 0.0, -0.9, 0.0], \n                   [0.0, 0.7, 0.2, -0.4]]),\n         np.array([[0, 0, 1, 0], \n                   [0, 0, 0, 1], \n                   [0, 0, 0, 0], \n                   [0, 0, 1, 0]])),\n        # Test case 3\n        (np.array([[-0.3, 0.2, 0.0], \n                   [0.1, -0.4, 0.1], \n                   [0.0, 0.2, -0.5]]),\n         np.array([[0, 0, 0], \n                   [0, 0, 0], \n                   [0, 0, 0]])),\n        # Test case 4\n        (np.array([[-0.1, -1.2, 0.0], \n                   [1.5, -0.1, 0.0], \n                   [0.0, 0.0, -0.8]]),\n         np.array([[1, 1, 0], \n                   [1, 1, 0], \n                   [0, 0, 0]]))\n    ]\n\n    results = []\n    for A, M in test_cases:\n        result = calculate_max_decrease(A, M)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "2510908"}]}