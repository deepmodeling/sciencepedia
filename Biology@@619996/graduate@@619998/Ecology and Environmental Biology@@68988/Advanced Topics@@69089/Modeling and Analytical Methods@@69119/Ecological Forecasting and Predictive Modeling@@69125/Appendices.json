{"hands_on_practices": [{"introduction": "Fitting complex ecological models often reveals that multiple distinct parameter sets can explain the data almost equally well, a phenomenon known as equifinality. This practice tackles this challenge head-on by guiding you through the implementation of profile likelihood analysis for a state-space population model. By identifying which parameters are poorly constrained by the data, you will develop a crucial skill for assessing model reliability and strategically planning future research to reduce uncertainty [@problem_id:2482790].", "problem": "You are tasked with writing a complete, runnable program that detects equifinality among parameter sets in an ecological forecasting model using profile likelihoods and then proposes additional data to resolve it. The ecological context is a linear Gaussian state-space formulation of the Gompertz population model for log-abundance time series. Your program must follow a principle-based design starting from standard probability rules and well-tested facts about Gaussian state-space models and must produce a single line of output with specified content and format.\n\nModel and foundational base:\n- Consider the state process defined by\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t,\n$$\nwhere $\\eta_t \\sim \\mathcal{N}(0,\\sigma_p^2)$ are independent and identically distributed process innovations, and $\\mathcal{N}$ denotes a Gaussian distribution with the given variance. The observation process is\n$$\ny_t = z_t + \\epsilon_t,\n$$\nwhere $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_o^2)$ are independent and identically distributed measurement errors, independent of $\\eta_t$. The parameter vector is $\\theta = (\\phi,c,\\sigma_p,\\sigma_o)$.\n\n- For a given time series $\\{y_t\\}_{t=1}^T$, the one-step-ahead predictive distribution under this linear Gaussian state-space model is Gaussian. The exact log-likelihood can be computed by iteratively applying the Kalman filter prediction-update recursions implied by Gaussian conjugacy:\n  1. Prediction step (state): from posterior $(m_t, P_t)$, get prior $(m_{t+1|t}, P_{t+1|t})$ via\n  $$\n  m_{t+1|t} = \\phi m_t + c,\\quad P_{t+1|t} = \\phi^2 P_t + \\sigma_p^2.\n  $$\n  2. Observation prediction for $y_{t+1}$:\n  $$\n  \\hat{y}_{t+1} = m_{t+1|t},\\quad S_{t+1} = P_{t+1|t} + \\sigma_o^2,\\quad v_{t+1} = y_{t+1} - \\hat{y}_{t+1}.\n  $$\n  3. Update step (state): with Kalman gain $K_{t+1} = P_{t+1|t} / S_{t+1}$,\n  $$\n  m_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1},\\quad P_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}.\n  $$\n  The one-step-ahead log-likelihood contribution for $y_{t+1}$ is\n  $$\n  \\ell_{t+1} = -\\tfrac{1}{2}\\left(\\log(2\\pi) + \\log S_{t+1} + \\frac{v_{t+1}^2}{S_{t+1}}\\right).\n  $$\n  Use a diffuse prior for the state by initializing $m_1 = y_1$ and $P_1$ as a large variance (e.g., $P_1 = 10^6$), then apply the above for $t = 1,\\dots,T-1$ to sum $\\ell = \\sum_{t=1}^{T-1} \\ell_{t+1}$.\n\n- Forecast skill metric: Define the point-forecast for $y_{t+1}$ as $\\hat{y}_{t+1}=m_{t+1|t}$. Define one-step-ahead root mean square error (RMSE) as\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\left(y_{t+1} - \\hat{y}_{t+1}\\right)^2}.\n$$\n\nProfile likelihood and equifinality:\n- For a focal scalar parameter $\\psi \\in \\{\\phi, c, \\sigma_p, \\sigma_o\\}$, the profile log-likelihood over a grid $\\mathcal{G}_\\psi$ is\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ \\ell(\\theta),\n$$\nwhere $\\ell(\\theta)$ is the Kalman-filter log-likelihood. Similarly, define the minimal achievable forecast error at each fixed $\\psi$ as\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\ \\mathrm{RMSE}(\\theta).\n$$\n- Detect equifinality as follows. For each $\\psi$, compute:\n  - The set $\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}$ where $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ and $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$, with $\\chi^2_{1,0.95}$ the $0.95$ quantile of a chi-square distribution with $1$ degree of freedom.\n  - The set $\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}$, with $\\varepsilon = 0.05$.\n  - The intersection $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$. Define the discrete width ratio\n  $$\n  w_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}.\n  $$\n  Declare equifinality present if $\\max_{\\psi} w_\\psi \\ge \\tau$ with $\\tau = 0.3$. The parameter indicating the strongest equifinality is the $\\psi$ achieving the maximum $w_\\psi$ (break ties by choosing the smallest index as defined below).\n- To quantify the similarity in forecast skill among equifinal parameter values for the detected $\\psi^\\star$, compute the skill gap\n$$\ng = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}.\n$$\nIf no equifinality is detected, set $g = 0$.\n\nProgram requirements:\n- Implement the exact Kalman-filter-based log-likelihood and one-step-ahead RMSE as specified above.\n- Use the following fixed grids for reproducibility:\n  - Profile grids:\n    - $\\phi$: $\\mathcal{G}_\\phi = \\{\\text{linspace from } 0.2 \\text{ to } 1.2 \\text{ with } 21 \\text{ points}\\}$.\n    - $c$: $\\mathcal{G}_c = \\{\\text{linspace from } -0.1 \\text{ to } 0.5 \\text{ with } 21 \\text{ points}\\}$.\n    - $\\sigma_p$: $\\mathcal{G}_{\\sigma_p} = \\{\\text{logspace from } 0.03 \\text{ to } 0.6 \\text{ with } 21 \\text{ points}\\}$.\n    - $\\sigma_o$: $\\mathcal{G}_{\\sigma_o} = \\{\\text{logspace from } 0.03 \\text{ to } 0.6 \\text{ with } 21 \\text{ points}\\}$.\n  - Nuisance parameter grids for profiling:\n    - $\\phi$: $\\{\\text{linspace from } 0.0 \\text{ to } 1.2 \\text{ with } 11 \\text{ points}\\}$.\n    - $c$: $\\{\\text{linspace from } -0.3 \\text{ to } 0.7 \\text{ with } 11 \\text{ points}\\}$.\n    - $\\sigma_p$: $\\{\\text{logspace from } 0.01 \\text{ to } 1.0 \\text{ with } 11 \\text{ points}\\}$.\n    - $\\sigma_o$: $\\{\\text{logspace from } 0.01 \\text{ to } 1.0 \\text{ with } 11 \\text{ points}\\}$.\n  Here, “linspace” means evenly spaced in the linear scale and “logspace” denotes evenly spaced in the logarithmic scale for strictly positive bounds.\n\nTest suite:\n- Use exactly the following three time series (dimensionless log-abundances), each as a list of real numbers in the given order.\n  - Case $1$ (length $16$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.1,\\,\n    0.285,\\,\n    0.44225,\\,\n    0.5769125,\\,\n    0.690375625,\\,\n    0.78681928125,\\,\n    0.8687963890625,\\,\n    0.938477930703125,\\,\n    0.9977062410976562,\\,\n    1.0480503049320078,\\,\n    1.0908427591922066,\\,\n    1.1272163453133756,\\,\n    1.1581338935163692,\\,\n    1.1844138094889148,\\,\n    1.2067517380655776,\\,\n    1.22573997735574\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - Case $2$ (length $8$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.22,\\,\n    0.28,\\,\n    0.29,\\,\n    0.35,\\,\n    0.34,\\,\n    0.42,\\,\n    0.40,\\,\n    0.47\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - Case $3$ (length $16$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.52,\\,\n    0.49,\\,\n    0.51,\\,\n    0.5,\\,\n    0.53,\\,\n    0.48,\\,\n    0.5,\\,\n    0.51,\\,\n    0.49,\\,\n    0.5,\\,\n    0.52,\\,\n    0.47,\\,\n    0.5,\\,\n    0.51,\\,\n    0.5,\\,\n    0.49\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n\nIndices, decision thresholds, and proposal codes:\n- Parameter indices must be encoded as integers as follows: $\\phi \\rightarrow 0$, $c \\rightarrow 1$, $\\sigma_p \\rightarrow 2$, $\\sigma_o \\rightarrow 3$.\n- Equifinality decision thresholds must be $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$, $\\varepsilon = 0.05$, and $\\tau = 0.3$ exactly as defined above.\n- If equifinality is detected for the parameter with the largest width ratio $w_\\psi$, propose a single additional data type to most directly resolve it using the following integer code:\n  - $0$: no additional data needed (used only if no equifinality is detected).\n  - $1$: increase temporal resolution or time series length (more time points).\n  - $2$: replicate observations at each time to better estimate $\\sigma_o$.\n  - $3$: replicate process units (parallel populations) to better estimate $\\sigma_p$.\n  - $4$: measure an environmental covariate affecting the intercept $c$.\n  Mapping rule: if the detected parameter is $\\phi$, output $1$; if it is $c$, output $4$; if it is $\\sigma_o$, output $2$; if it is $\\sigma_p$, output $3$.\n\nOutput specification:\n- For each of the three cases, compute:\n  - an integer equifinality flag $f \\in \\{0,1\\}$ (with $1$ indicating equifinality detected),\n  - the integer index `p` of the parameter with the largest $w_\\psi$ (use `-1` if $f=0$),\n  - the integer proposal code `d` (use `0` if $f=0$),\n  - the skill gap float `g` as defined above (use `0.0` if $f=0$).\n- Your program should produce a single line of output containing the results as a comma-separated list of the three case results, each result itself being a list in the form $[f,p,d,g]$. For example, a syntactically correct output is\n$$\n[[1,0,1,0.0375],[0,-1,0,0.0],[1,3,2,0.0123]]\n$$\nwith the actual numeric values determined by your implementation on the provided test suite. No additional text must be printed.", "solution": "The problem posed is to develop and implement a computational procedure for detecting and characterizing equifinality in an ecological forecasting model. The model is a linear Gaussian state-space representation of Gompertz population dynamics. The procedure must be based on the principle of profile likelihood. If equifinality is detected, a recommendation for collecting additional data must be provided based on a predefined heuristic. The validity of the problem statement is confirmed; it is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. We now proceed with the derivation of the required algorithm from first principles.\n\nThe system is described by a state process for the log-abundance $z_t$ and an observation process for the measurement $y_t$.\nState Process:\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_p^2)\n$$\nObservation Process:\n$$\ny_t = z_t + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_o^2)\n$$\nThe complete parameter vector is $\\theta = (\\phi, c, \\sigma_p, \\sigma_o)$. For a given time series of observations $\\{y_t\\}_{t=1}^T$, our first task is to compute the log-likelihood function $\\ell(\\theta) = \\log P(\\{y_t\\}_{t=1}^T | \\theta)$ and the one-step-ahead forecast error.\n\n**1. Log-Likelihood and Forecast Error via Kalman Filter**\n\nFor a linear Gaussian state-space model, the exact log-likelihood is computable via the prediction-error decomposition, where the algorithm for recursively finding the conditional distributions is the Kalman filter. The distribution of the state $z_t$ conditional on observations up to time $t$, $Y_t = \\{y_1, \\dots, y_t\\}$, is Gaussian: $P(z_t | Y_t, \\theta) = \\mathcal{N}(z_t | m_t, P_t)$. The filter proceeds in two steps for each time point $t = 1, \\dots, T-1$.\n\nFirst, the **prediction step**: one projects the state distribution forward in time. The posterior at time $t$, $\\mathcal{N}(m_t, P_t)$, is propagated through the state equation to yield a prior for time $t+1$:\n$$\nP(z_{t+1} | Y_t, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1|t}, P_{t+1|t})\n$$\nwhere the moments are:\n$$\nm_{t+1|t} = \\phi m_t + c\n$$\n$$\nP_{t+1|t} = \\phi^2 P_t + \\sigma_p^2\n$$\nThis prior on the state $z_{t+1}$ induces a predictive distribution for the next observation $y_{t+1}$:\n$$\nP(y_{t+1} | Y_t, \\theta) = \\mathcal{N}(y_{t+1} | \\hat{y}_{t+1}, S_{t+1})\n$$\nwith mean $\\hat{y}_{t+1} = m_{t+1|t}$ and variance $S_{t+1} = P_{t+1|t} + \\sigma_o^2$. The term $v_{t+1} = y_{t+1} - \\hat{y}_{t+1}$ is the one-step-ahead prediction error or innovation.\n\nSecond, the **update step**: the new observation $y_{t+1}$ is used to update the state distribution via Bayes' rule, yielding the posterior at time $t+1$:\n$$\nP(z_{t+1} | Y_{t+1}, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1}, P_{t+1})\n$$\nThe updated moments are given by:\n$$\nK_{t+1} = P_{t+1|t} / S_{t+1} \\quad (\\text{Kalman Gain})\n$$\n$$\nm_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1}\n$$\n$$\nP_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}\n$$\nThe total log-likelihood is the sum of the log-likelihoods of the one-step-ahead predictions:\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T-1} \\log P(y_{t+1} | Y_t, \\theta) = \\sum_{t=1}^{T-1} \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log S_{t+1} - \\frac{1}{2}\\frac{v_{t+1}^2}{S_{t+1}} \\right)\n$$\nThe recursion is initialized with a diffuse prior on the state $z_1$. Following the specification, we set $m_1 = y_1$ and $P_1 = 10^6$. The one-step-ahead root mean square error (RMSE) is calculated from the innovations:\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} v_{t+1}^2}\n$$\n\n**2. Profile Likelihood and Profile RMSE**\n\nEquifinality, a condition where multiple distinct parameter sets yield similarly good fits to data, is analyzed using profile likelihoods. For a focal parameter $\\psi$ (which can be any of $\\phi, c, \\sigma_p, \\sigma_o$), the profile log-likelihood $\\ell_p(\\psi)$ is the maximal log-likelihood achievable by varying all other (nuisance) parameters.\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ell(\\theta)\n$$\nAnalogously, the profile RMSE measures the minimal forecast error for a fixed $\\psi$:\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\mathrm{RMSE}(\\theta)\n$$\nThe optimization is performed via a grid search over the specified nuisance parameter grids. For each point $\\psi_j$ in the focal parameter's grid $\\mathcal{G}_\\psi$, we evaluate $\\ell(\\theta)$ and $\\mathrm{RMSE}(\\theta)$ for all combinations of nuisance parameter values from their respective grids and retain the maximum $\\ell$ and minimum RMSE.\n\n**3. Equifinality Detection**\n\nEquifinality is detected by examining the flatness of the profile likelihood and profile RMSE surfaces.\nA likelihood-based confidence set for $\\psi$ is defined as:\n$$\n\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}\n$$\nwhere $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ and $\\Delta = \\frac{1}{2}\\chi^2_{1,0.95}$ is the threshold based on Wilks' theorem for a $95\\%$ confidence interval.\nA forecast-skill-based set of plausible parameters is defined as:\n$$\n\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}\n$$\nwith a relative error tolerance of $\\varepsilon = 0.05$.\nThe intersection of these sets, $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$, contains parameter values that are both statistically plausible and produce near-optimal forecasts. The degree of equifinality for parameter $\\psi$ is quantified by the relative size of this set:\n$$\nw_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}\n$$\nEquifinality is declared present if the maximum width ratio over all parameters exceeds a threshold $\\tau=0.3$: that is, if $\\max_\\psi w_\\psi \\ge 0.3$. The parameter $\\psi^\\star$ that maximizes $w_\\psi$ is deemed the one most affected by equifinality.\n\n**4. Skill Gap and Data Proposal**\n\nIf equifinality is detected for parameter $\\psi^\\star$, we quantify the variation in forecast skill among the equifinal parameter values by the skill gap $g$:\n$$\ng = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE_p}(\\psi)}\n$$\nThis quantifies the relative difference in forecast error between the best and worst-performing parameters within the equifinal set. If no equifinality is detected, $g$ is set to $0$.\n\nFinally, a recommendation for new data collection is made based on which parameter $\\psi^\\star$ exhibits the strongest equifinality. The mapping is based on the role of each parameter in the model:\n- $\\phi$ governs temporal dynamics: resolved by more time points (proposal code $1$).\n- $c$ is the intercept/mean-reversion level: resolved by measuring environmental drivers (proposal code $4$).\n- $\\sigma_p$ is process variance: resolved by replicating process units (e.g., parallel populations) (proposal code $3$).\n- $\\sigma_o$ is observation variance: resolved by replicating observations at each time point (proposal code $2$).\n\nIf no equifinality is detected, no new data is proposed (code $0$). The index of the maximally equifinal parameter is recorded, or set to $-1$ if none is detected. This completes the formal specification of the algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to perform equifinality analysis for all test cases.\n    \"\"\"\n    \n    # --- Define problem constants, thresholds, and grids ---\n    \n    # Test cases\n    test_cases = [\n        [0.1, 0.285, 0.44225, 0.5769125, 0.690375625, 0.78681928125,\n         0.8687963890625, 0.938477930703125, 0.9977062410976562, 1.0480503049320078,\n         1.0908427591922066, 1.1272163453133756, 1.1581338935163692, 1.1844138094889148,\n         1.2067517380655776, 1.22573997735574],\n        [0.22, 0.28, 0.29, 0.35, 0.34, 0.42, 0.40, 0.47],\n        [0.52, 0.49, 0.51, 0.5, 0.53, 0.48, 0.5, 0.51, 0.49, 0.5,\n         0.52, 0.47, 0.5, 0.51, 0.5, 0.49]\n    ]\n\n    # Parameter names and indices\n    PARAM_NAMES = ['phi', 'c', 'sigma_p', 'sigma_o']\n    PARAM_INDICES = {name: i for i, name in enumerate(PARAM_NAMES)}\n\n    # Parameter grids for profiling\n    PROFILE_GRIDS = {\n        'phi': np.linspace(0.2, 1.2, 21),\n        'c': np.linspace(-0.1, 0.5, 21),\n        'sigma_p': np.logspace(np.log10(0.03), np.log10(0.6), 21),\n        'sigma_o': np.logspace(np.log10(0.03), np.log10(0.6), 21)\n    }\n\n    # Nuisance parameter grids for optimization\n    NUISANCE_GRIDS = {\n        'phi': np.linspace(0.0, 1.2, 11),\n        'c': np.linspace(-0.3, 0.7, 11),\n        'sigma_p': np.logspace(np.log10(0.01), np.log10(1.0), 11),\n        'sigma_o': np.logspace(np.log10(0.01), np.log10(1.0), 11)\n    }\n    \n    # Decision thresholds and constants\n    DELTA = 0.5 * chi2.ppf(0.95, df=1)\n    EPSILON = 0.05\n    TAU = 0.3\n    \n    # Proposal code mapping: param_index -> data_code\n    PROPOSAL_MAP = {0: 1, 1: 4, 2: 3, 3: 2}\n\n    # --- Core functions ---\n\n    def kalman_filter_loglik_rmse(theta, y):\n        \"\"\"\n        Computes log-likelihood and RMSE using the Kalman filter.\n        theta: tuple (phi, c, sigma_p, sigma_o)\n        y: numpy array of time series data\n        \"\"\"\n        phi, c, sigma_p, sigma_o = theta\n        T = len(y)\n        \n        m_t = y[0]\n        P_t = 1e6\n        \n        log_likelihood_sum = 0.0\n        squared_error_sum = 0.0\n        \n        for t in range(1, T):\n            # Prediction\n            m_pred = phi * m_t + c\n            P_pred = phi**2 * P_t + sigma_p**2\n            \n            # Observation prediction\n            y_hat = m_pred\n            v = y[t] - y_hat\n            S = P_pred + sigma_o**2\n            \n            if S <= 0: # Numerical stability\n                return -np.inf, np.inf\n\n            # Log-likelihood contribution\n            log_likelihood_sum += -0.5 * (np.log(2 * np.pi) + np.log(S) + v**2 / S)\n            squared_error_sum += v**2\n\n            # Update\n            K = P_pred / S\n            m_t = m_pred + K * v\n            P_t = P_pred - K**2 * S\n\n        rmse = np.sqrt(squared_error_sum / (T - 1))\n        return log_likelihood_sum, rmse\n\n    # --- Main processing loop ---\n    \n    final_results = []\n    \n    for y_case in test_cases:\n        y_data = np.array(y_case)\n        w_values = []\n        all_profiles = {}\n\n        # Profile each of the 4 parameters\n        for focal_param_idx, focal_param_name in enumerate(PARAM_NAMES):\n            profile_loglik = []\n            profile_rmse = []\n            \n            nuisance_param_names = [p for p in PARAM_NAMES if p != focal_param_name]\n            nuisance_grids = [NUISANCE_GRIDS[name] for name in nuisance_param_names]\n\n            for focal_val in PROFILE_GRIDS[focal_param_name]:\n                max_loglik = -np.inf\n                min_rmse = np.inf\n                \n                # Grid search over nuisance parameters\n                for nuisance_vals in itertools.product(*nuisance_grids):\n                    params = {focal_param_name: focal_val}\n                    for i, name in enumerate(nuisance_param_names):\n                        params[name] = nuisance_vals[i]\n                    \n                    theta = (params['phi'], params['c'], params['sigma_p'], params['sigma_o'])\n                    \n                    loglik, rmse = kalman_filter_loglik_rmse(theta, y_data)\n                    \n                    max_loglik = max(max_loglik, loglik)\n                    min_rmse = min(min_rmse, rmse)\n                \n                profile_loglik.append(max_loglik)\n                profile_rmse.append(min_rmse)\n\n            profile_loglik = np.array(profile_loglik)\n            profile_rmse = np.array(profile_rmse)\n            all_profiles[focal_param_name] = {'loglik': profile_loglik, 'rmse': profile_rmse}\n\n            # Calculate width ratio w_psi\n            l_max = np.max(profile_loglik)\n            rmse_min = np.min(profile_rmse)\n\n            # Check for invalid profiles (e.g., all -inf)\n            if np.isinf(l_max) or np.isinf(rmse_min):\n                w = 0.0\n            else:\n                A_indices = np.where(profile_loglik >= l_max - DELTA)[0]\n                B_indices = np.where(profile_rmse <= (1.0 + EPSILON) * rmse_min)[0]\n                I_indices = np.intersect1d(A_indices, B_indices)\n                w = len(I_indices) / len(PROFILE_GRIDS[focal_param_name])\n            \n            w_values.append(w)\n\n        # Equifinality decision logic\n        max_w = np.max(w_values)\n        \n        if max_w >= TAU:\n            f = 1\n            p = np.argmax(w_values)\n            d = PROPOSAL_MAP[p]\n            \n            # Calculate skill gap g\n            winning_param_name = PARAM_NAMES[p]\n            profile_for_g = all_profiles[winning_param_name]\n            \n            l_max_g = np.max(profile_for_g['loglik'])\n            rmse_min_g = np.min(profile_for_g['rmse'])\n            \n            A_indices_g = np.where(profile_for_g['loglik'] >= l_max_g - DELTA)[0]\n            B_indices_g = np.where(profile_for_g['rmse'] <= (1.0 + EPSILON) * rmse_min_g)[0]\n            I_indices_g = np.intersect1d(A_indices_g, B_indices_g)\n            \n            rmses_in_I = profile_for_g['rmse'][I_indices_g]\n            min_rmse_in_I = np.min(rmses_in_I)\n            max_rmse_in_I = np.max(rmses_in_I)\n            \n            g = (max_rmse_in_I - min_rmse_in_I) / min_rmse_in_I if min_rmse_in_I > 0 else 0.0\n\n        else:\n            f = 0\n            p = -1\n            d = 0\n            g = 0.0\n            \n        final_results.append([f, p, d, g])\n\n    # Format and print the final output\n    case_strings = [f\"[{res[0]},{res[1]},{res[2]},{res[3]:.4g}]\" for res in final_results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "2482790"}, {"introduction": "A critical aspect of predictive modeling is defining the boundaries of a model's reliability. This practice addresses the risk of extrapolation, where a model is applied to conditions different from its training data, potentially yielding nonsensical forecasts. You will implement a principled abstention rule using the Mahalanobis distance to quantify how \"novel\" a new set of conditions is relative to the training domain [@problem_id:2482817]. This provides a hands-on method for building safeguards into forecasting workflows, ensuring that predictions are made with a clear understanding of their domain of applicability.", "problem": "You are tasked with implementing a principled out-of-domain abstention rule for ecological forecasting features using Mahalanobis distance to the estimated training feature distribution. The fundamental base is the well-tested property that, under a multivariate normal model, the squared Mahalanobis distance of a feature vector to its population mean with respect to its population covariance is chi-squared distributed with degrees of freedom equal to the feature dimension. You must work entirely from first principles and avoid any shortcut formulas not derived from this base.\n\nDefinitions and setup:\n- Let the training feature set be $X_{\\text{train}} = \\{\\mathbf{x}_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, where each $\\mathbf{x}_i \\in \\mathbb{R}^d$ represents ecological covariates (for example, temperature anomaly, precipitation, normalized difference vegetation index, human footprint index), and $n \\in \\mathbb{N}$, $d \\in \\mathbb{N}$.\n- Estimate the mean $\\widehat{\\boldsymbol{\\mu}} \\in \\mathbb{R}^d$ by the unbiased sample mean\n$$\n\\widehat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i.\n$$\n- Estimate the covariance $\\widehat{\\boldsymbol{\\Sigma}} \\in \\mathbb{R}^{d \\times d}$ by the unbiased sample covariance\n$$\n\\widehat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})^\\top.\n$$\n- To ensure numerical stability and handle singular or ill-conditioned covariance estimates, use Tikhonov regularization (ridge) to form\n$$\n\\widehat{\\boldsymbol{\\Sigma}}_\\gamma = \\widehat{\\boldsymbol{\\Sigma}} + \\gamma \\mathbf{I}_d,\n$$\nwhere $\\mathbf{I}_d$ is the $d \\times d$ identity matrix, and the ridge parameter $\\gamma \\ge 0$ is defined by\n$$\n\\gamma = \\lambda \\cdot \\frac{\\operatorname{tr}(\\widehat{\\boldsymbol{\\Sigma}})}{d},\n$$\nwith user-specified $\\lambda \\ge 0$.\n- For any new feature vector $\\mathbf{x} \\in \\mathbb{R}^d$, define the squared Mahalanobis distance\n$$\nM^2(\\mathbf{x}) = (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})^\\top \\widehat{\\boldsymbol{\\Sigma}}_\\gamma^{-1} (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}}).\n$$\n- Under the assumption that the features are generated from a $d$-dimensional multivariate normal distribution with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$, it is a well-tested fact that the population version satisfies $M^2(\\mathbf{x}) \\sim \\chi^2_d$ when evaluated with the true parameters. This motivates a conservative abstention rule using the chi-squared quantile with degree $d$:\n  - Let the significance level be $\\alpha \\in (0,1)$ and the threshold be\n  $$\n  \\tau = F^{-1}_{\\chi^2_d}(1 - \\alpha),\n  $$\n  where $F^{-1}_{\\chi^2_d}$ is the quantile function of the chi-squared distribution with $d$ degrees of freedom.\n  - The abstention rule is: abstain on $\\mathbf{x}$ if and only if $M^2(\\mathbf{x}) > \\tau$; otherwise, accept.\n\nYour program must:\n- Implement the estimation of $\\widehat{\\boldsymbol{\\mu}}$, $\\widehat{\\boldsymbol{\\Sigma}}$, the regularized covariance $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$, and compute $M^2(\\mathbf{x})$ for all provided test points using a numerically stable method.\n- For each test case, compute:\n  1. The fraction of abstained points among the provided new points, as a decimal in $[0,1]$.\n  2. The maximum squared Mahalanobis distance $\\max_j M^2(\\mathbf{x}^{\\text{new}}_j)$ among the provided new points.\n- Round both quantities to three decimal places for reporting.\n- The acceptance rule at the threshold must be strict: accept if $M^2(\\mathbf{x}) \\le \\tau$ and abstain if $M^2(\\mathbf{x}) > \\tau$.\n\nTest suite:\nYou must run exactly 4 test cases. For each case, the inputs are $(X_{\\text{train}}, X_{\\text{new}}, \\alpha, \\lambda)$, except where the new points are constructed from the training statistics as specified. All numbers below are given explicitly to ensure scientific realism and reproducibility.\n\n- Test case $1$ (general, well-conditioned):\n  - Dimension $d = 3$.\n  - Training data $X^{(1)}_{\\text{train}}$ with $n = 10$:\n    $$\n    X^{(1)}_{\\text{train}} =\n    \\begin{bmatrix}\n    1.0 & 2.0 & 1.0 \\\\\n    0.0 & 1.0 & 0.9 \\\\\n    2.0 & 2.5 & 1.1 \\\\\n    1.5 & 1.8 & 1.2 \\\\\n    1.2 & 2.2 & 0.8 \\\\\n    0.8 & 1.7 & 1.0 \\\\\n    1.1 & 1.9 & 1.3 \\\\\n    1.3 & 2.1 & 0.7 \\\\\n    0.9 & 2.0 & 1.0 \\\\\n    1.4 & 2.3 & 0.9\n    \\end{bmatrix}.\n    $$\n  - New data $X^{(1)}_{\\text{new}}$ with $m = 3$:\n    $$\n    X^{(1)}_{\\text{new}} =\n    \\begin{bmatrix}\n    1.1 & 2.0 & 1.0 \\\\\n    1.0 & 1.8 & 1.1 \\\\\n    4.0 & 6.0 & -1.0\n    \\end{bmatrix}.\n    $$\n  - Significance $\\alpha = 0.05$.\n  - Ridge weight $\\lambda = 0.0$.\n\n- Test case $2$ (highly correlated features, out-of-manifold deviation):\n  - Dimension $d = 2$.\n  - Training data $X^{(2)}_{\\text{train}}$ with $n = 8$:\n    $$\n    X^{(2)}_{\\text{train}} =\n    \\begin{bmatrix}\n    0.0 & 1.0 \\\\\n    0.5 & 2.5 \\\\\n    1.0 & 4.0 \\\\\n    1.5 & 5.5 \\\\\n    2.0 & 7.0 \\\\\n    2.5 & 8.5 \\\\\n    3.0 & 10.0 \\\\\n    3.5 & 11.5\n    \\end{bmatrix}.\n    $$\n  - New data $X^{(2)}_{\\text{new}}$ with $m = 2$:\n    $$\n    X^{(2)}_{\\text{new}} =\n    \\begin{bmatrix}\n    4.0 & 13.0 \\\\\n    2.0 & 12.0\n    \\end{bmatrix}.\n    $$\n  - Significance $\\alpha = 0.01$.\n  - Ridge weight $\\lambda = 0.0$.\n\n- Test case $3$ (near-singular covariance requiring regularization):\n  - Dimension $d = 4$.\n  - Training data $X^{(3)}_{\\text{train}}$ with $n = 3$:\n    $$\n    X^{(3)}_{\\text{train}} =\n    \\begin{bmatrix}\n    1.0 & 2.0 & 3.0 & 4.0 \\\\\n    1.1 & 2.1 & 3.1 & 4.1 \\\\\n    0.9 & 1.9 & 2.9 & 3.9\n    \\end{bmatrix}.\n    $$\n  - New data $X^{(3)}_{\\text{new}}$ with $m = 2$:\n    $$\n    X^{(3)}_{\\text{new}} =\n    \\begin{bmatrix}\n    1.05 & 2.05 & 3.05 & 4.05 \\\\\n    3.0 & 6.0 & 9.0 & 12.0\n    \\end{bmatrix}.\n    $$\n  - Significance $\\alpha = 0.05$.\n  - Ridge weight $\\lambda = 0.1$.\n\n- Test case $4$ (boundary case constructed from estimates; acceptance on the threshold):\n  - Use the training data $X^{(1)}_{\\text{train}}$ from Test case $1$ and its dimension $d = 3$.\n  - Significance $\\alpha = 0.05$.\n  - Ridge weight $\\lambda = 0.0$.\n  - Construct two new points as follows. Compute $\\widehat{\\boldsymbol{\\mu}}$ and $\\widehat{\\boldsymbol{\\Sigma}}$ from $X^{(1)}_{\\text{train}}$, set $\\gamma$ from $\\lambda$, form $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$, and compute the threshold $\\tau = F^{-1}_{\\chi^2_d}(1-\\alpha)$. Compute a Cholesky factor $\\mathbf{L}$ such that $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma = \\mathbf{L}\\mathbf{L}^\\top$. Let $\\mathbf{e}_1 \\in \\mathbb{R}^d$ be the first standard basis vector. Define\n    $$\n    \\mathbf{x}_{\\text{on}} = \\widehat{\\boldsymbol{\\mu}} + \\mathbf{L}\\,\\mathbf{e}_1 \\sqrt{\\tau}, \\quad\n    \\mathbf{x}_{\\text{beyond}} = \\widehat{\\boldsymbol{\\mu}} + \\mathbf{L}\\,\\mathbf{e}_1 \\left(\\sqrt{\\tau} + \\varepsilon\\right),\n    $$\n    with $\\varepsilon = 10^{-6}$. The abstention rule must accept $\\mathbf{x}_{\\text{on}}$ and abstain on $\\mathbf{x}_{\\text{beyond}}$.\n\nNumerical and algorithmic requirements:\n- Implement Mahalanobis computations via a numerically stable method. If a Cholesky factorization of $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$ succeeds, compute $M^2(\\mathbf{x})$ by solving $\\mathbf{L}\\mathbf{y} = (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})$ and evaluating $M^2(\\mathbf{x}) = \\|\\mathbf{y}\\|_2^2$. If Cholesky fails, fall back to a Moore–Penrose pseudoinverse to evaluate $M^2(\\mathbf{x})$.\n- For each test case, report:\n  - The fraction of abstained points among the new points as a decimal rounded to three decimal places.\n  - The maximum squared Mahalanobis distance among the new points rounded to three decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case in order and containing the two rounded values in the order [fraction_abstained, max_squared_distance]. For example, the output should look like\n$[[0.000,0.000],[0.000,0.000],[0.000,0.000],[0.000,0.000]]$\nbut with the actual computed values for the specified test suite.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in established principles of multivariate statistics, well-posed with all necessary information provided, and objective in its formulation. The problem asks for the implementation of an abstention mechanism for ecological forecasting models, a valid and practical application of outlier detection. The method is based on the Mahalanobis distance and its theoretical relationship to the chi-squared distribution under the assumption of multivariate normality. The provided test cases are designed to verify the correctness of the implementation under various conditions, including a well-conditioned case, a near-singular covariance matrix, a rank-deficient case where $n < d$, and a boundary case to test numerical precision.\n\nThe solution is developed from first principles as follows.\n\n**1. Statistical Foundation**\n\nThe core of the method lies in measuring the \"distance\" of a new feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ from the center of the training data distribution. The Mahalanobis distance provides a statistically principled way to do this by accounting for the variance and correlation of the features. For a feature distribution with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$, the squared Mahalanobis distance is defined as $M^2(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})$.\n\nA fundamental result of multivariate statistics states that if $\\mathbf{x}$ is drawn from a $d$-dimensional multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, then the quantity $M^2(\\mathbf{x})$ follows a chi-squared distribution with $d$ degrees of freedom, denoted $\\chi^2_d$. This property allows us to construct a hypothesis test. We can define a region of \"typical\" or \"in-domain\" points and classify any point falling outside this region as an outlier, for which the model should abstain from making a prediction.\n\n**2. Algorithmic Implementation Steps**\n\nThe procedure involves several sequential steps:\n\n**Step A: Estimation of Distribution Parameters**\nGiven a training dataset $X_{\\text{train}} = \\{\\mathbf{x}_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, the true population parameters $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are unknown and must be estimated. We use the unbiased sample estimators:\n-   Sample Mean: $\\widehat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i$\n-   Sample Covariance: $\\widehat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}})^\\top$\n\nThe denominator $n-1$ corresponds to Bessel's correction for an unbiased estimate of the variance.\n\n**Step B: Covariance Matrix Regularization**\nThe sample covariance matrix $\\widehat{\\boldsymbol{\\Sigma}}$ can be ill-conditioned or singular, especially if the number of samples $n$ is not much larger than the dimension $d$, or if features are highly correlated. Singularity is guaranteed if $n \\le d$, as is the case in one of the test scenarios. To ensure the matrix is invertible and the problem is numerically stable, Tikhonov regularization (also known as ridge regularization) is applied. The regularized covariance matrix $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$ is computed as:\n$$\n\\widehat{\\boldsymbol{\\Sigma}}_\\gamma = \\widehat{\\boldsymbol{\\Sigma}} + \\gamma \\mathbf{I}_d\n$$\nwhere $\\mathbf{I}_d$ is the $d \\times d$ identity matrix and $\\gamma \\ge 0$ is a regularization parameter. This is equivalent to adding a small positive value to the diagonal elements of the covariance matrix, which effectively inflates the variance of each feature slightly, ensuring the matrix becomes positive definite. The parameter $\\gamma$ is defined as:\n$$\n\\gamma = \\lambda \\cdot \\frac{\\operatorname{tr}(\\widehat{\\boldsymbol{\\Sigma}})}{d}\n$$\nwhere $\\operatorname{tr}(\\widehat{\\boldsymbol{\\Sigma}})$ is the trace of the sample covariance matrix (the sum of variances) and $\\lambda \\ge 0$ is a user-specified hyperparameter controlling the strength of regularization. For $\\lambda = 0$, no regularization is applied.\n\n**Step C: Abstention Threshold Calculation**\nUsing the $\\chi^2_d$ distribution as a model for the squared Mahalanobis distances of in-domain points, we define a threshold $\\tau$ for abstention. Given a significance level $\\alpha \\in (0, 1)$, which represents the acceptable rate of incorrectly abstaining on in-domain points, the threshold is the $(1 - \\alpha)$-quantile of the $\\chi^2_d$ distribution:\n$$\n\\tau = F^{-1}_{\\chi^2_d}(1 - \\alpha)\n$$\nwhere $F^{-1}_{\\chi^2_d}$ is the inverse cumulative distribution function (CDF), or quantile function, for the $\\chi^2_d$ distribution. A new point $\\mathbf{x}$ will be considered out-of-domain if its squared Mahalanobis distance exceeds this threshold.\n\n**Step D: Numerically Stable Mahalanobis Distance Calculation**\nFor any new feature vector $\\mathbf{x}$, we calculate its squared Mahalanobis distance with respect to the estimated and regularized parameters:\n$$\nM^2(\\mathbf{x}) = (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})^\\top \\widehat{\\boldsymbol{\\Sigma}}_\\gamma^{-1} (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})\n$$\nDirectly inverting $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$ can be numerically unstable. A superior method is to use a matrix factorization. Since $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$ is symmetric and (due to regularization) positive definite, we can use the Cholesky decomposition to find a lower-triangular matrix $\\mathbf{L}$ such that $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma = \\mathbf{L}\\mathbf{L}^\\top$. Then the inverse is $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma^{-1} = (\\mathbf{L}^\\top)^{-1}\\mathbf{L}^{-1}$. The distance calculation becomes:\n$$\nM^2(\\mathbf{x}) = (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})^\\top (\\mathbf{L}^\\top)^{-1}\\mathbf{L}^{-1} (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}}) = \\mathbf{y}^\\top\\mathbf{y} = \\|\\mathbf{y}\\|_2^2\n$$\nwhere $\\mathbf{y} = \\mathbf{L}^{-1}(\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})$. Instead of inverting $\\mathbf{L}$, we solve the stable triangular system $\\mathbf{L}\\mathbf{y} = (\\mathbf{x} - \\widehat{\\boldsymbol{\\mu}})$ for $\\mathbf{y}$ using forward substitution.\n\nIf $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma$ is not positive definite (which can happen if $\\lambda = 0$ and $\\widehat{\\boldsymbol{\\Sigma}}$ is singular), the Cholesky decomposition fails. In this event, we fall back to using the Moore-Penrose pseudoinverse, $\\widehat{\\boldsymbol{\\Sigma}}_\\gamma^+$, to compute the distance.\n\n**Step E: Decision and Reporting**\nFor each new point $\\mathbf{x}^{\\text{new}}_j$ in the test set $X_{\\text{new}}$, we compute $M^2(\\mathbf{x}^{\\text{new}}_j)$ and apply the decision rule:\n-   If $M^2(\\mathbf{x}^{\\text{new}}_j) > \\tau$, abstain.\n-   If $M^2(\\mathbf{x}^{\\text{new}}_j) \\le \\tau$, accept.\n\nThe final required outputs are the fraction of abstained points and the maximum observed squared Mahalanobis distance among the points in $X_{\\text{new}}$. This procedure is applied systematically to all specified test cases. Test Case 4 serves as a precise verification, confirming that a point constructed to lie exactly on the boundary, $M^2(\\mathbf{x}) = \\tau$, is correctly accepted, while a point infinitesimally beyond it is correctly rejected.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the Mahalanobis abstention rule.\n    \"\"\"\n\n    def _calculate_mahalanobis_sq(\n        X_new: np.ndarray, mu_hat: np.ndarray, S_gamma: np.ndarray\n    ) -> list[float]:\n        \"\"\"\n        Calculates the squared Mahalanobis distance for each point in X_new.\n\n        It uses Cholesky decomposition for numerical stability, falling back to\n        the Moore-Penrose pseudoinverse if the covariance matrix is not\n        positive definite.\n        \"\"\"\n        m2_distances = []\n        d = S_gamma.shape[0]\n\n        try:\n            # Attempt Cholesky decomposition, the preferred stable method.\n            L = np.linalg.cholesky(S_gamma)\n            for x_new in X_new:\n                delta = x_new - mu_hat\n                # Solve L * y = delta using forward substitution\n                y = solve_triangular(L, delta, lower=True)\n                # M^2 = y^T * y\n                m2_distances.append(np.dot(y, y))\n        except np.linalg.LinAlgError:\n            # Fallback for non-positive-definite matrices (e.g., singular cov with lambda=0)\n            S_gamma_pinv = np.linalg.pinv(S_gamma)\n            for x_new in X_new:\n                delta = x_new - mu_hat\n                m2 = delta.T @ S_gamma_pinv @ delta\n                m2_distances.append(m2)\n\n        return m2_distances\n\n    def _process_case(\n        X_train_raw: list, X_new_raw: list | None, alpha: float, lambda_val: float,\n        construct_X_new: bool = False\n    ) -> list[float]:\n        \"\"\"\n        Processes a single test case.\n        \"\"\"\n        X_train = np.array(X_train_raw, dtype=np.float64)\n        n, d = X_train.shape\n\n        # Step 1: Estimate mean and covariance\n        mu_hat = np.mean(X_train, axis=0)\n        # ddof=1 for unbiased sample covariance (division by n-1)\n        # The covariance of a 1-D array is 0, so handle n=1 case if needed, though not in these tests.\n        if n > 1:\n            S_hat = np.cov(X_train, rowvar=False, ddof=1)\n        else:\n            S_hat = np.zeros((d, d))\n        \n        # Step 2: Regularize covariance matrix\n        gamma = lambda_val * (np.trace(S_hat) / d) if d > 0 else 0\n        S_gamma = S_hat + gamma * np.identity(d)\n\n        # Step 3: Calculate abstention threshold\n        tau = chi2.ppf(1 - alpha, df=d)\n\n        # Handle Test Case 4's special construction of new points\n        if construct_X_new:\n            try:\n                L = np.linalg.cholesky(S_gamma)\n                e1 = np.zeros(d)\n                e1[0] = 1.0\n                sqrt_tau = np.sqrt(tau)\n                epsilon = 1e-6\n                \n                x_on = mu_hat + L @ (e1 * sqrt_tau)\n                x_beyond = mu_hat + L @ (e1 * (sqrt_tau + epsilon))\n                \n                X_new = np.array([x_on, x_beyond])\n            except np.linalg.LinAlgError:\n                # Should not happen if S_gamma is constructed to be positive definite\n                # but handle as a contingency.\n                raise ValueError(\"Cholesky decomposition failed for X_new construction.\")\n        else:\n            X_new = np.array(X_new_raw, dtype=np.float64)\n\n        # Step 4: Calculate Mahalanobis distances for new points\n        m2_distances = _calculate_mahalanobis_sq(X_new, mu_hat, S_gamma)\n\n        # Step 5: Apply abstention rule and collect results\n        abstained_count = sum(1 for m2 in m2_distances if m2 > tau)\n        fraction_abstained = abstained_count / len(X_new) if len(X_new) > 0 else 0.0\n        max_m2 = max(m2_distances) if m2_distances else 0.0\n        \n        return [fraction_abstained, max_m2]\n\n    # Test Case 1\n    X_train_1 = [\n        [1.0, 2.0, 1.0], [0.0, 1.0, 0.9], [2.0, 2.5, 1.1], [1.5, 1.8, 1.2],\n        [1.2, 2.2, 0.8], [0.8, 1.7, 1.0], [1.1, 1.9, 1.3], [1.3, 2.1, 0.7],\n        [0.9, 2.0, 1.0], [1.4, 2.3, 0.9]\n    ]\n    X_new_1 = [[1.1, 2.0, 1.0], [1.0, 1.8, 1.1], [4.0, 6.0, -1.0]]\n    alpha_1 = 0.05\n    lambda_1 = 0.0\n\n    # Test Case 2\n    X_train_2 = [\n        [0.0, 1.0], [0.5, 2.5], [1.0, 4.0], [1.5, 5.5],\n        [2.0, 7.0], [2.5, 8.5], [3.0, 10.0], [3.5, 11.5]\n    ]\n    X_new_2 = [[4.0, 13.0], [2.0, 12.0]]\n    alpha_2 = 0.01\n    lambda_2 = 0.0\n\n    # Test Case 3\n    X_train_3 = [\n        [1.0, 2.0, 3.0, 4.0],\n        [1.1, 2.1, 3.1, 4.1],\n        [0.9, 1.9, 2.9, 3.9]\n    ]\n    X_new_3 = [[1.05, 2.05, 3.05, 4.05], [3.0, 6.0, 9.0, 12.0]]\n    alpha_3 = 0.05\n    lambda_3 = 0.1\n\n    # Test Case 4 uses data from Test Case 1\n    X_train_4 = X_train_1\n    alpha_4 = 0.05\n    lambda_4 = 0.0\n\n    test_cases = [\n        (X_train_1, X_new_1, alpha_1, lambda_1, False),\n        (X_train_2, X_new_2, alpha_2, lambda_2, False),\n        (X_train_3, X_new_3, alpha_3, lambda_3, False),\n        (X_train_4, None, alpha_4, lambda_4, True),\n    ]\n\n    results = []\n    for case in test_cases:\n        X_train, X_new, alpha, lambda_val, construct_flag = case\n        result = _process_case(X_train, X_new, alpha, lambda_val, construct_flag)\n        results.append(result)\n\n    # Format output according to specification: [[val1,val2],[val3,val4],...]\n    # without spaces and with three decimal places.\n    formatted_cases = [f\"[{res[0]:.3f},{res[1]:.3f}]\" for res in results]\n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```", "id": "2482817"}, {"introduction": "Ecological monitoring networks are expensive to maintain and expand, demanding that we allocate resources as efficiently as possible. This practice introduces the powerful framework of Bayesian optimal experimental design, treating the augmentation of a monitoring network as a formal optimization problem. You will learn how to allocate a fixed sampling budget across different sites to achieve the greatest possible reduction in predictive uncertainty for a specific ecological quantity of interest [@problem_id:2482812]. This exercise demonstrates how to use models proactively to guide data collection, creating a powerful feedback loop between modeling and empirical work.", "problem": "You are tasked with designing a program that augments an ecological observation network by allocating fixed monitoring effort across observation sites so as to minimize the posterior predictive variance of a specified linear functional of an ecological state. The mathematical setting is the following. The unknown ecological state is a vector $\\theta \\in \\mathbb{R}^d$ with a Gaussian prior $\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$, where $\\mu_0 \\in \\mathbb{R}^d$ and $\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. You may assume that $\\mu_0$ is known but does not affect the variance calculations. There are $m$ candidate observation sites. If site $i \\in \\{1,\\dots,m\\}$ is sampled with nonnegative effort $e_i \\ge 0$, one obtains a scalar observation modeled as $y_i = h_i^\\top \\theta + \\varepsilon_i$, where $h_i \\in \\mathbb{R}^d$ is known and the observation errors are independent with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 / e_i)$, following the common variance–effort relationship from sampling theory in which the variance decreases inversely with effort. The efforts must satisfy a fixed total budget $\\sum_{i=1}^m e_i = B$, where $B \\ge 0$ is specified. The predictive target is a linear functional $\\psi = g^\\top \\theta$ for a known $g \\in \\mathbb{R}^d$. The objective is to choose the effort vector $e = (e_1,\\dots,e_m)$ to minimize the expected posterior predictive variance of $\\psi$ before observing any data. The expected information gain may be quantified by the reduction in uncertainty about $\\psi$ under the Gaussian model, and in the linear-Gaussian setting the expected posterior variance depends only on the design $e$ and not on the as-yet-unseen data.\n\nStarting only from the laws of Gaussian conditioning, independence of observation errors, and the variance–effort relationship for $\\varepsilon_i$ specified above, derive the optimization objective for the expected posterior predictive variance of $\\psi$ as a function of $e$, and set up a computational method to find an optimal $e$ subject to $\\sum_i e_i = B$ and $e_i \\ge 0$. Your program should numerically solve this allocation problem for each test case below using a principled continuous optimization method with gradients, and then report the optimized effort allocation together with the minimized expected posterior predictive variance of $\\psi$. All arithmetic and linear algebra must be carried out in real numbers.\n\nTest suite. For each case $k$, you are given the dimension $d_k$, number of sites $m_k$, prior covariance $\\Sigma_{0,k}$, site vectors $h_{i,k}$, noise scale parameters $\\sigma_{i,k}^2$, target vector $g_k$, and budget $B_k$. In every case, assume $\\mu_{0,k}$ is arbitrary and does not affect the variance calculation.\n\n- Case $1$ (general, informative in multiple directions):\n  - $d_1 = 3$, $m_1 = 3$.\n  - $\\Sigma_{0,1} = \\begin{bmatrix} 1.0 & 0.2 & 0.1 \\\\ 0.2 & 0.5 & 0.1 \\\\ 0.1 & 0.1 & 1.5 \\end{bmatrix}$.\n  - $h_{1,1}^\\top = [1.0, 0.2, 0.0]$, $h_{2,1}^\\top = [0.3, 1.0, 0.5]$, $h_{3,1}^\\top = [0.0, 0.2, 1.0]$.\n  - $\\sigma_{1,1}^2 = 2.0$, $\\sigma_{2,1}^2 = 1.5$, $\\sigma_{3,1}^2 = 3.0$.\n  - $g_1^\\top = [0.5, 0.3, 0.2]$.\n  - $B_1 = 4.0$.\n\n- Case $2$ (boundary: zero budget):\n  - Same as Case $1$ except $B_2 = 0.0$.\n\n- Case $3$ (mixed information geometry, more sites than state dimension):\n  - $d_3 = 2$, $m_3 = 4$.\n  - $\\Sigma_{0,3} = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.2 \\end{bmatrix}$.\n  - $h_{1,3}^\\top = [1.0, 0.0]$, $h_{2,3}^\\top = [0.0, 1.0]$, $h_{3,3}^\\top = [0.7, 0.7]$, $h_{4,3}^\\top = [1.0, -0.2]$.\n  - $\\sigma_{1,3}^2 = 0.6$, $\\sigma_{2,3}^2 = 2.0$, $\\sigma_{3,3}^2 = 1.5$, $\\sigma_{4,3}^2 = 0.8$.\n  - $g_3^\\top = [1.0, 0.5]$.\n  - $B_3 = 3.0$.\n\nYour program must, for each case $k \\in \\{1,2,3\\}$, compute an optimal allocation $e^\\star_k$ and the minimized expected posterior predictive variance $v^\\star_k$ of $\\psi_k = g_k^\\top \\theta_k$. Then produce a single line of output containing a list of length $3$, where the $k$-th element is itself a list consisting of the $m_k$ elements of $e^\\star_k$ followed by $v^\\star_k$. Format each effort $e^\\star_{i,k}$ rounded to exactly 4 decimal places and each minimized variance $v^\\star_k$ rounded to exactly 6 decimal places. The final output must be a single line of the form `[[e^\\star_{1,1},...,e^\\star_{m_1,1},v^\\star_1],[e^\\star_{1,2},...,e^\\star_{m_2,2},v^\\star_2],[e^\\star_{1,3},...,e^\\star_{m_3,3},v^\\star_3]]` with commas and brackets exactly as shown and no spaces inserted.\n\nAngle units are not applicable. There are no physical units to report. All outputs must be real numbers. The program must not read any input and must run to completion and print only the specified single-line output.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in Bayesian optimal experimental design. It is a specific instance of c-optimality, where the objective is to minimize the posterior variance of a single linear combination of the unknown state parameters. The problem is valid and can be solved using established principles of Bayesian statistics and convex optimization. We will proceed to derive the solution.\n\nThe problem is defined within a linear-Gaussian framework. The unknown state is a $d$-dimensional vector $\\theta \\in \\mathbb{R}^d$, with a Gaussian prior distribution:\n$$\n\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n$$\nwhere $\\mu_0$ is the prior mean and $\\Sigma_0$ is the $d \\times d$ symmetric positive definite prior covariance matrix.\n\nObservations are made at $m$ sites. For each site $i \\in \\{1, \\dots, m\\}$, the observation $y_i$ is a linear function of the state $\\theta$ corrupted by independent Gaussian noise:\n$$\ny_i = h_i^\\top \\theta + \\varepsilon_i\n$$\nThe observation vectors $h_i \\in \\mathbb{R}^d$ are known. The noise terms $\\varepsilon_i$ are independent and normally distributed, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 / e_i)$, where $e_i \\ge 0$ is the sampling effort at site $i$, and $\\sigma_i^2$ is a known scaling factor. The total effort is constrained by a budget $B$, such that $\\sum_{i=1}^m e_i = B$.\n\nWe can express the entire set of observations $y = (y_1, \\dots, y_m)^\\top$ in a single vector equation:\n$$\ny = H\\theta + \\varepsilon\n$$\nwhere $H$ is the $m \\times d$ design matrix whose rows are $h_i^\\top$, and $\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_m)^\\top$ is the vector of noise terms. Due to the independence of the $\\varepsilon_i$, the covariance matrix of $\\varepsilon$ is diagonal:\n$$\n\\Sigma_\\varepsilon(e) = \\text{Cov}(\\varepsilon) = \\text{diag}\\left(\\frac{\\sigma_1^2}{e_1}, \\frac{\\sigma_2^2}{e_2}, \\dots, \\frac{\\sigma_m^2}{e_m}\\right)\n$$\nThe likelihood of the data $y$ given the state $\\theta$ is therefore $y|\\theta \\sim \\mathcal{N}(H\\theta, \\Sigma_\\varepsilon(e))$.\n\nWe are interested in the posterior distribution of $\\theta$ after incorporating the information from the observations. According to Bayes' theorem for linear-Gaussian models, the posterior distribution $p(\\theta|y)$ is also Gaussian, $\\theta|y \\sim \\mathcal{N}(\\mu_e, \\Sigma_e)$. The posterior covariance matrix $\\Sigma_e$ is given by the inverse of the posterior precision matrix $\\Lambda_e$, which is the sum of the prior precision $\\Lambda_0 = \\Sigma_0^{-1}$ and the precision gained from the data, which is represented by the Fisher information matrix of the likelihood, $H^\\top\\Sigma_\\varepsilon(e)^{-1}H$.\n$$\n\\Lambda_e = \\Sigma_e^{-1} = \\Sigma_0^{-1} + H^\\top\\Sigma_\\varepsilon(e)^{-1}H\n$$\nThe inverse of the diagonal noise covariance matrix is:\n$$\n\\Sigma_\\varepsilon(e)^{-1} = \\text{diag}\\left(\\frac{e_1}{\\sigma_1^2}, \\frac{e_2}{\\sigma_2^2}, \\dots, \\frac{e_m}{\\sigma_m^2}\\right)\n$$\nSubstituting this into the expression for the data precision gives:\n$$\nH^\\top\\Sigma_\\varepsilon(e)^{-1}H = \\sum_{i=1}^m h_i \\left(\\frac{e_i}{\\sigma_i^2}\\right) h_i^\\top = \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top\n$$\nHere, $h_i h_i^\\top$ is a $d \\times d$ rank-one matrix (an outer product). Thus, the posterior covariance of $\\theta$ is:\n$$\n\\Sigma_e = \\left( \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top \\right)^{-1}\n$$\nA critical feature of this model is that the posterior covariance $\\Sigma_e$ depends on the experimental design (the effort allocation $e$) but not on the specific data values $y$ that will be observed. Therefore, the \"expected posterior predictive variance\" is simply the posterior variance calculated with this design-dependent covariance matrix.\n\nThe objective is to minimize the posterior predictive variance of the linear functional $\\psi = g^\\top\\theta$. For a given posterior distribution of $\\theta$, the variance of $\\psi$ is:\n$$\n\\text{Var}(\\psi|\\text{data}) = g^\\top \\text{Var}(\\theta|\\text{data}) g = g^\\top \\Sigma_e g\n$$\nThis gives us the objective function $V(e)$ to be minimized:\n$$\nV(e) = g^\\top \\left( \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top \\right)^{-1} g\n$$\nThe optimization problem is to find the effort vector $e = (e_1, \\dots, e_m)^\\top$ that solves:\n$$\n\\min_{e \\in \\mathbb{R}^m} V(e) \\quad \\text{subject to} \\quad \\sum_{i=1}^m e_i = B \\quad \\text{and} \\quad e_i \\ge 0 \\text{ for all } i.\n$$\nThis is a convex optimization problem. The objective function $V(e)$ is a composition of the matrix convex function $X \\mapsto g^\\top X^{-1} g$ (on the cone of positive definite matrices) with an affine mapping from $e$ to the posterior precision matrix. The constraint set, a simplex, is also convex. The convexity ensures that any local minimum found by a suitable algorithm is also a global minimum.\n\nTo employ a gradient-based optimization algorithm, we must compute the gradient of $V(e)$. Let $M(e) = \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top = \\Sigma_e^{-1}$. The partial derivative of $V(e)$ with respect to $e_k$ is found using the chain rule and the identity for the derivative of a matrix inverse, $\\frac{d}{dt}A^{-1} = -A^{-1} \\frac{dA}{dt} A^{-1}$:\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = g^\\top \\frac{\\partial M(e)^{-1}}{\\partial e_k} g = g^\\top \\left( -M(e)^{-1} \\frac{\\partial M(e)}{\\partial e_k} M(e)^{-1} \\right) g\n$$\nThe derivative of $M(e)$ with respect to $e_k$ is simply the coefficient of $e_k$:\n$$\n\\frac{\\partial M(e)}{\\partial e_k} = \\frac{1}{\\sigma_k^2} h_k h_k^\\top\n$$\nSubstituting this back, and recalling that $\\Sigma_e = M(e)^{-1}$, we have:\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = - g^\\top \\Sigma_e \\left( \\frac{1}{\\sigma_k^2} h_k h_k^\\top \\right) \\Sigma_e g\n$$\nSince $g^\\top \\Sigma_e h_k$ is a scalar, we can rearrange the expression:\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = - \\frac{1}{\\sigma_k^2} (g^\\top \\Sigma_e h_k) (h_k^\\top \\Sigma_e g) = - \\frac{1}{\\sigma_k^2} (h_k^\\top \\Sigma_e g)^2\n$$\nThe gradient vector is $\\nabla_e V(e) = \\left(\\frac{\\partial V(e)}{\\partial e_1}, \\dots, \\frac{\\partial V(e)}{\\partial e_m}\\right)^\\top$.\n\nThe numerical solution will be obtained using the Sequential Least Squares Programming (SLSQP) algorithm, a gradient-based method suitable for constrained nonlinear optimization. This method is available in the `scipy.optimize.minimize` library function. We will supply the objective function $V(e)$, its analytical Jacobian (gradient) $\\nabla_e V(e)$, the equality constraint $\\sum_i e_i - B = 0$, and non-negativity bounds $e_i \\ge 0$. A uniform allocation, $e_i = B/m$, serves as a reasonable initial guess for the optimizer.\n\nFor the special case where the budget $B=0$, no sampling is possible, so $e_i=0$ for all $i$. The posterior distribution is identical to the prior, and the variance is simply the prior variance of $\\psi$, which is $g^\\top\\Sigma_0 g$.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the ecological monitoring effort allocation problem for the given test cases.\n    \"\"\"\n    \n    # Define test cases\n    test_cases = [\n        # Case 1\n        {\n            \"d\": 3, \"m\": 3,\n            \"Sigma0\": np.array([[1.0, 0.2, 0.1], [0.2, 0.5, 0.1], [0.1, 0.1, 1.5]]),\n            \"H\": np.array([[1.0, 0.2, 0.0], [0.3, 1.0, 0.5], [0.0, 0.2, 1.0]]),\n            \"sigma2\": np.array([2.0, 1.5, 3.0]),\n            \"g\": np.array([0.5, 0.3, 0.2]),\n            \"B\": 4.0\n        },\n        # Case 2\n        {\n            \"d\": 3, \"m\": 3,\n            \"Sigma0\": np.array([[1.0, 0.2, 0.1], [0.2, 0.5, 0.1], [0.1, 0.1, 1.5]]),\n            \"H\": np.array([[1.0, 0.2, 0.0], [0.3, 1.0, 0.5], [0.0, 0.2, 1.0]]),\n            \"sigma2\": np.array([2.0, 1.5, 3.0]),\n            \"g\": np.array([0.5, 0.3, 0.2]),\n            \"B\": 0.0\n        },\n        # Case 3\n        {\n            \"d\": 2, \"m\": 4,\n            \"Sigma0\": np.array([[1.0, 0.3], [0.3, 1.2]]),\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0], [0.7, 0.7], [1.0, -0.2]]),\n            \"sigma2\": np.array([0.6, 2.0, 1.5, 0.8]),\n            \"g\": np.array([1.0, 0.5]),\n            \"B\": 3.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        e_star, v_star = solve_one_case(**case)\n        formatted_e = [f\"{val:.4f}\" for val in e_star]\n        formatted_v = f\"{v_star:.6f}\"\n        results.append(f'[{\",\".join(map(str, formatted_e))},{formatted_v}]')\n\n    print(f'[{\",\".join(results)}]')\n\ndef solve_one_case(d, m, Sigma0, H, sigma2, g, B):\n    \"\"\"\n    Solves a single instance of the effort allocation problem.\n    \"\"\"\n    # Handle the trivial case of zero budget\n    if B == 0.0:\n        e_star = np.zeros(m)\n        v_star = g.T @ Sigma0 @ g\n        return e_star, v_star\n\n    # Pre-compute the inverse of the prior covariance\n    Sigma0_inv = np.linalg.inv(Sigma0)\n\n    def objective(e, g_vec, S0_inv, H_mat, s2_vec):\n        \"\"\" The objective function V(e) to be minimized. \"\"\"\n        # Compute information gain from observations: sum(e_i/sigma_i^2 * h_i h_i^T)\n        info_gain = np.einsum('i,ij,ik->jk', e / s2_vec, H_mat, H_mat)\n        \n        # Posterior precision matrix M(e)\n        M = S0_inv + info_gain\n        \n        try:\n            # Posterior covariance matrix Sigma_e = M(e)^-1\n            Sigma_e = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # Return a large number if matrix is singular\n            return np.inf\n\n        # Posterior variance of psi = g^T theta\n        variance = g_vec.T @ Sigma_e @ g_vec\n        return variance\n\n    def jacobian(e, g_vec, S0_inv, H_mat, s2_vec):\n        \"\"\" The Jacobian (gradient) of the objective function. \"\"\"\n        # Compute information gain and posterior covariance as in the objective\n        info_gain = np.einsum('i,ij,ik->jk', e / s2_vec, H_mat, H_mat)\n        M = S0_inv + info_gain\n        \n        try:\n            Sigma_e = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            return np.full_like(e, np.inf)\n\n        # Gradient component: grad_k = - (h_k^T Sigma_e g)^2 / sigma_k^2\n        # Vectorized computation for all k:\n        term = H_mat @ Sigma_e @ g_vec\n        grad = - (term**2) / s2_vec\n        return grad\n\n    # Initial guess: uniform allocation\n    e_init = np.full(m, B / m)\n\n    # Constraints: sum(e_i) = B\n    constraints = ({'type': 'eq', 'fun': lambda e: np.sum(e) - B})\n\n    # Bounds: e_i >= 0\n    bounds = [(0, None) for _ in range(m)]\n\n    # Perform the optimization\n    res = minimize(\n        objective,\n        e_init,\n        args=(g, Sigma0_inv, H, sigma2),\n        method='SLSQP',\n        jac=jacobian,\n        constraints=constraints,\n        bounds=bounds,\n        tol=1e-12,\n        options={'maxiter': 500}\n    )\n\n    e_star = res.x\n    # clean up small negative values from numerical precision issues\n    e_star[e_star < 1e-9] = 0.0\n\n    v_star = res.fun\n    \n    return e_star, v_star\n\nsolve()\n```", "id": "2482812"}]}