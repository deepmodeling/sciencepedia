{"hands_on_practices": [{"introduction": "This first practice takes you to the heart of mark-recapture analysis, starting with the foundational Lincoln-Petersen estimator. You will not only compute this classic estimate of population size, $N$, but also explore a crucial bias-corrected alternative, the Chapman estimator. By deriving and calculating their standard errors and confidence intervals, you will gain hands-on experience with the delta method, a powerful tool for understanding the precision of your ecological estimates [@problem_id:2523184].", "problem": "A closed two-sample capture–recapture study of a single population is conducted under the following standard assumptions: the population is demographically and geographically closed between sampling occasions; marks are permanent, correctly recorded, and do not affect capture probability; and all individuals share a common capture probability on each occasion. On the first occasion, $M=100$ individuals are captured, marked, and released. On the second occasion, $n=120$ individuals are captured, of which $m=15$ are found to be marked from the first occasion.\n\nUnder the independence and closure assumptions, the number of marked individuals observed on the second occasion, $m$, has a hypergeometric distribution determined by the true abundance $N$, with mean $\\mathbb{E}[m]=n\\,M/N$ and variance $\\mathrm{Var}(m)=n\\,(M/N)\\,\\bigl(1-M/N\\bigr)\\,\\bigl((N-n)/(N-1)\\bigr)$.\n\nStarting from these foundational facts and the large-sample delta method for smooth transformations, do the following:\n\n1. Derive the classical method-of-moments Lincoln–Petersen abundance estimator $\\hat N_{\\mathrm{LP}}$ by equating $\\mathbb{E}[m]$ to its observed value, and compute its value for the observed data.\n2. Adopt the standard first-order bias-reduced alternative, the Chapman estimator $\\hat N_{\\mathrm{Ch}}$, and compute its value for the observed data.\n3. Using the delta method with the hypergeometric variance of $m$ and a plug-in for the unknown $N$ appropriate to each estimator, derive and compute approximate standard errors for $\\hat N_{\\mathrm{LP}}$ and $\\hat N_{\\mathrm{Ch}}$.\n4. Construct approximate $95\\%$ confidence intervals for each estimator using the normal approximation.\n\nReport your final numerical results in the following order as eight numbers: $\\hat N_{\\mathrm{LP}}$, the standard error of $\\hat N_{\\mathrm{LP}}$, the lower and upper endpoints of the $95\\%$ confidence interval for $\\hat N_{\\mathrm{LP}}$, then $\\hat N_{\\mathrm{Ch}}$, the standard error of $\\hat N_{\\mathrm{Ch}}$, and the lower and upper endpoints of the $95\\%$ confidence interval for $\\hat N_{\\mathrm{Ch}}$. Round all reported numerical quantities to three significant figures. Do not include any units in your final reported values.", "solution": "The problem presented is a standard exercise in population abundance estimation using mark-recapture data. We first validate the problem statement. The givens are: number of individuals marked in the first sample, $M=100$; size of the second sample, $n=120$; number of marked individuals recaptured in the second sample, $m=15$. The problem states the standard assumptions for a closed-population model and correctly identifies the distribution of $m$ as hypergeometric, providing the correct formulae for its mean and variance. The tasks are mathematically and statistically well-defined. The problem is therefore scientifically grounded, well-posed, and objective. It is valid for solution.\n\nWe will address the four specified tasks in sequence. All numerical results will be rounded to three significant figures as required.\n\n1. The Lincoln–Petersen Estimator ($\\hat{N}_{\\mathrm{LP}}$)\n\nThe method of moments equates the theoretical expectation of a statistic to its observed value. Here, the statistic is the number of recaptures, $m$. The expectation is given as $\\mathbb{E}[m] = \\frac{nM}{N}$. We set this equal to the observed value of $m$:\n$$m = \\frac{nM}{N}$$\nSolving for the unknown population size $N$ yields the Lincoln–Petersen estimator, $\\hat{N}_{\\mathrm{LP}}$:\n$$\\hat{N}_{\\mathrm{LP}} = \\frac{nM}{m}$$\nSubstituting the given data: $M=100$, $n=120$, and $m=15$:\n$$\\hat{N}_{\\mathrm{LP}} = \\frac{120 \\times 100}{15} = \\frac{12000}{15} = 800$$\nTo three significant figures, this is $8.00 \\times 10^{2}$.\n\n2. The Chapman Estimator ($\\hat{N}_{\\mathrm{Ch}}$)\n\nThe Chapman estimator is a bias-corrected modification of the Lincoln–Petersen estimator. Its formula, which we adopt as instructed, is:\n$$\\hat{N}_{\\mathrm{Ch}} = \\frac{(M+1)(n+1)}{m+1} - 1$$\nSubstituting the given data:\n$$\\hat{N}_{\\mathrm{Ch}} = \\frac{(100+1)(120+1)}{15+1} - 1 = \\frac{101 \\times 121}{16} - 1 = \\frac{12221}{16} - 1 = 763.8125 - 1 = 762.8125$$\nRounding to three significant figures, we get $\\hat{N}_{\\mathrm{Ch}} = 763$.\n\n3. Approximate Standard Errors\n\nWe derive the standard errors using the delta method. For an estimator $\\hat{N}$ that is a function of the random variable $m$, say $\\hat{N} = g(m)$, the approximate variance is given by:\n$$\\mathrm{Var}(\\hat{N}) \\approx \\left( \\frac{dg}{dm} \\right)^2 \\mathrm{Var}(m)$$\nThe problem provides the hypergeometric variance of $m$:\n$$\\mathrm{Var}(m) = n \\left(\\frac{M}{N}\\right) \\left(1-\\frac{M}{N}\\right) \\left(\\frac{N-n}{N-1}\\right)$$\nWe will evaluate this expression by plugging in the appropriate estimate $\\hat{N}$ for the unknown true value $N$. The standard error is the square root of this estimated variance.\n\nStandard Error of $\\hat{N}_{\\mathrm{LP}}$:\nThe estimator is $\\hat{N}_{\\mathrm{LP}} = g(m) = \\frac{nM}{m}$. The derivative is:\n$$\\frac{dg}{dm} = -\\frac{nM}{m^2}$$\nThe estimated variance, $\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}})$, is therefore:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\left(-\\frac{nM}{m^2}\\right)^2 \\widehat{\\mathrm{Var}}(m) = \\left(\\frac{\\hat{N}_{\\mathrm{LP}}}{m}\\right)^2 \\widehat{\\mathrm{Var}}(m)$$\nWe estimate $\\mathrm{Var}(m)$ by substituting $N = \\hat{N}_{\\mathrm{LP}} = 800$.\n$$\\widehat{\\mathrm{Var}}(m) = n \\left(\\frac{M}{\\hat{N}_{\\mathrm{LP}}}\\right) \\left(1-\\frac{M}{\\hat{N}_{\\mathrm{LP}}}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right)$$\nUsing the relation $\\frac{M}{\\hat{N}_{\\mathrm{LP}}} = \\frac{m}{n}$, this simplifies the expression for $\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}})$:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\left(\\frac{\\hat{N}_{\\mathrm{LP}}}{m}\\right)^2 \\left[ n \\left(\\frac{m}{n}\\right) \\left(1-\\frac{m}{n}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right) \\right] = \\frac{\\hat{N}_{\\mathrm{LP}}^2 (n-m)}{mn} \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right)$$\nSubstituting the numerical values:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\frac{(800)^2 (120-15)}{(15)(120)} \\left(\\frac{800-120}{800-1}\\right) = \\frac{640000 \\times 105}{1800} \\left(\\frac{680}{799}\\right) \\approx 31771.5$$\nThe standard error is the square root:\n$$\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = \\sqrt{31771.5} \\approx 178.2456$$\nTo three significant figures, $\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = 178$.\n\nStandard Error of $\\hat{N}_{\\mathrm{Ch}}$:\nThe estimator is $\\hat{N}_{\\mathrm{Ch}} = g(m) = \\frac{(M+1)(n+1)}{m+1} - 1$. The derivative is:\n$$\\frac{dg}{dm} = -\\frac{(M+1)(n+1)}{(m+1)^2}$$\nFrom the definition of $\\hat{N}_{\\mathrm{Ch}}$, we have $\\hat{N}_{\\mathrm{Ch}} + 1 = \\frac{(M+1)(n+1)}{m+1}$, so $\\frac{dg}{dm} = -\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}$.\nThe estimated variance is:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(-\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}\\right)^2 \\widehat{\\mathrm{Var}}(m)$$\nwhere we now plug $N = \\hat{N}_{\\mathrm{Ch}} = 762.8125$ into the variance formula for $m$:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}\\right)^2 \\left[ n \\left(\\frac{M}{\\hat{N}_{\\mathrm{Ch}}}\\right) \\left(1-\\frac{M}{\\hat{N}_{\\mathrm{Ch}}}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{Ch}}-n}{\\hat{N}_{\\mathrm{Ch}}-1}\\right) \\right]$$\nSubstituting numerical values:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(\\frac{763.8125}{16}\\right)^2 \\left[ 120 \\left(\\frac{100}{762.8125}\\right) \\left(1-\\frac{100}{762.8125}\\right) \\left(\\frac{762.8125-120}{762.8125-1}\\right) \\right]$$\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx (47.738...)^2 \\left[ 120 (0.13109...) (0.86890...) (0.84379...) \\right] \\approx (2278.94) (11.5286) \\approx 26274.5$$\nThe standard error is the square root:\n$$\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = \\sqrt{26274.5} \\approx 162.094$$\nTo three significant figures, $\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = 162$.\n\n4. Approximate $95\\%$ Confidence Intervals\n\nWe use the normal approximation to construct the confidence intervals. The general formula is $\\hat{N} \\pm z_{\\alpha/2} \\mathrm{SE}(\\hat{N})$. For a $95\\%$ confidence level, $\\alpha=0.05$, and the critical value is $z_{0.025} = 1.96$.\n\nConfidence Interval for $\\hat{N}_{\\mathrm{LP}}$:\n$$\\mathrm{CI}_{\\mathrm{LP}} = 800 \\pm 1.96 \\times 178.2456 = 800 \\pm 349.36$$\nLower bound: $800 - 349.36 = 450.64$. Rounded to three significant figures, this is $451$.\nUpper bound: $800 + 349.36 = 1149.36$. Rounded to three significant figures, this is $1.15 \\times 10^3$.\n\nConfidence Interval for $\\hat{N}_{\\mathrm{Ch}}$:\n$$\\mathrm{CI}_{\\mathrm{Ch}} = 762.8125 \\pm 1.96 \\times 162.094 = 762.8125 \\pm 317.704$$\nLower bound: $762.8125 - 317.704 = 445.1085$. Rounded to three significant figures, this is $445$.\nUpper bound: $762.8125 + 317.704 = 1080.5165$. Rounded to three significant figures, this is $1.08 \\times 10^3$.\n\nThe eight required numerical quantities are, in order and rounded to three significant figures:\n1. $\\hat{N}_{\\mathrm{LP}} = 8.00 \\times 10^2$\n2. $\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = 178$\n3. Lower CI for $\\hat{N}_{\\mathrm{LP}} = 451$\n4. Upper CI for $\\hat{N}_{\\mathrm{LP}} = 1.15 \\times 10^3$\n5. $\\hat{N}_{\\mathrm{Ch}} = 763$\n6. $\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = 162$\n7. Lower CI for $\\hat{N}_{\\mathrm{Ch}} = 445$\n8. Upper CI for $\\hat{N}_{\\mathrm{Ch}} = 1.08 \\times 10^3$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n8.00 \\times 10^{2} & 178 & 451 & 1.15 \\times 10^{3} & 763 & 162 & 445 & 1.08 \\times 10^{3}\n\\end{pmatrix}\n}\n$$", "id": "2523184"}, {"introduction": "Real-world data rarely conforms perfectly to a single set of assumptions. This exercise introduces you to the modern, robust approach of multi-model inference, where several competing hypotheses about capture probability are evaluated simultaneously. You will use the Akaike Information Criterion (AIC) to weigh evidence for different models and compute a model-averaged abundance estimate, learning to quantify the critical but often-overlooked component of model selection uncertainty [@problem_id:2523129].", "problem": "A closed population of stream salamanders was sampled using $T=5$ capture occasions within a short period where demographic closure is reasonable. The study recorded $n_{\\text{obs}}=73$ unique individuals. Three closed-capture models are considered: model $M_{0}$ (constant capture probability), model $M_{t}$ (time-varying capture probabilities), and model $M_{h}$ (individual heterogeneity in capture probabilities). For each model, a Maximum Likelihood (ML) analysis produced the following summaries, where $K$ is the total number of free parameters estimated by the model (including population size $N$), $\\ell$ is the maximized log-likelihood, $\\hat{N}_{i}$ is the model-specific point estimate for $N$, and $\\widehat{\\operatorname{Var}}_{i}$ is the model-based variance of $\\hat{N}_{i}$ under model $i \\in \\{0,t,h\\}$:\n\n- $M_{0}$: $K_{0}=2$, $\\ell_{0}=-210.3$, $\\hat{N}_{0}=92.4$, $\\widehat{\\operatorname{Var}}_{0}=56.7$.\n- $M_{t}$: $K_{t}=6$, $\\ell_{t}=-205.9$, $\\hat{N}_{t}=95.1$, $\\widehat{\\operatorname{Var}}_{t}=72.4$.\n- $M_{h}$: $K_{h}=4$, $\\ell_{h}=-205.1$, $\\hat{N}_{h}=98.7$, $\\widehat{\\operatorname{Var}}_{h}=64.5$.\n\nStarting from the definitions of the Akaike Information Criterion (AIC), its small-sample correction (AICc) using effective sample size $n_{\\text{obs}}$, and the law of total expectation and the law of total variance, do the following:\n\n1. Compute the small-sample corrected Akaike values (AICc) for $M_{0}$, $M_{t}$, and $M_{h}$, then derive the normalized Akaike weights that place a probability mass function over the model set $\\{M_{0},M_{t},M_{h}\\}$.\n2. Using the interpretation of these weights as an empirical model distribution under equal model priors, derive from first principles a model-averaged estimator for $N$ and compute its value.\n3. Using the law of total variance applied to the model-averaged estimator, derive an unconditional variance expression that accounts for both within-model estimation uncertainty and between-model selection uncertainty, and compute its numerical value.\n\nReport the unconditional variance from part 3 as a pure number. Round your answer to four significant figures.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extracted Givens**\n- Number of capture occasions: $T=5$.\n- Number of observed unique individuals (effective sample size): $n_{\\text{obs}}=73$.\n- Model set evaluated: $\\{M_{0}, M_{t}, M_{h}\\}$.\n- Model $M_{0}$ (constant capture probability):\n  - Number of parameters: $K_{0}=2$.\n  - Maximized log-likelihood: $\\ell_{0}=-210.3$.\n  - Population size estimate: $\\hat{N}_{0}=92.4$.\n  - Estimated variance of $\\hat{N}_{0}$: $\\widehat{\\operatorname{Var}}_{0}=56.7$.\n- Model $M_{t}$ (time-varying capture probability):\n  - Number of parameters: $K_{t}=6$.\n  - Maximized log-likelihood: $\\ell_{t}=-205.9$.\n  - Population size estimate: $\\hat{N}_{t}=95.1$.\n  - Estimated variance of $\\hat{N}_{t}$: $\\widehat{\\operatorname{Var}}_{t}=72.4$.\n- Model $M_{h}$ (individual heterogeneity in capture probability):\n  - Number of parameters: $K_{h}=4$.\n  - Maximized log-likelihood: $\\ell_{h}=-205.1$.\n  - Population size estimate: $\\hat{N}_{h}=98.7$.\n  - Estimated variance of $\\hat{N}_{h}$: $\\widehat{\\operatorname{Var}}_{h}=64.5$.\n\n**Step 2: Validation**\nThe problem is scientifically grounded, rooted in the established ecological and statistical practice of mark-recapture analysis and information-theoretic model selection. The models presented ($M_{0}$, $M_{t}$, $M_{h}$) constitute a standard set for closed-population studies. All necessary data are provided, the terminology is precise, and the objectives are quantifiable. The problem contains no scientific fallacies, contradictions, or ambiguities. It is a well-posed problem in quantitative ecology.\n\n**Step 3: Verdict**\nThe problem is valid. A solution will be formulated.\n\nThe solution proceeds in three parts as specified.\n\n**Part 1: AICc and Akaike Weights**\n\nThe small-sample corrected Akaike Information Criterion (AICc) is defined for a model with $K$ parameters and maximized log-likelihood $\\ell$, given a sample size of $n$, as:\n$$\n\\text{AICc} = -2\\ell + 2K + \\frac{2K(K+1)}{n-K-1}\n$$\nHere, the effective sample size is given as $n = n_{\\text{obs}} = 73$. We compute the AICc for each of the three models, $M_{i}$ where $i \\in \\{0, t, h\\}$.\n\nFor model $M_{0}$:\n$$\n\\text{AICc}_{0} = -2(-210.3) + 2(2) + \\frac{2(2)(2+1)}{73-2-1} = 420.6 + 4 + \\frac{12}{70} \\approx 424.7714\n$$\nFor model $M_{t}$:\n$$\n\\text{AICc}_{t} = -2(-205.9) + 2(6) + \\frac{2(6)(6+1)}{73-6-1} = 411.8 + 12 + \\frac{84}{66} \\approx 425.0727\n$$\nFor model $M_{h}$:\n$$\n\\text{AICc}_{h} = -2(-205.1) + 2(4) + \\frac{2(4)(4+1)}{73-4-1} = 410.2 + 8 + \\frac{40}{68} \\approx 418.7882\n$$\nThe model with the minimum AICc value is the best-supported model. We find $\\text{AICc}_{\\min} = \\text{AICc}_{h} \\approx 418.7882$.\n\nNext, we compute the delta AICc values, $\\Delta_i = \\text{AICc}_i - \\text{AICc}_{\\min}$:\n$$\n\\Delta_{0} = 424.7714 - 418.7882 = 5.9832\n$$\n$$\n\\Delta_{t} = 425.0727 - 418.7882 = 6.2845\n$$\n$$\n\\Delta_{h} = 418.7882 - 418.7882 = 0\n$$\nThe Akaike weight for model $i$, $w_i$, which represents the probability that model $i$ is the best model in the set, is given by:\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\Delta_i)}{\\sum_{j \\in \\{0,t,h\\}} \\exp(-\\frac{1}{2}\\Delta_j)}\n$$\nThe denominator is:\n$$\n\\sum_{j} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-\\frac{5.9832}{2}) + \\exp(-\\frac{6.2845}{2}) + \\exp(-\\frac{0}{2}) \\approx 0.05021 + 0.04319 + 1 = 1.0934\n$$\nThe individual weights are:\n$$\nw_{0} = \\frac{0.05021}{1.0934} \\approx 0.04592\n$$\n$$\nw_{t} = \\frac{0.04319}{1.0934} \\approx 0.03950\n$$\n$$\nw_{h} = \\frac{1}{1.0934} \\approx 0.91458\n$$\n\n**Part 2: Model-Averaged Estimator for N**\n\nWe are tasked to derive the model-averaged estimator for $N$ from first principles using the law of total expectation. Let $\\hat{N}_a$ be the model-averaged estimator for the true population size $N$. Let $M$ be a discrete random variable representing the model choice from the set $\\{M_0, M_t, M_h\\}$, with probability mass function $P(M=M_i) = w_i$. The law of total expectation states that the expected value of a random variable is the expected value of its conditional expectation. Applied to our estimator $\\hat{N}_a$:\n$$\n\\hat{N}_a = E[\\hat{N}] = E_M[E[\\hat{N} | M]]\n$$\nThe inner expectation, $E[\\hat{N} | M=M_i]$, is the point estimate of $N$ given that model $M_i$ is true, which is simply $\\hat{N}_i$. The outer expectation is taken over the distribution of models, weighted by the Akaike weights $w_i$. This expands to:\n$$\n\\hat{N}_a = \\sum_{i \\in \\{0,t,h\\}} E[\\hat{N} | M=M_i] P(M=M_i) = \\sum_{i \\in \\{0,t,h\\}} \\hat{N}_i w_i\n$$\nUsing the calculated weights and provided estimates:\n$$\n\\hat{N}_a = (92.4)(0.04592) + (95.1)(0.03950) + (98.7)(0.91458)\n$$\n$$\n\\hat{N}_a \\approx 4.243 + 3.756 + 90.269 = 98.268\n$$\nUsing higher precision values for the weights: $w_0 \\approx 0.04592186$, $w_t \\approx 0.03950005$, $w_h \\approx 0.91457809$.\n$$\n\\hat{N}_a = (92.4)(0.04592186) + (95.1)(0.03950005) + (98.7)(0.91457809) \\approx 98.26849\n$$\n\n**Part 3: Unconditional Variance**\n\nWe must derive the unconditional variance of $\\hat{N}_a$ using the law of total variance. This law states that for random variables $X$ and $Y$, $\\operatorname{Var}(X) = E[\\operatorname{Var}(X|Y)] + \\operatorname{Var}(E[X|Y])$. Applying this to our estimator $\\hat{N}_a$ with conditioning on the model choice $M$:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = E_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)] + \\operatorname{Var}_M(E[\\hat{N} | M])\n$$\nThe first term, $E_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)]$, is the expected within-model variance. The conditional variance, $\\widehat{\\operatorname{Var}}(\\hat{N} | M=M_i)$, is the variance of the estimator $\\hat{N}_i$ computed under model $M_i$, which is the given $\\widehat{\\operatorname{Var}}_i$. The expectation is then the weighted average over all models:\n$$\nE_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)] = \\sum_{i} \\widehat{\\operatorname{Var}}_i w_i\n$$\nThe second term, $\\operatorname{Var}_M(E[\\hat{N} | M])$, is the variance of the conditional expectation, which represents the uncertainty due to model selection. The conditional expectation $E[\\hat{N} | M=M_i]$ is the point estimate $\\hat{N}_i$. We need the variance of a discrete random variable that takes values $\\hat{N}_i$ with probabilities $w_i$. The mean of this variable is $\\hat{N}_a$. The variance is thus:\n$$\n\\operatorname{Var}_M(E[\\hat{N} | M]) = \\sum_{i} (\\hat{N}_i - E[\\hat{N}])^2 w_i = \\sum_{i} (\\hat{N}_i - \\hat{N}_a)^2 w_i\n$$\nCombining both terms yields the estimator for the unconditional variance as proposed by Buckland, Burnham, and Augustin (1997):\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = \\sum_{i} w_i \\widehat{\\operatorname{Var}}_i + \\sum_{i} w_i (\\hat{N}_i - \\hat{N}_a)^2 = \\sum_{i} w_i \\left( \\widehat{\\operatorname{Var}}_i + (\\hat{N}_i - \\hat{N}_a)^2 \\right)\n$$\nWe now compute this value. First, the component terms $(\\hat{N}_i - \\hat{N}_a)^2$:\n- $(\\hat{N}_0 - \\hat{N}_a)^2 = (92.4 - 98.26849)^2 \\approx (-5.86849)^2 \\approx 34.43927$\n- $(\\hat{N}_t - \\hat{N}_a)^2 = (95.1 - 98.26849)^2 \\approx (-3.16849)^2 \\approx 10.03943$\n- $(\\hat{N}_h - \\hat{N}_a)^2 = (98.7 - 98.26849)^2 \\approx (0.43151)^2 \\approx 0.18620$\n\nThe first component of variance (within-model):\n$$\n\\sum_{i} w_i \\widehat{\\operatorname{Var}}_i = (0.04592186)(56.7) + (0.03950005)(72.4) + (0.91457809)(64.5) \\approx 2.60477 + 2.85980 + 58.99029 \\approx 64.45486\n$$\nThe second component of variance (between-model):\n$$\n\\sum_{i} w_i (\\hat{N}_i - \\hat{N}_a)^2 = (0.04592186)(34.43927) + (0.03950005)(10.03943) + (0.91457809)(0.18620) \\approx 1.58148 + 0.39656 + 0.17024 \\approx 2.14828\n$$\nThe total unconditional variance is the sum of these two components:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = 64.45486 + 2.14828 = 66.60314\n$$\nRounding to four significant figures, the result is $66.60$.", "answer": "$$\n\\boxed{66.60}\n$$", "id": "2523129"}, {"introduction": "We now advance to the cutting edge of population estimation with Bayesian spatial capture-recapture (SCR), a framework that explicitly uses the locations of traps and animal detections. This practice will guide you through the initial steps of building a custom MCMC algorithm for a state-of-the-art SCR model, a vital skill for modern quantitative ecologists. You will derive the full conditional distributions for key parameters, giving you a look 'under the hood' of Bayesian statistical software and the powerful technique of data augmentation [@problem_id:2523149].", "problem": "You are to formulate a fully specified Bayesian spatial capture–recapture (SCR) model with data augmentation and implement programmatic calculations of the full conditional components required for a Gibbs or Metropolis-within-Gibbs sampler. The aim is to demonstrate how to compute, from first principles, the components of a Markov Chain Monte Carlo (MCMC) algorithm for a Poisson encounter model under a half-normal detection function with augmented individuals and latent activity centers.\n\nUse the following fundamental base:\n- Spatial capture–recapture (SCR) is an extension of capture–recapture that models detections as a function of the distance between an individual's latent activity center and trap locations.\n- Each augmented individual $i \\in \\{1,\\dots,M\\}$ has a binary inclusion indicator $z_i \\in \\{0,1\\}$, where $z_i = 1$ indicates presence in the population and $z_i=0$ indicates a pseudo-individual introduced for data augmentation.\n- The latent activity center of individual $i$ is $\\mathbf{s}_i \\in \\mathcal{S} \\subset \\mathbb{R}^2$ with prior $\\mathbf{s}_i \\sim \\text{Uniform}(\\mathcal{S})$.\n- Each trap $j \\in \\{1,\\dots,J\\}$ is located at $\\mathbf{x}_j \\in \\mathbb{R}^2$ and has known effort $K_j$ (e.g., number of sampling occasions at trap $j$).\n- The detection function is half-normal, $g(d;\\sigma) = \\exp\\!\\left(-\\frac{d^2}{2\\sigma^2}\\right)$, where $d$ is Euclidean distance and $\\sigma$ is the spatial scale.\n- The observation model is Poisson: the count $y_{ij}$ at trap $j$ for individual $i$ is $y_{ij} \\sim \\text{Poisson}(\\mu_{ij})$ with mean $\\mu_{ij} = z_i \\, K_j \\, \\lambda_0 \\, g\\!\\left(\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert;\\sigma\\right)$.\n- The prior for the inclusion probability is $\\psi \\sim \\text{Beta}(a_\\psi,b_\\psi)$, and $z_i \\mid \\psi \\sim \\text{Bernoulli}(\\psi)$ independently.\n- The prior for the baseline encounter rate $\\lambda_0$ uses the Gamma distribution with shape–rate parameterization, $\\lambda_0 \\sim \\text{Gamma}(a_\\lambda,b_\\lambda)$, meaning density proportional to $\\lambda_0^{a_\\lambda-1}\\exp(-b_\\lambda \\lambda_0)$ for $\\lambda_0>0$.\n- The prior for the spatial scale is uniform on a bounded interval, $\\sigma \\sim \\text{Uniform}(0,s_{\\max})$.\n\nFrom these definitions, derive the full conditional distributions or update kernels required for a Gibbs or Metropolis-within-Gibbs sampler. Then, implement computations that evaluate:\n- The posterior mean of $\\lambda_0$ under its full conditional (which is Gamma) given current $\\sigma$, $\\mathbf{s}_{1:M}$, $z_{1:M}$, and data $y_{ij}$.\n- The posterior mean of $\\psi$ under its full conditional (which is Beta) given $z_{1:M}$.\n- The posterior Bernoulli probability $\\Pr(z_i=1 \\mid \\cdot)$ for an individual $i$ with an all-zero capture history, given current $(\\psi,\\lambda_0,\\sigma,\\mathbf{s}_i)$, traps and efforts.\n- The log Metropolis–Hastings acceptance ratio for a proposed update $\\sigma' \\leftarrow \\sigma$ (random-walk proposal details are not needed; compute only the log posterior difference between $\\sigma'$ and $\\sigma$). Use the likelihood contribution from all $i$ with $z_i=1$ and the uniform prior on $\\sigma$.\n- The log Metropolis–Hastings acceptance ratio for a proposed update $\\mathbf{s}_i' \\leftarrow \\mathbf{s}_i$ for a specified $i$ (compute only the log posterior difference between $\\mathbf{s}_i'$ and $\\mathbf{s}_i$). Use only the likelihood for that $i$ and the uniform prior on $\\mathbf{s}_i \\in \\mathcal{S}$.\n\nAll outputs are pure real numbers without physical units. Any probability must be expressed as a decimal in $[0,1]$ (not as a percentage). All logarithms are natural logarithms.\n\nYour program must implement the following test suite. For each case, use exactly the provided constants, current values, and proposals. The state space is the rectangle $\\mathcal{S} = \\{(x,y): x_{\\min} \\le x \\le x_{\\max},\\, y_{\\min} \\le y \\le y_{\\max}\\}$.\n\nCase A (happy path):\n- $\\mathcal{S}$: $x_{\\min}=0$, $x_{\\max}=4$, $y_{\\min}=0$, $y_{\\max}=4$.\n- Traps ($J=3$):\n  - $\\mathbf{x}_1=(0.5,0.5)$,\n  - $\\mathbf{x}_2=(3.0,0.5)$,\n  - $\\mathbf{x}_3=(1.5,3.0)$.\n- Effort: $K=(3,3,4)$.\n- Augmentation size: $M=6$.\n- Current activity centers $\\mathbf{s}_i$:\n  - $\\mathbf{s}_1=(0.8,0.7)$,\n  - $\\mathbf{s}_2=(2.8,0.6)$,\n  - $\\mathbf{s}_3=(1.7,2.6)$,\n  - $\\mathbf{s}_4=(3.2,3.2)$,\n  - $\\mathbf{s}_5=(0.2,3.5)$,\n  - $\\mathbf{s}_6=(3.8,0.1)$.\n- Inclusion indicators: $z=(1,1,1,0,0,0)$.\n- Observed counts $y_{ij}$ (row $i$, column $j$): \n  - $[2,0,0]$,\n  - $[0,2,0]$,\n  - $[0,0,2]$,\n  - $[0,0,0]$,\n  - $[0,0,0]$,\n  - $[0,0,0]$.\n- Priors: $a_\\lambda=0.5$, $b_\\lambda=0.5$, $a_\\psi=1.0$, $b_\\psi=1.0$, $s_{\\max}=2.5$.\n- Current parameters: $\\lambda_0=0.7$, $\\sigma=0.6$, $\\psi=0.5$.\n- Proposals and indices: $\\sigma'=0.7$, $i_0=4$ (the index at which to compute $\\Pr(z_{i_0}=1\\mid\\cdot)$), $i_1=1$ with proposed $\\mathbf{s}_{i_1}'=(1.0,1.0)$.\n\nCase B (edge case with no detections):\n- $\\mathcal{S}$: $x_{\\min}=0$, $x_{\\max}=4$, $y_{\\min}=0$, $y_{\\max}=4$.\n- Traps ($J=2$):\n  - $\\mathbf{x}_1=(0.0,0.0)$,\n  - $\\mathbf{x}_2=(4.0,4.0)$.\n- Effort: $K=(2,2)$.\n- Augmentation size: $M=4$.\n- Current activity centers $\\mathbf{s}_i$:\n  - $\\mathbf{s}_1=(1.0,1.0)$,\n  - $\\mathbf{s}_2=(2.0,2.0)$,\n  - $\\mathbf{s}_3=(3.0,3.0)$,\n  - $\\mathbf{s}_4=(0.5,3.5)$.\n- Inclusion indicators: $z=(0,0,0,0)$.\n- Observed counts $y_{ij}$: all zeros for all $i,j$.\n- Priors: $a_\\lambda=1.0$, $b_\\lambda=1.0$, $a_\\psi=2.0$, $b_\\psi=3.0$, $s_{\\max}=3.0$.\n- Current parameters: $\\lambda_0=0.3$, $\\sigma=1.0$, $\\psi=0.2$.\n- Proposals and indices: $\\sigma'=1.2$, $i_0=1$, $i_1=2$ with proposed $\\mathbf{s}_{i_1}'=(2.0,2.0)$.\n\nCase C (higher encounter rates and effort):\n- $\\mathcal{S}$: $x_{\\min}=0$, $x_{\\max}=4$, $y_{\\min}=0$, $y_{\\max}=4$.\n- Traps ($J=3$):\n  - $\\mathbf{x}_1=(0.5,0.5)$,\n  - $\\mathbf{x}_2=(3.0,0.5)$,\n  - $\\mathbf{x}_3=(1.5,3.0)$.\n- Effort: $K=(5,5,5)$.\n- Augmentation size: $M=5$.\n- Current activity centers $\\mathbf{s}_i$:\n  - $\\mathbf{s}_1=(0.6,0.6)$,\n  - $\\mathbf{s}_2=(2.9,0.6)$,\n  - $\\mathbf{s}_3=(1.6,2.9)$,\n  - $\\mathbf{s}_4=(3.5,3.5)$,\n  - $\\mathbf{s}_5=(0.3,3.6)$.\n- Inclusion indicators: $z=(1,1,1,0,0)$.\n- Observed counts $y_{ij}$:\n  - $[6,1,0]$,\n  - $[0,5,0]$,\n  - $[0,0,6]$,\n  - $[0,0,0]$,\n  - $[0,0,0]$.\n- Priors: $a_\\lambda=0.8$, $b_\\lambda=0.7$, $a_\\psi=1.5$, $b_\\psi=1.5$, $s_{\\max}=3.0$.\n- Current parameters: $\\lambda_0=1.1$, $\\sigma=0.9$, $\\psi=0.6$.\n- Proposals and indices: $\\sigma'=1.0$, $i_0=4$, $i_1=2$ with proposed $\\mathbf{s}_{i_1}'=(3.1,0.5)$.\n\nFor each case, compute and output the following quantities in this order:\n1. Posterior mean of $\\lambda_0$ under its full conditional given current $(\\sigma,\\mathbf{s},z)$ and data.\n2. Posterior mean of $\\psi$ under its full conditional given current $z$.\n3. Posterior Bernoulli probability $\\Pr(z_{i_0}=1 \\mid \\cdot)$ given that individual $i_0$ has $y_{i_0j}=0$ for all $j$.\n4. Log Metropolis–Hastings acceptance ratio for $\\sigma' \\leftarrow \\sigma$.\n5. Log Metropolis–Hastings acceptance ratio for $\\mathbf{s}_{i_1}' \\leftarrow \\mathbf{s}_{i_1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$). The output must concatenate the five results from Case A, then the five from Case B, then the five from Case C, in the order specified above. No intermediate text should be printed. All outputs are real numbers without units, expressed as decimals (probabilities must be in $[0,1]$).", "solution": "The problem statement is found to be valid. It presents a clear, scientifically grounded, and well-posed task in the field of ecological statistics. The model specified is a standard spatial capture-recapture (SCR) model with data augmentation, and all parameters, priors, and data for the test cases are fully provided. We may therefore proceed with the derivation of the required quantities and the implementation of their computation.\n\nThe joint posterior distribution of all parameters $(\\psi, \\lambda_0, \\sigma)$ and latent variables $(\\{\\mathbf{s}_i\\}, \\{z_i\\})$ given the observed data $\\{y_{ij}\\}$ is proportional to the product of the likelihood and the priors:\n$$ p(\\psi, \\lambda_0, \\sigma, \\{\\mathbf{s}_i\\}_{i=1}^M, \\{z_i\\}_{i=1}^M \\mid \\{y_{ij}\\}) \\propto \\left( \\prod_{i=1}^M \\prod_{j=1}^J p(y_{ij} \\mid z_i, \\mathbf{s}_i, \\lambda_0, \\sigma) \\right) \\left( \\prod_{i=1}^M p(z_i \\mid \\psi) p(\\mathbf{s}_i) \\right) p(\\psi) p(\\lambda_0) p(\\sigma) $$\nThe components of this expression are defined as:\n- Likelihood: $y_{ij} \\sim \\text{Poisson}(\\mu_{ij})$, with mean $\\mu_{ij} = z_i K_j \\lambda_0 g(\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert; \\sigma)$. The detection function is $g(d; \\sigma) = \\exp(-d^2 / (2\\sigma^2))$.\n- Priors:\n  - $z_i \\mid \\psi \\sim \\text{Bernoulli}(\\psi)$, so $p(z_i \\mid \\psi) = \\psi^{z_i}(1-\\psi)^{1-z_i}$.\n  - $\\psi \\sim \\text{Beta}(a_\\psi, b_\\psi)$, so $p(\\psi) \\propto \\psi^{a_\\psi-1}(1-\\psi)^{b_\\psi-1}$.\n  - $\\lambda_0 \\sim \\text{Gamma}(a_\\lambda, b_\\lambda)$, so $p(\\lambda_0) \\propto \\lambda_0^{a_\\lambda-1} e^{-b_\\lambda \\lambda_0}$.\n  - $\\sigma \\sim \\text{Uniform}(0, s_{\\max})$, so $p(\\sigma) \\propto 1$ for $\\sigma \\in (0, s_{\\max})$.\n  - $\\mathbf{s}_i \\sim \\text{Uniform}(\\mathcal{S})$, so $p(\\mathbf{s}_i) \\propto 1$ for $\\mathbf{s}_i \\in \\mathcal{S}$.\n\nFrom this joint posterior, we derive the full conditional distributions required for the MCMC sampler.\n\n**1. Posterior Mean of $\\lambda_0$**\nThe full conditional for $\\lambda_0$ is found by collecting all terms in the joint posterior that involve $\\lambda_0$:\n$$ p(\\lambda_0 \\mid \\cdot) \\propto p(\\{y_{ij}\\} \\mid \\lambda_0, \\dots) p(\\lambda_0) $$\n$$ \\propto \\left( \\prod_{i,j} \\frac{(z_i K_j \\lambda_0 g_{ij})^{y_{ij}} e^{-z_i K_j \\lambda_0 g_{ij}}}{y_{ij}!} \\right) \\left( \\lambda_0^{a_\\lambda-1} e^{-b_\\lambda \\lambda_0} \\right) $$\nwhere $g_{ij} = g(\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert; \\sigma)$. Combining terms dependent on $\\lambda_0$:\n$$ \\propto \\lambda_0^{\\sum_{i,j} y_{ij}} e^{-\\lambda_0 \\sum_{i,j} z_i K_j g_{ij}} \\cdot \\lambda_0^{a_\\lambda-1} e^{-b_\\lambda \\lambda_0} $$\n$$ \\propto \\lambda_0^{(a_\\lambda + \\sum_{i,j} y_{ij}) - 1} \\exp\\left( - (b_\\lambda + \\sum_{i,j} z_i K_j g_{ij}) \\lambda_0 \\right) $$\nThis is the kernel of a Gamma distribution, $\\text{Gamma}(\\alpha', \\beta')$, with updated parameters:\n- Shape: $\\alpha' = a_\\lambda + \\sum_{i=1}^M \\sum_{j=1}^J y_{ij}$\n- Rate: $\\beta' = b_\\lambda + \\sum_{i=1}^M z_i \\sum_{j=1}^J K_j g(\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert; \\sigma)$\nThe mean of a Gamma distribution with shape $\\alpha'$ and rate $\\beta'$ is $\\alpha'/\\beta'$. Thus, the posterior mean is:\n$$ E[\\lambda_0 \\mid \\cdot] = \\frac{a_\\lambda + \\sum_{i,j} y_{ij}}{b_\\lambda + \\sum_{i: z_i=1} \\sum_{j=1}^J K_j g(\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert; \\sigma)} $$\n\n**2. Posterior Mean of $\\psi$**\nDue to the conjugacy of the Beta prior on $\\psi$ and the Bernoulli likelihood for the $z_i$ indicators, the full conditional for $\\psi$ is also a Beta distribution.\n$$ p(\\psi \\mid \\cdot) \\propto \\left( \\prod_{i=1}^M p(z_i \\mid \\psi) \\right) p(\\psi) = \\left( \\prod_{i=1}^M \\psi^{z_i}(1-\\psi)^{1-z_i} \\right) \\left( \\psi^{a_\\psi-1}(1-\\psi)^{b_\\psi-1} \\right) $$\n$$ \\propto \\psi^{\\sum z_i} (1-\\psi)^{M-\\sum z_i} \\psi^{a_\\psi-1}(1-\\psi)^{b_\\psi-1} = \\psi^{(a_\\psi + \\sum z_i) - 1} (1-\\psi)^{(b_\\psi + M - \\sum z_i) - 1} $$\nThis is a Beta distribution, $\\text{Beta}(\\alpha', \\beta')$, with updated parameters:\n- Shape 1: $\\alpha' = a_\\psi + \\sum_{i=1}^M z_i$\n- Shape 2: $\\beta' = b_\\psi + M - \\sum_{i=1}^M z_i$\nThe mean of this Beta distribution is $\\alpha'/(\\alpha'+\\beta')$. Therefore:\n$$ E[\\psi \\mid \\cdot] = \\frac{a_\\psi + \\sum z_i}{a_\\psi + b_\\psi + M} $$\n\n**3. Posterior Probability $\\Pr(z_{i_0}=1 \\mid \\cdot)$ for an Unobserved Individual**\nFor an individual $i_0$ with no detections ($y_{i_0,j}=0$ for all $j$), we update $z_{i_0}$ via a Bernoulli trial. The probability $\\Pr(z_{i_0}=1 \\mid \\cdot)$ is found by comparing the posterior probabilities of $z_{i_0}=1$ and $z_{i_0}=0$. Using Bayes' theorem, the odds are:\n$$ \\frac{\\Pr(z_{i_0}=1 \\mid \\cdot)}{\\Pr(z_{i_0}=0 \\mid \\cdot)} = \\frac{p(\\{y_{i_0,j}=0\\} \\mid z_{i_0}=1, \\cdot) p(z_{i_0}=1 \\mid \\psi)}{p(\\{y_{i_0,j}=0\\} \\mid z_{i_0}=0, \\cdot) p(z_{i_0}=0 \\mid \\psi)} $$\n- If $z_{i_0}=1$, $\\mu_{i_0,j} = K_j \\lambda_0 g_{i_0,j} > 0$. The likelihood is $L_1 = \\prod_j P(0|\\mu_{i_0,j}) = \\prod_j e^{-\\mu_{i_0,j}} = \\exp(-\\sum_j \\mu_{i_0,j})$.\n- If $z_{i_0}=0$, $\\mu_{i_0,j} = 0$. The likelihood is $L_0 = \\prod_j P(0|0) = 1$.\n- The prior ratio is $p(z_{i_0}=1 \\mid \\psi) / p(z_{i_0}=0 \\mid \\psi) = \\psi / (1-\\psi)$.\nThe odds are $\\text{Odds} = \\frac{L_1}{L_0} \\frac{\\psi}{1-\\psi} = \\frac{\\psi}{1-\\psi} \\exp\\left(-\\lambda_0 \\sum_{j=1}^J K_j g(\\lVert \\mathbf{s}_{i_0} - \\mathbf{x}_j \\rVert; \\sigma)\\right)$.\nThe probability is then recovered as $p = \\text{Odds} / (1+\\text{Odds})$.\n\n**4. Log Metropolis-Hastings Acceptance Ratio for $\\sigma' \\leftarrow \\sigma$**\nFor a Metropolis-Hastings update with a symmetric proposal distribution, the acceptance ratio is $\\alpha = \\min(1, \\frac{p(\\sigma' \\mid \\cdot)}{p(\\sigma \\mid \\cdot)})$. The log-ratio, which we must compute, is $\\log R = \\log p(\\sigma' \\mid \\cdot) - \\log p(\\sigma \\mid \\cdot)$. The conditional posterior is $p(\\sigma \\mid \\cdot) \\propto p(\\{y_{ij}\\} \\mid \\sigma, \\cdot) p(\\sigma)$. We only consider individuals with $z_i=1$, as the likelihood for $z_i=0$ individuals does not depend on $\\sigma$.\n$$ \\log p(\\sigma \\mid \\cdot) = \\text{const} + \\log(p(\\sigma)) + \\sum_{i:z_i=1} \\sum_j \\log P(y_{ij} \\mid \\sigma, \\cdot) $$\n$$ = \\text{const} + \\log(p(\\sigma)) + \\sum_{i:z_i=1} \\sum_j \\left( y_{ij} \\log \\mu_{ij} - \\mu_{ij} - \\log(y_{ij}!) \\right) $$\nSubstituting $\\mu_{ij} = K_j \\lambda_0 \\exp(-d_{ij}^2/(2\\sigma^2))$ and dropping terms that are constant with respect to $\\sigma$, the relevant part of the log-posterior is:\n$$ \\mathcal{L}(\\sigma) = \\sum_{i:z_i=1} \\sum_j \\left( -y_{ij} \\frac{d_{ij}^2}{2\\sigma^2} - K_j \\lambda_0 \\exp\\left(-\\frac{d_{ij}^2}{2\\sigma^2}\\right) \\right) $$\nThe prior $p(\\sigma)$ is uniform, so $\\log p(\\sigma)$ is constant for $\\sigma \\in (0, s_{\\max})$. As long as both $\\sigma$ and $\\sigma'$ are in this interval, the prior terms cancel. The log acceptance ratio is thus $\\log R = \\mathcal{L}(\\sigma') - \\mathcal{L}(\\sigma)$.\n\n**5. Log Metropolis-Hastings Acceptance Ratio for $\\mathbf{s}_{i_1}' \\leftarrow \\mathbf{s}_{i_1}$**\nSimilarly, for updating the activity center $\\mathbf{s}_{i_1}$ of an individual $i_1$ (with $z_{i_1}=1$) with a symmetric proposal, the log acceptance ratio is $\\log R = \\log p(\\mathbf{s}_{i_1}' \\mid \\cdot) - \\log p(\\mathbf{s}_{i_1} \\mid \\cdot)$. This depends only on the likelihood for individual $i_1$ and its prior.\n$$ \\log p(\\mathbf{s}_{i_1} \\mid \\cdot) = \\text{const} + \\log p(\\mathbf{s}_{i_1}) + \\sum_j \\log P(y_{i_1,j} \\mid \\mathbf{s}_{i_1}, \\cdot) $$\nThe prior $p(\\mathbf{s}_{i_1})$ is uniform on $\\mathcal{S}$, so its log is constant and cancels if both $\\mathbf{s}_{i_1}$ and $\\mathbf{s}_{i_1}'$ are in $\\mathcal{S}$. The relevant part of the log-posterior for individual $i_1$ is:\n$$ \\mathcal{L}_i(\\mathbf{s}_i) = \\sum_{j=1}^J \\left( -y_{ij} \\frac{\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert^2}{2\\sigma^2} - K_j \\lambda_0 \\exp\\left(-\\frac{\\lVert \\mathbf{s}_i - \\mathbf{x}_j \\rVert^2}{2\\sigma^2}\\right) \\right) $$\nThe log acceptance ratio is $\\log R = \\mathcal{L}_{i_1}(\\mathbf{s}_{i_1}') - \\mathcal{L}_{i_1}(\\mathbf{s}_{i_1})$. If $z_{i_1}=0$, $\\mu_{i_1,j}=0$, the likelihood does not depend on $\\mathbf{s}_{i_1}$, and $\\mathcal{L}_{i_1}(\\mathbf{s}_{i_1})=0$. In this case, the acceptance ratio for any proposal within $\\mathcal{S}$ is $1$, and the log-ratio is $0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute the required quantities for the \n    Bayesian spatial capture-recapture model.\n    \"\"\"\n\n    test_cases = [\n        # Case A: happy path\n        {\n            \"S_bounds\": (0, 4, 0, 4),\n            \"traps\": np.array([[0.5, 0.5], [3.0, 0.5], [1.5, 3.0]]),\n            \"effort\": np.array([3, 3, 4]),\n            \"M\": 6,\n            \"s_centers\": np.array([[0.8, 0.7], [2.8, 0.6], [1.7, 2.6], [3.2, 3.2], [0.2, 3.5], [3.8, 0.1]]),\n            \"z\": np.array([1, 1, 1, 0, 0, 0]),\n            \"y_counts\": np.array([[2, 0, 0], [0, 2, 0], [0, 0, 2], [0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"priors\": {\"a_lambda\": 0.5, \"b_lambda\": 0.5, \"a_psi\": 1.0, \"b_psi\": 1.0, \"s_max\": 2.5},\n            \"current_params\": {\"lambda0\": 0.7, \"sigma\": 0.6, \"psi\": 0.5},\n            \"proposals\": {\"sigma_prime\": 0.7, \"i0\": 4, \"i1\": 1, \"s_i1_prime\": np.array([1.0, 1.0])}\n        },\n        # Case B: edge case with no detections\n        {\n            \"S_bounds\": (0, 4, 0, 4),\n            \"traps\": np.array([[0.0, 0.0], [4.0, 4.0]]),\n            \"effort\": np.array([2, 2]),\n            \"M\": 4,\n            \"s_centers\": np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0], [0.5, 3.5]]),\n            \"z\": np.array([0, 0, 0, 0]),\n            \"y_counts\": np.array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n            \"priors\": {\"a_lambda\": 1.0, \"b_lambda\": 1.0, \"a_psi\": 2.0, \"b_psi\": 3.0, \"s_max\": 3.0},\n            \"current_params\": {\"lambda0\": 0.3, \"sigma\": 1.0, \"psi\": 0.2},\n            \"proposals\": {\"sigma_prime\": 1.2, \"i0\": 1, \"i1\": 2, \"s_i1_prime\": np.array([2.0, 2.0])}\n        },\n        # Case C: higher encounter rates and effort\n        {\n            \"S_bounds\": (0, 4, 0, 4),\n            \"traps\": np.array([[0.5, 0.5], [3.0, 0.5], [1.5, 3.0]]),\n            \"effort\": np.array([5, 5, 5]),\n            \"M\": 5,\n            \"s_centers\": np.array([[0.6, 0.6], [2.9, 0.6], [1.6, 2.9], [3.5, 3.5], [0.3, 3.6]]),\n            \"z\": np.array([1, 1, 1, 0, 0]),\n            \"y_counts\": np.array([[6, 1, 0], [0, 5, 0], [0, 0, 6], [0, 0, 0], [0, 0, 0]]),\n            \"priors\": {\"a_lambda\": 0.8, \"b_lambda\": 0.7, \"a_psi\": 1.5, \"b_psi\": 1.5, \"s_max\": 3.0},\n            \"current_params\": {\"lambda0\": 1.1, \"sigma\": 0.9, \"psi\": 0.6},\n            \"proposals\": {\"sigma_prime\": 1.0, \"i0\": 4, \"i1\": 2, \"s_i1_prime\": np.array([3.1, 0.5])}\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        results.extend(process_case(case))\n        \n    print(f\"[{','.join(f'{r:.10g}' for r in results)}]\")\n\ndef half_normal(d_sq, sigma):\n    return np.exp(-d_sq / (2 * sigma**2))\n\ndef calc_lambda0_mean(y, z, s, traps, K, sigma, priors):\n    a_lambda, b_lambda = priors[\"a_lambda\"], priors[\"b_lambda\"]\n    \n    y_sum = np.sum(y)\n    alpha_prime = a_lambda + y_sum\n\n    g_sum_term = 0.0\n    for i, s_i in enumerate(s):\n        if z[i] == 1:\n            d_sq = np.sum((s_i - traps)**2, axis=1)\n            g_ij = half_normal(d_sq, sigma)\n            g_sum_term += np.sum(K * g_ij)\n            \n    beta_prime = b_lambda + g_sum_term\n    \n    return alpha_prime / beta_prime\n\ndef calc_psi_mean(z, M, priors):\n    a_psi, b_psi = priors[\"a_psi\"], priors[\"b_psi\"]\n    n_pop = np.sum(z)\n    \n    alpha_prime = a_psi + n_pop\n    beta_prime = b_psi + M - n_pop\n    \n    return alpha_prime / (alpha_prime + beta_prime)\n\ndef calc_z_prob(i0_idx, s_i0, traps, K, lambda0, sigma, psi):\n    d_sq = np.sum((s_i0 - traps)**2, axis=1)\n    g_i0j = half_normal(d_sq, sigma)\n    \n    sum_kg = np.sum(K * g_i0j)\n    \n    exp_term = np.exp(-lambda0 * sum_kg)\n    odds = (psi / (1 - psi)) * exp_term\n    \n    return odds / (1 + odds)\n\ndef _log_likelihood_sigma_part(sigma, y, z, s, traps, K, lambda0):\n    log_post = 0.0\n    for i, s_i in enumerate(s):\n        if z[i] == 1:\n            d_sq = np.sum((s_i - traps)**2, axis=1)\n            g_ij = half_normal(d_sq, sigma)\n            log_post += np.sum(-y[i] * d_sq / (2 * sigma**2) - K * lambda0 * g_ij)\n    return log_post\n\ndef calc_sigma_log_mh_ratio(sigma, sigma_prime, y, z, s, traps, K, lambda0, priors):\n    s_max = priors[\"s_max\"]\n    if not (0  sigma_prime  s_max):\n        return -np.inf\n    \n    log_post_current = _log_likelihood_sigma_part(sigma, y, z, s, traps, K, lambda0)\n    log_post_prime = _log_likelihood_sigma_part(sigma_prime, y, z, s, traps, K, lambda0)\n    \n    return log_post_prime - log_post_current\n    \ndef _log_likelihood_s_part(s_i, y_i, traps, K, lambda0, sigma):\n    d_sq = np.sum((s_i - traps)**2, axis=1)\n    g_ij = half_normal(d_sq, sigma)\n    log_post = np.sum(-y_i * d_sq / (2 * sigma**2) - K * lambda0 * g_ij)\n    return log_post\n\ndef calc_s_log_mh_ratio(i1_idx, s_i1, s_i1_prime, y, z, traps, K, lambda0, sigma, S_bounds):\n    x_min, x_max, y_min, y_max = S_bounds\n    \n    if not (x_min = s_i1_prime[0] = x_max and y_min = s_i1_prime[1] = y_max):\n        return -np.inf\n    \n    if z[i1_idx] == 0:\n        return 0.0\n\n    log_post_current = _log_likelihood_s_part(s_i1, y[i1_idx], traps, K, lambda0, sigma)\n    log_post_prime = _log_likelihood_s_part(s_i1_prime, y[i1_idx], traps, K, lambda0, sigma)\n    \n    return log_post_prime - log_post_current\n\ndef process_case(case):\n    S_bounds = case[\"S_bounds\"]\n    traps = case[\"traps\"]\n    effort = case[\"effort\"]\n    M = case[\"M\"]\n    s = case[\"s_centers\"]\n    z = case[\"z\"]\n    y = case[\"y_counts\"]\n    priors = case[\"priors\"]\n    current_params = case[\"current_params\"]\n    proposals = case[\"proposals\"]\n    \n    lambda0 = current_params[\"lambda0\"]\n    sigma = current_params[\"sigma\"]\n    psi = current_params[\"psi\"]\n    \n    sigma_prime = proposals[\"sigma_prime\"]\n    # Convert 1-based index from problem to 0-based for array access\n    i0_idx = proposals[\"i0\"] - 1\n    i1_idx = proposals[\"i1\"] - 1\n    s_i1_prime = proposals[\"s_i1_prime\"]\n    \n    # 1. Posterior mean of lambda0\n    mean_lambda0 = calc_lambda0_mean(y, z, s, traps, effort, sigma, priors)\n    \n    # 2. Posterior mean of psi\n    mean_psi = calc_psi_mean(z, M, priors)\n    \n    # 3. Posterior Bernoulli probability Pr(z_i0=1 | .)\n    prob_z = calc_z_prob(i0_idx, s[i0_idx], traps, effort, lambda0, sigma, psi)\n    \n    # 4. Log M-H acceptance ratio for sigma'\n    log_mh_sigma = calc_sigma_log_mh_ratio(sigma, sigma_prime, y, z, s, traps, effort, lambda0, priors)\n    \n    # 5. Log M-H acceptance ratio for s_i1'\n    log_mh_s = calc_s_log_mh_ratio(i1_idx, s[i1_idx], s_i1_prime, y, z, traps, effort, lambda0, sigma, S_bounds)\n    \n    return [mean_lambda0, mean_psi, prob_z, log_mh_sigma, log_mh_s]\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "2523149"}]}