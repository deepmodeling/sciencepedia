{"hands_on_practices": [{"introduction": "Understanding the persistence of genetic diversity is fundamental to conservation biology. This practice explores the concept of effective population size ($N_e$), a crucial metric that reveals how fluctuations in census size, particularly population bottlenecks, impact the long-term retention of genetic variation. By deriving the harmonic mean effective size from first principles, you will gain a deeper appreciation for why small populations are of such great concern in conservation genetics. [@problem_id:2472477]", "problem": "A panmictic, diploid population follows the Wright–Fisher model with discrete, nonoverlapping generations, no mutation, and no migration. Let the census size in generation $t$ be $N_t$. Over a finite window of $T$ generations, define the effective population size ($N_e$) as the constant population size that would produce the same expected cumulative decline in neutral heterozygosity as the observed time-varying census sizes.\n\nYou observe the time series $N_1 = 50$, $N_2 = 200$, and $N_3 = 100$ (so $T=3$). Starting only from the fundamental single-generation relationship between expected neutral heterozygosity and census size under Wright–Fisher reproduction, derive an expression for $N_e$ over $T$ generations in terms of $\\{N_t\\}_{t=1}^T$ and use it to compute the harmonic-mean effective size for the given series. Report your final numerical answer in individuals as an exact rational number; do not round.\n\nIn your reasoning, explain why temporal variation in $\\{N_t\\}$ lowers $N_e$ relative to the arithmetic mean of $\\{N_t\\}$ and thus depresses neutral genetic diversity through time.", "solution": "The problem statement is scientifically grounded, well-posed, and contains sufficient information for a unique solution. It is a standard exercise in population genetics based on the Wright-Fisher model. Thus, the problem is deemed valid and a solution will be provided.\n\nThe derivation begins with the fundamental relationship describing the decay of expected heterozygosity due to genetic drift in a single generation under the Wright-Fisher model. For a panmictic, diploid population of census size $N_t$ in generation $t$, the expected heterozygosity in the subsequent generation, $H_{t+1}$, is related to the heterozygosity in generation $t$, $H_t$, by:\n$$E[H_{t+1}] = \\left(1 - \\frac{1}{2N_t}\\right) H_t$$\nThis relationship arises because the probability that two alleles, sampled with replacement from the gene pool of generation $t$ to form an individual in generation $t+1$, are identical by descent is $\\frac{1}{2N_t}$. The term $\\left(1 - \\frac{1}{2N_t}\\right)$ is therefore the probability that they are not identical by descent, which is the factor by which heterozygosity is expected to decline.\n\nTo determine the cumulative effect over $T$ generations with time-varying census sizes $\\{N_t\\}_{t=1}^T$, we apply this recurrence relation iteratively. Let $H_1$ be the heterozygosity at the start of the process (i.e., in generation $1$). The expected heterozygosity after $T$ generations, at the start of generation $T+1$, is:\n$$E[H_{T+1}] = H_1 \\prod_{t=1}^{T} \\left(1 - \\frac{1}{2N_t}\\right)$$\nThe problem defines the effective population size, $N_e$, as the size of an idealized constant population that would experience the same cumulative decline in heterozygosity over the same $T$ generations. For a constant population of size $N_e$, the expected heterozygosity after $T$ generations would be:\n$$E[H_{T+1}] = H_1 \\left(1 - \\frac{1}{2N_e}\\right)^T$$\nBy definition, these two cumulative decline factors must be equal:\n$$\\left(1 - \\frac{1}{2N_e}\\right)^T = \\prod_{t=1}^{T} \\left(1 - \\frac{1}{2N_t}\\right)$$\nFor census sizes $N_t$ that are much larger than $1$, which is a standard assumption in such models, we can use the approximation $1-x \\approx \\exp(-x)$ for small $x$. In our case, $x = \\frac{1}{2N_t}$ is very small. Applying this approximation to both sides of the equation yields:\n$$\\left(\\exp\\left(-\\frac{1}{2N_e}\\right)\\right)^T \\approx \\prod_{t=1}^{T} \\exp\\left(-\\frac{1}{2N_t}\\right)$$\nUsing the property of exponents $\\exp(a)\\exp(b) = \\exp(a+b)$, the right side simplifies:\n$$\\exp\\left(-\\frac{T}{2N_e}\\right) \\approx \\exp\\left(-\\sum_{t=1}^{T} \\frac{1}{2N_t}\\right)$$\nFor the two sides to be equal, their exponents must be equal:\n$$\\frac{T}{2N_e} = \\sum_{t=1}^{T} \\frac{1}{2N_t}$$\nMultiplying by $2$ and rearranging for $N_e$ gives the expression for the effective population size:\n$$\\frac{T}{N_e} = \\sum_{t=1}^{T} \\frac{1}{N_t}$$\n$$N_e = \\frac{T}{\\sum_{t=1}^{T} \\frac{1}{N_t}}$$\nThis expression shows that the effective population size over a period of fluctuating census sizes is the harmonic mean of those census sizes.\n\nNow, we compute the value of $N_e$ for the given time series: $T=3$, $N_1 = 50$, $N_2 = 200$, and $N_3 = 100$.\nSubstituting these values into the derived formula:\n$$N_e = \\frac{3}{\\frac{1}{50} + \\frac{1}{200} + \\frac{1}{100}}$$\nTo evaluate the denominator, we find a common denominator, which is $200$:\n$$\\sum_{t=1}^{3} \\frac{1}{N_t} = \\frac{4}{200} + \\frac{1}{200} + \\frac{2}{200} = \\frac{4+1+2}{200} = \\frac{7}{200}$$\nSubstituting this back into the expression for $N_e$:\n$$N_e = \\frac{3}{\\frac{7}{200}} = 3 \\times \\frac{200}{7} = \\frac{600}{7}$$\n\nFinally, we must explain why temporal variation in $\\{N_t\\}$ lowers $N_e$ relative to the arithmetic mean of $\\{N_t\\}$. The arithmetic mean is given by $\\bar{N}_{\\text{arith}} = \\frac{1}{T}\\sum_{t=1}^T N_t$. The effective size $N_e$ is the harmonic mean. By the arithmetic mean-harmonic mean (AM-HM) inequality, for any set of positive numbers $\\{N_t\\}$, the arithmetic mean is always greater than or equal to the harmonic mean:\n$$\\frac{1}{T}\\sum_{t=1}^T N_t \\ge \\frac{T}{\\sum_{t=1}^T \\frac{1}{N_t}}$$\nEquality holds only if all $N_t$ are equal. Therefore, $\\bar{N}_{\\text{arith}} \\ge N_e$, and any temporal variation in population size will cause the effective size to be strictly less than the arithmetic mean.\n\nThe biological reason for this is that the rate of heterozygosity loss in a given generation is inversely proportional to the population size ($\\frac{1}{2N_t}$). Generations with small population sizes, known as bottlenecks, contribute disproportionately to the cumulative loss of genetic diversity. A short period of small $N_t$ leads to a large value of $\\frac{1}{2N_t}$, causing a severe reduction in heterozygosity. Subsequent generations with large population sizes cannot reverse this loss; they can only slow the rate of future loss. The long-term rate of drift is therefore governed by the most extreme bottlenecks. The harmonic mean is a mathematical construct that is heavily weighted by the smallest values in a series, and thus it correctly captures this biological phenomenon. For the given series, the arithmetic mean is $\\frac{50+200+100}{3} = \\frac{350}{3} \\approx 116.7$, whereas the effective size is $N_e = \\frac{600}{7} \\approx 85.7$. The low census size of $50$ in the first generation has a disproportionate effect, pulling the effective size far below the arithmetic average and causing the population to lose genetic diversity at a much faster rate than the average size would suggest.", "answer": "$$\\boxed{\\frac{600}{7}}$$", "id": "2472477"}, {"introduction": "Moving from the genetic to the community level, biodiversity patterns are often shaped by the functional traits of species. This hands-on coding exercise challenges you to investigate the ecological processes of community assembly, such as environmental filtering and limiting similarity, by implementing a null model test. You will calculate functional dispersion (FDis) and its standardized effect size (SES) to quantitatively test hypotheses about how species with different traits are organized along an environmental gradient. [@problem_id:2472458]", "problem": "You are given species-by-trait data and site-by-species community membership along an environmental gradient. The goal is to implement a principled null-model test for environmental filtering versus limiting similarity using the standardized effect size of functional dispersion. All definitions and steps must be derived from core definitions of distance in trait space and randomization under a null that preserves community richness while destroying species–trait associations.\n\nDefinitions to use as foundations:\n- A functional trait vector for each species is an element of a Euclidean space of dimension $D$, written as $\\mathbf{t}_i \\in \\mathbb{R}^D$ for species index $i \\in \\{1,\\dots,S\\}$.\n- For a community at a site, let $\\mathbf{a} \\in \\{0,1\\}^S$ be the presence–absence vector over the species pool of size $S$. The community contains the index set $I = \\{ i \\mid a_i = 1 \\}$ and has richness $m = \\sum_{i=1}^S a_i$.\n- The centroid (community-weighted mean trait) is $\\mathbf{c} = \\left(\\sum_{i \\in I} w_i \\mathbf{t}_i\\right) \\Big/ \\left(\\sum_{i \\in I} w_i\\right)$ with presence weights $w_i = 1$ for all $i \\in I$.\n- The Euclidean distance from $\\mathbf{t}_i$ to $\\mathbf{c}$ is $d_i = \\lVert \\mathbf{t}_i - \\mathbf{c} \\rVert_2$ for $i \\in I$.\n- Functional dispersion (FDis) is the weighted mean distance to the centroid, $\\mathrm{FDis} = \\left(\\sum_{i \\in I} w_i d_i\\right) \\Big/ \\left(\\sum_{i \\in I} w_i\\right)$. With presence weights, this is the simple mean of $\\{ d_i \\}_{i \\in I}$.\n- The null model randomizes the mapping between species identities and trait vectors while preserving the community membership vector $\\mathbf{a}$ at each site. Specifically, for each null replicate, apply a uniform random permutation $\\pi$ to the rows of the trait matrix, producing a permuted trait set $\\{\\mathbf{t}_{\\pi(1)},\\dots,\\mathbf{t}_{\\pi(S)}\\}$, then compute $\\mathrm{FDis}^{(r)}$ using the same $\\mathbf{a}$, for replicate index $r \\in \\{1,\\dots,R\\}$.\n- The standardized effect size is $\\mathrm{SES} = \\left(\\mathrm{FDis}_{\\mathrm{obs}} - \\mu_{\\mathrm{null}}\\right) \\Big/ \\sigma_{\\mathrm{null}}$ where $\\mu_{\\mathrm{null}}$ is the mean and $\\sigma_{\\mathrm{null}}$ is the standard deviation of the null replicates. If $\\sigma_{\\mathrm{null}} = 0$, define $\\mathrm{SES} = 0$ by convention to avoid division by zero. Negative $\\mathrm{SES}$ indicates trait clustering consistent with environmental filtering; positive $\\mathrm{SES}$ indicates overdispersion consistent with limiting similarity.\n\nAlgorithmic requirements:\n- Implement computation of $\\mathrm{FDis}$ in $\\mathbb{R}^D$ using the definitions above.\n- Implement the trait-label randomization null model with $R$ replicates and a fixed random seed to ensure reproducibility.\n- Compute $\\mathrm{SES}$ for each site, rounding to three decimals for output.\n- Use the two-tailed interpretive threshold $\\alpha = 0.10$ with critical value $z_{\\alpha/2} = 1.645$ only for interpretation; do not include classification in the required output.\n\nTest suite specification:\n- Use the following two cases.\n\nCase A (univariate traits, $D = 1$, $S = 8$, $5$ sites):\n- Trait vector $\\mathbf{t} \\in \\mathbb{R}^8$:\n  $\\mathbf{t} = [\\,0.05,\\,0.12,\\,0.20,\\,0.50,\\,0.60,\\,0.80,\\,0.92,\\,0.97\\,]$.\n- Site-by-species presence matrix $\\mathbf{A} \\in \\{0,1\\}^{5 \\times 8}$ with rows as sites $1$ through $5$:\n  - Site $1$: $[\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$.\n  - Site $2$: $[\\,0,\\,1,\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]$.\n  - Site $3$: $[\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$.\n  - Site $4$: $[\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,1\\,]$.\n  - Site $5$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,0\\,]$.\n- Environmental gradient values (not used in computation, provided for context): $\\mathbf{E} = [\\,0.10,\\,0.50,\\,0.90,\\,0.60,\\,0.50\\,]$.\n\nCase B (bivariate traits, $D = 2$, $S = 6$, $3$ sites):\n- Trait matrix $\\mathbf{T} \\in \\mathbb{R}^{6 \\times 2}$ with rows as species $1$ through $6$:\n  $\\big[\\, [\\,0.10,\\,0.10\\,],\\; [\\,0.20,\\,0.20\\,],\\; [\\,0.30,\\,0.30\\,],\\; [\\,0.70,\\,0.70\\,],\\; [\\,0.80,\\,0.80\\,],\\; [\\,0.90,\\,0.90\\,] \\,\\big]$.\n- Site-by-species presence matrix $\\mathbf{A} \\in \\{0,1\\}^{3 \\times 6}$ with rows as sites $1$ through $3$:\n  - Site $1$: $[\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]$.\n  - Site $2$: $[\\,1,\\,0,\\,0,\\,0,\\,0,\\,1\\,]$.\n  - Site $3$: $[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$.\n- Environmental gradient values (not used in computation, provided for context): $\\mathbf{E} = [\\,0.15,\\,0.85,\\,0.50\\,]$.\n\nNull model settings and reproducibility:\n- Number of null replicates: $R = 499$.\n- Random seed: $123$.\n\nAngle units are not applicable. No physical units are involved. All outputs are pure numbers.\n\nRequired final output format:\n- Your program should produce a single line of output containing the concatenated list of $\\mathrm{SES}$ values across all sites and both cases, ordered by Case A sites $1$ through $5$ followed by Case B sites $1$ through $3$, each rounded to three decimals, as a comma-separated list enclosed in square brackets. For example: $[\\mathrm{ses}_1,\\mathrm{ses}_2,\\dots,\\mathrm{ses}_8]$.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n- **Functional Trait Vector**: For each species $i \\in \\{1, \\dots, S\\}$, the trait vector is $\\mathbf{t}_i \\in \\mathbb{R}^D$.\n- **Community Composition**: For a given site, the presence-absence vector is $\\mathbf{a} \\in \\{0, 1\\}^S$. The set of present species is $I = \\{ i \\mid a_i = 1 \\}$. Community richness is $m = \\sum_{i=1}^S a_i$.\n- **Community Centroid**: The centroid is defined as $\\mathbf{c} = \\left(\\sum_{i \\in I} w_i \\mathbf{t}_i\\right) \\Big/ \\left(\\sum_{i \\in I} w_i\\right)$, with presence-only weights $w_i = 1$ for all $i \\in I$. This simplifies to the arithmetic mean of the trait vectors of present species.\n- **Euclidean Distance**: The distance of a species' trait vector from the centroid is $d_i = \\lVert \\mathbf{t}_i - \\mathbf{c} \\rVert_2$ for $i \\in I$.\n- **Functional Dispersion (FDis)**: $\\mathrm{FDis} = \\left(\\sum_{i \\in I} w_i d_i\\right) \\Big/ \\left(\\sum_{i \\in I} w_i\\right)$. For $w_i=1$, this is the arithmetic mean of the distances $\\{d_i\\}_{i \\in I}$.\n- **Null Model**: The mapping between species identities and trait vectors is randomized. For each replicate $r \\in \\{1, \\dots, R\\}$, a uniform random permutation $\\pi$ is applied to the trait matrix rows, creating a permuted set $\\{\\mathbf{t}_{\\pi(1)}, \\dots, \\mathbf{t}_{\\pi(S)}\\}$. $\\mathrm{FDis}^{(r)}$ is then computed for each site using the original presence-absence vector $\\mathbf{a}$.\n- **Standardized Effect Size (SES)**: $\\mathrm{SES} = (\\mathrm{FDis}_{\\mathrm{obs}} - \\mu_{\\mathrm{null}}) / \\sigma_{\\mathrm{null}}$, where $\\mu_{\\mathrm{null}}$ and $\\sigma_{\\mathrm{null}}$ are the mean and standard deviation of the $R$ null replicate values of FDis. If $\\sigma_{\\mathrm{null}} = 0$, then $\\mathrm{SES}$ is defined to be $0$.\n- **Case A Data**:\n    - Dimension $D = 1$, Species count $S = 8$, Number of sites $= 5$.\n    - Trait vector $\\mathbf{t} = [\\,0.05,\\,0.12,\\,0.20,\\,0.50,\\,0.60,\\,0.80,\\,0.92,\\,0.97\\,]$.\n    - Presence matrix $\\mathbf{A} \\in \\{0,1\\}^{5 \\times 8}$:\n        - Site $1$: $[\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$.\n        - Site $2$: $[\\,0,\\,1,\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]$.\n        - Site $3$: $[\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$.\n        - Site $4$: $[\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,1\\,]$.\n        - Site $5$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,0\\,]$.\n- **Case B Data**:\n    - Dimension $D = 2$, Species count $S = 6$, Number of sites $= 3$.\n    - Trait matrix $\\mathbf{T} \\in \\mathbb{R}^{6 \\times 2}$: $\\big[\\, [\\,0.10,\\,0.10\\,],\\; [\\,0.20,\\,0.20\\,],\\; [\\,0.30,\\,0.30\\,],\\; [\\,0.70,\\,0.70\\,],\\; [\\,0.80,\\,0.80\\,],\\; [\\,0.90,\\,0.90\\,] \\,\\big]$.\n    - Presence matrix $\\mathbf{A} \\in \\{0,1\\}^{3 \\times 6}$:\n        - Site $1$: $[\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]$.\n        - Site $2$: $[\\,1,\\,0,\\,0,\\,0,\\,0,\\,1\\,]$.\n        - Site $3$: $[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$.\n- **Null Model Parameters**: Number of replicates $R = 499$. Random seed is $123$.\n- **Output Requirement**: A single list of SES values for all sites, rounded to three decimals.\n\nStep 2: Validate Using Extracted Givens.\nThe problem statement is examined for validity.\n- **Scientifically Grounded**: The problem utilizes established concepts and metrics from community ecology and functional biogeography, namely functional dispersion (FDis) and null model testing. The definitions are consistent with primary literature (e.g., Laliberté & Legendre, $2010$, Ecology). The ecological hypotheses being tested (environmental filtering vs. limiting similarity) are central to the field. The problem is scientifically sound.\n- **Well-Posed**: The problem is specified with mathematical precision. All necessary data (trait and community matrices), parameters ($R$, random seed), and explicit formulas for all calculations (centroid, FDis, SES) are provided. The specification of a fixed random seed ensures a unique, reproducible numerical result. The problem is well-posed.\n- **Objective**: The problem is formulated in objective, mathematical language. It is free of subjectivity or ambiguity. Contextual information (environmental gradient values) is clearly marked as not being part of the computation.\n- **Other Flaws**: The problem is self-contained, internally consistent, and requires non-trivial computation, thus it is not a tautology. Its parameters and data are realistic for ecological studies.\n\nStep 3: Verdict and Action.\nThe problem is valid. A reasoned solution is now developed.\n\nThe objective is to compute the Standardized Effect Size ($\\mathrm{SES}$) of Functional Dispersion ($\\mathrm{FDis}$) for several ecological communities. This requires a three-stage process for each community: ($1$) compute the observed $\\mathrm{FDis}$, ($2$) generate a null distribution of $\\mathrm{FDis}$ values, and ($3$) compute the $\\mathrm{SES}$ from the observed value and the null distribution.\n\nStage 1: Computation of Observed Functional Dispersion ($\\mathrm{FDis}_{\\mathrm{obs}}$).\nFor a given community at a specific site, defined by a presence-absence vector $\\mathbf{a}$, we first identify the set of present species, $I = \\{i \\mid a_i = 1\\}$, and their corresponding trait vectors $\\{\\mathbf{t}_i\\}_{i \\in I}$. The number of present species is the richness, $m = |I|$.\nIf $m \\le 1$, the dispersion is axiomatically zero, so $\\mathrm{FDis} = 0$. For $m > 1$, we proceed.\nFirst, the centroid of the community's trait space is computed. Since the weights $w_i$ are all $1$, the centroid $\\mathbf{c}$ is the arithmetic mean of the trait vectors of the present species:\n$$ \\mathbf{c} = \\frac{1}{m} \\sum_{i \\in I} \\mathbf{t}_i $$\nNext, for each present species $i \\in I$, the Euclidean distance $d_i$ from its trait vector $\\mathbf{t}_i$ to the centroid $\\mathbf{c}$ is calculated:\n$$ d_i = \\lVert \\mathbf{t}_i - \\mathbf{c} \\rVert_2 = \\sqrt{\\sum_{k=1}^D (t_{ik} - c_k)^2} $$\nwhere $D$ is the number of traits (the dimension of the space).\nFinally, the Functional Dispersion, $\\mathrm{FDis}$, is the arithmetic mean of these distances. This value, calculated using the original, non-permuted trait data, is the observed functional dispersion, $\\mathrm{FDis}_{\\mathrm{obs}}$.\n$$ \\mathrm{FDis}_{\\mathrm{obs}} = \\frac{1}{m} \\sum_{i \\in I} d_i $$\nThis procedure is applied to each site in both Case A and Case B.\n\nStage 2: Generation of the Null Distribution.\nThe null hypothesis posits that the observed community assembly is random with respect to species' traits. The chosen null model operationalizes this by randomizing the association between species identities and their trait vectors. This is repeated $R$ times to create a distribution of $\\mathrm{FDis}$ values expected under randomness.\nFor each replicate $r$ from $1$ to $R = 499$:\n1. A random permutation $\\pi$ of the species indices $\\{1, \\dots, S\\}$ is generated. A fixed random seed of $123$ ensures this sequence of permutations is identical for every execution.\n2. This permutation is applied to the rows of the global species-trait matrix $\\mathbf{T}$, creating a permuted trait matrix $\\mathbf{T}^{(r)}$. Thus, species $i$ is now assigned the trait vector that originally belonged to species $\\pi(i)$.\n3. For each site, the functional dispersion, $\\mathrm{FDis}^{(r)}$, is re-calculated using the original, fixed presence-absence vector $\\mathbf{a}$ but with the permuted traits from $\\mathbf{T}^{(r)}$.\nThis process populates a vector of $R$ null $\\mathrm{FDis}$ values for each site: $\\{\\mathrm{FDis}^{(1)}, \\mathrm{FDis}^{(2)}, \\dots, \\mathrm{FDis}^{(R)}\\}$.\n\nStage 3: Computation of Standardized Effect Size ($\\mathrm{SES}$).\nThe $\\mathrm{SES}$ quantifies the deviation of the observed $\\mathrm{FDis}_{\\mathrm{obs}}$ from the null expectation in units of standard deviation.\nFor each site, we first compute the mean, $\\mu_{\\mathrm{null}}$, and the standard deviation, $\\sigma_{\\mathrm{null}}$, of the null distribution generated in Stage 2:\n$$ \\mu_{\\mathrm{null}} = \\frac{1}{R} \\sum_{r=1}^R \\mathrm{FDis}^{(r)} $$\n$$ \\sigma_{\\mathrm{null}} = \\sqrt{\\frac{1}{R-1} \\sum_{r=1}^R (\\mathrm{FDis}^{(r)} - \\mu_{\\mathrm{null}})^2} $$\nThe sample standard deviation (with $R-1$ in the denominator) is appropriate here.\nThe $\\mathrm{SES}$ is then calculated as:\n$$ \\mathrm{SES} = \\frac{\\mathrm{FDis}_{\\mathrm{obs}} - \\mu_{\\mathrm{null}}}{\\sigma_{\\mathrm{null}}} $$\nA special condition is defined for cases where $\\sigma_{\\mathrm{null}} = 0$. This occurs if all null replicates produce the same $\\mathrm{FDis}$ value, which is expected for communities with richness $m \\le 1$. In such a case, the $\\mathrm{SES}$ is defined as $0$ to prevent division by zero.\nA negative $\\mathrm{SES}$ indicates that the observed traits are more clustered than expected by chance ($\\mathrm{FDis}_{\\mathrm{obs}} < \\mu_{\\mathrm{null}}$), which is interpreted as evidence for environmental filtering. A positive $\\mathrm{SES}$ indicates traits are more dispersed than expected by chance ($\\mathrm{FDis}_{\\mathrm{obs}} > \\mu_{\\mathrm{null}}$), interpreted as evidence for limiting similarity.\nThe final output is a list of these $\\mathrm{SES}$ values, rounded to three decimal places, for all $5$ sites in Case A followed by all $3$ sites in Case B.\n\nThe implementation will strictly follow these stages. For reproducibility, a `numpy` random number generator will be initialized with the specified seed, $123$. The entire process is deterministic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the standardized effect size of functional dispersion\n    for two test cases based on a trait-label randomization null model.\n    \"\"\"\n\n    # --- Define Test Cases ---\n\n    # Case A (univariate traits, D=1, S=8, 5 sites)\n    traits_A = np.array([0.05, 0.12, 0.20, 0.50, 0.60, 0.80, 0.92, 0.97]).reshape(-1, 1)\n    presences_A = np.array([\n        [1, 1, 1, 0, 0, 0, 0, 0],  # Site 1\n        [0, 1, 0, 1, 1, 0, 0, 0],  # Site 2\n        [0, 0, 0, 0, 0, 1, 1, 1],  # Site 3\n        [1, 0, 0, 1, 0, 0, 0, 1],  # Site 4\n        [0, 0, 0, 1, 0, 0, 0, 0],  # Site 5\n    ])\n\n    # Case B (bivariate traits, D=2, S=6, 3 sites)\n    traits_B = np.array([\n        [0.10, 0.10], [0.20, 0.20], [0.30, 0.30],\n        [0.70, 0.70], [0.80, 0.80], [0.90, 0.90]\n    ])\n    presences_B = np.array([\n        [1, 1, 1, 0, 0, 0],  # Site 1\n        [1, 0, 0, 0, 0, 1],  # Site 2\n        [0, 1, 0, 1, 0, 1],  # Site 3\n    ])\n\n    test_cases = [\n        (traits_A, presences_A),\n        (traits_B, presences_B),\n    ]\n\n    # --- Null Model Settings ---\n    R = 499\n    RANDOM_SEED = 123\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # --- Main Calculation Logic ---\n\n    def calculate_fdis(traits, presence_vector):\n        \"\"\"\n        Calculates Functional Dispersion (FDis) for a single community.\n        \"\"\"\n        present_indices = np.where(presence_vector == 1)[0]\n        richness = len(present_indices)\n\n        if richness = 1:\n            return 0.0\n\n        present_traits = traits[present_indices]\n        \n        # Calculate centroid (community-weighted mean trait)\n        centroid = np.mean(present_traits, axis=0)\n\n        # Calculate Euclidean distances to the centroid\n        distances = np.linalg.norm(present_traits - centroid, axis=1)\n\n        # FDis is the mean distance to the centroid\n        fdis = np.mean(distances)\n        \n        return fdis\n\n    all_ses_results = []\n\n    for traits, presences_matrix in test_cases:\n        num_species = traits.shape[0]\n        \n        for site_presence_vector in presences_matrix:\n            # 1. Calculate observed FDis\n            fdis_obs = calculate_fdis(traits, site_presence_vector)\n\n            # 2. Generate null distribution of FDis\n            fdis_null = np.zeros(R)\n            for r in range(R):\n                permuted_indices = rng.permutation(num_species)\n                permuted_traits = traits[permuted_indices]\n                fdis_null[r] = calculate_fdis(permuted_traits, site_presence_vector)\n            \n            # 3. Calculate SES\n            mu_null = np.mean(fdis_null)\n            # Use ddof=1 for sample standard deviation, though with R=499 difference is negligible.\n            # R-1 is specified in the solution text.\n            sigma_null = np.std(fdis_null, ddof=1)\n\n            if sigma_null == 0.0:\n                ses = 0.0\n            else:\n                ses = (fdis_obs - mu_null) / sigma_null\n\n            all_ses_results.append(round(ses, 3))\n\n    # --- Final Output Formatting ---\n    # The final print statement must produce only the specified single-line format.\n    formatted_results = [f\"{res:.3f}\" for res in all_ses_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n```", "id": "2472458"}, {"introduction": "The ultimate goal of studying biodiversity is often to inform its conservation. This exercise tackles a central problem in conservation planning: designing a network of reserves to protect species efficiently under a limited budget. You will implement both a rapid, heuristic greedy algorithm and a formally optimal integer programming solution to experience the trade-offs between computational speed and solution quality in real-world conservation decision-making. [@problem_id:2472463]", "problem": "You are given species-by-site presence data, site costs, and species-specific representation targets. Your task is to implement and evaluate two approaches to a minimum-cost reserve selection problem from conservation planning. The problem is to select a subset of sites that achieves representation targets for all species at minimal cost. Formally, let there be $m$ candidate sites and $n$ species. Let $P \\in \\{0,1\\}^{m \\times n}$ be the presence matrix with $P_{i j} = 1$ if site $i$ contains species $j$, and $P_{i j} = 0$ otherwise. Let $c \\in \\mathbb{R}_{0}^{m}$ be the vector of site costs. Let $t \\in \\mathbb{Z}_{\\ge 0}^{n}$ be the vector of representation targets where $t_j$ specifies the required number of distinct sites that must be selected in which species $j$ is present.\n\nYour program must implement:\n- A greedy complementarity algorithm that iteratively selects sites. Let $S$ be the current set of selected sites and $r \\in \\mathbb{Z}_{\\ge 0}^{n}$ be the current coverage counts with $r_j = \\sum_{i \\in S} P_{i j}$. In each iteration, compute for each unselected site $i$ the marginal gain $g_i = \\sum_{j=1}^{n} \\mathbf{1}\\{r_j  t_j\\} \\cdot P_{i j}$. Compute the efficiency score $e_i = g_i / c_i$. Select the site with maximum $e_i$. Break ties by selecting the smallest site index. Stop when either all targets are met (that is, $r_j \\ge t_j$ for all $j$) or no site has $g_i  0$, in which case the instance is infeasible by greedy reasoning.\n- An optimal solution via mixed-integer linear programming (MILP). Introduce binary decision variables $x_i \\in \\{0,1\\}$ for $i \\in \\{1,\\dots,m\\}$ with objective to minimize $\\sum_{i=1}^{m} c_i x_i$ subject to coverage constraints $\\sum_{i=1}^{m} P_{i j} x_i \\ge t_j$ for all $j \\in \\{1,\\dots,n\\}$.\n\nBase your logic on the fundamental definition of biodiversity representation as meeting species targets and on the well-known minimum-cost set multicover formulation. Do not assume any unstated shortcuts.\n\nFor each test case, return a list containing $5$ values:\n- The greedy total cost as a float. If the greedy method cannot meet all targets, return $-1.0$ for the cost.\n- The optimal total cost from MILP as a float. If the MILP detects infeasibility, return $-1.0$ for the cost.\n- A greedy feasibility indicator encoded as an integer $1$ if the greedy solution meets all targets, otherwise $0$.\n- An optimal feasibility indicator encoded as an integer $1$ if the MILP finds a feasible solution, otherwise $0$.\n- The greedy-to-optimal cost ratio rounded to three decimals, computed as $(\\text{greedy cost})/(\\text{optimal cost})$ if both methods are feasible, and $-1.0$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is the list described above for a test case. No other text should be printed.\n\nUse the following test suite, which consists of $4$ cases designed to test general performance, tie-breaking behavior, target multiplicity, and infeasibility.\n\n- Case A (general, greedy suboptimal):\n  - $P = [[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1], [1, 0, 0, 1]]$\n  - $c = [3, 2, 2, 4, 3]$\n  - $t = [1, 1, 1, 1]$\n- Case B (tie-breaking determinism):\n  - $P = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]$\n  - $c = [1, 1, 1]$\n  - $t = [1, 1, 1]$\n- Case C (higher targets, multiple coverage required):\n  - $P = [[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1], [0, 1, 1], [1, 0, 1]]$\n  - $c = [2, 3, 2, 2, 3, 2]$\n  - $t = [2, 2, 2]$\n- Case D (infeasible due to a missing species):\n  - $P = [[1, 0], [0, 0], [1, 0]]$\n  - $c = [1, 2, 1]$\n  - $t = [1, 1]$\n\nAll computations are unitless counts and costs; no physical units are required. For any ratio, you must round to three decimals as specified. Angles are not involved. The final output must be a single line with a bracketed, comma-separated list of per-case results, with each per-case result itself a bracketed list in the order specified above.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Task**: Implement and evaluate a greedy complementarity algorithm and a Mixed-Integer Linear Programming (MILP) model for the minimum-cost reserve selection problem.\n- **Inputs**:\n    - $m$: number of candidate sites.\n    - $n$: number of species.\n    - $P \\in \\{0,1\\}^{m \\times n}$: A presence matrix where $P_{ij} = 1$ if site $i$ contains species $j$.\n    - $c \\in \\mathbb{R}_{0}^{m}$: A vector of positive costs for each site.\n    - $t \\in \\mathbb{Z}_{\\ge 0}^{n}$: A vector of integer representation targets for each species.\n- **Greedy Algorithm Definition**:\n    - Let $S$ be the set of selected sites, initially empty.\n    - Let $r \\in \\mathbb{Z}_{\\ge 0}^{n}$ be current coverage, $r_j = \\sum_{i \\in S} P_{ij}$.\n    - In each iteration, for each unselected site $i$:\n        1.  Compute marginal gain: $g_i = \\sum_{j=1}^{n} \\mathbf{1}\\{r_j  t_j\\} \\cdot P_{ij}$.\n        2.  Compute efficiency score: $e_i = g_i / c_i$.\n    - Select the site with the maximum efficiency score $e_i$.\n    - Break ties by selecting the site with the smallest index.\n    - Stop when all targets are met ($r_j \\ge t_j$ for all $j$) or when no unselected site has a marginal gain $g_i  0$. If targets are not met in the latter case, the solution is deemed infeasible by this heuristic.\n- **MILP Formulation**:\n    - Decision variables: $x_i \\in \\{0,1\\}$ for $i \\in \\{1, \\dots, m\\}$, where $x_i=1$ if site $i$ is selected.\n    - Objective function: Minimize $\\sum_{i=1}^{m} c_i x_i$.\n    - Constraints: $\\sum_{i=1}^{m} P_{ij} x_i \\ge t_j$ for all species $j \\in \\{1, \\dots, n\\}$.\n- **Output Specification**:\n    - For each test case, return a list of 5 values:\n        1. Greedy total cost (float, $-1.0$ if infeasible).\n        2. Optimal MILP total cost (float, $-1.0$ if infeasible).\n        3. Greedy feasibility indicator (integer, $1$ if feasible, $0$ otherwise).\n        4. Optimal feasibility indicator (integer, $1$ if feasible, $0$ otherwise).\n        5. Greedy-to-optimal cost ratio, rounded to three decimals. This is computed as $(\\text{greedy cost})/(\\text{optimal cost})$ if both solutions are feasible, and $-1.0$ otherwise.\n- **Test Cases**: Four cases (A, B, C, D) are provided with specific matrices $P$ and vectors $c$ and $t$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem describes the set multicover problem, a canonical formulation in conservation planning and operations research. The concepts are standard in ecology and computational biology. The problem is scientifically sound.\n- **Well-Posed**: The objectives and constraints for both the greedy heuristic and the optimal MILP method are unambiguously defined with clear mathematical formulations. Input data for all test cases are fully specified. The tie-breaking rule ensures the greedy algorithm is deterministic. The problem is well-posed.\n- **Objective**: The problem is stated in precise mathematical language, free of subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data ($P, c, t$) and algorithmic rules are provided. There are no internal contradictions. An infeasible test case is included by design to test the algorithms' handling of such scenarios, which is a valid part of a problem specification.\n- **Other Flaws**: The problem is not metaphorical, trivial, unverifiable, or poorly structured. It is a standard, non-trivial optimization problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be developed.\n\n### Solution Derivation\n\nThe task is to solve the minimum-cost reserve selection problem using two methods: a greedy heuristic and an optimal solver based on Mixed-Integer Linear Programming (MILP). The problem is a variant of the set cover problem, specifically the set multicover problem, which is known to be NP-hard.\n\n#### 1. Greedy Complementarity Algorithm\n\nThe greedy algorithm is an iterative heuristic that builds a solution by making locally optimal choices. At each step, it selects the most \"efficient\" site.\n\nLet $m$ be the number of sites and $n$ be the number of species. The inputs are the presence-absence matrix $P \\in \\{0, 1\\}^{m \\times n}$, the site cost vector $c \\in \\mathbb{R}_{0}^{m}$, and the species target vector $t \\in \\mathbb{Z}_{\\ge 0}^{n}$.\n\nThe algorithm proceeds as follows:\n1.  Initialize the set of selected sites $S$ to be empty, $S = \\emptyset$.\n2.  Initialize the total cost $C_{greedy} = 0$.\n3.  Initialize the current coverage vector $r \\in \\mathbb{Z}_{\\ge 0}^{n}$ to all zeros.\n4.  Maintain a set of unselected sites, initially all sites.\n5.  Begin the iterative selection process. In each iteration:\n    a. Check for termination: If $r_j \\ge t_j$ for all species $j \\in \\{1, \\dots, n\\}$, the targets are met. The algorithm terminates successfully.\n    b. For each unselected site $i$, calculate its marginal gain, $g_i$. This gain is the number of new species-target pairs the site contributes to satisfying. A site contributes to a species $j$ if that species is present at the site ($P_{ij} = 1$) and its target has not yet been met ($r_j  t_j$). Mathematically:\n    $$ g_i = \\sum_{j=1}^{n} \\mathbf{1}\\{r_j  t_j\\} \\cdot P_{ij} $$\n    where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n    c. If the maximum marginal gain over all unselected sites is $0$ (i.e., $\\max_{i \\notin S} g_i = 0$), no further progress can be made. If targets are not met, the problem is declared infeasible by this algorithm. The cost is set to $-1.0$ and the process terminates.\n    d. For each unselected site $i$ with $g_i  0$, calculate its efficiency score $e_i$:\n    $$ e_i = \\frac{g_i}{c_i} $$\n    e. Identify the site $i^*$ that has the maximum efficiency. If there is a tie, select the site with the smallest index.\n    $$ i^* = \\min \\{ i \\mid i \\notin S \\text{ and } e_i = \\max_{k \\notin S} e_k \\} $$\n    f. Add site $i^*$ to the solution: update $S \\leftarrow S \\cup \\{i^*\\}$, add its cost to the total $C_{greedy} \\leftarrow C_{greedy} + c_{i^*}$, and update the coverage vector $r \\leftarrow r + P_{i^*, \\cdot}$, where $P_{i^*, \\cdot}$ is the $i^*$-th row of matrix $P$.\n6.  Once terminated, the final cost $C_{greedy}$ and a feasibility flag are reported.\n\n#### 2. Optimal Solution via MILP\n\nThe problem can be formulated as a Mixed-Integer Linear Program to find a globally optimal solution. This guarantees the minimum possible cost if a feasible solution exists.\n\nLet $x_i \\in \\{0, 1\\}$ be a binary decision variable for each site $i \\in \\{1, \\dots, m\\}$, where $x_i = 1$ if site $i$ is selected and $x_i = 0$ otherwise.\n\nThe objective is to minimize the total cost of selected sites:\n$$ \\text{Minimize } Z = \\sum_{i=1}^{m} c_i x_i $$\n\nThis objective is subject to constraints ensuring that representation targets for all species are met. For each species $j \\in \\{1, \\dots, n\\}$, the sum of occurrences in the selected sites must be at least its target $t_j$:\n$$ \\sum_{i=1}^{m} P_{ij} x_i \\ge t_j \\quad \\forall j \\in \\{1, \\dots, n\\} $$\n\nThis formulation will be solved using the `scipy.optimize.milp` function. The inputs to this function are:\n- The objective function coefficient vector, which is simply the cost vector $c$.\n- The integrality constraint, specifying that all $x_i$ variables are integers.\n- The bounds for each variable, $(0, 1)$, making them binary.\n- The set of linear constraints. The inequality $\\sum_{i=1}^{m} P_{ij} x_i \\ge t_j$ can be written in matrix form as $P^T x \\ge t$. This is supplied to the solver.\n\nThe solver will return an optimal solution if one exists. The `success` attribute of the result object indicates feasibility. If `success` is true, the optimal cost is given by `result.fun`. If `success` is false, no feasible solution exists, and the cost is reported as $-1.0$.\n\n#### 3. Output Calculation\n\nFor each test case, the program will execute both algorithms and compile the five required output values:\n1.  `greedy_cost`: The total cost from the greedy algorithm, or $-1.0$.\n2.  `optimal_cost`: The minimum cost from the MILP solver, or $-1.0$.\n3.  `greedy_feasible`: $1$ if the greedy algorithm met all targets, $0$ otherwise.\n4.  `optimal_feasible`: $1$ if the MILP solver found a feasible solution, $0$ otherwise.\n5.  `ratio`: Computed as `greedy_cost / optimal_cost` if both are feasible. The result is rounded to three decimal places. Otherwise, the ratio is $-1.0$.\n\nThe final output is a single string containing a list of these result lists, one for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import milp, LinearConstraint, Bounds\n\ndef solve_greedy(P, c, t):\n    \"\"\"\n    Solves the reserve selection problem using a greedy complementarity algorithm.\n    \"\"\"\n    num_sites, num_species = P.shape\n    \n    selected_sites_mask = np.zeros(num_sites, dtype=bool)\n    current_coverage = np.zeros(num_species, dtype=int)\n    total_cost = 0.0\n    \n    while True:\n        # Check if all targets are met\n        if np.all(current_coverage >= t):\n            return total_cost, 1\n\n        unselected_indices = np.where(~selected_sites_mask)[0]\n        \n        # If no sites are left to select but targets are not met\n        if len(unselected_indices) == 0:\n            return -1.0, 0\n\n        unmet_species_mask = current_coverage  t\n        \n        gains = np.zeros(num_sites)\n        \n        # Calculate marginal gain for unselected sites\n        for i in unselected_indices:\n            gains[i] = np.sum(P[i, unmet_species_mask])\n\n        # If no site can contribute to unmet targets, the problem is infeasible\n        if np.max(gains) == 0:\n            return -1.0, 0\n            \n        efficiencies = np.full(num_sites, -1.0)\n        \n        # Calculate efficiency for sites with non-zero gain\n        positive_gain_mask = (gains > 0)\n        valid_indices_for_eff = np.where(positive_gain_mask)[0]\n        \n        if len(valid_indices_for_eff) > 0:\n             efficiencies[valid_indices_for_eff] = gains[valid_indices_for_eff] / c[valid_indices_for_eff]\n\n        max_efficiency = np.max(efficiencies)\n        \n        # Find all sites with max efficiency and select the one with the smallest index\n        best_site_candidates = np.where(efficiencies == max_efficiency)[0]\n        best_site_to_select = np.min(best_site_candidates)\n        \n        # Update solution\n        selected_sites_mask[best_site_to_select] = True\n        total_cost += c[best_site_to_select]\n        current_coverage += P[best_site_to_select, :]\n\ndef solve_milp(P, c, t):\n    \"\"\"\n    Solves the reserve selection problem optimally using MILP.\n    \"\"\"\n    num_sites, num_species = P.shape\n\n    # Objective function: minimize sum(c_i * x_i)\n    c_obj = c\n\n    # Variables are binary: x_i in {0, 1}\n    integrality = np.ones(num_sites, dtype=int)\n    bounds = Bounds(0, 1)\n\n    # Pre-check for infeasibility: if a species has a target > 0 but is not in any site\n    if np.any((t > 0)  (np.sum(P, axis=0) == 0)):\n        return -1.0, 0\n\n    # Constraints: sum(P_ij * x_i) >= t_j for each species j\n    # This is equivalent to P.T @ x >= t\n    constraints = LinearConstraint(A=P.T, lb=t, ub=np.inf)\n\n    # Solve the MILP\n    res = milp(c=c_obj, constraints=constraints, integrality=integrality, bounds=bounds)\n\n    if res.success:\n        return float(res.fun), 1\n    else:\n        return -1.0, 0\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        { # Case A: General, greedy suboptimal\n            \"P\": np.array([[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1], [1, 0, 0, 1]]),\n            \"c\": np.array([3, 2, 2, 4, 3]),\n            \"t\": np.array([1, 1, 1, 1])\n        },\n        { # Case B: Tie-breaking determinism\n            \"P\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"c\": np.array([1, 1, 1]),\n            \"t\": np.array([1, 1, 1])\n        },\n        { # Case C: Higher targets\n            \"P\": np.array([[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1], [0, 1, 1], [1, 0, 1]]),\n            \"c\": np.array([2, 3, 2, 2, 3, 2]),\n            \"t\": np.array([2, 2, 2])\n        },\n        { # Case D: Infeasible\n            \"P\": np.array([[1, 0], [0, 0], [1, 0]]),\n            \"c\": np.array([1, 2, 1]),\n            \"t\": np.array([1, 1])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        P, c, t = case[\"P\"], case[\"c\"], case[\"t\"]\n        \n        g_cost, g_feas = solve_greedy(P, c, t)\n        o_cost, o_feas = solve_milp(P, c, t)\n\n        if g_feas == 1 and o_feas == 1:\n            ratio = g_cost / o_cost\n        else:\n            ratio = -1.0\n\n        # Format output strings\n        g_cost_str = f\"{g_cost:.1f}\"\n        o_cost_str = f\"{o_cost:.1f}\"\n        \n        if ratio != -1.0:\n            ratio_str = f\"{ratio:.3f}\"\n        else:\n            ratio_str = f\"{ratio:.1f}\"\n            \n        result_str = f\"[{g_cost_str},{o_cost_str},{g_feas},{o_feas},{ratio_str}]\"\n        all_results.append(result_str)\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2472463"}]}