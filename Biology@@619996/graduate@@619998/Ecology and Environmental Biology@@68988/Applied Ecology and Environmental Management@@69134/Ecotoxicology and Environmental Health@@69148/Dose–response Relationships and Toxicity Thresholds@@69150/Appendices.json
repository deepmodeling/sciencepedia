{"hands_on_practices": [{"introduction": "Effective experimental design is the cornerstone of rigorous scientific inquiry. This practice delves into statistical power analysis, a critical tool used to determine the necessary sample size and experimental configuration to reliably detect a hypothesized effect. By working through this exercise [@problem_id:2481284], you will learn how to quantify the trade-offs between dose placement, replication, and measurement variance, ensuring your experiments are both efficient and capable of producing conclusive results for estimating the half-maximal effective concentration ($EC_{50}$).", "problem": "You are tasked with constructing a program that performs a design-based power analysis to detect a two-fold difference in Effective Concentration 50 ($EC_{50}$) between two treatments in a normalized dose–response experiment, under a set of specified assumptions. Your approach must start from a fundamental base in statistical modeling and information theory and proceed to a fully specified algorithm suitable for calculation.\n\nAssumptions and model structure:\n- The biological response at dose $x$ in treatment $t \\in \\{\\mathrm{A},\\mathrm{B}\\}$ is modeled as\n$$\nY \\mid x,t \\sim \\mathcal{N}\\!\\big(\\mu(x;\\theta_t),\\,\\sigma^2\\big),\n$$\nwith independent and identically distributed residuals of constant variance $\\sigma^2$ and independent across treatments and doses. Responses are normalized to span from $0$ to $1$ so that no additional scale parameters are needed.\n- The mean function is a Hill-type dose–response with fixed top and bottom asymptotes:\n$$\n\\mu(x;\\theta) \\;=\\; \\frac{1}{1 + \\left(\\frac{x}{\\mathrm{EC}_{50}}\\right)^h} \\;=\\; \\frac{1}{1 + \\exp\\!\\big(h(\\ln x - \\theta)\\big)},\n$$\nwhere $\\theta = \\ln(\\mathrm{EC}_{50})$ and $h$ is a known Hill slope shared by both treatments. The top is fixed at $1$ and the bottom at $0$.\n- Treatment $\\mathrm{A}$ has parameter $\\theta_{\\mathrm{A}} = \\ln(\\mathrm{EC}_{50,\\mathrm{A}})$, and treatment $\\mathrm{B}$ has $\\theta_{\\mathrm{B}} = \\theta_{\\mathrm{A}} + \\ln f$, where $f$ is the fold-change in $EC_{50}$. The null hypothesis is $H_0: \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}} = 0$, and the alternative is $H_1: \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}} = \\ln f$. You must compute power under $H_1$ for a two-sided Wald test at significance level $\\alpha$.\n- Design is specified by a set of doses $\\{x_i\\}_{i=1}^m$ with $r_i$ independent replicates at each dose in each treatment. Assume the same allocation $\\{(x_i, r_i)\\}$ is used in both treatments.\n\nFundamental base for derivation:\n- For a nonlinear mean function under a Gaussian model with known variance, the per-observation Fisher information for a scalar parameter $\\theta$ is\n$$\n\\mathcal{I}(\\theta) \\;=\\; \\frac{1}{\\sigma^2} \\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^2.\n$$\n- For independent observations, Fisher information adds across observations and across dose groups (via replicate counts). In large samples, the maximum likelihood estimator has variance approximately equal to the inverse Fisher information.\n- A Wald statistic for testing $H_0: \\delta = 0$ with estimator $\\widehat{\\delta}$ satisfies\n$$\nZ \\;=\\; \\frac{\\widehat{\\delta}}{\\mathrm{SE}(\\widehat{\\delta})} \\;\\approx\\; \\mathcal{N}\\!\\big(\\delta/\\mathrm{SE}(\\widehat{\\delta}),\\,1\\big),\n$$\nso that two-sided power at level $\\alpha$ under true $\\delta \\neq 0$ is\n$$\n\\text{Power} \\;=\\; \\Pr\\big(|Z| > z_{1-\\alpha/2}\\big) \\;=\\; 1 - \\Phi\\!\\big(z_{1-\\alpha/2} - \\lambda\\big) + \\Phi\\!\\big(-z_{1-\\alpha/2} - \\lambda\\big),\n$$\nwhere $\\lambda = |\\delta|/\\mathrm{SE}(\\widehat{\\delta})$ is the noncentrality parameter and $\\Phi$ is the standard normal cumulative distribution function.\n\nRequired derivation and algorithmic steps to implement:\n1. Using the specified mean function, derive $\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}$ and express the total Fisher information for $\\theta$ for a given treatment in terms of $\\{x_i,r_i\\}$, $h$, and $\\sigma^2$.\n2. Express the asymptotic variance of $\\widehat{\\theta}_t$ for each treatment $t \\in \\{\\mathrm{A},\\mathrm{B}\\}$ as the inverse of its Fisher information evaluated at the true $\\theta_t$ under $H_1$.\n3. Since the two treatments are measured independently under the given assumptions, the variance of the difference estimator $\\widehat{\\delta} = \\widehat{\\theta}_{\\mathrm{B}} - \\widehat{\\theta}_{\\mathrm{A}}$ is the sum of the two variances. Thus,\n$$\n\\mathrm{SE}(\\widehat{\\delta}) \\;=\\; \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})}.\n$$\n4. Compute the noncentrality parameter $\\lambda = |\\ln f| / \\mathrm{SE}(\\widehat{\\delta})$, the critical value $z_{1-\\alpha/2}$, and finally the power using the two-sided normal formula above.\n\nAngle units are not applicable. If any quantity is to be expressed as a percentage, you must instead express it as a decimal fraction. Your program must output each power value as a decimal fraction in $[0,1]$, rounded to four decimal places.\n\nTest suite:\nFor each case below, compute the two-sided power to detect a two-fold shift, that is, use $f=2$.\n\n- Case 1 (balanced, informative, moderate variance):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{1,\\,3,\\,10,\\,30,\\,100\\}$\n  - Replicates per dose per treatment: $\\{6,\\,6,\\,6,\\,6,\\,6\\}$\n  - Residual standard deviation: $\\sigma = 0.1$\n  - Hill slope: $h = 1.2$\n  - Baseline $EC_{50,A} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 2 (doses far below the baseline $EC_{50}$, lower information):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{0.01,\\,0.03,\\,0.1,\\,0.3,\\,1\\}$\n  - Replicates per dose per treatment: $\\{4,\\,4,\\,4,\\,4,\\,4\\}$\n  - Residual standard deviation: $\\sigma = 0.15$\n  - Hill slope: $h = 1.2$\n  - Baseline $EC_{50,A} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 3 (highly informative near $EC_{50}$, small variance):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{6,\\,8,\\,10,\\,12,\\,15\\}$\n  - Replicates per dose per treatment: $\\{20,\\,20,\\,20,\\,20,\\,20\\}$\n  - Residual standard deviation: $\\sigma = 0.05$\n  - Hill slope: $h = 2.0$\n  - Baseline $EC_{50,A} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 4 (sparse design, minimal replication):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{5,\\,10,\\,20\\}$\n  - Replicates per dose per treatment: $\\{1,\\,1,\\,1\\}$\n  - Residual standard deviation: $\\sigma = 0.1$\n  - Hill slope: $h = 1.0$\n  - Baseline $EC_{50,A} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\nFinal output format:\n- Your program must produce a single line of output containing the four power values corresponding to the four cases, ordered as above, as a comma-separated list enclosed in square brackets, for example, $[\\;0.8021,\\,0.0543,\\,0.9999,\\,0.4120\\;]$. Each value must be rounded to four decimal places and expressed as a decimal fraction (no percentage sign).", "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in established statistical theory for nonlinear regression and power analysis. The model, assumptions, and required derivations are clearly defined, objective, and self-contained. The problem is well-posed, and all necessary parameters for the specified test cases are provided. No inconsistencies, ambiguities, or factual unsoundness are present. We may therefore proceed with the solution.\n\nThe task is to compute the statistical power to detect a two-fold difference in the $\\mathrm{EC}_{50}$ parameter between two treatments, A and B, in a dose-response experiment. The derivation and computation will follow the structured approach based on large-sample properties of maximum likelihood estimators and the Fisher information matrix.\n\nThe biological response $Y$ at a given dose $x$ for a treatment $t$ is modeled by a normal distribution $Y \\mid x,t \\sim \\mathcal{N}(\\mu(x;\\theta_t), \\sigma^2)$, where the mean response function $\\mu(x;\\theta)$ follows a four-parameter logistic model (Hill equation) with fixed top and bottom asymptotes of $1$ and $0$, respectively. The mean function is given by:\n$$\n\\mu(x;\\theta) = \\frac{1}{1 + \\exp(h(\\ln x - \\theta))}\n$$\nHere, $\\theta = \\ln(\\mathrm{EC}_{50})$ is the parameter of interest representing the natural logarithm of the effective concentration giving $50\\%$ response, and $h$ is the Hill slope, assumed known and constant across treatments.\n\nThe analysis proceeds in four steps as prescribed.\n\n**Step 1: Derivation of the Partial Derivative and Fisher Information**\n\nFirst, we derive the partial derivative of the mean function $\\mu(x;\\theta)$ with respect to the parameter $\\theta$. This derivative is essential for computing the Fisher information. Using the chain rule, let $u(x;\\theta) = h(\\ln x - \\theta)$. Then $\\mu = (1 + e^u)^{-1}$.\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = -\\frac{1}{(1 + e^u)^2} \\cdot \\frac{\\partial}{\\partial \\theta}(e^u) = -\\frac{e^u}{(1 + e^u)^2} \\cdot \\frac{\\partial u}{\\partial \\theta}\n$$\nThe derivative of $u$ with respect to $\\theta$ is:\n$$\n\\frac{\\partial u}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(h(\\ln x - \\theta)) = -h\n$$\nSubstituting this back, we obtain:\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = -\\frac{\\exp(h(\\ln x - \\theta))}{(1 + \\exp(h(\\ln x - \\theta)))^2} \\cdot (-h) = \\frac{h \\exp(h(\\ln x - \\theta))}{(1 + \\exp(h(\\ln x - \\theta)))^2}\n$$\nThis expression can be elegantly simplified by observing that $\\mu(x;\\theta) = \\frac{1}{1 + e^u}$ and $1 - \\mu(x;\\theta) = \\frac{e^u}{1 + e^u}$. Therefore, the derivative is:\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = h \\cdot \\left(\\frac{1}{1+e^u}\\right) \\cdot \\left(\\frac{e^u}{1+e^u}\\right) = h \\cdot \\mu(x;\\theta) \\cdot (1 - \\mu(x;\\theta))\n$$\nThe per-observation Fisher information for $\\theta$ at a given dose $x$ is defined as $\\mathcal{I}(\\theta; x) = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^2$. Substituting our derivative, we get:\n$$\n\\mathcal{I}(\\theta; x) = \\frac{1}{\\sigma^2} \\left[ h \\cdot \\mu(x;\\theta)(1 - \\mu(x;\\theta)) \\right]^2 = \\frac{h^2}{\\sigma^2} [\\mu(x;\\theta)(1 - \\mu(x;\\theta))]^2\n$$\nFor an experimental design specified by a set of $m$ doses $\\{x_i\\}_{i=1}^m$ with $r_i$ replicates at each dose, the total Fisher information for one treatment is the sum over all independent observations:\n$$\n\\mathcal{I}_{\\text{total}}(\\theta) = \\sum_{i=1}^{m} r_i \\cdot \\mathcal{I}(\\theta; x_i) = \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta)(1 - \\mu(x_i;\\theta))]^2\n$$\n\n**Step 2: Asymptotic Variance of Parameter Estimators**\n\nAccording to large-sample theory, the variance of the maximum likelihood estimator $\\widehat{\\theta}$ is approximated by the inverse of the total Fisher information, $\\mathrm{Var}(\\widehat{\\theta}) \\approx [\\mathcal{I}_{\\text{total}}(\\theta)]^{-1}$. We compute this variance for each treatment, evaluated at the true parameter values under the alternative hypothesis, $H_1$.\n\nUnder $H_1$, the true parameters are $\\theta_{\\mathrm{A}} = \\ln(\\mathrm{EC}_{50,\\mathrm{A}})$ for treatment A and $\\theta_{\\mathrm{B}} = \\theta_{\\mathrm{A}} + \\ln f$ for treatment B, where $f$ is the fold-change, given as $f=2$.\n\nThe variance for the estimator of $\\theta_{\\mathrm{A}}$ is:\n$$\n\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) \\approx \\left( \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1 - \\mu(x_i;\\theta_{\\mathrm{A}}))]^2 \\right)^{-1}\n$$\nSimilarly, the variance for the estimator of $\\theta_{\\mathrm{B}}$ is:\n$$\n\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}}) \\approx \\left( \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1 - \\mu(x_i;\\theta_{\\mathrm{B}}))]^2 \\right)^{-1}\n$$\n\n**Step 3: Standard Error of the Difference**\n\nWe are testing the difference $\\delta = \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}}$. The estimator for this difference is $\\widehat{\\delta} = \\widehat{\\theta}_{\\mathrm{B}} - \\widehat{\\theta}_{\\mathrm{A}}$. Since the experiments for treatments A and B are independent, the variance of $\\widehat{\\delta}$ is the sum of the individual variances:\n$$\n\\mathrm{Var}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})\n$$\nThe standard error of the difference, $\\mathrm{SE}(\\widehat{\\delta})$, is the square root of this variance:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})}\n$$\nSubstituting the expressions for the variances, we have:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\sqrt{ \\frac{\\sigma^2}{h^2} \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1-\\mu(x_i;\\theta_{\\mathrm{A}}))]^2\\right)^{-1} + \\frac{\\sigma^2}{h^2} \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1-\\mu(x_i;\\theta_{\\mathrm{B}}))]^2\\right)^{-1} }\n$$\nThis simplifies to:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\frac{\\sigma}{h} \\sqrt{ \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1-\\mu(x_i;\\theta_{\\mathrm{A}}))]^2\\right)^{-1} + \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1-\\mu(x_i;\\theta_{\\mathrm{B}}))]^2\\right)^{-1} }\n$$\n\n**Step 4: Power Calculation**\n\nThe final step is to compute the power of the two-sided Wald test. The test statistic under $H_1$ is approximately normally distributed: $Z = \\frac{\\widehat{\\delta}}{\\mathrm{SE}(\\widehat{\\delta})} \\approx \\mathcal{N}(\\lambda, 1)$, where $\\lambda = \\frac{\\delta}{\\mathrm{SE}(\\widehat{\\delta})}$ is the noncentrality parameter. The true difference under $H_1$ is $\\delta = \\ln f$.\n$$\n\\lambda = \\frac{|\\ln f|}{\\mathrm{SE}(\\widehat{\\delta})}\n$$\nThe power of a two-sided test at significance level $\\alpha$ is the probability of rejecting the null hypothesis $H_0: \\delta=0$ when the alternative $H_1$ is true. This probability is:\n$$\n\\text{Power} = \\Pr\\big(|Z| > z_{1-\\alpha/2} \\mid H_1\\big)\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. This can be computed using the standard normal cumulative distribution function, $\\Phi$:\n$$\n\\text{Power} = 1 - \\Pr\\big(-z_{1-\\alpha/2} \\le Z \\le z_{1-\\alpha/2} \\mid H_1\\big) = 1 - (\\Phi(z_{1-\\alpha/2} - \\lambda) - \\Phi(-z_{1-\\alpha/2} - \\lambda))\n$$\nThis is the final formula for power. The implementation will follow these derived steps. For each test case, we will substitute the given values for $\\{x_i, r_i\\}$, $\\sigma$, $h$, $\\mathrm{EC}_{50,\\mathrm{A}}$, $f=2$, and $\\alpha=0.05$ to compute the power.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves for the statistical power in four dose-response experiment scenarios.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: balanced, informative, moderate variance\n        {\n            \"doses\": np.array([1.0, 3.0, 10.0, 30.0, 100.0]),\n            \"replicates\": np.array([6, 6, 6, 6, 6]),\n            \"sigma\": 0.1,\n            \"h\": 1.2,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 2: doses far below baseline EC50, lower information\n        {\n            \"doses\": np.array([0.01, 0.03, 0.1, 0.3, 1.0]),\n            \"replicates\": np.array([4, 4, 4, 4, 4]),\n            \"sigma\": 0.15,\n            \"h\": 1.2,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 3: highly informative near EC50, small variance\n        {\n            \"doses\": np.array([6.0, 8.0, 10.0, 12.0, 15.0]),\n            \"replicates\": np.array([20, 20, 20, 20, 20]),\n            \"sigma\": 0.05,\n            \"h\": 2.0,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 4: sparse design, minimal replication\n        {\n            \"doses\": np.array([5.0, 10.0, 20.0]),\n            \"replicates\": np.array([1, 1, 1]),\n            \"sigma\": 0.1,\n            \"h\": 1.0,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        }\n    ]\n\n    def calculate_power(doses, replicates, sigma, h, ec50_a, f, alpha):\n        \"\"\"\n        Calculates statistical power based on Fisher information for a given design.\n        \"\"\"\n        # Step 1: Define parameters and mean function\n        theta_a = np.log(ec50_a)\n        theta_b = theta_a + np.log(f)\n\n        def mean_response(x, theta):\n            # Hill-type dose-response function\n            return 1.0 / (1.0 + np.exp(h * (np.log(x) - theta)))\n\n        # Step 2: Calculate total Fisher Information components\n        def get_fisher_info_summand(theta):\n            # Calculates the sum part of the Fisher Information formula\n            # Sum over i of r_i * [mu_i * (1-mu_i)]^2\n            sum_val = 0.0\n            for i in range(len(doses)):\n                mu = mean_response(doses[i], theta)\n                # The weight for each observation is [mu * (1-mu)]^2\n                weight = (mu * (1.0 - mu))**2\n                sum_val += replicates[i] * weight\n            return sum_val\n\n        sum_fisher_a = get_fisher_info_summand(theta_a)\n        sum_fisher_b = get_fisher_info_summand(theta_b)\n        \n        # Step 3: Calculate variances and standard error of the difference\n        # Var(theta_hat) = 1 / I_total(theta) = (sigma^2/h^2) / Sum(...)\n        if sum_fisher_a = 0 or sum_fisher_b = 0:\n            # Avoid division by zero for non-informative designs\n            return 0.0\n            \n        var_theta_a = (sigma**2 / h**2) / sum_fisher_a\n        var_theta_b = (sigma**2 / h**2) / sum_fisher_b\n        \n        se_delta = np.sqrt(var_theta_a + var_theta_b)\n\n        # Step 4: Calculate noncentrality parameter and power\n        delta = np.log(f)\n        if se_delta = 0:\n             return 1.0 if delta != 0 else alpha\n        \n        noncentrality_param = np.abs(delta) / se_delta\n        \n        z_crit = norm.ppf(1.0 - alpha / 2.0)\n        \n        power = 1.0 - (norm.cdf(z_crit - noncentrality_param) - norm.cdf(-z_crit - noncentrality_param))\n        \n        return power\n\n    results = []\n    for case in test_cases:\n        power = calculate_power(\n            doses=case[\"doses\"],\n            replicates=case[\"replicates\"],\n            sigma=case[\"sigma\"],\n            h=case[\"h\"],\n            ec50_a=case[\"ec50_a\"],\n            f=case[\"f\"],\n            alpha=case[\"alpha\"]\n        )\n        # Round to four decimal places\n        results.append(round(power, 4))\n    \n    # Format results as a string with no spaces as per template\n    # Example format required: \"[0.8021,0.0543,0.9999,0.4120]\"\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2481284"}, {"introduction": "Real-world data often present challenges that require advanced statistical solutions. This exercise addresses the common issue of \"complete separation\" in quantal dose-response data, where low doses produce zero responses and high doses produce 100% responses. You will explore why standard maximum likelihood methods fail in this scenario and evaluate principled alternatives like penalized likelihood that provide stable and finite estimates for the $EC_{50}$ [@problem_id:2481181], a crucial skill for robust data analysis in toxicology.", "problem": "An ecotoxicology team conducts a static renewal toxicity test on a freshwater amphipod to quantify the half maximal effective concentration ($EC_{50}$; half maximal effective concentration) for immobilization after $48$ hours. At each concentration $d_i$ (in mg L$^{-1}$), $n_i$ individuals are exposed, and $y_i$ immobilizations are recorded. The experimental design uses $n_i = 20$ for all $i$ and doses $d \\in \\{0.1, 0.3, 1, 3, 10\\}$ mg L$^{-1}$. The observed outcomes are\n- $y/n = 0/20$ at $d=0.1$ mg L$^{-1}$,\n- $y/n = 0/20$ at $d=0.3$ mg L$^{-1}$,\n- $y/n = 0/20$ at $d=1$ mg L$^{-1}$,\n- $y/n = 20/20$ at $d=3$ mg L$^{-1}$,\n- $y/n = 20/20$ at $d=10$ mg L$^{-1}$.\n\nAssume a generalized linear model (GLM; generalized linear model) with a binomial likelihood and a logit link for the probability of immobilization as a function of the log-dose $x_i = \\log d_i$:\n$$\nY_i \\sim \\text{Binomial}(n_i, p_i), \\quad \\text{logit}(p_i) = \\alpha + \\beta x_i,\n$$\nwith the usual definition $\\text{logit}(p) = \\log\\left( \\frac{p}{1-p} \\right)$. Define the half maximal effective concentration $\\mathrm{EC}_{50}$ as the dose $d_{50}$ at which the expected immobilization probability equals $0.5$ under the model; under the above parameterization this implies $x_{50} = \\log d_{50}$ satisfies $\\alpha + \\beta x_{50} = 0$.\n\nStarting only from the above definitions and the binomial GLM formulation, reason about existence and behavior of the maximum likelihood estimator under the observed pattern of complete separation (all nonresponses at low doses and all responses at high doses). Then determine which of the following approaches both (i) yields a finite, interpretable estimate of $\\mathrm{EC}_{50}$ in this setting and (ii) supports valid uncertainty quantification grounded in likelihood or posterior theory, without resorting to ad hoc data alterations.\n\nChoose the single best option.\n\nA. Fit the stated logistic regression by standard maximum likelihood on $x_i = \\log d_i$, accept the infinite slope if it occurs, and define $\\mathrm{EC}_{50}$ as the arithmetic midpoint between the largest dose with $y_i = 0$ and the smallest dose with $y_i = n_i$.\n\nB. Add a small continuity correction (e.g., add $0.5$ pseudo-events and $0.5$ pseudo-non-events to each $y_i$ and $n_i - y_i$), then fit the standard logistic regression by maximum likelihood and compute $\\mathrm{EC}_{50}$ via $d_{50} = \\exp(-\\alpha/\\beta)$ with Wald intervals from the corrected fit.\n\nC. Use a penalized likelihood for bias reduction in the logistic GLM, specifically maximize the penalized log-likelihood $\\ell^{\\ast}(\\alpha,\\beta) = \\ell(\\alpha,\\beta) + \\frac{1}{2}\\log |I(\\alpha,\\beta)|$ where $\\ell$ is the binomial log-likelihood and $I$ is the observed Fisher information, which is equivalent to using the Jeffreys prior. Then compute $d_{50} = \\exp(-\\hat{\\alpha}/\\hat{\\beta})$ and obtain confidence intervals for $d_{50}$ via the penalized profile likelihood on the transformed parameter or via a Bayesian formulation with weakly informative priors on $(\\alpha,\\beta)$.\n\nD. Switch the link function from logit to probit and refit by standard maximum likelihood; because the probit is less steep, separation cannot occur and the resulting $\\mathrm{EC}_{50}$ and its confidence interval are well-defined and unbiased under the stated design.", "solution": "The problem statement must first be rigorously validated for scientific and logical soundness before any attempt at a solution.\n\nStep 1: Extract Givens.\n- The experiment is a static renewal toxicity test on a freshwater amphipod.\n- The endpoint is immobilization after $48$ hours.\n- The target quantity is the half maximal effective concentration, $\\mathrm{EC}_{50}$.\n- The experimental design uses $n_i = 20$ individuals for all dose groups $i$.\n- The doses are $d \\in \\{0.1, 0.3, 1, 3, 10\\}$ in units of mg L$^{-1}$.\n- The observed outcomes $(y_i/n_i)$ are:\n  - $0/20$ at $d=0.1$ mg L$^{-1}$\n  - $0/20$ at $d=0.3$ mg L$^{-1}$\n  - $0/20$ at $d=1$ mg L$^{-1}$\n  - $20/20$ at $d=3$ mg L$^{-1}$\n  - $20/20$ at $d=10$ mg L$^{-1}$\n- The proposed model is a generalized linear model (GLM) where the number of immobilizations $Y_i$ at dose $d_i$ follows a binomial distribution:\n$$\nY_i \\sim \\text{Binomial}(n_i, p_i)\n$$\n- The probability of immobilization $p_i$ is related to the log-transformed dose $x_i = \\log d_i$ via a logit link function:\n$$\n\\text{logit}(p_i) = \\log\\left( \\frac{p_i}{1-p_i} \\right) = \\alpha + \\beta x_i\n$$\n- The $\\mathrm{EC}_{50}$ is defined as the dose $d_{50}$ where the probability of immobilization is $p=0.5$. This implies that the linear predictor is zero: $\\alpha + \\beta x_{50} = 0$, where $x_{50} = \\log d_{50}$.\n\nStep 2: Validate Using Extracted Givens.\n- **Scientific Grounding**: The problem is well-grounded in ecotoxicology and biostatistics. Dose-response modeling using binomial GLMs (logistic regression) is a standard and fundamental technique. The phenomenon described in the data — complete separation — is a known statistical issue in such models. The definition of $\\mathrm{EC}_{50}$ is correct. The problem is scientifically sound.\n- **Well-Posedness**: The problem asks to identify a suitable statistical methodology to handle a specific, real-world data pattern that causes standard methods to fail. The question is structured to evaluate different established or heuristic approaches. It is well-posed.\n- **Objectivity**: The problem is stated using precise, objective, and standard terminology from statistics and toxicology. There is no subjective language.\n- **Completeness and Consistency**: The data, model, and parameter definitions are all provided and are internally consistent. The log-transformation of dose is standard practice. The data pattern presents a clear case of complete separation: on the scale of $x_i = \\log d_i$, all non-responses ($y_i=0$) occur at or below $x = \\log(1) = 0$, and all responses ($y_i=n_i$) occur at or above $x = \\log(3) \\approx 1.0986$. Since the set of predictor values for the zero-response group is disjoint from and entirely below the set of predictor values for the full-response group, this is a classic case of complete separation.\n- **Conclusion**: The problem statement is valid. It describes a non-trivial but standard statistical problem and asks for the appropriate--that is, theoretically sound-—methodology to address it.\n\nProceeding to solution derivation.\n\nThe core of the problem lies in the phenomenon of **complete separation**. For a logistic regression model, the log-likelihood function is given by:\n$$\n\\ell(\\alpha, \\beta) = \\sum_{i=1}^{k} \\left[ y_i \\log(p_i) + (n_i - y_i) \\log(1 - p_i) \\right]\n$$\nwhere $p_i = \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)}$.\n\nIn our case, the data show that for all $x_i$ below a certain threshold (e.g., any value between $\\log(1)$ and $\\log(3)$), we observe $y_i=0$, and for all $x_i$ above this threshold, we observe $y_i=n_i=20$. A perfect fit would have $p_i \\to 0$ for the first three dose groups and $p_i \\to 1$ for the last two.\nTo achieve $p_i \\to 0$, we need the linear predictor $\\alpha + \\beta x_i \\to -\\infty$.\nTo achieve $p_i \\to 1$, we need the linear predictor $\\alpha + \\beta x_i \\to +\\infty$.\nThis can be achieved by letting the slope parameter $\\beta \\to \\infty$. If we fix the point of transition $x_{50} = -\\alpha/\\beta$ to be any value in the separation interval $(\\log(1), \\log(3))$, then we can write $\\alpha = -\\beta x_{50}$. The linear predictor becomes $\\beta (x_i - x_{50})$. As $\\beta \\to \\infty$, this term approaches $-\\infty$ for $x_i  x_{50}$ and $+\\infty$ for $x_i  x_{50}$. This configuration maximizes the likelihood function for any choice of $x_{50}$ in the interval.\nConsequently, the maximum likelihood estimate (MLE) does not exist as a finite point in the parameter space $(\\alpha, \\beta)$. The parameters diverge, and while their ratio $-\\alpha/\\beta$ is confined to the interval $(\\log(1), \\log(3))$, it is not uniquely determined. The standard maximum likelihood procedure fails to produce a unique, finite estimate for the model parameters and, by extension, for $\\mathrm{EC}_{50}$.\n\nWe must therefore evaluate the proposed options for their ability to resolve this failure in a statistically principled manner.\n\n**Option A Analysis**:\nThis option first suggests fitting the model by standard maximum likelihood, which, as established, leads to an infinite slope $\\beta$. It then proposes to define $\\mathrm{EC}_{50}$ as the arithmetic midpoint of the doses bracketing the separation, which are $d=1$ mg L$^{-1}$ and $d=3$ mg L$^{-1}$. This results in an estimate of $\\mathrm{EC}_{50} = (1+3)/2 = 2$ mg L$^{-1}$.\nThis procedure is fundamentally flawed. The first part (fitting the model) fails. The second part is a purely *ad hoc* rule, disengaged from the statistical model. It ignores the dose-spacing on the logarithmic scale, which is the relevant scale for the model. More critically, it does not provide any basis for \"valid uncertainty quantification\" grounded in likelihood theory, as required by the problem prompt. This is a heuristic guess, not a statistical inference.\nVerdict: **Incorrect**.\n\n**Option B Analysis**:\nThis option suggests adding a \"continuity correction,\" which involves altering the data by adding pseudo-counts. For example, adding $0.5$ to each observed count $y_i$ and also to each non-response count $n_i - y_i$. This transforms the data from $\\{0, 20\\}$ to $\\{0.5, 20.5\\}$ (and the total $n_i$ to $21$). This modification breaks the complete separation, allowing the standard MLE algorithm to converge to finite estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$. From these, an $\\mathrm{EC}_{50}$ and its confidence interval can be calculated.\nHowever, the problem explicitly requires a method that works \"without resorting to ad hoc data alterations.\" Adding pseudo-counts is precisely such an alteration. The choice of $0.5$ is arbitrary; other values could be chosen, leading to different results. While sometimes used in practice as a quick fix, it is not a principled solution derived from the original data and model.\nVerdict: **Incorrect**.\n\n**Option C Analysis**:\nThis option proposes using a penalized likelihood approach. Specifically, it mentions maximizing $\\ell^{\\ast}(\\alpha,\\beta) = \\ell(\\alpha,\\beta) + \\frac{1}{2}\\log |I(\\alpha,\\beta)|$, where $I$ is the Fisher information matrix. This is the well-known Firth's logistic regression, which results from applying a Jeffreys prior in a Bayesian context.\nThis method is a principled statistical solution to the problem of separation. The penalty term $\\frac{1}{2}\\log |I(\\alpha,\\beta)|$ regularizes the likelihood, preventing the parameters from diverging to infinity. It is guaranteed to produce finite estimates for $\\alpha$ and $\\beta$, even in cases of complete separation. The resulting estimators have reduced bias compared to the MLE. From these finite estimates $(\\hat{\\alpha}, \\hat{\\beta})$, one can compute the point estimate $d_{50} = \\exp(-\\hat{\\alpha}/\\hat{\\beta})$.\nCrucially, this method also supports valid uncertainty quantification. One can construct confidence intervals for the parameters, and thus for $\\mathrm{EC}_{50}$, using the penalized profile likelihood or related methods. The option correctly notes the equivalence to a Bayesian analysis with a Jeffreys prior or, more generally, weakly informative priors. These approaches provide a full posterior distribution for the parameters, from which credible intervals for $\\mathrm{EC}_{50}$ can be derived. This method is not an \"ad hoc data alteration\" but a modification of the estimation objective function itself, grounded in solid statistical theory (bias reduction, Bayesian inference).\nVerdict: **Correct**.\n\n**Option D Analysis**:\nThis option suggests switching the link function from logit to probit, which uses the inverse of the standard normal cumulative distribution function, $\\Phi^{-1}(p)$. The claim is that because the probit function is \"less steep,\" separation cannot occur.\nThis claim is false. The problem of separation is not specific to the logit link. It occurs with any sigmoid-shaped dose-response curve (probit, cloglog, etc.) whenever the predictor variable perfectly separates the binary outcomes. Just as with the logit link, the likelihood for a probit model with separated data will be maximized by making the curve an infinitely steep step function. This corresponds to the slope parameter $\\beta \\to \\infty$. Therefore, switching to a probit link does not solve the fundamental problem of non-existence of the MLE. The statement that \"separation cannot occur\" is patently wrong.\nVerdict: **Incorrect**.\n\nBased on a thorough analysis of the statistical theory, only Option C presents a method that is both theoretically sound and meets all conditions set forth in the problem statement.", "answer": "$$\\boxed{C}$$", "id": "2481181"}, {"introduction": "Toxicological effects are often dynamic processes that vary among individuals. This advanced practice moves beyond static endpoints to model longitudinal growth data using a nonlinear mixed-effects model, which provides deeper mechanistic insights. By implementing a model that incorporates individual-specific variability and time-correlated measurements [@problem_id:2481338], you will gain hands-on experience with a powerful, modern framework for analyzing complex, repeated-measures datasets common in chronic toxicity studies.", "problem": "You are given repeated-measures growth data generated under constant chemical exposure for multiple individuals in an ecological toxicity study. The scientific goal is to estimate the half-maximal effective concentration ($EC_{50}$), defined as the concentration at which the effect on growth rate reaches one-half of its maximal difference relative to control, under a mechanistic nonlinear mixed-effects model. You must implement a program that computes the maximum likelihood estimate (MLE) of $EC_{50}$ in milligrams per liter for each of several specified test cases, and outputs the estimates as floats in a single line.\n\nFundamental base and model components:\n- The effective concentration at half-maximal effect ($EC_{50}$) is a parameter that scales exposure-response according to a monotone inhibitory effect function on the intrinsic growth rate. Let the growth rate under exposure concentration $C$ be $r(C)$.\n- The inhibitory effect function follows a Hill-type form with Hill coefficient fixed at $h = 1$, so that the concentration-dependent growth rate is given by the mapping\n$$r(C) = \\dfrac{r_{\\max}}{1 + \\left(C/\\theta\\right)^{h}},$$\nwhere $\\theta$ is the $EC_{50}$ to be estimated, $r_{\\max}$ is the maximal growth rate in the absence of toxicant effect, and $h$ is fixed at $1$.\n- For an individual $i$ exposed to a constant concentration $C_i$, observed at times $t_{i1}, \\dots, t_{iT_i}$ (in days), the mean structural growth trajectory is modeled as\n$$m_{ij}(b_i, \\theta) = K_i \\left(1 - \\exp\\left(- r(C_i) \\, t_{ij}\\right)\\right),$$\nwhere the asymptotic size parameter is $K_i = \\exp(\\mu_K + b_i)$ with $b_i$ a subject-specific random effect.\n- The random effect is specified as $b_i \\sim \\mathcal{N}(0, \\tau^2)$ independently across individuals, and thus $K_i$ is lognormally distributed.\n- The residual error process within each individual follows a first-order autoregressive process (AR(1)) with Gaussian innovations, defined as $e_{i1} \\sim \\mathcal{N}\\!\\left(0, \\sigma^2/(1-\\rho^2)\\right)$ and, for $j \\ge 2$, $e_{ij} = \\rho \\, e_{i,j-1} + u_{ij}$ with $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, where $|\\rho|  1$, $\\sigma^2  0$.\n- The observation model is $y_{ij} = m_{ij}(b_i, \\theta) + e_{ij}$.\n\nLikelihood basis:\n- Conditional on $b_i$, the observation vector $\\mathbf{y}_i = (y_{i1}, \\dots, y_{iT_i})^\\top$ has a multivariate normal distribution implied by the AR(1) residual construction above. The conditional likelihood factors into $p(y_{i1} \\mid b_i) \\prod_{j=2}^{T_i} p(y_{ij} \\mid y_{i,j-1}, b_i)$ due to the Gaussian Markov property of the AR(1) process.\n- The marginal likelihood for $\\mathbf{y}_i$ integrates over the random effect:\n$$L_i(\\theta) = \\int \\left[\\prod_{j=1}^{T_i} p(y_{ij} \\mid b_i, \\theta)\\right] \\phi(b_i; 0, \\tau^2) \\, \\mathrm{d}b_i,$$\nwhere $\\phi(\\cdot;0,\\tau^2)$ is the normal density with mean $0$ and variance $\\tau^2$.\n- The total log-likelihood is\n$$\\ell(\\theta) = \\sum_{i=1}^{N} \\log L_i(\\theta).$$\n\nComputational requirement:\n- The integral over $b_i$ has no closed form for this nonlinear mean function; you must approximate it using Gaussian–Hermite quadrature with at least $n = 21$ nodes. Use the identity\n$$\n\\int_{-\\infty}^{\\infty} \\phi(b; 0, \\tau^2) \\, f(b) \\, \\mathrm{d}b \\;=\\; \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} e^{-x^2} f(\\sqrt{2}\\,\\tau x) \\, \\mathrm{d}x,\n$$\nso that the quadrature approximation is\n$$\n\\int \\phi(b;0,\\tau^2) f(b) \\, \\mathrm{d}b \\;\\approx\\; \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{n} w_k \\, f\\!\\left(\\sqrt{2}\\,\\tau \\, x_k\\right),\n$$\nwhere $\\{x_k, w_k\\}_{k=1}^{n}$ are the $n$-point Gaussian–Hermite nodes and weights.\n- You must maximize $\\ell(\\theta)$ over $\\theta  0$. Perform a univariate bounded optimization over $\\theta \\in [10^{-2}, 10^{2}]$, and report the MLE of $\\theta$ ($EC_{50}$) in milligrams per liter as a float, rounded to three decimal places.\n\nData generation protocol (to be reproduced exactly by your program):\n- For each test case, use the specified parameters and a pseudorandom seed to generate data according to the model. For individual $i$, draw $b_i \\sim \\mathcal{N}(0, \\tau^2)$ and set $K_i = \\exp(\\mu_K + b_i)$. For the AR(1) residuals, draw $e_{i1} \\sim \\mathcal{N}(0, \\sigma^2/(1-\\rho^2))$ and then $e_{ij} = \\rho e_{i,j-1} + u_{ij}$ with $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ independently. The observation is\n$$y_{ij} = K_i \\left(1 - \\exp\\left(- r(C_i) \\, t_{ij}\\right)\\right) + e_{ij},$$\nwhere\n$$r(C_i) = \\dfrac{r_{\\max}}{1 + (C_i/\\theta_{\\mathrm{true}})^{h}}$$\nand $h = 1$. All times are in days, all concentrations and $EC_{50}$ are in milligrams per liter, and $r_{\\max}$ is in day$^{-1}$. The true $EC_{50}$ used to generate each dataset is provided, but must be treated as unknown in estimation.\n\nTest suite:\nFor each case, your program must generate the dataset using the specified seed and parameters, then compute the MLE of $\\theta$ ($EC_{50}$) as described. The cases are:\n\n- Case A:\n    - Seed: $202311$\n    - Individuals: $N = 6$\n    - Times (days): $[1, 2, 4, 7, 10, 14]$\n    - Exposures (mg/L): $[0.5, 1.0, 2.0, 4.0, 8.0, 16.0]$\n    - True $EC_{50}$ (mg/L): $\\theta_{\\mathrm{true}} = 4.0$\n    - $r_{\\max} = 0.6$ day$^{-1}$\n    - $\\mu_K = \\ln(100)$\n    - $\\tau = 0.2$\n    - $\\sigma = 3.0$\n    - $\\rho = 0.5$\n    - $h = 1$\n\n- Case B:\n    - Seed: $202312$\n    - Individuals: $N = 8$\n    - Times (days): $[1, 2, 3, 5, 8, 12]$\n    - Exposures (mg/L): $[0.25, 0.75, 1.5, 3.0, 6.0, 12.0, 18.0, 24.0]$\n    - True $EC_{50}$ (mg/L): $\\theta_{\\mathrm{true}} = 6.0$\n    - $r_{\\max} = 0.5$ day$^{-1}$\n    - $\\mu_K = \\ln(80)$\n    - $\\tau = 0.15$\n    - $\\sigma = 2.0$\n    - $\\rho = 0.2$\n    - $h = 1$\n\n- Case C:\n    - Seed: $202313$\n    - Individuals: $N = 5$\n    - Times (days): $[2, 4, 6, 9, 13]$\n    - Exposures (mg/L): $[1.0, 2.0, 5.0, 10.0, 20.0]$\n    - True $EC_{50}$ (mg/L): $\\theta_{\\mathrm{true}} = 5.0$\n    - $r_{\\max} = 0.7$ day$^{-1}$\n    - $\\mu_K = \\ln(150)$\n    - $\\tau = 0.3$\n    - $\\sigma = 4.0$\n    - $\\rho = 0.8$\n    - $h = 1$\n\n- Case D:\n    - Seed: $202314$\n    - Individuals: $N = 4$\n    - Times (days): $[1, 3, 7, 14]$\n    - Exposures (mg/L): $[0.5, 2.0, 4.0, 12.0]$\n    - True $EC_{50}$ (mg/L): $\\theta_{\\mathrm{true}} = 3.0$\n    - $r_{\\max} = 0.4$ day$^{-1}$\n    - $\\mu_K = \\ln(60)$\n    - $\\tau = 0.05$\n    - $\\sigma = 1.0$\n    - $\\rho = 0.0$\n    - $h = 1$\n\nWhat you must compute and output:\n- For each case, compute the MLE $\\hat{\\theta}$ of $EC_{50}$ in milligrams per liter by maximizing the marginal log-likelihood $\\ell(\\theta)$ constructed as above with Gaussian–Hermite quadrature using at least $n = 21$ nodes.\n- Use a bounded univariate optimizer over $\\theta \\in [10^{-2}, 10^{2}]$. Internally, you may enforce positivity by reparameterizing if desired, but the reported value must be on the original scale.\n- Report the four estimated $EC_{50}$ values as floats, each rounded to three decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, Case D (e.g., [$\\hat{\\theta}_A, \\hat{\\theta}_B, \\hat{\\theta}_C, \\hat{\\theta}_D$]), but without any units in the output line.\n\nAll assumptions and parameter values must be implemented exactly as specified. No user input is allowed. The output must be reproducible. Express $EC_{50}$ in milligrams per liter as floats rounded to three decimal places.", "solution": "The problem statement has been critically examined and found to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, albeit computationally intensive, task in statistical modeling for ecotoxicology: the maximum likelihood estimation of a parameter in a nonlinear mixed-effects model. The model components, including the von Bertalanffy-type growth function, Hill-type dose-response relationship, log-normal random effects, and an AR(1) residual error structure, are all standard constructs in this field. The instructions for data generation and numerical approximation are precise and allow for a unique, verifiable solution. We shall proceed with the derivation and implementation of the solution.\n\nThe objective is to find the maximum likelihood estimate (MLE) of the half-maximal effective concentration, denoted $\\theta$ ($EC_{50}$). The MLE $\\hat{\\theta}$ is the value of $\\theta$ that maximizes the total log-likelihood function, $\\ell(\\theta)$, for the observed data. The data consists of $N$ individuals, each with a set of observations $\\mathbf{y}_i = (y_{i1}, \\dots, y_{iT_i})^\\top$ measured at times $\\mathbf{t}_i = (t_{i1}, \\dots, t_{iT_i})^\\top$ under a constant exposure concentration $C_i$.\n\nThe total log-likelihood is the sum of the log-likelihoods for each individual, assuming independence across individuals:\n$$\n\\ell(\\theta) = \\sum_{i=1}^{N} \\log L_i(\\theta)\n$$\nHere, $L_i(\\theta)$ is the marginal likelihood for the observation vector $\\mathbf{y}_i$ of individual $i$. This is obtained by integrating the conditional likelihood $p(\\mathbf{y}_i \\mid b_i, \\theta)$ over the distribution of the individual-specific random effect $b_i$. The random effect $b_i$ is specified to follow a normal distribution $b_i \\sim \\mathcal{N}(0, \\tau^2)$, with density $\\phi(b_i; 0, \\tau^2)$. Thus, the marginal likelihood is:\n$$\nL_i(\\theta) = \\int_{-\\infty}^{\\infty} p(\\mathbf{y}_i \\mid b_i, \\theta) \\, \\phi(b_i; 0, \\tau^2) \\, \\mathrm{d}b_i\n$$\nThe term $p(\\mathbf{y}_i \\mid b_i, \\theta)$ is the joint probability density of the observations for individual $i$, conditional on the random effect $b_i$ and the parameter $\\theta$. The problem specifies a first-order autoregressive, AR(1), process for the residuals $e_{ij} = y_{ij} - m_{ij}(b_i, \\theta)$. Due to the Markov property of the AR(1) process, the conditional likelihood can be factored as:\n$$\np(\\mathbf{y}_i \\mid b_i, \\theta) = p(y_{i1} \\mid b_i, \\theta) \\prod_{j=2}^{T_i} p(y_{ij} \\mid y_{i,j-1}, b_i, \\theta)\n$$\nThe terms in this product are densities of normal distributions:\n1.  The first observation $y_{i1}$ is conditional only on $b_i$ and $\\theta$. Its distribution is $y_{i1} \\sim \\mathcal{N}\\left(m_{i1}(b_i, \\theta), \\frac{\\sigma^2}{1-\\rho^2}\\right)$.\n2.  For subsequent observations ($j \\ge 2$), the distribution of $y_{ij}$ is conditional on the previous observation $y_{i,j-1}$, as well as $b_i$ and $\\theta$. The relationship $e_{ij} = \\rho e_{i,j-1} + u_{ij}$ implies $y_{ij} - m_{ij} = \\rho(y_{i,j-1} - m_{i,j-1}) + u_{ij}$, where $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, $y_{ij} \\mid y_{i,j-1}, b_i, \\theta \\sim \\mathcal{N}\\left(m_{ij} + \\rho(y_{i,j-1} - m_{i,j-1}), \\sigma^2\\right)$.\n\nThe mean trajectory function $m_{ij}(b_i, \\theta)$ is given by:\n$$\nm_{ij}(b_i, \\theta) = K_i \\left(1 - \\exp\\left(- r(C_i, \\theta) \\, t_{ij}\\right)\\right)\n$$\nwhere $K_i = \\exp(\\mu_K + b_i)$ and $r(C_i, \\theta) = \\dfrac{r_{\\max}}{1 + (C_i/\\theta)^{h}}$ with Hill coefficient $h=1$.\n\nThe integral for $L_i(\\theta)$ does not have a closed-form solution due to the nonlinear dependence of $m_{ij}$ on $b_i$. It must be approximated numerically. The problem specifies Gaussian-Hermite quadrature. Using the provided identity, the integral is approximated as:\n$$\nL_i(\\theta) \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{n} w_k \\, p(\\mathbf{y}_i \\mid b_k, \\theta)\n$$\nwhere $\\{x_k, w_k\\}_{k=1}^n$ are the $n$-point Gaussian-Hermite nodes and weights, and the random effect values are evaluated at $b_k = \\sqrt{2}\\tau x_k$. We will use $n=21$ nodes.\n\nTo implement this robustly, we compute the log of the marginal likelihood, $\\log L_i(\\theta)$. Direct computation of the sum can lead to numerical underflow or overflow. We apply the log-sum-exp trick. The individual log-likelihood is:\n$$\n\\log L_i(\\theta) \\approx \\log\\left(\\frac{1}{\\sqrt{\\pi}}\\right) + \\log\\left(\\sum_{k=1}^{n} w_k \\, p(\\mathbf{y}_i \\mid b_k, \\theta)\\right) = -\\frac{1}{2}\\log\\pi + \\log\\left(\\sum_{k=1}^{n} \\exp\\left(\\log w_k + \\log p(\\mathbf{y}_i \\mid b_k, \\theta)\\right)\\right)\n$$\nLetting $A_k = \\log w_k + \\log p(\\mathbf{y}_i \\mid b_k, \\theta)$ and $A_{\\max} = \\max_k A_k$, the sum is computed as $A_{\\max} + \\log(\\sum_k \\exp(A_k - A_{\\max}))$.\n\nThe computational procedure is as follows:\n1.  For each test case, generate the dataset precisely according to the specified protocol. This involves setting the random seed, drawing the random effects and residual errors from their specified normal distributions, and computing the observations.\n2.  Define an objective function that computes the negative total log-likelihood, $-\\ell(\\theta)$, given a value of $\\theta$ and the generated data. This function iterates through each individual, calculates their marginal log-likelihood $\\log L_i(\\theta)$ using $n=21$ point Gaussian-Hermite quadrature with the log-sum-exp stabilization, and sums them up.\n3.  Use a numerical optimization routine to find the value of $\\theta$ that minimizes this objective function, subject to the bounds $\\theta \\in [10^{-2}, 10^{2}]$. This yields the MLE, $\\hat{\\theta}$.\n4.  The final result for each case is rounded to three decimal places. The complete implementation is provided in the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom numpy.polynomial.hermite import hermgauss\n\n# Global constants as per problem specification\nN_QUAD = 21\nOPT_BOUNDS = (1e-2, 1e2)\nGH_NODES, GH_WEIGHTS = hermgauss(N_QUAD)\n\ndef generate_data(seed, N, times, exposures, theta_true, r_max, mu_K, tau, sigma, rho, h):\n    \"\"\"\n    Generates simulated toxicity data for N individuals based on the model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    dataset = []\n\n    for i in range(N):\n        C_i = exposures[i]\n        \n        # 1. Draw random effect b_i and determine individual asymptotic size K_i\n        b_i = rng.normal(loc=0.0, scale=tau)\n        K_i = np.exp(mu_K + b_i)\n        \n        # 2. Calculate the concentration-dependent growth rate r(C_i)\n        r_c = r_max / (1.0 + (C_i / theta_true)**h)\n        \n        # 3. Generate AR(1) residual error series e_ij\n        T_i = len(times)\n        errors = np.zeros(T_i)\n        \n        # Variance of the first error term e_i1\n        var_e1 = sigma**2 / (1.0 - rho**2) if rho != 0.0 else sigma**2\n        errors[0] = rng.normal(loc=0.0, scale=np.sqrt(var_e1))\n        \n        # Subsequent error terms e_ij for j > 1\n        for j in range(1, T_i):\n            u_ij = rng.normal(loc=0.0, scale=sigma)\n            errors[j] = rho * errors[j-1] + u_ij\n            \n        # 4. Compute the mean trajectory and the final observations\n        mean_trajectory = K_i * (1.0 - np.exp(-r_c * times))\n        observations = mean_trajectory + errors\n        \n        dataset.append({'C': C_i, 't': times, 'y': observations})\n        \n    return dataset\n\ndef neg_log_likelihood(theta, data, r_max, mu_K, tau, sigma, rho, h):\n    \"\"\"\n    Calculates the negative marginal log-likelihood for the entire dataset.\n    This is the objective function for the optimizer.\n    \"\"\"\n    total_log_lik = 0.0\n\n    # Iterate over each individual in the dataset\n    for ind_data in data:\n        C_i, t_i, y_i = ind_data['C'], ind_data['t'], ind_data['y']\n        T_i = len(t_i)\n        \n        # Terms for the log-sum-exp computation over quadrature nodes\n        log_lik_terms = np.zeros(N_QUAD)\n\n        # Growth rate for the current candidate theta\n        r_c = r_max / (1.0 + (C_i / theta)**h)\n\n        # Iterate over Gaussian-Hermite quadrature nodes\n        for k in range(N_QUAD):\n            x_k, w_k = GH_NODES[k], GH_WEIGHTS[k]\n            \n            # Map quadrature node to the random effect scale\n            b_k = np.sqrt(2.0) * tau * x_k\n            \n            # Individual's asymptotic size for this value of the random effect\n            K_k = np.exp(mu_K + b_k)\n            \n            # Mean growth trajectory for this b_k and theta\n            m_k = K_k * (1.0 - np.exp(-r_c * t_i))\n            \n            # Calculate conditional log-likelihood log p(y_i | b_k, theta)\n            cond_log_lik = 0.0\n            \n            # Log-likelihood contribution from the first time point (j=1)\n            var1 = sigma**2 / (1.0 - rho**2) if rho != 0.0 else sigma**2\n            log_pdf_1 = -0.5 * np.log(2.0 * np.pi * var1) - ((y_i[0] - m_k[0])**2) / (2.0 * var1)\n            cond_log_lik += log_pdf_1\n            \n            # Log-likelihood contribution from subsequent time points (j>=2)\n            if T_i > 1.0:\n                innovations = (y_i[1:] - m_k[1:]) - rho * (y_i[:-1] - m_k[:-1])\n                log_pdf_ar1 = -0.5 * np.log(2.0 * np.pi * sigma**2) - (innovations**2) / (2.0 * sigma**2)\n                cond_log_lik += np.sum(log_pdf_ar1)\n                \n            log_lik_terms[k] = np.log(w_k) + cond_log_lik\n\n        # Use log-sum-exp for stable computation of log(integral)\n        max_log = np.max(log_lik_terms)\n        log_L_i = max_log + np.log(np.sum(np.exp(log_lik_terms - max_log)))\n        \n        # Final individual log-likelihood including quadrature constant\n        individual_log_lik = -0.5 * np.log(np.pi) + log_L_i\n        \n        total_log_lik += individual_log_lik\n        \n    return -total_log_lik\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            'seed': 202311, 'N': 6, 'times': np.array([1, 2, 4, 7, 10, 14]),\n            'exposures': np.array([0.5, 1.0, 2.0, 4.0, 8.0, 16.0]), 'theta_true': 4.0,\n            'r_max': 0.6, 'mu_K': np.log(100), 'tau': 0.2, 'sigma': 3.0, 'rho': 0.5, 'h': 1\n        },\n        {\n            'seed': 202312, 'N': 8, 'times': np.array([1, 2, 3, 5, 8, 12]),\n            'exposures': np.array([0.25, 0.75, 1.5, 3.0, 6.0, 12.0, 18.0, 24.0]), 'theta_true': 6.0,\n            'r_max': 0.5, 'mu_K': np.log(80), 'tau': 0.15, 'sigma': 2.0, 'rho': 0.2, 'h': 1\n        },\n        {\n            'seed': 202313, 'N': 5, 'times': np.array([2, 4, 6, 9, 13]),\n            'exposures': np.array([1.0, 2.0, 5.0, 10.0, 20.0]), 'theta_true': 5.0,\n            'r_max': 0.7, 'mu_K': np.log(150), 'tau': 0.3, 'sigma': 4.0, 'rho': 0.8, 'h': 1\n        },\n        {\n            'seed': 202314, 'N': 4, 'times': np.array([1, 3, 7, 14]),\n            'exposures': np.array([0.5, 2.0, 4.0, 12.0]), 'theta_true': 3.0,\n            'r_max': 0.4, 'mu_K': np.log(60), 'tau': 0.05, 'sigma': 1.0, 'rho': 0.0, 'h': 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data for the current case\n        data = generate_data(\n            case['seed'], case['N'], case['times'], case['exposures'], case['theta_true'],\n            case['r_max'], case['mu_K'], case['tau'], case['sigma'], case['rho'], case['h']\n        )\n        \n        # 2. Define the objective function for optimization\n        objective_func = lambda theta: neg_log_likelihood(\n            theta, data, case['r_max'], case['mu_K'], case['tau'],\n            case['sigma'], case['rho'], case['h']\n        )\n        \n        # 3. Perform bounded minimization to find the MLE of theta (EC50)\n        opt_result = minimize_scalar(\n            objective_func,\n            bounds=OPT_BOUNDS,\n            method='bounded'\n        )\n        \n        mle_theta = opt_result.x\n        results.append(f\"{mle_theta:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2481338"}]}