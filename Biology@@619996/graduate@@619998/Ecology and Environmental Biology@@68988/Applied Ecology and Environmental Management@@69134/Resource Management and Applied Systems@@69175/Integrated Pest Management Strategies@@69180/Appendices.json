{"hands_on_practices": [{"introduction": "Effective Integrated Pest Management begins in the field with robust monitoring protocols. A common challenge is that pest populations are rarely distributed uniformly; they often exhibit aggregation or clumping. This exercise demonstrates how to translate this ecological reality into a practical sampling design [@problem_id:2499122]. By modeling pest counts with the negative binomial distribution, you will derive a formula to determine the minimum number of samples required to estimate the mean pest density with a pre-specified level of precision, a foundational skill for designing cost-effective scouting programs.", "problem": "In an Integrated Pest Management (IPM) program, a field entomologist must fix the number of primary sampling units (for example, leaves or quadrats) to estimate the mean pest density with a pre-specified relative precision. Empirical evidence supports modeling the number of pests per sampling unit by a negative binomial distribution with mean $m$ and aggregation parameter $k$, for which the variance is well described by $m + \\frac{m^{2}}{k}$. The fixed-precision criterion used by Greenâ€™s plan is that the relative standard error of the sample mean, defined as the standard error of the sample mean divided by the true mean, does not exceed a target $D$.\n\nStarting from the definitions of the mean and variance of the negative binomial distribution and from the variance of the sample mean for $n$ independent and identically distributed sampling units, derive an expression for the minimal $n$ (as a function of $m$, $k$, and $D$) that achieves the fixed-precision criterion. Then, evaluate this expression for $k = 1.2$, $m = 5$, and $D = 0.25$. Report the smallest integer $n$ that satisfies the fixed-precision criterion. No rounding by significant figures is required; report the minimal integer.", "solution": "Let $X_i$ for $i = 1, 2, \\dots, n$ be the number of pests counted in the $i$-th primary sampling unit. The problem states that these are independent and identically distributed (i.i.d.) random variables. The distribution of $X_i$ is a negative binomial distribution.\n\nThe mean of this distribution is given as:\n$$E[X_i] = m$$\n\nThe variance is given as:\n$$\\text{Var}(X_i) = \\sigma^2 = m + \\frac{m^2}{k}$$\nwhere $k$ is the aggregation parameter.\n\nThe goal is to estimate the true mean pest density, $m$, using the sample mean, $\\bar{X}$, defined as:\n$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$$\n\nThe expected value of the sample mean is:\n$$E[\\bar{X}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\frac{1}{n} (n \\cdot m) = m$$\nThis demonstrates that the sample mean $\\bar{X}$ is an unbiased estimator for the true mean $m$.\n\nDue to the independence of the $X_i$ variables, the variance of the sample mean is:\n$$\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} (n \\cdot \\sigma^2) = \\frac{\\sigma^2}{n}$$\n\nThe standard error of the sample mean, denoted $SE(\\bar{X})$, is the square root of its variance:\n$$SE(\\bar{X}) = \\sqrt{\\text{Var}(\\bar{X})} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}$$\n\nThe problem specifies a fixed-precision criterion based on the relative standard error of the sample mean. The relative standard error is defined as the standard error of the sample mean divided by the true mean, $m$. This quantity must not exceed a pre-specified value $D$.\nThe mathematical formulation of this criterion is:\n$$\\frac{SE(\\bar{X})}{m} \\le D$$\n\nSubstituting our expression for $SE(\\bar{X})$:\n$$\\frac{\\sigma/{\\sqrt{n}}}{m} \\le D$$\n\nWe must now solve this inequality for the sample size $n$.\n$$\\frac{\\sigma}{m\\sqrt{n}} \\le D$$\nRearranging the terms to isolate $n$:\n$$\\sqrt{n} \\ge \\frac{\\sigma}{mD}$$\nSquaring both sides of the inequality gives the condition for the minimal sample size:\n$$n \\ge \\left(\\frac{\\sigma}{mD}\\right)^2 = \\frac{\\sigma^2}{m^2 D^2}$$\n\nNow, we substitute the provided expression for the variance $\\sigma^2 = m + \\frac{m^2}{k}$:\n$$n \\ge \\frac{m + \\frac{m^2}{k}}{m^2 D^2}$$\nThis expression can be simplified by distributing the denominator:\n$$n \\ge \\frac{m}{m^2 D^2} + \\frac{m^2/k}{m^2 D^2}$$\n$$n \\ge \\frac{1}{m D^2} + \\frac{1}{k D^2}$$\nFactoring out the term $\\frac{1}{D^2}$, we obtain the final expression for the minimal sample size $n$ as a function of $m$, $k$, and $D$:\n$$n \\ge \\frac{1}{D^2} \\left(\\frac{1}{m} + \\frac{1}{k}\\right)$$\nThis is the first required result of the problem.\n\nNext, we must evaluate this expression for the given parameter values: $k = 1.2$, $m = 5$, and $D = 0.25$.\nWe have:\n$$D = 0.25 = \\frac{1}{4}$$\n$$m = 5$$\n$$k = 1.2 = \\frac{12}{10} = \\frac{6}{5}$$\n\nSubstituting these values into the inequality for $n$:\n$$n \\ge \\frac{1}{(1/4)^2} \\left(\\frac{1}{5} + \\frac{1}{6/5}\\right)$$\n$$n \\ge \\frac{1}{1/16} \\left(\\frac{1}{5} + \\frac{5}{6}\\right)$$\n$$n \\ge 16 \\left(\\frac{6}{30} + \\frac{25}{30}\\right)$$\n$$n \\ge 16 \\left(\\frac{31}{30}\\right)$$\n$$n \\ge \\frac{16 \\times 31}{30} = \\frac{8 \\times 31}{15} = \\frac{248}{15}$$\n\nTo find the minimum integer value for $n$, we must compute the value of the fraction:\n$$\\frac{248}{15} = 16.5333\\dots$$\nThe condition is thus $n \\ge 16.5333\\dots$. Since $n$ must be an integer representing the number of sampling units, we must find the smallest integer that satisfies this condition. This is achieved by taking the ceiling of the numerical value:\n$$n_{min} = \\lceil 16.5333\\dots \\rceil = 17$$\nTherefore, the minimal number of sampling units required is $17$.", "answer": "$$\\boxed{17}$$", "id": "2499122"}, {"introduction": "A well-designed sampling scheme is the first step, but a reliable estimate of pest abundance also requires accounting for the observation process itself. In nearly all ecological surveys, some individuals go undetected, a phenomenon known as imperfect detection, where the detection probability $p_d \\lt 1$. This practice guides you through a formal analysis of how this imperfect detection introduces a systematic bias into naive population estimates [@problem_id:2499113]. You will use first principles of probability, including the laws of total expectation and variance, to derive an unbiased estimator for the true mean density and to understand its statistical properties, providing a crucial correction for more accurate IPM decisions.", "problem": "An Integrated Pest Management (IPM) program monitors a pest species in an orchard using repeated quadrat sampling. Let there be $n$ independent sampling units (quadrats), indexed by $i \\in \\{1,\\dots,n\\}$. In each unit $i$, the unobserved true number of individuals $N_i$ is modeled as a realization from a Poisson distribution with mean $m$, written as $N_i \\sim \\text{Poisson}(m)$. Each individual present in a unit is detected independently with probability $p_d \\in (0,1)$, which is known from an independent calibration experiment conducted under matching conditions. Let $X_i$ denote the observed count in unit $i$, obtained by surveying each unit once with the calibrated protocol.\n\nStarting from the fundamental definitions of the Poisson distribution, the Bernoulli distribution, and the law of total expectation and law of total variance, do the following:\n\n1. Derive the marginal distribution of $X_i$ given $m$ and $p_d$. Then compute $\\mathbb{E}[X_i]$ and $\\operatorname{Var}(X_i)$ explicitly in terms of $m$ and $p_d$.\n\n2. Let $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean of observed counts that ignores imperfect detection. Determine whether $\\bar{X}$ is an unbiased estimator of $m$ when $p_d < 1$, and quantify the bias $\\operatorname{Bias}(\\bar{X}; m) = \\mathbb{E}[\\bar{X}] - m$ and the relative bias $\\frac{\\mathbb{E}[\\bar{X}] - m}{m}$, expressed in terms of $p_d$ and $m$.\n\n3. Derive an unbiased estimator $\\hat{m}$ of $m$ that is a function of $\\bar{X}$ and $p_d$ only. Then derive $\\operatorname{Var}(\\hat{m})$ in closed form under the model described above, expressing your answer in terms of $m$, $n$, and $p_d$ only. Use only first principles such as the law of total expectation and the law of total variance to justify each step; do not invoke results that are not derived from these laws.\n\nReport the final result for part 3 as two expressions in the order $(\\hat{m}, \\operatorname{Var}(\\hat{m}))$. If you choose to present any intermediate numerical examples, express any such quantities in individuals per sampling unit, but the final reported expressions should be symbolic and not include units.", "solution": "The hierarchical model is defined as follows:\n$1$. The true number of individuals in sampling unit $i$, $N_i$, follows a Poisson distribution with mean $m$. The probability mass function (PMF) is $P(N_i = k) = \\frac{\\exp(-m)m^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n$2$. Given $N_i = k$ individuals, the observed count $X_i$ is the number of successes in $k$ independent Bernoulli trials, each with success probability $p_d$. This is a binomial distribution, $X_i | N_i=k \\sim \\text{Binomial}(k, p_d)$. The conditional PMF is $P(X_i = x | N_i = k) = \\binom{k}{x} p_d^x (1-p_d)^{k-x}$ for $x \\in \\{0, 1, \\dots, k\\}$.\n\n**Part 1: Marginal distribution of $X_i$, $\\mathbb{E}[X_i]$, and $\\operatorname{Var}(X_i)$**\n\nTo find the marginal distribution of $X_i$, we use the law of total probability, summing over all possible values of the unobserved true count $N_i$. The support for $X_i$ given $N_i=k$ is $x \\le k$, so for a given $x$, we must have $k \\ge x$.\n$$P(X_i = x) = \\sum_{k=x}^{\\infty} P(X_i = x | N_i = k) P(N_i = k)$$\nSubstituting the PMFs for the binomial and Poisson distributions:\n$$P(X_i = x) = \\sum_{k=x}^{\\infty} \\left[ \\binom{k}{x} p_d^x (1-p_d)^{k-x} \\right] \\left[ \\frac{\\exp(-m)m^k}{k!} \\right]$$\nWe expand the binomial coefficient $\\binom{k}{x} = \\frac{k!}{x!(k-x)!}$ and rearrange the terms:\n$$P(X_i = x) = \\sum_{k=x}^{\\infty} \\frac{k!}{x!(k-x)!} p_d^x (1-p_d)^{k-x} \\frac{\\exp(-m)m^k}{k!}$$\n$$P(X_i = x) = \\frac{\\exp(-m) p_d^x}{x!} \\sum_{k=x}^{\\infty} \\frac{m^k (1-p_d)^{k-x}}{(k-x)!}$$\nLet the index of summation be changed to $j = k-x$. As $k$ goes from $x$ to $\\infty$, $j$ goes from $0$ to $\\infty$. Also, $k=j+x$.\n$$P(X_i = x) = \\frac{\\exp(-m) p_d^x}{x!} \\sum_{j=0}^{\\infty} \\frac{m^{j+x} (1-p_d)^j}{j!}$$\nWe can factor out $m^x$ from the sum:\n$$P(X_i = x) = \\frac{\\exp(-m) (m p_d)^x}{x!} \\sum_{j=0}^{\\infty} \\frac{m^j (1-p_d)^j}{j!}$$\n$$P(X_i = x) = \\frac{\\exp(-m) (m p_d)^x}{x!} \\sum_{j=0}^{\\infty} \\frac{(m(1-p_d))^j}{j!}$$\nThe sum is the Taylor series expansion for the exponential function, $\\sum_{j=0}^{\\infty} \\frac{a^j}{j!} = \\exp(a)$. Here, $a = m(1-p_d)$.\n$$P(X_i = x) = \\frac{\\exp(-m) (m p_d)^x}{x!} \\exp(m(1-p_d))$$\nCombining the exponential terms: $\\exp(-m) \\exp(m - m p_d) = \\exp(-m + m - m p_d) = \\exp(-m p_d)$.\n$$P(X_i = x) = \\frac{\\exp(-m p_d) (m p_d)^x}{x!}$$\nThis is the PMF of a Poisson distribution with mean $m p_d$. Therefore, the marginal distribution of $X_i$ is $\\text{Poisson}(m p_d)$.\nFor a Poisson distribution with parameter $\\lambda$, the expectation and variance are both equal to $\\lambda$. Thus, for $X_i \\sim \\text{Poisson}(m p_d)$:\n$$\\mathbb{E}[X_i] = m p_d$$\n$$\\operatorname{Var}(X_i) = m p_d$$\n\nAs stipulated, we now re-derive these moments using the laws of total expectation and total variance.\nThe conditional expectation of $X_i$ given $N_i$ is $\\mathbb{E}[X_i | N_i] = N_i p_d$.\nBy the law of total expectation:\n$$\\mathbb{E}[X_i] = \\mathbb{E}_{N_i}[\\mathbb{E}[X_i | N_i]] = \\mathbb{E}[N_i p_d] = p_d \\mathbb{E}[N_i]$$\nSince $N_i \\sim \\text{Poisson}(m)$, we have $\\mathbb{E}[N_i] = m$.\n$$\\mathbb{E}[X_i] = m p_d$$\nThe conditional variance of $X_i$ given $N_i$ is $\\operatorname{Var}(X_i | N_i) = N_i p_d (1-p_d)$.\nBy the law of total variance (also known as Eve's law):\n$$\\operatorname{Var}(X_i) = \\mathbb{E}_{N_i}[\\operatorname{Var}(X_i | N_i)] + \\operatorname{Var}_{N_i}(\\mathbb{E}[X_i | N_i])$$\nWe compute each term separately:\nFirst term: $\\mathbb{E}[\\operatorname{Var}(X_i | N_i)] = \\mathbb{E}[N_i p_d (1-p_d)] = p_d(1-p_d)\\mathbb{E}[N_i] = m p_d(1-p_d)$.\nSecond term: $\\operatorname{Var}(\\mathbb{E}[X_i | N_i]) = \\operatorname{Var}(N_i p_d) = p_d^2 \\operatorname{Var}(N_i)$.\nSince $N_i \\sim \\text{Poisson}(m)$, we have $\\operatorname{Var}(N_i) = m$.\nSo, $\\operatorname{Var}(\\mathbb{E}[X_i | N_i]) = m p_d^2$.\nAdding the two terms:\n$$\\operatorname{Var}(X_i) = m p_d(1-p_d) + m p_d^2 = m p_d - m p_d^2 + m p_d^2 = m p_d$$\nThe results are identical, confirming our analysis.\n\n**Part 2: Bias of the sample mean $\\bar{X}$**\n\nThe sample mean of the observed counts is $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. We determine its expectation to assess its bias as an estimator for $m$. Using the linearity of expectation and the result from Part $1$:\n$$\\mathbb{E}[\\bar{X}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[X_i]$$\nSince all $X_i$ are identically distributed, $\\mathbb{E}[X_i] = m p_d$ for all $i \\in \\{1,\\dots,n\\}$.\n$$\\mathbb{E}[\\bar{X}] = \\frac{1}{n} \\sum_{i=1}^{n} (m p_d) = \\frac{1}{n} (n m p_d) = m p_d$$\nAn estimator is unbiased if its expectation equals the parameter being estimated. Here, we are estimating $m$. For $\\bar{X}$ to be an unbiased estimator of $m$, we would need $\\mathbb{E}[\\bar{X}] = m$. However, we have $\\mathbb{E}[\\bar{X}] = m p_d$.\nThe problem specifies $p_d \\in (0,1)$, so $p_d < 1$. In this case, $m p_d \\neq m$ (assuming $m>0$, which is implicit for a population mean). Therefore, $\\bar{X}$ is a biased estimator of $m$.\n\nThe bias is defined as $\\operatorname{Bias}(\\bar{X}; m) = \\mathbb{E}[\\bar{X}] - m$.\n$$\\operatorname{Bias}(\\bar{X}; m) = m p_d - m = m(p_d - 1)$$\nSince $p_d < 1$, the term $(p_d - 1)$ is negative, and thus the bias is negative. This indicates that ignoring imperfect detection leads to a systematic underestimation of the true mean abundance.\n\nThe relative bias is the bias divided by the true parameter value:\n$$\\text{Relative Bias} = \\frac{\\mathbb{E}[\\bar{X}] - m}{m} = \\frac{m(p_d - 1)}{m} = p_d - 1$$\n\n**Part 3: Unbiased estimator $\\hat{m}$ and its variance**\n\nWe seek an estimator $\\hat{m}$ that is a function of $\\bar{X}$ and $p_d$ and is unbiased for $m$. That is, we require $\\mathbb{E}[\\hat{m}] = m$.\nFrom Part $2$, we know that $\\mathbb{E}[\\bar{X}] = m p_d$. To construct an unbiased estimator, we can correct for the factor $p_d$. Let us define the estimator $\\hat{m}$ as:\n$$\\hat{m} = \\frac{\\bar{X}}{p_d}$$\nThe expectation of this estimator is:\n$$\\mathbb{E}[\\hat{m}] = \\mathbb{E}\\left[\\frac{\\bar{X}}{p_d}\\right] = \\frac{1}{p_d} \\mathbb{E}[\\bar{X}] = \\frac{1}{p_d} (m p_d) = m$$\nSince $\\mathbb{E}[\\hat{m}] = m$, $\\hat{m}$ is an unbiased estimator of $m$.\n\nNext, we derive the variance of $\\hat{m}$. Using the property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ for a constant $a$ and random variable $Y$:\n$$\\operatorname{Var}(\\hat{m}) = \\operatorname{Var}\\left(\\frac{\\bar{X}}{p_d}\\right) = \\frac{1}{p_d^2} \\operatorname{Var}(\\bar{X})$$\nTo find $\\operatorname{Var}(\\bar{X})$, we use the fact that the $n$ sampling units are independent, which implies that the random variables $N_i$ are independent. Since the detection process within each unit is also independent of others, the observed counts $X_i$ are independent and identically distributed (IID).\n$$\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right)$$\nFor independent random variables, the variance of the sum is the sum of the variances:\n$$\\operatorname{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(X_i)$$\nSince the $X_i$ are identically distributed, $\\operatorname{Var}(X_i) = m p_d$ for all $i$.\n$$\\operatorname{Var}(\\bar{X}) = \\frac{1}{n^2} (n \\cdot m p_d) = \\frac{m p_d}{n}$$\nFinally, we substitute this result into the expression for $\\operatorname{Var}(\\hat{m})$:\n$$\\operatorname{Var}(\\hat{m}) = \\frac{1}{p_d^2} \\left( \\frac{m p_d}{n} \\right) = \\frac{m}{n p_d}$$\nThis final expression for the variance of the unbiased estimator $\\hat{m}$ is given in terms of the model parameters $m$, $n$, and $p_d$, as required.\nThe results for Part $3$ are the estimator $\\hat{m} = \\frac{\\bar{X}}{p_d}$ and its variance $\\operatorname{Var}(\\hat{m}) = \\frac{m}{n p_d}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\bar{X}}{p_d} & \\frac{m}{n p_d}\n\\end{pmatrix}\n}\n$$", "id": "2499113"}, {"introduction": "Once monitoring provides a reliable picture of pest pressure, the focus of IPM shifts from estimation to economic decision-making. The goal is not necessarily to eradicate the pest, but to manage it in the most profitable way. This practice challenges you to find the economically optimal dose of a biopesticide by synthesizing a biological dose-response model with an economic cost framework [@problem_id:2499149]. By applying the principles of marginal analysis, you will determine the precise application level that maximizes profit, perfectly illustrating the balance between treatment costs and avoided crop losses that lies at the heart of integrated management.", "problem": "An agronomist designing an Integrated Pest Management (IPM) program for a wheat field must choose the application dose of a biopesticide, denoted by $D$ (in $\\mathrm{L\\ ha^{-1}}$), to control a pest whose damage depresses yield. The expected marketable yield under dose $D$ is modeled as\n$$\nY(D) \\;=\\; Y_{\\min} \\;+\\; \\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\,\\bigl(1 - \\exp(-k\\,D)\\bigr),\n$$\nwhich captures diminishing returns in a standard doseâ€“response form. Here $Y_{\\min}$ is the expected marketable yield with zero intervention, $Y_{\\max}$ is the attainable yield under near-complete control, and $k$ is a positive response parameter. The per-hectare total cost of applying dose $D$ is\n$$\nC(D) \\;=\\; c\\,D \\;+\\; F,\n$$\nwhere $c$ is the variable cost per unit dose and $F$ is a fixed per-hectare cost associated with scouting and application setup. The crop price is $V$ (revenue per unit yield). The agronomist seeks to choose $D$ to maximize profit\n$$\n\\Pi(D) \\;=\\; V\\,Y(D) \\;-\\; C(D).\n$$\nAssume a single application and that the doseâ€“response exhibits diminishing marginal returns for all $D \\ge 0$.\n\nGiven the following parameter values:\n- $Y_{\\min} = 3\\,\\mathrm{t\\ ha^{-1}}$,\n- $Y_{\\max} = 7\\,\\mathrm{t\\ ha^{-1}}$,\n- $k = 0.6\\,\\mathrm{(L\\ ha^{-1})^{-1}}$,\n- $V = 300\\,\\mathrm{USD\\ t^{-1}}$,\n- $c = 180\\,\\mathrm{USD\\ L^{-1}}$,\n- $F = 50\\,\\mathrm{USD\\ ha^{-1}}$,\n\ndetermine the economically optimal dose $D^{\\ast}$ (in $\\mathrm{L\\ ha^{-1}}$) that maximizes $\\Pi(D)$ under the stated assumptions. Round your answer to three significant figures. Express the dose in $\\mathrm{L\\ ha^{-1}}$.", "solution": "The objective is to find the dose $D$ that maximizes the profit function $\\Pi(D)$. The profit per hectare is given by:\n$$ \\Pi(D) = V\\,Y(D) - C(D) $$\nSubstituting the expressions for $Y(D)$ and $C(D)$:\n$$ \\Pi(D) = V\\left[Y_{\\min} + \\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\,\\bigl(1 - \\exp(-k\\,D)\\bigr)\\right] - \\left(c\\,D + F\\right) $$\nTo find the optimal dose $D^{\\ast}$, we must find the value of $D$ where the first derivative of the profit function with respect to $D$ is equal to zero. This is the first-order condition for a maximum.\n$$ \\frac{d\\Pi}{dD} = V\\frac{dY}{dD} - \\frac{dC}{dD} = 0 $$\nLet us compute the individual derivatives:\n$$ \\frac{dY}{dD} = \\frac{d}{dD}\\left[Y_{\\min} + \\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\,\\bigl(1 - \\exp(-k\\,D)\\bigr)\\right] = k\\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\exp(-k\\,D) $$\n$$ \\frac{dC}{dD} = \\frac{d}{dD}\\left[c\\,D + F\\right] = c $$\nNote that the fixed cost $F$ does not influence the optimal dose, as it is a constant with respect to $D$.\nSetting the derivative of the profit function to zero at the optimal dose $D = D^{\\ast}$:\n$$ V\\left[k\\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\exp(-k\\,D^{\\ast})\\right] - c = 0 $$\nThis equation represents the economic principle that profit is maximized when marginal revenue equals marginal cost. Here, the marginal revenue from an additional unit of dose is $V \\frac{dY}{dD}$ and the marginal cost is $c$.\nWe now solve for $D^{\\ast}$:\n$$ V k \\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\exp(-k\\,D^{\\ast}) = c $$\n$$ \\exp(-k\\,D^{\\ast}) = \\frac{c}{V k \\bigl(Y_{\\max} - Y_{\\min}\\bigr)} $$\nTaking the natural logarithm of both sides:\n$$ -k\\,D^{\\ast} = \\ln\\left(\\frac{c}{V k \\bigl(Y_{\\max} - Y_{\\min}\\bigr)}\\right) $$\n$$ D^{\\ast} = -\\frac{1}{k}\\ln\\left(\\frac{c}{V k \\bigl(Y_{\\max} - Y_{\\min}\\bigr)}\\right) $$\nUsing the property of logarithms $\\ln(1/x) = -\\ln(x)$, we can write this as:\n$$ D^{\\ast} = \\frac{1}{k}\\ln\\left(\\frac{V k \\bigl(Y_{\\max} - Y_{\\min}\\bigr)}{c}\\right) $$\nFor an optimal dose $D^{\\ast} > 0$ to exist, the argument of the logarithm must be greater than $1$, which means $V k (Y_{\\max} - Y_{\\min}) > c$. This condition states that the initial marginal revenue at $D=0$ must exceed the marginal cost.\n\nTo confirm this is a maximum, we examine the second-order condition. The second derivative of the profit function is:\n$$ \\frac{d^2\\Pi}{dD^2} = V\\frac{d^2Y}{dD^2} = V\\left[-k^2\\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\exp(-k\\,D)\\right] = -V k^2 \\bigl(Y_{\\max} - Y_{\\min}\\bigr)\\exp(-k\\,D) $$\nGiven that $V>0$, $k>0$, and $Y_{\\max} > Y_{\\min}$, the second derivative $\\frac{d^2\\Pi}{dD^2}$ is strictly negative for all $D \\ge 0$. This proves that the profit function $\\Pi(D)$ is strictly concave, and thus the critical point $D^{\\ast}$ corresponds to a unique global maximum.\n\nNow, we substitute the given numerical values:\n$Y_{\\max} - Y_{\\min} = 7 - 3 = 4\\,\\mathrm{t\\ ha^{-1}}$\n$V = 300\\,\\mathrm{USD\\ t^{-1}}$\n$k = 0.6\\,\\mathrm{(L\\ ha^{-1})^{-1}}$\n$c = 180\\,\\mathrm{USD\\ L^{-1}}$\n\nFirst, let us check the condition for an interior solution:\n$V k (Y_{\\max} - Y_{\\min}) = 300 \\times 0.6 \\times 4 = 720\\,\\mathrm{USD\\ L^{-1}}$.\nSince $720 > 180$, which is $c$, the condition is satisfied and an optimal dose $D^{\\ast}>0$ exists.\n\nSubstituting these values into the expression for $D^{\\ast}$:\n$$ D^{\\ast} = \\frac{1}{0.6}\\ln\\left(\\frac{300 \\times 0.6 \\times 4}{180}\\right) $$\n$$ D^{\\ast} = \\frac{1}{0.6}\\ln\\left(\\frac{720}{180}\\right) $$\n$$ D^{\\ast} = \\frac{1}{0.6}\\ln(4) $$\nUsing the value $\\ln(4) \\approx 1.386294$:\n$$ D^{\\ast} \\approx \\frac{1.386294}{0.6} \\approx 2.31049\\,\\mathrm{L\\ ha^{-1}} $$\nRounding the result to three significant figures as required by the problem statement, we get:\n$$ D^{\\ast} \\approx 2.31\\,\\mathrm{L\\ ha^{-1}} $$", "answer": "$$\n\\boxed{2.31}\n$$", "id": "2499149"}]}