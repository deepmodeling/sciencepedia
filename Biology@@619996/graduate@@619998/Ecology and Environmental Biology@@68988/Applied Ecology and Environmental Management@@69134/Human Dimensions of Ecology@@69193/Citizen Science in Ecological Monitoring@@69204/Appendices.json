{"hands_on_practices": [{"introduction": "The single-season occupancy model is a cornerstone of modern ecological analysis, particularly for processing detection/non-detection data commonly generated by citizen science programs. Before applying or extending this powerful tool, it's crucial to understand its probabilistic engine. This practice guides you through deriving the model's likelihood function from first principles, solidifying your understanding of how it links latent ecological states like species presence ($\\psi$) to observable data under imperfect detection ($p$) [@problem_id:2476164].", "problem": "A regional biodiversity observatory uses volunteers in a citizen science program to monitor the presence of a focal amphibian species at individual wetland sites during a single season. For a given site, volunteers visit the site $K$ times following a standardized protocol. On each visit $i \\in \\{1,\\dots,K\\}$ they record a binary detection indicator $y_i \\in \\{0,1\\}$, where $y_i=1$ denotes at least one detection and $y_i=0$ denotes no detection. Assume the standard single-season occupancy framework with the following scientifically grounded assumptions: a latent occupancy state $Z \\in \\{0,1\\}$ is constant across the season (closure assumption), the site is occupied with probability $\\psi$, detections on different visits are conditionally independent given $Z$, the per-visit detection probability given $Z=1$ is a constant $p \\in (0,1)$ across visits, and there are no false positives, so that if $Z=0$ then $y_i=0$ with probability $1$ for all $i \\in \\{1,\\dots,K\\}$. Using only core probabilistic principles (Bernoulli trials, conditional independence, and the law of total probability), derive the likelihood for the observed detection history $y=(y_1,\\dots,y_K)$ at this site as a closed-form function of $\\psi$, $p$, $K$, and $y$. Express your final answer as a single analytic expression in terms of $\\psi$, $p$, $K$, and $y_1,\\dots,y_K$. No rounding is required.", "solution": "The problem asks for the derivation of the likelihood for an observed detection history $y=(y_1,\\dots,y_K)$ from a single-season occupancy model. First, a validation of the problem statement is required.\n\nStep 1: Extract Givens.\nThe problem provides the following:\n- A site is visited $K$ times, where $K$ is a positive integer.\n- The detection history is a vector $y=(y_1,\\dots,y_K)$, where $y_i \\in \\{0,1\\}$ is the binary detection indicator for visit $i$.\n- A latent occupancy state $Z \\in \\{0,1\\}$ is constant for the season, where $Z=1$ means occupied and $Z=0$ means unoccupied.\n- The probability of occupancy is $P(Z=1) = \\psi$.\n- The probability of non-occupancy is $P(Z=0) = 1-\\psi$.\n- The per-visit detection probability, given the site is occupied, is $P(y_i=1 | Z=1) = p$, where $p \\in (0,1)$. This implies the probability of non-detection on a visit, given occupancy, is $P(y_i=0 | Z=1) = 1-p$.\n- There are no false positives, meaning $P(y_i=1 | Z=0) = 0$. This implies $P(y_i=0 | Z=0) = 1$.\n- Detections on different visits are conditionally independent given the latent state $Z$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded:** The problem describes the standard single-season occupancy model (MacKenzie et al., 2002), which is a fundamental and widely accepted framework in ecology and conservation biology for analyzing detection/non-detection data. All assumptions are standard for this model.\n- **Well-Posed:** The problem is clearly defined. It requests the derivation of a likelihood function based on a complete and consistent set of probabilistic definitions and assumptions. A unique, meaningful solution exists.\n- **Objective:** The language is formal, precise, and devoid of any subjective or non-scientific content.\n\nThe problem statement does not violate any of the specified invalidity criteria. It is scientifically sound, well-posed, objective, complete, and formalizable.\n\nStep 3: Verdict and Action.\nThe problem is deemed **valid**. A solution will be derived.\n\nThe objective is to derive the likelihood function $L(\\psi, p | y)$, which is equivalent to the probability of observing the specific detection history $y=(y_1, \\dots, y_K)$ given the parameters $\\psi$ and $p$. We denote this as $P(y | \\psi, p)$.\n\nThe observation of vector $y$ is conditional on the unobserved (latent) state of occupancy, $Z$. We must therefore use the law of total probability to express $P(y)$ by marginalizing over the two possible states of $Z$:\n$$P(y) = P(y | Z=1) P(Z=1) + P(y | Z=0) P(Z=0)$$\n\nWe are given $P(Z=1)=\\psi$ and $P(Z=0)=1-\\psi$. We must now derive the conditional probabilities $P(y|Z=1)$ and $P(y|Z=0)$.\n\nFirst, consider the case where the site is occupied ($Z=1$). The problem states that individual detections $y_i$ are conditionally independent given $Z$. Therefore, the probability of the entire history $y$ given $Z=1$ is the product of the probabilities of the individual observations:\n$$P(y | Z=1) = P(y_1, \\dots, y_K | Z=1) = \\prod_{i=1}^{K} P(y_i | Z=1)$$\nFor each visit $i$, the outcome $y_i$ given $Z=1$ follows a Bernoulli trial with success probability $p$. The probability mass function for a single observation $y_i$ can be written concisely as $p^{y_i} (1-p)^{1-y_i}$. If $y_i=1$, this gives $p^1(1-p)^0=p$. If $y_i=0$, this gives $p^0(1-p)^1=1-p$. Substituting this into the product gives the expression for the joint probability:\n$$P(y | Z=1) = \\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i}$$\n\nSecond, consider the case where the site is unoccupied ($Z=0$). Again, using conditional independence:\n$$P(y | Z=0) = P(y_1, \\dots, y_K | Z=0) = \\prod_{i=1}^{K} P(y_i | Z=0)$$\nThe assumption of no false positives states that $P(y_i=1|Z=0)=0$ and consequently $P(y_i=0|Z=0)=1$. We can write a general expression for $P(y_i|Z=0)$ using the value of $y_i$. Note that if $y_i=1$, the probability is $0$. If $y_i=0$, the probability is $1$. This corresponds exactly to the expression $(1-y_i)$. Thus, $P(y_i|Z=0) = (1-y_i)$.\nSubstituting this into the product:\n$$P(y | Z=0) = \\prod_{i=1}^{K} (1-y_i)$$\nThis product has the property that if any $y_i=1$, one term in the product will be $(1-1)=0$, making the entire product zero. This correctly reflects that it is impossible to observe a detection if the site is unoccupied. If all $y_i=0$, then every term is $(1-0)=1$, and the product is $1$.\n\nFinally, we substitute these two conditional probabilities back into the law of total probability:\n$$P(y | \\psi, p) = \\left( \\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i} \\right) P(Z=1) + \\left( \\prod_{i=1}^{K} (1-y_i) \\right) P(Z=0)$$\nReplacing $P(Z=1)$ with $\\psi$ and $P(Z=0)$ with $(1-\\psi)$ yields the final likelihood expression:\n$$L(\\psi, p | y_1, \\dots, y_K) = \\psi \\left( \\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i} \\right) + (1-\\psi) \\left( \\prod_{i=1}^{K} (1-y_i) \\right)$$\nThis is the required closed-form function. It is a single analytic expression that correctly handles both cases:\n1. If at least one detection occurs (some $y_j=1$), the term $\\prod_{i=1}^{K} (1-y_i)$ becomes $0$, and the likelihood simplifies to $L = \\psi \\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i}$, indicating the site must be occupied.\n2. If no detections occur (all $y_i=0$), then $\\prod_{i=1}^{K} (1-y_i) = 1$ and $\\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i} = \\prod_{i=1}^{K} (1-p) = (1-p)^K$. The likelihood becomes $L = \\psi (1-p)^K + (1-\\psi)$, representing the sum of probabilities of two scenarios: the site being occupied but the species was missed, and the site being truly unoccupied.\nThis completes the derivation.", "answer": "$$\n\\boxed{\\psi \\left( \\prod_{i=1}^{K} p^{y_i} (1-p)^{1-y_i} \\right) + (1-\\psi) \\left( \\prod_{i=1}^{K} (1-y_i) \\right)}\n$$", "id": "2476164"}, {"introduction": "While the standard occupancy model accounts for imperfect detection, it assumes perfect species identification—a condition often unmet in citizen science projects where observer skill varies. This leads to both false-positive ($\\alpha$) and false-negative ($\\beta$) errors. This exercise moves from the idealized model to a more realistic scenario by asking you to analytically derive the bias in a naive occupancy estimate when such misclassification is ignored [@problem_id:2476129]. Mastering this derivation provides critical insight into how data quality issues can systematically distort ecological conclusions.", "problem": "Citizen-science observers conduct presence–absence surveys for a single focal species at $n$ independent sites. Each site $i \\in \\{1,\\dots,n\\}$ is visited on $K \\geq 1$ independent occasions by observers drawn from a large pool of volunteers. Let the true occupancy state be $Z_i \\in \\{0,1\\}$, where $Z_i = 1$ denotes that the species is truly present and $Z_i = 0$ denotes absence, with $Z_i \\sim \\mathrm{Bernoulli}(\\psi)$ and $0 < \\psi < 1$. On visit $j \\in \\{1,\\dots,K\\}$ to site $i$, the observer reports a binary identification $R_{ij} \\in \\{0,1\\}$, where $R_{ij} = 1$ denotes a reported presence. Assume conditional independence of reports across visits given $Z_i$. Define false-negative and false-positive identification errors per visit as follows:\n- The false-negative rate $\\beta$ is the per-visit probability of reporting absence when the species is truly present, i.e., $\\beta = \\mathbb{P}(R_{ij} = 0 \\mid Z_i = 1)$ with $0 \\leq \\beta < 1$.\n- The false-positive rate $\\alpha$ is the per-visit probability of reporting presence when the species is truly absent, i.e., $\\alpha = \\mathbb{P}(R_{ij} = 1 \\mid Z_i = 0)$ with $0 \\leq \\alpha < 1$.\nAll observers share the same $(\\alpha,\\beta)$, and $(\\alpha,\\beta)$ do not vary across sites or visits.\n\nSuppose an analyst ignores misclassification and defines a site-level detection indicator $Y_i = \\mathbb{I}\\!\\left(\\max_{1 \\leq j \\leq K} R_{ij} = 1\\right)$, then estimates occupancy by the naive estimator $\\hat{\\psi}_{\\mathrm{naive}} = \\frac{1}{n}\\sum_{i=1}^n Y_i$. Under the law of large numbers, as $n \\to \\infty$, $\\hat{\\psi}_{\\mathrm{naive}}$ converges in probability to $\\mathbb{P}(Y_i = 1)$.\n\nStarting only from the definitions above and the laws of conditional probability and independence, derive the large-sample bias of the naive estimator when misclassification is ignored, defined as\n$$\nb(\\psi,\\alpha,\\beta,K) \\equiv \\lim_{n \\to \\infty} \\mathbb{E}\\!\\left[\\hat{\\psi}_{\\mathrm{naive}}\\right] - \\psi,\n$$\nand express $b(\\psi,\\alpha,\\beta,K)$ in closed form as a function of $\\psi$, $\\alpha$, $\\beta$, and $K$. Provide the final answer as a single analytical expression. No numerical rounding is needed, and no units are required.", "solution": "The problem requires the derivation of the large-sample bias of the naive occupancy estimator, $\\hat{\\psi}_{\\mathrm{naive}}$. The bias is defined as $b(\\psi,\\alpha,\\beta,K) \\equiv \\lim_{n \\to \\infty} \\mathbb{E}\\!\\left[\\hat{\\psi}_{\\mathrm{naive}}\\right] - \\psi$.\n\nFirst, we must evaluate the large-sample expectation of the estimator $\\hat{\\psi}_{\\mathrm{naive}} = \\frac{1}{n}\\sum_{i=1}^n Y_i$. The sites are independent and identically distributed, which implies that the random variables $Y_i$ are also independent and identically distributed. The expectation of the estimator for any finite sample size $n$ is:\n$$ \\mathbb{E}\\!\\left[\\hat{\\psi}_{\\mathrm{naive}}\\right] = \\mathbb{E}\\!\\left[\\frac{1}{n}\\sum_{i=1}^n Y_i\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[Y_i] $$\nSince the $Y_i$ are identically distributed, $\\mathbb{E}[Y_i]$ is the same for all $i$. Let us denote this a generic $\\mathbb{E}[Y]$. The variable $Y_i$ is an indicator variable, $Y_i \\in \\{0, 1\\}$. Its expectation is the probability of the event it indicates: $\\mathbb{E}[Y_i] = \\mathbb{P}(Y_i=1)$.\nTherefore, for any $n$,\n$$ \\mathbb{E}\\!\\left[\\hat{\\psi}_{\\mathrm{naive}}\\right] = \\frac{1}{n} \\cdot n \\cdot \\mathbb{P}(Y_i=1) = \\mathbb{P}(Y_i=1) $$\nThe limit as $n \\to \\infty$ is thus also $\\mathbb{P}(Y_i=1)$. The problem statement also asserts this by noting that $\\hat{\\psi}_{\\mathrm{naive}}$ converges in probability to this value. The large-sample bias is:\n$$ b(\\psi,\\alpha,\\beta,K) = \\mathbb{P}(Y_i=1) - \\psi $$\nOur task reduces to finding a closed-form expression for $\\mathbb{P}(Y_i=1)$. We use the law of total probability, conditioning on the true occupancy state $Z_i$ of site $i$.\n$$ \\mathbb{P}(Y_i=1) = \\mathbb{P}(Y_i=1 \\mid Z_i=1)\\mathbb{P}(Z_i=1) + \\mathbb{P}(Y_i=1 \\mid Z_i=0)\\mathbb{P}(Z_i=0) $$\nFrom the problem statement, we are given $\\mathbb{P}(Z_i=1) = \\psi$ and consequently $\\mathbb{P}(Z_i=0) = 1-\\psi$. Substituting these gives:\n$$ \\mathbb{P}(Y_i=1) = \\psi \\cdot \\mathbb{P}(Y_i=1 \\mid Z_i=1) + (1-\\psi) \\cdot \\mathbb{P}(Y_i=1 \\mid Z_i=0) $$\nNext, we must compute the two conditional probabilities.\n\nThe term $\\mathbb{P}(Y_i=1 \\mid Z_i=1)$ is the probability of observing at least one detection at a site that is truly occupied. The event $Y_i=1$ is defined as $\\max_{1 \\leq j \\leq K} R_{ij} = 1$, which is equivalent to at least one $R_{ij}$ being equal to $1$. It is more direct to compute the probability of the complement event, $Y_i=0$, which corresponds to $\\max_{1 \\leq j \\leq K} R_{ij} = 0$, or $R_{ij}=0$ for all $j \\in \\{1,\\dots,K\\}$.\n$$ \\mathbb{P}(Y_i=1 \\mid Z_i=1) = 1 - \\mathbb{P}(Y_i=0 \\mid Z_i=1) = 1 - \\mathbb{P}(R_{i1}=0, R_{i2}=0, \\dots, R_{iK}=0 \\mid Z_i=1) $$\nThe reports $R_{ij}$ are conditionally independent across visits $j$ given the true state $Z_i$. Therefore:\n$$ \\mathbb{P}(R_{i1}=0, \\dots, R_{iK}=0 \\mid Z_i=1) = \\prod_{j=1}^{K} \\mathbb{P}(R_{ij}=0 \\mid Z_i=1) $$\nThe probability $\\mathbb{P}(R_{ij}=0 \\mid Z_i=1)$ is the false-negative rate, given as $\\beta$. This rate is constant for all visits. So, the product becomes $\\beta^K$.\nThus, the probability of detecting an occupied site is:\n$$ \\mathbb{P}(Y_i=1 \\mid Z_i=1) = 1 - \\beta^K $$\nThe term $\\mathbb{P}(Y_i=1 \\mid Z_i=0)$ is the probability of observing at least one detection at a site that is truly unoccupied. This corresponds to a site-level false positive. We again compute this via the complement:\n$$ \\mathbb{P}(Y_i=1 \\mid Z_i=0) = 1 - \\mathbb{P}(Y_i=0 \\mid Z_i=0) = 1 - \\mathbb{P}(R_{i1}=0, \\dots, R_{iK}=0 \\mid Z_i=0) $$\nUsing conditional independence:\n$$ \\mathbb{P}(R_{i1}=0, \\dots, R_{iK}=0 \\mid Z_i=0) = \\prod_{j=1}^{K} \\mathbb{P}(R_{ij}=0 \\mid Z_i=0) $$\nThe probability of a false-positive report is $\\mathbb{P}(R_{ij}=1 \\mid Z_i=0) = \\alpha$. The probability of correctly reporting an absence at an unoccupied site is the complement, $\\mathbb{P}(R_{ij}=0 \\mid Z_i=0) = 1 - \\alpha$. As this is constant for all visits, the product becomes $(1-\\alpha)^K$.\nThus, the site-level false-positive probability is:\n$$ \\mathbb{P}(Y_i=1 \\mid Z_i=0) = 1 - (1-\\alpha)^K $$\nNow we substitute these two conditional probabilities back into the expression for $\\mathbb{P}(Y_i=1)$:\n$$ \\mathbb{P}(Y_i=1) = \\psi (1 - \\beta^K) + (1-\\psi) (1 - (1-\\alpha)^K) $$\nFinally, we compute the bias $b(\\psi,\\alpha,\\beta,K) = \\mathbb{P}(Y_i=1) - \\psi$:\n$$ b(\\psi,\\alpha,\\beta,K) = \\left[ \\psi(1 - \\beta^K) + (1-\\psi)(1 - (1-\\alpha)^K) \\right] - \\psi $$\nWe expand and simplify the expression:\n$$ b(\\psi,\\alpha,\\beta,K) = \\psi - \\psi\\beta^K + (1-\\psi)(1 - (1-\\alpha)^K) - \\psi $$\n$$ b(\\psi,\\alpha,\\beta,K) = -\\psi\\beta^K + (1-\\psi)(1 - (1-\\alpha)^K) $$\nThis expression can be rearranged for clarity.\n$$ b(\\psi,\\alpha,\\beta,K) = (1-\\psi)\\left(1 - (1-\\alpha)^K\\right) - \\psi\\beta^K $$\nThis is the final closed-form expression for the large-sample bias. It consists of two terms: the positive bias component from false positives at unoccupied sites, $(1-\\psi)(1 - (1-\\alpha)^K)$, and the negative bias component from false negatives at occupied sites, $-\\psi\\beta^K$.", "answer": "$$\n\\boxed{(1-\\psi)(1-(1-\\alpha)^{K}) - \\psi\\beta^{K}}\n$$", "id": "2476129"}, {"introduction": "Having established that misclassification can introduce significant bias, the next logical step is to correct for it. This practice provides a direct, hands-on method for post-hoc correction using a misclassification matrix, a common tool for quantifying observer error. By inverting the misclassification process, you will learn to estimate the true underlying species frequencies from the raw, error-prone counts provided by observers, a vital skill for robust inference from citizen science data [@problem_id:2476114].", "problem": "A regional biodiversity observatory uses citizen science classifications from camera-trap images to monitor three visually similar bird species, labeled species $A$, species $B$, and species $C$. Each citizen classification may be incorrect. From an expert-validated subset, the project estimates a misclassification matrix $M$ with columns representing the true species and rows representing the observed citizen label, where the entry $M_{ij}$ is the probability that a true species $j$ individual is labeled as species $i$ by a citizen scientist. The matrix is\n$$\nM \\;=\\;\n\\begin{pmatrix}\n0.80 & 0.10 & 0.05 \\\\\n0.15 & 0.80 & 0.20 \\\\\n0.05 & 0.10 & 0.75\n\\end{pmatrix}.\n$$\nOver a field season, the citizen scientists submitted $N = 2000$ labeled images with observed counts $n^{\\mathrm{obs}} = (n^{\\mathrm{obs}}_A, n^{\\mathrm{obs}}_B, n^{\\mathrm{obs}}_C)^{\\top} = (900, 800, 300)^{\\top}$. Let $q = (q_A, q_B, q_C)^{\\top}$ denote the observed label proportions, defined by $q_i = n^{\\mathrm{obs}}_i / N$. Let $p = (p_A, p_B, p_C)^{\\top}$ denote the true underlying population label proportions of species $A$, $B$, and $C$ in the sampled images, assumed constant during the season.\n\nStarting only from the law of total probability and the definition of $M$ as conditional misclassification probabilities, derive a method to estimate $p$ from $q$ and $M$. Then, using the provided $M$ and $n^{\\mathrm{obs}}$, compute the corrected frequency estimate for species $A$, that is, the first component of your estimator of $p$. Express your final answer as a decimal fraction and round to four significant figures.\n\nAs part of your derivation, explain the conditions under which your method is numerically stable. Briefly describe at least one regularization strategy appropriate when numerical instability is a concern, and state its algebraic form. You do not need to compute any regularized numerical value. The final answer to submit is only the corrected frequency estimate for species $A$, expressed as a decimal fraction rounded to four significant figures.", "solution": "We begin from the law of total probability for label outcomes under misclassification. For a fixed true population proportion vector $p = (p_A, p_B, p_C)^{\\top}$ on the species simplex (that is, $p_i \\ge 0$ and $\\sum_i p_i = 1$), and a misclassification matrix $M$ with entries $M_{ij} = \\mathbb{P}(\\text{observed label } i \\mid \\text{true species } j)$ and column sums equal to $1$, the observed label distribution $q$ satisfies\n$$\nq \\;=\\; M\\,p.\n$$\nThis follows because for each observed label $i$, $q_i = \\sum_{j} \\mathbb{P}(\\text{observed } i \\mid \\text{true } j)\\,\\mathbb{P}(\\text{true } j) = \\sum_{j} M_{ij}\\,p_j$ by the law of total probability. In practice, we estimate $q$ by the empirical proportion vector $\\hat{q}$ with components $\\hat{q}_i = n^{\\mathrm{obs}}_i/N$.\n\nProvided that $M$ is nonsingular, the mapping from $p$ to $q$ is bijective in the interior of the simplex, and we can recover $p$ by matrix inversion:\n$$\n\\hat{p} \\;=\\; M^{-1}\\,\\hat{q}.\n$$\nThis is the unbiased linear deconvolution estimator of the true proportions under a correctly specified $M$.\n\nWe now compute $\\hat{p}$ for the given data. First, compute observed proportions:\n$$\n\\hat{q} \\;=\\; \\begin{pmatrix} \\hat{q}_A \\\\ \\hat{q}_B \\\\ \\hat{q}_C \\end{pmatrix} \\;=\\; \\frac{1}{N}\\begin{pmatrix} 900 \\\\ 800 \\\\ 300 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.45 \\\\ 0.40 \\\\ 0.15 \\end{pmatrix}.\n$$\nWe solve the linear system $M \\hat{p} = \\hat{q}$ for $\\hat{p} = (\\hat{p}_A, \\hat{p}_B, \\hat{p}_C)^{\\top}$. Writing the system componentwise,\n$$\n\\begin{aligned}\n0.80\\,\\hat{p}_A + 0.10\\,\\hat{p}_B + 0.05\\,\\hat{p}_C &= 0.45, \\\\\n0.15\\,\\hat{p}_A + 0.80\\,\\hat{p}_B + 0.20\\,\\hat{p}_C &= 0.40, \\\\\n0.05\\,\\hat{p}_A + 0.10\\,\\hat{p}_B + 0.75\\,\\hat{p}_C &= 0.15.\n\\end{aligned}\n$$\nFor exact arithmetic, we convert to rational form by clearing denominators. Substituting $\\hat{p}_A = 0.5625 - 0.125\\,\\hat{p}_B - 0.0625\\,\\hat{p}_C$ from the first equation into the latter two yields the reduced $2 \\times 2$ system\n$$\n\\begin{aligned}\n\\frac{25}{32}\\,\\hat{p}_B + \\frac{61}{320}\\,\\hat{p}_C &= \\frac{101}{320}, \\\\\n\\frac{3}{32}\\,\\hat{p}_B + \\frac{239}{320}\\,\\hat{p}_C &= \\frac{39}{320}.\n\\end{aligned}\n$$\nMultiplying both equations by $320$ to clear denominators gives\n$$\n\\begin{aligned}\n250\\,\\hat{p}_B + 61\\,\\hat{p}_C &= 101, \\\\\n30\\,\\hat{p}_B + 239\\,\\hat{p}_C &= 39.\n\\end{aligned}\n$$\nSolving, from the second equation $\\hat{p}_B = \\frac{39 - 239\\,\\hat{p}_C}{30}$; substituting into the first gives\n$$\n250\\left(\\frac{39 - 239\\,\\hat{p}_C}{30}\\right) + 61\\,\\hat{p}_C \\;=\\; 101 \\;\\;\\Longrightarrow\\;\\; \\hat{p}_C \\;=\\; \\frac{21}{181}.\n$$\nThen\n$$\n\\hat{p}_B \\;=\\; \\frac{39 - 239 \\cdot \\frac{21}{181}}{30} \\;=\\; \\frac{68}{181}, \\qquad \\hat{p}_A \\;=\\; 1 - \\hat{p}_B - \\hat{p}_C \\;=\\; \\frac{92}{181}.\n$$\nThus the corrected frequency estimate for species $A$ is $\\hat{p}_A = \\frac{92}{181} \\approx 0.50829$. Rounding to four significant figures yields $0.5083$.\n\nWe now discuss numerical stability and regularization. The inversion step $\\hat{p} = M^{-1}\\hat{q}$ is numerically stable if and only if $M$ is well-conditioned. A standard measure is the condition number in the matrix $2$-norm, $\\kappa_{2}(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$, where $\\sigma_{\\max}(M)$ and $\\sigma_{\\min}(M)$ are the largest and smallest singular values from the Singular Value Decomposition (SVD). When $\\kappa_{2}(M)$ is large (that is, $\\sigma_{\\min}(M)$ is small), small sampling noise in $\\hat{q}$ is amplified in $\\hat{p}$, leading to high variance and possibly negative components. In the citizen science context, $M$ becomes ill-conditioned when species are frequently confused (columns of $M$ nearly collinear), for example if off-diagonal misclassification probabilities are high and similar across species.\n\nWhen instability is a concern, one may use Tikhonov regularization (also called ridge regularization). In its unconstrained quadratic form with identity penalty, it estimates\n$$\n\\hat{p}_{\\lambda} \\;=\\; \\arg\\min_{x \\in \\mathbb{R}^{3}} \\;\\big\\| M x - \\hat{q} \\big\\|_{2}^{2} \\;+\\; \\lambda \\big\\| x \\big\\|_{2}^{2},\n$$\nwhose closed-form solution is\n$$\n\\hat{p}_{\\lambda} \\;=\\; \\big(M^{\\top} M + \\lambda I\\big)^{-1} M^{\\top} \\hat{q},\n$$\nwith $\\lambda > 0$ and $I$ the identity matrix. To respect the simplex constraints $\\{x \\ge 0, \\; \\mathbf{1}^{\\top}x = 1\\}$, one can instead solve the constrained Tikhonov problem\n$$\n\\min_{x \\ge 0, \\;\\mathbf{1}^{\\top}x = 1} \\;\\big\\| M x - \\hat{q} \\big\\|_{2}^{2} \\;+\\; \\lambda \\big\\| L x \\big\\|_{2}^{2},\n$$\nwith $L = I$ (ridge) or another smoothing operator, via quadratic programming. Alternative regularization strategies include Bayesian estimators with Dirichlet priors on $p$ or early-stopped iterative deconvolution. In all cases, larger $\\lambda$ trades bias against variance and reduces sensitivity to noise and to small singular values of $M$.", "answer": "$$\\boxed{0.5083}$$", "id": "2476114"}]}