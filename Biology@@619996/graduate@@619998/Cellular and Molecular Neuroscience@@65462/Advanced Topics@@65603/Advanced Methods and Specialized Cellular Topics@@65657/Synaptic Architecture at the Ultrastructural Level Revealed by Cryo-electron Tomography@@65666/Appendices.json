{"hands_on_practices": [{"introduction": "The journey from a raw cryo-electron tomogram to biological insight begins with understanding and correcting the data itself. A tomogram is not a perfect snapshot of cellular reality; it is a convolution of the true structure with the microscope's Point Spread Function (PSF), which describes the blurring inherent to the imaging process. This exercise [@problem_id:2757132] tackles this fundamental concept head-on, demonstrating how anisotropic resolution—a common artifact—can distort the apparent shape of an isotropic object like a synaptic vesicle. You will develop a principled deconvolution method to mathematically reverse this distortion and recover the object's true dimensions, a critical first step for any accurate quantitative analysis.", "problem": "Design and implement a program that simulates how anisotropic resolution in Cryogenic Electron Tomography (Cryo-ET) leads to apparent elongation of synaptic vesicles and then derives a principled deconvolution-based correction to recover the true vesicle radii. Use the following physical and mathematical bases as starting points only: the recorded image can be modeled as a convolution of the underlying object intensity with the imaging Point Spread Function (PSF); the PSF can be approximated by an anisotropic three-dimensional Gaussian; and the convolution of Gaussian functions yields another Gaussian whose covariance matrix equals the sum of the covariances of the convolved Gaussians. You must not assume any additional result beyond these bases.\n\nAssume each vesicle can be modeled as an isotropic three-dimensional Gaussian intensity distribution with an unknown true scale parameter (the target \"radius parameter\") denoted by $r$ (in $\\mathrm{nm}$). Assume the anisotropic PSF is a three-dimensional Gaussian with standard deviations $\\sigma_x$, $\\sigma_y$, and $\\sigma_z$ (in $\\mathrm{nm}$) along the orthogonal axes. The convolution of the vesicle intensity with the PSF yields a measured intensity whose apparent spreads along the three axes are influenced by both $r$ and $\\sigma_x,\\sigma_y,\\sigma_z$, causing apparent elongation along axes with poorer resolution. From this model and the Gaussian-convolution property, derive a deconvolution procedure that, given the apparent axis-aligned spreads, recovers an estimator for the true isotropic radius parameter $r$.\n\nYour program must perform the following for each test case:\n- Simulate the apparent axis-aligned standard deviations along the $x$, $y$, and $z$ axes that would be measured from the convolved intensity, using only the bases specified above.\n- From these apparent measurements and the known PSF anisotropy, derive and implement a deconvolution-based estimator to recover the true isotropic radius parameter $r$ (in $\\mathrm{nm}$).\n- Output the recovered $r$ values, each rounded to $3$ decimals, in a single Python list literal on one line.\n\nUnits: All lengths are in $\\mathrm{nm}$. Your outputs must be in $\\mathrm{nm}$.\n\nAngle units: Not applicable.\n\nPercentages: Not applicable.\n\nTest Suite (scientifically plausible Cryo-ET conditions at synapses):\n- Case $1$: $r = 20\\,\\mathrm{nm}$, $\\sigma_x = 3\\,\\mathrm{nm}$, $\\sigma_y = 3\\,\\mathrm{nm}$, $\\sigma_z = 9\\,\\mathrm{nm}$.\n- Case $2$: $r = 25\\,\\mathrm{nm}$, $\\sigma_x = 4\\,\\mathrm{nm}$, $\\sigma_y = 5\\,\\mathrm{nm}$, $\\sigma_z = 10\\,\\mathrm{nm}$.\n- Case $3$: $r = 22\\,\\mathrm{nm}$, $\\sigma_x = 3\\,\\mathrm{nm}$, $\\sigma_y = 3\\,\\mathrm{nm}$, $\\sigma_z = 3\\,\\mathrm{nm}$.\n- Case $4$: $r = 12\\,\\mathrm{nm}$, $\\sigma_x = 2\\,\\mathrm{nm}$, $\\sigma_y = 2\\,\\mathrm{nm}$, $\\sigma_z = 8\\,\\mathrm{nm}$.\n- Case $5$: $r = 28\\,\\mathrm{nm}$, $\\sigma_x = 2\\,\\mathrm{nm}$, $\\sigma_y = 3\\,\\mathrm{nm}$, $\\sigma_z = 12\\,\\mathrm{nm}$.\n\nFinal Output Format: Your program should produce a single line of output containing the recovered radii, rounded to $3$ decimals, as a comma-separated list enclosed in square brackets (for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$). The results must be in the order of the cases above.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of Fourier optics as applied to microscopy, mathematically well-posed, and objective in its formulation. The model of image formation as a convolution of an object with a Point Spread Function (PSF), and the approximation of these entities by Gaussian distributions, is a standard and physically meaningful simplification in the field of image analysis. The parameters provided are plausible for Cryogenic Electron Tomography (Cryo-ET) at synaptic junctions. The task requires the derivation of a restoration algorithm from first principles, which is a legitimate scientific exercise.\n\nThe solution proceeds as follows. First, we establish the mathematical model for the synaptic vesicle and the imaging PSF. Second, we use the given convolution property to derive the forward model, which predicts the apparent, measured dimensions of the vesicle in the tomogram. Third, we invert this model to derive a principled deconvolution-based estimator for the true vesicle radius parameter.\n\nLet the true synaptic vesicle's three-dimensional intensity distribution be modeled as an isotropic Gaussian function, $I_{\\text{vesicle}}$. Its shape is defined by a single scale parameter, the true radius parameter $r$, which represents the standard deviation along any axis. The corresponding covariance matrix, $C_{\\text{vesicle}}$, is a diagonal matrix with identical variance $r^2$ for each dimension:\n$$\nC_{\\text{vesicle}} = \\begin{pmatrix} r^2 & 0 & 0 \\\\ 0 & r^2 & 0 \\\\ 0 & 0 & r^2 \\end{pmatrix}\n$$\n\nThe imaging system's Point Spread Function, $I_{\\text{PSF}}$, is modeled as an anisotropic three-dimensional Gaussian. The anisotropy arises from instrument limitations, such as the \"missing wedge\" effect in tomography, leading to poorer resolution along one axis (typically designated as the $z$-axis). Its standard deviations are given as $\\sigma_x$, $\\sigma_y$, and $\\sigma_z$. The corresponding covariance matrix, $C_{\\text{PSF}}$, is:\n$$\nC_{\\text{PSF}} = \\begin{pmatrix} \\sigma_x^2 & 0 & 0 \\\\ 0 & \\sigma_y^2 & 0 \\\\ 0 & 0 & \\sigma_z^2 \\end{pmatrix}\n$$\n\nThe recorded image, $I_{\\text{obs}}$, is the convolution of the true object's intensity with the PSF: $I_{\\text{obs}} = I_{\\text{vesicle}} \\ast I_{\\text{PSF}}$. A fundamental property of Gaussian functions, provided in the problem statement, is that their convolution results in another Gaussian. The covariance matrix of the resulting Gaussian, $C_{\\text{obs}}$, is the sum of the covariance matrices of the functions being convolved.\n\nThis constitutes the forward model of image formation:\n$$\nC_{\\text{obs}} = C_{\\text{vesicle}} + C_{\\text{PSF}}\n$$\nSubstituting the matrix definitions:\n$$\nC_{\\text{obs}} = \\begin{pmatrix} r^2 & 0 & 0 \\\\ 0 & r^2 & 0 \\\\ 0 & 0 & r^2 \\end{pmatrix} + \\begin{pmatrix} \\sigma_x^2 & 0 & 0 \\\\ 0 & \\sigma_y^2 & 0 \\\\ 0 & 0 & \\sigma_z^2 \\end{pmatrix} = \\begin{pmatrix} r^2 + \\sigma_x^2 & 0 & 0 \\\\ 0 & r^2 + \\sigma_y^2 & 0 \\\\ 0 & 0 & r^2 + \\sigma_z^2 \\end{pmatrix}\n$$\n\nThe diagonal elements of $C_{\\text{obs}}$ are the variances of the observed intensity distribution along each axis. Let us denote the observed, or \"apparent,\" standard deviations as $s_{\\text{obs},x}$, $s_{\\text{obs},y}$, and $s_{\\text{obs},z}$. Their squares, the variances, are directly obtained from $C_{\\text{obs}}$:\n$$\ns_{\\text{obs},x}^2 = r^2 + \\sigma_x^2\n$$\n$$\ns_{\\text{obs},y}^2 = r^2 + \\sigma_y^2\n$$\n$$\ns_{\\text{obs},z}^2 = r^2 + \\sigma_z^2\n$$\nThese equations represent the simulation of the apparent axis-aligned spreads. Anisotropy in the PSF (e.g., $\\sigma_z > \\sigma_x$) leads to a corresponding elongation in the observed image ($s_{\\text{obs},z} > s_{\\text{obs},x}$), even if the underlying object is perfectly isotropic.\n\nNow, we address the inverse problem: deriving a deconvolution-based estimator for the true radius parameter $r$. The task is to recover $r$ from the measured apparent variances ($s_{\\text{obs},x}^2$, $s_{\\text{obs},y}^2$, $s_{\\text{obs},z}^2$) and the known PSF variances ($\\sigma_x^2$, $\\sigma_y^2$, $\\sigma_z^2$). We can rearrange the forward model equations to solve for $r^2$ from each axis measurement:\n$$\nr^2 = s_{\\text{obs},x}^2 - \\sigma_x^2 \\quad (\\text{from the } x\\text{-axis measurement})\n$$\n$$\nr^2 = s_{\\text{obs},y}^2 - \\sigma_y^2 \\quad (\\text{from the } y\\text{-axis measurement})\n$$\n$$\nr^2 = s_{\\text{obs},z}^2 - \\sigma_z^2 \\quad (\\text{from the } z\\text{-axis measurement})\n$$\nThis is the mathematical essence of deconvolution in this context: subtracting the blurring contribution (PSF variance) from the observed signal variance to recover the object's intrinsic variance.\n\nIn an ideal, noise-free measurement, each of these three equations would yield the exact same value for $r^2$. In practice, measurement noise would cause slight discrepancies. A principled estimator should combine the information from all three independent measurements to produce a single, more robust estimate. The most direct and robust method is to average the three independent estimates of $r^2$:\n$$\nr_{\\text{est}}^2 = \\frac{(s_{\\text{obs},x}^2 - \\sigma_x^2) + (s_{\\text{obs},y}^2 - \\sigma_y^2) + (s_{\\text{obs},z}^2 - \\sigma_z^2)}{3}\n$$\nThe final estimator for the true radius parameter $r$ is then the square root of this averaged variance:\n$$\nr_{\\text{est}} = \\sqrt{\\frac{(s_{\\text{obs},x}^2 - \\sigma_x^2) + (s_{\\text{obs},y}^2 - \\sigma_y^2) + (s_{\\text{obs},z}^2 - \\sigma_z^2)}{3}}\n$$\nTo verify the logical consistency of this estimator, we substitute the forward model expressions for $s_{\\text{obs}}^2$ into the equation for $r_{\\text{est}}^2$. Let the true radius parameter be $r_{\\text{true}}$:\n$$\ns_{\\text{obs},x}^2 = r_{\\text{true}}^2 + \\sigma_x^2\n$$\n$$\ns_{\\text{obs},y}^2 = r_{\\text{true}}^2 + \\sigma_y^2\n$$\n$$\ns_{\\text{obs},z}^2 = r_{\\text{true}}^2 + \\sigma_z^2\n$$\nSubstituting these into the estimator for $r_{\\text{est}}^2$:\n$$\nr_{\\text{est}}^2 = \\frac{((r_{\\text{true}}^2 + \\sigma_x^2) - \\sigma_x^2) + ((r_{\\text{true}}^2 + \\sigma_y^2) - \\sigma_y^2) + ((r_{\\text{true}}^2 + \\sigma_z^2) - \\sigma_z^2)}{3}\n$$\n$$\nr_{\\text{est}}^2 = \\frac{r_{\\text{true}}^2 + r_{\\text{true}}^2 + r_{\\text{true}}^2}{3} = \\frac{3 \\cdot r_{\\text{true}}^2}{3} = r_{\\text{true}}^2\n$$\nThus, $r_{\\text{est}} = \\sqrt{r_{\\text{true}}^2} = r_{\\text{true}}$. The derived estimator perfectly recovers the true radius parameter in this idealized, noise-free model. This confirms the validity of our deconvolution procedure.\n\nThe program to be implemented will perform these steps for each test case:\n$1$. Given the true radius parameter $r$ and PSF parameters $\\sigma_x$, $\\sigma_y$, $\\sigma_z$.\n$2$. Simulate the apparent variances: $s_{\\text{obs},x}^2 = r^2 + \\sigma_x^2$, $s_{\\text{obs},y}^2 = r^2 + \\sigma_y^2$, $s_{\\text{obs},z}^2 = r^2 + \\sigma_z^2$.\n$3$. Apply the derived deconvolution estimator to an average of the axis-specific estimates to recover $r$. As shown, this calculation will yield the original value of $r$. The program serves to implement and demonstrate this derived principle.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the recovered true isotropic radius parameter of a synaptic vesicle\n    after deconvolution from an anisotropic Point Spread Function (PSF).\n    \"\"\"\n\n    # Test Suite (scientifically plausible Cryo-ET conditions at synapses):\n    # Each case is a tuple: (r_true, sigma_x, sigma_y, sigma_z) in nm.\n    test_cases = [\n        # Case 1: r = 20 nm, sigma_x = 3 nm, sigma_y = 3 nm, sigma_z = 9 nm.\n        (20.0, 3.0, 3.0, 9.0),\n        # Case 2: r = 25 nm, sigma_x = 4 nm, sigma_y = 5 nm, sigma_z = 10 nm.\n        (25.0, 4.0, 5.0, 10.0),\n        # Case 3: r = 22 nm, sigma_x = 3 nm, sigma_y = 3 nm, sigma_z = 3 nm.\n        (22.0, 3.0, 3.0, 3.0),\n        # Case 4: r = 12 nm, sigma_x = 2 nm, sigma_y = 2 nm, sigma_z = 8 nm.\n        (12.0, 2.0, 2.0, 8.0),\n        # Case 5: r = 28 nm, sigma_x = 2 nm, sigma_y = 3 nm, sigma_z = 12 nm.\n        (28.0, 2.0, 3.0, 12.0),\n    ]\n\n    recovered_radii = []\n\n    for case in test_cases:\n        r_true, sigma_x, sigma_y, sigma_z = case\n\n        # The image formation model is a convolution, which in the case of Gaussians\n        # corresponds to the addition of variances.\n        # Step 1: Simulate the apparent (observed) variances along each axis.\n        # The observed variance is the sum of the true object variance (r_true^2)\n        # and the PSF variance (sigma^2).\n        var_obs_x = r_true**2 + sigma_x**2\n        var_obs_y = r_true**2 + sigma_y**2\n        var_obs_z = r_true**2 + sigma_z**2\n\n        # Step 2: Apply the deconvolution-based estimator to recover r.\n        # From the simulated measurements, we derive an estimate for the true\n        # variance (r^2) by subtracting the known PSF variance from the\n        # observed variance for each axis.\n        r_sq_est_x = var_obs_x - sigma_x**2\n        r_sq_est_y = var_obs_y - sigma_y**2\n        r_sq_est_z = var_obs_z - sigma_z**2\n\n        # A principled estimator combines the information from all three axes.\n        # We average the three independent estimates of the true variance.\n        r_sq_est_combined = (r_sq_est_x + r_sq_est_y + r_sq_est_z) / 3.0\n\n        # The final estimate for the radius parameter r is the square root of the\n        # estimated combined variance. In this idealized model, this will exactly\n        # recover the true radius parameter.\n        r_recovered = np.sqrt(r_sq_est_combined)\n\n        # Round the result to 3 decimal places as required.\n        recovered_radii.append(round(r_recovered, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, recovered_radii))}]\")\n\nsolve()\n```", "id": "2757132"}, {"introduction": "Once a tomogram is processed to reduce imaging artifacts, the next challenge is to identify the specific macromolecules within the crowded synaptic environment. Template matching, which compares segments of the tomogram to known atomic models, is a powerful technique for this purpose. However, a high correlation score alone is not sufficient; we must ask how likely it is that such a score could arise purely by chance. This practice [@problem_id:2757162] introduces a cornerstone of modern quantitative biology: controlling the False Discovery Rate (FDR) using a target-decoy strategy. This provides a rigorous statistical framework to assign confidence to your molecular identifications, ensuring your findings are robust and reliable.", "problem": "A cryogenic electron tomography (cryo-ET) study of excitatory synapses aims to assign individual receptor-containing subtomograms to either the alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid receptor (AMPAR) or the N-methyl-D-aspartate receptor (NMDAR) based on the maximum normalized cross-correlation score between each subtomogram and atomic models of AMPAR and NMDAR. For each subtomogram, the assignment is to the receptor model with the higher score. To estimate the rate of incorrect assignments arising from noise or model mismatch, a decoy strategy is used: for each subtomogram, a corresponding decoy score is computed as the maximum normalized cross-correlation against a library of randomized decoy volumes derived from AMPAR and NMDAR models. Assume the following:\n\n- The maximum target score for each subtomogram is denoted by $s_{t}$, defined as the maximum of the two target correlations (AMPAR and NMDAR).\n- The decoy score for each subtomogram is denoted by $s_{d}$, defined as the maximum across the decoy libraries for both receptor classes.\n- For a threshold $t$, a subtomogram is called a discovery if $s_{t} \\ge t$.\n- The false discovery rate (FDR) at threshold $t$ is defined as the expected fraction of false discoveries among all discoveries, and the decoy scores $s_{d}$ represent the null distribution of scores when there is no genuine match.\n\nAcross $N$ subtomograms, the following aggregated counts were measured for several thresholds $t$:\n- At $t = 0.38$: number of target discoveries $T(t) = 2680$, number of decoy exceedances $D(t) = 210$.\n- At $t = 0.40$: $T(t) = 2300$, $D(t) = 140$.\n- At $t = 0.42$: $T(t) = 1950$, $D(t) = 98$.\n- At $t = 0.44$: $T(t) = 1600$, $D(t) = 64$.\n- At $t = 0.46$: $T(t) = 1280$, $D(t) = 41$.\n\nStarting from the definition of FDR and the use of decoys as an empirical null, derive an estimator for the FDR at threshold $t$ in terms of $T(t)$ and $D(t)$, state any assumptions you make explicit, and use it to determine the smallest threshold $\\theta$ among the listed values such that the estimated FDR is at most $0.05$.\n\nReport only the threshold $\\theta$ as a unitless value, rounded to three significant figures.", "solution": "The problem requires the derivation of an estimator for the False Discovery Rate (FDR) using a target-decoy strategy and its application to find a score threshold that meets a specified FDR criterion.\n\nFirst, we derive the estimator for the FDR at a given score threshold $t$. Let $T(t)$ be the number of target discoveries, which are subtomograms whose maximum correlation score $s_t$ satisfies $s_t \\ge t$. These discoveries are a mixture of true positives (TP, genuine receptor matches) and false positives (FP, spurious matches).\n$$T(t) = \\text{TP}(t) + \\text{FP}(t)$$\nThe False Discovery Rate, $\\text{FDR}(t)$, is the expected proportion of false positives among the total discoveries. A common estimator for the FDR is given by the ratio of the estimated number of false positives to the total number of discoveries:\n$$\\widehat{\\text{FDR}}(t) = \\frac{\\widehat{\\text{FP}}(t)}{T(t)}$$\nThe critical step is to estimate the number of false positives, $\\widehat{\\text{FP}}(t)$. The target-decoy approach is employed for this purpose. The fundamental assumption of this method is that the distribution of scores for the decoy dataset, which consists of randomized non-matching volumes, accurately models the distribution of scores for null targets (i.e., target subtomograms that do not contain a genuine receptor).\n\nLet $D(t)$ be the number of decoy scores $s_d$ that exceed the threshold $t$, i.e., $s_d \\ge t$. These are referred to as decoy exceedances. Let $N$ be the total number of subtomograms analyzed. The problem states that for each subtomogram, a target score $s_t$ and a corresponding decoy score $s_d$ are computed, implying that the size of the target set and the decoy set are both $N$.\n\nThe expected number of false positives is the number of true null hypotheses, $N_0$, multiplied by the probability that a null target scores above $t$.\n$$E[\\text{FP}(t)] = N_0 \\times P(s_t \\ge t | \\text{null})$$\nThe proportion of true nulls in the entire population of subtomograms is denoted by $\\pi_0 = N_0 / N$.\n$$E[\\text{FP}(t)] = \\pi_0 N \\times P(s_t \\ge t | \\text{null})$$\nUnder the primary assumption of the target-decoy strategy, the probability of a null target scoring above $t$ is equal to the probability of a decoy scoring above $t$.\n$$P(s_t \\ge t | \\text{null}) = P(s_d \\ge t)$$\nThis probability can be estimated empirically from the decoy data as the fraction of decoys exceeding the threshold $t$.\n$$\\widehat{P}(s_d \\ge t) = \\frac{D(t)}{N}$$\nSubstituting this into the expression for the expected number of false positives, we obtain an estimate:\n$$\\widehat{\\text{FP}}(t) = \\pi_0 N \\times \\frac{D(t)}{N} = \\pi_0 D(t)$$\nTherefore, the FDR estimator is:\n$$\\widehat{\\text{FDR}}(t) = \\frac{\\pi_0 D(t)}{T(t)}$$\nIn the absence of a method to estimate $\\pi_0$, the standard and conservative approach is to assume $\\pi_0 = 1$. This is equivalent to assuming that all target candidates are, a priori, nulls. This provides an upper bound on the FDR. With $\\pi_0 = 1$, the estimator simplifies to:\n$$\\widehat{\\text{FDR}}(t) = \\frac{D(t)}{T(t)}$$\nThis is the estimator we will use. To ensure the estimate is a valid proportion, it is formally defined as $\\min(1, D(t)/T(t))$, although in this case $D(t) \\le T(t)$ for all given data points.\n\nThe assumptions for this derivation are:\n1.  The score distribution of decoy entries is an accurate representation of the score distribution for null target entries.\n2.  The proportion of true nulls, $\\pi_0$, is conservatively set to $1$.\n3.  The number of decoy score calculations is equal to the number of target score calculations.\n\nNow, we apply this estimator to the provided data to find the smallest threshold $\\theta$ for which $\\widehat{\\text{FDR}}(\\theta) \\le 0.05$. We calculate the estimated FDR for each threshold $t$:\n\n-   At $t = 0.38$: $T(0.38) = 2680$, $D(0.38) = 210$.\n    $$\\widehat{\\text{FDR}}(0.38) = \\frac{210}{2680} \\approx 0.07836$$\n    Since $0.07836 > 0.05$, this threshold is not selected.\n\n-   At $t = 0.40$: $T(0.40) = 2300$, $D(0.40) = 140$.\n    $$\\widehat{\\text{FDR}}(0.40) = \\frac{140}{2300} \\approx 0.06087$$\n    Since $0.06087 > 0.05$, this threshold is not selected.\n\n-   At $t = 0.42$: $T(0.42) = 1950$, $D(0.42) = 98$.\n    $$\\widehat{\\text{FDR}}(0.42) = \\frac{98}{1950} \\approx 0.05026$$\n    Since $0.05026 > 0.05$, this threshold is not selected.\n\n-   At $t = 0.44$: $T(0.44) = 1600$, $D(0.44) = 64$.\n    $$\\widehat{\\text{FDR}}(0.44) = \\frac{64}{1600} = 0.04$$\n    Since $0.04 \\le 0.05$, this threshold is acceptable.\n\n-   At $t = 0.46$: $T(0.46) = 1280$, $D(0.46) = 41$.\n    $$\\widehat{\\text{FDR}}(0.46) = \\frac{41}{1280} \\approx 0.03203$$\n    Since $0.03203 \\le 0.05$, this threshold is also acceptable.\n\nThe problem asks for the smallest threshold $\\theta$ from the list that satisfies the condition $\\widehat{\\text{FDR}}(\\theta) \\le 0.05$. The set of acceptable thresholds is $\\{0.44, 0.46\\}$. The smallest value in this set is $0.44$.\nThe required threshold is $\\theta = 0.44$. The problem asks for this value to be rounded to three significant figures, which is $0.440$.", "answer": "$$\\boxed{0.440}$$", "id": "2757162"}, {"introduction": "With synaptic components accurately measured and confidently identified, we can address the core questions of neurobiology: how is the synapse organized to function as a sophisticated signaling machine? The spatial arrangement of molecules is often the key to their mechanism of action. This final hands-on practice [@problem_id:2757202] equips you with the tools of spatial point process analysis to quantify the functional architecture of the active zone. By calculating the cross-correlation between voltage-gated calcium channels and docked vesicles and then rigorously testing its statistical significance, you can probe the spatial coupling that is thought to enable rapid, efficient neurotransmission.", "problem": "You are given planar coordinates of voltage-gated calcium channel subtype 2 (Cav2) particles and synaptic vesicles segmented from cryo-electron tomography (cryo-ET) of the active zone, idealized as a circular domain. Use the cross pair-correlation function between Cav2 channel positions and the subset of vesicles labeled \"docked\" to quantify spatial association and evaluate statistical significance by permutation of vesicle docked labels.\n\nFundamental base. The cross pair-correlation function quantifies excess or deficit of pair density at separation radius relative to independent homogeneous reference processes. Let the active zone be a disk of radius $R$ with area $A = \\pi R^2$. Let the Cav2 channel positions be $\\{ \\mathbf{c}_j \\}_{j=1}^{N_c}$ and all vesicle positions be $\\{ \\mathbf{v}_i \\}_{i=1}^{N_v}$, of which a subset with indices $\\mathcal{D} \\subset \\{1,\\dots,N_v\\}$ are labeled docked. For a radial bin with inner edge $r_k$ and width $\\Delta r$, define the midpoint $m_k = r_k + \\Delta r / 2$ and the border-corrected estimator\n$$\n\\widehat{g}_{cv}(m_k) \\;=\\; \\frac{A}{N_c \\cdot M_k} \\cdot \\frac{1}{2\\pi m_k \\Delta r} \\cdot \\sum_{i \\in \\mathcal{D}} \\mathbf{1}\\!\\left[ d_{\\text{edge}}(\\mathbf{v}_i) \\ge r_k + \\Delta r \\right] \\sum_{j=1}^{N_c} \\mathbf{1}\\!\\left[ r_k \\le \\lVert \\mathbf{v}_i - \\mathbf{c}_j \\rVert &lt; r_k + \\Delta r \\right],\n$$\nwhere $d_{\\text{edge}}(\\mathbf{v}_i) = R - \\lVert \\mathbf{v}_i \\rVert$ is the distance from vesicle $i$ to the circular boundary (the origin is the center of the disk) and\n$$\nM_k \\;=\\; \\sum_{i \\in \\mathcal{D}} \\mathbf{1}\\!\\left[ d_{\\text{edge}}(\\mathbf{v}_i) \\ge r_k + \\Delta r \\right]\n$$\nis the number of docked vesicles whose annulus of radii $[r_k, r_k + \\Delta r)$ lies entirely inside the domain. Under spatial independence of Cav2 and docked vesicles with homogeneous intensities, the theoretical cross pair-correlation equals $g_{cv}(r) = 1$.\n\nPermutation test. Under the null hypothesis that “docked” labels are exchangeable among vesicles, define a summary statistic\n$$\nS \\;=\\; \\max_{k} \\left| \\widehat{g}_{cv}(m_k) - 1 \\right|\n$$\ncomputed over all bins with $M_k \\ge 1$. Generate $B$ random permutations of the docked labels that preserve the number of docked vesicles, recompute $S$ for each, and estimate the $p$-value as\n$$\np \\;=\\; \\frac{1 + \\#\\{ b \\in \\{1,\\dots,B\\} : S^{(b)} \\ge S^{\\text{obs}} \\}}{B + 1}.\n$$\n\nImplementation requirements.\n- Use the exact estimator defined above with the border correction as specified by the indicator $\\mathbf{1}\\!\\left[ d_{\\text{edge}}(\\mathbf{v}_i) \\ge r_k + \\Delta r \\right]$.\n- Use bin midpoints $m_k$ in the annular area factor $2\\pi m_k \\Delta r$ and include pair distances in half-open intervals $[r_k, r_k + \\Delta r)$.\n- Use the random number generator with seed $s = 0$ for reproducibility.\n- The final answers (the $p$-values) are dimensionless. No physical unit should be included in the output.\n- Your program must compute the $p$-values for all test cases below and print them on a single line as a comma-separated list enclosed in square brackets, rounded to $4$ decimal places, for example, $[0.1234,0.5678]$.\n\nTest suite. For all test cases, positions are in nanometers in a disk of radius $R$ (origin at the center), and angles are not used. All indices are zero-based.\n\n- Test case $1$ (approximately null association):\n  - $R = 150$, $\\Delta r = 10$, $r_{\\max} = 60$, $B = 2000$, $s = 0$.\n  - Cav2: $[(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)]$.\n  - Vesicles (all): $[(0,0),(20,20),(-25,15),(30,-10),(-40,-30),(70,0),(0,80),(-70,5),(15,-65),(90,40),(-90,-20),(10,110)]$.\n  - Docked indices: $[1,4,7,10]$.\n\n- Test case $2$ (enrichment of docked vesicles near Cav2):\n  - $R = 150$, $\\Delta r = 10$, $r_{\\max} = 60$, $B = 2000$, $s = 0$.\n  - Cav2: $[(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)]$.\n  - Vesicles (all): $[(70,0),(0,70),(-70,0),(0,-70),(0,0),(130,0),(0,130),(-130,0),(0,-130),(90,90),(-90,90),(-90,-90)]$.\n  - Docked indices: $[0,1,2,3]$.\n\n- Test case $3$ (depletion of docked vesicles near Cav2):\n  - $R = 150$, $\\Delta r = 10$, $r_{\\max} = 60$, $B = 2000$, $s = 0$.\n  - Cav2: $[(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)]$.\n  - Vesicles (all): $[(0,0),(10,0),(0,10),(-10,0),(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)]$.\n  - Docked indices: $[0,1,2,3]$.\n\n- Test case $4$ (edge-dominated configuration to exercise border correction):\n  - $R = 150$, $\\Delta r = 10$, $r_{\\max} = 50$, $B = 2000$, $s = 0$.\n  - Cav2: $[(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)]$.\n  - Vesicles (all): $[(140,0),(99,99),(0,140),(-99,99),(-140,0),(-99,-99),(0,-140),(99,-99),(0,0),(30,30),(-30,30),(-30,-30)]$.\n  - Docked indices: $[0,2,4,6]$.\n\nFinal output format. Your program should produce a single line of output containing the four $p$-values as a comma-separated list enclosed in square brackets, rounded to $4$ decimal places, in the exact order of the test cases, for example $[0.1234,0.5678,0.2222,0.9999]$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the principles of spatial statistics and cellular neuroscience, is well-posed with a clear and complete set of definitions and data, and poses a non-trivial computational task that is both feasible and algorithmically unambiguous.\n\nThe solution entails the implementation of a permutation test to assess the statistical significance of the spatial association between Cav2 channels and docked synaptic vesicles within a circular active zone. The core of this test is the cross pair-correlation function, $\\widehat{g}_{cv}(r)$, whose deviation from unity under the null hypothesis of spatial independence is quantified by a summary statistic, $S$.\n\nThe computational procedure is structured as follows:\n\nFirst, a function is designed to compute the summary statistic $S$ for a given configuration of docked vesicles. This function takes as input the coordinates of Cav2 channels $\\{\\mathbf{c}_j\\}_{j=1}^{N_c}$, the coordinates of all vesicles $\\{\\mathbf{v}_i\\}_{i=1}^{N_v}$, the set of indices $\\mathcal{D}$ corresponding to docked vesicles, the radius of the active zone $R$, the radial bin width $\\Delta r$, and the maximum radius of interest $r_{\\max}$. The analysis proceeds over a set of radial bins defined by inner edges $r_k = k \\cdot \\Delta r$ for $k=0, 1, \\dots$ such that $r_k + \\Delta r \\le r_{\\max}$.\n\nFor each radial bin $[r_k, r_k + \\Delta r)$, the value of the cross pair-correlation function estimator, $\\widehat{g}_{cv}(m_k)$, is calculated at the bin midpoint $m_k = r_k + \\Delta r / 2$. This calculation adheres strictly to the provided formula:\n$$\n\\widehat{g}_{cv}(m_k) \\;=\\; \\frac{A}{N_c \\cdot M_k} \\cdot \\frac{1}{2\\pi m_k \\Delta r} \\cdot \\sum_{i \\in \\mathcal{D}} \\mathbf{1}\\!\\left[ d_{\\text{edge}}(\\mathbf{v}_i) \\ge r_k + \\Delta r \\right] \\sum_{j=1}^{N_c} \\mathbf{1}\\!\\left[ r_k \\le \\lVert \\mathbf{v}_i - \\mathbf{c}_j \\rVert < r_k + \\Delta r \\right]\n$$\nwhere $A = \\pi R^2$ is the area of the domain.\n\nThe calculation involves several key steps for each bin $k$:\n1.  The border correction term, $M_k$, is computed. This term represents the number of docked vesicles that are sufficiently far from the boundary of the active zone for the given bin. A vesicle $i$ is included in this count if its distance to the edge, $d_{\\text{edge}}(\\mathbf{v}_i) = R - \\lVert \\mathbf{v}_i \\rVert$, is greater than or equal to the outer radius of the bin, $r_k + \\Delta r$.\n    $$\n    M_k \\;=\\; \\sum_{i \\in \\mathcal{D}} \\mathbf{1}\\!\\left[ d_{\\text{edge}}(\\mathbf{v}_i) \\ge r_k + \\Delta r \\right]\n    $$\n2.  If $M_k  1$, the estimator $\\widehat{g}_{cv}(m_k)$ is undefined for this bin, and the bin is excluded from the analysis, as per the problem specification.\n3.  If $M_k \\ge 1$, the double summation is evaluated. This term counts the number of pairs $(\\mathbf{v}_i, \\mathbf{c}_j)$ where vesicle $i$ is a docked vesicle satisfying the border condition for bin $k$, and the distance $\\lVert \\mathbf{v}_i - \\mathbf{c}_j \\rVert$ falls within the half-open interval $[r_k, r_k + \\Delta r)$.\n4.  These components are combined to yield $\\widehat{g}_{cv}(m_k)$. For computational efficiency, this is implemented using vectorized operations on arrays of coordinates and distances. Pairwise distances $\\lVert \\mathbf{v}_i - \\mathbf{c}_j \\rVert$ are pre-computed for all vesicle-channel pairs.\n\nAfter computing $\\widehat{g}_{cv}(m_k)$ for all valid bins (where $M_k \\ge 1$), the summary statistic $S$ is calculated as the maximum absolute deviation from $1$:\n$$\nS \\;=\\; \\max_{k \\,:\\, M_k \\ge 1} \\left| \\widehat{g}_{cv}(m_k) - 1 \\right|\n$$\nIf no bins are valid for a given configuration, $S$ is taken to be $0$.\n\nThe main procedure then uses this function to perform the permutation test for each test case.\n1.  The observed statistic, $S^{\\text{obs}}$, is calculated using the experimentally observed docked vesicle indices $\\mathcal{D}$.\n2.  A null distribution for $S$ is generated. The null hypothesis states that the 'docked' property is independent of vesicle position. This is simulated by creating $B$ permutations of the docked labels. For each permutation $b \\in \\{1, \\dots, B\\}$, a new set of docked indices $\\mathcal{D}^{(b)}$ is generated by randomly sampling without replacement from the indices of all vesicles, while preserving the total number of docked vesicles. The statistic $S^{(b)}$ is then computed for this permuted configuration.\n3.  The random sampling is performed using a pseudo-random number generator initialized with the specified seed, $s=0$, to ensure reproducibility.\n4.  Finally, the $p$-value is estimated by comparing $S^{\\text{obs}}$ to the null distribution of $\\{S^{(b)}\\}_{b=1}^B$. The number of permutations yielding a statistic greater than or equal to the observed one is counted. The $p$-value is given by:\n    $$\n    p \\;=\\; \\frac{1 + \\#\\{ b \\in \\{1,\\dots,B\\} : S^{(b)} \\ge S^{\\text{obs}} \\}}{B + 1}\n    $$\nThis entire process is repeated for each of the four test cases provided. The resulting $p$-values are collected, rounded to four decimal places, and formatted into the required output string.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef compute_s_statistic(docked_indices, ves_coords, cav_coords, R, r_bins, delta_r):\n    \"\"\"\n    Computes the summary statistic S for a given set of docked vesicles.\n    \"\"\"\n    N_c = cav_coords.shape[0]\n    if len(docked_indices) == 0:\n        return 0.0\n\n    docked_coords = ves_coords[docked_indices]\n    N_d = docked_coords.shape[0]\n    \n    # Pre-calculate distances between docked vesicles and Cav2 channels\n    dists_cv = cdist(docked_coords, cav_coords)\n    \n    # Pre-calculate distance of each docked vesicle to the circular boundary\n    d_edge_docked = R - np.linalg.norm(docked_coords, axis=1)\n\n    g_cv_deviations = []\n    \n    for k in range(len(r_bins) - 1):\n        r_k = r_bins[k]\n        r_k_plus_1 = r_bins[k+1]\n        m_k = r_k + delta_r / 2.0\n\n        # Border correction: find vesicles whose annulus is fully inside the domain\n        border_condition_mask = (d_edge_docked = r_k_plus_1)\n        M_k = np.sum(border_condition_mask)\n\n        if M_k  1:\n            continue\n\n        # Filter distances for vesicles that satisfy the border condition\n        valid_dists_cv = dists_cv[border_condition_mask, :]\n        \n        # Count pairs in the radial bin [r_k, r_k+delta_r)\n        pair_counts = np.sum((valid_dists_cv = r_k)  (valid_dists_cv  r_k_plus_1))\n        \n        # Calculate g_cv(m_k) using the simplified formula to avoid large intermediate values\n        # g_cv = (A / (N_c * M_k)) * (pair_counts / (2 * pi * m_k * delta_r))\n        # g_cv = (pi * R**2 / (N_c * M_k)) * (pair_counts / (2 * pi * m_k * delta_r))\n        # g_cv = (R**2 / (2 * N_c * M_k * m_k * delta_r)) * pair_counts\n        # Annular area term 2*m_k*delta_r can be 0 if m_k is 0.\n        # First bin r_k=0, m_k = delta_r/2  0 provided delta_r  0.\n        if m_k == 0: # Should not happen with delta_r  0\n            g_k = 0.0\n        else:\n            g_k = (R**2 * pair_counts) / (2.0 * N_c * M_k * m_k * delta_r)\n\n        g_cv_deviations.append(np.abs(g_k - 1.0))\n\n    if not g_cv_deviations:\n        return 0.0\n    \n    return np.max(g_cv_deviations)\n\ndef calculate_p_value(case):\n    \"\"\"\n    Performs the permutation test for a single test case.\n    \"\"\"\n    R = case['R']\n    delta_r = case['delta_r']\n    r_max = case['r_max']\n    B = case['B']\n    s = case['s']\n    cav_coords = np.array(case['cav_coords'], dtype=float)\n    ves_coords = np.array(case['ves_coords'], dtype=float)\n    docked_indices = np.array(case['docked_indices'])\n    \n    N_v = ves_coords.shape[0]\n    N_d = len(docked_indices)\n    all_ves_indices = np.arange(N_v)\n\n    # Define radial bins\n    r_bins = np.arange(0, r_max + delta_r, delta_r)\n\n    # 1. Calculate observed statistic S_obs\n    s_obs = compute_s_statistic(docked_indices, ves_coords, cav_coords, R, r_bins, delta_r)\n\n    # 2. Generate null distribution by permutation\n    rng = np.random.default_rng(seed=s)\n    extreme_count = 0\n    for _ in range(B):\n        permuted_docked_indices = rng.choice(all_ves_indices, size=N_d, replace=False)\n        s_perm = compute_s_statistic(permuted_docked_indices, ves_coords, cav_coords, R, r_bins, delta_r)\n        if s_perm = s_obs:\n            extreme_count += 1\n            \n    # 3. Calculate p-value\n    p_value = (1.0 + extreme_count) / (1.0 + B)\n    \n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"R\": 150, \"delta_r\": 10, \"r_max\": 60, \"B\": 2000, \"s\": 0,\n            \"cav_coords\": [(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)],\n            \"ves_coords\": [(0,0),(20,20),(-25,15),(30,-10),(-40,-30),(70,0),(0,80),(-70,5),(15,-65),(90,40),(-90,-20),(10,110)],\n            \"docked_indices\": [1,4,7,10]\n        },\n        {\n            \"R\": 150, \"delta_r\": 10, \"r_max\": 60, \"B\": 2000, \"s\": 0,\n            \"cav_coords\": [(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)],\n            \"ves_coords\": [(70,0),(0,70),(-70,0),(0,-70),(0,0),(130,0),(0,130),(-130,0),(0,-130),(90,90),(-90,90),(-90,-90)],\n            \"docked_indices\": [0,1,2,3]\n        },\n        {\n            \"R\": 150, \"delta_r\": 10, \"r_max\": 60, \"B\": 2000, \"s\": 0,\n            \"cav_coords\": [(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)],\n            \"ves_coords\": [(0,0),(10,0),(0,10),(-10,0),(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)],\n            \"docked_indices\": [0,1,2,3]\n        },\n        {\n            \"R\": 150, \"delta_r\": 10, \"r_max\": 50, \"B\": 2000, \"s\": 0,\n            \"cav_coords\": [(60,0),(0,60),(-60,0),(0,-60),(42,42),(-42,42),(-42,-42),(42,-42)],\n            \"ves_coords\": [(140,0),(99,99),(0,140),(-99,99),(-140,0),(-99,-99),(0,-140),(99,-99),(0,0),(30,30),(-30,30),(-30,-30)],\n            \"docked_indices\": [0,2,4,6]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = calculate_p_value(case)\n        results.append(p_val)\n\n    print(f\"[{','.join(f'{p:.4f}' for p in results)}]\")\n\nsolve()\n```", "id": "2757202"}]}