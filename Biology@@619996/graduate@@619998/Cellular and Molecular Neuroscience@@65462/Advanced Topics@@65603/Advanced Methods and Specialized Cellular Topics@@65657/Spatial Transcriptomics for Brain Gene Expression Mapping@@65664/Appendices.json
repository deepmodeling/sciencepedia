{"hands_on_practices": [{"introduction": "Before analyzing any data, it's crucial to understand the physical limitations of the technology itself. The resolution of spatial transcriptomics is not infinite; it is fundamentally constrained by the size of the capture spots and the distance between them. This exercise will guide you through calculating the tissue coverage fraction and, more importantly, connect these physical parameters to core principles of signal processing like the Nyquist-Shannon sampling theorem, providing a rigorous understanding of what biological features can—and cannot—be resolved. [@problem_id:2753018]", "problem": "In a spatial transcriptomics (ST) experiment, capture spots are arranged on a hexagonal lattice etched onto a glass slide. Each spot is a circular patch of capture oligonucleotides that binds ribonucleic acid (RNA) from the tissue placed on top. Assume the following for an idealized, uniformly covered region of the slide: the center-to-center pitch of neighboring spots is $p = 100\\,\\mu\\mathrm{m}$, each spot has diameter $d = 55\\,\\mu\\mathrm{m}$, and the lattice is an infinite, perfectly periodic hexagonal (triangular) lattice. Define the coverage fraction $f$ as the fraction of the tissue area that lies directly above spots (i.e., the ratio of the union of circular spot areas to the total area), under the idealization that the tissue fully covers the array and that each spot contributes exactly its geometric footprint.\n\nUsing only geometric definitions and lattice periodicity, derive an expression for $f$ and compute its numerical value for the values of $p$ and $d$ given above. Report the coverage fraction as a pure decimal (no percentage sign), rounded to four significant figures.\n\nThen, using first principles from sampling theory and spatial averaging, provide a principled argument about how the values of $p$ and $d$ constrain the ability to resolve thin laminar structures in the brain (for example, cortical layers whose thickness is comparable to or smaller than $p$ and $d$). Your argument should rely on the Nyquist-Shannon sampling theorem and on the definition of partial volume effects arising from the finite spot diameter. Your final numerical answer should be the coverage fraction value only, as specified above.", "solution": "The problem presented is well-posed, scientifically grounded, and contains sufficient information for a rigorous solution. We shall proceed by first addressing the geometric calculation of the coverage fraction, and second, by providing a formal argument on the resolution limits imposed by the system parameters.\n\nThe problem asks for the coverage fraction $f$, defined as the ratio of the area occupied by capture spots to the total area. Given an infinite, perfectly periodic hexagonal lattice, this global ratio is identical to the ratio of the area of a single spot to the area of the lattice's unit cell.\n\nA hexagonal lattice is defined by basis vectors and can be tiled by rhombic unit cells. Each unit cell contains one lattice point. Let the center-to-center pitch between adjacent spots be $p$. This corresponds to the side length of the rhombic unit cell. The interior angles of this rhombus are $\\frac{\\pi}{3}$ radians ($60^\\circ$) and $\\frac{2\\pi}{3}$ radians ($120^\\circ$). The area of such a unit cell, $A_{\\text{cell}}$, is given by the formula for the area of a rhombus, which can also be seen as two adjoined equilateral triangles of side length $p$.\nThe area of an equilateral triangle with side $p$ is $\\frac{\\sqrt{3}}{4} p^2$. The rhombic unit cell consists of two such triangles, so its area is:\n$$A_{\\text{cell}} = 2 \\times \\left(\\frac{\\sqrt{3}}{4} p^2\\right) = \\frac{\\sqrt{3}}{2} p^2$$\n\nEach capture spot is a circle of diameter $d$. The radius of a spot is $r = \\frac{d}{2}$. The area of a single spot, $A_{\\text{spot}}$, is therefore:\n$$A_{\\text{spot}} = \\pi r^2 = \\pi \\left(\\frac{d}{2}\\right)^2 = \\frac{\\pi d^2}{4}$$\nThe problem states that the pitch is $p = 100\\,\\mu\\mathrm{m}$ and the spot diameter is $d = 55\\,\\mu\\mathrm{m}$. Since the distance between centers $p$ is greater than the spot diameter $d$, the spots do not overlap.\n\nThe coverage fraction $f$ is the ratio of $A_{\\text{spot}}$ to $A_{\\text{cell}}$.\n$$f = \\frac{A_{\\text{spot}}}{A_{\\text{cell}}} = \\frac{\\frac{\\pi d^2}{4}}{\\frac{\\sqrt{3} p^2}{2}}$$\nSimplifying this expression, we obtain:\n$$f = \\frac{\\pi d^2}{2\\sqrt{3} p^2}$$\n\nNow, we substitute the given numerical values: $p = 100\\,\\mu\\mathrm{m}$ and $d = 55\\,\\mu\\mathrm{m}$.\n$$f = \\frac{\\pi (55)^2}{2\\sqrt{3} (100)^2} = \\frac{3025 \\pi}{20000 \\sqrt{3}}$$\nNumerically, this evaluates to:\n$$f \\approx \\frac{3025 \\times 3.14159265}{2 \\times 1.7320508 \\times 10000} \\approx \\frac{9503.3188}{34641.016} \\approx 0.274333$$\nRounding to four significant figures as required, the coverage fraction is $f \\approx 0.2743$.\n\nNext, we must provide a principled argument regarding the physical resolution limits for mapping thin laminar structures in the brain. The spatial resolution of this technology is constrained by two fundamental parameters: the sampling pitch $p$ and the spot diameter $d$.\n\n1.  **Constraint from Sampling Pitch ($p$) and the Nyquist-Shannon Theorem**:\n    The arrangement of spots constitutes a discrete spatial sampling grid. The ability to resolve spatial features is dictated by the Nyquist-Shannon sampling theorem. In its one-dimensional form, the theorem states that to losslessly reconstruct a signal, the sampling frequency must be at least twice the maximum frequency present in the signal. In a spatial context, a periodic feature of wavelength $\\lambda$ has a spatial frequency of $k = \\frac{1}{\\lambda}$. The sampling interval $p$ imposes an upper limit on the spatial frequencies that can be resolved without ambiguity. The highest resolvable frequency, the Nyquist frequency, is $k_{\\text{Nyquist}} = \\frac{1}{2p}$. This corresponds to a minimum resolvable wavelength, or feature size, of $\\lambda_{\\text{min}} = 2p$.\n    For the given pitch $p=100\\,\\mu\\mathrm{m}$, the theoretical minimum resolvable feature size is $\\lambda_{\\text{min}} = 2 \\times 100\\,\\mu\\mathrm{m} = 200\\,\\mu\\mathrm{m}$.\n    Any biological structure, such as a cortical layer, with a thickness smaller than this $\\lambda_{\\text{min}}$ is undersampled. This undersampling leads to aliasing, where high spatial frequency information (fine details) is incorrectly represented as low spatial frequency information (coarse artifacts). Consequently, a thin layer of high gene expression might be missed if it falls between sample points, or its spatial profile will be distorted, potentially appearing as a wider, lower-intensity band or merging with signals from adjacent layers. The fine anatomical structure is thus lost.\n\n2.  **Constraint from Spot Diameter ($d$) and Partial Volume Effects**:\n    The finite size of the capture spots introduces a different limitation. Each spot does not measure gene expression at an infinitesimal point, but instead captures and averages the RNA content from all cells within its circular area of diameter $d = 55\\,\\mu\\mathrm{m}$. This process is equivalent to a spatial low-pass filter. The transfer function of this circular aperture has its first zero at a spatial frequency of approximately $\\frac{1.22}{d}$.\n    This averaging leads to the \"partial volume effect\". If a spot lies across the boundary of two distinct tissue regions or cellular layers, the resulting measurement is a conflation of the molecular profiles of both. A thin laminar structure with thickness $T  d$ cannot be resolved clearly. For instance, if a cortical layer of thickness $30\\,\\mu\\mathrm{m}$ is overlaid by a spot of diameter $55\\,\\mu\\mathrm{m}$, the unique gene expression signature of that layer will be heavily diluted by the contributions from adjacent layers also covered by the same spot. The signal-to-noise ratio for the layer-specific signature is severely degraded, potentially to an undetectable level. The larger the spot diameter $d$ relative to the feature size, the more pronounced the blurring and loss of detail.\n\nIn summary, the sampling pitch $p$ dictates the limit for avoiding aliasing artifacts (a sampling problem), while the spot diameter $d$ dictates the limit of spatial averaging and blurring (a measurement aperture problem). For accurate mapping of a laminar structure of thickness $T$, it is a necessary, though not sufficient, condition that both $p \\ll T$ and $d \\ll T$. With $p=100\\,\\mu\\mathrm{m}$ and $d=55\\,\\mu\\mathrm{m}$, any layer with a thickness on the order of tens of microns, which is common in the mammalian cortex, will be poorly resolved, its boundaries blurred, and its quantitative expression profile confounded with its neighbors. The technology, with these parameters, is fundamentally unsuited for resolving such fine anatomical detail.", "answer": "$$\\boxed{0.2743}$$", "id": "2753018"}, {"introduction": "Spatial transcriptomics data is only as valuable as its spatial context. However, the physical process of preparing a tissue slice often introduces non-linear warping, distorting the tissue's true geometry and complicating its alignment with a reference atlas. This practice introduces a classic and powerful solution: the Thin-Plate Spline (TPS), which models these complex deformations by finding a \"smoothest\" possible mapping between landmarks. By implementing this core registration technique, you will gain a hands-on understanding of how to place your spatial data into an accurate anatomical framework. [@problem_id:2753036]", "problem": "You are working on correcting spatial distortions introduced by cryosectioning in Spatial Transcriptomics (ST) for brain tissue, where gene expression barcodes are observed as point coordinates in a section that is warped relative to a known anatomical reference. A principled way to correct such warping is to fit a Thin-Plate Spline (TPS) warp to matched anatomical landmarks and then apply the fitted warp to transform barcode coordinates. TPS arises from minimizing the bending energy of a thin elastic plate governed by the biharmonic operator, subject to interpolation constraints at the landmarks.\n\nStarting from core definitions in elasticity and variational calculus, specifically that the energy of bending for a scalar displacement field is proportional to the integral of squared second derivatives and that the minimizer under interpolation constraints is unique in the corresponding native space, derive the algorithmic steps to fit a two-dimensional TPS mapping from a set of paired landmarks and then apply it to arbitrary query points. Ensure that the derived mapping is the sum of a global affine component and a radial basis component anchored at the landmarks, and that it exactly interpolates the given landmark correspondences while minimizing bending energy.\n\nThen implement a program that:\n- Fits the TPS warp from given source landmarks (warped tissue section) to target landmarks (reference atlas) using only the mathematical principles above as the foundation.\n- Applies the fitted warp to given query points.\n- Computes the root-mean-square error (RMSE) between the warped query points and their expected target positions for each test case.\n- Reports the RMSE values in micrometers, rounded to six decimal places.\n\nAll coordinates are two-dimensional planar coordinates expressed in micrometers, denoted $\\mu \\mathrm{m}$. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example $[r_1,r_2,r_3]$. Each $r_i$ must be a floating-point number rounded to six decimal places. No units or additional text may be printed; the numeric values are understood to be in $\\mu \\mathrm{m}$.\n\nAngle units, where applicable for internal derivations, should be in radians, but no angles are provided as inputs.\n\nTest Suite:\nProvide a TPS implementation that is tested on the following three cases. For each case, fit the TPS on the provided landmark pairs, warp the provided query points, and compute the RMSE relative to the provided expected targets.\n\n- Test Case $1$ (identity warp; happy path):\n  - Source landmarks (warped): $[(0,0),(100,0),(0,100),(100,100)]$ in $\\mu \\mathrm{m}$.\n  - Target landmarks (reference): $[(0,0),(100,0),(0,100),(100,100)]$ in $\\mu \\mathrm{m}$.\n  - Query points (to warp): $[(50,50),(25,75)]$ in $\\mu \\mathrm{m}$.\n  - Expected warped positions: $[(50,50),(25,75)]$ in $\\mu \\mathrm{m}$.\n\n- Test Case $2$ (pure affine transform embedded in TPS; boundary of nonlinearity):\n  - Source landmarks (warped): $[(0,0),(100,0),(0,100),(100,100),(50,50)]$ in $\\mu \\mathrm{m}$.\n  - Target landmarks (reference): $[(10.000000,-5.000000),(105.262794,50.000000),(-45.000000,90.262794),(50.262794,145.262794),(30.131397,70.131397)]$ in $\\mu \\mathrm{m}$.\n  - Query points (to warp): $[(20,80),(60,40)]$ in $\\mu \\mathrm{m}$.\n  - Expected warped positions: $[(-14.947441,82.210236),(45.157677,66.105118)]$ in $\\mu \\mathrm{m}$.\n\n- Test Case $3$ (non-affine smooth warp; significant edge case):\n  - Source landmarks (warped): $[(0,0),(50,0),(100,0),(0,50),(50,50),(100,50),(0,100),(50,100),(100,100)]$ in $\\mu \\mathrm{m}$.\n  - Target landmarks (reference): $[(5.000000,0.350000),(0.000000,-0.400000),(95.000000,0.350000),(0.000000,50.750000),(50.000000,50.000000),(100.000000,50.750000),(-5.000000,100.350000),(50.000000,99.600000),(105.000000,100.350000)]$ in $\\mu \\mathrm{m}$.\n  - Query points (to warp): $[(25,75),(75,25)]$ in $\\mu \\mathrm{m}$.\n  - Expected warped positions: $[(23.750000,75.087500),(73.750000,25.087500)]$ in $\\mu \\mathrm{m}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the RMSE values for the three test cases as a comma-separated list with no spaces, enclosed in square brackets, for example $[r_1,r_2,r_3]$, where each $r_i$ is rounded to six decimal places and implicitly in $\\mu \\mathrm{m}$.", "solution": "The user has requested a derivation and implementation of the Thin-Plate Spline (TPS) algorithm for correcting spatial distortions in two-dimensional coordinate data. The problem is scientifically sound, well-posed, and objective. All necessary data and conditions for a unique solution are provided. The problem is valid.\n\nThe core of the Thin-Plate Spline method lies in finding a mapping function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ that interpolates a set of control points while minimizing a physical quantity known as bending energy. This ensures the smoothest possible transformation that respects the given landmark correspondences. For a mapping from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, we solve for two independent scalar functions, $f_x(x, y) = x'$ and $f_y(x, y) = y'$.\n\nThe derivation proceeds from the principles of variational calculus and elasticity theory.\n\nFirst Principle: Bending Energy Functional\nThe bending energy $J$ of a thin, infinite elastic plate, represented by a displacement field $f(x, y)$, is proportional to the integral of the squared second partial derivatives over the entire plane. The functional to be minimized is:\n$$ J(f) = \\iint_{\\mathbb{R}^2} \\left[ \\left(\\frac{\\partial^2 f}{\\partial x^2}\\right)^2 + 2\\left(\\frac{\\partial^2 f}{\\partial x \\partial y}\\right)^2 + \\left(\\frac{\\partial^2 f}{\\partial y^2}\\right)^2 \\right] dx dy $$\nThis functional represents the physical energy required to bend the plate. Minimizing it yields the smoothest possible interpolating function.\n\nSecond Principle: Interpolation Constraints\nThe mapping function must exactly match the source landmarks to the target landmarks. Given $N$ source landmarks $p_i = (x_i, y_i)$ and their corresponding target values $v_i$ (which will be either the x or y coordinate of the target landmark), the interpolation constraints are:\n$$ f(p_i) = v_i \\quad \\text{for } i = 1, 2, \\ldots, N $$\n\nSolution via Variational Calculus\nThe problem is to minimize $J(f)$ subject to the $N$ constraints $f(p_i) = v_i$. The solution to this variational problem has a specific analytical form. The Euler-Lagrange equation for this functional leads to the biharmonic equation, $\\Delta^2 f = 0$, away from the landmarks. The solution is constructed using the fundamental solution of the biharmonic operator, which acts as a radial basis function (RBF).\n\nThe general form of the minimizer function $f(p)$ at a point $p=(x,y)$ consists of two components: a non-affine warping component based on the RBF and a global affine component.\n$$ f(p) = \\sum_{i=1}^{N} w_i U(\\|p - p_i\\|) + a_1 + a_2 x + a_3 y $$\nHere:\n- $p_i$ are the $N$ source landmark coordinates.\n- $w_i$ are the weights for the non-affine warping component.\n- $U(r)$ is the radial basis function. For two dimensions, it is defined as $U(r) = r^2 \\log(r)$. We define $U(0) = 0$.\n- $a_1 + a_2 x + a_3 y$ is the affine component. An affine transformation has zero bending energy, hence its inclusion.\n\nDerivation of the Linear System\nTo find the $N+3$ unknown coefficients ($N$ weights $w_i$ and $3$ affine coefficients $a_j$), we construct a system of $N+3$ linear equations.\n\nThe $N$ interpolation constraints provide the first set of equations:\n$$ f(p_j) = v_j \\Rightarrow \\sum_{i=1}^{N} w_i U(\\|p_j - p_i\\|) + a_1 + a_2 x_j + a_3 y_j = v_j \\quad \\text{for } j=1, \\dots, N $$\n\nFor the bending energy $J(f)$ to be finite and the solution unique, the non-affine part must be orthogonal to any affine function. This imposes three additional constraints on the weights $w_i$:\n$$ \\sum_{i=1}^{N} w_i = 0 $$\n$$ \\sum_{i=1}^{N} w_i x_i = 0 $$\n$$ \\sum_{i=1}^{N} w_i y_i = 0 $$\nThese constraints ensure that the affine transformation is uniquely captured by the coefficients $a_1, a_2, a_3$.\n\nThis system of $N+3$ equations can be expressed in matrix form. Let $\\mathbf{w} = [w_1, \\dots, w_N]^T$ be the vector of weights, $\\mathbf{a} = [a_1, a_2, a_3]^T$ be the vector of affine coefficients, and $\\mathbf{v} = [v_1, \\dots, v_N]^T$ be the vector of target values. Define the matrices $K$ and $P$:\n- $K$ is an $N \\times N$ matrix where $K_{ij} = U(\\|p_i - p_j\\|)$.\n- $P$ is an $N \\times 3$ matrix where the $i$-th row is $[1, x_i, y_i]$.\n\nThe system of equations becomes:\n$$ \\begin{cases} K \\mathbf{w} + P \\mathbf{a} = \\mathbf{v} \\\\ P^T \\mathbf{w} = \\mathbf{0}_{3 \\times 1} \\end{cases} $$\nThis is written as a single block matrix equation:\n$$ L \\mathbf{c} = \\mathbf{y}' \\iff \\begin{pmatrix} K  P \\\\ P^T  0_{3 \\times 3} \\end{pmatrix} \\begin{pmatrix} \\mathbf{w} \\\\ \\mathbf{a} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{v} \\\\ \\mathbf{0}_{3 \\times 1} \\end{pmatrix} $$\nThe $(N+3) \\times (N+3)$ matrix $L$ is invertible provided the landmarks are not collinear.\n\nAlgorithmic Steps\n\n1.  **Fit the TPS Model**:\n    a. Given $N$ source landmarks $p_i=(x_i, y_i)$ and $N$ target landmarks $p'_i=(x'_i, y'_i)$.\n    b. Construct the matrix $K$ where $K_{ij} = r_{ij}^2 \\log(r_{ij})$ and $r_{ij} = \\|p_i - p_j\\|$.\n    c. Construct the matrix $P$ with rows $[1, x_i, y_i]$.\n    d. Assemble the block matrix $L = \\begin{pmatrix} K  P \\\\ P^T  0 \\end{pmatrix}$.\n    e. To find the mapping for the x-coordinates, define the vector $\\mathbf{y}'_x = [x'_1, \\dots, x'_N, 0, 0, 0]^T$ and solve the linear system $L \\mathbf{c}_x = \\mathbf{y}'_x$ for the coefficient vector $\\mathbf{c}_x = [\\mathbf{w}_x^T, \\mathbf{a}_x^T]^T$.\n    f. Similarly, to find the mapping for the y-coordinates, define $\\mathbf{y}'_y = [y'_1, \\dots, y'_N, 0, 0, 0]^T$ and solve $L \\mathbf{c}_y = \\mathbf{y}'_y$ for $\\mathbf{c}_y = [\\mathbf{w}_y^T, \\mathbf{a}_y^T]^T$.\n\n2.  **Apply the Warp**:\n    a. For a new query point $q=(x_q, y_q)$, compute its warped coordinates $(x'_q, y'_q)$.\n    b. The warped x-coordinate is:\n       $$ x'_q = f_x(q) = \\sum_{i=1}^{N} w_{x,i} U(\\|q - p_i\\|) + a_{x,1} + a_{x,2} x_q + a_{x,3} y_q $$\n    c. The warped y-coordinate is:\n       $$ y'_q = f_y(q) = \\sum_{i=1}^{N} w_{y,i} U(\\|q - p_i\\|) + a_{y,1} + a_{y,2} x_q + a_{y,3} y_q $$\n\n3.  **Compute RMSE**:\n    For a set of $M$ query points where the warped positions are $q'_j$ and the expected positions are $q''_j$, the Root-Mean-Square Error (RMSE) is:\n    $$ \\text{RMSE} = \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} \\|q'_j - q''_j\\|^2} $$\n    This metric quantifies the average deviation between the computed and expected warped positions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass ThinPlateSpline:\n    \"\"\"\n    A class to fit and apply a 2D Thin-Plate Spline (TPS) transformation.\n    \n    The TPS finds the smoothest possible transformation that interpolates a set of\n    source landmarks to a set of target landmarks, based on minimizing a bending\n    energy functional.\n    \"\"\"\n    def __init__(self):\n        self.source_landmarks = None\n        self.coeffs_x = None\n        self.coeffs_y = None\n        self._fitted = False\n\n    def _U(self, r):\n        \"\"\"\n        Radial Basis Function U(r) = r^2 * log(r).\n        \n        Args:\n            r (np.ndarray): Array of distances.\n        \n        Returns:\n            np.ndarray: Result of the RBF applied element-wise.\n        \"\"\"\n        # Handle the r=0 case, where U(0) is defined as 0.\n        # np.log(0) is -inf, and 0 * -inf is nan. This must be handled explicitly.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            log_r = np.log(r)\n        log_r[r == 0] = 0.0  # Convention for U(0) = 0\n        return r**2 * log_r\n\n    def fit(self, source_landmarks, target_landmarks):\n        \"\"\"\n        Fits the TPS model by solving for the transformation coefficients.\n        \n        Args:\n            source_landmarks (list or np.ndarray): An N x 2 array of source landmark coordinates.\n            target_landmarks (list or np.ndarray): An N x 2 array of target landmark coordinates.\n        \"\"\"\n        source_landmarks = np.asarray(source_landmarks, dtype=float)\n        target_landmarks = np.asarray(target_landmarks, dtype=float)\n\n        if source_landmarks.shape != target_landmarks.shape or source_landmarks.ndim != 2 or source_landmarks.shape[1] != 2:\n            raise ValueError(\"Landmarks must be N x 2 arrays.\")\n        \n        n = source_landmarks.shape[0]\n        self.source_landmarks = source_landmarks\n\n        # 1. Construct the matrix L\n        # L = [[K, P], [P.T, 0]]\n        \n        # K matrix from distances between source landmarks\n        dists = cdist(source_landmarks, source_landmarks)\n        K = self._U(dists)\n\n        # P matrix\n        P = np.hstack([np.ones((n, 1)), source_landmarks])\n\n        # Assemble the full (n+3) x (n+3) L matrix\n        L = np.zeros((n + 3, n + 3))\n        L[:n, :n] = K\n        L[:n, n:] = P\n        L[n:, :n] = P.T\n\n        # 2. Construct the right-hand side vectors y'\n        # y_x' = [target_x, 0, 0, 0]^T\n        # y_y' = [target_y, 0, 0, 0]^T\n        rhs_x = np.concatenate([target_landmarks[:, 0], np.zeros(3)])\n        rhs_y = np.concatenate([target_landmarks[:, 1], np.zeros(3)])\n        \n        # 3. Solve the linear systems L*c_x = y_x' and L*c_y = y_y'\n        self.coeffs_x = np.linalg.solve(L, rhs_x)\n        self.coeffs_y = np.linalg.solve(L, rhs_y)\n        self._fitted = True\n\n    def transform(self, query_points):\n        \"\"\"\n        Applies the fitted TPS warp to a set of query points.\n        \n        Args:\n            query_points (list or np.ndarray): An M x 2 array of points to warp.\n            \n        Returns:\n            np.ndarray: An M x 2 array of warped point coordinates.\n        \"\"\"\n        if not self._fitted:\n            raise RuntimeError(\"TPS model has not been fitted. Call fit() first.\")\n        \n        query_points = np.asarray(query_points, dtype=float)\n        n_query = query_points.shape[0]\n        n_landmarks = self.source_landmarks.shape[0]\n        \n        # Deconstruct coefficients into non-affine (w) and affine (a) parts\n        w_x, a_x = self.coeffs_x[:n_landmarks], self.coeffs_x[n_landmarks:]\n        w_y, a_y = self.coeffs_y[:n_landmarks], self.coeffs_y[n_landmarks:]\n\n        # Calculate kernel values between query points and source landmarks\n        dists_query = cdist(query_points, self.source_landmarks)\n        kernel_matrix = self._U(dists_query)\n\n        # Calculate the affine component of the transformation\n        # P_query @ a = [1, x_q, y_q] @ [a1, a2, a3]^T\n        affine_x = a_x[0] + query_points @ a_x[1:]\n        affine_y = a_y[0] + query_points @ a_y[1:]\n\n        # Calculate the non-affine (warping) component\n        non_affine_x = kernel_matrix @ w_x\n        non_affine_y = kernel_matrix @ w_y\n\n        # Sum the components to get the final warped coordinates\n        warped_x = affine_x + non_affine_x\n        warped_y = affine_y + non_affine_y\n        \n        return np.vstack([warped_x, warped_y]).T\n\ndef calculate_rmse(predicted, actual):\n    \"\"\"Computes the Root-Mean-Square Error between two sets of points.\"\"\"\n    predicted = np.asarray(predicted, dtype=float)\n    actual = np.asarray(actual, dtype=float)\n    errors = predicted - actual\n    return np.sqrt(np.mean(np.sum(errors**2, axis=1)))\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"source_landmarks\": [(0,0),(100,0),(0,100),(100,100)],\n            \"target_landmarks\": [(0,0),(100,0),(0,100),(100,100)],\n            \"query_points\": [(50,50),(25,75)],\n            \"expected_warped\": [(50,50),(25,75)]\n        },\n        {\n            \"source_landmarks\": [(0,0),(100,0),(0,100),(100,100),(50,50)],\n            \"target_landmarks\": [(10.000000,-5.000000),(105.262794,50.000000),(-45.000000,90.262794),(50.262794,145.262794),(30.131397,70.131397)],\n            \"query_points\": [(20,80),(60,40)],\n            \"expected_warped\": [(-14.947441,82.210236),(45.157677,66.105118)]\n        },\n        {\n            \"source_landmarks\": [(0,0),(50,0),(100,0),(0,50),(50,50),(100,50),(0,100),(50,100),(100,100)],\n            \"target_landmarks\": [(5.000000,0.350000),(0.000000,-0.400000),(95.000000,0.350000),(0.000000,50.750000),(50.000000,50.000000),(100.000000,50.750000),(-5.000000,100.350000),(50.000000,99.600000),(105.000000,100.350000)],\n            \"query_points\": [(25,75),(75,25)],\n            \"expected_warped\": [(23.750000,75.087500),(73.750000,25.087500)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        tps = ThinPlateSpline()\n        \n        # Fit the TPS model using the landmark pairs\n        tps.fit(case[\"source_landmarks\"], case[\"target_landmarks\"])\n        \n        # Warp the query points using the fitted model\n        warped_points = tps.transform(case[\"query_points\"])\n        \n        # Compute the RMSE between the warped points and their expected positions\n        rmse = calculate_rmse(warped_points, case[\"expected_warped\"])\n        \n        # Format the result to six decimal places\n        results.append(f\"{rmse:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2753036"}, {"introduction": "With data correctly placed in a spatial context, a key analytical goal is to uncover the underlying biological organization, such as identifying distinct cellular layers or functional domains. This advanced problem introduces a powerful statistical approach for this task: a hierarchical Bayesian model combining a Markov Random Field (MRF) prior, which encourages spatial smoothness, with a Multinomial emission model for the gene counts. Implementing this Variational EM algorithm will equip you with a state-of-the-art method for unsupervised discovery of spatial patterns in tissue. [@problem_id:2752997]", "problem": "You are modeling spatial domains in a brain tissue section profiled by spatial transcriptomics. Each spot on a two-dimensional lattice contains gene expression counts across a fixed set of genes, and each spot belongs to one latent domain label. You will derive a probabilistic model and an inference algorithm, and then implement it to estimate domain parameters and evaluate performance on a fixed test suite.\n\nBegin from the following fundamental bases:\n- The Markov property: a label at a node in a graphical model is conditionally independent of all other non-neighbor labels given its neighbors.\n- The pairwise Gibbs distribution for a finite-state undirected graphical model with local energy and pairwise energy is a widely used and well-tested construction for Markov Random Fields (MRFs).\n- A spot’s observed gene counts arise from a composition over genes, and conditional on the composition, sequencing counts across genes can be modeled using the Multinomial distribution, a well-tested model for counts given a total count and category probabilities.\n- The Expectation-Maximization (EM) principle maximizes the observed-data likelihood by alternating between computing conditional expectations of latent variables and maximizing expected complete-data log-likelihood.\n\nTask Part A (Derivation):\n1) Derive a probabilistic model for latent domain labels on a two-dimensional lattice using a Markov Random Field (MRF) with a Potts prior. Specifically, suppose each spot $i$ carries a latent label $z_i \\in \\{1,\\dots,K\\}$, and pairs of spatially adjacent spots are connected in an undirected lattice graph with edge set $E$. The Potts prior imposes that adjacent labels prefer to be equal, controlled by a scalar smoothness parameter $\\beta \\ge 0$. Write down the unnormalized prior density over $z = (z_1,\\dots,z_n)$ using the indicator of label equality on edges and the scalar $\\beta$.\n2) For observed counts $x_i \\in \\mathbb{N}_0^G$ with total $N_i = \\sum_{g=1}^G x_{ig}$, assume a Multinomial emission: conditional on $z_i = k$, the probability of $x_i$ is Multinomial with total $N_i$ and class-specific gene probabilities $\\phi_k = (\\phi_{k1},\\dots,\\phi_{kG})$ that satisfy $\\sum_{g=1}^G \\phi_{kg} = 1$ and $\\phi_{kg}  0$. Write the complete-data likelihood and the complete-data log-likelihood.\n3) Explain why the exact posterior marginals $p(z_i = k \\mid x)$ are intractable in general for this model and motivate a mean-field variational approximation that factorizes the posterior as $\\prod_{i=1}^n q_i(z_i)$ with categorical variational parameters $q_i(k)$. State the fixed-point equation for the coordinate ascent variational updates of $q_i(k)$ up to a proportionality constant, combining the Potts interactions and the Multinomial emission.\n4) Add a symmetric Dirichlet prior with concentration $\\alpha  1$ on each $\\phi_k$, that is, $p(\\phi_k) \\propto \\prod_{g=1}^G \\phi_{kg}^{\\alpha - 1}$. Derive the maximizer of the expected complete-data log-posterior with respect to $\\phi$ given variational responsibilities $q$, and express the solution as a normalized count with pseudo-counts determined by $\\alpha$.\n\nTask Part B (Algorithmic Specification):\nDesign an Expectation-Maximization (EM) algorithm with a mean-field variational E-step for this model:\n- E-step: For fixed $\\phi$, update $q_i(k)$ using coordinate ascent mean-field updates until convergence or for a fixed number of iterations.\n- M-step: For fixed $q$, update $\\phi$ using the Dirichlet-regularized solution you derived.\n- Iterate for a fixed number of outer iterations.\nUse numerical stabilization where appropriate. Initialize $\\phi$ by drawing each row independently from a symmetric Dirichlet with concentration $1$ with a fixed pseudo-random seed to break label symmetry.\n\nTask Part C (Implementation and Evaluation):\nImplement the algorithm in Python to run on a test suite defined below. The lattice is a regular grid with four-neighbor adjacency (up, down, left, right when present). Indices are enumerated in row-major order starting at $0$. For each test case, you are given the grid shape, the number of labels $K$, the number of genes $G$, the Potts smoothness $\\beta$, the ground-truth label field, the ground-truth emission probabilities $\\phi^{\\text{true}}$, and the total count $N_i$ per spot. Construct observed counts deterministically from $\\phi^{\\text{true}}$ and $N_i$ as follows: compute $y_{ig} = \\lfloor N_i \\phi^{\\text{true}}_{z_i, g} \\rfloor$ for all $g \\in \\{1,\\dots,G\\}$, then compute the remainder $r_i = N_i - \\sum_{g=1}^G y_{ig}$, and let $d_{ig} = N_i \\phi^{\\text{true}}_{z_i, g} - y_{ig}$. Assign the remaining $r_i$ counts by adding $1$ to the $r_i$ genes with the largest $d_{ig}$ ties broken by increasing gene index. The result is $x_i$ with $\\sum_{g=1}^G x_{ig} = N_i$.\n\nUse the following hyperparameters and numerical details for all cases:\n- Dirichlet prior concentration $\\alpha = 1.1$.\n- Number of outer EM iterations $T = 25$.\n- Number of inner mean-field iterations per E-step $L = 50$ with a stopping tolerance of $\\varepsilon = 10^{-6}$ on the maximum absolute change per update if reached earlier.\n- Initialization: draw each row of $\\phi$ from a symmetric Dirichlet with concentration $1$ using pseudo-random seed $13$.\n- Use $\\ell_1$-normalized row constraints on $\\phi$.\n- Use base $e$ logarithms and standard log-sum-exp stabilization for categorical normalization.\n\nTest Suite:\n- Case $1$ (happy path): grid shape $3 \\times 3$, $K = 2$, $G = 4$, $\\beta = 0.8$, ground-truth labels in row-major grid\n  $\\begin{bmatrix}\n  1  1  2\\\\\n  1  1  2\\\\\n  1  2  2\n  \\end{bmatrix}$\n  mapped to zero-based indices $\\{0,1\\}$ as $\\begin{bmatrix}0  0  1\\\\ 0  0  1\\\\ 0  1  1\\end{bmatrix}$, totals $N = [40, 35, 45, 50, 30, 42, 25, 38, 60]$, and\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.55  0.25  0.15  0.05\\\\\n  0.05  0.20  0.35  0.40\n  \\end{bmatrix}$.\n- Case $2$ (multi-class with stronger spatial coupling): grid shape $2 \\times 4$, $K = 3$, $G = 3$, $\\beta = 1.2$, ground-truth labels in row-major grid\n  $\\begin{bmatrix}\n  0  1  2  2\\\\\n  0  1  1  2\n  \\end{bmatrix}$, totals $N = [30, 35, 40, 45, 28, 32, 36, 44]$, and\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.70  0.20  0.10\\\\\n  0.20  0.60  0.20\\\\\n  0.15  0.15  0.70\n  \\end{bmatrix}$.\n- Case $3$ (no spatial coupling, near-uniform emissions): grid shape $2 \\times 2$, $K = 2$, $G = 3$, $\\beta = 0.0$, ground-truth labels in row-major grid\n  $\\begin{bmatrix}\n  0  1\\\\\n  0  1\n  \\end{bmatrix}$, totals $N = [25, 25, 25, 25]$, and\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.34  0.33  0.33\\\\\n  0.33  0.34  0.33\n  \\end{bmatrix}$.\n\nFor each test case, run the EM algorithm described above with the specified hyperparameters and initialization protocol. After convergence, form hard assignments $\\hat{z}_i = \\arg\\max_k q_i(k)$. Because label identities are arbitrary up to permutation, compute the best permutation mapping from the estimated labels to the ground-truth labels that maximizes the number of matches. Then compute two evaluation metrics:\n- Label accuracy: the fraction in $[0,1]$ of spots whose permuted hard assignment equals the ground-truth label.\n- Mean per-class $\\ell_1$-distance between the estimated $\\phi$ and the ground-truth $\\phi^{\\text{true}}$ after applying the same optimal permutation to the rows of the estimated $\\phi$, averaged over classes.\n\nYour program should produce a single line of output containing a list of results, one per test case, where each result is a two-element list $[\\text{accuracy}, \\text{phi\\_l1}]$. All floating-point numbers must be rounded to $6$ decimal places. The output format must be exactly a single Python-style list, for example, $[[0.95,0.12],[0.85,0.20],[1.0,0.0]]$.", "solution": "This is a problem in computational statistics, applying a hierarchical Bayesian model to spatial transcriptomics data. The problem is well-posed, scientifically sound, and provides all necessary information for a unique solution. I will proceed with the derivation and algorithmic specification as requested.\n\n**Part A.1: Potts Prior Derivation**\nThe model is defined on a two-dimensional lattice of $n$ spots. Let the set of latent domain labels be $z = (z_1, \\dots, z_n)$, where each $z_i \\in \\{1, \\dots, K\\}$. The spatial arrangement is represented by an undirected graph with an edge set $E$ connecting spatially adjacent spots. The Potts model, a type of Markov Random Field (MRF), assigns a probability to each configuration $z$ based on local interactions. The prior over labels $z$ is given by a Gibbs distribution. The unnormalized prior density is proportional to the exponentiated negative energy of the configuration, $p(z \\mid \\beta) \\propto \\exp(-H(z))$. For the Potts model, the energy function is defined as:\n$$H(z) = -\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)$$\nHere, $\\beta \\ge 0$ is the smoothness parameter, controlling the strength of the interaction. A larger $\\beta$ imposes a stronger penalty on neighboring spots having different labels, thus promoting larger, more homogeneous domains. The term $\\mathbb{I}(z_i = z_j)$ is an indicator function, which equals $1$ if $z_i = z_j$ and $0$ otherwise. Therefore, the unnormalized prior density can be written as:\n$$p(z \\mid \\beta) \\propto \\exp\\left(\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)\\right)$$\nThis probability is unnormalized because it omits the partition function $Z(\\beta) = \\sum_z \\exp\\left(\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)\\right)$, which is generally intractable to compute.\n\n**Part A.2: Complete-Data Likelihood Derivation**\nThe observed data at each spot $i$ are the gene counts $x_i = (x_{i1}, \\dots, x_{iG})$, with a total count of $N_i = \\sum_{g=1}^G x_{ig}$. The emission probability is modeled as a Multinomial distribution, conditional on the latent label $z_i$. If spot $i$ belongs to domain $k$ (i.e., $z_i = k$), the counts $x_i$ are drawn from a Multinomial distribution with parameters $N_i$ and $\\phi_k = (\\phi_{k1}, \\dots, \\phi_{kG})$:\n$$p(x_i \\mid z_i = k, N_i, \\phi_k) = \\frac{N_i!}{\\prod_{g=1}^G x_{ig}!} \\prod_{g=1}^G \\phi_{kg}^{x_{ig}}$$\nThe complete data consists of both the observed counts $x = (x_1, \\dots, x_n)$ and the latent labels $z = (z_1, \\dots, z_n)$. The complete-data likelihood, given the parameters $\\phi = \\{\\phi_1, \\dots, \\phi_K\\}$, is the joint probability $p(x, z \\mid \\phi, \\beta)$. Assuming conditional independence of observations given labels, and incorporating the MRF prior on labels, this is:\n$$p(x, z \\mid \\phi, \\beta) = p(x \\mid z, \\phi) p(z \\mid \\beta) = \\left( \\prod_{i=1}^n p(x_i \\mid z_i, \\phi_{z_i}) \\right) p(z \\mid \\beta)$$\nThe complete-data log-likelihood, $\\mathcal{L}(\\phi, \\beta; x, z) = \\log p(x, z \\mid \\phi, \\beta)$, is:\n$$\\mathcal{L} = \\sum_{i=1}^n \\log p(x_i \\mid z_i, \\phi_{z_i}) + \\log p(z \\mid \\beta)$$\nUsing an indicator variable $\\mathbb{I}(z_i=k)$ and substituting the Multinomial log-probability and the Potts prior form, we obtain:\n$$\\mathcal{L} = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k) \\left( \\log\\left(\\frac{N_i!}{\\prod_{g=1}^G x_{ig}!}\\right) + \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} \\right) + \\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j) - \\log Z(\\beta)$$\n\n**Part A.3: Variational Inference Motivation and Derivation**\nExact inference in this model is intractable. The posterior distribution over latent labels $p(z \\mid x, \\phi, \\beta)$ is given by Bayes' rule:\n$$p(z \\mid x, \\phi, \\beta) = \\frac{p(x \\mid z, \\phi) p(z \\mid \\beta)}{p(x \\mid \\phi, \\beta)}$$\nThe intractability arises from the evidence term in the denominator, $p(x \\mid \\phi, \\beta) = \\sum_z p(x \\mid z, \\phi) p(z \\mid \\beta)$. This sum is over all $K^n$ possible label configurations, which is computationally prohibitive for any non-trivial lattice size $n$. Consequently, computing posterior marginals like $p(z_i = k \\mid x)$ is also intractable.\n\nTo overcome this, we use mean-field variational inference. We approximate the true posterior $p(z \\mid x)$ with a simpler, factorized distribution $q(z)$:\n$$q(z) = \\prod_{i=1}^n q_i(z_i)$$\nwhere each $q_i(z_i)$ is a categorical distribution over the $K$ labels for spot $i$, with parameters $q_i(k) = q_i(z_i=k)$. The optimal solution for each factor $q_j(z_j)$ is found by minimizing the KL divergence $D_{KL}(q(z) || p(z|x))$, which is equivalent to maximizing the Evidence Lower BOund (ELBO). This leads to the fixed-point equation:\n$$\\log q_j(z_j) = \\mathbb{E}_{q_{\\sim j}}[\\log p(x, z \\mid \\phi, \\beta)] + \\text{const.}$$\nwhere the expectation $\\mathbb{E}_{q_{\\sim j}}$ is over all latent variables except $z_j$. We expand the log joint probability and isolate terms involving $z_j$:\n$$\\log q_j(z_j=k) \\propto \\mathbb{E}_{q_{\\sim j}}\\left[ \\log p(x_j \\mid z_j=k, \\phi_k) + \\sum_{m \\in \\mathcal{N}(j)} \\beta \\mathbb{I}(z_j=k, z_m) \\right]$$\nwhere $\\mathcal{N}(j)$ is the set of neighbors of spot $j$. The first term is independent of other $z_m$, and the expectation of the second term under the mean-field approximation is:\n$$\\mathbb{E}_{q_m}[\\beta \\mathbb{I}(z_j=k, z_m)] = \\beta \\sum_{l=1}^K q_m(z_m=l) \\cdot \\mathbb{I}(k=l) = \\beta q_m(k)$$\nThus, the fixed-point update equation for the variational parameter $q_j(k)$ is, up to a proportionality constant:\n$$\\log q_j(k) \\propto \\log p(x_j \\mid z_j=k, \\phi_k) + \\beta \\sum_{m \\in \\mathcal{N}(j)} q_m(k)$$\nSubstituting the Multinomial log-likelihood (omitting terms constant in $k$):\n$$\\log q_j(k) \\propto \\sum_{g=1}^G x_{jg} \\log \\phi_{kg} + \\beta \\sum_{m \\in \\mathcal{N}(j)} q_m(k)$$\nThe variational parameters $q_j(k)$ are then obtained by exponentiating and normalizing over $k \\in \\{1, \\dots, K\\}$.\n\n**Part A.4: M-step Derivation with Dirichlet Prior**\nA symmetric Dirichlet prior is placed on each set of emission probabilities $\\phi_k$, with concentration parameter $\\alpha  1$:\n$$p(\\phi_k \\mid \\alpha) \\propto \\prod_{g=1}^G \\phi_{kg}^{\\alpha-1}$$\nThe objective in the M-step of the EM algorithm is to maximize the expected complete-data log-posterior with respect to the parameters $\\phi$, conditioned on the data and the variational distribution $q(z)$. This objective is:\n$$\\mathcal{Q}(\\phi) = \\mathbb{E}_q[\\log p(x, z, \\phi \\mid \\alpha, \\beta)] = \\mathbb{E}_q[\\log p(x,z \\mid \\phi,\\beta) + \\log p(\\phi \\mid \\alpha)]$$\nWe only consider terms that depend on $\\phi$:\n$$\\mathcal{Q}(\\phi) = \\mathbb{E}_q\\left[\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k) \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} \\right] + \\sum_{k=1}^K \\sum_{g=1}^G (\\alpha - 1) \\log \\phi_{kg} + \\text{const.}$$\nUsing the property $\\mathbb{E}_q[\\mathbb{I}(z_i=k)] = q_i(k)$, we get:\n$$\\mathcal{Q}(\\phi) = \\sum_{i=1}^n \\sum_{k=1}^K q_i(k) \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} + \\sum_{k=1}^K \\sum_{g=1}^G (\\alpha - 1) \\log \\phi_{kg}$$\nThe objective decouples for each $\\phi_k$. For a given $k$, we maximize:\n$$J(\\phi_k) = \\sum_{g=1}^G \\left( \\left(\\sum_{i=1}^n q_i(k) x_{ig}\\right) + \\alpha - 1 \\right) \\log \\phi_{kg}$$\nsubject to the constraint $\\sum_{g=1}^G \\phi_{kg} = 1$. This is the kernel of a Dirichlet distribution's log-density. The maximizer for $\\phi_k$ is given by normalizing the coefficients of the $\\log \\phi_{kg}$ terms:\n$$\\phi_{kg} = \\frac{\\sum_{i=1}^n q_i(k) x_{ig} + \\alpha - 1}{\\sum_{g'=1}^G \\left( \\sum_{i=1}^n q_i(k) x_{ig'} + \\alpha - 1 \\right)}$$\nThe denominator simplifies to the total expected counts assigned to class $k$ plus the total pseudo-counts from the prior:\n$$\\sum_{g'=1}^G \\left( \\dots \\right) = \\sum_{i=1}^n q_i(k) \\left(\\sum_{g'=1}^G x_{ig'}\\right) + \\sum_{g'=1}^G (\\alpha-1) = \\sum_{i=1}^n q_i(k) N_i + G(\\alpha-1)$$\nThe term $\\alpha-1  0$ acts as a pseudo-count, regularizing the estimate of $\\phi_k$ and preventing probabilities of zero.\n\n**Part B: Algorithmic Specification**\nThe algorithm is a Mean-Field Variational Expectation-Maximization (VEM) procedure. It alternates between updating the approximate posterior over labels (E-step) and updating the model parameters (M-step).\n\n1.  **Initialization:**\n    -   Fix hyperparameters: Potts coupling $\\beta$, Dirichlet concentration $\\alpha$, number of EM iterations $T$, number of mean-field iterations $L$, and tolerance $\\varepsilon$.\n    -   Initialize emission parameters $\\phi^{(0)}$ by drawing each row $\\phi_k^{(0)}$ independently from a symmetric Dirichlet distribution with concentration $1.0$, using a fixed pseudo-random seed for reproducibility.\n\n2.  **Iteration:** For $t = 1, \\dots, T$:\n    -   **Variational E-step:** Update the variational distributions $q$ to approximate the posterior $p(z \\mid x, \\phi^{(t-1)})$.\n        -   Initialize $q^{(t,0)}$ using the result from the previous EM iteration, or uniformly for the first iteration.\n        -   For $l=1, \\dots, L$ (inner coordinate ascent loop):\n            -   For each spot $i = 1, \\dots, n$:\n                -   Compute the unnormalized log-posterior for each class $k = 1, \\dots, K$:\n                    $$\\lambda_{ik} = \\sum_{g=1}^G x_{ig} \\log \\phi_{kg}^{(t-1)} + \\beta \\sum_{j \\in \\mathcal{N}(i)} q_j(k)^{(t, l-1)}$$\n                -   Update $q_i(k)$ by normalizing using the log-sum-exp trick for numerical stability:\n                    $$q_i(k)^{(t,l)} = \\frac{\\exp(\\lambda_{ik})}{\\sum_{k'=1}^K \\exp(\\lambda_{ik'})}$$\n            -   Check for convergence: if $\\max_{i,k} |q_i(k)^{(t,l)} - q_i(k)^{(t,l-1)}|  \\varepsilon$, break the inner loop.\n        -   Set the final $q^{(t)}$ to the converged values.\n\n    -   **M-step:** Update the emission parameters $\\phi$ to maximize the expected complete-data log-posterior given $q^{(t)}$.\n        -   For each class $k = 1, \\dots, K$ and gene $g = 1, \\dots, G$:\n            -   Compute the updated $\\phi_{kg}^{(t)}$ using the derived regularized formula:\n                $$\\phi_{kg}^{(t)} = \\frac{\\sum_{i=1}^n q_i(k)^{(t)} x_{ig} + \\alpha - 1}{\\sum_{i=1}^n q_i(k)^{(t)} N_i + G(\\alpha-1)}$$\n\n3.  **Termination:** After $T$ iterations, the algorithm returns the final estimated parameters $\\phi^{(T)}$ and variational posteriors $q^{(T)}$.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the variational EM algorithm on the test suite\n    and print the evaluation results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"shape\": (3, 3), \"K\": 2, \"G\": 4, \"beta\": 0.8,\n            \"z_true\": np.array([0, 0, 1, 0, 0, 1, 0, 1, 1]),\n            \"N\": np.array([40, 35, 45, 50, 30, 42, 25, 38, 60]),\n            \"phi_true\": np.array([\n                [0.55, 0.25, 0.15, 0.05],\n                [0.05, 0.20, 0.35, 0.40]\n            ])\n        },\n        {\n            \"shape\": (2, 4), \"K\": 3, \"G\": 3, \"beta\": 1.2,\n            \"z_true\": np.array([0, 1, 2, 2, 0, 1, 1, 2]),\n            \"N\": np.array([30, 35, 40, 45, 28, 32, 36, 44]),\n            \"phi_true\": np.array([\n                [0.70, 0.20, 0.10],\n                [0.20, 0.60, 0.20],\n                [0.15, 0.15, 0.70]\n            ])\n        },\n        {\n            \"shape\": (2, 2), \"K\": 2, \"G\": 3, \"beta\": 0.0,\n            \"z_true\": np.array([0, 1, 0, 1]),\n            \"N\": np.array([25, 25, 25, 25]),\n            \"phi_true\": np.array([\n                [0.34, 0.33, 0.33],\n                [0.33, 0.34, 0.33]\n            ])\n        }\n    ]\n\n    hyperparams = {\n        \"alpha\": 1.1,\n        \"T\": 25,\n        \"L\": 50,\n        \"epsilon\": 1e-6,\n        \"seed\": 13\n    }\n\n    results = []\n    for case in test_cases:\n        adj_list = build_adjacency_list(case[\"shape\"])\n        x = generate_counts(case[\"z_true\"], case[\"N\"], case[\"phi_true\"])\n\n        q, phi = run_vem(x,\n                         case[\"N\"],\n                         adj_list,\n                         case[\"K\"],\n                         case[\"G\"],\n                         case[\"beta\"],\n                         hyperparams)\n        \n        accuracy, phi_l1 = evaluate(q, phi, case[\"z_true\"], case[\"phi_true\"], case[\"K\"])\n        results.append([round(accuracy, 6), round(phi_l1, 6)])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef build_adjacency_list(shape):\n    \"\"\"\n    Builds a 4-neighbor adjacency list for a grid.\n    \"\"\"\n    H, W = shape\n    n = H * W\n    adj = [[] for _ in range(n)]\n    for i in range(n):\n        r, c = divmod(i, W)\n        # Up\n        if r > 0: adj[i].append(i - W)\n        # Down\n        if r  H - 1: adj[i].append(i + W)\n        # Left\n        if c > 0: adj[i].append(i - 1)\n        # Right\n        if c  W - 1: adj[i].append(i + 1)\n    return adj\n\ndef generate_counts(z_true, N, phi_true):\n    \"\"\"\n    Deterministically generates gene counts based on ground truth.\n    \"\"\"\n    n, G = len(N), phi_true.shape[1]\n    x = np.zeros((n, G), dtype=int)\n    for i in range(n):\n        true_label = z_true[i]\n        expected_counts = N[i] * phi_true[true_label, :]\n        \n        y_i = np.floor(expected_counts).astype(int)\n        remainder_counts = N[i] - y_i.sum()\n        \n        residuals = expected_counts - y_i\n        \n        # Assign remainder counts to genes with largest residuals\n        remainder_indices = np.argsort(-residuals)[:remainder_counts]\n        \n        x[i, :] = y_i\n        x[i, remainder_indices] += 1\n    return x\n\ndef log_sum_exp(vec, axis=-1, keepdims=False):\n    \"\"\"\n    Computes log(sum(exp(vec))) in a numerically stable way.\n    \"\"\"\n    max_val = np.max(vec, axis=axis, keepdims=True)\n    return max_val + np.log(np.sum(np.exp(vec - max_val), axis=axis, keepdims=True))\n\ndef run_vem(x, N, adj, K, G, beta, hp):\n    \"\"\"\n    Runs the Variational EM algorithm.\n    \"\"\"\n    n = x.shape[0]\n    rng = np.random.default_rng(hp[\"seed\"])\n    phi = rng.dirichlet(np.ones(G), size=K)\n    \n    q = np.full((n, K), 1.0 / K)\n\n    log_phi = np.log(phi + 1e-100) # Add small const to avoid log(0)\n    \n    for t in range(hp[\"T\"]):\n        # E-step: Variational inference for q\n        for l in range(hp[\"L\"]):\n            q_old = q.copy()\n            \n            mrf_term = np.zeros((n, K))\n            for i in range(n):\n                if adj[i]:\n                    mrf_term[i, :] = beta * np.sum(q[adj[i], :], axis=0)\n\n            log_lik_term = x @ log_phi.T\n            log_q_unnorm = log_lik_term + mrf_term\n            \n            log_q = log_q_unnorm - log_sum_exp(log_q_unnorm, axis=1, keepdims=True)\n            q = np.exp(log_q)\n\n            if np.max(np.abs(q - q_old))  hp[\"epsilon\"]:\n                break\n        \n        # M-step: Update phi\n        numerator = q.T @ x + hp[\"alpha\"] - 1.0\n        denominator = (q.T @ N) + G * (hp[\"alpha\"] - 1.0)\n        \n        phi = numerator / denominator[:, np.newaxis]\n        log_phi = np.log(phi + 1e-100)\n\n    return q, phi\n\ndef evaluate(q, phi_est, z_true, phi_true, K):\n    \"\"\"\n    Evaluates the model performance against ground truth.\n    \"\"\"\n    n = len(z_true)\n    z_hat = np.argmax(q, axis=1)\n\n    # Find optimal permutation of labels for matching\n    cost_matrix = np.zeros((K, K))\n    for i in range(n):\n        cost_matrix[z_hat[i], z_true[i]] += 1\n    \n    # linear_sum_assignment finds a minimum cost assignment. We want max matches.\n    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n    \n    # Accuracy\n    accuracy = cost_matrix[row_ind, col_ind].sum() / n\n\n    # Permute estimated phi to match true phi ordering\n    phi_permuted = np.zeros_like(phi_est)\n    phi_permuted[col_ind] = phi_est[row_ind]\n\n    # Mean L1 distance\n    l1_distances = np.sum(np.abs(phi_permuted - phi_true), axis=1)\n    mean_l1_dist = np.mean(l1_distances)\n    \n    return accuracy, mean_l1_dist\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2752997"}]}