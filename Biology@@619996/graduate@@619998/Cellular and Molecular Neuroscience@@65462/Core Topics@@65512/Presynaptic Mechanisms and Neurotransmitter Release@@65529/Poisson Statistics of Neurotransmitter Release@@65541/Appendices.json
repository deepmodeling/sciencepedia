{"hands_on_practices": [{"introduction": "The Poisson model provides a powerful framework for describing neurotransmitter release, but it is often an approximation of a more fundamental biophysical reality. Many synapses are thought to possess a finite number of independent release sites, $n$, each with a certain probability of release, $p$. This exercise [@problem_id:2738674] invites you to explore the statistical consequences of this binomial model, calculating its exact count distribution and key moments. By comparing these properties to a Poisson model with a matched mean, you will uncover the origin of \"sub-Poissonian\" variability and develop a deeper intuition for the Fano factor as a diagnostic tool for release mechanisms.", "problem": "A single synaptic terminal contains $n=5$ independent, functionally identical release sites. In response to a brief presynaptic action potential, each site releases at most one synaptic vesicle with probability $p=0.2$, independently of other sites and trials. Let $K$ denote the number of vesicles released in a single trial (one action potential). Assume that the postsynaptic detection is perfect so that $K$ is directly observed as the vesicle count per trial.\n\nStarting from first principles appropriate to cellular and molecular neuroscience and probability theory—namely, independent Bernoulli trials for release at each site, combinatorial counting of microstates, and definitions of expectation, variance, and the Fano factor defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$—derive the exact count distribution of $K$ for $k=0,1,2,3,4,5$, and compute its mean, variance, and Fano factor. Then consider a Poisson process model of neurotransmitter release with the same mean $\\lambda = n p$. Compare the mean, variance, and Fano factor of the binomial site model to those of the Poisson model.\n\nReport as your final answer the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model. Round your answer to $4$ significant figures and report it as a pure number without units.", "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Number of independent, functionally identical release sites: $n=5$.\n- Probability of release at each site per trial: $p=0.2$.\n- $K$ is the number of vesicles released in a single trial.\n- A Poisson process model is to be considered with a mean $\\lambda = n p$.\n- The Fano factor is defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$.\n- The final answer is the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model, rounded to $4$ significant figures.\n\nValidation assessment:\n- **Scientifically Grounded:** The problem uses the binomial model of synaptic transmission, a foundational concept in cellular neuroscience established by Katz and colleagues. The comparison with a Poisson distribution is a standard and informative analysis in this field. The parameters $n=5$ and $p=0.2$ are within a realistic range for certain types of synapses. The problem is scientifically sound.\n- **Well-Posed:** All necessary parameters and definitions are provided. The tasks are explicit and lead to a unique, stable, and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of subjective or speculative content.\n\nVerdict: The problem is valid. It presents a standard, well-defined exercise in quantitative neuroscience and probability theory. The solution process may proceed.\n\nThe problem describes a process of $n=5$ independent Bernoulli trials, where each trial (the potential release from one site) has a success probability of $p=0.2$. The total number of successes (released vesicles), $K$, therefore follows a binomial distribution, $K \\sim \\mathrm{Binomial}(n, p)$.\n\nThe probability mass function (PMF) for a binomial distribution is given by:\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nwhere $k$ is the number of successes, which can range from $0$ to $n$.\n\nFirst, we will compute the exact count distribution for $K$ given $n=5$ and $p=0.2$. This requires calculating $P(K=k)$ for $k \\in \\{0, 1, 2, 3, 4, 5\\}$. Note that $1-p = 1 - 0.2 = 0.8$.\n\n- For $k=0$: $P(K=0) = \\binom{5}{0} (0.2)^0 (0.8)^5 = 1 \\times 1 \\times 0.32768 = 0.32768$\n- For $k=1$: $P(K=1) = \\binom{5}{1} (0.2)^1 (0.8)^4 = 5 \\times 0.2 \\times 0.4096 = 0.4096$\n- For $k=2$: $P(K=2) = \\binom{5}{2} (0.2)^2 (0.8)^3 = 10 \\times 0.04 \\times 0.512 = 0.2048$\n- For $k=3$: $P(K=3) = \\binom{5}{3} (0.2)^3 (0.8)^2 = 10 \\times 0.008 \\times 0.64 = 0.0512$\n- For $k=4$: $P(K=4) = \\binom{5}{4} (0.2)^4 (0.8)^1 = 5 \\times 0.0016 \\times 0.8 = 0.0064$\n- For $k=5$: $P(K=5) = \\binom{5}{5} (0.2)^5 (0.8)^0 = 1 \\times 0.00032 \\times 1 = 0.00032$\n\nNext, we compute the mean, variance, and Fano factor for this binomial model.\nThe expectation (mean) of a binomial distribution is given by:\n$$\\mathbb{E}[K] = np$$\nSubstituting the given values:\n$$\\mathbb{E}[K] = 5 \\times 0.2 = 1$$\n\nThe variance of a binomial distribution is given by:\n$$\\mathrm{Var}(K) = np(1-p)$$\nSubstituting the given values:\n$$\\mathrm{Var}(K) = 5 \\times 0.2 \\times (1 - 0.2) = 1 \\times 0.8 = 0.8$$\n\nThe Fano factor for the binomial model, $\\mathrm{FF}_{\\text{Binomial}}$, is the ratio of the variance to the mean:\n$$\\mathrm{FF}_{\\text{Binomial}} = \\frac{\\mathrm{Var}(K)}{\\mathbb{E}[K]} = \\frac{np(1-p)}{np} = 1-p$$\nUsing the given value of $p$:\n$$\\mathrm{FF}_{\\text{Binomial}} = 1 - 0.2 = 0.8$$\n\nNow, we consider a Poisson process model with a mean $\\lambda$ matched to the mean of the binomial model. Let $K_{\\text{Poisson}}$ be the random variable for this model.\nThe matching condition is:\n$$\\lambda = \\mathbb{E}[K] = np = 1$$\nSo, we consider a Poisson distribution with parameter $\\lambda=1$.\n\nThe fundamental properties of a Poisson distribution are that its mean and variance are both equal to its parameter $\\lambda$.\nThe mean of the Poisson model is:\n$$\\mathbb{E}[K_{\\text{Poisson}}] = \\lambda = 1$$\nThe variance of the Poisson model is:\n$$\\mathrm{Var}(K_{\\text{Poisson}}) = \\lambda = 1$$\n\nThe Fano factor for the Poisson model, $\\mathrm{FF}_{\\text{Poisson}}$, is therefore:\n$$\\mathrm{FF}_{\\text{Poisson}} = \\frac{\\mathrm{Var}(K_{\\text{Poisson}})}{\\mathbb{E}[K_{\\text{Poisson}}]} = \\frac{\\lambda}{\\lambda} = 1$$\nThis result is definitional for any Poisson process; its variance is always equal to its mean, yielding a Fano factor of unity.\n\nThe final task is to compute the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model.\n$$\\Delta \\mathrm{FF} = |\\mathrm{FF}_{\\text{Binomial}} - \\mathrm{FF}_{\\text{Poisson}}|$$\nSubstituting the calculated values:\n$$\\Delta \\mathrm{FF} = |0.8 - 1| = |-0.2| = 0.2$$\n\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$0.2 \\rightarrow 0.2000$$\n\nThis difference highlights a key feature of the binomial release model: because the number of release sites $n$ is finite, the variance is suppressed relative to the mean. The release process is \"sub-Poissonian\", with a Fano factor strictly less than $1$ (for $p>0$). The magnitude of this deviation from the Poissonian Fano factor of $1$ is simply the release probability $p$.", "answer": "$$\\boxed{0.2000}$$", "id": "2738674"}, {"introduction": "Theories and models are only as good as their ability to describe experimental observations. A crucial skill for a neuroscientist is to statistically test whether a given model, such as the Poisson model of release, is consistent with collected data. This practice [@problem_id:2738672] guides you through the construction and application of a dispersion test, a formal method to evaluate if observed count variability aligns with the Poisson prediction where variance equals the mean. Successfully completing this task will equip you to identify statistically significant deviations like overdispersion, a common finding that often points toward more complex biological regulation than a simple Poisson process can capture.", "problem": "You record vesicle counts from a single synapse under fixed extracellular calcium and stimulation strength in a voltage-clamp experiment. In each of $n = 100$ identical trials, you count the number of released synaptic vesicles, yielding independent and identically distributed (i.i.d.) integer-valued random variables $\\{X_{i}\\}_{i=1}^{n}$. The observed sample mean is $\\bar{X} = 3.2$ and the unbiased sample variance is $s^{2} = 5.8$. You wish to test the null hypothesis that trial-to-trial variability is consistent with a homogeneous Poisson release model with common mean $\\mu$ across trials, that is, $H_{0}: X_{i} \\sim \\text{Poisson}(\\mu)$ i.i.d., versus the alternative $H_{1}$ that the counts are overdispersed relative to the Poisson model.\n\nStarting only from fundamental properties of the Poisson process and count distributions—namely, that for a homogeneous Poisson count the variance equals the mean, that sums of independent Poisson variables are Poisson, and that conditional on the total count, Poisson components are multinomially allocated—construct a dispersion test for $H_{0}$. Derive the null asymptotic distribution of your test statistic under $H_{0}$ without invoking any pre-packaged formula, then compute the value of that statistic for the data above. Using a significance level $\\alpha = 0.01$, state whether $H_{0}$ would be rejected in favor of overdispersion, and briefly interpret a rejection in biological terms in the context of neurotransmitter release mechanisms.\n\nReport only the numerical value of the dispersion test statistic as your final answer. Round your answer to four significant figures. Express the final answer as a pure number (no units).", "solution": "The problem as stated requires the construction and application of a statistical test for Poisson dispersion. Before proceeding, we must validate its premises.\n\nStep 1: Extract Givens.\nThe problem provides the following data and conditions:\n- Number of trials: $n = 100$.\n- The observations $\\{X_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) integer-valued random variables.\n- The sample mean is $\\bar{X} = 3.2$.\n- The unbiased sample variance is $s^{2} = 5.8$.\n- Null hypothesis $H_{0}: X_{i} \\sim \\text{Poisson}(\\mu)$ i.i.d. for a common mean $\\mu$.\n- Alternative hypothesis $H_{1}$: The counts exhibit overdispersion relative to the Poisson model (i.e., variance is greater than the mean).\n- Significance level for the test is $\\alpha = 0.01$.\n- The derivation must start from fundamental properties of the Poisson distribution.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. The Poisson model is a canonical starting point for describing synaptic vesicle release, and testing for deviations like overdispersion is a standard and meaningful procedure in quantitative neuroscience. The problem is well-posed, providing all necessary information ($n$, $\\bar{X}$, $s^{2}$, $\\alpha$) to construct the test statistic, determine its null distribution, and make a decision. The language is objective and precise. The numerical values are physically plausible for synaptic recordings. The problem violates no principles of scientific validity, logic, or well-posedness.\n\nStep 3: Verdict and Action.\nThe problem is valid. We will proceed with the full derivation and solution.\n\nThe fundamental property of a Poisson distribution with mean $\\mu$ is that its variance is also equal to $\\mu$. The null hypothesis $H_{0}$ states that all observations $X_i$ are drawn from a common Poisson distribution $\\text{Poisson}(\\mu)$. This implies that under $H_0$, the true variance of the data-generating process is equal to its true mean. We expect, therefore, that the sample variance $s^2$ should be close to the sample mean $\\bar{X}$. The alternative hypothesis $H_1$ of overdispersion suggests that the true variance is greater than the true mean, which would manifest as $s^2 > \\bar{X}$.\n\nTo formalize this comparison, we construct a test statistic, often called the index of dispersion or variance-to-mean ratio test statistic. Let this statistic be $T$. A logical choice is a scaled ratio of the sample variance to the sample mean. We define it as:\n$$T = \\frac{(n-1)s^2}{\\bar{X}}$$\nwhere $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$ is the unbiased sample variance. Substituting the definition of $s^2$ gives:\n$$T = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}{\\bar{X}}$$\n\nWe must now derive the distribution of $T$ under the null hypothesis $H_0$ from first principles, as instructed. Let $X_i \\sim \\text{Poisson}(\\mu)$ for $i=1, 2, \\dots, n$. Let $N = \\sum_{i=1}^{n} X_i$ be the total number of observed events across all trials. A fundamental property of the Poisson distribution is that the sum of independent Poisson variables is also a Poisson variable. Specifically, $N \\sim \\text{Poisson}(n\\mu)$.\n\nThe crucial insight, provided as a hint, comes from considering the distribution of the individual counts $X_i$ conditional on the total count $N$. For independent Poisson variates, the conditional distribution of the vector $(X_1, X_2, \\dots, X_n)$ given that $\\sum_{i=1}^{n} X_i = k$ is a Multinomial distribution:\n$$(X_1, \\dots, X_n) \\, | \\, \\sum_{i=1}^{n} X_i = k \\sim \\text{Multinomial}(k; p_1, \\dots, p_n)$$\nwhere the probability for the $i$-th category is $p_i = \\frac{\\mu_i}{\\sum_{j=1}^{n} \\mu_j}$. Under our null hypothesis $H_0$, all means are equal, $\\mu_i = \\mu$, so $p_i = \\frac{\\mu}{n\\mu} = \\frac{1}{n}$ for all $i$.\n\nThus, conditional on the total count $N = k$, the problem is equivalent to allocating $k$ items into $n$ bins, with each bin having an equal probability $1/n$ of being chosen. The expected count in each bin is $E[X_i | N=k] = k \\cdot p_i = \\frac{k}{n}$.\nNotice that $\\frac{k}{n} = \\frac{\\sum X_i}{n} = \\bar{X}$. So, the conditional expected value for each $X_i$ is simply the sample mean $\\bar{X}$.\n\nOur test statistic $T$ can be rewritten as:\n$$T = \\sum_{i=1}^{n} \\frac{(X_i - \\bar{X})^2}{\\bar{X}}$$\nThis is precisely the form of the Pearson chi-squared goodness-of-fit statistic, $\\sum \\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}}$. Here, the \"Observed\" counts are the $X_i$ and the \"Expected\" counts are $\\bar{X}$. This statistic tests whether the observed counts $X_i$ are consistent with the expected counts derived from a uniform multinomial model.\n\nFor such a test with $n$ categories, the Pearson statistic is asymptotically distributed as a chi-squared ($\\chi^2$) random variable. The degrees of freedom are given by the number of categories minus $1$, minus the number of parameters estimated from the data to compute the expectations. Here, we have $n$ categories (the trials). The expectation $\\bar{X}$ is determined by the total count $N$, which imposes one linear constraint on the $X_i$ ($\\sum X_i = n\\bar{X}$). Therefore, the degrees of freedom are $n-1$.\n\nSo, conditional on $N$, the statistic $T$ is asymptotically distributed as $\\chi^2_{n-1}$. An important feature of this result is that the limiting distribution does not depend on the value of the conditioning variable $N$ (as long as it is large enough for the asymptotic approximation to be valid). Because the conditional distribution is the same regardless of the value of $N$, the unconditional distribution of $T$ is also asymptotically $\\chi^2_{n-1}$. This relies on the total number of trials $n$ being large. With $n=100$, the asymptotic approximation is appropriate.\n\nUnder $H_0$: $T = \\frac{(n-1)s^2}{\\bar{X}} \\xrightarrow{d} \\chi^2_{n-1}$ as $n \\to \\infty$.\n\nNow, we compute the value of the test statistic for the given data:\n- $n = 100$\n- $\\bar{X} = 3.2$\n- $s^2 = 5.8$\n\nThe value of our test statistic is:\n$$T_{obs} = \\frac{(100-1) \\times 5.8}{3.2} = \\frac{99 \\times 5.8}{3.2} = \\frac{574.2}{3.2} = 179.4375$$\nRounding to four significant figures, we get $T_{obs} = 179.4$.\n\nTo decide whether to reject $H_0$, we compare $T_{obs}$ to the critical value from the $\\chi^2_{n-1} = \\chi^2_{99}$ distribution at a significance level of $\\alpha = 0.01$. The alternative hypothesis is overdispersion ($s^2 > \\bar{X}$), so we perform a one-tailed (upper-tail) test. We need to find the value $c$ such that $P(\\chi^2_{99} > c) = 0.01$.\nThe $0.99$ quantile of the $\\chi^2_{99}$ distribution is approximately $c \\approx 134.64$.\nOur observed statistic is $T_{obs} \\approx 179.4$. Since $179.4 > 134.64$, our observed statistic falls in the rejection region. We therefore reject the null hypothesis $H_0$ at the $\\alpha=0.01$ significance level.\n\nThe rejection of $H_0$ implies that the observed variability in vesicle counts is significantly greater than what would be expected from a simple Poisson process. A simple Poisson model assumes that vesicles are released independently with a constant, low probability across trials. The observed overdispersion ($s^2 > \\bar{X}$) suggests a violation of this assumption. Biologically, this indicates the presence of an additional source of trial-to-trial variability. This could be due to fluctuations in key presynaptic parameters such as:\n1.  The number of vesicles available for release in the \"readily releasable pool\" (RRP) is not constant.\n2.  The probability of release for each vesicle ($p_{ves}$) is not constant across trials, perhaps due to fluctuations in presynaptic calcium influx or in the sensitivity of the release machinery.\nThe rejection of the simple Poisson model in favor of an overdispersed model is a common finding in neuroscience and points toward more complex, heterogeneous mechanisms governing neurotransmitter release.\n\nThe problem requires reporting only the numerical value of the dispersion test statistic.\n$T_{obs} = 179.4375$.\nRounded to four significant figures, the final value is $179.4$.", "answer": "$$\\boxed{179.4}$$", "id": "2738672"}, {"introduction": "After establishing a plausible statistical model for synaptic release, the next step is to estimate its parameters from experimental data. This hands-on practice [@problem_id:2738703] introduces the \"method of failures,\" a classic and powerful technique for estimating the mean quantal content, $\\lambda$, from the proportion of trials where no release occurs. You will apply the principle of maximum likelihood to not only derive a point estimate for $\\lambda$ but also to construct a confidence interval, providing a measure of uncertainty. This exercise is a direct application of statistical theory to a common problem in synaptic physiology, turning raw experimental observations into a quantitative characterization of synaptic strength.", "problem": "A single synapse is stimulated with identical presynaptic voltage steps that evoke action potentials, one per trial, across $M$ independent trials. Assume that the number of neurotransmitter quanta released per action potential is a Poisson random variable with mean $\\lambda$ that is constant across trials. Define a \"failure\" as a trial with zero quanta released. Under the Poisson model, the failure probability per trial is equal to the probability that a Poisson random variable with mean $\\lambda$ equals zero. Across $M$ trials, the number of failures can be modeled as the sum of independent Bernoulli trials, one per action potential.\n\nIn an experiment with $M=200$ trials, the number of observed failures is $56$. Starting from the definitions of the Poisson and Bernoulli/binomial models and the principle of maximum likelihood, derive an estimator for $\\lambda$ per action potential. Then, using large-sample likelihood theory together with Fisher information (or the delta method) based on the Bernoulli/binomial sampling model for failures, construct an approximate two-sided $0.95$ confidence interval for $\\lambda$.\n\nExpress the final answer as three dimensionless numbers in the order: the point estimate $\\hat{\\lambda}$, the lower bound, and the upper bound of the $0.95$ confidence interval. Round your three numbers to four significant figures. No units should be included in the final numerical answer.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard application of maximum likelihood estimation and large-sample theory to a canonical model in cellular neuroscience. The problem is therefore valid. We proceed with the derivation.\n\nThe problem requires the derivation of a point estimate and a confidence interval for the mean $\\lambda$ of a Poisson distribution governing neurotransmitter release. The data provided consist of the total number of trials, $M$, and the number of trials that resulted in a \"failure\" (zero quanta released), $N_f$.\n\nLet $k$ be the random variable representing the number of quanta released in a single trial. According to the problem statement, $k$ follows a Poisson distribution with mean $\\lambda$. The probability mass function is:\n$$ P(k|\\lambda) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!} $$\nA failure is defined as the event $k=0$. The probability of a failure in a single trial, which we denote by $p$, is therefore:\n$$ p = P(k=0|\\lambda) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!} = \\exp(-\\lambda) $$\nThe number of failures, $N_f$, in $M$ independent trials is the sum of $M$ independent Bernoulli trials, each with success probability $p$. Thus, $N_f$ follows a binomial distribution:\n$$ N_f \\sim \\text{Binomial}(M, p) $$\nThe likelihood of observing $N_f$ failures in $M$ trials is given by the binomial probability mass function:\n$$ L(p|N_f, M) = \\binom{M}{N_f} p^{N_f} (1-p)^{M-N_f} $$\nTo find the maximum likelihood estimator (MLE) for $\\lambda$, we first re-parameterize the likelihood function in terms of $\\lambda$ by substituting $p = \\exp(-\\lambda)$:\n$$ L(\\lambda|N_f, M) = \\binom{M}{N_f} (\\exp(-\\lambda))^{N_f} (1-\\exp(-\\lambda))^{M-N_f} $$\nFor analytical convenience, we maximize the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$:\n$$ \\ell(\\lambda) = \\ln\\binom{M}{N_f} + N_f \\ln(\\exp(-\\lambda)) + (M-N_f) \\ln(1-\\exp(-\\lambda)) $$\n$$ \\ell(\\lambda) = \\ln\\binom{M}{N_f} - N_f\\lambda + (M-N_f) \\ln(1-\\exp(-\\lambda)) $$\nTo find the MLE, $\\hat{\\lambda}$, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero:\n$$ \\frac{d\\ell}{d\\lambda} = -N_f + (M-N_f) \\frac{1}{1-\\exp(-\\lambda)} \\cdot (-\\exp(-\\lambda)) \\cdot (-1) = -N_f + \\frac{(M-N_f)\\exp(-\\lambda)}{1-\\exp(-\\lambda)} $$\nSetting the derivative to zero:\n$$ -N_f + \\frac{(M-N_f)\\exp(-\\hat{\\lambda})}{1-\\exp(-\\hat{\\lambda})} = 0 $$\n$$ N_f(1-\\exp(-\\hat{\\lambda})) = (M-N_f)\\exp(-\\hat{\\lambda}) $$\n$$ N_f - N_f\\exp(-\\hat{\\lambda}) = M\\exp(-\\hat{\\lambda}) - N_f\\exp(-\\hat{\\lambda}) $$\n$$ N_f = M\\exp(-\\hat{\\lambda}) $$\nSolving for $\\hat{\\lambda}$:\n$$ \\exp(-\\hat{\\lambda}) = \\frac{N_f}{M} $$\n$$ -\\hat{\\lambda} = \\ln\\left(\\frac{N_f}{M}\\right) $$\n$$ \\hat{\\lambda} = -\\ln\\left(\\frac{N_f}{M}\\right) = \\ln\\left(\\frac{M}{N_f}\\right) $$\nThis is the maximum likelihood estimator for $\\lambda$. Using the provided data, $M=200$ and $N_f=56$:\n$$ \\hat{\\lambda} = \\ln\\left(\\frac{200}{56}\\right) \\approx 1.272965 $$\n\nNext, we construct an approximate $0.95$ confidence interval for $\\lambda$. We use large-sample theory and the delta method. The MLE for the failure probability $p$ is $\\hat{p} = \\frac{N_f}{M}$. For a large number of trials $M$, the sampling distribution of $\\hat{p}$ is approximately normal with mean $p$ and variance $\\text{Var}(\\hat{p}) \\approx \\frac{p(1-p)}{M}$. The estimated variance is $\\frac{\\hat{p}(1-\\hat{p})}{M}$.\n\nWe have the relationship $\\lambda = g(p) = -\\ln(p)$. The estimator is $\\hat{\\lambda} = g(\\hat{p}) = -\\ln(\\hat{p})$. The delta method states that for a function $g(p)$, the variance of $g(\\hat{p})$ can be approximated as:\n$$ \\text{Var}(\\hat{\\lambda}) \\approx [g'(p)]^2 \\text{Var}(\\hat{p}) $$\nThe derivative of $g(p)$ is $g'(p) = \\frac{d}{dp}(-\\ln(p)) = -\\frac{1}{p}$.\nSubstituting this into the variance approximation, evaluated at the MLE $\\hat{p}$:\n$$ \\text{Var}(\\hat{\\lambda}) \\approx \\left(-\\frac{1}{\\hat{p}}\\right)^2 \\frac{\\hat{p}(1-\\hat{p})}{M} = \\frac{1}{\\hat{p}^2} \\frac{\\hat{p}(1-\\hat{p})}{M} = \\frac{1-\\hat{p}}{M\\hat{p}} $$\nThe standard error of $\\hat{\\lambda}$ is the square root of this variance:\n$$ \\text{SE}(\\hat{\\lambda}) = \\sqrt{\\frac{1-\\hat{p}}{M\\hat{p}}} $$\nAn approximate two-sided $(1-\\alpha)$ confidence interval for $\\lambda$ is given by:\n$$ \\hat{\\lambda} \\pm z_{\\alpha/2} \\cdot \\text{SE}(\\hat{\\lambda}) $$\nFor a $0.95$ confidence interval, $\\alpha=0.05$, and the critical value from the standard normal distribution is $z_{\\alpha/2} = z_{0.025} \\approx 1.96$.\n\nNow we substitute the numerical values:\n$$ \\hat{p} = \\frac{56}{200} = 0.28 $$\nThe point estimate for $\\lambda$ is $\\hat{\\lambda} = -\\ln(0.28) \\approx 1.272965$.\nThe standard error of $\\hat{\\lambda}$ is:\n$$ \\text{SE}(\\hat{\\lambda}) = \\sqrt{\\frac{1-0.28}{200 \\times 0.28}} = \\sqrt{\\frac{0.72}{56}} \\approx 0.113389 $$\nThe margin of error for the confidence interval is:\n$$ \\text{ME} = z_{0.025} \\cdot \\text{SE}(\\hat{\\lambda}) \\approx 1.96 \\times 0.113389 \\approx 0.222243 $$\nThe lower bound of the confidence interval is:\n$$ L = \\hat{\\lambda} - \\text{ME} \\approx 1.272965 - 0.222243 = 1.050722 $$\nThe upper bound of the confidence interval is:\n$$ U = \\hat{\\lambda} + \\text{ME} \\approx 1.272965 + 0.222243 = 1.495208 $$\nRounding the point estimate, lower bound, and upper bound to four significant figures gives:\nPoint estimate $\\hat{\\lambda} \\approx 1.273$\nLower bound $L \\approx 1.051$\nUpper bound $U \\approx 1.495$", "answer": "$$ \\boxed{\\begin{pmatrix} 1.273  1.051  1.495 \\end{pmatrix}} $$", "id": "2738703"}]}