{"hands_on_practices": [{"introduction": "The action of Spo11 is orchestrated through its interaction with a suite of partner proteins. This first practice invites you to apply fundamental principles of biochemical equilibrium to predict the assembly of the Spo11 machinery. By using dissociation constants and the law of mass action, you will calculate the fraction of Spo11 engaged in a key ternary complex, providing a quantitative glimpse into the molecular census within a meiotic cell [@problem_id:2828608].", "problem": "In meiotic cells, the topoisomerase-like protein Spo11 initiates recombination by forming a complex with partner proteins, including Ski8 and Rec102. Consider a simplified in vitro system at thermodynamic equilibrium where Spo11 binds Ski8 and Rec102 at two non-overlapping, independent sites with no cooperativity. The dissociation constant for the Spo11–Ski8 interaction is defined by the law of mass action as $K_{d, \\mathrm{Ski8}} = \\dfrac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{free}}}{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}]}$, and analogously $K_{d, \\mathrm{Rec102}} = \\dfrac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Rec102}]_{\\mathrm{free}}}{[\\mathrm{Spo11} \\cdot \\mathrm{Rec102}]}$. Assume Ski8 and Rec102 do not bind each other directly and that their bindings to Spo11 are independent in the sense that the dissociation constant of one ligand is unchanged by the presence or absence of the other on Spo11.\n\nAn experiment is set up with total concentrations $[\\mathrm{Spo11}]_{\\mathrm{T}} = 50$ nM, $[\\mathrm{Ski8}]_{\\mathrm{T}} = 0.50$ µM, and $[\\mathrm{Rec102}]_{\\mathrm{T}} = 0.20$ µM. The dissociation constants are $K_{d, \\mathrm{Ski8}} = 30$ nM and $K_{d, \\mathrm{Rec102}} = 100$ nM. Ski8 and Rec102 are each present in large excess over Spo11 so that ligand depletion is negligible and $[\\mathrm{Ski8}]_{\\mathrm{free}} \\approx [\\mathrm{Ski8}]_{\\mathrm{T}}$ and $[\\mathrm{Rec102}]_{\\mathrm{free}} \\approx [\\mathrm{Rec102}]_{\\mathrm{T}}$.\n\nUsing only the law of mass action and the definition of the dissociation constant as foundational principles, derive from first principles and compute the equilibrium fraction of Spo11 molecules that are in the ternary complex with both Ski8 and Rec102, $f_{\\mathrm{ternary}}$. Express your answer as a dimensionless decimal rounded to four significant figures.", "solution": "The problem presented is a standard exercise in biochemical equilibrium and is scientifically sound, well-posed, and objective. All necessary parameters and simplifying assumptions are provided. We may proceed directly to the derivation.\n\nThe objective is to determine the equilibrium fraction of Spo11 engaged in a ternary complex with both Ski8 and Rec102, which we denote as $f_{\\mathrm{ternary}}$. This fraction is defined as:\n$$f_{\\mathrm{ternary}} = \\frac{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}]}{[\\mathrm{Spo11}]_{\\mathrm{T}}}$$\nwhere $[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}]$ is the equilibrium concentration of the ternary complex and $[\\mathrm{Spo11}]_{\\mathrm{T}}$ is the total concentration of Spo11.\n\nThe fundamental principle governing this system is the conservation of mass for Spo11. At equilibrium, the total concentration of Spo11 is the sum of the concentrations of all its possible forms: free Spo11, Spo11 bound only to Ski8, Spo11 bound only to Rec102, and Spo11 bound to both.\n$$[\\mathrm{Spo11}]_{\\mathrm{T}} = [\\mathrm{Spo11}]_{\\mathrm{free}} + [\\mathrm{Spo11} \\cdot \\mathrm{Ski8}] + [\\mathrm{Spo11} \\cdot \\mathrm{Rec102}] + [\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}]$$\n\nThe problem states that the binding of Ski8 and Rec102 to Spo11 are independent events. This allows us to use the law of mass action for each binding equilibrium separately. The dissociation constants are given as:\n$$K_{d, \\mathrm{Ski8}} = \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{free}}}{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}]}$$\n$$K_{d, \\mathrm{Rec102}} = \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Rec102}]_{\\mathrm{free}}}{[\\mathrm{Spo11} \\cdot \\mathrm{Rec102}]}$$\n\nFrom these definitions, we can express the concentrations of the binary complexes in terms of the free Spo11 concentration, $[\\mathrm{Spo11}]_{\\mathrm{free}}$:\n$$[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}] = \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{free}}}{K_{d, \\mathrm{Ski8}}}$$\n$$[\\mathrm{Spo11} \\cdot \\mathrm{Rec102}] = \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Rec102}]_{\\mathrm{free}}}{K_{d, \\mathrm{Rec102}}}$$\n\nTo find the concentration of the ternary complex, we consider the binding of one ligand to the complex already formed with the other. Due to the independence of binding sites, the dissociation constant for Rec102 binding to the $[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}]$ complex is the same as its binding to free Spo11.\n$$\\mathrm{Spo11} \\cdot \\mathrm{Ski8} + \\mathrm{Rec102} \\rightleftharpoons \\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}$$\nThe equilibrium for this reaction is described by:\n$$K_{d, \\mathrm{Rec102}} = \\frac{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}][\\mathrm{Rec102}]_{\\mathrm{free}}}{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}]}$$\nRearranging and substituting the expression for $[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}]$:\n$$[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}] = \\frac{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8}][\\mathrm{Rec102}]_{\\mathrm{free}}}{K_{d, \\mathrm{Rec102}}} = \\left(\\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{free}}}{K_{d, \\mathrm{Ski8}}}\\right) \\frac{[\\mathrm{Rec102}]_{\\mathrm{free}}}{K_{d, \\mathrm{Rec102}}}$$\n$$[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}] = \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{free}}[\\mathrm{Rec102}]_{\\mathrm{free}}}{K_{d, \\mathrm{Ski8}} K_{d, \\mathrm{Rec102}}}$$\nThe same result is obtained by considering the binding of Ski8 to the $[\\mathrm{Spo11} \\cdot \\mathrm{Rec102}]$ complex, confirming consistency.\n\nThe problem states that the ligand concentrations are in large excess, which justifies the approximation that their free concentrations are equal to their total concentrations: $[\\mathrm{Ski8}]_{\\mathrm{free}} \\approx [\\mathrm{Ski8}]_{\\mathrm{T}}$ and $[\\mathrm{Rec102}]_{\\mathrm{free}} \\approx [\\mathrm{Rec102}]_{\\mathrm{T}}$. We apply this simplification.\n\nNow we substitute the expressions for all complex concentrations into the conservation of mass equation:\n$$[\\mathrm{Spo11}]_{\\mathrm{T}} = [\\mathrm{Spo11}]_{\\mathrm{free}} + \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} + \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}} + \\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}} K_{d, \\mathrm{Rec102}}}$$\nFactor out $[\\mathrm{Spo11}]_{\\mathrm{free}}$:\n$$[\\mathrm{Spo11}]_{\\mathrm{T}} = [\\mathrm{Spo11}]_{\\mathrm{free}} \\left(1 + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} + \\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}} + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}K_{d, \\mathrm{Rec102}}}\\right)$$\nTo find the desired fraction $f_{\\mathrm{ternary}}$, we divide the expression for the ternary complex concentration by the expression for the total Spo11 concentration:\n$$f_{\\mathrm{ternary}} = \\frac{[\\mathrm{Spo11} \\cdot \\mathrm{Ski8} \\cdot \\mathrm{Rec102}]}{[\\mathrm{Spo11}]_{\\mathrm{T}}} = \\frac{\\frac{[\\mathrm{Spo11}]_{\\mathrm{free}}[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}} K_{d, \\mathrm{Rec102}}}}{[\\mathrm{Spo11}]_{\\mathrm{free}} \\left(1 + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} + \\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}} + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}K_{d, \\mathrm{Rec102}}}\\right)}$$\nThe term $[\\mathrm{Spo11}]_{\\mathrm{free}}$ cancels, yielding the final expression for $f_{\\mathrm{ternary}}$:\n$$f_{\\mathrm{ternary}} = \\frac{\\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} \\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}}}{\\left(1 + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}}\\right)\\left(1 + \\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}}\\right)} = \\frac{\\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}K_{d, \\mathrm{Rec102}}}}{1 + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} + \\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}} + \\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}K_{d, \\mathrm{Rec102}}}}$$\nThe latter form is more convenient for calculation.\n\nNow we substitute the given numerical values. First, ensure consistent units.\n$[\\mathrm{Ski8}]_{\\mathrm{T}} = 0.50$ µM = $500$ nM.\n$[\\mathrm{Rec102}]_{\\mathrm{T}} = 0.20$ µM = $200$ nM.\n$K_{d, \\mathrm{Ski8}} = 30$ nM.\n$K_{d, \\mathrm{Rec102}} = 100$ nM.\n\nWe compute the dimensionless ratios:\n$$\\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}} = \\frac{500 \\text{ nM}}{30 \\text{ nM}} = \\frac{50}{3}$$\n$$\\frac{[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Rec102}}} = \\frac{200 \\text{ nM}}{100 \\text{ nM}} = 2$$\nThe product of these ratios is:\n$$\\frac{[\\mathrm{Ski8}]_{\\mathrm{T}}[\\mathrm{Rec102}]_{\\mathrm{T}}}{K_{d, \\mathrm{Ski8}}K_{d, \\mathrm{Rec102}}} = \\left(\\frac{50}{3}\\right)(2) = \\frac{100}{3}$$\nNow substitute these values into the denominator of the expression for $f_{\\mathrm{ternary}}$:\n$$\\text{Denominator} = 1 + \\frac{50}{3} + 2 + \\frac{100}{3} = 3 + \\frac{150}{3} = 3 + 50 = 53$$\nThe numerator is the product term $\\frac{100}{3}$.\nThus, the fraction is:\n$$f_{\\mathrm{ternary}} = \\frac{100/3}{53} = \\frac{100}{3 \\times 53} = \\frac{100}{159}$$\nPerforming the division:\n$$f_{\\mathrm{ternary}} \\approx 0.6289308...$$\nRounding to four significant figures as required by the problem statement gives:\n$$f_{\\mathrm{ternary}} \\approx 0.6289$$\nThis is the final computed value for the fraction of Spo11 in the ternary complex.", "answer": "$$\\boxed{0.6289}$$", "id": "2828608"}, {"introduction": "The creation of double-strand breaks (DSBs) by Spo11 is not an end in itself; it is a means to ensure that every chromosome pair obtains at least one crossover, a prerequisite for accurate segregation. This problem challenges you to build a probabilistic model connecting the molecular rate of DSB formation to this crucial chromosome-level outcome. Using the Poisson distribution to model DSB events, you will determine the minimum activity required from Spo11 to satisfy crossover assurance, linking molecular parameters to cellular function [@problem_id:2828624].", "problem": "Spo11-catalyzed double-strand breaks (DSBs) initiate meiotic recombination. Consider a species with haploid chromosome number $C = 16$ (one bivalent per homolog pair). Assume the following foundational model grounded in standard population-genetic and molecular observations:\n- Along each chromosome, DSBs arise as independent, rare events and can be modeled as a Poisson process with mean $N$ DSBs per chromosome.\n- Each DSB independently matures into a designated crossover (CO) with probability $p$, where $p$ is assumed constant with respect to $N$ over the range considered.\n- Chromosomes behave independently with respect to DSB formation and CO designation.\n\nWithin this framework, crossover assurance requires at least one CO per chromosome (per bivalent). Reduced Spo11 activity lowers the mean DSB count $N$, thereby altering the assurance probability. Let $p = 0.3$ represent the probability that a DSB matures into a CO under the conditions of interest.\n\nUsing only these assumptions, derive from first principles the expression for the probability that all $C$ chromosomes each receive at least one CO in a given meiosis, and then compute the minimal mean DSB count $N$ per chromosome required to achieve this event with probability at least $0.95$. Round your final numerical answer for $N$ to three significant figures. No units are required for $N$.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a simplified but valid model of meiotic recombination initiation and crossover formation, built upon established principles in genetics. All necessary parameters are provided, and the question is formulated without ambiguity. We may therefore proceed with a rigorous derivation.\n\nThe problem asks for two results: first, an expression for the probability that all $C$ chromosomes receive at least one crossover (CO), and second, the minimal mean number of double-strand breaks (DSBs), $N$, per chromosome to achieve this with a probability of at least $0.95$.\n\nLet us begin by modeling the formation of COs on a single chromosome. The number of DSBs on a given chromosome, which we denote by the random variable $K$, is stipulated to follow a Poisson distribution with mean $N$. The probability mass function (PMF) is:\n$$ P(K=k) = \\frac{N^k e^{-N}}{k!} \\quad \\text{for } k = 0, 1, 2, \\dots $$\nEach of these $k$ DSBs matures into a CO with an independent probability $p$. This is a classic thinning process. Let $M$ be the random variable representing the number of COs on the chromosome. For a fixed number of $k$ DSBs, the number of COs $M$ follows a binomial distribution:\n$$ P(M=m | K=k) = \\binom{k}{m} p^m (1-p)^{k-m} \\quad \\text{for } m \\le k $$\nTo find the unconditional distribution of $M$, we must sum over all possible numbers of DSBs, $k$:\n$$ P(M=m) = \\sum_{k=m}^{\\infty} P(M=m | K=k) P(K=k) $$\nSubstituting the respective PMFs:\n$$ P(M=m) = \\sum_{k=m}^{\\infty} \\left( \\binom{k}{m} p^m (1-p)^{k-m} \\right) \\left( \\frac{N^k e^{-N}}{k!} \\right) $$\nWe rearrange the terms to isolate the summation:\n$$ P(M=m) = \\frac{e^{-N} p^m}{m!} \\sum_{k=m}^{\\infty} \\frac{k!}{(k-m)!} (1-p)^{k-m} \\frac{N^k}{k!} $$\n$$ P(M=m) = \\frac{e^{-N} p^m}{m!} \\sum_{k=m}^{\\infty} \\frac{N^k (1-p)^{k-m}}{(k-m)!} $$\nLet the index of summation be changed to $j = k-m$. When $k=m$, $j=0$. The sum becomes:\n$$ P(M=m) = \\frac{e^{-N} p^m}{m!} \\sum_{j=0}^{\\infty} \\frac{N^{j+m} (1-p)^j}{j!} $$\n$$ P(M=m) = \\frac{e^{-N} (Np)^m}{m!} \\sum_{j=0}^{\\infty} \\frac{(N(1-p))^j}{j!} $$\nThe summation is the Taylor series expansion for $\\exp\\left(N(1-p)\\right)$.\n$$ P(M=m) = \\frac{e^{-N} (Np)^m}{m!} \\exp(N(1-p)) = \\frac{(Np)^m}{m!} e^{-N + N(1-p)} = \\frac{(Np)^m e^{-Np}}{m!} $$\nThis demonstrates that the number of COs, $M$, on a single chromosome also follows a Poisson distribution, but with a new mean $\\lambda_{CO} = Np$.\n\nThe next step is to calculate the probability of \"crossover assurance\" for a single chromosome, which is defined as the event of having at least one CO. Let this probability be $P_{single}$.\n$$ P_{single} = P(M \\ge 1) = 1 - P(M=0) $$\nUsing the Poisson PMF for $M$ with mean $Np$:\n$$ P(M=0) = \\frac{(Np)^0 e^{-Np}}{0!} = e^{-Np} $$\nTherefore, the probability of at least one CO on a single chromosome is:\n$$ P_{single} = 1 - e^{-Np} $$\n\nThe problem requires that all $C$ chromosomes achieve crossover assurance. The chromosomes are stated to behave independently. Thus, the total probability, $P_{total}$, is the product of the individual probabilities for each of the $C$ chromosomes:\n$$ P_{total} = (P_{single})^C $$\nSubstituting the expression for $P_{single}$, we arrive at the first part of the answer, the general expression for the probability that all $C$ chromosomes each receive at least one CO:\n$$ P_{total}(N, p, C) = (1 - e^{-Np})^C $$\n\nNow, we must find the minimal mean DSB count $N$ that satisfies $P_{total} \\ge 0.95$. We are given $C=16$ and $p=0.3$. The inequality to be solved is:\n$$ (1 - e^{-N \\cdot 0.3})^{16} \\ge 0.95 $$\nWe solve for $N$:\n$$ 1 - e^{-0.3N} \\ge (0.95)^{1/16} $$\n$$ e^{-0.3N} \\le 1 - (0.95)^{1/16} $$\nTaking the natural logarithm of both sides. Since $\\ln(x)$ is a strictly increasing function, the inequality direction is preserved.\n$$ \\ln(e^{-0.3N}) \\le \\ln\\left(1 - (0.95)^{1/16}\\right) $$\n$$ -0.3N \\le \\ln\\left(1 - (0.95)^{1/16}\\right) $$\nTo isolate $N$, we multiply by $-1/0.3$. This is a negative number, so the inequality must be reversed:\n$$ N \\ge -\\frac{1}{0.3} \\ln\\left(1 - (0.95)^{1/16}\\right) $$\nThe minimal value for $N$ is the lower bound of this range. We compute this value:\n$$ N_{min} = \\frac{-\\ln\\left(1 - (0.95)^{1/16}\\right)}{0.3} $$\nFirst, we evaluate the term $(0.95)^{1/16}$:\n$$ (0.95)^{1/16} = \\exp\\left(\\frac{1}{16}\\ln(0.95)\\right) \\approx \\exp\\left(\\frac{-0.051293}{16}\\right) \\approx \\exp(-0.0032058) \\approx 0.996798 $$\nThen we compute the argument of the logarithm:\n$$ 1 - (0.95)^{1/16} \\approx 1 - 0.996798 = 0.003202 $$\nThe natural logarithm of this value is:\n$$ \\ln(0.003202) \\approx -5.744 $$\nFinally, we compute $N_{min}$:\n$$ N_{min} \\approx \\frac{-(-5.744)}{0.3} = \\frac{5.744}{0.3} \\approx 19.1467 $$\nRounding to three significant figures as required by the problem statement yields $19.1$. This is the minimal mean number of DSBs per chromosome required.", "answer": "$$\n\\boxed{19.1}\n$$", "id": "2828624"}, {"introduction": "The theoretical models of Spo11 activity are tested and refined using data from high-throughput sequencing, which allows us to map DSB hotspots genome-wide. However, raw sequencing counts are confounded by multiple technical and biological biases. This advanced practice puts you in the role of a bioinformatician, tasking you with designing a principled normalization framework to correct for sequencing depth, GC-content bias, and copy-number variation. By implementing this algorithm, you will learn how to transform raw data into a reliable, quantitative measure of hotspot strength, a critical skill in modern genomics [@problem_id:2828634].", "problem": "You are given the task of designing a principled normalization framework for comparing the relative hotspot strength of meiotic recombination initiation events catalyzed by Sporulation Protein 11 (Spo11) across multiple sequencing samples. In practice, Spo11-associated double-strand breaks (DSBs) are detected by sequencing short oligonucleotides covalently bound to Spo11, which are enriched at hotspots. The observed read count at a genomic locus in a given sample is confounded by three main factors: total sequencing depth, deoxyribonucleic acid (DNA) guanine-cytosine (GC) content bias, and copy-number variation (CNV). Your goal is to compute normalized hotspot strengths that are comparable across samples by removing these confounders in a logically consistent way.\n\nBase your solution on the following fundamental definitions and well-tested observations:\n- The Central Dogma of molecular biology establishes the flow of genetic information, and sequencing readouts are quantitative assays of nucleic acid abundance.\n- For Spo11-oligonucleotide maps, the expected read count at a locus is proportional to its true break frequency (hotspot strength), but also scales with total sequencing yield (depth), sequence-dependent assay bias (e.g., GC-content bias), and local copy-number state.\n- It is appropriate to model multiplicative nuisance factors on the observed counts using a log-link and to estimate smooth GC-bias functions by regression over genome-wide background bins that are not specifically enriched for hotspots.\n\nConstruct an algorithm that, for each sample, uses background bins to estimate a smooth GC-bias function and then normalizes hotspot counts by sequencing depth, the estimated GC bias, and the copy-number state. In addition, ensure cross-sample comparability by calibrating a per-sample scale such that the median normalized background signal equals $1$.\n\nYour program must implement the following, derived from first principles:\n- Assume a multiplicative model for observed counts at locus $i$ in sample $s$: observed counts are proportional to the product of sequencing depth, a GC-bias function evaluated at the locus GC fraction, the copy-number at the locus, and the underlying locus-specific strength. Concretely, derive an estimator of the form\n  $$\\hat{S}_{s,h} = \\kappa_s \\cdot \\frac{C_{s,h} + \\alpha}{D_s \\cdot \\hat{g}_s(GC_h) \\cdot CN_{s,h}},$$\n  where $C_{s,h}$ is the observed hotspot count in sample $s$ at hotspot $h$, $D_s$ is the total sequencing depth for sample $s$, $CN_{s,h}$ is the copy-number at hotspot $h$ in sample $s$, $\\hat{g}_s(\\cdot)$ is an estimated GC-bias function for sample $s$, $\\alpha$ is a small pseudocount to handle zeros, and $\\kappa_s$ is a per-sample scale chosen so that the median of normalized background-bin signals equals $1$. You must justify this structure and estimate $\\hat{g}_s$ from background bins using a second-degree polynomial in GC with a log-link.\n- Estimate the GC-bias function by fitting a regression on background bins in each sample: for background bin $i$ with GC fraction $GC_i$, count $B_{s,i}$, copy-number $CN_{s,i}$, and sample depth $D_s$, fit\n  $$\\log\\left(\\frac{B_{s,i} + \\alpha}{D_s \\cdot CN_{s,i}}\\right) \\approx \\beta_{0,s} + \\beta_{1,s} \\cdot GC_i + \\beta_{2,s} \\cdot GC_i^2,$$\n  obtain $\\hat{\\beta}_{s}$, define the unnormalized bias $\\tilde{g}_s(GC) = \\exp\\{\\hat{\\beta}_{0,s} + \\hat{\\beta}_{1,s} \\cdot GC + \\hat{\\beta}_{2,s} \\cdot GC^2\\}$, and renormalize it to have mean $1$ over the background GC values used for fitting:\n  $$\\hat{g}_s(GC) = \\frac{\\tilde{g}_s(GC)}{\\frac{1}{N_s}\\sum_{i=1}^{N_s}\\tilde{g}_s(GC_i)},$$\n  where $N_s$ is the number of background bins in sample $s$.\n- Define the per-sample calibration constant $\\kappa_s$ so that the median normalized background equals $1$:\n  $$\\kappa_s = \\left[\\operatorname{median}_{i=1,\\dots,N_s} \\left(\\frac{B_{s,i} + \\alpha}{D_s \\cdot \\hat{g}_s(GC_i) \\cdot CN_{s,i}}\\right)\\right]^{-1}.$$\n- Use $\\alpha = 0.5$.\n\nInput and test suite:\n- Your program must embed the following three test cases directly in the source code. For each test case, there are two samples, a set of background bins for fitting GC bias, and a set of hotspots for evaluation. The GC fraction of hotspots is the same across samples, but copy-number and counts may differ by sample.\n\n- Test case A (happy path):\n  - Sample $1$: depth $D_1 = 1000000$; background GC fractions $[0.30, 0.40, 0.50, 0.60, 0.70, 0.80]$; background counts $[900, 1000, 1100, 1300, 1500, 1650]$; background copy-numbers $[2, 2, 2, 2, 2, 2]$; hotspot GC fractions $[0.35, 0.55, 0.75]$; hotspot counts $[120, 200, 300]$; hotspot copy-numbers $[2, 2, 2]$.\n  - Sample $2$: depth $D_2 = 750000$; background GC fractions $[0.30, 0.40, 0.50, 0.60, 0.70, 0.80]$; background counts $[500, 600, 700, 900, 1100, 1200]$; background copy-numbers $[2, 2, 2, 2, 2, 2]$; hotspot GC fractions $[0.35, 0.55, 0.75]$; hotspot counts $[80, 140, 360]$; hotspot copy-numbers $[2, 2, 3]$.\n\n- Test case B (boundary conditions: zero counts and single-copy region):\n  - Sample $1$: depth $D_1 = 500000$; background GC fractions $[0.20, 0.35, 0.50, 0.65, 0.80]$; background counts $[600, 700, 700, 710, 680]$; background copy-numbers $[2, 2, 2, 2, 2]$; hotspot GC fractions $[0.20, 0.65, 0.80]$; hotspot counts $[50, 0, 90]$; hotspot copy-numbers $[2, 2, 2]$.\n  - Sample $2$: depth $D_2 = 500000$; background GC fractions $[0.20, 0.35, 0.50, 0.65, 0.80]$; background counts $[300, 400, 420, 320, 340]$; background copy-numbers $[2, 2, 2, 1, 2]$; hotspot GC fractions $[0.20, 0.65, 0.80]$; hotspot counts $[40, 15, 60]$; hotspot copy-numbers $[2, 1, 2]$.\n\n- Test case C (edge case: variable background copy-number and diverging GC bias):\n  - Sample $1$: depth $D_1 = 2000000$; background GC fractions $[0.30, 0.45, 0.60, 0.75]$; background counts $[1500, 1800, 2100, 2500]$; background copy-numbers $[2, 2, 2, 2]$; hotspot GC fractions $[0.45, 0.75]$; hotspot counts $[400, 500]$; hotspot copy-numbers $[2, 2]$.\n  - Sample $2$: depth $D_2 = 1000000$; background GC fractions $[0.30, 0.45, 0.60, 0.75]$; background counts $[900, 1600, 1100, 600]$; background copy-numbers $[2, 3, 2, 1]$; hotspot GC fractions $[0.45, 0.75]$; hotspot counts $[450, 200]$; hotspot copy-numbers $[3, 1]$.\n\nOutput specification:\n- For each test case, compute the normalized hotspot strengths for sample $1$ in the order provided for hotspots, followed by those for sample $2$ in the same hotspot order.\n- Aggregate all results from all test cases into a single list in this order: test case A sample $1$ hotspots, test case A sample $2$ hotspots, test case B sample $1$ hotspots, test case B sample $2$ hotspots, test case C sample $1$ hotspots, test case C sample $2$ hotspots.\n- Each numerical result must be printed as a decimal rounded to exactly $6$ digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\"[1.234000,2.000000,3.141593]\"$).\n\nAngle units are not applicable. No physical units are involved. The outputs are dimensionless real numbers. The program must be self-contained and must not require user input.", "solution": "The problem as stated is scientifically sound and computationally well-posed. It outlines a standard, principled approach for the normalization of sequencing data in genomics, a critical step for a wide range of quantitative analyses. The multiplicative model for confounding factors is a well-established and empirically validated assumption for count-based sequencing assays. The problem provides all necessary data and a clear algorithmic specification. We shall proceed with a formal derivation and implementation.\n\nThe fundamental objective is to estimate the intrinsic strength of Spo$11$-induced double-strand break (DSB) formation, denoted $S_{s,h}$, at a specific genomic hotspot locus $h$ in a sample $s$. The raw data consists of read counts, $C_{s,h}$, obtained from sequencing Spo$11$-associated oligonucleotides. These counts are not directly comparable because they are influenced by several confounding factors. We adopt a multiplicative model for the expected observed count, $\\mathbb{E}[C_{s,h}]$, which is a cornerstone of modern genomic data analysis:\n$$ \\mathbb{E}[C_{s,h}] \\propto D_s \\cdot g_s(GC_h) \\cdot CN_{s,h} \\cdot S_{s,h} $$\nHere, the terms represent:\n1.  $D_s$: The total sequencing depth of sample $s$. A higher sequencing effort will proportionally increase counts across all loci.\n2.  $g_s(GC_h)$: A sample-specific function representing systematic technical bias related to the guanine-cytosine (GC) content of the DNA sequence at locus $h$. This bias arises from various steps in the sequencing library preparation, such as DNA fragmentation and PCR amplification, whose efficiencies are sequence-dependent.\n3.  $CN_{s,h}$: The local DNA copy number at locus $h$ in sample $s$. Most loci in a diploid organism have $CN = 2$, but deletions ($CN  2$) or amplifications ($CN > 2$) alter the amount of available DNA template, proportionally affecting read counts.\n4.  $S_{s,h}$: The true, underlying biological signal—the intrinsic frequency of DSB formation per DNA copy, which we aim to estimate.\n\nTo obtain an estimator $\\hat{S}_{s,h}$ for the true strength $S_{s,h}$, we must invert this model. We substitute the expected count with the observed count $C_{s,h}$ (plus a small pseudocount $\\alpha$ to stabilize estimates at low counts and avoid logarithms of zero) and divide by the estimated confounding factors. This leads to the proposed form:\n$$ \\hat{S}_{s,h} = \\kappa_s \\cdot \\frac{C_{s,h} + \\alpha}{D_s \\cdot \\hat{g}_s(GC_h) \\cdot CN_{s,h}} $$\nThe term $\\kappa_s$ is a further cross-sample calibration constant, which will be discussed later. The pseudocount is given as $\\alpha = 0.5$. Our task reduces to estimating the functions $\\hat{g}_s(\\cdot)$ and the constants $\\kappa_s$ for each sample.\n\n**Step 1: Estimation of the GC-Bias Function, $\\hat{g}_s(GC)$**\n\nThe GC-bias function $g_s(\\cdot)$ is unknown and must be estimated from the data. The problem correctly specifies using \"background\" genomic bins for this purpose. These regions are assumed to be devoid of specific enrichment (i.e., their intrinsic DSB signal is uniformly low and can be treated as a constant). Thus, for a background bin $i$ with count $B_{s,i}$, the model simplifies. By rearranging and taking the logarithm, we can establish a regression framework. The quantity $\\log\\left(\\frac{B_{s,i} + \\alpha}{D_s \\cdot CN_{s,i}}\\right)$ should be linearly related to a function of its GC content, $GC_i$. The problem specifies a quadratic relationship:\n$$ \\log\\left(\\frac{B_{s,i} + \\alpha}{D_s \\cdot CN_{s,i}}\\right) = \\beta_{0,s} + \\beta_{1,s} \\cdot GC_i + \\beta_{2,s} \\cdot GC_i^2 + \\epsilon_i $$\nwhere $\\epsilon_i$ represents residual error. This is a standard multiple linear regression problem. We define the response variable $y_i = \\log\\left(\\frac{B_{s,i} + \\alpha}{D_s \\cdot CN_{s,i}}\\right)$ and a design matrix $X$ where the $i$-th row is $[1, GC_i, GC_i^2]$. The coefficients $\\hat{\\beta}_s = [\\hat{\\beta}_{0,s}, \\hat{\\beta}_{1,s}, \\hat{\\beta}_{2,s}]^T$ are found using ordinary least squares:\n$$ \\hat{\\beta}_s = (X^T X)^{-1} X^T y $$\nWith the estimated coefficients, we define the unnormalized GC-bias function:\n$$ \\tilde{g}_s(GC) = \\exp(\\hat{\\beta}_{0,s} + \\hat{\\beta}_{1,s} \\cdot GC + \\hat{\\beta}_{2,s} \\cdot GC^2) $$\nThis function captures the relative effect of GC content on read density. To ensure it represents a pure bias factor that averages out to one, we normalize it by its mean value across the background regions used for the fit:\n$$ \\hat{g}_s(GC) = \\frac{\\tilde{g}_s(GC)}{\\frac{1}{N_s}\\sum_{i=1}^{N_s}\\tilde{g}_s(GC_i)} $$\nwhere $N_s$ is the number of background bins. This $\\hat{g}_s(GC)$ is our final estimator for the GC-bias correction factor.\n\n**Step 2: Cross-Sample Calibration via $\\kappa_s$**\n\nAfter correcting for depth, GC-bias, and copy number, the resulting signals may still differ in their overall scale between samples due to subtle, unmodeled global effects. To ensure fair comparison, we must calibrate all samples to a common baseline. The problem mandates that the median normalized signal in background regions be set to $1$. First, we compute the partially normalized signal for each background bin $i$:\n$$ S'_{s,i} = \\frac{B_{s,i} + \\alpha}{D_s \\cdot \\hat{g}_s(GC_i) \\cdot CN_{s,i}} $$\nThe calibration constant $\\kappa_s$ is then defined as the inverse of the median of these values:\n$$ \\kappa_s = \\left[\\operatorname{median}_{i=1,\\dots,N_s} \\left( S'_{s,i} \\right)\\right]^{-1} $$\nMultiplying all signals in sample $s$ by $\\kappa_s$ will enforce the condition that the median of fully normalized background signals is unity, thereby making the signal scales comparable across different samples.\n\n**Step 3: Computation of Final Normalized Hotspot Strength**\n\nWith all components estimated, we apply the full normalization formula to the hotspot counts $C_{s,h}$ to obtain our final estimate of intrinsic hotspot strength:\n$$ \\hat{S}_{s,h} = \\kappa_s \\cdot \\frac{C_{s,h} + \\alpha}{D_s \\cdot \\hat{g}_s(GC_h) \\cdot CN_{s,h}} $$\nThis value represents the best estimate of the relative DSB activity at hotspot $h$ in sample $s$, systematically adjusted for the primary known sources of experimental and biological noise. The resulting $\\hat{S}_{s,h}$ values are dimensionless quantities that can be meaningfully compared across different hotspots and different samples. The following program implements this logic for the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Spo11 hotspot normalization problem for the given test cases.\n    \"\"\"\n    alpha = 0.5\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        (\n            {  # Sample 1\n                \"depth\": 1000000,\n                \"bg_gc\": [0.30, 0.40, 0.50, 0.60, 0.70, 0.80],\n                \"bg_counts\": [900, 1000, 1100, 1300, 1500, 1650],\n                \"bg_cn\": [2, 2, 2, 2, 2, 2],\n                \"hs_gc\": [0.35, 0.55, 0.75],\n                \"hs_counts\": [120, 200, 300],\n                \"hs_cn\": [2, 2, 2],\n            },\n            {  # Sample 2\n                \"depth\": 750000,\n                \"bg_gc\": [0.30, 0.40, 0.50, 0.60, 0.70, 0.80],\n                \"bg_counts\": [500, 600, 700, 900, 1100, 1200],\n                \"bg_cn\": [2, 2, 2, 2, 2, 2],\n                \"hs_gc\": [0.35, 0.55, 0.75],\n                \"hs_counts\": [80, 140, 360],\n                \"hs_cn\": [2, 2, 3],\n            },\n        ),\n        # Test Case B\n        (\n            {  # Sample 1\n                \"depth\": 500000,\n                \"bg_gc\": [0.20, 0.35, 0.50, 0.65, 0.80],\n                \"bg_counts\": [600, 700, 700, 710, 680],\n                \"bg_cn\": [2, 2, 2, 2, 2],\n                \"hs_gc\": [0.20, 0.65, 0.80],\n                \"hs_counts\": [50, 0, 90],\n                \"hs_cn\": [2, 2, 2],\n            },\n            {  # Sample 2\n                \"depth\": 500000,\n                \"bg_gc\": [0.20, 0.35, 0.50, 0.65, 0.80],\n                \"bg_counts\": [300, 400, 420, 320, 340],\n                \"bg_cn\": [2, 2, 2, 1, 2],\n                \"hs_gc\": [0.20, 0.65, 0.80],\n                \"hs_counts\": [40, 15, 60],\n                \"hs_cn\": [2, 1, 2],\n            },\n        ),\n        # Test Case C\n        (\n            {  # Sample 1\n                \"depth\": 2000000,\n                \"bg_gc\": [0.30, 0.45, 0.60, 0.75],\n                \"bg_counts\": [1500, 1800, 2100, 2500],\n                \"bg_cn\": [2, 2, 2, 2],\n                \"hs_gc\": [0.45, 0.75],\n                \"hs_counts\": [400, 500],\n                \"hs_cn\": [2, 2],\n            },\n            {  # Sample 2\n                \"depth\": 1000000,\n                \"bg_gc\": [0.30, 0.45, 0.60, 0.75],\n                \"bg_counts\": [900, 1600, 1100, 600],\n                \"bg_cn\": [2, 3, 2, 1],\n                \"hs_gc\": [0.45, 0.75],\n                \"hs_counts\": [450, 200],\n                \"hs_cn\": [3, 1],\n            },\n        ),\n    ]\n    \n    def process_sample(depth, bg_gc, bg_counts, bg_cn, hs_gc, hs_counts, hs_cn):\n        # Convert lists to numpy arrays for vectorized operations\n        bg_gc = np.array(bg_gc)\n        bg_counts = np.array(bg_counts)\n        bg_cn = np.array(bg_cn)\n        hs_gc = np.array(hs_gc)\n        hs_counts = np.array(hs_counts)\n        hs_cn = np.array(hs_cn)\n\n        # Step 1: Estimate GC-bias function\n        # 1a: Set up and solve the linear regression for quadratic polynomial\n        y = np.log((bg_counts + alpha) / (depth * bg_cn))\n        X = np.vstack([np.ones_like(bg_gc), bg_gc, bg_gc**2]).T\n        beta = np.linalg.lstsq(X, y, rcond=None)[0]\n\n        # 1b: Define unnormalized bias function from regression coefficients\n        def tilde_g(gc_values):\n            gc_values = np.array(gc_values)\n            return np.exp(beta[0] + beta[1] * gc_values + beta[2] * gc_values**2)\n\n        # 1c: Normalize the bias function so its mean is 1 over background GCs\n        mean_bias = np.mean(tilde_g(bg_gc))\n        def hat_g(gc_values):\n            return tilde_g(gc_values) / mean_bias\n\n        # Step 2: Calculate the cross-sample calibration constant kappa\n        # 2a: Calculate raw signal for background bins using the normalized GC-bias function\n        raw_bg_signal = (bg_counts + alpha) / (depth * hat_g(bg_gc) * bg_cn)\n        \n        # 2b: Kappa is the reciprocal of the median background signal\n        median_bg_signal = np.median(raw_bg_signal)\n        kappa = 1.0 / median_bg_signal\n\n        # Step 3: Compute final normalized hotspot strengths\n        # 3a: Evaluate GC bias at hotspot locations\n        hotspot_gc_bias = hat_g(hs_gc)\n        \n        # 3b: Calculate the final normalized strengths using all correction factors\n        normalized_strengths = kappa * (hs_counts + alpha) / (depth * hotspot_gc_bias * hs_cn)\n        \n        return normalized_strengths\n\n    all_results = []\n    for case in test_cases:\n        s1_data, s2_data = case\n        \n        s1_results = process_sample(**s1_data)\n        all_results.extend(s1_results)\n\n        s2_results = process_sample(**s2_data)\n        all_results.extend(s2_results)\n\n    # Format results to 6 decimal places and join into a single string\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2828634"}]}