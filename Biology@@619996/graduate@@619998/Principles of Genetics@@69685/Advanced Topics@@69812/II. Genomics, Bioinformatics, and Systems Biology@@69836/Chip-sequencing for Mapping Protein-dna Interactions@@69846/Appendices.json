{"hands_on_practices": [{"introduction": "The first step in analyzing Chromatin Immunoprecipitation sequencing (ChIP-seq) data is to pinpoint the genomic locations of protein binding. This exercise illuminates the mechanical and geometric basis of peak calling, where read positions from single-end sequencing are bioinformatically shifted to reconstruct the centers of the original DNA fragments. By working through this problem [@problem_id:2796471], you will see how a mismatch between the true fragment length, $\\ell$, and the analysis software's shift parameter, $d$, can create predictable artifacts in the data, a crucial insight for troubleshooting analysis pipelines and for appreciating the superior positional accuracy offered by paired-end sequencing.", "problem": "A research group performs Chromatin Immunoprecipitation sequencing (ChIP-seq; Chromatin Immunoprecipitation followed by high-throughput sequencing) to map a transcription factor’s binding sites in a eukaryotic genome. By experimental design, they aim for a fragment size distribution centered near $150$ base pairs (bp), but a pilot quality-control gel reveals over-fragmentation with mean fragment length $\\ell \\approx 120$ bp and a relatively narrow distribution. They sequence either single-end ($r \\approx 50$ bp reads) or paired-end ($r \\approx 50$ bp on each end) libraries from the same material.\n\nAssume the following widely accepted facts:\n- After crosslinking and immunoprecipitation, sequencing reads represent ends of DNA fragments whose centers are enriched near the true protein-DNA binding site at genomic coordinate $x_0$.\n- For a fragment of length $\\ell$, the left end (mapping to the plus strand) lies at the fragment center minus $\\ell/2$, and the right end (mapping to the minus strand) lies at the fragment center plus $\\ell/2$.\n- In single-end analysis, a common peak-calling step shifts plus-strand reads by $+d$ and minus-strand reads by $-d$ to approximate fragment centers; when calibrated correctly, $d \\approx \\ell/2$. In paired-end analysis, fragment midpoints are computed directly from mate pairs, yielding an unbiased estimate of fragment centers under symmetric fragmentation.\n\nThe group’s single-end peak-calling pipeline is configured with a fixed shift $d_0 = 75$ bp (tuned for $\\ell_0 = 150$ bp) and is not automatically re-estimated for the actual data. For the over-fragmented library with mean $\\ell \\approx 120$ bp, they ask how the apparent peak summit positions would compare between single-end and paired-end analyses relative to the true binding coordinate $x_0$. You may assume symmetric fragmentation around $x_0$ and high coverage such that expected positions dominate over sampling noise.\n\nWhich statement best describes the expected behavior of the apparent summits?\n\nA. With $\\ell = 120$ bp and a fixed single-end shift $d_0 = 75$ bp, the merged single-end coverage will show maxima symmetrically flanking the true site at approximately $x_0 \\pm 15$ bp, because $d_0 - \\ell/2 = 75 - 60 = 15$ bp; the paired-end midpoint pileup will peak at $x_0$ (unbiased).\n\nB. With $\\ell = 120$ bp, single-end peaks will still center at $x_0$ because shorter fragments reduce the strand separation, but paired-end midpoint pileups will be biased by $+15$ bp due to inward-facing reads.\n\nC. Both single-end and paired-end summits will be shifted by $+75$ bp relative to $x_0$ because the library was size-selected to less than $150$ bp.\n\nD. With $\\ell = 120$ bp, single-end shifted peaks will be displaced by approximately $\\pm 60$ bp relative to $x_0$, while paired-end peaks will be displaced by approximately $\\pm 75$ bp.", "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and clarity before any attempt at a solution is made.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- **Technique**: Chromatin Immunoprecipitation sequencing (ChIP-seq).\n- **Goal**: Map binding sites of a transcription factor.\n- **Target fragment length**: centered near $150$ base pairs (bp).\n- **Actual mean fragment length**: $\\ell \\approx 120$ bp, with a narrow distribution.\n- **Sequencing read length**: $r \\approx 50$ bp for both single-end and paired-end.\n- **Fundamental assumption 1**: Reads represent ends of DNA fragments whose centers are enriched near the true binding site at genomic coordinate $x_0$.\n- **Fundamental assumption 2**: For a fragment of length $\\ell$, the plus-strand read (left end) is at fragment center minus $\\ell/2$, and the minus-strand read (right end) is at fragment center plus $\\ell/2$.\n- **Fundamental assumption 3**:\n    - Single-end analysis shifts plus-strand reads by $+d$ and minus-strand reads by $-d$. The correct shift is $d \\approx \\ell/2$.\n    - Paired-end analysis computes fragment midpoints directly, providing an unbiased estimate.\n- **Experimental condition**: A fixed, pre-configured shift $d_0 = 75$ bp is used for single-end analysis. This shift was tuned for an expected fragment length of $\\ell_0 = 150$ bp.\n- **Question**: Compare the apparent peak summit positions for single-end versus paired-end analysis on the over-fragmented library ($\\ell \\approx 120$ bp) relative to the true binding coordinate $x_0$.\n- **Simplifying assumptions**: Symmetric fragmentation around $x_0$; high coverage, allowing analysis of expected positions.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the required criteria.\n- **Scientifically Grounded**: The problem describes a standard and realistic scenario in the bioinformatics analysis of ChIP-seq data. The concepts of fragment length distribution, read shifting in single-end analysis, direct midpoint calculation in paired-end analysis, and the consequences of a mismatched shift parameter are all fundamental and factually correct within the domain of genomics. The numerical values provided ($\\ell \\approx 120$ bp, $d_0 = 75$ bp, etc.) are entirely plausible.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information and simplifying assumptions (symmetric fragmentation, high coverage) to derive a deterministic and unique answer regarding the expected positions of signal peaks.\n- **Objective**: The language is technical, precise, and free of any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no contradictions or missing information that would preclude a rigorous solution. Therefore, the problem is **valid**. I will proceed with the derivation and evaluation of options.\n\n### Derivation of Expected Peak Positions\n\nLet the true protein-DNA binding event be centered at the genomic coordinate $x_0$. Due to the nature of sonication or enzymatic digestion, DNA fragments are generated with their centers, which we denote as $c$, distributed symmetrically around $x_0$. Given the assumption of high coverage, the distribution of these centers will form a peak precisely at $x_0$.\n\nThe problem states the mean fragment length is $\\ell = 120$ bp. Due to the \"narrow distribution\" assumption, we can model the system as if all fragments have this length.\n\nA single DNA fragment is defined by its center $c$ and length $\\ell$.\n- Its left end, which gives rise to a plus-strand read, is located at position $p_+ = c - \\ell/2$.\n- Its right end, which gives rise to a minus-strand read, is located at position $p_- = c + \\ell/2$.\n\n**Paired-End Analysis:**\nIn paired-end sequencing, the reads from both ends of a fragment are linked. The fragment center can be directly calculated for each pair:\n$$\n\\text{Midpoint} = \\frac{p_+ + p_-}{2} = \\frac{(c - \\ell/2) + (c + \\ell/2)}{2} = \\frac{2c}{2} = c\n$$\nThe resulting coverage profile is a pileup of the fragment centers $c$. Since the distribution of $c$ is centered at $x_0$, the summit of the paired-end data peak will be located at $x_0$. This method provides an unbiased estimate of the true binding site, as stated in the problem's premises.\n\n**Single-End Analysis with Mismatched Shift:**\nIn single-end sequencing, the plus-strand reads and minus-strand reads are analyzed as two separate distributions. To reconstruct the likely fragment centers, the analysis pipeline shifts the reads.\n- Plus-strand reads at positions $p_+$ are shifted by $+d_0$.\n- Minus-strand reads at positions $p_-$ are shifted by $-d_0$.\n\nThe problem specifies that an incorrect, fixed shift $d_0 = 75$ bp is used, while the actual data has a mean fragment length of $\\ell = 120$ bp, for which the correct shift would have been $d = \\ell/2 = 120/2 = 60$ bp.\n\nLet's calculate the positions of the shifted reads.\n- For a plus-strand read originating from a fragment centered at $c$, its initial position is $p_+ = c - \\ell/2$. Its shifted position is:\n$$\np_{+, \\text{shifted}} = p_+ + d_0 = (c - \\ell/2) + d_0 = c + (d_0 - \\ell/2)\n$$\n- For a minus-strand read originating from the same fragment, its initial position is $p_- = c + \\ell/2$. Its shifted position is:\n$$\np_{-, \\text{shifted}} = p_- - d_0 = (c + \\ell/2) - d_0 = c - (d_0 - \\ell/2)\n$$\nNow, substitute the given values $\\ell = 120$ bp and $d_0 = 75$ bp. The shift error is $\\Delta = d_0 - \\ell/2 = 75 - 60 = 15$ bp.\n\n- The distribution of shifted plus-strand reads will be centered at $c + \\Delta$. Since the distribution of $c$ peaks at $x_0$, the shifted plus-strand coverage will peak at $x_0 + 15$ bp.\n- The distribution of shifted minus-strand reads will be centered at $c - \\Delta$. The shifted minus-strand coverage will peak at $x_0 - 15$ bp.\n\nThe final \"merged coverage\" is the sum of these two shifted distributions. This sum will exhibit two distinct maxima (a \"bimodal\" peak) located symmetrically around the true binding site, at approximately $x_0 - 15$ bp and $x_0 + 15$ bp.\n\n### Option-by-Option Analysis\n\n**A. With $\\ell = 120$ bp and a fixed single-end shift $d_0 = 75$ bp, the merged single-end coverage will show maxima symmetrically flanking the true site at approximately $x_0 \\pm 15$ bp, because $d_0 - \\ell/2 = 75 - 60 = 15$ bp; the paired-end midpoint pileup will peak at $x_0$ (unbiased).**\nThis statement is a precise and accurate description of the derived behavior. The single-end analysis with an oversized shift parameter ($d_0 > \\ell/2$) creates a split peak with maxima at $x_0 \\pm (d_0 - \\ell/2) = x_0 \\pm 15$ bp. The paired-end analysis remains unbiased and correctly identifies the peak at $x_0$. The calculation and reasoning are correct.\n**Verdict: Correct**\n\n**B. With $\\ell = 120$ bp, single-end peaks will still center at $x_0$ because shorter fragments reduce the strand separation, but paired-end midpoint pileups will be biased by $+15$ bp due to inward-facing reads.**\nThis statement is incorrect on both counts. The single-end peak does not center at $x_0$; the mismatched shift $d_0$ explicitly causes a positional artifact. If the shift had been correctly re-estimated to $d = 60$ bp, then the peak would center at $x_0$. The paired-end analysis is unbiased, as demonstrated mathematically; it will peak at $x_0$, not at a biased position.\n**Verdict: Incorrect**\n\n**C. Both single-end and paired-end summits will be shifted by $+75$ bp relative to $x_0$ because the library was size-selected to less than $150$ bp.**\nThis statement is grossly incorrect. The paired-end summit is not shifted. The single-end analysis does not produce a single peak shifted by $+75$ bp; it creates two peaks shifted by $\\pm 15$ bp. The value $75$ bp is the shift parameter $d_0$, not the resulting peak displacement. The reasoning provided is also nonsensical.\n**Verdict: Incorrect**\n\n**D. With $\\ell = 120$ bp, single-end shifted peaks will be displaced by approximately $\\pm 60$ bp relative to $x_0$, while paired-end peaks will be displaced by approximately $\\pm 75$ bp.**\nThis statement misinterprets all the relevant values. The single-end displacement is $\\pm 15$ bp, not $\\pm 60$ bp. The value $60$ bp corresponds to the ideal shift $\\ell/2$. The paired-end analysis yields a single, undisplaced peak at $x_0$, not two peaks displaced by $\\pm 75$ bp.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "2796471"}, {"introduction": "Once a potential binding site—a \"peak\" of reads—is located, its statistical significance must be determined to distinguish it from background noise. This hands-on coding practice [@problem_id:2796499] demystifies the statistical core of peak-calling algorithms by having you implement their logic from first principles. You will construct and compare different null models for background read counts, progressing from a simple global Poisson model to a more sophisticated local Negative Binomial model, thereby learning how choices in background estimation and accounting for overdispersion, $\\phi$, directly impact the discovery of true biological signals.", "problem": "Chromatin Immunoprecipitation sequencing (ChIP-seq) peak calling relies on estimating a background model for read counts in fixed-width genomic windows. Construct a method, grounded in first principles of count modeling for sequencing assays, to estimate local background rates using a matched control track, and quantify how this choice alters the null distribution used for testing enrichment. Use the following core definitions and well-tested modeling assumptions as the starting point: under background, reads landing in a window of length $L$ arise from a Poisson process with mean equal to the expected number of reads in that window, control (input) reads approximate the background distribution of fragmentation and mappability, and overdispersion relative to the Poisson model can be represented by a Gamma-Poisson mixture, yielding a Negative Binomial (NB) marginal for counts.\n\nYour program must implement the following, without using any external data beyond the parameters given in each test case:\n\n- Library scaling between Chromatin Immunoprecipitation sequencing (ChIP-seq) and control: if the total mapped reads are $N_{\\mathrm{ChIP}}$ for ChIP and $N_{\\mathrm{Ctrl}}$ for control, define the scale factor $s = N_{\\mathrm{ChIP}}/N_{\\mathrm{Ctrl}}$.\n\n- Global background rate in a window: let the genome length be $G$ in base pairs and the window length be $L$ in base pairs. The expected global background count in a window under uniform background is\n$$\n\\lambda_{\\mathrm{global}} = \\frac{N_{\\mathrm{ChIP}}}{G}\\,L.\n$$\n\n- Local background rate using the control track: for a local region of length $W$ (in base pairs) centered around the window and containing $C_{\\mathrm{loc}}$ control reads, the raw local background expectation for the window is\n$$\n\\lambda_{\\mathrm{local,raw}} = s \\cdot C_{\\mathrm{loc}} \\cdot \\frac{L}{W}.\n$$\nTo avoid degenerate estimates when $C_{\\mathrm{loc}}$ is small, floor the local rate by the global rate:\n$$\n\\lambda_{\\mathrm{local}} = \\max\\!\\left(\\lambda_{\\mathrm{local,raw}},\\,\\lambda_{\\mathrm{global}}\\right).\n$$\n\n- Null distributions for testing enrichment of an observed ChIP count $X$ in the window:\n  1. Global Poisson null: $X \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{global}})$.\n  2. Local Poisson null: $X \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{local}})$.\n  3. Overdispersed local null via a Gamma-Poisson (Negative Binomial) mixture: assume $X$ has mean $\\mu=\\lambda_{\\mathrm{local}}$ and variance $\\mu + \\phi \\mu^{2}$ with dispersion parameter $\\phi \\ge 0$. Equivalently, $X$ follows a Negative Binomial distribution with shape $r = 1/\\phi$ and success probability $p = r/(r+\\mu)$, yielding\n  $$\n  \\mathbb{E}[X]=\\mu,\\quad \\mathrm{Var}(X)=\\mu+\\phi \\mu^{2}.\n  $$\n  For $\\phi = 0$, this reduces to the Poisson distribution.\n\n- For each null, compute the one-sided tail probability (p-value) for enrichment,\n$$\np = \\Pr\\{X' \\ge X \\mid \\text{null}\\}.\n$$\n\nImplement the above and apply it to the test suite below. For each test case, you are given parameters $(N_{\\mathrm{ChIP}}, N_{\\mathrm{Ctrl}}, G, L, W, C_{\\mathrm{loc}}, X, \\phi)$:\n\n- Test case $1$ (happy path, moderate local enrichment): $(2\\times 10^{7},\\,1.5\\times 10^{7},\\,3\\times 10^{9},\\,500,\\,10{,}000,\\,60,\\,12,\\,0.2)$.\n- Test case $2$ (boundary, zero local control reads; local rate floors to global): $(1.2\\times 10^{7},\\,8\\times 10^{6},\\,3\\times 10^{9},\\,1000,\\,10{,}000,\\,0,\\,5,\\,0.3)$.\n- Test case $3$ (high local background; local rate dominates): $(2\\times 10^{7},\\,2\\times 10^{7},\\,3\\times 10^{9},\\,200,\\,5{,}000,\\,300,\\,20,\\,0.1)$.\n- Test case $4$ (strong overdispersion): $(1.5\\times 10^{7},\\,1.2\\times 10^{7},\\,3\\times 10^{9},\\,400,\\,8{,}000,\\,40,\\,8,\\,0.5)$.\n\nYour program must output, for each test case in the order listed, a list of three floating-point p-values:\n$[p_{\\mathrm{global\\_Poisson}}, p_{\\mathrm{local\\_Poisson}}, p_{\\mathrm{local\\_NB}}]$, where each value is rounded to $6$ decimal places. Aggregate the results for all test cases into a single line containing a list of these per-case lists, with no spaces, for example: $[[a,b,c],[d,e,f],\\dots]$.\n\nYour program should produce exactly one line of output in this format, and must not read any input.", "solution": "The problem presented is a valid, well-posed, and scientifically grounded exercise in the statistical modeling of sequencing data, specifically for the purpose of peak calling in Chromatin Immunoprecipitation sequencing (ChIP-seq) experiments. It correctly formulates the core task of distinguishing true signal from background noise by comparing different null models for read counts. The models proposed—Poisson for uniform background, Poisson for locally adjusted background, and Negative Binomial for overdispersed local background—are standard and fundamental tools in computational genomics. The problem provides all necessary parameters and defines a clear, deterministic procedure for calculation. I will now proceed with a first-principles-based solution.\n\nThe fundamental goal of ChIP-seq analysis is to identify genomic regions where a specific protein of interest is enriched. This is achieved by measuring the number of DNA fragments immunoprecipitated by an antibody targeting the protein. However, the raw count of sequencing reads, denoted by $X$, in any given genomic window is not sufficient to claim enrichment. This count must be evaluated against a null hypothesis that the observed reads arise merely from background processes. The problem requires us to construct and compare three such null models and calculate the statistical significance (p-value) of an observed count $X$ under each.\n\nFirst, we must normalize the sequencing libraries. A ChIP-seq experiment is typically paired with a control experiment (e.g., input DNA) to account for biases. The total number of mapped reads, $N_{\\mathrm{ChIP}}$ and $N_{\\mathrm{Ctrl}}$, may differ. We compute a scaling factor $s = N_{\\mathrm{ChIP}}/N_{\\mathrm{Ctrl}}$ to normalize the control library to the same effective depth as the ChIP library. This ensures that read counts are comparable.\n\nThe next step is to define the expected background read count, $\\lambda$, within a genomic window of length $L$. This $\\lambda$ serves as the parameter for our null distributions.\n\n**1. Global Background Model:**\nThe simplest model assumes that background reads are uniformly distributed across the entire genome of length $G$. The average read density for the ChIP sample is $N_{\\mathrm{ChIP}}/G$ reads per base pair. The expected number of reads in a window of length $L$ is therefore:\n$$\n\\lambda_{\\mathrm{global}} = \\frac{N_{\\mathrm{ChIP}}}{G}\\,L\n$$\nUnder this model, the null hypothesis is that the observed count $X$ is drawn from a Poisson distribution with this global rate: $X' \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{global}})$. The Poisson distribution is the canonical choice for modeling counts of independent events occurring at a constant average rate.\n\n**2. Local Background Model:**\nThe uniform background assumption is a coarse approximation. In reality, the genome exhibits significant local variations in accessibility, mappability, and GC content, which cause background reads to be non-uniformly distributed. A matched control experiment captures these local biases. We can use the read count in the control sample from a larger region of length $W$ centered on our window of interest to derive a more accurate local background estimate. Let $C_{\\mathrm{loc}}$ be the number of control reads in this local region. The expected number of control reads in the smaller window of length $L$ would be $C_{\\mathrm{loc}} \\cdot (L/W)$, assuming uniform distribution *within the local region*. We then scale this by $s$ to match the ChIP library depth, yielding the raw local rate:\n$$\n\\lambda_{\\mathrm{local,raw}} = s \\cdot C_{\\mathrm{loc}} \\cdot \\frac{L}{W}\n$$\nIn regions with very few or zero control reads ($C_{\\mathrm{loc}} \\approx 0$), this estimate can become degenerate and artificially low, leading to spurious significance. To create a more robust estimator, we regularize it by taking the maximum of the local and global estimates. This ensures the local background rate is never lower than the baseline global rate:\n$$\n\\lambda_{\\mathrm{local}} = \\max\\!\\left(\\lambda_{\\mathrm{local,raw}},\\,\\lambda_{\\mathrm{global}}\\right)\n$$\nThe corresponding null hypothesis is then $X' \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{local}})$.\n\n**3. Overdispersed Local Background Model:**\nEmpirical analysis of sequencing data reveals that the variance of read counts is often greater than the mean ($\\mathrm{Var}(X) > \\mathbb{E}[X]$), a phenomenon known as overdispersion. The Poisson model, which assumes $\\mathrm{Var}(X) = \\mathbb{E}[X] = \\lambda$, is inadequate in such cases. A common and theoretically elegant way to model overdispersion is to assume the Poisson rate parameter $\\lambda$ is itself a random variable drawn from a Gamma distribution. This Gamma-Poisson mixture model results in a marginal distribution for the counts that is a Negative Binomial (NB) distribution.\n\nThe NB distribution can be parameterized in several ways. The problem specifies a parameterization with mean $\\mu$ and a dispersion parameter $\\phi$ such that the variance is quadratic in the mean:\n$$\n\\mathrm{Var}(X) = \\mu + \\phi \\mu^2\n$$\nFor our null model, we set the mean to the best estimate of the background, which is the local rate, so $\\mu = \\lambda_{\\mathrm{local}}$. The dispersion parameter $\\phi \\ge 0$ captures the degree of overdispersion; if $\\phi = 0$, the variance reduces to $\\mu$, and the NB distribution converges to the Poisson distribution with mean $\\mu$.\nFor $\\phi > 0$, the NB distribution can be defined by a shape parameter $r = 1/\\phi$ and a success probability $p = r/(r+\\mu)$. The null hypothesis is then $X' \\sim \\mathrm{NB}(r, p)$. This model is more flexible as it can account for biological and technical variability not captured by the simple Poisson process.\n\n**Significance Testing:**\nFor each of the three null models, we compute a p-value, which is the probability of observing a count at least as extreme as the measured count $X$, assuming the null hypothesis is true. This is a one-sided tail probability:\n$$\np = \\Pr\\{X' \\ge X \\mid \\text{null}\\} = \\sum_{k=X}^{\\infty} \\Pr\\{X' = k \\mid \\text{null}\\}\n$$\nThis quantity is calculated using the survival function (SF) of the respective distribution, where $\\mathrm{SF}(k) = \\Pr\\{X' > k\\}$. Thus, $\\Pr\\{X' \\ge X\\} = \\mathrm{SF}(X-1)$.\nThe three p-values to be computed are:\n1.  $p_{\\mathrm{global\\_Poisson}} = \\Pr\\{X' \\ge X \\mid X' \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{global}})\\}$\n2.  $p_{\\mathrm{local\\_Poisson}} = \\Pr\\{X' \\ge X \\mid X' \\sim \\mathrm{Poisson}(\\lambda_{\\mathrm{local}})\\}$\n3.  $p_{\\mathrm{local\\_NB}} = \\Pr\\{X' \\ge X \\mid X' \\sim \\mathrm{NB}(r=1/\\phi, p=r/(r+\\lambda_{\\mathrm{local}}))\\}$\n\nThe following program implements this logic for the specified test cases. It calculates the scaling factor, the global and local background rates, and then the three p-values using the survival functions from the `scipy.stats` library. The results are formatted as per the problem specification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, nbinom\n\ndef solve():\n    \"\"\"\n    Calculates p-values for ChIP-seq enrichment under three different null models.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (N_ChIP, N_Ctrl, G, L, W, C_loc, X, phi)\n    test_cases = [\n        (2e7, 1.5e7, 3e9, 500, 10000, 60, 12, 0.2),\n        (1.2e7, 8e6, 3e9, 1000, 10000, 0, 5, 0.3),\n        (2e7, 2e7, 3e9, 200, 5000, 300, 20, 0.1),\n        (1.5e7, 1.2e7, 3e9, 400, 8000, 40, 8, 0.5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N_ChIP, N_Ctrl, G, L, W, C_loc, X, phi = case\n\n        # Calculate library scaling factor\n        if N_Ctrl == 0:\n            # Avoid division by zero, though not present in test cases.\n            # In a real scenario, a non-zero pseudocount or different logic would be needed.\n            s = 1.0 \n        else:\n            s = N_ChIP / N_Ctrl\n\n        # Calculate global background rate\n        lambda_global = (N_ChIP / G) * L\n\n        # Calculate local background rate\n        lambda_local_raw = s * C_loc * (L / W)\n        lambda_local = max(lambda_local_raw, lambda_global)\n\n        # --- Compute p-values for the three null models ---\n        \n        # 1. Global Poisson null: X ~ Poisson(lambda_global)\n        # p-value is P(X' >= X) = 1 - CDF(X-1) = SF(X-1)\n        p_global_poisson = poisson.sf(X - 1, lambda_global)\n        \n        # 2. Local Poisson null: X ~ Poisson(lambda_local)\n        p_local_poisson = poisson.sf(X - 1, lambda_local)\n        \n        # 3. Overdispersed local null (Negative Binomial)\n        # The problem statement ensures phi > 0 for all test cases.\n        # If phi were 0, the NB model would reduce to the local Poisson model.\n        if phi > 0:\n            mu = lambda_local\n            r = 1.0 / phi\n            # Scipy's nbinom uses parameter p = probability of success.\n            # Mean is mu = n * (1-p) / p, where n is our r.\n            # Solving for p: p = n / (n + mu)\n            # This matches the problem's definition: p = r / (r + mu).\n            p = r / (r + mu)\n            \n            p_local_nb = nbinom.sf(X - 1, n=r, p=p)\n        else:\n            # Fallback to local Poisson if no overdispersion.\n            p_local_nb = p_local_poisson\n            \n        all_results.append([p_global_poisson, p_local_poisson, p_local_nb])\n\n    # Format the final output string exactly as specified.\n    # E.g., [[val1,val2,val3],[val4,val5,val6]] with no spaces.\n    inner_strings = []\n    for res_list in all_results:\n        rounded_res = [f\"{p:.6f}\" for p in res_list]\n        inner_str = f\"[{','.join(rounded_res)}]\"\n        inner_strings.append(inner_str)\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2796499"}, {"introduction": "Beyond identifying binding sites, a primary goal of many ChIP-seq experiments is to quantify and compare the strength of protein binding across different biological conditions. Raw read counts are not directly comparable due to technical variability in factors like library size and immunoprecipitation efficiency. This exercise [@problem_id:2796414] introduces a gold-standard solution to this challenge: the use of an exogenous \"spike-in\" control. You will learn to calculate and apply a spike-in-derived normalization factor, a robust method that corrects for technical variance and enables accurate, quantitative comparisons of protein occupancy.", "problem": "A laboratory performs Chromatin Immunoprecipitation followed by high-throughput DNA sequencing (ChIP-seq) with an exogenous spike-in control to enable cross-sample normalization of protein–DNA binding profiles. Each biological sample receives the same amount of spike-in chromatin from a distinct species, and the sequencing data are processed identically to yield uniquely mapped, duplicate-removed read counts for both endogenous and spike-in genomes.\n\nAssume the following foundational facts. First, in sequencing-based assays, read counts are proportional to the number of DNA fragments sampled from the underlying fragment pool. Second, if an exogenous spike-in chromatin is added in equal amount across samples and is processed together with endogenous chromatin, then any differences in the observed spike-in coverage across samples arise from sample-specific library sampling factors (such as sequencing depth and immunoprecipitation efficiency), not from biological differences in the spike-in itself. Consequently, equalizing spike-in coverage across samples isolates and corrects for these technical factors, and the same multiplicative correction should be applied to the endogenous counts to enable cross-sample comparison.\n\nYou are given two samples: a reference sample and a treated sample. After alignment and quality control, the total spike-in counts are $4.80 \\times 10^{6}$ in the reference sample and $3.20 \\times 10^{6}$ in the treated sample. In the treated sample, a particular promoter region has a raw endogenous read count of $3740$.\n\nUsing only the principles above, first derive an expression for the treated-sample scaling factor that equalizes spike-in coverage to the reference sample, and then apply this factor to rescale the treated sample’s endogenous count for the specified promoter. Report only the rescaled endogenous count for that promoter in the treated sample after normalization to the reference sample’s spike-in level. Round your answer to four significant figures. No units are required for counts.", "solution": "The problem is scientifically valid and well-posed. It describes a standard and methodologically sound procedure in quantitative genomics: the use of an exogenous spike-in control for the normalization of Chromatin Immunoprecipitation sequencing (ChIP-seq) data. The principles provided are cornerstones of quantitative analysis in high-throughput sequencing.\n\nThe central premise is that an equal quantity of spike-in chromatin, distinct from the endogenous chromatin, is introduced into every sample. Consequently, any observed variations in the number of sequencing reads mapped to the spike-in genome are attributable solely to technical differences between the samples, such as variations in immunoprecipitation (IP) efficiency, library construction, and sequencing depth. These differences are not of biological origin with respect to the system under study. To correct for this technical variability, a scaling factor is computed to equalize the spike-in read counts across all samples to a common reference level. This same factor is then applied to the endogenous read counts to enable a valid quantitative comparison of protein-DNA binding events.\n\nLet $C_{spike, ref}$ and $C_{spike, treat}$ denote the total number of uniquely mapped, duplicate-removed reads from the spike-in genome in the reference and treated samples, respectively.\nLet $C_{endo, treat, raw}$ be the raw read count for a specific endogenous promoter region in the treated sample.\nOur objective is to compute the normalized endogenous read count for this promoter in the treated sample, which we shall denote as $C_{endo, treat, norm}$.\n\nWe must first derive a scaling factor, let's call it $\\alpha$, such that when it multiplies the read counts in the treated sample, the spike-in count of the treated sample becomes equal to that of the reference sample. This condition is expressed mathematically as:\n$$ \\alpha \\cdot C_{spike, treat} = C_{spike, ref} $$\nSolving for the scaling factor $\\alpha$ gives:\n$$ \\alpha = \\frac{C_{spike, ref}}{C_{spike, treat}} $$\nThis derived factor encapsulates the ratio of technical efficiencies between the reference and treated samples. A factor greater than $1$ indicates that the treated sample had lower overall signal recovery than the reference sample, and thus its counts must be scaled up.\n\nThe problem mandates that this same multiplicative correction be applied to the endogenous read counts. Therefore, the normalized endogenous count for the promoter region in the treated sample is calculated as:\n$$ C_{endo, treat, norm} = \\alpha \\cdot C_{endo, treat, raw} $$\nBy substituting the expression for $\\alpha$, we obtain the complete formula for normalization:\n$$ C_{endo, treat, norm} = \\left( \\frac{C_{spike, ref}}{C_{spike, treat}} \\right) \\cdot C_{endo, treat, raw} $$\nWe are provided with the following numerical data:\n-   Spike-in count in the reference sample, $C_{spike, ref} = 4.80 \\times 10^{6}$.\n-   Spike-in count in the treated sample, $C_{spike, treat} = 3.20 \\times 10^{6}$.\n-   Raw endogenous count at the promoter in the treated sample, $C_{endo, treat, raw} = 3740$.\n\nFirst, we compute the scaling factor $\\alpha$:\n$$ \\alpha = \\frac{4.80 \\times 10^{6}}{3.20 \\times 10^{6}} = \\frac{4.80}{3.20} = 1.5 $$\nThe scaling factor is an exact value of $1.5$.\n\nNext, we apply this factor to the raw endogenous read count to obtain the normalized count:\n$$ C_{endo, treat, norm} = 1.5 \\cdot 3740 = 5610 $$\nThe calculation results in an integer value of $5610$. The problem asks for the answer to be rounded to four significant figures. The number $5610$ can be written as $5.610 \\times 10^3$, which explicitly has four significant figures. As the result of the calculation is exact and already presented in a form with four digits, no further rounding is required. The normalized read count is $5610$.", "answer": "$$\n\\boxed{5610}\n$$", "id": "2796414"}]}