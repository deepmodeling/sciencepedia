{"hands_on_practices": [{"introduction": "A successful single-cell experiment begins long before the data is generated; it starts with meticulous planning. A critical design question is determining the required sequencing depth to achieve a scientific goal, such as discovering a rare cell type. This foundational practice [@problem_id:2851229] challenges you to apply the binomial distribution to calculate the minimum number of cells, $n$, required to detect a rare population with a given probability, providing a quantitative basis for effective experimental design.", "problem": "A researcher is designing a single-cell RNA sequencing (scRNA-seq) experiment to detect a rare cell type in a heterogeneous tissue. Assume the tissue is sufficiently large that sampling without replacement can be approximated by independent sampling with replacement, and that each captured cell is independently of the rare type with probability $f$, where $0<f<1$. The researcher plans to sequence $n$ cells and will declare success if at least one cell of the rare type is captured among the $n$ sequenced cells.\n\nStarting only from the foundational definitions of Bernoulli trials and the binomial distribution, model the detection event and derive the minimal integer $n$ such that the probability of detecting at least one rare cell is at least $0.95$. Express your final answer as a single closed-form expression in terms of $f$ only. Do not substitute any numerical values. No units are required.", "solution": "The problem statement must first be subject to rigorous validation.\n\n**Step 1: Extracted Givens**\n- The process of capturing a single cell is approximated as an independent Bernoulli trial.\n- The probability that a captured cell is of the rare type is denoted by $f$, where $0 < f < 1$.\n- The total number of cells to be sequenced is $n$.\n- A successful experiment is defined as the capture of at least one rare cell.\n- The condition is that the probability of success must be at least $0.95$.\n- The objective is to derive the minimal integer $n$ that satisfies this condition.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded**: The problem uses a binomial distribution to model cell sampling, which is a standard and fundamentally correct approach in statistical genomics and population genetics for such problems. The assumption of sampling with replacement is a valid approximation for a large population, which is typical for a tissue sample.\n- **Well-Posed**: The problem is clearly stated. It provides all necessary parameters ($f$ and the probability threshold of $0.95$) to determine a unique, minimal integer solution for $n$.\n- **Objective**: The problem is formulated with precise, objective mathematical language. Terms like \"probability\", \"at least one\", and \"minimal integer\" are unambiguous.\n\nThe problem exhibits no flaws. It is not scientifically unsound, incomplete, contradictory, or ill-posed. It is a standard, formalizable problem in applied probability theory relevant to its stated field.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will now proceed with the derivation.\n\nThe task is to find the minimal integer number of cells, $n$, to be sequenced to ensure the probability of detecting at least one rare cell is no less than $0.95$. The fraction of rare cells is $f$.\n\nLet $X$ be the random variable representing the number of rare cells detected in a sample of $n$ cells. Each cell capture is an independent Bernoulli trial with a probability of success (detecting a rare cell) equal to $f$. Consequently, the random variable $X$ follows a binomial distribution, denoted as $X \\sim \\text{Binomial}(n, f)$.\n\nThe probability mass function for a binomial distribution is given by:\n$$P(X=k) = \\binom{n}{k} f^{k} (1-f)^{n-k}$$\nwhere $k$ is the number of successes, which can be any integer from $0$ to $n$.\n\nThe condition stated in the problem is that the probability of detecting at least one rare cell is at least $0.95$. Mathematically, this is expressed as:\n$$P(X \\ge 1) \\ge 0.95$$\n\nIt is more direct to calculate this probability using the complementary event, which is the event of detecting zero rare cells, $P(X=0)$. The relationship is:\n$$P(X \\ge 1) = 1 - P(X=0)$$\n\nSubstituting this into our inequality gives:\n$$1 - P(X=0) \\ge 0.95$$\n\nNow, we calculate $P(X=0)$ using the binomial probability mass function with $k=0$:\n$$P(X=0) = \\binom{n}{0} f^{0} (1-f)^{n-0} = 1 \\cdot 1 \\cdot (1-f)^{n} = (1-f)^{n}$$\n\nSubstituting this result back into the inequality yields:\n$$1 - (1-f)^{n} \\ge 0.95$$\n\nWe must now solve this inequality for $n$.\nFirst, rearrange the terms to isolate the expression containing $n$:\n$$-(1-f)^{n} \\ge 0.95 - 1$$\n$$-(1-f)^{n} \\ge -0.05$$\nMultiplying both sides by $-1$ reverses the inequality sign:\n$$(1-f)^{n} \\le 0.05$$\n\nTo solve for the exponent $n$, we take the natural logarithm of both sides. Since $0 < f < 1$, the base of the exponent, $(1-f)$, is strictly between $0$ and $1$. Both sides of the inequality, $(1-f)^{n}$ and $0.05$, are positive, so the logarithm is well-defined.\n$$\\ln\\left((1-f)^{n}\\right) \\le \\ln(0.05)$$\nUsing the logarithm property $\\ln(a^b) = b\\ln(a)$, we get:\n$$n \\ln(1-f) \\le \\ln(0.05)$$\n\nTo isolate $n$, we must divide by $\\ln(1-f)$. As established, because $0 < (1-f) < 1$, its natural logarithm, $\\ln(1-f)$, is a negative number. Dividing an inequality by a negative number requires reversing the direction of the inequality sign:\n$$n \\ge \\frac{\\ln(0.05)}{\\ln(1-f)}$$\n\nThe problem requires the minimal *integer* $n$ that satisfies this condition. The set of solutions for $n$ is a continuous interval starting from $\\frac{\\ln(0.05)}{\\ln(1-f)}$. The smallest integer that belongs to this set is the ceiling of this lower bound. The ceiling function, denoted $\\lceil x \\rceil$, gives the smallest integer greater than or equal to $x$.\n\nTherefore, the minimal integer $n$ is:\n$$n_{\\min} = \\left\\lceil \\frac{\\ln(0.05)}{\\ln(1-f)} \\right\\rceil$$\n\nThis is the final, closed-form expression for the minimal number of cells $n$ in terms of the rare cell fraction $f$.", "answer": "$$\n\\boxed{\n\\left\\lceil \\frac{\\ln(0.05)}{\\ln(1-f)} \\right\\rceil\n}\n$$", "id": "2851229"}, {"introduction": "Once raw sequencing data is obtained, the crucial first step is quality control (QC) to remove artifacts and low-quality cells that can otherwise distort downstream analyses. This exercise [@problem_id:2851235] guides you through a typical QC workflow, where you will use standard metrics like mitochondrial read fraction ($f_{\\mathrm{mt},i}$) and total UMI count ($u_i$) to construct a summary score. You will then apply principles from statistical decision theory to derive an optimal threshold for separating high-quality cells from low-quality ones based on a Gaussian mixture model.", "problem": "A single-cell RNA sequencing (scRNA-seq) experiment yields a count matrix with entries $c_{ij}$ denoting the number of Unique Molecular Identifier (UMI) molecules for gene $j$ in cell $i$. For each cell $i$, define three quality control metrics grounded in core definitions from single-cell transcriptomics:\n\n- The mitochondrial read fraction $f_{\\mathrm{mt},i}$ is the ratio of UMI counts aligning to the set of annotated mitochondrial genes to the total UMI count in cell $i$, i.e., $f_{\\mathrm{mt},i} = \\frac{c_{i,\\mathrm{mt}}}{c_{i,\\mathrm{tot}}}$ where $c_{i,\\mathrm{tot}} = \\sum_{j} c_{ij}$ and $c_{i,\\mathrm{mt}}$ sums over mitochondrial genes.\n- The number of detected genes $g_i$ is the count of genes with at least one UMI in cell $i$, i.e., $g_i = \\sum_{j} \\mathbf{1}\\{c_{ij} > 0\\}$.\n- The total UMI count $u_i$ is $u_i = \\sum_{j} c_{ij}$.\n\nA commonly used approach to separate low-quality from high-quality cells begins by summarizing these metrics into a scalar score and then fitting a $2$-component mixture model. Consider the scalar summary score\n$$\ns_i = \\ln(1+u_i) + \\ln(1+g_i) - 4\\, f_{\\mathrm{mt},i}.\n$$\nAssume that, in a given dataset, the empirical distribution of $\\{s_i\\}$ is well approximated by a two-component Gaussian mixture reflecting two latent classes: low-quality cells ($\\mathrm{L}$) and high-quality cells ($\\mathrm{H}$). Specifically, suppose that $s \\mid \\mathrm{L} \\sim \\mathcal{N}(\\mu_{\\mathrm{L}}, \\sigma^2)$ and $s \\mid \\mathrm{H} \\sim \\mathcal{N}(\\mu_{\\mathrm{H}}, \\sigma^2)$ with equal variances, and prior class probabilities $\\pi_{\\mathrm{L}}$ and $\\pi_{\\mathrm{H}} = 1 - \\pi_{\\mathrm{L}}$. For a particular experiment, let\n$$\n\\mu_{\\mathrm{L}} = 6.0,\\quad \\mu_{\\mathrm{H}} = 10.5,\\quad \\sigma = 0.8,\\quad \\pi_{\\mathrm{L}} = 0.35,\\quad \\pi_{\\mathrm{H}} = 0.65.\n$$\n\nStarting only from the definitions above, the form of the Gaussian probability density function, and Bayes’ rule under $0$–$1$ loss with equal misclassification costs, derive the Bayes-optimal decision threshold $t$ on $s$ that separates low-quality from high-quality cells by the rule “classify as high-quality if $s \\ge t$ and low-quality otherwise.”\n\nReport the numerical value of $t$ as a pure number (unitless). Round your answer to $4$ significant figures.", "solution": "The problem requires the derivation of the Bayes-optimal decision threshold, $t$, for classifying cells into low-quality ($\\mathrm{L}$) or high-quality ($\\mathrm{H}$) categories based on a scalar score $s$. The decision rule is to classify a cell as high-quality if its score $s$ is greater than or equal to the threshold $t$, and as low-quality otherwise. The analysis is based on a two-component Gaussian mixture model for the distribution of $s$.\n\nFirst, we must perform a validation of the problem statement.\n\nStep 1: Extract Givens\n- Quality control metrics definitions:\n  - Mitochondrial read fraction: $f_{\\mathrm{mt},i} = \\frac{c_{i,\\mathrm{mt}}}{c_{i,\\mathrm{tot}}}$ where $c_{i,\\mathrm{tot}} = \\sum_{j} c_{ij}$.\n  - Number of detected genes: $g_i = \\sum_{j} \\mathbf{1}\\{c_{ij} > 0\\}$.\n  - Total UMI count: $u_i = \\sum_{j} c_{ij}$.\n- Scalar summary score: $s_i = \\ln(1+u_i) + \\ln(1+g_i) - 4\\, f_{\\mathrm{mt},i}$.\n- Latent class conditional distributions for the score $s$:\n  - Low-quality cells: $s \\mid \\mathrm{L} \\sim \\mathcal{N}(\\mu_{\\mathrm{L}}, \\sigma^2)$.\n  - High-quality cells: $s \\mid \\mathrm{H} \\sim \\mathcal{N}(\\mu_{\\mathrm{H}}, \\sigma^2)$.\n- Prior class probabilities: $\\pi_{\\mathrm{L}}$ and $\\pi_{\\mathrm{H}} = 1 - \\pi_{\\mathrm{L}}$.\n- Specific parameter values:\n  - $\\mu_{\\mathrm{L}} = 6.0$\n  - $\\mu_{\\mathrm{H}} = 10.5$\n  - $\\sigma = 0.8$\n  - $\\pi_{\\mathrm{L}} = 0.35$\n  - $\\pi_{\\mathrm{H}} = 0.65$\n- Decision criterion: Bayes-optimal decision rule under a $0$–$1$ loss function with equal misclassification costs.\n- Decision rule structure: Classify as $\\mathrm{H}$ if $s \\ge t$, and as $\\mathrm{L}$ if $s < t$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it uses standard quality control metrics and modeling approaches (Gaussian mixture models) from the field of single-cell transcriptomics. It is well-posed, providing all necessary parameters and a clear objective rooted in statistical decision theory. The language is objective and precise. The problem is self-contained and mathematically consistent. Therefore, the problem is deemed valid.\n\nStep 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\nThe objective is to find the decision threshold $t$ that minimizes the probability of misclassification. For a $0$-$1$ loss function with equal misclassification costs, the Bayes-optimal decision rule is to assign an observation $s$ to the class with the highest posterior probability. The decision rule is to classify as $\\mathrm{H}$ if $P(\\mathrm{H}|s) > P(\\mathrm{L}|s)$ and as $\\mathrm{L}$ if $P(\\mathrm{L}|s) > P(\\mathrm{H}|s)$. The decision threshold $t$ is the value of $s$ where these posterior probabilities are equal:\n$$\nP(\\mathrm{H}|s=t) = P(\\mathrm{L}|s=t)\n$$\nUsing Bayes' rule, the posterior probability for a class $C \\in \\{\\mathrm{L}, \\mathrm{H}\\}$ is given by $P(C|s) = \\frac{p(s|C)P(C)}{p(s)}$, where $p(s|C)$ is the class-conditional probability density (the likelihood), $P(C)$ is the prior probability of the class, and $p(s)$ is the marginal density of $s$. In our notation, $P(C)$ corresponds to $\\pi_C$.\n\nThe equality of posteriors at $s=t$ implies:\n$$\n\\frac{p(t|\\mathrm{H})\\pi_{\\mathrm{H}}}{p(t)} = \\frac{p(t|\\mathrm{L})\\pi_{\\mathrm{L}}}{p(t)}\n$$\nThe marginal density $p(t)$ is non-zero and cancels, yielding an equality between the likelihoods weighted by the priors:\n$$\np(t|\\mathrm{H})\\pi_{\\mathrm{H}} = p(t|\\mathrm{L})\\pi_{\\mathrm{L}}\n$$\nThe problem states that the class-conditional distributions are Gaussian with equal variance: $s \\mid \\mathrm{H} \\sim \\mathcal{N}(\\mu_{\\mathrm{H}}, \\sigma^2)$ and $s \\mid \\mathrm{L} \\sim \\mathcal{N}(\\mu_{\\mathrm{L}}, \\sigma^2)$. The probability density function (PDF) for a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. Substituting the PDFs into our equation gives:\n$$\n\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t-\\mu_{\\mathrm{H}})^2}{2\\sigma^2}\\right)\\right) \\pi_{\\mathrm{H}} = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t-\\mu_{\\mathrm{L}})^2}{2\\sigma^2}\\right)\\right) \\pi_{\\mathrm{L}}\n$$\nThe normalization constant $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ is identical on both sides and cancels out. We are left with:\n$$\n\\exp\\left(-\\frac{(t-\\mu_{\\mathrm{H}})^2}{2\\sigma^2}\\right) \\pi_{\\mathrm{H}} = \\exp\\left(-\\frac{(t-\\mu_{\\mathrm{L}})^2}{2\\sigma^2}\\right) \\pi_{\\mathrm{L}}\n$$\nTo solve for $t$, we take the natural logarithm of both sides:\n$$\n\\ln\\left(\\exp\\left(-\\frac{(t-\\mu_{\\mathrm{H}})^2}{2\\sigma^2}\\right) \\pi_{\\mathrm{H}}\\right) = \\ln\\left(\\exp\\left(-\\frac{(t-\\mu_{\\mathrm{L}})^2}{2\\sigma^2}\\right) \\pi_{\\mathrm{L}}\\right)\n$$\n$$\n-\\frac{(t-\\mu_{\\mathrm{H}})^2}{2\\sigma^2} + \\ln(\\pi_{\\mathrm{H}}) = -\\frac{(t-\\mu_{\\mathrm{L}})^2}{2\\sigma^2} + \\ln(\\pi_{\\mathrm{L}})\n$$\nRearranging the terms to isolate $t$:\n$$\n\\frac{(t-\\mu_{\\mathrm{L}})^2 - (t-\\mu_{\\mathrm{H}})^2}{2\\sigma^2} = \\ln(\\pi_{\\mathrm{L}}) - \\ln(\\pi_{\\mathrm{H}})\n$$\n$$\n(t-\\mu_{\\mathrm{L}})^2 - (t-\\mu_{\\mathrm{H}})^2 = 2\\sigma^2 \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\nExpanding the quadratic terms:\n$$\n(t^2 - 2t\\mu_{\\mathrm{L}} + \\mu_{\\mathrm{L}}^2) - (t^2 - 2t\\mu_{\\mathrm{H}} + \\mu_{\\mathrm{H}}^2) = 2\\sigma^2 \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\n$$\n2t\\mu_{\\mathrm{H}} - 2t\\mu_{\\mathrm{L}} + \\mu_{\\mathrm{L}}^2 - \\mu_{\\mathrm{H}}^2 = 2\\sigma^2 \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\n$$\n2t(\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}}) = \\mu_{\\mathrm{H}}^2 - \\mu_{\\mathrm{L}}^2 + 2\\sigma^2 \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\nFactoring the difference of squares $\\mu_{\\mathrm{H}}^2 - \\mu_{\\mathrm{L}}^2 = (\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}})(\\mu_{\\mathrm{H}} + \\mu_{\\mathrm{L}})$ and dividing by $2(\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}})$ (since $\\mu_{\\mathrm{H}} \\neq \\mu_{\\mathrm{L}}$):\n$$\nt = \\frac{(\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}})(\\mu_{\\mathrm{H}} + \\mu_{\\mathrm{L}})}{2(\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}})} + \\frac{2\\sigma^2}{2(\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}})} \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\nThis simplifies to the general expression for the threshold:\n$$\nt = \\frac{\\mu_{\\mathrm{H}} + \\mu_{\\mathrm{L}}}{2} + \\frac{\\sigma^2}{\\mu_{\\mathrm{H}} - \\mu_{\\mathrm{L}}} \\ln\\left(\\frac{\\pi_{\\mathrm{L}}}{\\pi_{\\mathrm{H}}}\\right)\n$$\nNow, we substitute the given numerical values: $\\mu_{\\mathrm{L}} = 6.0$, $\\mu_{\\mathrm{H}} = 10.5$, $\\sigma = 0.8$, $\\pi_{\\mathrm{L}} = 0.35$, and $\\pi_{\\mathrm{H}} = 0.65$.\n$$\nt = \\frac{10.5 + 6.0}{2} + \\frac{(0.8)^2}{10.5 - 6.0} \\ln\\left(\\frac{0.35}{0.65}\\right)\n$$\n$$\nt = \\frac{16.5}{2} + \\frac{0.64}{4.5} \\ln\\left(\\frac{7}{13}\\right)\n$$\n$$\nt = 8.25 + \\frac{0.64}{4.5} \\ln\\left(\\frac{7}{13}\\right)\n$$\nWe now compute the numerical value:\n$$\n\\ln\\left(\\frac{7}{13}\\right) \\approx -0.6190392\n$$\n$$\n\\frac{0.64}{4.5} \\approx 0.1422222\n$$\n$$\nt \\approx 8.25 + (0.1422222) \\times (-0.6190392)\n$$\n$$\nt \\approx 8.25 - 0.0880354\n$$\n$$\nt \\approx 8.1619646\n$$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$\nt \\approx 8.162\n$$\nThe resulting threshold $t$ is the boundary value for the score $s$. Any cell with $s \\ge 8.162$ will be classified as high-quality, and any cell with $s < 8.162$ will be classified as low-quality, in accordance with the Bayes-optimal decision rule.", "answer": "$$\n\\boxed{8.162}\n$$", "id": "2851235"}, {"introduction": "Technical artifacts are an unavoidable reality of high-throughput sequencing, and one of the most common in droplet-based scRNA-seq is the formation of doublets—single droplets containing two or more cells. This practice [@problem_id:2851174] delves into the statistical modeling of these artifacts. Starting from the Poisson model for UMI counts, you will compute the expected expression profiles, $\\mu$, for different types of doublets and then design a linear classifier to distinguish between them, a key skill in robust data pre-processing.", "problem": "A single-cell ribonucleic acid sequencing (single-cell RNA-seq) experiment with Unique Molecular Identifier (UMI) counting yields integer counts per gene that, conditional on a cell’s transcript fraction profile and library size, can be modeled as independent Poisson random variables. Consider two cell types, denoted type $A$ and type $B$, with known transcript fraction profiles over $G=3$ genes:\n- Type $A$: $p_{A}=\\big(0.5,\\,0.3,\\,0.2\\big)$,\n- Type $B$: $p_{B}=\\big(0.2,\\,0.5,\\,0.3\\big)$,\nwhere $\\sum_{g=1}^{3}p_{A,g}=1$ and $\\sum_{g=1}^{3}p_{B,g}=1$. A pure cell of type $k\\in\\{A,B\\}$ with library size $L$ has gene-wise counts $X_{g}\\sim\\text{Poisson}(L\\,p_{k,g})$, independently across $g$.\n\nAssume that a doublet is the sum of two independently captured cells. Consider two doublet scenarios with fixed library sizes: a homotypic doublet composed of two type $A$ cells with $L_{1}=4000$ and $L_{2}=6000$, and a heterotypic doublet composed of one type $A$ cell with $L_{1}=4000$ and one type $B$ cell with $L_{2}=6000$. For each doublet, the observed count vector is the sum of the two cells’ count vectors.\n\nTasks:\n1. Using only the foundational properties of the Poisson model and the additivity of independent counts, compute the expected gene expression vector (the mean count per gene) for the homotypic doublet and for the heterotypic doublet.\n2. Using a Gaussian approximation to the Poisson model justified by large counts, and assuming a common diagonal covariance across the two doublet classes equal to the pooled mean across classes, design a linear discriminant to separate homotypic from heterotypic doublets. Assume equal class priors and equal misclassification costs. Derive from first principles the scalar decision rule obtained by projecting an observation $x\\in\\mathbb{R}^{3}$ onto a weight vector and thresholding the result.\n3. Compute the Bayes-optimal scalar threshold $t^{\\ast}$ on that projection that minimizes the classification error under the stated assumptions.\n\nExpress the final answer as the exact value of $t^{\\ast}$ in UMI counts (do not include units in the final boxed answer).", "solution": "The problem requires the derivation of a linear discriminant to classify two types of cell doublets from single-cell RNA-sequencing data, and the computation of the optimal decision threshold. The solution will be constructed from first principles, as stipulated.\n\nThe problem is valid. It is scientifically grounded in the standard Poisson model for UMI counts, well-posed with sufficient data, and objectively formulated.\n\nLet the two classes be homotypic ($H_O$) and heterotypic ($H_E$). Let $x \\in \\mathbb{R}^{3}$ be the observed vector of gene counts.\n\n**Task 1: Expected Gene Expression Vectors**\n\nThe expected count for a single gene $g$ in a cell of type $k \\in \\{A, B\\}$ with library size $L$ is $E[X_g] = L \\cdot p_{k,g}$, as $X_g \\sim \\text{Poisson}(L \\cdot p_{k,g})$. A doublet's count vector is the sum of the count vectors of its two constituent cells, which are assumed to be independent. By the linearity of expectation, the expected count vector of a doublet is the sum of the expected count vectors of the two individual cells.\n\nFor the homotypic doublet ($H_O$), we have two cells of type $A$ with library sizes $L_1=4000$ and $L_2=6000$. The expected count vector, $\\mu_{H_O}$, is:\n$$ \\mu_{H_O} = L_1 p_A + L_2 p_A = (L_1 + L_2) p_A $$\nSubstituting the given values:\n$$ \\mu_{H_O} = (4000 + 6000) \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} = 10000 \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 5000 \\\\ 3000 \\\\ 2000 \\end{pmatrix} $$\n\nFor the heterotypic doublet ($H_E$), we have one cell of type $A$ with library size $L_1=4000$ and one cell of type $B$ with library size $L_2=6000$. The expected count vector, $\\mu_{H_E}$, is:\n$$ \\mu_{H_E} = L_1 p_A + L_2 p_B $$\nSubstituting the given values:\n$$ \\mu_{H_E} = 4000 \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} + 6000 \\begin{pmatrix} 0.2 \\\\ 0.5 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 2000 \\\\ 1200 \\\\ 800 \\end{pmatrix} + \\begin{pmatrix} 1200 \\\\ 3000 \\\\ 1800 \\end{pmatrix} = \\begin{pmatrix} 3200 \\\\ 4200 \\\\ 2600 \\end{pmatrix} $$\n\n**Task 2: Linear Discriminant Design**\n\nThe problem specifies using a Gaussian approximation for the count distributions, which is justified as the expected counts (mean parameters $\\lambda$) are large. The class-conditional distributions for an observation $x$ are thus modeled as multivariate normal:\n$$ x | H_O \\sim \\mathcal{N}(\\mu_{H_O}, \\Sigma) $$\n$$ x | H_E \\sim \\mathcal{N}(\\mu_{H_E}, \\Sigma) $$\nA common covariance matrix $\\Sigma$ is assumed. The problem defines $\\Sigma$ as a diagonal matrix with diagonal entries equal to the pooled mean across classes. The pooled mean vector is $\\mu_{pool} = \\frac{1}{2}(\\mu_{H_O} + \\mu_{H_E})$.\n$$ \\mu_{pool} = \\frac{1}{2} \\left( \\begin{pmatrix} 5000 \\\\ 3000 \\\\ 2000 \\end{pmatrix} + \\begin{pmatrix} 3200 \\\\ 4200 \\\\ 2600 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 8200 \\\\ 7200 \\\\ 4600 \\end{pmatrix} = \\begin{pmatrix} 4100 \\\\ 3600 \\\\ 2300 \\end{pmatrix} $$\nThe covariance matrix is therefore:\n$$ \\Sigma = \\text{diag}(\\mu_{pool}) = \\begin{pmatrix} 4100 & 0 & 0 \\\\ 0 & 3600 & 0 \\\\ 0 & 0 & 2300 \\end{pmatrix} $$\nThe inverse covariance matrix is:\n$$ \\Sigma^{-1} = \\begin{pmatrix} 1/4100 & 0 & 0 \\\\ 0 & 1/3600 & 0 \\\\ 0 & 0 & 1/2300 \\end{pmatrix} $$\nFor Linear Discriminant Analysis (LDA) with equal class priors ($P(H_O)=P(H_E)$) and equal misclassification costs, the Bayes-optimal decision rule is to classify an observation $x$ to the class with the higher posterior probability, which simplifies to the class with the higher likelihood. We classify to $H_O$ if $p(x|H_O) > p(x|H_E)$. Taking the logarithm, this is equivalent to $\\ln p(x|H_O) > \\ln p(x|H_E)$.\nThe log-likelihood for a class $k$ is:\n$$ \\ln p(x|k) = -\\frac{G}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\det(\\Sigma)| - \\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) $$\nComparing $\\ln p(x|H_O)$ and $\\ln p(x|H_E)$, the terms not dependent on the class mean $\\mu_k$ cancel. The decision rule simplifies to choosing the class that minimizes $(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)$. We choose $H_O$ if:\n$$ (x - \\mu_{H_E})^T \\Sigma^{-1} (x - \\mu_{H_E}) > (x - \\mu_{H_O})^T \\Sigma^{-1} (x - \\mu_{H_O}) $$\nExpanding both sides:\n$$ x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mu_{H_E} + \\mu_{H_E}^T \\Sigma^{-1} \\mu_{H_E} > x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mu_{H_O} + \\mu_{H_O}^T \\Sigma^{-1} \\mu_{H_O} $$\nAfter simplification and rearrangement, we obtain a linear decision rule:\n$$ 2x^T \\Sigma^{-1} (\\mu_{H_O} - \\mu_{H_E}) > \\mu_{H_O}^T \\Sigma^{-1} \\mu_{H_O} - \\mu_{H_E}^T \\Sigma^{-1} \\mu_{H_E} $$\nThis is of the form $w^T x > t^*$, which is a projection of the observation $x$ onto a weight vector $w$ compared to a threshold $t^*$.\nThe weight vector is $w = \\Sigma^{-1}(\\mu_{H_O} - \\mu_{H_E})$. The scalar decision rule is to compute the projection $w^T x$ and classify as $H_O$ if it exceeds the threshold $t^*$.\n\n**Task 3: Bayes-Optimal Threshold Calculation**\n\nThe Bayes-optimal threshold $t^*$ from the preceding derivation is:\n$$ t^* = \\frac{1}{2} (\\mu_{H_O}^T \\Sigma^{-1} \\mu_{H_O} - \\mu_{H_E}^T \\Sigma^{-1} \\mu_{H_E}) $$\nThis can also be written as $t^* = w^T \\left( \\frac{\\mu_{H_O}+\\mu_{H_E}}{2} \\right) = w^T \\mu_{pool}$.\nGiven the problem's specific definition of $\\Sigma = \\text{diag}(\\mu_{pool})$, the calculation of $t^*$ simplifies considerably.\n$$ t^* = w^T \\mu_{pool} = (\\Sigma^{-1}(\\mu_{H_O} - \\mu_{H_E}))^T \\mu_{pool} = (\\mu_{H_O} - \\mu_{H_E})^T (\\Sigma^{-1})^T \\mu_{pool} $$\nSince $\\Sigma^{-1}$ is diagonal, it is symmetric, so $(\\Sigma^{-1})^T = \\Sigma^{-1}$.\n$$ t^* = (\\mu_{H_O} - \\mu_{H_E})^T \\Sigma^{-1} \\mu_{pool} $$\nThe components of this dot product are $(\\mu_{H_O,g} - \\mu_{H_E,g}) \\cdot \\frac{1}{\\mu_{pool,g}} \\cdot \\mu_{pool,g}$.\nTherefore, the threshold simplifies to the sum of the differences of the mean vector components:\n$$ t^* = \\sum_{g=1}^{3} (\\mu_{H_O,g} - \\mu_{H_E,g}) $$\nThis sum represents the difference in the total expected library sizes of the two doublet types.\nTotal expected counts for $H_O$:\n$$ \\sum_{g=1}^{3} \\mu_{H_O,g} = \\sum_{g=1}^{3} (L_1+L_2)p_{A,g} = (L_1+L_2)\\sum_{g=1}^{3} p_{A,g} = (4000+6000)(1) = 10000 $$\nTotal expected counts for $H_E$:\n$$ \\sum_{g=1}^{3} \\mu_{H_E,g} = \\sum_{g=1}^{3} (L_1 p_{A,g} + L_2 p_{B,g}) = L_1\\sum_{g=1}^{3} p_{A,g} + L_2\\sum_{g=1}^{3} p_{B,g} = 4000(1) + 6000(1) = 10000 $$\nThe total expected counts are identical. Therefore, the threshold must be zero:\n$$ t^* = 10000 - 10000 = 0 $$\nTo verify this result, we can perform the full numerical calculation using the initial formula for $t^*$:\nLet's compute the difference of means vector $\\Delta\\mu = \\mu_{H_O} - \\mu_{H_E}$:\n$$ \\Delta\\mu = \\begin{pmatrix} 5000 \\\\ 3000 \\\\ 2000 \\end{pmatrix} - \\begin{pmatrix} 3200 \\\\ 4200 \\\\ 2600 \\end{pmatrix} = \\begin{pmatrix} 1800 \\\\ -1200 \\\\ -600 \\end{pmatrix} $$\nThe weight vector is $w = \\Sigma^{-1}\\Delta\\mu$:\n$$ w = \\begin{pmatrix} 1/4100 & 0 & 0 \\\\ 0 & 1/3600 & 0 \\\\ 0 & 0 & 1/2300 \\end{pmatrix} \\begin{pmatrix} 1800 \\\\ -1200 \\\\ -600 \\end{pmatrix} = \\begin{pmatrix} 1800/4100 \\\\ -1200/3600 \\\\ -600/2300 \\end{pmatrix} = \\begin{pmatrix} 18/41 \\\\ -1/3 \\\\ -6/23 \\end{pmatrix} $$\nThe threshold is $t^* = w^T \\mu_{pool}$:\n$$ t^* = \\begin{pmatrix} 18/41 & -1/3 & -6/23 \\end{pmatrix} \\begin{pmatrix} 4100 \\\\ 3600 \\\\ 2300 \\end{pmatrix} $$\n$$ t^* = (\\frac{18}{41})(4100) - (\\frac{1}{3})(3600) - (\\frac{6}{23})(2300) $$\n$$ t^* = 18 \\times 100 - 1 \\times 1200 - 6 \\times 100 = 1800 - 1200 - 600 = 0 $$\nThe calculations are consistent. The optimal threshold is zero.", "answer": "$$\\boxed{0}$$", "id": "2851174"}]}