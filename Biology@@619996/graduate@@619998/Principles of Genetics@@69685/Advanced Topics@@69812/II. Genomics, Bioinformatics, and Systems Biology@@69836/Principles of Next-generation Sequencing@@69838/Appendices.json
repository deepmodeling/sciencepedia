{"hands_on_practices": [{"introduction": "Before any biological insights can be gleaned from Next-Generation Sequencing (NGS) data, a critical first step is rigorous quality control and filtering. This practice exercise [@problem_id:2417424] simulates a common real-world scenario where a flaw in the laboratory library preparation process creates artifactual 'adapter-dimer' sequences. By diagnosing the issue and selecting the correct computational fix, you will develop essential troubleshooting skills and understand the critical link between wet-lab procedures and data pre-processing.", "problem": "A paired-end Next-Generation Sequencing (NGS) run yields a high proportion of reads dominated by known adapter motifs. Quality control reports indicate strong adapter content from the start of many reads, and after adapter removal many reads have negligible insert length. Which step in the library construction most plausibly produced these sequences, and what computational strategy is most appropriate to remove them before downstream analysis?\n\nA. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.\n\nB. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.\n\nC. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.\n\nD. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.\n\nE. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.", "solution": "The validity of the problem statement must first be established.\n\n### Step 1: Extract Givens\nThe problem provides the following observations from a paired-end Next-Generation Sequencing (NGS) run:\n1.  A high proportion of reads are dominated by known adapter motifs.\n2.  Quality control reports indicate strong adapter content from the start of many reads.\n3.  After adapter removal, many reads have negligible insert length.\n\nThe problem asks for the most plausible cause of these observations during the library construction phase and the most appropriate computational strategy to remove the resulting artifactual sequences before downstream analysis.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement will now be assessed for scientific validity and clarity.\n- **Scientific Groundedness**: The scenario described is a classic and frequent artifact in NGS library preparation. The concepts presented—paired-end sequencing, adapter motifs, quality control, insert length, adapter ligation, size selection, and various computational filtering strategies—are fundamental and well-established in the field of genomics and bioinformatics. The observations are consistent with the formation of so-called \"adapter-dimers\". This is not speculative.\n- **Well-Posedness**: The problem is well-posed. The set of observations points strongly toward a specific molecular artifact, and the question requests the identification of the generating mechanism and the corresponding corrective computational procedure. A standard, unique, and well-accepted answer exists within the discipline.\n- **Objectivity**: The language is technical and objective. Phrases like \"high proportion,\" \"strong adapter content,\" and \"negligible insert length\" are standard descriptors derived from common QC software outputs (e.g., FastQC). There is no ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and objective. It is based on a realistic and common scenario in experimental genomics. Therefore, the problem is **valid**. A full solution will be derived.\n\n### Derivation and Option Analysis\n\nThe provided observations must be logically connected to a mechanism in library preparation.\n1.  **\"Adapter content from the start of many reads\"**: In a standard sequencing library, a read begins at the 5' end of the DNA insert. Adapter sequences are ligated to the ends of this insert. Therefore, adapter sequence should only appear at the 3' end of a read, and only if the DNA insert is shorter than the sequencing read length (a phenomenon known as \"read-through\"). The presence of adapter sequence at the very beginning (5' end) of the read indicates that the sequencing process began directly on an adapter molecule, not a DNA insert.\n2.  **\"High proportion of reads dominated by known adapter motifs\" and \"negligible insert length after adapter removal\"**: These two points reinforce the first. If a read consists almost entirely of adapter sequence, then after computationally trimming this adapter sequence away, the remaining \"insert\" will have a length close to zero. The high abundance suggests a systematic issue in the library preparation.\n\nThe only plausible mechanism that produces a sequencing template consisting of two adapters ligated together is the formation of **adapter-dimers**. This occurs during the adapter ligation step. If the molar concentration of adapter molecules is excessively high relative to the concentration of DNA fragments, the ligase is more likely to join two adapter molecules together than to join an adapter to a DNA fragment. These adapter-dimer constructs are short, typically between $120$ and $150$ base pairs.\n\nA subsequent step in library preparation is size selection, which is designed to enrich for the desired fragment size distribution (e.g., $300$-$500$ base pairs) and eliminate very short fragments. If this size selection step is performed suboptimally or is not sufficiently stringent, these short adapter-dimer constructs will be retained in the final library, amplified by PCR, and sequenced.\n\nThe computational strategy to correct this issue must specifically target these artifactual reads. The strategy is twofold:\n1.  **Identification**: Use an adapter trimming tool (e.g., `cutadapt`, `Trimmomatic`) with the known sequences of the adapters to identify and remove adapter content from all reads.\n2.  **Filtering**: After trimming, reads that originated from adapter-dimers will be very short. Therefore, a length filter must be applied to discard any read (or, in this paired-end case, the entire read pair) if the remaining sequence length falls below a specified minimum threshold, $\\ell_{\\min}$.\n\nNow, each option will be evaluated against this derived understanding.\n\n**A. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.**\nThis option correctly identifies the cause: adapter-dimer formation from excess adapter concentration, which is then inadequately removed by size selection. The proposed computational strategy—adapter trimming followed by filtering based on a minimum length threshold $\\ell_{\\min}$—is precisely the standard and most effective method for removing these artifacts.\n**Verdict: Correct.**\n\n**B. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.**\nThe cause is inconsistent with the evidence. Ultra-short inserts would lead to adapter read-through at the 3' end of reads, not adapter content at the 5' start. The computational strategy is also incorrect. The problem is one of sequence content (adapter vs. insert), not base quality. While quality can be a proxy for certain issues, it is not the direct solution here. Discarding reads based on an average quality cutoff $Q_{\\min}$ would not specifically target adapter-dimers.\n**Verdict: Incorrect.**\n\n**C. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.**\nThis describes a completely different artifact. Index hopping (or index misassignment) leads to reads from one sample being incorrectly assigned to another sample during demultiplexing. It does not create reads that are composed entirely of adapter sequence. The cause and the proposed solution are irrelevant to the problem described.\n**Verdict: Incorrect.**\n\n**D. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.**\nIncomplete end-repair or A-tailing results in reduced ligation efficiency, leading to lower library yield, not a specific artifact of high abundance. The proposed computational strategy is also suboptimal. Using alignment as the primary filter for adapter-dimers is computationally expensive and presupposes the existence of a reference genome. The standard workflow is to perform adapter/quality trimming and filtering *before* alignment.\n**Verdict: Incorrect.**\n\n**E. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.**\nWhile primer-dimers can form during PCR, the problem specifies reads dominated by \"known adapter motifs.\" In this context, \"adapter\" typically refers to the ligation adapters, not the PCR primers that anneal to a site on the adapters. Adapter-dimers form during ligation, which is pre-PCR. The computational strategy described is a form of digital normalization or error correction based on $k$-mer counts, which is not the standard procedure for removing adapter contamination. The goal is complete removal of these artifactual reads, not collapsing them to a single representative sequence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2417424"}, {"introduction": "Once reads are cleaned, the challenge of genome assembly begins, often by constructing a De Bruijn graph where reads are broken into smaller, overlapping $k$-mers. This exercise [@problem_id:2417480] focuses on resolving a common ambiguous structure called a 'bubble,' which can represent a heterozygous site in a diploid genome. You will apply the physical constraints of paired-end sequencing to determine the most probable path, gaining insight into the sophisticated logic that underpins modern assembly algorithms.", "problem": "In a De Bruijn graph (DBG) constructed from Next-Generation Sequencing (NGS) reads using a $k$-mer size of $k$, nodes represent $(k-1)$-mers and directed edges represent $k$-mers connecting overlapping $(k-1)$-mers. A common local substructure in such graphs is a “bubble,” where two directed paths diverge from a source node and reconverge at a sink node.\n\nSuppose a DBG built from a diploid genome exhibits a bubble between nodes $S$ and $T$, with two candidate paths $P_1$ and $P_2$ that differ by a small number of nucleotides but reconnect to the same sink $T$. Assume you have Illumina paired-end (PE) reads of length $r = 150$ base pairs (bp), generated from a library whose fragment length (the genomic distance between the outermost sequenced bases of a read pair) is approximately normally distributed with mean $\\mu = 450$ bp and standard deviation $\\sigma = 30$ bp. A particular read pair maps such that the left mate aligns upstream of $S$ and ends exactly at the boundary before $S$, and the right mate aligns downstream of $T$ and begins exactly at the boundary after $T$ (so the interior of the fragment traverses exactly one of the two alternative paths between $S$ and $T$). The two alternative paths have lengths $L_1 = 120$ bp for $P_1$ and $L_2 = 200$ bp for $P_2$.\n\nWhich option best explains what the bubble represents in a DBG and correctly states how paired-end information can be used to resolve it in this scenario, including which path is more strongly supported by the given data?\n\nA. A bubble is formed by two nearly identical alternative paths between the same source and sink, typically due to a local polymorphism (for example, a single nucleotide polymorphism) or a transient sequencing error. Paired-end reads that anchor the flanks supply a fragment-length constraint: for each candidate path, compute the implied fragment length and compare it to the library’s insert-size distribution; here, $P_1$ is better supported.\n\nB. A bubble is formed by two nearly identical alternative paths between the same source and sink, typically due to a local polymorphism or a transient sequencing error. Paired-end reads that anchor the flanks supply a fragment-length constraint; here, $P_2$ is better supported.\n\nC. A bubble arises from incorrect read orientation in the graph; it should be resolved by discarding inward-facing (FR) pairs and favoring the path consistent with outward-facing (RF) orientation, which selects $P_2$.\n\nD. A bubble primarily reflects uneven coverage; resolution should rely on choosing the branch with higher $k$-mer counts, and paired-end information should be ignored because both paths reconnect to the same sink node.", "solution": "The problem statement shall first be subjected to rigorous validation.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n\nThe problem provides the following explicit information:\n-   **Graph Structure**: A De Bruijn graph (DBG) is constructed from Next-Generation Sequencing (NGS) reads.\n-   **k-mer size**: $k$.\n-   **Node and Edge Definition**: Nodes are $(k-1)$-mers. Directed edges are $k$-mers.\n-   **Local Substructure**: A \"bubble\" exists between a source node $S$ and a sink node $T$.\n-   **Alternative Paths**: Two paths, $P_1$ and $P_2$, exist between $S$ and $T$.\n-   **Path Lengths**: The sequence length of path $P_1$ is $L_1 = 120$ bp. The sequence length of path $P_2$ is $L_2 = 200$ bp.\n-   **Sequencing Data**: Illumina paired-end (PE) reads.\n-   **Read Length**: $r = 150$ bp.\n-   **Fragment Length Distribution**: Approximately normal with mean $\\mu = 450$ bp and standard deviation $\\sigma = 30$ bp.\n-   **Read Pair Mapping**: A specific PE read pair has its left mate ending immediately before $S$ and its right mate beginning immediately after $T$. The unsequenced interior of the fragment corresponds to one of the paths, $P_1$ or $P_2$.\n\n**Step 2: Validation Using Extracted Givens**\n\nThe problem is evaluated against established scientific and logical criteria.\n\n-   **Scientifically Grounded**: The problem describes a standard scenario in bioinformatics, specifically in the context of genome assembly. De Bruijn graphs, bubbles caused by polymorphisms in diploid genomes, and the use of paired-end read constraints for resolving such ambiguities are all fundamental and well-established concepts. The numerical values provided for read length, fragment length distribution, and path lengths are realistic for modern sequencing experiments.\n-   **Well-Posed**: The problem is clearly stated and provides all necessary data to arrive at a unique, logical conclusion. The question asks for both a conceptual explanation and a quantitative decision, for which the givens are sufficient.\n-   **Objective**: The problem is framed in precise, technical language, free from subjectivity or ambiguity.\n\nThe problem does not violate any criteria for invalidity. It is not scientifically unsound, non-formalizable, incomplete, contradictory, or ill-posed.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Solution\n\nA bubble in a De Bruijn graph represents a divergence and re-convergence of paths between two nodes, $S$ and $T$. In the context of a diploid genome, such a structure is typically caused by a heterozygous locus, such as a single nucleotide polymorphism (SNP) or a small insertion/deletion (indel), where the two paths represent the two different alleles. Sequencing errors can also create bubbles, but these are generally distinguishable by their significantly lower k-mer coverage.\n\nPaired-end sequencing provides a powerful constraint for resolving these ambiguities. DNA is fragmented, and fragments of a certain size-selected distribution are sequenced from both ends. This generates pairs of reads (mates) with a known relative distance and orientation. If a read pair spans a bubble—meaning one mate maps to the sequence before the bubble and the other mate maps after it—the total length of the original DNA fragment can be inferred for each possible path through the bubble. The path that results in an implied fragment length more consistent with the known library distribution is considered the more likely correct path.\n\nLet us formalize the calculation for the given scenario.\nThe fragment length, $F$, is the total length of the DNA molecule from which a paired-end read is derived. The reads themselves, each of length $r$, correspond to the sequences at the two ends of this fragment. The problem states that the left mate ends just before node $S$ and the right mate begins just after node $T$. This implies that the unsequenced gap between the two reads, i.e., the \"interior of the fragment\", is precisely the sequence corresponding to the path taken from $S$ to $T$.\n\nThe unsequenced gap has a length equal to the length of the path taken, $L_1$ or $L_2$. The standard model for inferring fragment length from paired-end reads where the reads do not overlap is that the fragment length $F$ is the sum of the lengths of the two reads plus the length of the unsequenced gap.\n$$F = \\text{length(read 1)} + L_{gap} + \\text{length(read 2)}$$\nHowever, the problem uses a slightly non-standard but explicit definition for fragment length: \"the genomic distance between the outermost sequenced bases of a read pair\". In the common case where the reads face inward, this is the distance from the 5' end of the forward read to the 5' end of the reverse read, which is equivalent to the total length of the original DNA fragment. The unsequenced gap is the part of the fragment not covered by either read, so its length is $F - 2r$. The problem states this gap corresponds to the path, so $L_{gap} = F - 2r$, which implies the inferred total fragment length is $F = L_{gap} + 2r$.\n\n**Case 1: The fragment traverses path $P_1$.**\nThe length of the unsequenced gap is $L_{gap} = L_1 = 120$ bp.\nThe implied fragment length, $F_1$, is:\n$$F_1 = L_1 + 2r = 120 + 2 \\times 150 = 120 + 300 = 420 \\text{ bp}$$\n\n**Case 2: The fragment traverses path $P_2$.**\nThe length of the unsequenced gap is $L_{gap} = L_2 = 200$ bp.\nThe implied fragment length, $F_2$, is:\n$$F_2 = L_2 + 2r = 200 + 2 \\times 150 = 200 + 300 = 500 \\text{ bp}$$\n\nNow, we must assess which of these implied fragment lengths, $F_1 = 420$ bp or $F_2 = 500$ bp, is more probable given a normal distribution $N(\\mu=450, \\sigma=30)$. The probability density is maximized at the mean and decreases for values further from the mean. We can quantify this by calculating the distance of each value from the mean $\\mu = 450$ bp.\n\nFor $F_1$: The deviation from the mean is $|F_1 - \\mu| = |420 - 450| = 30$ bp. This corresponds to a Z-score of $(420 - 450) / 30 = -1.0$.\nFor $F_2$: The deviation from the mean is $|F_2 - \\mu| = |500 - 450| = 50$ bp. This corresponds to a Z-score of $(500 - 450) / 30 = 50/30 \\approx +1.67$.\n\nSince $|-1.0| < |+1.67|$, the implied fragment length $F_1 = 420$ bp is significantly closer to the mean of the distribution than $F_2 = 500$ bp. Therefore, the paired-end read data provides strong evidence in support of path $P_1$.\n\n### Evaluation of Options\n\n**A. A bubble is formed by two nearly identical alternative paths between the same source and sink, typically due to a local polymorphism (for example, a single nucleotide polymorphism) or a transient sequencing error. Paired-end reads that anchor the flanks supply a fragment-length constraint: for each candidate path, compute the implied fragment length and compare it to the library’s insert-size distribution; here, $P_1$ is better supported.**\nThis statement correctly describes the origin of a bubble and the principle of its resolution using PE reads. The conclusion that $P_1$ is better supported matches our derivation.\n**Verdict: Correct.**\n\n**B. A bubble is formed by two nearly identical alternative paths between the same source and sink, typically due to a local polymorphism or a transient sequencing error. Paired-end reads that anchor the flanks supply a fragment-length constraint; here, $P_2$ is better supported.**\nThe explanation is correct, but the conclusion is false. As calculated, the evidence supports path $P_1$, not $P_2$.\n**Verdict: Incorrect.**\n\n**C. A bubble arises from incorrect read orientation in the graph; it should be resolved by discarding inward-facing (FR) pairs and favoring the path consistent with outward-facing (RF) orientation, which selects $P_2$.**\nThis statement is fundamentally flawed. Bubbles arise from sequence variation, not incorrect read orientation. Furthermore, standard Illumina libraries produce inward-facing (FR) pairs; outward-facing (RF) pairs are anomalous and suggest structural variants like inversions, not simple bubbles. The proposed resolution mechanism is invalid.\n**Verdict: Incorrect.**\n\n**D. A bubble primarily reflects uneven coverage; resolution should rely on choosing the branch with higher $k$-mer counts, and paired-end information should be ignored because both paths reconnect to the same sink node.**\nThis statement is incorrect. The primary cause of a bubble is sequence divergence, although this can lead to uneven coverage. While using k-mer counts is a valid complementary strategy, the explicit claim that paired-end information should be ignored is a grave error. The fact that the paths reconnect is precisely what makes the fragment length constraint so effective for resolving the ambiguity.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2417480"}, {"introduction": "After an assembly is generated, we must ask: how good is it? This final practice [@problem_id:2417502] introduces the NG50 statistic, a universal and essential metric for quantifying the contiguity of a draft genome. By working through the calculation, you will learn how to assess the quality of an assembly in relation to an estimated genome size, a critical skill for evaluating the outcome of a sequencing and assembly project.", "problem": "A draft genome assembly derived from Next-Generation Sequencing (NGS) consists of contigs with lengths $\\{100, 50, 20, 5, 5\\}$ kilobase pairs (kbp). The estimated haploid genome size is $150$ kilobase pairs (kbp). The NG50 statistic is defined as follows: given an estimated genome size $G$, sort all contigs in nonincreasing order by length and compute the running cumulative sum from the longest to the shortest; the NG50 is the contig length $L$ at which the cumulative sum first equals or exceeds $\\frac{1}{2}G$. Compute the NG50 for this assembly. Express your answer in kilobase pairs (kbp). Do not use a percentage sign anywhere in your work.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n1.  A set of contig lengths: $\\{100, 50, 20, 5, 5\\}$ kilobase pairs (kbp).\n2.  Estimated haploid genome size: $150$ kilobase pairs (kbp).\n3.  Definition of NG50: Given an estimated genome size $G$, sort all contigs in nonincreasing order by length and compute the running cumulative sum from the longest to the shortest; the NG50 is the contig length $L$ at which the cumulative sum first equals or exceeds $\\frac{1}{2}G$.\n\nValidation verdict:\nThe problem is valid. It is scientifically grounded, as it concerns the calculation of the NG50 statistic, a standard and well-defined metric in bioinformatics for evaluating genome assembly continuity. The problem is well-posed, providing all necessary data and a clear, unambiguous definition to arrive at a unique solution. The data provided are self-consistent and realistic. The problem is objective and free of any logical or factual flaws.\n\nWe proceed with the solution.\n\nThe task is to compute the NG50 statistic for a given draft genome assembly.\nThe set of contig lengths is given as $C = \\{100, 50, 20, 5, 5\\}$ in units of kilobase pairs (kbp).\nThe estimated haploid genome size is $G = 150$ kbp.\n\nThe definition of NG50 requires us to find the contig length $L$ that pushes the cumulative sum of contig lengths to meet or exceed $50\\%$ of the estimated genome size. The contigs must be sorted by length in descending order for this calculation.\n\nFirst, we calculate the target cumulative length, which is half of the estimated genome size:\n$$\n\\text{Target Sum} = \\frac{1}{2} G = \\frac{1}{2} \\times 150 \\text{ kbp} = 75 \\text{ kbp}\n$$\n\nNext, we sort the given contig lengths in nonincreasing (descending) order. The provided list is already sorted:\n$L = [100, 50, 20, 5, 5]$.\n\nNow, we compute the cumulative sum of these lengths, starting from the longest contig, and we check at each step if the sum has reached or exceeded the target of $75$ kbp.\n\nLet $l_i$ be the length of the $i$-th contig in the sorted list.\nLet $S_k$ be the cumulative sum of the lengths of the first $k$ contigs, $S_k = \\sum_{i=1}^{k} l_i$. We are looking for the smallest integer $k$ such that $S_k \\ge 75$. The NG50 is then the length of the $k$-th contig, $l_k$.\n\nStep 1: Consider the first (longest) contig.\nThe length is $l_1 = 100$ kbp.\nThe cumulative sum is $S_1 = l_1 = 100$ kbp.\n\nWe check if the cumulative sum meets the condition:\nIs $S_1 \\ge 75$ kbp?\n$100 \\ge 75$. Yes, the condition is satisfied.\n\nSince the condition is met with the very first contig, the process stops here. The index $k$ is $1$. The NG50 statistic is defined as the length of this specific contig, $l_k$, which is $l_1$.\n\nThus, the NG50 for this assembly is the length of the first contig.\n$$\n\\text{NG50} = l_1 = 100 \\text{ kbp}\n$$\nThe question asks for the answer to be expressed in kilobase pairs. The computed value is $100$.", "answer": "$$\n\\boxed{100}\n$$", "id": "2417502"}]}