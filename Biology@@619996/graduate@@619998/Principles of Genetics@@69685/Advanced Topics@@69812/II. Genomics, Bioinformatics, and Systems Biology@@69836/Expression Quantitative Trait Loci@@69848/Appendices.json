{"hands_on_practices": [{"introduction": "In eQTL analysis, unmeasured factors like batch effects or cell type composition can create spurious associations or mask true ones. A common strategy to mitigate this is to use Principal Component Analysis (PCA) on the expression data to capture and regress out these dominant sources of variation; however, this powerful technique harbors a subtle pitfall where a strong, true eQTL itself contributes significantly to a principal component, causing the genetic signal to be removed along with the confounder. This exercise [@problem_id:2810275] challenges you to mathematically derive the extent of this signal attenuation and then to devise a more principled correction method that purges confounding variation without corrupting the true genetic effect.", "problem": "A study of expression Quantitative Trait Loci (eQTLs) assesses how genetic variation influences messenger ribonucleic acid (mRNA) expression levels across individuals, consistent with the Central Dogma of molecular biology. Consider a cohort of $n$ unrelated individuals and a single gene whose centered expression vector is $y \\in \\mathbb{R}^{n}$. Let $g \\in \\mathbb{R}^{n}$ be the centered genotype dosage vector (e.g., minor allele count) at a candidate cis-acting variant, scaled so that $g^{\\top} g = n$. Suppose the gene’s expression is generated by a linear model\n$$\ny = \\beta\\, g + \\gamma\\, z + \\varepsilon,\n$$\nwhere $\\beta \\in \\mathbb{R}$ is the true eQTL effect, $z \\in \\mathbb{R}^{n}$ is a single unobserved batch confounder with $z^{\\top} z = n$ and $z^{\\top} g = 0$, $\\gamma \\in \\mathbb{R}$ is the confounder loading, and $\\varepsilon \\in \\mathbb{R}^{n}$ is zero-mean noise with $\\mathbb{E}[\\varepsilon] = 0$ and $\\operatorname{Cov}(\\varepsilon) = \\sigma^{2} I_{n}$ independent of $g$ and $z$. In practice, researchers often compute Principal Components (PCs) of $y$ to remove confounding. Assume that the top Principal Component (PC) extracted from $y$ is a unit vector $p \\in \\mathbb{R}^{n}$ that lies in the span of $z$ and $g$, parameterized by an angle $\\theta \\in \\mathbb{R}$ as\n$$\np \\;=\\; \\cos(\\theta)\\,\\frac{z}{\\sqrt{n}} \\;+\\; \\sin(\\theta)\\,\\frac{g}{\\sqrt{n}},\n$$\nso that $p^{\\top}p=1$. Consider the following two-step “naive” correction commonly used in expression analysis:\n1. Residualize expression by the top PC: $y_{\\mathrm{res}} = (I_{n} - p p^{\\top}) y$.\n2. Estimate the eQTL effect by ordinary least squares (OLS): $\\widehat{\\beta}_{\\mathrm{naive}} = \\dfrac{g^{\\top} y_{\\mathrm{res}}}{g^{\\top} g}$.\n\nStarting only from the definitions of linear regression and orthogonal projection in Euclidean space, derive the large-sample expectation (ignoring $\\varepsilon$ in the limit $n \\to \\infty$) of the naive estimator $\\widehat{\\beta}_{\\mathrm{naive}}$ in terms of $\\beta$, $\\gamma$, and $\\theta$. Then, propose a procedure to avoid attenuating true eQTL effects while removing confounders by constructing a PC direction $p_{\\perp g}$ that is orthogonal to $g$ and using it in place of $p$ in the same two-step correction; briefly justify why this change addresses the attenuation. \n\nReport as your final answer the closed-form analytic expression for the attenuation factor $A_{\\mathrm{naive}} \\equiv \\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}]/\\beta$ as a function of $\\theta$, $\\beta$, and $\\gamma$. No numerical evaluation is required. The final answer must be a single closed-form expression with no units.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a simplified but canonical model for investigating the effects of confounding on expression Quantitative Trait Loci (eQTL) analysis. We shall proceed with a rigorous derivation.\n\nThe analysis is partitioned into two parts as requested. First, we derive the large-sample expectation of the naive estimator. Second, we propose and justify a corrected procedure.\n\nPart 1: Derivation of the large-sample expectation of $\\widehat{\\beta}_{\\mathrm{naive}}$\n\nThe naive estimator for the eQTL effect $\\beta$ is given by\n$$\n\\widehat{\\beta}_{\\mathrm{naive}} = \\frac{g^{\\top} y_{\\mathrm{res}}}{g^{\\top} g}\n$$\nwhere $y_{\\mathrm{res}} = (I_{n} - p p^{\\top}) y$. The matrix $P_p = p p^{\\top}$ is the projection matrix onto the one-dimensional subspace spanned by the unit vector $p$, and $(I_n - P_p)$ is the projection matrix onto its orthogonal complement. The denominator is given as $g^{\\top} g = n$.\n\nSubstituting the expression for $y_{\\mathrm{res}}$ into the estimator, we have:\n$$\n\\widehat{\\beta}_{\\mathrm{naive}} = \\frac{g^{\\top} (I_{n} - p p^{\\top}) y}{n} = \\frac{g^{\\top}y - g^{\\top}p p^{\\top} y}{n} = \\frac{g^{\\top}y - (g^{\\top}p)(p^{\\top}y)}{n}\n$$\nThe problem requires the large-sample expectation, ignoring the noise term $\\varepsilon$. The expectation is taken with respect to the noise distribution.\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}] = \\frac{g^{\\top}\\mathbb{E}[y] - (g^{\\top}p)(p^{\\top}\\mathbb{E}[y])}{n}\n$$\nThe model for the expression vector is $y = \\beta g + \\gamma z + \\varepsilon$. Given $\\mathbb{E}[\\varepsilon]=0$, the expected value of $y$ is:\n$$\n\\mathbb{E}[y] = \\beta g + \\gamma z\n$$\nWe substitute this into our expression for $\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}]$. For clarity, let $y^{*} = \\mathbb{E}[y]$.\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}] = \\frac{g^{\\top}y^{*} - (g^{\\top}p)(p^{\\top}y^{*})}{n}\n$$\nWe must now compute the scalar products $g^{\\top}p$ and $p^{\\top}y^{*}$. The vector $p$ is defined as $p = \\cos(\\theta)\\,\\frac{z}{\\sqrt{n}} + \\sin(\\theta)\\,\\frac{g}{\\sqrt{n}}$.\n\nFirst, we calculate $g^{\\top}p$:\n$$\ng^{\\top}p = g^{\\top} \\left( \\cos(\\theta)\\frac{z}{\\sqrt{n}} + \\sin(\\theta)\\frac{g}{\\sqrt{n}} \\right) = \\frac{\\cos(\\theta)}{\\sqrt{n}} (g^{\\top}z) + \\frac{\\sin(\\theta)}{\\sqrt{n}} (g^{\\top}g)\n$$\nUsing the given conditions $g^{\\top}z = 0$ and $g^{\\top}g = n$, we find:\n$$\ng^{\\top}p = \\frac{\\cos(\\theta)}{\\sqrt{n}}(0) + \\frac{\\sin(\\theta)}{\\sqrt{n}}(n) = \\sqrt{n}\\sin(\\theta)\n$$\n\nNext, we calculate $p^{\\top}y^{*}$:\n$$\np^{\\top}y^{*} = p^{\\top}(\\beta g + \\gamma z) = \\beta (p^{\\top}g) + \\gamma (p^{\\top}z)\n$$\nWe already have $p^{\\top}g = g^{\\top}p = \\sqrt{n}\\sin(\\theta)$. We now need $p^{\\top}z$:\n$$\np^{\\top}z = \\left( \\cos(\\theta)\\frac{z}{\\sqrt{n}} + \\sin(\\theta)\\frac{g}{\\sqrt{n}} \\right)^{\\top} z = \\frac{\\cos(\\theta)}{\\sqrt{n}}(z^{\\top}z) + \\frac{\\sin(\\theta)}{\\sqrt{n}}(g^{\\top}z)\n$$\nUsing the conditions $z^{\\top}z = n$ and $g^{\\top}z = 0$:\n$$\np^{\\top}z = \\frac{\\cos(\\theta)}{\\sqrt{n}}(n) + \\frac{\\sin(\\theta)}{\\sqrt{n}}(0) = \\sqrt{n}\\cos(\\theta)\n$$\nSubstituting back into the expression for $p^{\\top}y^{*}$:\n$$\np^{\\top}y^{*} = \\beta(\\sqrt{n}\\sin(\\theta)) + \\gamma(\\sqrt{n}\\cos(\\theta)) = \\sqrt{n}(\\beta\\sin(\\theta) + \\gamma\\cos(\\theta))\n$$\n\nNow we assemble the numerator of $\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}]$: $g^{\\top}y^{*} - (g^{\\top}p)(p^{\\top}y^{*})$. The first term is $g^{\\top}y^{*} = g^{\\top}(\\beta g + \\gamma z) = \\beta(g^{\\top}g) + \\gamma(g^{\\top}z) = \\beta n$.\n$$\n\\text{Numerator} = \\beta n - (\\sqrt{n}\\sin(\\theta)) \\cdot (\\sqrt{n}(\\beta\\sin(\\theta) + \\gamma\\cos(\\theta)))\n$$\n$$\n= \\beta n - n(\\beta\\sin^{2}(\\theta) + \\gamma\\sin(\\theta)\\cos(\\theta))\n$$\n$$\n= n(\\beta - \\beta\\sin^{2}(\\theta) - \\gamma\\sin(\\theta)\\cos(\\theta))\n$$\nUsing the identity $1 - \\sin^{2}(\\theta) = \\cos^{2}(\\theta)$:\n$$\n= n(\\beta\\cos^{2}(\\theta) - \\gamma\\sin(\\theta)\\cos(\\theta))\n$$\nFinally, we divide by the denominator $n$ to obtain the large-sample expectation:\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}] = \\beta\\cos^{2}(\\theta) - \\gamma\\sin(\\theta)\\cos(\\theta)\n$$\nThis result demonstrates that the naive estimator is biased. The term $\\beta\\cos^{2}(\\theta)$ represents an attenuation of the true effect $\\beta$, as $\\cos^{2}(\\theta) \\leq 1$. The term $-\\gamma\\sin(\\theta)\\cos(\\theta)$ represents a bias dependent on the confounder effect $\\gamma$, which can be non-zero even if the true genetic effect $\\beta$ is zero, leading to false discoveries.\n\nPart 2: Corrected procedure to avoid attenuation\n\nThe cause of the attenuation and bias is that the Principal Component $p$ is correlated with the genotype vector $g$ (since $g^{\\top}p \\neq 0$ whenever $\\sin(\\theta) \\neq 0$). The corrective procedure must remove confounding variation without removing the genetic signal of interest. This can be achieved by ensuring that the vector used for correction is orthogonal to the genotype vector $g$.\n\nThe proposed procedure is as follows:\n$1$. Construct a new vector $p_{\\perp g}$ from the original PC $p$ by making it orthogonal to $g$. This is achieved by removing the component of $p$ that lies along $g$ via Gram-Schmidt orthogonalization. The unnormalized vector is:\n$$\np'_{\\perp g} = p - \\frac{p^{\\top}g}{g^{\\top}g}g\n$$\nThis vector $p'_{\\perp g}$ is orthogonal to $g$ by construction, as $g^{\\top}p'_{\\perp g} = g^{\\top}p - \\frac{(p^{\\top}g)(g^{\\top}g)}{g^{\\top}g} = 0$. For use in a projection matrix, it must be normalized to a unit vector:\n$$\np_{\\perp g} = \\frac{p'_{\\perp g}}{\\|p'_{\\perp g}\\|}\n$$\n$2$. Use this new vector $p_{\\perp g}$ in place of $p$ in the same two-step procedure:\n   a. Residualize expression: $y_{\\mathrm{res,corr}} = (I_{n} - p_{\\perp g} p_{\\perp g}^{\\top}) y$.\n   b. Estimate the eQTL effect: $\\widehat{\\beta}_{\\mathrm{corr}} = \\dfrac{g^{\\top} y_{\\mathrm{res,corr}}}{g^{\\top} g}$.\n\nTo justify this correction, we analyze the expectation of $\\widehat{\\beta}_{\\mathrm{corr}}$.\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{corr}}] = \\mathbb{E}\\left[\\frac{g^{\\top}(I_{n} - p_{\\perp g} p_{\\perp g}^{\\top}) y}{n}\\right] = \\frac{g^{\\top}y^{*} - (g^{\\top}p_{\\perp g})(p_{\\perp g}^{\\top}y^{*})}{n}\n$$\nBy construction, $g^{\\top}p_{\\perp g} = 0$. Therefore, the second term vanishes.\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{corr}}] = \\frac{g^{\\top}y^{*}}{n} = \\frac{g^{\\top}(\\beta g + \\gamma z)}{n} = \\frac{\\beta (g^{\\top}g) + \\gamma (g^{\\top}z)}{n}\n$$\nUsing $g^{\\top}g = n$ and $g^{\\top}z = 0$, we get:\n$$\n\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{corr}}] = \\frac{\\beta n + 0}{n} = \\beta\n$$\nThe estimator $\\widehat{\\beta}_{\\mathrm{corr}}$ is unbiased for $\\beta$. This procedure successfully removes variation associated with the confounder (as captured by the component of $p$ orthogonal to $g$) without attenuating or biasing the estimate of the true eQTL effect. This is a general principle: in a linear model, adjusting for covariates that are orthogonal to a regressor of interest does not bias the coefficient of that regressor.\n\nFinally, we compute the requested attenuation factor for the naive estimator, assuming $\\beta \\neq 0$.\n$$\nA_{\\mathrm{naive}} = \\frac{\\mathbb{E}[\\widehat{\\beta}_{\\mathrm{naive}}]}{\\beta} = \\frac{\\beta\\cos^{2}(\\theta) - \\gamma\\sin(\\theta)\\cos(\\theta)}{\\beta} = \\cos^{2}(\\theta) - \\frac{\\gamma}{\\beta}\\sin(\\theta)\\cos(\\theta)\n$$\nThis is the required closed-form expression.", "answer": "$$\n\\boxed{\\cos^{2}(\\theta) - \\frac{\\gamma}{\\beta} \\sin(\\theta) \\cos(\\theta)}\n$$", "id": "2810275"}, {"introduction": "When searching for a cis-eQTL, we test many variants near a gene for association, but due to linkage disequilibrium (LD), these tests are not independent. To determine if the strongest association found is truly significant, we need a method that accounts for both the number of tests and their correlation structure, as simple corrections are often too conservative. Permutation testing offers an elegant solution by empirically generating a null distribution for the maximum association statistic under the specific LD pattern of the gene. This practice [@problem_id:2810319] will guide you through the logic of this powerful technique and the derivation of the correct empirical $p$-value, a fundamental tool in computational genetics.", "problem": "A laboratory is mapping cis-expression quantitative trait loci (cis-eQTLs; expression quantitative trait loci) for a single gene using genotype and messenger ribonucleic acid (mRNA) expression from $n$ unrelated individuals. The cis-window is defined as $1$ megabase (Mb) upstream and downstream of the transcription start site. For this gene, there are $m$ single nucleotide polymorphisms (SNPs) in the cis-window. The association statistic for variant $k$ is computed by fitting a linear model of the form $Y = \\alpha + \\beta_{k} G_{k} + \\boldsymbol{\\gamma}^{\\top} \\mathbf{C} + \\varepsilon$, where $Y$ is log-transformed expression, $G_{k}$ is the genotype dosage at variant $k$, $\\mathbf{C}$ are measured covariates, and $\\varepsilon$ is an error term. The test statistic $T_{k}$ is taken to be the absolute value of the $t$-statistic for $\\beta_{k}$. The lead variant is the variant with the largest $T_{k}$, and its observed statistic is $T_{\\max}^{\\mathrm{obs}} = \\max_{1 \\leq k \\leq m} T_{k}$.\n\nTo calibrate the null distribution for this gene while respecting the correlation structure induced by linkage disequilibrium (LD) among the $m$ SNPs and the multiplicity of testing across these $m$ SNPs, the lab performs a permutation procedure: they first regress $Y$ on $\\mathbf{C}$ to obtain residuals $\\hat{\\varepsilon}_{i}$ for individuals $i = 1, \\dots, n$. Under the null hypothesis that no cis-variant affects expression after covariate adjustment, the residuals are independent of genotypes and are exchangeable across individuals. The lab then draws $B$ independent permutations of the residual vector, $\\pi_{1}, \\dots, \\pi_{B}$, and for each permutation $j$ recomputes the variant-level test statistics $\\{T_{k}^{(j)}\\}_{k=1}^{m}$ using the permuted residuals paired with the original genotypes and covariates, and records the permuted gene-level maximum $M^{(j)} = \\max_{1 \\leq k \\leq m} T_{k}^{(j)}$.\n\nExplain, from first principles, why this permutation-based calibration yields a valid empirical null distribution for $T_{\\max}$ in cis-eQTL mapping for this gene. Then, let $b$ denote the number of permutations among the $B$ draws for which $M^{(j)} \\geq T_{\\max}^{\\mathrm{obs}}$ (counting ties as exceedances to be conservative). Using only these ingredients and without invoking any external formulas, derive the finite-sample empirical $p$-value assigned to the observed lead variant statistic $T_{\\max}^{\\mathrm{obs}}$ based on the $B$ permutations and the $b$ exceedances. Express your final answer as a single closed-form analytic expression in terms of $B$ and $b$. No rounding is required, and no units are needed in the final expression.", "solution": "The problem presented is a valid and well-posed question in the field of statistical genetics. It describes a standard and statistically sound procedure for assessing the significance of a cis-expression quantitative trait locus (cis-eQTL) while properly accounting for confounders and the multiple testing burden across correlated genetic variants. I will first explain the validity of the permutation scheme from first principles and then derive the requested empirical $p$-value.\n\nThe first step is a formal statement of the null hypothesis, $H_{0}$. For a given gene, the null hypothesis is that there is no association between gene expression and the genotype at any of the $m$ single nucleotide polymorphisms (SNPs) within the cis-window, after accounting for the effects of a set of covariates $\\mathbf{C}$. More formally, for the linear model $Y = \\alpha + \\beta_{k} G_{k} + \\boldsymbol{\\gamma}^{\\top} \\mathbf{C} + \\varepsilon$ for each variant $k \\in \\{1, \\dots, m\\}$, the null hypothesis is $H_{0}: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{m} = 0$.\n\nUnder this null hypothesis, the model for gene expression $Y$ simplifies to $Y = \\alpha + \\boldsymbol{\\gamma}^{\\top} \\mathbf{C} + \\varepsilon$. This model implies that any variation in $Y$ not explained by the covariates $\\mathbf{C}$ is captured by the error term $\\varepsilon$, which is assumed to be independent of the genotypes $\\{G_{k}\\}_{k=1}^{m}$. The procedure described begins by regressing $Y$ on $\\mathbf{C}$ to obtain the vector of residuals, $\\hat{\\boldsymbol{\\varepsilon}}$. Each element $\\hat{\\varepsilon}_{i}$ of this vector represents the portion of individual $i$'s gene expression that is not linearly predicted by the measured covariates. Crucially, under $H_{0}$, these residuals are independent of the individuals' genotypes.\n\nBecause the $n$ individuals are stated to be unrelated, and under the assumption that the residuals are independent of genotypes, the vector of residuals $(\\hat{\\varepsilon}_{1}, \\dots, \\hat{\\varepsilon}_{n})$ is *exchangeable*. Exchangeability means that any permutation of the indices of the residuals results in a joint probability distribution identical to that of the original vector. Thus, shuffling the vector of residuals and re-pairing its elements with the fixed matrix of genotypes and covariates creates a new, synthetic dataset that is statistically consistent with the null hypothesis.\n\nThe described procedure leverages this principle. For each of the $B$ permutations, a permuted residual vector $\\hat{\\boldsymbol{\\varepsilon}}^{(\\pi_{j})}$ is generated. This is then used to construct a permuted expression vector, let us call it $Y^{(j)}$, where the effect of the covariates is added back to the permuted residuals. The test statistics $\\{T_{k}^{(j)}\\}_{k=1}^{m}$ are then recomputed by testing the association between $Y^{(j)}$ and each genotype vector $G_{k}$. Since any true association between expression and genotype has been deliberately broken by the permutation, any large test statistic that arises in this permuted dataset is purely a result of chance under the null model.\n\nThis permutation method offers two critical advantages over simpler analytical approaches. First, it non-parametrically accounts for the complex correlation structure among the test statistics $\\{T_{k}\\}_{k=1}^{m}$. This correlation arises from linkage disequilibrium (LD) between the SNPs; nearby variants on a chromosome are not inherited independently. By keeping the genotype data for each individual intact and only permuting the phenotype (residuals) across individuals, the procedure correctly preserves the LD structure in every permutation.\n\nSecond, it directly addresses the multiple testing problem. The gene-level statistic of interest is $T_{\\max} = \\max_{1 \\leq k \\leq m} T_{k}$, the maximum of $m$ correlated statistics. The theoretical distribution of this maximum is generally intractable. By calculating the maximum statistic $M^{(j)} = \\max_{1 \\leq k \\leq m} T_{k}^{(j)}$ for each of the $B$ permuted datasets, we generate an empirical sample $\\{M^{(j)}\\}_{j=1}^{B}$ from the correct null distribution of $T_{\\max}$. This collection of $B$ values represents the distribution of the maximum association statistic one would expect to see for this gene under the null hypothesis of no genetic effect, while fully respecting the existing covariate effects and SNP correlation structure. Therefore, the procedure yields a valid empirical null distribution for $T_{\\max}$.\n\nWe now derive the empirical $p$-value. A $p$-value is defined as the probability of observing a test statistic at least as extreme as the one actually observed, assuming the null hypothesis is true. Here, the observed statistic is $T_{\\max}^{\\mathrm{obs}}$, and we want to calculate $p = \\mathbb{P}(T_{\\max} \\geq T_{\\max}^{\\mathrm{obs}} | H_{0})$.\n\nOur permutation procedure has provided a sample of size $B$ from the null distribution: $\\{M^{(1)}, M^{(2)}, \\dots, M^{(B)}\\}$. To compute the empirical $p$-value, we must compare our observed statistic, $T_{\\max}^{\\mathrm{obs}}$, to this empirical null distribution. In the framework of a finite-sample permutation test, the observed data configuration (i.e., the \"identity permutation\") is considered as one of the possible outcomes under the null hypothesis. The total set of test statistics to consider for our reference distribution is therefore the union of the permuted statistics and the observed statistic itself: $\\{T_{\\max}^{\\mathrm{obs}}, M^{(1)}, M^{(2)}, \\dots, M^{(B)}\\}$. The total number of equally plausible statistics in this reference set is $B+1$.\n\nThe problem states that $b$ is the number of permutations for which the permuted maximum, $M^{(j)}$, is greater than or equal to the observed maximum, $T_{\\max}^{\\mathrm{obs}}$. That is, $b = \\sum_{j=1}^{B} \\mathbb{I}(M^{(j)} \\geq T_{\\max}^{\\mathrm{obs}})$, where $\\mathbb{I}(\\cdot)$ is the indicator function. These $b$ outcomes are at least as extreme as the observed outcome. In addition to these $b$ values, the observed statistic $T_{\\max}^{\\mathrm{obs}}$ is itself always greater than or equal to itself ($T_{\\max}^{\\mathrm{obs}} \\geq T_{\\max}^{\\mathrm{obs}}$). Therefore, we must count the observed statistic as one of the \"extreme\" outcomes. This gives a total of $b+1$ statistics in our reference set that are at least as extreme as $T_{\\max}^{\\mathrm{obs}}$.\n\nThe empirical $p$-value is the proportion of statistics in the full reference set that are at least as extreme as the observed value. This is the ratio of the count of extreme outcomes to the total number of outcomes.\n\n$$\np_{\\text{empirical}} = \\frac{\\text{Number of statistics } \\geq T_{\\max}^{\\mathrm{obs}}}{\\text{Total number of statistics}}\n$$\n\nSubstituting the counts derived above:\n\n$$\np_{\\text{empirical}} = \\frac{b + 1}{B + 1}\n$$\n\nThis formulation is standard for Monte Carlo and permutation tests as it avoids a $p$-value of $0$, which is philosophically difficult to justify with a finite number of permutations, and it has more favorable statistical properties than the alternative $b/B$. The minimum possible $p$-value is $1/(B+1)$, correctly reflecting the resolution of the test.", "answer": "$$\\boxed{\\frac{b+1}{B+1}}$$", "id": "2810319"}, {"introduction": "A typical eQTL study tests for associations across thousands of genes, generating a massive volume of hypotheses and corresponding $p$-values, where using a conventional significance threshold like $p  0.05$ would lead to an unacceptably high number of false positives. This exercise [@problem_id:2810282] moves beyond single-test statistics to the crucial challenge of multiple testing correction, asking you to derive from first principles a procedure that controls the False Discovery Rate (FDR). Understanding how to implement and interpret FDR control is an essential skill for making reliable discoveries in any genome-scale analysis.", "problem": "In a population genetics study of expression Quantitative Trait Locus (eQTL) mapping, you test for association between genotype and messenger ribonucleic acid (mRNA) expression for $m$ independent variant–gene pairs using a linear model for each pair. For each test you compute a $p$-value, denoted $p_{1},\\dots,p_{m}$. An expression Quantitative Trait Locus (eQTL) is declared when a variant–gene pair shows statistical evidence of association between genotype and expression. You wish to control the False Discovery Rate (FDR), defined as the expected proportion of false eQTL calls among all eQTL calls, at a target level $q \\in (0,1)$.\n\nStarting from the following foundational facts:\n- Under a true null hypothesis, the $p$-value is uniformly distributed on $[0,1]$ and independent across tests.\n- The False Discovery Rate (FDR) is defined as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{R}\\right]$, where $V$ is the number of false eQTL calls and $R$ is the total number of eQTL calls, with the convention that $\\frac{0}{0}=0$.\n\nTasks:\n1) Derive, from these principles, a decision rule that maps the vector of $p$-values $(p_{1},\\dots,p_{m})$ to a set of discoveries that controls the FDR at the target level $q$. Your answer must be an explicit analytic condition that a $p$-value must satisfy in order to be called significant, expressed in terms of $m$, $q$, and the order statistics of the $p$-values.\n\n2) Suppose the decision rule in part $1$ yields exactly $r \\ge 0$ eQTL calls in a particular experiment. Assuming the attained False Discovery Rate equals the design target $q$ under the stated conditions (independent and continuous null $p$-values), give a closed-form expression for the expected number of false eQTL calls in terms of $q$ and $r$. Your final answer must be a single analytic expression. No rounding is required, and no units are needed.", "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. It describes the canonical problem of multiple hypothesis testing correction using the False Discovery Rate (FDR), a fundamental concept in modern statistical genetics and other fields involving large-scale data analysis. We shall, therefore, proceed to the solution.\n\nThe problem consists of two parts. First, we must derive a decision rule for controlling the FDR. Second, we must compute the expected number of false discoveries given a specific outcome.\n\nPart 1: Derivation of the FDR Control Procedure\n\nLet there be $m$ independent hypothesis tests, with corresponding $p$-values $p_{1}, \\dots, p_{m}$. Let $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$ be the $p$-values sorted in non-decreasing order. The goal is to devise a procedure that identifies a set of \"discoveries\" (rejected null hypotheses) such that the FDR is controlled at a pre-specified level $q \\in (0,1)$.\n\nThe FDR is defined as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{R}\\right]$, where $V$ is the number of false discoveries (true null hypotheses that are incorrectly rejected) and $R$ is the total number of discoveries. The convention $\\frac{0}{0}=0$ is used. A foundational fact is that for a true null hypothesis, its $p$-value follows a Uniform distribution on $[0,1]$.\n\nThe decision rule that achieves this control is the Benjamini-Hochberg (BH) procedure. We derive this rule and prove its controlling property.\n\nThe BH procedure is as follows:\n1.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m}q$.\n2.  If such a $k$ exists, reject the $k$ null hypotheses corresponding to the $p$-values $p_{(1)}, p_{(2)}, \\dots, p_{(k)}$.\n3.  If no such $k$ exists, reject no hypotheses.\n\nNow, we must prove that this procedure controls the FDR at level $q$. Let $R$ denote the total number of rejections. By the procedure's definition, $R = k$. Let $\\mathcal{H}_0$ be the set of indices corresponding to true null hypotheses, and let $m_0 = |\\mathcal{H}_0|$ be the (unknown) number of true nulls. The number of false rejections is $V = \\sum_{i \\in \\mathcal{H}_0} \\mathbf{1}\\{H_i \\text{ is rejected}\\}$. The quantity $\\frac{V}{R}$ is the False Discovery Proportion (FDP).\n\nIf $R=0$, then $V=0$ and $\\frac{V}{R}=0$. Consider the case $R  0$. Every rejected hypothesis $H_i$ has a $p$-value $p_i \\le p_{(R)}$. By the definition of the procedure, the threshold $p_{(R)}$ satisfies $p_{(R)} \\le \\frac{R}{m}q$. Consequently, for any rejected hypothesis $H_i$, its $p$-value must satisfy $p_i \\le \\frac{R}{m}q$.\n\nThis allows us to write an inequality for the indicator function of a false rejection:\n$$ \\mathbf{1}\\{H_i \\text{ is rejected}, i \\in \\mathcal{H}_0\\} \\le \\mathbf{1}\\{p_i \\le \\frac{R}{m}q, i \\in \\mathcal{H}_0\\} $$\nThe FDP can be bounded:\n$$ \\frac{V}{R} = \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} \\mathbf{1}\\{H_i \\text{ is rejected}\\} \\le \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} \\mathbf{1}\\left\\{p_i \\le \\frac{R}{m}q\\right\\} $$\nWe now take the expectation to find the FDR. Using the law of total expectation by conditioning on $R$:\n$$ \\mathrm{FDR} = \\mathbb{E}\\left[\\frac{V}{R}\\right] \\le \\mathbb{E}\\left[ \\mathbb{E}\\left[ \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} \\mathbf{1}\\left\\{p_i \\le \\frac{R}{m}q\\right\\} \\Bigg| R \\right] \\right] $$\nInside the conditional expectation, $R$ is treated as a fixed value. Let $t_R = \\frac{R}{m}q$. The expression becomes:\n$$ \\mathbb{E}\\left[ \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} \\mathbf{1}\\{p_i \\le t_R\\} \\Bigg| R \\right] = \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} P(p_i \\le t_R \\mid R) $$\nUnder the stated assumption that tests are independent, the distribution of a null $p$-value $p_i$ is independent of the outcomes of other tests, and thus conditioning on $R$ does not alter its marginal probability. For any true null $H_i$, $p_i \\sim U[0,1]$, so $P(p_i \\le t_R) = t_R$.\n$$ \\frac{1}{R} \\sum_{i \\in \\mathcal{H}_0} t_R = \\frac{1}{R} \\cdot m_0 \\cdot t_R = \\frac{m_0}{R} \\cdot \\frac{R}{m}q = \\frac{m_0}{m}q $$\nThis result does not depend on the value of $R$. Therefore, the outer expectation is also $\\frac{m_0}{m}q$.\n$$ \\mathrm{FDR} \\le \\frac{m_0}{m}q $$\nSince the number of true nulls $m_0$ cannot exceed the total number of tests $m$, we have $m_0 \\le m$, which implies $\\frac{m_0}{m}q \\le q$. Thus, the procedure controls the FDR at a level no greater than $q$.\n\nThe explicit analytic condition for a given $p$-value $p_j$ to be declared significant is that it must be less than or equal to the largest $p$-value that is rejected. Let $t_{BH}$ be this data-dependent threshold.\n$$ t_{BH} = \\max\\left\\{ p_{(k)} \\mid k \\in \\{1,\\dots,m\\} \\text{ and } p_{(k)} \\le \\frac{k q}{m} \\right\\} $$\nwhere the maximum over an empty set is defined to be $0$. A $p$-value $p_j$ leads to a discovery if and only if $p_j \\le t_{BH}$. This completes the first part.\n\nPart 2: Expected Number of False Discoveries\n\nWe are given that an experiment yields exactly $R=r$ discoveries, where $r \\ge 0$ is a fixed number. We must find the expected number of false discoveries, conditioned on this observation: $\\mathbb{E}[V | R=r]$.\n\nWe are also given the critical assumption that the attained FDR equals the design target $q$, i.e., $\\mathbb{E}\\left[\\frac{V}{R}\\right] = q$.\nThe FDR is the expectation of the FDP, $\\frac{V}{R}$. This expectation can be decomposed by conditioning on the value of $R$:\n$$ \\mathbb{E}\\left[\\frac{V}{R}\\right] = \\sum_{k=1}^{m} \\mathbb{E}\\left[\\frac{V}{R} \\Bigg| R=k\\right] P(R=k) + \\mathbb{E}\\left[\\frac{V}{R} \\Bigg| R=0\\right] P(R=0) $$\nBy convention, the term for $R=0$ is $0$. For $k  0$, the random variable $R$ is fixed to the value $k$ inside the conditional expectation, so $\\mathbb{E}\\left[\\frac{V}{R} \\Big| R=k\\right] = \\frac{\\mathbb{E}[V|R=k]}{k}$.\n\nThe assumption that the attained FDR is exactly $q$ is a strong simplification. It implies that the complex average over all possible outcomes $k$ is equal to $q$. The most direct and practical interpretation of such a statement in this context is that the expected proportion of false discoveries is $q$ for any non-zero number of discoveries. This means assuming that the conditional FDR, $\\mathbb{E}\\left[\\frac{V}{R} \\Big| R=r\\right]$, is equal to $q$ for any specific outcome $r  0$.\n\nUnder this interpretation, for a given number of discoveries $r  0$, we have:\n$$ \\mathbb{E}\\left[\\frac{V}{R} \\Bigg| R=r\\right] = q $$\n$$ \\frac{\\mathbb{E}[V|R=r]}{r} = q $$\nSolving for the expected number of false discoveries gives:\n$$ \\mathbb{E}[V|R=r] = qr $$\nThis expression gives the expected number of false positives when $r$ discoveries are made. We must also check the case $r=0$. If no discoveries are made ($r=0$), then the number of false discoveries $V$ must be $0$. The expectation is $\\mathbb{E}[V|R=0] = 0$. Our formula $qr$ gives $q \\cdot 0 = 0$, so it holds for all $r \\ge 0$.\nThe problem asks for a single analytic expression, and this result fulfills that requirement.", "answer": "$$\\boxed{qr}$$", "id": "2810282"}]}